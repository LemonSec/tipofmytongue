1
00:00:28,480 --> 00:00:37,610
Goswami diminish your good evening

2
00:00:32,210 --> 00:00:41,780
ladies and gentlemen Bienvenue or IETF

3
00:00:37,610 --> 00:00:43,190
welcome to the IETF we have an

4
00:00:41,780 --> 00:00:46,660
announcement to make

5
00:00:43,190 --> 00:00:48,739
you know for and I'll say kefka's shows

6
00:00:46,660 --> 00:00:50,750
we're having a bit of a technical

7
00:00:48,739 --> 00:00:52,640
problem up front with the projectors

8
00:00:50,750 --> 00:00:57,280
these lovely people are working on it

9
00:00:52,640 --> 00:01:00,559
but we want to let you know that the

10
00:00:57,280 --> 00:01:03,079
download all of the presentations from

11
00:01:00,559 --> 00:01:06,649
the from the website now this is a

12
00:01:03,079 --> 00:01:10,009
stress test for the network because we

13
00:01:06,649 --> 00:01:13,520
may have to go forward with the rest of

14
00:01:10,009 --> 00:01:17,229
the program with those in your laptop's

15
00:01:13,520 --> 00:01:19,880
as opposed to on these nice big screens

16
00:01:17,229 --> 00:01:22,009
so if you would like to find your way to

17
00:01:19,880 --> 00:01:24,500
the agenda page now and start at

18
00:01:22,009 --> 00:01:26,270
downloading the next step will be Brian

19
00:01:24,500 --> 00:01:28,940
Trammell who will introduce our speakers

20
00:01:26,270 --> 00:01:30,829
and provide backing interpretive dance

21
00:01:28,940 --> 00:01:35,060
I've been joking about that but there's

22
00:01:30,829 --> 00:01:37,970
an actual separate stage for it normally

23
00:01:35,060 --> 00:01:39,799
we would let this debugging go on a

24
00:01:37,970 --> 00:01:41,030
little bit longer but we have a quite a

25
00:01:39,799 --> 00:01:43,399
tough schedule tonight

26
00:01:41,030 --> 00:01:46,040
and a lot of people would be upset if we

27
00:01:43,399 --> 00:01:48,100
ran late so we're going to try it this

28
00:01:46,040 --> 00:01:48,100
way

29
00:01:48,110 --> 00:01:53,750
please do download them and please do

30
00:01:51,619 --> 00:01:55,520
give your attention to the speakers even

31
00:01:53,750 --> 00:01:59,320
if there aren't any pretty lights

32
00:01:55,520 --> 00:01:59,320
many thanks right

33
00:02:21,370 --> 00:02:26,450
so I have the slides here on the monitor

34
00:02:24,680 --> 00:02:30,859
in front of me so if everyone could

35
00:02:26,450 --> 00:02:33,078
please come to the front hi I'm Brian

36
00:02:30,859 --> 00:02:36,139
Trammell I be the emcee for the

37
00:02:33,079 --> 00:02:37,639
technical portion of tonight's whoo or

38
00:02:36,139 --> 00:02:42,260
you can hang out on me Tyco which will

39
00:02:37,639 --> 00:02:45,769
also be a stress test for me deco also a

40
00:02:42,260 --> 00:02:48,200
stress test so as a beginning note were

41
00:02:45,769 --> 00:02:50,569
you'll notice on the agendas that we're

42
00:02:48,200 --> 00:02:53,000
trying out something new this time we're

43
00:02:50,569 --> 00:02:55,189
more explicitly splitting the tech and

44
00:02:53,000 --> 00:02:57,169
tech and admin plenaries um you know

45
00:02:55,189 --> 00:02:59,388
wait way way back in the past these were

46
00:02:57,169 --> 00:03:00,739
on separate evenings and then they sort

47
00:02:59,389 --> 00:03:02,840
of came together and then they sort of

48
00:03:00,739 --> 00:03:07,099
came even farther together into a single

49
00:03:02,840 --> 00:03:08,389
session we are splitting these out

50
00:03:07,099 --> 00:03:10,369
explicitly to make sure that we have

51
00:03:08,389 --> 00:03:12,370
enough time for the tech plenary as well

52
00:03:10,370 --> 00:03:15,260
as enough time for the admin plenary

53
00:03:12,370 --> 00:03:16,579
this part of the session is one hour

54
00:03:15,260 --> 00:03:19,040
long which is why we were going ahead

55
00:03:16,579 --> 00:03:23,150
and getting started even though we don't

56
00:03:19,040 --> 00:03:25,909
have video yet um we're going to be

57
00:03:23,150 --> 00:03:27,169
holding pretty strictly to time there

58
00:03:25,909 --> 00:03:28,250
will be time for questions and

59
00:03:27,169 --> 00:03:32,090
discussion at the end

60
00:03:28,250 --> 00:03:33,530
um so um you know clarifying questions

61
00:03:32,090 --> 00:03:40,790
only but please know clarifying

62
00:03:33,530 --> 00:03:46,370
questions um with that privacy question

63
00:03:40,790 --> 00:03:49,459
mark what's the delay on okay all right

64
00:03:46,370 --> 00:03:52,879
yes next slide please

65
00:03:49,459 --> 00:03:54,290
as some of you may be aware the IB and

66
00:03:52,879 --> 00:03:56,780
the IETF at least we hope many in the

67
00:03:54,290 --> 00:03:58,698
IETF is deeply interested in

68
00:03:56,780 --> 00:04:01,269
confidentiality on the Internet this is

69
00:03:58,699 --> 00:04:04,099
a conversation that we've had ongoing

70
00:04:01,269 --> 00:04:07,430
for a while we're interested in this in

71
00:04:04,099 --> 00:04:08,509
large part for reasons of privacy um we

72
00:04:07,430 --> 00:04:12,739
spend a lot of time in the working

73
00:04:08,509 --> 00:04:14,388
groups these days hmm that is the most

74
00:04:12,739 --> 00:04:16,969
applause I thought I would ever get for

75
00:04:14,389 --> 00:04:21,380
privacy with a question mark and an IETF

76
00:04:16,969 --> 00:04:25,370
meeting post Vancouver I'll go ahead and

77
00:04:21,380 --> 00:04:27,139
it gets better um uh we've been talking

78
00:04:25,370 --> 00:04:29,719
a lot about in the in the the working

79
00:04:27,139 --> 00:04:31,490
groups and in the hallways and you know

80
00:04:29,719 --> 00:04:33,800
around the working groups and um

81
00:04:31,490 --> 00:04:35,419
the press and sort of everywhere it's

82
00:04:33,800 --> 00:04:38,330
been a while since we've addressed it in

83
00:04:35,419 --> 00:04:40,310
plenary so we'd like to change that

84
00:04:38,330 --> 00:04:42,948
tonight we have a program where we'd

85
00:04:40,310 --> 00:04:46,430
like to talk about current issues and

86
00:04:42,949 --> 00:04:49,160
eternal issues in Internet privacy with

87
00:04:46,430 --> 00:04:50,360
Arvind or Ivan and Ryan I worked so hard

88
00:04:49,160 --> 00:04:55,400
to get your name right

89
00:04:50,360 --> 00:04:56,720
and Steve elleven Arvin Ryan is an

90
00:04:55,400 --> 00:04:59,448
associate professor of computer science

91
00:04:56,720 --> 00:05:01,370
at Princeton he leads the Princeton web

92
00:04:59,449 --> 00:05:02,780
transparency and accountability project

93
00:05:01,370 --> 00:05:04,130
he's also the recipient of the

94
00:05:02,780 --> 00:05:07,630
presidential Early Career Award for

95
00:05:04,130 --> 00:05:09,680
scientists and engineers if there's a

96
00:05:07,630 --> 00:05:11,419
award ceremony for this that he's

97
00:05:09,680 --> 00:05:14,090
missing to be with us tonight so we're

98
00:05:11,419 --> 00:05:14,780
very very honored to have him here he'll

99
00:05:14,090 --> 00:05:16,099
be talking about some of the

100
00:05:14,780 --> 00:05:18,109
implications and current trends and

101
00:05:16,099 --> 00:05:22,099
communicate on communications privacy in

102
00:05:18,110 --> 00:05:23,539
large current trends on communication

103
00:05:22,099 --> 00:05:26,389
parts the internet sort of in the large

104
00:05:23,539 --> 00:05:28,639
so he's sort of a contextual look at

105
00:05:26,389 --> 00:05:30,470
this Steve Delavan needs no introduction

106
00:05:28,639 --> 00:05:34,370
in this room but I'm gonna try to do so

107
00:05:30,470 --> 00:05:37,430
anyway um he is a former member of the

108
00:05:34,370 --> 00:05:38,690
IAB a former security Area Director he

109
00:05:37,430 --> 00:05:41,509
was instrumental in the creation of

110
00:05:38,690 --> 00:05:42,469
Usenet um some of you may have heard of

111
00:05:41,509 --> 00:05:44,449
that

112
00:05:42,469 --> 00:05:46,849
he's currently professor of computer

113
00:05:44,449 --> 00:05:49,400
science at Columbia and affiliate at

114
00:05:46,849 --> 00:05:50,599
Columbia law he will be showing us

115
00:05:49,400 --> 00:05:53,359
tonight that an Internet privacy

116
00:05:50,599 --> 00:06:15,199
everything old is new again so thank you

117
00:05:53,360 --> 00:06:17,150
both Arvind come on so I'd like to share

118
00:06:15,199 --> 00:06:20,210
with you what I've learned from a decade

119
00:06:17,150 --> 00:06:22,400
of doing privacy measurement is kind of

120
00:06:20,210 --> 00:06:24,440
a boring sounding term but what it

121
00:06:22,400 --> 00:06:26,750
really means is trying to find privacy

122
00:06:24,440 --> 00:06:28,789
vulnerabilities ideally on a large scale

123
00:06:26,750 --> 00:06:30,860
I'm talking millions of endpoints and an

124
00:06:28,789 --> 00:06:33,080
automated or mostly automated way and

125
00:06:30,860 --> 00:06:35,210
before I do that I'll start with a

126
00:06:33,080 --> 00:06:37,340
couple of caveats one is I'm going to be

127
00:06:35,210 --> 00:06:39,560
really upfront that most of my work has

128
00:06:37,340 --> 00:06:41,479
been in the web space and my prior

129
00:06:39,560 --> 00:06:43,699
engagement with standards agencies spend

130
00:06:41,479 --> 00:06:45,359
with the w3c and that's what a lot of

131
00:06:43,699 --> 00:06:46,590
this is going to be informed by but

132
00:06:45,360 --> 00:06:48,810
that's what I'm going to try really hard

133
00:06:46,590 --> 00:06:50,219
to do is extract some principles that

134
00:06:48,810 --> 00:06:51,870
are much more broadly applicable and

135
00:06:50,219 --> 00:06:53,789
that's what I'd like to share with you

136
00:06:51,870 --> 00:06:55,650
today and another thing that's going to

137
00:06:53,789 --> 00:06:57,180
be a common theme of the presentation is

138
00:06:55,650 --> 00:06:58,830
that I'm going to be talking about

139
00:06:57,180 --> 00:07:00,360
issues beyond encryption I'm going to

140
00:06:58,830 --> 00:07:02,729
assume that we're in a world with

141
00:07:00,360 --> 00:07:04,110
pervasive encryption and in fact some of

142
00:07:02,729 --> 00:07:06,240
the things that I'll touch upon are

143
00:07:04,110 --> 00:07:08,430
perhaps some downsides of encryption or

144
00:07:06,240 --> 00:07:10,469
privacy and how we can try to mitigate

145
00:07:08,430 --> 00:07:11,939
them so that already sounds surprising

146
00:07:10,469 --> 00:07:14,099
to some of you so I hope this will be an

147
00:07:11,939 --> 00:07:16,319
interesting discussion I should also say

148
00:07:14,099 --> 00:07:18,419
that that as you've heard from the intro

149
00:07:16,319 --> 00:07:20,009
I'm an academic so my job is to think

150
00:07:18,419 --> 00:07:21,240
you know idealistic lucify thoughts

151
00:07:20,009 --> 00:07:22,620
there are going to be points where

152
00:07:21,240 --> 00:07:24,029
you're going to feel oh this will never

153
00:07:22,620 --> 00:07:25,860
work in the real world and you're

154
00:07:24,029 --> 00:07:27,449
welcome to come say that I'm Q&A that's

155
00:07:25,860 --> 00:07:30,300
totally our game and I appreciate that

156
00:07:27,449 --> 00:07:31,830
okay so with those caveats here are

157
00:07:30,300 --> 00:07:34,229
three things that I want to share with

158
00:07:31,830 --> 00:07:36,029
you the first thing is an issue that

159
00:07:34,229 --> 00:07:38,219
very options up when we're talking about

160
00:07:36,029 --> 00:07:40,860
privacy beyond approaching especially

161
00:07:38,219 --> 00:07:42,449
when we're talking about more subtle

162
00:07:40,860 --> 00:07:44,819
privacy threats such as device

163
00:07:42,449 --> 00:07:46,319
fingerprinting an argument that often

164
00:07:44,819 --> 00:07:48,990
comes up is oh forget about

165
00:07:46,319 --> 00:07:50,940
fingerprinting that ship has sailed the

166
00:07:48,990 --> 00:07:52,979
horses left the barn it's too late for

167
00:07:50,940 --> 00:07:55,050
fingerprinting defenses there are too

168
00:07:52,979 --> 00:07:56,460
many fingerprinting vectors too easy to

169
00:07:55,050 --> 00:07:58,229
do tracking so we should just forget

170
00:07:56,460 --> 00:07:59,940
about that and accept that we're going

171
00:07:58,229 --> 00:08:10,740
to be in a world where it's really easy

172
00:07:59,940 --> 00:08:12,839
to track profile people that's the first

173
00:08:10,740 --> 00:08:14,430
thing I tell you about and specifically

174
00:08:12,839 --> 00:08:17,060
this this has come up a lot in the web

175
00:08:14,430 --> 00:08:17,060
context

176
00:08:30,380 --> 00:08:33,448
[Music]

177
00:08:52,730 --> 00:08:55,850
[Music]

178
00:09:10,580 --> 00:09:13,629
[Music]

179
00:09:59,450 --> 00:10:04,350
and ironically you can see on the list

180
00:10:02,010 --> 00:10:06,150
privacy features like do not track also

181
00:10:04,350 --> 00:10:07,830
contribute to fingerprinting because do

182
00:10:06,150 --> 00:10:09,689
you have do not track and able do not

183
00:10:07,830 --> 00:10:12,480
have it enable so that's a small amount

184
00:10:09,690 --> 00:10:14,670
of function entropy etc so one thing you

185
00:10:12,480 --> 00:10:15,900
might conclude from this is you know the

186
00:10:14,670 --> 00:10:17,520
horses left the barn

187
00:10:15,900 --> 00:10:19,350
fingerprinting is devastatingly

188
00:10:17,520 --> 00:10:22,560
effective we shouldn't even try to

189
00:10:19,350 --> 00:10:24,090
minimize the fingerprint ability of new

190
00:10:22,560 --> 00:10:26,310
features that we put into the standard

191
00:10:24,090 --> 00:10:28,830
now the w3c to their credit there were a

192
00:10:26,310 --> 00:10:30,239
lot of people who still try to minimize

193
00:10:28,830 --> 00:10:32,640
the fingerprint ability of new features

194
00:10:30,240 --> 00:10:34,080
and I don't want to present this as a

195
00:10:32,640 --> 00:10:36,470
criticism of other people this was

196
00:10:34,080 --> 00:10:38,850
absolutely me up until about a year ago

197
00:10:36,470 --> 00:10:39,930
so this is what I believed and here's

198
00:10:38,850 --> 00:10:42,030
why I changed my mind

199
00:10:39,930 --> 00:10:44,969
so those studies that I present it they

200
00:10:42,030 --> 00:10:47,130
were really you know excellent studies

201
00:10:44,970 --> 00:10:48,570
but there was something wrong with the

202
00:10:47,130 --> 00:10:50,430
way that a lot of people interpreted

203
00:10:48,570 --> 00:10:52,680
them one weird thing about those studies

204
00:10:50,430 --> 00:10:55,260
is that the users who participated were

205
00:10:52,680 --> 00:10:56,910
self selected so does that mean that the

206
00:10:55,260 --> 00:10:58,500
results could be non representative in

207
00:10:56,910 --> 00:11:00,900
some way what could be different about

208
00:10:58,500 --> 00:11:02,850
self selected users one possibility is

209
00:11:00,900 --> 00:11:04,500
that all the users who had self select

210
00:11:02,850 --> 00:11:06,360
into a study like that are actually

211
00:11:04,500 --> 00:11:07,740
really tech-savvy people the kind of

212
00:11:06,360 --> 00:11:09,630
people who are likely to make a lot of

213
00:11:07,740 --> 00:11:11,190
modifications in their browsers that

214
00:11:09,630 --> 00:11:14,130
would make them more unique and more

215
00:11:11,190 --> 00:11:16,200
fingerprint so that was one interesting

216
00:11:14,130 --> 00:11:17,760
kind of bias that some researchers

217
00:11:16,200 --> 00:11:19,530
suspected could be in those studies

218
00:11:17,760 --> 00:11:21,150
including some of the researchers at

219
00:11:19,530 --> 00:11:22,829
INRIA who were responsible for one of

220
00:11:21,150 --> 00:11:24,150
those studies and then what they did was

221
00:11:22,830 --> 00:11:26,550
they partnered with a major french

222
00:11:24,150 --> 00:11:28,740
website and fingerprinted all of the

223
00:11:26,550 --> 00:11:30,510
users of the of that website without

224
00:11:28,740 --> 00:11:31,920
really telling them so by doing this

225
00:11:30,510 --> 00:11:33,660
lightly ethically questionable thing

226
00:11:31,920 --> 00:11:35,699
they did a statistically much more

227
00:11:33,660 --> 00:11:38,400
rigorous thing and they published that

228
00:11:35,700 --> 00:11:40,110
study which found in fact contrary to

229
00:11:38,400 --> 00:11:42,420
some of the previous findings only a

230
00:11:40,110 --> 00:11:44,940
third of the users were unique and as

231
00:11:42,420 --> 00:11:46,680
more and more activity shifts to mobile

232
00:11:44,940 --> 00:11:48,240
less than a fifth of mobile users were

233
00:11:46,680 --> 00:11:51,209
unique because those devices are less

234
00:11:48,240 --> 00:11:53,730
customizable and further has flash and

235
00:11:51,209 --> 00:11:55,619
Java and other old plugins are

236
00:11:53,730 --> 00:11:57,630
phased-out that number is actually going

237
00:11:55,620 --> 00:11:59,820
down and if you look at for me when I

238
00:11:57,630 --> 00:12:01,589
looked at the findings of this study the

239
00:11:59,820 --> 00:12:03,389
conclusion was very different even

240
00:12:01,589 --> 00:12:05,070
little things that browsers can do in

241
00:12:03,389 --> 00:12:06,510
order to minimize the fingerprint

242
00:12:05,070 --> 00:12:09,089
ability of features are going to have a

243
00:12:06,510 --> 00:12:11,490
big impact and so I came away with this

244
00:12:09,089 --> 00:12:14,100
with a very different point of view then

245
00:12:11,490 --> 00:12:15,389
a lot of people have had before before

246
00:12:14,100 --> 00:12:17,579
this study which is let's not even

247
00:12:15,389 --> 00:12:20,970
bother let's not cripple features for

248
00:12:17,579 --> 00:12:23,040
the sake of privacy but instead after

249
00:12:20,970 --> 00:12:23,639
the study what I concluded was quite the

250
00:12:23,040 --> 00:12:25,410
opposite

251
00:12:23,639 --> 00:12:27,899
so I think this is a much more general

252
00:12:25,410 --> 00:12:29,579
principle in a lot of contexts we hear

253
00:12:27,899 --> 00:12:31,620
the ship has sailed the horses left the

254
00:12:29,579 --> 00:12:35,099
bar in kind of argument and if you were

255
00:12:31,620 --> 00:12:37,459
at Pete Snyder's talk at PRG earlier

256
00:12:35,100 --> 00:12:40,949
today you heard a lot of similar points

257
00:12:37,459 --> 00:12:43,579
from him as well you know the the same

258
00:12:40,949 --> 00:12:48,510
kind of thing that that I'm saying here

259
00:12:43,579 --> 00:12:50,638
so that's the first kind of insight that

260
00:12:48,510 --> 00:12:53,790
I want to that I want to give you the

261
00:12:50,639 --> 00:12:55,139
ship has not sailed and one of the

262
00:12:53,790 --> 00:12:57,000
reasons that people will say that the

263
00:12:55,139 --> 00:12:58,620
ship has sailed is that if you don't

264
00:12:57,000 --> 00:13:00,389
have a perfect defense even if you try

265
00:12:58,620 --> 00:13:01,829
to mitigate fingerprint ability oh

266
00:13:00,389 --> 00:13:03,569
here's a clever way that somebody can

267
00:13:01,829 --> 00:13:07,079
get around that well I have an imperfect

268
00:13:03,569 --> 00:13:10,319
defense at all is it is it not better to

269
00:13:07,079 --> 00:13:13,709
you know to not give people a false

270
00:13:10,319 --> 00:13:15,389
sense of security so that is a point on

271
00:13:13,709 --> 00:13:17,250
which I will disagree I think that

272
00:13:15,389 --> 00:13:19,560
imperfect defenses are still very useful

273
00:13:17,250 --> 00:13:21,959
and one reason I believe that is because

274
00:13:19,560 --> 00:13:24,119
technology doesn't have to bear the full

275
00:13:21,959 --> 00:13:25,709
burden of privacy protection what do I

276
00:13:24,120 --> 00:13:28,620
mean by this here's an interesting

277
00:13:25,709 --> 00:13:30,930
example Safari has third-party cookie

278
00:13:28,620 --> 00:13:32,430
blocking as you might know and it's not

279
00:13:30,930 --> 00:13:34,589
a perfect defense it can be circumvented

280
00:13:32,430 --> 00:13:36,479
in fact Google decided to do exactly

281
00:13:34,589 --> 00:13:37,800
that Google decided to circumvent it and

282
00:13:36,480 --> 00:13:40,050
once they did something interesting

283
00:13:37,800 --> 00:13:41,699
happened in the US the Federal Trade

284
00:13:40,050 --> 00:13:43,829
Commission got involved they said hey

285
00:13:41,699 --> 00:13:45,719
you can't do that you can't circumvent a

286
00:13:43,829 --> 00:13:47,310
privacy measure that's actually a

287
00:13:45,720 --> 00:13:50,040
violation of the law and they went after

288
00:13:47,310 --> 00:13:51,268
Google and they find Google so that's an

289
00:13:50,040 --> 00:13:52,889
interesting phenomenon where the

290
00:13:51,269 --> 00:13:55,350
technology itself was not bulletproof

291
00:13:52,889 --> 00:13:57,480
but it turns out that circumventing even

292
00:13:55,350 --> 00:13:59,160
a weak privacy protection measure can

293
00:13:57,480 --> 00:14:01,050
actually get companies into trouble with

294
00:13:59,160 --> 00:14:02,819
the law it can also be a reputational

295
00:14:01,050 --> 00:14:04,920
harm so when we're talking about the

296
00:14:02,819 --> 00:14:06,569
privacy adversaries here we're talking

297
00:14:04,920 --> 00:14:07,059
about the Facebook's and googles of the

298
00:14:06,569 --> 00:14:09,368
world where

299
00:14:07,059 --> 00:14:11,618
talking about somebody you know from a

300
00:14:09,369 --> 00:14:12,999
poorly regulated jurisdiction somewhere

301
00:14:11,619 --> 00:14:14,859
out there in the world and therefore

302
00:14:12,999 --> 00:14:16,899
technology doesn't have to bear the full

303
00:14:14,859 --> 00:14:19,659
burden and perfect defenses can still be

304
00:14:16,899 --> 00:14:21,429
useful even if all that it does is raise

305
00:14:19,659 --> 00:14:23,139
the cost of some of these fingerprinting

306
00:14:21,429 --> 00:14:26,439
and privacy invasive features and it

307
00:14:23,139 --> 00:14:28,119
takes a couple more years for those kind

308
00:14:26,439 --> 00:14:30,009
of tracking technologies to become very

309
00:14:28,119 --> 00:14:31,689
widespread that is still useful because

310
00:14:30,009 --> 00:14:33,399
it gives a couple of years for new

311
00:14:31,689 --> 00:14:35,139
defenses to be developed whether they

312
00:14:33,399 --> 00:14:35,489
may be technical or legal or something

313
00:14:35,139 --> 00:14:37,749
like that

314
00:14:35,489 --> 00:14:41,099
so that was point number one point

315
00:14:37,749 --> 00:14:43,959
number two that I want to talk about is

316
00:14:41,099 --> 00:14:45,909
we're in a world where what privacy

317
00:14:43,959 --> 00:14:47,649
means to people changes very quickly

318
00:14:45,909 --> 00:14:49,599
whether or not something is a privacy

319
00:14:47,649 --> 00:14:51,729
breach changes very quickly so both

320
00:14:49,599 --> 00:14:53,559
privacy attitudes and privacy infringing

321
00:14:51,729 --> 00:14:55,269
technologies change pretty quickly

322
00:14:53,559 --> 00:14:56,829
how can standards cope in this world

323
00:14:55,269 --> 00:15:00,879
given that standards are intended to be

324
00:14:56,829 --> 00:15:02,589
pretty long-lasting documents and so how

325
00:15:00,879 --> 00:15:07,029
do you resolve the tension between these

326
00:15:02,589 --> 00:15:08,949
two so one good example of this is one

327
00:15:07,029 --> 00:15:11,139
of my favorite examples of how privacy

328
00:15:08,949 --> 00:15:13,839
attitudes evolved quickly is that if you

329
00:15:11,139 --> 00:15:15,519
thought about privacy 10 years ago most

330
00:15:13,839 --> 00:15:17,769
users would have been concerned with

331
00:15:15,519 --> 00:15:19,869
what are the individual harms that can

332
00:15:17,769 --> 00:15:22,569
accrue to me out of all of those data

333
00:15:19,869 --> 00:15:24,939
collection out of all of the databases

334
00:15:22,569 --> 00:15:26,919
owned by companies that have my personal

335
00:15:24,939 --> 00:15:29,228
information is it identity theft is the

336
00:15:26,919 --> 00:15:30,968
data breaches is it perhaps targeted

337
00:15:29,229 --> 00:15:32,289
price discrimination what should I be

338
00:15:30,969 --> 00:15:33,879
worried about those were the kinds of

339
00:15:32,289 --> 00:15:35,589
privacy questions that people were

340
00:15:33,879 --> 00:15:38,289
asking people are still worried about

341
00:15:35,589 --> 00:15:39,789
those privacy issues but now people are

342
00:15:38,289 --> 00:15:41,949
increasingly worried about a very

343
00:15:39,789 --> 00:15:43,779
different kind of privacy issue which is

344
00:15:41,949 --> 00:15:46,269
what are the threats to society overall

345
00:15:43,779 --> 00:15:47,829
and perhaps to democracy from all of

346
00:15:46,269 --> 00:15:51,629
these massive collections of personal

347
00:15:47,829 --> 00:15:54,549
information especially after a recent

348
00:15:51,629 --> 00:15:56,619
recent stories like Cambridge analytic

349
00:15:54,549 --> 00:15:58,119
apeople are very concerned about what is

350
00:15:56,619 --> 00:16:01,029
the potential of hyper personalized

351
00:15:58,119 --> 00:16:03,699
targeting to affect you know the overall

352
00:16:01,029 --> 00:16:05,559
society that we live in so I want to say

353
00:16:03,699 --> 00:16:07,299
that there has been a shift from these

354
00:16:05,559 --> 00:16:09,459
very individualized concerns about

355
00:16:07,299 --> 00:16:11,739
privacy to more collective societal

356
00:16:09,459 --> 00:16:13,748
concerns about privacy among privacy

357
00:16:11,739 --> 00:16:15,459
scholars and privacy advocates that

358
00:16:13,749 --> 00:16:16,869
shift has been pretty stark and even

359
00:16:15,459 --> 00:16:18,909
among the general public I think there

360
00:16:16,869 --> 00:16:20,259
has been a substantial shift and so what

361
00:16:18,909 --> 00:16:21,270
this means is that a certain type of

362
00:16:20,259 --> 00:16:22,800
data collide

363
00:16:21,270 --> 00:16:25,290
that might have seen pretty innocuous

364
00:16:22,800 --> 00:16:27,270
ten years ago begins to look very

365
00:16:25,290 --> 00:16:29,490
different today so that was one example

366
00:16:27,270 --> 00:16:31,770
I have a couple of other examples that

367
00:16:29,490 --> 00:16:33,900
I'll skip but the result of this is that

368
00:16:31,770 --> 00:16:35,910
it's very hard in a standard document to

369
00:16:33,900 --> 00:16:37,560
write down and fixed privacy definition

370
00:16:35,910 --> 00:16:39,150
and then say that I've analyzed this

371
00:16:37,560 --> 00:16:41,010
protocol with respect to this privacy

372
00:16:39,150 --> 00:16:42,990
definition and I'm confident that this

373
00:16:41,010 --> 00:16:45,410
is going to be a privacy respecting

374
00:16:42,990 --> 00:16:47,730
protocol now and for all time to come

375
00:16:45,410 --> 00:16:49,680
and so going back to that example of

376
00:16:47,730 --> 00:16:51,810
individual versus collective harms let

377
00:16:49,680 --> 00:16:54,000
me show you very quickly that paper by

378
00:16:51,810 --> 00:16:55,859
Cambridge researchers this was in 2013

379
00:16:54,000 --> 00:16:57,450
this was the paper that realised that

380
00:16:55,860 --> 00:16:59,640
you could take people's Facebook Likes

381
00:16:57,450 --> 00:17:01,860
which is a very innocuous sounding type

382
00:16:59,640 --> 00:17:03,390
of information and use that to predict

383
00:17:01,860 --> 00:17:05,660
their so-called Big Five personality

384
00:17:03,390 --> 00:17:08,160
traits and those are things like

385
00:17:05,660 --> 00:17:09,870
emotional stability agreeableness

386
00:17:08,160 --> 00:17:11,370
extraversion and so on the stuff that

387
00:17:09,869 --> 00:17:12,719
you see in green over there if you can

388
00:17:11,369 --> 00:17:14,790
even greet that text sorry about that

389
00:17:12,720 --> 00:17:16,350
the font size is a little small and this

390
00:17:14,790 --> 00:17:18,209
is exactly the research that was

391
00:17:16,349 --> 00:17:21,419
allegedly weaponized by Cambridge

392
00:17:18,209 --> 00:17:23,220
analytic ofor psychographic targeting so

393
00:17:21,420 --> 00:17:24,959
this was not necessarily anticipated a

394
00:17:23,220 --> 00:17:26,670
few years ago there are many other

395
00:17:24,959 --> 00:17:29,250
examples of this of improvements in

396
00:17:26,670 --> 00:17:31,200
machine learning training innocuous data

397
00:17:29,250 --> 00:17:34,110
into something that can be used for

398
00:17:31,200 --> 00:17:36,230
something much more problematic this was

399
00:17:34,110 --> 00:17:37,949
a headline from a few years ago

400
00:17:36,230 --> 00:17:39,450
statisticians at Target

401
00:17:37,950 --> 00:17:41,220
had figured out how to use a person's

402
00:17:39,450 --> 00:17:44,370
shopping records to figure out whether

403
00:17:41,220 --> 00:17:46,940
they were pregnant or not and so a one

404
00:17:44,370 --> 00:17:49,649
concrete threat along these lines is

405
00:17:46,940 --> 00:17:51,660
well stated by Paul ohm who's a legal

406
00:17:49,650 --> 00:17:53,430
scholar who calls this the database of

407
00:17:51,660 --> 00:17:55,200
Rouen he asks us to imagine the

408
00:17:53,430 --> 00:17:57,270
consequences of a single massive

409
00:17:55,200 --> 00:17:59,640
database containing secrets about every

410
00:17:57,270 --> 00:18:01,980
individual formed by linking different

411
00:17:59,640 --> 00:18:03,690
companies data stores and I think one of

412
00:18:01,980 --> 00:18:04,770
the technologies that is enabling

413
00:18:03,690 --> 00:18:07,050
something like this today

414
00:18:04,770 --> 00:18:08,879
is cross device tracking techniques that

415
00:18:07,050 --> 00:18:10,980
enable the linking of our activities

416
00:18:08,880 --> 00:18:13,170
between different devices even if we're

417
00:18:10,980 --> 00:18:15,630
not identifying ourselves using explicit

418
00:18:13,170 --> 00:18:17,520
identifiers that allow such linkage just

419
00:18:15,630 --> 00:18:19,350
using statistical patterns to link these

420
00:18:17,520 --> 00:18:21,090
different devices together and I think

421
00:18:19,350 --> 00:18:23,280
these types of concerns should perhaps

422
00:18:21,090 --> 00:18:26,580
be at the forefront of some of our

423
00:18:23,280 --> 00:18:28,200
privacy efforts including in standards

424
00:18:26,580 --> 00:18:30,270
efforts but these are not things that we

425
00:18:28,200 --> 00:18:32,850
really recognized as privacy concerns

426
00:18:30,270 --> 00:18:34,639
maybe ten years ago as much as we do

427
00:18:32,850 --> 00:18:37,189
today so that's kind of what

428
00:18:34,640 --> 00:18:38,840
by the landscape of privacy is shifting

429
00:18:37,190 --> 00:18:40,130
pretty quickly and this is a challenge

430
00:18:38,840 --> 00:18:41,929
for our standards document in a

431
00:18:40,130 --> 00:18:44,060
standards process which needs to be

432
00:18:41,930 --> 00:18:46,250
really long-lived so we thought about

433
00:18:44,060 --> 00:18:49,090
this in a paper recently where we looked

434
00:18:46,250 --> 00:18:52,160
at specifically the battery status API

435
00:18:49,090 --> 00:18:54,530
in the web context and this was an API

436
00:18:52,160 --> 00:18:56,750
that turned out to have much more

437
00:18:54,530 --> 00:18:58,399
serious fingerprint ability privacy

438
00:18:56,750 --> 00:19:00,590
consequences then was realized and

439
00:18:58,400 --> 00:19:02,450
therefore was taken out of a number of

440
00:19:00,590 --> 00:19:04,189
browsers after it had shipped and after

441
00:19:02,450 --> 00:19:06,080
people had started using it that was

442
00:19:04,190 --> 00:19:08,150
kind of unprecedented so we looked at

443
00:19:06,080 --> 00:19:09,949
how did this go wrong and how can we be

444
00:19:08,150 --> 00:19:12,770
more aware of these potential and

445
00:19:09,950 --> 00:19:15,790
misuses during the standards process and

446
00:19:12,770 --> 00:19:18,530
so here's a paper citation at the bottom

447
00:19:15,790 --> 00:19:21,560
and what we proposed in this paper at a

448
00:19:18,530 --> 00:19:23,899
high level what we called for is a much

449
00:19:21,560 --> 00:19:26,030
tighter loop between standards agencies

450
00:19:23,900 --> 00:19:27,920
as well as researchers and developers

451
00:19:26,030 --> 00:19:29,660
and by developers I mean both

452
00:19:27,920 --> 00:19:31,250
implementers and also developers in a

453
00:19:29,660 --> 00:19:33,830
much more general sense people who are

454
00:19:31,250 --> 00:19:36,920
using the api's that are you know

455
00:19:33,830 --> 00:19:39,530
implemented by by the browser vendors

456
00:19:36,920 --> 00:19:42,170
for example and as part of this we think

457
00:19:39,530 --> 00:19:44,480
that it would be really useful to

458
00:19:42,170 --> 00:19:46,340
incentivize academics to do two things

459
00:19:44,480 --> 00:19:47,990
one is to get involved in the standards

460
00:19:46,340 --> 00:19:49,840
process and do privacy reviews of

461
00:19:47,990 --> 00:19:53,420
standards and the other one this is

462
00:19:49,840 --> 00:19:55,669
perhaps still quite missing which is

463
00:19:53,420 --> 00:19:57,650
once an API is out in the wild and once

464
00:19:55,670 --> 00:19:59,870
people are using it to do regular

465
00:19:57,650 --> 00:20:01,940
privacy audits of how it's being used

466
00:19:59,870 --> 00:20:03,590
and abused I've talked about this a few

467
00:20:01,940 --> 00:20:06,530
times and one question that I get is

468
00:20:03,590 --> 00:20:08,540
sure this sounds good in theory but it's

469
00:20:06,530 --> 00:20:11,120
hard to convince researchers to do this

470
00:20:08,540 --> 00:20:12,950
how do we do that now one good thing

471
00:20:11,120 --> 00:20:14,360
I'll say about this this actually sounds

472
00:20:12,950 --> 00:20:16,070
like a horrible thing but I'll claim

473
00:20:14,360 --> 00:20:18,580
it's a good thing is that it's fairly

474
00:20:16,070 --> 00:20:21,200
easy to influence academic researchers

475
00:20:18,580 --> 00:20:22,820
influenced influenced them not in the

476
00:20:21,200 --> 00:20:24,110
sense of what they'll say but it

477
00:20:22,820 --> 00:20:26,210
influenced them in the sense of what

478
00:20:24,110 --> 00:20:28,040
they want to work on by funding certain

479
00:20:26,210 --> 00:20:30,620
work or by making it more prestigious by

480
00:20:28,040 --> 00:20:32,420
creating awards for example for certain

481
00:20:30,620 --> 00:20:35,959
types of work such as privacy reviews of

482
00:20:32,420 --> 00:20:38,830
standards I think it's there's a there's

483
00:20:35,960 --> 00:20:41,180
a fairly straightforward path to

484
00:20:38,830 --> 00:20:42,980
incentivizing much more academic work as

485
00:20:41,180 --> 00:20:45,170
part of the standards process which I

486
00:20:42,980 --> 00:20:47,330
think will be a good thing another thing

487
00:20:45,170 --> 00:20:47,900
that I think would be useful is as part

488
00:20:47,330 --> 00:20:50,300
of the standard

489
00:20:47,900 --> 00:20:51,890
process to be explicit about assumptions

490
00:20:50,300 --> 00:20:54,649
because privacy changes so quickly

491
00:20:51,890 --> 00:20:56,660
because we can't anticipate what new

492
00:20:54,650 --> 00:20:58,670
privacy infringing technologies will be

493
00:20:56,660 --> 00:21:00,830
out there in five years it helps to be

494
00:20:58,670 --> 00:21:04,070
explicit about assumptions as part of

495
00:21:00,830 --> 00:21:06,800
the standards process and that is to be

496
00:21:04,070 --> 00:21:10,129
able to explicitly say we have created

497
00:21:06,800 --> 00:21:12,139
the standard assuming that this API will

498
00:21:10,130 --> 00:21:13,910
not be highly susceptible to fingerprint

499
00:21:12,140 --> 00:21:16,040
ability but if it turns out that that's

500
00:21:13,910 --> 00:21:18,290
the case if it turns out that this is

501
00:21:16,040 --> 00:21:19,760
being exploited in the wild here are

502
00:21:18,290 --> 00:21:21,740
some things that implementers could do

503
00:21:19,760 --> 00:21:22,690
to mitigate that risk so that's the

504
00:21:21,740 --> 00:21:25,160
second point

505
00:21:22,690 --> 00:21:27,230
okay the third and final point that I

506
00:21:25,160 --> 00:21:29,150
want to talk about is that this idea of

507
00:21:27,230 --> 00:21:31,010
measurement which is finding these

508
00:21:29,150 --> 00:21:32,630
privacy violations on a large scale I'm

509
00:21:31,010 --> 00:21:35,540
claiming that it's been really useful

510
00:21:32,630 --> 00:21:37,460
for privacy but unfortunately it's going

511
00:21:35,540 --> 00:21:39,889
away and I want to talk about whether

512
00:21:37,460 --> 00:21:41,330
there is a way to preserve it I don't

513
00:21:39,890 --> 00:21:43,640
want to make the sound like a sky is

514
00:21:41,330 --> 00:21:45,500
falling kind of claim but in my little

515
00:21:43,640 --> 00:21:47,150
corner of the research world the sky has

516
00:21:45,500 --> 00:21:49,460
already fallen and a lot of us that have

517
00:21:47,150 --> 00:21:50,630
moved on to other research areas so let

518
00:21:49,460 --> 00:21:52,070
me tell you why that is and why that

519
00:21:50,630 --> 00:21:53,900
should worry us from a privacy

520
00:21:52,070 --> 00:21:56,720
perspective and to see whether there's a

521
00:21:53,900 --> 00:21:58,640
way to preserve it so I'm claiming that

522
00:21:56,720 --> 00:22:00,710
at least in the web context measurement

523
00:21:58,640 --> 00:22:02,750
has played a very key role in keeping

524
00:22:00,710 --> 00:22:05,060
the worst of the privacy abuses in check

525
00:22:02,750 --> 00:22:06,830
many teams around the world have been

526
00:22:05,060 --> 00:22:09,020
working on web privacy measurement I'll

527
00:22:06,830 --> 00:22:11,090
tell you a tiny bit about my own team's

528
00:22:09,020 --> 00:22:13,700
work something that we built is a tool

529
00:22:11,090 --> 00:22:15,110
called open wpm this is a the github

530
00:22:13,700 --> 00:22:16,850
page if you want to check it out as you

531
00:22:15,110 --> 00:22:18,530
can see it's an actively developed

532
00:22:16,850 --> 00:22:20,689
open-source project it was developed at

533
00:22:18,530 --> 00:22:22,040
Princeton and now the main developer

534
00:22:20,690 --> 00:22:24,650
Steve Englehart has moved to Mozilla

535
00:22:22,040 --> 00:22:26,120
it's maintained by Mozilla now so what

536
00:22:24,650 --> 00:22:27,560
it is I don't mean for any of the

537
00:22:26,120 --> 00:22:29,929
details on this page to be important

538
00:22:27,560 --> 00:22:33,139
this it's just the URL if you want to

539
00:22:29,930 --> 00:22:34,580
look at it or the name open wpm now what

540
00:22:33,140 --> 00:22:37,220
it is is an instrumented version of

541
00:22:34,580 --> 00:22:39,110
Firefox it's basically a bot that visits

542
00:22:37,220 --> 00:22:42,260
the web's top 1 million websites every

543
00:22:39,110 --> 00:22:45,020
month and looks at what kind of privacy

544
00:22:42,260 --> 00:22:47,300
violation violating techniques are out

545
00:22:45,020 --> 00:22:49,490
there it even does things like put in

546
00:22:47,300 --> 00:22:51,260
fake PII into various forms and tries to

547
00:22:49,490 --> 00:22:53,210
see where they go and it saves all that

548
00:22:51,260 --> 00:22:55,250
data we have half a terabyte of data per

549
00:22:53,210 --> 00:22:57,020
month and then we run various scripts on

550
00:22:55,250 --> 00:22:58,970
that data to try to find privacy

551
00:22:57,020 --> 00:23:00,830
violations and publicize them and get

552
00:22:58,970 --> 00:23:02,029
people to change their practices we've

553
00:23:00,830 --> 00:23:04,039
written a number of papers

554
00:23:02,029 --> 00:23:05,479
based on this data this is one example

555
00:23:04,039 --> 00:23:07,399
it's called online tracking of 1 million

556
00:23:05,479 --> 00:23:09,049
site measurement analysis and as you can

557
00:23:07,399 --> 00:23:11,178
see one of the key things here is to be

558
00:23:09,049 --> 00:23:13,489
able to do this on a large scale in a

559
00:23:11,179 --> 00:23:15,859
mostly automated way it's had a number

560
00:23:13,489 --> 00:23:17,539
of positive impacts on privacy one of

561
00:23:15,859 --> 00:23:19,489
them is enhancing block lists for

562
00:23:17,539 --> 00:23:21,590
example if you use adblock plus or you

563
00:23:19,489 --> 00:23:23,090
block origin those tools use filter

564
00:23:21,590 --> 00:23:25,279
lists and the developers of those filter

565
00:23:23,090 --> 00:23:27,080
lists often look to research like hours

566
00:23:25,279 --> 00:23:29,210
to try to figure out what are some of

567
00:23:27,080 --> 00:23:31,399
the new privacy violating endpoints in

568
00:23:29,210 --> 00:23:34,070
the URLs in order to add them to their

569
00:23:31,399 --> 00:23:36,199
block lists various other things for

570
00:23:34,070 --> 00:23:37,999
example in some cases there's there's

571
00:23:36,200 --> 00:23:39,469
been an enforcement action by data

572
00:23:37,999 --> 00:23:41,869
protection authorities by the federal

573
00:23:39,469 --> 00:23:43,820
agency and things like that so I'm

574
00:23:41,869 --> 00:23:45,468
claiming that the the this kind of

575
00:23:43,820 --> 00:23:47,359
research that's been done by many groups

576
00:23:45,469 --> 00:23:50,269
around the world is one of the main

577
00:23:47,359 --> 00:23:52,158
reasons why web privacy has been you

578
00:23:50,269 --> 00:23:55,599
know kind of at an equilibrium hasn't

579
00:23:52,159 --> 00:23:58,669
been even worse than it already is now

580
00:23:55,599 --> 00:24:01,369
the important point though is that five

581
00:23:58,669 --> 00:24:03,289
years ago a lot of fuss that we were

582
00:24:01,369 --> 00:24:05,238
going to do this very same kind of work

583
00:24:03,289 --> 00:24:08,259
for IOT because we were hearing that a

584
00:24:05,239 --> 00:24:10,279
lot of the IOT devices in our homes have

585
00:24:08,259 --> 00:24:12,200
occasionally have surreptitious data

586
00:24:10,279 --> 00:24:14,059
collection that consumers did not know

587
00:24:12,200 --> 00:24:16,009
about and so we want to do this kind of

588
00:24:14,059 --> 00:24:18,019
work but we very quickly realized that

589
00:24:16,009 --> 00:24:20,210
we can't actually do this work because

590
00:24:18,019 --> 00:24:22,099
of one very simple reason which is that

591
00:24:20,210 --> 00:24:24,559
most devices are end-to-end encrypted

592
00:24:22,099 --> 00:24:26,239
which to be very clear is a great thing

593
00:24:24,559 --> 00:24:27,099
it's great for privacy they should be

594
00:24:26,239 --> 00:24:29,239
into an encrypted

595
00:24:27,099 --> 00:24:31,639
unfortunately the downside of that is

596
00:24:29,239 --> 00:24:33,200
that the two ends of course off into an

597
00:24:31,639 --> 00:24:35,299
encryption are the device in the server

598
00:24:33,200 --> 00:24:37,639
it doesn't involve the user it doesn't

599
00:24:35,299 --> 00:24:39,679
involve a researcher a researcher can

600
00:24:37,639 --> 00:24:41,029
Smitham these devices a researcher can't

601
00:24:39,679 --> 00:24:44,119
figure out what data is being collected

602
00:24:41,029 --> 00:24:46,279
and where it's being sent and we think

603
00:24:44,119 --> 00:24:47,718
this is you know kind of a crisis for

604
00:24:46,279 --> 00:24:49,159
this kind of research it makes

605
00:24:47,719 --> 00:24:52,609
meaningful privacy measurements

606
00:24:49,159 --> 00:24:54,139
basically infeasible the public is very

607
00:24:52,609 --> 00:24:55,609
interested in these questions for

608
00:24:54,139 --> 00:24:57,859
example there was this article called

609
00:24:55,609 --> 00:24:59,239
the house that spied on me that just

610
00:24:57,859 --> 00:25:01,789
looked at what are the endpoints of

611
00:24:59,239 --> 00:25:03,379
communication of various IOT devices

612
00:25:01,789 --> 00:25:05,210
including sex toys why is that

613
00:25:03,379 --> 00:25:07,309
contacting 13 different servers you know

614
00:25:05,210 --> 00:25:09,679
people want to know what data is going

615
00:25:07,309 --> 00:25:12,019
out there and this is important not just

616
00:25:09,679 --> 00:25:14,029
from a privacy advocate point of view if

617
00:25:12,019 --> 00:25:15,180
you're a company and you're a reputable

618
00:25:14,029 --> 00:25:16,920
company and you want to

619
00:25:15,180 --> 00:25:19,350
able to show your users that your data

620
00:25:16,920 --> 00:25:21,540
collection is completely you know

621
00:25:19,350 --> 00:25:23,459
according to your specified privacy

622
00:25:21,540 --> 00:25:25,770
policies there's no good way to do that

623
00:25:23,460 --> 00:25:27,870
today because researchers can't examine

624
00:25:25,770 --> 00:25:29,940
the plaintext of these communications

625
00:25:27,870 --> 00:25:32,159
and I think this is a serious issue for

626
00:25:29,940 --> 00:25:33,810
example if we wanted to know if the

627
00:25:32,160 --> 00:25:35,790
smart light bulbs in our homes are

628
00:25:33,810 --> 00:25:37,830
transmitting conversations because they

629
00:25:35,790 --> 00:25:39,180
actually have microphones we really

630
00:25:37,830 --> 00:25:41,669
don't have a good way to check that

631
00:25:39,180 --> 00:25:43,590
today and this is not a totally paranoid

632
00:25:41,670 --> 00:25:45,840
scenario something somewhat similar has

633
00:25:43,590 --> 00:25:47,909
happened for example this interesting

634
00:25:45,840 --> 00:25:50,520
thing happened a few months ago where

635
00:25:47,910 --> 00:25:52,500
Google sent an email to all of the

636
00:25:50,520 --> 00:25:54,990
owners of nest thermostats and said hey

637
00:25:52,500 --> 00:25:57,780
your nest thermostat is also a Google

638
00:25:54,990 --> 00:25:59,250
Voice assistant now and people like what

639
00:25:57,780 --> 00:26:01,200
how is that possible it doesn't have a

640
00:25:59,250 --> 00:26:02,970
microphone and Google said no it does

641
00:26:01,200 --> 00:26:04,230
have a microphone and if people said

642
00:26:02,970 --> 00:26:06,840
what we didn't know it had a microphone

643
00:26:04,230 --> 00:26:09,540
and Google said yes it does check the

644
00:26:06,840 --> 00:26:11,250
privacy policy and people said what

645
00:26:09,540 --> 00:26:12,870
privacy policy nobody reads the privacy

646
00:26:11,250 --> 00:26:14,670
policy also when they read the privacy

647
00:26:12,870 --> 00:26:16,679
policy it was actually not in there and

648
00:26:14,670 --> 00:26:18,360
then Google said oh we meant to disclose

649
00:26:16,680 --> 00:26:22,050
that in the privacy policy sorry that

650
00:26:18,360 --> 00:26:23,490
was an oversight so of course in this

651
00:26:22,050 --> 00:26:25,470
case I'm willing to believe that it was

652
00:26:23,490 --> 00:26:27,690
an oversight on the part of Google but

653
00:26:25,470 --> 00:26:29,970
if there was a malicious you know vendor

654
00:26:27,690 --> 00:26:32,100
who put microphones and devices that are

655
00:26:29,970 --> 00:26:33,810
in millions of people homes we literally

656
00:26:32,100 --> 00:26:35,610
don't have a good way to know about it

657
00:26:33,810 --> 00:26:37,470
this measurement research has been in

658
00:26:35,610 --> 00:26:39,780
the past one way to know about it but it

659
00:26:37,470 --> 00:26:41,880
doesn't work for IMT so with that I'll

660
00:26:39,780 --> 00:26:44,370
just end by saying that what it likes a

661
00:26:41,880 --> 00:26:46,290
call for is some kind of debug mode for

662
00:26:44,370 --> 00:26:48,290
IOT devices I think this is a critical

663
00:26:46,290 --> 00:26:51,750
need the idea being that when you enable

664
00:26:48,290 --> 00:26:54,420
this kind of debug mode the user or more

665
00:26:51,750 --> 00:26:55,890
likely a researcher you know the details

666
00:26:54,420 --> 00:26:57,810
and user experience will depend on the

667
00:26:55,890 --> 00:26:59,910
device but some way to be able to

668
00:26:57,810 --> 00:27:01,860
intercept the plane tax in order to be

669
00:26:59,910 --> 00:27:04,170
able to audit what's going on out there

670
00:27:01,860 --> 00:27:08,399
there's a Stanford project related to

671
00:27:04,170 --> 00:27:12,360
this called TLS TLS what does it called

672
00:27:08,400 --> 00:27:14,250
TLS replays something and so what I'm

673
00:27:12,360 --> 00:27:16,320
proposing is slightly different I'm

674
00:27:14,250 --> 00:27:18,420
happy to hash out the details later this

675
00:27:16,320 --> 00:27:20,760
is not necessarily the time for that but

676
00:27:18,420 --> 00:27:22,650
I think some way of being able to

677
00:27:20,760 --> 00:27:24,780
examine the communications of IOT

678
00:27:22,650 --> 00:27:27,210
devices is critical and I think there's

679
00:27:24,780 --> 00:27:28,879
a role for Standardization here with

680
00:27:27,210 --> 00:27:47,330
that I'll just put the summary back up

681
00:27:28,880 --> 00:27:48,890
thank you for your time thanks okay so

682
00:27:47,330 --> 00:27:51,760
I'm going to talk about some modern

683
00:27:48,890 --> 00:27:54,980
issues in privacy today and you know

684
00:27:51,760 --> 00:27:56,720
privacy is not a new issue when I

685
00:27:54,980 --> 00:27:58,310
started doing the research that led to

686
00:27:56,720 --> 00:28:01,100
this talk and by the way these slides

687
00:27:58,310 --> 00:28:04,040
were already on my webpage is the

688
00:28:01,100 --> 00:28:06,980
reference link to references a technical

689
00:28:04,040 --> 00:28:10,970
class legal document is also on my

690
00:28:06,980 --> 00:28:14,720
webpage a lot of this stuff goes back to

691
00:28:10,970 --> 00:28:17,030
the 1960s you know the New York City Bar

692
00:28:14,720 --> 00:28:20,990
Association started studying computers

693
00:28:17,030 --> 00:28:22,430
in privacy in 1962 Alan Weston prepared

694
00:28:20,990 --> 00:28:26,180
basically a report of that committee in

695
00:28:22,430 --> 00:28:28,400
67 been very influential the US Congress

696
00:28:26,180 --> 00:28:31,430
held hearings on this legal academics

697
00:28:28,400 --> 00:28:34,370
for writing papers on this all the 1960s

698
00:28:31,430 --> 00:28:35,930
it actually goes back the right to

699
00:28:34,370 --> 00:28:37,820
privacy is mentioned in Jewish

700
00:28:35,930 --> 00:28:41,810
literature about eighteen hundred years

701
00:28:37,820 --> 00:28:44,510
ago it's not a new issue and the privacy

702
00:28:41,810 --> 00:28:48,050
that we work used today the privacy

703
00:28:44,510 --> 00:28:52,280
paradigm called notice and consent goes

704
00:28:48,050 --> 00:28:54,860
back to Westen's 1967 book which is the

705
00:28:52,280 --> 00:28:57,620
report of this Bar Association the city

706
00:28:54,860 --> 00:29:00,370
of New York Committee that users

707
00:28:57,620 --> 00:29:04,040
individuals can determine for themselves

708
00:29:00,370 --> 00:29:08,199
what they want to share and what they're

709
00:29:04,040 --> 00:29:11,240
willing to reveal in this statement from

710
00:29:08,200 --> 00:29:16,030
1967 has been the basis for virtually

711
00:29:11,240 --> 00:29:18,890
all privacy regulation since then and

712
00:29:16,030 --> 00:29:22,760
you look at the timeline he published

713
00:29:18,890 --> 00:29:25,670
this book in 67 six years later a US

714
00:29:22,760 --> 00:29:26,870
government committee came up with what

715
00:29:25,670 --> 00:29:29,750
became known as the Fair Information

716
00:29:26,870 --> 00:29:32,679
practice principles of consent of

717
00:29:29,750 --> 00:29:36,950
security of openness of use

718
00:29:32,680 --> 00:29:39,110
specification and so on in 1974 a year

719
00:29:36,950 --> 00:29:41,390
later the US government actually enacted

720
00:29:39,110 --> 00:29:42,689
this into law but only is applied to the

721
00:29:41,390 --> 00:29:45,180
US government

722
00:29:42,690 --> 00:29:48,990
didn't apply to private corporations not

723
00:29:45,180 --> 00:29:52,470
the American Way a few years later the

724
00:29:48,990 --> 00:29:54,290
OECD suggested more or less the same

725
00:29:52,470 --> 00:29:58,890
thing but applying to the private sector

726
00:29:54,290 --> 00:30:03,240
in 94 the EU and active data privacy

727
00:29:58,890 --> 00:30:05,220
director seven years ago the gdpr was

728
00:30:03,240 --> 00:30:09,480
enacted when it to affect a couple of

729
00:30:05,220 --> 00:30:11,460
years ago but from 10,000 meters all of

730
00:30:09,480 --> 00:30:14,030
these are substantially as India

731
00:30:11,460 --> 00:30:17,760
tremendous difference in details but

732
00:30:14,030 --> 00:30:21,240
fundamentally if you consent the data

733
00:30:17,760 --> 00:30:25,050
that you have data about you can and

734
00:30:21,240 --> 00:30:28,950
will be collected and notice and consent

735
00:30:25,050 --> 00:30:31,560
and so notice and consent is sites tell

736
00:30:28,950 --> 00:30:34,290
you what they're going to collect and

737
00:30:31,560 --> 00:30:36,710
what they're going to do with it and by

738
00:30:34,290 --> 00:30:40,500
using the website by using the device

739
00:30:36,710 --> 00:30:46,290
you are deemed to have consented to this

740
00:30:40,500 --> 00:30:51,300
policy and some of the risks were known

741
00:30:46,290 --> 00:30:53,340
back in the 1960s academics law

742
00:30:51,300 --> 00:30:54,870
professors wrote people are just going

743
00:30:53,340 --> 00:30:57,899
to go along with the request because

744
00:30:54,870 --> 00:31:00,120
they want the service 1960s we didn't

745
00:30:57,900 --> 00:31:02,670
have Google we didn't have Facebook they

746
00:31:00,120 --> 00:31:06,209
realized people are going to go along to

747
00:31:02,670 --> 00:31:07,680
get the benefits they realized they told

748
00:31:06,210 --> 00:31:09,120
the US Congress people are going to

749
00:31:07,680 --> 00:31:12,960
share passwords maybe we need

750
00:31:09,120 --> 00:31:15,179
multi-factor authentication 1967 folks

751
00:31:12,960 --> 00:31:18,060
how many sites you log in to adjust a

752
00:31:15,180 --> 00:31:20,580
password today they worried about

753
00:31:18,060 --> 00:31:22,879
hackers they even cited MIT the MIT

754
00:31:20,580 --> 00:31:26,159
students breaking into systems for fun

755
00:31:22,880 --> 00:31:30,240
insider threats why are tapping the need

756
00:31:26,160 --> 00:31:32,340
for encryption the importance of

757
00:31:30,240 --> 00:31:37,380
metadata and the inferences you can draw

758
00:31:32,340 --> 00:31:40,590
from metadata in again 1967-1969 the

759
00:31:37,380 --> 00:31:44,280
danger of large searchable aggregate

760
00:31:40,590 --> 00:31:49,949
able databases all of this was known and

761
00:31:44,280 --> 00:31:51,840
largely forgotten so we haven't solved

762
00:31:49,950 --> 00:31:54,300
the technical problems of north for more

763
00:31:51,840 --> 00:31:56,419
than 50 years ago we still have noticed

764
00:31:54,300 --> 00:32:01,040
in consent though does it work

765
00:31:56,420 --> 00:32:02,900
nope not even close to working there's a

766
00:32:01,040 --> 00:32:05,990
tremendous amount of data is collected

767
00:32:02,900 --> 00:32:07,790
and we don't know who is collecting it

768
00:32:05,990 --> 00:32:10,430
we have privacy policies we have

769
00:32:07,790 --> 00:32:13,300
location data and of course there are

770
00:32:10,430 --> 00:32:15,680
the governments of the world

771
00:32:13,300 --> 00:32:18,169
there's a tremendous amount of over

772
00:32:15,680 --> 00:32:20,540
collection apart from all the folks whom

773
00:32:18,170 --> 00:32:24,260
you give consent there are the data

774
00:32:20,540 --> 00:32:28,550
brokers outside parties who business is

775
00:32:24,260 --> 00:32:31,190
to collect data about people and sell it

776
00:32:28,550 --> 00:32:33,470
they collect it they buy it and they

777
00:32:31,190 --> 00:32:36,710
sell it sometimes from public records

778
00:32:33,470 --> 00:32:39,650
sometimes from private transactions that

779
00:32:36,710 --> 00:32:41,960
you know nothing about last year I sold

780
00:32:39,650 --> 00:32:45,160
the car I found out that my odometer

781
00:32:41,960 --> 00:32:50,090
readings had been sold by my mechanic

782
00:32:45,160 --> 00:32:52,010
well yeah did I could send to it no that

783
00:32:50,090 --> 00:32:55,850
was a private deal between the mechanic

784
00:32:52,010 --> 00:32:57,590
and some data collection company the ads

785
00:32:55,850 --> 00:32:59,300
that you see on the web they're not

786
00:32:57,590 --> 00:33:00,740
generally not coming from the website

787
00:32:59,300 --> 00:33:03,500
you're visiting they're coming from ad

788
00:33:00,740 --> 00:33:07,100
brokers often multiple levels of ad

789
00:33:03,500 --> 00:33:08,840
brokers who do HTTP redirects each

790
00:33:07,100 --> 00:33:12,409
warrant is a separate website can

791
00:33:08,840 --> 00:33:14,750
collect and set cookies so lots of folks

792
00:33:12,410 --> 00:33:17,150
are gathering data about you and you

793
00:33:14,750 --> 00:33:19,850
don't even know who they are and the

794
00:33:17,150 --> 00:33:21,830
third party like buttons like Facebook

795
00:33:19,850 --> 00:33:24,740
and Twitter and the third party

796
00:33:21,830 --> 00:33:28,129
authentication Facebook and Google tells

797
00:33:24,740 --> 00:33:32,630
these collection sites what sites on the

798
00:33:28,130 --> 00:33:35,870
web you're visiting these analytic

799
00:33:32,630 --> 00:33:39,230
platforms are used to build up profiles

800
00:33:35,870 --> 00:33:41,179
on people and their companies in calling

801
00:33:39,230 --> 00:33:46,280
out rubicon simply because they're cited

802
00:33:41,180 --> 00:33:48,260
in New York Times article they take what

803
00:33:46,280 --> 00:33:50,870
they know about you from the tracking

804
00:33:48,260 --> 00:33:52,790
cookies they combine that with

805
00:33:50,870 --> 00:33:56,209
information from third-party data

806
00:33:52,790 --> 00:33:58,399
aggregators and estimate based your age

807
00:33:56,210 --> 00:34:00,800
your gender your income and use that to

808
00:33:58,400 --> 00:34:02,960
say how valuable a customer are you and

809
00:34:00,800 --> 00:34:08,330
therefore what ad is appropriate to show

810
00:34:02,960 --> 00:34:10,010
you and you don't see any of this but we

811
00:34:08,330 --> 00:34:12,859
have privacy policies

812
00:34:10,010 --> 00:34:15,710
nope curiosity who in this room reads

813
00:34:12,859 --> 00:34:18,668
every privacy policy they encounter I'm

814
00:34:15,710 --> 00:34:26,090
impressed I am seriously impressed

815
00:34:18,668 --> 00:34:27,679
security these hands down there are lori

816
00:34:26,090 --> 00:34:29,510
crater and her colleagues at the

817
00:34:27,679 --> 00:34:31,580
carnegie mellon estimated the

818
00:34:29,510 --> 00:34:34,129
opportunity cost for reading all the

819
00:34:31,580 --> 00:34:35,989
privacy policies you encounter would be

820
00:34:34,129 --> 00:34:39,109
about thirty five hundred US dollars per

821
00:34:35,989 --> 00:34:42,199
year and they're deliberately vague

822
00:34:39,109 --> 00:34:44,540
deliberately expansive because at least

823
00:34:42,199 --> 00:34:46,638
in the US regulators will come down on

824
00:34:44,540 --> 00:34:49,609
you not for what they collect but where

825
00:34:46,639 --> 00:34:51,590
you collect but from when you break your

826
00:34:49,609 --> 00:34:54,469
promise that's an unfair and deceptive

827
00:34:51,590 --> 00:34:57,020
trade practice according to US law so if

828
00:34:54,469 --> 00:35:00,680
you say you might do everything then you

829
00:34:57,020 --> 00:35:02,359
don't lie when you do everything you

830
00:35:00,680 --> 00:35:05,359
know we may collect personal information

831
00:35:02,359 --> 00:35:06,470
and other information about you remember

832
00:35:05,359 --> 00:35:08,299
the date of Roker's remember the

833
00:35:06,470 --> 00:35:10,310
analytic platforms from business

834
00:35:08,300 --> 00:35:14,930
partners contractors and other third

835
00:35:10,310 --> 00:35:17,450
parties in other words the world in

836
00:35:14,930 --> 00:35:19,700
quota Advisory Committee report to

837
00:35:17,450 --> 00:35:22,250
President Obama about five years ago

838
00:35:19,700 --> 00:35:23,868
only in some fantasy world the users

839
00:35:22,250 --> 00:35:25,490
actually read these notices and

840
00:35:23,869 --> 00:35:28,070
understand their implications before

841
00:35:25,490 --> 00:35:30,529
clicking to indicate their consent by

842
00:35:28,070 --> 00:35:32,900
and large that's true and remember

843
00:35:30,530 --> 00:35:34,700
because of all these third and fourth

844
00:35:32,900 --> 00:35:37,010
and fifth and sixth parties on the web

845
00:35:34,700 --> 00:35:41,000
you don't even know what websites you're

846
00:35:37,010 --> 00:35:44,480
consenting to you go to a news site a

847
00:35:41,000 --> 00:35:46,220
sports like what have you and you you're

848
00:35:44,480 --> 00:35:48,710
careful you read it and you look at the

849
00:35:46,220 --> 00:35:51,020
fine print says by the way read our

850
00:35:48,710 --> 00:35:55,359
advertising partners privacy policies to

851
00:35:51,020 --> 00:35:55,359
who are they good luck finding out

852
00:35:55,450 --> 00:36:01,819
location data is a huge issue for mobile

853
00:35:58,400 --> 00:36:04,820
devices lots of apps are collecting and

854
00:36:01,820 --> 00:36:08,200
analyzing this kind of data and even if

855
00:36:04,820 --> 00:36:13,940
the app is not doing the collection and

856
00:36:08,200 --> 00:36:17,240
transmission IP geolocation very mature

857
00:36:13,940 --> 00:36:21,350
technology reveals a lot is it perfect

858
00:36:17,240 --> 00:36:24,160
no is it very very good yes and this

859
00:36:21,350 --> 00:36:27,319
stuff doesn't have to be perfect

860
00:36:24,160 --> 00:36:30,230
if data exists it's available to

861
00:36:27,320 --> 00:36:32,180
governments sometimes in some government

862
00:36:30,230 --> 00:36:33,710
you've got a complex restricted and

863
00:36:32,180 --> 00:36:35,899
somewhat painful process

864
00:36:33,710 --> 00:36:38,120
to gain access to your data I said the

865
00:36:35,900 --> 00:36:40,250
US government at this is this 45 year

866
00:36:38,120 --> 00:36:42,080
old privacy law you can under certain

867
00:36:40,250 --> 00:36:44,450
circumstances gain access to certain

868
00:36:42,080 --> 00:36:46,850
information about the held about you

869
00:36:44,450 --> 00:36:49,009
other governments don't really care

870
00:36:46,850 --> 00:36:52,460
about the nice ease of privacy policies

871
00:36:49,010 --> 00:36:56,570
and access yeah it is your data we

872
00:36:52,460 --> 00:37:02,470
wanted we haven't go away and of course

873
00:36:56,570 --> 00:37:05,480
that's even ignoring what you know 193

874
00:37:02,470 --> 00:37:08,870
nations in the in the UN I think about

875
00:37:05,480 --> 00:37:11,390
192 of them have espionage agency is

876
00:37:08,870 --> 00:37:13,940
they collect data via technical means

877
00:37:11,390 --> 00:37:19,129
and other means and this you don't get

878
00:37:13,940 --> 00:37:22,160
to look at it all the privacy laws that

879
00:37:19,130 --> 00:37:23,960
we have are largely based on what's

880
00:37:22,160 --> 00:37:27,950
called PII personally identifiable

881
00:37:23,960 --> 00:37:32,360
information your name your email address

882
00:37:27,950 --> 00:37:34,609
a government ID number of some sort the

883
00:37:32,360 --> 00:37:35,870
definition varies the EU considers IP

884
00:37:34,610 --> 00:37:38,210
addresses PII

885
00:37:35,870 --> 00:37:41,540
much of the United States government

886
00:37:38,210 --> 00:37:43,160
does not I'm someplace to be I think

887
00:37:41,540 --> 00:37:45,290
they're both rights under depending on

888
00:37:43,160 --> 00:37:48,620
the circumstances but it turns out you

889
00:37:45,290 --> 00:37:52,190
don't need PII to invade somebody's

890
00:37:48,620 --> 00:37:54,529
privacy Hamazon doesn't need your name

891
00:37:52,190 --> 00:37:57,740
and address to recommend products oh

892
00:37:54,530 --> 00:37:59,360
they might like it oh you live in a

893
00:37:57,740 --> 00:38:01,910
well-to-do neighborhood we're going to

894
00:37:59,360 --> 00:38:05,420
recommend more expensive products you

895
00:38:01,910 --> 00:38:07,580
have an ethnic surname family name let

896
00:38:05,420 --> 00:38:10,040
me go recommend products that appeals to

897
00:38:07,580 --> 00:38:12,290
that ethnic group so can help but they

898
00:38:10,040 --> 00:38:15,350
don't really need that you know people

899
00:38:12,290 --> 00:38:17,090
who bought this also bought that Netflix

900
00:38:15,350 --> 00:38:19,190
doesn't need to know who you are to

901
00:38:17,090 --> 00:38:21,520
recommend movies TiVo doesn't need to

902
00:38:19,190 --> 00:38:24,590
know who you are to recommend TV shows

903
00:38:21,520 --> 00:38:28,100
it's a great essay out there you can

904
00:38:24,590 --> 00:38:30,220
find search for it called my TiVo thinks

905
00:38:28,100 --> 00:38:32,810
I'm gay

906
00:38:30,220 --> 00:38:34,850
somebody overreacted when he started

907
00:38:32,810 --> 00:38:36,859
getting recommendations from TiVo

908
00:38:34,850 --> 00:38:39,200
forgetting movies so

909
00:38:36,859 --> 00:38:41,989
we started this line to overcorrect by

910
00:38:39,200 --> 00:38:43,819
watching nanri he-man movies war movies

911
00:38:41,989 --> 00:38:46,580
and so on at that point you started

912
00:38:43,819 --> 00:38:49,730
showing him Nazi propaganda movies in

913
00:38:46,580 --> 00:38:49,730
[Applause]

914
00:38:49,749 --> 00:38:55,098
PII is actually just a database key but

915
00:38:53,359 --> 00:38:56,328
the database records exist on their own

916
00:38:55,099 --> 00:38:58,700
can be used for lots of things

917
00:38:56,329 --> 00:39:03,619
even without the key to look it up and

918
00:38:58,700 --> 00:39:05,808
to merge it if you're worried about PII

919
00:39:03,619 --> 00:39:08,329
some people try to anonymize the data

920
00:39:05,809 --> 00:39:12,140
what will strip off the identifying

921
00:39:08,329 --> 00:39:15,170
information it doesn't work first of all

922
00:39:12,140 --> 00:39:18,009
for most kinds of anonymization the real

923
00:39:15,170 --> 00:39:20,509
worlds are shown is easy to re identify

924
00:39:18,009 --> 00:39:24,710
or Earvin you've done some of that as I

925
00:39:20,509 --> 00:39:26,900
recall haven't you and if you do too

926
00:39:24,710 --> 00:39:29,150
good a job of anonymization you may

927
00:39:26,900 --> 00:39:31,249
actually destroy the utility of the date

928
00:39:29,150 --> 00:39:34,190
of a certain very important things for

929
00:39:31,249 --> 00:39:36,019
example some medical dosage calculations

930
00:39:34,190 --> 00:39:37,730
done based on machine learning on a

931
00:39:36,019 --> 00:39:40,308
large database of patient information

932
00:39:37,730 --> 00:39:42,769
very successful to calculating the

933
00:39:40,309 --> 00:39:47,450
proper dose of warfarin there which have

934
00:39:42,769 --> 00:39:49,459
been a very tricky problem but some

935
00:39:47,450 --> 00:39:51,049
academics showed that if you anonymize

936
00:39:49,460 --> 00:39:53,329
the data well enough to really hide the

937
00:39:51,049 --> 00:39:55,880
patient's identity the calculations

938
00:39:53,329 --> 00:39:57,920
wouldn't work you've hidden too much of

939
00:39:55,880 --> 00:40:00,859
the subtle details about the patient's

940
00:39:57,920 --> 00:40:04,670
medical condition so take your choice

941
00:40:00,859 --> 00:40:06,339
identification utility even for things

942
00:40:04,670 --> 00:40:10,539
that we all agree are useful like

943
00:40:06,339 --> 00:40:10,538
medical research to benefit everybody

944
00:40:11,890 --> 00:40:19,670
PII is focusing on PII also misses the

945
00:40:17,029 --> 00:40:23,599
importance today of machine learning and

946
00:40:19,670 --> 00:40:26,720
the inferences that it can make you can

947
00:40:23,599 --> 00:40:29,059
tell someone's sexual orientation from

948
00:40:26,720 --> 00:40:32,950
the kinds of things they do I will

949
00:40:29,059 --> 00:40:36,710
ignore them my Tivo thinks I'm gay but

950
00:40:32,950 --> 00:40:38,808
you can infer this is this good as event

951
00:40:36,710 --> 00:40:44,660
its private information to a lot of

952
00:40:38,809 --> 00:40:46,970
people whether or not it should be it's

953
00:40:44,660 --> 00:40:50,239
much much harder to control because it's

954
00:40:46,970 --> 00:40:50,509
not based on data directly collected you

955
00:40:50,239 --> 00:40:53,540
can

956
00:40:50,510 --> 00:40:56,800
say you cannot collect information say

957
00:40:53,540 --> 00:41:01,640
from my doctor on my sexual orientation

958
00:40:56,800 --> 00:41:07,400
but maybe there are proxy variables that

959
00:41:01,640 --> 00:41:12,140
tend to indicate it the foods that I buy

960
00:41:07,400 --> 00:41:14,480
might indicate my ethnicity proxy

961
00:41:12,140 --> 00:41:18,109
variables are a very powerful thing

962
00:41:14,480 --> 00:41:19,760
there was a study done by the Federal

963
00:41:18,110 --> 00:41:22,250
Trade US Federal Trade Commission about

964
00:41:19,760 --> 00:41:24,650
10 years ago they discovered that auto

965
00:41:22,250 --> 00:41:28,010
insurance companies were using credit

966
00:41:24,650 --> 00:41:29,600
scores to set rapes what is your ability

967
00:41:28,010 --> 00:41:30,860
or willingness to pay a debt have to do

968
00:41:29,600 --> 00:41:32,779
with whether or not you're going to get

969
00:41:30,860 --> 00:41:36,590
into an auto mode or a mobile accident

970
00:41:32,780 --> 00:41:39,380
and the FTC staff came to three

971
00:41:36,590 --> 00:41:41,480
conclusions one it was a valid predictor

972
00:41:39,380 --> 00:41:43,160
why what machine learning doesn't tell

973
00:41:41,480 --> 00:41:45,380
us why it just says there's a

974
00:41:43,160 --> 00:41:47,330
correlation this is a good predictor and

975
00:41:45,380 --> 00:41:49,490
insurance is about prediction and

976
00:41:47,330 --> 00:41:53,600
statistics not about causation

977
00:41:49,490 --> 00:41:57,259
two it was also a correlation with that

978
00:41:53,600 --> 00:41:59,480
with ethnicity the higher rates were

979
00:41:57,260 --> 00:42:02,660
going to certain ethnic groups based on

980
00:41:59,480 --> 00:42:05,600
credit scores well that's bad social

981
00:42:02,660 --> 00:42:07,490
policy so the FTC staff said we're going

982
00:42:05,600 --> 00:42:09,740
to solve this we're going to try to

983
00:42:07,490 --> 00:42:11,930
build a model that's just as predictive

984
00:42:09,740 --> 00:42:15,859
but not discriminatory and guess what

985
00:42:11,930 --> 00:42:19,759
they couldn't do it there was something

986
00:42:15,860 --> 00:42:22,550
deep in the data that said yes there is

987
00:42:19,760 --> 00:42:24,290
this true correlation that's going to

988
00:42:22,550 --> 00:42:26,600
discriminate if we don't do something

989
00:42:24,290 --> 00:42:28,900
regular in a regulatory fashion against

990
00:42:26,600 --> 00:42:32,150
certain ethnic groups in setting rates

991
00:42:28,900 --> 00:42:37,660
it's very hard to find and eliminate all

992
00:42:32,150 --> 00:42:37,660
of this and got nothing to do with PII

993
00:42:39,310 --> 00:42:47,420
so to me notice and consent is dead no

994
00:42:44,270 --> 00:42:49,310
one knows who collects the data no one

995
00:42:47,420 --> 00:42:51,830
knows what they'll do with it no one

996
00:42:49,310 --> 00:42:53,810
knows where it's stored and some of the

997
00:42:51,830 --> 00:42:56,240
most sensitive stuff like location is

998
00:42:53,810 --> 00:42:59,390
dual use it's used for your benefit

999
00:42:56,240 --> 00:43:02,689
you know how do I get from point A to

1000
00:42:59,390 --> 00:43:04,400
point B in a map program and as part of

1001
00:43:02,690 --> 00:43:07,039
what's called your data shadow

1002
00:43:04,400 --> 00:43:09,980
you know even the US Supreme Court has

1003
00:43:07,039 --> 00:43:12,740
noted how sensitive location data can be

1004
00:43:09,980 --> 00:43:15,349
in the aggregate so if we don't have

1005
00:43:12,740 --> 00:43:18,529
notice and consent what should we do

1006
00:43:15,349 --> 00:43:21,259
what should we replace it with one

1007
00:43:18,529 --> 00:43:24,499
answer is use control it's controversial

1008
00:43:21,259 --> 00:43:26,869
but give up on data collection

1009
00:43:24,499 --> 00:43:29,618
restriction it doesn't work better in

1010
00:43:26,869 --> 00:43:33,440
the EU but still doesn't work that well

1011
00:43:29,619 --> 00:43:35,329
instead let people specify how their

1012
00:43:33,440 --> 00:43:37,869
data can be used not what can be

1013
00:43:35,329 --> 00:43:40,490
collected but what it can be used for

1014
00:43:37,869 --> 00:43:42,980
targeted advertising statistical

1015
00:43:40,490 --> 00:43:45,618
analysis medical research what-have-you

1016
00:43:42,980 --> 00:43:49,579
it sounds like a great idea it's not

1017
00:43:45,619 --> 00:43:52,880
that easy how do you define your use

1018
00:43:49,579 --> 00:43:56,740
categories how do you give people a

1019
00:43:52,880 --> 00:43:58,759
really usable interface to specify

1020
00:43:56,740 --> 00:44:02,118
here's a kind of data we're collecting

1021
00:43:58,759 --> 00:44:04,339
and here's a kind of use that you may or

1022
00:44:02,119 --> 00:44:09,589
may not want to permit for this kind of

1023
00:44:04,339 --> 00:44:13,700
data usability of privacy settings is a

1024
00:44:09,589 --> 00:44:16,460
fiendishly difficult problem very few of

1025
00:44:13,700 --> 00:44:18,019
anyone has gotten that right you've got

1026
00:44:16,460 --> 00:44:21,650
to give consent across long time

1027
00:44:18,019 --> 00:44:25,899
intervals you know I have been posting

1028
00:44:21,650 --> 00:44:28,249
stuff on the net for about 40 years now

1029
00:44:25,900 --> 00:44:30,319
Brian mentions one of the people created

1030
00:44:28,249 --> 00:44:32,779
net News which went live in January of

1031
00:44:30,319 --> 00:44:36,410
1980 I was one of the founders so I was

1032
00:44:32,779 --> 00:44:39,140
out there from the very beginning do I

1033
00:44:36,410 --> 00:44:42,230
have the same preferences today as I had

1034
00:44:39,140 --> 00:44:44,450
40 years ago I was lucky my first boss

1035
00:44:42,230 --> 00:44:45,980
at Bell Labs when I walked into his

1036
00:44:44,450 --> 00:44:48,140
office a few years after that said I've

1037
00:44:45,980 --> 00:44:49,130
seen your flames on net news Steve yeah

1038
00:44:48,140 --> 00:44:51,200
okay

1039
00:44:49,130 --> 00:44:56,660
upper management reads these things yeah

1040
00:44:51,200 --> 00:44:59,328
okay data that exists can be abused by

1041
00:44:56,660 --> 00:45:01,450
hackers scofflaws governments were

1042
00:44:59,329 --> 00:45:05,749
simply through a change in the law and

1043
00:45:01,450 --> 00:45:08,629
it turns out that under US law it may be

1044
00:45:05,749 --> 00:45:11,390
impossible to mandate use restrictions

1045
00:45:08,630 --> 00:45:13,400
for companies they could adopt it but

1046
00:45:11,390 --> 00:45:17,580
you may not be able to mandate it but a

1047
00:45:13,400 --> 00:45:18,900
US law so how do we implement these

1048
00:45:17,580 --> 00:45:22,319
control you could start with a

1049
00:45:18,900 --> 00:45:25,500
privacy-preserving credential scheme tag

1050
00:45:22,320 --> 00:45:28,020
all the data that you create with a

1051
00:45:25,500 --> 00:45:31,080
privacy-preserving sub identity and a

1052
00:45:28,020 --> 00:45:34,670
data type and you can publish tuple

1053
00:45:31,080 --> 00:45:37,770
saying data type anonymous identity

1054
00:45:34,670 --> 00:45:39,660
allowed uses and all digitally signed

1055
00:45:37,770 --> 00:45:41,430
with your anonymous credential supermoms

1056
00:45:39,660 --> 00:45:44,190
credential and where we put it well gee

1057
00:45:41,430 --> 00:45:51,960
do we put it in the blockchain no no

1058
00:45:44,190 --> 00:45:53,490
tomatoes please and if if you change

1059
00:45:51,960 --> 00:45:55,380
your mind about something you just push

1060
00:45:53,490 --> 00:45:57,990
out something at your newest it's your

1061
00:45:55,380 --> 00:46:00,180
new a statement that wins enforcement

1062
00:45:57,990 --> 00:46:02,729
well it's often pointed out governments

1063
00:46:00,180 --> 00:46:04,529
have a role if you break a legally

1064
00:46:02,730 --> 00:46:08,180
binding promise if you break a law

1065
00:46:04,530 --> 00:46:08,180
governments can come down on you

1066
00:46:08,570 --> 00:46:12,780
difficult but might be doable might

1067
00:46:11,010 --> 00:46:15,720
being worth examining as a research

1068
00:46:12,780 --> 00:46:20,190
project what we really need is a new

1069
00:46:15,720 --> 00:46:22,709
privacy paradigm he's got a scales are

1070
00:46:20,190 --> 00:46:24,630
very many data collectors known and

1071
00:46:22,710 --> 00:46:26,370
unknown in the future it has to scale

1072
00:46:24,630 --> 00:46:28,950
across time it's got to be

1073
00:46:26,370 --> 00:46:31,470
comprehensible by individuals it's got

1074
00:46:28,950 --> 00:46:33,210
to account for inferences it's got to

1075
00:46:31,470 --> 00:46:35,279
trade-off the harms and benefits of

1076
00:46:33,210 --> 00:46:37,560
different kinds of data use and I have

1077
00:46:35,280 --> 00:46:39,510
no idea what such a paradigm would look

1078
00:46:37,560 --> 00:46:41,790
like or is that for me as an academic

1079
00:46:39,510 --> 00:46:44,520
that's great if we knew the answer it

1080
00:46:41,790 --> 00:46:48,480
wouldn't be research but yeah this is

1081
00:46:44,520 --> 00:46:52,050
the real challenge how do we do this so

1082
00:46:48,480 --> 00:46:54,420
what should the IETF do obviously

1083
00:46:52,050 --> 00:46:55,950
encrypt as much as possible the ITF has

1084
00:46:54,420 --> 00:46:59,090
been moving in that direction for more

1085
00:46:55,950 --> 00:47:02,250
than 20 years and that's great avoid

1086
00:46:59,090 --> 00:47:05,580
creating unnecessary third party

1087
00:47:02,250 --> 00:47:09,240
metadata one place this really shows up

1088
00:47:05,580 --> 00:47:11,580
in protocol definitions is stuff that's

1089
00:47:09,240 --> 00:47:15,450
left to the implementation because that

1090
00:47:11,580 --> 00:47:18,029
becomes finger printable how about to

1091
00:47:15,450 --> 00:47:20,430
pick one random example if the HTTP

1092
00:47:18,030 --> 00:47:22,950
header headers could only be in a

1093
00:47:20,430 --> 00:47:24,870
certain specified order and even if they

1094
00:47:22,950 --> 00:47:27,240
know you like to specify it and didn't

1095
00:47:24,870 --> 00:47:29,870
and how you know just a semicolon or

1096
00:47:27,240 --> 00:47:33,950
something design

1097
00:47:29,870 --> 00:47:36,850
privacy protocols do a privacy analysis

1098
00:47:33,950 --> 00:47:39,529
of protocols similar to what is done for

1099
00:47:36,850 --> 00:47:41,210
security considerations today you know

1100
00:47:39,530 --> 00:47:45,310
some years ago there was the Geo Kirov

1101
00:47:41,210 --> 00:47:47,720
working group geo locations and okay we

1102
00:47:45,310 --> 00:47:52,430
this is dangerous stuff from a privacy

1103
00:47:47,720 --> 00:47:54,169
perspective let's look at it first

1104
00:47:52,430 --> 00:47:57,109
tagging might help create some more

1105
00:47:54,170 --> 00:47:59,390
metadata so here are the references most

1106
00:47:57,110 --> 00:48:01,100
of the quotes are from my comments on

1107
00:47:59,390 --> 00:48:04,040
privacy again written for us legal

1108
00:48:01,100 --> 00:48:07,200
context I apologize quoted assorted

1109
00:48:04,040 --> 00:48:10,330
academics from 50 years ago thank you

1110
00:48:07,200 --> 00:48:10,330
[Applause]

1111
00:48:14,140 --> 00:48:18,470
thank you very much week that we open

1112
00:48:17,300 --> 00:48:23,450
the floor to questions

1113
00:48:18,470 --> 00:48:25,669
I have Eliot Steve Arvind thanks very

1114
00:48:23,450 --> 00:48:30,410
much for your presentations two comments

1115
00:48:25,670 --> 00:48:32,570
first of all related to Arbenz work I'm

1116
00:48:30,410 --> 00:48:34,940
very pleased to find a colleague of

1117
00:48:32,570 --> 00:48:36,520
yours sir gentleman at Berkeley who's

1118
00:48:34,940 --> 00:48:38,960
done a lot of work in this space

1119
00:48:36,520 --> 00:48:42,740
particularly around linkages on

1120
00:48:38,960 --> 00:48:47,060
cellphones and the idea that you have

1121
00:48:42,740 --> 00:48:49,220
about TLS and privacy of IOT is

1122
00:48:47,060 --> 00:48:51,740
something that I that I am deeply

1123
00:48:49,220 --> 00:48:54,290
involved in and one of the things that

1124
00:48:51,740 --> 00:48:57,020
it raises the question of privacy

1125
00:48:54,290 --> 00:49:00,200
brokerage we release this information

1126
00:48:57,020 --> 00:49:02,120
some often times we release information

1127
00:49:00,200 --> 00:49:04,250
for a purpose and the notion of

1128
00:49:02,120 --> 00:49:07,190
contextual privacy is something that I

1129
00:49:04,250 --> 00:49:09,650
think is a relatively nation if I

1130
00:49:07,190 --> 00:49:11,630
understand in terms of research and I'd

1131
00:49:09,650 --> 00:49:13,430
be very interested to see us continue

1132
00:49:11,630 --> 00:49:16,400
that discussion here at the IDF or at

1133
00:49:13,430 --> 00:49:18,140
least at the IRT F as to how as to what

1134
00:49:16,400 --> 00:49:20,900
that means and this goes to the tagging

1135
00:49:18,140 --> 00:49:22,640
that you mentioned Steve so thanks for

1136
00:49:20,900 --> 00:49:25,010
your research and if people haven't

1137
00:49:22,640 --> 00:49:26,839
looked at surges work to he's done a lot

1138
00:49:25,010 --> 00:49:31,000
of work particularly around the Amazon

1139
00:49:26,840 --> 00:49:36,950
echo recently that's very useful Thanks

1140
00:49:31,000 --> 00:49:38,570
Thank You Barry hi this is very this is

1141
00:49:36,950 --> 00:49:39,140
a very live Mike hi this is Barry

1142
00:49:38,570 --> 00:49:42,910
Lieberstein

1143
00:49:39,140 --> 00:49:46,049
how does notice in consent and

1144
00:49:42,910 --> 00:49:48,460
GDP our work with being trapped by

1145
00:49:46,049 --> 00:49:50,349
Facebook and Twitter icons in the one

1146
00:49:48,460 --> 00:49:52,510
that's another reason notice that

1147
00:49:50,349 --> 00:49:54,160
consent doesn't work you're being

1148
00:49:52,510 --> 00:49:57,069
tracked by lots of people with whom you

1149
00:49:54,160 --> 00:49:59,049
don't have a direct relationship and how

1150
00:49:57,069 --> 00:50:01,230
can you consent I don't consume can I

1151
00:49:59,049 --> 00:50:04,650
say I don't consent to seeing a facebook

1152
00:50:01,230 --> 00:50:08,380
like button on a webpage I want to visit

1153
00:50:04,650 --> 00:50:12,460
do you do you have any idea how they

1154
00:50:08,380 --> 00:50:14,410
have not been attacked by the GDP RP I'm

1155
00:50:12,460 --> 00:50:25,839
not a lawyer I will let the lawyers

1156
00:50:14,410 --> 00:50:28,328
answer that one so this is actually I'm

1157
00:50:25,839 --> 00:50:29,529
interested in responses from both of you

1158
00:50:28,329 --> 00:50:32,530
that there's been a lot of interest in

1159
00:50:29,530 --> 00:50:34,710
the IHF in privacy issues around the DNS

1160
00:50:32,530 --> 00:50:37,960
and so I'm wondering what you think of

1161
00:50:34,710 --> 00:50:41,170
the risks privacy risks around that and

1162
00:50:37,960 --> 00:50:44,710
whether it dot and doe provide useful

1163
00:50:41,170 --> 00:50:46,450
solutions privacy like any of the

1164
00:50:44,710 --> 00:50:48,700
security problem has to be done in the

1165
00:50:46,450 --> 00:50:51,308
context of a threat model who is trying

1166
00:50:48,700 --> 00:50:55,980
to collect this data what are they going

1167
00:50:51,309 --> 00:50:59,829
to do with it you know with DNS over

1168
00:50:55,980 --> 00:51:01,539
HTTP over TLS you know you might get a

1169
00:50:59,829 --> 00:51:04,270
central aggregation point and do you

1170
00:51:01,539 --> 00:51:06,609
trust them to be honest secure against

1171
00:51:04,270 --> 00:51:08,410
governments secure against governments

1172
00:51:06,609 --> 00:51:10,359
who come armed with legal process and

1173
00:51:08,410 --> 00:51:13,589
you don't necessarily have a business

1174
00:51:10,359 --> 00:51:19,119
relationship it's not clear to me that

1175
00:51:13,589 --> 00:51:23,349
guarding against the NSA your GCHQ or

1176
00:51:19,119 --> 00:51:25,869
the FSB or GRU the Mossad or whomever is

1177
00:51:23,349 --> 00:51:27,640
the best threat model versus the

1178
00:51:25,869 --> 00:51:30,700
commercial threat model that one might

1179
00:51:27,640 --> 00:51:33,879
actually be best dealt with with laws

1180
00:51:30,700 --> 00:51:36,038
saying your ISP can't use collect or use

1181
00:51:33,880 --> 00:51:38,200
this data in any way rather than this

1182
00:51:36,039 --> 00:51:40,539
technical mechanism and to Voyage the

1183
00:51:38,200 --> 00:51:41,890
central point of collection which is a

1184
00:51:40,539 --> 00:51:46,589
greater threat for against certain

1185
00:51:41,890 --> 00:51:49,629
threat miles watch the threat model mary

1186
00:51:46,589 --> 00:51:51,578
hi Malory knodel article 19 and so my

1187
00:51:49,630 --> 00:51:53,829
questions for Arvind I really liked your

1188
00:51:51,579 --> 00:51:56,710
example of how a limited technical

1189
00:51:53,829 --> 00:51:58,839
mitigation was encouraged a strong

1190
00:51:56,710 --> 00:52:00,640
policy that then filled that gap for

1191
00:51:58,839 --> 00:52:03,099
user privacy this was like about a third

1192
00:52:00,640 --> 00:52:05,078
true your presentation and I think that

1193
00:52:03,099 --> 00:52:07,750
pair like paralleling that example with

1194
00:52:05,079 --> 00:52:10,150
your conclusion is also interesting so I

1195
00:52:07,750 --> 00:52:11,619
guess my question would be and then I

1196
00:52:10,150 --> 00:52:15,190
can explain a bit more if that's helpful

1197
00:52:11,619 --> 00:52:18,280
is I mean I think there's there is a

1198
00:52:15,190 --> 00:52:20,890
role that that measurement and research

1199
00:52:18,280 --> 00:52:23,050
can still play even if it's limited now

1200
00:52:20,890 --> 00:52:25,629
because of the privacy enhanced

1201
00:52:23,050 --> 00:52:27,339
protocols that were using how can we

1202
00:52:25,630 --> 00:52:29,470
instead pivot instead of trying to

1203
00:52:27,339 --> 00:52:31,029
bargain like in the stages of grief or

1204
00:52:29,470 --> 00:52:33,069
in the bargaining stage like how can we

1205
00:52:31,030 --> 00:52:35,530
do both of these things when there's an

1206
00:52:33,069 --> 00:52:38,890
actual inherent technical paradox

1207
00:52:35,530 --> 00:52:42,220
between the two and rather go into pivot

1208
00:52:38,890 --> 00:52:45,660
into a more complicated relationship

1209
00:52:42,220 --> 00:52:48,308
between policy incentives policy sticks

1210
00:52:45,660 --> 00:52:51,250
and then you know so I I wonder how

1211
00:52:48,309 --> 00:52:54,790
academic researchers can help for

1212
00:52:51,250 --> 00:52:57,540
example human right or not Human Rights

1213
00:52:54,790 --> 00:53:00,009
but like impact assessments or getting

1214
00:52:57,540 --> 00:53:02,950
companies to take more responsibility

1215
00:53:00,010 --> 00:53:05,890
for doing privacy audits security

1216
00:53:02,950 --> 00:53:08,078
audience and it's a slower approach it's

1217
00:53:05,890 --> 00:53:10,808
not as fast as scanning a million

1218
00:53:08,079 --> 00:53:13,119
websites every month but I think that

1219
00:53:10,809 --> 00:53:16,089
where we're at now and the way that

1220
00:53:13,119 --> 00:53:19,210
we've advanced privacy for end-users to

1221
00:53:16,089 --> 00:53:21,779
get the higher hanging fruit we actually

1222
00:53:19,210 --> 00:53:25,480
have to have more complicated approaches

1223
00:53:21,780 --> 00:53:28,299
that's great thank you the way in which

1224
00:53:25,480 --> 00:53:31,390
I think measurement research has helped

1225
00:53:28,299 --> 00:53:32,380
at a very high level is in economic

1226
00:53:31,390 --> 00:53:34,690
terms

1227
00:53:32,380 --> 00:53:38,200
closing the information asymmetry and

1228
00:53:34,690 --> 00:53:40,980
what I mean by that is when a product

1229
00:53:38,200 --> 00:53:44,618
doesn't live up to its privacy claims

1230
00:53:40,980 --> 00:53:46,569
oftentimes the buyers users consumers of

1231
00:53:44,619 --> 00:53:50,200
that products don't know and have no way

1232
00:53:46,569 --> 00:53:52,540
of knowing and this parallels in in the

1233
00:53:50,200 --> 00:53:54,759
United States one of the critical

1234
00:53:52,540 --> 00:53:57,609
situations that we had with respect to

1235
00:53:54,760 --> 00:53:59,890
used car sales 40 years ago and the

1236
00:53:57,609 --> 00:54:02,078
matter became so critical that buyers of

1237
00:53:59,890 --> 00:54:03,910
used cars would not know if the car had

1238
00:54:02,079 --> 00:54:05,770
a critical defect whereas the seller

1239
00:54:03,910 --> 00:54:07,960
would know and would not tell them and

1240
00:54:05,770 --> 00:54:09,640
it was exactly the same kind of problem

1241
00:54:07,960 --> 00:54:10,450
that was faced economists call this an

1242
00:54:09,640 --> 00:54:12,250
information age

1243
00:54:10,450 --> 00:54:15,040
symmetry and that particular information

1244
00:54:12,250 --> 00:54:17,490
asymmetry was closed by lemon laws that

1245
00:54:15,040 --> 00:54:20,230
mandated certain information exclosure

1246
00:54:17,490 --> 00:54:23,200
disclosure pardon me that's guaranteed

1247
00:54:20,230 --> 00:54:25,270
the right for buyers of cars to first

1248
00:54:23,200 --> 00:54:25,689
take it to mechanics to be inspected and

1249
00:54:25,270 --> 00:54:28,329
so on

1250
00:54:25,690 --> 00:54:30,070
so broadly to your question as long as

1251
00:54:28,329 --> 00:54:32,349
we have some way of closing this

1252
00:54:30,070 --> 00:54:34,960
information asymmetry that exists

1253
00:54:32,349 --> 00:54:36,880
between the sellers of products and

1254
00:54:34,960 --> 00:54:38,320
services and the people who use them I

1255
00:54:36,880 --> 00:54:39,880
think we're in good shape

1256
00:54:38,320 --> 00:54:41,530
and one of the ways we've been doing

1257
00:54:39,880 --> 00:54:43,150
that is with academic research that's

1258
00:54:41,530 --> 00:54:45,220
been you know scanning a million

1259
00:54:43,150 --> 00:54:48,010
endpoints at once but it doesn't have to

1260
00:54:45,220 --> 00:54:49,839
be the only way another critical way to

1261
00:54:48,010 --> 00:54:52,150
do that has been journalists have been

1262
00:54:49,839 --> 00:54:54,549
you know individually examining these

1263
00:54:52,150 --> 00:54:56,589
products in a lot of detail and holding

1264
00:54:54,550 --> 00:54:58,480
companies feet to the fire so as long as

1265
00:54:56,589 --> 00:55:00,250
we have some oversight mechanism whether

1266
00:54:58,480 --> 00:55:02,410
that comes from moi whether that comes

1267
00:55:00,250 --> 00:55:04,270
from academia whether that comes from

1268
00:55:02,410 --> 00:55:06,910
journalism or whether it simply comes

1269
00:55:04,270 --> 00:55:09,160
from a more informed public that helps

1270
00:55:06,910 --> 00:55:12,930
close this information asymmetry then I

1271
00:55:09,160 --> 00:55:12,930
think we'll be in better shape thank you

1272
00:55:14,099 --> 00:55:20,260
so Peter file Deutsche Telekom so from

1273
00:55:18,670 --> 00:55:23,290
my point of view that was an excellent

1274
00:55:20,260 --> 00:55:27,700
presentation that it was very technology

1275
00:55:23,290 --> 00:55:31,089
oriented so we have and also very North

1276
00:55:27,700 --> 00:55:34,029
American oriented so in in Europe

1277
00:55:31,089 --> 00:55:38,920
especially in Germany we have very

1278
00:55:34,030 --> 00:55:40,500
severe laws regarding privacy so you

1279
00:55:38,920 --> 00:55:44,170
mentioned the gdpr

1280
00:55:40,500 --> 00:55:48,339
which is in effect since last year

1281
00:55:44,170 --> 00:55:50,829
basically and I don't think that we can

1282
00:55:48,339 --> 00:55:54,430
solve this issue from a technical point

1283
00:55:50,829 --> 00:55:55,839
of view it's a legal issue so in in

1284
00:55:54,430 --> 00:55:58,210
Europe I don't know if you were at the

1285
00:55:55,839 --> 00:56:01,750
news but Facebook will probably have to

1286
00:55:58,210 --> 00:56:05,980
pay some billions because they did not

1287
00:56:01,750 --> 00:56:09,550
follow the rules of this law and in

1288
00:56:05,980 --> 00:56:12,579
Europe the any data I give to any any

1289
00:56:09,550 --> 00:56:14,749
company is owned by me and not by the

1290
00:56:12,579 --> 00:56:16,969
company who gets this

1291
00:56:14,749 --> 00:56:20,269
so this is something that has to be

1292
00:56:16,969 --> 00:56:25,279
changed worldwide so especially in the

1293
00:56:20,269 --> 00:56:27,319
US and again I just wanted to find out

1294
00:56:25,279 --> 00:56:30,319
that it's not a technical issue it's an

1295
00:56:27,319 --> 00:56:35,299
Eevee issue I agree I agree completely

1296
00:56:30,319 --> 00:56:37,609
the document that that I wrote to it my

1297
00:56:35,299 --> 00:56:40,309
truck was derived from was a submission

1298
00:56:37,609 --> 00:56:42,288
to a US government process on privacy

1299
00:56:40,309 --> 00:56:45,549
because I agree completely there's a

1300
00:56:42,289 --> 00:56:47,839
very important legal role for the

1301
00:56:45,549 --> 00:56:51,170
legalities here and governments around

1302
00:56:47,839 --> 00:56:53,049
the world but I think that trying to

1303
00:56:51,170 --> 00:56:55,519
base your privacy on notice and consent

1304
00:56:53,049 --> 00:56:58,640
from a technical perspective is not

1305
00:56:55,519 --> 00:57:01,788
going to work and I want what I just

1306
00:56:58,640 --> 00:57:04,939
what my paper said is we need to find a

1307
00:57:01,789 --> 00:57:09,349
different paradigm for regulators and

1308
00:57:04,939 --> 00:57:10,819
legislators to mandate so thank you for

1309
00:57:09,349 --> 00:57:14,119
your comment I completely take your

1310
00:57:10,819 --> 00:57:16,189
point that the role of regulation in the

1311
00:57:14,119 --> 00:57:18,559
law is very critical here thank you for

1312
00:57:16,189 --> 00:57:20,328
bringing up the gdpr one thing I want to

1313
00:57:18,559 --> 00:57:22,789
slightly push back on is that I wouldn't

1314
00:57:20,329 --> 00:57:25,279
see it as a technical or legal issue I

1315
00:57:22,789 --> 00:57:27,199
don't think it's a dichotomy in fact a

1316
00:57:25,279 --> 00:57:29,509
lot of the investigations that have come

1317
00:57:27,199 --> 00:57:31,670
about under the gdpr and the fines that

1318
00:57:29,509 --> 00:57:33,469
have resulted from that those privacy

1319
00:57:31,670 --> 00:57:34,999
issues only came to be known because of

1320
00:57:33,469 --> 00:57:36,410
the kind of research that I described

1321
00:57:34,999 --> 00:57:38,209
whether it was done by academics

1322
00:57:36,410 --> 00:57:40,038
journalists or some other third parties

1323
00:57:38,209 --> 00:57:41,899
so that's technical work in a sense and

1324
00:57:40,039 --> 00:57:43,219
and for me the real success stories

1325
00:57:41,900 --> 00:57:48,049
involve the collaboration between

1326
00:57:43,219 --> 00:57:49,339
technical teams and legal measures thank

1327
00:57:48,049 --> 00:57:51,109
you we're gonna head and close the mic

1328
00:57:49,339 --> 00:57:54,259
line so we'll drain the cues please be

1329
00:57:51,109 --> 00:57:55,279
brief carlos winning a seat books thank

1330
00:57:54,259 --> 00:57:57,829
you very much for your thoughts they

1331
00:57:55,279 --> 00:57:59,119
both first even though harvest I want to

1332
00:57:57,829 --> 00:58:00,829
actually follow up on this discussion

1333
00:57:59,119 --> 00:58:02,719
because in fact that's precisely what I

1334
00:58:00,829 --> 00:58:04,519
wanted to say in this world where we

1335
00:58:02,719 --> 00:58:06,199
leave privacy is getting harder the

1336
00:58:04,519 --> 00:58:08,839
battle is lost I very much appreciate

1337
00:58:06,199 --> 00:58:11,689
the message of no it's not lost we can

1338
00:58:08,839 --> 00:58:13,308
keep improving things and also just

1339
00:58:11,689 --> 00:58:15,739
following up from the previous speaker

1340
00:58:13,309 --> 00:58:17,449
that we are going all the way from the

1341
00:58:15,739 --> 00:58:19,549
extremes of regulations towards the

1342
00:58:17,449 --> 00:58:21,619
purely technical and I think they both

1343
00:58:19,549 --> 00:58:23,929
have to now at some point in time it's

1344
00:58:21,619 --> 00:58:26,809
true we don't have the answers to all

1345
00:58:23,929 --> 00:58:27,470
the questions but and it's true that for

1346
00:58:26,809 --> 00:58:29,720
instance I was

1347
00:58:27,470 --> 00:58:32,779
regulatory discussion where they were

1348
00:58:29,720 --> 00:58:36,980
just discussing okay in a world where

1349
00:58:32,780 --> 00:58:38,510
the watch is checking your vital signs

1350
00:58:36,980 --> 00:58:39,980
and then it's sending it to your phone

1351
00:58:38,510 --> 00:58:41,180
and then that's anything to an app and

1352
00:58:39,980 --> 00:58:43,520
that's sending it to our cloud provider

1353
00:58:41,180 --> 00:58:45,618
and who do you regulate it's nothing

1354
00:58:43,520 --> 00:58:48,770
more the world of one piece does one

1355
00:58:45,619 --> 00:58:50,480
thing so it's important I agree with

1356
00:58:48,770 --> 00:58:53,720
Arvind and message that we can help

1357
00:58:50,480 --> 00:58:55,099
their regulatory bodies understand that

1358
00:58:53,720 --> 00:58:56,899
is the service provider probably the

1359
00:58:55,099 --> 00:58:59,810
most accountable one that will make sure

1360
00:58:56,900 --> 00:59:03,140
that the information flows down and also

1361
00:58:59,810 --> 00:59:06,859
on our side as a technical writers we do

1362
00:59:03,140 --> 00:59:09,020
have indeed the the role of writing the

1363
00:59:06,859 --> 00:59:12,619
right standard but also educating people

1364
00:59:09,020 --> 00:59:14,960
as much as we can regulatory it could be

1365
00:59:12,619 --> 00:59:16,640
a section Steven you mentioned the

1366
00:59:14,960 --> 00:59:18,590
privacy considerations I think that's a

1367
00:59:16,640 --> 00:59:20,089
great way to communicate what the

1368
00:59:18,590 --> 00:59:22,310
standard should do what what are the

1369
00:59:20,090 --> 00:59:25,460
issues what should be looked after and

1370
00:59:22,310 --> 00:59:27,440
then follow up and make sure that we do

1371
00:59:25,460 --> 00:59:29,570
improve because definitely the the

1372
00:59:27,440 --> 00:59:35,900
battle is not lost and there's a lot of

1373
00:59:29,570 --> 00:59:37,910
things we can keep you high riad Wahby

1374
00:59:35,900 --> 00:59:40,760
Arvind you mentioned at the very end of

1375
00:59:37,910 --> 00:59:42,710
your talk a more project of Stanford TLS

1376
00:59:40,760 --> 00:59:44,750
are AR rotate and release I was one of

1377
00:59:42,710 --> 00:59:47,119
the authors on that so I think you're

1378
00:59:44,750 --> 00:59:48,890
absolutely right that there are some

1379
00:59:47,119 --> 00:59:50,510
technical measures like that but just to

1380
00:59:48,890 --> 00:59:52,430
provide a little background in kind of a

1381
00:59:50,510 --> 00:59:54,050
counterpoint while we were working on

1382
00:59:52,430 --> 00:59:55,609
that we actually spoke with some of the

1383
00:59:54,050 --> 00:59:57,200
people in the TLS working group and said

1384
00:59:55,609 --> 00:59:58,790
hey look it might be the case that like

1385
00:59:57,200 --> 01:00:02,450
a small change to TLS would actually

1386
00:59:58,790 --> 01:00:04,520
make this easier and be very rightly got

1387
01:00:02,450 --> 01:00:06,140
pushback them from the TLS working group

1388
01:00:04,520 --> 01:00:08,060
who said yeah but we don't want to make

1389
01:00:06,140 --> 01:00:09,830
this easier because yeah you might want

1390
01:00:08,060 --> 01:00:11,720
to use it for watching your own devices

1391
01:00:09,830 --> 01:00:13,369
but anything that we make easier for you

1392
01:00:11,720 --> 01:00:16,580
is can also be easier for somebody who's

1393
01:00:13,369 --> 01:00:19,730
spying on you so while it's true that we

1394
01:00:16,580 --> 01:00:22,220
want to look at our devices it seems

1395
01:00:19,730 --> 01:00:25,130
like technical measures at the level of

1396
01:00:22,220 --> 01:00:26,868
you know the encryption standards maybe

1397
01:00:25,130 --> 01:00:29,060
not the right way to go we may be in

1398
01:00:26,869 --> 01:00:30,830
some sense at the mercy of the people

1399
01:00:29,060 --> 01:00:33,109
who are building the devices almost no

1400
01:00:30,830 --> 01:00:35,330
matter what we do because you know we

1401
01:00:33,109 --> 01:00:36,859
shouldn't insert back doors into TLS for

1402
01:00:35,330 --> 01:00:37,560
our own good they hurt us more than they

1403
01:00:36,859 --> 01:00:40,620
look

1404
01:00:37,560 --> 01:00:42,660
so just yeah thank you for your comment

1405
01:00:40,620 --> 01:00:45,000
and somewhat aware of the debates that

1406
01:00:42,660 --> 01:00:48,149
have gone on in the in the tls working

1407
01:00:45,000 --> 01:00:49,890
group my main goal is to call attention

1408
01:00:48,150 --> 01:00:51,090
to the severity of the problem I'm not

1409
01:00:49,890 --> 01:00:53,370
claiming that I know what the right

1410
01:00:51,090 --> 01:01:01,980
solution is but I think the current

1411
01:00:53,370 --> 01:01:03,750
situation is perhaps not optimal I would

1412
01:01:01,980 --> 01:01:09,060
like to ask you a question following the

1413
01:01:03,750 --> 01:01:11,820
the gentleman from German Telecom about

1414
01:01:09,060 --> 01:01:13,920
ownership of the data this is a very big

1415
01:01:11,820 --> 01:01:16,020
difference in the US and Europe for

1416
01:01:13,920 --> 01:01:18,420
example where ownership of the data is

1417
01:01:16,020 --> 01:01:20,790
always about me when I'm in Europe and

1418
01:01:18,420 --> 01:01:22,830
once is collected in the u.s. is

1419
01:01:20,790 --> 01:01:27,300
property of who collected the data and

1420
01:01:22,830 --> 01:01:30,029
as I think is the biggest issue in when

1421
01:01:27,300 --> 01:01:32,370
privacy instead of giving out your data

1422
01:01:30,030 --> 01:01:35,520
you can borrow my data but I can always

1423
01:01:32,370 --> 01:01:38,100
ask you to remove your my data from your

1424
01:01:35,520 --> 01:01:39,780
system whenever I want to you know I

1425
01:01:38,100 --> 01:01:43,650
don't want to have this very issue with

1426
01:01:39,780 --> 01:01:45,540
us etc and and if you can elaborate on

1427
01:01:43,650 --> 01:01:48,720
that if you think that this might be a

1428
01:01:45,540 --> 01:01:50,040
tool that enhance privacy from a legal

1429
01:01:48,720 --> 01:01:51,750
standpoint of unit since that give me

1430
01:01:50,040 --> 01:01:53,940
give me as a user

1431
01:01:51,750 --> 01:01:55,950
a possible recourse of action in case

1432
01:01:53,940 --> 01:01:58,920
you're you're failing to protect my

1433
01:01:55,950 --> 01:02:00,870
privacy and the second point is about

1434
01:01:58,920 --> 01:02:03,120
the measurement you say that you know

1435
01:02:00,870 --> 01:02:05,370
this type of privacy issue came out

1436
01:02:03,120 --> 01:02:07,620
because of measurements maybe it's time

1437
01:02:05,370 --> 01:02:11,250
to talk about having measurements as

1438
01:02:07,620 --> 01:02:13,589
part of the legal framework so that it's

1439
01:02:11,250 --> 01:02:15,540
not left to academic to expose this but

1440
01:02:13,590 --> 01:02:21,480
with what is it actually have authority

1441
01:02:15,540 --> 01:02:23,850
to and to follow up on that data

1442
01:02:21,480 --> 01:02:26,520
ownership is a really complicated

1443
01:02:23,850 --> 01:02:29,040
question there's been a fair amount of

1444
01:02:26,520 --> 01:02:31,230
legal writing lately legal academic

1445
01:02:29,040 --> 01:02:34,350
writing on why trying to treat data as

1446
01:02:31,230 --> 01:02:36,750
property can have bad side effects one

1447
01:02:34,350 --> 01:02:39,089
of the interesting things from US law is

1448
01:02:36,750 --> 01:02:41,040
that a lot of these transactions there

1449
01:02:39,090 --> 01:02:44,280
are two different parties that have

1450
01:02:41,040 --> 01:02:47,180
ownership so I mentioned about the my

1451
01:02:44,280 --> 01:02:50,660
mechanics uploading it was selling my

1452
01:02:47,180 --> 01:02:52,430
odometer readings oh yes my ODOT my mic

1453
01:02:50,660 --> 01:02:54,109
is recording the odometer reading to go

1454
01:02:52,430 --> 01:02:56,660
let me know when I should change my oil

1455
01:02:54,110 --> 01:02:58,730
again and that's perfectly that becomes

1456
01:02:56,660 --> 01:03:02,210
a business record of the mechanic and

1457
01:02:58,730 --> 01:03:04,130
that's the property the data belonged to

1458
01:03:02,210 --> 01:03:07,040
the mechanic and therefore the mechanic

1459
01:03:04,130 --> 01:03:08,780
can sell it as well as me and my privacy

1460
01:03:07,040 --> 01:03:11,779
problem is that it gets aggregated and

1461
01:03:08,780 --> 01:03:13,940
attributed to me as well so there are

1462
01:03:11,780 --> 01:03:16,730
very complicated questions with trying

1463
01:03:13,940 --> 01:03:20,360
to treat this as as property even apart

1464
01:03:16,730 --> 01:03:23,720
from the international issues and

1465
01:03:20,360 --> 01:03:26,240
different philosophies there's a lot of

1466
01:03:23,720 --> 01:03:28,899
data that businesses very legitimately

1467
01:03:26,240 --> 01:03:31,490
have to collect and medical personnel

1468
01:03:28,900 --> 01:03:32,270
utterly rely on it they need this to

1469
01:03:31,490 --> 01:03:35,180
keep you healthy

1470
01:03:32,270 --> 01:03:36,950
they have to have this data and then who

1471
01:03:35,180 --> 01:03:40,009
owns it so it's an it's not an easy

1472
01:03:36,950 --> 01:03:43,939
question so which is why I like the

1473
01:03:40,010 --> 01:03:45,650
notion of use control instead I just

1474
01:03:43,940 --> 01:03:47,240
want to say a couple sentences about the

1475
01:03:45,650 --> 01:03:48,800
measurement issue that you raised I

1476
01:03:47,240 --> 01:03:50,600
agree to hundred percent that

1477
01:03:48,800 --> 01:03:53,300
measurement should be a standard part of

1478
01:03:50,600 --> 01:03:54,589
the regulatory process and just to tell

1479
01:03:53,300 --> 01:03:56,450
you how much I agree with that

1480
01:03:54,590 --> 01:03:59,390
at Princeton and part of the Center for

1481
01:03:56,450 --> 01:04:01,640
Information Technology Policy and it was

1482
01:03:59,390 --> 01:04:03,470
started 15 years ago with precisely the

1483
01:04:01,640 --> 01:04:05,960
notion that there need to be more

1484
01:04:03,470 --> 01:04:07,879
technologists in government exactly

1485
01:04:05,960 --> 01:04:09,710
because we can do more of the sort of

1486
01:04:07,880 --> 01:04:11,030
things you're calling for because today

1487
01:04:09,710 --> 01:04:12,980
the main limitation is just the

1488
01:04:11,030 --> 01:04:15,830
technical expertise that exists in

1489
01:04:12,980 --> 01:04:17,660
regulatory agencies yeah if you're a

1490
01:04:15,830 --> 01:04:18,410
technical person get involved with your

1491
01:04:17,660 --> 01:04:21,500
own government

1492
01:04:18,410 --> 01:04:24,230
make sure the lawyers judges legislators

1493
01:04:21,500 --> 01:04:26,570
regulations on understand the technology

1494
01:04:24,230 --> 01:04:31,580
I've done that I've done this twice I

1495
01:04:26,570 --> 01:04:34,910
highly recommend it and never the last

1496
01:04:31,580 --> 01:04:37,670
word Phil you have negative four minutes

1497
01:04:34,910 --> 01:04:39,049
so please be extremely brief I'm sorry

1498
01:04:37,670 --> 01:04:41,630
back Mike we cut the mic lines or I

1499
01:04:39,050 --> 01:04:44,540
would okay name withheld actually so

1500
01:04:41,630 --> 01:04:46,760
Steve said we had the wrong trust model

1501
01:04:44,540 --> 01:04:49,940
you know we got thinking beyond the CIA

1502
01:04:46,760 --> 01:04:53,420
etc attacking I think goes beyond that

1503
01:04:49,940 --> 01:04:57,020
these third-party databases they are a

1504
01:04:53,420 --> 01:04:59,660
national security threat and we saw them

1505
01:04:57,020 --> 01:05:03,560
weaponized in 2016

1506
01:04:59,660 --> 01:05:07,069
and it isn't just personal data

1507
01:05:03,560 --> 01:05:12,320
I know of an insurance company that is

1508
01:05:07,070 --> 01:05:15,890
operated by an individual who is widely

1509
01:05:12,320 --> 01:05:19,460
believed to be operating on behalf of an

1510
01:05:15,890 --> 01:05:22,220
intelligence agency a hostile one this

1511
01:05:19,460 --> 01:05:25,430
insurance agency specializes in

1512
01:05:22,220 --> 01:05:27,919
commercial vehicles if you think about

1513
01:05:25,430 --> 01:05:31,970
what such an insurance agency would be

1514
01:05:27,920 --> 01:05:35,990
doing it is collecting data on all the

1515
01:05:31,970 --> 01:05:38,450
trucks that are moving in that country

1516
01:05:35,990 --> 01:05:41,060
and they managed to get 70% of the

1517
01:05:38,450 --> 01:05:44,689
market in a markedly short time because

1518
01:05:41,060 --> 01:05:47,480
businesses are very price sensitive so

1519
01:05:44,690 --> 01:05:50,900
when we're thinking about this thing it

1520
01:05:47,480 --> 01:05:53,810
is no longer just us as individuals

1521
01:05:50,900 --> 01:05:56,750
having concerned about our personal

1522
01:05:53,810 --> 01:06:01,880
privacy it is also a matter of national

1523
01:05:56,750 --> 01:06:06,550
security and patriotism two major data

1524
01:06:01,880 --> 01:06:09,310
breaches of multinational US firms are

1525
01:06:06,550 --> 01:06:12,280
frequently thought to have been

1526
01:06:09,310 --> 01:06:16,430
perpetrated by a foreign intelligence

1527
01:06:12,280 --> 01:06:18,770
agency equi fact the Equifax and the

1528
01:06:16,430 --> 01:06:21,169
Marriott breach have been attributed to

1529
01:06:18,770 --> 01:06:24,950
foreign intelligence agencies in fact

1530
01:06:21,170 --> 01:06:27,290
the someone Fairfax said justice just

1531
01:06:24,950 --> 01:06:29,419
yesterday this is zero evidence that any

1532
01:06:27,290 --> 01:06:31,310
of the stolen data has been used

1533
01:06:29,420 --> 01:06:33,350
commercially for identity theft or

1534
01:06:31,310 --> 01:06:35,180
anything else which kind of goes along

1535
01:06:33,350 --> 01:06:37,460
pretty well with the notion that it was

1536
01:06:35,180 --> 01:06:42,379
an intelligence agency that took it so

1537
01:06:37,460 --> 01:06:43,400
yeah this is very plausible so thank you

1538
01:06:42,380 --> 01:06:45,560
very much

1539
01:06:43,400 --> 01:06:47,270
we now have a four minute break I think

1540
01:06:45,560 --> 01:06:49,670
before the administrative plenary I'd

1541
01:06:47,270 --> 01:06:52,430
like to thank again very much both Steve

1542
01:06:49,670 --> 01:06:55,490
and Arvind this is an excellent evening

1543
01:06:52,430 --> 01:06:57,319
I had I learned a lot i specially

1544
01:06:55,490 --> 01:06:58,819
appreciate the challenges to the IETF

1545
01:06:57,320 --> 01:07:00,520
from both of you and we hope to live up

1546
01:06:58,820 --> 01:07:05,860
to them so thank you very much

1547
01:07:00,520 --> 01:07:05,860
we'll see you in 180 seconds

1548
01:07:09,890 --> 01:07:12,890
yes

