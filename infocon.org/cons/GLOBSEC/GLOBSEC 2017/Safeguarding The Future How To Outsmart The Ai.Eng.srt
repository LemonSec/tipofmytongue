1
00:00:24,770 --> 00:00:28,770
to kind of frame the mood for our

2
00:00:27,660 --> 00:00:31,440
and here today on the future of

3
00:00:28,770 --> 00:00:35,030
artificial intelligence and robotics so

4
00:00:31,440 --> 00:00:35,030
we roll that video please

5
00:00:37,750 --> 00:01:33,269
[Music]

6
00:01:30,000 --> 00:01:33,269
[Applause]

7
00:01:38,670 --> 00:01:44,760
[Music]

8
00:01:55,210 --> 00:01:58,460
[Music]

9
00:01:59,670 --> 00:02:07,180
all right all right very exciting so

10
00:02:02,920 --> 00:02:09,550
again joining us to my immediate left is

11
00:02:07,180 --> 00:02:12,760
mr. Cahill rohana who is the deputy

12
00:02:09,550 --> 00:02:14,410
director in general deputy director

13
00:02:12,760 --> 00:02:16,540
general excuse me of communications

14
00:02:14,410 --> 00:02:20,079
networks and technology for the European

15
00:02:16,540 --> 00:02:22,359
Commission next to him in the center mr.

16
00:02:20,080 --> 00:02:25,980
Marek Rosa the CEO and chief technical

17
00:02:22,360 --> 00:02:30,099
officer of good AI and next to him

18
00:02:25,980 --> 00:02:34,780
filling in is andreas Evert who is the

19
00:02:30,099 --> 00:02:37,238
EU Technical Officer for Microsoft

20
00:02:34,780 --> 00:02:40,080
that's our panel here today a wonderful

21
00:02:37,239 --> 00:02:43,480
wonderful video a little bit frightening

22
00:02:40,080 --> 00:02:44,440
to kind of but that's okay that's what

23
00:02:43,480 --> 00:02:45,940
we're talking about when we talk about

24
00:02:44,440 --> 00:02:48,340
artificial intelligence it's although

25
00:02:45,940 --> 00:02:53,290
the wonder and terror of the 21st

26
00:02:48,340 --> 00:02:55,810
century so a quick note as a matter of

27
00:02:53,290 --> 00:02:57,160
framing the discussion that we're going

28
00:02:55,810 --> 00:02:58,780
to be having here today and I hope it's

29
00:02:57,160 --> 00:03:01,269
a boisterous discussion please feel free

30
00:02:58,780 --> 00:03:03,579
if you have ideas to raise your hand and

31
00:03:01,269 --> 00:03:05,319
I'll have a lot of questions and I

32
00:03:03,579 --> 00:03:07,630
envision something of a good

33
00:03:05,319 --> 00:03:12,010
free-flowing discussion but as a matter

34
00:03:07,630 --> 00:03:14,799
of framing who here is familiar with the

35
00:03:12,010 --> 00:03:16,560
English origin of the word robot raise

36
00:03:14,799 --> 00:03:21,760
your hand

37
00:03:16,560 --> 00:03:24,910
are you check yes it's a it's a Czech

38
00:03:21,760 --> 00:03:27,090
time it's originally a play by wonderful

39
00:03:24,910 --> 00:03:31,569
Czech playwright named Karl Capek

40
00:03:27,090 --> 00:03:33,760
premiered in 1921 and it's the first

41
00:03:31,569 --> 00:03:36,850
English use of the word robot the play

42
00:03:33,760 --> 00:03:38,920
is called Rossum's Universal robots and

43
00:03:36,850 --> 00:03:41,230
what's fascinating about it is that the

44
00:03:38,920 --> 00:03:43,149
play actually progresses exactly as

45
00:03:41,230 --> 00:03:45,429
almost every narrative about artificial

46
00:03:43,150 --> 00:03:51,670
intelligence that we've ever seen so by

47
00:03:45,430 --> 00:03:53,230
the end of act 1 robots have a created

48
00:03:51,670 --> 00:03:55,720
enormous wealth for their original

49
00:03:53,230 --> 00:03:58,090
creator who is an inventor named Rossum

50
00:03:55,720 --> 00:04:01,299
a young entrepreneur sort of in the mold

51
00:03:58,090 --> 00:04:02,620
of Elon Musk so they've created fabulous

52
00:04:01,299 --> 00:04:05,169
wealth they've reshaped the global

53
00:04:02,620 --> 00:04:07,889
economy by the end of act 2 they've

54
00:04:05,169 --> 00:04:10,690
caused massive economic disruption and

55
00:04:07,889 --> 00:04:12,610
bankruptcy and by the end of act 3

56
00:04:10,690 --> 00:04:15,790
they've revolted against humanity and

57
00:04:12,610 --> 00:04:17,230
and caused the apocalypse so I bring

58
00:04:15,790 --> 00:04:18,760
this up because I think it's it's a

59
00:04:17,230 --> 00:04:24,010
useful way to think about this problem

60
00:04:18,760 --> 00:04:26,440
we invented a fear of robots right after

61
00:04:24,010 --> 00:04:28,900
inventing the idea of robots and

62
00:04:26,440 --> 00:04:32,110
artificial intelligence is really the

63
00:04:28,900 --> 00:04:36,310
expansion of this same theme this idea

64
00:04:32,110 --> 00:04:38,470
of service mechanized displacing first

65
00:04:36,310 --> 00:04:42,460
human workers and then humans because we

66
00:04:38,470 --> 00:04:45,400
can't control what we created so as

67
00:04:42,460 --> 00:04:47,049
we've seen this is a idea that continues

68
00:04:45,400 --> 00:04:49,450
to capture the attention the imagination

69
00:04:47,050 --> 00:04:51,310
of many captains of industry many of our

70
00:04:49,450 --> 00:04:52,719
great thinkers including in the case of

71
00:04:51,310 --> 00:04:54,220
Elon Musk someone who's both heavily

72
00:04:52,720 --> 00:04:58,000
invested in the future of artificial

73
00:04:54,220 --> 00:05:00,700
intelligence and also has a lot to say

74
00:04:58,000 --> 00:05:03,930
about its potential downsides so let me

75
00:05:00,700 --> 00:05:07,960
begin with you Merrick because you are

76
00:05:03,930 --> 00:05:11,260
the head of startup that seeks to create

77
00:05:07,960 --> 00:05:13,330
something called general AI so I'd like

78
00:05:11,260 --> 00:05:14,770
you to tell us a little bit about why on

79
00:05:13,330 --> 00:05:19,659
earth you would try to make such a

80
00:05:14,770 --> 00:05:22,270
terrifying thing and what that is first

81
00:05:19,660 --> 00:05:23,920
let me talk about what is general AI and

82
00:05:22,270 --> 00:05:25,840
how we differs from narrow AI so

83
00:05:23,920 --> 00:05:28,240
basically narrow AI is what we have

84
00:05:25,840 --> 00:05:30,609
today it's usually some AI technology

85
00:05:28,240 --> 00:05:33,880
used to solve problems in very specific

86
00:05:30,610 --> 00:05:36,250
and vertical domains and it requires

87
00:05:33,880 --> 00:05:38,590
researchers and engineers to really

88
00:05:36,250 --> 00:05:40,660
fine-tune and design this AI algorithm

89
00:05:38,590 --> 00:05:44,229
for the particular problem on the other

90
00:05:40,660 --> 00:05:44,830
side general AI will bring us because we

91
00:05:44,230 --> 00:05:47,050
don't have it yet

92
00:05:44,830 --> 00:05:49,330
will bring us the possibility to have an

93
00:05:47,050 --> 00:05:51,600
AI system that can learn in a similar

94
00:05:49,330 --> 00:05:54,159
way as as people can learn gradually

95
00:05:51,600 --> 00:05:56,170
accumulate skills one after another use

96
00:05:54,160 --> 00:05:58,830
learned skills to learn new skills more

97
00:05:56,170 --> 00:06:02,290
efficiently learn how to learn and

98
00:05:58,830 --> 00:06:05,620
therefore making the next learning more

99
00:06:02,290 --> 00:06:07,900
efficient and the reason why I am and

100
00:06:05,620 --> 00:06:10,320
also my colleagues are why we are

101
00:06:07,900 --> 00:06:13,690
interested in this is basically that

102
00:06:10,320 --> 00:06:16,120
automating the intelligence we see this

103
00:06:13,690 --> 00:06:18,580
as a best way how to solve everything

104
00:06:16,120 --> 00:06:20,830
else you know so instead of me working

105
00:06:18,580 --> 00:06:21,830
on like hundred different things that I

106
00:06:20,830 --> 00:06:24,469
would like to work

107
00:06:21,830 --> 00:06:27,830
I would like to first create a general

108
00:06:24,470 --> 00:06:30,040
AI and then use it to solve all the

109
00:06:27,830 --> 00:06:34,520
other things so it's like meta solution

110
00:06:30,040 --> 00:06:36,800
okay so this is an important distinction

111
00:06:34,520 --> 00:06:37,909
because we are surrounded by artificial

112
00:06:36,800 --> 00:06:39,350
intelligence all the time we just don't

113
00:06:37,910 --> 00:06:42,170
perceive it as such but it's definitely

114
00:06:39,350 --> 00:06:44,540
guiding aspects of traffic it certainly

115
00:06:42,170 --> 00:06:46,880
guides a lot of transactions a lot of

116
00:06:44,540 --> 00:06:49,940
economic logistics factory logistics

117
00:06:46,880 --> 00:06:52,460
within the military space increasing

118
00:06:49,940 --> 00:06:55,640
number of different operations have some

119
00:06:52,460 --> 00:06:57,289
narrow AI component and general AI is

120
00:06:55,640 --> 00:07:00,770
kind of the dream this is where we

121
00:06:57,290 --> 00:07:03,110
create a robust intelligence that has

122
00:07:00,770 --> 00:07:04,549
the capability that we associate with

123
00:07:03,110 --> 00:07:07,280
human intelligence but that doesn't

124
00:07:04,550 --> 00:07:09,110
necessarily mean that it resembles or

125
00:07:07,280 --> 00:07:11,239
looks exactly like human intelligence is

126
00:07:09,110 --> 00:07:12,230
just sort of a parallel intelligence to

127
00:07:11,240 --> 00:07:15,350
our own a thing that we've never

128
00:07:12,230 --> 00:07:18,350
actually created but but dream to so I'd

129
00:07:15,350 --> 00:07:21,620
like to turn to you Cahill in thinking

130
00:07:18,350 --> 00:07:24,530
about all of this as somebody from a

131
00:07:21,620 --> 00:07:26,600
policy perspective it's got to look like

132
00:07:24,530 --> 00:07:31,099
a lot of downside and unpredictability

133
00:07:26,600 --> 00:07:33,350
what do you see as the potential in the

134
00:07:31,100 --> 00:07:34,850
next five to ten years the great

135
00:07:33,350 --> 00:07:37,010
potential upside of artificial

136
00:07:34,850 --> 00:07:40,910
intelligence and what is the great fear

137
00:07:37,010 --> 00:07:42,469
or danger to avoid or mitigate yeah

138
00:07:40,910 --> 00:07:46,610
Thank You Patrick and good morning

139
00:07:42,470 --> 00:07:49,490
everyone so I think artificial

140
00:07:46,610 --> 00:07:51,860
intelligence is not new and the way we

141
00:07:49,490 --> 00:07:54,220
have had I'll say peeks of hype around

142
00:07:51,860 --> 00:07:57,200
artificial intelligence in the past

143
00:07:54,220 --> 00:07:58,820
starting and the 70s than the 80s and

144
00:07:57,200 --> 00:08:02,180
those of you familiar with the area

145
00:07:58,820 --> 00:08:04,659
would have remembered you know these big

146
00:08:02,180 --> 00:08:07,520
hopes around artificial intelligence

147
00:08:04,660 --> 00:08:10,400
that I would describe an enduring

148
00:08:07,520 --> 00:08:14,409
artifacts with more capacity of autonomy

149
00:08:10,400 --> 00:08:17,179
so as Mark explained learning and

150
00:08:14,410 --> 00:08:21,500
gaining skills and be able to act based

151
00:08:17,180 --> 00:08:23,930
on the skills so the what is happening

152
00:08:21,500 --> 00:08:27,020
now is that I think the technology

153
00:08:23,930 --> 00:08:28,820
mainly computing power and the large

154
00:08:27,020 --> 00:08:33,710
amounts of data that we can gather and

155
00:08:28,820 --> 00:08:34,919
compute and store gives that I'll say

156
00:08:33,710 --> 00:08:36,479
that Trent oven though

157
00:08:34,919 --> 00:08:38,699
artifacts with more autonomy more

158
00:08:36,479 --> 00:08:42,750
intelligence gives it a new impulse and

159
00:08:38,700 --> 00:08:45,329
we see it happening indeed and as Mark

160
00:08:42,750 --> 00:08:47,399
said it is indeed developing on specific

161
00:08:45,329 --> 00:08:51,149
applications are two big ones that we

162
00:08:47,399 --> 00:08:53,490
see today I mean if I simplify the what

163
00:08:51,149 --> 00:08:56,180
we call the embedded intelligence or

164
00:08:53,490 --> 00:08:59,130
that's the intelligence that we put an

165
00:08:56,180 --> 00:09:03,349
articulated physical objects like robots

166
00:08:59,130 --> 00:09:05,880
and that is developing with specific

167
00:09:03,350 --> 00:09:08,699
solutions to for robots to gain more

168
00:09:05,880 --> 00:09:11,579
autonomy and the other area that is

169
00:09:08,699 --> 00:09:14,699
developing also is in the area of big

170
00:09:11,579 --> 00:09:17,370
data and harnessing a knowledge from

171
00:09:14,699 --> 00:09:19,079
data that would hable us to improve our

172
00:09:17,370 --> 00:09:22,139
decision support systems or search

173
00:09:19,079 --> 00:09:24,420
engines etc so it's still the and that's

174
00:09:22,139 --> 00:09:25,920
the other that area has been developing

175
00:09:24,420 --> 00:09:28,529
because of the development of deep

176
00:09:25,920 --> 00:09:32,969
learning and that's as I said based on

177
00:09:28,529 --> 00:09:35,730
the capacity to handle data and compute

178
00:09:32,970 --> 00:09:39,240
with more power and in these two areas I

179
00:09:35,730 --> 00:09:42,410
think the the developments are you know

180
00:09:39,240 --> 00:09:44,730
there are some conversions and basic

181
00:09:42,410 --> 00:09:47,310
technologies developing for both of them

182
00:09:44,730 --> 00:09:50,430
could lead us the more generic AI in the

183
00:09:47,310 --> 00:09:52,529
future indeed and for both of them there

184
00:09:50,430 --> 00:09:54,479
are opportunities and threats like and I

185
00:09:52,529 --> 00:09:56,399
think it's like every technology you

186
00:09:54,480 --> 00:09:58,529
know they even forgot electricity in the

187
00:09:56,399 --> 00:10:00,930
19th century people would have thought

188
00:09:58,529 --> 00:10:03,449
that it's going to be very dangerous

189
00:10:00,930 --> 00:10:06,060
etc and I'm sure there were articles not

190
00:10:03,449 --> 00:10:07,649
probably as diffused esterday that

191
00:10:06,060 --> 00:10:10,529
described the technology with a lot of

192
00:10:07,649 --> 00:10:13,019
fear and indeed those fears are always

193
00:10:10,529 --> 00:10:15,120
justified because if you don't if you're

194
00:10:13,019 --> 00:10:16,649
not able to master the technology if

195
00:10:15,120 --> 00:10:18,890
you're not able to put around it all the

196
00:10:16,649 --> 00:10:23,069
framework that would make it safe and

197
00:10:18,890 --> 00:10:25,560
used properly by citizens and businesses

198
00:10:23,069 --> 00:10:29,790
it can create big dangers and threats

199
00:10:25,560 --> 00:10:32,279
but I I think I have an optimistic view

200
00:10:29,790 --> 00:10:34,500
on the way things would develop for both

201
00:10:32,279 --> 00:10:36,350
areas I think the opportunities are here

202
00:10:34,500 --> 00:10:39,209
which we're more in town you know

203
00:10:36,350 --> 00:10:40,860
intelligence and smartness and so the

204
00:10:39,209 --> 00:10:42,750
artifacts the more things they can do

205
00:10:40,860 --> 00:10:44,850
for you you can improve the safety of

206
00:10:42,750 --> 00:10:46,430
the products you can and the services

207
00:10:44,850 --> 00:10:48,170
that are behind them you can bring that

208
00:10:46,430 --> 00:10:51,279
Comfort you can bring additional

209
00:10:48,170 --> 00:10:53,810
resource efficiency additional security

210
00:10:51,279 --> 00:10:55,850
from you know both of these big

211
00:10:53,810 --> 00:10:58,790
application areas that are developing we

212
00:10:55,850 --> 00:11:02,330
see it these are huge opportunities for

213
00:10:58,790 --> 00:11:04,099
new business development and for new

214
00:11:02,330 --> 00:11:05,570
ways to address our big societal

215
00:11:04,100 --> 00:11:06,950
challenges and we see the big you know

216
00:11:05,570 --> 00:11:09,649
breakthroughs will come in the health

217
00:11:06,950 --> 00:11:11,510
area today one of the biggest area for

218
00:11:09,649 --> 00:11:13,040
development of robots is robotics for

219
00:11:11,510 --> 00:11:14,990
health and we could see what it can

220
00:11:13,040 --> 00:11:18,079
bring mm-hmm but there are the fears of

221
00:11:14,990 --> 00:11:20,270
our citizens and we have a euro

222
00:11:18,080 --> 00:11:22,490
barometer that that we have every year

223
00:11:20,270 --> 00:11:24,800
on the different technology including

224
00:11:22,490 --> 00:11:26,810
robotics at the EU level I will show

225
00:11:24,800 --> 00:11:29,420
that most of our citizens 70% think

226
00:11:26,810 --> 00:11:31,160
robots are good thing for example yeah

227
00:11:29,420 --> 00:11:33,709
but we tell them will you like the robot

228
00:11:31,160 --> 00:11:36,170
to be your your doctor they say no no no

229
00:11:33,709 --> 00:11:37,819
so I think they think it will bring a

230
00:11:36,170 --> 00:11:39,439
lot but there are the fears that this

231
00:11:37,820 --> 00:11:41,630
machine will do something for us that

232
00:11:39,440 --> 00:11:43,610
weed can throw yeah so huge

233
00:11:41,630 --> 00:11:45,920
opportunities new ways of addressing

234
00:11:43,610 --> 00:11:47,600
societal challenges which I think some

235
00:11:45,920 --> 00:11:49,729
of the economic analysis do not grasp

236
00:11:47,600 --> 00:11:52,220
because you know technology opens up new

237
00:11:49,730 --> 00:11:55,370
doors for new business development for

238
00:11:52,220 --> 00:11:58,550
new ways of addressing cyber challenges

239
00:11:55,370 --> 00:12:00,260
enhance new jobs etc the the

240
00:11:58,550 --> 00:12:01,609
opportunities are then I see the

241
00:12:00,260 --> 00:12:03,890
opportunities also on the deep learning

242
00:12:01,610 --> 00:12:05,870
and all the generic AI that could

243
00:12:03,890 --> 00:12:10,420
develop after new applications new

244
00:12:05,870 --> 00:12:12,830
services and the threats are yeah

245
00:12:10,420 --> 00:12:17,150
threats of safety threats on all

246
00:12:12,830 --> 00:12:20,720
security threats on jobs threats on the

247
00:12:17,150 --> 00:12:24,439
way our lives could change in the future

248
00:12:20,720 --> 00:12:26,900
that and that the different levels of

249
00:12:24,440 --> 00:12:28,400
automation show already that these

250
00:12:26,900 --> 00:12:32,360
threats are there that we need to

251
00:12:28,400 --> 00:12:34,370
address I can go through all the

252
00:12:32,360 --> 00:12:37,310
solutions what you can do but what we

253
00:12:34,370 --> 00:12:40,880
can do but what I think is that for the

254
00:12:37,310 --> 00:12:43,010
five to ten years to conclude we can see

255
00:12:40,880 --> 00:12:44,510
a lot of new opportunities coming from

256
00:12:43,010 --> 00:12:46,760
increased automation or increased

257
00:12:44,510 --> 00:12:48,950
autonomy that you can bring into all

258
00:12:46,760 --> 00:12:53,839
types of artifacts and increased

259
00:12:48,950 --> 00:12:54,850
smartness that you could put and our web

260
00:12:53,839 --> 00:12:59,290
and

261
00:12:54,850 --> 00:13:01,180
on the internet and channel and the 5-10

262
00:12:59,290 --> 00:13:02,980
years I don't think it is at the level

263
00:13:01,180 --> 00:13:04,510
of the hive that we see in the movies

264
00:13:02,980 --> 00:13:07,540
yet I mean if we look at the

265
00:13:04,510 --> 00:13:09,519
technologies we run in the Commission

266
00:13:07,540 --> 00:13:11,769
one of the largest program worldwide on

267
00:13:09,519 --> 00:13:14,440
robotics and I could see what robots can

268
00:13:11,769 --> 00:13:17,230
do at the latest development a very

269
00:13:14,440 --> 00:13:19,930
difficult to grasp you know manipulation

270
00:13:17,230 --> 00:13:22,180
is still very rudimentary so when I see

271
00:13:19,930 --> 00:13:24,339
some of the movies I say well I mean

272
00:13:22,180 --> 00:13:26,920
they it's far from reality yeah it's

273
00:13:24,339 --> 00:13:29,440
really far from reality so 5-10 years

274
00:13:26,920 --> 00:13:31,510
there be creased automation increased

275
00:13:29,440 --> 00:13:33,190
autonomy that will bring into our cars

276
00:13:31,510 --> 00:13:35,410
with the autonomous vehicles of course

277
00:13:33,190 --> 00:13:39,490
that will bring into all these machines

278
00:13:35,410 --> 00:13:42,880
and I think the the the the real

279
00:13:39,490 --> 00:13:44,589
challenge for policymaking is to strike

280
00:13:42,880 --> 00:13:48,790
the right balance and not to start

281
00:13:44,589 --> 00:13:54,430
regulating too early on the threats like

282
00:13:48,790 --> 00:13:57,130
liability like safety security all the

283
00:13:54,430 --> 00:14:01,870
ethics related to that not to regulate

284
00:13:57,130 --> 00:14:03,760
too too early to make sure that we let

285
00:14:01,870 --> 00:14:06,430
the innovation develop but at the same

286
00:14:03,760 --> 00:14:08,529
time remain vigilant in terms of

287
00:14:06,430 --> 00:14:11,439
developing the skills of our workers

288
00:14:08,529 --> 00:14:13,660
because this is a transformation will

289
00:14:11,440 --> 00:14:16,120
attend including with artificial

290
00:14:13,660 --> 00:14:19,800
intelligence will will affect the jobs

291
00:14:16,120 --> 00:14:24,300
of the future make sure that we keep our

292
00:14:19,800 --> 00:14:27,699
workforce up-to-date on skills and

293
00:14:24,300 --> 00:14:29,529
educate our next generation on what

294
00:14:27,699 --> 00:14:31,240
these technologies and what they can do

295
00:14:29,529 --> 00:14:33,939
so we need to remain vigilant on the

296
00:14:31,240 --> 00:14:37,449
regulatory part not to regulate too fast

297
00:14:33,940 --> 00:14:39,850
and at the same time make sure that the

298
00:14:37,449 --> 00:14:42,219
opportunities are available for our

299
00:14:39,850 --> 00:14:44,290
businesses and our citizens right so I

300
00:14:42,220 --> 00:14:46,170
want it I want to get to invest in a

301
00:14:44,290 --> 00:14:48,370
second to talk a little bit about where

302
00:14:46,170 --> 00:14:50,469
Microsoft and some of your peer

303
00:14:48,370 --> 00:14:53,110
companies see the potential and the

304
00:14:50,470 --> 00:14:54,699
peril from an investment standpoint from

305
00:14:53,110 --> 00:14:56,709
a company standpoint in artificial

306
00:14:54,699 --> 00:14:57,939
intelligence in the next 10 years well

307
00:14:56,709 --> 00:15:00,760
thank you so much for having me here

308
00:14:57,940 --> 00:15:03,010
anyway and before I do this one I had a

309
00:15:00,760 --> 00:15:04,480
kind of flashback another flashback but

310
00:15:03,010 --> 00:15:06,220
the kind of surprising thing when I saw

311
00:15:04,480 --> 00:15:07,900
the title of this session how to

312
00:15:06,220 --> 00:15:10,510
outsmart the AI

313
00:15:07,900 --> 00:15:12,160
it triggered things like a hundred years

314
00:15:10,510 --> 00:15:14,770
ago man we talked about innovation of

315
00:15:12,160 --> 00:15:18,010
called car how to run faster than a car

316
00:15:14,770 --> 00:15:20,230
or to lift more than a forklift kind of

317
00:15:18,010 --> 00:15:22,390
stuff so I do think that the title

318
00:15:20,230 --> 00:15:25,090
suggests that we should be better than

319
00:15:22,390 --> 00:15:26,980
AI in things AI could do I would rather

320
00:15:25,090 --> 00:15:29,800
flip the argument around and would say

321
00:15:26,980 --> 00:15:31,510
how can we develop intelligent

322
00:15:29,800 --> 00:15:34,000
technologies to support and enhance

323
00:15:31,510 --> 00:15:35,680
human ingenuity anyway which is the

324
00:15:34,000 --> 00:15:38,290
positive the gods have full kind of

325
00:15:35,680 --> 00:15:39,849
perspective on this one so kind of let's

326
00:15:38,290 --> 00:15:41,680
say initial observation and I saw the

327
00:15:39,850 --> 00:15:43,540
title of our session anyways I do think

328
00:15:41,680 --> 00:15:45,640
there's a huge opportunity agreeing I

329
00:15:43,540 --> 00:15:48,430
think it's both panelists of the here as

330
00:15:45,640 --> 00:15:50,410
well the fundament I want to just

331
00:15:48,430 --> 00:15:51,939
reiterate how fundamental the kind of

332
00:15:50,410 --> 00:15:55,420
transformation is for the last 50 years

333
00:15:51,940 --> 00:15:58,300
we spent as industry a society most of

334
00:15:55,420 --> 00:15:59,560
the time to understand computers and

335
00:15:58,300 --> 00:16:03,640
what's happening the last couple of

336
00:15:59,560 --> 00:16:06,040
years and Carly mentioned that AI had

337
00:16:03,640 --> 00:16:09,130
been around since the 56s in the famous

338
00:16:06,040 --> 00:16:11,319
Dallas Mouse conference 56 was the the

339
00:16:09,130 --> 00:16:12,880
kickstart officially of this one but

340
00:16:11,320 --> 00:16:16,120
it's only the last couple of years that

341
00:16:12,880 --> 00:16:18,070
a couple of factors came together leap

342
00:16:16,120 --> 00:16:20,080
frogging men of the kind of developments

343
00:16:18,070 --> 00:16:23,350
and machine learning being one part of

344
00:16:20,080 --> 00:16:26,410
AI only really kick started seven years

345
00:16:23,350 --> 00:16:28,570
ago and in seven years everybody else's

346
00:16:26,410 --> 00:16:31,420
experience let say this kind of

347
00:16:28,570 --> 00:16:33,670
enhancing your own kind of let's say

348
00:16:31,420 --> 00:16:36,400
experience with technology bead routing

349
00:16:33,670 --> 00:16:38,620
on map speed translation of services

350
00:16:36,400 --> 00:16:41,140
voice interaction missile devices which

351
00:16:38,620 --> 00:16:43,720
wouldn't be possible with many forms of

352
00:16:41,140 --> 00:16:45,819
narrow AI and fully agree that the over

353
00:16:43,720 --> 00:16:48,520
objective long term might be more the

354
00:16:45,820 --> 00:16:51,400
generic learning capability but it is

355
00:16:48,520 --> 00:16:53,890
really very very short from a societal

356
00:16:51,400 --> 00:16:56,920
perspective the technology really spread

357
00:16:53,890 --> 00:17:00,640
out to everybody's individual lives in

358
00:16:56,920 --> 00:17:02,349
this way so our approach and the

359
00:17:00,640 --> 00:17:04,540
approach we've taken for the last couple

360
00:17:02,350 --> 00:17:05,920
of years around a is that we wanna make

361
00:17:04,540 --> 00:17:07,990
this the kind of official title sorry

362
00:17:05,920 --> 00:17:10,089
for using a kind of branding name on

363
00:17:07,990 --> 00:17:12,459
this one is democratizing AI because we

364
00:17:10,089 --> 00:17:14,889
do think that those kind of technologies

365
00:17:12,459 --> 00:17:17,320
enhancing capabilities of existing

366
00:17:14,890 --> 00:17:19,510
system enhancing systems that you can

367
00:17:17,319 --> 00:17:21,069
use instead of instance imagine if

368
00:17:19,510 --> 00:17:23,920
you're from a condo

369
00:17:21,069 --> 00:17:26,050
hopefully not but from from a human

370
00:17:23,920 --> 00:17:28,960
perspective not able to use a keyboard

371
00:17:26,050 --> 00:17:31,629
or a or a pen that you can do voice

372
00:17:28,960 --> 00:17:34,120
interaction AI is a technology helping

373
00:17:31,630 --> 00:17:36,310
those people to get deeper included in

374
00:17:34,120 --> 00:17:38,530
society's activities in a way it's

375
00:17:36,310 --> 00:17:40,870
something which we're really working day

376
00:17:38,530 --> 00:17:42,430
over day over day that we reduce the

377
00:17:40,870 --> 00:17:44,350
threshold that those kind of

378
00:17:42,430 --> 00:17:47,800
technologies can be part of our

379
00:17:44,350 --> 00:17:49,689
interaction with technology and what we

380
00:17:47,800 --> 00:17:51,790
also want to do is that it shouldn't be

381
00:17:49,690 --> 00:17:54,160
in the hands of a few companies in a way

382
00:17:51,790 --> 00:17:56,139
and I would guess that currently only a

383
00:17:54,160 --> 00:18:00,850
handful of companies have the majority

384
00:17:56,140 --> 00:18:02,800
of of AI resources paper skills like

385
00:18:00,850 --> 00:18:04,899
that and we want to contribute that this

386
00:18:02,800 --> 00:18:07,629
becomes a much broader kind of ecosystem

387
00:18:04,900 --> 00:18:10,930
thing that millions of developers on the

388
00:18:07,630 --> 00:18:13,240
planet can reduce their effort to

389
00:18:10,930 --> 00:18:15,130
participate in developing innovative

390
00:18:13,240 --> 00:18:16,900
solutions helping addressing some of the

391
00:18:15,130 --> 00:18:18,850
societal kind of things sometimes

392
00:18:16,900 --> 00:18:21,400
creating new business models sometimes

393
00:18:18,850 --> 00:18:23,379
new just products in a way and so we're

394
00:18:21,400 --> 00:18:25,180
trying to contribute on this one and the

395
00:18:23,380 --> 00:18:28,720
last thing in this kind of four step

396
00:18:25,180 --> 00:18:30,880
pill strategy around democratizing be

397
00:18:28,720 --> 00:18:33,370
invest heavily in our own infrastructure

398
00:18:30,880 --> 00:18:35,530
which is called the cloud in a way that

399
00:18:33,370 --> 00:18:39,280
we create and and offer what's called an

400
00:18:35,530 --> 00:18:41,350
AI supercomputer again creating much

401
00:18:39,280 --> 00:18:46,930
much more performance necessary and a is

402
00:18:41,350 --> 00:18:48,639
usually interdependent on hive let's say

403
00:18:46,930 --> 00:18:50,470
high-performance computing resource on

404
00:18:48,640 --> 00:18:52,120
this one that is normally not only

405
00:18:50,470 --> 00:18:54,040
available for instance to us and a few

406
00:18:52,120 --> 00:18:55,989
other companies but it's available to

407
00:18:54,040 --> 00:18:57,700
millions of developers and hundreds of

408
00:18:55,990 --> 00:19:00,370
millions of people's to much more

409
00:18:57,700 --> 00:19:02,140
economic kind of realistic level okay

410
00:19:00,370 --> 00:19:03,580
wonderful so I want to remind everybody

411
00:19:02,140 --> 00:19:05,530
please feel free to participate in this

412
00:19:03,580 --> 00:19:06,639
conversation raise your hands if you

413
00:19:05,530 --> 00:19:08,260
have a question and we're going to get a

414
00:19:06,640 --> 00:19:11,800
microphone to you when I of course have

415
00:19:08,260 --> 00:19:14,470
plenty of my own in hearing you talk

416
00:19:11,800 --> 00:19:16,270
about the AI winter for instance scale

417
00:19:14,470 --> 00:19:18,040
this is a period in the 1980s where

418
00:19:16,270 --> 00:19:19,750
there was a lot of enthusiasm about the

419
00:19:18,040 --> 00:19:22,570
future of AI and it all kind of fell

420
00:19:19,750 --> 00:19:26,260
apart because of a couple of reasons but

421
00:19:22,570 --> 00:19:27,790
robotic vision and and the creation of

422
00:19:26,260 --> 00:19:30,100
artificial intelligence that can

423
00:19:27,790 --> 00:19:33,159
manipulate the world around it

424
00:19:30,100 --> 00:19:34,269
is has been a key area of enthusiasm and

425
00:19:33,160 --> 00:19:36,729
disappointment I remember

426
00:19:34,269 --> 00:19:38,679
1966 Seymour Peppard suggested that

427
00:19:36,729 --> 00:19:40,149
robotic vision teaching robots to

428
00:19:38,679 --> 00:19:42,129
understand the three-dimensional

429
00:19:40,149 --> 00:19:44,468
environment he thought this was going to

430
00:19:42,129 --> 00:19:46,629
be a summer program for masters students

431
00:19:44,469 --> 00:19:48,879
and we still haven't really mastered it

432
00:19:46,629 --> 00:19:51,570
but there has been a shift there's been

433
00:19:48,879 --> 00:19:56,168
a real shift in part because of big data

434
00:19:51,570 --> 00:19:58,689
and the ability to teach systems on the

435
00:19:56,169 --> 00:20:02,440
basis of an enormous amount of evidence

436
00:19:58,690 --> 00:20:05,019
how to do something better than was

437
00:20:02,440 --> 00:20:07,629
previously thought possible and also

438
00:20:05,019 --> 00:20:11,289
just because of the exponential race and

439
00:20:07,629 --> 00:20:15,149
and factors affecting miniaturization of

440
00:20:11,289 --> 00:20:18,609
computation the ability to squeeze more

441
00:20:15,149 --> 00:20:24,639
integrated circuits together this is you

442
00:20:18,609 --> 00:20:27,579
know Moore's law so I wonder if thinking

443
00:20:24,639 --> 00:20:31,570
about this you think of this statistic

444
00:20:27,579 --> 00:20:32,259
we've heard this is a recent statistic

445
00:20:31,570 --> 00:20:34,119
from Accenture

446
00:20:32,259 --> 00:20:37,690
predicts that AI is going to double

447
00:20:34,119 --> 00:20:41,549
economic growth in about 15 years at the

448
00:20:37,690 --> 00:20:43,959
same time PwC has projected that

449
00:20:41,549 --> 00:20:48,879
artificial intelligence and particularly

450
00:20:43,959 --> 00:20:50,589
automation could result in 40% of job

451
00:20:48,879 --> 00:20:53,079
losses in the United States so we could

452
00:20:50,589 --> 00:20:54,549
they would we would lose 40% of the jobs

453
00:20:53,079 --> 00:20:57,219
in the United States as a result of

454
00:20:54,549 --> 00:21:00,279
factors relating to automation so how do

455
00:20:57,219 --> 00:21:02,139
you double economic output and also lose

456
00:21:00,279 --> 00:21:05,559
about half your jobs and deal with that

457
00:21:02,139 --> 00:21:07,988
those two futures that to me seem to be

458
00:21:05,559 --> 00:21:10,529
competitive from a policy perspective

459
00:21:07,989 --> 00:21:13,419
how do you make a decision around that

460
00:21:10,529 --> 00:21:15,849
absolutely I think it's the one of the

461
00:21:13,419 --> 00:21:17,919
most challenges challenges one of the

462
00:21:15,849 --> 00:21:20,229
biggest challenge that at least Western

463
00:21:17,919 --> 00:21:23,289
economies are facing today with the

464
00:21:20,229 --> 00:21:25,809
development of technologies and I'm sure

465
00:21:23,289 --> 00:21:28,709
the rest of the world will we'll also go

466
00:21:25,809 --> 00:21:31,479
into this and it's the challenge of

467
00:21:28,709 --> 00:21:33,159
making sure that the effect of

468
00:21:31,479 --> 00:21:35,169
technology is widely spread across the

469
00:21:33,159 --> 00:21:38,619
economy it could increase your

470
00:21:35,169 --> 00:21:41,529
productivity you can crease your GDP you

471
00:21:38,619 --> 00:21:45,699
can have growth but at the same time if

472
00:21:41,529 --> 00:21:48,790
ultimately you're the trend of

473
00:21:45,700 --> 00:21:54,670
I mean the the figures that are related

474
00:21:48,790 --> 00:22:00,899
to the wealth creation across the

475
00:21:54,670 --> 00:22:04,900
economy are not following yo you you're

476
00:22:00,900 --> 00:22:07,030
creating large inequalities and we have

477
00:22:04,900 --> 00:22:10,810
disruptions as we've seen recently

478
00:22:07,030 --> 00:22:15,010
across so you know a lot of our member

479
00:22:10,810 --> 00:22:17,409
states that also around the world mainly

480
00:22:15,010 --> 00:22:19,690
in Western economies so far so I think

481
00:22:17,410 --> 00:22:22,180
the question is not only AI it's the

482
00:22:19,690 --> 00:22:24,820
impact of technology and digital

483
00:22:22,180 --> 00:22:27,430
technologies and our economy and society

484
00:22:24,820 --> 00:22:29,620
and distribution of wealth yeah that's

485
00:22:27,430 --> 00:22:32,530
the issue that we're facing I think the

486
00:22:29,620 --> 00:22:34,620
the solution is probably what Andreas

487
00:22:32,530 --> 00:22:38,080
started saying is making sure that

488
00:22:34,620 --> 00:22:40,510
digital technologies and automation and

489
00:22:38,080 --> 00:22:46,090
artificial tanagers are widely spread so

490
00:22:40,510 --> 00:22:48,550
that every business every company in any

491
00:22:46,090 --> 00:22:50,770
sector the economy can make benefits out

492
00:22:48,550 --> 00:22:53,320
of it to innovate in its products

493
00:22:50,770 --> 00:22:55,480
innovate in its product is to improve

494
00:22:53,320 --> 00:22:58,810
its processes to innovate in its

495
00:22:55,480 --> 00:23:01,990
business models and to make sure that

496
00:22:58,810 --> 00:23:04,149
that but these benefits are spread all

497
00:23:01,990 --> 00:23:06,370
across the economy and society and for

498
00:23:04,150 --> 00:23:10,120
Europe this is EV one of a main target

499
00:23:06,370 --> 00:23:13,090
okay ah so they just to say that that

500
00:23:10,120 --> 00:23:17,250
that's I think one way to address that

501
00:23:13,090 --> 00:23:20,169
issue it's not only AI automation is

502
00:23:17,250 --> 00:23:23,890
part of it so all levels of automation

503
00:23:20,170 --> 00:23:27,040
will Inc will bring in a transformation

504
00:23:23,890 --> 00:23:29,590
to our jobs transformation to our

505
00:23:27,040 --> 00:23:31,690
businesses etc making sure that first

506
00:23:29,590 --> 00:23:34,629
second I think was importance that make

507
00:23:31,690 --> 00:23:37,540
sure that our citizens have the

508
00:23:34,630 --> 00:23:39,580
necessary education the necessary skills

509
00:23:37,540 --> 00:23:42,610
to be able to cope with digital

510
00:23:39,580 --> 00:23:46,840
transformation that's a very important

511
00:23:42,610 --> 00:23:48,978
challenge for all economies today and I

512
00:23:46,840 --> 00:23:54,359
think it's

513
00:23:48,979 --> 00:23:58,259
the the it is not taken at the right

514
00:23:54,359 --> 00:23:59,639
level yet given what is at stake and

515
00:23:58,259 --> 00:24:01,109
that is a policymaking that's what I

516
00:23:59,639 --> 00:24:02,580
want to nail down on it's not given at

517
00:24:01,109 --> 00:24:04,978
the right level yet when you talk to

518
00:24:02,580 --> 00:24:06,239
policy makers across Europe and and

519
00:24:04,979 --> 00:24:08,749
around the world that you talk to do you

520
00:24:06,239 --> 00:24:11,789
think that they have an appreciation for

521
00:24:08,749 --> 00:24:16,169
how very economically or labored

522
00:24:11,789 --> 00:24:17,908
disruptive automation and AI could be in

523
00:24:16,169 --> 00:24:20,459
the next 10 years or are they sort of

524
00:24:17,909 --> 00:24:21,899
like our own Treasury secretary in the

525
00:24:20,459 --> 00:24:23,190
United States Steve Munchen who said

526
00:24:21,899 --> 00:24:25,408
that he's not particularly worried about

527
00:24:23,190 --> 00:24:27,239
job displacement as a result of

528
00:24:25,409 --> 00:24:32,309
artificial intelligence we're about do

529
00:24:27,239 --> 00:24:38,159
they fall they yeah I think they you

530
00:24:32,309 --> 00:24:42,629
know it's not as big as some might some

531
00:24:38,159 --> 00:24:44,639
reports might show today the I think we

532
00:24:42,629 --> 00:24:47,428
have very good examples of economies

533
00:24:44,639 --> 00:24:51,629
that including within the EU that have

534
00:24:47,429 --> 00:24:55,019
been able to cope with the digital

535
00:24:51,629 --> 00:24:58,259
transformation of their industry this

536
00:24:55,019 --> 00:25:00,239
transformational service sector if and

537
00:24:58,259 --> 00:25:03,959
if you look I mean the just an

538
00:25:00,239 --> 00:25:06,690
observation if I take the EU

539
00:25:03,959 --> 00:25:10,909
member-states today those that are

540
00:25:06,690 --> 00:25:14,009
invested the most in robot ization and

541
00:25:10,909 --> 00:25:16,099
automation and the transformation of

542
00:25:14,009 --> 00:25:21,619
their economy are those that have the

543
00:25:16,099 --> 00:25:25,468
lowest the lowest unemployment right the

544
00:25:21,619 --> 00:25:29,399
Germany is one of the highest robot eyes

545
00:25:25,469 --> 00:25:30,709
nation worldwide where the number of

546
00:25:29,399 --> 00:25:33,359
robots where

547
00:25:30,709 --> 00:25:34,709
you know per capita is the highest in

548
00:25:33,359 --> 00:25:38,728
the Union and the highest one of the

549
00:25:34,709 --> 00:25:40,919
highest worldwide yeah and other member

550
00:25:38,729 --> 00:25:45,479
states that have not have invested in

551
00:25:40,919 --> 00:25:49,009
this including big ones have some of the

552
00:25:45,479 --> 00:25:52,919
lowest robot ization or automation rates

553
00:25:49,009 --> 00:25:55,559
in the Western economies and they have

554
00:25:52,919 --> 00:25:59,070
the highest the highest unemployment

555
00:25:55,559 --> 00:26:01,299
rate so I think the there is a there is

556
00:25:59,070 --> 00:26:02,799
an observation that we do so it is

557
00:26:01,299 --> 00:26:05,350
managable this is what I want to see it

558
00:26:02,799 --> 00:26:09,668
is manageable if you develop the right

559
00:26:05,350 --> 00:26:12,580
ecosystems if you train your workforce

560
00:26:09,669 --> 00:26:15,580
if you adapt your workforce and if it's

561
00:26:12,580 --> 00:26:19,840
done collectively between unions between

562
00:26:15,580 --> 00:26:22,629
social society and between the all

563
00:26:19,840 --> 00:26:25,119
actors across society and the end the

564
00:26:22,629 --> 00:26:25,809
public administration and businesses it

565
00:26:25,119 --> 00:26:27,639
can work

566
00:26:25,809 --> 00:26:29,499
ok that's if you've got a question a

567
00:26:27,639 --> 00:26:32,080
comment again please feel ready and

568
00:26:29,499 --> 00:26:34,090
we've got one hello my name is Varun

569
00:26:32,080 --> 00:26:36,519
akhantar Horvath from the entire Jeff

570
00:26:34,090 --> 00:26:39,759
Knowledge Center from Budapest and I

571
00:26:36,519 --> 00:26:43,600
have a question regarding the regulation

572
00:26:39,759 --> 00:26:47,499
and automation what will happen if

573
00:26:43,600 --> 00:26:51,070
driverless car hits somebody who will be

574
00:26:47,499 --> 00:26:54,879
responsible for that and my second

575
00:26:51,070 --> 00:26:57,519
question is a can or will machine

576
00:26:54,879 --> 00:27:00,759
learning software do machine learning

577
00:26:57,519 --> 00:27:02,769
software thank you all right very good

578
00:27:00,759 --> 00:27:04,779
very good question well what are you

579
00:27:02,769 --> 00:27:07,629
saying in terms of our self-driving car

580
00:27:04,779 --> 00:27:11,950
policy on both things I mean I leave it

581
00:27:07,629 --> 00:27:13,449
to my panelists colleague on the

582
00:27:11,950 --> 00:27:15,039
regulatory kind of thinking on this one

583
00:27:13,450 --> 00:27:17,139
but just from a technology and ethically

584
00:27:15,039 --> 00:27:19,059
kind of perspective what you see is that

585
00:27:17,139 --> 00:27:21,609
we are entering a phase where this

586
00:27:19,059 --> 00:27:23,350
discussion what should happen from a

587
00:27:21,609 --> 00:27:24,399
liability perspective also but also from

588
00:27:23,350 --> 00:27:25,928
a software development perspective

589
00:27:24,399 --> 00:27:28,238
should happen I give you a very simple

590
00:27:25,929 --> 00:27:30,730
example imagine you are developing a

591
00:27:28,239 --> 00:27:33,429
self-driving car and you need to

592
00:27:30,730 --> 00:27:35,710
accommodate with software a situation

593
00:27:33,429 --> 00:27:37,989
which is an emergency shoots the softer

594
00:27:35,710 --> 00:27:40,299
bacon decision saying I'd rather harm my

595
00:27:37,989 --> 00:27:42,369
driver or should I rather harm a person

596
00:27:40,299 --> 00:27:44,559
on the street which is outside of the

597
00:27:42,369 --> 00:27:47,439
car I'm driving in a way which creates

598
00:27:44,559 --> 00:27:50,080
the kind of interesting challenge saying

599
00:27:47,440 --> 00:27:52,119
should software be in a position to make

600
00:27:50,080 --> 00:27:54,730
a judgment call saying who do harm on

601
00:27:52,119 --> 00:27:55,959
one side you could say as a society

602
00:27:54,730 --> 00:27:57,369
saying we don't want to get in the

603
00:27:55,960 --> 00:27:59,710
situation that machine should make a

604
00:27:57,369 --> 00:28:01,449
decision on the other side if you

605
00:27:59,710 --> 00:28:03,940
currently look at it know the numbers

606
00:28:01,450 --> 00:28:06,039
for Hungary but in let's say in Germany

607
00:28:03,940 --> 00:28:08,169
and in Austria in Germany I think there

608
00:28:06,039 --> 00:28:10,658
are 5000 people dying each year on the

609
00:28:08,169 --> 00:28:13,059
street and most of the accidents are

610
00:28:10,659 --> 00:28:15,070
driven by potential human drivers in a

611
00:28:13,059 --> 00:28:16,870
way so there is agreement in

612
00:28:15,070 --> 00:28:19,210
industry and academics that for instance

613
00:28:16,870 --> 00:28:21,489
if a society would completely transition

614
00:28:19,210 --> 00:28:25,450
to self-driving cars you would have not

615
00:28:21,490 --> 00:28:27,730
5,000 a tragic accidents and incidents

616
00:28:25,450 --> 00:28:31,059
in in in Germany for instance in maybe

617
00:28:27,730 --> 00:28:33,490
500 so if you don't embark on this on

618
00:28:31,059 --> 00:28:36,340
this journey for self-driving cars maybe

619
00:28:33,490 --> 00:28:38,590
you could rescue 4000 of them every

620
00:28:36,340 --> 00:28:40,928
single year so I do think this kind of

621
00:28:38,590 --> 00:28:43,090
very tricky not tricky but challenging

622
00:28:40,929 --> 00:28:45,309
kind of situation need to be discussed

623
00:28:43,090 --> 00:28:47,408
at societal level because each society

624
00:28:45,309 --> 00:28:49,928
have different reference points ethical

625
00:28:47,409 --> 00:28:52,629
rules cultural backgrounds objectives in

626
00:28:49,929 --> 00:28:54,399
a way and I don't think that any single

627
00:28:52,629 --> 00:28:55,959
actor beats the Commission beat your

628
00:28:54,399 --> 00:28:57,758
national government beat then the

629
00:28:55,960 --> 00:28:59,919
company developing the car be the

630
00:28:57,759 --> 00:29:01,720
technology provider should be in a

631
00:28:59,919 --> 00:29:03,250
position to make this decision what is

632
00:29:01,720 --> 00:29:04,570
right and what is wrong and the single

633
00:29:03,250 --> 00:29:06,759
Square in the kind of ethical

634
00:29:04,570 --> 00:29:09,399
conversation on this one and coming back

635
00:29:06,759 --> 00:29:10,840
to the kind of good machine learning

636
00:29:09,399 --> 00:29:13,000
software right machine learning software

637
00:29:10,840 --> 00:29:15,009
that's the objective I think mark is

638
00:29:13,000 --> 00:29:16,509
working on because at the end of the day

639
00:29:15,009 --> 00:29:18,100
there's this notion of called

640
00:29:16,509 --> 00:29:20,529
technically singularity the kind of

641
00:29:18,100 --> 00:29:23,860
doomsday men technology has more

642
00:29:20,529 --> 00:29:26,049
intelligence than then the humanity kind

643
00:29:23,860 --> 00:29:27,370
of thing and that books out there and I

644
00:29:26,049 --> 00:29:29,879
don't know when it will happen maybe it

645
00:29:27,370 --> 00:29:32,649
well and it's just a matter of when and

646
00:29:29,879 --> 00:29:35,168
if you take this kind of point two

647
00:29:32,649 --> 00:29:38,080
preconditions are necessary one thing is

648
00:29:35,169 --> 00:29:40,990
that software or computers have a

649
00:29:38,080 --> 00:29:42,610
generic learning ability that they can

650
00:29:40,990 --> 00:29:44,799
learn whatever task at hand and think

651
00:29:42,610 --> 00:29:48,070
they're still outsmarting any technology

652
00:29:44,799 --> 00:29:50,289
on the generic learning ability because

653
00:29:48,070 --> 00:29:52,120
we can take a person saying be an expert

654
00:29:50,289 --> 00:29:55,019
in five years from now on robotics or

655
00:29:52,120 --> 00:29:58,178
five years from now on any other kind of

656
00:29:55,019 --> 00:29:59,980
craft in a way and the second capability

657
00:29:58,179 --> 00:30:02,110
necessaries that your quote can

658
00:29:59,980 --> 00:30:05,830
reproduce yourself which is what we

659
00:30:02,110 --> 00:30:07,360
currently sing as a kind of human human

660
00:30:05,830 --> 00:30:09,549
and from a living being kind of it's a

661
00:30:07,360 --> 00:30:11,709
different jetting thing and it triggers

662
00:30:09,549 --> 00:30:13,600
an interesting kind of question saying

663
00:30:11,710 --> 00:30:15,669
what will happen if software can write

664
00:30:13,600 --> 00:30:17,529
software is reproducing itself in a way

665
00:30:15,669 --> 00:30:20,710
but this is something we're losing

666
00:30:17,529 --> 00:30:22,240
Society has decades to make a decision

667
00:30:20,710 --> 00:30:25,120
because they don't think it will happen

668
00:30:22,240 --> 00:30:28,780
in the next ten years very briefly where

669
00:30:25,120 --> 00:30:30,520
are we on liability policy

670
00:30:28,780 --> 00:30:32,230
in terms of self-driving cars in Europe

671
00:30:30,520 --> 00:30:35,580
do we have one should we get one soon

672
00:30:32,230 --> 00:30:43,540
yeah it's one of the errors we're

673
00:30:35,580 --> 00:30:46,689
working on no very intensively they and

674
00:30:43,540 --> 00:30:48,250
this is what I mentioned when the this

675
00:30:46,690 --> 00:30:50,410
is what I meant when I mentioned that we

676
00:30:48,250 --> 00:30:52,830
should not jump onto regulation very

677
00:30:50,410 --> 00:30:56,530
quickly because I think we have to learn

678
00:30:52,830 --> 00:30:59,620
first what what is a what are the

679
00:30:56,530 --> 00:31:04,180
various parameters and how to handle

680
00:30:59,620 --> 00:31:07,540
them they it's a difficult question I

681
00:31:04,180 --> 00:31:09,700
mean just and I tell you we don't have

682
00:31:07,540 --> 00:31:12,850
the full answer yet yeah who would be

683
00:31:09,700 --> 00:31:16,000
liable I mean you know fully liable or

684
00:31:12,850 --> 00:31:21,240
should there be full liability in case

685
00:31:16,000 --> 00:31:25,440
of an accident where R naught on immers

686
00:31:21,240 --> 00:31:28,090
vehicle for example autonomous robot has

687
00:31:25,440 --> 00:31:32,380
you know what was the cause and if the

688
00:31:28,090 --> 00:31:34,959
cause is real is not directly related to

689
00:31:32,380 --> 00:31:38,020
any defect coming from the manufacturer

690
00:31:34,960 --> 00:31:39,730
or from the user but if the defect is

691
00:31:38,020 --> 00:31:46,330
coming from the learning algorithm

692
00:31:39,730 --> 00:31:49,030
itself okay as the and from the the the

693
00:31:46,330 --> 00:31:53,560
learning capacity of the of the vehicle

694
00:31:49,030 --> 00:31:56,590
now the what we're doing in order is

695
00:31:53,560 --> 00:31:58,990
that issue now work on something and we

696
00:31:56,590 --> 00:32:01,570
have a big consultation on what we call

697
00:31:58,990 --> 00:32:04,740
the product liability directive so

698
00:32:01,570 --> 00:32:07,600
that's the law that handles the

699
00:32:04,740 --> 00:32:09,730
liability issues of products in general

700
00:32:07,600 --> 00:32:11,770
so we're consulting to see what we

701
00:32:09,730 --> 00:32:16,000
gather as views and constitution still

702
00:32:11,770 --> 00:32:19,320
ongoing we are also looking at testing

703
00:32:16,000 --> 00:32:24,040
and experimentation and we issued a

704
00:32:19,320 --> 00:32:26,560
strategy on data in general how to

705
00:32:24,040 --> 00:32:30,190
exploit data including for machine

706
00:32:26,560 --> 00:32:31,139
learning and within that we engaged in

707
00:32:30,190 --> 00:32:33,190
two

708
00:32:31,140 --> 00:32:34,659
testing large-scale testing and

709
00:32:33,190 --> 00:32:36,880
experimentation around autonomous

710
00:32:34,659 --> 00:32:39,010
vehicle and we sitting up now corridors

711
00:32:36,880 --> 00:32:40,809
across Europe to be able to test an

712
00:32:39,010 --> 00:32:43,270
experiment and see what we can learn

713
00:32:40,809 --> 00:32:45,370
from this yeah I think that's the the

714
00:32:43,270 --> 00:32:47,309
the way you know we're trying to

715
00:32:45,370 --> 00:32:50,559
progress on this on this issue

716
00:32:47,309 --> 00:32:51,908
consultation we're trying to test an

717
00:32:50,559 --> 00:32:57,928
experiment the corridor should be

718
00:32:51,909 --> 00:33:01,500
running throughout 2018 hopefully and we

719
00:32:57,929 --> 00:33:05,020
we will try out of that to gather enough

720
00:33:01,500 --> 00:33:07,240
information to see how to adapt current

721
00:33:05,020 --> 00:33:09,280
regulatory frameworks so experimentation

722
00:33:07,240 --> 00:33:13,360
and then the normalization right that's

723
00:33:09,280 --> 00:33:15,610
a I think something just so G holiday

724
00:33:13,360 --> 00:33:16,990
because it's not only legal yes make

725
00:33:15,610 --> 00:33:19,600
sure it's legal issue there's the

726
00:33:16,990 --> 00:33:21,789
economics also behind it yeah if you put

727
00:33:19,600 --> 00:33:26,918
the liability for example 100 percent

728
00:33:21,789 --> 00:33:30,309
just on the the user so the the owner of

729
00:33:26,919 --> 00:33:32,490
the machine given the complexity of the

730
00:33:30,309 --> 00:33:35,139
technology you might end up with no-one

731
00:33:32,490 --> 00:33:38,080
buying by at all so you have to have

732
00:33:35,140 --> 00:33:39,789
what we call shared liability these are

733
00:33:38,080 --> 00:33:41,590
the things that are developing at scales

734
00:33:39,789 --> 00:33:44,530
so nerrac I wonder if you could touch a

735
00:33:41,590 --> 00:33:46,299
little bit on a difficult problem of

736
00:33:44,530 --> 00:33:48,070
actually trying to teach cars to

737
00:33:46,299 --> 00:33:51,039
understand human behavior in a in a

738
00:33:48,070 --> 00:33:53,049
driver context and also how realistic is

739
00:33:51,039 --> 00:33:55,270
this idea of self-driving cars within a

740
00:33:53,049 --> 00:33:59,860
real time frame of like 10 years you

741
00:33:55,270 --> 00:34:02,168
think oh so I've been quite surprised by

742
00:33:59,860 --> 00:34:04,840
the success in this field and how

743
00:34:02,169 --> 00:34:07,600
machine learning what kind of problems

744
00:34:04,840 --> 00:34:11,020
it was able to solve on the other side

745
00:34:07,600 --> 00:34:14,049
there are still some last mile problems

746
00:34:11,020 --> 00:34:16,659
in self-driving so even if your car can

747
00:34:14,050 --> 00:34:18,970
drive you for highway and maybe in

748
00:34:16,659 --> 00:34:21,339
cities when there is no complicated

749
00:34:18,969 --> 00:34:24,040
situation but then it fails in certain

750
00:34:21,339 --> 00:34:26,139
situations and you will not want to rely

751
00:34:24,040 --> 00:34:29,109
on the car you know so it fails in some

752
00:34:26,139 --> 00:34:32,080
situations or you will have to take the

753
00:34:29,109 --> 00:34:34,929
take the steering wheel and finish it

754
00:34:32,080 --> 00:34:38,679
yourself so the thing is that there may

755
00:34:34,929 --> 00:34:40,599
be too many complex situations that the

756
00:34:38,679 --> 00:34:43,110
current deep learning will not be able

757
00:34:40,599 --> 00:34:45,360
to solve you know without some

758
00:34:43,110 --> 00:34:47,460
another technological breakthroughs but

759
00:34:45,360 --> 00:34:50,340
on the other side I may be surprised

760
00:34:47,460 --> 00:34:53,820
because like basically every month I'm

761
00:34:50,340 --> 00:34:56,840
surprised by what people are able to

762
00:34:53,820 --> 00:35:01,380
achieve or accomplish with deep learning

763
00:34:56,840 --> 00:35:03,330
so it's possible that without some new

764
00:35:01,380 --> 00:35:05,040
technology better technology then deep

765
00:35:03,330 --> 00:35:08,430
learning will not be able to do this but

766
00:35:05,040 --> 00:35:13,080
also it's possible that due to sheer

767
00:35:08,430 --> 00:35:14,640
amount of data examples and humans smart

768
00:35:13,080 --> 00:35:17,790
humans designing these self-driving

769
00:35:14,640 --> 00:35:23,190
algorithms will be able to get even even

770
00:35:17,790 --> 00:35:26,700
to this last mile problems and it's

771
00:35:23,190 --> 00:35:29,400
alright if you were to make a data set

772
00:35:26,700 --> 00:35:30,930
that would be critical to self-driving

773
00:35:29,400 --> 00:35:32,400
cars actually learning about human

774
00:35:30,930 --> 00:35:34,160
behavior maybe outfitting people with

775
00:35:32,400 --> 00:35:36,720
different sensors or things like this

776
00:35:34,160 --> 00:35:39,810
what's the data set that's you feel

777
00:35:36,720 --> 00:35:42,509
missing for deep learning to take us to

778
00:35:39,810 --> 00:35:45,810
a point where autonomous driving becomes

779
00:35:42,510 --> 00:35:48,120
more realistic I think that the AI the

780
00:35:45,810 --> 00:35:51,360
global learning to drive in all the

781
00:35:48,120 --> 00:35:53,490
situation need to experience the very

782
00:35:51,360 --> 00:35:55,950
same situations that vs drivers

783
00:35:53,490 --> 00:35:58,950
experience and we all have to have

784
00:35:55,950 --> 00:36:01,109
training examples that will train it to

785
00:35:58,950 --> 00:36:04,470
react in the same way as a human would

786
00:36:01,110 --> 00:36:06,210
react the thing is that it's still

787
00:36:04,470 --> 00:36:09,029
possible that these training set will

788
00:36:06,210 --> 00:36:11,040
not contain all the nuances because

789
00:36:09,030 --> 00:36:13,020
there may be situations or they may be

790
00:36:11,040 --> 00:36:16,380
missing situations that we as a humans

791
00:36:13,020 --> 00:36:18,330
learn while developing in our childhood

792
00:36:16,380 --> 00:36:21,330
in while interacting with other people

793
00:36:18,330 --> 00:36:22,950
and so on and if these situations will

794
00:36:21,330 --> 00:36:24,540
not be present in the training data they

795
00:36:22,950 --> 00:36:26,700
may be missing and that the car will

796
00:36:24,540 --> 00:36:28,529
basically and not know how to react in

797
00:36:26,700 --> 00:36:31,950
such situation so for example when I see

798
00:36:28,530 --> 00:36:35,580
a person are standing on a side of a

799
00:36:31,950 --> 00:36:37,680
road my brain because I know what the

800
00:36:35,580 --> 00:36:40,200
person is probably thinking will be like

801
00:36:37,680 --> 00:36:42,120
okay is he going to step there or not is

802
00:36:40,200 --> 00:36:43,770
he drunk not and I will like start

803
00:36:42,120 --> 00:36:45,930
running all the simulations in my head

804
00:36:43,770 --> 00:36:47,370
yeah and if the car doesn't have the

805
00:36:45,930 --> 00:36:49,440
same experience like the car doesn't

806
00:36:47,370 --> 00:36:50,670
know what drunk person means or

807
00:36:49,440 --> 00:36:52,769
something like this

808
00:36:50,670 --> 00:36:55,319
if you'll not be able to simulate the

809
00:36:52,769 --> 00:36:57,118
similar scenarios on the other side if

810
00:36:55,319 --> 00:36:59,400
the designers of these algorithms and

811
00:36:57,119 --> 00:37:01,950
the teachers of these algorithms will

812
00:36:59,400 --> 00:37:04,230
include all these potential situations

813
00:37:01,950 --> 00:37:06,618
in the sub train car maybe they will

814
00:37:04,230 --> 00:37:09,599
just hit the this low-hanging fruit of

815
00:37:06,619 --> 00:37:12,739
what they need so I think it's doable

816
00:37:09,599 --> 00:37:16,140
like I wouldn't rule it out completely

817
00:37:12,739 --> 00:37:17,910
even without having general I I okay

818
00:37:16,140 --> 00:37:20,670
first we're gonna go to Everton and we

819
00:37:17,910 --> 00:37:22,288
have I'd like to add on something to

820
00:37:20,670 --> 00:37:24,049
really keep the kind of innovation

821
00:37:22,289 --> 00:37:26,249
dimension in this whole conversation

822
00:37:24,049 --> 00:37:28,710
there's so many ways how you can measure

823
00:37:26,249 --> 00:37:31,019
and observe innovation you can look at

824
00:37:28,710 --> 00:37:32,999
the front end of the innovation train

825
00:37:31,019 --> 00:37:34,649
what are we introducing as new

826
00:37:32,999 --> 00:37:37,529
technology in your process new

827
00:37:34,650 --> 00:37:39,119
regulations in our society but it can

828
00:37:37,529 --> 00:37:41,130
also take a look at the innovation at

829
00:37:39,119 --> 00:37:43,829
the back end of this train what I'll be

830
00:37:41,130 --> 00:37:45,480
able to leave behind in a way and use a

831
00:37:43,829 --> 00:37:48,269
very simple example if you're in an

832
00:37:45,480 --> 00:37:50,910
organization 20 years ago and 99 percent

833
00:37:48,269 --> 00:37:53,609
of your employees had an individual

834
00:37:50,910 --> 00:37:56,098
email box you and the other one didn't

835
00:37:53,609 --> 00:37:57,960
have it you still couldn't shut down the

836
00:37:56,099 --> 00:38:01,230
previous business process of sharing

837
00:37:57,960 --> 00:38:04,230
information only going from 99 to 100

838
00:38:01,230 --> 00:38:06,269
percent allowed you to leave 50 years of

839
00:38:04,230 --> 00:38:08,880
paper-based interaction in your company

840
00:38:06,269 --> 00:38:10,410
behind otherwise you would left one

841
00:38:08,880 --> 00:38:12,569
percent of your employees out which you

842
00:38:10,410 --> 00:38:14,788
shouldn't in the way so you can look at

843
00:38:12,569 --> 00:38:17,308
innovation being introduced the email

844
00:38:14,789 --> 00:38:20,099
system as a company for many people and

845
00:38:17,309 --> 00:38:22,619
you can look it we reached 100 percent

846
00:38:20,099 --> 00:38:24,599
and in the in the area of cars think

847
00:38:22,619 --> 00:38:26,509
through innovation in the similar way so

848
00:38:24,599 --> 00:38:29,039
for instance currently a ton of

849
00:38:26,509 --> 00:38:31,019
computing power algorithm software

850
00:38:29,039 --> 00:38:33,509
design sophistication innovation is

851
00:38:31,019 --> 00:38:37,078
being spent to help cars understand for

852
00:38:33,509 --> 00:38:39,480
instance the street signs stop left turn

853
00:38:37,079 --> 00:38:42,239
right turn in a way imagine a world

854
00:38:39,480 --> 00:38:42,960
where human drivers aren't part of the

855
00:38:42,239 --> 00:38:44,880
equation anymore

856
00:38:42,960 --> 00:38:47,309
we don't need street signs anymore and

857
00:38:44,880 --> 00:38:50,359
you could get to a way that cars would

858
00:38:47,309 --> 00:38:53,039
for instance approach a crossing and

859
00:38:50,359 --> 00:38:55,499
maybe it's just future kind of

860
00:38:53,039 --> 00:38:57,869
conversation maybe the cars would engage

861
00:38:55,499 --> 00:39:01,379
in an auctioning system saying whoever

862
00:38:57,869 --> 00:39:03,180
is willing to pay more micro payments

863
00:39:01,380 --> 00:39:04,020
kind of still will get preferential

864
00:39:03,180 --> 00:39:06,480
treatment

865
00:39:04,020 --> 00:39:08,640
crossing this crossing faster than

866
00:39:06,480 --> 00:39:10,590
anybody else in way so thinks through

867
00:39:08,640 --> 00:39:13,290
many many layers of potential

868
00:39:10,590 --> 00:39:15,690
innovations will complete ecosystems can

869
00:39:13,290 --> 00:39:17,610
be created changed in a way which we

870
00:39:15,690 --> 00:39:20,160
couldn't first cannot foresee today and

871
00:39:17,610 --> 00:39:22,650
because only using let's say artificial

872
00:39:20,160 --> 00:39:25,549
intelligence to create a migration

873
00:39:22,650 --> 00:39:27,690
scenario scenario where humans and out

874
00:39:25,550 --> 00:39:30,210
self-driving cars at the same time on

875
00:39:27,690 --> 00:39:31,770
the street but think about 20 30 40

876
00:39:30,210 --> 00:39:33,210
years in the future maybe there's not a

877
00:39:31,770 --> 00:39:34,140
single human driver anymore in the

878
00:39:33,210 --> 00:39:35,520
equation yeah

879
00:39:34,140 --> 00:39:37,379
is that is that something that Microsoft

880
00:39:35,520 --> 00:39:40,830
is actively now talking about this idea

881
00:39:37,380 --> 00:39:42,450
of transaction taxes for autonomous

882
00:39:40,830 --> 00:39:44,790
systems as they encounter each other yes

883
00:39:42,450 --> 00:39:48,330
you have we do have chief economist

884
00:39:44,790 --> 00:39:50,700
since about 10-15 years in a way because

885
00:39:48,330 --> 00:39:52,680
many of those things which are part of

886
00:39:50,700 --> 00:39:54,930
consideration which is called market

887
00:39:52,680 --> 00:39:57,600
design considerations that the market

888
00:39:54,930 --> 00:39:59,460
design is basically defined by the

889
00:39:57,600 --> 00:40:00,630
interactions at the economic interaction

890
00:39:59,460 --> 00:40:02,460
within different parties but it's just

891
00:40:00,630 --> 00:40:04,290
looking into this one I don't have a

892
00:40:02,460 --> 00:40:06,000
solution and we don't have a business

893
00:40:04,290 --> 00:40:08,040
plan on this one but I want to create

894
00:40:06,000 --> 00:40:10,860
visibility that innovation can happen

895
00:40:08,040 --> 00:40:12,870
when many many different layers of of

896
00:40:10,860 --> 00:40:14,460
the innovation that's the nature it's

897
00:40:12,870 --> 00:40:17,160
such an interesting news item I know I'm

898
00:40:14,460 --> 00:40:18,840
having talked to Sebastian Thrun who's

899
00:40:17,160 --> 00:40:20,460
the original creator of the Google

900
00:40:18,840 --> 00:40:22,020
self-driving car he says that one of the

901
00:40:20,460 --> 00:40:25,020
biggest problems that he encounters now

902
00:40:22,020 --> 00:40:26,850
is exactly this intersection problem you

903
00:40:25,020 --> 00:40:28,680
can teach the cars to understand the

904
00:40:26,850 --> 00:40:30,060
rules of the road very easily when they

905
00:40:28,680 --> 00:40:32,730
get to intersections they don't

906
00:40:30,060 --> 00:40:34,920
understand that people tend to be jerks

907
00:40:32,730 --> 00:40:38,010
and they kind of go out a little bit

908
00:40:34,920 --> 00:40:39,810
first and so teaching the car to

909
00:40:38,010 --> 00:40:41,580
understand that when the when the human

910
00:40:39,810 --> 00:40:43,410
is impatiently pushing into the

911
00:40:41,580 --> 00:40:45,480
intersection even if it's not his turn

912
00:40:43,410 --> 00:40:47,490
you have to let him go because the car

913
00:40:45,480 --> 00:40:48,810
would just obey the rules and people

914
00:40:47,490 --> 00:40:50,850
aren't like that I'm sure I would wind

915
00:40:48,810 --> 00:40:52,799
up paying that in patience tax like

916
00:40:50,850 --> 00:40:55,589
every day so let's go here to the

917
00:40:52,800 --> 00:40:58,710
audience and a question from one of the

918
00:40:55,590 --> 00:41:01,680
participants thank you first question is

919
00:40:58,710 --> 00:41:04,920
a short term so for mr. ohana what kind

920
00:41:01,680 --> 00:41:06,600
of cooperation do you have with maybe

921
00:41:04,920 --> 00:41:08,790
through the other DG's whether deaf

922
00:41:06,600 --> 00:41:11,350
cotija NIR and others in terms of

923
00:41:08,790 --> 00:41:13,060
international cooperation on these

924
00:41:11,350 --> 00:41:15,390
choose especially on the regulatory way

925
00:41:13,060 --> 00:41:17,680
but also on cooperation with with other

926
00:41:15,390 --> 00:41:19,900
technology sectors outside the EU

927
00:41:17,680 --> 00:41:21,250
because my fear while I totally

928
00:41:19,900 --> 00:41:23,860
understand and support the idea of not

929
00:41:21,250 --> 00:41:26,110
over regulating too fast is to reach a

930
00:41:23,860 --> 00:41:27,820
situation where because of the weakness

931
00:41:26,110 --> 00:41:30,010
of the judiciary system legislative

932
00:41:27,820 --> 00:41:31,510
systems in neighboring countries you'll

933
00:41:30,010 --> 00:41:33,160
create a system where in the EU or

934
00:41:31,510 --> 00:41:35,110
elsewhere you have ethical

935
00:41:33,160 --> 00:41:37,089
considerations liability considerations

936
00:41:35,110 --> 00:41:39,880
whereas it's a complete jungle outside

937
00:41:37,090 --> 00:41:41,980
where innovators do exist and operate so

938
00:41:39,880 --> 00:41:42,670
it would create a system that is really

939
00:41:41,980 --> 00:41:45,430
difficult

940
00:41:42,670 --> 00:41:48,220
that's why judiciary systems need to be

941
00:41:45,430 --> 00:41:50,319
ready need to be informed early on and a

942
00:41:48,220 --> 00:41:52,120
more general question similar to the one

943
00:41:50,320 --> 00:41:54,460
that the colleague earlier asked what

944
00:41:52,120 --> 00:41:56,830
about its what about the implications on

945
00:41:54,460 --> 00:41:59,560
politics on elections understanding

946
00:41:56,830 --> 00:42:02,020
people's behavior and who decides what

947
00:41:59,560 --> 00:42:04,029
on elections and so it's implication on

948
00:42:02,020 --> 00:42:04,390
social contracts and democracy in

949
00:42:04,030 --> 00:42:07,930
general

950
00:42:04,390 --> 00:42:09,790
okay so briefly a large policy

951
00:42:07,930 --> 00:42:12,210
frameworks for thinking through some of

952
00:42:09,790 --> 00:42:14,140
these issues and then is AI going to

953
00:42:12,210 --> 00:42:16,480
affect our elections in unpredictable

954
00:42:14,140 --> 00:42:18,660
ways it will open that second one up to

955
00:42:16,480 --> 00:42:24,370
everybody I think yeah on the

956
00:42:18,660 --> 00:42:25,990
international cooperation and on the

957
00:42:24,370 --> 00:42:27,670
legal ethical issues regarding

958
00:42:25,990 --> 00:42:31,109
artificial intelligence at our radar

959
00:42:27,670 --> 00:42:33,790
hundred percent today we have dialogues

960
00:42:31,110 --> 00:42:35,350
related to the digital transformation of

961
00:42:33,790 --> 00:42:39,340
the economy and society that we have

962
00:42:35,350 --> 00:42:43,180
with all our trading partners US Japan

963
00:42:39,340 --> 00:42:47,260
China Korea and we have it also a part

964
00:42:43,180 --> 00:42:51,129
of our all neighbouring policies just

965
00:42:47,260 --> 00:42:54,070
they to mention that you know these

966
00:42:51,130 --> 00:42:56,380
issues are new and we started

967
00:42:54,070 --> 00:43:02,500
introducing them in our dialogues and

968
00:42:56,380 --> 00:43:05,350
say in 2016 and now 2017 they an even

969
00:43:02,500 --> 00:43:07,720
inside the the institutions of

970
00:43:05,350 --> 00:43:10,299
institutions or in the Member State with

971
00:43:07,720 --> 00:43:14,410
whom we have dialogue all the time on

972
00:43:10,300 --> 00:43:18,700
these issues I think the the awareness

973
00:43:14,410 --> 00:43:21,069
of the importance of these issues that

974
00:43:18,700 --> 00:43:22,319
was really intend to started really in

975
00:43:21,070 --> 00:43:25,859
2016

976
00:43:22,319 --> 00:43:28,288
17 so these are new and they are on our

977
00:43:25,859 --> 00:43:30,058
radar it's a very good point indeed very

978
00:43:28,289 --> 00:43:32,130
very good point indeed as you see this

979
00:43:30,059 --> 00:43:33,749
is why we're not you know jumping on

980
00:43:32,130 --> 00:43:37,529
regulation because we want to make sure

981
00:43:33,749 --> 00:43:40,859
that we get a better understanding of

982
00:43:37,529 --> 00:43:44,669
its implication inside the U and on our

983
00:43:40,859 --> 00:43:47,578
trading with our and our relations with

984
00:43:44,669 --> 00:43:49,049
the rest of the world yeah I so we've

985
00:43:47,579 --> 00:43:51,469
come to the second part is this question

986
00:43:49,049 --> 00:43:54,209
of artificial intelligence and its

987
00:43:51,469 --> 00:43:56,609
effect on micro targeting of political

988
00:43:54,209 --> 00:43:59,249
advertisement I think is basically how

989
00:43:56,609 --> 00:44:01,859
you reduce that is this something that

990
00:43:59,249 --> 00:44:03,118
is a realistic concern is that over

991
00:44:01,859 --> 00:44:05,669
something that we shouldn't worry about

992
00:44:03,119 --> 00:44:07,439
I think we should but before I start

993
00:44:05,669 --> 00:44:09,390
addressing this one I want to come back

994
00:44:07,439 --> 00:44:11,669
to your first question around the kind

995
00:44:09,390 --> 00:44:13,529
of regulatory or let's say the

996
00:44:11,669 --> 00:44:16,140
competition between countries let's call

997
00:44:13,529 --> 00:44:17,819
it this way because coming back to the

998
00:44:16,140 --> 00:44:19,859
original question around there's so much

999
00:44:17,819 --> 00:44:21,869
opportunity productivity increase has

1000
00:44:19,859 --> 00:44:23,999
been mentioned by the moderator by

1001
00:44:21,869 --> 00:44:26,369
Patrick and on the other side there will

1002
00:44:23,999 --> 00:44:28,529
be chopped losses let me just turn the

1003
00:44:26,369 --> 00:44:30,269
wheel of time a hundred years back in my

1004
00:44:28,529 --> 00:44:31,409
country I'm from Austria I don't know

1005
00:44:30,269 --> 00:44:33,959
the numbers for other kinds but they

1006
00:44:31,409 --> 00:44:36,390
would assume they're similar 50% of the

1007
00:44:33,959 --> 00:44:40,169
population worked in agriculture today

1008
00:44:36,390 --> 00:44:42,538
it's 0.7% and we don't have forty-nine

1009
00:44:40,169 --> 00:44:45,769
point three percent unemployment rate so

1010
00:44:42,539 --> 00:44:49,949
we had an opportunity and we had a

1011
00:44:45,769 --> 00:44:52,529
society a strategy to transition from a

1012
00:44:49,949 --> 00:44:54,179
previous job model agriculture to

1013
00:44:52,529 --> 00:44:56,400
service industry and all these kind of

1014
00:44:54,179 --> 00:44:58,229
things but the thing is that it happened

1015
00:44:56,400 --> 00:45:00,269
over the course of a hundred years and

1016
00:44:58,229 --> 00:45:03,149
what's happening these days is that

1017
00:45:00,269 --> 00:45:05,249
given the kind of economic models and

1018
00:45:03,150 --> 00:45:07,589
and and and the progress we have the

1019
00:45:05,249 --> 00:45:09,988
digital technologies we need to consider

1020
00:45:07,589 --> 00:45:12,089
similar kind of dependencies that on

1021
00:45:09,989 --> 00:45:13,650
once that opportunity is created on the

1022
00:45:12,089 --> 00:45:15,659
other side there will be some side

1023
00:45:13,650 --> 00:45:17,729
effects on this one and the interesting

1024
00:45:15,659 --> 00:45:19,140
thing is that it might happen in a

1025
00:45:17,729 --> 00:45:20,609
single human generation which is

1026
00:45:19,140 --> 00:45:22,769
normally thirty years kind of saying

1027
00:45:20,609 --> 00:45:24,869
which changes fundamentally a couple of

1028
00:45:22,769 --> 00:45:26,698
things so for instance you can't pass on

1029
00:45:24,869 --> 00:45:28,469
the baton from one generation to the

1030
00:45:26,699 --> 00:45:31,499
next one so you need to learn yourself

1031
00:45:28,469 --> 00:45:33,509
how to basically get an opportunity for

1032
00:45:31,499 --> 00:45:34,529
yourself in your professional life in

1033
00:45:33,509 --> 00:45:36,570
one generation

1034
00:45:34,530 --> 00:45:38,790
one side and what the second thing is

1035
00:45:36,570 --> 00:45:40,830
that countries and societies make

1036
00:45:38,790 --> 00:45:42,330
different choices some society is

1037
00:45:40,830 --> 00:45:43,890
potentially scared by the new

1038
00:45:42,330 --> 00:45:45,480
development and by the way the

1039
00:45:43,890 --> 00:45:48,270
University of California Berkeley brie

1040
00:45:45,480 --> 00:45:50,580
Institute is looking since many many

1041
00:45:48,270 --> 00:45:52,650
years how technology is changing over

1042
00:45:50,580 --> 00:45:54,750
the last 200 years societal kind of

1043
00:45:52,650 --> 00:45:56,670
cohesion professor Tom ties men in case

1044
00:45:54,750 --> 00:45:58,350
you're gonna look it up and so whenever

1045
00:45:56,670 --> 00:46:00,720
there is a kind of new development

1046
00:45:58,350 --> 00:46:02,970
Society make a choice in I don't we

1047
00:46:00,720 --> 00:46:05,430
don't want to embark too fast on this

1048
00:46:02,970 --> 00:46:06,899
one which is a rightful choice if the

1049
00:46:05,430 --> 00:46:09,810
society is making this kind of choice

1050
00:46:06,900 --> 00:46:11,280
but maybe the neighboring society is

1051
00:46:09,810 --> 00:46:13,380
making different choices we want to

1052
00:46:11,280 --> 00:46:15,510
embark it as fast as possible and then

1053
00:46:13,380 --> 00:46:17,550
you see that potentially competition

1054
00:46:15,510 --> 00:46:20,880
between those two societies on welcome

1055
00:46:17,550 --> 00:46:22,440
progress on safety on on whatever kind

1056
00:46:20,880 --> 00:46:24,930
of social cohesion they're interested in

1057
00:46:22,440 --> 00:46:27,090
it gets to a point of tension because

1058
00:46:24,930 --> 00:46:28,470
this society is developing faster than

1059
00:46:27,090 --> 00:46:30,570
the other one and then potentially there

1060
00:46:28,470 --> 00:46:32,640
is a rebalancing activity and they do

1061
00:46:30,570 --> 00:46:35,370
think similar things will happen in the

1062
00:46:32,640 --> 00:46:36,960
adoption of AI as well that we need to

1063
00:46:35,370 --> 00:46:38,279
have this conversation don't don't get

1064
00:46:36,960 --> 00:46:40,440
me wrong we need to have the ethically

1065
00:46:38,280 --> 00:46:42,000
conversation about how is the proper

1066
00:46:40,440 --> 00:46:44,250
balance between the benefits and the

1067
00:46:42,000 --> 00:46:45,960
drawbacks on this one but I do expect

1068
00:46:44,250 --> 00:46:47,460
the different regions different

1069
00:46:45,960 --> 00:46:48,330
countries will come to different

1070
00:46:47,460 --> 00:46:49,950
conclusions

1071
00:46:48,330 --> 00:46:51,690
should we rather slow down the

1072
00:46:49,950 --> 00:46:54,029
transition or should we go full speed

1073
00:46:51,690 --> 00:46:56,040
ahead this is a societal conversation

1074
00:46:54,030 --> 00:46:57,810
not an individual company conversation

1075
00:46:56,040 --> 00:46:58,650
alright really because I want to make

1076
00:46:57,810 --> 00:46:59,970
sure there's lots of time for audience

1077
00:46:58,650 --> 00:47:02,700
questions can you touch on this question

1078
00:46:59,970 --> 00:47:05,459
of the applications of artificial

1079
00:47:02,700 --> 00:47:07,649
intelligence for changing and

1080
00:47:05,460 --> 00:47:09,300
influencing public perception either

1081
00:47:07,650 --> 00:47:11,820
politically or in other areas is this

1082
00:47:09,300 --> 00:47:14,730
absolutely you have spot on on this one

1083
00:47:11,820 --> 00:47:16,920
and I don't think that any individual or

1084
00:47:14,730 --> 00:47:19,080
any individual company can provide the

1085
00:47:16,920 --> 00:47:21,480
solution but there's a ton of suggested

1086
00:47:19,080 --> 00:47:23,490
steps going forward so for instance we

1087
00:47:21,480 --> 00:47:26,820
created a partnership called partnership

1088
00:47:23,490 --> 00:47:28,529
on AI and by the way it's open for new

1089
00:47:26,820 --> 00:47:30,210
members in ways so just look it up

1090
00:47:28,530 --> 00:47:34,140
partnership on the AI very large

1091
00:47:30,210 --> 00:47:36,390
companies started to create a platform

1092
00:47:34,140 --> 00:47:38,850
for having exactly this conversation and

1093
00:47:36,390 --> 00:47:40,710
they give a couple of examples about the

1094
00:47:38,850 --> 00:47:43,080
dimension you just mentioned from a

1095
00:47:40,710 --> 00:47:44,940
democratic perspective how should we

1096
00:47:43,080 --> 00:47:47,640
address bias in data

1097
00:47:44,940 --> 00:47:48,410
bias in algorithms but even bias in

1098
00:47:47,640 --> 00:47:51,480
Sousa

1099
00:47:48,410 --> 00:47:55,500
because we do have the situation that

1100
00:47:51,480 --> 00:47:57,930
our notion about the world is founded in

1101
00:47:55,500 --> 00:47:59,430
the culture the background and in the

1102
00:47:57,930 --> 00:48:01,259
cultural concepts we have in our

1103
00:47:59,430 --> 00:48:03,000
societies and way and so if you develop

1104
00:48:01,260 --> 00:48:05,220
a software with this kind of embedded

1105
00:48:03,000 --> 00:48:07,170
not on sometimes explicit kind of the

1106
00:48:05,220 --> 00:48:12,000
same knowledge that your background is

1107
00:48:07,170 --> 00:48:13,830
let's say Eurasian kind of people your

1108
00:48:12,000 --> 00:48:15,540
face recognition software might not

1109
00:48:13,830 --> 00:48:18,090
recognize them people with different

1110
00:48:15,540 --> 00:48:20,700
colors in a way and so we do have this

1111
00:48:18,090 --> 00:48:22,650
earlier let's say evidence that those

1112
00:48:20,700 --> 00:48:24,419
things need to be addressed but it's not

1113
00:48:22,650 --> 00:48:26,280
to be addressed only by technology but

1114
00:48:24,420 --> 00:48:28,860
it need to be addressed by a common

1115
00:48:26,280 --> 00:48:30,540
understanding by ethical rules by moral

1116
00:48:28,860 --> 00:48:32,420
kind of judgement away which is an

1117
00:48:30,540 --> 00:48:35,460
interesting kind of intersection between

1118
00:48:32,420 --> 00:48:37,800
Catholic technology developments versus

1119
00:48:35,460 --> 00:48:40,500
societal kind of of things on this one

1120
00:48:37,800 --> 00:48:42,270
and just to close on this one it's also

1121
00:48:40,500 --> 00:48:46,110
the kind of thing imagined that many

1122
00:48:42,270 --> 00:48:48,300
people are depending on on technology to

1123
00:48:46,110 --> 00:48:50,070
make decisions in a way and the ultimate

1124
00:48:48,300 --> 00:48:52,230
decision in democracies for instance

1125
00:48:50,070 --> 00:48:54,570
elections how should technology in the

1126
00:48:52,230 --> 00:48:55,380
future behave if you're asking in the

1127
00:48:54,570 --> 00:48:57,510
voting booth

1128
00:48:55,380 --> 00:48:59,880
who should I elect should we stay away

1129
00:48:57,510 --> 00:49:02,400
should this be a voluntary process

1130
00:48:59,880 --> 00:49:04,410
should we give to basically your kind of

1131
00:49:02,400 --> 00:49:06,480
filter bubble a kind of reinforcement

1132
00:49:04,410 --> 00:49:09,390
going to elect who you want to elect

1133
00:49:06,480 --> 00:49:11,700
anyway so it's a tricky question which

1134
00:49:09,390 --> 00:49:13,770
not a single actor can answer from my

1135
00:49:11,700 --> 00:49:15,480
perspective that's reason why we were

1136
00:49:13,770 --> 00:49:18,180
among the founding members of

1137
00:49:15,480 --> 00:49:19,920
partnership of AI and if there's any

1138
00:49:18,180 --> 00:49:23,490
company interesting in joining please

1139
00:49:19,920 --> 00:49:25,050
let me know ok well let's go to we've

1140
00:49:23,490 --> 00:49:27,379
got a few questions here it's going all

1141
00:49:25,050 --> 00:49:27,380
right over here

1142
00:49:27,910 --> 00:49:31,899
hi my name is Kim bong and I'm a

1143
00:49:29,530 --> 00:49:35,410
Canadian from from Toronto which was a

1144
00:49:31,900 --> 00:49:36,940
great Center for for AI and my question

1145
00:49:35,410 --> 00:49:39,250
is actually for our friend from

1146
00:49:36,940 --> 00:49:41,740
Microsoft you've talked about the

1147
00:49:39,250 --> 00:49:44,560
importance of democratizing AI and that

1148
00:49:41,740 --> 00:49:46,569
it cannot should not be controlled by

1149
00:49:44,560 --> 00:49:48,279
the few and a sounds great

1150
00:49:46,570 --> 00:49:51,310
and makes for a great sound button in

1151
00:49:48,280 --> 00:49:54,670
fact I tweeted it yeah my question for

1152
00:49:51,310 --> 00:49:57,520
you though is as a corporation and as

1153
00:49:54,670 --> 00:49:59,110
agents of shareholders yeah your

1154
00:49:57,520 --> 00:50:01,480
objective is to maximize shareholder

1155
00:49:59,110 --> 00:50:04,150
value yeah which would suggest the

1156
00:50:01,480 --> 00:50:05,230
concentration of power in your hands so

1157
00:50:04,150 --> 00:50:08,170
how do you reconcile these two

1158
00:50:05,230 --> 00:50:10,270
challenges thanks so much for an

1159
00:50:08,170 --> 00:50:12,580
interesting question and let me qualify

1160
00:50:10,270 --> 00:50:14,380
it democratizing democratizing from a

1161
00:50:12,580 --> 00:50:19,180
political process is not objective what

1162
00:50:14,380 --> 00:50:21,340
we embarked on democratization is giving

1163
00:50:19,180 --> 00:50:22,810
as many people as possible access and

1164
00:50:21,340 --> 00:50:25,470
the benefit of this kind of technology

1165
00:50:22,810 --> 00:50:27,279
is the notion of democratization on

1166
00:50:25,470 --> 00:50:28,870
technology

1167
00:50:27,280 --> 00:50:30,700
this is regard to your second question

1168
00:50:28,870 --> 00:50:32,740
about how do we consign the kind of

1169
00:50:30,700 --> 00:50:34,689
society the responsibility and the

1170
00:50:32,740 --> 00:50:37,560
shareholder is mobility this is the

1171
00:50:34,690 --> 00:50:41,050
reason why we started our AI journey

1172
00:50:37,560 --> 00:50:44,410
this pledge our chief executive officers

1173
00:50:41,050 --> 00:50:45,850
such an adela made according I need to

1174
00:50:44,410 --> 00:50:48,190
read because it's such a long thing

1175
00:50:45,850 --> 00:50:50,890
data ethics principles for human

1176
00:50:48,190 --> 00:50:53,860
humanistic approach to AI so what we

1177
00:50:50,890 --> 00:50:56,740
started with is six statements around

1178
00:50:53,860 --> 00:50:59,140
what will be the guiding principle how

1179
00:50:56,740 --> 00:51:00,759
we develop software AI software anyway

1180
00:50:59,140 --> 00:51:02,500
so I'm let me just read because I think

1181
00:51:00,760 --> 00:51:05,170
it might be interesting it need to be

1182
00:51:02,500 --> 00:51:07,390
designed to assist humanity coming back

1183
00:51:05,170 --> 00:51:09,280
we shouldn't replace your manager we

1184
00:51:07,390 --> 00:51:11,290
should assist humanity it should be

1185
00:51:09,280 --> 00:51:13,420
transparent in this creates all this

1186
00:51:11,290 --> 00:51:15,400
notion around algorithmic transparency

1187
00:51:13,420 --> 00:51:17,590
algorithmic accountability everything

1188
00:51:15,400 --> 00:51:19,960
which is I think a topic which need to

1189
00:51:17,590 --> 00:51:22,420
be debated then maximize efficiencies

1190
00:51:19,960 --> 00:51:24,580
without destroying the dignity of people

1191
00:51:22,420 --> 00:51:27,640
is an important component that they

1192
00:51:24,580 --> 00:51:29,740
don't roll over the dignity of people by

1193
00:51:27,640 --> 00:51:31,569
saying look technology's making decision

1194
00:51:29,740 --> 00:51:33,129
who is going to be promoted in a company

1195
00:51:31,570 --> 00:51:35,320
or not for instance it's a dignity

1196
00:51:33,130 --> 00:51:37,870
problem not the kind of productivity

1197
00:51:35,320 --> 00:51:39,910
problem in this context it should be

1198
00:51:37,870 --> 00:51:41,319
designed for privacy and I do think

1199
00:51:39,910 --> 00:51:43,509
Europe is really at the leading

1200
00:51:41,319 --> 00:51:46,989
globally at the leading edge to sing

1201
00:51:43,509 --> 00:51:49,180
through how in a digital based economy

1202
00:51:46,989 --> 00:51:51,519
previous in data protection should be

1203
00:51:49,180 --> 00:51:53,558
taken to the next level and I do applaud

1204
00:51:51,519 --> 00:51:55,180
the Commission's effort not only to

1205
00:51:53,559 --> 00:51:56,559
create a digital single market political

1206
00:51:55,180 --> 00:52:00,368
vision which I think is the right

1207
00:51:56,559 --> 00:52:02,559
strategy to do but also to create a kind

1208
00:52:00,369 --> 00:52:04,059
of common understanding that 28

1209
00:52:02,559 --> 00:52:05,969
different date of election rules don't

1210
00:52:04,059 --> 00:52:08,680
support the vision that we should have

1211
00:52:05,969 --> 00:52:11,049
around state of protection away so how

1212
00:52:08,680 --> 00:52:13,598
do we basically leverage then this

1213
00:52:11,049 --> 00:52:15,459
progress on the data protection side and

1214
00:52:13,599 --> 00:52:18,819
gives it a tension point in data

1215
00:52:15,459 --> 00:52:21,098
protection versus a eye health scare the

1216
00:52:18,819 --> 00:52:23,499
European data protection regulation just

1217
00:52:21,099 --> 00:52:26,499
to be clear Microsoft fully supports the

1218
00:52:23,499 --> 00:52:29,169
the journey towards data protection but

1219
00:52:26,499 --> 00:52:30,759
I want to point out the kind of conflict

1220
00:52:29,170 --> 00:52:33,849
of policies let's call it this way

1221
00:52:30,759 --> 00:52:36,309
health care most European countries are

1222
00:52:33,849 --> 00:52:37,779
challenged from a budget perspective to

1223
00:52:36,309 --> 00:52:40,359
deliver the health care services

1224
00:52:37,779 --> 00:52:42,519
efficiently to a growing stakeholder of

1225
00:52:40,359 --> 00:52:44,650
people who need support and we have a

1226
00:52:42,519 --> 00:52:46,269
growing group of elderly people which

1227
00:52:44,650 --> 00:52:49,229
need to be supported well in the future

1228
00:52:46,269 --> 00:52:52,238
as well machine learning and America

1229
00:52:49,229 --> 00:52:54,968
pointed out need huge amount of data to

1230
00:52:52,239 --> 00:52:56,440
be sophisticated way how they understand

1231
00:52:54,969 --> 00:52:58,660
and contribute to solving these problems

1232
00:52:56,440 --> 00:53:01,440
we do have a concept in the data

1233
00:52:58,660 --> 00:53:04,089
protection there are regulation which is

1234
00:53:01,440 --> 00:53:06,640
based on the content giving that you as

1235
00:53:04,089 --> 00:53:08,859
a data subject as a citizen can define

1236
00:53:06,640 --> 00:53:11,440
what the processor is doing this your

1237
00:53:08,859 --> 00:53:13,390
data so if you are a healthcare provider

1238
00:53:11,440 --> 00:53:15,219
if you are a university working in

1239
00:53:13,390 --> 00:53:17,549
healthcare research and the objective is

1240
00:53:15,219 --> 00:53:20,979
to reduce the European let's say

1241
00:53:17,549 --> 00:53:23,229
fatality on cancer obesity or whatever

1242
00:53:20,979 --> 00:53:25,149
kind of thing you need access to data

1243
00:53:23,229 --> 00:53:27,759
and so what is the proper balance

1244
00:53:25,150 --> 00:53:29,979
between on one side protecting debate

1245
00:53:27,759 --> 00:53:33,160
the data for the individual patient on

1246
00:53:29,979 --> 00:53:34,989
one side versus allowing researchers to

1247
00:53:33,160 --> 00:53:38,680
get to the next level helping to reduce

1248
00:53:34,989 --> 00:53:40,509
our fatality rate on cancer by 50% in a

1249
00:53:38,680 --> 00:53:42,759
way and I don't know what the proper

1250
00:53:40,509 --> 00:53:45,309
solution is but we want to contribute to

1251
00:53:42,759 --> 00:53:46,869
this awareness this need to be discussed

1252
00:53:45,309 --> 00:53:48,549
because if you would be on the extreme

1253
00:53:46,869 --> 00:53:50,170
and saying everything should be

1254
00:53:48,549 --> 00:53:52,839
protected nobody should care about

1255
00:53:50,170 --> 00:53:54,290
versus everything should be I think both

1256
00:53:52,839 --> 00:53:56,509
extremes are not the right

1257
00:53:54,290 --> 00:53:58,009
similar to the kind of community

1258
00:53:56,510 --> 00:54:00,530
thinking you have in a small

1259
00:53:58,010 --> 00:54:02,570
municipality if you have one miscibility

1260
00:54:00,530 --> 00:54:05,060
where everybody is trying to optimize

1261
00:54:02,570 --> 00:54:07,970
his own life individually I think

1262
00:54:05,060 --> 00:54:09,799
welfare safety is lower progressing

1263
00:54:07,970 --> 00:54:11,689
versus a community where a couple of

1264
00:54:09,800 --> 00:54:13,280
things are being shared and agreed upon

1265
00:54:11,690 --> 00:54:14,780
who is doing the infrastructure

1266
00:54:13,280 --> 00:54:16,610
investment on building the bridge on

1267
00:54:14,780 --> 00:54:19,190
building the kind of Road and reducing

1268
00:54:16,610 --> 00:54:21,650
even the data-driven economy we need to

1269
00:54:19,190 --> 00:54:23,900
create this kind of balancing point what

1270
00:54:21,650 --> 00:54:25,340
should be shared and which condition how

1271
00:54:23,900 --> 00:54:27,380
protected but it should create

1272
00:54:25,340 --> 00:54:29,440
opportunity everyone I think this

1273
00:54:27,380 --> 00:54:31,820
question about democracy and

1274
00:54:29,440 --> 00:54:33,950
democratizing technology is really core

1275
00:54:31,820 --> 00:54:36,950
to the question of what happens with AI

1276
00:54:33,950 --> 00:54:38,359
in the future and it's this question of

1277
00:54:36,950 --> 00:54:40,279
Rossum right like the individual

1278
00:54:38,360 --> 00:54:43,070
entrepreneur makes a pile of money and

1279
00:54:40,280 --> 00:54:46,190
everyone else uses the service but the

1280
00:54:43,070 --> 00:54:47,870
beans of production aren't shared and so

1281
00:54:46,190 --> 00:54:50,770
the wealth is concentrated there's

1282
00:54:47,870 --> 00:54:53,420
another very good sort of fictional

1283
00:54:50,770 --> 00:54:55,130
recreation of that it's called player

1284
00:54:53,420 --> 00:54:56,840
piano by Kurt Vonnegut where he

1285
00:54:55,130 --> 00:54:58,880
envisions a single corporation that has

1286
00:54:56,840 --> 00:55:00,410
like 300 employees that provides robotic

1287
00:54:58,880 --> 00:55:02,450
services to the entire world

1288
00:55:00,410 --> 00:55:05,450
great great book he originally wrote it

1289
00:55:02,450 --> 00:55:07,640
about IPM and it's this question of

1290
00:55:05,450 --> 00:55:11,000
market capitalization per employee in a

1291
00:55:07,640 --> 00:55:13,970
software company versus output so you

1292
00:55:11,000 --> 00:55:14,560
know Microsoft has right at the time he

1293
00:55:13,970 --> 00:55:17,899
wrote it

1294
00:55:14,560 --> 00:55:21,670
IBM was making about $300,000 per

1295
00:55:17,900 --> 00:55:25,220
employee today Microsoft makes about

1296
00:55:21,670 --> 00:55:26,990
700,000 per employee and Google makes

1297
00:55:25,220 --> 00:55:31,040
over a million dollars per employee

1298
00:55:26,990 --> 00:55:33,250
so tumeric as you know the young Rossum

1299
00:55:31,040 --> 00:55:35,870
is you know the voice of AI

1300
00:55:33,250 --> 00:55:40,030
entrepreneurs of the future how do you

1301
00:55:35,870 --> 00:55:42,529
approach this question of sharing

1302
00:55:40,030 --> 00:55:45,860
potentially enormous wealth from future

1303
00:55:42,530 --> 00:55:48,230
AI breakthroughs and for instance are

1304
00:55:45,860 --> 00:55:51,140
you a fan of the idea of universal basic

1305
00:55:48,230 --> 00:55:54,200
income what should we be paying

1306
00:55:51,140 --> 00:55:57,259
attention to policy-wise distribution

1307
00:55:54,200 --> 00:55:59,330
wise what should we avoid yeah I think

1308
00:55:57,260 --> 00:56:03,720
is a good question also regarding the

1309
00:55:59,330 --> 00:56:09,098
universal basic income I think it's a

1310
00:56:03,720 --> 00:56:12,399
maybe maybe it's not the best answer to

1311
00:56:09,099 --> 00:56:14,500
this problem but at least it's a is

1312
00:56:12,400 --> 00:56:16,750
helping to ask the right question and

1313
00:56:14,500 --> 00:56:18,700
like how to help people in the future

1314
00:56:16,750 --> 00:56:20,859
decades when the automation will

1315
00:56:18,700 --> 00:56:22,899
basically take all the jobs because the

1316
00:56:20,859 --> 00:56:24,700
robots or AI will be able to do

1317
00:56:22,900 --> 00:56:28,420
everything better than people and

1318
00:56:24,700 --> 00:56:31,868
cheaper faster and I think the economy

1319
00:56:28,420 --> 00:56:34,450
will just not let us do anything else

1320
00:56:31,869 --> 00:56:36,849
than let the robots do the do the work

1321
00:56:34,450 --> 00:56:39,490
even the creative work and so in this

1322
00:56:36,849 --> 00:56:41,650
situation I think something like maybe

1323
00:56:39,490 --> 00:56:45,549
not exactly universal basic income but

1324
00:56:41,650 --> 00:56:47,890
some kind of altruism with the other

1325
00:56:45,549 --> 00:56:50,799
people will come in hand of course there

1326
00:56:47,890 --> 00:56:53,470
are still will be people exchanging some

1327
00:56:50,799 --> 00:56:56,349
work between each other and exchanging

1328
00:56:53,470 --> 00:56:59,140
the value and so on but maybe a majority

1329
00:56:56,349 --> 00:57:01,900
of the people will not need to work

1330
00:56:59,140 --> 00:57:05,859
because the robots or the AI will be

1331
00:57:01,900 --> 00:57:09,099
able to provide all the value for us you

1332
00:57:05,859 --> 00:57:10,960
know so it will be like robots producing

1333
00:57:09,099 --> 00:57:14,740
all these things and people just using

1334
00:57:10,960 --> 00:57:15,940
it I think it's not a simple as I just

1335
00:57:14,740 --> 00:57:18,009
said like of course it will be a little

1336
00:57:15,940 --> 00:57:19,869
bit more complex and regarding the

1337
00:57:18,010 --> 00:57:23,289
sharing of this like if one group of

1338
00:57:19,869 --> 00:57:25,779
people we all have the robots will be

1339
00:57:23,289 --> 00:57:27,460
the first in the literature it's called

1340
00:57:25,779 --> 00:57:31,029
a singleton you know like you will have

1341
00:57:27,460 --> 00:57:32,410
the first strongest AI and then there

1342
00:57:31,029 --> 00:57:35,230
are different scenarios possible because

1343
00:57:32,410 --> 00:57:37,328
the AI are the people who control it can

1344
00:57:35,230 --> 00:57:39,700
really start making sure that there will

1345
00:57:37,329 --> 00:57:42,430
not be the other AIS you know that's one

1346
00:57:39,700 --> 00:57:45,549
risk another is that there will be some

1347
00:57:42,430 --> 00:57:48,640
internal conflict even in this group you

1348
00:57:45,549 --> 00:57:50,170
know which can go in the wrong way there

1349
00:57:48,640 --> 00:57:52,839
can be a problem with value of alignment

1350
00:57:50,170 --> 00:57:55,960
so the AI will not have its values

1351
00:57:52,839 --> 00:57:58,180
aligned even with its creators it is can

1352
00:57:55,960 --> 00:58:00,309
be problem or understanding problem

1353
00:57:58,180 --> 00:58:02,558
which is that AI will not really

1354
00:58:00,309 --> 00:58:04,000
understand what the creator's wanted to

1355
00:58:02,559 --> 00:58:08,200
do you know there will be some confusion

1356
00:58:04,000 --> 00:58:14,020
and if you let it run for a few cycles

1357
00:58:08,200 --> 00:58:15,600
and the cycles are like building on the

1358
00:58:14,020 --> 00:58:17,490
previous cycles and so on it

1359
00:58:15,600 --> 00:58:19,830
very hard to foresee where the system

1360
00:58:17,490 --> 00:58:23,419
will actually evolve and how damaging it

1361
00:58:19,830 --> 00:58:26,730
can be so I think it's a good question

1362
00:58:23,420 --> 00:58:29,400
my personal answer to this is that if

1363
00:58:26,730 --> 00:58:32,730
for example we in my company get to

1364
00:58:29,400 --> 00:58:35,700
develop general AI first then we would

1365
00:58:32,730 --> 00:58:38,340
try to help as many people as possible

1366
00:58:35,700 --> 00:58:41,129
with the fruits of this technology like

1367
00:58:38,340 --> 00:58:43,320
I could cannot talk about or for the

1368
00:58:41,130 --> 00:58:45,360
other people for the other companies or

1369
00:58:43,320 --> 00:58:48,330
countries that are developing this but

1370
00:58:45,360 --> 00:58:51,360
this is we this would be my my Gaul so

1371
00:58:48,330 --> 00:58:53,370
again I would of course use the AI for

1372
00:58:51,360 --> 00:59:00,230
the stuff that I'm interested in but I

1373
00:58:53,370 --> 00:59:03,450
will also help the other people and but

1374
00:59:00,230 --> 00:59:05,610
like people may not believe me or trust

1375
00:59:03,450 --> 00:59:08,990
me like that's all possible so I think

1376
00:59:05,610 --> 00:59:12,210
it's also very interesting to start

1377
00:59:08,990 --> 00:59:14,040
investigating this let's call it like

1378
00:59:12,210 --> 00:59:16,680
game theoretical scenarios you know

1379
00:59:14,040 --> 00:59:19,380
where they're different actors they are

1380
00:59:16,680 --> 00:59:21,419
a different level of progress different

1381
00:59:19,380 --> 00:59:24,210
level of trust between each other and so

1382
00:59:21,420 --> 00:59:25,980
on and at this moment I don't have a

1383
00:59:24,210 --> 00:59:29,790
clear answer how to make sure that we

1384
00:59:25,980 --> 00:59:34,050
can safely go through all these stages

1385
00:59:29,790 --> 00:59:35,880
and I don't know no one who has the

1386
00:59:34,050 --> 00:59:37,920
right answer I the people that are

1387
00:59:35,880 --> 00:59:39,660
interested in AI safety and these air

1388
00:59:37,920 --> 00:59:42,660
futures and stuff they're still looking

1389
00:59:39,660 --> 00:59:45,149
for the right ways how to mitigate this

1390
00:59:42,660 --> 00:59:46,799
yeah these risks well I think it's a bit

1391
00:59:45,150 --> 00:59:48,480
of a bit of a jungle right now really

1392
00:59:46,800 --> 00:59:50,430
because what we're talking about is a

1393
00:59:48,480 --> 00:59:53,100
lot of potential wealth concentrated in

1394
00:59:50,430 --> 00:59:55,259
the hands of very few people and at some

1395
00:59:53,100 --> 00:59:58,049
point there might be a bit of a conflict

1396
00:59:55,260 --> 00:59:59,310
internally about democratizing services

1397
00:59:58,050 --> 01:00:01,770
as you say in a way that's broadly

1398
00:59:59,310 --> 01:00:03,450
beneficial to humanity that perhaps eats

1399
01:00:01,770 --> 01:00:05,150
into the bottom line right now that's

1400
01:00:03,450 --> 01:00:07,250
made it sort of a company by company

1401
01:00:05,150 --> 01:00:09,840
entrepreneur by entrepreneur level

1402
01:00:07,250 --> 01:00:11,640
absent of any larger legislation or tax

1403
01:00:09,840 --> 01:00:12,960
agenda and in the future this might

1404
01:00:11,640 --> 01:00:15,060
change but let's go to the audience for

1405
01:00:12,960 --> 01:00:18,320
another round of questions you've been

1406
01:00:15,060 --> 01:00:18,320
very patient right here please

1407
01:00:19,599 --> 01:00:24,219
as a reward can I have two questions

1408
01:00:22,269 --> 01:00:26,019
well if we have time perhaps okay my

1409
01:00:24,219 --> 01:00:31,539
name is MJ Cox digital intelligence

1410
01:00:26,019 --> 01:00:34,868
Slovakia policymaking 130 years ago

1411
01:00:31,539 --> 01:00:38,289
there were cars that were only used by

1412
01:00:34,869 --> 01:00:40,209
very few nowadays we have education for

1413
01:00:38,289 --> 01:00:43,989
even kids in primary schools even

1414
01:00:40,209 --> 01:00:48,308
kindergarten about traffic traffic rules

1415
01:00:43,989 --> 01:00:49,569
right how do we join all the discussion

1416
01:00:48,309 --> 01:00:51,670
that we are now having with the

1417
01:00:49,569 --> 01:00:53,920
education because we all know that it's

1418
01:00:51,670 --> 01:00:56,650
digital skills should be also

1419
01:00:53,920 --> 01:01:00,699
accompanied by digital literacy skills

1420
01:00:56,650 --> 01:01:03,459
so is there a push thank you and the

1421
01:01:00,699 --> 01:01:06,549
second one is at the moment we mentioned

1422
01:01:03,459 --> 01:01:10,269
software nowadays we think about cyber

1423
01:01:06,549 --> 01:01:12,549
attacks so for example do you use white

1424
01:01:10,269 --> 01:01:16,180
heads also thinking about you know white

1425
01:01:12,549 --> 01:01:19,749
our testing who is going to hack a

1426
01:01:16,180 --> 01:01:21,640
soldering car thank you very good

1427
01:01:19,749 --> 01:01:23,890
questions how do we educate the youth of

1428
01:01:21,640 --> 01:01:29,440
the future for a future dominated by

1429
01:01:23,890 --> 01:01:33,699
artificial intelligence I just allow me

1430
01:01:29,440 --> 01:01:38,380
just to comment on the the the subject

1431
01:01:33,699 --> 01:01:40,859
of democratization I think that was of

1432
01:01:38,380 --> 01:01:43,869
AI and then I because it's related to

1433
01:01:40,859 --> 01:01:46,989
this the skills in cybersecurity as well

1434
01:01:43,869 --> 01:01:49,959
but just to say that you know in terms

1435
01:01:46,989 --> 01:01:54,999
of regulation of policymaking how to

1436
01:01:49,959 --> 01:02:00,839
handle the issue of dominance well you

1437
01:01:54,999 --> 01:02:03,910
know the term and our market economies

1438
01:02:00,839 --> 01:02:06,759
have market dominance is not the issue

1439
01:02:03,910 --> 01:02:10,899
is the views of your dominant position

1440
01:02:06,759 --> 01:02:13,479
to stifle competition and we have ndu

1441
01:02:10,900 --> 01:02:17,739
one of the you know most efficient

1442
01:02:13,479 --> 01:02:22,839
competition policy has shown with time

1443
01:02:17,739 --> 01:02:24,270
and there are means to block and to

1444
01:02:22,839 --> 01:02:27,390
block

1445
01:02:24,270 --> 01:02:29,759
their views of market dominance as we've

1446
01:02:27,390 --> 01:02:32,129
seen recently even with large companies

1447
01:02:29,760 --> 01:02:37,050
and giants that sometimes seem to be

1448
01:02:32,130 --> 01:02:38,790
unattackable and the I think the first

1449
01:02:37,050 --> 01:02:41,520
thing we're looking at the policy making

1450
01:02:38,790 --> 01:02:45,020
is to make sure that we're still with

1451
01:02:41,520 --> 01:02:48,480
our competition mechanism competition

1452
01:02:45,020 --> 01:02:50,790
rules and the mechanism behind them we

1453
01:02:48,480 --> 01:02:53,160
can cope with the fast development of

1454
01:02:50,790 --> 01:02:54,900
Technology and think it needs to some

1455
01:02:53,160 --> 01:02:58,560
adaptation to make sure that we can

1456
01:02:54,900 --> 01:03:01,230
exposed first using competition rule

1457
01:02:58,560 --> 01:03:03,420
avoid the blocking of innovation because

1458
01:03:01,230 --> 01:03:05,610
that's the key issue in order not to

1459
01:03:03,420 --> 01:03:08,040
have it to have innovation value

1460
01:03:05,610 --> 01:03:12,360
creation and the happy few yeah you need

1461
01:03:08,040 --> 01:03:15,390
to make sure that you're the innovation

1462
01:03:12,360 --> 01:03:17,370
is possible for the widest but factors

1463
01:03:15,390 --> 01:03:19,710
range is very important for us now we

1464
01:03:17,370 --> 01:03:22,980
can't do it expose you could do it

1465
01:03:19,710 --> 01:03:25,910
ex-ante and this is you know the things

1466
01:03:22,980 --> 01:03:28,710
we're looking at so try to find means

1467
01:03:25,910 --> 01:03:31,080
ex-ante because you cannot cope goes

1468
01:03:28,710 --> 01:03:33,030
very fast to expose you could do it

1469
01:03:31,080 --> 01:03:35,610
extent in terms of policy making and

1470
01:03:33,030 --> 01:03:37,140
terms of rules as we've done in telecom

1471
01:03:35,610 --> 01:03:39,660
market we're currently looking at data

1472
01:03:37,140 --> 01:03:41,879
and see whether for data because that's

1473
01:03:39,660 --> 01:03:43,560
the issue ultimately for the question

1474
01:03:41,880 --> 01:03:45,660
from our colleagues from Toronto yeah

1475
01:03:43,560 --> 01:03:47,250
the it's the issue of access to data and

1476
01:03:45,660 --> 01:03:48,779
what you could do with data we're

1477
01:03:47,250 --> 01:03:51,390
looking at data whether we need to

1478
01:03:48,780 --> 01:03:54,090
legislate ex-ante and then the coming

1479
01:03:51,390 --> 01:03:56,549
year we will be coming with proposals

1480
01:03:54,090 --> 01:03:59,250
whether we need to legislate exactly on

1481
01:03:56,550 --> 01:04:01,620
the access to data and on other things

1482
01:03:59,250 --> 01:04:05,340
that are related to the tools for data

1483
01:04:01,620 --> 01:04:07,500
oh no oh that's one way of ensuring in a

1484
01:04:05,340 --> 01:04:10,110
market economy that you're not killing

1485
01:04:07,500 --> 01:04:13,710
innovation or concentration the value of

1486
01:04:10,110 --> 01:04:15,890
innovation and to the apogee in case

1487
01:04:13,710 --> 01:04:18,990
there is an abuse of market dominance

1488
01:04:15,890 --> 01:04:22,170
that's one thing we're really following

1489
01:04:18,990 --> 01:04:24,180
very very carefully and I think the this

1490
01:04:22,170 --> 01:04:28,110
is one of us Rajeev og single market as

1491
01:04:24,180 --> 01:04:32,460
andreas mentioned and it's a big

1492
01:04:28,110 --> 01:04:33,069
challenge but we at ndu we're working on

1493
01:04:32,460 --> 01:04:35,679
it

1494
01:04:33,069 --> 01:04:39,880
I think on on educational skills you're

1495
01:04:35,679 --> 01:04:42,160
totally right it is the key issue what

1496
01:04:39,880 --> 01:04:44,289
we've done since this

1497
01:04:42,160 --> 01:04:46,739
I mean we've since three years now we

1498
01:04:44,289 --> 01:04:49,089
have created sort of coalition between

1499
01:04:46,739 --> 01:04:51,599
industry and the member states around

1500
01:04:49,089 --> 01:04:54,759
the digital skills and how they could be

1501
01:04:51,599 --> 01:04:57,009
brought in at the earliest level how can

1502
01:04:54,759 --> 01:05:00,429
they be also part of the lifelong

1503
01:04:57,009 --> 01:05:02,650
learning across tu and it's something

1504
01:05:00,429 --> 01:05:04,929
now that we see it in all member states

1505
01:05:02,650 --> 01:05:07,049
with reflection how introduce discus as

1506
01:05:04,929 --> 01:05:09,309
quickly as possible but it's not only

1507
01:05:07,049 --> 01:05:11,140
you know public sector it's also

1508
01:05:09,309 --> 01:05:18,880
something that the private sector is

1509
01:05:11,140 --> 01:05:24,009
helping doing I think the we will see

1510
01:05:18,880 --> 01:05:28,479
changes in the coming our euro to our

1511
01:05:24,009 --> 01:05:30,429
education systems across the EU and I've

1512
01:05:28,479 --> 01:05:36,549
seen it from the plans that the member

1513
01:05:30,429 --> 01:05:39,939
states are having now to to update our

1514
01:05:36,549 --> 01:05:42,719
educational systems and we see digital

1515
01:05:39,939 --> 01:05:45,908
skills now as part of all life on

1516
01:05:42,719 --> 01:05:48,819
lifelong learning and schemes across the

1517
01:05:45,909 --> 01:05:51,099
EU I think it's a very important will

1518
01:05:48,819 --> 01:05:52,929
continue with us the the problem is that

1519
01:05:51,099 --> 01:05:55,569
digital skills were introduced as

1520
01:05:52,929 --> 01:05:58,419
gadgets and I'll say you know additions

1521
01:05:55,569 --> 01:06:03,069
add in into educational at the beginning

1522
01:05:58,419 --> 01:06:07,629
and in the last ten years and that with

1523
01:06:03,069 --> 01:06:09,609
very bad experiences digital skills will

1524
01:06:07,630 --> 01:06:12,729
have to be taught in the future as you

1525
01:06:09,609 --> 01:06:14,409
teach mathematics physics all languages

1526
01:06:12,729 --> 01:06:16,808
they have to be part of our medical

1527
01:06:14,409 --> 01:06:18,759
curriculum and by not I mean yeah so I

1528
01:06:16,809 --> 01:06:20,439
think and that's coming yeah that's

1529
01:06:18,759 --> 01:06:22,329
going well I want to I want to ask that

1530
01:06:20,439 --> 01:06:24,549
same thing of Merit because I have a

1531
01:06:22,329 --> 01:06:25,529
sense of what digital skills is but I'm

1532
01:06:24,549 --> 01:06:27,880
not sure we're teaching the right

1533
01:06:25,529 --> 01:06:30,159
digital skills when I was a kid that my

1534
01:06:27,880 --> 01:06:32,589
digital skill class was typing like you

1535
01:06:30,159 --> 01:06:34,479
learned how that type it was like kind

1536
01:06:32,589 --> 01:06:36,640
of a crazy thing the idea of teaching a

1537
01:06:34,479 --> 01:06:38,529
kid now Pascal or even a very particular

1538
01:06:36,640 --> 01:06:40,709
programming language even if it's I

1539
01:06:38,529 --> 01:06:44,769
thought seems like it you might be

1540
01:06:40,709 --> 01:06:46,540
sacrificing creativity to focus on one

1541
01:06:44,769 --> 01:06:47,589
particular method of approaching

1542
01:06:46,540 --> 01:06:49,690
technology that's going to be obsolete

1543
01:06:47,590 --> 01:06:52,030
in five years because of accelerations

1544
01:06:49,690 --> 01:06:53,620
in technology so America to your mind

1545
01:06:52,030 --> 01:06:55,990
what are the most important digital

1546
01:06:53,620 --> 01:06:57,910
skills to teach kids at a level that

1547
01:06:55,990 --> 01:07:00,790
where they might be able to do something

1548
01:06:57,910 --> 01:07:03,040
with them so maybe instead of focusing

1549
01:07:00,790 --> 01:07:05,230
on some really very specific and let's

1550
01:07:03,040 --> 01:07:07,480
go a like a narrow digital skills I

1551
01:07:05,230 --> 01:07:10,120
would focus on the general skills so

1552
01:07:07,480 --> 01:07:11,560
just like with this general AI that we

1553
01:07:10,120 --> 01:07:14,650
are building they're kind of avoiding

1554
01:07:11,560 --> 01:07:17,080
the narrow approach because every narrow

1555
01:07:14,650 --> 01:07:19,570
domain that you master will become

1556
01:07:17,080 --> 01:07:21,430
obsolete very shortly like this the

1557
01:07:19,570 --> 01:07:23,260
world we are going to live in and we are

1558
01:07:21,430 --> 01:07:26,680
already in this world and it will be

1559
01:07:23,260 --> 01:07:28,750
just accelerating so what is a more

1560
01:07:26,680 --> 01:07:30,850
efficient is to be expert at this field

1561
01:07:28,750 --> 01:07:32,680
this field is still or to be an expert

1562
01:07:30,850 --> 01:07:36,250
how to become an expert in any field

1563
01:07:32,680 --> 01:07:38,680
that I may need in the future or to put

1564
01:07:36,250 --> 01:07:40,960
it under way it's like learn learn how

1565
01:07:38,680 --> 01:07:43,089
to learn or learn to learn you know

1566
01:07:40,960 --> 01:07:47,080
these meta skills and I think they are

1567
01:07:43,090 --> 01:07:47,410
more important than than knowing Pascal

1568
01:07:47,080 --> 01:07:50,080
or

1569
01:07:47,410 --> 01:07:52,180
those or Windows or something like this

1570
01:07:50,080 --> 01:07:54,810
because like this day is one thing

1571
01:07:52,180 --> 01:07:57,419
another day if you believe anything and

1572
01:07:54,810 --> 01:08:03,310
there has been actually some some

1573
01:07:57,420 --> 01:08:05,830
studies where when they trained students

1574
01:08:03,310 --> 01:08:08,799
or when they started to teach them how

1575
01:08:05,830 --> 01:08:10,569
to learn their capacity to learn the

1576
01:08:08,800 --> 01:08:13,180
specific topics actually increased

1577
01:08:10,570 --> 01:08:15,790
because they realized what skills they

1578
01:08:13,180 --> 01:08:17,649
are using to learn new skills it wasn't

1579
01:08:15,790 --> 01:08:19,600
just like I need to learn this thing so

1580
01:08:17,649 --> 01:08:21,818
I will memorize these you know so I can

1581
01:08:19,600 --> 01:08:24,250
pass the exams but actually I will learn

1582
01:08:21,819 --> 01:08:26,400
that if I use this skill to understand

1583
01:08:24,250 --> 01:08:29,799
this thing and then other other skill to

1584
01:08:26,399 --> 01:08:34,269
to maybe have some dialogue with someone

1585
01:08:29,799 --> 01:08:37,299
and learn that thing using that way my

1586
01:08:34,270 --> 01:08:40,150
ability to learn any new problem or to

1587
01:08:37,299 --> 01:08:41,710
adopt novel and unseen problems will be

1588
01:08:40,149 --> 01:08:43,899
much better so this is more important

1589
01:08:41,710 --> 01:08:47,620
for me so just like I don't want to be

1590
01:08:43,899 --> 01:08:50,170
building narrow AI I would also not be

1591
01:08:47,620 --> 01:08:52,269
like if I was a student I would not be

1592
01:08:50,170 --> 01:08:54,250
investing my time in learning just some

1593
01:08:52,270 --> 01:08:56,770
specific things but I would rather focus

1594
01:08:54,250 --> 01:08:58,029
on how to master the meta skills mm-hmm

1595
01:08:56,770 --> 01:09:00,250
I sound like a wonderful plug for

1596
01:08:58,029 --> 01:09:03,279
humanities education which I'm a big fan

1597
01:09:00,250 --> 01:09:04,479
of and so let's have another question

1598
01:09:03,279 --> 01:09:06,130
here from the audience while we still

1599
01:09:04,479 --> 01:09:08,290
have a little bit of time right here in

1600
01:09:06,130 --> 01:09:09,819
the second row I think that was there

1601
01:09:08,290 --> 01:09:12,130
was a question on side but on suppose it

1602
01:09:09,819 --> 01:09:13,600
said oh yeah sorry on cyber security I

1603
01:09:12,130 --> 01:09:14,830
artificial intelligence - cyber security

1604
01:09:13,600 --> 01:09:16,510
and then we'll go to another question as

1605
01:09:14,830 --> 01:09:18,220
I go ahead yes Sinha

1606
01:09:16,510 --> 01:09:20,620
yes this is Microsoft here this is very

1607
01:09:18,220 --> 01:09:22,270
good for me yeah yeah thank you so much

1608
01:09:20,620 --> 01:09:25,000
for asking a question because I saw the

1609
01:09:22,270 --> 01:09:28,299
title of this whole conference is global

1610
01:09:25,000 --> 01:09:30,520
security so cyber security is of course

1611
01:09:28,299 --> 01:09:33,370
a hugely important topic and in a way

1612
01:09:30,520 --> 01:09:34,990
and it fans back into the country

1613
01:09:33,370 --> 01:09:37,029
education thing as well because digital

1614
01:09:34,990 --> 01:09:40,089
literacy is one of the preconditions

1615
01:09:37,029 --> 01:09:42,099
that you know how to cross the digital

1616
01:09:40,089 --> 01:09:44,380
highway safely in a way and what we've

1617
01:09:42,100 --> 01:09:46,950
seen is that for instance over the and

1618
01:09:44,380 --> 01:09:50,290
you mentioned cars here 100 years ago

1619
01:09:46,950 --> 01:09:51,790
our generations had a kind of

1620
01:09:50,290 --> 01:09:53,620
perspectives that parents are in a

1621
01:09:51,790 --> 01:09:56,560
position to help kids learn how to

1622
01:09:53,620 --> 01:09:58,630
safely cross street wait until the car

1623
01:09:56,560 --> 01:10:00,460
is passing by and maybe the car is fast

1624
01:09:58,630 --> 01:10:03,040
or slow and then you can go before after

1625
01:10:00,460 --> 01:10:05,530
the car what we experiences is a one

1626
01:10:03,040 --> 01:10:08,500
generation kind of gap that the parental

1627
01:10:05,530 --> 01:10:10,660
generation is not experienced in

1628
01:10:08,500 --> 01:10:14,080
personal kind of experiences how to

1629
01:10:10,660 --> 01:10:16,330
cross digital save there's a highway

1630
01:10:14,080 --> 01:10:19,540
safer in a way so I do think that when

1631
01:10:16,330 --> 01:10:21,550
our kids our parents this kind of gap is

1632
01:10:19,540 --> 01:10:24,400
closing down it might not go away but

1633
01:10:21,550 --> 01:10:26,440
they have any embedded knowledge how not

1634
01:10:24,400 --> 01:10:28,330
to click on a suspicious mail on an

1635
01:10:26,440 --> 01:10:30,099
attachment or this kind of things which

1636
01:10:28,330 --> 01:10:32,320
currently parents are rather clicking

1637
01:10:30,100 --> 01:10:34,480
and the kids are saying that don't do

1638
01:10:32,320 --> 01:10:36,820
that kind of thing so I do think that we

1639
01:10:34,480 --> 01:10:38,919
have a kind of unique period in time and

1640
01:10:36,820 --> 01:10:41,380
we are embarking on a journey where the

1641
01:10:38,920 --> 01:10:44,230
next generation have better embedded

1642
01:10:41,380 --> 01:10:46,450
skills recognizing potential suspicious

1643
01:10:44,230 --> 01:10:48,070
mails for instance an example coming

1644
01:10:46,450 --> 01:10:53,889
back to this island cybersecurity one

1645
01:10:48,070 --> 01:10:56,019
it's a global problem it's both thing

1646
01:10:53,890 --> 01:10:58,650
that technology can be used for both

1647
01:10:56,020 --> 01:11:00,179
things it could be used for defense

1648
01:10:58,650 --> 01:11:02,280
the fact is that you make sure that your

1649
01:11:00,179 --> 01:11:04,920
service is being used and the can't tell

1650
01:11:02,280 --> 01:11:07,440
you don't have enough time to list up

1651
01:11:04,920 --> 01:11:09,600
all the kind of advanced technologies we

1652
01:11:07,440 --> 01:11:11,549
are using to protect assets data assets

1653
01:11:09,600 --> 01:11:15,900
of our customers the technology the

1654
01:11:11,550 --> 01:11:18,780
system measure what's available on the

1655
01:11:15,900 --> 01:11:20,699
planetary write scale how threads are

1656
01:11:18,780 --> 01:11:22,650
moving from one region to the other ring

1657
01:11:20,699 --> 01:11:24,750
and artificial intelligences at the

1658
01:11:22,650 --> 01:11:27,120
centrally at the center to get this kind

1659
01:11:24,750 --> 01:11:29,460
of worldwide view what is happening and

1660
01:11:27,120 --> 01:11:29,849
we think we can contribute to this

1661
01:11:29,460 --> 01:11:31,650
debate

1662
01:11:29,850 --> 01:11:34,170
so we're partnering with the

1663
01:11:31,650 --> 01:11:36,629
law-enforcement agencies to take down

1664
01:11:34,170 --> 01:11:39,330
botnets in a way maybe see this global

1665
01:11:36,630 --> 01:11:41,699
pattern of it's a very tags coming from

1666
01:11:39,330 --> 01:11:43,980
on the other side you need to think that

1667
01:11:41,699 --> 01:11:46,530
the dark web on the dark side of

1668
01:11:43,980 --> 01:11:49,199
business is using very same approaches

1669
01:11:46,530 --> 01:11:52,259
they even have cloud infrastructures on

1670
01:11:49,199 --> 01:11:53,969
the dark side of economy using the very

1671
01:11:52,260 --> 01:11:55,800
same economic argument if you are an

1672
01:11:53,969 --> 01:11:58,920
attacker and you need very first short

1673
01:11:55,800 --> 01:12:01,620
period of time a kind of competitive

1674
01:11:58,920 --> 01:12:03,690
compute here is my offer that you can

1675
01:12:01,620 --> 01:12:05,219
rent our computers in a way or there's

1676
01:12:03,690 --> 01:12:07,710
this supply chain kind of methodology

1677
01:12:05,219 --> 01:12:10,130
way so many of the kind of approaches

1678
01:12:07,710 --> 01:12:13,500
being taken from the wide side kind of

1679
01:12:10,130 --> 01:12:16,140
the equation are being used on the dark

1680
01:12:13,500 --> 01:12:18,060
side as well so this one challenge in a

1681
01:12:16,140 --> 01:12:19,890
way but if you're making this kind of

1682
01:12:18,060 --> 01:12:22,409
assumption what is good what is not good

1683
01:12:19,890 --> 01:12:24,150
it's also the kind of ethical moral

1684
01:12:22,409 --> 01:12:25,620
question who is the judge what is good

1685
01:12:24,150 --> 01:12:29,009
what is not good because on one side

1686
01:12:25,620 --> 01:12:31,590
maybe in one law I'm sorry in one

1687
01:12:29,010 --> 01:12:33,270
jurisdiction some activities are being

1688
01:12:31,590 --> 01:12:35,010
considered and especially in cyber

1689
01:12:33,270 --> 01:12:37,770
attacks because sometimes they're

1690
01:12:35,010 --> 01:12:40,530
calling it in one country forwards

1691
01:12:37,770 --> 01:12:42,510
defense activities which are considered

1692
01:12:40,530 --> 01:12:45,409
attacks by other legal systems in a way

1693
01:12:42,510 --> 01:12:49,290
so I do think that a couple of things

1694
01:12:45,409 --> 01:12:52,710
potentially could happen over the period

1695
01:12:49,290 --> 01:12:55,140
of years number one if there's a global

1696
01:12:52,710 --> 01:12:57,690
problem it has been proven over and over

1697
01:12:55,140 --> 01:12:59,940
again that regionalised solutions don't

1698
01:12:57,690 --> 01:13:03,900
work well for addressing a global

1699
01:12:59,940 --> 01:13:06,000
problem so I do think that a global

1700
01:13:03,900 --> 01:13:09,540
approach like for instance a suggestion

1701
01:13:06,000 --> 01:13:11,520
which our chief our president and chief

1702
01:13:09,540 --> 01:13:13,050
legal officer brett Smith made the cop

1703
01:13:11,520 --> 01:13:15,510
months ago about the digital Geneva

1704
01:13:13,050 --> 01:13:17,550
Convention that we have a common

1705
01:13:15,510 --> 01:13:20,070
societal understanding what is allowed

1706
01:13:17,550 --> 01:13:21,990
in a way that they have basically same

1707
01:13:20,070 --> 01:13:23,969
understanding which then is mapped into

1708
01:13:21,990 --> 01:13:26,010
local regulatory frameworks that it is

1709
01:13:23,970 --> 01:13:26,910
prohibited by law that you're doing

1710
01:13:26,010 --> 01:13:29,220
something like that

1711
01:13:26,910 --> 01:13:31,410
is a precondition that we simplify this

1712
01:13:29,220 --> 01:13:34,230
complexity we're currently having in the

1713
01:13:31,410 --> 01:13:36,210
way yeah I think also if you're

1714
01:13:34,230 --> 01:13:37,679
interested in the future of applications

1715
01:13:36,210 --> 01:13:39,390
of artificial intelligence particularly

1716
01:13:37,680 --> 01:13:41,520
in cybersecurity you might Google the

1717
01:13:39,390 --> 01:13:43,680
dark bug grand cyber challenge which

1718
01:13:41,520 --> 01:13:45,240
translates last year and it was pretty

1719
01:13:43,680 --> 01:13:46,770
fascinating it showed that right now

1720
01:13:45,240 --> 01:13:48,690
sort of in line with the beginning of

1721
01:13:46,770 --> 01:13:52,650
this discussion that humans are still

1722
01:13:48,690 --> 01:13:55,559
very good actually better at at hacking

1723
01:13:52,650 --> 01:13:56,910
per se anomaly detection and anomaly

1724
01:13:55,560 --> 01:13:59,040
reporting that's something that can be

1725
01:13:56,910 --> 01:14:01,050
done at better scale but as specific

1726
01:13:59,040 --> 01:14:03,500
hacking is still much better done by

1727
01:14:01,050 --> 01:14:05,510
humans but humans paired with machines

1728
01:14:03,500 --> 01:14:07,860
this is sort of the future of

1729
01:14:05,510 --> 01:14:10,890
exploitation and thus also I think

1730
01:14:07,860 --> 01:14:13,740
defense so let's go here we've got not a

1731
01:14:10,890 --> 01:14:15,570
ton of time left but we'll try to get as

1732
01:14:13,740 --> 01:14:18,540
many as few as we can thank you

1733
01:14:15,570 --> 01:14:20,670
ma Melanesia it's a mr. tan Bratislava I

1734
01:14:18,540 --> 01:14:22,019
would like the panelists to dwell a

1735
01:14:20,670 --> 01:14:24,720
little bit on the concept of

1736
01:14:22,020 --> 01:14:26,310
responsibility okay because you know

1737
01:14:24,720 --> 01:14:29,610
when we have a machine machine follows a

1738
01:14:26,310 --> 01:14:32,070
command artificial intelligence makes

1739
01:14:29,610 --> 01:14:34,530
decisions on its own

1740
01:14:32,070 --> 01:14:35,969
Oh principally in other words when we

1741
01:14:34,530 --> 01:14:39,870
have a car which is winding on a

1742
01:14:35,970 --> 01:14:42,750
mountain road and it it sees a person

1743
01:14:39,870 --> 01:14:45,269
who is in the middle of the road without

1744
01:14:42,750 --> 01:14:47,310
the ability to stop so it has two

1745
01:14:45,270 --> 01:14:49,380
options either to brake or to take the

1746
01:14:47,310 --> 01:14:51,150
car of the cliff and kill the operator

1747
01:14:49,380 --> 01:14:53,250
and there are you know we have a

1748
01:14:51,150 --> 01:14:55,650
different notions of responsibility it

1749
01:14:53,250 --> 01:14:57,780
could be either the software producer

1750
01:14:55,650 --> 01:14:59,969
who has programmed the car it could be

1751
01:14:57,780 --> 01:15:02,969
the the producer of the car or it could

1752
01:14:59,970 --> 01:15:04,530
be the operator who has consented to use

1753
01:15:02,970 --> 01:15:05,940
the car which was programmed in a

1754
01:15:04,530 --> 01:15:08,000
specific way but it's still a machine

1755
01:15:05,940 --> 01:15:11,639
because it was program and followed an

1756
01:15:08,000 --> 01:15:14,240
algorithm how do we how do we take the

1757
01:15:11,640 --> 01:15:17,640
notion of artificial intelligence once

1758
01:15:14,240 --> 01:15:20,219
it has the ability to make decisions on

1759
01:15:17,640 --> 01:15:22,470
its own so meaning it's it's clear of

1760
01:15:20,220 --> 01:15:24,500
algorithms so what kind of commandments

1761
01:15:22,470 --> 01:15:26,300
are it's going to follow

1762
01:15:24,500 --> 01:15:30,590
how do we make sure because we are very

1763
01:15:26,300 --> 01:15:35,360
imperfect a race that they they use our

1764
01:15:30,590 --> 01:15:37,340
imperfections in order to to get even

1765
01:15:35,360 --> 01:15:38,719
with us okay so we've only got about ten

1766
01:15:37,340 --> 01:15:39,740
minutes left so invite everyone to be

1767
01:15:38,720 --> 01:15:41,210
brief because we want to get to as many

1768
01:15:39,740 --> 01:15:42,559
questions as possible go ahead the

1769
01:15:41,210 --> 01:15:47,020
question that we try to address under

1770
01:15:42,560 --> 01:15:53,060
the liability under the liability angle

1771
01:15:47,020 --> 01:15:54,680
just not to wrap up a bit what I said at

1772
01:15:53,060 --> 01:15:57,500
the beginning on unlabeled and how we're

1773
01:15:54,680 --> 01:16:01,270
trying to address it today it is rated

1774
01:15:57,500 --> 01:16:04,790
to the responsibility are absolutely so

1775
01:16:01,270 --> 01:16:06,860
first thing to do is to identify where

1776
01:16:04,790 --> 01:16:08,840
we do have the problem when it comes to

1777
01:16:06,860 --> 01:16:11,690
the autonomy itself because there could

1778
01:16:08,840 --> 01:16:14,090
be defects where the responsibilities

1779
01:16:11,690 --> 01:16:17,299
are ended you identify it so you need to

1780
01:16:14,090 --> 01:16:20,510
make sure that you we have in a complex

1781
01:16:17,300 --> 01:16:22,220
system like an autonomous vehicle where

1782
01:16:20,510 --> 01:16:25,070
we have artificial intelligence that we

1783
01:16:22,220 --> 01:16:27,680
can track first you have a tracking

1784
01:16:25,070 --> 01:16:29,929
system that could identify where is the

1785
01:16:27,680 --> 01:16:31,790
fact coming from and that is a part of

1786
01:16:29,930 --> 01:16:34,280
the defect that is related to the

1787
01:16:31,790 --> 01:16:37,280
learning and the learning to learn of

1788
01:16:34,280 --> 01:16:39,469
the machine itself where we are in a

1789
01:16:37,280 --> 01:16:44,990
sort of vacuum today that's the issue

1790
01:16:39,470 --> 01:16:47,510
and the the solution as I said is a

1791
01:16:44,990 --> 01:16:49,070
solution that we'll have to look at it

1792
01:16:47,510 --> 01:16:51,320
from the legal perspective but also from

1793
01:16:49,070 --> 01:16:54,830
the economics perspective and from the

1794
01:16:51,320 --> 01:16:56,990
innovation perspective so we and that's

1795
01:16:54,830 --> 01:16:58,280
what we're currently looking at in a way

1796
01:16:56,990 --> 01:17:00,410
that we're sure that innovation

1797
01:16:58,280 --> 01:17:03,650
continues that the benefits of these

1798
01:17:00,410 --> 01:17:06,430
systems come to fruition and materialize

1799
01:17:03,650 --> 01:17:08,389
and we're not blocked by you know

1800
01:17:06,430 --> 01:17:10,580
over-regulating to make sure that every

1801
01:17:08,390 --> 01:17:12,740
safe because as Andrea said with you

1802
01:17:10,580 --> 01:17:15,110
know most of the accidents that we have

1803
01:17:12,740 --> 01:17:17,330
today for example n no turn on n cars

1804
01:17:15,110 --> 01:17:19,610
come from the humans so we think that

1805
01:17:17,330 --> 01:17:21,530
the safety will be increased with these

1806
01:17:19,610 --> 01:17:24,589
systems we're looking at the economics

1807
01:17:21,530 --> 01:17:26,660
which means very most probably will be

1808
01:17:24,590 --> 01:17:28,850
shared responsibility shared

1809
01:17:26,660 --> 01:17:31,849
responsibility and responsibility will

1810
01:17:28,850 --> 01:17:36,329
go where the weight of the sharing that

1811
01:17:31,850 --> 01:17:44,579
will go to those who can afford to have

1812
01:17:36,329 --> 01:17:45,869
- - you know to take that responsibility

1813
01:17:44,579 --> 01:17:48,389
you can afford to take that

1814
01:17:45,869 --> 01:17:50,880
responsibility in a way that makes it

1815
01:17:48,389 --> 01:17:53,190
the solution economically viable okay I

1816
01:17:50,880 --> 01:17:55,409
mean I think it's that's the way we're

1817
01:17:53,190 --> 01:17:58,079
addressing the situation today okay very

1818
01:17:55,409 --> 01:18:00,869
quickly Everton Barrick go ahead

1819
01:17:58,079 --> 01:18:03,900
basically thank you yeah so I will talk

1820
01:18:00,869 --> 01:18:05,820
about this from little bit technological

1821
01:18:03,900 --> 01:18:08,429
perspective so basically when you have a

1822
01:18:05,820 --> 01:18:11,280
system and you either programmed or

1823
01:18:08,429 --> 01:18:14,040
trained the system it's the system

1824
01:18:11,280 --> 01:18:16,320
basically is a set of some policies or

1825
01:18:14,040 --> 01:18:18,179
behaviors or actions or strategies like

1826
01:18:16,320 --> 01:18:20,340
this is what the system is and it's

1827
01:18:18,179 --> 01:18:22,739
modeling and if it is an overall network

1828
01:18:20,340 --> 01:18:24,840
or something else it's just a detail in

1829
01:18:22,739 --> 01:18:27,869
my opinion but would basically have

1830
01:18:24,840 --> 01:18:31,530
either set a set of skills set of

1831
01:18:27,869 --> 01:18:34,009
behavior set of policies and so if the

1832
01:18:31,530 --> 01:18:37,409
manufacturer of a self-driving car

1833
01:18:34,010 --> 01:18:40,829
programs a car then trains the car and

1834
01:18:37,409 --> 01:18:43,530
the car has a fixed set set of skills so

1835
01:18:40,829 --> 01:18:45,750
it will not adopt it will not change

1836
01:18:43,530 --> 01:18:47,849
this set of skills after it leaves the

1837
01:18:45,750 --> 01:18:50,190
factory then I think the responsibility

1838
01:18:47,849 --> 01:18:52,250
up is up to the manufacturer it like

1839
01:18:50,190 --> 01:18:56,969
they must make sure that the car is

1840
01:18:52,250 --> 01:18:59,429
generalizing to all situations that the

1841
01:18:56,969 --> 01:19:02,610
trend situations are generalized to the

1842
01:18:59,429 --> 01:19:04,079
real situations later different

1843
01:19:02,610 --> 01:19:05,969
situation will happen if you will have a

1844
01:19:04,079 --> 01:19:08,900
robot or a self-driving car that is able

1845
01:19:05,969 --> 01:19:11,730
to be adopted even after it leaves the

1846
01:19:08,900 --> 01:19:14,759
factory you know so it will be able to

1847
01:19:11,730 --> 01:19:16,619
receive some command from a user and for

1848
01:19:14,760 --> 01:19:18,270
example if the user will say like no

1849
01:19:16,619 --> 01:19:21,089
just hate that person like it's okay

1850
01:19:18,270 --> 01:19:24,030
then I think the responsibility is on

1851
01:19:21,090 --> 01:19:27,989
the user if the user has a way how to

1852
01:19:24,030 --> 01:19:31,050
adopt these set of policies of the robot

1853
01:19:27,989 --> 01:19:34,019
then it's up to him okay all right very

1854
01:19:31,050 --> 01:19:35,610
quickly quick comment on the technology

1855
01:19:34,020 --> 01:19:38,849
side because you use the term which I've

1856
01:19:35,610 --> 01:19:40,110
come across which is a challenging term

1857
01:19:38,849 --> 01:19:43,650
in this conversation

1858
01:19:40,110 --> 01:19:45,839
you use the term algorithm and algorithm

1859
01:19:43,650 --> 01:19:47,400
is normally being considered human sort

1860
01:19:45,840 --> 01:19:49,630
of developing the set of instructions

1861
01:19:47,400 --> 01:19:53,049
that the computer with the proper input

1862
01:19:49,630 --> 01:19:55,510
I can process creating a basically

1863
01:19:53,050 --> 01:19:56,950
predictable output the way how machine

1864
01:19:55,510 --> 01:19:59,730
learning is fundamentally different and

1865
01:19:56,950 --> 01:20:02,590
initially I said after 50 years of

1866
01:19:59,730 --> 01:20:05,019
attempting as humans to understand

1867
01:20:02,590 --> 01:20:07,060
computers we are now trying to let

1868
01:20:05,020 --> 01:20:08,590
computers learn to understand the world

1869
01:20:07,060 --> 01:20:10,960
machine learning is a bit different

1870
01:20:08,590 --> 01:20:13,090
insect exactly in this domain what's

1871
01:20:10,960 --> 01:20:15,460
happening is that in machine learning we

1872
01:20:13,090 --> 01:20:17,860
have two phases the first phase is the

1873
01:20:15,460 --> 01:20:20,110
Machine learns from initial set of let's

1874
01:20:17,860 --> 01:20:22,540
say weightings and and and let's say

1875
01:20:20,110 --> 01:20:25,030
assumptions it's learning how to

1876
01:20:22,540 --> 01:20:26,920
recognize for instance a certain thing

1877
01:20:25,030 --> 01:20:29,259
on a picture a cat on the pictures of

1878
01:20:26,920 --> 01:20:31,840
famous example anyway you learn it with

1879
01:20:29,260 --> 01:20:34,180
100 million pictures and every time the

1880
01:20:31,840 --> 01:20:36,070
software gets a picture you label it

1881
01:20:34,180 --> 01:20:39,630
saying there is a cat on the picture or

1882
01:20:36,070 --> 01:20:43,000
not and at the end of this process the

1883
01:20:39,630 --> 01:20:45,420
program writes a program an algorithm

1884
01:20:43,000 --> 01:20:48,580
and the algorithm is supposed to be

1885
01:20:45,420 --> 01:20:51,280
generated to generalize recognition of

1886
01:20:48,580 --> 01:20:53,410
cats in this example in a way so at the

1887
01:20:51,280 --> 01:20:55,120
end of the day a machine has written not

1888
01:20:53,410 --> 01:20:57,190
a human but the machine has written the

1889
01:20:55,120 --> 01:20:59,230
final program which should be able to

1890
01:20:57,190 --> 01:21:01,120
like a human to channelize that you see

1891
01:20:59,230 --> 01:21:02,830
a cat in a different color you haven't

1892
01:21:01,120 --> 01:21:05,559
been trained on you are still saying

1893
01:21:02,830 --> 01:21:07,240
with 80% probability on a picture you're

1894
01:21:05,560 --> 01:21:09,700
seeing later on in the usage phase

1895
01:21:07,240 --> 01:21:12,490
there's a cat on this picture okay and

1896
01:21:09,700 --> 01:21:14,080
the important thing is on the real

1897
01:21:12,490 --> 01:21:17,260
basically on the continuous learning

1898
01:21:14,080 --> 01:21:20,950
part similar to what Mark said on the

1899
01:21:17,260 --> 01:21:23,230
manufacturer is delivering 5,000 cars to

1900
01:21:20,950 --> 01:21:25,630
customers in a certain country each

1901
01:21:23,230 --> 01:21:27,580
customer has a different way of driving

1902
01:21:25,630 --> 01:21:28,900
or let's say a different environment one

1903
01:21:27,580 --> 01:21:30,250
is only driving in the city the other

1904
01:21:28,900 --> 01:21:32,860
one on the beach the next one in the

1905
01:21:30,250 --> 01:21:36,670
mountains so the cars adopt and change

1906
01:21:32,860 --> 01:21:38,620
the algorithms on an ongoing basis they

1907
01:21:36,670 --> 01:21:41,560
are continuing learning in this context

1908
01:21:38,620 --> 01:21:44,349
which means at the point of accident you

1909
01:21:41,560 --> 01:21:46,600
might have 5,000 different algorithms in

1910
01:21:44,350 --> 01:21:48,520
those 5,000 cars

1911
01:21:46,600 --> 01:21:49,900
happening away and I do think that's one

1912
01:21:48,520 --> 01:21:52,240
of the kind of regulatory challenges

1913
01:21:49,900 --> 01:21:53,589
then the design whose fault is it was at

1914
01:21:52,240 --> 01:21:55,960
the training data set was it the

1915
01:21:53,590 --> 01:21:58,360
manufacturer who trained the car was it

1916
01:21:55,960 --> 01:22:00,580
the user saying constantly go faster go

1917
01:21:58,360 --> 01:22:02,349
faster go faster it's a shared

1918
01:22:00,580 --> 01:22:02,920
responsibility but it has to be

1919
01:22:02,350 --> 01:22:05,320
discussed

1920
01:22:02,920 --> 01:22:08,050
what is the proper solution well folks

1921
01:22:05,320 --> 01:22:10,030
we are just about out of time as we as

1922
01:22:08,050 --> 01:22:12,550
we close up I wonder if I could ask our

1923
01:22:10,030 --> 01:22:15,099
panelists to in about a sentence or so

1924
01:22:12,550 --> 01:22:16,989
sum up the most important thing that

1925
01:22:15,100 --> 01:22:21,340
this crowd should be thinking about as

1926
01:22:16,989 --> 01:22:27,428
they contemplate the future of AI well

1927
01:22:21,340 --> 01:22:31,090
it's going to be a long sentence first

1928
01:22:27,429 --> 01:22:36,640
so for policymaking the what I can speak

1929
01:22:31,090 --> 01:22:39,070
about we look at AI under three three

1930
01:22:36,640 --> 01:22:41,140
major angles if you want one angle is

1931
01:22:39,070 --> 01:22:44,590
about innovation and research to make

1932
01:22:41,140 --> 01:22:46,929
sure that this technology is developed

1933
01:22:44,590 --> 01:22:50,469
in Europe we have it we master it and

1934
01:22:46,929 --> 01:22:52,710
that it is widely available so we're

1935
01:22:50,469 --> 01:22:55,900
supporting competent centers in all

1936
01:22:52,710 --> 01:22:57,429
parts of Europe widely available and we

1937
01:22:55,900 --> 01:22:59,769
supporting the creation of open

1938
01:22:57,429 --> 01:23:04,390
platforms for AI with golden AI on

1939
01:22:59,770 --> 01:23:06,820
demand so that everyone can try test an

1940
01:23:04,390 --> 01:23:10,179
experiment where the technology is very

1941
01:23:06,820 --> 01:23:12,489
important for us and we have large

1942
01:23:10,179 --> 01:23:14,320
programs that we do at your level and

1943
01:23:12,489 --> 01:23:15,968
work with the Member State and in

1944
01:23:14,320 --> 01:23:17,949
partnership with industry you have the

1945
01:23:15,969 --> 01:23:20,590
big partnership on robotics with

1946
01:23:17,949 --> 01:23:24,428
industry with close to 2.1 billion euros

1947
01:23:20,590 --> 01:23:28,570
of investment and in seven years so it's

1948
01:23:24,429 --> 01:23:31,150
a it is very important that's one thing

1949
01:23:28,570 --> 01:23:34,660
okay research innovation second angle

1950
01:23:31,150 --> 01:23:37,000
we're looking at is on the regulatory as

1951
01:23:34,660 --> 01:23:39,730
a regulatory part and regulatory part we

1952
01:23:37,000 --> 01:23:42,370
have four four very important areas

1953
01:23:39,730 --> 01:23:44,468
liability is indeed and responsibility

1954
01:23:42,370 --> 01:23:46,120
it's very important second data and

1955
01:23:44,469 --> 01:23:49,750
availability of data which was the

1956
01:23:46,120 --> 01:23:54,910
source big source of AI third thing is

1957
01:23:49,750 --> 01:23:59,320
cybersecurity and they all these

1958
01:23:54,910 --> 01:24:02,080
security aspects and safety related to

1959
01:23:59,320 --> 01:24:03,730
data and general and the fourth is the

1960
01:24:02,080 --> 01:24:05,650
diffusion of the single market so we

1961
01:24:03,730 --> 01:24:08,820
have the immigration across Europe the

1962
01:24:05,650 --> 01:24:10,690
and the third item is about skills and

1963
01:24:08,820 --> 01:24:13,840
developing of our skills and our

1964
01:24:10,690 --> 01:24:16,509
workforce this is the these are the main

1965
01:24:13,840 --> 01:24:18,550
okay three tracks it was a long sentence

1966
01:24:16,510 --> 01:24:20,920
it was okay

1967
01:24:18,550 --> 01:24:23,290
Marik most important thing they got to

1968
01:24:20,920 --> 01:24:27,100
leave to understand about it I think

1969
01:24:23,290 --> 01:24:28,960
just to expect the unexpected or like to

1970
01:24:27,100 --> 01:24:32,950
embrace that there will be surprises

1971
01:24:28,960 --> 01:24:34,930
coming from from AI even in coming years

1972
01:24:32,950 --> 01:24:38,769
and even surprises that are really hard

1973
01:24:34,930 --> 01:24:41,410
to anticipate okay and andreas quick

1974
01:24:38,770 --> 01:24:44,380
comment we see it as an opportunity and

1975
01:24:41,410 --> 01:24:46,059
visit comes huge responsibility not only

1976
01:24:44,380 --> 01:24:48,010
for us as a company but reducing for us

1977
01:24:46,060 --> 01:24:50,590
as a society to find the proper

1978
01:24:48,010 --> 01:24:52,480
framework and the boundaries what can be

1979
01:24:50,590 --> 01:24:55,510
used in a way I've mentioned the

1980
01:24:52,480 --> 01:24:57,339
partnership on AI so whoever feels

1981
01:24:55,510 --> 01:24:59,440
interested to participate please let me

1982
01:24:57,340 --> 01:25:02,500
know and the last thing is I do wanna

1983
01:24:59,440 --> 01:25:04,089
reinforce it's a European topic and it's

1984
01:25:02,500 --> 01:25:06,130
for a very particular reason for my

1985
01:25:04,090 --> 01:25:08,290
perspective a European topic and I hope

1986
01:25:06,130 --> 01:25:10,780
that the Commission is continuing

1987
01:25:08,290 --> 01:25:12,970
shaping the kind of conversation because

1988
01:25:10,780 --> 01:25:14,679
we talked about the bigger the data is

1989
01:25:12,970 --> 01:25:17,230
the better the machines can learn and

1990
01:25:14,680 --> 01:25:19,450
imagine if we have 28 maybe in the

1991
01:25:17,230 --> 01:25:23,049
future 27 European countries with

1992
01:25:19,450 --> 01:25:25,690
smaller data sets AI can't contribute as

1993
01:25:23,050 --> 01:25:27,100
much to the developments in the

1994
01:25:25,690 --> 01:25:29,110
individual country because the

1995
01:25:27,100 --> 01:25:31,930
fragmentation would reduce significantly

1996
01:25:29,110 --> 01:25:33,880
the ability of machines to learn in a

1997
01:25:31,930 --> 01:25:36,460
way so the scale effects and the network

1998
01:25:33,880 --> 01:25:38,860
effects to have a kind of common single

1999
01:25:36,460 --> 01:25:41,080
market spirit kind of approach across

2000
01:25:38,860 --> 01:25:42,820
Europe is fundamental if your opponent

2001
01:25:41,080 --> 01:25:47,320
maintain a competitive edge globally

2002
01:25:42,820 --> 01:25:49,030
thinking ok well folks so right now we'd

2003
01:25:47,320 --> 01:25:52,269
like to invite you to the next session

2004
01:25:49,030 --> 01:25:54,309
starts here on intelligence reform it

2005
01:25:52,270 --> 01:25:55,900
starts in this space the Danube space

2006
01:25:54,310 --> 01:25:58,890
after coffee break at about 11 o'clock

2007
01:25:55,900 --> 01:26:00,879
which is 11 o'clock sharp I'm told

2008
01:25:58,890 --> 01:26:02,980
refreshments served in the hotel

2009
01:26:00,880 --> 01:26:04,690
ground-floor please join me in thanking

2010
01:26:02,980 --> 01:26:07,589
our panelists for this discussion we

2011
01:26:04,690 --> 01:26:07,589
really think it may be

2012
01:26:09,440 --> 01:26:13,759
all righty

