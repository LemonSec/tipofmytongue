1
00:00:02,210 --> 00:00:06,359
brilliant okay now we have our waters

2
00:00:04,589 --> 00:00:08,280
open thank you ladies and gentlemen for

3
00:00:06,359 --> 00:00:11,190
joining us I know there's an awful lot

4
00:00:08,280 --> 00:00:13,500
of competing panels at them in it so

5
00:00:11,190 --> 00:00:16,800
really do appreciate you joining us for

6
00:00:13,500 --> 00:00:21,660
a discussion about AI and humanity and I

7
00:00:16,800 --> 00:00:24,810
guess how we can keep humanity as a I

8
00:00:21,660 --> 00:00:26,880
slowly encroaches on us

9
00:00:24,810 --> 00:00:28,619
my name is crystal Walker I'm a

10
00:00:26,880 --> 00:00:30,500
technology journalist for a number of

11
00:00:28,619 --> 00:00:35,579
publications in the UK including Wired

12
00:00:30,500 --> 00:00:37,950
The Economist BBC Bloomberg and I'm

13
00:00:35,579 --> 00:00:40,649
joined by some really really interesting

14
00:00:37,950 --> 00:00:42,480
contributors to our panel so immediately

15
00:00:40,649 --> 00:00:46,100
to my left I have Tracy Chu who is the

16
00:00:42,480 --> 00:00:49,319
founder and CEO of Bloc Party which is a

17
00:00:46,100 --> 00:00:52,590
london-based start-up working on kind of

18
00:00:49,320 --> 00:00:54,989
rethinking social platforms so Tracy was

19
00:00:52,590 --> 00:00:57,000
an engineer at kora and Pinterest she's

20
00:00:54,989 --> 00:00:59,339
believed consulted with the US

21
00:00:57,000 --> 00:01:02,309
government as well trying to keep them

22
00:00:59,340 --> 00:01:04,350
honest which is always very very nice in

23
00:01:02,309 --> 00:01:05,939
the middle we have Bridget Bridget

24
00:01:04,349 --> 00:01:08,429
hyacinth who is the founder and director

25
00:01:05,939 --> 00:01:11,669
of the MBA Caribbean organization so

26
00:01:08,430 --> 00:01:13,280
that basically is trying to give leaders

27
00:01:11,670 --> 00:01:16,049
around the world a bit of a conscience

28
00:01:13,280 --> 00:01:17,820
and some provoking some critical

29
00:01:16,049 --> 00:01:20,390
thinking I think she's also the author

30
00:01:17,820 --> 00:01:22,949
of a number of books on leadership so

31
00:01:20,390 --> 00:01:26,310
very very interesting contributor to our

32
00:01:22,950 --> 00:01:28,829
debate on the human side and then on

33
00:01:26,310 --> 00:01:30,810
your right we have Oren Etzioni who is

34
00:01:28,829 --> 00:01:34,139
the CEO of the Allen Institute for

35
00:01:30,810 --> 00:01:36,150
artificial intelligence in Seattle that

36
00:01:34,140 --> 00:01:39,509
was set up by Paul Allen who is the

37
00:01:36,150 --> 00:01:41,250
co-founder of Microsoft in 2014 and it's

38
00:01:39,509 --> 00:01:43,680
basically developing AI for public good

39
00:01:41,250 --> 00:01:46,009
so we are already biased I think

40
00:01:43,680 --> 00:01:49,009
probably in that we want AI to be

41
00:01:46,009 --> 00:01:51,899
developed sustainably and well but

42
00:01:49,009 --> 00:01:56,820
perhaps the best place to start and I

43
00:01:51,899 --> 00:01:59,670
mean Tracy what is it that separates AI

44
00:01:56,820 --> 00:02:00,619
from humanity do you think in this sort

45
00:01:59,670 --> 00:02:05,670
of thing when you're developing

46
00:02:00,619 --> 00:02:07,530
intelligent platforms and using AI what

47
00:02:05,670 --> 00:02:10,429
is it that's different from just humans

48
00:02:07,530 --> 00:02:10,429
involved in that

49
00:02:10,720 --> 00:02:17,350
that's a big question to start with one

50
00:02:14,470 --> 00:02:19,600
thing to clarify up front might be what

51
00:02:17,350 --> 00:02:23,049
we actually mean by the term artificial

52
00:02:19,600 --> 00:02:27,340
intelligence or AI and in some contexts

53
00:02:23,050 --> 00:02:30,280
it is actually people are talking about

54
00:02:27,340 --> 00:02:33,370
AI it is a very intelligent super super

55
00:02:30,280 --> 00:02:36,220
intelligent form of technology and in

56
00:02:33,370 --> 00:02:38,620
many cases the way it's used now in the

57
00:02:36,220 --> 00:02:41,020
industry is actually not so

58
00:02:38,620 --> 00:02:43,360
sophisticated and so a lot of the AI

59
00:02:41,020 --> 00:02:47,350
that we're talking about in technology

60
00:02:43,360 --> 00:02:50,200
companies is if you actually look at it

61
00:02:47,350 --> 00:02:55,570
really basic mathematical models that

62
00:02:50,200 --> 00:02:58,030
take inputs like clicks and view time

63
00:02:55,570 --> 00:03:01,200
and then tries to get people to click

64
00:02:58,030 --> 00:03:05,560
more and view more on these platforms

65
00:03:01,200 --> 00:03:07,000
and so some of these AI models that are

66
00:03:05,560 --> 00:03:09,010
being used right now are not very

67
00:03:07,000 --> 00:03:13,320
intelligent they're just trading off of

68
00:03:09,010 --> 00:03:15,519
existing data that is not very smart and

69
00:03:13,320 --> 00:03:17,320
what then happens is when we try to

70
00:03:15,519 --> 00:03:21,880
deploy this at scale it is much faster

71
00:03:17,320 --> 00:03:23,410
than having humans look at say cranking

72
00:03:21,880 --> 00:03:27,280
what people should look at when they're

73
00:03:23,410 --> 00:03:28,810
logging into Facebook or Twitter or what

74
00:03:27,280 --> 00:03:30,760
moderation decisions should be so the AI

75
00:03:28,810 --> 00:03:33,040
is faster but it's not actually more

76
00:03:30,760 --> 00:03:34,390
intelligent and so what then ends up

77
00:03:33,040 --> 00:03:36,370
happening is that we have dumber

78
00:03:34,390 --> 00:03:40,480
decisions being made much more quickly

79
00:03:36,370 --> 00:03:42,700
at a global scale and the Orang I mean I

80
00:03:40,480 --> 00:03:44,380
know that you will see you're helping

81
00:03:42,700 --> 00:03:48,160
try and develop AI for the public good

82
00:03:44,380 --> 00:03:50,350
but is that fear amongst tech companies

83
00:03:48,160 --> 00:03:52,150
is that they just want to throw AI at

84
00:03:50,350 --> 00:03:56,470
any problem without thinking about the

85
00:03:52,150 --> 00:04:00,130
long-term impact of it well first of all

86
00:03:56,470 --> 00:04:03,100
I really like Tracy's characterization

87
00:04:00,130 --> 00:04:06,100
because people do and it comes I think

88
00:04:03,100 --> 00:04:09,489
largely from Hollywood people do get an

89
00:04:06,100 --> 00:04:11,709
overinflated notion of what AI is it's

90
00:04:09,489 --> 00:04:14,049
very much the next generation of

91
00:04:11,709 --> 00:04:16,180
computer technology I like to say you

92
00:04:14,049 --> 00:04:19,120
have to separate science from science

93
00:04:16,180 --> 00:04:20,890
fiction and reality from Hollywood and

94
00:04:19,120 --> 00:04:23,710
hype right we've all seen movies like

95
00:04:20,890 --> 00:04:24,520
her and Terminator and so on great

96
00:04:23,710 --> 00:04:26,380
movies

97
00:04:24,520 --> 00:04:28,599
if you like that sort of thing but it's

98
00:04:26,380 --> 00:04:31,150
not really what's happening now Christy

99
00:04:28,600 --> 00:04:33,190
your question I don't think we can paint

100
00:04:31,150 --> 00:04:36,489
everything with the same brush so

101
00:04:33,190 --> 00:04:39,639
certainly the companies like Facebook

102
00:04:36,490 --> 00:04:42,550
and Google are using this algorithm to

103
00:04:39,639 --> 00:04:43,960
target advertising to collect a large

104
00:04:42,550 --> 00:04:49,930
amount of data they're doing that very

105
00:04:43,960 --> 00:04:51,880
effectively a number of authorities like

106
00:04:49,930 --> 00:04:54,280
the Chinese but also various forces

107
00:04:51,880 --> 00:04:57,729
inside the United States use AI for

108
00:04:54,280 --> 00:05:00,820
facial recognition and we're finding

109
00:04:57,729 --> 00:05:03,030
more and more that while Tracy is right

110
00:05:00,820 --> 00:05:06,699
that the algorithms are not particularly

111
00:05:03,030 --> 00:05:10,359
smart in any sense when you have massive

112
00:05:06,699 --> 00:05:11,979
amounts of labeled data millions of

113
00:05:10,360 --> 00:05:15,400
examples and you have a lot of

114
00:05:11,979 --> 00:05:18,729
computational power increasingly you can

115
00:05:15,400 --> 00:05:20,979
be accurate at least some of the time

116
00:05:18,729 --> 00:05:23,620
and that leads to achievements at least

117
00:05:20,979 --> 00:05:25,659
in these narrow arenas so then we get to

118
00:05:23,620 --> 00:05:28,840
the question ok what do we do with this

119
00:05:25,660 --> 00:05:31,449
and I think all of us think that's a

120
00:05:28,840 --> 00:05:32,830
fascinating question I've probably

121
00:05:31,449 --> 00:05:34,930
talked for too long all right I don't

122
00:05:32,830 --> 00:05:36,609
want to turn this into a soliloquy let

123
00:05:34,930 --> 00:05:38,500
me just say that my answer to that it's

124
00:05:36,610 --> 00:05:40,599
not going to be one thing different

125
00:05:38,500 --> 00:05:43,150
players are going to be doing very very

126
00:05:40,599 --> 00:05:46,000
different things with AI and and

127
00:05:43,150 --> 00:05:50,710
Bridgette I mean you spend your life

128
00:05:46,000 --> 00:05:53,409
trying to make company founders think

129
00:05:50,710 --> 00:05:57,099
sort of critically and intelligently and

130
00:05:53,409 --> 00:05:59,289
nicely almost I mean are you waking up

131
00:05:57,099 --> 00:06:03,280
in cold sweats at the idea of the AI

132
00:05:59,289 --> 00:06:06,250
dawn coming upon us here I would say no

133
00:06:03,280 --> 00:06:09,729
um the reality is here just like you

134
00:06:06,250 --> 00:06:12,099
know electricity and I know as Orrin was

135
00:06:09,729 --> 00:06:13,750
seeing that you know sometimes people

136
00:06:12,099 --> 00:06:16,810
focus on just in negatives but there are

137
00:06:13,750 --> 00:06:18,449
a lot of positives you know in in the

138
00:06:16,810 --> 00:06:21,759
development of artificial intelligence

139
00:06:18,449 --> 00:06:24,430
so you know we can focus on that I what

140
00:06:21,759 --> 00:06:25,780
I really try to work with leaders is to

141
00:06:24,430 --> 00:06:29,110
bring back that human touch that

142
00:06:25,780 --> 00:06:31,539
elements I was raised by my mother

143
00:06:29,110 --> 00:06:33,820
single parents and six children and

144
00:06:31,539 --> 00:06:35,919
because of people giving back you know

145
00:06:33,820 --> 00:06:38,380
in the community I'm able to be where I

146
00:06:35,919 --> 00:06:41,530
am today so this is my stress

147
00:06:38,380 --> 00:06:43,540
not even in this you know technological

148
00:06:41,530 --> 00:06:45,969
age we still have to remain that have to

149
00:06:43,540 --> 00:06:48,670
keep that human touch that that caring

150
00:06:45,970 --> 00:06:51,130
aspect that ability to give back how do

151
00:06:48,670 --> 00:06:53,470
you leave a like legacy it's true

152
00:06:51,130 --> 00:06:55,270
inspiring it's true helping others is

153
00:06:53,470 --> 00:06:56,380
true giving back so we still have to

154
00:06:55,270 --> 00:06:57,789
balance yes

155
00:06:56,380 --> 00:07:00,730
you know the profits and the balance

156
00:06:57,790 --> 00:07:04,390
sheet but also to make a lasting impact

157
00:07:00,730 --> 00:07:07,150
and when you look um also when you look

158
00:07:04,390 --> 00:07:09,219
at how take for example right if you

159
00:07:07,150 --> 00:07:11,409
look at sometimes on social media when

160
00:07:09,220 --> 00:07:13,390
someone does a kind act it was

161
00:07:11,410 --> 00:07:17,170
immediately viral when someone crosses

162
00:07:13,390 --> 00:07:19,659
an LED street it goes viral why because

163
00:07:17,170 --> 00:07:21,370
it's not in norm so things like kindness

164
00:07:19,660 --> 00:07:23,800
and empathy and compassion and

165
00:07:21,370 --> 00:07:25,510
generosity it is not in normal it should

166
00:07:23,800 --> 00:07:28,810
be and this is what is gonna

167
00:07:25,510 --> 00:07:30,730
differentiate us from AI that's really

168
00:07:28,810 --> 00:07:34,180
interesting I should also say we do have

169
00:07:30,730 --> 00:07:36,360
slider working for our panel so if you

170
00:07:34,180 --> 00:07:39,070
want to ask us any questions I have a

171
00:07:36,360 --> 00:07:40,780
the joys of Technology it's not AI but I

172
00:07:39,070 --> 00:07:43,150
have a little iPad that will feed in

173
00:07:40,780 --> 00:07:45,150
some of those questions and that's

174
00:07:43,150 --> 00:07:47,349
that's really fascinating Bridgette so I

175
00:07:45,150 --> 00:07:49,929
mean Tracy you you work with these

176
00:07:47,350 --> 00:07:53,260
companies and try and advocate for them

177
00:07:49,930 --> 00:07:55,930
I mean do you find it difficult to

178
00:07:53,260 --> 00:07:57,969
convince some of what Bridget says in

179
00:07:55,930 --> 00:07:59,890
terms of like making sure that they do

180
00:07:57,970 --> 00:08:02,080
keep humanity at the core that they are

181
00:07:59,890 --> 00:08:06,789
over reliant on this sort of stuff

182
00:08:02,080 --> 00:08:10,270
I think these companies use big tech

183
00:08:06,790 --> 00:08:12,280
companies they do have good people who

184
00:08:10,270 --> 00:08:15,580
work at them and want to do good and

185
00:08:12,280 --> 00:08:19,739
they are human and understand humanity

186
00:08:15,580 --> 00:08:22,240
in that that sense I think one of the

187
00:08:19,740 --> 00:08:23,350
risks right now is that a lot of the

188
00:08:22,240 --> 00:08:26,370
people who work at these companies

189
00:08:23,350 --> 00:08:28,930
belong to pretty homogenous cohorts and

190
00:08:26,370 --> 00:08:31,960
despite trying to do good often have

191
00:08:28,930 --> 00:08:35,340
pretty big blind spots and that can be

192
00:08:31,960 --> 00:08:37,740
because many people are not trained in

193
00:08:35,340 --> 00:08:40,180
humanities thinking or thinking beyond

194
00:08:37,740 --> 00:08:42,640
the media technology they're working on

195
00:08:40,179 --> 00:08:44,949
and some of it is a lack of demographic

196
00:08:42,640 --> 00:08:47,500
diversity so really understanding what

197
00:08:44,950 --> 00:08:48,520
is meaningful to their demographics but

198
00:08:47,500 --> 00:08:51,910
not understanding what the implications

199
00:08:48,520 --> 00:08:52,689
might be for others and so these

200
00:08:51,910 --> 00:08:58,120
blind-spot

201
00:08:52,690 --> 00:09:00,580
can end up being painful when there are

202
00:08:58,120 --> 00:09:02,950
examples of things like Facebook sending

203
00:09:00,580 --> 00:09:04,810
reminders of here

204
00:09:02,950 --> 00:09:07,210
your most liked photograph in the last

205
00:09:04,810 --> 00:09:09,280
year and it might be of a child that

206
00:09:07,210 --> 00:09:10,570
passed away and that's very painful it

207
00:09:09,280 --> 00:09:12,730
was meant to be here our happy memories

208
00:09:10,570 --> 00:09:15,010
in the last year but sometimes they just

209
00:09:12,730 --> 00:09:18,040
miss out on what the use case might end

210
00:09:15,010 --> 00:09:20,350
up being and in other cases it can be

211
00:09:18,040 --> 00:09:23,230
very insidious when there are impacts

212
00:09:20,350 --> 00:09:25,360
that spiral out of control so you can

213
00:09:23,230 --> 00:09:27,370
see like the spread of misinformation or

214
00:09:25,360 --> 00:09:31,330
the weaponization of social media to

215
00:09:27,370 --> 00:09:33,640
spread misinformation disinformation so

216
00:09:31,330 --> 00:09:35,110
despite good intentions I think we still

217
00:09:33,640 --> 00:09:36,850
have a lot of blind spots that need to

218
00:09:35,110 --> 00:09:39,370
be addressed and I think sometimes

219
00:09:36,850 --> 00:09:40,630
people who think that they're doing good

220
00:09:39,370 --> 00:09:42,370
but don't understand

221
00:09:40,630 --> 00:09:44,080
what they're missing can get very

222
00:09:42,370 --> 00:09:45,910
defensive and feel like well technology

223
00:09:44,080 --> 00:09:48,130
is always progress and we're not going

224
00:09:45,910 --> 00:09:51,040
to stand against progress and that leads

225
00:09:48,130 --> 00:09:53,500
to sometimes destructive effects on

226
00:09:51,040 --> 00:09:56,770
community like if you look at uber or

227
00:09:53,500 --> 00:09:58,690
the these like car services that now

228
00:09:56,770 --> 00:10:00,730
employ millions of drivers but are also

229
00:09:58,690 --> 00:10:02,770
working very hard to get to self-driving

230
00:10:00,730 --> 00:10:05,140
cars so that they can actually be

231
00:10:02,770 --> 00:10:06,520
profitable when they are able to make

232
00:10:05,140 --> 00:10:08,470
that transition there will be many

233
00:10:06,520 --> 00:10:10,360
drivers will be out of jobs and they're

234
00:10:08,470 --> 00:10:11,500
not thinking about what that impact is

235
00:10:10,360 --> 00:10:13,390
going to be because they're very

236
00:10:11,500 --> 00:10:16,480
narrowly focused on let's try to achieve

237
00:10:13,390 --> 00:10:19,270
self-driving cars and that's the

238
00:10:16,480 --> 00:10:21,310
interesting thing because these even any

239
00:10:19,270 --> 00:10:24,460
2ai development still requires human

240
00:10:21,310 --> 00:10:27,310
input so I mean or on how do we how does

241
00:10:24,460 --> 00:10:30,070
the human interface with the AI at the

242
00:10:27,310 --> 00:10:32,170
minute and how does that relationship

243
00:10:30,070 --> 00:10:33,970
work is today sort of back and forth or

244
00:10:32,170 --> 00:10:36,459
is it that human and humanity is

245
00:10:33,970 --> 00:10:41,110
essentially populating and creating this

246
00:10:36,460 --> 00:10:44,080
AI so a great question and a complex

247
00:10:41,110 --> 00:10:46,240
topic let me just highlight a couple of

248
00:10:44,080 --> 00:10:49,839
points the first one is I like to think

249
00:10:46,240 --> 00:10:52,510
of AI as augmented intelligence because

250
00:10:49,839 --> 00:10:54,910
when we have the positive outcomes it's

251
00:10:52,510 --> 00:10:59,110
when we have the tool being used by

252
00:10:54,910 --> 00:11:00,850
people in a good way so one example of

253
00:10:59,110 --> 00:11:03,910
course is just the Google search engine

254
00:11:00,850 --> 00:11:07,870
which most of us use and it has

255
00:11:03,910 --> 00:11:10,180
a number of AI systems in it if you're

256
00:11:07,870 --> 00:11:12,190
using the Google Translation tool so

257
00:11:10,180 --> 00:11:15,040
again you're using it the same way that

258
00:11:12,190 --> 00:11:17,410
you use a calculator and when we have

259
00:11:15,040 --> 00:11:20,709
for example what's called autonomous

260
00:11:17,410 --> 00:11:23,650
vehicles and those will arrive gradually

261
00:11:20,710 --> 00:11:25,870
over time we have the potential to write

262
00:11:23,650 --> 00:11:29,020
another rung of Technology which is in

263
00:11:25,870 --> 00:11:31,690
the United States over 40,000 people die

264
00:11:29,020 --> 00:11:33,250
each year in car accidents on the

265
00:11:31,690 --> 00:11:36,100
highways over a million people are

266
00:11:33,250 --> 00:11:37,660
injured and the estimates are that as

267
00:11:36,100 --> 00:11:40,420
much as 80 percent of that could be

268
00:11:37,660 --> 00:11:44,800
prevented with with this kind of AI

269
00:11:40,420 --> 00:11:46,750
technology so again we will have that be

270
00:11:44,800 --> 00:11:48,370
good it's still going to be the case of

271
00:11:46,750 --> 00:11:50,410
course that the people tell the cars

272
00:11:48,370 --> 00:11:53,140
were to go right it's not like the cars

273
00:11:50,410 --> 00:11:55,150
take over and you know all the cars say

274
00:11:53,140 --> 00:11:57,460
okay we're going to do brexit we're

275
00:11:55,150 --> 00:12:00,579
going to leave the EU or something like

276
00:11:57,460 --> 00:12:04,030
that so in all these situations I think

277
00:12:00,580 --> 00:12:06,010
when the system works well is is when

278
00:12:04,030 --> 00:12:08,890
it's augmented intelligence of course

279
00:12:06,010 --> 00:12:10,930
there are also plenty of situations some

280
00:12:08,890 --> 00:12:12,910
of the Tracy alluded to and in the

281
00:12:10,930 --> 00:12:16,390
criminal justice system and other ones

282
00:12:12,910 --> 00:12:18,969
where that connection between people and

283
00:12:16,390 --> 00:12:22,120
the AI isn't done right people are over

284
00:12:18,970 --> 00:12:25,300
overly subservient to the technology

285
00:12:22,120 --> 00:12:28,870
they listen it to it too much and then

286
00:12:25,300 --> 00:12:30,459
it works quite quite badly so so I like

287
00:12:28,870 --> 00:12:32,650
to advocate that we think of AI as

288
00:12:30,460 --> 00:12:35,830
Augmented intelligence and we think

289
00:12:32,650 --> 00:12:37,959
about how to use it to to build a better

290
00:12:35,830 --> 00:12:40,570
society and do you think that it can be

291
00:12:37,960 --> 00:12:43,270
used there for Bridgette - AI to build a

292
00:12:40,570 --> 00:12:46,780
better leadership model as well can can

293
00:12:43,270 --> 00:12:49,810
a I be used to help decision-makers be a

294
00:12:46,780 --> 00:12:52,260
bit more human perversely it can help

295
00:12:49,810 --> 00:12:56,439
decision makers definitely but it

296
00:12:52,260 --> 00:12:58,900
depends on if you look at City what is

297
00:12:56,440 --> 00:13:01,900
happening with with the many masses in

298
00:12:58,900 --> 00:13:04,240
terms of information and manipulation of

299
00:13:01,900 --> 00:13:06,760
data and things like that so it depends

300
00:13:04,240 --> 00:13:09,010
upon accountability transparency and

301
00:13:06,760 --> 00:13:11,170
having the right purpose what I do I

302
00:13:09,010 --> 00:13:15,189
want to do what do I want to achieve and

303
00:13:11,170 --> 00:13:16,810
it's things like that I mean we also I

304
00:13:15,190 --> 00:13:17,260
mean you touched a bit Tracy on the

305
00:13:16,810 --> 00:13:18,630
delay

306
00:13:17,260 --> 00:13:23,620
the market aspect we do have a question

307
00:13:18,630 --> 00:13:25,810
from me and on slide Oh which is you do

308
00:13:23,620 --> 00:13:28,150
we need to think more carefully before

309
00:13:25,810 --> 00:13:32,319
sort of running headlong into adopting

310
00:13:28,150 --> 00:13:34,329
AI of the potential impact on employment

311
00:13:32,320 --> 00:13:37,150
and society and things like that do we

312
00:13:34,330 --> 00:13:39,460
need a sort of overarching AI strategy

313
00:13:37,150 --> 00:13:41,439
and if so what would that look like do

314
00:13:39,460 --> 00:13:43,420
you think it gave me another big

315
00:13:41,440 --> 00:13:46,330
question sorry 25 words or less yeah

316
00:13:43,420 --> 00:13:49,709
Tori we sorry we have until 5:15 we can

317
00:13:46,330 --> 00:13:51,820
talk ideally we would have an AI

318
00:13:49,710 --> 00:13:54,760
strategy that considers what the

319
00:13:51,820 --> 00:13:56,950
implications might be but I think very

320
00:13:54,760 --> 00:13:58,630
realistically the technology development

321
00:13:56,950 --> 00:14:00,880
is just running so quickly ahead of

322
00:13:58,630 --> 00:14:02,680
where we can be in terms of even like

323
00:14:00,880 --> 00:14:05,350
discussion but definitely very far ahead

324
00:14:02,680 --> 00:14:07,120
of regulation and policy and I don't

325
00:14:05,350 --> 00:14:09,520
think it's realistic to say we need to

326
00:14:07,120 --> 00:14:11,590
craft our whole strategy before we start

327
00:14:09,520 --> 00:14:14,500
implementing more AI I think the other

328
00:14:11,590 --> 00:14:16,630
thing that makes it impossible is that

329
00:14:14,500 --> 00:14:21,670
we don't actually know what AI will be

330
00:14:16,630 --> 00:14:25,240
capable of even I think let's see seven

331
00:14:21,670 --> 00:14:28,630
or eight years ago the deep learning was

332
00:14:25,240 --> 00:14:31,510
not yet working and I saw a member

333
00:14:28,630 --> 00:14:33,790
actually when I studied AI machine

334
00:14:31,510 --> 00:14:35,200
learning in my master's program we just

335
00:14:33,790 --> 00:14:37,000
skipped over the whole neural net

336
00:14:35,200 --> 00:14:39,160
section because my professor Andrew

337
00:14:37,000 --> 00:14:40,600
eating said oh we tried this a few

338
00:14:39,160 --> 00:14:42,640
decades ago and it didn't work so we're

339
00:14:40,600 --> 00:14:44,770
just skipping over it we're just not

340
00:14:42,640 --> 00:14:46,360
going to cover it and then in 2012 they

341
00:14:44,770 --> 00:14:48,069
actually didn't manage to make neural

342
00:14:46,360 --> 00:14:49,530
Nets work and now that's the the whole

343
00:14:48,070 --> 00:14:51,880
craze around deep learning and it's

344
00:14:49,530 --> 00:14:53,500
really propelled computer vision to new

345
00:14:51,880 --> 00:14:55,720
levels and some of that was because of

346
00:14:53,500 --> 00:14:58,030
the increase of compute power and data

347
00:14:55,720 --> 00:15:00,160
and so things that we thought even 10

348
00:14:58,030 --> 00:15:03,970
years ago are not going to work actually

349
00:15:00,160 --> 00:15:05,260
have completely transformed AI now and

350
00:15:03,970 --> 00:15:06,460
so just because it's so difficult to

351
00:15:05,260 --> 00:15:08,920
predict where we're going to go it also

352
00:15:06,460 --> 00:15:13,630
makes it impossible to design strategy

353
00:15:08,920 --> 00:15:15,880
around it I very much agree yeah it's

354
00:15:13,630 --> 00:15:18,310
there's so many races military and

355
00:15:15,880 --> 00:15:21,070
economic we can't stop I just wanted to

356
00:15:18,310 --> 00:15:23,800
highlight one piece with a metaphor of

357
00:15:21,070 --> 00:15:27,430
an old professor of mine herb Simon who

358
00:15:23,800 --> 00:15:29,510
won the Nobel Prize in Economics and the

359
00:15:27,430 --> 00:15:30,829
Turing Award in computer science I think

360
00:15:29,510 --> 00:15:32,689
one of the most Illustrated

361
00:15:30,830 --> 00:15:37,660
intellectuals of all time and there's a

362
00:15:32,690 --> 00:15:39,740
parable called simon's ant ant not as an

363
00:15:37,660 --> 00:15:43,520
family member but is in the little

364
00:15:39,740 --> 00:15:45,530
critter and if you were to draw the

365
00:15:43,520 --> 00:15:47,900
graph of an ant walking on a beach

366
00:15:45,530 --> 00:15:51,770
you'll see an extremely complex and

367
00:15:47,900 --> 00:15:55,370
convoluted path it's difficult to

368
00:15:51,770 --> 00:15:57,920
predict or or to model yet the ant is a

369
00:15:55,370 --> 00:15:59,810
very very simple creature so how is that

370
00:15:57,920 --> 00:16:03,140
possible and the answer is well the ant

371
00:15:59,810 --> 00:16:05,989
is responding to local stimuli right

372
00:16:03,140 --> 00:16:08,780
going up the little sand dunes and

373
00:16:05,990 --> 00:16:10,520
responding to where the Sun is and so on

374
00:16:08,780 --> 00:16:12,350
now how is this relevant here it's

375
00:16:10,520 --> 00:16:14,800
relevant because as Tracy said earlier

376
00:16:12,350 --> 00:16:18,880
right these algorithms are in some sense

377
00:16:14,800 --> 00:16:22,160
dumb or in some sense you know simple

378
00:16:18,880 --> 00:16:25,370
there they've been around for certainly

379
00:16:22,160 --> 00:16:27,410
since the 50s and 60s in various

380
00:16:25,370 --> 00:16:32,170
variations so how is this possible well

381
00:16:27,410 --> 00:16:34,459
again what's happened in around 2012

382
00:16:32,170 --> 00:16:36,650
with some successes this really

383
00:16:34,460 --> 00:16:39,170
crystallized a combination of tremendous

384
00:16:36,650 --> 00:16:41,470
compute power and tremendous data fed

385
00:16:39,170 --> 00:16:45,439
into these relatively simple algorithms

386
00:16:41,470 --> 00:16:47,540
resulted in in success so so this is

387
00:16:45,440 --> 00:16:49,970
what we're seeing today and at the same

388
00:16:47,540 --> 00:16:52,640
time I just want to reiterate these are

389
00:16:49,970 --> 00:16:56,000
narrow successes right we still don't

390
00:16:52,640 --> 00:16:59,449
have an AI system that's nearly as

391
00:16:56,000 --> 00:17:01,430
capable as my my eight-year-old boy for

392
00:16:59,450 --> 00:17:04,069
example or you know really any child

393
00:17:01,430 --> 00:17:06,349
five or six years old is still a lot

394
00:17:04,069 --> 00:17:10,000
more sophisticated as a generalist

395
00:17:06,349 --> 00:17:13,310
compared to any AI system you're good I

396
00:17:10,000 --> 00:17:16,310
think um that companies should be

397
00:17:13,310 --> 00:17:18,050
rescaling and upskilling whicker's but

398
00:17:16,310 --> 00:17:21,159
you know right now there's not no

399
00:17:18,050 --> 00:17:23,869
massive you know um strategy with this

400
00:17:21,160 --> 00:17:25,699
when change happens you know it's it's

401
00:17:23,869 --> 00:17:27,948
difficult for people nobody likes change

402
00:17:25,699 --> 00:17:30,380
so right now

403
00:17:27,949 --> 00:17:32,720
employers should be preparing employees

404
00:17:30,380 --> 00:17:35,030
for change your job is not permanent

405
00:17:32,720 --> 00:17:37,610
change is coming and when that change

406
00:17:35,030 --> 00:17:41,450
comes you know go about it in a humane

407
00:17:37,610 --> 00:17:43,370
way for example John comes to work after

408
00:17:41,450 --> 00:17:47,830
two eighteen years working a company

409
00:17:43,370 --> 00:17:50,560
and they told him he no longer has a job

410
00:17:47,830 --> 00:17:54,409
security escorts him out the building

411
00:17:50,560 --> 00:17:57,169
what could he have done differently they

412
00:17:54,410 --> 00:17:59,300
have been more humane in the process we

413
00:17:57,170 --> 00:18:01,130
understand we can't understand what

414
00:17:59,300 --> 00:18:04,370
you're going through but we are here to

415
00:18:01,130 --> 00:18:07,760
help we can offer counseling we can help

416
00:18:04,370 --> 00:18:09,979
you but resume writing interviewing do

417
00:18:07,760 --> 00:18:11,990
you need a recommendation what can we do

418
00:18:09,980 --> 00:18:14,090
to help this is happening we cannot

419
00:18:11,990 --> 00:18:16,460
change because job descript disruption

420
00:18:14,090 --> 00:18:18,860
is gonna happen but how do we handle it

421
00:18:16,460 --> 00:18:20,540
to prepare people for it and it doesn't

422
00:18:18,860 --> 00:18:23,000
take much you know to be human human

423
00:18:20,540 --> 00:18:24,950
it's not like a hard thing is these

424
00:18:23,000 --> 00:18:27,320
simple things that we do that County

425
00:18:24,950 --> 00:18:28,130
Moose the kind would the thoughtful

426
00:18:27,320 --> 00:18:30,379
gesture

427
00:18:28,130 --> 00:18:34,580
that impacts people I will mean with

428
00:18:30,380 --> 00:18:36,500
them so how do we keep that in cold hard

429
00:18:34,580 --> 00:18:39,820
computer logic and how do we make sure

430
00:18:36,500 --> 00:18:44,540
we don't lose that that moral grounding

431
00:18:39,820 --> 00:18:47,000
well when I did that I wrote a little

432
00:18:44,540 --> 00:18:49,250
while ago in Wired magazine if you want

433
00:18:47,000 --> 00:18:51,710
to look it up we're specifically related

434
00:18:49,250 --> 00:18:53,480
to the labor markets and these ideas and

435
00:18:51,710 --> 00:18:56,120
I pointed out that the place that

436
00:18:53,480 --> 00:18:59,360
computers and AI systems will get to

437
00:18:56,120 --> 00:19:00,860
last in some sense is is human empathy

438
00:18:59,360 --> 00:19:02,990
very much the sort of thing you were

439
00:19:00,860 --> 00:19:05,060
saying Bridgette and so if we were

440
00:19:02,990 --> 00:19:08,450
willing to invest in a society more

441
00:19:05,060 --> 00:19:14,389
resources in you know delivering food to

442
00:19:08,450 --> 00:19:16,520
the elderly and in a care for you know

443
00:19:14,390 --> 00:19:18,620
autistic children and just care for the

444
00:19:16,520 --> 00:19:21,620
weakest members in society we could

445
00:19:18,620 --> 00:19:23,689
actually employ people people who are

446
00:19:21,620 --> 00:19:25,790
displaced by the rapid change in the

447
00:19:23,690 --> 00:19:27,710
technology in that capacity and it's

448
00:19:25,790 --> 00:19:29,780
actually the the weakest members of

449
00:19:27,710 --> 00:19:31,550
society or at people who say didn't

450
00:19:29,780 --> 00:19:33,740
finish high school didn't finish college

451
00:19:31,550 --> 00:19:35,600
may struggle with you know upscaling

452
00:19:33,740 --> 00:19:37,910
programs people talked about you know

453
00:19:35,600 --> 00:19:39,770
coal miners to data miners right but

454
00:19:37,910 --> 00:19:41,840
that's not realistic how many of our

455
00:19:39,770 --> 00:19:45,020
coal miners are going to become data

456
00:19:41,840 --> 00:19:47,720
miners but perhaps they can be there to

457
00:19:45,020 --> 00:19:50,270
care for four members of society and

458
00:19:47,720 --> 00:19:51,980
this is not a simple thing but if we

459
00:19:50,270 --> 00:19:55,340
could do that I think that would be a

460
00:19:51,980 --> 00:19:56,800
powerful transition and Tracy I think

461
00:19:55,340 --> 00:19:59,139
you worked with the

462
00:19:56,800 --> 00:20:01,450
labor statistics in the US at some

463
00:19:59,140 --> 00:20:03,360
points city I didn't work with them but

464
00:20:01,450 --> 00:20:05,670
I have looked at the data they all like

465
00:20:03,360 --> 00:20:09,729
but more generally in terms of I guess

466
00:20:05,670 --> 00:20:13,420
preparing us for these changes what can

467
00:20:09,730 --> 00:20:16,480
we do to kind of as Orrin says to get

468
00:20:13,420 --> 00:20:20,500
people prepared for a world where AI and

469
00:20:16,480 --> 00:20:22,570
humans really do coexist this is an area

470
00:20:20,500 --> 00:20:27,100
where I don't have any answers I do have

471
00:20:22,570 --> 00:20:29,830
some questions and one big space of

472
00:20:27,100 --> 00:20:32,649
questions is around capitalism and what

473
00:20:29,830 --> 00:20:34,990
we value and so this question around

474
00:20:32,650 --> 00:20:37,750
upskilling workers that are going to be

475
00:20:34,990 --> 00:20:41,110
displaced I'm not sure who has that

476
00:20:37,750 --> 00:20:42,910
responsibility ideally we are all human

477
00:20:41,110 --> 00:20:45,760
and humane and want to take care of each

478
00:20:42,910 --> 00:20:47,770
other but in a capitalist society where

479
00:20:45,760 --> 00:20:49,750
the overarching motive is the profit

480
00:20:47,770 --> 00:20:51,730
motive it doesn't necessarily make sense

481
00:20:49,750 --> 00:20:54,130
for corporations to be upskilling people

482
00:20:51,730 --> 00:20:55,150
just make sense profit wise to switch

483
00:20:54,130 --> 00:21:00,340
over to the robots when they're

484
00:20:55,150 --> 00:21:03,270
available and there are a lot of jobs

485
00:21:00,340 --> 00:21:05,980
that rely more on empathy and humanity

486
00:21:03,270 --> 00:21:08,050
but these are ones that our society

487
00:21:05,980 --> 00:21:09,610
right now does not value very much if

488
00:21:08,050 --> 00:21:12,669
you look at how much caretakers are paid

489
00:21:09,610 --> 00:21:14,740
it's pretty much the bottom so this is

490
00:21:12,670 --> 00:21:16,960
maybe not so much capitalism as it is

491
00:21:14,740 --> 00:21:20,500
potentially misogyny since a lot of

492
00:21:16,960 --> 00:21:22,630
these jobs are coded as more feminine

493
00:21:20,500 --> 00:21:25,630
and jobs that are more feminine are

494
00:21:22,630 --> 00:21:27,040
usually valued less and there's a lot of

495
00:21:25,630 --> 00:21:28,990
research on this over many different

496
00:21:27,040 --> 00:21:31,180
industries like one more men move into a

497
00:21:28,990 --> 00:21:34,570
field and it becomes viewed as more

498
00:21:31,180 --> 00:21:36,160
masculine and lucrative yeah the one we

499
00:21:34,570 --> 00:21:38,409
get pushed out so there's a lot of

500
00:21:36,160 --> 00:21:40,510
things swirling around this I don't

501
00:21:38,410 --> 00:21:43,930
actually have any good answers I do

502
00:21:40,510 --> 00:21:45,970
think we should be looking at these jobs

503
00:21:43,930 --> 00:21:49,480
that will be difficult for machines to

504
00:21:45,970 --> 00:21:53,830
do and figure out how to support people

505
00:21:49,480 --> 00:21:56,710
in those jobs yes I agree with you there

506
00:21:53,830 --> 00:21:58,929
but also governments you know

507
00:21:56,710 --> 00:22:01,390
institutions but also it comes back to

508
00:21:58,930 --> 00:22:04,000
the individual you can't beat and say

509
00:22:01,390 --> 00:22:06,250
well you know it depends upon them this

510
00:22:04,000 --> 00:22:08,530
is your life so I tell individuals take

511
00:22:06,250 --> 00:22:10,420
charge keep abreast of the information

512
00:22:08,530 --> 00:22:11,050
what is going on there are free courses

513
00:22:10,420 --> 00:22:13,210
you can

514
00:22:11,050 --> 00:22:15,820
online that Microsoft on these companies

515
00:22:13,210 --> 00:22:18,340
at least has some basic knowledge and

516
00:22:15,820 --> 00:22:21,310
look at your skills what skills cannot

517
00:22:18,340 --> 00:22:23,709
be cannot be easily automated in this

518
00:22:21,310 --> 00:22:25,780
digital age what the reference kills do

519
00:22:23,710 --> 00:22:27,430
you have what are your strengths what

520
00:22:25,780 --> 00:22:30,129
are your passions what do you like to do

521
00:22:27,430 --> 00:22:31,900
and also network network network network

522
00:22:30,130 --> 00:22:35,650
is about connecting and building

523
00:22:31,900 --> 00:22:36,940
relationships and Tracy touched on a

524
00:22:35,650 --> 00:22:39,630
really interesting point there which is

525
00:22:36,940 --> 00:22:42,760
kind of the implicit biases and how I

526
00:22:39,630 --> 00:22:46,570
reflects human biases I mean orang how

527
00:22:42,760 --> 00:22:49,090
do we iron out basically perpetuating

528
00:22:46,570 --> 00:22:51,450
things like gender bias I mean we saw in

529
00:22:49,090 --> 00:22:54,310
relatively recent months Amazon's

530
00:22:51,450 --> 00:22:56,670
automatic filtering of job applicants

531
00:22:54,310 --> 00:23:00,879
that just completely blanked out women

532
00:22:56,670 --> 00:23:03,070
right so let me start by just explaining

533
00:23:00,880 --> 00:23:05,470
this issue because it is a very serious

534
00:23:03,070 --> 00:23:07,000
issue so these programs that we've been

535
00:23:05,470 --> 00:23:08,920
talking about whether they're deep

536
00:23:07,000 --> 00:23:11,170
learning neural network models or other

537
00:23:08,920 --> 00:23:13,480
ones all have this characteristic that

538
00:23:11,170 --> 00:23:15,940
they take data from the past they

539
00:23:13,480 --> 00:23:17,650
collected and build models in the

540
00:23:15,940 --> 00:23:20,230
present and use these to make

541
00:23:17,650 --> 00:23:22,330
predictions or future decisions now the

542
00:23:20,230 --> 00:23:25,510
problem with this story is if the data

543
00:23:22,330 --> 00:23:27,310
from the past capture biases that we had

544
00:23:25,510 --> 00:23:30,129
in the past whether they're sexist or

545
00:23:27,310 --> 00:23:32,740
racist or what-have-you and we no longer

546
00:23:30,130 --> 00:23:34,690
find those acceptable they're going to

547
00:23:32,740 --> 00:23:36,640
be carried over by the technology into

548
00:23:34,690 --> 00:23:39,490
the future what's worse is the

549
00:23:36,640 --> 00:23:42,220
technology seeks to find patterns in the

550
00:23:39,490 --> 00:23:44,260
data and so they can generalize and

551
00:23:42,220 --> 00:23:47,620
amplify the biases so at the end of this

552
00:23:44,260 --> 00:23:50,530
process the bias is even worse than it

553
00:23:47,620 --> 00:23:52,899
was before the good news about this is

554
00:23:50,530 --> 00:23:55,240
that we do first of all there's very

555
00:23:52,900 --> 00:23:59,170
strong awareness now in the technical

556
00:23:55,240 --> 00:24:02,110
community about this issue and I think

557
00:23:59,170 --> 00:24:04,570
most things do not have technical

558
00:24:02,110 --> 00:24:08,020
solutions but here at least part of this

559
00:24:04,570 --> 00:24:10,990
does so we are increasingly aware of

560
00:24:08,020 --> 00:24:13,420
tools that we can use to to mitigate

561
00:24:10,990 --> 00:24:16,480
that whether by augmenting the data

562
00:24:13,420 --> 00:24:17,500
changing the data or whether by changing

563
00:24:16,480 --> 00:24:20,710
what's called the loss function

564
00:24:17,500 --> 00:24:23,350
basically changing the the algorithm to

565
00:24:20,710 --> 00:24:26,230
at least mitigate that to some extent

566
00:24:23,350 --> 00:24:29,889
at the end of the day though the key is

567
00:24:26,230 --> 00:24:32,140
to have people be aware don't accept you

568
00:24:29,890 --> 00:24:34,270
know caveat emptor if i'm pronouncing

569
00:24:32,140 --> 00:24:37,120
that correctly don't expect what they

570
00:24:34,270 --> 00:24:39,670
don't accept with the machine spits out

571
00:24:37,120 --> 00:24:43,600
at face value right you have to very

572
00:24:39,670 --> 00:24:46,750
carefully audit and try to understand is

573
00:24:43,600 --> 00:24:50,830
this reliable is this high-quality does

574
00:24:46,750 --> 00:24:53,670
this contain biases I mean has this how

575
00:24:50,830 --> 00:24:55,840
does the AI revolution actually oddly

576
00:24:53,670 --> 00:24:58,000
highlighted some of the wider issues in

577
00:24:55,840 --> 00:25:01,659
society do you think Tracy like when

578
00:24:58,000 --> 00:25:04,350
it's in black and white that robots that

579
00:25:01,660 --> 00:25:07,840
are designed to mimic human behavior

580
00:25:04,350 --> 00:25:10,300
perpetuate biases do we then sort of

581
00:25:07,840 --> 00:25:12,370
have more of a stark be honest yeah I do

582
00:25:10,300 --> 00:25:15,220
think it is easier to quantify some of

583
00:25:12,370 --> 00:25:16,750
that now and you can actually test out

584
00:25:15,220 --> 00:25:18,340
an algorithm by putting an image in

585
00:25:16,750 --> 00:25:24,510
front of it and asking it to classify it

586
00:25:18,340 --> 00:25:28,149
so it'll be easier to test but yeah I

587
00:25:24,510 --> 00:25:29,740
would also say there's there's been bias

588
00:25:28,150 --> 00:25:31,660
for a long time people have talked about

589
00:25:29,740 --> 00:25:33,580
it for a long time so it's not as if we

590
00:25:31,660 --> 00:25:35,260
didn't know that these things existed I

591
00:25:33,580 --> 00:25:36,939
do think there's a general shift in

592
00:25:35,260 --> 00:25:39,580
conversation very broadly it's not

593
00:25:36,940 --> 00:25:41,290
necessarily specifically just from AI

594
00:25:39,580 --> 00:25:43,960
highlighting some of these issues now

595
00:25:41,290 --> 00:25:46,300
and in terms of shifts of conversation I

596
00:25:43,960 --> 00:25:48,100
mean I think probably the majority of

597
00:25:46,300 --> 00:25:50,260
this debate has been relatively negative

598
00:25:48,100 --> 00:25:52,090
and bashing AI for a bit I mean how do

599
00:25:50,260 --> 00:25:53,770
we you know because it is potentially

600
00:25:52,090 --> 00:25:55,990
something that is an enormous public

601
00:25:53,770 --> 00:25:59,280
good how do we shift the conversation

602
00:25:55,990 --> 00:26:02,490
about this and make it more about a eyes

603
00:25:59,280 --> 00:26:04,720
powerful abilities to change the world

604
00:26:02,490 --> 00:26:05,940
well Chris I'm glad you asked that

605
00:26:04,720 --> 00:26:08,830
question

606
00:26:05,940 --> 00:26:11,050
since we did have some exchange before

607
00:26:08,830 --> 00:26:14,409
so so our mission is the eye for the

608
00:26:11,050 --> 00:26:16,960
common good and I do see enormous

609
00:26:14,410 --> 00:26:19,450
potential benefits and I think one of

610
00:26:16,960 --> 00:26:21,940
the biggest ways that we can shift the

611
00:26:19,450 --> 00:26:23,800
conversation is to deliver some of those

612
00:26:21,940 --> 00:26:27,120
benefits whether they're as I mentioned

613
00:26:23,800 --> 00:26:30,000
in cars and fewer accidents which will

614
00:26:27,120 --> 00:26:32,350
is already starting to happen with

615
00:26:30,000 --> 00:26:34,090
safety devices and cars that are based

616
00:26:32,350 --> 00:26:36,250
on AI whether it's in the medical

617
00:26:34,090 --> 00:26:36,820
systems where in the United States at

618
00:26:36,250 --> 00:26:39,010
least

619
00:26:36,820 --> 00:26:41,260
the third leading cause of death in

620
00:26:39,010 --> 00:26:43,750
American hospitals is physician error

621
00:26:41,260 --> 00:26:47,680
and again information systems and AI

622
00:26:43,750 --> 00:26:48,940
systems can help reduce that at the

623
00:26:47,680 --> 00:26:51,880
Allen Institute for artificial

624
00:26:48,940 --> 00:26:54,850
intelligence we're building search

625
00:26:51,880 --> 00:26:57,910
engines for free search engines for

626
00:26:54,850 --> 00:27:00,550
scientists and researchers that

627
00:26:57,910 --> 00:27:02,830
hopefully make them more efficient in

628
00:27:00,550 --> 00:27:05,080
doing their science and that will

629
00:27:02,830 --> 00:27:07,840
hopefully help us whether we're trying

630
00:27:05,080 --> 00:27:11,080
to deal with climate change or superbugs

631
00:27:07,840 --> 00:27:13,149
or whatever the intellectual and

632
00:27:11,080 --> 00:27:18,000
technical challenges of the day so I

633
00:27:13,150 --> 00:27:20,050
think as we deliver these high quality

634
00:27:18,000 --> 00:27:21,970
beneficial applications they'll make a

635
00:27:20,050 --> 00:27:26,230
difference I do want to make one point

636
00:27:21,970 --> 00:27:29,290
about bias before in there generally so

637
00:27:26,230 --> 00:27:31,660
far right it has been negative but again

638
00:27:29,290 --> 00:27:33,550
as Tracy pointed out bias is part of the

639
00:27:31,660 --> 00:27:35,410
human condition and one interesting

640
00:27:33,550 --> 00:27:38,379
thing about bias when we see it in

641
00:27:35,410 --> 00:27:42,070
people is they tend to lie about it

642
00:27:38,380 --> 00:27:44,050
right whereas REI systems at least

643
00:27:42,070 --> 00:27:47,860
unless they're designed to lie won't

644
00:27:44,050 --> 00:27:50,409
they're more able to stick to a set of

645
00:27:47,860 --> 00:27:53,530
rules in a set of constraints so the

646
00:27:50,410 --> 00:27:55,900
potential in AI systems if we design

647
00:27:53,530 --> 00:27:59,050
them in the right way is to reduce bias

648
00:27:55,900 --> 00:28:05,580
to note when bias exists in human

649
00:27:59,050 --> 00:28:07,600
behavior for example say black you know

650
00:28:05,580 --> 00:28:10,720
defendants of color are treated

651
00:28:07,600 --> 00:28:13,209
differently than defendants of the white

652
00:28:10,720 --> 00:28:15,430
race and so on so I think the potential

653
00:28:13,210 --> 00:28:17,620
is there for a lot of good we just have

654
00:28:15,430 --> 00:28:19,750
to build the systems the right way I

655
00:28:17,620 --> 00:28:22,570
want to add on to that just a little bit

656
00:28:19,750 --> 00:28:24,540
bit on what Oren was saying there there

657
00:28:22,570 --> 00:28:28,000
are people who are looking at how to

658
00:28:24,540 --> 00:28:30,280
correct for biases that we don't want to

659
00:28:28,000 --> 00:28:32,290
see in these systems that we're building

660
00:28:30,280 --> 00:28:34,780
so in one example they're looking at

661
00:28:32,290 --> 00:28:36,250
correlations of words where male was

662
00:28:34,780 --> 00:28:38,530
more often associated with doctor and

663
00:28:36,250 --> 00:28:41,080
female with nurse but knowing that this

664
00:28:38,530 --> 00:28:43,330
bias exists it meant that the people

665
00:28:41,080 --> 00:28:45,189
developing the models could actually go

666
00:28:43,330 --> 00:28:46,899
in and zero out those correlations which

667
00:28:45,190 --> 00:28:48,460
is very promising so if you tell

668
00:28:46,900 --> 00:28:50,260
somebody or even if somebody

669
00:28:48,460 --> 00:28:52,420
acknowledges that they are biased

670
00:28:50,260 --> 00:28:54,670
yes I am sexist because I live in

671
00:28:52,420 --> 00:28:56,890
society and I have picked up cues from

672
00:28:54,670 --> 00:28:59,830
society it's very difficult to correct

673
00:28:56,890 --> 00:29:01,570
for that and so even when humans are

674
00:28:59,830 --> 00:29:04,659
very aware in trying to address it it's

675
00:29:01,570 --> 00:29:06,040
very difficult to fix whereas in the

676
00:29:04,660 --> 00:29:08,830
machine learning models you can actually

677
00:29:06,040 --> 00:29:11,649
go server out those biases let me give

678
00:29:08,830 --> 00:29:13,270
just a really concrete example of a

679
00:29:11,650 --> 00:29:14,860
trace who's saying that we've we've

680
00:29:13,270 --> 00:29:17,889
thought of it we haven't built this but

681
00:29:14,860 --> 00:29:20,409
I hope that we will so again

682
00:29:17,890 --> 00:29:23,470
unconscious bias in job applications is

683
00:29:20,410 --> 00:29:26,230
well known we currently do not have a

684
00:29:23,470 --> 00:29:27,880
way of stripping a gender

685
00:29:26,230 --> 00:29:29,800
characteristics from resumes you can

686
00:29:27,880 --> 00:29:32,530
take out the names you can take out the

687
00:29:29,800 --> 00:29:34,300
pictures but then you know president of

688
00:29:32,530 --> 00:29:36,220
the women's rowing club or whatever it

689
00:29:34,300 --> 00:29:38,860
is it's pretty easy to tell a person's

690
00:29:36,220 --> 00:29:42,820
gender from a standard resume well we

691
00:29:38,860 --> 00:29:45,340
can build models right where the models

692
00:29:42,820 --> 00:29:47,379
would strip away that information

693
00:29:45,340 --> 00:29:50,230
automatically and when the resume is

694
00:29:47,380 --> 00:29:53,320
presented to the recruiter it can be

695
00:29:50,230 --> 00:29:55,060
very difficult to tell the gender or the

696
00:29:53,320 --> 00:29:57,189
race of the person and that would

697
00:29:55,060 --> 00:30:00,129
actually be a learning problem right

698
00:29:57,190 --> 00:30:02,800
build something that makes it as

699
00:30:00,130 --> 00:30:05,980
difficult as possible or impossible to

700
00:30:02,800 --> 00:30:08,230
tell the person's characteristics that

701
00:30:05,980 --> 00:30:10,240
you want to obscure and then give the

702
00:30:08,230 --> 00:30:12,280
resume to people so that's a great

703
00:30:10,240 --> 00:30:15,160
example of what you were saying in a

704
00:30:12,280 --> 00:30:17,440
very concrete context there would make a

705
00:30:15,160 --> 00:30:20,050
difference I want to respond to that

706
00:30:17,440 --> 00:30:21,430
very quickly which is that that is very

707
00:30:20,050 --> 00:30:23,800
powerful that we could strip those

708
00:30:21,430 --> 00:30:25,510
identifying characteristics the next

709
00:30:23,800 --> 00:30:28,000
question I would have is does that

710
00:30:25,510 --> 00:30:29,920
actually improve the screening process

711
00:30:28,000 --> 00:30:31,480
because actually in some cases when I've

712
00:30:29,920 --> 00:30:33,400
reviewed resumes if I see that someone

713
00:30:31,480 --> 00:30:35,650
let's say someone is a black woman and

714
00:30:33,400 --> 00:30:38,860
they are still in tech I feel like they

715
00:30:35,650 --> 00:30:40,570
must have faced much more obstacles and

716
00:30:38,860 --> 00:30:43,149
the fact that they're still around is

717
00:30:40,570 --> 00:30:45,070
probably a positive signal on their

718
00:30:43,150 --> 00:30:46,960
tenacity and ability to stick it out in

719
00:30:45,070 --> 00:30:48,340
an unfriendly environment so I may

720
00:30:46,960 --> 00:30:50,920
actually want to know some of those

721
00:30:48,340 --> 00:30:54,040
details in some cases because it

722
00:30:50,920 --> 00:30:56,320
contextualizes and you know the prestige

723
00:30:54,040 --> 00:30:58,000
of university there's bias and who has

724
00:30:56,320 --> 00:30:59,860
access to certain universities we can't

725
00:30:58,000 --> 00:31:02,410
afford to pay for them and so if

726
00:30:59,860 --> 00:31:04,539
somebody has stuck it out through

727
00:31:02,410 --> 00:31:07,179
State College a man should still become

728
00:31:04,539 --> 00:31:10,059
very successful that sort of distance

729
00:31:07,179 --> 00:31:11,980
traveled metric is very powerful and so

730
00:31:10,059 --> 00:31:13,330
even if we could remove all the

731
00:31:11,980 --> 00:31:16,240
information I'm not sure that I always

732
00:31:13,330 --> 00:31:18,820
would but there are actually still

733
00:31:16,240 --> 00:31:21,100
interesting ways of doing I'm a

734
00:31:18,820 --> 00:31:23,678
candidate screening that could address

735
00:31:21,100 --> 00:31:25,928
some of these issues in a different way

736
00:31:23,679 --> 00:31:28,059
so I I know some people were working on

737
00:31:25,929 --> 00:31:30,580
predictors of job performance at our

738
00:31:28,059 --> 00:31:33,010
base on more psychometric fingerprint

739
00:31:30,580 --> 00:31:34,539
type of details so if you play a game

740
00:31:33,010 --> 00:31:35,620
you can start to assess if in

741
00:31:34,539 --> 00:31:37,480
characteristics of how somebody

742
00:31:35,620 --> 00:31:39,908
approaches problem solving and that can

743
00:31:37,480 --> 00:31:40,960
be very independent of gender race or a

744
00:31:39,909 --> 00:31:43,030
lot of these other superficial

745
00:31:40,960 --> 00:31:45,700
characteristics but can be correlated to

746
00:31:43,030 --> 00:31:47,830
success metrics in an in a specific

747
00:31:45,700 --> 00:31:50,020
workplace and so then you could have

748
00:31:47,830 --> 00:31:51,610
people who are candidates play those

749
00:31:50,020 --> 00:31:54,340
games it doesn't really matter what

750
00:31:51,610 --> 00:31:55,629
their resume says but if they have the

751
00:31:54,340 --> 00:31:57,250
sort of psychometric fingerprint that

752
00:31:55,630 --> 00:31:58,750
matches to high performers in this

753
00:31:57,250 --> 00:32:01,270
specific job then they are better

754
00:31:58,750 --> 00:32:03,220
candidates to hire I presume Bridget is

755
00:32:01,270 --> 00:32:04,480
the one trying to improve the quality of

756
00:32:03,220 --> 00:32:05,460
our leadership that gets you very

757
00:32:04,480 --> 00:32:09,100
excited

758
00:32:05,460 --> 00:32:12,880
it does hopefully does it use of AI

759
00:32:09,100 --> 00:32:15,399
brillkids we are in our last sort of 10

760
00:32:12,880 --> 00:32:17,080
minutes or so so I just wanted to see if

761
00:32:15,400 --> 00:32:19,600
anybody from our audience has any

762
00:32:17,080 --> 00:32:23,668
questions oh we do brilliant fantastic

763
00:32:19,600 --> 00:32:23,668
we've got three there you go

764
00:32:39,929 --> 00:32:45,879
okay thank you so my question addresses

765
00:32:43,440 --> 00:32:48,879
something could be different it's about

766
00:32:45,879 --> 00:32:54,969
the future of warfare we discussed that

767
00:32:48,879 --> 00:32:57,330
yesterday essentially AI will be a very

768
00:32:54,970 --> 00:33:00,399
important component of a future warfare

769
00:32:57,330 --> 00:33:03,189
especially when you think about a

770
00:33:00,399 --> 00:33:07,799
possible conflict with a dictatorship

771
00:33:03,190 --> 00:33:10,659
authoritarian regime how much human

772
00:33:07,799 --> 00:33:13,989
decision-making or power do you think

773
00:33:10,659 --> 00:33:15,879
will be left in in the future on the

774
00:33:13,989 --> 00:33:20,230
military level or even the political

775
00:33:15,879 --> 00:33:22,238
level is there any hope well this is

776
00:33:20,230 --> 00:33:24,940
something that I've written about and of

777
00:33:22,239 --> 00:33:26,739
course I don't have a crystal ball but

778
00:33:24,940 --> 00:33:29,379
I'd like to make a conceptual

779
00:33:26,739 --> 00:33:31,119
distinction that I do think is important

780
00:33:29,379 --> 00:33:34,359
here and that's the distinction between

781
00:33:31,119 --> 00:33:36,220
intelligence and autonomy and we're used

782
00:33:34,359 --> 00:33:38,949
to people who are of course are both and

783
00:33:36,220 --> 00:33:40,600
so it's easy to conflate those but I

784
00:33:38,950 --> 00:33:43,539
would argue that we can have

785
00:33:40,600 --> 00:33:46,289
increasingly intelligent weapons in the

786
00:33:43,539 --> 00:33:48,759
sense that they're more targeted

787
00:33:46,289 --> 00:33:50,679
potentially more powerful and that has

788
00:33:48,759 --> 00:33:53,019
its own mixed dynamic right more

789
00:33:50,679 --> 00:33:55,119
targeted might reduce civilian

790
00:33:53,019 --> 00:33:57,489
casualties more powerful is always a

791
00:33:55,119 --> 00:34:00,249
scary thing if you think of incredibly

792
00:33:57,489 --> 00:34:02,499
powerful cyber weapons but I do think

793
00:34:00,249 --> 00:34:04,450
that's somewhat inevitable simply

794
00:34:02,499 --> 00:34:06,549
because of the dynamics of the arms race

795
00:34:04,450 --> 00:34:08,199
but the autonomy question which is the

796
00:34:06,549 --> 00:34:10,210
second part of which you raised is a

797
00:34:08,199 --> 00:34:13,210
completely separate thing so as we have

798
00:34:10,210 --> 00:34:16,059
increasingly intelligent weapons we can

799
00:34:13,210 --> 00:34:19,949
retain the control of these weapons in

800
00:34:16,059 --> 00:34:22,119
the hands of humans and I personally

801
00:34:19,949 --> 00:34:23,888
even though humans have their own

802
00:34:22,119 --> 00:34:27,579
weaknesses certainly feel more

803
00:34:23,889 --> 00:34:30,369
comfortable that way that people are

804
00:34:27,579 --> 00:34:32,169
still the ones making the decisions so I

805
00:34:30,369 --> 00:34:35,220
just want to draw that distinction and

806
00:34:32,168 --> 00:34:38,848
at least argue that it's possible to

807
00:34:35,219 --> 00:34:38,848
separate those two dimensions

808
00:34:41,679 --> 00:34:47,629
yes so just as an example in a defensive

809
00:34:45,320 --> 00:34:50,780
context right if the Israel has deployed

810
00:34:47,629 --> 00:34:52,940
weapons to defend against you know the

811
00:34:50,780 --> 00:34:56,750
indiscriminate shelling of civilian

812
00:34:52,940 --> 00:34:59,600
population centers by Palestinians and

813
00:34:56,750 --> 00:35:01,510
others and in that context right there

814
00:34:59,600 --> 00:35:05,118
is no human-in-the-loop it's a purely

815
00:35:01,510 --> 00:35:08,630
defensive measure in other situations

816
00:35:05,119 --> 00:35:12,340
you do lose some speed but on the other

817
00:35:08,630 --> 00:35:15,470
end you can gain an accuracy right so I

818
00:35:12,340 --> 00:35:18,320
it is a choice but I think we can draw

819
00:35:15,470 --> 00:35:22,609
the line there we'll leave some speed

820
00:35:18,320 --> 00:35:25,780
but we will still have the authority for

821
00:35:22,610 --> 00:35:28,010
taking human life rest with a person I

822
00:35:25,780 --> 00:35:31,100
think we have one question in the back

823
00:35:28,010 --> 00:35:33,170
there as well so what do you tell people

824
00:35:31,100 --> 00:35:34,730
you know when they look at systems I

825
00:35:33,170 --> 00:35:36,680
mean the current crop of systems in the

826
00:35:34,730 --> 00:35:39,950
current wave they produce systems which

827
00:35:36,680 --> 00:35:41,960
are basically totally opaque actually

828
00:35:39,950 --> 00:35:43,910
have no way to look inside the current

829
00:35:41,960 --> 00:35:45,020
type of systems not like in old days we

830
00:35:43,910 --> 00:35:47,029
had ruled these systems you could look

831
00:35:45,020 --> 00:35:48,490
inside I mean there was fine so these

832
00:35:47,030 --> 00:35:51,380
systems are totally unexplainable

833
00:35:48,490 --> 00:35:53,390
they're totally opaque and there's

834
00:35:51,380 --> 00:35:55,640
actually no really couldn't you know

835
00:35:53,390 --> 00:35:57,200
convincing way to do you know

836
00:35:55,640 --> 00:35:59,359
verification validation test and

837
00:35:57,200 --> 00:36:00,859
evaluation of a system when you can't

838
00:35:59,359 --> 00:36:03,350
actually ever look inside I mean you can

839
00:36:00,859 --> 00:36:04,490
make a guess but that's about it so what

840
00:36:03,350 --> 00:36:06,109
do you tell people when they say okay

841
00:36:04,490 --> 00:36:14,060
well I'm gonna put it in a critical

842
00:36:06,109 --> 00:36:16,730
situation what about that so two points

843
00:36:14,060 --> 00:36:19,070
one is actually three points very

844
00:36:16,730 --> 00:36:20,510
quickly one is even in the old days of

845
00:36:19,070 --> 00:36:23,690
rule-based systems right when you had a

846
00:36:20,510 --> 00:36:25,220
large set of rules there was a bunch of

847
00:36:23,690 --> 00:36:28,040
opacity there particularly the

848
00:36:25,220 --> 00:36:30,020
interactions between them so we were

849
00:36:28,040 --> 00:36:32,270
younger in the past but it wasn't always

850
00:36:30,020 --> 00:36:34,940
so much better point one point two

851
00:36:32,270 --> 00:36:36,380
people are working on more explainable

852
00:36:34,940 --> 00:36:39,380
systems you're absolutely right it's

853
00:36:36,380 --> 00:36:41,960
extremely difficult with these nonlinear

854
00:36:39,380 --> 00:36:43,369
neural networks but the third piece

855
00:36:41,960 --> 00:36:46,400
which is actually where I have the most

856
00:36:43,369 --> 00:36:48,320
hope is externally auditing them so what

857
00:36:46,400 --> 00:36:50,780
you can do even if the system is

858
00:36:48,320 --> 00:36:51,790
relatively a black box you can fire at

859
00:36:50,780 --> 00:36:53,710
it

860
00:36:51,790 --> 00:36:56,529
literally hundreds of thousands if not

861
00:36:53,710 --> 00:36:59,160
millions of cases and see how its loan

862
00:36:56,530 --> 00:37:01,450
processing or criminal justice or

863
00:36:59,160 --> 00:37:04,540
weapons behavior and this external

864
00:37:01,450 --> 00:37:07,049
auditing can give you a model of how the

865
00:37:04,540 --> 00:37:09,190
system will behave so I believe that

866
00:37:07,050 --> 00:37:11,770
we're gonna have increasingly

867
00:37:09,190 --> 00:37:14,860
sophisticated auditing system and that

868
00:37:11,770 --> 00:37:17,440
will help one thing I had as a flipside

869
00:37:14,860 --> 00:37:18,820
to being able to peer into some of these

870
00:37:17,440 --> 00:37:21,700
systems is that if they are being used

871
00:37:18,820 --> 00:37:24,040
in context that may become adversarial

872
00:37:21,700 --> 00:37:25,480
having too much visibility is dangerous

873
00:37:24,040 --> 00:37:26,920
so there are some of these computer

874
00:37:25,480 --> 00:37:28,420
vision systems now that can be very

875
00:37:26,920 --> 00:37:31,030
easily tricked just by adding in a few

876
00:37:28,420 --> 00:37:32,620
pixels in the right places which is kind

877
00:37:31,030 --> 00:37:34,270
of fun for now you could put on one of

878
00:37:32,620 --> 00:37:36,520
these like stickers and then confuse the

879
00:37:34,270 --> 00:37:38,890
computer vision algorithms but if you

880
00:37:36,520 --> 00:37:41,320
wanted to use these it's a kick in

881
00:37:38,890 --> 00:37:43,000
defense applications you don't

882
00:37:41,320 --> 00:37:45,700
necessarily want the adversary to be

883
00:37:43,000 --> 00:37:46,570
able to trick your system so I'm not

884
00:37:45,700 --> 00:37:47,950
saying that we shouldn't have

885
00:37:46,570 --> 00:37:50,860
interpretable systems and we definitely

886
00:37:47,950 --> 00:37:52,600
should try to understand more what is

887
00:37:50,860 --> 00:37:54,610
going to be the output with certain

888
00:37:52,600 --> 00:37:56,759
inputs but that's another consideration

889
00:37:54,610 --> 00:37:59,620
which is like how can they be weaponized

890
00:37:56,760 --> 00:38:01,390
you can see this in more I guess like

891
00:37:59,620 --> 00:38:03,670
the civilian or commercial context with

892
00:38:01,390 --> 00:38:06,430
a Google search so Google is very

893
00:38:03,670 --> 00:38:09,070
protective of what inputs are being used

894
00:38:06,430 --> 00:38:11,230
and how they're being combined to do

895
00:38:09,070 --> 00:38:13,420
search result because you always have

896
00:38:11,230 --> 00:38:16,780
people trying to rise to the top that's

897
00:38:13,420 --> 00:38:18,730
like the whole domain of SEO so people

898
00:38:16,780 --> 00:38:20,440
are they used to do really simplistic

899
00:38:18,730 --> 00:38:21,730
things like keyword stuffing to get the

900
00:38:20,440 --> 00:38:23,440
results that rise to the top but then

901
00:38:21,730 --> 00:38:25,330
you have these spammers and fraudsters

902
00:38:23,440 --> 00:38:27,550
who are trying to rise the top so Google

903
00:38:25,330 --> 00:38:30,549
has to be very protective there so it's

904
00:38:27,550 --> 00:38:40,000
just a very difficult thing to advocate

905
00:38:30,550 --> 00:38:41,230
for true transparency if you don't

906
00:38:40,000 --> 00:38:43,900
really if you don't really know what's

907
00:38:41,230 --> 00:38:45,760
happening in a system the attack may not

908
00:38:43,900 --> 00:38:47,410
be adversarial or intention it may be if

909
00:38:45,760 --> 00:38:49,180
you're running a system in a natural

910
00:38:47,410 --> 00:38:52,230
environment you're subject to you have

911
00:38:49,180 --> 00:38:54,399
no idea what exactly so it's not only

912
00:38:52,230 --> 00:38:56,710
adversarial attacks it's also just

913
00:38:54,400 --> 00:38:58,810
natural occurrences couldn't screw up

914
00:38:56,710 --> 00:39:00,610
just as badly I mean we've seen Facebook

915
00:38:58,810 --> 00:39:01,990
and Twitter get weaponized and they I

916
00:39:00,610 --> 00:39:04,750
don't think they even fully understood

917
00:39:01,990 --> 00:39:05,620
what they were getting used for we had a

918
00:39:04,750 --> 00:39:07,060
chance at the fur

919
00:39:05,620 --> 00:39:09,339
student at the frontier and then we'll

920
00:39:07,060 --> 00:39:11,860
go to the back so these two people here

921
00:39:09,340 --> 00:39:13,750
first thank you could we have a woman

922
00:39:11,860 --> 00:39:17,170
ask a question yeah I was gonna I was

923
00:39:13,750 --> 00:39:22,060
hoping that we could make maybe a lady's

924
00:39:17,170 --> 00:39:24,040
voice and if that would I'm Victor soot

925
00:39:22,060 --> 00:39:27,250
from the Coptic boys Institute I really

926
00:39:24,040 --> 00:39:30,400
like the examples of the benefits if I

927
00:39:27,250 --> 00:39:32,410
may go back to that question but all but

928
00:39:30,400 --> 00:39:34,210
they sort of make me feel they are a

929
00:39:32,410 --> 00:39:36,490
little bit more statistical when we come

930
00:39:34,210 --> 00:39:37,600
when we talked about reducing number of

931
00:39:36,490 --> 00:39:40,629
incidents whether there are car

932
00:39:37,600 --> 00:39:43,210
incidents or incidents that happen in

933
00:39:40,630 --> 00:39:45,160
surgeries because they're basically also

934
00:39:43,210 --> 00:39:46,570
non events are on day if something

935
00:39:45,160 --> 00:39:48,790
doesn't happen to you you might not

936
00:39:46,570 --> 00:39:51,490
register it on a on a personal level so

937
00:39:48,790 --> 00:39:53,980
I was just wondering if the trick to

938
00:39:51,490 --> 00:39:56,919
make the conversation you know shift

939
00:39:53,980 --> 00:40:01,060
towards the benefits of the AI also in

940
00:39:56,920 --> 00:40:02,830
argumentum maybe the the experience for

941
00:40:01,060 --> 00:40:05,549
instance in cars it would be the luxury

942
00:40:02,830 --> 00:40:08,319
of not having to pay attention you can

943
00:40:05,550 --> 00:40:10,360
it's more comfortable you can be

944
00:40:08,320 --> 00:40:13,030
focusing on other things or would there

945
00:40:10,360 --> 00:40:15,310
be other examples of benefits that would

946
00:40:13,030 --> 00:40:21,220
register more on individual or more

947
00:40:15,310 --> 00:40:24,009
personal level individually for the

948
00:40:21,220 --> 00:40:26,410
first question there what's the what is

949
00:40:24,010 --> 00:40:30,730
the best way of how can we reduce

950
00:40:26,410 --> 00:40:32,379
friction I guess here I like your

951
00:40:30,730 --> 00:40:38,890
suggestion of making it more personal I

952
00:40:32,380 --> 00:40:40,000
don't have much to add ok bro we have we

953
00:40:38,890 --> 00:40:42,730
have an ir developer there I think

954
00:40:40,000 --> 00:40:44,680
basically yes hello thank you for for

955
00:40:42,730 --> 00:40:47,050
your insight so far so my name is David

956
00:40:44,680 --> 00:40:48,790
and I work for Google in protecting

957
00:40:47,050 --> 00:40:51,070
elections actually so it's a bit maybe

958
00:40:48,790 --> 00:40:52,930
not relevant in this discussion however

959
00:40:51,070 --> 00:40:55,960
my question is connected to I think what

960
00:40:52,930 --> 00:40:58,450
Tracy mentioned when we speak about a I

961
00:40:55,960 --> 00:41:00,430
think AI will increase the these

962
00:40:58,450 --> 00:41:03,490
discrepancies between between the you

963
00:41:00,430 --> 00:41:04,720
know the haves and the have-nots and for

964
00:41:03,490 --> 00:41:06,430
the have-nots what do you think will be

965
00:41:04,720 --> 00:41:07,899
a solution way when when AI will

966
00:41:06,430 --> 00:41:09,490
potentially displace them for their jobs

967
00:41:07,900 --> 00:41:12,520
do you believe in a universal basic

968
00:41:09,490 --> 00:41:14,709
income is this an option or taxing

969
00:41:12,520 --> 00:41:16,150
robots how do you first see a future in

970
00:41:14,710 --> 00:41:19,420
which there will be for sure people that

971
00:41:16,150 --> 00:41:21,400
will be let's say not benefit

972
00:41:19,420 --> 00:41:23,859
from AI as some of the other groups of

973
00:41:21,400 --> 00:41:25,000
people how will we help them that's my

974
00:41:23,859 --> 00:41:28,420
question thank you

975
00:41:25,000 --> 00:41:29,890
I mean I'm personally an advocate for

976
00:41:28,420 --> 00:41:32,140
the universal basic income but that's

977
00:41:29,890 --> 00:41:35,319
just as a human rather than necessarily

978
00:41:32,140 --> 00:41:37,779
as anything connected to AI

979
00:41:35,319 --> 00:41:39,308
I mean how yeah how do we we have talked

980
00:41:37,780 --> 00:41:40,930
about those people do get left behind in

981
00:41:39,309 --> 00:41:44,849
this in these technological revolution

982
00:41:40,930 --> 00:41:47,558
so how do you prevent that it is a

983
00:41:44,849 --> 00:41:51,099
complex issue I would just make a couple

984
00:41:47,559 --> 00:41:53,290
of points one is a universal basic

985
00:41:51,099 --> 00:41:54,730
income at least in the United States is

986
00:41:53,290 --> 00:41:57,430
not practical

987
00:41:54,730 --> 00:41:59,319
we haven't even even though we're a very

988
00:41:57,430 --> 00:42:02,919
rich society we haven't even been able

989
00:41:59,319 --> 00:42:05,589
to deliver universal health care so I

990
00:42:02,920 --> 00:42:08,619
think just as a practical matter we may

991
00:42:05,589 --> 00:42:12,099
need a different solution I also think

992
00:42:08,619 --> 00:42:13,540
that the problem with taxing robots on

993
00:42:12,099 --> 00:42:16,299
the one hand it seems sensible right

994
00:42:13,540 --> 00:42:19,390
then we could use that income to deliver

995
00:42:16,299 --> 00:42:22,359
social programs you tend to tax things

996
00:42:19,390 --> 00:42:24,520
that you want less of right like a sin

997
00:42:22,359 --> 00:42:28,390
tax right you know we tax cigarettes we

998
00:42:24,520 --> 00:42:32,290
tax alcohol it's unclear whether we want

999
00:42:28,390 --> 00:42:35,400
fewer robots or not because again that

1000
00:42:32,290 --> 00:42:37,720
puts us somewhat at a disadvantage

1001
00:42:35,400 --> 00:42:39,190
internationally some I'm just concerned

1002
00:42:37,720 --> 00:42:41,859
about those two solutions for different

1003
00:42:39,190 --> 00:42:45,250
reasons the one thing that I do like is

1004
00:42:41,859 --> 00:42:47,828
if we were to provide tax credits to

1005
00:42:45,250 --> 00:42:50,470
people for or other benefits for

1006
00:42:47,829 --> 00:42:53,859
improving their education for for

1007
00:42:50,470 --> 00:42:56,439
working and so if we can provide

1008
00:42:53,859 --> 00:42:58,089
incentives for positive behaviors that

1009
00:42:56,440 --> 00:43:00,430
strikes me as something that would be

1010
00:42:58,089 --> 00:43:02,589
perhaps more practical than universal

1011
00:43:00,430 --> 00:43:06,098
basic income and perhaps promote

1012
00:43:02,589 --> 00:43:08,920
positive things rather than tax robots

1013
00:43:06,099 --> 00:43:11,589
but I don't have an answer and the

1014
00:43:08,920 --> 00:43:13,299
problem is absolutely a big one the last

1015
00:43:11,589 --> 00:43:14,920
point I want to make real quick is that

1016
00:43:13,299 --> 00:43:17,530
this is part of a trend that's been

1017
00:43:14,920 --> 00:43:21,400
taking place over some decades right so

1018
00:43:17,530 --> 00:43:22,930
where did all these uber drivers come

1019
00:43:21,400 --> 00:43:27,720
from right there were a displace from

1020
00:43:22,930 --> 00:43:31,069
other positions Amazon and e-commerce

1021
00:43:27,720 --> 00:43:32,870
even without a eyes already creating

1022
00:43:31,070 --> 00:43:36,230
you know loss of jobs in the retail

1023
00:43:32,870 --> 00:43:37,970
sector so we've had some decades of

1024
00:43:36,230 --> 00:43:40,490
these trends and they're going to

1025
00:43:37,970 --> 00:43:43,279
continue we need to do something about

1026
00:43:40,490 --> 00:43:46,009
it that's broader than just dealing with

1027
00:43:43,280 --> 00:43:54,470
AI it's really about digitization and

1028
00:43:46,010 --> 00:43:57,260
autom automation great back yes since

1029
00:43:54,470 --> 00:43:59,359
we're in the overarching theme is

1030
00:43:57,260 --> 00:44:03,230
navigating through disruptions can we

1031
00:43:59,360 --> 00:44:06,220
sort of talk about the global political

1032
00:44:03,230 --> 00:44:12,370
implications in terms of the disruptions

1033
00:44:06,220 --> 00:44:16,790
across countries like China India

1034
00:44:12,370 --> 00:44:19,069
Estonia as a one-element in Pakistan and

1035
00:44:16,790 --> 00:44:22,009
the implications of how those

1036
00:44:19,070 --> 00:44:25,220
disruptions will be managed and your

1037
00:44:22,010 --> 00:44:28,790
sense of the scale and impact of the

1038
00:44:25,220 --> 00:44:31,330
disruptions as it affects countries

1039
00:44:28,790 --> 00:44:34,100
differently and democracies versus

1040
00:44:31,330 --> 00:44:38,450
authoritarian regimes and the ability to

1041
00:44:34,100 --> 00:44:40,430
respond to those disruptions so this is

1042
00:44:38,450 --> 00:44:54,140
the AI arms race I guess right who's

1043
00:44:40,430 --> 00:44:57,319
gonna win out of this China okay then

1044
00:44:54,140 --> 00:44:59,690
say Estonia versus India and China and

1045
00:44:57,320 --> 00:45:01,570
so if you have a lot of people and

1046
00:44:59,690 --> 00:45:04,250
they're out of work because of the

1047
00:45:01,570 --> 00:45:06,200
disruptions of of Technology

1048
00:45:04,250 --> 00:45:08,840
they're greater political and social

1049
00:45:06,200 --> 00:45:11,810
implications may or may not be true so

1050
00:45:08,840 --> 00:45:13,580
the question is is anyone thinking about

1051
00:45:11,810 --> 00:45:17,090
that how do you manage it and those

1052
00:45:13,580 --> 00:45:18,620
disruptions is Traci said policy is not

1053
00:45:17,090 --> 00:45:21,550
going to be ahead of it it's going to be

1054
00:45:18,620 --> 00:45:23,569
behind it so the disruptions and

1055
00:45:21,550 --> 00:45:25,550
dislocations are going to be here and

1056
00:45:23,570 --> 00:45:28,640
the implications are going to be here

1057
00:45:25,550 --> 00:45:30,920
and are going to be not evenly felt

1058
00:45:28,640 --> 00:45:36,740
across countries what are you guys

1059
00:45:30,920 --> 00:45:40,250
thinking about that again very sweeping

1060
00:45:36,740 --> 00:45:42,000
question and a good one let me offer a

1061
00:45:40,250 --> 00:45:44,070
partial answer just

1062
00:45:42,000 --> 00:45:47,730
in the hopes of giving my co-panelists a

1063
00:45:44,070 --> 00:45:49,560
chance to think I don't know that

1064
00:45:47,730 --> 00:45:51,540
anybody has an adequate answer but two

1065
00:45:49,560 --> 00:45:54,200
points something that's working well and

1066
00:45:51,540 --> 00:45:57,900
I hope we continue to preserve it is the

1067
00:45:54,200 --> 00:46:01,710
open dissemination of information across

1068
00:45:57,900 --> 00:46:04,440
universities open source archive and so

1069
00:46:01,710 --> 00:46:06,170
on so even as tensions between the US

1070
00:46:04,440 --> 00:46:09,450
and China and other tensions are

1071
00:46:06,170 --> 00:46:12,840
inflamed so far there hasn't been there

1072
00:46:09,450 --> 00:46:14,819
has not been an attempt to curtail the

1073
00:46:12,840 --> 00:46:16,830
exchange of information in academic

1074
00:46:14,820 --> 00:46:19,050
forums and I hope that continues and

1075
00:46:16,830 --> 00:46:22,799
that of course gives smaller players

1076
00:46:19,050 --> 00:46:26,670
equal access to this sort of you know

1077
00:46:22,800 --> 00:46:29,010
the papers the the open source code the

1078
00:46:26,670 --> 00:46:30,960
open source data etc so I think that's a

1079
00:46:29,010 --> 00:46:34,050
good thing something that I think is a

1080
00:46:30,960 --> 00:46:36,410
huge negative both because of lack of

1081
00:46:34,050 --> 00:46:39,180
cross fertilization and also simply

1082
00:46:36,410 --> 00:46:42,750
weakens the United States I do think we

1083
00:46:39,180 --> 00:46:47,029
have increasingly adversarial

1084
00:46:42,750 --> 00:46:51,960
relationship with China is the

1085
00:46:47,030 --> 00:46:54,920
restraints on immigration so we have

1086
00:46:51,960 --> 00:46:57,240
fewer and fewer people applying to our

1087
00:46:54,920 --> 00:46:59,760
undergrad programs to our graduate

1088
00:46:57,240 --> 00:47:02,430
programs from outside of the United

1089
00:46:59,760 --> 00:47:04,800
States we have fewer people being

1090
00:47:02,430 --> 00:47:06,779
allowed to stay due to a variety of

1091
00:47:04,800 --> 00:47:10,440
measures that the Trump administration

1092
00:47:06,780 --> 00:47:12,720
has taken and the result of that is a

1093
00:47:10,440 --> 00:47:14,640
real negative for artificial

1094
00:47:12,720 --> 00:47:17,160
intelligence in the United States and

1095
00:47:14,640 --> 00:47:20,640
that's something I hope that we we

1096
00:47:17,160 --> 00:47:25,049
reverse put in plain terms we need more

1097
00:47:20,640 --> 00:47:27,029
visas for people to study AI and to be

1098
00:47:25,050 --> 00:47:29,100
part of the ecosystem in the United

1099
00:47:27,030 --> 00:47:32,370
States we need more visas rather than

1100
00:47:29,100 --> 00:47:33,390
less I was little bit hesitant to answer

1101
00:47:32,370 --> 00:47:35,490
because I don't feel like I'm a good

1102
00:47:33,390 --> 00:47:38,279
enough student of geopolitics and

1103
00:47:35,490 --> 00:47:41,250
society to give a good answer but I'll

1104
00:47:38,280 --> 00:47:43,590
also offer some thoughts on where

1105
00:47:41,250 --> 00:47:48,660
technology and AI are potentially going

1106
00:47:43,590 --> 00:47:52,620
to cause disruption in bad ways so with

1107
00:47:48,660 --> 00:47:54,779
technology in independent of AI if we're

1108
00:47:52,620 --> 00:47:55,770
looking at some of the big companies

1109
00:47:54,780 --> 00:47:58,950
like Google

1110
00:47:55,770 --> 00:48:01,200
 uber they're starting to provide

1111
00:47:58,950 --> 00:48:02,759
services across the world but all the

1112
00:48:01,200 --> 00:48:04,710
wealth concentration all that power

1113
00:48:02,760 --> 00:48:07,680
concentration is still primarily in

1114
00:48:04,710 --> 00:48:10,200
those companies in Silicon Valley or in

1115
00:48:07,680 --> 00:48:14,100
the United States so they control the

1116
00:48:10,200 --> 00:48:18,089
the algorithms that determine who sees

1117
00:48:14,100 --> 00:48:21,140
what who can connect with whom the

1118
00:48:18,090 --> 00:48:23,160
economic so for uber the economics of

1119
00:48:21,140 --> 00:48:25,259
providing the service across the world

1120
00:48:23,160 --> 00:48:31,109
have primarily concentrated in San

1121
00:48:25,260 --> 00:48:32,430
Francisco so when we look at an AI it

1122
00:48:31,110 --> 00:48:35,100
gets even more powerful it's no longer

1123
00:48:32,430 --> 00:48:37,589
just the technology the data the

1124
00:48:35,100 --> 00:48:39,870
computer or the compute but all right

1125
00:48:37,590 --> 00:48:42,480
the data part of it becomes an even

1126
00:48:39,870 --> 00:48:45,450
bigger concentrator of that power and

1127
00:48:42,480 --> 00:48:47,720
we're seeing it primarily in the US and

1128
00:48:45,450 --> 00:48:51,750
in China and there are not any other

1129
00:48:47,720 --> 00:48:53,009
very compelling players so now we have a

1130
00:48:51,750 --> 00:48:56,190
little bit this arms between the US and

1131
00:48:53,010 --> 00:48:58,290
China as Orrin mentioned you know the

1132
00:48:56,190 --> 00:48:59,820
u.s. is doing some really dumb things

1133
00:48:58,290 --> 00:49:03,470
around immigration that it's going to

1134
00:48:59,820 --> 00:49:05,640
hurt it at a very high level right now

1135
00:49:03,470 --> 00:49:08,009
China is getting much better at

1136
00:49:05,640 --> 00:49:11,129
execution on AI the u.s. is still very

1137
00:49:08,010 --> 00:49:12,690
good at rnd but potentially is going to

1138
00:49:11,130 --> 00:49:14,700
lose out going forward

1139
00:49:12,690 --> 00:49:16,860
so then if we end up in a world where

1140
00:49:14,700 --> 00:49:19,950
China holds most of the power around AI

1141
00:49:16,860 --> 00:49:22,500
and these technology products are used

1142
00:49:19,950 --> 00:49:24,390
around the world they can then control

1143
00:49:22,500 --> 00:49:25,770
the technology that's used by all these

1144
00:49:24,390 --> 00:49:28,040
other countries so now we're not even

1145
00:49:25,770 --> 00:49:30,600
just looking at disruptions within

1146
00:49:28,040 --> 00:49:33,540
individual countries but at a global

1147
00:49:30,600 --> 00:49:35,970
scale we have a change of power where

1148
00:49:33,540 --> 00:49:37,410
China may be even more influential it's

1149
00:49:35,970 --> 00:49:41,339
already been very influential in terms

1150
00:49:37,410 --> 00:49:43,080
of like the trade and economics but if

1151
00:49:41,340 --> 00:49:45,270
it can control the technology as well

1152
00:49:43,080 --> 00:49:46,830
then other countries around the world

1153
00:49:45,270 --> 00:49:49,530
may just have to be beholden to what

1154
00:49:46,830 --> 00:49:51,029
China wants to do then we also have the

1155
00:49:49,530 --> 00:49:52,710
issues like within each country if

1156
00:49:51,030 --> 00:49:54,960
they're not able to control their own

1157
00:49:52,710 --> 00:49:57,540
destiny with technology what happens I

1158
00:49:54,960 --> 00:49:59,970
don't have real answers around what we

1159
00:49:57,540 --> 00:50:03,060
do about a growing income inequality or

1160
00:49:59,970 --> 00:50:05,399
wealth inequality but I think the

1161
00:50:03,060 --> 00:50:07,020
questions we were grappling with earlier

1162
00:50:05,400 --> 00:50:08,400
around like how do we rescale people or

1163
00:50:07,020 --> 00:50:09,480
put them into positions where they can

1164
00:50:08,400 --> 00:50:11,640
still live

1165
00:50:09,480 --> 00:50:13,440
in the absence of the job that they had

1166
00:50:11,640 --> 00:50:16,440
previously we still need to figure out

1167
00:50:13,440 --> 00:50:17,670
all of those questions mmm and speaking

1168
00:50:16,440 --> 00:50:19,320
of questions we are basically out of

1169
00:50:17,670 --> 00:50:22,020
time startlingly so will and a little

1170
00:50:19,320 --> 00:50:26,700
late and one very very very brief answer

1171
00:50:22,020 --> 00:50:29,369
from each of you what is the development

1172
00:50:26,700 --> 00:50:31,439
you're most excited for the use case of

1173
00:50:29,369 --> 00:50:34,470
AI in the future so we'll start with or

1174
00:50:31,440 --> 00:50:36,450
will go down line autonomous vehicles

1175
00:50:34,470 --> 00:50:40,169
because they have the potential to save

1176
00:50:36,450 --> 00:50:42,868
just so many lives richer um health care

1177
00:50:40,170 --> 00:50:44,280
because I think as humans the basic

1178
00:50:42,869 --> 00:50:48,150
thing we all want is health and better

1179
00:50:44,280 --> 00:50:50,730
health and Tracey I think better

1180
00:50:48,150 --> 00:50:52,560
understanding of people so we're

1181
00:50:50,730 --> 00:50:54,300
starting to clock more data on people

1182
00:50:52,560 --> 00:50:56,520
and workforce and how they interact and

1183
00:50:54,300 --> 00:50:58,320
I think if we have a better

1184
00:50:56,520 --> 00:51:00,240
understanding of the the humans that are

1185
00:50:58,320 --> 00:51:02,130
involved hopefully we can make better

1186
00:51:00,240 --> 00:51:04,470
decisions around how to treat people as

1187
00:51:02,130 --> 00:51:06,930
well I think that's a good manifesto for

1188
00:51:04,470 --> 00:51:08,549
life thank you very much for all of

1189
00:51:06,930 --> 00:51:09,990
their inputs and thank you very much for

1190
00:51:08,550 --> 00:51:12,690
joining us hope let you enjoy your

1191
00:51:09,990 --> 00:51:15,740
coffee break and thank you Chris for

1192
00:51:12,690 --> 00:51:15,740
being great

1193
00:51:23,269 --> 00:51:25,328
you

1194
00:51:27,950 --> 00:51:30,009
you

1195
00:51:31,900 --> 00:51:33,960
you

