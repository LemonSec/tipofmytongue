1
00:00:07,839 --> 00:00:08,480
all right

2
00:00:08,480 --> 00:00:10,800
hi everybody welcome to the last session

3
00:00:10,800 --> 00:00:12,400
for the day

4
00:00:12,400 --> 00:00:14,960
thanks for joining us just a few very

5
00:00:14,960 --> 00:00:17,520
quick reminders you are on mute

6
00:00:17,520 --> 00:00:19,840
if you are using the event app awesome

7
00:00:19,840 --> 00:00:21,760
please check into sessions please fill

8
00:00:21,760 --> 00:00:23,359
out this session survey

9
00:00:23,359 --> 00:00:25,439
if you're having trouble please email us

10
00:00:25,439 --> 00:00:27,680
at eventsetfirst.org so we can help you

11
00:00:27,680 --> 00:00:28,560
out

12
00:00:28,560 --> 00:00:30,560
the session is tlp white and is being

13
00:00:30,560 --> 00:00:32,079
recorded the recordings will be

14
00:00:32,079 --> 00:00:34,559
available within 24 hours via the app or

15
00:00:34,559 --> 00:00:36,160
the desktop mobile site

16
00:00:36,160 --> 00:00:37,680
um and with that i would like to

17
00:00:37,680 --> 00:00:39,680
introduce you to your session moderator

18
00:00:39,680 --> 00:00:41,840
serge dros

19
00:00:41,840 --> 00:00:44,960
hello everyone thanks tracy uh

20
00:00:44,960 --> 00:00:46,640
as you said my name is serge ross i'm

21
00:00:46,640 --> 00:00:49,039
session moderator just one little thing

22
00:00:49,039 --> 00:00:50,800
a request to all the attendees if you

23
00:00:50,800 --> 00:00:52,719
have questions please put them in the q

24
00:00:52,719 --> 00:00:53,280
a

25
00:00:53,280 --> 00:00:55,120
and i'll make sure that we find some

26
00:00:55,120 --> 00:00:57,280
time at the end to actually answer these

27
00:00:57,280 --> 00:00:59,039
uh having said that i'd like to

28
00:00:59,039 --> 00:01:01,280
introduce holly stewart anna

29
00:01:01,280 --> 00:01:04,640
bertiger and charada asharia

30
00:01:04,640 --> 00:01:07,040
i hope i said that correctly to talk

31
00:01:07,040 --> 00:01:09,760
about uh

32
00:01:09,760 --> 00:01:11,680
doing more with less detecting malicious

33
00:01:11,680 --> 00:01:13,360
activities through responsible and

34
00:01:13,360 --> 00:01:15,119
privacy preserving ai

35
00:01:15,119 --> 00:01:18,799
and i think after ron's keynote i think

36
00:01:18,799 --> 00:01:20,799
the privacy preserving talk

37
00:01:20,799 --> 00:01:22,640
should kind of generate a mental

38
00:01:22,640 --> 00:01:24,000
applause already now

39
00:01:24,000 --> 00:01:26,320
so without further ado i give the word

40
00:01:26,320 --> 00:01:28,479
to you thanks a lot

41
00:01:28,479 --> 00:01:30,799
thanks serge uh thanks everybody for

42
00:01:30,799 --> 00:01:33,040
joining us at the last talk of the day

43
00:01:33,040 --> 00:01:36,640
um sharda anna and i are from the

44
00:01:36,640 --> 00:01:38,799
microsoft defender security research

45
00:01:38,799 --> 00:01:39,840
team

46
00:01:39,840 --> 00:01:42,159
on our team we build protection

47
00:01:42,159 --> 00:01:43,680
capabilities for

48
00:01:43,680 --> 00:01:46,240
our email endpoint and cross-service

49
00:01:46,240 --> 00:01:47,360
security products

50
00:01:47,360 --> 00:01:50,399
and in particular chiva and

51
00:01:50,399 --> 00:01:54,079
anna and i we focus on using ai as our

52
00:01:54,079 --> 00:01:55,759
sort of superpower

53
00:01:55,759 --> 00:01:58,960
in the fight against cybercrime and so

54
00:01:58,960 --> 00:02:01,200
our teams have spent a lot of time

55
00:02:01,200 --> 00:02:02,479
thinking about

56
00:02:02,479 --> 00:02:04,799
the both the privacy of the people that

57
00:02:04,799 --> 00:02:06,159
we're trying to protect and

58
00:02:06,159 --> 00:02:09,199
also helping to keep them safe

59
00:02:09,199 --> 00:02:12,239
and i think as a lot of folks know these

60
00:02:12,239 --> 00:02:12,720
days

61
00:02:12,720 --> 00:02:16,400
ai as has just become a critical tool

62
00:02:16,400 --> 00:02:18,400
in this fight and just to give you sort

63
00:02:18,400 --> 00:02:20,800
of a day in the life for us

64
00:02:20,800 --> 00:02:24,080
um it's pretty typical for on

65
00:02:24,080 --> 00:02:27,280
any average day to see millions of

66
00:02:27,280 --> 00:02:28,239
threats

67
00:02:28,239 --> 00:02:31,040
across the world um hitting millions of

68
00:02:31,040 --> 00:02:32,640
people

69
00:02:32,640 --> 00:02:33,840
that are just that are first seen

70
00:02:33,840 --> 00:02:36,400
attacks that we've never seen before

71
00:02:36,400 --> 00:02:39,040
and what's worse is that these are often

72
00:02:39,040 --> 00:02:39,440
over

73
00:02:39,440 --> 00:02:42,560
within the hour so there's no time for

74
00:02:42,560 --> 00:02:43,440
response

75
00:02:43,440 --> 00:02:46,080
it would take a security researcher over

76
00:02:46,080 --> 00:02:47,680
a million hours to go

77
00:02:47,680 --> 00:02:49,200
research these threats and figure out

78
00:02:49,200 --> 00:02:50,720
what they are

79
00:02:50,720 --> 00:02:53,599
so to keep people safe today we have to

80
00:02:53,599 --> 00:02:55,840
rely on protection that blocks the tax

81
00:02:55,840 --> 00:02:57,280
the first time we see it

82
00:02:57,280 --> 00:02:59,599
this is the promise of ai and data

83
00:02:59,599 --> 00:03:02,000
science and it's that predictive power

84
00:03:02,000 --> 00:03:05,120
that we are effectively using today to

85
00:03:05,120 --> 00:03:06,640
block this malicious activity

86
00:03:06,640 --> 00:03:10,720
at first sight so ai at this scale

87
00:03:10,720 --> 00:03:13,200
it requires data lots and lots and lots

88
00:03:13,200 --> 00:03:14,000
of data

89
00:03:14,000 --> 00:03:17,040
and any ai practitioner is going to tell

90
00:03:17,040 --> 00:03:18,800
you that the more data you feed the

91
00:03:18,800 --> 00:03:21,120
system the better it's going to perform

92
00:03:21,120 --> 00:03:23,040
especially when it comes to approaches

93
00:03:23,040 --> 00:03:24,879
like deep learning

94
00:03:24,879 --> 00:03:27,760
and so as defenders we are striving to

95
00:03:27,760 --> 00:03:29,280
collect the best information the best

96
00:03:29,280 --> 00:03:29,840
data

97
00:03:29,840 --> 00:03:32,319
to train our ai systems to help these

98
00:03:32,319 --> 00:03:35,040
folks that we're trying to protect

99
00:03:35,040 --> 00:03:37,840
but as does this have to come at the

100
00:03:37,840 --> 00:03:38,640
cost of

101
00:03:38,640 --> 00:03:41,280
privacy so what do you do when these two

102
00:03:41,280 --> 00:03:41,760
things

103
00:03:41,760 --> 00:03:44,239
are at odds do you sacrifice the data

104
00:03:44,239 --> 00:03:44,879
collection

105
00:03:44,879 --> 00:03:47,280
and the privacy knowing that detection

106
00:03:47,280 --> 00:03:49,440
and protection is going to suffer or do

107
00:03:49,440 --> 00:03:51,200
you fight for the data that you

108
00:03:51,200 --> 00:03:54,720
know you need but risk of backlash on

109
00:03:54,720 --> 00:03:58,480
privacy and trust uh that's not really

110
00:03:58,480 --> 00:04:00,000
an option for us

111
00:04:00,000 --> 00:04:02,480
you may have heard this thing microsoft

112
00:04:02,480 --> 00:04:03,840
runs on trust

113
00:04:03,840 --> 00:04:07,120
um and we have to be trusted by the

114
00:04:07,120 --> 00:04:09,040
billions of people that we serve and the

115
00:04:09,040 --> 00:04:10,400
organizations

116
00:04:10,400 --> 00:04:14,000
that are relying on us to protect them

117
00:04:14,000 --> 00:04:16,079
we've made these public commitments to

118
00:04:16,079 --> 00:04:17,839
privacy many years ago

119
00:04:17,839 --> 00:04:20,478
and we are evolving these principles

120
00:04:20,478 --> 00:04:21,040
further

121
00:04:21,040 --> 00:04:23,280
and we've created a framework for

122
00:04:23,280 --> 00:04:24,880
responsible ai

123
00:04:24,880 --> 00:04:26,479
and although all of these principles

124
00:04:26,479 --> 00:04:28,000
that you see here are

125
00:04:28,000 --> 00:04:29,600
important we're going to focus on two of

126
00:04:29,600 --> 00:04:31,360
them today uh

127
00:04:31,360 --> 00:04:34,240
reliability and safety so providing

128
00:04:34,240 --> 00:04:35,840
reliable protection

129
00:04:35,840 --> 00:04:38,560
and how this does not have to be at odds

130
00:04:38,560 --> 00:04:40,639
with privacy and security

131
00:04:40,639 --> 00:04:42,000
so in the session we're going to talk

132
00:04:42,000 --> 00:04:43,680
about several approaches

133
00:04:43,680 --> 00:04:46,880
and we that can deliver on both of these

134
00:04:46,880 --> 00:04:50,479
principles so i want to talk a little

135
00:04:50,479 --> 00:04:51,199
bit about

136
00:04:51,199 --> 00:04:53,120
our journey and talk about the

137
00:04:53,120 --> 00:04:55,759
approaches at a high level

138
00:04:55,759 --> 00:04:58,479
so we'll start with what we call

139
00:04:58,479 --> 00:05:00,639
personal data obfuscation

140
00:05:00,639 --> 00:05:04,080
so as a data scientist what you

141
00:05:04,080 --> 00:05:05,919
tend to do when you are first

142
00:05:05,919 --> 00:05:07,280
constructing a model

143
00:05:07,280 --> 00:05:10,320
is data exploration and so if i were

144
00:05:10,320 --> 00:05:11,120
going to

145
00:05:11,120 --> 00:05:14,000
classify hey holly real quick this is

146
00:05:14,000 --> 00:05:15,199
tracy from the first team

147
00:05:15,199 --> 00:05:18,560
are you sharing slides by chance yeah

148
00:05:18,560 --> 00:05:20,960
uh i think they're not showing up for

149
00:05:20,960 --> 00:05:22,000
all of us here

150
00:05:22,000 --> 00:05:25,919
oh okay go ahead and click share screen

151
00:05:25,919 --> 00:05:28,479
let's see

152
00:05:32,000 --> 00:05:35,759
crack me the talk is great

153
00:05:36,560 --> 00:05:38,560
thanks everybody just some technical

154
00:05:38,560 --> 00:05:39,600
difficulties

155
00:05:39,600 --> 00:05:41,360
all right i couldn't see the chat uh

156
00:05:41,360 --> 00:05:43,520
there you go all right

157
00:05:43,520 --> 00:05:45,199
all right you're you're back it's not

158
00:05:45,199 --> 00:05:48,080
the cute child

159
00:05:48,080 --> 00:05:51,680
all right it's my child actually okay so

160
00:05:51,680 --> 00:05:54,160
this first example of privacy preserving

161
00:05:54,160 --> 00:05:54,880
ai

162
00:05:54,880 --> 00:05:57,440
is what we call ditty obfuscation i i

163
00:05:57,440 --> 00:05:59,039
think what i was describing is sort of

164
00:05:59,039 --> 00:05:59,840
the

165
00:05:59,840 --> 00:06:01,680
the practice that a data scientist

166
00:06:01,680 --> 00:06:03,199
typically goes through

167
00:06:03,199 --> 00:06:05,520
when they are creating a model so they

168
00:06:05,520 --> 00:06:07,520
do data exploration they look at the

169
00:06:07,520 --> 00:06:09,440
data to try to understand

170
00:06:09,440 --> 00:06:11,039
what it means they may come up with a

171
00:06:11,039 --> 00:06:12,880
labeling strategy if you're doing

172
00:06:12,880 --> 00:06:14,720
supervised learning to teach

173
00:06:14,720 --> 00:06:16,800
the learner how to predict whatever it

174
00:06:16,800 --> 00:06:17,759
is that you're trying to predict

175
00:06:17,759 --> 00:06:20,400
and so in this case i might say hey this

176
00:06:20,400 --> 00:06:21,120
is a

177
00:06:21,120 --> 00:06:22,720
child in a fuzzy suit and that's what

178
00:06:22,720 --> 00:06:24,800
i'm trying to predict everything else is

179
00:06:24,800 --> 00:06:27,199
not a child in a fuzzy suit

180
00:06:27,199 --> 00:06:28,960
if i wanted to do personal data

181
00:06:28,960 --> 00:06:30,479
obfuscation

182
00:06:30,479 --> 00:06:32,800
i would just remove the face of said

183
00:06:32,800 --> 00:06:34,080
child and

184
00:06:34,080 --> 00:06:36,479
that way the personal data which child

185
00:06:36,479 --> 00:06:38,160
it is i don't need to know that because

186
00:06:38,160 --> 00:06:39,680
i'm just trying to identify a small

187
00:06:39,680 --> 00:06:41,440
person in a fuzzy suit

188
00:06:41,440 --> 00:06:45,280
um without the personal data

189
00:06:45,280 --> 00:06:46,800
showing to me or to the model that's

190
00:06:46,800 --> 00:06:48,720
trying to learn it

191
00:06:48,720 --> 00:06:51,759
so what if we don't have to look at

192
00:06:51,759 --> 00:06:54,960
the data at all um in certain

193
00:06:54,960 --> 00:06:57,039
circumstances we can do what's called

194
00:06:57,039 --> 00:06:57,680
eyes off

195
00:06:57,680 --> 00:07:01,039
training so the human or the researcher

196
00:07:01,039 --> 00:07:03,360
doesn't lay eyes on the data that may be

197
00:07:03,360 --> 00:07:04,880
sensitive

198
00:07:04,880 --> 00:07:07,440
but the model does and so the model is

199
00:07:07,440 --> 00:07:08,479
learning that data

200
00:07:08,479 --> 00:07:10,560
and it is able to sufficiently learn on

201
00:07:10,560 --> 00:07:12,800
that data and predict the right things

202
00:07:12,800 --> 00:07:14,720
in the future

203
00:07:14,720 --> 00:07:16,720
what if we could take that one step

204
00:07:16,720 --> 00:07:19,199
further where the model doesn't even see

205
00:07:19,199 --> 00:07:19,919
the data

206
00:07:19,919 --> 00:07:22,639
and it's encrypted um this is much

207
00:07:22,639 --> 00:07:23,919
harder and in fact

208
00:07:23,919 --> 00:07:26,000
i talked about this as a journey because

209
00:07:26,000 --> 00:07:28,080
personal data obfuscation

210
00:07:28,080 --> 00:07:31,680
pretty simple approach uh not uh

211
00:07:31,680 --> 00:07:34,880
not perfect but a pretty good first step

212
00:07:34,880 --> 00:07:36,639
in the journey eyes off training a

213
00:07:36,639 --> 00:07:38,639
little more difficult homomorphic

214
00:07:38,639 --> 00:07:39,520
encryption

215
00:07:39,520 --> 00:07:42,960
much more challenging as and uh

216
00:07:42,960 --> 00:07:45,360
anna will tell you in a little bit so

217
00:07:45,360 --> 00:07:46,479
first of all i want to give you a

218
00:07:46,479 --> 00:07:48,240
specific security example for personal

219
00:07:48,240 --> 00:07:50,160
data obfuscation

220
00:07:50,160 --> 00:07:52,160
again using the the cute child without a

221
00:07:52,160 --> 00:07:53,199
face uh

222
00:07:53,199 --> 00:07:54,319
and just to make sure we're on the same

223
00:07:54,319 --> 00:07:56,639
page about personal data so personal

224
00:07:56,639 --> 00:07:58,080
data is in the information that

225
00:07:58,080 --> 00:07:59,440
identifies

226
00:07:59,440 --> 00:08:03,680
a living individual or human person

227
00:08:03,759 --> 00:08:06,400
so an example of personal data

228
00:08:06,400 --> 00:08:08,400
obfuscation using a security

229
00:08:08,400 --> 00:08:12,080
example is this malware called dexa it

230
00:08:12,080 --> 00:08:12,800
likes to

231
00:08:12,800 --> 00:08:16,479
hide files in the favorites folder

232
00:08:16,479 --> 00:08:19,280
and so knowing that information and

233
00:08:19,280 --> 00:08:20,800
looking at the files that go into the

234
00:08:20,800 --> 00:08:22,160
favorites folder can help

235
00:08:22,160 --> 00:08:25,039
a researcher or a model predict that

236
00:08:25,039 --> 00:08:27,599
these files are related to this malware

237
00:08:27,599 --> 00:08:29,280
but it's not important that you know

238
00:08:29,280 --> 00:08:31,120
that it's my favorites folder

239
00:08:31,120 --> 00:08:32,640
so if you obfuscate this piece of

240
00:08:32,640 --> 00:08:34,640
information the important security

241
00:08:34,640 --> 00:08:35,919
context is still there

242
00:08:35,919 --> 00:08:37,839
for both the researcher and the model to

243
00:08:37,839 --> 00:08:39,599
be able to interpret that information

244
00:08:39,599 --> 00:08:40,000
and say

245
00:08:40,000 --> 00:08:43,200
yeah this is deck spot

246
00:08:43,679 --> 00:08:46,320
and now i will hand it off to sharda for

247
00:08:46,320 --> 00:08:48,480
eyes off training

248
00:08:48,480 --> 00:08:51,440
thank you holly so i'm going to talk

249
00:08:51,440 --> 00:08:52,880
about eyes off training

250
00:08:52,880 --> 00:08:56,560
which refers to uh training ml models

251
00:08:56,560 --> 00:08:58,800
without looking at the data

252
00:08:58,800 --> 00:09:00,720
uh so now we are looking we were looking

253
00:09:00,720 --> 00:09:02,720
at this cute child oops advanced

254
00:09:02,720 --> 00:09:05,279
uh now we are not only hiding the face

255
00:09:05,279 --> 00:09:06,880
of the child but we are also hiding

256
00:09:06,880 --> 00:09:08,640
other details of the child such as

257
00:09:08,640 --> 00:09:09,519
costume

258
00:09:09,519 --> 00:09:11,360
and we are only looking at the shape of

259
00:09:11,360 --> 00:09:13,200
the data so here

260
00:09:13,200 --> 00:09:16,399
conceptually it refers to not looking at

261
00:09:16,399 --> 00:09:17,360
the data but

262
00:09:17,360 --> 00:09:19,600
only looking at maybe volume or some

263
00:09:19,600 --> 00:09:20,880
other metrics

264
00:09:20,880 --> 00:09:24,240
of the data so let's dive into the

265
00:09:24,240 --> 00:09:26,160
details

266
00:09:26,160 --> 00:09:28,399
uh in off so starting from the top of

267
00:09:28,399 --> 00:09:31,360
the box in office 365 we have lots of

268
00:09:31,360 --> 00:09:32,959
different applications

269
00:09:32,959 --> 00:09:35,519
these applications generate a lot of

270
00:09:35,519 --> 00:09:36,959
user telemetry data

271
00:09:36,959 --> 00:09:38,880
as well as they contain customer

272
00:09:38,880 --> 00:09:41,680
artifacts such as documents and emails

273
00:09:41,680 --> 00:09:45,200
so uh as the second box shows uh

274
00:09:45,200 --> 00:09:48,959
we use this data to build a lot of

275
00:09:48,959 --> 00:09:51,839
machine learning models as well as a

276
00:09:51,839 --> 00:09:53,519
metrics and dashboard which are

277
00:09:53,519 --> 00:09:55,200
important to maintain the health of the

278
00:09:55,200 --> 00:09:56,320
system

279
00:09:56,320 --> 00:09:59,279
uh and then going towards the third box

280
00:09:59,279 --> 00:09:59,600
uh

281
00:09:59,600 --> 00:10:01,440
the way we use these machine learning

282
00:10:01,440 --> 00:10:03,680
models is to build the intelligent

283
00:10:03,680 --> 00:10:07,440
solutions that we need at office 365 atp

284
00:10:07,440 --> 00:10:09,839
it stands for advanced threat protection

285
00:10:09,839 --> 00:10:11,760
uh today we are going to talk about

286
00:10:11,760 --> 00:10:16,640
um anti-anti-fishing scenarios

287
00:10:16,839 --> 00:10:18,000
specifically

288
00:10:18,000 --> 00:10:21,440
so uh let's look at uh what the oops the

289
00:10:21,440 --> 00:10:23,519
slides are advancing a bit faster

290
00:10:23,519 --> 00:10:26,720
uh let's look at this uh uh life cycle

291
00:10:26,720 --> 00:10:27,920
of a typical uh

292
00:10:27,920 --> 00:10:30,800
ml model classifier so here we are

293
00:10:30,800 --> 00:10:32,240
working to uh

294
00:10:32,240 --> 00:10:34,000
working on an anti-phishing email

295
00:10:34,000 --> 00:10:36,399
classifier so the data scientist starts

296
00:10:36,399 --> 00:10:37,519
with a sample

297
00:10:37,519 --> 00:10:41,279
a sample of known phishing and

298
00:10:41,279 --> 00:10:44,720
non-phishing and phishing emails

299
00:10:44,720 --> 00:10:46,399
then the data scientist looks at the

300
00:10:46,399 --> 00:10:48,480
sample to decide some features

301
00:10:48,480 --> 00:10:50,880
that will help us identify or

302
00:10:50,880 --> 00:10:52,720
differentiate between phishing versus

303
00:10:52,720 --> 00:10:54,800
non-phishing examples

304
00:10:54,800 --> 00:10:58,079
using these features we build a machine

305
00:10:58,079 --> 00:11:00,160
learning models which is nothing but an

306
00:11:00,160 --> 00:11:01,920
algorithm which decides

307
00:11:01,920 --> 00:11:05,279
how important a particular feature is in

308
00:11:05,279 --> 00:11:08,079
making the decision of uh an email being

309
00:11:08,079 --> 00:11:09,920
phishing or not

310
00:11:09,920 --> 00:11:13,120
uh and then once uh this algorithm is

311
00:11:13,120 --> 00:11:15,600
tuned with the right parameters uh we go

312
00:11:15,600 --> 00:11:16,480
ahead and ship

313
00:11:16,480 --> 00:11:20,640
that now monitoring is also a big part

314
00:11:20,640 --> 00:11:21,519
of this uh

315
00:11:21,519 --> 00:11:24,880
process uh in monitoring uh it

316
00:11:24,880 --> 00:11:28,160
we uh look at the spikes in the model

317
00:11:28,160 --> 00:11:28,800
behavior

318
00:11:28,800 --> 00:11:31,200
as well as customer escalations which is

319
00:11:31,200 --> 00:11:33,519
as we all know who work in the security

320
00:11:33,519 --> 00:11:35,440
uh wherever there is an adversary

321
00:11:35,440 --> 00:11:37,600
involved monitoring becomes an

322
00:11:37,600 --> 00:11:40,959
important part of the journey

323
00:11:40,959 --> 00:11:43,279
so now let's look at the same process of

324
00:11:43,279 --> 00:11:45,440
building the ml classifier but in an

325
00:11:45,440 --> 00:11:46,959
eyesoft environment

326
00:11:46,959 --> 00:11:50,720
uh the customer is starting with uh so

327
00:11:50,720 --> 00:11:52,480
at microsoft we are starting with the

328
00:11:52,480 --> 00:11:54,399
customer submitted emails

329
00:11:54,399 --> 00:11:56,639
which refers to the emails that

330
00:11:56,639 --> 00:11:58,959
customers submit to microsoft uh

331
00:11:58,959 --> 00:12:00,880
and give us the consent to look at those

332
00:12:00,880 --> 00:12:02,160
emails so

333
00:12:02,160 --> 00:12:04,399
here i am talking about the first uh

334
00:12:04,399 --> 00:12:05,839
column on the left

335
00:12:05,839 --> 00:12:08,720
uh we employ human graders who grade

336
00:12:08,720 --> 00:12:09,200
these

337
00:12:09,200 --> 00:12:12,959
emails and then uh we we create a corpus

338
00:12:12,959 --> 00:12:15,200
that i showed in the previous example

339
00:12:15,200 --> 00:12:17,120
so till this point so the first two

340
00:12:17,120 --> 00:12:19,279
columns uh the process is same as what i

341
00:12:19,279 --> 00:12:20,880
showed in the last slide

342
00:12:20,880 --> 00:12:22,959
when it comes to eyes on and eyes off

343
00:12:22,959 --> 00:12:24,480
environments uh

344
00:12:24,480 --> 00:12:26,720
here let's recall that in an eyesoft

345
00:12:26,720 --> 00:12:28,480
environment the data scientist can

346
00:12:28,480 --> 00:12:30,240
actually look at the data they can look

347
00:12:30,240 --> 00:12:32,079
at the email how it looks like

348
00:12:32,079 --> 00:12:33,680
what is the image in it what is the

349
00:12:33,680 --> 00:12:35,360
content how does it look when it's

350
00:12:35,360 --> 00:12:36,320
rendered

351
00:12:36,320 --> 00:12:39,839
uh after uh so uh so then

352
00:12:39,839 --> 00:12:42,160
the data scientist is using this data

353
00:12:42,160 --> 00:12:44,480
set and building a model

354
00:12:44,480 --> 00:12:46,480
uh using the similar process that we saw

355
00:12:46,480 --> 00:12:47,760
in the last slide

356
00:12:47,760 --> 00:12:50,880
uh after this model is built and ready

357
00:12:50,880 --> 00:12:51,519
on grade

358
00:12:51,519 --> 00:12:55,120
uh graded emails uh we uh move towards

359
00:12:55,120 --> 00:12:56,639
eyes of platform

360
00:12:56,639 --> 00:12:59,040
in eyes of platform uh the data centers

361
00:12:59,040 --> 00:13:01,680
is executing the exactly same code

362
00:13:01,680 --> 00:13:05,120
but uh they cannot look at the data so

363
00:13:05,120 --> 00:13:07,440
uh they are only able to look at the

364
00:13:07,440 --> 00:13:09,760
final accuracy metrics such as precision

365
00:13:09,760 --> 00:13:11,040
and recall

366
00:13:11,040 --> 00:13:13,360
uh and we need to train the model again

367
00:13:13,360 --> 00:13:15,360
on eyes of data because graded emails

368
00:13:15,360 --> 00:13:16,480
don't represent

369
00:13:16,480 --> 00:13:19,600
uh all the samples uh that we see in

370
00:13:19,600 --> 00:13:22,079
entire office 365 traffic so that's why

371
00:13:22,079 --> 00:13:23,680
we need to retrain the model

372
00:13:23,680 --> 00:13:26,800
in with more representative data

373
00:13:26,800 --> 00:13:28,480
which definitely works better in the

374
00:13:28,480 --> 00:13:32,160
real production scenario

375
00:13:32,639 --> 00:13:34,639
now let's look at some challenges with

376
00:13:34,639 --> 00:13:35,920
eyes off system

377
00:13:35,920 --> 00:13:38,800
as as we all know troubleshooting any

378
00:13:38,800 --> 00:13:40,240
production system

379
00:13:40,240 --> 00:13:42,240
uh especially in the security domain

380
00:13:42,240 --> 00:13:44,560
needs the sme to be able to reproduce

381
00:13:44,560 --> 00:13:46,079
and debug the scenario

382
00:13:46,079 --> 00:13:48,399
uh like we uh we need to look at the

383
00:13:48,399 --> 00:13:50,480
positive and negative negative samples

384
00:13:50,480 --> 00:13:52,480
during training to identify if there are

385
00:13:52,480 --> 00:13:54,480
any unintended patterns that were picked

386
00:13:54,480 --> 00:13:55,839
up by the model

387
00:13:55,839 --> 00:13:58,720
uh also inspecting a false negative

388
00:13:58,720 --> 00:14:00,720
examples many times you get uh

389
00:14:00,720 --> 00:14:02,639
escalations from customers saying

390
00:14:02,639 --> 00:14:04,639
oh this was a big phishing attack which

391
00:14:04,639 --> 00:14:06,480
was not caught by your model

392
00:14:06,480 --> 00:14:09,600
or your system what went wrong here

393
00:14:09,600 --> 00:14:12,399
doing this in an eyes of system where we

394
00:14:12,399 --> 00:14:14,000
cannot look at the data is a big

395
00:14:14,000 --> 00:14:14,880
challenge and

396
00:14:14,880 --> 00:14:17,040
we we have started on a journey to solve

397
00:14:17,040 --> 00:14:18,639
that in creative ways

398
00:14:18,639 --> 00:14:20,480
so i want to show you that with an

399
00:14:20,480 --> 00:14:21,760
example here

400
00:14:21,760 --> 00:14:25,279
uh here we have an uh email phishing

401
00:14:25,279 --> 00:14:26,480
email

402
00:14:26,480 --> 00:14:29,199
and i'm showing the html version of the

403
00:14:29,199 --> 00:14:30,160
text

404
00:14:30,160 --> 00:14:32,959
and then the the one on the right side

405
00:14:32,959 --> 00:14:35,360
is the final render text so you can see

406
00:14:35,360 --> 00:14:37,440
here the attacker played a trick on our

407
00:14:37,440 --> 00:14:38,720
tokenizer

408
00:14:38,720 --> 00:14:41,760
where the render text doesn't have any

409
00:14:41,760 --> 00:14:43,760
spaces and looks like a normal text to

410
00:14:43,760 --> 00:14:44,639
human eye

411
00:14:44,639 --> 00:14:47,120
but what our tokenizer parses is the

412
00:14:47,120 --> 00:14:49,440
html text and you can see because of the

413
00:14:49,440 --> 00:14:51,680
spaces added between characters

414
00:14:51,680 --> 00:14:54,320
it completely broke the tokenizer and

415
00:14:54,320 --> 00:14:56,480
this is where we saw our model

416
00:14:56,480 --> 00:14:58,560
performance dropping suddenly because

417
00:14:58,560 --> 00:15:00,480
the attack was bigger and we were able

418
00:15:00,480 --> 00:15:02,160
to catch that in the spikes

419
00:15:02,160 --> 00:15:05,360
that we saw so how do we uh deal with

420
00:15:05,360 --> 00:15:07,279
how do we debug these scenarios that's

421
00:15:07,279 --> 00:15:09,440
what we are working on and one of the

422
00:15:09,440 --> 00:15:11,920
important techniques that we use is

423
00:15:11,920 --> 00:15:14,639
using similarity-based techniques

424
00:15:14,639 --> 00:15:18,560
to identify uh whether this sample

425
00:15:18,560 --> 00:15:21,040
is similar to a cluster of emails that

426
00:15:21,040 --> 00:15:23,040
we have seen in the graded

427
00:15:23,040 --> 00:15:25,199
or customer submitted emails and then we

428
00:15:25,199 --> 00:15:27,199
go from there to identify

429
00:15:27,199 --> 00:15:29,519
uh what is it that may have gone wrong

430
00:15:29,519 --> 00:15:31,600
with this particular email

431
00:15:31,600 --> 00:15:35,199
uh so um with that i will let

432
00:15:35,199 --> 00:15:41,839
anna talk about homomorphic encryption

433
00:15:43,360 --> 00:15:44,959
all right i'm excited to talk to you

434
00:15:44,959 --> 00:15:46,480
about homomorphic encryption

435
00:15:46,480 --> 00:15:50,240
uh homomorphic encryption

436
00:15:50,720 --> 00:15:53,120
uh is the idea all right instead of just

437
00:15:53,120 --> 00:15:54,079
thinking about

438
00:15:54,079 --> 00:15:55,680
uh how do we obviously get cute child

439
00:15:55,680 --> 00:15:57,120
let's just encrypt the whole picture of

440
00:15:57,120 --> 00:15:58,320
the cute child

441
00:15:58,320 --> 00:16:01,199
um so if we're thinking in the security

442
00:16:01,199 --> 00:16:02,000
or domain

443
00:16:02,000 --> 00:16:05,120
um phishing urls so say you click on

444
00:16:05,120 --> 00:16:06,399
that fish email

445
00:16:06,399 --> 00:16:09,279
that uh sharda had a miss on we'd still

446
00:16:09,279 --> 00:16:10,639
like to protect you

447
00:16:10,639 --> 00:16:12,399
um and if you go to a fish website

448
00:16:12,399 --> 00:16:14,320
visual information can be really

449
00:16:14,320 --> 00:16:16,639
key signal anti-phishing ml models does

450
00:16:16,639 --> 00:16:17,600
this look

451
00:16:17,600 --> 00:16:19,519
like it's asked a microsoft website

452
00:16:19,519 --> 00:16:21,120
asking for microsoft creds but it's not

453
00:16:21,120 --> 00:16:24,079
really a microsoft website

454
00:16:24,079 --> 00:16:26,399
and so that would entail capturing a

455
00:16:26,399 --> 00:16:28,320
screenshot of the user's browser and

456
00:16:28,320 --> 00:16:29,680
sending it to the cloud for

457
00:16:29,680 --> 00:16:33,519
the computation um and we can't

458
00:16:33,519 --> 00:16:36,639
just say oh it's this url because it

459
00:16:36,639 --> 00:16:38,720
might not render the same for me

460
00:16:38,720 --> 00:16:41,920
it does users uh so

461
00:16:41,920 --> 00:16:44,880
this of course has some privacy concerns

462
00:16:44,880 --> 00:16:46,320
that it might raise and it might feel

463
00:16:46,320 --> 00:16:47,839
like we're in this classic

464
00:16:47,839 --> 00:16:50,880
security versus privacy situation

465
00:16:50,880 --> 00:16:53,279
so we started doing some research about

466
00:16:53,279 --> 00:16:54,959
how we might

467
00:16:54,959 --> 00:16:58,720
get around that classic feeling tension

468
00:16:58,720 --> 00:17:01,519
and provide some built-in privacy

469
00:17:01,519 --> 00:17:02,639
protection

470
00:17:02,639 --> 00:17:04,559
so let's think first options you might

471
00:17:04,559 --> 00:17:05,839
have one is

472
00:17:05,839 --> 00:17:08,480
just don't use this kind of information

473
00:17:08,480 --> 00:17:09,599
like training

474
00:17:09,599 --> 00:17:12,959
pages that might make user service

475
00:17:12,959 --> 00:17:16,160
and do the best another

476
00:17:16,160 --> 00:17:18,480
is don't send that information to the

477
00:17:18,480 --> 00:17:20,240
cloud and do all the computation

478
00:17:20,240 --> 00:17:23,679
on which changes updating the model here

479
00:17:23,679 --> 00:17:25,359
would be really difficult

480
00:17:25,359 --> 00:17:28,960
uh potentially um we found that

481
00:17:28,960 --> 00:17:31,520
the cloud protection is important uh a

482
00:17:31,520 --> 00:17:32,000
third

483
00:17:32,000 --> 00:17:33,760
would be to encrypt those images or

484
00:17:33,760 --> 00:17:36,240
websites don't give microsoft the keys

485
00:17:36,240 --> 00:17:39,760
and still somehow expect a computation

486
00:17:39,760 --> 00:17:43,280
uh of of this model and a result to be

487
00:17:43,280 --> 00:17:43,919
returned

488
00:17:43,919 --> 00:17:47,919
anyway uh which if you're familiar with

489
00:17:47,919 --> 00:17:49,360
classical encryption that third one

490
00:17:49,360 --> 00:17:51,360
sounds like a completely insane option

491
00:17:51,360 --> 00:17:53,360
because if i take this screenshot i

492
00:17:53,360 --> 00:17:55,840
encrypt it i compute the model func i

493
00:17:55,840 --> 00:17:56,799
guess i just write compute on this

494
00:17:56,799 --> 00:17:58,400
nonsense ciphertext

495
00:17:58,400 --> 00:18:01,919
i return the results to the client

496
00:18:01,919 --> 00:18:04,320
they decrypt total nonsense results this

497
00:18:04,320 --> 00:18:06,000
is this is a terrible plan with

498
00:18:06,000 --> 00:18:08,080
classical encryption

499
00:18:08,080 --> 00:18:11,120
and so the rescue is a special kind of

500
00:18:11,120 --> 00:18:13,039
encryption called homomorphic encryption

501
00:18:13,039 --> 00:18:16,240
and it is special because the sum of

502
00:18:16,240 --> 00:18:18,880
two ciphertexts encrypted with this e is

503
00:18:18,880 --> 00:18:21,200
the ciphertext of their sum

504
00:18:21,200 --> 00:18:24,640
and the product of two ciphertexts

505
00:18:24,640 --> 00:18:26,400
encrypted with the same key

506
00:18:26,400 --> 00:18:28,960
is the ciphertext of their product this

507
00:18:28,960 --> 00:18:30,400
allows us to apply

508
00:18:30,400 --> 00:18:32,160
some machine learning algorithms that

509
00:18:32,160 --> 00:18:33,679
are really carefully chosen

510
00:18:33,679 --> 00:18:36,559
to date about being able to decrypt this

511
00:18:36,559 --> 00:18:37,280
free

512
00:18:37,280 --> 00:18:40,480
update accordingly

513
00:18:40,840 --> 00:18:43,840
relativity

514
00:18:52,880 --> 00:18:57,840
there we go um

515
00:18:59,360 --> 00:19:03,120
is that we would

516
00:19:04,240 --> 00:19:07,039
i'm so sorry

517
00:19:07,760 --> 00:19:11,120
okay here we go uh the the scenario that

518
00:19:11,120 --> 00:19:14,159
we settled on

519
00:19:15,919 --> 00:19:18,960
sound better now

520
00:19:19,760 --> 00:19:22,799
um the scenario that we that we settled

521
00:19:22,799 --> 00:19:24,240
on here is that we would

522
00:19:24,240 --> 00:19:26,720
take the screenshot on the client uh

523
00:19:26,720 --> 00:19:28,640
extract features using some

524
00:19:28,640 --> 00:19:32,559
standard visual uh

525
00:19:32,559 --> 00:19:36,240
uh visual analysis tools encrypt those

526
00:19:36,240 --> 00:19:36,799
features

527
00:19:36,799 --> 00:19:39,200
send them to the cloud for for encrypted

528
00:19:39,200 --> 00:19:40,559
computation

529
00:19:40,559 --> 00:19:42,799
on a model in the cloud that was more

530
00:19:42,799 --> 00:19:44,400
computationally efficient

531
00:19:44,400 --> 00:19:47,120
uh send those encrypted results back to

532
00:19:47,120 --> 00:19:48,480
the client where we could decrypt and

533
00:19:48,480 --> 00:19:50,160
say hey maybe don't go to this website

534
00:19:50,160 --> 00:19:51,360
it's a bad

535
00:19:51,360 --> 00:19:55,280
website to go to um

536
00:19:55,280 --> 00:19:58,400
so we have talked

537
00:19:58,400 --> 00:20:02,000
about three ways that we

538
00:20:02,000 --> 00:20:05,039
can deal with uh privacy

539
00:20:05,039 --> 00:20:07,600
preserving ml with personal data

540
00:20:07,600 --> 00:20:10,399
obfuscation

541
00:20:10,880 --> 00:20:13,600
with izoff training and with homomorph

542
00:20:13,600 --> 00:20:14,960
encryption

543
00:20:14,960 --> 00:20:18,559
and this isn't an either or which one's

544
00:20:18,559 --> 00:20:19,120
better

545
00:20:19,120 --> 00:20:21,120
what's the right thing to do this is a

546
00:20:21,120 --> 00:20:22,480
yes and

547
00:20:22,480 --> 00:20:25,039
all of the above and more there are many

548
00:20:25,039 --> 00:20:26,000
more

549
00:20:26,000 --> 00:20:27,760
uh technical solutions to privacy that

550
00:20:27,760 --> 00:20:29,679
we haven't talked about here

551
00:20:29,679 --> 00:20:31,840
and they all have their basis for the

552
00:20:31,840 --> 00:20:33,360
right stereos

553
00:20:33,360 --> 00:20:36,720
and we started gathering all of them

554
00:20:36,720 --> 00:20:37,600
together

555
00:20:37,600 --> 00:20:39,280
they all have pluses and minuses for

556
00:20:39,280 --> 00:20:40,880
example i didn't tell you how to train a

557
00:20:40,880 --> 00:20:42,799
model using homomorphic encryption

558
00:20:42,799 --> 00:20:46,320
that's a very hard problem so

559
00:20:46,320 --> 00:20:50,720
thanks very much and questions

560
00:20:51,760 --> 00:20:53,919
okay thank you very much this was uh

561
00:20:53,919 --> 00:20:55,039
especially the part

562
00:20:55,039 --> 00:20:57,600
the last part was kind of mind-boggling

563
00:20:57,600 --> 00:20:58,320
but i

564
00:20:58,320 --> 00:20:59,760
really like it so there's a couple of

565
00:20:59,760 --> 00:21:02,480
questions actually

566
00:21:04,159 --> 00:21:07,600
asks would i off models

567
00:21:07,600 --> 00:21:10,159
model create creation provide the

568
00:21:10,159 --> 00:21:11,280
possibility

569
00:21:11,280 --> 00:21:13,600
for the adversary to hide the payload

570
00:21:13,600 --> 00:21:14,640
among

571
00:21:14,640 --> 00:21:17,280
for example gdpr protected data gdpr

572
00:21:17,280 --> 00:21:19,600
protected date

573
00:21:19,600 --> 00:21:22,480
gdpr protected data possibly cannot be

574
00:21:22,480 --> 00:21:26,000
used in all of the training models

575
00:21:26,400 --> 00:21:29,520
uh so yeah

576
00:21:29,520 --> 00:21:32,720
well gdpr product microsoft does comply

577
00:21:32,720 --> 00:21:34,480
to the gdpr standard

578
00:21:34,480 --> 00:21:38,240
uh but gdpr protected data can be used

579
00:21:38,240 --> 00:21:39,840
in the training the machine learning

580
00:21:39,840 --> 00:21:42,320
models it's just that the way the data

581
00:21:42,320 --> 00:21:44,159
is used is different

582
00:21:44,159 --> 00:21:46,880
we have to de-link the information like

583
00:21:46,880 --> 00:21:49,039
if it is an email address of the user

584
00:21:49,039 --> 00:21:51,840
you have to encrypt the email address

585
00:21:51,840 --> 00:21:53,840
such that it cannot be tracked back to

586
00:21:53,840 --> 00:21:55,360
what was the original email

587
00:21:55,360 --> 00:21:58,320
so as long as as long as we take those

588
00:21:58,320 --> 00:21:59,280
measures we

589
00:21:59,280 --> 00:22:02,480
we can use uh gdpr protected data to

590
00:22:02,480 --> 00:22:04,320
train the models

591
00:22:04,320 --> 00:22:06,480
and i'll add on to shadow what you said

592
00:22:06,480 --> 00:22:07,840
and we've also

593
00:22:07,840 --> 00:22:11,280
done a lot of research in adversarial ml

594
00:22:11,280 --> 00:22:13,760
we're not talking about that today but

595
00:22:13,760 --> 00:22:16,159
there are a lot of techniques that we

596
00:22:16,159 --> 00:22:16,799
embed

597
00:22:16,799 --> 00:22:20,080
into the way that the models train

598
00:22:20,080 --> 00:22:23,360
to look for anomalies like that and look

599
00:22:23,360 --> 00:22:26,000
for potential examples of people trying

600
00:22:26,000 --> 00:22:27,679
to do data poisoning

601
00:22:27,679 --> 00:22:30,880
and so those algorithms as well

602
00:22:30,880 --> 00:22:34,000
help clean the data to make sure that

603
00:22:34,000 --> 00:22:36,159
the data that is training the model is

604
00:22:36,159 --> 00:22:39,280
also trustworthy

605
00:22:39,360 --> 00:22:41,760
and we can make references to those

606
00:22:41,760 --> 00:22:44,159
talks if you're interested

607
00:22:44,159 --> 00:22:47,760
thanks then silva martinez asks does

608
00:22:47,760 --> 00:22:49,039
your homomorphic

609
00:22:49,039 --> 00:22:51,280
encryption solution only work for

610
00:22:51,280 --> 00:22:53,200
comparing exact sets of data

611
00:22:53,200 --> 00:22:55,760
or can can it work with similar sets of

612
00:22:55,760 --> 00:22:57,200
data

613
00:22:57,200 --> 00:23:01,360
i'm not sure i understand that question

614
00:23:01,360 --> 00:23:09,840
can you uh silvan could you elaborate

615
00:23:12,080 --> 00:23:13,600
okay the question i just asked is

616
00:23:13,600 --> 00:23:15,440
basically to try understanding if

617
00:23:15,440 --> 00:23:17,679
there is are limitations on using

618
00:23:17,679 --> 00:23:19,760
homomorphic encryption

619
00:23:19,760 --> 00:23:21,600
there are definitely tons of limitations

620
00:23:21,600 --> 00:23:23,280
in using homomorphic encryption it's

621
00:23:23,280 --> 00:23:23,840
super

622
00:23:23,840 --> 00:23:27,120
um time and space expensive um

623
00:23:27,120 --> 00:23:29,039
and you'll notice that in the example i

624
00:23:29,039 --> 00:23:30,400
gave we did a bunch of

625
00:23:30,400 --> 00:23:33,360
feature extraction on the client

626
00:23:33,360 --> 00:23:34,480
pre-encryption

627
00:23:34,480 --> 00:23:37,039
and that's because we couldn't afford to

628
00:23:37,039 --> 00:23:37,840
do

629
00:23:37,840 --> 00:23:40,799
all of that deep learning based feature

630
00:23:40,799 --> 00:23:42,240
extraction

631
00:23:42,240 --> 00:23:45,919
encrypted in the cloud um and

632
00:23:45,919 --> 00:23:49,039
so we did that on the client instead

633
00:23:49,039 --> 00:23:52,080
pre-encryption um and so we we there is

634
00:23:52,080 --> 00:23:54,080
a huge trade-off here we did have to be

635
00:23:54,080 --> 00:23:55,279
really careful

636
00:23:55,279 --> 00:23:58,880
um and and and that's sort of why it's

637
00:23:58,880 --> 00:24:00,320
organized the way it is and if you want

638
00:24:00,320 --> 00:24:01,919
more details of the exact

639
00:24:01,919 --> 00:24:03,919
details of how we did it in this example

640
00:24:03,919 --> 00:24:05,679
uh one of the links there is to a paper

641
00:24:05,679 --> 00:24:07,520
that explains that focuses solely on

642
00:24:07,520 --> 00:24:08,080
that

643
00:24:08,080 --> 00:24:11,279
so you can go read 15 pages of technical

644
00:24:11,279 --> 00:24:13,679
details

645
00:24:14,320 --> 00:24:18,080
um and the other question about what

646
00:24:18,080 --> 00:24:19,840
homomorphic encryption algorithm are we

647
00:24:19,840 --> 00:24:20,480
looking into

648
00:24:20,480 --> 00:24:22,480
the answer is this is based on the

649
00:24:22,480 --> 00:24:24,240
simple encrypted arithmetic

650
00:24:24,240 --> 00:24:26,720
library maintained at microsoft research

651
00:24:26,720 --> 00:24:28,159
uh which is open source you can go have

652
00:24:28,159 --> 00:24:32,320
a look at it too

653
00:24:32,320 --> 00:24:35,039
so um

654
00:24:35,840 --> 00:24:37,919
do you now just ask one of the questions

655
00:24:37,919 --> 00:24:39,919
in the qa because i lost track

656
00:24:39,919 --> 00:24:42,400
and yes i did i'm sorry no worries no

657
00:24:42,400 --> 00:24:44,240
really you did so well with the last one

658
00:24:44,240 --> 00:24:44,480
i

659
00:24:44,480 --> 00:24:47,039
i i just answered it i'm sorry that

660
00:24:47,039 --> 00:24:48,400
that's what i thought

661
00:24:48,400 --> 00:24:52,000
it's a no and then uh

662
00:24:52,000 --> 00:24:54,000
i guess another fairly technical thing

663
00:24:54,000 --> 00:24:55,600
is about how does the al

664
00:24:55,600 --> 00:24:57,840
jabra extraction necessarily homomorphic

665
00:24:57,840 --> 00:24:59,840
encryption impact the feature selection

666
00:24:59,840 --> 00:25:01,679
what ml algorithms

667
00:25:01,679 --> 00:25:03,360
are compatible with homomorphic

668
00:25:03,360 --> 00:25:05,120
encryptions

669
00:25:05,120 --> 00:25:08,320
so um so

670
00:25:08,320 --> 00:25:11,200
there's two ways you could think of ml

671
00:25:11,200 --> 00:25:12,480
algorithms

672
00:25:12,480 --> 00:25:14,480
um you could think of the both the

673
00:25:14,480 --> 00:25:15,840
scoring part and the

674
00:25:15,840 --> 00:25:18,640
training part um in this case we didn't

675
00:25:18,640 --> 00:25:19,039
do

676
00:25:19,039 --> 00:25:22,000
encrypted training we only did encrypted

677
00:25:22,000 --> 00:25:24,080
scoring so we've somehow gotten this

678
00:25:24,080 --> 00:25:26,400
ml algorithm by hook or by crook some

679
00:25:26,400 --> 00:25:27,440
other way

680
00:25:27,440 --> 00:25:31,279
um and then for your particular

681
00:25:31,279 --> 00:25:33,200
website we're saying is this fish or not

682
00:25:33,200 --> 00:25:34,880
in an encrypted fashion

683
00:25:34,880 --> 00:25:38,000
um and in this case

684
00:25:38,000 --> 00:25:40,640
we we made it so that the encrypted

685
00:25:40,640 --> 00:25:42,240
operations we needed to do were

686
00:25:42,240 --> 00:25:46,640
only uh multiplication and addition

687
00:25:46,640 --> 00:25:48,640
uh and that was partly we're sort of

688
00:25:48,640 --> 00:25:50,960
balancing how much can we do encrypted

689
00:25:50,960 --> 00:25:53,760
with the speed questions and and that's

690
00:25:53,760 --> 00:25:57,600
where we landed

691
00:25:57,600 --> 00:25:59,679
okay there's no more questions in the in

692
00:25:59,679 --> 00:26:01,039
the chat but i actually have

693
00:26:01,039 --> 00:26:03,679
have a question um so have you applied

694
00:26:03,679 --> 00:26:05,840
this to other use cases but then

695
00:26:05,840 --> 00:26:07,840
phishing pages for example like one

696
00:26:07,840 --> 00:26:09,520
thing that comes to mind is kind of a

697
00:26:09,520 --> 00:26:12,240
recognizing spammers uh another thing is

698
00:26:12,240 --> 00:26:13,919
recognizing actually

699
00:26:13,919 --> 00:26:16,480
content on the cloud storage that's

700
00:26:16,480 --> 00:26:19,520
somehow not okay

701
00:26:19,520 --> 00:26:22,080
homomorphic encryption particularly no

702
00:26:22,080 --> 00:26:22,559
just

703
00:26:22,559 --> 00:26:25,600
any of the the data preserving i mean

704
00:26:25,600 --> 00:26:27,440
for the privacy preserving algorithms

705
00:26:27,440 --> 00:26:28,799
even like that the personal data

706
00:26:28,799 --> 00:26:31,200
obfuscation the eyes off stuff

707
00:26:31,200 --> 00:26:34,240
all of them yeah yeah i can i can take

708
00:26:34,240 --> 00:26:34,799
that one

709
00:26:34,799 --> 00:26:37,679
so personal data obfuscation is used

710
00:26:37,679 --> 00:26:38,880
throughout

711
00:26:38,880 --> 00:26:40,880
pretty much many of the different

712
00:26:40,880 --> 00:26:43,279
services that we have in security

713
00:26:43,279 --> 00:26:46,960
uh isof training is more specific to

714
00:26:46,960 --> 00:26:50,000
email mostly i don't know harder if you

715
00:26:50,000 --> 00:26:51,440
know of other

716
00:26:51,440 --> 00:26:55,360
applications uh yeah sure i can talk

717
00:26:55,360 --> 00:26:56,240
about that so

718
00:26:56,240 --> 00:26:58,720
um the eyes of training platform right

719
00:26:58,720 --> 00:27:01,360
now is uh specific to office 365

720
00:27:01,360 --> 00:27:05,039
and we do use it for spam detection so

721
00:27:05,039 --> 00:27:08,159
uh all kinds of uh security scenarios

722
00:27:08,159 --> 00:27:10,159
identifying compromise accounts

723
00:27:10,159 --> 00:27:12,480
identifying spam emails

724
00:27:12,480 --> 00:27:14,960
uh not so much about the cloud app

725
00:27:14,960 --> 00:27:16,159
protection yet

726
00:27:16,159 --> 00:27:19,440
that's kind of a separate but um yes

727
00:27:19,440 --> 00:27:21,520
spam phishing emails anything that

728
00:27:21,520 --> 00:27:22,480
requires us

729
00:27:22,480 --> 00:27:25,440
to process emails is uh done in eyes of

730
00:27:25,440 --> 00:27:27,679
platform

731
00:27:27,679 --> 00:27:31,279
okay thanks a lot and uh if there's

732
00:27:31,279 --> 00:27:34,399
not any more questions um thanks again

733
00:27:34,399 --> 00:27:36,640
to you i thought this is a a really

734
00:27:36,640 --> 00:27:37,840
exciting

735
00:27:37,840 --> 00:27:40,159
a really exciting topic i'm actually

736
00:27:40,159 --> 00:27:42,720
glad we moving to that that

737
00:27:42,720 --> 00:27:44,640
so and i like that talk very much hey

738
00:27:44,640 --> 00:27:45,919
thanks a lot and

739
00:27:45,919 --> 00:27:48,720
uh hopefully we meet each other in

740
00:27:48,720 --> 00:27:50,799
person at some stage and can take this

741
00:27:50,799 --> 00:27:53,919
conversation on our drink or a piece of

742
00:27:53,919 --> 00:27:56,480
cake or whatever

743
00:27:56,480 --> 00:27:58,799
sounds great thanks for having us yes

744
00:27:58,799 --> 00:27:59,679
thank you

745
00:27:59,679 --> 00:28:03,679
the app is actually a link to a gather

746
00:28:03,679 --> 00:28:05,600
town social event that we are trying

747
00:28:05,600 --> 00:28:07,200
online so

748
00:28:07,200 --> 00:28:09,919
everyone is welcome we're gonna try it

749
00:28:09,919 --> 00:28:10,880
thanks

750
00:28:10,880 --> 00:28:15,840
awesome thanks bye bye

