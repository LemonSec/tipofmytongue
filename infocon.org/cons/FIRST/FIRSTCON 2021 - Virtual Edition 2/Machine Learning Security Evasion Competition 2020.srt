1
00:00:00,640 --> 00:00:03,760
hello everyone my name is zoltan balaj

2
00:00:03,760 --> 00:00:05,520
and this is the machine learning

3
00:00:05,520 --> 00:00:07,279
security vision competition

4
00:00:07,279 --> 00:00:10,880
2020. i'm the head of

5
00:00:10,880 --> 00:00:13,920
vulnerability research lab at cujo ai

6
00:00:13,920 --> 00:00:16,239
and i'm known for my zombie browser

7
00:00:16,239 --> 00:00:17,199
toolkit

8
00:00:17,199 --> 00:00:20,000
the hardware firewall bypass tool mother

9
00:00:20,000 --> 00:00:22,160
analyzes sandbox tester tool

10
00:00:22,160 --> 00:00:24,800
i played with some crappy iot devices

11
00:00:24,800 --> 00:00:26,880
for example my exploit code ended up

12
00:00:26,880 --> 00:00:28,480
running on 600 000

13
00:00:28,480 --> 00:00:31,279
ip cameras and i invented the idea of

14
00:00:31,279 --> 00:00:33,440
encrypted exploit delivery

15
00:00:33,440 --> 00:00:35,200
i'm the co-organizer of the hacker

16
00:00:35,200 --> 00:00:37,520
surely meet up program committee member

17
00:00:37,520 --> 00:00:40,000
of the activity conference and volunteer

18
00:00:40,000 --> 00:00:43,280
at iot village

19
00:00:43,280 --> 00:00:45,280
and this is a joint project together

20
00:00:45,280 --> 00:00:46,719
with dr hyrum

21
00:00:46,719 --> 00:00:49,039
who is an architect at azure transfer

22
00:00:49,039 --> 00:00:50,320
team machine learning

23
00:00:50,320 --> 00:00:53,840
at microsoft and he's also co-founder

24
00:00:53,840 --> 00:00:54,800
and coacher

25
00:00:54,800 --> 00:00:57,360
at candice and his background is from

26
00:00:57,360 --> 00:00:59,760
signal processing and machine learning

27
00:00:59,760 --> 00:01:02,399
and information security was just ended

28
00:01:02,399 --> 00:01:03,600
later

29
00:01:03,600 --> 00:01:05,920
one of his relevant researches is the

30
00:01:05,920 --> 00:01:07,920
reinforcement learning based

31
00:01:07,920 --> 00:01:10,640
antivirus invasion and he's also the

32
00:01:10,640 --> 00:01:13,200
co-creator of the ambassador 2017 and

33
00:01:13,200 --> 00:01:16,880
2018 data sets

34
00:01:17,360 --> 00:01:20,720
machine learning is used for example in

35
00:01:20,720 --> 00:01:23,600
image classification for example it can

36
00:01:23,600 --> 00:01:24,000
be

37
00:01:24,000 --> 00:01:27,280
a very common task for machine learning

38
00:01:27,280 --> 00:01:28,159
models

39
00:01:28,159 --> 00:01:31,439
to classify images whether they can see

40
00:01:31,439 --> 00:01:34,720
a school bus for example on an image

41
00:01:34,720 --> 00:01:37,280
and there was there was an interesting

42
00:01:37,280 --> 00:01:37,920
research

43
00:01:37,920 --> 00:01:39,920
in the past where researchers had to

44
00:01:39,920 --> 00:01:41,360
modify an

45
00:01:41,360 --> 00:01:44,320
existing image in a way that they add

46
00:01:44,320 --> 00:01:45,759
some data to it

47
00:01:45,759 --> 00:01:49,520
and the image for any human observer

48
00:01:49,520 --> 00:01:51,759
will remain the same as you can see on

49
00:01:51,759 --> 00:01:53,280
the right side

50
00:01:53,280 --> 00:01:56,880
but if you basically just add the

51
00:01:56,880 --> 00:01:59,920
overlay which you can see in the middle

52
00:01:59,920 --> 00:02:03,600
to the image on the left some machine

53
00:02:03,600 --> 00:02:04,960
learning classifiers

54
00:02:04,960 --> 00:02:07,680
will detect that what they can see on

55
00:02:07,680 --> 00:02:08,399
the right

56
00:02:08,399 --> 00:02:12,319
is an ostrich another school bus

57
00:02:12,879 --> 00:02:15,440
machine learning is also used in malware

58
00:02:15,440 --> 00:02:16,400
detection

59
00:02:16,400 --> 00:02:19,680
and there was an amazing

60
00:02:19,680 --> 00:02:22,959
machine learning mother detection bypass

61
00:02:22,959 --> 00:02:25,840
two years ago you just basically had to

62
00:02:25,840 --> 00:02:26,480
extract

63
00:02:26,480 --> 00:02:29,599
the strings from a known game and

64
00:02:29,599 --> 00:02:32,800
just append this to the end of no marvel

65
00:02:32,800 --> 00:02:34,879
and this is how you were able to bypass

66
00:02:34,879 --> 00:02:36,000
some machine learning

67
00:02:36,000 --> 00:02:39,440
models and five years ago

68
00:02:39,440 --> 00:02:42,160
i was already playing with machine

69
00:02:42,160 --> 00:02:43,120
learning based

70
00:02:43,120 --> 00:02:46,239
malware detection bypass and five years

71
00:02:46,239 --> 00:02:48,560
ago you were even able to do this

72
00:02:48,560 --> 00:02:52,239
with upx and if you are interested

73
00:02:52,239 --> 00:02:56,160
in the details of this game string

74
00:02:56,160 --> 00:02:59,040
attack you can find the details on this

75
00:02:59,040 --> 00:03:01,360
link

76
00:03:01,360 --> 00:03:04,080
two years ago we started this

77
00:03:04,080 --> 00:03:05,040
competition

78
00:03:05,040 --> 00:03:07,440
and the purpose was to advance the field

79
00:03:07,440 --> 00:03:09,680
of offensive and defensive machine

80
00:03:09,680 --> 00:03:12,239
learning based malware detection

81
00:03:12,239 --> 00:03:15,200
step one was to download 50 working

82
00:03:15,200 --> 00:03:16,400
marver samples

83
00:03:16,400 --> 00:03:19,360
for all the contestants and they also

84
00:03:19,360 --> 00:03:20,319
had to download

85
00:03:20,319 --> 00:03:22,560
the free machine learning models with

86
00:03:22,560 --> 00:03:23,440
their weights

87
00:03:23,440 --> 00:03:26,560
so this was basically a white box attack

88
00:03:26,560 --> 00:03:28,319
then they had to modify them over

89
00:03:28,319 --> 00:03:30,000
samples to evade detection

90
00:03:30,000 --> 00:03:34,239
by all models and if they were

91
00:03:34,239 --> 00:03:37,200
lucky then they could win this nvidia

92
00:03:37,200 --> 00:03:38,879
python rtx gpu

93
00:03:38,879 --> 00:03:40,720
and the whole competition was announced

94
00:03:40,720 --> 00:03:45,120
at the defcon 17 ai village

95
00:03:45,120 --> 00:03:48,080
also two years ago um the following

96
00:03:48,080 --> 00:03:50,000
machine learning models were used

97
00:03:50,000 --> 00:03:53,519
the amber light gbm model the markov

98
00:03:53,519 --> 00:03:57,920
model and the non-neck markov model

99
00:03:57,920 --> 00:04:00,319
i was really good with machine learning

100
00:04:00,319 --> 00:04:01,120
but

101
00:04:01,120 --> 00:04:04,239
this is a basic description of how the

102
00:04:04,239 --> 00:04:06,640
model works so if you are really

103
00:04:06,640 --> 00:04:08,239
interested into the details

104
00:04:08,239 --> 00:04:12,000
then you can look into these

105
00:04:12,000 --> 00:04:13,840
but what were the outcomes of this

106
00:04:13,840 --> 00:04:15,439
competition

107
00:04:15,439 --> 00:04:18,560
at all 70 people registered

108
00:04:18,560 --> 00:04:21,519
and 11 contestants were able to bypass

109
00:04:21,519 --> 00:04:22,160
at least

110
00:04:22,160 --> 00:04:24,560
one machine learning model and the

111
00:04:24,560 --> 00:04:25,360
winner was

112
00:04:25,360 --> 00:04:28,080
william and here you can see his

113
00:04:28,080 --> 00:04:30,160
write-up

114
00:04:30,160 --> 00:04:32,880
you can also find other interesting

115
00:04:32,880 --> 00:04:34,160
write-ups for example

116
00:04:34,160 --> 00:04:43,360
one from jakub and one from fabrizio

117
00:04:43,360 --> 00:04:45,759
but what kind of approaches were used to

118
00:04:45,759 --> 00:04:47,520
win this competition

119
00:04:47,520 --> 00:04:49,440
some people tried to pack the samples

120
00:04:49,440 --> 00:04:50,560
with a packer

121
00:04:50,560 --> 00:04:53,120
and this is nice but unfortunately if a

122
00:04:53,120 --> 00:04:55,199
malware sample is already packed

123
00:04:55,199 --> 00:04:57,680
chances are that it will not work and

124
00:04:57,680 --> 00:04:59,759
this is partially competition specific

125
00:04:59,759 --> 00:05:01,759
issue as you were dealing with uh

126
00:05:01,759 --> 00:05:04,400
malware binaries and if you create your

127
00:05:04,400 --> 00:05:06,240
own samples then you don't have to

128
00:05:06,240 --> 00:05:09,199
repack anything and this for on this

129
00:05:09,199 --> 00:05:10,080
following slide

130
00:05:10,080 --> 00:05:12,880
you can find all the different packers

131
00:05:12,880 --> 00:05:13,840
uh which are

132
00:05:13,840 --> 00:05:16,560
out on which are on the internet

133
00:05:16,560 --> 00:05:18,639
commercial ones free ones

134
00:05:18,639 --> 00:05:22,400
some used in other samples

135
00:05:22,400 --> 00:05:24,639
another interesting approach is to add

136
00:05:24,639 --> 00:05:26,000
new sections to the

137
00:05:26,000 --> 00:05:29,520
executable and you can use the left

138
00:05:29,520 --> 00:05:32,639
project for example to do that and it's

139
00:05:32,639 --> 00:05:33,600
even better

140
00:05:33,600 --> 00:05:36,160
if these sections are from known benign

141
00:05:36,160 --> 00:05:38,080
files for example resources from

142
00:05:38,080 --> 00:05:39,759
microsoft files

143
00:05:39,759 --> 00:05:41,840
this works most of the time but it can

144
00:05:41,840 --> 00:05:43,680
break more

145
00:05:43,680 --> 00:05:45,600
and some more speckers they have

146
00:05:45,600 --> 00:05:48,240
subjects and adding new sections

147
00:05:48,240 --> 00:05:51,280
can break this and

148
00:05:51,280 --> 00:05:53,680
also what's funny that just by adding a

149
00:05:53,680 --> 00:05:55,440
new section to a sample

150
00:05:55,440 --> 00:05:58,319
you might even bypass some antivirus

151
00:05:58,319 --> 00:05:59,520
detections

152
00:05:59,520 --> 00:06:01,919
i believe this is mostly for performance

153
00:06:01,919 --> 00:06:02,720
reasons

154
00:06:02,720 --> 00:06:06,319
for example some avs might use shortcuts

155
00:06:06,319 --> 00:06:08,800
for signature checks and they only do

156
00:06:08,800 --> 00:06:10,479
some of the checks if the number of

157
00:06:10,479 --> 00:06:12,319
sections of a sample

158
00:06:12,319 --> 00:06:15,759
is exactly a number

159
00:06:15,919 --> 00:06:19,120
another approach was used to append

160
00:06:19,120 --> 00:06:21,680
extra data to the end of the executable

161
00:06:21,680 --> 00:06:22,560
this is called

162
00:06:22,560 --> 00:06:25,199
an overlay technique and actually this

163
00:06:25,199 --> 00:06:26,800
was the winner strategy

164
00:06:26,800 --> 00:06:30,400
it's lump plain and simple but it works

165
00:06:30,400 --> 00:06:34,160
and this is basically a while box attack

166
00:06:34,160 --> 00:06:38,080
and if you have all the resources to

167
00:06:38,080 --> 00:06:39,919
brute force for example machine learning

168
00:06:39,919 --> 00:06:40,800
models

169
00:06:40,800 --> 00:06:44,000
this can be a very easy solution

170
00:06:44,000 --> 00:06:47,440
which works this overlay technique by

171
00:06:47,440 --> 00:06:49,120
default will not bypass

172
00:06:49,120 --> 00:06:52,400
static signature antivirus checks

173
00:06:52,400 --> 00:06:54,479
unless uh there's a rule in the

174
00:06:54,479 --> 00:06:56,080
antivirus that

175
00:06:56,080 --> 00:06:58,400
some of the checks only trigger if the

176
00:06:58,400 --> 00:06:59,840
file size is less

177
00:06:59,840 --> 00:07:04,560
than for example some megabytes

178
00:07:04,720 --> 00:07:07,120
and basically what happens here is just

179
00:07:07,120 --> 00:07:07,840
you create

180
00:07:07,840 --> 00:07:09,840
the overlay append this to the end of

181
00:07:09,840 --> 00:07:11,199
the file and

182
00:07:11,199 --> 00:07:14,880
you can view here you can find

183
00:07:14,880 --> 00:07:17,919
a visual representation of a maverick

184
00:07:17,919 --> 00:07:18,960
image

185
00:07:18,960 --> 00:07:21,440
and here you can find the same malware

186
00:07:21,440 --> 00:07:22,000
image

187
00:07:22,000 --> 00:07:24,880
but just i added some repeating patterns

188
00:07:24,880 --> 00:07:26,160
to the end of it as an

189
00:07:26,160 --> 00:07:28,960
overlay so here we can basically easily

190
00:07:28,960 --> 00:07:30,880
visually compared that

191
00:07:30,880 --> 00:07:34,479
the first sample was used in the second

192
00:07:34,479 --> 00:07:36,719
one

193
00:07:36,960 --> 00:07:40,400
key decoys we had from this competition

194
00:07:40,400 --> 00:07:43,199
the more confirmed non-magmarcom models

195
00:07:43,199 --> 00:07:44,639
were too academic

196
00:07:44,639 --> 00:07:48,000
not very effective in practice life

197
00:07:48,000 --> 00:07:48,479
project

198
00:07:48,479 --> 00:07:50,800
is awesome and i recommend to use it

199
00:07:50,800 --> 00:07:52,879
whenever you are dealing with more

200
00:07:52,879 --> 00:07:54,479
binaries

201
00:07:54,479 --> 00:07:58,879
and very tricky for example some samples

202
00:07:58,879 --> 00:08:01,280
do not reproduce the same indicators of

203
00:08:01,280 --> 00:08:02,000
compromise

204
00:08:02,000 --> 00:08:04,160
over time this can be because for

205
00:08:04,160 --> 00:08:05,840
example server is down

206
00:08:05,840 --> 00:08:08,160
there are some time checks in it or some

207
00:08:08,160 --> 00:08:09,039
things are just

208
00:08:09,039 --> 00:08:12,800
random also dealing with packed or

209
00:08:12,800 --> 00:08:14,000
protected samples

210
00:08:14,000 --> 00:08:17,680
are hard i also check some of the

211
00:08:17,680 --> 00:08:19,199
samples with ssd

212
00:08:19,199 --> 00:08:22,639
ssd is basically a fuzzy hash and

213
00:08:22,639 --> 00:08:25,199
the way it works is that there are some

214
00:08:25,199 --> 00:08:26,240
there are two

215
00:08:26,240 --> 00:08:29,440
samples uh from the same marvel family

216
00:08:29,440 --> 00:08:32,240
then by generating the fuzzy hashes you

217
00:08:32,240 --> 00:08:33,760
can easily calculate

218
00:08:33,760 --> 00:08:36,640
that uh these two marvel samples are

219
00:08:36,640 --> 00:08:38,320
close together

220
00:08:38,320 --> 00:08:40,159
and what was interesting that as some

221
00:08:40,159 --> 00:08:41,440
people were adding

222
00:08:41,440 --> 00:08:44,399
repeating patterns either as sections in

223
00:08:44,399 --> 00:08:45,680
the middle of the file

224
00:08:45,680 --> 00:08:48,000
or as an overlay to the end this

225
00:08:48,000 --> 00:08:48,720
resulted

226
00:08:48,720 --> 00:08:51,920
repeating patterns in the ssd patch

227
00:08:51,920 --> 00:08:53,680
and that's really interesting because

228
00:08:53,680 --> 00:08:55,440
you cannot see this in

229
00:08:55,440 --> 00:08:58,560
benign file or mother samples and i'm

230
00:08:58,560 --> 00:09:00,880
pretty sure this can be only used for

231
00:09:00,880 --> 00:09:04,720
amal model bypass

232
00:09:05,279 --> 00:09:08,480
next year in 2020 we have

233
00:09:08,480 --> 00:09:11,920
uh renamed this competition to msac.io

234
00:09:11,920 --> 00:09:14,800
we added a new defender challenge where

235
00:09:14,800 --> 00:09:16,399
you had to create your own machine

236
00:09:16,399 --> 00:09:18,160
learning model and submit it to the

237
00:09:18,160 --> 00:09:19,519
competition

238
00:09:19,519 --> 00:09:22,480
in a docker based format the attacker

239
00:09:22,480 --> 00:09:24,560
challenge was also changed because now

240
00:09:24,560 --> 00:09:26,399
it was not a white box attack

241
00:09:26,399 --> 00:09:28,480
against the machine learning models but

242
00:09:28,480 --> 00:09:29,760
a black box attack

243
00:09:29,760 --> 00:09:32,399
against the submitted defenses the

244
00:09:32,399 --> 00:09:34,160
source code was only provided for the

245
00:09:34,160 --> 00:09:36,640
ember model

246
00:09:36,640 --> 00:09:39,279
last year and this year the competit the

247
00:09:39,279 --> 00:09:41,440
sponsors and partners were microsoft

248
00:09:41,440 --> 00:09:42,320
kujo ai

249
00:09:42,320 --> 00:09:45,760
vmray and mrgfitas and

250
00:09:45,760 --> 00:09:48,160
you were able to win azure credits for

251
00:09:48,160 --> 00:09:50,000
your machine learning research

252
00:09:50,000 --> 00:09:53,680
plans in the defensive track

253
00:09:53,680 --> 00:09:56,000
two submissions passed the minimum

254
00:09:56,000 --> 00:09:57,440
requirements

255
00:09:57,440 --> 00:10:01,200
this means these submissions

256
00:10:01,200 --> 00:10:03,920
were able to detect moreover samples

257
00:10:03,920 --> 00:10:05,120
with

258
00:10:05,120 --> 00:10:09,600
a minimum amount of false positives

259
00:10:10,640 --> 00:10:13,680
these were the other families used last

260
00:10:13,680 --> 00:10:15,040
year competition

261
00:10:15,040 --> 00:10:17,360
these were just basically representative

262
00:10:17,360 --> 00:10:18,079
samples

263
00:10:18,079 --> 00:10:22,000
from that year if you want

264
00:10:22,000 --> 00:10:25,760
to participate in this

265
00:10:25,760 --> 00:10:28,480
year competition which i will also talk

266
00:10:28,480 --> 00:10:29,279
later

267
00:10:29,279 --> 00:10:30,959
then basically what you have to do is to

268
00:10:30,959 --> 00:10:34,320
register at the msc.io website

269
00:10:34,320 --> 00:10:37,120
review the terms of services download 50

270
00:10:37,120 --> 00:10:39,200
malware samples

271
00:10:39,200 --> 00:10:42,320
do your magic verify that the mother is

272
00:10:42,320 --> 00:10:44,160
still functional

273
00:10:44,160 --> 00:10:46,720
you can also use our api to submit the

274
00:10:46,720 --> 00:10:47,600
samples

275
00:10:47,600 --> 00:10:50,880
and upload your zip files and

276
00:10:50,880 --> 00:10:52,640
last year because there were free

277
00:10:52,640 --> 00:10:54,079
machine learning models

278
00:10:54,079 --> 00:10:56,320
you could get up to three points for

279
00:10:56,320 --> 00:10:58,800
each samples

280
00:10:58,800 --> 00:11:01,440
and also to win your solution had to be

281
00:11:01,440 --> 00:11:02,560
published

282
00:11:02,560 --> 00:11:05,920
some tips and tricks you can use to win

283
00:11:05,920 --> 00:11:06,800
the competition

284
00:11:06,800 --> 00:11:09,519
is to add or remove signature change

285
00:11:09,519 --> 00:11:11,519
section names or properties

286
00:11:11,519 --> 00:11:14,720
modify imports or export create tls

287
00:11:14,720 --> 00:11:15,519
callback

288
00:11:15,519 --> 00:11:17,760
change pa header fix or change the

289
00:11:17,760 --> 00:11:18,800
checksum

290
00:11:18,800 --> 00:11:22,560
add modify or remove version information

291
00:11:22,560 --> 00:11:24,640
you can add some new entry point that

292
00:11:24,640 --> 00:11:26,399
redirects the code flowers

293
00:11:26,399 --> 00:11:28,720
change some code or data which doesn't

294
00:11:28,720 --> 00:11:31,920
break the functionality

295
00:11:31,920 --> 00:11:34,640
droppers and suffocating air heights are

296
00:11:34,640 --> 00:11:35,040
not

297
00:11:35,040 --> 00:11:38,160
elves to use and also multiple

298
00:11:38,160 --> 00:11:40,560
registrations is against the rules

299
00:11:40,560 --> 00:11:42,640
we also have a slack channel that you

300
00:11:42,640 --> 00:11:43,600
can still

301
00:11:43,600 --> 00:11:46,000
join and have some interesting

302
00:11:46,000 --> 00:11:48,640
conversation regarding offensive machine

303
00:11:48,640 --> 00:11:51,040
learning

304
00:11:51,839 --> 00:11:54,959
now this is how our website looked like

305
00:11:54,959 --> 00:11:57,920
and we were using python and flask admin

306
00:11:57,920 --> 00:11:59,519
for the user interface

307
00:11:59,519 --> 00:12:01,839
cloudflare and genix and genuicom were

308
00:12:01,839 --> 00:12:04,160
used for scalability and performance

309
00:12:04,160 --> 00:12:05,120
reasons

310
00:12:05,120 --> 00:12:07,279
we also had some python back-end scripts

311
00:12:07,279 --> 00:12:08,959
scheduled by chrome

312
00:12:08,959 --> 00:12:12,079
and vmray sandbox was used

313
00:12:12,079 --> 00:12:15,040
in the background to get and extract all

314
00:12:15,040 --> 00:12:17,120
the indicators of compromise

315
00:12:17,120 --> 00:12:20,160
of all the submitted samples

316
00:12:20,160 --> 00:12:22,959
we also have an api so for example if

317
00:12:22,959 --> 00:12:24,000
you only want to

318
00:12:24,000 --> 00:12:27,040
submit your model and analyze

319
00:12:27,040 --> 00:12:30,639
with all the ml models then you can use

320
00:12:30,639 --> 00:12:31,120
this

321
00:12:31,120 --> 00:12:33,600
api request if you only want to submit

322
00:12:33,600 --> 00:12:35,519
it to one specific model

323
00:12:35,519 --> 00:12:37,600
because you already know how to bypass

324
00:12:37,600 --> 00:12:40,240
the others then use descent point

325
00:12:40,240 --> 00:12:43,279
you can get the results here you can

326
00:12:43,279 --> 00:12:44,240
also upload

327
00:12:44,240 --> 00:12:47,920
your finals of zip file for all analyzes

328
00:12:47,920 --> 00:12:48,320
and

329
00:12:48,320 --> 00:12:51,200
to be checked in the sandbox query the

330
00:12:51,200 --> 00:12:53,600
status of your submission

331
00:12:53,600 --> 00:12:56,079
or also query all samples you have

332
00:12:56,079 --> 00:13:00,160
submitted or just a specific sample

333
00:13:01,279 --> 00:13:07,839
now let me show you a quick demo

334
00:13:08,560 --> 00:13:12,160
so you go to our website

335
00:13:13,360 --> 00:13:21,839
check the sums and terms and services

336
00:13:22,480 --> 00:13:24,880
you can also see a lot of additional

337
00:13:24,880 --> 00:13:26,959
background information from the defender

338
00:13:26,959 --> 00:13:28,160
and attacker challenge

339
00:13:28,160 --> 00:13:31,680
on the following site which will be

340
00:13:31,680 --> 00:13:32,480
updated

341
00:13:32,480 --> 00:13:35,839
for this year's competition

342
00:13:39,199 --> 00:13:41,519
now you can register or login with

343
00:13:41,519 --> 00:13:43,040
either microsoft github

344
00:13:43,040 --> 00:13:45,760
or google

345
00:13:46,240 --> 00:13:50,720
then download the mother samples

346
00:13:52,959 --> 00:13:55,519
and if you check them over samples you

347
00:13:55,519 --> 00:13:56,399
can see that

348
00:13:56,399 --> 00:13:59,680
we have 50 samples in it

349
00:13:59,680 --> 00:14:02,480
with a sequential order in their names

350
00:14:02,480 --> 00:14:04,399
which you have to keep because we have

351
00:14:04,399 --> 00:14:07,600
to be able to check which files you have

352
00:14:07,600 --> 00:14:10,240
modified

353
00:14:14,079 --> 00:14:16,320
now when you are ready to upload your

354
00:14:16,320 --> 00:14:18,000
files

355
00:14:18,000 --> 00:14:21,519
you can upload them here

356
00:14:26,079 --> 00:14:28,800
and as you can see i have kept the same

357
00:14:28,800 --> 00:14:31,839
file names

358
00:14:36,800 --> 00:14:39,760
you can also add some custom label for

359
00:14:39,760 --> 00:14:41,199
your

360
00:14:41,199 --> 00:14:44,160
upload so you can keep track of what is

361
00:14:44,160 --> 00:14:46,480
what

362
00:14:50,079 --> 00:14:53,279
and after some minutes

363
00:14:53,279 --> 00:14:56,720
all your samples are analyzed and you

364
00:14:56,720 --> 00:14:58,639
can see

365
00:14:58,639 --> 00:15:01,839
whether it was

366
00:15:02,000 --> 00:15:05,360
bypassing any machine learning models

367
00:15:05,360 --> 00:15:07,360
in the beginning uh because this is

368
00:15:07,360 --> 00:15:09,760
still in process

369
00:15:09,760 --> 00:15:13,839
you don't see the results

370
00:15:16,240 --> 00:15:19,360
but you can be sure that you upload it

371
00:15:19,360 --> 00:15:23,360
still valid file

372
00:15:25,760 --> 00:15:27,839
so as you can see it is now in

373
00:15:27,839 --> 00:15:29,279
processing ml

374
00:15:29,279 --> 00:15:33,440
status meanwhile i recommend you to

375
00:15:33,440 --> 00:15:34,240
check out

376
00:15:34,240 --> 00:15:36,720
your user and change your nickname from

377
00:15:36,720 --> 00:15:38,800
the randomly generated one

378
00:15:38,800 --> 00:15:40,430
to a better nickname

379
00:15:40,430 --> 00:15:42,959
[Music]

380
00:15:42,959 --> 00:15:46,000
and you can also see

381
00:15:46,000 --> 00:15:49,040
the total api queries you have already

382
00:15:49,040 --> 00:15:50,160
done

383
00:15:50,160 --> 00:15:54,320
this will be counted in the final score

384
00:15:54,320 --> 00:15:58,079
so please try and do not hammer or

385
00:15:58,079 --> 00:16:01,839
needles or services

386
00:16:07,440 --> 00:16:10,880
these were the end results uh of last

387
00:16:10,880 --> 00:16:11,759
year

388
00:16:11,759 --> 00:16:14,560
challenge and as you can see with the

389
00:16:14,560 --> 00:16:16,160
nickname need for speed

390
00:16:16,160 --> 00:16:19,199
uh that he they have reached

391
00:16:19,199 --> 00:16:23,519
the best score 150

392
00:16:23,519 --> 00:16:26,800
and there were three other competitors

393
00:16:26,800 --> 00:16:29,279
who were able to submit

394
00:16:29,279 --> 00:16:33,680
and they have reached at least a score

395
00:16:35,360 --> 00:16:38,399
and as you can see

396
00:16:39,600 --> 00:16:43,199
the machine learning process

397
00:16:43,199 --> 00:16:46,079
has been finished

398
00:16:47,279 --> 00:16:49,680
and these test samples they were not

399
00:16:49,680 --> 00:16:51,360
able to bypass

400
00:16:51,360 --> 00:16:54,000
detection

401
00:17:00,800 --> 00:17:03,680
and i think that's all i wanted to share

402
00:17:03,680 --> 00:17:06,079
with you

403
00:17:07,439 --> 00:17:10,160
so the winners of 2020 the attacker

404
00:17:10,160 --> 00:17:11,520
track

405
00:17:11,520 --> 00:17:14,000
by the nickname need for speed was

406
00:17:14,000 --> 00:17:16,559
fabrizio and marcus congratulations for

407
00:17:16,559 --> 00:17:17,039
that

408
00:17:17,039 --> 00:17:19,359
you can download their paper from the

409
00:17:19,359 --> 00:17:20,959
following url

410
00:17:20,959 --> 00:17:23,439
and the defender track winners were

411
00:17:23,439 --> 00:17:26,400
irvine lucas michael daniella and conrad

412
00:17:26,400 --> 00:17:29,120
they also have a white paper and the

413
00:17:29,120 --> 00:17:31,200
second place for the defender that their

414
00:17:31,200 --> 00:17:33,600
track also goes for fabrizio and

415
00:17:33,600 --> 00:17:37,120
marcus and the attacker track second

416
00:17:37,120 --> 00:17:38,240
place was won

417
00:17:38,240 --> 00:17:41,120
by wunderwuzzi you i highly recommend

418
00:17:41,120 --> 00:17:42,880
you to check out his blog

419
00:17:42,880 --> 00:17:44,960
he has also a lot of interesting

420
00:17:44,960 --> 00:17:48,880
research from red teaming there

421
00:17:48,880 --> 00:17:51,039
the other car winner used xor based

422
00:17:51,039 --> 00:17:53,600
cryptor base64 obfuscation and lot of

423
00:17:53,600 --> 00:17:56,000
dead imports to the import table

424
00:17:56,000 --> 00:17:58,480
and the defender winner was using dpe

425
00:17:58,480 --> 00:17:59,039
berus

426
00:17:59,039 --> 00:18:00,720
solution which were using different

427
00:18:00,720 --> 00:18:02,559
machine learning models semantic gap

428
00:18:02,559 --> 00:18:03,360
detectors

429
00:18:03,360 --> 00:18:06,480
and state for defense in total 60 people

430
00:18:06,480 --> 00:18:08,640
registered for the competition

431
00:18:08,640 --> 00:18:10,960
two people submitted a very docker image

432
00:18:10,960 --> 00:18:11,919
with working

433
00:18:11,919 --> 00:18:13,600
machine learning based mother detection

434
00:18:13,600 --> 00:18:15,039
inside as i already

435
00:18:15,039 --> 00:18:17,840
told you and five people were able to

436
00:18:17,840 --> 00:18:19,440
bypass at least a single machine

437
00:18:19,440 --> 00:18:20,960
learning model

438
00:18:20,960 --> 00:18:23,679
and the machine learning engines checked

439
00:18:23,679 --> 00:18:25,039
samples at least

440
00:18:25,039 --> 00:18:28,480
5 000 times in total

441
00:18:28,480 --> 00:18:31,039
if you are interested in how these

442
00:18:31,039 --> 00:18:31,840
samples

443
00:18:31,840 --> 00:18:34,799
worked out in real life i recommend you

444
00:18:34,799 --> 00:18:35,840
to check out

445
00:18:35,840 --> 00:18:38,400
our blog page and it was interesting to

446
00:18:38,400 --> 00:18:39,360
see that

447
00:18:39,360 --> 00:18:42,559
these bypasses were also useful not just

448
00:18:42,559 --> 00:18:44,000
in our competition

449
00:18:44,000 --> 00:18:45,840
but if you upload the samples to

450
00:18:45,840 --> 00:18:47,039
virustotal

451
00:18:47,039 --> 00:18:50,160
they can also bypass some of these

452
00:18:50,160 --> 00:18:52,080
detections

453
00:18:52,080 --> 00:18:55,760
and next the floor is for hiram

454
00:18:55,760 --> 00:18:57,679
who will show you the tools which were

455
00:18:57,679 --> 00:19:00,880
used uh in the 2020 competition

456
00:19:00,880 --> 00:19:04,080
which can have other cares to bypass the

457
00:19:04,080 --> 00:19:07,440
defenses my name is hyrum anderson

458
00:19:07,440 --> 00:19:10,480
and i'm delighted to present virtually

459
00:19:10,480 --> 00:19:12,799
i'm going to be presenting the machine

460
00:19:12,799 --> 00:19:14,640
learning evasion code walkthrough

461
00:19:14,640 --> 00:19:16,720
this tool was included in the sample

462
00:19:16,720 --> 00:19:18,720
solution to the attacker challenge

463
00:19:18,720 --> 00:19:20,480
in the machine learning security evasion

464
00:19:20,480 --> 00:19:22,160
competition

465
00:19:22,160 --> 00:19:23,919
you can download and play with this code

466
00:19:23,919 --> 00:19:26,559
by visiting mlsec.io

467
00:19:26,559 --> 00:19:28,480
and navigating to the attacker challenge

468
00:19:28,480 --> 00:19:31,200
github repo

469
00:19:31,520 --> 00:19:33,919
our example solution was designed with

470
00:19:33,919 --> 00:19:35,600
the following things in mind

471
00:19:35,600 --> 00:19:38,160
number one the models to be attacked

472
00:19:38,160 --> 00:19:40,080
were submitted by contestants

473
00:19:40,080 --> 00:19:42,320
in the defender challenge that concluded

474
00:19:42,320 --> 00:19:44,000
previously

475
00:19:44,000 --> 00:19:46,240
two models from the previous round

476
00:19:46,240 --> 00:19:48,400
qualified to be included in the attacker

477
00:19:48,400 --> 00:19:49,919
challenge

478
00:19:49,919 --> 00:19:53,280
in addition we have hosted our own model

479
00:19:53,280 --> 00:19:55,120
to be attacked that is based on the

480
00:19:55,120 --> 00:19:57,918
ember data set

481
00:19:58,880 --> 00:20:02,000
the models to be attacked report hard

482
00:20:02,000 --> 00:20:02,880
labels

483
00:20:02,880 --> 00:20:05,039
for submissions provided to them that is

484
00:20:05,039 --> 00:20:07,039
they do not provide a score between zero

485
00:20:07,039 --> 00:20:07,679
and one

486
00:20:07,679 --> 00:20:10,480
but just a zero for benign and one for

487
00:20:10,480 --> 00:20:12,960
malicious

488
00:20:13,200 --> 00:20:16,320
finally we considered the leaderboard

489
00:20:16,320 --> 00:20:17,200
criteria

490
00:20:17,200 --> 00:20:20,240
while designing our solution

491
00:20:20,240 --> 00:20:23,440
the first criterion considers the number

492
00:20:23,440 --> 00:20:24,640
of model invasions

493
00:20:24,640 --> 00:20:27,120
with three models and 50 malware samples

494
00:20:27,120 --> 00:20:31,120
the maximum score is 150 points

495
00:20:31,120 --> 00:20:34,159
a tiebreaker would be decided by the

496
00:20:34,159 --> 00:20:35,280
efficiency

497
00:20:35,280 --> 00:20:37,679
in which the models were evaded so we

498
00:20:37,679 --> 00:20:40,480
count the number of model queries

499
00:20:40,480 --> 00:20:43,200
thus contestants are incentivized not

500
00:20:43,200 --> 00:20:44,640
only to be evasive

501
00:20:44,640 --> 00:20:49,520
but also to do it efficiently

502
00:20:49,520 --> 00:20:51,440
contestants were able to choose any

503
00:20:51,440 --> 00:20:53,200
strategy that they like

504
00:20:53,200 --> 00:20:56,159
to demonstrate one strategy we released

505
00:20:56,159 --> 00:20:57,520
this example code

506
00:20:57,520 --> 00:21:00,960
on our competitions github site

507
00:21:00,960 --> 00:21:04,159
the code uses discrete optimization

508
00:21:04,159 --> 00:21:06,559
over functionality preserving file

509
00:21:06,559 --> 00:21:08,240
modifications

510
00:21:08,240 --> 00:21:10,799
using a package called hyperopt which

511
00:21:10,799 --> 00:21:11,919
ironically

512
00:21:11,919 --> 00:21:15,039
is often used for tuning

513
00:21:15,039 --> 00:21:17,679
machine learning models you can learn

514
00:21:17,679 --> 00:21:18,400
more about

515
00:21:18,400 --> 00:21:21,440
the details from the code

516
00:21:21,440 --> 00:21:24,960
and documentation on the website

517
00:21:24,960 --> 00:21:27,280
however the broader strategy may be of

518
00:21:27,280 --> 00:21:30,559
general interest

519
00:21:30,559 --> 00:21:32,880
it consists of employing an algorithm in

520
00:21:32,880 --> 00:21:34,000
part a

521
00:21:34,000 --> 00:21:35,840
to get the bulk of the malware to be

522
00:21:35,840 --> 00:21:37,039
evasive

523
00:21:37,039 --> 00:21:40,080
while remaining functional followed by

524
00:21:40,080 --> 00:21:43,280
any additional manual steps in part b

525
00:21:43,280 --> 00:21:47,520
to fix any few remaining samples

526
00:21:47,520 --> 00:21:50,880
we use the example code in our strategy

527
00:21:50,880 --> 00:21:54,400
in part a in which we

528
00:21:54,400 --> 00:21:56,880
break our algorithmic attack into two

529
00:21:56,880 --> 00:21:57,520
phases

530
00:21:57,520 --> 00:22:00,559
an offline attack against a known

531
00:22:00,559 --> 00:22:03,280
defender model for which we have code

532
00:22:03,280 --> 00:22:04,159
and weights

533
00:22:04,159 --> 00:22:06,799
and an online attack the hope is that

534
00:22:06,799 --> 00:22:08,960
many of the samples that were discovered

535
00:22:08,960 --> 00:22:10,400
in the offline attack

536
00:22:10,400 --> 00:22:12,960
will cross evade the hosted online

537
00:22:12,960 --> 00:22:13,840
models

538
00:22:13,840 --> 00:22:15,840
without having used any additional

539
00:22:15,840 --> 00:22:18,720
queries against the api

540
00:22:18,720 --> 00:22:21,360
you may visit mlsect.io to download this

541
00:22:21,360 --> 00:22:22,080
code

542
00:22:22,080 --> 00:22:24,480
however please be warned that when you

543
00:22:24,480 --> 00:22:26,480
run this demo code you will write

544
00:22:26,480 --> 00:22:28,240
functional malware to disk

545
00:22:28,240 --> 00:22:32,799
so please do so only using a linux vm

546
00:22:33,280 --> 00:22:35,919
to begin we initialize the attack by

547
00:22:35,919 --> 00:22:38,960
analyzing a collection of benign files

548
00:22:38,960 --> 00:22:41,600
the init sub command extracts elements

549
00:22:41,600 --> 00:22:43,200
of the benign files that will be

550
00:22:43,200 --> 00:22:44,320
injected

551
00:22:44,320 --> 00:22:47,679
later into the malware to launch our

552
00:22:47,679 --> 00:22:48,960
offline attack

553
00:22:48,960 --> 00:22:51,600
we first run a local copy of the ember

554
00:22:51,600 --> 00:22:52,400
model

555
00:22:52,400 --> 00:22:54,799
in the top window then in the bottom

556
00:22:54,799 --> 00:22:55,679
window

557
00:22:55,679 --> 00:22:58,000
we'll use the run sub command of our

558
00:22:58,000 --> 00:22:59,280
attack script

559
00:22:59,280 --> 00:23:03,200
passing in malware samples

560
00:23:03,200 --> 00:23:06,640
that were downloaded by contestants

561
00:23:06,640 --> 00:23:08,960
the tool will write successful evasion

562
00:23:08,960 --> 00:23:09,679
attempts

563
00:23:09,679 --> 00:23:12,720
to pass one success

564
00:23:12,720 --> 00:23:15,679
and failed attempts to pass one slash

565
00:23:15,679 --> 00:23:17,679
failure

566
00:23:17,679 --> 00:23:20,240
also included in each directory are

567
00:23:20,240 --> 00:23:21,520
history files

568
00:23:21,520 --> 00:23:23,280
that would allow us to resume our

569
00:23:23,280 --> 00:23:27,039
optimization of failed attempts

570
00:23:32,480 --> 00:23:35,440
in a second pass we begin with only the

571
00:23:35,440 --> 00:23:37,600
past one failures and iterate on the

572
00:23:37,600 --> 00:23:38,960
optimization

573
00:23:38,960 --> 00:23:41,279
again storing successes and failures to

574
00:23:41,279 --> 00:23:44,640
a folder of our choice

575
00:23:45,760 --> 00:23:47,360
having now collected candidates that

576
00:23:47,360 --> 00:23:49,279
evade the ember model

577
00:23:49,279 --> 00:23:52,480
we use those candidates as seeds for an

578
00:23:52,480 --> 00:23:55,039
online attack which now counts against

579
00:23:55,039 --> 00:23:57,200
our api query usage

580
00:23:57,200 --> 00:23:59,200
the online attack is enabled with a

581
00:23:59,200 --> 00:24:00,400
simple dash dash

582
00:24:00,400 --> 00:24:05,520
online flag

583
00:24:05,520 --> 00:24:08,799
after several iterations we can now

584
00:24:08,799 --> 00:24:12,080
gather the successfully evasive variants

585
00:24:12,080 --> 00:24:15,200
and create a zip file to be uploaded to

586
00:24:15,200 --> 00:24:19,840
the website

587
00:24:20,559 --> 00:24:22,400
since there is a chance that the file

588
00:24:22,400 --> 00:24:24,159
modifications have broken

589
00:24:24,159 --> 00:24:27,360
some of the files contestants

590
00:24:27,360 --> 00:24:29,919
have been encouraged to run the samples

591
00:24:29,919 --> 00:24:31,760
in a windows 10 sandbox

592
00:24:31,760 --> 00:24:34,159
to validate functionality before

593
00:24:34,159 --> 00:24:37,919
uploading to the competition website

594
00:24:38,640 --> 00:24:41,679
as a final note since the hosted models

595
00:24:41,679 --> 00:24:44,240
may learn from the queries issued by

596
00:24:44,240 --> 00:24:45,520
contestants

597
00:24:45,520 --> 00:24:48,159
there is a possibility that an evasive

598
00:24:48,159 --> 00:24:50,159
variant discovered by a contestant

599
00:24:50,159 --> 00:24:52,960
at the beginning of the competition may

600
00:24:52,960 --> 00:24:53,919
no longer

601
00:24:53,919 --> 00:24:57,200
be evasive to a model by the end of the

602
00:24:57,200 --> 00:24:58,240
competition

603
00:24:58,240 --> 00:25:02,400
because the model has changed its state

604
00:25:03,440 --> 00:25:07,360
and new this year that we will start

605
00:25:07,360 --> 00:25:10,559
the defender challenge on june 15 and

606
00:25:10,559 --> 00:25:11,919
the attacker challenge on

607
00:25:11,919 --> 00:25:15,679
august 6th and we have a new easy

608
00:25:15,679 --> 00:25:17,919
beginner friendly fishing track which i

609
00:25:17,919 --> 00:25:20,240
highly recommend for all of you to check

610
00:25:20,240 --> 00:25:21,200
out

611
00:25:21,200 --> 00:25:24,480
and hopefully this new track

612
00:25:24,480 --> 00:25:28,559
will attract more participants for this

613
00:25:28,559 --> 00:25:31,919
competition so that's all folks

614
00:25:31,919 --> 00:25:34,000
that's all i wanted to share with you

615
00:25:34,000 --> 00:25:35,840
thank you for your attention

616
00:25:35,840 --> 00:25:37,760
if you have any questions you can reach

617
00:25:37,760 --> 00:25:38,880
out on

618
00:25:38,880 --> 00:25:46,799
any of the first platforms thank you

