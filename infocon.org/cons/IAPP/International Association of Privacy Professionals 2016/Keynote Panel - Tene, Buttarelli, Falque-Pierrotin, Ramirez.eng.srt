1
00:00:00,370 --> 00:00:05,280
[Music]

2
00:00:08,300 --> 00:00:14,639
so either obviously this is a dramatic

3
00:00:14,639 --> 00:00:23,519
morning and AH you know too we have to

4
00:00:23,519 --> 00:00:25,800
discuss the elephant in the room excuse

5
00:00:25,800 --> 00:00:28,050
the pun because elephant is the symbol

6
00:00:28,050 --> 00:00:32,070
of the Republicans also mm I think

7
00:00:32,070 --> 00:00:34,380
regardless of political opinion this is

8
00:00:34,380 --> 00:00:39,510
a stunning surprise and I know it's very

9
00:00:39,510 --> 00:00:43,620
early but I want to ask you what will a

10
00:00:43,620 --> 00:00:47,910
trump presidency mean for privacy for

11
00:00:47,910 --> 00:00:51,960
the privacy profession for the agency

12
00:00:51,960 --> 00:00:58,020
and if you want for Washington DC it's a

13
00:00:58,020 --> 00:01:01,050
big question where and let me just start

14
00:01:01,050 --> 00:01:04,979
though by by thanking I a PP and you for

15
00:01:04,979 --> 00:01:06,720
the invitation to be here I really am

16
00:01:06,720 --> 00:01:08,130
delighted although I confess that it

17
00:01:08,130 --> 00:01:10,200
seems a bit odd for me to be here in

18
00:01:10,200 --> 00:01:12,390
Brussels and not to be in the u.s. today

19
00:01:12,390 --> 00:01:15,570
of all days yes there has unquestionably

20
00:01:15,570 --> 00:01:19,560
been a stunning and unexpected outcome

21
00:01:19,560 --> 00:01:23,930
in the election in the United States

22
00:01:23,930 --> 00:01:28,409
that being said I believe that the

23
00:01:28,409 --> 00:01:30,420
issues that we are discussing today

24
00:01:30,420 --> 00:01:35,130
about privacy and data security are ones

25
00:01:35,130 --> 00:01:37,110
that are of monumental importance and

26
00:01:37,110 --> 00:01:41,009
with any change in administration there

27
00:01:41,009 --> 00:01:43,350
are always going to be of course shifts

28
00:01:43,350 --> 00:01:45,270
and different approaches taken but I

29
00:01:45,270 --> 00:01:47,509
believe that there will be a continued

30
00:01:47,509 --> 00:01:50,460
emphasis placed on these very important

31
00:01:50,460 --> 00:01:53,640
issues I certainly know from our work at

32
00:01:53,640 --> 00:01:55,590
the FTC that these are issues that are

33
00:01:55,590 --> 00:01:57,899
of significant importance to the

34
00:01:57,899 --> 00:02:00,180
American people and as a consequence I

35
00:02:00,180 --> 00:02:02,670
think that will continue to be there

36
00:02:02,670 --> 00:02:05,130
will be continued effort to focus on the

37
00:02:05,130 --> 00:02:08,008
protection of privacy and investment in

38
00:02:08,008 --> 00:02:09,550
cyber security and

39
00:02:09,550 --> 00:02:12,190
tending to the privacy implications and

40
00:02:12,190 --> 00:02:15,220
and security implications of emerging

41
00:02:15,220 --> 00:02:17,650
technologies I can certainly speak much

42
00:02:17,650 --> 00:02:20,740
more reliably of course with respect to

43
00:02:20,740 --> 00:02:23,800
the FTC in particular and the Federal

44
00:02:23,800 --> 00:02:26,770
Trade Commission is an independent

45
00:02:26,770 --> 00:02:30,760
bipartisan agency and I can tell you

46
00:02:30,760 --> 00:02:34,600
that historically there have been less

47
00:02:34,600 --> 00:02:36,760
of a swing across administration's when

48
00:02:36,760 --> 00:02:40,360
it comes to the agency priorities and

49
00:02:40,360 --> 00:02:42,480
there is no question in my mind that

50
00:02:42,480 --> 00:02:45,460
even in the face of a change of

51
00:02:45,460 --> 00:02:48,460
leadership which will certainly occur at

52
00:02:48,460 --> 00:02:51,340
the agency these issues will continue it

53
00:02:51,340 --> 00:02:53,710
important for for the FTC I think we

54
00:02:53,710 --> 00:02:55,840
will continue to be focused on

55
00:02:55,840 --> 00:02:59,890
enforcement on significant policy work

56
00:02:59,890 --> 00:03:03,190
and research that the agency is engaging

57
00:03:03,190 --> 00:03:04,930
I think that will continue and of course

58
00:03:04,930 --> 00:03:07,960
also focus on the importance of

59
00:03:07,960 --> 00:03:09,880
educating consumers and educating

60
00:03:09,880 --> 00:03:15,640
businesses so I am confident that any

61
00:03:15,640 --> 00:03:17,140
change in leadership those priorities

62
00:03:17,140 --> 00:03:19,930
will continue equally important of

63
00:03:19,930 --> 00:03:22,739
course will also be I believe the

64
00:03:22,739 --> 00:03:24,750
enforcement of the privacy shield

65
00:03:24,750 --> 00:03:29,890
program great so Isabella want to hear

66
00:03:29,890 --> 00:03:34,450
your thoughts about the same topic as

67
00:03:34,450 --> 00:03:37,090
Giovanni said in his keynote address

68
00:03:37,090 --> 00:03:40,780
really a staggering change over a short

69
00:03:40,780 --> 00:03:43,890
period of time breaks it the Trump

70
00:03:43,890 --> 00:03:47,260
election French elections are coming up

71
00:03:47,260 --> 00:03:49,510
in a couple of months what's next for

72
00:03:49,510 --> 00:03:54,700
privacy well and I will have three

73
00:03:54,700 --> 00:03:57,850
reactions towards the at least the US

74
00:03:57,850 --> 00:04:00,880
election and probably one of the remark

75
00:04:00,880 --> 00:04:03,130
is common between the brexit and these

76
00:04:03,130 --> 00:04:06,670
presidential election I think first it

77
00:04:06,670 --> 00:04:09,310
shows that we have to listen to the

78
00:04:09,310 --> 00:04:14,260
peoples and in all these countries

79
00:04:14,260 --> 00:04:18,048
peoples have expressed something and so

80
00:04:18,048 --> 00:04:19,850
I think it's a very strong political

81
00:04:19,850 --> 00:04:24,740
message to the politicians and probably

82
00:04:24,740 --> 00:04:27,470
in France we should realize what could

83
00:04:27,470 --> 00:04:32,120
be the consequences second I would say

84
00:04:32,120 --> 00:04:35,990
that it makes me feel that all the polls

85
00:04:35,990 --> 00:04:38,870
and all the algorithm you know that are

86
00:04:38,870 --> 00:04:42,560
presented as key leve to all types of

87
00:04:42,560 --> 00:04:46,000
problems should be taken with

88
00:04:46,000 --> 00:04:51,500
cautiousness because obviously the

89
00:04:51,500 --> 00:04:54,740
figures are not so simple and you just

90
00:04:54,740 --> 00:04:57,590
can't rely only on the device on an

91
00:04:57,590 --> 00:05:00,500
algorithm to have the results I mean

92
00:05:00,500 --> 00:05:02,150
human nature is a little bit more

93
00:05:02,150 --> 00:05:06,770
complex and third and I would echo to

94
00:05:06,770 --> 00:05:12,110
what edit just said and more technically

95
00:05:12,110 --> 00:05:14,330
we have developed with the United States

96
00:05:14,330 --> 00:05:17,630
a very good relationship as regards

97
00:05:17,630 --> 00:05:20,330
international data flows and we have

98
00:05:20,330 --> 00:05:22,520
very close contacts with the department

99
00:05:22,520 --> 00:05:27,169
of commerce with the FTC with the D odni

100
00:05:27,169 --> 00:05:30,080
and and i hope this very good

101
00:05:30,080 --> 00:05:32,720
relationship is going to to be pursued

102
00:05:32,720 --> 00:05:36,229
and i'm quite happy to what edit just

103
00:05:36,229 --> 00:05:38,240
said because i think this is something

104
00:05:38,240 --> 00:05:40,880
of great value the relationship we have

105
00:05:40,880 --> 00:05:43,190
developed between our two countries on

106
00:05:43,190 --> 00:05:47,210
privacy this is a common subject we had

107
00:05:47,210 --> 00:05:51,050
in together and so I hope the new

108
00:05:51,050 --> 00:05:55,130
administration is going to to to carry

109
00:05:55,130 --> 00:05:58,880
along the same lines great Giovanni I'm

110
00:05:58,880 --> 00:06:01,430
eager to hear your thoughts about the

111
00:06:01,430 --> 00:06:05,479
same issue but picking off on something

112
00:06:05,479 --> 00:06:09,380
that Isabel just said the whole premise

113
00:06:09,380 --> 00:06:12,530
of big data analysis and I know there is

114
00:06:12,530 --> 00:06:17,360
a project that EDPs is working on data

115
00:06:17,360 --> 00:06:22,190
FX trying to frame the ethical context

116
00:06:22,190 --> 00:06:25,580
for big data the whole premise is that

117
00:06:25,580 --> 00:06:27,820
we collect a lot of data we look

118
00:06:27,820 --> 00:06:30,040
the machines kind of churned through it

119
00:06:30,040 --> 00:06:33,780
and we draw lessons about individual

120
00:06:33,780 --> 00:06:36,910
preferences and thoughts and behaviors

121
00:06:36,910 --> 00:06:41,410
and yet here we've seen over the past

122
00:06:41,410 --> 00:06:46,330
year poll after poll all over the US and

123
00:06:46,330 --> 00:06:50,950
in each of the 50 states get absolutely

124
00:06:50,950 --> 00:06:53,650
wrong you know the choice between just

125
00:06:53,650 --> 00:06:57,190
two alternatives up what does this mean

126
00:06:57,190 --> 00:07:02,080
for big data analysis oh good question

127
00:07:02,080 --> 00:07:04,690
but I cannot say business as usual

128
00:07:04,690 --> 00:07:07,900
because of course I mean this long night

129
00:07:07,900 --> 00:07:12,610
will have an impact but who knows we see

130
00:07:12,610 --> 00:07:17,020
a distance that the elected president is

131
00:07:17,020 --> 00:07:19,750
considered by someone as partly

132
00:07:19,750 --> 00:07:21,760
unpredictable so it could be that he

133
00:07:21,760 --> 00:07:24,700
will surprise us with something positive

134
00:07:24,700 --> 00:07:26,700
in terms of data but I see a trend

135
00:07:26,700 --> 00:07:30,810
internationally in terms of I mean

136
00:07:30,810 --> 00:07:35,170
cooperation mutual recognition synergies

137
00:07:35,170 --> 00:07:39,180
and I think it would be difficult to

138
00:07:39,180 --> 00:07:43,060
oppose to this trend big data is only a

139
00:07:43,060 --> 00:07:46,180
future if we will invest a lot of

140
00:07:46,180 --> 00:07:48,790
efforts in terms of big data protection

141
00:07:48,790 --> 00:07:52,800
if we allow people to be in control so

142
00:07:52,800 --> 00:07:55,630
transparency first and and and new

143
00:07:55,630 --> 00:07:59,290
effective ways to I mean make existing

144
00:07:59,290 --> 00:08:01,480
principles more effective in practice I

145
00:08:01,480 --> 00:08:04,090
think this is the challenge the ethical

146
00:08:04,090 --> 00:08:08,560
exercise of course aims at focusing on

147
00:08:08,560 --> 00:08:10,930
what else we should do in addition to

148
00:08:10,930 --> 00:08:14,080
two rules my vision is that data

149
00:08:14,080 --> 00:08:16,570
protection legislation consumer low

150
00:08:16,570 --> 00:08:18,520
competition law cannot give all the

151
00:08:18,520 --> 00:08:21,250
answers something more is to be to be

152
00:08:21,250 --> 00:08:24,370
explored and I would really welcome that

153
00:08:24,370 --> 00:08:26,620
even the new administration can can

154
00:08:26,620 --> 00:08:31,139
simply be on the same track we

155
00:08:31,139 --> 00:08:35,130
we have a lot of homeworks in terms of

156
00:08:35,130 --> 00:08:37,708
joint review of existing legal

157
00:08:37,708 --> 00:08:40,110
instruments I think they are essential

158
00:08:40,110 --> 00:08:44,690
for business purposes and institutional

159
00:08:44,690 --> 00:08:48,959
relationships and we do in Europe have

160
00:08:48,959 --> 00:08:52,589
homeworks we we are working to ensure

161
00:08:52,589 --> 00:08:56,820
that the let's say implementation do

162
00:08:56,820 --> 00:08:58,350
implement a shin is not the right word

163
00:08:58,350 --> 00:09:03,510
of the gtp are by member states level is

164
00:09:03,510 --> 00:09:06,560
consistent with this process in terms of

165
00:09:06,560 --> 00:09:09,899
harmonization and I really appreciated

166
00:09:09,899 --> 00:09:13,050
the commitment of Christopher Graham and

167
00:09:13,050 --> 00:09:15,300
Elizabeth denim to say regardless of

168
00:09:15,300 --> 00:09:19,860
what the outcome will be about brexit in

169
00:09:19,860 --> 00:09:22,740
any event GDP are will be will be a

170
00:09:22,740 --> 00:09:26,459
reality in in UK it will enter into full

171
00:09:26,459 --> 00:09:29,640
application by May 2018 among confident

172
00:09:29,640 --> 00:09:33,149
that regardless of the of the outcome

173
00:09:33,149 --> 00:09:36,630
that they will continue working in in

174
00:09:36,630 --> 00:09:41,160
such and such a perspective so perhaps

175
00:09:41,160 --> 00:09:44,339
you may expect that I comment in a

176
00:09:44,339 --> 00:09:47,250
pessimistic way but but after such a

177
00:09:47,250 --> 00:09:49,649
long night let me be optimistic that's

178
00:09:49,649 --> 00:09:53,899
great we appreciate that Giovanni either

179
00:09:53,899 --> 00:09:57,329
I've you'd the Fair Credit Reporting Act

180
00:09:57,329 --> 00:10:03,269
vikre as one of the first pieces of

181
00:10:03,269 --> 00:10:05,640
legislation that actually first saw some

182
00:10:05,640 --> 00:10:07,980
of the issues and problems involved with

183
00:10:07,980 --> 00:10:11,910
big data including getting it wrong and

184
00:10:11,910 --> 00:10:15,449
assigning people scores that don't

185
00:10:15,449 --> 00:10:18,360
really fit their profile which might

186
00:10:18,360 --> 00:10:21,180
have a real impact on their employment

187
00:10:21,180 --> 00:10:25,079
or credit or insurance and of course

188
00:10:25,079 --> 00:10:27,510
faker is one of the pillars of FTC

189
00:10:27,510 --> 00:10:30,930
enforcement and privacy space how do you

190
00:10:30,930 --> 00:10:35,279
see this you know potential of big data

191
00:10:35,279 --> 00:10:39,540
to get it wrong look the question that

192
00:10:39,540 --> 00:10:42,089
you raised and certainly the outcome of

193
00:10:42,089 --> 00:10:43,260
our election really does

194
00:10:43,260 --> 00:10:45,570
this rate the fact that we have to be

195
00:10:45,570 --> 00:10:47,430
very careful that we've become enamored

196
00:10:47,430 --> 00:10:50,730
of data and big data and algorithms and

197
00:10:50,730 --> 00:10:54,240
I know that we at the FTC have been

198
00:10:54,240 --> 00:10:56,940
saying this and other DPAs have been

199
00:10:56,940 --> 00:10:58,800
saying this which is we have to be

200
00:10:58,800 --> 00:11:01,170
careful when it comes to the conclusions

201
00:11:01,170 --> 00:11:05,090
that one draws from big data analytics

202
00:11:05,090 --> 00:11:08,220
we the FTC issued a report a big data

203
00:11:08,220 --> 00:11:10,830
report in January of this year we

204
00:11:10,830 --> 00:11:12,420
cautioned that there may be

205
00:11:12,420 --> 00:11:16,020
circumstances in which just simply the

206
00:11:16,020 --> 00:11:18,360
fact that you have a huge volume of data

207
00:11:18,360 --> 00:11:20,160
doesn't necessarily mean that the out

208
00:11:20,160 --> 00:11:22,170
that the results are more accurate and

209
00:11:22,170 --> 00:11:25,470
that there may be incomplete data sets

210
00:11:25,470 --> 00:11:27,660
you may be missing the views of a large

211
00:11:27,660 --> 00:11:29,250
segment of the population and I

212
00:11:29,250 --> 00:11:32,010
certainly think that's one lesson to be

213
00:11:32,010 --> 00:11:34,290
drawn from what happened in the United

214
00:11:34,290 --> 00:11:38,280
States in this election in addition to

215
00:11:38,280 --> 00:11:39,930
that I think there is also a concern

216
00:11:39,930 --> 00:11:43,560
that we have that the use of big data

217
00:11:43,560 --> 00:11:48,080
analytics may also reinforce existing

218
00:11:48,080 --> 00:11:50,790
biases and I think we have to be very

219
00:11:50,790 --> 00:11:53,790
cautious about that and we need to and

220
00:11:53,790 --> 00:11:56,250
and certainly in turn our sphere in

221
00:11:56,250 --> 00:11:58,980
terms of how companies use this data we

222
00:11:58,980 --> 00:12:00,060
think that it's very important that

223
00:12:00,060 --> 00:12:03,960
companies really examine the the results

224
00:12:03,960 --> 00:12:05,760
that they're getting from the use of big

225
00:12:05,760 --> 00:12:08,970
data analytics and ensure that not only

226
00:12:08,970 --> 00:12:11,580
are they complying with consumer

227
00:12:11,580 --> 00:12:12,570
protections like the Fair Credit

228
00:12:12,570 --> 00:12:17,370
Reporting Act but also that they ensure

229
00:12:17,370 --> 00:12:20,160
that the outcomes truly are our

230
00:12:20,160 --> 00:12:22,320
appropriate fair and so I think we need

231
00:12:22,320 --> 00:12:24,480
to be taking a hard look at this and I

232
00:12:24,480 --> 00:12:25,560
know that certainly in the u.s. there

233
00:12:25,560 --> 00:12:28,950
will be a very close self-examination

234
00:12:28,950 --> 00:12:31,290
about what went wrong when it came to

235
00:12:31,290 --> 00:12:34,920
trying to evaluate consumer preferences

236
00:12:34,920 --> 00:12:39,060
public open and yeah um so all three of

237
00:12:39,060 --> 00:12:41,550
you mentioned privacy shield that now

238
00:12:41,550 --> 00:12:44,450
it's gonna be a big theme here and

239
00:12:44,450 --> 00:12:48,900
conference I want to ask you javonni and

240
00:12:48,900 --> 00:12:54,630
usable up in just about six months we

241
00:12:54,630 --> 00:12:56,279
are going to have the first

242
00:12:56,279 --> 00:12:59,730
annual review of privacy shield how are

243
00:12:59,730 --> 00:13:04,680
we doing and also maybe is shield GDP

244
00:13:04,680 --> 00:13:07,560
already itself Giovanni if you want to

245
00:13:07,560 --> 00:13:15,689
start you know how DPAs have interacted

246
00:13:15,689 --> 00:13:19,800
and commented the privacy shield we we

247
00:13:19,800 --> 00:13:21,600
should recognize that the the final

248
00:13:21,600 --> 00:13:23,759
outcome is much better than the initial

249
00:13:23,759 --> 00:13:27,449
proposal many improvements particularly

250
00:13:27,449 --> 00:13:29,610
on the commercial side but we were not

251
00:13:29,610 --> 00:13:32,279
of course when under percent satisfied

252
00:13:32,279 --> 00:13:36,569
and it's difficult to say the privacy

253
00:13:36,569 --> 00:13:40,110
shield is a court proof and as you see

254
00:13:40,110 --> 00:13:44,569
from headlines it may be subject to

255
00:13:44,569 --> 00:13:47,610
scrutiny soon but the joint to review

256
00:13:47,610 --> 00:13:51,540
exercise goes in in parallel it is not

257
00:13:51,540 --> 00:13:55,980
performed by DPS exclusively but we are

258
00:13:55,980 --> 00:13:57,870
respecting to be associated as the

259
00:13:57,870 --> 00:14:00,750
Commission as declare in in the relevant

260
00:14:00,750 --> 00:14:03,809
decision other exercises are taking

261
00:14:03,809 --> 00:14:05,879
place with regard to other legal

262
00:14:05,879 --> 00:14:08,639
instruments as the commissioners

263
00:14:08,639 --> 00:14:12,389
publicly announced a decision is to be

264
00:14:12,389 --> 00:14:16,980
taken now to amend decisions on

265
00:14:16,980 --> 00:14:18,509
contractual closest binding corporate

266
00:14:18,509 --> 00:14:20,699
rules and others adequacy decisions

267
00:14:20,699 --> 00:14:24,089
included with regard to the exercise of

268
00:14:24,089 --> 00:14:26,160
powers by by DPS and this would be

269
00:14:26,160 --> 00:14:31,050
extremely extremely relevant so 20

270
00:14:31,050 --> 00:14:34,860
networking party is working hard on all

271
00:14:34,860 --> 00:14:37,740
these dossiers so looking to the future

272
00:14:37,740 --> 00:14:41,429
about GDP are but also administering the

273
00:14:41,429 --> 00:14:43,740
the present so we have to consider the

274
00:14:43,740 --> 00:14:46,589
current legal framework but we have to

275
00:14:46,589 --> 00:14:49,649
see how the shield and any other legal

276
00:14:49,649 --> 00:14:53,220
eastman may work in in the next in the

277
00:14:53,220 --> 00:14:57,480
next future it's a lot of work with

278
00:14:57,480 --> 00:15:00,449
regard to different items I don't go now

279
00:15:00,449 --> 00:15:05,360
into into details and it absorbs

280
00:15:05,360 --> 00:15:07,620
something like thirty thirty five

281
00:15:07,620 --> 00:15:09,030
percent of

282
00:15:09,030 --> 00:15:12,980
the activities of the relevant subgroups

283
00:15:12,980 --> 00:15:18,530
am I positive or I mean possible

284
00:15:18,530 --> 00:15:23,400
optimistic here okay give me a couple of

285
00:15:23,400 --> 00:15:26,100
minutes optimistic down today yes back

286
00:15:26,100 --> 00:15:29,460
off man on the specific issue give me a

287
00:15:29,460 --> 00:15:31,890
couple of minutes to think about okay

288
00:15:31,890 --> 00:15:34,980
Isabell you want uncommon yeah just to

289
00:15:34,980 --> 00:15:39,210
add that it's true that the first joint

290
00:15:39,210 --> 00:15:42,110
review is going to be a critical meeting

291
00:15:42,110 --> 00:15:46,020
because at this meeting we will estimate

292
00:15:46,020 --> 00:15:49,680
if the commitments of the US government

293
00:15:49,680 --> 00:15:54,690
is real are real effective in reality or

294
00:15:54,690 --> 00:15:58,890
not because as Giovanni said we have on

295
00:15:58,890 --> 00:16:01,670
the final text of the shield expressed

296
00:16:01,670 --> 00:16:05,160
concerns on various points in particular

297
00:16:05,160 --> 00:16:07,890
on the Ombudsperson and on the mass

298
00:16:07,890 --> 00:16:10,680
collection and in front of these

299
00:16:10,680 --> 00:16:14,010
concerns the US administration has said

300
00:16:14,010 --> 00:16:17,580
has taken some commitments so the

301
00:16:17,580 --> 00:16:20,400
question now is all these commitments

302
00:16:20,400 --> 00:16:26,490
real effective proved through metrics or

303
00:16:26,490 --> 00:16:30,660
not so the the joint the first joint

304
00:16:30,660 --> 00:16:33,860
meeting is a real is a real day and

305
00:16:33,860 --> 00:16:39,440
through this real day we will be able as

306
00:16:39,440 --> 00:16:45,200
the Working Party 29 level to assess

307
00:16:45,200 --> 00:16:49,880
finally if the shield is adequate or not

308
00:16:49,880 --> 00:16:53,460
so we are preparing this meeting very

309
00:16:53,460 --> 00:16:56,460
seriously with the all our American

310
00:16:56,460 --> 00:17:00,470
counterparts and we are because they are

311
00:17:00,470 --> 00:17:04,470
various procedures to set in particular

312
00:17:04,470 --> 00:17:08,339
the EU centralized body you may don't

313
00:17:08,339 --> 00:17:10,650
not know what is the EU centralized body

314
00:17:10,650 --> 00:17:14,339
it is the process the channel through

315
00:17:14,339 --> 00:17:17,579
which possible complaints on public

316
00:17:17,579 --> 00:17:18,940
security issues

317
00:17:18,940 --> 00:17:21,280
related to the shield should be conveyed

318
00:17:21,280 --> 00:17:24,579
from Europe to the United States and use

319
00:17:24,579 --> 00:17:27,310
know that this is one of the subject on

320
00:17:27,310 --> 00:17:30,400
which the American government has really

321
00:17:30,400 --> 00:17:34,270
innovated through the ombudsperson so we

322
00:17:34,270 --> 00:17:38,410
have to set this gu centralized body for

323
00:17:38,410 --> 00:17:41,470
the possible complaints to be simply

324
00:17:41,470 --> 00:17:45,220
conveyed to the US we also have to set

325
00:17:45,220 --> 00:17:49,030
the system for the commercial complaints

326
00:17:49,030 --> 00:17:51,850
if they exist so all these elements will

327
00:17:51,850 --> 00:17:54,880
allow us to have information to prepare

328
00:17:54,880 --> 00:17:57,670
the joint of view great either comments

329
00:17:57,670 --> 00:18:01,410
on this yes and let me sound a note of

330
00:18:01,410 --> 00:18:04,450
of optimism and cautious optimism but

331
00:18:04,450 --> 00:18:06,670
optimism nonetheless I can certainly

332
00:18:06,670 --> 00:18:08,560
tell you from the American perspective

333
00:18:08,560 --> 00:18:11,620
that the commitment to the privacy

334
00:18:11,620 --> 00:18:16,870
shield is absolutely real sincere and I

335
00:18:16,870 --> 00:18:18,850
can say this firsthand just simply from

336
00:18:18,850 --> 00:18:20,530
the significant effort that was

337
00:18:20,530 --> 00:18:22,240
undertaken in conjunction with the

338
00:18:22,240 --> 00:18:24,220
Department of Commerce and with our

339
00:18:24,220 --> 00:18:25,990
colleagues at the European Commission to

340
00:18:25,990 --> 00:18:28,600
negotiate what we believed is their

341
00:18:28,600 --> 00:18:30,310
robust what we believe is a robust

342
00:18:30,310 --> 00:18:34,330
framework that we will ultimately stand

343
00:18:34,330 --> 00:18:38,230
the test of time it was also conceived

344
00:18:38,230 --> 00:18:43,620
and negotiated as a forward-looking

345
00:18:43,620 --> 00:18:47,380
framework and also we set up

346
00:18:47,380 --> 00:18:49,960
intentionally this annual review process

347
00:18:49,960 --> 00:18:53,470
to address concerns that might arise but

348
00:18:53,470 --> 00:18:56,980
let me also say that the success of the

349
00:18:56,980 --> 00:19:00,660
privacy shield program does depend on

350
00:19:00,660 --> 00:19:04,680
collaboration and one very significant

351
00:19:04,680 --> 00:19:07,210
effort on the part certainly of the

352
00:19:07,210 --> 00:19:08,440
Federal Trade Commission when it comes

353
00:19:08,440 --> 00:19:10,990
to enforcement is the strengthening of

354
00:19:10,990 --> 00:19:15,460
the ties with European DPAs in order to

355
00:19:15,460 --> 00:19:18,030
ensure success to deepen that

356
00:19:18,030 --> 00:19:20,350
collaboration and that level of

357
00:19:20,350 --> 00:19:21,850
communication which I don't think had

358
00:19:21,850 --> 00:19:23,590
been in place in the past when it came

359
00:19:23,590 --> 00:19:25,570
to safe harbor and I think that we've

360
00:19:25,570 --> 00:19:27,130
made significant efforts to rectify it

361
00:19:27,130 --> 00:19:28,710
and I want to really acknowledge

362
00:19:28,710 --> 00:19:31,100
Isabelle in her work

363
00:19:31,100 --> 00:19:34,160
in her both capacity is a head of the

364
00:19:34,160 --> 00:19:36,049
article 29 working party but also have a

365
00:19:36,049 --> 00:19:39,320
cameo in this regard and I also know

366
00:19:39,320 --> 00:19:42,220
that we at the FTC look forward to

367
00:19:42,220 --> 00:19:44,570
continuing our engagement with with

368
00:19:44,570 --> 00:19:47,270
other GPA so I so I believe that it's a

369
00:19:47,270 --> 00:19:48,830
robust framework and I'm cautiously

370
00:19:48,830 --> 00:19:53,360
optimistic great mm-hmm Edith Isabelle

371
00:19:53,360 --> 00:19:57,289
Giovanni I think on behalf of everyone

372
00:19:57,289 --> 00:19:58,880
in the room here at the privacy

373
00:19:58,880 --> 00:20:01,580
profession and I PP thank you for your

374
00:20:01,580 --> 00:20:03,650
leadership and for your engagement with

375
00:20:03,650 --> 00:20:08,230
us the privacy profession thank you

