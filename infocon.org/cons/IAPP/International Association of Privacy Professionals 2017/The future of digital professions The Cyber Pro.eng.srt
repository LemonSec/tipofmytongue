1
00:00:00,390 --> 00:00:10,619
[Music]

2
00:00:14,330 --> 00:00:18,000
welcome back joining us to discuss the

3
00:00:18,000 --> 00:00:20,070
privacy and security professional of

4
00:00:20,070 --> 00:00:22,769
tomorrow please welcome to the stage

5
00:00:22,769 --> 00:00:26,520
moderator Ruby Zeppo trevor hughes and

6
00:00:26,520 --> 00:00:41,070
matt Loeb I'm just gonna dive right into

7
00:00:41,070 --> 00:00:42,899
it because we've got a short session so

8
00:00:42,899 --> 00:00:46,410
I EPP worked with Ohio State to release

9
00:00:46,410 --> 00:00:49,020
a recent research study the topic of

10
00:00:49,020 --> 00:00:51,899
which was the convergence of the privacy

11
00:00:51,899 --> 00:00:54,059
security and to some extent IT

12
00:00:54,059 --> 00:00:56,730
professional roles so my question right

13
00:00:56,730 --> 00:00:59,850
out of the gate is why is this happening

14
00:00:59,850 --> 00:01:01,559
and why is it happening now is it

15
00:01:01,559 --> 00:01:03,989
technology is it culture is it politics

16
00:01:03,989 --> 00:01:06,979
is it you know what what's causing it

17
00:01:06,979 --> 00:01:09,360
sure so I'd be happy to start I'm at

18
00:01:09,360 --> 00:01:12,330
it's great to see you hi Ruby great to

19
00:01:12,330 --> 00:01:16,950
be here so we worked with Ohio State to

20
00:01:16,950 --> 00:01:18,690
create this paper because we were seeing

21
00:01:18,690 --> 00:01:20,430
something happening in the marketplace

22
00:01:20,430 --> 00:01:23,850
we were seeing that the skills of the

23
00:01:23,850 --> 00:01:26,070
privacy professional born largely of

24
00:01:26,070 --> 00:01:28,680
long compliance traditionally we're

25
00:01:28,680 --> 00:01:31,560
bleeding into engineering and IT were

26
00:01:31,560 --> 00:01:34,770
bleeding into IT risk were bleeding into

27
00:01:34,770 --> 00:01:37,920
information security and vice versa and

28
00:01:37,920 --> 00:01:41,070
that solutions that were being created

29
00:01:41,070 --> 00:01:43,710
in those siloed domains we're no longer

30
00:01:43,710 --> 00:01:45,869
sufficient to fully answer the risks the

31
00:01:45,869 --> 00:01:48,060
challenges that were being faced across

32
00:01:48,060 --> 00:01:51,270
an organization and looking at that the

33
00:01:51,270 --> 00:01:53,909
board the senior leadership at the IEP P

34
00:01:53,909 --> 00:01:56,670
said we should start talking about how

35
00:01:56,670 --> 00:01:58,650
do we talk to those other professional

36
00:01:58,650 --> 00:02:01,170
domains how do we talk to IT risk how do

37
00:02:01,170 --> 00:02:02,490
we talk to information security

38
00:02:02,490 --> 00:02:04,350
professionals and what are the skills

39
00:02:04,350 --> 00:02:06,360
that we need clearly we cannot be

40
00:02:06,360 --> 00:02:10,098
masters of all domains we have to have

41
00:02:10,098 --> 00:02:13,019
fluency conversational skills across

42
00:02:13,019 --> 00:02:15,180
those spaces that led to some

43
00:02:15,180 --> 00:02:17,689
conversations between Matt and myself

44
00:02:17,689 --> 00:02:20,309
aisaka of course one of the leading

45
00:02:20,309 --> 00:02:21,040
organization

46
00:02:21,040 --> 00:02:22,150
in the world for certifying

47
00:02:22,150 --> 00:02:24,430
professionals in information security

48
00:02:24,430 --> 00:02:26,560
and IT risk and Matt can definitely tell

49
00:02:26,560 --> 00:02:28,930
us a bit more about that but the minute

50
00:02:28,930 --> 00:02:30,790
that I raised the idea with Matt that we

51
00:02:30,790 --> 00:02:32,409
were seeing something that immediately

52
00:02:32,409 --> 00:02:35,140
sprung back to me the idea that he was

53
00:02:35,140 --> 00:02:36,730
seeing something to that I Sokka was

54
00:02:36,730 --> 00:02:38,560
seeing something and that there was a

55
00:02:38,560 --> 00:02:41,109
hybridized future for us where

56
00:02:41,109 --> 00:02:44,319
professionals did again not need to be

57
00:02:44,319 --> 00:02:47,379
experts in every single domain but they

58
00:02:47,379 --> 00:02:49,599
needed to know enough so that they could

59
00:02:49,599 --> 00:02:51,639
maintain a conversation so now what do

60
00:02:51,639 --> 00:02:53,530
you think is causing it like what's what

61
00:02:53,530 --> 00:02:56,260
lies behind this sudden you know these

62
00:02:56,260 --> 00:02:57,849
fields have been around for a long time

63
00:02:57,849 --> 00:03:00,250
why is it suddenly seem at least to me

64
00:03:00,250 --> 00:03:02,889
that this has been a huge uptick in the

65
00:03:02,889 --> 00:03:04,480
need to have more collaborative

66
00:03:04,480 --> 00:03:06,549
solutions well I think there there's

67
00:03:06,549 --> 00:03:08,049
just two things that it really comes

68
00:03:08,049 --> 00:03:10,930
down to number one is the continuing

69
00:03:10,930 --> 00:03:13,810
pace of change it's getting faster

70
00:03:13,810 --> 00:03:17,139
around technology so technology is now

71
00:03:17,139 --> 00:03:19,900
embedded in everything that everybody

72
00:03:19,900 --> 00:03:23,049
every company every government every

73
00:03:23,049 --> 00:03:25,540
Enterprise is is involved in fact it's

74
00:03:25,540 --> 00:03:27,599
affecting our personal lives as well

75
00:03:27,599 --> 00:03:29,919
from the enterprise standpoint though

76
00:03:29,919 --> 00:03:32,440
what's also happening simultaneously is

77
00:03:32,440 --> 00:03:35,650
this increase in complexity around the

78
00:03:35,650 --> 00:03:38,970
the legal compliance and regulatory

79
00:03:38,970 --> 00:03:41,709
aspects of things and part of this is

80
00:03:41,709 --> 00:03:44,199
really being driven by the fact that

81
00:03:44,199 --> 00:03:47,709
technology is generally for good in fact

82
00:03:47,709 --> 00:03:52,180
at aisaka we we our purpose statement is

83
00:03:52,180 --> 00:03:53,590
to help you realize the positive

84
00:03:53,590 --> 00:03:56,049
potential of technology but with that

85
00:03:56,049 --> 00:03:59,530
comes the requirement of looking at the

86
00:03:59,530 --> 00:04:01,750
risk aspects that technology bring to

87
00:04:01,750 --> 00:04:05,799
the table and as a result the the focus

88
00:04:05,799 --> 00:04:08,500
on mitigating that risk is actually I

89
00:04:08,500 --> 00:04:11,319
think driving the issues around security

90
00:04:11,319 --> 00:04:13,780
and privacy and it's really on one big

91
00:04:13,780 --> 00:04:16,298
bundle and I think that puts a spotlight

92
00:04:16,298 --> 00:04:18,250
on what Trevor was talking about about

93
00:04:18,250 --> 00:04:21,759
the need for this cross-pollination of

94
00:04:21,759 --> 00:04:24,240
knowledge and and actually goes beyond

95
00:04:24,240 --> 00:04:27,009
privacy security and risk I mean our

96
00:04:27,009 --> 00:04:29,500
professionals also need to be I would

97
00:04:29,500 --> 00:04:31,419
say everybody in this room needs to

98
00:04:31,419 --> 00:04:32,490
start

99
00:04:32,490 --> 00:04:34,380
having a grasp of what are the

100
00:04:34,380 --> 00:04:36,210
technologies that are affecting us today

101
00:04:36,210 --> 00:04:38,990
and more importantly also being

102
00:04:38,990 --> 00:04:42,330
anticipating anticipating those new

103
00:04:42,330 --> 00:04:43,620
things that are going to come in and

104
00:04:43,620 --> 00:04:44,310
impact us

105
00:04:44,310 --> 00:04:46,860
let me just carry on from that I think

106
00:04:46,860 --> 00:04:49,169
in terms of causes it's the digital

107
00:04:49,169 --> 00:04:52,020
economy it's the rise of our digital

108
00:04:52,020 --> 00:04:54,030
lives and the technologies that are

109
00:04:54,030 --> 00:04:56,720
challenging us they bring with them

110
00:04:56,720 --> 00:04:59,400
traditional IT engineering InfoSec

111
00:04:59,400 --> 00:05:04,500
concerns and we are using tools from the

112
00:05:04,500 --> 00:05:07,020
legal and compliance world privacy laws

113
00:05:07,020 --> 00:05:10,080
many of which were really conceived of

114
00:05:10,080 --> 00:05:12,599
at least the frameworks 50 years ago are

115
00:05:12,599 --> 00:05:14,250
Fair Information practices that were

116
00:05:14,250 --> 00:05:16,440
applying to these new standards were

117
00:05:16,440 --> 00:05:19,320
really designed 40-plus almost 50 years

118
00:05:19,320 --> 00:05:21,810
ago in fact it is the 50th anniversary

119
00:05:21,810 --> 00:05:23,520
of the writing and privacy and freedom

120
00:05:23,520 --> 00:05:27,240
by ellen weston and that is I think a

121
00:05:27,240 --> 00:05:29,070
notable anniversary for us this year

122
00:05:29,070 --> 00:05:31,169
because that really was the first time

123
00:05:31,169 --> 00:05:34,530
we thought of this idea of control we're

124
00:05:34,530 --> 00:05:37,050
seeing some tension there and we need

125
00:05:37,050 --> 00:05:40,110
better and new solutions and part of

126
00:05:40,110 --> 00:05:43,259
that is people who have worked in public

127
00:05:43,259 --> 00:05:44,940
policy people have worked in long

128
00:05:44,940 --> 00:05:46,979
compliance understanding how these new

129
00:05:46,979 --> 00:05:49,409
technologies are creating new issues you

130
00:05:49,409 --> 00:05:51,870
said you don't have to be an expert and

131
00:05:51,870 --> 00:05:54,090
everything and we can't do everything so

132
00:05:54,090 --> 00:05:56,610
what's enough like how do we figure out

133
00:05:56,610 --> 00:05:58,500
how much is enough let's put a pin in

134
00:05:58,500 --> 00:06:00,659
today and put the future aside for a

135
00:06:00,659 --> 00:06:02,759
moment what should people be doing right

136
00:06:02,759 --> 00:06:04,560
now that's enough to give them the

137
00:06:04,560 --> 00:06:06,889
background they need to do a good job so

138
00:06:06,889 --> 00:06:11,400
I mean it is a tough challenge because

139
00:06:11,400 --> 00:06:14,750
generally we've come up through our

140
00:06:14,750 --> 00:06:16,590
educational backgrounds and our

141
00:06:16,590 --> 00:06:19,289
experience is focused on things that

142
00:06:19,289 --> 00:06:23,219
we've become really good at and and now

143
00:06:23,219 --> 00:06:25,650
what's happening is we're being in fact

144
00:06:25,650 --> 00:06:28,320
this is kind of a phenomenon that that

145
00:06:28,320 --> 00:06:31,710
has really plagued and the engineering

146
00:06:31,710 --> 00:06:33,659
community for a long time because now

147
00:06:33,659 --> 00:06:35,669
it's not just about electrical or

148
00:06:35,669 --> 00:06:38,639
mechanical or civil engineering now it's

149
00:06:38,639 --> 00:06:41,400
about engineering and you know we've

150
00:06:41,400 --> 00:06:43,280
added biologists to the mix and

151
00:06:43,280 --> 00:06:46,580
chemist to the mix and all of us and you

152
00:06:46,580 --> 00:06:47,750
know bring it back to our space we've

153
00:06:47,750 --> 00:06:50,600
all learned a language and one of the

154
00:06:50,600 --> 00:06:52,370
hardest challenges first is

155
00:06:52,370 --> 00:06:54,350
communications which is trying to

156
00:06:54,350 --> 00:06:56,630
understand the perspectives of others

157
00:06:56,630 --> 00:06:59,300
and the reason that this is important is

158
00:06:59,300 --> 00:07:01,610
that the problems were child or the

159
00:07:01,610 --> 00:07:03,200
challenges we're trying to address today

160
00:07:03,200 --> 00:07:07,010
are systems problems so they all fit

161
00:07:07,010 --> 00:07:09,350
together and part of our our challenge

162
00:07:09,350 --> 00:07:11,900
is to understand how changing one

163
00:07:11,900 --> 00:07:13,550
variable is going to affect the other

164
00:07:13,550 --> 00:07:17,060
and so in that it comes to the fact that

165
00:07:17,060 --> 00:07:19,880
you may have your core area of expertise

166
00:07:19,880 --> 00:07:22,400
but you're gonna have to be a voracious

167
00:07:22,400 --> 00:07:26,120
reader studier listener to other people

168
00:07:26,120 --> 00:07:30,050
too and to capture that knowledge and

169
00:07:30,050 --> 00:07:33,169
make it part of fitting into your think

170
00:07:33,169 --> 00:07:37,250
thought process and and it means that we

171
00:07:37,250 --> 00:07:39,229
have to be broader we can have our

172
00:07:39,229 --> 00:07:40,760
specialty but we really have to think

173
00:07:40,760 --> 00:07:43,250
more broadly across all of the aspects

174
00:07:43,250 --> 00:07:46,190
of the things that are that we're being

175
00:07:46,190 --> 00:07:48,050
asked to deal with I think Matt's

176
00:07:48,050 --> 00:07:50,300
exactly right and I want to restate one

177
00:07:50,300 --> 00:07:51,560
of the points he made I think the

178
00:07:51,560 --> 00:07:53,630
privacy probe today the InfoSec probe

179
00:07:53,630 --> 00:07:56,419
today and the cyber probe the future

180
00:07:56,419 --> 00:07:58,760
need the cyber pro of the future needs

181
00:07:58,760 --> 00:08:01,520
to be a perennial student you always

182
00:08:01,520 --> 00:08:03,950
have to be learning more because

183
00:08:03,950 --> 00:08:05,780
technology and innovation is moving so

184
00:08:05,780 --> 00:08:07,729
quickly it's really difficult to keep up

185
00:08:07,729 --> 00:08:10,850
with Ruby your question though was when

186
00:08:10,850 --> 00:08:12,979
what what is enough and what is the

187
00:08:12,979 --> 00:08:14,810
amount of knowledge that you need to

188
00:08:14,810 --> 00:08:17,030
learn as you cross pollinate across

189
00:08:17,030 --> 00:08:18,830
these different domains and I think the

190
00:08:18,830 --> 00:08:20,479
answer is like so many things in our

191
00:08:20,479 --> 00:08:22,310
field it depends it depends on the role

192
00:08:22,310 --> 00:08:24,200
you're in the organization in your in

193
00:08:24,200 --> 00:08:27,200
the type of risks that you're facing you

194
00:08:27,200 --> 00:08:29,630
know I came up through a privacy role in

195
00:08:29,630 --> 00:08:32,360
an online advertising company back in 99

196
00:08:32,360 --> 00:08:35,570
2000 and I had to know as a lawyer how

197
00:08:35,570 --> 00:08:37,849
cookies worked back then I would argue

198
00:08:37,849 --> 00:08:39,469
that anyone that works in an online

199
00:08:39,469 --> 00:08:42,429
space today needs to understand how the

200
00:08:42,429 --> 00:08:44,510
technology of state management the

201
00:08:44,510 --> 00:08:46,280
technology of things like cookies work

202
00:08:46,280 --> 00:08:47,900
in order for them to understand the

203
00:08:47,900 --> 00:08:49,160
privacy controls that sit inside

204
00:08:49,160 --> 00:08:51,320
browsers the things that are coming in

205
00:08:51,320 --> 00:08:53,900
the e privacy directive in Europe but

206
00:08:53,900 --> 00:08:56,160
that's just one small slice whatever

207
00:08:56,160 --> 00:08:58,740
it is for your organization maybe you're

208
00:08:58,740 --> 00:09:01,079
in life sciences working on the human

209
00:09:01,079 --> 00:09:03,420
genome or DNA and you need to understand

210
00:09:03,420 --> 00:09:04,829
some of the privacy implications that

211
00:09:04,829 --> 00:09:08,550
may emerge from using the human genome

212
00:09:08,550 --> 00:09:11,009
and that is going to require a

213
00:09:11,009 --> 00:09:13,800
sophistication around what DNA can tell

214
00:09:13,800 --> 00:09:16,019
us and what providing results to someone

215
00:09:16,019 --> 00:09:20,160
might de close about someone else so it

216
00:09:20,160 --> 00:09:22,350
is role based and it's situational its

217
00:09:22,350 --> 00:09:25,019
contextual but the thing that I know is

218
00:09:25,019 --> 00:09:28,259
that it is it is multi-denominational it

219
00:09:28,259 --> 00:09:32,129
reaches across other domains so a law

220
00:09:32,129 --> 00:09:34,649
student graduating today not for one

221
00:09:34,649 --> 00:09:36,689
second should think that their education

222
00:09:36,689 --> 00:09:38,759
is sufficient for them to be a fully

223
00:09:38,759 --> 00:09:40,649
functioning and successful privacy

224
00:09:40,649 --> 00:09:43,170
lawyer going forward they got to learn a

225
00:09:43,170 --> 00:09:45,899
lot more so what's interesting about

226
00:09:45,899 --> 00:09:47,730
what you were just saying too is that

227
00:09:47,730 --> 00:09:50,339
when you know aisaka spends a lot more

228
00:09:50,339 --> 00:09:52,529
time working with the information

229
00:09:52,529 --> 00:09:54,449
security community and the the people

230
00:09:54,449 --> 00:09:56,220
who are on the operational side and what

231
00:09:56,220 --> 00:09:58,949
we're finding is that the traditional

232
00:09:58,949 --> 00:10:03,000
paths of Education are becoming not

233
00:10:03,000 --> 00:10:05,730
irrelevant but less and less important

234
00:10:05,730 --> 00:10:08,459
to some of these roles because for

235
00:10:08,459 --> 00:10:12,300
instance our lead in security at aisaka

236
00:10:12,300 --> 00:10:15,420
was an English major it just happened to

237
00:10:15,420 --> 00:10:18,569
fall into the technology side worked for

238
00:10:18,569 --> 00:10:21,089
NSA and a few other government agencies

239
00:10:21,089 --> 00:10:23,519
on things he can't talk about and he

240
00:10:23,519 --> 00:10:24,600
went back and got a master's in

241
00:10:24,600 --> 00:10:27,410
international business the thing that's

242
00:10:27,410 --> 00:10:29,819
interesting when you talk to him though

243
00:10:29,819 --> 00:10:32,370
is that what you realize is that he has

244
00:10:32,370 --> 00:10:35,069
technology aptitude so there is that

245
00:10:35,069 --> 00:10:38,009
thirst for learning but the the other

246
00:10:38,009 --> 00:10:40,519
aspect that brings into it is that

247
00:10:40,519 --> 00:10:43,889
whether it's security or privacy when

248
00:10:43,889 --> 00:10:45,329
you're looking at your enterprise

249
00:10:45,329 --> 00:10:47,069
there's this question of do you

250
00:10:47,069 --> 00:10:48,630
understand the business that you're in

251
00:10:48,630 --> 00:10:51,689
right because even if you're an

252
00:10:51,689 --> 00:10:54,269
operational security person when a

253
00:10:54,269 --> 00:10:56,970
breach happens if you don't understand

254
00:10:56,970 --> 00:10:59,009
the construct of how your business works

255
00:10:59,009 --> 00:11:01,110
if you don't understand the architecture

256
00:11:01,110 --> 00:11:03,980
of the technology system that's in place

257
00:11:03,980 --> 00:11:06,779
it makes it very difficult to actually

258
00:11:06,779 --> 00:11:07,980
take the correct

259
00:11:07,980 --> 00:11:12,050
actions so this is just it's really

260
00:11:12,050 --> 00:11:14,250
complementary to the story you were you

261
00:11:14,250 --> 00:11:16,440
were talking about the privacy of route

262
00:11:16,440 --> 00:11:18,990
that you type and and it does raise the

263
00:11:18,990 --> 00:11:22,080
point that academic institutions around

264
00:11:22,080 --> 00:11:24,330
the world really are still teaching in

265
00:11:24,330 --> 00:11:26,970
those silo domains I came up through

266
00:11:26,970 --> 00:11:29,430
political science and you know we did

267
00:11:29,430 --> 00:11:31,470
not venture into the science buildings

268
00:11:31,470 --> 00:11:34,880
if we have if we had any power not to

269
00:11:34,880 --> 00:11:38,240
except for like the one or two mandatory

270
00:11:38,240 --> 00:11:40,320
science credits that we had to get in

271
00:11:40,320 --> 00:11:42,540
order to graduate we've got to change

272
00:11:42,540 --> 00:11:46,110
that dynamic you know we need humanities

273
00:11:46,110 --> 00:11:47,820
graduates to understand that they need

274
00:11:47,820 --> 00:11:50,070
to understand how technology affects

275
00:11:50,070 --> 00:11:52,770
Society and in order to do that they

276
00:11:52,770 --> 00:11:54,120
under have to understand a little bit

277
00:11:54,120 --> 00:11:57,800
about technology and vice versa I think

278
00:11:57,800 --> 00:12:01,290
exactly exactly and so computer science

279
00:12:01,290 --> 00:12:03,120
graduates need to understand a little

280
00:12:03,120 --> 00:12:05,100
bit about ethics and a little bit about

281
00:12:05,100 --> 00:12:12,060
privacy right we can pick any of those

282
00:12:12,060 --> 00:12:14,690
disciplines I mean you know in biology

283
00:12:14,690 --> 00:12:17,040
and there's probably people here in this

284
00:12:17,040 --> 00:12:19,020
audience who are working for pharma

285
00:12:19,020 --> 00:12:20,520
companies and they probably know more

286
00:12:20,520 --> 00:12:21,900
about what I'm going to mention but I

287
00:12:21,900 --> 00:12:24,480
mean there's this system called CRISPR

288
00:12:24,480 --> 00:12:28,610
which actually allows you to look at

289
00:12:28,610 --> 00:12:31,530
that's right and and then when you think

290
00:12:31,530 --> 00:12:33,870
about the potential of first of all

291
00:12:33,870 --> 00:12:35,610
you're cataloging this you're comparing

292
00:12:35,610 --> 00:12:37,890
it to two other individuals and then you

293
00:12:37,890 --> 00:12:40,170
think about customized medicine but now

294
00:12:40,170 --> 00:12:42,390
you start layering on top of that the

295
00:12:42,390 --> 00:12:44,130
people who are doing that work if

296
00:12:44,130 --> 00:12:47,010
they're not thinking about privacy if

297
00:12:47,010 --> 00:12:49,170
they're not thinking about security if

298
00:12:49,170 --> 00:12:51,000
they're not thinking about social

299
00:12:51,000 --> 00:12:53,730
implications and ethical implications of

300
00:12:53,730 --> 00:12:58,110
the innovations that are happening so I

301
00:12:58,110 --> 00:13:01,590
think all of these things are it's kind

302
00:13:01,590 --> 00:13:04,320
of like this big massive stuff that's

303
00:13:04,320 --> 00:13:05,910
coming and you have to just kind of

304
00:13:05,910 --> 00:13:07,980
figure out how do you cut through all of

305
00:13:07,980 --> 00:13:10,290
it to make sense out of it all so let's

306
00:13:10,290 --> 00:13:11,880
talk about that pre with you that things

307
00:13:11,880 --> 00:13:13,560
are contextual and by the way a lot of

308
00:13:13,560 --> 00:13:15,660
us just fell fell right into our careers

309
00:13:15,660 --> 00:13:18,510
right so but now we're talking about

310
00:13:18,510 --> 00:13:20,640
particular fields where it may be more

311
00:13:20,640 --> 00:13:21,000
important

312
00:13:21,000 --> 00:13:23,310
is there a particular business type or

313
00:13:23,310 --> 00:13:26,640
field where this you know convergence is

314
00:13:26,640 --> 00:13:28,920
most important that we get right now and

315
00:13:28,920 --> 00:13:31,280
should be prioritized over others

316
00:13:31,280 --> 00:13:37,290
industries I mean when when you've got a

317
00:13:37,290 --> 00:13:39,360
change you try to go after the lowest

318
00:13:39,360 --> 00:13:40,830
hanging fruit and if it works you use

319
00:13:40,830 --> 00:13:42,180
that as a model as it trickles down

320
00:13:42,180 --> 00:13:44,370
right so I'm looking what's the lowest

321
00:13:44,370 --> 00:13:46,050
hanging fruit here for us to start

322
00:13:46,050 --> 00:13:47,970
saying that needs the convergence now

323
00:13:47,970 --> 00:13:53,940
yeah yeah and that's my answer to is is

324
00:13:53,940 --> 00:13:57,480
that the the ubiquity of technological

325
00:13:57,480 --> 00:14:01,200
innovation is so complete so many

326
00:14:01,200 --> 00:14:02,540
organizations are data-driven

327
00:14:02,540 --> 00:14:04,470
organizations today so many

328
00:14:04,470 --> 00:14:06,380
organizations are technology driven

329
00:14:06,380 --> 00:14:08,520
organizations when I think of just the

330
00:14:08,520 --> 00:14:10,770
small ia pp and we are a very small

331
00:14:10,770 --> 00:14:12,900
organization the number of technologies

332
00:14:12,900 --> 00:14:15,870
that we use is dizzying and our need to

333
00:14:15,870 --> 00:14:17,820
understand how our members data is being

334
00:14:17,820 --> 00:14:19,890
used I mean that's it that's a tall task

335
00:14:19,890 --> 00:14:23,040
for a small organization so I don't

336
00:14:23,040 --> 00:14:26,070
think Ruby that there is sort of the

337
00:14:26,070 --> 00:14:28,530
early adopter of this convergence idea

338
00:14:28,530 --> 00:14:30,150
the cyber pro of the future is not going

339
00:14:30,150 --> 00:14:32,640
to emerge in one place first I actually

340
00:14:32,640 --> 00:14:34,350
would think we are seeing that

341
00:14:34,350 --> 00:14:36,180
development across the global

342
00:14:36,180 --> 00:14:38,580
marketplace in many many organizations

343
00:14:38,580 --> 00:14:41,160
and I can tell you I shared yesterday

344
00:14:41,160 --> 00:14:44,250
we're just about at 33,000 members we

345
00:14:44,250 --> 00:14:46,260
started the year 25,000 members we've

346
00:14:46,260 --> 00:14:50,010
added 8,000 members this year there is a

347
00:14:50,010 --> 00:14:53,250
real resource strain in the marketplace

348
00:14:53,250 --> 00:14:55,620
privacy professionals people with these

349
00:14:55,620 --> 00:14:59,130
hybridized skills they are in absolute

350
00:14:59,130 --> 00:15:02,100
hot and high demand it is a tough market

351
00:15:02,100 --> 00:15:04,589
to find those kinds of people and it

352
00:15:04,589 --> 00:15:07,640
really raises the importance of

353
00:15:07,640 --> 00:15:10,110
cross-functional training you know so

354
00:15:10,110 --> 00:15:12,510
for all of the traditional privacy pros

355
00:15:12,510 --> 00:15:14,880
in the audience going out and getting

356
00:15:14,880 --> 00:15:17,640
some InfoSec experience getting maybe

357
00:15:17,640 --> 00:15:19,920
not experience but training doing

358
00:15:19,920 --> 00:15:22,110
programs from groups like I saw that's

359
00:15:22,110 --> 00:15:24,420
only gonna make you a much more valuable

360
00:15:24,420 --> 00:15:26,220
asset inside your organization and

361
00:15:26,220 --> 00:15:28,950
improve the value of your contributions

362
00:15:28,950 --> 00:15:31,700
to your organization and conversely

363
00:15:31,700 --> 00:15:33,870
information security professionals IT

364
00:15:33,870 --> 00:15:34,780
risk

365
00:15:34,780 --> 00:15:36,040
um even business management

366
00:15:36,040 --> 00:15:38,260
professionals understanding enough about

367
00:15:38,260 --> 00:15:40,630
privacy so that you've got that spidey

368
00:15:40,630 --> 00:15:43,630
sense when a really dumb privacy

369
00:15:43,630 --> 00:15:45,400
decision gets put up on a whiteboard you

370
00:15:45,400 --> 00:15:47,590
can say hey guys that doesn't seem right

371
00:15:47,590 --> 00:15:49,720
why don't we run that past Ruby in the

372
00:15:49,720 --> 00:15:52,180
privacy team or let's let's run that up

373
00:15:52,180 --> 00:15:53,770
to the privacy group and see what

374
00:15:53,770 --> 00:15:56,140
happens we need that kind of

375
00:15:56,140 --> 00:15:58,210
sophistication inside organizations so I

376
00:15:58,210 --> 00:15:59,560
mean two things you were really hitting

377
00:15:59,560 --> 00:16:01,450
on there working in reverse order I mean

378
00:16:01,450 --> 00:16:03,340
the last thing again comes back to this

379
00:16:03,340 --> 00:16:05,410
whole notion of systems thinking right

380
00:16:05,410 --> 00:16:08,070
so that when you're doing your work

381
00:16:08,070 --> 00:16:11,350
being able to say okay what else is this

382
00:16:11,350 --> 00:16:15,040
going to impact right so if you're if

383
00:16:15,040 --> 00:16:17,860
you're an information security you have

384
00:16:17,860 --> 00:16:19,630
to be thinking about the privacy

385
00:16:19,630 --> 00:16:24,370
implications that's actually becoming

386
00:16:24,370 --> 00:16:26,980
more of a second nature now but but the

387
00:16:26,980 --> 00:16:28,390
other point that you were talking about

388
00:16:28,390 --> 00:16:33,550
was you know we don't know none of my

389
00:16:33,550 --> 00:16:35,500
crystal ball with me here but we don't

390
00:16:35,500 --> 00:16:38,320
know how the markets are going to evolve

391
00:16:38,320 --> 00:16:40,330
in terms of how the work that we're all

392
00:16:40,330 --> 00:16:42,670
engaged in today and how that's going to

393
00:16:42,670 --> 00:16:48,550
change so I suck as a community of a

394
00:16:48,550 --> 00:16:51,340
hundred and sixty thousand business

395
00:16:51,340 --> 00:16:54,460
technology professionals a large number

396
00:16:54,460 --> 00:16:58,060
of the people who have been been engaged

397
00:16:58,060 --> 00:16:59,740
in our organization started in the area

398
00:16:59,740 --> 00:17:03,430
of information systems audit you know

399
00:17:03,430 --> 00:17:06,099
just assurance that the systems that

400
00:17:06,099 --> 00:17:08,589
we're using are going to work properly

401
00:17:08,589 --> 00:17:10,690
that the data in is the data out I mean

402
00:17:10,690 --> 00:17:13,599
I'm oversimplifying it but but what

403
00:17:13,599 --> 00:17:16,960
we're seeing now is that if you look at

404
00:17:16,960 --> 00:17:19,950
the makeup of information security

405
00:17:19,950 --> 00:17:22,839
departments now most of them have

406
00:17:22,839 --> 00:17:25,240
already come with a nice acha credential

407
00:17:25,240 --> 00:17:28,410
and information systems audio and now

408
00:17:28,410 --> 00:17:31,210
we're also seeing as some of the people

409
00:17:31,210 --> 00:17:33,190
who have started out and information

410
00:17:33,190 --> 00:17:35,650
systems audit are looking at operational

411
00:17:35,650 --> 00:17:37,660
security because that's a place where

412
00:17:37,660 --> 00:17:39,910
there's a huge gap you know who are the

413
00:17:39,910 --> 00:17:42,680
people that we're trusting to monitor

414
00:17:42,680 --> 00:17:45,500
the perimeters of our systems and more

415
00:17:45,500 --> 00:17:47,300
importantly who are the people that we

416
00:17:47,300 --> 00:17:49,730
will want to count on when the breach

417
00:17:49,730 --> 00:17:52,940
happens because none of us are our

418
00:17:52,940 --> 00:17:56,120
bullet-proof from from that regard so

419
00:17:56,120 --> 00:17:58,640
we're seeing migration already there and

420
00:17:58,640 --> 00:18:01,010
and I think it's worth mentioning that

421
00:18:01,010 --> 00:18:04,490
maybe if we're having trouble finding

422
00:18:04,490 --> 00:18:07,280
the right people that we also need to

423
00:18:07,280 --> 00:18:10,309
think about our expectations of what

424
00:18:10,309 --> 00:18:12,860
we're of who we're hiring in the

425
00:18:12,860 --> 00:18:14,840
security space in particular every

426
00:18:14,840 --> 00:18:16,910
company wants somebody with credentials

427
00:18:16,910 --> 00:18:18,950
and five years of experience it doesn't

428
00:18:18,950 --> 00:18:22,070
exist and so one of the positions that

429
00:18:22,070 --> 00:18:23,630
we've taken is that we need to think

430
00:18:23,630 --> 00:18:26,420
about apprenticeship and the fact that

431
00:18:26,420 --> 00:18:29,179
anybody with technical technical with

432
00:18:29,179 --> 00:18:31,070
technology aptitude can actually learn

433
00:18:31,070 --> 00:18:33,110
how to do this work as long as you've

434
00:18:33,110 --> 00:18:34,850
teamed them with somebody who is really

435
00:18:34,850 --> 00:18:37,550
good at it so there are sources in the

436
00:18:37,550 --> 00:18:40,130
security space we talk about people who

437
00:18:40,130 --> 00:18:41,750
are of who are completing their armed

438
00:18:41,750 --> 00:18:45,500
services careers people who might be

439
00:18:45,500 --> 00:18:47,840
looking to do something different in

440
00:18:47,840 --> 00:18:49,670
fact our friends in the UK spent a

441
00:18:49,670 --> 00:18:54,080
million dollars doing a test they put 55

442
00:18:54,080 --> 00:18:56,179
people from non-traditional experiences

443
00:18:56,179 --> 00:18:58,910
in a room in a 10-week program

444
00:18:58,910 --> 00:19:01,130
bartenders barbers mortician

445
00:19:01,130 --> 00:19:04,190
psychiatrists and taught them how to do

446
00:19:04,190 --> 00:19:06,970
security work 27 of them were employed

447
00:19:06,970 --> 00:19:09,230
literally within a month after the

448
00:19:09,230 --> 00:19:12,950
program great so ask the more difficult

449
00:19:12,950 --> 00:19:15,740
question convergence isn't a hundred

450
00:19:15,740 --> 00:19:17,870
percent I can tell you I've worked with

451
00:19:17,870 --> 00:19:19,640
difficult professionals on both sides

452
00:19:19,640 --> 00:19:22,520
who get entrenched one example I think I

453
00:19:22,520 --> 00:19:24,470
still see a lot I'm just by way of

454
00:19:24,470 --> 00:19:28,160
example is employee monitoring why

455
00:19:28,160 --> 00:19:30,860
wouldn't the CEO have a giant pissy fit

456
00:19:30,860 --> 00:19:33,679
about protecting the IP right its job

457
00:19:33,679 --> 00:19:35,450
one like don't let the IP out I don't

458
00:19:35,450 --> 00:19:37,220
care if it's a negligent employee a

459
00:19:37,220 --> 00:19:39,110
reckless one or a you know angry

460
00:19:39,110 --> 00:19:41,809
intentional one do what you have to to

461
00:19:41,809 --> 00:19:43,309
you know make sure the employees aren't

462
00:19:43,309 --> 00:19:46,130
losing our IP and then you know the

463
00:19:46,130 --> 00:19:47,750
privacy professional comes along and

464
00:19:47,750 --> 00:19:50,690
says well we probably shouldn't put CCTV

465
00:19:50,690 --> 00:19:52,400
cameras behind every potted plant just

466
00:19:52,400 --> 00:19:54,260
because Harvey Weinstein entered the

467
00:19:54,260 --> 00:19:56,400
building so

468
00:19:56,400 --> 00:20:01,200
come on it's not too soon and and so I

469
00:20:01,200 --> 00:20:03,540
have these kinds of comment I I do both

470
00:20:03,540 --> 00:20:04,530
so I'm in the middle of these

471
00:20:04,530 --> 00:20:06,660
conversations a lot and I understand

472
00:20:06,660 --> 00:20:09,630
both sides how and and then they get

473
00:20:09,630 --> 00:20:11,760
polarized it's an us or them right right

474
00:20:11,760 --> 00:20:15,510
how do you create a stake for each side

475
00:20:15,510 --> 00:20:17,580
now in the game so that they're

476
00:20:17,580 --> 00:20:19,410
collaborating on a solution that isn't

477
00:20:19,410 --> 00:20:21,300
the loudest one wins that isn't the

478
00:20:21,300 --> 00:20:23,460
biggest budget wins but the company wins

479
00:20:23,460 --> 00:20:27,420
sure so it's a great question and it is

480
00:20:27,420 --> 00:20:30,330
a classic tension and I think the answer

481
00:20:30,330 --> 00:20:33,150
is the answer and that is those two

482
00:20:33,150 --> 00:20:34,800
sides need to understand more about the

483
00:20:34,800 --> 00:20:36,300
interests and the positions of the other

484
00:20:36,300 --> 00:20:38,550
side and we only get that point we get

485
00:20:38,550 --> 00:20:40,680
to that understanding when they have a

486
00:20:40,680 --> 00:20:42,420
bit of fluency and a bit of experience

487
00:20:42,420 --> 00:20:45,300
on the other side of the fence so for a

488
00:20:45,300 --> 00:20:47,940
privacy professional who may be driven

489
00:20:47,940 --> 00:20:49,620
by a strong interest in protecting

490
00:20:49,620 --> 00:20:52,500
employee privacy understanding that

491
00:20:52,500 --> 00:20:55,710
competing interest in IP security within

492
00:20:55,710 --> 00:20:57,840
the organization or just even physical

493
00:20:57,840 --> 00:21:00,060
security within the organization that

494
00:21:00,060 --> 00:21:02,490
demands that there are CCTV cameras in

495
00:21:02,490 --> 00:21:05,640
certain places the privacy Pro needs to

496
00:21:05,640 --> 00:21:09,390
put into their algorithm of what is an

497
00:21:09,390 --> 00:21:12,120
appropriate response in terms of privacy

498
00:21:12,120 --> 00:21:14,370
for that situation those interests as

499
00:21:14,370 --> 00:21:17,520
well so too does the security Pro need

500
00:21:17,520 --> 00:21:19,860
to understand that yeah there probably

501
00:21:19,860 --> 00:21:21,810
doesn't need to be a camera behind every

502
00:21:21,810 --> 00:21:23,700
potted plant where are your interests

503
00:21:23,700 --> 00:21:25,650
how can we focus on them how can we

504
00:21:25,650 --> 00:21:27,270
resolve them in the least privacy

505
00:21:27,270 --> 00:21:29,460
intrusive way possible when we do have

506
00:21:29,460 --> 00:21:31,710
to implement things like CCTV cameras or

507
00:21:31,710 --> 00:21:33,540
email monitoring or whatever whatever it

508
00:21:33,540 --> 00:21:35,700
is from an employee monitoring

509
00:21:35,700 --> 00:21:37,860
perspective how can we make sure that

510
00:21:37,860 --> 00:21:41,150
employees are aware of it that they are

511
00:21:41,150 --> 00:21:43,530
recognising when it's being used that

512
00:21:43,530 --> 00:21:45,390
they have the ability to raise concerns

513
00:21:45,390 --> 00:21:47,940
about it how do you engage a dialogue

514
00:21:47,940 --> 00:21:49,740
between the privacy and security

515
00:21:49,740 --> 00:21:54,210
functions when we have both of those

516
00:21:54,210 --> 00:21:56,760
sides we're recognizing the legitimacy

517
00:21:56,760 --> 00:21:59,940
of each other's interests then you have

518
00:21:59,940 --> 00:22:02,550
a powerful dialogue and I don't know

519
00:22:02,550 --> 00:22:04,470
what the ultimate answer ever will be I

520
00:22:04,470 --> 00:22:05,640
think that's something that needs to be

521
00:22:05,640 --> 00:22:06,750
worked through for every organization

522
00:22:06,750 --> 00:22:09,870
and sometimes the CEO may say you know

523
00:22:09,870 --> 00:22:10,620
what I

524
00:22:10,620 --> 00:22:13,140
P that's the crown jewels for our shop

525
00:22:13,140 --> 00:22:15,660
and every potted plant gets a CCTV

526
00:22:15,660 --> 00:22:18,450
camera . that might be the answer might

527
00:22:18,450 --> 00:22:19,770
not be the right answer for other

528
00:22:19,770 --> 00:22:21,690
organizations and it's a dialogue that

529
00:22:21,690 --> 00:22:23,250
has to happen between the privacy and

530
00:22:23,250 --> 00:22:24,690
security presence so Matt do you have

531
00:22:24,690 --> 00:22:27,230
any privacy and security whisperer

532
00:22:27,230 --> 00:22:29,910
techniques where you are able to plant

533
00:22:29,910 --> 00:22:34,380
the bug successfully in a gentle way you

534
00:22:34,380 --> 00:22:37,559
know I think Trevor makes one point

535
00:22:37,559 --> 00:22:39,480
that's really important again and we're

536
00:22:39,480 --> 00:22:41,250
coming back to you know how do we all

537
00:22:41,250 --> 00:22:43,800
communicate we all get along and I think

538
00:22:43,800 --> 00:22:44,970
we really need to be thinking about

539
00:22:44,970 --> 00:22:47,760
walking in the other shoes and I'll come

540
00:22:47,760 --> 00:22:50,160
back to your original question you know

541
00:22:50,160 --> 00:22:52,650
when you start thinking about this right

542
00:22:52,650 --> 00:22:55,230
we can talk today about security and

543
00:22:55,230 --> 00:22:56,910
privacy professionals talking more

544
00:22:56,910 --> 00:22:58,830
together but security and privacy

545
00:22:58,830 --> 00:23:00,780
professionals also have to talk to a

546
00:23:00,780 --> 00:23:02,309
whole lot of other people in the

547
00:23:02,309 --> 00:23:06,179
organization and it all to me is coming

548
00:23:06,179 --> 00:23:08,760
up too and this may be reflective of my

549
00:23:08,760 --> 00:23:11,010
sociology background but I mean it's all

550
00:23:11,010 --> 00:23:13,080
about culture the cultures of

551
00:23:13,080 --> 00:23:17,670
organizations are changing and and a lot

552
00:23:17,670 --> 00:23:20,880
of what we're dealing with today is it

553
00:23:20,880 --> 00:23:23,460
is new because when we talk about

554
00:23:23,460 --> 00:23:28,440
security and privacy and risk the the

555
00:23:28,440 --> 00:23:30,690
biggest vulnerability and all of that is

556
00:23:30,690 --> 00:23:35,100
people and so in order for us to be

557
00:23:35,100 --> 00:23:36,950
effective as security and privacy

558
00:23:36,950 --> 00:23:39,630
professionals we have to understand the

559
00:23:39,630 --> 00:23:42,420
people side and we have to be thinking

560
00:23:42,420 --> 00:23:44,309
about the implications of the things

561
00:23:44,309 --> 00:23:46,020
that we're doing in the stands that were

562
00:23:46,020 --> 00:23:48,690
taking and being able to articulate that

563
00:23:48,690 --> 00:23:50,670
to people who may not be as close to

564
00:23:50,670 --> 00:23:52,500
what we're doing but we have to do it

565
00:23:52,500 --> 00:23:53,820
with a lot of empathy and understanding

566
00:23:53,820 --> 00:23:58,290
to what their situation is I also think

567
00:23:58,290 --> 00:24:00,330
back to that how do you create that

568
00:24:00,330 --> 00:24:02,580
stake for everybody

569
00:24:02,580 --> 00:24:06,210
I will take a risk myself and say first

570
00:24:06,210 --> 00:24:07,650
of all how many people in this room were

571
00:24:07,650 --> 00:24:11,520
affected by the equifax if I can't

572
00:24:11,520 --> 00:24:12,929
believe every hand in this audience

573
00:24:12,929 --> 00:24:15,570
doesn't go up because 143 million I mean

574
00:24:15,570 --> 00:24:16,390
that

575
00:24:16,390 --> 00:24:18,850
you know probably 75% of the adults in

576
00:24:18,850 --> 00:24:21,760
the United States that could have been

577
00:24:21,760 --> 00:24:25,960
avoided by a simple patch a simple patch

578
00:24:25,960 --> 00:24:28,030
so I don't want to dwell on that issue

579
00:24:28,030 --> 00:24:30,610
but I think it is illustrative of

580
00:24:30,610 --> 00:24:33,070
creating the stake that we're all in it

581
00:24:33,070 --> 00:24:36,160
together and the state to create in

582
00:24:36,160 --> 00:24:39,000
terms of how we build this culture of

583
00:24:39,000 --> 00:24:42,160
not only sharing our expertise but also

584
00:24:42,160 --> 00:24:44,289
having an understanding of what our

585
00:24:44,289 --> 00:24:47,500
fellow colleagues are doing and

586
00:24:47,500 --> 00:24:49,030
understanding the nature of the business

587
00:24:49,030 --> 00:24:51,610
it's a system if there's a hole in the

588
00:24:51,610 --> 00:24:55,750
system we can lose and so I think that

589
00:24:55,750 --> 00:24:59,409
just puts you know means that we all

590
00:24:59,409 --> 00:25:02,679
have to double down on building more

591
00:25:02,679 --> 00:25:04,750
empathy into what we do making sure

592
00:25:04,750 --> 00:25:06,429
we're taking time to explain I know the

593
00:25:06,429 --> 00:25:08,110
world's moving very fast but we have to

594
00:25:08,110 --> 00:25:10,419
find ways to do this in a way that we

595
00:25:10,419 --> 00:25:13,870
understand what others are challenged

596
00:25:13,870 --> 00:25:15,669
with so that we are all working together

597
00:25:15,669 --> 00:25:17,320
to make sure that we help our

598
00:25:17,320 --> 00:25:19,179
enterprises achieve the outcomes that

599
00:25:19,179 --> 00:25:21,880
they're trying to get to okay I think we

600
00:25:21,880 --> 00:25:23,559
have time for one more question and I

601
00:25:23,559 --> 00:25:25,809
like to kind of go back to the keynote

602
00:25:25,809 --> 00:25:27,400
last night the one we're gonna see in a

603
00:25:27,400 --> 00:25:29,340
few minutes because I think the

604
00:25:29,340 --> 00:25:32,110
juxtaposition of their books brings up a

605
00:25:32,110 --> 00:25:33,820
very nice representation of how people

606
00:25:33,820 --> 00:25:35,320
feel about their privacy and security

607
00:25:35,320 --> 00:25:37,809
rights as individuals so you have last

608
00:25:37,809 --> 00:25:40,330
night how we need to belong to tribes

609
00:25:40,330 --> 00:25:42,250
and when people have their tribe taken

610
00:25:42,250 --> 00:25:44,770
away they suffer greatly and you know

611
00:25:44,770 --> 00:25:46,750
the community's gone and then we'll hear

612
00:25:46,750 --> 00:25:49,000
in a few minutes about a man who

613
00:25:49,000 --> 00:25:51,059
probably valued his privacy and

614
00:25:51,059 --> 00:25:53,770
aloneness more than anything else and

615
00:25:53,770 --> 00:25:55,630
was willing to potentially die for it

616
00:25:55,630 --> 00:25:57,940
and I think that's really the continuum

617
00:25:57,940 --> 00:26:00,490
we face in terms of how each individual

618
00:26:00,490 --> 00:26:02,409
views their privacy and security right

619
00:26:02,409 --> 00:26:04,780
so how can privacy and security

620
00:26:04,780 --> 00:26:06,640
individuals collaborate better to

621
00:26:06,640 --> 00:26:11,200
provide a more flexible and a custom you

622
00:26:11,200 --> 00:26:13,960
know sort of customized privacy and data

623
00:26:13,960 --> 00:26:16,210
security for people knowing people have

624
00:26:16,210 --> 00:26:18,220
such vast differences of opinion on what

625
00:26:18,220 --> 00:26:23,549
that means you first Wow

626
00:26:23,590 --> 00:26:25,490
so I

627
00:26:25,490 --> 00:26:28,520
let me tell you what I hear when I think

628
00:26:28,520 --> 00:26:29,780
of the two key notes that we haven't we

629
00:26:29,780 --> 00:26:31,190
haven't heard Michael Finkel and and

630
00:26:31,190 --> 00:26:32,720
we're expecting a lot from him I know

631
00:26:32,720 --> 00:26:35,240
he's backstage listening right now poor

632
00:26:35,240 --> 00:26:38,420
guy but between these two keynotes what

633
00:26:38,420 --> 00:26:40,130
I see for privacy and security

634
00:26:40,130 --> 00:26:44,420
professionals is job security because it

635
00:26:44,420 --> 00:26:47,480
turns out I know I think it turns out

636
00:26:47,480 --> 00:26:49,970
that this idea of privacy is a human

637
00:26:49,970 --> 00:26:52,610
constant this idea of a desire for a

638
00:26:52,610 --> 00:26:55,070
sense of security is a human constant

639
00:26:55,070 --> 00:26:58,370
and so we need to share information with

640
00:26:58,370 --> 00:27:00,350
each other in order to stitch ourselves

641
00:27:00,350 --> 00:27:02,950
together within society but we also

642
00:27:02,950 --> 00:27:07,450
concurrently need identity autonomy

643
00:27:07,450 --> 00:27:10,670
repose a little bit of seclusion so that

644
00:27:10,670 --> 00:27:12,980
we can protect ourselves and and find

645
00:27:12,980 --> 00:27:15,140
space to think different thoughts and

646
00:27:15,140 --> 00:27:19,250
those two ideas are not perfectly

647
00:27:19,250 --> 00:27:20,810
aligned all the time

648
00:27:20,810 --> 00:27:24,680
and when you intersect governments and

649
00:27:24,680 --> 00:27:26,630
other institutions of control like

650
00:27:26,630 --> 00:27:29,000
corporations and others you create a

651
00:27:29,000 --> 00:27:31,100
very complex environment then throw

652
00:27:31,100 --> 00:27:33,230
technology in the mix what it adds up to

653
00:27:33,230 --> 00:27:36,290
is we are going to be renegotiating the

654
00:27:36,290 --> 00:27:39,530
terms of privacy and security forever

655
00:27:39,530 --> 00:27:42,800
forever and so when I think of what are

656
00:27:42,800 --> 00:27:45,110
these two keynotes mean and and how can

657
00:27:45,110 --> 00:27:47,660
we be engaging it it's don't look for

658
00:27:47,660 --> 00:27:50,150
the answer because the answer is never

659
00:27:50,150 --> 00:27:52,610
going to be found what you have to do is

660
00:27:52,610 --> 00:27:55,520
engage in the ongoing negotiation the

661
00:27:55,520 --> 00:27:57,920
ongoing fight for us to be finding

662
00:27:57,920 --> 00:28:00,800
better answers over time because privacy

663
00:28:00,800 --> 00:28:04,310
and security are eternal negotiations of

664
00:28:04,310 --> 00:28:07,640
humanity and we as privacy and security

665
00:28:07,640 --> 00:28:10,010
professionals collectively we have this

666
00:28:10,010 --> 00:28:13,370
amazingly awesome responsibility to try

667
00:28:13,370 --> 00:28:16,850
to defend that within society that's a

668
00:28:16,850 --> 00:28:19,280
ridiculously cool job that we get to do

669
00:28:19,280 --> 00:28:21,980
but it's also an incredibly complex and

670
00:28:21,980 --> 00:28:24,830
challenging job every day go mad and I

671
00:28:24,830 --> 00:28:27,020
think that the spotlight is on us Trevor

672
00:28:27,020 --> 00:28:29,720
but the lessons are the fact that what

673
00:28:29,720 --> 00:28:32,380
you're describing going forward is

674
00:28:32,380 --> 00:28:35,300
actually demonstrated throughout history

675
00:28:35,300 --> 00:28:38,480
I mean

676
00:28:38,480 --> 00:28:42,120
for long before technology came into

677
00:28:42,120 --> 00:28:44,430
play there were issues around

678
00:28:44,430 --> 00:28:47,220
information security and privacy I mean

679
00:28:47,220 --> 00:28:50,460
the military was has been notorious over

680
00:28:50,460 --> 00:28:54,120
time about you know the need to know and

681
00:28:54,120 --> 00:28:57,270
so the notion of trying to keep that

682
00:28:57,270 --> 00:28:59,520
information you know secure and private

683
00:28:59,520 --> 00:29:02,670
dates way back you can actually look at

684
00:29:02,670 --> 00:29:06,570
security if you go back to if anybody

685
00:29:06,570 --> 00:29:08,880
here saw the imitation game the story

686
00:29:08,880 --> 00:29:12,570
about Alan Turing and him cracking the

687
00:29:12,570 --> 00:29:14,790
enigma code what a great example of

688
00:29:14,790 --> 00:29:17,540
cyber security and its earliest

689
00:29:17,540 --> 00:29:19,800
demonstration and at least that time it

690
00:29:19,800 --> 00:29:22,860
was for the good guys wait that's right

691
00:29:22,860 --> 00:29:27,660
that's right but I the this notion of

692
00:29:27,660 --> 00:29:29,550
continuing to redefinition I would say

693
00:29:29,550 --> 00:29:31,440
there's two things I mean number one is

694
00:29:31,440 --> 00:29:34,830
some of it is being created by the the

695
00:29:34,830 --> 00:29:38,460
fact that technology which you know 80

696
00:29:38,460 --> 00:29:41,400
years ago wasn't as much a part of our

697
00:29:41,400 --> 00:29:44,070
life is now embedded in every aspect and

698
00:29:44,070 --> 00:29:46,530
as a result you know most most of us

699
00:29:46,530 --> 00:29:48,360
that are they're old geezers like myself

700
00:29:48,360 --> 00:29:50,040
you know we grew up with air land sea

701
00:29:50,040 --> 00:29:52,230
space but now it's air land sea space

702
00:29:52,230 --> 00:29:54,990
cyber cyber meaning that technology

703
00:29:54,990 --> 00:29:58,140
space and that is actually helping us as

704
00:29:58,140 --> 00:30:00,270
security and privacy professionals it's

705
00:30:00,270 --> 00:30:04,020
actually calling us to come really help

706
00:30:04,020 --> 00:30:06,180
try to solve issues that are being

707
00:30:06,180 --> 00:30:08,490
created by this new domain new domain

708
00:30:08,490 --> 00:30:10,140
and I'm sure we're going to figure it

709
00:30:10,140 --> 00:30:10,680
out

710
00:30:10,680 --> 00:30:12,720
maybe it's 15 years from now and guess

711
00:30:12,720 --> 00:30:14,010
what then there's gonna be something

712
00:30:14,010 --> 00:30:15,420
else on our doorstep and we're gonna

713
00:30:15,420 --> 00:30:19,050
have to be called in to do it again I

714
00:30:19,050 --> 00:30:20,850
want to thank you both I thought it was

715
00:30:20,850 --> 00:30:23,310
a very thought-provoking beginning of a

716
00:30:23,310 --> 00:30:25,050
very long conversation which according

717
00:30:25,050 --> 00:30:27,600
to you will be infinite but we all have

718
00:30:27,600 --> 00:30:29,670
job security so thanks for that and we

719
00:30:29,670 --> 00:30:31,320
have two organizations ready and willing

720
00:30:31,320 --> 00:30:33,510
to help us all learn and get out of our

721
00:30:33,510 --> 00:30:35,580
silos and do a better job so thank you

722
00:30:35,580 --> 00:30:37,200
and really looking forward to our next

723
00:30:37,200 --> 00:30:38,909
keynote

724
00:30:38,909 --> 00:30:40,970
you

725
00:30:48,770 --> 00:30:50,830
you

