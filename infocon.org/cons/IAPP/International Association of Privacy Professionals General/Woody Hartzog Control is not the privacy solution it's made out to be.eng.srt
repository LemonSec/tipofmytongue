1
00:00:05,260 --> 00:00:07,770
[Music]

2
00:00:07,770 --> 00:00:10,360
hello everyone it's a pleasure to be

3
00:00:10,360 --> 00:00:18,960
here I want to start off the way I do

4
00:00:18,960 --> 00:00:22,600
every year when I teach information

5
00:00:22,600 --> 00:00:26,230
privacy law on the very first day before

6
00:00:26,230 --> 00:00:28,480
we've really gotten into anything I

7
00:00:28,480 --> 00:00:32,409
always ask my students to define privacy

8
00:00:32,409 --> 00:00:36,160
for me we go and write it on the board

9
00:00:36,160 --> 00:00:37,590
and many of the definitions and

10
00:00:37,590 --> 00:00:40,899
conceptualizations of privacy you would

11
00:00:40,899 --> 00:00:43,239
no doubt be very familiar with and so

12
00:00:43,239 --> 00:00:47,230
it's the standard articulation such as

13
00:00:47,230 --> 00:00:49,360
the right to be left alone or privacy is

14
00:00:49,360 --> 00:00:54,610
intimacy or privacy is dignity but by

15
00:00:54,610 --> 00:00:58,930
far the most common articulation of what

16
00:00:58,930 --> 00:01:04,330
privacy is is control

17
00:01:04,330 --> 00:01:08,380
well not that control this control the

18
00:01:08,380 --> 00:01:11,500
kind of control the ability to determine

19
00:01:11,500 --> 00:01:15,159
who does what with your personal

20
00:01:15,159 --> 00:01:19,840
information and although it seems to be

21
00:01:19,840 --> 00:01:23,500
a little bit of an accepted truism in

22
00:01:23,500 --> 00:01:26,380
our world that the concept of privacy

23
00:01:26,380 --> 00:01:28,830
eludes definition and I've argued this

24
00:01:28,830 --> 00:01:32,380
many times looking around it actually

25
00:01:32,380 --> 00:01:34,720
appears as though we've already come to

26
00:01:34,720 --> 00:01:37,869
some kind of consensus at least with

27
00:01:37,869 --> 00:01:39,100
respect to the protection of our

28
00:01:39,100 --> 00:01:41,979
personal information the name of the

29
00:01:41,979 --> 00:01:45,549
game is control let's look at the

30
00:01:45,549 --> 00:01:49,180
evidence Mark Zuckerberg has long

31
00:01:49,180 --> 00:01:52,270
insisted that the way facebook seeks to

32
00:01:52,270 --> 00:01:54,759
protect user privacy is by putting users

33
00:01:54,759 --> 00:02:00,100
in control of their personal data but

34
00:02:00,100 --> 00:02:04,869
it's not just Facebook looking around

35
00:02:04,869 --> 00:02:07,270
the entire tech industry you will see

36
00:02:07,270 --> 00:02:09,280
that the control is the espoused

37
00:02:09,280 --> 00:02:12,459
fundamental ethos that drives industry

38
00:02:12,459 --> 00:02:15,780
data protection efforts

39
00:02:16,030 --> 00:02:19,550
the list goes on usually this notion of

40
00:02:19,550 --> 00:02:22,100
control is often linked to the ways in

41
00:02:22,100 --> 00:02:25,400
which systems are engineered and the and

42
00:02:25,400 --> 00:02:27,770
the it refers to the tools that are

43
00:02:27,770 --> 00:02:30,400
given to users that purportedly let them

44
00:02:30,400 --> 00:02:34,490
determine their own fates it's also the

45
00:02:34,490 --> 00:02:37,010
defining concept for the Internet of

46
00:02:37,010 --> 00:02:41,960
Things as well and actually it turns out

47
00:02:41,960 --> 00:02:46,040
it's not just industry academics and

48
00:02:46,040 --> 00:02:48,410
advocates also adopt the frame of

49
00:02:48,410 --> 00:02:51,920
control as the dominant goal for data

50
00:02:51,920 --> 00:03:00,140
protection efforts in fact I must admit

51
00:03:00,140 --> 00:03:04,220
I am in this boat I have advocated

52
00:03:04,220 --> 00:03:07,220
strongly for control I yearn for it for

53
00:03:07,220 --> 00:03:11,270
data subjects I think that it's it's

54
00:03:11,270 --> 00:03:14,750
it's a worthy and and critical sort of

55
00:03:14,750 --> 00:03:17,390
thing for people to have however it's

56
00:03:17,390 --> 00:03:19,670
not just us it's not just advocates and

57
00:03:19,670 --> 00:03:22,550
academics but where it matters arguably

58
00:03:22,550 --> 00:03:26,209
the most is with regulators and when it

59
00:03:26,209 --> 00:03:28,489
comes to regulators they seem to be all

60
00:03:28,489 --> 00:03:32,450
on board with the idea of control as the

61
00:03:32,450 --> 00:03:36,050
way to effectuate data protection the

62
00:03:36,050 --> 00:03:40,000
GDP are in fact makes it quite explicit

63
00:03:40,000 --> 00:03:43,820
but it's actually not just Europe right

64
00:03:43,820 --> 00:03:46,040
if you look around the list goes on and

65
00:03:46,040 --> 00:03:49,370
on among those who value the control of

66
00:03:49,370 --> 00:03:52,670
personal information in fact control

67
00:03:52,670 --> 00:03:55,610
seems to be the one data protection

68
00:03:55,610 --> 00:03:59,269
principle that everybody can agree on in

69
00:03:59,269 --> 00:04:02,239
particular I think the control is

70
00:04:02,239 --> 00:04:05,030
relevant for the design of information

71
00:04:05,030 --> 00:04:08,480
technologies it is the espoused dominant

72
00:04:08,480 --> 00:04:11,500
privacy goal for privacy by design

73
00:04:11,500 --> 00:04:14,500
efforts

74
00:04:16,800 --> 00:04:21,880
the regulator's seem to have embraced it

75
00:04:21,880 --> 00:04:23,800
informed consent regimes like the e

76
00:04:23,800 --> 00:04:26,080
privacy directive are in essence control

77
00:04:26,080 --> 00:04:28,770
regime's

78
00:04:29,070 --> 00:04:32,860
privacy by design as a concept seems to

79
00:04:32,860 --> 00:04:36,810
embrace control at almost every turn and

80
00:04:36,810 --> 00:04:42,090
in fact in the United States as well

81
00:04:42,090 --> 00:04:46,810
even the pioneer of privacy by design in

82
00:04:46,810 --> 00:04:49,000
Kabuki and has embraced the idea of

83
00:04:49,000 --> 00:04:53,410
privacy as control now the arguments

84
00:04:53,410 --> 00:04:55,660
that I would like to make in this

85
00:04:55,660 --> 00:05:00,450
keynotes is this I have come to believe

86
00:05:00,450 --> 00:05:04,420
that control is the wrong goal for

87
00:05:04,420 --> 00:05:08,740
privacy by design it's actually I think

88
00:05:08,740 --> 00:05:10,300
perhaps the wrong goal for data

89
00:05:10,300 --> 00:05:13,870
protection in general the idealization

90
00:05:13,870 --> 00:05:17,110
of control in modern data protection

91
00:05:17,110 --> 00:05:20,530
regimes ends up being actually adverse I

92
00:05:20,530 --> 00:05:22,840
think to safe and sustainable data

93
00:05:22,840 --> 00:05:26,890
practices too much zeal for control risk

94
00:05:26,890 --> 00:05:29,290
deluding people and regulators about the

95
00:05:29,290 --> 00:05:31,690
potential efficacy of protection

96
00:05:31,690 --> 00:05:35,500
initiatives if we are to be we are not

97
00:05:35,500 --> 00:05:39,370
more careful about the the goals and

98
00:05:39,370 --> 00:05:41,940
values to be reflected in the design of

99
00:05:41,940 --> 00:05:44,380
information technologies we risk

100
00:05:44,380 --> 00:05:46,840
building an entire data protection

101
00:05:46,840 --> 00:05:49,750
Empire at the crumbling edifice of

102
00:05:49,750 --> 00:05:53,590
control now building this out a little

103
00:05:53,590 --> 00:05:55,750
I'd like to make two distinct points in

104
00:05:55,750 --> 00:05:58,150
this talk one I think control is a

105
00:05:58,150 --> 00:06:01,330
compromised concept for personal data

106
00:06:01,330 --> 00:06:04,450
and design second instead of using

107
00:06:04,450 --> 00:06:06,790
control sort of as a proxy which is the

108
00:06:06,790 --> 00:06:08,790
way in which we seem to be using it

109
00:06:08,790 --> 00:06:11,020
privacy by design framework should

110
00:06:11,020 --> 00:06:13,350
embrace more direct values like trust

111
00:06:13,350 --> 00:06:16,770
obscurity and autonomy

112
00:06:16,770 --> 00:06:20,020
now at this point you may be saying to

113
00:06:20,020 --> 00:06:22,720
yourself well sure if control is

114
00:06:22,720 --> 00:06:26,050
conditional if it's vague uninformed and

115
00:06:26,050 --> 00:06:27,669
contingent upon action and of course

116
00:06:27,669 --> 00:06:29,060
it's no good

117
00:06:29,060 --> 00:06:32,930
but our industry / framework / goal is

118
00:06:32,930 --> 00:06:36,100
different we advocate for real control

119
00:06:36,100 --> 00:06:40,340
now I think that this kind of idealized

120
00:06:40,340 --> 00:06:43,430
control is probably impossible in

121
00:06:43,430 --> 00:06:46,430
mediated environments and here's why

122
00:06:46,430 --> 00:06:51,020
first I think that control is illusory

123
00:06:51,020 --> 00:06:54,169
it's a little bit of a shell game in

124
00:06:54,169 --> 00:06:56,330
practice because the control we are

125
00:06:56,330 --> 00:06:59,570
given online is mediated which means it

126
00:06:59,570 --> 00:07:01,729
cannot help but be engineered to produce

127
00:07:01,729 --> 00:07:05,030
a particular results when it comes to

128
00:07:05,030 --> 00:07:07,880
control design is everything the

129
00:07:07,880 --> 00:07:10,790
realities of Technology at scale mean

130
00:07:10,790 --> 00:07:12,229
that the services we use must

131
00:07:12,229 --> 00:07:14,660
necessarily be built in a way that

132
00:07:14,660 --> 00:07:17,300
constrains our choices can you imagine a

133
00:07:17,300 --> 00:07:19,580
world where everyone got to dictate

134
00:07:19,580 --> 00:07:21,800
their own terms in an Open Text box with

135
00:07:21,800 --> 00:07:23,930
this the services with its within which

136
00:07:23,930 --> 00:07:25,669
they interact instead of some sort of

137
00:07:25,669 --> 00:07:28,070
boilerplate term or a standard set of

138
00:07:28,070 --> 00:07:32,000
tools our mediated perception of control

139
00:07:32,000 --> 00:07:34,190
obscures the fact that design funnels

140
00:07:34,190 --> 00:07:37,370
behavior and people can only pick on the

141
00:07:37,370 --> 00:07:40,340
options that are provided to them we

142
00:07:40,340 --> 00:07:43,010
give this sort of fake control to our

143
00:07:43,010 --> 00:07:46,190
kids very often so for example when I

144
00:07:46,190 --> 00:07:47,960
give my kids a choice between going to

145
00:07:47,960 --> 00:07:50,270
the park or going to the movies they

146
00:07:50,270 --> 00:07:52,520
feel empowered and I get to avoid a trip

147
00:07:52,520 --> 00:07:54,860
to the pet store so that I can stave off

148
00:07:54,860 --> 00:07:56,570
a conversation about getting that new

149
00:07:56,570 --> 00:08:01,760
puppy and so I think that it's important

150
00:08:01,760 --> 00:08:03,919
to consider that design also nudges us

151
00:08:03,919 --> 00:08:06,470
by sending signals and making tasks

152
00:08:06,470 --> 00:08:08,650
easier or making things harder

153
00:08:08,650 --> 00:08:11,389
encouraging us to act in predictable

154
00:08:11,389 --> 00:08:14,330
ways companies deployed dark patterns to

155
00:08:14,330 --> 00:08:17,060
exploit our built-in tendencies to

156
00:08:17,060 --> 00:08:19,850
prefer shiny colorful buttons and ignore

157
00:08:19,850 --> 00:08:21,860
the dull gray ones are shameless into

158
00:08:21,860 --> 00:08:24,229
feeling bad about withholding data or

159
00:08:24,229 --> 00:08:26,180
declining options are simply making

160
00:08:26,180 --> 00:08:28,789
exercising control possible but costly

161
00:08:28,789 --> 00:08:31,669
through things like forced work subtle

162
00:08:31,669 --> 00:08:34,640
misdirection and incentive tethering how

163
00:08:34,640 --> 00:08:37,190
many people have seen this ad pop up on

164
00:08:37,190 --> 00:08:39,650
your laptop and tried to click decline

165
00:08:39,650 --> 00:08:41,510
only to realize there's no decline

166
00:08:41,510 --> 00:08:42,250
button there

167
00:08:42,250 --> 00:08:44,140
have to open up the app store in order

168
00:08:44,140 --> 00:08:47,320
to get this thing to disappear here's an

169
00:08:47,320 --> 00:08:48,970
example of what Harry brignall calls

170
00:08:48,970 --> 00:08:52,450
confirmed shaming on his really

171
00:08:52,450 --> 00:08:56,460
excellent dark patterns website

172
00:08:59,520 --> 00:09:02,680
sometimes we get wheedled into over

173
00:09:02,680 --> 00:09:05,140
sharing through the design of services

174
00:09:05,140 --> 00:09:07,000
not only a sharing meant to make us feel

175
00:09:07,000 --> 00:09:09,190
good such as keeping a street going like

176
00:09:09,190 --> 00:09:12,430
here on snapchat or nudges to share old

177
00:09:12,430 --> 00:09:14,830
posts on social media or congratulate

178
00:09:14,830 --> 00:09:17,470
someone else on Facebook or LinkedIn but

179
00:09:17,470 --> 00:09:20,800
sometimes they just make sharing so damn

180
00:09:20,800 --> 00:09:24,100
easy the key to keeping the data spigot

181
00:09:24,100 --> 00:09:26,290
flowing is to get people to want to say

182
00:09:26,290 --> 00:09:29,080
yes and to require the minimum amount of

183
00:09:29,080 --> 00:09:32,280
efforts to have them impulsively do so

184
00:09:32,280 --> 00:09:35,620
design has a powerful tendency to dampen

185
00:09:35,620 --> 00:09:39,040
skepticism if we want if users want it

186
00:09:39,040 --> 00:09:41,140
bad enough then they can justify and

187
00:09:41,140 --> 00:09:43,180
rationalize lots of different decisions

188
00:09:43,180 --> 00:09:46,270
now of course we could create rules to

189
00:09:46,270 --> 00:09:48,550
mitigate against this sort of thing but

190
00:09:48,550 --> 00:09:51,400
of course the essence of design is to

191
00:09:51,400 --> 00:09:53,190
nudge people into a particular direction

192
00:09:53,190 --> 00:09:55,960
now of course these choices are not

193
00:09:55,960 --> 00:09:58,720
always harmful for us many of them in

194
00:09:58,720 --> 00:10:01,420
fact might be quite useful but asking

195
00:10:01,420 --> 00:10:04,990
companies to engineer user control paves

196
00:10:04,990 --> 00:10:07,750
the way for abuse and self dealing at

197
00:10:07,750 --> 00:10:10,780
the margins and these margins matter

198
00:10:10,780 --> 00:10:13,420
even among those acting in good faith we

199
00:10:13,420 --> 00:10:15,790
are left with the problem of relying on

200
00:10:15,790 --> 00:10:17,680
the notion of control and choice to do

201
00:10:17,680 --> 00:10:20,290
more work for us than it is capable of

202
00:10:20,290 --> 00:10:23,140
and we risk looking around at the robust

203
00:10:23,140 --> 00:10:25,030
new frameworks that we have for data

204
00:10:25,030 --> 00:10:27,010
protection the rules built to encourage

205
00:10:27,010 --> 00:10:29,290
meaningful control over personal

206
00:10:29,290 --> 00:10:31,270
information patting ourselves on the

207
00:10:31,270 --> 00:10:33,430
back and saying mission accomplished

208
00:10:33,430 --> 00:10:37,630
but it wasn't even the right mission the

209
00:10:37,630 --> 00:10:40,210
second reason that control is a

210
00:10:40,210 --> 00:10:43,630
compromised goal for design is that it

211
00:10:43,630 --> 00:10:47,500
is overwhelming at scale control over

212
00:10:47,500 --> 00:10:49,480
personal information is highly

213
00:10:49,480 --> 00:10:51,910
attractive in isolation who wouldn't

214
00:10:51,910 --> 00:10:53,890
want more power over the things that

215
00:10:53,890 --> 00:10:54,720
affect our law

216
00:10:54,720 --> 00:10:57,180
but within this power often comes a

217
00:10:57,180 --> 00:10:59,970
practical obligation sometimes when we

218
00:10:59,970 --> 00:11:02,310
do not exercise that control we are put

219
00:11:02,310 --> 00:11:05,220
at risk companies can take our our

220
00:11:05,220 --> 00:11:08,370
inaction as acquiescence the problem

221
00:11:08,370 --> 00:11:10,380
with thinking about privacy is control

222
00:11:10,380 --> 00:11:13,140
is that if users are given our wish for

223
00:11:13,140 --> 00:11:15,660
more privacy it means we are given so

224
00:11:15,660 --> 00:11:17,400
much control that we can choke on it

225
00:11:17,400 --> 00:11:20,430
some recent designs some recent strides

226
00:11:20,430 --> 00:11:22,650
and design rules such as data protection

227
00:11:22,650 --> 00:11:25,680
by defaults actually does help mitigate

228
00:11:25,680 --> 00:11:28,560
some of this problem however even if the

229
00:11:28,560 --> 00:11:31,050
default works demands are still being

230
00:11:31,050 --> 00:11:34,980
made of us so that we relents anyone

231
00:11:34,980 --> 00:11:37,290
that has turned off notification for

232
00:11:37,290 --> 00:11:39,540
apps like Facebook's messenger can

233
00:11:39,540 --> 00:11:42,240
attest to the relentless grindin

234
00:11:42,240 --> 00:11:43,890
requests of the user to turn on

235
00:11:43,890 --> 00:11:46,110
notifications almost every single time

236
00:11:46,110 --> 00:11:48,300
the app is launched many can relate to

237
00:11:48,300 --> 00:11:49,980
the experience of a child asking for

238
00:11:49,980 --> 00:11:53,340
candy over and over until the requests

239
00:11:53,340 --> 00:11:55,620
become too much to ignore and we finally

240
00:11:55,620 --> 00:11:58,650
give in simply to quiet them willpower

241
00:11:58,650 --> 00:12:01,800
can feel like a finite vulnerable and

242
00:12:01,800 --> 00:12:05,070
highly subjective resource and systems

243
00:12:05,070 --> 00:12:08,370
are designed to deplete any rodents once

244
00:12:08,370 --> 00:12:10,110
willpower and the ability to make

245
00:12:10,110 --> 00:12:12,270
choices become compromised the control

246
00:12:12,270 --> 00:12:14,580
that users have been given will largely

247
00:12:14,580 --> 00:12:17,390
be meaningless

248
00:12:17,390 --> 00:12:21,630
even if industry figures out the

249
00:12:21,630 --> 00:12:24,300
Platonic ideal of how a company can give

250
00:12:24,300 --> 00:12:28,500
users and data subjects control it still

251
00:12:28,500 --> 00:12:30,540
wouldn't solve another dilemma which is

252
00:12:30,540 --> 00:12:33,120
the bandwidth dilemma people only have

253
00:12:33,120 --> 00:12:36,420
24 hours in a day and so many resources

254
00:12:36,420 --> 00:12:39,150
to dedicate to making decisions even if

255
00:12:39,150 --> 00:12:41,910
one company or all companies perfected a

256
00:12:41,910 --> 00:12:44,190
control interface people would still be

257
00:12:44,190 --> 00:12:46,200
faced with the barrage of larger

258
00:12:46,200 --> 00:12:48,540
decisions because they use multiple apps

259
00:12:48,540 --> 00:12:51,570
and services control will not change the

260
00:12:51,570 --> 00:12:53,660
fact that the data ecosystem is

261
00:12:53,660 --> 00:12:54,810
mind-bogglingly

262
00:12:54,810 --> 00:12:57,120
complex with many different kinds of

263
00:12:57,120 --> 00:12:59,280
information collected in many different

264
00:12:59,280 --> 00:12:59,640
ways

265
00:12:59,640 --> 00:13:01,580
stored in many different places

266
00:13:01,580 --> 00:13:03,810
processed from many different functions

267
00:13:03,810 --> 00:13:05,730
and shared with many different parties

268
00:13:05,730 --> 00:13:08,080
and even if every single techcamp

269
00:13:08,080 --> 00:13:09,850
merged together to form some sort of

270
00:13:09,850 --> 00:13:13,210
giant Google APIs on book'll then the

271
00:13:13,210 --> 00:13:15,130
tension between simplicity and nuance

272
00:13:15,130 --> 00:13:17,410
inherent in the most complex and fraud

273
00:13:17,410 --> 00:13:19,750
environments imaginable would seem

274
00:13:19,750 --> 00:13:22,570
irresolvable this is because nuance

275
00:13:22,570 --> 00:13:24,670
often gets glossed over when companies

276
00:13:24,670 --> 00:13:27,130
tried to simplify information for our

277
00:13:27,130 --> 00:13:30,340
benefits risk is either hidden away

278
00:13:30,340 --> 00:13:31,990
through abstraction or made so

279
00:13:31,990 --> 00:13:34,270
voluminous and explicit that we don't

280
00:13:34,270 --> 00:13:37,540
even know where to begin because of

281
00:13:37,540 --> 00:13:39,160
course the way in which control gets

282
00:13:39,160 --> 00:13:41,200
expressed in design is through things

283
00:13:41,200 --> 00:13:43,690
like this but it's never just things

284
00:13:43,690 --> 00:13:47,050
like this right it's one toggle switch

285
00:13:47,050 --> 00:13:50,320
that exists among a nested layer of

286
00:13:50,320 --> 00:13:52,120
toggle switches that then exists among

287
00:13:52,120 --> 00:13:55,390
an entire environment of apps and we

288
00:13:55,390 --> 00:13:57,880
look back and we feel overwhelmed at the

289
00:13:57,880 --> 00:14:00,850
number of choices privacy by default

290
00:14:00,850 --> 00:14:03,730
again helps with this but if but the

291
00:14:03,730 --> 00:14:06,400
nudging will continue until morale

292
00:14:06,400 --> 00:14:10,960
improves now finally control is a myopic

293
00:14:10,960 --> 00:14:13,840
value for privacy which i think is the

294
00:14:13,840 --> 00:14:16,030
third reason why control is compromised

295
00:14:16,030 --> 00:14:20,320
as a goal for privacy by design notions

296
00:14:20,320 --> 00:14:23,140
of individual control really don't sit

297
00:14:23,140 --> 00:14:26,350
well with privacy as a collective value

298
00:14:26,350 --> 00:14:28,390
when privacy is thought of an

299
00:14:28,390 --> 00:14:30,700
individualistic transactional terms

300
00:14:30,700 --> 00:14:32,710
people's privacy is always being

301
00:14:32,710 --> 00:14:35,610
negotiated against what others value

302
00:14:35,610 --> 00:14:38,050
control should be tempered against other

303
00:14:38,050 --> 00:14:40,870
values because no one privacy ideology

304
00:14:40,870 --> 00:14:43,660
should reign in a diverse society where

305
00:14:43,660 --> 00:14:47,170
other people's autonomy matters what

306
00:14:47,170 --> 00:14:49,840
makes us think that the collective

307
00:14:49,840 --> 00:14:51,640
result of atomized decisions will be

308
00:14:51,640 --> 00:14:54,160
best for privacy anyway scholars have

309
00:14:54,160 --> 00:14:56,080
noted that a large body of research

310
00:14:56,080 --> 00:14:58,120
shows that people's privacy preferences

311
00:14:58,120 --> 00:15:00,520
are uncertain they are contextually

312
00:15:00,520 --> 00:15:02,520
dependent and they are highly malleable

313
00:15:02,520 --> 00:15:04,990
the availability of knowledge doesn't

314
00:15:04,990 --> 00:15:07,690
necessarily translate meaningfully into

315
00:15:07,690 --> 00:15:10,360
good privacy decisions lawmakers may

316
00:15:10,360 --> 00:15:12,400
favour mandated disclosures like the

317
00:15:12,400 --> 00:15:14,380
warnings on cigarette boxes because they

318
00:15:14,380 --> 00:15:16,270
are cheap and they counterbalance the

319
00:15:16,270 --> 00:15:19,120
information disparity the reality the

320
00:15:19,120 --> 00:15:20,870
companies often know

321
00:15:20,870 --> 00:15:22,610
if that they know much more than

322
00:15:22,610 --> 00:15:24,980
consumers regarding the wisdom of a

323
00:15:24,980 --> 00:15:27,380
decision but in this context users are

324
00:15:27,380 --> 00:15:29,270
being asked to consider the privacy

325
00:15:29,270 --> 00:15:31,790
implications of every single disclosure

326
00:15:31,790 --> 00:15:34,250
that they make an impossibly complex

327
00:15:34,250 --> 00:15:38,090
calculation to make the future of risks

328
00:15:38,090 --> 00:15:39,650
and consequences very difficult to

329
00:15:39,650 --> 00:15:44,510
ascertain now another problem with

330
00:15:44,510 --> 00:15:46,670
control and this is getting a little

331
00:15:46,670 --> 00:15:48,680
closer to it is that it leaves us

332
00:15:48,680 --> 00:15:52,700
sometimes lacking a more firm moral

333
00:15:52,700 --> 00:15:55,730
anchor control presumptively gives

334
00:15:55,730 --> 00:15:58,700
people choices and serves autonomy but

335
00:15:58,700 --> 00:16:01,310
idealizing control can leave designers

336
00:16:01,310 --> 00:16:02,810
and data subjects without much of a

337
00:16:02,810 --> 00:16:04,700
sense of which values are really truly

338
00:16:04,700 --> 00:16:07,010
at stake to be preserved by the system's

339
00:16:07,010 --> 00:16:09,560
purpose or design the lack of an ethical

340
00:16:09,560 --> 00:16:12,110
mooring encourages ideological drift and

341
00:16:12,110 --> 00:16:14,920
abuse when control is the Northstar

342
00:16:14,920 --> 00:16:17,450
company leaders and policy makers aren't

343
00:16:17,450 --> 00:16:19,310
given much to work with when tapped with

344
00:16:19,310 --> 00:16:21,440
improvements in the absence of more

345
00:16:21,440 --> 00:16:24,590
articulate values CEOs and lawmakers say

346
00:16:24,590 --> 00:16:27,110
what is needed is perhaps more control

347
00:16:27,110 --> 00:16:28,940
if something goes wrong well then we

348
00:16:28,940 --> 00:16:31,160
must not have had enough control as

349
00:16:31,160 --> 00:16:33,290
though that would solve the problem but

350
00:16:33,290 --> 00:16:35,240
it's not clear that more control and

351
00:16:35,240 --> 00:16:36,680
more choices are actually going to

352
00:16:36,680 --> 00:16:39,230
really help us maybe what we need are

353
00:16:39,230 --> 00:16:42,050
actually fewer better choices driven by

354
00:16:42,050 --> 00:16:44,720
different moral values but it's hard to

355
00:16:44,720 --> 00:16:47,090
rank which choices are good without a

356
00:16:47,090 --> 00:16:49,520
more concrete theory of exactly what

357
00:16:49,520 --> 00:16:51,440
we're protecting against other than just

358
00:16:51,440 --> 00:16:53,810
control this is where additional value

359
00:16:53,810 --> 00:16:55,880
such as trust obscurity and autonomy

360
00:16:55,880 --> 00:16:58,220
become important they give us a better

361
00:16:58,220 --> 00:17:00,020
roadmap on how to allocate the resources

362
00:17:00,020 --> 00:17:02,960
of control which is finite and precious

363
00:17:02,960 --> 00:17:05,060
we need an approach that incorporates

364
00:17:05,060 --> 00:17:07,250
multiple values and it's capable of

365
00:17:07,250 --> 00:17:09,349
recognizing privacy is a more social

366
00:17:09,349 --> 00:17:14,900
good now to my second point we seem to

367
00:17:14,900 --> 00:17:19,099
be using control as a proxy when control

368
00:17:19,099 --> 00:17:22,849
is expressed as choice in a way it sort

369
00:17:22,849 --> 00:17:26,990
of undercuts its own mission and given

370
00:17:26,990 --> 00:17:30,950
the pathologies that are inherent in

371
00:17:30,950 --> 00:17:33,260
mediating choice people should have a

372
00:17:33,260 --> 00:17:34,650
more fundamental base

373
00:17:34,650 --> 00:17:37,170
level of protection regardless of what

374
00:17:37,170 --> 00:17:40,440
they choose what we have have now in

375
00:17:40,440 --> 00:17:42,510
certain pockets is a system that allows

376
00:17:42,510 --> 00:17:44,430
companies to offload the risks of data

377
00:17:44,430 --> 00:17:47,070
on to the data subjects justifying

378
00:17:47,070 --> 00:17:49,310
control measures on privacy grounds

379
00:17:49,310 --> 00:17:53,040
requires so much justification sometimes

380
00:17:53,040 --> 00:17:55,530
so much stretching and bending and tying

381
00:17:55,530 --> 00:17:57,960
ourselves into knots that it feels like

382
00:17:57,960 --> 00:18:01,080
we're using control as a proxy for some

383
00:18:01,080 --> 00:18:03,180
other protection goal that's just out of

384
00:18:03,180 --> 00:18:06,330
reach for us control feels intuitively

385
00:18:06,330 --> 00:18:09,450
right and it has selling power so we use

386
00:18:09,450 --> 00:18:12,360
it but but what is the result that

387
00:18:12,360 --> 00:18:15,030
policy makers industry advocates and

388
00:18:15,030 --> 00:18:17,120
data subjects are really hoping for

389
00:18:17,120 --> 00:18:19,950
surely it's not control for control zone

390
00:18:19,950 --> 00:18:23,550
sake control ostensibly serves autonomy

391
00:18:23,550 --> 00:18:25,740
but in mediating environments involved

392
00:18:25,740 --> 00:18:28,230
in personal data idealizing control

393
00:18:28,230 --> 00:18:31,920
actually seems corrosive to autonomy its

394
00:18:31,920 --> 00:18:33,750
control valuable because people have

395
00:18:33,750 --> 00:18:37,290
such different privacy preferences or is

396
00:18:37,290 --> 00:18:39,000
it just does it just appear that people

397
00:18:39,000 --> 00:18:40,170
have different privacy preferences

398
00:18:40,170 --> 00:18:42,330
because personal data risks are

399
00:18:42,330 --> 00:18:43,980
virtually impossible for people to

400
00:18:43,980 --> 00:18:46,170
assess consistently across a broad

401
00:18:46,170 --> 00:18:49,050
spectrum if data processing is so

402
00:18:49,050 --> 00:18:50,880
dangerous that it requires formal

403
00:18:50,880 --> 00:18:53,250
permission and meaningful in such

404
00:18:53,250 --> 00:18:56,130
meaningful control consent in choice can

405
00:18:56,130 --> 00:18:59,100
only exist in elusive demanding and

406
00:18:59,100 --> 00:19:01,140
bounded environments with pre conditions

407
00:19:01,140 --> 00:19:04,170
such as freely given specific informed

408
00:19:04,170 --> 00:19:06,750
retractible and unambiguous why are we

409
00:19:06,750 --> 00:19:08,670
allowing controllers to proceed in the

410
00:19:08,670 --> 00:19:11,310
first place is it just a contorted and

411
00:19:11,310 --> 00:19:13,350
indirect way to pressure companies to

412
00:19:13,350 --> 00:19:16,620
lay off risky data practices if so why

413
00:19:16,620 --> 00:19:18,360
not dispense with the pretense of

414
00:19:18,360 --> 00:19:20,640
demanding a form of control that seems

415
00:19:20,640 --> 00:19:22,820
destined and misdirect industry efforts

416
00:19:22,820 --> 00:19:25,350
towards a more formalistic compliance

417
00:19:25,350 --> 00:19:28,410
and a less meaningful change in

418
00:19:28,410 --> 00:19:31,700
processor behavior

419
00:19:34,060 --> 00:19:36,980
this brings me to the point about design

420
00:19:36,980 --> 00:19:39,920
for design I think what matters are

421
00:19:39,920 --> 00:19:40,940
relationships

422
00:19:40,940 --> 00:19:43,640
and transaction costs in other words

423
00:19:43,640 --> 00:19:45,980
what is the nature of the relationship

424
00:19:45,980 --> 00:19:47,780
between the data subject and the person

425
00:19:47,780 --> 00:19:48,920
who's collecting or using the

426
00:19:48,920 --> 00:19:51,500
information and what ways does the

427
00:19:51,500 --> 00:19:53,900
design of an information technology make

428
00:19:53,900 --> 00:19:56,480
things easier reducing the transaction

429
00:19:56,480 --> 00:19:58,670
cost of any particular action or make

430
00:19:58,670 --> 00:20:01,130
things harder by raising the transaction

431
00:20:01,130 --> 00:20:04,040
cost of any particular action and when

432
00:20:04,040 --> 00:20:05,360
you take those two things in

433
00:20:05,360 --> 00:20:06,920
consideration and you look at what's

434
00:20:06,920 --> 00:20:09,590
truly I think important the values that

435
00:20:09,590 --> 00:20:12,680
I think tend to rise to the top are not

436
00:20:12,680 --> 00:20:15,830
necessarily control rather I encourage

437
00:20:15,830 --> 00:20:17,860
an embrace of the values of trust

438
00:20:17,860 --> 00:20:23,300
obscurity and autonomy now trust I think

439
00:20:23,300 --> 00:20:26,770
is the key value for a data protection

440
00:20:26,770 --> 00:20:29,810
moving forward trust the willingness to

441
00:20:29,810 --> 00:20:32,240
make yourself vulnerable to the actions

442
00:20:32,240 --> 00:20:34,580
of others is a key component in so many

443
00:20:34,580 --> 00:20:36,410
different aspects of our lives it

444
00:20:36,410 --> 00:20:39,110
underlies commerce it unders underlies

445
00:20:39,110 --> 00:20:42,290
our our relationships and intimacy and

446
00:20:42,290 --> 00:20:45,200
underlies self-exploration and I think

447
00:20:45,200 --> 00:20:48,050
that's if the rules that we have about

448
00:20:48,050 --> 00:20:49,970
the design of technologies are meant to

449
00:20:49,970 --> 00:20:52,100
make sure that the the trust that is

450
00:20:52,100 --> 00:20:55,160
given is respected then that's a much

451
00:20:55,160 --> 00:20:58,510
more concrete and and in the end

452
00:20:58,510 --> 00:21:01,580
effective way to channel the design of

453
00:21:01,580 --> 00:21:04,160
information technologies and and Trust

454
00:21:04,160 --> 00:21:06,620
contains several different components so

455
00:21:06,620 --> 00:21:08,210
I think that that Trust contains a

456
00:21:08,210 --> 00:21:10,670
responsibility to be discreet sometimes

457
00:21:10,670 --> 00:21:13,700
confidential but maybe just anonymizing

458
00:21:13,700 --> 00:21:16,610
or sadhana maizing data a requirement to

459
00:21:16,610 --> 00:21:18,650
protect information by having good data

460
00:21:18,650 --> 00:21:21,380
security practices a requirement to be

461
00:21:21,380 --> 00:21:24,050
honest which i think may be technically

462
00:21:24,050 --> 00:21:26,860
different than transparency honesty

463
00:21:26,860 --> 00:21:28,700
requirements within trustworthy

464
00:21:28,700 --> 00:21:32,030
relationships mean that that those that

465
00:21:32,030 --> 00:21:33,710
are entrusted with data are actually

466
00:21:33,710 --> 00:21:36,020
forthright about things that they might

467
00:21:36,020 --> 00:21:37,850
rather not disclose we're actually

468
00:21:37,850 --> 00:21:40,100
important to the data subjects so in

469
00:21:40,100 --> 00:21:41,630
other words we don't we don't bear

470
00:21:41,630 --> 00:21:43,490
everything we bear the things that are

471
00:21:43,490 --> 00:21:46,310
most relevant to the data subject and

472
00:21:46,310 --> 00:21:47,720
then finally in the

473
00:21:47,720 --> 00:21:51,230
the key points loyal companies and loyal

474
00:21:51,230 --> 00:21:54,440
and designed the design of information

475
00:21:54,440 --> 00:21:57,770
technologies are those that that indeed

476
00:21:57,770 --> 00:21:59,900
reflect a loyalty on behalf of the data

477
00:21:59,900 --> 00:22:01,520
processor that they will not

478
00:22:01,520 --> 00:22:04,039
unreasonably elevate their own interests

479
00:22:04,039 --> 00:22:07,909
above those of the data subjects the

480
00:22:07,909 --> 00:22:09,679
next value that I advocate for its

481
00:22:09,679 --> 00:22:11,960
obscurity which is a little bit less

482
00:22:11,960 --> 00:22:14,980
popular in our existing data protection

483
00:22:14,980 --> 00:22:18,470
language but obscurity is is the idea

484
00:22:18,470 --> 00:22:20,030
that when information is hard or

485
00:22:20,030 --> 00:22:21,860
unlikely to be understood is to a

486
00:22:21,860 --> 00:22:24,080
relative degree safe

487
00:22:24,080 --> 00:22:26,960
and we rely upon this obscurity all the

488
00:22:26,960 --> 00:22:27,559
time

489
00:22:27,559 --> 00:22:29,870
we walk around cities being technically

490
00:22:29,870 --> 00:22:31,669
in public but realizing that most people

491
00:22:31,669 --> 00:22:34,070
won't see or remember things that happen

492
00:22:34,070 --> 00:22:36,289
gossip that we we engage in at

493
00:22:36,289 --> 00:22:39,140
restaurants we freely do so even though

494
00:22:39,140 --> 00:22:40,730
people are around could hear us but

495
00:22:40,730 --> 00:22:42,770
chances are and most people want we

496
00:22:42,770 --> 00:22:44,179
don't even remember the people that sat

497
00:22:44,179 --> 00:22:46,640
next to us probably the last time we ate

498
00:22:46,640 --> 00:22:49,309
out and obscurity strikes me as a

499
00:22:49,309 --> 00:22:52,820
fundamental components of the ways in

500
00:22:52,820 --> 00:22:54,260
which we should channel the design of

501
00:22:54,260 --> 00:22:56,799
information technologies when we have

502
00:22:56,799 --> 00:22:59,090
obscurity when we have trust and when we

503
00:22:59,090 --> 00:23:01,340
have autonomy which I would argue as as

504
00:23:01,340 --> 00:23:03,350
the value that control ostensibly serves

505
00:23:03,350 --> 00:23:06,590
then we can have better rules for how

506
00:23:06,590 --> 00:23:08,990
our information technologies are created

507
00:23:08,990 --> 00:23:12,679
we can have rules that that may ensure

508
00:23:12,679 --> 00:23:16,400
that design is honest that it does what

509
00:23:16,400 --> 00:23:19,070
it presents itself to do we could have

510
00:23:19,070 --> 00:23:21,049
rules that prevent what I call abusive

511
00:23:21,049 --> 00:23:24,080
design this is the triple negative right

512
00:23:24,080 --> 00:23:26,390
I don't not disagree to something like

513
00:23:26,390 --> 00:23:29,030
this most people do not affirmatively

514
00:23:29,030 --> 00:23:31,909
choose this option it's when design

515
00:23:31,909 --> 00:23:33,679
leverages our own limitations to

516
00:23:33,679 --> 00:23:36,919
understand against us and and control

517
00:23:36,919 --> 00:23:38,690
doesn't always tend to get us to the

518
00:23:38,690 --> 00:23:40,700
place where the technologies would

519
00:23:40,700 --> 00:23:42,740
indeed avoid being abusive and then

520
00:23:42,740 --> 00:23:45,380
finally these values can help protect us

521
00:23:45,380 --> 00:23:47,450
against dangerous design design that

522
00:23:47,450 --> 00:23:50,270
surveil things that puts us unduly at

523
00:23:50,270 --> 00:23:55,130
risk so in the end I think that

524
00:23:55,130 --> 00:23:58,429
lawmakers have more direct options than

525
00:23:58,429 --> 00:24:00,649
embracing control as the value for the

526
00:24:00,649 --> 00:24:01,490
design of in firm

527
00:24:01,490 --> 00:24:04,070
technologies prohibit collection out

528
00:24:04,070 --> 00:24:07,730
rights mandate deletion get serious with

529
00:24:07,730 --> 00:24:09,559
purpose limitations and legitimate

530
00:24:09,559 --> 00:24:11,630
interests change the nature of the

531
00:24:11,630 --> 00:24:13,790
relationship between users and companies

532
00:24:13,790 --> 00:24:16,940
entrusted with their data so to one that

533
00:24:16,940 --> 00:24:20,059
is more fiduciary like in nature mandate

534
00:24:20,059 --> 00:24:23,090
non-delegable duties of care duties of

535
00:24:23,090 --> 00:24:25,490
loyalty duties of honesty and duties of

536
00:24:25,490 --> 00:24:28,309
discretion make rules to ensure that

537
00:24:28,309 --> 00:24:30,470
companies cannot unreasonably favor

538
00:24:30,470 --> 00:24:32,480
their own interests at the expense of

539
00:24:32,480 --> 00:24:35,030
the data subjects the case for privacy

540
00:24:35,030 --> 00:24:37,190
the case against privacy is control is

541
00:24:37,190 --> 00:24:39,559
actually an appeal to more substantive

542
00:24:39,559 --> 00:24:42,740
and effective privacy related values by

543
00:24:42,740 --> 00:24:44,990
expanding beyond the notion of privacy

544
00:24:44,990 --> 00:24:47,540
as control lawmakers and industry would

545
00:24:47,540 --> 00:24:50,030
be free to create rules and practices to

546
00:24:50,030 --> 00:24:53,030
ensure that designers data processors

547
00:24:53,030 --> 00:24:54,640
and controllers are trustworthy

548
00:24:54,640 --> 00:24:56,750
regardless of the control that we are

549
00:24:56,750 --> 00:25:00,170
given and how we exercise it thank you

550
00:25:00,170 --> 00:25:03,309
very much I appreciate it

551
00:25:03,670 --> 00:25:06,910
[Applause]

