1
00:00:04,279 --> 00:00:07,080
hi everyone great to see you all this

2
00:00:07,080 --> 00:00:09,420
morning today I'm going to talk about

3
00:00:09,420 --> 00:00:11,490
how we can collaborate to make

4
00:00:11,490 --> 00:00:15,839
technology move faster as an academic I

5
00:00:15,839 --> 00:00:17,940
spend a lot of time thinking about how

6
00:00:17,940 --> 00:00:20,070
come the ideas that we're coming up with

7
00:00:20,070 --> 00:00:22,500
aren't making their way out into the

8
00:00:22,500 --> 00:00:25,769
real world this talk is about how we can

9
00:00:25,769 --> 00:00:27,570
better use what we already know about

10
00:00:27,570 --> 00:00:31,439
cyber security in case you've forgotten

11
00:00:31,439 --> 00:00:33,480
to attend any other session in this

12
00:00:33,480 --> 00:00:35,160
conference I'll start by saying two

13
00:00:35,160 --> 00:00:38,309
things first the future runs on software

14
00:00:38,309 --> 00:00:41,430
we have smart homes smart cars and even

15
00:00:41,430 --> 00:00:44,399
smart dating if you haven't right about

16
00:00:44,399 --> 00:00:46,079
the tinderbox algorithm for

17
00:00:46,079 --> 00:00:48,570
automatically liking faces based on your

18
00:00:48,570 --> 00:00:51,000
previous likes well I'm not sure if you

19
00:00:51,000 --> 00:00:54,809
want to but to get to this future we

20
00:00:54,809 --> 00:00:56,850
first have to solve the problem of

21
00:00:56,850 --> 00:00:59,969
software security as Kazmir mentioned

22
00:00:59,969 --> 00:01:02,760
hackers can control our electric

23
00:01:02,760 --> 00:01:05,250
skateboards they can control our rifles

24
00:01:05,250 --> 00:01:08,369
they can even control our cars some of

25
00:01:08,369 --> 00:01:10,290
these hacks take years to get fixed and

26
00:01:10,290 --> 00:01:12,090
sometimes they never get discovered

27
00:01:12,090 --> 00:01:18,930
until it's too late in academia there

28
00:01:18,930 --> 00:01:22,200
are many ideas that comfort me there are

29
00:01:22,200 --> 00:01:24,479
all these research ideas that promise to

30
00:01:24,479 --> 00:01:26,820
help secure our software there are

31
00:01:26,820 --> 00:01:29,790
encrypted databases there are mechanisms

32
00:01:29,790 --> 00:01:32,250
for replaying programs so we can find

33
00:01:32,250 --> 00:01:34,590
the vulnerabilities there are techniques

34
00:01:34,590 --> 00:01:36,869
for scrutinizing our programs to find

35
00:01:36,869 --> 00:01:39,780
bugs before it's too late and there are

36
00:01:39,780 --> 00:01:43,259
also mechanisms to write software that's

37
00:01:43,259 --> 00:01:47,430
provably secure but the curious thing is

38
00:01:47,430 --> 00:01:50,780
that in industry the state of the art is

39
00:01:50,780 --> 00:01:55,200
firewalls so in this talk I invite you

40
00:01:55,200 --> 00:01:58,439
to think with me about how we can get

41
00:01:58,439 --> 00:02:01,409
research ideas to make their way more

42
00:02:01,409 --> 00:02:06,060
into practice in this talk I'll first

43
00:02:06,060 --> 00:02:08,429
describe what I've been thinking about

44
00:02:08,429 --> 00:02:11,099
in academia my own work on programming

45
00:02:11,099 --> 00:02:11,980
languages and

46
00:02:11,980 --> 00:02:14,680
in privacy then I'll talk about some

47
00:02:14,680 --> 00:02:16,599
work I've done on better connecting

48
00:02:16,599 --> 00:02:20,230
academia with industry so with startups

49
00:02:20,230 --> 00:02:23,409
venture capital and bigger companies and

50
00:02:23,409 --> 00:02:25,709
a missing part of the piece is

51
00:02:25,709 --> 00:02:29,200
policymakers and consumers so I'll end

52
00:02:29,200 --> 00:02:32,290
by talking about some of that so I

53
00:02:32,290 --> 00:02:34,660
invite you to think with me about how we

54
00:02:34,660 --> 00:02:37,090
can better connect researchers to

55
00:02:37,090 --> 00:02:39,459
everyone else and this isn't just about

56
00:02:39,459 --> 00:02:41,890
getting research out there but it's also

57
00:02:41,890 --> 00:02:43,750
about giving researchers feedback about

58
00:02:43,750 --> 00:02:49,599
what it's useful for us to work on so

59
00:02:49,599 --> 00:02:51,519
let's start by talking about what we

60
00:02:51,519 --> 00:02:53,650
already know this is what we've been

61
00:02:53,650 --> 00:02:56,530
thinking about in academia and something

62
00:02:56,530 --> 00:02:58,209
that many of us think about is how can

63
00:02:58,209 --> 00:03:00,940
we get security by design instead of

64
00:03:00,940 --> 00:03:02,920
finding bugs before it's too late or

65
00:03:02,920 --> 00:03:05,290
waiting for hackers to find our bugs how

66
00:03:05,290 --> 00:03:08,170
can we design our systems with security

67
00:03:08,170 --> 00:03:11,890
and privacy in mind and from my point of

68
00:03:11,890 --> 00:03:13,840
view as a programming languages

69
00:03:13,840 --> 00:03:16,150
researcher we're still living in the

70
00:03:16,150 --> 00:03:19,750
1970s here are two screenshots of code

71
00:03:19,750 --> 00:03:22,569
from the popular hot crop conference

72
00:03:22,569 --> 00:03:24,579
management system yes that's what it's

73
00:03:24,579 --> 00:03:27,250
really called and I don't expect you to

74
00:03:27,250 --> 00:03:29,590
read this code but what I want to show

75
00:03:29,590 --> 00:03:32,560
is that in order to enforce security and

76
00:03:32,560 --> 00:03:34,959
privacy policies programmers have to

77
00:03:34,959 --> 00:03:36,489
write conditional checks

78
00:03:36,489 --> 00:03:40,450
if-then-else checks across the code if

79
00:03:40,450 --> 00:03:43,660
we think about it it makes sense if we

80
00:03:43,660 --> 00:03:45,940
look at all of the programming paradigms

81
00:03:45,940 --> 00:03:48,010
in mainstream languages they've been

82
00:03:48,010 --> 00:03:50,919
around since the 1970s private privacy

83
00:03:50,919 --> 00:03:52,690
and security were not concerns in the

84
00:03:52,690 --> 00:03:54,730
1970's and so it makes sense that

85
00:03:54,730 --> 00:03:57,069
languages weren't built with those in

86
00:03:57,069 --> 00:03:57,519
mind

87
00:03:57,519 --> 00:03:59,799
and so if programmers want to protect

88
00:03:59,799 --> 00:04:01,870
sensitive data everywhere that it's

89
00:04:01,870 --> 00:04:03,730
being hugh's across the program they

90
00:04:03,730 --> 00:04:05,410
have to check to see where is this data

91
00:04:05,410 --> 00:04:07,720
coming from where is it going what's the

92
00:04:07,720 --> 00:04:10,030
other program state that's allowing me

93
00:04:10,030 --> 00:04:15,040
to make these decisions to address this

94
00:04:15,040 --> 00:04:18,339
problem my PhD work has been about a new

95
00:04:18,339 --> 00:04:20,469
programming paradigm called policy

96
00:04:20,469 --> 00:04:23,140
agnostic programming the main idea is

97
00:04:23,140 --> 00:04:25,750
that programmers can attach policies to

98
00:04:25,750 --> 00:04:28,240
broccoli to data and the Rustler program

99
00:04:28,240 --> 00:04:30,550
can be written in a way that's agnostic

100
00:04:30,550 --> 00:04:32,830
to these policies and the programmer can

101
00:04:32,830 --> 00:04:35,260
then rely on the language and the

102
00:04:35,260 --> 00:04:37,120
language runtime to enforce these

103
00:04:37,120 --> 00:04:41,020
policies automatically I spent some time

104
00:04:41,020 --> 00:04:43,090
developing a programming model that

105
00:04:43,090 --> 00:04:45,100
provides mathematical guarantees about

106
00:04:45,100 --> 00:04:46,930
what it means for the program's to

107
00:04:46,930 --> 00:04:49,600
comply with these policies and I also

108
00:04:49,600 --> 00:04:51,280
developed an implementation strategy

109
00:04:51,280 --> 00:04:53,710
that demonstrates this can work on

110
00:04:53,710 --> 00:05:00,210
real-world programs and so I hope that

111
00:05:00,210 --> 00:05:03,010
policy agnostic programming is what can

112
00:05:03,010 --> 00:05:05,640
take us into the 21st century of

113
00:05:05,640 --> 00:05:07,870
programming with security and privacy in

114
00:05:07,870 --> 00:05:10,030
mind on the left here we have our friend

115
00:05:10,030 --> 00:05:11,890
the program with manually enforced

116
00:05:11,890 --> 00:05:13,960
policies you have the conditional checks

117
00:05:13,960 --> 00:05:15,940
that are repeated across the program and

118
00:05:15,940 --> 00:05:18,070
on the right hand side there are are

119
00:05:18,070 --> 00:05:20,080
screenshots of code from the Jaqueline

120
00:05:20,080 --> 00:05:22,360
web framework a policy agnostic web

121
00:05:22,360 --> 00:05:24,070
framework that i've built as an

122
00:05:24,070 --> 00:05:26,080
extension of the Django Python web

123
00:05:26,080 --> 00:05:28,300
framework and what you can see is that

124
00:05:28,300 --> 00:05:30,330
all the policies are in one place

125
00:05:30,330 --> 00:05:33,700
associated with the data what you can't

126
00:05:33,700 --> 00:05:35,950
see is that you don't have to write the

127
00:05:35,950 --> 00:05:38,200
policies as repeated checks anymore but

128
00:05:38,200 --> 00:05:39,940
each of them needs to be written now

129
00:05:39,940 --> 00:05:45,490
only once so by reducing the opportunity

130
00:05:45,490 --> 00:05:48,370
for program or error we can reduce the

131
00:05:48,370 --> 00:05:53,650
opportunity for information leaks now

132
00:05:53,650 --> 00:05:55,180
you might be saying well this sounds

133
00:05:55,180 --> 00:05:58,030
good but how can we get this into

134
00:05:58,030 --> 00:06:01,090
practice the next part of my talk is

135
00:06:01,090 --> 00:06:04,169
about why more of these ideas aren't

136
00:06:04,169 --> 00:06:06,580
going into practice and how we can do

137
00:06:06,580 --> 00:06:09,550
better with this to answer this question

138
00:06:09,550 --> 00:06:11,590
we first have to think about what the

139
00:06:11,590 --> 00:06:14,080
barriers are to industry adoption of

140
00:06:14,080 --> 00:06:17,470
these ideas the first is that managers

141
00:06:17,470 --> 00:06:19,870
need to fight the status quo they have

142
00:06:19,870 --> 00:06:22,570
to make arguments often economic about

143
00:06:22,570 --> 00:06:24,240
why they have to do things differently

144
00:06:24,240 --> 00:06:26,800
so unless there's a catastrophe it's

145
00:06:26,800 --> 00:06:29,140
often hard to justify using a completely

146
00:06:29,140 --> 00:06:32,470
new paradigm or tool steam for

147
00:06:32,470 --> 00:06:34,450
programmers programmers struggle with

148
00:06:34,450 --> 00:06:37,030
legacy code so even if they really

149
00:06:37,030 --> 00:06:38,440
wanted to take the time to

150
00:06:38,440 --> 00:06:39,970
learn a new language or a new technique

151
00:06:39,970 --> 00:06:41,890
they have to make sure that this

152
00:06:41,890 --> 00:06:43,840
interoperates with all of the code

153
00:06:43,840 --> 00:06:48,730
that's been written before so one way

154
00:06:48,730 --> 00:06:50,710
that we can think about tech transfer is

155
00:06:50,710 --> 00:06:53,200
through startups startups aren't

156
00:06:53,200 --> 00:06:56,470
burdened by the same problems that large

157
00:06:56,470 --> 00:06:59,170
organizations have and in fact there's

158
00:06:59,170 --> 00:07:00,820
been an increasing interest and

159
00:07:00,820 --> 00:07:03,970
investment in startups for security Sam

160
00:07:03,970 --> 00:07:06,010
Altman who's the president of the Y

161
00:07:06,010 --> 00:07:08,650
Combinator incubator tweeted recently

162
00:07:08,650 --> 00:07:11,100
that he wants to invest in dozens of

163
00:07:11,100 --> 00:07:14,820
security companies in the next few years

164
00:07:14,820 --> 00:07:17,470
but before we get too excited about this

165
00:07:17,470 --> 00:07:19,690
we need to think about why there haven't

166
00:07:19,690 --> 00:07:23,020
been more security startups already and

167
00:07:23,020 --> 00:07:27,120
my answer is that security is no tin dog

168
00:07:27,120 --> 00:07:30,310
so let's think about the quintessential

169
00:07:30,310 --> 00:07:32,350
hot new Silicon Valley startup I

170
00:07:32,350 --> 00:07:34,600
recently heard about tinder for dog

171
00:07:34,600 --> 00:07:37,300
lovers where you can you can post

172
00:07:37,300 --> 00:07:39,160
pictures of you and your dog and you can

173
00:07:39,160 --> 00:07:41,200
swipe through to meet other people who

174
00:07:41,200 --> 00:07:44,470
also love their dogs this has a fun

175
00:07:44,470 --> 00:07:47,169
concept it has a slick design it's so

176
00:07:47,169 --> 00:07:48,730
easy that even your toddler enough you

177
00:07:48,730 --> 00:07:50,770
can use it and you don't have to change

178
00:07:50,770 --> 00:07:52,570
anything about your life in order to use

179
00:07:52,570 --> 00:07:54,190
it except for maybe give up several

180
00:07:54,190 --> 00:07:57,610
hours of your day looking at dogs in

181
00:07:57,610 --> 00:08:00,790
contrast the startup that's going to

182
00:08:00,790 --> 00:08:02,770
help us secure our software is going to

183
00:08:02,770 --> 00:08:05,140
have a technical concept probably

184
00:08:05,140 --> 00:08:08,110
verifiable only by experts and because

185
00:08:08,110 --> 00:08:10,240
existing infrastructure is not designed

186
00:08:10,240 --> 00:08:11,470
with security in mind

187
00:08:11,470 --> 00:08:13,630
it's probably going to require us to

188
00:08:13,630 --> 00:08:15,580
completely change the way the

189
00:08:15,580 --> 00:08:19,300
infrastructure already is it's not

190
00:08:19,300 --> 00:08:21,720
really the sexiest start of proposition

191
00:08:21,720 --> 00:08:25,360
in short there are unique challenges for

192
00:08:25,360 --> 00:08:28,270
security startups there's a concept

193
00:08:28,270 --> 00:08:29,830
that's probably going to be highly

194
00:08:29,830 --> 00:08:32,559
technical there usually are not flashy

195
00:08:32,559 --> 00:08:33,969
demos if you're doing more than

196
00:08:33,969 --> 00:08:36,400
penetration testing or bug finding you

197
00:08:36,400 --> 00:08:38,049
can't just go into a company and wow

198
00:08:38,049 --> 00:08:41,460
them with your absence of bugs and

199
00:08:41,460 --> 00:08:43,659
adoption of these technical solutions

200
00:08:43,659 --> 00:08:45,970
requires either the client to understand

201
00:08:45,970 --> 00:08:48,339
fully what's going on or trust the

202
00:08:48,339 --> 00:08:50,770
company fully that they're doing what

203
00:08:50,770 --> 00:08:52,120
there's a say they're doing

204
00:08:52,120 --> 00:08:55,930
and finally building a security product

205
00:08:55,930 --> 00:08:58,269
is quite different than building a

206
00:08:58,269 --> 00:09:01,569
technical security solution Justin so

207
00:09:01,569 --> 00:09:03,309
meanie who's the chief trust officer of

208
00:09:03,309 --> 00:09:06,220
box says that a major reason security

209
00:09:06,220 --> 00:09:07,600
products fail is because they're made by

210
00:09:07,600 --> 00:09:10,980
security people security people are

211
00:09:10,980 --> 00:09:13,329
doing back-end coating they're not

212
00:09:13,329 --> 00:09:15,309
worried about user experience products

213
00:09:15,309 --> 00:09:18,430
are all about user experience so to

214
00:09:18,430 --> 00:09:20,259
address this problem a friend and I

215
00:09:20,259 --> 00:09:21,879
started an accelerator called cyber

216
00:09:21,879 --> 00:09:24,040
security factory justin is wearing one

217
00:09:24,040 --> 00:09:26,970
of our shirts this tweet is from us and

218
00:09:26,970 --> 00:09:29,439
it's an a weak accelerate that gives

219
00:09:29,439 --> 00:09:32,439
teams $20,000 in funding a network of

220
00:09:32,439 --> 00:09:34,509
seasoned security entrepreneurs

221
00:09:34,509 --> 00:09:36,490
potential investors and potential

222
00:09:36,490 --> 00:09:38,589
clients office space and legal support

223
00:09:38,589 --> 00:09:42,129
we piloted our program this summer with

224
00:09:42,129 --> 00:09:44,680
two teams in collaboration with Highland

225
00:09:44,680 --> 00:09:47,199
Capital Ventures one thing that our

226
00:09:47,199 --> 00:09:48,759
accelerator does that's different from

227
00:09:48,759 --> 00:09:51,459
any other accelerator we know is focused

228
00:09:51,459 --> 00:09:54,490
mentorship so our mentors included

229
00:09:54,490 --> 00:09:57,040
seasoned security entrepreneurs and also

230
00:09:57,040 --> 00:09:59,220
heads of security at large companies

231
00:09:59,220 --> 00:10:03,009
including Justin Sami nyet Box Max Crone

232
00:10:03,009 --> 00:10:05,050
who was the founder of OkCupid and key

233
00:10:05,050 --> 00:10:07,839
base Raj Shah from Palo Alto Networks

234
00:10:07,839 --> 00:10:12,129
and David King from Improv odda our two

235
00:10:12,129 --> 00:10:14,350
teams had not formed companies when they

236
00:10:14,350 --> 00:10:16,360
joined our program and they're both

237
00:10:16,360 --> 00:10:18,579
going on to raise funding after the

238
00:10:18,579 --> 00:10:22,629
program what we learned from the summer

239
00:10:22,629 --> 00:10:24,519
is that having this focused mentorship

240
00:10:24,519 --> 00:10:27,160
helps but there's a final missing piece

241
00:10:27,160 --> 00:10:30,279
which is how to motivate customers to

242
00:10:30,279 --> 00:10:34,209
actually pay for security and if we

243
00:10:34,209 --> 00:10:35,920
think about security from a high level

244
00:10:35,920 --> 00:10:39,009
view it's a no-brainer that we want to

245
00:10:39,009 --> 00:10:41,439
all chip in and pay there was a recent

246
00:10:41,439 --> 00:10:43,839
report from the Atlantic Council in

247
00:10:43,839 --> 00:10:46,120
Zurich insurance group that says that by

248
00:10:46,120 --> 00:10:49,149
estimates that by 2030 an insecure

249
00:10:49,149 --> 00:10:53,379
internet would cost us 90 trillion

250
00:10:53,379 --> 00:10:57,040
dollars and in contrast a completely

251
00:10:57,040 --> 00:11:00,730
secure internet would increase our net

252
00:11:00,730 --> 00:11:02,889
benefit by a hundred ninety trillion

253
00:11:02,889 --> 00:11:05,529
dollars and while I'm having some

254
00:11:05,529 --> 00:11:05,800
trouble

255
00:11:05,800 --> 00:11:07,839
managing these trillions of dollars or

256
00:11:07,839 --> 00:11:10,089
what it means to have a completely

257
00:11:10,089 --> 00:11:13,450
secure internet what this is just to me

258
00:11:13,450 --> 00:11:15,880
is that there's a sort of prisoner's

259
00:11:15,880 --> 00:11:18,779
dilemma when it comes to security in

260
00:11:18,779 --> 00:11:21,640
game theory the prisoner's dilemma is an

261
00:11:21,640 --> 00:11:23,560
example where you have two prisoners and

262
00:11:23,560 --> 00:11:26,620
if they cooperate their combined

263
00:11:26,620 --> 00:11:28,899
sentence is the least but the cost of

264
00:11:28,899 --> 00:11:30,760
one of them betraying the other is so

265
00:11:30,760 --> 00:11:33,850
high that it's theoretically optimal for

266
00:11:33,850 --> 00:11:36,060
both prisoners to betray each other and

267
00:11:36,060 --> 00:11:39,269
for security this seems to be the case

268
00:11:39,269 --> 00:11:42,339
individual entities lack incentive to

269
00:11:42,339 --> 00:11:44,350
secure their software if you want to

270
00:11:44,350 --> 00:11:45,910
secure your software you have to train

271
00:11:45,910 --> 00:11:48,160
your employees you your programmers have

272
00:11:48,160 --> 00:11:50,050
to spend extra time writing secure code

273
00:11:50,050 --> 00:11:52,600
and finally secure software isn't

274
00:11:52,600 --> 00:11:55,209
actually providing much competitive

275
00:11:55,209 --> 00:11:57,850
advantage right now it's much more worth

276
00:11:57,850 --> 00:12:02,829
your while to develop a new feature so I

277
00:12:02,829 --> 00:12:05,079
can't speak to the policy changes that

278
00:12:05,079 --> 00:12:07,510
can improve this but I've thought a lot

279
00:12:07,510 --> 00:12:09,490
about what we can do with policies that

280
00:12:09,490 --> 00:12:12,070
exist and what we can do is we can

281
00:12:12,070 --> 00:12:14,170
create a culture around caring about

282
00:12:14,170 --> 00:12:16,959
security and privacy there are two main

283
00:12:16,959 --> 00:12:18,010
parts of this that I've thought about

284
00:12:18,010 --> 00:12:21,220
and one is getting consumers to care

285
00:12:21,220 --> 00:12:24,510
more you might have heard about how the

286
00:12:24,510 --> 00:12:27,250
snapchat ephemeral photo messaging app

287
00:12:27,250 --> 00:12:30,490
has egregious privacy violations they

288
00:12:30,490 --> 00:12:32,110
promise to delete their photos they

289
00:12:32,110 --> 00:12:34,570
don't actually delete their photos but

290
00:12:34,570 --> 00:12:37,420
the thing is they're benefiting from

291
00:12:37,420 --> 00:12:40,810
this it seems their estimated value is

292
00:12:40,810 --> 00:12:42,790
16 billion dollars and they have a

293
00:12:42,790 --> 00:12:45,550
hundred million users it seemed that the

294
00:12:45,550 --> 00:12:46,810
worst thing that happened to them was a

295
00:12:46,810 --> 00:12:50,880
couple weeks of questionable press and

296
00:12:50,880 --> 00:12:54,459
some criticism from the Federal Trade

297
00:12:54,459 --> 00:12:56,500
Commission and in fact I'm the only

298
00:12:56,500 --> 00:12:59,829
person I know who deleted snapchat from

299
00:12:59,829 --> 00:13:01,959
my phone not because I passed the age of

300
00:13:01,959 --> 00:13:05,199
13 but because I objected to the the

301
00:13:05,199 --> 00:13:08,380
privacy practices that they had so we

302
00:13:08,380 --> 00:13:09,699
can do a much better job of getting

303
00:13:09,699 --> 00:13:12,100
consumers to stand up for their security

304
00:13:12,100 --> 00:13:14,980
and privacy the other part that I

305
00:13:14,980 --> 00:13:16,270
discovered this summer with the

306
00:13:16,270 --> 00:13:19,060
accelerator was that there are privacy

307
00:13:19,060 --> 00:13:19,660
standard

308
00:13:19,660 --> 00:13:21,340
people don't seem to be enforcing them

309
00:13:21,340 --> 00:13:23,680
and what we found out was that it's

310
00:13:23,680 --> 00:13:26,200
because people aren't getting audited so

311
00:13:26,200 --> 00:13:28,510
one of our cybersecurity factory teams

312
00:13:28,510 --> 00:13:30,190
spent a bunch of time talking on the

313
00:13:30,190 --> 00:13:32,050
phone with dentists and they said do you

314
00:13:32,050 --> 00:13:34,180
want our secure system and the dentist

315
00:13:34,180 --> 00:13:35,980
said no we're happy just emailing around

316
00:13:35,980 --> 00:13:37,840
our x-rays and records because we're not

317
00:13:37,840 --> 00:13:41,380
getting audited so what this showed us

318
00:13:41,380 --> 00:13:43,360
was well it seems that there are

319
00:13:43,360 --> 00:13:45,250
standards and that these standards say

320
00:13:45,250 --> 00:13:47,260
that people should care about our

321
00:13:47,260 --> 00:13:49,300
solutions but if these standards are not

322
00:13:49,300 --> 00:13:52,150
getting enforced then it's not going to

323
00:13:52,150 --> 00:13:55,630
happen so we can do a much better job of

324
00:13:55,630 --> 00:13:58,090
working together and enforcing these

325
00:13:58,090 --> 00:14:00,640
policies and if we look to the FTC this

326
00:14:00,640 --> 00:14:03,940
requires technology for detecting when

327
00:14:03,940 --> 00:14:06,580
these policies are being violated so

328
00:14:06,580 --> 00:14:10,300
that something can happen about it I'm

329
00:14:10,300 --> 00:14:14,230
running out of time so and by giving you

330
00:14:14,230 --> 00:14:16,270
a recipe for how we can secure our

331
00:14:16,270 --> 00:14:19,570
software number one we ask smart people

332
00:14:19,570 --> 00:14:21,160
to come up with technical solutions

333
00:14:21,160 --> 00:14:24,370
pretty simple number to you we put the

334
00:14:24,370 --> 00:14:26,560
solutions into place and usually the

335
00:14:26,560 --> 00:14:27,640
solutions don't work the first time

336
00:14:27,640 --> 00:14:30,480
around so then number three we iterate

337
00:14:30,480 --> 00:14:34,360
pretty simple but as you saw in this

338
00:14:34,360 --> 00:14:36,880
talk there are some things we need to do

339
00:14:36,880 --> 00:14:38,380
in order to make this happen

340
00:14:38,380 --> 00:14:40,660
first of all we need to better connect

341
00:14:40,660 --> 00:14:43,180
research with industry so the idea is

342
00:14:43,180 --> 00:14:44,710
that the smart people in academia are

343
00:14:44,710 --> 00:14:46,750
coming up with are actually making their

344
00:14:46,750 --> 00:14:49,840
way out into the real world but to make

345
00:14:49,840 --> 00:14:52,240
industry care about these ideas we also

346
00:14:52,240 --> 00:14:54,670
have to change the incentives for

347
00:14:54,670 --> 00:14:57,160
securing our software and this whole

348
00:14:57,160 --> 00:14:59,800
process requires communication and

349
00:14:59,800 --> 00:15:03,730
education to end I'd like to say that

350
00:15:03,730 --> 00:15:05,800
I'm incredibly optimistic about the

351
00:15:05,800 --> 00:15:07,840
state of software security because there

352
00:15:07,840 --> 00:15:09,340
are so many smart people like all of you

353
00:15:09,340 --> 00:15:11,980
working on it and there are many pieces

354
00:15:11,980 --> 00:15:14,200
that are already there so I invite you

355
00:15:14,200 --> 00:15:16,660
to think with me about how we can open

356
00:15:16,660 --> 00:15:18,580
up these channels of communication and

357
00:15:18,580 --> 00:15:20,590
connect the pieces so that we can

358
00:15:20,590 --> 00:15:23,470
eventually have some approximation of a

359
00:15:23,470 --> 00:15:26,910
completely secure Internet

