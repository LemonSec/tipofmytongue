1
00:00:08,240 --> 00:00:10,400
[Music]

2
00:00:10,400 --> 00:00:11,440
thank you

3
00:00:11,440 --> 00:00:13,360
david olashaga

4
00:00:13,360 --> 00:00:17,039
and good morning iap it's so wonderful

5
00:00:17,039 --> 00:00:19,760
to see you all

6
00:00:20,000 --> 00:00:23,199
my name is julie brill i am a member of

7
00:00:23,199 --> 00:00:26,400
the iapp board of directors

8
00:00:26,400 --> 00:00:29,920
and i serve as microsoft's chief privacy

9
00:00:29,920 --> 00:00:31,039
officer

10
00:00:31,039 --> 00:00:33,280
and corporate vice president of our

11
00:00:33,280 --> 00:00:36,160
global privacy and regulatory affairs

12
00:00:36,160 --> 00:00:37,840
organization

13
00:00:37,840 --> 00:00:40,960
today i have the distinct privilege of

14
00:00:40,960 --> 00:00:43,440
introducing you to our next keynote

15
00:00:43,440 --> 00:00:44,399
speaker

16
00:00:44,399 --> 00:00:46,000
brad smith

17
00:00:46,000 --> 00:00:48,640
brad smith is the president and vice

18
00:00:48,640 --> 00:00:50,960
chair of microsoft

19
00:00:50,960 --> 00:00:53,840
and he's my boss

20
00:00:53,840 --> 00:00:56,640
brad leads microsoft's global teams

21
00:00:56,640 --> 00:00:59,039
working on the most critical issues at

22
00:00:59,039 --> 00:01:01,440
the intersection of technology

23
00:01:01,440 --> 00:01:04,959
regulation and society today

24
00:01:04,959 --> 00:01:07,520
this includes cyber security privacy of

25
00:01:07,520 --> 00:01:09,920
course artificial intelligence

26
00:01:09,920 --> 00:01:13,200
sustainability human rights immigration

27
00:01:13,200 --> 00:01:16,560
and philanthropy and so much more

28
00:01:16,560 --> 00:01:19,360
in all of these areas brad has been a

29
00:01:19,360 --> 00:01:22,400
leader in addressing how the technology

30
00:01:22,400 --> 00:01:24,960
industry can both innovate

31
00:01:24,960 --> 00:01:28,000
and advance the rights of individuals

32
00:01:28,000 --> 00:01:30,880
the new york times has named brad the

33
00:01:30,880 --> 00:01:33,360
ambassador of the technology industry at

34
00:01:33,360 --> 00:01:34,400
large

35
00:01:34,400 --> 00:01:37,280
in recognition of his role in building

36
00:01:37,280 --> 00:01:38,400
bridges

37
00:01:38,400 --> 00:01:40,240
bridges of understanding with

38
00:01:40,240 --> 00:01:43,680
policymakers and regulators that are so

39
00:01:43,680 --> 00:01:45,600
critical to the development of

40
00:01:45,600 --> 00:01:48,720
approaches that guide our industry and

41
00:01:48,720 --> 00:01:50,960
better serve humanity

42
00:01:50,960 --> 00:01:53,200
in his best-selling book

43
00:01:53,200 --> 00:01:55,759
which is called tools and weapons the

44
00:01:55,759 --> 00:01:58,799
promise and the peril of the digital age

45
00:01:58,799 --> 00:02:02,159
brad urged the tech in the tech industry

46
00:02:02,159 --> 00:02:04,719
to assume more responsibility

47
00:02:04,719 --> 00:02:07,920
and he encouraged governments to move

48
00:02:07,920 --> 00:02:10,959
faster to address the challenges that

49
00:02:10,959 --> 00:02:13,599
new technologies are creating

50
00:02:13,599 --> 00:02:16,640
the continued vitality of brad's message

51
00:02:16,640 --> 00:02:19,280
is well demonstrated by the discussions

52
00:02:19,280 --> 00:02:22,879
all of us are having here this week at

53
00:02:22,879 --> 00:02:26,239
the at our global privacy summit

54
00:02:26,239 --> 00:02:27,920
as we all continue to navigate the

55
00:02:27,920 --> 00:02:30,400
complex environment of data protection

56
00:02:30,400 --> 00:02:31,760
and ai

57
00:02:31,760 --> 00:02:35,200
and cyber security and online safety

58
00:02:35,200 --> 00:02:37,760
brad will continue to encourage us to

59
00:02:37,760 --> 00:02:39,920
think holistically about these

60
00:02:39,920 --> 00:02:43,040
challenges and how to work cooperatively

61
00:02:43,040 --> 00:02:46,000
towards solutions and with that please

62
00:02:46,000 --> 00:02:50,040
welcome brad smith

63
00:02:52,540 --> 00:02:55,280
[Music]

64
00:02:55,280 --> 00:02:57,840
good morning

65
00:02:57,840 --> 00:02:59,840
it's so great to be here i have to say

66
00:02:59,840 --> 00:03:01,680
one of the great things about my job is

67
00:03:01,680 --> 00:03:05,120
i get to work every day with julie brill

68
00:03:05,120 --> 00:03:06,560
yeah yeah

69
00:03:06,560 --> 00:03:07,760
the uh

70
00:03:07,760 --> 00:03:10,239
another great thing about my job is i

71
00:03:10,239 --> 00:03:13,040
periodically get to come and speak to

72
00:03:13,040 --> 00:03:15,920
all of you and be part of these what are

73
00:03:15,920 --> 00:03:19,040
now called summits and interact with

74
00:03:19,040 --> 00:03:21,200
folks at the iapp

75
00:03:21,200 --> 00:03:24,879
i joined microsoft in 1993 my first job

76
00:03:24,879 --> 00:03:28,480
was in paris the iapp didn't yet exist

77
00:03:28,480 --> 00:03:30,640
but i can say i remember when you could

78
00:03:30,640 --> 00:03:33,280
go to an iapp meeting and you would just

79
00:03:33,280 --> 00:03:35,120
sit around a table

80
00:03:35,120 --> 00:03:37,599
and then it would fill a room

81
00:03:37,599 --> 00:03:41,200
and now as you all attest

82
00:03:41,200 --> 00:03:44,159
it literally is bursting at the seams of

83
00:03:44,159 --> 00:03:47,760
a huge convention center

84
00:03:47,760 --> 00:03:48,959
and

85
00:03:48,959 --> 00:03:52,000
each year the issues have changed

86
00:03:52,000 --> 00:03:54,480
the profession has grown

87
00:03:54,480 --> 00:03:56,319
but along the way over the last two

88
00:03:56,319 --> 00:03:59,040
decades i think something remarkable has

89
00:03:59,040 --> 00:04:00,640
happened

90
00:04:00,640 --> 00:04:04,159
you have advanced what has become one of

91
00:04:04,159 --> 00:04:07,680
the great human rights issues of our

92
00:04:07,680 --> 00:04:10,080
time quite literally

93
00:04:10,080 --> 00:04:12,319
and it's because of the explosion of

94
00:04:12,319 --> 00:04:13,599
technology

95
00:04:13,599 --> 00:04:15,680
but it's not just that the cause is

96
00:04:15,680 --> 00:04:16,798
critical

97
00:04:16,798 --> 00:04:19,358
it's also complex

98
00:04:19,358 --> 00:04:21,600
that's why we need so many people in the

99
00:04:21,600 --> 00:04:24,800
privacy profession you can't protect

100
00:04:24,800 --> 00:04:27,360
human rights if you can't manage the

101
00:04:27,360 --> 00:04:31,040
complexity that technology has created

102
00:04:31,040 --> 00:04:32,800
for the world

103
00:04:32,800 --> 00:04:35,919
and in a sense where you have gone we

104
00:04:35,919 --> 00:04:37,680
are now at a point where i think many

105
00:04:37,680 --> 00:04:39,759
others will need to follow not just for

106
00:04:39,759 --> 00:04:41,360
privacy alone

107
00:04:41,360 --> 00:04:43,759
but for many other legal and regulatory

108
00:04:43,759 --> 00:04:45,759
fields as well in a much more

109
00:04:45,759 --> 00:04:47,840
closely integrated way

110
00:04:47,840 --> 00:04:50,400
so fundamentally my thesis this morning

111
00:04:50,400 --> 00:04:53,280
is relatively straightforward it's this

112
00:04:53,280 --> 00:04:54,840
this decade the

113
00:04:54,840 --> 00:04:58,320
2020s have created a new era for

114
00:04:58,320 --> 00:04:59,759
technology

115
00:04:59,759 --> 00:05:02,400
unlike the last few decades we've

116
00:05:02,400 --> 00:05:06,160
entered a decade we've entered an era

117
00:05:06,160 --> 00:05:08,800
that is characterized by both technology

118
00:05:08,800 --> 00:05:13,360
innovation and technology regulation

119
00:05:13,360 --> 00:05:16,160
and we are going to need to find new

120
00:05:16,160 --> 00:05:18,720
ways to manage through this

121
00:05:18,720 --> 00:05:21,199
we're going to need a large community of

122
00:05:21,199 --> 00:05:24,800
people like you you will be in some ways

123
00:05:24,800 --> 00:05:27,520
the foundation for it we will need

124
00:05:27,520 --> 00:05:29,680
thoughtful conversations literally

125
00:05:29,680 --> 00:05:31,520
around the world but perhaps more than

126
00:05:31,520 --> 00:05:32,479
anything

127
00:05:32,479 --> 00:05:34,800
it will take a new mindset

128
00:05:34,800 --> 00:05:36,560
if the world

129
00:05:36,560 --> 00:05:40,000
is going to manage this well

130
00:05:40,000 --> 00:05:42,560
so let me show you what we see and let

131
00:05:42,560 --> 00:05:44,479
me show you why

132
00:05:44,479 --> 00:05:47,039
we think this is the case and then i

133
00:05:47,039 --> 00:05:48,720
want to talk about what it will take

134
00:05:48,720 --> 00:05:50,720
what the ingredients will be

135
00:05:50,720 --> 00:05:53,280
in order to develop the mindset and the

136
00:05:53,280 --> 00:05:56,319
capability for all of us to manage

137
00:05:56,319 --> 00:05:59,680
technology through the decade ahead

138
00:05:59,680 --> 00:06:01,680
the first thing is in some ways obvious

139
00:06:01,680 --> 00:06:03,680
but only if you really look up and look

140
00:06:03,680 --> 00:06:07,600
at the world as a whole every day

141
00:06:07,600 --> 00:06:10,080
governments are creating complex

142
00:06:10,080 --> 00:06:13,680
regulatory requirements for technology

143
00:06:13,680 --> 00:06:14,800
we like

144
00:06:14,800 --> 00:06:17,360
many companies and institutions follow

145
00:06:17,360 --> 00:06:19,440
this not just in one place but every

146
00:06:19,440 --> 00:06:21,759
place and what we see

147
00:06:21,759 --> 00:06:24,960
is an explosion of technology laws and

148
00:06:24,960 --> 00:06:27,600
regulations and proposals that is

149
00:06:27,600 --> 00:06:28,720
literally

150
00:06:28,720 --> 00:06:30,880
sweeping the planet

151
00:06:30,880 --> 00:06:33,360
i just came back from nine days in four

152
00:06:33,360 --> 00:06:35,680
countries in africa and everywhere i met

153
00:06:35,680 --> 00:06:37,039
it wasn't just

154
00:06:37,039 --> 00:06:40,319
on the agenda of ministries of ict it

155
00:06:40,319 --> 00:06:42,639
was important to presidents and prime

156
00:06:42,639 --> 00:06:45,039
ministers as well

157
00:06:45,039 --> 00:06:46,880
and what we're therefore seeing is

158
00:06:46,880 --> 00:06:48,800
something that started with privacy in

159
00:06:48,800 --> 00:06:50,800
the 1990s

160
00:06:50,800 --> 00:06:52,080
and is now

161
00:06:52,080 --> 00:06:54,800
leading to a long list

162
00:06:54,800 --> 00:06:57,759
of new and emerging and changing legal

163
00:06:57,759 --> 00:07:00,319
and regulatory fields

164
00:07:00,319 --> 00:07:02,319
cyber security digital safety

165
00:07:02,319 --> 00:07:04,120
responsible ai

166
00:07:04,120 --> 00:07:06,400
telecommunications competition trade

167
00:07:06,400 --> 00:07:09,360
accessibility this list is not even

168
00:07:09,360 --> 00:07:11,360
complete

169
00:07:11,360 --> 00:07:13,360
that is what is happening around the

170
00:07:13,360 --> 00:07:14,639
world

171
00:07:14,639 --> 00:07:16,400
but i think it's important to step back

172
00:07:16,400 --> 00:07:18,560
and not just ask what is happening it's

173
00:07:18,560 --> 00:07:21,360
perhaps even more important to ask

174
00:07:21,360 --> 00:07:22,639
why

175
00:07:22,639 --> 00:07:24,880
why is this happening why is it

176
00:07:24,880 --> 00:07:27,680
happening now why is this decade going

177
00:07:27,680 --> 00:07:30,720
to be different from the past

178
00:07:30,720 --> 00:07:33,840
i think there's two ways to look at it

179
00:07:33,840 --> 00:07:35,599
the first is through the lens of the

180
00:07:35,599 --> 00:07:36,720
present

181
00:07:36,720 --> 00:07:39,280
and look at the role that technology is

182
00:07:39,280 --> 00:07:40,400
playing

183
00:07:40,400 --> 00:07:42,639
it really is a tool that can be put to

184
00:07:42,639 --> 00:07:46,160
use to help solve any problem anywhere

185
00:07:46,160 --> 00:07:47,599
in the world

186
00:07:47,599 --> 00:07:50,080
as you just heard you can't manage

187
00:07:50,080 --> 00:07:53,759
through a pandemic without use of data

188
00:07:53,759 --> 00:07:56,879
you can't address climate change without

189
00:07:56,879 --> 00:07:58,960
putting data to work

190
00:07:58,960 --> 00:08:00,639
you can't

191
00:08:00,639 --> 00:08:03,759
address anything effectively without

192
00:08:03,759 --> 00:08:05,599
harnessing the power of digital

193
00:08:05,599 --> 00:08:08,720
technology and data together

194
00:08:08,720 --> 00:08:11,039
but technology is not just a tool it

195
00:08:11,039 --> 00:08:13,840
really has become a weapon it's become a

196
00:08:13,840 --> 00:08:16,319
weapon in so many ways and shapes and

197
00:08:16,319 --> 00:08:18,639
forms directly and indirectly and

198
00:08:18,639 --> 00:08:22,240
believe me 2022 has made this clearer

199
00:08:22,240 --> 00:08:25,360
than ever as we at microsoft see every

200
00:08:25,360 --> 00:08:28,560
day often times every hour as we are

201
00:08:28,560 --> 00:08:31,520
helping the government of ukraine defend

202
00:08:31,520 --> 00:08:34,080
its military and civilian institutions

203
00:08:34,080 --> 00:08:36,880
from an ongoing and unprecedented wave

204
00:08:36,880 --> 00:08:40,519
of cyber attacks

205
00:08:43,440 --> 00:08:46,720
this is what technology has brought to

206
00:08:46,720 --> 00:08:48,000
the world

207
00:08:48,000 --> 00:08:50,160
and for all of us who create it we're so

208
00:08:50,160 --> 00:08:53,200
proud rightly so of what we get to do

209
00:08:53,200 --> 00:08:56,640
and yet we also have to be clear-eyed as

210
00:08:56,640 --> 00:08:58,800
we look around the world

211
00:08:58,800 --> 00:09:01,600
we have to address the problems that

212
00:09:01,600 --> 00:09:04,000
technology has created

213
00:09:04,000 --> 00:09:05,920
but there's another way that i find it

214
00:09:05,920 --> 00:09:07,760
helpful to think about why this is

215
00:09:07,760 --> 00:09:09,440
happening it's to look at this not just

216
00:09:09,440 --> 00:09:12,080
through the lens of the present

217
00:09:12,080 --> 00:09:15,200
but through the lens of the past

218
00:09:15,200 --> 00:09:17,920
when we think about the future

219
00:09:17,920 --> 00:09:19,680
in some ways

220
00:09:19,680 --> 00:09:23,360
i think it all started in one place

221
00:09:23,360 --> 00:09:27,600
in one year the place was washington d.c

222
00:09:27,600 --> 00:09:29,040
and the year

223
00:09:29,040 --> 00:09:31,360
was 1887.

224
00:09:31,360 --> 00:09:34,080
what happened in 1887.

225
00:09:34,080 --> 00:09:36,320
well let me take you back for a moment

226
00:09:36,320 --> 00:09:39,360
to the greatest technology change of the

227
00:09:39,360 --> 00:09:41,120
1800s

228
00:09:41,120 --> 00:09:43,440
it was called the railroad

229
00:09:43,440 --> 00:09:46,160
in 1840 there were just a few railroad

230
00:09:46,160 --> 00:09:49,040
lines in the northeast united states but

231
00:09:49,040 --> 00:09:51,760
as you look at this unfold decade by

232
00:09:51,760 --> 00:09:53,040
decade

233
00:09:53,040 --> 00:09:55,200
rare roads stretched across the

234
00:09:55,200 --> 00:09:57,120
continent they helped the north defeat

235
00:09:57,120 --> 00:09:58,959
the south in the civil war they

236
00:09:58,959 --> 00:10:01,360
connected the east coast with the west

237
00:10:01,360 --> 00:10:03,600
and by 1890

238
00:10:03,600 --> 00:10:06,320
they led to a map that basically looked

239
00:10:06,320 --> 00:10:07,519
like what it

240
00:10:07,519 --> 00:10:08,880
had become

241
00:10:08,880 --> 00:10:10,399
rare roads

242
00:10:10,399 --> 00:10:12,880
had become the central nervous system of

243
00:10:12,880 --> 00:10:15,040
an entire country

244
00:10:15,040 --> 00:10:16,880
one of my favorite books that i think

245
00:10:16,880 --> 00:10:19,519
speaks to the issues that we deal with

246
00:10:19,519 --> 00:10:21,920
every day all of us in this room was

247
00:10:21,920 --> 00:10:23,839
written two decades ago it wasn't about

248
00:10:23,839 --> 00:10:25,680
digital technology it was about

249
00:10:25,680 --> 00:10:28,320
railroads it's called railroads and

250
00:10:28,320 --> 00:10:30,480
american law

251
00:10:30,480 --> 00:10:32,560
and the professor who wrote it said few

252
00:10:32,560 --> 00:10:35,040
aspects of american society were

253
00:10:35,040 --> 00:10:37,120
untouched by the railroad he quoted a

254
00:10:37,120 --> 00:10:40,000
newspaper story that said the rare road

255
00:10:40,000 --> 00:10:42,160
has revolutionized

256
00:10:42,160 --> 00:10:44,320
everything

257
00:10:44,320 --> 00:10:47,279
if that sounds familiar

258
00:10:47,279 --> 00:10:49,760
this author recognized two decades ago

259
00:10:49,760 --> 00:10:52,320
the familiarity as well he said at its

260
00:10:52,320 --> 00:10:53,200
peak

261
00:10:53,200 --> 00:10:55,680
the railroad was the internet of its day

262
00:10:55,680 --> 00:10:57,920
in the extent of its impact on american

263
00:10:57,920 --> 00:10:59,440
life

264
00:10:59,440 --> 00:11:01,519
and law

265
00:11:01,519 --> 00:11:03,839
in fact if you just read the table of

266
00:11:03,839 --> 00:11:06,720
contents of this book what you see is

267
00:11:06,720 --> 00:11:10,720
the way the railroad changed influence

268
00:11:10,720 --> 00:11:11,920
altered

269
00:11:11,920 --> 00:11:16,079
field after field of american law and

270
00:11:16,079 --> 00:11:17,519
regulation

271
00:11:17,519 --> 00:11:21,600
it literally changed the way americans

272
00:11:21,600 --> 00:11:24,240
checked the time it led to the invention

273
00:11:24,240 --> 00:11:26,240
of the time zone you had to have

274
00:11:26,240 --> 00:11:28,079
standardized time zones so everybody

275
00:11:28,079 --> 00:11:29,680
would know when the darn train was going

276
00:11:29,680 --> 00:11:33,040
to arrive at the station no longer could

277
00:11:33,040 --> 00:11:34,320
each town

278
00:11:34,320 --> 00:11:36,160
set its own clock

279
00:11:36,160 --> 00:11:39,680
but it wasn't just this piecemeal change

280
00:11:39,680 --> 00:11:41,920
of law after law although that is

281
00:11:41,920 --> 00:11:44,959
fundamentally what took decades

282
00:11:44,959 --> 00:11:46,160
to happen

283
00:11:46,160 --> 00:11:49,040
there was something even more remarkable

284
00:11:49,040 --> 00:11:51,279
that took place

285
00:11:51,279 --> 00:11:53,920
not surprisingly the state governments

286
00:11:53,920 --> 00:11:55,920
recognized that in order to regulate

287
00:11:55,920 --> 00:11:57,920
railroads effectively they needed to

288
00:11:57,920 --> 00:11:59,440
worry about what was happening on the

289
00:11:59,440 --> 00:12:01,600
train before it got to its state and

290
00:12:01,600 --> 00:12:03,279
after it left

291
00:12:03,279 --> 00:12:05,279
they increasingly reached beyond their

292
00:12:05,279 --> 00:12:08,480
borders and in 1886 the supreme court

293
00:12:08,480 --> 00:12:10,880
said that state law did not reach beyond

294
00:12:10,880 --> 00:12:13,839
a state's border so it was the next year

295
00:12:13,839 --> 00:12:15,760
in 1887

296
00:12:15,760 --> 00:12:18,160
that congress broke a decade-long

297
00:12:18,160 --> 00:12:20,720
stalemate and created the interstate

298
00:12:20,720 --> 00:12:23,760
commerce commission for one specific

299
00:12:23,760 --> 00:12:25,120
purpose

300
00:12:25,120 --> 00:12:28,000
to regulate railroads

301
00:12:28,000 --> 00:12:29,519
that act

302
00:12:29,519 --> 00:12:30,880
that year

303
00:12:30,880 --> 00:12:32,959
really marked the birth

304
00:12:32,959 --> 00:12:36,000
of the modern regulatory state

305
00:12:36,000 --> 00:12:38,720
and so much of what so many people work

306
00:12:38,720 --> 00:12:40,320
with today

307
00:12:40,320 --> 00:12:43,519
is the legacy of what was created

308
00:12:43,519 --> 00:12:46,320
there are cars that are regulated

309
00:12:46,320 --> 00:12:48,639
by the department of transportation

310
00:12:48,639 --> 00:12:50,399
there are airplanes by the federal

311
00:12:50,399 --> 00:12:52,079
aviation administration there are

312
00:12:52,079 --> 00:12:53,920
pharmaceuticals by the food and drug

313
00:12:53,920 --> 00:12:56,320
administration there are telephones by

314
00:12:56,320 --> 00:12:58,720
the federal communications commission

315
00:12:58,720 --> 00:13:01,040
all while the department of justice and

316
00:13:01,040 --> 00:13:04,320
ftc do their jobs as well

317
00:13:04,320 --> 00:13:06,320
but what we've seen in the united states

318
00:13:06,320 --> 00:13:08,399
and what we've seen around the world is

319
00:13:08,399 --> 00:13:10,240
increasingly over the course of a

320
00:13:10,240 --> 00:13:11,440
century

321
00:13:11,440 --> 00:13:14,079
agencies that were created

322
00:13:14,079 --> 00:13:17,200
to regulate specific industries

323
00:13:17,200 --> 00:13:20,639
that were based on specific inventions

324
00:13:20,639 --> 00:13:23,279
and areas of technology

325
00:13:23,279 --> 00:13:25,760
that is the past

326
00:13:25,760 --> 00:13:27,839
that almost inevitably

327
00:13:27,839 --> 00:13:30,320
will shape the future

328
00:13:30,320 --> 00:13:33,040
but the future is never a replay of

329
00:13:33,040 --> 00:13:35,440
something we like to say one of our

330
00:13:35,440 --> 00:13:37,519
favorite quotes history doesn't

331
00:13:37,519 --> 00:13:40,399
necessarily repeat itself

332
00:13:40,399 --> 00:13:42,800
but it often rhymes

333
00:13:42,800 --> 00:13:44,800
and what's important to think about are

334
00:13:44,800 --> 00:13:47,360
both the similarities and differences

335
00:13:47,360 --> 00:13:49,360
and there is one fundamental difference

336
00:13:49,360 --> 00:13:51,440
about the world today for digital

337
00:13:51,440 --> 00:13:53,839
technology and data

338
00:13:53,839 --> 00:13:56,959
no industry has ever had to adapt so

339
00:13:56,959 --> 00:14:00,560
quickly to new law and regulation on a

340
00:14:00,560 --> 00:14:02,800
global basis

341
00:14:02,800 --> 00:14:04,720
these other industries were national

342
00:14:04,720 --> 00:14:06,160
industries

343
00:14:06,160 --> 00:14:07,839
the telephone was national even

344
00:14:07,839 --> 00:14:09,360
something that we to think of as a

345
00:14:09,360 --> 00:14:12,079
global market today like automobiles

346
00:14:12,079 --> 00:14:15,120
took a long time to globalize

347
00:14:15,120 --> 00:14:18,480
there were 49 years between henry ford's

348
00:14:18,480 --> 00:14:21,760
invention of the model t in 1908 and the

349
00:14:21,760 --> 00:14:25,199
first import of a japanese car

350
00:14:25,199 --> 00:14:28,079
into america

351
00:14:28,079 --> 00:14:31,040
that was a long time for regulators to

352
00:14:31,040 --> 00:14:33,360
work on a national basis

353
00:14:33,360 --> 00:14:34,399
today

354
00:14:34,399 --> 00:14:36,800
technology has gone global and as you

355
00:14:36,800 --> 00:14:39,839
all know as you all represent given all

356
00:14:39,839 --> 00:14:42,880
the countries here in this room

357
00:14:42,880 --> 00:14:43,760
this

358
00:14:43,760 --> 00:14:46,880
is a global exercise

359
00:14:46,880 --> 00:14:49,279
so what are we going to need to do

360
00:14:49,279 --> 00:14:51,920
i think it involves four ingredients

361
00:14:51,920 --> 00:14:54,720
first i think it's time to recognize

362
00:14:54,720 --> 00:14:56,000
reality

363
00:14:56,000 --> 00:14:58,560
the tech sector needs to mature

364
00:14:58,560 --> 00:15:02,079
and we need to lean in

365
00:15:02,079 --> 00:15:06,320
to help make a new era of regulation

366
00:15:06,320 --> 00:15:07,519
work

367
00:15:07,519 --> 00:15:10,000
we need to recognize that the need to

368
00:15:10,000 --> 00:15:13,120
serve the common good vastly outweighs

369
00:15:13,120 --> 00:15:14,880
the regulatory opportunities for

370
00:15:14,880 --> 00:15:17,680
competitive advantage the press loves to

371
00:15:17,680 --> 00:15:20,079
focus on a new law is it going to help

372
00:15:20,079 --> 00:15:21,920
this company is it going to hurt that

373
00:15:21,920 --> 00:15:24,320
company the reality is regulation

374
00:15:24,320 --> 00:15:26,560
applies to everyone and there will be

375
00:15:26,560 --> 00:15:28,639
only fleeting days

376
00:15:28,639 --> 00:15:31,519
and very short periods of time

377
00:15:31,519 --> 00:15:34,160
when one company or part of the industry

378
00:15:34,160 --> 00:15:37,040
benefits at the expense of another

379
00:15:37,040 --> 00:15:39,680
we all need to recognize in my view and

380
00:15:39,680 --> 00:15:41,920
not just in the tech sector but across

381
00:15:41,920 --> 00:15:44,079
the business community

382
00:15:44,079 --> 00:15:46,399
that our common goals and the common

383
00:15:46,399 --> 00:15:50,480
good and even the common bonds we share

384
00:15:50,480 --> 00:15:53,759
vastly outweigh any differences in

385
00:15:53,759 --> 00:15:56,240
perspective that's part of what it will

386
00:15:56,240 --> 00:15:59,120
take for our industry frankly

387
00:15:59,120 --> 00:16:00,560
to mature

388
00:16:00,560 --> 00:16:02,240
but there's a second thing we need to

389
00:16:02,240 --> 00:16:04,320
recognize we need to recognize it around

390
00:16:04,320 --> 00:16:06,079
the world but we need to recognize that

391
00:16:06,079 --> 00:16:08,560
most especially in the united states and

392
00:16:08,560 --> 00:16:10,800
it is important not just for the tech

393
00:16:10,800 --> 00:16:13,920
sector it is important for every part of

394
00:16:13,920 --> 00:16:15,839
the economy

395
00:16:15,839 --> 00:16:18,320
progress in a democracy

396
00:16:18,320 --> 00:16:19,600
always

397
00:16:19,600 --> 00:16:22,560
requires compromise

398
00:16:22,560 --> 00:16:26,240
we live in an era where compromise is

399
00:16:26,240 --> 00:16:29,199
too difficult to find

400
00:16:29,199 --> 00:16:31,839
and there is no better case study in my

401
00:16:31,839 --> 00:16:33,680
view of this than

402
00:16:33,680 --> 00:16:36,800
u.s privacy law

403
00:16:36,800 --> 00:16:39,680
in 2005 i came to washington i gave a

404
00:16:39,680 --> 00:16:41,360
speech to the congressional internet

405
00:16:41,360 --> 00:16:43,360
caucus and we were

406
00:16:43,360 --> 00:16:44,480
one of the

407
00:16:44,480 --> 00:16:47,519
first companies to call for the adoption

408
00:16:47,519 --> 00:16:50,560
of a broad and comprehensive national

409
00:16:50,560 --> 00:16:53,519
privacy law that was 17 years ago you

410
00:16:53,519 --> 00:16:55,680
can just see how influential that speech

411
00:16:55,680 --> 00:16:57,759
was

412
00:16:57,759 --> 00:17:00,560
but just think for a moment

413
00:17:00,560 --> 00:17:03,279
about where we are today

414
00:17:03,279 --> 00:17:05,359
i would argue that in some ways it's

415
00:17:05,359 --> 00:17:06,880
shocking

416
00:17:06,880 --> 00:17:09,280
i think we need to recognize here in

417
00:17:09,280 --> 00:17:11,760
washington d.c and across the country

418
00:17:11,760 --> 00:17:14,000
that comprehensive privacy legislation

419
00:17:14,000 --> 00:17:16,959
for the united states is not just needed

420
00:17:16,959 --> 00:17:20,880
it's long overdue

421
00:17:25,199 --> 00:17:27,199
let's just look at what has happened

422
00:17:27,199 --> 00:17:30,000
around the world while our congress has

423
00:17:30,000 --> 00:17:32,720
been frozen in time

424
00:17:32,720 --> 00:17:35,919
when we look at national privacy laws it

425
00:17:35,919 --> 00:17:38,080
started in france and germany and

426
00:17:38,080 --> 00:17:41,440
denmark and the nordics in 1989

427
00:17:41,440 --> 00:17:44,960
in the 1990s it started to move across

428
00:17:44,960 --> 00:17:49,520
to 2009 it spread further by 2018 it had

429
00:17:49,520 --> 00:17:52,120
reached most of the world here we are in

430
00:17:52,120 --> 00:17:56,480
2022 there's more than 120 jurisdictions

431
00:17:56,480 --> 00:17:58,640
around the world that have passed a

432
00:17:58,640 --> 00:18:00,400
national privacy law

433
00:18:00,400 --> 00:18:02,640
and the united states increasingly

434
00:18:02,640 --> 00:18:05,360
stands alone

435
00:18:05,360 --> 00:18:08,640
and the fact of the matter is in my view

436
00:18:08,640 --> 00:18:11,039
there is a critical element that we are

437
00:18:11,039 --> 00:18:13,039
failing to think about in the united

438
00:18:13,039 --> 00:18:15,600
states is that the failure of the united

439
00:18:15,600 --> 00:18:17,840
states to legislate doesn't stop global

440
00:18:17,840 --> 00:18:19,120
regulation

441
00:18:19,120 --> 00:18:22,720
it doesn't even slow it down it just

442
00:18:22,720 --> 00:18:25,360
makes our country less influential in

443
00:18:25,360 --> 00:18:26,559
the world

444
00:18:26,559 --> 00:18:29,679
and for a country like ours that has

445
00:18:29,679 --> 00:18:33,120
rightly so long aspired to be a beacon

446
00:18:33,120 --> 00:18:35,360
of hope and the protection of

447
00:18:35,360 --> 00:18:37,520
fundamental rights

448
00:18:37,520 --> 00:18:40,240
we don't do ourselves a service we don't

449
00:18:40,240 --> 00:18:45,679
help the world when we sit on our hands

450
00:18:45,679 --> 00:18:47,679
and just think for a moment about the

451
00:18:47,679 --> 00:18:49,760
two issues that lead to continued

452
00:18:49,760 --> 00:18:53,120
deadlock in congress in 2022

453
00:18:53,120 --> 00:18:55,760
one involves a private right of action

454
00:18:55,760 --> 00:18:58,960
the other involves preemption these are

455
00:18:58,960 --> 00:19:01,280
two issues that have been part of every

456
00:19:01,280 --> 00:19:04,080
consumer protection law in the united

457
00:19:04,080 --> 00:19:07,760
states for more than a century

458
00:19:07,760 --> 00:19:10,799
why is it that in the 1930s and 50s and

459
00:19:10,799 --> 00:19:13,200
70s and 90s people could come together

460
00:19:13,200 --> 00:19:16,240
but in our own day they cannot

461
00:19:16,240 --> 00:19:19,360
it's because we're not signed up

462
00:19:19,360 --> 00:19:23,600
to embrace the importance of compromise

463
00:19:23,600 --> 00:19:26,799
and that is part of the mindset

464
00:19:26,799 --> 00:19:29,360
that we need to change

465
00:19:29,360 --> 00:19:31,600
but this isn't something that the tech

466
00:19:31,600 --> 00:19:34,480
sector or the business community can

467
00:19:34,480 --> 00:19:38,080
work by itself to advance

468
00:19:38,080 --> 00:19:41,280
we need well-informed coordination

469
00:19:41,280 --> 00:19:42,960
within governments

470
00:19:42,960 --> 00:19:46,799
within every government around the world

471
00:19:46,799 --> 00:19:49,280
for those of us who build technology for

472
00:19:49,280 --> 00:19:51,440
those of you who work with technology

473
00:19:51,440 --> 00:19:53,919
and apply it to

474
00:19:53,919 --> 00:19:56,480
your employer's business

475
00:19:56,480 --> 00:19:59,440
one thing is clear as i said before it

476
00:19:59,440 --> 00:20:01,360
is complicated

477
00:20:01,360 --> 00:20:03,039
and that the more you work with

478
00:20:03,039 --> 00:20:05,679
technology the more you come to realize

479
00:20:05,679 --> 00:20:07,840
that frankly some things are easier to

480
00:20:07,840 --> 00:20:09,919
design than they are to build

481
00:20:09,919 --> 00:20:11,520
some things are easier to build than

482
00:20:11,520 --> 00:20:13,840
they are to operate i love this photo

483
00:20:13,840 --> 00:20:16,240
this is the plane the spruce goose it

484
00:20:16,240 --> 00:20:18,159
was paid for by the united states

485
00:20:18,159 --> 00:20:20,240
government of world war ii it was

486
00:20:20,240 --> 00:20:22,799
designed and built by howard hughes and

487
00:20:22,799 --> 00:20:26,640
in 1948 it got 30 feet off the water and

488
00:20:26,640 --> 00:20:29,840
that's the highest it ever flew

489
00:20:29,840 --> 00:20:32,320
we can't afford

490
00:20:32,320 --> 00:20:34,720
from a regulatory perspective

491
00:20:34,720 --> 00:20:36,880
or a legal perspective

492
00:20:36,880 --> 00:20:39,760
to put in place the kinds of rules

493
00:20:39,760 --> 00:20:42,320
that are going to make it impossible

494
00:20:42,320 --> 00:20:45,440
to build the technology the world needs

495
00:20:45,440 --> 00:20:47,280
and that's going to require more

496
00:20:47,280 --> 00:20:49,440
dialogue it's going to require a broader

497
00:20:49,440 --> 00:20:51,440
vision it's going to require governments

498
00:20:51,440 --> 00:20:53,440
to think from left to right about every

499
00:20:53,440 --> 00:20:55,760
technology issue and that's what we're

500
00:20:55,760 --> 00:20:57,200
starting to see

501
00:20:57,200 --> 00:20:58,720
that's what this white house has

502
00:20:58,720 --> 00:21:01,679
advanced with a bipartisan commission to

503
00:21:01,679 --> 00:21:04,799
look at the future of technology and

504
00:21:04,799 --> 00:21:07,120
that is a good example of what we're

505
00:21:07,120 --> 00:21:08,640
going to need

506
00:21:08,640 --> 00:21:10,880
i think another of the most important

507
00:21:10,880 --> 00:21:13,280
early innovations of the 2020s comes

508
00:21:13,280 --> 00:21:14,880
from the united kingdom

509
00:21:14,880 --> 00:21:16,480
where they brought together the

510
00:21:16,480 --> 00:21:19,039
principal regulatory agencies and have

511
00:21:19,039 --> 00:21:21,679
created a digital regulation cooperation

512
00:21:21,679 --> 00:21:25,679
forum to coordinate among them

513
00:21:25,679 --> 00:21:27,039
but think about

514
00:21:27,039 --> 00:21:29,520
what the world has done for every other

515
00:21:29,520 --> 00:21:32,240
industry in area of technology think

516
00:21:32,240 --> 00:21:34,080
about the food and drug administration

517
00:21:34,080 --> 00:21:35,840
think about the federal aviation

518
00:21:35,840 --> 00:21:38,080
administration think about the federal

519
00:21:38,080 --> 00:21:40,080
communications commission

520
00:21:40,080 --> 00:21:42,159
the most important question for us to

521
00:21:42,159 --> 00:21:44,480
think about is this

522
00:21:44,480 --> 00:21:46,480
what would a digital regulatory

523
00:21:46,480 --> 00:21:48,960
commission look like

524
00:21:48,960 --> 00:21:51,280
what would its scope be how would it

525
00:21:51,280 --> 00:21:52,159
work

526
00:21:52,159 --> 00:21:55,360
would we be better served to place in

527
00:21:55,360 --> 00:21:57,440
the hands of people

528
00:21:57,440 --> 00:22:00,159
pursuant to the rule of law

529
00:22:00,159 --> 00:22:01,679
the ability to

530
00:22:01,679 --> 00:22:04,000
learn and master the facts for an

531
00:22:04,000 --> 00:22:07,520
industry and craft carefully

532
00:22:07,520 --> 00:22:09,600
very thoughtful rules

533
00:22:09,600 --> 00:22:11,760
is that a better future than asking a

534
00:22:11,760 --> 00:22:13,679
congress or a legislature or a

535
00:22:13,679 --> 00:22:17,840
parliament to go on a piecemeal basis

536
00:22:17,840 --> 00:22:19,039
and

537
00:22:19,039 --> 00:22:22,159
change each and every law

538
00:22:22,159 --> 00:22:24,840
separately and with less

539
00:22:24,840 --> 00:22:28,799
coordination one should always i think

540
00:22:28,799 --> 00:22:31,120
be a little nervous

541
00:22:31,120 --> 00:22:33,280
in inviting a government to be more

542
00:22:33,280 --> 00:22:36,400
powerful in regulating yourself

543
00:22:36,400 --> 00:22:38,320
but the truth is in a world where

544
00:22:38,320 --> 00:22:41,600
regulation is becoming a reality

545
00:22:41,600 --> 00:22:44,080
we need thoughtful regulation

546
00:22:44,080 --> 00:22:46,799
and this is probably the conversation we

547
00:22:46,799 --> 00:22:48,480
need to have

548
00:22:48,480 --> 00:22:52,720
in the years and in this decade ahead

549
00:22:52,720 --> 00:22:54,720
but because all of this is happening on

550
00:22:54,720 --> 00:22:56,720
a global basis we need a third thing as

551
00:22:56,720 --> 00:22:59,679
well we need coordination across borders

552
00:22:59,679 --> 00:23:01,600
we're not in great shape to be honest

553
00:23:01,600 --> 00:23:03,840
look at the privacy shield

554
00:23:03,840 --> 00:23:05,440
an issue that took four months to

555
00:23:05,440 --> 00:23:08,640
resolve five years ago has taken 18

556
00:23:08,640 --> 00:23:10,480
months this time things are getting in

557
00:23:10,480 --> 00:23:12,480
some ways more difficult rather than

558
00:23:12,480 --> 00:23:16,080
easier and oh my gosh

559
00:23:16,080 --> 00:23:18,400
just think about as you all i think

560
00:23:18,400 --> 00:23:20,320
often see

561
00:23:20,320 --> 00:23:23,120
what it means to work with technology

562
00:23:23,120 --> 00:23:25,919
where regulation is spreading so broadly

563
00:23:25,919 --> 00:23:28,559
and in so many legal fields

564
00:23:28,559 --> 00:23:30,240
i sometimes get up in the morning and i

565
00:23:30,240 --> 00:23:31,600
feel like i'm supposed to go to work

566
00:23:31,600 --> 00:23:34,400
with some giant spreadsheet in my head

567
00:23:34,400 --> 00:23:37,360
every row is an area of law every column

568
00:23:37,360 --> 00:23:39,440
is a jurisdiction that's adopted a new

569
00:23:39,440 --> 00:23:41,760
law every day there seems to be another

570
00:23:41,760 --> 00:23:44,960
column and that's just for one product

571
00:23:44,960 --> 00:23:47,440
every spreadsheet i'm sorry every

572
00:23:47,440 --> 00:23:50,799
product needs its own spreadsheet

573
00:23:50,799 --> 00:23:53,360
the world will not be able to manage

574
00:23:53,360 --> 00:23:55,760
this level of regulatory growth this

575
00:23:55,760 --> 00:23:58,880
creation of complexity without better

576
00:23:58,880 --> 00:24:01,600
coordination across borders

577
00:24:01,600 --> 00:24:03,600
and then finally we need one last thing

578
00:24:03,600 --> 00:24:05,440
as well maybe it's the most important

579
00:24:05,440 --> 00:24:07,679
thing it is definitely in my opinion

580
00:24:07,679 --> 00:24:11,919
where you all will be critical

581
00:24:11,919 --> 00:24:14,799
we need people who can think creatively

582
00:24:14,799 --> 00:24:17,039
we need people who can think across

583
00:24:17,039 --> 00:24:20,080
boundaries we need privacy regulators

584
00:24:20,080 --> 00:24:22,080
and professionals and practitioners who

585
00:24:22,080 --> 00:24:24,720
recognize that even privacy itself is

586
00:24:24,720 --> 00:24:27,200
much less siloed than it used to be

587
00:24:27,200 --> 00:24:29,120
some of the leading privacy issues of

588
00:24:29,120 --> 00:24:30,880
our time involve the intersection

589
00:24:30,880 --> 00:24:33,039
between say privacy and the protection

590
00:24:33,039 --> 00:24:35,600
of children with digital safety it's the

591
00:24:35,600 --> 00:24:37,840
intersection between privacy and cyber

592
00:24:37,840 --> 00:24:40,880
security or as you just heard

593
00:24:40,880 --> 00:24:43,120
the intersection of privacy and the

594
00:24:43,120 --> 00:24:45,520
protection of public health

595
00:24:45,520 --> 00:24:46,960
we need

596
00:24:46,960 --> 00:24:49,360
to think across the traditional

597
00:24:49,360 --> 00:24:51,919
intellectual boundaries that in some

598
00:24:51,919 --> 00:24:53,840
ways have perhaps

599
00:24:53,840 --> 00:24:56,000
too narrowly subscribed our thinking in

600
00:24:56,000 --> 00:24:58,240
the past

601
00:24:58,240 --> 00:25:00,240
two months ago i took my third trip to

602
00:25:00,240 --> 00:25:02,559
the vatican my first since covet and it

603
00:25:02,559 --> 00:25:05,600
to me is always a profound experience

604
00:25:05,600 --> 00:25:07,600
because more than anything else it

605
00:25:07,600 --> 00:25:10,640
stands for a simple proposition

606
00:25:10,640 --> 00:25:12,640
the world's great philosophers the

607
00:25:12,640 --> 00:25:15,520
world's great religious leaders

608
00:25:15,520 --> 00:25:19,039
the humanities the social sciences

609
00:25:19,039 --> 00:25:23,039
all have so much to contribute they all

610
00:25:23,039 --> 00:25:26,159
have so many views that need to be heard

611
00:25:26,159 --> 00:25:28,480
for us to think creatively

612
00:25:28,480 --> 00:25:31,440
and comprehensively the way the world

613
00:25:31,440 --> 00:25:33,039
will need

614
00:25:33,039 --> 00:25:35,760
in some i will say this this is not

615
00:25:35,760 --> 00:25:38,000
going to be easy

616
00:25:38,000 --> 00:25:40,000
i had a conversation with one of the

617
00:25:40,000 --> 00:25:42,400
senior most regulatory officials in

618
00:25:42,400 --> 00:25:44,880
europe a month ago and as that person

619
00:25:44,880 --> 00:25:46,640
said look the fundamental problem is

620
00:25:46,640 --> 00:25:49,440
straightforward democracies have waited

621
00:25:49,440 --> 00:25:52,159
so long to regulate this technology that

622
00:25:52,159 --> 00:25:54,320
now we need to catch up we need to go

623
00:25:54,320 --> 00:25:58,880
fast and to quote verbatim this will not

624
00:25:58,880 --> 00:26:01,600
be beautiful

625
00:26:01,600 --> 00:26:03,520
that may be the case but

626
00:26:03,520 --> 00:26:07,520
we don't want it to be ugly either

627
00:26:07,520 --> 00:26:10,000
we need to make it as beautiful as we

628
00:26:10,000 --> 00:26:11,120
can

629
00:26:11,120 --> 00:26:13,279
and at the end of the day while i think

630
00:26:13,279 --> 00:26:16,000
the challenges to doing this well are so

631
00:26:16,000 --> 00:26:18,320
complicated and formidable i also

632
00:26:18,320 --> 00:26:20,880
believe in a very optimistic way that

633
00:26:20,880 --> 00:26:24,880
the future can indeed be bright

634
00:26:24,880 --> 00:26:26,640
and i will end on a personal note

635
00:26:26,640 --> 00:26:30,799
because i think my personal experience

636
00:26:30,799 --> 00:26:32,240
represents

637
00:26:32,240 --> 00:26:33,679
the journey

638
00:26:33,679 --> 00:26:35,679
that one can take

639
00:26:35,679 --> 00:26:37,360
when i was in my early years at

640
00:26:37,360 --> 00:26:40,880
microsoft we faced the worst anti-trust

641
00:26:40,880 --> 00:26:44,480
legal decision that any company had been

642
00:26:44,480 --> 00:26:46,559
handed in decades

643
00:26:46,559 --> 00:26:50,000
literally a court order to break the

644
00:26:50,000 --> 00:26:53,600
company up when i became the company's

645
00:26:53,600 --> 00:26:56,159
general counsel in 2002 it became my

646
00:26:56,159 --> 00:26:57,600
task to try to go

647
00:26:57,600 --> 00:26:59,440
enter into peace agreements not just

648
00:26:59,440 --> 00:27:00,880
with governments around the world but

649
00:27:00,880 --> 00:27:02,799
with companies across the industry and

650
00:27:02,799 --> 00:27:05,120
it was a long and

651
00:27:05,120 --> 00:27:08,000
arduous and even painful tasks

652
00:27:08,000 --> 00:27:11,200
on this day in 2007 i got to stand up in

653
00:27:11,200 --> 00:27:14,240
front of 200 reporters and explain why

654
00:27:14,240 --> 00:27:16,480
this 200-page legal decision that had

655
00:27:16,480 --> 00:27:19,039
just been handed down didn't even have a

656
00:27:19,039 --> 00:27:21,919
footnote in our favor

657
00:27:21,919 --> 00:27:23,360
it was not

658
00:27:23,360 --> 00:27:26,159
an easy process

659
00:27:26,159 --> 00:27:28,240
but we came to terms with it and more

660
00:27:28,240 --> 00:27:31,200
than anything else i realized that you

661
00:27:31,200 --> 00:27:34,080
can find a way to navigate through the

662
00:27:34,080 --> 00:27:37,200
thicket of demands expectations

663
00:27:37,200 --> 00:27:39,360
that others have of you

664
00:27:39,360 --> 00:27:42,559
and when i look at my job today

665
00:27:42,559 --> 00:27:44,960
i am so happy

666
00:27:44,960 --> 00:27:47,520
that most of my time is spent

667
00:27:47,520 --> 00:27:50,080
on very different issues

668
00:27:50,080 --> 00:27:52,320
we focus on our mission on how to bring

669
00:27:52,320 --> 00:27:54,880
internet and broadband access to more

670
00:27:54,880 --> 00:27:57,039
people and promote skilling and create

671
00:27:57,039 --> 00:27:59,679
job opportunities how to put technology

672
00:27:59,679 --> 00:28:01,840
to work not just to protect ukraine but

673
00:28:01,840 --> 00:28:04,559
every democracy and the most fundamental

674
00:28:04,559 --> 00:28:06,880
rights around the world how to harness

675
00:28:06,880 --> 00:28:09,919
the power of technology to build a more

676
00:28:09,919 --> 00:28:12,000
sustainable future and perhaps most

677
00:28:12,000 --> 00:28:13,279
fundamentally

678
00:28:13,279 --> 00:28:15,760
to build technology that the world can

679
00:28:15,760 --> 00:28:16,880
trust

680
00:28:16,880 --> 00:28:19,679
starting with privacy

681
00:28:19,679 --> 00:28:24,240
this is what you all represent

682
00:28:24,240 --> 00:28:27,919
this is the foundation that you have

683
00:28:27,919 --> 00:28:29,039
built

684
00:28:29,039 --> 00:28:32,080
but it's not just a foundation to which

685
00:28:32,080 --> 00:28:35,120
the world can look for inspiration

686
00:28:35,120 --> 00:28:37,120
it is the future

687
00:28:37,120 --> 00:28:40,240
it is a future that can be bright if we

688
00:28:40,240 --> 00:28:43,520
lean in if we embrace a new mindset

689
00:28:43,520 --> 00:28:45,200
and we go forward

690
00:28:45,200 --> 00:28:46,559
together

691
00:28:46,559 --> 00:28:50,200
thank you very much

692
00:28:56,210 --> 00:28:59,479
[Music]

693
00:29:05,120 --> 00:29:07,520
so thank you brad smith for that very

694
00:29:07,520 --> 00:29:09,760
compelling and vehement case for

695
00:29:09,760 --> 00:29:12,880
regulation and new privacy rules that

696
00:29:12,880 --> 00:29:14,720
span the federal

697
00:29:14,720 --> 00:29:16,559
government so

698
00:29:16,559 --> 00:29:18,000
ironically

699
00:29:18,000 --> 00:29:19,679
i'm with a technology company that

700
00:29:19,679 --> 00:29:21,840
really aims to help organizations

701
00:29:21,840 --> 00:29:24,080
automate some of that regulation and

702
00:29:24,080 --> 00:29:26,640
compliance so my name is dimitri sirota

703
00:29:26,640 --> 00:29:28,960
i'm the ceo of big id

704
00:29:28,960 --> 00:29:31,679
this is my third time at

705
00:29:31,679 --> 00:29:34,360
ipp global congress i first came here in

706
00:29:34,360 --> 00:29:37,200
2018 as a vendor

707
00:29:37,200 --> 00:29:39,200
and it's incredible to see how far it's

708
00:29:39,200 --> 00:29:40,880
come just in the three years that i've

709
00:29:40,880 --> 00:29:42,799
been coming

710
00:29:42,799 --> 00:29:44,960
in 2018

711
00:29:44,960 --> 00:29:46,880
myself and my co-founder actually had to

712
00:29:46,880 --> 00:29:49,919
set up our own booth and technology for

713
00:29:49,919 --> 00:29:51,919
global regulations compliance were a new

714
00:29:51,919 --> 00:29:54,880
thing gdpr was just about to come out

715
00:29:54,880 --> 00:29:56,799
um and we were really part of a

716
00:29:56,799 --> 00:29:58,480
pioneering crew

717
00:29:58,480 --> 00:30:00,159
and it's amazing to see today not only

718
00:30:00,159 --> 00:30:01,760
with all of you

719
00:30:01,760 --> 00:30:03,360
here in the audience in terms of

720
00:30:03,360 --> 00:30:04,960
professionals

721
00:30:04,960 --> 00:30:06,720
working and helping organizations deal

722
00:30:06,720 --> 00:30:09,360
with some of these regulations and

723
00:30:09,360 --> 00:30:11,039
provide greater transparency to

724
00:30:11,039 --> 00:30:13,840
consumers and employees and and that but

725
00:30:13,840 --> 00:30:15,520
also seeing all of the new vendors and

726
00:30:15,520 --> 00:30:18,080
technologies out there that are aiming

727
00:30:18,080 --> 00:30:18,880
to

728
00:30:18,880 --> 00:30:22,880
help provide um greater controls um and

729
00:30:22,880 --> 00:30:25,120
uh transparency around how organizations

730
00:30:25,120 --> 00:30:27,760
collect and process and dispose data so

731
00:30:27,760 --> 00:30:29,520
uh it's been an amazing ride looking

732
00:30:29,520 --> 00:30:32,320
forward to see many more years here uh

733
00:30:32,320 --> 00:30:35,200
in dc with uh with you folks so without

734
00:30:35,200 --> 00:30:37,279
further ado i am going to introduce our

735
00:30:37,279 --> 00:30:40,399
next panel um i'm pleased to introduce

736
00:30:40,399 --> 00:30:42,880
neil richards a coke distinguished

737
00:30:42,880 --> 00:30:44,640
professor in law at washington

738
00:30:44,640 --> 00:30:47,120
university uh school of law he'll be the

739
00:30:47,120 --> 00:30:49,600
moderator and with him will be julia

740
00:30:49,600 --> 00:30:51,840
anguin editor-in-chief and founder of

741
00:30:51,840 --> 00:30:52,960
the markup

742
00:30:52,960 --> 00:30:54,799
and cecilia kang

743
00:30:54,799 --> 00:30:57,200
national technology correspondent for

744
00:30:57,200 --> 00:30:59,039
the new york times so without further

745
00:30:59,039 --> 00:31:03,320
ado i welcome them thank you

746
00:31:03,980 --> 00:31:10,080
[Music]

747
00:31:10,080 --> 00:31:13,080
so

748
00:31:17,860 --> 00:31:22,990
[Music]

749
00:31:24,480 --> 00:31:28,000
hello i'm i'm neil richards as he said

750
00:31:28,000 --> 00:31:30,480
and i'm delighted to be joined today by

751
00:31:30,480 --> 00:31:33,039
by two of the the most insightful

752
00:31:33,039 --> 00:31:35,600
articulate uh journalists working i

753
00:31:35,600 --> 00:31:37,279
think working at all but certainly in

754
00:31:37,279 --> 00:31:40,240
technology uh cecilia kong and julia

755
00:31:40,240 --> 00:31:43,360
anguin um i thought to start things off

756
00:31:43,360 --> 00:31:45,279
uh it'd be interesting to hear about the

757
00:31:45,279 --> 00:31:47,360
different perspectives that our two

758
00:31:47,360 --> 00:31:49,440
journalists bring from very different

759
00:31:49,440 --> 00:31:52,159
organizations the new york times and

760
00:31:52,159 --> 00:31:54,240
and and the markup so i'll start with uh

761
00:31:54,240 --> 00:31:56,720
with you julia uh how did you get to be

762
00:31:56,720 --> 00:31:59,120
running the markup and what is your beat

763
00:31:59,120 --> 00:32:01,120
uh or what is that the mark of speed and

764
00:32:01,120 --> 00:32:02,399
then i'll ask the same question of

765
00:32:02,399 --> 00:32:03,600
cecilia

766
00:32:03,600 --> 00:32:05,120
sure it's so great to be here by the way

767
00:32:05,120 --> 00:32:07,360
i've been coming to iapp for

768
00:32:07,360 --> 00:32:09,440
so many years and

769
00:32:09,440 --> 00:32:12,080
it's amazing how many people are here it

770
00:32:12,080 --> 00:32:14,240
used to be a couple hundred people in a

771
00:32:14,240 --> 00:32:17,679
hotel lobby if i remember correctly um i

772
00:32:17,679 --> 00:32:20,880
am a technology journalist and i um i

773
00:32:20,880 --> 00:32:23,360
grew up in silicon valley and actually

774
00:32:23,360 --> 00:32:24,240
thought

775
00:32:24,240 --> 00:32:28,000
that i would go into technology and i um

776
00:32:28,000 --> 00:32:29,679
i

777
00:32:29,679 --> 00:32:31,760
wanted to go into software but then i

778
00:32:31,760 --> 00:32:33,519
fell in love with journalism

779
00:32:33,519 --> 00:32:35,120
and flash forward i was at the wall

780
00:32:35,120 --> 00:32:36,399
street journal i think it was around

781
00:32:36,399 --> 00:32:38,480
2008

782
00:32:38,480 --> 00:32:40,960
and i realized that

783
00:32:40,960 --> 00:32:43,840
the industry that i covered software was

784
00:32:43,840 --> 00:32:44,960
um

785
00:32:44,960 --> 00:32:47,200
had changed you used to buy software in

786
00:32:47,200 --> 00:32:49,440
boxes it was shrink wrapped and you paid

787
00:32:49,440 --> 00:32:52,000
money for it and all of a sudden it was

788
00:32:52,000 --> 00:32:53,440
free

789
00:32:53,440 --> 00:32:55,679
and they were doing something with your

790
00:32:55,679 --> 00:32:57,440
data that seemed weird and i was like

791
00:32:57,440 --> 00:32:59,600
what is this new i really thought of it

792
00:32:59,600 --> 00:33:01,039
as a what is this new business model

793
00:33:01,039 --> 00:33:02,480
that's out there because it seems kind

794
00:33:02,480 --> 00:33:05,360
of creepy and so i started a series

795
00:33:05,360 --> 00:33:06,960
there called what they know at the wall

796
00:33:06,960 --> 00:33:08,799
street journal about privacy and like

797
00:33:08,799 --> 00:33:11,279
what was happening with your data

798
00:33:11,279 --> 00:33:12,320
and

799
00:33:12,320 --> 00:33:14,320
i eventually i thought it would be a

800
00:33:14,320 --> 00:33:17,600
one-year series and that's uh 12 years

801
00:33:17,600 --> 00:33:20,080
later and i'm still on the same story

802
00:33:20,080 --> 00:33:21,679
essentially

803
00:33:21,679 --> 00:33:23,120
i i did three years of that

804
00:33:23,120 --> 00:33:24,399
investigation at the wall street journal

805
00:33:24,399 --> 00:33:26,559
then i went to pro publica and then i

806
00:33:26,559 --> 00:33:28,320
eventually felt that there was just a

807
00:33:28,320 --> 00:33:30,080
need for there to be

808
00:33:30,080 --> 00:33:32,159
a newsroom really devoted to this

809
00:33:32,159 --> 00:33:36,399
particular question of the world of i

810
00:33:36,399 --> 00:33:37,760
don't like to call it privacy i call it

811
00:33:37,760 --> 00:33:41,120
the data exploitation market um and so i

812
00:33:41,120 --> 00:33:42,640
i felt like there needed to be some

813
00:33:42,640 --> 00:33:44,399
place that really specialized in writing

814
00:33:44,399 --> 00:33:46,240
about this and so i founded the markup

815
00:33:46,240 --> 00:33:47,919
which is a non-profit newsroom and if

816
00:33:47,919 --> 00:33:49,559
you haven't been to our website it's

817
00:33:49,559 --> 00:33:51,200
themarkup.org

818
00:33:51,200 --> 00:33:54,880
and we write about all of the issues

819
00:33:54,880 --> 00:33:57,760
that technology enables privacy being

820
00:33:57,760 --> 00:34:00,000
one of them but we also write a lot

821
00:34:00,000 --> 00:34:03,600
about algorithmic biases and other

822
00:34:03,600 --> 00:34:07,120
ways that the world of digitization has

823
00:34:07,120 --> 00:34:10,399
allowed policy decisions to be embedded

824
00:34:10,399 --> 00:34:12,960
into opaque black box systems that you

825
00:34:12,960 --> 00:34:16,079
can't examine and i think that that is

826
00:34:16,079 --> 00:34:18,000
all enabled by

827
00:34:18,000 --> 00:34:20,239
the incredible massive collection of

828
00:34:20,239 --> 00:34:21,839
data about us

829
00:34:21,839 --> 00:34:24,560
that builds and empowers all of those

830
00:34:24,560 --> 00:34:26,639
systems and so we have a newsroom that

831
00:34:26,639 --> 00:34:27,679
is half

832
00:34:27,679 --> 00:34:30,239
engineers and half journalists and so we

833
00:34:30,239 --> 00:34:32,480
take a very technological approach to

834
00:34:32,480 --> 00:34:35,520
our coverage using advanced techniques

835
00:34:35,520 --> 00:34:38,320
to examine this world

836
00:34:38,320 --> 00:34:39,360
so that was a long answer

837
00:34:39,360 --> 00:34:41,839
[Laughter]

838
00:34:41,839 --> 00:34:43,839
so that's a great answer and i i love

839
00:34:43,839 --> 00:34:46,159
the way you describe um

840
00:34:46,159 --> 00:34:48,239
this term that you you you just

841
00:34:48,239 --> 00:34:51,359
mentioned data expectation market um

842
00:34:51,359 --> 00:34:53,199
instead of privacy

843
00:34:53,199 --> 00:34:56,239
because i view a lot of my job

844
00:34:56,239 --> 00:34:58,880
as just telling readers what's really

845
00:34:58,880 --> 00:35:00,800
going on you know as a technology

846
00:35:00,800 --> 00:35:02,720
journalist and i think this is

847
00:35:02,720 --> 00:35:04,480
and that question the answer to that

848
00:35:04,480 --> 00:35:06,560
question has really changed over time

849
00:35:06,560 --> 00:35:07,920
we've seen a real evolution in

850
00:35:07,920 --> 00:35:09,920
technology reporting and just to back up

851
00:35:09,920 --> 00:35:11,520
i'm a technology reporter based in

852
00:35:11,520 --> 00:35:13,760
washington for the new york times and

853
00:35:13,760 --> 00:35:15,520
before joining the times about seven

854
00:35:15,520 --> 00:35:16,960
years ago i was at the washington post

855
00:35:16,960 --> 00:35:18,320
pretty much doing the same thing for

856
00:35:18,320 --> 00:35:20,480
about a decade so i've also been

857
00:35:20,480 --> 00:35:22,480
following a lot of the same stories for

858
00:35:22,480 --> 00:35:23,680
a long time

859
00:35:23,680 --> 00:35:25,920
and i can humbly say we're still

860
00:35:25,920 --> 00:35:27,440
figuring it out and we're still trying

861
00:35:27,440 --> 00:35:29,839
to tell you what's really going on and

862
00:35:29,839 --> 00:35:32,480
just to flash back to an anecdote that

863
00:35:32,480 --> 00:35:34,880
that i think illustrates that is around

864
00:35:34,880 --> 00:35:36,800
2010

865
00:35:36,800 --> 00:35:39,680
i remember having um being in a press

866
00:35:39,680 --> 00:35:41,520
conference actually with

867
00:35:41,520 --> 00:35:45,440
then facebook now meta um executives and

868
00:35:45,440 --> 00:35:47,440
they kept saying the executives in this

869
00:35:47,440 --> 00:35:49,760
call saying we have the best privacy

870
00:35:49,760 --> 00:35:52,720
controls of any company in the world we

871
00:35:52,720 --> 00:35:55,280
have the most like the mo we give

872
00:35:55,280 --> 00:35:57,599
consumers the most control

873
00:35:57,599 --> 00:35:59,920
over all of their their data

874
00:35:59,920 --> 00:36:01,760
and they would say that on the hill when

875
00:36:01,760 --> 00:36:04,480
they would visit members of congress and

876
00:36:04,480 --> 00:36:07,440
what we really realized is that

877
00:36:07,440 --> 00:36:09,040
we didn't add that to what i realized

878
00:36:09,040 --> 00:36:10,800
now is that at that time i didn't really

879
00:36:10,800 --> 00:36:12,800
quite understand what that meant it was

880
00:36:12,800 --> 00:36:15,359
almost a distraction from what was

881
00:36:15,359 --> 00:36:17,839
really happening yes there were tons of

882
00:36:17,839 --> 00:36:19,920
different drop-down

883
00:36:19,920 --> 00:36:21,920
options that you'd have on facebook on

884
00:36:21,920 --> 00:36:23,839
how who could see your data but what

885
00:36:23,839 --> 00:36:25,680
that was obscuring or distracting from

886
00:36:25,680 --> 00:36:27,839
was the fact that the company was

887
00:36:27,839 --> 00:36:30,160
gobbling up so much data and that was

888
00:36:30,160 --> 00:36:32,000
and that advertisers were getting access

889
00:36:32,000 --> 00:36:34,400
to that data and it took quite some time

890
00:36:34,400 --> 00:36:36,560
for folks on the hill for journalists to

891
00:36:36,560 --> 00:36:38,480
realize that's actually the ball we

892
00:36:38,480 --> 00:36:41,280
should be chasing not how many controls

893
00:36:41,280 --> 00:36:42,960
every consumer has

894
00:36:42,960 --> 00:36:45,520
and i use that i mentioned that anecdote

895
00:36:45,520 --> 00:36:47,119
to describe that these are the kinds of

896
00:36:47,119 --> 00:36:48,880
things that we're trying to figure out

897
00:36:48,880 --> 00:36:51,119
to look behind the messaging oftentimes

898
00:36:51,119 --> 00:36:53,920
of companies to tell consumers how does

899
00:36:53,920 --> 00:36:56,880
this how do these decisions and the

900
00:36:56,880 --> 00:36:58,560
technologies that are being created by

901
00:36:58,560 --> 00:37:00,560
these incredibly powerful

902
00:37:00,560 --> 00:37:03,280
companies affecting you as individuals

903
00:37:03,280 --> 00:37:05,040
and so we're just digging constantly and

904
00:37:05,040 --> 00:37:07,200
we're still in the same story

905
00:37:07,200 --> 00:37:08,800
i completely agree with you about

906
00:37:08,800 --> 00:37:11,359
control um as an academic whose job it

907
00:37:11,359 --> 00:37:14,000
is to be uh insightful and perhaps

908
00:37:14,000 --> 00:37:16,480
critical of the world that we that we

909
00:37:16,480 --> 00:37:19,119
see around us um i agree that i think i

910
00:37:19,119 --> 00:37:20,560
think control is a myth it's an argument

911
00:37:20,560 --> 00:37:22,000
that i've explored in some of my writing

912
00:37:22,000 --> 00:37:23,200
including at the risk of shameless

913
00:37:23,200 --> 00:37:25,520
self-promotion my new book why privacy

914
00:37:25,520 --> 00:37:28,720
matters but putting that to one side um

915
00:37:28,720 --> 00:37:31,760
in identifying things like the the the

916
00:37:31,760 --> 00:37:34,079
smoke screen that is

917
00:37:34,079 --> 00:37:36,400
informational self-determination or

918
00:37:36,400 --> 00:37:39,599
putting users in control of their data

919
00:37:39,599 --> 00:37:41,920
how do you guys approach

920
00:37:41,920 --> 00:37:44,240
your role of informing the public if

921
00:37:44,240 --> 00:37:46,000
that is indeed what you see your role as

922
00:37:46,000 --> 00:37:48,560
and but assume that it is how do you

923
00:37:48,560 --> 00:37:52,079
figure out how which stories to

924
00:37:52,079 --> 00:37:54,880
prioritize which concepts to try and get

925
00:37:54,880 --> 00:37:56,320
in people's minds

926
00:37:56,320 --> 00:37:57,760
and how do you go about

927
00:37:57,760 --> 00:37:58,880
crafting

928
00:37:58,880 --> 00:38:01,680
those stories so that your readers can

929
00:38:01,680 --> 00:38:04,079
understand what what they need to

930
00:38:04,079 --> 00:38:07,040
understand what's going on

931
00:38:07,359 --> 00:38:08,800
well it's a great question because i

932
00:38:08,800 --> 00:38:10,560
think when i first started writing about

933
00:38:10,560 --> 00:38:14,560
privacy in 2010

934
00:38:14,800 --> 00:38:16,640
it was actually you just had to convince

935
00:38:16,640 --> 00:38:18,079
readers that there was even something to

936
00:38:18,079 --> 00:38:19,440
worry about really people weren't

937
00:38:19,440 --> 00:38:22,079
focused on this issue and so

938
00:38:22,079 --> 00:38:23,280
um

939
00:38:23,280 --> 00:38:25,520
there was a lot of

940
00:38:25,520 --> 00:38:27,839
why this matters and that's still true

941
00:38:27,839 --> 00:38:30,320
to some degree but i feel like the

942
00:38:30,320 --> 00:38:32,720
people now understand a lot better than

943
00:38:32,720 --> 00:38:33,839
they did

944
00:38:33,839 --> 00:38:34,720
that

945
00:38:34,720 --> 00:38:36,720
in all their data's being collected it's

946
00:38:36,720 --> 00:38:38,640
being used for weird things cambridge

947
00:38:38,640 --> 00:38:40,320
analytica was a big wake-up call for

948
00:38:40,320 --> 00:38:43,280
people and so i think that the

949
00:38:43,280 --> 00:38:44,240
the

950
00:38:44,240 --> 00:38:46,480
issue for journalists now is

951
00:38:46,480 --> 00:38:48,640
to be a little bit more precise about

952
00:38:48,640 --> 00:38:50,880
the harm about

953
00:38:50,880 --> 00:38:53,680
um exactly what types of cutting-edge

954
00:38:53,680 --> 00:38:55,760
techniques are being used against them

955
00:38:55,760 --> 00:38:58,400
one thing that we did at the markup is

956
00:38:58,400 --> 00:39:00,400
we realized that

957
00:39:00,400 --> 00:39:04,160
there was so much happening um and we

958
00:39:04,160 --> 00:39:06,880
can't a lot of people um

959
00:39:06,880 --> 00:39:08,720
you might read a story about some app

960
00:39:08,720 --> 00:39:10,079
that's stealing your data but you don't

961
00:39:10,079 --> 00:39:11,760
use that app so it doesn't really matter

962
00:39:11,760 --> 00:39:14,079
to you so we thought let's build a tool

963
00:39:14,079 --> 00:39:16,320
where readers can actually learn about

964
00:39:16,320 --> 00:39:17,839
whatever they want so we built this

965
00:39:17,839 --> 00:39:19,200
thing called blacklight and if you

966
00:39:19,200 --> 00:39:21,599
haven't gone to it it's a tool where you

967
00:39:21,599 --> 00:39:24,800
just enter the url of any website and we

968
00:39:24,800 --> 00:39:27,200
actually run a real-time forensic

969
00:39:27,200 --> 00:39:30,560
privacy analysis at that moment of what

970
00:39:30,560 --> 00:39:32,880
types of techniques it's using so it

971
00:39:32,880 --> 00:39:34,960
tells you about how many cookies and how

972
00:39:34,960 --> 00:39:36,079
many

973
00:39:36,079 --> 00:39:38,079
trackers and which companies but it also

974
00:39:38,079 --> 00:39:39,920
tells you things that you wouldn't

975
00:39:39,920 --> 00:39:41,040
really know

976
00:39:41,040 --> 00:39:43,520
that are pretty advanced like are they

977
00:39:43,520 --> 00:39:44,960
doing what's called session recording

978
00:39:44,960 --> 00:39:46,720
where they record every movement of your

979
00:39:46,720 --> 00:39:49,119
bra of your mouse and where you scroll

980
00:39:49,119 --> 00:39:51,119
on the page and send like a little video

981
00:39:51,119 --> 00:39:54,000
of your actions back are they using some

982
00:39:54,000 --> 00:39:56,160
techniques like the google remarketing

983
00:39:56,160 --> 00:39:58,880
cookie which basically is

984
00:39:58,880 --> 00:40:01,839
tracking you across other websites um in

985
00:40:01,839 --> 00:40:03,440
addition to the google analytics that

986
00:40:03,440 --> 00:40:06,480
you're on that page and so it really is

987
00:40:06,480 --> 00:40:08,880
an amazing way to see how different

988
00:40:08,880 --> 00:40:11,200
websites act and people write to us all

989
00:40:11,200 --> 00:40:12,720
the time like you know

990
00:40:12,720 --> 00:40:14,800
i was able to check my kids school and i

991
00:40:14,800 --> 00:40:16,319
saw that they were doing this thing and

992
00:40:16,319 --> 00:40:17,920
that's really wrong and i wrote to the

993
00:40:17,920 --> 00:40:19,760
principal and they changed it and so i

994
00:40:19,760 --> 00:40:22,079
feel like we've given people a chance to

995
00:40:22,079 --> 00:40:24,640
do their own advocacy around these

996
00:40:24,640 --> 00:40:27,200
issues and i think that is what people

997
00:40:27,200 --> 00:40:28,960
want is they want to know a little bit

998
00:40:28,960 --> 00:40:30,960
more precisely not just generally bad

999
00:40:30,960 --> 00:40:32,240
scary things are happening but what bad

1000
00:40:32,240 --> 00:40:35,119
scary things are happening to me

1001
00:40:35,119 --> 00:40:37,200
yeah i would say that's it's a real

1002
00:40:37,200 --> 00:40:39,920
challenge in writing about privacy is

1003
00:40:39,920 --> 00:40:41,839
that it can feel very abstract and that

1004
00:40:41,839 --> 00:40:44,160
the the harm does not feel immediate nor

1005
00:40:44,160 --> 00:40:46,640
does it feel tangible for for a lot of

1006
00:40:46,640 --> 00:40:47,839
readers so

1007
00:40:47,839 --> 00:40:50,480
um looking for the stories that can be

1008
00:40:50,480 --> 00:40:54,000
animated by real life anecdotes

1009
00:40:54,000 --> 00:40:55,520
you know how

1010
00:40:55,520 --> 00:40:58,240
how a new technology tool

1011
00:40:58,240 --> 00:41:01,440
a new collaboration between companies

1012
00:41:01,440 --> 00:41:04,000
a regulatory decision translates or has

1013
00:41:04,000 --> 00:41:06,400
a downwind effect and affects real

1014
00:41:06,400 --> 00:41:07,599
individuals

1015
00:41:07,599 --> 00:41:11,200
that's sort of the the the magic sort of

1016
00:41:11,200 --> 00:41:12,880
rubric we're trying to

1017
00:41:12,880 --> 00:41:14,800
look through and trying to find stories

1018
00:41:14,800 --> 00:41:16,240
through which to write

1019
00:41:16,240 --> 00:41:18,720
about it's hard it's really hard and

1020
00:41:18,720 --> 00:41:20,480
we're talking backstage about for

1021
00:41:20,480 --> 00:41:21,920
example even

1022
00:41:21,920 --> 00:41:24,880
cambridge analytica is a story which was

1023
00:41:24,880 --> 00:41:27,440
a watershed moment incredibly important

1024
00:41:27,440 --> 00:41:29,680
to really and surfaced a lot of

1025
00:41:29,680 --> 00:41:32,000
concern and interest in business models

1026
00:41:32,000 --> 00:41:34,319
and decisions from like decades ago

1027
00:41:34,319 --> 00:41:35,760
about opening up a platform like

1028
00:41:35,760 --> 00:41:38,079
facebook and how data is being used

1029
00:41:38,079 --> 00:41:39,280
differently

1030
00:41:39,280 --> 00:41:41,280
it takes like five paragraphs to explain

1031
00:41:41,280 --> 00:41:43,040
even what cape breton look at is and it

1032
00:41:43,040 --> 00:41:45,520
can be for some readers it's kind of

1033
00:41:45,520 --> 00:41:47,119
something that either makes their eyes

1034
00:41:47,119 --> 00:41:48,720
glaze over or just goes over their head

1035
00:41:48,720 --> 00:41:52,000
and the challenge is to we we believe at

1036
00:41:52,000 --> 00:41:54,800
the times is our service is really to

1037
00:41:54,800 --> 00:41:57,520
explain to the public in very accessible

1038
00:41:57,520 --> 00:42:00,560
ways why they should care what's really

1039
00:42:00,560 --> 00:42:02,560
going on and

1040
00:42:02,560 --> 00:42:03,839
sometimes the best way is to do it

1041
00:42:03,839 --> 00:42:04,880
through anecdotes through real

1042
00:42:04,880 --> 00:42:07,119
individuals but you know in terms of

1043
00:42:07,119 --> 00:42:09,440
trying to find the stories there's

1044
00:42:09,440 --> 00:42:12,240
there are limits frankly in a newspaper

1045
00:42:12,240 --> 00:42:14,480
story and this is one reason why we

1046
00:42:14,480 --> 00:42:16,160
wrote our book my co-author and ice

1047
00:42:16,160 --> 00:42:17,119
because

1048
00:42:17,119 --> 00:42:19,040
we realized that there was so much more

1049
00:42:19,040 --> 00:42:20,880
than the one-off stories to explain what

1050
00:42:20,880 --> 00:42:24,560
was really happening at now meta

1051
00:42:24,560 --> 00:42:25,760
we needed to

1052
00:42:25,760 --> 00:42:28,240
we wanted to and we needed to explain to

1053
00:42:28,240 --> 00:42:31,760
readers what the business model was and

1054
00:42:31,760 --> 00:42:33,359
those are the kinds of things again that

1055
00:42:33,359 --> 00:42:34,800
is sort of a work in progress that all

1056
00:42:34,800 --> 00:42:36,319
journalists are doing a fantastic job

1057
00:42:36,319 --> 00:42:38,240
and i should say julia was like a

1058
00:42:38,240 --> 00:42:40,400
pioneer her wall street journal story

1059
00:42:40,400 --> 00:42:42,480
series was like that opened everyone's

1060
00:42:42,480 --> 00:42:44,640
eyes that they should care it's that

1061
00:42:44,640 --> 00:42:46,880
kind of evolution that took place to get

1062
00:42:46,880 --> 00:42:48,640
to the point where we can now

1063
00:42:48,640 --> 00:42:51,119
assume that consumers have a readers

1064
00:42:51,119 --> 00:42:53,280
have a baseline knowledge of data

1065
00:42:53,280 --> 00:42:55,440
privacy and what that means and build on

1066
00:42:55,440 --> 00:42:58,240
that in terms of news stories it's

1067
00:42:58,240 --> 00:43:01,200
interesting that the the process of of

1068
00:43:01,200 --> 00:43:03,839
public education um and the the the

1069
00:43:03,839 --> 00:43:06,000
level of sophistication with which the

1070
00:43:06,000 --> 00:43:09,200
public can talk about privacy today um

1071
00:43:09,200 --> 00:43:10,880
we're no longer having the conversation

1072
00:43:10,880 --> 00:43:13,599
is privacy out of date or is it people

1073
00:43:13,599 --> 00:43:15,920
get it they seem to they seem to care

1074
00:43:15,920 --> 00:43:17,280
because they do care because they

1075
00:43:17,280 --> 00:43:18,640
realize that it's about avoiding harm

1076
00:43:18,640 --> 00:43:19,920
and it's also about avoiding

1077
00:43:19,920 --> 00:43:22,240
manipulation and the data confers power

1078
00:43:22,240 --> 00:43:24,240
um but along the lines of your book

1079
00:43:24,240 --> 00:43:26,480
you've both written long-form pieces of

1080
00:43:26,480 --> 00:43:29,359
journalism in books your book an ugly

1081
00:43:29,359 --> 00:43:32,319
truth and and julia's book um dragnet

1082
00:43:32,319 --> 00:43:34,000
nation but both of which are highly

1083
00:43:34,000 --> 00:43:36,640
recommended wonderful treatments how do

1084
00:43:36,640 --> 00:43:38,800
you see the relationship between the

1085
00:43:38,800 --> 00:43:40,400
long-form journalism

1086
00:43:40,400 --> 00:43:41,520
in a book

1087
00:43:41,520 --> 00:43:43,920
versus the the the shorter form

1088
00:43:43,920 --> 00:43:45,200
journalism of

1089
00:43:45,200 --> 00:43:46,800
of you know new

1090
00:43:46,800 --> 00:43:48,560
traditional or in julia's case some

1091
00:43:48,560 --> 00:43:50,079
non-traditional but

1092
00:43:50,079 --> 00:43:51,680
news stories that

1093
00:43:51,680 --> 00:43:52,800
that are

1094
00:43:52,800 --> 00:43:55,839
more ephemeral

1095
00:43:56,319 --> 00:43:58,240
yeah i mean my book uh dragnet nation

1096
00:43:58,240 --> 00:44:01,040
came out in 2014 and it's so hilarious

1097
00:44:01,040 --> 00:44:04,079
to me now because i felt at the time

1098
00:44:04,079 --> 00:44:07,280
that i was writing a pretty

1099
00:44:07,280 --> 00:44:09,599
you know overheated like

1100
00:44:09,599 --> 00:44:12,240
things are gonna be bad you guys need to

1101
00:44:12,240 --> 00:44:14,319
wake up call and then when i look back

1102
00:44:14,319 --> 00:44:18,319
at it i was actually under paranoid like

1103
00:44:18,319 --> 00:44:21,280
i did not see some of the things coming

1104
00:44:21,280 --> 00:44:23,119
i did not see how quickly facial

1105
00:44:23,119 --> 00:44:25,520
recognition was going to become

1106
00:44:25,520 --> 00:44:28,400
mainstream and used ubiquitously and and

1107
00:44:28,400 --> 00:44:30,240
certainly not in like these crazy ways

1108
00:44:30,240 --> 00:44:32,560
like with clearview ai where just

1109
00:44:32,560 --> 00:44:35,920
wild and unregulated scraping of photos

1110
00:44:35,920 --> 00:44:37,920
from the internet and

1111
00:44:37,920 --> 00:44:41,119
and so i think back on that book as like

1112
00:44:41,119 --> 00:44:43,359
wow

1113
00:44:43,599 --> 00:44:45,280
i was trying to i was trying to get

1114
00:44:45,280 --> 00:44:46,319
people

1115
00:44:46,319 --> 00:44:48,000
interested in this topic and the way i

1116
00:44:48,000 --> 00:44:50,160
did it was by trying to protect my own

1117
00:44:50,160 --> 00:44:52,240
privacy i spent a year trying all these

1118
00:44:52,240 --> 00:44:54,400
different techniques using burner phones

1119
00:44:54,400 --> 00:44:57,359
using tor web browser using lots of

1120
00:44:57,359 --> 00:44:58,800
things that most of you are familiar

1121
00:44:58,800 --> 00:45:00,960
with probably and

1122
00:45:00,960 --> 00:45:02,880
the point of that exercise was not

1123
00:45:02,880 --> 00:45:04,400
actually to succeed

1124
00:45:04,400 --> 00:45:07,280
it was to show the reader that this is

1125
00:45:07,280 --> 00:45:10,079
no way to live and maybe regulation or

1126
00:45:10,079 --> 00:45:11,599
some other

1127
00:45:11,599 --> 00:45:13,359
way that we can handle this because

1128
00:45:13,359 --> 00:45:15,280
trying to deal with it as an individual

1129
00:45:15,280 --> 00:45:17,839
problem is not really solvable and that

1130
00:45:17,839 --> 00:45:19,280
was the question that i was trying to

1131
00:45:19,280 --> 00:45:21,359
answer that i couldn't answer in my

1132
00:45:21,359 --> 00:45:23,920
journalism right was the question that

1133
00:45:23,920 --> 00:45:25,839
everyone asked me the reason i wanted to

1134
00:45:25,839 --> 00:45:26,960
write that book was they were like well

1135
00:45:26,960 --> 00:45:28,400
why should i care

1136
00:45:28,400 --> 00:45:31,920
i don't have anything to hide i um i use

1137
00:45:31,920 --> 00:45:32,800
you know

1138
00:45:32,800 --> 00:45:35,280
this one web browser so i'm safe and i

1139
00:45:35,280 --> 00:45:36,560
wanted to explain that this is a

1140
00:45:36,560 --> 00:45:38,560
collective problem and it needs

1141
00:45:38,560 --> 00:45:40,640
collective solutions and i'm going to

1142
00:45:40,640 --> 00:45:42,160
show you by trying to pursue the

1143
00:45:42,160 --> 00:45:43,680
individualistic solution and show you

1144
00:45:43,680 --> 00:45:46,160
how failed it is and so i think that's

1145
00:45:46,160 --> 00:45:48,480
the purpose of these books is to answer

1146
00:45:48,480 --> 00:45:50,640
those bigger meta questions that are in

1147
00:45:50,640 --> 00:45:51,680
that are

1148
00:45:51,680 --> 00:45:53,200
raised by the reporting that are really

1149
00:45:53,200 --> 00:45:55,839
hard to answer in any of these formats

1150
00:45:55,839 --> 00:45:57,839
in a long form investigative piece or

1151
00:45:57,839 --> 00:45:59,040
even a short

1152
00:45:59,040 --> 00:46:01,200
explainer article and cecilia's book is

1153
00:46:01,200 --> 00:46:03,440
about the massive question so to speak

1154
00:46:03,440 --> 00:46:04,880
yeah yeah no

1155
00:46:04,880 --> 00:46:08,079
pun intended so they um

1156
00:46:08,079 --> 00:46:09,680
so yeah our

1157
00:46:09,680 --> 00:46:11,599
i agree with everything that julie just

1158
00:46:11,599 --> 00:46:15,760
said for for us our book um was actually

1159
00:46:15,760 --> 00:46:17,520
born from our reporting at the new york

1160
00:46:17,520 --> 00:46:18,480
times

1161
00:46:18,480 --> 00:46:20,720
we found that we were in many ways

1162
00:46:20,720 --> 00:46:22,240
writing the same story over and over

1163
00:46:22,240 --> 00:46:24,480
again when it came to meta

1164
00:46:24,480 --> 00:46:25,680
it was

1165
00:46:25,680 --> 00:46:28,079
it was big scandal or crisis that

1166
00:46:28,079 --> 00:46:30,400
surfaces into the public

1167
00:46:30,400 --> 00:46:33,200
you know huge cry of apology from an

1168
00:46:33,200 --> 00:46:35,839
executive promises to do better

1169
00:46:35,839 --> 00:46:37,119
and then

1170
00:46:37,119 --> 00:46:38,720
repeat the cycle

1171
00:46:38,720 --> 00:46:40,000
and

1172
00:46:40,000 --> 00:46:42,640
we found ourselves shira frankel and i

1173
00:46:42,640 --> 00:46:46,400
asking ourselves as well as you know

1174
00:46:46,400 --> 00:46:48,400
the smart people who we turn to for our

1175
00:46:48,400 --> 00:46:50,000
sources over and over

1176
00:46:50,000 --> 00:46:51,119
is this

1177
00:46:51,119 --> 00:46:53,200
are these accidental or is there is

1178
00:46:53,200 --> 00:46:54,880
there some sort of

1179
00:46:54,880 --> 00:46:57,440
idea or like some sort of theory behind

1180
00:46:57,440 --> 00:46:59,280
these this pattern

1181
00:46:59,280 --> 00:47:01,200
and that was our animating question in

1182
00:47:01,200 --> 00:47:03,040
terms of asking the big meta question

1183
00:47:03,040 --> 00:47:06,000
was like what is behind this this series

1184
00:47:06,000 --> 00:47:08,240
of apologies over and over again

1185
00:47:08,240 --> 00:47:10,880
and what we found is actually it was not

1186
00:47:10,880 --> 00:47:12,960
really a frankenstein's monster getting

1187
00:47:12,960 --> 00:47:14,720
away from its creator story that was

1188
00:47:14,720 --> 00:47:16,319
totally unpredictable in fact the

1189
00:47:16,319 --> 00:47:18,880
machinery of facebook being the business

1190
00:47:18,880 --> 00:47:20,720
model and the technology were working as

1191
00:47:20,720 --> 00:47:21,760
intended

1192
00:47:21,760 --> 00:47:24,240
and that was sort of our thesis and

1193
00:47:24,240 --> 00:47:25,760
that's what we came up with after

1194
00:47:25,760 --> 00:47:27,920
interviewing so many hundreds of people

1195
00:47:27,920 --> 00:47:30,480
and so that was that we could not do

1196
00:47:30,480 --> 00:47:32,880
necessarily in a newspaper story but was

1197
00:47:32,880 --> 00:47:35,599
absolutely developed from the corpus of

1198
00:47:35,599 --> 00:47:37,359
work that we had developed at the new

1199
00:47:37,359 --> 00:47:39,040
york times at the washington post and

1200
00:47:39,040 --> 00:47:40,720
for her as a foreign correspondent and

1201
00:47:40,720 --> 00:47:42,880
so many other places and so it all built

1202
00:47:42,880 --> 00:47:44,640
on each other but the book

1203
00:47:44,640 --> 00:47:46,880
the service i think to julia's point as

1204
00:47:46,880 --> 00:47:49,520
well that a book provides is to connect

1205
00:47:49,520 --> 00:47:51,839
the dots also because you're reading all

1206
00:47:51,839 --> 00:47:53,599
these stories and you're thinking okay i

1207
00:47:53,599 --> 00:47:55,280
think i should feel bad about

1208
00:47:55,280 --> 00:47:56,880
edward snowden i think i should feel bad

1209
00:47:56,880 --> 00:47:58,720
about you know cambridge analytica i

1210
00:47:58,720 --> 00:48:00,319
think i should feel bad about these

1211
00:48:00,319 --> 00:48:01,839
things that we're reading about off and

1212
00:48:01,839 --> 00:48:04,559
on but how does this all what's the

1213
00:48:04,559 --> 00:48:07,280
method or the theory behind this and it

1214
00:48:07,280 --> 00:48:08,960
was kind of clear it was the business

1215
00:48:08,960 --> 00:48:11,280
model

1216
00:48:12,240 --> 00:48:13,839
what do you

1217
00:48:13,839 --> 00:48:15,520
what are you working on now what what

1218
00:48:15,520 --> 00:48:17,520
what issues

1219
00:48:17,520 --> 00:48:19,440
or what will you be working on what are

1220
00:48:19,440 --> 00:48:22,000
the issues that you guys think

1221
00:48:22,000 --> 00:48:23,839
are going to be the ones that are that

1222
00:48:23,839 --> 00:48:26,480
are worth covering in depth the ones

1223
00:48:26,480 --> 00:48:28,240
that are that

1224
00:48:28,240 --> 00:48:30,720
sorry yeah the capacity to capture the

1225
00:48:30,720 --> 00:48:32,319
public imagination

1226
00:48:32,319 --> 00:48:35,760
um as we as as you hope to advance this

1227
00:48:35,760 --> 00:48:38,000
public understanding in the service of

1228
00:48:38,000 --> 00:48:40,160
of advancing informed public debate and

1229
00:48:40,160 --> 00:48:42,240
deliberation on on the sorts of

1230
00:48:42,240 --> 00:48:44,079
regulatory questions that brad smith was

1231
00:48:44,079 --> 00:48:45,599
just talking about

1232
00:48:45,599 --> 00:48:46,640
i mean i think we're in a really

1233
00:48:46,640 --> 00:48:49,760
interesting time as brad

1234
00:48:49,760 --> 00:48:53,280
talked about we're on the cusp of

1235
00:48:53,280 --> 00:48:55,040
regulation and

1236
00:48:55,040 --> 00:48:56,960
tech policy

1237
00:48:56,960 --> 00:48:59,920
in the u.s which has been really stalled

1238
00:48:59,920 --> 00:49:01,680
is now happening it's not happening at

1239
00:49:01,680 --> 00:49:03,280
the federal level but it's happening at

1240
00:49:03,280 --> 00:49:05,760
the state level and state after state

1241
00:49:05,760 --> 00:49:08,800
are passing privacy laws and

1242
00:49:08,800 --> 00:49:10,960
the reality is that

1243
00:49:10,960 --> 00:49:13,119
those are probably going to

1244
00:49:13,119 --> 00:49:16,240
conflict with each other and create the

1245
00:49:16,240 --> 00:49:18,800
need for a federal harmonization

1246
00:49:18,800 --> 00:49:21,520
and so what's happening is the policy is

1247
00:49:21,520 --> 00:49:23,040
being written

1248
00:49:23,040 --> 00:49:24,319
in places where people aren't paying

1249
00:49:24,319 --> 00:49:27,040
attention right most state capitals

1250
00:49:27,040 --> 00:49:28,720
their journalism has been totally

1251
00:49:28,720 --> 00:49:31,040
depleted you know and it's worth noting

1252
00:49:31,040 --> 00:49:32,079
that um

1253
00:49:32,079 --> 00:49:35,280
the rise of big tech uh has

1254
00:49:35,280 --> 00:49:38,319
directly impacted um journalists

1255
00:49:38,319 --> 00:49:41,119
revenues and so one of the things that

1256
00:49:41,119 --> 00:49:43,599
has happened over the years is that um

1257
00:49:43,599 --> 00:49:46,480
you know my profession our profession is

1258
00:49:46,480 --> 00:49:48,559
um cratering

1259
00:49:48,559 --> 00:49:51,280
and and that's a real harm to democracy

1260
00:49:51,280 --> 00:49:52,640
and so one thing by the way that just

1261
00:49:52,640 --> 00:49:54,240
people ask all the time is what is the

1262
00:49:54,240 --> 00:49:56,800
harm of creepy ads following you on the

1263
00:49:56,800 --> 00:49:58,559
internet and one of the harms is the

1264
00:49:58,559 --> 00:50:00,400
destruction of journalism which by the

1265
00:50:00,400 --> 00:50:02,400
way might lead to

1266
00:50:02,400 --> 00:50:04,319
the destruction of democracy because you

1267
00:50:04,319 --> 00:50:06,800
need watchdogs on power

1268
00:50:06,800 --> 00:50:08,079
otherwise you don't have a check on

1269
00:50:08,079 --> 00:50:10,400
power and journalists our job is to be a

1270
00:50:10,400 --> 00:50:13,119
check on power and so right now what you

1271
00:50:13,119 --> 00:50:15,200
see that i'm very worried about is a lot

1272
00:50:15,200 --> 00:50:17,280
of really important laws being written

1273
00:50:17,280 --> 00:50:18,880
at the state level without very much

1274
00:50:18,880 --> 00:50:21,359
oversight and so i think that we're in a

1275
00:50:21,359 --> 00:50:22,559
world where

1276
00:50:22,559 --> 00:50:24,800
honestly the the companies who are the

1277
00:50:24,800 --> 00:50:27,839
most to gain from gaming these laws can

1278
00:50:27,839 --> 00:50:30,640
easily influence them at the state level

1279
00:50:30,640 --> 00:50:33,760
and um really set policy in a way where

1280
00:50:33,760 --> 00:50:35,440
people are not paying any attention so

1281
00:50:35,440 --> 00:50:36,400
i'm

1282
00:50:36,400 --> 00:50:37,680
the markup was written about this and

1283
00:50:37,680 --> 00:50:39,760
we're writing more about this and this

1284
00:50:39,760 --> 00:50:41,440
is one of many issues obviously there's

1285
00:50:41,440 --> 00:50:44,160
european regulation as well but i think

1286
00:50:44,160 --> 00:50:46,640
in the u.s the state privacy laws is the

1287
00:50:46,640 --> 00:50:48,000
thing to watch

1288
00:50:48,000 --> 00:50:50,240
there is this great irony that um as

1289
00:50:50,240 --> 00:50:52,559
someone who's sort of been an adult and

1290
00:50:52,559 --> 00:50:55,040
a lawyer and an academic through the

1291
00:50:55,040 --> 00:50:57,040
the the growth of what we think of now

1292
00:50:57,040 --> 00:50:59,280
as the internets that uh that a set of

1293
00:50:59,280 --> 00:51:01,119
technologies run by companies that were

1294
00:51:01,119 --> 00:51:04,400
touted sincerely as as promoting the

1295
00:51:04,400 --> 00:51:06,880
information age and allowing everyone to

1296
00:51:06,880 --> 00:51:08,400
have access to information

1297
00:51:08,400 --> 00:51:10,640
have simultaneously had a business model

1298
00:51:10,640 --> 00:51:12,319
that has undermined the foundation of

1299
00:51:12,319 --> 00:51:14,240
the journalism that we need to make

1300
00:51:14,240 --> 00:51:17,680
sense of this this fire hose or torrent

1301
00:51:17,680 --> 00:51:18,640
or

1302
00:51:18,640 --> 00:51:20,880
choose your watery metaphor here of

1303
00:51:20,880 --> 00:51:23,119
information that is that that is washing

1304
00:51:23,119 --> 00:51:24,559
over us um

1305
00:51:24,559 --> 00:51:26,960
what do you think is is coming next and

1306
00:51:26,960 --> 00:51:28,800
yeah i mean how journalism particularly

1307
00:51:28,800 --> 00:51:31,200
traditional print journalism uh can rise

1308
00:51:31,200 --> 00:51:32,400
to the challenge

1309
00:51:32,400 --> 00:51:33,920
yeah i mean i think

1310
00:51:33,920 --> 00:51:35,680
the

1311
00:51:35,680 --> 00:51:38,400
technology

1312
00:51:38,400 --> 00:51:42,079
data expectation markets as well as

1313
00:51:42,079 --> 00:51:44,240
a lot of the regulatory action that's

1314
00:51:44,240 --> 00:51:46,000
that's happening right now

1315
00:51:46,000 --> 00:51:47,359
makes this probably one of the best

1316
00:51:47,359 --> 00:51:49,040
beats i think i probably have one of the

1317
00:51:49,040 --> 00:51:51,200
best beats in the paper right now and i

1318
00:51:51,200 --> 00:51:52,880
think very few people would agree with

1319
00:51:52,880 --> 00:51:54,319
me

1320
00:51:54,319 --> 00:51:55,839
so which is fine with me because i'm

1321
00:51:55,839 --> 00:51:58,400
happy to do this because this is the

1322
00:51:58,400 --> 00:52:00,000
nexus of power

1323
00:52:00,000 --> 00:52:03,520
it's the nexus of um

1324
00:52:03,520 --> 00:52:06,000
of of government accountability this is

1325
00:52:06,000 --> 00:52:07,680
these are the things that we should be

1326
00:52:07,680 --> 00:52:09,520
writing about as journalists so

1327
00:52:09,520 --> 00:52:11,280
i'm really fascinated with what's

1328
00:52:11,280 --> 00:52:13,760
happening in the market itself um how

1329
00:52:13,760 --> 00:52:14,880
apple

1330
00:52:14,880 --> 00:52:16,079
um

1331
00:52:16,079 --> 00:52:18,640
as my colleague kara swisher says tim

1332
00:52:18,640 --> 00:52:20,960
cook is america's privacy regulator

1333
00:52:20,960 --> 00:52:22,960
right now um i think that that's you

1334
00:52:22,960 --> 00:52:24,319
know whether you agree with or not

1335
00:52:24,319 --> 00:52:26,160
there's a there's some interesting truth

1336
00:52:26,160 --> 00:52:27,760
in that like we're seeing

1337
00:52:27,760 --> 00:52:29,680
real market changes because of a

1338
00:52:29,680 --> 00:52:33,119
decision by apple on its att product

1339
00:52:33,119 --> 00:52:34,640
um it's really interesting to see

1340
00:52:34,640 --> 00:52:36,720
consumer behavior change and you see

1341
00:52:36,720 --> 00:52:38,480
this with the earnings reports from

1342
00:52:38,480 --> 00:52:39,760
google and meta and all these other

1343
00:52:39,760 --> 00:52:42,640
companies and especially at um at meta

1344
00:52:42,640 --> 00:52:45,200
you're seeing younger users leave the

1345
00:52:45,200 --> 00:52:47,359
site so consumers are changing their

1346
00:52:47,359 --> 00:52:49,839
habits and on the regulatory landscape

1347
00:52:49,839 --> 00:52:51,440
super fascinating what's happening on

1348
00:52:51,440 --> 00:52:53,680
the local state level national level

1349
00:52:53,680 --> 00:52:55,440
which is i'm sorry i'm one of the only

1350
00:52:55,440 --> 00:52:56,880
people here that may think i don't think

1351
00:52:56,880 --> 00:52:57,760
that there's going to be federal

1352
00:52:57,760 --> 00:52:59,920
regulation regulation very soon at all

1353
00:52:59,920 --> 00:53:03,440
but maybe i will eat my lunch when i if

1354
00:53:03,440 --> 00:53:06,559
i'm wrong um but really importantly also

1355
00:53:06,559 --> 00:53:08,559
the splintering of the global internet

1356
00:53:08,559 --> 00:53:10,559
and different regulatory changes we hear

1357
00:53:10,559 --> 00:53:12,640
about the splinter net but how what does

1358
00:53:12,640 --> 00:53:14,880
it mean to have essentially three types

1359
00:53:14,880 --> 00:53:16,319
of internet maybe now two different

1360
00:53:16,319 --> 00:53:18,480
types of internet and what does that

1361
00:53:18,480 --> 00:53:21,200
mean in terms of our world and thinking

1362
00:53:21,200 --> 00:53:23,920
about sort of big themes like as a

1363
00:53:23,920 --> 00:53:26,400
global society and how we are

1364
00:53:26,400 --> 00:53:28,720
accessing and viewing and thinking about

1365
00:53:28,720 --> 00:53:30,400
information differently communicating

1366
00:53:30,400 --> 00:53:31,760
differently

1367
00:53:31,760 --> 00:53:33,520
that's fascinating we only have a couple

1368
00:53:33,520 --> 00:53:35,680
of minutes left so i want to end with a

1369
00:53:35,680 --> 00:53:37,119
final question that i think will be of

1370
00:53:37,119 --> 00:53:39,520
great interest to possibly everybody in

1371
00:53:39,520 --> 00:53:42,720
the room which is how do peop privacy

1372
00:53:42,720 --> 00:53:45,040
professionals uh people in what is in

1373
00:53:45,040 --> 00:53:46,880
large part a compliance and risk

1374
00:53:46,880 --> 00:53:48,400
management industry

1375
00:53:48,400 --> 00:53:50,960
avoid appearing on the front page of the

1376
00:53:50,960 --> 00:53:54,839
new york times or the byline of the

1377
00:53:54,839 --> 00:53:57,440
markup um

1378
00:53:57,440 --> 00:53:59,119
try not to

1379
00:53:59,119 --> 00:54:02,480
sell your users data in bulk to

1380
00:54:02,480 --> 00:54:06,800
massive data brokers unanimized um

1381
00:54:06,800 --> 00:54:08,480
and um

1382
00:54:08,480 --> 00:54:09,920
and then not be honest about it in your

1383
00:54:09,920 --> 00:54:12,720
privacy policy would be a starting point

1384
00:54:12,720 --> 00:54:15,599
um but i would say like it is are you

1385
00:54:15,599 --> 00:54:17,440
reading privacy policies

1386
00:54:17,440 --> 00:54:19,760
uh we read yeah we i mean yeah we are

1387
00:54:19,760 --> 00:54:21,920
one of the few people who read them um

1388
00:54:21,920 --> 00:54:24,559
you can uh so you guys will be

1389
00:54:24,559 --> 00:54:26,000
appreciative there is an audience for

1390
00:54:26,000 --> 00:54:29,440
your work fist bump from somebody

1391
00:54:29,440 --> 00:54:31,839
right here

1392
00:54:31,839 --> 00:54:34,558
i read them too

1393
00:54:34,960 --> 00:54:37,359
so this is a conflict of interest i need

1394
00:54:37,359 --> 00:54:38,880
you to be on

1395
00:54:38,880 --> 00:54:41,760
my stories well listen i think so so in

1396
00:54:41,760 --> 00:54:44,240
negative stories right so so what are

1397
00:54:44,240 --> 00:54:46,880
the sorts of yeah it's a conflict

1398
00:54:46,880 --> 00:54:47,920
but um

1399
00:54:47,920 --> 00:54:49,440
no i think what's

1400
00:54:49,440 --> 00:54:52,559
what what the smarter we are the better

1401
00:54:52,559 --> 00:54:54,240
off everyone is right

1402
00:54:54,240 --> 00:54:57,680
if you try to deduct and you know dodge

1403
00:54:57,680 --> 00:54:58,400
and

1404
00:54:58,400 --> 00:55:01,119
you know actually distract and sometimes

1405
00:55:01,119 --> 00:55:02,160
deceive

1406
00:55:02,160 --> 00:55:03,599
then that's problematic for everybody

1407
00:55:03,599 --> 00:55:05,440
the story eventually gets out and you

1408
00:55:05,440 --> 00:55:07,920
also don't build goodwill and the public

1409
00:55:07,920 --> 00:55:10,400
needs to understand you know if you

1410
00:55:10,400 --> 00:55:11,920
hear you know

1411
00:55:11,920 --> 00:55:13,359
if if they're

1412
00:55:13,359 --> 00:55:16,400
these are complicated topics and

1413
00:55:16,400 --> 00:55:17,920
you know like just for example i was

1414
00:55:17,920 --> 00:55:19,200
trying to write a story about the ad

1415
00:55:19,200 --> 00:55:21,520
tech market oh my god like try to write

1416
00:55:21,520 --> 00:55:23,280
that for a new york times audience you

1417
00:55:23,280 --> 00:55:25,440
know it's like really really

1418
00:55:25,440 --> 00:55:26,319
um

1419
00:55:26,319 --> 00:55:28,640
hard to make it accessible and

1420
00:55:28,640 --> 00:55:31,119
it writes about that in plain language

1421
00:55:31,119 --> 00:55:34,720
just if like i'm very grateful when i'm

1422
00:55:34,720 --> 00:55:37,200
able to talk to privacy experts and

1423
00:55:37,200 --> 00:55:39,680
professionals who can

1424
00:55:39,680 --> 00:55:41,200
be so generous as to lend their

1425
00:55:41,200 --> 00:55:44,640
knowledge and um that's a good way to

1426
00:55:44,640 --> 00:55:47,839
appear in a story i would imagine so not

1427
00:55:47,839 --> 00:55:49,680
such a conflict of interest not such a

1428
00:55:49,680 --> 00:55:51,280
conflict of interest

1429
00:55:51,280 --> 00:55:52,079
um

1430
00:55:52,079 --> 00:55:53,839
i i think unfortunately we're at the end

1431
00:55:53,839 --> 00:55:55,520
of our time so please join me in

1432
00:55:55,520 --> 00:55:59,799
thanking cecilia kong and julian

1433
00:56:01,520 --> 00:56:04,520
you

1434
00:56:07,910 --> 00:56:17,658
[Music]

1435
00:56:20,559 --> 00:56:23,119
first of all i just want to thank our

1436
00:56:23,119 --> 00:56:24,720
keynote panel

1437
00:56:24,720 --> 00:56:26,960
professor olasoga

1438
00:56:26,960 --> 00:56:28,400
brad smith

1439
00:56:28,400 --> 00:56:31,839
neil richards julia anguin and cecilia

1440
00:56:31,839 --> 00:56:33,359
king let's give them a round of applause

1441
00:56:33,359 --> 00:56:35,630
for an amazing keynote

1442
00:56:35,630 --> 00:56:38,849
[Applause]

1443
00:56:39,520 --> 00:56:42,559
my name is dominique shelton leipzig i

1444
00:56:42,559 --> 00:56:46,240
am a proud member of the iapp board and

1445
00:56:46,240 --> 00:56:49,280
a partner at mayor brown i do lead the

1446
00:56:49,280 --> 00:56:51,599
global data innovation team

1447
00:56:51,599 --> 00:56:54,880
what we heard today from brad and

1448
00:56:54,880 --> 00:56:58,880
professor olusoga is that data is now no

1449
00:56:58,880 --> 00:57:01,599
longer a compliance issue but a business

1450
00:57:01,599 --> 00:57:02,799
imperative

1451
00:57:02,799 --> 00:57:06,160
we have a situation where data is in the

1452
00:57:06,160 --> 00:57:08,640
positionally in the midst of moving

1453
00:57:08,640 --> 00:57:11,680
markets i was reading about the nasdaq

1454
00:57:11,680 --> 00:57:15,040
market share of 1.3 trillion lost just

1455
00:57:15,040 --> 00:57:16,880
at the end of march

1456
00:57:16,880 --> 00:57:20,319
we are generating 2.5 quintillion bytes

1457
00:57:20,319 --> 00:57:23,359
of data per day uh so the global

1458
00:57:23,359 --> 00:57:24,799
pandemic has

1459
00:57:24,799 --> 00:57:26,799
in that matter will only increase as

1460
00:57:26,799 --> 00:57:28,559
brad was talking about the connected

1461
00:57:28,559 --> 00:57:30,400
devices that we have

1462
00:57:30,400 --> 00:57:33,440
listening to uh cecilia and julia talk

1463
00:57:33,440 --> 00:57:36,960
about tracking ad tech this is uh really

1464
00:57:36,960 --> 00:57:39,520
a 800 billion dollar market that is in

1465
00:57:39,520 --> 00:57:40,720
flux

1466
00:57:40,720 --> 00:57:42,720
all of us here have an important role to

1467
00:57:42,720 --> 00:57:45,040
play and how the the future is shaped

1468
00:57:45,040 --> 00:57:46,640
and so i hope you all will continue to

1469
00:57:46,640 --> 00:57:49,200
enjoy the rest of the conference

1470
00:57:49,200 --> 00:57:51,440
interact with each other and use this

1471
00:57:51,440 --> 00:57:53,680
opportunity to continue the dialogue

1472
00:57:53,680 --> 00:57:55,760
when you get back to your homes thanks

1473
00:57:55,760 --> 00:57:58,960
so much and enjoy the rest of the

1474
00:57:59,540 --> 00:58:06,960
[Music]

1475
00:58:06,960 --> 00:58:10,799
i can feel afternoon in the future

1476
00:58:10,799 --> 00:58:13,920
i can see it in the culture

1477
00:58:13,920 --> 00:58:16,880
i see the

