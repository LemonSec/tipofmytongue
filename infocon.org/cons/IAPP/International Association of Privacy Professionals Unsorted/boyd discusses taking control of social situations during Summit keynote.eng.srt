1
00:00:04,560 --> 00:00:07,680
[Music]

2
00:00:08,889 --> 00:00:12,080
good morning it is an honor to be here

3
00:00:12,080 --> 00:00:14,540
with you today I am stunned by how many

4
00:00:14,540 --> 00:00:16,670
of you are here it's actually really

5
00:00:16,670 --> 00:00:17,720
exciting you know as somebody who's been

6
00:00:17,720 --> 00:00:19,430
looking at privacy for so long to see so

7
00:00:19,430 --> 00:00:21,890
many people deeply invested in it and my

8
00:00:21,890 --> 00:00:24,050
goal today is to give you a twisted way

9
00:00:24,050 --> 00:00:25,550
of thinking about your profession and

10
00:00:25,550 --> 00:00:27,350
the challenges you face so work with me

11
00:00:27,350 --> 00:00:31,460
promote so privacy is about control most

12
00:00:31,460 --> 00:00:33,530
people in the business of privacy think

13
00:00:33,530 --> 00:00:35,210
that privacy is about the control of

14
00:00:35,210 --> 00:00:37,730
data who should access data and under

15
00:00:37,730 --> 00:00:39,739
what circumstances and tech companies

16
00:00:39,739 --> 00:00:42,380
build privacy policies to circumscribe

17
00:00:42,380 --> 00:00:45,230
access engineers build access control

18
00:00:45,230 --> 00:00:47,690
structures people try to communicate

19
00:00:47,690 --> 00:00:50,210
what privacy might mean to users and

20
00:00:50,210 --> 00:00:52,520
regulators try to drill into what the

21
00:00:52,520 --> 00:00:53,960
implications might be when companies

22
00:00:53,960 --> 00:00:56,480
violate this yet time and time again

23
00:00:56,480 --> 00:00:59,210
people come back and say what is wrong

24
00:00:59,210 --> 00:01:01,280
with people don't they understand

25
00:01:01,280 --> 00:01:03,800
privacy and you know people will talk

26
00:01:03,800 --> 00:01:05,239
about how much they care about privacy

27
00:01:05,239 --> 00:01:07,159
but everybody else complains they don't

28
00:01:07,159 --> 00:01:10,069
seem to be doing it right and it's a

29
00:01:10,069 --> 00:01:11,450
really funny thing because we call it

30
00:01:11,450 --> 00:01:13,189
the privacy paradox right people's

31
00:01:13,189 --> 00:01:15,829
attitudes compared to their actions and

32
00:01:15,829 --> 00:01:17,509
I was found this sort of fascinating

33
00:01:17,509 --> 00:01:19,549
because my life is spending time with

34
00:01:19,549 --> 00:01:20,929
people trying to understand what it is

35
00:01:20,929 --> 00:01:23,420
that they do and so for 10 years I spent

36
00:01:23,420 --> 00:01:25,369
time in the trenches talking to

37
00:01:25,369 --> 00:01:27,200
teenagers around this country as social

38
00:01:27,200 --> 00:01:28,630
media was becoming more and more popular

39
00:01:28,630 --> 00:01:30,829
trying to understand what they were

40
00:01:30,829 --> 00:01:32,270
doing and how it in fits in our lives

41
00:01:32,270 --> 00:01:34,459
and one of the things I learned really

42
00:01:34,459 --> 00:01:35,959
quickly is that they do care about

43
00:01:35,959 --> 00:01:37,729
privacy but they're speaking a very

44
00:01:37,729 --> 00:01:39,889
different language than you are and that

45
00:01:39,889 --> 00:01:41,799
disconnect has caused tremendous

46
00:01:41,799 --> 00:01:45,319
problems now teenagers talk about their

47
00:01:45,319 --> 00:01:46,939
desire for privacy but what do they mean

48
00:01:46,939 --> 00:01:49,399
by it they actually also talk

49
00:01:49,399 --> 00:01:50,840
simultaneously the same voice about

50
00:01:50,840 --> 00:01:52,520
wanting to have access to public life

51
00:01:52,520 --> 00:01:54,590
and that's where the contradiction seems

52
00:01:54,590 --> 00:01:57,200
to occur because they want to share so

53
00:01:57,200 --> 00:01:58,880
that they're part of everyday life and

54
00:01:58,880 --> 00:02:01,279
they want to have privacy so what does

55
00:02:01,279 --> 00:02:03,289
that look like my colleague Alice

56
00:02:03,289 --> 00:02:04,819
Marwick and I spent all this time trying

57
00:02:04,819 --> 00:02:06,739
to untangle that and we came to

58
00:02:06,739 --> 00:02:09,288
something really important young people

59
00:02:09,288 --> 00:02:11,000
aren't focused on the control of

60
00:02:11,000 --> 00:02:13,310
information they're not Kisan the

61
00:02:13,310 --> 00:02:14,480
control of access

62
00:02:14,480 --> 00:02:16,879
they're focused on the control of a

63
00:02:16,879 --> 00:02:20,870
social situation now that phrase might

64
00:02:20,870 --> 00:02:22,790
seem a little weird right because when

65
00:02:22,790 --> 00:02:24,379
it comes down to is that they want to

66
00:02:24,379 --> 00:02:26,000
know that their interpersonal dynamics

67
00:02:26,000 --> 00:02:28,580
will be managed appropriately by the

68
00:02:28,580 --> 00:02:30,050
systems as they work through them

69
00:02:30,050 --> 00:02:32,540
they're not really trying to think about

70
00:02:32,540 --> 00:02:33,860
it in terms of the information they're

71
00:02:33,860 --> 00:02:35,690
trying to think about what will unfold

72
00:02:35,690 --> 00:02:38,030
how people will interact with them how

73
00:02:38,030 --> 00:02:39,920
they can predict and understand what is

74
00:02:39,920 --> 00:02:42,769
going to happen and that actually has

75
00:02:42,769 --> 00:02:45,170
phenomenal implications for a lot of

76
00:02:45,170 --> 00:02:47,170
what's going on today in particular

77
00:02:47,170 --> 00:02:50,120
current crises regarding things like

78
00:02:50,120 --> 00:02:51,980
media manipulation but let me first

79
00:02:51,980 --> 00:02:54,170
unpack what it means to control a social

80
00:02:54,170 --> 00:02:55,519
situation because that's actually a

81
00:02:55,519 --> 00:02:57,890
really weird thing to think about so I

82
00:02:57,890 --> 00:02:59,660
want you know dial back to middle school

83
00:02:59,660 --> 00:03:02,090
right I know is your favorite time ever

84
00:03:02,090 --> 00:03:04,250
right so think about that time think

85
00:03:04,250 --> 00:03:06,829
about your friend Carol and you told

86
00:03:06,829 --> 00:03:08,420
Carol something you told her not to tell

87
00:03:08,420 --> 00:03:11,239
anyone right it was a secret you really

88
00:03:11,239 --> 00:03:13,040
wanted to share with her but you really

89
00:03:13,040 --> 00:03:13,940
wanted her not to spread that

90
00:03:13,940 --> 00:03:16,130
information and of course what did she

91
00:03:16,130 --> 00:03:17,660
do she turned around she started

92
00:03:17,660 --> 00:03:19,160
gossiping and she spread it to all sorts

93
00:03:19,160 --> 00:03:20,930
of other people and you were embarrassed

94
00:03:20,930 --> 00:03:22,280
and you were horrified you were mad at

95
00:03:22,280 --> 00:03:24,500
Carol the floor fell out from you

96
00:03:24,500 --> 00:03:25,819
because it's terrible to be in middle

97
00:03:25,819 --> 00:03:27,709
school and be shamed it was awful and

98
00:03:27,709 --> 00:03:31,579
you felt like privacy was invaded trust

99
00:03:31,579 --> 00:03:35,150
was violated but you also learned a

100
00:03:35,150 --> 00:03:37,880
really interesting lesson you learned

101
00:03:37,880 --> 00:03:39,920
that if you wanted something to spread

102
00:03:39,920 --> 00:03:40,549
far and wide

103
00:03:40,549 --> 00:03:43,549
you should tell Carol not to tell anyone

104
00:03:43,549 --> 00:03:46,010
all right and that's a really

105
00:03:46,010 --> 00:03:47,540
interesting lesson because what you've

106
00:03:47,540 --> 00:03:50,090
done is taken a moment of complete

107
00:03:50,090 --> 00:03:52,940
horror and turned it around to a moment

108
00:03:52,940 --> 00:03:54,620
where you have power where you can

109
00:03:54,620 --> 00:03:56,389
control the situation by understanding

110
00:03:56,389 --> 00:03:59,989
the chessboard there is nothing more

111
00:03:59,989 --> 00:04:01,609
empowering than being able to have that

112
00:04:01,609 --> 00:04:03,470
control of a social situation to

113
00:04:03,470 --> 00:04:06,170
understand how information will flow to

114
00:04:06,170 --> 00:04:07,819
understand who will say what what

115
00:04:07,819 --> 00:04:10,340
systems will work the difficulty is is

116
00:04:10,340 --> 00:04:12,950
that in a land of technology having that

117
00:04:12,950 --> 00:04:16,250
flow that understanding is extremely

118
00:04:16,250 --> 00:04:18,560
difficult and for many people social

119
00:04:18,560 --> 00:04:20,599
media is up ended the way in which they

120
00:04:20,599 --> 00:04:22,460
can manage us control of a social

121
00:04:22,460 --> 00:04:24,860
situation so everyone seems to complain

122
00:04:24,860 --> 00:04:26,240
teeth like you'll hear these complaints

123
00:04:26,240 --> 00:04:27,720
oh my gosh Facebook

124
00:04:27,720 --> 00:04:29,850
violated my privacy and you're like what

125
00:04:29,850 --> 00:04:31,110
happened and they're like they brought

126
00:04:31,110 --> 00:04:33,120
this thing I posted 60 years ago back to

127
00:04:33,120 --> 00:04:35,490
the present and Facebook's like you gave

128
00:04:35,490 --> 00:04:37,770
us permission and people are like I

129
00:04:37,770 --> 00:04:40,110
don't care that was really creepy

130
00:04:40,110 --> 00:04:42,090
and that's the really important word in

131
00:04:42,090 --> 00:04:44,550
all of this creepy well you hear over

132
00:04:44,550 --> 00:04:47,070
and over again when people feel like

133
00:04:47,070 --> 00:04:48,780
their privacy has been invaded in some

134
00:04:48,780 --> 00:04:50,820
way is creepy and that will have nothing

135
00:04:50,820 --> 00:04:53,760
to do with an access violation that will

136
00:04:53,760 --> 00:04:54,870
have to do with how something doesn't

137
00:04:54,870 --> 00:04:57,000
feel right your parents did something

138
00:04:57,000 --> 00:04:59,010
that they shouldn't have done the social

139
00:04:59,010 --> 00:05:01,380
norms were violated something went wrong

140
00:05:01,380 --> 00:05:03,810
because somebody lost control of a

141
00:05:03,810 --> 00:05:08,010
social situation so I wrote a book it's

142
00:05:08,010 --> 00:05:10,440
complicated and in an ID tale in gory

143
00:05:10,440 --> 00:05:12,030
detail along with other works from my

144
00:05:12,030 --> 00:05:15,570
colleague Alice Marek what it took and

145
00:05:15,570 --> 00:05:16,950
the strategies that young people took

146
00:05:16,950 --> 00:05:19,320
during the rise of social media in order

147
00:05:19,320 --> 00:05:21,960
to achieve privacy and it's a really

148
00:05:21,960 --> 00:05:23,460
fascinating set of conversations and if

149
00:05:23,460 --> 00:05:24,900
you really want to dive in I'm speaking

150
00:05:24,900 --> 00:05:26,460
again on that topic at eleven o'clock

151
00:05:26,460 --> 00:05:28,380
and there's a book where I can find it

152
00:05:28,380 --> 00:05:30,360
for you and we can talk about it but I

153
00:05:30,360 --> 00:05:31,650
actually want to connect the dots for

154
00:05:31,650 --> 00:05:33,750
you of what that work is that I've been

155
00:05:33,750 --> 00:05:35,370
doing for so long with a lot of the work

156
00:05:35,370 --> 00:05:37,260
that I'm doing now because there was a

157
00:05:37,260 --> 00:05:39,419
really interesting shift that happened

158
00:05:39,419 --> 00:05:41,370
because the fact that we didn't take

159
00:05:41,370 --> 00:05:44,160
teenagers privacy seriously has really

160
00:05:44,160 --> 00:05:46,620
hurt us now and I want to explain what

161
00:05:46,620 --> 00:05:48,720
that looks like because it started at

162
00:05:48,720 --> 00:05:50,910
the margins and it is coming to hurt us

163
00:05:50,910 --> 00:05:53,250
in significant ways right because what

164
00:05:53,250 --> 00:05:54,750
happens when you don't give people

165
00:05:54,750 --> 00:05:57,540
control over the social situation they

166
00:05:57,540 --> 00:06:00,990
find a way to get control so I watched a

167
00:06:00,990 --> 00:06:03,150
group of teenagers sort of on the

168
00:06:03,150 --> 00:06:05,729
margins starting around 2004 who were

169
00:06:05,729 --> 00:06:07,800
really frustrated by the fact that they

170
00:06:07,800 --> 00:06:09,900
didn't have control over social

171
00:06:09,900 --> 00:06:12,000
situations they couldn't stand social

172
00:06:12,000 --> 00:06:13,200
media it made them feel really

173
00:06:13,200 --> 00:06:15,510
destabilized and they wanted control and

174
00:06:15,510 --> 00:06:17,729
like those of you who realized that the

175
00:06:17,729 --> 00:06:19,979
key was to share information with Carol

176
00:06:19,979 --> 00:06:21,870
so it would spread these teenagers

177
00:06:21,870 --> 00:06:23,760
realize that the information landscape

178
00:06:23,760 --> 00:06:26,550
created by social media gave them a new

179
00:06:26,550 --> 00:06:28,470
opportunity to control a social

180
00:06:28,470 --> 00:06:31,229
situation in a much more mischievous way

181
00:06:31,229 --> 00:06:34,110
right and they started by creating memes

182
00:06:34,110 --> 00:06:37,320
it involved Lowell Katz it was fun but

183
00:06:37,320 --> 00:06:38,680
then they started hacking the attention

184
00:06:38,680 --> 00:06:41,080
economy and it was funny at first I mean

185
00:06:41,080 --> 00:06:43,000
after years of listening to Oprah

186
00:06:43,000 --> 00:06:45,400
Winfrey and other TV people talking

187
00:06:45,400 --> 00:06:46,990
obsessively about how the internet was

188
00:06:46,990 --> 00:06:49,990
dangerous for teenagers the idea of

189
00:06:49,990 --> 00:06:52,330
getting Oprah to share your message was

190
00:06:52,330 --> 00:06:53,860
really funny let me explain what

191
00:06:53,860 --> 00:06:56,380
happened she had been doing a series of

192
00:06:56,380 --> 00:06:59,410
TV shows about how the internet was

193
00:06:59,410 --> 00:07:01,150
filled with online sexual predators and

194
00:07:01,150 --> 00:07:02,560
these teenagers were annoyed because

195
00:07:02,560 --> 00:07:04,210
their parents were actually going after

196
00:07:04,210 --> 00:07:05,440
them and telling them the Internet was

197
00:07:05,440 --> 00:07:06,870
unsafe

198
00:07:06,870 --> 00:07:09,160
so they started writing things on her

199
00:07:09,160 --> 00:07:10,750
various bulletin boards trying to

200
00:07:10,750 --> 00:07:13,389
contact very various people who work on

201
00:07:13,389 --> 00:07:16,270
her show and basically they engaged in a

202
00:07:16,270 --> 00:07:17,800
game that we would now call trolling

203
00:07:17,800 --> 00:07:20,259
right because they trolled Oprah's folks

204
00:07:20,259 --> 00:07:22,900
so much so that she spent a half an hour

205
00:07:22,900 --> 00:07:26,110
on TV dedicated to a whole culture of

206
00:07:26,110 --> 00:07:29,169
online predators that had an icon known

207
00:07:29,169 --> 00:07:33,070
as pedobear and that she talked about in

208
00:07:33,070 --> 00:07:34,900
on TV about all of these guys who were

209
00:07:34,900 --> 00:07:38,139
talking in her message boards about how

210
00:07:38,139 --> 00:07:39,669
they were coming to harm children and

211
00:07:39,669 --> 00:07:42,669
she specifically quoted one who says he

212
00:07:42,669 --> 00:07:44,800
doesn't forgive he doesn't forget his

213
00:07:44,800 --> 00:07:46,840
group has over 9000 penises and they're

214
00:07:46,840 --> 00:07:49,120
raping children now if you know anything

215
00:07:49,120 --> 00:07:51,820
about internet subcultures well

216
00:07:51,820 --> 00:07:53,110
doesn't forgive doesn't forget that's

217
00:07:53,110 --> 00:07:55,659
anonymous over 9,000 is a meme reference

218
00:07:55,659 --> 00:07:58,800
to a Japanese a video game that autumn a

219
00:07:58,800 --> 00:08:02,349
in other words it was a massive in joke

220
00:08:02,349 --> 00:08:04,960
within these early environments a site

221
00:08:04,960 --> 00:08:07,690
called 4chan and she did it she spoke

222
00:08:07,690 --> 00:08:09,070
loud and you can imagine the internet

223
00:08:09,070 --> 00:08:10,840
erupted with laughter right it's funny

224
00:08:10,840 --> 00:08:12,870
to get Oprah to say ridiculous things

225
00:08:12,870 --> 00:08:16,389
but that kind of useful youthful media

226
00:08:16,389 --> 00:08:19,690
manipulation shifted over time a media

227
00:08:19,690 --> 00:08:21,729
manipulation it may not seem like it has

228
00:08:21,729 --> 00:08:23,860
anything to do with privacy but actually

229
00:08:23,860 --> 00:08:25,539
it's fundamentally rooted in that same

230
00:08:25,539 --> 00:08:27,909
dynamic of the ability to control a

231
00:08:27,909 --> 00:08:30,430
social situation media manipulation

232
00:08:30,430 --> 00:08:32,260
allows disenfranchised members of

233
00:08:32,260 --> 00:08:34,240
society to come up with alternative ways

234
00:08:34,240 --> 00:08:36,760
of controlling ways of having power

235
00:08:36,760 --> 00:08:39,700
after they feel disempowered and that's

236
00:08:39,700 --> 00:08:41,380
a really important component because

237
00:08:41,380 --> 00:08:44,140
people think about 4chan as a space of

238
00:08:44,140 --> 00:08:46,870
anonymity and people hiding especially

239
00:08:46,870 --> 00:08:48,350
back in those days but

240
00:08:48,350 --> 00:08:50,300
many ways it was about purposeful

241
00:08:50,300 --> 00:08:52,580
disruption of the status quo it was

242
00:08:52,580 --> 00:08:55,160
about achieving a form of chaos when the

243
00:08:55,160 --> 00:08:57,590
status quo was actually undermining any

244
00:08:57,590 --> 00:09:00,440
form of agency and so what happened is

245
00:09:00,440 --> 00:09:02,000
this coordinated group of teenagers

246
00:09:02,000 --> 00:09:04,850
started to learn how to manipulate media

247
00:09:04,850 --> 00:09:07,460
they learned how to alter structures of

248
00:09:07,460 --> 00:09:10,310
information play SEO games figure out

249
00:09:10,310 --> 00:09:12,350
the very architectures of the system so

250
00:09:12,350 --> 00:09:13,850
that they could actually control a

251
00:09:13,850 --> 00:09:16,970
social situation in their way it was

252
00:09:16,970 --> 00:09:18,470
supposed to be fun and games a kind of

253
00:09:18,470 --> 00:09:21,140
teen subculture a joke unfortunately it

254
00:09:21,140 --> 00:09:23,290
stopped being funny a couple years ago

255
00:09:23,290 --> 00:09:25,580
so there are many different aspects of

256
00:09:25,580 --> 00:09:27,410
media manipulation that I can't go into

257
00:09:27,410 --> 00:09:29,870
in 25 minutes but I want you to realize

258
00:09:29,870 --> 00:09:31,730
that this aspect of media manipulation

259
00:09:31,730 --> 00:09:33,710
is the control of a social situation and

260
00:09:33,710 --> 00:09:35,870
a lot has changed since what I was

261
00:09:35,870 --> 00:09:38,810
seeing in the early aughts so we've seen

262
00:09:38,810 --> 00:09:41,000
sort of this evolution right jokesters

263
00:09:41,000 --> 00:09:43,340
messing with Scientology the rise of

264
00:09:43,340 --> 00:09:45,530
anonymous the sharing of information

265
00:09:45,530 --> 00:09:47,690
from WikiLeaks we saw people hacking

266
00:09:47,690 --> 00:09:50,090
into systems people getting doxxed the

267
00:09:50,090 --> 00:09:52,370
swarms of people mocking people after

268
00:09:52,370 --> 00:09:55,130
someone had died by 2014 we were seeing

269
00:09:55,130 --> 00:09:56,990
the development of sophisticated and

270
00:09:56,990 --> 00:09:58,690
coordinated harassment campaigns

271
00:09:58,690 --> 00:10:00,910
implemented as media manipulation

272
00:10:00,910 --> 00:10:04,700
implemented as ironic play but 2015 we

273
00:10:04,700 --> 00:10:07,660
were seeing large coordinated efforts to

274
00:10:07,660 --> 00:10:10,310
manipulate media for political purposes

275
00:10:10,310 --> 00:10:13,640
in addition to economic purposes and

276
00:10:13,640 --> 00:10:15,950
what ended up happening is we saw all

277
00:10:15,950 --> 00:10:17,810
this unfold is the rise of another thing

278
00:10:17,810 --> 00:10:19,460
that we saw from early days of Usenet

279
00:10:19,460 --> 00:10:22,070
something that's known as Poe's law you

280
00:10:22,070 --> 00:10:23,630
can read what Poe's law is here but the

281
00:10:23,630 --> 00:10:25,430
really short version of Poe's law is

282
00:10:25,430 --> 00:10:27,560
that online it becomes increasingly

283
00:10:27,560 --> 00:10:29,150
difficult to tell the difference between

284
00:10:29,150 --> 00:10:30,740
somebody who is joking around and

285
00:10:30,740 --> 00:10:32,600
pretending to be a Nazi and someone who

286
00:10:32,600 --> 00:10:35,450
is an actual Nazi and that dynamic is

287
00:10:35,450 --> 00:10:37,190
something that we are living with right

288
00:10:37,190 --> 00:10:38,260
now

289
00:10:38,260 --> 00:10:40,460
Charlottesville was a wake-up call for

290
00:10:40,460 --> 00:10:42,500
many but not a that much for the tech

291
00:10:42,500 --> 00:10:45,080
industry not a lot changed but the

292
00:10:45,080 --> 00:10:47,390
violence keeps coming you know I was

293
00:10:47,390 --> 00:10:49,100
hoping that I wouldn't have to reference

294
00:10:49,100 --> 00:10:50,810
another violent incident this weekend

295
00:10:50,810 --> 00:10:54,440
and yet we did I want you to understand

296
00:10:54,440 --> 00:10:57,650
that the extreme of media manipulation

297
00:10:57,650 --> 00:10:59,900
is really about this

298
00:10:59,900 --> 00:11:02,210
on this transition over trying to have

299
00:11:02,210 --> 00:11:05,750
control because that shifting nature has

300
00:11:05,750 --> 00:11:07,460
actually made this turn into something

301
00:11:07,460 --> 00:11:09,920
that's far more violent and the flipside

302
00:11:09,920 --> 00:11:11,720
of is when we talk about privacy is

303
00:11:11,720 --> 00:11:12,950
realizing that the technical

304
00:11:12,950 --> 00:11:15,800
architectures we're working with are in

305
00:11:15,800 --> 00:11:18,230
the business of amplification and that

306
00:11:18,230 --> 00:11:20,630
the news media is also in the business

307
00:11:20,630 --> 00:11:22,910
of amplification and when we have these

308
00:11:22,910 --> 00:11:25,400
two systems of amplification that are

309
00:11:25,400 --> 00:11:27,650
intertwined they become something that

310
00:11:27,650 --> 00:11:31,520
people can manipulate at scale so let's

311
00:11:31,520 --> 00:11:32,740
talk about Christchurch

312
00:11:32,740 --> 00:11:35,630
one month ago a terrorist walked into a

313
00:11:35,630 --> 00:11:37,610
mosque in Christchurch New Zealand and

314
00:11:37,610 --> 00:11:40,160
brutally murdered 50 worshippers in a

315
00:11:40,160 --> 00:11:42,890
mosque okay and I want you to understand

316
00:11:42,890 --> 00:11:45,440
that this incident has three parts to it

317
00:11:45,440 --> 00:11:48,560
the first part is that absolute horrific

318
00:11:48,560 --> 00:11:51,140
violent attack that the people of New

319
00:11:51,140 --> 00:11:53,930
Zealand had to face and the consequence

320
00:11:53,930 --> 00:11:55,520
of the media and people wanting to cover

321
00:11:55,520 --> 00:11:57,110
that and doing what we talked about

322
00:11:57,110 --> 00:11:59,000
whenever there is a terrorist attack but

323
00:11:59,000 --> 00:12:01,580
there are two other technical components

324
00:12:01,580 --> 00:12:04,070
to this that are important the first is

325
00:12:04,070 --> 00:12:06,380
what that particular terrorist did in

326
00:12:06,380 --> 00:12:09,200
setting up this attack this terrorist

327
00:12:09,200 --> 00:12:11,390
understood the vulnerabilities of social

328
00:12:11,390 --> 00:12:13,460
media the vulnerabilities of the news

329
00:12:13,460 --> 00:12:16,460
media he knew how to play both of those

330
00:12:16,460 --> 00:12:18,860
strategically so let me explain some of

331
00:12:18,860 --> 00:12:21,080
what that looked like if you read his

332
00:12:21,080 --> 00:12:23,210
original 8 sham post you would have

333
00:12:23,210 --> 00:12:25,010
noticed something fascinating he

334
00:12:25,010 --> 00:12:28,600
provided live links to absolutely racist

335
00:12:28,600 --> 00:12:31,100
manifestos all sorts of content that was

336
00:12:31,100 --> 00:12:33,830
horrific but the reference to the live

337
00:12:33,830 --> 00:12:35,660
stream on Facebook was not a live link

338
00:12:35,660 --> 00:12:38,090
it had spaces between it and that was

339
00:12:38,090 --> 00:12:40,010
intentional because that meant that

340
00:12:40,010 --> 00:12:42,590
people put the URL into their browser

341
00:12:42,590 --> 00:12:45,410
and Facebook couldn't see that all of

342
00:12:45,410 --> 00:12:48,410
the traffic was coming from a champ that

343
00:12:48,410 --> 00:12:50,420
meant also that he played on the news

344
00:12:50,420 --> 00:12:52,070
media by making certain that there were

345
00:12:52,070 --> 00:12:53,600
all sorts of references to things that

346
00:12:53,600 --> 00:12:55,670
the news media didn't understand so they

347
00:12:55,670 --> 00:12:57,800
would immediately report things but he

348
00:12:57,800 --> 00:12:59,780
had also staged that because if you

349
00:12:59,780 --> 00:13:01,880
search for the title of his manifesto

350
00:13:01,880 --> 00:13:04,490
what you got was a stream of horrific

351
00:13:04,490 --> 00:13:06,530
content that had been purposefully se

352
00:13:06,530 --> 00:13:09,110
owed so you could radicalize a whole set

353
00:13:09,110 --> 00:13:11,840
of different people he did a whole set

354
00:13:11,840 --> 00:13:12,230
of a

355
00:13:12,230 --> 00:13:15,140
tactics that were very much attempting

356
00:13:15,140 --> 00:13:17,120
to undermine any of the detection

357
00:13:17,120 --> 00:13:19,280
mechanisms of any major platforms and

358
00:13:19,280 --> 00:13:21,740
that's what occurred before he walked

359
00:13:21,740 --> 00:13:24,620
into that mosque signaled and physically

360
00:13:24,620 --> 00:13:26,420
from Peppard portraying himself as a

361
00:13:26,420 --> 00:13:28,550
first-person shooter to get past a lot

362
00:13:28,550 --> 00:13:31,070
of original expectations so much so that

363
00:13:31,070 --> 00:13:34,370
the people that I know that do analysis

364
00:13:34,370 --> 00:13:36,920
of Isis videos who watched it live

365
00:13:36,920 --> 00:13:38,840
didn't realize what was going on

366
00:13:38,840 --> 00:13:41,240
now the third act to it is also

367
00:13:41,240 --> 00:13:43,100
problematic because the third act was

368
00:13:43,100 --> 00:13:45,650
what happened afterwards people took

369
00:13:45,650 --> 00:13:47,450
that video that livestream it quickly

370
00:13:47,450 --> 00:13:49,070
went into a variety of different for it

371
00:13:49,070 --> 00:13:50,990
went to tour in matter of minutes and

372
00:13:50,990 --> 00:13:53,690
they kept modifying it and uploading it

373
00:13:53,690 --> 00:13:56,210
Facebook publicly announced 1.5 million

374
00:13:56,210 --> 00:13:58,610
uploads many serve it and that was in

375
00:13:58,610 --> 00:14:00,620
the first 24 hours many services saw

376
00:14:00,620 --> 00:14:02,540
many more uploads and each of those

377
00:14:02,540 --> 00:14:04,610
uploads was a modification meant to

378
00:14:04,610 --> 00:14:07,520
figure out how to get past all of the

379
00:14:07,520 --> 00:14:08,840
content detection

380
00:14:08,840 --> 00:14:12,230
what is this this is about manipulating

381
00:14:12,230 --> 00:14:14,270
vulnerabilities in the information

382
00:14:14,270 --> 00:14:16,460
landscape and you may recognize that

383
00:14:16,460 --> 00:14:18,470
that language is a language of security

384
00:14:18,470 --> 00:14:21,830
right exploits vulnerabilities hacking

385
00:14:21,830 --> 00:14:24,440
in privacy circles we've long talked

386
00:14:24,440 --> 00:14:25,850
about the relationship between privacy

387
00:14:25,850 --> 00:14:27,860
and security talking about what it means

388
00:14:27,860 --> 00:14:30,050
to think about the ability to access

389
00:14:30,050 --> 00:14:32,170
private information about controlling

390
00:14:32,170 --> 00:14:34,610
and building the secure structures to

391
00:14:34,610 --> 00:14:36,950
control data but I want you to rethink

392
00:14:36,950 --> 00:14:39,080
what it means about the relationship

393
00:14:39,080 --> 00:14:40,820
between security and privacy when we

394
00:14:40,820 --> 00:14:42,890
talk about an information landscape when

395
00:14:42,890 --> 00:14:44,840
we talk about the ability to control a

396
00:14:44,840 --> 00:14:47,450
social situation in order to secure an

397
00:14:47,450 --> 00:14:48,950
information landscape where people are

398
00:14:48,950 --> 00:14:51,260
trying to manipulate the social context

399
00:14:51,260 --> 00:14:54,140
we need to recognize that data becomes

400
00:14:54,140 --> 00:14:57,230
infrastructure and the idea that you end

401
00:14:57,230 --> 00:14:59,150
up having here is not simply that you

402
00:14:59,150 --> 00:15:02,030
want to exploit it by actually accessing

403
00:15:02,030 --> 00:15:04,480
the data but by corrupting the data

404
00:15:04,480 --> 00:15:07,670
right actually making it difficult for

405
00:15:07,670 --> 00:15:10,370
any system to operate because you can do

406
00:15:10,370 --> 00:15:11,900
a lot of things that we've often thought

407
00:15:11,900 --> 00:15:13,400
of as separate search engine

408
00:15:13,400 --> 00:15:17,180
optimization algorithmic gameplay all of

409
00:15:17,180 --> 00:15:18,590
the different ways of contorting

410
00:15:18,590 --> 00:15:21,080
recommendation engines and search but

411
00:15:21,080 --> 00:15:23,300
what happens is that because the data

412
00:15:23,300 --> 00:15:24,910
that people put in

413
00:15:24,910 --> 00:15:26,770
through the front door and the kinds of

414
00:15:26,770 --> 00:15:28,810
models that get built off of the variety

415
00:15:28,810 --> 00:15:30,730
of different sources of data becomes so

416
00:15:30,730 --> 00:15:33,190
powerful the way in which you control a

417
00:15:33,190 --> 00:15:36,010
situation is that you exploit the an

418
00:15:36,010 --> 00:15:38,590
underlying infrastructure you exploit

419
00:15:38,590 --> 00:15:40,690
the data that builds the models and

420
00:15:40,690 --> 00:15:42,040
that's what we see people doing on a

421
00:15:42,040 --> 00:15:45,220
regular basis you know as we think about

422
00:15:45,220 --> 00:15:46,930
our current obsession with data we're

423
00:15:46,930 --> 00:15:49,120
not just thinking about how we can

424
00:15:49,120 --> 00:15:51,040
actually follow compliance rules and I'm

425
00:15:51,040 --> 00:15:53,200
so grateful for those of you what worked

426
00:15:53,200 --> 00:15:55,150
so hard in this room both to make

427
00:15:55,150 --> 00:15:57,010
certain that GD P R is a reality and to

428
00:15:57,010 --> 00:15:57,790
make certain that it could be

429
00:15:57,790 --> 00:15:59,740
implemented in a variety of different

430
00:15:59,740 --> 00:16:01,360
ways because we want to give people

431
00:16:01,360 --> 00:16:04,510
control over their information but it's

432
00:16:04,510 --> 00:16:06,610
not enough because you can give them

433
00:16:06,610 --> 00:16:08,170
control over their personal information

434
00:16:08,170 --> 00:16:10,150
but if you don't actually secure and

435
00:16:10,150 --> 00:16:12,610
protect the data that actually builds

436
00:16:12,610 --> 00:16:14,110
the models the data that builds our

437
00:16:14,110 --> 00:16:16,330
information landscape you end up with a

438
00:16:16,330 --> 00:16:17,890
new kind of vulnerability amongst those

439
00:16:17,890 --> 00:16:20,610
who wants to control a social situation

440
00:16:20,610 --> 00:16:23,800
you know I don't know how many of you

441
00:16:23,800 --> 00:16:27,070
have ever read Snow Crash so those are

442
00:16:27,070 --> 00:16:28,930
the geeks in the room say hi to them

443
00:16:28,930 --> 00:16:31,060
Snow Crash is a really interesting book

444
00:16:31,060 --> 00:16:32,170
for those who've you have not read it I

445
00:16:32,170 --> 00:16:34,210
strongly recommend it because it was

446
00:16:34,210 --> 00:16:37,060
basically the Bible of tech culture in

447
00:16:37,060 --> 00:16:39,130
the late 90s and when I was in you know

448
00:16:39,130 --> 00:16:40,510
in the end in the industry as an

449
00:16:40,510 --> 00:16:42,760
engineer I was intrigued by how many

450
00:16:42,760 --> 00:16:44,020
people you couldn't go into a meeting

451
00:16:44,020 --> 00:16:46,390
without somebody referencing what they

452
00:16:46,390 --> 00:16:48,250
wanted to build from the world and Snow

453
00:16:48,250 --> 00:16:49,720
Crash and they were so excited they were

454
00:16:49,720 --> 00:16:50,830
like this is what we're gonna build the

455
00:16:50,830 --> 00:16:52,660
Metaverse is the solution to everything

456
00:16:52,660 --> 00:16:54,640
the Metaverse is basically like what

457
00:16:54,640 --> 00:16:55,990
would you imagine when you fully came

458
00:16:55,990 --> 00:16:57,490
into the internet right all of the

459
00:16:57,490 --> 00:16:59,980
immersive environment conversations the

460
00:16:59,980 --> 00:17:01,090
problem with it is is that Neal

461
00:17:01,090 --> 00:17:03,040
Stephenson's book is a dystopian novel

462
00:17:03,040 --> 00:17:06,490
it's not a utopia he argued what it

463
00:17:06,490 --> 00:17:07,960
would mean to build these complex

464
00:17:07,960 --> 00:17:09,970
information and landscapes many of the

465
00:17:09,970 --> 00:17:11,829
things that our engineers have designed

466
00:17:11,829 --> 00:17:14,890
for but the whole book the title is

467
00:17:14,890 --> 00:17:17,470
rooted in the idea that there is a virus

468
00:17:17,470 --> 00:17:20,050
that will spread that will fragment

469
00:17:20,050 --> 00:17:22,390
society by making it impossible for them

470
00:17:22,390 --> 00:17:24,550
to communicate the collapse of the Tower

471
00:17:24,550 --> 00:17:26,680
of Babel right and the whole point of

472
00:17:26,680 --> 00:17:28,210
this is that if you connect everybody

473
00:17:28,210 --> 00:17:30,430
there will be new exploits and those

474
00:17:30,430 --> 00:17:32,110
exploits will actually go into the

475
00:17:32,110 --> 00:17:35,260
vulnerability of the human social fabric

476
00:17:35,260 --> 00:17:37,420
and I want you to read this book when

477
00:17:37,420 --> 00:17:38,470
you go home it's fun

478
00:17:38,470 --> 00:17:40,600
there's even a good audiobook so you can

479
00:17:40,600 --> 00:17:42,820
actually understand where we designed

480
00:17:42,820 --> 00:17:44,799
for and what the implications are for

481
00:17:44,799 --> 00:17:47,200
now but I want to talk about what that

482
00:17:47,200 --> 00:17:49,270
is right now because it's not just an

483
00:17:49,270 --> 00:17:51,280
exploit of a terrace is an exploit

484
00:17:51,280 --> 00:17:52,690
that's actually happening on a more

485
00:17:52,690 --> 00:17:53,830
everyday basis

486
00:17:53,830 --> 00:17:56,799
so epistemology is a concept that says

487
00:17:56,799 --> 00:17:59,350
how do we know what we know right how do

488
00:17:59,350 --> 00:18:01,179
we build a set of knowledge how do we

489
00:18:01,179 --> 00:18:02,650
construct it socially how do we have

490
00:18:02,650 --> 00:18:03,850
competing in different epistemologies

491
00:18:03,850 --> 00:18:06,880
and in those frames the idea is that

492
00:18:06,880 --> 00:18:08,860
ignorance is what we don't yet know

493
00:18:08,860 --> 00:18:11,650
right but there's another way to

494
00:18:11,650 --> 00:18:14,200
actually think about ignorant ignorance

495
00:18:14,200 --> 00:18:17,080
can be strategically manufactured and in

496
00:18:17,080 --> 00:18:20,679
fact a 1995 Procter & Bowl point II term

497
00:18:20,679 --> 00:18:24,309
called AG Natali G and AG Natali G is

498
00:18:24,309 --> 00:18:26,640
the idea that you can strategically

499
00:18:26,640 --> 00:18:29,530
enable ignorant you can make in

500
00:18:29,530 --> 00:18:31,570
ignorance happen by making it difficult

501
00:18:31,570 --> 00:18:34,419
to tell what is reality by seeding doubt

502
00:18:34,419 --> 00:18:37,570
by just asking questions about whether

503
00:18:37,570 --> 00:18:39,880
or not you know there is a really a

504
00:18:39,880 --> 00:18:41,650
climate problem whether or not there's

505
00:18:41,650 --> 00:18:43,179
any link between tobacco and cancer

506
00:18:43,179 --> 00:18:46,960
whether or not as CNN put out a racist

507
00:18:46,960 --> 00:18:49,030
epitaph saying something like are Jews

508
00:18:49,030 --> 00:18:52,210
really people those frames are ways in

509
00:18:52,210 --> 00:18:55,299
which you concede ignorant you concede

510
00:18:55,299 --> 00:18:58,360
ideas of horror and they've been used

511
00:18:58,360 --> 00:19:00,370
politically for quite some time Russia

512
00:19:00,370 --> 00:19:02,679
today has been engaged in a coordinated

513
00:19:02,679 --> 00:19:05,380
campaign for many years trying to say

514
00:19:05,380 --> 00:19:07,600
asking questions and the whole program

515
00:19:07,600 --> 00:19:09,580
is called question more so for those who

516
00:19:09,580 --> 00:19:11,740
can't read this it says just how

517
00:19:11,740 --> 00:19:13,450
reliable is the evidence that suggests

518
00:19:13,450 --> 00:19:15,220
human activity impacts on climate change

519
00:19:15,220 --> 00:19:17,950
the answer isn't always clear-cut it's

520
00:19:17,950 --> 00:19:19,240
only possible to make a balanced

521
00:19:19,240 --> 00:19:21,340
judgment if you are better informed by

522
00:19:21,340 --> 00:19:23,620
challenging the accepted view we reveal

523
00:19:23,620 --> 00:19:25,539
the side of the news that you wouldn't

524
00:19:25,539 --> 00:19:27,730
normally see because we believe the more

525
00:19:27,730 --> 00:19:30,220
you question the more you know that's a

526
00:19:30,220 --> 00:19:32,559
way of seeding agna ecology seeding

527
00:19:32,559 --> 00:19:34,840
ignorance creating fractures within

528
00:19:34,840 --> 00:19:37,179
society and people who are engaged in

529
00:19:37,179 --> 00:19:40,630
doing so feel power they feel as though

530
00:19:40,630 --> 00:19:43,179
they can control information by actually

531
00:19:43,179 --> 00:19:45,580
in many ways corrupting the information

532
00:19:45,580 --> 00:19:48,309
landscape many of the reasons that

533
00:19:48,309 --> 00:19:50,140
people get into privacy is because they

534
00:19:50,140 --> 00:19:50,970
recognize

535
00:19:50,970 --> 00:19:53,520
privacy is connected to user trust we

536
00:19:53,520 --> 00:19:55,260
realize when we build systems that we

537
00:19:55,260 --> 00:19:57,480
want our users to trust what we're

538
00:19:57,480 --> 00:20:00,030
building to have confidence in us and we

539
00:20:00,030 --> 00:20:01,710
are in a situation in this industry

540
00:20:01,710 --> 00:20:03,690
where the companies that have been

541
00:20:03,690 --> 00:20:05,549
struggling around this are struggling

542
00:20:05,549 --> 00:20:07,200
with ability to communicate and engage

543
00:20:07,200 --> 00:20:09,450
people on trust because they don't know

544
00:20:09,450 --> 00:20:11,640
how to actually help people understand

545
00:20:11,640 --> 00:20:12,840
that they can have control over social

546
00:20:12,840 --> 00:20:15,299
situation I'm really proud to be at

547
00:20:15,299 --> 00:20:16,679
Microsoft because Microsoft has actually

548
00:20:16,679 --> 00:20:18,480
put privacy and trust at the center of

549
00:20:18,480 --> 00:20:19,919
things and I'm not saying the company

550
00:20:19,919 --> 00:20:21,539
has always done it right but really

551
00:20:21,539 --> 00:20:23,549
making that commitment and making that

552
00:20:23,549 --> 00:20:25,860
part of your values is really essential

553
00:20:25,860 --> 00:20:27,570
to me and it's one of the things we need

554
00:20:27,570 --> 00:20:29,700
to be doing collectively because one of

555
00:20:29,700 --> 00:20:30,809
the things we're dealing with is it's

556
00:20:30,809 --> 00:20:32,190
not just about whether or not we can

557
00:20:32,190 --> 00:20:34,620
fill the compliance but can we actually

558
00:20:34,620 --> 00:20:36,720
find a way through this industry to

559
00:20:36,720 --> 00:20:38,789
rebuild trust and that's gonna require

560
00:20:38,789 --> 00:20:40,830
us to really deal with the issue of

561
00:20:40,830 --> 00:20:42,750
creepiness and right now I would argue

562
00:20:42,750 --> 00:20:44,520
we're not doing it because we're not

563
00:20:44,520 --> 00:20:46,620
actually starting from a place of values

564
00:20:46,620 --> 00:20:48,630
we're not starting from a saying like we

565
00:20:48,630 --> 00:20:50,429
are gonna put the user Center we're

566
00:20:50,429 --> 00:20:51,870
still struggling to understand what we

567
00:20:51,870 --> 00:20:53,730
all mean and we're thinking that we can

568
00:20:53,730 --> 00:20:55,710
do this in a technical form and what I'm

569
00:20:55,710 --> 00:20:57,179
saying is that there isn't a technical

570
00:20:57,179 --> 00:20:59,340
solution through this this is a set of

571
00:20:59,340 --> 00:21:00,780
processes and practices and

572
00:21:00,780 --> 00:21:03,090
understanding like with security that

573
00:21:03,090 --> 00:21:06,659
the vulnerabilities will keep coming so

574
00:21:06,659 --> 00:21:08,370
fundamentally for me it's unacceptable

575
00:21:08,370 --> 00:21:10,679
that the technical skills and economic

576
00:21:10,679 --> 00:21:12,720
power allow people to exploit systems

577
00:21:12,720 --> 00:21:14,460
and we need to recognize that those

578
00:21:14,460 --> 00:21:16,200
exploitation --zz around privacy are

579
00:21:16,200 --> 00:21:17,549
connected to the other forms of security

580
00:21:17,549 --> 00:21:19,860
and that the very nature of security is

581
00:21:19,860 --> 00:21:22,530
evolving but fundamentally it comes back

582
00:21:22,530 --> 00:21:23,970
to this question of what are we trying

583
00:21:23,970 --> 00:21:26,250
to protect in the tech industry we often

584
00:21:26,250 --> 00:21:28,710
talk about the right to speak but it's

585
00:21:28,710 --> 00:21:30,360
actually kind of evolved to do a weird

586
00:21:30,360 --> 00:21:32,309
way of the right to be amplified and we

587
00:21:32,309 --> 00:21:33,870
don't know how to deal with that tension

588
00:21:33,870 --> 00:21:34,559
at all

589
00:21:34,559 --> 00:21:36,720
we don't know how to deal with what it

590
00:21:36,720 --> 00:21:38,280
means to reclaim control over social

591
00:21:38,280 --> 00:21:40,140
situations because we don't have a

592
00:21:40,140 --> 00:21:44,039
framework of power well actually some of

593
00:21:44,039 --> 00:21:45,720
our favorite people in the history of

594
00:21:45,720 --> 00:21:48,030
privacy do have a theory of power at a

595
00:21:48,030 --> 00:21:50,130
pathway through this I have to imagine

596
00:21:50,130 --> 00:21:52,350
that most of you know and perhaps love

597
00:21:52,350 --> 00:21:54,840
Brandeis I certainly do well I'm

598
00:21:54,840 --> 00:21:56,730
Brandeis and Warren pens their famous

599
00:21:56,730 --> 00:21:58,350
article on the right to privacy they

600
00:21:58,350 --> 00:22:00,510
built an argument around involuntarily

601
00:22:00,510 --> 00:22:03,570
disclosure and how it harmed people but

602
00:22:03,570 --> 00:22:04,560
the thing is I've also

603
00:22:04,560 --> 00:22:06,300
the other things throughout Brandeis is

604
00:22:06,300 --> 00:22:08,190
career so you may also know his essay

605
00:22:08,190 --> 00:22:10,280
work he argues that sunlight is the best

606
00:22:10,280 --> 00:22:12,510
disinfectant because the point of that

607
00:22:12,510 --> 00:22:13,860
was you actually have to purposefully

608
00:22:13,860 --> 00:22:16,530
expose systems of power in order to make

609
00:22:16,530 --> 00:22:18,540
a difference and it's at some level they

610
00:22:18,540 --> 00:22:20,250
seem contradictory but they're not

611
00:22:20,250 --> 00:22:22,590
because they actually can be understood

612
00:22:22,590 --> 00:22:25,110
as a way of seeing how he started to

613
00:22:25,110 --> 00:22:28,080
model the role of power across the

614
00:22:28,080 --> 00:22:30,150
information landscape a hundred years

615
00:22:30,150 --> 00:22:32,310
ago and that's the case that I actually

616
00:22:32,310 --> 00:22:34,050
find the most interesting is one that is

617
00:22:34,050 --> 00:22:35,460
not usually discussed in privacy

618
00:22:35,460 --> 00:22:38,280
conversations is the 1932 case known as

619
00:22:38,280 --> 00:22:41,610
Packer versus Utah and in his opinion in

620
00:22:41,610 --> 00:22:44,130
this he laid out a true theory of power

621
00:22:44,130 --> 00:22:46,200
that allows us to unlock some of the

622
00:22:46,200 --> 00:22:48,240
challenges and connect the dots between

623
00:22:48,240 --> 00:22:50,100
privacy in these kinds of manipulations

624
00:22:50,100 --> 00:22:53,250
so the case was that Utah outlawed

625
00:22:53,250 --> 00:22:56,730
smoking ads tobacco ads on billboards

626
00:22:56,730 --> 00:23:00,240
and packer corporations sued under the

627
00:23:00,240 --> 00:23:01,710
First Amendment saying that they should

628
00:23:01,710 --> 00:23:05,670
have the right to post on this why could

629
00:23:05,670 --> 00:23:08,790
they not put anything up and Brandeis

630
00:23:08,790 --> 00:23:13,140
you know wrote the the opinion as a

631
00:23:13,140 --> 00:23:15,420
member of the SCOTUS and he said that

632
00:23:15,420 --> 00:23:18,120
actually the Billboard is different than

633
00:23:18,120 --> 00:23:20,010
a magazine a billboard is part of a

634
00:23:20,010 --> 00:23:22,500
public sphere and a magazine you can

635
00:23:22,500 --> 00:23:24,630
choose the buyer not buy and then if you

636
00:23:24,630 --> 00:23:26,310
put it in the public sphere you will

637
00:23:26,310 --> 00:23:27,870
force people to have to divert their

638
00:23:27,870 --> 00:23:30,510
eyes and to do so would be creating

639
00:23:30,510 --> 00:23:33,420
captive audiences and captive audiences

640
00:23:33,420 --> 00:23:35,220
for him was a way of actually looking at

641
00:23:35,220 --> 00:23:37,380
the power differential and if you start

642
00:23:37,380 --> 00:23:38,940
to build this through you see that

643
00:23:38,940 --> 00:23:41,670
there's a way through legally socially

644
00:23:41,670 --> 00:23:43,470
and culturally through understanding the

645
00:23:43,470 --> 00:23:45,540
power dynamics in other words the

646
00:23:45,540 --> 00:23:47,970
commitment to speech the commitment to

647
00:23:47,970 --> 00:23:50,040
First Amendment is about giving people

648
00:23:50,040 --> 00:23:52,050
the right to speak but not necessarily

649
00:23:52,050 --> 00:23:54,120
the right to be amplified certainly not

650
00:23:54,120 --> 00:23:56,240
when that amplification can harm people

651
00:23:56,240 --> 00:23:58,380
so as we think about privacy going

652
00:23:58,380 --> 00:24:00,450
forward we want to actually converge

653
00:24:00,450 --> 00:24:02,970
privacy through that theory of power we

654
00:24:02,970 --> 00:24:05,010
want by focusing simply on the technical

655
00:24:05,010 --> 00:24:08,340
access issues we lose visibility by

656
00:24:08,340 --> 00:24:10,320
focusing on the individual we lose the

657
00:24:10,320 --> 00:24:12,720
structure privacy needs to actually

658
00:24:12,720 --> 00:24:14,550
understand that privacy is going to be

659
00:24:14,550 --> 00:24:16,240
networked going forward it

660
00:24:16,240 --> 00:24:18,190
part of a broader social fabric and we

661
00:24:18,190 --> 00:24:20,770
need a framework that allows us to see

662
00:24:20,770 --> 00:24:23,440
that the manipulation of systems is a

663
00:24:23,440 --> 00:24:25,570
way of exploiting privacy at the

664
00:24:25,570 --> 00:24:27,100
interstitials between people

665
00:24:27,100 --> 00:24:28,870
privacy can't simply be about the

666
00:24:28,870 --> 00:24:30,520
individual because it won't give us the

667
00:24:30,520 --> 00:24:32,679
tools to actually deal with the power

668
00:24:32,679 --> 00:24:34,990
dynamics and when speakers and

669
00:24:34,990 --> 00:24:37,900
amplifiers have the unchecked power to

670
00:24:37,900 --> 00:24:39,700
turn a blind eye to the cost of harm

671
00:24:39,700 --> 00:24:42,220
what's at stake for all of us is privacy

672
00:24:42,220 --> 00:24:45,540
dignity democracy and social cohesion so

673
00:24:45,540 --> 00:24:47,890
I am deeply grateful for all of you

674
00:24:47,890 --> 00:24:50,020
doing privacy work and I'm excited that

675
00:24:50,020 --> 00:24:51,340
you're gonna get to spend some time

676
00:24:51,340 --> 00:24:52,960
together really thinking through this I

677
00:24:52,960 --> 00:24:55,090
might hope and offering this up to you

678
00:24:55,090 --> 00:24:57,280
today aside from going into a dark place

679
00:24:57,280 --> 00:24:59,650
is to actually give you a way of

680
00:24:59,650 --> 00:25:02,530
thinking about and complicating your own

681
00:25:02,530 --> 00:25:04,900
views of what's going forward and I look

682
00:25:04,900 --> 00:25:06,460
forward to talking with you to more more

683
00:25:06,460 --> 00:25:09,670
across today and thank you for your time

684
00:25:09,670 --> 00:25:15,170
[Applause]

