1
00:00:04,360 --> 00:00:08,400
our panel here today is going to focus

2
00:00:08,400 --> 00:00:11,620
significantly on some recent work that

3
00:00:11,620 --> 00:00:14,500
seeks to certainly bridge those islands

4
00:00:14,500 --> 00:00:16,090
and builds on some of the work that has

5
00:00:16,090 --> 00:00:17,220
been done over the years on

6
00:00:17,220 --> 00:00:21,010
accountability and risk based models but

7
00:00:21,010 --> 00:00:22,630
I think in a sophisticated way that

8
00:00:22,630 --> 00:00:25,390
tries to really build on some of the

9
00:00:25,390 --> 00:00:29,310
earlier work and really address both

10
00:00:29,310 --> 00:00:31,780
global principles and European

11
00:00:31,780 --> 00:00:33,789
principles of data protection law in a

12
00:00:33,789 --> 00:00:35,829
way that seeks to update them and we'll

13
00:00:35,829 --> 00:00:38,940
debate here today whether or not these

14
00:00:38,940 --> 00:00:42,130
papers and processes that the gentleman

15
00:00:42,130 --> 00:00:45,880
here have been involved with do so and

16
00:00:45,880 --> 00:00:48,579
we'll try to critique and push back and

17
00:00:48,579 --> 00:00:51,280
and debate it so let me lead off with

18
00:00:51,280 --> 00:00:54,220
Fred Kate who as the one of the lead

19
00:00:54,220 --> 00:00:57,670
offers authors of some of these papers

20
00:00:57,670 --> 00:01:00,040
together with Victor Meyer Schoenberger

21
00:01:00,040 --> 00:01:02,739
and with Peter Cullen and input and work

22
00:01:02,739 --> 00:01:04,809
from Richard have really now sought to

23
00:01:04,809 --> 00:01:06,130
put this issue on the table so Fred

24
00:01:06,130 --> 00:01:07,570
would you give us a bit of an overview

25
00:01:07,570 --> 00:01:11,470
of the recent work and kind of the key

26
00:01:11,470 --> 00:01:13,510
points and then we'll we'll try to

27
00:01:13,510 --> 00:01:15,790
elucidate and and debate and see if we

28
00:01:15,790 --> 00:01:18,340
can make some progress I'm delighted

29
00:01:18,340 --> 00:01:19,990
thank you I'm delighted to be here I'm

30
00:01:19,990 --> 00:01:21,909
still really in a bit from being called

31
00:01:21,909 --> 00:01:24,100
pragmatic and interesting and in an

32
00:01:24,100 --> 00:01:25,960
academic world that would lead to my

33
00:01:25,960 --> 00:01:29,229
almost certain firing so for those of

34
00:01:29,229 --> 00:01:31,240
you watching on TV she was talking about

35
00:01:31,240 --> 00:01:33,580
somebody somebody else it also had not

36
00:01:33,580 --> 00:01:34,570
occurred to me Jules until your

37
00:01:34,570 --> 00:01:36,189
introduction that jakub I'm afraid

38
00:01:36,189 --> 00:01:37,570
you're surrounded by people who have

39
00:01:37,570 --> 00:01:38,950
actually worked in part on this project

40
00:01:38,950 --> 00:01:42,850
it's it's not it's not as it's not a it

41
00:01:42,850 --> 00:01:44,590
wasn't intended that way but isn't it

42
00:01:44,590 --> 00:01:49,720
but I gosh I think it exactly right but

43
00:01:49,720 --> 00:01:52,420
let me do say how important i think the

44
00:01:52,420 --> 00:01:54,220
project you were announcing earlier is

45
00:01:54,220 --> 00:01:57,460
and how proud i am to be part of it and

46
00:01:57,460 --> 00:01:59,049
i think the idea of working

47
00:01:59,049 --> 00:02:01,210
pragmatically to try to bridge gaps

48
00:02:01,210 --> 00:02:04,180
which have for far too long separated us

49
00:02:04,180 --> 00:02:06,040
and that I think we have with more

50
00:02:06,040 --> 00:02:08,949
experience more practical ideas now for

51
00:02:08,949 --> 00:02:11,140
to how to make those gaps be less of a

52
00:02:11,140 --> 00:02:13,209
barrier I think it's an incredibly

53
00:02:13,209 --> 00:02:14,760
important initiative and

54
00:02:14,760 --> 00:02:18,569
I offer congratulations even while

55
00:02:18,569 --> 00:02:20,549
people might wonder why you allowed me

56
00:02:20,549 --> 00:02:22,230
to be part of it they never there you

57
00:02:22,230 --> 00:02:24,299
may wonder by the but by the end of this

58
00:02:24,299 --> 00:02:27,540
so let me just say a very very briefly

59
00:02:27,540 --> 00:02:29,519
what would normally take on you know an

60
00:02:29,519 --> 00:02:31,799
hour or 45 minutes to cover which is I

61
00:02:31,799 --> 00:02:33,599
think we're all aware of this rapidly

62
00:02:33,599 --> 00:02:35,940
changing world in which data protections

63
00:02:35,940 --> 00:02:37,560
taking place and that's very much a

64
00:02:37,560 --> 00:02:40,560
theme here the advent of big data the

65
00:02:40,560 --> 00:02:41,819
tremendous rise of government

66
00:02:41,819 --> 00:02:43,859
surveillance that tremendous rise of

67
00:02:43,859 --> 00:02:46,230
surveillance technologies in general and

68
00:02:46,230 --> 00:02:47,790
i don't mean surveillance necessarily in

69
00:02:47,790 --> 00:02:49,829
a in a bad sense in other words we all

70
00:02:49,829 --> 00:02:51,569
carry cell phones that locators that

71
00:02:51,569 --> 00:02:53,790
provide us maps we increasingly are

72
00:02:53,790 --> 00:02:55,590
integrating technology into our lives in

73
00:02:55,590 --> 00:02:57,209
ways that the technology will be

74
00:02:57,209 --> 00:02:59,519
embedded will be connected will be

75
00:02:59,519 --> 00:03:02,609
wearable you know i wear an insulin pump

76
00:03:02,609 --> 00:03:04,889
now which provides a constant stream of

77
00:03:04,889 --> 00:03:07,590
data about me all the time but over time

78
00:03:07,590 --> 00:03:08,909
that's going to get smaller and smaller

79
00:03:08,909 --> 00:03:11,250
and it recently will be implanted it

80
00:03:11,250 --> 00:03:13,620
won't even be a separate device and of

81
00:03:13,620 --> 00:03:15,060
course has also been discussed here the

82
00:03:15,060 --> 00:03:16,500
Internet of Things where we're going to

83
00:03:16,500 --> 00:03:19,859
be surrounded by devices by lights by

84
00:03:19,859 --> 00:03:22,709
cameras by refrigerators and toasters

85
00:03:22,709 --> 00:03:25,879
and other appliances that are constantly

86
00:03:25,879 --> 00:03:29,040
yielding a set of data that will be

87
00:03:29,040 --> 00:03:30,720
linked to individuals and so I think the

88
00:03:30,720 --> 00:03:32,910
challenge that we face across the world

89
00:03:32,910 --> 00:03:35,190
by no means limited to Europe of the

90
00:03:35,190 --> 00:03:37,650
United States is how to keep data

91
00:03:37,650 --> 00:03:39,750
protection current in the face of those

92
00:03:39,750 --> 00:03:42,930
challenges how to exactly the challenge

93
00:03:42,930 --> 00:03:45,209
that a commission Graham was talking

94
00:03:45,209 --> 00:03:47,190
about how to make sure that without

95
00:03:47,190 --> 00:03:49,680
changing any of the fundamental goals

96
00:03:49,680 --> 00:03:51,870
here we make sure that we're using tools

97
00:03:51,870 --> 00:03:55,109
that work in this weather you think it's

98
00:03:55,109 --> 00:03:57,989
a brave new world or horrible new world

99
00:03:57,989 --> 00:03:59,940
it seems to be the world in which we are

100
00:03:59,940 --> 00:04:02,280
emerging into so to address that

101
00:04:02,280 --> 00:04:04,409
challenge of 18 months ago I've just

102
00:04:04,409 --> 00:04:07,139
outside of London a group of us got

103
00:04:07,139 --> 00:04:09,299
together three former three former

104
00:04:09,299 --> 00:04:11,489
commissioners including including

105
00:04:11,489 --> 00:04:15,569
Richard and economist and Victor and I

106
00:04:15,569 --> 00:04:18,539
as academics and Peter who was there as

107
00:04:18,539 --> 00:04:20,430
the token Canadian so it sounds a little

108
00:04:20,430 --> 00:04:22,889
bit like a bad joke five lawyers and

109
00:04:22,889 --> 00:04:24,810
economists in a Canadian walk into a bar

110
00:04:24,810 --> 00:04:25,890
and

111
00:04:25,890 --> 00:04:28,570
this is what happens but our goal was

112
00:04:28,570 --> 00:04:30,100
trying to think in terms of meeting

113
00:04:30,100 --> 00:04:32,470
these new challenges what what are the

114
00:04:32,470 --> 00:04:35,890
fundamental changes or evolution of our

115
00:04:35,890 --> 00:04:37,300
current system that would be necessary

116
00:04:37,300 --> 00:04:39,910
to ensure that the rights that we

117
00:04:39,910 --> 00:04:42,310
consider fundamental are in fact

118
00:04:42,310 --> 00:04:45,100
protected in this in this face so let me

119
00:04:45,100 --> 00:04:47,050
just make for quick points here the

120
00:04:47,050 --> 00:04:49,600
first and that is that the data

121
00:04:49,600 --> 00:04:52,120
protection systems around the world need

122
00:04:52,120 --> 00:04:54,130
to place more responsibility for data

123
00:04:54,130 --> 00:04:57,130
stewardship and liability when the

124
00:04:57,130 --> 00:04:59,170
stewardship fails so both stewardship

125
00:04:59,170 --> 00:05:02,290
and liability on the users on the

126
00:05:02,290 --> 00:05:05,530
processors of data and place less

127
00:05:05,530 --> 00:05:08,050
responsibility on the individual in

128
00:05:08,050 --> 00:05:10,570
terms of notice and choice right we all

129
00:05:10,570 --> 00:05:12,970
live through a sea of notices virtually

130
00:05:12,970 --> 00:05:14,140
nobody ever reads them they're

131
00:05:14,140 --> 00:05:17,050
incomprehensible when you do notice is

132
00:05:17,050 --> 00:05:18,700
may serve many important purposes but

133
00:05:18,700 --> 00:05:20,080
there may be better ways to accomplish

134
00:05:20,080 --> 00:05:22,420
those purposes and in fact of greatest

135
00:05:22,420 --> 00:05:24,520
concern to all of us in that group was

136
00:05:24,520 --> 00:05:27,670
the fear that choice was being used too

137
00:05:27,670 --> 00:05:30,160
frequently on both sides of the Atlantic

138
00:05:30,160 --> 00:05:32,140
but frankly may be no place clearer is

139
00:05:32,140 --> 00:05:34,150
this in Canada and the United States a

140
00:05:34,150 --> 00:05:35,710
choice was being used as a way of

141
00:05:35,710 --> 00:05:37,810
shifting responsibility onto individuals

142
00:05:37,810 --> 00:05:40,810
to say you consent it and therefore now

143
00:05:40,810 --> 00:05:42,760
we can do something with your data which

144
00:05:42,760 --> 00:05:44,380
might not actually be in your best

145
00:05:44,380 --> 00:05:47,260
interest so the second point is that as

146
00:05:47,260 --> 00:05:50,950
we engage in this increasing focus on

147
00:05:50,950 --> 00:05:53,410
data stewardship and data land liability

148
00:05:53,410 --> 00:05:56,830
for for uses of data outside of

149
00:05:56,830 --> 00:05:58,900
appropriate stewardship but we should

150
00:05:58,900 --> 00:06:01,420
focus more and more attention on uses of

151
00:06:01,420 --> 00:06:03,520
big data as opposed to focusing

152
00:06:03,520 --> 00:06:06,730
exclusively or or significantly as is

153
00:06:06,730 --> 00:06:08,320
the case for example the United States

154
00:06:08,320 --> 00:06:11,500
on collection and retention of data and

155
00:06:11,500 --> 00:06:13,330
the reasons for this I think we're

156
00:06:13,330 --> 00:06:15,340
obvious but especially in a world of big

157
00:06:15,340 --> 00:06:17,500
data in which data have enormous value

158
00:06:17,500 --> 00:06:20,040
for all sorts of uses that will have

159
00:06:20,040 --> 00:06:22,420
utility that in some cases can be in

160
00:06:22,420 --> 00:06:25,810
fact life-saving if we still go back to

161
00:06:25,810 --> 00:06:28,180
what was the moment at time in which the

162
00:06:28,180 --> 00:06:30,490
data were collected and what was the the

163
00:06:30,490 --> 00:06:32,320
terms of consent at that moment or what

164
00:06:32,320 --> 00:06:34,870
was a stated purpose we are inevitably

165
00:06:34,870 --> 00:06:36,790
going to be restricting valuable

166
00:06:36,790 --> 00:06:39,259
potentially critical uses of day

167
00:06:39,259 --> 00:06:42,180
or over I think one of the things that

168
00:06:42,180 --> 00:06:44,550
really struck us in this process is that

169
00:06:44,550 --> 00:06:46,770
uses of data are where individuals tend

170
00:06:46,770 --> 00:06:49,050
to be most affected and I don't at all

171
00:06:49,050 --> 00:06:50,460
mean to suggest that individuals are not

172
00:06:50,460 --> 00:06:52,409
affected by collection or access of data

173
00:06:52,409 --> 00:06:53,940
by the way access would be considered to

174
00:06:53,940 --> 00:06:56,849
use but rather to say that there's there

175
00:06:56,849 --> 00:06:59,039
so often is an obvious reason for

176
00:06:59,039 --> 00:07:01,199
collecting the data look at Edward

177
00:07:01,199 --> 00:07:03,569
Snowden's revelations for example the

178
00:07:03,569 --> 00:07:05,490
data were all lawfully collected the

179
00:07:05,490 --> 00:07:06,930
phone companies collected them others

180
00:07:06,930 --> 00:07:08,699
collected them the question was the

181
00:07:08,699 --> 00:07:11,310
subsequent use made of the data it's in

182
00:07:11,310 --> 00:07:13,729
that context that we suddenly see a

183
00:07:13,729 --> 00:07:16,319
significant threatening privacy

184
00:07:16,319 --> 00:07:18,659
violation when they are repurposed in a

185
00:07:18,659 --> 00:07:21,900
dramatic way that takes no aventis focus

186
00:07:21,900 --> 00:07:24,180
so much more on the power of the

187
00:07:24,180 --> 00:07:25,800
government or on the terms of collection

188
00:07:25,800 --> 00:07:27,990
rather than on an assessment of risk

189
00:07:27,990 --> 00:07:31,620
around use and that assessment for risk

190
00:07:31,620 --> 00:07:34,099
seems incredibly important as a way to

191
00:07:34,099 --> 00:07:36,990
allocate scarce resources as a way to

192
00:07:36,990 --> 00:07:39,900
focus attention but not as a replacement

193
00:07:39,900 --> 00:07:42,360
for other types of protection rather as

194
00:07:42,360 --> 00:07:44,639
a focusing mechanism as many of our

195
00:07:44,639 --> 00:07:46,830
participants refer to it the third

196
00:07:46,830 --> 00:07:49,409
principle is that as we increasingly

197
00:07:49,409 --> 00:07:51,630
focus on risk which we're already doing

198
00:07:51,630 --> 00:07:53,849
we have risk assessment requirements

199
00:07:53,849 --> 00:07:55,259
throughout Europe we have them in the

200
00:07:55,259 --> 00:07:57,000
United States we have them in much of

201
00:07:57,000 --> 00:07:59,819
Asia for privacy impact assessment or

202
00:07:59,819 --> 00:08:02,250
for other types of assessment we need to

203
00:08:02,250 --> 00:08:03,750
start thinking more collectively about

204
00:08:03,750 --> 00:08:05,460
what are the types of risks were worried

205
00:08:05,460 --> 00:08:08,099
about up until this point it's largely

206
00:08:08,099 --> 00:08:10,259
been satisfactory to say well we worry

207
00:08:10,259 --> 00:08:15,270
about collects such as non compliance

208
00:08:15,270 --> 00:08:16,620
with the law well that is a real risk

209
00:08:16,620 --> 00:08:18,840
and we should worry about that but

210
00:08:18,840 --> 00:08:20,940
increasingly because of the role that

211
00:08:20,940 --> 00:08:23,130
individuals of that information plays in

212
00:08:23,130 --> 00:08:26,099
individuals lives we better be worried

213
00:08:26,099 --> 00:08:28,979
about the the the other the practical

214
00:08:28,979 --> 00:08:31,440
harms the tangible harms the harms that

215
00:08:31,440 --> 00:08:33,049
can result in significant economic

216
00:08:33,049 --> 00:08:35,399
disparity that can result in people

217
00:08:35,399 --> 00:08:38,309
being disadvantaged in extremely clear

218
00:08:38,309 --> 00:08:41,219
and direct ways and we really need to

219
00:08:41,219 --> 00:08:45,060
focus on a way of developing a common

220
00:08:45,060 --> 00:08:47,910
vocabulary for risk assessment what is

221
00:08:47,910 --> 00:08:49,980
it we think when we're doing privacy

222
00:08:49,980 --> 00:08:51,540
impact assessments what are those

223
00:08:51,540 --> 00:08:53,459
impacts we're worried about what our

224
00:08:53,459 --> 00:08:55,680
categories of impacts what our tools

225
00:08:55,680 --> 00:08:57,420
what our methodologies and this is an

226
00:08:57,420 --> 00:08:59,339
area in which richard has been quite

227
00:08:59,339 --> 00:09:00,899
influential in spoke I think yesterday

228
00:09:00,899 --> 00:09:03,360
you're about and then finally I know

229
00:09:03,360 --> 00:09:04,639
those are the words you're looking for

230
00:09:04,639 --> 00:09:08,370
we need to ensure that as we continue to

231
00:09:08,370 --> 00:09:10,230
evolve data protection to respond to

232
00:09:10,230 --> 00:09:12,540
these significant challenges that we

233
00:09:12,540 --> 00:09:15,360
always are enhancing transparency and

234
00:09:15,360 --> 00:09:18,480
redress and enforcement and transparency

235
00:09:18,480 --> 00:09:20,370
especially if we focus less on long

236
00:09:20,370 --> 00:09:22,290
notice is that nobody reads which I

237
00:09:22,290 --> 00:09:23,970
would argue don't really provide very

238
00:09:23,970 --> 00:09:25,949
effective transparency we need to be

239
00:09:25,949 --> 00:09:27,420
thinking of better tools for providing

240
00:09:27,420 --> 00:09:29,610
transparency how do we achieve the type

241
00:09:29,610 --> 00:09:31,410
of openness that that builds the

242
00:09:31,410 --> 00:09:33,810
confidence that we care about that

243
00:09:33,810 --> 00:09:35,399
people have that their privacy will be

244
00:09:35,399 --> 00:09:38,130
protected how do we ensure that there's

245
00:09:38,130 --> 00:09:40,440
redress for individuals who are harmed

246
00:09:40,440 --> 00:09:43,139
or injured or affected in some way by

247
00:09:43,139 --> 00:09:44,790
the use of their data and how do we

248
00:09:44,790 --> 00:09:47,610
ensure effective well-financed well

249
00:09:47,610 --> 00:09:50,069
targeted enforcement and without those

250
00:09:50,069 --> 00:09:52,620
tools no system is going to be effective

251
00:09:52,620 --> 00:09:55,139
in the future so let me say I don't

252
00:09:55,139 --> 00:09:58,560
think we proposed January now 18 months

253
00:09:58,560 --> 00:10:00,690
ago an answer to all of these problems

254
00:10:00,690 --> 00:10:03,720
we proposed a big inning of a process to

255
00:10:03,720 --> 00:10:06,209
try to look at them a way to not in any

256
00:10:06,209 --> 00:10:08,100
way alter our understanding of

257
00:10:08,100 --> 00:10:10,440
fundamental rights but rather to ensure

258
00:10:10,440 --> 00:10:12,120
that those rights continue to be a

259
00:10:12,120 --> 00:10:15,120
protected as we move into the brave new

260
00:10:15,120 --> 00:10:17,750
future

