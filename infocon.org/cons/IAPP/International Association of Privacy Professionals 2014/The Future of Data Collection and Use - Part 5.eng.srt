1
00:00:04,510 --> 00:00:06,260
collection is obviously an issue that

2
00:00:06,260 --> 00:00:09,170
comes up around the world and efforts

3
00:00:09,170 --> 00:00:11,059
that simply deal with you so not

4
00:00:11,059 --> 00:00:13,250
collection end up bumping into those who

5
00:00:13,250 --> 00:00:14,930
are worried that collection means the

6
00:00:14,930 --> 00:00:18,380
NSA collection means who goes how do you

7
00:00:18,380 --> 00:00:21,740
were sort of respond to the concern

8
00:00:21,740 --> 00:00:24,400
about collection is some of it let's

9
00:00:24,400 --> 00:00:26,869
recognize that the data stewards or the

10
00:00:26,869 --> 00:00:30,109
policy makers will recognize the

11
00:00:30,109 --> 00:00:32,840
legitimate value of certain collection

12
00:00:32,840 --> 00:00:35,390
and then it's other kinds of collection

13
00:00:35,390 --> 00:00:37,040
that might end up being subject to

14
00:00:37,040 --> 00:00:39,050
choice or restrictions how does this

15
00:00:39,050 --> 00:00:41,510
address the collection concern I think

16
00:00:41,510 --> 00:00:44,960
yacouba's really framed the reality of

17
00:00:44,960 --> 00:00:46,850
it and one of the messaging challenges

18
00:00:46,850 --> 00:00:49,040
is the the core of all my work starts

19
00:00:49,040 --> 00:00:52,070
with principles it doesn't doesn't leave

20
00:00:52,070 --> 00:00:55,250
those alone it says that and i hearken

21
00:00:55,250 --> 00:00:58,460
back to David Smith's wonderful critique

22
00:00:58,460 --> 00:01:00,350
at the OECD which said you know that the

23
00:01:00,350 --> 00:01:01,730
principles really aren't the problem is

24
00:01:01,730 --> 00:01:03,080
the way we've implemented them over the

25
00:01:03,080 --> 00:01:04,699
last 20 or 30 years that have become the

26
00:01:04,699 --> 00:01:07,930
problem and now it's time to change

27
00:01:07,930 --> 00:01:10,910
collection of course has to play a role

28
00:01:10,910 --> 00:01:13,820
in it but a byproduct in my view of

29
00:01:13,820 --> 00:01:15,980
what's happened over the last 20 or 30

30
00:01:15,980 --> 00:01:18,800
years is that we've called focused

31
00:01:18,800 --> 00:01:21,080
perhaps too much on collection which

32
00:01:21,080 --> 00:01:23,570
made lots of sense when you could count

33
00:01:23,570 --> 00:01:25,640
the number of transatlantic data

34
00:01:25,640 --> 00:01:27,350
transfers on one hand and still have

35
00:01:27,350 --> 00:01:30,020
fingers left over may not make the same

36
00:01:30,020 --> 00:01:32,150
degree of sense in a ubiquitous world of

37
00:01:32,150 --> 00:01:34,100
wearable computing but that doesn't mean

38
00:01:34,100 --> 00:01:36,979
to say the collection is important it

39
00:01:36,979 --> 00:01:38,810
also doesn't mean to say that most of

40
00:01:38,810 --> 00:01:41,900
the other existing fair information

41
00:01:41,900 --> 00:01:44,060
principles still applies of course data

42
00:01:44,060 --> 00:01:46,640
security still applies of course a right

43
00:01:46,640 --> 00:01:49,700
to access information still applies but

44
00:01:49,700 --> 00:01:53,750
I want to actually the consent thing is

45
00:01:53,750 --> 00:01:55,640
just an interesting a messaging example

46
00:01:55,640 --> 00:01:56,720
that actually want to borrow from

47
00:01:56,720 --> 00:01:57,920
something you said this morning which

48
00:01:57,920 --> 00:02:01,000
was so profound having a debate about

49
00:02:01,000 --> 00:02:03,680
whether consent actually is the European

50
00:02:03,680 --> 00:02:05,570
model and the American model probably

51
00:02:05,570 --> 00:02:07,310
isn't all that useful you and you you

52
00:02:07,310 --> 00:02:08,780
framed it in terms of instead of trying

53
00:02:08,780 --> 00:02:12,499
to rationalize or fight between the two

54
00:02:12,499 --> 00:02:14,420
sides saying your model needs to change

55
00:02:14,420 --> 00:02:16,819
to ours our needs to change drawers one

56
00:02:16,819 --> 00:02:17,930
of the things I think we would be do

57
00:02:17,930 --> 00:02:21,140
great disturbances if we say well you're

58
00:02:21,140 --> 00:02:23,000
wrong in your premise consent isn't

59
00:02:23,000 --> 00:02:24,950
dominant therefore your conclusions are

60
00:02:24,950 --> 00:02:27,799
all wrong that would be disastrous what

61
00:02:27,799 --> 00:02:30,530
I think we say is look that's but one

62
00:02:30,530 --> 00:02:32,329
reason why the model needs to change it

63
00:02:32,329 --> 00:02:33,680
doesn't much matter we should actually

64
00:02:33,680 --> 00:02:37,129
almost close that debate and say hmm in

65
00:02:37,129 --> 00:02:40,510
a world of ubiquitous computing of

66
00:02:40,510 --> 00:02:42,950
automated data collection of machine

67
00:02:42,950 --> 00:02:45,859
learning machine decision-making we do

68
00:02:45,859 --> 00:02:48,379
need to find a different model and it

69
00:02:48,379 --> 00:02:50,510
should be likely more based on

70
00:02:50,510 --> 00:02:52,730
responsibility of data stewards around

71
00:02:52,730 --> 00:02:55,340
use recognizing that there are still

72
00:02:55,340 --> 00:02:57,859
many other principles one thing that has

73
00:02:57,859 --> 00:03:00,200
actually got no press which i think is

74
00:03:00,200 --> 00:03:02,780
kind of interesting in a way one of the

75
00:03:02,780 --> 00:03:04,310
suggestions we actually made to the

76
00:03:04,310 --> 00:03:06,290
revisions to the principal's was to

77
00:03:06,290 --> 00:03:10,250
embed a principle of enforcement that

78
00:03:10,250 --> 00:03:12,230
seems a pretty powerful advancement to

79
00:03:12,230 --> 00:03:14,030
data protection and somewhat at odds

80
00:03:14,030 --> 00:03:15,709
with perhaps the people you met in

81
00:03:15,709 --> 00:03:17,150
Silicon Valley that said that it's

82
00:03:17,150 --> 00:03:19,489
contrary to the business model so I you

83
00:03:19,489 --> 00:03:20,959
know I think that it will thats that's

84
00:03:20,959 --> 00:03:24,290
the one point I fully agree well-funded

85
00:03:24,290 --> 00:03:30,530
dps always and although i'll note there

86
00:03:30,530 --> 00:03:32,690
obviously many well-funded stirrups in

87
00:03:32,690 --> 00:03:35,269
Silicon Valley and yes I feel like a

88
00:03:35,269 --> 00:03:36,919
grandfather to and we were visiting a

89
00:03:36,919 --> 00:03:39,169
couple of companies let's just note that

90
00:03:39,169 --> 00:03:42,199
about maybe half of the economy in the

91
00:03:42,199 --> 00:03:45,229
u.s. is regulated by significant you

92
00:03:45,229 --> 00:03:47,090
space restrictions banking credit

93
00:03:47,090 --> 00:03:49,069
employment a whole and many of those

94
00:03:49,069 --> 00:03:51,470
startups start moving along and then

95
00:03:51,470 --> 00:03:53,389
they learn that well there's health data

96
00:03:53,389 --> 00:03:55,909
involved and so sorry they actually are

97
00:03:55,909 --> 00:03:58,879
now subject to some very strict stricter

98
00:03:58,879 --> 00:04:00,859
often than many of the EU based laws

99
00:04:00,859 --> 00:04:03,560
around restrictions for HIPAA on health

100
00:04:03,560 --> 00:04:05,419
data or banking data or credit later or

101
00:04:05,419 --> 00:04:07,099
employment data and so what one day

102
00:04:07,099 --> 00:04:08,389
someone's going to do a good economic

103
00:04:08,389 --> 00:04:09,859
analysis but of course the sexy

104
00:04:09,859 --> 00:04:11,090
companies who are doing some of the

105
00:04:11,090 --> 00:04:12,859
wildest and craziest things obviously

106
00:04:12,859 --> 00:04:16,009
are some of our sometimes our tech

107
00:04:16,009 --> 00:04:19,940
startups who do indeed need to bump into

108
00:04:19,940 --> 00:04:21,918
the concern I think we're going to try

109
00:04:21,918 --> 00:04:22,970
to squeeze in one question but a

110
00:04:22,970 --> 00:04:26,060
reaction from anybody else on the panel

111
00:04:26,060 --> 00:04:28,340
poking just so I think that Yakoub and I

112
00:04:28,340 --> 00:04:30,320
are very close on most of these poets or

113
00:04:30,320 --> 00:04:30,930
one or two to

114
00:04:30,930 --> 00:04:32,910
it says but I think absolutely yeah I

115
00:04:32,910 --> 00:04:34,699
think none of us are seeking to

116
00:04:34,699 --> 00:04:37,500
legitimize unacceptable business

117
00:04:37,500 --> 00:04:38,970
practices we talk about some business

118
00:04:38,970 --> 00:04:40,710
models we're trying to legitimize those

119
00:04:40,710 --> 00:04:43,169
absolutely not i mean what i would say

120
00:04:43,169 --> 00:04:45,360
some of these startups if you tell them

121
00:04:45,360 --> 00:04:47,370
you know you can do you can go ahead but

122
00:04:47,370 --> 00:04:48,930
you must not depend fundamental rights

123
00:04:48,930 --> 00:04:51,660
for freedoms they will just carry on so

124
00:04:51,660 --> 00:04:54,090
I worry about I worry about that i think

125
00:04:54,090 --> 00:04:56,070
the point we've made you know really

126
00:04:56,070 --> 00:04:59,190
this is all about trying to shift more

127
00:04:59,190 --> 00:05:02,039
real responsibility onto the shoulders

128
00:05:02,039 --> 00:05:04,289
of the data controllers and give more

129
00:05:04,289 --> 00:05:06,449
confidence to the consumers i mean in

130
00:05:06,449 --> 00:05:08,669
another area food safety there's been

131
00:05:08,669 --> 00:05:11,550
big debates around how you communicate

132
00:05:11,550 --> 00:05:13,860
the risks of certain sorts of diets and

133
00:05:13,860 --> 00:05:17,130
certain ingredients to consumers and in

134
00:05:17,130 --> 00:05:19,740
the end that debate finally to quite a

135
00:05:19,740 --> 00:05:22,590
large extent got boiled down to the

136
00:05:22,590 --> 00:05:25,080
traffic light approach and consumers now

137
00:05:25,080 --> 00:05:27,240
have red amber green if we can do more

138
00:05:27,240 --> 00:05:29,820
of that we can signal the risk areas the

139
00:05:29,820 --> 00:05:32,130
green areas which completely safe the

140
00:05:32,130 --> 00:05:34,229
low priority is and maybe the amber

141
00:05:34,229 --> 00:05:36,360
areas where you need to take a little

142
00:05:36,360 --> 00:05:38,849
more care on both sides and just finally

143
00:05:38,849 --> 00:05:41,639
responding to your point Jules I mean

144
00:05:41,639 --> 00:05:44,010
absolutely papers we are developing do

145
00:05:44,010 --> 00:05:45,599
mention benefits there wasn't time in my

146
00:05:45,599 --> 00:05:47,250
three minute presentation just then but

147
00:05:47,250 --> 00:05:49,860
balancing the benefits and is absolutely

148
00:05:49,860 --> 00:05:53,720
fundamental we completely accept that

