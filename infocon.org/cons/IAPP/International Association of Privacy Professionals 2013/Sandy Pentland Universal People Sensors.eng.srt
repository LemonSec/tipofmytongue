1
00:00:05,660 --> 00:00:10,380
so I was interesting yeah I'll have to

2
00:00:10,380 --> 00:00:12,570
be as interesting so you like this this

3
00:00:12,570 --> 00:00:16,529
is given to me by the slide by a fellow

4
00:00:16,529 --> 00:00:19,080
from a big telco so you can sort of see

5
00:00:19,080 --> 00:00:22,830
what they're thinking so what I want to

6
00:00:22,830 --> 00:00:24,449
talk about is I want to talk about the

7
00:00:24,449 --> 00:00:26,880
universal people sensors and what are

8
00:00:26,880 --> 00:00:28,529
the pluses and minuses and where we're

9
00:00:28,529 --> 00:00:30,779
going so these are people moving around

10
00:00:30,779 --> 00:00:33,690
in San Francisco the most likely stores

11
00:00:33,690 --> 00:00:35,790
and by watching what people do you can

12
00:00:35,790 --> 00:00:38,280
tell what their preferences are now I

13
00:00:38,280 --> 00:00:40,920
should mention that I'm on the board of

14
00:00:40,920 --> 00:00:44,190
Motorola telefonica Nissan Bruno so I

15
00:00:44,190 --> 00:00:46,440
know something about this and the last

16
00:00:46,440 --> 00:00:48,450
speaker made it very clear that there's

17
00:00:48,450 --> 00:00:50,309
a lot of information that you can get

18
00:00:50,309 --> 00:00:52,920
out of this data and the normal reaction

19
00:00:52,920 --> 00:00:56,579
that people have is think that I've got

20
00:00:56,579 --> 00:01:00,329
to stop this um and I'm gonna tell you

21
00:01:00,329 --> 00:01:02,969
that that's wrong I think that you

22
00:01:02,969 --> 00:01:05,188
should rename this conference not

23
00:01:05,188 --> 00:01:08,729
privacy but something having to do with

24
00:01:08,729 --> 00:01:12,150
data as an asset and it's an asset that

25
00:01:12,150 --> 00:01:15,090
you should control not the company's and

26
00:01:15,090 --> 00:01:17,070
that's really the question is who

27
00:01:17,070 --> 00:01:19,710
controls the data and can you be secure

28
00:01:19,710 --> 00:01:21,830
and sharing it for particular purposes

29
00:01:21,830 --> 00:01:25,229
um so let me give you some quick

30
00:01:25,229 --> 00:01:29,189
examples so for the car company it turns

31
00:01:29,189 --> 00:01:32,520
out that if I can watch all those GPS

32
00:01:32,520 --> 00:01:34,549
things that are in your cars nowadays

33
00:01:34,549 --> 00:01:37,200
there are many circumstances where I can

34
00:01:37,200 --> 00:01:39,420
predict whether or not you will have an

35
00:01:39,420 --> 00:01:41,700
accident with about a fifty percent

36
00:01:41,700 --> 00:01:44,149
chance in the next two or three seconds

37
00:01:44,149 --> 00:01:46,350
because if i watch somebody else and

38
00:01:46,350 --> 00:01:48,420
they have a breaking event and you're

39
00:01:48,420 --> 00:01:50,640
going down the same place at a slightly

40
00:01:50,640 --> 00:01:53,880
higher speed it turns out you are at

41
00:01:53,880 --> 00:01:57,149
great risk so don't you want to share

42
00:01:57,149 --> 00:02:00,509
that data so you can have your kids grow

43
00:02:00,509 --> 00:02:02,670
up so that you can continue to have a

44
00:02:02,670 --> 00:02:06,719
whole body maybe or here's another one

45
00:02:06,719 --> 00:02:09,330
we build a system is for returning

46
00:02:09,330 --> 00:02:11,190
soldiers but I think it'll be available

47
00:02:11,190 --> 00:02:13,630
it watches you

48
00:02:13,630 --> 00:02:14,890
on your phone it looks at your

49
00:02:14,890 --> 00:02:17,980
socialization your physical activity how

50
00:02:17,980 --> 00:02:19,720
the pattern of your life and it turns

51
00:02:19,720 --> 00:02:22,180
out that those are diagnostic criteria

52
00:02:22,180 --> 00:02:25,600
formal medical diagnostic criteria for

53
00:02:25,600 --> 00:02:28,150
mental diseases like depression like

54
00:02:28,150 --> 00:02:31,660
PTSD like schizophrenia so you can on

55
00:02:31,660 --> 00:02:35,440
your phone visualize your mental social

56
00:02:35,440 --> 00:02:38,170
health in terms that are understandable

57
00:02:38,170 --> 00:02:41,020
focus activity social and what this does

58
00:02:41,020 --> 00:02:43,300
is this uses a system i'll talk about in

59
00:02:43,300 --> 00:02:46,480
just a second to share data in a

60
00:02:46,480 --> 00:02:49,840
completely secure manner so that you can

61
00:02:49,840 --> 00:02:52,860
see how you compare to people like you

62
00:02:52,860 --> 00:02:54,970
because when people see these things

63
00:02:54,970 --> 00:02:57,400
they say oh it's just a bad day or I'm a

64
00:02:57,400 --> 00:02:59,770
bad person but when they begin to

65
00:02:59,770 --> 00:03:02,710
compare with other people they realize

66
00:03:02,710 --> 00:03:04,600
that maybe they should take action about

67
00:03:04,600 --> 00:03:06,610
this I should mention that the mental

68
00:03:06,610 --> 00:03:09,340
diseases in terms of disability are the

69
00:03:09,340 --> 00:03:12,010
number one health problem in the world

70
00:03:12,010 --> 00:03:17,040
or the cancer more than more you name it

71
00:03:17,040 --> 00:03:19,780
that's the thing is the burden of

72
00:03:19,780 --> 00:03:22,030
humanity and we can actually watch it

73
00:03:22,030 --> 00:03:24,610
and through sharing we can begin to do

74
00:03:24,610 --> 00:03:27,660
things about it here's another one

75
00:03:27,660 --> 00:03:30,610
you're all familiar with pandemics and

76
00:03:30,610 --> 00:03:32,620
flu and and you know about google flu

77
00:03:32,620 --> 00:03:35,590
okay but it turns out that if I can

78
00:03:35,590 --> 00:03:38,740
launch your phones on a minute-by-minute

79
00:03:38,740 --> 00:03:42,640
basis how you move who you call where

80
00:03:42,640 --> 00:03:45,910
you go I can do a really good job of

81
00:03:45,910 --> 00:03:48,430
telling if you've been infected with the

82
00:03:48,430 --> 00:03:51,820
flu and if I can watch the people around

83
00:03:51,820 --> 00:03:55,270
you I can do a tremendously good job of

84
00:03:55,270 --> 00:03:57,730
actually watching the flu move from

85
00:03:57,730 --> 00:04:01,570
person to person to person now that's

86
00:04:01,570 --> 00:04:06,220
George Orwell to cube power okay it's

87
00:04:06,220 --> 00:04:08,250
the most invasive thing you can imagine

88
00:04:08,250 --> 00:04:11,620
but now here's the trade-off there are

89
00:04:11,620 --> 00:04:14,280
no protections today I guess pandemics

90
00:04:14,280 --> 00:04:17,320
the vaccines don't really work they have

91
00:04:17,320 --> 00:04:18,970
huge side effects not everybody takes

92
00:04:18,970 --> 00:04:23,229
them anyhow so you have to ask are there

93
00:04:23,229 --> 00:04:24,669
conditions under which we're going to

94
00:04:24,669 --> 00:04:27,210
allow this level of spying on people

95
00:04:27,210 --> 00:04:29,920
or are we gonna just let the 200 million

96
00:04:29,920 --> 00:04:32,470
people die because that's what's

97
00:04:32,470 --> 00:04:34,810
predicted for the X pandemic those are

98
00:04:34,810 --> 00:04:36,190
the trade-offs so what I'm telling you

99
00:04:36,190 --> 00:04:39,790
is really important is the big data this

100
00:04:39,790 --> 00:04:43,330
sort of invasive data has rewards public

101
00:04:43,330 --> 00:04:47,170
goods public goods that cannot really be

102
00:04:47,170 --> 00:04:51,010
ignored because the lives of your kids

103
00:04:51,010 --> 00:04:55,390
depend on them they also have enormous

104
00:04:55,390 --> 00:04:57,250
risks which I think the people in this

105
00:04:57,250 --> 00:05:00,160
room are very sensitive to what are we

106
00:05:00,160 --> 00:05:04,180
going to do well I was concerned enough

107
00:05:04,180 --> 00:05:07,170
about this that about five years ago I

108
00:05:07,170 --> 00:05:09,550
convinced the World Economic Forum to

109
00:05:09,550 --> 00:05:12,580
start a discussion about this with the

110
00:05:12,580 --> 00:05:14,230
vice president of the EU a

111
00:05:14,230 --> 00:05:16,300
representative from the US National

112
00:05:16,300 --> 00:05:18,850
Security Agency the chairman of the

113
00:05:18,850 --> 00:05:21,490
Federal Trade Commission the chairman of

114
00:05:21,490 --> 00:05:25,630
Vodafone the chairman of Microsoft and

115
00:05:25,630 --> 00:05:28,840
on and on as well as advocacy groups and

116
00:05:28,840 --> 00:05:31,210
our conversation centered around what I

117
00:05:31,210 --> 00:05:35,050
call the New Deal on data because in my

118
00:05:35,050 --> 00:05:38,590
view the politics of this are that we

119
00:05:38,590 --> 00:05:40,900
should not be talking about privacy we

120
00:05:40,900 --> 00:05:42,160
should talking about how i can get

121
00:05:42,160 --> 00:05:45,280
public goods to save our kids to make

122
00:05:45,280 --> 00:05:47,020
our life better to cure global warming

123
00:05:47,020 --> 00:05:49,510
we can actually do amazing things on

124
00:05:49,510 --> 00:05:52,120
problems like that while at the same

125
00:05:52,120 --> 00:05:53,800
time making sure the companies don't go

126
00:05:53,800 --> 00:05:55,720
bankrupt wouldn't be any good to let

127
00:05:55,720 --> 00:05:59,140
them go bankrupt but also it has to be

128
00:05:59,140 --> 00:06:00,550
something that's in the direct in

129
00:06:00,550 --> 00:06:02,410
immediate interest of the citizens of

130
00:06:02,410 --> 00:06:05,680
the consumers it has to be a win-win-win

131
00:06:05,680 --> 00:06:08,110
solution and the results of this

132
00:06:08,110 --> 00:06:10,750
conversation were put out in a thing

133
00:06:10,750 --> 00:06:12,580
that's called personal data emergence of

134
00:06:12,580 --> 00:06:15,490
a new asset class and what it argues is

135
00:06:15,490 --> 00:06:19,570
that the value and data is when it flows

136
00:06:19,570 --> 00:06:22,090
and is shared that's how you save kids

137
00:06:22,090 --> 00:06:23,919
that's how you make the buses run on

138
00:06:23,919 --> 00:06:26,410
time that's how you by the consumer

139
00:06:26,410 --> 00:06:28,060
goods that make your life may be more

140
00:06:28,060 --> 00:06:32,860
interesting and if it's a good if it's a

141
00:06:32,860 --> 00:06:35,830
public if it's if it's a good then it's

142
00:06:35,830 --> 00:06:39,250
something that is an asset and the first

143
00:06:39,250 --> 00:06:40,810
question you have to ask about acid

144
00:06:40,810 --> 00:06:44,169
who owns this asset it's like land right

145
00:06:44,169 --> 00:06:46,810
the first question is title to the asset

146
00:06:46,810 --> 00:06:49,180
and the only politically viable thing

147
00:06:49,180 --> 00:06:51,600
and industry agrees with this obviously

148
00:06:51,600 --> 00:06:54,669
government does is that individuals have

149
00:06:54,669 --> 00:06:58,900
to own in some sense data about them now

150
00:06:58,900 --> 00:07:01,030
that's difficult in various ways but

151
00:07:01,030 --> 00:07:03,280
i'll show you how you can do a lot of it

152
00:07:03,280 --> 00:07:06,760
okay so we've gotten people the

153
00:07:06,760 --> 00:07:09,040
industries were just talked about bought

154
00:07:09,040 --> 00:07:11,410
off on the idea that individuals have to

155
00:07:11,410 --> 00:07:14,169
control data about them they have to

156
00:07:14,169 --> 00:07:15,550
have ownership in the sense of Old

157
00:07:15,550 --> 00:07:18,610
English law common law the rights of

158
00:07:18,610 --> 00:07:20,650
disposal the rights to take back data

159
00:07:20,650 --> 00:07:23,020
the right to know where the data went

160
00:07:23,020 --> 00:07:26,010
and these have helped shape the EU

161
00:07:26,010 --> 00:07:29,020
initiative on human rights and data and

162
00:07:29,020 --> 00:07:31,150
the consumer privacy Bill of Rights here

163
00:07:31,150 --> 00:07:33,970
in the United States and in the last

164
00:07:33,970 --> 00:07:37,300
Davos they have these wonderful artists

165
00:07:37,300 --> 00:07:40,860
who draw the conversation as you do it

166
00:07:40,860 --> 00:07:43,930
we had an industry spouse to these

167
00:07:43,930 --> 00:07:47,590
regulatory proposals and it was clearly

168
00:07:47,590 --> 00:07:50,320
we have to have true informed consent no

169
00:07:50,320 --> 00:07:53,919
more euless no more terms and conditions

170
00:07:53,919 --> 00:07:57,280
as they are today people were

171
00:07:57,280 --> 00:07:59,350
enthusiastic industry behind you right

172
00:07:59,350 --> 00:08:01,000
industry was enthusiastic by the idea

173
00:08:01,000 --> 00:08:04,090
that all personal data would be legally

174
00:08:04,090 --> 00:08:07,690
required to have metadata that gives the

175
00:08:07,690 --> 00:08:10,270
provenance who gave permissions what

176
00:08:10,270 --> 00:08:13,000
exactly were the permissions and that

177
00:08:13,000 --> 00:08:15,280
that has to be attached to the data and

178
00:08:15,280 --> 00:08:17,110
what that lets you do is that lets you

179
00:08:17,110 --> 00:08:20,320
automatically audit where the data goes

180
00:08:20,320 --> 00:08:22,330
who has it what they did with it and

181
00:08:22,330 --> 00:08:23,590
where they compliant with the original

182
00:08:23,590 --> 00:08:27,490
permissions and the local law it also

183
00:08:27,490 --> 00:08:32,589
lets you revoke permissions say don't

184
00:08:32,589 --> 00:08:34,270
like what you're doing with my data give

185
00:08:34,270 --> 00:08:37,270
it back and because it's auditable you

186
00:08:37,270 --> 00:08:39,370
can enforce that let me tell you a

187
00:08:39,370 --> 00:08:41,679
little bit about it it's not just a

188
00:08:41,679 --> 00:08:44,320
computer science problem but we've built

189
00:08:44,320 --> 00:08:47,200
a sort of reference standard so together

190
00:08:47,200 --> 00:08:49,570
with NGO we have Institute for

191
00:08:49,570 --> 00:08:51,670
data-driven design John clip and juror

192
00:08:51,670 --> 00:08:52,750
was head of the

193
00:08:52,750 --> 00:08:54,460
harvard law lab and myself set this up

194
00:08:54,460 --> 00:08:56,260
we've built a reference standard

195
00:08:56,260 --> 00:08:59,440
software that implements these ideas

196
00:08:59,440 --> 00:09:01,570
it's open source it's available for

197
00:09:01,570 --> 00:09:03,790
everyone it includes things like open ID

198
00:09:03,790 --> 00:09:09,040
connect so we some of my guys helped

199
00:09:09,040 --> 00:09:11,020
chair the national strategy for trusted

200
00:09:11,020 --> 00:09:15,330
identities in cyberspace and stick and

201
00:09:15,330 --> 00:09:18,820
said daza Greenwood and as part of that

202
00:09:18,820 --> 00:09:22,740
we've taken over from the military some

203
00:09:22,740 --> 00:09:26,740
technology for highly secure identity

204
00:09:26,740 --> 00:09:29,380
assurance in order to get rid of

205
00:09:29,380 --> 00:09:31,510
passwords and get rid of many other

206
00:09:31,510 --> 00:09:33,670
things that go along with that but

207
00:09:33,670 --> 00:09:35,230
importantly it's not just a technical

208
00:09:35,230 --> 00:09:36,520
solution there has to be a legal

209
00:09:36,520 --> 00:09:39,130
solution not new laws as the last

210
00:09:39,130 --> 00:09:40,630
speaker pointed out because that's like

211
00:09:40,630 --> 00:09:43,240
really unlikely to happen but it turns

212
00:09:43,240 --> 00:09:45,460
out that in contract law you can do a

213
00:09:45,460 --> 00:09:49,990
very good job of enforcing data sharing

214
00:09:49,990 --> 00:09:54,430
data privacy data ownership rights just

215
00:09:54,430 --> 00:09:55,720
by contracts and that's wonderful

216
00:09:55,720 --> 00:09:57,400
because it's interoperable among

217
00:09:57,400 --> 00:10:00,790
different jurisdictions okay and an

218
00:10:00,790 --> 00:10:02,589
example is the Swift network for

219
00:10:02,589 --> 00:10:05,230
interbank transfer there's no laws about

220
00:10:05,230 --> 00:10:07,890
it it's just a peer-to-peer contract

221
00:10:07,890 --> 00:10:10,930
where the contract and the actual

222
00:10:10,930 --> 00:10:14,080
technical computer science e messaging

223
00:10:14,080 --> 00:10:17,950
are one-to-one the messages implement

224
00:10:17,950 --> 00:10:20,440
contracts the contracts can be reduced

225
00:10:20,440 --> 00:10:23,380
directly into computer code and that

226
00:10:23,380 --> 00:10:25,960
combination of the two is what gives you

227
00:10:25,960 --> 00:10:28,089
security because you can know precisely

228
00:10:28,089 --> 00:10:30,360
what's happening and it's enforceable

229
00:10:30,360 --> 00:10:34,720
under a contract law a framework and in

230
00:10:34,720 --> 00:10:37,150
fact in the contract already is written

231
00:10:37,150 --> 00:10:40,600
in harms and remedies and mediation so

232
00:10:40,600 --> 00:10:42,220
you don't have to hire lawyers you don't

233
00:10:42,220 --> 00:10:44,230
have to sorry for those are you or

234
00:10:44,230 --> 00:10:46,290
alerts you don't have to go to court

235
00:10:46,290 --> 00:10:49,210
another thing that's in the system that

236
00:10:49,210 --> 00:10:51,850
is best practice is people talk about

237
00:10:51,850 --> 00:10:55,150
sharing data most times that's stupid

238
00:10:55,150 --> 00:10:57,700
you don't need to share data you need

239
00:10:57,700 --> 00:11:01,990
share answers so the system here has a

240
00:11:01,990 --> 00:11:05,320
secure way of answering questions

241
00:11:05,320 --> 00:11:07,510
an oil you want and that way you can

242
00:11:07,510 --> 00:11:09,570
answer the minimum amount of information

243
00:11:09,570 --> 00:11:12,100
possible like yes I'm interested or yes

244
00:11:12,100 --> 00:11:14,290
I'm in San Francisco not your exact

245
00:11:14,290 --> 00:11:17,470
location so we've spun this up in

246
00:11:17,470 --> 00:11:20,170
various places for instance in trento

247
00:11:20,170 --> 00:11:23,470
we've got the government there the local

248
00:11:23,470 --> 00:11:25,540
telcos the bus company in the power

249
00:11:25,540 --> 00:11:28,210
company to adopt this framework and

250
00:11:28,210 --> 00:11:31,390
experiment it to look at how secure is

251
00:11:31,390 --> 00:11:33,640
it how does sharing data help I

252
00:11:33,640 --> 00:11:35,170
mentioned some of the public goods at

253
00:11:35,170 --> 00:11:36,970
the beginning and how do people feel

254
00:11:36,970 --> 00:11:38,980
about it so we're starting with young

255
00:11:38,980 --> 00:11:41,470
families and with elders and giving them

256
00:11:41,470 --> 00:11:43,960
the ability to share data and see data

257
00:11:43,960 --> 00:11:46,090
in a way that hasn't been possible

258
00:11:46,090 --> 00:11:49,450
before and that's the point of spinning

259
00:11:49,450 --> 00:11:51,070
these things up is to get facts on the

260
00:11:51,070 --> 00:11:53,260
ground too many of these conversations

261
00:11:53,260 --> 00:11:56,560
are done in policy it's all just talk

262
00:11:56,560 --> 00:11:58,930
talk talk no facts we're going to get

263
00:11:58,930 --> 00:12:01,330
facts let me give you one more example

264
00:12:01,330 --> 00:12:04,210
and then conclude so one of the things

265
00:12:04,210 --> 00:12:05,740
that comes out of this type of reasoning

266
00:12:05,740 --> 00:12:07,930
this is that there ought to be a data

267
00:12:07,930 --> 00:12:11,980
Commons we have taxes we have public

268
00:12:11,980 --> 00:12:14,410
informations the government supplies why

269
00:12:14,410 --> 00:12:16,330
doesn't this data that we're all talking

270
00:12:16,330 --> 00:12:19,180
about have a Commons and so I helped

271
00:12:19,180 --> 00:12:21,820
talk orage into creating what is

272
00:12:21,820 --> 00:12:25,020
probably the first large big data

273
00:12:25,020 --> 00:12:27,970
Commons it's in the country the ivory

274
00:12:27,970 --> 00:12:30,370
coast which had a civil war that ended

275
00:12:30,370 --> 00:12:33,430
about two years ago at this point and

276
00:12:33,430 --> 00:12:34,690
what they did is they released all the

277
00:12:34,690 --> 00:12:37,320
telephone records for the entire country

278
00:12:37,320 --> 00:12:40,900
along with financial data etc etc but

279
00:12:40,900 --> 00:12:45,900
aggregated in a very interesting way and

280
00:12:46,260 --> 00:12:48,760
with a legal agreement under contract

281
00:12:48,760 --> 00:12:51,730
law as I just talked about and we

282
00:12:51,730 --> 00:12:54,490
recruited almost 90 universities from

283
00:12:54,490 --> 00:12:56,650
all around the world to look at this

284
00:12:56,650 --> 00:12:59,170
data asked wasn't safe could you break

285
00:12:59,170 --> 00:13:01,990
it could you read n tify people and what

286
00:13:01,990 --> 00:13:03,820
could you do that was a public good some

287
00:13:03,820 --> 00:13:05,140
of the public goods as we figured out

288
00:13:05,140 --> 00:13:06,640
how to reduce commute time by ten

289
00:13:06,640 --> 00:13:09,840
percent in the big city that's enormous

290
00:13:09,840 --> 00:13:13,210
we figured out how to radically improve

291
00:13:13,210 --> 00:13:16,330
their malaria mitigation strategy and

292
00:13:16,330 --> 00:13:18,209
their reaction to Penn

293
00:13:18,209 --> 00:13:21,569
amex we also were able for the first

294
00:13:21,569 --> 00:13:22,829
time to produce something that's like

295
00:13:22,829 --> 00:13:25,709
census data in a place that's just done

296
00:13:25,709 --> 00:13:28,709
with a civil war millions dead you don't

297
00:13:28,709 --> 00:13:30,629
have census you don't know where poverty

298
00:13:30,629 --> 00:13:32,279
is you don't know where the kids are

299
00:13:32,279 --> 00:13:34,649
dying it turns out you can use this data

300
00:13:34,649 --> 00:13:36,329
to determine where the kids are dying

301
00:13:36,329 --> 00:13:39,179
with very high accuracy because people

302
00:13:39,179 --> 00:13:42,179
have reliable behavioral changes when

303
00:13:42,179 --> 00:13:43,740
they're in different conditions and you

304
00:13:43,740 --> 00:13:45,749
could even look at ethnic divisions in a

305
00:13:45,749 --> 00:13:47,339
way that had never been possible before

306
00:13:47,339 --> 00:13:49,709
and of course those ethnic divisions are

307
00:13:49,709 --> 00:13:52,529
the source of the Civil War so those are

308
00:13:52,529 --> 00:13:53,999
some of the public goods and I will

309
00:13:53,999 --> 00:13:57,360
notice ninety teams University teams

310
00:13:57,360 --> 00:13:59,879
people who are experts at this no one

311
00:13:59,879 --> 00:14:02,519
was able to reify anything no one was

312
00:14:02,519 --> 00:14:04,170
able to do anything particularly bad and

313
00:14:04,170 --> 00:14:05,279
if they had we would have thrown the

314
00:14:05,279 --> 00:14:07,439
suckers in jail because they had sent a

315
00:14:07,439 --> 00:14:08,819
contract that said they were going to do

316
00:14:08,819 --> 00:14:11,069
this and only this and they would not

317
00:14:11,069 --> 00:14:13,740
let the data go further so we had on

318
00:14:13,740 --> 00:14:18,300
okay so what's next well there's a lot

319
00:14:18,300 --> 00:14:21,269
of things that are next so this is me 15

320
00:14:21,269 --> 00:14:25,230
years ago this is when we invented the

321
00:14:25,230 --> 00:14:26,910
things that are now Google glass my

322
00:14:26,910 --> 00:14:29,369
student there in front pad is actually

323
00:14:29,369 --> 00:14:31,709
the technical lead for google glass so

324
00:14:31,709 --> 00:14:34,049
we literally started it all and at it I

325
00:14:34,049 --> 00:14:36,179
can tell you the privacy debates around

326
00:14:36,179 --> 00:14:40,379
that were rather intense and that is why

327
00:14:40,379 --> 00:14:42,629
google glass flashes and makes noises

328
00:14:42,629 --> 00:14:45,119
when it takes pictures now we have some

329
00:14:45,119 --> 00:14:47,670
other security elements to it which they

330
00:14:47,670 --> 00:14:49,949
haven't yet put in but one can imagine

331
00:14:49,949 --> 00:14:53,220
that they'll go in that direction and I

332
00:14:53,220 --> 00:14:56,040
can also tell you that you know this

333
00:14:56,040 --> 00:14:57,980
type of thing is just the beginning

334
00:14:57,980 --> 00:15:00,839
you're going to see far more data about

335
00:15:00,839 --> 00:15:03,899
far more things publicly available

336
00:15:03,899 --> 00:15:05,939
personally available then you can even

337
00:15:05,939 --> 00:15:10,079
imagine and it's up to us to figure out

338
00:15:10,079 --> 00:15:12,199
how to turn that into a public good

339
00:15:12,199 --> 00:15:14,869
rather than just simply to lock it down

340
00:15:14,869 --> 00:15:18,299
so safety for individuals but public

341
00:15:18,299 --> 00:15:20,339
good please don't forget the public good

342
00:15:20,339 --> 00:15:24,379
I want my kids to grow up safe

