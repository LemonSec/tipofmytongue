1
00:00:00,030 --> 00:00:14,910
Parliament can't make sure to the stage

2
00:00:06,270 --> 00:00:17,279
and give her a warm applause thank you

3
00:00:14,910 --> 00:00:19,650
very much one of us on the stage enjoys

4
00:00:17,279 --> 00:00:21,630
applauses more than than the other and I

5
00:00:19,650 --> 00:00:27,240
don't think holding her back will will

6
00:00:21,630 --> 00:00:30,150
help and this European flag is is all of

7
00:00:27,240 --> 00:00:33,110
ours but it's also what has been the

8
00:00:30,150 --> 00:00:36,300
center of a lot of debates and policy

9
00:00:33,110 --> 00:00:38,930
fights for a free and open Internet for

10
00:00:36,300 --> 00:00:41,640
the last few years for last few decades

11
00:00:38,930 --> 00:00:44,129
there is copyright reform there is a

12
00:00:41,640 --> 00:00:46,559
fight for net neutrality there is upload

13
00:00:44,129 --> 00:00:50,129
filters there is a lot of the things

14
00:00:46,559 --> 00:00:52,530
that you've been discussing in this camp

15
00:00:50,129 --> 00:00:59,309
and that you'll be discussing in the CCC

16
00:00:52,530 --> 00:01:01,920
camp further on we now have a new term

17
00:00:59,309 --> 00:01:04,438
for both the European Parliament we also

18
00:01:01,920 --> 00:01:09,060
have a new term for the European

19
00:01:04,438 --> 00:01:12,419
Commission and we will be having let's

20
00:01:09,060 --> 00:01:16,799
see if this works it might not work flag

21
00:01:12,420 --> 00:01:21,689
is blowing in the wind we will be having

22
00:01:16,799 --> 00:01:23,670
this lady was laughs underlined as the

23
00:01:21,689 --> 00:01:27,419
head of the European Commission

24
00:01:23,670 --> 00:01:29,820
she had a speech in the European

25
00:01:27,420 --> 00:01:34,500
Parliament where she described the

26
00:01:29,820 --> 00:01:36,919
digital change as follows she was

27
00:01:34,500 --> 00:01:40,189
discussing the French wheat farmers

28
00:01:36,920 --> 00:01:43,710
experiencing drought she was discussing

29
00:01:40,189 --> 00:01:46,500
the French population experiencing a

30
00:01:43,710 --> 00:01:48,389
deadly heat wave and then she was

31
00:01:46,500 --> 00:01:52,649
talking about the concrete effects of

32
00:01:48,390 --> 00:01:54,030
digitalization if this is the view from

33
00:01:52,649 --> 00:01:57,450
the highest place in the European

34
00:01:54,030 --> 00:01:59,520
Commission on how digitalization is I

35
00:01:57,450 --> 00:02:02,369
think we should be kind of worried if

36
00:01:59,520 --> 00:02:05,759
it's seen as something as destructive to

37
00:02:02,369 --> 00:02:12,319
the way that we live as the climate

38
00:02:05,759 --> 00:02:13,920
changes I have titled this talk as a a

39
00:02:12,319 --> 00:02:16,170
new digit

40
00:02:13,920 --> 00:02:19,980
frontier because I think that we're

41
00:02:16,170 --> 00:02:22,200
moving into uncharted territory during

42
00:02:19,980 --> 00:02:26,760
the different battles that we've had so

43
00:02:22,200 --> 00:02:28,980
far we have had a community of people

44
00:02:26,760 --> 00:02:32,220
fighting for the internet fighting for

45
00:02:28,980 --> 00:02:35,459
the use of digitalization that have not

46
00:02:32,220 --> 00:02:40,890
been meeting that much unified

47
00:02:35,459 --> 00:02:44,060
resistance from politicians and media as

48
00:02:40,890 --> 00:02:48,299
we have seen the last few years I

49
00:02:44,060 --> 00:02:51,390
believe that the copyright reform

50
00:02:48,300 --> 00:02:54,269
recently was one of the times where we

51
00:02:51,390 --> 00:02:56,700
saw the old industry protecting their

52
00:02:54,269 --> 00:02:58,980
business models in an effective way

53
00:02:56,700 --> 00:03:00,959
where we were not able to stop the

54
00:02:58,980 --> 00:03:02,640
reforms and to stop the upload filters

55
00:03:00,959 --> 00:03:06,840
that have been included in the new

56
00:03:02,640 --> 00:03:12,059
directive so how what is what is on our

57
00:03:06,840 --> 00:03:15,420
horizon as we as we start out in the

58
00:03:12,060 --> 00:03:16,739
next five years there has been leaked

59
00:03:15,420 --> 00:03:19,320
the document and there has been the

60
00:03:16,739 --> 00:03:21,980
elite some intentions from the European

61
00:03:19,320 --> 00:03:25,320
Commission about looking into especially

62
00:03:21,980 --> 00:03:27,959
digital platforms and also looking into

63
00:03:25,320 --> 00:03:31,320
the e-commerce directive the e-commerce

64
00:03:27,959 --> 00:03:35,040
directive allows us as users to upload

65
00:03:31,320 --> 00:03:38,340
content without the platforms being held

66
00:03:35,040 --> 00:03:40,950
responsible what we're seeing is the

67
00:03:38,340 --> 00:03:43,500
European Commission wanting to hold

68
00:03:40,950 --> 00:03:45,780
these platforms responsible for the

69
00:03:43,500 --> 00:03:48,090
content that the users are uploading

70
00:03:45,780 --> 00:03:52,620
it's a much larger extent than it has

71
00:03:48,090 --> 00:03:56,130
been up until now the problem for me is

72
00:03:52,620 --> 00:03:59,329
that they see platforms as a unified

73
00:03:56,130 --> 00:04:02,400
thing instead of seeing platforms as

74
00:03:59,329 --> 00:04:04,890
multifunctional they only see them as

75
00:04:02,400 --> 00:04:08,040
having having one function and only

76
00:04:04,890 --> 00:04:11,279
being regulated in one way but each

77
00:04:08,040 --> 00:04:15,420
platform has different users usages for

78
00:04:11,280 --> 00:04:17,310
each user and also the number of uses

79
00:04:15,420 --> 00:04:19,709
that we have for each platform is

80
00:04:17,310 --> 00:04:24,200
ever-increasing and the way that we

81
00:04:19,709 --> 00:04:27,260
regulate the platforms shouldn't be

82
00:04:24,200 --> 00:04:29,630
exclusive to one way of seeing

83
00:04:27,260 --> 00:04:33,140
each platform if we look at for example

84
00:04:29,630 --> 00:04:33,710
facebook it's a sales platform on their

85
00:04:33,140 --> 00:04:38,120
marketplace

86
00:04:33,710 --> 00:04:40,489
it's an repository for pictures it is a

87
00:04:38,120 --> 00:04:43,760
message platform it is an iMessage

88
00:04:40,490 --> 00:04:46,820
platform and all of these uses have

89
00:04:43,760 --> 00:04:49,219
different responsibilities for the

90
00:04:46,820 --> 00:04:52,040
platform and for the users but if you

91
00:04:49,220 --> 00:04:54,200
view as a from a regulatory point of

92
00:04:52,040 --> 00:04:57,020
view if you view the platform as just

93
00:04:54,200 --> 00:04:59,150
being one thing you won't be able to

94
00:04:57,020 --> 00:05:04,219
actually regulate it in a in a

95
00:04:59,150 --> 00:05:06,799
well-functioning way I think we should

96
00:05:04,220 --> 00:05:09,500
instead of looking at the platforms and

97
00:05:06,800 --> 00:05:13,490
looking at the way the digital tools are

98
00:05:09,500 --> 00:05:16,070
as a knife on our dinner table we should

99
00:05:13,490 --> 00:05:18,200
perhaps more see it as the nice Swiss

100
00:05:16,070 --> 00:05:21,080
Army knife that we used to have in our

101
00:05:18,200 --> 00:05:24,159
pockets when we could still have pocket

102
00:05:21,080 --> 00:05:27,080
knives it has multiple functions

103
00:05:24,160 --> 00:05:29,090
according to what you pull out and we

104
00:05:27,080 --> 00:05:32,000
should therefore allow for this in the

105
00:05:29,090 --> 00:05:34,729
regulation that we have unfortunately it

106
00:05:32,000 --> 00:05:38,560
seems like the way that we regulate at

107
00:05:34,730 --> 00:05:40,990
the moment is not used to having

108
00:05:38,560 --> 00:05:43,790
multifunctional digital tools

109
00:05:40,990 --> 00:05:47,270
multifunctional digital platforms or

110
00:05:43,790 --> 00:05:50,780
media if you see if you see the

111
00:05:47,270 --> 00:05:53,539
platforms as a distributor of news and

112
00:05:50,780 --> 00:05:56,809
you want to regulate it as a traditional

113
00:05:53,540 --> 00:05:59,900
media I think you've misunderstood what

114
00:05:56,810 --> 00:06:02,780
kind of a platform what the platform is

115
00:05:59,900 --> 00:06:05,870
and how it can be used from different

116
00:06:02,780 --> 00:06:08,809
different users perspectives and we

117
00:06:05,870 --> 00:06:11,300
would need to have a way of regulating

118
00:06:08,810 --> 00:06:15,740
this that corresponds to the different

119
00:06:11,300 --> 00:06:18,530
uses that we have it's also a new

120
00:06:15,740 --> 00:06:21,140
frontier in the way that we've been used

121
00:06:18,530 --> 00:06:24,049
to having digital tools we've been used

122
00:06:21,140 --> 00:06:26,870
to having the Internet as a tool that

123
00:06:24,050 --> 00:06:30,740
regulates part of our life but where I

124
00:06:26,870 --> 00:06:34,160
see it now is that everything in the way

125
00:06:30,740 --> 00:06:37,720
that we live our lives is being formed

126
00:06:34,160 --> 00:06:39,560
by the way that digital tools and and

127
00:06:37,720 --> 00:06:43,340
digitalization

128
00:06:39,560 --> 00:06:45,800
is regulated and is being used therefore

129
00:06:43,340 --> 00:06:49,369
we need to be able to build in the

130
00:06:45,800 --> 00:06:52,460
values that we want in our society into

131
00:06:49,370 --> 00:06:57,830
the digital world instead of just having

132
00:06:52,460 --> 00:07:00,320
it as something that is is considered a

133
00:06:57,830 --> 00:07:04,520
tool is considered technological behind

134
00:07:00,320 --> 00:07:06,620
a screen in order to do that we need to

135
00:07:04,520 --> 00:07:09,289
look at well what are what are the

136
00:07:06,620 --> 00:07:11,620
values that we have what are the things

137
00:07:09,290 --> 00:07:14,600
that we want to protect what are the

138
00:07:11,620 --> 00:07:17,840
what are the people that are creating

139
00:07:14,600 --> 00:07:20,000
the tools we all come with different

140
00:07:17,840 --> 00:07:24,590
backgrounds and different ways of seeing

141
00:07:20,000 --> 00:07:27,860
the world if we are not aware of the

142
00:07:24,590 --> 00:07:31,520
consequences that the tools that we

143
00:07:27,860 --> 00:07:35,690
build then we will be a then we will be

144
00:07:31,520 --> 00:07:40,969
creating a world where we build in

145
00:07:35,690 --> 00:07:43,490
values into society that we do not agree

146
00:07:40,970 --> 00:07:46,430
with and perhaps build in discrimination

147
00:07:43,490 --> 00:07:51,530
and biases into the technology that

148
00:07:46,430 --> 00:07:54,080
we're building in order to look at this

149
00:07:51,530 --> 00:07:55,849
we need to actually think about where

150
00:07:54,080 --> 00:07:58,729
what is the end goal of what you're

151
00:07:55,850 --> 00:08:01,850
doing you can't any longer say well I'm

152
00:07:58,729 --> 00:08:04,760
only building a small mechanical thing a

153
00:08:01,850 --> 00:08:07,160
technological thing that will be helping

154
00:08:04,760 --> 00:08:08,570
to a short-term goal you actually have

155
00:08:07,160 --> 00:08:11,000
to look at well what will the

156
00:08:08,570 --> 00:08:14,210
consequences of the tool that you're

157
00:08:11,000 --> 00:08:19,250
building be for the society that you

158
00:08:14,210 --> 00:08:22,210
want to be living in there is there is

159
00:08:19,250 --> 00:08:25,940
not a lot of ways of doing this that is

160
00:08:22,210 --> 00:08:29,479
clear at the moment you will have to

161
00:08:25,940 --> 00:08:30,770
actually think about well what are how

162
00:08:29,479 --> 00:08:34,338
do you see the world from different

163
00:08:30,770 --> 00:08:36,468
perspectives we all have our own

164
00:08:34,339 --> 00:08:38,510
personality that we bring into the room

165
00:08:36,469 --> 00:08:41,990
some of us also bring a dog that you

166
00:08:38,510 --> 00:08:44,179
might be able to hear on the video but

167
00:08:41,990 --> 00:08:45,830
you need to be aware of what you're

168
00:08:44,179 --> 00:08:47,780
bringing into a team what you're

169
00:08:45,830 --> 00:08:50,750
bringing into the technology that you're

170
00:08:47,780 --> 00:08:53,089
building in order to avoid the

171
00:08:50,750 --> 00:08:55,700
technology that you're building being

172
00:08:53,090 --> 00:08:58,520
imbued with values that you don't want

173
00:08:55,700 --> 00:09:01,310
to see enlarged or increased in the

174
00:08:58,520 --> 00:09:03,740
world and I think that's why we need to

175
00:09:01,310 --> 00:09:08,170
stop looking at digital regulation as

176
00:09:03,740 --> 00:09:11,180
just a technological mean and a way of

177
00:09:08,170 --> 00:09:14,719
regulating a platform or a technology

178
00:09:11,180 --> 00:09:16,880
but a way of actually looking at which

179
00:09:14,720 --> 00:09:20,330
world we want to create in which world

180
00:09:16,880 --> 00:09:23,360
we want to live in and that's why we

181
00:09:20,330 --> 00:09:26,990
need to have not just technology but

182
00:09:23,360 --> 00:09:30,290
values as the focus for for the

183
00:09:26,990 --> 00:09:32,900
regulation that we have a lot of the

184
00:09:30,290 --> 00:09:35,180
time though when we talk about values

185
00:09:32,900 --> 00:09:38,060
when we talk about the ideology that we

186
00:09:35,180 --> 00:09:41,000
have we often talk about what we do not

187
00:09:38,060 --> 00:09:43,839
want we do not want discrimination we do

188
00:09:41,000 --> 00:09:47,360
not want criminalization of free speech

189
00:09:43,840 --> 00:09:50,390
we do not want journalists to be put in

190
00:09:47,360 --> 00:09:52,760
jail we do not want whistleblowers to be

191
00:09:50,390 --> 00:09:56,270
put in jail there is a whole list of

192
00:09:52,760 --> 00:09:58,520
what we do not want but that's not going

193
00:09:56,270 --> 00:10:00,829
to make that's not going to be able to

194
00:09:58,520 --> 00:10:03,710
change the world into the place that we

195
00:10:00,830 --> 00:10:06,860
that we would like to see if we're only

196
00:10:03,710 --> 00:10:08,750
fighting against something I don't think

197
00:10:06,860 --> 00:10:11,540
that we can change the world into the

198
00:10:08,750 --> 00:10:14,570
future that we want I think we need to

199
00:10:11,540 --> 00:10:18,380
be able to instill a sense of hope to

200
00:10:14,570 --> 00:10:21,530
people rather than just a fear or a

201
00:10:18,380 --> 00:10:25,640
protest because the protest movements

202
00:10:21,530 --> 00:10:28,329
have have made us come so far but I

203
00:10:25,640 --> 00:10:32,210
don't think that they will be able to

204
00:10:28,330 --> 00:10:34,250
continue working in creating a world

205
00:10:32,210 --> 00:10:36,830
that we want it will perhaps only be

206
00:10:34,250 --> 00:10:41,270
able to stop certain things and I think

207
00:10:36,830 --> 00:10:45,260
if we if we act in contrary to something

208
00:10:41,270 --> 00:10:47,510
that we do not want we actually boost

209
00:10:45,260 --> 00:10:50,390
what we do not want by giving it more

210
00:10:47,510 --> 00:10:53,230
oxygen and more power by continuing to

211
00:10:50,390 --> 00:10:55,939
address the things that we do not want

212
00:10:53,230 --> 00:10:59,180
so what should we what should we do to

213
00:10:55,940 --> 00:11:02,570
actually encourage and to let grow the

214
00:10:59,180 --> 00:11:06,109
values that we do want I think we should

215
00:11:02,570 --> 00:11:09,649
try and talk not about what we do not

216
00:11:06,110 --> 00:11:12,200
want but what we do want we do want a

217
00:11:09,649 --> 00:11:15,709
society where we all have equal

218
00:11:12,200 --> 00:11:19,399
opportunity and we are able to create

219
00:11:15,709 --> 00:11:21,229
the future for ourselves that we want no

220
00:11:19,399 --> 00:11:24,920
matter where we are born what our skin

221
00:11:21,230 --> 00:11:28,040
color is which gender that we are and we

222
00:11:24,920 --> 00:11:29,599
should do that in a way in order to

223
00:11:28,040 --> 00:11:33,290
create that world we need to celebrate

224
00:11:29,600 --> 00:11:35,209
those that open up the world for each

225
00:11:33,290 --> 00:11:38,599
other and create equal opportunities

226
00:11:35,209 --> 00:11:42,290
instead of going after those who are

227
00:11:38,600 --> 00:11:45,589
being discriminatory we should also look

228
00:11:42,290 --> 00:11:47,540
at ourselves and say what is what am i

229
00:11:45,589 --> 00:11:51,140
bringing to the table what am i what is

230
00:11:47,540 --> 00:11:53,060
my outset on the world a lot of us look

231
00:11:51,140 --> 00:11:56,300
at the world and see well I have fought

232
00:11:53,060 --> 00:11:59,750
my I fought my way here I have achieved

233
00:11:56,300 --> 00:12:03,800
these things I have gotten an education

234
00:11:59,750 --> 00:12:07,220
I've made it through university I've

235
00:12:03,800 --> 00:12:10,519
gotten a job because I was self-made

236
00:12:07,220 --> 00:12:13,699
because I had all of these capabilities

237
00:12:10,519 --> 00:12:17,240
in myself that allowed me to come there

238
00:12:13,699 --> 00:12:20,599
I think often we forget the privileges

239
00:12:17,240 --> 00:12:22,579
that we have each of us and believe that

240
00:12:20,600 --> 00:12:26,660
we are pulling ourselves up by our

241
00:12:22,579 --> 00:12:29,959
bootstraps and have made the world our

242
00:12:26,660 --> 00:12:32,269
place and our place in the world we need

243
00:12:29,959 --> 00:12:34,550
to be able to see well what is it that

244
00:12:32,269 --> 00:12:38,390
I've been given by the world as well and

245
00:12:34,550 --> 00:12:40,069
also therefore realize what luck and

246
00:12:38,390 --> 00:12:43,880
what privileges that we have and

247
00:12:40,070 --> 00:12:47,480
therefore be able to give those same

248
00:12:43,880 --> 00:12:49,579
privileges to others so instead of only

249
00:12:47,480 --> 00:12:51,620
saying well we don't want we don't want

250
00:12:49,579 --> 00:12:54,319
Trump we don't want Danish People's

251
00:12:51,620 --> 00:12:57,560
Party we don't want populism we don't

252
00:12:54,320 --> 00:13:00,199
want censorship well then try and look

253
00:12:57,560 --> 00:13:03,260
at well what kind of what kind of world

254
00:13:00,199 --> 00:13:05,120
which values do we want and how will the

255
00:13:03,260 --> 00:13:06,860
tools and the digital world that we are

256
00:13:05,120 --> 00:13:09,470
creating how will that actually

257
00:13:06,860 --> 00:13:11,750
contributes to this and look into well

258
00:13:09,470 --> 00:13:16,670
what are what could the bias is that

259
00:13:11,750 --> 00:13:19,330
we're building in end up creating or

260
00:13:16,670 --> 00:13:23,079
destroying and the world that we see

261
00:13:19,330 --> 00:13:25,510
I think that it's important to to try

262
00:13:23,080 --> 00:13:27,670
and say well what are what are the hopes

263
00:13:25,510 --> 00:13:31,180
that we what are the hopes that we see I

264
00:13:27,670 --> 00:13:33,160
think instead of we need to put pressure

265
00:13:31,180 --> 00:13:35,829
on the politicians that are regulating

266
00:13:33,160 --> 00:13:39,310
them and I'm hoping that you will be be

267
00:13:35,829 --> 00:13:42,910
helping me in in being able to shift the

268
00:13:39,310 --> 00:13:44,920
Digital Agenda and also push for the

269
00:13:42,910 --> 00:13:46,510
right things instead of instead of the

270
00:13:44,920 --> 00:13:48,040
wrong but I think we should also

271
00:13:46,510 --> 00:13:51,250
celebrate the advances that we've

272
00:13:48,040 --> 00:13:53,469
already had that we have made some

273
00:13:51,250 --> 00:13:56,980
positive progress we have ensured net

274
00:13:53,470 --> 00:14:01,540
neutrality in the U we did stop actor

275
00:13:56,980 --> 00:14:03,850
now many years ago the reform of the

276
00:14:01,540 --> 00:14:07,689
copyright directive was not quite as bad

277
00:14:03,850 --> 00:14:09,550
as we could have seen it be we did get

278
00:14:07,690 --> 00:14:13,360
exceptions for libraries we did get

279
00:14:09,550 --> 00:14:16,959
exceptions for Wikipedia and try and

280
00:14:13,360 --> 00:14:20,560
promote the champions of the values that

281
00:14:16,959 --> 00:14:23,770
we want instead of only pushing and

282
00:14:20,560 --> 00:14:28,660
punishing the the things that are going

283
00:14:23,770 --> 00:14:33,130
against there are a few things that will

284
00:14:28,660 --> 00:14:35,980
be becoming up during the next five

285
00:14:33,130 --> 00:14:38,110
years there is the reform of e-commerce

286
00:14:35,980 --> 00:14:41,010
directive that I mentioned earlier there

287
00:14:38,110 --> 00:14:43,329
is also the regulation of terrorists

288
00:14:41,010 --> 00:14:45,880
content online and I think it's

289
00:14:43,329 --> 00:14:48,969
important in order to win these fights

290
00:14:45,880 --> 00:14:52,839
that we try and actually go in with a

291
00:14:48,970 --> 00:14:57,790
more sort of optimistic and positive

292
00:14:52,839 --> 00:14:59,500
approach to things rather than going

293
00:14:57,790 --> 00:15:02,260
against things that we don't want I

294
00:14:59,500 --> 00:15:06,480
think I'm I'm going slightly in circles

295
00:15:02,260 --> 00:15:12,910
now so I hope that I hope that I've

296
00:15:06,480 --> 00:15:17,370
tried to give you an idea of trying of

297
00:15:12,910 --> 00:15:20,160
using values of as the basis of

298
00:15:17,370 --> 00:15:23,920
technological regulation and not only

299
00:15:20,160 --> 00:15:28,750
looking at regulation of digital issues

300
00:15:23,920 --> 00:15:31,689
as a technical aspect but also looking

301
00:15:28,750 --> 00:15:33,010
into well how do we how do you make sure

302
00:15:31,690 --> 00:15:34,899
that the values that we

303
00:15:33,010 --> 00:15:37,120
to live by and the values that we want

304
00:15:34,899 --> 00:15:44,980
in our world are actually part of the

305
00:15:37,120 --> 00:15:48,370
technology that we have and I hope that

306
00:15:44,980 --> 00:15:52,839
there is a microphone somewhere and that

307
00:15:48,370 --> 00:15:55,660
you have comments and critique and ideas

308
00:15:52,839 --> 00:16:03,010
of what I should be spending my time on

309
00:15:55,660 --> 00:16:05,170
for the next five years yes I have a

310
00:16:03,010 --> 00:16:07,870
question you say that you shouldn't

311
00:16:05,170 --> 00:16:10,329
Punisher though say that we don't like

312
00:16:07,870 --> 00:16:12,850
and lately it has been a Google and ever

313
00:16:10,329 --> 00:16:16,529
that got some fines do you think that is

314
00:16:12,850 --> 00:16:21,040
wrong to find those for sure no I don't

315
00:16:16,529 --> 00:16:23,529
it's not of course if you have I should

316
00:16:21,040 --> 00:16:26,469
stop moving around on the stage you

317
00:16:23,529 --> 00:16:28,420
should of course use the powers that you

318
00:16:26,470 --> 00:16:31,209
have in order to make sure that

319
00:16:28,420 --> 00:16:34,829
companies actually adhere to the laws

320
00:16:31,209 --> 00:16:38,138
that we have but I think as a way of

321
00:16:34,829 --> 00:16:40,180
arguing for regulation and a way of

322
00:16:38,139 --> 00:16:42,399
arguing for the policies that you want I

323
00:16:40,180 --> 00:16:45,010
think it's more effective to say well

324
00:16:42,399 --> 00:16:49,079
what is it what is it that we do want

325
00:16:45,010 --> 00:16:53,439
rather than saying well I do not want

326
00:16:49,079 --> 00:16:56,260
Trump I do not want racism I what is

327
00:16:53,440 --> 00:17:00,389
what is it that you actually are wanting

328
00:16:56,260 --> 00:17:00,389
to promote and and celebrate

329
00:17:03,830 --> 00:17:16,250
i working in the beginning you said

330
00:17:14,150 --> 00:17:18,620
something about that people who are

331
00:17:16,250 --> 00:17:20,900
creating new technologies and are like

332
00:17:18,619 --> 00:17:23,178
innovative somehow they should really

333
00:17:20,900 --> 00:17:26,720
consider and take care of what they are

334
00:17:23,179 --> 00:17:29,480
doing because it can be used in in a bad

335
00:17:26,720 --> 00:17:31,700
way do you have some examples or a

336
00:17:29,480 --> 00:17:35,090
custom from what I see my perspective is

337
00:17:31,700 --> 00:17:38,890
everything that is quite an aversive can

338
00:17:35,090 --> 00:17:42,620
be used by the bad guys so don't like

339
00:17:38,890 --> 00:17:44,660
would put a pressure on making a smaller

340
00:17:42,620 --> 00:17:47,270
project just to be sure that no one can

341
00:17:44,660 --> 00:17:48,260
use it for a bad thing what could be

342
00:17:47,270 --> 00:17:50,929
used for a bad thing

343
00:17:48,260 --> 00:17:52,670
oh did you said something that people

344
00:17:50,929 --> 00:17:55,880
but what what could be used like

345
00:17:52,670 --> 00:17:57,530
everything if we stopped innovation by

346
00:17:55,880 --> 00:18:00,650
saying that this technology could also

347
00:17:57,530 --> 00:18:02,540
be used for bad then I think the world

348
00:18:00,650 --> 00:18:04,760
will kind of stop and the world

349
00:18:02,540 --> 00:18:08,928
shouldn't stop but I think you should be

350
00:18:04,760 --> 00:18:11,929
aware of are we building a technology

351
00:18:08,929 --> 00:18:15,890
that will allow for discrimination for

352
00:18:11,929 --> 00:18:18,370
example that will allow for using the

353
00:18:15,890 --> 00:18:21,559
date for example if you collect data and

354
00:18:18,370 --> 00:18:23,780
profile people what kind of data will

355
00:18:21,559 --> 00:18:25,550
you allow to be collected and which kind

356
00:18:23,780 --> 00:18:29,710
of profiles were you allowed to build on

357
00:18:25,550 --> 00:18:35,629
people so that you don't end up having a

358
00:18:29,710 --> 00:18:38,150
ed filter mechanism or a digital profile

359
00:18:35,630 --> 00:18:40,280
on people that allows for discrimination

360
00:18:38,150 --> 00:18:42,800
that you would not have allowed for

361
00:18:40,280 --> 00:18:44,840
otherwise by collecting different bits

362
00:18:42,800 --> 00:18:47,870
of information than for seeing that this

363
00:18:44,840 --> 00:18:50,480
would be a Hispanic or a black person or

364
00:18:47,870 --> 00:18:54,110
a person living in a specific area or

365
00:18:50,480 --> 00:18:56,720
their sexuality and therefore excluding

366
00:18:54,110 --> 00:18:59,719
them from seeing certain results or for

367
00:18:56,720 --> 00:19:05,980
getting certain jobs or for having

368
00:18:59,720 --> 00:19:10,130
insurances and these kinds of things hi

369
00:19:05,980 --> 00:19:11,929
thanks for the talk and having someone's

370
00:19:10,130 --> 00:19:14,600
time actually available for asking you

371
00:19:11,929 --> 00:19:17,500
questions so I'm a journalist I work for

372
00:19:14,600 --> 00:19:21,350
an organization called OCC RP

373
00:19:17,500 --> 00:19:26,870
we conduct investigations into organized

374
00:19:21,350 --> 00:19:29,600
crime and cross-border corruption this

375
00:19:26,870 --> 00:19:34,789
is going to be slightly so I know you're

376
00:19:29,600 --> 00:19:36,799
working parliamentary so a collaboration

377
00:19:34,789 --> 00:19:39,830
between the EU for example lacasa

378
00:19:36,799 --> 00:19:42,500
bhaijaan Azerbaijan doesn't really have

379
00:19:39,830 --> 00:19:45,110
like the best track record when it comes

380
00:19:42,500 --> 00:19:49,669
to being very nice to journalists to put

381
00:19:45,110 --> 00:19:51,889
this very lightly you know I would

382
00:19:49,669 --> 00:19:55,429
really love to hear your talks thoughts

383
00:19:51,889 --> 00:19:58,399
about you know how do we how do we still

384
00:19:55,429 --> 00:20:00,409
collaborate with the people that we can

385
00:19:58,399 --> 00:20:04,789
see as allies in the parliament in

386
00:20:00,409 --> 00:20:11,899
Azerbaijan and how do we I think that's

387
00:20:04,789 --> 00:20:13,820
my question I think it's you for the

388
00:20:11,899 --> 00:20:16,129
rest of you I in the European Parliament

389
00:20:13,820 --> 00:20:18,649
you both have different committees and

390
00:20:16,129 --> 00:20:21,399
then you also work with parliaments in

391
00:20:18,649 --> 00:20:25,008
other countries and I am working with

392
00:20:21,399 --> 00:20:28,820
parliaments in Georgia in Azerbaijan and

393
00:20:25,009 --> 00:20:31,220
Armenia and Azerbaijan does not have

394
00:20:28,820 --> 00:20:34,428
what any of us would call a democracy

395
00:20:31,220 --> 00:20:35,870
and there is repercussions for

396
00:20:34,429 --> 00:20:38,570
journalists and for human rights

397
00:20:35,870 --> 00:20:41,508
activists and I would find it very

398
00:20:38,570 --> 00:20:43,158
difficult to work with the government

399
00:20:41,509 --> 00:20:44,919
that's in place at the moment in

400
00:20:43,159 --> 00:20:48,559
Azerbaijan and therefore it's very

401
00:20:44,919 --> 00:20:50,269
important to work with civil society and

402
00:20:48,559 --> 00:20:52,759
the people that are actually working for

403
00:20:50,269 --> 00:20:54,950
democracy in that country I'd love to

404
00:20:52,759 --> 00:20:58,730
talk to you more about it afterwards but

405
00:20:54,950 --> 00:21:01,639
it's slightly off the digital digital

406
00:20:58,730 --> 00:21:03,799
subject lines here but I mean what you

407
00:21:01,639 --> 00:21:06,649
could I mean the technology that you

408
00:21:03,799 --> 00:21:09,049
build could also be technology in

409
00:21:06,649 --> 00:21:11,508
looking into well how does money

410
00:21:09,049 --> 00:21:13,549
actually move around how do we open up

411
00:21:11,509 --> 00:21:16,730
the streams of money through the banks

412
00:21:13,549 --> 00:21:18,620
how do we open up public budgets so that

413
00:21:16,730 --> 00:21:21,009
we can see corruption so that we can

414
00:21:18,620 --> 00:21:23,658
give people the tools to actually

415
00:21:21,009 --> 00:21:26,000
protect their own society and protect

416
00:21:23,659 --> 00:21:28,730
their democracy against misuse of funds

417
00:21:26,000 --> 00:21:30,530
and corruption and in that way you can

418
00:21:28,730 --> 00:21:32,510
you can use the technology

419
00:21:30,530 --> 00:21:36,260
to actually promote the values that you

420
00:21:32,510 --> 00:21:38,900
want to see in society and that is the

421
00:21:36,260 --> 00:21:41,720
openness that we can that we can provide

422
00:21:38,900 --> 00:21:43,910
society with in society is hugely

423
00:21:41,720 --> 00:21:48,670
important to actually promote the values

424
00:21:43,910 --> 00:21:48,670
that I at least would like to promote

425
00:21:53,620 --> 00:21:57,489
any other questions

426
00:22:04,340 --> 00:22:11,310
hello so we have seen more and more that

427
00:22:08,670 --> 00:22:13,290
the nation states of Europe are

428
00:22:11,310 --> 00:22:15,330
beginning to ignore human rights and

429
00:22:13,290 --> 00:22:17,610
with the European Union Charter on Human

430
00:22:15,330 --> 00:22:20,730
Rights we've gotten a new way of

431
00:22:17,610 --> 00:22:23,790
fighting that however the Commission

432
00:22:20,730 --> 00:22:26,270
seems to be less than keen on opening

433
00:22:23,790 --> 00:22:28,860
infringement lawsuits against the states

434
00:22:26,270 --> 00:22:30,870
do you think that'll change in the

435
00:22:28,860 --> 00:22:35,129
future or will it be up to us to file

436
00:22:30,870 --> 00:22:37,800
the lawsuits I don't think that that

437
00:22:35,130 --> 00:22:38,700
will change without us actually doing

438
00:22:37,800 --> 00:22:41,340
something about it

439
00:22:38,700 --> 00:22:43,290
I think that it's really important that

440
00:22:41,340 --> 00:22:45,689
we have individuals that file lawsuits

441
00:22:43,290 --> 00:22:48,659
and I think it's the work that has been

442
00:22:45,690 --> 00:22:52,470
done on Facebook's use of data in

443
00:22:48,660 --> 00:22:54,780
Austria the work that you write must

444
00:22:52,470 --> 00:22:58,380
have been doing with lowly logging in

445
00:22:54,780 --> 00:23:01,620
Denmark on highlighting the the tracking

446
00:22:58,380 --> 00:23:04,020
of our use of our cell phones and the

447
00:23:01,620 --> 00:23:05,780
internet has been highly important to

448
00:23:04,020 --> 00:23:09,000
actually bring this to the forefront of

449
00:23:05,780 --> 00:23:12,090
national and international media I think

450
00:23:09,000 --> 00:23:15,020
we have an important role in the

451
00:23:12,090 --> 00:23:18,570
European Parliament to actually work for

452
00:23:15,020 --> 00:23:21,320
the implementation and upholding the

453
00:23:18,570 --> 00:23:24,659
rights that we have as European citizens

454
00:23:21,320 --> 00:23:27,000
because we have a lot of fine regulation

455
00:23:24,660 --> 00:23:29,340
some of it let's find some of it more

456
00:23:27,000 --> 00:23:32,880
fine but we have a lot of regulation and

457
00:23:29,340 --> 00:23:34,889
if we don't uphold that then it's not

458
00:23:32,880 --> 00:23:37,080
really worth having it written down on

459
00:23:34,890 --> 00:23:39,720
paper if it's not going to be respected

460
00:23:37,080 --> 00:23:41,310
and that's why I think we should in the

461
00:23:39,720 --> 00:23:43,830
European Parliament not only look at

462
00:23:41,310 --> 00:23:45,870
well how much how many more laws can we

463
00:23:43,830 --> 00:23:48,419
have written how many more pages of

464
00:23:45,870 --> 00:23:50,820
texts but also looking at getting the

465
00:23:48,420 --> 00:23:55,140
texts and the laws that we have actually

466
00:23:50,820 --> 00:23:56,879
and being upheld and having both Member

467
00:23:55,140 --> 00:23:59,750
States and companies living up to their

468
00:23:56,880 --> 00:23:59,750
responsibilities

469
00:24:04,700 --> 00:24:14,970
no further questions like this then I

470
00:24:12,179 --> 00:24:17,429
would thank you all for both being here

471
00:24:14,970 --> 00:24:20,029
at the camp and also coming to to my

472
00:24:17,429 --> 00:24:22,950
keynote which was a bit more

473
00:24:20,029 --> 00:24:26,100
values-based and aspirational and the

474
00:24:22,950 --> 00:24:27,899
ideas of how I would like to see the

475
00:24:26,100 --> 00:24:31,889
regulation that we have and how we act

476
00:24:27,899 --> 00:24:34,469
on it there is I'm starting out in five

477
00:24:31,889 --> 00:24:37,289
four five years working in the European

478
00:24:34,470 --> 00:24:39,690
Parliament and I'm quite curious to see

479
00:24:37,289 --> 00:24:41,908
what will actually be proposed by the

480
00:24:39,690 --> 00:24:44,100
Commission there has been a leaked

481
00:24:41,909 --> 00:24:47,070
document on on how they want to look at

482
00:24:44,100 --> 00:24:50,879
the inter digital service market single

483
00:24:47,070 --> 00:24:53,178
market but new things will appear and I

484
00:24:50,879 --> 00:24:55,799
would really like if all of you would

485
00:24:53,179 --> 00:24:58,470
help both me and other deputies of the

486
00:24:55,799 --> 00:25:00,539
European Parliament to keep fighting for

487
00:24:58,470 --> 00:25:03,090
free and open Internet and a free and

488
00:25:00,539 --> 00:25:05,490
open society a lot of the people that

489
00:25:03,090 --> 00:25:08,070
have left the European Parliament this

490
00:25:05,490 --> 00:25:12,090
summer such as Julia Rida and Malaysia

491
00:25:08,070 --> 00:25:14,549
Chaka were strong proponents for enough

492
00:25:12,090 --> 00:25:16,350
free and open Internet and have left

493
00:25:14,549 --> 00:25:18,330
some quite large shoes to fill

494
00:25:16,350 --> 00:25:22,248
and I hope you will be able to help me

495
00:25:18,330 --> 00:25:22,249
fill those shoes thank you very much

496
00:25:25,360 --> 00:25:29,340
I have

