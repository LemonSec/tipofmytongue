1
00:00:00,000 --> 00:00:11,940
enjoy hora ante todo muchas gracias por

2
00:00:08,940 --> 00:00:14,989
venir a la primera vez que voy a hablar

3
00:00:11,940 --> 00:00:14,989
un poquito de matemáticas

4
00:00:48,000 --> 00:00:50,060
you

5
00:01:06,710 --> 00:01:14,570
video and audio software for real it was

6
00:01:13,010 --> 00:01:17,090
a small company there were four

7
00:01:14,570 --> 00:01:19,009
programmers and we got an award to the

8
00:01:17,090 --> 00:01:20,960
best european software of the

9
00:01:19,010 --> 00:01:24,350
audiovisual environment because we were

10
00:01:20,960 --> 00:01:27,500
the first ones to broadcast directly

11
00:01:24,350 --> 00:01:32,630
from a mobile phone that was the time

12
00:01:27,500 --> 00:01:38,509
when the mobiles were very rudimentary

13
00:01:32,630 --> 00:01:41,600
and from that night and I worked on

14
00:01:38,509 --> 00:01:46,009
military software specifically

15
00:01:41,600 --> 00:01:49,280
multilateral stability program which is

16
00:01:46,009 --> 00:01:52,310
basically their way in which the NATO

17
00:01:49,280 --> 00:01:54,619
countries exchange data in a secure way

18
00:01:52,310 --> 00:01:57,159
and from then on I moved on to sopra

19
00:01:54,619 --> 00:02:04,270
working as an architect to the MDS a

20
00:01:57,159 --> 00:02:06,979
project of the of the harbor's 400 and I

21
00:02:04,270 --> 00:02:08,929
discovered that I like innovation and I

22
00:02:06,979 --> 00:02:11,620
started doing quite a lot of innovation

23
00:02:08,929 --> 00:02:14,269
working in Chopra and I moved to action

24
00:02:11,620 --> 00:02:17,360
and have been dead for a while doing

25
00:02:14,269 --> 00:02:22,430
chat war computer vision

26
00:02:17,360 --> 00:02:27,440
I know everything that's so far away

27
00:02:22,430 --> 00:02:32,930
from Java on the other hand I organized

28
00:02:27,440 --> 00:02:37,100
meetups I participate actively I have

29
00:02:32,930 --> 00:02:39,230
done some a code pushed further noted

30
00:02:37,100 --> 00:02:42,890
and I hope tonight another come from

31
00:02:39,230 --> 00:02:45,380
Barcelona like this but much smaller and

32
00:02:42,890 --> 00:02:49,760
talking about not it so why am I here

33
00:02:45,380 --> 00:02:52,920
isn't my current work what is about

34
00:02:49,760 --> 00:02:56,099
doing neural networks and computer

35
00:02:52,920 --> 00:02:59,369
vision of things that are itself not

36
00:02:56,099 --> 00:03:03,619
standard and there are no suppliers for

37
00:02:59,370 --> 00:03:06,780
first ants to find the angle off a car

38
00:03:03,620 --> 00:03:08,430
to know when they send pictures from a

39
00:03:06,780 --> 00:03:10,680
car but they're all anchors have been

40
00:03:08,430 --> 00:03:13,319
covered and this was one of the projects

41
00:03:10,680 --> 00:03:15,629
it was to give a DNA and document to

42
00:03:13,319 --> 00:03:18,988
find the signature and compare them it's

43
00:03:15,629 --> 00:03:22,828
not what an expert in signatures would

44
00:03:18,989 --> 00:03:26,010
do but it's good enough to be comparable

45
00:03:22,829 --> 00:03:27,930
to what human beings can do and we still

46
00:03:26,010 --> 00:03:30,929
have a problem because problems come

47
00:03:27,930 --> 00:03:33,900
within in PDF or whatever and the

48
00:03:30,930 --> 00:03:37,290
signature can be on top of writing

49
00:03:33,900 --> 00:03:40,590
printed right in our the question I may

50
00:03:37,290 --> 00:03:43,048
be written handwritten and with

51
00:03:40,590 --> 00:03:47,010
different problems we got here but my

52
00:03:43,049 --> 00:03:49,199
question was who ensures that nobody has

53
00:03:47,010 --> 00:03:52,909
manipulated the images were receiving so

54
00:03:49,199 --> 00:03:55,620
that we do not see the signature but

55
00:03:52,909 --> 00:03:57,959
artificial intelligence believes there

56
00:03:55,620 --> 00:04:00,000
is a a signature there and both March

57
00:03:57,959 --> 00:04:03,239
that would be a huge problem that you

58
00:04:00,000 --> 00:04:06,349
cannot see the signature visually but

59
00:04:03,239 --> 00:04:09,150
the the computer vision says it is there

60
00:04:06,349 --> 00:04:13,379
so I asked myself first of all how many

61
00:04:09,150 --> 00:04:16,798
of you work in a company that deals with

62
00:04:13,379 --> 00:04:18,230
a computer security many of you how many

63
00:04:16,798 --> 00:04:20,989
of you

64
00:04:18,230 --> 00:04:24,590
the company that you do pen tests of

65
00:04:20,990 --> 00:04:27,560
computer vision systems no hands there

66
00:04:24,590 --> 00:04:31,010
so this is what I found that we are very

67
00:04:27,560 --> 00:04:33,920
much focusing on transfers communication

68
00:04:31,010 --> 00:04:35,680
communications pen tests and nobody is

69
00:04:33,920 --> 00:04:37,970
realizing that there are more and more

70
00:04:35,680 --> 00:04:39,740
artificial intelligences them and nobody

71
00:04:37,970 --> 00:04:42,050
is testing this and there's no company

72
00:04:39,740 --> 00:05:02,060
that gives you the service of pen

73
00:04:42,050 --> 00:05:05,330
testing computer vision system stop

74
00:05:02,060 --> 00:05:07,430
seeing the first character and obscene

75
00:05:05,330 --> 00:05:11,810
the second one in frame seven

76
00:05:07,430 --> 00:05:14,060
approximately that was and what about

77
00:05:11,810 --> 00:05:16,490
computer vision systems do they do that

78
00:05:14,060 --> 00:05:18,560
at the same speed well that's not the

79
00:05:16,490 --> 00:05:21,230
case but you train them they tell you

80
00:05:18,560 --> 00:05:23,690
whether a picture belongs to one face or

81
00:05:21,230 --> 00:05:25,720
the next one and they switch the point

82
00:05:23,690 --> 00:05:29,540
at which the computer vision system

83
00:05:25,720 --> 00:05:32,210
recognizes that a frame is from the face

84
00:05:29,540 --> 00:05:35,480
of one of the players is not the same

85
00:05:32,210 --> 00:05:40,239
one then in our case because we look at

86
00:05:35,480 --> 00:05:42,590
some things and the computer vision

87
00:05:40,240 --> 00:05:44,360
recognizes features with certain

88
00:05:42,590 --> 00:05:46,640
characteristics of the pictures and why

89
00:05:44,360 --> 00:05:48,260
they cost elevation computer systems

90
00:05:46,640 --> 00:05:50,870
that are being used already protocol

91
00:05:48,260 --> 00:05:52,810
particularly in self-driving cars and if

92
00:05:50,870 --> 00:05:56,150
you move in the military environment

93
00:05:52,810 --> 00:06:00,170
with the military drone self-driven and

94
00:05:56,150 --> 00:06:04,840
with weapons that kind of higher what

95
00:06:00,170 --> 00:06:08,720
happens if somebody managed to fool them

96
00:06:04,840 --> 00:06:11,169
if you have armed systems that have

97
00:06:08,720 --> 00:06:13,520
computer vision you have to be very

98
00:06:11,170 --> 00:06:16,100
careful because that computer vision

99
00:06:13,520 --> 00:06:19,280
could be fooled having said that I

100
00:06:16,100 --> 00:06:21,320
started with my research that was all of

101
00:06:19,280 --> 00:06:27,500
this is why I started doing this

102
00:06:21,320 --> 00:06:29,659
research why because there isn't that

103
00:06:27,500 --> 00:06:33,650
much knowledge around it is a neural

104
00:06:29,660 --> 00:06:36,500
network is it's just that you get input

105
00:06:33,650 --> 00:06:38,870
you we calculate weight and multiply

106
00:06:36,500 --> 00:06:42,410
them the input by the weight and then we

107
00:06:38,870 --> 00:06:44,810
add them up that's all and we get an

108
00:06:42,410 --> 00:06:46,970
activation function wait what's that

109
00:06:44,810 --> 00:06:48,770
it's about that decides whether yes or

110
00:06:46,970 --> 00:06:51,350
not and how much he is and how much no

111
00:06:48,770 --> 00:06:53,870
when I started at the University

112
00:06:51,350 --> 00:06:56,630
if the sigmoid was being used but that

113
00:06:53,870 --> 00:06:58,880
the minds a lot of computation time and

114
00:06:56,630 --> 00:07:01,130
you will see that the one that's more

115
00:06:58,880 --> 00:07:04,040
often used now is the red which is

116
00:07:01,130 --> 00:07:07,070
linear and that justice whether if the

117
00:07:04,040 --> 00:07:09,980
value is below zero zero and if the

118
00:07:07,070 --> 00:07:13,159
value is above zero then you get the

119
00:07:09,980 --> 00:07:14,630
value so it's very easy very simple to

120
00:07:13,160 --> 00:07:16,430
implement and requires very little

121
00:07:14,630 --> 00:07:17,840
income in the computation and that's why

122
00:07:16,430 --> 00:07:19,820
it's used but the systems that it

123
00:07:17,840 --> 00:07:23,080
produces are more lenient because the

124
00:07:19,820 --> 00:07:25,730
sigmoid has a greater curve therefore

125
00:07:23,080 --> 00:07:27,270
there is a greater sort of space for

126
00:07:25,730 --> 00:07:30,660
decision making

127
00:07:27,270 --> 00:07:33,299
so that would be a perceptron but what

128
00:07:30,660 --> 00:07:34,620
happens when I put many of them you know

129
00:07:33,300 --> 00:07:36,960
it's one of these circles is a

130
00:07:34,620 --> 00:07:40,560
perceptron and each colored line is a

131
00:07:36,960 --> 00:07:42,750
way okay so what a neural network does

132
00:07:40,560 --> 00:07:44,970
is that it has two directions a forward

133
00:07:42,750 --> 00:07:47,639
direction from input to output and him

134
00:07:44,970 --> 00:07:50,639
back enumeration a direction from the

135
00:07:47,639 --> 00:07:52,620
output to the input so when we train

136
00:07:50,639 --> 00:07:58,560
what we do is the back propagation that

137
00:07:52,620 --> 00:08:01,610
is we have these pictures this or these

138
00:07:58,560 --> 00:08:04,440
data is this and we tell them how

139
00:08:01,610 --> 00:08:06,360
they're when they're going wrong and the

140
00:08:04,440 --> 00:08:09,630
system starts learning and how do they

141
00:08:06,360 --> 00:08:12,570
learn this is not new this is from

142
00:08:09,630 --> 00:08:16,229
Newton's time with a certain gradient

143
00:08:12,570 --> 00:08:21,180
Newton or a curve Newton devised a

144
00:08:16,229 --> 00:08:32,610
method the minimum of the curve the

145
00:08:21,180 --> 00:08:35,370
gradient descent so it's not of the loss

146
00:08:32,610 --> 00:08:38,130
function which tells us which far away

147
00:08:35,370 --> 00:08:40,299
we are from this system working properly

148
00:08:38,130 --> 00:08:43,030
roughly

149
00:08:40,299 --> 00:08:45,849
so depending on how weights rain we're

150
00:08:43,030 --> 00:08:48,310
going to find a local minimum or another

151
00:08:45,850 --> 00:08:52,360
local minimum or another local minimum

152
00:08:48,310 --> 00:08:53,890
that's right impossible to ensure that

153
00:08:52,360 --> 00:08:57,940
the minimum that we have found is

154
00:08:53,890 --> 00:09:00,220
actually is small as possible 1 & 2 the

155
00:08:57,940 --> 00:09:05,200
heart we have to add something else when

156
00:09:00,220 --> 00:09:06,970
we talk about this there is a concept

157
00:09:05,200 --> 00:09:12,270
that's called the Linnaean rate weight

158
00:09:06,970 --> 00:09:16,120
and we can think of this you have a

159
00:09:12,270 --> 00:09:17,980
bigger number it will learn faster

160
00:09:16,120 --> 00:09:21,790
rather than me but the reading rate

161
00:09:17,980 --> 00:09:26,680
actually tells you also roughly speaking

162
00:09:21,790 --> 00:09:30,699
the size that of the step that you take

163
00:09:26,680 --> 00:09:32,890
to look for that minimum if you take two

164
00:09:30,700 --> 00:09:35,500
larger step your you may be over step in

165
00:09:32,890 --> 00:09:37,660
one minimum but if you the step is too

166
00:09:35,500 --> 00:09:40,570
small then then you go too slowly so you

167
00:09:37,660 --> 00:09:42,130
have to adjust or find a strike the

168
00:09:40,570 --> 00:09:45,700
right balance so that it doesn't take

169
00:09:42,130 --> 00:09:50,020
five years to work or all right not

170
00:09:45,700 --> 00:09:52,360
finding anything what I have to the

171
00:09:50,020 --> 00:09:54,550
right are the different training methods

172
00:09:52,360 --> 00:10:01,270
and at the end what I do is to solve

173
00:09:54,550 --> 00:10:05,890
that gradient descent and the one that

174
00:10:01,270 --> 00:10:08,710
has been used more often is the SGD the

175
00:10:05,890 --> 00:10:11,890
following one's are more recent more

176
00:10:08,710 --> 00:10:14,470
modern and better for instance I Delta

177
00:10:11,890 --> 00:10:19,569
adapt to what your training therefore is

178
00:10:14,470 --> 00:10:21,580
white likely that the rate changes often

179
00:10:19,570 --> 00:10:25,000
to the circumstances you're following me

180
00:10:21,580 --> 00:10:26,500
right okay so when we talk about

181
00:10:25,000 --> 00:10:29,470
computer vision we have to make a

182
00:10:26,500 --> 00:10:32,410
difference between a recognizing a

183
00:10:29,470 --> 00:10:34,720
single object or multi multiple objects

184
00:10:32,410 --> 00:10:37,150
and in this case we have the

185
00:10:34,720 --> 00:10:39,880
classification which is what's done most

186
00:10:37,150 --> 00:10:43,900
often in computer vision I give you a

187
00:10:39,880 --> 00:10:45,460
picture tell me what's there then we

188
00:10:43,900 --> 00:10:46,810
have localization I'll give you a

189
00:10:45,460 --> 00:10:49,960
picture tell me what's in the picture

190
00:10:46,810 --> 00:10:53,619
and where it is where is it this is what

191
00:10:49,960 --> 00:10:56,080
I use for signatures I have an image of

192
00:10:53,620 --> 00:10:58,870
the document and I have a neural network

193
00:10:56,080 --> 00:11:00,220
that looks where in the documentary is a

194
00:10:58,870 --> 00:11:05,020
signature because the signature can be

195
00:11:00,220 --> 00:11:09,520
to the side or at the bottom or so I

196
00:11:05,020 --> 00:11:12,040
need first of all to localize it more

197
00:11:09,520 --> 00:11:13,540
complex is object detection when when

198
00:11:12,040 --> 00:11:15,969
there are multiple objects and I said

199
00:11:13,540 --> 00:11:19,839
more complex because they may overlap

200
00:11:15,970 --> 00:11:22,450
and what's even harder is instant

201
00:11:19,840 --> 00:11:25,150
segmentation which is to give me a pixel

202
00:11:22,450 --> 00:11:26,100
by pixel whether that pixel contains the

203
00:11:25,150 --> 00:11:30,360
object or not

204
00:11:26,100 --> 00:11:32,890
okay so far so good within neural

205
00:11:30,360 --> 00:11:36,940
networks are great

206
00:11:32,890 --> 00:11:40,380
when it comes to real work it is not so

207
00:11:36,940 --> 00:11:43,630
wonderful in real work you're given

208
00:11:40,380 --> 00:11:45,400
eight thousand documents with signatures

209
00:11:43,630 --> 00:11:48,580
in different positions and the first job

210
00:11:45,400 --> 00:11:52,000
you have to do is open them in a picture

211
00:11:48,580 --> 00:11:53,860
editor look for the signature in mark

212
00:11:52,000 --> 00:11:56,680
where it is to be able to train and it's

213
00:11:53,860 --> 00:11:58,210
quite time-consuming and you cannot send

214
00:11:56,680 --> 00:12:00,339
it to a third party because these are

215
00:11:58,210 --> 00:12:02,700
classified data and you have to do it

216
00:12:00,340 --> 00:12:02,700
in-house

217
00:12:07,699 --> 00:12:16,019
this is images' name which is the one

218
00:12:11,249 --> 00:12:20,430
that been driving a computer vision it's

219
00:12:16,019 --> 00:12:23,550
a great it's a huge picture collection

220
00:12:20,430 --> 00:12:26,519
that they have already classified they

221
00:12:23,550 --> 00:12:28,920
did that with a Mechanical Turk Amazon

222
00:12:26,519 --> 00:12:31,889
Mechanical Turk is actually a platform

223
00:12:28,920 --> 00:12:34,829
in which you upload a problem to resolve

224
00:12:31,889 --> 00:12:36,899
and that was so that is all by human

225
00:12:34,829 --> 00:12:40,349
beings for is that in mechanical to one

226
00:12:36,899 --> 00:12:43,709
days and 1,200,000 images to localize

227
00:12:40,350 --> 00:12:45,839
were these objects was or some other

228
00:12:43,709 --> 00:12:49,399
object what happened behind was that

229
00:12:45,839 --> 00:12:49,399
there were people from all countries

230
00:12:49,519 --> 00:12:56,879
defining in a program where the objects

231
00:12:54,269 --> 00:12:58,439
were so it's quite painstaking work and

232
00:12:56,879 --> 00:13:02,610
in the last few years there were some

233
00:12:58,439 --> 00:13:05,248
challenges on their data again which

234
00:13:02,610 --> 00:13:07,470
were better at classifying and there

235
00:13:05,249 --> 00:13:10,290
were different categories for that and

236
00:13:07,470 --> 00:13:13,290
the first thing is to to go between top

237
00:13:10,290 --> 00:13:15,329
one and top five it means that you in a

238
00:13:13,290 --> 00:13:18,449
picture you say what it is and it's in

239
00:13:15,329 --> 00:13:21,899
top one and top five and in a picture

240
00:13:18,449 --> 00:13:25,109
you can say but it could be the top five

241
00:13:21,899 --> 00:13:28,230
of things that you say it is so there is

242
00:13:25,110 --> 00:13:31,279
a radical change in 2012 why because

243
00:13:28,230 --> 00:13:31,279
until the year before

244
00:13:32,350 --> 00:13:38,680
the CDs were being used which were

245
00:13:35,680 --> 00:13:41,699
classical never near our networks and in

246
00:13:38,680 --> 00:13:43,900
2012 Alice Nate came along which was a

247
00:13:41,700 --> 00:13:45,910
convolutional Network which is what's

248
00:13:43,900 --> 00:13:49,860
used in computer vision right now and

249
00:13:45,910 --> 00:13:54,880
since there's been an improvement in

250
00:13:49,860 --> 00:13:58,450
2012 alex neck net1 and since 2015

251
00:13:54,880 --> 00:14:03,540
Google and the margin of error were say

252
00:13:58,450 --> 00:14:05,440
3% in 2060 and so below human error and

253
00:14:03,540 --> 00:14:07,530
sometimes is due to the fact that not

254
00:14:05,440 --> 00:14:08,920
that the pictures are not alright and in

255
00:14:07,530 --> 00:14:11,410
2017

256
00:14:08,920 --> 00:14:14,319
we know although it has not been

257
00:14:11,410 --> 00:14:17,260
publicly of made public that it's below

258
00:14:14,320 --> 00:14:18,640
2% when we talk about convolution we ask

259
00:14:17,260 --> 00:14:21,970
ourselves what does it mean well it's

260
00:14:18,640 --> 00:14:25,390
only the product of matrixes so an image

261
00:14:21,970 --> 00:14:27,640
is a matrix of colors we all agree on

262
00:14:25,390 --> 00:14:30,550
that and if there's another matrix if we

263
00:14:27,640 --> 00:14:32,680
multiply in the window we're going to

264
00:14:30,550 --> 00:14:35,189
obtain images like this one so a

265
00:14:32,680 --> 00:14:38,819
convolutional network is the one that

266
00:14:35,190 --> 00:14:41,560
can consider those compensation

267
00:14:38,820 --> 00:14:43,750
convolutions as part of the weights so

268
00:14:41,560 --> 00:14:45,550
those convolutions are calculated and

269
00:14:43,750 --> 00:14:47,470
what are they good for to the tech

270
00:14:45,550 --> 00:14:49,510
features for instance here in this

271
00:14:47,470 --> 00:14:51,430
convolution here we see that we are

272
00:14:49,510 --> 00:14:54,520
removing all colors and we're staying

273
00:14:51,430 --> 00:14:56,859
with the borders with so here we could

274
00:14:54,520 --> 00:15:00,640
see straight lines that will tell us

275
00:14:56,860 --> 00:15:03,040
whether this is a built in a door etc

276
00:15:00,640 --> 00:15:06,939
and if we're not in calculating the

277
00:15:03,040 --> 00:15:12,520
future but 200 all different and those

278
00:15:06,940 --> 00:15:16,240
200 features are the input for 400 more

279
00:15:12,520 --> 00:15:17,650
below so the quantity the quantity of

280
00:15:16,240 --> 00:15:20,170
features that your computer is

281
00:15:17,650 --> 00:15:25,780
multiplying but there is a problem if

282
00:15:20,170 --> 00:15:27,610
you have an image size and I didn't say

283
00:15:25,780 --> 00:15:30,850
that billion computer vision systems

284
00:15:27,610 --> 00:15:33,970
input has a predefined size if you

285
00:15:30,850 --> 00:15:37,810
decide that improves 290 by 1990

286
00:15:33,970 --> 00:15:41,019
you put in a picture it escalates so if

287
00:15:37,810 --> 00:15:43,479
all the layers are done in 299 title

288
00:15:41,019 --> 00:15:45,459
none' 299 we are going to take several

289
00:15:43,480 --> 00:15:48,250
years to train a computer vision system

290
00:15:45,459 --> 00:15:51,008
so we try to minimize the size and each

291
00:15:48,250 --> 00:15:54,759
step of the images to keep only the

292
00:15:51,009 --> 00:16:03,339
feature so that it's faster in faster so

293
00:15:54,759 --> 00:16:05,980
X by Y squared X by Y so you're applying

294
00:16:03,339 --> 00:16:12,339
the square to the complexity so in for

295
00:16:05,980 --> 00:16:15,610
that we use 14 max boolean if we have to

296
00:16:12,339 --> 00:16:19,360
reduce the size to half of it I go

297
00:16:15,610 --> 00:16:24,069
beyond four points you keep the highest

298
00:16:19,360 --> 00:16:30,240
one so six eight three and four if we

299
00:16:24,069 --> 00:16:30,240
combine convolution is it that late

300
00:16:30,980 --> 00:16:38,690
okay I'm gonna have to rush this is what

301
00:16:35,030 --> 00:16:41,120
the layer see and you take picture of

302
00:16:38,690 --> 00:16:45,170
the Asia we are going to see in your on

303
00:16:41,120 --> 00:16:47,210
network in your portable computers and

304
00:16:45,170 --> 00:16:51,890
here let's move on to the attacks there

305
00:16:47,210 --> 00:16:54,170
are small attacks heart attacks and

306
00:16:51,890 --> 00:16:56,420
picture attacks and real-life attacks

307
00:16:54,170 --> 00:16:59,000
there are many articles on that here

308
00:16:56,420 --> 00:17:01,250
there's one that summarizes them all and

309
00:16:59,000 --> 00:17:03,830
this is the 1 pixel attack what you

310
00:17:01,250 --> 00:17:07,400
still modify a single picture pixel

311
00:17:03,830 --> 00:17:09,560
itself working against the fire and the

312
00:17:07,400 --> 00:17:12,680
pictures are 32 by 32 what's happening

313
00:17:09,560 --> 00:17:15,500
internally whether a single pixel in one

314
00:17:12,680 --> 00:17:20,240
picture when you take it to three layers

315
00:17:15,500 --> 00:17:22,970
the ren21 of the emitters is taking 255

316
00:17:20,240 --> 00:17:26,800
and the other to zero and that entails a

317
00:17:22,970 --> 00:17:29,540
huge perturbation of the size of the

318
00:17:26,800 --> 00:17:31,550
image that distributes throughout the

319
00:17:29,540 --> 00:17:35,120
whole network and it is misinterpreting

320
00:17:31,550 --> 00:17:40,070
what's in the image so this came along

321
00:17:35,120 --> 00:17:43,090
in August 2017 the cars have been hacked

322
00:17:40,070 --> 00:17:43,090
we're all going to die

323
00:17:43,120 --> 00:17:50,830
with stickers they managed to do the

324
00:17:46,670 --> 00:17:54,710
face and in traffic linear science and

325
00:17:50,830 --> 00:17:56,480
and it fooled cars in 67% of cases

326
00:17:54,710 --> 00:17:58,280
what's the reality well this is the real

327
00:17:56,480 --> 00:18:01,010
biggest in size of the pictures they

328
00:17:58,280 --> 00:18:05,300
were using them are using 32 by 32 I

329
00:18:01,010 --> 00:18:07,480
don't even know I don't know what's

330
00:18:05,300 --> 00:18:07,480
there

331
00:18:07,690 --> 00:18:12,039
they didn't actually use a real

332
00:18:10,169 --> 00:18:15,700
autonomous car they did their own

333
00:18:12,039 --> 00:18:20,789
networking and it's a homemade network

334
00:18:15,700 --> 00:18:20,789
so they weren't doing image filtering

335
00:18:20,909 --> 00:18:26,470
they were working with raw data and they

336
00:18:23,830 --> 00:18:28,360
were not doing augmentation imitation

337
00:18:26,470 --> 00:18:31,179
means from a picture to calculate

338
00:18:28,360 --> 00:18:34,149
several pictures to to have more

339
00:18:31,179 --> 00:18:36,700
advanced data they adversarial what's

340
00:18:34,149 --> 00:18:40,629
that an adversarial is given an input

341
00:18:36,700 --> 00:18:43,000
image X the result of the CNAs are

342
00:18:40,629 --> 00:18:45,129
probably distribution over labels so you

343
00:18:43,000 --> 00:18:47,080
can modify the image to I'd make it

344
00:18:45,129 --> 00:18:51,330
identified or something else so you

345
00:18:47,080 --> 00:18:51,330
proposed this theory which is basically

346
00:18:51,720 --> 00:18:58,299
given an input image X you have to

347
00:18:55,360 --> 00:19:00,158
calculate an experiment which is the

348
00:18:58,299 --> 00:19:02,408
original picture slightly modified so

349
00:19:00,159 --> 00:19:07,659
that it means that the probability given

350
00:19:02,409 --> 00:19:09,299
back by the network is y prima and you

351
00:19:07,659 --> 00:19:13,509
have to do that with a very small

352
00:19:09,299 --> 00:19:16,629
perturbation and I chose to over 255 I

353
00:19:13,509 --> 00:19:19,659
don't want to move every pixel be more

354
00:19:16,629 --> 00:19:23,320
than two colors so if you have a

355
00:19:19,659 --> 00:19:25,409
discriminator it's an existing network

356
00:19:23,320 --> 00:19:28,509
there is a generator and that generator

357
00:19:25,409 --> 00:19:35,309
generates images that keep attacking and

358
00:19:28,509 --> 00:19:39,789
the problem that it's a black box system

359
00:19:35,309 --> 00:19:42,820
but it takes a very long time but we

360
00:19:39,789 --> 00:19:44,980
but Kristen said II did that my a system

361
00:19:42,820 --> 00:19:47,139
by applying this perturbation to this

362
00:19:44,980 --> 00:19:48,549
picture generates a picture that for us

363
00:19:47,139 --> 00:19:50,949
is the same but all of these pictures

364
00:19:48,549 --> 00:19:53,879
here and all these pictures here the

365
00:19:50,950 --> 00:19:56,919
networks say that they are an ostrich

366
00:19:53,880 --> 00:19:59,139
they discovered that it doesn't depend

367
00:19:56,919 --> 00:20:00,700
on the topology they you're using they

368
00:19:59,139 --> 00:20:02,500
thought that it was only going to use

369
00:20:00,700 --> 00:20:06,279
for a certain type of topology but

370
00:20:02,500 --> 00:20:09,070
whatever the polity you use the adverse

371
00:20:06,279 --> 00:20:11,169
earned the advisory works in many

372
00:20:09,070 --> 00:20:14,490
topologies not all but quite a lot and

373
00:20:11,169 --> 00:20:17,409
they are not sure about why but they

374
00:20:14,490 --> 00:20:19,600
think it's not due to the linearity of

375
00:20:17,409 --> 00:20:24,309
rather they are thinking that neuron

376
00:20:19,600 --> 00:20:25,959
networks learn some master features and

377
00:20:24,309 --> 00:20:28,090
beyond the ones that are running that

378
00:20:25,960 --> 00:20:30,580
and they tend to be present in other

379
00:20:28,090 --> 00:20:34,230
networks I would train with a picture

380
00:20:30,580 --> 00:20:34,230
maritime this is going to be fast

381
00:20:45,980 --> 00:20:52,050
what i'm doing here is i'm taking the

382
00:20:49,650 --> 00:20:54,480
inception will be tree already trained

383
00:20:52,050 --> 00:21:03,270
by Google that they you're already in

384
00:20:54,480 --> 00:21:05,910
bistro and honor voted a logo I'm saying

385
00:21:03,270 --> 00:21:12,200
what does it see it sees something

386
00:21:05,910 --> 00:21:12,200
strange I can do that with a rifle

387
00:21:15,520 --> 00:21:19,450
but I'll stick to the rooted because I

388
00:21:17,710 --> 00:21:21,220
don't have that much time so on the

389
00:21:19,450 --> 00:21:23,590
network that I already have trained

390
00:21:21,220 --> 00:21:26,110
rather than using a black box method I

391
00:21:23,590 --> 00:21:29,409
am using a white box so if the network

392
00:21:26,110 --> 00:21:31,209
is already a trained and I already have

393
00:21:29,410 --> 00:21:35,410
the gradient in this and I pick another

394
00:21:31,210 --> 00:21:38,440
label and I'm going to modify the

395
00:21:35,410 --> 00:21:45,550
picture gradually you know random way

396
00:21:38,440 --> 00:21:49,120
more or less you see a training here and

397
00:21:45,550 --> 00:21:51,370
you see how the LSA goes down and I

398
00:21:49,120 --> 00:21:57,040
didn't tells me that the effort that

399
00:21:51,370 --> 00:22:03,040
logo is dark but there's a problem here

400
00:21:57,040 --> 00:22:05,920
and that is that it's not robust in

401
00:22:03,040 --> 00:22:08,230
rotation but for my example the the

402
00:22:05,920 --> 00:22:12,010
manner about signatures is robust enough

403
00:22:08,230 --> 00:22:15,850
because we do not get several images and

404
00:22:12,010 --> 00:22:18,610
by attacking just one you can already

405
00:22:15,850 --> 00:22:21,809
fool the neural network that only

406
00:22:18,610 --> 00:22:21,809
identifies one image

407
00:22:28,070 --> 00:22:33,899
so for rotation it can't be done but

408
00:22:31,470 --> 00:22:38,100
since I have achieved me you will know

409
00:22:33,899 --> 00:22:41,399
who DP you but it can't be done and I

410
00:22:38,100 --> 00:22:45,649
can make a rifle look like a hot dog for

411
00:22:41,399 --> 00:22:45,649
the machine how do we do that

412
00:22:50,140 --> 00:22:57,990
we knew the P of y prima they had to be

413
00:22:54,490 --> 00:23:02,740
happy by the minimum volume but now the

414
00:22:57,990 --> 00:23:04,990
P of Y Ramon transformation of X Abram

415
00:23:02,740 --> 00:23:06,880
has to be the minimum of the gradient

416
00:23:04,990 --> 00:23:09,580
ation for the transformation

417
00:23:06,880 --> 00:23:12,670
distribution that we have this can also

418
00:23:09,580 --> 00:23:15,129
be programmed but it's a more

419
00:23:12,670 --> 00:23:17,140
time-consuming something else that I

420
00:23:15,130 --> 00:23:20,230
found as I was doing this experiment

421
00:23:17,140 --> 00:23:22,299
it's not the images that are black on Y

422
00:23:20,230 --> 00:23:24,429
for instance a true logo of the route

423
00:23:22,299 --> 00:23:27,100
that cannot be preserved why because we

424
00:23:24,429 --> 00:23:29,620
have the limitation of two colors per

425
00:23:27,100 --> 00:23:31,330
pixel and if you have something that has

426
00:23:29,620 --> 00:23:33,699
only two colors getting a perturbation

427
00:23:31,330 --> 00:23:36,159
further the machine to believe that is

428
00:23:33,700 --> 00:23:39,850
something else you have to the machine

429
00:23:36,160 --> 00:23:41,710
will notice you that would be perceived

430
00:23:39,850 --> 00:23:44,230
by a human being that's what we don't

431
00:23:41,710 --> 00:23:46,540
want so pictures in black and white only

432
00:23:44,230 --> 00:23:47,230
two colors are very few colors and this

433
00:23:46,540 --> 00:23:50,950
is not gonna work

434
00:23:47,230 --> 00:23:55,870
I got we've got a lot of colors around

435
00:23:50,950 --> 00:23:57,850
yet so doing the test on the signature

436
00:23:55,870 --> 00:24:00,489
itself I see that when I'm attacking the

437
00:23:57,850 --> 00:24:02,500
DNA the identity piece a it works

438
00:24:00,490 --> 00:24:04,540
because there are colors behind that I

439
00:24:02,500 --> 00:24:07,570
can modify it without anybody notice and

440
00:24:04,540 --> 00:24:10,980
I can fool the Machine and say that

441
00:24:07,570 --> 00:24:17,559
there's a signature but in a white paper

442
00:24:10,980 --> 00:24:20,309
you know form Skype more complex if if

443
00:24:17,559 --> 00:24:22,510
the quality in the picture is not bad

444
00:24:20,309 --> 00:24:25,690
maybe maybe you could do it but it would

445
00:24:22,510 --> 00:24:28,690
be a lot more complex and you go beyond

446
00:24:25,690 --> 00:24:31,150
that you can go beyond these people the

447
00:24:28,690 --> 00:24:35,299
paper here at the bottom can do 3d

448
00:24:31,150 --> 00:24:38,690
models that not it's not only about it

449
00:24:35,299 --> 00:24:40,759
dimensions but three dimension in and

450
00:24:38,690 --> 00:24:43,279
they never know that work and believe

451
00:24:40,759 --> 00:24:44,989
that this turtle actually a rifle so

452
00:24:43,279 --> 00:24:47,480
here come here this is my question what

453
00:24:44,989 --> 00:24:51,379
do you think it's more dangerous that if

454
00:24:47,480 --> 00:24:53,960
we have defense in a network that a guy

455
00:24:51,379 --> 00:24:56,689
with a rifle it can be interpreted as a

456
00:24:53,960 --> 00:24:59,389
turtle or somebody with the turtle can

457
00:24:56,690 --> 00:25:01,220
be considered a rifle because if a guy

458
00:24:59,389 --> 00:25:02,928
comes in with a rifle we know what he's

459
00:25:01,220 --> 00:25:07,009
after and everybody's going to see that

460
00:25:02,929 --> 00:25:10,869
but imagine somebody giving Turtles to

461
00:25:07,009 --> 00:25:10,869
the kids at the entrance of a school and

462
00:25:11,529 --> 00:25:19,909
this is a thought experiment or a

463
00:25:14,570 --> 00:25:22,639
concept test that was done when we when

464
00:25:19,909 --> 00:25:26,840
it was said that they who are going to

465
00:25:22,639 --> 00:25:30,519
die this is a yeah it is not very smart

466
00:25:26,840 --> 00:25:34,399
and what surrounded it's a it's a salt

467
00:25:30,519 --> 00:25:38,359
made circle sort of a bit of esoterism

468
00:25:34,399 --> 00:25:41,508
here and it has not been proven that in

469
00:25:38,359 --> 00:25:45,070
real life this attack can work against

470
00:25:41,509 --> 00:25:49,059
the tesla so so far we seem to be safe

471
00:25:45,070 --> 00:25:49,059
that's all questioned

472
00:26:10,070 --> 00:26:18,909
hello I have a question with networks or

473
00:26:17,090 --> 00:26:23,449
rather the computer we should network

474
00:26:18,910 --> 00:26:27,620
which is so trendy in China right now

475
00:26:23,450 --> 00:26:30,680
with face recognition is there any

476
00:26:27,620 --> 00:26:32,419
device or technology that can make the

477
00:26:30,680 --> 00:26:34,760
distortion in real time that is

478
00:26:32,420 --> 00:26:36,800
something that without you touching the

479
00:26:34,760 --> 00:26:45,350
screen can actually distort the image

480
00:26:36,800 --> 00:26:49,250
that it's the machine is seen yeah the

481
00:26:45,350 --> 00:26:52,280
example that I have given you that has

482
00:26:49,250 --> 00:26:57,680
taken a few seconds it's working on CPU

483
00:26:52,280 --> 00:27:00,800
but but with the video I'm not saying

484
00:26:57,680 --> 00:27:03,380
that it's going to go by 20 frames per

485
00:27:00,800 --> 00:27:11,200
second but perhaps 4 frames per second

486
00:27:03,380 --> 00:27:14,330
has to face recognition one is about

487
00:27:11,200 --> 00:27:17,330
generative adversary adversarial

488
00:27:14,330 --> 00:27:20,120
networks is that the that you are

489
00:27:17,330 --> 00:27:23,449
generating a random faces there is

490
00:27:20,120 --> 00:27:26,689
something that generates faces and that

491
00:27:23,450 --> 00:27:29,570
the facial recognition recognizer say

492
00:27:26,690 --> 00:27:32,060
yes it is a face and they can tell you

493
00:27:29,570 --> 00:27:37,370
whether it's smiling or looking sad so

494
00:27:32,060 --> 00:27:40,570
the technology is there no more

495
00:27:37,370 --> 00:27:40,570
questions ok thank you

496
00:27:41,870 --> 00:27:46,389
[Applause]

497
00:27:48,190 --> 00:27:55,059
[Music]

