1
00:00:00,000 --> 00:00:14,070
enjoy good evening folks

2
00:00:10,110 --> 00:00:16,619
whoops evening folks thanks thanks for

3
00:00:14,070 --> 00:00:19,470
being here sorry about the wait my name

4
00:00:16,619 --> 00:00:20,460
is Raghav I'm a I'm a student at

5
00:00:19,470 --> 00:00:24,359
University of Illinois at

6
00:00:20,460 --> 00:00:26,279
urbana-champaign and over the last

7
00:00:24,359 --> 00:00:29,189
summer I've been working on a little bit

8
00:00:26,279 --> 00:00:31,800
of adversarial machine learning and for

9
00:00:29,189 --> 00:00:34,200
those of you who don't know adversarial

10
00:00:31,800 --> 00:00:35,850
machine learning is basically you want

11
00:00:34,200 --> 00:00:42,210
to fool or break the machine learning

12
00:00:35,850 --> 00:00:45,239
algorithm so there's a lot of hype

13
00:00:42,210 --> 00:00:46,980
around AI in these days you know from

14
00:00:45,239 --> 00:00:49,589
from machine translation you can

15
00:00:46,980 --> 00:00:52,050
translate from French to English a lot

16
00:00:49,590 --> 00:00:55,020
of image recognition yours your iPhones

17
00:00:52,050 --> 00:00:58,788
have your facial recognition lots of

18
00:00:55,020 --> 00:01:02,070
hype around the self-driving car I

19
00:00:58,789 --> 00:01:04,890
personally don't believe in but hey it's

20
00:01:02,070 --> 00:01:07,640
good to live in the dream I guess so

21
00:01:04,890 --> 00:01:09,810
lots of people following that track and

22
00:01:07,640 --> 00:01:12,619
supposedly mushrik machine learning and

23
00:01:09,810 --> 00:01:16,049
AI is going to be the next big change

24
00:01:12,619 --> 00:01:19,080
you know so machine learning is

25
00:01:16,049 --> 00:01:22,830
basically based on a concept that we the

26
00:01:19,080 --> 00:01:25,408
machines think like humans and so

27
00:01:22,830 --> 00:01:27,780
especially in visual computing when you

28
00:01:25,409 --> 00:01:30,570
have these facial detectors and you know

29
00:01:27,780 --> 00:01:32,700
self-driving cars that so so it really

30
00:01:30,570 --> 00:01:36,899
begs the question how does the computer

31
00:01:32,700 --> 00:01:39,960
look at images and you know how are they

32
00:01:36,900 --> 00:01:45,960
different in perception as we look at

33
00:01:39,960 --> 00:01:49,589
objects so it turns out the computers

34
00:01:45,960 --> 00:01:52,460
look at extremely differently and so the

35
00:01:49,590 --> 00:01:56,399
images you see here are 32-bit float and

36
00:01:52,460 --> 00:01:58,320
you know so it turns out that in a

37
00:01:56,399 --> 00:02:01,320
machine learning classifier so first of

38
00:01:58,320 --> 00:02:02,669
all neural networks if you guys have

39
00:02:01,320 --> 00:02:05,210
heard about it and have recently

40
00:02:02,670 --> 00:02:08,098
achieved human level performance and

41
00:02:05,210 --> 00:02:10,139
they've been really successful and you

42
00:02:08,098 --> 00:02:13,140
know they've been deployed deployed in

43
00:02:10,139 --> 00:02:14,019
your iphones and stuff but turns out you

44
00:02:13,140 --> 00:02:17,019
can actually

45
00:02:14,020 --> 00:02:21,190
introduced a perturbation or as you can

46
00:02:17,020 --> 00:02:24,580
you can see a random noise out here that

47
00:02:21,190 --> 00:02:27,130
looks random to the human eye but if you

48
00:02:24,580 --> 00:02:29,730
add that noise to your original image

49
00:02:27,130 --> 00:02:33,880
can completely miss classify the output

50
00:02:29,730 --> 00:02:36,119
so if you look at the school bus here so

51
00:02:33,880 --> 00:02:38,859
this is one of the perceptions of

52
00:02:36,120 --> 00:02:42,130
machine learning and it gets classified

53
00:02:38,860 --> 00:02:43,900
as school bus now this is another

54
00:02:42,130 --> 00:02:46,740
perception of a school bus by the

55
00:02:43,900 --> 00:02:49,630
computer and this is based on the direct

56
00:02:46,740 --> 00:02:53,170
encoding so you basically are looking at

57
00:02:49,630 --> 00:02:55,570
a pixel level mapping and here's a the

58
00:02:53,170 --> 00:02:58,750
second picture the stripes one is an

59
00:02:55,570 --> 00:03:00,459
indirect encoding so you basically take

60
00:02:58,750 --> 00:03:04,420
a pixel and you're trying to map a

61
00:03:00,460 --> 00:03:06,940
pattern on the nearest neighbors and so

62
00:03:04,420 --> 00:03:10,000
surprisingly you know with the human

63
00:03:06,940 --> 00:03:11,590
level performance of these neural

64
00:03:10,000 --> 00:03:14,020
networks you're able to get these

65
00:03:11,590 --> 00:03:18,070
perceptions and hey these are not school

66
00:03:14,020 --> 00:03:20,800
buses so why are these algorithms being

67
00:03:18,070 --> 00:03:24,690
deployed in you know facial recognition

68
00:03:20,800 --> 00:03:30,910
and self-driving cars another example is

69
00:03:24,690 --> 00:03:34,540
you know these from top to bottom are

70
00:03:30,910 --> 00:03:37,079
the representations of zero so this is

71
00:03:34,540 --> 00:03:39,670
what zero looks like if you visualize

72
00:03:37,080 --> 00:03:43,300
inside a neural network and you look at

73
00:03:39,670 --> 00:03:47,579
the intermediate layers so something

74
00:03:43,300 --> 00:03:47,580
really sketchy is going on inside and

75
00:03:47,760 --> 00:03:54,730
there was this concept introduced by a

76
00:03:50,680 --> 00:03:58,240
few researchers from Google brain and so

77
00:03:54,730 --> 00:03:59,769
they took this image of a panda and so

78
00:03:58,240 --> 00:04:01,600
they passed it on on to a machine

79
00:03:59,770 --> 00:04:05,670
learning classifier and they got about

80
00:04:01,600 --> 00:04:08,769
57% confidence that hey it's a panda

81
00:04:05,670 --> 00:04:11,320
then they add this visually

82
00:04:08,770 --> 00:04:14,140
imperceptible noise which is

83
00:04:11,320 --> 00:04:16,149
imperceptible to the humans but when

84
00:04:14,140 --> 00:04:18,969
they add that noise it gets completely

85
00:04:16,149 --> 00:04:21,849
misclassified as a Gibbon so to give you

86
00:04:18,970 --> 00:04:24,490
a perspective if your self-driving car

87
00:04:21,850 --> 00:04:26,130
tomorrow miss classify as a green light

88
00:04:24,490 --> 00:04:28,920
as a stop sign

89
00:04:26,130 --> 00:04:32,120
stop sign is a green light so that's a

90
00:04:28,920 --> 00:04:32,120
precarious situation

91
00:04:34,380 --> 00:04:41,700
moreover last year I think a couple of

92
00:04:39,720 --> 00:04:44,100
researchers again from Google showed

93
00:04:41,700 --> 00:04:46,710
that you know when you take these

94
00:04:44,100 --> 00:04:48,900
so-called perturbed images adversarial

95
00:04:46,710 --> 00:04:52,560
images and you print them out on a sheet

96
00:04:48,900 --> 00:04:56,669
of paper and you you use feed them as an

97
00:04:52,560 --> 00:05:00,620
image digitally again it still gets

98
00:04:56,670 --> 00:05:03,000
misclassified so here you see the image

99
00:05:00,620 --> 00:05:06,540
so they actually printed it out on a

100
00:05:03,000 --> 00:05:10,040
paper the noise the noisy image and it

101
00:05:06,540 --> 00:05:13,500
got misclassified by the algorithm so

102
00:05:10,040 --> 00:05:15,870
that shows that these perturb images

103
00:05:13,500 --> 00:05:19,500
these noisy images that can fool the

104
00:05:15,870 --> 00:05:21,780
classifier can exist in real world they

105
00:05:19,500 --> 00:05:25,310
also tried that with a 3d turtle so they

106
00:05:21,780 --> 00:05:29,640
actually 3d printed a turtle with these

107
00:05:25,310 --> 00:05:31,530
noisy images and this was able to miss

108
00:05:29,640 --> 00:05:33,330
classify it so I don't know if you can

109
00:05:31,530 --> 00:05:36,239
see it but it gets misclassified as a

110
00:05:33,330 --> 00:05:38,729
rat rifle over there and that's not a

111
00:05:36,240 --> 00:05:42,530
rifle but any any guesses where the

112
00:05:38,730 --> 00:05:50,460
noises in this image in the turtle image

113
00:05:42,530 --> 00:05:54,830
and he guess yes that's that's right

114
00:05:50,460 --> 00:06:05,010
answer so the noise is right here so

115
00:05:54,830 --> 00:06:07,530
that came that so yeah a lot of being a

116
00:06:05,010 --> 00:06:10,349
lot of work has been done on this and

117
00:06:07,530 --> 00:06:12,359
people have gone crazy

118
00:06:10,350 --> 00:06:16,140
researchers have gone crazy on this it

119
00:06:12,360 --> 00:06:18,240
releases about 15 new attacks depending

120
00:06:16,140 --> 00:06:21,360
on an optimization and gradient based

121
00:06:18,240 --> 00:06:23,160
and but they all share one common thing

122
00:06:21,360 --> 00:06:26,370
that you try to find the internal

123
00:06:23,160 --> 00:06:28,470
structure of what's going on and you try

124
00:06:26,370 --> 00:06:32,580
to replicate it and then you try to fool

125
00:06:28,470 --> 00:06:36,870
it by going in an opposite direction so

126
00:06:32,580 --> 00:06:39,870
and so this these attacks can be carried

127
00:06:36,870 --> 00:06:40,289
out in two ways one is the untargeted

128
00:06:39,870 --> 00:06:43,409
and

129
00:06:40,289 --> 00:06:45,210
targeted so by untargeted it can just

130
00:06:43,409 --> 00:06:46,919
misclassify to anything else

131
00:06:45,210 --> 00:06:50,068
whereas targeted is the most dangerous

132
00:06:46,919 --> 00:06:52,438
one because you can actually target what

133
00:06:50,069 --> 00:06:55,110
do you want to miss classify it so as I

134
00:06:52,439 --> 00:06:57,599
said you can miss classify stop sign to

135
00:06:55,110 --> 00:07:01,379
a green green light and that's that's a

136
00:06:57,599 --> 00:07:04,259
dangerous situation again white box and

137
00:07:01,379 --> 00:07:06,689
black box if the attacker or the

138
00:07:04,259 --> 00:07:13,110
adversary knows what kind of network

139
00:07:06,689 --> 00:07:15,569
you're using another interesting thing

140
00:07:13,110 --> 00:07:19,770
last year so this is one of my friends

141
00:07:15,569 --> 00:07:23,729
from Google and so he's working on these

142
00:07:19,770 --> 00:07:26,159
adversary things not just for images

143
00:07:23,729 --> 00:07:31,349
they can actually also exist for speech

144
00:07:26,159 --> 00:07:34,369
so let's look at this I don't know if

145
00:07:31,349 --> 00:07:34,369
not if it's gonna give me

146
00:07:42,860 --> 00:07:45,520
okay

147
00:07:54,980 --> 00:07:59,790
did you guys hear it sorry about the

148
00:07:57,420 --> 00:08:02,360
guys in the back but it says without the

149
00:07:59,790 --> 00:08:05,100
data said the article is useless and

150
00:08:02,360 --> 00:08:07,050
that's the original output so this is

151
00:08:05,100 --> 00:08:10,020
basically a speech to text conversion

152
00:08:07,050 --> 00:08:15,510
converter that that's used in Siri and

153
00:08:10,020 --> 00:08:17,700
Google now and when this when this

154
00:08:15,510 --> 00:08:19,620
speech is passed on so the output is

155
00:08:17,700 --> 00:08:22,590
without the data set the article is

156
00:08:19,620 --> 00:08:27,630
useless but if I if they passed on a

157
00:08:22,590 --> 00:08:30,690
slightly noisier version of it so it's

158
00:08:27,630 --> 00:08:33,240
still the same speech but that the text

159
00:08:30,690 --> 00:08:36,299
that they get is okay Google browse to

160
00:08:33,240 --> 00:08:38,490
evil calm so that's kind of sketchy

161
00:08:36,299 --> 00:08:40,650
right so if you take a phone and say

162
00:08:38,490 --> 00:08:43,289
sorry hey you know what's the weather

163
00:08:40,650 --> 00:08:46,709
outside and it takes you to a malicious

164
00:08:43,289 --> 00:08:50,579
site so you don't want that so the point

165
00:08:46,710 --> 00:08:52,770
being that these adversarial things they

166
00:08:50,580 --> 00:08:55,290
just don't exist for images they exist

167
00:08:52,770 --> 00:08:58,680
for speech they can exist for text as

168
00:08:55,290 --> 00:09:01,050
well and another interesting thing is

169
00:08:58,680 --> 00:09:03,479
that they're not specific to neural

170
00:09:01,050 --> 00:09:09,000
networks they can there they're spread

171
00:09:03,480 --> 00:09:12,720
across all these networks so why do they

172
00:09:09,000 --> 00:09:14,550
exist right and I mean what's what's the

173
00:09:12,720 --> 00:09:16,770
reason why why these things are

174
00:09:14,550 --> 00:09:19,530
happening I mean we're just passing on

175
00:09:16,770 --> 00:09:23,310
simple images and asking the algorithm

176
00:09:19,530 --> 00:09:27,329
to learn so the the reason is a little

177
00:09:23,310 --> 00:09:29,969
mathematical but laid out as simple as

178
00:09:27,330 --> 00:09:31,620
possible so a neural network or a

179
00:09:29,970 --> 00:09:33,690
machine learning algorithm tries to be

180
00:09:31,620 --> 00:09:36,840
nonlinear so let's say a cubic equation

181
00:09:33,690 --> 00:09:38,820
or X to the power 5 X J's so a neural

182
00:09:36,840 --> 00:09:41,610
network is typically to the power

183
00:09:38,820 --> 00:09:43,260
hundred or in in hundreds of dimensions

184
00:09:41,610 --> 00:09:44,910
thousands of dimensions so it's

185
00:09:43,260 --> 00:09:47,040
extremely nonlinear but the

186
00:09:44,910 --> 00:09:49,260
non-linearity right lies in it in its

187
00:09:47,040 --> 00:09:51,689
parameters but the input and output are

188
00:09:49,260 --> 00:09:54,840
still very linear so imagine you're

189
00:09:51,690 --> 00:09:57,480
you're separating a from B and let's say

190
00:09:54,840 --> 00:09:59,580
it's a linear boundary and so the

191
00:09:57,480 --> 00:10:03,630
distance between a and B is still small

192
00:09:59,580 --> 00:10:05,460
so you can point or find a space in the

193
00:10:03,630 --> 00:10:08,130
opposite class

194
00:10:05,460 --> 00:10:14,100
and you know try try and miss miss

195
00:10:08,130 --> 00:10:18,839
classify it where are these adversarial

196
00:10:14,100 --> 00:10:21,270
images being used so the ones the

197
00:10:18,839 --> 00:10:24,060
CAPTCHAs I'm sure most of you must have

198
00:10:21,270 --> 00:10:27,089
seen this so to to find out if person is

199
00:10:24,060 --> 00:10:28,739
a rope if it's a person or a robot so

200
00:10:27,089 --> 00:10:30,779
now with all these state-of-the-art

201
00:10:28,740 --> 00:10:32,970
neural networks and you know you can do

202
00:10:30,779 --> 00:10:35,189
image recognition with human level

203
00:10:32,970 --> 00:10:38,550
performance these CAPTCHAs are useless

204
00:10:35,190 --> 00:10:40,860
so they're replacing these images with

205
00:10:38,550 --> 00:10:46,349
adversarial images so that the robots

206
00:10:40,860 --> 00:10:49,110
cannot you know recognize it another in

207
00:10:46,350 --> 00:10:51,720
direct application of adversarial or

208
00:10:49,110 --> 00:10:54,420
these noisy images are generative

209
00:10:51,720 --> 00:10:57,810
adversarial networks so it's basically a

210
00:10:54,420 --> 00:11:00,930
combination of two networks that combine

211
00:10:57,810 --> 00:11:06,359
to produce an output so for example this

212
00:11:00,930 --> 00:11:08,880
guy the horse I'm sorry the horse so

213
00:11:06,360 --> 00:11:12,630
they feed in two images one is the horse

214
00:11:08,880 --> 00:11:15,930
one is the zebra and it is able to

215
00:11:12,630 --> 00:11:20,300
generate a probability distribution that

216
00:11:15,930 --> 00:11:23,189
converts it from a horse to the zebra

217
00:11:20,300 --> 00:11:25,439
and you can you can use this for

218
00:11:23,190 --> 00:11:27,570
generating more images it's basically a

219
00:11:25,440 --> 00:11:30,839
computer's perception a machine learning

220
00:11:27,570 --> 00:11:34,380
algorithms perception of the images that

221
00:11:30,839 --> 00:11:36,720
it can generate this can be used for

222
00:11:34,380 --> 00:11:39,240
more data generation for machine

223
00:11:36,720 --> 00:11:41,640
learning or can be used for you know

224
00:11:39,240 --> 00:11:44,760
applications like these style transfer

225
00:11:41,640 --> 00:11:47,189
so interestingly a bunch of researchers

226
00:11:44,760 --> 00:11:50,579
were able to transfer style from Picasso

227
00:11:47,190 --> 00:11:57,120
Picasso's paintings to regular images of

228
00:11:50,579 --> 00:11:58,979
people and kind of mix it up but

229
00:11:57,120 --> 00:12:00,720
adversarial images you know there are

230
00:11:58,980 --> 00:12:05,510
malicious there they're harmful but

231
00:12:00,720 --> 00:12:08,220
they're not perfect so interestingly

232
00:12:05,510 --> 00:12:11,880
simple operations on adversarial images

233
00:12:08,220 --> 00:12:16,200
if you crop they're in the image so

234
00:12:11,880 --> 00:12:18,480
let's look at the guy here and the dog

235
00:12:16,200 --> 00:12:19,050
here so the adversarial image gets

236
00:12:18,480 --> 00:12:21,029
missed

237
00:12:19,050 --> 00:12:23,040
fired as a tennis ball and look at the

238
00:12:21,029 --> 00:12:24,450
confidence score seventy five point six

239
00:12:23,040 --> 00:12:28,260
five so it's it's still it's pretty

240
00:12:24,450 --> 00:12:31,019
confident but if you drop it or magnify

241
00:12:28,260 --> 00:12:33,959
this it returns back to its original

242
00:12:31,019 --> 00:12:36,149
class Labrador Retriever so that's very

243
00:12:33,959 --> 00:12:38,489
interesting so it's so that means it's

244
00:12:36,149 --> 00:12:41,959
there is a existing a very specific

245
00:12:38,490 --> 00:12:44,160
pattern in that image that is only

246
00:12:41,959 --> 00:12:47,359
maintained if you do not make these

247
00:12:44,160 --> 00:12:53,160
input transformations same thing for

248
00:12:47,360 --> 00:12:54,839
brightness right so in the case here it

249
00:12:53,160 --> 00:12:58,529
gets misclassified as a tennis ball

250
00:12:54,839 --> 00:13:00,660
again with 7075 percent accuracy in the

251
00:12:58,529 --> 00:13:03,990
second case when I increase the

252
00:13:00,660 --> 00:13:07,319
brightness by 50% it still remains

253
00:13:03,990 --> 00:13:09,690
tennis ball by 76% but if you increase

254
00:13:07,320 --> 00:13:12,540
the brightness too much it switch

255
00:13:09,690 --> 00:13:14,100
switches back to its original class so

256
00:13:12,540 --> 00:13:16,260
the reason I'm showing you all these is

257
00:13:14,100 --> 00:13:19,110
that okay adversarial images are scary

258
00:13:16,260 --> 00:13:21,569
but if they cannot withstand change of

259
00:13:19,110 --> 00:13:24,329
brightness cropping magnification how

260
00:13:21,570 --> 00:13:27,180
can they exist in the real world because

261
00:13:24,329 --> 00:13:29,760
in the real world yourself Ivan cars are

262
00:13:27,180 --> 00:13:32,579
not gonna look at your images at only

263
00:13:29,760 --> 00:13:35,370
one angle or only one size so it's gonna

264
00:13:32,579 --> 00:13:41,160
be a variety of sizes or variety of

265
00:13:35,370 --> 00:13:43,890
lighting conditions I'm gonna skip over

266
00:13:41,160 --> 00:13:46,380
this so again coming back to the point

267
00:13:43,890 --> 00:13:48,630
that can add versatile images actually

268
00:13:46,380 --> 00:13:54,600
full self-driving cars and detectors

269
00:13:48,630 --> 00:13:58,110
answer's no at the moment no and you

270
00:13:54,600 --> 00:14:00,209
know a simple illustration here so this

271
00:13:58,110 --> 00:14:03,060
is the original image is the adversarial

272
00:14:00,209 --> 00:14:05,670
image and I'm not sure if it is large

273
00:14:03,060 --> 00:14:08,729
enough but it's still classified as both

274
00:14:05,670 --> 00:14:12,449
of them as a person and the reason being

275
00:14:08,730 --> 00:14:14,430
is that when you have some like these

276
00:14:12,449 --> 00:14:17,040
croppings so for example this bounding

277
00:14:14,430 --> 00:14:19,649
box is a cropping operation so it loses

278
00:14:17,040 --> 00:14:21,510
this adversarial property so at the

279
00:14:19,649 --> 00:14:24,480
moment detectors and self-driving cars

280
00:14:21,510 --> 00:14:29,550
are not susceptible to adverse sale

281
00:14:24,480 --> 00:14:32,370
examples but still your your simple

282
00:14:29,550 --> 00:14:35,339
algorithms are still very susceptible so

283
00:14:32,370 --> 00:14:37,980
the official recognition people the

284
00:14:35,339 --> 00:14:40,589
researchers were able to fool iPhones

285
00:14:37,980 --> 00:14:44,220
facial recognition models by my

286
00:14:40,589 --> 00:14:47,820
adversary images is training on these

287
00:14:44,220 --> 00:14:51,750
images a solution it might be but it's

288
00:14:47,820 --> 00:14:54,089
it's too bulky you need to generate too

289
00:14:51,750 --> 00:14:56,790
many corner cases it's like testing a

290
00:14:54,089 --> 00:14:59,130
code if you're a developer you need to

291
00:14:56,790 --> 00:15:01,230
find all the extreme cases but what if

292
00:14:59,130 --> 00:15:02,910
you have a thousand extreme cases so it

293
00:15:01,230 --> 00:15:07,290
becomes kind of cumbersome to find all

294
00:15:02,910 --> 00:15:09,360
those extreme cases I guess so yeah

295
00:15:07,290 --> 00:15:16,290
adversarial images are still a very

296
00:15:09,360 --> 00:15:18,600
prominent problem yeah it's still a very

297
00:15:16,290 --> 00:15:22,949
common problem people are working on a

298
00:15:18,600 --> 00:15:24,990
lot of defensive strategies and people

299
00:15:22,950 --> 00:15:28,440
are also working on extending these two

300
00:15:24,990 --> 00:15:31,790
detectors as I showed you for the

301
00:15:28,440 --> 00:15:34,180
cropping operations and stuff that's it

302
00:15:31,790 --> 00:15:39,209
thank you

303
00:15:34,180 --> 00:15:42,050
[Applause]

304
00:15:39,209 --> 00:15:42,050
any questions

305
00:15:52,269 --> 00:16:04,010
so the way it works is if you see the

306
00:16:00,050 --> 00:16:06,649
Sigma the epsilon term here which is

307
00:16:04,010 --> 00:16:08,269
zero point zero seven so that term

308
00:16:06,649 --> 00:16:10,670
actually determines how much you are

309
00:16:08,269 --> 00:16:13,070
perturbing your image so it kind of

310
00:16:10,670 --> 00:16:15,079
becomes like a beam search you wanna you

311
00:16:13,070 --> 00:16:17,269
want to have a balance between how much

312
00:16:15,079 --> 00:16:19,250
you want to perturb your image so it's a

313
00:16:17,269 --> 00:16:21,560
it's a strong perturbation strong miss

314
00:16:19,250 --> 00:16:23,300
classification but you also don't want

315
00:16:21,560 --> 00:16:27,229
to perturb it too much otherwise it

316
00:16:23,300 --> 00:16:29,389
becomes perceptible to the human eye so

317
00:16:27,230 --> 00:16:31,310
the more you increase the epsilon the

318
00:16:29,389 --> 00:16:45,350
computation increases but there's a

319
00:16:31,310 --> 00:16:47,959
trade-off I in fact used all the

320
00:16:45,350 --> 00:16:50,570
possible networks and one one

321
00:16:47,959 --> 00:16:52,010
interesting fact is when you so if

322
00:16:50,570 --> 00:16:54,290
you're if you're familiar with most of

323
00:16:52,010 --> 00:16:55,579
the image recognition models so another

324
00:16:54,290 --> 00:16:57,139
interesting thing about adversarial

325
00:16:55,579 --> 00:16:58,880
image is that they don't transfer very

326
00:16:57,139 --> 00:17:01,399
well so for example if you use a bgg

327
00:16:58,880 --> 00:17:03,529
based network and it resonate if you

328
00:17:01,399 --> 00:17:05,929
generate from ouija they don't fool the

329
00:17:03,529 --> 00:17:08,540
ResNet and so there's some

330
00:17:05,929 --> 00:17:11,120
transferability issues out there this is

331
00:17:08,540 --> 00:17:14,000
really cool library by Google called

332
00:17:11,119 --> 00:17:16,188
clever hands it's maintained by one of

333
00:17:14,000 --> 00:17:19,540
their their security staff and that's

334
00:17:16,189 --> 00:17:19,540
that's really cool that's what I used

335
00:17:20,740 --> 00:17:27,269
ok thank you

336
00:17:23,990 --> 00:17:27,269
[Applause]

