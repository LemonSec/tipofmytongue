1
00:00:00,880 --> 00:00:03,040
hello everyone my name is eid marwaina

2
00:00:03,040 --> 00:00:04,560
i'm from the technical university of

3
00:00:04,560 --> 00:00:06,640
darmstadt germany and today i will

4
00:00:06,640 --> 00:00:09,599
present you um the user level label

5
00:00:09,599 --> 00:00:11,920
leakage from gradients in federated

6
00:00:11,920 --> 00:00:14,240
learning which is a collaboration work

7
00:00:14,240 --> 00:00:15,200
um

8
00:00:15,200 --> 00:00:17,600
between privacy and trust for mobile

9
00:00:17,600 --> 00:00:18,640
users

10
00:00:18,640 --> 00:00:20,320
uh research training group and teleco

11
00:00:20,320 --> 00:00:22,080
operation lab and also

12
00:00:22,080 --> 00:00:24,080
the artificial intelligence and machine

13
00:00:24,080 --> 00:00:26,480
learning um lab

14
00:00:26,480 --> 00:00:28,560
so let's start from the end of my title

15
00:00:28,560 --> 00:00:30,240
so what is federated learning so

16
00:00:30,240 --> 00:00:31,920
federated learning the machine learning

17
00:00:31,920 --> 00:00:35,680
technique was proposed by mcmahon 2016

18
00:00:35,680 --> 00:00:38,800
um to train a collaborative

19
00:00:38,800 --> 00:00:41,440
model actually under the

20
00:00:41,440 --> 00:00:44,000
coordination of a central server and the

21
00:00:44,000 --> 00:00:46,879
process starts with the server sends an

22
00:00:46,879 --> 00:00:48,000
initial

23
00:00:48,000 --> 00:00:50,559
model to the users who will train the

24
00:00:50,559 --> 00:00:52,800
model locally on their own

25
00:00:52,800 --> 00:00:54,960
data and send back the gradients of this

26
00:00:54,960 --> 00:00:56,640
model to the server the server

27
00:00:56,640 --> 00:00:58,559
aggregates all these gradients and

28
00:00:58,559 --> 00:01:01,840
obtain obtains their uh updated global

29
00:01:01,840 --> 00:01:03,920
model and send it back again to the

30
00:01:03,920 --> 00:01:05,840
users and we go through multiple

31
00:01:05,840 --> 00:01:07,600
iterations

32
00:01:07,600 --> 00:01:08,720
till we

33
00:01:08,720 --> 00:01:10,240
got get the

34
00:01:10,240 --> 00:01:12,799
model converged

35
00:01:12,799 --> 00:01:15,600
and actually the main privacy advantage

36
00:01:15,600 --> 00:01:17,600
of this technique that the data of the

37
00:01:17,600 --> 00:01:20,400
users never leave on their own on

38
00:01:20,400 --> 00:01:23,600
devices so it's not shared the users do

39
00:01:23,600 --> 00:01:25,680
not share their data with the server but

40
00:01:25,680 --> 00:01:27,759
they are only sharing their

41
00:01:27,759 --> 00:01:31,040
model um gradients or updates and if you

42
00:01:31,040 --> 00:01:33,040
are talking about neural networks model

43
00:01:33,040 --> 00:01:35,439
so here we are the gradients actually

44
00:01:35,439 --> 00:01:37,920
are the derivative of the loss function

45
00:01:37,920 --> 00:01:40,479
with regard to the weights which located

46
00:01:40,479 --> 00:01:45,200
on every edge in our neural network

47
00:01:46,159 --> 00:01:50,240
now the question was how risky is to it

48
00:01:50,240 --> 00:01:52,960
is to um share the gradients with the

49
00:01:52,960 --> 00:01:56,000
server so if the adversary

50
00:01:56,000 --> 00:01:59,200
acquired or got this

51
00:01:59,200 --> 00:02:01,119
gradient so how much information they

52
00:02:01,119 --> 00:02:04,240
can learn about our labels or our

53
00:02:04,240 --> 00:02:06,240
training data

54
00:02:06,240 --> 00:02:07,439
and in this work actually we are

55
00:02:07,439 --> 00:02:10,639
focusing on on the labels and the labels

56
00:02:10,639 --> 00:02:12,560
actually we are referring to the piece

57
00:02:12,560 --> 00:02:14,879
of information which describe

58
00:02:14,879 --> 00:02:16,640
our training data so if we are talking

59
00:02:16,640 --> 00:02:17,520
about

60
00:02:17,520 --> 00:02:19,520
classification tasks here for example so

61
00:02:19,520 --> 00:02:21,599
the image would be

62
00:02:21,599 --> 00:02:23,120
our data and

63
00:02:23,120 --> 00:02:25,440
the cat or the label cat is

64
00:02:25,440 --> 00:02:29,120
um the label however the labels can be

65
00:02:29,120 --> 00:02:30,959
actually quite sensitive so in in

66
00:02:30,959 --> 00:02:33,120
medical application for example so the

67
00:02:33,120 --> 00:02:34,959
data can be a medical record of some

68
00:02:34,959 --> 00:02:38,640
patient and the label can might be fair

69
00:02:38,640 --> 00:02:41,440
or might refer to to the diseases of

70
00:02:41,440 --> 00:02:44,079
this patient

71
00:02:44,319 --> 00:02:46,400
so there are some works already in the

72
00:02:46,400 --> 00:02:48,080
related work where

73
00:02:48,080 --> 00:02:50,800
they are actually investigating the

74
00:02:50,800 --> 00:02:53,200
leakage of the labels from

75
00:02:53,200 --> 00:02:55,280
the gradients however and these works

76
00:02:55,280 --> 00:02:57,519
actually they are focusing only on the

77
00:02:57,519 --> 00:03:00,560
case where the batch side of the batch

78
00:03:00,560 --> 00:03:02,080
are of

79
00:03:02,080 --> 00:03:05,360
small sizes and in our collaboration or

80
00:03:05,360 --> 00:03:07,040
on our contribution

81
00:03:07,040 --> 00:03:07,840
we

82
00:03:07,840 --> 00:03:10,959
went um after arbitrary patch size so we

83
00:03:10,959 --> 00:03:12,879
want to investigate this leakage when we

84
00:03:12,879 --> 00:03:14,879
have different patch sizes

85
00:03:14,879 --> 00:03:17,760
and we were able to identify two

86
00:03:17,760 --> 00:03:19,040
gradient

87
00:03:19,040 --> 00:03:22,720
properties that expose the labels and we

88
00:03:22,720 --> 00:03:25,680
exploited these properties to perform

89
00:03:25,680 --> 00:03:28,319
our attack

90
00:03:28,480 --> 00:03:30,480
so what are these properties so that

91
00:03:30,480 --> 00:03:32,400
properties actually are found or were

92
00:03:32,400 --> 00:03:35,360
found in the gradients of the last layer

93
00:03:35,360 --> 00:03:37,840
of a neural network so

94
00:03:37,840 --> 00:03:40,560
what right before the output

95
00:03:40,560 --> 00:03:42,879
layer

96
00:03:44,159 --> 00:03:46,959
and the first property says that if

97
00:03:46,959 --> 00:03:49,120
the gradient is negative

98
00:03:49,120 --> 00:03:51,280
that means the corresponding label

99
00:03:51,280 --> 00:03:52,480
exists

100
00:03:52,480 --> 00:03:54,560
um exists in

101
00:03:54,560 --> 00:03:58,159
the batch so if the gradient of the cat

102
00:03:58,159 --> 00:04:00,799
here is negative that means

103
00:04:00,799 --> 00:04:03,280
sorry definitely there is a cat image in

104
00:04:03,280 --> 00:04:07,519
the patch or in the training patch

105
00:04:07,920 --> 00:04:09,360
and we proved this property

106
00:04:09,360 --> 00:04:10,799
mathematically and it's actually

107
00:04:10,799 --> 00:04:13,360
generate generalization of

108
00:04:13,360 --> 00:04:17,279
of um another finding in our from our

109
00:04:17,279 --> 00:04:19,440
related work

110
00:04:19,440 --> 00:04:20,639
and

111
00:04:20,639 --> 00:04:23,680
the question remains here is how many

112
00:04:23,680 --> 00:04:25,919
cats are there

113
00:04:25,919 --> 00:04:28,000
and to figure this out we derived

114
00:04:28,000 --> 00:04:29,440
mathematic

115
00:04:29,440 --> 00:04:30,880
the mathematical

116
00:04:30,880 --> 00:04:33,120
expression of the gradient and we showed

117
00:04:33,120 --> 00:04:36,240
that it indeed relates to the number of

118
00:04:36,240 --> 00:04:38,720
occurrences of of the label which is

119
00:04:38,720 --> 00:04:40,560
lambda cat here

120
00:04:40,560 --> 00:04:43,360
and also relate to the batch size to the

121
00:04:43,360 --> 00:04:46,000
prediction scores and to the output of

122
00:04:46,000 --> 00:04:47,680
the activation function in the previous

123
00:04:47,680 --> 00:04:48,639
slide

124
00:04:48,639 --> 00:04:50,800
and the last actually two components are

125
00:04:50,800 --> 00:04:52,720
not accessible by an adversary because

126
00:04:52,720 --> 00:04:55,280
they are intermediate um output of the

127
00:04:55,280 --> 00:04:58,960
calculations happened on the user

128
00:04:58,960 --> 00:05:02,960
device while training the model

129
00:05:02,960 --> 00:05:04,960
however we noticed

130
00:05:04,960 --> 00:05:09,120
we noticed um in our empirical analysis

131
00:05:09,120 --> 00:05:11,440
that the sum of the prediction scores is

132
00:05:11,440 --> 00:05:13,039
close to zero when the model is

133
00:05:13,039 --> 00:05:14,720
untrained

134
00:05:14,720 --> 00:05:17,039
and that we can express the gradient

135
00:05:17,039 --> 00:05:20,800
value as a linear equation of the lambda

136
00:05:20,800 --> 00:05:21,759
where

137
00:05:21,759 --> 00:05:25,120
we define m as the impact which is a

138
00:05:25,120 --> 00:05:27,520
change of the gradient value caused by

139
00:05:27,520 --> 00:05:30,320
one occurrence of the label

140
00:05:30,320 --> 00:05:32,720
based on this observation we formulated

141
00:05:32,720 --> 00:05:34,639
our second property

142
00:05:34,639 --> 00:05:39,840
that is in untrained model the gradients

143
00:05:39,840 --> 00:05:41,840
are approximately proportional to the

144
00:05:41,840 --> 00:05:44,800
number of occurrences

145
00:05:44,800 --> 00:05:47,280
additionally we notice that the gradient

146
00:05:47,280 --> 00:05:50,240
is shifted sometimes with a positive

147
00:05:50,240 --> 00:05:53,680
value we call it offset

148
00:05:53,680 --> 00:05:55,759
we explain this shift as a result of

149
00:05:55,759 --> 00:05:57,280
misclassification

150
00:05:57,280 --> 00:06:02,479
penalty so if we pass a dog image

151
00:06:02,479 --> 00:06:05,199
to the model in the training phase the

152
00:06:05,199 --> 00:06:07,919
and the model misclassifies this image

153
00:06:07,919 --> 00:06:10,560
as a cat the gradient of the cat will

154
00:06:10,560 --> 00:06:13,759
receive the shift or this penalty

155
00:06:13,759 --> 00:06:14,560
now

156
00:06:14,560 --> 00:06:16,080
we know that

157
00:06:16,080 --> 00:06:19,199
if we know the impact and the offset we

158
00:06:19,199 --> 00:06:21,680
can derive then

159
00:06:21,680 --> 00:06:23,280
lambda

160
00:06:23,280 --> 00:06:24,960
so we propose

161
00:06:24,960 --> 00:06:27,840
uh heuristic methods to estimate the

162
00:06:27,840 --> 00:06:30,000
impact and the offset and there are

163
00:06:30,000 --> 00:06:31,680
three scenarios i will present two of

164
00:06:31,680 --> 00:06:33,199
them today

165
00:06:33,199 --> 00:06:35,360
first scenario where the adversary has

166
00:06:35,360 --> 00:06:36,880
access to the

167
00:06:36,880 --> 00:06:37,919
shared

168
00:06:37,919 --> 00:06:40,240
gradients and here we estimate the

169
00:06:40,240 --> 00:06:43,120
impact as an average of the negative

170
00:06:43,120 --> 00:06:44,319
gradient

171
00:06:44,319 --> 00:06:46,880
because we know that these gradients

172
00:06:46,880 --> 00:06:48,880
correspond to existing

173
00:06:48,880 --> 00:06:51,599
um labels in in the batch

174
00:06:51,599 --> 00:06:53,759
and we found also imperfectly that

175
00:06:53,759 --> 00:06:56,160
multiplying with with the factor of one

176
00:06:56,160 --> 00:06:59,440
plus one over n improves the estimation

177
00:06:59,440 --> 00:07:00,639
um

178
00:07:00,639 --> 00:07:02,720
of the impact where n is the total

179
00:07:02,720 --> 00:07:05,039
number of classes

180
00:07:05,039 --> 00:07:06,479
and in the second

181
00:07:06,479 --> 00:07:08,560
scenario the adversary has access to the

182
00:07:08,560 --> 00:07:11,199
gradients and the data set with the same

183
00:07:11,199 --> 00:07:15,039
um classes as the original one

184
00:07:15,039 --> 00:07:18,160
here we can estimate the offset also

185
00:07:18,160 --> 00:07:20,560
of of the cats by producing

186
00:07:20,560 --> 00:07:22,960
misclassification cases

187
00:07:22,960 --> 00:07:26,080
we do that by passing patches

188
00:07:26,080 --> 00:07:29,280
full of labels other than the cats

189
00:07:29,280 --> 00:07:32,720
we can we do that for several times and

190
00:07:32,720 --> 00:07:36,160
take the average of the cat gradient

191
00:07:36,160 --> 00:07:37,120
now

192
00:07:37,120 --> 00:07:39,199
based on these

193
00:07:39,199 --> 00:07:40,639
parameters

194
00:07:40,639 --> 00:07:43,759
we proposed an algorithm to extract the

195
00:07:43,759 --> 00:07:46,319
labels and i invite you to have a look

196
00:07:46,319 --> 00:07:49,120
at the paper in order to see um

197
00:07:49,120 --> 00:07:51,039
details of the algorithm

198
00:07:51,039 --> 00:07:52,560
now we evaluated our attack by

199
00:07:52,560 --> 00:07:54,319
conducting an extensive set of

200
00:07:54,319 --> 00:07:56,240
experiments that i will have part i will

201
00:07:56,240 --> 00:07:59,199
highlight part of them we use um the

202
00:07:59,199 --> 00:08:00,400
success

203
00:08:00,400 --> 00:08:01,919
rate to add a metric to measure the

204
00:08:01,919 --> 00:08:03,520
effectiveness of our attack and we

205
00:08:03,520 --> 00:08:05,840
looked on two different path sizes data

206
00:08:05,840 --> 00:08:07,680
sets machine federated learning

207
00:08:07,680 --> 00:08:10,080
algorithms and model architectures today

208
00:08:10,080 --> 00:08:12,000
i will show the results of the of a

209
00:08:12,000 --> 00:08:13,680
basic scene and

210
00:08:13,680 --> 00:08:16,879
model and also we started the attack

211
00:08:16,879 --> 00:08:18,960
under different convergence status and

212
00:08:18,960 --> 00:08:21,280
defense mechanisms and we compared with

213
00:08:21,280 --> 00:08:23,280
the random guess and duplication from

214
00:08:23,280 --> 00:08:25,120
attack as baselines

215
00:08:25,120 --> 00:08:27,520
so again we test our attack under two

216
00:08:27,520 --> 00:08:29,280
scenarios the first where we are

217
00:08:29,280 --> 00:08:30,960
estimating the impact and the second

218
00:08:30,960 --> 00:08:33,039
where we are estimating both impact and

219
00:08:33,039 --> 00:08:34,479
offsets

220
00:08:34,479 --> 00:08:36,479
and looking at the different batch sizes

221
00:08:36,479 --> 00:08:38,799
and data sets we can see for any status

222
00:08:38,799 --> 00:08:40,640
set or attack under the auxiliary

223
00:08:40,640 --> 00:08:43,200
knowledge actually outperforms the dlg

224
00:08:43,200 --> 00:08:44,240
and

225
00:08:44,240 --> 00:08:46,320
we see that all the attacks actually are

226
00:08:46,320 --> 00:08:49,680
effective and having um rates way higher

227
00:08:49,680 --> 00:08:51,760
than the random gas

228
00:08:51,760 --> 00:08:54,240
on the cfar data set we can where we

229
00:08:54,240 --> 00:08:57,760
have 100 um classes and more complex

230
00:08:57,760 --> 00:09:00,320
images our attack performs quite good

231
00:09:00,320 --> 00:09:03,600
under both uh both scenarios and finally

232
00:09:03,600 --> 00:09:05,600
for syllab a data set where the images

233
00:09:05,600 --> 00:09:07,920
are of higher dimensions we see that the

234
00:09:07,920 --> 00:09:10,320
dlg degrades significantly while our

235
00:09:10,320 --> 00:09:13,200
attacks still perform more than 95

236
00:09:13,200 --> 00:09:15,120
percent for the auxiliary knowledge

237
00:09:15,120 --> 00:09:16,640
scenario

238
00:09:16,640 --> 00:09:18,399
it's known that when the model is more

239
00:09:18,399 --> 00:09:20,640
trained the gradients are

240
00:09:20,640 --> 00:09:22,720
less informative so we studied our

241
00:09:22,720 --> 00:09:24,560
attack under different model convergence

242
00:09:24,560 --> 00:09:27,120
status and we can see in this experiment

243
00:09:27,120 --> 00:09:29,519
that increasing the the model accuracy

244
00:09:29,519 --> 00:09:31,600
which is in black here

245
00:09:31,600 --> 00:09:34,959
actually led uh led to a decrease in in

246
00:09:34,959 --> 00:09:37,440
our attack success rate however even

247
00:09:37,440 --> 00:09:40,080
with the model accuracy of 90 percent

248
00:09:40,080 --> 00:09:43,440
our attack um success rate are still

249
00:09:43,440 --> 00:09:46,560
more than 60 percent and uh performing

250
00:09:46,560 --> 00:09:48,480
better than the random cause i guess

251
00:09:48,480 --> 00:09:51,040
which is ndk which indicates that the

252
00:09:51,040 --> 00:09:54,719
information leakage still exists

253
00:09:55,360 --> 00:09:56,959
finally to mitigate our attack we

254
00:09:56,959 --> 00:10:00,480
investigated three defenses on the user

255
00:10:00,480 --> 00:10:03,040
side so first we added gaussian noise to

256
00:10:03,040 --> 00:10:04,480
the gradient before sharing them with

257
00:10:04,480 --> 00:10:06,160
the server and we tried different

258
00:10:06,160 --> 00:10:08,000
standard deviation as we can see the

259
00:10:08,000 --> 00:10:10,399
attack success rate decreased but it's

260
00:10:10,399 --> 00:10:12,959
still effective better and better than

261
00:10:12,959 --> 00:10:15,440
the random with differential privacy we

262
00:10:15,440 --> 00:10:17,519
clipped the gradients and then we added

263
00:10:17,519 --> 00:10:20,000
noise we tried different clipping norm

264
00:10:20,000 --> 00:10:22,720
uh bounds better and for better one we

265
00:10:22,720 --> 00:10:24,480
can see that that tag is not effective

266
00:10:24,480 --> 00:10:27,920
especially for batch sizes more than 16

267
00:10:27,920 --> 00:10:29,680
however this defense

268
00:10:29,680 --> 00:10:31,600
also led to a remarkable drop in the

269
00:10:31,600 --> 00:10:34,120
model accuracy from

270
00:10:34,120 --> 00:10:38,320
1993 to 52. finally we tried

271
00:10:38,320 --> 00:10:41,040
um gradient compression technique where

272
00:10:41,040 --> 00:10:44,000
the small gradients are pruned to zeros

273
00:10:44,000 --> 00:10:47,040
with different um teta ratios

274
00:10:47,040 --> 00:10:49,760
we see that the theta 80

275
00:10:49,760 --> 00:10:51,600
can mitigate our attack while

276
00:10:51,600 --> 00:10:54,800
maintaining high accuracy of 92.

277
00:10:54,800 --> 00:10:56,640
we conclude that this defense is

278
00:10:56,640 --> 00:10:59,279
effective against our attack

279
00:10:59,279 --> 00:11:01,120
by that i would like to conclude my talk

280
00:11:01,120 --> 00:11:02,720
where we showed that

281
00:11:02,720 --> 00:11:06,240
um the gradient sign and magnitude can

282
00:11:06,240 --> 00:11:08,560
indicate the ground truth labels and we

283
00:11:08,560 --> 00:11:10,959
propose an attack to exploit or to

284
00:11:10,959 --> 00:11:13,200
exploit these properties and extract

285
00:11:13,200 --> 00:11:15,839
these labels and our evaluation showed

286
00:11:15,839 --> 00:11:17,440
that our attack is effective under

287
00:11:17,440 --> 00:11:19,760
different path sizes and number of

288
00:11:19,760 --> 00:11:21,120
classes and

289
00:11:21,120 --> 00:11:23,200
model architectures and also training

290
00:11:23,200 --> 00:11:26,880
stages and our also evaluation suggests

291
00:11:26,880 --> 00:11:28,640
that gradient compression is an

292
00:11:28,640 --> 00:11:30,800
effective defense by that i would like

293
00:11:30,800 --> 00:11:33,200
to end my talk and thank you very much

294
00:11:33,200 --> 00:11:37,160
for your attention

