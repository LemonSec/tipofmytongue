1
00:00:02,639 --> 00:00:04,720
hello everyone i'm venue

2
00:00:04,720 --> 00:00:07,520
uh i'm presenting a three-legged rest

3
00:00:07,520 --> 00:00:10,080
uh it is uh privacy preserving the

4
00:00:10,080 --> 00:00:11,599
entrance method

5
00:00:11,599 --> 00:00:14,559
that can leverage both keys and gpus to

6
00:00:14,559 --> 00:00:15,679
achieve

7
00:00:15,679 --> 00:00:17,520
privacy protection and also the

8
00:00:17,520 --> 00:00:20,240
computing performance

9
00:00:20,240 --> 00:00:22,080
yeah so just a little background about

10
00:00:22,080 --> 00:00:25,119
myself i'm a fourth year phd student

11
00:00:25,119 --> 00:00:27,920
from university of southern california

12
00:00:27,920 --> 00:00:30,960
and my research topics covers various

13
00:00:30,960 --> 00:00:33,040
aspects about you know

14
00:00:33,040 --> 00:00:35,440
privacy privacy protecting in you know

15
00:00:35,440 --> 00:00:37,920
in previous protection

16
00:00:37,920 --> 00:00:41,120
in machine learning training

17
00:00:41,920 --> 00:00:44,000
yeah so before i you know before i

18
00:00:44,000 --> 00:00:47,280
discussing the details about the pepper

19
00:00:47,280 --> 00:00:48,719
let me spend a few minutes on the

20
00:00:48,719 --> 00:00:51,199
motivation

21
00:00:51,199 --> 00:00:52,879
about the work

22
00:00:52,879 --> 00:00:54,480
so yeah so if you see the machine

23
00:00:54,480 --> 00:00:56,719
learning from that privacy protective

24
00:00:56,719 --> 00:00:59,280
you know perspective we can you know

25
00:00:59,280 --> 00:01:01,359
categorize the machine learning in two

26
00:01:01,359 --> 00:01:04,080
classes the first one is using the gpus

27
00:01:04,080 --> 00:01:06,560
and tpus to achieve a strong computing

28
00:01:06,560 --> 00:01:07,760
performance

29
00:01:07,760 --> 00:01:10,479
but if this you know platform like like

30
00:01:10,479 --> 00:01:12,640
sufficient privacy protection

31
00:01:12,640 --> 00:01:14,400
on the data

32
00:01:14,400 --> 00:01:16,640
so attackers can easily get the

33
00:01:16,640 --> 00:01:19,520
sensitive information about the dataset

34
00:01:19,520 --> 00:01:21,759
and another classes is the you know

35
00:01:21,759 --> 00:01:25,439
trusted execution environment cortese

36
00:01:25,439 --> 00:01:27,600
you know by providing a very strong in a

37
00:01:27,600 --> 00:01:30,400
secure environment that he's can give a

38
00:01:30,400 --> 00:01:32,799
very strong you know privacy guarantee

39
00:01:32,799 --> 00:01:35,200
and it is very difficult for the

40
00:01:35,200 --> 00:01:36,960
attackers to access the you know

41
00:01:36,960 --> 00:01:38,560
sensitive information

42
00:01:38,560 --> 00:01:41,360
and during transmission of the data or

43
00:01:41,360 --> 00:01:42,880
you know the runtime in the te

44
00:01:42,880 --> 00:01:44,240
environment

45
00:01:44,240 --> 00:01:46,560
but the cost comes with the privacy

46
00:01:46,560 --> 00:01:48,320
guarantees the you know very low

47
00:01:48,320 --> 00:01:50,399
computing performance compared to the

48
00:01:50,399 --> 00:01:52,960
you know the gpu or tpu

49
00:01:52,960 --> 00:01:55,119
platform

50
00:01:55,119 --> 00:01:57,200
yeah so the problem setting in this

51
00:01:57,200 --> 00:02:00,079
paper is that can we combine gpus and

52
00:02:00,079 --> 00:02:02,479
keys to achieve both performance and

53
00:02:02,479 --> 00:02:05,520
privacy guarantees

54
00:02:05,520 --> 00:02:07,600
yes let me you know provide a little

55
00:02:07,600 --> 00:02:10,318
more context here in terms of the model

56
00:02:10,318 --> 00:02:13,120
in france in terms of the in terms of

57
00:02:13,120 --> 00:02:14,560
the privacy preserving modern

58
00:02:14,560 --> 00:02:16,640
infrastructure training so for the

59
00:02:16,640 --> 00:02:18,480
private model influence

60
00:02:18,480 --> 00:02:21,040
uh we have uh you know very

61
00:02:21,040 --> 00:02:23,520
commonly you have very famous algorithm

62
00:02:23,520 --> 00:02:26,400
called slalom so the basic idea in islam

63
00:02:26,400 --> 00:02:27,520
is that

64
00:02:27,520 --> 00:02:29,920
in these it add noise to the vision data

65
00:02:29,920 --> 00:02:33,360
and send noisy data to the gpus the gpu

66
00:02:33,360 --> 00:02:35,040
is computing the you know the

67
00:02:35,040 --> 00:02:36,959
convolution operations and have this

68
00:02:36,959 --> 00:02:39,440
noisy output and sending this noisy

69
00:02:39,440 --> 00:02:42,640
output to the keys and the keys can get

70
00:02:42,640 --> 00:02:44,560
the original output by decrypting the

71
00:02:44,560 --> 00:02:46,319
noise output

72
00:02:46,319 --> 00:02:48,239
yeah so it's very private in terms of

73
00:02:48,239 --> 00:02:51,040
model influence but the issue here is

74
00:02:51,040 --> 00:02:53,920
that slalom does not support inner model

75
00:02:53,920 --> 00:02:55,599
training which is a more computation

76
00:02:55,599 --> 00:02:58,560
intensive task

77
00:02:58,959 --> 00:03:01,360
yeah so for model training this you know

78
00:03:01,360 --> 00:03:04,000
other method like in know private ml or

79
00:03:04,000 --> 00:03:07,280
dark knight so all of this method have

80
00:03:07,280 --> 00:03:09,040
you know all of this method that they're

81
00:03:09,040 --> 00:03:11,120
adopting in a multi-party computing

82
00:03:11,120 --> 00:03:14,239
magazine mechanism that you use you know

83
00:03:14,239 --> 00:03:17,040
jointly you know collaborating nodes to

84
00:03:17,040 --> 00:03:18,239
support his

85
00:03:18,239 --> 00:03:20,319
privacy protecting

86
00:03:20,319 --> 00:03:21,360
so the

87
00:03:21,360 --> 00:03:23,760
yes we assume that they have you know

88
00:03:23,760 --> 00:03:24,959
non including

89
00:03:24,959 --> 00:03:27,680
you know gpu knows that keying you know

90
00:03:27,680 --> 00:03:30,480
protected the data but assumption the

91
00:03:30,480 --> 00:03:32,799
assumption of the noncloning nodes

92
00:03:32,799 --> 00:03:35,680
might feel might fail in some cases so

93
00:03:35,680 --> 00:03:37,280
if this assumption

94
00:03:37,280 --> 00:03:38,239
fails

95
00:03:38,239 --> 00:03:40,239
any single nodes can reconstruct the

96
00:03:40,239 --> 00:03:41,760
data

97
00:03:41,760 --> 00:03:43,920
so the you know the privacy protecting

98
00:03:43,920 --> 00:03:48,480
is actually significantly compromised

99
00:03:49,440 --> 00:03:51,599
yeah so here we are proposing a method

100
00:03:51,599 --> 00:03:53,760
to you know to leverage that you know

101
00:03:53,760 --> 00:03:56,239
the the keys and gpus to achieve both

102
00:03:56,239 --> 00:03:57,840
privacy protecting and also the

103
00:03:57,840 --> 00:04:00,239
computing performance

104
00:04:00,239 --> 00:04:04,319
the basic idea in the method small is

105
00:04:04,319 --> 00:04:06,400
our method is like

106
00:04:06,400 --> 00:04:08,000
we first

107
00:04:08,000 --> 00:04:10,799
have a symmetric you know decomposition

108
00:04:10,799 --> 00:04:12,799
framework to decompose the data into

109
00:04:12,799 --> 00:04:15,519
local lowering part and the residuals

110
00:04:15,519 --> 00:04:18,000
the lowering part has most information

111
00:04:18,000 --> 00:04:19,600
about in the dataset but if the

112
00:04:19,600 --> 00:04:21,440
computation involving the lower

113
00:04:21,440 --> 00:04:23,600
important is very light on the other

114
00:04:23,600 --> 00:04:27,040
hand the residual csi is not allowing

115
00:04:27,040 --> 00:04:29,040
but it has less information about it

116
00:04:29,040 --> 00:04:30,800
that is it

117
00:04:30,800 --> 00:04:32,479
so yeah so most information you know

118
00:04:32,479 --> 00:04:35,199
computations is actually offloaded to

119
00:04:35,199 --> 00:04:37,440
not to the residuals

120
00:04:37,440 --> 00:04:40,240
so then we accordingly decompose the

121
00:04:40,240 --> 00:04:42,639
model into two parts the first part is

122
00:04:42,639 --> 00:04:45,600
performed in keys and we did the long

123
00:04:45,600 --> 00:04:47,199
part xd

124
00:04:47,199 --> 00:04:49,280
and as second part is performed in the

125
00:04:49,280 --> 00:04:54,559
gpus and avoided the the residual sexu

126
00:04:55,280 --> 00:04:57,199
and if we take a you know a closer view

127
00:04:57,199 --> 00:04:59,280
about sml

128
00:04:59,280 --> 00:05:02,160
the asymmetric data decomposition

129
00:05:02,160 --> 00:05:04,880
happens in the convolutional layers

130
00:05:04,880 --> 00:05:07,759
so we first decompose the data x

131
00:05:07,759 --> 00:05:10,880
into xt and xu and then we have the you

132
00:05:10,880 --> 00:05:13,520
know the convolution t in the t used to

133
00:05:13,520 --> 00:05:16,160
read the xd to the you know secure

134
00:05:16,160 --> 00:05:18,639
conversions in these

135
00:05:18,639 --> 00:05:20,240
and for xu we have you know

136
00:05:20,240 --> 00:05:23,039
convolutional in gpus to read the x unit

137
00:05:23,039 --> 00:05:24,479
and you know do the conversion

138
00:05:24,479 --> 00:05:26,400
operations

139
00:05:26,400 --> 00:05:28,479
so after we get it that would weight in

140
00:05:28,479 --> 00:05:30,800
you we combine that put in these and

141
00:05:30,800 --> 00:05:33,520
apply the nonlinear operations

142
00:05:33,520 --> 00:05:35,680
and before proceeding to the next

143
00:05:35,680 --> 00:05:37,919
convolution layers we will do the

144
00:05:37,919 --> 00:05:40,160
asymmetric decomposition again together

145
00:05:40,160 --> 00:05:44,000
the lowering part and the residuals

146
00:05:44,560 --> 00:05:46,400
yeah so here we summarize the you know

147
00:05:46,400 --> 00:05:48,160
the contributions in this paper the

148
00:05:48,160 --> 00:05:50,560
first contribution is that we develop

149
00:05:50,560 --> 00:05:52,400
this asymmetric data and model

150
00:05:52,400 --> 00:05:56,319
decomposition by using a lightweight svd

151
00:05:56,319 --> 00:05:57,759
algorithm

152
00:05:57,759 --> 00:06:00,720
so the computation complexity in these

153
00:06:00,720 --> 00:06:02,400
is you know

154
00:06:02,400 --> 00:06:04,560
a computational complexity is

155
00:06:04,560 --> 00:06:06,960
increasingly increases linearly with the

156
00:06:06,960 --> 00:06:08,880
number of number of you know principal

157
00:06:08,880 --> 00:06:11,280
channels in t

158
00:06:11,280 --> 00:06:13,440
and giving the you know the long data in

159
00:06:13,440 --> 00:06:15,680
the input lowering structure in inputs

160
00:06:15,680 --> 00:06:19,039
and uh and the intermediate activations

161
00:06:19,039 --> 00:06:20,960
the number for the number of principal

162
00:06:20,960 --> 00:06:23,039
channels in ts can be much smaller than

163
00:06:23,039 --> 00:06:25,039
the number of you know total input

164
00:06:25,039 --> 00:06:26,479
channels

165
00:06:26,479 --> 00:06:29,280
and if we take a more concrete example

166
00:06:29,280 --> 00:06:30,240
uh

167
00:06:30,240 --> 00:06:32,080
where the number of you know principal

168
00:06:32,080 --> 00:06:34,479
channels in t over the number of total

169
00:06:34,479 --> 00:06:38,080
channels is one over 16 we can see that

170
00:06:38,080 --> 00:06:40,720
the computational complexity is much

171
00:06:40,720 --> 00:06:43,120
indeed is much less than

172
00:06:43,120 --> 00:06:47,199
the complexity in gpus

173
00:06:47,280 --> 00:06:49,599
and the second contribution we have is

174
00:06:49,599 --> 00:06:52,240
we have uh is that we have a theoretical

175
00:06:52,240 --> 00:06:54,880
guarantee for the privacy so by adding

176
00:06:54,880 --> 00:06:58,160
noise to the residuals

177
00:06:58,160 --> 00:07:00,560
and sending the noisy uh you know

178
00:07:00,560 --> 00:07:03,199
residuals in the gpus we achieve the

179
00:07:03,199 --> 00:07:04,720
differential private

180
00:07:04,720 --> 00:07:08,560
uh you know protecting on the data and

181
00:07:08,560 --> 00:07:11,120
compared to you know the method that

182
00:07:11,120 --> 00:07:14,639
directly adds noise to the vision data

183
00:07:14,639 --> 00:07:17,440
the noisy recording smell is much lesser

184
00:07:17,440 --> 00:07:19,759
than you know the method that add noise

185
00:07:19,759 --> 00:07:22,240
to the residual data

186
00:07:22,240 --> 00:07:24,960
and add noise to a vision data

187
00:07:24,960 --> 00:07:26,800
so the third contribution we have is

188
00:07:26,800 --> 00:07:28,960
that we have a theoretical analysis on

189
00:07:28,960 --> 00:07:31,599
the luang structuring you know in models

190
00:07:31,599 --> 00:07:34,400
so yeah so to analyze the long structure

191
00:07:34,400 --> 00:07:37,120
we first defined the 3d a metric called

192
00:07:37,120 --> 00:07:39,360
svd channel entropy

193
00:07:39,360 --> 00:07:42,479
and then using this metric speedy chain

194
00:07:42,479 --> 00:07:44,560
entropy we measure the you know the

195
00:07:44,560 --> 00:07:46,000
entropy bond

196
00:07:46,000 --> 00:07:48,560
in the output after every you know

197
00:07:48,560 --> 00:07:51,280
operation in scenes

198
00:07:51,280 --> 00:07:53,280
yeah so for for commonly used operation

199
00:07:53,280 --> 00:07:55,680
like convolutional loop ruling and the

200
00:07:55,680 --> 00:07:58,240
batch analyzing layer we demonstrated

201
00:07:58,240 --> 00:08:01,039
that up output after all of these layers

202
00:08:01,039 --> 00:08:05,120
is expounded by the input

203
00:08:05,120 --> 00:08:07,360
so that is the long structures you know

204
00:08:07,360 --> 00:08:09,360
is well preserved in the

205
00:08:09,360 --> 00:08:11,840
in e

206
00:08:11,840 --> 00:08:15,599
yes so the the you know the

207
00:08:15,599 --> 00:08:17,680
another contribution we have is that we

208
00:08:17,680 --> 00:08:20,479
have this asymmetric implementation

209
00:08:20,479 --> 00:08:23,840
so this uh asthma asthma implementation

210
00:08:23,840 --> 00:08:26,000
this framework can automatically convert

211
00:08:26,000 --> 00:08:28,080
a model into smear format

212
00:08:28,080 --> 00:08:30,479
and then initiate the two parallel

213
00:08:30,479 --> 00:08:32,958
running context which is gpu and sdx

214
00:08:32,958 --> 00:08:36,240
context and the framework supports both

215
00:08:36,240 --> 00:08:38,399
forward and backward process

216
00:08:38,399 --> 00:08:42,719
for most commonly used in the uh models

217
00:08:42,719 --> 00:08:46,320
yeah so we also conduct uh comprehensive

218
00:08:46,320 --> 00:08:48,160
you know numerical evaluations on the

219
00:08:48,160 --> 00:08:50,399
smear performance so the first one we

220
00:08:50,399 --> 00:08:52,399
have we have is uh the you know running

221
00:08:52,399 --> 00:08:55,519
time a master commonly used you know uh

222
00:08:55,519 --> 00:08:56,560
models

223
00:08:56,560 --> 00:08:58,160
so if we compare the training time we

224
00:08:58,160 --> 00:09:01,279
can see that small achieve

225
00:09:01,279 --> 00:09:04,800
like almost eight times faster than sdx

226
00:09:04,800 --> 00:09:06,720
only training in hd

227
00:09:06,720 --> 00:09:09,519
uh network and almost six times faster

228
00:09:09,519 --> 00:09:14,080
than sdx only training on resnet models

229
00:09:14,080 --> 00:09:16,080
and also we also compare the inference

230
00:09:16,080 --> 00:09:17,760
time on games

231
00:09:17,760 --> 00:09:20,240
and compare the you know compared with a

232
00:09:20,240 --> 00:09:23,600
very famous baseline called asylum and

233
00:09:23,600 --> 00:09:27,920
the inference time is almost the same as

234
00:09:27,920 --> 00:09:30,399
thanks to you know the

235
00:09:30,399 --> 00:09:34,720
the long structures in intermediate data

236
00:09:35,040 --> 00:09:37,279
yes so the second experiment were wrong

237
00:09:37,279 --> 00:09:39,600
is that to use the you know is to

238
00:09:39,600 --> 00:09:41,920
measure the training accuracy so if we

239
00:09:41,920 --> 00:09:44,000
compare the small under the original

240
00:09:44,000 --> 00:09:45,360
vest line you can see the training

241
00:09:45,360 --> 00:09:47,040
accuracy is almost

242
00:09:47,040 --> 00:09:49,440
of using small is almost the same as the

243
00:09:49,440 --> 00:09:51,519
original model

244
00:09:51,519 --> 00:09:54,399
and if we compare html with dpx which is

245
00:09:54,399 --> 00:09:56,880
the method that directly add noise to

246
00:09:56,880 --> 00:09:59,920
the vision data to achieve the same

247
00:09:59,920 --> 00:10:03,279
privacy guarantee dpx dpx

248
00:10:03,279 --> 00:10:04,959
you know suffers a lot of you know

249
00:10:04,959 --> 00:10:07,839
accuracy jobs

250
00:10:08,240 --> 00:10:10,399
yeah so we also conduct the modern

251
00:10:10,399 --> 00:10:14,560
inversion text um you know against small

252
00:10:14,560 --> 00:10:16,399
this modern version text is a very

253
00:10:16,399 --> 00:10:18,640
powerful method that you use any prior

254
00:10:18,640 --> 00:10:21,360
knowledge and you know we have to do the

255
00:10:21,360 --> 00:10:23,760
you know to do the modding version

256
00:10:23,760 --> 00:10:25,200
and to measure the you know the

257
00:10:25,200 --> 00:10:26,959
performance of this modern inversion

258
00:10:26,959 --> 00:10:28,240
text we

259
00:10:28,240 --> 00:10:30,880
uh have the following matrix the first

260
00:10:30,880 --> 00:10:33,760
two is piercing and ssim so they are

261
00:10:33,760 --> 00:10:35,519
aimed to you know to measure the

262
00:10:35,519 --> 00:10:37,440
similarities between the training images

263
00:10:37,440 --> 00:10:40,160
and also the reconstruct images and the

264
00:10:40,160 --> 00:10:42,959
last one is accuracy in the top model so

265
00:10:42,959 --> 00:10:45,839
in this metric we input the reconstruct

266
00:10:45,839 --> 00:10:48,000
images to the attacked mode and measured

267
00:10:48,000 --> 00:10:50,240
accuracy

268
00:10:50,240 --> 00:10:53,200
and from this table can see that the you

269
00:10:53,200 --> 00:10:57,760
know psn ssim is very low so that means

270
00:10:57,760 --> 00:10:59,519
there are few similarities between the

271
00:10:59,519 --> 00:11:01,760
training images and the reconstruct

272
00:11:01,760 --> 00:11:04,959
images and as shown in the last column

273
00:11:04,959 --> 00:11:07,519
if we imported the reconstruct images

274
00:11:07,519 --> 00:11:09,680
through the tagged model the qc is also

275
00:11:09,680 --> 00:11:11,040
very low

276
00:11:11,040 --> 00:11:14,640
so that means the attack model can not

277
00:11:14,640 --> 00:11:18,720
recognize the reconstruct images

278
00:11:19,519 --> 00:11:22,880
yeah so this is like some samples and we

279
00:11:22,880 --> 00:11:24,320
can see the difference between the

280
00:11:24,320 --> 00:11:25,920
reconstruct data and the original

281
00:11:25,920 --> 00:11:28,160
training data this you know from this

282
00:11:28,160 --> 00:11:30,240
you know samples we can see that this

283
00:11:30,240 --> 00:11:32,399
you know not a lot of

284
00:11:32,399 --> 00:11:34,399
similarities between this

285
00:11:34,399 --> 00:11:36,480
uh between these two

286
00:11:36,480 --> 00:11:39,120
uh data

287
00:11:39,120 --> 00:11:41,360
yeah so this is the experiment we want

288
00:11:41,360 --> 00:11:43,600
to you know to evaluate the small

289
00:11:43,600 --> 00:11:44,800
performance

290
00:11:44,800 --> 00:11:48,640
and of course smu has some limitations

291
00:11:48,640 --> 00:11:50,000
and one

292
00:11:50,000 --> 00:11:51,120
significant

293
00:11:51,120 --> 00:11:54,639
limitation in the cpu gpu communications

294
00:11:54,639 --> 00:11:57,040
so to measure you know this limitation

295
00:11:57,040 --> 00:11:58,800
we're breaking the running time into

296
00:11:58,800 --> 00:12:01,360
three parts forward backward and the cpu

297
00:12:01,360 --> 00:12:03,120
gpu communication time

298
00:12:03,120 --> 00:12:06,639
so from this figure we can see the

299
00:12:06,639 --> 00:12:08,839
cpu gpu communication

300
00:12:08,839 --> 00:12:11,279
is very significant

301
00:12:11,279 --> 00:12:12,959
especially around in the early

302
00:12:12,959 --> 00:12:15,200
conclusion layers it dominated the whole

303
00:12:15,200 --> 00:12:18,399
you know um training time

304
00:12:18,399 --> 00:12:20,480
yeah so one possible color and one you

305
00:12:20,480 --> 00:12:22,079
know one part

306
00:12:22,079 --> 00:12:24,160
possible solution is that we can

307
00:12:24,160 --> 00:12:26,800
compress the data before sending the

308
00:12:26,800 --> 00:12:29,279
residuals to the gpus yeah so currently

309
00:12:29,279 --> 00:12:31,279
we are working along this direction and

310
00:12:31,279 --> 00:12:33,519
hopefully we can resolve this you know

311
00:12:33,519 --> 00:12:36,399
this issue

312
00:12:37,040 --> 00:12:39,120
yeah so this is a very in a high-level

313
00:12:39,120 --> 00:12:42,720
description about small and yes so i'm

314
00:12:42,720 --> 00:12:45,040
very happy to take any questions if you

315
00:12:45,040 --> 00:12:45,839
have

316
00:12:45,839 --> 00:12:48,160
and also if you read the papers and have

317
00:12:48,160 --> 00:12:50,480
any you know concerns or questions

318
00:12:50,480 --> 00:12:52,639
please you know can take me offline you

319
00:12:52,639 --> 00:12:54,800
know i'm very happy to discuss with you

320
00:12:54,800 --> 00:12:57,200
about any you know potential issues in

321
00:12:57,200 --> 00:12:58,720
esme

322
00:12:58,720 --> 00:13:01,360
yeah thank you

