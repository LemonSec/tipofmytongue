1
00:00:02,639 --> 00:00:05,200
hi my name is dimitri sinen and today

2
00:00:05,200 --> 00:00:07,040
i'm going to talk to you about the

3
00:00:07,040 --> 00:00:09,120
attacks and defenses on collaboratively

4
00:00:09,120 --> 00:00:11,440
trained machine learning models

5
00:00:11,440 --> 00:00:13,840
so let's begin by introducing the

6
00:00:13,840 --> 00:00:16,079
collaborative learning methods so the

7
00:00:16,079 --> 00:00:18,480
first one you're very well familiar with

8
00:00:18,480 --> 00:00:19,680
which is the centralized data

9
00:00:19,680 --> 00:00:22,480
aggregation approach where all the data

10
00:00:22,480 --> 00:00:23,439
gets

11
00:00:23,439 --> 00:00:25,519
aggregated at a single site

12
00:00:25,519 --> 00:00:27,359
and all the algorithmic processing is

13
00:00:27,359 --> 00:00:29,439
performed there there are some obvious

14
00:00:29,439 --> 00:00:32,000
problems such as data governance privacy

15
00:00:32,000 --> 00:00:34,719
regulations lack of transparency and the

16
00:00:34,719 --> 00:00:36,160
fact that some of this high dimensional

17
00:00:36,160 --> 00:00:38,800
data may be expensive to transfer to the

18
00:00:38,800 --> 00:00:40,160
main site

19
00:00:40,160 --> 00:00:42,719
as a result two approaches were proposed

20
00:00:42,719 --> 00:00:44,879
uh that are both decentralized so the

21
00:00:44,879 --> 00:00:46,399
peer-to-peer learning approach and the

22
00:00:46,399 --> 00:00:48,559
federated learning approach

23
00:00:48,559 --> 00:00:51,600
the idea here is that the the data never

24
00:00:51,600 --> 00:00:52,719
leaves the

25
00:00:52,719 --> 00:00:54,320
local device all the learning is

26
00:00:54,320 --> 00:00:56,320
performed locally and any of the results

27
00:00:56,320 --> 00:00:57,360
of the

28
00:00:57,360 --> 00:00:59,440
computation are shared in terms of

29
00:00:59,440 --> 00:01:02,320
peer-to-peer models uh that this happens

30
00:01:02,320 --> 00:01:04,559
with uh all the other participants so

31
00:01:04,559 --> 00:01:06,080
your model gets sent to all the other

32
00:01:06,080 --> 00:01:07,680
participants in in

33
00:01:07,680 --> 00:01:10,159
this kind of direction but in federated

34
00:01:10,159 --> 00:01:11,920
learning uh there is a single

35
00:01:11,920 --> 00:01:13,360
centralized server that does all the

36
00:01:13,360 --> 00:01:16,240
aggregation and that performs all the uh

37
00:01:16,240 --> 00:01:18,880
further processing of the results of the

38
00:01:18,880 --> 00:01:20,479
computation

39
00:01:20,479 --> 00:01:22,720
both of these approaches suffer from the

40
00:01:22,720 --> 00:01:24,560
potential lack of trust because you can

41
00:01:24,560 --> 00:01:26,320
never assume the true intentions of the

42
00:01:26,320 --> 00:01:28,240
other people in the consortium of

43
00:01:28,240 --> 00:01:29,520
learners

44
00:01:29,520 --> 00:01:32,240
as a result we're trying to design a

45
00:01:32,240 --> 00:01:33,759
learning setting which would be

46
00:01:33,759 --> 00:01:36,000
resilient against as many attacks as

47
00:01:36,000 --> 00:01:37,920
possible and the question we're trying

48
00:01:37,920 --> 00:01:39,840
to answer here is is it actually

49
00:01:39,840 --> 00:01:41,280
possible to

50
00:01:41,280 --> 00:01:43,439
to get to that training setting that is

51
00:01:43,439 --> 00:01:45,360
generic enough to apply to

52
00:01:45,360 --> 00:01:48,000
most collaborative learning protocols

53
00:01:48,000 --> 00:01:51,200
and learning tasks as well as be

54
00:01:51,200 --> 00:01:53,680
protected against most attacks on

55
00:01:53,680 --> 00:01:55,920
collaborative learning

56
00:01:55,920 --> 00:01:56,799
and

57
00:01:56,799 --> 00:02:00,479
the two attacks that exist in the wild

58
00:02:00,479 --> 00:02:02,079
are the privacy centered attacks and the

59
00:02:02,079 --> 00:02:04,000
utility centered attacks

60
00:02:04,000 --> 00:02:05,280
so in this work we particularly

61
00:02:05,280 --> 00:02:06,799
concentrate on the privacy-centered

62
00:02:06,799 --> 00:02:08,878
attacks the one that tried to expose the

63
00:02:08,878 --> 00:02:10,560
information that you did not want

64
00:02:10,560 --> 00:02:11,680
exposed

65
00:02:11,680 --> 00:02:13,760
and the examples of this membership and

66
00:02:13,760 --> 00:02:15,680
attribute inference attacks model

67
00:02:15,680 --> 00:02:17,840
inversion etc etc

68
00:02:17,840 --> 00:02:20,080
the utility centered attacks however try

69
00:02:20,080 --> 00:02:21,680
to undermine the integrity of the

70
00:02:21,680 --> 00:02:23,200
trained model they're trying to

71
00:02:23,200 --> 00:02:25,520
introduce collateral learning tasks

72
00:02:25,520 --> 00:02:27,599
they're trying to hinder the performance

73
00:02:27,599 --> 00:02:29,760
of the modern specific underrepresented

74
00:02:29,760 --> 00:02:32,000
subgroups etc etc the examples of this

75
00:02:32,000 --> 00:02:33,760
would be backdoor attacks or model

76
00:02:33,760 --> 00:02:35,840
poisoning

77
00:02:35,840 --> 00:02:37,120
so in this work we consider two

78
00:02:37,120 --> 00:02:38,400
different variations of the model

79
00:02:38,400 --> 00:02:39,760
inversion attack

80
00:02:39,760 --> 00:02:41,920
so what the modern version attack does

81
00:02:41,920 --> 00:02:44,160
it captures some internal representation

82
00:02:44,160 --> 00:02:47,519
of the uh trained model or the model in

83
00:02:47,519 --> 00:02:48,400
training

84
00:02:48,400 --> 00:02:50,640
and it uses this information to try to

85
00:02:50,640 --> 00:02:53,440
leverage it to reconstruct uh certain

86
00:02:53,440 --> 00:02:55,120
sensitive information about the training

87
00:02:55,120 --> 00:02:56,480
data

88
00:02:56,480 --> 00:02:58,720
the first implementation is the

89
00:02:58,720 --> 00:03:00,720
generative decoder attack this was

90
00:03:00,720 --> 00:03:03,840
originally used as uh as a collaborative

91
00:03:03,840 --> 00:03:06,560
inference attack we've also

92
00:03:06,560 --> 00:03:08,959
modified the threat model to potentially

93
00:03:08,959 --> 00:03:11,040
also include the aggregation server in

94
00:03:11,040 --> 00:03:12,400
federated learning

95
00:03:12,400 --> 00:03:14,879
it is of note that here in addition to

96
00:03:14,879 --> 00:03:16,319
federated learning we also use the

97
00:03:16,319 --> 00:03:18,560
concepts of split learning

98
00:03:18,560 --> 00:03:20,640
where the model is uh trained to

99
00:03:20,640 --> 00:03:23,599
specific layer on on site locally and

100
00:03:23,599 --> 00:03:25,040
then the rest of the computation is

101
00:03:25,040 --> 00:03:27,200
performed on the centralized server and

102
00:03:27,200 --> 00:03:28,560
the results of the first part of the

103
00:03:28,560 --> 00:03:31,760
module are sent to the aggregation

104
00:03:31,760 --> 00:03:34,159
server that allows to reduce the amount

105
00:03:34,159 --> 00:03:36,879
of information transferred as well as

106
00:03:36,879 --> 00:03:38,799
reduce the amount of information

107
00:03:38,799 --> 00:03:41,599
available for the adversarial server

108
00:03:41,599 --> 00:03:44,080
so for modular version our threat model

109
00:03:44,080 --> 00:03:46,560
would be white box access honest but

110
00:03:46,560 --> 00:03:48,080
curious because

111
00:03:48,080 --> 00:03:50,640
neither the malicious participants nor

112
00:03:50,640 --> 00:03:52,080
the

113
00:03:52,080 --> 00:03:54,799
malicious central server would be able

114
00:03:54,799 --> 00:03:55,599
to

115
00:03:55,599 --> 00:03:58,000
do anything with the learning context

116
00:03:58,000 --> 00:03:59,920
itself all they do is they try to infer

117
00:03:59,920 --> 00:04:01,840
the information they aren't supposed to

118
00:04:01,840 --> 00:04:03,680
infer

119
00:04:03,680 --> 00:04:05,680
so the results of the attack

120
00:04:05,680 --> 00:04:08,080
can be seen on this slide uh

121
00:04:08,080 --> 00:04:11,040
they are pretty uh pretty effective when

122
00:04:11,040 --> 00:04:12,560
it comes to capturing

123
00:04:12,560 --> 00:04:15,920
data of low dimensions however

124
00:04:15,920 --> 00:04:17,680
uh when it comes to more complex data

125
00:04:17,680 --> 00:04:19,680
sets in this case even so far

126
00:04:19,680 --> 00:04:21,600
safer 10 or 600

127
00:04:21,600 --> 00:04:23,600
the attacker may experience certain

128
00:04:23,600 --> 00:04:25,680
difficulties when the models are too

129
00:04:25,680 --> 00:04:27,360
large but more of that

130
00:04:27,360 --> 00:04:29,120
more on that later

131
00:04:29,120 --> 00:04:31,120
the second attack that we

132
00:04:31,120 --> 00:04:32,880
test here is the deep leakage from

133
00:04:32,880 --> 00:04:36,800
gradients and the idea here is that the

134
00:04:36,800 --> 00:04:39,919
um the client or the

135
00:04:39,919 --> 00:04:40,960
the central server that's been

136
00:04:40,960 --> 00:04:43,919
compromised captures the gradient shared

137
00:04:43,919 --> 00:04:45,600
by another participant

138
00:04:45,600 --> 00:04:47,280
and this gradient corresponds to an

139
00:04:47,280 --> 00:04:49,280
image that the participant uses to train

140
00:04:49,280 --> 00:04:51,199
the model and what the adversary tries

141
00:04:51,199 --> 00:04:53,840
to do is uh they

142
00:04:53,840 --> 00:04:55,520
they replicate the dimensions of the

143
00:04:55,520 --> 00:04:57,680
target image so because they're one of

144
00:04:57,680 --> 00:04:59,759
the participants or the central server

145
00:04:59,759 --> 00:05:02,560
and they try to perturb the image that

146
00:05:02,560 --> 00:05:05,600
they control i.e the noise to match

147
00:05:05,600 --> 00:05:08,080
uh the gradients that they captured so

148
00:05:08,080 --> 00:05:09,759
the gradient that they capture at some

149
00:05:09,759 --> 00:05:12,720
point after you perturb the image uh for

150
00:05:12,720 --> 00:05:16,080
for long enough time will match what the

151
00:05:16,080 --> 00:05:19,840
adversary is trying to replicate

152
00:05:19,840 --> 00:05:21,280
and the results of this attack are

153
00:05:21,280 --> 00:05:24,400
pretty impressive uh within within 40 or

154
00:05:24,400 --> 00:05:26,880
50 iterations you can already

155
00:05:26,880 --> 00:05:29,199
get the not just the outline of what the

156
00:05:29,199 --> 00:05:30,720
trading data looks like but in some

157
00:05:30,720 --> 00:05:34,320
cases the exact reconstruction of the

158
00:05:34,320 --> 00:05:36,560
image used to train the model using this

159
00:05:36,560 --> 00:05:38,560
reconstruction method

160
00:05:38,560 --> 00:05:40,639
the second type of attack that we deploy

161
00:05:40,639 --> 00:05:42,080
is the membership inference attack so

162
00:05:42,080 --> 00:05:43,680
the adversary has access to a single

163
00:05:43,680 --> 00:05:46,240
data record and they wish to determine

164
00:05:46,240 --> 00:05:48,320
if this data record was part of the

165
00:05:48,320 --> 00:05:49,759
trading data set

166
00:05:49,759 --> 00:05:52,080
uh which which could mean severe privacy

167
00:05:52,080 --> 00:05:54,479
violations when we talk about uh disease

168
00:05:54,479 --> 00:05:56,800
data sets in healthcare

169
00:05:56,800 --> 00:05:58,720
they have black box access to the

170
00:05:58,720 --> 00:06:00,400
trained model and they're trying to

171
00:06:00,400 --> 00:06:04,080
predict uh using the confidence interval

172
00:06:04,080 --> 00:06:06,639
that the model produces based on the

173
00:06:06,639 --> 00:06:08,880
data record that they supply it with if

174
00:06:08,880 --> 00:06:10,639
this record was indeed part of the

175
00:06:10,639 --> 00:06:13,680
training data set or not

176
00:06:13,680 --> 00:06:16,000
in this work our main contribution is

177
00:06:16,000 --> 00:06:18,880
the concept of model adaptations so

178
00:06:18,880 --> 00:06:21,039
these are easy to implement empirical

179
00:06:21,039 --> 00:06:22,720
measures that could be

180
00:06:22,720 --> 00:06:25,759
uh seen as the slight changes that you

181
00:06:25,759 --> 00:06:27,759
need to make in the

182
00:06:27,759 --> 00:06:28,880
uh in the collaborative learning

183
00:06:28,880 --> 00:06:31,280
protocol in order to reduce the

184
00:06:31,280 --> 00:06:32,880
effectiveness of most privacy

185
00:06:32,880 --> 00:06:35,120
privacy-based attackers

186
00:06:35,120 --> 00:06:36,880
so by these we mean trivial things such

187
00:06:36,880 --> 00:06:37,520
as

188
00:06:37,520 --> 00:06:39,280
uh the width of the models the layers

189
00:06:39,280 --> 00:06:40,960
that you use

190
00:06:40,960 --> 00:06:42,639
batch sizes

191
00:06:42,639 --> 00:06:44,319
whether whether or not to use noisy

192
00:06:44,319 --> 00:06:47,600
labels specific normalization strategies

193
00:06:47,600 --> 00:06:49,919
etc etc

194
00:06:49,919 --> 00:06:52,560
the the slight teaser is that

195
00:06:52,560 --> 00:06:54,479
uh they do work if you look at the on

196
00:06:54,479 --> 00:06:56,400
the on the right hand side of the

197
00:06:56,400 --> 00:06:59,440
picture the left hand side is the uh

198
00:06:59,440 --> 00:07:01,599
generative decoder attack on

199
00:07:01,599 --> 00:07:04,240
safarten without adaptations and the

200
00:07:04,240 --> 00:07:07,680
right hand side with adaptations

201
00:07:07,680 --> 00:07:09,840
i'm going to briefly present some

202
00:07:09,840 --> 00:07:13,120
some noteworthy results so for the

203
00:07:13,120 --> 00:07:14,639
decoder attack

204
00:07:14,639 --> 00:07:16,479
we found that

205
00:07:16,479 --> 00:07:18,080
increasing the

206
00:07:18,080 --> 00:07:20,880
width of the convolutional layers

207
00:07:20,880 --> 00:07:22,560
presented the attacker with more data

208
00:07:22,560 --> 00:07:24,800
that you could that they could utilize

209
00:07:24,800 --> 00:07:27,199
to reconstruct the images

210
00:07:27,199 --> 00:07:28,639
and

211
00:07:28,639 --> 00:07:32,240
if the if the model is uh pretty shallow

212
00:07:32,240 --> 00:07:34,639
if there isn't enough width

213
00:07:34,639 --> 00:07:37,280
then the attacker may struggle to obtain

214
00:07:37,280 --> 00:07:40,240
meaningful uh representations

215
00:07:40,240 --> 00:07:43,919
and as you can see on the uh bottom left

216
00:07:43,919 --> 00:07:46,160
there are different variations based on

217
00:07:46,160 --> 00:07:49,360
which activation functions you use and

218
00:07:49,360 --> 00:07:50,400
which

219
00:07:50,400 --> 00:07:52,800
pulling layers you deploy as well for

220
00:07:52,800 --> 00:07:54,720
this specific attack we found tange to

221
00:07:54,720 --> 00:07:57,520
be detrimental to the attacker and we

222
00:07:57,520 --> 00:07:59,680
found that max paul

223
00:07:59,680 --> 00:08:01,440
is also detrimental to the results of

224
00:08:01,440 --> 00:08:02,879
the attack

225
00:08:02,879 --> 00:08:05,120
now when we discuss these results it's

226
00:08:05,120 --> 00:08:06,960
important to put them into context of

227
00:08:06,960 --> 00:08:08,960
the deep leakage from gradients

228
00:08:08,960 --> 00:08:11,360
because contrary to the previous ones

229
00:08:11,360 --> 00:08:12,960
that we just discussed

230
00:08:12,960 --> 00:08:15,199
here you have a little bit of a narrow

231
00:08:15,199 --> 00:08:17,440
window where if the model is too simple

232
00:08:17,440 --> 00:08:19,599
it doesn't memorize the data well enough

233
00:08:19,599 --> 00:08:21,680
but if the model is too complex then the

234
00:08:21,680 --> 00:08:23,360
adversary is going to struggle with the

235
00:08:23,360 --> 00:08:25,199
reconstruction procedure

236
00:08:25,199 --> 00:08:26,879
this is particularly noticeable for

237
00:08:26,879 --> 00:08:28,720
convolutional layers

238
00:08:28,720 --> 00:08:30,960
and for the fully connected layers we

239
00:08:30,960 --> 00:08:33,919
found that this is a single-sided uh

240
00:08:33,919 --> 00:08:36,240
single-sided limit that the model can

241
00:08:36,240 --> 00:08:37,679
get too complex for the industrial

242
00:08:37,679 --> 00:08:40,399
optimizer to try to figure out what was

243
00:08:40,399 --> 00:08:42,159
the training data

244
00:08:42,159 --> 00:08:45,680
but there is no lower limit that we are

245
00:08:45,680 --> 00:08:46,800
aware of

246
00:08:46,800 --> 00:08:48,720
and as you can see on the bottom

247
00:08:48,720 --> 00:08:52,080
uh this attack gets mitigated in full

248
00:08:52,080 --> 00:08:53,760
unlike the generative decoder there is

249
00:08:53,760 --> 00:08:55,920
no partial reconstruction there is no

250
00:08:55,920 --> 00:08:58,560
there's no background uh

251
00:08:58,560 --> 00:09:00,560
dissection there was no way to determine

252
00:09:00,560 --> 00:09:01,920
what was the training record it's just

253
00:09:01,920 --> 00:09:04,240
the noise that you end up with

254
00:09:04,240 --> 00:09:05,920
for membership inference the results are

255
00:09:05,920 --> 00:09:08,640
slightly different so for convolutions

256
00:09:08,640 --> 00:09:10,800
the wider the model is the more

257
00:09:10,800 --> 00:09:14,640
successful the adversary get

258
00:09:14,640 --> 00:09:16,000
because there is more information they

259
00:09:16,000 --> 00:09:17,519
can leverage

260
00:09:17,519 --> 00:09:19,600
at the same time

261
00:09:19,600 --> 00:09:21,440
this does not hold true for the fully

262
00:09:21,440 --> 00:09:22,959
connected layers there is no clear

263
00:09:22,959 --> 00:09:24,720
correlation between the

264
00:09:24,720 --> 00:09:27,279
between the width of the fully connected

265
00:09:27,279 --> 00:09:29,839
layers and the results of the attack

266
00:09:29,839 --> 00:09:32,720
and we also as a little extension

267
00:09:32,720 --> 00:09:33,839
compared

268
00:09:33,839 --> 00:09:36,000
our experiments against formal

269
00:09:36,000 --> 00:09:38,640
privacy-preserving meta protocols such

270
00:09:38,640 --> 00:09:40,959
as differential privacy and we found

271
00:09:40,959 --> 00:09:43,040
that by selecting the hyper parameters

272
00:09:43,040 --> 00:09:45,680
correctly by selecting the architecture

273
00:09:45,680 --> 00:09:47,440
by being very careful with our threat

274
00:09:47,440 --> 00:09:50,959
model we were able to obtain uh

275
00:09:50,959 --> 00:09:52,800
models that are resistant against these

276
00:09:52,800 --> 00:09:56,080
types of attacks without the associated

277
00:09:56,080 --> 00:09:58,000
utility penalty that you normally see in

278
00:09:58,000 --> 00:10:01,440
differentially private uh deep learning

279
00:10:01,440 --> 00:10:03,519
here is just a bit of a visual overview

280
00:10:03,519 --> 00:10:06,160
so this is the dog attack uh the

281
00:10:06,160 --> 00:10:08,480
reconstruction was run on the pneumonia

282
00:10:08,480 --> 00:10:11,200
prediction data set by kamani resized to

283
00:10:11,200 --> 00:10:13,920
32 by 32 and 64 by 64.

284
00:10:13,920 --> 00:10:16,560
and you can see uh precisely that

285
00:10:16,560 --> 00:10:17,519
neither

286
00:10:17,519 --> 00:10:21,120
the dp trained model uh nor the model

287
00:10:21,120 --> 00:10:23,279
trained under model adaptations was

288
00:10:23,279 --> 00:10:26,079
vulnerable to this attack

289
00:10:26,079 --> 00:10:28,720
here is a little trdr the result uh as

290
00:10:28,720 --> 00:10:30,800
you can see the the big problem here is

291
00:10:30,800 --> 00:10:33,839
that there was no unified solution to

292
00:10:33,839 --> 00:10:35,440
all three attacks at the same time and

293
00:10:35,440 --> 00:10:37,360
these are just three attacks out of many

294
00:10:37,360 --> 00:10:39,040
potentially 20 different types that the

295
00:10:39,040 --> 00:10:41,040
adversary can leverage

296
00:10:41,040 --> 00:10:44,160
so can we mitigate even even those three

297
00:10:44,160 --> 00:10:46,720
with a specific architecture and a set

298
00:10:46,720 --> 00:10:49,680
of mod adaptations no we cannot but the

299
00:10:49,680 --> 00:10:51,680
point here is that we can significantly

300
00:10:51,680 --> 00:10:53,920
reduce the attacker's surface

301
00:10:53,920 --> 00:10:56,720
and completely uh completely mitigate

302
00:10:56,720 --> 00:10:58,880
certain attacks in full while also

303
00:10:58,880 --> 00:11:00,320
slightly improving the robustness

304
00:11:00,320 --> 00:11:03,440
against other types of adversaries

305
00:11:03,440 --> 00:11:04,480
however

306
00:11:04,480 --> 00:11:06,880
this research doesn't stop here

307
00:11:06,880 --> 00:11:08,880
because between when this work was

308
00:11:08,880 --> 00:11:11,440
submitted and now there were many

309
00:11:11,440 --> 00:11:13,680
different attacks the new developed that

310
00:11:13,680 --> 00:11:15,839
were developed some attacks such as the

311
00:11:15,839 --> 00:11:17,920
generative decoder are now more or less

312
00:11:17,920 --> 00:11:19,519
obsolete because split learning is not

313
00:11:19,519 --> 00:11:22,079
as popular as it used to be

314
00:11:22,079 --> 00:11:25,200
but the trends uh mostly remain the same

315
00:11:25,200 --> 00:11:26,959
so the conclusions that were reached on

316
00:11:26,959 --> 00:11:28,079
the width and the complexity of the

317
00:11:28,079 --> 00:11:29,760
model as well as the data can still be

318
00:11:29,760 --> 00:11:30,959
applied to the more modern

319
00:11:30,959 --> 00:11:33,600
implementations of the attack

320
00:11:33,600 --> 00:11:35,760
uh the future work

321
00:11:35,760 --> 00:11:37,600
that we are currently

322
00:11:37,600 --> 00:11:39,839
investigating is can we obtain a very

323
00:11:39,839 --> 00:11:41,360
similar study

324
00:11:41,360 --> 00:11:43,680
on model poisoning and backdoor attacks

325
00:11:43,680 --> 00:11:45,360
and the the

326
00:11:45,360 --> 00:11:47,760
the tldr is yes we can but again there

327
00:11:47,760 --> 00:11:49,200
are complications when it comes to

328
00:11:49,200 --> 00:11:50,800
assessing what exactly contributes to a

329
00:11:50,800 --> 00:11:53,279
better or worse attack which are the two

330
00:11:53,279 --> 00:11:55,519
main challenges associated with this

331
00:11:55,519 --> 00:11:56,959
work because there are simply too many

332
00:11:56,959 --> 00:11:59,760
adaptations for us to consider and the

333
00:11:59,760 --> 00:12:01,200
number of attacks were posed over the

334
00:12:01,200 --> 00:12:03,120
last two years is extreme

335
00:12:03,120 --> 00:12:04,800
these are just the different variations

336
00:12:04,800 --> 00:12:07,279
of dog some of which are actually able

337
00:12:07,279 --> 00:12:09,360
to mitigate our

338
00:12:09,360 --> 00:12:11,920
model adaptation defenses particularly

339
00:12:11,920 --> 00:12:15,519
the the final three

340
00:12:15,680 --> 00:12:17,360
finally uh

341
00:12:17,360 --> 00:12:19,120
we've uh

342
00:12:19,120 --> 00:12:21,600
during during our work on uh

343
00:12:21,600 --> 00:12:23,200
on model inversion we've produced a

344
00:12:23,200 --> 00:12:25,600
number of contexts uh clinical ones

345
00:12:25,600 --> 00:12:27,920
which were vulnerable to

346
00:12:27,920 --> 00:12:29,760
uh to deep leakage from gradients and

347
00:12:29,760 --> 00:12:30,560
its

348
00:12:30,560 --> 00:12:32,320
advanced versions

349
00:12:32,320 --> 00:12:34,480
this is actually true for for

350
00:12:34,480 --> 00:12:37,680
image segmentation models uh as well as

351
00:12:37,680 --> 00:12:39,920
for the classification models some of

352
00:12:39,920 --> 00:12:42,320
you may recognize the bratz data set on

353
00:12:42,320 --> 00:12:44,399
the right hand side this is just a big

354
00:12:44,399 --> 00:12:47,600
showcase of uh of the fact that no model

355
00:12:47,600 --> 00:12:50,720
is safe by default and privacy should be

356
00:12:50,720 --> 00:12:52,160
included in the learning context

357
00:12:52,160 --> 00:12:53,600
particularly a collaborative one from

358
00:12:53,600 --> 00:12:55,440
the ground up if you don't want to

359
00:12:55,440 --> 00:12:57,920
obtain the results that look like this

360
00:12:57,920 --> 00:13:01,120
during your collaborative training

361
00:13:01,120 --> 00:13:02,639
thank you very much for your attention

362
00:13:02,639 --> 00:13:04,800
if you have any questions or proposals

363
00:13:04,800 --> 00:13:08,719
please feel free to email me

