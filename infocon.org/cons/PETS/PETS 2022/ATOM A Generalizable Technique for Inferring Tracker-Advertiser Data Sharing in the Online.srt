1
00:00:01,360 --> 00:00:02,879
hello everyone

2
00:00:02,879 --> 00:00:05,520
today i will be presenting my work atom

3
00:00:05,520 --> 00:00:07,759
a generalizable technique for inferring

4
00:00:07,759 --> 00:00:10,000
tracker advertiser data sharing in the

5
00:00:10,000 --> 00:00:13,120
online behavior advertising ecosystem

6
00:00:13,120 --> 00:00:14,880
my advisor on this project was rishabh

7
00:00:14,880 --> 00:00:16,720
nathan and we both represent the

8
00:00:16,720 --> 00:00:19,439
university of iowa

9
00:00:19,439 --> 00:00:21,600
ladies and gentlemen our data is being

10
00:00:21,600 --> 00:00:22,640
gathered

11
00:00:22,640 --> 00:00:25,199
when a person visits a website they are

12
00:00:25,199 --> 00:00:26,640
exposing some information about

13
00:00:26,640 --> 00:00:29,039
themselves to the online world

14
00:00:29,039 --> 00:00:31,679
such as their likes dislikes browsing

15
00:00:31,679 --> 00:00:34,079
habits or browsing history

16
00:00:34,079 --> 00:00:37,200
we call this data gathering

17
00:00:37,200 --> 00:00:39,760
once gathered this data can be used for

18
00:00:39,760 --> 00:00:42,399
various reasons but usually it is used

19
00:00:42,399 --> 00:00:45,120
to create a user's profile

20
00:00:45,120 --> 00:00:47,200
now if there are trackers present on

21
00:00:47,200 --> 00:00:49,680
this website they also have access to

22
00:00:49,680 --> 00:00:52,719
this information about the user

23
00:00:52,719 --> 00:00:55,520
these trackers can further share or sell

24
00:00:55,520 --> 00:00:57,520
our data to advertisers

25
00:00:57,520 --> 00:01:00,000
who in turn can use this to deliver

26
00:01:00,000 --> 00:01:02,480
relevant ads to the user

27
00:01:02,480 --> 00:01:04,479
now the problem with our information

28
00:01:04,479 --> 00:01:06,720
flowing in such a manner is that this

29
00:01:06,720 --> 00:01:09,040
entire portion of the ecosystem is a

30
00:01:09,040 --> 00:01:11,040
black box to the user or even the

31
00:01:11,040 --> 00:01:13,680
researchers meaning our data is being

32
00:01:13,680 --> 00:01:16,799
shared and we don't know with whom

33
00:01:16,799 --> 00:01:18,799
and there is limited work done on making

34
00:01:18,799 --> 00:01:20,560
this black box more transparent in the

35
00:01:20,560 --> 00:01:21,759
past

36
00:01:21,759 --> 00:01:23,759
and this work is usually limited by the

37
00:01:23,759 --> 00:01:26,080
ad delivery mechanism or it is dependent

38
00:01:26,080 --> 00:01:29,439
on an artifact such as the bid value

39
00:01:29,439 --> 00:01:31,759
moreover over the last five years or so

40
00:01:31,759 --> 00:01:34,960
data privacy policies such as gdpr ccpa

41
00:01:34,960 --> 00:01:37,920
and cpra have been put in place with the

42
00:01:37,920 --> 00:01:40,240
sole purpose of making this ecosystem

43
00:01:40,240 --> 00:01:42,640
more accountable and transparent

44
00:01:42,640 --> 00:01:44,960
however with limited resources and

45
00:01:44,960 --> 00:01:47,680
absence of a genetic auditing framework

46
00:01:47,680 --> 00:01:50,079
these policies have not been utilized or

47
00:01:50,079 --> 00:01:52,479
enforced fully

48
00:01:52,479 --> 00:01:55,600
now if we zoom in on the problem

49
00:01:55,600 --> 00:01:58,079
we can see that the input signal to this

50
00:01:58,079 --> 00:02:00,719
black box is the user profile

51
00:02:00,719 --> 00:02:03,040
and the output signal in this case is

52
00:02:03,040 --> 00:02:04,880
the ad that we observe

53
00:02:04,880 --> 00:02:07,439
and our goal is to make the internals of

54
00:02:07,439 --> 00:02:10,160
this black box more transparent

55
00:02:10,160 --> 00:02:12,239
turns out this reduces extremely well to

56
00:02:12,239 --> 00:02:14,640
a network tomography problem now what is

57
00:02:14,640 --> 00:02:16,160
network demography

58
00:02:16,160 --> 00:02:18,239
network demography is the study of a

59
00:02:18,239 --> 00:02:21,120
network's internal characteristics using

60
00:02:21,120 --> 00:02:24,080
information derived from endpoint data

61
00:02:24,080 --> 00:02:26,959
and in this case our end points can be

62
00:02:26,959 --> 00:02:29,599
the user's profile or browsing history

63
00:02:29,599 --> 00:02:32,720
and the ads that we receive

64
00:02:32,720 --> 00:02:34,879
now theoretically our problem reduces

65
00:02:34,879 --> 00:02:37,440
well to network tomography however to

66
00:02:37,440 --> 00:02:40,000
verify this we first use the principles

67
00:02:40,000 --> 00:02:41,680
of network tomography to answer the

68
00:02:41,680 --> 00:02:42,640
question

69
00:02:42,640 --> 00:02:44,959
can ads represent browsing history of a

70
00:02:44,959 --> 00:02:46,000
user

71
00:02:46,000 --> 00:02:47,120
and if true

72
00:02:47,120 --> 00:02:50,720
this means we have a valid input and

73
00:02:50,720 --> 00:02:52,959
input endpoint which is the user profile

74
00:02:52,959 --> 00:02:56,000
which represents or influences an output

75
00:02:56,000 --> 00:02:59,360
endpoint which is the ads

76
00:02:59,360 --> 00:03:02,159
second using this input output signal or

77
00:03:02,159 --> 00:03:04,879
endpoints we check does presence or

78
00:03:04,879 --> 00:03:07,040
absence of a tracker change the ads that

79
00:03:07,040 --> 00:03:08,480
we observe

80
00:03:08,480 --> 00:03:10,400
and essentially this will help us

81
00:03:10,400 --> 00:03:13,040
uncover or infer tracker advertiser

82
00:03:13,040 --> 00:03:15,440
relationships

83
00:03:15,440 --> 00:03:17,760
so we ran an experiment to help us

84
00:03:17,760 --> 00:03:19,680
answer our first question

85
00:03:19,680 --> 00:03:22,400
for this we needed user profiles

86
00:03:22,400 --> 00:03:25,360
so we trained seven types of users by

87
00:03:25,360 --> 00:03:27,840
instrumenting open wpm which is a web

88
00:03:27,840 --> 00:03:29,920
measurement tool

89
00:03:29,920 --> 00:03:32,640
for training each user type we

90
00:03:32,640 --> 00:03:35,280
essentially visited 100 websites that

91
00:03:35,280 --> 00:03:37,120
were relevant to that user type for

92
00:03:37,120 --> 00:03:38,799
example if we were turning the games

93
00:03:38,799 --> 00:03:41,120
user type we visited 100 websites that

94
00:03:41,120 --> 00:03:45,360
were specifically related to games

95
00:03:45,440 --> 00:03:47,440
once we had these users

96
00:03:47,440 --> 00:03:49,440
we collected ads that were delivered to

97
00:03:49,440 --> 00:03:51,120
each of them

98
00:03:51,120 --> 00:03:53,439
to analyze these ads we extracted

99
00:03:53,439 --> 00:03:55,680
textual information from them and then

100
00:03:55,680 --> 00:03:57,680
converted that into a simple count

101
00:03:57,680 --> 00:03:59,519
vector of the words

102
00:03:59,519 --> 00:04:01,760
we repeated this process nine times and

103
00:04:01,760 --> 00:04:04,799
essentially obtained a total of 63 users

104
00:04:04,799 --> 00:04:09,040
and their ads or count vectors

105
00:04:09,040 --> 00:04:10,799
now going over the results from this

106
00:04:10,799 --> 00:04:12,080
experiment

107
00:04:12,080 --> 00:04:15,439
we will analyze the following heat map

108
00:04:15,439 --> 00:04:18,000
on the x-axis and the y-axis

109
00:04:18,000 --> 00:04:20,720
of this heat map we see

110
00:04:20,720 --> 00:04:22,639
seven user types

111
00:04:22,639 --> 00:04:24,479
that we trained

112
00:04:24,479 --> 00:04:27,040
each cell represents the count vector of

113
00:04:27,040 --> 00:04:29,280
each of the seven user types and each of

114
00:04:29,280 --> 00:04:31,680
them have nine entries so essentially we

115
00:04:31,680 --> 00:04:33,919
have 63 count vectors on the x-axis and

116
00:04:33,919 --> 00:04:35,840
63 on the y-axis

117
00:04:35,840 --> 00:04:37,840
for example if you zoom in on this one

118
00:04:37,840 --> 00:04:40,080
cell we have this represents one count

119
00:04:40,080 --> 00:04:42,639
vector of the user type sports in one

120
00:04:42,639 --> 00:04:45,040
run and there will be nine of this nine

121
00:04:45,040 --> 00:04:47,360
of these in this heat map

122
00:04:47,360 --> 00:04:49,600
the heat map itself is based on the

123
00:04:49,600 --> 00:04:51,600
similarity between the count vectors we

124
00:04:51,600 --> 00:04:54,320
obtained from our experiment

125
00:04:54,320 --> 00:04:56,160
so there are three major takeaways we

126
00:04:56,160 --> 00:04:57,919
obtain from this heat map

127
00:04:57,919 --> 00:05:00,720
first we observe a high similarity

128
00:05:00,720 --> 00:05:04,160
between users of the same type over time

129
00:05:04,160 --> 00:05:07,520
essentially near the diagonal

130
00:05:07,520 --> 00:05:09,199
where we have similarity between the

131
00:05:09,199 --> 00:05:12,240
same user types

132
00:05:12,240 --> 00:05:14,960
on the other hand we see low similarity

133
00:05:14,960 --> 00:05:17,280
between users of different types across

134
00:05:17,280 --> 00:05:18,240
time

135
00:05:18,240 --> 00:05:20,880
basically away from the diagonal we see

136
00:05:20,880 --> 00:05:21,919
a lower

137
00:05:21,919 --> 00:05:22,840
cosine

138
00:05:22,840 --> 00:05:24,479
similarity

139
00:05:24,479 --> 00:05:27,039
this indicates that ads of the same user

140
00:05:27,039 --> 00:05:29,440
type don't change over time and they are

141
00:05:29,440 --> 00:05:31,600
different from other user types

142
00:05:31,600 --> 00:05:33,280
consistently

143
00:05:33,280 --> 00:05:34,800
and to hammer in this point we

144
00:05:34,800 --> 00:05:36,960
established that the difference between

145
00:05:36,960 --> 00:05:38,880
the inter and the intra similarity

146
00:05:38,880 --> 00:05:42,320
distributions is significant

147
00:05:42,720 --> 00:05:45,440
so our question about ads representing a

148
00:05:45,440 --> 00:05:48,320
user is answered and hence we have a

149
00:05:48,320 --> 00:05:51,360
valid input output endpoint to our

150
00:05:51,360 --> 00:05:52,800
problem

151
00:05:52,800 --> 00:05:55,360
now let's move on to how we use network

152
00:05:55,360 --> 00:05:57,600
tomography and these input output

153
00:05:57,600 --> 00:06:00,160
endpoints to infer tracker advertiser

154
00:06:00,160 --> 00:06:02,319
relationships

155
00:06:02,319 --> 00:06:04,560
for this let's go back to our reduced

156
00:06:04,560 --> 00:06:06,400
network demography diagram but with a

157
00:06:06,400 --> 00:06:08,720
few changes this time let's suppose we

158
00:06:08,720 --> 00:06:10,400
only have three trackers and three

159
00:06:10,400 --> 00:06:12,400
advertisers in the world

160
00:06:12,400 --> 00:06:14,720
when a user visits a website

161
00:06:14,720 --> 00:06:16,880
these trackers gather information about

162
00:06:16,880 --> 00:06:18,160
the user

163
00:06:18,160 --> 00:06:20,080
after this the information is out in the

164
00:06:20,080 --> 00:06:22,400
online ecosystem and god knows how it's

165
00:06:22,400 --> 00:06:23,600
shared

166
00:06:23,600 --> 00:06:26,400
finally out of this black box comes ads

167
00:06:26,400 --> 00:06:28,400
from three advertisers

168
00:06:28,400 --> 00:06:30,720
now our goal is to see what happens in

169
00:06:30,720 --> 00:06:32,160
that black box

170
00:06:32,160 --> 00:06:34,479
in other words which tracker is linked

171
00:06:34,479 --> 00:06:37,680
to which advertiser if at all

172
00:06:37,680 --> 00:06:39,520
now let's let's focus on finding

173
00:06:39,520 --> 00:06:42,000
relationships of the advertiser x in

174
00:06:42,000 --> 00:06:43,600
this case

175
00:06:43,600 --> 00:06:45,840
so in the case where we don't interfere

176
00:06:45,840 --> 00:06:46,800
at all

177
00:06:46,800 --> 00:06:49,120
x shows us these ads

178
00:06:49,120 --> 00:06:51,199
if we look closely these are all food

179
00:06:51,199 --> 00:06:52,800
related ads

180
00:06:52,800 --> 00:06:55,680
we can call this the control ads as we

181
00:06:55,680 --> 00:06:58,800
did not make any change to the trackers

182
00:06:58,800 --> 00:07:01,120
now what happens when we block

183
00:07:01,120 --> 00:07:03,199
tracker a

184
00:07:03,199 --> 00:07:06,240
turns out nothing we still see the same

185
00:07:06,240 --> 00:07:09,680
kinds of ad that we saw in our control

186
00:07:09,680 --> 00:07:10,960
run

187
00:07:10,960 --> 00:07:13,680
we record this in a table as we see no

188
00:07:13,680 --> 00:07:15,759
difference

189
00:07:15,759 --> 00:07:18,800
next we block tracker b

190
00:07:18,800 --> 00:07:21,039
now suddenly we see ads that are all

191
00:07:21,039 --> 00:07:22,639
sports related

192
00:07:22,639 --> 00:07:24,880
so essentially the advertiser now thinks

193
00:07:24,880 --> 00:07:27,599
of this user as someone else

194
00:07:27,599 --> 00:07:30,800
we record this as well as

195
00:07:30,800 --> 00:07:32,160
seeing a difference in the difference

196
00:07:32,160 --> 00:07:33,599
column

197
00:07:33,599 --> 00:07:35,599
we do all possible combinations of

198
00:07:35,599 --> 00:07:38,080
blocking and record how that changes the

199
00:07:38,080 --> 00:07:41,039
ads that we observe

200
00:07:41,199 --> 00:07:43,360
using this table we train an

201
00:07:43,360 --> 00:07:45,680
interpretable statistical model

202
00:07:45,680 --> 00:07:47,919
that tries to predict the difference

203
00:07:47,919 --> 00:07:48,960
column

204
00:07:48,960 --> 00:07:50,800
based on the combination of the tracker

205
00:07:50,800 --> 00:07:52,639
blocking as a feature

206
00:07:52,639 --> 00:07:55,199
if this model has high accuracy we

207
00:07:55,199 --> 00:07:57,199
extract the most important features from

208
00:07:57,199 --> 00:08:00,639
this model in this case b and c

209
00:08:00,639 --> 00:08:02,720
and finally this tells us that b and c

210
00:08:02,720 --> 00:08:04,639
have a relationship with the advertiser

211
00:08:04,639 --> 00:08:06,319
x

212
00:08:06,319 --> 00:08:08,400
now running this on scale we gathered

213
00:08:08,400 --> 00:08:10,479
the following relationships

214
00:08:10,479 --> 00:08:13,599
the the three columns on the left denote

215
00:08:13,599 --> 00:08:15,759
the advertiser in the first column the

216
00:08:15,759 --> 00:08:17,520
accuracy of the model in the second

217
00:08:17,520 --> 00:08:19,759
column and the inferred relationships in

218
00:08:19,759 --> 00:08:22,479
the third column

219
00:08:22,479 --> 00:08:24,879
for example the first row shows open x

220
00:08:24,879 --> 00:08:27,440
has a relationship with oracle and

221
00:08:27,440 --> 00:08:29,039
alphabet

222
00:08:29,039 --> 00:08:31,199
to validate our relationships we used

223
00:08:31,199 --> 00:08:33,279
three sources of ground truth

224
00:08:33,279 --> 00:08:34,399
first

225
00:08:34,399 --> 00:08:36,320
we visited the attorney general's

226
00:08:36,320 --> 00:08:38,320
website where data brokers are supposed

227
00:08:38,320 --> 00:08:41,440
to register themselves we explored those

228
00:08:41,440 --> 00:08:44,159
data brokers and found and enumerated

229
00:08:44,159 --> 00:08:45,920
any disclosed relationships that they

230
00:08:45,920 --> 00:08:48,480
had and we were able to verify open x's

231
00:08:48,480 --> 00:08:50,160
relationship with oracle using this

232
00:08:50,160 --> 00:08:51,519
methodology

233
00:08:51,519 --> 00:08:53,680
secondly we used client-side cookie

234
00:08:53,680 --> 00:08:56,880
syncing to validate other relationships

235
00:08:56,880 --> 00:08:58,160
or as you can see most of our

236
00:08:58,160 --> 00:09:00,560
relationships that we uncover in this

237
00:09:00,560 --> 00:09:01,920
experiment

238
00:09:01,920 --> 00:09:04,240
and finally we used header bidding which

239
00:09:04,240 --> 00:09:06,959
is a technique previously introduced by

240
00:09:06,959 --> 00:09:08,959
previous literature to validate another

241
00:09:08,959 --> 00:09:10,720
one of our relationships

242
00:09:10,720 --> 00:09:12,320
we can see some of the relationships

243
00:09:12,320 --> 00:09:16,240
have no way of being validated

244
00:09:17,519 --> 00:09:18,480
now

245
00:09:18,480 --> 00:09:20,240
we have successfully executed an

246
00:09:20,240 --> 00:09:22,800
experiment that helps us infer never

247
00:09:22,800 --> 00:09:24,480
before seen track advertiser

248
00:09:24,480 --> 00:09:27,120
relationships

249
00:09:28,320 --> 00:09:30,399
however this work is limited in three

250
00:09:30,399 --> 00:09:31,760
major aspects

251
00:09:31,760 --> 00:09:33,360
firstly scale

252
00:09:33,360 --> 00:09:35,440
our approach does not scale well as it

253
00:09:35,440 --> 00:09:38,080
would require two to the n combinations

254
00:09:38,080 --> 00:09:40,000
if we're monitoring end trackers and

255
00:09:40,000 --> 00:09:42,720
that number grows exponentially as we

256
00:09:42,720 --> 00:09:43,920
increase the number of trackers that

257
00:09:43,920 --> 00:09:46,080
we're monitoring

258
00:09:46,080 --> 00:09:48,880
secondly completeness of our ads so we

259
00:09:48,880 --> 00:09:51,360
annotate images as ads based on filter

260
00:09:51,360 --> 00:09:53,760
lists which is not a complete resource

261
00:09:53,760 --> 00:09:57,120
itself but it is as good as it gets

262
00:09:57,120 --> 00:10:00,000
and finally us our ad features are

263
00:10:00,000 --> 00:10:03,040
extremely simplistic we only use text

264
00:10:03,040 --> 00:10:05,040
but other complex features such as pixel

265
00:10:05,040 --> 00:10:07,120
entities sentiments can be used and they

266
00:10:07,120 --> 00:10:09,519
will only improve our performance our

267
00:10:09,519 --> 00:10:12,079
work shows that by using just simplistic

268
00:10:12,079 --> 00:10:13,839
features such as text we're able to

269
00:10:13,839 --> 00:10:15,040
achieve

270
00:10:15,040 --> 00:10:16,480
we're able to infer relationships that

271
00:10:16,480 --> 00:10:18,560
we've never seen before

272
00:10:18,560 --> 00:10:20,079
and with that i would like to make my

273
00:10:20,079 --> 00:10:22,160
concluding remarks that we have firstly

274
00:10:22,160 --> 00:10:24,480
established a generic framework to

275
00:10:24,480 --> 00:10:26,959
uncover data sharing relationships

276
00:10:26,959 --> 00:10:29,360
and that this facilitates the regulatory

277
00:10:29,360 --> 00:10:31,760
authorities by providing them a generic

278
00:10:31,760 --> 00:10:34,160
auditing framework that they can use to

279
00:10:34,160 --> 00:10:36,560
monitor a tracker or advertiser and

280
00:10:36,560 --> 00:10:38,480
their relationships and then hold the

281
00:10:38,480 --> 00:10:40,320
responsible people accountable or

282
00:10:40,320 --> 00:10:42,720
enforce their regulatory policies upon

283
00:10:42,720 --> 00:10:43,440
them

284
00:10:43,440 --> 00:10:45,200
with that i would like to end my talk

285
00:10:45,200 --> 00:10:46,959
the floor is open for any questions

286
00:10:46,959 --> 00:10:50,199
thank you

