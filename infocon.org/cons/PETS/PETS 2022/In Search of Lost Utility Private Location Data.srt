1
00:00:00,399 --> 00:00:02,399
hello everyone my name is sylvia

2
00:00:02,399 --> 00:00:04,720
laschtian and i'm going to present a

3
00:00:04,720 --> 00:00:05,680
paper

4
00:00:05,680 --> 00:00:08,480
titled in search of lost utility private

5
00:00:08,480 --> 00:00:10,160
location data

6
00:00:10,160 --> 00:00:13,120
which is a work done together with my

7
00:00:13,120 --> 00:00:17,199
advisors gergai arch and gary

8
00:00:17,199 --> 00:00:18,480
so our

9
00:00:18,480 --> 00:00:21,439
problem is based on location data about

10
00:00:21,439 --> 00:00:23,680
location data the only thing you can you

11
00:00:23,680 --> 00:00:26,480
have to think about now is just gps

12
00:00:26,480 --> 00:00:29,199
coordinates and time sims and a time

13
00:00:29,199 --> 00:00:30,880
series that you can get

14
00:00:30,880 --> 00:00:33,040
out of it so like traces when you go

15
00:00:33,040 --> 00:00:35,840
from one point to another and then uh

16
00:00:35,840 --> 00:00:38,079
the locations in between

17
00:00:38,079 --> 00:00:38,879
so

18
00:00:38,879 --> 00:00:41,520
uh inherently it's very hard to publish

19
00:00:41,520 --> 00:00:43,360
because it's sensitive

20
00:00:43,360 --> 00:00:44,399
and even

21
00:00:44,399 --> 00:00:47,360
even us researchers we struggle with

22
00:00:47,360 --> 00:00:50,079
publishing this kind of data sets and it

23
00:00:50,079 --> 00:00:51,120
has

24
00:00:51,120 --> 00:00:53,680
had problems in the past and even though

25
00:00:53,680 --> 00:00:56,000
i heard people uh

26
00:00:56,000 --> 00:00:58,480
complaining about it quite some uh quite

27
00:00:58,480 --> 00:01:00,559
a lot

28
00:01:00,559 --> 00:01:02,640
so we can't publish it because uh if

29
00:01:02,640 --> 00:01:04,479
it's just naively identified your

30
00:01:04,479 --> 00:01:07,520
identification is easy and

31
00:01:07,520 --> 00:01:09,760
anonymization is hard

32
00:01:09,760 --> 00:01:11,680
i'm going to show you why it is hard and

33
00:01:11,680 --> 00:01:14,640
i'm also going to show you our solution

34
00:01:14,640 --> 00:01:18,880
given to this problem our model which is

35
00:01:18,880 --> 00:01:21,439
scalable and more scalable than previous

36
00:01:21,439 --> 00:01:24,640
methods gives better utility and most

37
00:01:24,640 --> 00:01:28,720
importantly it includes time information

38
00:01:28,720 --> 00:01:31,759
and we call it dp lock

39
00:01:31,759 --> 00:01:34,960
okay so why re-identification easy there

40
00:01:34,960 --> 00:01:39,040
has been several works before however

41
00:01:39,040 --> 00:01:41,439
i think the most prominent one

42
00:01:41,439 --> 00:01:44,560
is uh this one on the screen

43
00:01:44,560 --> 00:01:46,159
which shows that

44
00:01:46,159 --> 00:01:50,799
in a large dataset of 1.5 million users

45
00:01:50,799 --> 00:01:52,560
it's still

46
00:01:52,560 --> 00:01:54,079
possible

47
00:01:54,079 --> 00:01:57,840
to uniquely re-identify 95

48
00:01:57,840 --> 00:02:02,000
of them using sign information

49
00:02:03,360 --> 00:02:04,399
so

50
00:02:04,399 --> 00:02:05,840
it's easy

51
00:02:05,840 --> 00:02:06,880
to

52
00:02:06,880 --> 00:02:10,160
re-identify people but it's very hard

53
00:02:10,160 --> 00:02:14,160
it's also because it's very hard to use

54
00:02:14,560 --> 00:02:16,800
formal methods because of cursive

55
00:02:16,800 --> 00:02:19,440
dimensionality so the larger your input

56
00:02:19,440 --> 00:02:22,840
space is

57
00:02:23,120 --> 00:02:25,920
uh the more problematic it is to to

58
00:02:25,920 --> 00:02:28,400
introduce noise to your data set

59
00:02:28,400 --> 00:02:31,519
and uh yes originally we can we know

60
00:02:31,519 --> 00:02:33,680
that ad hoc methods don't give any

61
00:02:33,680 --> 00:02:35,840
formal guarantee so this this is what

62
00:02:35,840 --> 00:02:36,879
happens

63
00:02:36,879 --> 00:02:39,760
this happens uh

64
00:02:39,760 --> 00:02:41,840
the re-identification happens after

65
00:02:41,840 --> 00:02:43,920
attack magnets quite often and you can't

66
00:02:43,920 --> 00:02:45,760
guarantee anything

67
00:02:45,760 --> 00:02:48,879
so there is a solution of generating

68
00:02:48,879 --> 00:02:50,480
synthetic data

69
00:02:50,480 --> 00:02:52,879
in a

70
00:02:53,360 --> 00:02:56,400
privacy preserving way so

71
00:02:56,400 --> 00:02:58,239
for example with differential privacy

72
00:02:58,239 --> 00:03:00,800
guarantee because it gives you a formal

73
00:03:00,800 --> 00:03:04,400
guarantee okay just a very quick recap

74
00:03:04,400 --> 00:03:06,800
of differential privacy for those who do

75
00:03:06,800 --> 00:03:08,959
not really understand it

76
00:03:08,959 --> 00:03:11,280
just remember that the first line on the

77
00:03:11,280 --> 00:03:13,760
slide the outcome of the anonymization

78
00:03:13,760 --> 00:03:16,400
scheme should be more or less

79
00:03:16,400 --> 00:03:18,239
independent of the value of a single

80
00:03:18,239 --> 00:03:20,319
record okay so we

81
00:03:20,319 --> 00:03:23,440
hide specific user specific information

82
00:03:23,440 --> 00:03:26,239
in a and it gives us a formal guarantee

83
00:03:26,239 --> 00:03:29,920
and a worst case guarantee

84
00:03:30,840 --> 00:03:34,000
okay so

85
00:03:34,000 --> 00:03:35,599
previous methods

86
00:03:35,599 --> 00:03:38,640
uh as i mentioned were not always

87
00:03:38,640 --> 00:03:42,879
scalable sometimes yes sometimes not

88
00:03:42,879 --> 00:03:44,239
usually give

89
00:03:44,239 --> 00:03:47,760
low utility in metrics that we use the

90
00:03:47,760 --> 00:03:49,599
same metrics in the paper as

91
00:03:49,599 --> 00:03:52,480
previous uh papers suggested

92
00:03:52,480 --> 00:03:54,560
in order to compare them

93
00:03:54,560 --> 00:03:55,680
and uh

94
00:03:55,680 --> 00:03:58,000
our method give give higher better

95
00:03:58,000 --> 00:03:59,360
utility

96
00:03:59,360 --> 00:04:01,519
but i think personally i think that the

97
00:04:01,519 --> 00:04:04,879
most important is that

98
00:04:04,879 --> 00:04:07,760
previous model previous matters didn't

99
00:04:07,760 --> 00:04:10,879
really generate realistic traces

100
00:04:10,879 --> 00:04:12,799
most of them

101
00:04:12,799 --> 00:04:15,280
used random walk methods

102
00:04:15,280 --> 00:04:17,680
and um

103
00:04:17,680 --> 00:04:20,720
it's it's it was very far from realistic

104
00:04:20,720 --> 00:04:23,280
uh realistic traces usually were not

105
00:04:23,280 --> 00:04:26,240
even long enough and and then they were

106
00:04:26,240 --> 00:04:29,280
they had an arbitrary ending

107
00:04:29,280 --> 00:04:31,440
but most so after all this most

108
00:04:31,440 --> 00:04:33,840
important is that none of them included

109
00:04:33,840 --> 00:04:36,000
time information and we did include it

110
00:04:36,000 --> 00:04:38,479
in our model it and did and it did

111
00:04:38,479 --> 00:04:39,759
improve

112
00:04:39,759 --> 00:04:41,600
accuracy in utility

113
00:04:41,600 --> 00:04:42,800
okay so a

114
00:04:42,800 --> 00:04:46,320
summary of our trace generation method

115
00:04:46,320 --> 00:04:47,280
first

116
00:04:47,280 --> 00:04:49,520
um i have to just look at the first line

117
00:04:49,520 --> 00:04:50,560
first

118
00:04:50,560 --> 00:04:53,840
uh it uh you see that we have four

119
00:04:53,840 --> 00:04:57,040
uh parts four sections first one is

120
00:04:57,040 --> 00:04:59,040
dimensionality reduction it's it's still

121
00:04:59,040 --> 00:05:00,960
a must you you still have to do it

122
00:05:00,960 --> 00:05:01,919
because

123
00:05:01,919 --> 00:05:04,080
uh

124
00:05:04,080 --> 00:05:05,039
the

125
00:05:05,039 --> 00:05:08,240
input of uh of a of a map of a large

126
00:05:08,240 --> 00:05:10,560
city the input space is thousands and

127
00:05:10,560 --> 00:05:13,120
thousands of cells if you put a grid

128
00:05:13,120 --> 00:05:13,919
over

129
00:05:13,919 --> 00:05:16,240
your map so see you still have to use

130
00:05:16,240 --> 00:05:18,160
some kind of dimensionality reduction so

131
00:05:18,160 --> 00:05:19,919
we got that first we done that first not

132
00:05:19,919 --> 00:05:21,039
a big deal

133
00:05:21,039 --> 00:05:22,400
and then

134
00:05:22,400 --> 00:05:24,880
the important part here

135
00:05:24,880 --> 00:05:28,160
is the second step that you take a

136
00:05:28,160 --> 00:05:30,400
source a destination and a time

137
00:05:30,400 --> 00:05:32,639
information from a trace

138
00:05:32,639 --> 00:05:34,400
you

139
00:05:34,400 --> 00:05:37,680
train a variational encoder on it

140
00:05:37,680 --> 00:05:39,919
so you're going to get in the end

141
00:05:39,919 --> 00:05:42,320
synthetic source destination and time

142
00:05:42,320 --> 00:05:43,520
information

143
00:05:43,520 --> 00:05:45,759
so we have now this and in the third

144
00:05:45,759 --> 00:05:48,320
step we generate

145
00:05:48,320 --> 00:05:50,840
the transition probability

146
00:05:50,840 --> 00:05:54,479
between each and any

147
00:05:54,479 --> 00:05:58,160
location in our input space

148
00:05:58,160 --> 00:06:00,880
so this is also a deep learning model

149
00:06:00,880 --> 00:06:03,440
with word and bending layer in it and

150
00:06:03,440 --> 00:06:07,039
the fully connected layer or maybe two

151
00:06:07,039 --> 00:06:10,160
the input is your current location your

152
00:06:10,160 --> 00:06:12,800
destination and the time of the day and

153
00:06:12,800 --> 00:06:14,400
the output

154
00:06:14,400 --> 00:06:17,440
is a probability distribution over all

155
00:06:17,440 --> 00:06:20,440
locations

156
00:06:20,560 --> 00:06:22,720
in your input space

157
00:06:22,720 --> 00:06:23,600
so

158
00:06:23,600 --> 00:06:26,800
using the generated synthetic source

159
00:06:26,800 --> 00:06:29,199
destination time and the transition

160
00:06:29,199 --> 00:06:32,880
probability we build a graph

161
00:06:32,880 --> 00:06:35,840
it's a it's a full graph so

162
00:06:35,840 --> 00:06:38,240
between any point there is a probability

163
00:06:38,240 --> 00:06:42,160
to move from one to another

164
00:06:42,560 --> 00:06:44,240
every node sorry

165
00:06:44,240 --> 00:06:45,280
and

166
00:06:45,280 --> 00:06:47,360
in order to generate a trace we have now

167
00:06:47,360 --> 00:06:49,520
the source and destination from the

168
00:06:49,520 --> 00:06:53,280
trace initializator so we run dijkstra's

169
00:06:53,280 --> 00:06:55,680
shortest path algorithm

170
00:06:55,680 --> 00:06:57,840
a little it's a little bit tweaked in

171
00:06:57,840 --> 00:07:00,960
order to get the most probable path

172
00:07:00,960 --> 00:07:03,280
between the two

173
00:07:03,280 --> 00:07:06,000
locations in and that has been generated

174
00:07:06,000 --> 00:07:09,039
by the variational autoencoder plus we

175
00:07:09,039 --> 00:07:10,479
introduced some randomness with

176
00:07:10,479 --> 00:07:12,960
metropolis hastings because dijkstra is

177
00:07:12,960 --> 00:07:14,400
of course it's the deterministic

178
00:07:14,400 --> 00:07:16,960
algorithm okay just quick one for the

179
00:07:16,960 --> 00:07:19,520
machine learning enthusiasts this is our

180
00:07:19,520 --> 00:07:22,400
variational and to auto encoder so again

181
00:07:22,400 --> 00:07:25,679
source destination time

182
00:07:26,160 --> 00:07:28,000
you generate the latent space how it

183
00:07:28,000 --> 00:07:30,240
works i'm not gonna go into virtual auto

184
00:07:30,240 --> 00:07:32,560
encoders now and then

185
00:07:32,560 --> 00:07:34,639
the final layer gives you the synthetic

186
00:07:34,639 --> 00:07:36,960
ones the next one is the transition

187
00:07:36,960 --> 00:07:38,800
probability generator you can see that

188
00:07:38,800 --> 00:07:41,120
we use the embedding layer 50

189
00:07:41,120 --> 00:07:43,199
dimensional vector on on

190
00:07:43,199 --> 00:07:45,840
locations and we didn't embed

191
00:07:45,840 --> 00:07:48,000
time uh in the word demanding just the

192
00:07:48,000 --> 00:07:50,319
location and the output is the

193
00:07:50,319 --> 00:07:52,800
probability distribution between

194
00:07:52,800 --> 00:07:54,479
locations

195
00:07:54,479 --> 00:07:57,199
okay so and then the last final step

196
00:07:57,199 --> 00:08:00,160
is again i show you a fully connected a

197
00:08:00,160 --> 00:08:03,280
full graph and then we run dijkstra's

198
00:08:03,280 --> 00:08:04,879
shortest path

199
00:08:04,879 --> 00:08:07,840
with metropolis hastings after it

200
00:08:07,840 --> 00:08:10,240
okay so but this is also it has been

201
00:08:10,240 --> 00:08:12,240
very nice it was just simply synthetic

202
00:08:12,240 --> 00:08:15,520
data generation but we also have to

203
00:08:15,520 --> 00:08:18,400
introduce differential private noise and

204
00:08:18,400 --> 00:08:20,639
it you have to introduce it

205
00:08:20,639 --> 00:08:24,319
everywhere where your model uses the

206
00:08:24,319 --> 00:08:26,800
original row data

207
00:08:26,800 --> 00:08:28,639
so in the first part dimensional

208
00:08:28,639 --> 00:08:30,319
reduction we use the original we

209
00:08:30,319 --> 00:08:33,519
introduced some noise it gaussian noise

210
00:08:33,519 --> 00:08:34,880
um

211
00:08:34,880 --> 00:08:36,799
and then the trade the variational auto

212
00:08:36,799 --> 00:08:38,080
encoder

213
00:08:38,080 --> 00:08:38,839
and

214
00:08:38,839 --> 00:08:42,000
the the probability the transition

215
00:08:42,000 --> 00:08:44,640
probability generator models they were

216
00:08:44,640 --> 00:08:47,839
both trained with um

217
00:08:47,839 --> 00:08:50,240
dpsgds the differential private storage

218
00:08:50,240 --> 00:08:53,680
degradation descent so the training of

219
00:08:53,680 --> 00:08:55,440
both of these algorithms are

220
00:08:55,440 --> 00:08:58,640
differentially private thus using them

221
00:08:58,640 --> 00:09:00,800
later the already trained algorithms

222
00:09:00,800 --> 00:09:03,760
going to be differentially they're going

223
00:09:03,760 --> 00:09:07,440
to guarantee differential privacy

224
00:09:07,440 --> 00:09:10,080
okay and now in the first in the last

225
00:09:10,080 --> 00:09:12,640
part at the trace generation we don't

226
00:09:12,640 --> 00:09:15,200
need to introduce any more noise

227
00:09:15,200 --> 00:09:18,880
because this part doesn't use any of the

228
00:09:18,880 --> 00:09:21,279
original data

229
00:09:21,279 --> 00:09:25,519
okay we calculated um

230
00:09:25,920 --> 00:09:28,480
the amount of noise we have to use with

231
00:09:28,480 --> 00:09:30,480
the moment's accountant method it's just

232
00:09:30,480 --> 00:09:32,000
for the enthusiasts

233
00:09:32,000 --> 00:09:34,959
okay so what we got in the end we see we

234
00:09:34,959 --> 00:09:37,360
could get better utility in several

235
00:09:37,360 --> 00:09:38,880
metrics such as

236
00:09:38,880 --> 00:09:42,080
trace length visit density source

237
00:09:42,080 --> 00:09:44,959
destination source destination density

238
00:09:44,959 --> 00:09:47,920
and frequent patterns we calculated it

239
00:09:47,920 --> 00:09:51,839
for pair r and overall for the whole

240
00:09:51,839 --> 00:09:53,920
for a whole day

241
00:09:53,920 --> 00:09:55,920
and of course we cannot compare

242
00:09:55,920 --> 00:09:58,640
the hourly distribution

243
00:09:58,640 --> 00:10:01,760
to previous models which didn't use time

244
00:10:01,760 --> 00:10:02,480
but

245
00:10:02,480 --> 00:10:05,360
even though those showed really nice

246
00:10:05,360 --> 00:10:08,560
results the overall

247
00:10:08,560 --> 00:10:11,120
overall utility for a whole day without

248
00:10:11,120 --> 00:10:14,160
time information it still gave better

249
00:10:14,160 --> 00:10:16,959
results in the majority of the cases and

250
00:10:16,959 --> 00:10:18,800
below you can see

251
00:10:18,800 --> 00:10:21,519
just uh this is i think there's the

252
00:10:21,519 --> 00:10:23,120
visit density

253
00:10:23,120 --> 00:10:25,760
and the original next to it our model

254
00:10:25,760 --> 00:10:27,680
the ap lock and then another trace and

255
00:10:27,680 --> 00:10:30,240
the angular models output okay so what's

256
00:10:30,240 --> 00:10:32,800
the conclusion so what we did again just

257
00:10:32,800 --> 00:10:35,440
we did synthetic location uh data

258
00:10:35,440 --> 00:10:38,800
generation with differential privacy and

259
00:10:38,800 --> 00:10:41,279
we learned that anonymization with

260
00:10:41,279 --> 00:10:45,040
formal guarantee is hard but

261
00:10:45,040 --> 00:10:47,760
machine learning methods can help

262
00:10:47,760 --> 00:10:49,360
machine learning methods like deep

263
00:10:49,360 --> 00:10:51,440
learning what we used

264
00:10:51,440 --> 00:10:54,320
find specific specific stable

265
00:10:54,320 --> 00:10:57,200
user features in a high dimensional pre

266
00:10:57,200 --> 00:10:59,600
space that it otherwise is very hard to

267
00:10:59,600 --> 00:11:02,079
find and you must introduce a lot of

268
00:11:02,079 --> 00:11:04,079
noise otherwise so machine learning

269
00:11:04,079 --> 00:11:05,440
helps great

270
00:11:05,440 --> 00:11:07,519
so what we want to reach we want to we

271
00:11:07,519 --> 00:11:09,360
want to re we want to facilitate

272
00:11:09,360 --> 00:11:12,000
location data sharing among scientists

273
00:11:12,000 --> 00:11:15,279
among industrial people uh and this like

274
00:11:15,279 --> 00:11:17,839
uh just a copy was like a perfect

275
00:11:17,839 --> 00:11:20,000
example of it of course

276
00:11:20,000 --> 00:11:22,560
that is a very hard to share data

277
00:11:22,560 --> 00:11:24,800
so we hope in the future that our

278
00:11:24,800 --> 00:11:27,760
method will facilitate this process

279
00:11:27,760 --> 00:11:29,440
thank you very much for your attention

280
00:11:29,440 --> 00:11:33,880
and i'm happy to answer any questions

