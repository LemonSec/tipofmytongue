1
00:00:00,320 --> 00:00:02,639
hello and um thank you for watching this

2
00:00:02,639 --> 00:00:04,560
presentation my name is george i am

3
00:00:04,560 --> 00:00:06,879
i am the last author of the paper

4
00:00:06,879 --> 00:00:08,400
neurophysics track there's a secure way

5
00:00:08,400 --> 00:00:09,760
to use artificial neural networks for

6
00:00:09,760 --> 00:00:12,240
biometric user user authentication

7
00:00:12,240 --> 00:00:14,799
so the paper deals with how to do secure

8
00:00:14,799 --> 00:00:17,440
bios authentication and um

9
00:00:17,440 --> 00:00:18,720
we know that secure environment

10
00:00:18,720 --> 00:00:20,800
authentication can be done with fuzzy

11
00:00:20,800 --> 00:00:22,640
extractors uh specifically a variation

12
00:00:22,640 --> 00:00:24,480
of fuzz extractors

13
00:00:24,480 --> 00:00:26,720
so that we avoid storing representative

14
00:00:26,720 --> 00:00:28,640
biometrics from each user for example a

15
00:00:28,640 --> 00:00:30,480
database of fingerprints where each user

16
00:00:30,480 --> 00:00:32,320
would have one or a few fingerprints

17
00:00:32,320 --> 00:00:33,360
stored

18
00:00:33,360 --> 00:00:34,640
so we want to avoid that so for that

19
00:00:34,640 --> 00:00:36,239
reason we'd like to only store hashes

20
00:00:36,239 --> 00:00:37,600
for example a representative is

21
00:00:37,600 --> 00:00:39,120
fingerprint and

22
00:00:39,120 --> 00:00:41,120
such a mechanism would not allow

23
00:00:41,120 --> 00:00:43,120
attacker to reproduce a biometric would

24
00:00:43,120 --> 00:00:44,879
not allow them to to authenticate as a

25
00:00:44,879 --> 00:00:46,320
legitimate user

26
00:00:46,320 --> 00:00:48,399
um an attacker with access to this to

27
00:00:48,399 --> 00:00:50,000
the system who could potentially look

28
00:00:50,000 --> 00:00:51,520
into the database if such a database

29
00:00:51,520 --> 00:00:53,199
were available and so this is very

30
00:00:53,199 --> 00:00:55,039
similar to to how passwords are handled

31
00:00:55,039 --> 00:00:56,320
passwords are not stored in a database

32
00:00:56,320 --> 00:00:58,079
but rather only some salted hashes of

33
00:00:58,079 --> 00:00:59,760
the passwords are stored

34
00:00:59,760 --> 00:01:01,199
the problem with secure uh physics

35
00:01:01,199 --> 00:01:02,879
extractors with passive services is that

36
00:01:02,879 --> 00:01:05,119
they exhibit relatively low performance

37
00:01:05,119 --> 00:01:07,280
and it turns out that scorebase

38
00:01:07,280 --> 00:01:09,360
techniques do a lot better and so what

39
00:01:09,360 --> 00:01:10,640
about neural network based biometric

40
00:01:10,640 --> 00:01:11,920
authentication well

41
00:01:11,920 --> 00:01:14,479
they don't store a database of um

42
00:01:14,479 --> 00:01:16,560
representative biometrics but rather

43
00:01:16,560 --> 00:01:18,000
they store a classifier so there's got

44
00:01:18,000 --> 00:01:19,680
to be a classifier that's stored on the

45
00:01:19,680 --> 00:01:21,280
authentication server somewhere

46
00:01:21,280 --> 00:01:22,720
that will take the user's biometric and

47
00:01:22,720 --> 00:01:24,000
produce an answer whether this is the

48
00:01:24,000 --> 00:01:25,680
authenticator or not and so if an

49
00:01:25,680 --> 00:01:27,040
attacker has access to this classifier

50
00:01:27,040 --> 00:01:28,479
they could easily invert the classifier

51
00:01:28,479 --> 00:01:30,159
and produce an

52
00:01:30,159 --> 00:01:31,600
input that would then allow them to

53
00:01:31,600 --> 00:01:34,640
authenticate as a specific user

54
00:01:34,640 --> 00:01:37,040
so the motivation for this paper

55
00:01:37,040 --> 00:01:40,159
is the question whether we can combine

56
00:01:40,159 --> 00:01:42,079
the uh performance of neural network

57
00:01:42,079 --> 00:01:43,200
based by maximum authentication with the

58
00:01:43,200 --> 00:01:44,240
security

59
00:01:44,240 --> 00:01:46,720
of fuzzy extractors and to solve this uh

60
00:01:46,720 --> 00:01:49,119
issue we first asked well why do fuzzy

61
00:01:49,119 --> 00:01:50,720
extractors exhibit bad performance when

62
00:01:50,720 --> 00:01:52,640
compared to other methods and can we

63
00:01:52,640 --> 00:01:54,159
change something can we modify perhaps

64
00:01:54,159 --> 00:01:55,520
the input to the physics directors in

65
00:01:55,520 --> 00:01:57,759
order to um avoid the performance

66
00:01:57,759 --> 00:01:59,360
degradation and so first we have to

67
00:01:59,360 --> 00:02:00,799
understand how fast extract works and

68
00:02:00,799 --> 00:02:02,240
fuzzy structures work and a physics

69
00:02:02,240 --> 00:02:03,520
track is basically composed of a secure

70
00:02:03,520 --> 00:02:05,360
sketch and some hash function

71
00:02:05,360 --> 00:02:07,840
and the secure sketch is technically a

72
00:02:07,840 --> 00:02:10,000
decoding mechanism just like an error

73
00:02:10,000 --> 00:02:12,239
correction code that needs to be decoded

74
00:02:12,239 --> 00:02:13,920
so in this figure on the left you can

75
00:02:13,920 --> 00:02:15,760
see different shapes different colors

76
00:02:15,760 --> 00:02:17,760
each shape represents a specific user so

77
00:02:17,760 --> 00:02:19,520
you have an output space perhaps of a

78
00:02:19,520 --> 00:02:22,080
neural network a vector space where the

79
00:02:22,080 --> 00:02:24,160
representation for each user has a

80
00:02:24,160 --> 00:02:25,680
center shape or column and you can see

81
00:02:25,680 --> 00:02:27,680
for example that the blue triangles at

82
00:02:27,680 --> 00:02:29,840
the bottom right of the first figure

83
00:02:29,840 --> 00:02:31,040
represent a certain user and they

84
00:02:31,040 --> 00:02:32,560
cluster together kind of like in a

85
00:02:32,560 --> 00:02:34,319
sphere and hypersphere if you think in a

86
00:02:34,319 --> 00:02:36,160
high dimensional vector space and it

87
00:02:36,160 --> 00:02:38,160
turns out that in the scenario most

88
00:02:38,160 --> 00:02:40,720
users embeddings or presentations in

89
00:02:40,720 --> 00:02:41,920
this vector space kind of cluster

90
00:02:41,920 --> 00:02:44,239
together in a nice hypersphere and then

91
00:02:44,239 --> 00:02:45,840
what you do is you you take the center

92
00:02:45,840 --> 00:02:47,040
of this hypersphere say that's a

93
00:02:47,040 --> 00:02:49,280
representative of the user and then you

94
00:02:49,280 --> 00:02:51,040
define a code book on this entire space

95
00:02:51,040 --> 00:02:52,319
and the decoding region of this code

96
00:02:52,319 --> 00:02:54,160
book has to be pretty much the same as

97
00:02:54,160 --> 00:02:56,560
the um the same size as the sphere that

98
00:02:56,560 --> 00:02:58,959
encapsulates um the legitimate user's

99
00:02:58,959 --> 00:03:01,440
biometric representations or embeddings

100
00:03:01,440 --> 00:03:03,599
and so how the skip sketch works is that

101
00:03:03,599 --> 00:03:05,680
you choose randomly code word from the

102
00:03:05,680 --> 00:03:07,519
code box so here the code book is all

103
00:03:07,519 --> 00:03:09,599
the set the entire set of black dots and

104
00:03:09,599 --> 00:03:11,680
each black dot has this decoding region

105
00:03:11,680 --> 00:03:13,040
around it

106
00:03:13,040 --> 00:03:14,640
so that if we are presented with

107
00:03:14,640 --> 00:03:16,400
something that's inside a certain

108
00:03:16,400 --> 00:03:17,760
decoding region that would decode it to

109
00:03:17,760 --> 00:03:19,200
the center of that region to the closest

110
00:03:19,200 --> 00:03:21,440
code word and so what we do is we choose

111
00:03:21,440 --> 00:03:22,959
a random code and for our purposes we

112
00:03:22,959 --> 00:03:24,080
could simply take this representative

113
00:03:24,080 --> 00:03:25,840
and decode it and produce the code in

114
00:03:25,840 --> 00:03:27,760
that manner but that's not necessary

115
00:03:27,760 --> 00:03:30,319
and so we only we calculated the

116
00:03:30,319 --> 00:03:31,280
distance vector between the

117
00:03:31,280 --> 00:03:33,360
representative and that code word and we

118
00:03:33,360 --> 00:03:34,799
only store this distance vector so it's

119
00:03:34,799 --> 00:03:37,360
secure this way because uh given only

120
00:03:37,360 --> 00:03:39,280
the distance vector an attacker has no

121
00:03:39,280 --> 00:03:42,000
way of knowing to what code word

122
00:03:42,000 --> 00:03:43,599
this distance vector refers to and so

123
00:03:43,599 --> 00:03:45,680
they cannot find the representative so

124
00:03:45,680 --> 00:03:47,360
the attacker's entropy if you want is

125
00:03:47,360 --> 00:03:49,040
given by the number of coders that fit

126
00:03:49,040 --> 00:03:51,599
into the space and so how do we recover

127
00:03:51,599 --> 00:03:53,360
this representative

128
00:03:53,360 --> 00:03:55,680
uh given another input by metric that's

129
00:03:55,680 --> 00:03:56,720
that's slightly different than that

130
00:03:56,720 --> 00:03:58,080
represented but you can see here in the

131
00:03:58,080 --> 00:04:00,319
uh right hand side figure there's this

132
00:04:00,319 --> 00:04:02,000
blue triangle it's a new biometric entry

133
00:04:02,000 --> 00:04:03,920
from the same user and it happens to be

134
00:04:03,920 --> 00:04:05,760
within the decoding region

135
00:04:05,760 --> 00:04:06,959
around the representative of course

136
00:04:06,959 --> 00:04:08,319
there's not a property coding region so

137
00:04:08,319 --> 00:04:10,959
we take that uh input and we add the the

138
00:04:10,959 --> 00:04:12,400
the difference values as well yeah we

139
00:04:12,400 --> 00:04:13,840
subtract the respect from it and we

140
00:04:13,840 --> 00:04:15,920
obtain uh some other point then that

141
00:04:15,920 --> 00:04:17,440
point with the code to the closest code

142
00:04:17,440 --> 00:04:18,880
or so we recover the code that we chose

143
00:04:18,880 --> 00:04:20,880
originally and then uh we we add the

144
00:04:20,880 --> 00:04:22,560
distance vector to that code and we

145
00:04:22,560 --> 00:04:24,240
recover the origin representative and

146
00:04:24,240 --> 00:04:25,520
then we can store the hash of that

147
00:04:25,520 --> 00:04:27,120
representative and then every time the

148
00:04:27,120 --> 00:04:28,479
user inputs in your biometric it should

149
00:04:28,479 --> 00:04:29,600
be close to that representative so we

150
00:04:29,600 --> 00:04:30,800
can recover the representative and then

151
00:04:30,800 --> 00:04:32,000
we can produce the hash and we can

152
00:04:32,000 --> 00:04:33,600
authenticate with a hash so that's how

153
00:04:33,600 --> 00:04:35,120
the secure sketch works the problem of

154
00:04:35,120 --> 00:04:37,040
course is that

155
00:04:37,040 --> 00:04:38,960
decoding generally

156
00:04:38,960 --> 00:04:41,280
implies especially in a

157
00:04:41,280 --> 00:04:43,440
in a continuous vector space like say r

158
00:04:43,440 --> 00:04:46,160
to the power n uh decoding implies uh

159
00:04:46,160 --> 00:04:47,440
spherical usually spherically decoding

160
00:04:47,440 --> 00:04:49,199
regions again there's a problem now if

161
00:04:49,199 --> 00:04:51,280
the outputs of if the imbalance of all

162
00:04:51,280 --> 00:04:52,800
the users and this vector space cluster

163
00:04:52,800 --> 00:04:54,080
nicely in spherical regions that

164
00:04:54,080 --> 00:04:55,120
shouldn't be a problem but in reality

165
00:04:55,120 --> 00:04:56,720
that's usually not the case reality can

166
00:04:56,720 --> 00:04:58,639
have a situation like the bottom left

167
00:04:58,639 --> 00:05:00,240
figure where you can see that the blue

168
00:05:00,240 --> 00:05:03,039
triangles and the orange pentagons

169
00:05:03,039 --> 00:05:04,800
they're clearly clustering in distinct

170
00:05:04,800 --> 00:05:06,240
clusters and you can think well yeah it

171
00:05:06,240 --> 00:05:07,440
would be relatively easy to find the

172
00:05:07,440 --> 00:05:08,720
separation between these clusters and

173
00:05:08,720 --> 00:05:10,000
net neural networks i'm very good at

174
00:05:10,000 --> 00:05:11,199
doing that

175
00:05:11,199 --> 00:05:12,320
but

176
00:05:12,320 --> 00:05:14,080
they are no longer aminable to spherical

177
00:05:14,080 --> 00:05:15,680
decoding regions so that's the problem

178
00:05:15,680 --> 00:05:17,520
how do we use the coding which forces

179
00:05:17,520 --> 00:05:19,120
spherically regions

180
00:05:19,120 --> 00:05:20,960
with the coding regions that are highly

181
00:05:20,960 --> 00:05:23,440
non-regular like the bottom right figure

182
00:05:23,440 --> 00:05:25,680
we could technically take the embeddings

183
00:05:25,680 --> 00:05:27,440
of the users and process them further to

184
00:05:27,440 --> 00:05:29,199
cause them to cluster in distinct fields

185
00:05:29,199 --> 00:05:31,360
and that's how uh we do it we actually

186
00:05:31,360 --> 00:05:33,440
take the artificial network and then we

187
00:05:33,440 --> 00:05:35,440
extend it or we fit it with an expander

188
00:05:35,440 --> 00:05:36,720
and this is the purpose of the expander

189
00:05:36,720 --> 00:05:38,400
is to take the the say the coding

190
00:05:38,400 --> 00:05:40,720
regions or the clusters uh shapes uh

191
00:05:40,720 --> 00:05:42,400
that occur naturally at the artificial

192
00:05:42,400 --> 00:05:43,680
neural network output each classic

193
00:05:43,680 --> 00:05:45,280
respond to a different user and to push

194
00:05:45,280 --> 00:05:46,720
them into spherical distinct spherical

195
00:05:46,720 --> 00:05:47,919
regions and the output expander

196
00:05:47,919 --> 00:05:50,080
hopefully will have this property where

197
00:05:50,080 --> 00:05:51,759
all the users embeddings were clustered

198
00:05:51,759 --> 00:05:53,120
nicely into this things fears and if

199
00:05:53,120 --> 00:05:54,639
that's the case then when we apply the

200
00:05:54,639 --> 00:05:56,800
secure sketch with the uh decoding

201
00:05:56,800 --> 00:05:58,639
mechanism that that kind of uses these

202
00:05:58,639 --> 00:06:00,479
uh spherical regions there will be no

203
00:06:00,479 --> 00:06:01,759
loss of performance because the

204
00:06:01,759 --> 00:06:03,600
embeddings already clustered its spheres

205
00:06:03,600 --> 00:06:05,039
so this is the neural fuzzy structure

206
00:06:05,039 --> 00:06:06,639
architecture in addition to the expander

207
00:06:06,639 --> 00:06:07,759
here we have the secure sketch and then

208
00:06:07,759 --> 00:06:09,360
the hash function of course

209
00:06:09,360 --> 00:06:10,800
the comparison module

210
00:06:10,800 --> 00:06:12,639
so how do we build the expander well we

211
00:06:12,639 --> 00:06:14,240
want to build it using the neural

212
00:06:14,240 --> 00:06:16,000
network and

213
00:06:16,000 --> 00:06:18,880
one way that we found that works for

214
00:06:18,880 --> 00:06:21,280
specifying the loss function is uh using

215
00:06:21,280 --> 00:06:22,800
a triplet loss that's associated usually

216
00:06:22,800 --> 00:06:24,479
with this um siamese neural network

217
00:06:24,479 --> 00:06:26,080
architecture so we use a simus

218
00:06:26,080 --> 00:06:28,080
neurometric training architecture uh in

219
00:06:28,080 --> 00:06:30,080
in which we choose an anchor for each

220
00:06:30,080 --> 00:06:31,919
user for example uh an input by method

221
00:06:31,919 --> 00:06:33,120
which is called the anchor and then we

222
00:06:33,120 --> 00:06:35,600
feed um into one copy of the neural

223
00:06:35,600 --> 00:06:37,680
network we feed that anchor and then

224
00:06:37,680 --> 00:06:39,199
into another copy we feed a positive

225
00:06:39,199 --> 00:06:40,880
input which is also another binary from

226
00:06:40,880 --> 00:06:42,160
the same user and then into another copy

227
00:06:42,160 --> 00:06:43,919
of the internet with a negative input

228
00:06:43,919 --> 00:06:45,280
which is a biometric from a different

229
00:06:45,280 --> 00:06:47,280
user and the loss function is based on

230
00:06:47,280 --> 00:06:48,800
the output of all three neural networks

231
00:06:48,800 --> 00:06:51,199
and it aims to decrease the distance

232
00:06:51,199 --> 00:06:52,560
between the outputs of the first two

233
00:06:52,560 --> 00:06:54,160
neural networks uh which take the anchor

234
00:06:54,160 --> 00:06:55,680
and the positive and to increase as much

235
00:06:55,680 --> 00:06:57,280
as possible the distance between the uh

236
00:06:57,280 --> 00:06:58,479
output of the neural network in the

237
00:06:58,479 --> 00:06:59,840
anchor and the output of the neural

238
00:06:59,840 --> 00:07:01,360
network taking the negative example so

239
00:07:01,360 --> 00:07:03,280
as uh depicted here on the right hand

240
00:07:03,280 --> 00:07:05,360
side you can see that after learning

241
00:07:05,360 --> 00:07:06,479
basically the distance between anchor

242
00:07:06,479 --> 00:07:07,680
and negative is supposed to increase

243
00:07:07,680 --> 00:07:08,880
while the distance between a composite

244
00:07:08,880 --> 00:07:10,160
is supposed to decrease so that

245
00:07:10,160 --> 00:07:12,240
naturally turns out to kind of cluster

246
00:07:12,240 --> 00:07:15,039
um user embeddings into distinct and you

247
00:07:15,039 --> 00:07:16,720
know space as far away as possible

248
00:07:16,720 --> 00:07:17,759
spheres

249
00:07:17,759 --> 00:07:19,520
and so what about the secure sketch what

250
00:07:19,520 --> 00:07:20,960
we do for the secure sketch well we do

251
00:07:20,960 --> 00:07:22,400
two things first for a baseline

252
00:07:22,400 --> 00:07:23,840
comparison we use a distance-based code

253
00:07:23,840 --> 00:07:25,199
because we thought well if we're going

254
00:07:25,199 --> 00:07:26,960
to see some degradation it could be

255
00:07:26,960 --> 00:07:28,080
because of two reasons it could be a

256
00:07:28,080 --> 00:07:29,680
degradation because of the code itself

257
00:07:29,680 --> 00:07:31,199
that we use or it could be a degradation

258
00:07:31,199 --> 00:07:32,319
because the expander doesn't do a good

259
00:07:32,319 --> 00:07:34,639
job uh forming spherical regions and so

260
00:07:34,639 --> 00:07:36,400
we wanted to do a compressor so we used

261
00:07:36,400 --> 00:07:38,319
a distance-based decoder so this one

262
00:07:38,319 --> 00:07:39,919
requires representatives to be supported

263
00:07:39,919 --> 00:07:41,680
for each user so clearly we cannot

264
00:07:41,680 --> 00:07:43,840
function with our secure fuzzy extractor

265
00:07:43,840 --> 00:07:46,319
architecture because we would need to to

266
00:07:46,319 --> 00:07:47,599
uh store representatives so that kind of

267
00:07:47,599 --> 00:07:48,879
defeats the purpose so the distance

268
00:07:48,879 --> 00:07:50,560
based code is only for uh comparison

269
00:07:50,560 --> 00:07:52,720
purposes so uh how this is going to work

270
00:07:52,720 --> 00:07:54,800
in reality it's going to use a code a

271
00:07:54,800 --> 00:07:56,160
decoder the decoder of a child

272
00:07:56,160 --> 00:07:57,599
correction code

273
00:07:57,599 --> 00:07:58,960
among these codes are the low density

274
00:07:58,960 --> 00:08:00,639
lattice codes or ldl c codes there are

275
00:08:00,639 --> 00:08:02,400
other codes as well like polar codes do

276
00:08:02,400 --> 00:08:04,240
pretty well um getting close capacity

277
00:08:04,240 --> 00:08:05,599
but we only tried low density lattice

278
00:08:05,599 --> 00:08:07,199
codes and

279
00:08:07,199 --> 00:08:09,120
they so so because they work pretty well

280
00:08:09,120 --> 00:08:10,560
then they should do similarly to

281
00:08:10,560 --> 00:08:11,680
business-based coding they should kind

282
00:08:11,680 --> 00:08:13,840
of form a spherical

283
00:08:13,840 --> 00:08:15,599
decoding regions

284
00:08:15,599 --> 00:08:18,479
and so the ldl c code the ldlc's or the

285
00:08:18,479 --> 00:08:19,840
low-density status codes are defined by

286
00:08:19,840 --> 00:08:21,280
temperature matrix that's parse hence

287
00:08:21,280 --> 00:08:23,759
the the word low density uh our uh

288
00:08:23,759 --> 00:08:26,160
perfect check matrix h is 120 by 128 so

289
00:08:26,160 --> 00:08:27,680
we use an expander with output of size

290
00:08:27,680 --> 00:08:29,919
128 and then each row in each column

291
00:08:29,919 --> 00:08:32,159
consists of a bunch of zeros and a few

292
00:08:32,159 --> 00:08:33,919
elements uh and we choose it there's

293
00:08:33,919 --> 00:08:34,958
several ways there are several ways to

294
00:08:34,958 --> 00:08:36,320
choose these items but we chose them

295
00:08:36,320 --> 00:08:37,440
according to the original paper that

296
00:08:37,440 --> 00:08:39,039
introduces low dangerous codes i would

297
00:08:39,039 --> 00:08:40,479
choose them as different versions of the

298
00:08:40,479 --> 00:08:42,080
set one half one third one fifth one

299
00:08:42,080 --> 00:08:43,760
seventh one eleventh one thirteenth so

300
00:08:43,760 --> 00:08:45,279
four so for example experimented with

301
00:08:45,279 --> 00:08:46,800
three and five weights meaning that for

302
00:08:46,800 --> 00:08:48,560
d equal to three we only use the first

303
00:08:48,560 --> 00:08:49,920
three elements so one over two thirty

304
00:08:49,920 --> 00:08:51,440
one one over three seven one over five

305
00:08:51,440 --> 00:08:52,880
eleven in this case and the rest of the

306
00:08:52,880 --> 00:08:54,560
elements of the columns and the rows are

307
00:08:54,560 --> 00:08:55,760
zero so in each column in each row there

308
00:08:55,760 --> 00:08:57,440
are exactly these three elements in

309
00:08:57,440 --> 00:08:59,200
different positions so we tested the

310
00:08:59,200 --> 00:09:00,959
codes and you can see here two uh

311
00:09:00,959 --> 00:09:02,560
diagrams showing the performance and uh

312
00:09:02,560 --> 00:09:04,320
they look pretty similarly in fact it

313
00:09:04,320 --> 00:09:05,519
turns out that the equal to three kind

314
00:09:05,519 --> 00:09:06,640
of performs a little bit better so we

315
00:09:06,640 --> 00:09:07,600
just went with equal to three it's

316
00:09:07,600 --> 00:09:08,640
easier

317
00:09:08,640 --> 00:09:11,120
and h is sparser now the generator

318
00:09:11,120 --> 00:09:12,480
matrix of the code is this g which is

319
00:09:12,480 --> 00:09:14,080
the inverse of the of the apparent check

320
00:09:14,080 --> 00:09:16,320
matrix so given a message you represent

321
00:09:16,320 --> 00:09:18,880
a message as a vector of integers

322
00:09:18,880 --> 00:09:20,800
and then uh let's call this vector b and

323
00:09:20,800 --> 00:09:22,880
then the code is x equal to g times b so

324
00:09:22,880 --> 00:09:24,560
now given a noisy version of x say x

325
00:09:24,560 --> 00:09:25,680
prime which in our case would correspond

326
00:09:25,680 --> 00:09:27,040
x would correspond to the representative

327
00:09:27,040 --> 00:09:28,880
biomaterial x prime would be a new entry

328
00:09:28,880 --> 00:09:30,080
a new biometric from the same user so

329
00:09:30,080 --> 00:09:32,080
given a noisy version x power of x then

330
00:09:32,080 --> 00:09:33,839
you have to find a vector of integer

331
00:09:33,839 --> 00:09:35,839
that's b prime that's the most likely to

332
00:09:35,839 --> 00:09:37,680
have generated this version of x prime

333
00:09:37,680 --> 00:09:39,680
and for our experiment we used two

334
00:09:39,680 --> 00:09:40,800
fingerprint databases we went with

335
00:09:40,800 --> 00:09:42,240
fingerprints they're rather easy to

336
00:09:42,240 --> 00:09:44,720
process the design can work very well we

337
00:09:44,720 --> 00:09:47,279
believe with any type of biometrics

338
00:09:47,279 --> 00:09:49,680
and so we use two fingerprint databases

339
00:09:49,680 --> 00:09:52,640
the fvg2006 database has about 150

340
00:09:52,640 --> 00:09:53,760
fingers twelve impressions with fingers

341
00:09:53,760 --> 00:09:55,360
and a polyu database which has pretty

342
00:09:55,360 --> 00:09:56,800
much double the number of fingers but

343
00:09:56,800 --> 00:09:58,240
only six impressions were fingered uh we

344
00:09:58,240 --> 00:09:59,360
used three different classifier

345
00:09:59,360 --> 00:10:01,040
architectures that are pretty well um

346
00:10:01,040 --> 00:10:03,279
established vgg16 resonant 15 mobile net

347
00:10:03,279 --> 00:10:04,720
and then we can

348
00:10:04,720 --> 00:10:06,880
we we concocted four experiments we have

349
00:10:06,880 --> 00:10:08,240
a baseline classified experiment with a

350
00:10:08,240 --> 00:10:09,519
classified training assignment network

351
00:10:09,519 --> 00:10:11,519
configuration uh and then casper

352
00:10:11,519 --> 00:10:12,560
training assigns another configuration

353
00:10:12,560 --> 00:10:13,680
to which we had expanded and we want to

354
00:10:13,680 --> 00:10:15,920
train the expander and then we take both

355
00:10:15,920 --> 00:10:17,040
the classifier and expander and we'll

356
00:10:17,040 --> 00:10:18,079
train them both simultaneously and

357
00:10:18,079 --> 00:10:19,440
assignment configuration baseline

358
00:10:19,440 --> 00:10:20,560
classifier is just for baseline

359
00:10:20,560 --> 00:10:22,560
comparison it's a classifier as it was

360
00:10:22,560 --> 00:10:23,680
originally designed but of course

361
00:10:23,680 --> 00:10:25,040
adapted to the fingerprint to the

362
00:10:25,040 --> 00:10:27,120
databases so we did

363
00:10:27,120 --> 00:10:29,920
learn the classifier on our databases um

364
00:10:29,920 --> 00:10:34,399
and then it turns out that our our um

365
00:10:34,399 --> 00:10:35,600
semi's natural configuration was

366
00:10:35,600 --> 00:10:37,360
achieving much better results so we

367
00:10:37,360 --> 00:10:38,800
actually took the baseline classifier

368
00:10:38,800 --> 00:10:39,760
and trained it in assignment

369
00:10:39,760 --> 00:10:42,000
configurations to make things fair more

370
00:10:42,000 --> 00:10:44,800
fair uh the questions more fair and then

371
00:10:44,800 --> 00:10:46,720
uh clearly we expect that the last

372
00:10:46,720 --> 00:10:48,880
scenario which trains the classified

373
00:10:48,880 --> 00:10:50,160
expansion together should perform the

374
00:10:50,160 --> 00:10:52,880
best um but it turns out this

375
00:10:52,880 --> 00:10:54,399
depends on the situation

376
00:10:54,399 --> 00:10:56,079
and then the third the third one is the

377
00:10:56,079 --> 00:10:57,200
more realistic one where you already

378
00:10:57,200 --> 00:10:58,800
have a trained classifier and just stick

379
00:10:58,800 --> 00:11:00,720
on the expander you know trying to

380
00:11:00,720 --> 00:11:02,079
expand this we're most interested in how

381
00:11:02,079 --> 00:11:04,320
that performs and here are some of the

382
00:11:04,320 --> 00:11:06,640
um results we we have plotted here the

383
00:11:06,640 --> 00:11:08,160
equilary then the area under the rbc

384
00:11:08,160 --> 00:11:09,440
curve so just looking at the area on the

385
00:11:09,440 --> 00:11:11,440
roc curve we see that the baseline

386
00:11:11,440 --> 00:11:13,760
classifier usually does worst and then

387
00:11:13,760 --> 00:11:15,839
this um simon's network and classifier

388
00:11:15,839 --> 00:11:17,040
that's quite a bit better it turns out

389
00:11:17,040 --> 00:11:19,360
that when you add the expander

390
00:11:19,360 --> 00:11:21,040
the um

391
00:11:21,040 --> 00:11:23,040
the the area under the rvc curve

392
00:11:23,040 --> 00:11:25,360
actually grows you know most of the time

393
00:11:25,360 --> 00:11:26,720
it grows so

394
00:11:26,720 --> 00:11:27,920
there's no loss it turns out there's no

395
00:11:27,920 --> 00:11:30,160
loss and this these tables also show you

396
00:11:30,160 --> 00:11:31,040
the difference between the

397
00:11:31,040 --> 00:11:33,360
distance-based coding and the ld lcd

398
00:11:33,360 --> 00:11:35,680
decoding and it turns out surprising to

399
00:11:35,680 --> 00:11:37,600
us that ldlc decoding actually does

400
00:11:37,600 --> 00:11:38,560
better than this is the code and we

401
00:11:38,560 --> 00:11:39,920
believe that and that's because

402
00:11:39,920 --> 00:11:41,600
distance-based decoding does not

403
00:11:41,600 --> 00:11:44,000
completely full film fill the the vector

404
00:11:44,000 --> 00:11:47,120
space at 120 dimensions so we think the

405
00:11:47,120 --> 00:11:49,760
code fills the space a bit better

406
00:11:49,760 --> 00:11:52,079
so this is pretty much what we did at a

407
00:11:52,079 --> 00:11:53,519
very high level

408
00:11:53,519 --> 00:11:55,519
thank you for your attention and i would

409
00:11:55,519 --> 00:11:56,800
strongly encourage you to read the paper

410
00:11:56,800 --> 00:12:00,439
and find out more details

