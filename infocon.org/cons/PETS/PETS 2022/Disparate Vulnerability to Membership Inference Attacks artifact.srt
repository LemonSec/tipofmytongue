1
00:00:00,560 --> 00:00:02,240
hello everyone my name is bo dawn

2
00:00:02,240 --> 00:00:03,919
kolinic and i'm going to talk about our

3
00:00:03,919 --> 00:00:05,839
paper on disparate vulnerability to

4
00:00:05,839 --> 00:00:08,160
membership inference attacks

5
00:00:08,160 --> 00:00:10,320
so what are membership inference attacks

6
00:00:10,320 --> 00:00:12,480
these are attacks against privacy of

7
00:00:12,480 --> 00:00:14,799
machine learning or statistical models

8
00:00:14,799 --> 00:00:16,960
in which an adversary aims to learn for

9
00:00:16,960 --> 00:00:19,600
a given record x whether this record was

10
00:00:19,600 --> 00:00:21,840
part of the training data set or not

11
00:00:21,840 --> 00:00:23,439
because machine learning models are a

12
00:00:23,439 --> 00:00:25,519
bit like parrots they can memorize what

13
00:00:25,519 --> 00:00:27,439
they have seen in the training data by

14
00:00:27,439 --> 00:00:29,679
querying and inspecting the model in

15
00:00:29,679 --> 00:00:32,079
different ways an adversary can get a

16
00:00:32,079 --> 00:00:34,160
non-trivial advantage at identifying if

17
00:00:34,160 --> 00:00:36,640
a record was used in training or not

18
00:00:36,640 --> 00:00:38,079
researchers have noticed that different

19
00:00:38,079 --> 00:00:40,399
records can be differently vulnerable to

20
00:00:40,399 --> 00:00:42,160
membership inference attacks this

21
00:00:42,160 --> 00:00:44,559
observation comes in two forms

22
00:00:44,559 --> 00:00:46,480
first somewheres have pointed out that

23
00:00:46,480 --> 00:00:48,960
there could be outlier records that are

24
00:00:48,960 --> 00:00:50,800
more vulnerable than others

25
00:00:50,800 --> 00:00:52,480
second other researchers have noticed

26
00:00:52,480 --> 00:00:54,559
that whole semantically related

27
00:00:54,559 --> 00:00:56,320
subgroups of data can have higher

28
00:00:56,320 --> 00:00:58,640
vulnerability than other groups in our

29
00:00:58,640 --> 00:01:00,480
work we study the latter aspect

30
00:01:00,480 --> 00:01:02,160
desperate vulnerability to membership

31
00:01:02,160 --> 00:01:03,920
inference across data population

32
00:01:03,920 --> 00:01:05,199
subgroups

33
00:01:05,199 --> 00:01:06,720
studying this form of disparate

34
00:01:06,720 --> 00:01:08,799
vulnerability is important among all

35
00:01:08,799 --> 00:01:11,360
else for from the legal point of view

36
00:01:11,360 --> 00:01:12,640
legal scholars have argued that

37
00:01:12,640 --> 00:01:14,479
membership inference attacks are a good

38
00:01:14,479 --> 00:01:16,240
tool for auditing machine learning and

39
00:01:16,240 --> 00:01:17,920
statistical models for their privacy

40
00:01:17,920 --> 00:01:20,640
leakage for example under gdpr the

41
00:01:20,640 --> 00:01:22,240
schools have argued that if a model is

42
00:01:22,240 --> 00:01:24,080
vulnerable to membership inference the

43
00:01:24,080 --> 00:01:25,920
model should be considered personal data

44
00:01:25,920 --> 00:01:28,159
and that's treated in a special way

45
00:01:28,159 --> 00:01:30,320
the issue that can come up here is

46
00:01:30,320 --> 00:01:32,000
privacy washing through the usage of

47
00:01:32,000 --> 00:01:34,240
average metrics if we only look at

48
00:01:34,240 --> 00:01:35,759
average vulnerability across the

49
00:01:35,759 --> 00:01:38,479
population we might miss situations as

50
00:01:38,479 --> 00:01:40,479
before and we might miss some records

51
00:01:40,479 --> 00:01:42,320
that have higher vulnerability

52
00:01:42,320 --> 00:01:43,840
if these records are semantically

53
00:01:43,840 --> 00:01:46,079
disconnected outliers this vulnerability

54
00:01:46,079 --> 00:01:48,720
is easy to dismiss as acceptable risk

55
00:01:48,720 --> 00:01:50,799
if these records however all belong to a

56
00:01:50,799 --> 00:01:52,720
socialist selling group this is much

57
00:01:52,720 --> 00:01:55,040
more concerning and for example privacy

58
00:01:55,040 --> 00:01:56,880
auditors might be accountable to those

59
00:01:56,880 --> 00:01:57,920
groups

60
00:01:57,920 --> 00:02:00,240
this is why being able to reason about

61
00:02:00,240 --> 00:02:02,159
and audit disparate vulnerability

62
00:02:02,159 --> 00:02:04,240
specifically across population subgroups

63
00:02:04,240 --> 00:02:05,439
is important

64
00:02:05,439 --> 00:02:06,880
in this talk i will discuss three

65
00:02:06,880 --> 00:02:08,479
aspects of our paper

66
00:02:08,479 --> 00:02:10,399
first regardless of any disparity

67
00:02:10,399 --> 00:02:12,080
considerations we have to answer the

68
00:02:12,080 --> 00:02:14,000
question when exactly the models are

69
00:02:14,000 --> 00:02:16,000
vulnerable to membership inference

70
00:02:16,000 --> 00:02:17,599
second we extend this insight to

71
00:02:17,599 --> 00:02:19,280
disparate vulnerability

72
00:02:19,280 --> 00:02:21,120
and then third and finally i will talk

73
00:02:21,120 --> 00:02:22,800
about how we should measure this part

74
00:02:22,800 --> 00:02:24,640
vulnerability so let's start with

75
00:02:24,640 --> 00:02:26,160
figuring out when exactly the models are

76
00:02:26,160 --> 00:02:28,239
vulnerable to membership inference

77
00:02:28,239 --> 00:02:29,840
it is intuitive and well known that

78
00:02:29,840 --> 00:02:32,400
overfeeding or lack of generalization is

79
00:02:32,400 --> 00:02:34,239
a sufficient condition for a model to be

80
00:02:34,239 --> 00:02:36,080
vulnerable to membership inference

81
00:02:36,080 --> 00:02:37,760
generalization is characterized by this

82
00:02:37,760 --> 00:02:39,840
gap between the average loss in the

83
00:02:39,840 --> 00:02:41,680
training data on the left here and the

84
00:02:41,680 --> 00:02:43,440
average loss on the test data on the

85
00:02:43,440 --> 00:02:44,720
right

86
00:02:44,720 --> 00:02:47,200
indeed if the model has much lower error

87
00:02:47,200 --> 00:02:49,120
on its training data an adversary can

88
00:02:49,120 --> 00:02:51,280
recognize training examples by looking

89
00:02:51,280 --> 00:02:53,200
at the loss

90
00:02:53,200 --> 00:02:55,040
there is a recent notion that

91
00:02:55,040 --> 00:02:57,599
generalizes generalization pun intended

92
00:02:57,599 --> 00:02:59,680
called distribution realization if

93
00:02:59,680 --> 00:03:02,000
classical generalization only considers

94
00:03:02,000 --> 00:03:05,280
the average of the distribution of loss

95
00:03:05,280 --> 00:03:06,959
distribution internalization uses a

96
00:03:06,959 --> 00:03:09,200
probabilistic metric between train and

97
00:03:09,200 --> 00:03:11,440
test distributions specifically we use

98
00:03:11,440 --> 00:03:13,200
total variation distance

99
00:03:13,200 --> 00:03:15,440
moreover not only

100
00:03:15,440 --> 00:03:18,239
we look at the loss but any other

101
00:03:18,239 --> 00:03:20,000
numeric property of the model here

102
00:03:20,000 --> 00:03:21,920
denoted as fee

103
00:03:21,920 --> 00:03:23,519
in this work we analyze the worst case

104
00:03:23,519 --> 00:03:25,040
vulnerability to membership inference

105
00:03:25,040 --> 00:03:27,440
attacks that use a certain feature fee

106
00:03:27,440 --> 00:03:29,599
to conduct an attack most commonly this

107
00:03:29,599 --> 00:03:31,760
feature is loss but any other numeric

108
00:03:31,760 --> 00:03:33,280
function of the model and an example

109
00:03:33,280 --> 00:03:34,400
could work

110
00:03:34,400 --> 00:03:35,599
we formalized the worst case

111
00:03:35,599 --> 00:03:37,840
vulnerability as the best possible

112
00:03:37,840 --> 00:03:40,159
normalized attack accuracy in a standard

113
00:03:40,159 --> 00:03:42,799
formalism by yemen colleagues we show

114
00:03:42,799 --> 00:03:44,959
that this worst case vulnerability is

115
00:03:44,959 --> 00:03:46,720
exactly equal to the distribution

116
00:03:46,720 --> 00:03:48,159
realization gap

117
00:03:48,159 --> 00:03:50,000
so if poor

118
00:03:50,000 --> 00:03:52,159
classical generalization is a sufficient

119
00:03:52,159 --> 00:03:53,920
condition for vulnerability the

120
00:03:53,920 --> 00:03:55,519
necessary and sufficient condition is

121
00:03:55,519 --> 00:03:58,000
for distribution of generalization

122
00:03:58,000 --> 00:03:59,680
next we extend this insight to disparate

123
00:03:59,680 --> 00:04:01,760
vulnerability and for this we introduce

124
00:04:01,760 --> 00:04:04,560
a special subgroup aware adversary if we

125
00:04:04,560 --> 00:04:06,159
can conduct an attack using certain

126
00:04:06,159 --> 00:04:08,480
features fee for example losses before

127
00:04:08,480 --> 00:04:10,239
then a subgroup aware version of that

128
00:04:10,239 --> 00:04:12,000
attack would additionally use the

129
00:04:12,000 --> 00:04:14,480
subgroup to which an example belongs

130
00:04:14,480 --> 00:04:15,519
we showed that the worst case

131
00:04:15,519 --> 00:04:18,000
vulnerability with cyberware adversary

132
00:04:18,000 --> 00:04:19,839
is always equal or higher than the worst

133
00:04:19,839 --> 00:04:21,519
case vulnerability to an address that

134
00:04:21,519 --> 00:04:23,919
does not use subgroup information so

135
00:04:23,919 --> 00:04:26,320
using that is always beneficial to the

136
00:04:26,320 --> 00:04:28,880
adversary in the worst case we use this

137
00:04:28,880 --> 00:04:30,880
insight as a tool to connect software

138
00:04:30,880 --> 00:04:32,240
vulnerability and distribution

139
00:04:32,240 --> 00:04:33,680
generalization

140
00:04:33,680 --> 00:04:35,840
we can look at a subgroup specific

141
00:04:35,840 --> 00:04:38,160
version of distribution generalization

142
00:04:38,160 --> 00:04:40,160
which as before is the distance between

143
00:04:40,160 --> 00:04:42,160
training and test distributions but this

144
00:04:42,160 --> 00:04:44,400
time the distributions are conditional

145
00:04:44,400 --> 00:04:47,040
on the subgroup and as a technicality on

146
00:04:47,040 --> 00:04:48,400
the fact that the trained data contains

147
00:04:48,400 --> 00:04:50,800
at least one example of the subgroup

148
00:04:50,800 --> 00:04:52,960
then we can show that the worst case

149
00:04:52,960 --> 00:04:55,120
vulnerability to a sacrifice adversary

150
00:04:55,120 --> 00:04:57,040
denoted as v sub z

151
00:04:57,040 --> 00:04:58,639
is equal to subgroup specific

152
00:04:58,639 --> 00:05:00,639
distribution generalization gap

153
00:05:00,639 --> 00:05:02,639
because cyber adversary always causes

154
00:05:02,639 --> 00:05:04,160
higher vulnerability in the worst case

155
00:05:04,160 --> 00:05:06,560
startup vulnerability for any other

156
00:05:06,560 --> 00:05:08,080
adversary is upper bounded by this

157
00:05:08,080 --> 00:05:10,639
distribution journalization gap as well

158
00:05:10,639 --> 00:05:11,919
and by the way just a disparate

159
00:05:11,919 --> 00:05:14,080
vulnerability is simply the difference

160
00:05:14,080 --> 00:05:16,000
between the subgroup vulnerabilities for

161
00:05:16,000 --> 00:05:18,080
two subgroups so it is sufficient to

162
00:05:18,080 --> 00:05:20,720
study subway vulnerability as an object

163
00:05:20,720 --> 00:05:22,320
this concludes the theoretical part of

164
00:05:22,320 --> 00:05:24,560
the talk and i would like to now discuss

165
00:05:24,560 --> 00:05:26,720
some practical considerations about

166
00:05:26,720 --> 00:05:30,320
measuring dispersed vulnerability

167
00:05:30,320 --> 00:05:31,360
and i would like to start with the

168
00:05:31,360 --> 00:05:33,199
curious observation that we had

169
00:05:33,199 --> 00:05:35,039
we tried to measure disparity in cyber

170
00:05:35,039 --> 00:05:36,800
vulnerability between two synthetic

171
00:05:36,800 --> 00:05:39,039
groups we fixed the size of one majority

172
00:05:39,039 --> 00:05:40,720
group and vary the size of the minority

173
00:05:40,720 --> 00:05:42,560
group and then we ran some membership

174
00:05:42,560 --> 00:05:44,960
inference attacks in the setup and what

175
00:05:44,960 --> 00:05:46,880
is interesting the models that we were

176
00:05:46,880 --> 00:05:49,120
attacking were kind of dummy models at

177
00:05:49,120 --> 00:05:50,800
the training stage the training

178
00:05:50,800 --> 00:05:52,320
algorithm of these models would take us

179
00:05:52,320 --> 00:05:53,919
into the chain data set and just throw

180
00:05:53,919 --> 00:05:56,000
it away and return the constant model

181
00:05:56,000 --> 00:05:58,400
every time so the models that we were

182
00:05:58,400 --> 00:06:00,880
attacking were independent of the data

183
00:06:00,880 --> 00:06:02,800
and we did the strange experiment to

184
00:06:02,800 --> 00:06:05,039
verify that vulnerability and disparity

185
00:06:05,039 --> 00:06:06,639
and vulnerability for all such models

186
00:06:06,639 --> 00:06:09,039
would be zero right because the models

187
00:06:09,039 --> 00:06:11,199
have not really touched or used the data

188
00:06:11,199 --> 00:06:12,720
they cannot be vulnerable to membership

189
00:06:12,720 --> 00:06:14,080
inference

190
00:06:14,080 --> 00:06:15,600
we ran three kinds of attacks i will

191
00:06:15,600 --> 00:06:17,919
talk about them more in a bit and to our

192
00:06:17,919 --> 00:06:19,759
surprise whereas for some attacks such

193
00:06:19,759 --> 00:06:21,120
as average lost trash for attack and

194
00:06:21,120 --> 00:06:22,960
general model attack the disparity was

195
00:06:22,960 --> 00:06:24,960
around zero which is what we expected

196
00:06:24,960 --> 00:06:26,479
for another attack which is a much more

197
00:06:26,479 --> 00:06:28,800
powerful one the optimum was threshold

198
00:06:28,800 --> 00:06:30,800
we saw statistically significant

199
00:06:30,800 --> 00:06:32,840
non-zero disparity and this should not

200
00:06:32,840 --> 00:06:36,160
happen because the models that we attack

201
00:06:36,160 --> 00:06:38,240
have not used any data there can be

202
00:06:38,240 --> 00:06:39,600
neither any membership influence

203
00:06:39,600 --> 00:06:41,520
advantage nor any disparities in this

204
00:06:41,520 --> 00:06:42,639
advantage

205
00:06:42,639 --> 00:06:44,560
to understand what's happening here why

206
00:06:44,560 --> 00:06:46,560
this is happening uh we first have to

207
00:06:46,560 --> 00:06:47,680
discuss

208
00:06:47,680 --> 00:06:49,599
different approaches to adversary to

209
00:06:49,599 --> 00:06:51,280
model an adversary's knowledge in

210
00:06:51,280 --> 00:06:52,880
membership inference

211
00:06:52,880 --> 00:06:54,400
first approach

212
00:06:54,400 --> 00:06:56,400
the shadow model or reference model

213
00:06:56,400 --> 00:06:57,520
approach

214
00:06:57,520 --> 00:06:59,280
here the adversary is assumed to know

215
00:06:59,280 --> 00:07:00,880
the data distribution and the target

216
00:07:00,880 --> 00:07:02,639
moral training algorithm with this

217
00:07:02,639 --> 00:07:04,240
information the addresser can reproduce

218
00:07:04,240 --> 00:07:05,840
some models that look kind of like the

219
00:07:05,840 --> 00:07:07,120
target model

220
00:07:07,120 --> 00:07:09,120
learn an attack on them and then deploy

221
00:07:09,120 --> 00:07:11,280
the attack against the target

222
00:07:11,280 --> 00:07:13,599
in the other approach which

223
00:07:13,599 --> 00:07:15,520
i'm going to call worst case here the

224
00:07:15,520 --> 00:07:18,000
adversary directly has access to some

225
00:07:18,000 --> 00:07:20,400
information about the target model's

226
00:07:20,400 --> 00:07:22,800
properties on the training data for

227
00:07:22,800 --> 00:07:24,319
example in one such attacks the

228
00:07:24,319 --> 00:07:26,160
adversary obtains the average loss on

229
00:07:26,160 --> 00:07:27,360
the training data

230
00:07:27,360 --> 00:07:28,960
in other attacks the recall optimum

231
00:07:28,960 --> 00:07:31,120
threshold the adversary gets the

232
00:07:31,120 --> 00:07:32,720
threshold on the loss that is actually

233
00:07:32,720 --> 00:07:34,720
carefully chosen to distinguish train

234
00:07:34,720 --> 00:07:37,440
loss values from test values

235
00:07:37,440 --> 00:07:39,280
the danger of this approach of course is

236
00:07:39,280 --> 00:07:41,680
that it goes a bit against the goal of

237
00:07:41,680 --> 00:07:43,919
membership inference because in

238
00:07:43,919 --> 00:07:46,000
membership and inference the adversary

239
00:07:46,000 --> 00:07:47,440
wants to learn something about the

240
00:07:47,440 --> 00:07:49,759
training data yet here the address

241
00:07:49,759 --> 00:07:51,120
already starts with some non-trivial

242
00:07:51,120 --> 00:07:53,440
information about that training data

243
00:07:53,440 --> 00:07:55,599
however this enables us to estimate

244
00:07:55,599 --> 00:07:57,039
worst case vulnerability which is a

245
00:07:57,039 --> 00:07:59,280
standard practice and security

246
00:07:59,280 --> 00:08:01,039
unfortunately it is inherently

247
00:08:01,039 --> 00:08:03,759
problematic for estimating specifically

248
00:08:03,759 --> 00:08:06,479
disparity in vulnerability

249
00:08:06,479 --> 00:08:08,160
um this is because we want the

250
00:08:08,160 --> 00:08:09,599
membership inference to exploit the

251
00:08:09,599 --> 00:08:11,520
model's memorization behavior would

252
00:08:11,520 --> 00:08:13,520
exploit the fact that the model behaves

253
00:08:13,520 --> 00:08:15,120
differently on training examples and

254
00:08:15,120 --> 00:08:16,639
test examples

255
00:08:16,639 --> 00:08:18,560
with the worst case approach however

256
00:08:18,560 --> 00:08:20,240
because the adversary has access to

257
00:08:20,240 --> 00:08:21,840
non-trill information about the training

258
00:08:21,840 --> 00:08:23,919
data of the target model there is

259
00:08:23,919 --> 00:08:26,319
another source of information beyond

260
00:08:26,319 --> 00:08:28,080
model memorization that we do not want

261
00:08:28,080 --> 00:08:29,919
to capture

262
00:08:29,919 --> 00:08:31,680
and it's the sampling bias which is the

263
00:08:31,680 --> 00:08:34,320
fact that define it training data set is

264
00:08:34,320 --> 00:08:35,760
always going to be somewhat different

265
00:08:35,760 --> 00:08:37,200
from the actual

266
00:08:37,200 --> 00:08:39,440
data distribution and this is what also

267
00:08:39,440 --> 00:08:41,120
enables the addresses to think trading

268
00:08:41,120 --> 00:08:42,958
uh to tell train examples from test

269
00:08:42,958 --> 00:08:45,360
examples this bias depends on the size

270
00:08:45,360 --> 00:08:48,160
of data and it vanishes pretty rapidly

271
00:08:48,160 --> 00:08:49,839
except when we are deploying our attacks

272
00:08:49,839 --> 00:08:52,399
on small subgroups of data for example

273
00:08:52,399 --> 00:08:54,160
the subgroup of size 50 is something

274
00:08:54,160 --> 00:08:56,080
that we saw in the literature and the

275
00:08:56,080 --> 00:08:57,760
semolin bias definitely has an effect at

276
00:08:57,760 --> 00:09:00,160
this scale as we see in the blood

277
00:09:00,160 --> 00:09:02,080
thus if we measure disparity between two

278
00:09:02,080 --> 00:09:04,240
subgroups using the worst-case approach

279
00:09:04,240 --> 00:09:06,240
we will always see disparity if the

280
00:09:06,240 --> 00:09:07,680
subgroups

281
00:09:07,680 --> 00:09:10,080
differ in size so the measurement just

282
00:09:10,080 --> 00:09:13,040
becomes meaningless and also misleading

283
00:09:13,040 --> 00:09:14,720
to test the tax for this unwanted

284
00:09:14,720 --> 00:09:17,279
property we propose the so-called

285
00:09:17,279 --> 00:09:19,519
new model bias test which is exactly

286
00:09:19,519 --> 00:09:21,600
what we did before evaluate attacks

287
00:09:21,600 --> 00:09:23,440
against data independent models and if

288
00:09:23,440 --> 00:09:25,120
we see any non-zero vulnerability then

289
00:09:25,120 --> 00:09:26,560
the measurement of vulnerability is

290
00:09:26,560 --> 00:09:28,880
biased and should not in general be used

291
00:09:28,880 --> 00:09:30,880
to measure disparate vulnerability

292
00:09:30,880 --> 00:09:32,959
should not be used to measure

293
00:09:32,959 --> 00:09:35,360
regular vulnerability but as long as the

294
00:09:35,360 --> 00:09:37,920
size of the data is big the the bias is

295
00:09:37,920 --> 00:09:40,080
going to be negligible

296
00:09:40,080 --> 00:09:41,920
so to be safe one should only use a tax

297
00:09:41,920 --> 00:09:43,600
in which the adversary cannot access

298
00:09:43,600 --> 00:09:44,959
anything about the training data when

299
00:09:44,959 --> 00:09:47,360
building the attack as an exception our

300
00:09:47,360 --> 00:09:48,800
experiments showed that the average loss

301
00:09:48,800 --> 00:09:50,480
threshold that was not biased in

302
00:09:50,480 --> 00:09:52,399
practice probably because just a single

303
00:09:52,399 --> 00:09:54,399
average loss value is not sufficient to

304
00:09:54,399 --> 00:09:56,160
bias the results

305
00:09:56,160 --> 00:09:58,000
in the paper we also discussed some

306
00:09:58,000 --> 00:09:59,279
other considerations one should take

307
00:09:59,279 --> 00:10:00,880
when measuring disparate vulnerability

308
00:10:00,880 --> 00:10:02,720
for example because the groups could be

309
00:10:02,720 --> 00:10:04,480
small we want to use statistical tests

310
00:10:04,480 --> 00:10:06,160
to make sure that the observed disparity

311
00:10:06,160 --> 00:10:07,440
is not a fluke

312
00:10:07,440 --> 00:10:09,839
we also apply methods to verify that we

313
00:10:09,839 --> 00:10:11,200
can detect this vulnerability in

314
00:10:11,200 --> 00:10:13,120
realistic settings and we do that with

315
00:10:13,120 --> 00:10:15,519
high degree statistical significance

316
00:10:15,519 --> 00:10:17,360
finally we'll also discuss how one can

317
00:10:17,360 --> 00:10:19,040
prevent the spirit vulnerability we show

318
00:10:19,040 --> 00:10:20,640
the differential privacy bounded and

319
00:10:20,640 --> 00:10:21,839
that some algorithmic fairness

320
00:10:21,839 --> 00:10:24,000
interventions can reduce it but only for

321
00:10:24,000 --> 00:10:25,600
specific adversarial models so they are

322
00:10:25,600 --> 00:10:27,839
not a valid defense

323
00:10:27,839 --> 00:10:29,760
to sum up in this talk i briefly

324
00:10:29,760 --> 00:10:31,440
outlined the following aspects of our

325
00:10:31,440 --> 00:10:32,959
paper

326
00:10:32,959 --> 00:10:34,800
first we have argued from a legal

327
00:10:34,800 --> 00:10:36,560
standpoint that it is important to

328
00:10:36,560 --> 00:10:38,399
evaluate disparate vulnerability across

329
00:10:38,399 --> 00:10:40,560
demographic subgroups to avoid privacy

330
00:10:40,560 --> 00:10:42,560
washing through aggregate metrics on the

331
00:10:42,560 --> 00:10:44,320
theoretical side the worst case

332
00:10:44,320 --> 00:10:46,079
vulnerability to membership inference is

333
00:10:46,079 --> 00:10:47,600
characterized by distribution or

334
00:10:47,600 --> 00:10:50,160
generalization so we showed that if poor

335
00:10:50,160 --> 00:10:51,680
standard generalization is a sufficient

336
00:10:51,680 --> 00:10:52,880
condition for membership influence

337
00:10:52,880 --> 00:10:54,800
vulnerability or distributional

338
00:10:54,800 --> 00:10:56,160
generalization is necessary and

339
00:10:56,160 --> 00:10:57,360
sufficient

340
00:10:57,360 --> 00:10:59,279
and then disparate vulnerability is

341
00:10:59,279 --> 00:11:01,120
bonded by difference in subgroup level

342
00:11:01,120 --> 00:11:03,120
distribution marginalization

343
00:11:03,120 --> 00:11:05,040
finally on the practical side we argued

344
00:11:05,040 --> 00:11:06,640
that one should not use attacks that

345
00:11:06,640 --> 00:11:08,320
have non-trivial access to the training

346
00:11:08,320 --> 00:11:10,880
data to measure desperate vulnerability

347
00:11:10,880 --> 00:11:12,640
something that we have in the paper but

348
00:11:12,640 --> 00:11:14,959
not much in the talk is that we should

349
00:11:14,959 --> 00:11:16,880
use statistical procedures to establish

350
00:11:16,880 --> 00:11:19,120
statistical significance and we

351
00:11:19,120 --> 00:11:21,440
discussed various mitigation strategies

352
00:11:21,440 --> 00:11:23,519
against dispersed vulnerability

353
00:11:23,519 --> 00:11:25,519
this concludes the talk please check the

354
00:11:25,519 --> 00:11:28,000
paper for more details and thank you for

355
00:11:28,000 --> 00:11:31,079
your attention

