1
00:00:00,659 --> 00:00:03,120
hi welcome my name is Michael and the

2
00:00:03,120 --> 00:00:05,040
title of our work is towards improving

3
00:00:05,040 --> 00:00:07,020
code stylometry analysis in underground

4
00:00:07,020 --> 00:00:10,139
forums so how will we tell who wrote a

5
00:00:10,139 --> 00:00:11,160
piece of text

6
00:00:11,160 --> 00:00:13,139
well we would use something called

7
00:00:13,139 --> 00:00:16,560
stylometric analysis which is the study

8
00:00:16,560 --> 00:00:19,680
of writing style

9
00:00:19,680 --> 00:00:21,480
and in order to do this we need to

10
00:00:21,480 --> 00:00:23,359
extract features from the piece of text

11
00:00:23,359 --> 00:00:27,000
that would enable us to capture the

12
00:00:27,000 --> 00:00:29,220
artifacts of the writing style

13
00:00:29,220 --> 00:00:32,460
and then on the state of the art we

14
00:00:32,460 --> 00:00:34,500
would use machine learning algorithms to

15
00:00:34,500 --> 00:00:36,360
train a model

16
00:00:36,360 --> 00:00:39,360
um give it samples from a lot of

17
00:00:39,360 --> 00:00:40,620
different authors

18
00:00:40,620 --> 00:00:45,000
and then when giving a new sample a new

19
00:00:45,000 --> 00:00:46,980
unseen sample to the classifier you

20
00:00:46,980 --> 00:00:49,680
should be able to tell us who wrote it

21
00:00:49,680 --> 00:00:51,480
this is the authorship attribution

22
00:00:51,480 --> 00:00:52,860
problem

23
00:00:52,860 --> 00:00:55,760
and so in the literature

24
00:00:55,760 --> 00:01:00,359
these techniques are evaluated using for

25
00:01:00,359 --> 00:01:02,160
example the Google code jump contest

26
00:01:02,160 --> 00:01:05,060
code very popular data set

27
00:01:05,060 --> 00:01:08,939
and it comes from the good cold jump

28
00:01:08,939 --> 00:01:10,439
contest hosted by Google where

29
00:01:10,439 --> 00:01:14,880
programmers sign up and they solve a set

30
00:01:14,880 --> 00:01:16,799
of programming problems and submit their

31
00:01:16,799 --> 00:01:18,720
Solutions

32
00:01:18,720 --> 00:01:19,560
um

33
00:01:19,560 --> 00:01:23,400
also there's been work uh using Code

34
00:01:23,400 --> 00:01:25,979
scripts from GitHub repositories uh

35
00:01:25,979 --> 00:01:28,560
that's also been used to evaluate these

36
00:01:28,560 --> 00:01:31,140
kinds of tools but we wanted and even

37
00:01:31,140 --> 00:01:35,240
more organic environment and for that we

38
00:01:35,240 --> 00:01:37,560
wanted to see what would happen if we

39
00:01:37,560 --> 00:01:38,759
applied these techniques to code

40
00:01:38,759 --> 00:01:40,200
gathered from underground forums

41
00:01:40,200 --> 00:01:42,479
underground forums are

42
00:01:42,479 --> 00:01:44,820
places on the internet where malicious

43
00:01:44,820 --> 00:01:49,320
actors gather to buy and sell illicit

44
00:01:49,320 --> 00:01:52,799
products including narcotics and malware

45
00:01:52,799 --> 00:01:56,820
as well and they also share source code

46
00:01:56,820 --> 00:01:58,219
with one another

47
00:01:58,219 --> 00:02:00,420
and learn from each other on how to

48
00:02:00,420 --> 00:02:02,759
write malware

49
00:02:02,759 --> 00:02:04,020
um

50
00:02:04,020 --> 00:02:07,020
and so when applying these

51
00:02:07,020 --> 00:02:09,598
state-of-the-art techniques

52
00:02:09,598 --> 00:02:12,920
um using a random Forest classifier

53
00:02:12,920 --> 00:02:16,620
we get the following where

54
00:02:16,620 --> 00:02:18,620
using the Google code jump contest code

55
00:02:18,620 --> 00:02:22,440
we get very high accuracies across the

56
00:02:22,440 --> 00:02:25,800
board from 20 authors to 100 authors we

57
00:02:25,800 --> 00:02:28,680
get very accurate authorship attribution

58
00:02:28,680 --> 00:02:31,500
when performing on GitHub repository

59
00:02:31,500 --> 00:02:32,280
code

60
00:02:32,280 --> 00:02:34,379
there is a mark degradation in

61
00:02:34,379 --> 00:02:37,260
performance but when applying these same

62
00:02:37,260 --> 00:02:39,000
techniques to code gas from underground

63
00:02:39,000 --> 00:02:42,420
forums the results are even worse

64
00:02:42,420 --> 00:02:46,620
ranging from 66 accuracy

65
00:02:46,620 --> 00:02:50,580
um with 20 authors to have that with 100

66
00:02:50,580 --> 00:02:52,140
authors

67
00:02:52,140 --> 00:02:53,819
and so the first contribution that we

68
00:02:53,819 --> 00:02:55,980
present is we saw that the state of the

69
00:02:55,980 --> 00:02:58,500
art in coastal Elementary techniques do

70
00:02:58,500 --> 00:03:00,659
not work well for code shared in

71
00:03:00,659 --> 00:03:02,819
underground forums

72
00:03:02,819 --> 00:03:05,060
and well why is that what's the problem

73
00:03:05,060 --> 00:03:08,659
well Google code jump contest code

74
00:03:08,659 --> 00:03:11,400
has properties to it that are not

75
00:03:11,400 --> 00:03:14,879
present in more organic environments so

76
00:03:14,879 --> 00:03:17,099
for example the the code samples tend to

77
00:03:17,099 --> 00:03:18,180
be very long

78
00:03:18,180 --> 00:03:21,659
uh the grand truth of authorship of each

79
00:03:21,659 --> 00:03:24,840
sample is pretty much guaranteed as

80
00:03:24,840 --> 00:03:26,879
people will get disqualified if they

81
00:03:26,879 --> 00:03:30,060
plagiarize the samples are guaranteed to

82
00:03:30,060 --> 00:03:31,560
compile

83
00:03:31,560 --> 00:03:34,200
um there's no exotic libraries being

84
00:03:34,200 --> 00:03:36,060
used because these contests are about

85
00:03:36,060 --> 00:03:38,700
more logical puzzles than software

86
00:03:38,700 --> 00:03:41,159
engineering problems

87
00:03:41,159 --> 00:03:42,900
and there's a very limited scope of

88
00:03:42,900 --> 00:03:47,420
problems that are solved

89
00:03:47,700 --> 00:03:48,959
um

90
00:03:48,959 --> 00:03:51,659
so everyone solves the same set of say

91
00:03:51,659 --> 00:03:54,599
10 problems in in one year

92
00:03:54,599 --> 00:03:56,640
so the second contribution that we want

93
00:03:56,640 --> 00:03:58,260
to present is that we designed a

94
00:03:58,260 --> 00:03:59,939
methodology aimed at improving code

95
00:03:59,939 --> 00:04:02,400
stylometry authorship attribution in

96
00:04:02,400 --> 00:04:04,620
underground forms and we use conformal

97
00:04:04,620 --> 00:04:06,540
prediction to inform our confidence

98
00:04:06,540 --> 00:04:08,580
thresholding which is one of the steps

99
00:04:08,580 --> 00:04:11,239
of our methodology

100
00:04:11,580 --> 00:04:13,920
and so this is your standard

101
00:04:13,920 --> 00:04:14,580
um

102
00:04:14,580 --> 00:04:18,238
machine learning pipeline if you will

103
00:04:18,238 --> 00:04:20,639
for authorship attribution you want to

104
00:04:20,639 --> 00:04:23,220
get your samples you want to extract

105
00:04:23,220 --> 00:04:26,100
abstract syntax trees from them and then

106
00:04:26,100 --> 00:04:27,840
extract features from both the abstract

107
00:04:27,840 --> 00:04:30,419
syntax trees and the raw code itself

108
00:04:30,419 --> 00:04:33,240
uh chain a random forest classifier

109
00:04:33,240 --> 00:04:35,100
model for example perform your

110
00:04:35,100 --> 00:04:36,720
classification and from that get your

111
00:04:36,720 --> 00:04:38,940
authorship attribution

112
00:04:38,940 --> 00:04:41,940
um labels

113
00:04:41,940 --> 00:04:43,680
and so our first addition to this

114
00:04:43,680 --> 00:04:45,360
pipeline the first step of our

115
00:04:45,360 --> 00:04:48,660
methodology is granularity selection so

116
00:04:48,660 --> 00:04:53,759
what that is is when we scrape a source

117
00:04:53,759 --> 00:04:56,880
code file from these forums we split it

118
00:04:56,880 --> 00:04:59,280
up into its constituent functions

119
00:04:59,280 --> 00:05:02,880
and we treat those as separate samples

120
00:05:02,880 --> 00:05:05,820
secondly the second step is code clone

121
00:05:05,820 --> 00:05:08,820
removal so if we have duplicates if we

122
00:05:08,820 --> 00:05:13,039
have duplicate functions for example

123
00:05:13,039 --> 00:05:16,800
author a and author B has submitted

124
00:05:16,800 --> 00:05:18,900
um the same function

125
00:05:18,900 --> 00:05:21,120
has posted it

126
00:05:21,120 --> 00:05:24,300
then we remove both of those samples as

127
00:05:24,300 --> 00:05:25,860
we say that we cannot guarantee ground

128
00:05:25,860 --> 00:05:28,979
truth in this in this case so

129
00:05:28,979 --> 00:05:31,199
um we reject both

130
00:05:31,199 --> 00:05:34,160
thirdly we have the sample pruning step

131
00:05:34,160 --> 00:05:37,820
whose goal is to

132
00:05:38,280 --> 00:05:41,039
um simply discard any sample that is not

133
00:05:41,039 --> 00:05:43,620
informative enough to be helpful in our

134
00:05:43,620 --> 00:05:46,220
classifications so for example samples

135
00:05:46,220 --> 00:05:49,380
uh shorter than 25 characters long will

136
00:05:49,380 --> 00:05:50,580
be discarded

137
00:05:50,580 --> 00:05:52,560
then after the training and

138
00:05:52,560 --> 00:05:55,440
classification phases

139
00:05:55,440 --> 00:05:56,100
um

140
00:05:56,100 --> 00:05:57,960
the fourth step of our methodology is

141
00:05:57,960 --> 00:06:00,120
confidence thresholding in which we

142
00:06:00,120 --> 00:06:04,680
reject samples which are judged to be

143
00:06:04,680 --> 00:06:08,639
to uncertain

144
00:06:08,699 --> 00:06:09,479
um

145
00:06:09,479 --> 00:06:11,460
and for this we use the conformer

146
00:06:11,460 --> 00:06:12,979
predictor to give us

147
00:06:12,979 --> 00:06:16,800
these confidences that we can then use

148
00:06:16,800 --> 00:06:20,340
to either reject or keep a sample and

149
00:06:20,340 --> 00:06:22,319
the fifth and final step of our

150
00:06:22,319 --> 00:06:24,780
methodology is function majority voting

151
00:06:24,780 --> 00:06:28,199
so for example if author a has

152
00:06:28,199 --> 00:06:29,220
um

153
00:06:29,220 --> 00:06:32,520
if someone's if if a specific source

154
00:06:32,520 --> 00:06:35,160
code file

155
00:06:35,160 --> 00:06:37,259
was split up into five constituent

156
00:06:37,259 --> 00:06:40,080
functions and four of those functions

157
00:06:40,080 --> 00:06:41,639
are

158
00:06:41,639 --> 00:06:43,639
um

159
00:06:44,340 --> 00:06:46,620
um classified as author a but one of

160
00:06:46,620 --> 00:06:48,840
them is classified as author B then we

161
00:06:48,840 --> 00:06:51,300
would give all five the label of author

162
00:06:51,300 --> 00:06:54,000
a because most of them were classified

163
00:06:54,000 --> 00:06:56,220
as author a and then from this we get

164
00:06:56,220 --> 00:06:58,979
our authorship labels

165
00:06:58,979 --> 00:07:01,740
and so the third contribution

166
00:07:01,740 --> 00:07:04,199
um is that we show the benefits of

167
00:07:04,199 --> 00:07:06,539
applying our methodology both in a

168
00:07:06,539 --> 00:07:08,520
closed World setting at an open world

169
00:07:08,520 --> 00:07:10,500
setting so very briefly a close World

170
00:07:10,500 --> 00:07:12,319
setting is one in which

171
00:07:12,319 --> 00:07:14,539
the classifier

172
00:07:14,539 --> 00:07:18,120
knows all of the authors in the testing

173
00:07:18,120 --> 00:07:24,300
set so you give it samples from a set of

174
00:07:24,300 --> 00:07:27,360
authors which it Trends on and then

175
00:07:27,360 --> 00:07:28,620
every sample that you give it in the

176
00:07:28,620 --> 00:07:30,900
testing set it will belong to one of

177
00:07:30,900 --> 00:07:32,520
those authors that has already seen in

178
00:07:32,520 --> 00:07:33,840
the training set

179
00:07:33,840 --> 00:07:36,780
whereas in the open world setting this

180
00:07:36,780 --> 00:07:39,000
is not necessarily the case so you give

181
00:07:39,000 --> 00:07:42,419
it samples from a set of authors to

182
00:07:42,419 --> 00:07:45,060
train on but in the testing set

183
00:07:45,060 --> 00:07:47,759
it will give it samples that do not

184
00:07:47,759 --> 00:07:51,240
belong to necessarily any of the authors

185
00:07:51,240 --> 00:07:54,000
it has seen in the training set

186
00:07:54,000 --> 00:07:56,580
and so for the close world we evaluate

187
00:07:56,580 --> 00:07:58,800
on 20 50 and 100 authors in line with

188
00:07:58,800 --> 00:08:02,759
the literature uh in on code gathered

189
00:08:02,759 --> 00:08:04,560
from underground forums GitHub

190
00:08:04,560 --> 00:08:06,300
repositories and the Google Cloud Jam

191
00:08:06,300 --> 00:08:07,500
contest

192
00:08:07,500 --> 00:08:09,240
we apply a methodology and see the

193
00:08:09,240 --> 00:08:10,379
results

194
00:08:10,379 --> 00:08:12,620
and so here are the results

195
00:08:12,620 --> 00:08:15,560
we can see that our methodology

196
00:08:15,560 --> 00:08:18,720
results in a performance Improvement

197
00:08:18,720 --> 00:08:21,780
across the board but especially

198
00:08:21,780 --> 00:08:24,419
um in the setting in underground forums

199
00:08:24,419 --> 00:08:26,879
with 100 authors where the performance

200
00:08:26,879 --> 00:08:32,580
goes from 33 to 77 post methodology uh

201
00:08:32,580 --> 00:08:35,458
more than doubling the performance

202
00:08:35,458 --> 00:08:37,620
for the open world setting we take 100

203
00:08:37,620 --> 00:08:40,320
authors and we label 20 of them as

204
00:08:40,320 --> 00:08:43,500
in-world and 80 of them as out of world

205
00:08:43,500 --> 00:08:45,060
and um

206
00:08:45,060 --> 00:08:47,640
We compare the thresholding confidence

207
00:08:47,640 --> 00:08:50,339
thresholding we have two different

208
00:08:50,339 --> 00:08:53,580
confidence metrics one is provided by

209
00:08:53,580 --> 00:08:55,260
the random Forest classifier itself is

210
00:08:55,260 --> 00:08:57,360
the probability that it gives of a

211
00:08:57,360 --> 00:08:58,320
certain

212
00:08:58,320 --> 00:09:01,200
um prediction and the other is using the

213
00:09:01,200 --> 00:09:02,940
conformal predictor

214
00:09:02,940 --> 00:09:04,740
and so this is kind of a complicated

215
00:09:04,740 --> 00:09:08,279
graph but what it shows is how many of

216
00:09:08,279 --> 00:09:11,519
the in-world authors you retain as you

217
00:09:11,519 --> 00:09:13,680
vary the the confidence threshold of

218
00:09:13,680 --> 00:09:17,760
accepting of keeping samples

219
00:09:17,760 --> 00:09:20,820
and so with the conformal predictor we

220
00:09:20,820 --> 00:09:22,800
retain 11 authors

221
00:09:22,800 --> 00:09:23,700
um

222
00:09:23,700 --> 00:09:25,620
while achieving

223
00:09:25,620 --> 00:09:26,220
um

224
00:09:26,220 --> 00:09:27,380
100

225
00:09:27,380 --> 00:09:31,260
in attribution accuracy

226
00:09:31,260 --> 00:09:34,860
uh when we tell the conformer predictor

227
00:09:34,860 --> 00:09:35,820
to keep

228
00:09:35,820 --> 00:09:39,839
all samples at a confidence of 90

229
00:09:39,839 --> 00:09:40,800
percent

230
00:09:40,800 --> 00:09:43,019
or thereabouts

231
00:09:43,019 --> 00:09:45,959
whereas at the comparable accuracy with

232
00:09:45,959 --> 00:09:48,300
the run the first classifier

233
00:09:48,300 --> 00:09:52,620
we only retain one author and so it's

234
00:09:52,620 --> 00:09:54,660
it has a tendency to reject more

235
00:09:54,660 --> 00:09:57,600
in-world authors than the conformal

236
00:09:57,600 --> 00:09:58,920
predictor

237
00:09:58,920 --> 00:10:01,459
even when

238
00:10:01,459 --> 00:10:04,079
achieving similar performance and so by

239
00:10:04,079 --> 00:10:05,220
this we showed that the conform

240
00:10:05,220 --> 00:10:08,279
predictor is giving a better trade-off

241
00:10:08,279 --> 00:10:09,600
of

242
00:10:09,600 --> 00:10:12,839
um rejecting out of old samples

243
00:10:12,839 --> 00:10:15,660
so in summary then the challenges with

244
00:10:15,660 --> 00:10:17,820
authorship attribution

245
00:10:17,820 --> 00:10:19,320
um are that there's there's not enough

246
00:10:19,320 --> 00:10:22,320
data really per author and per sample in

247
00:10:22,320 --> 00:10:25,860
these more organic domains to perform uh

248
00:10:25,860 --> 00:10:29,040
proper analysis to get these

249
00:10:29,040 --> 00:10:32,100
um results as you do with curated data

250
00:10:32,100 --> 00:10:34,440
sets like the Google code jump data set

251
00:10:34,440 --> 00:10:36,660
and classifiers rely more on lexical

252
00:10:36,660 --> 00:10:38,040
features rather than higher level

253
00:10:38,040 --> 00:10:40,200
syntactical features

254
00:10:40,200 --> 00:10:41,240
um

255
00:10:41,240 --> 00:10:43,079
so

256
00:10:43,079 --> 00:10:46,380
the classifiers will learn more on the

257
00:10:46,380 --> 00:10:49,700
context of the code itself rather than

258
00:10:49,700 --> 00:10:54,839
the the writing style of the author

259
00:10:54,839 --> 00:10:56,760
and so the Lessons Learned here are that

260
00:10:56,760 --> 00:10:58,980
we need more capable fuzzy passes first

261
00:10:58,980 --> 00:11:01,800
of all because a lot of the samples that

262
00:11:01,800 --> 00:11:03,240
we gathered from these underground

263
00:11:03,240 --> 00:11:06,899
forums they were unusable because you

264
00:11:06,899 --> 00:11:08,760
couldn't extract any syntactical

265
00:11:08,760 --> 00:11:10,200
information from them because they could

266
00:11:10,200 --> 00:11:12,959
not be passed function level granularity

267
00:11:12,959 --> 00:11:14,480
is very important

268
00:11:14,480 --> 00:11:17,220
and this leads into function-wise voting

269
00:11:17,220 --> 00:11:18,779
mechanisms which is the step in our

270
00:11:18,779 --> 00:11:20,240
methodology that actually

271
00:11:20,240 --> 00:11:23,040
improved the results the most the effect

272
00:11:23,040 --> 00:11:26,640
of plagiarism is quite small so the code

273
00:11:26,640 --> 00:11:28,860
clone removal step actually resulted in

274
00:11:28,860 --> 00:11:31,740
the smallest performance Improvement and

275
00:11:31,740 --> 00:11:34,740
the importance of context of code

276
00:11:34,740 --> 00:11:37,860
so the the classifiers tend to learn on

277
00:11:37,860 --> 00:11:40,279
the context the code is in again

278
00:11:40,279 --> 00:11:43,200
rather than the writing style

279
00:11:43,200 --> 00:11:45,560
and so if you're interested

280
00:11:45,560 --> 00:11:48,420
all the code is at this GitHub

281
00:11:48,420 --> 00:11:50,160
repository that we have used for

282
00:11:50,160 --> 00:11:52,680
experiments and that's my contact email

283
00:11:52,680 --> 00:11:54,240
if you have any questions

284
00:11:54,240 --> 00:11:56,899
thank you

