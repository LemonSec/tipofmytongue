1
00:00:00,399 --> 00:00:02,560
hello my name is anissa haddie i'm a

2
00:00:02,560 --> 00:00:05,200
research scientist at ibm this talk is

3
00:00:05,200 --> 00:00:07,040
about privacy preserving and efficient

4
00:00:07,040 --> 00:00:08,960
verification of the outcome into

5
00:00:08,960 --> 00:00:11,360
non-white association studies

6
00:00:11,360 --> 00:00:13,519
computational errors might occur during

7
00:00:13,519 --> 00:00:15,679
the workflow for example the published

8
00:00:15,679 --> 00:00:18,080
results may be computed incorrectly or

9
00:00:18,080 --> 00:00:20,400
during the quality control for example a

10
00:00:20,400 --> 00:00:22,640
researcher might use low quality data to

11
00:00:22,640 --> 00:00:24,400
conduct the research

12
00:00:24,400 --> 00:00:26,560
it is trivial to verify the correctness

13
00:00:26,560 --> 00:00:28,480
of the research fundings

14
00:00:28,480 --> 00:00:30,560
if besides the workflow and its

15
00:00:30,560 --> 00:00:33,120
associated metadata the input data set

16
00:00:33,120 --> 00:00:35,840
is provided however the input data set

17
00:00:35,840 --> 00:00:37,760
might not always be released as it may

18
00:00:37,760 --> 00:00:39,760
contain sensitive information about

19
00:00:39,760 --> 00:00:42,239
individuals

20
00:00:42,239 --> 00:00:44,879
for example genome contains sensitive

21
00:00:44,879 --> 00:00:46,559
information about an individual's

22
00:00:46,559 --> 00:00:48,960
phenotype ethnicity family members and

23
00:00:48,960 --> 00:00:51,520
disease conditions in such cases

24
00:00:51,520 --> 00:00:53,120
verifying the correctness of the

25
00:00:53,120 --> 00:00:56,320
computations becomes non-trivial

26
00:00:56,320 --> 00:00:58,399
the research question we tackle in this

27
00:00:58,399 --> 00:01:00,640
work is how to verify the correctness

28
00:01:00,640 --> 00:01:03,199
fatigues given only the workflow and its

29
00:01:03,199 --> 00:01:06,240
associated metadata

30
00:01:06,240 --> 00:01:07,840
let's first provide some background

31
00:01:07,840 --> 00:01:10,400
about genomics the whole human genome

32
00:01:10,400 --> 00:01:12,080
consists of three billion pairs of

33
00:01:12,080 --> 00:01:15,119
nucleotides it is organized in 23 pairs

34
00:01:15,119 --> 00:01:18,000
of chromosomes where 99.9

35
00:01:18,000 --> 00:01:20,159
of the genome is identical between any

36
00:01:20,159 --> 00:01:22,960
two individuals

37
00:01:22,960 --> 00:01:24,799
snips is the most common genetic

38
00:01:24,799 --> 00:01:26,880
variation which stems from differences

39
00:01:26,880 --> 00:01:28,960
in single nucleotides in the vast

40
00:01:28,960 --> 00:01:32,000
majority of cases a snap is by alalic in

41
00:01:32,000 --> 00:01:33,840
other words it can take two possible

42
00:01:33,840 --> 00:01:38,320
alleles major allele and minor allele

43
00:01:38,320 --> 00:01:40,479
genome-wide association studies have

44
00:01:40,479 --> 00:01:42,799
become a popular method for identifying

45
00:01:42,799 --> 00:01:45,040
genetic variations that are associated

46
00:01:45,040 --> 00:01:47,520
to a particular trait or phenotype

47
00:01:47,520 --> 00:01:50,000
the most common approach of chivas is

48
00:01:50,000 --> 00:01:52,560
the case control setup where the genomes

49
00:01:52,560 --> 00:01:54,399
of the individuals that carry the

50
00:01:54,399 --> 00:01:56,880
phonotype cases are compared with

51
00:01:56,880 --> 00:01:58,719
genomes of the individuals that do not

52
00:01:58,719 --> 00:02:00,960
create controls

53
00:02:00,960 --> 00:02:03,119
so basically we get the data for each

54
00:02:03,119 --> 00:02:05,040
snip from the case group and control

55
00:02:05,040 --> 00:02:07,520
group before contivas and obtain its

56
00:02:07,520 --> 00:02:08,479
output

57
00:02:08,479 --> 00:02:11,038
the output of tivas usually consists of

58
00:02:11,038 --> 00:02:13,440
the v value pulse ratio and minor allele

59
00:02:13,440 --> 00:02:15,280
frequencies for the most significant

60
00:02:15,280 --> 00:02:17,680
slips

61
00:02:18,080 --> 00:02:20,480
local differential privacy the variation

62
00:02:20,480 --> 00:02:22,879
of traditional differential privacy

63
00:02:22,879 --> 00:02:25,280
formally an algorithm a satisfies

64
00:02:25,280 --> 00:02:27,760
epsilon local differential privacy if

65
00:02:27,760 --> 00:02:31,360
and only if for any input v1 and v2 the

66
00:02:31,360 --> 00:02:34,720
ratio of the probability av1 equal to y

67
00:02:34,720 --> 00:02:37,519
to probability of av to equal to y is

68
00:02:37,519 --> 00:02:40,879
bounded by e to the power of epsilon

69
00:02:40,879 --> 00:02:43,200
the most common way of achieving epsilon

70
00:02:43,200 --> 00:02:46,640
ldp is the randomized response mechanism

71
00:02:46,640 --> 00:02:49,280
in randomized response a user reports

72
00:02:49,280 --> 00:02:51,200
the true value of a single bit of

73
00:02:51,200 --> 00:02:54,160
information with probability p and flips

74
00:02:54,160 --> 00:02:58,079
the true value with probability q

75
00:02:58,400 --> 00:03:00,640
we consider a system that includes two

76
00:03:00,640 --> 00:03:03,440
parties the researcher and the verifier

77
00:03:03,440 --> 00:03:05,599
the researcher creates a case control

78
00:03:05,599 --> 00:03:08,080
setup performs divas and shares its

79
00:03:08,080 --> 00:03:10,560
workflow along with its research

80
00:03:10,560 --> 00:03:13,360
findings and associated metadata

81
00:03:13,360 --> 00:03:15,360
the metadata includes the phenotype

82
00:03:15,360 --> 00:03:17,200
being studied the demographics of the

83
00:03:17,200 --> 00:03:19,200
research participants the number of

84
00:03:19,200 --> 00:03:21,440
research participants and the number of

85
00:03:21,440 --> 00:03:22,400
snips

86
00:03:22,400 --> 00:03:24,720
as part of the metadata the researcher

87
00:03:24,720 --> 00:03:27,040
also provides part of the original data

88
00:03:27,040 --> 00:03:29,519
set for the most significant snips after

89
00:03:29,519 --> 00:03:31,760
adding noise to it using the randomized

90
00:03:31,760 --> 00:03:35,200
response mechanism to achieve ldp which

91
00:03:35,200 --> 00:03:37,440
is referred as the partial noisy data

92
00:03:37,440 --> 00:03:40,319
set along with epsilon value

93
00:03:40,319 --> 00:03:42,720
the verifier has access to the workflow

94
00:03:42,720 --> 00:03:45,440
its output and metadata and wants to

95
00:03:45,440 --> 00:03:47,280
check the correctness of the provided

96
00:03:47,280 --> 00:03:48,799
associations

97
00:03:48,799 --> 00:03:50,879
on the other hand the researcher wants

98
00:03:50,879 --> 00:03:52,720
to ensure that the vulnerability of the

99
00:03:52,720 --> 00:03:55,519
dataset participants to genomic privacy

100
00:03:55,519 --> 00:03:57,280
attacks does not increase due to the

101
00:03:57,280 --> 00:04:00,080
provided metadata

102
00:04:00,080 --> 00:04:02,400
we assume an honest researcher that uses

103
00:04:02,400 --> 00:04:04,480
a legitimate data set and that may

104
00:04:04,480 --> 00:04:06,720
unintentionally provide wrong results as

105
00:04:06,720 --> 00:04:09,519
the output achieve us

106
00:04:09,519 --> 00:04:12,080
we use a toy example to describe how the

107
00:04:12,080 --> 00:04:14,239
verification of received statistics is

108
00:04:14,239 --> 00:04:17,199
done by the verifier let d represent the

109
00:04:17,199 --> 00:04:19,680
data set owned by the researcher as

110
00:04:19,680 --> 00:04:21,759
discussed the researcher provides to the

111
00:04:21,759 --> 00:04:23,759
verifier the statistics of the most

112
00:04:23,759 --> 00:04:26,639
associated snips together with snip ids

113
00:04:26,639 --> 00:04:29,759
and its associated metadata

114
00:04:29,759 --> 00:04:31,919
in all the sharing the ids statistics of

115
00:04:31,919 --> 00:04:34,320
the most associated snips is required

116
00:04:34,320 --> 00:04:36,639
and allowed by many institutions for

117
00:04:36,639 --> 00:04:38,720
example nih

118
00:04:38,720 --> 00:04:40,960
first the purifier performs chivas on

119
00:04:40,960 --> 00:04:43,199
the partial noise data cell

120
00:04:43,199 --> 00:04:45,360
then the verifier computes the distance

121
00:04:45,360 --> 00:04:47,360
of the computed statistics in the

122
00:04:47,360 --> 00:04:49,199
partial nodes data set from the

123
00:04:49,199 --> 00:04:51,520
statistics provided by the researcher

124
00:04:51,520 --> 00:04:53,840
using the relative error metric

125
00:04:53,840 --> 00:04:56,240
our main assumption which we validate in

126
00:04:56,240 --> 00:04:58,479
the paper is that independent of the

127
00:04:58,479 --> 00:05:00,800
phenotype being studied the distance

128
00:05:00,800 --> 00:05:02,960
obtained between the statistics computed

129
00:05:02,960 --> 00:05:04,000
over d

130
00:05:04,000 --> 00:05:06,880
epsilon and d follows a similar trend

131
00:05:06,880 --> 00:05:08,800
with different data sets for a given

132
00:05:08,800 --> 00:05:10,960
epsilon value

133
00:05:10,960 --> 00:05:13,360
to utilize this the verifier computes

134
00:05:13,360 --> 00:05:15,440
the expected distance by using a

135
00:05:15,440 --> 00:05:18,240
publicly available genomic data set e

136
00:05:18,240 --> 00:05:21,360
and its noisy version e epsilon in which

137
00:05:21,360 --> 00:05:23,840
its data point is obfuscated using

138
00:05:23,840 --> 00:05:27,039
randomized response to achieve ldp

139
00:05:27,039 --> 00:05:29,440
so first the verifier performs chibas

140
00:05:29,440 --> 00:05:32,240
both on e and e epsilon

141
00:05:32,240 --> 00:05:34,080
then using the previously introduced

142
00:05:34,080 --> 00:05:36,560
distance matrix the verifier computes

143
00:05:36,560 --> 00:05:39,280
the expected distance for each statistic

144
00:05:39,280 --> 00:05:41,680
for the most associated snips from e and

145
00:05:41,680 --> 00:05:43,440
d epsilon

146
00:05:43,440 --> 00:05:45,520
after that the verifier computes the

147
00:05:45,520 --> 00:05:47,440
error between the deviation of each

148
00:05:47,440 --> 00:05:50,160
statistic in d and the deviation of that

149
00:05:50,160 --> 00:05:52,880
particular statistic in e it compares

150
00:05:52,880 --> 00:05:55,360
this value with a cutoff point but how

151
00:05:55,360 --> 00:05:58,400
to compute the cutoff point

152
00:05:58,400 --> 00:06:00,319
for each statistic the verifier can

153
00:06:00,319 --> 00:06:02,560
heuristically set a cutoff point

154
00:06:02,560 --> 00:06:04,560
depending on the error value they choose

155
00:06:04,560 --> 00:06:07,440
to tolerate or can use another label

156
00:06:07,440 --> 00:06:09,440
publicly available to know the data set

157
00:06:09,440 --> 00:06:11,759
f whose phenotype does not need to be

158
00:06:11,759 --> 00:06:15,199
the same as the one in d

159
00:06:15,280 --> 00:06:17,199
if the error is smaller than the cutoff

160
00:06:17,199 --> 00:06:19,520
point then the statistic provided by the

161
00:06:19,520 --> 00:06:21,520
researcher is classified as being

162
00:06:21,520 --> 00:06:23,840
computed correctly otherwise it is

163
00:06:23,840 --> 00:06:26,319
classified as incorrect please check the

164
00:06:26,319 --> 00:06:29,520
paper for more details

165
00:06:29,520 --> 00:06:31,600
we use two different genomic data sets

166
00:06:31,600 --> 00:06:33,919
for evaluation the open snip data set

167
00:06:33,919 --> 00:06:36,319
and the thousand genomes from open sleep

168
00:06:36,319 --> 00:06:38,639
we use lactose intolerance hair color

169
00:06:38,639 --> 00:06:41,360
and handedness to quantify the success

170
00:06:41,360 --> 00:06:43,680
of the proposed verification scheme for

171
00:06:43,680 --> 00:06:45,759
each statistic we use true positive rate

172
00:06:45,759 --> 00:06:48,319
and true negative rate to quantify the

173
00:06:48,319 --> 00:06:50,319
impact of the computational errors done

174
00:06:50,319 --> 00:06:52,400
by the researcher we evaluate the

175
00:06:52,400 --> 00:06:54,639
utility loss of each statistic as a

176
00:06:54,639 --> 00:06:57,120
result of the provided incorrect values

177
00:06:57,120 --> 00:06:59,039
we compute this as the distance between

178
00:06:59,039 --> 00:07:00,479
the statistics provided by the

179
00:07:00,479 --> 00:07:02,319
researcher and the ones that shouldn't

180
00:07:02,319 --> 00:07:06,080
be returned as part of the research

181
00:07:06,160 --> 00:07:08,160
first we study the impact of epsilon

182
00:07:08,160 --> 00:07:10,240
value the amount of noise added to the

183
00:07:10,240 --> 00:07:12,080
original data set be a randomized

184
00:07:12,080 --> 00:07:14,240
response to the true positive rate and

185
00:07:14,240 --> 00:07:17,039
true negative rate values in the table

186
00:07:17,039 --> 00:07:19,280
we showed true positive rate values for

187
00:07:19,280 --> 00:07:21,360
each statistic with varying values of

188
00:07:21,360 --> 00:07:23,919
epsilon in lactose intolerance

189
00:07:23,919 --> 00:07:27,599
we observe a true positive rate of 0.73

190
00:07:27,599 --> 00:07:30,800
for p-value when epsilon is equal to 3.

191
00:07:30,800 --> 00:07:32,400
this means that the verifier can

192
00:07:32,400 --> 00:07:35,280
correctly classify 73 out of 100

193
00:07:35,280 --> 00:07:37,919
p-values provided by the researcher

194
00:07:37,919 --> 00:07:39,520
we show the variation of the true

195
00:07:39,520 --> 00:07:41,520
negative rate values with respect to the

196
00:07:41,520 --> 00:07:45,440
utility loss for p-values in the figure

197
00:07:45,440 --> 00:07:48,400
for instance for epsilon equal to 3 a

198
00:07:48,400 --> 00:07:50,319
verifier can determine with high

199
00:07:50,319 --> 00:07:52,240
confidence a true negative rate of at

200
00:07:52,240 --> 00:07:55,280
least 0.8 if the researcher has released

201
00:07:55,280 --> 00:07:57,680
incorrect p-values when the utility loss

202
00:07:57,680 --> 00:08:00,240
is at least 0.28

203
00:08:00,240 --> 00:08:03,039
whereas the verifier obtains a loan true

204
00:08:03,039 --> 00:08:06,000
negative rate smaller than 0.4 when the

205
00:08:06,000 --> 00:08:07,919
released p values are close to the

206
00:08:07,919 --> 00:08:09,919
correct ones

207
00:08:09,919 --> 00:08:12,479
in other words the return p-value still

208
00:08:12,479 --> 00:08:14,800
preserve a high utility plus the

209
00:08:14,800 --> 00:08:17,120
proposed framework successfully detects

210
00:08:17,120 --> 00:08:19,120
when a weekly associated snip is

211
00:08:19,120 --> 00:08:21,440
reported to have a low p-value and vice

212
00:08:21,440 --> 00:08:24,560
versa we obtain similar results for the

213
00:08:24,560 --> 00:08:28,639
other two data sets and statistics

214
00:08:29,120 --> 00:08:31,199
the previous results do not consider the

215
00:08:31,199 --> 00:08:33,519
p-values of the return statistics in

216
00:08:33,519 --> 00:08:35,839
practice the verifier is only interested

217
00:08:35,839 --> 00:08:38,080
in strongly associated snips with

218
00:08:38,080 --> 00:08:40,880
p-values smaller than 0.05

219
00:08:40,880 --> 00:08:42,880
in the figure on the left we show the

220
00:08:42,880 --> 00:08:45,360
error obtained by the verifier per 100

221
00:08:45,360 --> 00:08:48,080
correctly computed p-values with respect

222
00:08:48,080 --> 00:08:50,080
to the cutoff point and their p-values

223
00:08:50,080 --> 00:08:52,160
for epsilon equal to 3.

224
00:08:52,160 --> 00:08:54,480
we observe that the verifier achieves a

225
00:08:54,480 --> 00:08:56,720
true positive rate of almost 1 for the

226
00:08:56,720 --> 00:08:58,560
strong associations

227
00:08:58,560 --> 00:09:00,480
in the figure on the right we show the

228
00:09:00,480 --> 00:09:02,640
error of 100 incorrect statistics and

229
00:09:02,640 --> 00:09:04,800
their reported p-values whose correct

230
00:09:04,800 --> 00:09:07,519
p-value is around 0.08

231
00:09:07,519 --> 00:09:09,760
we observe that the verifier obtains a

232
00:09:09,760 --> 00:09:12,320
true negative rate of almost 1.

233
00:09:12,320 --> 00:09:14,720
we obtain similar results when we reduce

234
00:09:14,720 --> 00:09:16,480
the p-value threshold for strong

235
00:09:16,480 --> 00:09:18,880
associations

236
00:09:18,880 --> 00:09:21,040
these results show that a verifier can

237
00:09:21,040 --> 00:09:22,720
use the proposed scheme with high

238
00:09:22,720 --> 00:09:24,800
confidence especially for snips that are

239
00:09:24,800 --> 00:09:29,120
highly associated with studied phenotype

240
00:09:29,440 --> 00:09:31,519
to quantify the membership inference

241
00:09:31,519 --> 00:09:33,839
risk due to the release statistics we

242
00:09:33,839 --> 00:09:36,000
use the likelihood ratio test by some

243
00:09:36,000 --> 00:09:38,560
grammar at all we assume that under the

244
00:09:38,560 --> 00:09:40,720
null hypothesis a target individual is

245
00:09:40,720 --> 00:09:42,959
not part of the case group and under the

246
00:09:42,959 --> 00:09:45,279
alternate hypothesis target individual

247
00:09:45,279 --> 00:09:47,680
is part of the case group the overall

248
00:09:47,680 --> 00:09:50,320
likelihood ratio of an individual can be

249
00:09:50,320 --> 00:09:51,920
computed as shown

250
00:09:51,920 --> 00:09:54,080
the aggregate allele frequencies of the

251
00:09:54,080 --> 00:09:56,160
case group are provided as part of the

252
00:09:56,160 --> 00:09:58,320
trivia's output while the population

253
00:09:58,320 --> 00:10:00,320
only frequencies can be acquired from

254
00:10:00,320 --> 00:10:02,240
public data sources such as thousand

255
00:10:02,240 --> 00:10:05,040
genomes

256
00:10:05,040 --> 00:10:07,120
to quantify the risk due to the shared

257
00:10:07,120 --> 00:10:09,360
partial noise data set we use the

258
00:10:09,360 --> 00:10:11,519
hamming distance between the genomes

259
00:10:11,519 --> 00:10:13,200
hamming distance shows the minimum

260
00:10:13,200 --> 00:10:15,680
number of positions at which the genome

261
00:10:15,680 --> 00:10:18,240
sequences are different for instance

262
00:10:18,240 --> 00:10:20,959
given x and y the minimum number of

263
00:10:20,959 --> 00:10:24,000
substitution required to convert x to y

264
00:10:24,000 --> 00:10:26,000
is 2.

265
00:10:26,000 --> 00:10:28,399
to quantify the risk for each individual

266
00:10:28,399 --> 00:10:30,079
i we compute the hamming distance

267
00:10:30,079 --> 00:10:32,240
between the target individual and all

268
00:10:32,240 --> 00:10:34,079
the individuals in the case group of the

269
00:10:34,079 --> 00:10:36,160
partial nosy data set

270
00:10:36,160 --> 00:10:37,920
and identify the minimum hamming

271
00:10:37,920 --> 00:10:40,640
distance if this system is smaller than

272
00:10:40,640 --> 00:10:42,480
the threshold then it is part of the

273
00:10:42,480 --> 00:10:46,720
data set otherwise it is not

274
00:10:46,720 --> 00:10:48,880
we use lactose intolerance to evaluate

275
00:10:48,880 --> 00:10:50,720
the performance of membership inference

276
00:10:50,720 --> 00:10:53,040
attacks we empirically build the null

277
00:10:53,040 --> 00:10:55,600
hypothesis using 25 individuals that are

278
00:10:55,600 --> 00:10:58,079
not part of the case group for testing

279
00:10:58,079 --> 00:11:00,800
we use 25 randomly selected individuals

280
00:11:00,800 --> 00:11:02,480
from the case group

281
00:11:02,480 --> 00:11:04,800
the figure displays the power curves for

282
00:11:04,800 --> 00:11:06,959
likelihood ratio test for varying number

283
00:11:06,959 --> 00:11:09,839
of statistics in the same figure we also

284
00:11:09,839 --> 00:11:11,519
show the power curves for having

285
00:11:11,519 --> 00:11:13,760
distance for different epsilon values

286
00:11:13,760 --> 00:11:15,519
while varying the number of sniffs in

287
00:11:15,519 --> 00:11:17,600
the partial nozzy data set

288
00:11:17,600 --> 00:11:19,680
we observe that the privacy risk due to

289
00:11:19,680 --> 00:11:22,160
the proposed framework is lower than the

290
00:11:22,160 --> 00:11:24,560
one due to the release given statistics

291
00:11:24,560 --> 00:11:28,000
when epsilon value is smaller than 5.

292
00:11:28,000 --> 00:11:30,000
the verifier also achieves a high true

293
00:11:30,000 --> 00:11:31,839
positive rate and true negative rate for

294
00:11:31,839 --> 00:11:33,760
such cases

295
00:11:33,760 --> 00:11:36,079
to conclude we have proposed a framework

296
00:11:36,079 --> 00:11:38,160
that can be used by verifier to

297
00:11:38,160 --> 00:11:40,240
efficiently verify the correctness of

298
00:11:40,240 --> 00:11:42,320
the computations in chivas with high

299
00:11:42,320 --> 00:11:43,519
confidence

300
00:11:43,519 --> 00:11:45,760
we have shown that the bios risk of the

301
00:11:45,760 --> 00:11:48,240
dataset participants does not increase

302
00:11:48,240 --> 00:11:49,839
due to the additional information

303
00:11:49,839 --> 00:11:53,600
required by the proposed framework

304
00:11:53,600 --> 00:11:55,600
as a future award we will integrate the

305
00:11:55,600 --> 00:11:57,360
verification of the quality control

306
00:11:57,360 --> 00:11:59,040
steps

307
00:11:59,040 --> 00:12:01,200
we will extend the proposed framework so

308
00:12:01,200 --> 00:12:03,040
that the verifier can check both the

309
00:12:03,040 --> 00:12:04,720
relevance and the correctness of the

310
00:12:04,720 --> 00:12:07,200
research findings

311
00:12:07,200 --> 00:12:09,120
thank you for your attention if you have

312
00:12:09,120 --> 00:12:11,120
any questions please feel free to

313
00:12:11,120 --> 00:12:14,160
contact us

