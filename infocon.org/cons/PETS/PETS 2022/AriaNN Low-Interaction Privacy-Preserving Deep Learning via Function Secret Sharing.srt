1
00:00:00,240 --> 00:00:02,720
welcome to this presentation of aryan

2
00:00:02,720 --> 00:00:05,440
a low interaction framework for privacy

3
00:00:05,440 --> 00:00:07,120
presenting deep learning using

4
00:00:07,120 --> 00:00:09,040
functional signaturing

5
00:00:09,040 --> 00:00:12,000
this is a drawing work with piertolonia

6
00:00:12,000 --> 00:00:15,120
david penshaval and francis back

7
00:00:15,120 --> 00:00:19,198
and it was done at ens and in riya

8
00:00:19,279 --> 00:00:21,840
so what is the context here

9
00:00:21,840 --> 00:00:23,600
the objective is to

10
00:00:23,600 --> 00:00:26,160
investigate new algorithm for private

11
00:00:26,160 --> 00:00:28,640
evaluation but also training of real

12
00:00:28,640 --> 00:00:30,560
world deep learning

13
00:00:30,560 --> 00:00:32,960
neural networks

14
00:00:32,960 --> 00:00:36,239
we will focus on algebra secret sharing

15
00:00:36,239 --> 00:00:38,079
and i will give you like a small

16
00:00:38,079 --> 00:00:40,480
reminder of how it works

17
00:00:40,480 --> 00:00:42,640
so say you have alice and bob

18
00:00:42,640 --> 00:00:44,800
that have their private inputs so seven

19
00:00:44,800 --> 00:00:46,160
and three and they want to do some

20
00:00:46,160 --> 00:00:48,559
computation maybe the addition uh on

21
00:00:48,559 --> 00:00:51,440
this input without previewing them

22
00:00:51,440 --> 00:00:53,680
so first they build secret shares of

23
00:00:53,680 --> 00:00:55,440
these inputs

24
00:00:55,440 --> 00:00:57,120
so here's

25
00:00:57,120 --> 00:00:59,039
it's simple numbers for the sake of

26
00:00:59,039 --> 00:01:01,359
arithmetics but it should be completely

27
00:01:01,359 --> 00:01:03,680
random numbers in each space

28
00:01:03,680 --> 00:01:06,400
they distribute their shares

29
00:01:06,400 --> 00:01:08,240
across the different parties

30
00:01:08,240 --> 00:01:10,720
and then they are able to interact

31
00:01:10,720 --> 00:01:13,600
and compute over the shares to

32
00:01:13,600 --> 00:01:15,680
do some computation so here for the case

33
00:01:15,680 --> 00:01:17,840
of addition they only need to add

34
00:01:17,840 --> 00:01:19,840
locally the shares and they build new

35
00:01:19,840 --> 00:01:22,320
shares that happen to be

36
00:01:22,320 --> 00:01:25,119
the private shares of the result of

37
00:01:25,119 --> 00:01:26,720
seven plus

38
00:01:26,720 --> 00:01:28,960
three

39
00:01:29,040 --> 00:01:30,880
so

40
00:01:30,880 --> 00:01:32,320
this is what we call secret sharing

41
00:01:32,320 --> 00:01:34,880
because no single party can reconstruct

42
00:01:34,880 --> 00:01:37,040
alone sensitive data because the shares

43
00:01:37,040 --> 00:01:38,640
are completely random

44
00:01:38,640 --> 00:01:40,560
and a cool feature about it secret

45
00:01:40,560 --> 00:01:42,720
sharing is shared governance

46
00:01:42,720 --> 00:01:45,040
which means that data can only be used

47
00:01:45,040 --> 00:01:47,920
or decrypted if everyone or sufficient

48
00:01:47,920 --> 00:01:51,920
subset of the parties agrees to do so

49
00:01:52,640 --> 00:01:54,960
and so back to our

50
00:01:54,960 --> 00:01:57,280
to our use case of

51
00:01:57,280 --> 00:01:59,840
machine learning and data and models can

52
00:01:59,840 --> 00:02:01,360
also be secret shared exactly in the

53
00:02:01,360 --> 00:02:04,399
same way if you take data data maybe

54
00:02:04,399 --> 00:02:07,040
like a picture is composed of pixels and

55
00:02:07,040 --> 00:02:08,959
these pixels can just be

56
00:02:08,959 --> 00:02:11,599
secret shared and the same for neural

57
00:02:11,599 --> 00:02:13,520
networks which are just composed of

58
00:02:13,520 --> 00:02:15,120
parameters that can also be secret

59
00:02:15,120 --> 00:02:17,360
shared

60
00:02:17,440 --> 00:02:18,480
so

61
00:02:18,480 --> 00:02:20,959
what operations are needed to do to do

62
00:02:20,959 --> 00:02:22,640
private machine learning

63
00:02:22,640 --> 00:02:24,480
we need addition and we have already

64
00:02:24,480 --> 00:02:26,000
seen how to do this

65
00:02:26,000 --> 00:02:27,760
um we also need multiplication and

66
00:02:27,760 --> 00:02:29,920
matrix multiplication this is actually

67
00:02:29,920 --> 00:02:33,360
not very difficult in the sense that um

68
00:02:33,360 --> 00:02:35,120
the only difference is that we will

69
00:02:35,120 --> 00:02:37,440
require now a third party that we call

70
00:02:37,440 --> 00:02:39,519
the cryptographic providers

71
00:02:39,519 --> 00:02:40,959
that will provide some cryptographic

72
00:02:40,959 --> 00:02:43,360
primitives for the competition and one

73
00:02:43,360 --> 00:02:45,040
extra round of interaction between the

74
00:02:45,040 --> 00:02:47,360
parties will be needed

75
00:02:47,360 --> 00:02:49,120
the most challenging part here is the

76
00:02:49,120 --> 00:02:51,760
comparison comparisons are widely used

77
00:02:51,760 --> 00:02:54,000
used for um the real activation

78
00:02:54,000 --> 00:02:56,560
functions that we that we have in in

79
00:02:56,560 --> 00:02:58,800
neural networks

80
00:02:58,800 --> 00:03:01,440
there there were already a long line of

81
00:03:01,440 --> 00:03:04,319
work around how to compute some

82
00:03:04,319 --> 00:03:06,000
efficiently

83
00:03:06,000 --> 00:03:08,480
the comparison in

84
00:03:08,480 --> 00:03:10,640
in mpc

85
00:03:10,640 --> 00:03:12,640
but most of these techniques actually

86
00:03:12,640 --> 00:03:15,120
rely on a quite important number of

87
00:03:15,120 --> 00:03:17,120
front of communication and this is what

88
00:03:17,120 --> 00:03:20,080
we are trying to investigate here

89
00:03:20,080 --> 00:03:22,239
so say we have this function f that we

90
00:03:22,239 --> 00:03:23,280
want to

91
00:03:23,280 --> 00:03:27,519
to compute on an input x

92
00:03:27,920 --> 00:03:31,200
for example it can it can be comparison

93
00:03:31,200 --> 00:03:34,239
to a threshold so zero if you want to do

94
00:03:34,239 --> 00:03:37,200
the roller activation function

95
00:03:37,200 --> 00:03:39,280
when we are in classical typical sharing

96
00:03:39,280 --> 00:03:41,280
what we do is we have our sql shared

97
00:03:41,280 --> 00:03:43,120
input x

98
00:03:43,120 --> 00:03:46,959
and we will apply a public protocol

99
00:03:46,959 --> 00:03:49,680
and which will be applied by each party

100
00:03:49,680 --> 00:03:51,519
in order to output

101
00:03:51,519 --> 00:03:54,720
secret shares of the results

102
00:03:54,720 --> 00:03:56,159
in functional secret sharing it's

103
00:03:56,159 --> 00:03:58,080
actually the other way around

104
00:03:58,080 --> 00:04:01,200
it's supposed to have a public input

105
00:04:01,200 --> 00:04:04,159
and private protocols that we call

106
00:04:04,159 --> 00:04:06,159
function shares

107
00:04:06,159 --> 00:04:08,000
so in this case

108
00:04:08,000 --> 00:04:10,080
the function shares could be

109
00:04:10,080 --> 00:04:12,000
still a comparison but a comparison to

110
00:04:12,000 --> 00:04:14,640
an unknown threshold alpha

111
00:04:14,640 --> 00:04:15,680
and

112
00:04:15,680 --> 00:04:17,839
then each party will use the function

113
00:04:17,839 --> 00:04:20,798
shares on the public input to

114
00:04:20,798 --> 00:04:23,120
compute private results

115
00:04:23,120 --> 00:04:25,360
that will actually sub up to the

116
00:04:25,360 --> 00:04:27,680
original value

117
00:04:27,680 --> 00:04:28,720
so

118
00:04:28,720 --> 00:04:32,880
to get to this framework we actually and

119
00:04:32,880 --> 00:04:34,479
as we have

120
00:04:34,479 --> 00:04:37,360
sequential inputs so here's the y

121
00:04:37,360 --> 00:04:38,800
input

122
00:04:38,800 --> 00:04:43,600
what we do is we resort to masking

123
00:04:43,600 --> 00:04:46,639
a private and signature mask alpha

124
00:04:46,639 --> 00:04:48,639
is also provided by

125
00:04:48,639 --> 00:04:51,199
the crypto provider alongside the

126
00:04:51,199 --> 00:04:52,639
function chairs

127
00:04:52,639 --> 00:04:54,720
and this allows to

128
00:04:54,720 --> 00:04:57,840
publicly reveal x which is

129
00:04:57,840 --> 00:04:58,639
so

130
00:04:58,639 --> 00:05:00,639
y plus alpha where alpha is a completely

131
00:05:00,639 --> 00:05:02,720
random mask

132
00:05:02,720 --> 00:05:04,400
and

133
00:05:04,400 --> 00:05:06,800
and because the crypto provider has

134
00:05:06,800 --> 00:05:09,520
chosen the mask alpha it can make sure

135
00:05:09,520 --> 00:05:12,080
that the threshold in the in the

136
00:05:12,080 --> 00:05:14,880
function shares is also alpha so what we

137
00:05:14,880 --> 00:05:17,520
do at the end is that we are

138
00:05:17,520 --> 00:05:19,280
computing x

139
00:05:19,280 --> 00:05:21,919
compared to alpha which is equivalent

140
00:05:21,919 --> 00:05:24,000
for real numbers

141
00:05:24,000 --> 00:05:25,680
to having

142
00:05:25,680 --> 00:05:28,160
y compared to zero

143
00:05:28,160 --> 00:05:29,840
so the protocols

144
00:05:29,840 --> 00:05:32,479
actually relies on

145
00:05:32,479 --> 00:05:34,160
bit decomposition and bit per bit

146
00:05:34,160 --> 00:05:36,160
comparison

147
00:05:36,160 --> 00:05:39,440
because x is public it's very easy to

148
00:05:39,440 --> 00:05:42,639
compute the bit decomposition of x

149
00:05:42,639 --> 00:05:45,759
and for alpha because it's chosen by the

150
00:05:45,759 --> 00:05:47,840
crypto provider

151
00:05:47,840 --> 00:05:50,080
the kepler provider is also able to

152
00:05:50,080 --> 00:05:53,840
compute this decomposition

153
00:05:55,600 --> 00:05:58,479
so the bit bit comparison actually spans

154
00:05:58,479 --> 00:06:01,520
a binary tree which i've shown here for

155
00:06:01,520 --> 00:06:02,800
only

156
00:06:02,800 --> 00:06:05,199
for any quality three bits but in

157
00:06:05,199 --> 00:06:09,120
practice we will rather use 32 bits

158
00:06:09,120 --> 00:06:11,280
and goes this way we start by the most

159
00:06:11,280 --> 00:06:15,120
significant the most significant bit

160
00:06:15,120 --> 00:06:16,800
and so that's

161
00:06:16,800 --> 00:06:18,240
our x1

162
00:06:18,240 --> 00:06:21,600
and um say now that x1 is bigger than

163
00:06:21,600 --> 00:06:22,880
alpha one

164
00:06:22,880 --> 00:06:25,440
so we go on the right hand side of the

165
00:06:25,440 --> 00:06:27,680
part of the tree and we know that

166
00:06:27,680 --> 00:06:30,000
whatever happened next

167
00:06:30,000 --> 00:06:32,319
the output of the comparison will be

168
00:06:32,319 --> 00:06:33,600
will be false

169
00:06:33,600 --> 00:06:37,039
here alpha is represented on this red

170
00:06:37,039 --> 00:06:39,199
line here

171
00:06:39,199 --> 00:06:41,919
so maybe we will now compare the second

172
00:06:41,919 --> 00:06:43,199
the second bit

173
00:06:43,199 --> 00:06:46,319
say that x2 is strictly smaller than

174
00:06:46,319 --> 00:06:49,039
alpha 2 so we are on the left side of

175
00:06:49,039 --> 00:06:50,160
the tree

176
00:06:50,160 --> 00:06:52,160
and we know that no matter what happens

177
00:06:52,160 --> 00:06:53,120
next

178
00:06:53,120 --> 00:06:55,759
the result of the comparison will be 2

179
00:06:55,759 --> 00:06:58,720
and if now the 2 bits are equal we need

180
00:06:58,720 --> 00:07:01,199
to go to the next part to decide whether

181
00:07:01,199 --> 00:07:05,360
the the computation is true or false

182
00:07:05,360 --> 00:07:07,120
so this is the spirit of the

183
00:07:07,120 --> 00:07:10,720
implementation of our protocol

184
00:07:10,720 --> 00:07:13,039
this protocol achieves honest particular

185
00:07:13,039 --> 00:07:14,160
security

186
00:07:14,160 --> 00:07:16,960
it's a two-party computation protocol

187
00:07:16,960 --> 00:07:20,160
and it's relying on a trusted deal

188
00:07:20,160 --> 00:07:22,240
it actually has a small error rate

189
00:07:22,240 --> 00:07:24,800
because of edge effects

190
00:07:24,800 --> 00:07:25,919
but

191
00:07:25,919 --> 00:07:29,360
it can be avoided with extra computation

192
00:07:29,360 --> 00:07:32,080
and actually there was also a follow-up

193
00:07:32,080 --> 00:07:33,599
work that

194
00:07:33,599 --> 00:07:35,840
i've shown how to

195
00:07:35,840 --> 00:07:38,400
have a similar protocol but without this

196
00:07:38,400 --> 00:07:39,360
error

197
00:07:39,360 --> 00:07:40,800
but actually as we show in our

198
00:07:40,800 --> 00:07:42,880
experiment it does not affect the

199
00:07:42,880 --> 00:07:46,800
accuracy of our models

200
00:07:46,960 --> 00:07:48,400
so why did we choose function

201
00:07:48,400 --> 00:07:50,960
signaturing for private machine learning

202
00:07:50,960 --> 00:07:53,039
the first reason is

203
00:07:53,039 --> 00:07:54,960
that it is part of the family of

204
00:07:54,960 --> 00:07:57,599
multiparty computation protocols that

205
00:07:57,599 --> 00:07:59,199
only rely on very lightweight

206
00:07:59,199 --> 00:08:01,039
cryptographic primitives

207
00:08:01,039 --> 00:08:02,960
actually all we need to do is to compute

208
00:08:02,960 --> 00:08:04,479
ies

209
00:08:04,479 --> 00:08:06,560
so this allowed us to

210
00:08:06,560 --> 00:08:07,520
provide

211
00:08:07,520 --> 00:08:11,120
an implementation on the gpu

212
00:08:11,120 --> 00:08:14,240
uh which is also uh very important for

213
00:08:14,240 --> 00:08:17,520
uh for machine learning practitioners

214
00:08:17,520 --> 00:08:18,639
and

215
00:08:18,639 --> 00:08:20,879
most importantly it reduces considerably

216
00:08:20,879 --> 00:08:22,479
the number of front of communication

217
00:08:22,479 --> 00:08:24,160
compared to other protocols that like

218
00:08:24,160 --> 00:08:25,680
like securing for example that we have

219
00:08:25,680 --> 00:08:26,560
shown

220
00:08:26,560 --> 00:08:29,840
because we only need one

221
00:08:30,639 --> 00:08:32,479
one communication round for private

222
00:08:32,479 --> 00:08:33,760
comparison

223
00:08:33,760 --> 00:08:36,240
and in addition this round is very

224
00:08:36,240 --> 00:08:37,599
is very efficient

225
00:08:37,599 --> 00:08:39,519
in terms of the number of information

226
00:08:39,519 --> 00:08:41,519
that you transmit

227
00:08:41,519 --> 00:08:44,560
it is of the size of the input

228
00:08:44,560 --> 00:08:47,519
the drawback is actually the size of the

229
00:08:47,519 --> 00:08:49,440
proposing keys this function shares that

230
00:08:49,440 --> 00:08:51,200
we have shown

231
00:08:51,200 --> 00:08:54,240
for a resnet model

232
00:08:54,240 --> 00:08:57,760
taking as input a 200 200 pixel image

233
00:08:57,760 --> 00:09:00,800
the keys can be as heavy as 2 gigabytes

234
00:09:00,800 --> 00:09:02,480
it means that it can be really too

235
00:09:02,480 --> 00:09:04,399
tricky to store all these keys in

236
00:09:04,399 --> 00:09:06,560
advance

237
00:09:06,560 --> 00:09:07,440
so

238
00:09:07,440 --> 00:09:10,000
here's uh the results we got for uh

239
00:09:10,000 --> 00:09:12,399
doing private training on small uh

240
00:09:12,399 --> 00:09:15,360
models like linux on amnest and

241
00:09:15,360 --> 00:09:17,120
what we show is that in a couple of

242
00:09:17,120 --> 00:09:20,399
hours would get the same accuracy

243
00:09:20,399 --> 00:09:23,360
than if it was trained on plaintext

244
00:09:23,360 --> 00:09:25,600
we were able to do a private evaluation

245
00:09:25,600 --> 00:09:29,839
on bigger models like vg16 or resnet18

246
00:09:29,839 --> 00:09:31,360
we see that we get

247
00:09:31,360 --> 00:09:33,680
inference in a couple of seconds

248
00:09:33,680 --> 00:09:36,640
and we can even improve this result with

249
00:09:36,640 --> 00:09:39,920
our gpu implementation

250
00:09:39,920 --> 00:09:41,360
so that's quite interesting because it

251
00:09:41,360 --> 00:09:43,040
means that we can

252
00:09:43,040 --> 00:09:46,320
it can match real scenarios

253
00:09:46,320 --> 00:09:48,160
where we can only wait for a couple of

254
00:09:48,160 --> 00:09:50,399
seconds to get the result

255
00:09:50,399 --> 00:09:52,080
and this is actually what we did in

256
00:09:52,080 --> 00:09:54,640
another study um while we were doing

257
00:09:54,640 --> 00:09:56,560
end-to-end privacy preserving deep

258
00:09:56,560 --> 00:09:58,959
learning on medical imaging

259
00:09:58,959 --> 00:10:01,760
so we had a task which was to

260
00:10:01,760 --> 00:10:04,800
detect if an image would be a viral

261
00:10:04,800 --> 00:10:07,360
bacteria or normal and functional secret

262
00:10:07,360 --> 00:10:09,680
sharing so iron was used here as secure

263
00:10:09,680 --> 00:10:11,680
inference as a service

264
00:10:11,680 --> 00:10:13,200
and that's um

265
00:10:13,200 --> 00:10:15,839
that's very interesting because it's

266
00:10:15,839 --> 00:10:18,240
it's a private private evaluation of a

267
00:10:18,240 --> 00:10:20,880
resident 18 that had expert level

268
00:10:20,880 --> 00:10:22,880
accuracy on this task

269
00:10:22,880 --> 00:10:25,279
and in this scenario latency matters

270
00:10:25,279 --> 00:10:27,200
because the practitioner is waiting for

271
00:10:27,200 --> 00:10:28,720
the response and

272
00:10:28,720 --> 00:10:30,640
we are not always able to provide

273
00:10:30,640 --> 00:10:31,839
batching

274
00:10:31,839 --> 00:10:33,680
as we do in our

275
00:10:33,680 --> 00:10:36,959
in in our in and also in other many

276
00:10:36,959 --> 00:10:39,760
results where we provide a

277
00:10:39,760 --> 00:10:42,320
batch result of

278
00:10:42,320 --> 00:10:44,560
our inference time which allows to

279
00:10:44,560 --> 00:10:47,920
amortize the cost of communication

280
00:10:47,920 --> 00:10:49,839
and because we are quite efficient with

281
00:10:49,839 --> 00:10:51,440
our single one of communication

282
00:10:51,440 --> 00:10:53,360
information secret sharing

283
00:10:53,360 --> 00:10:54,240
here

284
00:10:54,240 --> 00:10:58,000
here we can gain a real advantage

285
00:10:58,720 --> 00:11:01,519
a few remarks uh so we are only in the

286
00:11:01,519 --> 00:11:03,920
honest but curious security setting

287
00:11:03,920 --> 00:11:06,240
uh that would be like an interesting

288
00:11:06,240 --> 00:11:08,079
party um

289
00:11:08,079 --> 00:11:09,360
to work on

290
00:11:09,360 --> 00:11:11,440
and also uh

291
00:11:11,440 --> 00:11:13,120
it means that the model is not visible

292
00:11:13,120 --> 00:11:15,360
at all during training and this is very

293
00:11:15,360 --> 00:11:17,120
interesting because that's a condition

294
00:11:17,120 --> 00:11:19,440
that we need for some different privacy

295
00:11:19,440 --> 00:11:21,600
algorithm like the one based on launcher

296
00:11:21,600 --> 00:11:22,640
division

297
00:11:22,640 --> 00:11:25,120
and which suggests that we can expect

298
00:11:25,120 --> 00:11:27,120
powerful combination of techniques like

299
00:11:27,120 --> 00:11:30,160
function sequentially with for example

300
00:11:30,160 --> 00:11:32,959
differential previous

301
00:11:33,120 --> 00:11:34,079
if you want to hear more about

302
00:11:34,079 --> 00:11:35,920
functional sequentially

303
00:11:35,920 --> 00:11:39,200
tomorrow there is a session

304
00:11:39,200 --> 00:11:42,160
with two other work pika and alma

305
00:11:42,160 --> 00:11:43,200
that

306
00:11:43,200 --> 00:11:45,760
rely on this work and provide some very

307
00:11:45,760 --> 00:11:47,600
interesting extension

308
00:11:47,600 --> 00:11:48,399
and

309
00:11:48,399 --> 00:11:50,639
alongside nice benchmark

310
00:11:50,639 --> 00:11:53,040
so thank you very much and i'm happy to

311
00:11:53,040 --> 00:11:56,680
take any questions

