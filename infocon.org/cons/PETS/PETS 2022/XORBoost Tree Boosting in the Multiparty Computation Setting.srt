1
00:00:00,640 --> 00:00:02,240
hello everyone and welcome to our

2
00:00:02,240 --> 00:00:05,359
presentation entitled exorboost tree

3
00:00:05,359 --> 00:00:07,200
boosting in the multi-party computation

4
00:00:07,200 --> 00:00:08,559
setting

5
00:00:08,559 --> 00:00:11,360
the presenters are mark de grosse and

6
00:00:11,360 --> 00:00:13,840
marius vui and we both work for a

7
00:00:13,840 --> 00:00:16,000
company called infer that is u.s and

8
00:00:16,000 --> 00:00:18,880
switzerland based

9
00:00:18,880 --> 00:00:20,800
so a small overview about this

10
00:00:20,800 --> 00:00:22,400
presentation first we're going to talk

11
00:00:22,400 --> 00:00:24,560
about tree-based learning

12
00:00:24,560 --> 00:00:26,560
so people that are already familiar with

13
00:00:26,560 --> 00:00:28,840
that it will be more like

14
00:00:28,840 --> 00:00:31,679
recalls then we're gonna explain how we

15
00:00:31,679 --> 00:00:34,399
adapted this to the mpc setting

16
00:00:34,399 --> 00:00:35,760
and in the end we're gonna show some

17
00:00:35,760 --> 00:00:39,040
benchmarks and results

18
00:00:39,040 --> 00:00:40,640
so what are

19
00:00:40,640 --> 00:00:42,960
what are our model assumptions so first

20
00:00:42,960 --> 00:00:44,719
of all we are in

21
00:00:44,719 --> 00:00:47,600
passive we have a passive security model

22
00:00:47,600 --> 00:00:48,879
so the honest

23
00:00:48,879 --> 00:00:51,440
and curious um

24
00:00:51,440 --> 00:00:54,079
security meaning that all the

25
00:00:54,079 --> 00:00:56,559
participants they faithfully execute a

26
00:00:56,559 --> 00:00:58,399
protocol but they still try to learn

27
00:00:58,399 --> 00:01:00,399
everything that they can

28
00:01:00,399 --> 00:01:03,520
from the information they're given

29
00:01:03,520 --> 00:01:06,400
we have full threshold security

30
00:01:06,400 --> 00:01:09,280
we assume that the number of players is

31
00:01:09,280 --> 00:01:11,680
a rather small let's say

32
00:01:11,680 --> 00:01:14,880
not more than 9 10 participants

33
00:01:14,880 --> 00:01:17,280
and we do support both horizontal and

34
00:01:17,280 --> 00:01:20,320
vertical stacking of the input data and

35
00:01:20,320 --> 00:01:23,758
even a combination of both

36
00:01:24,840 --> 00:01:28,960
so a small reminder about tree-based

37
00:01:28,960 --> 00:01:30,159
learning

38
00:01:30,159 --> 00:01:32,640
suppose you're given a data set x

39
00:01:32,640 --> 00:01:35,200
um let's say it has capital n samples so

40
00:01:35,200 --> 00:01:38,479
each sample being a row of x and it has

41
00:01:38,479 --> 00:01:42,159
k features so k is the number of columns

42
00:01:42,159 --> 00:01:44,399
and let's also suppose here that we have

43
00:01:44,399 --> 00:01:46,799
fixed for each column

44
00:01:46,799 --> 00:01:47,600
um

45
00:01:47,600 --> 00:01:49,439
we have a fixed number of threshold

46
00:01:49,439 --> 00:01:51,200
values

47
00:01:51,200 --> 00:01:53,119
which is typically much smaller than

48
00:01:53,119 --> 00:01:56,000
capital n could be 100 200 so each for

49
00:01:56,000 --> 00:01:57,759
each column we have the same number of

50
00:01:57,759 --> 00:02:00,399
threshold values that we have fixed

51
00:02:00,399 --> 00:02:02,799
and we also have a response vector

52
00:02:02,799 --> 00:02:04,560
capital y

53
00:02:04,560 --> 00:02:07,119
so a tree is basically a function a

54
00:02:07,119 --> 00:02:09,679
function from r to the k to r

55
00:02:09,679 --> 00:02:12,879
which is evaluated in the following way

56
00:02:12,879 --> 00:02:15,120
so let's given a

57
00:02:15,120 --> 00:02:17,360
little x so this is a sample a row of

58
00:02:17,360 --> 00:02:19,280
capital x

59
00:02:19,280 --> 00:02:21,920
you would first if you would first take

60
00:02:21,920 --> 00:02:24,959
its j1 coordinate and compare it against

61
00:02:24,959 --> 00:02:26,959
the threshold t1

62
00:02:26,959 --> 00:02:29,040
if the predicate is satisfied you would

63
00:02:29,040 --> 00:02:31,120
go to the left otherwise you would go to

64
00:02:31,120 --> 00:02:33,200
the right and then you would say

65
00:02:33,200 --> 00:02:34,319
evaluate

66
00:02:34,319 --> 00:02:36,800
the j2's coordinate of x against the

67
00:02:36,800 --> 00:02:38,560
threshold t2

68
00:02:38,560 --> 00:02:40,160
and then depending on the outcome you'd

69
00:02:40,160 --> 00:02:42,959
go to the left or to the right and those

70
00:02:42,959 --> 00:02:44,319
wi's

71
00:02:44,319 --> 00:02:47,200
these are scalars so this would be

72
00:02:47,200 --> 00:02:48,080
the

73
00:02:48,080 --> 00:02:50,239
the value or the evaluation of your

74
00:02:50,239 --> 00:02:51,680
function

75
00:02:51,680 --> 00:02:53,599
obviously a tree could be much larger

76
00:02:53,599 --> 00:02:55,200
than this could be

77
00:02:55,200 --> 00:02:58,239
quite asymmetric in case it got pruned

78
00:02:58,239 --> 00:03:00,720
and you should also

79
00:03:00,720 --> 00:03:02,800
keep in mind that you can have several

80
00:03:02,800 --> 00:03:05,200
trees you can then evaluate them

81
00:03:05,200 --> 00:03:07,440
individually and then your rk to the r

82
00:03:07,440 --> 00:03:09,120
function would just be the sum of these

83
00:03:09,120 --> 00:03:13,120
values of all the evaluated trees

84
00:03:14,080 --> 00:03:16,640
so let me talk a bit about xg boost

85
00:03:16,640 --> 00:03:18,879
which stands for extreme gradient

86
00:03:18,879 --> 00:03:20,400
boosting first of all why are we

87
00:03:20,400 --> 00:03:22,400
interested in this algorithm

88
00:03:22,400 --> 00:03:25,680
because it kind of

89
00:03:26,640 --> 00:03:28,959
it got quite popular by winning a couple

90
00:03:28,959 --> 00:03:31,040
of machine learning competitions so

91
00:03:31,040 --> 00:03:32,480
people are quite interested in this

92
00:03:32,480 --> 00:03:34,319
algorithm

93
00:03:34,319 --> 00:03:36,319
what is the objective so let's say you

94
00:03:36,319 --> 00:03:38,640
have a loss function which comes from

95
00:03:38,640 --> 00:03:40,560
the task you want to achieve could be

96
00:03:40,560 --> 00:03:41,920
regression or you want to do

97
00:03:41,920 --> 00:03:43,519
classification

98
00:03:43,519 --> 00:03:46,239
and you want to determine trees you want

99
00:03:46,239 --> 00:03:49,440
to grow them we say in order to minimize

100
00:03:49,440 --> 00:03:52,319
this loss function

101
00:03:52,319 --> 00:03:53,439
and

102
00:03:53,439 --> 00:03:55,760
what does boosting mean so unlike random

103
00:03:55,760 --> 00:03:57,200
forests

104
00:03:57,200 --> 00:03:58,799
where for example you would

105
00:03:58,799 --> 00:04:01,360
grow several trees simultaneously in

106
00:04:01,360 --> 00:04:03,920
boosting you do things successively so

107
00:04:03,920 --> 00:04:05,680
your model is always

108
00:04:05,680 --> 00:04:09,280
a certain set of trees and given a

109
00:04:09,280 --> 00:04:11,120
once you have determined the tree you

110
00:04:11,120 --> 00:04:13,599
would add it to your model and then it

111
00:04:13,599 --> 00:04:16,000
would not change anymore so first you

112
00:04:16,000 --> 00:04:17,680
start with zero trees you grow one tree

113
00:04:17,680 --> 00:04:19,759
you put this tree to your model

114
00:04:19,759 --> 00:04:22,240
then you start growing a second tree

115
00:04:22,240 --> 00:04:24,160
and you would add the secondary to your

116
00:04:24,160 --> 00:04:28,479
model and then this doesn't move anymore

117
00:04:28,479 --> 00:04:29,600
and

118
00:04:29,600 --> 00:04:31,680
given a fixed model

119
00:04:31,680 --> 00:04:33,600
which is the tree that you would add to

120
00:04:33,600 --> 00:04:35,600
your model well the one

121
00:04:35,600 --> 00:04:37,520
that yields the largest loss reduction

122
00:04:37,520 --> 00:04:40,080
obviously

123
00:04:40,240 --> 00:04:44,560
and um so we do so by

124
00:04:44,560 --> 00:04:46,080
by

125
00:04:46,080 --> 00:04:49,040
determining all these j and t values

126
00:04:49,040 --> 00:04:51,280
of an approximation of your loss

127
00:04:51,280 --> 00:04:53,919
function so obviously this rings a bell

128
00:04:53,919 --> 00:04:55,600
there's nothing else in a second order

129
00:04:55,600 --> 00:04:57,440
gradient descent

130
00:04:57,440 --> 00:05:01,520
that we do in order to determine j and t

131
00:05:01,520 --> 00:05:03,759
and a bit more precisely what does that

132
00:05:03,759 --> 00:05:04,800
mean

133
00:05:04,800 --> 00:05:07,840
for a given node so

134
00:05:07,840 --> 00:05:11,039
as we said we have number of features

135
00:05:11,039 --> 00:05:13,039
columns

136
00:05:13,039 --> 00:05:15,840
and for each of those columns we have a

137
00:05:15,840 --> 00:05:18,400
fixed number of thresholds typically 100

138
00:05:18,400 --> 00:05:19,919
200

139
00:05:19,919 --> 00:05:20,720
and

140
00:05:20,720 --> 00:05:23,039
for all of these candidates we compute

141
00:05:23,039 --> 00:05:25,759
the loss reduction that one would obtain

142
00:05:25,759 --> 00:05:27,919
by choosing the split so by split we

143
00:05:27,919 --> 00:05:29,440
mean j and t

144
00:05:29,440 --> 00:05:31,120
and since we do a second order gradient

145
00:05:31,120 --> 00:05:33,680
descent we have a close formula

146
00:05:33,680 --> 00:05:36,160
for the the weight so these w's that you

147
00:05:36,160 --> 00:05:38,400
would attach to each split which gives

148
00:05:38,400 --> 00:05:40,080
you also a closed formula for the

149
00:05:40,080 --> 00:05:42,560
reduction of the loss so you can really

150
00:05:42,560 --> 00:05:45,199
compute this explicitly

151
00:05:45,199 --> 00:05:47,520
and then we determine

152
00:05:47,520 --> 00:05:49,199
j star and t star

153
00:05:49,199 --> 00:05:51,039
that maximize this

154
00:05:51,039 --> 00:05:52,880
loss reduction

155
00:05:52,880 --> 00:05:55,039
we do so by an oblivious arc max

156
00:05:55,039 --> 00:05:56,800
algorithm

157
00:05:56,800 --> 00:05:58,479
and then we're going to determine

158
00:05:58,479 --> 00:06:00,639
amongst all samples in the node which

159
00:06:00,639 --> 00:06:03,919
ones do satisfy the predicate x j star

160
00:06:03,919 --> 00:06:05,600
less than t star

161
00:06:05,600 --> 00:06:07,600
for those who do satisfy we send them to

162
00:06:07,600 --> 00:06:08,960
the left child

163
00:06:08,960 --> 00:06:10,560
or the other ones we send them to the

164
00:06:10,560 --> 00:06:11,759
right child

165
00:06:11,759 --> 00:06:14,960
and then obviously for both these notes

166
00:06:14,960 --> 00:06:16,720
we're going to do the same thing again

167
00:06:16,720 --> 00:06:20,080
until we have reached the desired depth

168
00:06:20,080 --> 00:06:21,840
where we

169
00:06:21,840 --> 00:06:23,360
have our weights

170
00:06:23,360 --> 00:06:25,840
that we obtained by this close formula

171
00:06:25,840 --> 00:06:27,360
coming from a second order gradient

172
00:06:27,360 --> 00:06:28,560
descent

173
00:06:28,560 --> 00:06:29,360
and

174
00:06:29,360 --> 00:06:33,039
then we have we're done training

175
00:06:33,039 --> 00:06:35,039
so now my colleague mark is going to

176
00:06:35,039 --> 00:06:37,520
tell you how we adapted all those ideas

177
00:06:37,520 --> 00:06:40,799
to the mpc setting

178
00:06:44,240 --> 00:06:46,560
let's now discuss how to adapt xg boost

179
00:06:46,560 --> 00:06:48,800
to the mpc setting

180
00:06:48,800 --> 00:06:51,039
first of all the indices threshold

181
00:06:51,039 --> 00:06:52,880
values and prediction values are going

182
00:06:52,880 --> 00:06:54,479
to be secret shared

183
00:06:54,479 --> 00:06:57,520
that means that the training operation

184
00:06:57,520 --> 00:06:58,880
for the

185
00:06:58,880 --> 00:07:00,639
for the machine learning model does not

186
00:07:00,639 --> 00:07:03,440
reveal any information about the

187
00:07:03,440 --> 00:07:06,240
original data set

188
00:07:06,240 --> 00:07:09,360
threshold candidates so these t values

189
00:07:09,360 --> 00:07:12,240
are obtained by a sorting algorithm and

190
00:07:12,240 --> 00:07:14,319
extracting specific rows i'll go into a

191
00:07:14,319 --> 00:07:17,039
bit more detail on the next slide

192
00:07:17,039 --> 00:07:19,759
for each node we need to keep track of

193
00:07:19,759 --> 00:07:22,160
which sample is to be considered for

194
00:07:22,160 --> 00:07:23,840
this node and we're going to do this

195
00:07:23,840 --> 00:07:26,080
through the use of a 0 1 indicator

196
00:07:26,080 --> 00:07:29,120
vector for each node

197
00:07:29,120 --> 00:07:32,240
we fully grow the tree to full depth so

198
00:07:32,240 --> 00:07:34,000
the depth is a parameter of the

199
00:07:34,000 --> 00:07:37,280
algorithm we grow the tree fully

200
00:07:37,280 --> 00:07:40,080
and then we have a procedure to

201
00:07:40,080 --> 00:07:42,840
obliviously prune the

202
00:07:42,840 --> 00:07:46,080
predicted uh values so we we prune the

203
00:07:46,080 --> 00:07:48,160
branches of the tree and adapt the

204
00:07:48,160 --> 00:07:51,199
predicted values to if the branches is

205
00:07:51,199 --> 00:07:52,879
pruned or not

206
00:07:52,879 --> 00:07:54,639
and finally one last thing to note is

207
00:07:54,639 --> 00:07:58,479
that to compute the loss reduction for

208
00:07:58,479 --> 00:07:59,360
the

209
00:07:59,360 --> 00:08:01,360
different entries of the

210
00:08:01,360 --> 00:08:03,039
of the match of the matrix to which we

211
00:08:03,039 --> 00:08:05,280
will apply the r max we require a

212
00:08:05,280 --> 00:08:08,720
private division algorithm

213
00:08:08,720 --> 00:08:11,360
so i mentioned that we need a sorting

214
00:08:11,360 --> 00:08:13,680
permutation this is going to be done

215
00:08:13,680 --> 00:08:15,840
through an oblivious quick sort

216
00:08:15,840 --> 00:08:17,039
here's a

217
00:08:17,039 --> 00:08:19,360
quick overview so first of all for each

218
00:08:19,360 --> 00:08:21,520
column you apply a uniformly random

219
00:08:21,520 --> 00:08:23,199
permutation

220
00:08:23,199 --> 00:08:24,879
and then you apply the quick sort

221
00:08:24,879 --> 00:08:26,879
algorithm you need to do comparisons

222
00:08:26,879 --> 00:08:29,520
with the pivot you will not reveal

223
00:08:29,520 --> 00:08:31,280
the values

224
00:08:31,280 --> 00:08:33,599
of the array that is to be sorted but

225
00:08:33,599 --> 00:08:35,679
instead you will reveal the result of

226
00:08:35,679 --> 00:08:37,919
these comparisons and by the fact that

227
00:08:37,919 --> 00:08:39,839
you apply the uniform random permutation

228
00:08:39,839 --> 00:08:41,599
this does not reveal any information

229
00:08:41,599 --> 00:08:44,880
about the original data

230
00:08:45,440 --> 00:08:48,480
with these sorting permutations

231
00:08:48,480 --> 00:08:49,760
in end

232
00:08:49,760 --> 00:08:52,480
we're going to call them pi j and this

233
00:08:52,480 --> 00:08:55,200
allows us to have the threshold values

234
00:08:55,200 --> 00:08:57,360
because they're in the sorted matrix

235
00:08:57,360 --> 00:09:00,720
they're going to be at a fixed position

236
00:09:00,720 --> 00:09:03,360
in the in the sorted matrix so that's

237
00:09:03,360 --> 00:09:04,959
great

238
00:09:04,959 --> 00:09:07,279
a few more details about the indicator

239
00:09:07,279 --> 00:09:09,680
vector here's an example so you see you

240
00:09:09,680 --> 00:09:10,640
start with

241
00:09:10,640 --> 00:09:12,399
all one indicator vector because you

242
00:09:12,399 --> 00:09:14,800
consider all the samples and then only a

243
00:09:14,800 --> 00:09:16,880
subset of the samples are considered in

244
00:09:16,880 --> 00:09:20,080
the left and the right child

245
00:09:20,080 --> 00:09:22,240
so then the question is how can you

246
00:09:22,240 --> 00:09:24,080
update these indicator vector if i give

247
00:09:24,080 --> 00:09:26,320
you the indicator vector of a node

248
00:09:26,320 --> 00:09:28,959
um and uh you would like to compute the

249
00:09:28,959 --> 00:09:31,200
indicator vector of the left and right

250
00:09:31,200 --> 00:09:32,640
child

251
00:09:32,640 --> 00:09:34,959
node nodes

252
00:09:34,959 --> 00:09:36,560
then how are you going to do this well

253
00:09:36,560 --> 00:09:38,160
we're going to consider

254
00:09:38,160 --> 00:09:39,600
these um

255
00:09:39,600 --> 00:09:42,160
bucket vectors which have

256
00:09:42,160 --> 00:09:44,399
uh all ones

257
00:09:44,399 --> 00:09:47,920
uh and then all zeros and and

258
00:09:47,920 --> 00:09:51,120
so these would consider these would be

259
00:09:51,120 --> 00:09:52,959
if the

260
00:09:52,959 --> 00:09:55,760
if the value is less than the

261
00:09:55,760 --> 00:09:58,240
the b threshold then it's a one if it's

262
00:09:58,240 --> 00:10:00,480
greater than the b threshold then it's a

263
00:10:00,480 --> 00:10:04,399
zero and if you think about it the if

264
00:10:04,399 --> 00:10:07,920
you have the j star and the t star

265
00:10:07,920 --> 00:10:08,880
the

266
00:10:08,880 --> 00:10:11,200
the entries that need to go

267
00:10:11,200 --> 00:10:13,279
left those that satisfy the predicate

268
00:10:13,279 --> 00:10:15,279
they will be exactly

269
00:10:15,279 --> 00:10:19,839
uh pi j inverse of the b v b so you

270
00:10:19,839 --> 00:10:21,519
apply the inverse of the sorting

271
00:10:21,519 --> 00:10:24,079
permutation to this bucket vector and

272
00:10:24,079 --> 00:10:26,880
that's going to be the trick that's

273
00:10:26,880 --> 00:10:28,640
using this vector this is what is going

274
00:10:28,640 --> 00:10:30,399
to allow us to update the instance

275
00:10:30,399 --> 00:10:31,519
vectors

276
00:10:31,519 --> 00:10:34,720
the indicator vectors

277
00:10:37,839 --> 00:10:38,880
so

278
00:10:38,880 --> 00:10:41,920
the one trick that we use to speed up

279
00:10:41,920 --> 00:10:43,279
the computation

280
00:10:43,279 --> 00:10:46,320
is instead of computing all of these pi

281
00:10:46,320 --> 00:10:50,800
j inverse of b b for all features and

282
00:10:50,800 --> 00:10:52,480
all buckets

283
00:10:52,480 --> 00:10:56,079
we're going to compute log b generator

284
00:10:56,079 --> 00:10:58,079
vectors and then

285
00:10:58,079 --> 00:11:01,040
use these to um

286
00:11:01,040 --> 00:11:04,320
to compute the required pj inverse of

287
00:11:04,320 --> 00:11:06,560
bbb if you want more details about this

288
00:11:06,560 --> 00:11:08,959
procedure you can go in the paper but

289
00:11:08,959 --> 00:11:11,440
the idea is that instead of having

290
00:11:11,440 --> 00:11:12,240
b

291
00:11:12,240 --> 00:11:15,360
vectors we use only log b vectors and

292
00:11:15,360 --> 00:11:16,320
that

293
00:11:16,320 --> 00:11:18,560
results in some speed up and some some

294
00:11:18,560 --> 00:11:20,880
gains

295
00:11:21,040 --> 00:11:24,000
and so finally some benchmark data so

296
00:11:24,000 --> 00:11:27,440
you can see if you uh here the results

297
00:11:27,440 --> 00:11:30,320
are for the wall time and for the memory

298
00:11:30,320 --> 00:11:32,959
according to the data set size so as you

299
00:11:32,959 --> 00:11:35,120
can see even for large data sets of a

300
00:11:35,120 --> 00:11:37,680
hundred thousand by 300

301
00:11:37,680 --> 00:11:38,959
features

302
00:11:38,959 --> 00:11:40,640
um with the

303
00:11:40,640 --> 00:11:43,519
the time stays at around four minutes

304
00:11:43,519 --> 00:11:45,920
per tree which is quite reasonable given

305
00:11:45,920 --> 00:11:48,000
that usually uh

306
00:11:48,000 --> 00:11:49,680
exuberant models they range in the

307
00:11:49,680 --> 00:11:51,920
hundreds of trees

308
00:11:51,920 --> 00:11:54,160
and you and here you can also see the

309
00:11:54,160 --> 00:11:57,440
dependency on the number of buckets and

310
00:11:57,440 --> 00:11:59,920
the tree depth and as expected the wall

311
00:11:59,920 --> 00:12:02,639
time and memory requirements they

312
00:12:02,639 --> 00:12:04,079
increase

313
00:12:04,079 --> 00:12:05,279
with

314
00:12:05,279 --> 00:12:08,639
with three depths and number of buckets

315
00:12:08,639 --> 00:12:11,839
thank you for your attention

