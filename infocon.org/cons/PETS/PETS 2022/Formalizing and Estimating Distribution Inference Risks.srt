1
00:00:00,560 --> 00:00:02,879
hey everyone i'm anshuman from uva and

2
00:00:02,879 --> 00:00:04,720
today i'll talk about a research on

3
00:00:04,720 --> 00:00:07,040
distribution influence risks with my

4
00:00:07,040 --> 00:00:09,440
advisor david evans and right before we

5
00:00:09,440 --> 00:00:11,040
get started i'd like to show you figure

6
00:00:11,040 --> 00:00:14,080
that captures the essence of our work

7
00:00:14,080 --> 00:00:16,160
what you see on the x-axis here is alpha

8
00:00:16,160 --> 00:00:18,160
and that's essentially the probability

9
00:00:18,160 --> 00:00:20,800
of a particular attribute being true for

10
00:00:20,800 --> 00:00:22,960
any given sample that you take from the

11
00:00:22,960 --> 00:00:24,960
training distribution of a given machine

12
00:00:24,960 --> 00:00:27,760
learning model and on the y-axis is the

13
00:00:27,760 --> 00:00:29,760
mean square error of an adversary that

14
00:00:29,760 --> 00:00:31,840
looks at the same model and then tries

15
00:00:31,840 --> 00:00:33,600
to predict this alpha so let's say you

16
00:00:33,600 --> 00:00:37,200
have face images then the alpha for

17
00:00:37,200 --> 00:00:38,719
your specific scenario could be let's

18
00:00:38,719 --> 00:00:40,719
say the probability of that person being

19
00:00:40,719 --> 00:00:42,800
a female and this is what we'll vary in

20
00:00:42,800 --> 00:00:44,399
these experiments

21
00:00:44,399 --> 00:00:45,760
and

22
00:00:45,760 --> 00:00:48,640
this quantity here n leaked is a measure

23
00:00:48,640 --> 00:00:51,199
to look at the given model and setting

24
00:00:51,199 --> 00:00:52,879
and try to quantify how much leakage

25
00:00:52,879 --> 00:00:55,600
there is and what we see is

26
00:00:55,600 --> 00:00:57,199
across all the data sets that we looked

27
00:00:57,199 --> 00:00:59,600
at the leakage is quite non-trivial and

28
00:00:59,600 --> 00:01:03,680
that's what most of the work is about

29
00:01:03,680 --> 00:01:06,080
as part of our work we propose a general

30
00:01:06,080 --> 00:01:07,840
definition for distribution inference

31
00:01:07,840 --> 00:01:09,760
that allows us to come up with a metric

32
00:01:09,760 --> 00:01:11,439
that helps measure the amount of leakage

33
00:01:11,439 --> 00:01:12,799
in these cases

34
00:01:12,799 --> 00:01:14,720
we also propose two new black box

35
00:01:14,720 --> 00:01:16,799
attacks and perform systematic

36
00:01:16,799 --> 00:01:18,799
evaluation of risk across different

37
00:01:18,799 --> 00:01:20,560
properties and data sets and that's what

38
00:01:20,560 --> 00:01:24,080
i'll be focusing on in today's talk

39
00:01:24,880 --> 00:01:26,799
let's say you have some sort of

40
00:01:26,799 --> 00:01:29,200
distribution of data from which you

41
00:01:29,200 --> 00:01:31,200
sample some dataset d

42
00:01:31,200 --> 00:01:33,040
the general pipeline in any such

43
00:01:33,040 --> 00:01:34,960
computation is

44
00:01:34,960 --> 00:01:36,720
you perform computation on the given

45
00:01:36,720 --> 00:01:39,280
dataset and that generates an output so

46
00:01:39,280 --> 00:01:40,640
if you were to draw a parallel with

47
00:01:40,640 --> 00:01:42,159
machine learning the distribution would

48
00:01:42,159 --> 00:01:44,240
be the underlying training distribution

49
00:01:44,240 --> 00:01:45,600
the data set would be the training data

50
00:01:45,600 --> 00:01:47,200
set and then the computation is training

51
00:01:47,200 --> 00:01:48,799
a machine learning model which generates

52
00:01:48,799 --> 00:01:50,560
the machine learning model in the output

53
00:01:50,560 --> 00:01:52,960
itself

54
00:01:53,600 --> 00:01:55,759
the most well-studied level of influence

55
00:01:55,759 --> 00:01:57,920
risk granularity is data set inference

56
00:01:57,920 --> 00:01:59,680
and the most popular example perhaps is

57
00:01:59,680 --> 00:02:01,200
membership inference where you're given

58
00:02:01,200 --> 00:02:02,560
a machine learning model and you're

59
00:02:02,560 --> 00:02:04,320
trying to infer whether a particular

60
00:02:04,320 --> 00:02:05,200
point

61
00:02:05,200 --> 00:02:07,360
was part of the training data set or not

62
00:02:07,360 --> 00:02:09,520
and there's tons of work in both in

63
00:02:09,520 --> 00:02:11,520
terms of attacks defenses as well as

64
00:02:11,520 --> 00:02:13,840
theoretical contributions

65
00:02:13,840 --> 00:02:15,440
now coming over to the distribution

66
00:02:15,440 --> 00:02:16,959
level things get slightly different

67
00:02:16,959 --> 00:02:18,800
because the adversary no longer cares

68
00:02:18,800 --> 00:02:20,560
about the particular dataset but the

69
00:02:20,560 --> 00:02:22,319
underlying distribution from which that

70
00:02:22,319 --> 00:02:24,000
data set for example

71
00:02:24,000 --> 00:02:25,599
so let's say the adversary looks at the

72
00:02:25,599 --> 00:02:27,760
machine learning model and wants to

73
00:02:27,760 --> 00:02:28,879
infer whether the underlying

74
00:02:28,879 --> 00:02:31,760
distribution had let's say more

75
00:02:31,760 --> 00:02:34,400
green samples than red samples and that

76
00:02:34,400 --> 00:02:35,680
is clearly a property of the

77
00:02:35,680 --> 00:02:37,599
distribution because there are multiple

78
00:02:37,599 --> 00:02:39,360
parts possible data sets from the

79
00:02:39,360 --> 00:02:41,120
distribution that would satisfy the same

80
00:02:41,120 --> 00:02:43,680
property and yet would output obviously

81
00:02:43,680 --> 00:02:45,280
different data sets and consequently

82
00:02:45,280 --> 00:02:47,840
different machine learning models

83
00:02:47,840 --> 00:02:49,599
set this up as a cryptographic game

84
00:02:49,599 --> 00:02:52,000
inspired by yom and all's work we start

85
00:02:52,000 --> 00:02:53,440
off with this underlying natural

86
00:02:53,440 --> 00:02:55,280
distribution d which is assumed to be

87
00:02:55,280 --> 00:02:57,120
public and then we have these two

88
00:02:57,120 --> 00:02:59,200
distribution transformation functions g

89
00:02:59,200 --> 00:03:00,640
naught and g one

90
00:03:00,640 --> 00:03:02,800
as a concrete example let's assume d is

91
00:03:02,800 --> 00:03:04,560
the distribution of face images

92
00:03:04,560 --> 00:03:06,720
available on the internet g naught could

93
00:03:06,720 --> 00:03:09,040
be something that creates a distribution

94
00:03:09,040 --> 00:03:11,200
from this distribution such that the

95
00:03:11,200 --> 00:03:13,120
probability of something

96
00:03:13,120 --> 00:03:15,519
a female or not a female is equally

97
00:03:15,519 --> 00:03:17,920
likely whereas g1 is something that

98
00:03:17,920 --> 00:03:20,000
results in the probability of sampling a

99
00:03:20,000 --> 00:03:22,480
female as twice as likely as me let's

100
00:03:22,480 --> 00:03:24,159
say

101
00:03:24,159 --> 00:03:25,760
then what the victim does like any

102
00:03:25,760 --> 00:03:27,760
standard cryptographic game is

103
00:03:27,760 --> 00:03:29,120
samples

104
00:03:29,120 --> 00:03:31,040
a random number between zero and one so

105
00:03:31,040 --> 00:03:32,879
it's either zero or one and based on

106
00:03:32,879 --> 00:03:35,680
that number it picks a training data set

107
00:03:35,680 --> 00:03:36,959
from the underlying training

108
00:03:36,959 --> 00:03:38,720
distributions and trains a machine

109
00:03:38,720 --> 00:03:40,640
learning model m on it

110
00:03:40,640 --> 00:03:42,799
and then the adversary's task is to look

111
00:03:42,799 --> 00:03:45,040
at the model itself along with whatever

112
00:03:45,040 --> 00:03:47,280
public information it has and try to

113
00:03:47,280 --> 00:03:48,879
predict which of these two training

114
00:03:48,879 --> 00:03:50,799
distributions the models training data

115
00:03:50,799 --> 00:03:52,080
came from

116
00:03:52,080 --> 00:03:54,560
and this is very different from a kind

117
00:03:54,560 --> 00:03:56,159
of inference list that looks at the data

118
00:03:56,159 --> 00:03:58,080
set level because this is no longer a

119
00:03:58,080 --> 00:03:59,840
property of the particular data set the

120
00:03:59,840 --> 00:04:02,640
adversary is not interested in this part

121
00:04:02,640 --> 00:04:03,519
of the

122
00:04:03,519 --> 00:04:06,080
whole setup it's interested in this part

123
00:04:06,080 --> 00:04:07,599
of the setup so it really cares about

124
00:04:07,599 --> 00:04:09,519
the distributions and the properties not

125
00:04:09,519 --> 00:04:12,640
the exact data set

126
00:04:13,200 --> 00:04:14,640
and of course

127
00:04:14,640 --> 00:04:16,639
even though the adversary has access to

128
00:04:16,639 --> 00:04:18,478
the victims model there's different

129
00:04:18,478 --> 00:04:20,880
levels of access so the black box access

130
00:04:20,880 --> 00:04:23,440
assumes standard public information that

131
00:04:23,440 --> 00:04:24,720
is the distributions and the

132
00:04:24,720 --> 00:04:26,960
transformation functions itself as well

133
00:04:26,960 --> 00:04:29,440
as api access to the victim model so it

134
00:04:29,440 --> 00:04:31,280
has access to the victim model and

135
00:04:31,280 --> 00:04:33,120
whatever confidence values the victim

136
00:04:33,120 --> 00:04:35,520
model generates for any given data

137
00:04:35,520 --> 00:04:36,960
and for the white box setting the

138
00:04:36,960 --> 00:04:38,880
adversary has of course full access to

139
00:04:38,880 --> 00:04:40,320
the victim model that includes the

140
00:04:40,320 --> 00:04:43,919
parameters architectures and all of that

141
00:04:43,919 --> 00:04:46,400
now i'll describe one of the black box

142
00:04:46,400 --> 00:04:47,840
attacks that we

143
00:04:47,840 --> 00:04:50,639
proposed in our work

144
00:04:50,639 --> 00:04:52,320
now this is a very straightforward test

145
00:04:52,320 --> 00:04:54,160
and which we call the lost test and the

146
00:04:54,160 --> 00:04:55,759
way this works is

147
00:04:55,759 --> 00:04:57,520
the adversary starts with these two

148
00:04:57,520 --> 00:04:59,680
distributions which is which it anyway

149
00:04:59,680 --> 00:05:02,639
has public access to and it samples some

150
00:05:02,639 --> 00:05:05,440
data from both of these distributions

151
00:05:05,440 --> 00:05:07,759
and then let's say it has a given vector

152
00:05:07,759 --> 00:05:10,639
model

153
00:05:10,639 --> 00:05:12,960
it will perform inference on the first

154
00:05:12,960 --> 00:05:14,720
set of data and take note of the

155
00:05:14,720 --> 00:05:17,600
accuracy it observes

156
00:05:17,600 --> 00:05:19,039
and then repeat the same thing for the

157
00:05:19,039 --> 00:05:21,919
second set of data

158
00:05:22,000 --> 00:05:23,520
and then follow a very straightforward

159
00:05:23,520 --> 00:05:25,919
rule that essentially says that if the

160
00:05:25,919 --> 00:05:28,000
accuracy observed for this model is

161
00:05:28,000 --> 00:05:29,199
higher for

162
00:05:29,199 --> 00:05:30,479
let's say data from the second

163
00:05:30,479 --> 00:05:32,000
distribution then it's more likely that

164
00:05:32,000 --> 00:05:33,280
the model was from the second

165
00:05:33,280 --> 00:05:34,639
distribution

166
00:05:34,639 --> 00:05:37,199
which is expected because after the two

167
00:05:37,199 --> 00:05:38,960
possible distributions the one that it

168
00:05:38,960 --> 00:05:41,440
was actually trained on data from it's

169
00:05:41,440 --> 00:05:43,199
more likely to be better performing on

170
00:05:43,199 --> 00:05:45,440
that and we do have another variant of

171
00:05:45,440 --> 00:05:47,120
the black box test which we don't the

172
00:05:47,120 --> 00:05:49,120
threshold test that involves another

173
00:05:49,120 --> 00:05:50,960
step for training a threshold and that

174
00:05:50,960 --> 00:05:52,560
performs slightly better than the last

175
00:05:52,560 --> 00:05:53,919
test

176
00:05:53,919 --> 00:05:56,160
here's a look at some of the results so

177
00:05:56,160 --> 00:05:58,160
here we talk about experiments for the

178
00:05:58,160 --> 00:06:00,319
census and boneage datasets the first

179
00:06:00,319 --> 00:06:02,560
one is essentially looking at different

180
00:06:02,560 --> 00:06:04,560
attributes for people and trying to

181
00:06:04,560 --> 00:06:06,160
predict the income group and the second

182
00:06:06,160 --> 00:06:08,160
one looks at hand x-ray images and

183
00:06:08,160 --> 00:06:11,600
predicts the age group of a patient

184
00:06:11,600 --> 00:06:13,280
and in these experimental settings we

185
00:06:13,280 --> 00:06:15,120
said the first distribution

186
00:06:15,120 --> 00:06:17,120
such that the probability of female in

187
00:06:17,120 --> 00:06:19,520
these experiments is 0.5 so it's equally

188
00:06:19,520 --> 00:06:21,600
likely to be a female or not and then

189
00:06:21,600 --> 00:06:23,520
for the second distribution

190
00:06:23,520 --> 00:06:25,440
the same probability said to alpha and

191
00:06:25,440 --> 00:06:27,280
the alpha value is what we vary on the

192
00:06:27,280 --> 00:06:28,800
x-axis here

193
00:06:28,800 --> 00:06:30,479
and what we see is

194
00:06:30,479 --> 00:06:32,560
the metaclassifier which is the whitebox

195
00:06:32,560 --> 00:06:34,319
attack performs better than the black

196
00:06:34,319 --> 00:06:36,319
box attacks understandably and the

197
00:06:36,319 --> 00:06:38,000
second and more interesting point here

198
00:06:38,000 --> 00:06:40,319
is that the influences seems to be

199
00:06:40,319 --> 00:06:42,160
different for different data sets and

200
00:06:42,160 --> 00:06:43,680
there's quite some variation as we'll

201
00:06:43,680 --> 00:06:46,720
see in the future slides

202
00:06:47,600 --> 00:06:50,319
and that is what we propose the end leak

203
00:06:50,319 --> 00:06:51,759
term for and this is what you saw at the

204
00:06:51,759 --> 00:06:53,840
very beginning of the presentation so

205
00:06:53,840 --> 00:06:57,199
what this essentially means is

206
00:06:57,199 --> 00:06:59,280
if you look at a particular attack and

207
00:06:59,280 --> 00:07:01,520
the observed attack effectiveness

208
00:07:01,520 --> 00:07:03,919
it's comparable to what the adversary

209
00:07:03,919 --> 00:07:06,319
could learn if it could directly sample

210
00:07:06,319 --> 00:07:08,319
n leaked samples from the underlying

211
00:07:08,319 --> 00:07:09,599
distributions

212
00:07:09,599 --> 00:07:11,759
so let's say an n leak value of 2 means

213
00:07:11,759 --> 00:07:14,880
that the adversary had the capability to

214
00:07:14,880 --> 00:07:17,120
sample two records from the underlying

215
00:07:17,120 --> 00:07:18,720
distribution then

216
00:07:18,720 --> 00:07:20,639
the best performance that it could get

217
00:07:20,639 --> 00:07:22,240
is what you are seeing with the current

218
00:07:22,240 --> 00:07:24,560
attack so naturally a higher value of n

219
00:07:24,560 --> 00:07:26,720
leak for the same setting means that the

220
00:07:26,720 --> 00:07:28,639
attack in that particular setting is

221
00:07:28,639 --> 00:07:32,000
much leaking much more information

222
00:07:32,000 --> 00:07:34,080
and what we see that even though for

223
00:07:34,080 --> 00:07:35,840
most experiments this number is less

224
00:07:35,840 --> 00:07:37,599
than two technically anything more than

225
00:07:37,599 --> 00:07:39,360
zero is privacy leakage when the

226
00:07:39,360 --> 00:07:42,000
property is not intended to be leaked

227
00:07:42,000 --> 00:07:44,560
and this metric is useful for comparison

228
00:07:44,560 --> 00:07:46,400
across different data sets properties

229
00:07:46,400 --> 00:07:49,679
and experimental settings

230
00:07:49,919 --> 00:07:52,240
as a concrete example consider this heat

231
00:07:52,240 --> 00:07:53,840
map where we

232
00:07:53,840 --> 00:07:55,440
instead of grading just one of the

233
00:07:55,440 --> 00:07:57,360
distributions for the alpha values that

234
00:07:57,360 --> 00:07:59,360
you saw a couple slides ago we vary both

235
00:07:59,360 --> 00:08:01,840
of them one at the time and we plot

236
00:08:01,840 --> 00:08:04,400
those on the x and y axis

237
00:08:04,400 --> 00:08:05,840
on the top right triangle you see the

238
00:08:05,840 --> 00:08:08,000
distinguishing accuracy and what we see

239
00:08:08,000 --> 00:08:11,199
is as the distributions become more

240
00:08:11,199 --> 00:08:12,720
divergent in the sense that the

241
00:08:12,720 --> 00:08:14,400
difference in their alpha values

242
00:08:14,400 --> 00:08:16,720
increases the distinguishing accuracy

243
00:08:16,720 --> 00:08:18,879
also increases which is expected

244
00:08:18,879 --> 00:08:21,120
while in reality the value of n leaked

245
00:08:21,120 --> 00:08:22,960
shows us that this is not the case for

246
00:08:22,960 --> 00:08:24,240
most cases

247
00:08:24,240 --> 00:08:26,560
in fact the extremes are what leak more

248
00:08:26,560 --> 00:08:28,960
information and everything else has n

249
00:08:28,960 --> 00:08:30,800
leaked values of much less than one or

250
00:08:30,800 --> 00:08:33,039
two

251
00:08:35,279 --> 00:08:37,200
now all of the settings that we looked

252
00:08:37,200 --> 00:08:39,039
at in the experiment so far consider

253
00:08:39,039 --> 00:08:41,519
these different possible alpha values

254
00:08:41,519 --> 00:08:43,360
and then we pick any two of them and

255
00:08:43,360 --> 00:08:45,279
then we try to perform this binary kind

256
00:08:45,279 --> 00:08:47,839
of distinguishing task

257
00:08:47,839 --> 00:08:49,519
which is useful to try and understand

258
00:08:49,519 --> 00:08:51,360
what the distribution inference risk is

259
00:08:51,360 --> 00:08:55,120
but it's not very practical

260
00:08:55,120 --> 00:08:56,080
the more

261
00:08:56,080 --> 00:08:57,680
practically useful setting would be one

262
00:08:57,680 --> 00:08:59,200
where the adversary could directly look

263
00:08:59,200 --> 00:09:01,040
at the models and predict what the alpha

264
00:09:01,040 --> 00:09:03,440
value would be

265
00:09:03,440 --> 00:09:05,279
and we tried the setting for our data

266
00:09:05,279 --> 00:09:07,440
sets and what we see here is that the

267
00:09:07,440 --> 00:09:10,080
inference risk for direct regression is

268
00:09:10,080 --> 00:09:12,480
much higher than classification so for

269
00:09:12,480 --> 00:09:15,040
context the dashed gray line here is

270
00:09:15,040 --> 00:09:16,640
what you would get for perfectly

271
00:09:16,640 --> 00:09:19,279
predicting the x-axis is the actual

272
00:09:19,279 --> 00:09:21,120
alpha values for the models that we test

273
00:09:21,120 --> 00:09:23,760
on and the y-axis is what the attacks

274
00:09:23,760 --> 00:09:25,360
predict

275
00:09:25,360 --> 00:09:27,200
and what we see is that not only are the

276
00:09:27,200 --> 00:09:29,600
metaclassifiers very successful

277
00:09:29,600 --> 00:09:32,000
they don't over fit to the models that

278
00:09:32,000 --> 00:09:34,080
they see so it's not the case that they

279
00:09:34,080 --> 00:09:35,680
only perform well on the ratios that

280
00:09:35,680 --> 00:09:37,440
they've seen in training they also seem

281
00:09:37,440 --> 00:09:39,519
to generalize to ratios that they

282
00:09:39,519 --> 00:09:43,040
haven't seen in the training at all

283
00:09:43,040 --> 00:09:44,800
and this is further evident when we look

284
00:09:44,800 --> 00:09:46,080
at the n league values for the

285
00:09:46,080 --> 00:09:48,640
regression case so compared to the

286
00:09:48,640 --> 00:09:50,720
classification case where most values

287
00:09:50,720 --> 00:09:52,880
were less than two the values here can

288
00:09:52,880 --> 00:09:55,760
go as high as 270 in specific for the

289
00:09:55,760 --> 00:09:59,600
bone age data set which is quite high

290
00:09:59,600 --> 00:10:01,440
there's still a lot of open questions in

291
00:10:01,440 --> 00:10:03,440
the field of distribution inference

292
00:10:03,440 --> 00:10:05,120
first of all there's no known good

293
00:10:05,120 --> 00:10:06,880
defenses right now and even though there

294
00:10:06,880 --> 00:10:08,320
are good candidates like differential

295
00:10:08,320 --> 00:10:10,320
privacy and adversarial training which

296
00:10:10,320 --> 00:10:11,920
might work out even though there's no

297
00:10:11,920 --> 00:10:14,240
theoretical reason for them to our

298
00:10:14,240 --> 00:10:16,800
empirical experiments show that there's

299
00:10:16,800 --> 00:10:18,320
no hope for them and they actually don't

300
00:10:18,320 --> 00:10:21,279
seem to work in any of the cases

301
00:10:21,279 --> 00:10:23,200
next is trying to understand if there's

302
00:10:23,200 --> 00:10:25,040
any trade-offs with distribution

303
00:10:25,040 --> 00:10:26,720
influence and other kinds of inference

304
00:10:26,720 --> 00:10:29,279
or even things like model generalization

305
00:10:29,279 --> 00:10:30,720
and our initial experiments seem to

306
00:10:30,720 --> 00:10:32,640
suggest that protecting against

307
00:10:32,640 --> 00:10:34,560
distribution level inference attacks and

308
00:10:34,560 --> 00:10:36,399
membership level inference attacks might

309
00:10:36,399 --> 00:10:37,920
be at odds

310
00:10:37,920 --> 00:10:39,440
and then further trying to understand

311
00:10:39,440 --> 00:10:42,000
how differences in training processes

312
00:10:42,000 --> 00:10:43,839
for these models can impact influence

313
00:10:43,839 --> 00:10:46,839
risk

314
00:10:46,959 --> 00:10:48,959
in summary machine learning models seem

315
00:10:48,959 --> 00:10:50,079
to leak

316
00:10:50,079 --> 00:10:51,839
unintended information about the

317
00:10:51,839 --> 00:10:53,760
training distributions not just the

318
00:10:53,760 --> 00:10:55,440
training data sets as we have known so

319
00:10:55,440 --> 00:10:57,440
far and the factors that affect this

320
00:10:57,440 --> 00:10:59,760
leakage are not very well understood

321
00:10:59,760 --> 00:11:00,959
if you'd like to know more about our

322
00:11:00,959 --> 00:11:02,640
work you can scan the qr code here and

323
00:11:02,640 --> 00:11:04,480
here are the links to our paper and code

324
00:11:04,480 --> 00:11:07,600
base thank you

