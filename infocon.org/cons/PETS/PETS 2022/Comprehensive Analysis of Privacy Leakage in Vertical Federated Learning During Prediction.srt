1
00:00:00,399 --> 00:00:02,639
hello everyone i'm xi jiang from the

2
00:00:02,639 --> 00:00:04,880
technical university of munich

3
00:00:04,880 --> 00:00:06,799
today i'm going to present our work on

4
00:00:06,799 --> 00:00:08,960
comprehensive analysis of privacy

5
00:00:08,960 --> 00:00:10,960
leakage in vertical fetid learning

6
00:00:10,960 --> 00:00:12,559
during prediction

7
00:00:12,559 --> 00:00:14,639
this is joint work with shaving joe from

8
00:00:14,639 --> 00:00:16,880
huawei munich research center and yes

9
00:00:16,880 --> 00:00:18,560
growth clocks from the technical

10
00:00:18,560 --> 00:00:21,439
university of munich

11
00:00:21,439 --> 00:00:23,199
federalist learning is an immersion

12
00:00:23,199 --> 00:00:25,519
distributed learning mechanism which

13
00:00:25,519 --> 00:00:27,920
allows multiple organizations to jointly

14
00:00:27,920 --> 00:00:30,080
train am models without revealing the

15
00:00:30,080 --> 00:00:31,679
private data

16
00:00:31,679 --> 00:00:33,760
according to the partition strategies of

17
00:00:33,760 --> 00:00:36,399
local data existing effort frameworks

18
00:00:36,399 --> 00:00:38,160
can be further categorized into the

19
00:00:38,160 --> 00:00:41,040
horizontal setting and vertical setting

20
00:00:41,040 --> 00:00:43,280
in the horizontal setting each party

21
00:00:43,280 --> 00:00:45,600
holds data from different users while

22
00:00:45,600 --> 00:00:47,760
these data share an overlapped set of

23
00:00:47,760 --> 00:00:48,960
features

24
00:00:48,960 --> 00:00:51,840
for instance in a healthcare application

25
00:00:51,840 --> 00:00:54,160
a model for predicting cancer risks is

26
00:00:54,160 --> 00:00:56,399
jointly trained by multiple hospitals

27
00:00:56,399 --> 00:00:58,399
using the medical records of different

28
00:00:58,399 --> 00:00:59,920
patients

29
00:00:59,920 --> 00:01:02,399
in contrast the vertical setting assumes

30
00:01:02,399 --> 00:01:04,559
that each party holds different features

31
00:01:04,559 --> 00:01:07,200
of an intersecting set of users

32
00:01:07,200 --> 00:01:09,520
moreover only one of the parties has the

33
00:01:09,520 --> 00:01:12,640
real labels for instance in a smartphone

34
00:01:12,640 --> 00:01:14,960
scenario a bank wants to train a model

35
00:01:14,960 --> 00:01:17,600
to evaluate customers credit scores

36
00:01:17,600 --> 00:01:20,000
besides using song features such as age

37
00:01:20,000 --> 00:01:22,320
and income it also collaborates with an

38
00:01:22,320 --> 00:01:24,640
e-commerce company and uses additional

39
00:01:24,640 --> 00:01:27,119
features such as purchasing history to

40
00:01:27,119 --> 00:01:29,360
improve the model's accuracy

41
00:01:29,360 --> 00:01:31,119
since it is common for companies to

42
00:01:31,119 --> 00:01:32,960
collect different user features for

43
00:01:32,960 --> 00:01:34,320
their services

44
00:01:34,320 --> 00:01:36,560
vfl systems attracted increasing

45
00:01:36,560 --> 00:01:39,200
attention in recent years

46
00:01:39,200 --> 00:01:41,759
here we present a common two-party vfr

47
00:01:41,759 --> 00:01:42,640
system

48
00:01:42,640 --> 00:01:45,040
which includes an elective party who has

49
00:01:45,040 --> 00:01:47,680
part of the features and real labels and

50
00:01:47,680 --> 00:01:49,759
a passive party who only provides

51
00:01:49,759 --> 00:01:51,520
additional features

52
00:01:51,520 --> 00:01:54,560
moreover several existing vfr systems

53
00:01:54,560 --> 00:01:56,799
also adopt a trusted coordinator to

54
00:01:56,799 --> 00:01:58,560
manage the communication between the

55
00:01:58,560 --> 00:02:00,799
local parties and to defend against the

56
00:02:00,799 --> 00:02:02,640
privacy leakage

57
00:02:02,640 --> 00:02:05,119
during prediction each party inputs the

58
00:02:05,119 --> 00:02:07,119
features of new samples to the trained

59
00:02:07,119 --> 00:02:09,280
models and computes the intermediate

60
00:02:09,280 --> 00:02:10,479
results

61
00:02:10,479 --> 00:02:12,160
the coordinator then privately

62
00:02:12,160 --> 00:02:14,560
aggregates these results computes the

63
00:02:14,560 --> 00:02:16,160
confidence scores by an output

64
00:02:16,160 --> 00:02:18,480
activation function and returns them to

65
00:02:18,480 --> 00:02:20,319
the active party

66
00:02:20,319 --> 00:02:22,080
although the trusted coordinator

67
00:02:22,080 --> 00:02:24,160
effectively prevents the direct data

68
00:02:24,160 --> 00:02:26,239
sharing between local parties

69
00:02:26,239 --> 00:02:28,640
a recent work by law at all shows that

70
00:02:28,640 --> 00:02:31,040
during prediction the active party can

71
00:02:31,040 --> 00:02:32,879
still use the confidence scores to

72
00:02:32,879 --> 00:02:35,680
reconstruct pacific features and cause

73
00:02:35,680 --> 00:02:37,840
privacy leakage

74
00:02:37,840 --> 00:02:40,319
in their paper the authors started with

75
00:02:40,319 --> 00:02:42,480
a simple setting where two parties

76
00:02:42,480 --> 00:02:44,480
jointly trained a logistic regression

77
00:02:44,480 --> 00:02:46,480
model with k classes

78
00:02:46,480 --> 00:02:48,879
they proposed an equation solving attack

79
00:02:48,879 --> 00:02:50,640
which mathematically calculates the

80
00:02:50,640 --> 00:02:52,800
passive party features by solving the

81
00:02:52,800 --> 00:02:55,280
following linear equation system

82
00:02:55,280 --> 00:02:57,040
they show that when the number of

83
00:02:57,040 --> 00:02:59,280
unknown features d pass is less than the

84
00:02:59,280 --> 00:03:02,159
number of classes k the equation system

85
00:03:02,159 --> 00:03:04,239
has a unique solution and the feature

86
00:03:04,239 --> 00:03:07,040
values can be fully reconstructed

87
00:03:07,040 --> 00:03:09,920
however the esc attack only works for

88
00:03:09,920 --> 00:03:12,159
linear models and cannot be applied to

89
00:03:12,159 --> 00:03:14,159
complex neural networks

90
00:03:14,159 --> 00:03:16,480
therefore the authors further proposed a

91
00:03:16,480 --> 00:03:19,120
generic regression network attack

92
00:03:19,120 --> 00:03:21,360
which uses an attack model to learn the

93
00:03:21,360 --> 00:03:23,920
future correlations between both parties

94
00:03:23,920 --> 00:03:25,760
and to generate the passive party

95
00:03:25,760 --> 00:03:27,120
features

96
00:03:27,120 --> 00:03:29,040
nevertheless the attack requests

97
00:03:29,040 --> 00:03:31,280
collecting a large number of predictions

98
00:03:31,280 --> 00:03:33,280
to train the attack model

99
00:03:33,280 --> 00:03:36,080
finally both attacks only considered the

100
00:03:36,080 --> 00:03:38,480
white box setting which assumes the

101
00:03:38,480 --> 00:03:40,480
attacker can access the passive party

102
00:03:40,480 --> 00:03:41,440
model

103
00:03:41,440 --> 00:03:43,440
this may not always be achievable in

104
00:03:43,440 --> 00:03:45,440
real life scenarios

105
00:03:45,440 --> 00:03:47,519
motivated by these observations and

106
00:03:47,519 --> 00:03:50,159
limitations we propose and illustrate

107
00:03:50,159 --> 00:03:52,319
the threat posed by a gradient-based

108
00:03:52,319 --> 00:03:53,840
inversion attack

109
00:03:53,840 --> 00:03:56,159
the attack can be generically applied to

110
00:03:56,159 --> 00:03:58,159
different machine learning models and

111
00:03:58,159 --> 00:04:00,400
does not need to train an attack model e

112
00:04:00,400 --> 00:04:01,599
at once

113
00:04:01,599 --> 00:04:04,400
moreover besides the white box setting

114
00:04:04,400 --> 00:04:06,640
we give the first attempt to explore the

115
00:04:06,640 --> 00:04:09,360
attack under the black box setting

116
00:04:09,360 --> 00:04:11,599
next we will introduce this attack in

117
00:04:11,599 --> 00:04:13,760
detail

118
00:04:13,760 --> 00:04:16,079
start with the white box attack

119
00:04:16,079 --> 00:04:18,478
similar to the previous work we assume

120
00:04:18,478 --> 00:04:21,040
the attacker namely the active fighter

121
00:04:21,040 --> 00:04:22,880
has full knowledge of the entire model

122
00:04:22,880 --> 00:04:24,320
parameters

123
00:04:24,320 --> 00:04:27,040
also for each new sample he knows its

124
00:04:27,040 --> 00:04:29,040
own input features as well as the real

125
00:04:29,040 --> 00:04:30,720
confidence scores

126
00:04:30,720 --> 00:04:32,800
the goal of the attacker is to use this

127
00:04:32,800 --> 00:04:34,880
information to estimate the passive

128
00:04:34,880 --> 00:04:36,560
party features

129
00:04:36,560 --> 00:04:38,800
our attack is conducted in an iterative

130
00:04:38,800 --> 00:04:39,759
fashion

131
00:04:39,759 --> 00:04:42,160
to begin with the attacker randomly

132
00:04:42,160 --> 00:04:44,960
initializes the passive party features

133
00:04:44,960 --> 00:04:47,280
then both part of the features are fed

134
00:04:47,280 --> 00:04:49,199
to train the model to compute the

135
00:04:49,199 --> 00:04:51,440
estimated confidence scores

136
00:04:51,440 --> 00:04:53,120
the distance between the real and

137
00:04:53,120 --> 00:04:54,880
estimated confidence scores is

138
00:04:54,880 --> 00:04:57,199
calculated and used to update the

139
00:04:57,199 --> 00:04:59,280
passive party features

140
00:04:59,280 --> 00:05:01,280
this process is repeated for several

141
00:05:01,280 --> 00:05:03,520
rounds until the estimated confidence

142
00:05:03,520 --> 00:05:05,199
scores are close enough to the real

143
00:05:05,199 --> 00:05:06,479
values

144
00:05:06,479 --> 00:05:09,120
here we use two metrics namely the mean

145
00:05:09,120 --> 00:05:11,280
squared error and the k outer versions

146
00:05:11,280 --> 00:05:13,680
to measure the distance

147
00:05:13,680 --> 00:05:16,080
next we explore the potential privacy

148
00:05:16,080 --> 00:05:18,639
leakage under the black box setting

149
00:05:18,639 --> 00:05:20,960
here the attacker only knows his model

150
00:05:20,960 --> 00:05:22,800
and cannot access the passive party

151
00:05:22,800 --> 00:05:23,759
model

152
00:05:23,759 --> 00:05:25,919
we further slightly relax the setting by

153
00:05:25,919 --> 00:05:28,479
assuming that the attacker is aware of a

154
00:05:28,479 --> 00:05:30,720
small set of auxiliary data

155
00:05:30,720 --> 00:05:32,800
this can be obtained by for instance

156
00:05:32,800 --> 00:05:35,360
colluding with a few users whose data

157
00:05:35,360 --> 00:05:37,440
achieved in both parties

158
00:05:37,440 --> 00:05:39,199
the black box attack can then be

159
00:05:39,199 --> 00:05:41,199
conducted in two steps

160
00:05:41,199 --> 00:05:43,520
first the attacker uses the auxiliary

161
00:05:43,520 --> 00:05:45,840
detail to train a shadow model to mimic

162
00:05:45,840 --> 00:05:47,600
the performance of the passive party

163
00:05:47,600 --> 00:05:48,639
model

164
00:05:48,639 --> 00:05:50,880
then he can use the shadow model to

165
00:05:50,880 --> 00:05:52,800
repeat the white box attack and to

166
00:05:52,800 --> 00:05:55,440
recover the passive party features

167
00:05:55,440 --> 00:05:57,360
for training the shadow model the

168
00:05:57,360 --> 00:05:59,520
attacker first initializes a random

169
00:05:59,520 --> 00:06:01,280
passive party model

170
00:06:01,280 --> 00:06:03,759
then the auxiliary data are iteratively

171
00:06:03,759 --> 00:06:06,080
input into the model and the distance

172
00:06:06,080 --> 00:06:07,600
between the real and estimated

173
00:06:07,600 --> 00:06:10,160
confidence scores will be used to update

174
00:06:10,160 --> 00:06:12,400
the model parameters

175
00:06:12,400 --> 00:06:14,560
finally we wonder how much auxiliary

176
00:06:14,560 --> 00:06:16,880
data is needed to build a good shadow

177
00:06:16,880 --> 00:06:17,840
model

178
00:06:17,840 --> 00:06:19,919
given the number of model parameters and

179
00:06:19,919 --> 00:06:22,400
classes we derive an approximated

180
00:06:22,400 --> 00:06:24,720
threshold top as the minimum auxiliary

181
00:06:24,720 --> 00:06:26,639
data for achieving similar attack

182
00:06:26,639 --> 00:06:29,680
performance as in the white box setting

183
00:06:29,680 --> 00:06:31,520
note that the attack can still be

184
00:06:31,520 --> 00:06:34,479
performed with less auxiliary data but

185
00:06:34,479 --> 00:06:36,400
this may lead to a worse attack

186
00:06:36,400 --> 00:06:38,720
performance

187
00:06:38,720 --> 00:06:40,800
we conduct our attack on four open

188
00:06:40,800 --> 00:06:42,960
source data sets and compare the attack

189
00:06:42,960 --> 00:06:45,039
results against the logistic regression

190
00:06:45,039 --> 00:06:47,840
models and multi-layer neural networks

191
00:06:47,840 --> 00:06:50,240
we respectively use the attacks proposed

192
00:06:50,240 --> 00:06:52,720
by royaltal as the baselines

193
00:06:52,720 --> 00:06:55,120
we adopted two metrics to evaluate the

194
00:06:55,120 --> 00:06:56,639
attack performance

195
00:06:56,639 --> 00:06:58,880
for the attack arrow we use the average

196
00:06:58,880 --> 00:07:00,800
the misgrade error to measure the

197
00:07:00,800 --> 00:07:02,240
difference between the real and

198
00:07:02,240 --> 00:07:04,000
reconstructed features

199
00:07:04,000 --> 00:07:06,080
for the attack accuracy we treat an

200
00:07:06,080 --> 00:07:08,479
evaluation classifier and investigate

201
00:07:08,479 --> 00:07:10,479
whether the reconstructed features can

202
00:07:10,479 --> 00:07:12,560
provide the same prediction results as

203
00:07:12,560 --> 00:07:14,319
real features

204
00:07:14,319 --> 00:07:16,720
we first report the attack performance

205
00:07:16,720 --> 00:07:18,800
under the wet box setting against both

206
00:07:18,800 --> 00:07:21,520
types of models it can be seen that we

207
00:07:21,520 --> 00:07:23,520
can achieve a zero attack error when the

208
00:07:23,520 --> 00:07:25,440
number of passive priority features is

209
00:07:25,440 --> 00:07:27,759
small meaning that the feature values

210
00:07:27,759 --> 00:07:29,360
are fully recovered

211
00:07:29,360 --> 00:07:30,800
although the attack performance

212
00:07:30,800 --> 00:07:32,720
decreased with the increase of unknown

213
00:07:32,720 --> 00:07:35,440
features our attack shows a significant

214
00:07:35,440 --> 00:07:37,440
improvement compared to the baseline

215
00:07:37,440 --> 00:07:39,440
algorithms

216
00:07:39,440 --> 00:07:41,919
we further conduct black box attacks

217
00:07:41,919 --> 00:07:44,000
against the both types of models using

218
00:07:44,000 --> 00:07:46,479
different amount of auxiliary data

219
00:07:46,479 --> 00:07:48,479
it can be seen that the increase of

220
00:07:48,479 --> 00:07:50,800
auxiliary data can in general contribute

221
00:07:50,800 --> 00:07:53,360
to a lower attack error and the attack

222
00:07:53,360 --> 00:07:55,280
error is similar to the one under the

223
00:07:55,280 --> 00:07:57,440
white box setting when the amount of

224
00:07:57,440 --> 00:08:00,400
obstacle data approaches the threshold

225
00:08:00,400 --> 00:08:02,879
here for both models we can already

226
00:08:02,879 --> 00:08:05,280
achieve comparable performance with only

227
00:08:05,280 --> 00:08:07,440
less than 100 required

228
00:08:07,440 --> 00:08:09,360
the results show that even without

229
00:08:09,360 --> 00:08:11,520
knowing the passive party models the

230
00:08:11,520 --> 00:08:13,680
attacker can still use limited auxiliary

231
00:08:13,680 --> 00:08:16,000
data to build the shadow model and to

232
00:08:16,000 --> 00:08:18,639
apply the reconstruction attack

233
00:08:18,639 --> 00:08:20,800
we further investigate whether the prior

234
00:08:20,800 --> 00:08:22,639
knowledge of the model structure is

235
00:08:22,639 --> 00:08:25,039
essential for building the shadow models

236
00:08:25,039 --> 00:08:27,280
to this end we use the two layer neural

237
00:08:27,280 --> 00:08:29,039
network as the real passive parting

238
00:08:29,039 --> 00:08:31,199
model and compare the attack error of

239
00:08:31,199 --> 00:08:32,958
using shadow models with different

240
00:08:32,958 --> 00:08:34,159
structures

241
00:08:34,159 --> 00:08:36,000
it can be seen that building the shadow

242
00:08:36,000 --> 00:08:38,000
model with the original structure leads

243
00:08:38,000 --> 00:08:40,320
to the lowest attack error although

244
00:08:40,320 --> 00:08:41,839
altering the model structure may

245
00:08:41,839 --> 00:08:44,000
increase the attack error we can still

246
00:08:44,000 --> 00:08:45,760
achieve a satisfactory attack

247
00:08:45,760 --> 00:08:47,839
performance when the shadow model does

248
00:08:47,839 --> 00:08:50,560
not differ too much from the real model

249
00:08:50,560 --> 00:08:52,959
the result indicates that it is still

250
00:08:52,959 --> 00:08:55,200
possible to conduct the attack even

251
00:08:55,200 --> 00:08:57,360
without knowing the exact structure of

252
00:08:57,360 --> 00:09:00,080
the passive parting model

253
00:09:00,080 --> 00:09:02,480
finally we analyze the performance of

254
00:09:02,480 --> 00:09:04,560
different defense techniques which are

255
00:09:04,560 --> 00:09:06,560
either applied during training or during

256
00:09:06,560 --> 00:09:08,640
prediction for the defenses during

257
00:09:08,640 --> 00:09:10,720
training we investigated where the

258
00:09:10,720 --> 00:09:13,200
private sector section the commonly used

259
00:09:13,200 --> 00:09:15,279
sample alignment technique can help

260
00:09:15,279 --> 00:09:17,200
prevent privacy leakage

261
00:09:17,200 --> 00:09:19,519
moreover we also analyze the capability

262
00:09:19,519 --> 00:09:21,279
of differentially private training

263
00:09:21,279 --> 00:09:22,959
against the attack

264
00:09:22,959 --> 00:09:25,600
here epsilon is the privacy budget the

265
00:09:25,600 --> 00:09:27,600
smaller epsilon the higher the privacy

266
00:09:27,600 --> 00:09:28,800
protection

267
00:09:28,800 --> 00:09:30,880
for the defenses during prediction we

268
00:09:30,880 --> 00:09:33,040
process the original confidence scores

269
00:09:33,040 --> 00:09:34,800
by rounding the values to limited

270
00:09:34,800 --> 00:09:37,200
decimal places or adding random noise

271
00:09:37,200 --> 00:09:39,120
with different noise skills

272
00:09:39,120 --> 00:09:41,200
we also investigate a purification

273
00:09:41,200 --> 00:09:43,279
defense which uses a pre-trained

274
00:09:43,279 --> 00:09:45,440
purifier to perturb the real confidence

275
00:09:45,440 --> 00:09:46,480
scores

276
00:09:46,480 --> 00:09:48,640
the precessor scores will be decoupled

277
00:09:48,640 --> 00:09:50,880
with the input features but can still

278
00:09:50,880 --> 00:09:53,680
provide a good labor prediction accuracy

279
00:09:53,680 --> 00:09:56,000
here we compare the performance of pure

280
00:09:56,000 --> 00:09:58,560
buyers respectively trained on real data

281
00:09:58,560 --> 00:10:01,920
or randomly sampled data

282
00:10:02,240 --> 00:10:04,240
the two figures report the privacy

283
00:10:04,240 --> 00:10:06,240
utility analysis of different defense

284
00:10:06,240 --> 00:10:08,880
techniques under the white box setting

285
00:10:08,880 --> 00:10:10,959
the red star represents the area of the

286
00:10:10,959 --> 00:10:13,600
ideal privacy utility tradeoff

287
00:10:13,600 --> 00:10:16,079
from the results it can be seen that psa

288
00:10:16,079 --> 00:10:18,959
cannot prevent reconstruction attacks

289
00:10:18,959 --> 00:10:21,040
while training models with dp can reduce

290
00:10:21,040 --> 00:10:22,880
the attack accuracy when eastern is

291
00:10:22,880 --> 00:10:25,120
small it also results in much lower

292
00:10:25,120 --> 00:10:26,880
motor utility

293
00:10:26,880 --> 00:10:28,720
on the other hand processing the

294
00:10:28,720 --> 00:10:30,560
confidence scores can better enhance

295
00:10:30,560 --> 00:10:33,680
privacy protection for instance rounding

296
00:10:33,680 --> 00:10:35,519
the confidence scores to integers can

297
00:10:35,519 --> 00:10:37,760
effectively prevent the attack without

298
00:10:37,760 --> 00:10:39,920
affecting the model accuracy

299
00:10:39,920 --> 00:10:41,839
adding noise with a noise skill larger

300
00:10:41,839 --> 00:10:44,320
than 0.1 can also defend against the

301
00:10:44,320 --> 00:10:46,640
attack but too much noise may cause a

302
00:10:46,640 --> 00:10:49,200
significant utility loss

303
00:10:49,200 --> 00:10:51,440
finally perturbing the confidence scores

304
00:10:51,440 --> 00:10:53,600
with purifiers trained with both real

305
00:10:53,600 --> 00:10:56,079
data and randomly sample data can in

306
00:10:56,079 --> 00:10:58,480
general achieve a satisfactory privacy

307
00:10:58,480 --> 00:11:01,279
utility balance

308
00:11:01,279 --> 00:11:03,839
in conclusion in this paper we propose

309
00:11:03,839 --> 00:11:05,760
and discuss the threat posed by the

310
00:11:05,760 --> 00:11:07,920
gradient-based inversion attack a

311
00:11:07,920 --> 00:11:10,079
generic framework that performs special

312
00:11:10,079 --> 00:11:12,399
reconstruction attacks on bfo systems

313
00:11:12,399 --> 00:11:14,720
during prediction the attack can be

314
00:11:14,720 --> 00:11:16,560
flexibly applied to different machine

315
00:11:16,560 --> 00:11:18,399
learning models and outperforms the

316
00:11:18,399 --> 00:11:19,519
prior work

317
00:11:19,519 --> 00:11:21,519
we further explore the attack under the

318
00:11:21,519 --> 00:11:23,600
black box setting and show that

319
00:11:23,600 --> 00:11:25,680
attackers with limited prior knowledge

320
00:11:25,680 --> 00:11:27,279
can still infer the passive party

321
00:11:27,279 --> 00:11:29,519
features even without knowing the target

322
00:11:29,519 --> 00:11:32,800
model's railways or the model structures

323
00:11:32,800 --> 00:11:35,200
finally we analyze the privacy utility

324
00:11:35,200 --> 00:11:37,680
trade-off of several defense techniques

325
00:11:37,680 --> 00:11:39,839
we show that the proposed attack cannot

326
00:11:39,839 --> 00:11:42,160
be simply prevented by using a trusted

327
00:11:42,160 --> 00:11:43,440
coordinator

328
00:11:43,440 --> 00:11:45,600
instead processing the confidence scores

329
00:11:45,600 --> 00:11:47,680
with quantization and randomization

330
00:11:47,680 --> 00:11:49,440
approaches can be useful for

331
00:11:49,440 --> 00:11:51,519
strengthening privacy protection while

332
00:11:51,519 --> 00:11:53,680
having only negligible effect on model

333
00:11:53,680 --> 00:11:57,839
utility thank you for listening

