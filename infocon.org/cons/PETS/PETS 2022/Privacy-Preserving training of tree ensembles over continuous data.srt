1
00:00:01,599 --> 00:00:02,399
hi

2
00:00:02,399 --> 00:00:04,960
my name is davis railsback and

3
00:00:04,960 --> 00:00:06,640
i'll be presenting to your paper privacy

4
00:00:06,640 --> 00:00:08,320
preserving training of decision trees

5
00:00:08,320 --> 00:00:09,920
over continuous data

6
00:00:09,920 --> 00:00:13,280
i'm a phd student at uwt and i am

7
00:00:13,280 --> 00:00:14,719
working with a group of students and

8
00:00:14,719 --> 00:00:19,119
faculty out of uwt and monash university

9
00:00:20,880 --> 00:00:23,600
okay so supervised machine learning is

10
00:00:23,600 --> 00:00:25,279
all about training models on historical

11
00:00:25,279 --> 00:00:26,400
data that can be used to make

12
00:00:26,400 --> 00:00:28,640
predictions on future data so for

13
00:00:28,640 --> 00:00:30,800
instance uh predicting whether a patient

14
00:00:30,800 --> 00:00:32,399
who just tested positive for covid will

15
00:00:32,399 --> 00:00:35,360
develop long-term complications

16
00:00:35,360 --> 00:00:37,440
so in this circumstance having more

17
00:00:37,440 --> 00:00:39,600
training data is usually going to make

18
00:00:39,600 --> 00:00:41,680
more more accurate models so the more

19
00:00:41,680 --> 00:00:43,120
data that can be

20
00:00:43,120 --> 00:00:46,239
ascertained the better

21
00:00:46,239 --> 00:00:47,840
however the training data especially

22
00:00:47,840 --> 00:00:49,600
with medical stuff like this is often

23
00:00:49,600 --> 00:00:51,520
kept in silos that are controlled by

24
00:00:51,520 --> 00:00:53,280
entities like hospitals

25
00:00:53,280 --> 00:00:54,879
who are not allowed to disclose the data

26
00:00:54,879 --> 00:00:56,879
to each other because of privacy laws

27
00:00:56,879 --> 00:00:58,559
like hipaa

28
00:00:58,559 --> 00:01:00,399
so a subfield of ppml which we're

29
00:01:00,399 --> 00:01:02,079
concerned with is

30
00:01:02,079 --> 00:01:03,760
all about developing

31
00:01:03,760 --> 00:01:07,119
mpc protocols to train ml models on

32
00:01:07,119 --> 00:01:09,439
this type of combined data or augmented

33
00:01:09,439 --> 00:01:11,119
data sets that are distributed between

34
00:01:11,119 --> 00:01:12,880
entities that are not able to disclose

35
00:01:12,880 --> 00:01:15,920
their data in the clear

36
00:01:17,280 --> 00:01:19,200
okay so within this setting

37
00:01:19,200 --> 00:01:20,799
we focus on decision trees because

38
00:01:20,799 --> 00:01:22,799
they're still the most widely deployed

39
00:01:22,799 --> 00:01:24,880
machine learning models in industry

40
00:01:24,880 --> 00:01:26,479
that's a quote from pedro domingos in

41
00:01:26,479 --> 00:01:27,759
2016.

42
00:01:27,759 --> 00:01:28,799
um

43
00:01:28,799 --> 00:01:31,360
so in terms of actual application and

44
00:01:31,360 --> 00:01:33,200
you know being employed

45
00:01:33,200 --> 00:01:35,040
on these uh on these data sets in

46
00:01:35,040 --> 00:01:37,200
question that are siloed uh it seems

47
00:01:37,200 --> 00:01:40,320
like a pretty reasonable thing

48
00:01:40,720 --> 00:01:42,000
in the clear

49
00:01:42,000 --> 00:01:43,439
decision trees are induced from a

50
00:01:43,439 --> 00:01:45,439
top-down algorithm that selects the

51
00:01:45,439 --> 00:01:46,880
features and puts them into internal

52
00:01:46,880 --> 00:01:47,840
nodes

53
00:01:47,840 --> 00:01:50,000
and in the case of real value data

54
00:01:50,000 --> 00:01:52,159
it also selects the query thresholds for

55
00:01:52,159 --> 00:01:53,520
those nodes

56
00:01:53,520 --> 00:01:56,000
so the the optimal candidates for

57
00:01:56,000 --> 00:01:57,840
splitting are selected based on a

58
00:01:57,840 --> 00:01:59,200
sorting algorithm

59
00:01:59,200 --> 00:02:02,719
and in mpc sorting is quite expensive so

60
00:02:02,719 --> 00:02:04,560
our work kind of comes in to figure out

61
00:02:04,560 --> 00:02:07,439
how can we solve the same problem

62
00:02:07,439 --> 00:02:09,440
potentially throwing out the c 4.5

63
00:02:09,440 --> 00:02:11,599
algorithm which is

64
00:02:11,599 --> 00:02:13,599
the one that uses the sorting technique

65
00:02:13,599 --> 00:02:16,080
and achieves similar results

66
00:02:16,080 --> 00:02:19,599
with non-sorting based methods

67
00:02:20,319 --> 00:02:22,160
so concretely what we did contribute in

68
00:02:22,160 --> 00:02:24,879
this paper is a novel subset selection

69
00:02:24,879 --> 00:02:27,760
protocol which is a method by which we

70
00:02:27,760 --> 00:02:30,319
convert the entire data set into random

71
00:02:30,319 --> 00:02:32,160
subsets that can be used to train trees

72
00:02:32,160 --> 00:02:34,400
within an ensemble

73
00:02:34,400 --> 00:02:36,319
as i referenced above we have a

74
00:02:36,319 --> 00:02:38,720
discretization protocol that we convert

75
00:02:38,720 --> 00:02:41,120
into mpc for training turning real value

76
00:02:41,120 --> 00:02:44,080
data into categorical data

77
00:02:44,080 --> 00:02:46,720
and to our knowledge this is the

78
00:02:46,720 --> 00:02:49,200
fastest implementation of decision tree

79
00:02:49,200 --> 00:02:51,280
training on real features that currently

80
00:02:51,280 --> 00:02:54,319
exists in the literature

81
00:02:55,840 --> 00:02:57,680
okay so next we'll just breeze through

82
00:02:57,680 --> 00:03:00,319
some npc preliminaries so we use

83
00:03:00,319 --> 00:03:03,280
additive uh two-party secret sharing so

84
00:03:03,280 --> 00:03:05,040
in this situation you can imagine two

85
00:03:05,040 --> 00:03:06,560
parties alice and bob each have two

86
00:03:06,560 --> 00:03:09,360
secret values and they would like to

87
00:03:09,360 --> 00:03:11,360
compute some function on the combination

88
00:03:11,360 --> 00:03:13,760
of those two values so they encrypt them

89
00:03:13,760 --> 00:03:16,080
by each generating a number at random

90
00:03:16,080 --> 00:03:17,599
and then taking the difference between

91
00:03:17,599 --> 00:03:20,480
that number and uh and their original

92
00:03:20,480 --> 00:03:22,800
secret value and then by keeping one of

93
00:03:22,800 --> 00:03:24,319
these and distributing the other one to

94
00:03:24,319 --> 00:03:26,799
the other party each of them will have

95
00:03:26,799 --> 00:03:30,080
um two numbers that appear random

96
00:03:30,080 --> 00:03:34,400
that add up to the original secret value

97
00:03:34,400 --> 00:03:36,799
so um just by virtue of modular

98
00:03:36,799 --> 00:03:38,560
arithmetic

99
00:03:38,560 --> 00:03:41,280
the the two parties are able to compute

100
00:03:41,280 --> 00:03:44,239
sums of two secrets sums of secrets and

101
00:03:44,239 --> 00:03:47,440
constants and multiplying a scalar by a

102
00:03:47,440 --> 00:03:48,879
secret

103
00:03:48,879 --> 00:03:50,159
however in order to compute

104
00:03:50,159 --> 00:03:51,680
multiplication

105
00:03:51,680 --> 00:03:54,159
of two secret values

106
00:03:54,159 --> 00:03:56,879
we use beaver triples which is an

107
00:03:56,879 --> 00:03:58,400
element of correlated randomness that

108
00:03:58,400 --> 00:04:01,040
allows an extra one-time pad of each of

109
00:04:01,040 --> 00:04:03,439
the secret values along with a factor

110
00:04:03,439 --> 00:04:04,319
that

111
00:04:04,319 --> 00:04:05,439
that is able to convert the

112
00:04:05,439 --> 00:04:08,159
multiplication into uh the other local

113
00:04:08,159 --> 00:04:11,120
operations that we're already able to do

114
00:04:11,120 --> 00:04:13,120
so i mentioned that

115
00:04:13,120 --> 00:04:14,400
we're operating in the two-party

116
00:04:14,400 --> 00:04:15,439
scenario

117
00:04:15,439 --> 00:04:17,440
so the two-party honest but curious

118
00:04:17,440 --> 00:04:18,320
setting

119
00:04:18,320 --> 00:04:21,199
uh is basically such that both of the

120
00:04:21,199 --> 00:04:22,639
two players cannot deviate from the

121
00:04:22,639 --> 00:04:24,320
protocol at all they can just sit there

122
00:04:24,320 --> 00:04:25,680
and try to learn as much information as

123
00:04:25,680 --> 00:04:26,720
they want

124
00:04:26,720 --> 00:04:27,840
and uh

125
00:04:27,840 --> 00:04:28,639
this

126
00:04:28,639 --> 00:04:30,720
is basically the weakest security

127
00:04:30,720 --> 00:04:32,639
setting possible but it does make sense

128
00:04:32,639 --> 00:04:35,040
when legal repercussions are a good

129
00:04:35,040 --> 00:04:36,800
enough deterrent against

130
00:04:36,800 --> 00:04:38,560
against deviating from the protocol for

131
00:04:38,560 --> 00:04:39,600
instance if

132
00:04:39,600 --> 00:04:41,600
protecting the data isn't the absolute

133
00:04:41,600 --> 00:04:44,160
key priority but

134
00:04:44,160 --> 00:04:45,680
the chance that somebody got caught

135
00:04:45,680 --> 00:04:47,520
would be enough to to deter them from

136
00:04:47,520 --> 00:04:50,080
that action

137
00:04:50,240 --> 00:04:52,320
so

138
00:04:52,320 --> 00:04:53,919
this is actually not a restrictive

139
00:04:53,919 --> 00:04:56,080
security setting so in the simplest case

140
00:04:56,080 --> 00:04:57,840
we can imagine that

141
00:04:57,840 --> 00:04:59,600
alice and bob are directly computing on

142
00:04:59,600 --> 00:05:00,800
their own data

143
00:05:00,800 --> 00:05:02,240
getting correlated randomness from a

144
00:05:02,240 --> 00:05:04,160
trusted initializer but just the same

145
00:05:04,160 --> 00:05:07,039
way any number of parties can also

146
00:05:07,039 --> 00:05:09,919
send secret shares of their individual

147
00:05:09,919 --> 00:05:13,759
data to both alice and bob uh who then

148
00:05:13,759 --> 00:05:14,960
just operate

149
00:05:14,960 --> 00:05:18,638
an npc protocol on their behalf

150
00:05:19,199 --> 00:05:20,880
finally in the security setting we can

151
00:05:20,880 --> 00:05:22,560
eliminate the trusted initializer

152
00:05:22,560 --> 00:05:25,759
entirely by splitting the protocol

153
00:05:25,759 --> 00:05:27,680
execution into an online and offline

154
00:05:27,680 --> 00:05:29,520
phase

155
00:05:29,520 --> 00:05:31,360
and this simulating of the trusted

156
00:05:31,360 --> 00:05:33,280
initializer just kind of costs extra

157
00:05:33,280 --> 00:05:35,440
time and resources but

158
00:05:35,440 --> 00:05:37,600
it's not

159
00:05:37,600 --> 00:05:39,280
it's not required to include a trusted

160
00:05:39,280 --> 00:05:41,680
initializer

161
00:05:41,680 --> 00:05:43,280
all right so now we're ready to discuss

162
00:05:43,280 --> 00:05:44,960
the actual protocols that we propose in

163
00:05:44,960 --> 00:05:46,880
this paper

164
00:05:46,880 --> 00:05:49,680
so the first of the two is called random

165
00:05:49,680 --> 00:05:52,080
forests and it's a pretty standard

166
00:05:52,080 --> 00:05:54,240
method for training ensembles of trees

167
00:05:54,240 --> 00:05:56,080
over categorical data

168
00:05:56,080 --> 00:05:57,039
so

169
00:05:57,039 --> 00:05:59,039
the first phase is to select a number of

170
00:05:59,039 --> 00:06:00,720
trees that we want to train

171
00:06:00,720 --> 00:06:01,759
and then

172
00:06:01,759 --> 00:06:02,960
split out

173
00:06:02,960 --> 00:06:05,759
that many random subsets of the data set

174
00:06:05,759 --> 00:06:07,440
using replacement on both the columns

175
00:06:07,440 --> 00:06:08,880
and features

176
00:06:08,880 --> 00:06:11,199
and then for each of the subsets we

177
00:06:11,199 --> 00:06:13,600
discretize the columns into

178
00:06:13,600 --> 00:06:16,080
some number of categorical variables

179
00:06:16,080 --> 00:06:18,319
and then for each of those we learn a

180
00:06:18,319 --> 00:06:20,710
tree with id3

181
00:06:20,710 --> 00:06:22,160
[Music]

182
00:06:22,160 --> 00:06:24,639
the second method does not discretize

183
00:06:24,639 --> 00:06:26,880
and instead we compute the same random

184
00:06:26,880 --> 00:06:29,680
subsets that we do for random forests

185
00:06:29,680 --> 00:06:32,400
and then instead of using id3 straight

186
00:06:32,400 --> 00:06:36,000
up on categorical variables we

187
00:06:36,000 --> 00:06:38,479
use a modified version where at each

188
00:06:38,479 --> 00:06:40,400
node when we need to determine the

189
00:06:40,400 --> 00:06:42,960
splitting threshold we instead pick a

190
00:06:42,960 --> 00:06:44,720
random value between the min and max of

191
00:06:44,720 --> 00:06:46,240
each feature and use that as the

192
00:06:46,240 --> 00:06:50,479
threshold and then the idea is that by

193
00:06:50,560 --> 00:06:52,160
by having enough of these those random

194
00:06:52,160 --> 00:06:53,599
thresholds will be close to the ideal

195
00:06:53,599 --> 00:06:56,160
one in the average case

196
00:06:56,160 --> 00:06:58,000
so um looking at these two algorithms

197
00:06:58,000 --> 00:07:00,319
then in the context of mpc

198
00:07:00,319 --> 00:07:01,680
the things that are going to take a long

199
00:07:01,680 --> 00:07:03,039
time are

200
00:07:03,039 --> 00:07:05,599
this comparison between uh or

201
00:07:05,599 --> 00:07:07,360
comparisons required to find the minimum

202
00:07:07,360 --> 00:07:09,520
and max values of vectors which is

203
00:07:09,520 --> 00:07:10,880
needed both for

204
00:07:10,880 --> 00:07:11,680
um

205
00:07:11,680 --> 00:07:12,639
the

206
00:07:12,639 --> 00:07:14,240
extra trees learning algorithm and then

207
00:07:14,240 --> 00:07:16,400
also for discretization

208
00:07:16,400 --> 00:07:18,319
and then the other thing is going to be

209
00:07:18,319 --> 00:07:20,880
computing the subsets that are used for

210
00:07:20,880 --> 00:07:24,160
training each tree so um figuring out

211
00:07:24,160 --> 00:07:26,080
how to do that obliviously

212
00:07:26,080 --> 00:07:30,599
is um going to be time consuming

213
00:07:32,240 --> 00:07:34,479
so um it's worthwhile

214
00:07:34,479 --> 00:07:36,800
uh looking into how we can optimize both

215
00:07:36,800 --> 00:07:39,520
of these protocols

216
00:07:39,520 --> 00:07:41,199
so

217
00:07:41,199 --> 00:07:43,759
in each case we heavily use this idea of

218
00:07:43,759 --> 00:07:45,919
reusing correlated randomness

219
00:07:45,919 --> 00:07:47,039
so

220
00:07:47,039 --> 00:07:48,960
we know that a beaver triple is a one

221
00:07:48,960 --> 00:07:50,879
use only object

222
00:07:50,879 --> 00:07:52,639
but it's worth reminding ourselves of

223
00:07:52,639 --> 00:07:55,919
why that's the case so if you have

224
00:07:55,919 --> 00:07:57,599
two numbers being multiplied together

225
00:07:57,599 --> 00:07:59,120
with respect to one

226
00:07:59,120 --> 00:08:01,199
beaver triple and then you take two

227
00:08:01,199 --> 00:08:03,039
different numbers and you multiply them

228
00:08:03,039 --> 00:08:04,400
together

229
00:08:04,400 --> 00:08:05,919
in the phase of uh

230
00:08:05,919 --> 00:08:08,080
of one time padding the two secret

231
00:08:08,080 --> 00:08:10,400
values and revealing them to each other

232
00:08:10,400 --> 00:08:11,759
both parties are going to learn enough

233
00:08:11,759 --> 00:08:13,599
information to derive the difference

234
00:08:13,599 --> 00:08:17,120
between the two operands so

235
00:08:17,120 --> 00:08:19,360
clearly that's insecure

236
00:08:19,360 --> 00:08:20,400
however

237
00:08:20,400 --> 00:08:23,199
if it's baked into the protocol that one

238
00:08:23,199 --> 00:08:25,280
operand isn't going to change like we

239
00:08:25,280 --> 00:08:27,039
need to compute the products between x

240
00:08:27,039 --> 00:08:28,000
and y

241
00:08:28,000 --> 00:08:29,919
and x and y prime

242
00:08:29,919 --> 00:08:33,360
and the protocol itself specifies that x

243
00:08:33,360 --> 00:08:34,559
doesn't change between those two

244
00:08:34,559 --> 00:08:36,799
operations then it only needs to be

245
00:08:36,799 --> 00:08:39,599
masked once so only one u value is

246
00:08:39,599 --> 00:08:40,719
required

247
00:08:40,719 --> 00:08:42,299
and so you can imagine that as

248
00:08:42,299 --> 00:08:43,360
[Music]

249
00:08:43,360 --> 00:08:46,320
as the number of these y's increases and

250
00:08:46,320 --> 00:08:47,760
x stays the same

251
00:08:47,760 --> 00:08:49,519
what we're doing is that

252
00:08:49,519 --> 00:08:53,040
in the moment of execution in the online

253
00:08:53,040 --> 00:08:54,399
phase

254
00:08:54,399 --> 00:08:56,160
the number of

255
00:08:56,160 --> 00:08:57,600
triples that are actually being sent

256
00:08:57,600 --> 00:08:59,519
across the network is approaching 50

257
00:08:59,519 --> 00:09:01,120
percent of what it would be if you

258
00:09:01,120 --> 00:09:02,560
aren't

259
00:09:02,560 --> 00:09:04,640
reusing triples here

260
00:09:04,640 --> 00:09:05,760
so

261
00:09:05,760 --> 00:09:07,519
using this fact we attempt to build

262
00:09:07,519 --> 00:09:09,920
protocols that reuse operands as much as

263
00:09:09,920 --> 00:09:12,399
possible

264
00:09:13,920 --> 00:09:16,000
so applying this idea to random subset

265
00:09:16,000 --> 00:09:17,279
selection

266
00:09:17,279 --> 00:09:20,240
we start with a data set that's m by n

267
00:09:20,240 --> 00:09:22,000
and then we define a random feature

268
00:09:22,000 --> 00:09:24,720
selection matrix that is

269
00:09:24,720 --> 00:09:26,720
n times n prime where n prime is the

270
00:09:26,720 --> 00:09:29,600
number of features we want in the subset

271
00:09:29,600 --> 00:09:33,040
and we randomly sample these

272
00:09:33,040 --> 00:09:34,720
indices of

273
00:09:34,720 --> 00:09:36,480
the initial attribute

274
00:09:36,480 --> 00:09:38,720
and we use those as the columns of this

275
00:09:38,720 --> 00:09:39,839
matrix

276
00:09:39,839 --> 00:09:42,000
so computing one subset would be the

277
00:09:42,000 --> 00:09:43,600
product between the original data set

278
00:09:43,600 --> 00:09:46,560
and this matrix however if we

279
00:09:46,560 --> 00:09:48,720
if we zoom out to compute all of the

280
00:09:48,720 --> 00:09:51,200
trees that tree subsets at once

281
00:09:51,200 --> 00:09:54,000
then we can reuse the randomness that

282
00:09:54,000 --> 00:09:56,320
was applied to the data set

283
00:09:56,320 --> 00:09:58,000
against every random feature selection

284
00:09:58,000 --> 00:10:00,160
matrix so

285
00:10:00,160 --> 00:10:01,120
the

286
00:10:01,120 --> 00:10:02,240
beaver triples that are actually

287
00:10:02,240 --> 00:10:04,480
required are

288
00:10:04,480 --> 00:10:06,480
you know 1 over t

289
00:10:06,480 --> 00:10:07,279
for

290
00:10:07,279 --> 00:10:11,760
the u value of what we would expect

291
00:10:13,120 --> 00:10:15,200
similarly

292
00:10:15,200 --> 00:10:17,120
for comparison protocols which are

293
00:10:17,120 --> 00:10:20,240
required for the min max finding

294
00:10:20,240 --> 00:10:22,320
bid decomposition is the essential

295
00:10:22,320 --> 00:10:24,800
component of that converting additive

296
00:10:24,800 --> 00:10:26,640
secret shares into

297
00:10:26,640 --> 00:10:28,800
bit vectors of secret shares over which

298
00:10:28,800 --> 00:10:29,839
integer

299
00:10:29,839 --> 00:10:32,480
logic is possible so

300
00:10:32,480 --> 00:10:34,000
this particular architecture that we

301
00:10:34,000 --> 00:10:35,279
propose

302
00:10:35,279 --> 00:10:37,600
uh is intentionally designed so that at

303
00:10:37,600 --> 00:10:39,680
each layer the fewest number of operands

304
00:10:39,680 --> 00:10:41,760
are reused

305
00:10:41,760 --> 00:10:43,920
where each node where two values are

306
00:10:43,920 --> 00:10:45,600
being paired up that's representing a

307
00:10:45,600 --> 00:10:47,519
two by two matrix multiplication to

308
00:10:47,519 --> 00:10:50,959
compute the um the carry bit in a

309
00:10:50,959 --> 00:10:53,760
lookahead adder

310
00:10:53,760 --> 00:10:55,920
so um

311
00:10:55,920 --> 00:10:57,519
if we do this naively the number of

312
00:10:57,519 --> 00:10:59,920
multiplications required are going to be

313
00:10:59,920 --> 00:11:01,440
l log l

314
00:11:01,440 --> 00:11:04,000
but in this particular architecture and

315
00:11:04,000 --> 00:11:05,440
making sure that we're reusing

316
00:11:05,440 --> 00:11:07,839
correlated randomness where possible

317
00:11:07,839 --> 00:11:10,160
it reduces it to this complex expression

318
00:11:10,160 --> 00:11:11,440
over here

319
00:11:11,440 --> 00:11:12,839
but in the case

320
00:11:12,839 --> 00:11:16,160
of 128 bit integers which is what we use

321
00:11:16,160 --> 00:11:17,519
for our ring

322
00:11:17,519 --> 00:11:20,000
it corresponds to a 40 reduction in data

323
00:11:20,000 --> 00:11:22,720
transfer size

324
00:11:23,680 --> 00:11:26,480
so in terms of what this got us

325
00:11:26,480 --> 00:11:28,800
you can see that for the protocol min

326
00:11:28,800 --> 00:11:30,640
max here which is finding the minimum

327
00:11:30,640 --> 00:11:32,800
and maximum element over a vector of

328
00:11:32,800 --> 00:11:34,079
some length

329
00:11:34,079 --> 00:11:35,440
we're able to get

330
00:11:35,440 --> 00:11:37,680
uh the minimum and maximum of a million

331
00:11:37,680 --> 00:11:40,800
bit vector in less than two seconds

332
00:11:40,800 --> 00:11:42,240
which is pretty surprising to us and

333
00:11:42,240 --> 00:11:44,640
then extending that to

334
00:11:44,640 --> 00:11:46,720
discretization over an entire data set

335
00:11:46,720 --> 00:11:48,720
with a thousand features you can see

336
00:11:48,720 --> 00:11:52,399
here that if we split into only two bins

337
00:11:52,399 --> 00:11:53,360
then

338
00:11:53,360 --> 00:11:56,079
even for a hundred thousand number of

339
00:11:56,079 --> 00:11:57,519
instances in that data set it's still

340
00:11:57,519 --> 00:11:59,360
less than two minutes to

341
00:11:59,360 --> 00:12:02,880
discretize that entire data set

342
00:12:03,600 --> 00:12:05,920
as for the top level protocols

343
00:12:05,920 --> 00:12:07,440
the left side of this is showing in the

344
00:12:07,440 --> 00:12:09,519
clear accuracy results

345
00:12:09,519 --> 00:12:11,360
and then the right side is showing our

346
00:12:11,360 --> 00:12:14,000
secure protocol

347
00:12:14,000 --> 00:12:15,839
execution

348
00:12:15,839 --> 00:12:17,600
so

349
00:12:17,600 --> 00:12:18,800
you can see that there's a little bit of

350
00:12:18,800 --> 00:12:20,639
an accuracy drop

351
00:12:20,639 --> 00:12:22,320
between in the clear and

352
00:12:22,320 --> 00:12:24,320
in the secure protocol execution which

353
00:12:24,320 --> 00:12:26,240
is to be expected

354
00:12:26,240 --> 00:12:28,639
these values can be tuned

355
00:12:28,639 --> 00:12:30,639
such as the number of bins that are used

356
00:12:30,639 --> 00:12:32,800
or the number of trees that are used to

357
00:12:32,800 --> 00:12:35,200
increase that accuracy but

358
00:12:35,200 --> 00:12:37,519
you can see here that

359
00:12:37,519 --> 00:12:40,000
the execution time for any of these is

360
00:12:40,000 --> 00:12:41,120
still

361
00:12:41,120 --> 00:12:43,040
measured in seconds

362
00:12:43,040 --> 00:12:45,200
and um

363
00:12:45,200 --> 00:12:47,600
with the data set sizes of you know

364
00:12:47,600 --> 00:12:49,839
fifteen thousand or so features

365
00:12:49,839 --> 00:12:52,399
uh it's uh it's pretty clear that this

366
00:12:52,399 --> 00:12:54,980
is extensible to real size data sets

367
00:12:54,980 --> 00:12:58,070
[Music]

368
00:12:58,800 --> 00:13:00,639
so as for comparison with contemporary

369
00:13:00,639 --> 00:13:01,440
work

370
00:13:01,440 --> 00:13:03,040
um

371
00:13:03,040 --> 00:13:04,720
there was a paper that was being

372
00:13:04,720 --> 00:13:06,480
published around the same time as ours

373
00:13:06,480 --> 00:13:10,000
that was um taking a new approach to

374
00:13:10,000 --> 00:13:12,240
the c 4.5 sorting based algorithm that

375
00:13:12,240 --> 00:13:13,120
used

376
00:13:13,120 --> 00:13:15,360
three-party computation sorting based

377
00:13:15,360 --> 00:13:16,399
networks

378
00:13:16,399 --> 00:13:18,399
so we took their data set dimensions

379
00:13:18,399 --> 00:13:20,480
that they tested against

380
00:13:20,480 --> 00:13:22,639
in which they report 28 hours for

381
00:13:22,639 --> 00:13:25,399
computing 200 trees with 11 features and

382
00:13:25,399 --> 00:13:28,000
8192 instances

383
00:13:28,000 --> 00:13:29,920
and our protocol is able to execute in

384
00:13:29,920 --> 00:13:30,959
eight minutes

385
00:13:30,959 --> 00:13:33,200
so as a disclaimer this comparison isn't

386
00:13:33,200 --> 00:13:36,000
100 fair because if they're using three

387
00:13:36,000 --> 00:13:38,240
pc the multiplication protocol itself is

388
00:13:38,240 --> 00:13:39,199
different

389
00:13:39,199 --> 00:13:41,040
however i think the difference is

390
00:13:41,040 --> 00:13:42,959
staggering enough that it's pretty clear

391
00:13:42,959 --> 00:13:44,560
if their protocol is implemented in two

392
00:13:44,560 --> 00:13:47,040
pc or ours was implemented in three pc

393
00:13:47,040 --> 00:13:49,040
that we would be able to see still

394
00:13:49,040 --> 00:13:50,750
multiple orders of magnitude difference

395
00:13:50,750 --> 00:13:52,160
[Music]

396
00:13:52,160 --> 00:13:54,240
another difference is that they do not

397
00:13:54,240 --> 00:13:56,320
randomly compute the

398
00:13:56,320 --> 00:13:57,760
subsets or they're not using an

399
00:13:57,760 --> 00:14:00,959
oblivious protocol for that and we are

400
00:14:00,959 --> 00:14:03,199
okay so wrapping up

401
00:14:03,199 --> 00:14:04,720
the um

402
00:14:04,720 --> 00:14:06,320
some of the takeaways are that shallow

403
00:14:06,320 --> 00:14:08,639
ensembles seem to work well with mpc's

404
00:14:08,639 --> 00:14:10,560
constraints and so

405
00:14:10,560 --> 00:14:11,680
uh

406
00:14:11,680 --> 00:14:13,760
by by virtue of instead of training one

407
00:14:13,760 --> 00:14:15,600
large decision tree which requires a lot

408
00:14:15,600 --> 00:14:17,040
of waiting for

409
00:14:17,040 --> 00:14:18,480
communication rounds and things like

410
00:14:18,480 --> 00:14:19,199
that

411
00:14:19,199 --> 00:14:22,000
training a lot of shallow trees

412
00:14:22,000 --> 00:14:23,920
can yield similar results even better a

413
00:14:23,920 --> 00:14:25,519
lot of the time

414
00:14:25,519 --> 00:14:27,440
and it works well with

415
00:14:27,440 --> 00:14:29,440
the constraints that are imposed by

416
00:14:29,440 --> 00:14:31,920
secure multiplication

417
00:14:31,920 --> 00:14:32,639
so

418
00:14:32,639 --> 00:14:34,480
as we've shown empirically

419
00:14:34,480 --> 00:14:36,800
discretization or using categorical

420
00:14:36,800 --> 00:14:39,920
training of some type with id3 seems to

421
00:14:39,920 --> 00:14:43,760
be more practical than c4.5 for

422
00:14:43,760 --> 00:14:46,000
secure training

423
00:14:46,000 --> 00:14:47,199
though

424
00:14:47,199 --> 00:14:48,560
since we are losing a little bit of

425
00:14:48,560 --> 00:14:50,639
information and discretizing it might be

426
00:14:50,639 --> 00:14:52,320
less accurate if hyper parameters aren't

427
00:14:52,320 --> 00:14:54,240
chosen correctly

428
00:14:54,240 --> 00:14:56,639
and of interest outside of this paper

429
00:14:56,639 --> 00:14:58,959
i think that uh just a general

430
00:14:58,959 --> 00:15:01,279
um design practice of trying to reuse

431
00:15:01,279 --> 00:15:03,839
operands as much as possible will bring

432
00:15:03,839 --> 00:15:05,839
things out of the realm of

433
00:15:05,839 --> 00:15:07,440
theoretical and into the realm of

434
00:15:07,440 --> 00:15:09,680
practical a lot more quickly than other

435
00:15:09,680 --> 00:15:11,680
things

436
00:15:11,680 --> 00:15:14,079
and if you would like to use our current

437
00:15:14,079 --> 00:15:15,760
primitives that i showed on the other

438
00:15:15,760 --> 00:15:17,040
slide

439
00:15:17,040 --> 00:15:19,199
then feel free to check out our mpc

440
00:15:19,199 --> 00:15:23,120
library here which is called rust links

441
00:15:23,120 --> 00:15:25,279
and as a separate note about those those

442
00:15:25,279 --> 00:15:26,880
protocols we're currently at a point

443
00:15:26,880 --> 00:15:29,120
where seventy percent of

444
00:15:29,120 --> 00:15:31,360
the the time spent on a protocol is

445
00:15:31,360 --> 00:15:33,199
local computation and thirty percent is

446
00:15:33,199 --> 00:15:36,240
network um and this is a really kind of

447
00:15:36,240 --> 00:15:37,839
unheard of problem to have an npc

448
00:15:37,839 --> 00:15:40,720
because uh it basically means that now

449
00:15:40,720 --> 00:15:42,959
standard operations uh

450
00:15:42,959 --> 00:15:46,079
for optimization are going to

451
00:15:46,079 --> 00:15:47,839
be able to reduce the running time of

452
00:15:47,839 --> 00:15:50,240
these protocols uh even further such as

453
00:15:50,240 --> 00:15:51,759
implementing gpus for matrix

454
00:15:51,759 --> 00:15:55,199
multiplication and and the like

455
00:15:55,199 --> 00:15:57,680
so thank you and i'm happy to take any

456
00:15:57,680 --> 00:16:00,680
questions

