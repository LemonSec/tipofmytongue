1
00:00:00,299 --> 00:00:02,939
hello everyone I'm glad to present our

2
00:00:02,939 --> 00:00:05,100
paper Machinery with differentially

3
00:00:05,100 --> 00:00:07,740
private labels mechanism and Frameworks

4
00:00:07,740 --> 00:00:09,840
this is the joint work with my amazing

5
00:00:09,840 --> 00:00:11,460
collaborators from Princeton University

6
00:00:11,460 --> 00:00:14,700
and UMass Amherst first let me introduce

7
00:00:14,700 --> 00:00:16,500
a definition of Naval differential

8
00:00:16,500 --> 00:00:19,199
privacy we call a randomized mechanism M

9
00:00:19,199 --> 00:00:21,180
with domain D which includes three

10
00:00:21,180 --> 00:00:23,340
feature set acts and labels it y to

11
00:00:23,340 --> 00:00:25,920
preserve apps on digital label DP if the

12
00:00:25,920 --> 00:00:28,320
probability of outputs for any two label

13
00:00:28,320 --> 00:00:31,080
laboring data sets are similar here is

14
00:00:31,080 --> 00:00:33,480
an example of the label labor data sets

15
00:00:33,480 --> 00:00:35,460
where we can say that the fission sets

16
00:00:35,460 --> 00:00:37,680
here are all the same while there is one

17
00:00:37,680 --> 00:00:40,559
label by three difference in a label set

18
00:00:40,559 --> 00:00:43,800
We Now understand what is labeled DP the

19
00:00:43,800 --> 00:00:46,140
next question is why do we study labeled

20
00:00:46,140 --> 00:00:48,719
EP label DP fits for several practical

21
00:00:48,719 --> 00:00:51,059
scenarios will feature the public or

22
00:00:51,059 --> 00:00:53,940
labels to be keep private one example is

23
00:00:53,940 --> 00:00:56,760
ADS ranking while the ad attack notes as

24
00:00:56,760 --> 00:00:58,980
content delivered to the user which can

25
00:00:58,980 --> 00:01:00,899
be regarded as public feature however

26
00:01:00,899 --> 00:01:03,359
the user's interaction with as content

27
00:01:03,359 --> 00:01:06,000
should be kept private another example

28
00:01:06,000 --> 00:01:08,100
is some sensitive survey which may ask

29
00:01:08,100 --> 00:01:09,540
questions about income or medical

30
00:01:09,540 --> 00:01:12,180
history in the meanwhile the profile of

31
00:01:12,180 --> 00:01:14,159
the participants such as gender or race

32
00:01:14,159 --> 00:01:16,080
might be public

33
00:01:16,080 --> 00:01:18,600
as label DP only request label to be

34
00:01:18,600 --> 00:01:20,580
privileged we understand that if a

35
00:01:20,580 --> 00:01:22,860
mechanism ends preserves absolute RDP

36
00:01:22,860 --> 00:01:24,900
then it also preserves Epsilon digital

37
00:01:24,900 --> 00:01:27,960
label DP therefore we can use current DB

38
00:01:27,960 --> 00:01:30,360
techniques such as dpsgd to achieve

39
00:01:30,360 --> 00:01:33,479
labeled AP the problem is that dpsgd may

40
00:01:33,479 --> 00:01:35,640
suffer from a loss of utility

41
00:01:35,640 --> 00:01:38,040
as labeldp is a much more relaxed form

42
00:01:38,040 --> 00:01:40,439
of privacy where only label its private

43
00:01:40,439 --> 00:01:43,079
the key question will ask in this work

44
00:01:43,079 --> 00:01:46,259
is how can optimized utility with this

45
00:01:46,259 --> 00:01:48,899
relaxed privacy restriction

46
00:01:48,899 --> 00:01:51,780
all work on labeldp focus on how to

47
00:01:51,780 --> 00:01:55,079
improve the utility specifically our key

48
00:01:55,079 --> 00:01:57,659
insights are true false first we can

49
00:01:57,659 --> 00:01:59,040
leverage the public feature information

50
00:01:59,040 --> 00:02:02,460
to deny the travel labels second we will

51
00:02:02,460 --> 00:02:03,899
also leverage the most recent

52
00:02:03,899 --> 00:02:05,640
advancements in machine learning to

53
00:02:05,640 --> 00:02:07,979
further improve the utility all

54
00:02:07,979 --> 00:02:09,899
Frameworks leverage on supervised

55
00:02:09,899 --> 00:02:11,700
learning semi-supervised learning and

56
00:02:11,700 --> 00:02:13,620
learning with noise Mabel techniques to

57
00:02:13,620 --> 00:02:15,780
add less noise and also reduce the

58
00:02:15,780 --> 00:02:18,959
effect of noise besides we also make a

59
00:02:18,959 --> 00:02:21,180
comprehensive comparison with existing

60
00:02:21,180 --> 00:02:23,220
works and also make a specific

61
00:02:23,220 --> 00:02:26,819
recommendations regarding our framework

62
00:02:26,819 --> 00:02:28,620
one good property for differential

63
00:02:28,620 --> 00:02:30,959
privacy is proofs processing and this

64
00:02:30,959 --> 00:02:33,780
also holds for label DP that is if a

65
00:02:33,780 --> 00:02:36,180
mechanism and plan which was domain on

66
00:02:36,180 --> 00:02:38,459
the labels is y and also ranging y such

67
00:02:38,459 --> 00:02:41,400
as on digital dpny which also implies

68
00:02:41,400 --> 00:02:44,280
label DP not for any other mechanism and

69
00:02:44,280 --> 00:02:47,519
which take X and M Prime y as input will

70
00:02:47,519 --> 00:02:49,560
also satisfy the epsilometers are

71
00:02:49,560 --> 00:02:52,140
labeled AP based on existing works on

72
00:02:52,140 --> 00:02:53,760
how to generate the differential travel

73
00:02:53,760 --> 00:02:56,400
labels we can proceed to focus on how to

74
00:02:56,400 --> 00:02:59,099
improve utility we use two existing

75
00:02:59,099 --> 00:03:00,239
differential probability label

76
00:03:00,239 --> 00:03:02,580
generation one is a randomized response

77
00:03:02,580 --> 00:03:04,980
which probability replace a true label

78
00:03:04,980 --> 00:03:07,440
with a random label the second one is

79
00:03:07,440 --> 00:03:09,239
party effects match which is the

80
00:03:09,239 --> 00:03:11,459
variation of the part A framework

81
00:03:11,459 --> 00:03:14,159
love as a label DP allows the feature to

82
00:03:14,159 --> 00:03:16,620
be public the partifix match will use

83
00:03:16,620 --> 00:03:18,659
the same public feature to clear the

84
00:03:18,659 --> 00:03:20,400
teacher model and train the student

85
00:03:20,400 --> 00:03:21,360
model

86
00:03:21,360 --> 00:03:24,060
same as part A Part FX match also

87
00:03:24,060 --> 00:03:25,560
benefits from the flexible number

88
00:03:25,560 --> 00:03:28,500
queries of the Privacy to the trade-off

89
00:03:28,500 --> 00:03:31,920
that has low low fuel queries the lower

90
00:03:31,920 --> 00:03:34,620
absolute law particular Flex match is

91
00:03:34,620 --> 00:03:36,180
used as one of our pipeline which is

92
00:03:36,180 --> 00:03:38,760
pipeline three I will if I will

93
00:03:38,760 --> 00:03:40,500
introduce the python 1 to intellect

94
00:03:40,500 --> 00:03:42,720
slides to generate the differential

95
00:03:42,720 --> 00:03:44,340
private labels on the whole training set

96
00:03:44,340 --> 00:03:46,739
by Patrick fixed match we will query the

97
00:03:46,739 --> 00:03:49,920
student model to generate such labels

98
00:03:49,920 --> 00:03:52,200
our first pipeline is based solely on

99
00:03:52,200 --> 00:03:54,180
unsupervised learning which can add less

100
00:03:54,180 --> 00:03:56,700
noise compared to a directly added noise

101
00:03:56,700 --> 00:03:59,700
to the individual Sample When thinking

102
00:03:59,700 --> 00:04:02,340
of the labor privacy an easy approach to

103
00:04:02,340 --> 00:04:04,439
protect the privacy of sensitive label

104
00:04:04,439 --> 00:04:07,140
is to not use them at all therefore we

105
00:04:07,140 --> 00:04:09,900
consider clustery after the clustery we

106
00:04:09,900 --> 00:04:11,340
still need to assign the label for

107
00:04:11,340 --> 00:04:13,140
clusters so we still need some label

108
00:04:13,140 --> 00:04:15,299
information we propose a noise classic

109
00:04:15,299 --> 00:04:17,579
which adds a gaussian noise to the class

110
00:04:17,579 --> 00:04:19,620
count for each cluster on the two labels

111
00:04:19,620 --> 00:04:22,740
and then does the majority voting here

112
00:04:22,740 --> 00:04:24,840
is an example of the two classes space

113
00:04:24,840 --> 00:04:27,540
where all the points here are with a

114
00:04:27,540 --> 00:04:30,240
true label that is the Blue Points our

115
00:04:30,240 --> 00:04:32,400
true label blue and the red points are

116
00:04:32,400 --> 00:04:34,199
with the true label red after the

117
00:04:34,199 --> 00:04:36,060
unsupervised learning we have two

118
00:04:36,060 --> 00:04:38,580
classes while cluster one has most blue

119
00:04:38,580 --> 00:04:41,160
points and cluster 2 has most red points

120
00:04:41,160 --> 00:04:43,500
let's look at a potential two

121
00:04:43,500 --> 00:04:46,080
we know the county in cluster 2 for red

122
00:04:46,080 --> 00:04:49,080
versus blue is 18 versus three although

123
00:04:49,080 --> 00:04:50,759
soon as the gaussian noise to this

124
00:04:50,759 --> 00:04:53,580
country after the majority voting of the

125
00:04:53,580 --> 00:04:55,860
gaussian noise content we can get

126
00:04:55,860 --> 00:04:58,620
success label for each cluster as the

127
00:04:58,620 --> 00:05:01,500
cluster 2 has the most red points the

128
00:05:01,500 --> 00:05:03,180
majority voting of the gaussian noise

129
00:05:03,180 --> 00:05:05,340
content is likely to be the majority

130
00:05:05,340 --> 00:05:08,400
voting on The Noise free Counting

131
00:05:08,400 --> 00:05:10,620
here is an overview of our pipeline one

132
00:05:10,620 --> 00:05:12,840
we refers to the unsupervised learning

133
00:05:12,840 --> 00:05:14,940
to get the classroom model within

134
00:05:14,940 --> 00:05:17,820
generated classes and after that we will

135
00:05:17,820 --> 00:05:20,039
apply our noise class algorithm to

136
00:05:20,039 --> 00:05:22,199
assign a class label on the cluster

137
00:05:22,199 --> 00:05:25,139
model and get a final Model F

138
00:05:25,139 --> 00:05:28,020
during this process no noise is added to

139
00:05:28,020 --> 00:05:30,360
individual sample and after unsupervised

140
00:05:30,360 --> 00:05:32,460
learning no learning process is further

141
00:05:32,460 --> 00:05:33,539
need

142
00:05:33,539 --> 00:05:35,699
our second pipeline focus on how to

143
00:05:35,699 --> 00:05:38,100
reduce the effect of noise based on

144
00:05:38,100 --> 00:05:39,840
existing advancements in machine

145
00:05:39,840 --> 00:05:42,180
learning the noise cluster has a

146
00:05:42,180 --> 00:05:44,160
limitation that its performance is

147
00:05:44,160 --> 00:05:45,780
bottlenecked by unsupervised learning

148
00:05:45,780 --> 00:05:48,900
that is even my Epsilon is very high and

149
00:05:48,900 --> 00:05:50,940
gives almost a true label at private

150
00:05:50,940 --> 00:05:53,280
label set the noise cluster will still

151
00:05:53,280 --> 00:05:54,479
have the same performance as

152
00:05:54,479 --> 00:05:56,039
unsupervised learning

153
00:05:56,039 --> 00:05:58,380
we proposed dinos SSL which combines

154
00:05:58,380 --> 00:06:00,240
unsupervised learning and semi surprise

155
00:06:00,240 --> 00:06:02,460
learning our strategy is that we will

156
00:06:02,460 --> 00:06:04,560
only select samples which have SIM noise

157
00:06:04,560 --> 00:06:07,020
label and pseudo label the noise label

158
00:06:07,020 --> 00:06:09,000
is either by randomized response or part

159
00:06:09,000 --> 00:06:11,819
FX match under suitable is by the

160
00:06:11,819 --> 00:06:13,800
majority voting on accounts of noise

161
00:06:13,800 --> 00:06:16,080
level set for clustering and no extra

162
00:06:16,080 --> 00:06:18,780
noise is added in this process

163
00:06:18,780 --> 00:06:21,600
here is an example the points in this

164
00:06:21,600 --> 00:06:23,639
blue point in this blue area are always

165
00:06:23,639 --> 00:06:25,560
a true label blue and the blue point

166
00:06:25,560 --> 00:06:27,840
indicate that there was two label while

167
00:06:27,840 --> 00:06:29,699
the red points indicate that there was a

168
00:06:29,699 --> 00:06:32,580
noise level red we also had a pseudo

169
00:06:32,580 --> 00:06:34,979
label set with by the classroom which

170
00:06:34,979 --> 00:06:37,560
means I'm most points as blue but still

171
00:06:37,560 --> 00:06:39,840
zero as red due to the performance of

172
00:06:39,840 --> 00:06:42,419
clustering by comparing the noise level

173
00:06:42,419 --> 00:06:44,940
set and the pseudo label search we can

174
00:06:44,940 --> 00:06:47,280
find the mismatch in this set and we

175
00:06:47,280 --> 00:06:50,220
will remove the three mismatch points we

176
00:06:50,220 --> 00:06:52,380
can see that compared to the noise label

177
00:06:52,380 --> 00:06:55,199
set the output of the denoising is a

178
00:06:55,199 --> 00:06:57,360
smaller label set but with a higher

179
00:06:57,360 --> 00:07:00,060
position as we still have the full

180
00:07:00,060 --> 00:07:02,280
public feature set we will consider

181
00:07:02,280 --> 00:07:04,680
semi-supervised learning

182
00:07:04,680 --> 00:07:07,380
besides the noise SSO we will also

183
00:07:07,380 --> 00:07:09,780
consider directly leveraging risk and

184
00:07:09,780 --> 00:07:11,520
advancements in machine learning beyond

185
00:07:11,520 --> 00:07:13,199
the standard optimization

186
00:07:13,199 --> 00:07:16,259
we consider ldp SSL which directly

187
00:07:16,259 --> 00:07:17,759
applies the music provides learning on

188
00:07:17,759 --> 00:07:20,160
the whole training set specifically we

189
00:07:20,160 --> 00:07:22,680
will use fixed match we also consider

190
00:07:22,680 --> 00:07:25,860
ldpl which will barely take a use

191
00:07:25,860 --> 00:07:27,419
techniques from the learning with noise

192
00:07:27,419 --> 00:07:29,819
label techniques this is because on

193
00:07:29,819 --> 00:07:31,560
moderners can learn more label

194
00:07:31,560 --> 00:07:33,240
information during the training process

195
00:07:33,240 --> 00:07:35,639
and learning with noise level techniques

196
00:07:35,639 --> 00:07:37,699
can leverage such labor information

197
00:07:37,699 --> 00:07:41,580
specifically we will use organisms

198
00:07:41,580 --> 00:07:43,860
here is an overview of our pipeline too

199
00:07:43,860 --> 00:07:46,080
we will generate the private labels by

200
00:07:46,080 --> 00:07:47,580
either by randomized response or

201
00:07:47,580 --> 00:07:49,620
partifix match we will also do

202
00:07:49,620 --> 00:07:51,539
unsupervised learning to generate the

203
00:07:51,539 --> 00:07:53,759
Clusters and then generate the pseudo

204
00:07:53,759 --> 00:07:56,460
label after that we will train the

205
00:07:56,460 --> 00:07:58,199
Machinery models under label

206
00:07:58,199 --> 00:08:00,419
differential privacy either by standard

207
00:08:00,419 --> 00:08:02,699
optimization semi-supervised learning or

208
00:08:02,699 --> 00:08:04,740
learning with noise label techniques or

209
00:08:04,740 --> 00:08:07,199
our denois SSO

210
00:08:07,199 --> 00:08:09,300
we conduct our experiments on three

211
00:08:09,300 --> 00:08:10,919
Benchmark data sets for image

212
00:08:10,919 --> 00:08:13,440
classification see if I turn c500 and

213
00:08:13,440 --> 00:08:15,720
significant the number of classes vary

214
00:08:15,720 --> 00:08:18,120
from 10 to 100 and the total number of

215
00:08:18,120 --> 00:08:21,060
images vary from sixty thousand to two

216
00:08:21,060 --> 00:08:22,740
hundred and seventeen thousand

217
00:08:22,740 --> 00:08:26,340
will consider models including 2018 with

218
00:08:26,340 --> 00:08:30,240
gg16 and wireless network 28 and 4.

219
00:08:30,240 --> 00:08:32,760
the first evaluate of python 1 notes

220
00:08:32,760 --> 00:08:34,559
cluster where we can say that the noise

221
00:08:34,559 --> 00:08:37,019
cluster performs well at small Epsilon

222
00:08:37,019 --> 00:08:40,200
for example on safer turn even Epsilon

223
00:08:40,200 --> 00:08:41,479
is equal to

224
00:08:41,479 --> 00:08:44,399
0.003 while the randomized response only

225
00:08:44,399 --> 00:08:47,160
gives the label arches a 10.03 percent

226
00:08:47,160 --> 00:08:50,160
which is too close to totally random the

227
00:08:50,160 --> 00:08:52,260
noise cluster can still have the high

228
00:08:52,260 --> 00:08:55,560
performance as 88.3 percent

229
00:08:55,560 --> 00:08:58,440
and this is also true for 700 and

230
00:08:58,440 --> 00:08:59,580
signature

231
00:08:59,580 --> 00:09:02,700
the next value of the dinos SSO and ldps

232
00:09:02,700 --> 00:09:05,399
so in our pipeline too so here we

233
00:09:05,399 --> 00:09:07,380
compare the noise cluster and you know

234
00:09:07,380 --> 00:09:09,959
Dino's SSO and ldb has also combined

235
00:09:09,959 --> 00:09:12,480
with randomized response we can see that

236
00:09:12,480 --> 00:09:14,820
while the noise cluster keeps the same

237
00:09:14,820 --> 00:09:16,860
performance at all reported Epsilon

238
00:09:16,860 --> 00:09:20,339
which is 88.3 percent Latino's assets

239
00:09:20,339 --> 00:09:23,760
both adenos SSO and ldps so increase the

240
00:09:23,760 --> 00:09:26,160
performance as absolute increases

241
00:09:26,160 --> 00:09:28,500
besides though do you know says so is

242
00:09:28,500 --> 00:09:31,200
always has a better performance Compared

243
00:09:31,200 --> 00:09:33,540
to North class at all reported Epsilon

244
00:09:33,540 --> 00:09:36,360
and ldp I say so it's better than noise

245
00:09:36,360 --> 00:09:39,839
classes at Epsilon equal to 4.

246
00:09:39,839 --> 00:09:42,120
as for the pathway effect match as our

247
00:09:42,120 --> 00:09:44,880
pipeline three we can uh we can compare

248
00:09:44,880 --> 00:09:46,860
we can see that compared to randomized

249
00:09:46,860 --> 00:09:49,920
response but combined with the noise SSO

250
00:09:49,920 --> 00:09:53,040
and ldpsso the pathfx match is always

251
00:09:53,040 --> 00:09:55,200
better than these two approaches on a

252
00:09:55,200 --> 00:09:57,720
randomized response this is because the

253
00:09:57,720 --> 00:10:00,120
Pacific's match will benefit from the

254
00:10:00,120 --> 00:10:02,339
choice of flexible number of queries to

255
00:10:02,339 --> 00:10:04,320
add less noise

256
00:10:04,320 --> 00:10:07,740
we next evaluates the RDP artisans in

257
00:10:07,740 --> 00:10:09,839
your pipeline too which is the newest

258
00:10:09,839 --> 00:10:11,760
noise label techniques so here we

259
00:10:11,760 --> 00:10:14,279
consider the randomized response or

260
00:10:14,279 --> 00:10:16,440
partifix match combined with best

261
00:10:16,440 --> 00:10:18,839
performance of other approaches or the

262
00:10:18,839 --> 00:10:21,720
rdpr distance we can see that a DP

263
00:10:21,720 --> 00:10:24,000
organism performs well in most cases

264
00:10:24,000 --> 00:10:26,519
except for Epsilon equal to 0.5 by

265
00:10:26,519 --> 00:10:29,399
randomized response besides when Epsilon

266
00:10:29,399 --> 00:10:31,740
is low low oil is lower the best

267
00:10:31,740 --> 00:10:34,380
accuracy is filed from the Pacific's

268
00:10:34,380 --> 00:10:36,420
match combined with ldb August or

269
00:10:36,420 --> 00:10:38,700
descent this is because the party fixed

270
00:10:38,700 --> 00:10:40,560
match can provide the label set with

271
00:10:40,560 --> 00:10:43,140
higher accuracy one lab or absolutely is

272
00:10:43,140 --> 00:10:44,940
high the best accuracy is from the

273
00:10:44,940 --> 00:10:46,920
randomized response combined with ldp

274
00:10:46,920 --> 00:10:49,140
Arc distance this is this is because

275
00:10:49,140 --> 00:10:51,720
runway's response can provide more label

276
00:10:51,720 --> 00:10:53,399
information

277
00:10:53,399 --> 00:10:56,519
we also make a comparison with existing

278
00:10:56,519 --> 00:10:59,100
works and we can uh with uh which is

279
00:10:59,100 --> 00:11:02,339
Alaba and lp1c random trial we can see

280
00:11:02,339 --> 00:11:05,040
that our work is better than The Alibi

281
00:11:05,040 --> 00:11:07,860
and rp1sc at all reported Epsilon

282
00:11:07,860 --> 00:11:10,920
besides we can also observe that the

283
00:11:10,920 --> 00:11:13,980
utility of of advantage of our work

284
00:11:13,980 --> 00:11:17,220
decreases as absolute increases not at

285
00:11:17,220 --> 00:11:19,620
Epsilon equal to 4 the utility Advantage

286
00:11:19,620 --> 00:11:21,779
is still significant for more

287
00:11:21,779 --> 00:11:23,880
experimental results please take Cloud

288
00:11:23,880 --> 00:11:24,779
paper

289
00:11:24,779 --> 00:11:27,360
we finally make recommendations on how

290
00:11:27,360 --> 00:11:29,399
to maximize the utility of our framework

291
00:11:29,399 --> 00:11:32,100
when Epsilon is extremely low please

292
00:11:32,100 --> 00:11:34,620
consider noise cluster if a bounded

293
00:11:34,620 --> 00:11:37,019
Computer Resources is available and with

294
00:11:37,019 --> 00:11:39,300
a high label credit earned and a low

295
00:11:39,300 --> 00:11:41,700
Epsilon please consider party effects

296
00:11:41,700 --> 00:11:44,040
match without above specific conditions

297
00:11:44,040 --> 00:11:46,200
choosing randomized response can make

298
00:11:46,200 --> 00:11:48,899
sense simpler specifically when absolute

299
00:11:48,899 --> 00:11:51,060
is high you can directly use standard

300
00:11:51,060 --> 00:11:52,920
optimization and randomized response

301
00:11:52,920 --> 00:11:54,540
already get the label set of high

302
00:11:54,540 --> 00:11:57,300
accuracy well Epsilon is low you can

303
00:11:57,300 --> 00:11:59,339
consider denoise algorithms such as the

304
00:11:59,339 --> 00:12:03,060
noise SSO or oxycinct to reduce effect

305
00:12:03,060 --> 00:12:04,560
of noise

306
00:12:04,560 --> 00:12:08,359
that's all thank you for listening

