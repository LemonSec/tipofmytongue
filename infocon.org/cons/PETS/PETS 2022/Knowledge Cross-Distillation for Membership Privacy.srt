1
00:00:01,439 --> 00:00:04,240
i'm knit roytel from nac

2
00:00:04,240 --> 00:00:06,960
i talk about our privacy protection

3
00:00:06,960 --> 00:00:10,800
algorithm for machine learning

4
00:00:12,960 --> 00:00:15,839
these days there is increasing concern

5
00:00:15,839 --> 00:00:18,960
on privacy of sensitive data

6
00:00:18,960 --> 00:00:21,680
to obtain machine learning models

7
00:00:21,680 --> 00:00:25,279
we use various domains such as health

8
00:00:25,279 --> 00:00:28,560
care finance natural language and

9
00:00:28,560 --> 00:00:31,039
autonomous driving

10
00:00:31,039 --> 00:00:33,920
according to recent privacy guidelines

11
00:00:33,920 --> 00:00:35,920
like zdpr

12
00:00:35,920 --> 00:00:38,800
machine learners have to protect privacy

13
00:00:38,800 --> 00:00:41,919
of training data

14
00:00:43,040 --> 00:00:45,760
a specific threat is membership

15
00:00:45,760 --> 00:00:47,600
inference attack

16
00:00:47,600 --> 00:00:50,719
which is an attempt that info whether

17
00:00:50,719 --> 00:00:54,000
target data is used as a training date

18
00:00:54,000 --> 00:00:56,320
or not

19
00:00:56,320 --> 00:00:59,039
adversary queries the target to a

20
00:00:59,039 --> 00:01:00,800
machine learning model

21
00:01:00,800 --> 00:01:01,840
to

22
00:01:01,840 --> 00:01:04,799
get output confidence score

23
00:01:04,799 --> 00:01:06,400
and observe it

24
00:01:06,400 --> 00:01:07,600
infer

25
00:01:07,600 --> 00:01:10,799
if the target is in the training dataset

26
00:01:10,799 --> 00:01:13,040
or not

27
00:01:13,040 --> 00:01:15,680
such leakage of membership information

28
00:01:15,680 --> 00:01:17,280
can be a threat

29
00:01:17,280 --> 00:01:20,000
when training data is collected from

30
00:01:20,000 --> 00:01:21,520
sensitive domain

31
00:01:21,520 --> 00:01:26,280
like patient of cancer

32
00:01:27,600 --> 00:01:30,479
membership influence attack is mainly

33
00:01:30,479 --> 00:01:33,200
caused by overfeeding

34
00:01:33,200 --> 00:01:36,000
that is tendency of machine learning

35
00:01:36,000 --> 00:01:37,040
model

36
00:01:37,040 --> 00:01:41,520
excessively around the training data set

37
00:01:41,520 --> 00:01:43,200
for instance

38
00:01:43,200 --> 00:01:46,720
there are gap between confidence score

39
00:01:46,720 --> 00:01:50,159
of training data and that of known

40
00:01:50,159 --> 00:01:52,000
training data

41
00:01:52,000 --> 00:01:52,840
so

42
00:01:52,840 --> 00:01:56,320
adversary can exploit this to infill

43
00:01:56,320 --> 00:01:59,679
membership information

44
00:02:02,880 --> 00:02:06,320
there are two kinds of differences

45
00:02:06,320 --> 00:02:08,399
started by defense

46
00:02:08,399 --> 00:02:12,080
their algorithms give some guarantee

47
00:02:12,080 --> 00:02:14,080
but nobody knows

48
00:02:14,080 --> 00:02:16,400
they have optimal privacy utility

49
00:02:16,400 --> 00:02:19,599
tradeoffs especially under real world

50
00:02:19,599 --> 00:02:22,000
settings

51
00:02:22,000 --> 00:02:24,080
empirical difference

52
00:02:24,080 --> 00:02:27,120
their algorithms have no guarantee

53
00:02:27,120 --> 00:02:32,160
but not bad if designed properly

54
00:02:32,400 --> 00:02:34,879
empirical differences are often either

55
00:02:34,879 --> 00:02:37,519
blocked with some attack were proved to

56
00:02:37,519 --> 00:02:40,879
be an optimal defense in the future

57
00:02:40,879 --> 00:02:44,480
so inventing empirical defenses that use

58
00:02:44,480 --> 00:02:47,040
different fundamental principles is

59
00:02:47,040 --> 00:02:50,040
important

60
00:02:51,599 --> 00:02:53,840
instead of the state-of-the-art

61
00:02:53,840 --> 00:02:57,280
empirical defense prevents overfilling

62
00:02:57,280 --> 00:03:01,599
by distillation with public data

63
00:03:01,599 --> 00:03:02,480
first

64
00:03:02,480 --> 00:03:05,120
train teacher model using private

65
00:03:05,120 --> 00:03:07,680
training data

66
00:03:07,680 --> 00:03:08,959
prepare

67
00:03:08,959 --> 00:03:11,360
another public digit

68
00:03:11,360 --> 00:03:13,760
to obtain soft score

69
00:03:13,760 --> 00:03:16,080
using that data

70
00:03:16,080 --> 00:03:16,959
then

71
00:03:16,959 --> 00:03:19,200
the leveling data set

72
00:03:19,200 --> 00:03:22,958
and train student model

73
00:03:23,920 --> 00:03:26,239
the resulting model is

74
00:03:26,239 --> 00:03:28,080
protected

75
00:03:28,080 --> 00:03:29,040
since

76
00:03:29,040 --> 00:03:31,920
the student runs the moderate confidence

77
00:03:31,920 --> 00:03:35,760
effects of public data

78
00:03:35,760 --> 00:03:38,799
and utility is also kept

79
00:03:38,799 --> 00:03:42,959
that is benefit of distillation

80
00:03:43,040 --> 00:03:45,200
cool

81
00:03:45,200 --> 00:03:46,480
but

82
00:03:46,480 --> 00:03:49,120
there is a difficulty

83
00:03:49,120 --> 00:03:52,080
how to obtain public data from the same

84
00:03:52,080 --> 00:03:55,680
domain with private data

85
00:03:56,480 --> 00:03:59,599
we propose a solution

86
00:03:59,599 --> 00:04:01,360
here is a comparison

87
00:04:01,360 --> 00:04:04,319
between bubbles defenses and existing

88
00:04:04,319 --> 00:04:06,720
schemes

89
00:04:06,720 --> 00:04:09,200
proposed defense and state object

90
00:04:09,200 --> 00:04:12,560
defense dmp have the same level of

91
00:04:12,560 --> 00:04:15,599
privacy utility trade-off

92
00:04:15,599 --> 00:04:16,959
nonetheless

93
00:04:16,959 --> 00:04:20,160
proposed defense doesn't require any

94
00:04:20,160 --> 00:04:23,040
public data

95
00:04:25,199 --> 00:04:26,639
okay

96
00:04:26,639 --> 00:04:29,919
to introduce proposed difference we

97
00:04:29,919 --> 00:04:33,520
begin with two basic ideas

98
00:04:33,520 --> 00:04:37,280
first idea using private data again

99
00:04:37,280 --> 00:04:40,840
instead of public data

100
00:04:40,840 --> 00:04:43,919
first train teacher model by training

101
00:04:43,919 --> 00:04:45,919
data

102
00:04:45,919 --> 00:04:48,080
copy training data

103
00:04:48,080 --> 00:04:51,199
use them as alternative of public data

104
00:04:51,199 --> 00:04:54,320
for this duration

105
00:04:54,960 --> 00:04:57,199
but this idea fails

106
00:04:57,199 --> 00:05:01,039
since overfitting remains

107
00:05:01,039 --> 00:05:03,680
students learns extreme confidence

108
00:05:03,680 --> 00:05:08,520
called effects of training data

109
00:05:10,160 --> 00:05:11,919
the second idea

110
00:05:11,919 --> 00:05:15,120
reading a part of training data aside

111
00:05:15,120 --> 00:05:18,880
and treat them as a public data

112
00:05:18,880 --> 00:05:22,479
first split training data

113
00:05:22,479 --> 00:05:26,880
train teacher model by using one part

114
00:05:26,880 --> 00:05:31,440
and train student model by another vert

115
00:05:31,680 --> 00:05:34,880
soft trouble have a moderate confidence

116
00:05:34,880 --> 00:05:39,120
so resulting model is protected

117
00:05:39,120 --> 00:05:40,960
cool

118
00:05:40,960 --> 00:05:42,080
but

119
00:05:42,080 --> 00:05:44,800
inevitably decrease utility

120
00:05:44,800 --> 00:05:50,280
due to small amounts of training data

121
00:05:53,280 --> 00:05:56,800
okay combining the above two ideas

122
00:05:56,800 --> 00:05:58,639
proposes defense

123
00:05:58,639 --> 00:06:01,440
knowledge class distribution is stated

124
00:06:01,440 --> 00:06:04,160
as follows

125
00:06:04,479 --> 00:06:07,840
divided training data set into and

126
00:06:07,840 --> 00:06:11,758
disjoint subsets bi

127
00:06:11,919 --> 00:06:15,600
train teacher models fi by complement

128
00:06:15,600 --> 00:06:18,720
the sets of di

129
00:06:19,440 --> 00:06:23,840
obtain soft levels in phi of x for x in

130
00:06:23,840 --> 00:06:25,919
d

131
00:06:26,080 --> 00:06:30,400
and finally join student model

132
00:06:31,199 --> 00:06:34,880
the resulting model is protected

133
00:06:34,880 --> 00:06:35,680
since

134
00:06:35,680 --> 00:06:38,639
soft scores with moderate confidence

135
00:06:38,639 --> 00:06:41,680
are obtained by data bi

136
00:06:41,680 --> 00:06:44,319
which are not used for training the

137
00:06:44,319 --> 00:06:47,120
teacher models

138
00:06:47,120 --> 00:06:49,680
also keeping utility

139
00:06:49,680 --> 00:06:52,000
since the amount of train

140
00:06:52,000 --> 00:06:54,880
amount of data used to teach training

141
00:06:54,880 --> 00:06:56,400
teacher models

142
00:06:56,400 --> 00:06:59,520
are asymptotic to that of original

143
00:06:59,520 --> 00:07:01,360
training data

144
00:07:01,360 --> 00:07:02,960
as number of

145
00:07:02,960 --> 00:07:07,359
data set partition and increases

146
00:07:09,680 --> 00:07:11,759
we have training

147
00:07:11,759 --> 00:07:14,479
trade-off control for privacy and

148
00:07:14,479 --> 00:07:17,199
utility

149
00:07:17,199 --> 00:07:19,919
the parameter alpha inverse function

150
00:07:19,919 --> 00:07:21,919
controls how much

151
00:07:21,919 --> 00:07:24,960
student runs soft level and

152
00:07:24,960 --> 00:07:27,520
to level

153
00:07:27,919 --> 00:07:29,199
for instance

154
00:07:29,199 --> 00:07:33,039
alpha equals to 1 means full defense

155
00:07:33,039 --> 00:07:38,880
and alpha calls to 0 means unprotected

156
00:07:41,360 --> 00:07:43,120
ok

157
00:07:43,120 --> 00:07:44,560
we conduct

158
00:07:44,560 --> 00:07:48,160
experiments on three benchmark data sets

159
00:07:48,160 --> 00:07:51,919
cipher taxes and purchase

160
00:07:51,919 --> 00:07:54,000
through two metrics

161
00:07:54,000 --> 00:07:57,599
test accuracy and attack accuracy

162
00:07:57,599 --> 00:08:00,720
against two kinds of mia

163
00:08:00,720 --> 00:08:03,759
metric based and not neural net drug

164
00:08:03,759 --> 00:08:06,080
based

165
00:08:08,400 --> 00:08:11,440
here is an overview

166
00:08:11,440 --> 00:08:12,720
firstly

167
00:08:12,720 --> 00:08:16,960
kcd is the best among defense that

168
00:08:16,960 --> 00:08:20,799
doesn't use public dataset

169
00:08:23,440 --> 00:08:26,960
kcd is the only defense achieving high

170
00:08:26,960 --> 00:08:32,079
utility and the low emir risk

171
00:08:32,479 --> 00:08:35,919
secondly kcd is comparable to the

172
00:08:35,919 --> 00:08:38,958
state-of-the-art defense that requires

173
00:08:38,958 --> 00:08:41,200
public deficit

174
00:08:41,200 --> 00:08:44,080
both have a high utility and similar

175
00:08:44,080 --> 00:08:45,839
privacy level

176
00:08:45,839 --> 00:08:49,440
nonetheless kcd doesn't require any

177
00:08:49,440 --> 00:08:52,320
public data

178
00:08:53,680 --> 00:08:56,720
to state more completely we conduct

179
00:08:56,720 --> 00:08:59,760
comparison experiment on various trade

180
00:08:59,760 --> 00:09:02,640
of parameters

181
00:09:02,640 --> 00:09:03,519
here

182
00:09:03,519 --> 00:09:06,480
x-axis means utility

183
00:09:06,480 --> 00:09:10,000
y-axis means a miya risk

184
00:09:10,000 --> 00:09:14,240
so bottom line is believer

185
00:09:14,800 --> 00:09:18,959
but left figure shows kcd is superior to

186
00:09:18,959 --> 00:09:22,399
existing defenses that requires not

187
00:09:22,399 --> 00:09:26,560
using public dataset

188
00:09:26,720 --> 00:09:30,080
as light figures shows kcd is comparable

189
00:09:30,080 --> 00:09:35,880
with tmp that uses public data set

190
00:09:38,399 --> 00:09:42,480
we conduct further experiments to

191
00:09:42,480 --> 00:09:45,839
understand how kcd protects machine

192
00:09:45,839 --> 00:09:48,560
learning models

193
00:09:48,560 --> 00:09:52,000
kcd is composed with two techniques

194
00:09:52,000 --> 00:09:55,200
splitting and using dataset

195
00:09:55,200 --> 00:09:58,000
we examine the cases where one of them

196
00:09:58,000 --> 00:10:00,560
are missing

197
00:10:00,640 --> 00:10:04,800
first scheme we're using dmd

198
00:10:04,800 --> 00:10:06,880
train teacher model

199
00:10:06,880 --> 00:10:09,279
copy training data

200
00:10:09,279 --> 00:10:11,440
you state the percentage of training

201
00:10:11,440 --> 00:10:12,640
data

202
00:10:12,640 --> 00:10:18,079
of them as alternative of public data

203
00:10:18,079 --> 00:10:20,800
this is equivalent to kcd without

204
00:10:20,800 --> 00:10:23,040
splitting technique

205
00:10:23,040 --> 00:10:26,160
and can be seen as a dmp that uses

206
00:10:26,160 --> 00:10:28,959
private data again instead of a public

207
00:10:28,959 --> 00:10:31,959
data

208
00:10:33,920 --> 00:10:37,920
second scheme splitting dmp

209
00:10:37,920 --> 00:10:40,480
split training data with the ratio

210
00:10:40,480 --> 00:10:42,000
tanker

211
00:10:42,000 --> 00:10:45,680
join teacher model by using one part

212
00:10:45,680 --> 00:10:48,160
then training student model by another

213
00:10:48,160 --> 00:10:50,480
part

214
00:10:50,480 --> 00:10:53,440
this is equivalent to kcd without

215
00:10:53,440 --> 00:10:56,160
reusing technique

216
00:10:56,160 --> 00:10:59,440
and can be seen as dmp that leaves a

217
00:10:59,440 --> 00:11:02,079
part of the joining data aside and

218
00:11:02,079 --> 00:11:06,079
treats them as a public data

219
00:11:08,079 --> 00:11:12,160
we conduct experiments on various tables

220
00:11:12,160 --> 00:11:16,079
figures shows kcd is superior to reusing

221
00:11:16,079 --> 00:11:18,880
and splitting dmp

222
00:11:18,880 --> 00:11:21,680
so both splitting and the reusing

223
00:11:21,680 --> 00:11:26,680
techniques are necessary for kcd

224
00:11:28,399 --> 00:11:29,920
conclusion

225
00:11:29,920 --> 00:11:32,800
we proposed noble and mia defense that

226
00:11:32,800 --> 00:11:35,760
does not use public dataset

227
00:11:35,760 --> 00:11:38,240
through splitting and reusing techniques

228
00:11:38,240 --> 00:11:41,519
on prime training dataset

229
00:11:41,519 --> 00:11:44,240
experiments show that kcd is the best

230
00:11:44,240 --> 00:11:46,800
difference among existing scheme that

231
00:11:46,800 --> 00:11:49,279
doesn't use public dataset

232
00:11:49,279 --> 00:11:51,519
and even is comparable to the state of

233
00:11:51,519 --> 00:11:54,320
the art defense that requires public

234
00:11:54,320 --> 00:11:56,240
data

235
00:11:56,240 --> 00:11:57,440
that's all

236
00:11:57,440 --> 00:12:00,680
thank you

