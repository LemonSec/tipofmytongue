1
00:00:00,659 --> 00:00:03,479
hello I'm Vanessa bracamonte and I'm

2
00:00:03,480 --> 00:00:06,420
glad to present today our paper all apps

3
00:00:06,420 --> 00:00:07,319
to this

4
00:00:07,319 --> 00:00:09,900
comparing privacy concerns towards

5
00:00:09,900 --> 00:00:13,080
privacy tools and non-privacy tools for

6
00:00:13,080 --> 00:00:14,940
social media content

7
00:00:14,940 --> 00:00:16,920
This research was conducted in

8
00:00:16,920 --> 00:00:18,840
collaboration with colleagues Agatha

9
00:00:18,840 --> 00:00:21,240
University Frankfurt whom I would like

10
00:00:21,240 --> 00:00:24,799
to thank for their work and support

11
00:00:26,880 --> 00:00:28,920
first I would like to introduce the

12
00:00:28,920 --> 00:00:31,700
background of our research

13
00:00:31,700 --> 00:00:35,160
currently users reveal a considerable

14
00:00:35,160 --> 00:00:37,440
amount of private information about

15
00:00:37,440 --> 00:00:42,739
themselves or others on social media

16
00:00:42,960 --> 00:00:45,600
but unfortunately this of course can

17
00:00:45,600 --> 00:00:48,840
have unintended consequences and users

18
00:00:48,840 --> 00:00:50,940
report that they regret sharing this

19
00:00:50,940 --> 00:00:53,180
information

20
00:00:54,660 --> 00:00:57,180
to help users protect their privacy on

21
00:00:57,180 --> 00:01:00,059
social media researchers have proposed

22
00:01:00,059 --> 00:01:03,120
methods and tools for identifying and

23
00:01:03,120 --> 00:01:05,400
transforming personal information and

24
00:01:05,400 --> 00:01:06,780
content

25
00:01:06,780 --> 00:01:09,180
for example there are proposals for

26
00:01:09,180 --> 00:01:11,580
image data that work by identifying

27
00:01:11,580 --> 00:01:14,700
certain subjects and obscuring them so

28
00:01:14,700 --> 00:01:17,460
that they cannot be recognized and there

29
00:01:17,460 --> 00:01:19,560
are similar proposals for text Data as

30
00:01:19,560 --> 00:01:21,740
well

31
00:01:23,100 --> 00:01:26,220
in general this proposals take some user

32
00:01:26,220 --> 00:01:29,520
data as input analyze it for example

33
00:01:29,520 --> 00:01:31,799
using a machine learning model

34
00:01:31,799 --> 00:01:34,079
and output a prediction or inference

35
00:01:34,079 --> 00:01:36,780
about the content of the user's data

36
00:01:36,780 --> 00:01:39,659
in this General sense these proposals

37
00:01:39,659 --> 00:01:42,600
are not different from for example face

38
00:01:42,600 --> 00:01:45,659
apps that are used for entertainment

39
00:01:45,659 --> 00:01:48,119
except in their beneficial purpose of

40
00:01:48,119 --> 00:01:50,399
enhancing privacy

41
00:01:50,399 --> 00:01:52,799
we know from previous research that

42
00:01:52,799 --> 00:01:54,899
users have privacy concerns towards

43
00:01:54,899 --> 00:01:56,759
services that ask for their personal

44
00:01:56,759 --> 00:01:58,079
data

45
00:01:58,079 --> 00:02:00,719
it is logical that users also have this

46
00:02:00,719 --> 00:02:02,759
concern towards data processing and

47
00:02:02,759 --> 00:02:05,399
processing enhancing tools which also

48
00:02:05,399 --> 00:02:07,920
require access to user data

49
00:02:07,920 --> 00:02:10,619
in studies on tools such as browser

50
00:02:10,619 --> 00:02:13,080
extensions to block third-party tracking

51
00:02:13,080 --> 00:02:15,900
web browsing protection and more

52
00:02:15,900 --> 00:02:17,400
specifically sensitive information

53
00:02:17,400 --> 00:02:20,580
detection users have reported that they

54
00:02:20,580 --> 00:02:22,140
are concerned about their data and

55
00:02:22,140 --> 00:02:24,619
privacy

56
00:02:25,920 --> 00:02:28,500
however we do not know how privacy

57
00:02:28,500 --> 00:02:31,440
concern towards privacy tools compares

58
00:02:31,440 --> 00:02:33,959
the concerned or tools that are not for

59
00:02:33,959 --> 00:02:35,040
privacy

60
00:02:35,040 --> 00:02:37,920
that is how does the priming of a

61
00:02:37,920 --> 00:02:41,400
privacy purpose affect these tools

62
00:02:41,400 --> 00:02:44,220
therefore we investigated the following

63
00:02:44,220 --> 00:02:46,080
questions

64
00:02:46,080 --> 00:02:48,840
is a level of privacy concern towards

65
00:02:48,840 --> 00:02:51,660
privacy tools different from privacy

66
00:02:51,660 --> 00:02:55,579
concern towards non-privacity

67
00:02:55,739 --> 00:02:58,440
are the reasons for privacy concern

68
00:02:58,440 --> 00:03:00,000
different

69
00:03:00,000 --> 00:03:02,940
and since we were also interested in how

70
00:03:02,940 --> 00:03:06,000
these concerns might be reduced we also

71
00:03:06,000 --> 00:03:07,800
investigated the question of whether

72
00:03:07,800 --> 00:03:09,720
there are differences in perception of

73
00:03:09,720 --> 00:03:12,379
assurances

74
00:03:16,140 --> 00:03:19,500
to answer these questions we conducted

75
00:03:19,500 --> 00:03:21,480
an experiment where we showed

76
00:03:21,480 --> 00:03:24,540
participants a mocha of a hypothetical

77
00:03:24,540 --> 00:03:27,900
prototype path for social media content

78
00:03:27,900 --> 00:03:30,959
we manipulated the purpose of the tool

79
00:03:30,959 --> 00:03:34,260
privacy versus a non-privacy tool as

80
00:03:34,260 --> 00:03:37,140
well as the type of data text versus

81
00:03:37,140 --> 00:03:39,798
image

82
00:03:40,920 --> 00:03:43,500
the mock-ups had the same general design

83
00:03:43,500 --> 00:03:46,260
for all conditions and the privacy and

84
00:03:46,260 --> 00:03:48,480
non-privacy conditions were only

85
00:03:48,480 --> 00:03:51,420
different interceded purpose in the

86
00:03:51,420 --> 00:03:53,700
alert message that they showed and in

87
00:03:53,700 --> 00:03:56,298
their output

88
00:03:56,459 --> 00:03:58,980
after showing the mock-up and additional

89
00:03:58,980 --> 00:04:01,799
examples we asked participants questions

90
00:04:01,799 --> 00:04:04,799
about their privacy concern towards the

91
00:04:04,799 --> 00:04:06,720
hypothetical app

92
00:04:06,720 --> 00:04:08,959
we use the mobile users information

93
00:04:08,959 --> 00:04:11,819
privacy concerns scale

94
00:04:11,819 --> 00:04:14,760
which consists of three dimensions

95
00:04:14,760 --> 00:04:18,000
these are perceived surveillance which

96
00:04:18,000 --> 00:04:20,339
is concerned about data being collected

97
00:04:20,339 --> 00:04:23,639
and being tracked and monitored

98
00:04:23,639 --> 00:04:26,639
perceived intrusion which is concerned

99
00:04:26,639 --> 00:04:28,560
about the spread of information and

100
00:04:28,560 --> 00:04:30,720
innovation of privacy

101
00:04:30,720 --> 00:04:33,380
and finally secondary use of information

102
00:04:33,380 --> 00:04:35,580
which is concerned about personal

103
00:04:35,580 --> 00:04:40,280
information being shared and misused

104
00:04:43,440 --> 00:04:45,600
the survey was conducted on Amazon

105
00:04:45,600 --> 00:04:49,320
Mechanical Turk among U.S workers and

106
00:04:49,320 --> 00:04:52,919
our final sample consisted of 185

107
00:04:52,919 --> 00:04:54,600
responses

108
00:04:54,600 --> 00:04:57,600
in general participants had a neutral to

109
00:04:57,600 --> 00:04:59,880
somewhat positive perception of the

110
00:04:59,880 --> 00:05:01,500
hypothetical app

111
00:05:01,500 --> 00:05:04,020
and were somewhat satisfied with the

112
00:05:04,020 --> 00:05:07,199
transformation they saw in the mock-up

113
00:05:07,199 --> 00:05:09,120
and there were no differences between

114
00:05:09,120 --> 00:05:12,120
groups for these characteristics

115
00:05:12,120 --> 00:05:14,820
please refer to the paper for details on

116
00:05:14,820 --> 00:05:17,699
the experiment design and Survey as well

117
00:05:17,699 --> 00:05:22,160
as how we analyze the data we obtained

118
00:05:25,919 --> 00:05:28,500
the results show that yes there was a

119
00:05:28,500 --> 00:05:30,300
difference in the level of concern

120
00:05:30,300 --> 00:05:33,120
related to perceived surveillance

121
00:05:33,120 --> 00:05:35,699
however there were no significant

122
00:05:35,699 --> 00:05:38,300
differences in concern about intrusion

123
00:05:38,300 --> 00:05:41,160
and about secondary use of their

124
00:05:41,160 --> 00:05:43,259
personal information between privacy

125
00:05:43,259 --> 00:05:45,960
tools and non-privacy tools

126
00:05:45,960 --> 00:05:48,539
in addition we found no differences due

127
00:05:48,539 --> 00:05:51,300
to the type of data that is between text

128
00:05:51,300 --> 00:05:54,000
or image

129
00:05:54,000 --> 00:05:56,460
on the other hand we did not identify

130
00:05:56,460 --> 00:05:59,039
any differences in the reasons that

131
00:05:59,039 --> 00:06:00,479
participants gave

132
00:06:00,479 --> 00:06:03,680
for their level of privacy concern

133
00:06:03,680 --> 00:06:06,240
participants refer to the same reasons

134
00:06:06,240 --> 00:06:08,759
for concerns towards the Privacy tool

135
00:06:08,759 --> 00:06:11,639
ask for towards the known privacy tool

136
00:06:11,639 --> 00:06:15,360
such as lack of trust participants own

137
00:06:15,360 --> 00:06:17,699
mindset towards privacy

138
00:06:17,699 --> 00:06:20,160
how much control the participants feel

139
00:06:20,160 --> 00:06:22,560
they have over the app and information

140
00:06:22,560 --> 00:06:24,300
they give

141
00:06:24,300 --> 00:06:26,280
concerns about their data being

142
00:06:26,280 --> 00:06:29,880
collected sold and misused

143
00:06:29,880 --> 00:06:32,400
security risks

144
00:06:32,400 --> 00:06:35,100
how their data would be processed

145
00:06:35,100 --> 00:06:37,440
and a lack of information about the app

146
00:06:37,440 --> 00:06:39,919
itself

147
00:06:41,280 --> 00:06:43,860
the one exception or answers that

148
00:06:43,860 --> 00:06:46,380
indicated that the Privacy objective of

149
00:06:46,380 --> 00:06:49,440
the tool was the reason why participants

150
00:06:49,440 --> 00:06:53,039
concerned about privacy was not so high

151
00:06:53,039 --> 00:06:55,319
but these types of answers were not

152
00:06:55,319 --> 00:06:57,800
frequent

153
00:06:59,759 --> 00:07:03,000
finally the results show no significant

154
00:07:03,000 --> 00:07:06,000
main effect of the type of tool or data

155
00:07:06,000 --> 00:07:07,979
on the perception of different

156
00:07:07,979 --> 00:07:11,099
assurances related to the type of

157
00:07:11,099 --> 00:07:14,460
Provider ads institutional assurances

158
00:07:14,460 --> 00:07:17,639
and how the data was processed

159
00:07:17,639 --> 00:07:20,880
in general assurances related to data

160
00:07:20,880 --> 00:07:21,960
processing

161
00:07:21,960 --> 00:07:24,539
that is that the data would be processed

162
00:07:24,539 --> 00:07:27,360
client-side and that the user data

163
00:07:27,360 --> 00:07:30,300
cannot be traced back to the participant

164
00:07:30,300 --> 00:07:32,520
were the ones that participants thought

165
00:07:32,520 --> 00:07:36,979
would reduce their privacy concerns more

166
00:07:37,259 --> 00:07:41,819
in summary the results indicate that I

167
00:07:41,819 --> 00:07:44,160
participants had the same level of

168
00:07:44,160 --> 00:07:46,620
concern about intrusion and secondary

169
00:07:46,620 --> 00:07:49,440
use of in their information for privacy

170
00:07:49,440 --> 00:07:52,440
tools as for non-privacy tools

171
00:07:52,440 --> 00:07:54,900
and worst participants were more

172
00:07:54,900 --> 00:07:57,060
concerned about being surveilled by

173
00:07:57,060 --> 00:07:58,620
privacy tools

174
00:07:58,620 --> 00:08:01,379
this is important because proposals for

175
00:08:01,379 --> 00:08:04,080
the detection of personal information or

176
00:08:04,080 --> 00:08:06,479
of course rely on the assumption that

177
00:08:06,479 --> 00:08:09,599
there will be access to the user's data

178
00:08:09,599 --> 00:08:12,720
however users may be reluctant to allow

179
00:08:12,720 --> 00:08:16,440
access to their data without assurances

180
00:08:16,440 --> 00:08:18,960
unfortunately proposals for privacy

181
00:08:18,960 --> 00:08:21,539
tools do not often include discussions

182
00:08:21,539 --> 00:08:25,199
on how users data would be protected

183
00:08:25,199 --> 00:08:27,960
or if it's possible to implement privacy

184
00:08:27,960 --> 00:08:30,240
preserving techniques in the development

185
00:08:30,240 --> 00:08:33,360
of these proposals and if so how would

186
00:08:33,360 --> 00:08:35,200
this impact performance

187
00:08:35,200 --> 00:08:36,479
[Music]

188
00:08:36,479 --> 00:08:38,399
in conclusion

189
00:08:38,399 --> 00:08:40,979
we set to investigate whether privacy

190
00:08:40,979 --> 00:08:43,620
concern towards privacy tools for social

191
00:08:43,620 --> 00:08:46,560
media content was different than towards

192
00:08:46,560 --> 00:08:48,720
non-privacy tools

193
00:08:48,720 --> 00:08:51,300
and we found that the answer was yes to

194
00:08:51,300 --> 00:08:52,680
some extent

195
00:08:52,680 --> 00:08:56,160
concern about surveillance was higher on

196
00:08:56,160 --> 00:08:58,140
the other hand the level of privacy

197
00:08:58,140 --> 00:09:00,480
concerned about intrusion and secondary

198
00:09:00,480 --> 00:09:03,180
use of information was not different

199
00:09:03,180 --> 00:09:06,240
from that towards on privacy tools

200
00:09:06,240 --> 00:09:09,480
also we did not identify differences in

201
00:09:09,480 --> 00:09:11,399
the reasons for the higher level of

202
00:09:11,399 --> 00:09:13,320
privacy concern

203
00:09:13,320 --> 00:09:15,600
and we found that participants prefer

204
00:09:15,600 --> 00:09:17,880
the same assurances for privacy and

205
00:09:17,880 --> 00:09:19,260
non-privacy tools

206
00:09:19,260 --> 00:09:21,959
[Music]

207
00:09:21,959 --> 00:09:24,420
in the future we want to of course

208
00:09:24,420 --> 00:09:26,580
address the limitations of the current

209
00:09:26,580 --> 00:09:29,700
research by using a more realistic

210
00:09:29,700 --> 00:09:32,220
scenario and a different set of

211
00:09:32,220 --> 00:09:34,260
participants

212
00:09:34,260 --> 00:09:37,080
and also revisit the question of the

213
00:09:37,080 --> 00:09:39,000
reason for the difference in privacy

214
00:09:39,000 --> 00:09:40,140
concern

215
00:09:40,140 --> 00:09:43,019
that is what is the mechanism through

216
00:09:43,019 --> 00:09:45,959
which privacy concern increases

217
00:09:45,959 --> 00:09:48,420
and if there are antecedent variables

218
00:09:48,420 --> 00:09:51,079
affected

219
00:09:52,019 --> 00:09:55,160
thank you for listening

