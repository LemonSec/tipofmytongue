1
00:00:00,880 --> 00:00:03,600
hello everyone i'm thomas celloni from

2
00:00:03,600 --> 00:00:05,600
the university of mississippi and i'm

3
00:00:05,600 --> 00:00:08,080
here to present you lexus a facial

4
00:00:08,080 --> 00:00:10,320
recognition privacy system that uses

5
00:00:10,320 --> 00:00:12,480
adversarial machine learning

6
00:00:12,480 --> 00:00:14,719
this is a collaborative project with

7
00:00:14,719 --> 00:00:17,279
professors weiwan from chantal liverpool

8
00:00:17,279 --> 00:00:19,920
university and professors charles

9
00:00:19,920 --> 00:00:22,800
fleming and charles walter from the

10
00:00:22,800 --> 00:00:24,960
university of mississippi

11
00:00:24,960 --> 00:00:28,320
let me dive straight into it

12
00:00:28,800 --> 00:00:30,640
state-of-the-art facial recognition

13
00:00:30,640 --> 00:00:33,120
systems have become exceptionally

14
00:00:33,120 --> 00:00:35,680
accurate nowadays um you can think

15
00:00:35,680 --> 00:00:37,920
fistnet achieves 99 plus percent

16
00:00:37,920 --> 00:00:40,960
accuracy in fracture verification tasks

17
00:00:40,960 --> 00:00:42,239
and

18
00:00:42,239 --> 00:00:44,800
so do many other models

19
00:00:44,800 --> 00:00:47,760
however current legislation is falling

20
00:00:47,760 --> 00:00:48,879
behind

21
00:00:48,879 --> 00:00:51,360
mainly because

22
00:00:51,360 --> 00:00:52,879
it needs to be interpreted on a

23
00:00:52,879 --> 00:00:55,600
case-by-case basis and this does not

24
00:00:55,600 --> 00:00:59,039
offer strong privacy guarantees

25
00:00:59,039 --> 00:01:01,440
also data can be moved to different

26
00:01:01,440 --> 00:01:04,400
jurisdictions making it easy to avoid

27
00:01:04,400 --> 00:01:06,799
the law

28
00:01:07,119 --> 00:01:10,080
cameras if you if you think about this

29
00:01:10,080 --> 00:01:12,799
together with the fact that cameras and

30
00:01:12,799 --> 00:01:15,520
video surveillance systems are nowadays

31
00:01:15,520 --> 00:01:16,720
everywhere

32
00:01:16,720 --> 00:01:21,119
this is a strong privacy threat

33
00:01:22,320 --> 00:01:24,640
one solution that researchers have found

34
00:01:24,640 --> 00:01:27,040
is to use adversarial machine learning

35
00:01:27,040 --> 00:01:29,600
which is a set of techniques to reduce

36
00:01:29,600 --> 00:01:32,640
the accuracy of machine learning models

37
00:01:32,640 --> 00:01:35,119
now if the accuracy of a facial

38
00:01:35,119 --> 00:01:37,680
recognition model is reduced then

39
00:01:37,680 --> 00:01:40,240
privacy is promoted

40
00:01:40,240 --> 00:01:42,399
some examples of these recent solutions

41
00:01:42,399 --> 00:01:43,759
include fox

42
00:01:43,759 --> 00:01:46,640
presented in use nx 2020 which is based

43
00:01:46,640 --> 00:01:48,399
on dataset poisoning

44
00:01:48,399 --> 00:01:51,520
and fogicite also based on dataset

45
00:01:51,520 --> 00:01:54,079
poisoning and faceoff which were both

46
00:01:54,079 --> 00:01:56,640
presented in pets last year

47
00:01:56,640 --> 00:01:58,799
now while these are great systems

48
00:01:58,799 --> 00:02:01,119
recent research has shown that data

49
00:02:01,119 --> 00:02:04,000
poisoning often gives us misleading

50
00:02:04,000 --> 00:02:05,520
privacy measures

51
00:02:05,520 --> 00:02:07,759
and we should therefore look elsewhere

52
00:02:07,759 --> 00:02:11,200
for stronger privacy measures

53
00:02:11,200 --> 00:02:12,879
what we propose

54
00:02:12,879 --> 00:02:16,080
with ulexis is a great gradient based

55
00:02:16,080 --> 00:02:19,360
adversarial algorithm to limit privacy

56
00:02:19,360 --> 00:02:22,480
threats in the context of pre-trained

57
00:02:22,480 --> 00:02:24,879
recognition systems visual recognition

58
00:02:24,879 --> 00:02:26,239
systems

59
00:02:26,239 --> 00:02:29,040
its main advantage over literature work

60
00:02:29,040 --> 00:02:31,440
is that it does not require any data

61
00:02:31,440 --> 00:02:32,560
poisoning

62
00:02:32,560 --> 00:02:35,040
it works against models they're not

63
00:02:35,040 --> 00:02:37,200
tampered with basically

64
00:02:37,200 --> 00:02:40,239
it's also fast on relatively mobile

65
00:02:40,239 --> 00:02:43,360
hardware like my business laptop cpu and

66
00:02:43,360 --> 00:02:45,760
provides empirically significant privacy

67
00:02:45,760 --> 00:02:47,599
improvements

68
00:02:47,599 --> 00:02:49,440
not all the shines is good though in

69
00:02:49,440 --> 00:02:52,080
fact privacy gains are dependent on the

70
00:02:52,080 --> 00:02:54,080
transferability of the adversarial

71
00:02:54,080 --> 00:02:57,200
perturbations in black box settings

72
00:02:57,200 --> 00:03:00,159
sometimes we find strong transferability

73
00:03:00,159 --> 00:03:02,239
other times not

74
00:03:02,239 --> 00:03:04,879
they said however we can use

75
00:03:04,879 --> 00:03:07,440
modern assembling to significantly

76
00:03:07,440 --> 00:03:09,280
improve the transferability of these

77
00:03:09,280 --> 00:03:11,360
attacks

78
00:03:11,360 --> 00:03:13,680
let me give you a background on facial

79
00:03:13,680 --> 00:03:17,519
recognition and other cerebellum

80
00:03:17,519 --> 00:03:19,920
facial recognition studies techniques to

81
00:03:19,920 --> 00:03:23,040
extract information from face images

82
00:03:23,040 --> 00:03:25,280
now in practice this means

83
00:03:25,280 --> 00:03:28,159
that we're working with mainly two

84
00:03:28,159 --> 00:03:29,200
tasks

85
00:03:29,200 --> 00:03:32,400
verification which is a binary binary

86
00:03:32,400 --> 00:03:33,599
task

87
00:03:33,599 --> 00:03:35,120
given two images

88
00:03:35,120 --> 00:03:36,879
tell whether they belong to the same

89
00:03:36,879 --> 00:03:38,560
person or not

90
00:03:38,560 --> 00:03:41,360
and identification which uses

91
00:03:41,360 --> 00:03:42,239
a

92
00:03:42,239 --> 00:03:44,720
reference data set with many

93
00:03:44,720 --> 00:03:47,120
images of many people

94
00:03:47,120 --> 00:03:50,400
and given a query image tries to

95
00:03:50,400 --> 00:03:54,080
determine which identity it belongs to

96
00:03:54,080 --> 00:03:55,920
speaking of the adversarial machine

97
00:03:55,920 --> 00:03:57,120
learning

98
00:03:57,120 --> 00:03:59,360
adversarial machine learning is the task

99
00:03:59,360 --> 00:04:03,760
of trying to reduce model utility by

100
00:04:03,760 --> 00:04:06,720
carefully crafting data points

101
00:04:06,720 --> 00:04:08,799
this image taken from

102
00:04:08,799 --> 00:04:12,159
godfallo's famous paper in 2014 shows

103
00:04:12,159 --> 00:04:15,280
you an example of this a model is fooled

104
00:04:15,280 --> 00:04:18,560
into thinking that a panda is instead a

105
00:04:18,560 --> 00:04:20,959
monkey by applying an almost

106
00:04:20,959 --> 00:04:23,919
imperceptible noise mask that from this

107
00:04:23,919 --> 00:04:26,320
video stream you probably cannot even

108
00:04:26,320 --> 00:04:29,320
recognize

109
00:04:30,639 --> 00:04:33,040
let's now talk about the methodology and

110
00:04:33,040 --> 00:04:35,759
the algorithm of you lexus

111
00:04:35,759 --> 00:04:37,919
the threat model that we use

112
00:04:37,919 --> 00:04:40,000
is a game between an attacker and a

113
00:04:40,000 --> 00:04:41,360
defender

114
00:04:41,360 --> 00:04:45,040
the attacker wants to correctly identify

115
00:04:45,040 --> 00:04:48,320
the defender among a small group of

116
00:04:48,320 --> 00:04:49,360
reference

117
00:04:49,360 --> 00:04:52,160
labeled images

118
00:04:52,160 --> 00:04:55,040
while the defender is the end user or

119
00:04:55,040 --> 00:04:57,520
unfortunately product of a facial

120
00:04:57,520 --> 00:04:59,120
recognition service

121
00:04:59,120 --> 00:05:02,960
and wants to quote to perturb their own

122
00:05:02,960 --> 00:05:04,160
images

123
00:05:04,160 --> 00:05:06,560
to make them unrecognizable by the

124
00:05:06,560 --> 00:05:09,039
attacker

125
00:05:09,039 --> 00:05:11,840
the algorithm is a gradient based one

126
00:05:11,840 --> 00:05:13,840
it's a gradient based optimization

127
00:05:13,840 --> 00:05:16,080
problem where we look at the triplet

128
00:05:16,080 --> 00:05:18,000
loss which tells us

129
00:05:18,000 --> 00:05:18,800
how

130
00:05:18,800 --> 00:05:20,400
more similar

131
00:05:20,400 --> 00:05:21,440
a

132
00:05:21,440 --> 00:05:25,360
reference sample a is to a positive

133
00:05:25,360 --> 00:05:28,080
example with the same identity as a

134
00:05:28,080 --> 00:05:31,280
than it is to a negative example with a

135
00:05:31,280 --> 00:05:33,680
different identity from a

136
00:05:33,680 --> 00:05:35,440
and we use a slightly

137
00:05:35,440 --> 00:05:37,520
right version of this triplet loss to

138
00:05:37,520 --> 00:05:39,600
define the cost function of our

139
00:05:39,600 --> 00:05:42,479
optimization problem

140
00:05:42,479 --> 00:05:45,600
so the target of our optimization is to

141
00:05:45,600 --> 00:05:48,960
find a perturbation that when applied on

142
00:05:48,960 --> 00:05:51,360
our reference image a

143
00:05:51,360 --> 00:05:54,160
makes it more similar to the negative

144
00:05:54,160 --> 00:05:56,479
example than it is to the positive

145
00:05:56,479 --> 00:05:59,360
example and therefore it's

146
00:05:59,360 --> 00:06:00,720
it can be

147
00:06:00,720 --> 00:06:03,720
misidentified

148
00:06:04,479 --> 00:06:07,919
a question now is how do we select the

149
00:06:07,919 --> 00:06:12,240
positive and negative examples to use

150
00:06:12,240 --> 00:06:15,280
for simplicity we're going to use

151
00:06:15,280 --> 00:06:18,240
the positive sample

152
00:06:18,240 --> 00:06:19,759
we're going to set the positive sample

153
00:06:19,759 --> 00:06:22,560
to be the same as the query image that

154
00:06:22,560 --> 00:06:24,160
we use

155
00:06:24,160 --> 00:06:26,560
as after all they belong to the same

156
00:06:26,560 --> 00:06:27,919
person

157
00:06:27,919 --> 00:06:30,240
as for the negative example we look at

158
00:06:30,240 --> 00:06:33,440
five different ways to choose this

159
00:06:33,440 --> 00:06:34,800
we look at the

160
00:06:34,800 --> 00:06:37,120
feature space the embedding space of the

161
00:06:37,120 --> 00:06:38,160
images

162
00:06:38,160 --> 00:06:41,840
and we pick either the farthest example

163
00:06:41,840 --> 00:06:44,960
from the query image or an example

164
00:06:44,960 --> 00:06:48,560
random or a combination of the two to

165
00:06:48,560 --> 00:06:50,880
mitigate the chances that

166
00:06:50,880 --> 00:06:52,720
randomly picking an example may not

167
00:06:52,720 --> 00:06:55,840
produce a good result

168
00:06:55,919 --> 00:06:58,560
for sake of completeness we pick the

169
00:06:58,560 --> 00:07:01,520
closest sample in the feature space

170
00:07:01,520 --> 00:07:04,800
and finally we do not pick a negative

171
00:07:04,800 --> 00:07:07,280
example we instead use the

172
00:07:07,280 --> 00:07:09,280
positive example with a slightly

173
00:07:09,280 --> 00:07:11,199
perturbed

174
00:07:11,199 --> 00:07:13,120
with a slightly perturbation applied on

175
00:07:13,120 --> 00:07:16,120
it

176
00:07:17,919 --> 00:07:19,360
now

177
00:07:19,360 --> 00:07:22,000
as for the results first of all here are

178
00:07:22,000 --> 00:07:24,319
the visuals of it

179
00:07:24,319 --> 00:07:26,639
on the first row on the top you can see

180
00:07:26,639 --> 00:07:28,560
some sample images

181
00:07:28,560 --> 00:07:30,639
original uncloaked

182
00:07:30,639 --> 00:07:31,440
and

183
00:07:31,440 --> 00:07:33,199
in the rows below you see how they're

184
00:07:33,199 --> 00:07:36,240
cloaked by different algorithms eulex's

185
00:07:36,240 --> 00:07:39,120
being on the second row

186
00:07:39,120 --> 00:07:40,560
from the video stream you probably

187
00:07:40,560 --> 00:07:42,400
cannot tell the difference between the

188
00:07:42,400 --> 00:07:45,440
original images elixis and fox whereas

189
00:07:45,440 --> 00:07:46,560
face-off

190
00:07:46,560 --> 00:07:51,360
clearly shows significant perturbation

191
00:07:51,759 --> 00:07:56,240
now as for the effect of how we pick the

192
00:07:56,240 --> 00:07:58,479
negative examples

193
00:07:58,479 --> 00:08:00,000
let's look at this

194
00:08:00,000 --> 00:08:02,000
scatter plots

195
00:08:02,000 --> 00:08:05,039
on the top left corner you see

196
00:08:05,039 --> 00:08:08,240
the original uncloaked images

197
00:08:08,240 --> 00:08:10,000
and you can see that they form many

198
00:08:10,000 --> 00:08:13,199
clusters with different colors now

199
00:08:13,199 --> 00:08:15,280
this means that an attacker is able to

200
00:08:15,280 --> 00:08:19,039
correctly identify all these people

201
00:08:19,039 --> 00:08:20,319
all these

202
00:08:20,319 --> 00:08:22,560
images of these people

203
00:08:22,560 --> 00:08:24,800
and in the other scatter plots instead

204
00:08:24,800 --> 00:08:27,759
you see different ways to pick the

205
00:08:27,759 --> 00:08:30,000
negative examples with

206
00:08:30,000 --> 00:08:32,000
no target than random the two in the

207
00:08:32,000 --> 00:08:32,799
middle

208
00:08:32,799 --> 00:08:34,640
being the most

209
00:08:34,640 --> 00:08:38,080
effective in scattering around the

210
00:08:38,080 --> 00:08:40,399
images in the feature space introducing

211
00:08:40,399 --> 00:08:42,719
chaos in the feature map

212
00:08:42,719 --> 00:08:47,120
with the no target approach so using the

213
00:08:47,120 --> 00:08:49,360
as negative example the

214
00:08:49,360 --> 00:08:51,040
positive example with a slight

215
00:08:51,040 --> 00:08:53,279
perturbation applied on it

216
00:08:53,279 --> 00:08:56,560
being the most effective

217
00:08:56,720 --> 00:09:00,720
and this is an effect of the

218
00:09:00,880 --> 00:09:01,680
the

219
00:09:01,680 --> 00:09:04,240
reference data set depth

220
00:09:04,240 --> 00:09:05,040
and

221
00:09:05,040 --> 00:09:07,200
breadth on the

222
00:09:07,200 --> 00:09:11,360
identification accuracy of an attacker

223
00:09:11,360 --> 00:09:14,320
the more identities an attacker has to

224
00:09:14,320 --> 00:09:18,240
choose from the lower is the accuracy

225
00:09:18,240 --> 00:09:22,399
and that's what you see on the ashisa

226
00:09:22,800 --> 00:09:25,680
whereas the three rows tell us how many

227
00:09:25,680 --> 00:09:28,800
samples per each person the attacker has

228
00:09:28,800 --> 00:09:30,080
available

229
00:09:30,080 --> 00:09:32,880
and you can tell that the more

230
00:09:32,880 --> 00:09:36,000
samples are available the more powerful

231
00:09:36,000 --> 00:09:38,080
the more accurate an attacker is in

232
00:09:38,080 --> 00:09:41,040
identifying users however

233
00:09:41,040 --> 00:09:42,959
when we go from left to right

234
00:09:42,959 --> 00:09:45,920
so we applied our cloaks you can see

235
00:09:45,920 --> 00:09:48,800
that the accuracy of the attacker is

236
00:09:48,800 --> 00:09:52,640
significant is consistently reduced

237
00:09:52,640 --> 00:09:56,240
as long as the problem is not trivial

238
00:09:56,240 --> 00:09:58,399
you can tell that with

239
00:09:58,399 --> 00:10:01,680
10 people in the reference data set the

240
00:10:01,680 --> 00:10:07,359
attacker's accuracy is always below 50

241
00:10:08,240 --> 00:10:10,399
now for the applicability or the

242
00:10:10,399 --> 00:10:13,360
transferability of these attacks

243
00:10:13,360 --> 00:10:15,600
on the top row you see the

244
00:10:15,600 --> 00:10:18,959
models that we use to generate the close

245
00:10:18,959 --> 00:10:20,079
and on the

246
00:10:20,079 --> 00:10:22,959
leftmost column you see the models that

247
00:10:22,959 --> 00:10:24,320
we

248
00:10:24,320 --> 00:10:27,920
attack that we try to defend from

249
00:10:28,560 --> 00:10:30,720
generally as has been shown in

250
00:10:30,720 --> 00:10:32,240
literature

251
00:10:32,240 --> 00:10:34,000
similar architectures

252
00:10:34,000 --> 00:10:37,200
transfer adversarial attacks very well

253
00:10:37,200 --> 00:10:38,800
but they do not transfer well across

254
00:10:38,800 --> 00:10:40,079
architectures

255
00:10:40,079 --> 00:10:42,560
so we use ensembling on the leftmost

256
00:10:42,560 --> 00:10:43,920
three columns

257
00:10:43,920 --> 00:10:47,120
to mitigate these negative effects

258
00:10:47,120 --> 00:10:51,440
and craft examples that transfer

259
00:10:51,440 --> 00:10:53,920
acceptably well across different

260
00:10:53,920 --> 00:10:56,719
architectures

261
00:10:57,279 --> 00:11:00,240
let me conclude with a couple of remarks

262
00:11:00,240 --> 00:11:02,240
we've seen that eulexism is an

263
00:11:02,240 --> 00:11:04,480
optimization based adversarial ml

264
00:11:04,480 --> 00:11:05,680
algorithm

265
00:11:05,680 --> 00:11:09,040
that uses an invasion attack to increase

266
00:11:09,040 --> 00:11:11,760
the privacy against invasive facial

267
00:11:11,760 --> 00:11:13,360
recognition systems

268
00:11:13,360 --> 00:11:15,120
and while fast and effective and

269
00:11:15,120 --> 00:11:17,519
controlled experiments its

270
00:11:17,519 --> 00:11:19,440
applicability depends on the

271
00:11:19,440 --> 00:11:21,920
transferability of adversarial examples

272
00:11:21,920 --> 00:11:24,719
across models

273
00:11:24,800 --> 00:11:27,120
and

274
00:11:27,120 --> 00:11:29,920
in adversarial machine learning is a

275
00:11:29,920 --> 00:11:32,720
fast evolving research area and until we

276
00:11:32,720 --> 00:11:34,240
can find

277
00:11:34,240 --> 00:11:37,440
mathematical guarantees

278
00:11:37,440 --> 00:11:39,600
such as differential privacy in the

279
00:11:39,600 --> 00:11:41,440
context of

280
00:11:41,440 --> 00:11:42,800
tabular data

281
00:11:42,800 --> 00:11:47,519
to ensure privacy in the computer vision

282
00:11:47,519 --> 00:11:49,839
this remains one of the most promising

283
00:11:49,839 --> 00:11:51,519
research fields to

284
00:11:51,519 --> 00:11:54,000
to promote privacy preserving machine

285
00:11:54,000 --> 00:11:56,560
learning applications

286
00:11:56,560 --> 00:11:58,399
this is the end of the presentation

287
00:11:58,399 --> 00:12:02,279
thank you for staying with me

