1
00:00:00,960 --> 00:00:03,439
hello i'm kanov today i'll be presenting

2
00:00:03,439 --> 00:00:05,839
lama a low latency math library for

3
00:00:05,839 --> 00:00:07,919
secure influence this is a joint work

4
00:00:07,919 --> 00:00:10,320
with deepak kumaraswamy nishan chandan

5
00:00:10,320 --> 00:00:12,320
and

6
00:00:12,320 --> 00:00:14,960
let's understand the problem of secure

7
00:00:14,960 --> 00:00:15,679
secure inference

8
00:00:15,679 --> 00:00:17,520
a server starts with a model m and

9
00:00:17,520 --> 00:00:19,520
private weights w and the client starts

10
00:00:19,520 --> 00:00:21,199
with the model input x

11
00:00:21,199 --> 00:00:22,880
we want the client to only learn the

12
00:00:22,880 --> 00:00:25,199
inference output m of w comma x while

13
00:00:25,199 --> 00:00:27,439
the server learns nothing about x this

14
00:00:27,439 --> 00:00:29,039
problem can be solved using

15
00:00:29,039 --> 00:00:30,560
cryptographic technique of secure

16
00:00:30,560 --> 00:00:32,399
computation or more specifically

17
00:00:32,399 --> 00:00:34,640
two-party secure computation to pc for

18
00:00:34,640 --> 00:00:36,559
short there has been much work on

19
00:00:36,559 --> 00:00:38,000
improving the efficiency of these

20
00:00:38,000 --> 00:00:40,239
protocols recently

21
00:00:40,239 --> 00:00:41,840
the biggest bottleneck towards that

22
00:00:41,840 --> 00:00:43,760
option of secure inference protocols is

23
00:00:43,760 --> 00:00:45,760
the high communication cost associated

24
00:00:45,760 --> 00:00:46,719
with them

25
00:00:46,719 --> 00:00:48,800
consider a simple seven layer cnn

26
00:00:48,800 --> 00:00:51,680
oversafe art and data set as an example

27
00:00:51,680 --> 00:00:53,760
even for this very simple model crypt

28
00:00:53,760 --> 00:00:56,640
flow to the current state of the 2pc cnn

29
00:00:56,640 --> 00:00:59,359
inference library costs 340 mb of total

30
00:00:59,359 --> 00:01:01,600
communication 10 seconds of runtime in

31
00:01:01,600 --> 00:01:04,239
345 communication rounds

32
00:01:04,239 --> 00:01:06,000
delphi focuses on moving the bulk of

33
00:01:06,000 --> 00:01:08,000
total communication cost to the input

34
00:01:08,000 --> 00:01:10,400
independent offline phase this model

35
00:01:10,400 --> 00:01:12,240
also known as pre-processing model is

36
00:01:12,240 --> 00:01:14,400
very interesting as the parties can

37
00:01:14,400 --> 00:01:16,560
pre-compute and store some input

38
00:01:16,560 --> 00:01:19,200
independent key material beforehand and

39
00:01:19,200 --> 00:01:21,040
when they have their inputs ready they

40
00:01:21,040 --> 00:01:23,119
can perform inference faster

41
00:01:23,119 --> 00:01:25,680
even then delphi communicates 196 mb in

42
00:01:25,680 --> 00:01:27,759
the online phase in 2.7 seconds of

43
00:01:27,759 --> 00:01:29,759
runtime

44
00:01:29,759 --> 00:01:32,560
furthermore consider an example of rnn

45
00:01:32,560 --> 00:01:35,439
on google 30 dataset here the situation

46
00:01:35,439 --> 00:01:37,439
is even worse due to mathematical

47
00:01:37,439 --> 00:01:39,600
functions like sigmoid and 10h which are

48
00:01:39,600 --> 00:01:41,840
expensive to calculate securely

49
00:01:41,840 --> 00:01:43,520
siren the current state-of-the-art

50
00:01:43,520 --> 00:01:45,680
inference library with support for such

51
00:01:45,680 --> 00:01:47,439
precise math functions

52
00:01:47,439 --> 00:01:50,079
communicates 415 mb of total data and

53
00:01:50,079 --> 00:01:52,880
runs in 37 seconds in nearly 60 000

54
00:01:52,880 --> 00:01:53,680
nodes

55
00:01:53,680 --> 00:01:56,079
in our work lama we use a technique of

56
00:01:56,079 --> 00:01:57,920
function secret sharing to reduce the

57
00:01:57,920 --> 00:02:00,560
online cost in the pre-processing model

58
00:02:00,560 --> 00:02:02,799
we achieve up to 48 x reduction in

59
00:02:02,799 --> 00:02:05,680
communication and runs 19x faster in

60
00:02:05,680 --> 00:02:07,680
these examples

61
00:02:07,680 --> 00:02:09,280
let me draw out the structure of the

62
00:02:09,280 --> 00:02:10,479
talk first

63
00:02:10,479 --> 00:02:12,480
we start with the brief description of

64
00:02:12,480 --> 00:02:14,640
function secret sharing and 2pc using

65
00:02:14,640 --> 00:02:16,400
function secret sharing

66
00:02:16,400 --> 00:02:18,000
then we discuss the drawbacks of

67
00:02:18,000 --> 00:02:20,239
existing works and how lama addresses

68
00:02:20,239 --> 00:02:21,280
those

69
00:02:21,280 --> 00:02:23,520
we start the technical part of our talk

70
00:02:23,520 --> 00:02:26,080
with bit with changing gates then we

71
00:02:26,080 --> 00:02:28,400
discuss how we create protocols for

72
00:02:28,400 --> 00:02:30,160
mathematical functions using spline

73
00:02:30,160 --> 00:02:31,760
approximations

74
00:02:31,760 --> 00:02:34,000
finally we discuss the benchmarks we ran

75
00:02:34,000 --> 00:02:36,640
using lama and how it compares with

76
00:02:36,640 --> 00:02:38,400
other works

77
00:02:38,400 --> 00:02:41,040
fss was introduced in 2015 by the work

78
00:02:41,040 --> 00:02:43,200
of boyle gilboa and ishai

79
00:02:43,200 --> 00:02:45,920
it allows a dealer to split a function f

80
00:02:45,920 --> 00:02:48,560
into two functions f 0 and f 1 such that

81
00:02:48,560 --> 00:02:50,879
when applied to the same input x the two

82
00:02:50,879 --> 00:02:53,760
functions return secret shares of f x

83
00:02:53,760 --> 00:02:55,519
the security requirement states that

84
00:02:55,519 --> 00:02:58,400
each of the f 0 and f 1 hides f

85
00:02:58,400 --> 00:03:01,680
these f 0 and f 1 are called fss keys

86
00:03:01,680 --> 00:03:03,599
for this to be practical these keys

87
00:03:03,599 --> 00:03:07,120
should be also compact in size

88
00:03:07,120 --> 00:03:10,239
in 2019 boyle gilboa nhi provided a way

89
00:03:10,239 --> 00:03:12,800
to realize 2 pc interested dealer model

90
00:03:12,800 --> 00:03:15,440
using fss in this the dealer provide

91
00:03:15,440 --> 00:03:17,519
input independent correlated randomness

92
00:03:17,519 --> 00:03:20,000
to the two parties this dealer can also

93
00:03:20,000 --> 00:03:23,120
be emulated as an expensive two pc

94
00:03:23,120 --> 00:03:25,760
then using this key material round and

95
00:03:25,760 --> 00:03:27,920
online communication optimal protocols

96
00:03:27,920 --> 00:03:29,599
for many functionalities are known due

97
00:03:29,599 --> 00:03:31,040
to previous works

98
00:03:31,040 --> 00:03:32,799
these functionalities are called fss

99
00:03:32,799 --> 00:03:33,760
gates

100
00:03:33,760 --> 00:03:36,000
each fss gate costs online communication

101
00:03:36,000 --> 00:03:37,760
of two times the output bit length of

102
00:03:37,760 --> 00:03:40,319
the functionality that is each property

103
00:03:40,319 --> 00:03:42,000
sends an output element to the other one

104
00:03:42,000 --> 00:03:44,319
in just a single round

105
00:03:44,319 --> 00:03:45,840
there are number of works on secure

106
00:03:45,840 --> 00:03:49,440
computation using fss like bj 19 and bcg

107
00:03:49,440 --> 00:03:51,760
plus 21. ariane is a work which

108
00:03:51,760 --> 00:03:53,280
considered the problem of secure

109
00:03:53,280 --> 00:03:56,159
inference using fss but these existing

110
00:03:56,159 --> 00:03:59,680
fss based solution had some drawbacks

111
00:03:59,680 --> 00:04:01,760
first all of them lacked support for

112
00:04:01,760 --> 00:04:04,319
precise math functions like sigmoid and

113
00:04:04,319 --> 00:04:06,959
h as we saw rnns make heavy use of these

114
00:04:06,959 --> 00:04:08,799
functions and to support them we need

115
00:04:08,799 --> 00:04:10,959
protocols to securely evaluate this math

116
00:04:10,959 --> 00:04:13,120
function due to transcendental nature of

117
00:04:13,120 --> 00:04:15,439
these functions exact evaluation is not

118
00:04:15,439 --> 00:04:17,680
possible in finite bit width and error

119
00:04:17,680 --> 00:04:20,320
bound is measured in terms of alps error

120
00:04:20,320 --> 00:04:22,479
there are some works in secure inference

121
00:04:22,479 --> 00:04:25,120
which like secure ml which provide math

122
00:04:25,120 --> 00:04:27,360
functions but they suffer from high alps

123
00:04:27,360 --> 00:04:28,479
error

124
00:04:28,479 --> 00:04:29,919
repeated use of these high error

125
00:04:29,919 --> 00:04:31,759
implementations can lead to wrong output

126
00:04:31,759 --> 00:04:33,840
with a large probability

127
00:04:33,840 --> 00:04:36,320
second most existing mpc frameworks

128
00:04:36,320 --> 00:04:38,720
don't support mixed bit with operations

129
00:04:38,720 --> 00:04:40,880
while most ml models perform inference

130
00:04:40,880 --> 00:04:43,600
on floating point numbers mpc frameworks

131
00:04:43,600 --> 00:04:45,759
operate on fixed point numbers

132
00:04:45,759 --> 00:04:48,080
existing state of the art flow to fixed

133
00:04:48,080 --> 00:04:49,680
quantizers like shift tree and

134
00:04:49,680 --> 00:04:52,240
tensorflow lite use low bit width fixed

135
00:04:52,240 --> 00:04:54,000
point representations for efficiency

136
00:04:54,000 --> 00:04:55,040
reasons

137
00:04:55,040 --> 00:04:57,440
simple computations like multiplication

138
00:04:57,440 --> 00:04:59,120
required increased bit width of the

139
00:04:59,120 --> 00:05:01,360
intermediate output variable to prevent

140
00:05:01,360 --> 00:05:03,680
overflows without support for mixed bit

141
00:05:03,680 --> 00:05:06,240
width operations the existing wave works

142
00:05:06,240 --> 00:05:08,000
are forced to operate on higher bit

143
00:05:08,000 --> 00:05:11,199
width resulting in bad performance

144
00:05:11,199 --> 00:05:14,000
finally the only prior fss based secure

145
00:05:14,000 --> 00:05:17,120
inference library aryan uses protocols

146
00:05:17,120 --> 00:05:18,800
for relu and truncation that are only

147
00:05:18,800 --> 00:05:20,800
probabilistically correct since the

148
00:05:20,800 --> 00:05:22,479
error is inversely proportional to the

149
00:05:22,479 --> 00:05:24,080
bit with use they are forced to use

150
00:05:24,080 --> 00:05:26,320
higher bit width also as the number of

151
00:05:26,320 --> 00:05:28,240
relu and truncation increases the

152
00:05:28,240 --> 00:05:30,240
probability of overall computation being

153
00:05:30,240 --> 00:05:32,880
incorrect increases

154
00:05:32,880 --> 00:05:35,600
in lama we address all these drawbacks

155
00:05:35,600 --> 00:05:38,080
first lama supports precise computation

156
00:05:38,080 --> 00:05:40,240
over low between inputs by supporting

157
00:05:40,240 --> 00:05:42,080
mixed bit with operations to calculate

158
00:05:42,080 --> 00:05:44,160
intermediate variables in higher bit

159
00:05:44,160 --> 00:05:45,039
width

160
00:05:45,039 --> 00:05:47,759
second we provide a low bit width spline

161
00:05:47,759 --> 00:05:49,919
approximation for math functions with a

162
00:05:49,919 --> 00:05:52,639
maximum error of 4 alps and provide fss

163
00:05:52,639 --> 00:05:54,800
protocols for these

164
00:05:54,800 --> 00:05:57,039
third unlike aryan our protocols for

165
00:05:57,039 --> 00:05:58,960
railway and truncation are correct

166
00:05:58,960 --> 00:06:00,720
lower comparisons are taken from the

167
00:06:00,720 --> 00:06:03,440
previous work of bcg plus 21 and we use

168
00:06:03,440 --> 00:06:05,759
them to build secure protocols for

169
00:06:05,759 --> 00:06:08,240
maximal average pool and arc banks

170
00:06:08,240 --> 00:06:10,080
finally we integrate lama as a

171
00:06:10,080 --> 00:06:12,160
cryptographic backend to easy pc

172
00:06:12,160 --> 00:06:13,919
creating a complete end-to-end infill

173
00:06:13,919 --> 00:06:16,318
system

174
00:06:16,800 --> 00:06:18,639
distributed comparison function

175
00:06:18,639 --> 00:06:21,440
abbreviated dcf is a very fundamental fs

176
00:06:21,440 --> 00:06:24,400
scheme to perform comparisons here the

177
00:06:24,400 --> 00:06:27,039
function returns beta when x is less

178
00:06:27,039 --> 00:06:29,199
than alpha and 0 otherwise

179
00:06:29,199 --> 00:06:32,479
the fss key for dcf hides these alpha

180
00:06:32,479 --> 00:06:35,440
and beta parameters we call this fss key

181
00:06:35,440 --> 00:06:37,039
dcf key

182
00:06:37,039 --> 00:06:39,600
bcg plus 21 provided an optimized

183
00:06:39,600 --> 00:06:42,880
construction for dcf

184
00:06:42,880 --> 00:06:44,880
we start our technical discussion with a

185
00:06:44,880 --> 00:06:47,039
brief description of bit with changing

186
00:06:47,039 --> 00:06:49,680
fss gates as the name suggests they

187
00:06:49,680 --> 00:06:51,360
change the bit width and scale of fixed

188
00:06:51,360 --> 00:06:52,720
point numbers

189
00:06:52,720 --> 00:06:54,479
first we have an extension gate which

190
00:06:54,479 --> 00:06:56,400
increases the bit width over number

191
00:06:56,400 --> 00:06:58,560
second we have a truncate reduce gate

192
00:06:58,560 --> 00:07:00,639
which reduces both the scale and bit

193
00:07:00,639 --> 00:07:03,199
width of the number both of them need

194
00:07:03,199 --> 00:07:06,160
one dcf key and since they are fss gates

195
00:07:06,160 --> 00:07:09,120
they take one round of communication

196
00:07:09,120 --> 00:07:11,199
as we saw earlier we need math functions

197
00:07:11,199 --> 00:07:13,599
like sigmoid and tan a to support rns

198
00:07:13,599 --> 00:07:15,360
siren the state of the art prior work

199
00:07:15,360 --> 00:07:17,520
which support precise math functions

200
00:07:17,520 --> 00:07:19,840
uses gold smith's iteration to estimate

201
00:07:19,840 --> 00:07:20,880
this function

202
00:07:20,880 --> 00:07:22,560
but this approach is not ideal in

203
00:07:22,560 --> 00:07:24,479
emphasis as it would require a round for

204
00:07:24,479 --> 00:07:27,120
each iteration our first design choice

205
00:07:27,120 --> 00:07:29,360
is to use piecewise polynomial also

206
00:07:29,360 --> 00:07:30,800
known as splines

207
00:07:30,800 --> 00:07:33,520
approximation as we have fss gate for it

208
00:07:33,520 --> 00:07:35,440
due to previous work let's understand

209
00:07:35,440 --> 00:07:38,400
how the evaluation of splines work

210
00:07:38,400 --> 00:07:41,039
consider a six piece line as shown we

211
00:07:41,039 --> 00:07:45,199
want to evaluate input x on this spline

212
00:07:45,199 --> 00:07:47,039
we first find the spline interval in

213
00:07:47,039 --> 00:07:48,960
which x lies and fetch the corresponding

214
00:07:48,960 --> 00:07:50,960
polynomial coefficients this step

215
00:07:50,960 --> 00:07:53,120
requires comparisons

216
00:07:53,120 --> 00:07:54,800
and then we simply apply this polynomial

217
00:07:54,800 --> 00:07:56,960
to the input x this step requires

218
00:07:56,960 --> 00:07:59,120
multiplications

219
00:07:59,120 --> 00:08:01,840
bcg plus 21 divides an fss gate for

220
00:08:01,840 --> 00:08:04,240
uniform bit spline which means that all

221
00:08:04,240 --> 00:08:06,080
input and output numbers and all the

222
00:08:06,080 --> 00:08:07,199
intermediate

223
00:08:07,199 --> 00:08:09,840
operations happen in the same bit width

224
00:08:09,840 --> 00:08:12,240
they were able to achieve this in using

225
00:08:12,240 --> 00:08:14,560
a single dcf key

226
00:08:14,560 --> 00:08:17,199
but suppose we want to apply spline on a

227
00:08:17,199 --> 00:08:19,039
low bit with input

228
00:08:19,039 --> 00:08:21,199
if we use the spline of the same old bit

229
00:08:21,199 --> 00:08:23,039
width we fear overflows due to

230
00:08:23,039 --> 00:08:24,800
multiplications which would eventually

231
00:08:24,800 --> 00:08:26,720
lead to high alps error in math

232
00:08:26,720 --> 00:08:28,319
functions

233
00:08:28,319 --> 00:08:30,400
using spline of bit with just enough to

234
00:08:30,400 --> 00:08:32,799
prevent overflows is bad as well because

235
00:08:32,799 --> 00:08:34,559
it leads to high key size and large

236
00:08:34,559 --> 00:08:36,479
runtime

237
00:08:36,479 --> 00:08:39,440
hence we need mix between splines where

238
00:08:39,440 --> 00:08:42,000
the intermediate multiplications haven't

239
00:08:42,000 --> 00:08:43,519
happen over large bit width through

240
00:08:43,519 --> 00:08:46,320
prevent overflows

241
00:08:46,320 --> 00:08:48,560
now to quickly devise a protocol for

242
00:08:48,560 --> 00:08:51,120
mixed bit with spline one could use fss

243
00:08:51,120 --> 00:08:53,440
gate for extension to extend the input

244
00:08:53,440 --> 00:08:55,279
value to bit width just enough to

245
00:08:55,279 --> 00:08:56,959
prevent overloads

246
00:08:56,959 --> 00:08:59,040
and then pass this extended value to the

247
00:08:59,040 --> 00:09:02,240
uniform bit with spline from bcg plus 21

248
00:09:02,240 --> 00:09:05,120
and finally adjust the output to decide

249
00:09:05,120 --> 00:09:07,920
bit width and scale using truncation

250
00:09:07,920 --> 00:09:09,760
this serves as a good starting point on

251
00:09:09,760 --> 00:09:13,760
which we now present two optimizations

252
00:09:14,000 --> 00:09:16,080
our first optimization is based on the

253
00:09:16,080 --> 00:09:18,320
observation that the coefficient finding

254
00:09:18,320 --> 00:09:20,480
step does not need to

255
00:09:20,480 --> 00:09:22,560
the input to be signed extended

256
00:09:22,560 --> 00:09:24,720
comparisons can be done in original

257
00:09:24,720 --> 00:09:26,480
smaller bit width

258
00:09:26,480 --> 00:09:28,800
so we pass the original smaller bit with

259
00:09:28,800 --> 00:09:31,440
input in the coefficient finding part

260
00:09:31,440 --> 00:09:33,680
and use the sign expanded input only in

261
00:09:33,680 --> 00:09:35,839
the polynomial evaluation hence we

262
00:09:35,839 --> 00:09:37,920
reduce the dcf key size

263
00:09:37,920 --> 00:09:40,160
in case of sigmoid this leads to forex

264
00:09:40,160 --> 00:09:42,720
reduction in key size

265
00:09:42,720 --> 00:09:44,959
our second optimization is based on the

266
00:09:44,959 --> 00:09:47,360
observation that most of the output of

267
00:09:47,360 --> 00:09:49,920
dcf evaluation in the splined protocol

268
00:09:49,920 --> 00:09:52,000
it discarded and only some part of it is

269
00:09:52,000 --> 00:09:53,519
used

270
00:09:53,519 --> 00:09:55,279
here is the original protocol for

271
00:09:55,279 --> 00:09:59,519
uniform between spline from bcg plus 21.

272
00:09:59,519 --> 00:10:01,839
to quickly verify this claim observe

273
00:10:01,839 --> 00:10:04,800
that we are generating m squared values

274
00:10:04,800 --> 00:10:07,680
here above while we use only 2m of these

275
00:10:07,680 --> 00:10:09,040
values here

276
00:10:09,040 --> 00:10:11,279
here m denotes the number of intervals

277
00:10:11,279 --> 00:10:13,519
in the spline

278
00:10:13,519 --> 00:10:15,519
to prevent this wastage we found a way

279
00:10:15,519 --> 00:10:18,000
to partially evaluate dcf leading to

280
00:10:18,000 --> 00:10:20,480
reduction of prg calls by a factor

281
00:10:20,480 --> 00:10:23,440
roughly of m by 2. in sigmoid this

282
00:10:23,440 --> 00:10:25,760
factor is roughly equal to 10x the best

283
00:10:25,760 --> 00:10:27,360
part of all this

284
00:10:27,360 --> 00:10:29,360
is that both of these optimizations are

285
00:10:29,360 --> 00:10:31,519
composable with each other overall we

286
00:10:31,519 --> 00:10:33,839
get a forex reduction in key size 4x

287
00:10:33,839 --> 00:10:36,320
reduction in dealer prg cost and 40x

288
00:10:36,320 --> 00:10:38,240
reduction in evaluated prg costs in

289
00:10:38,240 --> 00:10:40,160
sigmoid

290
00:10:40,160 --> 00:10:42,079
we also propose many fss protocols for

291
00:10:42,079 --> 00:10:43,760
functionalities in inference like

292
00:10:43,760 --> 00:10:46,240
convolution relu and max pool but we

293
00:10:46,240 --> 00:10:47,839
skipped that discussion due to shortage

294
00:10:47,839 --> 00:10:50,240
of time

295
00:10:50,320 --> 00:10:52,959
we evaluated lama on a number of

296
00:10:52,959 --> 00:10:54,959
benchmarks and compared with existing

297
00:10:54,959 --> 00:10:57,519
works we ran these benchmarks on both

298
00:10:57,519 --> 00:11:00,079
lan and van setting we present some of

299
00:11:00,079 --> 00:11:03,920
those numbers here in lan setting

300
00:11:03,920 --> 00:11:06,640
we compared the fast grnn model on

301
00:11:06,640 --> 00:11:09,920
google 30 dataset with siren and observe

302
00:11:09,920 --> 00:11:13,600
48x less communication 19x less runtime

303
00:11:13,600 --> 00:11:16,800
and 22x less communication rounds

304
00:11:16,800 --> 00:11:18,959
we compared our work with delphi which

305
00:11:18,959 --> 00:11:21,200
focused on online offline split on a

306
00:11:21,200 --> 00:11:24,720
simple 7 layer cnn we observed 23x less

307
00:11:24,720 --> 00:11:28,160
communication and 5x less runtime

308
00:11:28,160 --> 00:11:30,720
finally we compare with aryan on

309
00:11:30,720 --> 00:11:34,000
resnet18 model on hymanoptera dataset

310
00:11:34,000 --> 00:11:35,839
note that we don't have math functions

311
00:11:35,839 --> 00:11:37,200
in this benchmark

312
00:11:37,200 --> 00:11:38,959
here even in the presence of correct

313
00:11:38,959 --> 00:11:41,600
relevant truncation protocols in lama we

314
00:11:41,600 --> 00:11:44,560
achieved 3x less communication in 1.7x

315
00:11:44,560 --> 00:11:46,720
less runtime we attribute this

316
00:11:46,720 --> 00:11:48,640
improvement due to relu being a two

317
00:11:48,640 --> 00:11:51,040
round protocol in arya compared to an

318
00:11:51,040 --> 00:11:53,600
fss gate in llama

319
00:11:53,600 --> 00:11:55,760
this marks the end of matter in

320
00:11:55,760 --> 00:11:58,160
conclusion lama proposes an fsf-based

321
00:11:58,160 --> 00:12:00,240
solution to the 2pc secure inference

322
00:12:00,240 --> 00:12:02,480
problem in pre-processing model

323
00:12:02,480 --> 00:12:04,399
while lama aims to reduce the online

324
00:12:04,399 --> 00:12:06,639
complexity it remains an interesting

325
00:12:06,639 --> 00:12:08,480
problem to reduce the large offline

326
00:12:08,480 --> 00:12:10,480
communication cost mainly due to large

327
00:12:10,480 --> 00:12:12,000
dcf key size

328
00:12:12,000 --> 00:12:14,399
reducing the size of dcf keys can be an

329
00:12:14,399 --> 00:12:16,639
interesting problem with impact greater

330
00:12:16,639 --> 00:12:19,120
than just secure influence

331
00:12:19,120 --> 00:12:22,360
thank you

