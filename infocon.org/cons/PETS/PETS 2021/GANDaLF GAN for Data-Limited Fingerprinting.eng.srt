1
00:00:00,399 --> 00:00:03,040
hello everyone i'm nate matthews and

2
00:00:03,040 --> 00:00:04,880
today i will be presenting our work on a

3
00:00:04,880 --> 00:00:06,399
new website concluding attack

4
00:00:06,399 --> 00:00:09,519
named gandalf this work was led by dr se

5
00:00:09,519 --> 00:00:11,280
uno from the university of minnesota

6
00:00:11,280 --> 00:00:14,000
while she was still pursuing her phd

7
00:00:14,000 --> 00:00:15,920
in this project she was joined by myself

8
00:00:15,920 --> 00:00:17,119
muhammad said rahman

9
00:00:17,119 --> 00:00:19,199
and dr matthew wright the rochester

10
00:00:19,199 --> 00:00:21,439
institute of technology

11
00:00:21,439 --> 00:00:23,920
um this is alongside uh our advisor dr

12
00:00:23,920 --> 00:00:26,640
nicholas stocker

13
00:00:27,039 --> 00:00:28,320
since many of you are likely already

14
00:00:28,320 --> 00:00:30,480
familiar with tor i'll try to keep this

15
00:00:30,480 --> 00:00:31,519
brief

16
00:00:31,519 --> 00:00:33,600
por is a widely used anonymity system

17
00:00:33,600 --> 00:00:34,960
for protecting one's privacy and

18
00:00:34,960 --> 00:00:36,880
circumventing censorship

19
00:00:36,880 --> 00:00:38,640
to defend its users tor inc

20
00:00:38,640 --> 00:00:40,399
incrementally creates a circuit

21
00:00:40,399 --> 00:00:41,040
containing

22
00:00:41,040 --> 00:00:43,520
a chain of three nodes that includes the

23
00:00:43,520 --> 00:00:46,160
guard the middle and the exit relay

24
00:00:46,160 --> 00:00:47,920
this circuit is encrypted and extended

25
00:00:47,920 --> 00:00:50,079
one hop at a time and consequently no

26
00:00:50,079 --> 00:00:51,760
individual node has a complete path

27
00:00:51,760 --> 00:00:52,960
information

28
00:00:52,960 --> 00:00:54,800
this means that each particular node is

29
00:00:54,800 --> 00:00:55,920
only aware of the nodes directly

30
00:00:55,920 --> 00:00:57,680
adjacent to itself

31
00:00:57,680 --> 00:01:00,079
so if for an example an attacker is

32
00:01:00,079 --> 00:01:01,920
monitoring the traffic between the user

33
00:01:01,920 --> 00:01:03,039
and the guard node

34
00:01:03,039 --> 00:01:04,640
the doctor will know the identities of

35
00:01:04,640 --> 00:01:06,240
the user and the guard but not the

36
00:01:06,240 --> 00:01:07,840
identity of the site that the user is

37
00:01:07,840 --> 00:01:09,200
visiting

38
00:01:09,200 --> 00:01:11,520
in this way user ana anonymity is

39
00:01:11,520 --> 00:01:13,200
preserved while browsing the internet on

40
00:01:13,200 --> 00:01:15,200
form

41
00:01:15,200 --> 00:01:16,799
unfortunately there's a vantage point

42
00:01:16,799 --> 00:01:19,200
here that an attacker can exploit

43
00:01:19,200 --> 00:01:20,880
in our attack scenario the adversary

44
00:01:20,880 --> 00:01:22,320
knows the identity of the client

45
00:01:22,320 --> 00:01:24,000
but not the website as its projector

46
00:01:24,000 --> 00:01:25,759
partor however

47
00:01:25,759 --> 00:01:27,600
there is a leak of information within

48
00:01:27,600 --> 00:01:29,040
the metadata produced by the network

49
00:01:29,040 --> 00:01:30,159
traffic

50
00:01:30,159 --> 00:01:32,240
this information is relatively distinct

51
00:01:32,240 --> 00:01:34,240
for each website

52
00:01:34,240 --> 00:01:36,320
so an attacker can use this information

53
00:01:36,320 --> 00:01:38,079
to train a classifier to predict what

54
00:01:38,079 --> 00:01:40,799
website a user has visited

55
00:01:40,799 --> 00:01:42,560
such an adversary can carry out this

56
00:01:42,560 --> 00:01:44,560
attack by passively eavesdropping on the

57
00:01:44,560 --> 00:01:45,040
client

58
00:01:45,040 --> 00:01:46,240
in such a way that the client is

59
00:01:46,240 --> 00:01:48,079
completely unaware that the privacy of

60
00:01:48,079 --> 00:01:50,720
their communication has been compromised

61
00:01:50,720 --> 00:01:52,240
and so this is what we call a website

62
00:01:52,240 --> 00:01:55,439
fingerprint attack

63
00:01:55,439 --> 00:01:57,040
now over the years there have been many

64
00:01:57,040 --> 00:01:59,200
website fingerprint attack proposals

65
00:01:59,200 --> 00:02:01,200
early attacks such as kamal by panchenko

66
00:02:01,200 --> 00:02:03,040
at all and k favorite printing by hazel

67
00:02:03,040 --> 00:02:04,960
process website traces into handcrafted

68
00:02:04,960 --> 00:02:06,159
teachers

69
00:02:06,159 --> 00:02:07,520
these attacks use standard machine

70
00:02:07,520 --> 00:02:09,280
learning classifiers such as support

71
00:02:09,280 --> 00:02:10,080
vector machines

72
00:02:10,080 --> 00:02:12,000
random force and the k-near favorite

73
00:02:12,000 --> 00:02:13,840
algorithm to learn to predict websites

74
00:02:13,840 --> 00:02:15,840
using these features

75
00:02:15,840 --> 00:02:17,920
in general these works show that the

76
00:02:17,920 --> 00:02:19,520
features used to train the classifier

77
00:02:19,520 --> 00:02:20,800
are more important than the particular

78
00:02:20,800 --> 00:02:23,040
classification algorithm used

79
00:02:23,040 --> 00:02:25,120
more recently deep learning techniques

80
00:02:25,120 --> 00:02:27,040
such as tactic noising auto encoders and

81
00:02:27,040 --> 00:02:28,640
convolutional neural networks

82
00:02:28,640 --> 00:02:31,200
have begun to be applied to the field

83
00:02:31,200 --> 00:02:32,000
these systems

84
00:02:32,000 --> 00:02:34,640
use learn abstract features

85
00:02:34,640 --> 00:02:37,200
automatically from raw traffic data

86
00:02:37,200 --> 00:02:38,720
and have been shown to achieve superior

87
00:02:38,720 --> 00:02:40,239
performance

88
00:02:40,239 --> 00:02:42,480
however in order to feed these data on

89
00:02:42,480 --> 00:02:44,319
green networks new larger data sets need

90
00:02:44,319 --> 00:02:46,560
to be collected

91
00:02:46,560 --> 00:02:48,239
and so the current state of the art

92
00:02:48,239 --> 00:02:50,480
attacks predominantly use convolutional

93
00:02:50,480 --> 00:02:54,480
models to achieve their high performance

94
00:02:54,959 --> 00:02:56,720
now these deep learning techniques

95
00:02:56,720 --> 00:02:58,879
however created a new problem

96
00:02:58,879 --> 00:03:00,560
the requirement for large data sets can

97
00:03:00,560 --> 00:03:02,319
be difficult to collect

98
00:03:02,319 --> 00:03:04,480
since websites change often new data

99
00:03:04,480 --> 00:03:06,159
must be frequently recollected

100
00:03:06,159 --> 00:03:08,560
as old samples can become stale and a

101
00:03:08,560 --> 00:03:10,159
model training on stale data has been

102
00:03:10,159 --> 00:03:11,599
shown to experience severe performance

103
00:03:11,599 --> 00:03:13,440
degradation

104
00:03:13,440 --> 00:03:15,599
to address this problem recent research

105
00:03:15,599 --> 00:03:17,120
has focused on developing new

106
00:03:17,120 --> 00:03:18,800
low data deep learning models that

107
00:03:18,800 --> 00:03:20,239
preserve the performance of deep

108
00:03:20,239 --> 00:03:22,080
learning without the demand of the many

109
00:03:22,080 --> 00:03:24,799
hundreds of instances per website

110
00:03:24,799 --> 00:03:26,959
so but it all recently showed that an

111
00:03:26,959 --> 00:03:28,640
optimized ensemble restaurant model

112
00:03:28,640 --> 00:03:29,120
using

113
00:03:29,120 --> 00:03:31,200
dilated convolutions could learn much

114
00:03:31,200 --> 00:03:33,280
more efficiently than prior techniques

115
00:03:33,280 --> 00:03:35,760
requiring only 100 samples to achieve

116
00:03:35,760 --> 00:03:38,560
state-of-the-art performance

117
00:03:38,560 --> 00:03:41,680
serineum at all instead identified that

118
00:03:41,680 --> 00:03:43,440
the discarding of the large stale data

119
00:03:43,440 --> 00:03:44,879
sets was wasteful

120
00:03:44,879 --> 00:03:46,239
and designed a model that could be

121
00:03:46,239 --> 00:03:48,080
pre-trained to learn to extract class

122
00:03:48,080 --> 00:03:50,159
agnostic features using the stable data

123
00:03:50,159 --> 00:03:53,680
with the triplet loss training technique

124
00:03:53,680 --> 00:03:57,519
now however 100 samples such as used by

125
00:03:57,519 --> 00:03:58,799
var cnn is still

126
00:03:58,799 --> 00:04:00,239
quite a lot given the quickly changing

127
00:04:00,239 --> 00:04:02,080
nature of websites

128
00:04:02,080 --> 00:04:05,360
and triplet fingerprinting however we

129
00:04:05,360 --> 00:04:05,760
see

130
00:04:05,760 --> 00:04:07,200
performs other poorly in the more

131
00:04:07,200 --> 00:04:09,920
realistic open world setting

132
00:04:09,920 --> 00:04:12,560
so our newest hack aims to improve along

133
00:04:12,560 --> 00:04:15,760
this line of research

134
00:04:15,760 --> 00:04:17,918
when examining the current state of the

135
00:04:17,918 --> 00:04:19,120
fingerprinting domain

136
00:04:19,120 --> 00:04:20,560
we realized that there was an additional

137
00:04:20,560 --> 00:04:22,639
data that is available to some

138
00:04:22,639 --> 00:04:23,520
adversaries

139
00:04:23,520 --> 00:04:25,759
but was being left largely unused by the

140
00:04:25,759 --> 00:04:27,360
current techniques

141
00:04:27,360 --> 00:04:29,520
this data is a vast amount of unlabeled

142
00:04:29,520 --> 00:04:31,199
website traffic data that is generated

143
00:04:31,199 --> 00:04:31,919
by the minute

144
00:04:31,919 --> 00:04:33,360
from the millions of users using tor

145
00:04:33,360 --> 00:04:35,360
daily in this case

146
00:04:35,360 --> 00:04:36,880
it would be trivial for organizations

147
00:04:36,880 --> 00:04:39,520
such as your isp government entities

148
00:04:39,520 --> 00:04:41,840
or even just operators of the tor middle

149
00:04:41,840 --> 00:04:42,800
relay nodes

150
00:04:42,800 --> 00:04:44,320
to save a portion of this unlabeled

151
00:04:44,320 --> 00:04:46,880
traffic for future training

152
00:04:46,880 --> 00:04:48,639
so in order to use this data we need to

153
00:04:48,639 --> 00:04:51,199
develop a new training technique

154
00:04:51,199 --> 00:04:53,040
to this end we have developed a deep

155
00:04:53,040 --> 00:04:54,240
learning model that uses

156
00:04:54,240 --> 00:04:56,240
semi-supervised learning to train using

157
00:04:56,240 --> 00:04:59,280
a large repository of unlabeled data

158
00:04:59,280 --> 00:05:01,120
and requires only a small amount of

159
00:05:01,120 --> 00:05:04,080
labeled samples for the targeted classes

160
00:05:04,080 --> 00:05:06,960
we call this the gandalf attack in order

161
00:05:06,960 --> 00:05:08,240
to do this we have what versus the power

162
00:05:08,240 --> 00:05:11,840
of generative adversarial networks

163
00:05:12,800 --> 00:05:15,280
so in the next few slides i'll go over

164
00:05:15,280 --> 00:05:16,800
the design of our new model

165
00:05:16,800 --> 00:05:18,880
followed by a review of the experiments

166
00:05:18,880 --> 00:05:20,160
so that we

167
00:05:20,160 --> 00:05:25,199
use to run to evaluate our system

168
00:05:25,199 --> 00:05:27,600
so to understand our technique we first

169
00:05:27,600 --> 00:05:28,800
need to briefly discuss

170
00:05:28,800 --> 00:05:30,560
generative adversarial networks

171
00:05:30,560 --> 00:05:32,560
otherwise known as scams

172
00:05:32,560 --> 00:05:34,639
games are first introduced in 2014 by

173
00:05:34,639 --> 00:05:37,520
goodfellow at all to produce fake images

174
00:05:37,520 --> 00:05:39,759
again is a minimax two players game

175
00:05:39,759 --> 00:05:41,520
between a generator and a discriminator

176
00:05:41,520 --> 00:05:42,720
network

177
00:05:42,720 --> 00:05:44,479
the goal of the generator network is to

178
00:05:44,479 --> 00:05:46,639
produce realistic images without access

179
00:05:46,639 --> 00:05:48,560
to real examples

180
00:05:48,560 --> 00:05:50,000
the discriminator competes with the

181
00:05:50,000 --> 00:05:51,680
generator network to identify which

182
00:05:51,680 --> 00:05:53,199
samples were produced by the generator

183
00:05:53,199 --> 00:05:56,160
and which are the real examples the

184
00:05:56,160 --> 00:05:58,080
generator uses information obtained for

185
00:05:58,080 --> 00:05:59,919
the discriminator to update its weights

186
00:05:59,919 --> 00:06:02,960
through every iteration so in this way

187
00:06:02,960 --> 00:06:04,400
the generator lines produce images that

188
00:06:04,400 --> 00:06:04,960
fold the

189
00:06:04,960 --> 00:06:07,919
discriminator the networks are carefully

190
00:06:07,919 --> 00:06:08,720
trained as a pair

191
00:06:08,720 --> 00:06:10,400
such that neither is ever significantly

192
00:06:10,400 --> 00:06:11,520
better at its task than the other to

193
00:06:11,520 --> 00:06:13,280
avoid collapse

194
00:06:13,280 --> 00:06:17,199
um and in the computer vision domain

195
00:06:17,199 --> 00:06:18,880
we see that we see that this technique

196
00:06:18,880 --> 00:06:20,560
has been shown to be very successful at

197
00:06:20,560 --> 00:06:22,319
producing high quality high pressure

198
00:06:22,319 --> 00:06:24,160
images that can sometimes fool even the

199
00:06:24,160 --> 00:06:27,199
discerning eye of a human

200
00:06:27,600 --> 00:06:29,600
now this technique can be extended to

201
00:06:29,600 --> 00:06:32,400
perform semi-supervised learning

202
00:06:32,400 --> 00:06:34,000
so in this model the goal is to train a

203
00:06:34,000 --> 00:06:35,600
powerful discriminator that can perform

204
00:06:35,600 --> 00:06:38,639
effective multi-class classification

205
00:06:38,639 --> 00:06:40,479
this is achieved by introducing class

206
00:06:40,479 --> 00:06:42,240
labels to some of the data used to train

207
00:06:42,240 --> 00:06:44,240
the discriminator

208
00:06:44,240 --> 00:06:45,759
however with a more powerful

209
00:06:45,759 --> 00:06:48,240
discriminator the generator can be

210
00:06:48,240 --> 00:06:49,840
quickly out-competed it may fail to

211
00:06:49,840 --> 00:06:52,319
produce realistic fixed samples

212
00:06:52,319 --> 00:06:54,479
so to address this styling selling men

213
00:06:54,479 --> 00:06:55,360
at all proposed

214
00:06:55,360 --> 00:06:57,919
use of feature matching loss this new

215
00:06:57,919 --> 00:06:59,280
loss function computes the difference

216
00:06:59,280 --> 00:07:00,479
between the middle layer

217
00:07:00,479 --> 00:07:02,479
feature outputs of the discriminator for

218
00:07:02,479 --> 00:07:04,639
the real and the fake data

219
00:07:04,639 --> 00:07:06,639
this loss is then a measure of how

220
00:07:06,639 --> 00:07:08,400
statistically similar the features are

221
00:07:08,400 --> 00:07:11,199
for the real and the fake samples

222
00:07:11,199 --> 00:07:12,800
in this way the generator is trained

223
00:07:12,800 --> 00:07:14,479
explicitly to produce samples that

224
00:07:14,479 --> 00:07:16,160
closely resemble the real features that

225
00:07:16,160 --> 00:07:17,520
the discriminator is using to classify

226
00:07:17,520 --> 00:07:20,159
the labeled data

227
00:07:20,240 --> 00:07:22,479
now on the other hand the distributor

228
00:07:22,479 --> 00:07:23,919
must now learn to perform

229
00:07:23,919 --> 00:07:26,319
two functions first it must learn to

230
00:07:26,319 --> 00:07:27,680
classify the label data with high

231
00:07:27,680 --> 00:07:28,639
accuracy

232
00:07:28,639 --> 00:07:30,400
and second it must learn robust features

233
00:07:30,400 --> 00:07:32,319
that allow it to separate the real

234
00:07:32,319 --> 00:07:34,000
unlabeled data from the fake data

235
00:07:34,000 --> 00:07:36,000
produced by the generator

236
00:07:36,000 --> 00:07:37,039
the point of this is that the

237
00:07:37,039 --> 00:07:38,800
discriminator will learn how to extract

238
00:07:38,800 --> 00:07:39,520
features of the

239
00:07:39,520 --> 00:07:41,599
unlabeled data from the enabled data

240
00:07:41,599 --> 00:07:43,039
because it needs them to distinguish

241
00:07:43,039 --> 00:07:45,440
between real and fake samples

242
00:07:45,440 --> 00:07:47,759
so in this way the unlabeled and

243
00:07:47,759 --> 00:07:49,680
generated samples are used to bootstrap

244
00:07:49,680 --> 00:07:51,280
the feature learning process allowing

245
00:07:51,280 --> 00:07:52,879
the model to learn robust features to

246
00:07:52,879 --> 00:07:56,000
classify labeled data

247
00:07:57,039 --> 00:07:58,720
now when adapting this construction to

248
00:07:58,720 --> 00:08:00,240
our website fingerprinting domain

249
00:08:00,240 --> 00:08:01,520
there are some differences that must be

250
00:08:01,520 --> 00:08:04,080
considered in the computer vision domain

251
00:08:04,080 --> 00:08:06,160
saliman sampled examples from the same

252
00:08:06,160 --> 00:08:08,080
amnes distribution to fill both

253
00:08:08,080 --> 00:08:11,520
their unlabeled and labeled data sets

254
00:08:11,520 --> 00:08:13,280
however in our website fingerprinting

255
00:08:13,280 --> 00:08:15,440
problem the unlabeled data is instead

256
00:08:15,440 --> 00:08:17,039
sampled from a different data

257
00:08:17,039 --> 00:08:19,120
distribution than the label samples

258
00:08:19,120 --> 00:08:21,120
as one will contain samples from many

259
00:08:21,120 --> 00:08:22,400
unknown classes whereas the other

260
00:08:22,400 --> 00:08:23,919
contained samples from only a handful of

261
00:08:23,919 --> 00:08:26,720
targeted classes so that's to say that

262
00:08:26,720 --> 00:08:27,360
there is

263
00:08:27,360 --> 00:08:29,120
little to no overlap of the classes

264
00:08:29,120 --> 00:08:30,639
present in our labeled and unlabeled

265
00:08:30,639 --> 00:08:32,958
data sets

266
00:08:32,958 --> 00:08:34,640
so in our experiments we found that the

267
00:08:34,640 --> 00:08:36,080
distance between our labeled and

268
00:08:36,080 --> 00:08:37,279
unlabeled samples

269
00:08:37,279 --> 00:08:39,039
are generally much greater than that of

270
00:08:39,039 --> 00:08:42,080
the distances between solum's datasets

271
00:08:42,080 --> 00:08:43,599
and this may somewhat limit the

272
00:08:43,599 --> 00:08:45,360
effectiveness of this technique within

273
00:08:45,360 --> 00:08:48,880
our budget marketing domain

274
00:08:50,399 --> 00:08:53,120
so we built our gandalf model on top of

275
00:08:53,120 --> 00:08:56,080
the model used by sally manito

276
00:08:56,080 --> 00:08:58,800
however there was required several

277
00:08:58,800 --> 00:09:00,160
architectural changes

278
00:09:00,160 --> 00:09:03,200
to adapt to our domain so first we

279
00:09:03,200 --> 00:09:04,640
converted the model to operate on

280
00:09:04,640 --> 00:09:05,839
one-dimensional data

281
00:09:05,839 --> 00:09:07,440
and expanded the depth of the networks

282
00:09:07,440 --> 00:09:09,600
to be much deeper

283
00:09:09,600 --> 00:09:10,959
to address the instability that the

284
00:09:10,959 --> 00:09:12,399
deeper networks caused we added

285
00:09:12,399 --> 00:09:13,519
additional regular

286
00:09:13,519 --> 00:09:15,519
regularization in the form of dropout

287
00:09:15,519 --> 00:09:18,800
and vast normalization layers

288
00:09:18,800 --> 00:09:21,040
additionally we trained we modified how

289
00:09:21,040 --> 00:09:22,720
the feature matching loss was computed

290
00:09:22,720 --> 00:09:23,839
to instead use the

291
00:09:23,839 --> 00:09:25,200
mean squared distance between the

292
00:09:25,200 --> 00:09:28,160
featured real and the fake samples

293
00:09:28,160 --> 00:09:29,680
using the mean squared distance more

294
00:09:29,680 --> 00:09:31,600
severely punishes large errors in the

295
00:09:31,600 --> 00:09:32,320
generator

296
00:09:32,320 --> 00:09:33,920
which we found empirically improves the

297
00:09:33,920 --> 00:09:35,360
performance of the model when operating

298
00:09:35,360 --> 00:09:36,000
on website

299
00:09:36,000 --> 00:09:39,440
traffic we also adjusted many hybrid

300
00:09:39,440 --> 00:09:40,560
parameters using a

301
00:09:40,560 --> 00:09:43,040
grid space search for which the full

302
00:09:43,040 --> 00:09:46,719
details of can be found in our paper

303
00:09:47,680 --> 00:09:49,920
so up next we'll be reviewing the

304
00:09:49,920 --> 00:09:50,880
results of our

305
00:09:50,880 --> 00:09:55,040
of our evaluations against gandalf

306
00:09:56,560 --> 00:09:59,519
website fingerprinting experiments are

307
00:09:59,519 --> 00:10:00,640
performed in two

308
00:10:00,640 --> 00:10:02,959
scenarios closed world and open world

309
00:10:02,959 --> 00:10:04,640
setting

310
00:10:04,640 --> 00:10:06,079
these scenarios describe what

311
00:10:06,079 --> 00:10:07,519
restrictions have been put on the

312
00:10:07,519 --> 00:10:09,200
experiment

313
00:10:09,200 --> 00:10:11,120
in the closed world experiments the

314
00:10:11,120 --> 00:10:12,880
classification model is only given

315
00:10:12,880 --> 00:10:14,560
traffic from a small set of monitored

316
00:10:14,560 --> 00:10:17,680
websites this experiment type

317
00:10:17,680 --> 00:10:19,440
does not actively represent the real

318
00:10:19,440 --> 00:10:20,880
world performance

319
00:10:20,880 --> 00:10:23,120
but is often used to benchmark modeling

320
00:10:23,120 --> 00:10:26,000
techniques against one another

321
00:10:26,000 --> 00:10:27,760
on the other hand the open world setting

322
00:10:27,760 --> 00:10:29,839
makes the experiment more realistic by

323
00:10:29,839 --> 00:10:31,519
including samples from a large set of

324
00:10:31,519 --> 00:10:32,160
sites

325
00:10:32,160 --> 00:10:33,839
that were unseen by the model during

326
00:10:33,839 --> 00:10:35,519
training this

327
00:10:35,519 --> 00:10:37,680
a monitor unmonitored set of sites

328
00:10:37,680 --> 00:10:39,040
represents traffic from the many

329
00:10:39,040 --> 00:10:40,880
millions of sites that the adversary is

330
00:10:40,880 --> 00:10:43,279
not interested in identifying

331
00:10:43,279 --> 00:10:45,760
the opel world scenario as such is a

332
00:10:45,760 --> 00:10:47,440
more difficult path the closed world

333
00:10:47,440 --> 00:10:48,480
scenario

334
00:10:48,480 --> 00:10:50,399
and is a better indicator of the real

335
00:10:50,399 --> 00:10:53,200
world performance

336
00:10:53,519 --> 00:10:56,240
and we use precision and recall metrics

337
00:10:56,240 --> 00:11:00,079
in this setting

338
00:11:00,079 --> 00:11:02,720
so in this paper we also performed our

339
00:11:02,720 --> 00:11:04,320
evaluations under two

340
00:11:04,320 --> 00:11:06,480
different additional perspectives in the

341
00:11:06,480 --> 00:11:07,920
first what we call website

342
00:11:07,920 --> 00:11:09,760
fingerprinting for index pages

343
00:11:09,760 --> 00:11:11,519
we use the large data sets collected by

344
00:11:11,519 --> 00:11:14,000
serinum at all in rumor at all

345
00:11:14,000 --> 00:11:15,680
in this scenario each website is

346
00:11:15,680 --> 00:11:17,760
represented by samples produced from

347
00:11:17,760 --> 00:11:19,440
visiting its index page

348
00:11:19,440 --> 00:11:21,920
and only its index page this represents

349
00:11:21,920 --> 00:11:23,440
the way website fingerprinting has been

350
00:11:23,440 --> 00:11:24,480
performed throughout much of the

351
00:11:24,480 --> 00:11:26,560
previous literature

352
00:11:26,560 --> 00:11:28,399
now in addition to this we also run

353
00:11:28,399 --> 00:11:29,839
experiments where each website is

354
00:11:29,839 --> 00:11:30,880
instead represented

355
00:11:30,880 --> 00:11:32,720
using samples produced through visiting

356
00:11:32,720 --> 00:11:34,399
a subset of the money sub pages

357
00:11:34,399 --> 00:11:37,120
available on that site

358
00:11:37,120 --> 00:11:38,640
this type of website you can printing

359
00:11:38,640 --> 00:11:40,720
was first examined by pentangle at all

360
00:11:40,720 --> 00:11:42,480
however we are the we have collected a

361
00:11:42,480 --> 00:11:44,240
new more recent data set

362
00:11:44,240 --> 00:11:46,000
and are the first to evaluate many of

363
00:11:46,000 --> 00:11:47,519
the deep learning methods under this

364
00:11:47,519 --> 00:11:49,279
scenario

365
00:11:49,279 --> 00:11:51,600
this new subpage data set contains

366
00:11:51,600 --> 00:11:53,839
samples from 25 websites

367
00:11:53,839 --> 00:11:56,160
each website is represented by 96 unique

368
00:11:56,160 --> 00:11:58,639
subpages with each subpage having 39

369
00:11:58,639 --> 00:12:00,800
individual samples

370
00:12:00,800 --> 00:12:02,720
this produces a total of approximately

371
00:12:02,720 --> 00:12:04,560
93 000 samples for the

372
00:12:04,560 --> 00:12:07,760
for the new data set

373
00:12:08,720 --> 00:12:11,200
so we will first look at the performance

374
00:12:11,200 --> 00:12:13,040
of the models in the closed world index

375
00:12:13,040 --> 00:12:14,399
page setting

376
00:12:14,399 --> 00:12:16,000
each model is trained using a number of

377
00:12:16,000 --> 00:12:17,760
samples per website

378
00:12:17,760 --> 00:12:19,839
that varies from 5 to 90 samples in the

379
00:12:19,839 --> 00:12:21,279
experiments

380
00:12:21,279 --> 00:12:22,800
in this setting we find that when the

381
00:12:22,800 --> 00:12:24,399
number of training samples is 10

382
00:12:24,399 --> 00:12:26,079
gandalf matches the performance of my

383
00:12:26,079 --> 00:12:27,440
fingerprinting

384
00:12:27,440 --> 00:12:29,040
when the samp when the number of samples

385
00:12:29,040 --> 00:12:30,880
increases gandalf outperforms for the

386
00:12:30,880 --> 00:12:31,680
fingerprinting

387
00:12:31,680 --> 00:12:36,319
but is itself in turn outperformed by

388
00:12:36,839 --> 00:12:38,160
varsino

389
00:12:38,160 --> 00:12:40,240
now when we switch the open world

390
00:12:40,240 --> 00:12:41,279
setting we find that

391
00:12:41,279 --> 00:12:43,120
gamble performs much more admirably

392
00:12:43,120 --> 00:12:44,639
despite its somewhat underwhelming

393
00:12:44,639 --> 00:12:46,480
closed build performance

394
00:12:46,480 --> 00:12:48,320
here we have plotted the precision and

395
00:12:48,320 --> 00:12:51,839
the recall of kfp to the fingerprinting

396
00:12:51,839 --> 00:12:54,000
scandal also plotted when tested at

397
00:12:54,000 --> 00:12:55,519
three different sizes of the background

398
00:12:55,519 --> 00:12:56,240
set

399
00:12:56,240 --> 00:12:58,320
while ksp and fingerprinting were both

400
00:12:58,320 --> 00:13:00,720
evaluated using a vacuum set size of 360

401
00:13:00,720 --> 00:13:02,320
thousand

402
00:13:02,320 --> 00:13:04,880
the kfp and fingerprinting plots can be

403
00:13:04,880 --> 00:13:06,959
directly compared to the dark blue plant

404
00:13:06,959 --> 00:13:09,279
of candles

405
00:13:09,279 --> 00:13:11,440
we see that gandalf achieves much higher

406
00:13:11,440 --> 00:13:13,600
recall at comparable levels of precision

407
00:13:13,600 --> 00:13:16,560
than the other two attacks it is however

408
00:13:16,560 --> 00:13:17,519
interesting to note

409
00:13:17,519 --> 00:13:20,079
that kfp achieves the highest maximum

410
00:13:20,079 --> 00:13:20,959
precision

411
00:13:20,959 --> 00:13:24,638
albeit with a much lower recall

412
00:13:26,720 --> 00:13:28,240
so up next we'll take a look at the

413
00:13:28,240 --> 00:13:29,760
performance against the subpage

414
00:13:29,760 --> 00:13:32,160
fingerprinting dataset

415
00:13:32,160 --> 00:13:34,399
so first all across the board we see

416
00:13:34,399 --> 00:13:36,880
that the performance is much reduced

417
00:13:36,880 --> 00:13:38,480
this is likely due to the fact that the

418
00:13:38,480 --> 00:13:41,199
intra-class variance is much higher here

419
00:13:41,199 --> 00:13:42,480
than in the index page

420
00:13:42,480 --> 00:13:46,320
only sub data set we see that gandalf

421
00:13:46,320 --> 00:13:48,000
marginally outperforms the other models

422
00:13:48,000 --> 00:13:49,600
when the number of training samples are

423
00:13:49,600 --> 00:13:53,680
50 or more however is beat by kfp

424
00:13:53,680 --> 00:13:56,240
when the trading instance count is very

425
00:13:56,240 --> 00:13:58,480
low

426
00:13:59,760 --> 00:14:01,440
next we'll look at the open world

427
00:14:01,440 --> 00:14:04,399
performance in the same setting

428
00:14:04,399 --> 00:14:06,959
in this situation we find that kfp

429
00:14:06,959 --> 00:14:08,800
significantly outperforms scandals

430
00:14:08,800 --> 00:14:10,959
achieving both higher maximum precision

431
00:14:10,959 --> 00:14:12,240
and recall when evaluated with a

432
00:14:12,240 --> 00:14:14,800
background set of 70 000.

433
00:14:14,800 --> 00:14:16,560
the effectiveness of kfp in this setting

434
00:14:16,560 --> 00:14:18,320
is likely due to the high difficulty of

435
00:14:18,320 --> 00:14:20,320
learning good features with few training

436
00:14:20,320 --> 00:14:21,839
samples

437
00:14:21,839 --> 00:14:23,920
kfp uses handcrafted handcrafted

438
00:14:23,920 --> 00:14:27,760
features so avoids many of these

439
00:14:28,839 --> 00:14:30,160
problems

440
00:14:30,160 --> 00:14:32,720
now furthermore we also examine the

441
00:14:32,720 --> 00:14:34,320
impact of circuit speed and congestion

442
00:14:34,320 --> 00:14:35,519
on our model

443
00:14:35,519 --> 00:14:36,880
in these experiments we separate the

444
00:14:36,880 --> 00:14:38,639
data from steering them at all

445
00:14:38,639 --> 00:14:40,959
by the circuits they were collected on

446
00:14:40,959 --> 00:14:42,880
first we examined the performance of

447
00:14:42,880 --> 00:14:44,800
gandalf when the model is trained on one

448
00:14:44,800 --> 00:14:47,120
set of circuits and tested on another

449
00:14:47,120 --> 00:14:48,720
well we also vary the number of unique

450
00:14:48,720 --> 00:14:50,639
circuits used for training

451
00:14:50,639 --> 00:14:53,360
then we separate circuits by their speed

452
00:14:53,360 --> 00:14:54,800
samples are completed in the top

453
00:14:54,800 --> 00:14:56,959
25th percentile the label is fast

454
00:14:56,959 --> 00:14:59,120
whereas samples within the lower 25th

455
00:14:59,120 --> 00:14:59,920
percentile

456
00:14:59,920 --> 00:15:02,320
are labeled as slow we then trained on

457
00:15:02,320 --> 00:15:05,199
one set and tested on the other

458
00:15:05,199 --> 00:15:07,120
now in both of these experiments we

459
00:15:07,120 --> 00:15:08,720
found that the model does not perform

460
00:15:08,720 --> 00:15:10,399
significantly differently

461
00:15:10,399 --> 00:15:11,600
indicating that the effect of the

462
00:15:11,600 --> 00:15:13,199
circuit characteristics on the traffic

463
00:15:13,199 --> 00:15:14,240
fingerprintability

464
00:15:14,240 --> 00:15:17,760
fingerprintability is minimal

465
00:15:19,199 --> 00:15:21,519
now finally we evaluated the effect of

466
00:15:21,519 --> 00:15:22,880
using different data sets

467
00:15:22,880 --> 00:15:24,880
as the unlabeled data to train our

468
00:15:24,880 --> 00:15:26,560
gandalf model

469
00:15:26,560 --> 00:15:28,800
we select data sets of variant size

470
00:15:28,800 --> 00:15:29,920
network conditions

471
00:15:29,920 --> 00:15:31,839
and collection time to evaluate their

472
00:15:31,839 --> 00:15:34,079
effects on the performance of our model

473
00:15:34,079 --> 00:15:35,920
we saw that generally the model

474
00:15:35,920 --> 00:15:37,519
performed better with the

475
00:15:37,519 --> 00:15:40,720
unlabeled data of a larger variety yet

476
00:15:40,720 --> 00:15:42,480
overall the effect on performance is

477
00:15:42,480 --> 00:15:44,800
smooth

478
00:15:47,120 --> 00:15:49,600
so now in conclusion we have presented a

479
00:15:49,600 --> 00:15:51,279
new website fingerprinting attack

480
00:15:51,279 --> 00:15:53,759
for the load data setting and perform

481
00:15:53,759 --> 00:15:55,199
extensive evaluations

482
00:15:55,199 --> 00:15:57,759
of several models under the setting we

483
00:15:57,759 --> 00:15:59,440
found that semi-supervised learning with

484
00:15:59,440 --> 00:16:01,360
gandalf offers strong performance

485
00:16:01,360 --> 00:16:02,720
in the open world setting and

486
00:16:02,720 --> 00:16:04,079
competitive performance in the closed

487
00:16:04,079 --> 00:16:04,800
world

488
00:16:04,800 --> 00:16:06,480
by taking advantage of the previously

489
00:16:06,480 --> 00:16:09,199
untapped unlabeled data

490
00:16:09,199 --> 00:16:11,040
using our newly collected subpage data

491
00:16:11,040 --> 00:16:12,800
set we have found that

492
00:16:12,800 --> 00:16:14,399
fingerprinting a website from the

493
00:16:14,399 --> 00:16:16,480
broader perspective of its many sub

494
00:16:16,480 --> 00:16:17,120
pages

495
00:16:17,120 --> 00:16:18,800
is a particularly difficult task

496
00:16:18,800 --> 00:16:21,839
especially for deep learning models

497
00:16:21,839 --> 00:16:23,759
when examined in the low data setting

498
00:16:23,759 --> 00:16:25,279
low data setting we were surprised to

499
00:16:25,279 --> 00:16:28,079
find that the kfp model outperformed

500
00:16:28,079 --> 00:16:29,920
all the deep learning models using

501
00:16:29,920 --> 00:16:32,160
handcrafted features

502
00:16:32,160 --> 00:16:37,839
under the subpage setting

503
00:16:39,199 --> 00:16:41,759
and so finally uh there exists a number

504
00:16:41,759 --> 00:16:43,040
of interesting directions that could be

505
00:16:43,040 --> 00:16:43,759
built on

506
00:16:43,759 --> 00:16:46,639
this research in the future the

507
00:16:46,639 --> 00:16:48,079
semi-supervised learning structure of

508
00:16:48,079 --> 00:16:48,880
gandalf

509
00:16:48,880 --> 00:16:50,720
could be extended using a conditional

510
00:16:50,720 --> 00:16:51,920
generator

511
00:16:51,920 --> 00:16:53,440
this may allow the generator to produce

512
00:16:53,440 --> 00:16:54,959
more convincing fake samples

513
00:16:54,959 --> 00:16:56,320
that lead to the discriminator finding

514
00:16:56,320 --> 00:16:59,040
more robust features

515
00:16:59,040 --> 00:17:01,120
additionally unlabeled data may be taken

516
00:17:01,120 --> 00:17:02,800
advantage of by assigning

517
00:17:02,800 --> 00:17:04,880
fuzzy labels and could be used to

518
00:17:04,880 --> 00:17:06,480
augment the size of

519
00:17:06,480 --> 00:17:08,880
the label data set although it's yet to

520
00:17:08,880 --> 00:17:11,119
be seen how effective these

521
00:17:11,119 --> 00:17:15,039
additional techniques would be

522
00:17:15,199 --> 00:17:17,280
now we also believe that there is much

523
00:17:17,280 --> 00:17:19,760
room for improvement in the combination

524
00:17:19,760 --> 00:17:20,480
of

525
00:17:20,480 --> 00:17:22,880
many previous techniques to better

526
00:17:22,880 --> 00:17:24,319
enhance the classification system by

527
00:17:24,319 --> 00:17:26,480
taking advantage of many data sources

528
00:17:26,480 --> 00:17:29,120
such as a stale data as done interplay

529
00:17:29,120 --> 00:17:30,799
fingerprinting and unlabeled data

530
00:17:30,799 --> 00:17:34,080
in our attack

531
00:17:34,080 --> 00:17:36,720
and now finally our performance in the

532
00:17:36,720 --> 00:17:37,760
subpage setting

533
00:17:37,760 --> 00:17:39,760
has shown that there's still much work

534
00:17:39,760 --> 00:17:41,200
still to be done in improving the

535
00:17:41,200 --> 00:17:42,559
performance of fingerprinting websites

536
00:17:42,559 --> 00:17:47,200
more broadly using their subpages

537
00:17:47,200 --> 00:17:49,600
and so with that um you can find our

538
00:17:49,600 --> 00:17:52,080
code for the project on our github

539
00:17:52,080 --> 00:17:53,679
and you're welcome to contact any of us

540
00:17:53,679 --> 00:17:55,120
regarding the project through our emails

541
00:17:55,120 --> 00:17:57,760
as listed on the slide

542
00:17:57,760 --> 00:18:01,840
and so are there any questions

