1
00:00:03,600 --> 00:00:05,600
good morning everyone i am ashish and i

2
00:00:05,600 --> 00:00:07,359
will be presenting our work on

3
00:00:07,359 --> 00:00:09,360
differential privacy at risk

4
00:00:09,360 --> 00:00:11,360
when this work was published i was post

5
00:00:11,360 --> 00:00:12,719
doctorate at ens

6
00:00:12,719 --> 00:00:14,240
and there was a post doctorate at

7
00:00:14,240 --> 00:00:15,839
chalmers sweden

8
00:00:15,839 --> 00:00:17,600
i was i'm presently a lecturer at

9
00:00:17,600 --> 00:00:19,520
national university of singapore and

10
00:00:19,520 --> 00:00:22,400
dev is a researcher at indriya leel this

11
00:00:22,400 --> 00:00:24,640
work was funded by two projects

12
00:00:24,640 --> 00:00:27,039
single research lab at nus as well as

13
00:00:27,039 --> 00:00:28,480
bio qop project

14
00:00:28,480 --> 00:00:31,679
at ens without further ado let me start

15
00:00:31,679 --> 00:00:32,399
with the

16
00:00:32,399 --> 00:00:35,360
presentation i am sure that at pulpits

17
00:00:35,360 --> 00:00:36,880
everyone is familiar with the

18
00:00:36,880 --> 00:00:38,480
differential privacy

19
00:00:38,480 --> 00:00:40,559
our work exploits a few caveats in the

20
00:00:40,559 --> 00:00:42,960
definition so for the sake of complement

21
00:00:42,960 --> 00:00:45,120
completeness let's start with it

22
00:00:45,120 --> 00:00:47,120
differential privacy is defined for a

23
00:00:47,120 --> 00:00:48,480
randomized algorithm

24
00:00:48,480 --> 00:00:50,320
it is characterized by the parameter

25
00:00:50,320 --> 00:00:51,559
epsilon that bounce the

26
00:00:51,559 --> 00:00:53,680
indistinguishability between the outputs

27
00:00:53,680 --> 00:00:55,760
of the algorithm on two neighboring data

28
00:00:55,760 --> 00:00:56,640
sets

29
00:00:56,640 --> 00:00:58,879
we call a randomized algorithm to be

30
00:00:58,879 --> 00:01:00,640
epsilon differentially private

31
00:01:00,640 --> 00:01:03,199
if it satisfies the inequality shown in

32
00:01:03,199 --> 00:01:05,280
the equation on the screen

33
00:01:05,280 --> 00:01:07,760
epsilon is also known as the privacy

34
00:01:07,760 --> 00:01:08,960
budget

35
00:01:08,960 --> 00:01:11,920
we find two issues with the existing

36
00:01:11,920 --> 00:01:13,119
formulation

37
00:01:13,119 --> 00:01:15,119
firstly the definition is a worst case

38
00:01:15,119 --> 00:01:17,360
guarantee the inequality in the earlier

39
00:01:17,360 --> 00:01:18,000
definition

40
00:01:18,000 --> 00:01:19,920
holds not only for all possible

41
00:01:19,920 --> 00:01:21,439
neighboring data sets

42
00:01:21,439 --> 00:01:24,159
but for all possible outputs this is

43
00:01:24,159 --> 00:01:26,159
indeed a worst case guarantee that

44
00:01:26,159 --> 00:01:28,240
requires high amount of noise in the

45
00:01:28,240 --> 00:01:30,400
randomized algorithms for strong privacy

46
00:01:30,400 --> 00:01:31,759
guarantees

47
00:01:31,759 --> 00:01:34,079
application of the off-the-shelf privacy

48
00:01:34,079 --> 00:01:36,159
preserving mechanism such as the laplace

49
00:01:36,159 --> 00:01:37,280
mechanism

50
00:01:37,280 --> 00:01:40,079
without a careful analysis end up in

51
00:01:40,079 --> 00:01:41,600
suffering

52
00:01:41,600 --> 00:01:43,680
higher loss in the utility for

53
00:01:43,680 --> 00:01:45,520
traditional businesses the utility of

54
00:01:45,520 --> 00:01:47,680
their products is of the essence

55
00:01:47,680 --> 00:01:49,680
business oriented minds are always ready

56
00:01:49,680 --> 00:01:50,799
to take risks

57
00:01:50,799 --> 00:01:53,600
that can be quantified therefore instead

58
00:01:53,600 --> 00:01:55,600
of providing the worst case guarantee we

59
00:01:55,600 --> 00:01:57,280
envision a privacy guarantee with a

60
00:01:57,280 --> 00:01:58,079
quantifiable

61
00:01:58,079 --> 00:02:00,799
risk as a good compromise the second

62
00:02:00,799 --> 00:02:02,479
issue is the abstract nature of the

63
00:02:02,479 --> 00:02:03,600
definition

64
00:02:03,600 --> 00:02:05,439
businesses understand metrics that can

65
00:02:05,439 --> 00:02:07,439
be related to monetary values

66
00:02:07,439 --> 00:02:09,280
a non-technical person would need to

67
00:02:09,280 --> 00:02:11,599
invest a significant amount of time

68
00:02:11,599 --> 00:02:14,000
to understand the caveats in the

69
00:02:14,000 --> 00:02:15,200
mechanisms

70
00:02:15,200 --> 00:02:18,000
to get a meaningful result we observe

71
00:02:18,000 --> 00:02:19,680
that there is a need to bridge the gap

72
00:02:19,680 --> 00:02:21,760
between this abstract definition and the

73
00:02:21,760 --> 00:02:23,440
monetary value

74
00:02:23,440 --> 00:02:25,520
we address these two issues in this

75
00:02:25,520 --> 00:02:27,920
paper we propose privacy at risk which

76
00:02:27,920 --> 00:02:29,599
is a probabilistic extension of

77
00:02:29,599 --> 00:02:31,120
differential privacy

78
00:02:31,120 --> 00:02:33,120
it provides a way to fine tune the

79
00:02:33,120 --> 00:02:35,440
privacy risk of a differentially private

80
00:02:35,440 --> 00:02:36,560
algorithm

81
00:02:36,560 --> 00:02:39,120
we provide closed form solutions for the

82
00:02:39,120 --> 00:02:39,599
final

83
00:02:39,599 --> 00:02:42,239
fine tuning of the laplace mechanism to

84
00:02:42,239 --> 00:02:43,519
address the second issue

85
00:02:43,519 --> 00:02:45,599
we propose a cost model that bridges the

86
00:02:45,599 --> 00:02:47,840
gap between the abstract privacy level

87
00:02:47,840 --> 00:02:50,160
and a monetary budget for a project in a

88
00:02:50,160 --> 00:02:51,120
company

89
00:02:51,120 --> 00:02:53,280
we instantiate the cost model by putting

90
00:02:53,280 --> 00:02:56,080
it in the context of gdpr

91
00:02:56,080 --> 00:02:58,239
let's start with the definition we

92
00:02:58,239 --> 00:03:00,000
define privacy at risk as a

93
00:03:00,000 --> 00:03:01,599
probabilistic extension

94
00:03:01,599 --> 00:03:03,920
of the differential privacy definition

95
00:03:03,920 --> 00:03:05,920
the proposed extension is inspired by

96
00:03:05,920 --> 00:03:07,680
the value of risk concept

97
00:03:07,680 --> 00:03:10,239
from the risk analysis literature value

98
00:03:10,239 --> 00:03:11,920
at risk quantifies the loss in the

99
00:03:11,920 --> 00:03:14,159
investments for a given portfolio and a

100
00:03:14,159 --> 00:03:15,440
confidence bound

101
00:03:15,440 --> 00:03:17,840
in the same spirit risk in the proposed

102
00:03:17,840 --> 00:03:18,640
definition

103
00:03:18,640 --> 00:03:20,560
which is quantified by the parameter

104
00:03:20,560 --> 00:03:22,480
gamma is an upper bound on the

105
00:03:22,480 --> 00:03:24,480
probability that a randomized algorithm

106
00:03:24,480 --> 00:03:26,080
fails to satisfy

107
00:03:26,080 --> 00:03:28,239
differential privacy to keep the

108
00:03:28,239 --> 00:03:29,280
definition as

109
00:03:29,280 --> 00:03:32,400
general as possible we explicitly denote

110
00:03:32,400 --> 00:03:34,640
a privacy preserving mechanism by the

111
00:03:34,640 --> 00:03:35,440
parameter

112
00:03:35,440 --> 00:03:38,319
f which is query and theta which is the

113
00:03:38,319 --> 00:03:39,599
space of parameters

114
00:03:39,599 --> 00:03:41,760
as well as the space of query specific

115
00:03:41,760 --> 00:03:43,760
parameters

116
00:03:43,760 --> 00:03:45,599
researchers have so far proposed

117
00:03:45,599 --> 00:03:46,879
multiple extensions

118
00:03:46,879 --> 00:03:49,440
of the vanilla definition we list the

119
00:03:49,440 --> 00:03:51,360
most relevant extensions here

120
00:03:51,360 --> 00:03:53,200
approximate differential privacy also

121
00:03:53,200 --> 00:03:54,799
known as epsilon delta differential

122
00:03:54,799 --> 00:03:56,480
privacy is by its nature

123
00:03:56,480 --> 00:03:58,720
a non-probabilistic extension of

124
00:03:58,720 --> 00:04:00,239
differential privacy

125
00:04:00,239 --> 00:04:03,120
the definition itself does not impose a

126
00:04:03,120 --> 00:04:04,239
direct connection

127
00:04:04,239 --> 00:04:06,799
between epsilon and delta delta

128
00:04:06,799 --> 00:04:08,799
parameter is simply a slack in the

129
00:04:08,799 --> 00:04:10,720
inequality definition

130
00:04:10,720 --> 00:04:12,560
random differential privacy and

131
00:04:12,560 --> 00:04:14,319
probabilistic differential privacy they

132
00:04:14,319 --> 00:04:16,320
are indeed closely related to our

133
00:04:16,320 --> 00:04:17,040
definition

134
00:04:17,040 --> 00:04:20,238
which we will see soon concentrated and

135
00:04:20,238 --> 00:04:22,479
rainy differential privacy are stronger

136
00:04:22,479 --> 00:04:25,919
but yet more abstract definition

137
00:04:25,919 --> 00:04:28,080
extensions of differential privacy

138
00:04:28,080 --> 00:04:29,120
instead of bounding

139
00:04:29,120 --> 00:04:31,840
a privacy loss they bound the moments of

140
00:04:31,840 --> 00:04:32,639
privacy

141
00:04:32,639 --> 00:04:35,600
loss random variable and which sets the

142
00:04:35,600 --> 00:04:37,199
these works apart

143
00:04:37,199 --> 00:04:39,840
from our work we also assess the

144
00:04:39,840 --> 00:04:41,360
traditional properties such as

145
00:04:41,360 --> 00:04:43,680
post-processing and convexity as well as

146
00:04:43,680 --> 00:04:46,240
composition for the proposed definition

147
00:04:46,240 --> 00:04:48,800
miser ethanol they have proved that any

148
00:04:48,800 --> 00:04:50,240
probabilistic extension

149
00:04:50,240 --> 00:04:53,199
of differential privacy cannot satisfy

150
00:04:53,199 --> 00:04:55,040
cross-processing property

151
00:04:55,040 --> 00:04:58,000
so does the privacy at risk does not

152
00:04:58,000 --> 00:04:59,759
satisfy

153
00:04:59,759 --> 00:05:02,960
post processing property privacy at risk

154
00:05:02,960 --> 00:05:05,600
although it does satisfy convexity

155
00:05:05,600 --> 00:05:06,560
property

156
00:05:06,560 --> 00:05:09,919
in theorem 3 we have extended the proof

157
00:05:09,919 --> 00:05:11,600
for advanced compositions in

158
00:05:11,600 --> 00:05:13,919
differential privacy to adapt to

159
00:05:13,919 --> 00:05:16,479
privacy at risk you can already see the

160
00:05:16,479 --> 00:05:17,440
resemblance

161
00:05:17,440 --> 00:05:20,240
in the result so far we have talked

162
00:05:20,240 --> 00:05:20,720
about

163
00:05:20,720 --> 00:05:23,120
the definition and related properties

164
00:05:23,120 --> 00:05:24,000
now let's look at

165
00:05:24,000 --> 00:05:26,320
how one can compute privacy at risk for

166
00:05:26,320 --> 00:05:28,240
a particular mechanism

167
00:05:28,240 --> 00:05:30,080
to compute there is gamma in the

168
00:05:30,080 --> 00:05:32,240
definition of privacy at risk we have to

169
00:05:32,240 --> 00:05:33,919
account for multiple sources of

170
00:05:33,919 --> 00:05:36,560
randomness firstly it can be explicit

171
00:05:36,560 --> 00:05:38,240
randomness induced by the privacy

172
00:05:38,240 --> 00:05:40,160
preserving mechanism

173
00:05:40,160 --> 00:05:42,400
a privacy preserving mechanism with one

174
00:05:42,400 --> 00:05:43,759
set of parameters can

175
00:05:43,759 --> 00:05:47,120
indeed satisfy this privacy guarantee

176
00:05:47,120 --> 00:05:48,880
with another sets of parameters

177
00:05:48,880 --> 00:05:51,039
for example one differentially private

178
00:05:51,039 --> 00:05:52,720
algorithm can always

179
00:05:52,720 --> 00:05:54,479
is is naturally too differentially

180
00:05:54,479 --> 00:05:56,319
private but it can also be

181
00:05:56,319 --> 00:05:58,720
0.5 differentially private with certain

182
00:05:58,720 --> 00:05:59,520
risk

183
00:05:59,520 --> 00:06:02,080
secondly to compute the risk we can

184
00:06:02,080 --> 00:06:02,720
consider

185
00:06:02,720 --> 00:06:05,039
implicit randomness induced by the data

186
00:06:05,039 --> 00:06:06,720
generation distribution

187
00:06:06,720 --> 00:06:08,800
the vanilla definition demands the bound

188
00:06:08,800 --> 00:06:09,919
to be true for

189
00:06:09,919 --> 00:06:11,919
all possible pairs of neighboring data

190
00:06:11,919 --> 00:06:14,160
sets but not all possible

191
00:06:14,160 --> 00:06:16,960
neighboring pairs are equally likely to

192
00:06:16,960 --> 00:06:18,479
appear in the practice

193
00:06:18,479 --> 00:06:20,880
we can exploit this caviar to provide a

194
00:06:20,880 --> 00:06:23,039
quantifiable bound on the risk

195
00:06:23,039 --> 00:06:25,039
such computations require us to work

196
00:06:25,039 --> 00:06:26,319
under a specific

197
00:06:26,319 --> 00:06:28,080
noise distribution as well as data

198
00:06:28,080 --> 00:06:29,840
generation distribution

199
00:06:29,840 --> 00:06:32,400
therefore we instantiate this idea on a

200
00:06:32,400 --> 00:06:34,880
widely used laplace mechanism

201
00:06:34,880 --> 00:06:36,960
the laplace mechanism is characterized

202
00:06:36,960 --> 00:06:38,160
by two parameters

203
00:06:38,160 --> 00:06:39,919
just like most of the mechanisms it

204
00:06:39,919 --> 00:06:41,600
requires sensitivity

205
00:06:41,600 --> 00:06:43,520
which depends on the query and the

206
00:06:43,520 --> 00:06:45,039
privacy budget

207
00:06:45,039 --> 00:06:48,080
let's go case by case let's consider

208
00:06:48,080 --> 00:06:50,560
the first case of explicit randomness in

209
00:06:50,560 --> 00:06:52,479
this case we assume that all numeric

210
00:06:52,479 --> 00:06:54,479
neighboring data sets are equally likely

211
00:06:54,479 --> 00:06:54,960
to

212
00:06:54,960 --> 00:06:57,280
occur and we assume that the sensitivity

213
00:06:57,280 --> 00:06:59,280
of the query is also known

214
00:06:59,280 --> 00:07:01,599
therefore for a specified value of say

215
00:07:01,599 --> 00:07:02,720
epsilon 0

216
00:07:02,720 --> 00:07:04,960
the laplace mechanism is already epsilon

217
00:07:04,960 --> 00:07:06,639
0 differentially private

218
00:07:06,639 --> 00:07:09,520
what we are interested in is is it

219
00:07:09,520 --> 00:07:11,360
epsilon differentially private for

220
00:07:11,360 --> 00:07:13,520
any value of epsilon that is less than

221
00:07:13,520 --> 00:07:14,960
epsilon 0

222
00:07:14,960 --> 00:07:17,440
we compute a closed form so formula in

223
00:07:17,440 --> 00:07:18,960
theorem 4 which is

224
00:07:18,960 --> 00:07:21,440
shown as equation 2 on the screen that

225
00:07:21,440 --> 00:07:22,319
connects

226
00:07:22,319 --> 00:07:25,520
epsilon 0 with epsilon and gamma in this

227
00:07:25,520 --> 00:07:27,680
plot we show the relationship between

228
00:07:27,680 --> 00:07:29,360
epsilon and gamma for the laplace

229
00:07:29,360 --> 00:07:31,759
mechanism with different parameters

230
00:07:31,759 --> 00:07:33,759
if you can focus on the blue curve you

231
00:07:33,759 --> 00:07:35,919
can see that one differentially private

232
00:07:35,919 --> 00:07:38,160
mechanism is almost 84

233
00:07:38,160 --> 00:07:41,280
of the times 0.8 differentially private

234
00:07:41,280 --> 00:07:43,440
by sliding along these curves one can

235
00:07:43,440 --> 00:07:46,080
fine tune the laplace mechanism

236
00:07:46,080 --> 00:07:48,400
why would one do this when it is already

237
00:07:48,400 --> 00:07:49,919
known that the mechanism is one

238
00:07:49,919 --> 00:07:51,520
differentially private

239
00:07:51,520 --> 00:07:53,599
a privacy budget of 0.8 is indeed a

240
00:07:53,599 --> 00:07:56,160
stronger privacy guarantee than 1.0

241
00:07:56,160 --> 00:07:58,000
it can be profitable for companies to

242
00:07:58,000 --> 00:07:59,360
provide a stronger

243
00:07:59,360 --> 00:08:01,919
guarantee with a quantifiable risk than

244
00:08:01,919 --> 00:08:04,000
providing a weaker guarantee

245
00:08:04,000 --> 00:08:06,639
additionally the privacy budget of 1.0

246
00:08:06,639 --> 00:08:09,039
induces a smaller quantity of noise

247
00:08:09,039 --> 00:08:12,080
than the privacy budget of 0.8 and hence

248
00:08:12,080 --> 00:08:14,000
it offers the higher utility

249
00:08:14,000 --> 00:08:16,000
thus we can obtain a better utility with

250
00:08:16,000 --> 00:08:17,680
a stronger privacy guarantee

251
00:08:17,680 --> 00:08:20,639
of course under an acceptable risk in

252
00:08:20,639 --> 00:08:22,240
the interest of time we will provide

253
00:08:22,240 --> 00:08:23,840
just a quick summary of the rest of the

254
00:08:23,840 --> 00:08:24,720
cases

255
00:08:24,720 --> 00:08:26,080
in the second case we study the

256
00:08:26,080 --> 00:08:28,160
randomness inherent in the training data

257
00:08:28,160 --> 00:08:28,639
set

258
00:08:28,639 --> 00:08:30,879
therefore all pairs of neighboring data

259
00:08:30,879 --> 00:08:32,799
sets are not equally probable

260
00:08:32,799 --> 00:08:34,640
it allows us to relax sensitivity

261
00:08:34,640 --> 00:08:37,599
computation sensitivity is in the end

262
00:08:37,599 --> 00:08:39,760
a worst case value it is an upper bound

263
00:08:39,760 --> 00:08:41,679
on the change on the output of the query

264
00:08:41,679 --> 00:08:43,440
for neighboring data sets

265
00:08:43,440 --> 00:08:45,680
we define sample sensitivity which is an

266
00:08:45,680 --> 00:08:48,080
estimate of the sensitivity computed by

267
00:08:48,080 --> 00:08:49,600
accounting for the noise

268
00:08:49,600 --> 00:08:51,120
induced by the data generation

269
00:08:51,120 --> 00:08:53,680
distribution we provide an upper bound

270
00:08:53,680 --> 00:08:55,920
on the sample sensitivity which can be

271
00:08:55,920 --> 00:08:56,399
tuned

272
00:08:56,399 --> 00:08:58,880
according to the specified risk in the

273
00:08:58,880 --> 00:08:59,680
third case

274
00:08:59,680 --> 00:09:02,800
we consider the coupled effect it

275
00:09:02,800 --> 00:09:04,399
computes the risk for a specified

276
00:09:04,399 --> 00:09:05,519
privacy budget

277
00:09:05,519 --> 00:09:08,480
as well as the sampled sensitivity value

278
00:09:08,480 --> 00:09:10,800
so far we have discussed how we can fine

279
00:09:10,800 --> 00:09:12,880
tune the privacy preserving mechanisms

280
00:09:12,880 --> 00:09:14,720
using privacy at risk

281
00:09:14,720 --> 00:09:16,160
it has indeed been an abstract

282
00:09:16,160 --> 00:09:18,240
discussion so let's move to the second

283
00:09:18,240 --> 00:09:20,480
contribution of the work which indeed

284
00:09:20,480 --> 00:09:22,000
reduces this

285
00:09:22,000 --> 00:09:25,360
abstractness all business entities that

286
00:09:25,360 --> 00:09:26,560
deal with the data

287
00:09:26,560 --> 00:09:29,600
from eu citizens they are abided by gdpr

288
00:09:29,600 --> 00:09:32,000
article 82 in gdpr states that every

289
00:09:32,000 --> 00:09:33,920
user has a right to seek compensation

290
00:09:33,920 --> 00:09:34,800
from the company

291
00:09:34,800 --> 00:09:37,120
in case of a privacy breach in such a

292
00:09:37,120 --> 00:09:39,440
case a company needs to reserve a budget

293
00:09:39,440 --> 00:09:39,920
for an

294
00:09:39,920 --> 00:09:42,399
unfortunate event of privacy breach we

295
00:09:42,399 --> 00:09:43,519
will show you how

296
00:09:43,519 --> 00:09:46,640
privacy risk privacy at risk helps in

297
00:09:46,640 --> 00:09:48,080
such a case

298
00:09:48,080 --> 00:09:50,080
let e be the compensation budget per

299
00:09:50,080 --> 00:09:51,360
stakeholder in

300
00:09:51,360 --> 00:09:54,160
in the case of privacy breach epsilon be

301
00:09:54,160 --> 00:09:56,240
the compensation budget per stakeholder

302
00:09:56,240 --> 00:09:57,920
if the company provides epsilon

303
00:09:57,920 --> 00:10:00,080
differential privacy solutions

304
00:10:00,080 --> 00:10:01,760
a cost model is a function that

305
00:10:01,760 --> 00:10:03,839
establishes the connection between e and

306
00:10:03,839 --> 00:10:04,959
epsilon

307
00:10:04,959 --> 00:10:07,040
the cost model has to satisfy following

308
00:10:07,040 --> 00:10:08,480
properties firstly

309
00:10:08,480 --> 00:10:10,160
e epsilon should be less than or equal

310
00:10:10,160 --> 00:10:12,320
to e for all values of epsilons

311
00:10:12,320 --> 00:10:14,399
if a company uses non-trivial ways to

312
00:10:14,399 --> 00:10:16,000
minimize privacy breaches

313
00:10:16,000 --> 00:10:18,240
it should not pay the same compensation

314
00:10:18,240 --> 00:10:19,440
it would have paid

315
00:10:19,440 --> 00:10:22,000
had it not taken any privacy measures

316
00:10:22,000 --> 00:10:24,079
secondly epsilon tends to zero as

317
00:10:24,079 --> 00:10:25,680
epsilon tends to zero

318
00:10:25,680 --> 00:10:27,760
when epsilon is zero all outputs are

319
00:10:27,760 --> 00:10:29,120
equally likely

320
00:10:29,120 --> 00:10:32,000
therefore it does not mean make sense to

321
00:10:32,000 --> 00:10:32,720
that such a

322
00:10:32,720 --> 00:10:34,399
useless mechanism would have revealed

323
00:10:34,399 --> 00:10:35,760
something which is

324
00:10:35,760 --> 00:10:39,600
useful therefore thirdly epsilon tends

325
00:10:39,600 --> 00:10:40,079
to e

326
00:10:40,079 --> 00:10:42,079
as epsilon tends to infinity when

327
00:10:42,079 --> 00:10:44,160
epsilon grows the privacy guarantee of

328
00:10:44,160 --> 00:10:46,000
the mechanism weakens with it

329
00:10:46,000 --> 00:10:48,560
therefore in the worst case epsilon

330
00:10:48,560 --> 00:10:50,320
should be same as e

331
00:10:50,320 --> 00:10:52,560
lastly the cost model should be

332
00:10:52,560 --> 00:10:54,560
monotonically increasing function of

333
00:10:54,560 --> 00:10:56,399
epsilon in between

334
00:10:56,399 --> 00:10:58,560
multiple functions satisfy these four

335
00:10:58,560 --> 00:10:59,680
characteristics but

336
00:10:59,680 --> 00:11:02,560
we chose one particular function that is

337
00:11:02,560 --> 00:11:03,760
a convex function

338
00:11:03,760 --> 00:11:06,000
most of our results naturally extend to

339
00:11:06,000 --> 00:11:06,959
the other choices

340
00:11:06,959 --> 00:11:09,600
as well we can further extend the cost

341
00:11:09,600 --> 00:11:10,160
model

342
00:11:10,160 --> 00:11:11,920
of differential privacy to privacy at

343
00:11:11,920 --> 00:11:14,079
risk we can show that epsilon zero

344
00:11:14,079 --> 00:11:16,079
differentially private mechanism can be

345
00:11:16,079 --> 00:11:18,240
epsilon gamma privacy at risk

346
00:11:18,240 --> 00:11:20,959
for certain choices of gamma and epsilon

347
00:11:20,959 --> 00:11:22,959
such a mechanism satisfies epsilon

348
00:11:22,959 --> 00:11:24,560
differential privacy with gamma

349
00:11:24,560 --> 00:11:26,480
probability and with one minus gamma

350
00:11:26,480 --> 00:11:28,720
probability it is still

351
00:11:28,720 --> 00:11:31,279
epsilon not differentially private this

352
00:11:31,279 --> 00:11:33,440
extension will justify the choice of

353
00:11:33,440 --> 00:11:36,720
convex function the

354
00:11:36,720 --> 00:11:39,839
convex combination of convex function is

355
00:11:39,839 --> 00:11:42,000
in the end a convex function thus the

356
00:11:42,000 --> 00:11:43,920
cost model for privacy at risk is a

357
00:11:43,920 --> 00:11:45,200
convex function

358
00:11:45,200 --> 00:11:47,360
the convexity ensures the existence of a

359
00:11:47,360 --> 00:11:49,600
unique value of epsilon that minimizes

360
00:11:49,600 --> 00:11:51,279
the overall budget

361
00:11:51,279 --> 00:11:53,680
let's have a realistic illustration to

362
00:11:53,680 --> 00:11:55,279
help you understand the advantage of

363
00:11:55,279 --> 00:11:56,560
this formulation

364
00:11:56,560 --> 00:11:58,399
let us consider an organization of

365
00:11:58,399 --> 00:12:00,079
hundred employees which is working

366
00:12:00,079 --> 00:12:03,200
on obesity survey among the employees

367
00:12:03,200 --> 00:12:05,200
actuarial research show that a person

368
00:12:05,200 --> 00:12:07,360
can expect up to 5500

369
00:12:07,360 --> 00:12:09,680
of increment in their health insurance

370
00:12:09,680 --> 00:12:10,639
premium

371
00:12:10,639 --> 00:12:12,639
if the obesity related status is

372
00:12:12,639 --> 00:12:15,760
revealed let that be our estimate for e

373
00:12:15,760 --> 00:12:18,639
the organization uses let's say 0.5

374
00:12:18,639 --> 00:12:21,360
differentially private laplace mechanism

375
00:12:21,360 --> 00:12:23,839
we can use the cost model and we can

376
00:12:23,839 --> 00:12:25,680
estimate the budget to be seventy four

377
00:12:25,680 --> 00:12:28,079
thousand dollars approximately

378
00:12:28,079 --> 00:12:30,480
the the organization decides to use

379
00:12:30,480 --> 00:12:32,639
privacy at risk to fine tune the laplace

380
00:12:32,639 --> 00:12:33,600
mechanism

381
00:12:33,600 --> 00:12:36,000
it computes the optimal level of privacy

382
00:12:36,000 --> 00:12:37,440
using the convexity

383
00:12:37,440 --> 00:12:40,680
and it turns out that it is 0 say 0.29

384
00:12:40,680 --> 00:12:43,279
0.64 privacy at risk

385
00:12:43,279 --> 00:12:46,320
if we use these values in the cost model

386
00:12:46,320 --> 00:12:47,200
for privacy

387
00:12:47,200 --> 00:12:50,079
at risk we will get the compensation

388
00:12:50,079 --> 00:12:52,000
budget of approximately thirty eight

389
00:12:52,000 --> 00:12:53,200
thousand dollars

390
00:12:53,200 --> 00:12:55,360
thus the use of privacy at risk allows

391
00:12:55,360 --> 00:12:57,519
the company to save approximately thirty

392
00:12:57,519 --> 00:12:58,639
six thousand dollars

393
00:12:58,639 --> 00:13:01,040
without compromising on the utility of

394
00:13:01,040 --> 00:13:02,720
their analysis

395
00:13:02,720 --> 00:13:04,880
we can also use privacy at risk to

396
00:13:04,880 --> 00:13:06,399
balance the trade-off between the

397
00:13:06,399 --> 00:13:08,079
utility and privacy

398
00:13:08,079 --> 00:13:10,399
we had conducted experiments on the 2000

399
00:13:10,399 --> 00:13:12,160
u.s census data set

400
00:13:12,160 --> 00:13:14,000
we trained a differentially private rich

401
00:13:14,000 --> 00:13:15,680
regression model to predict the

402
00:13:15,680 --> 00:13:18,480
income of a person based on the features

403
00:13:18,480 --> 00:13:20,800
such as marital status education status

404
00:13:20,800 --> 00:13:21,839
etc

405
00:13:21,839 --> 00:13:24,480
we use rmse as a metric of utility the

406
00:13:24,480 --> 00:13:25,920
y-axis in red

407
00:13:25,920 --> 00:13:28,800
shows the utility for various values of

408
00:13:28,800 --> 00:13:29,519
epsilon

409
00:13:29,519 --> 00:13:31,760
whereas the y-axis in blue shows the

410
00:13:31,760 --> 00:13:33,600
plot of privacy at risk confidence

411
00:13:33,600 --> 00:13:34,320
levels

412
00:13:34,320 --> 00:13:36,399
such a plot helps the data steward to

413
00:13:36,399 --> 00:13:38,079
choose a privacy parameter that

414
00:13:38,079 --> 00:13:40,320
satisfies utility requirement

415
00:13:40,320 --> 00:13:42,480
for instance if the desired rmse cannot

416
00:13:42,480 --> 00:13:44,160
be more than 0.2

417
00:13:44,160 --> 00:13:46,160
the permissible parameterization for

418
00:13:46,160 --> 00:13:48,240
privacy mechanism should be 0.8

419
00:13:48,240 --> 00:13:50,240
you can look in the red look at the red

420
00:13:50,240 --> 00:13:52,560
curve looking at the privacy at risk

421
00:13:52,560 --> 00:13:53,040
curves

422
00:13:53,040 --> 00:13:55,519
the data steward can fine tune and

423
00:13:55,519 --> 00:13:58,160
report the same mechanism to be 0.4

424
00:13:58,160 --> 00:14:00,399
differentially private 60 percent of the

425
00:14:00,399 --> 00:14:01,519
times

426
00:14:01,519 --> 00:14:04,079
please refer to the full paper for

427
00:14:04,079 --> 00:14:06,079
further illustrations of how different

428
00:14:06,079 --> 00:14:09,760
cases can be used by data steward

429
00:14:09,760 --> 00:14:11,920
in this work we proposed a probabilistic

430
00:14:11,920 --> 00:14:14,079
extension of differential privacy

431
00:14:14,079 --> 00:14:16,399
it provides a means to fine-tune the

432
00:14:16,399 --> 00:14:18,399
privacy preserving mechanisms if

433
00:14:18,399 --> 00:14:19,839
quantifiable risks are

434
00:14:19,839 --> 00:14:23,360
acceptable if we provide realistic

435
00:14:23,360 --> 00:14:25,600
illustrations of how a data steward can

436
00:14:25,600 --> 00:14:26,480
use

437
00:14:26,480 --> 00:14:29,199
such privacy at risk to balance the

438
00:14:29,199 --> 00:14:31,120
privacy utility trade-off

439
00:14:31,120 --> 00:14:33,519
we also proposed a cost model that helps

440
00:14:33,519 --> 00:14:35,839
to connect abstract privacy levels to a

441
00:14:35,839 --> 00:14:37,120
monetary budget

442
00:14:37,120 --> 00:14:39,040
such a cost model helps business

443
00:14:39,040 --> 00:14:41,600
organizations to use privacy solutions

444
00:14:41,600 --> 00:14:42,959
with ease

445
00:14:42,959 --> 00:14:45,440
which also illustrate how the choice of

446
00:14:45,440 --> 00:14:48,000
a convex function assists in the use of

447
00:14:48,000 --> 00:14:50,320
privacy at risk to fine-tune the privacy

448
00:14:50,320 --> 00:14:51,360
parameter

449
00:14:51,360 --> 00:14:54,480
to minimize the compensation budget

450
00:14:54,480 --> 00:14:57,720
to conclude we believe our work helps

451
00:14:57,720 --> 00:15:00,000
non-tech businesses

452
00:15:00,000 --> 00:15:02,560
non-tech business entities to choose an

453
00:15:02,560 --> 00:15:04,480
appropriate privacy solution

454
00:15:04,480 --> 00:15:08,000
without an extensive overhead of privacy

455
00:15:08,000 --> 00:15:09,920
expert

456
00:15:09,920 --> 00:15:13,839
thank you

