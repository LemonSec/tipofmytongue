1
00:00:00,000 --> 00:00:02,560
hello everyone i'm alex from rogue and

2
00:00:02,560 --> 00:00:03,840
i'm here to present you

3
00:00:03,840 --> 00:00:05,440
growing synthetic data through

4
00:00:05,440 --> 00:00:08,320
differentially private divine couples

5
00:00:08,320 --> 00:00:10,320
let us swiftly begin talking about

6
00:00:10,320 --> 00:00:11,599
synthetic data

7
00:00:11,599 --> 00:00:13,440
companies are catching up on the hype

8
00:00:13,440 --> 00:00:16,239
for the good man for the bad but

9
00:00:16,239 --> 00:00:18,000
here's the thing we need a way to

10
00:00:18,000 --> 00:00:19,520
privately share that

11
00:00:19,520 --> 00:00:22,080
can you synthetic that obedience is it

12
00:00:22,080 --> 00:00:23,279
the answer

13
00:00:23,279 --> 00:00:24,800
well we might to answer that question

14
00:00:24,800 --> 00:00:26,560
but it for sure seems to be an

15
00:00:26,560 --> 00:00:27,840
inadequate way to provide

16
00:00:27,840 --> 00:00:31,359
private yet realistic data and

17
00:00:31,359 --> 00:00:33,760
there's also a huge literature subject

18
00:00:33,760 --> 00:00:36,000
with few methods ensuring differential

19
00:00:36,000 --> 00:00:37,840
privacy by design

20
00:00:37,840 --> 00:00:41,360
here's three there's deepika which is a

21
00:00:41,360 --> 00:00:43,440
coupler-based generation they use

22
00:00:43,440 --> 00:00:44,960
different private histograms

23
00:00:44,960 --> 00:00:47,840
as well as deficient private estimation

24
00:00:47,840 --> 00:00:49,520
of a gaussian goblet estimated chain

25
00:00:49,520 --> 00:00:50,960
distribution

26
00:00:50,960 --> 00:00:54,160
major drawback from this model

27
00:00:54,160 --> 00:00:56,079
is that they simplify this dependent

28
00:00:56,079 --> 00:00:58,559
structure to a quotient model

29
00:00:58,559 --> 00:01:00,719
there's player base as a statistical

30
00:01:00,719 --> 00:01:01,680
based

31
00:01:01,680 --> 00:01:04,159
model or more precisely amazing

32
00:01:04,159 --> 00:01:05,920
network-based method

33
00:01:05,920 --> 00:01:07,760
and the major drawback from this one is

34
00:01:07,760 --> 00:01:09,200
that the greedy construction of the

35
00:01:09,200 --> 00:01:10,479
device network is

36
00:01:10,479 --> 00:01:13,040
time consuming which is may which makes

37
00:01:13,040 --> 00:01:15,840
it impractical for high dimensional data

38
00:01:15,840 --> 00:01:19,280
there's also deepikan which is a

39
00:01:19,280 --> 00:01:23,680
um which is for generating synthetic

40
00:01:23,680 --> 00:01:25,600
data with a private differential private

41
00:01:25,600 --> 00:01:29,439
gun frameworks but guns can be tricky to

42
00:01:29,439 --> 00:01:30,640
play with

43
00:01:30,640 --> 00:01:35,040
so here we come here we come in

44
00:01:35,040 --> 00:01:37,439
with coupler shirley for coppula based

45
00:01:37,439 --> 00:01:38,159
generation

46
00:01:38,159 --> 00:01:40,560
of synthetic differentially private data

47
00:01:40,560 --> 00:01:41,680
framework

48
00:01:41,680 --> 00:01:43,920
it is based on point couples to be able

49
00:01:43,920 --> 00:01:47,840
to model complex distributions

50
00:01:48,000 --> 00:01:51,280
and its main strengths is simple

51
00:01:51,280 --> 00:01:54,000
to use and deploy flexible and private

52
00:01:54,000 --> 00:01:55,439
simple due to the nature of the

53
00:01:55,439 --> 00:01:57,360
mathematical objects upon which it is

54
00:01:57,360 --> 00:01:58,000
based

55
00:01:58,000 --> 00:02:00,240
uh mainly density functions and

56
00:02:00,240 --> 00:02:01,200
histograms

57
00:02:01,200 --> 00:02:03,600
flexible due to the highly customizable

58
00:02:03,600 --> 00:02:05,040
framework centered around

59
00:02:05,040 --> 00:02:08,239
capitalism and private as

60
00:02:08,239 --> 00:02:10,639
it is definitely private by design and

61
00:02:10,639 --> 00:02:11,520
coupled with

62
00:02:11,520 --> 00:02:14,160
privacy tests for membership inference

63
00:02:14,160 --> 00:02:15,360
uh first thing first

64
00:02:15,360 --> 00:02:19,440
let's talk about couplers they date back

65
00:02:19,440 --> 00:02:23,760
to 1959 by they were formalized in 1959

66
00:02:23,760 --> 00:02:26,879
by scholar in the last decade couples

67
00:02:26,879 --> 00:02:28,480
have explained a significant role in

68
00:02:28,480 --> 00:02:29,920
many domains

69
00:02:29,920 --> 00:02:32,560
um in a nutshell coupler is a

70
00:02:32,560 --> 00:02:33,519
multivariate

71
00:02:33,519 --> 00:02:35,599
cumulative density function on the unit

72
00:02:35,599 --> 00:02:38,239
cube modeling the dependent structure

73
00:02:38,239 --> 00:02:40,720
with many families to pick from for the

74
00:02:40,720 --> 00:02:41,840
best fit

75
00:02:41,840 --> 00:02:44,160
coupled with the marginal densities or

76
00:02:44,160 --> 00:02:44,959
simply

77
00:02:44,959 --> 00:02:48,640
marginals the model is complete

78
00:02:48,640 --> 00:02:51,680
here are some key concepts uh centered

79
00:02:51,680 --> 00:02:52,879
around couplets

80
00:02:52,879 --> 00:02:55,120
absolute observations are ranked

81
00:02:55,120 --> 00:02:56,720
normalized data

82
00:02:56,720 --> 00:03:00,480
marginals are the univariate densities

83
00:03:00,480 --> 00:03:03,440
and the p i t which is the probability

84
00:03:03,440 --> 00:03:05,840
integral transform

85
00:03:05,840 --> 00:03:09,120
enables us to transform data to zero

86
00:03:09,120 --> 00:03:10,400
observation

87
00:03:10,400 --> 00:03:13,440
the whole process goes like that

88
00:03:13,440 --> 00:03:16,480
with data that we have here we estimated

89
00:03:16,480 --> 00:03:17,440
marginals

90
00:03:17,440 --> 00:03:21,120
given here from these marginals we can

91
00:03:21,120 --> 00:03:26,319
compute cumulative density functions

92
00:03:26,400 --> 00:03:29,200
which are used which are then applied to

93
00:03:29,200 --> 00:03:31,200
the data itself

94
00:03:31,200 --> 00:03:34,000
to produce pure observation this is the

95
00:03:34,000 --> 00:03:35,200
pit

96
00:03:35,200 --> 00:03:38,560
so the applying

97
00:03:38,560 --> 00:03:41,519
cdf to the data itself this is just the

98
00:03:41,519 --> 00:03:42,640
pid

99
00:03:42,640 --> 00:03:45,920
so with and with the observation we can

100
00:03:45,920 --> 00:03:48,000
then fit couplers

101
00:03:48,000 --> 00:03:51,120
from many families like here is the

102
00:03:51,120 --> 00:03:52,000
independent

103
00:03:52,000 --> 00:03:55,840
coupler fine calculus

104
00:03:55,840 --> 00:03:58,400
were introduced in the early 2000s to

105
00:03:58,400 --> 00:04:00,480
refine the modeling capacities

106
00:04:00,480 --> 00:04:03,840
of calculus the principle behind is

107
00:04:03,840 --> 00:04:05,920
simple decomposing the multivariate

108
00:04:05,920 --> 00:04:07,439
function of coupler

109
00:04:07,439 --> 00:04:11,519
into multiple bivariate couplers

110
00:04:11,519 --> 00:04:14,319
a vine is a nested a set of nested

111
00:04:14,319 --> 00:04:15,280
connected trees

112
00:04:15,280 --> 00:04:17,040
such that the edges of the tree

113
00:04:17,040 --> 00:04:19,040
corresponds to the vertexes of the

114
00:04:19,040 --> 00:04:20,238
following tree

115
00:04:20,238 --> 00:04:24,400
so here are two representation of a vine

116
00:04:24,400 --> 00:04:28,080
when i say the edge of a tree

117
00:04:28,080 --> 00:04:30,560
correspond to the vertex vertex of the

118
00:04:30,560 --> 00:04:32,160
following tree this is what

119
00:04:32,160 --> 00:04:35,840
we mean so edge vertex

120
00:04:35,840 --> 00:04:39,360
edge vertex here's a more

121
00:04:39,360 --> 00:04:41,759
clear example down here we have the

122
00:04:41,759 --> 00:04:42,880
variables

123
00:04:42,880 --> 00:04:45,199
and this is the coupler that models the

124
00:04:45,199 --> 00:04:46,479
two variables

125
00:04:46,479 --> 00:04:48,400
this scapula is the complete that

126
00:04:48,400 --> 00:04:50,880
modalizes the two

127
00:04:50,880 --> 00:04:54,160
couplers here this one modalize these

128
00:04:54,160 --> 00:04:54,800
two

129
00:04:54,800 --> 00:04:58,960
and so on the construction of a vine is

130
00:04:58,960 --> 00:04:59,759
not triple

131
00:04:59,759 --> 00:05:01,440
the current state-of-the-art technique

132
00:05:01,440 --> 00:05:03,280
for van selection

133
00:05:03,280 --> 00:05:06,479
vines uses a top-down greedy algorithm

134
00:05:06,479 --> 00:05:08,320
known as desmond's algorithm

135
00:05:08,320 --> 00:05:10,479
the technique is based on the hypothesis

136
00:05:10,479 --> 00:05:12,320
at the top of the line is more important

137
00:05:12,320 --> 00:05:14,160
in modeling the data than the lower

138
00:05:14,160 --> 00:05:14,880
branches

139
00:05:14,880 --> 00:05:16,720
as they contain raw information about

140
00:05:16,720 --> 00:05:19,199
correlation between attributes

141
00:05:19,199 --> 00:05:21,280
distance algorithm can be summarized in

142
00:05:21,280 --> 00:05:22,960
three steps

143
00:05:22,960 --> 00:05:24,800
uh choose the optimal tree that

144
00:05:24,800 --> 00:05:26,800
maximizes the sum of correlation

145
00:05:26,800 --> 00:05:29,600
coefficients between variables

146
00:05:29,600 --> 00:05:31,759
choose the optimal coupler family for

147
00:05:31,759 --> 00:05:34,639
each pair of attributes

148
00:05:34,639 --> 00:05:37,440
the the pair are imposed by the tree

149
00:05:37,440 --> 00:05:39,840
that minimizes the information loss

150
00:05:39,840 --> 00:05:43,120
then construct all the trees for the

151
00:05:43,120 --> 00:05:44,639
following level

152
00:05:44,639 --> 00:05:47,919
from the previous optimal tree

153
00:05:47,919 --> 00:05:51,199
and then go back to step one until

154
00:05:51,199 --> 00:05:54,320
note 3 can be constructed

155
00:05:54,320 --> 00:05:57,840
now for coupler shirley here's a high

156
00:05:57,840 --> 00:05:59,759
level overview

157
00:05:59,759 --> 00:06:02,720
four inputs so the training data set the

158
00:06:02,720 --> 00:06:04,000
privacy budget

159
00:06:04,000 --> 00:06:05,840
the number of records to generate and

160
00:06:05,840 --> 00:06:07,440
the encoding method

161
00:06:07,440 --> 00:06:09,759
uh the encoding method here is quite

162
00:06:09,759 --> 00:06:10,639
important

163
00:06:10,639 --> 00:06:13,120
this is the next slide three steps

164
00:06:13,120 --> 00:06:14,000
pre-processing

165
00:06:14,000 --> 00:06:16,720
vine selection and that generation quite

166
00:06:16,720 --> 00:06:17,280
simple

167
00:06:17,280 --> 00:06:20,639
indeed why that encoding well

168
00:06:20,639 --> 00:06:23,039
couplers mostly use rank correlation for

169
00:06:23,039 --> 00:06:24,479
modeling dependencies

170
00:06:24,479 --> 00:06:26,479
so we need an encoding technique for

171
00:06:26,479 --> 00:06:29,120
handling categorical values

172
00:06:29,120 --> 00:06:31,440
we studied four possible encoding

173
00:06:31,440 --> 00:06:32,479
techniques uh

174
00:06:32,479 --> 00:06:34,000
of course there are more available out

175
00:06:34,000 --> 00:06:35,600
there uh the first

176
00:06:35,600 --> 00:06:39,160
one is a common trick that is to view

177
00:06:39,160 --> 00:06:42,160
categorical data simply as discrete data

178
00:06:42,160 --> 00:06:44,960
in which the order is chosen arbitrarily

179
00:06:44,960 --> 00:06:48,160
uh for example our alphabetical order

180
00:06:48,160 --> 00:06:50,319
this is known as the ordinal encoding

181
00:06:50,319 --> 00:06:52,160
another another technique is the use of

182
00:06:52,160 --> 00:06:53,520
dummy variables in which

183
00:06:53,520 --> 00:06:55,280
categorical values are transformed into

184
00:06:55,280 --> 00:06:56,880
binary indicator variables

185
00:06:56,880 --> 00:06:59,680
and this is known as one heart encoding

186
00:06:59,680 --> 00:07:00,800
there are also

187
00:07:00,800 --> 00:07:03,039
we studied we also studied two

188
00:07:03,039 --> 00:07:05,199
supervised encoding techniques which are

189
00:07:05,199 --> 00:07:06,240
the weight of evidence

190
00:07:06,240 --> 00:07:08,400
and the general linear mixed model

191
00:07:08,400 --> 00:07:09,680
encoding

192
00:07:09,680 --> 00:07:12,080
and supervised because they both need a

193
00:07:12,080 --> 00:07:13,840
reference attribute called the predictor

194
00:07:13,840 --> 00:07:15,680
to encode the categorical attributes in

195
00:07:15,680 --> 00:07:17,280
a way that maximizes the correlation

196
00:07:17,280 --> 00:07:20,000
between the pair

197
00:07:20,400 --> 00:07:23,199
first step is preprocessing so this is

198
00:07:23,199 --> 00:07:25,360
quite a crucial step because it

199
00:07:25,360 --> 00:07:27,919
it includes the encoding of categorical

200
00:07:27,919 --> 00:07:29,440
value

201
00:07:29,440 --> 00:07:32,560
and we need to split the data set for

202
00:07:32,560 --> 00:07:35,039
enemy cdfs in the selecting of

203
00:07:35,039 --> 00:07:37,919
one model in a differentially private

204
00:07:37,919 --> 00:07:38,880
way

205
00:07:38,880 --> 00:07:41,280
um since our framework needs two

206
00:07:41,280 --> 00:07:43,510
sequential training processes that is

207
00:07:43,510 --> 00:07:44,800
[Music]

208
00:07:44,800 --> 00:07:46,960
the differential private cognitive

209
00:07:46,960 --> 00:07:48,000
density functions

210
00:07:48,000 --> 00:07:50,800
and the vine coupler model will rely

211
00:07:50,800 --> 00:07:52,639
more on the parallel composition

212
00:07:52,639 --> 00:07:54,720
of the prior differential privacy and

213
00:07:54,720 --> 00:07:56,560
splitting the input data set into two

214
00:07:56,560 --> 00:07:57,440
distant subsets

215
00:07:57,440 --> 00:07:58,720
instead of using the sequential

216
00:07:58,720 --> 00:08:00,720
composition of the differential privacy

217
00:08:00,720 --> 00:08:04,960
and building the global budget epsilon

218
00:08:04,960 --> 00:08:07,120
the third step is computing the

219
00:08:07,120 --> 00:08:10,000
cumulative intensity functions

220
00:08:10,000 --> 00:08:13,199
we do that by the way of histograms

221
00:08:13,199 --> 00:08:15,520
uh by the way of different private

222
00:08:15,520 --> 00:08:17,759
histograms to estimate the probability

223
00:08:17,759 --> 00:08:19,919
density functions

224
00:08:19,919 --> 00:08:22,720
then a simple normalized cumulative sum

225
00:08:22,720 --> 00:08:24,479
over the pin counts provide a good

226
00:08:24,479 --> 00:08:27,919
enough estimation of cdfs with the cdfs

227
00:08:27,919 --> 00:08:28,800
we can now compute

228
00:08:28,800 --> 00:08:33,360
two observations because as i repeated

229
00:08:33,360 --> 00:08:37,039
as a repeat couplers can only model

230
00:08:37,039 --> 00:08:40,159
uniform standard marginals

231
00:08:40,159 --> 00:08:43,039
suit observations but easily enough the

232
00:08:43,039 --> 00:08:45,120
probability integral transform does that

233
00:08:45,120 --> 00:08:46,560
exact thing for us

234
00:08:46,560 --> 00:08:49,680
the second step is their fine selection

235
00:08:49,680 --> 00:08:52,240
we propose the use of distance algorithm

236
00:08:52,240 --> 00:08:54,399
on the noise observations

237
00:08:54,399 --> 00:08:58,000
as it is the the state of the art

238
00:08:58,000 --> 00:09:00,080
and this is where we kind of set apart

239
00:09:00,080 --> 00:09:01,279
in the literature

240
00:09:01,279 --> 00:09:03,839
because the existing previous work the

241
00:09:03,839 --> 00:09:06,320
intersection of differential privacy

242
00:09:06,320 --> 00:09:08,160
and couplers are restricted to gaussian

243
00:09:08,160 --> 00:09:10,480
couplers and as it

244
00:09:10,480 --> 00:09:12,959
as it is easier to build a gaussian

245
00:09:12,959 --> 00:09:14,399
couplers in a differential

246
00:09:14,399 --> 00:09:17,200
private manner it's quite simple in fact

247
00:09:17,200 --> 00:09:19,760
the use of vanco plus in the same

248
00:09:19,760 --> 00:09:22,720
in the same way is a bit more complex as

249
00:09:22,720 --> 00:09:25,200
there are multiple sequential estimation

250
00:09:25,200 --> 00:09:27,279
which would result in an overall huge

251
00:09:27,279 --> 00:09:28,240
injection of

252
00:09:28,240 --> 00:09:30,800
noise due to differential privacy in

253
00:09:30,800 --> 00:09:31,360
contrast

254
00:09:31,360 --> 00:09:34,480
couples surely reduces the complexity by

255
00:09:34,480 --> 00:09:36,959
computing the dependencies on the noisy

256
00:09:36,959 --> 00:09:39,040
two observations rather than computing

257
00:09:39,040 --> 00:09:40,480
tp parameters

258
00:09:40,480 --> 00:09:42,000
and thus enabling the user to find

259
00:09:42,000 --> 00:09:44,959
calculus in a private manner

260
00:09:44,959 --> 00:09:47,920
um differential privacy is guaranteed as

261
00:09:47,920 --> 00:09:49,600
both the pre-processing and device

262
00:09:49,600 --> 00:09:51,040
selection only interacts

263
00:09:51,040 --> 00:09:54,160
with the data through db mechanism

264
00:09:54,160 --> 00:09:56,640
the last step is data generation it is

265
00:09:56,640 --> 00:09:58,399
the simplest one

266
00:09:58,399 --> 00:10:01,440
so first is sampling

267
00:10:01,440 --> 00:10:04,560
observations from the vine using inverse

268
00:10:04,560 --> 00:10:06,640
probability integral transform to map

269
00:10:06,640 --> 00:10:07,680
back the data to its

270
00:10:07,680 --> 00:10:09,600
original domain and then reversing the

271
00:10:09,600 --> 00:10:12,480
encoding if necessary

272
00:10:12,480 --> 00:10:15,600
for the experimental evaluation

273
00:10:15,600 --> 00:10:18,079
here's the setting we used three data

274
00:10:18,079 --> 00:10:20,800
sets adult compass and texas hospital

275
00:10:20,800 --> 00:10:25,200
the later one we sampled down to 150

276
00:10:25,200 --> 00:10:28,399
000 records and 17 attributes

277
00:10:28,399 --> 00:10:32,720
due to a computation limitation

278
00:10:32,720 --> 00:10:36,160
we used a k-fold cross-validation with

279
00:10:36,160 --> 00:10:38,160
5-fold

280
00:10:38,160 --> 00:10:41,360
for each fold the mall

281
00:10:41,360 --> 00:10:44,480
generated the same number of records as

282
00:10:44,480 --> 00:10:45,760
the training set

283
00:10:45,760 --> 00:10:49,120
we compared also five values of uh

284
00:10:49,120 --> 00:10:52,320
epsilon the tp budget so zero

285
00:10:52,320 --> 00:10:55,120
point zero one point one one and eight

286
00:10:55,120 --> 00:10:56,880
zero here means that the difference

287
00:10:56,880 --> 00:10:57,519
privacy

288
00:10:57,519 --> 00:11:00,560
is disabled we compared

289
00:11:00,560 --> 00:11:03,600
three differentially private algorithms

290
00:11:03,600 --> 00:11:07,120
db histograms which is simply computing

291
00:11:07,120 --> 00:11:09,279
the dps to grants and then sampling from

292
00:11:09,279 --> 00:11:10,320
them

293
00:11:10,320 --> 00:11:12,640
driftbase and db coupler that we

294
00:11:12,640 --> 00:11:13,519
introduced

295
00:11:13,519 --> 00:11:17,519
a bit before we used a few metrics

296
00:11:17,519 --> 00:11:20,880
so to test the statistical similarity we

297
00:11:20,880 --> 00:11:23,279
use the common graphs and of distance

298
00:11:23,279 --> 00:11:25,600
and the mean correlation coefficient

299
00:11:25,600 --> 00:11:26,560
difference

300
00:11:26,560 --> 00:11:29,680
we also use the mat use correlation

301
00:11:29,680 --> 00:11:30,640
coefficient on

302
00:11:30,640 --> 00:11:32,399
two classification tasks which is the

303
00:11:32,399 --> 00:11:34,000
binary and multi-label

304
00:11:34,000 --> 00:11:37,040
classification when we use the root mean

305
00:11:37,040 --> 00:11:39,120
square error on our linear regression

306
00:11:39,120 --> 00:11:40,000
task

307
00:11:40,000 --> 00:11:42,480
we also tested four parameters so to

308
00:11:42,480 --> 00:11:44,000
splitting ratios

309
00:11:44,000 --> 00:11:47,839
uh between between learning the

310
00:11:47,839 --> 00:11:50,880
cdfs and learning the vine

311
00:11:50,880 --> 00:11:54,560
we also tested the encoding techniques

312
00:11:54,560 --> 00:11:56,639
some db mechanism and the truncation

313
00:11:56,639 --> 00:11:58,000
level of a vine because

314
00:11:58,000 --> 00:12:01,040
you can stop the the select the desmond

315
00:12:01,040 --> 00:12:01,839
algorithm at

316
00:12:01,839 --> 00:12:05,920
any level we also evaluated the privacy

317
00:12:05,920 --> 00:12:08,079
through a membership inference attack

318
00:12:08,079 --> 00:12:09,760
we used a monte carlo membership

319
00:12:09,760 --> 00:12:12,200
inference attack introduced in

320
00:12:12,200 --> 00:12:15,519
2019 at pets uh the main strength of

321
00:12:15,519 --> 00:12:16,560
this attack is

322
00:12:16,560 --> 00:12:19,680
a nonparametric in its agnostic model

323
00:12:19,680 --> 00:12:23,760
um the privacy score is

324
00:12:23,760 --> 00:12:27,519
can be briefly briefly summarized as the

325
00:12:27,519 --> 00:12:28,320
ratio of

326
00:12:28,320 --> 00:12:30,240
training records successfully

327
00:12:30,240 --> 00:12:32,560
distinguished from control records

328
00:12:32,560 --> 00:12:35,120
one would be perfect distinguishability

329
00:12:35,120 --> 00:12:38,399
in 0.5 will be indistinguishability

330
00:12:38,399 --> 00:12:40,720
uh the best configuration that we

331
00:12:40,720 --> 00:12:41,600
observed

332
00:12:41,600 --> 00:12:45,040
for the splitting ratio is 50 50.

333
00:12:45,040 --> 00:12:47,680
well there was no clear consensus so we

334
00:12:47,680 --> 00:12:48,480
kind of

335
00:12:48,480 --> 00:12:51,839
used 50 50 here had encoded um the

336
00:12:51,839 --> 00:12:53,440
anchoring method for categorical

337
00:12:53,440 --> 00:12:54,800
attributes we used the weight of

338
00:12:54,800 --> 00:12:56,240
evidence encoder

339
00:12:56,240 --> 00:12:58,000
uh the difference of privacy mechanism

340
00:12:58,000 --> 00:12:59,839
we used a geometric

341
00:12:59,839 --> 00:13:03,200
mechanism as it is a re uh

342
00:13:03,200 --> 00:13:04,959
a refined version of the lackluster

343
00:13:04,959 --> 00:13:06,720
mechanism for

344
00:13:06,720 --> 00:13:10,639
discrete histogram histogram

345
00:13:10,639 --> 00:13:14,320
um histogram computation

346
00:13:14,320 --> 00:13:17,440
which is that's why it performed a bit a

347
00:13:17,440 --> 00:13:17,839
bit

348
00:13:17,839 --> 00:13:21,440
more um better in that way

349
00:13:21,440 --> 00:13:25,279
and we we

350
00:13:25,279 --> 00:13:29,279
we also view that uh

351
00:13:29,279 --> 00:13:31,440
from custom level of divine copper model

352
00:13:31,440 --> 00:13:32,560
doesn't impact

353
00:13:32,560 --> 00:13:36,560
much of there was not a huge gain

354
00:13:36,560 --> 00:13:40,399
in um in utility

355
00:13:40,399 --> 00:13:43,920
past the second level

356
00:13:44,240 --> 00:13:48,000
the so the overall result is that our

357
00:13:48,000 --> 00:13:50,399
method is second to preface on

358
00:13:50,399 --> 00:13:54,079
ml tasks uh it is the it has the highest

359
00:13:54,079 --> 00:13:55,600
distribution similarity

360
00:13:55,600 --> 00:13:58,639
um it's comparable to tv histogram in

361
00:13:58,639 --> 00:13:59,680
that fact

362
00:13:59,680 --> 00:14:02,320
and there's a better protection on large

363
00:14:02,320 --> 00:14:03,760
datasets

364
00:14:03,760 --> 00:14:08,079
and it's also uh quite faster

365
00:14:08,079 --> 00:14:11,279
uh for classification uh the dot the

366
00:14:11,279 --> 00:14:12,560
dashed lines

367
00:14:12,560 --> 00:14:16,639
are at the the baseline which is uh

368
00:14:16,639 --> 00:14:20,160
computed on the right asset um

369
00:14:20,160 --> 00:14:24,320
prev base performs outperforms are

370
00:14:24,320 --> 00:14:26,800
our method but our method seems to be a

371
00:14:26,800 --> 00:14:28,160
bit more stable

372
00:14:28,160 --> 00:14:31,760
in that way if you check the the width

373
00:14:31,760 --> 00:14:32,639
of the

374
00:14:32,639 --> 00:14:35,920
of the status then

375
00:14:35,920 --> 00:14:39,279
for the regression and correlation uh

376
00:14:39,279 --> 00:14:41,360
the root needs greater the lower better

377
00:14:41,360 --> 00:14:43,360
and the same is for the mean correlation

378
00:14:43,360 --> 00:14:44,240
delta

379
00:14:44,240 --> 00:14:47,519
and here brief base

380
00:14:47,519 --> 00:14:50,560
also outperforms a bit uh

381
00:14:50,560 --> 00:14:53,600
couple surely but still capacitor is

382
00:14:53,600 --> 00:14:58,160
more stable overall in that matter

383
00:14:58,160 --> 00:15:00,560
uh as for the distribution similarity

384
00:15:00,560 --> 00:15:01,440
and privacy

385
00:15:01,440 --> 00:15:04,639
uh goblet surely is often

386
00:15:04,639 --> 00:15:08,000
offers uh the close or the

387
00:15:08,000 --> 00:15:11,120
most similar distributions uh

388
00:15:11,120 --> 00:15:14,320
way ahead of brief pace in that matter

389
00:15:14,320 --> 00:15:18,079
and for the privacy um

390
00:15:18,079 --> 00:15:21,120
here is the closer to 0.5 is the better

391
00:15:21,120 --> 00:15:23,600
so for texas which is the biggest data

392
00:15:23,600 --> 00:15:25,120
set uh couples surely

393
00:15:25,120 --> 00:15:28,399
provided a better protection

394
00:15:28,399 --> 00:15:31,519
same here for for

395
00:15:31,519 --> 00:15:34,800
adult it did not outperform

396
00:15:34,800 --> 00:15:39,120
brave base on compass though

397
00:15:39,120 --> 00:15:42,480
uh we also did an additional

398
00:15:42,480 --> 00:15:44,320
analysis on multivariate correlation we

399
00:15:44,320 --> 00:15:45,839
created a small synthetic dataset

400
00:15:45,839 --> 00:15:46,399
composed of

401
00:15:46,399 --> 00:15:49,360
type 5000 records and six attribute with

402
00:15:49,360 --> 00:15:49,920
various

403
00:15:49,920 --> 00:15:51,600
various correlation coefficients between

404
00:15:51,600 --> 00:15:53,440
the attributes to study the model

405
00:15:53,440 --> 00:15:54,959
strength in capturing the

406
00:15:54,959 --> 00:15:57,120
dependent structure and the data that

407
00:15:57,120 --> 00:15:58,800
that was created so that

408
00:15:58,800 --> 00:16:01,920
some attributes are highly positive

409
00:16:01,920 --> 00:16:04,160
are highly positively correlated when

410
00:16:04,160 --> 00:16:05,519
the values

411
00:16:05,519 --> 00:16:07,759
of the first attribute is better zero

412
00:16:07,759 --> 00:16:08,560
and only slightly

413
00:16:08,560 --> 00:16:11,120
negatively correlated when the values

414
00:16:11,120 --> 00:16:12,480
are above zeros

415
00:16:12,480 --> 00:16:15,759
we can see here so above below zero is

416
00:16:15,759 --> 00:16:16,480
highly

417
00:16:16,480 --> 00:16:19,680
positive and above zero is

418
00:16:19,680 --> 00:16:22,959
negatively in slightly

419
00:16:22,959 --> 00:16:26,480
um both capsule

420
00:16:26,480 --> 00:16:28,160
couplers shirley and brief base

421
00:16:28,160 --> 00:16:30,560
performed quite well for the

422
00:16:30,560 --> 00:16:33,279
for the four

423
00:16:33,759 --> 00:16:37,040
highly correlated and slightly

424
00:16:37,040 --> 00:16:38,160
correlated

425
00:16:38,160 --> 00:16:40,720
and but pretty base outperform

426
00:16:40,720 --> 00:16:41,600
completely

427
00:16:41,600 --> 00:16:44,880
when it comes to splitted

428
00:16:44,880 --> 00:16:48,839
correlation correlations and

429
00:16:48,839 --> 00:16:51,839
data

430
00:16:56,880 --> 00:17:00,320
a bit more than a brief base so here's a

431
00:17:00,320 --> 00:17:01,759
bit of a summer result

432
00:17:01,759 --> 00:17:03,360
completely surely displays the highest

433
00:17:03,360 --> 00:17:04,959
ethical similarity between the run

434
00:17:04,959 --> 00:17:08,240
synthesized data uh brief base is often

435
00:17:08,240 --> 00:17:10,079
the winner in terms of classification

436
00:17:10,079 --> 00:17:12,079
tasks with coverage very often being

437
00:17:12,079 --> 00:17:15,119
close second typical and deep histogram

438
00:17:15,119 --> 00:17:17,199
just by the worst agility

439
00:17:17,199 --> 00:17:20,319
so complex surely can be seen as an

440
00:17:20,319 --> 00:17:22,640
upgrade of those two

441
00:17:22,640 --> 00:17:24,880
uh one thing to mention is capacitor is

442
00:17:24,880 --> 00:17:26,240
much faster than pretty base

443
00:17:26,240 --> 00:17:29,039
and we were also made aware a day prior

444
00:17:29,039 --> 00:17:30,480
to recording the video that the

445
00:17:30,480 --> 00:17:32,559
implementation of cliff base we use

446
00:17:32,559 --> 00:17:35,120
violates the formal assumption of its tb

447
00:17:35,120 --> 00:17:36,080
guarantees

448
00:17:36,080 --> 00:17:40,480
so we will not discuss the brave base

449
00:17:40,480 --> 00:17:45,360
versus copula surely protection here

450
00:17:45,360 --> 00:17:48,200
so this that's it here's some

451
00:17:48,200 --> 00:17:49,440
perspectives

452
00:17:49,440 --> 00:17:53,440
on the future work uh thank you for

453
00:17:53,440 --> 00:17:56,480
your time i've been alexander hogamon

454
00:17:56,480 --> 00:17:59,440
you've been you

