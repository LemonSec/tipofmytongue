1
00:00:00,320 --> 00:00:04,080
hi i'm jory from university of georgia

2
00:00:04,080 --> 00:00:06,560
today i'm going to introduce my work on

3
00:00:06,560 --> 00:00:07,279
scalable

4
00:00:07,279 --> 00:00:08,880
differentially private deep learning

5
00:00:08,880 --> 00:00:11,120
framework this is a joint work with

6
00:00:11,120 --> 00:00:12,240
daniel kiefer

7
00:00:12,240 --> 00:00:15,280
at penn state university

8
00:00:15,280 --> 00:00:17,039
in this world we are going to consider

9
00:00:17,039 --> 00:00:18,480
training a neural network

10
00:00:18,480 --> 00:00:21,279
on the privacy constraint although there

11
00:00:21,279 --> 00:00:23,600
exists many algorithms you can use to

12
00:00:23,600 --> 00:00:25,199
train on your network

13
00:00:25,199 --> 00:00:27,439
in practice one typically uses

14
00:00:27,439 --> 00:00:29,359
stochastic gradient descent

15
00:00:29,359 --> 00:00:32,479
and back propagation algorithm the back

16
00:00:32,479 --> 00:00:34,719
propagation algorithm consists of two

17
00:00:34,719 --> 00:00:36,160
phases

18
00:00:36,160 --> 00:00:38,800
during the fourth step we pass the input

19
00:00:38,800 --> 00:00:40,000
through the network

20
00:00:40,000 --> 00:00:43,040
to obtain the output

21
00:00:43,040 --> 00:00:46,079
the loss function l defines the error

22
00:00:46,079 --> 00:00:49,120
of our neural network f which is

23
00:00:49,120 --> 00:00:51,280
the difference between the output of the

24
00:00:51,280 --> 00:00:52,399
neural network

25
00:00:52,399 --> 00:00:55,760
and the ground truth

26
00:00:56,399 --> 00:00:58,800
during the backward step this error

27
00:00:58,800 --> 00:00:59,680
information

28
00:00:59,680 --> 00:01:02,640
is propagated back through the network

29
00:01:02,640 --> 00:01:04,159
from the top to bottom

30
00:01:04,159 --> 00:01:08,080
layer by layer using the gradient

31
00:01:08,080 --> 00:01:10,080
calculated by this back propagation

32
00:01:10,080 --> 00:01:11,600
algorithm we can

33
00:01:11,600 --> 00:01:13,920
update the parameter of our neural

34
00:01:13,920 --> 00:01:15,439
network

35
00:01:15,439 --> 00:01:18,479
in this work we are going to assume our

36
00:01:18,479 --> 00:01:20,240
input data set d

37
00:01:20,240 --> 00:01:23,119
contains some sensitive information for

38
00:01:23,119 --> 00:01:24,080
example

39
00:01:24,080 --> 00:01:28,960
medical records or insurance claims

40
00:01:30,000 --> 00:01:32,960
the specific notion of privacy that we

41
00:01:32,960 --> 00:01:34,479
will be using in this

42
00:01:34,479 --> 00:01:38,079
project is differential privacy

43
00:01:38,079 --> 00:01:40,560
differential privacy is a mathematically

44
00:01:40,560 --> 00:01:42,640
rigorous notion of privacy

45
00:01:42,640 --> 00:01:45,520
that provides strong privacy protection

46
00:01:45,520 --> 00:01:47,920
against the adversaries with background

47
00:01:47,920 --> 00:01:49,119
knowledge

48
00:01:49,119 --> 00:01:52,079
before we define differential privacy we

49
00:01:52,079 --> 00:01:54,240
first need to define

50
00:01:54,240 --> 00:01:57,600
neighboring data sets we say two data

51
00:01:57,600 --> 00:01:58,159
sets

52
00:01:58,159 --> 00:02:01,280
d1 and d2 are neighboring

53
00:02:01,280 --> 00:02:04,159
if they are exactly the same except one

54
00:02:04,159 --> 00:02:05,840
individual's record

55
00:02:05,840 --> 00:02:08,318
for example as shown in the figure the

56
00:02:08,318 --> 00:02:09,038
one with

57
00:02:09,038 --> 00:02:14,000
alice and one without alice record

58
00:02:14,000 --> 00:02:17,280
differential privacy enforces that the

59
00:02:17,280 --> 00:02:21,840
output distribution of the algorithm m

60
00:02:21,840 --> 00:02:22,319
over

61
00:02:22,319 --> 00:02:25,680
two neighboring data set d1 and d2

62
00:02:25,680 --> 00:02:28,959
are indistinguishable to the adversary

63
00:02:28,959 --> 00:02:31,200
given the output of the algorithm the

64
00:02:31,200 --> 00:02:33,920
adversary cannot distinguish

65
00:02:33,920 --> 00:02:38,400
the d1 from d2

66
00:02:38,400 --> 00:02:41,440
and hence the presence or absence of

67
00:02:41,440 --> 00:02:45,920
alice is hidden to the adversary

68
00:02:45,920 --> 00:02:48,319
there also exists other variants of

69
00:02:48,319 --> 00:02:50,080
differential privacy

70
00:02:50,080 --> 00:02:53,280
for example the moment accountant or

71
00:02:53,280 --> 00:02:56,319
zero concentrated differential privacy

72
00:02:56,319 --> 00:02:59,360
and rainy differential privacy

73
00:02:59,360 --> 00:03:02,560
in this work we use the

74
00:03:02,560 --> 00:03:05,680
rainy differential privacy to trace the

75
00:03:05,680 --> 00:03:10,000
privacy loss during the training

76
00:03:10,319 --> 00:03:13,599
this diverges based definition of

77
00:03:13,599 --> 00:03:15,120
differential privacy

78
00:03:15,120 --> 00:03:20,159
ensures that the output distribution of

79
00:03:20,480 --> 00:03:23,519
our algorithm n over two enabling data

80
00:03:23,519 --> 00:03:24,000
sets

81
00:03:24,000 --> 00:03:27,280
d and d prime are close to each other

82
00:03:27,280 --> 00:03:29,599
and it measured the distance between

83
00:03:29,599 --> 00:03:31,440
these two distributions

84
00:03:31,440 --> 00:03:34,560
using the diverse measures and this

85
00:03:34,560 --> 00:03:37,120
is expressed as the constraint on the

86
00:03:37,120 --> 00:03:37,760
moment

87
00:03:37,760 --> 00:03:42,000
of privacy loss random variable

88
00:03:42,000 --> 00:03:44,000
an important concept in differential

89
00:03:44,000 --> 00:03:46,720
privacy is the sensitivity

90
00:03:46,720 --> 00:03:49,760
sensitivity of a creative function q is

91
00:03:49,760 --> 00:03:52,239
defined as the largest difference

92
00:03:52,239 --> 00:03:54,560
of function outputs between two

93
00:03:54,560 --> 00:03:56,560
neighboring data sets

94
00:03:56,560 --> 00:03:59,920
intuitively this sensitivity measures

95
00:03:59,920 --> 00:04:01,840
how much

96
00:04:01,840 --> 00:04:05,120
difference one individual can makes on

97
00:04:05,120 --> 00:04:08,000
the output of a function

98
00:04:08,000 --> 00:04:10,959
in our context it measures how much

99
00:04:10,959 --> 00:04:11,840
influence

100
00:04:11,840 --> 00:04:16,720
one individual can have on the gradient

101
00:04:18,320 --> 00:04:20,720
okay so we are going to look at the

102
00:04:20,720 --> 00:04:21,839
details of

103
00:04:21,839 --> 00:04:25,599
neural network training steps

104
00:04:25,680 --> 00:04:29,840
first we sample a mini batch of size b

105
00:04:29,840 --> 00:04:33,520
from the input data set

106
00:04:33,520 --> 00:04:37,199
for each example in the mini batch

107
00:04:37,199 --> 00:04:41,120
we compute the per example gradient

108
00:04:41,120 --> 00:04:44,560
and we take this summation or average

109
00:04:44,560 --> 00:04:48,320
to obtain an aggregate gradient

110
00:04:48,320 --> 00:04:51,440
once we have this aggregate gradient

111
00:04:51,440 --> 00:04:54,639
then using these gradient descent update

112
00:04:54,639 --> 00:04:55,360
equation

113
00:04:55,360 --> 00:04:57,440
we can update the parameter of our

114
00:04:57,440 --> 00:05:00,080
neural network

115
00:05:00,080 --> 00:05:03,520
now we turn our attention into

116
00:05:03,520 --> 00:05:05,360
the differentially private neural

117
00:05:05,360 --> 00:05:08,000
network training

118
00:05:08,000 --> 00:05:10,880
as before we start by sampling our mini

119
00:05:10,880 --> 00:05:11,440
batch

120
00:05:11,440 --> 00:05:14,639
from the input data set and for each

121
00:05:14,639 --> 00:05:15,919
example

122
00:05:15,919 --> 00:05:18,479
in our mini batch we compute the per

123
00:05:18,479 --> 00:05:19,120
example

124
00:05:19,120 --> 00:05:23,440
gradient once the per example gradients

125
00:05:23,440 --> 00:05:25,039
are calculated

126
00:05:25,039 --> 00:05:27,840
then we inspect the norm of each

127
00:05:27,840 --> 00:05:29,199
gradient

128
00:05:29,199 --> 00:05:31,199
if the norm is greater than the

129
00:05:31,199 --> 00:05:32,479
predefined

130
00:05:32,479 --> 00:05:35,680
clipping threshold c then we click the

131
00:05:35,680 --> 00:05:38,880
magnitude of the gradient

132
00:05:38,880 --> 00:05:42,479
if the norm is smaller than c

133
00:05:42,479 --> 00:05:46,000
then we use it as it is after

134
00:05:46,000 --> 00:05:49,120
clipping the gradients we take the sum

135
00:05:49,120 --> 00:05:52,080
of those clipped gradients and add

136
00:05:52,080 --> 00:05:54,560
gaussian noise to satisfy differential

137
00:05:54,560 --> 00:05:55,840
privacy

138
00:05:55,840 --> 00:05:59,440
we call this aggregated noisy gradient

139
00:05:59,440 --> 00:06:03,440
of private gradient once we have this

140
00:06:03,440 --> 00:06:06,960
private gradient then we can update

141
00:06:06,960 --> 00:06:10,720
the parameters of our neural network

142
00:06:10,720 --> 00:06:13,440
okay so what is the challenge in

143
00:06:13,440 --> 00:06:15,440
implementing this gradient clipping

144
00:06:15,440 --> 00:06:17,199
method

145
00:06:17,199 --> 00:06:21,120
as we have seen before

146
00:06:21,120 --> 00:06:22,720
training a neural network under

147
00:06:22,720 --> 00:06:24,800
differential privacy requires

148
00:06:24,800 --> 00:06:28,479
accessing per example gradient and

149
00:06:28,479 --> 00:06:31,120
modify the per example gradient

150
00:06:31,120 --> 00:06:32,080
depending on

151
00:06:32,080 --> 00:06:35,680
their magnitude however existing deep

152
00:06:35,680 --> 00:06:37,039
learning framework

153
00:06:37,039 --> 00:06:39,120
they they are tailored to the

154
00:06:39,120 --> 00:06:40,080
calculation

155
00:06:40,080 --> 00:06:42,960
of the aggregate gradient and they don't

156
00:06:42,960 --> 00:06:43,600
provide

157
00:06:43,600 --> 00:06:46,840
an easy access to the per example

158
00:06:46,840 --> 00:06:49,840
grading

159
00:06:50,160 --> 00:06:53,599
so there exists a nine way to

160
00:06:53,599 --> 00:06:56,400
implement this gradient clipping using

161
00:06:56,400 --> 00:06:59,120
the existing deep learning framework

162
00:06:59,120 --> 00:07:02,160
given a mini batch of examples we

163
00:07:02,160 --> 00:07:05,360
take one example and then

164
00:07:05,360 --> 00:07:08,319
back propagate through the computational

165
00:07:08,319 --> 00:07:09,199
graph

166
00:07:09,199 --> 00:07:12,960
which will give us per example gradient

167
00:07:12,960 --> 00:07:15,440
we repeat this process for all

168
00:07:15,440 --> 00:07:19,360
observations in our mini batch

169
00:07:19,520 --> 00:07:22,639
while this allows us to implement the

170
00:07:22,639 --> 00:07:23,840
gradient clipping

171
00:07:23,840 --> 00:07:26,720
without making any modification to the

172
00:07:26,720 --> 00:07:29,039
existing deep learning framework

173
00:07:29,039 --> 00:07:32,080
this process is extremely slow because

174
00:07:32,080 --> 00:07:34,000
we have to back propagate

175
00:07:34,000 --> 00:07:38,560
for all observations in our mini batch

176
00:07:38,560 --> 00:07:41,199
now the question is whether we can do

177
00:07:41,199 --> 00:07:44,800
better than this or not

178
00:07:44,800 --> 00:07:48,160
to get an idea we take an example

179
00:07:48,160 --> 00:07:51,039
of fully connected network we are going

180
00:07:51,039 --> 00:07:52,479
to inspect

181
00:07:52,479 --> 00:07:55,360
the mathematical derivation of

182
00:07:55,360 --> 00:07:57,120
calculating the per example

183
00:07:57,120 --> 00:08:02,000
gradient for linear layers

184
00:08:02,240 --> 00:08:06,160
we denote the pre-activation by z

185
00:08:06,160 --> 00:08:09,280
and we use small letter a

186
00:08:09,280 --> 00:08:13,280
to denote the activation of the layer

187
00:08:13,280 --> 00:08:16,800
here the greek letter v corresponds to

188
00:08:16,800 --> 00:08:17,199
the

189
00:08:17,199 --> 00:08:21,199
element wise activation function

190
00:08:21,199 --> 00:08:24,840
given this notation let's look into more

191
00:08:24,840 --> 00:08:26,080
details

192
00:08:26,080 --> 00:08:28,960
using the chain rule we can express the

193
00:08:28,960 --> 00:08:30,240
derivative of

194
00:08:30,240 --> 00:08:32,958
our loss function l with respect to

195
00:08:32,958 --> 00:08:34,479
parameter w

196
00:08:34,479 --> 00:08:37,839
as the product of the two terms in which

197
00:08:37,839 --> 00:08:40,000
the first term is the gradient with

198
00:08:40,000 --> 00:08:42,240
respect to pre-activation

199
00:08:42,240 --> 00:08:45,600
and the second term is the derivative

200
00:08:45,600 --> 00:08:48,560
of pre-activation with respect to

201
00:08:48,560 --> 00:08:50,399
parameter w

202
00:08:50,399 --> 00:08:52,720
and it turns out that the second term is

203
00:08:52,720 --> 00:08:53,440
nothing but

204
00:08:53,440 --> 00:08:56,640
input to the layer

205
00:08:56,640 --> 00:08:59,519
combining all of this we see that the

206
00:08:59,519 --> 00:09:00,399
gradient

207
00:09:00,399 --> 00:09:04,080
of l with respect to parameter w

208
00:09:04,080 --> 00:09:07,600
can be expressed as the outer product

209
00:09:07,600 --> 00:09:11,120
between two objects the first object

210
00:09:11,120 --> 00:09:12,880
is the gradient with respect to

211
00:09:12,880 --> 00:09:14,160
pre-activation

212
00:09:14,160 --> 00:09:16,880
and the second object is input to the

213
00:09:16,880 --> 00:09:19,279
gradient

214
00:09:19,279 --> 00:09:21,519
let's take another example the second

215
00:09:21,519 --> 00:09:24,720
example is the convolutional layer

216
00:09:24,720 --> 00:09:26,640
the convolution is also a linear

217
00:09:26,640 --> 00:09:27,760
operation

218
00:09:27,760 --> 00:09:31,120
and similarly we denote

219
00:09:31,120 --> 00:09:34,720
the pre-activation by z and w

220
00:09:34,720 --> 00:09:38,080
denotes the kernel

221
00:09:38,080 --> 00:09:42,160
and x denotes the input image

222
00:09:42,160 --> 00:09:45,440
here the star symbol corresponds to

223
00:09:45,440 --> 00:09:48,880
element wise product between

224
00:09:48,880 --> 00:09:51,839
two tensors

225
00:09:52,320 --> 00:09:55,360
again using the chain rule

226
00:09:55,360 --> 00:09:58,560
we can decompose the gradient of l with

227
00:09:58,560 --> 00:09:59,920
respect to w

228
00:09:59,920 --> 00:10:03,839
as the product of two terms the gradient

229
00:10:03,839 --> 00:10:06,399
with respect to preactivation

230
00:10:06,399 --> 00:10:08,959
and the derivative of reactivation with

231
00:10:08,959 --> 00:10:11,040
respect to parameter

232
00:10:11,040 --> 00:10:13,680
after applying simple calculus and

233
00:10:13,680 --> 00:10:15,519
manipulating the terms

234
00:10:15,519 --> 00:10:17,760
we observe that the results can be

235
00:10:17,760 --> 00:10:19,360
expressed as

236
00:10:19,360 --> 00:10:22,480
the inner product between two tensors

237
00:10:22,480 --> 00:10:24,959
where the first tensor is the gradient

238
00:10:24,959 --> 00:10:25,600
of l

239
00:10:25,600 --> 00:10:28,720
with respect to pre-activation z and the

240
00:10:28,720 --> 00:10:32,320
second term is the input to the gradient

241
00:10:32,320 --> 00:10:35,120
recall that we obtained the similar

242
00:10:35,120 --> 00:10:35,839
results

243
00:10:35,839 --> 00:10:39,360
for the linear layer the per example

244
00:10:39,360 --> 00:10:40,079
gradient

245
00:10:40,079 --> 00:10:43,360
was expressed as the results of

246
00:10:43,360 --> 00:10:46,240
performing an operation between the

247
00:10:46,240 --> 00:10:49,120
gradient with respect to pre-activation

248
00:10:49,120 --> 00:10:51,920
and input to the layer the only

249
00:10:51,920 --> 00:10:53,120
difference is that

250
00:10:53,120 --> 00:10:56,240
for linear layer we have to take the

251
00:10:56,240 --> 00:10:57,519
outer product

252
00:10:57,519 --> 00:11:01,440
while in here we take

253
00:11:01,440 --> 00:11:05,360
tensor in a product actually this is not

254
00:11:05,360 --> 00:11:06,480
a coincidence

255
00:11:06,480 --> 00:11:09,200
we can generalize this observation to

256
00:11:09,200 --> 00:11:12,399
other types of the layers

257
00:11:12,399 --> 00:11:15,680
in this table we present

258
00:11:15,680 --> 00:11:18,880
the per example grid gradient for

259
00:11:18,880 --> 00:11:21,519
different types of the layers

260
00:11:21,519 --> 00:11:24,399
if you look at the third column they all

261
00:11:24,399 --> 00:11:25,760
correspond to

262
00:11:25,760 --> 00:11:27,519
the gradient with respect to

263
00:11:27,519 --> 00:11:29,200
pre-activation

264
00:11:29,200 --> 00:11:32,959
and the second operand is always

265
00:11:32,959 --> 00:11:37,200
input to the layer and depending on the

266
00:11:37,200 --> 00:11:39,920
layer type we just perform different

267
00:11:39,920 --> 00:11:41,279
type of the operation

268
00:11:41,279 --> 00:11:45,120
between these two operands

269
00:11:45,760 --> 00:11:49,200
okay how can we implement this

270
00:11:49,200 --> 00:11:51,839
our observation is this in order to

271
00:11:51,839 --> 00:11:54,399
calculate the per example gradient

272
00:11:54,399 --> 00:11:56,959
all we need is the gradient with respect

273
00:11:56,959 --> 00:11:58,639
to pre-activation

274
00:11:58,639 --> 00:12:01,760
and input to the gradient

275
00:12:01,760 --> 00:12:04,959
to implement this we create a wrapper

276
00:12:04,959 --> 00:12:06,160
classes

277
00:12:06,160 --> 00:12:09,279
for different types of the layers during

278
00:12:09,279 --> 00:12:10,880
the fourth step

279
00:12:10,880 --> 00:12:13,920
we store the input to the layer and

280
00:12:13,920 --> 00:12:17,040
during the back propagation step

281
00:12:17,040 --> 00:12:18,959
the gradient with respect to

282
00:12:18,959 --> 00:12:20,160
pre-activation

283
00:12:20,160 --> 00:12:23,440
is calculated by auto differentiation

284
00:12:23,440 --> 00:12:27,040
library and it will be available to us

285
00:12:27,040 --> 00:12:29,519
once we have this gradient with respect

286
00:12:29,519 --> 00:12:31,040
to pre-activation

287
00:12:31,040 --> 00:12:35,760
we can calculate for example gradient

288
00:12:35,839 --> 00:12:38,320
we make another observation so let's

289
00:12:38,320 --> 00:12:39,040
define

290
00:12:39,040 --> 00:12:42,079
mu sub i as the minimum between these

291
00:12:42,079 --> 00:12:44,240
two values

292
00:12:44,240 --> 00:12:48,160
and once we define the new in this way

293
00:12:48,160 --> 00:12:50,800
then the clipped gradient can be

294
00:12:50,800 --> 00:12:51,519
expressed

295
00:12:51,519 --> 00:12:54,160
as the weighted sum of per example

296
00:12:54,160 --> 00:12:55,839
gradient

297
00:12:55,839 --> 00:12:59,040
and in turn we observe that this

298
00:12:59,040 --> 00:13:02,240
weighted gradient is the gradient of

299
00:13:02,240 --> 00:13:06,480
v weighted objective function l

300
00:13:06,480 --> 00:13:10,079
that means once we have calculated

301
00:13:10,079 --> 00:13:13,200
this weights for each example

302
00:13:13,200 --> 00:13:15,920
then using the existing deep learning

303
00:13:15,920 --> 00:13:16,560
framework

304
00:13:16,560 --> 00:13:20,880
we can calculate the clipped gradient

305
00:13:20,880 --> 00:13:23,680
this allows us to use the efficient

306
00:13:23,680 --> 00:13:25,200
implementation

307
00:13:25,200 --> 00:13:28,560
of existing deep learning library to

308
00:13:28,560 --> 00:13:31,040
to calculate the per example gradient

309
00:13:31,040 --> 00:13:33,440
for differential privacy

310
00:13:33,440 --> 00:13:36,480
when we compare the performance

311
00:13:36,480 --> 00:13:39,600
here we compare the performance of

312
00:13:39,600 --> 00:13:43,040
our proposed method against the

313
00:13:43,040 --> 00:13:45,680
baseline algorithms on different neural

314
00:13:45,680 --> 00:13:47,199
network architectures

315
00:13:47,199 --> 00:13:51,199
and on different data sets

316
00:13:51,199 --> 00:13:54,480
so we consider multi-layer perceptron

317
00:13:54,480 --> 00:13:57,839
convolutional neural network lstm and

318
00:13:57,839 --> 00:14:01,199
transform network for datasets

319
00:14:01,199 --> 00:14:05,199
we use mnist fashion analyst c410

320
00:14:05,199 --> 00:14:08,959
and elson and imdb

321
00:14:08,959 --> 00:14:12,479
movie review datasets

322
00:14:13,040 --> 00:14:15,760
here are the baselines we consider in

323
00:14:15,760 --> 00:14:16,079
our

324
00:14:16,079 --> 00:14:18,560
experiments non-private is the

325
00:14:18,560 --> 00:14:19,920
non-private method

326
00:14:19,920 --> 00:14:23,440
and nxbp is the knife implementation

327
00:14:23,440 --> 00:14:26,320
of grading clipping which requires back

328
00:14:26,320 --> 00:14:28,000
propagating

329
00:14:28,000 --> 00:14:31,279
multiple times rewrite gp

330
00:14:31,279 --> 00:14:34,560
is our proposed method and multi loss is

331
00:14:34,560 --> 00:14:36,240
the existing method

332
00:14:36,240 --> 00:14:39,600
that defines the multiple loss functions

333
00:14:39,600 --> 00:14:42,079
and asks the auto differentiation

334
00:14:42,079 --> 00:14:43,120
library

335
00:14:43,120 --> 00:14:47,040
to calculate the grading

336
00:14:47,040 --> 00:14:51,760
over all those loss functions

337
00:14:52,240 --> 00:14:54,959
here we present the results of comparing

338
00:14:54,959 --> 00:14:55,760
our method

339
00:14:55,760 --> 00:14:58,800
to other existing methods

340
00:14:58,800 --> 00:15:01,360
so y-axis represents per epoch

341
00:15:01,360 --> 00:15:04,240
processing time

342
00:15:04,240 --> 00:15:06,959
and x-axis corresponds to different type

343
00:15:06,959 --> 00:15:10,160
of neural network architecture

344
00:15:10,160 --> 00:15:13,680
notice that the y-axis in log scale

345
00:15:13,680 --> 00:15:17,199
as you can see our proposed method

346
00:15:17,199 --> 00:15:20,160
greatly outperforms the existing method

347
00:15:20,160 --> 00:15:21,199
especially

348
00:15:21,199 --> 00:15:23,360
on very complex neural network

349
00:15:23,360 --> 00:15:24,560
architecture

350
00:15:24,560 --> 00:15:27,199
such as the transform network the gap

351
00:15:27,199 --> 00:15:27,920
between

352
00:15:27,920 --> 00:15:30,240
the proposed method and knives

353
00:15:30,240 --> 00:15:31,600
implementation

354
00:15:31,600 --> 00:15:34,800
is larger

355
00:15:36,240 --> 00:15:38,240
here we compare the peripheral

356
00:15:38,240 --> 00:15:40,480
processing time

357
00:15:40,480 --> 00:15:44,079
on bearing batch sizes

358
00:15:44,079 --> 00:15:46,959
as you can see as we increase the mini

359
00:15:46,959 --> 00:15:48,560
batches size

360
00:15:48,560 --> 00:15:51,440
the part epoch processing time decreases

361
00:15:51,440 --> 00:15:53,040
this is due to

362
00:15:53,040 --> 00:15:55,759
the fact that our proposed method can

363
00:15:55,759 --> 00:15:56,880
utilize

364
00:15:56,880 --> 00:16:01,600
the gpu parallelism

365
00:16:02,800 --> 00:16:06,399
here's the results on different

366
00:16:06,399 --> 00:16:10,639
depths so as we expected

367
00:16:10,639 --> 00:16:13,120
as the depths of the network increases

368
00:16:13,120 --> 00:16:15,279
the processing time

369
00:16:15,279 --> 00:16:18,720
increases the rate of increase for our

370
00:16:18,720 --> 00:16:19,360
method

371
00:16:19,360 --> 00:16:22,720
is similar to that of full non-private

372
00:16:22,720 --> 00:16:25,199
method

373
00:16:28,320 --> 00:16:31,360
here we present the performance

374
00:16:31,360 --> 00:16:32,320
comparison

375
00:16:32,320 --> 00:16:35,120
on the larger network for for example

376
00:16:35,120 --> 00:16:36,079
the resnet

377
00:16:36,079 --> 00:16:39,599
and vzz network

378
00:16:41,040 --> 00:16:46,399
we note that for larger image

379
00:16:46,399 --> 00:16:49,839
for larger input image size the existing

380
00:16:49,839 --> 00:16:52,320
method for example the multi-loss

381
00:16:52,320 --> 00:16:58,000
method fails to run on resnet101

382
00:17:00,320 --> 00:17:03,600
since vcc network requires huge amount

383
00:17:03,600 --> 00:17:04,959
of memory

384
00:17:04,959 --> 00:17:07,919
and multi-loss method failed to run on

385
00:17:07,919 --> 00:17:11,760
this neural network architecture

386
00:17:14,000 --> 00:17:17,199
so in this work we presented on

387
00:17:17,199 --> 00:17:20,240
scalable method to calculate the per

388
00:17:20,240 --> 00:17:22,799
example gradient for differential

389
00:17:22,799 --> 00:17:23,839
privacy

390
00:17:23,839 --> 00:17:27,359
and we showed that we empirically showed

391
00:17:27,359 --> 00:17:27,679
that

392
00:17:27,679 --> 00:17:30,400
our proposed reweighted gp method

393
00:17:30,400 --> 00:17:31,200
greatly

394
00:17:31,200 --> 00:17:33,200
improves the scalability of

395
00:17:33,200 --> 00:17:35,600
differentially private deep learning

396
00:17:35,600 --> 00:17:38,400
there is also existing limitation our

397
00:17:38,400 --> 00:17:41,200
proposed method is not comparable

398
00:17:41,200 --> 00:17:44,559
compatible with a batch normalization

399
00:17:44,559 --> 00:17:45,440
layer

400
00:17:45,440 --> 00:17:48,080
so whenever we have batch normalization

401
00:17:48,080 --> 00:17:49,440
layer

402
00:17:49,440 --> 00:17:52,720
we will have to replace that with

403
00:17:52,720 --> 00:17:55,440
other types of normalization method for

404
00:17:55,440 --> 00:17:56,160
example

405
00:17:56,160 --> 00:18:00,480
layer normalization

406
00:18:00,480 --> 00:18:02,720
okay thanks for your attention and if

407
00:18:02,720 --> 00:18:04,160
you have question

408
00:18:04,160 --> 00:18:08,960
feel free to ask me thank you

