1
00:00:00,880 --> 00:00:02,879
hi hello my name is sushan liu today i

2
00:00:02,879 --> 00:00:04,319
want to introduce our work about

3
00:00:04,319 --> 00:00:05,839
defending against microphone based

4
00:00:05,839 --> 00:00:07,919
attacks with personalized noise

5
00:00:07,919 --> 00:00:10,480
this is a joint work with sushang ej

6
00:00:10,480 --> 00:00:11,040
sung

7
00:00:11,040 --> 00:00:13,519
apu kapatia and donald williamson

8
00:00:13,519 --> 00:00:15,280
conducted at indiana university

9
00:00:15,280 --> 00:00:16,640
bloomington

10
00:00:16,640 --> 00:00:18,560
microphones are everywhere there are

11
00:00:18,560 --> 00:00:20,720
estimates 3.3 billion actively used

12
00:00:20,720 --> 00:00:22,560
smartphones around the globe today

13
00:00:22,560 --> 00:00:24,080
and there is a growing market for

14
00:00:24,080 --> 00:00:26,320
digital assistance such as google home

15
00:00:26,320 --> 00:00:27,519
and amazon echo

16
00:00:27,519 --> 00:00:30,240
this microphone enable devices featuring

17
00:00:30,240 --> 00:00:31,760
an always listening mode

18
00:00:31,760 --> 00:00:34,239
to support voice based commands as a

19
00:00:34,239 --> 00:00:35,120
convenience

20
00:00:35,120 --> 00:00:37,200
over physical pressing a button people

21
00:00:37,200 --> 00:00:39,040
can trigger voice commands with spoken

22
00:00:39,040 --> 00:00:39,680
phrase

23
00:00:39,680 --> 00:00:42,879
such as hey alexa google google and hey

24
00:00:42,879 --> 00:00:43,440
siri

25
00:00:43,440 --> 00:00:45,280
microphones in this ubiquiti device

26
00:00:45,280 --> 00:00:47,440
however raise significant privacy

27
00:00:47,440 --> 00:00:48,800
concerns

28
00:00:48,800 --> 00:00:51,199
first digital system conduct speech

29
00:00:51,199 --> 00:00:52,640
recognition in the cloud once

30
00:00:52,640 --> 00:00:54,559
it has locally recognized the trigger

31
00:00:54,559 --> 00:00:55,920
phrase however

32
00:00:55,920 --> 00:00:57,440
they are often incorrectly triggered

33
00:00:57,440 --> 00:00:59,359
through false positives and violate

34
00:00:59,359 --> 00:01:00,480
people's privacy

35
00:01:00,480 --> 00:01:02,640
by uploading unauthorized conversation

36
00:01:02,640 --> 00:01:03,520
to the cloud

37
00:01:03,520 --> 00:01:06,479
or worse by sending them to a context by

38
00:01:06,479 --> 00:01:07,520
incorrectly

39
00:01:07,520 --> 00:01:09,760
interpreting casual conversations as

40
00:01:09,760 --> 00:01:12,560
complex complex commands second

41
00:01:12,560 --> 00:01:15,200
even if this micros phones are set to

42
00:01:15,200 --> 00:01:15,680
off

43
00:01:15,680 --> 00:01:17,439
attackers can potentially record

44
00:01:17,439 --> 00:01:19,680
conversations through eras dropping

45
00:01:19,680 --> 00:01:20,479
malware

46
00:01:20,479 --> 00:01:22,640
microphones in general pose a serious

47
00:01:22,640 --> 00:01:24,799
threat to the privacy of users

48
00:01:24,799 --> 00:01:26,640
they are ubiquitous in people's lives

49
00:01:26,640 --> 00:01:27,840
can be easily used

50
00:01:27,840 --> 00:01:30,479
and yet have no clear tangible way of

51
00:01:30,479 --> 00:01:32,479
being disabled by the user

52
00:01:32,479 --> 00:01:34,960
unlike cameras which also pose privacy

53
00:01:34,960 --> 00:01:36,960
risk but can be covered more tangible by

54
00:01:36,960 --> 00:01:37,759
users

55
00:01:37,759 --> 00:01:40,400
like using stickers microphones are not

56
00:01:40,400 --> 00:01:43,200
easily disabled or muted

57
00:01:43,200 --> 00:01:45,040
in the past external microphones were

58
00:01:45,040 --> 00:01:47,200
needed and could be simply disconnected

59
00:01:47,200 --> 00:01:49,759
to provide tangible assurance of privacy

60
00:01:49,759 --> 00:01:51,520
but recent devices need to address

61
00:01:51,520 --> 00:01:52,799
embedded microphones

62
00:01:52,799 --> 00:01:54,880
users have viable options to tangibly

63
00:01:54,880 --> 00:01:56,159
shut off the microphone

64
00:01:56,159 --> 00:01:59,520
to be assured of their privacy

65
00:01:59,520 --> 00:02:01,439
apparently one option is to shut down

66
00:02:01,439 --> 00:02:03,200
other devices when you don't want to use

67
00:02:03,200 --> 00:02:03,600
them

68
00:02:03,600 --> 00:02:06,079
however people have legitimate reason to

69
00:02:06,079 --> 00:02:07,360
have their smartphones

70
00:02:07,360 --> 00:02:10,160
and personal assistant on such as

71
00:02:10,160 --> 00:02:11,840
receiving incoming calls

72
00:02:11,840 --> 00:02:14,560
however there are situations when one

73
00:02:14,560 --> 00:02:17,280
will prefer only the microphone disabled

74
00:02:17,280 --> 00:02:20,560
therefore we seek defenses where a user

75
00:02:20,560 --> 00:02:21,760
can choose to mask

76
00:02:21,760 --> 00:02:24,160
or off escape their conversation where

77
00:02:24,160 --> 00:02:27,440
retaining the function of their devices

78
00:02:27,440 --> 00:02:29,520
existing microphone masking techniques

79
00:02:29,520 --> 00:02:31,040
normally you mainly use

80
00:02:31,040 --> 00:02:33,920
either ultrasonic signals or random loud

81
00:02:33,920 --> 00:02:34,480
noise

82
00:02:34,480 --> 00:02:37,840
to chan to jam the embedded microphones

83
00:02:37,840 --> 00:02:39,519
so that the mixed speech cannot be

84
00:02:39,519 --> 00:02:41,200
recognized however

85
00:02:41,200 --> 00:02:42,879
there are still drawbacks to this

86
00:02:42,879 --> 00:02:44,959
message for random loud noise

87
00:02:44,959 --> 00:02:48,080
user will be disrupted by noise and

88
00:02:48,080 --> 00:02:50,239
existing noise cancelling technique can

89
00:02:50,239 --> 00:02:52,160
help to directly remove the injecting

90
00:02:52,160 --> 00:02:52,879
noise

91
00:02:52,879 --> 00:02:55,200
for ultrasonic signals users may not be

92
00:02:55,200 --> 00:02:55,920
aware or

93
00:02:55,920 --> 00:02:58,319
sewer of their existence for ultrasonic

94
00:02:58,319 --> 00:03:00,480
jamming it is also useful to craft a

95
00:03:00,480 --> 00:03:03,280
more effective jamming noise

96
00:03:03,280 --> 00:03:05,920
therefore in this paper we seek to uh

97
00:03:05,920 --> 00:03:06,800
off

98
00:03:06,800 --> 00:03:09,519
uh confuse evil shopping adversaries by

99
00:03:09,519 --> 00:03:11,840
designing customized pattern of noise

100
00:03:11,840 --> 00:03:14,480
tailored to target user or victim these

101
00:03:14,480 --> 00:03:15,200
are played

102
00:03:15,200 --> 00:03:17,120
through an external speaker into the

103
00:03:17,120 --> 00:03:19,040
microphone with the goal of rendering

104
00:03:19,040 --> 00:03:20,239
capture speech

105
00:03:20,239 --> 00:03:22,879
and noise mixture unintangible to both

106
00:03:22,879 --> 00:03:23,440
sr

107
00:03:23,440 --> 00:03:26,720
and human adversaries this diagram shows

108
00:03:26,720 --> 00:03:28,239
how attacker can euros drop

109
00:03:28,239 --> 00:03:30,720
conversation from a voice controlled

110
00:03:30,720 --> 00:03:31,519
device

111
00:03:31,519 --> 00:03:33,519
first the human is captured by embedded

112
00:03:33,519 --> 00:03:35,120
microphone in the device

113
00:03:35,120 --> 00:03:38,000
then the analog to digital converter is

114
00:03:38,000 --> 00:03:39,519
used to digitalization

115
00:03:39,519 --> 00:03:41,920
and quantization to convert the input

116
00:03:41,920 --> 00:03:43,599
voicing to a digital

117
00:03:43,599 --> 00:03:46,560
speed signal the attack attack has the

118
00:03:46,560 --> 00:03:47,440
attacker

119
00:03:47,440 --> 00:03:50,239
can access the uh digital signal and can

120
00:03:50,239 --> 00:03:51,360
accomplish the tag

121
00:03:51,360 --> 00:03:53,439
by either directly listen to the signal

122
00:03:53,439 --> 00:03:55,599
and manually recognize the content

123
00:03:55,599 --> 00:03:58,159
or can use a state-of-the-art asr system

124
00:03:58,159 --> 00:04:00,159
to convert this signal into text

125
00:04:00,159 --> 00:04:03,200
transcript by injecting defensive noise

126
00:04:03,200 --> 00:04:04,319
physically into a

127
00:04:04,319 --> 00:04:06,480
microphone we can office case speech

128
00:04:06,480 --> 00:04:07,920
target by attacker

129
00:04:07,920 --> 00:04:10,159
when attacker received noisy mixture

130
00:04:10,159 --> 00:04:11,760
they cannot understand the content by

131
00:04:11,760 --> 00:04:12,720
listening to it

132
00:04:12,720 --> 00:04:15,040
the sr system will also be misleading

133
00:04:15,040 --> 00:04:15,920
and produce a

134
00:04:15,920 --> 00:04:19,120
incorrect transcript before we dive into

135
00:04:19,120 --> 00:04:20,399
the detail we first

136
00:04:20,399 --> 00:04:23,440
clarify our threat model first we assume

137
00:04:23,440 --> 00:04:24,080
a remote

138
00:04:24,080 --> 00:04:27,040
attacker who must rely on compromised

139
00:04:27,040 --> 00:04:27,759
device

140
00:04:27,759 --> 00:04:30,320
with a microphone to illustrate and

141
00:04:30,320 --> 00:04:30,880
target

142
00:04:30,880 --> 00:04:33,120
conversation this is also mean that

143
00:04:33,120 --> 00:04:35,360
attacker does not have physical access

144
00:04:35,360 --> 00:04:37,120
to the victim space to place

145
00:04:37,120 --> 00:04:39,520
unauthorized microphone bugs

146
00:04:39,520 --> 00:04:42,240
second we assume the attacker can gain

147
00:04:42,240 --> 00:04:44,000
full control of the target device or

148
00:04:44,000 --> 00:04:45,440
already have for control

149
00:04:45,440 --> 00:04:47,600
for example a user might consider the

150
00:04:47,600 --> 00:04:49,120
manufacturer of the device to be a

151
00:04:49,120 --> 00:04:50,479
potential adversary

152
00:04:50,479 --> 00:04:52,000
while the user is having a private

153
00:04:52,000 --> 00:04:53,600
conversation third

154
00:04:53,600 --> 00:04:55,680
if the sr failed to the attacker the

155
00:04:55,680 --> 00:04:56,720
attacker has the

156
00:04:56,720 --> 00:04:58,720
capability to physically listen to the

157
00:04:58,720 --> 00:05:00,000
capture audio

158
00:05:00,000 --> 00:05:02,080
those and the defense must be robust

159
00:05:02,080 --> 00:05:03,919
against both asr and human-based

160
00:05:03,919 --> 00:05:05,759
attackers

161
00:05:05,759 --> 00:05:08,639
we use whatever rate to evaluate to

162
00:05:08,639 --> 00:05:10,639
evaluate the attacker's performance

163
00:05:10,639 --> 00:05:12,720
at correctly recognizing the capture

164
00:05:12,720 --> 00:05:14,800
speech whatever radio fraction of the

165
00:05:14,800 --> 00:05:15,520
mistakes

166
00:05:15,520 --> 00:05:17,680
or errors in the transcription errors

167
00:05:17,680 --> 00:05:19,919
including incorrect word substitutions

168
00:05:19,919 --> 00:05:22,400
deletions and insertions by comparing

169
00:05:22,400 --> 00:05:24,560
the asr system's output transcription

170
00:05:24,560 --> 00:05:26,160
to the ground choose transcription

171
00:05:26,160 --> 00:05:27,919
whatever it is computed by dividing

172
00:05:27,919 --> 00:05:28,960
number of arrows

173
00:05:28,960 --> 00:05:30,800
by the total number of words in the

174
00:05:30,800 --> 00:05:32,240
reference transcription

175
00:05:32,240 --> 00:05:34,000
since the purpose of our work is to

176
00:05:34,000 --> 00:05:35,759
obscure the speech content

177
00:05:35,759 --> 00:05:37,840
we want the water rate to be as high as

178
00:05:37,840 --> 00:05:39,840
possible

179
00:05:39,840 --> 00:05:42,400
our baseline defense against u.s drop is

180
00:05:42,400 --> 00:05:43,280
to add

181
00:05:43,280 --> 00:05:45,520
a random non-stationary noise to the

182
00:05:45,520 --> 00:05:46,320
environment

183
00:05:46,320 --> 00:05:48,080
the reason that we choose non-stationary

184
00:05:48,080 --> 00:05:49,600
noise as our

185
00:05:49,600 --> 00:05:52,479
baseline is the statistic properties of

186
00:05:52,479 --> 00:05:53,680
stationary noise

187
00:05:53,680 --> 00:05:56,000
remain unchanged over time as opposite

188
00:05:56,000 --> 00:05:57,520
to a non-stationary noise

189
00:05:57,520 --> 00:06:00,560
which varies with time and thus harder

190
00:06:00,560 --> 00:06:02,240
to recognize and remove

191
00:06:02,240 --> 00:06:05,440
the snr level shows the ratio of the

192
00:06:05,440 --> 00:06:07,199
power of the speech to the power of the

193
00:06:07,199 --> 00:06:08,479
injecting noise

194
00:06:08,479 --> 00:06:10,720
lower external level indicates the noise

195
00:06:10,720 --> 00:06:11,600
is louder

196
00:06:11,600 --> 00:06:13,440
since we want to keep the disturbance

197
00:06:13,440 --> 00:06:16,080
level low so we want the essential level

198
00:06:16,080 --> 00:06:18,479
as high as possible when the water rate

199
00:06:18,479 --> 00:06:20,400
is satisfying

200
00:06:20,400 --> 00:06:22,560
the two baseline non-stationary noises

201
00:06:22,560 --> 00:06:24,400
that we choose are random baby noise and

202
00:06:24,400 --> 00:06:25,360
caffeine noise

203
00:06:25,360 --> 00:06:27,280
bible noise contain multiple people

204
00:06:27,280 --> 00:06:28,479
talking simultaneously

205
00:06:28,479 --> 00:06:30,800
titaniously and caffeine noise contains

206
00:06:30,800 --> 00:06:32,000
sounds like cafeteria

207
00:06:32,000 --> 00:06:35,039
environment let's hear what the

208
00:06:35,039 --> 00:06:38,639
noise noise like first

209
00:06:41,680 --> 00:06:44,880
so that's the bevel noise

210
00:06:44,980 --> 00:06:47,759
[Music]

211
00:06:47,759 --> 00:06:50,479
and that's the cafeteria real noise then

212
00:06:50,479 --> 00:06:53,840
let's hear the mixed speech

213
00:06:56,240 --> 00:06:58,960
nasty mixed speech between the target

214
00:06:58,960 --> 00:07:02,900
speech and the memories

215
00:07:02,900 --> 00:07:04,080
[Music]

216
00:07:04,080 --> 00:07:06,479
and that's the cafeteria mixture as you

217
00:07:06,479 --> 00:07:07,280
can hear

218
00:07:07,280 --> 00:07:10,720
from uh all the mixtures are

219
00:07:10,720 --> 00:07:12,960
under a zero deviation level where the

220
00:07:12,960 --> 00:07:14,880
both the signal and voids are the same

221
00:07:14,880 --> 00:07:16,240
decimal level

222
00:07:16,240 --> 00:07:17,840
by mixing the target speech with the

223
00:07:17,840 --> 00:07:19,759
baseline non-stationary noise

224
00:07:19,759 --> 00:07:21,440
we can find that the contents has been

225
00:07:21,440 --> 00:07:23,039
obfuscated however

226
00:07:23,039 --> 00:07:24,960
we can still hear some words and phrase

227
00:07:24,960 --> 00:07:26,319
from the target speech

228
00:07:26,319 --> 00:07:29,840
finally let's hear what the clean speech

229
00:07:29,840 --> 00:07:31,919
the goose was brought straight from the

230
00:07:31,919 --> 00:07:34,639
old market

231
00:07:34,720 --> 00:07:36,160
one problem that we find from the

232
00:07:36,160 --> 00:07:37,759
baseline noise is that we can still

233
00:07:37,759 --> 00:07:39,280
recognize some contents

234
00:07:39,280 --> 00:07:41,280
from the mixture because the voice

235
00:07:41,280 --> 00:07:42,880
characteristic of the background noise

236
00:07:42,880 --> 00:07:44,879
is not similar to the target speech

237
00:07:44,879 --> 00:07:47,759
our approach uses target users own voice

238
00:07:47,759 --> 00:07:49,919
to generate a specific bad noise

239
00:07:49,919 --> 00:07:52,960
to mask the actual speech here multiple

240
00:07:52,960 --> 00:07:54,960
tracks of computer audiences are stacked

241
00:07:54,960 --> 00:07:55,520
together

242
00:07:55,520 --> 00:07:59,840
to generate this specific my bible noise

243
00:08:00,319 --> 00:08:02,000
first let's hear what's the target

244
00:08:02,000 --> 00:08:05,039
speech we want to mask

245
00:08:05,039 --> 00:08:08,240
fine soap saves tender skin fine

246
00:08:08,240 --> 00:08:10,400
next let's hear the reference signal

247
00:08:10,400 --> 00:08:12,000
provided by another speaker

248
00:08:12,000 --> 00:08:15,599
used for voice conversion belief without

249
00:08:15,599 --> 00:08:16,240
evidence

250
00:08:16,240 --> 00:08:17,919
in what is told by one who speaks

251
00:08:17,919 --> 00:08:19,280
without knowledge

252
00:08:19,280 --> 00:08:22,638
of things without parallel

253
00:08:22,879 --> 00:08:24,840
next let's hear the voice converted

254
00:08:24,840 --> 00:08:26,319
signal

255
00:08:26,319 --> 00:08:28,560
belief without evidence and what is told

256
00:08:28,560 --> 00:08:30,800
by wireless speaks without knowledge

257
00:08:30,800 --> 00:08:33,279
of things without parallel as you can

258
00:08:33,279 --> 00:08:35,679
hear the computer signal has the same

259
00:08:35,679 --> 00:08:36,159
content

260
00:08:36,159 --> 00:08:38,159
as the reference signal but in target

261
00:08:38,159 --> 00:08:39,519
user's voice

262
00:08:39,519 --> 00:08:42,640
we used multiple discovery signals

263
00:08:42,640 --> 00:08:46,319
to craft the my double noise

264
00:08:50,000 --> 00:08:55,839
finally let's hear the mixer speech

265
00:08:57,120 --> 00:08:59,120
we can find that the content of the this

266
00:08:59,120 --> 00:09:01,279
mixture speech is harder to recognize

267
00:09:01,279 --> 00:09:04,480
than the baseline mixtures now let's see

268
00:09:04,480 --> 00:09:05,120
how

269
00:09:05,120 --> 00:09:07,600
we get the converse which conver contain

270
00:09:07,600 --> 00:09:09,760
target user's voice characteristic

271
00:09:09,760 --> 00:09:12,240
the user simulated speech is generated

272
00:09:12,240 --> 00:09:14,399
by adapting a deep learning based voice

273
00:09:14,399 --> 00:09:15,600
conversion technique

274
00:09:15,600 --> 00:09:18,080
that transforms voice characteristic

275
00:09:18,080 --> 00:09:19,680
from one speaker to another

276
00:09:19,680 --> 00:09:21,920
this generates new audiences in the

277
00:09:21,920 --> 00:09:23,519
targeted speaker's voice

278
00:09:23,519 --> 00:09:25,680
multiple simulated speech signals are

279
00:09:25,680 --> 00:09:27,839
generated and combined to form the

280
00:09:27,839 --> 00:09:29,920
my battle noise which complicates

281
00:09:29,920 --> 00:09:30,880
recognition

282
00:09:30,880 --> 00:09:33,360
for the attacker because of the spectral

283
00:09:33,360 --> 00:09:34,320
overlap

284
00:09:34,320 --> 00:09:36,320
we use convolutional variational

285
00:09:36,320 --> 00:09:38,320
autoencoder for the voice conversion

286
00:09:38,320 --> 00:09:38,959
system

287
00:09:38,959 --> 00:09:41,040
if ae based voice conversion system is

288
00:09:41,040 --> 00:09:42,640
chosen because first

289
00:09:42,640 --> 00:09:44,800
unlike conventional gaussian mixture

290
00:09:44,800 --> 00:09:47,040
model based approach

291
00:09:47,040 --> 00:09:49,120
parallel speech data are not required

292
00:09:49,120 --> 00:09:50,160
second

293
00:09:50,160 --> 00:09:52,800
this approach enable enables voice

294
00:09:52,800 --> 00:09:54,959
conversion purely from the audio data

295
00:09:54,959 --> 00:09:56,959
where phonetic and lexical information

296
00:09:56,959 --> 00:09:59,200
about the speech are not needed

297
00:09:59,200 --> 00:10:01,920
the pro training process is a

298
00:10:01,920 --> 00:10:03,519
reconstruction process

299
00:10:03,519 --> 00:10:05,360
in the training step the encoder takes

300
00:10:05,360 --> 00:10:07,600
the orders from both a reference speaker

301
00:10:07,600 --> 00:10:08,720
and the target speaker

302
00:10:08,720 --> 00:10:11,120
xn and extract the speaker independent

303
00:10:11,120 --> 00:10:12,640
information

304
00:10:12,640 --> 00:10:15,920
zn the speaker and then any vector spk

305
00:10:15,920 --> 00:10:18,320
of the given audience xn then

306
00:10:18,320 --> 00:10:19,839
concatenate with zn

307
00:10:19,839 --> 00:10:21,920
and fitting to the decoder the decoder

308
00:10:21,920 --> 00:10:23,760
helps to reconstruct the audio

309
00:10:23,760 --> 00:10:25,360
with the same content of the given

310
00:10:25,360 --> 00:10:27,040
audience xn

311
00:10:27,040 --> 00:10:30,640
and in the voice of speaker identity spk

312
00:10:30,640 --> 00:10:33,600
the result signal x and hat u should be

313
00:10:33,600 --> 00:10:35,600
the same as the input signal xn

314
00:10:35,600 --> 00:10:37,839
in the training process in the inference

315
00:10:37,839 --> 00:10:40,959
step by input the reference signal xn

316
00:10:40,959 --> 00:10:43,040
and change the speaker identity spk to

317
00:10:43,040 --> 00:10:44,560
the target user u

318
00:10:44,560 --> 00:10:47,360
the result audience x and hat u will

319
00:10:47,360 --> 00:10:48,399
have the content

320
00:10:48,399 --> 00:10:52,079
content of the reference uh audience xn

321
00:10:52,079 --> 00:10:55,120
in the voice of target user

322
00:10:55,120 --> 00:10:58,480
u the one hot vector spk is used to

323
00:10:58,480 --> 00:10:59,600
condition the

324
00:10:59,600 --> 00:11:02,000
decoder on the identity of the desired

325
00:11:02,000 --> 00:11:02,720
speaker

326
00:11:02,720 --> 00:11:06,160
so that the output audio xn can sound

327
00:11:06,160 --> 00:11:09,440
like spoken by target user u the voice

328
00:11:09,440 --> 00:11:10,399
conversion is then

329
00:11:10,399 --> 00:11:14,560
accomplished in the experiment step we

330
00:11:14,560 --> 00:11:16,560
focused on attaining high water rate

331
00:11:16,560 --> 00:11:17,440
against what

332
00:11:17,440 --> 00:11:20,399
a google state of art uh esr system

333
00:11:20,399 --> 00:11:22,160
first for the victim user

334
00:11:22,160 --> 00:11:24,240
we choose a male and female speaker from

335
00:11:24,240 --> 00:11:27,279
the ieee data set

336
00:11:27,279 --> 00:11:29,760
each speaker provides 720 audiences and

337
00:11:29,760 --> 00:11:31,839
our goal is to mask out all

338
00:11:31,839 --> 00:11:34,560
the audiences by voice converted

339
00:11:34,560 --> 00:11:35,600
audiences

340
00:11:35,600 --> 00:11:37,680
next we choose a random speaker from

341
00:11:37,680 --> 00:11:39,440
libya's speech dataset

342
00:11:39,440 --> 00:11:42,160
who produce 118 clean reference

343
00:11:42,160 --> 00:11:43,279
audiences

344
00:11:43,279 --> 00:11:45,760
then we use voice conversion technique

345
00:11:45,760 --> 00:11:48,160
to transfer the speaker identity of two

346
00:11:48,160 --> 00:11:49,200
target speakers

347
00:11:49,200 --> 00:11:52,160
from my triple e data set to speech

348
00:11:52,160 --> 00:11:54,000
provided by the speaker from

349
00:11:54,000 --> 00:11:56,240
libre speech data set we produce

350
00:11:56,240 --> 00:11:57,600
different combinations of these

351
00:11:57,600 --> 00:11:58,800
converted audiences

352
00:11:58,800 --> 00:12:00,240
and stack different number of

353
00:12:00,240 --> 00:12:02,160
soundtracks together to generate the

354
00:12:02,160 --> 00:12:04,160
mybible noise

355
00:12:04,160 --> 00:12:06,000
first we examine the performance of the

356
00:12:06,000 --> 00:12:08,560
asr system on clean target speech as

357
00:12:08,560 --> 00:12:09,440
baseline

358
00:12:09,440 --> 00:12:11,519
both speakers achieve an eleven percent

359
00:12:11,519 --> 00:12:12,480
whatever rate

360
00:12:12,480 --> 00:12:15,680
we then determine how inject specific my

361
00:12:15,680 --> 00:12:16,639
baby noise

362
00:12:16,639 --> 00:12:19,600
impact recognition performance we test

363
00:12:19,600 --> 00:12:21,120
six different my-valve noise

364
00:12:21,120 --> 00:12:22,320
configurations

365
00:12:22,320 --> 00:12:24,160
the member noise is combined with

366
00:12:24,160 --> 00:12:26,160
different attribute speech signals

367
00:12:26,160 --> 00:12:29,279
at 0 db snr the number of generated

368
00:12:29,279 --> 00:12:31,279
audiences varies between 1

369
00:12:31,279 --> 00:12:35,120
8 16 32 64 and 118

370
00:12:35,120 --> 00:12:38,320
converted signals the figure here shows

371
00:12:38,320 --> 00:12:40,959
the average order rate of the different

372
00:12:40,959 --> 00:12:43,360
noise realization for both attribute

373
00:12:43,360 --> 00:12:44,720
male and female signals

374
00:12:44,720 --> 00:12:47,760
the my bible signal uh the bible noise

375
00:12:47,760 --> 00:12:49,519
that generated from eight voice

376
00:12:49,519 --> 00:12:52,079
converted signals which is the vc8

377
00:12:52,079 --> 00:12:54,480
for uh produce the highest water rate

378
00:12:54,480 --> 00:12:56,480
the water rate gradually decrease as

379
00:12:56,480 --> 00:12:56,880
more

380
00:12:56,880 --> 00:12:59,600
converted speech signals are added this

381
00:12:59,600 --> 00:13:00,480
is likely

382
00:13:00,480 --> 00:13:02,720
because as more soundtracks are added

383
00:13:02,720 --> 00:13:04,480
each soundtrack creates

384
00:13:04,480 --> 00:13:06,720
too much overlap and eventually the

385
00:13:06,720 --> 00:13:08,720
noise will be sounds like a generic

386
00:13:08,720 --> 00:13:10,800
battle noise which results in a poor

387
00:13:10,800 --> 00:13:13,200
defense if two field soundtracks are

388
00:13:13,200 --> 00:13:14,480
added

389
00:13:14,480 --> 00:13:17,040
such as only one soundtrack vc1 there

390
00:13:17,040 --> 00:13:19,360
will be gaps revealing the target

391
00:13:19,360 --> 00:13:23,040
speech for attribute male signals

392
00:13:23,040 --> 00:13:27,040
these eight have 98 or 96 what i rate

393
00:13:27,040 --> 00:13:28,639
this is much higher than the baseline

394
00:13:28,639 --> 00:13:31,920
balance which produce only 61 water rate

395
00:13:31,920 --> 00:13:34,320
and caffeine noise which achieve a sixty

396
00:13:34,320 --> 00:13:35,760
percent moderate

397
00:13:35,760 --> 00:13:39,120
for entropy female signals of vc8

398
00:13:39,120 --> 00:13:41,920
outperforms all other noises with 92

399
00:13:41,920 --> 00:13:43,120
percent lower rate

400
00:13:43,120 --> 00:13:45,120
the generic bad one caffeine noise

401
00:13:45,120 --> 00:13:47,040
performs similarly to the same

402
00:13:47,040 --> 00:13:50,240
attribute male scenario with 59

403
00:13:50,240 --> 00:13:53,360
and 60 percent without rate

404
00:13:53,360 --> 00:13:56,079
besides asr system recognition result we

405
00:13:56,079 --> 00:13:57,839
also conduct a user study

406
00:13:57,839 --> 00:14:01,120
to evaluate how well the generated noise

407
00:14:01,120 --> 00:14:03,920
abstract speech against human attackers

408
00:14:03,920 --> 00:14:04,720
we recruit

409
00:14:04,720 --> 00:14:07,920
210 participants on amazon mturk

410
00:14:07,920 --> 00:14:10,639
and each participants were was present

411
00:14:10,639 --> 00:14:12,240
with 20 audio clips

412
00:14:12,240 --> 00:14:15,199
each audio eclipse was mixed with a

413
00:14:15,199 --> 00:14:16,880
baseline generic battle noise

414
00:14:16,880 --> 00:14:19,440
and our proposed 8 soundtrack voice

415
00:14:19,440 --> 00:14:21,199
conversion my babble noise

416
00:14:21,199 --> 00:14:24,000
at different snrs the number on the

417
00:14:24,000 --> 00:14:24,959
x-axis

418
00:14:24,959 --> 00:14:28,240
indication mixed at the nar level

419
00:14:28,240 --> 00:14:30,560
each participants was randomly assigned

420
00:14:30,560 --> 00:14:32,399
to one of the mixture condition

421
00:14:32,399 --> 00:14:34,639
where they were asked to type the words

422
00:14:34,639 --> 00:14:35,680
they heard

423
00:14:35,680 --> 00:14:38,959
the the study shows similar trends as as

424
00:14:38,959 --> 00:14:40,880
observed in the previous slides where

425
00:14:40,880 --> 00:14:42,800
mybible noise provides a stronger

426
00:14:42,800 --> 00:14:44,160
defense than baseline

427
00:14:44,160 --> 00:14:47,839
generic battle noise the vc8 migrable

428
00:14:47,839 --> 00:14:51,600
mixture reaches a water rate of 81 at

429
00:14:51,600 --> 00:14:54,240
0 dbsnr level which much higher than the

430
00:14:54,240 --> 00:14:58,000
generic battle noise

431
00:14:58,000 --> 00:15:01,279
water will decrease to 97.7 if we lower

432
00:15:01,279 --> 00:15:03,760
the sn level to 95 db

433
00:15:03,760 --> 00:15:07,440
however if we use snl level higher than

434
00:15:07,440 --> 00:15:09,920
0 db which indicates that the speed

435
00:15:09,920 --> 00:15:11,839
speed energy is higher than the noise

436
00:15:11,839 --> 00:15:12,560
energy

437
00:15:12,560 --> 00:15:14,959
human listeners can easily recognize the

438
00:15:14,959 --> 00:15:15,680
speech

439
00:15:15,680 --> 00:15:20,399
the mean at 5 db snr dropped to 37

440
00:15:20,399 --> 00:15:23,680
in conclusion comparing to

441
00:15:23,680 --> 00:15:26,800
other defensive approaches my bible

442
00:15:26,800 --> 00:15:27,440
noise

443
00:15:27,440 --> 00:15:31,199
mixed with speech at negative 5 db snr

444
00:15:31,199 --> 00:15:33,199
is a strong defense mechanism against

445
00:15:33,199 --> 00:15:36,639
human-based use dropping attacks

446
00:15:36,639 --> 00:15:38,720
we also built physical mybible box

447
00:15:38,720 --> 00:15:40,560
prototype of our

448
00:15:40,560 --> 00:15:42,560
defense approach on the left which is

449
00:15:42,560 --> 00:15:43,680
still far

450
00:15:43,680 --> 00:15:46,000
from the eventual vision on the right

451
00:15:46,000 --> 00:15:47,600
this approach seeks to maintain the

452
00:15:47,600 --> 00:15:49,279
usability of the phone screen and

453
00:15:49,279 --> 00:15:50,079
speaker

454
00:15:50,079 --> 00:15:53,519
yet allows for applications of the

455
00:15:53,519 --> 00:15:55,920
mybible defense on the microphone i

456
00:15:55,920 --> 00:15:56,880
described in

457
00:15:56,880 --> 00:15:59,360
more in the in the paper we also testing

458
00:15:59,360 --> 00:16:01,279
the disturbance level

459
00:16:01,279 --> 00:16:04,079
or for injecting noise in the future we

460
00:16:04,079 --> 00:16:06,079
may build a phone accessory similar to

461
00:16:06,079 --> 00:16:07,279
the right picture here

462
00:16:07,279 --> 00:16:09,120
which can have embedded speaker in your

463
00:16:09,120 --> 00:16:10,880
phone case we can

464
00:16:10,880 --> 00:16:13,120
produce my barbell noise to protect

465
00:16:13,120 --> 00:16:15,120
people's privacy

466
00:16:15,120 --> 00:16:17,440
for this paper we evaluate up skating

467
00:16:17,440 --> 00:16:18,959
noise as a defense against

468
00:16:18,959 --> 00:16:21,920
microphone based errors dropping attacks

469
00:16:21,920 --> 00:16:22,320
though

470
00:16:22,320 --> 00:16:24,720
through a comprehension evaluation we

471
00:16:24,720 --> 00:16:26,399
find that our personally

472
00:16:26,399 --> 00:16:29,279
craft my bible defense perform best and

473
00:16:29,279 --> 00:16:31,519
help to mislead current sr attacks

474
00:16:31,519 --> 00:16:34,639
as well as human-based attacks our

475
00:16:34,639 --> 00:16:36,240
proposed my back noise

476
00:16:36,240 --> 00:16:38,320
mechanism use voice conversion

477
00:16:38,320 --> 00:16:39,920
techniques to generate

478
00:16:39,920 --> 00:16:42,240
synthetic speech signals that can

479
00:16:42,240 --> 00:16:44,560
combine to form user-specific

480
00:16:44,560 --> 00:16:47,040
maturable noise these synthetic

481
00:16:47,040 --> 00:16:48,480
synthetic signals

482
00:16:48,480 --> 00:16:51,199
have the same voice characteristic as as

483
00:16:51,199 --> 00:16:52,320
target speaker

484
00:16:52,320 --> 00:16:55,360
which further confuses asr system future

485
00:16:55,360 --> 00:16:57,040
work should exploit

486
00:16:57,040 --> 00:17:00,000
the application of generative

487
00:17:00,000 --> 00:17:01,519
adversarial networks

488
00:17:01,519 --> 00:17:04,240
also known as game-based approaches to

489
00:17:04,240 --> 00:17:05,359
automatic

490
00:17:05,359 --> 00:17:09,119
early generated synthetic noise to for

491
00:17:09,119 --> 00:17:12,319
the best possible asr system also

492
00:17:12,319 --> 00:17:14,480
our work focused on evaluating

493
00:17:14,480 --> 00:17:15,679
evaluating

494
00:17:15,679 --> 00:17:18,480
a single microphone scenario particle

495
00:17:18,480 --> 00:17:19,359
defenses

496
00:17:19,359 --> 00:17:21,760
uses my bubble noise with multiple and

497
00:17:21,760 --> 00:17:22,880
directional

498
00:17:22,880 --> 00:17:25,039
microphone scenarios should also be

499
00:17:25,039 --> 00:17:28,160
exempt in the future

500
00:17:28,160 --> 00:17:30,400
thank you for listening we also want to

501
00:17:30,400 --> 00:17:33,520
thank nsf for funding this project

502
00:17:33,520 --> 00:17:39,440
thank you

