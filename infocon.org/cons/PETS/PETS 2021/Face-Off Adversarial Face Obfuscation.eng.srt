1
00:00:00,240 --> 00:00:02,879
hello my name is varun chandra shakran

2
00:00:02,879 --> 00:00:04,240
together with brian tang

3
00:00:04,240 --> 00:00:05,839
we'll be presenting our work titled

4
00:00:05,839 --> 00:00:08,639
face-off adversarial face obfuscation

5
00:00:08,639 --> 00:00:10,400
this is joint work with shu han gao

6
00:00:10,400 --> 00:00:12,080
kasam fawaz so mesha

7
00:00:12,080 --> 00:00:13,440
and suman banerjee from the university

8
00:00:13,440 --> 00:00:17,359
of wisconsin

9
00:00:17,359 --> 00:00:19,279
facial recognition is the technology

10
00:00:19,279 --> 00:00:21,119
where machines can identify faces from

11
00:00:21,119 --> 00:00:22,800
video streams or images

12
00:00:22,800 --> 00:00:25,680
this has many uses such as in airports

13
00:00:25,680 --> 00:00:26,880
to make the check-in and security

14
00:00:26,880 --> 00:00:28,560
procedures more seamless

15
00:00:28,560 --> 00:00:30,800
finding missing pets or even improving

16
00:00:30,800 --> 00:00:32,320
filters and backgrounds in web-based

17
00:00:32,320 --> 00:00:35,200
communication applications

18
00:00:35,200 --> 00:00:36,559
with the increasing use of facial

19
00:00:36,559 --> 00:00:37,920
recognition by law enforcement and

20
00:00:37,920 --> 00:00:39,040
government entities

21
00:00:39,040 --> 00:00:41,280
we're in a scenario where users have few

22
00:00:41,280 --> 00:00:42,879
options to preserve their visual privacy

23
00:00:42,879 --> 00:00:44,640
on the internet

24
00:00:44,640 --> 00:00:46,879
some companies like clearview ai have

25
00:00:46,879 --> 00:00:48,640
even scraped private and public profiles

26
00:00:48,640 --> 00:00:50,879
from facebook for faces

27
00:00:50,879 --> 00:00:52,800
data leaks are becoming more common

28
00:00:52,800 --> 00:00:55,440
leading to more fear in user minds

29
00:00:55,440 --> 00:00:57,039
so how does a concerned user protect

30
00:00:57,039 --> 00:00:58,879
their privacy their facial information

31
00:00:58,879 --> 00:01:00,399
while using social media and photo

32
00:01:00,399 --> 00:01:02,960
storage sites

33
00:01:02,960 --> 00:01:04,720
there are several options but each have

34
00:01:04,720 --> 00:01:07,040
their own pros and cons

35
00:01:07,040 --> 00:01:09,040
one can easily blur or pixelate their

36
00:01:09,040 --> 00:01:10,720
face and this provides strong privacy

37
00:01:10,720 --> 00:01:11,520
guarantees

38
00:01:11,520 --> 00:01:13,360
but it deprives the user of any utility

39
00:01:13,360 --> 00:01:14,960
benefit obtained by uploading images on

40
00:01:14,960 --> 00:01:17,439
social media platforms

41
00:01:17,439 --> 00:01:19,520
the work of sharif explores using

42
00:01:19,520 --> 00:01:21,040
physical wearables at full facial

43
00:01:21,040 --> 00:01:22,640
recognition models

44
00:01:22,640 --> 00:01:24,560
using insight from evasion they propose

45
00:01:24,560 --> 00:01:26,159
an approach that enables a recognition

46
00:01:26,159 --> 00:01:28,799
model to mislabel a user

47
00:01:28,799 --> 00:01:30,159
while these are more usable than the

48
00:01:30,159 --> 00:01:32,079
previous solution they still require the

49
00:01:32,079 --> 00:01:34,000
user to modify their image to include

50
00:01:34,000 --> 00:01:35,600
variables

51
00:01:35,600 --> 00:01:37,520
additionally their black box algorithm

52
00:01:37,520 --> 00:01:39,200
requires providing their optimization

53
00:01:39,200 --> 00:01:40,799
procedure the original facial input

54
00:01:40,799 --> 00:01:43,040
whose privacy they wish to preserve

55
00:01:43,040 --> 00:01:46,560
this leaks information about the input

56
00:01:46,560 --> 00:01:48,479
shan and his colleagues use data

57
00:01:48,479 --> 00:01:50,240
poisoning to corrupt the training of

58
00:01:50,240 --> 00:01:51,840
facial recognition models

59
00:01:51,840 --> 00:01:53,840
however this approach assumes the

60
00:01:53,840 --> 00:01:55,600
recognition models are trained on public

61
00:01:55,600 --> 00:01:56,159
data

62
00:01:56,159 --> 00:01:57,840
and will include the poison face image

63
00:01:57,840 --> 00:02:00,640
in the training set

64
00:02:01,920 --> 00:02:04,159
at a high level our solution faceoff

65
00:02:04,159 --> 00:02:06,079
digitally adds strategically generated

66
00:02:06,079 --> 00:02:08,399
noise to input faces

67
00:02:08,399 --> 00:02:10,318
this results in online facial

68
00:02:10,318 --> 00:02:12,080
recognition systems being unable to

69
00:02:12,080 --> 00:02:14,160
correctly identify the face

70
00:02:14,160 --> 00:02:16,000
better still it does this without

71
00:02:16,000 --> 00:02:17,840
requiring access to the online facial

72
00:02:17,840 --> 00:02:20,160
recognitions model

73
00:02:20,160 --> 00:02:23,440
this process is as follows first a user

74
00:02:23,440 --> 00:02:25,840
chooses a photo with a face to mask

75
00:02:25,840 --> 00:02:27,840
then faceoff generates strategic

76
00:02:27,840 --> 00:02:29,760
perturbations to the face and re-inserts

77
00:02:29,760 --> 00:02:31,920
it into the original photo

78
00:02:31,920 --> 00:02:34,319
finally the user can safely upload the

79
00:02:34,319 --> 00:02:36,160
photo to any social media site or cloud

80
00:02:36,160 --> 00:02:36,800
provider

81
00:02:36,800 --> 00:02:38,160
and the face will be tagged as a

82
00:02:38,160 --> 00:02:39,920
different identity

83
00:02:39,920 --> 00:02:41,840
note that faceoff does this without

84
00:02:41,840 --> 00:02:44,239
negatively impacting the facial detector

85
00:02:44,239 --> 00:02:47,760
permitting downstream applications

86
00:02:48,720 --> 00:02:50,720
to better understand how faceoff works

87
00:02:50,720 --> 00:02:52,319
let's begin with a primer on facial

88
00:02:52,319 --> 00:02:53,840
recognition

89
00:02:53,840 --> 00:02:56,080
first a service identifies the regional

90
00:02:56,080 --> 00:02:57,120
face

91
00:02:57,120 --> 00:02:59,360
then this region is cropped and resized

92
00:02:59,360 --> 00:03:00,640
as needed

93
00:03:00,640 --> 00:03:02,560
the crop face is fed through a deep

94
00:03:02,560 --> 00:03:04,319
neural network processing

95
00:03:04,319 --> 00:03:06,319
and the dnn returns an n-dimensional

96
00:03:06,319 --> 00:03:09,280
vector embedding as a node

97
00:03:09,280 --> 00:03:11,040
this embedding can be used in a variety

98
00:03:11,040 --> 00:03:12,640
of ways such as for clustering

99
00:03:12,640 --> 00:03:16,319
matching phases or classification

100
00:03:16,879 --> 00:03:19,040
more concretely facial recognition is

101
00:03:19,040 --> 00:03:20,800
based on metric learning

102
00:03:20,800 --> 00:03:22,400
metric embedding networks are very

103
00:03:22,400 --> 00:03:24,000
similar to deep neural networks that are

104
00:03:24,000 --> 00:03:25,519
deployed for classification

105
00:03:25,519 --> 00:03:28,239
but with some minor differences instead

106
00:03:28,239 --> 00:03:30,159
of cross-entropy loss contrast of loss

107
00:03:30,159 --> 00:03:31,599
functions such as the triplet loss is

108
00:03:31,599 --> 00:03:32,400
used

109
00:03:32,400 --> 00:03:35,120
here a learner first samples random sets

110
00:03:35,120 --> 00:03:37,840
of three faces to train on

111
00:03:37,840 --> 00:03:40,640
one is an anchor the next is a positive

112
00:03:40,640 --> 00:03:42,319
sample which matches the identity of the

113
00:03:42,319 --> 00:03:43,120
anchor

114
00:03:43,120 --> 00:03:45,200
and the third is a negative sample which

115
00:03:45,200 --> 00:03:47,360
has a different identity

116
00:03:47,360 --> 00:03:48,959
the goal of the loss function is to

117
00:03:48,959 --> 00:03:50,400
minimize the distance in the embedding

118
00:03:50,400 --> 00:03:51,760
space between the anchor and the

119
00:03:51,760 --> 00:03:52,959
positive sample

120
00:03:52,959 --> 00:03:54,400
and to maximize the distance in the

121
00:03:54,400 --> 00:03:55,920
embedding space between the anchor and

122
00:03:55,920 --> 00:03:58,239
negative sample

123
00:03:58,239 --> 00:03:59,920
after training the embedding space

124
00:03:59,920 --> 00:04:01,360
should look something like this where

125
00:04:01,360 --> 00:04:02,959
samples of morgan freeman are close to

126
00:04:02,959 --> 00:04:03,680
each other

127
00:04:03,680 --> 00:04:06,720
and the same for taylor swift when a new

128
00:04:06,720 --> 00:04:08,239
unseen face is introduced during

129
00:04:08,239 --> 00:04:09,120
inference time

130
00:04:09,120 --> 00:04:10,799
the network measures the distance in the

131
00:04:10,799 --> 00:04:12,400
embedding space to different cluster

132
00:04:12,400 --> 00:04:13,200
centroids

133
00:04:13,200 --> 00:04:15,599
and determine which identity it belongs

134
00:04:15,599 --> 00:04:17,760
to

135
00:04:18,560 --> 00:04:20,720
to generate masks we utilize insight

136
00:04:20,720 --> 00:04:22,400
from previous techniques for evasion

137
00:04:22,400 --> 00:04:23,840
which were primarily developed in the

138
00:04:23,840 --> 00:04:25,919
classification setting

139
00:04:25,919 --> 00:04:27,919
a very common example all of you must

140
00:04:27,919 --> 00:04:30,240
have seen is this panda

141
00:04:30,240 --> 00:04:32,000
the classification network predicts the

142
00:04:32,000 --> 00:04:34,720
label of this image correctly

143
00:04:34,720 --> 00:04:36,960
to ensure misclassification human

144
00:04:36,960 --> 00:04:38,639
imperceptible noise is added to the

145
00:04:38,639 --> 00:04:40,320
input

146
00:04:40,320 --> 00:04:42,000
this noise is generated by these

147
00:04:42,000 --> 00:04:43,840
optimization functions which minimize

148
00:04:43,840 --> 00:04:45,759
its perceptibility while ensuring that

149
00:04:45,759 --> 00:04:48,080
it induces a misclassification

150
00:04:48,080 --> 00:04:50,479
some common some commonly used attacks

151
00:04:50,479 --> 00:04:52,080
are the fast gradient sign method

152
00:04:52,080 --> 00:04:54,400
the carlini wagner method and projected

153
00:04:54,400 --> 00:04:56,560
gradient descent

154
00:04:56,560 --> 00:04:58,080
finally this noisy panda is

155
00:04:58,080 --> 00:05:00,000
misclassified by the network as a given

156
00:05:00,000 --> 00:05:01,680
instead of a panda

157
00:05:01,680 --> 00:05:05,360
for reference this is a given

158
00:05:06,479 --> 00:05:08,840
in our work we address the following two

159
00:05:08,840 --> 00:05:10,000
challenges

160
00:05:10,000 --> 00:05:11,600
to the best of our knowledge there were

161
00:05:11,600 --> 00:05:13,360
no known attacks against metric learning

162
00:05:13,360 --> 00:05:14,960
systems

163
00:05:14,960 --> 00:05:16,800
we do not have access to the internals

164
00:05:16,800 --> 00:05:18,560
of the models deployed on the internet

165
00:05:18,560 --> 00:05:20,560
in particular we operate in the black

166
00:05:20,560 --> 00:05:22,639
box setting without ever revealing the

167
00:05:22,639 --> 00:05:23,120
actual

168
00:05:23,120 --> 00:05:26,720
input whose privacy we wish to preserve

169
00:05:26,720 --> 00:05:28,960
we now describe how we address challenge

170
00:05:28,960 --> 00:05:30,320
one

171
00:05:30,320 --> 00:05:33,120
observe that in normal scenarios the

172
00:05:33,120 --> 00:05:34,240
embedding for a face

173
00:05:34,240 --> 00:05:36,080
is going to be closer to that of its

174
00:05:36,080 --> 00:05:38,000
source identity

175
00:05:38,000 --> 00:05:40,880
we create masks for faces such that the

176
00:05:40,880 --> 00:05:42,320
embedding of the mass face

177
00:05:42,320 --> 00:05:44,639
is further from the source identity and

178
00:05:44,639 --> 00:05:47,600
closer to a target identity

179
00:05:47,600 --> 00:05:49,440
in the embedding space this can be

180
00:05:49,440 --> 00:05:51,440
visualized as follows

181
00:05:51,440 --> 00:05:54,240
the embedding is close to its source and

182
00:05:54,240 --> 00:05:55,120
our attack

183
00:05:55,120 --> 00:05:57,680
moves the masked embedding closer to the

184
00:05:57,680 --> 00:05:59,759
target

185
00:05:59,759 --> 00:06:01,280
here's a simple formulation of our

186
00:06:01,280 --> 00:06:03,039
attack l

187
00:06:03,039 --> 00:06:04,639
denotes the new loss function we

188
00:06:04,639 --> 00:06:07,039
formalize which tries to ensure that the

189
00:06:07,039 --> 00:06:08,960
mass face is closer to the target

190
00:06:08,960 --> 00:06:12,160
than the source delta denotes the

191
00:06:12,160 --> 00:06:14,400
generated perturbation

192
00:06:14,400 --> 00:06:16,800
kappa is the margin and this can be

193
00:06:16,800 --> 00:06:19,120
thought of as the separation between the

194
00:06:19,120 --> 00:06:20,960
distance of the

195
00:06:20,960 --> 00:06:23,199
source centroid and the distance to the

196
00:06:23,199 --> 00:06:25,600
target centroid

197
00:06:25,600 --> 00:06:28,160
mu is some distance metric which is used

198
00:06:28,160 --> 00:06:30,160
to measure the perceptibility of the

199
00:06:30,160 --> 00:06:31,919
perturbation

200
00:06:31,919 --> 00:06:33,759
do read our paper for more details

201
00:06:33,759 --> 00:06:37,440
regarding the actual formulation

202
00:06:38,000 --> 00:06:39,919
the previous formulation assumes white

203
00:06:39,919 --> 00:06:41,680
box access to the model

204
00:06:41,680 --> 00:06:44,960
which is not the case in our setting

205
00:06:44,960 --> 00:06:47,919
to enable black box attacks some

206
00:06:47,919 --> 00:06:48,800
solutions do

207
00:06:48,800 --> 00:06:51,680
exist we can use expensive techniques

208
00:06:51,680 --> 00:06:52,319
like

209
00:06:52,319 --> 00:06:54,319
gradient free optimization to learn

210
00:06:54,319 --> 00:06:57,199
gradients which can further be used for

211
00:06:57,199 --> 00:07:00,240
downstream evasion efforts we can

212
00:07:00,240 --> 00:07:02,319
reverse engineer the black box model's

213
00:07:02,319 --> 00:07:04,800
parameters using model extraction

214
00:07:04,800 --> 00:07:06,479
attacks

215
00:07:06,479 --> 00:07:09,120
or we can generate adversarial examples

216
00:07:09,120 --> 00:07:13,440
that transfer from one model to another

217
00:07:13,440 --> 00:07:14,960
learning gradients through black box

218
00:07:14,960 --> 00:07:17,520
techniques is often query inefficient

219
00:07:17,520 --> 00:07:20,720
as demonstrated in prior work model

220
00:07:20,720 --> 00:07:21,599
extraction

221
00:07:21,599 --> 00:07:23,360
is unfortunately computationally

222
00:07:23,360 --> 00:07:24,639
expensive for

223
00:07:24,639 --> 00:07:27,599
complex models in addition models are

224
00:07:27,599 --> 00:07:28,319
sometimes

225
00:07:28,319 --> 00:07:31,759
updated requiring continual extraction

226
00:07:31,759 --> 00:07:33,440
so these two approaches are not very

227
00:07:33,440 --> 00:07:35,280
practical

228
00:07:35,280 --> 00:07:37,199
instead we transfer the masks we

229
00:07:37,199 --> 00:07:39,360
generate in the white box scenario

230
00:07:39,360 --> 00:07:42,400
over to unknown models however

231
00:07:42,400 --> 00:07:45,520
transferability itself is not a precise

232
00:07:45,520 --> 00:07:46,479
science and is

233
00:07:46,479 --> 00:07:50,080
still not yet well understood

234
00:07:52,400 --> 00:07:54,720
to better achieve transferability we

235
00:07:54,720 --> 00:07:56,960
leverage the process of amplification

236
00:07:56,960 --> 00:08:00,479
to increase the strength of our attacks

237
00:08:00,479 --> 00:08:03,039
given a normal image of matt damon we

238
00:08:03,039 --> 00:08:05,280
can generate an adversarial perturbation

239
00:08:05,280 --> 00:08:07,280
to mask the image

240
00:08:07,280 --> 00:08:09,759
we then amplify the perturbation with a

241
00:08:09,759 --> 00:08:11,039
constant alpha

242
00:08:11,039 --> 00:08:13,759
which is greater than one to achieve a

243
00:08:13,759 --> 00:08:15,919
stronger yet more noticeable

244
00:08:15,919 --> 00:08:18,878
masked image

245
00:08:18,960 --> 00:08:21,599
here you can see how amplification can

246
00:08:21,599 --> 00:08:23,759
push adversarial examples

247
00:08:23,759 --> 00:08:26,000
across a decision boundary that we do

248
00:08:26,000 --> 00:08:28,240
not have access to

249
00:08:28,240 --> 00:08:30,319
this is denoted by the white line in the

250
00:08:30,319 --> 00:08:31,840
figure

251
00:08:31,840 --> 00:08:34,000
and as you can see larger amplification

252
00:08:34,000 --> 00:08:35,200
factors results

253
00:08:35,200 --> 00:08:38,880
in more transferable attacks

254
00:08:38,880 --> 00:08:41,440
to empirically validate our intuition we

255
00:08:41,440 --> 00:08:43,120
ran an experiment with two face that

256
00:08:43,120 --> 00:08:45,120
models trained on different data sets

257
00:08:45,120 --> 00:08:46,959
to see how amplification affects

258
00:08:46,959 --> 00:08:49,200
transferability success

259
00:08:49,200 --> 00:08:52,160
the trends follow our intuition greater

260
00:08:52,160 --> 00:08:53,040
amplification

261
00:08:53,040 --> 00:08:55,600
improves transferability which is on the

262
00:08:55,600 --> 00:08:58,240
y-axis

263
00:08:59,839 --> 00:09:01,839
in our evaluation we wanted to

264
00:09:01,839 --> 00:09:03,680
understand the following

265
00:09:03,680 --> 00:09:06,800
do faces mass by face-off transfer to

266
00:09:06,800 --> 00:09:09,920
commercial apis in a black box setting

267
00:09:09,920 --> 00:09:12,000
can commercial apis deploy defense

268
00:09:12,000 --> 00:09:13,760
mechanisms against faceoff

269
00:09:13,760 --> 00:09:17,040
such as adversarial training and

270
00:09:17,040 --> 00:09:19,200
finally are users willing to upload

271
00:09:19,200 --> 00:09:21,120
adversarially perturbed faces

272
00:09:21,120 --> 00:09:23,360
to social media or are they too

273
00:09:23,360 --> 00:09:25,920
unsightly

274
00:09:26,000 --> 00:09:28,160
for our surrogate white box models used

275
00:09:28,160 --> 00:09:30,080
to generate masked faces

276
00:09:30,080 --> 00:09:32,560
we use facent models trained using

277
00:09:32,560 --> 00:09:34,480
different contrastive loss functions

278
00:09:34,480 --> 00:09:38,000
and different data sets we evaluate our

279
00:09:38,000 --> 00:09:38,480
face

280
00:09:38,480 --> 00:09:41,839
our masked faces on three apis as your

281
00:09:41,839 --> 00:09:42,640
face

282
00:09:42,640 --> 00:09:46,959
aws recognition and face plus plus

283
00:09:46,959 --> 00:09:49,279
and the faces we evaluate on are from a

284
00:09:49,279 --> 00:09:50,000
set of 10

285
00:09:50,000 --> 00:09:54,640
diverse individuals listed here

286
00:09:56,240 --> 00:09:58,320
we discovered that mass phases are

287
00:09:58,320 --> 00:09:59,360
misrecognized

288
00:09:59,360 --> 00:10:02,720
on commercial apis from this study

289
00:10:02,720 --> 00:10:04,640
carried out in 2018

290
00:10:04,640 --> 00:10:07,680
on the aws recognition service

291
00:10:07,680 --> 00:10:10,079
notice that the general trend is that

292
00:10:10,079 --> 00:10:10,800
increasing

293
00:10:10,800 --> 00:10:13,839
amplification causes the perturbation

294
00:10:13,839 --> 00:10:16,000
norm to be larger

295
00:10:16,000 --> 00:10:18,399
and as a result the matching confidence

296
00:10:18,399 --> 00:10:20,720
of the service provider decreases

297
00:10:20,720 --> 00:10:24,560
below the 50 mark indicating a suspect

298
00:10:24,560 --> 00:10:28,239
a successful transfer ability

299
00:10:28,320 --> 00:10:30,399
we also observe that increasing the

300
00:10:30,399 --> 00:10:32,320
margin does not have as prominent in

301
00:10:32,320 --> 00:10:32,959
fact

302
00:10:32,959 --> 00:10:36,320
as increasing the amplification also

303
00:10:36,320 --> 00:10:37,440
amplification is done

304
00:10:37,440 --> 00:10:39,920
as a post-processing step and is not on

305
00:10:39,920 --> 00:10:42,640
the critical path

306
00:10:42,640 --> 00:10:45,120
we also verified if these same images

307
00:10:45,120 --> 00:10:46,160
were misclassified

308
00:10:46,160 --> 00:10:49,360
in 2020 two years after being generated

309
00:10:49,360 --> 00:10:50,959
and we observed that with larger

310
00:10:50,959 --> 00:10:52,640
amplification factors

311
00:10:52,640 --> 00:10:56,480
they still do we also observe

312
00:10:56,480 --> 00:10:59,440
similar trends for face plus plus and as

313
00:10:59,440 --> 00:11:00,560
your face

314
00:11:00,560 --> 00:11:02,959
as well

315
00:11:04,560 --> 00:11:06,640
adversarial training is one of the most

316
00:11:06,640 --> 00:11:08,800
robust defenses against evasion in the

317
00:11:08,800 --> 00:11:11,040
classification setting

318
00:11:11,040 --> 00:11:12,560
to understand if defending against

319
00:11:12,560 --> 00:11:14,079
face-off is possible

320
00:11:14,079 --> 00:11:15,680
we perform a type of adversarial

321
00:11:15,680 --> 00:11:17,440
training where half of the training

322
00:11:17,440 --> 00:11:20,560
samples are adversarially perturbed

323
00:11:20,560 --> 00:11:23,040
holding all parameters constant we train

324
00:11:23,040 --> 00:11:25,120
at face that model on two different data

325
00:11:25,120 --> 00:11:25,920
sets

326
00:11:25,920 --> 00:11:29,040
and we evaluate them on a test set of

327
00:11:29,040 --> 00:11:31,760
normal images for two types of just

328
00:11:31,760 --> 00:11:33,600
metrics

329
00:11:33,600 --> 00:11:35,680
observe that the accuracy on the

330
00:11:35,680 --> 00:11:38,079
adversarially trained face net model

331
00:11:38,079 --> 00:11:40,640
is much lower than that of the naturally

332
00:11:40,640 --> 00:11:42,959
trained face that bottle

333
00:11:42,959 --> 00:11:45,200
this severely hampers usability of such

334
00:11:45,200 --> 00:11:47,680
recognition networks

335
00:11:47,680 --> 00:11:50,560
in summary we find that adversarial

336
00:11:50,560 --> 00:11:51,279
training

337
00:11:51,279 --> 00:11:52,880
does improve the robustness of the

338
00:11:52,880 --> 00:11:54,480
network but it

339
00:11:54,480 --> 00:11:57,279
decreases the matching accuracy

340
00:11:57,279 --> 00:12:00,079
significantly

341
00:12:01,600 --> 00:12:03,519
finally we wish to know whether users

342
00:12:03,519 --> 00:12:05,600
are comfortable with the faces generated

343
00:12:05,600 --> 00:12:06,639
by faceoff

344
00:12:06,639 --> 00:12:08,480
comfortable enough to upload them on

345
00:12:08,480 --> 00:12:10,639
social media so we perform

346
00:12:10,639 --> 00:12:12,880
three user studies and for the first two

347
00:12:12,880 --> 00:12:15,279
studies we consider portrait images

348
00:12:15,279 --> 00:12:17,120
where the desired subject is at the

349
00:12:17,120 --> 00:12:18,959
center of the images

350
00:12:18,959 --> 00:12:21,360
and background images where the desired

351
00:12:21,360 --> 00:12:22,839
subject is at the background of the

352
00:12:22,839 --> 00:12:24,560
image

353
00:12:24,560 --> 00:12:27,040
we ask users to rank their comfort of

354
00:12:27,040 --> 00:12:29,360
uploading said images

355
00:12:29,360 --> 00:12:31,600
and in the last study to maintain the

356
00:12:31,600 --> 00:12:34,160
ecological validity of our scenario

357
00:12:34,160 --> 00:12:36,320
we ask users to upload and mask their

358
00:12:36,320 --> 00:12:37,760
own faces

359
00:12:37,760 --> 00:12:41,040
using our face off web service and

360
00:12:41,040 --> 00:12:42,880
rank whether they'd be comfortable

361
00:12:42,880 --> 00:12:46,160
uploading the result to social media

362
00:12:46,160 --> 00:12:48,560
we found that users are generally more

363
00:12:48,560 --> 00:12:49,279
comfortable

364
00:12:49,279 --> 00:12:51,920
uploading lower amplification images to

365
00:12:51,920 --> 00:12:53,200
social media

366
00:12:53,200 --> 00:12:55,040
and they're also more comfortable

367
00:12:55,040 --> 00:12:57,120
uploading background images

368
00:12:57,120 --> 00:12:59,200
but not comfortable uploading portrait

369
00:12:59,200 --> 00:13:01,440
images of faces

370
00:13:01,440 --> 00:13:04,079
and finally in our last study we found

371
00:13:04,079 --> 00:13:06,160
privacy conscious users are also

372
00:13:06,160 --> 00:13:06,800
comfortable

373
00:13:06,800 --> 00:13:09,120
uploading their own masked faces that

374
00:13:09,120 --> 00:13:12,880
were output by our web service

375
00:13:14,079 --> 00:13:15,920
now let's look at some of the examples

376
00:13:15,920 --> 00:13:20,079
of mass faces generated by faceoff

377
00:13:20,079 --> 00:13:22,000
looking at the faces of morgan freeman

378
00:13:22,000 --> 00:13:23,920
and matt damon here

379
00:13:23,920 --> 00:13:26,720
the perturbation on morgan freeman is

380
00:13:26,720 --> 00:13:27,760
centered around

381
00:13:27,760 --> 00:13:30,000
the nose whereas on matt damon it's the

382
00:13:30,000 --> 00:13:31,600
entire face

383
00:13:31,600 --> 00:13:34,560
these differences in perturbation styles

384
00:13:34,560 --> 00:13:35,440
are a result

385
00:13:35,440 --> 00:13:38,320
of varying the amplification factor the

386
00:13:38,320 --> 00:13:40,720
surrogate model used in the attack loop

387
00:13:40,720 --> 00:13:42,880
the source target pair selection and

388
00:13:42,880 --> 00:13:44,480
other factors which we discussed

389
00:13:44,480 --> 00:13:49,440
in the paper

390
00:13:49,440 --> 00:13:52,160
so in conclusion we've discussed that

391
00:13:52,160 --> 00:13:52,959
online

392
00:13:52,959 --> 00:13:55,199
and cloud-based face recognition harms

393
00:13:55,199 --> 00:13:56,399
user privacy

394
00:13:56,399 --> 00:13:58,240
and there's few options the user can

395
00:13:58,240 --> 00:14:00,959
take to preserve their face data

396
00:14:00,959 --> 00:14:02,560
evasion attacks in the form of

397
00:14:02,560 --> 00:14:04,160
adversarial examples

398
00:14:04,160 --> 00:14:05,920
allow us to evade face recognition

399
00:14:05,920 --> 00:14:08,480
models while maintaining the utility of

400
00:14:08,480 --> 00:14:11,040
the masked face

401
00:14:11,040 --> 00:14:13,120
a user can still upload their photo to

402
00:14:13,120 --> 00:14:14,399
social media

403
00:14:14,399 --> 00:14:16,880
this way and we found that privacy

404
00:14:16,880 --> 00:14:18,000
conscious users

405
00:14:18,000 --> 00:14:20,000
are willing to protect their faces at

406
00:14:20,000 --> 00:14:21,360
the expense of

407
00:14:21,360 --> 00:14:24,560
minor perturbations and noise and for

408
00:14:24,560 --> 00:14:25,760
more information

409
00:14:25,760 --> 00:14:28,240
please check out our paper or our github

410
00:14:28,240 --> 00:14:29,680
or our website

411
00:14:29,680 --> 00:14:31,920
or contact us at one of the emails

412
00:14:31,920 --> 00:14:33,199
listed here

413
00:14:33,199 --> 00:14:41,599
thanks everyone for listening

