1
00:00:00,640 --> 00:00:03,360
hi this talk is about work i did

2
00:00:03,360 --> 00:00:04,799
together with daniel escudero and

3
00:00:04,799 --> 00:00:06,640
nikolai vogelschev on securely training

4
00:00:06,640 --> 00:00:08,160
decision trees with continuous

5
00:00:08,160 --> 00:00:09,599
attributes

6
00:00:09,599 --> 00:00:12,400
so decision tree is a very simple

7
00:00:12,400 --> 00:00:13,759
predictive model

8
00:00:13,759 --> 00:00:14,960
say i want to predict whether it's going

9
00:00:14,960 --> 00:00:17,039
to rain tomorrow

10
00:00:17,039 --> 00:00:19,119
then my model could be as follows i look

11
00:00:19,119 --> 00:00:20,240
at the temperature today

12
00:00:20,240 --> 00:00:22,240
if it's more than 20 degrees i say

13
00:00:22,240 --> 00:00:23,439
there's not it's not going to rain

14
00:00:23,439 --> 00:00:24,480
tomorrow

15
00:00:24,480 --> 00:00:26,000
if it is less than 20 degrees i look at

16
00:00:26,000 --> 00:00:27,519
the rainfall today and if it's more than

17
00:00:27,519 --> 00:00:28,960
two millimeters i say it's going to rain

18
00:00:28,960 --> 00:00:30,080
tomorrow and otherwise

19
00:00:30,080 --> 00:00:32,640
i say it's not going to rain tomorrow

20
00:00:32,640 --> 00:00:34,399
now to come up with this model

21
00:00:34,399 --> 00:00:36,960
um i could for example for one month

22
00:00:36,960 --> 00:00:38,239
keep track of

23
00:00:38,239 --> 00:00:40,239
rainfall temperature and look whether

24
00:00:40,239 --> 00:00:41,840
it's going to rain the next day

25
00:00:41,840 --> 00:00:43,760
feed this into a training algorithm and

26
00:00:43,760 --> 00:00:45,280
the training algorithm

27
00:00:45,280 --> 00:00:46,960
might come up with this model as sort of

28
00:00:46,960 --> 00:00:49,360
like the the best decision tree that

29
00:00:49,360 --> 00:00:50,160
matches the

30
00:00:50,160 --> 00:00:53,039
the data using this decision really can

31
00:00:53,039 --> 00:00:54,160
then predict

32
00:00:54,160 --> 00:00:57,600
outcomes on new data so

33
00:00:57,600 --> 00:00:59,120
here in this talk we are looking at

34
00:00:59,120 --> 00:01:00,879
securely training decision trees where

35
00:01:00,879 --> 00:01:02,879
the training data is sort of like

36
00:01:02,879 --> 00:01:06,159
um owned collectively by a set of

37
00:01:06,159 --> 00:01:06,799
parties

38
00:01:06,799 --> 00:01:08,400
they each own a part of the training

39
00:01:08,400 --> 00:01:10,080
data but they do not want to share this

40
00:01:10,080 --> 00:01:11,439
training data with each other but they

41
00:01:11,439 --> 00:01:12,159
are all

42
00:01:12,159 --> 00:01:14,560
interested in the output model so

43
00:01:14,560 --> 00:01:16,000
ideally they would have this magic box

44
00:01:16,000 --> 00:01:17,520
they send their part of the data to this

45
00:01:17,520 --> 00:01:19,280
metric box

46
00:01:19,280 --> 00:01:20,960
the box computes the decision tree and

47
00:01:20,960 --> 00:01:22,880
then outputs the decision tree to all of

48
00:01:22,880 --> 00:01:25,119
the parties

49
00:01:25,119 --> 00:01:28,560
um so when i say they jointly have the

50
00:01:28,560 --> 00:01:30,159
data what could that mean for example we

51
00:01:30,159 --> 00:01:31,439
have this this table

52
00:01:31,439 --> 00:01:34,560
each party might own a different set of

53
00:01:34,560 --> 00:01:35,119
records

54
00:01:35,119 --> 00:01:36,560
in this tree then we'd say it's

55
00:01:36,560 --> 00:01:38,159
horizontally partitioned

56
00:01:38,159 --> 00:01:39,520
vertically partitioned is when the

57
00:01:39,520 --> 00:01:41,520
parties have access to

58
00:01:41,520 --> 00:01:43,680
each to some attributes but for all of

59
00:01:43,680 --> 00:01:46,000
the records

60
00:01:46,000 --> 00:01:47,520
our approach is more general so we

61
00:01:47,520 --> 00:01:48,880
assume the input data and the output

62
00:01:48,880 --> 00:01:51,200
data are secret shared which means that

63
00:01:51,200 --> 00:01:54,479
basically any composition of this table

64
00:01:54,479 --> 00:01:57,600
can occur for the input data

65
00:01:57,600 --> 00:01:59,920
one party might just have one cell or

66
00:01:59,920 --> 00:02:01,680
the table might be the result

67
00:02:01,680 --> 00:02:05,759
of some earlier secure computation

68
00:02:06,079 --> 00:02:09,199
so we use multi-party computation which

69
00:02:09,199 --> 00:02:10,560
is a technique

70
00:02:10,560 --> 00:02:13,680
uh that allows us to evaluate any

71
00:02:13,680 --> 00:02:16,400
function securely and it offers an

72
00:02:16,400 --> 00:02:18,000
absolute privacy guarantee within the

73
00:02:18,000 --> 00:02:19,520
security assumptions the security

74
00:02:19,520 --> 00:02:21,040
assumptions might be that

75
00:02:21,040 --> 00:02:23,040
no two of the parties collude with each

76
00:02:23,040 --> 00:02:25,200
other and share their data

77
00:02:25,200 --> 00:02:27,520
so with mpc these part the parties are

78
00:02:27,520 --> 00:02:29,520
able to sort of like emulate this this

79
00:02:29,520 --> 00:02:30,959
magic box

80
00:02:30,959 --> 00:02:34,160
that computes any function um

81
00:02:34,160 --> 00:02:38,080
by running a protocol among the parties

82
00:02:38,080 --> 00:02:40,160
um this absolute privacy guarantee of

83
00:02:40,160 --> 00:02:41,760
npc is a contrast to for example

84
00:02:41,760 --> 00:02:43,200
federated learning

85
00:02:43,200 --> 00:02:44,720
or differential privacy where usually

86
00:02:44,720 --> 00:02:46,400
there's some bound on the privacy but

87
00:02:46,400 --> 00:02:48,160
there is always some information being

88
00:02:48,160 --> 00:02:48,640
leaked

89
00:02:48,640 --> 00:02:50,160
in npc the only information that is

90
00:02:50,160 --> 00:02:53,760
being leaked is the output data

91
00:02:53,760 --> 00:02:55,519
we use generic primitive so we build on

92
00:02:55,519 --> 00:02:58,159
top of a generic mpc protocol

93
00:02:58,159 --> 00:02:59,760
that just implements secret sharing

94
00:02:59,760 --> 00:03:01,920
which means sort of encryption of values

95
00:03:01,920 --> 00:03:04,000
where each party gets its share of the

96
00:03:04,000 --> 00:03:06,640
of the value and they have protocols for

97
00:03:06,640 --> 00:03:08,480
computing sums and products

98
00:03:08,480 --> 00:03:12,400
of these of these encrypted values

99
00:03:12,400 --> 00:03:14,000
also very important for decision trees

100
00:03:14,000 --> 00:03:15,840
is secure comparisons so we have two

101
00:03:15,840 --> 00:03:18,560
values and we want to learn a bit

102
00:03:18,560 --> 00:03:20,000
to learn whether a second value is less

103
00:03:20,000 --> 00:03:23,519
than or equal to the first value

104
00:03:24,239 --> 00:03:26,239
because that's the the main primitive

105
00:03:26,239 --> 00:03:30,080
that we we are going to be using

106
00:03:30,239 --> 00:03:32,799
so we can evaluate any function using

107
00:03:32,799 --> 00:03:34,720
npc but there's a large overhead so

108
00:03:34,720 --> 00:03:37,040
what if an algorithm takes one second to

109
00:03:37,040 --> 00:03:38,879
run on your local machine

110
00:03:38,879 --> 00:03:43,040
in the clear then using mpc the same

111
00:03:43,040 --> 00:03:46,000
same protocol might take days so there's

112
00:03:46,000 --> 00:03:46,480
a big

113
00:03:46,480 --> 00:03:49,280
incentive to develop efficient

114
00:03:49,280 --> 00:03:50,560
algorithms

115
00:03:50,560 --> 00:03:52,959
because the running costs overhead are

116
00:03:52,959 --> 00:03:54,879
large

117
00:03:54,879 --> 00:03:58,239
so we developed a secure algorithm

118
00:03:58,239 --> 00:04:00,560
for training decision tree and we

119
00:04:00,560 --> 00:04:01,599
wondered is this

120
00:04:01,599 --> 00:04:05,200
good enough basically do we get um

121
00:04:05,200 --> 00:04:06,640
an acceptable running cost in time and

122
00:04:06,640 --> 00:04:08,720
then the resulting model is it going to

123
00:04:08,720 --> 00:04:09,519
be similar

124
00:04:09,519 --> 00:04:12,159
to in the clear learning methods and as

125
00:04:12,159 --> 00:04:14,239
we shall see later in this talk

126
00:04:14,239 --> 00:04:17,599
we found the answer to be yes

127
00:04:18,000 --> 00:04:21,120
most prior work on on decision trees in

128
00:04:21,120 --> 00:04:23,520
npc focuses on secure evaluation so

129
00:04:23,520 --> 00:04:25,440
say we have two parties one of them has

130
00:04:25,440 --> 00:04:27,199
a model the other has a

131
00:04:27,199 --> 00:04:30,240
a a sample and it wants to learn

132
00:04:30,240 --> 00:04:31,919
what's to sort of feed the sample

133
00:04:31,919 --> 00:04:33,919
through the model without the party that

134
00:04:33,919 --> 00:04:35,600
owns the model to learn anything

135
00:04:35,600 --> 00:04:38,880
about the data or the output

136
00:04:38,880 --> 00:04:40,160
but here we focus on actually

137
00:04:40,160 --> 00:04:42,000
constructing this decision tree

138
00:04:42,000 --> 00:04:44,080
so there has been prior work on this

139
00:04:44,080 --> 00:04:45,199
which was a

140
00:04:45,199 --> 00:04:47,199
secure version of id3 which is limited

141
00:04:47,199 --> 00:04:49,120
to discrete attributes

142
00:04:49,120 --> 00:04:52,560
so we only have say binary or attributes

143
00:04:52,560 --> 00:04:54,880
with a few

144
00:04:54,880 --> 00:04:57,759
few possible set of values uh this

145
00:04:57,759 --> 00:04:59,759
allows for a very efficient algorithm

146
00:04:59,759 --> 00:05:02,720
um but uh in practice we often have

147
00:05:02,720 --> 00:05:04,240
continuous attributes

148
00:05:04,240 --> 00:05:07,120
so we're we are interested in this so

149
00:05:07,120 --> 00:05:07,600
this

150
00:05:07,600 --> 00:05:10,560
the in the clear algorithms for this are

151
00:05:10,560 --> 00:05:13,120
usually called c4.5 or cart and we have

152
00:05:13,120 --> 00:05:14,479
a sort of

153
00:05:14,479 --> 00:05:17,039
um very stripped down version of this

154
00:05:17,039 --> 00:05:18,080
that we're going to

155
00:05:18,080 --> 00:05:21,199
develop into a secure outward so this

156
00:05:21,199 --> 00:05:22,960
basic strip down algorithm

157
00:05:22,960 --> 00:05:26,000
looks like this for each attribute we're

158
00:05:26,000 --> 00:05:26,960
going to consider

159
00:05:26,960 --> 00:05:29,360
all of the values as potential splitting

160
00:05:29,360 --> 00:05:30,160
points

161
00:05:30,160 --> 00:05:32,560
so we take take an attribute we take

162
00:05:32,560 --> 00:05:33,840
first value and then we're going to

163
00:05:33,840 --> 00:05:34,639
consider

164
00:05:34,639 --> 00:05:37,840
which rows have value temperature less

165
00:05:37,840 --> 00:05:39,120
than or equal to

166
00:05:39,120 --> 00:05:41,600
19.1 degrees in this case we have four

167
00:05:41,600 --> 00:05:42,639
rows

168
00:05:42,639 --> 00:05:43,759
and then we're going to look at the

169
00:05:43,759 --> 00:05:45,759
corresponding outcomes

170
00:05:45,759 --> 00:05:49,919
ideally we want all yeses or all no's

171
00:05:49,919 --> 00:05:52,800
but this doesn't happen so this doesn't

172
00:05:52,800 --> 00:05:53,840
usually happen

173
00:05:53,840 --> 00:05:56,080
so we measure how good the splitting

174
00:05:56,080 --> 00:05:56,880
point is

175
00:05:56,880 --> 00:05:58,319
and we measure this with the gini

176
00:05:58,319 --> 00:06:00,240
impurity

177
00:06:00,240 --> 00:06:03,280
so for each of the values that we have

178
00:06:03,280 --> 00:06:05,600
we're going to take the weighted average

179
00:06:05,600 --> 00:06:08,639
of the gini impurity on the left

180
00:06:08,639 --> 00:06:10,639
of the split and the rows on the right

181
00:06:10,639 --> 00:06:11,680
of the split

182
00:06:11,680 --> 00:06:13,840
so on the left we have these four rows

183
00:06:13,840 --> 00:06:15,840
we see the fraction is one half so the

184
00:06:15,840 --> 00:06:16,639
gini

185
00:06:16,639 --> 00:06:19,919
impurity will be one half and we weight

186
00:06:19,919 --> 00:06:21,360
this with four rows

187
00:06:21,360 --> 00:06:23,199
and on the other side we have one row

188
00:06:23,199 --> 00:06:24,400
with

189
00:06:24,400 --> 00:06:25,919
perfect splits because we only have one

190
00:06:25,919 --> 00:06:27,600
value

191
00:06:27,600 --> 00:06:30,160
and so the algorithm proceeds as follows

192
00:06:30,160 --> 00:06:31,280
for all attributes

193
00:06:31,280 --> 00:06:34,560
for all splitting values calculate this

194
00:06:34,560 --> 00:06:35,520
weighted

195
00:06:35,520 --> 00:06:39,280
gini value and then minimize it

196
00:06:39,280 --> 00:06:42,000
then output the node that the attribute

197
00:06:42,000 --> 00:06:43,120
and the splitting

198
00:06:43,120 --> 00:06:45,840
value that minimizes this and the

199
00:06:45,840 --> 00:06:47,759
downside of this is that

200
00:06:47,759 --> 00:06:49,759
it requires a lot of secure comparisons

201
00:06:49,759 --> 00:06:51,840
per node and this is very expensive in

202
00:06:51,840 --> 00:06:53,120
npc

203
00:06:53,120 --> 00:06:55,440
so in our secure algorithm we're first

204
00:06:55,440 --> 00:06:56,639
going to sort

205
00:06:56,639 --> 00:06:59,039
based on each attribute for this we use

206
00:06:59,039 --> 00:07:00,560
a sorting network

207
00:07:00,560 --> 00:07:02,400
certain network looks like this for

208
00:07:02,400 --> 00:07:04,080
example for four values we have this

209
00:07:04,080 --> 00:07:05,280
sorting network

210
00:07:05,280 --> 00:07:07,440
uh we go from left to right we have this

211
00:07:07,440 --> 00:07:08,720
value we feed

212
00:07:08,720 --> 00:07:12,000
that we feed into it so these bars

213
00:07:12,000 --> 00:07:12,800
indicate

214
00:07:12,800 --> 00:07:15,039
comparator gates so here we compare

215
00:07:15,039 --> 00:07:16,960
whether y3

216
00:07:16,960 --> 00:07:20,960
is um less than or

217
00:07:20,960 --> 00:07:24,479
yeah say less than y1 this is true

218
00:07:24,479 --> 00:07:27,680
so we swap these two values so y3

219
00:07:27,680 --> 00:07:30,240
goes to the first position then we keep

220
00:07:30,240 --> 00:07:31,039
on going

221
00:07:31,039 --> 00:07:33,360
so y3 here we swap again here we swap

222
00:07:33,360 --> 00:07:35,360
again so y3 is going to end up

223
00:07:35,360 --> 00:07:38,960
in the third position now the nice thing

224
00:07:38,960 --> 00:07:40,880
uh so the constructions for this sorting

225
00:07:40,880 --> 00:07:42,479
network like an odd even merge sort

226
00:07:42,479 --> 00:07:43,199
exists with

227
00:07:43,199 --> 00:07:46,000
log square and depth the nice thing and

228
00:07:46,000 --> 00:07:48,240
why we use a sorting network is because

229
00:07:48,240 --> 00:07:50,160
we can evaluate this sorting network

230
00:07:50,160 --> 00:07:51,680
once and if we record

231
00:07:51,680 --> 00:07:55,039
these the results of these comparisons

232
00:07:55,039 --> 00:07:57,759
then we can feed other vectors into this

233
00:07:57,759 --> 00:07:58,960
sorting network and then

234
00:07:58,960 --> 00:08:02,160
gets the um sort of this allows us to

235
00:08:02,160 --> 00:08:03,280
store and apply this

236
00:08:03,280 --> 00:08:06,318
sorting permutation

237
00:08:06,479 --> 00:08:08,479
so in our secure algorithm we're first

238
00:08:08,479 --> 00:08:10,560
going to sort on each

239
00:08:10,560 --> 00:08:12,560
attribute now for now just assume

240
00:08:12,560 --> 00:08:14,080
distinct values in the next slide we're

241
00:08:14,080 --> 00:08:14,639
tackling

242
00:08:14,639 --> 00:08:17,199
uh duplicate values because this is an

243
00:08:17,199 --> 00:08:19,680
additional difficulty

244
00:08:19,680 --> 00:08:21,680
so and then for each node in a tree

245
00:08:21,680 --> 00:08:23,440
we're going to keep track of the active

246
00:08:23,440 --> 00:08:24,160
samples

247
00:08:24,160 --> 00:08:26,080
so for the root note we are considering

248
00:08:26,080 --> 00:08:27,360
all rows but

249
00:08:27,360 --> 00:08:28,800
after the root note we're going to split

250
00:08:28,800 --> 00:08:30,400
into the left

251
00:08:30,400 --> 00:08:32,640
uh branch and the right branch and for

252
00:08:32,640 --> 00:08:34,240
the left branch we're only considering a

253
00:08:34,240 --> 00:08:34,880
subset

254
00:08:34,880 --> 00:08:37,440
of the rows uh and because we're doing

255
00:08:37,440 --> 00:08:39,440
this obliviously we cannot just reduce

256
00:08:39,440 --> 00:08:40,799
the number of rows

257
00:08:40,799 --> 00:08:43,839
but we're going to um

258
00:08:43,839 --> 00:08:45,680
we're going to select based on oblivious

259
00:08:45,680 --> 00:08:47,360
bits whether we take the row into

260
00:08:47,360 --> 00:08:49,200
account or not

261
00:08:49,200 --> 00:08:50,720
so we have the secret chart indicator

262
00:08:50,720 --> 00:08:53,519
vector with binary binary values that

263
00:08:53,519 --> 00:08:55,920
we call active samples to indicate

264
00:08:55,920 --> 00:08:57,360
whether we take the row into account or

265
00:08:57,360 --> 00:08:58,160
not

266
00:08:58,160 --> 00:09:01,279
so if we have this table then we're

267
00:09:01,279 --> 00:09:02,560
going to sort

268
00:09:02,560 --> 00:09:06,160
based on an attribute and then we're

269
00:09:06,160 --> 00:09:08,000
going to consider

270
00:09:08,000 --> 00:09:10,080
first the first row as a splitting point

271
00:09:10,080 --> 00:09:12,000
and then the second row etc

272
00:09:12,000 --> 00:09:14,800
and then um because we have sorted

273
00:09:14,800 --> 00:09:15,920
already so we we

274
00:09:15,920 --> 00:09:17,760
we're also applying the we've also

275
00:09:17,760 --> 00:09:19,200
applied the sorting permutation to the

276
00:09:19,200 --> 00:09:20,959
outcome

277
00:09:20,959 --> 00:09:23,760
the outcome uh column and then we only

278
00:09:23,760 --> 00:09:26,000
need to count the number of yeses here

279
00:09:26,000 --> 00:09:28,560
to get this uh this weighted average

280
00:09:28,560 --> 00:09:29,279
ginny

281
00:09:29,279 --> 00:09:31,440
value so we're going to do this for each

282
00:09:31,440 --> 00:09:33,200
row

283
00:09:33,200 --> 00:09:35,519
and uh if the active column is zero then

284
00:09:35,519 --> 00:09:37,120
we just disregard

285
00:09:37,120 --> 00:09:40,240
the the gini value that we calculate

286
00:09:40,240 --> 00:09:43,040
and then we take the again the the best

287
00:09:43,040 --> 00:09:45,360
possible attribute

288
00:09:45,360 --> 00:09:48,000
and value so we need to apply this

289
00:09:48,000 --> 00:09:49,680
permutation for every node so this also

290
00:09:49,680 --> 00:09:50,000
becomes

291
00:09:50,000 --> 00:09:52,160
a bit expensive because the sorting

292
00:09:52,160 --> 00:09:53,360
network requires n

293
00:09:53,360 --> 00:09:57,040
log square n of these swaps

294
00:09:57,040 --> 00:09:58,959
and to optimize this we actually convert

295
00:09:58,959 --> 00:10:00,959
this into a permutation network which is

296
00:10:00,959 --> 00:10:01,760
a

297
00:10:01,760 --> 00:10:05,600
which we have of smaller size

298
00:10:05,600 --> 00:10:08,720
now to handle duplicate values

299
00:10:08,720 --> 00:10:11,200
basically we're going to check whether

300
00:10:11,200 --> 00:10:12,000
the next

301
00:10:12,000 --> 00:10:15,200
row has a different value if so we take

302
00:10:15,200 --> 00:10:17,279
their own into account and otherwise not

303
00:10:17,279 --> 00:10:19,519
so the first row yeah this is this is

304
00:10:19,519 --> 00:10:21,040
valid this is valid

305
00:10:21,040 --> 00:10:23,120
but here we have a row where we have an

306
00:10:23,120 --> 00:10:24,160
identical

307
00:10:24,160 --> 00:10:26,480
value and this means that this splitting

308
00:10:26,480 --> 00:10:28,000
point we can never get it because if we

309
00:10:28,000 --> 00:10:29,600
say temperature less than or equal to

310
00:10:29,600 --> 00:10:31,279
12.4 degrees

311
00:10:31,279 --> 00:10:33,120
then it will always take both of these

312
00:10:33,120 --> 00:10:35,120
rows into account so this is not a valid

313
00:10:35,120 --> 00:10:37,200
splitting point so this is why we have

314
00:10:37,200 --> 00:10:38,720
this check

315
00:10:38,720 --> 00:10:41,920
we keep on going through the table

316
00:10:41,920 --> 00:10:44,800
but we see this causes a problem here

317
00:10:44,800 --> 00:10:45,440
because

318
00:10:45,440 --> 00:10:47,279
here the next value is the same but it's

319
00:10:47,279 --> 00:10:48,560
not active

320
00:10:48,560 --> 00:10:50,720
so we need to take into account only the

321
00:10:50,720 --> 00:10:52,240
active rows when considering these

322
00:10:52,240 --> 00:10:54,720
neighbors

323
00:10:54,880 --> 00:10:56,160
and this becomes a problem because we

324
00:10:56,160 --> 00:10:58,880
don't know how how many non-active rows

325
00:10:58,880 --> 00:11:00,079
there are

326
00:11:00,079 --> 00:11:03,920
so our solution is basically to copy

327
00:11:03,920 --> 00:11:08,079
the rows that have active values over

328
00:11:08,079 --> 00:11:10,640
preceding inactive values

329
00:11:10,640 --> 00:11:13,600
and this with this we have a solution

330
00:11:13,600 --> 00:11:15,600
that does this in log and depth so we

331
00:11:15,600 --> 00:11:18,079
we basically per node we only have this

332
00:11:18,079 --> 00:11:19,920
log and depth factor which

333
00:11:19,920 --> 00:11:22,240
greatly improves the efficiency of our

334
00:11:22,240 --> 00:11:24,720
our method

335
00:11:27,680 --> 00:11:30,959
so um for the um basically we

336
00:11:30,959 --> 00:11:32,959
implemented our protocol using the mp

337
00:11:32,959 --> 00:11:34,800
speeds framework which offers a whole

338
00:11:34,800 --> 00:11:35,519
bunch of

339
00:11:35,519 --> 00:11:39,279
uh basic uh these mpc primitives

340
00:11:39,279 --> 00:11:42,320
uh different different uh different mpc

341
00:11:42,320 --> 00:11:44,079
protocols

342
00:11:44,079 --> 00:11:46,560
and we use computations in the ring so

343
00:11:46,560 --> 00:11:47,279
modulo

344
00:11:47,279 --> 00:11:49,920
2 to 64 which suffices when n is less

345
00:11:49,920 --> 00:11:50,880
than or equal to

346
00:11:50,880 --> 00:11:54,720
2 to the 13. we use three parties with

347
00:11:54,720 --> 00:11:56,639
tolerating one corruption so we want no

348
00:11:56,639 --> 00:11:57,760
two parties to collude

349
00:11:57,760 --> 00:11:59,360
uh based on replicated chicken sharing

350
00:11:59,360 --> 00:12:01,040
which is very fast it doesn't work

351
00:12:01,040 --> 00:12:03,839
especially in cpus so for passive

352
00:12:03,839 --> 00:12:05,120
security we have these

353
00:12:05,120 --> 00:12:06,639
cheap dot products the cost of one

354
00:12:06,639 --> 00:12:08,160
multiplication which we use often when

355
00:12:08,160 --> 00:12:09,519
we count these number of

356
00:12:09,519 --> 00:12:13,120
yes and no values in the outcome column

357
00:12:13,120 --> 00:12:15,680
um then we run benchmarks on the the

358
00:12:15,680 --> 00:12:18,399
amazon cloud

359
00:12:18,399 --> 00:12:22,480
and we use the the running times for

360
00:12:22,480 --> 00:12:23,440
these

361
00:12:23,440 --> 00:12:25,600
um so we have this as i mentioned in the

362
00:12:25,600 --> 00:12:27,200
at the start we have the use case we've

363
00:12:27,200 --> 00:12:29,279
sort of validated whether we can achieve

364
00:12:29,279 --> 00:12:30,720
comparable accuracy

365
00:12:30,720 --> 00:12:33,519
and in an acceptable running time with

366
00:12:33,519 --> 00:12:33,839
our

367
00:12:33,839 --> 00:12:35,680
protocol to state-of-the-art in the

368
00:12:35,680 --> 00:12:37,120
clear method

369
00:12:37,120 --> 00:12:39,120
so we have this real world data set

370
00:12:39,120 --> 00:12:40,880
which has has almost 300

371
00:12:40,880 --> 00:12:44,240
000 records and 128 attributes

372
00:12:44,240 --> 00:12:47,360
and so the

373
00:12:47,360 --> 00:12:49,920
the the goal of this this original

374
00:12:49,920 --> 00:12:50,880
analysis

375
00:12:50,880 --> 00:12:52,639
was to predict the risk of hospital

376
00:12:52,639 --> 00:12:54,079
transport

377
00:12:54,079 --> 00:12:56,399
of elderly patients in the next 30 days

378
00:12:56,399 --> 00:12:58,079
this was obtained through some

379
00:12:58,079 --> 00:13:00,399
personal emergency response system that

380
00:13:00,399 --> 00:13:02,639
the patients had

381
00:13:02,639 --> 00:13:04,880
and initially this this data was trained

382
00:13:04,880 --> 00:13:06,000
using extreme

383
00:13:06,000 --> 00:13:07,920
gradient boosts and trees so the

384
00:13:07,920 --> 00:13:09,120
advantage of decision trees because

385
00:13:09,120 --> 00:13:09,519
they're

386
00:13:09,519 --> 00:13:11,360
quite simple in nature they work well

387
00:13:11,360 --> 00:13:13,360
with these irrigation

388
00:13:13,360 --> 00:13:16,480
methods like gradient boosting

389
00:13:16,480 --> 00:13:18,959
and then the idea is to combine a lot of

390
00:13:18,959 --> 00:13:20,000
trees that are trained

391
00:13:20,000 --> 00:13:22,959
on subsets of the data to have uh one

392
00:13:22,959 --> 00:13:23,680
large

393
00:13:23,680 --> 00:13:26,480
uh model uh which performs sometimes

394
00:13:26,480 --> 00:13:27,120
similarly

395
00:13:27,120 --> 00:13:31,839
to neural network based approaches um

396
00:13:31,839 --> 00:13:35,279
so the here the study was done using um

397
00:13:35,279 --> 00:13:38,480
just in the clear data but we were told

398
00:13:38,480 --> 00:13:38,959
that

399
00:13:38,959 --> 00:13:43,040
this um there is there is a

400
00:13:43,040 --> 00:13:45,120
privacy aspect here especially when

401
00:13:45,120 --> 00:13:46,880
combining data from patients

402
00:13:46,880 --> 00:13:49,279
with multiple uh multiple operators that

403
00:13:49,279 --> 00:13:51,440
each have their their data on a set of

404
00:13:51,440 --> 00:13:52,639
patients we have again

405
00:13:52,639 --> 00:13:55,440
this this horizontally partitioned data

406
00:13:55,440 --> 00:13:56,240
so this was

407
00:13:56,240 --> 00:13:59,199
important for the the people that that

408
00:13:59,199 --> 00:14:00,959
did this study

409
00:14:00,959 --> 00:14:02,800
um so we validate our approach against

410
00:14:02,800 --> 00:14:04,880
this uh this data set we used uh

411
00:14:04,880 --> 00:14:06,639
um our decision trees protocol on a

412
00:14:06,639 --> 00:14:08,480
random force setting

413
00:14:08,480 --> 00:14:12,399
um because we we couldn't handle 290 000

414
00:14:12,399 --> 00:14:14,000
records so instead we did this

415
00:14:14,000 --> 00:14:18,240
2-13 like 8 000 about 8 000 records

416
00:14:18,240 --> 00:14:21,440
um we use 200 trees

417
00:14:21,440 --> 00:14:24,399
depth 4 so a fairly limited number of

418
00:14:24,399 --> 00:14:26,079
feet which is actually good if you use

419
00:14:26,079 --> 00:14:27,680
random force and you don't want to

420
00:14:27,680 --> 00:14:30,720
the depth to be too large and 11

421
00:14:30,720 --> 00:14:32,240
attributes

422
00:14:32,240 --> 00:14:35,040
um using this amulet zone cloud we

423
00:14:35,040 --> 00:14:36,720
determined that computing one tree takes

424
00:14:36,720 --> 00:14:37,600
about eight minutes

425
00:14:37,600 --> 00:14:39,920
using three parties which puts the total

426
00:14:39,920 --> 00:14:42,880
time at less than 20 to 28 hours which

427
00:14:42,880 --> 00:14:46,399
well we consider totally manageable um

428
00:14:46,399 --> 00:14:48,079
but now that it's parallelizable so if

429
00:14:48,079 --> 00:14:50,160
you want if you have several clusters of

430
00:14:50,160 --> 00:14:52,240
three machines then you can run it

431
00:14:52,240 --> 00:14:55,279
at the same time and especially in the

432
00:14:55,279 --> 00:14:56,959
amazon cloud this is of course very

433
00:14:56,959 --> 00:15:00,000
very easy you can have

434
00:15:00,480 --> 00:15:02,399
very easy parallelization so in the end

435
00:15:02,399 --> 00:15:04,160
the total would then just take

436
00:15:04,160 --> 00:15:07,279
eight minutes basically um so these are

437
00:15:07,279 --> 00:15:10,240
the accuracy numbers that we obtained

438
00:15:10,240 --> 00:15:12,240
because this data set was very skewed in

439
00:15:12,240 --> 00:15:13,920
output and outcomes there were a lot of

440
00:15:13,920 --> 00:15:14,480
no's

441
00:15:14,480 --> 00:15:18,000
and like two percent yeses um

442
00:15:18,000 --> 00:15:20,320
we looked at or the the initial study

443
00:15:20,320 --> 00:15:21,199
looked at

444
00:15:21,199 --> 00:15:24,160
the precision for several values of

445
00:15:24,160 --> 00:15:26,240
recall

446
00:15:26,240 --> 00:15:29,759
and they obtained these numbers and we

447
00:15:29,759 --> 00:15:31,440
obtained numbers that were only slightly

448
00:15:31,440 --> 00:15:32,240
worse

449
00:15:32,240 --> 00:15:34,399
so then we took this as validation to

450
00:15:34,399 --> 00:15:36,720
see that our methods actually work

451
00:15:36,720 --> 00:15:39,920
applied to large skill settings when we

452
00:15:39,920 --> 00:15:42,560
use this random forest approach

453
00:15:42,560 --> 00:15:45,839
so to sum up we developed this c 4.5

454
00:15:45,839 --> 00:15:47,279
like secure algorithm for training a

455
00:15:47,279 --> 00:15:48,480
decision tree based on continuous

456
00:15:48,480 --> 00:15:50,639
attributes

457
00:15:50,639 --> 00:15:52,800
we have this initial sorting phase and

458
00:15:52,800 --> 00:15:55,040
then we are limited to log and

459
00:15:55,040 --> 00:15:59,440
depth asymptotically per um per node

460
00:15:59,440 --> 00:16:01,040
we reduce complexity by converting a

461
00:16:01,040 --> 00:16:02,399
sorting network into a permutation

462
00:16:02,399 --> 00:16:03,839
network

463
00:16:03,839 --> 00:16:06,000
and using random force our protocol

464
00:16:06,000 --> 00:16:12,639
works on large data sets thank you

