1
00:00:00,880 --> 00:00:02,720
good everyone my name is simon chatel

2
00:00:02,720 --> 00:00:04,000
and today i'll be presenting our

3
00:00:04,000 --> 00:00:05,600
systematization of knowledge

4
00:00:05,600 --> 00:00:07,359
on privacy preserving collaborative

5
00:00:07,359 --> 00:00:08,960
three-based model learning

6
00:00:08,960 --> 00:00:11,360
this is joint work with apostolos pierce

7
00:00:11,360 --> 00:00:13,040
juan gramman focus of pasteurizer

8
00:00:13,040 --> 00:00:16,880
and gender reborn machine learning has

9
00:00:16,880 --> 00:00:18,400
become prevalent in the last couple of

10
00:00:18,400 --> 00:00:19,119
decades

11
00:00:19,119 --> 00:00:20,640
making data one of the most priced

12
00:00:20,640 --> 00:00:22,960
commodities among the plethora of

13
00:00:22,960 --> 00:00:24,400
machine learning algorithms

14
00:00:24,400 --> 00:00:26,160
tree-based models have received a lot of

15
00:00:26,160 --> 00:00:28,000
attention in research and industry

16
00:00:28,000 --> 00:00:30,080
for several reasons they are highly

17
00:00:30,080 --> 00:00:31,039
interpretable

18
00:00:31,039 --> 00:00:32,719
they require little effort in data

19
00:00:32,719 --> 00:00:34,640
preparation and can cope with nonlinear

20
00:00:34,640 --> 00:00:36,000
relationships

21
00:00:36,000 --> 00:00:38,079
finally they can also be used for fast

22
00:00:38,079 --> 00:00:39,920
prototyping and for instance

23
00:00:39,920 --> 00:00:41,920
they can be employed in medical research

24
00:00:41,920 --> 00:00:43,520
to do quick proof of concepts

25
00:00:43,520 --> 00:00:45,520
of an experiment before training more

26
00:00:45,520 --> 00:00:47,680
complex models

27
00:00:47,680 --> 00:00:49,760
recent years have seen the new trend of

28
00:00:49,760 --> 00:00:51,360
collaborative learning

29
00:00:51,360 --> 00:00:53,199
it refers to the setting where multiple

30
00:00:53,199 --> 00:00:55,199
entities wish to train on their joint

31
00:00:55,199 --> 00:00:57,760
data set to obtain a larger dataset

32
00:00:57,760 --> 00:01:00,079
with higher diversity which can in turn

33
00:01:00,079 --> 00:01:03,120
result in higher accuracy

34
00:01:03,120 --> 00:01:05,760
additionally as data is quite valuable

35
00:01:05,760 --> 00:01:07,200
there is a crucial need for data

36
00:01:07,200 --> 00:01:08,479
protection

37
00:01:08,479 --> 00:01:10,400
indeed the training process needs to

38
00:01:10,400 --> 00:01:12,240
respect the data privacy first

39
00:01:12,240 --> 00:01:14,000
to abide to new data protection

40
00:01:14,000 --> 00:01:16,479
regulations but also to incentivize the

41
00:01:16,479 --> 00:01:18,479
collaborative learning

42
00:01:18,479 --> 00:01:21,040
model privacy is also instrumental as

43
00:01:21,040 --> 00:01:23,759
first the model could be proprietary

44
00:01:23,759 --> 00:01:25,680
and second the resulting model could

45
00:01:25,680 --> 00:01:27,920
potentially be used to infer information

46
00:01:27,920 --> 00:01:30,560
about the training data

47
00:01:30,560 --> 00:01:32,799
as you can see on the right hand side

48
00:01:32,799 --> 00:01:35,119
the volume of publication on the topic

49
00:01:35,119 --> 00:01:36,479
of previously preserving

50
00:01:36,479 --> 00:01:38,320
collaborative three-based model learning

51
00:01:38,320 --> 00:01:39,920
has been booming in the last couple of

52
00:01:39,920 --> 00:01:41,200
years

53
00:01:41,200 --> 00:01:44,000
thus we propose a systematization of the

54
00:01:44,000 --> 00:01:44,880
literature

55
00:01:44,880 --> 00:01:46,640
on this topic and identify four

56
00:01:46,640 --> 00:01:49,600
dimension pivoting the constructions

57
00:01:49,600 --> 00:01:51,680
which are the learning algorithm the

58
00:01:51,680 --> 00:01:53,119
collaborative model

59
00:01:53,119 --> 00:01:55,360
the threat model and the protection

60
00:01:55,360 --> 00:01:57,040
mechanism

61
00:01:57,040 --> 00:01:59,119
we also identify that even with

62
00:01:59,119 --> 00:02:01,040
protection mechanism

63
00:02:01,040 --> 00:02:03,040
some information are still shared during

64
00:02:03,040 --> 00:02:04,159
the burning

65
00:02:04,159 --> 00:02:06,000
resulting in potential information

66
00:02:06,000 --> 00:02:07,360
leakage

67
00:02:07,360 --> 00:02:09,679
thus we also provide a leakage analysis

68
00:02:09,679 --> 00:02:10,399
framework

69
00:02:10,399 --> 00:02:12,720
to systematize literature with respect

70
00:02:12,720 --> 00:02:14,480
to this dimension

71
00:02:14,480 --> 00:02:16,959
finally we identify some open challenges

72
00:02:16,959 --> 00:02:19,040
and future directions

73
00:02:19,040 --> 00:02:20,840
so let us first look into the learning

74
00:02:20,840 --> 00:02:22,560
algorithms

75
00:02:22,560 --> 00:02:24,560
as a quick recap decision trees are

76
00:02:24,560 --> 00:02:26,160
supervised learning techniques

77
00:02:26,160 --> 00:02:27,920
that can be used for regressions or

78
00:02:27,920 --> 00:02:29,520
classifications

79
00:02:29,520 --> 00:02:32,080
while the former can be used to predict

80
00:02:32,080 --> 00:02:33,360
continuous values

81
00:02:33,360 --> 00:02:35,120
the latter can be used to predict or

82
00:02:35,120 --> 00:02:37,120
classify discrete values

83
00:02:37,120 --> 00:02:39,040
trees are often represented as direct

84
00:02:39,040 --> 00:02:40,640
security graph as you can see on the

85
00:02:40,640 --> 00:02:41,840
right hand side

86
00:02:41,840 --> 00:02:44,400
you basically have the data that has

87
00:02:44,400 --> 00:02:46,879
come from the path that is split

88
00:02:46,879 --> 00:02:48,800
with respect to a specific feature and a

89
00:02:48,800 --> 00:02:50,319
specific value

90
00:02:50,319 --> 00:02:52,000
the initial algorithms that were

91
00:02:52,000 --> 00:02:54,000
introduced were called greedy algorithms

92
00:02:54,000 --> 00:02:56,560
and for each node the splitting feature

93
00:02:56,560 --> 00:02:57,440
is selected

94
00:02:57,440 --> 00:03:00,000
by evaluating a cost function which is

95
00:03:00,000 --> 00:03:01,599
often non-linear

96
00:03:01,599 --> 00:03:04,400
evaluating an information gain or the

97
00:03:04,400 --> 00:03:06,560
genie index for instance

98
00:03:06,560 --> 00:03:08,480
for all the features and all the

99
00:03:08,480 --> 00:03:10,000
possible split points

100
00:03:10,000 --> 00:03:12,560
and eventually the best gain is selected

101
00:03:12,560 --> 00:03:13,440
as the feature

102
00:03:13,440 --> 00:03:16,000
and the split more recently random

103
00:03:16,000 --> 00:03:17,680
algorithms were introduced

104
00:03:17,680 --> 00:03:20,720
to alleviate some issues of overfitting

105
00:03:20,720 --> 00:03:22,720
with great algorithms

106
00:03:22,720 --> 00:03:25,200
the split is either selected at random

107
00:03:25,200 --> 00:03:25,920
or

108
00:03:25,920 --> 00:03:28,720
it's selected on a random subset of the

109
00:03:28,720 --> 00:03:30,720
features

110
00:03:30,720 --> 00:03:32,959
a simple algorithms basically evaluate

111
00:03:32,959 --> 00:03:35,040
multiple trays into a forest

112
00:03:35,040 --> 00:03:37,599
and average their outputs the most

113
00:03:37,599 --> 00:03:39,519
performing algorithms currently are

114
00:03:39,519 --> 00:03:40,560
based on

115
00:03:40,560 --> 00:03:43,360
boosting and include adaboost scratching

116
00:03:43,360 --> 00:03:44,959
boosting detergent trees

117
00:03:44,959 --> 00:03:47,360
or actually post in order to classify

118
00:03:47,360 --> 00:03:48,239
the literature

119
00:03:48,239 --> 00:03:50,080
it is important to identify what is the

120
00:03:50,080 --> 00:03:51,280
task at head

121
00:03:51,280 --> 00:03:53,599
that is what is the nature of the data

122
00:03:53,599 --> 00:03:55,280
which is the goal of the task

123
00:03:55,280 --> 00:03:56,959
if the final model should be

124
00:03:56,959 --> 00:03:58,560
deterministic or random

125
00:03:58,560 --> 00:04:00,879
and if the model should be a forest or

126
00:04:00,879 --> 00:04:01,920
tree

127
00:04:01,920 --> 00:04:03,920
on the right hand side you can see the

128
00:04:03,920 --> 00:04:05,519
most common algorithm found in the

129
00:04:05,519 --> 00:04:07,040
literature

130
00:04:07,040 --> 00:04:08,400
we know that the choice of the learning

131
00:04:08,400 --> 00:04:10,480
algorithm conditions drastically what

132
00:04:10,480 --> 00:04:11,519
needs to be computed

133
00:04:11,519 --> 00:04:14,000
and exchanged during the process for

134
00:04:14,000 --> 00:04:15,920
instance in ig3 which is a great

135
00:04:15,920 --> 00:04:18,000
algorithm

136
00:04:18,000 --> 00:04:20,798
the parties need to compute the

137
00:04:20,798 --> 00:04:21,918
information gain

138
00:04:21,918 --> 00:04:23,680
that requires the computation of the

139
00:04:23,680 --> 00:04:25,520
entropy function

140
00:04:25,520 --> 00:04:28,240
well in the cart algorithm the gene

141
00:04:28,240 --> 00:04:28,800
index

142
00:04:28,800 --> 00:04:30,720
needs to be evaluated which is

143
00:04:30,720 --> 00:04:34,400
represented by a quadratic function

144
00:04:34,400 --> 00:04:36,240
now let us look into the collaborative

145
00:04:36,240 --> 00:04:37,759
models

146
00:04:37,759 --> 00:04:39,360
the first thing is to understand how the

147
00:04:39,360 --> 00:04:41,360
data is partitioned among the different

148
00:04:41,360 --> 00:04:42,000
sites or

149
00:04:42,000 --> 00:04:44,479
parties in the collaboration in the

150
00:04:44,479 --> 00:04:45,280
literature we

151
00:04:45,280 --> 00:04:47,919
have identified two main partitioning

152
00:04:47,919 --> 00:04:50,080
the first one is called horizontal

153
00:04:50,080 --> 00:04:52,240
because the feature space remains the

154
00:04:52,240 --> 00:04:55,040
same across all the different sites

155
00:04:55,040 --> 00:04:58,560
only the data points differ the second

156
00:04:58,560 --> 00:04:59,360
partitioning

157
00:04:59,360 --> 00:05:01,600
is the vertical one where all the data

158
00:05:01,600 --> 00:05:02,800
points remain the same

159
00:05:02,800 --> 00:05:05,520
only the features differ as previously

160
00:05:05,520 --> 00:05:06,639
mentioned

161
00:05:06,639 --> 00:05:09,120
depending on the learning algorithm the

162
00:05:09,120 --> 00:05:10,320
partitioning can

163
00:05:10,320 --> 00:05:12,960
induce different exchange of data for

164
00:05:12,960 --> 00:05:14,880
instance in id3

165
00:05:14,880 --> 00:05:17,039
we need to compute the count of data

166
00:05:17,039 --> 00:05:18,560
point for every feature

167
00:05:18,560 --> 00:05:21,600
for every value of such feature in the

168
00:05:21,600 --> 00:05:23,360
vertical partitioning this is not an

169
00:05:23,360 --> 00:05:24,320
issue because

170
00:05:24,320 --> 00:05:26,479
every site has access to all the data

171
00:05:26,479 --> 00:05:28,560
points for a specific feature

172
00:05:28,560 --> 00:05:30,639
however in the horizontal partitioning

173
00:05:30,639 --> 00:05:32,320
collaboration is required

174
00:05:32,320 --> 00:05:35,520
to evaluate the global count for all the

175
00:05:35,520 --> 00:05:37,440
data points in the data set

176
00:05:37,440 --> 00:05:38,880
the second part of the collaborative

177
00:05:38,880 --> 00:05:40,639
model is to identify which

178
00:05:40,639 --> 00:05:43,520
entity is present in the collaboration

179
00:05:43,520 --> 00:05:45,840
we do not buy party the entity that

180
00:05:45,840 --> 00:05:48,240
owns some local data and wishes to take

181
00:05:48,240 --> 00:05:50,400
part in the collaborative learning

182
00:05:50,400 --> 00:05:53,280
the miner is the entity responsible to

183
00:05:53,280 --> 00:05:54,000
actually

184
00:05:54,000 --> 00:05:56,880
compute the learning process whilst the

185
00:05:56,880 --> 00:05:58,880
aggregator is here to

186
00:05:58,880 --> 00:06:02,000
aggregate intermediate values or

187
00:06:02,000 --> 00:06:05,280
raw data from multiple local

188
00:06:05,280 --> 00:06:09,440
parties we denote just briefly the query

189
00:06:09,440 --> 00:06:11,759
which is an entity that queries for the

190
00:06:11,759 --> 00:06:12,880
final model

191
00:06:12,880 --> 00:06:15,360
for prediction in the non-collaborative

192
00:06:15,360 --> 00:06:16,000
model

193
00:06:16,000 --> 00:06:18,160
the local party can communicate with a

194
00:06:18,160 --> 00:06:19,600
minor to establish

195
00:06:19,600 --> 00:06:21,280
a model that will be queried by the

196
00:06:21,280 --> 00:06:22,800
querier

197
00:06:22,800 --> 00:06:24,479
the first collaborative model we

198
00:06:24,479 --> 00:06:26,560
identify is the centralized one

199
00:06:26,560 --> 00:06:29,039
where all the parties send their raw

200
00:06:29,039 --> 00:06:29,600
data

201
00:06:29,600 --> 00:06:33,280
to a miner who would then compute the

202
00:06:33,280 --> 00:06:35,120
learning process as if the data was

203
00:06:35,120 --> 00:06:37,680
never partitioned

204
00:06:37,680 --> 00:06:40,240
a similar though different approach is

205
00:06:40,240 --> 00:06:41,680
the offloading one

206
00:06:41,680 --> 00:06:44,720
where the the local data is sent to

207
00:06:44,720 --> 00:06:47,759
an aggregator that creates a centralized

208
00:06:47,759 --> 00:06:48,560
data set

209
00:06:48,560 --> 00:06:51,919
and a different entity mines on the

210
00:06:51,919 --> 00:06:54,800
on the dataset that has been offloaded

211
00:06:54,800 --> 00:06:56,240
in the aggregator-based

212
00:06:56,240 --> 00:06:59,599
scenario each party keeps its local data

213
00:06:59,599 --> 00:07:02,160
local and only intermediate values are

214
00:07:02,160 --> 00:07:05,280
exchanged with an aggregator

215
00:07:05,280 --> 00:07:07,919
as previously mentioned this can be the

216
00:07:07,919 --> 00:07:08,880
feature counts

217
00:07:08,880 --> 00:07:12,479
in the id3 algorithm and the aggregator

218
00:07:12,479 --> 00:07:14,720
is here to compute the overall

219
00:07:14,720 --> 00:07:18,720
account for a specific feature value

220
00:07:18,720 --> 00:07:21,199
the final collaborative model is the

221
00:07:21,199 --> 00:07:22,800
fully distributed scenario

222
00:07:22,800 --> 00:07:25,440
in the two-party or multi-party setting

223
00:07:25,440 --> 00:07:26,240
where

224
00:07:26,240 --> 00:07:29,680
only the entities owning local data

225
00:07:29,680 --> 00:07:33,520
can be involved in the in the process

226
00:07:33,520 --> 00:07:35,199
as mentioned the collaborative model has

227
00:07:35,199 --> 00:07:37,440
a major impact on the learning algorithm

228
00:07:37,440 --> 00:07:40,800
and what information needs protection

229
00:07:40,800 --> 00:07:42,240
before looking into the protection

230
00:07:42,240 --> 00:07:44,080
mechanisms let's look into the threat

231
00:07:44,080 --> 00:07:45,440
models

232
00:07:45,440 --> 00:07:47,199
in order to protect the privacy of the

233
00:07:47,199 --> 00:07:48,879
data the intermediate values

234
00:07:48,879 --> 00:07:51,680
and the resulting models we need to

235
00:07:51,680 --> 00:07:53,599
establish what are the capabilities

236
00:07:53,599 --> 00:07:55,599
of the adversary or adversaries in the

237
00:07:55,599 --> 00:07:57,199
collaborative model

238
00:07:57,199 --> 00:07:59,680
indeed each entity in the collaboration

239
00:07:59,680 --> 00:08:01,840
can have an alternate motive to cheat or

240
00:08:01,840 --> 00:08:04,560
disrupt the learning process

241
00:08:04,560 --> 00:08:06,960
in the literature we distinguish three

242
00:08:06,960 --> 00:08:08,319
major behaviors

243
00:08:08,319 --> 00:08:10,879
the honest one the semi honest one and

244
00:08:10,879 --> 00:08:12,800
the malicious one

245
00:08:12,800 --> 00:08:15,039
most of the surveyed works consider the

246
00:08:15,039 --> 00:08:16,400
semi-honest scenario

247
00:08:16,400 --> 00:08:18,319
as they report the malicious setting

248
00:08:18,319 --> 00:08:20,800
introduces massive constraints

249
00:08:20,800 --> 00:08:22,960
that can lead to communication and

250
00:08:22,960 --> 00:08:24,879
computation overhead

251
00:08:24,879 --> 00:08:27,520
as an example the centralized

252
00:08:27,520 --> 00:08:28,639
collaborative model

253
00:08:28,639 --> 00:08:31,280
can either be in the honest scenario

254
00:08:31,280 --> 00:08:33,200
where the data is

255
00:08:33,200 --> 00:08:36,240
sent in the clear to the to the miner

256
00:08:36,240 --> 00:08:38,559
or can be considered in the seminars

257
00:08:38,559 --> 00:08:39,440
scenario

258
00:08:39,440 --> 00:08:41,919
where the data is protected prior to be

259
00:08:41,919 --> 00:08:43,519
offloaded

260
00:08:43,519 --> 00:08:44,959
now let's look into the protection

261
00:08:44,959 --> 00:08:47,120
mechanism from the literature we

262
00:08:47,120 --> 00:08:49,680
identify four main different approaches

263
00:08:49,680 --> 00:08:52,399
to previous enhancing technologies of

264
00:08:52,399 --> 00:08:54,000
course hybrid solutions can

265
00:08:54,000 --> 00:08:56,880
also combine multiple of them the first

266
00:08:56,880 --> 00:08:57,279
one

267
00:08:57,279 --> 00:08:58,959
is based on perturbation and

268
00:08:58,959 --> 00:09:00,560
randomization

269
00:09:00,560 --> 00:09:02,480
it basically creates a surrogate data

270
00:09:02,480 --> 00:09:05,200
set by perturbing the original one

271
00:09:05,200 --> 00:09:08,240
consider in the center an offloading

272
00:09:08,240 --> 00:09:11,600
models the parties would

273
00:09:11,600 --> 00:09:14,480
perturb the data set by either adding

274
00:09:14,480 --> 00:09:16,560
noise or doing discretization

275
00:09:16,560 --> 00:09:19,200
or swiping the data set with a new one

276
00:09:19,200 --> 00:09:20,839
following the same statistical

277
00:09:20,839 --> 00:09:22,000
distribution

278
00:09:22,000 --> 00:09:25,040
however this can come at some accuracy

279
00:09:25,040 --> 00:09:25,839
losses

280
00:09:25,839 --> 00:09:28,399
as the learning is eventually done on an

281
00:09:28,399 --> 00:09:30,000
approximate data set

282
00:09:30,000 --> 00:09:32,080
which might not be acceptable in some

283
00:09:32,080 --> 00:09:33,120
cases

284
00:09:33,120 --> 00:09:34,640
additionally some attacks have been

285
00:09:34,640 --> 00:09:36,160
shown to

286
00:09:36,160 --> 00:09:38,320
show vulnerability of such approaches

287
00:09:38,320 --> 00:09:40,160
specifically in the case of noise

288
00:09:40,160 --> 00:09:41,600
addition

289
00:09:41,600 --> 00:09:43,680
but these approaches lack formal privacy

290
00:09:43,680 --> 00:09:45,440
definitions and were introduced before

291
00:09:45,440 --> 00:09:48,160
the notion of differential privacy

292
00:09:48,160 --> 00:09:49,920
differentially private mechanisms can

293
00:09:49,920 --> 00:09:52,000
indeed be employed to protect

294
00:09:52,000 --> 00:09:54,399
different information it can be

295
00:09:54,399 --> 00:09:55,680
considered in numerous

296
00:09:55,680 --> 00:09:57,120
collaborative models such as the

297
00:09:57,120 --> 00:09:58,959
centralized the aggregate based or the

298
00:09:58,959 --> 00:10:00,959
even the fully distributed one

299
00:10:00,959 --> 00:10:02,480
four aspects need to be considered

300
00:10:02,480 --> 00:10:04,800
though that is who adds the noise and at

301
00:10:04,800 --> 00:10:06,000
which training stage

302
00:10:06,000 --> 00:10:08,240
and what is the privacy budget and the

303
00:10:08,240 --> 00:10:09,839
overall noise magnitude

304
00:10:09,839 --> 00:10:12,320
that is added to the information in the

305
00:10:12,320 --> 00:10:13,279
literature where

306
00:10:13,279 --> 00:10:15,279
the privacy budget can deplete quite

307
00:10:15,279 --> 00:10:16,320
rapidly

308
00:10:16,320 --> 00:10:18,240
thus several tricks have been employed

309
00:10:18,240 --> 00:10:20,399
to reduce this by either

310
00:10:20,399 --> 00:10:22,720
relaxing the learning algorithm by for

311
00:10:22,720 --> 00:10:24,720
instance obtaining for the gene index

312
00:10:24,720 --> 00:10:26,800
instead of the information gain as we

313
00:10:26,800 --> 00:10:28,880
have seen resulting in the use of a

314
00:10:28,880 --> 00:10:30,160
quadratic function

315
00:10:30,160 --> 00:10:33,120
instead of a logarithm or even

316
00:10:33,120 --> 00:10:35,920
considering a completely random split

317
00:10:35,920 --> 00:10:38,399
other tricks involve for instance

318
00:10:38,399 --> 00:10:39,600
learning on

319
00:10:39,600 --> 00:10:42,320
independent subset of the data but one

320
00:10:42,320 --> 00:10:44,480
might lose the benefit of collaborative

321
00:10:44,480 --> 00:10:45,360
learning

322
00:10:45,360 --> 00:10:48,320
or use differentially private generative

323
00:10:48,320 --> 00:10:49,680
adversarial network

324
00:10:49,680 --> 00:10:52,720
to create surrogate data sets

325
00:10:52,720 --> 00:10:55,040
another approach is to locally train a

326
00:10:55,040 --> 00:10:55,839
tree

327
00:10:55,839 --> 00:10:58,160
under differentially private guarantees

328
00:10:58,160 --> 00:10:59,760
and then share this tree to the next

329
00:10:59,760 --> 00:11:00,959
party who can

330
00:11:00,959 --> 00:11:03,760
use it to boost track its local learning

331
00:11:03,760 --> 00:11:05,040
however the issue

332
00:11:05,040 --> 00:11:07,519
is that there is a huge trade-off

333
00:11:07,519 --> 00:11:08,560
between privacy

334
00:11:08,560 --> 00:11:11,040
and utility as the more noise is added

335
00:11:11,040 --> 00:11:12,399
the more previous is obtained

336
00:11:12,399 --> 00:11:14,720
but the less accurate the resulting

337
00:11:14,720 --> 00:11:16,240
model can be

338
00:11:16,240 --> 00:11:18,399
cryptographic approaches are mainly

339
00:11:18,399 --> 00:11:20,000
based on section multiplied computation

340
00:11:20,000 --> 00:11:22,079
and homomorphic encryption

341
00:11:22,079 --> 00:11:24,480
in smsc the best performing solutions

342
00:11:24,480 --> 00:11:25,440
are using the

343
00:11:25,440 --> 00:11:28,480
famous speed framework to enable sharing

344
00:11:28,480 --> 00:11:29,279
of the secret

345
00:11:29,279 --> 00:11:31,920
and then the computation in an npc

346
00:11:31,920 --> 00:11:32,800
manner

347
00:11:32,800 --> 00:11:34,880
it can enable almost exact learning to

348
00:11:34,880 --> 00:11:36,560
the approximation of the nonlinear

349
00:11:36,560 --> 00:11:37,440
functions

350
00:11:37,440 --> 00:11:39,360
and can cope with numerous collaborative

351
00:11:39,360 --> 00:11:41,360
models such as the centralized one the

352
00:11:41,360 --> 00:11:43,120
aggregator based

353
00:11:43,120 --> 00:11:46,160
model and the fully distributed one

354
00:11:46,160 --> 00:11:48,720
however to keep efficiency several

355
00:11:48,720 --> 00:11:49,600
solutions

356
00:11:49,600 --> 00:11:51,920
also replace the gain function by a more

357
00:11:51,920 --> 00:11:52,959
amenable one

358
00:11:52,959 --> 00:11:55,360
switching the logarithmic evaluation and

359
00:11:55,360 --> 00:11:56,320
information gain

360
00:11:56,320 --> 00:11:58,240
by the genie index and the quadratic

361
00:11:58,240 --> 00:11:59,519
function

362
00:11:59,519 --> 00:12:01,279
however it's harder to tolerate an

363
00:12:01,279 --> 00:12:03,120
availability that is when one of the

364
00:12:03,120 --> 00:12:04,800
party might be offline during the

365
00:12:04,800 --> 00:12:06,399
training

366
00:12:06,399 --> 00:12:08,000
additionally communication overhead is

367
00:12:08,000 --> 00:12:09,440
also non-negligible

368
00:12:09,440 --> 00:12:11,600
and trust assumptions can be restrictive

369
00:12:11,600 --> 00:12:13,519
for instance in a trusted preprocessing

370
00:12:13,519 --> 00:12:15,120
scenario

371
00:12:15,120 --> 00:12:18,639
homomorphic encryption can enable

372
00:12:18,639 --> 00:12:20,320
computation to be executed directly in

373
00:12:20,320 --> 00:12:22,000
the ciphertext and can

374
00:12:22,000 --> 00:12:24,079
lower the communication overhead induced

375
00:12:24,079 --> 00:12:26,240
by smc solutions

376
00:12:26,240 --> 00:12:27,920
similarly they can adapt the learning

377
00:12:27,920 --> 00:12:29,920
algorithm to a more

378
00:12:29,920 --> 00:12:32,639
amenable one with the genie index or

379
00:12:32,639 --> 00:12:34,320
even with completely random

380
00:12:34,320 --> 00:12:38,399
splits however the drawback of

381
00:12:38,399 --> 00:12:40,880
encryption is that it's hard to compute

382
00:12:40,880 --> 00:12:42,000
comparison

383
00:12:42,000 --> 00:12:44,959
under encryption thus in numerous works

384
00:12:44,959 --> 00:12:46,639
the intermediate values are

385
00:12:46,639 --> 00:12:49,600
decrypted to identify the base game

386
00:12:49,600 --> 00:12:51,360
other solutions can be leader-based

387
00:12:51,360 --> 00:12:53,120
that is with the different entities

388
00:12:53,120 --> 00:12:55,200
having different role in the learning

389
00:12:55,200 --> 00:12:57,600
and specifically one for encryption one

390
00:12:57,600 --> 00:12:58,480
for decryption

391
00:12:58,480 --> 00:13:00,560
and one for blind tanks most of the

392
00:13:00,560 --> 00:13:02,000
solutions are relying on the

393
00:13:02,000 --> 00:13:04,079
bi-ecliptosystem or its threshold

394
00:13:04,079 --> 00:13:05,200
variant

395
00:13:05,200 --> 00:13:06,880
the main drawback of home of encryption

396
00:13:06,880 --> 00:13:08,800
however is that it's quite difficult to

397
00:13:08,800 --> 00:13:10,399
evaluate nonlinear functions

398
00:13:10,399 --> 00:13:13,519
and to evaluate the comparisons another

399
00:13:13,519 --> 00:13:16,079
approach is based on hollow solutions

400
00:13:16,079 --> 00:13:18,959
in this scenario each party has an

401
00:13:18,959 --> 00:13:20,079
enclave on which

402
00:13:20,079 --> 00:13:23,120
the data can be offloaded and the

403
00:13:23,120 --> 00:13:24,720
secure enclave is responsible for

404
00:13:24,720 --> 00:13:26,720
storage and computation of the sensitive

405
00:13:26,720 --> 00:13:27,680
data

406
00:13:27,680 --> 00:13:29,200
remote attestation protocols can

407
00:13:29,200 --> 00:13:31,040
cryptographically verify that the

408
00:13:31,040 --> 00:13:32,000
software has been

409
00:13:32,000 --> 00:13:34,320
loaded correctly however this relies on

410
00:13:34,320 --> 00:13:35,519
different trust assumptions

411
00:13:35,519 --> 00:13:38,160
as one needs to trust the manufacturer

412
00:13:38,160 --> 00:13:38,560
of the

413
00:13:38,560 --> 00:13:40,560
secure enclave additionally it might be

414
00:13:40,560 --> 00:13:42,160
susceptible to sidechain attacks

415
00:13:42,160 --> 00:13:43,680
and needs to be coupled with additional

416
00:13:43,680 --> 00:13:45,680
countermeasures the most promising

417
00:13:45,680 --> 00:13:46,720
approaches however

418
00:13:46,720 --> 00:13:49,519
are based on hybrid solutions for

419
00:13:49,519 --> 00:13:52,399
instance by combining smc randomization

420
00:13:52,399 --> 00:13:54,959
the amount of communication can be

421
00:13:54,959 --> 00:13:55,760
reduced

422
00:13:55,760 --> 00:13:57,839
and obtaining better accuracy than

423
00:13:57,839 --> 00:13:59,279
simple randomization

424
00:13:59,279 --> 00:14:00,959
numerous cryptographic approach can also

425
00:14:00,959 --> 00:14:03,120
be combined to obtain communication

426
00:14:03,120 --> 00:14:05,279
efficiency from homework encryption

427
00:14:05,279 --> 00:14:07,680
and using the smc for nonlinear

428
00:14:07,680 --> 00:14:09,920
functions such as comparisons

429
00:14:09,920 --> 00:14:12,000
db can also be combined with

430
00:14:12,000 --> 00:14:13,120
cryptography

431
00:14:13,120 --> 00:14:15,760
to reduce the amount of privacy budget

432
00:14:15,760 --> 00:14:16,399
required

433
00:14:16,399 --> 00:14:19,680
at each step and finally as mentioned

434
00:14:19,680 --> 00:14:21,519
section hardware and cryptography can be

435
00:14:21,519 --> 00:14:23,440
combined to prevent

436
00:14:23,440 --> 00:14:26,639
side channel attacks now let us briefly

437
00:14:26,639 --> 00:14:29,760
introduce our leakage analysis framework

438
00:14:29,760 --> 00:14:31,680
as previously mentioned in the

439
00:14:31,680 --> 00:14:33,120
collaborative learning

440
00:14:33,120 --> 00:14:35,040
several information can be shared during

441
00:14:35,040 --> 00:14:36,399
the process that

442
00:14:36,399 --> 00:14:39,279
are not always protected thus we design

443
00:14:39,279 --> 00:14:40,079
a framework

444
00:14:40,079 --> 00:14:42,240
to analyze which kind of information is

445
00:14:42,240 --> 00:14:44,240
leaked and at which step of the learning

446
00:14:44,240 --> 00:14:45,199
process

447
00:14:45,199 --> 00:14:47,279
can be for instance raw data

448
00:14:47,279 --> 00:14:48,639
intermediate values

449
00:14:48,639 --> 00:14:51,680
or model parameters a framework

450
00:14:51,680 --> 00:14:52,560
identifies

451
00:14:52,560 --> 00:14:54,720
what kind of information is leaked and

452
00:14:54,720 --> 00:14:55,920
who can potentially take

453
00:14:55,920 --> 00:14:59,360
advantage of such leakage for instance

454
00:14:59,360 --> 00:15:01,519
in the centralized model with dp

455
00:15:01,519 --> 00:15:03,360
mechanism as protection

456
00:15:03,360 --> 00:15:05,920
raw data is inherently linked to the

457
00:15:05,920 --> 00:15:06,720
miner

458
00:15:06,720 --> 00:15:09,279
as it is the one responsible to add the

459
00:15:09,279 --> 00:15:11,120
tp protection

460
00:15:11,120 --> 00:15:13,279
but we note that also improper

461
00:15:13,279 --> 00:15:15,040
previously enhancing technologies

462
00:15:15,040 --> 00:15:17,120
such as perturbation in the centralized

463
00:15:17,120 --> 00:15:18,399
collaborative model

464
00:15:18,399 --> 00:15:21,040
could lead to a raw data leakage as we

465
00:15:21,040 --> 00:15:23,440
have seen

466
00:15:23,440 --> 00:15:25,920
intermediate values can be used to

467
00:15:25,920 --> 00:15:27,760
obtain global information about the

468
00:15:27,760 --> 00:15:29,040
local dataset

469
00:15:29,040 --> 00:15:31,680
such as class counts for instance or

470
00:15:31,680 --> 00:15:32,399
they could

471
00:15:32,399 --> 00:15:36,000
potentially be used to in combination

472
00:15:36,000 --> 00:15:36,800
with the

473
00:15:36,800 --> 00:15:39,279
final model to do membership or

474
00:15:39,279 --> 00:15:40,800
inference attacks

475
00:15:40,800 --> 00:15:43,680
we note that intermediate values can be

476
00:15:43,680 --> 00:15:44,959
local

477
00:15:44,959 --> 00:15:48,000
pertaining to only one specific entity

478
00:15:48,000 --> 00:15:52,560
or global overall we identified that

479
00:15:52,560 --> 00:15:55,360
very few works keep the final model

480
00:15:55,360 --> 00:15:56,079
private

481
00:15:56,079 --> 00:15:58,560
and even fewer works prevent the leakage

482
00:15:58,560 --> 00:15:59,839
of intermediate values

483
00:15:59,839 --> 00:16:01,680
with appropriate previously enhancing

484
00:16:01,680 --> 00:16:04,320
technologies

485
00:16:04,320 --> 00:16:07,199
overall we identified four main open

486
00:16:07,199 --> 00:16:08,639
challenges

487
00:16:08,639 --> 00:16:11,360
the first one is to study the leakage in

488
00:16:11,360 --> 00:16:12,480
three base models

489
00:16:12,480 --> 00:16:14,720
by investigating privacy attacks from

490
00:16:14,720 --> 00:16:16,160
intermediate values

491
00:16:16,160 --> 00:16:19,040
and quantify the risk of third leakage

492
00:16:19,040 --> 00:16:20,399
the second one

493
00:16:20,399 --> 00:16:22,480
is how to cope with malicious setting

494
00:16:22,480 --> 00:16:24,639
porting the field to the any trust or

495
00:16:24,639 --> 00:16:26,560
even fully malicious setting

496
00:16:26,560 --> 00:16:29,120
remains a challenge but recent advances

497
00:16:29,120 --> 00:16:30,800
in verifiable computation

498
00:16:30,800 --> 00:16:33,920
and cryptography are promising byzantine

499
00:16:33,920 --> 00:16:35,839
tolerance is also an interesting avenue

500
00:16:35,839 --> 00:16:37,759
for future works

501
00:16:37,759 --> 00:16:40,560
the third challenge is how to provide

502
00:16:40,560 --> 00:16:42,240
end-to-end protection of the learning

503
00:16:42,240 --> 00:16:42,959
process

504
00:16:42,959 --> 00:16:45,360
protecting the input the intermediate

505
00:16:45,360 --> 00:16:46,240
values

506
00:16:46,240 --> 00:16:49,120
and the final model at the same time the

507
00:16:49,120 --> 00:16:50,000
last

508
00:16:50,000 --> 00:16:52,160
challenge is how to cope with resilience

509
00:16:52,160 --> 00:16:53,680
and fault tolerance

510
00:16:53,680 --> 00:16:56,480
that is what is happening when a party

511
00:16:56,480 --> 00:16:58,320
wishes to leave the federation

512
00:16:58,320 --> 00:17:00,720
or if a party is unavailable during some

513
00:17:00,720 --> 00:17:01,759
time in the

514
00:17:01,759 --> 00:17:04,160
in the learning process these points are

515
00:17:04,160 --> 00:17:04,880
inherent

516
00:17:04,880 --> 00:17:08,079
to the learning process collaboratively

517
00:17:08,079 --> 00:17:09,919
and very few works have considered it

518
00:17:09,919 --> 00:17:12,880
for the pre-base models

519
00:17:12,880 --> 00:17:14,880
in this work we have systematized the

520
00:17:14,880 --> 00:17:16,799
literature on previously preserving

521
00:17:16,799 --> 00:17:18,240
collaborative three-based modern

522
00:17:18,240 --> 00:17:20,079
learning we have identified that the

523
00:17:20,079 --> 00:17:21,119
collaboration

524
00:17:21,119 --> 00:17:24,000
introduces several challenges that can

525
00:17:24,000 --> 00:17:25,119
be cooked by

526
00:17:25,119 --> 00:17:27,199
modifying the way the learning is

527
00:17:27,199 --> 00:17:28,160
executed

528
00:17:28,160 --> 00:17:31,520
or adding new entities additionally

529
00:17:31,520 --> 00:17:32,960
while numerous previous intensive

530
00:17:32,960 --> 00:17:34,640
technologies have been employed and

531
00:17:34,640 --> 00:17:35,360
deployed

532
00:17:35,360 --> 00:17:38,480
hybrid solutions are the most promising

533
00:17:38,480 --> 00:17:40,480
we have also provided a leakage

534
00:17:40,480 --> 00:17:42,080
identification framework

535
00:17:42,080 --> 00:17:44,000
to help designers and researchers

536
00:17:44,000 --> 00:17:45,600
categorize the literature

537
00:17:45,600 --> 00:17:47,600
and reason about what information is

538
00:17:47,600 --> 00:17:48,799
exchanged to whom

539
00:17:48,799 --> 00:17:51,360
during the learning thank you for your

540
00:17:51,360 --> 00:17:51,919
attention

541
00:17:51,919 --> 00:17:59,679
and i'll be happy to take any questions

