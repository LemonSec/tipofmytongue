1
00:00:01,760 --> 00:00:03,600
hello everybody thank you very much for

2
00:00:03,600 --> 00:00:05,359
coming to my presentation

3
00:00:05,359 --> 00:00:08,240
i'm rafa alves i'm a phd student in

4
00:00:08,240 --> 00:00:08,800
kossik

5
00:00:08,800 --> 00:00:11,519
carlos and i will be presenting joint

6
00:00:11,519 --> 00:00:13,840
work with vidasha munsami

7
00:00:13,840 --> 00:00:16,720
from rural university of and claudia vf

8
00:00:16,720 --> 00:00:19,199
my advisor here in global

9
00:00:19,199 --> 00:00:22,000
the paper is called less is more a

10
00:00:22,000 --> 00:00:23,119
progressive respecting

11
00:00:23,119 --> 00:00:25,039
android mobile classifier using

12
00:00:25,039 --> 00:00:27,760
federated learning

13
00:00:27,760 --> 00:00:29,279
let me introduce you briefly to the

14
00:00:29,279 --> 00:00:31,840
problem in traditional federated

15
00:00:31,840 --> 00:00:33,040
learning

16
00:00:33,040 --> 00:00:37,120
the goal is to train a shared model

17
00:00:37,120 --> 00:00:39,040
that gives the data local to the users

18
00:00:39,040 --> 00:00:40,160
but still learns

19
00:00:40,160 --> 00:00:43,200
from the federation of all of them

20
00:00:43,200 --> 00:00:46,559
how does it do that well first

21
00:00:46,559 --> 00:00:49,600
users train a local model in their own

22
00:00:49,600 --> 00:00:50,879
devices using their local

23
00:00:50,879 --> 00:00:54,640
data and instead of sharing the data

24
00:00:54,640 --> 00:00:57,039
they shared the parameters of the model

25
00:00:57,039 --> 00:00:58,079
with a cloud

26
00:00:58,079 --> 00:01:00,320
or service provider that will aggregate

27
00:01:00,320 --> 00:01:01,359
all parameters

28
00:01:01,359 --> 00:01:04,559
across users and produce what is called

29
00:01:04,559 --> 00:01:06,560
the federated model

30
00:01:06,560 --> 00:01:08,880
this federated model and its parameters

31
00:01:08,880 --> 00:01:09,920
will be shared back

32
00:01:09,920 --> 00:01:12,880
to clients so that they can benefit from

33
00:01:12,880 --> 00:01:14,400
the learning process of all the other

34
00:01:14,400 --> 00:01:16,159
clients

35
00:01:16,159 --> 00:01:19,759
in traditional federated learning

36
00:01:19,759 --> 00:01:22,159
we assume that users can provide with

37
00:01:22,159 --> 00:01:23,280
global data

38
00:01:23,280 --> 00:01:25,439
ground truth for the local process

39
00:01:25,439 --> 00:01:28,320
training process

40
00:01:28,479 --> 00:01:32,240
on the other hand it has been shown that

41
00:01:32,240 --> 00:01:35,119
federated learning as depicted in this

42
00:01:35,119 --> 00:01:36,400
picture

43
00:01:36,400 --> 00:01:39,520
is vulnerable to privacy and integrity

44
00:01:39,520 --> 00:01:41,839
attacks

45
00:01:42,240 --> 00:01:45,840
our problem this we try to toggle

46
00:01:45,840 --> 00:01:49,040
is twofold first we get rid of the

47
00:01:49,040 --> 00:01:51,119
assumption where users need to provide

48
00:01:51,119 --> 00:01:52,479
ground truth

49
00:01:52,479 --> 00:01:54,479
this is important for applications such

50
00:01:54,479 --> 00:01:55,920
as malware detection

51
00:01:55,920 --> 00:01:57,680
where users just cannot provide with

52
00:01:57,680 --> 00:01:59,600
ground truth because the point of the

53
00:01:59,600 --> 00:02:00,479
classifier

54
00:02:00,479 --> 00:02:04,079
is to actually take tell the user if the

55
00:02:04,079 --> 00:02:06,880
application is managed or not

56
00:02:06,880 --> 00:02:09,199
on the other hand we also try to address

57
00:02:09,199 --> 00:02:11,360
the security and privacy problems

58
00:02:11,360 --> 00:02:15,680
by designing a system that is resistant

59
00:02:15,680 --> 00:02:18,560
against poisoning attacks and against

60
00:02:18,560 --> 00:02:22,400
membership inference attacks

61
00:02:22,400 --> 00:02:24,879
how do we do that well for the first

62
00:02:24,879 --> 00:02:25,760
part

63
00:02:25,760 --> 00:02:27,920
where the users can provide labels we

64
00:02:27,920 --> 00:02:29,440
make use of semi-supervised machine

65
00:02:29,440 --> 00:02:31,040
learning

66
00:02:31,040 --> 00:02:32,879
for the second part of security and

67
00:02:32,879 --> 00:02:35,519
privacy we address what we believe is at

68
00:02:35,519 --> 00:02:36,400
least one

69
00:02:36,400 --> 00:02:39,599
of the root causes of the problems

70
00:02:39,599 --> 00:02:41,120
in federal debt learning which is the

71
00:02:41,120 --> 00:02:43,280
number of parameters that is typically

72
00:02:43,280 --> 00:02:44,000
shared

73
00:02:44,000 --> 00:02:46,640
by clients so we reduce the number of

74
00:02:46,640 --> 00:02:47,840
parameters that are

75
00:02:47,840 --> 00:02:51,680
sent in each of the model updates

76
00:02:51,680 --> 00:02:53,680
let me explain briefly what semi

77
00:02:53,680 --> 00:02:55,360
supervised machine learning

78
00:02:55,360 --> 00:02:57,200
in supervised machine learning without

79
00:02:57,200 --> 00:02:58,480
this semi

80
00:02:58,480 --> 00:03:01,599
you have data points that are

81
00:03:01,599 --> 00:03:04,319
labeled for each of these data points we

82
00:03:04,319 --> 00:03:04,959
know if

83
00:03:04,959 --> 00:03:06,879
for example this app is malicious and

84
00:03:06,879 --> 00:03:09,360
this app is clean

85
00:03:09,360 --> 00:03:11,360
and the machine learning model will draw

86
00:03:11,360 --> 00:03:12,879
a line will infer

87
00:03:12,879 --> 00:03:15,440
a rule that distinguishes between both

88
00:03:15,440 --> 00:03:16,879
classes

89
00:03:16,879 --> 00:03:19,040
this is only making use of data for

90
00:03:19,040 --> 00:03:20,720
which we have labels

91
00:03:20,720 --> 00:03:22,959
unfortunately we not always have labels

92
00:03:22,959 --> 00:03:24,879
asked in the example of malware

93
00:03:24,879 --> 00:03:26,000
classification

94
00:03:26,000 --> 00:03:28,720
and locally trained models but this is

95
00:03:28,720 --> 00:03:30,080
more applicable to all

96
00:03:30,080 --> 00:03:32,319
to other applications where just a lot

97
00:03:32,319 --> 00:03:34,640
of unlabeled data is just discarded

98
00:03:34,640 --> 00:03:35,519
because

99
00:03:35,519 --> 00:03:37,920
there are no labels with some supervised

100
00:03:37,920 --> 00:03:39,200
machine learning

101
00:03:39,200 --> 00:03:41,680
you we make use of both labeled and

102
00:03:41,680 --> 00:03:43,120
enabled data

103
00:03:43,120 --> 00:03:45,280
to predict to infer a rule that

104
00:03:45,280 --> 00:03:48,480
separates classes

105
00:03:50,879 --> 00:03:53,040
one specific type of supervised machine

106
00:03:53,040 --> 00:03:53,920
learning is called

107
00:03:53,920 --> 00:03:58,799
safe ssl this safety does not come from

108
00:03:58,799 --> 00:04:00,640
security or privacy it comes from

109
00:04:00,640 --> 00:04:02,400
performance safety

110
00:04:02,400 --> 00:04:06,159
in the sense that the the classic the

111
00:04:06,159 --> 00:04:08,640
same supervised model this safe ssl

112
00:04:08,640 --> 00:04:09,519
model

113
00:04:09,519 --> 00:04:13,200
assures that the model itself will at

114
00:04:13,200 --> 00:04:14,879
least match the performance of some

115
00:04:14,879 --> 00:04:18,160
baseline classifier in our paper we'll

116
00:04:18,160 --> 00:04:18,478
make

117
00:04:18,478 --> 00:04:21,120
we will make use of one specific model

118
00:04:21,120 --> 00:04:21,839
called

119
00:04:21,839 --> 00:04:25,759
safe w where this semi-supervised model

120
00:04:25,759 --> 00:04:28,800
is just an ensemble of classifiers

121
00:04:28,800 --> 00:04:31,840
weighted with these alpha weights and

122
00:04:31,840 --> 00:04:32,639
the

123
00:04:32,639 --> 00:04:36,479
assurance is that the predictions in f

124
00:04:36,479 --> 00:04:39,759
that are yielded from these

125
00:04:39,759 --> 00:04:42,320
alpha weights and the baseline and the

126
00:04:42,320 --> 00:04:43,360
base learners

127
00:04:43,360 --> 00:04:46,720
each of these learners

128
00:04:46,720 --> 00:04:49,040
at least match the performance of the

129
00:04:49,040 --> 00:04:50,240
baseline

130
00:04:50,240 --> 00:04:53,040
predictions

131
00:04:54,160 --> 00:04:57,360
so how do we use say w

132
00:04:57,360 --> 00:05:01,199
to create limb or less standing for less

133
00:05:01,199 --> 00:05:02,639
is more

134
00:05:02,639 --> 00:05:06,720
well we essentially federate safew

135
00:05:06,720 --> 00:05:08,880
we make each of the clients install as

136
00:05:08,880 --> 00:05:10,160
sfw model

137
00:05:10,160 --> 00:05:13,120
and also the cloud as well the cloud

138
00:05:13,120 --> 00:05:14,400
will train the models

139
00:05:14,400 --> 00:05:16,720
and then each of the clients will just

140
00:05:16,720 --> 00:05:17,840
use their apps

141
00:05:17,840 --> 00:05:20,800
to predict the weights note that these

142
00:05:20,800 --> 00:05:22,880
waves these alpha weights

143
00:05:22,880 --> 00:05:28,240
are very few compared with traditional

144
00:05:28,240 --> 00:05:30,720
federated learning models if a deep

145
00:05:30,720 --> 00:05:32,320
neural network typically has

146
00:05:32,320 --> 00:05:34,560
hundreds if not hundred thousands of

147
00:05:34,560 --> 00:05:36,160
more fights

148
00:05:36,160 --> 00:05:38,720
our client weights the ensemble we have

149
00:05:38,720 --> 00:05:40,240
is for example

150
00:05:40,240 --> 00:05:43,440
five weights

151
00:05:43,440 --> 00:05:45,280
this will be very important later for

152
00:05:45,280 --> 00:05:48,400
the security analysis

153
00:05:48,400 --> 00:05:51,919
how does a federation round work where

154
00:05:51,919 --> 00:05:53,919
first of all the cloud will train

155
00:05:53,919 --> 00:05:55,919
the baseline classifier and the base

156
00:05:55,919 --> 00:05:57,600
learners using

157
00:05:57,600 --> 00:06:00,080
possibly supervised and supervised

158
00:06:00,080 --> 00:06:01,039
models

159
00:06:01,039 --> 00:06:02,880
or maybe some supervised as well in our

160
00:06:02,880 --> 00:06:05,120
case we only use fully supervised model

161
00:06:05,120 --> 00:06:05,759
that is

162
00:06:05,759 --> 00:06:08,160
we only we make use of the label data

163
00:06:08,160 --> 00:06:10,800
from the cloud

164
00:06:10,800 --> 00:06:12,160
and we believe this assumption is

165
00:06:12,160 --> 00:06:13,600
reasonable for applications such as

166
00:06:13,600 --> 00:06:15,520
mobile detection where service providers

167
00:06:15,520 --> 00:06:16,479
actually have

168
00:06:16,479 --> 00:06:19,280
ground truth for at least a set of apps

169
00:06:19,280 --> 00:06:21,039
once they have trained these models they

170
00:06:21,039 --> 00:06:23,440
will share them with each of the clients

171
00:06:23,440 --> 00:06:25,840
and then clients will use the locally

172
00:06:25,840 --> 00:06:27,039
installed apps

173
00:06:27,039 --> 00:06:28,960
for which the cloud has no visibility

174
00:06:28,960 --> 00:06:31,600
over to estimate the alpha weights of

175
00:06:31,600 --> 00:06:34,479
the zw model

176
00:06:34,479 --> 00:06:36,720
these weights will then be used to

177
00:06:36,720 --> 00:06:37,680
predict locally

178
00:06:37,680 --> 00:06:41,440
either as they are without

179
00:06:41,440 --> 00:06:45,680
any other input or in subsequent rounds

180
00:06:45,680 --> 00:06:49,280
not in round one but in further rounds

181
00:06:49,280 --> 00:06:52,400
with the federated waves averaged with

182
00:06:52,400 --> 00:06:54,638
them

183
00:06:54,960 --> 00:06:58,319
so that means that in step six the club

184
00:06:58,319 --> 00:06:59,919
and the clients will share

185
00:06:59,919 --> 00:07:01,759
their weights with the cloud and the

186
00:07:01,759 --> 00:07:05,520
cloud will do an aggregation with them

187
00:07:05,520 --> 00:07:07,440
what kind of aggregation well we've

188
00:07:07,440 --> 00:07:09,120
designed a pretty simple allocation

189
00:07:09,120 --> 00:07:09,919
algorithm

190
00:07:09,919 --> 00:07:11,919
where the cloud first averages the

191
00:07:11,919 --> 00:07:14,639
client weights

192
00:07:14,960 --> 00:07:16,880
and then takes the average for the

193
00:07:16,880 --> 00:07:18,800
median in this case the same

194
00:07:18,800 --> 00:07:21,039
between the client and the cloud weights

195
00:07:21,039 --> 00:07:23,360
remember we also have a cfw model in the

196
00:07:23,360 --> 00:07:24,560
cloud

197
00:07:24,560 --> 00:07:26,479
for this to compute these cloud weights

198
00:07:26,479 --> 00:07:28,560
the cloud will use

199
00:07:28,560 --> 00:07:32,000
its own unlabeled data

200
00:07:32,000 --> 00:07:35,039
so these weights will be the

201
00:07:35,039 --> 00:07:36,880
what we call the federated weights that

202
00:07:36,880 --> 00:07:39,280
will be sent back to all the clients

203
00:07:39,280 --> 00:07:41,680
we don't need to necessarily to update

204
00:07:41,680 --> 00:07:43,199
the baseline classifier and the page

205
00:07:43,199 --> 00:07:45,120
learners but we do need to send the

206
00:07:45,120 --> 00:07:47,599
federated weights back to all clients

207
00:07:47,599 --> 00:07:50,160
so that they can again average their own

208
00:07:50,160 --> 00:07:51,680
local estimated weights

209
00:07:51,680 --> 00:07:55,199
with the federated weights

210
00:07:57,039 --> 00:07:58,800
notice that the design is pretty

211
00:07:58,800 --> 00:08:00,400
conservative

212
00:08:00,400 --> 00:08:03,360
first of all as i said before there is a

213
00:08:03,360 --> 00:08:04,879
very low number of parameters in the

214
00:08:04,879 --> 00:08:06,560
model updates

215
00:08:06,560 --> 00:08:09,280
let me go back one slide these client

216
00:08:09,280 --> 00:08:11,039
weights the number of client weights

217
00:08:11,039 --> 00:08:12,720
is actually the same as the number of

218
00:08:12,720 --> 00:08:14,080
base learners

219
00:08:14,080 --> 00:08:16,800
that the cloud shares with the clients

220
00:08:16,800 --> 00:08:17,759
this number of

221
00:08:17,759 --> 00:08:20,800
base learners is not doesn't need to be

222
00:08:20,800 --> 00:08:21,919
big

223
00:08:21,919 --> 00:08:23,599
is the quality of the base learners that

224
00:08:23,599 --> 00:08:25,360
matter not the quantity

225
00:08:25,360 --> 00:08:27,840
that means that we can have very few

226
00:08:27,840 --> 00:08:29,599
client weights in our experiments we

227
00:08:29,599 --> 00:08:33,360
actually have only five client weights

228
00:08:34,958 --> 00:08:37,440
so we believe with that we will reduce a

229
00:08:37,440 --> 00:08:38,880
lot of the space for

230
00:08:38,880 --> 00:08:41,599
poisoning attacks and integrity and

231
00:08:41,599 --> 00:08:44,080
privacy attacks

232
00:08:44,080 --> 00:08:48,160
furthermore there is a strong influence

233
00:08:48,160 --> 00:08:49,279
of trusted data

234
00:08:49,279 --> 00:08:52,399
in the modern federation

235
00:08:52,720 --> 00:08:55,519
by virtue of averaging the already

236
00:08:55,519 --> 00:08:57,440
averaged client weights

237
00:08:57,440 --> 00:08:59,440
with the cloud weights we are giving a

238
00:08:59,440 --> 00:09:00,880
lot of importance

239
00:09:00,880 --> 00:09:02,480
with to the cloud to the weights that

240
00:09:02,480 --> 00:09:04,800
are computed using only the unlevel data

241
00:09:04,800 --> 00:09:05,839
from the cloud

242
00:09:05,839 --> 00:09:08,560
and we assume this cloud this cloud data

243
00:09:08,560 --> 00:09:09,440
is safe from

244
00:09:09,440 --> 00:09:12,480
an from a from an integrity

245
00:09:12,480 --> 00:09:14,240
adversary because the cloud has no

246
00:09:14,240 --> 00:09:17,279
interest in pursuing its own model

247
00:09:17,279 --> 00:09:19,200
and we assume the security the

248
00:09:19,200 --> 00:09:20,800
infrastructure is secure against

249
00:09:20,800 --> 00:09:22,000
external adversaries

250
00:09:22,000 --> 00:09:24,720
for the data

251
00:09:26,080 --> 00:09:28,160
so this is this was a very high level

252
00:09:28,160 --> 00:09:30,800
overview of the paper now let me go into

253
00:09:30,800 --> 00:09:34,640
the evaluation first with performance

254
00:09:34,640 --> 00:09:36,959
first we sample the clean apps from the

255
00:09:36,959 --> 00:09:37,760
androsu

256
00:09:37,760 --> 00:09:40,800
top three most popular stores and then

257
00:09:40,800 --> 00:09:43,120
we sample the malicious examples the

258
00:09:43,120 --> 00:09:46,160
malicious apps from anthony from the

259
00:09:46,160 --> 00:09:48,160
android genome and the android mower

260
00:09:48,160 --> 00:09:50,000
dataset projects

261
00:09:50,000 --> 00:09:53,680
we actually sample 50 000

262
00:09:54,160 --> 00:09:58,560
apps 25k clean 25k

263
00:09:58,560 --> 00:10:01,839
malicious in the paperwork we also

264
00:10:01,839 --> 00:10:02,959
report results with

265
00:10:02,959 --> 00:10:05,760
other data set drawn from the mammal

266
00:10:05,760 --> 00:10:06,720
zoid paper

267
00:10:06,720 --> 00:10:08,480
but in this presentation i will stick to

268
00:10:08,480 --> 00:10:12,079
our own data set results

269
00:10:12,959 --> 00:10:16,240
so we implement lim using python

270
00:10:16,240 --> 00:10:18,399
and we use we need to choose a set of

271
00:10:18,399 --> 00:10:19,519
base learners

272
00:10:19,519 --> 00:10:21,600
and a baseline classifier for the cfw

273
00:10:21,600 --> 00:10:22,800
ensemble

274
00:10:22,800 --> 00:10:25,200
in the safew paper they actually choose

275
00:10:25,200 --> 00:10:26,800
pretty fancy

276
00:10:26,800 --> 00:10:30,839
models in our case we wanted to focus

277
00:10:30,839 --> 00:10:34,000
on checking if we can learn from the

278
00:10:34,000 --> 00:10:35,200
federation without

279
00:10:35,200 --> 00:10:37,279
ground trooping them in the clients and

280
00:10:37,279 --> 00:10:38,959
also if we can withstand

281
00:10:38,959 --> 00:10:41,040
security and privacy attacks so we

282
00:10:41,040 --> 00:10:42,880
didn't want to to look for the best

283
00:10:42,880 --> 00:10:44,720
performance we wanted to check

284
00:10:44,720 --> 00:10:47,279
those two things for that reason we just

285
00:10:47,279 --> 00:10:49,120
use very vanilla

286
00:10:49,120 --> 00:10:49,920
models one

287
00:10:49,920 --> 00:10:50,560
knnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnn

288
00:10:50,560 --> 00:10:52,320
withening horse three

289
00:10:52,320 --> 00:10:55,120
one svm three random forests and one

290
00:10:55,120 --> 00:10:57,760
logistic operation

291
00:10:57,760 --> 00:10:59,920
and we make experiments with all of them

292
00:10:59,920 --> 00:11:01,040
as baseline

293
00:11:01,040 --> 00:11:04,240
and the rest as base learners

294
00:11:04,240 --> 00:11:06,640
in the paper again you have a table with

295
00:11:06,640 --> 00:11:10,720
all the averages of all the experiments

296
00:11:10,720 --> 00:11:14,240
we simulate a federation with 50 rounds

297
00:11:14,240 --> 00:11:18,000
and 200 clients each with a set of 96

298
00:11:18,000 --> 00:11:19,120
pre-installed apps

299
00:11:19,120 --> 00:11:23,440
i drew from my own one of my own phones

300
00:11:23,920 --> 00:11:26,240
so these are the performance results on

301
00:11:26,240 --> 00:11:28,399
the left hand side you can see the false

302
00:11:28,399 --> 00:11:29,600
positives and the

303
00:11:29,600 --> 00:11:32,800
right hand side you have the f1 score

304
00:11:32,800 --> 00:11:35,040
we use the f1 score because the dataset

305
00:11:35,040 --> 00:11:36,800
is unbalanced in the clients

306
00:11:36,800 --> 00:11:40,240
at least we make clients install

307
00:11:40,240 --> 00:11:43,360
apps from 0 to 5 per round

308
00:11:43,360 --> 00:11:45,279
with probability 0.1 that they will

309
00:11:45,279 --> 00:11:47,040
install a malicious app

310
00:11:47,040 --> 00:11:49,120
this is to reflect the fact that users

311
00:11:49,120 --> 00:11:50,160
are much more

312
00:11:50,160 --> 00:11:52,160
unlikely to install a monitorship than a

313
00:11:52,160 --> 00:11:53,760
cleanup

314
00:11:53,760 --> 00:11:56,639
on the other hand report the raw number

315
00:11:56,639 --> 00:11:58,320
of false positives this is the raw

316
00:11:58,320 --> 00:11:59,279
number

317
00:11:59,279 --> 00:12:02,240
in order to evaluate the feasibility of

318
00:12:02,240 --> 00:12:04,000
limb being installed as a client

319
00:12:04,000 --> 00:12:04,880
application

320
00:12:04,880 --> 00:12:07,839
and not annoying the user too much so

321
00:12:07,839 --> 00:12:10,000
one of the challenges of locally

322
00:12:10,000 --> 00:12:12,399
classified malware malicious app is that

323
00:12:12,399 --> 00:12:13,920
phosphates are very high

324
00:12:13,920 --> 00:12:16,399
so you will tire the user if you keep

325
00:12:16,399 --> 00:12:17,200
warning it

326
00:12:17,200 --> 00:12:19,279
about applications that maybe they know

327
00:12:19,279 --> 00:12:20,399
they are not malware

328
00:12:20,399 --> 00:12:23,040
or they get tired of all the alerts so

329
00:12:23,040 --> 00:12:24,480
we want to minimize the number of force

330
00:12:24,480 --> 00:12:26,079
positives

331
00:12:26,079 --> 00:12:29,200
in these figures we report results of

332
00:12:29,200 --> 00:12:31,120
one class of one

333
00:12:31,120 --> 00:12:34,399
configuration where the user or the

334
00:12:34,399 --> 00:12:36,720
baseline classifier is a k n

335
00:12:36,720 --> 00:12:39,839
and the rest are the base learners so in

336
00:12:39,839 --> 00:12:42,399
blue you have the baseline the k n

337
00:12:42,399 --> 00:12:45,120
in red you have saved w that is the base

338
00:12:45,120 --> 00:12:46,800
learners without taking into account the

339
00:12:46,800 --> 00:12:47,920
federation

340
00:12:47,920 --> 00:12:50,639
and then in yellow you have the lean

341
00:12:50,639 --> 00:12:51,360
results

342
00:12:51,360 --> 00:12:54,480
taking back on the federated weights

343
00:12:54,480 --> 00:12:57,920
and you can see that around round 24

344
00:12:57,920 --> 00:13:00,639
23 there is a big improvement of

345
00:13:00,639 --> 00:13:01,440
performance

346
00:13:01,440 --> 00:13:04,079
due to a drop from five to three false

347
00:13:04,079 --> 00:13:05,920
positives and this gets reflected in the

348
00:13:05,920 --> 00:13:06,880
f1 score

349
00:13:06,880 --> 00:13:12,000
or we jump from 0.4 to roughly 0.6

350
00:13:12,000 --> 00:13:13,920
so again we show that using the

351
00:13:13,920 --> 00:13:15,040
federated ways

352
00:13:15,040 --> 00:13:18,839
makes a difference with respect to safe

353
00:13:18,839 --> 00:13:21,040
volume

354
00:13:21,040 --> 00:13:23,040
so we learned from the federation now we

355
00:13:23,040 --> 00:13:24,639
want to check if we can withstand

356
00:13:24,639 --> 00:13:26,079
security and privacy attacks

357
00:13:26,079 --> 00:13:28,720
we will first check against an integrity

358
00:13:28,720 --> 00:13:29,680
adversary

359
00:13:29,680 --> 00:13:32,399
a pretty strong adversary that owns 50

360
00:13:32,399 --> 00:13:33,519
of the clients

361
00:13:33,519 --> 00:13:36,320
and wants to perform a targeted attack

362
00:13:36,320 --> 00:13:37,600
the targeted attack

363
00:13:37,600 --> 00:13:41,040
aims to trickle him into misclassifying

364
00:13:41,040 --> 00:13:42,639
a specific malicious app

365
00:13:42,639 --> 00:13:46,079
as clean to do that the adversary will

366
00:13:46,079 --> 00:13:47,680
leverage what we call the allied

367
00:13:47,680 --> 00:13:49,360
learners that is learners

368
00:13:49,360 --> 00:13:51,680
from the ensemble that already

369
00:13:51,680 --> 00:13:53,680
misclassified this malicious app

370
00:13:53,680 --> 00:13:56,480
as clean the goal is to have enormous

371
00:13:56,480 --> 00:13:58,079
majority of these allied learners so

372
00:13:58,079 --> 00:13:59,760
that the ensemble itself

373
00:13:59,760 --> 00:14:01,920
will make will predict that price

374
00:14:01,920 --> 00:14:05,199
malicious app is clean

375
00:14:05,440 --> 00:14:07,680
so again this the at the adversary is

376
00:14:07,680 --> 00:14:09,600
strategic so it takes into account

377
00:14:09,600 --> 00:14:12,800
the architecture of limb and that means

378
00:14:12,800 --> 00:14:15,680
we it has to buy it has to gain this

379
00:14:15,680 --> 00:14:17,680
honest majority in three places first of

380
00:14:17,680 --> 00:14:18,320
all

381
00:14:18,320 --> 00:14:20,240
its own compromised clients so this

382
00:14:20,240 --> 00:14:21,839
fifty percent of the clients

383
00:14:21,839 --> 00:14:23,519
need to submit personal weights where

384
00:14:23,519 --> 00:14:26,560
the allied learners have owned majority

385
00:14:26,560 --> 00:14:28,639
with the goal of tricking the honest

386
00:14:28,639 --> 00:14:29,680
client weights

387
00:14:29,680 --> 00:14:31,680
into getting the same honest majority

388
00:14:31,680 --> 00:14:33,199
for the ad learners

389
00:14:33,199 --> 00:14:35,680
and the same for the cloud we formulate

390
00:14:35,680 --> 00:14:37,839
this attack as an optimization problem

391
00:14:37,839 --> 00:14:40,800
where the goal is to submit weights that

392
00:14:40,800 --> 00:14:41,120
are

393
00:14:41,120 --> 00:14:43,600
as close as similar as possible to the

394
00:14:43,600 --> 00:14:46,160
honestly computed weights

395
00:14:46,160 --> 00:14:50,000
making sure that these three constraints

396
00:14:50,000 --> 00:14:54,320
hold so these are the results and here

397
00:14:54,320 --> 00:14:54,959
you see this

398
00:14:54,959 --> 00:14:57,040
the same graph as before but with one

399
00:14:57,040 --> 00:14:59,120
more line in

400
00:14:59,120 --> 00:15:02,720
black this black line is the one where

401
00:15:02,720 --> 00:15:05,120
reports results from the poisson clients

402
00:15:05,120 --> 00:15:07,120
and we can see that indeed

403
00:15:07,120 --> 00:15:08,720
the poison claims do not wreck

404
00:15:08,720 --> 00:15:10,800
performance it still

405
00:15:10,800 --> 00:15:14,800
improves over baseline but it goes from

406
00:15:14,800 --> 00:15:18,560
two roughly two to three

407
00:15:18,560 --> 00:15:20,639
and four specialties lim still

408
00:15:20,639 --> 00:15:22,480
outperforms safew

409
00:15:22,480 --> 00:15:25,360
but it does a six side so this zigzag

410
00:15:25,360 --> 00:15:26,480
means

411
00:15:26,480 --> 00:15:28,880
that every other round the adversary

412
00:15:28,880 --> 00:15:29,600
succeeds

413
00:15:29,600 --> 00:15:33,120
into poisoning a few of the clients

414
00:15:33,120 --> 00:15:37,040
so by owning 100 100 clients out of 200

415
00:15:37,040 --> 00:15:40,720
it will poison a few of them

416
00:15:40,720 --> 00:15:43,440
every other round and this also gets

417
00:15:43,440 --> 00:15:44,320
reflected

418
00:15:44,320 --> 00:15:49,519
in the f1 score of the clients

419
00:15:51,519 --> 00:15:54,560
finally to evaluate privacy we implement

420
00:15:54,560 --> 00:15:56,480
we adapt and implement the membership

421
00:15:56,480 --> 00:15:58,079
inference attack presented by melissa at

422
00:15:58,079 --> 00:16:01,600
all in 202019 smp

423
00:16:01,600 --> 00:16:04,480
the goal is to identify and which app

424
00:16:04,480 --> 00:16:06,480
users install in a specific round

425
00:16:06,480 --> 00:16:08,160
by isolating the contribution of a

426
00:16:08,160 --> 00:16:10,399
specific model update in that round

427
00:16:10,399 --> 00:16:12,639
so it will isolate the contribution and

428
00:16:12,639 --> 00:16:13,600
try to see

429
00:16:13,600 --> 00:16:16,079
if that contribution matches what we

430
00:16:16,079 --> 00:16:19,279
obviously can train

431
00:16:19,759 --> 00:16:23,680
with one single app

432
00:16:23,680 --> 00:16:25,360
so the event the vantage point is the

433
00:16:25,360 --> 00:16:26,720
service provider

434
00:16:26,720 --> 00:16:28,880
and the data is the submitted client

435
00:16:28,880 --> 00:16:31,759
white and the federated waits

436
00:16:31,759 --> 00:16:33,519
but we see that the attack just doesn't

437
00:16:33,519 --> 00:16:35,040
work why

438
00:16:35,040 --> 00:16:37,120
because there are too few ways too few

439
00:16:37,120 --> 00:16:39,759
information

440
00:16:39,759 --> 00:16:42,000
when the adversary trains the model

441
00:16:42,000 --> 00:16:43,440
using only one app

442
00:16:43,440 --> 00:16:46,800
the weights are actually the same for

443
00:16:46,800 --> 00:16:48,399
all of the base learners that's because

444
00:16:48,399 --> 00:16:49,519
there is not enough

445
00:16:49,519 --> 00:16:52,639
information to discriminate which

446
00:16:52,639 --> 00:16:55,199
of the baseliners are better or worse

447
00:16:55,199 --> 00:16:56,880
for this testing data

448
00:16:56,880 --> 00:17:00,880
that means that all of the clients

449
00:17:00,880 --> 00:17:02,959
all of the weights will be equal for

450
00:17:02,959 --> 00:17:04,559
example in our case they will be

451
00:17:04,559 --> 00:17:08,000
0.2 and that just doesn't match

452
00:17:08,000 --> 00:17:10,160
any of the contributions of one single

453
00:17:10,160 --> 00:17:12,000
update

454
00:17:12,000 --> 00:17:13,919
so for future work we would like to make

455
00:17:13,919 --> 00:17:15,359
experiments at scale

456
00:17:15,359 --> 00:17:17,599
more than 200 clients and more than 50

457
00:17:17,599 --> 00:17:19,119
runs federation

458
00:17:19,119 --> 00:17:21,520
more applications where users just

459
00:17:21,520 --> 00:17:23,280
cannot provide with labels

460
00:17:23,280 --> 00:17:25,760
and real-world data with with an

461
00:17:25,760 --> 00:17:27,039
implementation

462
00:17:27,039 --> 00:17:30,840
that can be used by real users in their

463
00:17:30,840 --> 00:17:33,200
smartphones

464
00:17:33,200 --> 00:17:36,000
so to conclude we present with presented

465
00:17:36,000 --> 00:17:36,640
lim

466
00:17:36,640 --> 00:17:38,559
that can do federated learning without

467
00:17:38,559 --> 00:17:40,080
user supervision

468
00:17:40,080 --> 00:17:42,000
clients benefit from more data and the

469
00:17:42,000 --> 00:17:43,600
federated ways

470
00:17:43,600 --> 00:17:45,360
and we've shown it is resistant against

471
00:17:45,360 --> 00:17:48,640
integrity and privacy attacks

472
00:17:48,640 --> 00:17:50,000
thank you very much if you have any

473
00:17:50,000 --> 00:17:54,200
question i am open to answer them thank

474
00:17:54,200 --> 00:17:57,200
you

