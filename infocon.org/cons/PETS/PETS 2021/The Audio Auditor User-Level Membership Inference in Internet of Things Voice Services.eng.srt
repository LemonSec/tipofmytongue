1
00:00:00,880 --> 00:00:03,600
hi everyone my name is jen tian meow

2
00:00:03,600 --> 00:00:06,000
a phd candidate from swimmer university

3
00:00:06,000 --> 00:00:07,440
of technology

4
00:00:07,440 --> 00:00:09,519
today i'd like to present our paper

5
00:00:09,519 --> 00:00:11,679
titled the audio auditor

6
00:00:11,679 --> 00:00:14,240
user level membership inference internet

7
00:00:14,240 --> 00:00:16,400
of things voice service

8
00:00:16,400 --> 00:00:18,000
we'd like to thank the precious

9
00:00:18,000 --> 00:00:19,680
suggestions from these papers

10
00:00:19,680 --> 00:00:23,439
reviewers chairs and the shepherd

11
00:00:23,439 --> 00:00:25,920
which greatly improved this paper's

12
00:00:25,920 --> 00:00:27,680
quality

13
00:00:27,680 --> 00:00:30,480
now allow me to elaborate this research

14
00:00:30,480 --> 00:00:31,840
step by step

15
00:00:31,840 --> 00:00:34,000
for your information this is today's

16
00:00:34,000 --> 00:00:35,280
roadmap

17
00:00:35,280 --> 00:00:37,680
in this research we focus on the core

18
00:00:37,680 --> 00:00:39,120
component of oil t

19
00:00:39,120 --> 00:00:41,440
voice service automatic speech

20
00:00:41,440 --> 00:00:42,719
recognition system

21
00:00:42,719 --> 00:00:46,079
in short asr system briefly

22
00:00:46,079 --> 00:00:48,480
we design an audio auditor to perform

23
00:00:48,480 --> 00:00:51,280
user level membership inference tasks

24
00:00:51,280 --> 00:00:53,600
the auditor can verify whether a

25
00:00:53,600 --> 00:00:54,719
specific user

26
00:00:54,719 --> 00:00:57,840
has unwillingly contributed audio used

27
00:00:57,840 --> 00:00:58,719
to train

28
00:00:58,719 --> 00:01:01,359
an asr model especially when the

29
00:01:01,359 --> 00:01:04,879
transcription is the only output

30
00:01:04,879 --> 00:01:07,920
the story begins that voice servers are

31
00:01:07,920 --> 00:01:10,880
emerged increasingly especially in iot

32
00:01:10,880 --> 00:01:12,159
devices

33
00:01:12,159 --> 00:01:15,040
like google assistant amazon alexa and

34
00:01:15,040 --> 00:01:16,400
etc

35
00:01:16,400 --> 00:01:19,040
however with lots of news reporting the

36
00:01:19,040 --> 00:01:20,320
unauthorized use

37
00:01:20,320 --> 00:01:23,360
of users audio recordings

38
00:01:23,360 --> 00:01:25,840
people start to worry about it from

39
00:01:25,840 --> 00:01:26,560
privacy

40
00:01:26,560 --> 00:01:29,200
and security concerns like what bbc

41
00:01:29,200 --> 00:01:30,479
reported that

42
00:01:30,479 --> 00:01:34,240
hmrc forced to delete 5 million voice

43
00:01:34,240 --> 00:01:35,119
files

44
00:01:35,119 --> 00:01:38,479
and gta pr and law have

45
00:01:38,479 --> 00:01:41,600
have been enforced to regulate personal

46
00:01:41,600 --> 00:01:42,079
data

47
00:01:42,079 --> 00:01:45,840
processing till here we concern

48
00:01:45,840 --> 00:01:48,799
is there any tools to check the regular

49
00:01:48,799 --> 00:01:51,119
data preprocessing

50
00:01:51,119 --> 00:01:53,439
especially the information hidden from

51
00:01:53,439 --> 00:01:54,799
the model itself

52
00:01:54,799 --> 00:01:57,520
for instance when a user intends to

53
00:01:57,520 --> 00:02:00,640
cancel the account of a voice service

54
00:02:00,640 --> 00:02:03,759
expect erasing the records stored in the

55
00:02:03,759 --> 00:02:04,880
database

56
00:02:04,880 --> 00:02:07,119
is the model behind to still remember

57
00:02:07,119 --> 00:02:08,959
this user or not

58
00:02:08,959 --> 00:02:10,959
is it in line with the right to be

59
00:02:10,959 --> 00:02:12,080
forgotten

60
00:02:12,080 --> 00:02:17,200
regulation in gdpr law

61
00:02:17,520 --> 00:02:20,879
firstly we unfold asr assistance

62
00:02:20,879 --> 00:02:24,080
behind the voice servers

63
00:02:24,080 --> 00:02:26,319
there are two state-of-art pipelines

64
00:02:26,319 --> 00:02:28,720
used to build the asr system

65
00:02:28,720 --> 00:02:30,959
including the typical hybrid is

66
00:02:30,959 --> 00:02:31,840
assistance

67
00:02:31,840 --> 00:02:36,000
and hnas assistance a hybrid asr

68
00:02:36,000 --> 00:02:38,480
system is composed of a pre-processing

69
00:02:38,480 --> 00:02:39,360
step

70
00:02:39,360 --> 00:02:42,640
an acoustic model training step and a

71
00:02:42,640 --> 00:02:44,800
decoding step with the help of a

72
00:02:44,800 --> 00:02:45,920
language model

73
00:02:45,920 --> 00:02:48,400
to output the transcription of an input

74
00:02:48,400 --> 00:02:49,680
audio

75
00:02:49,680 --> 00:02:53,200
it is the acoustic model that learns

76
00:02:53,200 --> 00:02:55,440
the audio information from its training

77
00:02:55,440 --> 00:02:57,760
data set

78
00:02:57,760 --> 00:03:01,200
an ancient nissl system mainly refers to

79
00:03:01,200 --> 00:03:04,400
an attention-based encoder decoder

80
00:03:04,400 --> 00:03:05,599
models

81
00:03:05,599 --> 00:03:08,640
unlike the hyper-asr system the model

82
00:03:08,640 --> 00:03:10,120
predicts

83
00:03:10,120 --> 00:03:12,800
sub-sub-word sequences which are

84
00:03:12,800 --> 00:03:13,599
converted

85
00:03:13,599 --> 00:03:17,760
directly as word sequences

86
00:03:17,760 --> 00:03:20,800
as we understand our target model now we

87
00:03:20,800 --> 00:03:21,519
explore

88
00:03:21,519 --> 00:03:24,560
some possible solutions membership in

89
00:03:24,560 --> 00:03:25,519
first attack

90
00:03:25,519 --> 00:03:28,879
in short mia was firstly introduced by

91
00:03:28,879 --> 00:03:30,319
chancry ato

92
00:03:30,319 --> 00:03:32,879
it can infer whether a record was once

93
00:03:32,879 --> 00:03:33,760
used to

94
00:03:33,760 --> 00:03:36,959
train a target model at or not under the

95
00:03:36,959 --> 00:03:38,799
black box success

96
00:03:38,799 --> 00:03:41,599
the reason is that there's always an

97
00:03:41,599 --> 00:03:43,440
overfitting issue within

98
00:03:43,440 --> 00:03:46,720
an ml model the model will behave

99
00:03:46,720 --> 00:03:49,519
better in predicting a record in no

100
00:03:49,519 --> 00:03:50,239
before

101
00:03:50,239 --> 00:03:54,319
than a record in never seen the mia aims

102
00:03:54,319 --> 00:03:55,840
to learn this kind of

103
00:03:55,840 --> 00:03:58,560
different behaviors to distinguish the

104
00:03:58,560 --> 00:04:00,560
record level membership

105
00:04:00,560 --> 00:04:03,680
specifically shockery at all use shadow

106
00:04:03,680 --> 00:04:05,439
models to learn the

107
00:04:05,439 --> 00:04:07,920
target model's prediction behavior with

108
00:04:07,920 --> 00:04:10,239
a few assumptions like

109
00:04:10,239 --> 00:04:12,959
knowing the training sense distribution

110
00:04:12,959 --> 00:04:14,879
the target model structure

111
00:04:14,879 --> 00:04:18,639
and the output contains the label and

112
00:04:18,639 --> 00:04:21,839
its confidential scores then

113
00:04:21,839 --> 00:04:24,479
leonato and hey settle utilize

114
00:04:24,479 --> 00:04:25,520
generative

115
00:04:25,520 --> 00:04:28,639
adversarial networks to generate

116
00:04:28,639 --> 00:04:31,520
more accurate shuttle models while

117
00:04:31,520 --> 00:04:32,720
selling metal

118
00:04:32,720 --> 00:04:36,160
relax the first two assumptions

119
00:04:36,160 --> 00:04:40,240
further soon and shematical

120
00:04:40,240 --> 00:04:43,280
performance user level mia against the

121
00:04:43,280 --> 00:04:44,240
tax

122
00:04:44,240 --> 00:04:47,759
generative models instead of knowing the

123
00:04:47,759 --> 00:04:51,040
confidential score they rely on

124
00:04:51,040 --> 00:04:54,479
several top ranked predictions to infer

125
00:04:54,479 --> 00:04:57,840
their membership

126
00:04:57,919 --> 00:05:00,960
however in our case we need an

127
00:05:00,960 --> 00:05:03,919
auditor to help users to determine their

128
00:05:03,919 --> 00:05:05,600
user level membership of

129
00:05:05,600 --> 00:05:08,639
an asr model while only

130
00:05:08,639 --> 00:05:10,960
one final transcription without

131
00:05:10,960 --> 00:05:12,080
confidential score

132
00:05:12,080 --> 00:05:15,759
is the output additionally because the

133
00:05:15,759 --> 00:05:16,720
characteristic

134
00:05:16,720 --> 00:05:20,160
of the voice we can't expect users

135
00:05:20,160 --> 00:05:23,759
to replay all of their past recordings

136
00:05:23,759 --> 00:05:25,039
when they want to audit

137
00:05:25,039 --> 00:05:28,400
an asm system therefore

138
00:05:28,400 --> 00:05:31,039
we defined our user level membership as

139
00:05:31,039 --> 00:05:32,160
these

140
00:05:32,160 --> 00:05:35,440
acquiring with a user's data if this

141
00:05:35,440 --> 00:05:37,120
user has any data

142
00:05:37,120 --> 00:05:40,880
within target models training set

143
00:05:40,880 --> 00:05:44,080
even if the query data are not members

144
00:05:44,080 --> 00:05:45,759
of the training set

145
00:05:45,759 --> 00:05:48,240
this user is the user level member of

146
00:05:48,240 --> 00:05:51,199
this training set

147
00:05:51,360 --> 00:05:53,759
so far you can sense that how

148
00:05:53,759 --> 00:05:55,440
challenging is this task

149
00:05:55,440 --> 00:05:58,880
on asr systems in the real world

150
00:05:58,880 --> 00:06:02,160
since asr systems in real world provide

151
00:06:02,160 --> 00:06:02,639
only

152
00:06:02,639 --> 00:06:05,520
one transcription without any confidence

153
00:06:05,520 --> 00:06:06,800
score

154
00:06:06,800 --> 00:06:09,520
the link of the information is our first

155
00:06:09,520 --> 00:06:11,199
challenge

156
00:06:11,199 --> 00:06:14,160
secondly the user level inference

157
00:06:14,160 --> 00:06:14,800
requires

158
00:06:14,800 --> 00:06:17,680
a higher level of robustness than the

159
00:06:17,680 --> 00:06:19,440
record level inference

160
00:06:19,440 --> 00:06:23,199
especially for audio recordings

161
00:06:23,199 --> 00:06:26,000
the third challenge brings from the asr

162
00:06:26,000 --> 00:06:26,560
model

163
00:06:26,560 --> 00:06:30,080
itself its complexity in the

164
00:06:30,080 --> 00:06:32,840
learning architectures results in

165
00:06:32,840 --> 00:06:34,560
computationally resourced

166
00:06:34,560 --> 00:06:38,000
and time consuming

167
00:06:38,000 --> 00:06:40,560
before we move on to the math knowledge

168
00:06:40,560 --> 00:06:42,720
we clearly defined the problems

169
00:06:42,720 --> 00:06:45,840
wanted to be solved here behind the iot

170
00:06:45,840 --> 00:06:46,840
voice service

171
00:06:46,840 --> 00:06:50,000
is an asr model like this trained by a

172
00:06:50,000 --> 00:06:52,560
set of audio recordings

173
00:06:52,560 --> 00:06:55,759
where a user query this model with

174
00:06:55,759 --> 00:06:59,199
an audio under the black box success in

175
00:06:59,199 --> 00:07:00,400
real world

176
00:07:00,400 --> 00:07:02,880
the only output is the final predicted

177
00:07:02,880 --> 00:07:04,400
transcription

178
00:07:04,400 --> 00:07:07,280
our audio auditor aims to help users to

179
00:07:07,280 --> 00:07:08,080
determine

180
00:07:08,080 --> 00:07:11,039
whether an iot voice service still

181
00:07:11,039 --> 00:07:13,039
obtains their audio recordings

182
00:07:13,039 --> 00:07:16,319
as training set or not instead of disc

183
00:07:16,319 --> 00:07:18,960
uh discussing a specific model's trans

184
00:07:18,960 --> 00:07:20,479
uh membership

185
00:07:20,479 --> 00:07:23,440
our auditor aims to distinguish this

186
00:07:23,440 --> 00:07:24,080
user's

187
00:07:24,080 --> 00:07:27,360
membership that means a user can still

188
00:07:27,360 --> 00:07:28,319
find out

189
00:07:28,319 --> 00:07:31,440
if he has audio recordings in

190
00:07:31,440 --> 00:07:34,160
that training set even acquiring with

191
00:07:34,160 --> 00:07:36,240
the audio that the model never seen

192
00:07:36,240 --> 00:07:37,360
before

193
00:07:37,360 --> 00:07:41,120
because users can be expected to replay

194
00:07:41,120 --> 00:07:43,759
all of their past recordings in the real

195
00:07:43,759 --> 00:07:44,479
world

196
00:07:44,479 --> 00:07:47,759
with black box success for our

197
00:07:47,759 --> 00:07:51,440
audio auditor the user level membership

198
00:07:51,440 --> 00:07:52,160
inference

199
00:07:52,160 --> 00:07:54,720
is more meaningful and practical than

200
00:07:54,720 --> 00:07:55,840
the record level

201
00:07:55,840 --> 00:07:59,360
membership inference here is our

202
00:07:59,360 --> 00:08:01,039
methodology

203
00:08:01,039 --> 00:08:03,840
there are two processes the training

204
00:08:03,840 --> 00:08:04,400
process

205
00:08:04,400 --> 00:08:07,360
is to build our audio auditor while the

206
00:08:07,360 --> 00:08:09,919
auditing process is to use this auditor

207
00:08:09,919 --> 00:08:12,240
and gain our user level membership

208
00:08:12,240 --> 00:08:15,599
result during the training process

209
00:08:15,599 --> 00:08:18,080
shuttle model are chained with different

210
00:08:18,080 --> 00:08:20,000
data sets to mimic the

211
00:08:20,000 --> 00:08:22,720
target model's behavior then we

212
00:08:22,720 --> 00:08:23,759
collected

213
00:08:23,759 --> 00:08:26,479
those shadow models different behaviors

214
00:08:26,479 --> 00:08:27,120
on their

215
00:08:27,120 --> 00:08:29,919
own training and testing samples in user

216
00:08:29,919 --> 00:08:31,039
level

217
00:08:31,039 --> 00:08:33,679
our auditor learn this kind of behavior

218
00:08:33,679 --> 00:08:34,640
and hope

219
00:08:34,640 --> 00:08:37,039
the target model shared a similar

220
00:08:37,039 --> 00:08:38,640
behavior on that

221
00:08:38,640 --> 00:08:41,279
which help us determine its training

222
00:08:41,279 --> 00:08:43,679
sets user level membership

223
00:08:43,679 --> 00:08:47,120
specifically with the feature extraction

224
00:08:47,120 --> 00:08:49,680
and some statistical analysis in user

225
00:08:49,680 --> 00:08:50,560
level

226
00:08:50,560 --> 00:08:53,680
our auditor's training set is prepared

227
00:08:53,680 --> 00:08:56,640
herein the record processed from the

228
00:08:56,640 --> 00:08:58,320
acquiring results of the

229
00:08:58,320 --> 00:09:01,120
shuttle model's training set is labeled

230
00:09:01,120 --> 00:09:01,440
as

231
00:09:01,440 --> 00:09:04,640
member otherwise the processed record

232
00:09:04,640 --> 00:09:07,839
is labeled as num member

233
00:09:07,839 --> 00:09:10,480
according to previous study when shadow

234
00:09:10,480 --> 00:09:12,800
models architecture are the same

235
00:09:12,800 --> 00:09:14,720
there are no big difference in the

236
00:09:14,720 --> 00:09:16,880
performances between using one shuttle

237
00:09:16,880 --> 00:09:17,519
model

238
00:09:17,519 --> 00:09:20,959
and using multiple shuttle models

239
00:09:20,959 --> 00:09:24,000
thus we only train one shadow model

240
00:09:24,000 --> 00:09:26,959
for each model architecture to

241
00:09:26,959 --> 00:09:28,399
differentiate the

242
00:09:28,399 --> 00:09:30,640
auditor trained with different settings

243
00:09:30,640 --> 00:09:32,160
of the shadow model

244
00:09:32,160 --> 00:09:35,600
we use the combined auditor inferring

245
00:09:35,600 --> 00:09:36,480
the settings

246
00:09:36,480 --> 00:09:40,560
of using multiple shutter models

247
00:09:40,560 --> 00:09:43,760
as for the auditing process firstly

248
00:09:43,760 --> 00:09:47,600
a user requires the target model with

249
00:09:47,600 --> 00:09:49,120
their audio

250
00:09:49,120 --> 00:09:51,040
with the same feature process

251
00:09:51,040 --> 00:09:52,160
pre-processed

252
00:09:52,160 --> 00:09:54,480
the record will be feeding to the

253
00:09:54,480 --> 00:09:55,360
auditor

254
00:09:55,360 --> 00:09:58,240
while the output is either member or

255
00:09:58,240 --> 00:10:00,800
non-member

256
00:10:00,880 --> 00:10:04,240
to achieve user levels determination

257
00:10:04,240 --> 00:10:06,399
the feature extraction in the training

258
00:10:06,399 --> 00:10:07,279
process

259
00:10:07,279 --> 00:10:10,160
is the most important part we firstly

260
00:10:10,160 --> 00:10:12,560
extract two kinds of features

261
00:10:12,560 --> 00:10:15,839
the audio specific feature like speed

262
00:10:15,839 --> 00:10:19,680
and frame lengths and modes

263
00:10:19,680 --> 00:10:22,079
behavior specific features like

264
00:10:22,079 --> 00:10:23,680
similarity score

265
00:10:23,680 --> 00:10:26,720
missing characters and extra characters

266
00:10:26,720 --> 00:10:29,519
here in glove model is used to measure

267
00:10:29,519 --> 00:10:30,720
the similarity score

268
00:10:30,720 --> 00:10:33,360
between the transcriptions and the true

269
00:10:33,360 --> 00:10:34,640
text

270
00:10:34,640 --> 00:10:38,160
here is an example about the missing

271
00:10:38,160 --> 00:10:41,360
characters in brown and

272
00:10:41,360 --> 00:10:44,880
the extra characters in green with some

273
00:10:44,880 --> 00:10:45,519
feature

274
00:10:45,519 --> 00:10:48,160
process in user level like statistical

275
00:10:48,160 --> 00:10:49,200
analysis

276
00:10:49,200 --> 00:10:52,560
we discussed the effect of different

277
00:10:52,560 --> 00:10:54,000
features

278
00:10:54,000 --> 00:10:57,200
here we find that the missing characters

279
00:10:57,200 --> 00:11:00,640
and extra characters can enhance the

280
00:11:00,640 --> 00:11:02,640
ability of our auditor

281
00:11:02,640 --> 00:11:05,600
while the performance is not improved

282
00:11:05,600 --> 00:11:07,120
when considering the

283
00:11:07,120 --> 00:11:10,320
additional mfcc information

284
00:11:10,320 --> 00:11:13,519
therefore we use the feature set

285
00:11:13,519 --> 00:11:16,720
5 in feature extraction to build

286
00:11:16,720 --> 00:11:20,240
our auditor to be noticed since our

287
00:11:20,240 --> 00:11:21,600
auditor is the first

288
00:11:21,600 --> 00:11:24,640
work investigating the user level

289
00:11:24,640 --> 00:11:25,279
membership

290
00:11:25,279 --> 00:11:28,560
in asr system the comparable baselines

291
00:11:28,560 --> 00:11:31,120
is considered as 50 percent

292
00:11:31,120 --> 00:11:34,480
in accuracy briefly

293
00:11:34,480 --> 00:11:37,120
we evaluate our audio auditor from four

294
00:11:37,120 --> 00:11:38,160
perspectives

295
00:11:38,160 --> 00:11:41,200
including effectiveness efficiency

296
00:11:41,200 --> 00:11:44,640
data transferability and robustness

297
00:11:44,640 --> 00:11:47,519
to test the auditor's effectiveness we

298
00:11:47,519 --> 00:11:48,399
evaluate

299
00:11:48,399 --> 00:11:51,040
whether users are feasible to use this

300
00:11:51,040 --> 00:11:51,760
auditor

301
00:11:51,760 --> 00:11:54,480
with acceptable performance especially

302
00:11:54,480 --> 00:11:56,079
in real world

303
00:11:56,079 --> 00:11:58,959
for efficiency we evaluate under what

304
00:11:58,959 --> 00:11:59,680
condition

305
00:11:59,680 --> 00:12:02,160
the audition can perform the best for

306
00:12:02,160 --> 00:12:03,839
data transferability

307
00:12:03,839 --> 00:12:05,839
we evaluate effect on the auditor's

308
00:12:05,839 --> 00:12:08,000
performance when it was trained

309
00:12:08,000 --> 00:12:11,200
with different data distributions and

310
00:12:11,200 --> 00:12:12,560
for robustness

311
00:12:12,560 --> 00:12:14,639
we evaluate the effect on the auditor's

312
00:12:14,639 --> 00:12:16,639
performance when it was trained

313
00:12:16,639 --> 00:12:18,959
with different architectures or in the

314
00:12:18,959 --> 00:12:21,040
noisy environment

315
00:12:21,040 --> 00:12:23,200
part of the results are shown in today's

316
00:12:23,200 --> 00:12:25,279
presentation

317
00:12:25,279 --> 00:12:28,160
this huge figure showed that our auditor

318
00:12:28,160 --> 00:12:30,720
still effectively discriminates the user

319
00:12:30,720 --> 00:12:31,839
level membership

320
00:12:31,839 --> 00:12:34,639
with only a limited number of audios

321
00:12:34,639 --> 00:12:35,200
used

322
00:12:35,200 --> 00:12:39,120
per user figure 3

323
00:12:39,120 --> 00:12:41,440
on a smaller number of users in the

324
00:12:41,440 --> 00:12:43,040
auditor's training set

325
00:12:43,040 --> 00:12:45,279
we observe a rapid increase in

326
00:12:45,279 --> 00:12:46,079
performance

327
00:12:46,079 --> 00:12:48,800
with an increasing number of users

328
00:12:48,800 --> 00:12:49,360
assume

329
00:12:49,360 --> 00:12:52,399
a user is one user level member of an

330
00:12:52,399 --> 00:12:55,760
asr model from figure 4 we know that

331
00:12:55,760 --> 00:12:58,000
the auditor can perform a much higher

332
00:12:58,000 --> 00:12:59,120
accuracy

333
00:12:59,120 --> 00:13:01,920
using the recordings that the asr model

334
00:13:01,920 --> 00:13:03,040
seen before

335
00:13:03,040 --> 00:13:06,160
than using that on-scene recordings

336
00:13:06,160 --> 00:13:09,600
specifically it has a peak accuracy of

337
00:13:09,600 --> 00:13:13,040
around 94 percent in conclude

338
00:13:13,040 --> 00:13:15,519
it's better for users to choose the

339
00:13:15,519 --> 00:13:19,279
audios once heard by the model

340
00:13:19,279 --> 00:13:22,720
the auditor audits target asr models

341
00:13:22,720 --> 00:13:25,519
trend with different data distributions

342
00:13:25,519 --> 00:13:26,800
we observe that

343
00:13:26,800 --> 00:13:29,920
for accuracy and recall the target model

344
00:13:29,920 --> 00:13:31,519
with the same distribution

345
00:13:31,519 --> 00:13:34,639
as the auditor performs the best in

346
00:13:34,639 --> 00:13:35,440
general

347
00:13:35,440 --> 00:13:37,680
the data transferability is well

348
00:13:37,680 --> 00:13:38,480
observed

349
00:13:38,480 --> 00:13:41,199
with reasonably high matrix for all data

350
00:13:41,199 --> 00:13:43,680
distributions

351
00:13:43,680 --> 00:13:46,800
in this table the target model is lstm

352
00:13:46,800 --> 00:13:48,480
based asr model

353
00:13:48,480 --> 00:13:52,000
however the auditor with the lstm based

354
00:13:52,000 --> 00:13:55,440
asr shadow model is not the best instead

355
00:13:55,440 --> 00:13:58,560
the auditor with giu-based asr shallow

356
00:13:58,560 --> 00:13:59,040
model

357
00:13:59,040 --> 00:14:01,920
shows the best result in fact we can

358
00:14:01,920 --> 00:14:02,639
observe

359
00:14:02,639 --> 00:14:05,199
from the table although they have

360
00:14:05,199 --> 00:14:06,560
different architectures

361
00:14:06,560 --> 00:14:08,320
the overfitting level are the most

362
00:14:08,320 --> 00:14:10,880
similar one we can conclude that the

363
00:14:10,880 --> 00:14:12,720
best is the shadow model

364
00:14:12,720 --> 00:14:15,360
shares a similar overfitting level as

365
00:14:15,360 --> 00:14:16,959
the tracking model

366
00:14:16,959 --> 00:14:20,000
and when the offering level is unknown

367
00:14:20,000 --> 00:14:22,160
the combined auditor can achieve

368
00:14:22,160 --> 00:14:23,519
accuracy higher

369
00:14:23,519 --> 00:14:27,600
than the average with small set

370
00:14:27,839 --> 00:14:30,399
to test our model in the real world we

371
00:14:30,399 --> 00:14:32,560
keep our auditory model locally

372
00:14:32,560 --> 00:14:35,279
and conduct a proof of concept child to

373
00:14:35,279 --> 00:14:35,839
audit

374
00:14:35,839 --> 00:14:39,279
iphone series speech text service

375
00:14:39,279 --> 00:14:41,600
according to series statements i can be

376
00:14:41,600 --> 00:14:43,519
considered as a member

377
00:14:43,519 --> 00:14:47,279
user of my iphone series asr model

378
00:14:47,279 --> 00:14:50,639
since i choose opt-in option to improve

379
00:14:50,639 --> 00:14:54,320
theory and dictation as stated by iphone

380
00:14:54,320 --> 00:14:54,880
series

381
00:14:54,880 --> 00:14:58,079
privacy policy users data can be

382
00:14:58,079 --> 00:14:58,959
retained

383
00:14:58,959 --> 00:15:02,959
for up to two years thus another 52

384
00:15:02,959 --> 00:15:06,560
non-member users were selected randomly

385
00:15:06,560 --> 00:15:09,440
from liberal speech datasets whose

386
00:15:09,440 --> 00:15:11,199
samples were collected

387
00:15:11,199 --> 00:15:15,519
before 2014. to get the average

388
00:15:15,519 --> 00:15:17,839
performance of our auditor in the real

389
00:15:17,839 --> 00:15:18,880
world

390
00:15:18,880 --> 00:15:22,240
we store 100 auditors under the same

391
00:15:22,240 --> 00:15:25,600
setting with the training set conducted

392
00:15:25,600 --> 00:15:27,279
a hundred times

393
00:15:27,279 --> 00:15:30,639
the average overall accuracy is around

394
00:15:30,639 --> 00:15:33,839
ninety percent while orc

395
00:15:33,839 --> 00:15:39,680
a usc score is around 73 percent

396
00:15:39,759 --> 00:15:42,320
further we'd like to discuss the threats

397
00:15:42,320 --> 00:15:45,360
to our auditor's finality

398
00:15:45,360 --> 00:15:47,279
first of all is voice prince and

399
00:15:47,279 --> 00:15:48,639
normalization

400
00:15:48,639 --> 00:15:51,199
the speaker's voice can be disguised by

401
00:15:51,199 --> 00:15:52,320
using robust

402
00:15:52,320 --> 00:15:54,800
voice conversation while ensuring the

403
00:15:54,800 --> 00:15:55,839
correctness of

404
00:15:55,839 --> 00:15:58,079
speech content recognition it might

405
00:15:58,079 --> 00:16:00,560
affect our auditor's performance

406
00:16:00,560 --> 00:16:04,320
second is differentially private

407
00:16:04,320 --> 00:16:07,519
recognition systems differential privacy

408
00:16:07,519 --> 00:16:08,560
is one of the most

409
00:16:08,560 --> 00:16:11,440
popular methods to prevent a male model

410
00:16:11,440 --> 00:16:12,560
from leaking

411
00:16:12,560 --> 00:16:15,920
any training data information in asr

412
00:16:15,920 --> 00:16:18,800
it might influence the transcriptions

413
00:16:18,800 --> 00:16:20,240
confidential score

414
00:16:20,240 --> 00:16:22,800
considering our auditor is not relying

415
00:16:22,800 --> 00:16:24,160
on this information

416
00:16:24,160 --> 00:16:27,600
its performance remains unknown third

417
00:16:27,600 --> 00:16:30,000
one is uh workarounds and

418
00:16:30,000 --> 00:16:31,920
countermeasures like

419
00:16:31,920 --> 00:16:35,440
dropout generally change the

420
00:16:35,440 --> 00:16:38,720
target model's output probability

421
00:16:38,720 --> 00:16:41,279
however the change to probabilities of

422
00:16:41,279 --> 00:16:43,759
the asl model's output are not as

423
00:16:43,759 --> 00:16:44,800
sensitive as

424
00:16:44,800 --> 00:16:49,199
change to its translated task

425
00:16:49,199 --> 00:16:52,240
to conclude we propose the use of user

426
00:16:52,240 --> 00:16:55,120
level membership inference for auditing

427
00:16:55,120 --> 00:16:58,480
our auditor is effective has promising

428
00:16:58,480 --> 00:17:00,240
data transferability

429
00:17:00,240 --> 00:17:03,360
and is robust to various architectures

430
00:17:03,360 --> 00:17:07,199
and pipelines thanks for your time

431
00:17:07,199 --> 00:17:09,280
the related codes and turkeys are

432
00:17:09,280 --> 00:17:11,280
published in this github

433
00:17:11,280 --> 00:17:13,839
feel free to explore it and any

434
00:17:13,839 --> 00:17:14,400
questions

435
00:17:14,400 --> 00:17:17,919
are welcome to discuss

