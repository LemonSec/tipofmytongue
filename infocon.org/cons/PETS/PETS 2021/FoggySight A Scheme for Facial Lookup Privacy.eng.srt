1
00:00:01,040 --> 00:00:03,439
uh hello my name is pascal

2
00:00:03,439 --> 00:00:05,359
and i will be talking about our work uh

3
00:00:05,359 --> 00:00:07,279
foggy site a scheme for facial search

4
00:00:07,279 --> 00:00:08,240
privacy

5
00:00:08,240 --> 00:00:10,800
uh so this work is joint work with uh

6
00:00:10,800 --> 00:00:12,719
yvonne eftimov and yoshikono

7
00:00:12,719 --> 00:00:14,920
and it was done at the university of

8
00:00:14,920 --> 00:00:17,520
washington

9
00:00:17,520 --> 00:00:20,720
so this work is

10
00:00:20,720 --> 00:00:23,519
about face recognition technology and

11
00:00:23,519 --> 00:00:24,400
proposing

12
00:00:24,400 --> 00:00:26,720
to use adversarial examples to defend

13
00:00:26,720 --> 00:00:29,359
against space recognition technology

14
00:00:29,359 --> 00:00:31,679
and you know to understand why this is

15
00:00:31,679 --> 00:00:33,600
an important problem overall

16
00:00:33,600 --> 00:00:34,960
we have to understand a little bit of

17
00:00:34,960 --> 00:00:37,680
background about face recognition

18
00:00:37,680 --> 00:00:40,960
uh so for those who don't know

19
00:00:40,960 --> 00:00:43,280
face recognition is the use of computer

20
00:00:43,280 --> 00:00:45,200
vision and machine learning algorithms

21
00:00:45,200 --> 00:00:48,000
to automatically detect who is in a

22
00:00:48,000 --> 00:00:50,640
particular image

23
00:00:50,640 --> 00:00:52,800
these days space recognition technology

24
00:00:52,800 --> 00:00:54,640
mostly uses deep learning

25
00:00:54,640 --> 00:00:56,800
and because of modern compute power and

26
00:00:56,800 --> 00:00:58,559
sort of the rise of larger and bigger

27
00:00:58,559 --> 00:01:00,480
deep models

28
00:01:00,480 --> 00:01:02,559
face recognition has gotten very very

29
00:01:02,559 --> 00:01:03,760
strong it's been very

30
00:01:03,760 --> 00:01:06,400
uh performant in the last couple years

31
00:01:06,400 --> 00:01:08,000
to the point that it poses

32
00:01:08,000 --> 00:01:11,920
a threat to individual privacy right um

33
00:01:11,920 --> 00:01:13,200
if

34
00:01:13,200 --> 00:01:15,360
people can identify you from just photos

35
00:01:15,360 --> 00:01:16,960
they take of you on the street

36
00:01:16,960 --> 00:01:18,960
any stranger could look you up and you

37
00:01:18,960 --> 00:01:20,320
can see why this can be a problem for

38
00:01:20,320 --> 00:01:22,560
your personal privacy it can enable

39
00:01:22,560 --> 00:01:24,880
you know stalking behavior it can be

40
00:01:24,880 --> 00:01:26,720
used it can be abused

41
00:01:26,720 --> 00:01:29,520
by law enforcement in criminal cases or

42
00:01:29,520 --> 00:01:30,560
by the government

43
00:01:30,560 --> 00:01:32,479
to crack them in descent so there's a

44
00:01:32,479 --> 00:01:34,400
lot of reasons why

45
00:01:34,400 --> 00:01:36,560
we want to be careful about face

46
00:01:36,560 --> 00:01:38,079
recognition technology

47
00:01:38,079 --> 00:01:39,920
and why we might want to find ways to

48
00:01:39,920 --> 00:01:41,920
protect our own privacy

49
00:01:41,920 --> 00:01:44,479
um against this right there are a lot of

50
00:01:44,479 --> 00:01:46,079
news reports recently about

51
00:01:46,079 --> 00:01:48,479
the subject about why it's important to

52
00:01:48,479 --> 00:01:50,240
protect one's personal privacy

53
00:01:50,240 --> 00:01:52,479
um against technology like base

54
00:01:52,479 --> 00:01:54,000
recognition technology so hopefully

55
00:01:54,000 --> 00:01:57,759
that statement is not so controversial

56
00:01:58,719 --> 00:02:01,119
so basically the way that this

57
00:02:01,119 --> 00:02:03,119
technology works and very brief

58
00:02:03,119 --> 00:02:06,000
is you have some query photo of a person

59
00:02:06,000 --> 00:02:08,160
you're trying to identify

60
00:02:08,160 --> 00:02:09,679
you run them through a face recognition

61
00:02:09,679 --> 00:02:11,760
model which then produces

62
00:02:11,760 --> 00:02:15,120
an embedding in n-dimensional space

63
00:02:15,120 --> 00:02:18,239
and this embedding has the property that

64
00:02:18,239 --> 00:02:20,840
photos of the same person tend to

65
00:02:20,840 --> 00:02:22,239
cluster

66
00:02:22,239 --> 00:02:25,599
far away from photos of other people so

67
00:02:25,599 --> 00:02:27,520
photos of the same person cluster into

68
00:02:27,520 --> 00:02:29,040
distinct clusters in in-dimensional

69
00:02:29,040 --> 00:02:29,920
space

70
00:02:29,920 --> 00:02:32,800
this embedding space is produced by a

71
00:02:32,800 --> 00:02:34,160
deep neural network

72
00:02:34,160 --> 00:02:35,519
and then a k-nearest neighbor's

73
00:02:35,519 --> 00:02:38,160
classifier is usually run on top of this

74
00:02:38,160 --> 00:02:38,959
photo

75
00:02:38,959 --> 00:02:41,280
to determine given the query photo which

76
00:02:41,280 --> 00:02:42,560
is the stark blue photo

77
00:02:42,560 --> 00:02:44,080
and the look up set photos which is all

78
00:02:44,080 --> 00:02:45,280
these other photos in the embedding

79
00:02:45,280 --> 00:02:46,800
space

80
00:02:46,800 --> 00:02:48,800
who is most likely to be depicted in

81
00:02:48,800 --> 00:02:50,959
this photo

82
00:02:50,959 --> 00:02:53,840
so you can see that a casino classifier

83
00:02:53,840 --> 00:02:57,040
in this case is very accurate

84
00:02:57,200 --> 00:02:59,360
um so we propose to use adversarial

85
00:02:59,360 --> 00:03:00,879
examples which are

86
00:03:00,879 --> 00:03:03,200
small perturbations and images that sort

87
00:03:03,200 --> 00:03:05,200
of fool deep learning models

88
00:03:05,200 --> 00:03:08,319
um to attack face recognition systems

89
00:03:08,319 --> 00:03:10,400
um in order to get there we need to

90
00:03:10,400 --> 00:03:12,000
understand the goals of our

91
00:03:12,000 --> 00:03:14,720
scheme of using adversarial examples the

92
00:03:14,720 --> 00:03:16,239
first of which is privacy so obviously

93
00:03:16,239 --> 00:03:17,680
we want to fool a face recognition

94
00:03:17,680 --> 00:03:18,480
system

95
00:03:18,480 --> 00:03:20,080
we want to make sure that that system

96
00:03:20,080 --> 00:03:21,920
cannot identify us

97
00:03:21,920 --> 00:03:25,440
in our query photos um

98
00:03:25,440 --> 00:03:28,159
two the goal is to not actually degrade

99
00:03:28,159 --> 00:03:29,680
the quality of the photos that we are

100
00:03:29,680 --> 00:03:32,239
changing so if we modify a photo of you

101
00:03:32,239 --> 00:03:35,040
that you put on social media or that you

102
00:03:35,040 --> 00:03:36,400
have online of yourself

103
00:03:36,400 --> 00:03:37,760
you want to make sure you're still happy

104
00:03:37,760 --> 00:03:39,120
with that photo and that photo is still

105
00:03:39,120 --> 00:03:41,599
useful to you as a human being

106
00:03:41,599 --> 00:03:44,000
and finally we want to make sure that

107
00:03:44,000 --> 00:03:45,599
it's not immediately obvious to an

108
00:03:45,599 --> 00:03:47,280
automated detection system

109
00:03:47,280 --> 00:03:49,760
that this photo is some adversarially

110
00:03:49,760 --> 00:03:50,720
modified photo

111
00:03:50,720 --> 00:03:53,760
right it's it's picked up by a face

112
00:03:53,760 --> 00:03:55,040
recognition company

113
00:03:55,040 --> 00:03:56,640
and put in their database without them

114
00:03:56,640 --> 00:03:58,480
knowing

115
00:03:58,480 --> 00:04:01,360
um so there's a lot of actually related

116
00:04:01,360 --> 00:04:01,840
work

117
00:04:01,840 --> 00:04:03,680
on this subject about using adversarial

118
00:04:03,680 --> 00:04:05,200
examples to

119
00:04:05,200 --> 00:04:07,439
attack face recognition systems uh

120
00:04:07,439 --> 00:04:09,439
sharif at all talk about

121
00:04:09,439 --> 00:04:12,319
um using physical adversarial examples

122
00:04:12,319 --> 00:04:12,879
um

123
00:04:12,879 --> 00:04:15,760
to protect one's privacy um using and

124
00:04:15,760 --> 00:04:16,079
those

125
00:04:16,079 --> 00:04:17,440
adversarial examples are sort of in the

126
00:04:17,440 --> 00:04:19,358
form of glasses uh

127
00:04:19,358 --> 00:04:21,519
shannon all morrison paper proposed the

128
00:04:21,519 --> 00:04:23,040
system called fox

129
00:04:23,040 --> 00:04:25,360
which uh uses pixel level changes to

130
00:04:25,360 --> 00:04:26,960
sort of attack the training set

131
00:04:26,960 --> 00:04:29,840
using data poisoning so there are a lot

132
00:04:29,840 --> 00:04:30,639
of similar

133
00:04:30,639 --> 00:04:33,759
papers on this work and hopefully as we

134
00:04:33,759 --> 00:04:35,199
keep going through this the

135
00:04:35,199 --> 00:04:37,040
presentation we highlight what makes our

136
00:04:37,040 --> 00:04:39,680
paper unique

137
00:04:39,759 --> 00:04:41,120
and of course there are other papers

138
00:04:41,120 --> 00:04:42,880
that we don't mention on the slide for

139
00:04:42,880 --> 00:04:44,080
the full list of related work i

140
00:04:44,080 --> 00:04:47,040
encourage you to see our paper

141
00:04:47,040 --> 00:04:50,080
so to talk about these kinds of privacy

142
00:04:50,080 --> 00:04:51,520
protecting schemes

143
00:04:51,520 --> 00:04:54,639
we need a way to determine okay how good

144
00:04:54,639 --> 00:04:56,800
numerically can we quantify the

145
00:04:56,800 --> 00:04:58,080
performance of a privacy predicting

146
00:04:58,080 --> 00:04:59,040
scheme how good is it

147
00:04:59,040 --> 00:05:01,199
at actually protecting our privacy so we

148
00:05:01,199 --> 00:05:02,880
introduce three different metrics to

149
00:05:02,880 --> 00:05:04,800
sort of quantify this

150
00:05:04,800 --> 00:05:07,440
uh the first is the recall rate so if we

151
00:05:07,440 --> 00:05:09,199
have some query photo which we embed and

152
00:05:09,199 --> 00:05:10,080
we

153
00:05:10,080 --> 00:05:12,720
use the deep learning network and we get

154
00:05:12,720 --> 00:05:14,960
a hundred photo closest photos to

155
00:05:14,960 --> 00:05:18,400
my query photo the recall rate asks

156
00:05:18,400 --> 00:05:21,039
how many of those photos are of the

157
00:05:21,039 --> 00:05:22,080
query photo

158
00:05:22,080 --> 00:05:24,160
so if you take a photo of me and you run

159
00:05:24,160 --> 00:05:25,600
it on your face recognition model and

160
00:05:25,600 --> 00:05:27,440
you get the top hundred matches

161
00:05:27,440 --> 00:05:29,280
the recall rate asks what percentage of

162
00:05:29,280 --> 00:05:30,479
those top hundred matches

163
00:05:30,479 --> 00:05:33,680
are me so if i appear in 70

164
00:05:33,680 --> 00:05:36,880
of those photos the recall rate is 0.7

165
00:05:36,880 --> 00:05:39,680
the discovery rate is a stricter metric

166
00:05:39,680 --> 00:05:40,800
than the recall rate

167
00:05:40,800 --> 00:05:44,000
and it asks how often do i appear

168
00:05:44,000 --> 00:05:47,759
at least once in the top k uh

169
00:05:47,759 --> 00:05:50,160
look uh lookup set so if you take a

170
00:05:50,160 --> 00:05:51,440
query photo of me and you're running

171
00:05:51,440 --> 00:05:52,000
against your

172
00:05:52,000 --> 00:05:54,400
database search and on average with

173
00:05:54,400 --> 00:05:56,479
different query photos of me

174
00:05:56,479 --> 00:06:00,000
i appear in the top case set 50 or

175
00:06:00,000 --> 00:06:03,120
60 or 90 of the time at least once even

176
00:06:03,120 --> 00:06:04,000
once in

177
00:06:04,000 --> 00:06:07,039
the top k nearest neighbors that's the

178
00:06:07,039 --> 00:06:08,960
discovery rate so it's a stricter metric

179
00:06:08,960 --> 00:06:10,880
strictly stricter

180
00:06:10,880 --> 00:06:14,240
strictly larger than the recoil rate

181
00:06:14,240 --> 00:06:17,199
and finally we have identity uniformity

182
00:06:17,199 --> 00:06:17,919
which is

183
00:06:17,919 --> 00:06:19,520
how many different identity which

184
00:06:19,520 --> 00:06:21,039
measures sort of the entropy of the

185
00:06:21,039 --> 00:06:22,160
recall set

186
00:06:22,160 --> 00:06:25,360
so intuitively we want

187
00:06:25,360 --> 00:06:28,400
uh a lot of different people

188
00:06:28,400 --> 00:06:30,319
to be in the nearest neighbor set of our

189
00:06:30,319 --> 00:06:31,759
query photo right if we have one query

190
00:06:31,759 --> 00:06:32,800
photo of me

191
00:06:32,800 --> 00:06:33,840
you want to make sure it's hard to

192
00:06:33,840 --> 00:06:36,000
identify me in the lookup set so you

193
00:06:36,000 --> 00:06:37,600
want lots of other people

194
00:06:37,600 --> 00:06:40,720
um in in this in the lookup set

195
00:06:40,720 --> 00:06:42,639
and the enemy uniformity is defined such

196
00:06:42,639 --> 00:06:44,000
that it's zero

197
00:06:44,000 --> 00:06:47,919
when there's perfect entropy so there's

198
00:06:47,919 --> 00:06:48,560
uh there's

199
00:06:48,560 --> 00:06:50,400
every person in the lookup set is a

200
00:06:50,400 --> 00:06:51,759
different human being

201
00:06:51,759 --> 00:06:55,360
and it's one when uh i'm the only person

202
00:06:55,360 --> 00:06:57,680
in my own lookup set for a given query

203
00:06:57,680 --> 00:06:59,199
photo

204
00:06:59,199 --> 00:07:00,560
so for all of these metrics it's

205
00:07:00,560 --> 00:07:01,919
important to realize that they're all

206
00:07:01,919 --> 00:07:03,280
between zero and one

207
00:07:03,280 --> 00:07:06,080
and the lower is better so all of these

208
00:07:06,080 --> 00:07:07,440
the goal of this privacy protecting

209
00:07:07,440 --> 00:07:08,080
scheme

210
00:07:08,080 --> 00:07:10,400
is to get these as close to zero as as

211
00:07:10,400 --> 00:07:12,880
possible

212
00:07:13,360 --> 00:07:16,560
okay so our work

213
00:07:16,560 --> 00:07:19,680
focuses on two different paradigms of

214
00:07:19,680 --> 00:07:20,800
defending

215
00:07:20,800 --> 00:07:22,960
uh one's privacy in this sort of

216
00:07:22,960 --> 00:07:24,720
embedding space

217
00:07:24,720 --> 00:07:26,720
uh the first is what we call the

218
00:07:26,720 --> 00:07:28,319
individual approach

219
00:07:28,319 --> 00:07:30,479
so let's say you're a person and you

220
00:07:30,479 --> 00:07:32,000
want to protect your privacy

221
00:07:32,000 --> 00:07:35,360
and you know that this face recognition

222
00:07:35,360 --> 00:07:36,319
company

223
00:07:36,319 --> 00:07:38,960
has access to you know these five photos

224
00:07:38,960 --> 00:07:40,400
of you which appear normally in the

225
00:07:40,400 --> 00:07:42,319
center

226
00:07:42,319 --> 00:07:45,440
uh what do you do well if you can modify

227
00:07:45,440 --> 00:07:47,360
those photos of yourself

228
00:07:47,360 --> 00:07:48,960
the best thing to do the most obvious

229
00:07:48,960 --> 00:07:51,919
thing to do is sort of scatter

230
00:07:51,919 --> 00:07:54,560
your photos as far away from the

231
00:07:54,560 --> 00:07:56,560
original location as possible

232
00:07:56,560 --> 00:07:58,319
because when you have a new photo this

233
00:07:58,319 --> 00:07:59,919
new query photo

234
00:07:59,919 --> 00:08:01,680
it's gonna appear close to your where

235
00:08:01,680 --> 00:08:04,240
your old photos used to be

236
00:08:04,240 --> 00:08:06,639
and since you've moved your old photos

237
00:08:06,639 --> 00:08:07,759
hopefully

238
00:08:07,759 --> 00:08:09,280
photos of different people will be the

239
00:08:09,280 --> 00:08:10,720
nearest neighbors as opposed to the

240
00:08:10,720 --> 00:08:12,160
original focus right that's the most

241
00:08:12,160 --> 00:08:13,599
intuitive thing to do

242
00:08:13,599 --> 00:08:15,680
and that's that's actually the proposal

243
00:08:15,680 --> 00:08:17,199
of many previous works is to

244
00:08:17,199 --> 00:08:20,879
talk about this line of thinking um

245
00:08:20,879 --> 00:08:24,479
the issue with this approach is that

246
00:08:24,479 --> 00:08:26,639
you don't have access to all the photos

247
00:08:26,639 --> 00:08:28,400
that a company might have scraped of you

248
00:08:28,400 --> 00:08:29,360
right so

249
00:08:29,360 --> 00:08:31,120
if you're thinking about this problem in

250
00:08:31,120 --> 00:08:32,479
2021

251
00:08:32,479 --> 00:08:35,839
um it's likely in fact highly likely

252
00:08:35,839 --> 00:08:37,360
that a face recognition company has

253
00:08:37,360 --> 00:08:40,000
already scraped several photos from your

254
00:08:40,000 --> 00:08:41,760
social media and has them erasing their

255
00:08:41,760 --> 00:08:42,640
database

256
00:08:42,640 --> 00:08:44,080
and you have no way to modify those

257
00:08:44,080 --> 00:08:46,000
photos so even if you modify all future

258
00:08:46,000 --> 00:08:47,440
photos that you put out

259
00:08:47,440 --> 00:08:50,160
in in on social media there are already

260
00:08:50,160 --> 00:08:51,680
some immovable points

261
00:08:51,680 --> 00:08:53,600
that you just can't change and that's

262
00:08:53,600 --> 00:08:55,279
going to make this problem harder

263
00:08:55,279 --> 00:08:57,839
right so we do a quick study on this we

264
00:08:57,839 --> 00:08:59,600
talk more about this in the paper

265
00:08:59,600 --> 00:09:02,560
but we find that if you can modify every

266
00:09:02,560 --> 00:09:04,720
photo of yourself in the database

267
00:09:04,720 --> 00:09:06,640
then you're golden then it's very easy

268
00:09:06,640 --> 00:09:10,240
to drive all metrics very close to zero

269
00:09:10,240 --> 00:09:12,480
problem solved you know without with no

270
00:09:12,480 --> 00:09:14,480
problem but the issue is that

271
00:09:14,480 --> 00:09:17,839
if you have even you know a small number

272
00:09:17,839 --> 00:09:18,560
of photos

273
00:09:18,560 --> 00:09:20,720
lingering behind that you can't modify

274
00:09:20,720 --> 00:09:22,480
for example from the past

275
00:09:22,480 --> 00:09:24,080
clean photos that a company has already

276
00:09:24,080 --> 00:09:26,080
scraped of you then

277
00:09:26,080 --> 00:09:28,000
this problem becomes much much harder

278
00:09:28,000 --> 00:09:30,800
and these individual approaches

279
00:09:30,800 --> 00:09:32,560
become less and less successful the more

280
00:09:32,560 --> 00:09:34,480
photos they have so we need to think

281
00:09:34,480 --> 00:09:35,760
about this problem in a slightly

282
00:09:35,760 --> 00:09:37,200
different way with a slightly different

283
00:09:37,200 --> 00:09:38,080
tack

284
00:09:38,080 --> 00:09:40,640
and that's the main thrust of our work

285
00:09:40,640 --> 00:09:42,000
today

286
00:09:42,000 --> 00:09:46,160
so the goal is to do this on a more

287
00:09:46,160 --> 00:09:49,760
community level to think about how

288
00:09:49,760 --> 00:09:52,160
we can protect each other using a large

289
00:09:52,160 --> 00:09:53,920
number of decoys as opposed to just sort

290
00:09:53,920 --> 00:09:54,560
of

291
00:09:54,560 --> 00:09:56,560
only focusing on our individual privacy

292
00:09:56,560 --> 00:09:57,839
and moving our own photos

293
00:09:57,839 --> 00:09:59,760
how we can sort of flood facial face

294
00:09:59,760 --> 00:10:00,880
recognition systems

295
00:10:00,880 --> 00:10:04,640
with lots of decoys in the internet

296
00:10:04,640 --> 00:10:07,200
so we have some look up set again an

297
00:10:07,200 --> 00:10:08,560
embedding space

298
00:10:08,560 --> 00:10:10,720
and we have some particular query photo

299
00:10:10,720 --> 00:10:12,079
um

300
00:10:12,079 --> 00:10:15,360
and the goal here is well normally

301
00:10:15,360 --> 00:10:17,040
without any intervention

302
00:10:17,040 --> 00:10:19,680
right the top five nearest neighbors i

303
00:10:19,680 --> 00:10:21,120
didn't identify us perfectly

304
00:10:21,120 --> 00:10:22,399
so we want to make sure that the top

305
00:10:22,399 --> 00:10:23,920
five nearest neighbors are not these

306
00:10:23,920 --> 00:10:26,079
five photos

307
00:10:26,079 --> 00:10:28,720
ideally they would be decoy photos

308
00:10:28,720 --> 00:10:29,600
ideally

309
00:10:29,600 --> 00:10:32,320
we could find some way to perturb other

310
00:10:32,320 --> 00:10:33,120
photos

311
00:10:33,120 --> 00:10:35,600
of other people or or celebrities or

312
00:10:35,600 --> 00:10:36,720
whatever

313
00:10:36,720 --> 00:10:40,000
to be closer to the average query photo

314
00:10:40,000 --> 00:10:42,000
than the rest of our our look up set

315
00:10:42,000 --> 00:10:43,120
photos

316
00:10:43,120 --> 00:10:45,440
right so this is the key idea of the

317
00:10:45,440 --> 00:10:46,480
foggy site

318
00:10:46,480 --> 00:10:49,839
uh scheme is to use lots of decoys

319
00:10:49,839 --> 00:10:53,120
to predict where nuclear photos will be

320
00:10:53,120 --> 00:10:54,640
such that the nearest neighbors of new

321
00:10:54,640 --> 00:10:56,560
query photos are going to be

322
00:10:56,560 --> 00:10:59,279
people who are not us who can't be used

323
00:10:59,279 --> 00:11:00,880
to link back to us who can't be used on

324
00:11:00,880 --> 00:11:03,439
nfis

325
00:11:03,519 --> 00:11:04,880
so there are a couple of different ways

326
00:11:04,880 --> 00:11:07,680
to sort of take decoy photos and move

327
00:11:07,680 --> 00:11:09,440
them into our space

328
00:11:09,440 --> 00:11:11,760
from a mathematical perspective we talk

329
00:11:11,760 --> 00:11:13,040
about four different approaches in the

330
00:11:13,040 --> 00:11:14,720
paper which i'm not going to go into a

331
00:11:14,720 --> 00:11:16,320
ton of detail here

332
00:11:16,320 --> 00:11:18,399
but they're listed here and we do

333
00:11:18,399 --> 00:11:20,240
compare against different methods of

334
00:11:20,240 --> 00:11:21,600
doing this

335
00:11:21,600 --> 00:11:23,519
although you know further investigation

336
00:11:23,519 --> 00:11:25,279
into this could constitute interesting

337
00:11:25,279 --> 00:11:25,760
for

338
00:11:25,760 --> 00:11:29,279
future work so

339
00:11:29,279 --> 00:11:31,680
given some decoy we can see the decoy

340
00:11:31,680 --> 00:11:32,640
photos with different levels of

341
00:11:32,640 --> 00:11:34,720
modification here

342
00:11:34,720 --> 00:11:37,839
it turns out that we do fairly well

343
00:11:37,839 --> 00:11:40,880
at driving metrics low as long as we can

344
00:11:40,880 --> 00:11:42,240
have a lot of decoys

345
00:11:42,240 --> 00:11:45,360
so we compare against uh the

346
00:11:45,360 --> 00:11:46,720
perturbation amounts of the

347
00:11:46,720 --> 00:11:48,959
the magnitude of the adversarial example

348
00:11:48,959 --> 00:11:49,920
on the

349
00:11:49,920 --> 00:11:53,040
on the x-axis and the mean recall

350
00:11:53,040 --> 00:11:54,320
which is one of our metrics on the

351
00:11:54,320 --> 00:11:56,560
y-axis and we show that as long as we

352
00:11:56,560 --> 00:11:58,399
perturb the image is enough which is

353
00:11:58,399 --> 00:11:58,880
still

354
00:11:58,880 --> 00:12:01,680
not visible to humans with a certain

355
00:12:01,680 --> 00:12:03,200
perturbation we can drive this recall

356
00:12:03,200 --> 00:12:03,920
rate very low

357
00:12:03,920 --> 00:12:07,120
which is great which is a success

358
00:12:07,120 --> 00:12:08,800
we do the same with other ways of

359
00:12:08,800 --> 00:12:10,480
targeting the lookup set

360
00:12:10,480 --> 00:12:12,959
the mean as opposed to an arbitrary

361
00:12:12,959 --> 00:12:15,200
vector

362
00:12:15,200 --> 00:12:16,720
and this seems like a success which is

363
00:12:16,720 --> 00:12:18,560
great um

364
00:12:18,560 --> 00:12:21,839
but one thing which is even more

365
00:12:21,839 --> 00:12:23,120
important question

366
00:12:23,120 --> 00:12:25,519
which is how many decoys do we actually

367
00:12:25,519 --> 00:12:26,720
need so

368
00:12:26,720 --> 00:12:30,240
for some fixed epsilon uh how many more

369
00:12:30,240 --> 00:12:32,000
decoys do we need

370
00:12:32,000 --> 00:12:36,160
than your average look up set to

371
00:12:36,160 --> 00:12:39,040
to cloak you to protect you and it turns

372
00:12:39,040 --> 00:12:39,600
out that

373
00:12:39,600 --> 00:12:41,519
depending on the epsilon depending on

374
00:12:41,519 --> 00:12:43,200
the model um

375
00:12:43,200 --> 00:12:46,320
you may need anywhere from you know two

376
00:12:46,320 --> 00:12:47,440
to five

377
00:12:47,440 --> 00:12:50,639
times as many decoys relative to look up

378
00:12:50,639 --> 00:12:51,200
photos

379
00:12:51,200 --> 00:12:54,480
to protect yourself which means

380
00:12:54,480 --> 00:12:57,040
let's say for example a company has

381
00:12:57,040 --> 00:12:58,399
scraped maybe five

382
00:12:58,399 --> 00:13:00,720
photos of you you would need upwards of

383
00:13:00,720 --> 00:13:02,959
10 to 25 photos

384
00:13:02,959 --> 00:13:06,399
to sort of decoy away from you

385
00:13:06,399 --> 00:13:08,399
to protect your privacy which sounds

386
00:13:08,399 --> 00:13:09,600
like a fair bit

387
00:13:09,600 --> 00:13:12,320
um and it is because this is a hard

388
00:13:12,320 --> 00:13:13,200
problem

389
00:13:13,200 --> 00:13:14,959
and that's why we're sort of talking

390
00:13:14,959 --> 00:13:16,399
about this at the community level rather

391
00:13:16,399 --> 00:13:19,040
than the individual level

392
00:13:19,040 --> 00:13:20,160
i'm going to blow through one more

393
00:13:20,160 --> 00:13:22,079
result or two more results because i

394
00:13:22,079 --> 00:13:22,800
want to

395
00:13:22,800 --> 00:13:26,079
get through the slides um here are the

396
00:13:26,079 --> 00:13:27,440
other metrics as

397
00:13:27,440 --> 00:13:30,639
a as a with the same x-axis as last time

398
00:13:30,639 --> 00:13:32,240
the proportion of decoys relative to

399
00:13:32,240 --> 00:13:34,800
clean look-up photos

400
00:13:34,800 --> 00:13:36,240
and it's important to note that in this

401
00:13:36,240 --> 00:13:38,000
slide and in the previous slide

402
00:13:38,000 --> 00:13:39,920
this assumes a white box model so this

403
00:13:39,920 --> 00:13:41,680
assumes that

404
00:13:41,680 --> 00:13:44,399
in some scenario we know which face

405
00:13:44,399 --> 00:13:46,000
recognition model the company we're

406
00:13:46,000 --> 00:13:48,480
trying to protect against is using

407
00:13:48,480 --> 00:13:50,240
this is not an entirely unreasonable

408
00:13:50,240 --> 00:13:52,800
assumption because

409
00:13:52,800 --> 00:13:54,399
you know these face recognition models

410
00:13:54,399 --> 00:13:56,880
are usually public in github

411
00:13:56,880 --> 00:13:58,320
and even if they're proprietary by the

412
00:13:58,320 --> 00:14:00,880
company the architecture that's used is

413
00:14:00,880 --> 00:14:02,320
usually from an academic paper

414
00:14:02,320 --> 00:14:06,079
which again is public in an archive

415
00:14:06,079 --> 00:14:08,320
but we also do talk about the white box

416
00:14:08,320 --> 00:14:09,760
setting

417
00:14:09,760 --> 00:14:12,000
there are some techniques to modify

418
00:14:12,000 --> 00:14:13,600
images

419
00:14:13,600 --> 00:14:15,199
even without knowing what model you're

420
00:14:15,199 --> 00:14:17,279
attacking

421
00:14:17,279 --> 00:14:19,199
and we use those techniques but we find

422
00:14:19,199 --> 00:14:21,120
that in the black box setting where you

423
00:14:21,120 --> 00:14:22,240
don't have access to the

424
00:14:22,240 --> 00:14:23,839
proprietary model that a company is

425
00:14:23,839 --> 00:14:27,040
using this problem becomes much harder

426
00:14:27,040 --> 00:14:30,320
so it's still possible to protect your

427
00:14:30,320 --> 00:14:31,279
own privacy

428
00:14:31,279 --> 00:14:33,120
but instead of needing two to five times

429
00:14:33,120 --> 00:14:34,320
as many decoys

430
00:14:34,320 --> 00:14:36,480
you might need a as as many as seven

431
00:14:36,480 --> 00:14:38,079
times as many decoys as clean look up

432
00:14:38,079 --> 00:14:39,040
photos

433
00:14:39,040 --> 00:14:40,480
and you need to make your perturbation

434
00:14:40,480 --> 00:14:42,079
magnitude much much higher so you need

435
00:14:42,079 --> 00:14:42,639
to

436
00:14:42,639 --> 00:14:45,760
visibly alter the image a lot more

437
00:14:45,760 --> 00:14:47,519
and this just speaks to the difficulty

438
00:14:47,519 --> 00:14:49,680
of this problem this shows that

439
00:14:49,680 --> 00:14:53,120
if a company already has photos of you

440
00:14:53,120 --> 00:14:54,480
that are clean especially a lot of

441
00:14:54,480 --> 00:14:55,760
photos

442
00:14:55,760 --> 00:14:58,480
and if that company takes great pains to

443
00:14:58,480 --> 00:15:00,399
keep its model private

444
00:15:00,399 --> 00:15:02,240
then protecting your own individual

445
00:15:02,240 --> 00:15:04,000
privacy computationally

446
00:15:04,000 --> 00:15:05,920
especially if your computational

447
00:15:05,920 --> 00:15:07,360
resources are limited

448
00:15:07,360 --> 00:15:10,560
is a very very hard problem um and our

449
00:15:10,560 --> 00:15:12,800
results show that while it's possible

450
00:15:12,800 --> 00:15:14,560
it's difficult it's difficult and we

451
00:15:14,560 --> 00:15:15,920
don't want to overstate

452
00:15:15,920 --> 00:15:17,600
the difficulty or understate the

453
00:15:17,600 --> 00:15:18,720
difficulty of protecting your own

454
00:15:18,720 --> 00:15:21,199
privacy

455
00:15:21,199 --> 00:15:24,480
um so that's sort of an overview of the

456
00:15:24,480 --> 00:15:27,040
foggy site system it uses adversarial

457
00:15:27,040 --> 00:15:29,440
examples to protect against

458
00:15:29,440 --> 00:15:31,600
face recognition technology and the

459
00:15:31,600 --> 00:15:32,959
novelty really

460
00:15:32,959 --> 00:15:35,440
is in the way it frames the problem so

461
00:15:35,440 --> 00:15:36,639
previous approaches

462
00:15:36,639 --> 00:15:38,079
really focus on this problem as

463
00:15:38,079 --> 00:15:40,959
protecting your own individual privacy

464
00:15:40,959 --> 00:15:43,199
but we find in our work that this speak

465
00:15:43,199 --> 00:15:44,800
is a very difficult thing

466
00:15:44,800 --> 00:15:48,639
because if if a company already has

467
00:15:48,639 --> 00:15:52,160
clean photos of you in their database

468
00:15:52,160 --> 00:15:54,399
then it's almost impossible to defend

469
00:15:54,399 --> 00:15:55,279
against

470
00:15:55,279 --> 00:15:57,680
any face recognition system uh on your

471
00:15:57,680 --> 00:15:58,399
own

472
00:15:58,399 --> 00:16:00,240
but we show that with the appropriate

473
00:16:00,240 --> 00:16:02,240
sort of community level approach

474
00:16:02,240 --> 00:16:04,320
it is possible actually to defend

475
00:16:04,320 --> 00:16:06,160
against your own privacy

476
00:16:06,160 --> 00:16:07,759
as long as you use the right targeting

477
00:16:07,759 --> 00:16:09,600
strategies and especially

478
00:16:09,600 --> 00:16:11,519
in the white box setting where for

479
00:16:11,519 --> 00:16:14,240
whatever litigation or publicly

480
00:16:14,240 --> 00:16:16,880
data access reason the company's model

481
00:16:16,880 --> 00:16:18,160
is is available

482
00:16:18,160 --> 00:16:21,279
um so that's sort of a quick overview of

483
00:16:21,279 --> 00:16:22,639
the fog state system i would encourage

484
00:16:22,639 --> 00:16:24,240
you to ask questions now and see

485
00:16:24,240 --> 00:16:25,519
original paper

486
00:16:25,519 --> 00:16:27,600
and also reach out for further questions

487
00:16:27,600 --> 00:16:29,440
um after the talk

488
00:16:29,440 --> 00:16:31,600
um you can email us and connect us on

489
00:16:31,600 --> 00:16:32,800
twitter

490
00:16:32,800 --> 00:16:34,639
all right thanks for talking and i'll

491
00:16:34,639 --> 00:16:37,839
see you in the q a session

