1
00:00:01,120 --> 00:00:04,319
hi everyone i'm wen xiaowong

2
00:00:04,319 --> 00:00:07,520
i'm here to present deeply's boosting

3
00:00:07,520 --> 00:00:09,920
utility of differentially private deep

4
00:00:09,920 --> 00:00:10,880
learning

5
00:00:10,880 --> 00:00:14,400
while randomized smoothing this is a

6
00:00:14,400 --> 00:00:15,200
drone work

7
00:00:15,200 --> 00:00:18,320
with tian hong lun nan ching

8
00:00:18,320 --> 00:00:22,240
pan tang and roshi

9
00:00:24,640 --> 00:00:27,199
first i will preview some background

10
00:00:27,199 --> 00:00:29,519
regarding deep learning

11
00:00:29,519 --> 00:00:31,840
as we know it has achieved quite some

12
00:00:31,840 --> 00:00:33,680
results

13
00:00:33,680 --> 00:00:36,960
in deep learning we use neural networks

14
00:00:36,960 --> 00:00:40,320
to be the parametric models which

15
00:00:40,320 --> 00:00:43,680
can be considered as compositions

16
00:00:43,680 --> 00:00:47,120
of parametric functions

17
00:00:47,120 --> 00:00:49,440
the parameters are learned through

18
00:00:49,440 --> 00:00:52,320
solving an organization problem

19
00:00:52,320 --> 00:00:55,360
for which the organization objectives

20
00:00:55,360 --> 00:00:58,640
or learning loss functions are usually

21
00:00:58,640 --> 00:01:01,840
the average of per sample loss over all

22
00:01:01,840 --> 00:01:04,559
training data

23
00:01:04,559 --> 00:01:07,119
an organization is typically solved

24
00:01:07,119 --> 00:01:08,640
using sgd

25
00:01:08,640 --> 00:01:12,000
stochastic gradient descent each

26
00:01:12,000 --> 00:01:15,439
iteration of sgd basically samples

27
00:01:15,439 --> 00:01:18,560
a subset of data and

28
00:01:18,560 --> 00:01:22,000
uses the subset to approximate the

29
00:01:22,000 --> 00:01:24,560
gradient of the loss function

30
00:01:24,560 --> 00:01:27,600
which is then used as the direction to

31
00:01:27,600 --> 00:01:30,640
update parameters

32
00:01:31,920 --> 00:01:35,520
well machine learning is about obtaining

33
00:01:35,520 --> 00:01:37,600
utility from data

34
00:01:37,600 --> 00:01:40,640
so it is natural to risk concerns

35
00:01:40,640 --> 00:01:43,439
regarding if the data are properly

36
00:01:43,439 --> 00:01:46,079
protected

37
00:01:46,399 --> 00:01:49,840
here are two types of privacy attacks

38
00:01:49,840 --> 00:01:53,200
one is membership inference attacks

39
00:01:53,200 --> 00:01:56,399
which is to determine whether a given

40
00:01:56,399 --> 00:01:57,360
individual's

41
00:01:57,360 --> 00:02:00,479
record is included in the training set

42
00:02:00,479 --> 00:02:03,520
of the target model

43
00:02:03,520 --> 00:02:07,280
another one is model inversion attacks

44
00:02:07,280 --> 00:02:10,080
which is to reconstruct sensitive

45
00:02:10,080 --> 00:02:10,959
features

46
00:02:10,959 --> 00:02:14,640
of the training data records

47
00:02:15,360 --> 00:02:17,920
the privacy attacks has been shown

48
00:02:17,920 --> 00:02:18,800
possible

49
00:02:18,800 --> 00:02:21,840
in many cases

50
00:02:21,840 --> 00:02:24,400
actually motivates the development of

51
00:02:24,400 --> 00:02:28,160
differentially private deep learning

52
00:02:30,800 --> 00:02:32,959
roughly speaking the notion of

53
00:02:32,959 --> 00:02:34,160
differential privacy

54
00:02:34,160 --> 00:02:36,640
says that for a function that preserves

55
00:02:36,640 --> 00:02:37,920
privacy

56
00:02:37,920 --> 00:02:40,959
its output distribution should not

57
00:02:40,959 --> 00:02:41,920
change much

58
00:02:41,920 --> 00:02:44,560
whenever only one record of its input

59
00:02:44,560 --> 00:02:47,519
data changes

60
00:02:49,720 --> 00:02:53,040
dpsgd is an approach

61
00:02:53,040 --> 00:02:56,400
to make sgd differentially private

62
00:02:56,400 --> 00:02:58,400
so we can have differentially private

63
00:02:58,400 --> 00:03:01,040
deep learning

64
00:03:01,040 --> 00:03:04,640
it basically introduced two extra steps

65
00:03:04,640 --> 00:03:08,959
into regular sgd gradient clipping

66
00:03:08,959 --> 00:03:11,920
and noising

67
00:03:12,080 --> 00:03:15,840
gradient clipping ensures that the scale

68
00:03:15,840 --> 00:03:18,480
of the data dependent components get

69
00:03:18,480 --> 00:03:20,480
bonded

70
00:03:20,480 --> 00:03:23,680
and in noising gaussian noise

71
00:03:23,680 --> 00:03:27,200
is mixed with the clipped gradient

72
00:03:27,200 --> 00:03:33,239
to ensure differential privacy

73
00:03:33,239 --> 00:03:36,879
dpsgd is widely applicable

74
00:03:36,879 --> 00:03:39,120
it offers guaranteed differential

75
00:03:39,120 --> 00:03:40,879
privacy

76
00:03:40,879 --> 00:03:44,840
but actually the privacy comes with a

77
00:03:44,840 --> 00:03:46,560
price because

78
00:03:46,560 --> 00:03:49,760
of the noise added to the gradient

79
00:03:49,760 --> 00:03:53,439
dpsgd typically leads to much lower

80
00:03:53,439 --> 00:03:57,360
model performance and different runs

81
00:03:57,360 --> 00:04:00,799
of dpsgd may result in models

82
00:04:00,799 --> 00:04:04,400
that performs differently well

83
00:04:04,400 --> 00:04:07,360
these are the two issues we target in

84
00:04:07,360 --> 00:04:09,360
this work

85
00:04:09,360 --> 00:04:11,840
now let's take a closer look through a

86
00:04:11,840 --> 00:04:15,840
toy example

87
00:04:16,880 --> 00:04:20,399
this is an organization in a 2d space

88
00:04:20,399 --> 00:04:23,919
the loss function is the mixture of two

89
00:04:23,919 --> 00:04:26,400
central symmetric functions

90
00:04:26,400 --> 00:04:29,520
so the lost surface contains

91
00:04:29,520 --> 00:04:32,800
two local minima both with

92
00:04:32,800 --> 00:04:36,240
near zero loss values one

93
00:04:36,240 --> 00:04:40,720
uh one is flat and one is sharp

94
00:04:40,720 --> 00:04:43,360
you can see the visualization on the

95
00:04:43,360 --> 00:04:45,919
right side

96
00:04:47,520 --> 00:04:50,479
now let's see what happens when we use

97
00:04:50,479 --> 00:04:51,520
dpsgd

98
00:04:51,520 --> 00:04:54,799
to solve this problem

99
00:04:54,880 --> 00:04:57,919
in the middle figure we plot the final

100
00:04:57,919 --> 00:04:58,880
parameters

101
00:04:58,880 --> 00:05:03,520
from 100 independent runs of dpsgd

102
00:05:03,520 --> 00:05:06,880
we can see that both the sha minima

103
00:05:06,880 --> 00:05:09,919
and the flat minima capture their share

104
00:05:09,919 --> 00:05:10,240
of

105
00:05:10,240 --> 00:05:13,919
outputs in their neighborhood however

106
00:05:13,919 --> 00:05:16,080
the sha minima is not quite noise

107
00:05:16,080 --> 00:05:17,199
tolerant

108
00:05:17,199 --> 00:05:19,919
the neighborhood around it offers worse

109
00:05:19,919 --> 00:05:22,320
less stable loss compared to the ones of

110
00:05:22,320 --> 00:05:25,280
the flat minima

111
00:05:25,280 --> 00:05:28,160
consequently we can see in the right

112
00:05:28,160 --> 00:05:29,520
right side that

113
00:05:29,520 --> 00:05:34,400
the loss division has a long tail

114
00:05:35,680 --> 00:05:38,400
now we see that the flat minimum can be

115
00:05:38,400 --> 00:05:39,680
preferred

116
00:05:39,680 --> 00:05:41,680
let's find out what will happen if we

117
00:05:41,680 --> 00:05:42,800
can smooth out

118
00:05:42,800 --> 00:05:46,639
the shaft minima assuming

119
00:05:46,639 --> 00:05:50,000
we apply some smoothing and we have a

120
00:05:50,000 --> 00:05:50,639
smooth

121
00:05:50,639 --> 00:05:54,160
loss landscapes as in the left side

122
00:05:54,160 --> 00:05:57,360
we can actually have the flat minima

123
00:05:57,360 --> 00:06:00,560
to capture the final parameters just as

124
00:06:00,560 --> 00:06:03,840
shown in the middle figure

125
00:06:04,160 --> 00:06:07,199
since the flat minima offers smaller and

126
00:06:07,199 --> 00:06:09,039
more stable loss values in its

127
00:06:09,039 --> 00:06:10,080
neighborhood

128
00:06:10,080 --> 00:06:13,120
the smoothing improves both the overall

129
00:06:13,120 --> 00:06:14,160
performance

130
00:06:14,160 --> 00:06:17,360
and the stability we can see that

131
00:06:17,360 --> 00:06:21,840
on the right

132
00:06:22,560 --> 00:06:25,360
okay let's get to how we apply the

133
00:06:25,360 --> 00:06:26,880
smoothing

134
00:06:26,880 --> 00:06:29,680
we utilize the randomized smoothing

135
00:06:29,680 --> 00:06:31,120
technique

136
00:06:31,120 --> 00:06:34,000
we simply convolve the original loss

137
00:06:34,000 --> 00:06:36,880
with a gaussian distribution

138
00:06:36,880 --> 00:06:40,240
and sigma smooth gear controls the

139
00:06:40,240 --> 00:06:40,880
strength of

140
00:06:40,880 --> 00:06:44,639
smoothing this technique is

141
00:06:44,639 --> 00:06:47,680
fairly simple but the question is

142
00:06:47,680 --> 00:06:51,919
how we can decide the smoothing strings

143
00:06:51,919 --> 00:06:55,039
here we introduce an alternative view

144
00:06:55,039 --> 00:06:58,240
of the smoothing recall that the

145
00:06:58,240 --> 00:07:02,160
output of dpsgd is intrinsically

146
00:07:02,160 --> 00:07:05,599
random because of the injecting noise

147
00:07:05,599 --> 00:07:08,960
but the original loss captures only the

148
00:07:08,960 --> 00:07:09,680
performance

149
00:07:09,680 --> 00:07:12,160
of a single point ignoring the

150
00:07:12,160 --> 00:07:14,240
randomness

151
00:07:14,240 --> 00:07:17,360
a reasonable idea is to

152
00:07:17,360 --> 00:07:20,720
instead minimize the expected loss

153
00:07:20,720 --> 00:07:24,080
over the corresponding population as we

154
00:07:24,080 --> 00:07:24,639
present

155
00:07:24,639 --> 00:07:28,720
here as loss average

156
00:07:29,599 --> 00:07:32,479
well comparing lost average with los

157
00:07:32,479 --> 00:07:33,440
mousse

158
00:07:33,440 --> 00:07:36,639
we can consider the gaussian noise f

159
00:07:36,639 --> 00:07:39,280
as a rough approximation of the

160
00:07:39,280 --> 00:07:42,639
corresponding population

161
00:07:42,639 --> 00:07:45,919
let motivates us to factorize sigma

162
00:07:45,919 --> 00:07:48,639
smooth as follows

163
00:07:48,639 --> 00:07:51,319
basically we consider it the

164
00:07:51,319 --> 00:07:52,479
multiplication

165
00:07:52,479 --> 00:07:55,680
of a parameter smoothing radius r and

166
00:07:55,680 --> 00:07:58,319
the scale of noise reflected on

167
00:07:58,319 --> 00:07:59,280
parameters

168
00:07:59,280 --> 00:08:02,720
in every set in every step

169
00:08:02,720 --> 00:08:05,440
we will later see how such factorization

170
00:08:05,440 --> 00:08:07,759
is beneficial

171
00:08:07,759 --> 00:08:11,440
well and of course empirically we can

172
00:08:11,440 --> 00:08:14,160
only approximate the expectation with

173
00:08:14,160 --> 00:08:16,560
finite sampling

174
00:08:16,560 --> 00:08:19,599
on the right we have the pseudo code for

175
00:08:19,599 --> 00:08:22,800
dphd with depletes presented

176
00:08:22,800 --> 00:08:26,000
all deeply starts is to tailor the

177
00:08:26,000 --> 00:08:27,120
original loss

178
00:08:27,120 --> 00:08:29,840
to a smooth version that works

179
00:08:29,840 --> 00:08:31,039
potentially better

180
00:08:31,039 --> 00:08:34,240
with dpsgd

181
00:08:37,760 --> 00:08:40,640
well previously we have built up some

182
00:08:40,640 --> 00:08:41,760
intuitions

183
00:08:41,760 --> 00:08:45,120
about how smoothing might help in this

184
00:08:45,120 --> 00:08:45,760
work

185
00:08:45,760 --> 00:08:48,160
we also support the benefits from

186
00:08:48,160 --> 00:08:48,880
depletes

187
00:08:48,880 --> 00:08:53,040
theoretically due to the time constraint

188
00:08:53,040 --> 00:08:54,240
i include here

189
00:08:54,240 --> 00:08:58,320
only a summary of implications

190
00:08:58,320 --> 00:09:02,959
our results are basically threefold

191
00:09:02,959 --> 00:09:05,920
first we show that randomized smoothing

192
00:09:05,920 --> 00:09:07,040
can indeed

193
00:09:07,040 --> 00:09:11,040
lead to a smoother loss landscape

194
00:09:11,040 --> 00:09:15,760
next we prove that dp sgd with depress

195
00:09:15,760 --> 00:09:20,839
can in fact be more stable than vanilla

196
00:09:20,839 --> 00:09:22,160
dpsgd

197
00:09:22,160 --> 00:09:26,000
lastly we show that dpsgd with dps

198
00:09:26,000 --> 00:09:28,959
can also generalize better than vanilla

199
00:09:28,959 --> 00:09:30,959
dpsgd

200
00:09:30,959 --> 00:09:33,519
i refer interested audience to check

201
00:09:33,519 --> 00:09:37,839
session 4 of our paper for details

202
00:09:41,040 --> 00:09:44,480
we also have dps evaluated extensively

203
00:09:44,480 --> 00:09:48,320
in many settings which includes

204
00:09:48,320 --> 00:09:51,040
non-transfer settings where models are

205
00:09:51,040 --> 00:09:52,720
trend with dpsgd

206
00:09:52,720 --> 00:09:57,200
from scratch and transfer settings

207
00:09:57,200 --> 00:10:00,320
where we have access to pre-changed

208
00:10:00,320 --> 00:10:04,720
non-private models as initializations

209
00:10:04,720 --> 00:10:08,320
besides as a representative nerp

210
00:10:08,320 --> 00:10:11,600
task we also have it evaluated

211
00:10:11,600 --> 00:10:16,320
on next world prediction on reddit

212
00:10:16,480 --> 00:10:19,040
what's remarkable is that even with

213
00:10:19,040 --> 00:10:20,640
hyper parameters of

214
00:10:20,640 --> 00:10:23,760
dpsgd fix dps

215
00:10:23,760 --> 00:10:26,720
with a fixed smoothing radius and a

216
00:10:26,720 --> 00:10:28,800
fixed number of sampling

217
00:10:28,800 --> 00:10:33,040
still improves the performance of dp hdd

218
00:10:33,040 --> 00:10:36,880
in all settings and of course

219
00:10:36,880 --> 00:10:39,839
even more improvements are offered with

220
00:10:39,839 --> 00:10:42,240
tuned radius

221
00:10:42,240 --> 00:10:45,680
this won't be possible without a proper

222
00:10:45,680 --> 00:10:46,880
factorization

223
00:10:46,880 --> 00:10:50,320
of the smoothing strengths

224
00:10:53,120 --> 00:10:56,800
in non-transfer settings dps boosts

225
00:10:56,800 --> 00:11:00,480
the accuracy by up to almost 6 percent

226
00:11:00,480 --> 00:11:03,839
with tuned smoothing radius r and

227
00:11:03,839 --> 00:11:06,480
up to almost 4 percent with a fixed

228
00:11:06,480 --> 00:11:08,959
radius

229
00:11:11,279 --> 00:11:14,320
in transfer settings the absolute

230
00:11:14,320 --> 00:11:15,279
improvements

231
00:11:15,279 --> 00:11:17,519
varies from about half a percent

232
00:11:17,519 --> 00:11:18,320
percentage

233
00:11:18,320 --> 00:11:21,839
to almost six percent

234
00:11:25,279 --> 00:11:29,120
on reddit which is an aop task

235
00:11:29,120 --> 00:11:32,800
the perplexity is reduced by at least 50

236
00:11:32,800 --> 00:11:37,760
when using duplice in all settings

237
00:11:41,680 --> 00:11:45,360
in d please we introduce two scalar

238
00:11:45,360 --> 00:11:47,360
hyper parameters

239
00:11:47,360 --> 00:11:50,399
the smoothing radius r and the

240
00:11:50,399 --> 00:11:53,760
number of smoothing samples k

241
00:11:53,760 --> 00:11:56,720
earlier we show already that simply

242
00:11:56,720 --> 00:11:57,760
setting r

243
00:11:57,760 --> 00:12:01,440
and k to be 10 works fairly well

244
00:12:01,440 --> 00:12:05,120
across all settings we have

245
00:12:05,120 --> 00:12:08,399
nevertheless we present ablation studies

246
00:12:08,399 --> 00:12:13,440
of layer effects to model performance

247
00:12:13,600 --> 00:12:16,800
on the left we can see that model

248
00:12:16,800 --> 00:12:17,839
performance

249
00:12:17,839 --> 00:12:21,120
is approximately a unimodal function

250
00:12:21,120 --> 00:12:24,560
of the smoothing radius r which is

251
00:12:24,560 --> 00:12:26,800
a nice property in tuning hyper

252
00:12:26,800 --> 00:12:29,359
parameter

253
00:12:29,519 --> 00:12:32,000
this is in fact compatible with the

254
00:12:32,000 --> 00:12:33,279
intuition

255
00:12:33,279 --> 00:12:36,639
from viewing smoothing as an

256
00:12:36,639 --> 00:12:41,839
approximation of the randomness in dpsgd

257
00:12:41,839 --> 00:12:45,920
as the approximation and the performance

258
00:12:45,920 --> 00:12:49,920
both decrease when r is away from the

259
00:12:49,920 --> 00:12:53,839
best radius

260
00:12:54,320 --> 00:12:57,519
on the right we can see that model

261
00:12:57,519 --> 00:12:58,639
performance

262
00:12:58,639 --> 00:13:02,880
generally benefits from a larger k

263
00:13:02,880 --> 00:13:06,079
this is fairly reasonable since a

264
00:13:06,079 --> 00:13:10,079
larger k implies a better approximation

265
00:13:10,079 --> 00:13:13,279
of the actual expectation

266
00:13:13,279 --> 00:13:16,480
of course more sampling will also

267
00:13:16,480 --> 00:13:20,000
mean more computational costs

268
00:13:20,000 --> 00:13:22,480
but since it takes no more privacy

269
00:13:22,480 --> 00:13:23,760
budget

270
00:13:23,760 --> 00:13:26,800
a practical solution will be to

271
00:13:26,800 --> 00:13:31,839
simply select the largest k that is

272
00:13:32,839 --> 00:13:34,639
accessible

273
00:13:34,639 --> 00:13:38,320
if you recall another issue we target

274
00:13:38,320 --> 00:13:42,240
is the stability of performance

275
00:13:42,240 --> 00:13:45,440
we compute here the range and the

276
00:13:45,440 --> 00:13:48,800
standard deviation of test accuracy

277
00:13:48,800 --> 00:13:51,600
across five independent runs for each

278
00:13:51,600 --> 00:13:53,440
setting

279
00:13:53,440 --> 00:13:57,040
we can see that with dps

280
00:13:57,040 --> 00:14:00,240
dpsgd can achieve not only

281
00:14:00,240 --> 00:14:04,000
better performance but also improved

282
00:14:04,000 --> 00:14:06,399
stability

283
00:14:06,399 --> 00:14:09,839
as it reduces the variations

284
00:14:09,839 --> 00:14:17,519
of performance across different runs

285
00:14:17,519 --> 00:14:20,800
another good thing about dps

286
00:14:20,800 --> 00:14:24,160
well besides improving performance

287
00:14:24,160 --> 00:14:27,920
and stability is that

288
00:14:27,920 --> 00:14:30,800
it tailors only the training loss

289
00:14:30,800 --> 00:14:32,720
function

290
00:14:32,720 --> 00:14:36,000
so it is by design compatible

291
00:14:36,000 --> 00:14:39,600
with many other strategies to improve

292
00:14:39,600 --> 00:14:42,560
dpsgd

293
00:14:43,440 --> 00:14:47,040
as a proof of concept here we

294
00:14:47,040 --> 00:14:51,519
show the results when combining dps

295
00:14:51,519 --> 00:14:54,639
with tempered simoy

296
00:14:54,639 --> 00:14:58,320
which modifies the activation functions

297
00:14:58,320 --> 00:15:02,639
to improve dpsgd

298
00:15:04,000 --> 00:15:07,600
we can see in the table that dps

299
00:15:07,600 --> 00:15:10,839
remains beneficial with the other

300
00:15:10,839 --> 00:15:13,839
strategy

301
00:15:16,079 --> 00:15:19,120
here is a bonus

302
00:15:19,120 --> 00:15:22,600
while we have dps specially designed for

303
00:15:22,600 --> 00:15:24,880
dpsgd

304
00:15:24,880 --> 00:15:28,560
with surprisingly violet the idea

305
00:15:28,560 --> 00:15:31,839
of smoothing learning loss can actually

306
00:15:31,839 --> 00:15:33,040
be extended

307
00:15:33,040 --> 00:15:37,040
to improve page as well

308
00:15:37,040 --> 00:15:40,160
paid or private aggregation

309
00:15:40,160 --> 00:15:44,000
of teacher examples is another popular

310
00:15:44,000 --> 00:15:44,959
framework

311
00:15:44,959 --> 00:15:48,480
for differentially private deep learning

312
00:15:48,480 --> 00:15:51,920
which unlike dpsgd requires

313
00:15:51,920 --> 00:15:55,759
access to public data

314
00:15:55,759 --> 00:15:59,240
in paid teacher models are first trained

315
00:15:59,240 --> 00:16:02,480
independently on destroying subsets of

316
00:16:02,480 --> 00:16:04,720
private data

317
00:16:04,720 --> 00:16:08,079
then the example of teacher models

318
00:16:08,079 --> 00:16:11,519
is used to label the public data

319
00:16:11,519 --> 00:16:15,600
in a differentially private fashion

320
00:16:15,600 --> 00:16:18,639
the labeled public data are eventually

321
00:16:18,639 --> 00:16:19,440
used to

322
00:16:19,440 --> 00:16:23,600
change the final model

323
00:16:23,600 --> 00:16:27,279
intuitively page introduces

324
00:16:27,279 --> 00:16:30,399
label noise in the training

325
00:16:30,399 --> 00:16:33,600
which eventually manifests itself as

326
00:16:33,600 --> 00:16:36,800
parameter noise and therefore can be

327
00:16:36,800 --> 00:16:41,120
mitigated by smoothing loss function

328
00:16:41,519 --> 00:16:44,639
we can see from the table that the

329
00:16:44,639 --> 00:16:46,480
utility of page

330
00:16:46,480 --> 00:16:49,600
is improved consistently in

331
00:16:49,600 --> 00:16:54,639
all settings with smooth loss function

332
00:16:56,560 --> 00:17:00,880
to sum up we propose dpx

333
00:17:00,880 --> 00:17:04,160
which utilizes randomized smoothing in

334
00:17:04,160 --> 00:17:06,000
the parameter space

335
00:17:06,000 --> 00:17:09,439
to boost the utility of differentially

336
00:17:09,439 --> 00:17:12,400
private deep learning

337
00:17:12,400 --> 00:17:15,599
as a result deeply offers

338
00:17:15,599 --> 00:17:20,000
increased performance improves stability

339
00:17:20,000 --> 00:17:23,119
and also compatibility to other

340
00:17:23,119 --> 00:17:25,839
strategies

341
00:17:26,160 --> 00:17:30,160
we consider dps as a nice add-on

342
00:17:30,160 --> 00:17:34,480
in a wide range of settings

343
00:17:35,120 --> 00:17:38,320
well this concludes my presentation i

344
00:17:38,320 --> 00:17:43,039
thank you all for listening

