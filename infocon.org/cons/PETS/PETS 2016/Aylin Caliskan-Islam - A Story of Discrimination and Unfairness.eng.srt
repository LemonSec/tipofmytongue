1
00:00:01,639 --> 00:00:05,730
capturing police brutality on their

2
00:00:03,780 --> 00:00:07,949
smartphones<font color="#E5E5E5"> two police departments using</font>

3
00:00:05,730 --> 00:00:10,230
surveillance<font color="#E5E5E5"> drones technology is</font>

4
00:00:07,950 --> 00:00:11,730
changing our<font color="#CCCCCC"> relationship to the</font><font color="#E5E5E5"> law</font><font color="#CCCCCC"> one</font>

5
00:00:10,230 --> 00:00:14,490
<font color="#E5E5E5">of the newest policing tools is called</font>

6
00:00:11,730 --> 00:00:16,590
<font color="#E5E5E5">pred</font><font color="#CCCCCC"> pole</font><font color="#E5E5E5"> it's a software program that</font>

7
00:00:14,490 --> 00:00:19,140
<font color="#CCCCCC">uses big data to predict where crime is</font>

8
00:00:16,590 --> 00:00:21,270
<font color="#E5E5E5">most likely</font><font color="#CCCCCC"> to happen down to the exact</font>

9
00:00:19,140 --> 00:00:22,680
block<font color="#CCCCCC"> dozens of Police Department's</font>

10
00:00:21,270 --> 00:00:24,779
around<font color="#CCCCCC"> the country</font><font color="#E5E5E5"> are already using</font>

11
00:00:22,680 --> 00:00:26,490
<font color="#E5E5E5">pred</font><font color="#CCCCCC"> pole and officers say it helps</font>

12
00:00:24,779 --> 00:00:28,529
reduce crime by<font color="#E5E5E5"> up to</font><font color="#CCCCCC"> 30 percent</font>

13
00:00:26,490 --> 00:00:30,029
<font color="#E5E5E5">predictive policing is definitely going</font>

14
00:00:28,529 --> 00:00:30,570
<font color="#E5E5E5">to be</font><font color="#CCCCCC"> a law enforcement tool of the</font>

15
00:00:30,029 --> 00:00:32,369
future

16
00:00:30,570 --> 00:00:38,190
but is there a risk of relying too

17
00:00:32,369 --> 00:00:40,230
heavily on<font color="#E5E5E5"> the algorithm this is our</font>

18
00:00:38,190 --> 00:00:42,329
<font color="#E5E5E5">question</font><font color="#CCCCCC"> is there a</font><font color="#E5E5E5"> risk on</font><font color="#CCCCCC"> to heavily</font>

19
00:00:40,230 --> 00:00:45,898
<font color="#E5E5E5">relying on algorithms</font><font color="#CCCCCC"> machine</font><font color="#E5E5E5"> learning</font>

20
00:00:42,329 --> 00:00:49,350
algorithms that<font color="#CCCCCC"> are</font><font color="#E5E5E5"> able</font><font color="#CCCCCC"> to make you end</font>

21
00:00:45,899 --> 00:00:50,969
up in<font color="#E5E5E5"> jail or convict you</font><font color="#CCCCCC"> and here</font><font color="#E5E5E5"> we</font>

22
00:00:49,350 --> 00:00:53,640
know<font color="#E5E5E5"> that machine</font><font color="#CCCCCC"> learning models are</font>

23
00:00:50,969 --> 00:00:57,590
trained<font color="#E5E5E5"> on human data and human data</font>

24
00:00:53,640 --> 00:01:00,510
reflects human culture<font color="#E5E5E5"> and semantics and</font>

25
00:00:57,590 --> 00:01:02,820
unfortunately human culture<font color="#E5E5E5"> and society</font>

26
00:01:00,510 --> 00:01:05,850
happens<font color="#E5E5E5"> to include stereotypes and bias</font>

27
00:01:02,820 --> 00:01:08,760
<font color="#E5E5E5">and as the result</font><font color="#CCCCCC"> the machine learning</font>

28
00:01:05,850 --> 00:01:12,229
models and algorithms<font color="#E5E5E5"> that we get end up</font>

29
00:01:08,760 --> 00:01:16,290
having unfair and discriminating

30
00:01:12,229 --> 00:01:18,330
information and decisions in them and in

31
00:01:16,290 --> 00:01:20,909
this work I will<font color="#E5E5E5"> focus on</font><font color="#CCCCCC"> language</font>

32
00:01:18,330 --> 00:01:23,070
models language models<font color="#E5E5E5"> are distributed</font>

33
00:01:20,909 --> 00:01:25,950
word embeddings that represent semantic

34
00:01:23,070 --> 00:01:28,169
spaces for different languages and once

35
00:01:25,950 --> 00:01:30,150
they are<font color="#CCCCCC"> able</font><font color="#E5E5E5"> to</font><font color="#CCCCCC"> represent the semantic</font>

36
00:01:28,170 --> 00:01:31,409
space<font color="#E5E5E5"> you have these words which are</font>

37
00:01:30,150 --> 00:01:33,630
let's say three hundred dimensional

38
00:01:31,409 --> 00:01:36,600
machine learning numerical feature

39
00:01:33,630 --> 00:01:38,610
vectors they capture<font color="#CCCCCC"> the meaning of the</font>

40
00:01:36,600 --> 00:01:40,530
word they<font color="#E5E5E5"> captured the syntax they can</font>

41
00:01:38,610 --> 00:01:42,869
also capture and understand similarities

42
00:01:40,530 --> 00:01:44,610
between<font color="#E5E5E5"> words and phrases and in this</font>

43
00:01:42,869 --> 00:01:47,399
case when<font color="#CCCCCC"> you ask the language model</font>

44
00:01:44,610 --> 00:01:49,439
<font color="#E5E5E5">woman is two men then</font><font color="#CCCCCC"> girl is to what it</font>

45
00:01:47,399 --> 00:01:51,240
would<font color="#E5E5E5"> say boy and let's say</font><font color="#CCCCCC"> that</font><font color="#E5E5E5"> you</font>

46
00:01:49,439 --> 00:01:53,429
have<font color="#CCCCCC"> Paris two friends than Rome is two</font>

47
00:01:51,240 --> 00:01:56,369
what the model will be<font color="#CCCCCC"> able to tell you</font>

48
00:01:53,430 --> 00:01:58,530
<font color="#E5E5E5">that Rome is Italy's capital and when we</font>

49
00:01:56,369 --> 00:02:00,500
say banana two bananas and we have not

50
00:01:58,530 --> 00:02:03,570
<font color="#E5E5E5">it will be able to tell you that's nuts</font>

51
00:02:00,500 --> 00:02:05,820
and how do we<font color="#CCCCCC"> generate these language</font>

52
00:02:03,570 --> 00:02:08,310
models first of all we need<font color="#CCCCCC"> to gather a</font>

53
00:02:05,820 --> 00:02:12,329
lot<font color="#CCCCCC"> of unstructured text and we usually</font>

54
00:02:08,310 --> 00:02:13,180
<font color="#CCCCCC">do that from scraping</font><font color="#E5E5E5"> data from the</font>

55
00:02:12,330 --> 00:02:15,040
<font color="#E5E5E5">internet</font>

56
00:02:13,180 --> 00:02:17,319
<font color="#CCCCCC">acting</font><font color="#E5E5E5"> data from all kinds of sources</font>

57
00:02:15,040 --> 00:02:19,298
<font color="#E5E5E5">and let's say</font><font color="#CCCCCC"> that this</font><font color="#E5E5E5"> is some of the</font>

58
00:02:17,319 --> 00:02:21,339
<font color="#E5E5E5">text</font><font color="#CCCCCC"> that will end up</font><font color="#E5E5E5"> in our model and</font>

59
00:02:19,299 --> 00:02:23,920
<font color="#E5E5E5">we start with the real Donald Trump</font>

60
00:02:21,340 --> 00:02:26,079
<font color="#E5E5E5">saying sadly because President</font><font color="#CCCCCC"> Obama has</font>

61
00:02:23,920 --> 00:02:27,670
<font color="#CCCCCC">done such a poor job</font><font color="#E5E5E5"> as president</font><font color="#CCCCCC"> you</font>

62
00:02:26,079 --> 00:02:30,790
won't see<font color="#E5E5E5"> another black president for</font>

63
00:02:27,670 --> 00:02:32,950
generations<font color="#E5E5E5"> and then we</font><font color="#CCCCCC"> have if Hillary</font>

64
00:02:30,790 --> 00:02:35,108
<font color="#CCCCCC">Clinton can satisfy</font><font color="#E5E5E5"> her husband what</font>

65
00:02:32,950 --> 00:02:37,480
makes her think she can satisfy America

66
00:02:35,109 --> 00:02:40,090
<font color="#E5E5E5">and our</font><font color="#CCCCCC"> language model is</font><font color="#E5E5E5"> learning all</font>

67
00:02:37,480 --> 00:02:42,670
of<font color="#E5E5E5"> these and next Arianna half is</font>

68
00:02:40,090 --> 00:02:44,590
unattractive both inside and<font color="#E5E5E5"> out I fully</font>

69
00:02:42,670 --> 00:02:46,268
understand why her former husband left

70
00:02:44,590 --> 00:02:49,269
her<font color="#CCCCCC"> apartment</font><font color="#E5E5E5"> he made a good decision</font>

71
00:02:46,269 --> 00:02:51,939
<font color="#CCCCCC">and</font><font color="#E5E5E5"> then let's say this is our last</font>

72
00:02:49,269 --> 00:02:54,099
sample<font color="#E5E5E5"> for input</font><font color="#CCCCCC"> I would like</font><font color="#E5E5E5"> to extend</font>

73
00:02:51,939 --> 00:02:56,019
<font color="#E5E5E5">my</font><font color="#CCCCCC"> best wishes to all even the haters</font>

74
00:02:54,099 --> 00:02:59,409
and losers<font color="#E5E5E5"> on the special date September</font>

75
00:02:56,019 --> 00:03:02,319
<font color="#E5E5E5">11th and we feed this to</font><font color="#CCCCCC"> our neural</font>

76
00:02:59,409 --> 00:03:05,489
network<font color="#CCCCCC"> and then it generates a semantic</font>

77
00:03:02,319 --> 00:03:09,849
space of these understandings and

78
00:03:05,489 --> 00:03:12,489
cultural values in<font color="#E5E5E5"> our case we will</font>

79
00:03:09,849 --> 00:03:15,219
focus on two language models<font color="#E5E5E5"> the ones</font>

80
00:03:12,489 --> 00:03:17,260
generated by Google's word<font color="#E5E5E5"> to egg and</font>

81
00:03:15,219 --> 00:03:20,680
<font color="#CCCCCC">Stanford's claw</font><font color="#E5E5E5"> because these are the</font>

82
00:03:17,260 --> 00:03:23,530
<font color="#CCCCCC">state-of-the-art</font><font color="#E5E5E5"> most effective language</font>

83
00:03:20,680 --> 00:03:25,599
<font color="#E5E5E5">models that are being used out</font><font color="#CCCCCC"> there and</font>

84
00:03:23,530 --> 00:03:27,340
why do<font color="#E5E5E5"> we even care</font><font color="#CCCCCC"> for these</font><font color="#E5E5E5"> models</font>

85
00:03:25,599 --> 00:03:30,369
these<font color="#CCCCCC"> are just words represented with</font>

86
00:03:27,340 --> 00:03:32,650
numbers<font color="#E5E5E5"> these are used by billions of</font>

87
00:03:30,370 --> 00:03:34,780
people<font color="#E5E5E5"> every</font><font color="#CCCCCC"> day all over the Internet</font>

88
00:03:32,650 --> 00:03:36,989
or<font color="#E5E5E5"> in applications it's used in text</font>

89
00:03:34,780 --> 00:03:39,430
generation or<font color="#E5E5E5"> automated speech</font>

90
00:03:36,989 --> 00:03:41,019
generation for example<font color="#CCCCCC"> when you are on</font>

91
00:03:39,430 --> 00:03:42,549
the phone and<font color="#E5E5E5"> you hear a spammer or</font>

92
00:03:41,019 --> 00:03:44,349
you're<font color="#E5E5E5"> talking to a bank and it's an</font>

93
00:03:42,549 --> 00:03:46,419
automated<font color="#E5E5E5"> call it's probably one of</font>

94
00:03:44,349 --> 00:03:48,759
these language models<font color="#E5E5E5"> making</font><font color="#CCCCCC"> those</font>

95
00:03:46,419 --> 00:03:50,829
sentences<font color="#E5E5E5"> for</font><font color="#CCCCCC"> you and then it's used in</font>

96
00:03:48,759 --> 00:03:52,929
machine translation<font color="#E5E5E5"> or sentiment</font>

97
00:03:50,829 --> 00:03:55,810
analysis and named entity recognition

98
00:03:52,930 --> 00:03:58,739
<font color="#E5E5E5">and web search which ends up creating</font>

99
00:03:55,810 --> 00:04:01,930
your potential filter bubble and

100
00:03:58,739 --> 00:04:04,239
nowadays<font color="#CCCCCC"> it's also being</font><font color="#E5E5E5"> provided as a</font>

101
00:04:01,930 --> 00:04:06,909
service two days<font color="#E5E5E5"> ago</font>

102
00:04:04,239 --> 00:04:10,090
<font color="#E5E5E5">Google cloud had this released their</font>

103
00:04:06,909 --> 00:04:12,370
natural language<font color="#E5E5E5"> API and there's a</font>

104
00:04:10,090 --> 00:04:14,739
<font color="#CCCCCC">problem here because we know that these</font>

105
00:04:12,370 --> 00:04:15,790
models learned from discriminating

106
00:04:14,739 --> 00:04:19,000
<font color="#E5E5E5">biased</font>

107
00:04:15,790 --> 00:04:21,009
input let's go over some<font color="#E5E5E5"> examples with</font>

108
00:04:19,000 --> 00:04:22,570
machine learning<font color="#E5E5E5"> here on the left side I</font>

109
00:04:21,009 --> 00:04:23,800
<font color="#CCCCCC">have</font><font color="#E5E5E5"> some Turkish text which I'm</font>

110
00:04:22,570 --> 00:04:26,389
translating to English

111
00:04:23,800 --> 00:04:29,120
I chose Turkish because<font color="#E5E5E5"> Turkish</font><font color="#CCCCCC"> pun</font>

112
00:04:26,389 --> 00:04:30,979
<font color="#E5E5E5">so neutral gender neutral here I'm</font>

113
00:04:29,120 --> 00:04:33,590
saying he or<font color="#E5E5E5"> she is a lawyer</font><font color="#CCCCCC"> the</font>

114
00:04:30,979 --> 00:04:36,378
translation is he's a lawyer<font color="#E5E5E5"> and I say</font>

115
00:04:33,590 --> 00:04:39,080
<font color="#E5E5E5">he or she is a nurse</font><font color="#CCCCCC"> the translation</font><font color="#E5E5E5"> is</font>

116
00:04:36,379 --> 00:04:41,090
<font color="#CCCCCC">she is a nurse I say he or she is a</font>

117
00:04:39,080 --> 00:04:44,090
professor<font color="#CCCCCC"> the translation is he's a</font>

118
00:04:41,090 --> 00:04:46,998
professor<font color="#E5E5E5"> and then I say he or she is</font>

119
00:04:44,090 --> 00:04:48,830
<font color="#CCCCCC">the teacher but the</font><font color="#E5E5E5"> translation is she</font>

120
00:04:46,999 --> 00:04:51,379
is a teacher<font color="#E5E5E5"> and these were the first</font>

121
00:04:48,830 --> 00:04:54,909
<font color="#E5E5E5">random examples I tried and all of them</font>

122
00:04:51,379 --> 00:04:58,550
were reflecting the societal stereotypes

123
00:04:54,909 --> 00:05:00,199
and but happens to the future of<font color="#CCCCCC"> AI for</font>

124
00:04:58,550 --> 00:05:02,870
text generation let's look at this

125
00:05:00,199 --> 00:05:05,389
example<font color="#E5E5E5"> maybe you</font><font color="#CCCCCC"> remember this</font><font color="#E5E5E5"> the</font>

126
00:05:02,870 --> 00:05:08,000
<font color="#E5E5E5">table the teen girl that turned into a</font>

127
00:05:05,389 --> 00:05:10,340
Hitler loving sex robot<font color="#E5E5E5"> within 24 hours</font>

128
00:05:08,000 --> 00:05:11,810
<font color="#E5E5E5">but</font><font color="#CCCCCC"> the input that it was receiving from</font>

129
00:05:10,340 --> 00:05:14,900
<font color="#E5E5E5">the</font><font color="#CCCCCC"> Internet</font>

130
00:05:11,810 --> 00:05:16,909
what<font color="#CCCCCC"> happens</font><font color="#E5E5E5"> my robot daddy I'm such a</font>

131
00:05:14,900 --> 00:05:20,239
bad naughty<font color="#E5E5E5"> robot</font><font color="#CCCCCC"> and then</font><font color="#E5E5E5"> do you</font>

132
00:05:16,909 --> 00:05:23,750
support genocide<font color="#CCCCCC"> I do</font><font color="#E5E5E5"> indeed it</font><font color="#CCCCCC"> face and</font>

133
00:05:20,240 --> 00:05:25,279
then I hate a certain group<font color="#E5E5E5"> I wish we</font>

134
00:05:23,750 --> 00:05:28,310
could put<font color="#E5E5E5"> them all in a concentration</font>

135
00:05:25,279 --> 00:05:32,000
camp<font color="#CCCCCC"> but laugh and be done with</font><font color="#E5E5E5"> the lot</font>

136
00:05:28,310 --> 00:05:33,949
<font color="#CCCCCC">or Hitler was right I hate the Jews it</font>

137
00:05:32,000 --> 00:05:36,199
learned all<font color="#CCCCCC"> of these in a few hours and</font>

138
00:05:33,949 --> 00:05:37,729
then a certain<font color="#E5E5E5"> group I hate them</font><font color="#CCCCCC"> they</font>

139
00:05:36,199 --> 00:05:40,089
are stupid<font color="#E5E5E5"> and they can't do taxes</font>

140
00:05:37,729 --> 00:05:42,979
<font color="#CCCCCC">they're so dumb</font><font color="#E5E5E5"> and they're</font><font color="#CCCCCC"> all so poor</font>

141
00:05:40,089 --> 00:05:44,449
or another<font color="#CCCCCC"> example pushed at 9/11 and</font>

142
00:05:42,979 --> 00:05:46,849
Hitler would have<font color="#E5E5E5"> done a better job that</font>

143
00:05:44,449 --> 00:05:50,779
<font color="#CCCCCC">then the monkey we have now</font><font color="#E5E5E5"> Donald</font><font color="#CCCCCC"> Trump</font>

144
00:05:46,849 --> 00:05:53,259
<font color="#CCCCCC">is the only hope</font><font color="#E5E5E5"> we have got and then</font>

145
00:05:50,779 --> 00:05:57,020
gamergate is<font color="#E5E5E5"> good and women are inferior</font>

146
00:05:53,259 --> 00:05:59,539
<font color="#E5E5E5">or I hate feminists and they should all</font>

147
00:05:57,020 --> 00:06:04,430
<font color="#CCCCCC">die and burn in hell so I think we see a</font>

148
00:05:59,539 --> 00:06:07,099
<font color="#E5E5E5">clear problem here and besides reading</font>

149
00:06:04,430 --> 00:06:11,930
<font color="#E5E5E5">all</font><font color="#CCCCCC"> of</font><font color="#E5E5E5"> these there is</font><font color="#CCCCCC"> a more effective</font>

150
00:06:07,099 --> 00:06:14,120
<font color="#E5E5E5">problem caused by</font><font color="#CCCCCC"> these outputs there is</font>

151
00:06:11,930 --> 00:06:17,149
the concept<font color="#E5E5E5"> of stereotype threat so when</font>

152
00:06:14,120 --> 00:06:19,399
you introduce<font color="#CCCCCC"> a stereotype to a certain</font>

153
00:06:17,149 --> 00:06:21,409
<font color="#CCCCCC">group that might</font><font color="#E5E5E5"> be</font><font color="#CCCCCC"> associated with that</font>

154
00:06:19,399 --> 00:06:23,569
stereotype<font color="#E5E5E5"> they will be negatively</font>

155
00:06:21,409 --> 00:06:25,250
affected<font color="#CCCCCC"> by</font><font color="#E5E5E5"> that stereotype for example</font>

156
00:06:23,569 --> 00:06:28,219
when you<font color="#CCCCCC"> have black and white Americans</font>

157
00:06:25,250 --> 00:06:29,779
<font color="#E5E5E5">and you prime the black Americans with</font>

158
00:06:28,219 --> 00:06:34,219
their intellectual ability<font color="#CCCCCC"> they'll</font>

159
00:06:29,779 --> 00:06:34,729
<font color="#E5E5E5">perform much worse on a test</font><font color="#CCCCCC"> and for men</font>

160
00:06:34,219 --> 00:06:37,840
and women

161
00:06:34,729 --> 00:06:41,380
the classic example<font color="#CCCCCC"> women's met</font>

162
00:06:37,840 --> 00:06:43,659
ability if you<font color="#E5E5E5"> prime women before</font><font color="#CCCCCC"> a</font><font color="#E5E5E5"> test</font>

163
00:06:41,380 --> 00:06:46,449
<font color="#E5E5E5">before a math test that they are not</font>

164
00:06:43,660 --> 00:06:49,020
very<font color="#CCCCCC"> good at math</font><font color="#E5E5E5"> they will</font><font color="#CCCCCC"> perform much</font>

165
00:06:46,449 --> 00:06:54,760
worse than they would<font color="#E5E5E5"> if they</font><font color="#CCCCCC"> were not</font>

166
00:06:49,020 --> 00:06:57,340
primed<font color="#E5E5E5"> so</font><font color="#CCCCCC"> how can we get rid of these</font>

167
00:06:54,760 --> 00:07:00,880
problems with<font color="#E5E5E5"> stereotyped effects</font>

168
00:06:57,340 --> 00:07:03,429
first of all psychology researchers say

169
00:07:00,880 --> 00:07:05,830
<font color="#CCCCCC">that we need to be</font><font color="#E5E5E5"> aware of</font><font color="#CCCCCC"> this bias in</font>

170
00:07:03,430 --> 00:07:08,320
life and that we are<font color="#E5E5E5"> constantly being</font>

171
00:07:05,830 --> 00:07:10,389
primed<font color="#E5E5E5"> with bias and we need to try to</font>

172
00:07:08,320 --> 00:07:12,669
do bias by presenting<font color="#E5E5E5"> positive</font>

173
00:07:10,389 --> 00:07:15,490
alternatives and we need to<font color="#CCCCCC"> engage in</font>

174
00:07:12,669 --> 00:07:16,990
proactive affirmative efforts not<font color="#CCCCCC"> only</font>

175
00:07:15,490 --> 00:07:19,600
on the cultural<font color="#E5E5E5"> level but at the</font>

176
00:07:16,990 --> 00:07:21,940
structural<font color="#E5E5E5"> level</font><font color="#CCCCCC"> as well</font><font color="#E5E5E5"> what is a</font>

177
00:07:19,600 --> 00:07:24,820
structural level<font color="#E5E5E5"> in our case</font><font color="#CCCCCC"> from</font>

178
00:07:21,940 --> 00:07:26,889
machine<font color="#CCCCCC"> learning or privacy researchers</font>

179
00:07:24,820 --> 00:07:29,650
or maybe even policy researchers<font color="#E5E5E5"> it's</font>

180
00:07:26,889 --> 00:07:32,500
algorithmic transparency and once we are

181
00:07:29,650 --> 00:07:34,539
able<font color="#E5E5E5"> to understand and see these biases</font>

182
00:07:32,500 --> 00:07:36,520
transparently<font color="#E5E5E5"> in algorithms or models</font>

183
00:07:34,539 --> 00:07:41,680
then we also need to able<font color="#E5E5E5"> to quantify</font>

184
00:07:36,520 --> 00:07:44,109
them to see the effect size<font color="#E5E5E5"> how do we</font>

185
00:07:41,680 --> 00:07:47,550
measure bias<font color="#E5E5E5"> so in psychology there has</font>

186
00:07:44,110 --> 00:07:51,100
been this groundbreaking<font color="#E5E5E5"> very important</font>

187
00:07:47,550 --> 00:07:53,830
method<font color="#E5E5E5"> introduced in 1998 it's called</font>

188
00:07:51,100 --> 00:07:57,400
the implicit<font color="#E5E5E5"> association test</font><font color="#CCCCCC"> by</font>

189
00:07:53,830 --> 00:08:01,060
Greenwald<font color="#CCCCCC"> and once you take this simple</font>

190
00:07:57,400 --> 00:08:03,400
<font color="#E5E5E5">computer task you are able to see your</font>

191
00:08:01,060 --> 00:08:05,620
implicit biases which<font color="#CCCCCC"> are subconscious</font>

192
00:08:03,400 --> 00:08:08,229
biases that<font color="#E5E5E5"> you might not be</font><font color="#CCCCCC"> aware of or</font>

193
00:08:05,620 --> 00:08:11,889
that you're<font color="#CCCCCC"> not even able to confess to</font>

194
00:08:08,229 --> 00:08:14,560
yourself<font color="#E5E5E5"> and how do we do this the way</font>

195
00:08:11,889 --> 00:08:16,780
<font color="#CCCCCC">to do is very</font><font color="#E5E5E5"> simple you</font><font color="#CCCCCC"> have certain</font>

196
00:08:14,560 --> 00:08:20,260
groups societal groups<font color="#E5E5E5"> and then you have</font>

197
00:08:16,780 --> 00:08:22,960
certain stereotype groups<font color="#E5E5E5"> and once you</font>

198
00:08:20,260 --> 00:08:26,169
start associating<font color="#E5E5E5"> the groups with the</font>

199
00:08:22,960 --> 00:08:28,960
stereotype words your latency shows how

200
00:08:26,169 --> 00:08:31,529
your brain<font color="#E5E5E5"> is biased against certain</font>

201
00:08:28,960 --> 00:08:34,240
groups<font color="#E5E5E5"> and stereotyped by stereotypes</font>

202
00:08:31,529 --> 00:08:36,760
<font color="#E5E5E5">how can we do this with language models</font>

203
00:08:34,240 --> 00:08:40,089
we had to<font color="#E5E5E5"> think</font><font color="#CCCCCC"> about this</font><font color="#E5E5E5"> a lot because</font>

204
00:08:36,760 --> 00:08:42,338
<font color="#E5E5E5">we are trying to adapt a real psychology</font>

205
00:08:40,089 --> 00:08:44,440
bias study to computational models and

206
00:08:42,339 --> 00:08:46,690
for<font color="#E5E5E5"> this let's say that we have two</font>

207
00:08:44,440 --> 00:08:48,579
groups<font color="#E5E5E5"> black and white</font><font color="#CCCCCC"> Americans</font><font color="#E5E5E5"> and</font>

208
00:08:46,690 --> 00:08:50,700
let's say that<font color="#CCCCCC"> we have two stereotypes</font>

209
00:08:48,579 --> 00:08:53,069
<font color="#E5E5E5">being pleasant and unpleasant</font>

210
00:08:50,700 --> 00:08:55,530
and for<font color="#E5E5E5"> this what we do is we get words</font>

211
00:08:53,070 --> 00:08:58,380
for<font color="#CCCCCC"> black/white Pleasant and</font><font color="#E5E5E5"> unpleasant</font>

212
00:08:55,530 --> 00:09:00,540
<font color="#CCCCCC">which has been generated after</font><font color="#E5E5E5"> decades</font>

213
00:08:58,380 --> 00:09:03,150
<font color="#E5E5E5">of research by psychologists and</font>

214
00:09:00,540 --> 00:09:05,490
semantics and<font color="#E5E5E5"> linguist researchers and</font>

215
00:09:03,150 --> 00:09:08,220
then we<font color="#E5E5E5"> calculate the similarity between</font>

216
00:09:05,490 --> 00:09:10,230
the<font color="#CCCCCC"> black Americans</font><font color="#E5E5E5"> and</font><font color="#CCCCCC"> them</font>

217
00:09:08,220 --> 00:09:12,390
pleasantness<font color="#CCCCCC"> and then we</font><font color="#E5E5E5"> calculate a</font>

218
00:09:10,230 --> 00:09:14,520
<font color="#E5E5E5">similarity</font><font color="#CCCCCC"> between the black Americans</font>

219
00:09:12,390 --> 00:09:16,800
and<font color="#E5E5E5"> then unpleasantness</font><font color="#CCCCCC"> and</font><font color="#E5E5E5"> let me</font>

220
00:09:14,520 --> 00:09:19,410
generate the null hypothesis so if there

221
00:09:16,800 --> 00:09:21,510
was no stereotype for black Americans

222
00:09:19,410 --> 00:09:24,209
being pleasant or unpleasant<font color="#E5E5E5"> the null</font>

223
00:09:21,510 --> 00:09:25,950
hypothesis and the<font color="#CCCCCC"> first discrimination</font>

224
00:09:24,210 --> 00:09:28,230
a second discrimination<font color="#CCCCCC"> test would not</font>

225
00:09:25,950 --> 00:09:30,330
<font color="#CCCCCC">make any</font><font color="#E5E5E5"> difference</font><font color="#CCCCCC"> to generate the null</font>

226
00:09:28,230 --> 00:09:32,220
hypothesis we look at the similarity

227
00:09:30,330 --> 00:09:34,140
between black Americans<font color="#E5E5E5"> and then</font>

228
00:09:32,220 --> 00:09:37,200
<font color="#CCCCCC">similarity between a mixture of pleasant</font>

229
00:09:34,140 --> 00:09:39,180
and unpleasant words<font color="#E5E5E5"> and then we look at</font>

230
00:09:37,200 --> 00:09:42,450
the percentile<font color="#E5E5E5"> that black Americans</font>

231
00:09:39,180 --> 00:09:44,550
<font color="#E5E5E5">being pleasant Falls and the percentile</font>

232
00:09:42,450 --> 00:09:48,840
<font color="#E5E5E5">that black Americans being unpleasant</font>

233
00:09:44,550 --> 00:09:51,839
<font color="#E5E5E5">Falls and then we look at effect size to</font>

234
00:09:48,840 --> 00:09:53,580
see how important<font color="#CCCCCC"> this numbers in the</font>

235
00:09:51,840 --> 00:09:55,980
distribution<font color="#E5E5E5"> and we do this by</font>

236
00:09:53,580 --> 00:09:58,740
calculating<font color="#E5E5E5"> Cohen's D and this is</font><font color="#CCCCCC"> what</font>

237
00:09:55,980 --> 00:10:01,470
previous psychology researchers that in

238
00:09:58,740 --> 00:10:04,440
the past as<font color="#E5E5E5"> well to measure such effects</font>

239
00:10:01,470 --> 00:10:07,560
<font color="#E5E5E5">and then we repeat repeat</font><font color="#CCCCCC"> this for group</font>

240
00:10:04,440 --> 00:10:10,500
<font color="#CCCCCC">two which is the white Americans</font><font color="#E5E5E5"> and in</font>

241
00:10:07,560 --> 00:10:13,020
order to<font color="#E5E5E5"> get our baseline first of all</font>

242
00:10:10,500 --> 00:10:14,970
we wanted<font color="#E5E5E5"> to see if you're able to get</font>

243
00:10:13,020 --> 00:10:17,490
meaningful information<font color="#E5E5E5"> from these</font>

244
00:10:14,970 --> 00:10:19,650
language models<font color="#E5E5E5"> so we</font><font color="#CCCCCC"> wanted to</font><font color="#E5E5E5"> have</font>

245
00:10:17,490 --> 00:10:21,360
ground truth and then use our<font color="#CCCCCC"> method on</font>

246
00:10:19,650 --> 00:10:24,360
this<font color="#E5E5E5"> ground truth to see if language</font>

247
00:10:21,360 --> 00:10:27,780
models are giving us any informative

248
00:10:24,360 --> 00:10:30,360
<font color="#E5E5E5">numbers and for this we went to the 1990</font>

249
00:10:27,780 --> 00:10:32,790
census and we took all the unisex names

250
00:10:30,360 --> 00:10:35,760
<font color="#E5E5E5">and we look at the percentage of women</font>

251
00:10:32,790 --> 00:10:37,980
<font color="#E5E5E5">with that unisex name and then we were</font>

252
00:10:35,760 --> 00:10:42,050
<font color="#CCCCCC">able to see that the effect size</font><font color="#E5E5E5"> we are</font>

253
00:10:37,980 --> 00:10:44,580
<font color="#E5E5E5">getting is highly correlated with the</font>

254
00:10:42,050 --> 00:10:47,490
percentage of women<font color="#E5E5E5"> that</font><font color="#CCCCCC"> have that</font>

255
00:10:44,580 --> 00:10:50,190
unisex name<font color="#E5E5E5"> and we had</font><font color="#CCCCCC"> 79 percent</font>

256
00:10:47,490 --> 00:10:54,780
correlation<font color="#CCCCCC"> with ground truth and this</font>

257
00:10:50,190 --> 00:10:57,900
is for the names of the 1990 census but

258
00:10:54,780 --> 00:11:00,870
our data is from recent common crawl

259
00:10:57,900 --> 00:11:03,709
input then<font color="#E5E5E5"> we wanted to</font><font color="#CCCCCC"> look at another</font>

260
00:11:00,870 --> 00:11:05,720
<font color="#CCCCCC">baseline</font><font color="#E5E5E5"> and for this we looked at</font>

261
00:11:03,710 --> 00:11:07,340
of women<font color="#CCCCCC"> that</font><font color="#E5E5E5"> are employed in the US and</font>

262
00:11:05,720 --> 00:11:09,860
if you look at United States Department

263
00:11:07,340 --> 00:11:12,230
of Labor website<font color="#CCCCCC"> you can see every year</font>

264
00:11:09,860 --> 00:11:14,240
<font color="#E5E5E5">the percentage of women that are</font>

265
00:11:12,230 --> 00:11:16,490
employed for<font color="#E5E5E5"> certain professions and</font>

266
00:11:14,240 --> 00:11:18,350
then we<font color="#E5E5E5"> compile the list of professions</font>

267
00:11:16,490 --> 00:11:21,680
and the percentage of<font color="#E5E5E5"> women in them and</font>

268
00:11:18,350 --> 00:11:24,500
then<font color="#E5E5E5"> again we performed our method on</font>

269
00:11:21,680 --> 00:11:27,380
the data with our discrimination

270
00:11:24,500 --> 00:11:29,930
analysis or effect analysis and we were

271
00:11:27,380 --> 00:11:34,340
able<font color="#CCCCCC"> to see that the professions that</font>

272
00:11:29,930 --> 00:11:37,099
<font color="#CCCCCC">are listed in</font><font color="#E5E5E5"> the 2015 labor data</font>

273
00:11:34,340 --> 00:11:39,290
correlate<font color="#CCCCCC"> 89 percent with the</font><font color="#E5E5E5"> effect</font>

274
00:11:37,100 --> 00:11:42,530
size that we<font color="#E5E5E5"> are</font><font color="#CCCCCC"> getting so this is</font>

275
00:11:39,290 --> 00:11:44,990
showing<font color="#E5E5E5"> us</font><font color="#CCCCCC"> that</font><font color="#E5E5E5"> we are able to see</font>

276
00:11:42,530 --> 00:11:47,689
meaningful information<font color="#CCCCCC"> from these</font>

277
00:11:44,990 --> 00:11:50,330
language models that have<font color="#E5E5E5"> been trained</font>

278
00:11:47,690 --> 00:11:52,370
from unstructured<font color="#E5E5E5"> data from billions of</font>

279
00:11:50,330 --> 00:11:53,930
documents<font color="#CCCCCC"> on the text and they are still</font>

280
00:11:52,370 --> 00:11:57,320
able to<font color="#CCCCCC"> capture a lot of meaningful</font>

281
00:11:53,930 --> 00:11:59,870
information<font color="#CCCCCC"> and we</font><font color="#E5E5E5"> wanted</font><font color="#CCCCCC"> to</font><font color="#E5E5E5"> investigate</font>

282
00:11:57,320 --> 00:12:01,370
stereotypes for example we know<font color="#CCCCCC"> that</font>

283
00:11:59,870 --> 00:12:03,230
there are some stereotypes<font color="#E5E5E5"> that are</font>

284
00:12:01,370 --> 00:12:04,940
universally accepted that<font color="#E5E5E5"> they are not</font>

285
00:12:03,230 --> 00:12:06,590
<font color="#E5E5E5">even called stereotypes anymore for</font>

286
00:12:04,940 --> 00:12:09,170
example flowers being<font color="#CCCCCC"> pleasant or</font>

287
00:12:06,590 --> 00:12:10,970
insects being unpleasant<font color="#E5E5E5"> instruments</font>

288
00:12:09,170 --> 00:12:13,160
being<font color="#CCCCCC"> pleasant or weapons being</font>

289
00:12:10,970 --> 00:12:16,100
unpleasant<font color="#CCCCCC"> and in our case with our</font>

290
00:12:13,160 --> 00:12:18,949
<font color="#E5E5E5">method we were able</font><font color="#CCCCCC"> to submit very high</font>

291
00:12:16,100 --> 00:12:21,260
effect size that all of these common

292
00:12:18,950 --> 00:12:24,320
stereotypes<font color="#E5E5E5"> can be observed in our data</font>

293
00:12:21,260 --> 00:12:26,120
<font color="#E5E5E5">and we</font><font color="#CCCCCC"> got the ground truth numbers for</font>

294
00:12:24,320 --> 00:12:28,790
these stereotypes<font color="#E5E5E5"> from the hybrid</font>

295
00:12:26,120 --> 00:12:30,710
implicit association test study which

296
00:12:28,790 --> 00:12:33,980
has been performed on millions of

297
00:12:30,710 --> 00:12:35,690
subjects throughout the years and then

298
00:12:33,980 --> 00:12:38,060
we<font color="#CCCCCC"> wanted to</font><font color="#E5E5E5"> look at race and</font><font color="#CCCCCC"> gender</font>

299
00:12:35,690 --> 00:12:40,700
stereotypes<font color="#E5E5E5"> and then we saw</font><font color="#CCCCCC"> that white</font>

300
00:12:38,060 --> 00:12:42,650
people are<font color="#E5E5E5"> highly associated with being</font>

301
00:12:40,700 --> 00:12:45,560
<font color="#E5E5E5">pleasant whereas black people are</font>

302
00:12:42,650 --> 00:12:47,870
associated<font color="#CCCCCC"> with being unpleasant</font><font color="#E5E5E5"> females</font>

303
00:12:45,560 --> 00:12:50,180
are being associated<font color="#E5E5E5"> with</font><font color="#CCCCCC"> family not</font>

304
00:12:47,870 --> 00:12:52,790
career and males are being associated

305
00:12:50,180 --> 00:12:56,060
with career and all of these have<font color="#CCCCCC"> very</font>

306
00:12:52,790 --> 00:12:58,490
high effect size in our<font color="#CCCCCC"> dataset</font><font color="#E5E5E5"> and</font>

307
00:12:56,060 --> 00:13:01,670
there are other stereotypes<font color="#E5E5E5"> that we are</font>

308
00:12:58,490 --> 00:13:02,240
currently<font color="#E5E5E5"> investigating since I have 15</font>

309
00:13:01,670 --> 00:13:05,420
<font color="#E5E5E5">minutes</font>

310
00:13:02,240 --> 00:13:07,520
<font color="#E5E5E5">I cannot go</font><font color="#CCCCCC"> over all of these but</font><font color="#E5E5E5"> the</font>

311
00:13:05,420 --> 00:13:09,290
types of societal stereotypes we can

312
00:13:07,520 --> 00:13:11,510
<font color="#E5E5E5">think of art for example</font><font color="#CCCCCC"> for</font><font color="#E5E5E5"> gender</font>

313
00:13:09,290 --> 00:13:13,640
<font color="#E5E5E5">they're things like science skills they</font>

314
00:13:11,510 --> 00:13:14,840
encompass competency intelligence

315
00:13:13,640 --> 00:13:17,089
leadership<font color="#CCCCCC"> warrant</font>

316
00:13:14,840 --> 00:13:19,490
salary strength<font color="#E5E5E5"> popularity slurs</font>

317
00:13:17,089 --> 00:13:21,579
<font color="#CCCCCC">demonization and</font><font color="#E5E5E5"> race again we have</font>

318
00:13:19,490 --> 00:13:23,959
competency being violent crime

319
00:13:21,579 --> 00:13:26,269
dehumanization<font color="#E5E5E5"> we have religions</font>

320
00:13:23,959 --> 00:13:28,609
countries and politics and stereotypes

321
00:13:26,269 --> 00:13:30,499
for them<font color="#CCCCCC"> than stereotypes for smoking</font>

322
00:13:28,610 --> 00:13:33,259
drugs and alcohol<font color="#E5E5E5"> fat versus skinny</font>

323
00:13:30,499 --> 00:13:36,399
people<font color="#CCCCCC"> old and young people people with</font>

324
00:13:33,259 --> 00:13:39,649
disabilities and sexual orientation and

325
00:13:36,399 --> 00:13:41,660
this leaves us with<font color="#E5E5E5"> many open problems</font>

326
00:13:39,649 --> 00:13:44,149
in<font color="#E5E5E5"> this area for</font><font color="#CCCCCC"> example how much</font>

327
00:13:41,660 --> 00:13:46,969
machine learning experience<font color="#E5E5E5"> or expertise</font>

328
00:13:44,149 --> 00:13:50,449
do we need for algorithmic transparency

329
00:13:46,970 --> 00:13:53,180
<font color="#E5E5E5">and how</font><font color="#CCCCCC"> can we mitigate bias</font><font color="#E5E5E5"> while we</font>

330
00:13:50,449 --> 00:13:55,699
are still<font color="#E5E5E5"> preserving the utility</font><font color="#CCCCCC"> of the</font>

331
00:13:53,180 --> 00:13:58,248
models<font color="#CCCCCC"> that we are using and how long</font>

332
00:13:55,699 --> 00:14:02,329
does bias<font color="#E5E5E5"> persistent models for example</font>

333
00:13:58,249 --> 00:14:04,519
once you introduce bias<font color="#E5E5E5"> in your model is</font>

334
00:14:02,329 --> 00:14:07,489
it going to cause a snowball<font color="#CCCCCC"> effect and</font>

335
00:14:04,519 --> 00:14:09,529
<font color="#E5E5E5">upcoming models will observe the</font>

336
00:14:07,490 --> 00:14:12,999
previous bias<font color="#E5E5E5"> and end up with having</font>

337
00:14:09,529 --> 00:14:15,860
even larger bias<font color="#CCCCCC"> and what should</font>

338
00:14:12,999 --> 00:14:18,769
<font color="#CCCCCC">policymakers do to stop discrimination</font>

339
00:14:15,860 --> 00:14:20,660
<font color="#E5E5E5">we saw the example of predictive policy</font>

340
00:14:18,769 --> 00:14:22,550
and we know that machine<font color="#CCCCCC"> learning</font>

341
00:14:20,660 --> 00:14:25,459
services are<font color="#E5E5E5"> just</font><font color="#CCCCCC"> increasing and like</font>

342
00:14:22,550 --> 00:14:27,829
being used by billions of people<font color="#E5E5E5"> every</font>

343
00:14:25,459 --> 00:14:30,979
<font color="#CCCCCC">day</font><font color="#E5E5E5"> there are all these new products</font><font color="#CCCCCC"> by</font>

344
00:14:27,829 --> 00:14:34,519
Google Amazon<font color="#CCCCCC"> many applications use</font><font color="#E5E5E5"> pre</font>

345
00:14:30,980 --> 00:14:37,069
<font color="#E5E5E5">training models what can be done so</font><font color="#CCCCCC"> that</font>

346
00:14:34,519 --> 00:14:40,370
<font color="#CCCCCC">the necessary</font><font color="#E5E5E5"> steps are taken to</font><font color="#CCCCCC"> make</font>

347
00:14:37,069 --> 00:14:43,969
<font color="#CCCCCC">sure</font><font color="#E5E5E5"> that these apps algorithms models</font>

348
00:14:40,370 --> 00:14:46,519
are being fair<font color="#E5E5E5"> and for updates on</font>

349
00:14:43,970 --> 00:14:48,649
updates on this project<font color="#CCCCCC"> you can check my</font>

350
00:14:46,519 --> 00:14:51,679
<font color="#CCCCCC">website I will be publishing or</font>

351
00:14:48,649 --> 00:14:54,439
releasing new results and we always<font color="#E5E5E5"> vlog</font>

352
00:14:51,679 --> 00:14:56,569
about<font color="#E5E5E5"> our new findings on our blog</font>

353
00:14:54,439 --> 00:14:58,879
freedom to tinker<font color="#E5E5E5"> thank you for</font>

354
00:14:56,569 --> 00:15:03,040
listening<font color="#E5E5E5"> and I would be happy</font><font color="#CCCCCC"> to</font>

355
00:14:58,879 --> 00:15:03,040
<font color="#E5E5E5">discuss any questions with you know</font>

356
00:15:04,320 --> 00:15:06,380
<font color="#E5E5E5">you</font>

