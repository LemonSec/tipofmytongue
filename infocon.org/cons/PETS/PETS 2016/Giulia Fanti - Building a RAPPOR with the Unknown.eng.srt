1
00:00:01,939 --> 00:00:06,839
hello everyone<font color="#E5E5E5"> I'm Julia and today I'm</font>

2
00:00:04,589 --> 00:00:09,719
going to<font color="#E5E5E5"> talk to you about how services</font>

3
00:00:06,839 --> 00:00:11,580
can learn aggregate statistics<font color="#E5E5E5"> about</font>

4
00:00:09,719 --> 00:00:14,340
<font color="#E5E5E5">their</font><font color="#CCCCCC"> users without</font><font color="#E5E5E5"> invading the</font>

5
00:00:11,580 --> 00:00:15,839
individual<font color="#E5E5E5"> privacy of their users and in</font>

6
00:00:14,340 --> 00:00:18,029
<font color="#E5E5E5">particular I'm going</font><font color="#CCCCCC"> to</font><font color="#E5E5E5"> focus on</font><font color="#CCCCCC"> a</font>

7
00:00:15,839 --> 00:00:21,330
system<font color="#CCCCCC"> called report that was deployed</font>

8
00:00:18,029 --> 00:00:22,830
by<font color="#E5E5E5"> Google recently so the idea is I'm</font>

9
00:00:21,330 --> 00:00:24,299
going<font color="#E5E5E5"> to talk about can</font><font color="#CCCCCC"> be generalized</font>

10
00:00:22,830 --> 00:00:26,669
beyond report but I'll present them in

11
00:00:24,300 --> 00:00:28,619
<font color="#E5E5E5">the context of report this was joint</font>

12
00:00:26,670 --> 00:00:32,780
work with<font color="#E5E5E5"> my collaborators at Google</font>

13
00:00:28,619 --> 00:00:35,010
<font color="#E5E5E5">vaso</font><font color="#CCCCCC"> Peter and move our</font><font color="#E5E5E5"> air links one</font>

14
00:00:32,780 --> 00:00:37,140
<font color="#CCCCCC">okay so many of you have</font><font color="#E5E5E5"> probably heard</font>

15
00:00:35,010 --> 00:00:39,360
<font color="#CCCCCC">about the recent</font><font color="#E5E5E5"> announcements from</font>

16
00:00:37,140 --> 00:00:41,489
Google and more<font color="#E5E5E5"> recently Apple</font><font color="#CCCCCC"> about how</font>

17
00:00:39,360 --> 00:00:43,860
they're launching programs to collect

18
00:00:41,489 --> 00:00:46,349
user data with differential<font color="#E5E5E5"> privacy</font>

19
00:00:43,860 --> 00:00:48,269
guarantees the idea here is<font color="#CCCCCC"> that</font><font color="#E5E5E5"> they</font>

20
00:00:46,350 --> 00:00:50,399
want<font color="#E5E5E5"> to learn some aggregate</font><font color="#CCCCCC"> statistics</font>

21
00:00:48,270 --> 00:00:54,870
about their user base<font color="#E5E5E5"> without learning</font>

22
00:00:50,399 --> 00:00:56,520
individual users data and we<font color="#CCCCCC"> all we</font>

23
00:00:54,870 --> 00:00:58,649
<font color="#E5E5E5">don't know that</font><font color="#CCCCCC"> much about</font><font color="#E5E5E5"> what exactly</font>

24
00:00:56,520 --> 00:01:01,140
Apple is<font color="#E5E5E5"> doing in the case of</font><font color="#CCCCCC"> Google the</font>

25
00:00:58,649 --> 00:01:03,840
<font color="#CCCCCC">setup looks something like this</font><font color="#E5E5E5"> you have</font>

26
00:01:01,140 --> 00:01:06,299
a<font color="#E5E5E5"> collection of users and an aggregator</font>

27
00:01:03,840 --> 00:01:08,250
<font color="#CCCCCC">so in this case Google</font><font color="#E5E5E5"> and these users</font>

28
00:01:06,299 --> 00:01:10,290
<font color="#CCCCCC">have some data</font><font color="#E5E5E5"> that's of</font><font color="#CCCCCC"> interest</font><font color="#E5E5E5"> to the</font>

29
00:01:08,250 --> 00:01:12,540
aggregator<font color="#E5E5E5"> so for example that data</font>

30
00:01:10,290 --> 00:01:16,920
might be<font color="#E5E5E5"> the</font><font color="#CCCCCC"> default browser home page</font>

31
00:01:12,540 --> 00:01:18,869
<font color="#E5E5E5">for</font><font color="#CCCCCC"> each user and so the aggregator is</font>

32
00:01:16,920 --> 00:01:20,790
interested<font color="#E5E5E5"> in learning some population</font>

33
00:01:18,869 --> 00:01:23,369
level statistics<font color="#E5E5E5"> like the distribution</font>

34
00:01:20,790 --> 00:01:26,640
of<font color="#E5E5E5"> these of this data without learning</font>

35
00:01:23,369 --> 00:01:28,380
individual users data elements so the

36
00:01:26,640 --> 00:01:30,930
idea<font color="#E5E5E5"> here</font><font color="#CCCCCC"> is that each user will add</font>

37
00:01:28,380 --> 00:01:34,229
some statistical<font color="#CCCCCC"> noise to their data in</font>

38
00:01:30,930 --> 00:01:36,659
such a<font color="#E5E5E5"> way that the noise masks the</font>

39
00:01:34,229 --> 00:01:39,360
underlying data piece so they'll

40
00:01:36,659 --> 00:01:41,369
generate some randomness and then send

41
00:01:39,360 --> 00:01:43,950
these randomized reports to the

42
00:01:41,369 --> 00:01:45,899
aggregator the aggregator<font color="#E5E5E5"> will then</font>

43
00:01:43,950 --> 00:01:47,790
apply<font color="#CCCCCC"> some post-processing to these</font>

44
00:01:45,899 --> 00:01:49,619
reports<font color="#CCCCCC"> and extract the desired</font>

45
00:01:47,790 --> 00:01:53,149
information<font color="#CCCCCC"> so in this case a</font>

46
00:01:49,619 --> 00:01:55,560
distribution<font color="#E5E5E5"> over the underlying strings</font>

47
00:01:53,149 --> 00:01:58,170
so notice here<font color="#E5E5E5"> that there are kind</font><font color="#CCCCCC"> of</font>

48
00:01:55,560 --> 00:02:00,270
<font color="#CCCCCC">two main components in this pipeline the</font>

49
00:01:58,170 --> 00:02:03,479
first is the data randomization phase

50
00:02:00,270 --> 00:02:05,729
and this<font color="#E5E5E5"> is choosing how to add noise</font><font color="#CCCCCC"> to</font>

51
00:02:03,479 --> 00:02:08,508
your<font color="#E5E5E5"> data and how much of</font><font color="#CCCCCC"> that</font><font color="#E5E5E5"> noise to</font>

52
00:02:05,729 --> 00:02:12,060
add and the second phase is actually

53
00:02:08,508 --> 00:02:12,959
<font color="#E5E5E5">extracting useful statistics from that</font>

54
00:02:12,060 --> 00:02:15,090
noise

55
00:02:12,960 --> 00:02:18,090
so in this talk I'm<font color="#CCCCCC"> going to focus</font>

56
00:02:15,090 --> 00:02:20,310
mainly on<font color="#CCCCCC"> Phase</font><font color="#E5E5E5"> two</font><font color="#CCCCCC"> so given a fixed</font>

57
00:02:18,090 --> 00:02:22,080
data randomization scheme<font color="#E5E5E5"> how can we</font>

58
00:02:20,310 --> 00:02:25,860
<font color="#E5E5E5">increase the</font><font color="#CCCCCC"> utility of this pipeline</font>

59
00:02:22,080 --> 00:02:28,140
<font color="#CCCCCC">and as I</font><font color="#E5E5E5"> mentioned earlier in particular</font>

60
00:02:25,860 --> 00:02:30,540
I'm<font color="#CCCCCC"> going to focus on the system</font><font color="#E5E5E5"> called</font>

61
00:02:28,140 --> 00:02:32,160
<font color="#E5E5E5">rapport which was recently deployed in</font>

62
00:02:30,540 --> 00:02:34,709
<font color="#E5E5E5">Chrome</font>

63
00:02:32,160 --> 00:02:36,720
<font color="#E5E5E5">and the idea here</font><font color="#CCCCCC"> is for</font><font color="#E5E5E5"> their data</font>

64
00:02:34,709 --> 00:02:39,650
randomization phase<font color="#E5E5E5"> they have a</font>

65
00:02:36,720 --> 00:02:41,790
differentially private mechanism for

66
00:02:39,650 --> 00:02:44,880
<font color="#E5E5E5">randomizing string valued</font><font color="#CCCCCC"> random</font>

67
00:02:41,790 --> 00:02:47,130
variables<font color="#CCCCCC"> and in the second</font><font color="#E5E5E5"> phase the</font>

68
00:02:44,880 --> 00:02:49,260
<font color="#CCCCCC">original report paper proposes a</font>

69
00:02:47,130 --> 00:02:51,269
<font color="#CCCCCC">decoding mechanism</font><font color="#E5E5E5"> that allows you to</font>

70
00:02:49,260 --> 00:02:56,220
<font color="#CCCCCC">learn marginal distributions over</font><font color="#E5E5E5"> these</font>

71
00:02:51,269 --> 00:02:59,100
strings<font color="#CCCCCC"> now there are two main problems</font>

72
00:02:56,220 --> 00:03:01,050
with rapport in its current<font color="#CCCCCC"> state the</font>

73
00:02:59,100 --> 00:03:03,900
first<font color="#E5E5E5"> problem is that</font><font color="#CCCCCC"> you can't estimate</font>

74
00:03:01,050 --> 00:03:05,700
joint distributions<font color="#E5E5E5"> so for example if I</font>

75
00:03:03,900 --> 00:03:09,989
want to<font color="#E5E5E5"> learn the Joint Distribution of</font>

76
00:03:05,700 --> 00:03:12,119
users home pages and also<font color="#CCCCCC"> the plugins</font>

77
00:03:09,989 --> 00:03:13,769
that<font color="#E5E5E5"> they have installed</font><font color="#CCCCCC"> the current</font>

78
00:03:12,120 --> 00:03:15,890
decoding<font color="#CCCCCC"> mechanism just</font><font color="#E5E5E5"> doesn't work</font>

79
00:03:13,769 --> 00:03:17,940
it'll<font color="#E5E5E5"> give you</font><font color="#CCCCCC"> the wrong distribution</font>

80
00:03:15,890 --> 00:03:20,940
<font color="#E5E5E5">but this is problematic because</font>

81
00:03:17,940 --> 00:03:23,250
oftentimes it's<font color="#E5E5E5"> the co-occurrence of</font>

82
00:03:20,940 --> 00:03:24,540
data that is indicative of something

83
00:03:23,250 --> 00:03:28,170
<font color="#E5E5E5">interesting</font><font color="#CCCCCC"> like</font><font color="#E5E5E5"> the emergence of</font>

84
00:03:24,540 --> 00:03:30,239
malware<font color="#E5E5E5"> and the second problem is</font><font color="#CCCCCC"> that</font>

85
00:03:28,170 --> 00:03:32,429
in order<font color="#CCCCCC"> for the current decoding</font>

86
00:03:30,239 --> 00:03:34,410
mechanism<font color="#CCCCCC"> to work</font><font color="#E5E5E5"> you have</font><font color="#CCCCCC"> to know the</font>

87
00:03:32,430 --> 00:03:36,750
underlying dictionary of possible

88
00:03:34,410 --> 00:03:39,030
strings<font color="#CCCCCC"> so for example if I'm</font><font color="#E5E5E5"> trying to</font>

89
00:03:36,750 --> 00:03:41,280
learn<font color="#E5E5E5"> a distribution of plugins</font><font color="#CCCCCC"> I need</font>

90
00:03:39,030 --> 00:03:44,850
to have a list of<font color="#E5E5E5"> every possible</font><font color="#CCCCCC"> plug-in</font>

91
00:03:41,280 --> 00:03:46,290
<font color="#E5E5E5">that could be in my users data set and</font>

92
00:03:44,850 --> 00:03:48,420
<font color="#E5E5E5">that's going to be challenging</font><font color="#CCCCCC"> because</font>

93
00:03:46,290 --> 00:03:49,828
<font color="#CCCCCC">oftentimes</font><font color="#E5E5E5"> these data sets are changing</font>

94
00:03:48,420 --> 00:03:54,958
over<font color="#CCCCCC"> time people</font><font color="#E5E5E5"> are building new</font>

95
00:03:49,829 --> 00:03:57,750
plugins<font color="#E5E5E5"> and so forth so in this paper</font><font color="#CCCCCC"> we</font>

96
00:03:54,959 --> 00:04:00,420
tackled<font color="#E5E5E5"> these both of these</font><font color="#CCCCCC"> problems so</font>

97
00:03:57,750 --> 00:04:02,930
in the first<font color="#E5E5E5"> part we present a toolbox</font>

98
00:04:00,420 --> 00:04:05,220
for<font color="#E5E5E5"> estimating joint distributions over</font>

99
00:04:02,930 --> 00:04:07,350
variables that are collected using<font color="#E5E5E5"> the</font>

100
00:04:05,220 --> 00:04:09,600
<font color="#CCCCCC">report mechanism</font><font color="#E5E5E5"> and we use the</font>

101
00:04:07,350 --> 00:04:11,130
<font color="#E5E5E5">expectation maximization algorithm to do</font>

102
00:04:09,600 --> 00:04:12,989
this and we also have a few other

103
00:04:11,130 --> 00:04:16,260
statistical tests which I won't<font color="#E5E5E5"> talk</font>

104
00:04:12,989 --> 00:04:18,478
about<font color="#CCCCCC"> today for the sake of time</font><font color="#E5E5E5"> and in</font>

105
00:04:16,260 --> 00:04:21,029
part<font color="#CCCCCC"> two we proposed an algorithm for</font>

106
00:04:18,478 --> 00:04:23,430
<font color="#E5E5E5">estimating distributions when you don't</font>

107
00:04:21,029 --> 00:04:25,559
know<font color="#E5E5E5"> the</font><font color="#CCCCCC"> underlying</font><font color="#E5E5E5"> data dictionary and</font>

108
00:04:23,430 --> 00:04:25,800
the intuition<font color="#CCCCCC"> for how</font><font color="#E5E5E5"> we do this</font>

109
00:04:25,560 --> 00:04:27,930
is

110
00:04:25,800 --> 00:04:29,970
by<font color="#E5E5E5"> first splitting the strings of</font>

111
00:04:27,930 --> 00:04:32,520
interest<font color="#E5E5E5"> into engrams or shorter</font>

112
00:04:29,970 --> 00:04:34,770
substrings<font color="#CCCCCC"> and this is useful</font><font color="#E5E5E5"> because we</font>

113
00:04:32,520 --> 00:04:35,849
can enumerate<font color="#E5E5E5"> every possible Engram as</font>

114
00:04:34,770 --> 00:04:37,560
long as it's short enough

115
00:04:35,849 --> 00:04:41,250
and that's still computationally

116
00:04:37,560 --> 00:04:43,139
feasible<font color="#CCCCCC"> and so then with these shorter</font>

117
00:04:41,250 --> 00:04:45,150
sub strings<font color="#CCCCCC"> we can learn the Joint</font>

118
00:04:43,139 --> 00:04:48,990
Distribution between<font color="#E5E5E5"> these sub strings</font>

119
00:04:45,150 --> 00:04:50,669
using<font color="#E5E5E5"> the toolbox from part one and once</font>

120
00:04:48,990 --> 00:04:54,300
we have these joint distributions we can

121
00:04:50,669 --> 00:04:56,219
then back out<font color="#E5E5E5"> a data dictionary so let's</font>

122
00:04:54,300 --> 00:04:58,500
get<font color="#E5E5E5"> started with part one estimating</font>

123
00:04:56,220 --> 00:05:00,990
joint distributions<font color="#E5E5E5"> so just to be</font>

124
00:04:58,500 --> 00:05:02,759
perfectly<font color="#E5E5E5"> clear</font><font color="#CCCCCC"> here the the problem is</font>

125
00:05:00,990 --> 00:05:03,509
you once again have<font color="#E5E5E5"> these users</font><font color="#CCCCCC"> and the</font>

126
00:05:02,759 --> 00:05:06,500
aggregator

127
00:05:03,509 --> 00:05:09,990
but now the users might have<font color="#CCCCCC"> multiple</font>

128
00:05:06,500 --> 00:05:12,090
pieces of<font color="#E5E5E5"> data so here we have</font><font color="#CCCCCC"> a</font><font color="#E5E5E5"> home</font>

129
00:05:09,990 --> 00:05:15,360
<font color="#E5E5E5">page and let's say the last</font><font color="#CCCCCC"> plug-in</font><font color="#E5E5E5"> that</font>

130
00:05:12,090 --> 00:05:17,669
<font color="#E5E5E5">each user installed and each user is</font>

131
00:05:15,360 --> 00:05:19,590
going to independently randomized both

132
00:05:17,669 --> 00:05:21,900
of these<font color="#E5E5E5"> pieces of data using</font><font color="#CCCCCC"> the</font>

133
00:05:19,590 --> 00:05:25,109
<font color="#E5E5E5">rapport mechanism</font><font color="#CCCCCC"> just as</font><font color="#E5E5E5"> black box as</font>

134
00:05:21,900 --> 00:05:28,289
it is and send those<font color="#E5E5E5"> randomized reports</font>

135
00:05:25,110 --> 00:05:30,449
to the aggregator<font color="#E5E5E5"> and now the aggregator</font>

136
00:05:28,289 --> 00:05:32,789
wants to apply<font color="#E5E5E5"> some post-processing to</font>

137
00:05:30,449 --> 00:05:34,889
these noisy reports and recover the

138
00:05:32,789 --> 00:05:37,889
<font color="#CCCCCC">Joint Distribution</font><font color="#E5E5E5"> of both variables or</font>

139
00:05:34,889 --> 00:05:39,509
an arbitrary number<font color="#CCCCCC"> of variables</font><font color="#E5E5E5"> and so</font>

140
00:05:37,889 --> 00:05:41,310
the challenge here<font color="#CCCCCC"> is designing</font><font color="#E5E5E5"> that</font>

141
00:05:39,509 --> 00:05:42,630
<font color="#CCCCCC">post-processing algorithm what should we</font>

142
00:05:41,310 --> 00:05:45,659
do in order<font color="#CCCCCC"> to learn that Joint</font>

143
00:05:42,630 --> 00:05:47,610
<font color="#CCCCCC">Distribution</font><font color="#E5E5E5"> so as I mentioned our</font>

144
00:05:45,659 --> 00:05:49,830
approach for dealing<font color="#CCCCCC"> with this is</font><font color="#E5E5E5"> using</font>

145
00:05:47,610 --> 00:05:51,900
the expectation maximization algorithm

146
00:05:49,830 --> 00:05:53,940
or<font color="#CCCCCC"> the</font><font color="#E5E5E5"> e/m algorithm and this is</font>

147
00:05:51,900 --> 00:05:56,219
commonly<font color="#E5E5E5"> used to learn parameters in</font>

148
00:05:53,940 --> 00:05:59,310
cases where you<font color="#CCCCCC"> have</font><font color="#E5E5E5"> latent variables in</font>

149
00:05:56,219 --> 00:06:03,000
this<font color="#E5E5E5"> case</font><font color="#CCCCCC"> the latent variables are the</font>

150
00:05:59,310 --> 00:06:05,279
ground truth<font color="#CCCCCC"> so each user's homepage and</font>

151
00:06:03,000 --> 00:06:06,690
the last<font color="#CCCCCC"> plug-in</font><font color="#E5E5E5"> that they installed we</font>

152
00:06:05,279 --> 00:06:10,770
don't get to observe<font color="#CCCCCC"> those we only</font>

153
00:06:06,690 --> 00:06:12,650
observe<font color="#CCCCCC"> a noisy version</font><font color="#E5E5E5"> of that so the</font>

154
00:06:10,770 --> 00:06:15,389
<font color="#CCCCCC">way this works</font><font color="#E5E5E5"> is we can write out the</font>

155
00:06:12,650 --> 00:06:19,739
this<font color="#E5E5E5"> posterior probability that's shown</font>

156
00:06:15,389 --> 00:06:23,069
up above and here the x and y are<font color="#E5E5E5"> ground</font>

157
00:06:19,740 --> 00:06:24,719
truth which are unobserved<font color="#CCCCCC"> x</font><font color="#E5E5E5"> prime and</font><font color="#CCCCCC"> y</font>

158
00:06:23,069 --> 00:06:27,860
prime are<font color="#E5E5E5"> the noisy reports that are</font>

159
00:06:24,719 --> 00:06:30,719
observed<font color="#E5E5E5"> by the aggregator and the pIJ</font>

160
00:06:27,860 --> 00:06:32,159
<font color="#E5E5E5">parameters are the values of</font><font color="#CCCCCC"> the</font>

161
00:06:30,719 --> 00:06:33,449
contingency table<font color="#CCCCCC"> or the joint</font>

162
00:06:32,159 --> 00:06:36,509
<font color="#E5E5E5">distribution which</font><font color="#CCCCCC"> is what we're trying</font>

163
00:06:33,449 --> 00:06:39,150
<font color="#CCCCCC">to estimate</font><font color="#E5E5E5"> and notice that the quantity</font>

164
00:06:36,509 --> 00:06:41,820
inside this box<font color="#E5E5E5"> can</font><font color="#CCCCCC"> be computed exact</font>

165
00:06:39,150 --> 00:06:45,419
we using the<font color="#CCCCCC"> rapport system parameters</font>

166
00:06:41,820 --> 00:06:47,010
and the observed data<font color="#E5E5E5"> so if we knew</font>

167
00:06:45,420 --> 00:06:49,650
these P I J's we could write this

168
00:06:47,010 --> 00:06:52,560
posterior<font color="#E5E5E5"> probability exactly but we</font>

169
00:06:49,650 --> 00:06:54,810
don't so what we're going to do is use a

170
00:06:52,560 --> 00:06:59,880
<font color="#E5E5E5">current estimate of the P IJ so we'll</font>

171
00:06:54,810 --> 00:07:01,740
call<font color="#E5E5E5"> it P IJ hat and estimate the value</font>

172
00:06:59,880 --> 00:07:03,900
<font color="#CCCCCC">of this</font><font color="#E5E5E5"> posterior probability</font><font color="#CCCCCC"> that's</font><font color="#E5E5E5"> the</font>

173
00:07:01,740 --> 00:07:06,270
first step of the algorithm<font color="#E5E5E5"> then in the</font>

174
00:07:03,900 --> 00:07:08,640
second<font color="#E5E5E5"> step we update our estimate of</font>

175
00:07:06,270 --> 00:07:12,539
<font color="#CCCCCC">the</font><font color="#E5E5E5"> Joint Distribution by averaging over</font>

176
00:07:08,640 --> 00:07:15,900
all<font color="#E5E5E5"> of the data points and and notice</font>

177
00:07:12,540 --> 00:07:17,580
that the argument of<font color="#CCCCCC"> this summation</font><font color="#E5E5E5"> is</font>

178
00:07:15,900 --> 00:07:19,560
just what we calculated<font color="#E5E5E5"> in the first</font>

179
00:07:17,580 --> 00:07:22,080
step and<font color="#CCCCCC"> you can</font><font color="#E5E5E5"> repeat this until</font>

180
00:07:19,560 --> 00:07:23,850
convergence<font color="#E5E5E5"> and the details of</font><font color="#CCCCCC"> this</font>

181
00:07:22,080 --> 00:07:25,800
<font color="#E5E5E5">aren't too important</font><font color="#CCCCCC"> but the point is</font>

182
00:07:23,850 --> 00:07:28,320
it's it's<font color="#E5E5E5"> pretty easy to show that</font><font color="#CCCCCC"> this</font>

183
00:07:25,800 --> 00:07:31,080
algorithm converges to the maximum

184
00:07:28,320 --> 00:07:33,659
likelihood estimate of our<font color="#CCCCCC"> joint</font>

185
00:07:31,080 --> 00:07:36,270
<font color="#CCCCCC">distribution and furthermore</font><font color="#E5E5E5"> that</font>

186
00:07:33,660 --> 00:07:37,680
estimate is<font color="#E5E5E5"> asymptotically unbiased</font><font color="#CCCCCC"> so</font>

187
00:07:36,270 --> 00:07:39,630
we have some<font color="#E5E5E5"> theoretical guarantees that</font>

188
00:07:37,680 --> 00:07:42,600
this estimator is giving us a good

189
00:07:39,630 --> 00:07:46,320
approximation<font color="#E5E5E5"> of our true Joint</font>

190
00:07:42,600 --> 00:07:49,290
<font color="#E5E5E5">Distribution all right so now we can</font><font color="#CCCCCC"> use</font>

191
00:07:46,320 --> 00:07:52,670
this for<font color="#CCCCCC"> the second part of that problem</font>

192
00:07:49,290 --> 00:07:55,560
<font color="#E5E5E5">which is learning unknown dictionaries</font>

193
00:07:52,670 --> 00:07:57,450
here the problem is we have<font color="#CCCCCC"> this</font>

194
00:07:55,560 --> 00:08:00,540
distribution<font color="#E5E5E5"> that we're trying to learn</font>

195
00:07:57,450 --> 00:08:03,450
but the aggregator doesn't know which

196
00:08:00,540 --> 00:08:05,400
strings are in the data dictionary<font color="#CCCCCC"> and</font>

197
00:08:03,450 --> 00:08:08,460
as I<font color="#E5E5E5"> mentioned earlier if that's the</font>

198
00:08:05,400 --> 00:08:10,650
case the current decoding mechanism in

199
00:08:08,460 --> 00:08:14,010
<font color="#CCCCCC">the</font><font color="#E5E5E5"> original report paper just fails it</font>

200
00:08:10,650 --> 00:08:17,460
gives you the wrong distribution so the

201
00:08:14,010 --> 00:08:20,460
approach that<font color="#E5E5E5"> we use to solve this solve</font>

202
00:08:17,460 --> 00:08:22,349
this issue is to divide strings into sub

203
00:08:20,460 --> 00:08:24,960
strings or engrams<font color="#E5E5E5"> so here for example</font>

204
00:08:22,350 --> 00:08:26,960
<font color="#CCCCCC">I've divided the string</font><font color="#E5E5E5"> into by grams or</font>

205
00:08:24,960 --> 00:08:30,020
sub strings of length<font color="#CCCCCC"> 2</font>

206
00:08:26,960 --> 00:08:33,390
now what each user is<font color="#E5E5E5"> going to do is</font>

207
00:08:30,020 --> 00:08:35,189
start<font color="#E5E5E5"> with its original</font><font color="#CCCCCC"> string</font><font color="#E5E5E5"> so for</font>

208
00:08:33,390 --> 00:08:37,620
<font color="#E5E5E5">example for user</font><font color="#CCCCCC"> 1 the original string</font>

209
00:08:35,190 --> 00:08:40,950
is<font color="#E5E5E5"> rabbit and then it's going</font><font color="#CCCCCC"> to choose</font>

210
00:08:37,620 --> 00:08:44,100
uniformly at random<font color="#CCCCCC"> two of the engrams</font>

211
00:08:40,950 --> 00:08:46,830
<font color="#CCCCCC">from its string so user 1 chose engrams</font>

212
00:08:44,100 --> 00:08:49,890
<font color="#E5E5E5">1 and 3 which correspond to strings are</font>

213
00:08:46,830 --> 00:08:51,779
<font color="#E5E5E5">a and I T and similarly for all the rest</font>

214
00:08:49,890 --> 00:08:54,120
of the users

215
00:08:51,779 --> 00:08:57,600
these<font color="#E5E5E5"> users</font><font color="#CCCCCC"> are going</font><font color="#E5E5E5"> to randomize each</font>

216
00:08:54,120 --> 00:09:00,209
of these three strings<font color="#CCCCCC"> that</font><font color="#E5E5E5"> rabbit</font><font color="#CCCCCC"> RA</font>

217
00:08:57,600 --> 00:09:01,680
and<font color="#E5E5E5"> I T and they're also and they're</font>

218
00:09:00,209 --> 00:09:04,079
going to send those<font color="#CCCCCC"> to the aggregator</font>

219
00:09:01,680 --> 00:09:06,628
<font color="#E5E5E5">along with the indices</font><font color="#CCCCCC"> of the substrings</font>

220
00:09:04,079 --> 00:09:10,589
<font color="#CCCCCC">or of the engrams that they chose and</font>

221
00:09:06,629 --> 00:09:11,970
those indices are sent in plain text<font color="#CCCCCC"> all</font>

222
00:09:10,589 --> 00:09:13,680
<font color="#E5E5E5">right and for those</font><font color="#CCCCCC"> of you who are</font>

223
00:09:11,970 --> 00:09:16,410
<font color="#E5E5E5">familiar with differential privacy we</font>

224
00:09:13,680 --> 00:09:19,378
still need to maintain or come within

225
00:09:16,410 --> 00:09:21,059
our privacy budget of epsilon and so we

226
00:09:19,379 --> 00:09:23,639
dealt<font color="#E5E5E5"> with that by just splitting the</font>

227
00:09:21,059 --> 00:09:26,430
privacy budget evenly among these three

228
00:09:23,639 --> 00:09:30,689
<font color="#E5E5E5">strings that each user sending to to the</font>

229
00:09:26,430 --> 00:09:33,870
adversary<font color="#CCCCCC"> all right</font><font color="#E5E5E5"> so not an adversary</font>

230
00:09:30,689 --> 00:09:37,589
<font color="#E5E5E5">aggregator sorry</font><font color="#CCCCCC"> okay so now the</font>

231
00:09:33,870 --> 00:09:42,149
aggregator has from each user a noisy

232
00:09:37,589 --> 00:09:45,120
<font color="#E5E5E5">report of the users full string and too</font>

233
00:09:42,149 --> 00:09:47,399
noisy reports from each<font color="#E5E5E5"> substring along</font>

234
00:09:45,120 --> 00:09:49,589
with the index of<font color="#E5E5E5"> that substring</font><font color="#CCCCCC"> all</font>

235
00:09:47,399 --> 00:09:51,420
<font color="#CCCCCC">right so what do they do well the first</font>

236
00:09:49,589 --> 00:09:56,009
thing that<font color="#E5E5E5"> the aggregator is going to do</font>

237
00:09:51,420 --> 00:09:59,029
is to split<font color="#E5E5E5"> up the data set into users</font>

238
00:09:56,009 --> 00:10:03,360
that reported<font color="#E5E5E5"> the same pair</font><font color="#CCCCCC"> of</font><font color="#E5E5E5"> engrams</font>

239
00:09:59,029 --> 00:10:05,459
<font color="#E5E5E5">so for example in our case we said that</font>

240
00:10:03,360 --> 00:10:07,680
the length of<font color="#E5E5E5"> the string was 6 so there</font>

241
00:10:05,459 --> 00:10:12,989
<font color="#E5E5E5">are three possible pairs of engrams</font><font color="#CCCCCC"> in</font>

242
00:10:07,680 --> 00:10:17,370
rows 1 & 2 2 &<font color="#E5E5E5"> 3 & 1</font><font color="#CCCCCC"> & 3 now for each</font>

243
00:10:12,990 --> 00:10:18,990
group of users<font color="#E5E5E5"> that is reporting the</font>

244
00:10:17,370 --> 00:10:21,930
same pair of<font color="#E5E5E5"> engrams</font>

245
00:10:18,990 --> 00:10:23,069
the the aggregator<font color="#CCCCCC"> is going to compute</font>

246
00:10:21,930 --> 00:10:25,529
the Joint Distribution

247
00:10:23,069 --> 00:10:29,550
<font color="#E5E5E5">between those engrams so here for</font>

248
00:10:25,529 --> 00:10:31,699
<font color="#E5E5E5">example</font><font color="#CCCCCC"> for engrams 1 & 2</font><font color="#E5E5E5"> here we have a</font>

249
00:10:29,550 --> 00:10:34,500
<font color="#CCCCCC">Joint Distribution</font><font color="#E5E5E5"> which places</font>

250
00:10:31,699 --> 00:10:37,410
probability mass at the co-occurrence<font color="#E5E5E5"> of</font>

251
00:10:34,500 --> 00:10:39,120
<font color="#E5E5E5">our</font><font color="#CCCCCC"> ABB which was the first</font><font color="#E5E5E5"> 4 letters</font><font color="#CCCCCC"> of</font>

252
00:10:37,410 --> 00:10:42,899
rabbit<font color="#E5E5E5"> one of the strings in our</font>

253
00:10:39,120 --> 00:10:44,279
dictionary<font color="#CCCCCC"> and H</font><font color="#E5E5E5"> erm which was the first</font>

254
00:10:42,899 --> 00:10:45,720
four letters<font color="#E5E5E5"> of</font><font color="#CCCCCC"> hermit</font><font color="#E5E5E5"> which was the</font>

255
00:10:44,279 --> 00:10:48,209
other string in our dictionary<font color="#CCCCCC"> and</font>

256
00:10:45,720 --> 00:10:51,569
notice here that it also<font color="#E5E5E5"> places some</font>

257
00:10:48,209 --> 00:10:53,099
mass at HEB<font color="#CCCCCC"> B which was</font><font color="#E5E5E5"> not any of the</font>

258
00:10:51,569 --> 00:10:56,279
strings in our dictionary<font color="#CCCCCC"> but this can</font>

259
00:10:53,100 --> 00:11:01,550
happen because of<font color="#E5E5E5"> the noise all right</font>

260
00:10:56,279 --> 00:11:01,550
<font color="#CCCCCC">and it does this for</font><font color="#E5E5E5"> each pairing of</font>

261
00:11:01,580 --> 00:11:06,030
<font color="#E5E5E5">all right so now that we've computed</font>

262
00:11:03,930 --> 00:11:10,109
these<font color="#E5E5E5"> joint distributions which was all</font>

263
00:11:06,030 --> 00:11:12,300
using the<font color="#CCCCCC"> tools</font><font color="#E5E5E5"> from part 1 we can</font>

264
00:11:10,110 --> 00:11:13,560
enumerate the list of possible strains

265
00:11:12,300 --> 00:11:16,199
that could have led<font color="#CCCCCC"> to these joint</font>

266
00:11:13,560 --> 00:11:18,989
distributions<font color="#E5E5E5"> so we do this by</font><font color="#CCCCCC"> writing</font>

267
00:11:16,200 --> 00:11:22,500
down<font color="#E5E5E5"> the engrams that were learned for</font>

268
00:11:18,990 --> 00:11:25,680
each<font color="#E5E5E5"> index by grams</font><font color="#CCCCCC"> one two</font><font color="#E5E5E5"> and</font><font color="#CCCCCC"> three</font>

269
00:11:22,500 --> 00:11:27,930
and<font color="#CCCCCC"> we</font><font color="#E5E5E5"> can think of these by grams as</font>

270
00:11:25,680 --> 00:11:30,810
nodes in a graph<font color="#E5E5E5"> where</font><font color="#CCCCCC"> the edges</font>

271
00:11:27,930 --> 00:11:32,729
represent co-occurrence<font color="#E5E5E5"> so for example</font>

272
00:11:30,810 --> 00:11:36,660
we<font color="#E5E5E5"> know</font><font color="#CCCCCC"> that from the Joint Distribution</font>

273
00:11:32,730 --> 00:11:40,410
of in grams one and two that are<font color="#CCCCCC"> ABB h</font>

274
00:11:36,660 --> 00:11:42,480
eb b and h erm all have co-occurrence<font color="#E5E5E5"> so</font>

275
00:11:40,410 --> 00:11:44,280
we can draw those<font color="#E5E5E5"> three arrows</font><font color="#CCCCCC"> and the</font>

276
00:11:42,480 --> 00:11:49,320
same for<font color="#CCCCCC"> bi grams two and three</font><font color="#E5E5E5"> and one</font>

277
00:11:44,280 --> 00:11:51,630
and three<font color="#E5E5E5"> all right so now we're trying</font>

278
00:11:49,320 --> 00:11:54,360
<font color="#E5E5E5">to recall we're trying to find the set</font>

279
00:11:51,630 --> 00:11:58,770
of<font color="#E5E5E5"> candidate strings and we can do this</font>

280
00:11:54,360 --> 00:12:00,780
by searching<font color="#E5E5E5"> for</font><font color="#CCCCCC"> cleats</font><font color="#E5E5E5"> in this graph</font><font color="#CCCCCC"> in</font>

281
00:11:58,770 --> 00:12:03,030
particular notice that this graph<font color="#E5E5E5"> is now</font>

282
00:12:00,780 --> 00:12:05,069
at a par<font color="#E5E5E5"> type graph where the partitions</font>

283
00:12:03,030 --> 00:12:08,699
are generated by the<font color="#E5E5E5"> index of your by</font>

284
00:12:05,070 --> 00:12:10,320
grant<font color="#E5E5E5"> yeah</font><font color="#CCCCCC"> your bigram</font><font color="#E5E5E5"> and so we can do</font>

285
00:12:08,700 --> 00:12:12,480
some<font color="#E5E5E5"> kind you can use whatever algorithm</font>

286
00:12:10,320 --> 00:12:15,840
<font color="#E5E5E5">you want but we want to find cliques</font>

287
00:12:12,480 --> 00:12:18,300
of size<font color="#CCCCCC"> K within this graph</font><font color="#E5E5E5"> and if we do</font>

288
00:12:15,840 --> 00:12:20,670
<font color="#E5E5E5">this on this</font><font color="#CCCCCC"> particular graph we can</font><font color="#E5E5E5"> get</font>

289
00:12:18,300 --> 00:12:22,770
out the strings<font color="#E5E5E5"> rabbit hermit and</font><font color="#CCCCCC"> habit</font>

290
00:12:20,670 --> 00:12:26,699
<font color="#E5E5E5">and of course one of</font><font color="#CCCCCC"> these does not</font>

291
00:12:22,770 --> 00:12:28,590
<font color="#E5E5E5">belong so now what do we do now we can</font>

292
00:12:26,700 --> 00:12:30,390
consider these strings as our data

293
00:12:28,590 --> 00:12:33,810
dictionary which we previously didn't

294
00:12:30,390 --> 00:12:36,210
have<font color="#E5E5E5"> and recall we still haven't used</font>

295
00:12:33,810 --> 00:12:39,449
those full string reports that<font color="#E5E5E5"> each user</font>

296
00:12:36,210 --> 00:12:44,610
<font color="#E5E5E5">sent where they randomized the full</font>

297
00:12:39,450 --> 00:12:46,920
string rabbit or hermit so<font color="#E5E5E5"> using just</font>

298
00:12:44,610 --> 00:12:48,780
<font color="#CCCCCC">the regular</font><font color="#E5E5E5"> report decoding</font><font color="#CCCCCC"> mechanism</font>

299
00:12:46,920 --> 00:12:51,030
that was already proposed in the

300
00:12:48,780 --> 00:12:53,280
original<font color="#E5E5E5"> paper you can</font><font color="#CCCCCC"> use this learned</font>

301
00:12:51,030 --> 00:12:57,240
dictionary to hopefully recover<font color="#E5E5E5"> the</font>

302
00:12:53,280 --> 00:12:59,069
desired<font color="#E5E5E5"> distribution so the question</font>

303
00:12:57,240 --> 00:13:00,870
then is<font color="#E5E5E5"> how how well does this work</font><font color="#CCCCCC"> and</font>

304
00:12:59,070 --> 00:13:02,720
<font color="#E5E5E5">the answer is you might expect as it</font>

305
00:13:00,870 --> 00:13:05,580
depends<font color="#CCCCCC"> on</font><font color="#E5E5E5"> a</font><font color="#CCCCCC"> number of system parameters</font>

306
00:13:02,720 --> 00:13:06,900
<font color="#CCCCCC">these include what kind of district what</font>

307
00:13:05,580 --> 00:13:09,300
kind<font color="#E5E5E5"> of distribution are you trying</font><font color="#CCCCCC"> to</font>

308
00:13:06,900 --> 00:13:12,339
estimate<font color="#E5E5E5"> how many</font><font color="#CCCCCC"> users do you have what</font>

309
00:13:09,300 --> 00:13:15,550
privacy parameters do you have

310
00:13:12,339 --> 00:13:18,370
<font color="#CCCCCC">a bunch of different things but</font><font color="#E5E5E5"> in</font>

311
00:13:15,550 --> 00:13:20,439
<font color="#E5E5E5">general we found that this approach</font><font color="#CCCCCC"> is</font>

312
00:13:18,370 --> 00:13:22,120
pretty good for<font color="#E5E5E5"> identifying the heavy</font>

313
00:13:20,439 --> 00:13:24,099
hitters in<font color="#CCCCCC"> a distribution</font><font color="#E5E5E5"> so if you have</font>

314
00:13:22,120 --> 00:13:26,620
a very skewed distribution<font color="#CCCCCC"> it will</font>

315
00:13:24,100 --> 00:13:28,660
identify the strings that carry<font color="#E5E5E5"> most of</font>

316
00:13:26,620 --> 00:13:29,860
<font color="#CCCCCC">the weight</font><font color="#E5E5E5"> however as I'll show</font><font color="#CCCCCC"> you</font><font color="#E5E5E5"> in</font>

317
00:13:28,660 --> 00:13:33,100
<font color="#CCCCCC">just a second there are</font><font color="#E5E5E5"> some</font><font color="#CCCCCC"> problems</font>

318
00:13:29,860 --> 00:13:35,980
<font color="#E5E5E5">here</font><font color="#CCCCCC"> so here I'm showing you a plot of</font>

319
00:13:33,100 --> 00:13:38,079
the Hellinger distance between the true

320
00:13:35,980 --> 00:13:40,300
<font color="#CCCCCC">distribution and the learned</font>

321
00:13:38,079 --> 00:13:42,849
distribution<font color="#E5E5E5"> for some simulations that</font>

322
00:13:40,300 --> 00:13:45,248
<font color="#E5E5E5">we ran where the underlying distribution</font>

323
00:13:42,850 --> 00:13:47,339
<font color="#E5E5E5">here is a power-law distribution</font><font color="#CCCCCC"> so it</font>

324
00:13:45,249 --> 00:13:50,019
is skewed<font color="#E5E5E5"> most of the mass is</font>

325
00:13:47,339 --> 00:13:54,249
represented by a few strings<font color="#E5E5E5"> but you do</font>

326
00:13:50,019 --> 00:13:56,410
also have a<font color="#E5E5E5"> heavy tail and these and</font>

327
00:13:54,249 --> 00:13:58,329
this<font color="#E5E5E5"> is shown as</font><font color="#CCCCCC"> a function of</font><font color="#E5E5E5"> the</font>

328
00:13:56,410 --> 00:14:00,939
<font color="#E5E5E5">number</font><font color="#CCCCCC"> of reports or</font><font color="#E5E5E5"> the number of users</font>

329
00:13:58,329 --> 00:14:03,008
<font color="#CCCCCC">and these</font><font color="#E5E5E5"> different these different</font>

330
00:14:00,939 --> 00:14:05,498
<font color="#E5E5E5">lines are representing</font><font color="#CCCCCC"> different privacy</font>

331
00:14:03,009 --> 00:14:07,809
parameters so this line on the far right

332
00:14:05,499 --> 00:14:10,360
the darkest purple<font color="#E5E5E5"> line represents a</font>

333
00:14:07,809 --> 00:14:11,769
privacy parameter of epsilon 1.5 so

334
00:14:10,360 --> 00:14:14,499
that's the highest privacy that<font color="#E5E5E5"> we</font>

335
00:14:11,769 --> 00:14:16,929
considered and notice that you have to

336
00:14:14,499 --> 00:14:20,589
have at<font color="#CCCCCC"> least a hundred thousand users</font>

337
00:14:16,929 --> 00:14:25,720
<font color="#E5E5E5">or reports in order to get reasonable</font>

338
00:14:20,589 --> 00:14:27,370
accuracy here so this is a little<font color="#E5E5E5"> bit</font>

339
00:14:25,720 --> 00:14:28,839
<font color="#E5E5E5">concerning</font><font color="#CCCCCC"> I mean that's totally</font><font color="#E5E5E5"> fine</font>

340
00:14:27,370 --> 00:14:30,730
for<font color="#CCCCCC"> a company like Google</font><font color="#E5E5E5"> but if you</font>

341
00:14:28,839 --> 00:14:33,069
want to extend<font color="#E5E5E5"> these kinds of tools to</font>

342
00:14:30,730 --> 00:14:34,829
startups or smaller companies this

343
00:14:33,069 --> 00:14:37,329
starts to<font color="#E5E5E5"> become challenging potentially</font>

344
00:14:34,829 --> 00:14:39,329
so the main<font color="#CCCCCC"> question we want to answer</font>

345
00:14:37,329 --> 00:14:42,819
is what what is<font color="#E5E5E5"> the source of Arab and</font>

346
00:14:39,329 --> 00:14:44,620
<font color="#CCCCCC">there two possibilities here</font><font color="#E5E5E5"> one is that</font>

347
00:14:42,819 --> 00:14:46,990
we're not<font color="#CCCCCC"> learning the dictionary well</font>

348
00:14:44,620 --> 00:14:48,819
enough<font color="#E5E5E5"> so we're just learning a subset</font>

349
00:14:46,990 --> 00:14:51,160
of this possible<font color="#E5E5E5"> strings in our data</font>

350
00:14:48,819 --> 00:14:52,689
dictionary<font color="#CCCCCC"> and the other</font><font color="#E5E5E5"> possibility is</font>

351
00:14:51,160 --> 00:14:54,429
<font color="#E5E5E5">that we're learning the right strings</font>

352
00:14:52,689 --> 00:14:57,490
but we're learning<font color="#E5E5E5"> the wrong</font>

353
00:14:54,429 --> 00:14:59,230
distribution over<font color="#E5E5E5"> those strings</font><font color="#CCCCCC"> and for</font>

354
00:14:57,490 --> 00:15:00,819
the sake of<font color="#E5E5E5"> time I won't show plots for</font>

355
00:14:59,230 --> 00:15:02,439
<font color="#E5E5E5">this but it turns out that the answer</font><font color="#CCCCCC"> is</font>

356
00:15:00,819 --> 00:15:04,329
<font color="#E5E5E5">that we're not learning enough strings</font>

357
00:15:02,439 --> 00:15:06,639
we're only learning<font color="#CCCCCC"> the heavy-hitters of</font>

358
00:15:04,329 --> 00:15:08,790
this distribution<font color="#CCCCCC"> and this is happening</font>

359
00:15:06,639 --> 00:15:10,839
<font color="#CCCCCC">because there's a lot of noise</font><font color="#E5E5E5"> in</font>

360
00:15:08,790 --> 00:15:12,870
estimating<font color="#E5E5E5"> those joint distributions</font>

361
00:15:10,839 --> 00:15:16,420
between the<font color="#CCCCCC"> anagrams</font>

362
00:15:12,870 --> 00:15:19,089
so just to wrap<font color="#E5E5E5"> up I presented to you</font>

363
00:15:16,420 --> 00:15:21,399
some tools for learning<font color="#E5E5E5"> joint</font>

364
00:15:19,089 --> 00:15:24,920
distributions between variables in their

365
00:15:21,399 --> 00:15:27,529
report mechanism<font color="#E5E5E5"> and we also proposed</font>

366
00:15:24,920 --> 00:15:29,060
reckon ISM for learning distributions

367
00:15:27,529 --> 00:15:31,459
when you don't know the underlying<font color="#E5E5E5"> data</font>

368
00:15:29,060 --> 00:15:32,810
dictionary<font color="#CCCCCC"> which seems to</font><font color="#E5E5E5"> work</font><font color="#CCCCCC"> pretty</font>

369
00:15:31,459 --> 00:15:34,339
well<font color="#E5E5E5"> for certain</font><font color="#CCCCCC"> classes of</font>

370
00:15:32,810 --> 00:15:36,160
distributions but there's<font color="#E5E5E5"> still a lot of</font>

371
00:15:34,339 --> 00:15:39,860
<font color="#E5E5E5">gaps and</font><font color="#CCCCCC"> a lot of</font><font color="#E5E5E5"> room for improvement</font>

372
00:15:36,160 --> 00:15:41,930
<font color="#CCCCCC">so if you're</font><font color="#E5E5E5"> interested we</font><font color="#CCCCCC"> have or</font>

373
00:15:39,860 --> 00:15:45,290
<font color="#E5E5E5">Google has published the code on github</font>

374
00:15:41,930 --> 00:15:46,609
and<font color="#E5E5E5"> it's a pretty transparent project so</font>

375
00:15:45,290 --> 00:15:50,529
I would encourage<font color="#CCCCCC"> you to</font><font color="#E5E5E5"> do any</font>

376
00:15:46,610 --> 00:15:50,529
<font color="#CCCCCC">follow-up work</font><font color="#E5E5E5"> thanks so much</font>

