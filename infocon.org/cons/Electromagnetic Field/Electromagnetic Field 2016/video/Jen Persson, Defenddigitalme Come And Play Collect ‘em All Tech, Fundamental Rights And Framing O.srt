1
00:00:00,000 --> 00:00:17,609
ok I'm welcome to our next next time for
this afternoon and today we have jen

2
00:00:17,609 --> 00:00:22,619
person who is the director of defending
General Lee and she'll be talking about

3
00:00:22,619 --> 00:00:30,240
fundamental rights and framing of future
in a talk called common play collect the

4
00:00:30,240 --> 00:00:30,900
mall

5
00:00:30,900 --> 00:00:41,010
thanks very much in thank you hello so
today if you saw the film the other

6
00:00:41,010 --> 00:00:45,629
night right Aaron spots about our spots
that internet on boy

7
00:00:46,200 --> 00:00:49,200
you might have seen this trailer and if
you haven't I hope you enjoy it

8
00:01:02,230 --> 00:01:05,979
yeah

9
00:01:08,470 --> 00:01:11,740
yeah

10
00:01:53,750 --> 00:01:58,790
right so I subtitle this talk

11
00:01:59,330 --> 00:02:05,420
fiction or future who controls are
present frame the future and what I want

12
00:02:05,420 --> 00:02:10,220
to talk to you mainly about is as a
coordinator of defendable me where a

13
00:02:10,220 --> 00:02:15,440
campaign group that is looking at the
protection of children's digital rights

14
00:02:15,440 --> 00:02:20,630
especially around data privacy with
regards to the National pupil database

15
00:02:20,630 --> 00:02:25,549
so children's data in schools and I
looked at the scenario that was a

16
00:02:25,550 --> 00:02:31,280
fictional future 20 22 children
restricted as to who to get into

17
00:02:31,280 --> 00:02:36,560
University the country of birth and
ethnicity data stops some from going to

18
00:02:36,560 --> 00:02:42,020
school at all and prospective employers
screen applicants lifetime web browsing

19
00:02:42,020 --> 00:02:42,860
history

20
00:02:42,860 --> 00:02:50,209
do you think this is a present a future
that we want to see

21
00:02:51,110 --> 00:02:59,060
fact or fiction twenty years ago Larry
Ellison was asked this question about

22
00:02:59,060 --> 00:03:04,280
how he saw the digital future of global
database oracle he said he thought it

23
00:03:04,280 --> 00:03:10,370
would exist and that we're going to
track everything now our campaign group

24
00:03:10,370 --> 00:03:15,530
looks specifically at Children's data
and we have become concerned about some

25
00:03:15,530 --> 00:03:20,720
of the data that is collected on a
national bait state basis in schools and

26
00:03:20,720 --> 00:03:26,359
the changes that are proposed today that
will start at the 2016-17 census

27
00:03:26,360 --> 00:03:32,690
that's the school sensors so from next
September country of birth for example

28
00:03:32,690 --> 00:03:40,489
will be collected from every child in
England currently some ethnicity some

29
00:03:40,489 --> 00:03:44,299
nationality data is already collected
that is going to be expanded to the

30
00:03:44,299 --> 00:03:47,360
rising tues and everybody up to age 19

31
00:03:47,360 --> 00:03:51,200
so when parents and children give their
data into schools

32
00:03:51,200 --> 00:03:55,220
what do they think happens to it well
some have told us in our research that

33
00:03:55,220 --> 00:03:56,370
they suspect

34
00:03:56,370 --> 00:04:00,810
some statistics are gathered and sent to
the Department of Education when they

35
00:04:00,810 --> 00:04:02,760
read the privacy notices in schools

36
00:04:02,760 --> 00:04:07,920
it says we share some of this data for
the purposes of education with third

37
00:04:07,920 --> 00:04:09,179
parties

38
00:04:09,180 --> 00:04:15,329
no people or parent or school has yet
told us that they understand what that

39
00:04:15,329 --> 00:04:22,680
means and how many third parties are
people data is currently shared with so

40
00:04:22,680 --> 00:04:23,880
fact or fiction

41
00:04:23,880 --> 00:04:31,260
this is framing our future who gave a
comment about some cornflakes getting to

42
00:04:31,260 --> 00:04:32,639
the top

43
00:04:32,639 --> 00:04:41,070
you might remember this one any good
guess

44
00:04:41,070 --> 00:04:45,900
first Johnson 20-30 addressing the
Center for Policy Studies London he was

45
00:04:45,900 --> 00:04:49,620
talking about IQ and children and
education and how he felt

46
00:04:50,130 --> 00:04:54,870
children would be restricted from what
they were able to learn not what was

47
00:04:54,870 --> 00:04:55,710
possible

48
00:04:55,710 --> 00:05:07,620
same year there was a discussion at the
Select Committee for education and a

49
00:05:07,620 --> 00:05:13,620
renowned professor of genetics and
genomics suggested that within five

50
00:05:13,620 --> 00:05:14,460
years

51
00:05:14,460 --> 00:05:19,320
that was in 2013 you can do the math
everyone would have the personal DNA

52
00:05:19,320 --> 00:05:23,490
sequence data chip and from that that
data will be used specifically in

53
00:05:23,490 --> 00:05:24,360
schools

54
00:05:24,360 --> 00:05:32,760
his idea in 2013 so these are current
concerts in how personal data is used in

55
00:05:32,760 --> 00:05:38,789
health and education and some ideas and
some policymakers and academics have for

56
00:05:38,789 --> 00:05:39,810
the future

57
00:05:39,810 --> 00:05:46,229
so how do we want that future to look
how will we will you as technologists as

58
00:05:46,229 --> 00:05:50,940
developers as policymakers as people in
civil society in government you as

59
00:05:50,940 --> 00:05:56,390
parents as pupils as people in society
what do you want the future to look

60
00:05:56,390 --> 00:06:01,520
because if we do not help frame it we
will not see the future that we want to

61
00:06:01,520 --> 00:06:07,820
achieve and i want to suggest three
things to you today that we need to

62
00:06:07,820 --> 00:06:12,980
frame our language differently that we
need to frame a legislation differently

63
00:06:12,980 --> 00:06:19,910
and that we need to frame out technology
differently because how we frame a

64
00:06:19,910 --> 00:06:26,000
conversation about data and how we frame
our legal boundaries today will change

65
00:06:26,000 --> 00:06:30,470
how that information is used about in
our future and it will shape our future

66
00:06:30,470 --> 00:06:37,670
if you saw the fantastic talk yesterday
and by dr. jeff barker she talked about

67
00:06:37,670 --> 00:06:42,380
cybersecurity myths and monsters and she
talked about the three principles of

68
00:06:42,380 --> 00:06:48,050
cyber security people policy and
technology that make data safe and those

69
00:06:48,050 --> 00:06:52,430
can get carried across not only in
around the concepts of cybersecurity but

70
00:06:52,430 --> 00:06:57,380
thinking in general about data and data
protection a lot of what we hear about

71
00:06:57,380 --> 00:07:04,280
today's future and data is about smart
smart city smart technologies all based

72
00:07:04,280 --> 00:07:07,460
on sensors on picking up more
information about you and me

73
00:07:08,120 --> 00:07:13,610
you may have read in the papers very
recently about how our oyster cards data

74
00:07:13,610 --> 00:07:18,170
would be used picked up from travel in
London and stored and research . and

75
00:07:18,170 --> 00:07:18,860
used

76
00:07:18,860 --> 00:07:25,220
you may have heard about how health data
was planned to be used in 2013 in the

77
00:07:25,220 --> 00:07:29,570
care data discussion how everyone's GP
records were to be extracted linked

78
00:07:29,570 --> 00:07:33,530
together with all the hospital records
in a central database and we used for a

79
00:07:33,530 --> 00:07:37,400
wide variety of secondary purposes
beyond our care

80
00:07:37,400 --> 00:07:43,130
there is lots of ideas about how data
should be used and could be used in the

81
00:07:43,130 --> 00:07:45,620
future but i want to ask

82
00:07:45,620 --> 00:07:50,150
not only do we need to think about smart
future we need to think about a wise

83
00:07:50,150 --> 00:07:51,200
future

84
00:07:51,200 --> 00:07:58,400
intelligence has to be being about
clever not just about cyber security and

85
00:07:58,400 --> 00:08:06,260
applications of sensor data so we need
to be intelligent in all senses about

86
00:08:06,260 --> 00:08:08,569
how we want to frame our future

87
00:08:08,569 --> 00:08:15,349
surveillance is a broad concept we need
to think about how our data is used on a

88
00:08:15,349 --> 00:08:19,878
day-to-day basis in order to make sure
that the data that is collected about us

89
00:08:19,879 --> 00:08:23,240
is used wisely and how do you want that
to be

90
00:08:23,240 --> 00:08:27,740
we need to protect public interest data
research

91
00:08:27,740 --> 00:08:33,799
most people would agree and the ipsos
mori poll and 2013 showed and engagement

92
00:08:33,799 --> 00:08:39,769
work with academic bodies have shown
most people support the use of data

93
00:08:39,769 --> 00:08:43,339
that's gathered about as by the
government in our public services

94
00:08:43,339 --> 00:08:49,399
whether it's in health or education or
tax or benefits or any other area of

95
00:08:49,399 --> 00:08:50,449
public service

96
00:08:50,449 --> 00:08:56,089
if it's used securely if we know how
it's being used and if it is not used in

97
00:08:56,089 --> 00:08:58,850
a commercial basis but in the public
interest

98
00:08:58,850 --> 00:09:03,889
it is trusted and it's trusted it will
be used well I want to show you a case

99
00:09:03,889 --> 00:09:08,600
study of a way i believe in our campaign
group believes it has not been used well

100
00:09:08,600 --> 00:09:15,709
today and that undermines the future of
our future public interest data use

101
00:09:19,850 --> 00:09:22,880
let's start with a language

102
00:09:22,880 --> 00:09:27,350
you've probably seen or heard most of
these terms used about personal data in

103
00:09:27,350 --> 00:09:33,709
the last few years we hear a lot about
data sharing in actual fact most of the

104
00:09:33,709 --> 00:09:38,268
time when we talk about data sharing it
means you have given your data to a

105
00:09:38,269 --> 00:09:42,470
public service to be used in a direct
way with you

106
00:09:42,470 --> 00:09:51,500
perhaps in a transaction with your tax
return or in a medical treatment all as

107
00:09:51,500 --> 00:09:54,529
well come back to giving your people's
data into schools

108
00:09:54,529 --> 00:10:00,800
I don't know about you but when I do
that I think my data is going to be used

109
00:10:00,800 --> 00:10:06,199
for the purposes i give it to the body
for that organization needs to manage my

110
00:10:06,199 --> 00:10:10,069
child's data in school because they need
to know who they are where they're going

111
00:10:10,069 --> 00:10:14,569
what they've done when I go to the
doctors expect them to use the data and

112
00:10:14,569 --> 00:10:17,569
give them in confidence and securely to
treat me

113
00:10:18,860 --> 00:10:24,770
that same data is being in color
increasingly used with the concept of

114
00:10:24,770 --> 00:10:31,040
data sharing that we should be expecting
that our data that we've given for one

115
00:10:31,040 --> 00:10:35,150
purpose is used for another and
secondary purposes by other government

116
00:10:35,150 --> 00:10:39,380
bodies and in public interest research
has said that gets widespread support

117
00:10:39,380 --> 00:10:44,720
but who else is it being shared with I
think we need to reclaim some of the

118
00:10:44,720 --> 00:10:48,800
language around privacy and the nothing
to fear nothing to hide

119
00:10:49,310 --> 00:10:56,839
motto around surveillance constantly we
are told the more data we have the more

120
00:10:56,840 --> 00:11:02,390
secure you'll be all the evidence shows
differently and if you got into a little

121
00:11:02,390 --> 00:11:05,569
bit later and have followed at all the
investigatory powers

122
00:11:05,570 --> 00:11:10,160
bill the current legislation that's
going through Parliament about how data

123
00:11:10,160 --> 00:11:12,620
will be used from our internet use

124
00:11:12,620 --> 00:11:17,720
you'll know that the way you use the
internet today will be very different if

125
00:11:17,720 --> 00:11:23,870
nothing changes in current legislation
by the next EMF so in two years time if

126
00:11:23,870 --> 00:11:28,760
you come to EMF camp and nothing has
changed in the proposals for the

127
00:11:28,760 --> 00:11:31,280
legislation for the investigatory powers
bill

128
00:11:31,280 --> 00:11:36,020
you will not be using the internet in
the same way you might not see any

129
00:11:36,020 --> 00:11:36,980
change

130
00:11:36,980 --> 00:11:41,270
but the change will have happened
because legislation is going to require

131
00:11:41,270 --> 00:11:47,930
everybody's internet use to be monitored
and stored by the isp the providers for

132
00:11:47,930 --> 00:11:48,829
a year at least

133
00:11:48,830 --> 00:11:52,400
and made available to the government
bodies

134
00:11:52,400 --> 00:11:57,680
so how do we look after data today

135
00:11:57,680 --> 00:12:01,339
well we have legislation the Data
Protection Act you will be familiar with

136
00:12:01,340 --> 00:12:06,470
to protect the identity of people from
exploitation effectively but I think we

137
00:12:06,470 --> 00:12:09,650
need to again to reframe the language
around data protection

138
00:12:09,650 --> 00:12:13,730
it's not the data itself that we just
need to protect that's perhaps the

139
00:12:13,730 --> 00:12:15,530
concept of cybersecurity

140
00:12:15,530 --> 00:12:20,870
how do you make sure the data is
maintained its integrity its quality and

141
00:12:20,870 --> 00:12:25,430
its security in one place but a lot of
the laws around data protection are

142
00:12:25,430 --> 00:12:27,319
actually to protect people

143
00:12:27,320 --> 00:12:30,720
they're not about protecting the
information they're protecting you

144
00:12:30,720 --> 00:12:34,889
me from how it would be potentially
misused potential harm

145
00:12:35,399 --> 00:12:39,120
so we need to always consider when we're
talking about personal data and data

146
00:12:39,120 --> 00:12:40,170
protection

147
00:12:40,170 --> 00:12:44,490
think of the people behind the data and
I'd like to reframe that discussion when

148
00:12:44,490 --> 00:12:46,290
we always talk about data protection

149
00:12:46,290 --> 00:12:49,290
remember we're also talking about people

150
00:12:50,279 --> 00:12:54,569
so some of our current legislation is
shifting we don't know what will happen

151
00:12:54,569 --> 00:12:59,550
now with EU data changing laws that are
coming in the general data protection

152
00:12:59,550 --> 00:13:03,870
regulation would have given some
significant changes i believe they still

153
00:13:03,870 --> 00:13:04,800
will

154
00:13:04,800 --> 00:13:09,990
it comes into effect in 2018 or rather
it's already been enacted but we need to

155
00:13:09,990 --> 00:13:15,149
see the changes actually made effective
in the UK so who knows what will happen

156
00:13:15,149 --> 00:13:20,490
with some of those changes but we do
know what has happened with some of the

157
00:13:20,490 --> 00:13:26,250
data uses that we have already seen in
the past and the recent studies and i'll

158
00:13:26,250 --> 00:13:31,589
show you a case study where the
boundaries of what legislation we had to

159
00:13:31,589 --> 00:13:37,980
protect our data to protect us and our
identities has moved and this is the

160
00:13:37,980 --> 00:13:39,360
core of our campaign

161
00:13:39,360 --> 00:13:44,579
we're campaigning to protect 20 million
children's identifiable data in the

162
00:13:44,579 --> 00:13:47,579
National people database and when I say
children

163
00:13:47,579 --> 00:13:54,809
if you're under 35 that means you this
data has been collected since 2000 its

164
00:13:54,809 --> 00:13:59,879
wide-ranging fully identifiable
everything on personal data that's given

165
00:13:59,879 --> 00:14:04,139
into schools the full attainment record
everything that is generated in schools

166
00:14:04,139 --> 00:14:06,720
and that is transferred through

167
00:14:06,720 --> 00:14:09,720
perhaps the local authority or directly
from the school depending on whether

168
00:14:09,720 --> 00:14:14,670
you're in a local authority academies or
other school set up its varied and and

169
00:14:14,670 --> 00:14:15,389
broad

170
00:14:15,389 --> 00:14:21,000
we're asking the Department for
Education to stop handing out individual

171
00:14:21,000 --> 00:14:26,550
children's identifiable personal data to
third parties because at the moment it

172
00:14:26,550 --> 00:14:28,620
goes outside of the database

173
00:14:28,620 --> 00:14:36,240
it's copied given to third parties and
so we have chunks of data of children's

174
00:14:36,240 --> 00:14:41,519
identifiable data sitting in various
places around the UK at the moment

175
00:14:42,130 --> 00:14:47,140
we want them to change that practice to
bring the data into a secure setting so

176
00:14:47,140 --> 00:14:52,150
whoever uses that data cannot walk away
with the information itself but uses it

177
00:14:52,150 --> 00:14:57,939
in a secure way for research for the
purposes it's been intended and then can

178
00:14:57,940 --> 00:15:02,080
walk away with their their research
results but not the data itself

179
00:15:02,860 --> 00:15:10,210
why because there's been 650 releases of
this data to third party since 2012

180
00:15:10,930 --> 00:15:15,790
you yourself can look up the releases on
the website that the department for

181
00:15:15,790 --> 00:15:22,120
education does publish and in May they
started to print to this web site this

182
00:15:22,120 --> 00:15:27,370
web link on the new revised privacy
notice that they give to schools which

183
00:15:27,370 --> 00:15:31,750
is supposed to go to every child to tell
us your parents

184
00:15:31,750 --> 00:15:38,320
what's happened and who uses their data
our research so far with in a small

185
00:15:38,320 --> 00:15:44,230
basis but with 75 schools is has come
back with even results like we don't

186
00:15:44,230 --> 00:15:47,290
give any data to the information to the
National people database

187
00:15:47,290 --> 00:15:52,540
we've never heard of it no we only we
only pass information to the schools

188
00:15:52,540 --> 00:15:57,430
that we're connected to in our area so
there is a complete breakdown between a

189
00:15:57,430 --> 00:16:00,609
privacy notice as being distributed at
national level

190
00:16:01,210 --> 00:16:07,930
- what is being actually understood on
the ground and used and implemented that

191
00:16:07,930 --> 00:16:12,550
is an example of legal boundaries
changing because in 2012

192
00:16:12,550 --> 00:16:18,880
the Education Act was changed to modify
who could use data and for what purposes

193
00:16:18,880 --> 00:16:25,090
the prescribed persons act 2009 was
updated to say these groups of people

194
00:16:25,090 --> 00:16:31,000
may now receive individual pupils data
from the National people database and

195
00:16:31,000 --> 00:16:33,460
the Education Act changed

196
00:16:33,460 --> 00:16:37,060
what they could use it for the purposes
they can be given data for for the

197
00:16:37,060 --> 00:16:39,910
purposes of Education and the well-being
of children

198
00:16:39,910 --> 00:16:43,990
now you could drive a bus through those
purposes because most people that could

199
00:16:43,990 --> 00:16:48,700
apply could in some way meet that
concept that department does have a

200
00:16:48,700 --> 00:16:52,300
process of application you have to apply

201
00:16:52,899 --> 00:16:56,920
and fill in a form and state the
purposes for which that data will be

202
00:16:56,920 --> 00:16:57,939
released

203
00:16:57,939 --> 00:17:03,790
we started to ask them in detail about
it last summer in July 2015 and at that

204
00:17:03,790 --> 00:17:11,260
time of the then 462 releases there had
been they had never done a single audit

205
00:17:11,260 --> 00:17:13,510
of any recipient

206
00:17:13,510 --> 00:17:19,209
so the data had been copied sent to the
recipient and SAT with them and at that

207
00:17:19,209 --> 00:17:21,279
time Department of Education didn't know

208
00:17:21,279 --> 00:17:25,179
did they ever delete it they are meant
to have it on license for a year or two

209
00:17:25,179 --> 00:17:26,020
perhaps

210
00:17:26,020 --> 00:17:29,379
how long have they kept it for what have
they done with it since

211
00:17:29,380 --> 00:17:32,860
is it still secure now those questions
have been asked because it hadn't been

212
00:17:32,860 --> 00:17:33,969
audited

213
00:17:33,970 --> 00:17:42,700
so we started asking some of the
recipients include Fleet Street Press

214
00:17:42,700 --> 00:17:47,080
The Telegraph for example received five
years worth

215
00:17:47,590 --> 00:17:52,000
that's approximately 10 million
children's data for the period of 12

216
00:17:52,000 --> 00:17:56,470
months we obtained this document three
freedom of information

217
00:17:58,299 --> 00:18:03,908
it's a publicly available and what do
they know they gave the assurance in the

218
00:18:03,909 --> 00:18:08,320
replication letter because at first
they'd ask not for tier 2 data which is

219
00:18:08,320 --> 00:18:10,539
identifying and sensitive data

220
00:18:10,539 --> 00:18:16,330
they'd ask for tier 1 data which is
identifying and highly sensitive all

221
00:18:16,330 --> 00:18:22,779
tears there are four are identifiable
data there is not an identity anonymous

222
00:18:22,779 --> 00:18:26,770
data doesn't doesn't come into this
we're talking only about identifiable

223
00:18:26,770 --> 00:18:32,470
children's data and they gave cast iron
assurances and none would be identified

224
00:18:32,470 --> 00:18:33,520
through the use of it

225
00:18:33,520 --> 00:18:36,730
well number be identified

226
00:18:36,730 --> 00:18:43,510
outside The Telegraph our contention
with this is that the data controlling

227
00:18:43,510 --> 00:18:48,039
the responsibility with the Department
of Education sits at the point of

228
00:18:48,039 --> 00:18:53,830
release from the Department of Education
we would argue that the Department of

229
00:18:53,830 --> 00:18:58,210
Education may have out sourced the
responsibility for what it considers

230
00:18:58,210 --> 00:19:01,870
research into some of the children's
attainment data and perhaps the

231
00:19:01,870 --> 00:19:03,719
Telegraph is using this for

232
00:19:03,720 --> 00:19:08,130
comparison tables and they publish
articles about it and so therefore they

233
00:19:08,130 --> 00:19:10,409
considered researchers

234
00:19:10,409 --> 00:19:14,490
I think you and I might disagree that
they weren't the same quality or bona

235
00:19:14,490 --> 00:19:18,240
fide a people that we expect to be
academic bona fide researchers in the

236
00:19:18,240 --> 00:19:21,240
public interest from say universities

237
00:19:21,900 --> 00:19:26,400
this data was handed out to the
telegraph and when we questioned it in

238
00:19:26,400 --> 00:19:27,150
20

239
00:19:27,150 --> 00:19:32,130
july 2015 it was confirmed they had
never deleted it

240
00:19:32,130 --> 00:19:35,789
they had not yet the department had not
yet received confirmation that it ever

241
00:19:35,789 --> 00:19:36,840
been deleted

242
00:19:36,840 --> 00:19:41,129
so it was intended to have been there
with 12 months and at the time it was

243
00:19:41,130 --> 00:19:43,919
about two years out of overdue and
nobody knew

244
00:19:43,919 --> 00:19:49,770
effectively what had happened to it no
Department argue that is you know

245
00:19:49,770 --> 00:19:55,020
securely monitored that they they have a
robust approvals process we think it is

246
00:19:55,020 --> 00:19:58,530
too much tighter and they need to be
auditing they need to be really

247
00:19:58,530 --> 00:19:59,460
questioning

248
00:19:59,460 --> 00:20:02,610
are these recipients who the public
expect their children's data are given

249
00:20:02,610 --> 00:20:09,840
to something else to look for the future
is the education technology market is

250
00:20:09,840 --> 00:20:12,928
expanding at a rapid rate of knots

251
00:20:12,929 --> 00:20:17,970
there's a huge amount of classroom data
collected today in apps and websites and

252
00:20:17,970 --> 00:20:22,230
parents and children are it invariably
signed up for them in schools without

253
00:20:22,230 --> 00:20:27,690
permission being asked at home and I can
even say from my own experience I know

254
00:20:27,690 --> 00:20:32,970
children get set up with with email
accounts get signed on to apps and we

255
00:20:32,970 --> 00:20:36,840
can start question know who is or owning
their personal data

256
00:20:36,840 --> 00:20:40,830
what consent of children giving really
and then not in a position to be able to

257
00:20:40,830 --> 00:20:45,570
give that and understand the
implications for companies such as this

258
00:20:45,570 --> 00:20:49,230
one that parents in america have had a
discussion about and i quote from

259
00:20:49,230 --> 00:20:56,070
they're very concerned that the types of
data are being used is gaming children

260
00:20:56,070 --> 00:20:59,639
effectively getting them to change their
behavior in the classroom and that's the

261
00:20:59,640 --> 00:21:04,980
intention of the app that is
quantifiable that they can report on it

262
00:21:04,980 --> 00:21:10,590
but that actually changing children's
behavior is not what technology should

263
00:21:10,590 --> 00:21:14,610
be used for in the classroom and this is
a group of parents who have have serious

264
00:21:14,610 --> 00:21:19,379
concerns about the direction of
Education data in the US and I know from

265
00:21:19,380 --> 00:21:23,520
from what we we seen you all to probably
better than i do and if you're in the

266
00:21:23,520 --> 00:21:26,970
education field at all that more is on
the way here

267
00:21:26,970 --> 00:21:30,510
so can we reframe the future

268
00:21:30,510 --> 00:21:33,539
well I think one of the things we can do
is really to understand if your policy

269
00:21:33,539 --> 00:21:37,830
make it a developer and any are anyone
who has an interest in technology and

270
00:21:37,830 --> 00:21:42,090
children's data look at its application
really try and understand how is your

271
00:21:42,090 --> 00:21:46,860
own or your children's data being used
and if you are in a position to do so

272
00:21:46,860 --> 00:21:51,840
ask policymakers what are they doing i'm
looking at the future landscape of

273
00:21:51,840 --> 00:21:54,000
legislation particularly around the EU

274
00:21:54,000 --> 00:21:58,919
we also need to consider the bigger
picture of legislative changes in the UK

275
00:21:58,919 --> 00:22:00,809
that are potentially on the horizon

276
00:22:00,809 --> 00:22:05,490
the importance of the Human Rights Act
is continually undermined

277
00:22:05,490 --> 00:22:10,590
I personally believe in the media and we
have heard repeatedly that there is a

278
00:22:10,590 --> 00:22:15,899
proposal to replace the Human Rights Act
with a bill of rights Human Rights Act

279
00:22:15,899 --> 00:22:19,439
officers protection and privacy along
with all the other principals the

280
00:22:19,440 --> 00:22:22,830
fundamental rights that apply to
everyone equally

281
00:22:23,429 --> 00:22:29,309
and we believe they should be protected
we need to look also at what's called

282
00:22:29,309 --> 00:22:32,820
the digital economy bill if you're
interested in policy i would recommend

283
00:22:32,820 --> 00:22:35,760
you look it up if you haven't already
been familiar with it

284
00:22:35,760 --> 00:22:40,799
the wording around data sharing and data
linkage and d identified data and

285
00:22:40,799 --> 00:22:46,679
anonymous data is all used rather
interchangeably public interest research

286
00:22:46,679 --> 00:22:52,559
and statistics and improving how our
national statistics are used has been

287
00:22:52,559 --> 00:22:54,629
conflated into this bill

288
00:22:54,630 --> 00:22:59,250
there are very separate distinct
sections of this legislation one big

289
00:22:59,250 --> 00:23:03,240
chunk of it is around using public
services data in a very different way

290
00:23:03,240 --> 00:23:07,890
from today and releasing everybody's
administer today to date that you give

291
00:23:07,890 --> 00:23:12,659
in for users bye-bye services for your
direct care by a wide range of public

292
00:23:12,659 --> 00:23:13,380
bodies

293
00:23:13,380 --> 00:23:18,299
if that concerns you look it up the
digital economy bill coming in we start

294
00:23:18,299 --> 00:23:19,950
to be debated in the autumn

295
00:23:19,950 --> 00:23:25,590
we need to protect public interest
research and the trust of the public

296
00:23:25,590 --> 00:23:31,559
as in good quality Public Interest
Research to allow it to be undermined by

297
00:23:31,559 --> 00:23:36,418
misuse or seemingly too broad use by
others is a risk

298
00:23:36,419 --> 00:23:40,470
I don't think that that hasn't been
properly adequately considered and we

299
00:23:40,470 --> 00:23:45,690
should be speaking up about the
investigatory powers will if you already

300
00:23:45,690 --> 00:23:52,620
aware of again as I said if we see no
changes in this legislation proposed by

301
00:23:52,620 --> 00:23:56,699
the next EMF camp this data
communications data

302
00:23:56,700 --> 00:24:01,620
what's known as metadata would be
recorded about every use of the internet

303
00:24:01,620 --> 00:24:04,860
that you and I and our children have in
the UK

304
00:24:04,860 --> 00:24:09,299
all of these individual items the
websites visited the billing data the

305
00:24:09,299 --> 00:24:11,490
location username and password

306
00:24:11,490 --> 00:24:15,990
I'll leave you to look at some of those
if you're more interested in this aspect

307
00:24:15,990 --> 00:24:18,570
of how data is used in surveillance

308
00:24:18,570 --> 00:24:24,928
i recommend you look at liberty privacy
international org and if you're very

309
00:24:24,929 --> 00:24:26,700
short of time but interested a bit

310
00:24:26,700 --> 00:24:30,299
Big Brother Watch has done a one page of
fact sheet on the investigatory powers

311
00:24:30,299 --> 00:24:35,370
bill and I urge you to write your MP ask
them a question and if you're interested

312
00:24:35,370 --> 00:24:36,149
in doing that

313
00:24:36,149 --> 00:24:39,570
get involved I've left at each door
there are postcards with some

314
00:24:39,570 --> 00:24:44,158
recommendations and information from
ours and on how you might do that and

315
00:24:44,159 --> 00:24:47,399
also on our website we have a guide if
you've never written to your MP if you

316
00:24:47,399 --> 00:24:48,870
think it makes no difference

317
00:24:48,870 --> 00:24:52,860
i urge you to change your mind and do it
take action because it does ask them a

318
00:24:52,860 --> 00:24:53,668
question

319
00:24:53,669 --> 00:24:57,690
they do need to come back to you with an
answer another change coming up fifth of

320
00:24:57,690 --> 00:25:01,140
september's statutory safeguarding and
schools requires web monitoring which

321
00:25:01,140 --> 00:25:06,570
involves keylogging that means every
child internet use will be monitored in

322
00:25:06,570 --> 00:25:10,379
schools from September the fifth
something you're not aware of

323
00:25:10,380 --> 00:25:13,350
please look at our website for more
information and again if that's

324
00:25:13,350 --> 00:25:16,469
something that concerns you ask your MP

325
00:25:16,470 --> 00:25:20,399
are you aware of this is not been
debated in parliament is going through

326
00:25:20,399 --> 00:25:23,969
only under statutory guidance which
means schools are required to follow it

327
00:25:23,970 --> 00:25:30,659
unless they have a very good reason why
not so are the boundaries in technology

328
00:25:30,659 --> 00:25:34,260
I don't think so the fabulous weekend
we've had here shows you the

329
00:25:34,260 --> 00:25:38,340
possibilities there is so much potential
and possibility for technology

330
00:25:38,340 --> 00:25:43,230
to use data well to use data in the
right interests and for all of our

331
00:25:43,230 --> 00:25:44,370
benefit

332
00:25:44,370 --> 00:25:49,649
we want to see the public interest
research continued and make the future

333
00:25:49,650 --> 00:25:50,669
better

334
00:25:50,669 --> 00:25:55,080
but when we do it we can reframe that
you and I and our involvement with it

335
00:25:55,080 --> 00:25:58,590
can design the futur differently if
you're involved in an app development

336
00:25:58,590 --> 00:26:03,990
think about privacy and they're really
clear understandable accessible privacy

337
00:26:03,990 --> 00:26:08,190
policy that doesn't just look about
protecting you and what happens if you

338
00:26:08,190 --> 00:26:12,000
use people's data but explains to them
clearly how their data will be used and

339
00:26:12,000 --> 00:26:14,399
not on 15 pages of small print

340
00:26:14,399 --> 00:26:19,379
think about your language come back to
those words and think what do these data

341
00:26:19,380 --> 00:26:22,230
concepts really mean and think about
what you'll do

342
00:26:22,230 --> 00:26:27,059
that's what I'd like you to take away
this campaign is our key campaign today

343
00:26:27,059 --> 00:26:32,639
about the National people database we're
concerned about his use we ask you to

344
00:26:32,640 --> 00:26:34,649
consider getting involved

345
00:26:34,649 --> 00:26:38,850
think about this country of birth
collections coming up ask your school's

346
00:26:38,850 --> 00:26:40,529
if you haven't already heard about it

347
00:26:40,529 --> 00:26:44,970
send us your privacy policies or the the
collection letters that have been sent

348
00:26:44,970 --> 00:26:47,039
out to you send as examples

349
00:26:47,039 --> 00:26:51,000
spread the word of your friends and
family if you're on social media ask

350
00:26:51,000 --> 00:26:53,429
about it get a discussion going

351
00:26:53,429 --> 00:26:57,120
if its web monitoring again look at our
website for more information please

352
00:26:57,120 --> 00:26:59,459
consider getting involved

353
00:26:59,460 --> 00:27:05,760
thank you

354
00:27:05,760 --> 00:27:08,220
yeah

355
00:27:08,220 --> 00:27:12,750
thank you very much time and I think
we've got time for a couple of questions

356
00:27:12,750 --> 00:27:13,440
for you got

357
00:27:13,440 --> 00:27:16,440
ok

358
00:27:19,020 --> 00:27:25,139
and two questions first of all have you
done any requests on the subject access

359
00:27:25,140 --> 00:27:29,789
requests for any of the day to see what
kind of data is held for different years

360
00:27:29,789 --> 00:27:35,070
and secondly what kind of do you know if
they've had in your refusal for data

361
00:27:35,070 --> 00:27:38,520
how many people are getting applying
compared to how many people are getting

362
00:27:38,520 --> 00:27:44,970
the data two great questions which are
also separately so i can answer from my

363
00:27:44,970 --> 00:27:49,950
own experience subject extra quest has
been refused three times the Information

364
00:27:49,950 --> 00:27:55,620
Commissioner's Office has upheld and yet
suggested that to the Department of

365
00:27:55,620 --> 00:27:59,459
Education it would be good practice if
they were to release two parents who

366
00:27:59,460 --> 00:28:03,450
make their own subject access request
that is you can ask the Department or

367
00:28:03,450 --> 00:28:07,440
any public body who holds information
about you to release the information a

368
00:28:07,440 --> 00:28:09,809
copy of the information about yourself

369
00:28:09,809 --> 00:28:15,120
so we did this my own children and it's
been refused on the basis that it's a

370
00:28:15,120 --> 00:28:21,750
given an exemption under the research
uses section 33 of that is a detail for

371
00:28:21,750 --> 00:28:27,210
you but with we're discussing that with
the moment at the moment with them and

372
00:28:27,210 --> 00:28:32,940
the second question is a refusal one
request that was refused was by from the

373
00:28:32,940 --> 00:28:39,480
Ministry of Defence where they put in
request to target and certain levels of

374
00:28:39,480 --> 00:28:43,650
achieving children in certain schools to
be able to target them for recruitment

375
00:28:44,190 --> 00:28:50,100
that was refused and another that was
refused was again targeting children for

376
00:28:50,100 --> 00:28:53,850
universities who wanted to go
effectively and market their

377
00:28:53,850 --> 00:28:59,280
universities to particular schools so it
does look that they're there have been

378
00:28:59,280 --> 00:29:02,070
but in as of 2014

379
00:29:02,070 --> 00:29:05,070
they had been nine refusals over the
last two years

380
00:29:06,090 --> 00:29:10,860
I great talk thanks to brief questions

381
00:29:10,860 --> 00:29:16,049
firstly what if you think about drm
technologies that there are technical

382
00:29:16,049 --> 00:29:21,360
means of restricting people's access to
data once you

383
00:29:21,360 --> 00:29:25,590
release it to them so I'm wondering what
your opinion is on whether any technical

384
00:29:25,590 --> 00:29:31,020
measures are in place to restrict
people's usage of release data and if

385
00:29:31,020 --> 00:29:31,950
not why not

386
00:29:31,950 --> 00:29:36,179
and second question is more
philosophical I understand why you come

387
00:29:36,179 --> 00:29:40,920
here in on this thing it's an important
issue but do you feel in general that

388
00:29:40,920 --> 00:29:43,920
government surveillance or corporate
surveillance is a bigger problem

389
00:29:44,610 --> 00:29:53,820
- good questions i think i'll start with
the last second first and i think we

390
00:29:53,820 --> 00:30:01,770
have various issues in commercial use of
data and public data use which are

391
00:30:01,770 --> 00:30:07,049
different but are linked we often see
data that is public data being picky

392
00:30:07,049 --> 00:30:12,059
back to effectively by professional
companies getting access to it but we

393
00:30:12,059 --> 00:30:19,049
also we also see a lot of commercial
companies who collect an awful lot of

394
00:30:19,049 --> 00:30:24,000
information about is that we don't see
how it is used behind the scenes and to

395
00:30:24,000 --> 00:30:25,020
come

396
00:30:25,020 --> 00:30:31,889
I guess to a core issue that is joins in
both is what do people understand what

397
00:30:31,890 --> 00:30:35,070
happens to the data once they give it to
anybody

398
00:30:35,070 --> 00:30:39,809
is it commercial company is a public
organization and we are I think badly

399
00:30:39,809 --> 00:30:45,418
let down privacy policies that seeming
you give consent but in fact don't show

400
00:30:45,419 --> 00:30:50,160
us how third parties get data and then
pass it on to others

401
00:30:50,160 --> 00:30:53,640
reuse it connect it with other privacy
policies that becomes unintelligible

402
00:30:54,510 --> 00:30:59,400
I think there is a big scope for
improving especially for children how

403
00:30:59,400 --> 00:31:04,020
privacy policies are communicated and I
think it should be a really a

404
00:31:04,020 --> 00:31:09,030
significant issue that should improve
both commercial and public data use and

405
00:31:09,030 --> 00:31:13,200
to come to the second and your
understanding of DRM is I'm sure

406
00:31:13,200 --> 00:31:19,080
superior to mine the only technical
restrictions i know of

407
00:31:19,830 --> 00:31:23,549
I haven't yet actually put in a Freedom
of Information request around the very

408
00:31:23,549 --> 00:31:24,600
specifics

409
00:31:24,600 --> 00:31:29,668
the license that users have to sign and
you saw an extract from and in the

410
00:31:29,669 --> 00:31:34,740
presentation will give them certain
criteria that they need to

411
00:31:34,740 --> 00:31:39,929
go once they receive the data and that
includes how they can use it on their

412
00:31:39,929 --> 00:31:44,970
console or how they use it in there in
the location a big problem with that is

413
00:31:44,970 --> 00:31:48,990
is actually trusting that that is
maintained and if you don't audit

414
00:31:48,990 --> 00:31:52,320
how do we know it's being with you know
upheld so we can have all the

415
00:31:52,320 --> 00:31:56,490
technological restrictions i guess in
place that we want but as we've seen in

416
00:31:56,490 --> 00:32:00,420
some of the talk this week human error
is generally the biggest problem we've

417
00:32:00,420 --> 00:32:05,100
got a table data leakage and we can't
always restrict that but it's something

418
00:32:05,100 --> 00:32:08,550
I we are working on with the department
education and hoping that they're going

419
00:32:08,550 --> 00:32:15,090
to change some of their technological
access using API and other things and

420
00:32:15,090 --> 00:32:18,870
how they can get into data and andrew
share it more safely and yet make it

421
00:32:18,870 --> 00:32:19,770
more secure

422
00:32:19,770 --> 00:32:23,850
ok well thank you very much done and
please join me in thanking her for a

423
00:32:23,850 --> 00:32:24,300
great talk

