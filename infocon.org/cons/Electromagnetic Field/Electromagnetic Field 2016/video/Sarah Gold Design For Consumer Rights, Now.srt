1
00:00:00,000 --> 00:00:12,940
and Dave and

2
00:00:12,940 --> 00:00:21,640
i'm sarah i'm a designer i'm interested
in the kind of interaction data networks

3
00:00:21,640 --> 00:00:28,210
within the public domain and I work in
the area of kind of techno politics

4
00:00:28,210 --> 00:00:36,070
privacy and civics and I run a small
design studio called if and we make

5
00:00:36,070 --> 00:00:41,980
things that change how people think
about privacy security and data and our

6
00:00:41,980 --> 00:00:47,319
mission is to make those things more
kind of empowering and transparent and

7
00:00:47,320 --> 00:00:53,739
active for people and we do this because
i think we live in pretty extraordinary

8
00:00:53,739 --> 00:00:55,269
times

9
00:00:55,269 --> 00:01:06,009
there's just 58 years between these two
photos on the left hand side and that's

10
00:01:06,010 --> 00:01:11,710
a computer used by norwich city to
handle payroll and rapes for its

11
00:01:11,710 --> 00:01:18,070
employees and it took a number of people
to negotiate getting that computer into

12
00:01:18,070 --> 00:01:21,669
the building and and in 2016

13
00:01:21,670 --> 00:01:28,180
you can see the Raspberry Pi zero which
i'm sure many of you know which costs

14
00:01:28,180 --> 00:01:33,010
just five pounds and it's smaller than a
credit card

15
00:01:33,010 --> 00:01:39,820
it kind of refers to the way that at the
moment it's never been cheaper to put a

16
00:01:39,820 --> 00:01:45,039
chip in things and so we are
increasingly moving towards this

17
00:01:45,040 --> 00:01:51,010
internet of things or internet of
everything we're all objects around us

18
00:01:51,010 --> 00:01:57,400
become data conscious and connected but
i don't think that this accurately

19
00:01:57,400 --> 00:02:02,350
described what we're building right now
and I think it's a little bit more like

20
00:02:02,350 --> 00:02:07,720
this and this is a really great

21
00:02:08,258 --> 00:02:13,208
twitter handle at the bottom I'm that
it's got a bit angry recently but i

22
00:02:13,209 --> 00:02:17,830
recommend you follow it if you're if
you're on Twitter and they post images

23
00:02:17,830 --> 00:02:20,230
of things like this

24
00:02:20,230 --> 00:02:25,959
so on the left you've got like a
connected watch you can't tell the time

25
00:02:25,959 --> 00:02:31,780
because there's a software update and
suggestion in the middle that we'd

26
00:02:31,780 --> 00:02:37,690
connect like toilets to facebook in a
services that have never been connected

27
00:02:37,690 --> 00:02:43,900
before and I just have visions of in
connected homes that it's really hard to

28
00:02:43,900 --> 00:02:44,860
grow up

29
00:02:44,860 --> 00:02:48,819
it's really hard to come home as a
teenager drunk and for your parents not

30
00:02:48,819 --> 00:02:53,470
to know about it and and on the right
there's this poor person that's got

31
00:02:53,470 --> 00:02:58,810
stuck in a lift where there's no kind of
panic button just a scream

32
00:02:58,810 --> 00:03:08,200
this telling them about a software crash
but seriously I think we're in it in a

33
00:03:08,200 --> 00:03:13,780
troubling place right now because you'd
be used to get products with kind of

34
00:03:13,780 --> 00:03:19,720
instruction manuals we now get products
with apps and terms and conditions that

35
00:03:19,720 --> 00:03:24,190
no one reads or understands because they
are longer than war and peace

36
00:03:24,730 --> 00:03:30,340
they tend to be written in gobbledygook
and as we kind of agree to these terms

37
00:03:30,340 --> 00:03:33,340
and conditions we don't just sign up to

38
00:03:33,880 --> 00:03:41,440
strange and policies we also sign up to
these strange new social contracts and

39
00:03:41,440 --> 00:03:47,620
like in our I think a couple of years
ago now with the samsung smart TV where

40
00:03:47,620 --> 00:03:52,840
there was a clause within the privacy
policy that said anything you say in

41
00:03:52,840 --> 00:03:58,180
front of your Smart TV and may be
recorded and sent to third parties as

42
00:03:58,180 --> 00:04:03,340
part of our kind of voice recognition of
services so suddenly you have to be

43
00:04:03,340 --> 00:04:08,049
really careful about what you say in
front of your TV and because it might

44
00:04:08,049 --> 00:04:10,900
gossip more than your next door neighbor

45
00:04:10,900 --> 00:04:19,750
so the objects around us might just
become informants they might betray you

46
00:04:19,750 --> 00:04:22,850
a bit like this

47
00:04:22,850 --> 00:04:29,990
yeah

48
00:04:30,560 --> 00:04:35,630
because we have no longer got like
static relationships with things around

49
00:04:35,630 --> 00:04:41,750
us our relationships are changing we
have a kind of beginning middle and end

50
00:04:41,750 --> 00:04:49,220
of a relationship with an object because
the rules of engagement change every

51
00:04:49,220 --> 00:04:56,180
time we get a new software update or the
terms and conditions and get changed or

52
00:04:56,180 --> 00:05:01,040
we add a new app and as more and more
things become connected

53
00:05:02,000 --> 00:05:05,660
that's going to be an awful lot of
things that we have different rules for

54
00:05:05,660 --> 00:05:12,620
or contracts with it's going to be very
hard if not impossible to know what you

55
00:05:12,620 --> 00:05:19,400
can trust and what you can't and that's
when you're the owner of a product

56
00:05:20,000 --> 00:05:24,590
what happens when you go around to a
friend's house and they have one of

57
00:05:24,590 --> 00:05:27,080
these elusive connected bridges

58
00:05:27,080 --> 00:05:35,719
what is your relationship with that
object what happens then the way that

59
00:05:35,720 --> 00:05:39,080
these products talk to us has to change

60
00:05:39,710 --> 00:05:44,840
I think that if the government can write
in plain English then why can't apple

61
00:05:45,380 --> 00:05:51,800
we need human readable explanations
alongside software so that people can

62
00:05:51,800 --> 00:05:58,820
understand what they're using and this
is something that my friend Richard Pope

63
00:05:58,820 --> 00:06:02,960
introduced me to get in syntax and I'm
kind of interested as to whether this

64
00:06:02,960 --> 00:06:07,250
could be a nugget of how we can ask
things questions

65
00:06:07,250 --> 00:06:13,400
it gives you a human readable and kind
of text that you can ask of software so

66
00:06:13,400 --> 00:06:17,030
you don't have to be a developer to know
what's going on under the hood of a

67
00:06:17,030 --> 00:06:17,809
thing

68
00:06:17,810 --> 00:06:26,000
and as we put the internet in more
things that have heating elements or

69
00:06:26,000 --> 00:06:26,870
blades

70
00:06:26,870 --> 00:06:32,090
I think that this is increasingly about
kind of Public Safety and policy

71
00:06:32,090 --> 00:06:35,510
development as much as it is about
privacy

72
00:06:36,820 --> 00:06:42,940
I think we need to make the products
that people use clear and accountable

73
00:06:42,940 --> 00:06:47,080
so we know what's going on that's
alright as consumers

74
00:06:47,590 --> 00:06:50,739
I think it's a bit like mr. Weasley said

75
00:06:58,040 --> 00:07:00,569
hmm

76
00:07:00,569 --> 00:07:04,889
never trust anything that can think for
itself if you can't see where it keeps

77
00:07:04,889 --> 00:07:07,349
its brain

78
00:07:07,349 --> 00:07:13,889
so that's what i've been exploring
recently how we can make the objects we

79
00:07:13,889 --> 00:07:19,080
own a clearer and more accountable and
if we've been thinking about this

80
00:07:19,080 --> 00:07:22,619
through the lens of a consumer rights
group

81
00:07:22,619 --> 00:07:27,929
how can they update what they offer
beyond a kind of saga like magazine that

82
00:07:27,929 --> 00:07:32,998
get sent to you each month so we've been
exploring this

83
00:07:32,999 --> 00:07:40,259
three of three different objects or
things and they are all design probes

84
00:07:40,259 --> 00:07:44,159
so they're reasonably speculative and
but we're using them to show the little

85
00:07:44,159 --> 00:07:48,209
bit of the future and to think ahead
about ways that we could question

86
00:07:48,209 --> 00:07:55,469
objects around us and I'd like to show
you one of those today it's a early

87
00:07:55,469 --> 00:08:00,599
version and but at the end i will share
a URL that you could track what we're

88
00:08:00,599 --> 00:08:01,769
doing with it

89
00:08:01,769 --> 00:08:09,839
and so this is an idea for a hub Home
Hub we are really interested in the idea

90
00:08:09,839 --> 00:08:15,869
of a routine as an object that can we
can explore more than just as something

91
00:08:15,869 --> 00:08:20,219
that kind of provides you with Wi-Fi and
so what we're thinking about is using

92
00:08:20,219 --> 00:08:26,550
network maps of objects in your home and
these objects don't have to just be

93
00:08:26,550 --> 00:08:27,300
connected

94
00:08:27,300 --> 00:08:34,289
they could be kind of analog objects to
and that they're all stored in one place

95
00:08:34,289 --> 00:08:38,429
so that you could log into your hub and
see the network of of objects in your

96
00:08:38,429 --> 00:08:50,069
home and you can see as you click
through and on to those objects and

97
00:08:50,069 --> 00:08:56,279
whether those particular things have
kind of past there any texts and passed

98
00:08:56,279 --> 00:08:58,689
tests for kind of the bugs

99
00:08:58,690 --> 00:09:05,590
security errors and and that you also
might be able to add on like

100
00:09:05,590 --> 00:09:06,880
significantly

101
00:09:06,880 --> 00:09:12,040
sorry significantly more kind of secure
tests and depending on who you are so if

102
00:09:12,040 --> 00:09:15,819
you're someone who for instance might be
a journalist you might want to have like

103
00:09:15,820 --> 00:09:21,190
a much like higher rated kind of
security package running on your devices

104
00:09:21,190 --> 00:09:26,860
and and so we're interested in having
giving people the option to run tests

105
00:09:26,860 --> 00:09:32,890
over their own objects to see what's
going on and you can see here on the

106
00:09:32,890 --> 00:09:39,910
right as we go through that you could
add on different packages and that means

107
00:09:39,910 --> 00:09:45,040
that like also the other like interested
parties like campaign groups or

108
00:09:45,040 --> 00:09:50,890
charities could review tests against
particular pieces of legislation and run

109
00:09:50,890 --> 00:09:55,900
them against the data to independently
verify that like a regulatory body is

110
00:09:55,900 --> 00:10:02,770
actually doing its job and this is an
idea about the different tests that you

111
00:10:02,770 --> 00:10:05,410
could run

112
00:10:05,410 --> 00:10:09,579
so there's different tests for different
devices for different professions for

113
00:10:09,580 --> 00:10:12,580
different kind of events

114
00:10:14,529 --> 00:10:19,420
and as we've been doing some research
with with people so we've been going out

115
00:10:19,420 --> 00:10:24,189
into interviewing about 50 people now
about objects in their home

116
00:10:24,189 --> 00:10:28,300
there's something really interesting
about giving people the tools that they

117
00:10:28,300 --> 00:10:36,550
that they can understand and can't take
control of their devices with and as we

118
00:10:36,550 --> 00:10:38,170
continue to develop this

119
00:10:38,170 --> 00:10:48,430
I'll show you the on this URL so if you
wanted to explore like where we're at

120
00:10:48,430 --> 00:10:50,439
and click through some ideas

121
00:10:50,439 --> 00:10:55,420
if you go into consumer advocacy .
projects by F dot com

122
00:10:55,420 --> 00:10:59,259
you can see where we're at with this
demo and we're going to be finishing up

123
00:10:59,259 --> 00:11:01,120
in about three weeks

124
00:11:01,120 --> 00:11:05,319
so the hub is one of those objects we've
been looking at thinking about what

125
00:11:05,319 --> 00:11:11,259
other kind of references pieces of
information that people want might want

126
00:11:11,259 --> 00:11:15,519
to know about their objects and and
delivering that in a way that makes

127
00:11:15,519 --> 00:11:16,300
sense

128
00:11:16,300 --> 00:11:22,000
that's clear so we've been looking at
things like cve numbers and drm i'm

129
00:11:22,000 --> 00:11:22,839
working way

130
00:11:22,839 --> 00:11:27,009
working out how we can take those kind
of very technical pieces of information

131
00:11:27,009 --> 00:11:32,620
and deliver them in a way that many more
people will understand and so

132
00:11:32,620 --> 00:11:36,790
unfortunately I didn't have a full kind
of prototype to show you but as I say do

133
00:11:36,790 --> 00:11:41,410
you have a look at that URL in a couple
of weeks and we'll have kind of put that

134
00:11:41,410 --> 00:11:45,879
we've finished it in a moment in a
moment in time I suppose

135
00:11:45,879 --> 00:11:50,350
and the other to pieces that we're
looking at two and are brown shopping

136
00:11:50,350 --> 00:11:55,930
say when you go to store how you can see
where there are certain devices are like

137
00:11:55,930 --> 00:12:01,000
more secure or less secure than others
through like a connected tag system so

138
00:12:01,000 --> 00:12:05,350
we'll have that too and thank you very
much

139
00:12:05,889 --> 00:12:10,509
short presentation but i would welcome
any questions and that you might have

140
00:12:10,509 --> 00:12:12,140
thanks

141
00:12:12,140 --> 00:12:16,380
yeah

142
00:12:16,380 --> 00:12:22,410
hey l800 you Mike ok

143
00:12:23,490 --> 00:12:31,740
I so one of the problems i foresee is
maybe why is what allowed device maker

144
00:12:31,740 --> 00:12:35,340
to kind of open up the platform enough
to allow you to run those kind of tests

145
00:12:35,340 --> 00:12:39,720
because it's not is in their interest to
do so or how do you kind of solve that

146
00:12:39,720 --> 00:12:40,500
problem

147
00:12:40,500 --> 00:12:44,400
can you say that one so I didn't get it
like what

148
00:12:44,400 --> 00:12:49,560
what does he need to do to kind of like
ye board a device manufacturer and

149
00:12:49,560 --> 00:12:52,949
object manufacturer allow you to run
those contests against their device when

150
00:12:52,950 --> 00:12:55,860
it's in their interest I commercial
interests to collect as much data value

151
00:12:55,860 --> 00:13:03,660
as possible and so I think there's two
two things as it is I actually being out

152
00:13:03,660 --> 00:13:08,850
to board it coded is really important in
so many so many instances like eff at

153
00:13:08,850 --> 00:13:13,170
the moment of running a campaign against
drm which is kind of interesting Corey

154
00:13:13,170 --> 00:13:18,750
dr. Rose is leading that campaign and
that means that you can audit and audit

155
00:13:18,750 --> 00:13:25,650
code and I think that it's increasingly
important to have and to be able to to

156
00:13:25,650 --> 00:13:29,850
run or it's over code because if we are
collecting increased like more and more

157
00:13:29,850 --> 00:13:34,830
personal data about individuals we need
to make sure that as being like stored

158
00:13:34,830 --> 00:13:38,580
in a safe way that it's not going
against like terms that we might have

159
00:13:38,580 --> 00:13:44,970
agreed to and so as objects connected
collect more and more data that auditing

160
00:13:44,970 --> 00:13:48,810
is super important for us to be able to
trust the companies that provide us with

161
00:13:48,810 --> 00:13:57,030
products and so I think there's and like
this is kind of inevitable that this is

162
00:13:57,030 --> 00:14:04,709
going to happen and and when you look at
objects so like two weeks ago I think an

163
00:14:04,710 --> 00:14:06,410
app called glow

164
00:14:06,410 --> 00:14:12,829
I was being tested by consumer reports
and this app blow

165
00:14:13,519 --> 00:14:18,500
I think let's have women track their
menstruation cycles and monitor like

166
00:14:18,500 --> 00:14:24,470
when they're most versatile and consumer
reports for running and other tests and

167
00:14:24,470 --> 00:14:29,209
found out that glow was really really
insecure so it let other users change

168
00:14:29,209 --> 00:14:34,128
someone's password without them having
their old password it let it

169
00:14:34,129 --> 00:14:38,810
I'm kind of had a huge amount of
personal data in like to form groups

170
00:14:38,810 --> 00:14:43,099
there was quite a few things that were
I'm really worrying about that

171
00:14:43,100 --> 00:14:47,000
particular application and it was any
possible to know that because they were

172
00:14:47,000 --> 00:14:52,550
able to have access to code bases so I
think it's really crucial particularly

173
00:14:52,550 --> 00:14:57,829
as these kind of applications take more
and more personal sensitive data but

174
00:14:57,829 --> 00:15:01,790
also objects that might keep us alive or
safe say hi

175
00:15:08,360 --> 00:15:12,320
which in a few a few weeks ago to a
service from publication in the

176
00:15:12,320 --> 00:15:16,730
Netherlands step ahead would present a
way to block camera from recording if

177
00:15:16,730 --> 00:15:23,029
certain infrared patterns were
broadcasted on stage so that would also

178
00:15:23,029 --> 00:15:28,010
prevent consumers from for example
recording politician emitting some words

179
00:15:28,010 --> 00:15:34,939
and so that those can be used later on
are also some legal means to X prevent

180
00:15:34,940 --> 00:15:41,240
search both form of censorship being
applied say you can you

181
00:15:41,240 --> 00:15:46,910
yeah yeah it's a paralegal our way to
wear in the UK for example to wear finer

182
00:15:46,910 --> 00:15:51,050
if it's possible to prevent such
censorship in a camera device being

183
00:15:51,050 --> 00:15:52,339
employed

184
00:15:52,339 --> 00:15:56,540
oh I don't know it yeah I don't know the
answer to that question and but a big

185
00:15:56,540 --> 00:16:02,360
part of this is certainly around kind of
policy change and you know making

186
00:16:02,360 --> 00:16:06,470
actually illegal to hide important piece
of information about the devices that we

187
00:16:06,470 --> 00:16:07,279
have

188
00:16:07,279 --> 00:16:10,279
so that's a big part of this project

189
00:16:19,020 --> 00:16:24,750
it's just I'm wondering I'm it seems
like with the investigatory powers bill

190
00:16:24,750 --> 00:16:28,230
also fighting uphill battle with the
government on that kind of front

191
00:16:28,230 --> 00:16:31,950
I was just wondering if perhaps more
education is necessary because the only

192
00:16:31,950 --> 00:16:36,030
education we really had has been i think
from biz in terms of my businesses put

193
00:16:36,030 --> 00:16:39,540
antivirus on your emails and really
basic stuff like that but it's not being

194
00:16:39,540 --> 00:16:44,880
any kind of real campaign from people
like BFF to the public to explain what

195
00:16:44,880 --> 00:16:48,210
rights they're losing through all these
things and also the vulnerabilities of

196
00:16:48,210 --> 00:16:51,840
their devices are exposing them to
perhaps that's power

197
00:16:51,840 --> 00:16:56,370
that's many everybody interesting . and
so do teenager research that we've had

198
00:16:56,370 --> 00:17:00,180
back there's been a few people who have
mentioned that when they get my laptop

199
00:17:00,180 --> 00:17:06,030
and they're told to kind of install like
norton antivirus or other kind of

200
00:17:06,030 --> 00:17:10,800
antivirus or packages when they go and
buy have bought their nest or like a

201
00:17:10,800 --> 00:17:15,599
mobile phone you don't have that kind of
warning from a shop assistant at all so

202
00:17:15,599 --> 00:17:20,010
I'm and that for them kind of suggested
that the products they were buying that

203
00:17:20,010 --> 00:17:23,310
didn't have these sort of warnings
wouldn't have these problems that it's

204
00:17:23,310 --> 00:17:25,230
not something I'd have to think about

205
00:17:25,230 --> 00:17:31,110
so I think there's certainly and a kind
of digital literacy p at the shopping .

206
00:17:31,110 --> 00:17:34,889
and my colleagues also went to you

207
00:17:34,890 --> 00:17:40,500
a connected home store at john lewis on
Oxford Street and with it they were

208
00:17:40,500 --> 00:17:46,560
asking and the shop assistants about a
couple of like the connected bizarre

209
00:17:46,560 --> 00:17:51,300
home things that they were on sale and
the shop assistants really couldn't give

210
00:17:51,300 --> 00:17:57,450
em my colleagues any good answers about
the kind of questions to do with privacy

211
00:17:57,450 --> 00:18:02,070
that they were asking and the shop
assistants also said that we get asked

212
00:18:02,070 --> 00:18:05,700
these questions all the time so I think
there's definitely something about

213
00:18:05,700 --> 00:18:10,320
around like a a better digital alleged
mistress ep

214
00:18:10,320 --> 00:18:16,139
that's quite complicated and hard but i
think there should be a much clearer

215
00:18:16,140 --> 00:18:20,580
kind of terms when you buy these things
that you you know that

216
00:18:21,270 --> 00:18:28,139
yeah this that those objects aren't
complete completely and and even using

217
00:18:28,140 --> 00:18:32,070
the word safe is kind of hard because
you don't want to scare people too but i

218
00:18:32,070 --> 00:18:35,010
think is even about finding a kind of
language that we should use to talk

219
00:18:35,010 --> 00:18:40,680
about the sort of security and
vulnerabilities in objects that we say

220
00:18:49,130 --> 00:18:56,090
I said great talk again thanks and what
agencies do you see being able to

221
00:18:56,090 --> 00:19:01,520
deliver this off things that you talk
about being able to drive standards for

222
00:19:01,520 --> 00:19:08,240
safety offer and could the consumer
experience and I think they can see my

223
00:19:08,240 --> 00:19:13,730
advocacy organizations are still super
relevant and but they need updating to

224
00:19:13,730 --> 00:19:18,380
be brought into the kind of it into the
Internet age I suppose so i still think

225
00:19:18,380 --> 00:19:21,380
that groups like which Consumer Reports
choice

226
00:19:21,380 --> 00:19:26,630
Arsene relevant as an excuse to
organizations that can and kind of

227
00:19:26,630 --> 00:19:30,260
verify like a certain vulnerabilities

228
00:19:30,260 --> 00:19:33,740
but also that there is something to be
said about kind of crowd sourcing some

229
00:19:33,740 --> 00:19:38,570
of this information to because there are
so many different versions of hardware

230
00:19:38,570 --> 00:19:42,800
and software that we're running and is
there an opportunity to have more

231
00:19:42,800 --> 00:19:47,960
decentralized tests running on objects
that can be fed back so I'm quite

232
00:19:47,960 --> 00:19:54,740
interested in like the RSPB and like
summer bird watch and have so many

233
00:19:54,740 --> 00:19:57,920
people report back on the kind of birds
that they've seen in their garden and

234
00:19:57,920 --> 00:20:01,790
whether there's something similar about
running tests on devices in your home

235
00:20:02,540 --> 00:20:07,310
I guess there's something around that
that we can start to start to encourage

236
00:20:07,310 --> 00:20:13,129
say because there are so many things now
to test to look at and how could we have

237
00:20:13,130 --> 00:20:15,080
more decentralized testing -

238
00:20:15,080 --> 00:20:19,340
and by still think there's a really
important role that consumer rights

239
00:20:19,340 --> 00:20:28,639
organizations play in this puzzle that
they need to become like yeah far more

240
00:20:28,640 --> 00:20:31,640
relevant I think

241
00:20:40,600 --> 00:20:41,290
ok thankee

