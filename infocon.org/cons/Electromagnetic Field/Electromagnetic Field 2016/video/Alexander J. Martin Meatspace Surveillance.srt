1
00:00:00,000 --> 00:00:12,719
welcome everyone

2
00:00:13,630 --> 00:00:17,260
it's time to surveil the meat space

3
00:00:18,880 --> 00:00:22,330
all right we are still seeking
volunteers

4
00:00:22,330 --> 00:00:25,689
last I heard the volunteer tent just
just yonder

5
00:00:26,349 --> 00:00:29,860
so if you've had a great time and EMF
camp and you want to get back this is a

6
00:00:29,860 --> 00:00:31,239
great way you can do that

7
00:00:31,239 --> 00:00:34,300
please welcome with meats based
surveillance

8
00:00:34,840 --> 00:00:41,410
Alexander Martin thank you all very much

9
00:00:41,950 --> 00:00:48,490
I should apologize first off my laptop
that the filesystem got corrupted so no

10
00:00:48,490 --> 00:00:55,180
slides and i will be talking which is a
something people do i guess i've got a

11
00:00:55,180 --> 00:01:01,120
video as well it might be interesting
and I'd like to thank most of for Fran

12
00:01:01,120 --> 00:01:05,740
Charlie and talks code for trying to
save it and making it worse

13
00:01:06,250 --> 00:01:10,420
also like to particularly talks code i'm
sure it's the first time anyone has

14
00:01:10,420 --> 00:01:14,290
asked to borrow a hacker's laptop to
turn off two factor authentication for

15
00:01:14,290 --> 00:01:15,579
their work emails

16
00:01:15,579 --> 00:01:21,068
thank you much appreciated so basically
first slide

17
00:01:21,700 --> 00:01:26,619
would have been a Galileo quotes measure
what is measurable make measurable what

18
00:01:26,619 --> 00:01:30,490
is not so so I didn't say that that's
just something in your head from seeing

19
00:01:30,490 --> 00:01:35,710
the slide and now we seamlessly enter
into the talk and essentially this is a

20
00:01:35,710 --> 00:01:39,640
talk about biometrics the etymology of
the word biometrics is obvious enough

21
00:01:39,640 --> 00:01:44,590
bio many organic life metric being to
measure and we understand what

22
00:01:44,590 --> 00:01:48,340
biometrics are pretty simple enough
right fingerprints they are patterns

23
00:01:48,340 --> 00:01:53,770
it's a fairly established maths in terms
of how you differentiate patterns and

24
00:01:53,770 --> 00:01:58,060
it's with DNA nucleotide sequences there
are more potential sequences and there

25
00:01:58,060 --> 00:02:02,170
are humans in existence by magnitudes
upon magnitudes

26
00:02:02,170 --> 00:02:06,250
so these can be used to identify people
pretty easily faces

27
00:02:06,250 --> 00:02:09,549
I mean how faces more difficult because
it's not necessarily completely obvious

28
00:02:09,549 --> 00:02:14,799
how you go about measuring of face and
essentially there are there are there

29
00:02:14,799 --> 00:02:20,380
are two ways you can approach this and
they really get their their best

30
00:02:20,380 --> 00:02:26,799
comparison in a 1993 right up by
Brunelli and poggio which is called

31
00:02:26,799 --> 00:02:32,200
facial recognition features vs templates
so i'm good at this is really where

32
00:02:32,200 --> 00:02:35,350
slides would have been helpful because
i'm not a scientist I'm a journalist

33
00:02:35,350 --> 00:02:39,790
when we get to the government stuff I'm
great but the science side i'm i'm not a

34
00:02:39,790 --> 00:02:42,670
scientist again I will try my best

35
00:02:42,670 --> 00:02:50,319
so brilliant poggio ran ran tests in
1993 these these were really set up to

36
00:02:50,319 --> 00:02:55,869
work to be successful and they found
that they got they got ninety percent

37
00:02:55,870 --> 00:03:00,430
accuracy when they use the feature
method of matching faces and perfect

38
00:03:00,430 --> 00:03:03,910
accuracy with the template matching and
these are still the dominant ways in

39
00:03:03,910 --> 00:03:07,209
which the facial recognition works today
so it might be going worth going into

40
00:03:07,209 --> 00:03:09,430
them a little bit briefly again

41
00:03:09,430 --> 00:03:15,430
lack of slides so what is feature-based
matching its geometric relative

42
00:03:15,430 --> 00:03:20,349
positions of your eyes your nose your
mouth and basically anything from the

43
00:03:20,349 --> 00:03:22,950
eyebrows down counters the face

44
00:03:22,950 --> 00:03:30,510
the faces on the contrary in terms of
template matching holistically

45
00:03:30,510 --> 00:03:36,510
represented and feature basis earlier
it's it's really one of the first it's

46
00:03:36,510 --> 00:03:41,310
in the fifties as a scientist called
Woodrow Wilson Bledsoe named after the

47
00:03:41,310 --> 00:03:45,420
former president who brought the u.s.
into the First World War and it was

48
00:03:45,420 --> 00:03:50,458
input on a graphics tablet which was
developed by r / who also or funded by

49
00:03:50,459 --> 00:03:55,170
are not developed by off up potentially
the rand tablet and who also potentially

50
00:03:55,170 --> 00:04:00,238
funded the initial facial recognition
research that's not public i mean i

51
00:04:00,239 --> 00:04:02,250
don't know that's a guess

52
00:04:02,250 --> 00:04:09,660
they probably did template matching is
well it's a it's a fairly more bit more

53
00:04:09,660 --> 00:04:14,970
typical and digital image processing
activity and again where the slides

54
00:04:14,970 --> 00:04:17,910
would have been perfect in terms of
presenting how it looks visually

55
00:04:17,910 --> 00:04:24,389
basically the face it the face is
represented in a by dimensional array of

56
00:04:24,389 --> 00:04:28,470
intensity value so you got your x axis
you got your y-axis and again

57
00:04:28,470 --> 00:04:31,740
fortunately most face is quite
symmetrical so you can have a look at

58
00:04:31,740 --> 00:04:36,330
how the habit you can see visually how
the faces and kind of presented the

59
00:04:36,330 --> 00:04:40,320
images are compared then you have
obviously within any facial recognition

60
00:04:40,320 --> 00:04:44,580
system you have an input sample image
captured through CCTV your body one

61
00:04:44,580 --> 00:04:49,320
video and you have a database of images
in the UK these are largely custody

62
00:04:49,320 --> 00:04:53,490
photo images and many of which seems are
being held illegally will get onto that

63
00:04:53,490 --> 00:05:00,960
and the sample image and the database
image are compared with sort of typical

64
00:05:00,960 --> 00:05:09,210
Euclidean distances or more commonly and
using a convolution masks and bit more

65
00:05:09,210 --> 00:05:14,638
into the science i doubt feature-based
matching depends on using a vector of a

66
00:05:14,639 --> 00:05:18,780
geometric features which in itself
requires the images to go through a

67
00:05:18,780 --> 00:05:25,280
normalization process so that the
features again nose eyes etc are

68
00:05:25,280 --> 00:05:28,849
represented can be represented
independently of the position scale and

69
00:05:28,850 --> 00:05:32,270
rotation of the face in there in the
sample image plane

70
00:05:32,270 --> 00:05:38,090
hey Cesar achieved by brilliant poggio
by setting the intraocular distance and

71
00:05:38,090 --> 00:05:43,280
the direction of the eye to eye axis and
that is still for those who are worried

72
00:05:43,280 --> 00:05:45,049
i want to maintain your privacy

73
00:05:45,050 --> 00:05:48,050
the key element in any facial
recognition technology as far as I'm

74
00:05:48,050 --> 00:05:52,700
aware if you have with the triangles of
your eyes and nose if thats obscured

75
00:05:52,700 --> 00:05:53,659
enough

76
00:05:53,660 --> 00:05:57,560
you're not going to be recognized by at
least an alternating facial recognition

77
00:05:57,560 --> 00:06:07,280
system potentially other than that you
good luck and I will leave aside the the

78
00:06:07,280 --> 00:06:11,030
process of extracting features from that
plane because it is it's pretty

79
00:06:11,030 --> 00:06:15,979
complicated but I i guess most of you
would be able to guess how you may do it

80
00:06:15,980 --> 00:06:17,750
when you're when you're dealing with
them

81
00:06:17,750 --> 00:06:23,210
great interactions in interface and and
and that's really what sir

82
00:06:23,210 --> 00:06:26,270
well I guess on the template matching as
well

83
00:06:26,270 --> 00:06:31,940
yeah we've gone through that and that is
that the idea at least behind the

84
00:06:31,940 --> 00:06:34,729
technology that underpins most of the
technology

85
00:06:34,729 --> 00:06:38,840
well that's what underpins most of the
tech bubble questions persist slow

86
00:06:38,840 --> 00:06:43,099
inevitably about how well it works and
I'm going to quote and a bit of length

87
00:06:43,100 --> 00:06:48,770
here from an introduction to a paper by
John darkman university of cambridge the

88
00:06:48,770 --> 00:06:53,599
guy who invented iris recognition
because he brings up an excellent point

89
00:06:53,600 --> 00:06:59,300
when it comes to the scalability of such
systems dogmen in his introduction to

90
00:06:59,300 --> 00:07:02,780
the paper which is titled searching for
doppelgangers assessing the universality

91
00:07:02,780 --> 00:07:08,270
of the iris code imposters distribution
starts off with the well-known birthday

92
00:07:08,270 --> 00:07:12,469
problem and I'm sure you're all familiar
with it for the sake of those who aren't

93
00:07:12,470 --> 00:07:17,690
it's a problem which asks how large is a
group of people need to be before it is

94
00:07:17,690 --> 00:07:20,900
probable that two of them share the same
birthday

95
00:07:20,900 --> 00:07:27,109
so you think 365 maybe a half of that
it's not it's 23 it is probable if there

96
00:07:27,110 --> 00:07:30,200
are 23 people in a room that two of them
will share the same birthday

97
00:07:31,370 --> 00:07:34,810
and this is a there is according to
document

98
00:07:34,810 --> 00:07:38,830
an analogous biometric birthday problem
i quote again for a given similarity

99
00:07:38,830 --> 00:07:43,539
threshold yielding some specified false
match rates for single comparisons he

100
00:07:43,540 --> 00:07:48,100
asks how many different persons was the
database contain before it comes in

101
00:07:48,100 --> 00:07:53,139
likely than not that there is at least
one biometric collision with a biometric

102
00:07:53,139 --> 00:07:57,100
technologies such as face recognition
are usually tested and operated at the

103
00:07:57,100 --> 00:08:03,340
very undemanding criterion of false
match rate equaling 0.001 which means

104
00:08:03,340 --> 00:08:09,820
that any given pair of random persons
have probability of 0.999 of not being

105
00:08:09,820 --> 00:08:13,690
matched to each other since and persons
i'm going to skip the size too much for

106
00:08:13,690 --> 00:08:17,500
the mass too much here basically diamond
says this occurs when there are just 38

107
00:08:17,500 --> 00:08:23,770
or more persons that is the point at
which a false match rate of 000.00 one

108
00:08:23,770 --> 00:08:31,719
and will bring up a false match and they
said when is is looking at this in a

109
00:08:31,720 --> 00:08:37,479
panic great degree of scale and as a as
in terms also have complete automation

110
00:08:37,479 --> 00:08:40,270
face recognition systems in practice

111
00:08:40,270 --> 00:08:44,260
don't just say match were no match they
will bring up a list of likely

112
00:08:44,260 --> 00:08:48,370
candidates of match you've been input
input to that system and as far as I'm

113
00:08:48,370 --> 00:08:49,120
aware

114
00:08:49,120 --> 00:08:56,500
that's a is mostly all of them but i'm
not sure i have to admit and especially

115
00:08:56,500 --> 00:08:59,410
when we go into the government side of
things given isn't tremendously

116
00:08:59,410 --> 00:09:00,910
transparent about this stuff

117
00:09:00,910 --> 00:09:03,939
and there is a limit to my knowledge but
i will present what I know

118
00:09:03,940 --> 00:09:11,230
on the other hand you have been a man
called alessandro XD

119
00:09:11,230 --> 00:09:14,620
who do I apologize to him if he ever
sees this and sees me murdering his

120
00:09:14,620 --> 00:09:19,600
surname who did a study for Carnegie
Mellon which I will again quickly quite

121
00:09:19,600 --> 00:09:25,750
the summary from because it is a very
interesting and study and particularly

122
00:09:25,750 --> 00:09:29,770
relevant at he says we investigate the
feasibility of combining publicly

123
00:09:29,770 --> 00:09:33,250
available web two point O data with
off-the-shelf face recognition software

124
00:09:33,250 --> 00:09:38,560
for the purpose of large-scale automated
individual like three identification two

125
00:09:38,560 --> 00:09:42,400
experiments demonstrated the ability of
identifying strangers online on a dating

126
00:09:42,400 --> 00:09:45,670
site where individuals protect their
identities by using pseudonyms and

127
00:09:45,670 --> 00:09:47,520
offline in a public space

128
00:09:47,520 --> 00:09:52,140
based on photos made publicly available
social on a social network sites after a

129
00:09:52,140 --> 00:09:55,260
proof-of-concept experiment illustrates
the ability of inferring strangers

130
00:09:55,260 --> 00:09:58,830
personal or sensitive information their
interests and social security numbers

131
00:09:58,830 --> 00:10:00,240
from their faces

132
00:10:00,240 --> 00:10:05,340
by combining face recognition data
mining algorithms and statistical real

133
00:10:05,340 --> 00:10:09,000
identification techniques the results
highlight the implications of the

134
00:10:09,000 --> 00:10:12,600
inevitable convergence of face
recognition technology and increasing

135
00:10:12,600 --> 00:10:16,110
online self disclosures and the
emergence of personally predictable

136
00:10:16,110 --> 00:10:19,410
information and accuracy

137
00:10:19,410 --> 00:10:23,550
absolutely thanks and expects that we
will have and it is feasible to

138
00:10:23,550 --> 00:10:29,069
implement enormous face recognition
systems and interestingly he looked a

139
00:10:29,070 --> 00:10:36,450
lot of at Facebook and Facebook air was
a sort of called up on this a few years

140
00:10:36,450 --> 00:10:41,520
ago in the states with particular
senator al franken bringing a Democratic

141
00:10:41,520 --> 00:10:46,079
senator bring forward the sort of
concern that's a facebook now has the

142
00:10:46,080 --> 00:10:50,430
the largest database of face prints on
people anywhere in the world and

143
00:10:50,430 --> 00:10:56,640
interestingly facebook was audited by
the IRS Data Protection Commission in

144
00:10:56,640 --> 00:11:04,710
2011-2012 but it was told then it was it
had to delete all EU face prints and for

145
00:11:04,710 --> 00:11:09,330
tag suggestions which is you know I
don't use facebook i assume you guys

146
00:11:09,330 --> 00:11:13,680
might way you know you upload a picture
and it suggests who your friends aren't

147
00:11:13,680 --> 00:11:18,900
you does that through using a a tag
suggestion bad feature and this was

148
00:11:18,900 --> 00:11:24,150
determined by the iris data protection
commission to be a unlawful and where

149
00:11:24,150 --> 00:11:28,050
the facebook is actually started
deleting stuff or not we don't know and

150
00:11:28,050 --> 00:11:31,170
I've asked tried to get in touch with
facebook they haven't returned my calls

151
00:11:31,170 --> 00:11:34,650
guys please please do you know i'll be
friendly them

152
00:11:35,430 --> 00:11:39,329
I spoke also to our to ross anderson at
the University of Cambridge about this

153
00:11:39,330 --> 00:11:43,770
and i want to quote his points as well
because they're they're pretty gap

154
00:11:43,770 --> 00:11:47,069
sterling as you can imagine process .
SAR em

155
00:11:47,690 --> 00:11:51,050
he says the practical question is
whether in a given application you can

156
00:11:51,050 --> 00:11:52,579
limit the number of suspects

157
00:11:52,580 --> 00:11:55,640
against whom you're matching and whether
you have photos of them

158
00:11:55,640 --> 00:11:59,180
the several million people whose mug
shots on the police national database in

159
00:11:59,180 --> 00:11:59,930
the UK

160
00:11:59,930 --> 00:12:04,819
maybe rather many and made gave rather
high false positive rates false negative

161
00:12:04,820 --> 00:12:10,340
rate or both depending on how you set
your recognizer but there are 3500

162
00:12:10,340 --> 00:12:15,740
terrorist suspects in the UK as the NCAA
says then that's doable and it just

163
00:12:15,740 --> 00:12:20,750
talking them potentially the 35,000
people involved in serious Organized

164
00:12:20,750 --> 00:12:22,430
Crime should also be doable

165
00:12:22,430 --> 00:12:25,219
that doesn't mean that you will
recognize any of them will with any

166
00:12:25,220 --> 00:12:28,670
certainty when they went through
Waterloo station but you may recognize

167
00:12:28,670 --> 00:12:33,920
them often enough for it to be useful
and Anderson's points are sharp and

168
00:12:33,920 --> 00:12:39,740
bring us really on to the state use of a
facial recognition in the UK and so you

169
00:12:39,740 --> 00:12:42,800
mentioned the police national database
this is really really old at this point

170
00:12:42,800 --> 00:12:48,290
is based on a 1970s fujitsu mainframe
and basically when you see companies

171
00:12:48,290 --> 00:12:53,569
pretenders out for people who know how
to work with that really old tech and it

172
00:12:53,570 --> 00:13:00,800
is to support government contracts
almost always the PMD holds at well in

173
00:13:00,800 --> 00:13:02,060
2014

174
00:13:02,060 --> 00:13:06,229
when you held 30 million images that is
roughly twenty percent of the uk's

175
00:13:06,230 --> 00:13:12,800
population has its face captured there
some of these may be duplicates so maybe

176
00:13:12,800 --> 00:13:16,819
not all of you have you have your faces
there but it is statistically likely

177
00:13:16,820 --> 00:13:21,320
that somewhere between fifteen and
twenty percent of their faces held by

178
00:13:21,320 --> 00:13:26,720
police and we know this particularly
because of biometrics Commissioner

179
00:13:26,720 --> 00:13:32,630
reports in 2014 the biometrics to mr. at
the time so QC called Alice McGregor and

180
00:13:32,630 --> 00:13:36,620
he brings up the fact that his role as
under the protection of freedoms Act

181
00:13:36,620 --> 00:13:41,990
covers DNA and covers and fingerprints
but does not cover facial images and in

182
00:13:41,990 --> 00:13:45,890
fact facial images do not occur anywhere
in British law

183
00:13:45,890 --> 00:13:50,120
this is a completely unregulated area
which is why the police have so quickly

184
00:13:50,120 --> 00:13:51,620
been able to go towards it

185
00:13:51,620 --> 00:13:53,180
although there still

186
00:13:53,180 --> 00:13:57,199
acting a lovely and I saw the
commission's report in areas of

187
00:13:57,200 --> 00:14:04,250
fingerprints and DNA profiles so in 2012

188
00:14:04,250 --> 00:14:10,400
if we jump back a bit there was a hike
or ruling and and the High Court ruling

189
00:14:10,400 --> 00:14:15,680
was was brought on behalf of individuals
who weren't at names with good reason I

190
00:14:15,680 --> 00:14:21,650
imagine because their facial images were
being kept on the police on the by the

191
00:14:21,650 --> 00:14:27,170
Metropolitan Police on the BND and the
High Court rules that this was really

192
00:14:27,170 --> 00:14:28,910
not acceptable

193
00:14:28,910 --> 00:14:34,069
I'm i can't quote again I'm very sorry
about that but determined that the met

194
00:14:34,070 --> 00:14:38,210
police needed to delete these pictures
and should do so I like this is a very

195
00:14:38,210 --> 00:14:42,980
quite close to paraphrase in a matter of
months not years and they said that the

196
00:14:42,980 --> 00:14:46,340
policy needs to be revised government
said it was revised the policy met

197
00:14:46,340 --> 00:14:48,650
police said it would would pay attention
to that

198
00:14:48,650 --> 00:14:53,540
so that was in 2012 and 2013 the
government hadn't yet produced a

199
00:14:53,540 --> 00:14:59,300
biometric strategy in 2014 it didn't
again last year it didn't again

200
00:14:59,300 --> 00:15:03,050
it hasn't done so this year yet maybe
it's coming

201
00:15:03,680 --> 00:15:08,120
the Metropolitan Police i do want to
find this quote and I won't know the

202
00:15:08,120 --> 00:15:11,330
Metropolitan place on the other hand
when asked they are freedom of

203
00:15:11,330 --> 00:15:15,230
information requested them under the
Freedom of Information Act was sent in

204
00:15:15,230 --> 00:15:19,400
december of last year to the
Metropolitan Police Liberty were and

205
00:15:19,400 --> 00:15:25,010
part of the supporting the claim against
the MPs and the 2012 hi coach that case

206
00:15:25,010 --> 00:15:29,689
and they said in that day so they asked
house is progressing you know it's been

207
00:15:29,690 --> 00:15:33,710
three years and the met police said
progressing brilliantly

208
00:15:33,710 --> 00:15:41,240
we've deleted 560 photographs and then
they said the issue is our IT systems

209
00:15:41,240 --> 00:15:44,660
aren't set up to handle a complex
retention policy

210
00:15:44,660 --> 00:15:49,100
so we so that presumably is a policy
which allows you to delete photographs

211
00:15:49,100 --> 00:15:57,530
as well as a store them so yeah great
great forward movement there and I met

212
00:15:57,530 --> 00:16:00,170
the met police said this is like 2015

213
00:16:00,170 --> 00:16:04,280
they said they're implementing a new
system this year I'm unaware that has

214
00:16:04,280 --> 00:16:05,360
been done yet

215
00:16:05,360 --> 00:16:08,540
I'm fairly ignorant though the met don't
come and tell me everything they do

216
00:16:08,540 --> 00:16:11,810
the biometrics Commissioner also unaware
that they've done anything yet

217
00:16:11,810 --> 00:16:17,510
I'm and that was in his 2015 report
which is interesting again I mean he

218
00:16:17,510 --> 00:16:23,030
submitted its report in december and 4
2015 they didn't publish it until march

219
00:16:23,030 --> 00:16:28,850
maybe even later the the movement from
government on this on this whole area

220
00:16:28,850 --> 00:16:33,740
forward has been pretty pretty am slow i
will say from a completely objective

221
00:16:33,740 --> 00:16:39,380
perspective and government is also the
size of Technology Committee of

222
00:16:39,380 --> 00:16:43,490
parliament also brought up some
criticisms of the massive amount of

223
00:16:43,490 --> 00:16:49,850
facial images which was being retained
by the state and and the government said

224
00:16:49,850 --> 00:16:54,770
yeah we're gonna we're doing a review
into this a review of custody images and

225
00:16:54,770 --> 00:16:56,569
that hasn't been published yet either

226
00:16:56,570 --> 00:17:01,430
from what I understand it was really
really really bad and they just pushed

227
00:17:01,430 --> 00:17:04,669
it away and it's going to be rolled in
some way into the biometrics review

228
00:17:04,670 --> 00:17:12,260
which again has not yet been published
so that's that's at least where we're at

229
00:17:12,260 --> 00:17:15,890
with a PNG there are other uses of
course and i don't know how many people

230
00:17:15,890 --> 00:17:19,400
here went to download festival at last
year

231
00:17:20,000 --> 00:17:26,210
show of hands yeah what you got your -
you're too cool for that your to take no

232
00:17:26,210 --> 00:17:31,400
that's cool and not at Leicestershire
Police is in charge of download leasing

233
00:17:31,400 --> 00:17:35,780
for downloads and they use some facial
recognition technology there you may

234
00:17:35,780 --> 00:17:40,190
have read that may be on the register so
good for you and we were the first to

235
00:17:40,190 --> 00:17:41,870
publish it

236
00:17:41,870 --> 00:17:48,379
this was not a mugshot matching this was
real time this was live

237
00:17:48,380 --> 00:17:52,970
this was everyone going to download
having their face compared to a bespoke

238
00:17:52,970 --> 00:18:00,080
database that Lester police said it had
constructed itself and so they created

239
00:18:00,080 --> 00:18:05,449
the database they basically collected
that data base was collected by am sort

240
00:18:05,450 --> 00:18:09,770
of bringing together the mug shots of
between a thousand two hundred and a

241
00:18:09,770 --> 00:18:13,400
thousand six hundred offenders from
across Europe who are known to target

242
00:18:13,400 --> 00:18:15,920
large music festivals to commit crime

243
00:18:15,920 --> 00:18:19,220
none of them turned up at download none
of them at all

244
00:18:19,220 --> 00:18:22,970
not only that but a in terms of how well
and actually works

245
00:18:23,720 --> 00:18:28,640
the force said that other the trial was
controlled trial a number of officers

246
00:18:28,640 --> 00:18:31,910
and staff who would want you to have
their photographs entered into the bait

247
00:18:31,910 --> 00:18:36,260
database were successfully picked out by
its 77 times during the course of their

248
00:18:36,260 --> 00:18:39,200
duties in endemic to the part where it
was held at

249
00:18:39,200 --> 00:18:43,160
no information is provided to tell you
how valuable this was all contextualize

250
00:18:43,160 --> 00:18:46,220
this figure such as the total number of
times they should've been picked out or

251
00:18:46,220 --> 00:18:48,020
how strangely they attempt to the test

252
00:18:48,020 --> 00:18:49,750
the proof of concept system

253
00:18:49,750 --> 00:18:56,020
furthermore letter please run a mugshot
matching a facial recognition sweet and

254
00:18:56,020 --> 00:19:02,020
out there there are offices in leicester
this is provided by a corporation NEC

255
00:19:02,020 --> 00:19:08,139
corporation and the suite is called no
face and NEC corporation get paid for

256
00:19:08,140 --> 00:19:10,780
new face now they won't paid

257
00:19:10,780 --> 00:19:14,889
however for download when they did it
off their own cuff at well they were

258
00:19:14,890 --> 00:19:15,730
asked to do it

259
00:19:15,730 --> 00:19:18,880
it's not quite clear why there are asked
to do it or how they were asked to do it

260
00:19:18,880 --> 00:19:23,230
but they went today they attended as a
big they sort of gave this stuff out for

261
00:19:23,230 --> 00:19:27,520
free there was no cost to it and
subsequently the police announced there

262
00:19:27,520 --> 00:19:29,620
was no policing purpose

263
00:19:29,620 --> 00:19:32,620
police and utility for it and again

264
00:19:32,620 --> 00:19:36,610
they also said the trial was not even
being evaluated by Lester to police it

265
00:19:36,610 --> 00:19:42,729
was all held all evaluations were held
by the vendor and so yeah out of 90

266
00:19:42,730 --> 00:19:43,690
thousand attendees

267
00:19:43,690 --> 00:19:53,500
no one was picked up so I mean yes so
this is CCTV ipt but the footage and we

268
00:19:53,500 --> 00:19:58,030
report on it shortly beforehand to this
is a moment in which you get some relief

269
00:19:58,030 --> 00:20:02,950
for looking at my mug provided with four
hands and we tried to get in contact

270
00:20:02,950 --> 00:20:04,510
with news actually

271
00:20:04,510 --> 00:20:12,169
they didn't respond to us they did
respond to attendees and this is a bit

272
00:20:12,169 --> 00:20:15,169
of them singing as soon as i work this
mouse

273
00:20:16,850 --> 00:20:19,850
yes I am

274
00:20:32,850 --> 00:20:36,179
and

275
00:20:37,430 --> 00:20:40,430
I see

276
00:20:45,490 --> 00:20:48,490
you can see you

277
00:20:51,010 --> 00:20:54,010
this

278
00:20:56,330 --> 00:21:00,720
yeah

279
00:21:00,720 --> 00:21:02,220
come on

280
00:21:02,220 --> 00:21:05,620
yeah

281
00:21:05,620 --> 00:21:11,229
that's the the second of that is just as
electrical to to me as though it was to

282
00:21:11,230 --> 00:21:17,170
you but fuc Leicestershire Police is
fairly audible and I was after I then

283
00:21:17,170 --> 00:21:21,610
went to a bit later i managed to speak
to Simon Cole who's the chief constable

284
00:21:21,610 --> 00:21:26,530
Leicestershire Police and he said he
felt very alone very targeted when foot

285
00:21:26,530 --> 00:21:28,629
Leicestershire Police was was said

286
00:21:28,630 --> 00:21:33,700
which I'm sympathetic for and i have no
one wants to hear a sort of 90,000

287
00:21:33,700 --> 00:21:37,330
people cheering the idea of of you as a
bad person

288
00:21:37,330 --> 00:21:42,669
especially if you are there to prevent
crime and that is his purpose and that

289
00:21:42,670 --> 00:21:44,830
is how letter to police look at the
matter

290
00:21:44,830 --> 00:21:50,889
I so when we talk about that all those
faces that are held on their own the pnd

291
00:21:50,890 --> 00:21:55,120
a lot of these are people who have been
convicted and therefore their photos

292
00:21:55,120 --> 00:21:58,479
should be released unless they've been
arrested and they've accepted a caution

293
00:21:58,480 --> 00:22:03,100
because if you accept a caution you are
accepting your guilt even though you've

294
00:22:03,100 --> 00:22:07,030
never been to court and you may not have
had any legal advice and if you do

295
00:22:07,030 --> 00:22:12,370
accept a caution under the protection
under the checks for freedom sac 2012 at

296
00:22:12,370 --> 00:22:16,659
your biometrics your face limit your
fingerprints or DNA may be stored

297
00:22:16,660 --> 00:22:17,920
indefinitely

298
00:22:17,920 --> 00:22:22,000
and that doesn't expire so don't accept
cautions if you're innocent

299
00:22:22,600 --> 00:22:29,230
what else he said those an interesting
anecdotes and and they know they love it

300
00:22:29,230 --> 00:22:32,290
they absolutely love that the use of the
technology and they they traveled near

301
00:22:32,290 --> 00:22:34,720
face for me and it is impressive

302
00:22:34,720 --> 00:22:38,350
a lot of an eye I did find at least and
the samples are we I was shown may well

303
00:22:38,350 --> 00:22:44,230
have been particularly presented to me
in a way that was going to make lights

304
00:22:44,230 --> 00:22:48,970
of the criticisms that could be placed
against it and show that the exceedingly

305
00:22:48,970 --> 00:22:53,500
good policing purpose but he pointed out
a case to me at an argument to me which

306
00:22:53,500 --> 00:22:57,100
is hard from a civil libertarian
perspective to question

307
00:22:57,100 --> 00:23:01,629
he said he talked to told me of a time
in which a man who'd been to have his

308
00:23:01,630 --> 00:23:03,100
passport photo taken

309
00:23:03,100 --> 00:23:08,678
I had subsequently a woman had alleged
he sexually attacked her and they

310
00:23:08,679 --> 00:23:11,800
arrested this man they managed to find
him because this passport photo on done

311
00:23:11,800 --> 00:23:14,740
using their face and then arrested him

312
00:23:14,740 --> 00:23:19,809
the woman however the victim did not
want to press charges she did not want

313
00:23:19,809 --> 00:23:20,889
to go through that

314
00:23:20,890 --> 00:23:25,090
so at that point this man has been
convicted he is innocent and the police

315
00:23:25,090 --> 00:23:29,530
have a lot of evidence they could not
prosecute I could go with the CPS to

316
00:23:29,530 --> 00:23:35,350
prosecute but the lady the victim whom
should be prioritized does not want to

317
00:23:35,350 --> 00:23:38,620
go through with it at that . what should
they do with his image

318
00:23:39,130 --> 00:23:42,880
Simon Cole's position is that they
should offend or manage that is there

319
00:23:42,880 --> 00:23:45,610
should be some facility to keep hold of
this image

320
00:23:45,610 --> 00:23:50,050
I'm not sure that they shouldn't be but
then following on from and I asked him

321
00:23:50,050 --> 00:23:54,610
why don't keep everyone's image then why
don't you wear if it's no harm to

322
00:23:54,610 --> 00:23:55,689
innocent people

323
00:23:55,690 --> 00:23:59,620
why not have every innocent persons
image on there I which point he was

324
00:23:59,620 --> 00:24:04,178
quite happy to say well we're in the
land of all well then so the cops saying

325
00:24:04,179 --> 00:24:11,140
that which is brilliant and he's right i
mean i'm not sure how we go about them

326
00:24:11,140 --> 00:24:20,980
finding the balance between measuring
who-who should be exposed to potential

327
00:24:20,980 --> 00:24:26,290
States surveillance through but their
facial recognition and who should not be

328
00:24:26,290 --> 00:24:30,790
and this is also happening in Ireland i
should say that the Irish Guards the

329
00:24:30,790 --> 00:24:36,940
garnish a corner and recently proposed
in their new five-year program to

330
00:24:36,940 --> 00:24:40,000
professionalize and modernize their
systems which are mostly still

331
00:24:40,000 --> 00:24:44,230
paper-based by adding some facial
recognition technology new wish you the

332
00:24:44,230 --> 00:24:46,360
best of luck of that they would be easy

333
00:24:46,360 --> 00:24:50,590
and you're also seeing it in Manchester
where I had the police have stepped on

334
00:24:50,590 --> 00:24:55,120
about 3,000 body cameras body one video
it's called officially which is being

335
00:24:55,120 --> 00:24:58,330
stored in the cloud and you're saying
that in London where the Metropolitan

336
00:24:58,330 --> 00:24:59,470
Police are doing it

337
00:24:59,470 --> 00:25:05,860
so there are a lot of questions here but
I'm there are there are very little bits

338
00:25:05,860 --> 00:25:10,870
of legislation and regulation of the
area and one particularly interesting

339
00:25:10,870 --> 00:25:16,959
bit is another Commissioner whose I'm
found out his position again comes to

340
00:25:16,960 --> 00:25:18,970
protecting our freedoms actually 13

341
00:25:18,970 --> 00:25:22,840
he's a surveillance camera commissioner
and basically what he does is here polls

342
00:25:22,840 --> 00:25:26,470
& and tries to encourage the adoption of
something called the surveillance camera

343
00:25:26,470 --> 00:25:27,760
code of practice

344
00:25:27,760 --> 00:25:32,740
and I've got a lovely statement from his
office that you want to read out at his

345
00:25:32,740 --> 00:25:37,360
position however is that and you know if
you can't read out the surveillance

346
00:25:37,360 --> 00:25:42,189
camera code of practice request that if
you are using a camera surveillance

347
00:25:42,190 --> 00:25:43,000
camera

348
00:25:43,000 --> 00:25:46,630
you tell the public that you're doing it
that we have policing in this country

349
00:25:46,630 --> 00:25:50,740
but it is by consent and if what you are
doing and what you're doing to the

350
00:25:50,740 --> 00:25:53,980
footage as well if you're applying face
recollect facial recognition technology

351
00:25:53,980 --> 00:25:55,270
to the footage

352
00:25:55,270 --> 00:26:00,580
the public should also be aware of that
and that is in his terms part of what

353
00:26:00,580 --> 00:26:01,419
man

354
00:26:01,420 --> 00:26:04,150
what is included in the
surveillance-camera code of practice and

355
00:26:04,150 --> 00:26:05,230
the code of practice

356
00:26:05,230 --> 00:26:09,790
it is mandatory for police and state
organizations to follow that anyone who

357
00:26:09,790 --> 00:26:14,710
controls a CCTV camera surveillance
camera in general it could be IPTV it

358
00:26:14,710 --> 00:26:15,730
could be anything

359
00:26:15,730 --> 00:26:20,800
I'm yeah I'm not sure where I'm signing
out on this because the the issue with

360
00:26:20,800 --> 00:26:25,720
my laptop and but in short we don't have
regulation on this matter at the moment

361
00:26:26,380 --> 00:26:31,420
from what I understand the new batch is
Alice McGregor fusee finished his two

362
00:26:31,420 --> 00:26:32,410
years in the role

363
00:26:32,410 --> 00:26:35,860
a few months ago from what I understand
the new by matrix Commissioner doesn't

364
00:26:35,860 --> 00:26:41,350
want to do what he did because it isn't
part of his statutory obligations and

365
00:26:41,350 --> 00:26:45,370
he's over new commissioner come from
science not a legal background and

366
00:26:45,370 --> 00:26:49,209
there's another thing I meant to mention
as well and there isn't much claim or

367
00:26:49,210 --> 00:26:53,920
cause at the moment to chase a facial
recognition a big part of this is that

368
00:26:53,920 --> 00:27:01,360
the Home Office's is settling complaints
to it on biometrics out of court

369
00:27:01,360 --> 00:27:06,189
so there was a and this is only from a
small home office minutes documents are

370
00:27:06,190 --> 00:27:10,120
biometrics group meeting where they say
they have six complaints against him at

371
00:27:10,120 --> 00:27:13,389
the moment at three of which from
convicted people three from convicted

372
00:27:13,390 --> 00:27:17,590
putting the convicted to the side you've
got three on convicted people who are

373
00:27:17,590 --> 00:27:22,000
taking the Home Office to the european
court of human rights to say that at

374
00:27:22,000 --> 00:27:25,150
their biometrics are being a normally
held and the home office in that

375
00:27:25,150 --> 00:27:28,690
situation because the European Court of
Human Rights will allow this

376
00:27:28,690 --> 00:27:34,150
our amicably arbitrating they're
settling this case out of the cords and

377
00:27:34,150 --> 00:27:38,260
this is two things potentially it saves
the taxpayer money if if those people

378
00:27:38,260 --> 00:27:40,650
were going to claim millions which i
think is unlikely

379
00:27:40,650 --> 00:27:47,880
what it also does it and avoid setting a
legal precedence for any nem any

380
00:27:47,880 --> 00:27:51,840
embarrassing opportunities any
opportunity to embarrass the government

381
00:27:51,840 --> 00:27:53,730
and it's very delayed

382
00:27:53,730 --> 00:27:57,210
biometrics review

383
00:27:57,210 --> 00:28:03,000
so if any of you guys have had your
biometrics slept up you by all means you

384
00:28:03,000 --> 00:28:07,440
need to protect your potential rights
and thank you very much more time

385
00:28:19,260 --> 00:28:24,570
I'm not sore if I'm going to be able to
to answer your questions but if you do

386
00:28:24,570 --> 00:28:27,570
want two questions at me I'll give it a
go

387
00:28:30,330 --> 00:28:34,439
yeah she's got to or if you guys need
the stage back we can do outside

388
00:28:36,360 --> 00:28:39,360
yeah i mean it we were we were a bit sir

389
00:28:39,900 --> 00:28:46,680
actually no all right close to 30
minutes yes so you said the and its

390
00:28:46,680 --> 00:28:51,630
twenty fifteen percent of the public
does that mean that they've got images

391
00:28:51,630 --> 00:28:57,120
of people who on criminals and haven't
been arrested or is this

392
00:28:57,120 --> 00:29:00,929
when they're looking people for arrest
and then they're releasing them how they

393
00:29:00,930 --> 00:29:05,010
getting the images from people that
haven't been convicted such like some

394
00:29:05,010 --> 00:29:08,220
point so just on the police national
database which i should say is a single

395
00:29:08,220 --> 00:29:13,500
database of comparison is absolutely
possible that they are using and built a

396
00:29:13,500 --> 00:29:17,610
top personal data sets as everything to
disclose to be a thing

397
00:29:17,610 --> 00:29:20,760
- especially it wouldn't come under
police wouldn't be able to do it but it

398
00:29:20,760 --> 00:29:23,879
could come under the direct surveillance
powers in the Ripper to get enormous

399
00:29:23,880 --> 00:29:29,400
data sets a the passports database and
44 and also facial recognition again

400
00:29:29,400 --> 00:29:32,520
though that is at such a level of
security that is impossible for us to

401
00:29:32,520 --> 00:29:38,040
know and of course i also forgot to
mention and social media images are

402
00:29:38,040 --> 00:29:41,520
being used i mean if you but they're
being used to sample images rather than

403
00:29:41,520 --> 00:29:45,930
the the custody images so and this is
done normally I mean these would be open

404
00:29:45,930 --> 00:29:49,590
source images from the police's
perspective but it is concerning I mean

405
00:29:49,590 --> 00:29:51,510
when we talk about putting stuff in
public

406
00:29:51,510 --> 00:29:55,680
it is very public and any any any one
will look at that and will potentially

407
00:29:55,680 --> 00:29:59,970
use it for any purpose as i'm sure this
audience will will know from its own

408
00:29:59,970 --> 00:30:05,160
experience of dealing with black hats or
or blue hats as that sort of a model

409
00:30:05,160 --> 00:30:10,050
seems to to threat model seems to be
developing and yeah how does you talked

410
00:30:10,050 --> 00:30:13,980
about the fact that facial images are
not mentioned in law at the moment from

411
00:30:13,980 --> 00:30:18,030
this perspective how does that interact
with data protection so that the Data

412
00:30:18,030 --> 00:30:21,990
Protection Act does cover personal
identifying information which facial

413
00:30:21,990 --> 00:30:25,410
facial images are considered portal so
they wouldn't be able to

414
00:30:26,169 --> 00:30:30,759
and simply just push that out everywhere
and it's still I mean it's been the case

415
00:30:30,759 --> 00:30:33,850
for a long time in libel or for instance
that if you publish someone's face

416
00:30:33,850 --> 00:30:35,168
without identifying them

417
00:30:35,169 --> 00:30:38,799
you're in breach of yo but without that
could be that could constitute

418
00:30:38,799 --> 00:30:43,960
identifying them and there was a case
with the talk and alleged talk actors

419
00:30:43,960 --> 00:30:49,960
we're at the sum of the Telegraph
published a picture of one of these kids

420
00:30:49,960 --> 00:30:55,090
it was quite reasonably and confiscated
his identity but again if he was

421
00:30:55,090 --> 00:30:59,799
identified from its and his family was
harassed etc it's really closely from

422
00:30:59,799 --> 00:31:00,850
The Telegraph

423
00:31:00,850 --> 00:31:05,799
yes it is protected a facial images are
protection and the data protection act

424
00:31:05,799 --> 00:31:10,359
but when you have a policing purpose
that will that's a clear exemption for

425
00:31:10,359 --> 00:31:11,889
sharing data

426
00:31:11,889 --> 00:31:15,158
additionally there is and i haven't
found it but I've been informative Lee

427
00:31:15,159 --> 00:31:21,190
informed that there is a exemption for
I'm policing for our for sharing data

428
00:31:21,190 --> 00:31:25,840
within government under the protection
of freedom Zach so if you have if you've

429
00:31:25,840 --> 00:31:30,908
said yes it's fine for the government to
have some data they don't need to stay

430
00:31:30,909 --> 00:31:33,879
if you find a purpose they don't need to
care about that anymore

431
00:31:33,879 --> 00:31:37,149
they can share it between departments
saying yeah you know it's been we've

432
00:31:37,149 --> 00:31:38,529
been allowed to have it

433
00:31:38,529 --> 00:31:45,220
hi that leads nicely onto my question
and i'm interested in biometric passport

434
00:31:45,220 --> 00:31:49,809
data because if you can share face
models from one to the other then that

435
00:31:49,809 --> 00:31:53,080
would seem to me to be a good source for
most of the people in this room right

436
00:31:53,739 --> 00:31:57,340
I can't hear you if you could shout out
to be okay

437
00:31:57,340 --> 00:32:01,720
yes yes

438
00:32:02,600 --> 00:32:14,240
very possibly i mean before it it wasn't
really so much

439
00:32:14,240 --> 00:32:18,110
AFR before with with passports until you
get there the egauge coming in because

440
00:32:18,110 --> 00:32:21,918
authentication is done by partisan I'd
like to have this in in Hamburg where

441
00:32:21,919 --> 00:32:27,140
are very sort of burly and surly John
police officer was diligently and

442
00:32:27,140 --> 00:32:28,610
looking over my passport

443
00:32:28,610 --> 00:32:34,129
comparing it to my face i'm the the a
gates is absolutely they are they do

444
00:32:34,130 --> 00:32:38,690
seem to be getting getting faster and
i'm going to use when once actually but

445
00:32:38,690 --> 00:32:43,640
that said that is and I think that is
also the primary design and and use and

446
00:32:43,640 --> 00:32:45,830
hope for biometric passports as well

447
00:32:45,830 --> 00:32:49,970
yeah and then something absolutely worth
looking into more detail

448
00:32:50,510 --> 00:32:58,010
hello ok so you spoke about facebook and
then drawing the images off if you have

449
00:32:58,010 --> 00:33:02,419
a herd of like any third parties using
Facebook to skip through liking the

450
00:33:02,419 --> 00:33:06,140
intelligence services or something but
not with facebook but there was a very

451
00:33:06,140 --> 00:33:10,309
famous like a case with the in Russia
where the Russell russian social

452
00:33:10,309 --> 00:33:15,740
networking is vk.com or vacate are you
or whatever and well basically the

453
00:33:15,740 --> 00:33:20,450
written version of fortune and a teenage
boys can be trusted to do anything

454
00:33:20,450 --> 00:33:26,030
it's a chase off to pornographic models
and so an API was made with to this

455
00:33:26,030 --> 00:33:30,710
version Facebook's and face print
database and they use it to identify

456
00:33:30,710 --> 00:33:38,750
amateur porn models and then harass them
online so we can happen and facebook has

457
00:33:38,750 --> 00:33:44,510
not at the moment allowed any access to
its to sort of a question or in query

458
00:33:44,510 --> 00:33:48,590
it's a database of face prince imitates
I mean it would be it is the largest

459
00:33:48,590 --> 00:33:53,059
face print database in the world at that
has ever existed i mean facebook of

460
00:33:53,059 --> 00:33:56,510
right on the frontier of this technology
and their researchers as well and

461
00:33:56,510 --> 00:33:57,980
they've got fantastic

462
00:33:57,980 --> 00:34:01,309
really they're doing they're taking so
much so many strides forward when it

463
00:34:01,309 --> 00:34:06,590
comes to AI research and Facebook's
artificial research team or fart as i

464
00:34:06,590 --> 00:34:10,290
like to call it is exceptional and

465
00:34:10,290 --> 00:34:15,600
but again I think there is some
recognition that from facebook

466
00:34:15,600 --> 00:34:19,889
particularly that there is hesitation
about that you their use of of users

467
00:34:19,889 --> 00:34:26,489
data and their response and finding out
more about their response to the our

468
00:34:26,489 --> 00:34:32,310
state protection commissioners informing
them that the the face bridge database

469
00:34:32,310 --> 00:34:35,820
must be deleted on EU citizens will be
very interesting

470
00:34:37,080 --> 00:34:43,650
I just thought I'd mention that and
there's a and a billboard in and coming

471
00:34:43,650 --> 00:34:46,350
new street that is doing

472
00:34:46,350 --> 00:34:51,630
not that they specifically say that it's
not doing facial recognition but this is

473
00:34:51,630 --> 00:34:58,800
doing demographic recognition and
targeting advertising to the people who

474
00:34:58,800 --> 00:35:01,020
are coming off the trains

475
00:35:01,020 --> 00:35:05,370
I wasn't sure whether you aware of it oh
that was really interesting yet and my

476
00:35:05,370 --> 00:35:11,640
concern is that yet there's a fine line
between a demographic recognition and

477
00:35:11,640 --> 00:35:13,259
facial recognition

478
00:35:13,260 --> 00:35:17,820
absolutely know how they're processing
it I i did find out that they were doing

479
00:35:17,820 --> 00:35:24,930
cloud and processing on it so the data
is at least going into AWS so you know

480
00:35:24,930 --> 00:35:25,980
who's running it

481
00:35:25,980 --> 00:35:30,120
I believe it's actually a dozen of it no
I was in a great

482
00:35:30,120 --> 00:35:36,060
yeah yeah i mean there was a a sort of a
website

483
00:35:36,060 --> 00:35:41,130
not too long ago which was no at age
robot or how old am I robots which you

484
00:35:41,130 --> 00:35:42,750
would upload a photograph -

485
00:35:42,750 --> 00:35:45,600
and it would give you a rough estimation
of your rate of course it was quite

486
00:35:45,600 --> 00:35:51,089
crude and it got people's ages wrong in
those especially i noticed i remember

487
00:35:51,090 --> 00:35:55,110
Louise Mensch I did it and she was told
she was a 25 year old woman she's quite

488
00:35:55,110 --> 00:35:58,860
happy to tweet about that and the
ability for this to get people's ages

489
00:35:58,860 --> 00:36:03,630
wrong was comical and it made it a bit
global viral and what this was was

490
00:36:03,630 --> 00:36:07,860
actually run by microsoft and they were
using it to show how good is your

491
00:36:08,990 --> 00:36:14,689
it was after handling surprise workloads
and needing rapidly to scale up in terms

492
00:36:14,690 --> 00:36:21,170
of and server that demands and I think
when we look at that face recognition

493
00:36:21,170 --> 00:36:25,280
the future favor of condition and it is
going to be connected very deeply with

494
00:36:25,280 --> 00:36:29,420
their without technologies that is
whether the processing in the the

495
00:36:29,420 --> 00:36:31,280
storage is going to be

496
00:36:31,280 --> 00:36:33,530
thank you very much i'll enter thank you
guys

