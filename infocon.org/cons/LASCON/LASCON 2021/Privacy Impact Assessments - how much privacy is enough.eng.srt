1
00:00:17,920 --> 00:00:20,080
welcome everybody um if you want to

2
00:00:20,080 --> 00:00:22,880
humor me and scan this qr code and pull

3
00:00:22,880 --> 00:00:25,279
up a browser tab to participate

4
00:00:25,279 --> 00:00:28,320
in this presentation that should be fine

5
00:00:28,320 --> 00:00:31,439
and we'll get started and test this so

6
00:00:31,439 --> 00:00:32,640
um

7
00:00:32,640 --> 00:00:35,040
let me start the first poll here and

8
00:00:35,040 --> 00:00:38,160
tell me how you feel about all of us

9
00:00:38,160 --> 00:00:40,480
you're attending a privacy talk at an

10
00:00:40,480 --> 00:00:42,480
absec conference

11
00:00:42,480 --> 00:00:44,480
um you excited

12
00:00:44,480 --> 00:00:47,120
not so much

13
00:00:48,399 --> 00:00:50,960
let's see what the results are

14
00:00:50,960 --> 00:00:54,079
five votes are in

15
00:00:56,480 --> 00:00:59,718
all right

16
00:01:03,760 --> 00:01:06,880
that's better than i expected um so i

17
00:01:06,880 --> 00:01:08,880
think i overheard that that they're

18
00:01:08,880 --> 00:01:12,159
starting the bar at 3 45. so that's our

19
00:01:12,159 --> 00:01:13,680
goal

20
00:01:13,680 --> 00:01:15,600
so get let's get going here my name is

21
00:01:15,600 --> 00:01:18,320
david ojo

22
00:01:19,280 --> 00:01:21,360
thanks for the intro joe

23
00:01:21,360 --> 00:01:23,119
the two things i want to point out here

24
00:01:23,119 --> 00:01:24,960
is you know we're

25
00:01:24,960 --> 00:01:26,880
talking about things that border onto

26
00:01:26,880 --> 00:01:29,520
legal topics and i'm not a lawyer

27
00:01:29,520 --> 00:01:31,119
and nothing i want to say here is going

28
00:01:31,119 --> 00:01:33,119
to be legal advice so if you need legal

29
00:01:33,119 --> 00:01:35,759
advice find a lawyer

30
00:01:35,759 --> 00:01:37,439
and any of the examples we're going to

31
00:01:37,439 --> 00:01:41,759
look at are fictional i make those up

32
00:01:41,759 --> 00:01:46,000
so things we want to talk about today

33
00:01:46,000 --> 00:01:48,240
is privacy impact assessments or really

34
00:01:48,240 --> 00:01:52,799
looking how do we look at privacy risk

35
00:01:52,799 --> 00:01:55,200
and we'll get started by looking at

36
00:01:55,200 --> 00:01:57,119
something that we're familiar with um

37
00:01:57,119 --> 00:02:00,000
which is uh security risk assessments

38
00:02:00,000 --> 00:02:01,119
um

39
00:02:01,119 --> 00:02:02,000
and

40
00:02:02,000 --> 00:02:02,960
so

41
00:02:02,960 --> 00:02:06,240
you know in order to assess risk

42
00:02:06,240 --> 00:02:08,239
what are we really looking for we're

43
00:02:08,239 --> 00:02:10,639
trying to anticipate how our

44
00:02:10,639 --> 00:02:13,040
security goals could be violated right

45
00:02:13,040 --> 00:02:15,040
you know what could happen that

46
00:02:15,040 --> 00:02:16,800
violates these goals we're all familiar

47
00:02:16,800 --> 00:02:19,040
with those confidential confidentiality

48
00:02:19,040 --> 00:02:20,800
integrity availability

49
00:02:20,800 --> 00:02:22,560
sometimes other ones that people throw

50
00:02:22,560 --> 00:02:24,480
in the mix

51
00:02:24,480 --> 00:02:25,280
so

52
00:02:25,280 --> 00:02:27,320
let's assess one so um

53
00:02:27,320 --> 00:02:29,040
[Music]

54
00:02:29,040 --> 00:02:31,360
let's assume we are an online merchant

55
00:02:31,360 --> 00:02:33,280
and we take credit card payments on the

56
00:02:33,280 --> 00:02:36,560
internet right and um

57
00:02:36,560 --> 00:02:39,920
our security posture is

58
00:02:39,920 --> 00:02:42,800
not super great we patch our database

59
00:02:42,800 --> 00:02:44,959
monthly but we don't have a firewall in

60
00:02:44,959 --> 00:02:46,080
front of it

61
00:02:46,080 --> 00:02:47,040
and so

62
00:02:47,040 --> 00:02:48,879
the threat that we're modeling here

63
00:02:48,879 --> 00:02:51,360
right now is that an adversary comes

64
00:02:51,360 --> 00:02:53,280
exploits the vulnerability

65
00:02:53,280 --> 00:02:55,519
um in our database steals all the

66
00:02:55,519 --> 00:02:57,680
counselor data and sells it on the black

67
00:02:57,680 --> 00:02:58,800
market

68
00:02:58,800 --> 00:03:00,720
and then what that would mean as the

69
00:03:00,720 --> 00:03:03,280
hypothetical exposure to us right as

70
00:03:03,280 --> 00:03:04,319
some

71
00:03:04,319 --> 00:03:07,360
banks experience fraud

72
00:03:07,360 --> 00:03:10,640
we might get fined by the pci council

73
00:03:10,640 --> 00:03:13,920
our customers unhappy and so

74
00:03:13,920 --> 00:03:16,239
we estimate that maybe our damages

75
00:03:16,239 --> 00:03:17,760
coming out of that might be two million

76
00:03:17,760 --> 00:03:19,920
dollars if it happens ever

77
00:03:19,920 --> 00:03:22,800
and we think um maybe it'll happen every

78
00:03:22,800 --> 00:03:25,440
10 years so if we break this down to an

79
00:03:25,440 --> 00:03:27,440
annualized loss

80
00:03:27,440 --> 00:03:29,840
the 200k a year is our risk exposure

81
00:03:29,840 --> 00:03:32,959
right so the question for you all is

82
00:03:32,959 --> 00:03:35,840
what are we going to do about this

83
00:03:35,840 --> 00:03:36,799
um

84
00:03:36,799 --> 00:03:39,360
and so i'm going to start the next poll

85
00:03:39,360 --> 00:03:41,680
here

86
00:03:44,840 --> 00:03:48,799
and let's see what our options are so

87
00:03:48,799 --> 00:03:50,400
you could say that's fine right we make

88
00:03:50,400 --> 00:03:52,640
so much money out of this we can we can

89
00:03:52,640 --> 00:03:54,319
uh

90
00:03:54,319 --> 00:03:57,280
easily assume the risk of losing 200k a

91
00:03:57,280 --> 00:03:58,959
year through a data breach or you could

92
00:03:58,959 --> 00:04:01,439
say we could implement a firewall and

93
00:04:01,439 --> 00:04:04,319
that reduces the likelihood of us being

94
00:04:04,319 --> 00:04:06,560
exploited and

95
00:04:06,560 --> 00:04:07,920
so our

96
00:04:07,920 --> 00:04:10,239
our annualized loss expectancy rate goes

97
00:04:10,239 --> 00:04:12,480
down to 40k a year or

98
00:04:12,480 --> 00:04:14,640
the third option i'm giving you here if

99
00:04:14,640 --> 00:04:16,639
you want to participate in the poll

100
00:04:16,639 --> 00:04:17,600
um

101
00:04:17,600 --> 00:04:19,600
we'll outsource it to a card processor

102
00:04:19,600 --> 00:04:22,639
and so on our web page is just an iframe

103
00:04:22,639 --> 00:04:24,800
that sends the all the processing to the

104
00:04:24,800 --> 00:04:26,639
processor we don't store any controller

105
00:04:26,639 --> 00:04:29,280
data or anymore in our database and

106
00:04:29,280 --> 00:04:32,638
that's how we reduce the risk

107
00:04:32,960 --> 00:04:37,199
has everybody voted anybody still voting

108
00:04:37,199 --> 00:04:41,080
going once going twice

109
00:04:44,560 --> 00:04:47,520
there are the results um

110
00:04:47,520 --> 00:04:50,639
yeah that's that's along the lines of

111
00:04:50,639 --> 00:04:52,720
what my answer would have been too

112
00:04:52,720 --> 00:04:55,600
um so what did we just do

113
00:04:55,600 --> 00:04:58,240
we did a risk assessment right

114
00:04:58,240 --> 00:05:00,080
so on the left here

115
00:05:00,080 --> 00:05:02,240
we looked at what does our threat look

116
00:05:02,240 --> 00:05:03,759
like

117
00:05:03,759 --> 00:05:05,440
my ability in the database could be

118
00:05:05,440 --> 00:05:07,280
exploited by who

119
00:05:07,280 --> 00:05:09,360
what would happen and then we evaluate

120
00:05:09,360 --> 00:05:10,880
the source and say

121
00:05:10,880 --> 00:05:12,400
well what's the likelihood and what

122
00:05:12,400 --> 00:05:13,919
would be the the consequence the

123
00:05:13,919 --> 00:05:15,440
potential impact be

124
00:05:15,440 --> 00:05:17,039
and

125
00:05:17,039 --> 00:05:19,680
do we have any controls in place

126
00:05:19,680 --> 00:05:23,280
we patch the database once a month

127
00:05:23,280 --> 00:05:25,520
and so we model the risk and then we

128
00:05:25,520 --> 00:05:27,919
make a risk treatment decision

129
00:05:27,919 --> 00:05:30,320
um we say yeah we can accept the risk

130
00:05:30,320 --> 00:05:32,880
it's you know within our risk tolerance

131
00:05:32,880 --> 00:05:34,960
or we could say um

132
00:05:34,960 --> 00:05:37,280
let's avoid the risk let's just stop

133
00:05:37,280 --> 00:05:39,280
taking cardholder data

134
00:05:39,280 --> 00:05:41,039
usually not a good business decision

135
00:05:41,039 --> 00:05:43,600
we'd say transfer the risk like

136
00:05:43,600 --> 00:05:46,240
get like cyber insurance uh choice was

137
00:05:46,240 --> 00:05:48,800
talking about that earlier this morning

138
00:05:48,800 --> 00:05:51,280
or we can mitigate the risk and say

139
00:05:51,280 --> 00:05:52,639
we're going to implement additional

140
00:05:52,639 --> 00:05:54,960
controls a firewall or something and

141
00:05:54,960 --> 00:05:56,950
it's outsourcing

142
00:05:56,950 --> 00:05:58,560
[Music]

143
00:05:58,560 --> 00:06:01,280
if we decide to mitigate the risk do

144
00:06:01,280 --> 00:06:03,840
something about it

145
00:06:03,840 --> 00:06:05,440
then what we'll do

146
00:06:05,440 --> 00:06:07,680
after that is to look at the residual

147
00:06:07,680 --> 00:06:10,080
risk right we're saying okay

148
00:06:10,080 --> 00:06:12,160
let's say we're implementing a firewall

149
00:06:12,160 --> 00:06:14,479
what's the risk gonna look like once

150
00:06:14,479 --> 00:06:17,440
that firewall has been implemented

151
00:06:17,440 --> 00:06:19,600
so we're predicting that the risk then

152
00:06:19,600 --> 00:06:21,120
is going to be

153
00:06:21,120 --> 00:06:22,479
hopefully somewhere where it's going to

154
00:06:22,479 --> 00:06:24,000
be acceptable to us and that's the

155
00:06:24,000 --> 00:06:26,240
residual risk

156
00:06:26,240 --> 00:06:28,160
so this should look familiar that's how

157
00:06:28,160 --> 00:06:31,280
we do security risk assessments

158
00:06:31,280 --> 00:06:32,840
make sense any

159
00:06:32,840 --> 00:06:36,159
questions cool

160
00:06:36,319 --> 00:06:37,280
so

161
00:06:37,280 --> 00:06:38,400
um

162
00:06:38,400 --> 00:06:40,639
let's look at privacy impact analysis

163
00:06:40,639 --> 00:06:43,039
and what that looks like and if you just

164
00:06:43,039 --> 00:06:44,960
joined us if you want to participate you

165
00:06:44,960 --> 00:06:48,639
can scan this qr code with your phone

166
00:06:48,639 --> 00:06:50,800
and participate in our

167
00:06:50,800 --> 00:06:53,919
little studies here

168
00:06:54,400 --> 00:06:57,199
that's a lot of text i'll explain it to

169
00:06:57,199 --> 00:06:59,280
you

170
00:06:59,280 --> 00:07:01,520
so let's assume

171
00:07:01,520 --> 00:07:03,759
we have a platform

172
00:07:03,759 --> 00:07:05,360
um

173
00:07:05,360 --> 00:07:08,080
that works with item tracking devices

174
00:07:08,080 --> 00:07:09,919
right you know those little things that

175
00:07:09,919 --> 00:07:12,160
you can slip in your backpack or glue to

176
00:07:12,160 --> 00:07:14,880
your bicycle or your car

177
00:07:14,880 --> 00:07:18,000
and then any time a participating phone

178
00:07:18,000 --> 00:07:19,599
with bluetooth

179
00:07:19,599 --> 00:07:21,840
walks by it you can get an update on its

180
00:07:21,840 --> 00:07:23,520
location um

181
00:07:23,520 --> 00:07:24,319
so

182
00:07:24,319 --> 00:07:26,560
here is a hyper hypothetical threat we

183
00:07:26,560 --> 00:07:28,479
want to look at um

184
00:07:28,479 --> 00:07:30,880
so you track your consumers and weather

185
00:07:30,880 --> 00:07:33,599
stuff is and they tell you in your

186
00:07:33,599 --> 00:07:36,479
platform what type of item it is roughly

187
00:07:36,479 --> 00:07:38,479
right is it a bicycle is it a car is it

188
00:07:38,479 --> 00:07:39,599
a laptop

189
00:07:39,599 --> 00:07:40,479
um

190
00:07:40,479 --> 00:07:43,599
and so you've got pretty good statistics

191
00:07:43,599 --> 00:07:45,919
about how frequently they lose these

192
00:07:45,919 --> 00:07:47,680
things because every time they lose one

193
00:07:47,680 --> 00:07:50,560
of these then flag it as lost in your

194
00:07:50,560 --> 00:07:52,560
platform

195
00:07:52,560 --> 00:07:54,639
and you see that in average this happens

196
00:07:54,639 --> 00:07:56,800
to a person maybe every 15 years that

197
00:07:56,800 --> 00:07:59,120
they lose something valuable

198
00:07:59,120 --> 00:08:01,199
but you also see that there's about 10

199
00:08:01,199 --> 00:08:03,199
of your customers and to them that

200
00:08:03,199 --> 00:08:05,199
happens pretty frequently for whatever

201
00:08:05,199 --> 00:08:07,280
reason they lose stuff left and right or

202
00:08:07,280 --> 00:08:11,039
it gets stolen um so uh interesting to

203
00:08:11,039 --> 00:08:13,280
see that right

204
00:08:13,280 --> 00:08:15,039
and because you want to keep the prices

205
00:08:15,039 --> 00:08:17,520
low for your customers

206
00:08:17,520 --> 00:08:21,039
you sell these statistics onward to data

207
00:08:21,039 --> 00:08:23,280
aggregators including identifying

208
00:08:23,280 --> 00:08:25,520
information

209
00:08:25,520 --> 00:08:28,319
so the potential exposure here is

210
00:08:28,319 --> 00:08:29,280
that

211
00:08:29,280 --> 00:08:31,520
um the next time one of those customers

212
00:08:31,520 --> 00:08:33,200
who are like the more risky people who

213
00:08:33,200 --> 00:08:35,360
lose all of their stuff all the time

214
00:08:35,360 --> 00:08:36,719
goes and

215
00:08:36,719 --> 00:08:39,599
shops for like home owners insurance

216
00:08:39,599 --> 00:08:41,839
the quotes they get back for premiums is

217
00:08:41,839 --> 00:08:44,240
like 200 bucks more than what they what

218
00:08:44,240 --> 00:08:46,240
their friends are seeing in terms of

219
00:08:46,240 --> 00:08:47,360
premiums

220
00:08:47,360 --> 00:08:49,120
um

221
00:08:49,120 --> 00:08:50,560
you know and that happens you think

222
00:08:50,560 --> 00:08:52,240
maybe it happens about like to two

223
00:08:52,240 --> 00:08:55,040
percent of your users a year so the

224
00:08:55,040 --> 00:08:56,320
question is

225
00:08:56,320 --> 00:08:58,080
what are we going to do about that if

226
00:08:58,080 --> 00:09:00,480
anything

227
00:09:00,480 --> 00:09:04,000
so if you want to look at your phones

228
00:09:05,519 --> 00:09:06,800
you can

229
00:09:06,800 --> 00:09:09,600
choose from these answers so

230
00:09:09,600 --> 00:09:11,680
you could just say we're going to go on

231
00:09:11,680 --> 00:09:12,480
as

232
00:09:12,480 --> 00:09:15,279
ss we don't talk about it

233
00:09:15,279 --> 00:09:16,880
or you could say well let's add it to

234
00:09:16,880 --> 00:09:18,959
our privacy policy and that way at least

235
00:09:18,959 --> 00:09:20,959
we disclose to our

236
00:09:20,959 --> 00:09:23,200
customers what we're doing

237
00:09:23,200 --> 00:09:25,760
or you could say

238
00:09:25,760 --> 00:09:28,560
disclose it and offer an opportunity to

239
00:09:28,560 --> 00:09:31,200
opt out of this data sale

240
00:09:31,200 --> 00:09:32,160
or

241
00:09:32,160 --> 00:09:34,320
you could say disclose it

242
00:09:34,320 --> 00:09:37,279
and don't do it until a customer has

243
00:09:37,279 --> 00:09:39,360
opted in and maybe you give them an

244
00:09:39,360 --> 00:09:42,800
incentive for opting in who knows

245
00:09:42,800 --> 00:09:47,719
so let's see how everybody voted

246
00:09:51,839 --> 00:09:54,320
haha

247
00:09:54,320 --> 00:09:56,560
cool

248
00:09:56,959 --> 00:09:58,399
so

249
00:09:58,399 --> 00:10:02,360
that may be what we prefer

250
00:10:04,079 --> 00:10:05,200
so

251
00:10:05,200 --> 00:10:06,399
so um

252
00:10:06,399 --> 00:10:08,160
we just did another risk assessment

253
00:10:08,160 --> 00:10:09,279
right

254
00:10:09,279 --> 00:10:11,839
um but so the question is what has

255
00:10:11,839 --> 00:10:13,120
changed

256
00:10:13,120 --> 00:10:15,839
in the way we look at risk um

257
00:10:15,839 --> 00:10:16,959
anybody

258
00:10:16,959 --> 00:10:19,360
we have literally goodies if you

259
00:10:19,360 --> 00:10:22,800
speak up you get goodies

260
00:10:22,959 --> 00:10:26,359
he says

261
00:10:31,279 --> 00:10:32,959
yes exactly

262
00:10:32,959 --> 00:10:35,518
what else

263
00:10:42,000 --> 00:10:43,839
all right well so

264
00:10:43,839 --> 00:10:46,320
um so a couple things changed right so

265
00:10:46,320 --> 00:10:48,640
our perspective changed when we look at

266
00:10:48,640 --> 00:10:50,720
this risk we're not looking at

267
00:10:50,720 --> 00:10:53,040
what's going to be the consequences for

268
00:10:53,040 --> 00:10:54,480
us as a business

269
00:10:54,480 --> 00:10:56,079
if this happens

270
00:10:56,079 --> 00:10:57,600
like you just said we're looking at

271
00:10:57,600 --> 00:10:59,680
what's the consequence to the individual

272
00:10:59,680 --> 00:11:03,199
whose personal data reports

273
00:11:03,279 --> 00:11:05,839
and so if you think about it the

274
00:11:05,839 --> 00:11:08,240
adversary the type of adversary has

275
00:11:08,240 --> 00:11:09,440
changed

276
00:11:09,440 --> 00:11:12,399
all of a sudden it's us as the company

277
00:11:12,399 --> 00:11:15,279
selling the service whose practice is

278
00:11:15,279 --> 00:11:16,800
actually creating

279
00:11:16,800 --> 00:11:19,120
exposure for our consumers

280
00:11:19,120 --> 00:11:20,560
um

281
00:11:20,560 --> 00:11:22,079
and

282
00:11:22,079 --> 00:11:22,959
you know

283
00:11:22,959 --> 00:11:25,120
sure could also still be like a bad guy

284
00:11:25,120 --> 00:11:26,880
that and it's a data breach and that

285
00:11:26,880 --> 00:11:28,880
might also be privacy relevant but so

286
00:11:28,880 --> 00:11:30,480
there's different adversaries at play

287
00:11:30,480 --> 00:11:32,000
here

288
00:11:32,000 --> 00:11:34,000
the types of harm

289
00:11:34,000 --> 00:11:36,399
that we consider change

290
00:11:36,399 --> 00:11:38,560
so in our example we still looked at

291
00:11:38,560 --> 00:11:40,720
like economic loss right i have to pay

292
00:11:40,720 --> 00:11:43,120
like a higher premium for insurance but

293
00:11:43,120 --> 00:11:45,360
it could also be that as a result of

294
00:11:45,360 --> 00:11:48,240
somebody processing my data

295
00:11:48,240 --> 00:11:50,800
i lose a limb or die

296
00:11:50,800 --> 00:11:51,839
or

297
00:11:51,839 --> 00:11:54,639
you know i just get intimidated or

298
00:11:54,639 --> 00:11:57,519
embarrassed so those are kinds of harms

299
00:11:57,519 --> 00:12:00,160
that aren't necessarily economical when

300
00:12:00,160 --> 00:12:02,000
we look at privacy risk

301
00:12:02,000 --> 00:12:02,800
and

302
00:12:02,800 --> 00:12:05,279
the last thing i think that changes is

303
00:12:05,279 --> 00:12:07,760
the toolbox we have available to

304
00:12:07,760 --> 00:12:10,000
mitigate some of these risks so we

305
00:12:10,000 --> 00:12:11,760
talked about you know opt-ins and

306
00:12:11,760 --> 00:12:14,720
opt-outs and privacy notices

307
00:12:14,720 --> 00:12:15,920
so that's

308
00:12:15,920 --> 00:12:16,959
uh

309
00:12:16,959 --> 00:12:18,959
there's some overlap but there's also

310
00:12:18,959 --> 00:12:20,399
different things we can do in the

311
00:12:20,399 --> 00:12:21,440
privacy

312
00:12:21,440 --> 00:12:25,040
space than in security

313
00:12:25,040 --> 00:12:27,040
what hasn't changed

314
00:12:27,040 --> 00:12:30,160
what did not change

315
00:12:30,399 --> 00:12:32,160
between this and our earlier risk

316
00:12:32,160 --> 00:12:35,920
assessment about the cartel data breach

317
00:12:35,920 --> 00:12:39,160
any guesses

318
00:12:40,079 --> 00:12:42,320
um so

319
00:12:42,320 --> 00:12:44,240
the the decision maker

320
00:12:44,240 --> 00:12:46,480
it's still the company that's processing

321
00:12:46,480 --> 00:12:49,040
the data that decides oh is what we are

322
00:12:49,040 --> 00:12:51,040
doing appropriate or not and do we want

323
00:12:51,040 --> 00:12:52,560
to make it more appropriate or is it

324
00:12:52,560 --> 00:12:54,399
okay

325
00:12:54,399 --> 00:12:56,480
so that's interesting right there the

326
00:12:56,480 --> 00:12:58,959
individual is at risk but

327
00:12:58,959 --> 00:13:02,480
the organization makes the decision

328
00:13:02,480 --> 00:13:03,279
and

329
00:13:03,279 --> 00:13:06,000
of course i think as probably is obvious

330
00:13:06,000 --> 00:13:07,680
to to

331
00:13:07,680 --> 00:13:10,000
all of us

332
00:13:10,000 --> 00:13:11,920
we're still doing a risk assessment and

333
00:13:11,920 --> 00:13:14,240
the general methodology has not changed

334
00:13:14,240 --> 00:13:16,639
we we model the thread

335
00:13:16,639 --> 00:13:19,040
and maybe we call the uh the thread

336
00:13:19,040 --> 00:13:21,279
agent a data controller instead because

337
00:13:21,279 --> 00:13:22,959
it's the person deciding what happens

338
00:13:22,959 --> 00:13:24,160
with the data

339
00:13:24,160 --> 00:13:25,200
and

340
00:13:25,200 --> 00:13:26,720
maybe because we don't want to talk

341
00:13:26,720 --> 00:13:29,600
about like adversarial actions we call

342
00:13:29,600 --> 00:13:32,720
it problematic data actions

343
00:13:32,720 --> 00:13:35,120
that might occur

344
00:13:35,120 --> 00:13:36,880
and then we're looking at okay what's

345
00:13:36,880 --> 00:13:39,920
the likelihood of this happening

346
00:13:39,920 --> 00:13:41,920
and what's the potential harm to an

347
00:13:41,920 --> 00:13:44,880
individual if this happens

348
00:13:44,880 --> 00:13:46,839
and then we decide is what we're doing

349
00:13:46,839 --> 00:13:48,959
appropriate um

350
00:13:48,959 --> 00:13:51,360
you know privacy people often talk about

351
00:13:51,360 --> 00:13:52,959
proportionality

352
00:13:52,959 --> 00:13:55,680
um and do we want to do anything to

353
00:13:55,680 --> 00:13:57,360
change it mitigate it

354
00:13:57,360 --> 00:14:00,079
or not

355
00:14:00,079 --> 00:14:02,880
make sense so far

356
00:14:02,880 --> 00:14:03,920
um

357
00:14:03,920 --> 00:14:05,600
i put up here

358
00:14:05,600 --> 00:14:08,639
you know risk assessments are really

359
00:14:08,639 --> 00:14:12,480
about reducing uncertainty about what

360
00:14:12,480 --> 00:14:14,560
might happen in terms of our goals being

361
00:14:14,560 --> 00:14:16,639
violated right and we looked earlier at

362
00:14:16,639 --> 00:14:19,600
the security goals cia

363
00:14:19,600 --> 00:14:21,279
so the question then is

364
00:14:21,279 --> 00:14:23,920
what what are our privacy goals

365
00:14:23,920 --> 00:14:24,820
and so

366
00:14:24,820 --> 00:14:26,399
[Music]

367
00:14:26,399 --> 00:14:28,560
privacy goals

368
00:14:28,560 --> 00:14:30,760
are maybe not as

369
00:14:30,760 --> 00:14:33,760
uniformly codified as security goals i

370
00:14:33,760 --> 00:14:35,680
want to say uh they vary a little bit

371
00:14:35,680 --> 00:14:37,279
more in their definitions depending on

372
00:14:37,279 --> 00:14:38,800
where you look

373
00:14:38,800 --> 00:14:42,399
but so the really common principles

374
00:14:42,399 --> 00:14:44,480
are the fair information practice

375
00:14:44,480 --> 00:14:47,519
principles fi ppps flips

376
00:14:47,519 --> 00:14:48,720
um

377
00:14:48,720 --> 00:14:51,839
and those we find reflected in in laws

378
00:14:51,839 --> 00:14:53,279
over the world

379
00:14:53,279 --> 00:14:55,199
um and and codes

380
00:14:55,199 --> 00:14:57,040
so what are they

381
00:14:57,040 --> 00:15:00,560
they are purpose specification if you

382
00:15:00,560 --> 00:15:03,040
process somebody's personal data

383
00:15:03,040 --> 00:15:05,360
you need to be specific about which

384
00:15:05,360 --> 00:15:08,480
purpose you're doing this for

385
00:15:08,480 --> 00:15:10,480
and then you need to limit the

386
00:15:10,480 --> 00:15:12,800
collection of your data

387
00:15:12,800 --> 00:15:15,199
to that purpose only collect what you

388
00:15:15,199 --> 00:15:18,480
need to achieve that purpose

389
00:15:18,480 --> 00:15:20,160
you need to make sure that the data is

390
00:15:20,160 --> 00:15:22,240
accurate if you base decisions on the

391
00:15:22,240 --> 00:15:23,360
data

392
00:15:23,360 --> 00:15:25,199
that affect the individual then it

393
00:15:25,199 --> 00:15:28,160
better be accurate

394
00:15:28,639 --> 00:15:32,000
you need to limit its use only use it

395
00:15:32,000 --> 00:15:34,639
for the stated purpose

396
00:15:34,639 --> 00:15:36,959
and only keep it around for as long as

397
00:15:36,959 --> 00:15:40,319
you need it for that purpose

398
00:15:40,480 --> 00:15:42,399
security is still a topic we still need

399
00:15:42,399 --> 00:15:44,800
to keep the data secure right

400
00:15:44,800 --> 00:15:47,199
confidentiality integrity availability

401
00:15:47,199 --> 00:15:50,079
that still matters

402
00:15:50,320 --> 00:15:52,800
transparency openness

403
00:15:52,800 --> 00:15:55,040
i tell

404
00:15:55,040 --> 00:15:57,519
your customers or employees

405
00:15:57,519 --> 00:15:59,199
what you're doing with that data what

406
00:15:59,199 --> 00:16:00,959
kind of data you're collecting for what

407
00:16:00,959 --> 00:16:03,360
purpose

408
00:16:03,360 --> 00:16:04,639
and

409
00:16:04,639 --> 00:16:08,800
let them participate number seven here

410
00:16:08,800 --> 00:16:11,519
offer individuals a way to ask for hey

411
00:16:11,519 --> 00:16:13,759
what kind of data do you have about me

412
00:16:13,759 --> 00:16:17,600
or like who did you give this to

413
00:16:17,600 --> 00:16:19,600
individual participation

414
00:16:19,600 --> 00:16:22,000
and lastly be accountable for what

415
00:16:22,000 --> 00:16:23,199
you're doing

416
00:16:23,199 --> 00:16:25,680
and so accountability is more than just

417
00:16:25,680 --> 00:16:28,160
you know having like your legal

418
00:16:28,160 --> 00:16:29,279
address

419
00:16:29,279 --> 00:16:31,920
on a privacy notice it's like

420
00:16:31,920 --> 00:16:33,600
actually making sure

421
00:16:33,600 --> 00:16:35,519
that you adhere to these principles and

422
00:16:35,519 --> 00:16:37,680
you have policies in place internally

423
00:16:37,680 --> 00:16:40,479
and processes

424
00:16:41,120 --> 00:16:42,880
so that you can show to whoever is

425
00:16:42,880 --> 00:16:43,920
asking

426
00:16:43,920 --> 00:16:45,360
yes i'm doing all the right things

427
00:16:45,360 --> 00:16:48,560
here's my documentation

428
00:16:48,639 --> 00:16:51,839
does that make sense any questions

429
00:16:51,839 --> 00:16:52,800
cool

430
00:16:52,800 --> 00:16:53,759
um

431
00:16:53,759 --> 00:16:55,600
i should have said earlier

432
00:16:55,600 --> 00:16:58,000
i uploaded my slides to shed

433
00:16:58,000 --> 00:16:59,759
so the scheduling app if you want to

434
00:16:59,759 --> 00:17:01,920
copy they are on there as a pdf

435
00:17:01,920 --> 00:17:04,079
um so you don't need to take notes if

436
00:17:04,079 --> 00:17:05,359
you don't want to

437
00:17:05,359 --> 00:17:06,160
um

438
00:17:06,160 --> 00:17:06,959
so

439
00:17:06,959 --> 00:17:08,799
i wanted to briefly talk because that's

440
00:17:08,799 --> 00:17:10,559
a pet peeve of mine

441
00:17:10,559 --> 00:17:12,880
about the different names we give risk

442
00:17:12,880 --> 00:17:15,760
assessments in the privacy space

443
00:17:15,760 --> 00:17:17,280
and you can see here it's a little hard

444
00:17:17,280 --> 00:17:19,359
to read

445
00:17:19,359 --> 00:17:20,559
but

446
00:17:20,559 --> 00:17:23,439
the sources are mostly legislation but

447
00:17:23,439 --> 00:17:25,280
not always

448
00:17:25,280 --> 00:17:27,280
and many of these

449
00:17:27,280 --> 00:17:28,480
including

450
00:17:28,480 --> 00:17:30,320
you know the more recent

451
00:17:30,320 --> 00:17:32,880
state legislations in in the u.s um

452
00:17:32,880 --> 00:17:35,600
california and colorado and virginia

453
00:17:35,600 --> 00:17:37,600
they all have provisions for saying you

454
00:17:37,600 --> 00:17:39,280
need to do some sort of privacy risk

455
00:17:39,280 --> 00:17:40,400
assessment

456
00:17:40,400 --> 00:17:43,760
but they all call it different names

457
00:17:43,760 --> 00:17:45,600
and it's all the same

458
00:17:45,600 --> 00:17:46,400
so

459
00:17:46,400 --> 00:17:49,039
what i'm trying to get at is like i

460
00:17:49,039 --> 00:17:50,799
usually use the term privacy impact

461
00:17:50,799 --> 00:17:52,480
assessment and i don't mean to be

462
00:17:52,480 --> 00:17:54,080
specific about

463
00:17:54,080 --> 00:17:56,400
any of this context and i think many

464
00:17:56,400 --> 00:17:58,720
people do the same and so if you if

465
00:17:58,720 --> 00:18:00,880
somebody comes to you and says

466
00:18:00,880 --> 00:18:02,880
pj can you help me do a privacy impact

467
00:18:02,880 --> 00:18:04,480
assessment

468
00:18:04,480 --> 00:18:06,320
your next question you know that okay

469
00:18:06,320 --> 00:18:08,640
what's the context like who's asking for

470
00:18:08,640 --> 00:18:10,880
it in which of these

471
00:18:10,880 --> 00:18:13,200
there might be methodologies implied

472
00:18:13,200 --> 00:18:15,200
there might be different definitions of

473
00:18:15,200 --> 00:18:17,280
privacy goals that could be violated so

474
00:18:17,280 --> 00:18:19,280
you need to understand the context

475
00:18:19,280 --> 00:18:21,360
and don't assume that because somebody

476
00:18:21,360 --> 00:18:23,440
is using the term

477
00:18:23,440 --> 00:18:25,840
data protection impact assessment they

478
00:18:25,840 --> 00:18:28,160
necessarily mean that it's the one

479
00:18:28,160 --> 00:18:30,400
that's referred to in the gdpr always

480
00:18:30,400 --> 00:18:33,200
ask for context

481
00:18:35,679 --> 00:18:39,039
there's there's a new industry term too

482
00:18:39,039 --> 00:18:40,880
at the bottom here

483
00:18:40,880 --> 00:18:43,679
data transfer impact assessments or

484
00:18:43,679 --> 00:18:46,000
third country assessments

485
00:18:46,000 --> 00:18:47,360
um

486
00:18:47,360 --> 00:18:48,480
so

487
00:18:48,480 --> 00:18:49,520
um

488
00:18:49,520 --> 00:18:51,520
i wanted to briefly look at that because

489
00:18:51,520 --> 00:18:53,280
i think it's just really interesting to

490
00:18:53,280 --> 00:18:55,200
understand

491
00:18:55,200 --> 00:18:57,039
so if you haven't yet and want to scan

492
00:18:57,039 --> 00:18:59,600
this qr code and participate go ahead do

493
00:18:59,600 --> 00:19:01,600
it

494
00:19:01,600 --> 00:19:02,810
it's going to be

495
00:19:02,810 --> 00:19:05,960
[Music]

496
00:19:07,200 --> 00:19:08,720
we're wandering more into the legal

497
00:19:08,720 --> 00:19:11,360
aspects of privacy goals here and again

498
00:19:11,360 --> 00:19:13,200
i'm not a lawyer but i

499
00:19:13,200 --> 00:19:15,200
i'll try to explain

500
00:19:15,200 --> 00:19:18,640
explain this the layman's way um

501
00:19:18,640 --> 00:19:20,880
here is the deal

502
00:19:20,880 --> 00:19:22,840
you are a u.s

503
00:19:22,840 --> 00:19:24,400
headquarters

504
00:19:24,400 --> 00:19:26,880
sas solution

505
00:19:26,880 --> 00:19:30,240
that lets companies process their job

506
00:19:30,240 --> 00:19:32,080
applications with so

507
00:19:32,080 --> 00:19:34,320
if you want to apply for a job

508
00:19:34,320 --> 00:19:36,240
of one of

509
00:19:36,240 --> 00:19:38,080
your clients at one of your clients

510
00:19:38,080 --> 00:19:39,360
companies

511
00:19:39,360 --> 00:19:41,440
you use the sas platform to put in your

512
00:19:41,440 --> 00:19:44,000
job application and then hit the send

513
00:19:44,000 --> 00:19:45,039
button

514
00:19:45,039 --> 00:19:48,880
and the employer gets a job application

515
00:19:48,880 --> 00:19:50,720
and while you are headquartered here in

516
00:19:50,720 --> 00:19:53,440
the us

517
00:19:53,440 --> 00:19:55,280
you have european clients and you make

518
00:19:55,280 --> 00:19:56,880
sure to store

519
00:19:56,880 --> 00:19:57,840
data

520
00:19:57,840 --> 00:19:59,679
from the european clients and job

521
00:19:59,679 --> 00:20:01,360
applicants only

522
00:20:01,360 --> 00:20:03,600
on european servers

523
00:20:03,600 --> 00:20:06,640
make sense so far

524
00:20:06,640 --> 00:20:09,520
so here's the hypothetical problem we're

525
00:20:09,520 --> 00:20:12,320
trying to model

526
00:20:12,559 --> 00:20:14,000
it could happen that you receive a

527
00:20:14,000 --> 00:20:16,559
request from the us government that says

528
00:20:16,559 --> 00:20:19,039
hey give me all the data you have about

529
00:20:19,039 --> 00:20:21,679
this one person in europe that has used

530
00:20:21,679 --> 00:20:24,400
your platform

531
00:20:24,960 --> 00:20:26,559
and you think that maybe happens to you

532
00:20:26,559 --> 00:20:28,799
once a year

533
00:20:28,799 --> 00:20:30,960
the potential privacy harm we're looking

534
00:20:30,960 --> 00:20:36,480
at is this um the eu regulations the

535
00:20:36,480 --> 00:20:38,159
gdpr

536
00:20:38,159 --> 00:20:40,400
basically says

537
00:20:40,400 --> 00:20:42,480
if you ask for somebody's data it needs

538
00:20:42,480 --> 00:20:44,480
to be for a specific purpose and it

539
00:20:44,480 --> 00:20:48,080
needs to be proportionate proportionate

540
00:20:48,080 --> 00:20:49,679
u.s law

541
00:20:49,679 --> 00:20:51,919
that allows you know

542
00:20:51,919 --> 00:20:54,159
counter-terrorism or law enforcement to

543
00:20:54,159 --> 00:20:55,600
ask for data

544
00:20:55,600 --> 00:20:56,880
secretly

545
00:20:56,880 --> 00:20:58,320
from a company

546
00:20:58,320 --> 00:21:00,480
doesn't say it needs to be purpose

547
00:21:00,480 --> 00:21:03,840
limited and proportionate so there's a

548
00:21:03,840 --> 00:21:05,280
conflict there

549
00:21:05,280 --> 00:21:06,720
and

550
00:21:06,720 --> 00:21:08,880
the other thing that's a problem

551
00:21:08,880 --> 00:21:11,600
as far as europeans are concerned

552
00:21:11,600 --> 00:21:13,520
is

553
00:21:13,520 --> 00:21:16,240
job applicants whose data you might give

554
00:21:16,240 --> 00:21:18,640
to law enforcement or whoever they can't

555
00:21:18,640 --> 00:21:20,000
know about it

556
00:21:20,000 --> 00:21:22,000
right it's supposed to be secret and so

557
00:21:22,000 --> 00:21:23,280
they can't

558
00:21:23,280 --> 00:21:25,440
like challenge that what we do it what

559
00:21:25,440 --> 00:21:27,840
we're doing is proportionate or ask for

560
00:21:27,840 --> 00:21:30,240
a copy of what you gave law enforcement

561
00:21:30,240 --> 00:21:32,400
so

562
00:21:32,400 --> 00:21:35,840
we are violating privacy goals afforded

563
00:21:35,840 --> 00:21:38,480
to europeans by the gdpr

564
00:21:38,480 --> 00:21:42,000
by doing this so the question is

565
00:21:42,000 --> 00:21:44,799
remember we're a sas provider

566
00:21:44,799 --> 00:21:46,640
what are we gonna do about it so let's

567
00:21:46,640 --> 00:21:49,440
let's start another poll

568
00:21:49,440 --> 00:21:51,760
um

569
00:21:53,600 --> 00:21:56,400
and let's look at our options here so on

570
00:21:56,400 --> 00:21:58,480
your phone you can vote

571
00:21:58,480 --> 00:22:00,480
um you could either say well it's not my

572
00:22:00,480 --> 00:22:03,200
problem right it's like

573
00:22:03,200 --> 00:22:05,760
my european clients are companies that

574
00:22:05,760 --> 00:22:08,159
pay me for the service and it's their

575
00:22:08,159 --> 00:22:10,240
job applicants so it's they have to

576
00:22:10,240 --> 00:22:12,720
worry about that not my problem

577
00:22:12,720 --> 00:22:14,720
or you could say it's not a problem at

578
00:22:14,720 --> 00:22:16,159
all

579
00:22:16,159 --> 00:22:18,720
that the u.s government can't compel me

580
00:22:18,720 --> 00:22:20,320
to

581
00:22:20,320 --> 00:22:22,240
get them data from one of my servers

582
00:22:22,240 --> 00:22:24,640
that are located in europe

583
00:22:24,640 --> 00:22:26,400
or you could say well let's just add it

584
00:22:26,400 --> 00:22:28,159
to our privacy policy

585
00:22:28,159 --> 00:22:30,320
so that that clients and job applicants

586
00:22:30,320 --> 00:22:32,640
know about it and and so you know we

587
00:22:32,640 --> 00:22:34,480
told them we're being transparent about

588
00:22:34,480 --> 00:22:35,280
it

589
00:22:35,280 --> 00:22:36,480
or

590
00:22:36,480 --> 00:22:37,919
you could say

591
00:22:37,919 --> 00:22:40,559
let's implement a complicated encryption

592
00:22:40,559 --> 00:22:44,320
scheme where customers can hold

593
00:22:44,320 --> 00:22:46,400
encryption keys

594
00:22:46,400 --> 00:22:49,440
that allow them and their job applicants

595
00:22:49,440 --> 00:22:51,440
to decrypt the data

596
00:22:51,440 --> 00:22:54,080
but while it's stored on our servers

597
00:22:54,080 --> 00:22:56,000
it's encrypted

598
00:22:56,000 --> 00:22:58,400
with a reasonably strong mechanism and

599
00:22:58,400 --> 00:23:00,960
so even if somebody comes and asks us

600
00:23:00,960 --> 00:23:03,200
for a copy of that data

601
00:23:03,200 --> 00:23:04,960
we can give them the encrypted data but

602
00:23:04,960 --> 00:23:06,840
we can't decrypt it for

603
00:23:06,840 --> 00:23:09,440
them or

604
00:23:09,440 --> 00:23:11,440
last option here you could say i need to

605
00:23:11,440 --> 00:23:14,720
hire a lawyer and figure this one out

606
00:23:14,720 --> 00:23:18,400
um has everybody voted

607
00:23:18,880 --> 00:23:22,520
yes okay

608
00:23:24,240 --> 00:23:25,200
okay

609
00:23:25,200 --> 00:23:27,440
cool

610
00:23:28,400 --> 00:23:30,480
so so um

611
00:23:30,480 --> 00:23:32,799
i am not a lawyer so technically the

612
00:23:32,799 --> 00:23:34,480
only answer i can tell you is right it's

613
00:23:34,480 --> 00:23:36,640
the last one there but let's talk about

614
00:23:36,640 --> 00:23:39,520
the other options briefly too

615
00:23:39,520 --> 00:23:41,279
so you could say that's not my problem

616
00:23:41,279 --> 00:23:43,679
and and technically that's true right

617
00:23:43,679 --> 00:23:45,600
you're just acting on the instruction of

618
00:23:45,600 --> 00:23:47,520
your european clients

619
00:23:47,520 --> 00:23:48,799
but

620
00:23:48,799 --> 00:23:50,640
you know the european clients are going

621
00:23:50,640 --> 00:23:52,400
to go somewhere else if you don't help

622
00:23:52,400 --> 00:23:53,679
them solve that issue right they're

623
00:23:53,679 --> 00:23:55,679
going to find a provider that can help

624
00:23:55,679 --> 00:23:57,200
them solve this issue

625
00:23:57,200 --> 00:23:58,640
um

626
00:23:58,640 --> 00:24:01,520
the the next one if you look at the the

627
00:24:01,520 --> 00:24:05,200
u.s cloud act

628
00:24:05,200 --> 00:24:07,279
so this one isn't true the cloud act

629
00:24:07,279 --> 00:24:09,440
says even if your data's outside of the

630
00:24:09,440 --> 00:24:11,440
country i can ask you for it if i'm in

631
00:24:11,440 --> 00:24:12,320
the us

632
00:24:12,320 --> 00:24:14,960
and have the legal right to do that

633
00:24:14,960 --> 00:24:16,640
the third one

634
00:24:16,640 --> 00:24:19,679
solves the issue about transparency

635
00:24:19,679 --> 00:24:21,200
it might happen

636
00:24:21,200 --> 00:24:23,919
but it doesn't solve the issue about

637
00:24:23,919 --> 00:24:26,640
is it proportionate limited to a purpose

638
00:24:26,640 --> 00:24:30,240
and can you as a

639
00:24:30,880 --> 00:24:33,200
as the individual whose data is being

640
00:24:33,200 --> 00:24:34,960
forwarded actually do something about it

641
00:24:34,960 --> 00:24:36,640
or ask about it so it doesn't really

642
00:24:36,640 --> 00:24:38,159
solve the problem

643
00:24:38,159 --> 00:24:40,400
um

644
00:24:40,799 --> 00:24:42,960
the the encryption scheme i think that

645
00:24:42,960 --> 00:24:44,400
might actually work

646
00:24:44,400 --> 00:24:46,480
um

647
00:24:46,480 --> 00:24:47,760
but uh

648
00:24:47,760 --> 00:24:49,360
yeah i would probably also go to a

649
00:24:49,360 --> 00:24:50,960
lawyer um

650
00:24:50,960 --> 00:24:52,799
so that's where you see you know it gets

651
00:24:52,799 --> 00:24:55,440
out of the hand of us as techies and can

652
00:24:55,440 --> 00:24:58,080
get uh

653
00:24:58,240 --> 00:25:01,360
really complicated but i i decided to

654
00:25:01,360 --> 00:25:04,240
include this example because

655
00:25:04,240 --> 00:25:06,720
there's such a big deal right now about

656
00:25:06,720 --> 00:25:08,480
international data transfer and if you

657
00:25:08,480 --> 00:25:10,799
hadn't looked into it yet now you know

658
00:25:10,799 --> 00:25:14,080
why and is does this actually happen

659
00:25:14,080 --> 00:25:17,039
there was an interesting

660
00:25:17,039 --> 00:25:19,600
blog article from tech robot the other

661
00:25:19,600 --> 00:25:21,279
day who looked at the responsible

662
00:25:21,279 --> 00:25:23,360
disclosure reports from facebook and

663
00:25:23,360 --> 00:25:24,799
apple and twitter

664
00:25:24,799 --> 00:25:27,279
and compiled numbers and you can see yes

665
00:25:27,279 --> 00:25:29,039
these requests happen

666
00:25:29,039 --> 00:25:31,360
um so it is an actual problem

667
00:25:31,360 --> 00:25:32,480
and

668
00:25:32,480 --> 00:25:34,400
you know is it likely to happen to

669
00:25:34,400 --> 00:25:36,320
somebody i don't know

670
00:25:36,320 --> 00:25:38,640
but when the europeans all say oh man

671
00:25:38,640 --> 00:25:41,039
how can we transfer data to u.s service

672
00:25:41,039 --> 00:25:42,400
providers

673
00:25:42,400 --> 00:25:44,080
that's what they're talking about and

674
00:25:44,080 --> 00:25:46,400
are worried about

675
00:25:46,400 --> 00:25:48,480
so

676
00:25:48,480 --> 00:25:51,760
next thing i wanted to do

677
00:25:53,520 --> 00:25:54,480
is to

678
00:25:54,480 --> 00:25:56,400
put together a little toolbox for you

679
00:25:56,400 --> 00:25:58,480
all so

680
00:25:58,480 --> 00:26:00,720
if we actually want to

681
00:26:00,720 --> 00:26:02,799
assess privacy impacts in a more

682
00:26:02,799 --> 00:26:04,880
methodic manner are there any tools we

683
00:26:04,880 --> 00:26:08,159
can use what can we look at

684
00:26:08,159 --> 00:26:10,480
and so again

685
00:26:10,480 --> 00:26:12,400
um

686
00:26:12,400 --> 00:26:14,159
there's a copy of the slides and chat

687
00:26:14,159 --> 00:26:15,840
you can download it and click on the

688
00:26:15,840 --> 00:26:17,679
links i've added to most of these slides

689
00:26:17,679 --> 00:26:18,799
you don't you don't need to write them

690
00:26:18,799 --> 00:26:19,679
down

691
00:26:19,679 --> 00:26:21,039
um so

692
00:26:21,039 --> 00:26:23,120
first question is like when is actually

693
00:26:23,120 --> 00:26:24,640
the right timing

694
00:26:24,640 --> 00:26:26,799
to do you know a privacy impact

695
00:26:26,799 --> 00:26:29,279
assessment

696
00:26:30,000 --> 00:26:32,240
and so you know i

697
00:26:32,240 --> 00:26:33,760
i'm gonna ask back to you as the

698
00:26:33,760 --> 00:26:36,159
application security professionals when

699
00:26:36,159 --> 00:26:37,919
do you do threat modeling for your

700
00:26:37,919 --> 00:26:39,840
applications

701
00:26:39,840 --> 00:26:42,480
earlier is better right ideally before a

702
00:26:42,480 --> 00:26:45,120
product hits the market

703
00:26:45,120 --> 00:26:48,400
but it's never too late either

704
00:26:48,400 --> 00:26:50,559
there is a

705
00:26:50,559 --> 00:26:52,720
an interesting concept called privacy

706
00:26:52,720 --> 00:26:54,720
threshold analysis

707
00:26:54,720 --> 00:26:55,520
um

708
00:26:55,520 --> 00:26:56,400
that

709
00:26:56,400 --> 00:26:58,640
you might want to think about that

710
00:26:58,640 --> 00:27:01,679
you know when i think about

711
00:27:01,679 --> 00:27:03,360
software development life cycles and

712
00:27:03,360 --> 00:27:04,960
devops

713
00:27:04,960 --> 00:27:06,559
maybe that could work where basically

714
00:27:06,559 --> 00:27:07,919
you say

715
00:27:07,919 --> 00:27:10,480
i have these five or ten questions that

716
00:27:10,480 --> 00:27:12,799
an application or product owner

717
00:27:12,799 --> 00:27:14,240
should be able to

718
00:27:14,240 --> 00:27:16,320
to answer and that will tell me as the

719
00:27:16,320 --> 00:27:19,039
privacy person

720
00:27:19,039 --> 00:27:21,039
whether there's a potential high risk on

721
00:27:21,039 --> 00:27:23,200
the privacy side or not

722
00:27:23,200 --> 00:27:25,600
and and if there isn't i just let them

723
00:27:25,600 --> 00:27:26,480
be

724
00:27:26,480 --> 00:27:29,039
and do their thing if if there's a red

725
00:27:29,039 --> 00:27:31,840
flag then i might come and say hey

726
00:27:31,840 --> 00:27:34,320
product owner can we do a privacy impact

727
00:27:34,320 --> 00:27:36,879
assessment

728
00:27:37,600 --> 00:27:39,919
and then last thing here sure when the

729
00:27:39,919 --> 00:27:41,840
law requires it and if you look at all

730
00:27:41,840 --> 00:27:44,000
these different legal statutes

731
00:27:44,000 --> 00:27:45,919
that say you should think about privacy

732
00:27:45,919 --> 00:27:47,120
risks

733
00:27:47,120 --> 00:27:50,880
they many of them imply that you do it

734
00:27:50,880 --> 00:27:54,080
before you start an activity

735
00:27:54,080 --> 00:27:56,559
but of course

736
00:27:56,559 --> 00:27:58,799
especially in europe

737
00:27:58,799 --> 00:28:01,840
if you already have processes in place

738
00:28:01,840 --> 00:28:04,000
and they are risky in terms of like

739
00:28:04,000 --> 00:28:05,600
privacy impact

740
00:28:05,600 --> 00:28:06,960
you should still go back and do it

741
00:28:06,960 --> 00:28:09,200
because they might still come and ask

742
00:28:09,200 --> 00:28:11,520
for it

743
00:28:13,120 --> 00:28:15,760
how do we categorize problematic data

744
00:28:15,760 --> 00:28:17,919
actions right those of you who have done

745
00:28:17,919 --> 00:28:20,720
threat modeling before and like

746
00:28:20,720 --> 00:28:21,840
you know

747
00:28:21,840 --> 00:28:23,840
using methodologies like microsoft's

748
00:28:23,840 --> 00:28:26,720
stride or whatnot they all give you like

749
00:28:26,720 --> 00:28:28,880
categories of threats to help you think

750
00:28:28,880 --> 00:28:31,200
about what could happen

751
00:28:31,200 --> 00:28:33,840
and so similar

752
00:28:33,840 --> 00:28:36,720
there's a couple places we can look for

753
00:28:36,720 --> 00:28:38,880
where people have already thought about

754
00:28:38,880 --> 00:28:40,240
what might be

755
00:28:40,240 --> 00:28:42,640
types of problematic data actions that

756
00:28:42,640 --> 00:28:44,720
could happen

757
00:28:44,720 --> 00:28:48,000
there is a methodology from nist

758
00:28:48,000 --> 00:28:49,360
called privacy risk assessment

759
00:28:49,360 --> 00:28:52,559
methodology pram

760
00:28:52,559 --> 00:28:54,320
that has a list

761
00:28:54,320 --> 00:28:56,799
there's also a frequently cited paper

762
00:28:56,799 --> 00:28:58,799
from salary

763
00:28:58,799 --> 00:29:00,960
that also tries to think about well

764
00:29:00,960 --> 00:29:03,120
problematic data actions so it could be

765
00:29:03,120 --> 00:29:05,360
surveillance could be

766
00:29:05,360 --> 00:29:08,240
i don't know data aggregation

767
00:29:08,240 --> 00:29:10,799
um identifying

768
00:29:10,799 --> 00:29:12,720
you know

769
00:29:12,720 --> 00:29:15,360
the i identify data again with

770
00:29:15,360 --> 00:29:16,720
individuals

771
00:29:16,720 --> 00:29:18,799
um all sorts of things and then of

772
00:29:18,799 --> 00:29:19,919
course

773
00:29:19,919 --> 00:29:22,559
also security issues right like a data

774
00:29:22,559 --> 00:29:24,399
breach could happen and then somebody's

775
00:29:24,399 --> 00:29:26,640
personal data gets leaked to the world

776
00:29:26,640 --> 00:29:28,399
and that's also a problematic data

777
00:29:28,399 --> 00:29:30,559
action

778
00:29:30,559 --> 00:29:32,559
so there's there's some

779
00:29:32,559 --> 00:29:34,880
catalogs to to check into

780
00:29:34,880 --> 00:29:37,200
and then same question

781
00:29:37,200 --> 00:29:39,919
on the privacy homicide how can we

782
00:29:39,919 --> 00:29:42,320
categorize

783
00:29:42,320 --> 00:29:44,799
the potential harm

784
00:29:44,799 --> 00:29:48,240
that an individual might experience

785
00:29:48,240 --> 00:29:50,559
as a result of the problematic data

786
00:29:50,559 --> 00:29:53,360
action that we are responsible for

787
00:29:53,360 --> 00:29:55,679
and so there's again a catalogue in this

788
00:29:55,679 --> 00:29:57,039
program

789
00:29:57,039 --> 00:29:58,799
there's a paper here

790
00:29:58,799 --> 00:30:00,960
again from solovey and

791
00:30:00,960 --> 00:30:02,480
keats

792
00:30:02,480 --> 00:30:04,799
so those are some some places to start

793
00:30:04,799 --> 00:30:05,919
it

794
00:30:05,919 --> 00:30:07,440
so just if you look at some of these

795
00:30:07,440 --> 00:30:09,039
homes

796
00:30:09,039 --> 00:30:11,279
we brushed over this earlier

797
00:30:11,279 --> 00:30:14,320
you know could be physical harm

798
00:30:14,320 --> 00:30:16,640
maybe a car crashed because like i don't

799
00:30:16,640 --> 00:30:17,760
know

800
00:30:17,760 --> 00:30:20,640
said personal data maybe not

801
00:30:20,640 --> 00:30:22,559
economic harms that's what's what's

802
00:30:22,559 --> 00:30:24,880
obvious to our horse

803
00:30:24,880 --> 00:30:27,919
could lose money could be reputation

804
00:30:27,919 --> 00:30:29,840
could be discrimination could be

805
00:30:29,840 --> 00:30:32,159
relationships relationships in your

806
00:30:32,159 --> 00:30:35,039
private life or with your employer in

807
00:30:35,039 --> 00:30:37,039
public life

808
00:30:37,039 --> 00:30:39,279
could be psychological harms

809
00:30:39,279 --> 00:30:41,600
you know

810
00:30:41,600 --> 00:30:43,520
you get distressed about

811
00:30:43,520 --> 00:30:45,679
people like surveilling you and you know

812
00:30:45,679 --> 00:30:48,880
that's causing you impact

813
00:30:48,880 --> 00:30:50,080
and then there's a whole list of

814
00:30:50,080 --> 00:30:53,520
autonomy autonomy autonomy harms

815
00:30:53,520 --> 00:30:54,320
um

816
00:30:54,320 --> 00:30:56,000
you know somebody could

817
00:30:56,000 --> 00:30:58,320
get personal data about you and try to

818
00:30:58,320 --> 00:30:59,360
uh

819
00:30:59,360 --> 00:31:00,320
um

820
00:31:00,320 --> 00:31:01,279
you know

821
00:31:01,279 --> 00:31:04,720
manipulate you or you know in terms of

822
00:31:04,720 --> 00:31:07,679
voting maybe who knows or correction or

823
00:31:07,679 --> 00:31:09,519
blackmail

824
00:31:09,519 --> 00:31:13,919
so some catalogs that can get us started

825
00:31:14,000 --> 00:31:16,399
um

826
00:31:16,720 --> 00:31:19,600
and then you know what are the controls

827
00:31:19,600 --> 00:31:22,159
available the the technical or

828
00:31:22,159 --> 00:31:24,080
organizing organizational measures we

829
00:31:24,080 --> 00:31:25,120
can use

830
00:31:25,120 --> 00:31:26,559
to reduce

831
00:31:26,559 --> 00:31:29,360
um privacy impact we might be causing

832
00:31:29,360 --> 00:31:30,480
um

833
00:31:30,480 --> 00:31:32,640
there's there's two buzzwords

834
00:31:32,640 --> 00:31:33,600
um

835
00:31:33,600 --> 00:31:35,200
that if you're interested you can you

836
00:31:35,200 --> 00:31:38,640
can dive into one is privacy by design

837
00:31:38,640 --> 00:31:39,600
um

838
00:31:39,600 --> 00:31:41,919
and so those are really uh process

839
00:31:41,919 --> 00:31:43,360
principles

840
00:31:43,360 --> 00:31:45,600
for like how can we design applications

841
00:31:45,600 --> 00:31:47,200
and processes

842
00:31:47,200 --> 00:31:50,159
with privacy in mind

843
00:31:50,159 --> 00:31:52,720
you know things like well

844
00:31:52,720 --> 00:31:55,519
privacy options like any settings you

845
00:31:55,519 --> 00:31:57,919
have that impact a person's

846
00:31:57,919 --> 00:32:00,559
data that's being processed the default

847
00:32:00,559 --> 00:32:02,320
should always be privacy-minded the

848
00:32:02,320 --> 00:32:04,799
default should always be

849
00:32:04,799 --> 00:32:08,720
we don't do it until you opt-in

850
00:32:09,440 --> 00:32:12,000
etc and then

851
00:32:12,000 --> 00:32:15,039
the second thing is privacy enhancing

852
00:32:15,039 --> 00:32:17,760
technologies pets

853
00:32:17,760 --> 00:32:18,960
and so

854
00:32:18,960 --> 00:32:20,960
that's a whole

855
00:32:20,960 --> 00:32:22,880
not super well

856
00:32:22,880 --> 00:32:24,640
categorized

857
00:32:24,640 --> 00:32:26,399
um

858
00:32:26,399 --> 00:32:29,279
reptile of availability of technical

859
00:32:29,279 --> 00:32:30,799
measures you could use you know it

860
00:32:30,799 --> 00:32:32,960
starts with

861
00:32:32,960 --> 00:32:35,200
let's just not collect the data or like

862
00:32:35,200 --> 00:32:38,320
once we have identity proof to you

863
00:32:38,320 --> 00:32:40,000
let's uh let's delete your social

864
00:32:40,000 --> 00:32:41,840
security number instead of storing it in

865
00:32:41,840 --> 00:32:44,399
our database forever

866
00:32:44,399 --> 00:32:46,080
you know it could be what's called

867
00:32:46,080 --> 00:32:47,600
pseudonymization

868
00:32:47,600 --> 00:32:49,760
let's keep identifiers over here in the

869
00:32:49,760 --> 00:32:52,320
data silo and other data that we need

870
00:32:52,320 --> 00:32:54,880
for statistics somewhere else and we

871
00:32:54,880 --> 00:32:57,760
only re-identify

872
00:32:57,760 --> 00:32:59,679
put those together if we need to but we

873
00:32:59,679 --> 00:33:02,799
keep them in different silos

874
00:33:02,799 --> 00:33:06,399
aggregation trying to anonymize data

875
00:33:06,399 --> 00:33:09,120
is a thing encrypting data and transfer

876
00:33:09,120 --> 00:33:11,679
on storage is also a privacy enhancing

877
00:33:11,679 --> 00:33:14,320
technology

878
00:33:14,440 --> 00:33:17,039
decentralizing storage can we figure out

879
00:33:17,039 --> 00:33:18,960
how to store somebody's personal data

880
00:33:18,960 --> 00:33:20,640
only on their phone

881
00:33:20,640 --> 00:33:24,720
so that it's not all in our database

882
00:33:24,720 --> 00:33:27,279
last example i put in here is like if we

883
00:33:27,279 --> 00:33:28,960
publish data sets

884
00:33:28,960 --> 00:33:31,440
can we introduce randomness so that it's

885
00:33:31,440 --> 00:33:33,519
easier

886
00:33:33,519 --> 00:33:36,559
that it's harder for adversaries to

887
00:33:36,559 --> 00:33:40,399
say oh this data belongs to david

888
00:33:40,399 --> 00:33:41,840
um

889
00:33:41,840 --> 00:33:44,240
and there's more so something else to

890
00:33:44,240 --> 00:33:46,720
look into

891
00:33:47,279 --> 00:33:49,440
are there any tools that do all of this

892
00:33:49,440 --> 00:33:50,799
for us

893
00:33:50,799 --> 00:33:52,960
i put a couple pointers in here and and

894
00:33:52,960 --> 00:33:54,240
remember that we need to think about

895
00:33:54,240 --> 00:33:55,760
context

896
00:33:55,760 --> 00:33:57,600
so for example the

897
00:33:57,600 --> 00:33:59,600
french

898
00:33:59,600 --> 00:34:02,320
privacy supervisory authority has an

899
00:34:02,320 --> 00:34:03,840
actual tool

900
00:34:03,840 --> 00:34:05,679
you can use to do privacy impact

901
00:34:05,679 --> 00:34:06,880
assessments

902
00:34:06,880 --> 00:34:09,040
that meet

903
00:34:09,040 --> 00:34:12,000
that agency conneal's interpretation of

904
00:34:12,000 --> 00:34:14,399
what's required under gdpr for privacy

905
00:34:14,399 --> 00:34:16,560
impact assessments so for that context

906
00:34:16,560 --> 00:34:18,960
you could look at that

907
00:34:18,960 --> 00:34:22,399
we mentioned nist's methodology

908
00:34:22,399 --> 00:34:24,639
if you're interested in transfer impact

909
00:34:24,639 --> 00:34:26,399
assessments

910
00:34:26,399 --> 00:34:28,560
again probably go hire a lawyer but the

911
00:34:28,560 --> 00:34:31,520
iapp has published some excel templates

912
00:34:31,520 --> 00:34:33,199
you can play with

913
00:34:33,199 --> 00:34:34,800
and of course there's lots of like

914
00:34:34,800 --> 00:34:36,560
commercial tools that tell you that

915
00:34:36,560 --> 00:34:38,879
they'll automate it for you and

916
00:34:38,879 --> 00:34:41,359
do it all like you know by themselves

917
00:34:41,359 --> 00:34:43,359
without you even looking at it

918
00:34:43,359 --> 00:34:45,599
um but so here's some things to look

919
00:34:45,599 --> 00:34:46,639
into

920
00:34:46,639 --> 00:34:49,359
and then

921
00:34:49,359 --> 00:34:51,918
last slide on this is like if you found

922
00:34:51,918 --> 00:34:53,839
this interesting today and want to learn

923
00:34:53,839 --> 00:34:55,359
more about privacy

924
00:34:55,359 --> 00:34:56,879
where can you go to learn more about

925
00:34:56,879 --> 00:34:58,320
privacy in general

926
00:34:58,320 --> 00:35:00,240
i put some links on here you can you can

927
00:35:00,240 --> 00:35:02,800
click on them later um

928
00:35:02,800 --> 00:35:06,400
so let's recap

929
00:35:06,560 --> 00:35:08,880
um

930
00:35:09,119 --> 00:35:11,359
why in the world would we want to do all

931
00:35:11,359 --> 00:35:14,720
of this in the first place

932
00:35:14,839 --> 00:35:19,759
anybody what motivates us to do this

933
00:35:21,599 --> 00:35:24,960
yes yes yes

934
00:35:26,320 --> 00:35:28,960
yes have some pens please

935
00:35:28,960 --> 00:35:32,079
do you want to end up prince

936
00:35:32,079 --> 00:35:35,119
uh anything else

937
00:35:36,560 --> 00:35:39,680
who who saw a dan cornell's keynote this

938
00:35:39,680 --> 00:35:41,839
morning

939
00:35:41,839 --> 00:35:43,440
yeah so

940
00:35:43,440 --> 00:35:45,599
it was like really when he said yeah at

941
00:35:45,599 --> 00:35:47,119
the end of the day the only thing that

942
00:35:47,119 --> 00:35:50,400
motivates corporations is like

943
00:35:50,400 --> 00:35:51,760
money

944
00:35:51,760 --> 00:35:54,400
um i i think that's probably true

945
00:35:54,400 --> 00:35:58,160
um but i i also like to think morality

946
00:35:58,160 --> 00:35:59,680
right and there's things that that you

947
00:35:59,680 --> 00:36:01,440
can turn into incentives for you as a

948
00:36:01,440 --> 00:36:04,160
company like establishing customer trust

949
00:36:04,160 --> 00:36:05,200
um

950
00:36:05,200 --> 00:36:07,599
maybe your clients ask you for it

951
00:36:07,599 --> 00:36:10,640
um cherise in in her talk earlier today

952
00:36:10,640 --> 00:36:13,520
like she was like

953
00:36:13,520 --> 00:36:15,920
said she had seen

954
00:36:15,920 --> 00:36:17,839
cyber insurance

955
00:36:17,839 --> 00:36:19,440
giving lower

956
00:36:19,440 --> 00:36:21,520
premiums to companies if they had done a

957
00:36:21,520 --> 00:36:22,960
data protection

958
00:36:22,960 --> 00:36:25,440
impact assessment i was like oh cool

959
00:36:25,440 --> 00:36:27,599
um but yeah at the end of the day you

960
00:36:27,599 --> 00:36:30,640
know we want it to be about morality and

961
00:36:30,640 --> 00:36:33,280
respecting uh privacy as as an essential

962
00:36:33,280 --> 00:36:34,960
human right but

963
00:36:34,960 --> 00:36:35,920
um

964
00:36:35,920 --> 00:36:37,839
you know it might be the legal finds

965
00:36:37,839 --> 00:36:39,760
whatever that motivates at the end of

966
00:36:39,760 --> 00:36:40,800
the day

967
00:36:40,800 --> 00:36:42,240
um

968
00:36:42,240 --> 00:36:43,839
so

969
00:36:43,839 --> 00:36:46,480
just to make this analogy one more time

970
00:36:46,480 --> 00:36:48,240
because that's what i really was trying

971
00:36:48,240 --> 00:36:50,160
to get at today why am i talking about

972
00:36:50,160 --> 00:36:53,040
this at an abstract conference right um

973
00:36:53,040 --> 00:36:56,000
it's if you look at

974
00:36:56,000 --> 00:36:57,920
the upper right here and look at how we

975
00:36:57,920 --> 00:37:00,720
do security threat modeling

976
00:37:00,720 --> 00:37:03,520
in appsec we look at our security goals

977
00:37:03,520 --> 00:37:05,359
we look at

978
00:37:05,359 --> 00:37:07,599
okay what's what's our application

979
00:37:07,599 --> 00:37:10,240
actually look like and what threats are

980
00:37:10,240 --> 00:37:13,280
there that could violate these goals

981
00:37:13,280 --> 00:37:15,200
how can we mitigate them

982
00:37:15,200 --> 00:37:17,040
we should probably test the mitigation

983
00:37:17,040 --> 00:37:19,119
once we've implemented it and then we

984
00:37:19,119 --> 00:37:20,560
start over right

985
00:37:20,560 --> 00:37:21,359
and

986
00:37:21,359 --> 00:37:23,119
it's

987
00:37:23,119 --> 00:37:25,520
exactly the same thing on the privacy

988
00:37:25,520 --> 00:37:27,200
side we look at what are the privacy

989
00:37:27,200 --> 00:37:28,640
goals

990
00:37:28,640 --> 00:37:30,800
where all do we process personal data

991
00:37:30,800 --> 00:37:32,240
where does it go

992
00:37:32,240 --> 00:37:35,040
what are the potential

993
00:37:35,040 --> 00:37:37,119
problematic data actions and harms we

994
00:37:37,119 --> 00:37:39,040
can cause to individuals

995
00:37:39,040 --> 00:37:40,960
by doing this

996
00:37:40,960 --> 00:37:42,640
and it's the way we can mitigate it and

997
00:37:42,640 --> 00:37:44,320
then we go in the circle

998
00:37:44,320 --> 00:37:46,960
and then of course

999
00:37:48,000 --> 00:37:50,720
usually outside of the realm of the the

1000
00:37:50,720 --> 00:37:52,880
pure abstract persons there's like a

1001
00:37:52,880 --> 00:37:54,480
corporate risk management function that

1002
00:37:54,480 --> 00:37:56,560
then looks at this and says okay what's

1003
00:37:56,560 --> 00:37:59,680
the risk to us you know as a company

1004
00:37:59,680 --> 00:38:01,680
when we try to translate this into into

1005
00:38:01,680 --> 00:38:03,599
dollars

1006
00:38:03,599 --> 00:38:05,280
and are we doing enough are we not doing

1007
00:38:05,280 --> 00:38:09,200
enough about it but so it's you know

1008
00:38:09,200 --> 00:38:11,040
pretty similar

1009
00:38:11,040 --> 00:38:12,560
um

1010
00:38:12,560 --> 00:38:13,839
and so

1011
00:38:13,839 --> 00:38:16,960
to to summarize once more privacy impact

1012
00:38:16,960 --> 00:38:18,400
assessments

1013
00:38:18,400 --> 00:38:20,000
um

1014
00:38:20,000 --> 00:38:22,240
look at the the potential for violating

1015
00:38:22,240 --> 00:38:23,839
privacy goals

1016
00:38:23,839 --> 00:38:26,079
um i think for us techies it's always

1017
00:38:26,079 --> 00:38:27,680
important to keep in mind that privacy

1018
00:38:27,680 --> 00:38:29,440
goals can be

1019
00:38:29,440 --> 00:38:30,560
um

1020
00:38:30,560 --> 00:38:32,400
you know i

1021
00:38:32,400 --> 00:38:34,240
violated

1022
00:38:34,240 --> 00:38:36,000
not just on the technical side but also

1023
00:38:36,000 --> 00:38:37,760
on the process side i can do stuff with

1024
00:38:37,760 --> 00:38:40,079
data that's not not in an application

1025
00:38:40,079 --> 00:38:42,400
and that can still be an issue

1026
00:38:42,400 --> 00:38:46,640
um so good to keep that in mind

1027
00:38:46,640 --> 00:38:48,320
and so again we're looking at the harm

1028
00:38:48,320 --> 00:38:50,400
done to individuals

1029
00:38:50,400 --> 00:38:52,320
primarily we're not supposed to look at

1030
00:38:52,320 --> 00:38:53,599
oh

1031
00:38:53,599 --> 00:38:56,320
if we do this can we get away with that

1032
00:38:56,320 --> 00:38:58,160
might we get a fine

1033
00:38:58,160 --> 00:39:00,000
that's not the intent of a privacy

1034
00:39:00,000 --> 00:39:01,599
impact assessment

1035
00:39:01,599 --> 00:39:03,599
um

1036
00:39:03,599 --> 00:39:05,839
minimize the impact where you can

1037
00:39:05,839 --> 00:39:08,320
and then you know assess the risk is

1038
00:39:08,320 --> 00:39:10,320
what we're doing proportionate with the

1039
00:39:10,320 --> 00:39:11,440
purpose

1040
00:39:11,440 --> 00:39:15,839
is the reason we're doing this for

1041
00:39:17,040 --> 00:39:19,359
you know

1042
00:39:19,359 --> 00:39:22,000
appropriate for the risks we're creating

1043
00:39:22,000 --> 00:39:24,160
for our customers or employees or

1044
00:39:24,160 --> 00:39:25,920
whoever the individuals are whose data

1045
00:39:25,920 --> 00:39:28,720
we're processing

1046
00:39:28,720 --> 00:39:30,720
you know and then like we just said at

1047
00:39:30,720 --> 00:39:32,560
the end of the day sure it always

1048
00:39:32,560 --> 00:39:34,880
translates into

1049
00:39:34,880 --> 00:39:38,800
into corporate risks too right um

1050
00:39:38,800 --> 00:39:41,200
but if a

1051
00:39:41,200 --> 00:39:43,280
in the regulatory space if somebody

1052
00:39:43,280 --> 00:39:45,040
comes and says hey where's your privacy

1053
00:39:45,040 --> 00:39:46,720
impact assessments they care about the

1054
00:39:46,720 --> 00:39:48,400
for first of all about it's not about

1055
00:39:48,400 --> 00:39:49,520
the fifth

1056
00:39:49,520 --> 00:39:50,640
um

1057
00:39:50,640 --> 00:39:52,079
and then

1058
00:39:52,079 --> 00:39:56,240
what i also put in here um

1059
00:39:56,240 --> 00:39:59,680
i really think these first three things

1060
00:39:59,680 --> 00:40:02,720
that that is things where we as as absec

1061
00:40:02,720 --> 00:40:04,880
practitioner practitioners can

1062
00:40:04,880 --> 00:40:06,400
we can help with that we can look at

1063
00:40:06,400 --> 00:40:09,280
that we understand where the data is um

1064
00:40:09,280 --> 00:40:10,800
and you know

1065
00:40:10,800 --> 00:40:13,839
threat modeling privacy impact modeling

1066
00:40:13,839 --> 00:40:15,680
we can totally help with that and take

1067
00:40:15,680 --> 00:40:18,560
initiative when it comes down to

1068
00:40:18,560 --> 00:40:20,240
assessing okay

1069
00:40:20,240 --> 00:40:22,079
what's the risk if

1070
00:40:22,079 --> 00:40:23,839
we get sued or have a data breach or

1071
00:40:23,839 --> 00:40:25,440
whatever that's probably not our job

1072
00:40:25,440 --> 00:40:27,440
that's where the executives and risk

1073
00:40:27,440 --> 00:40:30,000
managers and lawyers are on the hook but

1074
00:40:30,000 --> 00:40:31,920
there's some things we can do if if

1075
00:40:31,920 --> 00:40:33,200
we're interested

1076
00:40:33,200 --> 00:40:34,560
in doing that

1077
00:40:34,560 --> 00:40:39,560
um does that make sense any questions

1078
00:40:39,599 --> 00:40:43,680
okay i have one last doodle

1079
00:40:43,680 --> 00:40:47,359
and then we're done so

1080
00:40:47,359 --> 00:40:49,359
let's assume for a moment you're a

1081
00:40:49,359 --> 00:40:50,560
startup

1082
00:40:50,560 --> 00:40:53,119
and what you're trying to do is

1083
00:40:53,119 --> 00:40:55,599
digitize the way that people prove their

1084
00:40:55,599 --> 00:40:57,839
age when they buy

1085
00:40:57,839 --> 00:41:00,000
goods that are age restricted like

1086
00:41:00,000 --> 00:41:02,000
liquor or cigarettes or whatever right

1087
00:41:02,000 --> 00:41:04,640
and so let's just assume that

1088
00:41:04,640 --> 00:41:07,680
um you think you can convince whoever

1089
00:41:07,680 --> 00:41:09,599
government is issuing ideas to their

1090
00:41:09,599 --> 00:41:11,040
citizens that

1091
00:41:11,040 --> 00:41:14,480
you can do that as a trusted third party

1092
00:41:14,480 --> 00:41:16,880
so you have a plan on how you can do

1093
00:41:16,880 --> 00:41:20,000
that you basically enroll new customers

1094
00:41:20,000 --> 00:41:22,400
in your app by letting them scan their

1095
00:41:22,400 --> 00:41:23,520
id

1096
00:41:23,520 --> 00:41:24,640
and then you run that through an

1097
00:41:24,640 --> 00:41:26,480
identity verification provider to make

1098
00:41:26,480 --> 00:41:29,359
sure it's it's a real id it's them

1099
00:41:29,359 --> 00:41:31,839
those services exist no problem

1100
00:41:31,839 --> 00:41:34,800
and then we say okay so usually an issue

1101
00:41:34,800 --> 00:41:36,720
is account recovery somebody forgets

1102
00:41:36,720 --> 00:41:39,359
their credentials and you say okay

1103
00:41:39,359 --> 00:41:40,560
um

1104
00:41:40,560 --> 00:41:42,960
we'll just keep these scanned ids on

1105
00:41:42,960 --> 00:41:44,880
file and if somebody forgets the

1106
00:41:44,880 --> 00:41:47,200
credentials for the app they can just

1107
00:41:47,200 --> 00:41:49,520
re-upload the scan of their id we

1108
00:41:49,520 --> 00:41:51,359
compare to what we have on file and then

1109
00:41:51,359 --> 00:41:54,079
we let them back in

1110
00:41:54,079 --> 00:41:56,160
and on the merchant side if a customer

1111
00:41:56,160 --> 00:41:58,720
walks into a store and wants to buy a

1112
00:41:58,720 --> 00:42:01,520
bottle of tequila

1113
00:42:01,520 --> 00:42:02,560
they can

1114
00:42:02,560 --> 00:42:05,440
use the app to scan a customer's

1115
00:42:05,440 --> 00:42:07,839
qr code and then on their end get

1116
00:42:07,839 --> 00:42:10,400
presented the date of birth and the

1117
00:42:10,400 --> 00:42:12,319
address and the photo of the customer

1118
00:42:12,319 --> 00:42:14,000
and so that's that way they don't need

1119
00:42:14,000 --> 00:42:15,119
to

1120
00:42:15,119 --> 00:42:17,280
have their driver's license or passport

1121
00:42:17,280 --> 00:42:19,280
or whatever on them they can just use

1122
00:42:19,280 --> 00:42:20,480
their phone

1123
00:42:20,480 --> 00:42:21,599
and

1124
00:42:21,599 --> 00:42:23,520
of course for stats for statistics

1125
00:42:23,520 --> 00:42:25,440
purposes you will keep

1126
00:42:25,440 --> 00:42:27,359
a log of all interactions between

1127
00:42:27,359 --> 00:42:29,200
merchants and customers

1128
00:42:29,200 --> 00:42:32,160
makes sense to film

1129
00:42:32,400 --> 00:42:34,800
so my question for you all is is there

1130
00:42:34,800 --> 00:42:37,359
anything we can do

1131
00:42:37,359 --> 00:42:42,000
to make this more privacy minded

1132
00:42:43,280 --> 00:42:45,280
and we want to enable the business right

1133
00:42:45,280 --> 00:42:46,480
so we don't

1134
00:42:46,480 --> 00:42:49,520
want to say we can't do this at all

1135
00:42:49,520 --> 00:42:51,359
um so if you look at your phones you

1136
00:42:51,359 --> 00:42:52,160
have

1137
00:42:52,160 --> 00:42:54,640
options there and

1138
00:42:54,640 --> 00:42:56,880
this is a multiple choice one so you can

1139
00:42:56,880 --> 00:42:59,920
choose multiple of these answers

1140
00:42:59,920 --> 00:43:01,760
and you gotta hit the submit button in

1141
00:43:01,760 --> 00:43:03,440
the in the lower right corner once

1142
00:43:03,440 --> 00:43:05,839
you've chosen um but so let's look at

1143
00:43:05,839 --> 00:43:07,119
our options

1144
00:43:07,119 --> 00:43:08,240
um

1145
00:43:08,240 --> 00:43:10,160
you could say i scan the ids at the

1146
00:43:10,160 --> 00:43:12,560
beginning but then i don't store them

1147
00:43:12,560 --> 00:43:15,040
if somebody needs to

1148
00:43:15,040 --> 00:43:16,560
recover their account because they

1149
00:43:16,560 --> 00:43:18,880
forgot their credentials

1150
00:43:18,880 --> 00:43:20,240
they can just

1151
00:43:20,240 --> 00:43:22,560
rescan their id and we just send it

1152
00:43:22,560 --> 00:43:23,920
through the identity verification

1153
00:43:23,920 --> 00:43:26,400
provider again and maybe that'll cost us

1154
00:43:26,400 --> 00:43:28,319
a couple of cents but that way

1155
00:43:28,319 --> 00:43:30,240
we don't have like a database of ids

1156
00:43:30,240 --> 00:43:31,839
sitting around

1157
00:43:31,839 --> 00:43:33,119
you could say

1158
00:43:33,119 --> 00:43:34,640
second one here

1159
00:43:34,640 --> 00:43:36,640
well all we need to show to the merchant

1160
00:43:36,640 --> 00:43:40,319
is really the h is or not even that you

1161
00:43:40,319 --> 00:43:42,240
could also just say is the person old

1162
00:43:42,240 --> 00:43:44,640
enough or not to buy your goods

1163
00:43:44,640 --> 00:43:47,119
um you don't need to show them

1164
00:43:47,119 --> 00:43:49,040
you know the date of birth or their

1165
00:43:49,040 --> 00:43:50,800
photo or their address

1166
00:43:50,800 --> 00:43:53,520
merchant doesn't need to know that

1167
00:43:53,520 --> 00:43:56,800
or you could say

1168
00:43:57,119 --> 00:43:59,520
all these logs of customer and merchant

1169
00:43:59,520 --> 00:44:01,040
interactions we're going to aggregate

1170
00:44:01,040 --> 00:44:03,119
them so that they're not

1171
00:44:03,119 --> 00:44:05,200
nobody can go in that database anymore

1172
00:44:05,200 --> 00:44:07,280
and see

1173
00:44:07,280 --> 00:44:09,359
how many times david went to the liquor

1174
00:44:09,359 --> 00:44:12,078
store this week

1175
00:44:12,640 --> 00:44:14,880
and or you could say

1176
00:44:14,880 --> 00:44:16,960
hey let's let's do a pen test and make

1177
00:44:16,960 --> 00:44:18,960
sure that our infrastructure is secure

1178
00:44:18,960 --> 00:44:20,560
so that that personal data we're

1179
00:44:20,560 --> 00:44:21,760
processing

1180
00:44:21,760 --> 00:44:22,400
is

1181
00:44:22,400 --> 00:44:24,079
less likely to be exposed in the data

1182
00:44:24,079 --> 00:44:25,839
breach

1183
00:44:25,839 --> 00:44:27,680
and or you could say

1184
00:44:27,680 --> 00:44:30,240
let's put a data processing agreement in

1185
00:44:30,240 --> 00:44:32,400
place of our identity verification

1186
00:44:32,400 --> 00:44:34,000
providers so

1187
00:44:34,000 --> 00:44:34,800
that

1188
00:44:34,800 --> 00:44:37,280
we specify exactly what they can do with

1189
00:44:37,280 --> 00:44:38,800
the data that we sent to them and

1190
00:44:38,800 --> 00:44:41,200
whatnot

1191
00:44:41,200 --> 00:44:42,079
so

1192
00:44:42,079 --> 00:44:44,400
if all of you have voted let's look at

1193
00:44:44,400 --> 00:44:46,160
the options

1194
00:44:46,160 --> 00:44:48,560
and votes

1195
00:44:48,560 --> 00:44:50,880
anybody still voting

1196
00:44:50,880 --> 00:44:53,119
okay

1197
00:44:53,119 --> 00:44:55,040
um

1198
00:44:55,040 --> 00:44:56,400
yeah

1199
00:44:56,400 --> 00:44:58,079
i too think all of these are good

1200
00:44:58,079 --> 00:44:59,760
options

1201
00:44:59,760 --> 00:45:01,280
but but so that's

1202
00:45:01,280 --> 00:45:03,359
a lot of this is more like on the

1203
00:45:03,359 --> 00:45:04,880
technical side you can actually do

1204
00:45:04,880 --> 00:45:07,200
things to to minimize privacy risk to

1205
00:45:07,200 --> 00:45:09,599
individuals right

1206
00:45:09,599 --> 00:45:11,200
so there's a little bit of legalese at

1207
00:45:11,200 --> 00:45:12,640
the bottom there but there's a lot of

1208
00:45:12,640 --> 00:45:16,078
technical things you can do as well

1209
00:45:16,160 --> 00:45:18,800
that's all i had to say but i'm happy to

1210
00:45:18,800 --> 00:45:20,560
answer any questions if you guys have

1211
00:45:20,560 --> 00:45:22,799
any

1212
00:45:25,280 --> 00:45:28,280
okay

1213
00:45:31,119 --> 00:45:33,680
uh that's a very good question i was

1214
00:45:33,680 --> 00:45:35,280
actually at a privacy conference last

1215
00:45:35,280 --> 00:45:37,440
week and somebody was like okay so how

1216
00:45:37,440 --> 00:45:39,440
far away are we from

1217
00:45:39,440 --> 00:45:40,319
um

1218
00:45:40,319 --> 00:45:42,720
from national privacy legislation in the

1219
00:45:42,720 --> 00:45:44,319
u.s and

1220
00:45:44,319 --> 00:45:46,800
the people there also just had ideas

1221
00:45:46,800 --> 00:45:49,839
probably not next year

1222
00:45:51,200 --> 00:45:53,440
google and facebook have really good

1223
00:45:53,440 --> 00:45:56,160
lobbyists um

1224
00:45:56,240 --> 00:45:58,640
yeah

1225
00:45:58,640 --> 00:46:01,640
cool

1226
00:46:08,480 --> 00:46:10,079
well thanks everybody you are welcome to

1227
00:46:10,079 --> 00:46:11,440
follow up with me

1228
00:46:11,440 --> 00:46:15,000
questions anything

