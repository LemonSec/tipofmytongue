1
00:00:04,080 --> 00:00:05,279
all right

2
00:00:05,279 --> 00:00:08,480
um next up we have carrie leaning

3
00:00:08,480 --> 00:00:11,599
hopefully i pronounced that right

4
00:00:11,599 --> 00:00:13,920
and she's going to be giving a talk on

5
00:00:13,920 --> 00:00:15,120
privacy shield is dead

6
00:00:15,120 --> 00:00:17,199
using technology to make the lawyers

7
00:00:17,199 --> 00:00:18,960
happy

8
00:00:18,960 --> 00:00:21,279
hi my name is carrie lenning thanks for

9
00:00:21,279 --> 00:00:22,800
attending

10
00:00:22,800 --> 00:00:24,640
today we'll be talking about the privacy

11
00:00:24,640 --> 00:00:26,880
shield why it's dead

12
00:00:26,880 --> 00:00:28,800
and how to use technology to make the

13
00:00:28,800 --> 00:00:30,720
lawyers happy when it comes to data

14
00:00:30,720 --> 00:00:33,279
transfers

15
00:00:34,640 --> 00:00:37,280
so a little bit about me i'm about 20

16
00:00:37,280 --> 00:00:38,399
years of experience

17
00:00:38,399 --> 00:00:40,239
in data protection information security

18
00:00:40,239 --> 00:00:41,440
and risk

19
00:00:41,440 --> 00:00:43,360
i've had a pretty diverse background

20
00:00:43,360 --> 00:00:45,120
doing risk and data protection for a lot

21
00:00:45,120 --> 00:00:46,719
of companies like palantir

22
00:00:46,719 --> 00:00:50,079
and facebook but now i'm a consultant

23
00:00:50,079 --> 00:00:51,840
and i just sell myself out to the

24
00:00:51,840 --> 00:00:54,160
highest bidder it's good times

25
00:00:54,160 --> 00:00:55,840
little bonafide is about me i'm a

26
00:00:55,840 --> 00:00:58,160
recovering lawyer an analyst and a

27
00:00:58,160 --> 00:00:58,960
consultant

28
00:00:58,960 --> 00:01:01,680
now i have some nice certifications

29
00:01:01,680 --> 00:01:03,520
after my name

30
00:01:03,520 --> 00:01:06,799
that said i am a recovering lawyer

31
00:01:06,799 --> 00:01:10,640
which means that this isn't legal advice

32
00:01:10,640 --> 00:01:14,240
also a quick warning this presentation

33
00:01:14,240 --> 00:01:16,400
is full of cats and memes so if you

34
00:01:16,400 --> 00:01:17,520
don't like cats

35
00:01:17,520 --> 00:01:20,798
it's probably going to be tough to watch

36
00:01:21,119 --> 00:01:23,280
all right so it makes sense to start off

37
00:01:23,280 --> 00:01:24,640
first with what the gdpr

38
00:01:24,640 --> 00:01:27,920
actually is and what it covers so what

39
00:01:27,920 --> 00:01:29,680
does the gdpr cover

40
00:01:29,680 --> 00:01:32,240
in a word nearly anything that relates

41
00:01:32,240 --> 00:01:34,240
to or allows for the identification of a

42
00:01:34,240 --> 00:01:36,960
natural person within the eu

43
00:01:36,960 --> 00:01:39,520
that identification can be either direct

44
00:01:39,520 --> 00:01:40,960
or indirect

45
00:01:40,960 --> 00:01:42,720
and is much broader than most data

46
00:01:42,720 --> 00:01:45,040
protection or privacy laws that exist in

47
00:01:45,040 --> 00:01:47,200
countries like the us for instance

48
00:01:47,200 --> 00:01:50,479
it applies to anyone who processes data

49
00:01:50,479 --> 00:01:52,799
about eu data subjects regardless of

50
00:01:52,799 --> 00:01:55,840
where they are

51
00:01:56,159 --> 00:01:58,560
okay great so what the heck is

52
00:01:58,560 --> 00:01:59,680
processing

53
00:01:59,680 --> 00:02:01,680
well the gdpr talks about processing

54
00:02:01,680 --> 00:02:03,840
very broadly in that it covers a wide

55
00:02:03,840 --> 00:02:05,439
range of operations

56
00:02:05,439 --> 00:02:07,280
manual or automatic it includes things

57
00:02:07,280 --> 00:02:08,800
like sharing a photo of a person on a

58
00:02:08,800 --> 00:02:10,080
website

59
00:02:10,080 --> 00:02:12,800
or sending a promotional email storing

60
00:02:12,800 --> 00:02:14,239
ip addresses

61
00:02:14,239 --> 00:02:17,280
storing mac addresses cctv recording

62
00:02:17,280 --> 00:02:19,360
creating a contacts database with names

63
00:02:19,360 --> 00:02:21,360
and phone numbers in it for instance

64
00:02:21,360 --> 00:02:24,000
or even log data that might collect

65
00:02:24,000 --> 00:02:24,800
personal

66
00:02:24,800 --> 00:02:28,239
metadata of users it also involves you

67
00:02:28,239 --> 00:02:30,000
know the actual processing of that data

68
00:02:30,000 --> 00:02:30,800
in terms of

69
00:02:30,800 --> 00:02:32,640
you know drawing insights and

70
00:02:32,640 --> 00:02:34,160
conclusions

71
00:02:34,160 --> 00:02:37,680
from what's already being collected

72
00:02:37,680 --> 00:02:39,519
there are also seven basic principles

73
00:02:39,519 --> 00:02:41,680
underlying the gdpr

74
00:02:41,680 --> 00:02:43,920
information must be processed lawfully

75
00:02:43,920 --> 00:02:46,319
fairly and in a transparent manner

76
00:02:46,319 --> 00:02:47,840
which means it needs to be for a

77
00:02:47,840 --> 00:02:49,440
specific purpose that's clearly

78
00:02:49,440 --> 00:02:51,120
communicated to the data subject the

79
00:02:51,120 --> 00:02:51,840
purpose

80
00:02:51,840 --> 00:02:53,680
also needs to be limited in scope it

81
00:02:53,680 --> 00:02:55,680
can't just be a broad brush for every

82
00:02:55,680 --> 00:02:57,040
bit of data out there

83
00:02:57,040 --> 00:02:58,800
controllers need to take steps to

84
00:02:58,800 --> 00:03:00,800
minimize the data collected and to store

85
00:03:00,800 --> 00:03:03,120
it for only as long as it's necessary to

86
00:03:03,120 --> 00:03:05,040
fulfill their purposes

87
00:03:05,040 --> 00:03:07,360
the data must be accurately maintained

88
00:03:07,360 --> 00:03:09,440
and it must be kept securely

89
00:03:09,440 --> 00:03:10,959
finally there's a principle of

90
00:03:10,959 --> 00:03:12,560
accountability that's in place

91
00:03:12,560 --> 00:03:14,480
which means that you as a controller or

92
00:03:14,480 --> 00:03:16,000
processor of data

93
00:03:16,000 --> 00:03:17,519
need to demonstrate that you can comply

94
00:03:17,519 --> 00:03:19,120
with the gdpr's principles and

95
00:03:19,120 --> 00:03:21,280
obligations

96
00:03:21,280 --> 00:03:24,560
so many clients ask what is this

97
00:03:24,560 --> 00:03:27,680
lawful basis business all about well the

98
00:03:27,680 --> 00:03:30,319
gdpr lists a number of potential options

99
00:03:30,319 --> 00:03:32,640
only a few are really applicable to

100
00:03:32,640 --> 00:03:33,519
non-public

101
00:03:33,519 --> 00:03:35,599
entities the primary examples here

102
00:03:35,599 --> 00:03:37,360
include things like consent

103
00:03:37,360 --> 00:03:40,000
or performance of a contract legal

104
00:03:40,000 --> 00:03:41,440
obligation on the part of the data

105
00:03:41,440 --> 00:03:43,440
controller or legitimate interests

106
00:03:43,440 --> 00:03:44,720
now while this isn't going to be a big

107
00:03:44,720 --> 00:03:46,959
discussion about what lawful basis is or

108
00:03:46,959 --> 00:03:48,720
anything else i did want to at least lay

109
00:03:48,720 --> 00:03:50,000
those out because they'll come up

110
00:03:50,000 --> 00:03:53,280
later finally if you screw up as a

111
00:03:53,280 --> 00:03:54,159
controller

112
00:03:54,159 --> 00:03:55,760
or a processor by not meeting these

113
00:03:55,760 --> 00:03:58,239
principles you can face fines

114
00:03:58,239 --> 00:03:59,840
your processing activities can be

115
00:03:59,840 --> 00:04:01,920
curtailed or you'll you'll have to deal

116
00:04:01,920 --> 00:04:03,760
with a data protection authority which

117
00:04:03,760 --> 00:04:04,080
is

118
00:04:04,080 --> 00:04:06,879
never fun all right now that we've

119
00:04:06,879 --> 00:04:09,200
knocked out the gdpr and you're all gdpr

120
00:04:09,200 --> 00:04:09,840
experts

121
00:04:09,840 --> 00:04:11,280
let's talk a little bit about the meat

122
00:04:11,280 --> 00:04:13,280
and potatoes of this discussion

123
00:04:13,280 --> 00:04:17,199
which is about the shrems 2ks so in july

124
00:04:17,199 --> 00:04:19,519
the european court of justice ruled in

125
00:04:19,519 --> 00:04:20,959
data protection commissioner v

126
00:04:20,959 --> 00:04:23,600
facebook ireland limited maximilian

127
00:04:23,600 --> 00:04:24,479
shrems

128
00:04:24,479 --> 00:04:27,280
aka shrums ii that the u.s privacy

129
00:04:27,280 --> 00:04:29,040
shield framework was inadequate under

130
00:04:29,040 --> 00:04:30,080
the gdpr

131
00:04:30,080 --> 00:04:32,720
the court of justice found that the us's

132
00:04:32,720 --> 00:04:34,720
broad surveillance programs and limited

133
00:04:34,720 --> 00:04:37,199
right of redress for eu data subjects

134
00:04:37,199 --> 00:04:39,280
meant that the u.s did not maintain

135
00:04:39,280 --> 00:04:41,840
adequate data protection safeguards

136
00:04:41,840 --> 00:04:44,000
they also put into doubt whether other

137
00:04:44,000 --> 00:04:46,960
mechanisms for transferring eu data

138
00:04:46,960 --> 00:04:49,120
things like standard contractual clauses

139
00:04:49,120 --> 00:04:50,720
and binding corporate rules

140
00:04:50,720 --> 00:04:52,320
would also be adequate when it comes to

141
00:04:52,320 --> 00:04:54,800
transferring to the united states

142
00:04:54,800 --> 00:04:57,840
so just a quick note on adequacy

143
00:04:57,840 --> 00:05:00,720
just like i mentioned the fairness rules

144
00:05:00,720 --> 00:05:01,440
earlier

145
00:05:01,440 --> 00:05:03,280
and also the lawfulness of processing

146
00:05:03,280 --> 00:05:05,520
there are also rules governing how data

147
00:05:05,520 --> 00:05:07,360
can be transferred between eu

148
00:05:07,360 --> 00:05:09,600
and non-eu data controllers and

149
00:05:09,600 --> 00:05:10,560
processors

150
00:05:10,560 --> 00:05:12,639
one key concept here is that countries

151
00:05:12,639 --> 00:05:14,080
that receive the data

152
00:05:14,080 --> 00:05:17,120
from the eu have adequate legal

153
00:05:17,120 --> 00:05:19,759
safeguards that are in place to protect

154
00:05:19,759 --> 00:05:20,960
data subject rights

155
00:05:20,960 --> 00:05:22,720
and enforce obligations on the

156
00:05:22,720 --> 00:05:24,240
controllers and processors

157
00:05:24,240 --> 00:05:26,800
of the data and roughly that means they

158
00:05:26,800 --> 00:05:28,960
need to be equivalent to the gdpr

159
00:05:28,960 --> 00:05:30,080
so while there's some countries out

160
00:05:30,080 --> 00:05:31,919
there like switzerland and new zealand

161
00:05:31,919 --> 00:05:33,039
and japan

162
00:05:33,039 --> 00:05:36,000
that already have formal adequacy

163
00:05:36,000 --> 00:05:37,600
decisions that have been

164
00:05:37,600 --> 00:05:40,320
in place a lot of countries don't in

165
00:05:40,320 --> 00:05:40,720
fact

166
00:05:40,720 --> 00:05:43,680
the vast majority don't and so to get

167
00:05:43,680 --> 00:05:44,960
around that

168
00:05:44,960 --> 00:05:47,520
the drafters of the gdpr carved out a

169
00:05:47,520 --> 00:05:49,199
number of other mechanisms that

170
00:05:49,199 --> 00:05:50,720
organizations can use

171
00:05:50,720 --> 00:05:53,199
to meet or demonstrate adequacy for

172
00:05:53,199 --> 00:05:53,919
example

173
00:05:53,919 --> 00:05:56,720
standard contractual clauses can be used

174
00:05:56,720 --> 00:05:58,400
binding corporate rules between

175
00:05:58,400 --> 00:05:59,919
organizations

176
00:05:59,919 --> 00:06:01,280
and then there's this concept of

177
00:06:01,280 --> 00:06:03,199
derogations and

178
00:06:03,199 --> 00:06:05,120
up until shrimp's two there was also the

179
00:06:05,120 --> 00:06:06,400
privacy shield

180
00:06:06,400 --> 00:06:10,319
so that one's gone all right so

181
00:06:10,319 --> 00:06:13,840
beyond invalidating privacy shield what

182
00:06:13,840 --> 00:06:16,720
actually happened well the court also

183
00:06:16,720 --> 00:06:17,280
laid out

184
00:06:17,280 --> 00:06:19,440
a number of additional hurdles that data

185
00:06:19,440 --> 00:06:20,720
exporters need to

186
00:06:20,720 --> 00:06:23,680
jump prior to transferring data outside

187
00:06:23,680 --> 00:06:24,560
of the eu

188
00:06:24,560 --> 00:06:27,680
to quote inadequate regimes or locales

189
00:06:27,680 --> 00:06:28,960
specifically the court said that

190
00:06:28,960 --> 00:06:31,280
exporters now are required to quote

191
00:06:31,280 --> 00:06:34,000
verify prior to any transfer whether the

192
00:06:34,000 --> 00:06:35,680
level of protection in place

193
00:06:35,680 --> 00:06:38,400
required by eu law is respected in the

194
00:06:38,400 --> 00:06:40,080
third country concerned

195
00:06:40,080 --> 00:06:42,560
and also whether the data importer can

196
00:06:42,560 --> 00:06:43,360
itself

197
00:06:43,360 --> 00:06:45,840
meet the obligations to protect personal

198
00:06:45,840 --> 00:06:46,800
data

199
00:06:46,800 --> 00:06:49,039
if an importer can't legally comply with

200
00:06:49,039 --> 00:06:51,199
the contractual clauses

201
00:06:51,199 --> 00:06:53,919
that parties need to potentially apply

202
00:06:53,919 --> 00:06:54,400
other

203
00:06:54,400 --> 00:06:57,599
safeguards and supplementary measures or

204
00:06:57,599 --> 00:06:59,280
they may be able to seek

205
00:06:59,280 --> 00:07:02,319
derogation under article 49

206
00:07:02,319 --> 00:07:04,639
otherwise the transfers need to cease

207
00:07:04,639 --> 00:07:06,240
they just need to stop altogether

208
00:07:06,240 --> 00:07:08,160
and any data that's already been

209
00:07:08,160 --> 00:07:09,680
transferred must be returned

210
00:07:09,680 --> 00:07:12,160
or destroyed if a data exporter

211
00:07:12,160 --> 00:07:14,240
continues transferring data it needs to

212
00:07:14,240 --> 00:07:16,080
notify their data protection

213
00:07:16,080 --> 00:07:18,240
authority immediately and then the data

214
00:07:18,240 --> 00:07:20,080
protection authority has the fun task of

215
00:07:20,080 --> 00:07:21,520
actually telling them they need to stop

216
00:07:21,520 --> 00:07:23,280
their transfer basically

217
00:07:23,280 --> 00:07:26,000
now some data importers particularly

218
00:07:26,000 --> 00:07:27,759
those located in the us

219
00:07:27,759 --> 00:07:30,800
china and russia and other heavy

220
00:07:30,800 --> 00:07:32,080
surveillance regimes

221
00:07:32,080 --> 00:07:33,840
will be hard pressed to demonstrate that

222
00:07:33,840 --> 00:07:36,960
they can abide by the contractual clause

223
00:07:36,960 --> 00:07:38,639
obligations in practice

224
00:07:38,639 --> 00:07:41,039
especially if they're already targets of

225
00:07:41,039 --> 00:07:42,400
government surveillance

226
00:07:42,400 --> 00:07:44,960
but it's important to understand that

227
00:07:44,960 --> 00:07:46,720
this new standard applies to

228
00:07:46,720 --> 00:07:49,919
any transfer of personal data

229
00:07:49,919 --> 00:07:51,520
regardless of whatever country you're

230
00:07:51,520 --> 00:07:54,160
going to which in practice means that it

231
00:07:54,160 --> 00:07:55,599
probably applies to

232
00:07:55,599 --> 00:07:57,599
any transfer what's really interesting

233
00:07:57,599 --> 00:07:59,039
here is that

234
00:07:59,039 --> 00:08:01,360
under the existing standard contractual

235
00:08:01,360 --> 00:08:02,720
clause framework

236
00:08:02,720 --> 00:08:05,120
and even the new revisions to standard

237
00:08:05,120 --> 00:08:06,479
contractual clauses

238
00:08:06,479 --> 00:08:08,879
data exporters already have an

239
00:08:08,879 --> 00:08:10,879
affirmative duty to actually read the

240
00:08:10,879 --> 00:08:12,000
damn contract

241
00:08:12,000 --> 00:08:14,240
they just haven't been so really the

242
00:08:14,240 --> 00:08:16,000
byproduct of shrems is then

243
00:08:16,000 --> 00:08:17,840
is that everybody needs to read the

244
00:08:17,840 --> 00:08:19,440
contract for a change and they can't

245
00:08:19,440 --> 00:08:21,840
just ignore it like they usually do

246
00:08:21,840 --> 00:08:25,599
okay now what can we still do data

247
00:08:25,599 --> 00:08:26,479
transfers

248
00:08:26,479 --> 00:08:29,599
the answer is kinda so

249
00:08:29,599 --> 00:08:31,199
i have a few questions and some hopeful

250
00:08:31,199 --> 00:08:33,360
answers here based on some of the

251
00:08:33,360 --> 00:08:35,440
details that have emerged both from the

252
00:08:35,440 --> 00:08:36,640
court's ruling

253
00:08:36,640 --> 00:08:38,839
and subsequent decisions by various

254
00:08:38,839 --> 00:08:40,799
regulators and the european data

255
00:08:40,799 --> 00:08:42,000
protection board

256
00:08:42,000 --> 00:08:43,599
the first question of course is can we

257
00:08:43,599 --> 00:08:46,480
still transfer data to the us

258
00:08:46,480 --> 00:08:49,040
so as the court stated if the law in the

259
00:08:49,040 --> 00:08:50,959
importing country is inadequate

260
00:08:50,959 --> 00:08:52,880
that means no data transfer for you

261
00:08:52,880 --> 00:08:55,440
basically unless

262
00:08:55,440 --> 00:08:57,360
you can find another way to ensure that

263
00:08:57,360 --> 00:09:00,320
adequacy can be maintained

264
00:09:00,320 --> 00:09:03,279
similarly if you're a data importer and

265
00:09:03,279 --> 00:09:05,440
it becomes infeasible for you to avoid

266
00:09:05,440 --> 00:09:07,360
the long arm of the u.s surveillance

267
00:09:07,360 --> 00:09:10,240
state which is pretty common actually

268
00:09:10,240 --> 00:09:12,160
you'll need to notify the data exporter

269
00:09:12,160 --> 00:09:14,080
and legally seize transfers

270
00:09:14,080 --> 00:09:17,360
or find another way

271
00:09:17,360 --> 00:09:18,480
now there's been some positive

272
00:09:18,480 --> 00:09:20,959
developments in this area notably

273
00:09:20,959 --> 00:09:22,640
the draft updates to standard

274
00:09:22,640 --> 00:09:24,080
contractual clauses

275
00:09:24,080 --> 00:09:26,560
that i mentioned earlier as well as some

276
00:09:26,560 --> 00:09:28,080
guidance from the european data

277
00:09:28,080 --> 00:09:29,440
protection board

278
00:09:29,440 --> 00:09:32,080
on both the contractual and technical

279
00:09:32,080 --> 00:09:32,880
controls

280
00:09:32,880 --> 00:09:34,480
and supplementary measures that can be

281
00:09:34,480 --> 00:09:36,560
adopted we'll talk a little bit about

282
00:09:36,560 --> 00:09:37,680
those contractual

283
00:09:37,680 --> 00:09:40,240
and technical controls in a minute

284
00:09:40,240 --> 00:09:41,040
beyond those

285
00:09:41,040 --> 00:09:42,959
what there are a few other mechanisms

286
00:09:42,959 --> 00:09:44,640
that might exist

287
00:09:44,640 --> 00:09:46,959
and these are known as derogations under

288
00:09:46,959 --> 00:09:48,560
article 49.

289
00:09:48,560 --> 00:09:51,519
they're designed to be ad hoc and really

290
00:09:51,519 --> 00:09:53,760
really meant to apply to a limited

291
00:09:53,760 --> 00:09:56,240
set of data transfers not really large

292
00:09:56,240 --> 00:09:57,920
scale activities

293
00:09:57,920 --> 00:10:00,320
so your facebooks and your googles

294
00:10:00,320 --> 00:10:01,360
they're going to have a time and

295
00:10:01,360 --> 00:10:03,040
probably not be able to use most of

296
00:10:03,040 --> 00:10:04,480
these derivations

297
00:10:04,480 --> 00:10:06,959
obtaining a data subject's specific

298
00:10:06,959 --> 00:10:07,839
informed

299
00:10:07,839 --> 00:10:10,720
and unambiguous consent is probably the

300
00:10:10,720 --> 00:10:11,200
most

301
00:10:11,200 --> 00:10:14,000
popular and common one but companies can

302
00:10:14,000 --> 00:10:16,959
also transfer data to inadequate locales

303
00:10:16,959 --> 00:10:18,880
in certain situations if they're trying

304
00:10:18,880 --> 00:10:20,880
to perform say a contract

305
00:10:20,880 --> 00:10:24,079
between themselves and the data subject

306
00:10:24,079 --> 00:10:25,680
if they're trying to protect the vital

307
00:10:25,680 --> 00:10:27,519
interests of the data subject

308
00:10:27,519 --> 00:10:29,760
like if they're health care provider if

309
00:10:29,760 --> 00:10:31,440
they're trying to defend against

310
00:10:31,440 --> 00:10:34,640
eu legal claims or

311
00:10:34,640 --> 00:10:37,600
for other compelling legitimate

312
00:10:37,600 --> 00:10:39,279
interests

313
00:10:39,279 --> 00:10:40,800
now these legitimate interest

314
00:10:40,800 --> 00:10:43,440
derogations are really really narrow in

315
00:10:43,440 --> 00:10:44,399
scope and

316
00:10:44,399 --> 00:10:45,839
most of the time companies have been

317
00:10:45,839 --> 00:10:47,519
playing very fast and loose when they're

318
00:10:47,519 --> 00:10:49,120
using this derogation

319
00:10:49,120 --> 00:10:50,800
so i suspect that there's going to be a

320
00:10:50,800 --> 00:10:52,959
lot of interesting litigation coming up

321
00:10:52,959 --> 00:10:53,279
on

322
00:10:53,279 --> 00:10:56,480
those interpretations they also

323
00:10:56,480 --> 00:10:58,240
need to be thoroughly assessed for

324
00:10:58,240 --> 00:11:00,000
applicability which means

325
00:11:00,000 --> 00:11:02,079
that you'll probably be spending a

326
00:11:02,079 --> 00:11:03,440
decent amount of time if you're a

327
00:11:03,440 --> 00:11:04,000
company

328
00:11:04,000 --> 00:11:06,880
talking to your data protection officer

329
00:11:06,880 --> 00:11:07,200
or

330
00:11:07,200 --> 00:11:09,360
members of your privacy team other

331
00:11:09,360 --> 00:11:10,720
options here including

332
00:11:10,720 --> 00:11:12,720
limiting or reducing the quantity and

333
00:11:12,720 --> 00:11:15,519
duration of data that leaves the eu

334
00:11:15,519 --> 00:11:17,200
especially if it's likely to have a high

335
00:11:17,200 --> 00:11:19,200
impact on data subjects

336
00:11:19,200 --> 00:11:22,640
or considering going to other

337
00:11:22,640 --> 00:11:24,800
more adequate locales when you're

338
00:11:24,800 --> 00:11:26,640
transferring data

339
00:11:26,640 --> 00:11:29,600
so we should also talk a little bit

340
00:11:29,600 --> 00:11:30,720
about

341
00:11:30,720 --> 00:11:33,680
the data exporter guidance on how to

342
00:11:33,680 --> 00:11:35,200
transfer data by applying

343
00:11:35,200 --> 00:11:37,120
additional safeguards and supplementary

344
00:11:37,120 --> 00:11:38,720
measures

345
00:11:38,720 --> 00:11:40,560
now the new standard contractual clause

346
00:11:40,560 --> 00:11:42,640
language that's out there actually do

347
00:11:42,640 --> 00:11:45,760
offer some guidance things around data

348
00:11:45,760 --> 00:11:47,440
importers providing notice

349
00:11:47,440 --> 00:11:49,600
to exporters and where applicable to

350
00:11:49,600 --> 00:11:51,279
affected data subjects

351
00:11:51,279 --> 00:11:53,200
requirements around aggregate

352
00:11:53,200 --> 00:11:54,399
information sharing

353
00:11:54,399 --> 00:11:56,320
so these are the transparency reports

354
00:11:56,320 --> 00:11:58,160
that google and facebook and the like

355
00:11:58,160 --> 00:11:59,120
tend to share

356
00:11:59,120 --> 00:12:02,079
or also the idea of using and ensuring

357
00:12:02,079 --> 00:12:02,480
and

358
00:12:02,480 --> 00:12:04,880
that the importer will challenge

359
00:12:04,880 --> 00:12:07,360
governmental requests when they can

360
00:12:07,360 --> 00:12:09,040
one important thing if the eu data

361
00:12:09,040 --> 00:12:11,600
exporters really should consider

362
00:12:11,600 --> 00:12:14,000
is whether the data importers themselves

363
00:12:14,000 --> 00:12:14,720
are actually

364
00:12:14,720 --> 00:12:18,240
able to or likely to comply with

365
00:12:18,240 --> 00:12:21,760
standard contractual clause terms at all

366
00:12:21,760 --> 00:12:23,760
this means reassessing and actually

367
00:12:23,760 --> 00:12:25,519
reading all those data protection

368
00:12:25,519 --> 00:12:27,360
addenda clauses in your contracts as

369
00:12:27,360 --> 00:12:28,399
well as any

370
00:12:28,399 --> 00:12:30,160
standard contractual clauses that might

371
00:12:30,160 --> 00:12:31,600
be in place now

372
00:12:31,600 --> 00:12:33,839
if there's a history of non-compliance

373
00:12:33,839 --> 00:12:35,440
with the data protection principles

374
00:12:35,440 --> 00:12:36,160
generally

375
00:12:36,160 --> 00:12:38,880
or if the data importer is a frequent

376
00:12:38,880 --> 00:12:39,360
target

377
00:12:39,360 --> 00:12:41,839
say of national security letters and

378
00:12:41,839 --> 00:12:43,839
other government data requests

379
00:12:43,839 --> 00:12:45,760
it's worth evaluating whether it makes

380
00:12:45,760 --> 00:12:47,519
any business sense to continue in the

381
00:12:47,519 --> 00:12:48,240
relationship

382
00:12:48,240 --> 00:12:51,760
at all at least if secs are involved

383
00:12:51,760 --> 00:12:54,560
all right this is where the fun is and i

384
00:12:54,560 --> 00:12:57,040
think why most of us are here attending

385
00:12:57,040 --> 00:12:58,720
let's talk about the technical

386
00:12:58,720 --> 00:13:00,480
supplementary measures

387
00:13:00,480 --> 00:13:02,399
on the technical end of things there are

388
00:13:02,399 --> 00:13:04,399
a few exciting developments

389
00:13:04,399 --> 00:13:07,360
out there generally this class of

390
00:13:07,360 --> 00:13:09,680
technology is known as privacy enhancing

391
00:13:09,680 --> 00:13:10,720
technologies

392
00:13:10,720 --> 00:13:13,519
or pets most of these are still in

393
00:13:13,519 --> 00:13:15,040
development they aren't quite

394
00:13:15,040 --> 00:13:17,680
ready for prime time they take a lot of

395
00:13:17,680 --> 00:13:18,639
resources

396
00:13:18,639 --> 00:13:21,839
a lot of time commitments and you know

397
00:13:21,839 --> 00:13:24,000
don't always scale to all practical

398
00:13:24,000 --> 00:13:25,120
business purposes

399
00:13:25,120 --> 00:13:28,480
but what's really exciting about the

400
00:13:28,480 --> 00:13:31,279
follow-on of the shrems decision and our

401
00:13:31,279 --> 00:13:31,839
general

402
00:13:31,839 --> 00:13:34,480
interest in privacy is that these tools

403
00:13:34,480 --> 00:13:35,360
are really in

404
00:13:35,360 --> 00:13:38,240
active development and a lot of really

405
00:13:38,240 --> 00:13:41,120
big heavy hitters are working on them

406
00:13:41,120 --> 00:13:44,240
i think the first most prominent privacy

407
00:13:44,240 --> 00:13:45,839
enhancing technology is one we're all

408
00:13:45,839 --> 00:13:47,199
pretty familiar with and that's

409
00:13:47,199 --> 00:13:48,880
encryption

410
00:13:48,880 --> 00:13:51,839
now encryption obviously isn't new but

411
00:13:51,839 --> 00:13:53,920
what we're really talking about here

412
00:13:53,920 --> 00:13:56,639
is fully end to end encryption over

413
00:13:56,639 --> 00:13:57,199
secure

414
00:13:57,199 --> 00:14:00,240
channels especially when the keys

415
00:14:00,240 --> 00:14:02,160
are not made available to the data

416
00:14:02,160 --> 00:14:04,399
importer

417
00:14:04,399 --> 00:14:07,040
essentially the data importer is blind

418
00:14:07,040 --> 00:14:07,760
to

419
00:14:07,760 --> 00:14:10,880
what the data is and so the data itself

420
00:14:10,880 --> 00:14:13,120
ceases to be personal

421
00:14:13,120 --> 00:14:15,680
the importer also has no way to provide

422
00:14:15,680 --> 00:14:16,160
that

423
00:14:16,160 --> 00:14:19,279
information to say a u.s or other

424
00:14:19,279 --> 00:14:23,040
governmental entity who comes to calling

425
00:14:23,040 --> 00:14:24,720
now the downside here is is pretty

426
00:14:24,720 --> 00:14:26,480
obvious it's that

427
00:14:26,480 --> 00:14:29,360
while it's great if you're just storing

428
00:14:29,360 --> 00:14:31,040
personal data and not doing anything

429
00:14:31,040 --> 00:14:32,639
with it

430
00:14:32,639 --> 00:14:34,959
there's nothing that can really be done

431
00:14:34,959 --> 00:14:36,639
to the data there's no processing that

432
00:14:36,639 --> 00:14:38,720
can really occur

433
00:14:38,720 --> 00:14:42,240
so it's not really a realistic option

434
00:14:42,240 --> 00:14:44,560
for the vast majority of use cases

435
00:14:44,560 --> 00:14:46,880
you know if you're in a big company and

436
00:14:46,880 --> 00:14:49,760
you need to access your hr records

437
00:14:49,760 --> 00:14:51,519
a fully end-to-end solution where you

438
00:14:51,519 --> 00:14:53,760
don't have the decryption key

439
00:14:53,760 --> 00:14:55,680
on the import side means that you can't

440
00:14:55,680 --> 00:14:56,880
do anything with it

441
00:14:56,880 --> 00:14:59,199
so there is an alternative here it's a

442
00:14:59,199 --> 00:15:01,519
far more technically complex approach of

443
00:15:01,519 --> 00:15:02,800
course

444
00:15:02,800 --> 00:15:04,720
but it's something called homomorphic

445
00:15:04,720 --> 00:15:06,079
encryption which allows

446
00:15:06,079 --> 00:15:08,560
arbitrary computation and analysis of

447
00:15:08,560 --> 00:15:09,440
encrypted data

448
00:15:09,440 --> 00:15:12,720
without decrypting it first

449
00:15:12,720 --> 00:15:14,720
all right so how does this work

450
00:15:14,720 --> 00:15:16,639
homomorphic encryption

451
00:15:16,639 --> 00:15:18,320
specifically something called fully

452
00:15:18,320 --> 00:15:19,839
homomorphic encryption

453
00:15:19,839 --> 00:15:22,399
allows for actual processing of data to

454
00:15:22,399 --> 00:15:24,160
occur to the underlying

455
00:15:24,160 --> 00:15:26,079
information contained in the encrypted

456
00:15:26,079 --> 00:15:27,519
form without

457
00:15:27,519 --> 00:15:30,320
exposing all those private bits a really

458
00:15:30,320 --> 00:15:32,079
good example is to use the canonical

459
00:15:32,079 --> 00:15:34,079
case of alice and bob

460
00:15:34,079 --> 00:15:35,920
so let's say alice is very interested in

461
00:15:35,920 --> 00:15:37,279
finding out if she has

462
00:15:37,279 --> 00:15:40,800
any genetic markers for diseases like

463
00:15:40,800 --> 00:15:42,959
alzheimer's or autoimmune disorders

464
00:15:42,959 --> 00:15:45,600
or certain kinds of cancers she doesn't

465
00:15:45,600 --> 00:15:47,440
know how to do it but she knows that bob

466
00:15:47,440 --> 00:15:48,639
and bob's company

467
00:15:48,639 --> 00:15:51,920
can perform certain tests and look for

468
00:15:51,920 --> 00:15:53,360
genetic markers

469
00:15:53,360 --> 00:15:55,040
so she sends an encrypted copy of her

470
00:15:55,040 --> 00:15:56,800
genomic data to bob

471
00:15:56,800 --> 00:15:59,839
using a public private key pair

472
00:15:59,839 --> 00:16:02,160
bob gets the public key of course but

473
00:16:02,160 --> 00:16:04,959
alice keeps the private key to herself

474
00:16:04,959 --> 00:16:07,839
bob then goes ahead and performs

475
00:16:07,839 --> 00:16:10,000
processing on alice's encrypted genomic

476
00:16:10,000 --> 00:16:11,040
data

477
00:16:11,040 --> 00:16:13,199
to look for any sets of markers that she

478
00:16:13,199 --> 00:16:14,480
might be interested in

479
00:16:14,480 --> 00:16:16,959
after he's done bob can then pass the

480
00:16:16,959 --> 00:16:17,839
results

481
00:16:17,839 --> 00:16:20,320
of his findings back to alice fully

482
00:16:20,320 --> 00:16:21,040
encrypted

483
00:16:21,040 --> 00:16:23,519
at no point does bob ever gain any

484
00:16:23,519 --> 00:16:26,399
access or knowledge to alice's genome

485
00:16:26,399 --> 00:16:29,600
or the findings it's all completely

486
00:16:29,600 --> 00:16:30,880
secured and encrypted

487
00:16:30,880 --> 00:16:32,480
alice then uses her private key to

488
00:16:32,480 --> 00:16:34,720
decrypt the data and to obtain

489
00:16:34,720 --> 00:16:37,600
the computed results that bob has found

490
00:16:37,600 --> 00:16:38,800
voila

491
00:16:38,800 --> 00:16:40,320
here you've got processing that can

492
00:16:40,320 --> 00:16:43,199
happen anywhere but alice's private data

493
00:16:43,199 --> 00:16:47,599
never gets exposed to anyone but her

494
00:16:47,759 --> 00:16:49,600
the next technical measure that's in

495
00:16:49,600 --> 00:16:51,279
place is something called

496
00:16:51,279 --> 00:16:54,399
federated learning and of

497
00:16:54,399 --> 00:16:56,000
the of the three that i'm going to talk

498
00:16:56,000 --> 00:16:57,360
about here federated learning is

499
00:16:57,360 --> 00:16:58,639
actually already well

500
00:16:58,639 --> 00:17:01,199
in use in fact if you've got an android

501
00:17:01,199 --> 00:17:02,000
device

502
00:17:02,000 --> 00:17:03,680
you've likely been a party to federated

503
00:17:03,680 --> 00:17:05,839
learning and may not have even known

504
00:17:05,839 --> 00:17:08,079
with federated learning sometimes what's

505
00:17:08,079 --> 00:17:10,559
known as collaborative machine learning

506
00:17:10,559 --> 00:17:12,480
local machines like your phone or your

507
00:17:12,480 --> 00:17:14,240
personal computer train a shared

508
00:17:14,240 --> 00:17:16,400
prediction model with their local data

509
00:17:16,400 --> 00:17:19,439
on the device the data itself is never

510
00:17:19,439 --> 00:17:20,640
shared

511
00:17:20,640 --> 00:17:22,720
but instead a summary of the learnings

512
00:17:22,720 --> 00:17:23,679
are

513
00:17:23,679 --> 00:17:25,359
these results are averaged with other

514
00:17:25,359 --> 00:17:26,959
user updates to provide

515
00:17:26,959 --> 00:17:30,640
improved models and then those models

516
00:17:30,640 --> 00:17:32,320
are re-shared with the local machines

517
00:17:32,320 --> 00:17:34,960
for future training

518
00:17:34,960 --> 00:17:37,360
now i'll profess this came from the

519
00:17:37,360 --> 00:17:38,880
wikipedia page but

520
00:17:38,880 --> 00:17:40,799
i was going to slap on cat pictures but

521
00:17:40,799 --> 00:17:42,799
my photoshop skills kind of suck

522
00:17:42,799 --> 00:17:44,960
also i kind of procrastinated on this a

523
00:17:44,960 --> 00:17:46,720
little bit here's how it actually works

524
00:17:46,720 --> 00:17:47,440
so

525
00:17:47,440 --> 00:17:49,039
step one is where a central server

526
00:17:49,039 --> 00:17:50,559
decides to generate

527
00:17:50,559 --> 00:17:52,799
an initial model let's say of what a cat

528
00:17:52,799 --> 00:17:54,160
looks like in step two

529
00:17:54,160 --> 00:17:56,160
that central server transmits the

530
00:17:56,160 --> 00:17:57,760
initial model

531
00:17:57,760 --> 00:17:59,840
to various local machines like your cell

532
00:17:59,840 --> 00:18:01,360
phones step three

533
00:18:01,360 --> 00:18:03,520
the the local machines are training

534
00:18:03,520 --> 00:18:05,440
their models by using different cats or

535
00:18:05,440 --> 00:18:06,880
pictures of cats

536
00:18:06,880 --> 00:18:10,640
kittens big floofs hairless cats

537
00:18:10,640 --> 00:18:12,960
adorable cats whatever a few local

538
00:18:12,960 --> 00:18:14,400
machines even screw up

539
00:18:14,400 --> 00:18:16,000
and they pick a few dogs right because

540
00:18:16,000 --> 00:18:18,160
that happened fortunately that's okay

541
00:18:18,160 --> 00:18:20,880
because the central server receives the

542
00:18:20,880 --> 00:18:22,720
resulting training model data

543
00:18:22,720 --> 00:18:25,679
back from those local machines does some

544
00:18:25,679 --> 00:18:26,480
averaging

545
00:18:26,480 --> 00:18:29,039
and some other math and generates a new

546
00:18:29,039 --> 00:18:30,320
and even better model

547
00:18:30,320 --> 00:18:32,559
of what constitutes a cat and

548
00:18:32,559 --> 00:18:34,400
specifically what constitutes the best

549
00:18:34,400 --> 00:18:35,919
cat

550
00:18:35,919 --> 00:18:40,480
which are these cats because they're my

551
00:18:40,840 --> 00:18:42,160
cats

552
00:18:42,160 --> 00:18:45,280
awesome so moving on uh finally there's

553
00:18:45,280 --> 00:18:47,440
zero knowledge proofs

554
00:18:47,440 --> 00:18:49,600
essentially zero knowledge proofs allow

555
00:18:49,600 --> 00:18:51,520
for the evaluation of a truth of a

556
00:18:51,520 --> 00:18:52,799
statement

557
00:18:52,799 --> 00:18:55,440
done probabilistically without revealing

558
00:18:55,440 --> 00:18:56,160
the underlying

559
00:18:56,160 --> 00:18:58,720
data the best explanation of the xero

560
00:18:58,720 --> 00:19:00,080
knowledge proof model

561
00:19:00,080 --> 00:19:02,240
actually is the cave of alibaba first

562
00:19:02,240 --> 00:19:04,080
published by jean jacques

563
00:19:04,080 --> 00:19:07,440
quisquater essentially and explain it

564
00:19:07,440 --> 00:19:10,080
like i'm five in book form

565
00:19:10,080 --> 00:19:12,400
so in the cave of alibaba example we

566
00:19:12,400 --> 00:19:13,679
have two participants

567
00:19:13,679 --> 00:19:17,200
peggy known as the prover and victor

568
00:19:17,200 --> 00:19:19,600
or the verifier now peggy has uncovered

569
00:19:19,600 --> 00:19:20,880
a secret code

570
00:19:20,880 --> 00:19:23,280
and that opens a magic door within the

571
00:19:23,280 --> 00:19:24,240
cave

572
00:19:24,240 --> 00:19:26,799
the door blocks access presumably to

573
00:19:26,799 --> 00:19:27,600
treasure

574
00:19:27,600 --> 00:19:30,000
but more importantly for this thought

575
00:19:30,000 --> 00:19:30,799
experiment

576
00:19:30,799 --> 00:19:32,559
accessed through to the other side of

577
00:19:32,559 --> 00:19:34,559
the cave victor's interested in buying

578
00:19:34,559 --> 00:19:35,440
the code

579
00:19:35,440 --> 00:19:37,919
but peggy doesn't really just want to

580
00:19:37,919 --> 00:19:38,960
tell him

581
00:19:38,960 --> 00:19:41,280
so instead she proposes taking a path

582
00:19:41,280 --> 00:19:42,960
either a or b

583
00:19:42,960 --> 00:19:45,679
and coming out the other side peggy

584
00:19:45,679 --> 00:19:46,720
chooses her path

585
00:19:46,720 --> 00:19:49,280
while victor stands outside victor then

586
00:19:49,280 --> 00:19:50,640
enters the cave

587
00:19:50,640 --> 00:19:53,120
shouts to her the path that he wants her

588
00:19:53,120 --> 00:19:54,720
to return on

589
00:19:54,720 --> 00:19:56,880
if she knows the code great there's no

590
00:19:56,880 --> 00:19:58,160
problem she just goes

591
00:19:58,160 --> 00:20:00,720
right through the door now since zero

592
00:20:00,720 --> 00:20:01,600
knowledge proofs

593
00:20:01,600 --> 00:20:05,120
are probabilistic it's possible

594
00:20:05,120 --> 00:20:06,960
that peggy could simply have guessed

595
00:20:06,960 --> 00:20:09,760
correctly at least a few times

596
00:20:09,760 --> 00:20:11,360
and she could always come back along

597
00:20:11,360 --> 00:20:13,440
victor's desired route

598
00:20:13,440 --> 00:20:15,360
by just picking the route correctly the

599
00:20:15,360 --> 00:20:16,640
first time but

600
00:20:16,640 --> 00:20:18,880
if you do this many times in a row her

601
00:20:18,880 --> 00:20:20,720
chance of successfully anticipating

602
00:20:20,720 --> 00:20:21,760
victor's requests

603
00:20:21,760 --> 00:20:25,360
become vanishingly small

604
00:20:25,919 --> 00:20:30,000
cool so obviously there's a lot here

605
00:20:30,000 --> 00:20:32,840
and there's a lot of really exciting

606
00:20:32,840 --> 00:20:34,320
opportunities but

607
00:20:34,320 --> 00:20:36,320
i would be kind of a jerk if i didn't

608
00:20:36,320 --> 00:20:38,240
discuss a little bit about

609
00:20:38,240 --> 00:20:40,159
why they're still in the developing

610
00:20:40,159 --> 00:20:42,720
stages and why everybody isn't using

611
00:20:42,720 --> 00:20:45,360
them all at once

612
00:20:45,360 --> 00:20:47,280
generally speaking i think the biggest

613
00:20:47,280 --> 00:20:48,400
limiting factor

614
00:20:48,400 --> 00:20:51,840
to all three is that there are various

615
00:20:51,840 --> 00:20:53,600
limits in the number of insights that

616
00:20:53,600 --> 00:20:54,559
can be gained

617
00:20:54,559 --> 00:20:58,159
from the data and it involves a

618
00:20:58,159 --> 00:21:00,640
tremendous kind of reshift in how we're

619
00:21:00,640 --> 00:21:02,799
processing data today

620
00:21:02,799 --> 00:21:04,720
but there are also some specific

621
00:21:04,720 --> 00:21:06,720
challenges for each one of these models

622
00:21:06,720 --> 00:21:09,360
that i wanted to discuss

623
00:21:09,360 --> 00:21:11,200
now fully homomorphic encryption's

624
00:21:11,200 --> 00:21:13,200
biggest weakness is that it's

625
00:21:13,200 --> 00:21:16,320
incredibly slow and and very much not

626
00:21:16,320 --> 00:21:20,159
performant it's also computationally

627
00:21:20,159 --> 00:21:22,480
and economically costly and it usually

628
00:21:22,480 --> 00:21:23,919
requires either

629
00:21:23,919 --> 00:21:26,880
application modifications or dedicated

630
00:21:26,880 --> 00:21:28,640
and specialized client server

631
00:21:28,640 --> 00:21:29,440
applications

632
00:21:29,440 --> 00:21:31,679
in order to do its thing properly and of

633
00:21:31,679 --> 00:21:32,480
course

634
00:21:32,480 --> 00:21:35,520
there are also attackers that are very

635
00:21:35,520 --> 00:21:38,159
eager to find new ways to exploit fully

636
00:21:38,159 --> 00:21:40,080
homomorphic encryption systems

637
00:21:40,080 --> 00:21:41,360
and there's a lot of literature out

638
00:21:41,360 --> 00:21:43,200
there people doing just that

639
00:21:43,200 --> 00:21:45,360
with federated learning the challenges

640
00:21:45,360 --> 00:21:47,679
mostly come from expense

641
00:21:47,679 --> 00:21:50,799
both in money and computation time

642
00:21:50,799 --> 00:21:53,360
to get the same level of learning as say

643
00:21:53,360 --> 00:21:54,799
a typical

644
00:21:54,799 --> 00:21:56,960
machine learning data center model

645
00:21:56,960 --> 00:21:58,880
federated networks need to connect to

646
00:21:58,880 --> 00:22:01,360
massive numbers of devices

647
00:22:01,360 --> 00:22:03,919
similarly federated learning suffers

648
00:22:03,919 --> 00:22:05,760
from the pitfalls of having

649
00:22:05,760 --> 00:22:08,000
different storage computational and

650
00:22:08,000 --> 00:22:10,559
communications capabilities

651
00:22:10,559 --> 00:22:12,080
as well as differences in the rate of

652
00:22:12,080 --> 00:22:14,559
return for each device

653
00:22:14,559 --> 00:22:16,960
finally there are cases where federated

654
00:22:16,960 --> 00:22:17,679
learning can

655
00:22:17,679 --> 00:22:20,400
leak data private data back to the

656
00:22:20,400 --> 00:22:22,080
central servers

657
00:22:22,080 --> 00:22:24,080
though there's a lot of efforts being

658
00:22:24,080 --> 00:22:27,600
made to limit that

659
00:22:27,600 --> 00:22:29,919
possibility through the use of secure

660
00:22:29,919 --> 00:22:31,919
multi-party computation

661
00:22:31,919 --> 00:22:35,360
or differential privacy finally for zero

662
00:22:35,360 --> 00:22:36,400
knowledge proofs

663
00:22:36,400 --> 00:22:38,640
the limits largely come in the fact that

664
00:22:38,640 --> 00:22:40,799
processing of data in this case

665
00:22:40,799 --> 00:22:43,679
is inherently probabilistic so you need

666
00:22:43,679 --> 00:22:45,120
to have many potential

667
00:22:45,120 --> 00:22:47,360
challenge response tests to get an

668
00:22:47,360 --> 00:22:49,440
accurate result

669
00:22:49,440 --> 00:22:51,679
a second factor for this is for each

670
00:22:51,679 --> 00:22:52,720
test there's

671
00:22:52,720 --> 00:22:55,919
some amount of personal information lost

672
00:22:55,919 --> 00:22:58,880
for example if you're using zkp to

673
00:22:58,880 --> 00:23:01,120
identify a person's age it's possible

674
00:23:01,120 --> 00:23:03,520
with enough data points um to be able to

675
00:23:03,520 --> 00:23:04,240
derive

676
00:23:04,240 --> 00:23:07,200
use cases or use information about the

677
00:23:07,200 --> 00:23:08,480
individual

678
00:23:08,480 --> 00:23:10,559
finally there's currently a limited

679
00:23:10,559 --> 00:23:11,919
number of

680
00:23:11,919 --> 00:23:13,919
examples outside of some thought

681
00:23:13,919 --> 00:23:15,360
experiments

682
00:23:15,360 --> 00:23:18,960
and some numbers based situations where

683
00:23:18,960 --> 00:23:21,360
zero knowledge proofs can be employed

684
00:23:21,360 --> 00:23:23,039
practically

685
00:23:23,039 --> 00:23:27,039
so i don't want to leave everyone

686
00:23:27,039 --> 00:23:30,400
on a downer note i thought i'd briefly

687
00:23:30,400 --> 00:23:30,799
touch

688
00:23:30,799 --> 00:23:33,200
on some of the exciting advances that

689
00:23:33,200 --> 00:23:34,559
companies are making

690
00:23:34,559 --> 00:23:36,480
with all these various technology

691
00:23:36,480 --> 00:23:37,679
solutions

692
00:23:37,679 --> 00:23:40,799
so first up is fhe or fully homomorphic

693
00:23:40,799 --> 00:23:42,159
encryption

694
00:23:42,159 --> 00:23:44,159
so has a lot of promise in the open

695
00:23:44,159 --> 00:23:45,919
source sector where there's a lot of big

696
00:23:45,919 --> 00:23:47,919
hitters like microsoft and google

697
00:23:47,919 --> 00:23:50,320
and ibm and they're releasing lots of

698
00:23:50,320 --> 00:23:51,279
libraries

699
00:23:51,279 --> 00:23:54,159
out there to play with but in terms of

700
00:23:54,159 --> 00:23:55,760
practical uses

701
00:23:55,760 --> 00:23:58,720
perhaps the most promising are efforts

702
00:23:58,720 --> 00:23:59,840
from the security firm

703
00:23:59,840 --> 00:24:02,799
in vail who are using fhe for searches

704
00:24:02,799 --> 00:24:04,000
for analytics and

705
00:24:04,000 --> 00:24:06,640
even machine learning they're a bit

706
00:24:06,640 --> 00:24:07,679
cryptic

707
00:24:07,679 --> 00:24:09,520
almost a little bit like palantir like

708
00:24:09,520 --> 00:24:10,720
my old employer

709
00:24:10,720 --> 00:24:13,919
with regard to the important stuff like

710
00:24:13,919 --> 00:24:15,840
how they're able to do it but they do

711
00:24:15,840 --> 00:24:17,360
have contracts with some government and

712
00:24:17,360 --> 00:24:19,360
private sector organizations

713
00:24:19,360 --> 00:24:22,159
to aid in the supply chain and handle

714
00:24:22,159 --> 00:24:24,880
compliance and risk

715
00:24:24,880 --> 00:24:26,720
as i mentioned earlier federated

716
00:24:26,720 --> 00:24:28,320
learning is by far

717
00:24:28,320 --> 00:24:30,240
the most developed of the bunch when it

718
00:24:30,240 --> 00:24:32,799
comes to practical application

719
00:24:32,799 --> 00:24:35,520
and companies like google have been

720
00:24:35,520 --> 00:24:38,000
using it for a few years now

721
00:24:38,000 --> 00:24:40,480
one clear case beyond the cat image

722
00:24:40,480 --> 00:24:42,720
analysis example that i used

723
00:24:42,720 --> 00:24:45,279
is the use of fl for mobile keyboard

724
00:24:45,279 --> 00:24:46,640
protection

725
00:24:46,640 --> 00:24:49,279
google's gboard keyboard interface

726
00:24:49,279 --> 00:24:51,039
specifically

727
00:24:51,039 --> 00:24:53,520
google uses a distributed on-device

728
00:24:53,520 --> 00:24:54,960
learning framework to train their

729
00:24:54,960 --> 00:24:56,400
keyboard prediction models

730
00:24:56,400 --> 00:24:58,880
including the all-important advanced

731
00:24:58,880 --> 00:25:00,000
emoji prediction

732
00:25:00,000 --> 00:25:03,919
so there you go now you know it's done

733
00:25:03,919 --> 00:25:06,320
in fact a good chunk of google's

734
00:25:06,320 --> 00:25:07,279
tensorflow

735
00:25:07,279 --> 00:25:09,760
framework and their federated core layer

736
00:25:09,760 --> 00:25:11,200
in particular

737
00:25:11,200 --> 00:25:13,039
allows for direct integration with

738
00:25:13,039 --> 00:25:16,400
federated training and evaluation models

739
00:25:16,400 --> 00:25:18,159
anyone really could start working on

740
00:25:18,159 --> 00:25:20,000
federated learning models by leveraging

741
00:25:20,000 --> 00:25:21,440
google's infrastructure and their

742
00:25:21,440 --> 00:25:22,960
library development so it's really

743
00:25:22,960 --> 00:25:25,440
exciting stuff

744
00:25:25,440 --> 00:25:27,840
more recently there's been a partnership

745
00:25:27,840 --> 00:25:28,880
with nvidia

746
00:25:28,880 --> 00:25:31,039
and 20 hospitals around the world

747
00:25:31,039 --> 00:25:32,799
including mass general

748
00:25:32,799 --> 00:25:34,240
where they use federated learning to

749
00:25:34,240 --> 00:25:37,039
predict covid19 patient oxygen needs so

750
00:25:37,039 --> 00:25:39,520
this is directly applicable to the

751
00:25:39,520 --> 00:25:43,039
current pandemic under the initiative

752
00:25:43,039 --> 00:25:44,000
known as exam

753
00:25:44,000 --> 00:25:46,159
participating hospitals used chest

754
00:25:46,159 --> 00:25:47,360
x-rays

755
00:25:47,360 --> 00:25:50,320
patient vitals and lab values to train

756
00:25:50,320 --> 00:25:52,720
local on-demand models

757
00:25:52,720 --> 00:25:54,559
and they sent their updated model data

758
00:25:54,559 --> 00:25:56,320
back to a centralized server where it

759
00:25:56,320 --> 00:25:58,000
was analyzed

760
00:25:58,000 --> 00:26:00,080
it apparently achieved a remarkably high

761
00:26:00,080 --> 00:26:01,919
accuracy model that could be used to

762
00:26:01,919 --> 00:26:03,039
predict

763
00:26:03,039 --> 00:26:06,000
oxygen level intakes for incoming

764
00:26:06,000 --> 00:26:07,520
patients

765
00:26:07,520 --> 00:26:10,559
finally cannot have a tech talk without

766
00:26:10,559 --> 00:26:12,320
at least one mention of digital currency

767
00:26:12,320 --> 00:26:13,840
am i right

768
00:26:13,840 --> 00:26:17,760
in this case zcash actually already has

769
00:26:17,760 --> 00:26:22,720
zero knowledge proofs in the name

770
00:26:22,720 --> 00:26:25,360
it provides users with a private z

771
00:26:25,360 --> 00:26:26,720
address

772
00:26:26,720 --> 00:26:28,640
that shield transaction data on the

773
00:26:28,640 --> 00:26:29,919
blockchain and yet

774
00:26:29,919 --> 00:26:32,880
still allow for complete verification

775
00:26:32,880 --> 00:26:35,360
the zcash model uses something called zk

776
00:26:35,360 --> 00:26:37,840
snark which stands for zero knowledge

777
00:26:37,840 --> 00:26:38,679
succinct

778
00:26:38,679 --> 00:26:43,039
non-interactive argument of knowledge

779
00:26:43,039 --> 00:26:45,600
and they purportedly allow users to

780
00:26:45,600 --> 00:26:46,720
verify

781
00:26:46,720 --> 00:26:49,120
within a few milliseconds using a single

782
00:26:49,120 --> 00:26:50,840
message sent from the pervert to the

783
00:26:50,840 --> 00:26:52,320
verifier

784
00:26:52,320 --> 00:26:54,159
and apparently some pretty interesting

785
00:26:54,159 --> 00:26:56,480
math

786
00:26:57,120 --> 00:27:00,400
all right so that's it and this is me

787
00:27:00,400 --> 00:27:02,960
uh you can reach me at my website and

788
00:27:02,960 --> 00:27:03,520
email

789
00:27:03,520 --> 00:27:07,279
i'm at knowledge dot info i'm on twitter

790
00:27:07,279 --> 00:27:09,840
as privatecat and i'm also on linkedin

791
00:27:09,840 --> 00:27:10,880
as privatecat

792
00:27:10,880 --> 00:27:13,440
so yeah keep the cat things consistent

793
00:27:13,440 --> 00:27:14,080
thanks

794
00:27:14,080 --> 00:27:16,640
for attending and i'm happy to answer

795
00:27:16,640 --> 00:27:21,840
any questions that you might have

796
00:27:24,000 --> 00:27:26,240
there we go all right nope i uh stopped

797
00:27:26,240 --> 00:27:27,200
it prematurely

798
00:27:27,200 --> 00:27:29,840
um uh she asked me oh there we go carrie

799
00:27:29,840 --> 00:27:32,480
is joining us

800
00:27:32,960 --> 00:27:35,840
you're a mute

801
00:27:38,980 --> 00:27:42,059
[Music]

802
00:27:42,399 --> 00:27:44,000
it i don't even know how to describe

803
00:27:44,000 --> 00:27:48,559
what that sounds like it sounds like a

804
00:27:48,559 --> 00:27:50,480
cat meme if i had to pick what a cat

805
00:27:50,480 --> 00:27:52,080
meme would sound like

806
00:27:52,080 --> 00:27:55,120
that's what it would sound like

807
00:27:55,679 --> 00:27:59,039
very good no carrie uh you're on mute um

808
00:27:59,039 --> 00:28:02,000
or something it's still not um you're

809
00:28:02,000 --> 00:28:05,039
not on mute as far as we can tell

810
00:28:05,039 --> 00:28:08,960
but your sound is not coming through

811
00:28:13,600 --> 00:28:15,279
with that that was definitely a great

812
00:28:15,279 --> 00:28:17,360
presentation you know minus any hiccups

813
00:28:17,360 --> 00:28:19,279
that was uh

814
00:28:19,279 --> 00:28:21,919
super intuitive from a things i actually

815
00:28:21,919 --> 00:28:23,039
didn't know

816
00:28:23,039 --> 00:28:25,279
does this sound better there we go that

817
00:28:25,279 --> 00:28:27,279
sounds perfect we love it

818
00:28:27,279 --> 00:28:29,760
bitching all right yeah so that was fun

819
00:28:29,760 --> 00:28:30,480
um

820
00:28:30,480 --> 00:28:33,440
sorry about that i actually did do a

821
00:28:33,440 --> 00:28:34,799
whole bunch of editing and then of

822
00:28:34,799 --> 00:28:37,039
course

823
00:28:37,200 --> 00:28:40,880
you know but i do have my cat thing on

824
00:28:40,880 --> 00:28:43,840
so you know look at that wait where's

825
00:28:43,840 --> 00:28:45,279
the cat thing i missed it

826
00:28:45,279 --> 00:28:47,360
oh i had i have cat ears and it also

827
00:28:47,360 --> 00:28:48,640
says so we got the picture with the cat

828
00:28:48,640 --> 00:28:49,600
ears

829
00:28:49,600 --> 00:28:51,840
yes let me see i mean you're talking to

830
00:28:51,840 --> 00:28:53,039
the guy who

831
00:28:53,039 --> 00:28:56,159
has unicorn ears yes well that's why i

832
00:28:56,159 --> 00:28:58,240
put this on actually

833
00:28:58,240 --> 00:29:00,799
i put this on entirely because i saw

834
00:29:00,799 --> 00:29:02,240
your unicorn thing

835
00:29:02,240 --> 00:29:05,360
and i'm like here you go

836
00:29:05,360 --> 00:29:07,600
i got to be responsive you know um and

837
00:29:07,600 --> 00:29:09,039
also that's awesome

838
00:29:09,039 --> 00:29:10,880
it couldn't go any more wronger than

839
00:29:10,880 --> 00:29:12,320
whatever that was

840
00:29:12,320 --> 00:29:15,520
so sorry about that um anyway so

841
00:29:15,520 --> 00:29:18,640
i uh thank you guys for having me uh

842
00:29:18,640 --> 00:29:19,279
present

843
00:29:19,279 --> 00:29:22,720
i i did enjoy doing the talk

844
00:29:22,720 --> 00:29:26,159
um i don't know if anyone has any

845
00:29:26,159 --> 00:29:27,919
questions or if you want to

846
00:29:27,919 --> 00:29:29,440
rapid fire some of the really awesome

847
00:29:29,440 --> 00:29:31,039
questions you guys were asking to other

848
00:29:31,039 --> 00:29:33,600
panelists

849
00:29:34,000 --> 00:29:36,399
uh tyler can you take point i'm still um

850
00:29:36,399 --> 00:29:39,440
posting her cat pictures

851
00:29:39,440 --> 00:29:41,520
no problem no problem i mean one of the

852
00:29:41,520 --> 00:29:42,799
the questions that we have been posing

853
00:29:42,799 --> 00:29:43,120
to

854
00:29:43,120 --> 00:29:45,360
most of the speakers is kind of a

855
00:29:45,360 --> 00:29:46,720
general prediction of

856
00:29:46,720 --> 00:29:49,679
what 2021 might bring and so your

857
00:29:49,679 --> 00:29:51,279
perspective on kind of

858
00:29:51,279 --> 00:29:54,080
what that might play out with from any

859
00:29:54,080 --> 00:29:55,679
point of view you're welcome to kind of

860
00:29:55,679 --> 00:29:57,120
take that however you

861
00:29:57,120 --> 00:29:59,440
see fit but i'll be curious to see what

862
00:29:59,440 --> 00:30:00,399
your 21

863
00:30:00,399 --> 00:30:04,000
20 21 predictions are yeah so

864
00:30:04,000 --> 00:30:06,640
from a security standpoint actually this

865
00:30:06,640 --> 00:30:07,520
is interesting

866
00:30:07,520 --> 00:30:10,880
that it came up um i did a recent

867
00:30:10,880 --> 00:30:13,919
podcast that wasn't a cosmic failure

868
00:30:13,919 --> 00:30:17,600
i promise um and we were talking with

869
00:30:17,600 --> 00:30:21,120
uh another colleague about how uh

870
00:30:21,120 --> 00:30:24,159
you know library vulnerabilities like

871
00:30:24,159 --> 00:30:27,279
open source and even you know existing

872
00:30:27,279 --> 00:30:29,200
code-based library vulnerabilities are

873
00:30:29,200 --> 00:30:30,799
going to be even more present you know

874
00:30:30,799 --> 00:30:32,159
partly

875
00:30:32,159 --> 00:30:33,679
through these various supply chain

876
00:30:33,679 --> 00:30:34,720
attacks because those are going to

877
00:30:34,720 --> 00:30:36,159
definitely increase i mean i think

878
00:30:36,159 --> 00:30:37,919
solarwinds is a precursor of what we're

879
00:30:37,919 --> 00:30:39,200
going to see

880
00:30:39,200 --> 00:30:42,080
a lot more of um and the fact that

881
00:30:42,080 --> 00:30:44,240
there's all these just open

882
00:30:44,240 --> 00:30:45,440
you know libraries that haven't been

883
00:30:45,440 --> 00:30:47,919
updated in years you know maybe

884
00:30:47,919 --> 00:30:50,159
libraries that were on python 2.7 and

885
00:30:50,159 --> 00:30:51,600
now we're on three so you get some

886
00:30:51,600 --> 00:30:53,520
interesting you know edge cases and

887
00:30:53,520 --> 00:30:55,520
and behavioral changes in terms of what

888
00:30:55,520 --> 00:30:56,559
the

889
00:30:56,559 --> 00:30:58,159
what what's what's going on i think

890
00:30:58,159 --> 00:31:00,559
those are going to be heavily exploited

891
00:31:00,559 --> 00:31:04,240
um from a data protection standpoint

892
00:31:04,240 --> 00:31:06,960
some folks wisely thought wait a second

893
00:31:06,960 --> 00:31:08,399
what about brexit

894
00:31:08,399 --> 00:31:12,480
um so that's going to be a huge mess

895
00:31:12,480 --> 00:31:15,679
to to be politically correct about it

896
00:31:15,679 --> 00:31:17,919
because even though there has been a a

897
00:31:17,919 --> 00:31:19,679
deal that's been agreed to

898
00:31:19,679 --> 00:31:22,720
that deal is very limited and more

899
00:31:22,720 --> 00:31:25,200
importantly it has a lot of like

900
00:31:25,200 --> 00:31:28,240
six-month windows so there's a six-month

901
00:31:28,240 --> 00:31:29,519
exception

902
00:31:29,519 --> 00:31:32,080
for for the uk because the uk was part

903
00:31:32,080 --> 00:31:33,519
of the eu and they were declared

904
00:31:33,519 --> 00:31:34,399
adequate

905
00:31:34,399 --> 00:31:36,799
but the eu or the uk has the same kind

906
00:31:36,799 --> 00:31:38,320
of similar problems that

907
00:31:38,320 --> 00:31:40,559
uh the u.s does in terms of the

908
00:31:40,559 --> 00:31:42,480
surveillance state aspects

909
00:31:42,480 --> 00:31:45,279
um law enforcement access to personal

910
00:31:45,279 --> 00:31:46,159
data

911
00:31:46,159 --> 00:31:47,840
and how companies are processing

912
00:31:47,840 --> 00:31:49,279
personal data

913
00:31:49,279 --> 00:31:51,840
so all of those considerations still

914
00:31:51,840 --> 00:31:53,039
exist

915
00:31:53,039 --> 00:31:56,080
and um and so that that that is going to

916
00:31:56,080 --> 00:31:57,600
create a lot of challenges

917
00:31:57,600 --> 00:32:01,039
you know come june or july when

918
00:32:01,039 --> 00:32:04,000
when you know this this brexit you know

919
00:32:04,000 --> 00:32:05,519
half-assed brexit that we've got that

920
00:32:05,519 --> 00:32:07,360
they've got going on over there

921
00:32:07,360 --> 00:32:10,480
um actually has to

922
00:32:10,480 --> 00:32:11,519
you know they need to come up with

923
00:32:11,519 --> 00:32:14,080
something new basically

924
00:32:14,080 --> 00:32:16,720
so i got a question um one of the one of

925
00:32:16,720 --> 00:32:18,880
the things about gdpr is we we had a

926
00:32:18,880 --> 00:32:20,320
couple of years of

927
00:32:20,320 --> 00:32:23,519
of hand wringing and concern um because

928
00:32:23,519 --> 00:32:27,039
i mean it was supposed to be this

929
00:32:27,039 --> 00:32:30,000
very onerous scary regulatory framework

930
00:32:30,000 --> 00:32:30,960
that was coming down

931
00:32:30,960 --> 00:32:34,159
on uh data privacy and

932
00:32:34,159 --> 00:32:36,159
management and one of the things that

933
00:32:36,159 --> 00:32:37,519
i've been reading about lately

934
00:32:37,519 --> 00:32:40,640
is actually how toothless it has been

935
00:32:40,640 --> 00:32:43,760
on the enforcement side yeah

936
00:32:43,760 --> 00:32:45,760
so there's a i did an analysis of this

937
00:32:45,760 --> 00:32:46,799
actually recently

938
00:32:46,799 --> 00:32:48,399
in that mostly in the context of data

939
00:32:48,399 --> 00:32:50,000
breaches because data breaches are

940
00:32:50,000 --> 00:32:53,120
you know security related um

941
00:32:53,120 --> 00:32:56,080
and the regulators i think have only

942
00:32:56,080 --> 00:32:58,919
done enforcement actions against about

943
00:32:58,919 --> 00:33:02,000
350 organizations that's across the

944
00:33:02,000 --> 00:33:02,640
entire

945
00:33:02,640 --> 00:33:06,480
28 now 27 um eu countries

946
00:33:06,480 --> 00:33:08,640
and so that's kind of depressingly low

947
00:33:08,640 --> 00:33:10,080
given the number of

948
00:33:10,080 --> 00:33:11,600
you know data breaches that are

949
00:33:11,600 --> 00:33:13,440
happening and the number of you know

950
00:33:13,440 --> 00:33:13,919
other

951
00:33:13,919 --> 00:33:16,960
you know various uh gdpr violations that

952
00:33:16,960 --> 00:33:19,200
are out there because gdpr covers

953
00:33:19,200 --> 00:33:22,399
a lot of different things um and so

954
00:33:22,399 --> 00:33:24,399
the regulators need to step up there's

955
00:33:24,399 --> 00:33:25,960
some

956
00:33:25,960 --> 00:33:29,279
complications in terms of um so i live

957
00:33:29,279 --> 00:33:30,399
in ireland

958
00:33:30,399 --> 00:33:34,240
and ireland is a major um

959
00:33:34,240 --> 00:33:38,720
is a major data um

960
00:33:38,720 --> 00:33:41,360
data protection authority and and that's

961
00:33:41,360 --> 00:33:42,880
because google and

962
00:33:42,880 --> 00:33:44,559
facebook and twitter and all these other

963
00:33:44,559 --> 00:33:46,480
organizations have set up shop in

964
00:33:46,480 --> 00:33:48,080
ireland for tax reasons

965
00:33:48,080 --> 00:33:49,440
and also because they know that the

966
00:33:49,440 --> 00:33:50,960
regulators are going to be kind of light

967
00:33:50,960 --> 00:33:52,480
here and

968
00:33:52,480 --> 00:33:54,240
and so they haven't really pursued a lot

969
00:33:54,240 --> 00:33:56,080
of actions now the shrimp's case

970
00:33:56,080 --> 00:33:58,240
is is the notable exception and they're

971
00:33:58,240 --> 00:34:00,399
still dragging their heels

972
00:34:00,399 --> 00:34:02,960
seven and a half years later on dealing

973
00:34:02,960 --> 00:34:04,799
with shrims properly

974
00:34:04,799 --> 00:34:08,159
um but i but i think that if regulators

975
00:34:08,159 --> 00:34:09,199
don't step up you're right

976
00:34:09,199 --> 00:34:11,359
uh bryson it's just going to be kind of

977
00:34:11,359 --> 00:34:12,399
toothless

978
00:34:12,399 --> 00:34:14,079
but if they do start to step up and i

979
00:34:14,079 --> 00:34:15,839
think there's some

980
00:34:15,839 --> 00:34:19,040
um there's some indicators that that's

981
00:34:19,040 --> 00:34:20,399
actually going to happen at least in

982
00:34:20,399 --> 00:34:21,679
certain countries

983
00:34:21,679 --> 00:34:24,320
germany and belgium being primary

984
00:34:24,320 --> 00:34:25,599
examples you know they just

985
00:34:25,599 --> 00:34:27,839
and france for that matter so cnil the

986
00:34:27,839 --> 00:34:29,359
french data protection authority just

987
00:34:29,359 --> 00:34:31,440
issued a huge set of fines against

988
00:34:31,440 --> 00:34:33,918
google and amazon in belgium's issued

989
00:34:33,918 --> 00:34:35,679
fines against google

990
00:34:35,679 --> 00:34:37,679
say sorry do you see most of these fines

991
00:34:37,679 --> 00:34:39,440
coming towards

992
00:34:39,440 --> 00:34:41,040
the big companies that have the money to

993
00:34:41,040 --> 00:34:42,480
deal with this and

994
00:34:42,480 --> 00:34:44,960
are just going to take that fine and you

995
00:34:44,960 --> 00:34:46,560
know it costs them less to deal with the

996
00:34:46,560 --> 00:34:48,000
fine than it does to actually

997
00:34:48,000 --> 00:34:49,839
deal with the data and and provide real

998
00:34:49,839 --> 00:34:51,119
protection do you see that

999
00:34:51,119 --> 00:34:54,159
yeah i mean i think i think it's kind of

1000
00:34:54,159 --> 00:34:56,159
interesting the majority of the cases

1001
00:34:56,159 --> 00:34:57,599
actually the majority of the enforcement

1002
00:34:57,599 --> 00:34:59,359
actions have been largely against

1003
00:34:59,359 --> 00:35:02,800
small firms or against repeat offenders

1004
00:35:02,800 --> 00:35:05,440
uh vodafone espana for instance gets

1005
00:35:05,440 --> 00:35:07,440
tagged like every week

1006
00:35:07,440 --> 00:35:09,440
because they're just jerks and they they

1007
00:35:09,440 --> 00:35:11,280
do data you know they do

1008
00:35:11,280 --> 00:35:13,119
lots of direct marketing violations and

1009
00:35:13,119 --> 00:35:15,119
they do weird things where they're

1010
00:35:15,119 --> 00:35:16,480
sending like

1011
00:35:16,480 --> 00:35:18,800
um people's content and personal

1012
00:35:18,800 --> 00:35:21,200
information over sms chat messages and

1013
00:35:21,200 --> 00:35:23,599
crap lots of lots of hanky stuff that

1014
00:35:23,599 --> 00:35:25,680
they should really know better not to do

1015
00:35:25,680 --> 00:35:29,680
um but i think that regulators

1016
00:35:29,680 --> 00:35:32,160
if they decide to start taking more

1017
00:35:32,160 --> 00:35:33,520
notice or they

1018
00:35:33,520 --> 00:35:36,960
move away from what has

1019
00:35:36,960 --> 00:35:39,920
uh what is insulated companies so far

1020
00:35:39,920 --> 00:35:41,040
which is the idea of

1021
00:35:41,040 --> 00:35:44,320
um a single data protection authority

1022
00:35:44,320 --> 00:35:46,480
being the regulator

1023
00:35:46,480 --> 00:35:50,160
of record then i think it's going to be

1024
00:35:50,160 --> 00:35:52,240
a bit more interesting and i think where

1025
00:35:52,240 --> 00:35:54,079
that is likely to come up is through

1026
00:35:54,079 --> 00:35:55,359
adequacy

1027
00:35:55,359 --> 00:35:58,079
i think that some i think the senile and

1028
00:35:58,079 --> 00:35:58,720
i think

1029
00:35:58,720 --> 00:36:01,040
uh belgium and germany are going to take

1030
00:36:01,040 --> 00:36:03,200
a very hard-line approach

1031
00:36:03,200 --> 00:36:06,240
about uh interpreting this decision

1032
00:36:06,240 --> 00:36:09,200
and so they are going to start going

1033
00:36:09,200 --> 00:36:10,880
after companies and i think the moment

1034
00:36:10,880 --> 00:36:12,560
you start coming in and saying

1035
00:36:12,560 --> 00:36:14,240
look your your standard contractual

1036
00:36:14,240 --> 00:36:16,320
clauses don't mean

1037
00:36:16,320 --> 00:36:19,520
um you need to stop transferring data

1038
00:36:19,520 --> 00:36:21,839
that's that's actually going to be a bit

1039
00:36:21,839 --> 00:36:23,280
of a game changer

1040
00:36:23,280 --> 00:36:26,560
what's this do you see retroactive

1041
00:36:26,560 --> 00:36:29,520
um things happening for data breaches

1042
00:36:29,520 --> 00:36:30,160
that have been

1043
00:36:30,160 --> 00:36:32,160
previous that are within sight of the

1044
00:36:32,160 --> 00:36:33,200
gdprp

1045
00:36:33,200 --> 00:36:36,880
range and then going retroactively back

1046
00:36:36,880 --> 00:36:38,880
once data is leaked or there's a breach

1047
00:36:38,880 --> 00:36:40,640
or things are brought to light do you

1048
00:36:40,640 --> 00:36:43,119
see that happening at all

1049
00:36:43,119 --> 00:36:46,160
um well there's retroactive isn't quite

1050
00:36:46,160 --> 00:36:47,200
right it's more like

1051
00:36:47,200 --> 00:36:49,599
these breaches have already been

1052
00:36:49,599 --> 00:36:50,720
reported

1053
00:36:50,720 --> 00:36:52,800
the ico is notorious for this so there's

1054
00:36:52,800 --> 00:36:54,320
two big breeches you know there was the

1055
00:36:54,320 --> 00:36:55,040
marriott

1056
00:36:55,040 --> 00:36:59,200
and then there was um uh british airways

1057
00:36:59,200 --> 00:37:02,400
and those were reported like very very

1058
00:37:02,400 --> 00:37:03,440
shortly after

1059
00:37:03,440 --> 00:37:06,400
you know uh they occurred it just took

1060
00:37:06,400 --> 00:37:07,520
the ico

1061
00:37:07,520 --> 00:37:10,240
two years to get their act together to

1062
00:37:10,240 --> 00:37:12,400
actually do anything and then they

1063
00:37:12,400 --> 00:37:14,560
issued a statement saying oh we're going

1064
00:37:14,560 --> 00:37:16,160
to find them crazy amounts of money and

1065
00:37:16,160 --> 00:37:17,680
then they backtrack

1066
00:37:17,680 --> 00:37:20,000
so it's it's really just a process of um

1067
00:37:20,000 --> 00:37:21,680
the regulator

1068
00:37:21,680 --> 00:37:24,720
acting on it relatively uh

1069
00:37:24,720 --> 00:37:26,720
expediently and that's that's i don't

1070
00:37:26,720 --> 00:37:28,240
think it's gonna change but they

1071
00:37:28,240 --> 00:37:30,400
they have been once they hear about a

1072
00:37:30,400 --> 00:37:32,320
breach they tend to start an action they

1073
00:37:32,320 --> 00:37:34,079
start an investigation i just don't know

1074
00:37:34,079 --> 00:37:35,359
if they're gonna

1075
00:37:35,359 --> 00:37:38,079
go any faster about it do you think this

1076
00:37:38,079 --> 00:37:39,520
is a framework worth

1077
00:37:39,520 --> 00:37:41,440
uh following from other countries

1078
00:37:41,440 --> 00:37:43,040
standpoints such as the u.s

1079
00:37:43,040 --> 00:37:45,839
yes oh yeah yeah yeah gdpr is great

1080
00:37:45,839 --> 00:37:46,640
because the

1081
00:37:46,640 --> 00:37:49,440
one thing that it does is provides a

1082
00:37:49,440 --> 00:37:51,440
level of consistency

1083
00:37:51,440 --> 00:37:54,800
um the old data protection directive

1084
00:37:54,800 --> 00:37:57,920
it was a bit

1085
00:37:58,079 --> 00:38:02,240
mushy because different

1086
00:38:02,240 --> 00:38:04,800
eu countries could kind of carve out

1087
00:38:04,800 --> 00:38:06,160
specific

1088
00:38:06,160 --> 00:38:07,920
rules and provisions that were different

1089
00:38:07,920 --> 00:38:10,000
right so organizations had a hard time

1090
00:38:10,000 --> 00:38:10,880
following it

1091
00:38:10,880 --> 00:38:12,480
and of course it had no teeth that

1092
00:38:12,480 --> 00:38:14,000
didn't help there was less of an

1093
00:38:14,000 --> 00:38:15,359
incentive to follow it

1094
00:38:15,359 --> 00:38:17,440
uh the gdpr kind of standardizes a lot

1095
00:38:17,440 --> 00:38:19,200
of these things so countries have

1096
00:38:19,200 --> 00:38:22,160
certain wiggle room but not nearly as

1097
00:38:22,160 --> 00:38:23,280
much as they used to

1098
00:38:23,280 --> 00:38:25,839
and certainly a lot less than what the

1099
00:38:25,839 --> 00:38:27,920
us has which is this

1100
00:38:27,920 --> 00:38:31,280
50 state plus dc and puerto rico plus

1101
00:38:31,280 --> 00:38:33,280
whatever federal nonsense they've got

1102
00:38:33,280 --> 00:38:34,720
going on you know

1103
00:38:34,720 --> 00:38:37,760
in certain carve out areas um

1104
00:38:37,760 --> 00:38:39,359
a lot of countries are actually moving

1105
00:38:39,359 --> 00:38:42,079
into gdpr model mauritius is one

1106
00:38:42,079 --> 00:38:44,880
um and then of course japan has has lots

1107
00:38:44,880 --> 00:38:46,400
of things in this direction

1108
00:38:46,400 --> 00:38:50,320
singapore is passing um or is is moving

1109
00:38:50,320 --> 00:38:51,680
in that direction and a lot of

1110
00:38:51,680 --> 00:38:53,440
a lot of countries are going in that

1111
00:38:53,440 --> 00:38:54,800
because they realize there's

1112
00:38:54,800 --> 00:38:57,359
a level of uniformity and the practices

1113
00:38:57,359 --> 00:39:00,160
are generally sound this is based on

1114
00:39:00,160 --> 00:39:02,720
human rights and and general fundamental

1115
00:39:02,720 --> 00:39:04,160
principles of

1116
00:39:04,160 --> 00:39:06,560
fairness and transparency and most

1117
00:39:06,560 --> 00:39:07,280
people like

1118
00:39:07,280 --> 00:39:11,440
can intuitively understand that it gets

1119
00:39:11,440 --> 00:39:13,520
dodgy when it comes to some technical

1120
00:39:13,520 --> 00:39:15,200
enforcement aspects and that part of the

1121
00:39:15,200 --> 00:39:16,560
gdpr

1122
00:39:16,560 --> 00:39:18,320
needs some good technologists to sit

1123
00:39:18,320 --> 00:39:20,320
down with them and actually explain

1124
00:39:20,320 --> 00:39:21,839
what these different you know

1125
00:39:21,839 --> 00:39:24,000
technological solutions might be because

1126
00:39:24,000 --> 00:39:26,000
right now literally the gdpr only talks

1127
00:39:26,000 --> 00:39:27,680
about student anonymization

1128
00:39:27,680 --> 00:39:32,160
anonymization and encryption so

1129
00:39:32,160 --> 00:39:34,800
not so much so you have access right now

1130
00:39:34,800 --> 00:39:35,280
to

1131
00:39:35,280 --> 00:39:37,920
about 700 technologists and potentially

1132
00:39:37,920 --> 00:39:39,280
more who are going to watch all this

1133
00:39:39,280 --> 00:39:40,480
stuff

1134
00:39:40,480 --> 00:39:44,240
afterward um how do they get involved to

1135
00:39:44,240 --> 00:39:44,720
help

1136
00:39:44,720 --> 00:39:48,400
those folks right oh that's a good

1137
00:39:48,400 --> 00:39:48,960
question

1138
00:39:48,960 --> 00:39:52,800
um so there are a lot of various working

1139
00:39:52,800 --> 00:39:54,720
groups out there on the eu side

1140
00:39:54,720 --> 00:39:57,200
um the european data protection board

1141
00:39:57,200 --> 00:39:58,480
and

1142
00:39:58,480 --> 00:40:01,440
the european uh data protection working

1143
00:40:01,440 --> 00:40:03,359
group i think is the other one

1144
00:40:03,359 --> 00:40:05,280
forgive me i can send you the the links

1145
00:40:05,280 --> 00:40:07,040
all the different things but the un uh

1146
00:40:07,040 --> 00:40:10,400
there's also the un like um

1147
00:40:10,400 --> 00:40:13,040
data digital rights groups and things

1148
00:40:13,040 --> 00:40:14,000
like that

1149
00:40:14,000 --> 00:40:16,160
and and you can kind of get involved in

1150
00:40:16,160 --> 00:40:17,680
that direction

1151
00:40:17,680 --> 00:40:20,160
if you're in europe you can you know

1152
00:40:20,160 --> 00:40:22,240
start to actually submit complaints

1153
00:40:22,240 --> 00:40:25,040
to your various regulators um we try

1154
00:40:25,040 --> 00:40:26,800
that in ireland a lot and sometimes it

1155
00:40:26,800 --> 00:40:28,800
works and a lot of times it doesn't

1156
00:40:28,800 --> 00:40:31,920
um but but other than that i think

1157
00:40:31,920 --> 00:40:34,880
i think the the bigger solution and what

1158
00:40:34,880 --> 00:40:37,200
i would love to see more people doing

1159
00:40:37,200 --> 00:40:40,400
is um you know a lot of these sort a lot

1160
00:40:40,400 --> 00:40:40,960
of these

1161
00:40:40,960 --> 00:40:43,520
things like federated learning and and

1162
00:40:43,520 --> 00:40:44,880
homomorphic encryption

1163
00:40:44,880 --> 00:40:47,599
this is open source like guys you could

1164
00:40:47,599 --> 00:40:49,760
be developing on this stuff

1165
00:40:49,760 --> 00:40:51,920
how cool would it be if we actually

1166
00:40:51,920 --> 00:40:53,680
start getting a bunch of really smart

1167
00:40:53,680 --> 00:40:55,040
people to get around and get in a room

1168
00:40:55,040 --> 00:40:56,319
together and start going

1169
00:40:56,319 --> 00:40:58,160
all right let's fix this like it's

1170
00:40:58,160 --> 00:40:59,520
been in existence some of these things

1171
00:40:59,520 --> 00:41:01,359
for like 20 years

1172
00:41:01,359 --> 00:41:04,960
uh it's just it hasn't had a lot of

1173
00:41:04,960 --> 00:41:06,880
hasn't had as much love as it really

1174
00:41:06,880 --> 00:41:08,240
should have and

1175
00:41:08,240 --> 00:41:10,480
and now i think you know with shrems and

1176
00:41:10,480 --> 00:41:11,599
with some of these

1177
00:41:11,599 --> 00:41:12,960
later developments and of course the

1178
00:41:12,960 --> 00:41:15,280
supply chain attacks and all these other

1179
00:41:15,280 --> 00:41:17,680
um factors i think it's going to maybe

1180
00:41:17,680 --> 00:41:19,680
encourage more technologists to try to

1181
00:41:19,680 --> 00:41:20,319
find

1182
00:41:20,319 --> 00:41:21,839
technical solutions that will actually

1183
00:41:21,839 --> 00:41:23,119
work because they realize that the

1184
00:41:23,119 --> 00:41:24,079
lawyers

1185
00:41:24,079 --> 00:41:27,119
don't really have great answers

1186
00:41:27,119 --> 00:41:28,960
yeah so we think that that's even

1187
00:41:28,960 --> 00:41:30,800
possible with uh

1188
00:41:30,800 --> 00:41:35,040
with the monetization of that data that

1189
00:41:35,040 --> 00:41:36,319
it is really used

1190
00:41:36,319 --> 00:41:38,400
from a monetary standpoint for you know

1191
00:41:38,400 --> 00:41:40,000
the benefits of the companies that

1192
00:41:40,000 --> 00:41:41,839
ingest or utilize that data so if

1193
00:41:41,839 --> 00:41:44,640
they're not able to then monetize or

1194
00:41:44,640 --> 00:41:46,800
value that data is there really an

1195
00:41:46,800 --> 00:41:48,640
incentive for them to do that

1196
00:41:48,640 --> 00:41:50,560
well so i think you know i think

1197
00:41:50,560 --> 00:41:52,560
federated learning is a really good case

1198
00:41:52,560 --> 00:41:53,200
where

1199
00:41:53,200 --> 00:41:57,680
um the big technology companies

1200
00:41:57,680 --> 00:42:00,079
have actually made efforts and aren't

1201
00:42:00,079 --> 00:42:01,359
generating money

1202
00:42:01,359 --> 00:42:03,920
you know i think google makes money off

1203
00:42:03,920 --> 00:42:04,640
of

1204
00:42:04,640 --> 00:42:06,160
a lot of the stuff that our phones are

1205
00:42:06,160 --> 00:42:08,720
doing and a lot of the learnings that

1206
00:42:08,720 --> 00:42:09,440
are occurring

1207
00:42:09,440 --> 00:42:11,119
even if they don't necessarily know that

1208
00:42:11,119 --> 00:42:12,640
this particular learning about this

1209
00:42:12,640 --> 00:42:13,920
particular cat

1210
00:42:13,920 --> 00:42:16,400
is coming from me they understand that

1211
00:42:16,400 --> 00:42:18,560
you know the model is getting improved

1212
00:42:18,560 --> 00:42:20,640
right and that's money making you don't

1213
00:42:20,640 --> 00:42:22,400
need my personal data for that

1214
00:42:22,400 --> 00:42:24,800
you just need to you just need to have

1215
00:42:24,800 --> 00:42:26,560
the learning model improved so that it

1216
00:42:26,560 --> 00:42:28,560
can do these detections

1217
00:42:28,560 --> 00:42:30,720
and and i think that's that's one area

1218
00:42:30,720 --> 00:42:32,640
where you you can actually

1219
00:42:32,640 --> 00:42:34,880
marry those two what seem like

1220
00:42:34,880 --> 00:42:36,720
conflicting interests

1221
00:42:36,720 --> 00:42:38,800
together in a way that you know

1222
00:42:38,800 --> 00:42:40,480
companies can get behind

1223
00:42:40,480 --> 00:42:41,760
and i think homomorphic fully

1224
00:42:41,760 --> 00:42:44,079
homomorphic encryption in particular

1225
00:42:44,079 --> 00:42:46,160
can be another pathway you know the bob

1226
00:42:46,160 --> 00:42:49,440
analyst example is one where you can use

1227
00:42:49,440 --> 00:42:52,800
um you can you can generate results you

1228
00:42:52,800 --> 00:42:54,480
can generate findings

1229
00:42:54,480 --> 00:42:57,040
without necessarily compromising the the

1230
00:42:57,040 --> 00:42:59,119
integrity and the confidentiality of the

1231
00:42:59,119 --> 00:43:00,800
underlying data

1232
00:43:00,800 --> 00:43:02,640
and so it's just going to be so there's

1233
00:43:02,640 --> 00:43:03,920
there are some creative and

1234
00:43:03,920 --> 00:43:07,359
there is a later slide in my properly

1235
00:43:07,359 --> 00:43:09,119
edited version where i talked about some

1236
00:43:09,119 --> 00:43:10,480
of the use cases

1237
00:43:10,480 --> 00:43:13,280
an actual proper examples here and so

1238
00:43:13,280 --> 00:43:13,599
there

1239
00:43:13,599 --> 00:43:16,720
are um there are some really good

1240
00:43:16,720 --> 00:43:19,680
uh outside things tensorflow being one

1241
00:43:19,680 --> 00:43:20,160
of them

1242
00:43:20,160 --> 00:43:21,520
being the biggest one and then there's

1243
00:43:21,520 --> 00:43:23,280
some um

1244
00:43:23,280 --> 00:43:25,680
unveil is a security firm that's doing

1245
00:43:25,680 --> 00:43:27,839
stuff with fully homomorphic encryption

1246
00:43:27,839 --> 00:43:30,880
so this is happening and people can

1247
00:43:30,880 --> 00:43:32,560
make money at it it's just that they

1248
00:43:32,560 --> 00:43:36,079
need to get more motivated to want to

1249
00:43:36,079 --> 00:43:37,920
so we that's all the time we've got for

1250
00:43:37,920 --> 00:43:39,440
this we have to set up for

1251
00:43:39,440 --> 00:43:41,920
our next speaker sona soleil who's going

1252
00:43:41,920 --> 00:43:43,040
to be talking about

1253
00:43:43,040 --> 00:43:45,920
what is cyber war um carrie if you could

1254
00:43:45,920 --> 00:43:47,599
make sure we get an updated

1255
00:43:47,599 --> 00:43:50,480
uh recording without the worked slides

1256
00:43:50,480 --> 00:43:51,200
uh

1257
00:43:51,200 --> 00:43:52,800
we will get those posted when we

1258
00:43:52,800 --> 00:43:54,640
formally released everything

1259
00:43:54,640 --> 00:43:55,920
for the conference so everybody's got a

1260
00:43:55,920 --> 00:43:58,400
chance to go and see those

1261
00:43:58,400 --> 00:44:00,240
great thanks guys bye-bye all right

1262
00:44:00,240 --> 00:44:04,160
thank you see ya

