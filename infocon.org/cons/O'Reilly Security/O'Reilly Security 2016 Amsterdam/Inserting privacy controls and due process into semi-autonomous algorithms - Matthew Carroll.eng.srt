1
00:00:00,560 --> 00:00:04,460
what I'm here to talk about is privacy

2
00:00:02,750 --> 00:00:07,330
versus security notes a security

3
00:00:04,460 --> 00:00:10,400
conference but I spent the past decade

4
00:00:07,330 --> 00:00:14,600
building computational analytics for the

5
00:00:10,400 --> 00:00:15,740
US intelligence community and I guess

6
00:00:14,600 --> 00:00:18,189
there's some irony that I'm here to talk

7
00:00:15,740 --> 00:00:20,689
about privacy coming from that community

8
00:00:18,189 --> 00:00:22,279
but the reality is is that the world's

9
00:00:20,689 --> 00:00:24,710
kind of change very quickly and that

10
00:00:22,279 --> 00:00:26,749
we're starting to see this whole new

11
00:00:24,710 --> 00:00:30,470
group of analytics that are machine

12
00:00:26,749 --> 00:00:33,110
learning deep learning based and we're

13
00:00:30,470 --> 00:00:36,500
seeing that Silicon Valley is driving

14
00:00:33,110 --> 00:00:38,810
the community meaning startups to start

15
00:00:36,500 --> 00:00:41,030
learning more about all of you and

16
00:00:38,810 --> 00:00:42,500
everything that you're using Tim Cook

17
00:00:41,030 --> 00:00:44,480
had a really good quote here is is that

18
00:00:42,500 --> 00:00:45,650
these companies are starting to look at

19
00:00:44,480 --> 00:00:47,390
your personal information and that

20
00:00:45,650 --> 00:00:48,949
that's how they're monetizing that's how

21
00:00:47,390 --> 00:00:50,750
they're going to market is using

22
00:00:48,950 --> 00:00:52,460
information about you to give you

23
00:00:50,750 --> 00:00:56,090
something and they call innovation and I

24
00:00:52,460 --> 00:00:57,440
call it invasion and they like to call

25
00:00:56,090 --> 00:01:00,680
it relationships they're going to build

26
00:00:57,440 --> 00:01:03,290
a relationship with you except alexa

27
00:01:00,680 --> 00:01:05,269
alexa just yells at my son sort of a

28
00:01:03,290 --> 00:01:06,890
very good relationship right now but if

29
00:01:05,269 --> 00:01:08,780
you look at the numbers that are

30
00:01:06,890 --> 00:01:11,600
staggering how many people they can

31
00:01:08,780 --> 00:01:14,359
communicate with fifty six percent of

32
00:01:11,600 --> 00:01:16,100
the media from a mobile perspective is

33
00:01:14,359 --> 00:01:20,630
controlled by Google and Facebook right

34
00:01:16,100 --> 00:01:22,399
now and so what they're starting to do

35
00:01:20,630 --> 00:01:24,318
is is you're seeing these open source

36
00:01:22,399 --> 00:01:25,670
frameworks come out and the reason

37
00:01:24,319 --> 00:01:28,520
they're doing that is they can't build

38
00:01:25,670 --> 00:01:30,079
analytics fast enough to build a deeper

39
00:01:28,520 --> 00:01:31,579
relationship with you to knowing what

40
00:01:30,079 --> 00:01:32,779
you want to buy where you want to be who

41
00:01:31,579 --> 00:01:34,639
do you want to sleep with what type of

42
00:01:32,780 --> 00:01:36,530
drink do you want they want to know all

43
00:01:34,639 --> 00:01:38,539
that information as fast as possible and

44
00:01:36,530 --> 00:01:40,909
they can't build it fast enough and so

45
00:01:38,539 --> 00:01:44,569
they're open sourcing frameworks like

46
00:01:40,909 --> 00:01:46,670
tensorflow mtk and the idea is if you

47
00:01:44,569 --> 00:01:49,759
look at the commits on the bottom here

48
00:01:46,670 --> 00:01:53,990
they're going up fast and the reason is

49
00:01:49,759 --> 00:01:55,310
is that they know that these engineers

50
00:01:53,990 --> 00:01:57,619
aren't necessarily equipped to

51
00:01:55,310 --> 00:01:59,479
understand the implications as they

52
00:01:57,619 --> 00:02:01,009
build these models but they can buy them

53
00:01:59,479 --> 00:02:03,048
and when they can buy companies very

54
00:02:01,009 --> 00:02:04,639
quickly start learning about people the

55
00:02:03,049 --> 00:02:06,139
VCS get involved I don't how many vc's

56
00:02:04,639 --> 00:02:08,389
are here but I mean that's what you go

57
00:02:06,139 --> 00:02:10,098
on right you go on the market bias so

58
00:02:08,389 --> 00:02:12,790
they started investing if you look at

59
00:02:10,098 --> 00:02:14,329
the keywords you know here they're just

60
00:02:12,790 --> 00:02:17,079
investing in these company

61
00:02:14,330 --> 00:02:18,980
and if you look at the overall

62
00:02:17,080 --> 00:02:20,300
investment over time look at that

63
00:02:18,980 --> 00:02:21,920
staggering just look at the change from

64
00:02:20,300 --> 00:02:26,240
2014 at 2016 the amount of investment

65
00:02:21,920 --> 00:02:28,730
and artificial intelligence and the idea

66
00:02:26,240 --> 00:02:31,520
here is this just in 2016 look at these

67
00:02:28,730 --> 00:02:34,459
companies Google Apple Samsung buying up

68
00:02:31,520 --> 00:02:35,870
AI machine learning based companies and

69
00:02:34,460 --> 00:02:37,130
why that's a problem why is this how

70
00:02:35,870 --> 00:02:39,080
does this impact privacy like why am I

71
00:02:37,130 --> 00:02:42,560
even up here this is crazy right well

72
00:02:39,080 --> 00:02:44,810
the Microsoft court case in Ireland so

73
00:02:42,560 --> 00:02:48,650
we called to Microsoft Ireland case set

74
00:02:44,810 --> 00:02:51,950
an archaic result which is we're going

75
00:02:48,650 --> 00:02:54,170
to define the legality the sovereignty

76
00:02:51,950 --> 00:02:55,399
of data based on where it rests and the

77
00:02:54,170 --> 00:02:57,530
problem is is a machine learning

78
00:02:55,400 --> 00:03:00,350
especially is that we're a global

79
00:02:57,530 --> 00:03:01,850
economy now the data and the analytics

80
00:03:00,350 --> 00:03:02,840
are going to be where the user is it's

81
00:03:01,850 --> 00:03:04,850
on the edge you're gonna hear a lot

82
00:03:02,840 --> 00:03:07,490
about ilt and all the security and it's

83
00:03:04,850 --> 00:03:08,810
not my world but you know the idea is as

84
00:03:07,490 --> 00:03:10,190
we develop these analytics we need to

85
00:03:08,810 --> 00:03:12,200
push it forward what I learned in

86
00:03:10,190 --> 00:03:14,720
Baghdad way back when when you know data

87
00:03:12,200 --> 00:03:16,459
science wasn't even a word math and we

88
00:03:14,720 --> 00:03:18,590
had drones we had sensors on the ground

89
00:03:16,459 --> 00:03:19,850
was we couldn't push everything back to

90
00:03:18,590 --> 00:03:21,890
the United States we had to push

91
00:03:19,850 --> 00:03:23,989
real-time analytics forward onto the

92
00:03:21,890 --> 00:03:25,369
edge utilizing you know insane

93
00:03:23,989 --> 00:03:27,110
infrastructure that we piecemeal

94
00:03:25,370 --> 00:03:29,150
together and that's what's happening

95
00:03:27,110 --> 00:03:31,160
with AI is that it's all global and so

96
00:03:29,150 --> 00:03:33,350
countries can't protect their citizens

97
00:03:31,160 --> 00:03:35,299
regulators can't protect consumers

98
00:03:33,350 --> 00:03:37,220
because it's pushed to the edge and it's

99
00:03:35,300 --> 00:03:40,700
any country and so we're going to have

100
00:03:37,220 --> 00:03:42,709
to create a series of treaties and those

101
00:03:40,700 --> 00:03:44,690
are going to take years to be able to

102
00:03:42,709 --> 00:03:45,980
engage and by that time your privacy has

103
00:03:44,690 --> 00:03:49,430
already changed right technology is

104
00:03:45,980 --> 00:03:51,350
moving way too fast so if you look at a

105
00:03:49,430 --> 00:03:52,790
basic neural model here is that within

106
00:03:51,350 --> 00:03:54,079
these hidden layers you have no clue

107
00:03:52,790 --> 00:03:56,780
what's going on it's making arbitrary

108
00:03:54,080 --> 00:03:58,790
decisions based on the data that's being

109
00:03:56,780 --> 00:04:00,380
input and we have no way to insert

110
00:03:58,790 --> 00:04:02,630
ourselves as regulators so the problem

111
00:04:00,380 --> 00:04:04,040
is if you look at dr sources from let's

112
00:04:02,630 --> 00:04:05,299
say three parent sources from three

113
00:04:04,040 --> 00:04:07,700
different countries and then you create

114
00:04:05,300 --> 00:04:10,160
derived sources out of that chain of

115
00:04:07,700 --> 00:04:12,649
custody is broken and so the model just

116
00:04:10,160 --> 00:04:13,760
doesn't work and so to just kind of

117
00:04:12,650 --> 00:04:17,690
close up here because i only have five

118
00:04:13,760 --> 00:04:20,238
minutes is shameless plug but the the

119
00:04:17,690 --> 00:04:22,610
idea here is that we need a new metadata

120
00:04:20,238 --> 00:04:24,560
management model and we need to be able

121
00:04:22,610 --> 00:04:26,330
to track the origins of sources to all

122
00:04:24,560 --> 00:04:27,690
organizations and these shouldn't be

123
00:04:26,330 --> 00:04:29,880
countries that manage this

124
00:04:27,690 --> 00:04:31,170
should be third-party organizations that

125
00:04:29,880 --> 00:04:32,730
consumers can engage wouldn't have a

126
00:04:31,170 --> 00:04:34,470
conversation with as to how their dad is

127
00:04:32,730 --> 00:04:35,670
being used if they want to opt out and I

128
00:04:34,470 --> 00:04:37,920
like to call that like at a metadata

129
00:04:35,670 --> 00:04:40,830
administration server and use tokens and

130
00:04:37,920 --> 00:04:43,350
tokens that can describe analytics as

131
00:04:40,830 --> 00:04:45,300
well as the core metadata so what is the

132
00:04:43,350 --> 00:04:47,640
use of that data within that

133
00:04:45,300 --> 00:04:49,200
organization and then we can pull that

134
00:04:47,640 --> 00:04:51,659
data out that token away if a consumer

135
00:04:49,200 --> 00:04:53,810
wants and the idea is if you think from

136
00:04:51,660 --> 00:04:56,070
a social network down consumers can

137
00:04:53,810 --> 00:04:57,810
integrate with the metadata token

138
00:04:56,070 --> 00:05:00,090
administrator say hey not come from my

139
00:04:57,810 --> 00:05:02,880
data being used and then they can via

140
00:05:00,090 --> 00:05:06,150
proxy start to enforce regulation on top

141
00:05:02,880 --> 00:05:07,440
of those networks of data happy to talk

142
00:05:06,150 --> 00:05:09,630
about it length or you can read about

143
00:05:07,440 --> 00:05:11,490
some of our blogs but anyway thanks for

144
00:05:09,630 --> 00:05:13,440
your time and you know I know that was

145
00:05:11,490 --> 00:05:14,760
really short but we put a lot of

146
00:05:13,440 --> 00:05:17,270
information up there about this concept

147
00:05:14,760 --> 00:05:17,270
so thank you

