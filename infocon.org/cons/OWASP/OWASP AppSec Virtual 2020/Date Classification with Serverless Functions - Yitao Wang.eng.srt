1
00:00:01,439 --> 00:00:03,280
hi everyone in today's conversation i'm

2
00:00:03,280 --> 00:00:05,359
going to talk about data classification

3
00:00:05,359 --> 00:00:08,560
with serverless functions

4
00:00:09,599 --> 00:00:13,120
first a quick introduction of myself

5
00:00:13,120 --> 00:00:14,160
my name

6
00:00:14,160 --> 00:00:16,560
is ita wang and i work for fintech

7
00:00:16,560 --> 00:00:19,600
startup based in san francisco

8
00:00:19,600 --> 00:00:21,680
this virtual background shows where i am

9
00:00:21,680 --> 00:00:23,680
but actually i'm stuck somewhere outside

10
00:00:23,680 --> 00:00:27,119
san francisco we call this kobe 19.

11
00:00:27,119 --> 00:00:29,599
i have about 12 years of experience

12
00:00:29,599 --> 00:00:31,840
working on information security

13
00:00:31,840 --> 00:00:35,280
my recent two jobs are all in fintech

14
00:00:35,280 --> 00:00:37,360
focusing heavily on appsec

15
00:00:37,360 --> 00:00:40,399
red teaming and data security

16
00:00:40,399 --> 00:00:42,160
i'm not speaking on behalf of my

17
00:00:42,160 --> 00:00:44,480
employer or making any statements

18
00:00:44,480 --> 00:00:46,640
related to my companies however

19
00:00:46,640 --> 00:00:48,640
appreciate the help i got from my fellow

20
00:00:48,640 --> 00:00:53,399
colleagues to make this talk possible

21
00:00:58,640 --> 00:01:01,199
so let's talk about the agenda

22
00:01:01,199 --> 00:01:03,440
first i will try to talk about the data

23
00:01:03,440 --> 00:01:05,199
classification problems

24
00:01:05,199 --> 00:01:08,000
covering who should be the stakeholders

25
00:01:08,000 --> 00:01:09,920
and why data classification should draw

26
00:01:09,920 --> 00:01:12,799
the year draw their attentions

27
00:01:12,799 --> 00:01:15,360
in part two i will cover how companies

28
00:01:15,360 --> 00:01:17,759
solve this data classification problems

29
00:01:17,759 --> 00:01:18,880
today

30
00:01:18,880 --> 00:01:22,320
and what i think the gaps are

31
00:01:22,400 --> 00:01:23,840
in part three

32
00:01:23,840 --> 00:01:25,520
i will briefly touch on the serverless

33
00:01:25,520 --> 00:01:27,200
computing concepts

34
00:01:27,200 --> 00:01:29,280
and some other services offered by

35
00:01:29,280 --> 00:01:31,759
amazon cloud services

36
00:01:31,759 --> 00:01:34,000
please keep in mind that this solution

37
00:01:34,000 --> 00:01:37,520
is not aws specific

38
00:01:37,520 --> 00:01:39,680
the same concept could apply to other

39
00:01:39,680 --> 00:01:42,240
cloud service providers as well

40
00:01:42,240 --> 00:01:44,880
i use aws primarily because i'm pretty

41
00:01:44,880 --> 00:01:47,200
familiar with it and i have the demo

42
00:01:47,200 --> 00:01:49,439
setup on it as well

43
00:01:49,439 --> 00:01:51,119
i would love to hear the feedbacks from

44
00:01:51,119 --> 00:01:52,880
the audience on other cloud service

45
00:01:52,880 --> 00:01:55,759
providers as well

46
00:01:55,759 --> 00:01:57,360
in part four we'll look into the

47
00:01:57,360 --> 00:01:59,040
technical details

48
00:01:59,040 --> 00:02:00,799
to further understand how my solution

49
00:02:00,799 --> 00:02:03,439
works under the hood

50
00:02:03,439 --> 00:02:05,520
last but not least i will cover the

51
00:02:05,520 --> 00:02:07,759
current limitations of my solution and

52
00:02:07,759 --> 00:02:09,598
talk about future plans

53
00:02:09,598 --> 00:02:11,680
i'm open to feedbacks just feel free to

54
00:02:11,680 --> 00:02:14,319
reach out to me

55
00:02:21,040 --> 00:02:23,120
we saw this sign a lot

56
00:02:23,120 --> 00:02:25,520
especially movies

57
00:02:25,520 --> 00:02:28,080
documentaries this means data behind

58
00:02:28,080 --> 00:02:29,760
this label contains

59
00:02:29,760 --> 00:02:33,518
some level of sensitivity information

60
00:02:33,840 --> 00:02:36,800
data classification is broadly defined

61
00:02:36,800 --> 00:02:38,160
as

62
00:02:38,160 --> 00:02:40,720
the process of organizing the data by

63
00:02:40,720 --> 00:02:43,440
its relevant categories so that the data

64
00:02:43,440 --> 00:02:44,720
might be used

65
00:02:44,720 --> 00:02:47,680
and protected more efficiently

66
00:02:47,680 --> 00:02:49,519
it is particularly important when it

67
00:02:49,519 --> 00:02:52,560
comes to risk management compliance and

68
00:02:52,560 --> 00:02:55,360
data security

69
00:02:55,360 --> 00:02:57,680
on a basic level the classification

70
00:02:57,680 --> 00:02:59,840
process makes the data easier to locate

71
00:02:59,840 --> 00:03:01,519
and retrieve

72
00:03:01,519 --> 00:03:03,599
companies may be defining their own data

73
00:03:03,599 --> 00:03:06,480
control policy to classify the data

74
00:03:06,480 --> 00:03:08,480
accordingly so each company might have

75
00:03:08,480 --> 00:03:10,480
different data control policies or

76
00:03:10,480 --> 00:03:12,319
different kind of data classification

77
00:03:12,319 --> 00:03:14,400
levels

78
00:03:14,400 --> 00:03:16,480
what i typically saw

79
00:03:16,480 --> 00:03:18,959
is the classification mechanism

80
00:03:18,959 --> 00:03:21,360
that's put data in a three level or four

81
00:03:21,360 --> 00:03:22,480
level

82
00:03:22,480 --> 00:03:25,840
system um which generally includes um

83
00:03:25,840 --> 00:03:28,080
for three level system

84
00:03:28,080 --> 00:03:31,200
is public internal and restricted

85
00:03:31,200 --> 00:03:33,040
whereas the four level classification

86
00:03:33,040 --> 00:03:35,440
simply breaks down the restricted level

87
00:03:35,440 --> 00:03:38,159
to more granular levels just to allow

88
00:03:38,159 --> 00:03:40,879
better controls

89
00:03:41,519 --> 00:03:43,760
there's definitely a some cost benefit

90
00:03:43,760 --> 00:03:47,200
related to data classification as well

91
00:03:47,200 --> 00:03:49,200
a proper data classification allows your

92
00:03:49,200 --> 00:03:50,959
organization to apply

93
00:03:50,959 --> 00:03:52,560
appropriate controls based on

94
00:03:52,560 --> 00:03:54,640
predetermined categories and also keep

95
00:03:54,640 --> 00:03:56,480
in mind like different controls come

96
00:03:56,480 --> 00:03:59,360
with the cost so you don't necessarily

97
00:03:59,360 --> 00:04:00,959
want it to have the same level of

98
00:04:00,959 --> 00:04:03,519
control for all kinds of data

99
00:04:03,519 --> 00:04:05,519
some data such as static content on your

100
00:04:05,519 --> 00:04:07,760
website you want it to be viewable by

101
00:04:07,760 --> 00:04:08,720
anyone

102
00:04:08,720 --> 00:04:10,480
but you don't want to actually have

103
00:04:10,480 --> 00:04:13,040
someone to be able to change it

104
00:04:13,040 --> 00:04:14,959
that will make a difference if the data

105
00:04:14,959 --> 00:04:17,519
is sensitive data or like

106
00:04:17,519 --> 00:04:20,000
card holder information which generally

107
00:04:20,000 --> 00:04:22,079
should only be processed by the crypto

108
00:04:22,079 --> 00:04:24,479
systems

109
00:04:25,840 --> 00:04:27,280
you should only apply the security

110
00:04:27,280 --> 00:04:29,759
control that's required for data that

111
00:04:29,759 --> 00:04:31,840
you've classified based on predefined

112
00:04:31,840 --> 00:04:33,120
rules

113
00:04:33,120 --> 00:04:35,280
classify the data so you can save the

114
00:04:35,280 --> 00:04:37,280
time and money because you're able to

115
00:04:37,280 --> 00:04:39,520
focus on what's important

116
00:04:39,520 --> 00:04:42,080
and not wasting your resource putting

117
00:04:42,080 --> 00:04:45,599
unnecessary controls in place

118
00:04:51,680 --> 00:04:53,360
well on the other side

119
00:04:53,360 --> 00:04:55,759
we want necessary controls in place to

120
00:04:55,759 --> 00:04:58,800
protect sensitive data as well

121
00:04:58,800 --> 00:05:01,520
that means data classification is a

122
00:05:01,520 --> 00:05:03,840
fundamental step in cyber security risk

123
00:05:03,840 --> 00:05:05,280
management

124
00:05:05,280 --> 00:05:07,440
it involves identifying the types of

125
00:05:07,440 --> 00:05:10,960
data that are being processed and stored

126
00:05:10,960 --> 00:05:13,360
in information systems owned by

127
00:05:13,360 --> 00:05:16,960
or operated by your organizations

128
00:05:16,960 --> 00:05:18,960
we heard a lot of data leaks due to

129
00:05:18,960 --> 00:05:21,759
improper s3 bucket permissions

130
00:05:21,759 --> 00:05:25,039
or an encrypted database backups

131
00:05:25,039 --> 00:05:27,759
one of the reason this type of incident

132
00:05:27,759 --> 00:05:29,840
happened is that the control is not in

133
00:05:29,840 --> 00:05:32,000
place for the data that's being stored

134
00:05:32,000 --> 00:05:34,000
or processed

135
00:05:34,000 --> 00:05:35,840
in some cases

136
00:05:35,840 --> 00:05:37,680
the people might not even know this

137
00:05:37,680 --> 00:05:40,000
sensitive data is out there in this

138
00:05:40,000 --> 00:05:42,880
energy services

139
00:05:43,600 --> 00:05:45,360
to well understand the security risk of

140
00:05:45,360 --> 00:05:46,240
data

141
00:05:46,240 --> 00:05:48,320
you would need to ask yourself some

142
00:05:48,320 --> 00:05:49,919
questions such as

143
00:05:49,919 --> 00:05:52,960
what kind of data do you have is it

144
00:05:52,960 --> 00:05:56,240
is it intellectual property phi pii or

145
00:05:56,240 --> 00:05:57,840
credit card information

146
00:05:57,840 --> 00:06:00,400
or just public information

147
00:06:00,400 --> 00:06:02,479
and then where does where do those

148
00:06:02,479 --> 00:06:04,880
sensitive data reside

149
00:06:04,880 --> 00:06:07,440
what in the database or static storage

150
00:06:07,440 --> 00:06:08,639
or what

151
00:06:08,639 --> 00:06:10,720
and who have access to it and can they

152
00:06:10,720 --> 00:06:12,479
modify delete it or do something about

153
00:06:12,479 --> 00:06:14,080
it

154
00:06:14,080 --> 00:06:16,000
and then what your business will be

155
00:06:16,000 --> 00:06:17,840
affected

156
00:06:17,840 --> 00:06:19,520
how will your business be affected if

157
00:06:19,520 --> 00:06:21,600
the data is leaked destroyed or

158
00:06:21,600 --> 00:06:23,919
improperly changed

159
00:06:23,919 --> 00:06:25,759
with the answer of all these questions

160
00:06:25,759 --> 00:06:27,840
you may refer to organizations data

161
00:06:27,840 --> 00:06:30,160
control policy for gap analysis and

162
00:06:30,160 --> 00:06:31,840
further understand

163
00:06:31,840 --> 00:06:34,720
which data is at risk and come with the

164
00:06:34,720 --> 00:06:36,400
come up with a solution to control

165
00:06:36,400 --> 00:06:40,039
corresponding risk

166
00:06:48,000 --> 00:06:50,720
in some cases data classification is a

167
00:06:50,720 --> 00:06:53,039
regulatory or compliance requirement as

168
00:06:53,039 --> 00:06:54,080
well

169
00:06:54,080 --> 00:06:55,039
as

170
00:06:55,039 --> 00:06:57,919
data must be searchable and retrievable

171
00:06:57,919 --> 00:07:01,120
within specific time frames

172
00:07:01,120 --> 00:07:04,800
the common known ones here are ccpa gdpr

173
00:07:04,800 --> 00:07:07,599
hipaa pci dss and other

174
00:07:07,599 --> 00:07:09,520
privacy laws

175
00:07:09,520 --> 00:07:12,000
taking the ccpa as an example

176
00:07:12,000 --> 00:07:14,400
a california consumer has a right to

177
00:07:14,400 --> 00:07:15,520
access

178
00:07:15,520 --> 00:07:18,400
and the right delete of their data

179
00:07:18,400 --> 00:07:20,639
to fulfill certain requests

180
00:07:20,639 --> 00:07:22,240
a company will have to keep track of

181
00:07:22,240 --> 00:07:24,080
customer data

182
00:07:24,080 --> 00:07:26,800
knowing where the data is processed

183
00:07:26,800 --> 00:07:28,560
and how it is processed

184
00:07:28,560 --> 00:07:30,880
a proper labeling and monitoring of the

185
00:07:30,880 --> 00:07:31,919
data

186
00:07:31,919 --> 00:07:33,840
is the fundamental steps

187
00:07:33,840 --> 00:07:38,359
of achieving ccpa compliance

188
00:07:46,800 --> 00:07:48,479
well the stakeholders of data

189
00:07:48,479 --> 00:07:50,960
classification is listed on this page

190
00:07:50,960 --> 00:07:53,199
so compliance typically needs to keep

191
00:07:53,199 --> 00:07:55,280
inventory of the data and define the

192
00:07:55,280 --> 00:07:57,280
data control policy

193
00:07:57,280 --> 00:07:59,120
well the infosec team

194
00:07:59,120 --> 00:08:01,440
implements controls and ensures data is

195
00:08:01,440 --> 00:08:03,840
properly handled and monitored

196
00:08:03,840 --> 00:08:05,680
engineering team of course

197
00:08:05,680 --> 00:08:08,160
have to follow the instructions to use

198
00:08:08,160 --> 00:08:10,000
and process the data

199
00:08:10,000 --> 00:08:11,759
making sure that sensitive data is

200
00:08:11,759 --> 00:08:14,479
scrubbed from the log files and improper

201
00:08:14,479 --> 00:08:17,520
encryption is used

202
00:08:17,680 --> 00:08:19,599
management management have to care as

203
00:08:19,599 --> 00:08:21,759
well especially the department which

204
00:08:21,759 --> 00:08:23,440
oversee the risk

205
00:08:23,440 --> 00:08:25,120
they should have a risky

206
00:08:25,120 --> 00:08:27,520
profile based on the data inventory and

207
00:08:27,520 --> 00:08:30,000
constantly evaluate the data

208
00:08:30,000 --> 00:08:32,320
related risk

209
00:08:32,320 --> 00:08:34,159
and then external

210
00:08:34,159 --> 00:08:36,320
service providers such as

211
00:08:36,320 --> 00:08:39,360
auditing firms or pentax vendors are

212
00:08:39,360 --> 00:08:41,760
also interested in how company handles

213
00:08:41,760 --> 00:08:43,679
sensitive information

214
00:08:43,679 --> 00:08:45,760
they will totally appreciate if there's

215
00:08:45,760 --> 00:08:48,080
an existing report or a dashboard

216
00:08:48,080 --> 00:08:50,320
showing them where the data is where the

217
00:08:50,320 --> 00:08:51,360
data is

218
00:08:51,360 --> 00:08:54,720
and how data is processed

219
00:08:54,720 --> 00:08:56,959
i find one of the report quite helpful

220
00:08:56,959 --> 00:08:58,320
from iapp

221
00:08:58,320 --> 00:09:00,160
and then i've included in the reference

222
00:09:00,160 --> 00:09:03,560
of this talk

223
00:09:10,399 --> 00:09:12,480
so typically data classification needs

224
00:09:12,480 --> 00:09:15,200
to be done in the following three steps

225
00:09:15,200 --> 00:09:17,120
first you want to make sure you scan the

226
00:09:17,120 --> 00:09:20,320
data storage and do an initial discovery

227
00:09:20,320 --> 00:09:22,720
to identify what you have stored

228
00:09:22,720 --> 00:09:24,959
the data inventory should be generated

229
00:09:24,959 --> 00:09:27,839
after this step

230
00:09:28,080 --> 00:09:30,320
second once you have the inventory you

231
00:09:30,320 --> 00:09:31,839
will need to perform analysis on the

232
00:09:31,839 --> 00:09:33,440
data you have

233
00:09:33,440 --> 00:09:35,440
and label the data based on the

234
00:09:35,440 --> 00:09:39,120
predefined classification rules

235
00:09:39,120 --> 00:09:40,880
third there should be an ongoing

236
00:09:40,880 --> 00:09:42,959
monitoring mechanism to keep track of

237
00:09:42,959 --> 00:09:44,480
the changes

238
00:09:44,480 --> 00:09:47,279
to understand what's newly added

239
00:09:47,279 --> 00:09:51,839
and what's abnormal behaviors

240
00:09:52,240 --> 00:09:54,959
since there's always changes being made

241
00:09:54,959 --> 00:09:57,680
you need to keep keep iterating steps

242
00:09:57,680 --> 00:09:59,279
two and three

243
00:09:59,279 --> 00:10:03,560
just to keep up with the latest changes

244
00:10:10,880 --> 00:10:12,240
there are two ways of doing data

245
00:10:12,240 --> 00:10:13,920
classification today

246
00:10:13,920 --> 00:10:16,160
the first way is the human approach

247
00:10:16,160 --> 00:10:19,040
meaning you have internal reviewers

248
00:10:19,040 --> 00:10:20,480
which is human

249
00:10:20,480 --> 00:10:22,480
given access to the data sets and

250
00:10:22,480 --> 00:10:24,560
manually review the data source to

251
00:10:24,560 --> 00:10:27,279
discover and classify data fields

252
00:10:27,279 --> 00:10:29,120
and then the second you have

253
00:10:29,120 --> 00:10:30,880
this automated approach which allows

254
00:10:30,880 --> 00:10:32,959
third-party software or any software

255
00:10:32,959 --> 00:10:34,959
that scans the data set

256
00:10:34,959 --> 00:10:37,680
for the similar tasks

257
00:10:37,680 --> 00:10:39,440
there's some benefit and there's some

258
00:10:39,440 --> 00:10:41,360
drawbacks to each solution i'm going to

259
00:10:41,360 --> 00:10:45,640
touch down that in next slides

260
00:10:51,680 --> 00:10:54,640
so human based data classification

261
00:10:54,640 --> 00:10:56,560
is challenging especially for enterprise

262
00:10:56,560 --> 00:10:59,440
databases log storages

263
00:10:59,440 --> 00:11:02,480
where the the dev life cycle is so short

264
00:11:02,480 --> 00:11:05,440
and the data is constantly changing

265
00:11:05,440 --> 00:11:08,399
or given the significantly num

266
00:11:08,399 --> 00:11:10,320
significant number of data fields to

267
00:11:10,320 --> 00:11:12,880
monitor the ability to keep track of

268
00:11:12,880 --> 00:11:15,680
uh and correlate multiple data fields

269
00:11:15,680 --> 00:11:18,320
to human is very limited

270
00:11:18,320 --> 00:11:21,200
for policy makers such as grc team

271
00:11:21,200 --> 00:11:23,440
enforcing data related policy to make

272
00:11:23,440 --> 00:11:25,760
sure data is compliant with data control

273
00:11:25,760 --> 00:11:29,360
policy retention policy etc can be very

274
00:11:29,360 --> 00:11:31,519
challenging if there's no automated tool

275
00:11:31,519 --> 00:11:33,920
in place

276
00:11:34,160 --> 00:11:36,079
under this method

277
00:11:36,079 --> 00:11:38,560
employee or contractor will be given

278
00:11:38,560 --> 00:11:40,800
access to the sensitive data set to

279
00:11:40,800 --> 00:11:42,880
perform the task

280
00:11:42,880 --> 00:11:45,920
which will expose the database to

281
00:11:45,920 --> 00:11:49,200
actual risk such as the insider threats

282
00:11:49,200 --> 00:11:52,639
or client-side attacks

283
00:11:54,160 --> 00:11:56,800
what this may happen when access is

284
00:11:56,800 --> 00:11:58,800
compromised if the user credential is

285
00:11:58,800 --> 00:12:00,399
stolen

286
00:12:00,399 --> 00:12:03,200
i think the recent twitter examples

287
00:12:03,200 --> 00:12:05,279
think about the most recent twitter hack

288
00:12:05,279 --> 00:12:08,839
as one of the examples

289
00:12:14,800 --> 00:12:16,560
the alternative way is to automate the

290
00:12:16,560 --> 00:12:19,519
data classification process so today is

291
00:12:19,519 --> 00:12:20,880
typically done using third-party

292
00:12:20,880 --> 00:12:23,680
software it is capable of handling large

293
00:12:23,680 --> 00:12:26,480
data sets but may bring extra risk as

294
00:12:26,480 --> 00:12:28,240
well

295
00:12:28,240 --> 00:12:30,240
so in this case the company has to allow

296
00:12:30,240 --> 00:12:32,240
therapy applications to access

297
00:12:32,240 --> 00:12:35,279
production database

298
00:12:35,920 --> 00:12:37,760
this may risk data leaving the trusted

299
00:12:37,760 --> 00:12:39,839
boundaries the third party software

300
00:12:39,839 --> 00:12:42,720
might need maintenance update over time

301
00:12:42,720 --> 00:12:45,600
and resulting network traffic to an

302
00:12:45,600 --> 00:12:47,200
external server

303
00:12:47,200 --> 00:12:49,839
and then it needs maintenance

304
00:12:49,839 --> 00:12:51,200
and the maintenance will probably

305
00:12:51,200 --> 00:12:52,399
carried by

306
00:12:52,399 --> 00:12:54,720
i.t or infrastructure team which

307
00:12:54,720 --> 00:13:00,040
requires cross-functional work as well

308
00:13:09,519 --> 00:13:11,680
so with all that being said an ideal

309
00:13:11,680 --> 00:13:13,920
solution should automatically perform

310
00:13:13,920 --> 00:13:16,000
the data classification

311
00:13:16,000 --> 00:13:19,279
but compared to deploying a pre-compiled

312
00:13:19,279 --> 00:13:21,920
third-party software in my data center

313
00:13:21,920 --> 00:13:23,839
i prefer a solution

314
00:13:23,839 --> 00:13:26,639
that's designed with the risk in mind

315
00:13:26,639 --> 00:13:28,880
so i wrote the sentences here

316
00:13:28,880 --> 00:13:31,839
imagine you want a summary of a book

317
00:13:31,839 --> 00:13:34,240
or find out the author had a specific

318
00:13:34,240 --> 00:13:37,519
idea about some topics

319
00:13:37,519 --> 00:13:39,120
you would

320
00:13:39,120 --> 00:13:40,480
either read

321
00:13:40,480 --> 00:13:42,240
read the book by yourself

322
00:13:42,240 --> 00:13:44,240
or have someone read it for you and

323
00:13:44,240 --> 00:13:46,320
write a summary

324
00:13:46,320 --> 00:13:48,720
here we're facing three challenges

325
00:13:48,720 --> 00:13:49,680
first

326
00:13:49,680 --> 00:13:51,920
the book is too long for you to read but

327
00:13:51,920 --> 00:13:54,800
you need the summary quickly

328
00:13:54,800 --> 00:13:56,480
second

329
00:13:56,480 --> 00:13:58,079
you're not

330
00:13:58,079 --> 00:14:00,000
trust anyone

331
00:14:00,000 --> 00:14:02,079
you cannot trust anyone else to read the

332
00:14:02,079 --> 00:14:03,279
book for you

333
00:14:03,279 --> 00:14:05,760
because they may leak the content

334
00:14:05,760 --> 00:14:07,920
and third the new version is coming out

335
00:14:07,920 --> 00:14:10,880
fairly fast and then you want to know

336
00:14:10,880 --> 00:14:13,040
the content asap

337
00:14:13,040 --> 00:14:14,839
and you want to know the changes as

338
00:14:14,839 --> 00:14:17,600
well with all these challenges in mind

339
00:14:17,600 --> 00:14:19,839
you may want just one author to

340
00:14:19,839 --> 00:14:21,680
to just include the summary

341
00:14:21,680 --> 00:14:24,560
at the very beginning of the book

342
00:14:24,560 --> 00:14:27,760
but since he has not done that yet

343
00:14:27,760 --> 00:14:29,760
you need to have some type of magic to

344
00:14:29,760 --> 00:14:32,000
look into the book and generate the

345
00:14:32,000 --> 00:14:34,079
summary for you

346
00:14:34,079 --> 00:14:36,880
you might just wondering hold on what

347
00:14:36,880 --> 00:14:39,839
what magic was that

348
00:14:39,839 --> 00:14:41,760
what such solution does not exist in the

349
00:14:41,760 --> 00:14:44,639
real world it could be made possible

350
00:14:44,639 --> 00:14:46,079
with the context

351
00:14:46,079 --> 00:14:49,600
within the context of cloud computing

352
00:14:49,600 --> 00:14:53,320
let me explain why

353
00:14:58,800 --> 00:15:02,880
here's the overview of my solution

354
00:15:02,880 --> 00:15:05,519
the core of this solution is to have

355
00:15:05,519 --> 00:15:07,760
serverless functions in this case a

356
00:15:07,760 --> 00:15:10,240
lambda function of amazon cloud

357
00:15:10,240 --> 00:15:12,959
to be the actual worker that classified

358
00:15:12,959 --> 00:15:14,639
the data automatically

359
00:15:14,639 --> 00:15:17,199
perform the analysis and generate the

360
00:15:17,199 --> 00:15:19,279
alerts and reports

361
00:15:19,279 --> 00:15:20,720
within the target

362
00:15:20,720 --> 00:15:24,560
within the targeted aws account

363
00:15:25,600 --> 00:15:28,240
this serverless function

364
00:15:28,240 --> 00:15:30,720
is a piece of code that runs within the

365
00:15:30,720 --> 00:15:33,040
serverless runtime that's dedicated to

366
00:15:33,040 --> 00:15:34,240
this account

367
00:15:34,240 --> 00:15:36,320
so technically it's part of the

368
00:15:36,320 --> 00:15:37,600
infrastructure

369
00:15:37,600 --> 00:15:39,920
or if you go to my previous pages

370
00:15:39,920 --> 00:15:43,759
example it's part of the book

371
00:15:45,360 --> 00:15:46,959
in step 1

372
00:15:46,959 --> 00:15:48,959
we deploy the lambda function under this

373
00:15:48,959 --> 00:15:50,720
aws account

374
00:15:50,720 --> 00:15:53,199
and granted the proper

375
00:15:53,199 --> 00:15:55,120
privilege to read the data storage

376
00:15:55,120 --> 00:15:57,440
services

377
00:15:57,440 --> 00:15:59,600
at this point the lambda function is

378
00:15:59,600 --> 00:16:01,199
restricted

379
00:16:01,199 --> 00:16:03,360
within the account and will send no

380
00:16:03,360 --> 00:16:05,680
external traffic or have

381
00:16:05,680 --> 00:16:06,839
external

382
00:16:06,839 --> 00:16:09,279
dependencies in other words

383
00:16:09,279 --> 00:16:11,680
the lambda worker is restricted within

384
00:16:11,680 --> 00:16:12,880
my account

385
00:16:12,880 --> 00:16:14,240
the

386
00:16:14,240 --> 00:16:16,560
lambda function can be triggered

387
00:16:16,560 --> 00:16:18,079
by a crown job

388
00:16:18,079 --> 00:16:20,639
or a system event

389
00:16:20,639 --> 00:16:22,800
when when new changes make to the

390
00:16:22,800 --> 00:16:24,560
database

391
00:16:24,560 --> 00:16:28,480
tables or data storage services

392
00:16:28,480 --> 00:16:30,000
the lambda function can simply be

393
00:16:30,000 --> 00:16:30,959
triggered

394
00:16:30,959 --> 00:16:32,639
i will touch on that later when i talk

395
00:16:32,639 --> 00:16:35,279
about lambda functions

396
00:16:35,279 --> 00:16:38,240
in step 2 once triggered lambda function

397
00:16:38,240 --> 00:16:39,839
will try to read from data storage

398
00:16:39,839 --> 00:16:42,720
services to get the data set

399
00:16:42,720 --> 00:16:44,160
in this step

400
00:16:44,160 --> 00:16:46,399
the function reads about 20 record from

401
00:16:46,399 --> 00:16:47,680
each table

402
00:16:47,680 --> 00:16:50,079
and process them based on predefined

403
00:16:50,079 --> 00:16:52,560
matching rules

404
00:16:52,560 --> 00:16:54,800
in my mind the rules can be loaded along

405
00:16:54,800 --> 00:16:57,120
with the function to the lambda runtime

406
00:16:57,120 --> 00:16:59,440
as json or xml

407
00:16:59,440 --> 00:17:02,240
but i'm open to other ideas

408
00:17:02,240 --> 00:17:04,000
feel free to let me know if you know

409
00:17:04,000 --> 00:17:06,959
other better options

410
00:17:06,959 --> 00:17:08,880
in my demo the match is done through

411
00:17:08,880 --> 00:17:11,439
regular expression but it can be done in

412
00:17:11,439 --> 00:17:13,760
other advanced methods such as natural

413
00:17:13,760 --> 00:17:16,079
language processing

414
00:17:16,079 --> 00:17:18,400
which i see as a potential

415
00:17:18,400 --> 00:17:21,919
updates in the future

416
00:17:22,799 --> 00:17:25,199
in step 3 the function finds some

417
00:17:25,199 --> 00:17:27,359
matches it will try

418
00:17:27,359 --> 00:17:30,640
to send out notification to stakeholders

419
00:17:30,640 --> 00:17:33,280
i choose aws sns

420
00:17:33,280 --> 00:17:35,919
to link to my page duty instance and i

421
00:17:35,919 --> 00:17:40,000
can get paid notification from there

422
00:17:41,760 --> 00:17:43,120
there are some benefits come with the

423
00:17:43,120 --> 00:17:45,200
solution

424
00:17:45,200 --> 00:17:46,960
first the solution is based on lambda

425
00:17:46,960 --> 00:17:49,200
function

426
00:17:49,200 --> 00:17:50,320
and then it's

427
00:17:50,320 --> 00:17:51,600
it's replacing

428
00:17:51,600 --> 00:17:54,080
the manual data classification mechanism

429
00:17:54,080 --> 00:17:56,240
with automations

430
00:17:56,240 --> 00:17:58,320
with proper learning and tuning to the

431
00:17:58,320 --> 00:18:00,400
rules and signatures

432
00:18:00,400 --> 00:18:04,000
this task can be done fairly fast

433
00:18:04,000 --> 00:18:06,080
this is very helpful to a lot of data

434
00:18:06,080 --> 00:18:07,919
sets with a lot of changes

435
00:18:07,919 --> 00:18:10,000
which is quite typical in today's short

436
00:18:10,000 --> 00:18:13,880
development life cycles

437
00:18:14,080 --> 00:18:15,600
this solution is also a more like

438
00:18:15,600 --> 00:18:18,080
elegant solution compared to third-party

439
00:18:18,080 --> 00:18:19,520
software

440
00:18:19,520 --> 00:18:21,600
it doesn't introduce any pre-compiled

441
00:18:21,600 --> 00:18:24,000
code into my production environment to

442
00:18:24,000 --> 00:18:26,400
read my database

443
00:18:26,400 --> 00:18:27,600
and i can

444
00:18:27,600 --> 00:18:29,919
also leverage the access control from

445
00:18:29,919 --> 00:18:34,880
aws iem to restrict my lambda function

446
00:18:34,880 --> 00:18:36,880
besides since the security team can

447
00:18:36,880 --> 00:18:39,360
inspect the code of the lambda function

448
00:18:39,360 --> 00:18:41,520
we can clearly understand what this

449
00:18:41,520 --> 00:18:43,360
piece of code is doing to my cloud

450
00:18:43,360 --> 00:18:45,280
environment

451
00:18:45,280 --> 00:18:47,600
with third-party software i could hardly

452
00:18:47,600 --> 00:18:50,918
see that happening

453
00:19:02,000 --> 00:19:03,280
we're talking about some technical

454
00:19:03,280 --> 00:19:05,678
details

455
00:19:06,080 --> 00:19:09,200
this slide covers the aws basics that

456
00:19:09,200 --> 00:19:11,840
will be used in this solution

457
00:19:11,840 --> 00:19:15,039
the first concept is the aws account

458
00:19:15,039 --> 00:19:17,280
on this chart the account is basically

459
00:19:17,280 --> 00:19:18,559
the edge

460
00:19:18,559 --> 00:19:22,399
of the entire architecture overview

461
00:19:22,799 --> 00:19:25,200
at aws account is a container for your

462
00:19:25,200 --> 00:19:28,320
aws resources

463
00:19:28,880 --> 00:19:30,960
you can create manage adobe's resources

464
00:19:30,960 --> 00:19:34,160
in aws account and aws account provides

465
00:19:34,160 --> 00:19:35,039
the

466
00:19:35,039 --> 00:19:37,760
admin capabilities of accessing and

467
00:19:37,760 --> 00:19:40,240
billing and so on so forth it's a it's a

468
00:19:40,240 --> 00:19:42,640
logical layer that organizes all your

469
00:19:42,640 --> 00:19:45,360
resources and if if you want you can tie

470
00:19:45,360 --> 00:19:47,280
up all the resources to this account and

471
00:19:47,280 --> 00:19:51,639
restrict external access

472
00:19:52,480 --> 00:19:55,360
next concept is aws lambda

473
00:19:55,360 --> 00:19:58,000
aws lambda is a serverless is a

474
00:19:58,000 --> 00:20:00,240
computing service that lets you run code

475
00:20:00,240 --> 00:20:04,000
without provisioning or managing servers

476
00:20:04,000 --> 00:20:05,919
with aws lambda you can run different

477
00:20:05,919 --> 00:20:07,280
types of code

478
00:20:07,280 --> 00:20:09,600
with zero administration of any

479
00:20:09,600 --> 00:20:12,480
background servers

480
00:20:12,480 --> 00:20:14,559
all you need is to supply the code in

481
00:20:14,559 --> 00:20:16,320
one of the language that

482
00:20:16,320 --> 00:20:21,520
is supported by the aws lambda runtime

483
00:20:21,520 --> 00:20:23,280
this is the core service that we are

484
00:20:23,280 --> 00:20:24,720
leveraging

485
00:20:24,720 --> 00:20:27,520
in my solution

486
00:20:28,880 --> 00:20:31,679
next thing is called cloudwatch

487
00:20:31,679 --> 00:20:35,520
cloudwatch monitors the aws services

488
00:20:35,520 --> 00:20:37,760
and resources in real time

489
00:20:37,760 --> 00:20:39,760
you can use cloud service to collect and

490
00:20:39,760 --> 00:20:41,280
track metrics

491
00:20:41,280 --> 00:20:42,080
and

492
00:20:42,080 --> 00:20:44,400
send some events

493
00:20:44,400 --> 00:20:47,360
in my case i use cloudwatch to trigger a

494
00:20:47,360 --> 00:20:49,120
chrome job to run

495
00:20:49,120 --> 00:20:51,280
the lambda function weekly

496
00:20:51,280 --> 00:20:53,840
or i could use cloudwatch event to

497
00:20:53,840 --> 00:20:56,240
monitor database audit logs and trigger

498
00:20:56,240 --> 00:21:00,360
the event driven lambda function

499
00:21:03,280 --> 00:21:05,039
i would also wanted to point out the db

500
00:21:05,039 --> 00:21:06,960
audit logs

501
00:21:06,960 --> 00:21:07,840
if

502
00:21:07,840 --> 00:21:10,000
it's by default it's not enabled but

503
00:21:10,000 --> 00:21:12,799
once it's enabled mysql audit logs

504
00:21:12,799 --> 00:21:14,880
provide the event when table schema

505
00:21:14,880 --> 00:21:17,120
changes

506
00:21:17,120 --> 00:21:20,080
new such as you know new table creation

507
00:21:20,080 --> 00:21:23,840
dropping table rename table

508
00:21:24,080 --> 00:21:26,480
the way it works is just looking to

509
00:21:26,480 --> 00:21:29,039
the sql statement such as

510
00:21:29,039 --> 00:21:31,840
create outer drop or rename

511
00:21:31,840 --> 00:21:33,919
an operation that happens to certain

512
00:21:33,919 --> 00:21:36,000
tables

513
00:21:36,000 --> 00:21:38,320
monitoring the db table provide the

514
00:21:38,320 --> 00:21:40,720
signal to decide whether to trigger the

515
00:21:40,720 --> 00:21:45,559
lambda function for new data discovery

516
00:21:48,080 --> 00:21:51,120
last is s3 s3 is the storage services

517
00:21:51,120 --> 00:21:53,440
provided by aws

518
00:21:53,440 --> 00:21:56,000
s3 monitoring is a bit challenging since

519
00:21:56,000 --> 00:21:57,440
s3 does not

520
00:21:57,440 --> 00:21:59,120
follow

521
00:21:59,120 --> 00:22:02,000
schemas like databases

522
00:22:02,000 --> 00:22:04,880
but s3 events can also be used as to

523
00:22:04,880 --> 00:22:06,559
trigger lambda functions

524
00:22:06,559 --> 00:22:09,440
however the match of s3 content may not

525
00:22:09,440 --> 00:22:11,200
be as comprehensive

526
00:22:11,200 --> 00:22:13,360
since we cannot count out any schema

527
00:22:13,360 --> 00:22:14,559
changes

528
00:22:14,559 --> 00:22:17,840
in sampling s3 buckets

529
00:22:17,840 --> 00:22:19,600
again another

530
00:22:19,600 --> 00:22:23,959
future items to work on

531
00:22:34,320 --> 00:22:35,600
from the chart

532
00:22:35,600 --> 00:22:37,120
the first thing i'm wondering like you

533
00:22:37,120 --> 00:22:38,880
might wondering like how

534
00:22:38,880 --> 00:22:42,159
lambda function is getting triggered

535
00:22:42,159 --> 00:22:44,320
well there are two way of treating

536
00:22:44,320 --> 00:22:46,960
the lambda function in my case

537
00:22:46,960 --> 00:22:48,960
the first thing is time based with cloud

538
00:22:48,960 --> 00:22:51,039
watch rules which means

539
00:22:51,039 --> 00:22:53,039
under cloud watch there's a crown job

540
00:22:53,039 --> 00:22:54,960
that runs the lambda function on a

541
00:22:54,960 --> 00:22:56,880
frequent basis

542
00:22:56,880 --> 00:22:59,679
such as like such as a weekly job you

543
00:22:59,679 --> 00:23:02,960
can take weekly scans over your data set

544
00:23:02,960 --> 00:23:05,600
or you can have event

545
00:23:05,600 --> 00:23:07,360
driven

546
00:23:07,360 --> 00:23:09,840
or event based

547
00:23:09,840 --> 00:23:10,799
triggers

548
00:23:10,799 --> 00:23:12,960
so when something happens to db audit

549
00:23:12,960 --> 00:23:15,840
logs such such as detection of an alter

550
00:23:15,840 --> 00:23:18,000
table happens

551
00:23:18,000 --> 00:23:20,960
you then trigger a lambda function based

552
00:23:20,960 --> 00:23:23,840
on that event

553
00:23:33,600 --> 00:23:35,679
so for lambda function the way it works

554
00:23:35,679 --> 00:23:36,960
is that

555
00:23:36,960 --> 00:23:39,440
it try to scan different data sets and

556
00:23:39,440 --> 00:23:41,520
looking for sensitive information from

557
00:23:41,520 --> 00:23:43,279
there

558
00:23:43,279 --> 00:23:44,799
for databases

559
00:23:44,799 --> 00:23:46,720
it reads and monitors the db schema

560
00:23:46,720 --> 00:23:49,200
changes and pulls some data record for

561
00:23:49,200 --> 00:23:50,960
analysis

562
00:23:50,960 --> 00:23:52,559
schema is a good

563
00:23:52,559 --> 00:23:56,400
good source of truth because it contains

564
00:23:56,400 --> 00:23:59,440
contains metadata about

565
00:23:59,440 --> 00:24:00,880
the actual data that's stored in the

566
00:24:00,880 --> 00:24:02,880
database

567
00:24:02,880 --> 00:24:05,440
but for x-ray the function parses and

568
00:24:05,440 --> 00:24:07,840
analyze the s3 metadata for useful

569
00:24:07,840 --> 00:24:10,559
information and sample the s3 files for

570
00:24:10,559 --> 00:24:13,279
sensitive data

571
00:24:13,279 --> 00:24:15,760
s3 is a little bit different because it

572
00:24:15,760 --> 00:24:18,159
is unlike the database which has a

573
00:24:18,159 --> 00:24:21,520
schema file s3 only have the metadata

574
00:24:21,520 --> 00:24:24,320
which is defined by the s3 owner which

575
00:24:24,320 --> 00:24:27,760
when the bucket is created so it's uh

576
00:24:27,760 --> 00:24:30,000
it may not sampling the s3 might not be

577
00:24:30,000 --> 00:24:34,159
as effective as scanning the database

578
00:24:42,559 --> 00:24:46,000
so once you have the scan result

579
00:24:46,000 --> 00:24:48,159
or you have the data inventory or you

580
00:24:48,159 --> 00:24:50,000
identified any changes

581
00:24:50,000 --> 00:24:52,080
you weren't able to be able to notify

582
00:24:52,080 --> 00:24:54,799
the stakeholders

583
00:24:54,799 --> 00:24:56,640
i picked a few services that could be

584
00:24:56,640 --> 00:25:01,919
used in this situation such as sns

585
00:25:01,919 --> 00:25:03,360
which is the

586
00:25:03,360 --> 00:25:06,559
simple notification service by aws

587
00:25:06,559 --> 00:25:08,080
you can basically

588
00:25:08,080 --> 00:25:09,600
uh

589
00:25:09,600 --> 00:25:12,720
send send out notifications using sms in

590
00:25:12,720 --> 00:25:14,400
my demo i

591
00:25:14,400 --> 00:25:16,720
use sns to send alerts

592
00:25:16,720 --> 00:25:18,640
to pagerduty where i can get

593
00:25:18,640 --> 00:25:20,640
notifications

594
00:25:20,640 --> 00:25:22,320
or you have a sim

595
00:25:22,320 --> 00:25:23,440
or

596
00:25:23,440 --> 00:25:24,880
any

597
00:25:24,880 --> 00:25:27,760
mssp services you can send it to the sim

598
00:25:27,760 --> 00:25:30,400
for integration so that your

599
00:25:30,400 --> 00:25:32,720
security operational people will be able

600
00:25:32,720 --> 00:25:35,039
to understand or see what data is being

601
00:25:35,039 --> 00:25:36,000
changed

602
00:25:36,000 --> 00:25:38,159
or is there any sensitive information

603
00:25:38,159 --> 00:25:40,840
contained in the changed data

604
00:25:40,840 --> 00:25:43,679
sets and also you can have lambda

605
00:25:43,679 --> 00:25:46,640
function to generate you a pdf report

606
00:25:46,640 --> 00:25:48,240
if you wanted the report for

607
00:25:48,240 --> 00:25:51,880
documentation purposes

608
00:26:02,480 --> 00:26:04,960
uh in this car in this talk i originally

609
00:26:04,960 --> 00:26:07,600
planned a demo but uh since like the

610
00:26:07,600 --> 00:26:08,840
talks only

611
00:26:08,840 --> 00:26:12,080
uh 35 minutes i plan not to show the

612
00:26:12,080 --> 00:26:14,880
demo here because i i can fit that into

613
00:26:14,880 --> 00:26:16,400
a short time frame

614
00:26:16,400 --> 00:26:19,440
the plan is to upload the

615
00:26:19,440 --> 00:26:21,600
demo into a website and then i'll

616
00:26:21,600 --> 00:26:23,760
include a link

617
00:26:23,760 --> 00:26:26,159
to the description of this talk and also

618
00:26:26,159 --> 00:26:27,600
i will include

619
00:26:27,600 --> 00:26:29,440
the point of

620
00:26:29,440 --> 00:26:31,760
the proof of concept code in my github

621
00:26:31,760 --> 00:26:34,919
as well

622
00:26:42,320 --> 00:26:43,840
so they're definitely going to be future

623
00:26:43,840 --> 00:26:47,440
works related to this because right now

624
00:26:47,440 --> 00:26:49,279
it's basically a proof of concept and

625
00:26:49,279 --> 00:26:51,600
doesn't have

626
00:26:51,600 --> 00:26:52,640
all the

627
00:26:52,640 --> 00:26:54,640
all the fancy functions in it

628
00:26:54,640 --> 00:26:56,080
the first thing we need is better

629
00:26:56,080 --> 00:26:58,640
triggers so currently the trigger is set

630
00:26:58,640 --> 00:27:00,000
up using

631
00:27:00,000 --> 00:27:01,919
uh s3 events

632
00:27:01,919 --> 00:27:02,799
or

633
00:27:02,799 --> 00:27:04,480
db audit logs

634
00:27:04,480 --> 00:27:06,880
the current aws rds does not have a

635
00:27:06,880 --> 00:27:09,200
native cloud event

636
00:27:09,200 --> 00:27:12,080
for to monitor db schema changes

637
00:27:12,080 --> 00:27:15,039
i would hope that aws edit later on so

638
00:27:15,039 --> 00:27:17,360
that we can have better triggers that's

639
00:27:17,360 --> 00:27:21,440
more direct and easier to set up

640
00:27:21,440 --> 00:27:23,600
the current matching is based on regular

641
00:27:23,600 --> 00:27:27,039
expressions just for this demo purposes

642
00:27:27,039 --> 00:27:28,559
but in my mind

643
00:27:28,559 --> 00:27:29,440
the

644
00:27:29,440 --> 00:27:31,440
analysis is better done with nlp

645
00:27:31,440 --> 00:27:33,120
algorithms

646
00:27:33,120 --> 00:27:35,919
so that's the future improvement items

647
00:27:35,919 --> 00:27:36,799
um

648
00:27:36,799 --> 00:27:38,799
again i'm not an expert on natural

649
00:27:38,799 --> 00:27:40,399
language processing

650
00:27:40,399 --> 00:27:42,960
maybe i can borrow experience uh maybe i

651
00:27:42,960 --> 00:27:45,760
can learn a few things or get some help

652
00:27:45,760 --> 00:27:48,640
from the community

653
00:27:48,640 --> 00:27:50,640
and right now i've tested it on

654
00:27:50,640 --> 00:27:53,679
aws um but i would hope this would be a

655
00:27:53,679 --> 00:27:55,840
universal solution to other cloud

656
00:27:55,840 --> 00:27:57,440
platform as well

657
00:27:57,440 --> 00:27:59,919
i've not tried that on

658
00:27:59,919 --> 00:28:01,600
google cloud or

659
00:28:01,600 --> 00:28:02,720
azure

660
00:28:02,720 --> 00:28:05,039
maybe i were able to get some resource

661
00:28:05,039 --> 00:28:06,640
on that and then

662
00:28:06,640 --> 00:28:09,120
try some samples on those cloud service

663
00:28:09,120 --> 00:28:12,000
platform as well

664
00:28:14,640 --> 00:28:17,440
i've included two pages of of references

665
00:28:17,440 --> 00:28:19,679
which i used from this talk

666
00:28:19,679 --> 00:28:21,600
um there's definitely

667
00:28:21,600 --> 00:28:23,600
more things you're able to

668
00:28:23,600 --> 00:28:26,159
find online related to aws lambda

669
00:28:26,159 --> 00:28:27,360
because these things definitely get more

670
00:28:27,360 --> 00:28:28,399
popular

671
00:28:28,399 --> 00:28:31,120
uh in terms of computing

672
00:28:31,120 --> 00:28:31,919
um

673
00:28:31,919 --> 00:28:33,200
and i think

674
00:28:33,200 --> 00:28:35,200
that definitely

675
00:28:35,200 --> 00:28:36,960
there's a lot of interesting topic you

676
00:28:36,960 --> 00:28:38,960
could consider with these cloud service

677
00:28:38,960 --> 00:28:40,080
providers

678
00:28:40,080 --> 00:28:42,320
and then it's the cloud's definitely the

679
00:28:42,320 --> 00:28:45,120
next security

680
00:28:45,120 --> 00:28:48,879
area that people have to focus on

681
00:28:53,840 --> 00:28:57,520
well that wraps up my conversation

682
00:28:57,520 --> 00:28:59,760
but you guys and i

683
00:28:59,760 --> 00:29:02,000
this talk barely scratches services of

684
00:29:02,000 --> 00:29:04,159
data classification problem

685
00:29:04,159 --> 00:29:05,760
i'm hoping to provide a different

686
00:29:05,760 --> 00:29:09,039
approach to this known problem

687
00:29:09,039 --> 00:29:11,440
but within the different contexts of

688
00:29:11,440 --> 00:29:12,880
cloud computing

689
00:29:12,880 --> 00:29:15,760
i'm open to any questions

690
00:29:15,760 --> 00:29:18,720
and any further discussions feel free to

691
00:29:18,720 --> 00:29:21,760
reach out to me after this session

692
00:29:21,760 --> 00:29:24,159
i truly appreciate os for offering me

693
00:29:24,159 --> 00:29:26,480
this opportunity to share my idea

694
00:29:26,480 --> 00:29:28,159
with the community

695
00:29:28,159 --> 00:29:30,080
special thanks to the organizers of

696
00:29:30,080 --> 00:29:31,840
appsec 2020

697
00:29:31,840 --> 00:29:34,880
to make this virtual conference happen

698
00:29:34,880 --> 00:29:38,720
especially in this difficult time

699
00:29:38,720 --> 00:29:41,279
and i'd like to thank you all and let's

700
00:29:41,279 --> 00:29:43,600
keep in touch hope you all have a safe

701
00:29:43,600 --> 00:29:45,039
and healthy

702
00:29:45,039 --> 00:29:46,399
in the rest

703
00:29:46,399 --> 00:29:49,719
of 2020.

