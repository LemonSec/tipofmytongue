1
00:00:00,480 --> 00:00:03,120
welcome to this talk on machine learning

2
00:00:03,120 --> 00:00:05,359
security specifically

3
00:00:05,359 --> 00:00:08,160
going deeper with adversarial samples

4
00:00:08,160 --> 00:00:10,559
i want to start by saying thank you to

5
00:00:10,559 --> 00:00:12,719
the people that helped me to

6
00:00:12,719 --> 00:00:14,719
get this knowledge and

7
00:00:14,719 --> 00:00:18,160
improve my life kunal alireza buffet

8
00:00:18,160 --> 00:00:19,600
laurent jay

9
00:00:19,600 --> 00:00:23,199
you guys are awesome thank you

10
00:00:23,199 --> 00:00:24,400
so this is what we're going to cover

11
00:00:24,400 --> 00:00:26,320
today

12
00:00:26,320 --> 00:00:27,519
a lot of people been talking about

13
00:00:27,519 --> 00:00:29,760
adversarial samples

14
00:00:29,760 --> 00:00:31,760
in relation to deep learning

15
00:00:31,760 --> 00:00:33,120
today we're going to actually show you

16
00:00:33,120 --> 00:00:35,600
that it's a lot more broad

17
00:00:35,600 --> 00:00:38,879
than deep learning it actually affects

18
00:00:38,879 --> 00:00:41,680
almost every single model out there

19
00:00:41,680 --> 00:00:44,559
if not all of them

20
00:00:44,559 --> 00:00:48,719
so what are adversarial samples well

21
00:00:48,719 --> 00:00:50,719
think of a machine learning model that

22
00:00:50,719 --> 00:00:53,600
takes images as input

23
00:00:53,600 --> 00:00:56,239
and at on its output it tells you what

24
00:00:56,239 --> 00:00:58,800
it thinks the image is

25
00:00:58,800 --> 00:00:59,760
of

26
00:00:59,760 --> 00:01:01,199
and it gives a certain percentage

27
00:01:01,199 --> 00:01:02,480
confidence

28
00:01:02,480 --> 00:01:04,400
so on the left we have a picture of a

29
00:01:04,400 --> 00:01:05,600
cat

30
00:01:05,600 --> 00:01:08,479
and when we run it through the model

31
00:01:08,479 --> 00:01:09,680
if we don't

32
00:01:09,680 --> 00:01:11,119
mess with the image

33
00:01:11,119 --> 00:01:13,439
it's going to produce an output like

34
00:01:13,439 --> 00:01:15,040
tabby cat

35
00:01:15,040 --> 00:01:16,560
and it's going to give a percentage

36
00:01:16,560 --> 00:01:18,720
that's pretty confident

37
00:01:18,720 --> 00:01:22,640
now if we embed

38
00:01:22,799 --> 00:01:26,479
an adversarial perturbation and that's

39
00:01:26,479 --> 00:01:29,040
humanly indiscernible if you look at the

40
00:01:29,040 --> 00:01:31,600
right image that image has

41
00:01:31,600 --> 00:01:32,400
the

42
00:01:32,400 --> 00:01:34,400
perturbation in it

43
00:01:34,400 --> 00:01:37,280
when you run that through the model

44
00:01:37,280 --> 00:01:39,600
it gets 99

45
00:01:39,600 --> 00:01:44,158
confident that the pictures of guacamole

46
00:01:44,799 --> 00:01:48,000
so what this talks about today is

47
00:01:48,000 --> 00:01:50,320
why that occurs and

48
00:01:50,320 --> 00:01:52,240
in different models so

49
00:01:52,240 --> 00:01:54,159
let's take a look at this picture here

50
00:01:54,159 --> 00:01:56,880
now this is a very simplified picture of

51
00:01:56,880 --> 00:01:59,360
a decision boundary in that cat picture

52
00:01:59,360 --> 00:02:02,079
before there could be tens of thousands

53
00:02:02,079 --> 00:02:04,960
if not millions of decision boundaries

54
00:02:04,960 --> 00:02:08,239
because each pixel is an input that

55
00:02:08,239 --> 00:02:11,760
input represents a decision boundary in

56
00:02:11,760 --> 00:02:14,239
one of the dimensional spaces

57
00:02:14,239 --> 00:02:16,239
now when we look at this picture

58
00:02:16,239 --> 00:02:18,000
these c's

59
00:02:18,000 --> 00:02:19,920
are your training data for the machine

60
00:02:19,920 --> 00:02:23,440
learning model and the green area

61
00:02:23,440 --> 00:02:25,040
is the

62
00:02:25,040 --> 00:02:26,239
boundary

63
00:02:26,239 --> 00:02:27,760
for cat

64
00:02:27,760 --> 00:02:30,400
that the machine learning model learned

65
00:02:30,400 --> 00:02:32,480
by looking at all of the different

66
00:02:32,480 --> 00:02:34,640
images of cats that you gave it you know

67
00:02:34,640 --> 00:02:36,400
some of them are red cats some of them

68
00:02:36,400 --> 00:02:38,239
are black cats some of them are white

69
00:02:38,239 --> 00:02:40,400
cats some are orange and gray

70
00:02:40,400 --> 00:02:41,920
and they're going to what the machine

71
00:02:41,920 --> 00:02:44,640
learning model tries to do is you tell

72
00:02:44,640 --> 00:02:46,879
it hey this is a cat

73
00:02:46,879 --> 00:02:48,640
so what it does is it looks at all the

74
00:02:48,640 --> 00:02:50,879
features that you've given it and it

75
00:02:50,879 --> 00:02:54,160
tries to group all these cats together

76
00:02:54,160 --> 00:02:56,319
in this high dimensional space

77
00:02:56,319 --> 00:02:58,000
and then it creates

78
00:02:58,000 --> 00:03:00,879
the boundaries in between the different

79
00:03:00,879 --> 00:03:02,640
classes like here we're just looking at

80
00:03:02,640 --> 00:03:04,400
the boundary between

81
00:03:04,400 --> 00:03:07,120
cat and the picture of guacamole now the

82
00:03:07,120 --> 00:03:09,040
thing is there are many machine learning

83
00:03:09,040 --> 00:03:11,760
models with thousands of classes so you

84
00:03:11,760 --> 00:03:15,519
could have cat dog bird plane boat car

85
00:03:15,519 --> 00:03:17,840
guacamole

86
00:03:17,840 --> 00:03:19,200
thing is we're going to simplify this

87
00:03:19,200 --> 00:03:21,120
picture because we want to understand

88
00:03:21,120 --> 00:03:23,120
things on a fundamental level and what

89
00:03:23,120 --> 00:03:25,840
this red line is

90
00:03:25,840 --> 00:03:28,480
is basically the decision boundary

91
00:03:28,480 --> 00:03:31,440
between what a human would discern

92
00:03:31,440 --> 00:03:33,120
as a cat

93
00:03:33,120 --> 00:03:34,640
or guacamole

94
00:03:34,640 --> 00:03:38,720
and notice this red line crosses over

95
00:03:38,720 --> 00:03:41,760
both the blue and the green areas

96
00:03:41,760 --> 00:03:43,200
meaning that

97
00:03:43,200 --> 00:03:45,040
our machine learning model

98
00:03:45,040 --> 00:03:47,120
didn't learn

99
00:03:47,120 --> 00:03:49,760
the correct decision boundary

100
00:03:49,760 --> 00:03:52,239
for separating these two classes now

101
00:03:52,239 --> 00:03:53,680
it's normal

102
00:03:53,680 --> 00:03:55,680
your training data

103
00:03:55,680 --> 00:03:57,439
is going to

104
00:03:57,439 --> 00:03:59,920
depending on what your model has gonna

105
00:03:59,920 --> 00:04:01,040
learn good

106
00:04:01,040 --> 00:04:03,599
or bad decision boundaries based upon

107
00:04:03,599 --> 00:04:06,560
the training data you give it

108
00:04:06,560 --> 00:04:08,799
now

109
00:04:08,959 --> 00:04:11,200
let me ask you this question

110
00:04:11,200 --> 00:04:12,959
oh and and then the other thing is the

111
00:04:12,959 --> 00:04:15,680
boundaries the boundaries are pretty

112
00:04:15,680 --> 00:04:16,560
harsh

113
00:04:16,560 --> 00:04:19,199
so for example when we train a picture

114
00:04:19,199 --> 00:04:21,519
of a cat and we give it to the model we

115
00:04:21,519 --> 00:04:22,320
say

116
00:04:22,320 --> 00:04:24,560
this is a hundred percent cat

117
00:04:24,560 --> 00:04:27,360
and zero everything else out

118
00:04:27,360 --> 00:04:29,680
and and so what happens is a cat and a

119
00:04:29,680 --> 00:04:32,160
dog might share certain features pointy

120
00:04:32,160 --> 00:04:34,639
ears like a german shepherd

121
00:04:34,639 --> 00:04:36,960
and cats have pointy ears

122
00:04:36,960 --> 00:04:39,600
and they have fur but the thing is we're

123
00:04:39,600 --> 00:04:42,639
telling the model wait a second there's

124
00:04:42,639 --> 00:04:43,600
no

125
00:04:43,600 --> 00:04:46,000
similarity between these things

126
00:04:46,000 --> 00:04:49,520
it's either a cat or it's a dog and you

127
00:04:49,520 --> 00:04:52,000
need to find out what are the features

128
00:04:52,000 --> 00:04:55,040
that are distinctly different

129
00:04:55,040 --> 00:04:57,280
between a cat and a dog and guess what

130
00:04:57,280 --> 00:04:59,120
there's no gray area it's not like we're

131
00:04:59,120 --> 00:05:01,120
going to say well this picture is a

132
00:05:01,120 --> 00:05:04,080
80 a cat and 10 dog

133
00:05:04,080 --> 00:05:06,880
or you know this german shepherd is 90

134
00:05:06,880 --> 00:05:08,960
dog and 10 a cat because you know it's

135
00:05:08,960 --> 00:05:11,280
got the fur and the pointy ears

136
00:05:11,280 --> 00:05:12,880
they're not doing that

137
00:05:12,880 --> 00:05:14,560
they're saying everything has to be a

138
00:05:14,560 --> 00:05:17,199
one or a zero so what that means is

139
00:05:17,199 --> 00:05:19,120
these boundaries

140
00:05:19,120 --> 00:05:21,280
are very sharp

141
00:05:21,280 --> 00:05:23,600
and if you push something across them

142
00:05:23,600 --> 00:05:26,320
they quickly become

143
00:05:26,320 --> 00:05:28,400
high confidence

144
00:05:28,400 --> 00:05:30,639
predictions

145
00:05:30,639 --> 00:05:32,240
so let me ask you this

146
00:05:32,240 --> 00:05:35,280
if you look at this red boundary

147
00:05:35,280 --> 00:05:37,759
what area

148
00:05:37,759 --> 00:05:41,039
is what a human would say is a cat

149
00:05:41,039 --> 00:05:42,800
in relation to the red line

150
00:05:42,800 --> 00:05:45,120
and what area in relation to this red

151
00:05:45,120 --> 00:05:47,199
line is what a human would say is

152
00:05:47,199 --> 00:05:49,840
guacamole

153
00:05:49,840 --> 00:05:52,720
well if you said the area above the line

154
00:05:52,720 --> 00:05:55,520
is going to be categorized as a cat so

155
00:05:55,520 --> 00:05:58,319
this would be pictures of cats

156
00:05:58,319 --> 00:06:00,960
and that a human would see that as cats

157
00:06:00,960 --> 00:06:02,400
even if they're perturbed like that

158
00:06:02,400 --> 00:06:04,240
picture we had in the beginning

159
00:06:04,240 --> 00:06:06,560
they're going to see it as a cat

160
00:06:06,560 --> 00:06:08,800
now the thing is the model though is

161
00:06:08,800 --> 00:06:10,400
going to have these blue areas right so

162
00:06:10,400 --> 00:06:12,560
we're going to have a situation where we

163
00:06:12,560 --> 00:06:14,639
have things that look like cats

164
00:06:14,639 --> 00:06:16,319
but they're going to be categorized as

165
00:06:16,319 --> 00:06:17,759
guacamole

166
00:06:17,759 --> 00:06:18,880
and so

167
00:06:18,880 --> 00:06:21,039
in general we can push a cat to a

168
00:06:21,039 --> 00:06:22,880
blu-ray to be guacamole and push

169
00:06:22,880 --> 00:06:24,639
guacamole into a green area and say it's

170
00:06:24,639 --> 00:06:26,160
cat

171
00:06:26,160 --> 00:06:28,880
and even though it hasn't changed its

172
00:06:28,880 --> 00:06:30,639
overall look

173
00:06:30,639 --> 00:06:32,720
the machine learning model because we

174
00:06:32,720 --> 00:06:34,560
cross the boundary

175
00:06:34,560 --> 00:06:36,880
is going to say

176
00:06:36,880 --> 00:06:40,000
it's the other thing

177
00:06:40,720 --> 00:06:43,600
and that's the general idea

178
00:06:43,600 --> 00:06:46,319
with adversarial samples so what we're

179
00:06:46,319 --> 00:06:47,360
doing is

180
00:06:47,360 --> 00:06:50,160
let's say we have a new input image of a

181
00:06:50,160 --> 00:06:52,479
cat a guacamole

182
00:06:52,479 --> 00:06:55,280
and what we want to do is we want to

183
00:06:55,280 --> 00:06:56,560
do an ad we want to create an

184
00:06:56,560 --> 00:06:58,240
adversarial perturbation on this that's

185
00:06:58,240 --> 00:07:01,759
going to push it into the next class now

186
00:07:01,759 --> 00:07:05,520
because there are millions of dimensions

187
00:07:05,520 --> 00:07:07,759
there's a pretty strong likelihood

188
00:07:07,759 --> 00:07:09,039
that

189
00:07:09,039 --> 00:07:11,360
you pick any arbitrary image

190
00:07:11,360 --> 00:07:13,919
and it's going to be on the border with

191
00:07:13,919 --> 00:07:15,840
whatever target class you want to push

192
00:07:15,840 --> 00:07:17,919
this image to because there's millions

193
00:07:17,919 --> 00:07:18,880
of them

194
00:07:18,880 --> 00:07:21,360
and they go in all kind of crazy places

195
00:07:21,360 --> 00:07:23,039
and locations and so

196
00:07:23,039 --> 00:07:24,080
what you're going to do is you're going

197
00:07:24,080 --> 00:07:25,360
to find

198
00:07:25,360 --> 00:07:27,120
those inputs

199
00:07:27,120 --> 00:07:28,720
that are going to push

200
00:07:28,720 --> 00:07:30,639
the machine learning model to classify

201
00:07:30,639 --> 00:07:32,880
this as your target class so here we're

202
00:07:32,880 --> 00:07:34,479
saying we want cat

203
00:07:34,479 --> 00:07:36,400
to be a picture of a cat to be

204
00:07:36,400 --> 00:07:38,160
categorized as guacamole or we want a

205
00:07:38,160 --> 00:07:39,840
picture of guacamole to be categorized

206
00:07:39,840 --> 00:07:43,360
as clap cat by the motto

207
00:07:43,360 --> 00:07:44,479
and so

208
00:07:44,479 --> 00:07:46,800
that's what we're doing

209
00:07:46,800 --> 00:07:48,160
we're just

210
00:07:48,160 --> 00:07:50,720
pushing it across these boundaries but

211
00:07:50,720 --> 00:07:52,400
we're not pushing it

212
00:07:52,400 --> 00:07:55,680
across the human boundary right this red

213
00:07:55,680 --> 00:07:56,639
line

214
00:07:56,639 --> 00:07:58,720
this red line that's going to separate

215
00:07:58,720 --> 00:08:01,759
what a human is going to discern and

216
00:08:01,759 --> 00:08:04,000
distinguish between

217
00:08:04,000 --> 00:08:06,560
visible features that are what

218
00:08:06,560 --> 00:08:08,080
represents a cat

219
00:08:08,080 --> 00:08:10,800
visible features of what represents

220
00:08:10,800 --> 00:08:14,639
guacamole so we don't want to push the

221
00:08:14,639 --> 00:08:16,560
images across them because then

222
00:08:16,560 --> 00:08:18,319
they're going to physically look like

223
00:08:18,319 --> 00:08:20,479
the other thing that's not what you want

224
00:08:20,479 --> 00:08:22,319
you want something that

225
00:08:22,319 --> 00:08:24,240
looks like the original

226
00:08:24,240 --> 00:08:27,120
but is categorized radically different

227
00:08:27,120 --> 00:08:30,080
than what it should be

228
00:08:31,360 --> 00:08:34,399
so when we look at adversarial samples

229
00:08:34,399 --> 00:08:36,080
we ask well what

230
00:08:36,080 --> 00:08:39,120
what can you attack what types of models

231
00:08:39,120 --> 00:08:40,719
and if you look at this

232
00:08:40,719 --> 00:08:42,559
it's pretty much everything

233
00:08:42,559 --> 00:08:45,279
and this is specifically related to deep

234
00:08:45,279 --> 00:08:46,399
learning

235
00:08:46,399 --> 00:08:48,399
but today we're going to actually talk

236
00:08:48,399 --> 00:08:49,680
about

237
00:08:49,680 --> 00:08:52,000
the other machine learning models

238
00:08:52,000 --> 00:08:54,560
that are also prevalent in your

239
00:08:54,560 --> 00:08:56,800
enterprises and used within your

240
00:08:56,800 --> 00:08:58,399
enterprises to make

241
00:08:58,399 --> 00:09:01,519
decisions and to make predictions

242
00:09:01,519 --> 00:09:02,240
and

243
00:09:02,240 --> 00:09:05,040
they're vulnerable as well it's not just

244
00:09:05,040 --> 00:09:07,360
deep learning

245
00:09:07,360 --> 00:09:09,600
almost every single machine learning

246
00:09:09,600 --> 00:09:11,920
model has this vulnerability in it it's

247
00:09:11,920 --> 00:09:13,440
almost like

248
00:09:13,440 --> 00:09:16,640
basically saying every single

249
00:09:16,640 --> 00:09:18,480
web app that you write

250
00:09:18,480 --> 00:09:21,120
has got a big

251
00:09:21,120 --> 00:09:23,279
parameter tampering or

252
00:09:23,279 --> 00:09:25,200
cross-site scripting vulnerability

253
00:09:25,200 --> 00:09:27,279
inside of it

254
00:09:27,279 --> 00:09:30,160
no matter what you do

255
00:09:30,480 --> 00:09:32,880
so when we look at the causes or the

256
00:09:32,880 --> 00:09:35,680
theories of why adversarial samples

257
00:09:35,680 --> 00:09:37,200
exist

258
00:09:37,200 --> 00:09:40,320
there's kind of four prevalent ideas

259
00:09:40,320 --> 00:09:42,720
there's insufficient data which i'm

260
00:09:42,720 --> 00:09:45,040
going to show you examples of as we move

261
00:09:45,040 --> 00:09:47,519
forward where if you don't have the

262
00:09:47,519 --> 00:09:50,240
right data your model is going to learn

263
00:09:50,240 --> 00:09:53,680
incorrect decision boundaries

264
00:09:53,680 --> 00:09:56,080
we also have non-robust

265
00:09:56,080 --> 00:09:59,600
features so this goes back to basically

266
00:09:59,600 --> 00:10:01,040
how

267
00:10:01,040 --> 00:10:03,120
when you tell the model

268
00:10:03,120 --> 00:10:04,399
that

269
00:10:04,399 --> 00:10:07,200
something is a hundred percent this and

270
00:10:07,200 --> 00:10:09,760
zero percent everything else

271
00:10:09,760 --> 00:10:11,600
and that's all you're doing that's the

272
00:10:11,600 --> 00:10:13,600
what they call the loss function

273
00:10:13,600 --> 00:10:15,519
and you're telling it that

274
00:10:15,519 --> 00:10:18,640
then the model has got to figure out

275
00:10:18,640 --> 00:10:22,959
from these images of cats and guacamoles

276
00:10:22,959 --> 00:10:25,920
what specific feature in these images

277
00:10:25,920 --> 00:10:28,959
are distinct from each other

278
00:10:28,959 --> 00:10:32,160
because the models typically work on

279
00:10:32,160 --> 00:10:33,279
what they call

280
00:10:33,279 --> 00:10:35,440
filters that are sized

281
00:10:35,440 --> 00:10:37,360
you know three by three pixels or five

282
00:10:37,360 --> 00:10:39,680
by five pixels and extreme cases seven

283
00:10:39,680 --> 00:10:41,440
by seven

284
00:10:41,440 --> 00:10:44,240
you're looking at very small portions of

285
00:10:44,240 --> 00:10:47,279
an image even though they get scaled

286
00:10:47,279 --> 00:10:50,320
down through max pooling

287
00:10:50,320 --> 00:10:51,920
you're still looking at

288
00:10:51,920 --> 00:10:53,680
pretty small

289
00:10:53,680 --> 00:10:56,240
patches of the image to try to find

290
00:10:56,240 --> 00:10:57,360
features

291
00:10:57,360 --> 00:10:59,360
that distinguish

292
00:10:59,360 --> 00:11:02,880
one class from another distinctly

293
00:11:02,880 --> 00:11:04,800
and what happens is

294
00:11:04,800 --> 00:11:06,720
you may learn

295
00:11:06,720 --> 00:11:09,040
non-robust features

296
00:11:09,040 --> 00:11:11,920
that then are imperceptible to a human

297
00:11:11,920 --> 00:11:14,160
and can be embedded

298
00:11:14,160 --> 00:11:16,480
in other images and fool

299
00:11:16,480 --> 00:11:18,320
the machine learning models

300
00:11:18,320 --> 00:11:20,399
to look at those non-robust features and

301
00:11:20,399 --> 00:11:23,920
categorize them as the incorrect class

302
00:11:23,920 --> 00:11:26,640
we talked about harsh boundaries

303
00:11:26,640 --> 00:11:28,480
and how you can just push something

304
00:11:28,480 --> 00:11:30,399
across a machine learning boundary and

305
00:11:30,399 --> 00:11:31,600
it will

306
00:11:31,600 --> 00:11:33,200
suddenly be

307
00:11:33,200 --> 00:11:35,200
99 confident

308
00:11:35,200 --> 00:11:36,160
and then we talked about high

309
00:11:36,160 --> 00:11:38,480
dimensionality which means that

310
00:11:38,480 --> 00:11:40,880
there are millions

311
00:11:40,880 --> 00:11:42,399
of potential

312
00:11:42,399 --> 00:11:45,600
input values to tweak

313
00:11:45,600 --> 00:11:48,000
and because of the million

314
00:11:48,000 --> 00:11:50,880
input features we have a million

315
00:11:50,880 --> 00:11:54,240
set of these boundaries in between our

316
00:11:54,240 --> 00:11:55,519
classes

317
00:11:55,519 --> 00:11:58,079
that we can look for opportunities to

318
00:11:58,079 --> 00:12:00,959
take advantage of

319
00:12:02,160 --> 00:12:04,399
so when we talk about the goal and

320
00:12:04,399 --> 00:12:07,760
intuition of adversarial samples

321
00:12:07,760 --> 00:12:08,880
the goal

322
00:12:08,880 --> 00:12:11,200
is to get the model to produce outputs

323
00:12:11,200 --> 00:12:15,440
that are contrary to what a human

324
00:12:15,440 --> 00:12:17,760
would decide or would interpret them to

325
00:12:17,760 --> 00:12:19,920
be

326
00:12:20,639 --> 00:12:21,519
now

327
00:12:21,519 --> 00:12:23,279
this idea also

328
00:12:23,279 --> 00:12:24,480
can be used

329
00:12:24,480 --> 00:12:27,200
when we look at models that predict if a

330
00:12:27,200 --> 00:12:29,040
certain text

331
00:12:29,040 --> 00:12:31,760
is spam or not spam

332
00:12:31,760 --> 00:12:33,920
so we can basically

333
00:12:33,920 --> 00:12:35,680
try to get

334
00:12:35,680 --> 00:12:37,120
a model

335
00:12:37,120 --> 00:12:40,000
to categorize something as not spam when

336
00:12:40,000 --> 00:12:42,079
it was

337
00:12:42,079 --> 00:12:43,680
and

338
00:12:43,680 --> 00:12:45,519
when we think about the intuition behind

339
00:12:45,519 --> 00:12:46,880
this

340
00:12:46,880 --> 00:12:48,800
our machine learning models learn

341
00:12:48,800 --> 00:12:50,880
imperfect boundaries

342
00:12:50,880 --> 00:12:53,839
that vary from human

343
00:12:53,839 --> 00:12:55,200
boundaries

344
00:12:55,200 --> 00:12:57,440
so we want to push

345
00:12:57,440 --> 00:13:00,240
the inputs across the imperfectly

346
00:13:00,240 --> 00:13:02,240
learned machine learning boundaries

347
00:13:02,240 --> 00:13:04,320
between classes

348
00:13:04,320 --> 00:13:06,800
but not across the human boundaries

349
00:13:06,800 --> 00:13:08,320
and then get

350
00:13:08,320 --> 00:13:10,320
a result where the picture looks the

351
00:13:10,320 --> 00:13:11,839
same to a human

352
00:13:11,839 --> 00:13:14,560
but when it's fed through the model

353
00:13:14,560 --> 00:13:16,320
it's going to return a radically

354
00:13:16,320 --> 00:13:20,360
different result than what's expected

355
00:13:21,120 --> 00:13:22,160
so there's

356
00:13:22,160 --> 00:13:24,880
an overall attacking procedure when

357
00:13:24,880 --> 00:13:26,800
you're creating adversarial samples it

358
00:13:26,800 --> 00:13:29,200
doesn't matter if you're

359
00:13:29,200 --> 00:13:30,079
you know

360
00:13:30,079 --> 00:13:32,000
creating adversarial samples for neural

361
00:13:32,000 --> 00:13:35,200
networks or for other

362
00:13:35,200 --> 00:13:38,000
classic machine learning models

363
00:13:38,000 --> 00:13:40,079
you first need to understand the model's

364
00:13:40,079 --> 00:13:41,440
weaknesses

365
00:13:41,440 --> 00:13:43,519
and how the model works

366
00:13:43,519 --> 00:13:45,760
and

367
00:13:46,000 --> 00:13:48,000
here it's going to be important to

368
00:13:48,000 --> 00:13:50,160
understand the math if you want to do

369
00:13:50,160 --> 00:13:51,680
something creative if you want to come

370
00:13:51,680 --> 00:13:54,399
up with a creative attack

371
00:13:54,399 --> 00:13:56,639
on these models

372
00:13:56,639 --> 00:13:58,480
if you don't understand the math and how

373
00:13:58,480 --> 00:13:59,839
the models work

374
00:13:59,839 --> 00:14:01,760
intimately then what's going to happen

375
00:14:01,760 --> 00:14:03,600
is you're going to be stuck

376
00:14:03,600 --> 00:14:05,600
to whatever limitations

377
00:14:05,600 --> 00:14:07,199
or

378
00:14:07,199 --> 00:14:08,639
capabilities

379
00:14:08,639 --> 00:14:10,639
that the tools that i'm going to show

380
00:14:10,639 --> 00:14:12,000
you and mention

381
00:14:12,000 --> 00:14:14,880
later have

382
00:14:15,279 --> 00:14:17,519
the next step is to create a model with

383
00:14:17,519 --> 00:14:19,440
training data similar

384
00:14:19,440 --> 00:14:21,600
to the model that you're attacking

385
00:14:21,600 --> 00:14:23,040
and

386
00:14:23,040 --> 00:14:24,560
what you're going to find is that the

387
00:14:24,560 --> 00:14:26,880
models if you have a large enough sample

388
00:14:26,880 --> 00:14:28,560
of data

389
00:14:28,560 --> 00:14:31,199
in doing the same type of thing are

390
00:14:31,199 --> 00:14:32,399
going to learn

391
00:14:32,399 --> 00:14:36,959
very similar features and you can even

392
00:14:37,040 --> 00:14:40,000
supplement your model's training data by

393
00:14:40,000 --> 00:14:42,240
actually feeding inputs and looking at

394
00:14:42,240 --> 00:14:43,600
the outputs

395
00:14:43,600 --> 00:14:45,360
on the victim model

396
00:14:45,360 --> 00:14:47,680
to fine-tune your model

397
00:14:47,680 --> 00:14:50,480
once you have a cloned model you can

398
00:14:50,480 --> 00:14:52,639
create adversarial samples with white

399
00:14:52,639 --> 00:14:54,480
box techniques

400
00:14:54,480 --> 00:14:57,120
and they will transfer because both

401
00:14:57,120 --> 00:15:00,560
models have learned the same

402
00:15:00,560 --> 00:15:03,040
or similar decision boundaries from the

403
00:15:03,040 --> 00:15:05,680
training data

404
00:15:06,480 --> 00:15:09,519
so let's talk about k n this is the

405
00:15:09,519 --> 00:15:11,680
most basic machine learning model

406
00:15:11,680 --> 00:15:14,880
and what its premise is is that

407
00:15:14,880 --> 00:15:16,959
things that are close together

408
00:15:16,959 --> 00:15:17,920
should be

409
00:15:17,920 --> 00:15:20,160
classified the same

410
00:15:20,160 --> 00:15:23,120
and when we say k k is a number

411
00:15:23,120 --> 00:15:25,360
that will determine how many of your

412
00:15:25,360 --> 00:15:26,720
closest

413
00:15:26,720 --> 00:15:29,199
neighbors you're going to use

414
00:15:29,199 --> 00:15:31,040
to classify the thing you're trying to

415
00:15:31,040 --> 00:15:32,560
classify so if we look at the bottom

416
00:15:32,560 --> 00:15:34,079
left here we have

417
00:15:34,079 --> 00:15:36,240
red triangles they have blue squares

418
00:15:36,240 --> 00:15:37,519
those represent the two different

419
00:15:37,519 --> 00:15:39,120
classes that could be

420
00:15:39,120 --> 00:15:41,600
sick or healthy it could be

421
00:15:41,600 --> 00:15:44,000
cancer no cancer it could be

422
00:15:44,000 --> 00:15:47,120
fit unfit it could be

423
00:15:47,199 --> 00:15:48,959
spam not spam

424
00:15:48,959 --> 00:15:49,759
right

425
00:15:49,759 --> 00:15:50,880
and so

426
00:15:50,880 --> 00:15:52,880
the training points are these red and

427
00:15:52,880 --> 00:15:55,680
blue points but the green point is the

428
00:15:55,680 --> 00:15:57,759
new point that we're trying to figure

429
00:15:57,759 --> 00:16:00,000
out what should this be classified as

430
00:16:00,000 --> 00:16:01,920
spam or not spam

431
00:16:01,920 --> 00:16:03,680
and here

432
00:16:03,680 --> 00:16:06,399
what if k equals one we're going to look

433
00:16:06,399 --> 00:16:08,880
at the nearest point training point to

434
00:16:08,880 --> 00:16:10,399
that point that we're trying to classify

435
00:16:10,399 --> 00:16:12,000
and that happens to be a blue

436
00:16:12,000 --> 00:16:13,199
square

437
00:16:13,199 --> 00:16:15,279
so we're going to say is that if we use

438
00:16:15,279 --> 00:16:17,600
k equals 1 we're going to say this green

439
00:16:17,600 --> 00:16:20,480
point should be classified in the blue

440
00:16:20,480 --> 00:16:23,120
blue squares class

441
00:16:23,120 --> 00:16:25,120
now k can vary

442
00:16:25,120 --> 00:16:28,560
the larger k will smooth out boundaries

443
00:16:28,560 --> 00:16:29,759
and so

444
00:16:29,759 --> 00:16:31,920
you know companies will experiment with

445
00:16:31,920 --> 00:16:33,839
different values of k

446
00:16:33,839 --> 00:16:35,279
larger and smaller

447
00:16:35,279 --> 00:16:36,959
to get the level accuracy they're

448
00:16:36,959 --> 00:16:38,720
looking for but that presents

449
00:16:38,720 --> 00:16:41,279
opportunities and so here

450
00:16:41,279 --> 00:16:43,519
if we look at k equals three

451
00:16:43,519 --> 00:16:45,519
well now this green point has

452
00:16:45,519 --> 00:16:48,160
flip-flopped its classification because

453
00:16:48,160 --> 00:16:49,920
out of the three points

454
00:16:49,920 --> 00:16:50,880
two

455
00:16:50,880 --> 00:16:53,600
are the red triangle so now because of

456
00:16:53,600 --> 00:16:55,680
majority vote

457
00:16:55,680 --> 00:16:57,120
the two red triangles are going to

458
00:16:57,120 --> 00:16:59,839
outvote the one blue square and now this

459
00:16:59,839 --> 00:17:02,000
green point is going to be classified

460
00:17:02,000 --> 00:17:03,040
as

461
00:17:03,040 --> 00:17:07,359
a red triangle or class two point

462
00:17:07,359 --> 00:17:10,000
now if you look at the right here

463
00:17:10,000 --> 00:17:12,000
we have a situation remember that these

464
00:17:12,000 --> 00:17:14,000
are in two dimensions so it's highly

465
00:17:14,000 --> 00:17:15,199
simplified

466
00:17:15,199 --> 00:17:16,240
whereas

467
00:17:16,240 --> 00:17:17,359
in the real world you're going to deal

468
00:17:17,359 --> 00:17:20,400
with hundreds thousands or millions

469
00:17:20,400 --> 00:17:21,599
of

470
00:17:21,599 --> 00:17:24,160
dimensions and features so right now

471
00:17:24,160 --> 00:17:25,119
this

472
00:17:25,119 --> 00:17:27,039
plot example is only looking at two

473
00:17:27,039 --> 00:17:28,319
features

474
00:17:28,319 --> 00:17:29,360
and what we're looking at is two

475
00:17:29,360 --> 00:17:31,360
features and two classes

476
00:17:31,360 --> 00:17:33,360
the gold class

477
00:17:33,360 --> 00:17:35,360
or the blue class

478
00:17:35,360 --> 00:17:36,559
and so

479
00:17:36,559 --> 00:17:38,880
the one thing i want you to notice and

480
00:17:38,880 --> 00:17:41,280
think about is right now this is k

481
00:17:41,280 --> 00:17:43,039
equals one

482
00:17:43,039 --> 00:17:46,400
but what happens when

483
00:17:46,400 --> 00:17:50,400
when k increases

484
00:17:50,480 --> 00:17:51,600
what's going to happen to the points

485
00:17:51,600 --> 00:17:54,160
that are the peninsula or the

486
00:17:54,160 --> 00:17:55,360
islands

487
00:17:55,360 --> 00:17:57,760
is that they're going to

488
00:17:57,760 --> 00:17:59,120
change their classification so let's

489
00:17:59,120 --> 00:18:00,640
take a look at this point

490
00:18:00,640 --> 00:18:03,919
if we say that k equals five

491
00:18:03,919 --> 00:18:06,080
well then now

492
00:18:06,080 --> 00:18:06,880
the

493
00:18:06,880 --> 00:18:09,039
three blue points and these two yellow

494
00:18:09,039 --> 00:18:10,000
points

495
00:18:10,000 --> 00:18:11,840
the blue points are going to outnumber

496
00:18:11,840 --> 00:18:13,840
the yellow points and so this point and

497
00:18:13,840 --> 00:18:15,840
this point are going to be incorrectly

498
00:18:15,840 --> 00:18:17,280
classified

499
00:18:17,280 --> 00:18:18,000
as

500
00:18:18,000 --> 00:18:20,160
the blue class when they're the yellow

501
00:18:20,160 --> 00:18:22,160
class now what you've got to realize is

502
00:18:22,160 --> 00:18:24,880
that in a high dimensional space

503
00:18:24,880 --> 00:18:27,039
when you just look at two dimensions in

504
00:18:27,039 --> 00:18:29,360
isolation the high dimensional space is

505
00:18:29,360 --> 00:18:30,640
going to have these places where it juts

506
00:18:30,640 --> 00:18:32,559
out into another dimension or it sticks

507
00:18:32,559 --> 00:18:34,640
a peninsula you know peninsula into

508
00:18:34,640 --> 00:18:36,480
another dimension and so these will

509
00:18:36,480 --> 00:18:37,840
occur

510
00:18:37,840 --> 00:18:38,640
and

511
00:18:38,640 --> 00:18:40,880
they'll be the correct

512
00:18:40,880 --> 00:18:42,400
label

513
00:18:42,400 --> 00:18:43,840
but the thing is

514
00:18:43,840 --> 00:18:46,400
when we change our k value

515
00:18:46,400 --> 00:18:48,880
it's going to change how we

516
00:18:48,880 --> 00:18:51,120
classify these points

517
00:18:51,120 --> 00:18:53,679
and so when you're attacking k n

518
00:18:53,679 --> 00:18:55,360
that's what you're looking for

519
00:18:55,360 --> 00:18:56,880
you know to change something from spam

520
00:18:56,880 --> 00:18:59,360
to not spam is you're gonna

521
00:18:59,360 --> 00:19:01,280
train your model and find these places

522
00:19:01,280 --> 00:19:03,440
where

523
00:19:03,440 --> 00:19:04,160
the

524
00:19:04,160 --> 00:19:05,919
you have the peninsulas and the islands

525
00:19:05,919 --> 00:19:07,760
and then you're gonna try to

526
00:19:07,760 --> 00:19:08,880
craft

527
00:19:08,880 --> 00:19:11,679
your message such that it's going to be

528
00:19:11,679 --> 00:19:13,440
around these islands and then

529
00:19:13,440 --> 00:19:14,880
hope that

530
00:19:14,880 --> 00:19:16,799
they're using a k big enough that's

531
00:19:16,799 --> 00:19:18,799
going to allow you to

532
00:19:18,799 --> 00:19:20,080
change

533
00:19:20,080 --> 00:19:21,919
the classification now there are two

534
00:19:21,919 --> 00:19:24,160
papers here that

535
00:19:24,160 --> 00:19:26,080
go into specific detail

536
00:19:26,080 --> 00:19:28,320
on different techniques

537
00:19:28,320 --> 00:19:30,480
that you can use

538
00:19:30,480 --> 00:19:31,600
and if you're interested i would read

539
00:19:31,600 --> 00:19:34,240
those papers

540
00:19:34,240 --> 00:19:35,520
the next model we're looking at

541
00:19:35,520 --> 00:19:37,440
something called naive bayes

542
00:19:37,440 --> 00:19:39,760
and is based on bayes theorem

543
00:19:39,760 --> 00:19:42,240
and what this is doing is we have this

544
00:19:42,240 --> 00:19:44,559
means that what's the probability

545
00:19:44,559 --> 00:19:46,480
of a given label

546
00:19:46,480 --> 00:19:49,280
i mean of a label given

547
00:19:49,280 --> 00:19:52,400
a set of input features

548
00:19:52,400 --> 00:19:54,480
now typically you're going to have your

549
00:19:54,480 --> 00:19:56,400
data set that looks like this table up

550
00:19:56,400 --> 00:19:57,840
at the top

551
00:19:57,840 --> 00:20:00,640
and what you can do is use bayes theorem

552
00:20:00,640 --> 00:20:02,480
to actually figure out this probability

553
00:20:02,480 --> 00:20:06,080
in the case of the probability of spam

554
00:20:06,080 --> 00:20:09,200
given these words and the probability of

555
00:20:09,200 --> 00:20:12,000
not spam given the same words whichever

556
00:20:12,000 --> 00:20:13,200
is higher

557
00:20:13,200 --> 00:20:14,400
is going to be the prediction of the

558
00:20:14,400 --> 00:20:15,760
model

559
00:20:15,760 --> 00:20:18,080
now the way we calculate these

560
00:20:18,080 --> 00:20:20,240
conditional probabilities is we can

561
00:20:20,240 --> 00:20:21,679
flip-flop

562
00:20:21,679 --> 00:20:23,840
this here such that we're looking for

563
00:20:23,840 --> 00:20:25,200
the probability

564
00:20:25,200 --> 00:20:26,240
of

565
00:20:26,240 --> 00:20:28,960
these input features given

566
00:20:28,960 --> 00:20:31,280
the label spam and then we just

567
00:20:31,280 --> 00:20:33,360
basically like let's say the three words

568
00:20:33,360 --> 00:20:35,440
in our input are viagra hurry and

569
00:20:35,440 --> 00:20:36,559
limited

570
00:20:36,559 --> 00:20:37,360
well

571
00:20:37,360 --> 00:20:38,880
that those three words would be a

572
00:20:38,880 --> 00:20:40,880
combined conditional probability but

573
00:20:40,880 --> 00:20:43,039
what we can do is with naive bayes

574
00:20:43,039 --> 00:20:44,640
if we assume that each of these input

575
00:20:44,640 --> 00:20:46,799
features are independent then we can

576
00:20:46,799 --> 00:20:49,200
actually say that the combined

577
00:20:49,200 --> 00:20:51,039
probability is equal to the

578
00:20:51,039 --> 00:20:52,960
multiplication of the

579
00:20:52,960 --> 00:20:54,960
individual

580
00:20:54,960 --> 00:20:58,000
conditional probabilities so here

581
00:20:58,000 --> 00:20:59,520
that's what we're doing so

582
00:20:59,520 --> 00:21:01,120
we have

583
00:21:01,120 --> 00:21:02,799
given its spam

584
00:21:02,799 --> 00:21:04,159
what's the probability that the word

585
00:21:04,159 --> 00:21:06,159
fire appears and so we look at all the

586
00:21:06,159 --> 00:21:07,120
rows

587
00:21:07,120 --> 00:21:08,799
that are spam

588
00:21:08,799 --> 00:21:10,720
and then we look for how many times out

589
00:21:10,720 --> 00:21:12,880
of all those rows viagra appears and

590
00:21:12,880 --> 00:21:14,880
that's our probability do the same thing

591
00:21:14,880 --> 00:21:16,559
here we look at all the rows that are

592
00:21:16,559 --> 00:21:19,200
spam and say how many times did hurry

593
00:21:19,200 --> 00:21:20,320
show up

594
00:21:20,320 --> 00:21:22,640
what percentage and we do this on and on

595
00:21:22,640 --> 00:21:24,240
and then we also calculate the

596
00:21:24,240 --> 00:21:26,720
probability of spam by just looking at

597
00:21:26,720 --> 00:21:28,400
all the rows and saying how many were

598
00:21:28,400 --> 00:21:30,720
spam how many were not

599
00:21:30,720 --> 00:21:32,320
and we get a probability and multiply

600
00:21:32,320 --> 00:21:34,240
those all together and then we get a

601
00:21:34,240 --> 00:21:36,640
final probability and

602
00:21:36,640 --> 00:21:38,799
here i just put in sample values they

603
00:21:38,799 --> 00:21:41,600
don't tied to this table but

604
00:21:41,600 --> 00:21:43,120
which one do you think this model is

605
00:21:43,120 --> 00:21:45,120
predicting

606
00:21:45,120 --> 00:21:47,280
well because this one's higher than it's

607
00:21:47,280 --> 00:21:48,320
saying

608
00:21:48,320 --> 00:21:50,159
given the words

609
00:21:50,159 --> 00:21:52,559
viagra hurry unlimited

610
00:21:52,559 --> 00:21:55,600
it's a 90 or 89 probability that this is

611
00:21:55,600 --> 00:21:56,720
spam

612
00:21:56,720 --> 00:21:58,880
now

613
00:21:58,880 --> 00:22:00,559
when you're trying to attack naive bayes

614
00:22:00,559 --> 00:22:02,080
what you want to do is you want to pick

615
00:22:02,080 --> 00:22:03,280
words

616
00:22:03,280 --> 00:22:06,000
that are in not spam

617
00:22:06,000 --> 00:22:08,480
that aren't in spam

618
00:22:08,480 --> 00:22:10,960
and you want to put those in your

619
00:22:10,960 --> 00:22:12,880
text to then

620
00:22:12,880 --> 00:22:14,799
push this probability higher and this

621
00:22:14,799 --> 00:22:17,280
probability lower

622
00:22:17,280 --> 00:22:18,080
so

623
00:22:18,080 --> 00:22:19,440
that's how you attack

624
00:22:19,440 --> 00:22:22,159
naive bayes when we're looking at linear

625
00:22:22,159 --> 00:22:25,200
classifiers what we have is we have

626
00:22:25,200 --> 00:22:27,919
a line or hyperplane that we're trying

627
00:22:27,919 --> 00:22:29,919
to separate

628
00:22:29,919 --> 00:22:31,760
two classes

629
00:22:31,760 --> 00:22:33,520
and the way we define this line

630
00:22:33,520 --> 00:22:36,720
generically is up here

631
00:22:36,720 --> 00:22:38,320
is this

632
00:22:38,320 --> 00:22:40,240
formula here and what we do is we learn

633
00:22:40,240 --> 00:22:42,240
these w values and in this particular

634
00:22:42,240 --> 00:22:43,120
case

635
00:22:43,120 --> 00:22:46,559
the w values are negative one and one

636
00:22:46,559 --> 00:22:48,880
and we have a bias which allows this

637
00:22:48,880 --> 00:22:51,120
line to shift up and down

638
00:22:51,120 --> 00:22:53,679
on the y axis zero because it goes right

639
00:22:53,679 --> 00:22:54,960
through the origin

640
00:22:54,960 --> 00:22:56,880
if this bias was two

641
00:22:56,880 --> 00:22:58,400
well this line would go through two and

642
00:22:58,400 --> 00:23:01,280
it would go this way right

643
00:23:01,280 --> 00:23:02,000
so

644
00:23:02,000 --> 00:23:03,919
the way that a linear classifier works

645
00:23:03,919 --> 00:23:06,000
is you're going to take the points you

646
00:23:06,000 --> 00:23:07,760
want to classify like this point here 2

647
00:23:07,760 --> 00:23:10,080
2 you're going to plug it in

648
00:23:10,080 --> 00:23:12,559
and sorry this is

649
00:23:12,559 --> 00:23:15,200
x equals negative 2 y equals 2. so

650
00:23:15,200 --> 00:23:17,360
negative 2 times negative 1 is a

651
00:23:17,360 --> 00:23:20,240
positive 2 plus positive 2 plus 1. so

652
00:23:20,240 --> 00:23:21,600
that's a 4

653
00:23:21,600 --> 00:23:23,520
because it's a positive number that's

654
00:23:23,520 --> 00:23:25,280
come out of this when we plug in our

655
00:23:25,280 --> 00:23:27,760
values we're going to say this point

656
00:23:27,760 --> 00:23:30,320
is in the positive class we take this

657
00:23:30,320 --> 00:23:31,840
point here

658
00:23:31,840 --> 00:23:32,960
we're saying

659
00:23:32,960 --> 00:23:36,159
x equals three and one so if it's three

660
00:23:36,159 --> 00:23:37,360
it's three times negative one is

661
00:23:37,360 --> 00:23:39,600
negative three one

662
00:23:39,600 --> 00:23:41,919
minus negative and one plus negative

663
00:23:41,919 --> 00:23:43,919
three is negative two so we're gonna say

664
00:23:43,919 --> 00:23:45,600
now because the output is negative we're

665
00:23:45,600 --> 00:23:48,840
gonna say this is in the negative class

666
00:23:48,840 --> 00:23:52,400
now when you look at this

667
00:23:52,400 --> 00:23:53,919
we when you're thinking about

668
00:23:53,919 --> 00:23:56,240
adversarially attacking what happens if

669
00:23:56,240 --> 00:23:59,760
our training data was missing a lot of

670
00:23:59,760 --> 00:24:00,880
valid

671
00:24:00,880 --> 00:24:02,559
training points so if you look at these

672
00:24:02,559 --> 00:24:05,200
light blue stars

673
00:24:05,200 --> 00:24:08,240
they're associated with training data

674
00:24:08,240 --> 00:24:10,559
in the positive class we look at the

675
00:24:10,559 --> 00:24:11,760
light

676
00:24:11,760 --> 00:24:14,320
red or pink stars they're missing

677
00:24:14,320 --> 00:24:15,600
training data

678
00:24:15,600 --> 00:24:16,720
in the

679
00:24:16,720 --> 00:24:18,640
negative class

680
00:24:18,640 --> 00:24:20,480
if you had this training data then the

681
00:24:20,480 --> 00:24:22,240
real boundary

682
00:24:22,240 --> 00:24:23,679
is here

683
00:24:23,679 --> 00:24:25,679
but because we didn't have that training

684
00:24:25,679 --> 00:24:29,440
data we learned a boundary like this

685
00:24:29,440 --> 00:24:31,840
so what does that mean about our overall

686
00:24:31,840 --> 00:24:33,120
technique

687
00:24:33,120 --> 00:24:34,159
that we're going to use to create

688
00:24:34,159 --> 00:24:35,760
adversarial samples

689
00:24:35,760 --> 00:24:37,760
where do we want to start

690
00:24:37,760 --> 00:24:39,279
images with

691
00:24:39,279 --> 00:24:40,240
that

692
00:24:40,240 --> 00:24:42,080
we're going to be able to push them to

693
00:24:42,080 --> 00:24:43,279
another

694
00:24:43,279 --> 00:24:46,000
across the incorrect incorrectly learned

695
00:24:46,000 --> 00:24:47,600
boundary

696
00:24:47,600 --> 00:24:51,679
but not across the human boundary right

697
00:24:51,679 --> 00:24:52,880
so if you

698
00:24:52,880 --> 00:24:54,080
think about that

699
00:24:54,080 --> 00:24:56,080
where are we going to push

700
00:24:56,080 --> 00:24:58,000
you know points that are on this side of

701
00:24:58,000 --> 00:24:59,600
the line

702
00:24:59,600 --> 00:25:00,960
and where are we going to point where

703
00:25:00,960 --> 00:25:03,360
are we going to push points on this side

704
00:25:03,360 --> 00:25:04,880
of this

705
00:25:04,880 --> 00:25:06,799
incorrect boundary

706
00:25:06,799 --> 00:25:08,799
to get adversarial samples which

707
00:25:08,799 --> 00:25:11,360
basically look

708
00:25:11,360 --> 00:25:13,520
the same this is our human

709
00:25:13,520 --> 00:25:15,760
discerning human

710
00:25:15,760 --> 00:25:17,120
look the same

711
00:25:17,120 --> 00:25:20,080
but are categorized differently

712
00:25:20,080 --> 00:25:23,520
by the model because we've crossed

713
00:25:23,520 --> 00:25:26,840
the machine learning learned decision

714
00:25:26,840 --> 00:25:30,320
boundary well those points right

715
00:25:30,320 --> 00:25:32,880
so once we push

716
00:25:32,880 --> 00:25:35,919
one of these blue points across here but

717
00:25:35,919 --> 00:25:38,640
not across this

718
00:25:38,640 --> 00:25:40,080
now because we push it across the

719
00:25:40,080 --> 00:25:42,159
machine arm boundary it's going to be

720
00:25:42,159 --> 00:25:44,000
reported as belonging to the negative

721
00:25:44,000 --> 00:25:46,640
class when it's really a positive class

722
00:25:46,640 --> 00:25:48,240
and it's going to look like a positive

723
00:25:48,240 --> 00:25:51,520
class because it hasn't crossed

724
00:25:51,520 --> 00:25:53,039
the human boundary and the same thing

725
00:25:53,039 --> 00:25:54,880
goes with this location i'm going to

726
00:25:54,880 --> 00:25:56,799
push these

727
00:25:56,799 --> 00:25:58,480
negative class points across the

728
00:25:58,480 --> 00:26:00,320
decision boundary

729
00:26:00,320 --> 00:26:01,919
to

730
00:26:01,919 --> 00:26:03,600
you know points that are on the other

731
00:26:03,600 --> 00:26:05,440
side of the incorrectly learned machine

732
00:26:05,440 --> 00:26:06,960
boundary but we're going to make sure

733
00:26:06,960 --> 00:26:08,799
that we keep them

734
00:26:08,799 --> 00:26:10,480
on the same side

735
00:26:10,480 --> 00:26:13,679
of the human value

736
00:26:14,000 --> 00:26:15,840
and if you're looking for examples i

737
00:26:15,840 --> 00:26:17,279
have links down there where you can

738
00:26:17,279 --> 00:26:19,360
actually try this out

739
00:26:19,360 --> 00:26:22,480
and they have notebooks for you to

740
00:26:22,480 --> 00:26:25,440
do this the ibm adversarial robustness

741
00:26:25,440 --> 00:26:26,640
toolkit

742
00:26:26,640 --> 00:26:29,520
has a bunch of nice attacks and defenses

743
00:26:29,520 --> 00:26:32,480
that you can experiment with

744
00:26:32,480 --> 00:26:34,880
let's talk about decision trees

745
00:26:34,880 --> 00:26:36,960
so with decision trees

746
00:26:36,960 --> 00:26:38,799
you have all of your training data and

747
00:26:38,799 --> 00:26:41,120
you're trying to find

748
00:26:41,120 --> 00:26:42,880
which features

749
00:26:42,880 --> 00:26:44,080
and which

750
00:26:44,080 --> 00:26:46,400
values in this those features

751
00:26:46,400 --> 00:26:49,440
will serve as good split points

752
00:26:49,440 --> 00:26:50,799
to result

753
00:26:50,799 --> 00:26:52,799
in child nodes

754
00:26:52,799 --> 00:26:56,159
with the highest purity or the lowest

755
00:26:56,159 --> 00:26:58,159
impurity and what you want to do is you

756
00:26:58,159 --> 00:27:00,400
want to start and find

757
00:27:00,400 --> 00:27:03,120
that split point and that feature that's

758
00:27:03,120 --> 00:27:04,960
going to give you the highest split

759
00:27:04,960 --> 00:27:07,200
with the highest purity of children and

760
00:27:07,200 --> 00:27:09,919
then you're going to keep on splitting

761
00:27:09,919 --> 00:27:11,039
until

762
00:27:11,039 --> 00:27:13,120
all of the leaf nodes

763
00:27:13,120 --> 00:27:14,640
have

764
00:27:14,640 --> 00:27:15,520
one

765
00:27:15,520 --> 00:27:18,240
label or all of the same label

766
00:27:18,240 --> 00:27:20,320
or the leaf nodes have a certain number

767
00:27:20,320 --> 00:27:22,399
of nodes in them and then you can take a

768
00:27:22,399 --> 00:27:23,760
majority vote

769
00:27:23,760 --> 00:27:26,320
or an average of the results depending

770
00:27:26,320 --> 00:27:28,480
on if it's a regression tree or a

771
00:27:28,480 --> 00:27:30,399
classification tree

772
00:27:30,399 --> 00:27:32,159
but the important thing

773
00:27:32,159 --> 00:27:34,159
to understand about how these models

774
00:27:34,159 --> 00:27:35,840
work is that

775
00:27:35,840 --> 00:27:37,679
because you're dealing with

776
00:27:37,679 --> 00:27:38,720
that

777
00:27:38,720 --> 00:27:41,600
subset of the data

778
00:27:41,600 --> 00:27:44,240
your decision tree is going to learn

779
00:27:44,240 --> 00:27:46,320
jagged boundaries

780
00:27:46,320 --> 00:27:48,720
around that data you know when you make

781
00:27:48,720 --> 00:27:51,279
these decisions to cut left or right

782
00:27:51,279 --> 00:27:53,039
where to split the group

783
00:27:53,039 --> 00:27:54,720
that's what's happening

784
00:27:54,720 --> 00:27:56,000
and

785
00:27:56,000 --> 00:27:58,240
if you notice here this yellow line is

786
00:27:58,240 --> 00:28:00,320
the decision boundary

787
00:28:00,320 --> 00:28:03,679
for what a human would discern

788
00:28:03,679 --> 00:28:05,840
between everything that was blue in the

789
00:28:05,840 --> 00:28:07,840
blue class and everything that was in

790
00:28:07,840 --> 00:28:10,559
the red class but what's learned by the

791
00:28:10,559 --> 00:28:11,679
model

792
00:28:11,679 --> 00:28:13,279
is this boundary

793
00:28:13,279 --> 00:28:14,840
here between the red and the blue

794
00:28:14,840 --> 00:28:17,440
classes and if you look at this you can

795
00:28:17,440 --> 00:28:18,399
see that there are a lot of

796
00:28:18,399 --> 00:28:19,840
opportunities

797
00:28:19,840 --> 00:28:22,799
to cause misclassification where you

798
00:28:22,799 --> 00:28:24,080
don't change

799
00:28:24,080 --> 00:28:26,480
what's humanly discernible

800
00:28:26,480 --> 00:28:28,880
but you change the features and the

801
00:28:28,880 --> 00:28:32,320
model change its prediction

802
00:28:32,320 --> 00:28:34,480
so

803
00:28:35,120 --> 00:28:38,000
if i were to say

804
00:28:38,000 --> 00:28:40,880
where would you change

805
00:28:40,880 --> 00:28:42,399
a red point

806
00:28:42,399 --> 00:28:44,399
to a blue point

807
00:28:44,399 --> 00:28:46,080
but not make it look different to a

808
00:28:46,080 --> 00:28:48,240
human or a blue point

809
00:28:48,240 --> 00:28:51,279
to red point but not make it look

810
00:28:51,279 --> 00:28:53,840
different not change its blueness or

811
00:28:53,840 --> 00:28:58,159
whatever its characteristic or label

812
00:28:58,159 --> 00:29:00,480
but you're going to change the model's

813
00:29:00,480 --> 00:29:01,919
results

814
00:29:01,919 --> 00:29:04,480
where would you be looking

815
00:29:04,480 --> 00:29:07,039
at this decision here

816
00:29:07,039 --> 00:29:10,240
what boundary positions

817
00:29:10,559 --> 00:29:13,120
it's basically all the points that are

818
00:29:13,120 --> 00:29:14,399
sticking out

819
00:29:14,399 --> 00:29:16,639
right

820
00:29:16,720 --> 00:29:18,640
those are going to be where you can push

821
00:29:18,640 --> 00:29:21,039
blue into a red boundary

822
00:29:21,039 --> 00:29:23,600
not change what it looks like to a human

823
00:29:23,600 --> 00:29:27,120
or what it's perceived as by human but

824
00:29:27,120 --> 00:29:29,039
the machine learning model will

825
00:29:29,039 --> 00:29:30,399
suddenly

826
00:29:30,399 --> 00:29:31,200
make

827
00:29:31,200 --> 00:29:33,360
a different result to make a different

828
00:29:33,360 --> 00:29:35,279
prediction

829
00:29:35,279 --> 00:29:39,000
an incorrect prediction

830
00:29:40,000 --> 00:29:41,200
and if you want to look at different

831
00:29:41,200 --> 00:29:42,880
attacks

832
00:29:42,880 --> 00:29:45,760
i have a link here that you can

833
00:29:45,760 --> 00:29:47,840
go to and you have a notebook and you

834
00:29:47,840 --> 00:29:50,240
can try it out yourself

835
00:29:50,240 --> 00:29:53,120
when we look at random forest

836
00:29:53,120 --> 00:29:54,799
basically we have a situation like

837
00:29:54,799 --> 00:29:56,240
decision cheese where it's a collection

838
00:29:56,240 --> 00:29:58,960
of decision trees but each decision tree

839
00:29:58,960 --> 00:30:01,679
is sampled or created with training data

840
00:30:01,679 --> 00:30:03,279
sampled from the original training data

841
00:30:03,279 --> 00:30:05,039
set with replacement

842
00:30:05,039 --> 00:30:08,320
and at each decision split point we're

843
00:30:08,320 --> 00:30:10,720
only looking at a subset of the features

844
00:30:10,720 --> 00:30:12,720
to split on

845
00:30:12,720 --> 00:30:14,240
and so

846
00:30:14,240 --> 00:30:16,159
and we keep on doing this over and over

847
00:30:16,159 --> 00:30:18,159
again until we have

848
00:30:18,159 --> 00:30:20,799
a lot of decision trees and we

849
00:30:20,799 --> 00:30:22,559
average the results or take a majority

850
00:30:22,559 --> 00:30:25,120
of what vote uh the decision trees the

851
00:30:25,120 --> 00:30:27,120
individual decision trees are created

852
00:30:27,120 --> 00:30:29,360
using this technique and if you look

853
00:30:29,360 --> 00:30:30,799
to compare

854
00:30:30,799 --> 00:30:32,159
the boundary

855
00:30:32,159 --> 00:30:34,240
from what a decision tree learns

856
00:30:34,240 --> 00:30:36,480
there are a lot less

857
00:30:36,480 --> 00:30:37,840
opportunities

858
00:30:37,840 --> 00:30:40,880
and it's a lot tighter however

859
00:30:40,880 --> 00:30:42,320
there's still

860
00:30:42,320 --> 00:30:44,880
opportunities right where the places jut

861
00:30:44,880 --> 00:30:46,480
out

862
00:30:46,480 --> 00:30:48,640
from across the human boundary

863
00:30:48,640 --> 00:30:51,840
and where there's a mismatch

864
00:30:51,919 --> 00:30:55,200
those are opportunities for

865
00:30:57,440 --> 00:30:59,840
adversarial samples

866
00:30:59,840 --> 00:31:01,919
so what we have here is

867
00:31:01,919 --> 00:31:04,159
a another example of something that you

868
00:31:04,159 --> 00:31:06,480
look at to experimentally

869
00:31:06,480 --> 00:31:09,039
when you look at support vector machines

870
00:31:09,039 --> 00:31:11,120
basically what we're doing here is we're

871
00:31:11,120 --> 00:31:13,200
trying to

872
00:31:13,200 --> 00:31:15,039
maximize the difference between the

873
00:31:15,039 --> 00:31:18,399
closest point to the boundary line the

874
00:31:18,399 --> 00:31:20,640
machine learned boundary line

875
00:31:20,640 --> 00:31:22,080
and

876
00:31:22,080 --> 00:31:23,279
we have this

877
00:31:23,279 --> 00:31:26,880
formula here that we use which is our

878
00:31:26,880 --> 00:31:29,760
objective and our constraints

879
00:31:29,760 --> 00:31:31,039
and

880
00:31:31,039 --> 00:31:33,200
the same thing can occur

881
00:31:33,200 --> 00:31:35,039
with

882
00:31:35,039 --> 00:31:37,279
linear support vector machines so if

883
00:31:37,279 --> 00:31:39,120
your training data

884
00:31:39,120 --> 00:31:41,360
is bad well the support vector machine

885
00:31:41,360 --> 00:31:42,880
is going to learn

886
00:31:42,880 --> 00:31:43,679
a

887
00:31:43,679 --> 00:31:45,760
incorrect boundary and as we've been

888
00:31:45,760 --> 00:31:47,440
talking about

889
00:31:47,440 --> 00:31:49,440
once there's an incorrect boundary and

890
00:31:49,440 --> 00:31:52,000
it's mismatched with the human boundary

891
00:31:52,000 --> 00:31:54,320
it's going to give you an opportunity to

892
00:31:54,320 --> 00:31:57,200
create adversarial samples that

893
00:31:57,200 --> 00:31:58,960
do not look

894
00:31:58,960 --> 00:32:01,039
different to a human but have

895
00:32:01,039 --> 00:32:03,600
drastically changed the output

896
00:32:03,600 --> 00:32:06,240
prediction of the model

897
00:32:06,240 --> 00:32:08,080
again there's

898
00:32:08,080 --> 00:32:10,559
an example here in the links

899
00:32:10,559 --> 00:32:12,960
on this slide

900
00:32:12,960 --> 00:32:14,320
so when we talk about adversarial

901
00:32:14,320 --> 00:32:16,159
attacks on neural networks

902
00:32:16,159 --> 00:32:18,080
there are many ways to do this

903
00:32:18,080 --> 00:32:19,120
and

904
00:32:19,120 --> 00:32:21,679
i talk about this in my other

905
00:32:21,679 --> 00:32:23,120
talks on

906
00:32:23,120 --> 00:32:24,720
adversarial samples

907
00:32:24,720 --> 00:32:26,240
i wanted to do something different to

908
00:32:26,240 --> 00:32:29,519
let you know how expansive this problem

909
00:32:29,519 --> 00:32:30,320
is

910
00:32:30,320 --> 00:32:32,240
that it's not just

911
00:32:32,240 --> 00:32:34,799
a problem related to deep learning

912
00:32:34,799 --> 00:32:36,640
and neural networks

913
00:32:36,640 --> 00:32:38,559
has the same process

914
00:32:38,559 --> 00:32:39,840
basically you're going to clone the

915
00:32:39,840 --> 00:32:40,960
model

916
00:32:40,960 --> 00:32:42,240
but the thing is here you're taking

917
00:32:42,240 --> 00:32:43,840
partial derivatives of output class with

918
00:32:43,840 --> 00:32:45,519
respect to the inputs

919
00:32:45,519 --> 00:32:48,080
because a neural network models

920
00:32:48,080 --> 00:32:50,320
typically differentiable

921
00:32:50,320 --> 00:32:51,679
there might be some difficulties if

922
00:32:51,679 --> 00:32:53,519
you're using

923
00:32:53,519 --> 00:32:56,799
dropout but you can

924
00:32:56,799 --> 00:32:58,399
predict what the gradient values are

925
00:32:58,399 --> 00:33:00,720
that you need to modify

926
00:33:00,720 --> 00:33:01,840
but

927
00:33:01,840 --> 00:33:04,880
in general what you want to do is

928
00:33:04,880 --> 00:33:05,840
you want to take these partial

929
00:33:05,840 --> 00:33:07,679
derivatives to find out which inputs are

930
00:33:07,679 --> 00:33:09,679
going to push you in the direction

931
00:33:09,679 --> 00:33:11,360
of your target

932
00:33:11,360 --> 00:33:12,720
class

933
00:33:12,720 --> 00:33:14,320
and then

934
00:33:14,320 --> 00:33:16,320
once you've found the adversarial

935
00:33:16,320 --> 00:33:18,640
samples on your clone model they

936
00:33:18,640 --> 00:33:21,039
transfer it meaning they work

937
00:33:21,039 --> 00:33:23,200
on another model that's been similarly

938
00:33:23,200 --> 00:33:24,960
trained to solve a similar problem or

939
00:33:24,960 --> 00:33:26,960
the same problem

940
00:33:26,960 --> 00:33:29,279
which has learned the same features

941
00:33:29,279 --> 00:33:31,440
because you've used sufficiently large

942
00:33:31,440 --> 00:33:33,440
enough data set to

943
00:33:33,440 --> 00:33:35,360
learn the appropriate features

944
00:33:35,360 --> 00:33:37,360
that

945
00:33:37,360 --> 00:33:38,799
are going to lead the decision

946
00:33:38,799 --> 00:33:40,880
boundaries that are shared

947
00:33:40,880 --> 00:33:43,039
between your clone model

948
00:33:43,039 --> 00:33:44,000
and

949
00:33:44,000 --> 00:33:47,760
the victim model that you're attacking

950
00:33:47,919 --> 00:33:50,640
now when we talk about defenses

951
00:33:50,640 --> 00:33:52,720
the two most prominent they you know

952
00:33:52,720 --> 00:33:54,159
that have come up recently are

953
00:33:54,159 --> 00:33:55,840
adversarial training

954
00:33:55,840 --> 00:33:58,320
and removing non-robust features

955
00:33:58,320 --> 00:34:00,960
adversarial training means that you take

956
00:34:00,960 --> 00:34:01,919
your

957
00:34:01,919 --> 00:34:05,039
training data and you embed

958
00:34:05,039 --> 00:34:07,120
the adversarial perturbations by using

959
00:34:07,120 --> 00:34:08,560
these

960
00:34:08,560 --> 00:34:11,040
frameworks and toolkits like the

961
00:34:11,040 --> 00:34:14,079
art from ibm and clever hans and some of

962
00:34:14,079 --> 00:34:16,480
these other toolkits to embed

963
00:34:16,480 --> 00:34:18,879
these perturbations in your pictures

964
00:34:18,879 --> 00:34:20,639
like the cap you know the guacamole

965
00:34:20,639 --> 00:34:23,359
percubation and what you do is then you

966
00:34:23,359 --> 00:34:24,639
label

967
00:34:24,639 --> 00:34:25,760
the

968
00:34:25,760 --> 00:34:28,560
previously incorrected image as the

969
00:34:28,560 --> 00:34:30,800
correct label and you basically train

970
00:34:30,800 --> 00:34:32,399
the model to say hey

971
00:34:32,399 --> 00:34:34,399
this feature that was guacamole is not

972
00:34:34,399 --> 00:34:36,399
really guacamole

973
00:34:36,399 --> 00:34:38,719
and so you're trying to remove those

974
00:34:38,719 --> 00:34:39,760
features

975
00:34:39,760 --> 00:34:42,320
the downside is that

976
00:34:42,320 --> 00:34:44,719
because you're now

977
00:34:44,719 --> 00:34:47,040
taking these images and your training

978
00:34:47,040 --> 00:34:49,199
data and essentially duplicating them

979
00:34:49,199 --> 00:34:50,719
but just adding

980
00:34:50,719 --> 00:34:53,040
perturbations inside of those images

981
00:34:53,040 --> 00:34:54,800
you're over fitting or you're over

982
00:34:54,800 --> 00:34:55,839
training

983
00:34:55,839 --> 00:34:58,560
on your training set and so it could

984
00:34:58,560 --> 00:35:00,960
result in

985
00:35:00,960 --> 00:35:03,599
training data set leakage

986
00:35:03,599 --> 00:35:05,359
and in addition to that

987
00:35:05,359 --> 00:35:06,720
you're going to take a hit

988
00:35:06,720 --> 00:35:08,800
in terms of accuracy because

989
00:35:08,800 --> 00:35:11,119
these features that were found

990
00:35:11,119 --> 00:35:13,200
were

991
00:35:13,200 --> 00:35:15,599
kind of strong indicators although they

992
00:35:15,599 --> 00:35:17,040
weren't visible

993
00:35:17,040 --> 00:35:19,119
to us

994
00:35:19,119 --> 00:35:20,400
they were visible to the machine

995
00:35:20,400 --> 00:35:23,440
learning model to distinguish between

996
00:35:23,440 --> 00:35:26,560
all the different classes

997
00:35:27,359 --> 00:35:29,520
so if you want to experiment i suggest

998
00:35:29,520 --> 00:35:32,000
you take a look at clever hans ibm's

999
00:35:32,000 --> 00:35:34,400
adversarial robustness toolkit

1000
00:35:34,400 --> 00:35:36,079
toolbox and then many others and the

1001
00:35:36,079 --> 00:35:37,119
link

1002
00:35:37,119 --> 00:35:39,440
is actually upon the many others there's

1003
00:35:39,440 --> 00:35:41,680
a link to a page that has

1004
00:35:41,680 --> 00:35:43,359
the other

1005
00:35:43,359 --> 00:35:45,680
tools that you can use

1006
00:35:45,680 --> 00:35:47,599
so in retrospect

1007
00:35:47,599 --> 00:35:49,119
this leads to

1008
00:35:49,119 --> 00:35:51,920
a question about why adversarial samples

1009
00:35:51,920 --> 00:35:53,839
are so prevalent

1010
00:35:53,839 --> 00:35:55,040
and

1011
00:35:55,040 --> 00:35:57,280
what it's saying is that basically the

1012
00:35:57,280 --> 00:35:58,960
machine learning models

1013
00:35:58,960 --> 00:36:00,320
are learning

1014
00:36:00,320 --> 00:36:01,760
features

1015
00:36:01,760 --> 00:36:03,680
that

1016
00:36:03,680 --> 00:36:05,680
may be important to distinguishing

1017
00:36:05,680 --> 00:36:08,079
between these classes but that

1018
00:36:08,079 --> 00:36:11,359
are not the features that we use

1019
00:36:11,359 --> 00:36:13,839
and

1020
00:36:14,160 --> 00:36:17,280
in some cases this is going to be good

1021
00:36:17,280 --> 00:36:19,440
and in some cases it's going to be bad

1022
00:36:19,440 --> 00:36:21,200
so for example

1023
00:36:21,200 --> 00:36:22,800
if you're looking at

1024
00:36:22,800 --> 00:36:23,599
you know

1025
00:36:23,599 --> 00:36:25,359
images

1026
00:36:25,359 --> 00:36:27,920
radiological images of

1027
00:36:27,920 --> 00:36:31,599
you know tumors or people's brains or

1028
00:36:31,599 --> 00:36:33,680
blood vessels

1029
00:36:33,680 --> 00:36:36,240
you want the machine learning model to

1030
00:36:36,240 --> 00:36:38,880
use its

1031
00:36:38,880 --> 00:36:41,040
microscopic vision

1032
00:36:41,040 --> 00:36:43,599
to basically identify things that human

1033
00:36:43,599 --> 00:36:46,560
would not be able to see

1034
00:36:46,560 --> 00:36:47,920
and thus

1035
00:36:47,920 --> 00:36:51,040
give you early signs or early detection

1036
00:36:51,040 --> 00:36:52,400
of these things

1037
00:36:52,400 --> 00:36:54,960
and that would be good

1038
00:36:54,960 --> 00:36:56,480
however

1039
00:36:56,480 --> 00:36:58,400
when you have a cat it's categorized as

1040
00:36:58,400 --> 00:37:00,720
guacamole it's not a good thing

1041
00:37:00,720 --> 00:37:02,880
because you can see if

1042
00:37:02,880 --> 00:37:05,760
you know if an attacker can use this to

1043
00:37:05,760 --> 00:37:06,839
basically

1044
00:37:06,839 --> 00:37:10,720
get a tank categorized as a bus or

1045
00:37:10,720 --> 00:37:12,640
whatever then you're going to have

1046
00:37:12,640 --> 00:37:14,720
issues in the field

1047
00:37:14,720 --> 00:37:16,079
and so

1048
00:37:16,079 --> 00:37:18,240
what we have to do is we have to

1049
00:37:18,240 --> 00:37:19,839
understand that if

1050
00:37:19,839 --> 00:37:21,839
adversarials are very prevalent it means

1051
00:37:21,839 --> 00:37:23,520
that there's something fundamentally

1052
00:37:23,520 --> 00:37:24,560
wrong

1053
00:37:24,560 --> 00:37:26,480
with how machine learning models are

1054
00:37:26,480 --> 00:37:28,640
learning to distinguish

1055
00:37:28,640 --> 00:37:30,960
between different classes

1056
00:37:30,960 --> 00:37:32,400
and what we're doing

1057
00:37:32,400 --> 00:37:34,320
and we need to find a way to help the

1058
00:37:34,320 --> 00:37:36,720
machine learning models to distinguish

1059
00:37:36,720 --> 00:37:39,359
properly and in certain cases like the

1060
00:37:39,359 --> 00:37:42,160
radiological you know example

1061
00:37:42,160 --> 00:37:43,280
you may want

1062
00:37:43,280 --> 00:37:46,240
the current

1063
00:37:46,480 --> 00:37:49,480
setup

1064
00:37:50,560 --> 00:37:52,720
so uh if you have any questions please

1065
00:37:52,720 --> 00:37:55,119
contact me or

1066
00:37:55,119 --> 00:37:56,240
i pronounce the other person that

1067
00:37:56,240 --> 00:37:57,839
usually does this talk with me but he

1068
00:37:57,839 --> 00:38:01,599
was not able to to do this and

1069
00:38:01,599 --> 00:38:05,599
i'm going to open it up to questions

