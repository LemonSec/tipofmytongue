1
00:00:18,470 --> 00:00:26,269
afternoon and Caroline thanks for
joining me I suspect though at this

2
00:00:26,269 --> 00:00:27,529
conference

3
00:00:27,529 --> 00:00:31,550
you know there's a lot of folks who are
responsible for sort of doing

4
00:00:31,550 --> 00:00:35,570
application security and folks are here
to figure out

5
00:00:35,570 --> 00:00:41,059
am i doing the right thing how do i do
something new how AM learn about

6
00:00:41,059 --> 00:00:45,449
something that I might end up doing and
then there's some of us who are

7
00:00:45,449 --> 00:00:49,949
responsible for talking about
application security and and maybe you

8
00:00:49,949 --> 00:00:53,869
find yourself in both of those
situations both doing application

9
00:00:53,869 --> 00:00:58,869
security as well as having to talk about
it and there's all sorts of people that

10
00:00:58,869 --> 00:01:03,420
you might need to talk about it with
maybe your boss may be your boss's boss

11
00:01:03,420 --> 00:01:09,150
may be a regulator or an auditor or a
customer who wants to know what you're

12
00:01:09,150 --> 00:01:15,549
doing and application security as folks
are sort of trickling into the room I

13
00:01:15,549 --> 00:01:21,350
started with the slide from one of
actually our clients my eye I work for a

14
00:01:21,350 --> 00:01:25,690
consulting firm called digital we
specialize in software security and this

15
00:01:25,690 --> 00:01:29,759
is what one of our clients had to say
you know she said a lot of people use

16
00:01:29,760 --> 00:01:34,190
this like red light green light approach
you know that's really a major

17
00:01:34,190 --> 00:01:40,520
over-simplification of what's going on
typically another thing that happens is

18
00:01:40,520 --> 00:01:46,289
that you know we overwhelm people with
data there's a lot of data and if we

19
00:01:46,290 --> 00:01:52,590
present that data sort of unfiltered and
without context then perhaps we confuse

20
00:01:52,590 --> 00:01:57,780
the reader and I'd actually like to
point out what she says at the bottom

21
00:01:57,780 --> 00:02:01,610
here she says you know for most
organizations the red light green light

22
00:02:01,610 --> 00:02:07,590
thing the information overload that
doesn't work but and what if you do

23
00:02:07,590 --> 00:02:12,120
nothing you know then you have little to
no understanding of the effectiveness of

24
00:02:12,120 --> 00:02:16,360
your application security program and
today I'm really going to be talking

25
00:02:16,360 --> 00:02:21,569
about effectiveness of application
security really meaning risk management

26
00:02:21,569 --> 00:02:27,530
and we'll dive into that quite deeply
Caroline i've i've been an information

27
00:02:27,530 --> 00:02:29,210
security now for a little

28
00:02:29,210 --> 00:02:34,960
more than a decade it's funny I actually
the last time I was in Santa Monica I

29
00:02:34,960 --> 00:02:39,670
realized as I was taking a cab from the
airport this morning was when I was in

30
00:02:39,670 --> 00:02:43,769
college and a bunch of my friends are
going to UCLA to reserve a nice a nice

31
00:02:43,770 --> 00:02:49,980
memory after college I i worked in
information security and about a decade

32
00:02:49,980 --> 00:02:55,500
ago found myself working at eBay as the
chief of staff for the CISO at the time

33
00:02:55,500 --> 00:03:00,960
they've colony and dave was sort of one
of those really brilliant leaders who

34
00:03:00,960 --> 00:03:06,570
had this ability to go to the finance
organization and the executive

35
00:03:06,570 --> 00:03:13,750
management of you know eBay and say look
we're not investing enough in security

36
00:03:13,750 --> 00:03:20,690
we have this platform where strangers
transact on the internet securities

37
00:03:20,690 --> 00:03:24,560
really big deal and you guys aren't
putting enough money into it and they

38
00:03:24,560 --> 00:03:27,860
listen to and they gave him a bunch of
money and they gave him the opportunity

39
00:03:27,860 --> 00:03:33,500
to hire a bunch of headcount and he
turned to me and he said Caroline now

40
00:03:33,500 --> 00:03:37,700
that we've got the people and now we've
got the technology and we've got the

41
00:03:37,700 --> 00:03:43,970
projects and place now what we need is
the metrics to ensure ongoing investment

42
00:03:43,970 --> 00:03:49,420
in the program you know he said
executives don't understand security

43
00:03:49,420 --> 00:03:52,799
it's not their job to understand
security it's our job to understand

44
00:03:52,800 --> 00:03:56,460
security and tell them what they need to
know and he said I need them to

45
00:03:56,460 --> 00:04:01,980
understand that this is not a one-time
thing so that's kind of where my

46
00:04:01,980 --> 00:04:07,549
particular interest in the subject
started since that time I led

47
00:04:07,550 --> 00:04:12,530
information security teams that eBay and
that it before transitioning to what I

48
00:04:12,530 --> 00:04:15,920
jokingly referred to as the dark side
when I joined the vendor community

49
00:04:15,920 --> 00:04:20,349
working in product management at
Symantec and for the past two and a half

50
00:04:20,350 --> 00:04:24,550
years during management consulting at
City Hall and I and I really enjoyed

51
00:04:24,550 --> 00:04:29,340
consulting for me it's it's very cool
cuz I get to work with clients to solve

52
00:04:29,340 --> 00:04:31,289
really interesting problems

53
00:04:31,289 --> 00:04:38,128
mostly in how to measure for security
initiatives or how to build them or how

54
00:04:38,129 --> 00:04:44,099
to improve them so today we're going to
talk about some questions that

55
00:04:44,099 --> 00:04:50,229
executives may ask our gonna talk about
abstract capabilities and metrics that

56
00:04:50,229 --> 00:04:55,159
are associated with those capabilities
I'm gonna tell you some stories about

57
00:04:55,159 --> 00:05:00,930
metrics and areas that i've seen our
clients and not all of which are you

58
00:05:00,930 --> 00:05:05,699
know sort of optimized and then I'm
gonna tell you about my approach to

59
00:05:05,699 --> 00:05:11,580
developing key metrics we're gonna go
through a very detailed example so

60
00:05:11,580 --> 00:05:17,818
questions from executives more often
than not company executives ask the

61
00:05:17,819 --> 00:05:23,770
wrong questions about AB sec because
company executives are on a plane

62
00:05:23,770 --> 00:05:27,389
you know they pick up hemispheres
magazine or fortune and and they read

63
00:05:27,389 --> 00:05:31,189
this blurb you know and they're like oh
you know a jeep got hacked Ashley

64
00:05:31,189 --> 00:05:34,469
Madison got hacked you know what does
this mean for us and then they think

65
00:05:34,469 --> 00:05:39,830
about the way in which they measure
other parts of their business and they

66
00:05:39,830 --> 00:05:45,539
trying to plan out to upset and it
doesn't quite fit so if an executive

67
00:05:45,539 --> 00:05:51,279
said you how does our bug count compared
to that of our competitors that might

68
00:05:51,279 --> 00:05:57,039
not only be a difficult question to
answer but perhaps you know the folks in

69
00:05:57,039 --> 00:06:00,759
this room know that that's that's really
the wrong question to ask you know but

70
00:06:00,759 --> 00:06:06,209
but how how awkward is that to sit in
front of potentially your CEO you know

71
00:06:06,209 --> 00:06:09,399
maybe your vice president risk you know
somebody was kind of a big deal the

72
00:06:09,399 --> 00:06:13,099
company and that's not the right
question asked but it is and the reason

73
00:06:13,099 --> 00:06:18,808
is because as we know you know my apt is
probably different from my competitors

74
00:06:18,809 --> 00:06:21,289
up maybe it's written in different
language maybe it's it's on a different

75
00:06:21,289 --> 00:06:25,459
technology stack maybe you know we have
very different philosophies when it

76
00:06:25,459 --> 00:06:29,569
comes to the use of free and open source
software for all sorts of reasons number

77
00:06:29,569 --> 00:06:34,009
one are apps are probably different
number to our application security

78
00:06:34,009 --> 00:06:38,339
activities are probably very different
you know maybe I have an extremely

79
00:06:38,339 --> 00:06:43,159
mature static analysis program that I
apply to our entire code base and my

80
00:06:43,159 --> 00:06:43,830
competitor

81
00:06:43,830 --> 00:06:48,210
ur does pen testing only on their
critical applications once a year

82
00:06:48,210 --> 00:06:52,979
you know there's just there's no way to
say our bug count there but count

83
00:06:52,980 --> 00:06:55,390
anything useful

84
00:06:55,390 --> 00:07:01,979
another question you know an exact asks
because they don't know and again it's

85
00:07:01,980 --> 00:07:07,060
not their job to know might be what's
our mean time to recover from a security

86
00:07:07,060 --> 00:07:12,320
and so you know I'm maybe the intent is
maybe we can make that better and we all

87
00:07:12,320 --> 00:07:17,670
want to recover from security incidents
better but probably if you have an

88
00:07:17,670 --> 00:07:21,810
incident response plan in place if you
know who to call for what kind of

89
00:07:21,810 --> 00:07:27,130
scenario and maybe you're doing tabletop
exercises to practice various types of

90
00:07:27,130 --> 00:07:33,469
incidents you know beyond that the
incident is beyond your control and so

91
00:07:33,470 --> 00:07:37,710
this is just it it's something you can't
accuse Iran have control over it

92
00:07:37,710 --> 00:07:41,169
there's not gonna be any consistency and
gathering this type of data

93
00:07:42,680 --> 00:07:48,620
what about when they're asking some of
the right questions we've invested so

94
00:07:48,620 --> 00:07:54,070
much money into the absurd program
what's the impact on a risk posture what

95
00:07:54,070 --> 00:07:58,300
value are getting out of the dollar
spent these are actually really legit

96
00:07:58,300 --> 00:08:02,620
questions to ask but they're not easy to
answer and so today I want to talk about

97
00:08:02,620 --> 00:08:05,750
an approach to answering these types of
questions

98
00:08:06,950 --> 00:08:12,330
planning on how you're feeling this
could be like a guy talk to you kind of

99
00:08:12,330 --> 00:08:15,659
talk or this could be illegal we talk to
each other and it's completely up to you

100
00:08:15,660 --> 00:08:21,830
because I have signed up to talk if you
want I'm curious to know what kinds of

101
00:08:21,830 --> 00:08:25,710
questions folks might be getting from
from their executives you know that

102
00:08:25,710 --> 00:08:31,900
might be easier hard to answer anybody
talking executives and want to share

103
00:08:31,900 --> 00:08:34,900
talking to regulators share yes

104
00:08:36,140 --> 00:08:41,809
I we've honorable to heart bleed maybe
because they read about it in the news

105
00:08:41,809 --> 00:08:46,060
maybe because their CIO buddy is worried
about heart played what are you doing

106
00:08:46,060 --> 00:08:51,640
about hardly which of course we know
sorry I do I want to talk to you next

107
00:08:51,640 --> 00:08:56,819
which of course we know is a software
application security is not just hurt

108
00:08:56,820 --> 00:08:59,820
you know there's so much more about it
yes

109
00:09:03,529 --> 00:09:10,720
yeah because because I mean so this is
interesting maybe my sister is a

110
00:09:10,720 --> 00:09:15,120
pediatrician and I am the new mom of a
10 month old baby girl and so when

111
00:09:15,120 --> 00:09:19,459
something happens I will say to my
sister on a scale of one to 10 how

112
00:09:19,459 --> 00:09:23,329
worried do I need to be you know when
it's kind of the same thing but but but

113
00:09:23,329 --> 00:09:27,748
but maybe not you know there's there's a
more detailed conversation that needs to

114
00:09:27,749 --> 00:09:31,829
take place in maybe you give an answer
but then what you know you're like well

115
00:09:31,829 --> 00:09:35,349
over 300 time in their life well why you
know why that you know like so how does

116
00:09:35,350 --> 00:09:40,509
the rest of the conversation any other
questions from executives that might or

117
00:09:40,509 --> 00:09:42,399
might not be the right ones

118
00:09:42,399 --> 00:09:53,410
why does it keeps the same things keep
happening to us again and again and

119
00:09:53,410 --> 00:09:59,009
again you know some of that in our
control some of its not a hundred

120
00:09:59,009 --> 00:10:05,350
percent secure thats the thing not
really at all so you know there's

121
00:10:05,350 --> 00:10:10,519
there's a lot of opportunity for
education and it's unreasonable for us

122
00:10:10,519 --> 00:10:14,439
to expect that our executives understand
application security that's not their

123
00:10:14,439 --> 00:10:21,110
job that's our job but they do trust us
we hope to help educate them and to do

124
00:10:21,110 --> 00:10:27,050
the right thing thank you and we like
one of those we talked on sessions

125
00:10:28,689 --> 00:10:33,519
why do you need metrics why would you
consider doing metrics executives want

126
00:10:33,519 --> 00:10:34,250
to know

127
00:10:34,250 --> 00:10:38,600
bottom line they want to know about risk
management and and today is talk is all

128
00:10:38,600 --> 00:10:44,339
about how do you talk to them about
absurd and risk management the easy way

129
00:10:44,339 --> 00:10:50,470
the simple way perhaps one simple way is
to say look good software helps our

130
00:10:50,470 --> 00:10:57,990
business bad software hurts our business
and if we can try and do things to make

131
00:10:57,990 --> 00:11:02,589
software good and prevent software from
being bad thats risk management and

132
00:11:02,589 --> 00:11:09,019
that's a really that's actually a really
good place to start but then what how do

133
00:11:09,019 --> 00:11:12,470
we know we're doing the right things how
do we know if we're not doing enough

134
00:11:12,470 --> 00:11:15,220
when I was single

135
00:11:15,220 --> 00:11:19,199
we had a lot of really smart people on
the team and we were doing a lot of

136
00:11:19,199 --> 00:11:26,079
really smart things are CIO said I know
you guys are smart how do I know you're

137
00:11:26,079 --> 00:11:30,219
doing the right things and and you know
we didn't have a great answer for her

138
00:11:30,220 --> 00:11:33,860
and so she said I want I want us to
follow the ISO framework right so one

139
00:11:33,860 --> 00:11:37,879
way is to choose an industry best
practice our framework can say look

140
00:11:37,879 --> 00:11:43,220
we're doing that that's not what I'm
here to talk about what I'm here to talk

141
00:11:43,220 --> 00:11:48,209
about is starting with risk management
objectives so if risk management for

142
00:11:48,209 --> 00:11:55,000
application security has to do with
making software good and not bad there's

143
00:11:55,000 --> 00:11:58,879
a lot that comes underneath it and I
actually have like a twenty point they

144
00:11:58,879 --> 00:12:03,290
were going to get to you later in the
flight but maybe you start out with

145
00:12:03,290 --> 00:12:08,879
saying hey exact here are some of our
risk management objectives that are more

146
00:12:08,879 --> 00:12:14,680
detailed than we want to make good
software about software then we can

147
00:12:14,680 --> 00:12:18,500
think about those risk management
objectives and we can ask questions

148
00:12:18,500 --> 00:12:23,850
about how we get there and finally we
can answer those questions with data

149
00:12:23,850 --> 00:12:28,180
based on whatever it is we're doing or
or may be based on what we planned to be

150
00:12:28,180 --> 00:12:32,758
doing in the future and maybe we could
even use this to help us plan what we're

151
00:12:32,759 --> 00:12:39,290
gonna do in the future so we're gonna do
some vocabulary measurement vs metric

152
00:12:39,290 --> 00:12:41,480
what's the difference

153
00:12:41,480 --> 00:12:51,490
AV Security Agency so is you know what
they do in europe you remember things

154
00:12:51,490 --> 00:12:57,470
and inches so ha a measurement is a
value right it's probably like 70

155
00:12:57,470 --> 00:13:02,759
degrees I had one cup of coffee this
morning which was not enough

156
00:13:03,319 --> 00:13:08,849
a metric is the aggregation of one or
more measurements a lot of metrics are

157
00:13:08,850 --> 00:13:14,990
going to be ratios to create a piece of
business intelligence so a metric should

158
00:13:14,990 --> 00:13:20,680
answer a question a metric should
support a decision and a metric probably

159
00:13:20,680 --> 00:13:24,510
has some environmental context I was
what's the point

160
00:13:24,510 --> 00:13:28,550
so here's where we could talk about
metrics if you guys want is anyone

161
00:13:28,550 --> 00:13:33,780
interested in sharing the types of
metrics that they used today if if there

162
00:13:33,780 --> 00:13:46,650
are any number of bugs found time to
patch

163
00:13:51,769 --> 00:13:57,269
percentage of systems unpatched vs
packed these are all good numbers speak

164
00:13:57,269 --> 00:14:09,689
collecting and the last one is even kind
of a ratio percentage of developers went

165
00:14:09,689 --> 00:14:17,899
through security training percentage
that's cool thank you harder question

166
00:14:17,899 --> 00:14:22,869
maybe what types of questions and
decisions do these measurements help to

167
00:14:22,869 --> 00:14:25,869
support

168
00:14:34,170 --> 00:14:37,529
what we do next

169
00:14:38,029 --> 00:14:43,720
how do we expand our training coverage
what are you going to give me more money

170
00:14:43,720 --> 00:14:55,500
to do next year so there's lots of
different visas to doing metrics first

171
00:14:55,500 --> 00:14:59,199
you have to figure out what they're
gonna be used to define what they are

172
00:14:59,199 --> 00:15:04,120
and then you get some data may be put in
a dashboard and then you and then you

173
00:15:04,120 --> 00:15:08,050
talk to people about it on today we're
really talking about this first part I

174
00:15:08,050 --> 00:15:12,819
think it's really important to get the
first part right and to figure out how

175
00:15:12,820 --> 00:15:16,949
to how do you define your metric the
rest of it there's there's all sorts of

176
00:15:16,949 --> 00:15:21,300
interesting things about phase 2 and
phase three but today we're talking

177
00:15:21,300 --> 00:15:31,949
about this one so this is a slide that I
put together and it's just you know a

178
00:15:31,949 --> 00:15:39,899
way that people grow a knapsack a lot of
folks start out looking for and finding

179
00:15:39,899 --> 00:15:46,070
defects maybe they evolved to a point
where they require that you do some

180
00:15:46,070 --> 00:15:50,470
defect discovery may be certain types of
different discovery are required at

181
00:15:50,470 --> 00:15:55,029
different times under different
situations you know for example high

182
00:15:55,029 --> 00:16:00,470
risk apps need to have me or defect
discovery low risk apps you need to have

183
00:16:00,470 --> 00:16:05,380
less maybe certain types of defects
covering me but need to happen on a

184
00:16:05,380 --> 00:16:06,980
periodic basis whether that be

185
00:16:06,980 --> 00:16:09,980
quarterly or year or once every three
years

186
00:16:11,019 --> 00:16:14,620
you know some organizations once they're
looking for stuff

187
00:16:14,620 --> 00:16:20,880
finding it requiring that you look for
stuff and find you know get to a point

188
00:16:20,880 --> 00:16:24,350
where they're fixing it I think a lot of
us in this room know that those are

189
00:16:24,350 --> 00:16:29,860
those are not always not always a fair
assumption to say they just cause we're

190
00:16:29,860 --> 00:16:34,110
looking for problems were also fixing
them but some organizations you know

191
00:16:34,110 --> 00:16:35,300
grow

192
00:16:35,300 --> 00:16:38,569
along this maturity curving get to a
place where they're fixing them and then

193
00:16:38,570 --> 00:16:41,649
finally you know some people are
preventing defects and this is not

194
00:16:41,649 --> 00:16:47,310
necessarily like this is the way or this
is the one way but these are some ways

195
00:16:47,310 --> 00:16:55,189
in which application security programs
grow and correspondingly you know I

196
00:16:55,190 --> 00:16:58,890
think that metrics should change and
should grow accordingly so if you're

197
00:16:58,890 --> 00:17:04,670
looking for defects you know maybe it's
interesting to say well how many of our

198
00:17:04,670 --> 00:17:09,140
development teams are participating in
whatever defect discovery how much

199
00:17:09,140 --> 00:17:14,530
coverage we have you know later when
you're requiring it of people you know

200
00:17:14,530 --> 00:17:17,410
who is who is meeting our requirements

201
00:17:17,410 --> 00:17:22,900
you know if we have requirements around
fixes you know maybe you have you know

202
00:17:22,900 --> 00:17:28,580
remediation time frames that are
associated with severity of defects and

203
00:17:28,580 --> 00:17:32,939
then you know you could get to things
like effectiveness we've got a

204
00:17:32,940 --> 00:17:36,840
penetration testing capability how
effective is that capability was that

205
00:17:36,840 --> 00:17:41,770
means for us and then finally risk
prevention metrics you know maybe we've

206
00:17:41,770 --> 00:17:46,460
got like a top end bugs Louis you know
these are the top classes of bugs that

207
00:17:46,460 --> 00:17:50,740
we tend to find in our code base maybe
we'll take the top one of the top two

208
00:17:50,740 --> 00:17:56,040
and we're really going to target or
training you know to teach engineers you

209
00:17:56,040 --> 00:18:01,090
know this is a type of bug that you need
to know about you need to know how not

210
00:18:01,090 --> 00:18:05,428
introduced in Unicode if it is if you
find it you know this is how you fix it

211
00:18:05,429 --> 00:18:12,570
and maybe you even do things like builds
custom to find it automatically with

212
00:18:12,570 --> 00:18:21,750
tools so I don't like headers and here
but the next section of this talk I'm

213
00:18:21,750 --> 00:18:28,490
gonna talk about some example certain
areas that I find people and when they

214
00:18:28,490 --> 00:18:33,030
do when they're doing metrics and maybe
the user for the most part not the ideal

215
00:18:33,030 --> 00:18:42,350
scenarios one of them is when something
bad happens and the first as this is an

216
00:18:42,350 --> 00:18:46,370
executive question right something bad
happens in the executive wants to know

217
00:18:46,370 --> 00:18:47,959
what happened

218
00:18:47,960 --> 00:18:52,510
how do we stop it you know and and what
do I tell people about it you know maybe

219
00:18:52,510 --> 00:18:57,410
it's in the news maybe you are regulator
has a lot of questions for us maybe

220
00:18:57,410 --> 00:19:03,200
internal audit has a lot of questions
for us what do I tell people and then

221
00:19:03,200 --> 00:19:07,940
and then the response is usually well
this is what we're gonna do in the

222
00:19:07,940 --> 00:19:12,100
future and we're gonna do in the future
is usually some sort of minimum baseline

223
00:19:12,100 --> 00:19:17,610
you know here's the standard of due care
you know maybe we're gonna penned test

224
00:19:17,610 --> 00:19:22,280
all of our apps that have regulated
information in it you know once a

225
00:19:22,280 --> 00:19:26,930
quarter and they were just gonna do that
and then and then there becomes sort of

226
00:19:26,930 --> 00:19:32,130
this idea that maybe app seconds only
hurting the most critical absence

227
00:19:32,130 --> 00:19:39,160
quarter that's it so that's one scenario
and maybe you know every quarter

228
00:19:39,160 --> 00:19:43,750
executive says you know I have the
pretty graph that shows that we in fact

229
00:19:43,750 --> 00:19:49,600
our pen testing of our most critical
abscess quarter you know sure but but

230
00:19:49,600 --> 00:19:56,600
it's a really useful it's not really
managing risk better scenario the

231
00:19:56,600 --> 00:20:02,709
scenario or two is what I call vanity
metrics so in this case folks are

232
00:20:02,710 --> 00:20:09,410
counting things so for example you know
we've got eight people on our team we

233
00:20:09,410 --> 00:20:15,070
spent a hundred and nine percent of this
year's budget and last week we found 12

234
00:20:15,070 --> 00:20:22,919
bugs ok so I you know this app you know
we found sixty facts and on a scale of

235
00:20:22,920 --> 00:20:32,910
one to five each level 23 Tabata 12 and
thats yellow colored you know a lot of

236
00:20:32,910 --> 00:20:36,100
people find themselves in this situation
because they're like I wanna talk about

237
00:20:36,100 --> 00:20:41,050
my program i wanna talk about it in a
quantitative manner but maybe what's

238
00:20:41,050 --> 00:20:46,740
missing is some context you know and
some meaning and and

239
00:20:46,740 --> 00:20:52,990
and something that could be used to
drive action in this situation gets

240
00:20:52,990 --> 00:20:58,530
pretty bad which sometimes it's an app
seconds counting a lot of things I've

241
00:20:58,530 --> 00:21:04,899
worked with clients who produce like 300
slide PowerPoint decks full of numbers

242
00:21:04,900 --> 00:21:10,510
and they're measuring everything but
they're not really saying that much and

243
00:21:10,510 --> 00:21:15,540
so without context maybe there's not
understanding on the part of the reader

244
00:21:15,540 --> 00:21:20,200
or the executive that you're giving us
300 page PowerPoint doctor who has time

245
00:21:20,200 --> 00:21:24,990
to read three pages PowerPoint much less
you know every quarter of a year

246
00:21:24,990 --> 00:21:30,080
probably not probably it's not be read
maybe it's just being ignored and the

247
00:21:30,080 --> 00:21:38,889
other thing about sort of scenarios to
an to be without contacts and sort of a

248
00:21:38,890 --> 00:21:45,130
narrative to the company's some of these
numbers you know what could happen is

249
00:21:45,130 --> 00:21:49,640
the executive of where you're talking to
my respond positively or they might

250
00:21:49,640 --> 00:21:54,520
respond negatively how do I know the 12
defects is good thing or bad thing I I

251
00:21:54,520 --> 00:21:59,280
have no idea I have no idea how to react
and I'm just going to react and then

252
00:21:59,280 --> 00:22:00,870
we'll go from there

253
00:22:00,870 --> 00:22:06,860
not ideal and then you know maybe this
goes on for long enough that you get to

254
00:22:06,860 --> 00:22:11,570
a point where you know AB seconds
talking about stuff and it's just being

255
00:22:11,570 --> 00:22:16,210
totally turned out you know I've worked
with clients who say you know we've got

256
00:22:16,210 --> 00:22:19,860
metrics and where we're talking to
people about it you know we're giving it

257
00:22:19,860 --> 00:22:23,840
to our executive committee or giving it
to enter on it and we'd like you to come

258
00:22:23,840 --> 00:22:27,649
in and take a look at our metrics and
give us some advice you know one of the

259
00:22:27,650 --> 00:22:30,520
things I like to do is go talk to
internal audit and go talk to the

260
00:22:30,520 --> 00:22:34,690
Executive Committee and say so what
metrics are you getting from AB second

261
00:22:34,690 --> 00:22:38,620
there like we're not gonna take a nap
circus-like we send them this report

262
00:22:38,620 --> 00:22:44,360
every week it's in their email you know
but they're not reading it or maybe you

263
00:22:44,360 --> 00:22:49,979
know in a not great scenario which
actually happened to my client who had

264
00:22:49,980 --> 00:22:52,380
that quote on the first page

265
00:22:52,380 --> 00:22:57,130
maybe they just stopped attending
meetings and they just don't show up

266
00:22:57,130 --> 00:23:00,520
because I like why would I go to this
meeting I have no idea no I don't know

267
00:23:00,520 --> 00:23:05,550
what they're saying it doesn't make
sense to me and then finally you know

268
00:23:05,550 --> 00:23:12,570
maybe maybe if you are being effective
at doing metrics you know there's

269
00:23:12,570 --> 00:23:16,800
there's an opportunity to to go to
executives and say look here's our plan

270
00:23:16,800 --> 00:23:22,230
here's our progress against that plan
I'm gonna let you know if there are any

271
00:23:22,230 --> 00:23:28,280
issues or any risks you know hey we
found 9 critical bugs this month that

272
00:23:28,280 --> 00:23:33,720
was expected because we just rolled out
a new defect discovery capability we

273
00:23:33,720 --> 00:23:37,750
consider that to be acceptable because
the bugs were found in development and

274
00:23:37,750 --> 00:23:43,310
on in production and median remediation
tasks have been assigned and it looks

275
00:23:43,310 --> 00:23:46,830
like all the bugs are going to be fixed
within the recommended time so we found

276
00:23:46,830 --> 00:23:52,340
9 critical bugs but we're letting you
know that we have a plan and that that

277
00:23:52,340 --> 00:23:58,939
was a good thing we found them in the
first place so it sort of in the

278
00:23:58,940 --> 00:24:04,570
beginning of this talk I talked about
risk management risk management is about

279
00:24:04,570 --> 00:24:08,960
making good software because good
software makes better business and

280
00:24:08,960 --> 00:24:14,290
avoiding bad software because bad
software hurts the business and so how

281
00:24:14,290 --> 00:24:22,600
do we get to more detailed than good
software good bad software bad we

282
00:24:22,600 --> 00:24:26,270
thought about this a lot and so we have
like 20 or so

283
00:24:26,790 --> 00:24:30,920
risk management objectives that I want
to share with you the way that we talked

284
00:24:30,920 --> 00:24:33,900
about the way that I think about metrics
the way that social things about metrics

285
00:24:33,900 --> 00:24:39,470
is to start with these objectives and
then to look at ok what we doing you

286
00:24:39,470 --> 00:24:43,550
know against those objectives and the
news that to say we'll hear some metrics

287
00:24:43,550 --> 00:24:47,350
ok so we talk about this

288
00:24:48,679 --> 00:24:55,700
so some objectives and on this line
we've got six objectives so you could go

289
00:24:55,700 --> 00:24:59,710
to your exact and maybe you know you
care about all these six maybe you don't

290
00:24:59,710 --> 00:25:05,460
but here six that you can pick from and
talk to that that's that's more detailed

291
00:25:05,460 --> 00:25:10,440
ban good software good bad software bad
so maybe you can go to your executive

292
00:25:10,440 --> 00:25:15,659
and say look we are not appropriately
managing absurd risk if we are not doing

293
00:25:15,659 --> 00:25:19,730
the following things and this particular
slide is all about visibility into the

294
00:25:19,730 --> 00:25:24,990
portfolio because most organizations
have more than one at most organizations

295
00:25:24,990 --> 00:25:30,029
have more than one piece of software and
for the abstract people you know

296
00:25:30,029 --> 00:25:35,730
sometimes there is focus on like a
critical high risk god's what about all

297
00:25:35,730 --> 00:25:39,480
the rest of the apps or maybe there's
just one more out but if there are apps

298
00:25:39,480 --> 00:25:44,889
that were not doing anything to from a
software security perspective maybe

299
00:25:44,889 --> 00:25:50,649
that's something to be concerned about
even if the thing that we do is to

300
00:25:50,649 --> 00:25:55,739
evaluate that piece of software
designated as low risk and decide that

301
00:25:55,740 --> 00:26:00,710
you're not going to do anything but this
says if we're gonna do risk management

302
00:26:00,710 --> 00:26:05,230
then maybe we should know all the
software portfolio maybe we should know

303
00:26:05,230 --> 00:26:11,289
you know and maybe that includes you
know open source software maybe we

304
00:26:11,289 --> 00:26:15,679
should be able to enumerate our
applications in our databases and maybe

305
00:26:15,679 --> 00:26:21,039
that includes no way world a sensitive
information is stored or process maybe

306
00:26:21,039 --> 00:26:27,210
we should make sure that we know what
the appropriate security posture is for

307
00:26:27,210 --> 00:26:33,899
every application maybe we should for
every ask every software asset have a

308
00:26:33,899 --> 00:26:39,168
risk ranking assigned for every software
project have an impact rating assigned

309
00:26:39,169 --> 00:26:44,009
for every software security defect have
a severely rating designed for everyday

310
00:26:44,009 --> 00:26:48,799
tasks that have a have a classification
assigned maybe we need to be managing

311
00:26:48,799 --> 00:26:52,519
risk across the entire portfolio and not
just this tiny part of our portfolio

312
00:26:52,519 --> 00:26:57,149
that's the critical apps so that we can
provide a complete risk picture to

313
00:26:57,149 --> 00:26:58,610
executive management

314
00:26:58,610 --> 00:27:02,280
and I'm not saying that if you're not
doing this you're not doing your job

315
00:27:02,280 --> 00:27:07,610
application security is this thing that
grows and matures but maybe this is

316
00:27:07,610 --> 00:27:11,939
something that we can strive to cause
we're talking about a risk management

317
00:27:11,940 --> 00:27:16,910
objective then then maybe we can set
that bar quite high and then put

318
00:27:16,910 --> 00:27:22,700
together a plan to get there the next
set of risk management objectives have

319
00:27:22,700 --> 00:27:28,850
to do with a secure software development
lifecycle so maybe every project goes

320
00:27:28,850 --> 00:27:33,860
through the secure software development
lifecycle maybe we're making sure that

321
00:27:33,860 --> 00:27:38,370
we're doing appropriate levels of defect
discovery and that you know appropriate

322
00:27:38,370 --> 00:27:44,090
is sort of this term that allows you to
not have to do everything to everything

323
00:27:44,090 --> 00:27:51,139
but you can apply various controls and a
risk-based manner you want to make sure

324
00:27:51,140 --> 00:27:56,580
that your defects are documented and
that their fixed and that variances any

325
00:27:56,580 --> 00:28:03,100
exceptions to the mediating the effects
are documented attract we'd like to tune

326
00:28:03,100 --> 00:28:07,678
our secure a steel see so that we're not
pissing off engineering and so that

327
00:28:07,679 --> 00:28:13,690
we're going as fast as engineering needs
us to go we want to move efforts left in

328
00:28:13,690 --> 00:28:17,429
the secure software development
lifecycle so that we're not using our

329
00:28:17,429 --> 00:28:23,270
prevention efforts to analyze the risks
associated with hundreds of medium

330
00:28:23,270 --> 00:28:27,770
security defects in production you know
a lot of organizations say well we're

331
00:28:27,770 --> 00:28:32,570
gonna fix the critical defects if we
have time to fix the high defects you

332
00:28:32,570 --> 00:28:36,530
know but as far as medium or low
severity defects go we're not going to

333
00:28:36,530 --> 00:28:40,980
get to that in 10 years there's just not
enough time you know but maybe if we are

334
00:28:40,980 --> 00:28:45,520
holding ourselves to a really high bar
when we're talking about risk management

335
00:28:45,520 --> 00:28:50,660
objectives then we could even think
about considering if there's like

336
00:28:50,660 --> 00:28:55,080
hundreds of mediums very defects are not
doing anything about that you know she

337
00:28:55,080 --> 00:28:56,559
think about that

338
00:28:56,559 --> 00:29:04,610
using threatened attack intelligence to
continually improve our secure SDLC so

339
00:29:04,610 --> 00:29:08,439
we've talked about this a lot like a lot
of objections to talk about

340
00:29:09,370 --> 00:29:13,899
this next set has to do with policies
standards in our outreach so what sort

341
00:29:13,900 --> 00:29:19,380
of expected behavior how do we talk to
people internally about what we're doing

342
00:29:19,380 --> 00:29:26,250
we may not be appropriately managing the
risk if we don't have policies and

343
00:29:26,250 --> 00:29:30,470
standards if we're not incorporating
every stakeholder and the software

344
00:29:30,470 --> 00:29:34,150
security strategy is so maybe we're not
just talking about developers maybe

345
00:29:34,150 --> 00:29:40,270
we're talking about QA folks architects
project managers business analysts you

346
00:29:40,270 --> 00:29:46,280
know I T folks who were deploying the
software environments are applications

347
00:29:46,280 --> 00:29:51,090
gonna sit on or not appropriately
managing our application security risk

348
00:29:51,090 --> 00:29:55,889
for not reaching out to talk to
executives about it and talking to all

349
00:29:55,890 --> 00:29:59,340
the rest of the stakeholders about it
maybe you know less technical for

350
00:29:59,340 --> 00:30:03,559
executives more technical for everyone
else and ensuring that all stakeholders

351
00:30:03,559 --> 00:30:12,559
have the appropriate level of training
and I and we when it comes to risk

352
00:30:12,559 --> 00:30:18,040
management of application security you
know there's there's other things other

353
00:30:18,040 --> 00:30:21,590
than the application other than the
software that we have to worry about

354
00:30:21,590 --> 00:30:26,870
sometimes this offer environment making
sure that you know the IIT team whose

355
00:30:26,870 --> 00:30:30,669
deploying the software environment the
my application sits on is not screwing

356
00:30:30,670 --> 00:30:34,730
things up for me and also making sure
that the software vendors that I work

357
00:30:34,730 --> 00:30:40,800
with you provide me with software are
not screen comes up to me and finally

358
00:30:40,800 --> 00:30:46,639
you know making sure that you know if
we're really talking about risk

359
00:30:46,640 --> 00:30:47,540
management

360
00:30:47,540 --> 00:30:52,000
we're going beyond compliance and maybe
we even have two metrics to talk about

361
00:30:52,000 --> 00:30:57,350
what we're doing so the next thing once
we need to figure out what our risk

362
00:30:57,350 --> 00:31:02,010
management objectives are going to be
might be just say what we're doing today

363
00:31:02,010 --> 00:31:07,650
and what we gonna do tomorrow and this
is for anyone who saw dole talked

364
00:31:07,650 --> 00:31:08,990
earlier this morning

365
00:31:08,990 --> 00:31:13,530
sort of 20 software security
capabilities what are some things that

366
00:31:13,530 --> 00:31:16,250
you could do for a pSET

367
00:31:16,250 --> 00:31:20,220
it's not just paint are saying you know
pen testing being one of i think we've

368
00:31:20,220 --> 00:31:25,530
got like six or so defect discovery
methods listed here but there's other

369
00:31:25,530 --> 00:31:29,950
stuff that people do a nap SEC and
here's a list of all the things we could

370
00:31:29,950 --> 00:31:37,840
think of so here is where the rubber
meets the road and we talked about 20 or

371
00:31:37,840 --> 00:31:44,399
so upset risk management objectives I'm
taking one of those so this one says

372
00:31:44,400 --> 00:31:48,490
we're not appropriately managing apps
suck risk if we are not guiding every

373
00:31:48,490 --> 00:31:52,490
software project as secure as you'll see
that the term that determines whether

374
00:31:52,490 --> 00:31:58,370
the software is acceptable scare so why
is the secure as DLC you know out of

375
00:31:58,370 --> 00:32:04,129
very bare minimum maybe there are two
dates on our secure SDLC you one that

376
00:32:04,130 --> 00:32:10,110
says permit to build and one that says
permit to deploy maybe the permit to

377
00:32:10,110 --> 00:32:15,479
build gate so you know we're going to
evaluate the risk of this particular

378
00:32:15,480 --> 00:32:20,440
software outside and depending on if
it's high risk medium risk low risk you

379
00:32:20,440 --> 00:32:24,240
know we're going to dictate that certain
types of controls be applied maybe more

380
00:32:24,240 --> 00:32:27,990
controls for the high risk apps may be
fewer controls for Lois Capps may be

381
00:32:27,990 --> 00:32:32,860
different controls may be cheaper
controls for low-risk apps and then

382
00:32:32,860 --> 00:32:37,629
you've got your permit to deploy gate
that says well you know if you did some

383
00:32:37,630 --> 00:32:42,280
threat modeling or some architectural
risk analysis and you identified some

384
00:32:42,280 --> 00:32:46,230
flaws you know did you actually fix
those you know we want to check that

385
00:32:46,230 --> 00:32:50,960
before we go live or if you found some
bugs in static analysis and you're

386
00:32:50,960 --> 00:32:54,850
supposed to fix them you know we want to
make sure those pics before we go live

387
00:32:54,850 --> 00:32:59,740
if you were supposed to use some secure
libraries or or adhere to some secure

388
00:32:59,740 --> 00:33:06,140
quirements you want to make sure so
secure a steel see sort of a minimum

389
00:33:06,140 --> 00:33:12,010
perhaps is a permit to build and a
permit to deploy checkpoint so that's

390
00:33:12,010 --> 00:33:17,510
just one example of how you could do it
so given this risk management objective

391
00:33:17,510 --> 00:33:20,190
of which we talked about 20

392
00:33:20,190 --> 00:33:27,879
here I'm gonna share with you maybe five
or so questions asked about that risk

393
00:33:27,879 --> 00:33:33,100
management objective and the data that
answers these questions hopefully

394
00:33:33,100 --> 00:33:38,549
actually begins to pick paint a picture
of what's our risk situation and her

395
00:33:38,549 --> 00:33:43,370
when managing that risk in a way that
more interesting than good software good

396
00:33:43,370 --> 00:33:48,178
bad software bad how many what
percentage of the applications in the

397
00:33:48,179 --> 00:33:52,429
portfolio have been reviewed and signed
off indicating an unacceptable level of

398
00:33:52,429 --> 00:33:59,259
security because we're talking a
percentage got a ratio and there's also

399
00:33:59,259 --> 00:34:03,820
ways to kind of slice and dice that data
so that you can really zeroing in on

400
00:34:03,820 --> 00:34:07,009
problem areas maybe you could look at it

401
00:34:07,009 --> 00:34:11,179
according to different risk rankings of
applications or maybe just one risk

402
00:34:11,179 --> 00:34:16,020
ranking what about just are critical
risk applications per for various

403
00:34:16,020 --> 00:34:19,918
technology stacks across different
business unit four different software

404
00:34:19,918 --> 00:34:23,888
project types you know internally
developed comments you know free and

405
00:34:23,889 --> 00:34:30,329
open source software so the idea being
that you've got your risk management

406
00:34:30,329 --> 00:34:34,649
objective you got a question about that
risk management objective and if you're

407
00:34:34,649 --> 00:34:39,969
able to gather the data to answer that
question and slice and dice then maybe

408
00:34:39,969 --> 00:34:43,259
you've got something that can tell you
about what to do next

409
00:34:44,679 --> 00:34:50,119
another question for this you know every
piece of software should go through as I

410
00:34:50,119 --> 00:34:55,109
still see how many of the software
projects in the last 12 months have been

411
00:34:55,109 --> 00:35:00,440
reviewed and signed off how many of them
haven't and then we can again

412
00:35:00,440 --> 00:35:05,329
slice and dice that according to risk
ranking technologies tank business units

413
00:35:05,329 --> 00:35:09,400
offer project time and hopefully zero in
on areas that we want to focus on the

414
00:35:09,400 --> 00:35:15,849
future or you know get some visibility
into potentially glaring holes in our

415
00:35:15,849 --> 00:35:20,240
response to her if we decided that you
know we want to live up to this risk

416
00:35:20,240 --> 00:35:25,939
management objective what percentage of
software projects in the last 12 months

417
00:35:25,940 --> 00:35:30,609
did not go through the security I'll see
and then slice and dice

418
00:35:32,170 --> 00:35:37,240
what percentage of software projects in
the last 12 months have passed all

419
00:35:37,240 --> 00:35:41,770
software security checkpoints so you
know one example of software security

420
00:35:41,770 --> 00:35:47,240
checkpoints is that permit to build
permit to deploy gate but you know

421
00:35:47,240 --> 00:35:53,899
there's all sorts of different kinds
what percentage of the applications have

422
00:35:53,900 --> 00:36:02,130
one or more open exceptions for not
passing and as a security gate and I

423
00:36:02,130 --> 00:36:07,369
think there's a las 14 today's
discussion for each security checkpoint

424
00:36:07,369 --> 00:36:11,839
in the security I'll see what's the
average percentage of artifacts provided

425
00:36:11,839 --> 00:36:17,328
vs expected for all the software project
last four months so the point here is

426
00:36:17,329 --> 00:36:20,400
that for all of the 20 or so

427
00:36:21,079 --> 00:36:25,359
risk management objectives you can come
up with a bunch of questions to ask that

428
00:36:25,359 --> 00:36:31,450
are pretty specific and you know I'm not
standing appear saying these are the

429
00:36:31,450 --> 00:36:35,189
questions you should ask this is Sara
Lee you know because that this might or

430
00:36:35,190 --> 00:36:39,750
might not be a risk management objective
that up last year but for every

431
00:36:39,750 --> 00:36:43,670
objective you can define a lot of
questions and then you can begin to

432
00:36:43,670 --> 00:36:47,650
gather data to answer those questions
and that is sort of a methodology for

433
00:36:47,650 --> 00:36:51,490
creating metrics that hopefully are
meaningful and that you might be able to

434
00:36:51,490 --> 00:36:54,970
tell a story about because you've been
going in you can say look good software

435
00:36:54,970 --> 00:37:00,069
good bad software bad how do we get to
that stage where we're getting more good

436
00:37:00,069 --> 00:37:03,569
software unless bad software you know
we're going to manage the risk by

437
00:37:03,569 --> 00:37:07,460
putting everything through security I'll
see how do we do that you know I'm

438
00:37:07,460 --> 00:37:13,089
asking five questions about it you don't
have to ask five you guys just one and

439
00:37:13,089 --> 00:37:18,130
so it goes you know and as things change
in the program so things change in the

440
00:37:18,130 --> 00:37:23,150
metrics and then you just you just keep
on doing it you know every organization

441
00:37:23,150 --> 00:37:28,410
is gonna have different risk management
objectives and ask different questions

442
00:37:28,410 --> 00:37:33,509
about it but today when I was hoping to
resign was sort of a methodology and one

443
00:37:33,510 --> 00:37:37,790
example of how you might do that we
thought there was a lot we're actually

444
00:37:37,790 --> 00:37:41,080
in the process of building out a tool

445
00:37:41,080 --> 00:37:47,080
all that has for each of these risk
management objectives you know sort of

446
00:37:47,080 --> 00:37:51,430
all of the questions that you might ask
and the various pieces of data that you

447
00:37:51,430 --> 00:37:55,790
might gather in order to answer the
questions you know if if you're looking

448
00:37:55,790 --> 00:37:58,270
at one of these objectives and you're
saying yeah that's something I want to

449
00:37:58,270 --> 00:38:01,580
do and then you're looking at the
questions and you're like well not

450
00:38:01,580 --> 00:38:05,980
really able to answer them then maybe
that in itself is sort of marching

451
00:38:05,980 --> 00:38:11,260
orders to say well how do I put the
processes in place to get the data to

452
00:38:11,260 --> 00:38:15,850
answer the question to meet this risk
management objective and that's all I've

453
00:38:15,850 --> 00:38:22,560
got its its 40 after the hour so we've
got 10 minutes for Q&A if folks have

454
00:38:22,560 --> 00:38:25,560
anything that they want to talk about

455
00:38:58,350 --> 00:39:09,210
so you know I would say that every
organization has a different risk

456
00:39:09,210 --> 00:39:13,300
tolerance what kind of questions can you
ask your executives in order to figure

457
00:39:13,300 --> 00:39:19,320
out how they feel about what your risk
tolerance ought to be so this list and I

458
00:39:19,320 --> 00:39:22,280
think like all the slides are gonna be
provided see you guys will have this

459
00:39:22,280 --> 00:39:25,000
list this list is a decent place to
start

460
00:39:25,000 --> 00:39:28,240
you know and you could kind of go
through this list and say are some of

461
00:39:28,240 --> 00:39:32,250
these wildly out of our reach in which
case you're probably not going to get to

462
00:39:32,250 --> 00:39:36,270
them this year you know or some of them
may be there they are within our reach

463
00:39:36,270 --> 00:39:42,970
and and the other way to do it would be
to say for these when I kind of call it

464
00:39:42,970 --> 00:39:46,850
metrics refinements for risk ranking
protects stack for business unit per

465
00:39:46,850 --> 00:39:51,279
software product type you know maybe you
choose a risk objective and you ask a

466
00:39:51,280 --> 00:39:55,980
question about it but you're only
focused on Oracle Apps you know you're

467
00:39:55,980 --> 00:40:01,110
only focused on this particular
technology stack so there are ways to

468
00:40:01,110 --> 00:40:06,840
say it's impossible to boil the ocean
and we can't do everything it wants but

469
00:40:06,840 --> 00:40:11,750
how do we start and then and then the
ways to do that would be to pick some

470
00:40:11,750 --> 00:40:16,200
risk management objectives and then
decrease the scope by using it and then

471
00:40:16,200 --> 00:40:20,700
i'd say you know how do you have that
risk tolerance conversation with the

472
00:40:20,700 --> 00:40:24,839
executives one way that I would
recommend is you know out of these

473
00:40:24,840 --> 00:40:29,410
twenty or so risk management objectives
you know you pick the three or the five

474
00:40:29,410 --> 00:40:33,040
you know that you think that you care
about and that maybe your exact care

475
00:40:33,040 --> 00:40:38,190
about and you say to them you care about
these things you know and and then maybe

476
00:40:38,190 --> 00:40:42,410
they do maybe they don't and then but
but then you have something to start on

477
00:40:42,410 --> 00:40:47,890
you know if you go into a conversation
with your executives alike so we're you

478
00:40:47,890 --> 00:40:51,410
know how r scale of one to 10 we're
serious talk and you know that's that's

479
00:40:51,410 --> 00:40:56,240
a hard thing for them to necessarily
respond to in a productive way but if

480
00:40:56,240 --> 00:41:00,200
you take this list of twenty you know
and you pick three to five and you're

481
00:41:00,200 --> 00:41:02,720
like you know those really kinda
resonate with me

482
00:41:02,720 --> 00:41:06,350
resonate with my organization and then
you presented to them

483
00:41:06,350 --> 00:41:11,509
you say what do you think maybe that's a
good way to start the conversation

484
00:41:43,220 --> 00:41:48,220
yeah I think I think that's a good point
right so these metrics are not

485
00:41:48,220 --> 00:41:52,799
predictive you know and I don't claim to
be able to tell you who's gonna win the

486
00:41:52,800 --> 00:41:53,420
Super Bowl

487
00:41:53,420 --> 00:41:57,250
you know similarly I don't claim to be
able to tell you what kind of incidents

488
00:41:57,250 --> 00:42:02,040
going to happen tomorrow you know I
think if any of us could do that I don't

489
00:42:02,040 --> 00:42:04,400
know if we'd be here today you know
maybe we'd be doing something different

490
00:42:04,400 --> 00:42:10,740
because we have like a ton of money a
lot of our data so I think that's a good

491
00:42:10,740 --> 00:42:15,529
point and i also think that that an
opportunity for education right if

492
00:42:15,530 --> 00:42:20,570
you're executive comes to you and says
why are you able to predict my neck

493
00:42:20,570 --> 00:42:25,800
security and then that's something that
you can respond and say well that's not

494
00:42:25,800 --> 00:42:29,119
possible

495
00:42:29,119 --> 00:42:32,119
yeah

496
00:42:34,860 --> 00:42:37,860
yeah

497
00:42:38,750 --> 00:42:48,380
yeah i i agree agree agree I think yeah
you know there's other industries like

498
00:42:48,380 --> 00:42:51,980
insurance or financial they have all of
these like quantitative models and

499
00:42:51,980 --> 00:42:56,420
they're like we know that like you know
one out of 20 drivers is gonna do this

500
00:42:56,420 --> 00:42:59,810
damn thing and that's why we charge this
month for insurance I think I'm not

501
00:42:59,810 --> 00:43:07,500
mature yet but I also think we're not
completely done right so there's lots of

502
00:43:07,500 --> 00:43:15,650
different ways to do that and I would
actually go to our slide about various

503
00:43:15,650 --> 00:43:20,300
activities you know I think a really
high level you know it's proper to say

504
00:43:20,300 --> 00:43:26,250
the if you have controls in place versus
if you don't have controls in place if

505
00:43:26,250 --> 00:43:30,710
you have controls in place and those
controls are good you're probably in a

506
00:43:30,710 --> 00:43:36,290
better place than if you had zero
control so there's a coverage thing and

507
00:43:36,290 --> 00:43:40,520
and their coverage thing you know
breaths why so you can look at this

508
00:43:40,520 --> 00:43:44,600
church you could look at this page and
you could say you know I have a security

509
00:43:44,600 --> 00:43:45,080
or so

510
00:43:45,080 --> 00:43:48,720
SCLC with gates I don't have a satellite
I don't have metrics I don't do

511
00:43:48,720 --> 00:43:52,279
portfolio manager at all you know I do
this I don't do that there's so there

512
00:43:52,280 --> 00:43:59,090
are these gap analyses approaches to say
you know do I have controls coverage

513
00:43:59,090 --> 00:44:05,790
another way to do it is to save maybe I
take a threat and attack intelligence

514
00:44:05,790 --> 00:44:09,460
way of looking at it you know when I'm
brainstorming different ways in which I

515
00:44:09,460 --> 00:44:13,660
could be attacks you know maybe I'm
looking at companies that are like my

516
00:44:13,660 --> 00:44:17,390
company and I say well you know who got
attacked last weekend and how did that

517
00:44:17,390 --> 00:44:21,549
happen to them and AM I am i protected
in that way so I think we're not totally

518
00:44:21,550 --> 00:44:27,050
dumb about predicting if you have no
security control in place you're likely

519
00:44:27,050 --> 00:44:31,150
to get attacked me and then if you have
some in place you know but the funny

520
00:44:31,150 --> 00:44:35,690
flip side for us is if you're not
looking maybe you don't know so

521
00:44:35,690 --> 00:44:40,790
sometimes you know we meet with clients
and and as part of Use amor or whatever

522
00:44:40,790 --> 00:44:45,440
sort of interview data gathering thing
we're doing we say you know so what how

523
00:44:45,440 --> 00:44:47,380
how does a software security incident

524
00:44:47,380 --> 00:44:51,599
for you and they say to us we've never
had a software security and we're like

525
00:44:51,599 --> 00:44:57,960
ok well maybe you don't have detective
controls in place to know that you're

526
00:44:57,960 --> 00:45:02,599
being attacked because he's probably you
are so there's this funny thing where we

527
00:45:02,599 --> 00:45:06,940
also don't know we don't know I'm I
don't have a good answer for you

528
00:45:06,940 --> 00:45:12,869
unfortunately but I do think it's a good
question because it's a it's a really

529
00:45:12,869 --> 00:45:15,170
interesting question and I think if we
could get there that'd be really cool

530
00:45:15,170 --> 00:45:19,779
but we don't as an industry have the
data get are not mature yet to that

531
00:45:19,779 --> 00:45:25,210
point I think I think if we wanted to
get there and maybe O wasp is a cool

532
00:45:25,210 --> 00:45:29,359
platform for talking about something
like this you know maybe there's an

533
00:45:29,359 --> 00:45:33,839
opportunity to say well if we are
collected the same credit metrics and we

534
00:45:33,839 --> 00:45:39,680
put our data in a place you know we'd
have something so there's an

535
00:45:39,680 --> 00:45:43,649
organization CIS the Center for Internet
Security and they're the ones who do as

536
00:45:43,650 --> 00:45:48,450
host hardly standards and it may be like
five six years ago they tried to do that

537
00:45:48,450 --> 00:45:52,078
actually they they said hey we're going
to develop you know we're gonna talk to

538
00:45:52,079 --> 00:45:55,559
like seventy people who think
information security metrics are

539
00:45:55,559 --> 00:45:58,859
interesting and know something about it
and we're gonna put together these like

540
00:45:58,859 --> 00:46:00,339
10 or so

541
00:46:00,339 --> 00:46:03,440
information security metrics and they
did and it's available you can google

542
00:46:03,440 --> 00:46:07,809
the IRS consensus Metrix definitions and
there's a group of people myself

543
00:46:07,809 --> 00:46:12,289
included who tried to do that and the
idea was that if we could get you know

544
00:46:12,289 --> 00:46:17,440
all the organizations to gather these
metrics then maybe we can get to a point

545
00:46:17,440 --> 00:46:21,369
where they were all putting them in
place and we could build these types of

546
00:46:21,369 --> 00:46:26,109
data sets but no one was interested and
I think the reason I was interested with

547
00:46:26,109 --> 00:46:32,460
because these 10 metrics did apply to
everyone not where they were you know

548
00:46:32,460 --> 00:46:38,680
for their particular maturity or or they
were focusing on so we're just not the

549
00:46:38,680 --> 00:46:43,970
area where like your new you know how
new is software how new is it that

550
00:46:43,970 --> 00:46:48,038
software is used for everything you know
it's pretty well as insurance has been

551
00:46:48,039 --> 00:46:50,019
around for like a really long time

552
00:46:50,019 --> 00:46:52,049
financial turnaround for even longer

553
00:46:52,049 --> 00:47:06,400
but there are so many interests so you
should read the fine print because

554
00:47:06,400 --> 00:47:17,160
there's like all these scenarios under
which it doesn't apply but yes I bet

555
00:47:17,160 --> 00:47:27,069
they didn't I bet they do I'm yeah yeah
I think it's kind of a con to be honest

556
00:47:27,069 --> 00:47:35,150
this is my personal take I know it's
cool I I will stand behind my opinion

557
00:47:35,150 --> 00:47:38,150
but it's my personal opinion

558
00:48:15,730 --> 00:48:34,790
I think that's an excellent point I
think if you sort of you know reduce the

559
00:48:34,790 --> 00:48:38,820
scope of the question you know instead
of saying like predict the next security

560
00:48:38,820 --> 00:48:44,630
incident you know say well here's one
specific type of bug you know the way in

561
00:48:44,630 --> 00:48:48,450
which it occurs is discreet and we can
sort of manage that and we can serve

562
00:48:48,450 --> 00:48:52,470
manage the various things you know we
can we can reduce the attack surface and

563
00:48:52,470 --> 00:48:53,470
so forth

564
00:48:53,470 --> 00:48:58,549
I also think that in general most
application security organizations are

565
00:48:58,550 --> 00:49:05,630
not mature enough to where it's the best
call to jump and try and do predictive

566
00:49:05,630 --> 00:49:12,050
metrics when most likely there are not
very basic metrics such as coverage

567
00:49:12,050 --> 00:49:16,000
metrics participation metrics things
like defect density things like here's

568
00:49:16,000 --> 00:49:20,020
the plan are reacting on that plan are
we seeing any effectiveness from my plan

569
00:49:20,020 --> 00:49:25,180
my personal opinion is that predictive
metrics are like really far-out on

570
00:49:25,180 --> 00:49:28,839
maturity curve and the i've seen very
few organizations that are there

571
00:49:30,700 --> 00:49:33,970
the insurance question is interesting
I'm pretty sure they have no data and

572
00:49:33,970 --> 00:49:37,410
again I will stand behind I kind of
think it's a con because there are so

573
00:49:37,410 --> 00:49:41,819
many loopholes I bet they look like
hardly ever pay out about they hardly

574
00:49:41,820 --> 00:49:49,230
ever pay out and have a question its 350
so you can go if you want to thank you I

575
00:49:49,230 --> 00:49:50,200
really appreciate you being here

