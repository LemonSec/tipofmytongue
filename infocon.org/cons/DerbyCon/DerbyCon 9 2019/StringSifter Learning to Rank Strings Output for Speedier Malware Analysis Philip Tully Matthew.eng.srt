1
00:00:00,380 --> 00:00:06,930
alright hello everybody can you hear me

2
00:00:02,490 --> 00:00:09,240
okay alright welcome we're here to

3
00:00:06,930 --> 00:00:11,730
present string sifter today it's a tool

4
00:00:09,240 --> 00:00:14,250
for ranking strings according to their

5
00:00:11,730 --> 00:00:15,780
relevance for malware analysis we're

6
00:00:14,250 --> 00:00:18,690
really excited to share it with you guys

7
00:00:15,780 --> 00:00:21,869
to share it with the community so yeah I

8
00:00:18,690 --> 00:00:24,449
hope we can do that for you today this

9
00:00:21,869 --> 00:00:27,869
is us our team from left to right is

10
00:00:24,449 --> 00:00:31,109
Michael Sikorsky there is Phillip Thole

11
00:00:27,869 --> 00:00:33,690
our data science and me and J gibble

12
00:00:31,109 --> 00:00:35,219
were from the fire labs advanced reverse

13
00:00:33,690 --> 00:00:37,649
engineering team otherwise known as

14
00:00:35,219 --> 00:00:39,090
flare if any of you are familiar the

15
00:00:37,649 --> 00:00:42,440
flare on challenge is going on right now

16
00:00:39,090 --> 00:00:42,440
so you might have some familiarity

17
00:00:43,590 --> 00:00:49,530
I'm not sure how it works with the con

18
00:00:45,210 --> 00:00:51,269
do we share the slides okay we could

19
00:00:49,530 --> 00:00:54,239
probably put them on the github to share

20
00:00:51,269 --> 00:00:57,089
that at the end okay so thanks and also

21
00:00:54,239 --> 00:00:58,559
this is a collaboration between the fire

22
00:00:57,089 --> 00:01:02,040
I data science team in our reverse

23
00:00:58,559 --> 00:01:05,850
engineering flare team so what's

24
00:01:02,040 --> 00:01:08,970
interesting about this string at first

25
00:01:05,850 --> 00:01:11,429
glance I could say that this probably

26
00:01:08,970 --> 00:01:14,520
indicates a sample uses the HTTP

27
00:01:11,430 --> 00:01:16,020
protocol because we see the word knowing

28
00:01:14,520 --> 00:01:19,050
a little bit about the protocol I could

29
00:01:16,020 --> 00:01:21,090
say that this is a HTTP response header

30
00:01:19,050 --> 00:01:26,100
so it might be a H it might be capable

31
00:01:21,090 --> 00:01:28,890
of acting as an HTTP server there you go

32
00:01:26,100 --> 00:01:31,259
you nailed it so the interesting part is

33
00:01:28,890 --> 00:01:33,360
the space at the end and it's this

34
00:01:31,259 --> 00:01:35,159
little detail where we can see how much

35
00:01:33,360 --> 00:01:38,159
one string can have a significant impact

36
00:01:35,159 --> 00:01:40,950
on malware analysis and and analyzing

37
00:01:38,159 --> 00:01:45,810
campaigns over time as well so good

38
00:01:40,950 --> 00:01:48,329
observation and this is a see a strings

39
00:01:45,810 --> 00:01:50,460
output going by on the right as I go

40
00:01:48,329 --> 00:01:52,710
through this slide just as an example of

41
00:01:50,460 --> 00:01:54,240
what you go through trying to analyze

42
00:01:52,710 --> 00:02:00,449
the strings output that leads to this

43
00:01:54,240 --> 00:02:04,259
particular string so there's a hdtb HTTP

44
00:02:00,450 --> 00:02:06,990
web server called nano HTTP that has

45
00:02:04,259 --> 00:02:09,869
this typo it was released in 2012 and

46
00:02:06,990 --> 00:02:13,680
it's contained within this cobalt strike

47
00:02:09,869 --> 00:02:15,359
attack simulation tool and you know it's

48
00:02:13,680 --> 00:02:16,980
a tool for red teamers and also for

49
00:02:15,359 --> 00:02:20,430
malicious actors as many of you probably

50
00:02:16,980 --> 00:02:22,798
know and so that persisted until they

51
00:02:20,430 --> 00:02:25,470
patched it in january this year so for

52
00:02:22,799 --> 00:02:29,400
about seven years this erroneous string

53
00:02:25,470 --> 00:02:31,590
was inside of the public strike so what

54
00:02:29,400 --> 00:02:33,269
can we do with that information we can

55
00:02:31,590 --> 00:02:36,329
use that as a detection signature for

56
00:02:33,269 --> 00:02:38,340
intrusion detection systems so we can

57
00:02:36,329 --> 00:02:40,769
detect malware across the network and on

58
00:02:38,340 --> 00:02:41,940
disk as well but it goes a little beyond

59
00:02:40,769 --> 00:02:45,030
that and that's what's cool about this

60
00:02:41,940 --> 00:02:48,150
particular string this blog here linked

61
00:02:45,030 --> 00:02:50,820
at the bottom from Fox I t.com they did

62
00:02:48,150 --> 00:02:53,159
some interesting research about tracking

63
00:02:50,820 --> 00:02:55,680
campaigns over time because all the

64
00:02:53,159 --> 00:02:56,790
publicly available is a there's a set of

65
00:02:55,680 --> 00:02:59,489
data released by

66
00:02:56,790 --> 00:03:02,159
rapid7 of just IP traffic and you go

67
00:02:59,489 --> 00:03:04,049
through that traffic and find you know a

68
00:03:02,159 --> 00:03:05,459
campaign now involves overtime and how

69
00:03:04,049 --> 00:03:08,280
much people were using this server and

70
00:03:05,459 --> 00:03:09,810
even get see two addresses so it's

71
00:03:08,280 --> 00:03:11,010
pretty cool way to kind of track the

72
00:03:09,810 --> 00:03:13,260
right actors some of the traffic

73
00:03:11,010 --> 00:03:14,760
obviously is legitimate and all kinds of

74
00:03:13,260 --> 00:03:18,379
interesting material you can gain just

75
00:03:14,760 --> 00:03:18,379
from one white space in one Stern

76
00:03:18,580 --> 00:03:22,300
so running strings on binaries can

77
00:03:20,710 --> 00:03:25,180
produce upwards of ten thousands of

78
00:03:22,300 --> 00:03:27,310
strings and that produces a lot of noise

79
00:03:25,180 --> 00:03:29,890
as you saw in that previous slide mixed

80
00:03:27,310 --> 00:03:31,630
in with important information so we want

81
00:03:29,890 --> 00:03:34,390
to try to solve this problem how we can

82
00:03:31,630 --> 00:03:35,920
reduce the time an analyst has to spend

83
00:03:34,390 --> 00:03:39,459
sifting through the noise to get to this

84
00:03:35,920 --> 00:03:41,290
interesting information so just take a

85
00:03:39,460 --> 00:03:43,870
quick step back just to do a little

86
00:03:41,290 --> 00:03:46,870
review on how we define a string in this

87
00:03:43,870 --> 00:03:48,460
context a string is a sequence of n

88
00:03:46,870 --> 00:03:51,880
characters followed by a null

89
00:03:48,460 --> 00:03:54,100
terminating byte there's no context

90
00:03:51,880 --> 00:03:57,040
there's no metadata to indicate where

91
00:03:54,100 --> 00:03:59,290
the string is how long it is what kind

92
00:03:57,040 --> 00:04:02,799
of string it is therefore there's no

93
00:03:59,290 --> 00:04:06,790
context and arbitrary data can be

94
00:04:02,800 --> 00:04:08,110
misinterpreted as a string so anything

95
00:04:06,790 --> 00:04:10,570
within this sequence of printable

96
00:04:08,110 --> 00:04:12,820
characters on the ascii table if there's

97
00:04:10,570 --> 00:04:14,290
a sequence of however many then that can

98
00:04:12,820 --> 00:04:16,750
be interpreted as a string and this can

99
00:04:14,290 --> 00:04:18,969
be other things like for example x86

100
00:04:16,750 --> 00:04:21,670
code there are a lot of sequences of x86

101
00:04:18,970 --> 00:04:23,620
instructions that will produce printable

102
00:04:21,670 --> 00:04:25,270
characters and you don't know without

103
00:04:23,620 --> 00:04:26,470
any context whether that's an actual

104
00:04:25,270 --> 00:04:28,359
string or just the series of

105
00:04:26,470 --> 00:04:31,540
instructions and that can also happen

106
00:04:28,360 --> 00:04:33,580
with memory addresses and resources

107
00:04:31,540 --> 00:04:35,080
maybe an encryption key or an image or

108
00:04:33,580 --> 00:04:38,050
something like that so there's a lot of

109
00:04:35,080 --> 00:04:41,229
noise mixed in that we have to try to

110
00:04:38,050 --> 00:04:42,760
sift these strings out and then to

111
00:04:41,230 --> 00:04:44,890
further complicate you know window

112
00:04:42,760 --> 00:04:47,409
standard uses wide strings which is the

113
00:04:44,890 --> 00:04:49,270
same as the ASCII except different a

114
00:04:47,410 --> 00:04:51,340
little more complex generally just two

115
00:04:49,270 --> 00:04:54,030
characters with a double null

116
00:04:51,340 --> 00:04:54,030
terminating byte

117
00:04:55,150 --> 00:04:59,590
so it's interesting to examine you know

118
00:04:57,610 --> 00:05:01,270
how the how we have this a set of these

119
00:04:59,590 --> 00:05:03,729
strings and as you look through the

120
00:05:01,270 --> 00:05:06,010
compilation process we have this like

121
00:05:03,729 --> 00:05:08,800
simple C program that just prints the

122
00:05:06,010 --> 00:05:10,900
string Derby and when it's compiled into

123
00:05:08,800 --> 00:05:14,590
this compiled and assembled into this

124
00:05:10,900 --> 00:05:16,030
object file that string has to exist if

125
00:05:14,590 --> 00:05:18,460
the program has to use it somewhere

126
00:05:16,030 --> 00:05:20,679
unless it's dynamically generated it has

127
00:05:18,460 --> 00:05:22,090
to exist so it's on the disk and then

128
00:05:20,680 --> 00:05:24,370
eventually it's compiled into an

129
00:05:22,090 --> 00:05:26,229
executable and somewhere in some section

130
00:05:24,370 --> 00:05:28,090
you know there's there's certain

131
00:05:26,229 --> 00:05:29,620
conventions for where it would be but

132
00:05:28,090 --> 00:05:32,859
could be anywhere in the binary the

133
00:05:29,620 --> 00:05:34,900
string so that's our asset as malware

134
00:05:32,860 --> 00:05:37,150
analysts where you know we could take

135
00:05:34,900 --> 00:05:39,669
advantage of this process and then our

136
00:05:37,150 --> 00:05:41,409
plan is to invert the process and work

137
00:05:39,669 --> 00:05:43,448
backwards try to get from the string in

138
00:05:41,410 --> 00:05:46,210
the section back to what is the source

139
00:05:43,449 --> 00:05:47,979
code so strings persist on disk

140
00:05:46,210 --> 00:05:49,580
excuse me throughout the compilation

141
00:05:47,979 --> 00:05:52,340
process

142
00:05:49,580 --> 00:05:55,789
and then we have this tool this is the

143
00:05:52,340 --> 00:05:57,919
de facto tool for strings analysis it's

144
00:05:55,789 --> 00:05:58,699
different implementations on different

145
00:05:57,919 --> 00:06:00,409
platforms

146
00:05:58,699 --> 00:06:03,110
but it's generally called strings dot

147
00:06:00,409 --> 00:06:06,830
exe and it takes an input file or a

148
00:06:03,110 --> 00:06:09,490
stream of data and produces a output of

149
00:06:06,830 --> 00:06:12,258
all these printable character sequences

150
00:06:09,490 --> 00:06:16,069
here you have a little contrived example

151
00:06:12,259 --> 00:06:18,650
but this is kind of a sample of what you

152
00:06:16,069 --> 00:06:20,449
might see in a malware and the different

153
00:06:18,650 --> 00:06:23,628
types of noise that are mixed in with

154
00:06:20,449 --> 00:06:26,930
the fruitful strings up here you have

155
00:06:23,629 --> 00:06:30,530
you know an artifact of the PE header we

156
00:06:26,930 --> 00:06:32,389
have these C++ runtime strings you have

157
00:06:30,530 --> 00:06:34,849
a user agent which is a great Network

158
00:06:32,389 --> 00:06:36,889
indicator some API is which might start

159
00:06:34,849 --> 00:06:38,750
to give us a indication of what the

160
00:06:36,889 --> 00:06:42,409
malware or the sample may be capable of

161
00:06:38,750 --> 00:06:44,779
doing some paths file names and even

162
00:06:42,409 --> 00:06:46,789
usage strings which you'd be surprised

163
00:06:44,779 --> 00:06:48,349
how often you'll find things like this

164
00:06:46,789 --> 00:06:50,889
that tell you exactly what the sample is

165
00:06:48,349 --> 00:06:50,889
capable of

166
00:06:52,150 --> 00:07:00,429
so just a quick little examination of

167
00:06:55,900 --> 00:07:02,429
our process with fire I when we have a

168
00:07:00,430 --> 00:07:05,050
customer who suspects a compromise

169
00:07:02,430 --> 00:07:06,550
they'll likely you know they see

170
00:07:05,050 --> 00:07:09,250
something suspicious in the system they

171
00:07:06,550 --> 00:07:11,470
contact us we dispatch our incident

172
00:07:09,250 --> 00:07:13,660
responders they're going to do a

173
00:07:11,470 --> 00:07:16,720
forensic analysis on the system perhaps

174
00:07:13,660 --> 00:07:19,240
identify malware samples or suspicious

175
00:07:16,720 --> 00:07:21,520
samples that may be malware they'll

176
00:07:19,240 --> 00:07:24,100
perform a basic binary triage on the

177
00:07:21,520 --> 00:07:25,570
sample and if they need further analysis

178
00:07:24,100 --> 00:07:28,449
then they'll turn it over to the flare

179
00:07:25,570 --> 00:07:30,849
team and we'll do an advanced binary

180
00:07:28,449 --> 00:07:33,190
triage with full disassembly and try to

181
00:07:30,850 --> 00:07:34,630
identify all the capabilities of the

182
00:07:33,190 --> 00:07:37,080
malware the extent of the hack and

183
00:07:34,630 --> 00:07:39,969
things like that and the end goal is to

184
00:07:37,080 --> 00:07:44,139
produce this report which we can use to

185
00:07:39,970 --> 00:07:46,300
serve the customer so along this process

186
00:07:44,139 --> 00:07:49,210
every step of the way strings can be

187
00:07:46,300 --> 00:07:51,490
utilized the the sock analyst or whoever

188
00:07:49,210 --> 00:07:53,440
is working in the security department of

189
00:07:51,490 --> 00:07:55,960
the company might run strings to get an

190
00:07:53,440 --> 00:07:57,729
idea what's going on our forensic

191
00:07:55,960 --> 00:07:59,799
analysis will involve running strings

192
00:07:57,729 --> 00:08:01,450
just to get an idea and then when it

193
00:07:59,800 --> 00:08:03,849
gets to the reverse engineering level

194
00:08:01,450 --> 00:08:05,349
that's one of the first tools in our kit

195
00:08:03,849 --> 00:08:08,560
that we're gonna employ is running

196
00:08:05,349 --> 00:08:10,030
strings before we start our disassembly

197
00:08:08,560 --> 00:08:12,310
and oftentimes you never even have to

198
00:08:10,030 --> 00:08:14,409
open Ida Pro you can just figure out

199
00:08:12,310 --> 00:08:16,380
what it does from the strings run a

200
00:08:14,410 --> 00:08:20,050
quick google find the source code

201
00:08:16,380 --> 00:08:21,909
analysis done things like that and these

202
00:08:20,050 --> 00:08:24,340
aren't the only kind of users we would

203
00:08:21,910 --> 00:08:26,560
have some examples on the bottom red

204
00:08:24,340 --> 00:08:28,719
teamers for sure and malware researchers

205
00:08:26,560 --> 00:08:31,060
and there's lots of other people who are

206
00:08:28,720 --> 00:08:33,159
interested in ranking these strings and

207
00:08:31,060 --> 00:08:35,809
finding out the capabilities of malware

208
00:08:33,159 --> 00:08:37,729
by using the strengths

209
00:08:35,809 --> 00:08:42,409
so knowing which strings are relevant

210
00:08:37,729 --> 00:08:44,329
requires experienced analysts and Azen

211
00:08:42,409 --> 00:08:47,930
analysts the idea is to tell a story

212
00:08:44,329 --> 00:08:50,660
about the binary using the strengths so

213
00:08:47,930 --> 00:08:53,628
we want to order them like you see here

214
00:08:50,660 --> 00:08:56,930
in order of relevance on the left or

215
00:08:53,629 --> 00:08:58,610
your left you have things like runtime

216
00:08:56,930 --> 00:08:59,809
artifacts and library code that are

217
00:08:58,610 --> 00:09:02,779
going to produce a lot of this noise

218
00:08:59,809 --> 00:09:04,639
that isn't necessarily very useful then

219
00:09:02,779 --> 00:09:06,110
you have Windows API is which starts to

220
00:09:04,639 --> 00:09:07,579
be a little more helpful and it works

221
00:09:06,110 --> 00:09:09,800
its way up and so you get stuff like

222
00:09:07,579 --> 00:09:12,109
source code you can Google through open

223
00:09:09,800 --> 00:09:13,758
source intelligence user agent strings

224
00:09:12,110 --> 00:09:17,089
which are network indicators IP

225
00:09:13,759 --> 00:09:19,069
addresses things like that so we want to

226
00:09:17,089 --> 00:09:22,519
take that whole output and produce a

227
00:09:19,069 --> 00:09:25,309
story about what is this file capable of

228
00:09:22,519 --> 00:09:27,800
this malware is it malware and what is

229
00:09:25,309 --> 00:09:29,449
the way we could serve the customer the

230
00:09:27,800 --> 00:09:32,020
extent of the hack what has been stolen

231
00:09:29,449 --> 00:09:34,060
things like that

232
00:09:32,020 --> 00:09:36,579
of course this relevance is subjective

233
00:09:34,060 --> 00:09:39,099
and it requires experience now analysis

234
00:09:36,580 --> 00:09:41,410
and even amongst analysts in the same

235
00:09:39,100 --> 00:09:43,660
capacity it's just personal opinion on

236
00:09:41,410 --> 00:09:47,680
which is relevant and the exact ordering

237
00:09:43,660 --> 00:09:49,630
of these things so that brings us to the

238
00:09:47,680 --> 00:09:51,849
goal that we're trying to achieve with

239
00:09:49,630 --> 00:09:53,439
string scepter the idea is to develop a

240
00:09:51,850 --> 00:09:56,500
tool that can identify and prioritize

241
00:09:53,440 --> 00:09:58,720
these strings and in order of relevance

242
00:09:56,500 --> 00:10:01,060
for malware analysis and we want to do

243
00:09:58,720 --> 00:10:03,520
that in a way that's easy to use that

244
00:10:01,060 --> 00:10:05,410
can generalize across all users just for

245
00:10:03,520 --> 00:10:07,779
the simple you know a couple keystrokes

246
00:10:05,410 --> 00:10:09,730
and that inevitably the goal all this is

247
00:10:07,779 --> 00:10:13,089
the same time save time and malware

248
00:10:09,730 --> 00:10:15,520
analysis so how do we do this I'm going

249
00:10:13,089 --> 00:10:16,870
to bring Phil up from our data science

250
00:10:15,520 --> 00:10:20,130
team to get into some of the more

251
00:10:16,870 --> 00:10:20,130
interesting technical details

252
00:10:39,839 --> 00:10:46,170
sorry one second maybe the battery died

253
00:10:43,769 --> 00:10:48,240
I'll just go off the keyboard so so

254
00:10:46,170 --> 00:10:51,349
thanks Matt yeah so I want to kind of

255
00:10:48,240 --> 00:10:51,350
get you guys in the mood of

256
00:10:55,480 --> 00:11:02,770
yeah here we go of of how to go about

257
00:10:58,950 --> 00:11:04,060
computing a rank of something and ranks

258
00:11:02,770 --> 00:11:06,130
are everywhere we encounter them

259
00:11:04,060 --> 00:11:08,079
throughout you know in our daily lives

260
00:11:06,130 --> 00:11:10,180
extremely often now when we want to pick

261
00:11:08,080 --> 00:11:12,160
what's top we want to invest in we look

262
00:11:10,180 --> 00:11:13,270
at the fortune 500 we want to choose

263
00:11:12,160 --> 00:11:16,089
which college we want to send our kids

264
00:11:13,270 --> 00:11:18,130
to we look at the u.s. us best colleges

265
00:11:16,090 --> 00:11:22,660
rankings list decide what movie we want

266
00:11:18,130 --> 00:11:25,450
to watch you know Alexa keeps keeps

267
00:11:22,660 --> 00:11:28,839
track of how many of which domains are

268
00:11:25,450 --> 00:11:31,270
clicked most often on the web and you

269
00:11:28,840 --> 00:11:32,230
know we're all guilty of click falling

270
00:11:31,270 --> 00:11:34,120
through this kind of click bait you know

271
00:11:32,230 --> 00:11:36,220
ranks art are fundamental and they

272
00:11:34,120 --> 00:11:37,570
they're interesting because they tell us

273
00:11:36,220 --> 00:11:41,200
something about how things are related

274
00:11:37,570 --> 00:11:42,820
to each other in an ordered fashion the

275
00:11:41,200 --> 00:11:45,010
only that but our favorite products

276
00:11:42,820 --> 00:11:47,440
serve up rankings unbeknownst to us

277
00:11:45,010 --> 00:11:50,590
essentially so every time you type a

278
00:11:47,440 --> 00:11:52,510
query into a search engine you know that

279
00:11:50,590 --> 00:11:55,060
that search engine is gonna is gonna

280
00:11:52,510 --> 00:11:56,920
stack rank a list of websites for you

281
00:11:55,060 --> 00:11:59,829
that's going to be based on things like

282
00:11:56,920 --> 00:12:01,000
your previous click history users that

283
00:11:59,830 --> 00:12:02,650
are similar to you what they've clicked

284
00:12:01,000 --> 00:12:04,930
before as well what you've clicked in

285
00:12:02,650 --> 00:12:05,980
the past etc and this is kind of what

286
00:12:04,930 --> 00:12:08,650
keeps you going back these search

287
00:12:05,980 --> 00:12:11,050
engines every time you log into a social

288
00:12:08,650 --> 00:12:13,120
network you know typically your friends

289
00:12:11,050 --> 00:12:14,829
who are posting are not ordered

290
00:12:13,120 --> 00:12:16,270
chronologically on your timeline right

291
00:12:14,830 --> 00:12:18,520
they're ranked according to some

292
00:12:16,270 --> 00:12:20,110
relevance so maybe your your closer

293
00:12:18,520 --> 00:12:22,840
friends will be will appear higher up on

294
00:12:20,110 --> 00:12:24,580
your timeline or people who you share

295
00:12:22,840 --> 00:12:27,190
common followers with will also kind of

296
00:12:24,580 --> 00:12:29,410
be shot up in that list and then lastly

297
00:12:27,190 --> 00:12:32,770
kind of recommender systems the way that

298
00:12:29,410 --> 00:12:35,500
different web sites are web ads to us on

299
00:12:32,770 --> 00:12:37,840
like the side of the news articles we

300
00:12:35,500 --> 00:12:39,700
read typically we see like one to two to

301
00:12:37,840 --> 00:12:42,490
three of those things but generally for

302
00:12:39,700 --> 00:12:44,470
each kind of user on that website now

303
00:12:42,490 --> 00:12:45,880
you'll have a bunch of stacked ranked

304
00:12:44,470 --> 00:12:47,230
ads that are ready to go and be served

305
00:12:45,880 --> 00:12:50,110
up to that user

306
00:12:47,230 --> 00:12:52,080
anytime we choose a movie on Netflix or

307
00:12:50,110 --> 00:12:54,340
music on Spotify or something like this

308
00:12:52,080 --> 00:12:56,380
they're always making recommendations to

309
00:12:54,340 --> 00:12:58,030
you here's a stack drink list of things

310
00:12:56,380 --> 00:13:00,010
that should interest you based on what

311
00:12:58,030 --> 00:13:03,000
you've listened to or watched previously

312
00:13:00,010 --> 00:13:06,490
and so how do we kind of formulate this

313
00:13:03,000 --> 00:13:07,750
more systematic route and and in machine

314
00:13:06,490 --> 00:13:08,889
learning this is kind of referred to as

315
00:13:07,750 --> 00:13:13,029
the learning to

316
00:13:08,889 --> 00:13:15,369
paradigm so the goal here is to take a

317
00:13:13,029 --> 00:13:18,160
list of input items like you see on the

318
00:13:15,369 --> 00:13:19,959
left and create an optimal ordering or

319
00:13:18,160 --> 00:13:23,049
optimal permutation of those list of

320
00:13:19,959 --> 00:13:24,819
items and so you know you have a list of

321
00:13:23,049 --> 00:13:26,619
five items here and you apply some

322
00:13:24,819 --> 00:13:30,339
function to those items and what you get

323
00:13:26,619 --> 00:13:33,100
is an ordered list of those items and so

324
00:13:30,339 --> 00:13:35,139
the interesting part here is that the

325
00:13:33,100 --> 00:13:37,299
precise score of each individual item

326
00:13:35,139 --> 00:13:39,249
doesn't matter as much as the relative

327
00:13:37,299 --> 00:13:41,889
ordering all right so I don't care that

328
00:13:39,249 --> 00:13:44,439
how much that white box is ranked above

329
00:13:41,889 --> 00:13:45,910
that red box only that it is ranked

330
00:13:44,439 --> 00:13:47,559
above the red box right so precise

331
00:13:45,910 --> 00:13:49,329
scores are a little bit you know we

332
00:13:47,559 --> 00:13:50,799
don't care as much about that and so the

333
00:13:49,329 --> 00:13:53,529
objective here is fundamentally

334
00:13:50,799 --> 00:13:55,029
different then some of your more classic

335
00:13:53,529 --> 00:13:57,009
especially when you think about the

336
00:13:55,029 --> 00:13:59,079
information security the way that we

337
00:13:57,009 --> 00:14:01,149
think about tasks like classification

338
00:13:59,079 --> 00:14:03,429
where you're trying to predict something

339
00:14:01,149 --> 00:14:06,399
like is this email hem or is it spam or

340
00:14:03,429 --> 00:14:08,169
regression or clustering each of these

341
00:14:06,399 --> 00:14:10,899
kind of tasks means that you're

342
00:14:08,169 --> 00:14:12,639
assigning a score or a label to one

343
00:14:10,899 --> 00:14:15,309
specific item here our task is

344
00:14:12,639 --> 00:14:17,110
fundamentally different and and from

345
00:14:15,309 --> 00:14:19,449
what I've seen at least in in literature

346
00:14:17,110 --> 00:14:21,850
and and various talks this kind of

347
00:14:19,449 --> 00:14:24,609
notion is is rarely applied and in

348
00:14:21,850 --> 00:14:27,669
security applications so to make this a

349
00:14:24,610 --> 00:14:29,350
little bit more defined here just like

350
00:14:27,669 --> 00:14:31,179
matt said we have a series of input

351
00:14:29,350 --> 00:14:32,860
files and we were honor want to run

352
00:14:31,179 --> 00:14:34,329
strings over each of those input files

353
00:14:32,860 --> 00:14:37,209
and generate a set of training data

354
00:14:34,329 --> 00:14:40,540
right and so our training data here is a

355
00:14:37,209 --> 00:14:42,699
set of strings outputs can be as many as

356
00:14:40,540 --> 00:14:44,230
we want and each of the strings outputs

357
00:14:42,699 --> 00:14:46,449
is going to produce a different number

358
00:14:44,230 --> 00:14:48,939
of strings within their output and we're

359
00:14:46,449 --> 00:14:50,378
going to associate a label with each of

360
00:14:48,939 --> 00:14:53,169
those strings that's going to encode

361
00:14:50,379 --> 00:14:55,239
it's relative its relative relevance

362
00:14:53,169 --> 00:14:57,279
within that list so things like matt

363
00:14:55,239 --> 00:15:00,009
said like those PE artifacts this

364
00:14:57,279 --> 00:15:01,449
program is running in DOS mode is gonna

365
00:15:00,009 --> 00:15:03,660
have a lower relevance because it's

366
00:15:01,449 --> 00:15:06,008
encoded by zero versus something like

367
00:15:03,660 --> 00:15:07,059
HTTP header it's gonna have a much

368
00:15:06,009 --> 00:15:10,739
higher elements it's going to have a

369
00:15:07,059 --> 00:15:10,738
higher label associated with it

370
00:15:11,140 --> 00:15:15,760
and so again this is this is kind of our

371
00:15:13,120 --> 00:15:17,860
objective now we want to rank and create

372
00:15:15,760 --> 00:15:20,230
a permutation of these items within each

373
00:15:17,860 --> 00:15:22,209
individual strings output in order to

374
00:15:20,230 --> 00:15:24,190
push up the most relevant ones to the

375
00:15:22,209 --> 00:15:27,609
top and therefore make our analysts life

376
00:15:24,190 --> 00:15:29,110
just a lot easier how do we do this the

377
00:15:27,610 --> 00:15:31,060
underlying algorithm that we're going to

378
00:15:29,110 --> 00:15:34,209
use is called gradient boosted decision

379
00:15:31,060 --> 00:15:36,430
trees and so fundamentally these are

380
00:15:34,209 --> 00:15:38,920
built by individual week learners or

381
00:15:36,430 --> 00:15:40,209
decision trees and so a lot of people

382
00:15:38,920 --> 00:15:42,459
already probably know what this is but

383
00:15:40,209 --> 00:15:45,670
you know you you essentially build up a

384
00:15:42,459 --> 00:15:47,709
tree by by choosing splits based on this

385
00:15:45,670 --> 00:15:51,040
notion of Gini impurity so each split

386
00:15:47,709 --> 00:15:52,899
might consist of some features so you

387
00:15:51,040 --> 00:15:56,199
know whether or not a string contains

388
00:15:52,899 --> 00:15:58,870
the letters HTTP HTTP that might be a

389
00:15:56,200 --> 00:16:00,880
split in the tree and you know when you

390
00:15:58,870 --> 00:16:03,029
split it it's gonna optimize this Gini

391
00:16:00,880 --> 00:16:05,110
impurity notion and you're gonna kind of

392
00:16:03,029 --> 00:16:07,990
combine these set of features and build

393
00:16:05,110 --> 00:16:11,890
out these trees up to a certain level of

394
00:16:07,990 --> 00:16:14,320
of complexity so gradient boosted

395
00:16:11,890 --> 00:16:17,380
decision trees take that notion of a

396
00:16:14,320 --> 00:16:20,220
single decision tree and combine many of

397
00:16:17,380 --> 00:16:22,570
them and kind of produce several of them

398
00:16:20,220 --> 00:16:25,180
and and it's gonna iterate across

399
00:16:22,570 --> 00:16:28,240
several of them and each time it adds a

400
00:16:25,180 --> 00:16:30,069
new tree it's gonna try to reduce some

401
00:16:28,240 --> 00:16:32,410
loss and this process is called gradient

402
00:16:30,070 --> 00:16:33,699
descent because we already know the

403
00:16:32,410 --> 00:16:35,140
answer is we didn't already know what

404
00:16:33,699 --> 00:16:37,750
our labels are our ranking scores are

405
00:16:35,140 --> 00:16:39,430
and the ensemble here is gonna produce a

406
00:16:37,750 --> 00:16:41,410
weighted sum of all of these trees and

407
00:16:39,430 --> 00:16:44,250
that's gonna kind of help us generalize

408
00:16:41,410 --> 00:16:46,420
better this process is called boosting

409
00:16:44,250 --> 00:16:48,730
the specific algorithm we're gonna use

410
00:16:46,420 --> 00:16:52,329
is called like GBM which is based on

411
00:16:48,730 --> 00:16:54,250
histogram bins but the point here is

412
00:16:52,329 --> 00:16:56,349
that we're not using a classification

413
00:16:54,250 --> 00:16:58,120
objective function we're using learning

414
00:16:56,350 --> 00:17:00,070
to rank objective functions the point

415
00:16:58,120 --> 00:17:02,110
here is that we want to optimize the

416
00:17:00,070 --> 00:17:04,500
relative ordering of the items within

417
00:17:02,110 --> 00:17:04,500
this list

418
00:17:05,839 --> 00:17:10,789
what training data are we using we were

419
00:17:08,869 --> 00:17:13,029
relying on an open benchmark dataset

420
00:17:10,789 --> 00:17:16,069
that was that was released by end game

421
00:17:13,029 --> 00:17:20,419
called ember and so this data set

422
00:17:16,069 --> 00:17:23,059
consists of 1.1 million PE files 400,000

423
00:17:20,419 --> 00:17:24,500
of them are malicious they split it up

424
00:17:23,059 --> 00:17:26,329
into testing and training but we kind of

425
00:17:24,500 --> 00:17:30,049
just take all the malicious ones which

426
00:17:26,329 --> 00:17:32,450
they which they have an easy criteria of

427
00:17:30,049 --> 00:17:35,299
rejecting mal worth of any file that has

428
00:17:32,450 --> 00:17:37,370
more than 40 virus total vendors calling

429
00:17:35,299 --> 00:17:40,279
it malware and so this kind of a safe

430
00:17:37,370 --> 00:17:43,668
set of malware for us so we take all of

431
00:17:40,279 --> 00:17:45,409
those 400,000 malware samples and we run

432
00:17:43,669 --> 00:17:46,850
strings over all of them and we produce

433
00:17:45,409 --> 00:17:50,029
something like over three billion

434
00:17:46,850 --> 00:17:51,439
individual strings from those files but

435
00:17:50,029 --> 00:17:52,730
we don't need to necessarily train on

436
00:17:51,440 --> 00:17:54,049
all of that data so we perform some

437
00:17:52,730 --> 00:17:56,210
sampling and we middle it down to

438
00:17:54,049 --> 00:17:59,149
something like 500 million strings and

439
00:17:56,210 --> 00:18:02,090
this is our training dataset and now we

440
00:17:59,149 --> 00:18:04,070
produce our ground truth by performing

441
00:18:02,090 --> 00:18:05,870
some heuristics right so the guys down

442
00:18:04,070 --> 00:18:07,700
there who are colleagues of mine and

443
00:18:05,870 --> 00:18:11,330
also Mike and a lot of the flare reverse

444
00:18:07,700 --> 00:18:13,100
engineers have have a lot of insights

445
00:18:11,330 --> 00:18:15,110
into how these things should be ranked

446
00:18:13,100 --> 00:18:16,699
and so we go through a process of

447
00:18:15,110 --> 00:18:18,590
heuristic li wei-ting them using weekly

448
00:18:16,700 --> 00:18:22,010
supervision and we can produce a set of

449
00:18:18,590 --> 00:18:25,309
labels for us from which to now predict

450
00:18:22,010 --> 00:18:26,390
from unseen strings outputs so the next

451
00:18:25,309 --> 00:18:29,389
important thing we want to talk about

452
00:18:26,390 --> 00:18:31,909
because we have our learning algorithm

453
00:18:29,390 --> 00:18:33,470
down now we have our labels down now and

454
00:18:31,909 --> 00:18:35,059
so we need to take away to take those

455
00:18:33,470 --> 00:18:37,190
need to come up with a way to take those

456
00:18:35,059 --> 00:18:38,690
raw strings and convert them into

457
00:18:37,190 --> 00:18:40,580
something that the algorithm can

458
00:18:38,690 --> 00:18:42,980
understand and this is called future

459
00:18:40,580 --> 00:18:45,710
ization we have we produce hand-tuned

460
00:18:42,980 --> 00:18:48,559
features here and we produce about a set

461
00:18:45,710 --> 00:18:50,960
of 80 different features we also use

462
00:18:48,559 --> 00:18:54,289
tf-idf engrams so that set is much

463
00:18:50,960 --> 00:18:55,630
larger in fact but a few the interesting

464
00:18:54,289 --> 00:18:58,190
ones that we wanted to point out to you

465
00:18:55,630 --> 00:19:01,640
include natural language processing

466
00:18:58,190 --> 00:19:03,320
features so you know one of the problems

467
00:19:01,640 --> 00:19:04,610
that might identify before was that a

468
00:19:03,320 --> 00:19:06,799
bunch of these strings within these

469
00:19:04,610 --> 00:19:09,020
outputs look completely irrelevant as

470
00:19:06,799 --> 00:19:10,668
they look like you know a jumbled up

471
00:19:09,020 --> 00:19:12,860
bunch of mess that are not human

472
00:19:10,669 --> 00:19:14,510
interpretable you know they might be x86

473
00:19:12,860 --> 00:19:16,340
code or whatever they may be they may be

474
00:19:14,510 --> 00:19:17,600
just artifacts artifacts that we don't

475
00:19:16,340 --> 00:19:18,649
care about we want to shove them down to

476
00:19:17,600 --> 00:19:20,990
the bottom

477
00:19:18,650 --> 00:19:22,640
so how do we do that one way is in which

478
00:19:20,990 --> 00:19:24,620
we can do that is using a Markov model

479
00:19:22,640 --> 00:19:26,270
and so we can take an own set of

480
00:19:24,620 --> 00:19:27,800
relevant strings and train a Markov

481
00:19:26,270 --> 00:19:29,740
model over that and produce a set of

482
00:19:27,800 --> 00:19:31,730
transition probabilities between

483
00:19:29,740 --> 00:19:34,490
characters within all of those strings

484
00:19:31,730 --> 00:19:39,320
right so here you can imagine this is

485
00:19:34,490 --> 00:19:41,030
like a a matrix a 2d image like a 2d

486
00:19:39,320 --> 00:19:42,649
matrix where you have characters along

487
00:19:41,030 --> 00:19:44,180
the top and caracals in the along the

488
00:19:42,650 --> 00:19:46,130
bottom it's all 2 also you can have self

489
00:19:44,180 --> 00:19:48,410
connections as well but once you train

490
00:19:46,130 --> 00:19:50,390
this model you basically say that the

491
00:19:48,410 --> 00:19:52,790
probability of a capital F being

492
00:19:50,390 --> 00:19:56,360
followed by a lowercase T within this

493
00:19:52,790 --> 00:19:58,129
data set is 0.07 versus the probability

494
00:19:56,360 --> 00:20:01,629
of a lowercase T being followed by

495
00:19:58,130 --> 00:20:05,630
itself that self connection is 0.14 and

496
00:20:01,630 --> 00:20:07,010
so in this contrived example the cheese

497
00:20:05,630 --> 00:20:08,960
might be more interesting if they're

498
00:20:07,010 --> 00:20:10,730
followed by yourself because you know

499
00:20:08,960 --> 00:20:12,230
you have things like HTTP that are

500
00:20:10,730 --> 00:20:14,330
relevant within these strings and you

501
00:20:12,230 --> 00:20:17,120
can build this out once you build that

502
00:20:14,330 --> 00:20:19,428
out you can apply it to unseen data and

503
00:20:17,120 --> 00:20:21,409
you can you can in a data driven away

504
00:20:19,429 --> 00:20:24,620
come up with a threshold that hopefully

505
00:20:21,410 --> 00:20:26,000
will nicely separate your noisy crap

506
00:20:24,620 --> 00:20:29,030
strings that you don't care about from

507
00:20:26,000 --> 00:20:31,280
your more interesting strings yes that's

508
00:20:29,030 --> 00:20:33,889
one way so this is what this looks like

509
00:20:31,280 --> 00:20:36,110
on a set of malware I'm not sure if you

510
00:20:33,890 --> 00:20:39,290
can see this but what what you're seeing

511
00:20:36,110 --> 00:20:41,059
is a histogram here of average

512
00:20:39,290 --> 00:20:43,309
transition probabilities across entire

513
00:20:41,059 --> 00:20:46,220
strings and so the further you get to

514
00:20:43,309 --> 00:20:48,110
the left that's a lower score and you

515
00:20:46,220 --> 00:20:50,420
see all that junk on the left hand side

516
00:20:48,110 --> 00:20:51,889
so this is kind of a nice indication

517
00:20:50,420 --> 00:20:54,380
that this might help us filter out that

518
00:20:51,890 --> 00:20:56,150
Junkin and score it lower and then as

519
00:20:54,380 --> 00:20:58,190
you get it to the get to the right more

520
00:20:56,150 --> 00:21:00,320
this is like a higher average transition

521
00:20:58,190 --> 00:21:01,580
probability you see we start to get

522
00:21:00,320 --> 00:21:04,010
things that look more like actual words

523
00:21:01,580 --> 00:21:05,659
so this could be a helpful feature for

524
00:21:04,010 --> 00:21:08,210
us and since it's like a memory based

525
00:21:05,660 --> 00:21:11,120
model there's another model that we can

526
00:21:08,210 --> 00:21:14,840
come up with that's based on entropy and

527
00:21:11,120 --> 00:21:18,080
this is memoryless and so here we're

528
00:21:14,840 --> 00:21:19,639
just we're just trying to compute the

529
00:21:18,080 --> 00:21:21,110
actual randomness of the string using

530
00:21:19,640 --> 00:21:24,200
Shannon entropy and this is like a very

531
00:21:21,110 --> 00:21:25,850
classic technique and so if you do the

532
00:21:24,200 --> 00:21:27,950
same thing you take a histogram of that

533
00:21:25,850 --> 00:21:30,439
and you plot that across a bunch of

534
00:21:27,950 --> 00:21:31,929
strings inputs or sorry strings outputs

535
00:21:30,440 --> 00:21:34,059
as input

536
00:21:31,929 --> 00:21:35,380
you see the ones that are that have

537
00:21:34,059 --> 00:21:37,870
lower entropy didn't be a lot more

538
00:21:35,380 --> 00:21:39,640
regular right so there are lots of

539
00:21:37,870 --> 00:21:41,979
repeating characters there versus

540
00:21:39,640 --> 00:21:43,690
strings that have higher entropy and

541
00:21:41,980 --> 00:21:46,330
that's really small for you but this is

542
00:21:43,690 --> 00:21:48,130
basically an enumeration of all the

543
00:21:46,330 --> 00:21:49,539
characters in that alphabet so it's it

544
00:21:48,130 --> 00:21:52,360
almost represents like him complete

545
00:21:49,539 --> 00:21:54,640
chaos her complete randomness from the

546
00:21:52,360 --> 00:21:55,870
point of view of his feature and so for

547
00:21:54,640 --> 00:21:57,880
this one what we want to do is filter

548
00:21:55,870 --> 00:21:58,989
out both the bottom junk and a top junk

549
00:21:57,880 --> 00:22:00,429
so there's like a sweet spot in the

550
00:21:58,990 --> 00:22:01,600
middle there and that's where we want to

551
00:22:00,429 --> 00:22:03,760
kind of capture that important

552
00:22:01,600 --> 00:22:08,289
information and hopefully filter out

553
00:22:03,760 --> 00:22:09,370
those those meaningless strings lastly

554
00:22:08,289 --> 00:22:10,899
and I thought this was interesting

555
00:22:09,370 --> 00:22:14,080
because there is actually a Scrabble

556
00:22:10,899 --> 00:22:17,020
game up here so that's a signer but uh

557
00:22:14,080 --> 00:22:18,490
we want to produce some more randomness

558
00:22:17,020 --> 00:22:20,620
measures and we can use Scrabble scores

559
00:22:18,490 --> 00:22:24,399
everyone's played Scrabble you can take

560
00:22:20,620 --> 00:22:25,120
a string and compute for each character

561
00:22:24,399 --> 00:22:27,039
in that string

562
00:22:25,120 --> 00:22:29,590
what's its corresponding Scrabble

563
00:22:27,039 --> 00:22:31,059
Scrabble tile what score that is and

564
00:22:29,590 --> 00:22:32,649
just add them up and divide by the

565
00:22:31,059 --> 00:22:34,690
length of the string well now you have a

566
00:22:32,649 --> 00:22:36,850
normalized Scrabble score and this

567
00:22:34,690 --> 00:22:39,549
produces something like this where you

568
00:22:36,850 --> 00:22:41,139
have strings with characters that occur

569
00:22:39,549 --> 00:22:43,090
very commonly on the left-hand side and

570
00:22:41,140 --> 00:22:44,980
strings with very rare characters on the

571
00:22:43,090 --> 00:22:46,240
right-hand side says a lot disease lots

572
00:22:44,980 --> 00:22:48,299
of queues on the right-hand side and

573
00:22:46,240 --> 00:22:50,140
those are also indicators of you know

574
00:22:48,299 --> 00:22:51,668
garbage we don't really care about those

575
00:22:50,140 --> 00:22:55,049
so we have another kind of sweet spot in

576
00:22:51,669 --> 00:22:55,049
this space right

577
00:22:55,549 --> 00:22:59,629
you know obviously next we have other

578
00:22:57,890 --> 00:23:02,240
interesting malware based indicators

579
00:22:59,630 --> 00:23:03,679
like hosts and network indicators and we

580
00:23:02,240 --> 00:23:05,870
can also come up with reg X's that

581
00:23:03,679 --> 00:23:08,720
capture things like format specifiers

582
00:23:05,870 --> 00:23:10,250
and base64 encoding and user agents

583
00:23:08,720 --> 00:23:12,140
which are things that you would think

584
00:23:10,250 --> 00:23:15,890
also should provide the analyst with

585
00:23:12,140 --> 00:23:17,950
some important information just a side

586
00:23:15,890 --> 00:23:21,350
note about the Scrabble stuff

587
00:23:17,950 --> 00:23:23,299
Fenny was an avid player quicks 'try was

588
00:23:21,350 --> 00:23:26,120
the highest-scoring Scrabble word ever

589
00:23:23,299 --> 00:23:29,389
used in a professional match the the

590
00:23:26,120 --> 00:23:31,100
person who played this scored three

591
00:23:29,390 --> 00:23:33,919
hundred and sixty five points with this

592
00:23:31,100 --> 00:23:36,350
single word now of course in Scrabble

593
00:23:33,919 --> 00:23:37,760
you have tiles in the board that have

594
00:23:36,350 --> 00:23:39,409
things like double word score triple

595
00:23:37,760 --> 00:23:42,260
word score weeding the comforters in our

596
00:23:39,410 --> 00:23:45,020
future but I thought the definition but

597
00:23:42,260 --> 00:23:47,090
the definition here was was interesting

598
00:23:45,020 --> 00:23:49,370
behave that means the hot behavior

599
00:23:47,090 --> 00:23:52,970
inspired by idealistic beliefs without

600
00:23:49,370 --> 00:23:54,830
regard to reality and in in kind of a

601
00:23:52,970 --> 00:23:59,059
daze letting up this talk you know I was

602
00:23:54,830 --> 00:24:00,409
able to relate to this a lot so let's

603
00:23:59,059 --> 00:24:04,190
let's kind of put these things together

604
00:24:00,410 --> 00:24:07,190
right we have labels we have features

605
00:24:04,190 --> 00:24:08,870
that we take from input strings and we

606
00:24:07,190 --> 00:24:12,500
have an algorithm to perform our ranking

607
00:24:08,870 --> 00:24:15,229
for us so let's try to feed this a set

608
00:24:12,500 --> 00:24:16,160
of unseen input strings here is a very

609
00:24:15,230 --> 00:24:18,650
simple example

610
00:24:16,160 --> 00:24:21,590
it's a strings output from up from a

611
00:24:18,650 --> 00:24:23,900
malware binary that produces 44 strings

612
00:24:21,590 --> 00:24:27,350
and we feed this through string sifter

613
00:24:23,900 --> 00:24:29,690
and we get back something that should be

614
00:24:27,350 --> 00:24:32,959
more useful I mean this is not very many

615
00:24:29,690 --> 00:24:34,760
strings so you can probably do this

616
00:24:32,960 --> 00:24:36,799
yourself a little bit easier but you

617
00:24:34,760 --> 00:24:38,570
could imagine if there were thousands or

618
00:24:36,799 --> 00:24:40,790
tens of thousands strings that this tool

619
00:24:38,570 --> 00:24:41,928
can actually end up being super useful

620
00:24:40,790 --> 00:24:45,440
for you because you get things like

621
00:24:41,929 --> 00:24:47,059
domain and domain names and file file

622
00:24:45,440 --> 00:24:48,799
names another network based indicator so

623
00:24:47,059 --> 00:24:51,020
towards the top and then towards the

624
00:24:48,799 --> 00:24:53,629
bottom you get you know P artifacts you

625
00:24:51,020 --> 00:24:55,540
get common common dll's

626
00:24:53,630 --> 00:24:57,500
and you get a lot of this chunk that

627
00:24:55,540 --> 00:24:59,809
that you know we're successfully

628
00:24:57,500 --> 00:25:03,270
filtering out based on some of those NLP

629
00:24:59,809 --> 00:25:06,639
features that we that we invented there

630
00:25:03,270 --> 00:25:10,210
so that's great looks like it works

631
00:25:06,640 --> 00:25:12,250
pretty well for that one sample but you

632
00:25:10,210 --> 00:25:14,230
know I want to I want to be able to kind

633
00:25:12,250 --> 00:25:16,750
of create more evidence for myself that

634
00:25:14,230 --> 00:25:17,860
this works in a generalized sense so I

635
00:25:16,750 --> 00:25:20,080
want to be able to measure the

636
00:25:17,860 --> 00:25:22,120
performance of this across a host of

637
00:25:20,080 --> 00:25:25,120
different input strings not just that

638
00:25:22,120 --> 00:25:27,310
single input string a string exactly and

639
00:25:25,120 --> 00:25:30,399
so here's the way in which we do this

640
00:25:27,310 --> 00:25:32,740
right so like I've described before you

641
00:25:30,400 --> 00:25:35,170
have a query which is a single strings

642
00:25:32,740 --> 00:25:37,300
output from a malware binary and you

643
00:25:35,170 --> 00:25:39,610
have a trained model gradient boosted

644
00:25:37,300 --> 00:25:41,320
decision string so you can take that

645
00:25:39,610 --> 00:25:43,810
query and make a prediction using that

646
00:25:41,320 --> 00:25:46,750
model and that will produce a set of

647
00:25:43,810 --> 00:25:49,480
scores and so for each of the strings in

648
00:25:46,750 --> 00:25:51,070
that output you have a score that

649
00:25:49,480 --> 00:25:52,480
becomes associated with that and then

650
00:25:51,070 --> 00:25:54,189
kind of like the final step here is that

651
00:25:52,480 --> 00:25:58,840
you just have to sort that output

652
00:25:54,190 --> 00:26:02,710
according to those scores okay great so

653
00:25:58,840 --> 00:26:05,470
now we have we have a set of output

654
00:26:02,710 --> 00:26:06,730
strings that we we already know what

655
00:26:05,470 --> 00:26:08,920
their ranks should be because we have

656
00:26:06,730 --> 00:26:11,440
their labels and we want to we want to

657
00:26:08,920 --> 00:26:12,790
we want to come up with some metric that

658
00:26:11,440 --> 00:26:15,220
we can use to convince ourselves that

659
00:26:12,790 --> 00:26:16,570
this should generalize across strings

660
00:26:15,220 --> 00:26:19,330
that the algorithm hasn't been exposed

661
00:26:16,570 --> 00:26:22,210
to before during training and in the

662
00:26:19,330 --> 00:26:23,409
recommendation systems literature and in

663
00:26:22,210 --> 00:26:27,070
collaborative filtering literature this

664
00:26:23,410 --> 00:26:28,390
is this is typically used this is called

665
00:26:27,070 --> 00:26:31,600
the normalized discount accumulative

666
00:26:28,390 --> 00:26:33,280
game it sounds complicated it's not I'll

667
00:26:31,600 --> 00:26:35,230
describe it in a second but importantly

668
00:26:33,280 --> 00:26:37,270
this kind of metric is very different

669
00:26:35,230 --> 00:26:40,810
than what you might expect from you know

670
00:26:37,270 --> 00:26:42,610
false positive rate or Acuras other

671
00:26:40,810 --> 00:26:45,429
other typical accuracy metrics you might

672
00:26:42,610 --> 00:26:48,429
expect from like classification or or

673
00:26:45,430 --> 00:26:49,810
clustering algorithms and so the first

674
00:26:48,430 --> 00:26:51,610
portion of this I'll start with the gain

675
00:26:49,810 --> 00:26:53,020
it's the easiest it's just the magnitude

676
00:26:51,610 --> 00:26:55,000
of each strings relevance so that's

677
00:26:53,020 --> 00:26:57,940
exactly what the scores correspond to

678
00:26:55,000 --> 00:27:00,130
associated with those strings you don't

679
00:26:57,940 --> 00:27:02,530
care about one individual score you care

680
00:27:00,130 --> 00:27:04,360
about the cumulative effect of all of

681
00:27:02,530 --> 00:27:05,668
these individual strings so you want to

682
00:27:04,360 --> 00:27:08,070
add them up

683
00:27:05,669 --> 00:27:11,489
but before but before you add them up

684
00:27:08,070 --> 00:27:14,729
you want to apply some discount and this

685
00:27:11,489 --> 00:27:16,499
discounted game ends up enabling you to

686
00:27:14,729 --> 00:27:17,940
care more about the strings that are

687
00:27:16,499 --> 00:27:20,190
ranked more towards the top of that list

688
00:27:17,940 --> 00:27:21,809
because those end up being what you end

689
00:27:20,190 --> 00:27:24,690
up caring about a lot more so when you

690
00:27:21,809 --> 00:27:26,459
when you kind of you kind of involve

691
00:27:24,690 --> 00:27:29,519
this with some monotonically increasing

692
00:27:26,459 --> 00:27:31,529
function like a logarithm that does the

693
00:27:29,519 --> 00:27:33,269
job for you of but kind of over

694
00:27:31,529 --> 00:27:35,820
emphasizing the ones that end up being

695
00:27:33,269 --> 00:27:38,129
ranked higher than the rest and then

696
00:27:35,820 --> 00:27:41,579
finally we want to normalize this

697
00:27:38,129 --> 00:27:43,889
because different strings outputs will

698
00:27:41,579 --> 00:27:45,389
contain different numbers of strings so

699
00:27:43,889 --> 00:27:49,228
in order to kind of have a good

700
00:27:45,389 --> 00:27:52,079
comparison of of our of our algorithm

701
00:27:49,229 --> 00:27:55,799
across different strings outputs you

702
00:27:52,079 --> 00:27:58,649
need to kind of divide this by what what

703
00:27:55,799 --> 00:28:00,359
would be the ideal this kind of

704
00:27:58,649 --> 00:28:02,939
cumulative game for that strings output

705
00:28:00,359 --> 00:28:05,218
and this enables you to kind of compare

706
00:28:02,940 --> 00:28:06,359
across different algorithms that you

707
00:28:05,219 --> 00:28:07,619
might come up with or different

708
00:28:06,359 --> 00:28:09,418
parameters for these different

709
00:28:07,619 --> 00:28:12,089
algorithms and it just kind of allows

710
00:28:09,419 --> 00:28:15,179
you to tune things and make things a lot

711
00:28:12,089 --> 00:28:17,190
tighter so this again the whole point of

712
00:28:15,179 --> 00:28:19,259
this is to kind of make sure that we can

713
00:28:17,190 --> 00:28:21,239
convince ourselves that this this

714
00:28:19,259 --> 00:28:24,440
approach can generalize across unseen

715
00:28:21,239 --> 00:28:24,440
samples right

716
00:28:24,590 --> 00:28:29,760
and what does this look like for a set

717
00:28:27,360 --> 00:28:32,939
of test strings that the algorithm

718
00:28:29,760 --> 00:28:35,850
hasn't been exposed to before I have I

719
00:28:32,940 --> 00:28:39,180
have to kernel density estimate plots

720
00:28:35,850 --> 00:28:41,639
here one is the gray one on the left

721
00:28:39,180 --> 00:28:43,200
hand side this is when I compute this

722
00:28:41,640 --> 00:28:46,830
normalized discounted cumulative gain

723
00:28:43,200 --> 00:28:48,600
score over strings outputs that haven't

724
00:28:46,830 --> 00:28:49,860
been run through our string sifter

725
00:28:48,600 --> 00:28:51,540
algorithm it's just the raw strings

726
00:28:49,860 --> 00:28:54,149
output you know this is kind of

727
00:28:51,540 --> 00:28:56,760
expectedly bad all right this is this is

728
00:28:54,150 --> 00:28:58,020
unsorted unranked this is exactly kind

729
00:28:56,760 --> 00:29:00,750
of the problem we're trying to fix with

730
00:28:58,020 --> 00:29:03,210
our algorithm and our approach and then

731
00:29:00,750 --> 00:29:05,610
when we when we run string sifter over

732
00:29:03,210 --> 00:29:07,500
these set of strings from kind of over

733
00:29:05,610 --> 00:29:09,020
seven years of the malware of course

734
00:29:07,500 --> 00:29:12,630
that you get these guys are generating

735
00:29:09,020 --> 00:29:14,250
you see that distribution this NDC

736
00:29:12,630 --> 00:29:16,080
distributions shift over to the right

737
00:29:14,250 --> 00:29:17,580
hand side it's closer to one higher

738
00:29:16,080 --> 00:29:19,139
score isn't is better and it's

739
00:29:17,580 --> 00:29:21,960
normalized so it's bounded between 0 & 1

740
00:29:19,140 --> 00:29:23,700
so you see we have some improvement here

741
00:29:21,960 --> 00:29:25,630
quantitative improvement and this is a

742
00:29:23,700 --> 00:29:28,090
good sign

743
00:29:25,630 --> 00:29:30,750
so to kind of put it all together before

744
00:29:28,090 --> 00:29:33,010
I headed up to J for it for a tool demo

745
00:29:30,750 --> 00:29:36,850
you know we have a set of input files

746
00:29:33,010 --> 00:29:39,190
that we we trained on some and we test

747
00:29:36,850 --> 00:29:41,230
on some others and we run them all

748
00:29:39,190 --> 00:29:42,490
through strings so we produce a ton of

749
00:29:41,230 --> 00:29:45,520
different strings from these malware

750
00:29:42,490 --> 00:29:47,559
binaries and we have training data where

751
00:29:45,520 --> 00:29:50,350
we know the labels we've an input query

752
00:29:47,559 --> 00:29:51,730
which is something like a strings output

753
00:29:50,350 --> 00:29:55,270
that the algorithm has not been trained

754
00:29:51,730 --> 00:29:58,120
on and so we have our training data that

755
00:29:55,270 --> 00:29:59,950
we use to build our model with and then

756
00:29:58,120 --> 00:30:02,860
we take the query and make predictions

757
00:29:59,950 --> 00:30:05,260
from that train model and finally you

758
00:30:02,860 --> 00:30:07,178
have our prediction and so after a

759
00:30:05,260 --> 00:30:08,620
sorting step with those scores you can

760
00:30:07,179 --> 00:30:11,200
get back this kind of stacked ranked

761
00:30:08,620 --> 00:30:13,090
list of strings and the goal here again

762
00:30:11,200 --> 00:30:15,059
is to kind of make the malware analysts

763
00:30:13,090 --> 00:30:19,080
life much easier

764
00:30:15,059 --> 00:30:19,080
so I'll end it up to chain

765
00:30:23,560 --> 00:30:29,470
okay so a quick recap of what you've

766
00:30:26,230 --> 00:30:32,290
seen so far first you saw Matt from our

767
00:30:29,470 --> 00:30:33,910
reverse engineering team he talked to

768
00:30:32,290 --> 00:30:36,490
you about strings and the power of

769
00:30:33,910 --> 00:30:38,650
strings which among this community is

770
00:30:36,490 --> 00:30:40,090
probably you know somewhat mundane coal

771
00:30:38,650 --> 00:30:42,220
it's been around forever but it's

772
00:30:40,090 --> 00:30:45,429
powerful it could be used for effective

773
00:30:42,220 --> 00:30:48,910
binary triage to guide further analysis

774
00:30:45,430 --> 00:30:51,760
of your malware sample then Phil came up

775
00:30:48,910 --> 00:30:54,010
and dropped knowledge bombs of data

776
00:30:51,760 --> 00:30:56,170
science for you he showed you fancy tree

777
00:30:54,010 --> 00:30:59,800
plots he talked about entropy he talked

778
00:30:56,170 --> 00:31:02,050
about Scrabble scores so the third

779
00:30:59,800 --> 00:31:05,050
component of this is what we really hope

780
00:31:02,050 --> 00:31:07,540
is the takeaway for you for today's talk

781
00:31:05,050 --> 00:31:10,600
which is that we are open open sourcing

782
00:31:07,540 --> 00:31:13,120
this thing we put it online live just

783
00:31:10,600 --> 00:31:15,639
this morning it's on github it's on pi

784
00:31:13,120 --> 00:31:18,340
PI if you guys have your laptop if you

785
00:31:15,640 --> 00:31:20,020
have a Linux enabled phone you know you

786
00:31:18,340 --> 00:31:21,429
can pip install it and get running with

787
00:31:20,020 --> 00:31:26,070
it you know before we're done this talk

788
00:31:21,430 --> 00:31:26,070
here is there now

789
00:31:26,160 --> 00:31:34,880
a little can you hear okay so here it is

790
00:31:30,570 --> 00:31:34,879
github fire I string sifter

791
00:31:35,360 --> 00:31:41,389
as a tangent visit the github fireEye

792
00:31:39,160 --> 00:31:43,400
collection of repos there's a large

793
00:31:41,390 --> 00:31:45,380
number of things there and there's some

794
00:31:43,400 --> 00:31:48,049
real gold in the in the fire I repose

795
00:31:45,380 --> 00:31:51,530
but hopefully we added one more nugget

796
00:31:48,049 --> 00:31:53,330
today with this a stirring sifter this

797
00:31:51,530 --> 00:31:55,760
is also on pipe ice you can do a pip

798
00:31:53,330 --> 00:31:57,500
install string sniff there it contains

799
00:31:55,760 --> 00:32:00,620
some command-line tools it contains a

800
00:31:57,500 --> 00:32:01,670
docker image that you can build and

801
00:32:00,620 --> 00:32:04,280
we'll talk through all those things as

802
00:32:01,670 --> 00:32:06,500
we go so once you install this it gives

803
00:32:04,280 --> 00:32:08,090
you two primary scripts that you'll use

804
00:32:06,500 --> 00:32:11,210
there's a tool called flare strings

805
00:32:08,090 --> 00:32:14,570
there's a tool called rank strings and

806
00:32:11,210 --> 00:32:16,669
the way you use it very straightforward

807
00:32:14,570 --> 00:32:18,740
flare strings extracts the strings you

808
00:32:16,669 --> 00:32:22,010
pipe it into rank strings and it oh

809
00:32:18,740 --> 00:32:25,760
you'll get the ranked result that Philip

810
00:32:22,010 --> 00:32:29,330
I described so this thing is versatile

811
00:32:25,760 --> 00:32:31,879
the way the piping mechanism is set up

812
00:32:29,330 --> 00:32:34,580
you can use other string generators

813
00:32:31,880 --> 00:32:37,580
fireEye has an excellent tool called

814
00:32:34,580 --> 00:32:40,040
floss that will extract not only the raw

815
00:32:37,580 --> 00:32:43,460
strings from a binary but will also do

816
00:32:40,040 --> 00:32:45,200
code analysis and will extract say

817
00:32:43,460 --> 00:32:46,790
strings that are built on the stack or

818
00:32:45,200 --> 00:32:48,710
strings that are decrypted at runtime so

819
00:32:46,790 --> 00:32:51,200
a lot of those like indirectly present

820
00:32:48,710 --> 00:32:53,270
strings within a binary are exposed by

821
00:32:51,200 --> 00:32:55,700
floss you can use that to pipe into the

822
00:32:53,270 --> 00:32:57,590
string rancor but you could also take a

823
00:32:55,700 --> 00:33:01,190
live in them or you don't say an image

824
00:32:57,590 --> 00:33:02,780
of a PE file is collected at runtime you

825
00:33:01,190 --> 00:33:06,640
can pipe that through the strings and

826
00:33:02,780 --> 00:33:06,639
extract the real value here

827
00:33:06,750 --> 00:33:11,280
when we talked so far we've talked a lot

828
00:33:08,940 --> 00:33:13,350
about signal and noise and the other

829
00:33:11,280 --> 00:33:15,540
power of this tool is to take the

830
00:33:13,350 --> 00:33:17,490
strings output which generates a lot of

831
00:33:15,540 --> 00:33:19,200
both you know you get the little nuggets

832
00:33:17,490 --> 00:33:21,720
of information and you get a lot of

833
00:33:19,200 --> 00:33:24,030
garbage in there and this tool will

834
00:33:21,720 --> 00:33:28,740
float the nuggets all into one place you

835
00:33:24,030 --> 00:33:31,250
can see them okay so here's a quick demo

836
00:33:28,740 --> 00:33:31,250
of the tools

837
00:33:31,980 --> 00:33:37,169
like here's how you install it and use

838
00:33:34,179 --> 00:33:39,640
it we have a couple variations on this

839
00:33:37,169 --> 00:33:42,400
this first one is the the mode that I

840
00:33:39,640 --> 00:33:44,500
like to use this uses gets where you get

841
00:33:42,400 --> 00:33:46,600
clone repository on to your local

842
00:33:44,500 --> 00:33:49,539
machine I keep it like an SRC directory

843
00:33:46,600 --> 00:33:52,059
or something like that so you can see it

844
00:33:49,539 --> 00:33:53,320
lays bare the content of the repo and

845
00:33:52,059 --> 00:33:55,149
you can go in there you can look at the

846
00:33:53,320 --> 00:33:57,070
Python you can look at the data files

847
00:33:55,150 --> 00:33:59,890
it's all sitting there and then you can

848
00:33:57,070 --> 00:34:02,770
use pip install with a - e flag it does

849
00:33:59,890 --> 00:34:05,380
a that's called an editable install that

850
00:34:02,770 --> 00:34:07,090
will run the setup program it brings

851
00:34:05,380 --> 00:34:08,980
those scripts into your local working

852
00:34:07,090 --> 00:34:12,219
environment but you can like C and all

853
00:34:08,980 --> 00:34:13,300
through the code right there in place so

854
00:34:12,219 --> 00:34:15,279
the instructions for that on the right

855
00:34:13,300 --> 00:34:21,220
side you can see it's just a few just a

856
00:34:15,280 --> 00:34:22,840
few if you're maximally lazy this is a

857
00:34:21,219 --> 00:34:27,009
this is the way to get up and running

858
00:34:22,840 --> 00:34:29,859
pip install strings after one word all

859
00:34:27,010 --> 00:34:31,600
lowercase and once you do that it's

860
00:34:29,859 --> 00:34:32,799
ready to go then you can you can run

861
00:34:31,600 --> 00:34:37,118
flare strings and rank strings

862
00:34:32,800 --> 00:34:38,649
immediately the other one the first one

863
00:34:37,119 --> 00:34:40,480
is my preferred method because the

864
00:34:38,649 --> 00:34:42,159
source code it enables easier visibility

865
00:34:40,480 --> 00:34:45,129
of the source code when you do pip

866
00:34:42,159 --> 00:34:47,560
install pip likes to store things in a

867
00:34:45,129 --> 00:34:49,000
deeply nested directory or if you do if

868
00:34:47,560 --> 00:34:50,739
you're running anaconda or something

869
00:34:49,000 --> 00:34:52,210
like that it'll be 15 levels deep and

870
00:34:50,739 --> 00:34:54,449
it's very hard to actually find the good

871
00:34:52,210 --> 00:34:54,449
stuff

872
00:34:55,020 --> 00:35:01,080
but finally if you like to keep things

873
00:34:57,480 --> 00:35:02,940
contained in isolated way you can build

874
00:35:01,080 --> 00:35:04,560
a little docker image then run it from

875
00:35:02,940 --> 00:35:08,690
within there that will install all the

876
00:35:04,560 --> 00:35:08,690
dependencies within docker container

877
00:35:11,690 --> 00:35:16,079
oK we've a tool called flair strings all

878
00:35:14,849 --> 00:35:20,160
of you have strings running on your

879
00:35:16,079 --> 00:35:22,109
computer but we root our own special

880
00:35:20,160 --> 00:35:23,759
version and I'll give the motivation for

881
00:35:22,109 --> 00:35:28,098
that in a moment I don't want to call

882
00:35:23,759 --> 00:35:31,099
out again the the Flair team it's fire I

883
00:35:28,099 --> 00:35:34,489
labs advanced reverse engineering team

884
00:35:31,099 --> 00:35:37,680
this mat is on the team I'm on the team

885
00:35:34,489 --> 00:35:39,480
we do reverse engineering and in the

886
00:35:37,680 --> 00:35:42,509
what I would consider the hard core

887
00:35:39,480 --> 00:35:46,079
sense we disassemble we debug we unpack

888
00:35:42,509 --> 00:35:47,309
if you have a pcap will parse it if you

889
00:35:46,079 --> 00:35:48,900
have something is up you skated will do

890
00:35:47,309 --> 00:35:50,730
if you skate it that that's really

891
00:35:48,900 --> 00:35:53,749
something that we like to do and so that

892
00:35:50,730 --> 00:35:56,609
this tool is its plan for use as a

893
00:35:53,749 --> 00:35:58,230
preliminary reconnaissance tool that we

894
00:35:56,609 --> 00:36:00,769
can use to guide the further deep dive

895
00:35:58,230 --> 00:36:00,769
analysis

896
00:36:02,540 --> 00:36:07,190
so there are many versions of strings it

897
00:36:05,000 --> 00:36:08,420
strings is a classic tool it's been

898
00:36:07,190 --> 00:36:10,340
around forever and because of that

899
00:36:08,420 --> 00:36:13,670
there's many different versions of it

900
00:36:10,340 --> 00:36:15,470
so canoe if you if you're a Linux user

901
00:36:13,670 --> 00:36:17,660
you're accustomed to the binutils

902
00:36:15,470 --> 00:36:20,660
versions of strings if you run on Mac

903
00:36:17,660 --> 00:36:23,870
you have some fork at the bsd version of

904
00:36:20,660 --> 00:36:24,920
strings if you're on windows heaven

905
00:36:23,870 --> 00:36:27,890
knows what you're running but there's

906
00:36:24,920 --> 00:36:29,650
lots of different versions of that so

907
00:36:27,890 --> 00:36:34,759
they all have their own set of features

908
00:36:29,650 --> 00:36:36,830
so just just to resolve that different

909
00:36:34,760 --> 00:36:40,400
feature sets we have a simple tool in

910
00:36:36,830 --> 00:36:42,080
here called flare strings and so it's a

911
00:36:40,400 --> 00:36:45,050
pure Python and so if you run on any of

912
00:36:42,080 --> 00:36:49,220
these environments Linux Mac Windows you

913
00:36:45,050 --> 00:36:50,750
should get the same output just the side

914
00:36:49,220 --> 00:36:52,250
note it prints both the ASCII and the

915
00:36:50,750 --> 00:36:54,650
wide strings if you run the Linux

916
00:36:52,250 --> 00:36:56,720
strings you have to run it twice that's

917
00:36:54,650 --> 00:36:58,940
it to get both all the strings and

918
00:36:56,720 --> 00:37:00,830
especially for Windows executables the

919
00:36:58,940 --> 00:37:02,360
wide strings really have signal-to-noise

920
00:37:00,830 --> 00:37:04,279
ratio they they have much higher signals

921
00:37:02,360 --> 00:37:09,410
and City so you definitely want to not

922
00:37:04,280 --> 00:37:11,770
miss those okay so here's a demo let's

923
00:37:09,410 --> 00:37:11,770
see if it runs

924
00:37:23,810 --> 00:37:29,330
color it goes okay so I started with a

925
00:37:27,680 --> 00:37:31,069
sample file this is just called narrow

926
00:37:29,330 --> 00:37:32,990
and wide and what you see here the hex

927
00:37:31,070 --> 00:37:35,240
dump if you look closely you'll see

928
00:37:32,990 --> 00:37:37,009
there's some narrow strings and ASCII

929
00:37:35,240 --> 00:37:39,709
strings and there's some wide strings

930
00:37:37,010 --> 00:37:42,230
that have the zeros interleaved with

931
00:37:39,710 --> 00:37:46,400
actual characters so if you run the

932
00:37:42,230 --> 00:37:48,980
vanilla Linux strings on that you only

933
00:37:46,400 --> 00:37:51,320
see the narrow strings but if you run

934
00:37:48,980 --> 00:37:53,000
strings again with the minus CL that

935
00:37:51,320 --> 00:37:55,670
reveals the wide strings you only see

936
00:37:53,000 --> 00:37:58,370
the wide strings and then if you run

937
00:37:55,670 --> 00:38:00,890
this tool flare strings you get them all

938
00:37:58,370 --> 00:38:02,839
so the takeaway here is player strings

939
00:38:00,890 --> 00:38:05,859
just gives you a more more uniform more

940
00:38:02,840 --> 00:38:05,860
complete output

941
00:38:11,350 --> 00:38:15,549
okay then I'll show you a demo of this

942
00:38:13,390 --> 00:38:19,779
rank strings which is really the the

943
00:38:15,550 --> 00:38:24,940
core string sifter program before we

944
00:38:19,780 --> 00:38:27,490
start to two things to know one is that

945
00:38:24,940 --> 00:38:29,320
among our feature set Phillip mentioned

946
00:38:27,490 --> 00:38:31,569
is a kind of a site throw away but

947
00:38:29,320 --> 00:38:33,220
though we have a large feature set and

948
00:38:31,570 --> 00:38:35,310
one of them is we have collections of

949
00:38:33,220 --> 00:38:39,339
windows api that have been identified

950
00:38:35,310 --> 00:38:42,250
windows api that are consistent with

951
00:38:39,340 --> 00:38:44,620
malware activity are segregated from

952
00:38:42,250 --> 00:38:46,720
other windows api that are just kind of

953
00:38:44,620 --> 00:38:50,319
vanilla and we don't care much about so

954
00:38:46,720 --> 00:38:51,669
in a desireable rank strings output you

955
00:38:50,320 --> 00:38:54,540
would see those show up into like

956
00:38:51,670 --> 00:38:57,310
clusters in the in the hierarchical view

957
00:38:54,540 --> 00:38:59,320
you would see the ones that are more

958
00:38:57,310 --> 00:39:01,660
associated with malware activity flow

959
00:38:59,320 --> 00:39:04,920
toward the top relative to your

960
00:39:01,660 --> 00:39:04,920
innocuous Wenders api

961
00:39:05,170 --> 00:39:08,360
[Music]

962
00:39:11,250 --> 00:39:17,730
so what the demo does first is this is

963
00:39:14,160 --> 00:39:20,460
the classical Mao or analyst way of

964
00:39:17,730 --> 00:39:22,770
running strings so this is not the rank

965
00:39:20,460 --> 00:39:25,440
strings this is where we're computing

966
00:39:22,770 --> 00:39:28,619
the strings on a sample in piping it to

967
00:39:25,440 --> 00:39:30,720
Les in just like eyeballing it and so

968
00:39:28,619 --> 00:39:34,560
this is unranked strings and what you

969
00:39:30,720 --> 00:39:36,540
see here I'm paging through this using

970
00:39:34,560 --> 00:39:39,480
the les program this is like a real

971
00:39:36,540 --> 00:39:41,910
malware analyst workflow you just have

972
00:39:39,480 --> 00:39:45,079
to hunt through all this stuff and look

973
00:39:41,910 --> 00:39:47,310
for good things you may miss some things

974
00:39:45,079 --> 00:39:50,390
but you end up spending a lot of time

975
00:39:47,310 --> 00:39:52,009
looking at this crap here

976
00:39:50,390 --> 00:39:53,319
he wakes me with this workflow

977
00:39:52,010 --> 00:39:56,440
[Music]

978
00:39:53,320 --> 00:39:59,800
oh look there's some stuff and so I back

979
00:39:56,440 --> 00:40:01,960
up a little bit I see some API keys it's

980
00:39:59,800 --> 00:40:05,680
kind of hard to read on this but you

981
00:40:01,960 --> 00:40:07,180
guys all know what the strings strings

982
00:40:05,680 --> 00:40:11,100
output looks like I see a registry key

983
00:40:07,180 --> 00:40:11,100
somewhere in here there's a PDB path

984
00:40:11,520 --> 00:40:18,940
this malware is the Zeus remote access

985
00:40:15,640 --> 00:40:20,830
trojan I compiled it from source so yeah

986
00:40:18,940 --> 00:40:23,310
there is a PDB associated with where I

987
00:40:20,830 --> 00:40:23,310
built it from

988
00:40:30,210 --> 00:40:35,640
so eventually I either get to the bottom

989
00:40:32,609 --> 00:40:39,480
or just give up and I'm sure all of you

990
00:40:35,640 --> 00:40:40,950
if if you have a large binary like this

991
00:40:39,480 --> 00:40:42,390
can be prolific and you can spend some

992
00:40:40,950 --> 00:40:43,618
time iterating with the width of the

993
00:40:42,390 --> 00:40:45,810
strings or output by your strings

994
00:40:43,619 --> 00:40:49,170
program but really this can be a

995
00:40:45,810 --> 00:40:52,440
headache II okay so here this is a this

996
00:40:49,170 --> 00:40:55,859
is a ranked strings version here and the

997
00:40:52,440 --> 00:40:58,290
takeaway for you is the the density of

998
00:40:55,859 --> 00:41:01,319
the value of the strings that are in

999
00:40:58,290 --> 00:41:06,540
this set float up to the top so so if

1000
00:41:01,320 --> 00:41:08,099
you imagine relevance score versus where

1001
00:41:06,540 --> 00:41:09,869
the string is M placed in the file it's

1002
00:41:08,099 --> 00:41:11,820
like a descending slope you know the

1003
00:41:09,869 --> 00:41:14,070
juicy ones are up near the top is if

1004
00:41:11,820 --> 00:41:15,510
somebody raked all of the Nuggets into

1005
00:41:14,070 --> 00:41:16,859
one corner of the room and you just go

1006
00:41:15,510 --> 00:41:20,720
to that corner and you can immediately

1007
00:41:16,859 --> 00:41:20,720
address the things that are of interest

1008
00:41:23,109 --> 00:41:28,959
as a sidenote since the the feature

1009
00:41:26,859 --> 00:41:31,299
vector that is extracted from the

1010
00:41:28,959 --> 00:41:33,308
strings is going to be consistent with

1011
00:41:31,299 --> 00:41:35,410
the string input itself if you have a

1012
00:41:33,309 --> 00:41:40,630
duplicate string they'll show up ranked

1013
00:41:35,410 --> 00:41:42,190
as its high so they show up together in

1014
00:41:40,630 --> 00:41:44,259
this example we have the this little

1015
00:41:42,190 --> 00:41:46,420
program cannot be run in DOS mode

1016
00:41:44,259 --> 00:41:48,190
there's two instances of that because

1017
00:41:46,420 --> 00:41:50,670
this one is a file that contains another

1018
00:41:48,190 --> 00:41:52,779
file and the contained file is not

1019
00:41:50,670 --> 00:41:55,989
obfuscated in any way so you just get

1020
00:41:52,779 --> 00:41:58,690
the naked preamble there and so you see

1021
00:41:55,989 --> 00:42:03,219
it it sits there co-located and the

1022
00:41:58,690 --> 00:42:05,559
sareng strings output so I encourage you

1023
00:42:03,219 --> 00:42:06,849
do the PIP install download this to your

1024
00:42:05,559 --> 00:42:09,400
computer run it against your favorite

1025
00:42:06,849 --> 00:42:11,920
malware samples this is primarily tuned

1026
00:42:09,400 --> 00:42:16,539
for PE files in future work we're going

1027
00:42:11,920 --> 00:42:21,690
to be working on Mac files and Linux elf

1028
00:42:16,539 --> 00:42:21,690
files but I think a very powerful tool

1029
00:42:25,330 --> 00:42:29,830
as a good Python program this uses art

1030
00:42:28,090 --> 00:42:31,770
parts and we have some options that you

1031
00:42:29,830 --> 00:42:34,480
can use

1032
00:42:31,770 --> 00:42:37,320
I'll just rather than talk through these

1033
00:42:34,480 --> 00:42:40,429
I'll just give you a quick demo

1034
00:42:37,320 --> 00:42:40,429
[Music]

1035
00:42:40,490 --> 00:42:47,209
if you use this - - scores which is a

1036
00:42:45,080 --> 00:42:49,279
list - - s it'll just give you the

1037
00:42:47,210 --> 00:42:51,470
little numeric score associated with the

1038
00:42:49,280 --> 00:42:54,050
with the features you can see how hot

1039
00:42:51,470 --> 00:42:57,709
the information is so if you have a

1040
00:42:54,050 --> 00:42:59,930
sample like this one this one is a

1041
00:42:57,710 --> 00:43:02,300
sample of Dharma malware it's a

1042
00:42:59,930 --> 00:43:04,700
ransomware sample but it's up you skated

1043
00:43:02,300 --> 00:43:08,080
so there's a only two visible imported

1044
00:43:04,700 --> 00:43:13,310
API there's no user strings and here

1045
00:43:08,080 --> 00:43:16,040
it's a 99% this garbage so if you look

1046
00:43:13,310 --> 00:43:17,960
at the score on the Left we're looking

1047
00:43:16,040 --> 00:43:22,400
at negative scoring stuff here so this

1048
00:43:17,960 --> 00:43:24,109
is just noise so you can print when I

1049
00:43:22,400 --> 00:43:25,849
mentioned the descending slope of the

1050
00:43:24,109 --> 00:43:27,410
information value of each of those

1051
00:43:25,849 --> 00:43:30,070
strings this this actually shows you

1052
00:43:27,410 --> 00:43:30,069
that slope

1053
00:43:34,640 --> 00:43:40,100
and running again with the same sample

1054
00:43:36,260 --> 00:43:41,480
this Minsk or option just says you as

1055
00:43:40,100 --> 00:43:42,920
the user could say you know what I don't

1056
00:43:41,480 --> 00:43:45,140
even care about the things that score

1057
00:43:42,920 --> 00:43:46,910
less than something so in this case I

1058
00:43:45,140 --> 00:43:48,710
use four four because that's where the

1059
00:43:46,910 --> 00:43:50,359
noise cutoff was on this particular

1060
00:43:48,710 --> 00:43:53,590
sample and it just gives me those four

1061
00:43:50,360 --> 00:43:53,590
lines that work the top

1062
00:43:56,589 --> 00:44:02,589
okay so we have this tool it's uh it's

1063
00:43:59,720 --> 00:44:06,230
it's stable enough and useful enough

1064
00:44:02,589 --> 00:44:08,480
that we think it's ready for for sharing

1065
00:44:06,230 --> 00:44:11,059
with a community and so we did that's

1066
00:44:08,480 --> 00:44:13,640
why we're here today but we do we are we

1067
00:44:11,059 --> 00:44:18,279
do plan to take this further as we as we

1068
00:44:13,640 --> 00:44:21,259
continue to develop it we would like to

1069
00:44:18,279 --> 00:44:23,239
leverage the feature set that comes out

1070
00:44:21,259 --> 00:44:26,660
of the derived features from the string

1071
00:44:23,239 --> 00:44:29,180
we can use those if we if we take that

1072
00:44:26,660 --> 00:44:31,549
whole matrix of strings and feature

1073
00:44:29,180 --> 00:44:33,109
vector and transpose it so that we're

1074
00:44:31,549 --> 00:44:35,809
looking at the feature vectors and what

1075
00:44:33,109 --> 00:44:37,279
strings were corresponding with those we

1076
00:44:35,809 --> 00:44:42,170
can use that as a potential screening

1077
00:44:37,279 --> 00:44:44,630
tool for malware capabilities new types

1078
00:44:42,170 --> 00:44:46,940
a screening tool not a detection so we

1079
00:44:44,630 --> 00:44:48,890
really can't detect until we do a proper

1080
00:44:46,940 --> 00:44:50,239
reverse engineering and code flow essay

1081
00:44:48,890 --> 00:44:52,519
is this thing really hit is this thing

1082
00:44:50,239 --> 00:44:55,130
really used where we think it is but in

1083
00:44:52,519 --> 00:44:59,229
terms of a preliminary analysis this be

1084
00:44:55,130 --> 00:44:59,230
a valuable first step and time-saver

1085
00:45:00,549 --> 00:45:05,329
but we can detect and handle pack there

1086
00:45:03,200 --> 00:45:08,118
off you skated binaries that Dharma

1087
00:45:05,329 --> 00:45:09,650
example I just gave you quite an example

1088
00:45:08,119 --> 00:45:12,559
like if you took the ranks during the

1089
00:45:09,650 --> 00:45:15,650
output and did analysis on that curve

1090
00:45:12,559 --> 00:45:18,980
it's quite obvious there's not it's if

1091
00:45:15,650 --> 00:45:21,079
you get a very low signal noise ratio

1092
00:45:18,980 --> 00:45:23,690
on average that we could use that as a

1093
00:45:21,079 --> 00:45:26,420
tip-off for further analysis if we have

1094
00:45:23,690 --> 00:45:28,160
a dedicated standalone unpacker if we if

1095
00:45:26,420 --> 00:45:31,900
we need manual analysis that the office

1096
00:45:28,160 --> 00:45:31,899
Kate thought that's a signal for that

1097
00:45:33,770 --> 00:45:39,410
yeah we can leverage the future vectors

1098
00:45:35,540 --> 00:45:41,690
F to focus triage the Filip dropped a

1099
00:45:39,410 --> 00:45:44,480
couple great techniques for natural

1100
00:45:41,690 --> 00:45:48,470
language processing but that is a huge

1101
00:45:44,480 --> 00:45:50,210
area where if we view strings as a

1102
00:45:48,470 --> 00:45:52,609
communication between the software

1103
00:45:50,210 --> 00:45:54,890
developer who's writing code and writing

1104
00:45:52,610 --> 00:45:57,050
messages and us as the people that are

1105
00:45:54,890 --> 00:45:59,150
kind of looking at those messages the

1106
00:45:57,050 --> 00:46:01,660
the space for language processing is

1107
00:45:59,150 --> 00:46:01,660
just immense

1108
00:46:03,760 --> 00:46:07,260
and then as I mentioned before we do

1109
00:46:05,770 --> 00:46:12,750
definitely want to extend this to other

1110
00:46:07,260 --> 00:46:12,750
binary platforms or Marco and else

1111
00:46:14,390 --> 00:46:20,600
okay so for community it's here plug it

1112
00:46:17,120 --> 00:46:22,940
into your malware analysis stack look at

1113
00:46:20,600 --> 00:46:25,970
the red arrow we are looking for

1114
00:46:22,940 --> 00:46:29,600
critical feedback we expect you to use

1115
00:46:25,970 --> 00:46:32,180
this you may like it you may be annoyed

1116
00:46:29,600 --> 00:46:34,009
by it you may find features of it it

1117
00:46:32,180 --> 00:46:35,960
says you know this there's feature X

1118
00:46:34,010 --> 00:46:37,880
that's in my sample and it wasn't

1119
00:46:35,960 --> 00:46:39,560
illuminated by this or was it did a

1120
00:46:37,880 --> 00:46:43,400
great job with this like we want to hear

1121
00:46:39,560 --> 00:46:45,710
that stuff and so we have on our github

1122
00:46:43,400 --> 00:46:47,810
site the issues is active and enabled

1123
00:46:45,710 --> 00:46:48,440
the last time I looked which is about an

1124
00:46:47,810 --> 00:46:50,840
hour ago

1125
00:46:48,440 --> 00:46:53,840
there were zero on there so be the first

1126
00:46:50,840 --> 00:46:57,530
one to post an issue to our github but

1127
00:46:53,840 --> 00:46:59,090
we want this to not be a one-way here

1128
00:46:57,530 --> 00:47:00,590
take this tool and use it we want this

1129
00:46:59,090 --> 00:47:02,660
to be a dialog so that we can improve

1130
00:47:00,590 --> 00:47:04,250
the tool so that it makes us better at

1131
00:47:02,660 --> 00:47:06,680
fireEye but so it makes you guys better

1132
00:47:04,250 --> 00:47:09,820
to any room malware analysis uh care

1133
00:47:06,680 --> 00:47:09,819
about the techniques

1134
00:47:10,339 --> 00:47:15,779
okay so thank you we have a couple

1135
00:47:13,260 --> 00:47:19,430
minutes the guys are still here so if

1136
00:47:15,780 --> 00:47:19,430
any questions are there

1137
00:47:47,870 --> 00:47:54,620
okay excellent questions one the first

1138
00:47:51,770 --> 00:47:56,180
question was we have special handling

1139
00:47:54,620 --> 00:47:58,430
for this program can only be run in DOS

1140
00:47:56,180 --> 00:48:00,609
mode if there's a typo in that or some

1141
00:47:58,430 --> 00:48:03,049
deviation from that would it have been

1142
00:48:00,610 --> 00:48:07,730
DL you know push down like it was

1143
00:48:03,050 --> 00:48:10,490
alright pushed up yeah it's a great

1144
00:48:07,730 --> 00:48:11,870
question so that specific string is part

1145
00:48:10,490 --> 00:48:13,970
of like a little bit of a white list

1146
00:48:11,870 --> 00:48:16,569
that we have right as a feature but we

1147
00:48:13,970 --> 00:48:20,089
also have other features like these

1148
00:48:16,570 --> 00:48:22,790
these randomness scores right and so we

1149
00:48:20,090 --> 00:48:24,050
have features that detect when input

1150
00:48:22,790 --> 00:48:26,720
strings look more like the English

1151
00:48:24,050 --> 00:48:28,520
language and so because that string it's

1152
00:48:26,720 --> 00:48:30,890
not exactly hitting that white list that

1153
00:48:28,520 --> 00:48:32,450
white listed strings out but it should

1154
00:48:30,890 --> 00:48:34,040
be detected as someday that looks more

1155
00:48:32,450 --> 00:48:35,960
like English right because it is it has

1156
00:48:34,040 --> 00:48:39,950
that exclamation point but generally it

1157
00:48:35,960 --> 00:48:41,030
looks like English text and so I'm not

1158
00:48:39,950 --> 00:48:42,259
sure up to top my head what the score

1159
00:48:41,030 --> 00:48:44,060
would be but I would hope that you could

1160
00:48:42,260 --> 00:48:46,250
cover that that individual feature and

1161
00:48:44,060 --> 00:48:52,009
what was the other one with the Mandarin

1162
00:48:46,250 --> 00:48:54,290
characters yeah that's so we do have

1163
00:48:52,010 --> 00:48:56,990
what we have Cyrillic we have Mandarin

1164
00:48:54,290 --> 00:49:00,259
we have other like Arabic basically non

1165
00:48:56,990 --> 00:49:02,390
non Latin language set as also one of

1166
00:49:00,260 --> 00:49:05,960
these features so so those things should

1167
00:49:02,390 --> 00:49:09,460
also kind of push up but yeah fingers

1168
00:49:05,960 --> 00:49:09,460
crossed if you test it

1169
00:49:26,350 --> 00:49:31,940
okay the question was when we use floss

1170
00:49:29,180 --> 00:49:33,680
output as an input to rank strings do we

1171
00:49:31,940 --> 00:49:35,000
differentiate between the vanilla

1172
00:49:33,680 --> 00:49:39,399
strings and the ones that were D

1173
00:49:35,000 --> 00:49:43,040
obfuscated by floss the answer is no but

1174
00:49:39,400 --> 00:49:45,470
the further answer is we are developing

1175
00:49:43,040 --> 00:49:46,940
this with a view to since we have the

1176
00:49:45,470 --> 00:49:48,470
strings since we have control of the

1177
00:49:46,940 --> 00:49:50,450
strings program and we have control of

1178
00:49:48,470 --> 00:49:53,540
the rank strange program we can add in

1179
00:49:50,450 --> 00:49:55,009
side channel information like that so we

1180
00:49:53,540 --> 00:49:56,750
can say here's the floss output

1181
00:49:55,010 --> 00:49:58,340
here's two channels for it here's the

1182
00:49:56,750 --> 00:50:00,850
vanilla strings here's the exciting

1183
00:49:58,340 --> 00:50:05,170
strings and handle those differently so

1184
00:50:00,850 --> 00:50:05,170
no but that's definitely on the roadmap

1185
00:50:06,010 --> 00:50:11,240
okay yeah hit us outside we got the stop

1186
00:50:09,530 --> 00:50:13,430
sign so we're gonna have to migrate but

1187
00:50:11,240 --> 00:50:15,259
yeah will linger back in the corner or

1188
00:50:13,430 --> 00:50:19,098
something for a couple minutes

1189
00:50:15,260 --> 00:50:19,099
[Applause]

