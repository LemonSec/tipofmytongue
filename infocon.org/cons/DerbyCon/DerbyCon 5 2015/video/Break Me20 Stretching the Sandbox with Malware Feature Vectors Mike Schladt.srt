1
00:00:05,920 --> 00:00:40,089
stuck in here now carole we're gonna be
alright thanks for being here is like

2
00:00:40,089 --> 00:00:43,549
the second-to-last talking all of their
be gone so it's it's pretty great to see

3
00:00:43,549 --> 00:00:46,890
that there's actually know some people
here I was kind of a concern records got

4
00:00:46,890 --> 00:00:51,750
my time slot that would be basically
myself and my wife which is why I made a

5
00:00:51,750 --> 00:00:56,699
drug test here she also had me on the
project so I figured she had to make an

6
00:00:56,699 --> 00:01:01,269
appearance today we're talking about a
project I'm falling stretching the

7
00:01:01,269 --> 00:01:06,320
sandbox with our feature vectors and I'm
explain what those mean essentially a

8
00:01:06,320 --> 00:01:10,420
project that's been on my mind for a
while and then you can kind of gave me

9
00:01:10,420 --> 00:01:15,359
the motivation to go out there and put
some stuff out and now we're talking

10
00:01:15,359 --> 00:01:21,560
about it so I know about half the room
knows who I am because I I drug you hear

11
00:01:21,560 --> 00:01:28,549
the other half the room number to me
like that I am actually kind of local

12
00:01:28,549 --> 00:01:32,780
originally from Lexington Kentucky
currently living in Cincinnati where I

13
00:01:32,780 --> 00:01:39,359
work for a fortune 1000 company who I
did not give permission to use these

14
00:01:39,359 --> 00:01:43,159
slides that's why the name is not here
in any way shape or form and insert you

15
00:01:43,159 --> 00:01:49,460
know legal disclaimer there but my day
job is Angeli instant response and

16
00:01:49,460 --> 00:01:53,289
analysis yet our stuff

17
00:01:53,289 --> 00:01:58,499
analysis engineering that kind of stuff
in a previous life was active duty Air

18
00:01:58,499 --> 00:02:03,829
Force where I did four years working in
the power lab at the National Air and

19
00:02:03,829 --> 00:02:06,030
Space Intelligence dinner

20
00:02:06,030 --> 00:02:16,540
yes thank you just said things that I am
this is kinda nice part to my disclaimer

21
00:02:16,540 --> 00:02:21,100
you know I consider myself a haggard
specifically you know I R version Jerry

22
00:02:21,100 --> 00:02:22,359
malware that kind of stuff

23
00:02:22,360 --> 00:02:27,120
engineer you know tray error by
education I like to build things like to

24
00:02:27,120 --> 00:02:32,489
put stuff together that's sort of what I
do I am NOT a professional developer so

25
00:02:32,489 --> 00:02:35,430
it's kind of scary is actually one of
the first products that released now

26
00:02:35,430 --> 00:02:39,140
they're open source so I fully expect
you guys to terror that completely up

27
00:02:39,140 --> 00:02:48,300
and they welcome it I'm also but please
do it I'm also not a mathematician in

28
00:02:48,300 --> 00:02:51,670
any way shape or form so we're going to
be talking about the feature vectors and

29
00:02:51,670 --> 00:02:56,160
doing some statistical analysis on this
feature vectors I I had a

30
00:02:56,160 --> 00:03:01,350
undergraduate statistics couple times
and so have like a very elementary grasp

31
00:03:01,350 --> 00:03:08,260
of what I'm even talking about so I have
no idea what I'm doing right but I got

32
00:03:08,260 --> 00:03:13,850
this weird talk about anyways I had to
bring it up my wife jessie is actually a

33
00:03:13,850 --> 00:03:18,880
mechanical engineer she's just taking
all kinds of higher level classes right

34
00:03:18,880 --> 00:03:22,980
now and she makes sure that airplanes
don't fall skies are run by anything I

35
00:03:22,980 --> 00:03:27,720
did before her by her I put it up on a
slide she told me it was completely

36
00:03:27,720 --> 00:03:35,440
stupid but I did it he was sweating over
I guess the road man talking to be

37
00:03:35,440 --> 00:03:38,420
talking about the agenda

38
00:03:38,420 --> 00:03:43,090
reduce the product you know what it is
what the motivation is behind it may

39
00:03:43,090 --> 00:03:46,440
give a little bit of thoughts on
sandboxing in general I know that most

40
00:03:46,440 --> 00:03:50,370
people you probably are very familiar
sandboxing which is probably why you

41
00:03:50,370 --> 00:03:53,500
came to talk to to begin with but just
in case

42
00:03:53,500 --> 00:03:58,840
missing people gonna talk about a couple
points feeding the beast our collection

43
00:03:58,840 --> 00:04:05,709
so how do you actually populate your or
your body of knowledge I'm not going to

44
00:04:05,709 --> 00:04:07,350
make this like an Intel collection

45
00:04:07,350 --> 00:04:10,290
talking any way shape or form but
there's a couple basic things that you

46
00:04:10,290 --> 00:04:19,660
went into populating those populating
the initial until the real briefly that

47
00:04:19,660 --> 00:04:23,280
is believed that is basically what I
used for various reasons to support the

48
00:04:23,280 --> 00:04:28,010
back end of the analysis here and talk
about what our feature vectors actually

49
00:04:28,010 --> 00:04:28,530
are

50
00:04:28,530 --> 00:04:32,230
I made them up and you know what my
thought process for making them up was

51
00:04:32,230 --> 00:04:36,880
then they go over some of the code and
some of the plots that were able to to

52
00:04:36,880 --> 00:04:43,400
use to talk about all the things I
didn't finish so what we talk about when

53
00:04:43,400 --> 00:04:50,150
we hear so many analysts gets a piece of
malware the first thing they ask

54
00:04:50,150 --> 00:04:53,820
themselves is what the hell am i working
at I got this I got the sample maybe I

55
00:04:53,820 --> 00:04:57,360
got it from a host that wasn't directed
at you working operations you're pulling

56
00:04:57,360 --> 00:04:59,610
things are going to legends you know
from here

57
00:04:59,610 --> 00:05:03,810
guys every day I get a sample you might
know immediately what it is because it

58
00:05:03,810 --> 00:05:07,370
has a particularly no be getting bad you
might know immediately what it is

59
00:05:07,370 --> 00:05:10,380
because the registry keys that creates
because the new texts that it creates

60
00:05:10,380 --> 00:05:14,770
because of the positive rights the
things that drops however a lot of that

61
00:05:14,770 --> 00:05:18,300
to be mean or process we're moving more
towards you know at an age where we're

62
00:05:18,300 --> 00:05:22,030
getting a lot of power in you know from
automated sources and not all the time

63
00:05:22,030 --> 00:05:25,820
because somebody's looking eyes on these
things and so on those cases your

64
00:05:25,820 --> 00:05:29,390
processing thousands of malware samples
are your processing mount thousands of

65
00:05:29,390 --> 00:05:33,960
you know binaries maybe they're not even
malicious having that you know

66
00:05:33,960 --> 00:05:37,980
assessment of what is this how can I
classify the sky cluster these together

67
00:05:37,980 --> 00:05:43,340
and I do that sort of grouping and
correlation

68
00:05:43,340 --> 00:05:47,060
it becomes important kind of interesting
especially on the scale that we know

69
00:05:47,060 --> 00:05:50,690
that we work on today of course there's
lots of projects to do this there's lots

70
00:05:50,690 --> 00:05:56,020
of times and I'm not even doing like a
prior art page I hear mostly because I'm

71
00:05:56,020 --> 00:06:01,250
lazy but also because I assume that you
guys have Google you go out there you

72
00:06:01,250 --> 00:06:04,800
can view our clustering there's a ton of
great things that you out there a lot of

73
00:06:04,800 --> 00:06:10,840
its static one of my good friends who is
not here can actually did our DNA

74
00:06:10,840 --> 00:06:12,200
project which is exactly

75
00:06:12,200 --> 00:06:17,480
the same type of goals but using your
static analysis so the second part of

76
00:06:17,480 --> 00:06:23,140
the objectives here the motivation is
we've got a ton of data from marcia

77
00:06:23,140 --> 00:06:26,110
boxes right i mean everything very same
box I don't know about you and your

78
00:06:26,110 --> 00:06:31,910
organizations and how you guys operate a
sample of the sandbox and we can we do

79
00:06:31,910 --> 00:06:35,920
this sometimes for a virus hunting will
be doubling down hundreds or thousands

80
00:06:35,920 --> 00:06:43,010
samples through analysis engine and a
lot of times a day to just kind of sit

81
00:06:43,010 --> 00:06:45,750
here nobody ever looks at again right i
mean these things generate hundreds of

82
00:06:45,750 --> 00:06:50,910
megabytes per the data and it's just
gonna sit there we have it put you in a

83
00:06:50,910 --> 00:06:54,870
couple of projects to correlate some of
the stuff together one of them at work

84
00:06:54,870 --> 00:06:59,030
has been my baby the past year I'm not
presenting of that today but this one is

85
00:06:59,030 --> 00:07:03,800
something that they did we can share and
and and look at would like to have some

86
00:07:03,800 --> 00:07:09,040
other guys so here's here's the talk a
little bit about Sandboxie again i'm

87
00:07:09,040 --> 00:07:12,830
looking like nobody does down too hard
like bigger than most people are pretty

88
00:07:12,830 --> 00:07:16,380
familiar it was a boxing is the concept
behind it right you do it you know

89
00:07:16,380 --> 00:07:21,250
traditionally you have your static and
dynamic analysis tools that people been

90
00:07:21,250 --> 00:07:25,880
using for years and the whole idea of
ice and boxing is that we want to

91
00:07:25,880 --> 00:07:29,330
automate we went to great you know and
isolated environment for automating you

92
00:07:29,330 --> 00:07:34,100
know system observation and sandbox has
really exploded over the past five years

93
00:07:34,100 --> 00:07:38,510
and never prior to five years ago they
really weren't a whole lot of first one

94
00:07:38,510 --> 00:07:42,190
there wasn't a whole lot of open source
anything I think the project started

95
00:07:42,190 --> 00:07:46,910
about that time but there may be one or
two vendors and they're charging a whole

96
00:07:46,910 --> 00:07:51,540
bunch of the sand boxes and then a lot
of people kind of rolling their own that

97
00:07:51,540 --> 00:07:55,590
time but over the past five years is
kind of a kind of taking off to discover

98
00:07:55,590 --> 00:07:59,940
thoughts on rolling your own vs fender
vs open sores I don't want to start like

99
00:07:59,940 --> 00:08:04,650
any sort of like war here by talking
about this but just my $0.02 on your

100
00:08:04,650 --> 00:08:10,620
kind of the pros and cons sandbox you
have control over everything you can do

101
00:08:10,620 --> 00:08:21,360
very custom custom anti anti sandbox I
guess that's a word in science I say

102
00:08:21,360 --> 00:08:24,870
boxing technique so you can really know
have full control over or what that

103
00:08:24,870 --> 00:08:29,880
looks like a lot of times if you really
are the guys that right now where they

104
00:08:29,880 --> 00:08:36,760
have the inside you know inside baseball
knowledge of what you're doing to try to

105
00:08:36,760 --> 00:08:40,349
evade than 6 I'm kinda thing with a
vendor exactly don't have any control

106
00:08:40,349 --> 00:08:41,099
over it

107
00:08:41,099 --> 00:08:45,630
the vendor has all the control over it
they also charge a lot of money so there

108
00:08:45,630 --> 00:08:51,000
is that they consider whether there's
open source and there's two major major

109
00:08:51,000 --> 00:08:53,540
major major disadvantage major
advantages you got this great community

110
00:08:53,540 --> 00:08:58,200
to support its constantly being updated
the features that something like who's

111
00:08:58,200 --> 00:09:02,640
offered over the past couple years it's
absolutely exploded major disadvantages

112
00:09:02,640 --> 00:09:12,510
yes a lot of our does that out there has
been no somebody I don't you hate it

113
00:09:12,510 --> 00:09:19,870
absolutely that's absolutely true so we
gotta know what you're dealing with you

114
00:09:19,870 --> 00:09:22,620
know you know it's definitely possible
that members of aiding your open source

115
00:09:22,620 --> 00:09:26,150
and blogs and you need to take that with
a grain of salt

116
00:09:26,150 --> 00:09:29,430
so basically where we operate in our
requirements we do everything for you

117
00:09:29,430 --> 00:09:34,060
and we get back of course we can kind of
tell a distinct exit immediately didn't

118
00:09:34,060 --> 00:09:38,550
do a whole lot but even the way that it
exits sometimes can be very useful

119
00:09:38,550 --> 00:09:42,510
especially when you get into this
product is still gonna generate feature

120
00:09:42,510 --> 00:09:47,459
vectors which obviously talked about
that are depending on you it's inside of

121
00:09:47,459 --> 00:09:53,890
you and I you know Sam boxing techniques
as well and if you didn't know I am

122
00:09:53,890 --> 00:10:00,640
using Google beating the beat again I
don't want to blame for this point but

123
00:10:00,640 --> 00:10:04,640
so the whole idea behind this project is
going to take unknown samples were

124
00:10:04,640 --> 00:10:07,490
trying to classify them well you have to
start with something right you have to

125
00:10:07,490 --> 00:10:12,810
start with your known knowns so it
starts no known you have to have to have

126
00:10:12,810 --> 00:10:16,700
some sort of our collection of some sort
into a program of some sort of process

127
00:10:16,700 --> 00:10:20,010
it doesn't have to be elaborate maybe
its elaborate that's going to be

128
00:10:20,010 --> 00:10:22,820
your organization I'm not gonna tell you
how to run your your organization's

129
00:10:22,820 --> 00:10:26,080
Miller collection or insult program I'm
just gonna say that for the purpose of

130
00:10:26,080 --> 00:10:30,880
this project is a couple pieces
information that I collected collected

131
00:10:30,880 --> 00:10:34,960
hashes I collected the source that I got
the samples prom I collect the source

132
00:10:34,960 --> 00:10:40,430
that supposed to say day not dad I
collected the file type of our family so

133
00:10:40,430 --> 00:10:45,570
for the purpose of this project I
basically went out to open source

134
00:10:45,570 --> 00:10:52,150
research where you know somebody like
SecureWorks or hire somebody released a

135
00:10:52,150 --> 00:10:57,710
report on some some our taxes said hey
these samples belonged to the same

136
00:10:57,710 --> 00:11:04,060
family and I said I hope this report on
this report date these hashes belong to

137
00:11:04,060 --> 00:11:07,560
this this is the file types they were
because the when you actually do

138
00:11:07,560 --> 00:11:11,320
analysis file types very very important
is apparently a deal expiry very

139
00:11:11,320 --> 00:11:19,180
differently than an exe as far as when
you actually executed as a box of course

140
00:11:19,180 --> 00:11:22,590
I do I do this I create an awesome
spreadsheet as I'm just going through

141
00:11:22,590 --> 00:11:28,840
these reports demonstrated UCSB and
actually created a tool that is part of

142
00:11:28,840 --> 00:11:34,720
the project I tangentially had text up
high and it will basically take all that

143
00:11:34,720 --> 00:11:42,960
trades data to CSV and it's a very
simple table this tables were super

144
00:11:42,960 --> 00:11:46,540
simple and the purpose behind it is just
to get all the information in a

145
00:11:46,540 --> 00:11:51,930
relational format so that you can easily
tacit and associated with the analysis

146
00:11:51,930 --> 00:12:00,260
of course let's take a look when they
see could do they look at this they get

147
00:12:00,260 --> 00:12:04,860
a pretty web interface and show them
what they need

148
00:12:04,860 --> 00:12:08,890
I think most analysts that I know don't
really go further than this

149
00:12:08,890 --> 00:12:12,440
they they see the things that you see
they make it a determination whether or

150
00:12:12,440 --> 00:12:16,970
not they think it's militias they might
copy some of the c2 down into some sort

151
00:12:16,970 --> 00:12:17,600
of

152
00:12:17,600 --> 00:12:22,459
repository we use creates a familiar
that project and then life

153
00:12:22,459 --> 00:12:27,870
well there is a lot behind this name
there are these also like 500 megabytes

154
00:12:27,870 --> 00:12:31,720
are up to 50 megabytes JSON or is this
just nothing but raw data and that's

155
00:12:31,720 --> 00:12:44,259
really what we're looking at right now
our feature vectors what are we talking

156
00:12:44,259 --> 00:12:48,079
about so I started using the word
feature vector I wasn't even exactly

157
00:12:48,079 --> 00:12:51,930
sure I was using the right word but the
definition from Wikipedia makes me think

158
00:12:51,930 --> 00:12:56,160
that maybe I am i dont know so we can
video says that feature vector is an in

159
00:12:56,160 --> 00:13:00,660
dimensional vector of numerical features
that represents some object in the

160
00:13:00,660 --> 00:13:04,839
example that they actually use they said
well usually you're gonna see this like

161
00:13:04,839 --> 00:13:08,680
image processing and signal processing
that kind of stuff but you can also

162
00:13:08,680 --> 00:13:12,019
think about it conceptually like well if
you want to create a feature vector of

163
00:13:12,019 --> 00:13:15,310
all the fruit that's on the table you
can say that you for Apple's

164
00:13:15,310 --> 00:13:21,240
9 bananas and you have 10 players and
then your victory that one would be 49

165
00:13:21,240 --> 00:13:30,019
10 and that's just a very easy way to
just sign some numbers to feature set so

166
00:13:30,019 --> 00:13:34,569
when we're talking about an hour
whatever side is that what if we try to

167
00:13:34,569 --> 00:13:39,949
sign numerical values to the artifacts
that we see in this you know dynamic

168
00:13:39,949 --> 00:13:45,980
reports so things like number of API
calls registry keys you Texas network

169
00:13:45,980 --> 00:13:51,880
connections Dennis call outs all those
you can count them up and you Adam Jones

170
00:13:51,880 --> 00:13:53,230
who

171
00:13:53,230 --> 00:13:58,860
better basically now of course you run
into some issues that we have to deal

172
00:13:58,860 --> 00:14:03,430
with like well maybe you're going to
count the number of strings and that's

173
00:14:03,430 --> 00:14:06,680
kind of important for a particular our
family they usually have you know ten

174
00:14:06,680 --> 00:14:11,610
thousand dreams or something but they
only make to DNS queries well maybe

175
00:14:11,610 --> 00:14:13,430
there's two DNS queries are just as
important

176
00:14:13,430 --> 00:14:19,340
well obviously you're gonna need to some
sort of normalization in that case you

177
00:14:19,340 --> 00:14:23,250
can also do things like you know the
bite of a buffer copied or network sites

178
00:14:23,250 --> 00:14:28,520
of networked generated and then last
book there is no normalization and

179
00:14:28,520 --> 00:14:36,940
uniformity in comparison so created a
script basically is titled great feature

180
00:14:36,940 --> 00:14:40,420
vector case you know it wasn't so
painfully obvious from the title what it

181
00:14:40,420 --> 00:14:45,610
does it's basically turns Jason reports
from bukavu into into these feature

182
00:14:45,610 --> 00:14:49,660
vectors that we can then stored in
database so the process has been pretty

183
00:14:49,660 --> 00:14:52,840
several not go through all this but
essentially what it does and this is

184
00:14:52,840 --> 00:14:57,120
kind of a first pass like i just want to
see get some numbers down to see you

185
00:14:57,120 --> 00:15:02,650
know what I'm dealing with here goes to
reach the major sections of the cougar

186
00:15:02,650 --> 00:15:05,620
porn so you can see down there the
bottom you networking items and says

187
00:15:05,620 --> 00:15:09,590
board if network is in today's report
count all the things and headed to a

188
00:15:09,590 --> 00:15:16,290
feature called network underscore the
key items and so what that does is it

189
00:15:16,290 --> 00:15:20,910
allows a little flexibility because not
all reports contain all the things so

190
00:15:20,910 --> 00:15:27,160
you are going to get a little bit
different results every time you say

191
00:15:27,160 --> 00:15:32,270
they are static items drop files
behavior summary so I'm actually created

192
00:15:32,270 --> 00:15:35,180
this from the newest version of Google's
excellent diversion where they actually

193
00:15:35,180 --> 00:15:39,650
changed up the report format quite
substantially so this actually at this

194
00:15:39,650 --> 00:15:43,920
point does not work for 1.2 it only
works for the 2.0 death anybody want to

195
00:15:43,920 --> 00:15:46,360
go out and make this backwards
compatible they could I probably will

196
00:15:46,360 --> 00:15:49,250
because we're gonna have to for
operational environment so that is

197
00:15:49,250 --> 00:15:54,760
forthcoming but essentially this is very
very simple just goes in accounts all

198
00:15:54,760 --> 00:15:58,970
the different types of data the Cougars
recording calls that a feature

199
00:15:58,970 --> 00:16:05,069
then puts it in a bit in a dictionary so
that they're always associated with each

200
00:16:05,069 --> 00:16:11,259
other because we just put a ministry to
rate it won't line up and stuff so the

201
00:16:11,259 --> 00:16:16,540
behind the scenes stuff that happens is
once you've selected a substantially

202
00:16:16,540 --> 00:16:20,949
large number of these in fact I i kind
of front-loaded all the samples I think

203
00:16:20,949 --> 00:16:22,550
a lot about 300 samples

204
00:16:22,550 --> 00:16:29,889
different different actual 25 and 40
different families into your gonna wanna

205
00:16:29,889 --> 00:16:32,910
you're gonna want to do that whole
normalization process and so the weather

206
00:16:32,910 --> 00:16:38,160
normalization process works is
essentially the max values for all of

207
00:16:38,160 --> 00:16:41,879
the features you collect your gonna want
to calculate the mean values or you can

208
00:16:41,879 --> 00:16:45,569
just cheating assume the minimum value
is 0 then you're gonna obviously wanted

209
00:16:45,569 --> 00:16:50,829
to bide for each of the beach for each
vector gonna want to buy by the max

210
00:16:50,829 --> 00:16:58,990
value and so you get a number of its
between 0 and and one for each and that

211
00:16:58,990 --> 00:17:03,290
can only be done after you have a
substantially large number of features

212
00:17:03,290 --> 00:17:08,839
calculated because who knows what is
going to be the only have like two and I

213
00:17:08,839 --> 00:17:12,760
wrote functions in this project that
will he time you wanna like recalculate

214
00:17:12,760 --> 00:17:18,589
renormalized you run it a little run for
a few minutes and it'll be normalized

215
00:17:18,589 --> 00:17:23,490
vector's vectors are actually stored in
a table they're stored in a RAW format

216
00:17:23,490 --> 00:17:30,780
are not stored normalize for that exact
reason and it but it does store the does

217
00:17:30,780 --> 00:17:34,870
store the max values so that you can
quickly users quickly reference those as

218
00:17:34,870 --> 00:17:40,770
your calculating the normalized vector's
vector's tables kind of like this

219
00:17:40,770 --> 00:17:44,659
Associated the hash the machine is very
important to one thing that I didn't

220
00:17:44,659 --> 00:17:48,929
mention one thing I did not mention when
I talk about the case and box set up the

221
00:17:48,929 --> 00:17:52,429
biggest thing in the AM acknowledges
uniformity so you don't really want to

222
00:17:52,429 --> 00:17:57,320
be comparing at 32 bit Windows XP
machine to a 64 bit Windows 7 machine

223
00:17:57,320 --> 00:18:03,940
it's very important that those are
treated as a separate groupings task IDs

224
00:18:03,940 --> 00:18:06,010
so you can get back to the computer
bored

225
00:18:06,010 --> 00:18:14,330
the type of vector or type of victory is
the value so better type would be like

226
00:18:14,330 --> 00:18:15,800
network connection

227
00:18:15,800 --> 00:18:23,680
value would be like eight alright so the
bus so this presentation I'm gonna go

228
00:18:23,680 --> 00:18:24,970
over with you know what

229
00:18:24,970 --> 00:18:31,580
presented and in some of the results in
kind of walk through it so the main part

230
00:18:31,580 --> 00:18:36,050
of Project is a mess beat up high school
of the core resource the product to find

231
00:18:36,050 --> 00:18:40,130
you know that the vector classes and
lets you interact with the database is

232
00:18:40,130 --> 00:18:45,050
going to do any piano roll see anything
like that unless you want to find you

233
00:18:45,050 --> 00:18:48,060
know your basic functions are going to
be late the vectors for some statistical

234
00:18:48,060 --> 00:18:54,850
analysis and display pots so I created a
examples direct examples directory

235
00:18:54,850 --> 00:18:58,810
within the project that basically has
right now for different scripts that

236
00:18:58,810 --> 00:19:00,970
form

237
00:19:00,970 --> 00:19:04,650
give you some plots and we're going
through those to kind of walk through a

238
00:19:04,650 --> 00:19:14,180
lot of family stuff on the whole purpose
behind this group is to group the

239
00:19:14,180 --> 00:19:18,550
vectors based on those tax family
biotype source and the normalized them

240
00:19:18,550 --> 00:19:24,090
and then create something called so the
first part of this is is is very

241
00:19:24,090 --> 00:19:27,830
important and it's it's where you take
those samples are so the stacks that we

242
00:19:27,830 --> 00:19:32,470
are being put into the tax table in the
new kind of break those up into unique

243
00:19:32,470 --> 00:19:35,680
what I call labels and essentially what
those are as you know the family that

244
00:19:35,680 --> 00:19:39,320
you said that longs to the file type it
is the story that you got through text

245
00:19:39,320 --> 00:19:44,040
as the source data as well but it put it
up there and it pulls out it takes all

246
00:19:44,040 --> 00:19:46,990
the features that are associated with
each of those and then it normalizes

247
00:19:46,990 --> 00:19:51,940
them as part of that group and the
second thing it does is it creates an

248
00:19:51,940 --> 00:19:55,190
archetype and essentially what it an
archetype is very key to this entire

249
00:19:55,190 --> 00:19:58,559
project

250
00:19:58,559 --> 00:20:04,789
so what is an architect I found is that
you know directly comparing all features

251
00:20:04,789 --> 00:20:07,740
for every picture showed a very low
correlations wasn't very good results

252
00:20:07,740 --> 00:20:11,529
honestly you're talking about you have a
lot of different factors that do very

253
00:20:11,529 --> 00:20:15,649
very different things so and to think
about this conceptually it's pretty easy

254
00:20:15,649 --> 00:20:19,629
to think about you know particularly our
samples you know they have kind of their

255
00:20:19,629 --> 00:20:24,980
own calling card so particular sample my
create new texts of a particular nature

256
00:20:24,980 --> 00:20:30,759
and that's very important to define that
sample another sample my trade a whole

257
00:20:30,759 --> 00:20:34,809
bunch of domains as part of the domain
generating algorithm that uses and

258
00:20:34,809 --> 00:20:41,720
that's a very important facet of that in
our family and so comparing those things

259
00:20:41,720 --> 00:20:43,429
directly to each other

260
00:20:43,429 --> 00:20:47,100
don't really make a whole lot of sense
because one sample might not even know a

261
00:20:47,100 --> 00:20:51,600
great domain names in the notion of my
so what the archetype

262
00:20:51,600 --> 00:20:56,009
this was actually compare only the
things that matter for sample for family

263
00:20:56,009 --> 00:21:00,769
so I created a function that that tries
to figure out what is most statistically

264
00:21:00,769 --> 00:21:04,649
significant for a particular group or
family tries to figure out which of

265
00:21:04,649 --> 00:21:11,169
those tags which of those features
really makes the biggest difference and

266
00:21:11,169 --> 00:21:14,720
so the way that the archetypes are
created is essentially you give it your

267
00:21:14,720 --> 00:21:18,610
subset you're interested in which is
basically all the samples that match

268
00:21:18,610 --> 00:21:21,629
your particular label that we just
talked about and then you also give it

269
00:21:21,629 --> 00:21:25,889
the superset which is all the samples in
your universe I guess and then what you

270
00:21:25,889 --> 00:21:33,990
do is you say which features have the
lowest amount of variance in the

271
00:21:33,990 --> 00:21:38,890
lowest amount of variance want to still
lived in the subset but the greatest

272
00:21:38,890 --> 00:21:45,230
amount of variance in the superset so
it's a simple as simple as taking a

273
00:21:45,230 --> 00:21:47,960
variant of each of the vectors and
subtracting them and figuring out

274
00:21:47,960 --> 00:21:52,460
essentially you know what are your top
30 samples are sorry what are your top

275
00:21:52,460 --> 00:21:58,710
30 features that really defined you know
that sample and so just kind of walking

276
00:21:58,710 --> 00:22:01,470
through the code a little bit first
thing we do is we have a prune the

277
00:22:01,470 --> 00:22:04,550
better because if you care about you
really don't care about a lot of zero

278
00:22:04,550 --> 00:22:07,630
values for the most part I mean there
might be some weird cases where where

279
00:22:07,630 --> 00:22:11,170
the fact that there are 0 new network
connection for something really define a

280
00:22:11,170 --> 00:22:14,360
sample but for the most part you don't
care about 20 there's a lot of zeros

281
00:22:14,360 --> 00:22:21,280
that's what I've features total from the
drina samples that I looked at and I

282
00:22:21,280 --> 00:22:26,090
think most samples only actually had
about a hundred non-zero values so first

283
00:22:26,090 --> 00:22:27,750
thing you do is for the subset

284
00:22:27,750 --> 00:22:31,910
get rid of all the zeros gonna find the
standard deviations for both sets like I

285
00:22:31,910 --> 00:22:37,860
just talked about using your gonna do
that your bases and then you're gonna

286
00:22:37,860 --> 00:22:40,410
find the difference between the subset
in the superset

287
00:22:40,410 --> 00:22:45,760
deviations and then you want to take
basically the top 30

288
00:22:45,760 --> 00:22:54,910
the lowest it's gonna be as good a large
negative number so all that and get down

289
00:22:54,910 --> 00:22:59,480
to the bottom basically is going to
return a very proven set of features so

290
00:22:59,480 --> 00:23:03,410
instead of having 255 features were a
lot of them are like 0 you're going to

291
00:23:03,410 --> 00:23:07,980
get like 30 features and they're all
going to be a distinctly significant for

292
00:23:07,980 --> 00:23:14,960
the subset you provided as compared to
December set so then the next to the

293
00:23:14,960 --> 00:23:19,340
idea behind using archetype is then you
can compare unknown samples to that

294
00:23:19,340 --> 00:23:23,610
archetype to see hey does this fit the
curves is you know how how close are

295
00:23:23,610 --> 00:23:28,270
these doctors what is the distance
between these doctors at this point but

296
00:23:28,270 --> 00:23:31,160
you're going to want to actually
calculate

297
00:23:31,160 --> 00:23:36,010
the same feature sets from that
archetype for each for each archetype of

298
00:23:36,010 --> 00:23:42,790
you want to compare samples to make
sense of the words they're so and then

299
00:23:42,790 --> 00:23:45,240
when you can play these archetypes you
wanna be able to quickly retrieve them

300
00:23:45,240 --> 00:23:49,800
you don't want to have to do this every
single time so so its doors in the table

301
00:23:49,800 --> 00:23:53,630
and just like the max values that you
used to normalize their songs in there

302
00:23:53,630 --> 00:24:00,290
that you can pre-order type redo also
it's nice because you can actually going

303
00:24:00,290 --> 00:24:03,700
here in manually edit some of the
features and once I start selling some

304
00:24:03,700 --> 00:24:08,950
plots will make sense when you're going
to want to do that so then

305
00:24:08,950 --> 00:24:15,490
families is it kind of gives you some
stats on each of the labels labels look

306
00:24:15,490 --> 00:24:22,740
like something like this he got plug
attacks p32 from SecureWorks on you know

307
00:24:22,740 --> 00:24:28,960
2015 08053 windows 7 x64 and so that's
kind of what is it that's kinda like the

308
00:24:28,960 --> 00:24:32,620
unique identifier for this family and
then it tells you some stats about that

309
00:24:32,620 --> 00:24:37,700
family that he created as you know each
sample is the meeting distances point 26

310
00:24:37,700 --> 00:24:42,250
the units on that don't matter because
completely normal eyes so its point 26

311
00:24:42,250 --> 00:24:49,010
somethings their deviations 2.07 and
then it kind of compares that to the

312
00:24:49,010 --> 00:24:53,100
rest of the samples in your universe
there so you talking about having you

313
00:24:53,100 --> 00:24:56,700
know I mean just all samples as point
six that's roughly three times longer

314
00:24:56,700 --> 00:24:59,480
and the standard deviation is much
higher

315
00:24:59,480 --> 00:25:02,290
it is so the next thing it does is kinda
give you a little bit of sanity check it

316
00:25:02,290 --> 00:25:05,790
tells you that there are 48 you know
vectors that belongs that family they

317
00:25:05,790 --> 00:25:11,900
can basically be defined by that label
and the number of all sectors within one

318
00:25:11,900 --> 00:25:15,400
standard deviations 40 the number of all
vectors between one standard deviation

319
00:25:15,400 --> 00:25:19,950
to see you a shitty one and within three
standard deviations is a hundred

320
00:25:19,950 --> 00:25:23,180
thirteen so what you can see you there
somewhere between one and two standard

321
00:25:23,180 --> 00:25:27,220
deviations way you actually get all
those doctors and you can drill down to

322
00:25:27,220 --> 00:25:29,810
make sure those are actually catching
the ones you think they're getting in

323
00:25:29,810 --> 00:25:33,030
and and they are for those partners
couple outliers and we're going to

324
00:25:33,030 --> 00:25:39,480
that last thing it does actually plus
the entire the entire family and

325
00:25:39,480 --> 00:25:44,350
slightly you guys ever used properly
before but it is interesting I basically

326
00:25:44,350 --> 00:25:48,160
shows us for this project as I wanted to
use it I know that it's probably work

327
00:25:48,160 --> 00:25:50,980
for a lot of large organizations that
have restrictions on white putting their

328
00:25:50,980 --> 00:25:57,530
data in the cloud but essentially
apparently described as I hope for plots

329
00:25:57,530 --> 00:26:03,330
so it's pretty cool its allows you to
get very interactive but essentially all

330
00:26:03,330 --> 00:26:09,380
your data belongs to them so I it's
totally not something that is going to

331
00:26:09,380 --> 00:26:14,090
be feasible for a lot of corporations
but you can you can easily adjust this

332
00:26:14,090 --> 00:26:21,550
so this is like what when the family
buses gonna look like here so you got

333
00:26:21,550 --> 00:26:25,990
all your samples over on the on the key
and then on the x-axis there you have

334
00:26:25,990 --> 00:26:39,309
those features thats were defined as
being stood at least on the bottom

335
00:26:39,309 --> 00:26:56,139
so you can see that the the main line
there which is basically the orkut and

336
00:26:56,139 --> 00:27:00,519
he's probably read Saturday's in the
front row there but it has the different

337
00:27:00,519 --> 00:27:07,330
values that make that archetype up there
on the x-axis and you can see where each

338
00:27:07,330 --> 00:27:12,999
sample sample goes so you can see over
on the far left-hand side of the graph

339
00:27:12,999 --> 00:27:19,070
there that there are some some samples
for ATI stats inti duplicate objects

340
00:27:19,070 --> 00:27:24,090
that are so even though one standard
deviation so what that tells me is that

341
00:27:24,090 --> 00:27:26,809
something that I might want immediately
treat this feature vector for maybe I

342
00:27:26,809 --> 00:27:29,179
don't want that guy in there because
it's just too far out of the average

343
00:27:29,179 --> 00:27:38,600
something around here is another example
with even more this is a family group

344
00:27:38,600 --> 00:27:43,240
here that I got from security blog post
Monday hashes they will pull down and

345
00:27:43,240 --> 00:27:46,309
you can see there are pretty tightly
grouped but I really like to see those

346
00:27:46,309 --> 00:27:50,970
outliers I really like to see it like
there are two random samples and if you

347
00:27:50,970 --> 00:27:55,279
go and you kind of fall them through in
the back to unify zoom in and the bottom

348
00:27:55,279 --> 00:28:03,509
there you can see that there's that
sample for ninety this kind of just kind

349
00:28:03,509 --> 00:28:06,480
of outlying so what does that mean that
means there's a really good chance or

350
00:28:06,480 --> 00:28:10,830
something up with that do that do doing
something really different obviously the

351
00:28:10,830 --> 00:28:13,730
graphs I can't tell you why but it's
gonna tell you that of the hundred

352
00:28:13,730 --> 00:28:17,889
samples and i dont down maybe that one
is doing something you can go look at

353
00:28:17,889 --> 00:28:27,509
home here on the shape of these plots so
I ordered these again the the actual

354
00:28:27,509 --> 00:28:30,580
numbers themselves aren't really all
that important in the order

355
00:28:30,580 --> 00:28:33,710
themselves aren't that important these
are really more these could be more like

356
00:28:33,710 --> 00:28:37,029
bar graphs but bar graphs are really
really difficult to put like a hundred

357
00:28:37,029 --> 00:28:41,809
home on this created times that's why
they're more like a plot and I order

358
00:28:41,809 --> 00:28:46,679
from least to greatest so those on the
on the far left they're actually really

359
00:28:46,679 --> 00:28:51,820
important and they're just as important
as the ones on the far right even though

360
00:28:51,820 --> 00:28:56,759
they are physically smaller cool thing
about me is you can let you go home and

361
00:28:56,759 --> 00:29:00,450
you can see them but that's just kind of
a comment on on the shape of the graphs

362
00:29:00,450 --> 00:29:08,860
here's another one where the family is
pretty tightly grouped together so this

363
00:29:08,860 --> 00:29:16,340
is an example of a of a family group and
i think is pretty pretty solid hear

364
00:29:16,340 --> 00:29:25,179
those samples that I recently pulled
down and ran through the system so

365
00:29:25,179 --> 00:29:27,500
example script we have

366
00:29:27,500 --> 00:29:31,789
archetype stuff so this is actually very
similar to the previous one but it is

367
00:29:31,789 --> 00:29:35,769
you less noise essentially so it uses
the databases that are creating the

368
00:29:35,769 --> 00:29:40,340
archetype so the first one takes couple
minutes to run but the sample size I had

369
00:29:40,340 --> 00:29:45,929
I think about 1,300 at Alice's at 300
samples about 1,300 analyses it takes

370
00:29:45,929 --> 00:29:50,269
maybe I don't tell four minutes to run
the script this grip over I'm pretty

371
00:29:50,269 --> 00:29:54,590
much instantly because it pulls the
archetype straight out of database and I

372
00:29:54,590 --> 00:29:57,590
just bought the only place that means
their deviation so this just really

373
00:29:57,590 --> 00:30:05,019
gives you a good deal of the overall
distribution of Tiger standard

374
00:30:05,019 --> 00:30:14,020
deviations

375
00:30:14,020 --> 00:30:16,980
so this is kind of this what this gives
you it just gives you a good overall

376
00:30:16,980 --> 00:30:20,720
feel you can see it as was pretty tight
you can see that there's a couple couple

377
00:30:20,720 --> 00:30:26,240
pictures here you know the behavior API
said resource resource API calls kind of

378
00:30:26,240 --> 00:30:29,250
all over the place something that maybe
you and one immediately removes this is

379
00:30:29,250 --> 00:30:32,900
again it's another good visual way of
saying hey these bitches are really

380
00:30:32,900 --> 00:30:37,170
tight they're really the standard
deviations are really close to the mean

381
00:30:37,170 --> 00:30:41,980
we really want to use those and the ones
that aren't you can say hey what if we

382
00:30:41,980 --> 00:30:47,700
just meet in the next time that we
define this function here is the one

383
00:30:47,700 --> 00:30:54,840
that's not fair and honest and again
this could be good as the dad you put

384
00:30:54,840 --> 00:30:59,580
into the right so what I found is that a
lot of the automated like threats dreams

385
00:30:59,580 --> 00:31:04,300
sometimes they don't tell the truth when
they just give you a hash dumping right

386
00:31:04,300 --> 00:31:09,450
is like 200 hashes and they say all
these belong to projects are always

387
00:31:09,450 --> 00:31:14,450
going to go to store all these belong to
do it right exercise thing and turns out

388
00:31:14,450 --> 00:31:18,800
that they i mean like not a hundred
percent there and so if you're just

389
00:31:18,800 --> 00:31:23,900
dumping and stuff from particular
threats feeds you might find they have a

390
00:31:23,900 --> 00:31:28,860
much wider variance and their beecher
set standard deviations and that's a

391
00:31:28,860 --> 00:31:45,560
very good way of your kind of getting
here in 20 sources

392
00:31:45,560 --> 00:32:00,660
absolutely yes so that's why that's why
I always include the date of the report

393
00:32:00,660 --> 00:32:06,350
that guy got it from again like I said I
got these pretty much all from from open

394
00:32:06,350 --> 00:32:11,210
source reporting was my pretty much my
only source for you has jazz and whatnot

395
00:32:11,210 --> 00:32:16,790
and and I always made sure that the date
was something very very important to

396
00:32:16,790 --> 00:32:20,820
distinguish it is separate but
absolutely mean you're gonna get

397
00:32:20,820 --> 00:32:25,780
completely different results from the
wall from this year than you got two

398
00:32:25,780 --> 00:32:29,290
years ago

399
00:32:29,290 --> 00:32:36,210
alright so best guest up high so this is
this is the script that kind of does the

400
00:32:36,210 --> 00:32:39,550
hey I have a known sample I want to
figure out which architect is most

401
00:32:39,550 --> 00:32:42,870
closely fit to obviously doesn't fit any
of them and still gonna give you an

402
00:32:42,870 --> 00:32:48,920
answer but gonna give you the answer
this you know where that works great so

403
00:32:48,920 --> 00:32:53,350
compared to test back to each of the
story archetypes it normalizes improves

404
00:32:53,350 --> 00:32:56,180
the sample to fit the target I but they
are kind of explained that process

405
00:32:56,180 --> 00:33:00,520
earlier but the whole idea is to take
the sample you take the analysis you get

406
00:33:00,520 --> 00:33:05,820
from you know the sandbox and you only
look at the features that are relevant

407
00:33:05,820 --> 00:33:09,820
to that archetype when you're doing your
comparisons that defines Euclidean

408
00:33:09,820 --> 00:33:18,310
distance from not warm front aspect to
the archetype to end it also and

409
00:33:18,310 --> 00:33:25,690
distance between the archetypes vectors
to the archetypes mean to find standard

410
00:33:25,690 --> 00:33:29,810
deviation it figures out how many
standard deviations away you're seeing

411
00:33:29,810 --> 00:33:35,340
is based on the average standard
deviation that the internal did the

412
00:33:35,340 --> 00:33:41,850
internal families doctors were from the
internal mean that didn't make any sense

413
00:33:41,850 --> 00:33:45,120
why the talk later it's kinda hard to
explain but

414
00:33:45,120 --> 00:33:48,740
essentially what you get is you get a
standard you get a how many standard

415
00:33:48,740 --> 00:33:55,650
deviations is your feature set away from
the archetypes feature set and based on

416
00:33:55,650 --> 00:33:59,930
that rack and stack them and say how
close they are in this through my

417
00:33:59,930 --> 00:34:01,700
samples again

418
00:34:01,700 --> 00:34:07,030
relatively low number of samples bout
308 showed that number one guest was 75

419
00:34:07,030 --> 00:34:10,840
percent correct basically had no
knowledge of who it was or what it was

420
00:34:10,840 --> 00:34:14,560
and then after that I was able to go
back and say yeah yeah you told me

421
00:34:14,560 --> 00:34:20,860
correctly 75 percent of time the first
top our top three it that number jumped

422
00:34:20,860 --> 00:34:26,520
in ninety-eight percent so we take
something I think we can absolutely get

423
00:34:26,520 --> 00:34:29,760
better numbers than this I think the way
that you get better numbers in this

424
00:34:29,760 --> 00:34:33,580
process is you prune the feature sets
even more you go back and look at those

425
00:34:33,580 --> 00:34:36,520
rash said hey this is as too much
thinner deviations let's remove that

426
00:34:36,520 --> 00:34:40,350
line or hey you know i happened to me I
happen to know that was one in here that

427
00:34:40,350 --> 00:34:44,700
needs to be the snow and then you kind
of shape those little better in other

428
00:34:44,700 --> 00:34:48,830
ways you populate it with you more data
you give it more samples you give it

429
00:34:48,830 --> 00:34:55,710
more you give the more you know and tell
you make sure the samples that you

430
00:34:55,710 --> 00:35:01,920
actually say what you say samples no
particular thing it it it really is but

431
00:35:01,920 --> 00:35:07,670
your top 3 98 percent of things are good
here that looks like it's gonna tell you

432
00:35:07,670 --> 00:35:13,030
first thing is going to take the
distance from your your your vector to

433
00:35:13,030 --> 00:35:16,260
the archetype how many standard
deviation is this going to rack and

434
00:35:16,260 --> 00:35:19,060
stack them and and and pretty much tell
you your best guess so there at the

435
00:35:19,060 --> 00:35:27,240
bottom it says this goes for this guy's
whatever 32 C so

436
00:35:27,240 --> 00:35:33,170
what happens if the sake verify how can
you have been verified what you truly

437
00:35:33,170 --> 00:35:37,420
don't know obviously I cheated because I
you samples from itself so I knew what

438
00:35:37,420 --> 00:35:43,869
they were headed time will listen to it
now but I know so what would you have

439
00:35:43,869 --> 00:35:47,630
done that well you can look at it
graphically so compared to our sights

440
00:35:47,630 --> 00:35:56,680
high as the final example here in
essentially what it does is it improves

441
00:35:56,680 --> 00:36:01,350
the sample in the same in the same
regard as as the previous trip but then

442
00:36:01,350 --> 00:36:06,800
it plots against all the archives and
what it does is a very visually easy way

443
00:36:06,800 --> 00:36:12,200
you can even say hey you know this
really doesn't occur at all and its

444
00:36:12,200 --> 00:36:18,830
graphical way to verify this guess so
here we go here is a here's a quiz so

445
00:36:18,830 --> 00:36:23,210
what these what these samples are you
hear what these parts are right here is

446
00:36:23,210 --> 00:36:27,610
you have a sample in our tests and read
the blue is the archetype that's being

447
00:36:27,610 --> 00:36:31,300
compared to the yellow and green are the
standard deviations away from the blue

448
00:36:31,300 --> 00:36:42,869
myself I'm just looking at my polygraphs
which one does the red most of all I

449
00:36:42,869 --> 00:36:46,650
mean you can see this just came across
those you can see that that's one that

450
00:36:46,650 --> 00:36:54,530
if it's pretty much almost exactly as
you can see if all of her right there

451
00:36:54,530 --> 00:37:07,220
between the two upper and lower limits
so here's an example of where it was

452
00:37:07,220 --> 00:37:11,820
wrong so best guest told me that this
should have been the number one guy so

453
00:37:11,820 --> 00:37:20,580
it's not clear this not you can see red
dots that are clearly outside of the are

454
00:37:20,580 --> 00:37:25,140
clearly outside of the the standard
deviations all across the board so the

455
00:37:25,140 --> 00:37:30,830
second guess was actually this did and
so if you choose between the first guess

456
00:37:30,830 --> 00:37:38,060
the second guess the second is right on
so again it's a pretty it's a very nice

457
00:37:38,060 --> 00:37:40,680
way to graphically verify your best
guess

458
00:37:40,680 --> 00:37:46,040
was correct or if your best guess gives
you like you know five basically I've

459
00:37:46,040 --> 00:37:50,579
determine if you go look at the grass
you can pretty much tell that that the

460
00:37:50,579 --> 00:37:53,119
first guess was completely wrong for
whatever reason that the second guess

461
00:37:53,119 --> 00:38:01,109
was right so that's what I have right
now I do for long products I wanna me

462
00:38:01,109 --> 00:38:04,730
nobody ever does right so what are the
next steps that I currently have to this

463
00:38:04,730 --> 00:38:09,250
guy I went to operationalize this is a
couple things that I didn't do that I

464
00:38:09,250 --> 00:38:12,819
would like for 12 creating cooper
reporting model so this is like

465
00:38:12,819 --> 00:38:16,730
automatically generated every time you
run it and that's that's like literally

466
00:38:16,730 --> 00:38:20,829
next on the list I didn't actually get
that finished I have a stand-alone

467
00:38:20,829 --> 00:38:24,000
Python scripts that will basically
interact with the API right now so if

468
00:38:24,000 --> 00:38:27,700
you have a Google service running the
API you run the script will just get

469
00:38:27,700 --> 00:38:31,790
into these databases so you can run the
rest of the scripts but it would be

470
00:38:31,790 --> 00:38:37,440
really really just kind of built into
Google from the get-go and additional

471
00:38:37,440 --> 00:38:42,109
features right now it was very very
simple stuff writes all we did was count

472
00:38:42,109 --> 00:38:47,190
as count the number of essentially tags
that Google created for these samples I

473
00:38:47,190 --> 00:38:50,200
would like to do some more sophisticated
stuff like look at the number of bytes

474
00:38:50,200 --> 00:38:54,369
their pastor buffers look at the number
of bytes the past 30 caps look at some

475
00:38:54,369 --> 00:39:01,559
of the more things they're more
intrinsic than just counting to create

476
00:39:01,559 --> 00:39:05,290
some sort of like front end so that you
could easily get your archetypes without

477
00:39:05,290 --> 00:39:09,740
having to be a sequel person to go in
and manually edit them sequel and

478
00:39:09,740 --> 00:39:15,099
cluster grass so the one that he didn't
see didn't see any cost of gasoline see

479
00:39:15,099 --> 00:39:19,549
where you could take a whole group of
samples and put them on a single plot

480
00:39:19,549 --> 00:39:22,980
and see how close they were relationally
the challenge there is that when you

481
00:39:22,980 --> 00:39:26,299
have it in dimensional vector in you
want to block them on a two-dimensional

482
00:39:26,299 --> 00:39:30,690
space is gonna lose your gonna lose data
and there's all kinds of theories out

483
00:39:30,690 --> 00:39:32,650
there like you know what the best way to

484
00:39:32,650 --> 00:39:37,670
to do that is but some point you're
gonna like you're gonna lose data and

485
00:39:37,670 --> 00:39:40,839
the way you lose data is kind of
important so I'm in the process of China

486
00:39:40,839 --> 00:39:45,410
event out a couple different solutions
to put these on to put you know all

487
00:39:45,410 --> 00:39:50,210
these on a cluster graph correctly he
has any suggestions I love to hear your

488
00:39:50,210 --> 00:40:21,349
thoughts on that but that's about it so

489
00:40:21,349 --> 00:40:42,049
petition you can't explain these things
to me know see there's probably a much

490
00:40:42,049 --> 00:40:43,660
more intelligent way to do that

491
00:40:43,660 --> 00:40:56,640
of the presentation coming up tell me
how stupid I am NOT using bounded

492
00:40:56,640 --> 00:41:03,859
distributions and let me know you think
the product there's the obvious but

493
00:41:03,859 --> 00:41:04,229
thanks guys

