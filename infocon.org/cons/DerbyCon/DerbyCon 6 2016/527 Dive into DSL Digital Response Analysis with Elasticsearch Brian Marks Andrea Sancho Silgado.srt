1
00:00:00,000 --> 00:00:05,850
alright thanks guys for joining us for a
dive into dsl did response analysis with

2
00:00:05,850 --> 00:00:10,649
elasticsearch you're either in this room
because you are a elasticsearch fan or

3
00:00:10,650 --> 00:00:17,490
they're probably saving a seat for the
talk on how money is paid for four

4
00:00:17,490 --> 00:00:23,310
tickets for their be gone that's coming
up start that so before we get started I

5
00:00:23,310 --> 00:00:29,038
just want to honor friend and co-worker
of ours can Johnson 10 passed away that

6
00:00:29,039 --> 00:00:33,870
this past year in a tragic car accident
and he was honored at the sands dfi our

7
00:00:33,870 --> 00:00:39,629
summit and starting next year there will
be a scholarship in his honor for some

8
00:00:39,629 --> 00:00:46,589
new young and up-and-coming dfi our
candidate and they will get to go to the

9
00:00:46,590 --> 00:00:51,420
dfi our summit for free as well as
attend the class i don't have a URL for

10
00:00:51,420 --> 00:00:56,489
that but just keep watching sands and be
more information on that protects here

11
00:00:56,489 --> 00:01:01,800
so my name is Ryan marks and this is
andrea Sancho we're both senior

12
00:01:01,800 --> 00:01:05,309
associates with kpmg we do forensic
consulting for them

13
00:01:06,420 --> 00:01:12,210
go ahead and start off there's so much
we're going to make some assumptions

14
00:01:12,210 --> 00:01:15,658
because we only have 20 minutes so we're
going to see you already half

15
00:01:15,659 --> 00:01:19,770
elasticsearch deployed and you already
have all artifacts parts like

16
00:01:19,770 --> 00:01:25,199
prefetching follow some of t all that
into field value purse and there's a lot

17
00:01:25,200 --> 00:01:29,100
of tools and talks videos implementation
for that so we're going to go anywhere

18
00:01:29,100 --> 00:01:34,380
not gonna not covered that about what we
need is the last zsl library for python

19
00:01:34,380 --> 00:01:37,500
here's the command just to grab it

20
00:01:37,500 --> 00:01:42,960
go ahead so little bit of background on
the elasticsearch dsl what makes what

21
00:01:42,960 --> 00:01:48,089
makes it different from the elastic
search library is well first of all of

22
00:01:48,090 --> 00:01:52,380
the the functionality in this we're
going to be using Python but the same

23
00:01:52,380 --> 00:01:59,490
functionality is available in Ruby
java.net and PHP dsl has a bunch of

24
00:01:59,490 --> 00:02:03,329
different objects were primarily going
to be using query aggregation and search

25
00:02:03,329 --> 00:02:06,929
and the coolest thing about the
elasticsearch dsl is that you can

26
00:02:06,930 --> 00:02:12,959
combine query objects with boolean
operators like these up here these dsl

27
00:02:12,959 --> 00:02:13,680
objects can

28
00:02:13,680 --> 00:02:17,760
also be deserialized to a Python
dictionary and if there's any non coders

29
00:02:17,760 --> 00:02:22,439
out there there that confused on
deserializer whatever we'll cover that

30
00:02:22,439 --> 00:02:28,170
is just like printing out the string
query support any elastic search query

31
00:02:28,170 --> 00:02:34,559
query type we're going to be using a
query string and if you use Qabbani and

32
00:02:34,560 --> 00:02:41,549
then use query strings so another thing
to mention about the last search dsl and

33
00:02:41,549 --> 00:02:44,819
I went to elastic on and i was able to
meet santa crawl

34
00:02:44,819 --> 00:02:51,000
he's the developer of the elastic search
dsl he's also the developer of the last

35
00:02:51,000 --> 00:02:56,519
search python library is also a core
django developer so if you do use

36
00:02:56,519 --> 00:03:00,569
elasticsearch TSL don't be surprised if
it plugs in beautifully with your web

37
00:03:00,569 --> 00:03:08,189
apps and Python so we're going to make
an example query with the elastic search

38
00:03:08,189 --> 00:03:13,530
dsl so you would import the big Q object
for query from the last search dsl and

39
00:03:13,530 --> 00:03:19,139
let's say we want to build a query to to
to find evidence that Apollo executed

40
00:03:19,139 --> 00:03:23,790
great artifact is pretty much so we do
is build our query artifact type pre

41
00:03:23,790 --> 00:03:28,048
fetch and then now we can deserialize it
to a dictionary

42
00:03:28,049 --> 00:03:32,729
this is what that output looks like and
it's pretty straightforward just a

43
00:03:32,729 --> 00:03:38,760
couple of curly brackets json for that
query so let's get a bit more complex

44
00:03:38,760 --> 00:03:45,239
with this and take that same query but
let's say we also want to look for am

45
00:03:45,239 --> 00:03:49,019
catch artifacts let's say this is
windows 8 and above machine maybe

46
00:03:49,019 --> 00:03:52,620
prevention and enabled but we do have
the am cash we want to look in both

47
00:03:52,620 --> 00:03:58,199
places so we just build an additional
query and then we can combine these two

48
00:03:58,199 --> 00:04:04,379
with the or operator so this query would
be artifact type prefetch or artifact

49
00:04:04,379 --> 00:04:12,929
type am cash again we deserialize it to
a dictionary and this is the output that

50
00:04:12,930 --> 00:04:21,509
we get this is a elasticsearch rest api
body file that you would send whenever

51
00:04:21,509 --> 00:04:24,270
you're searching with Keanu this is
actually what gets sent in the

52
00:04:24,270 --> 00:04:30,150
background and this if you made this
type of query that this is a the output

53
00:04:30,150 --> 00:04:35,400
that you need for that body file i have
seen in code and presentations people

54
00:04:35,400 --> 00:04:40,169
doing it this way on the right with
string substitutions that they get they

55
00:04:40,169 --> 00:04:45,029
had they have a essentially and or
boolean operator query and then they

56
00:04:45,029 --> 00:04:50,969
just using triple string substitutions
there is an easier way that

57
00:04:50,969 --> 00:04:55,680
elasticsearch dsl this is why it was
created you can see this starts getting

58
00:04:55,680 --> 00:05:00,419
bit more complicated with square
brackets curly brackets if you miss one

59
00:05:00,419 --> 00:05:05,490
of those if you miss one comma it will
not work so this definitely makes your

60
00:05:05,490 --> 00:05:06,539
life easier

61
00:05:06,539 --> 00:05:10,710
so let's make this even more complex
we're going to go into our third version

62
00:05:10,710 --> 00:05:17,159
of this query let's say with the am cash
and cash also has all the DLLs that were

63
00:05:17,159 --> 00:05:20,820
loaded with an executable let's say we
just want filter those out just look at

64
00:05:20,820 --> 00:05:27,029
the executables you can use this tilde
is currently tilde i'm from the queue as

65
00:05:27,029 --> 00:05:34,320
a boolean not so we build this up this
last query this would be artifact type

66
00:05:34,320 --> 00:05:41,610
prefetch or artifact type and cash and
not path ending in dll on that query

67
00:05:41,610 --> 00:05:46,260
rewrite that we deserialize it to a
dictionary you guys probably can't see

68
00:05:46,260 --> 00:05:51,060
this really small pond barely fit onto a
presentation but that's my point

69
00:05:51,719 --> 00:05:57,150
so whenever I you first made one of
these complex queries I was just adding

70
00:05:57,150 --> 00:06:01,979
a couple together but then start going
within and and making a lot more complex

71
00:06:01,979 --> 00:06:09,570
ones it kind of struck me this looks a
bit like a yard signature if anyone's

72
00:06:09,570 --> 00:06:16,080
done any binary static analysis Yara is
a language that you can use just basic

73
00:06:16,080 --> 00:06:19,270
logic queries to

74
00:06:19,270 --> 00:06:26,109
to do static analysis on binaries so we
were thinking in the lab that we needed

75
00:06:26,110 --> 00:06:32,350
some way like this to query friends
artifacts and so with this we came up

76
00:06:32,350 --> 00:06:38,080
with a signature forensics using
elasticsearch dsl with this query logic

77
00:06:38,080 --> 00:06:43,270
so if you things like Yara and I of C's
for those use cases

78
00:06:43,930 --> 00:06:50,170
what do we have for our use case of
signature forensics well we have all the

79
00:06:50,170 --> 00:06:54,610
features of apache Lucene sentai says
built-in with the query string that's

80
00:06:54,610 --> 00:07:00,520
boolean operators in the search string
as well as wildcard queries on that you

81
00:07:00,520 --> 00:07:06,340
guys know wildcards phrase queries is
just a strengthening of of words but the

82
00:07:06,340 --> 00:07:10,960
cool thing is you can do proximity
queries on this phrase queries to find

83
00:07:10,960 --> 00:07:17,260
matches with in order to you can also
use fuzzy queries to find iterations up

84
00:07:17,260 --> 00:07:26,500
svchost svchost with a0 this uses on the
levenshtein distance to find the

85
00:07:26,500 --> 00:07:31,300
difference in and letters in the word
range queries there's a query to find

86
00:07:31,300 --> 00:07:37,480
all all user activity and windows event
logs on one little search regular

87
00:07:37,480 --> 00:07:42,430
expression where we be without regular
expression will be hard coding file pass

88
00:07:42,430 --> 00:07:48,070
so that this will match on both 32-bit
and 64-bit Program Files directory just

89
00:07:48,070 --> 00:07:53,110
has one use case as well as what we
covered in the previous slides on the

90
00:07:53,110 --> 00:07:59,200
query objects so and in since we have
this in that query string potential you

91
00:07:59,200 --> 00:08:04,060
could reverse that back to a coupon a
search string if you want to look at in

92
00:08:04,060 --> 00:08:08,560
there make some visualizations you get
it right back in that format so now that

93
00:08:08,560 --> 00:08:12,010
we have those queries and reigns going
to show some really cool stuff we can do

94
00:08:12,010 --> 00:08:18,310
those queries and aggregations so let's
talk about aggregations what our

95
00:08:18,310 --> 00:08:23,530
obligations aggregations our analytics
that are filled out search time are you

96
00:08:23,530 --> 00:08:26,890
don't get the data the whole document
indexing the last research back from it

97
00:08:26,890 --> 00:08:29,979
you just get the accounts allegations
build on them

98
00:08:29,980 --> 00:08:35,110
are they build by get special based on
the match criteria that you specify for

99
00:08:35,110 --> 00:08:38,470
your aggregation and there's so many
types of obligations here are some

100
00:08:38,470 --> 00:08:42,280
examples we're going to cover a few of
them are about every new version of

101
00:08:42,280 --> 00:08:46,329
elasticsearch just they just launched
new obligation to new formats which is

102
00:08:46,330 --> 00:08:50,050
really cool but the best part of all
these is that you can nest aggregations

103
00:08:50,050 --> 00:08:54,520
and there's no limit on how much how
many child packets you can be out of an

104
00:08:54,520 --> 00:08:57,460
aggregation so let's build one

105
00:08:57,460 --> 00:09:01,060
how do we do that we create the Cray
Brian already covered that pretty much

106
00:09:01,060 --> 00:09:05,079
arm here we're going to select all the
msdn trees that we have ingested

107
00:09:05,080 --> 00:09:10,360
previously in elasticsearch so we use
select type entity then we're going to

108
00:09:10,360 --> 00:09:13,930
create derogation this is going to be
the first part 1 the basic navigation

109
00:09:13,930 --> 00:09:18,609
that is going to be inserted in our
search so we import the a capital A

110
00:09:18,610 --> 00:09:22,060
object from the last search yourself and
we are going to build that term

111
00:09:22,060 --> 00:09:26,410
segregations that's going to be matching
exact term that we are specified and on

112
00:09:26,410 --> 00:09:30,250
the field that we specify we use the
extension extension from the mft file

113
00:09:30,250 --> 00:09:34,420
record entry that we have interested
then we just have to add that to our

114
00:09:34,420 --> 00:09:40,360
search object the search art is using
our es client the last search climb and

115
00:09:40,360 --> 00:09:43,990
we just have to a dark gray which is all
them at the entries and then the

116
00:09:43,990 --> 00:09:47,770
aggregation the top-level packet which
we're going to define hear the name that

117
00:09:47,770 --> 00:09:48,579
we're going to give it

118
00:09:48,580 --> 00:09:52,600
we need that name later for our printing
out that aggregation and accessing the

119
00:09:52,600 --> 00:09:56,920
results are name is extensions it just
makes sense and are we just executed

120
00:09:56,920 --> 00:10:02,979
search and elasticity returns everything
are both the query results in the

121
00:10:02,980 --> 00:10:07,720
irrigation in that result Arctic so we
can answer questions like that one how

122
00:10:07,720 --> 00:10:11,860
many different faster fastest temper
file extension so this is how you access

123
00:10:11,860 --> 00:10:16,090
irrigation object you just get the
result of you go to the aggregations and

124
00:10:16,090 --> 00:10:19,630
then the extensions which is the name
that we declare for our preaching and

125
00:10:19,630 --> 00:10:21,490
then you put on the items in the back

126
00:10:21,490 --> 00:10:24,610
this is pretty complicated but in my
money was just like these

127
00:10:24,610 --> 00:10:28,270
that's the thing about education and
then you have on their 50 entries and

128
00:10:28,270 --> 00:10:32,170
you find one entry which extension he
said stem you create a pocket and put

129
00:10:32,170 --> 00:10:36,939
that inside and a PNG yellow and so on
and you just have all the accounts of

130
00:10:36,940 --> 00:10:39,500
all the file extensions on amity

131
00:10:39,500 --> 00:10:43,910
let's nest and aggregation answer more
complicated question how many files are

132
00:10:43,910 --> 00:10:49,430
created and other devices on our file
system so now we have again the query

133
00:10:49,430 --> 00:10:54,109
with all the empties and now we're going
to have an aggregation of tape type that

134
00:10:54,110 --> 00:10:57,770
he strapped instead of the term ones our
partner organizations navigate histogram

135
00:10:57,770 --> 00:11:03,170
based on the creation date of that msg
entry and are for that and after that we

136
00:11:03,170 --> 00:11:07,010
create a towel on that our parent
education and that we're gonna be at

137
00:11:07,010 --> 00:11:12,710
terms of aggregation based on the under
father extension on them at the entry we

138
00:11:12,710 --> 00:11:16,490
are created to search again when we are
all top-level aggregation to the search

139
00:11:16,490 --> 00:11:20,270
object you don't need to add the tale to
tell these embedded in the in the parent

140
00:11:20,270 --> 00:11:25,040
application we executed a search and
here's what we get back all the files

141
00:11:25,040 --> 00:11:30,410
that have been created profile extension
per day on our file system and if you

142
00:11:30,410 --> 00:11:34,850
guys are familiar with forensics there's
something really particularly are in

143
00:11:34,850 --> 00:11:42,200
these india slide and I'm yeah so
actually this is from from real data

144
00:11:42,200 --> 00:11:48,890
that we were investigation that another
co-worker of us was working on and we

145
00:11:48,890 --> 00:11:53,240
need some test data so we ran this and
we're just looking at car I'd that

146
00:11:53,240 --> 00:11:59,150
there's a bunch of 81 82 83 files on
that system that is investigating and it

147
00:11:59,150 --> 00:12:04,670
was on new year's day and so we grabbed
him and we said wow you're working

148
00:12:04,670 --> 00:12:10,610
pretty hard on New Year's Day it you
know on this and and he said no no I was

149
00:12:10,610 --> 00:12:14,930
with my family why and we show them this
and he said oh that engagements started

150
00:12:14,930 --> 00:12:19,130
like the second week of $MONTH january
that that client paid another firm to

151
00:12:19,130 --> 00:12:23,720
come in and they did collections and
then no analysis and then just handed

152
00:12:23,720 --> 00:12:29,900
the guy's a bill and then they called us
and and so I said well I guess I i hope

153
00:12:29,900 --> 00:12:32,510
you don't have to carve any files or
anything like that because they just

154
00:12:32,510 --> 00:12:36,650
performed a logical acquisition on the
hard drive that we could see on there so

155
00:12:36,650 --> 00:12:43,640
you know thanks thanks first responders
appreciate that so we can keep nesting

156
00:12:43,640 --> 00:12:47,990
aggregations and answer more complex
questions like for example this is a

157
00:12:47,990 --> 00:12:48,650
good one

158
00:12:48,650 --> 00:12:50,499
how many users log into the system

159
00:12:50,499 --> 00:12:55,029
today which user accounts and logon type
so the foot to build that we're just

160
00:12:55,029 --> 00:12:58,149
going to get all the windows and locks
that we have ingested previously on the

161
00:12:58,149 --> 00:13:02,829
last research and we're going to query
on the event 4624 then we're going to

162
00:13:02,829 --> 00:13:08,319
add a bait histogram / date that's the
event and to that we're going to create

163
00:13:08,319 --> 00:13:13,959
as submarket a tailback it based on the
username and to that we're going to

164
00:13:13,959 --> 00:13:18,848
create an and a third packet based on
the term logon type and then we're going

165
00:13:18,849 --> 00:13:22,149
to exit this is how like that on the on
the right you see how the back as will

166
00:13:22,149 --> 00:13:25,929
be built each congregation will generate
some tell packets but you don't get more

167
00:13:25,929 --> 00:13:31,089
data that what the counter on those
packets and then we expect we are now 22

168
00:13:31,089 --> 00:13:35,319
shirts we are engaging two are certain
we are secured our search and this is

169
00:13:35,319 --> 00:13:36,429
why you get back

170
00:13:36,429 --> 00:13:43,269
this is the sense spoiler alert our this
is from sams five wait i think is a

171
00:13:43,269 --> 00:13:47,679
total logansport a producer named after
logon type that can provide you a lot of

172
00:13:47,679 --> 00:13:50,559
like inside when you're starting
investigation and you don't know what to

173
00:13:50,559 --> 00:13:56,108
look for first also another cool thing
about these aggregations if you've used

174
00:13:56,109 --> 00:14:00,129
Cubana Cuban know what does in the back
end is just that it's aggregations the

175
00:14:00,129 --> 00:14:03,819
aggregated and they just put pretty
colors and now I areas and shapes that

176
00:14:03,819 --> 00:14:09,969
and you can see things really spot the
needle in the hay really fast so here we

177
00:14:09,970 --> 00:14:14,499
have our what derogation that we just
did with the user logins persistent r /

178
00:14:14,499 --> 00:14:21,579
logon type user name and date is just
that is every every day like the x-axis

179
00:14:21,579 --> 00:14:27,909
will be the time range and then we have
the login times on the usernames so the

180
00:14:27,909 --> 00:14:32,289
each of those graphs is a username and
then the colors and logon type of you

181
00:14:32,289 --> 00:14:34,899
guys can see about our um that's it

182
00:14:34,899 --> 00:14:39,819
so what is called me so I think you guys
can kind of see what we're doing we kind

183
00:14:39,819 --> 00:14:44,289
of like abstracted a higher-level
language of forensics I and and then

184
00:14:44,289 --> 00:14:49,089
with our friends at Carfax that we
ingested into elasticsearch this enabled

185
00:14:49,089 --> 00:14:53,859
us to automate the easy things and
forensics what whenever we have an

186
00:14:53,859 --> 00:15:00,489
investigation to have pre-stored queries
and if if a new artifact comes up it's

187
00:15:00,489 --> 00:15:03,640
great because this is scalable that you
can just add

188
00:15:03,640 --> 00:15:07,930
on to that query of all the program
execution that we have if all of a

189
00:15:07,930 --> 00:15:11,439
sudden Windows 10 we have a new artifact
you just add on another query to that

190
00:15:11,440 --> 00:15:13,870
bigquery just keep going

191
00:15:13,870 --> 00:15:18,910
this also made us more consistent among
analysts because we all have access to

192
00:15:18,910 --> 00:15:22,870
all the logic that we're looking for
whenever we start to investigate a case

193
00:15:22,870 --> 00:15:27,220
it's not just depending on what you
learned at sands or or some other

194
00:15:27,220 --> 00:15:33,160
training we also do a more rigorous
investigation that you know let's say

195
00:15:33,160 --> 00:15:36,910
prefetch is enabled and that's the only
thing that someone was going to check

196
00:15:36,910 --> 00:15:43,120
for 450 execution we have almost like 10
other artifacts that that we check if

197
00:15:43,120 --> 00:15:49,630
those are there as well as reset queries
for like anti forensics to check it

198
00:15:49,630 --> 00:15:53,800
the event mogul event logs were clear
maybe not every investigation you check

199
00:15:53,800 --> 00:16:01,120
that even logs were cleared and we we
also ingest everything from volume

200
00:16:01,120 --> 00:16:05,320
shadow copies so we're talking lots and
lots of data and its really nice to have

201
00:16:05,320 --> 00:16:09,340
an elastic search because it's really
really fast and we can cut through on

202
00:16:09,340 --> 00:16:13,210
millions and millions of lines really
quick to and in our forensic

203
00:16:13,210 --> 00:16:14,800
investigations

204
00:16:14,800 --> 00:16:22,300
yeah it's free it's scales it's fast and
and and we've also used this

205
00:16:22,300 --> 00:16:26,620
investigations that involve more than
one host so in Alaska search you have an

206
00:16:26,620 --> 00:16:32,380
index that we typically do one host /
index but let's say there are three

207
00:16:32,380 --> 00:16:36,490
involved investigation maybe there's
possibly lateral movement you can create

208
00:16:36,490 --> 00:16:41,770
an alias and Alaska searches which is
essentially a pointer to those different

209
00:16:41,770 --> 00:16:47,470
indices and then query on that L and the
alias and then c program execution

210
00:16:47,470 --> 00:16:54,250
across all three machines in the same
normalized timeline so thank you guys

211
00:16:54,250 --> 00:16:55,120
for attending

212
00:16:55,120 --> 00:16:58,410
do we have any questions

213
00:16:58,410 --> 00:17:13,918
yes so whenever we're processing this
that there's on there's no

214
00:17:13,919 --> 00:17:18,059
authentication where we're running this
all locally so yeah we're not going from

215
00:17:18,059 --> 00:17:22,049
a remote host authenticating so in the
lab environment is closed down there's

216
00:17:22,049 --> 00:17:26,280
no access from the internet so because
it's evidence is like highly sensitive

217
00:17:26,280 --> 00:17:30,059
confidential data we don't want anyone
that's in that so we just decided to cut

218
00:17:30,059 --> 00:17:36,870
every access to enrollment outside but
if you do use I believe elasticsearch

219
00:17:36,870 --> 00:17:41,969
shield you know meet you could build
that in with elasticsearch dsl work with

220
00:17:41,970 --> 00:17:48,419
authentication if you need it thanks

