00:00:00.000-->00:00:07.333
>> I was born and raised in the
Deep South, right next to
Georgia. [Audience laughs]. No,

00:00:07.333-->00:00:14.250
factually it’s true, oh wait a
second this is, ah it just okay,
machines hate me, I told you.

00:00:14.250-->00:00:19.583
[Laughter] Yes, they just ah,
they just messed up, there’s not
one, one slide is missing, but

00:00:19.583-->00:00:27.042
look it was really Deep South of
the USSR [Laughter] In the
Republic was the origin, right

00:00:27.042-->00:00:34.958
next to the Republic of Georgia,
yeah. So, um, and a speaking ah
speaking of my homeland, it’s

00:00:34.958-->00:00:41.083
just a funny story that my
latest book ah, ‘Deep Thinking’
was about ah, yeah AI, in my own

00:00:41.083-->00:00:51.875
experience ah, fightin’ working
ways was machines um and ah, the
book before two years ago ah,

00:00:51.875-->00:00:55.875
was called, ‘Winter is coming’
it was not a synopsis of ah
‘Game of Thrones’ [Laughter] it

00:00:55.875-->00:01:00.625
was about Vladimir Putin and the
enemies of the free world. And
while I was on the book tour,

00:01:00.625-->00:01:08.167
everybody wanted to ask me about
chess and IBM deep blue. Now,
when I’m touring with ‘Deep

00:01:08.167-->00:01:14.833
Thinking’ everybody wants to ask
me about Putin [Laughter]. But
I’ll, I’ll try to stick to the

00:01:14.833-->00:01:19.958
topic, I’m sure there will be a
couple of questions ah
afterwards, so I’ll be very

00:01:19.958-->00:01:25.583
happy to answer them. So, I’m
not a politician, I don’t duck
questions [Laughter]. Um so, um,

00:01:25.583-->00:01:31.958
it might seem strange that ah,
ah the game of chess, ancient
game 1500 2000 year’s old, g*d

00:01:31.958-->00:01:39.125
knows. It’s you know, um, um,
it’s a perfect analogy of
artificial intelligence, because

00:01:39.125-->00:01:43.792
we’re talking about, when we
talk about AI, we should
remember that there’s the letter

00:01:43.792-->00:01:47.042
I, Intelligence. And what could
be better than chess to, to
demonstrate that. Um,

00:01:47.042-->00:01:51.667
surprisingly a lot of people
believe that chess is kind of
the odd game, played by nerds

00:01:51.667-->00:01:57.042
played in the dark corner of
café. Ah, but to the contrary
when just you, look at

00:01:57.042-->00:02:01.792
Hollywood, Hollywood always used
it as the shorthand ah for
smarts for their characters. And

00:02:01.792-->00:02:10.000
look you know, aliens played
chess, X-men, wizards, I can
even mention vampires, not on

00:02:10.000-->00:02:15.458
the picture. Humphrey Bogart,
it’s an opening ah stage of, of
Casa Blanca, one of my favorite

00:02:15.458-->00:02:19.250
movies and for, for the chess
geeks, if someone plays chess
here, I can tell you, because I

00:02:19.250-->00:02:23.625
studied this position and I
looked inside, yeah [Laughter].
It’s a, it’s a real opening a

00:02:23.625-->00:02:29.292
French defense that was popular
in the forties. Humphrey Bogart
was a decent chess player, so

00:02:29.292-->00:02:32.708
um, and um, I can also mention
that, Alfred Binet, one of the
co-creators of ah IQ tests, at

00:02:32.708-->00:02:41.542
the end of the nineteenth
century. He was fascinated with
the chess players minds, and he

00:02:41.542-->00:02:46.208
studied for years, again looking
for some, you know, shortcuts,
to, to the secrets of human,

00:02:46.208-->00:02:53.542
human intelligence. Yeah, and
ah, it’s not surprising that
also ah, um, game of chess

00:02:53.542-->00:02:59.875
attracted, ah, um those who
wanted to build intelligent
machines. But, as usual the

00:02:59.875-->00:03:05.667
first one, is, as you can see,
the ‘The Turk von Kempelens’
Turk, it was a hoax. Um, it’s

00:03:05.667-->00:03:12.583
ah, it was a big miracle at the
end of the eighteenth century,
it was touring Europe and

00:03:12.583-->00:03:18.708
America, it beat some decent
players and also some ah very ah
famous but weak players like,

00:03:18.708-->00:03:23.792
Franklin and Napoleon. But of
course it was a hoax, it was not
a real playing machine it was an

00:03:23.792-->00:03:28.500
ingenious system of panels and,
and the sliding panels and
mirrors and a strong player was

00:03:28.500-->00:03:35.667
hiding inside. The funny thing
is that today, one or two
hundred years later, almost two

00:03:35.667-->00:03:40.000
hundred and fifty years later,
the problem is the opposite. In
the tournaments, we have other

00:03:40.000-->00:03:47.542
kind of hoax when the chess
players are trying to hide a, a
device in their pockets.[Laughs]

00:03:47.542-->00:03:55.250
So now you have to look for, for
, for, computer hiding in the
human body. [Laughs] Inside. Ah,

00:03:55.250-->00:03:58.833
and the, ah, ‘von Kemelen’ story
is famous but his second one is
story circulator is very little

00:03:58.833-->00:04:04.750
known, um story about a
mechanical device; mechanical
device in 1912 was introduced.

00:04:04.750-->00:04:09.708
It could play only with one
piece but it actually could
make, ah, make a mate with the,

00:04:09.708-->00:04:13.958
with the Rook. Ah, but still it
was, you know, it wasn’t, you
can say the prototype, the first

00:04:13.958-->00:04:19.083
computer. The most, you know,
interesting thing is that, um,
the founding fathers of the, the

00:04:19.083-->00:04:26.792
science, like Allen Purin and
Glochen Ohm. They were those,
um, they were um, um had great

00:04:26.792-->00:04:33.417
interest for the game of chess.
And they believed, yes ah, they
believed that, um, um the game

00:04:33.417-->00:04:38.833
of chess could be, could, also,
could be um, an opener for this
ultimate secrets of human

00:04:38.833-->00:04:44.458
intelligence and if one then,
chess computer plays well
against the world champion or

00:04:44.458-->00:04:50.500
beats the world champion, that
would be the moment, ah of you
know, of, of revelation. Um,

00:04:50.500-->00:04:55.292
fewer, fewer, remember that,
Allen Furick actually wrote the
first chess program, all the way

00:04:55.292-->00:05:00.833
back in 1952 and it was a great
accomplishment, but the most
important one that there was no

00:05:00.833-->00:05:06.958
computer. So, it was, it was
just an algorithm that um he
used to play this one game ah,

00:05:06.958-->00:05:13.042
and, ah he acted like a human
CPU. So, um, now, it’s a’ it’s,
it’s important to remember that

00:05:13.042-->00:05:20.042
ah, the founding fathers thought
that, the way AI will manifest
itself is basically following

00:05:20.042-->00:05:25.667
the same path as humans. So it
would be kinda the replica the
way we, we work. Ah, to the

00:05:25.667-->00:05:29.917
contrary, contrary to the
expectations actually moved in
the opposite direction wiz, ah,

00:05:29.917-->00:05:37.917
wiz brute force. So, um, now,
um, I entered ah the competition
against machines in, ah, 1985.

00:05:37.917-->00:05:43.500
Um, you could look at this, this
picture it’s the, it’s not ten
actually thirty two boards and

00:05:43.500-->00:05:49.042
ah, I played humans, but as a
matter of fact the real game was
against computers. They have

00:05:49.042-->00:05:53.750
four leading manufacturers of
chess computers, at that time
there were some dedicated chess

00:05:53.750-->00:05:59.667
machines. Maybe some of you have
them still, like you know, a
piece of antique. Um and they

00:05:59.667-->00:06:06.458
had eight, ah models each and I
played thirty two. And um, I won
all thirty two games. Ah, but

00:06:06.458-->00:06:11.583
what’s very important, it was
not a surprise, everybody
thought it was a very, very

00:06:11.583-->00:06:17.125
natural result and every time I
look at this picture and look at
these games argh, I’m sighing,

00:06:17.125-->00:06:24.542
that was a golden age [Laughter]
of chess. Machines were weak and
my hair was strong [Laughter]

00:06:24.542-->00:06:40.958
[Audience applauses] Um, 1985,
June, twelve years later, I
faced just one computer, just

00:06:40.958-->00:06:48.042
one computer. Ah, by the way,
people tend to forget that match
in 1997 was a rematch, because I

00:06:48.042-->00:06:56.708
won the first one in 1996, in
Philadelphia. Um and ah, okay I
won this match, but just to be

00:06:56.708-->00:07:04.083
fair, the watershed moment for
the computer chess was not in
1997 but I would say 1996 in

00:07:04.083-->00:07:10.125
Philadelphia. Though I won the
match, but I lost game one, then
I fought back and I won three

00:07:10.125-->00:07:17.417
more games, winning the match
four, two, two. But fact, the
fact that a machine was able to

00:07:17.417-->00:07:24.250
beat a world chess champion in,
in a normal chess game, that was
already just like a big signing

00:07:24.250-->00:07:28.292
on the wall. The rest was a
matter of technique, though I
didn’t expect, ah IBM just to do

00:07:28.292-->00:07:32.083
so much work and just to come
back, you know later with a
stronger machine. But the

00:07:32.083-->00:07:38.375
biggest mistake, except not ask
people stock options, [Laughter]
you know two weeks of, two weeks

00:07:38.375-->00:07:44.083
of the evet and the ah, it’s
the, it, it grows ah, from the
11.4 billion dollars in value.

00:07:44.083-->00:07:48.375
[Harsh breath] okay, but the
biggest mistake was not reading
the fine print. Because one of

00:07:48.375-->00:07:55.292
the problems in 1996 that I, I
faced, while playing Deep blue,
was, it was a black box, I

00:07:55.292-->00:08:00.542
didn’t know anything about the
opponent and while preparing for
the game, whether it’s a chess

00:08:00.542-->00:08:04.917
game, or soccer game or
whatever. You always look at the
games and, and some strategies

00:08:04.917-->00:08:10.792
used by your opponent. Now Deep
blue, no information, now, I
tried to be smart and I said,

00:08:10.792-->00:08:17.917
for the next match, we have to
make sure that, um, I will have
access to the games played by

00:08:17.917-->00:08:23.542
Deep blue. They said absolutely,
but the fine print said, ‘played
in official competitions’. And

00:08:23.542-->00:08:30.250
of course Deep blue has not
played a single game outside of
the lab [Laughter]. So in 1997,

00:08:30.250-->00:08:33.750
I faced again the, the black
box. That’s, [Inaudible]
unfortunately [Inaudible] I won

00:08:33.750-->00:08:39.875
the first game and I, I lost the
match. So, um, by the way, where
were you hackers twenty years

00:08:39.875-->00:08:46.292
ago when I needed you
[Laughter]. So, I think that
looking at, ah front row, some

00:08:46.292-->00:08:55.417
of you may, might not yet been
born [Laughter]. Um, so, um, um,
the problem [Inaudible] states

00:08:55.417-->00:09:00.958
for me in that match was that, I
still treated the match as the
great scientific and social

00:09:00.958-->00:09:06.250
experiment. Because I thought it
would be great, you know, just
to actually check, at what point

00:09:06.250-->00:09:12.167
human intuition could be matched
or even just overshadowed by the
brute force of calculation. And

00:09:12.167-->00:09:17.833
again, Deep blue, even with this
phenomenal speed, 200 million
positions per second, pretty

00:09:17.833-->00:09:27.125
good speed for 1997. Ah, was
anything but intelligent. Um,
the way the Blue played, ah,

00:09:27.125-->00:09:36.958
that’s offered us no input in
the mysteries of ah, of, of
human intelligence. Ah, it was

00:09:36.958-->00:09:39.500
as intelligent as your alarm
clock, though losing to 10
million dollar alarm clock

00:09:39.500-->00:09:45.208
didn’t make me feel any better.
[Laughter] Ah, um and ah, um, I
just realized [Stutter] remember

00:09:45.208-->00:09:50.042
in the opening ceremony of the
match when ah, actually the
press conference when the, the,

00:09:50.042-->00:09:54.833
the um, the man who ah lead the
project said, ‘its standard
about the experiment, now it’s

00:09:54.833-->00:10:00.000
about winning.’ Okay, that, that
was definitely about winning or
losing so and I lost to the,

00:10:00.000-->00:10:05.375
Blue, of course I wanted to play
another match ah and I then
retired the computer. Um, okay,

00:10:05.375-->00:10:11.833
they, I said they killed the
only partial witness [Laughter]
Um, and eh, um, I was actually

00:10:11.833-->00:10:17.417
trying to find out, what
happened to the Blue, just I
couldn’t. But lately, actually I

00:10:17.417-->00:10:23.833
discovered, now it has a new
career. It’s making sushi at
JetBlue Terminal in JFK

00:10:23.833-->00:10:37.667
[Laughter] [Audience applauses].
I, I, I love sushi but I don’t
eat there, yeah guys [Laughter]

00:10:37.667-->00:10:45.125
some I don’t know why, yeah,
yeah. So, um, again that’s the,
that’s the story was over for

00:10:45.125-->00:10:48.708
chess and that’s very quickly
because, can as I’m sure some of
you playing chess is, well it’s

00:10:48.708-->00:10:54.792
chess. We talk about Gor, about
other games. Humans are
vulnerable because we don’t have

00:10:54.792-->00:11:00.292
steady hand, we make mistakes.
So even the great game played by
the world; world leading players

00:11:00.292-->00:11:06.083
at the very top, ah a world
championship match. Say fifty
moves, forty five good moves,

00:11:06.083-->00:11:10.250
four great moves, one tiny
inaccuracy, is inevitable. We,
in the human game it doesn’t

00:11:10.250-->00:11:15.667
matter, in the fightin machine,
you will be punished. Not
losing, maybe, maybe not losing

00:11:15.667-->00:11:20.208
the game but definitely not
winning so machine will escape.
So, ah I just realized at one

00:11:20.208-->00:11:24.792
point that, it just will be a
matter of time. Because we
cannot reach the same level of

00:11:24.792-->00:11:29.875
vigilance and precision that is
required, to beat the machine,
because the machine does the

00:11:29.875-->00:11:34.167
steady hand. Again we saw the
same at Gor, many, ah, ah years
later. [Inaudible comment]

00:11:34.167-->00:11:39.333
Lately, machine ah, conquer the
game of Gor as well. Ah, but
again, it was just about the

00:11:39.333-->00:11:44.417
game of chess. And ah it’s a
game proved to be vulnerable to
the brute force. Um, but it’s,

00:11:44.417-->00:11:50.833
you know, it was not ah, it was
not ah, um, yet um, an AI as had
been reported by IBM and by. Few

00:11:50.833-->00:12:00.083
people remember the match,
saying oh, it was the dawn of AI
actually not. Um, and later on

00:12:00.083-->00:12:05.208
um, I played few more matches
with the machines. Because, um
when, when these days are

00:12:05.208-->00:12:11.458
analyzed these games, using the
modern chess engines, ah it was
quite a painful experience,

00:12:11.458-->00:12:16.917
probably back to the past,
revisiting it and recognizing
how poor, I played in this

00:12:16.917-->00:12:22.750
match. I can blame myself, but
also the people were not strong
enough, this is something that

00:12:22.750-->00:12:28.083
you may not believe. But a free
chess app on your mobile device,
today, is stronger than Deep

00:12:28.083-->00:12:35.292
blue. Yes, frosty, yeah and of
course if you have a, a chess
engine like, you know, Stockfish

00:12:35.292-->00:12:40.833
or Commodore and you have it on
your laptop, it’s much, much
stronger. And ah, I just, you

00:12:40.833-->00:12:45.542
know, run these games and it’s
one of the moments, I think game
five, just looking at the end

00:12:45.542-->00:12:50.292
game and the Deep blue saved
game by miracle and everybody
talked about the great escape

00:12:50.292-->00:12:56.583
and phenomenal quality of chess.
Today, you put it in the
computer, it’ll, it’s, it

00:12:56.583-->00:13:02.417
laughs. It shows within thirty
seconds to a minute, depending
on the strength of your, of, of

00:13:02.417-->00:13:08.042
your ah the speed of your laptop
is that first it was a draw,
people made a mistake then I

00:13:08.042-->00:13:13.500
made a mistake missed the Queen
and then people save the game.
So, it’s okay, that’s the,

00:13:13.500-->00:13:20.375
that’s, that’s the Moore’s Law,
I guess and there’s nothing
wrong about it. Um, and ah, ah

00:13:20.375-->00:13:26.083
those two more matches I played
in 2003, they both match ended
in a draw. Ah, I played,

00:13:26.083-->00:13:31.292
actually they forced me to wear
the glasses, to play on X3D, as
if playing a machine was not,

00:13:31.292-->00:13:37.833
you know, tough enough
[Laughter] Yeah, ah, I ah did
well, so I was quite pleased

00:13:37.833-->00:13:45.333
wiz, wiz ah, wiz ah, my
accomplishment. But again, the
story was old, so I knew it and

00:13:45.333-->00:13:51.250
um I just was thinking in the
future. And just um just what
gave me a good thought, just

00:13:51.250-->00:13:56.250
look at this picture. This is
the kids, so you have the
nineties; you have the, the

00:13:56.250-->00:14:04.667
beginning of the century, the
century and then more than
modern days. So kids, you know they

00:14:04.667-->00:14:08.167
just um they have to look at
it’s like piece of antique, my
kids will not recognize it. So

00:14:08.167-->00:14:11.167
then there’s this more
sophisticated ah, um, keyboards
and know they just [makes

00:14:11.167-->00:14:16.000
sliding noise] they’re their
fingers. So, um, what is
important that is, it’s, it more

00:14:16.000-->00:14:20.625
intelligent machines make our
task easy again. I’m, I’m
telling you that so, you know

00:14:20.625-->00:14:25.042
better than anyone else. So, I’m
leashing our human creativity by
clearing the way of competitive

00:14:25.042-->00:14:33.792
and technical tasks. So, then I
had a thought, um, how about
combining the strength of

00:14:33.792-->00:14:45.375
machine and humans. And let’s
use chess as the um, as um, oh,
as an example, because in chess

00:14:45.375-->00:14:49.750
we have the resolve. You know
exactly where machine is strong,
and you know what machine can do

00:14:49.750-->00:14:56.750
as well as humans. So I came up
with, with a concept, um, that I
called, Advanced chess, okay.

00:14:56.750-->00:15:03.333
Following a famous Russian
saying, ‘you can’t beat them,
join them’ [Laughter] Um, so I

00:15:03.333-->00:15:09.708
go with, Advanced chess, men
plus machine facing another
human plus machine. So, um, and

00:15:09.708-->00:15:15.750
in 1998 I played another elite
player, Veselin Topolov from
Bulgaria you can see his

00:15:15.750-->00:15:22.042
picture. Ah, we both had ah,
again, pieces of antique. Ah,
now the interesting thing is

00:15:22.042-->00:15:28.958
that, we did not do well,
because we were not, we were not
able to maximize the effect

00:15:28.958-->00:15:34.333
working with the computer. And I
just couldn’t understand why,
we’re great players, so what’s

00:15:34.333-->00:15:40.500
wrong with that? So, we didn’t
do well, and the and the answer
came later with the introduction

00:15:40.500-->00:15:44.125
of the so called, Freestyle
chess tournament, on the
internet. What I call,

00:15:44.125-->00:15:49.375
invitation for cheating. You can
play on internet, ah, being
connected to the super computer,

00:15:49.375-->00:15:54.208
you can have your own computer,
you can have many computers, I
mean do whatever you want. Now,

00:15:54.208-->00:15:59.667
as predicted, human plus machine
always dominated super computer.
Again, the reason is very

00:15:59.667-->00:16:04.542
simple, because machine
compensates for our weaknesses,
so we get, we get into a good

00:16:04.542-->00:16:09.375
position and you can switch it
to the computer. So no more
vulnerabilities of, of, ah of

00:16:09.375-->00:16:15.417
humans that can be exploited by
the other machine. But the trick
was not that is not the

00:16:15.417-->00:16:19.667
sensational result. The
sensational result was that the
winners of the competition, the

00:16:19.667-->00:16:24.542
first one, and it was repeated
later, were not top players, but
actually relatively weak

00:16:24.542-->00:16:32.333
players. Ah, working with
ordinary machines, but having
superior processes. And that me

00:16:32.333-->00:16:39.000
do, to make this ah formulation,
which I think’s quite important
because it’s, it’s hard to

00:16:39.000-->00:16:45.375
understand, it sounds like a
paradox. That, a weak player
plus an, an ordinary machine

00:16:45.375-->00:16:50.167
plus a superior process will be
dominant in the game against a
strong player, even strong

00:16:50.167-->00:16:57.292
computer an inferior process.
Interface decides everything and
ah, it’s ah its quite amazing

00:16:57.292-->00:17:02.708
that it’s just ah you, you don’t
need a strong player, you don’t
need Garry Kasparov. Just to be

00:17:02.708-->00:17:09.083
at the, at, at the side of the
machine, finding the best move.
And the answer is simple,

00:17:09.083-->00:17:13.833
because when you look at the
relative strength of humans and
machines today and I will go

00:17:13.833-->00:17:17.292
beyond chess, but let’s start
with chess because with chess we
have numbers. If you are aware,

00:17:17.292-->00:17:22.667
with the, with the ratings and
the rankings in chess well,
let’s ah, give you an idea. Um, when

00:17:22.667-->00:17:31.625
my, my top rating was 2851, when
I retired, was, I dropped I was
2812. Magnus Carlsen was

00:17:31.625-->00:17:39.333
traversing 2800 territory as
well. There are about fifty
players or plus in 2700, early

00:17:39.333-->00:17:45.958
2800 category. That’s, that’s,
that’s elite, of the world of
chess. Now today’s strength of

00:17:45.958-->00:17:53.917
the computer, it’s about 3200,
now on dedicated software it
will be 3300 to 3400. Now we

00:17:53.917-->00:18:02.000
understand why we don’t need a
strong player, because a strong
player like myself will be

00:18:02.000-->00:18:06.917
tempted to, push the machine in
this direction or that
direction. I will be challenging

00:18:06.917-->00:18:10.875
machines, ah, evaluations while
to the contrary, I have to be an
operator. So a decent player,

00:18:10.875-->00:18:14.542
that doesn’t have the same
pride, the same honor as the
world champion or strong player

00:18:14.542-->00:18:19.375
will be far more effective; in
creating the human machine,
human machine combination. I

00:18:19.375-->00:18:24.625
think this is this is [stutter]
a very important discovery in
chess. And I believe it goes

00:18:24.625-->00:18:31.792
beyond chess, for instance in
medicine, we know today that,
ah, ah in many cases, machines

00:18:31.792-->00:18:38.083
are far more accurate in giving
diagnosis than the best doctors.
So, would you, would you like a

00:18:38.083-->00:18:43.500
good doctor to work with the
machine or a good nurse? That
with full instruction and a

00:18:43.500-->00:18:49.292
little guidance but not will
interfere, because if, I don’t
know the exact numbers, but say

00:18:49.292-->00:18:54.750
the doctor will be, do 66% of
cases and machine maybe 5%.
Numbers are on either side, but

00:18:54.750-->00:19:01.500
psychologically if you’re a good
doctor, you cannot accept it.
So, one can look at the progress

00:19:01.500-->00:19:05.167
of computers these days is just
basically we should realize
that, machines [Inaudible],

00:19:05.167-->00:19:13.542
medical diagnosis, you name it,
could be good at climbing at 80,
85 maybe 90%. But know that’s,

00:19:13.542-->00:19:19.500
that’s, that’s where we belong
to, humans, the last decimal
places. And it could make a hell

00:19:19.500-->00:19:23.958
of a difference, it’s like when
you know we shouldn’t move it,
just you know one degree

00:19:23.958-->00:19:29.458
difference in angle, and it
could you know be hundred
meters, ah, ah gap you know,

00:19:29.458-->00:19:35.708
wide on the target. So the same
is here, it’s, it’s about our
ability to actually channel this

00:19:35.708-->00:19:40.750
massive computing power and just
to find the right, right
direction for that. So and um,

00:19:40.750-->00:19:47.083
so I still believe that with all
the fears that machines are just
going to replace us and just you

00:19:47.083-->00:19:51.250
know, it will be the end of the
world and Armageddon. I believe
there’s room, there’s plenty of

00:19:51.250-->00:19:55.958
room because as I said, it’s
about human creativity and this
unique tools and intelligent

00:19:55.958-->00:20:00.167
machines will enhance our
creativity, unleash our
creativity if we know how to use

00:20:00.167-->00:20:06.750
it. Um, so ah one of the,
actually looking for the
answers, sometimes you go off,

00:20:06.750-->00:20:13.833
off side, not in the ah not
searching in the hall of
science. Ah, but in the hall of

00:20:13.833-->00:20:29.500
art, and ah I found quite a good
paradox that was ah, um, um,
allegedly said by, by a great

00:20:29.500-->00:20:34.458
artist, ‘Computers are useless
they can only give us answers.’
I think that its, it’s a piece

00:20:34.458-->00:20:38.292
of wisdom, then again you don’t
expect you know Picasso to be on
the side of philosophy. But I

00:20:38.292-->00:20:42.292
find it, I find it quite ah,
quite ah, um encouraging.
Because machines find answers

00:20:42.292-->00:20:50.208
and answers and an end and
Picasso could not accept ends,
he was an artist. It’s ah, he,

00:20:50.208-->00:20:57.208
he had to constantly re-invent;
he had to re-invent constantly
his art. That’s what we do, so

00:20:57.208-->00:21:02.417
this is exactly, where we, where
we have to start, asking
questions. Um, can a machine ask

00:21:02.417-->00:21:08.667
questions? Ah, once I ah, paid a
visit to the Bridge Waters,
Byhalia Hedge farm. The reason,

00:21:08.667-->00:21:13.083
I wanted to talk to David
Ferrucci, the father ah, of
Watson. And we talked about

00:21:13.083-->00:21:19.042
machine’s asking questions and
ah [Inaudible] he said yes
machines can ask questions, but

00:21:19.042-->00:21:25.458
they don’t know what questions
are relevant. Thank you, that’s
exactly the point, so we are, we

00:21:25.458-->00:21:32.208
are still in the game. We’re
still in the game, we still have
a chance to move on and ah and

00:21:32.208-->00:21:37.250
that gives me a lot of ah, a lot
of confidence that ah, the game,
the game is not over. And, ah

00:21:37.250-->00:21:46.458
Um, just a few pictures so I um,
um some photos from the future
of autonomous machines and

00:21:46.458-->00:21:52.917
machines that, you know
essentially program themselves.
So the one picture there is

00:21:52.917-->00:21:57.417
Demis Hassabis and his AlphaGo.
Actually this is a problem of
first machine that ah, that

00:21:57.417-->00:22:02.625
could be called ah a prototype
of AI. As I said Deep blue,
brute force, Watson still it’s,

00:22:02.625-->00:22:13.583
it’s maybe it’s a transition but
it’s not AI. Now AlphGo is it’s,
it’s a deep learning program

00:22:13.583-->00:22:21.375
that keeps, um re-inventing
itself by looking for the
patterns. While playing millions

00:22:21.375-->00:22:28.500
and millions of games. Now, I
can tell you it’s the first time
that we had link with, with real

00:22:28.500-->00:22:35.125
black box. Because with Deep
blue for instance, if you had
um, hundred years to spare and

00:22:35.125-->00:22:43.125
ah, and you be willing to look
for thousands of miles of ah of
logs, you’ll definitely go back

00:22:43.125-->00:22:48.583
to the original idea, why the
decision was made. Now with
AlphaGo, I don’t believe that

00:22:48.583-->00:22:53.875
even Demis Hassabis can tell you
why version six plays better
than version nine or other way

00:22:53.875-->00:22:59.708
around. So it’s a, it’s a
greater accomplishment on one
side, but on other side it ah it

00:22:59.708-->00:23:04.833
might be challenging because if
there’s ah, if there’s ah back,
so how we going to find it out.

00:23:04.833-->00:23:10.542
But again, let’s move, that,
that, that’s a move in, in, in
this AI is, AI direction. Um,

00:23:10.542-->00:23:19.583
and while I just you know I was,
I, I was um, spoke at um, ah
Googles HQ at um, Mountain View

00:23:19.583-->00:23:25.000
um, and ah they gave me tool of
ah Google X. This isn’t
[Inaudible] observation because

00:23:25.000-->00:23:31.417
obviously there are many
challenges for self-driving cars
and for other projects for, for

00:23:31.417-->00:23:38.083
the drones, flying drones
dropping goods. But the biggest
problem actually comes from not

00:23:38.083-->00:23:45.083
from maybe I’m wrong another
problem, as big as a technical
one comes from, regulations. And

00:23:45.083-->00:23:50.333
this is an interesting question,
it was oh, machines are you just
know you know killing jobs you

00:23:50.333-->00:23:56.083
know, that are replacing ah,
humans. So what are you going to
do? That’s called history of

00:23:56.083-->00:24:02.042
civilisation, that, that, that
has been happening over a
millions, hundreds of years. I

00:24:02.042-->00:24:08.000
think to the contrary the
problem is not that machines are
replacing, replacing human jobs,

00:24:08.000-->00:24:15.250
now on, on the intelligent,
intellectual side. I say now
machines are going after people

00:24:15.250-->00:24:22.333
with college degrees and twitter
accounts. [Laughter] Not too
fast, I think too slow, and let

00:24:22.333-->00:24:26.208
me tell you why because it’s a
normal cycle we just don’t
recognize that ah disruption

00:24:26.208-->00:24:32.417
means that the new technology,
breakthrough technology, before
it creates jobs it kills jobs.

00:24:32.417-->00:24:41.042
It, it, it, it renders whole
industries redundant, obsolete.
And then it creates new jobs,

00:24:41.042-->00:24:46.875
this is a process, this is a
cycle. Now if you try to protect
the agro, by sticking who is the

00:24:46.875-->00:24:51.292
old technologies by whatever,
printing money or just creating
some artificial advantages for

00:24:51.292-->00:24:56.875
the old industries, you made
this process slower and more
painful. It’s going to happen

00:24:56.875-->00:25:02.792
anyway, but the problem is that
with so many regulations, we’re
just facing that, many things

00:25:02.792-->00:25:08.708
are just, just been
intentionally slowed down. Ah
and um I believe this is, this

00:25:08.708-->00:25:12.208
is, it’s even a bigger problem
than, than the challenges we’re
facing. And, it’s

00:25:12.208-->00:25:20.958
psychologically, people say, oh
how can we sit in the driverless
car? Really, I just looked in

00:25:20.958-->00:25:26.125
the back and just found a
hundred years ago one of the
most powerful unions in New York

00:25:26.125-->00:25:36.667
City was the Union of elevator
operators. [Laughter] Really, 17
000 strong You know it’s, it’s ,

00:25:36.667-->00:25:41.458
cause people by the way
technology, to push the button,
was there already, but people

00:25:41.458-->00:25:46.375
didn’t trust it. No, how can you
get in the elevator and just to
push the button? [Laughter]

00:25:46.375-->00:25:56.125
Aaaah. [Laughter] You know what,
you know, why, why this Union
died and why people switched on?

00:25:56.125-->00:26:04.542
Because one day they decided to
go on strike, [Laughter] you
know, strike? [Audience claps]

00:26:04.542-->00:26:08.375
and when people had to climb to
the Empire State building, they
decided, maybe you, you ought to

00:26:08.375-->00:26:15.167
push the button. [Laughter] And
I’m thinking now just, you know,
twenty, thirty years from now,

00:26:15.167-->00:26:20.417
our kids, our grandchildren,
they say, how these crazy guys
they were driving cars. Look at

00:26:20.417-->00:26:25.625
the statistics; you know it’s
one of the greatest causes of
human death [Laughter] how could

00:26:25.625-->00:26:30.417
they afford to do that?
[Laughter] Um, and of course you
know with this, it’s ah, it's

00:26:30.417-->00:26:34.833
pure psychology. So many
accidents, we know, people being
killed in car accidents, but if

00:26:34.833-->00:26:40.333
you have one accident in a
driverless car, that’s a big
story. Any, any glitch any

00:26:40.333-->00:26:46.000
mistake, you know made with AI,
with new technology that’s a
story, you know, front page of

00:26:46.000-->00:26:50.667
newspaper. But again,
statistically, come on, that’s
you know; simply don’t just look

00:26:50.667-->00:26:54.292
at numbers. Yes, I understand
it’s bad if you are in this, you
know in this tiny percentage,

00:26:54.292-->00:26:58.917
but as the, as the humanity
we’ll all win if we just move
forward, you know just without,

00:26:58.917-->00:27:07.958
ah being paralyzed by, by this,
by this field. Um, so um, and
um, it’s ah, it’s a picture of

00:27:07.958-->00:27:16.208
the, of our security center and
ah, um, you know, um, another
portion is now, because we talk

00:27:16.208-->00:27:21.750
about fake news and we talk
about cyber security. It’s,
it’s, it’s a big political issue

00:27:21.750-->00:27:27.625
and there, there are many calls,
so how, how’re we going to fight
hate speech for instance? So, I

00:27:27.625-->00:27:32.375
do regular blogs for a boss, my
new one that will be released in
a couple of days, it’s, it’s

00:27:32.375-->00:27:37.167
about hate speech. As I say it’s
the fighting hate, saving
speech. It’s just; we should

00:27:37.167-->00:27:40.917
realize that, this problem did
exist before. It’s not that they
been invented, they’re been

00:27:40.917-->00:27:48.083
magnified, because internet just
involves millions and actually
billions of people in. Again, I

00:27:48.083-->00:27:52.417
think it's good news and we
should just simply realize that
it’s trying to stop it, trying

00:27:52.417-->00:28:00.542
to outlaw it, you know, it’s not
going to work. Because you still
have Putin’s of this world and

00:28:00.542-->00:28:05.333
you still have other, you know
bad guys sitting elsewhere. That
they will use our technology,

00:28:05.333-->00:28:11.333
created in the free world,
against us. So, I think we
should just embrace it, that’s

00:28:11.333-->00:28:17.167
my view, so I always say, it’s
about us. The answers inside us,
it’s about our own strengths,

00:28:17.167-->00:28:23.458
ah, and our own confidence and I
say that intelligent machines
will not make us obsolete, our

00:28:23.458-->00:28:29.208
complacency might. So, um ah I,
I think that is just you know
that we, we should just realize

00:28:29.208-->00:28:34.250
that again, there is certain
limitations in this corporations
of humans and machines, but

00:28:34.250-->00:28:38.708
there is plenty of room, there’s
plenty of room, as it happened
before, it opens new

00:28:38.708-->00:28:43.625
opportunities. It destroys the
old world and creates a new one
and sooner we move forward, the

00:28:43.625-->00:28:53.667
better we are. Now let’s just,
now let’s move to more, just ah
so, ah science fiction world.

00:28:53.667-->00:28:59.042
It’s an interesting paradox that
when you go back fifty, sixty
years, the science fiction was

00:28:59.042-->00:29:04.917
all positive, it was all
utopian. And then gradually it
moved from utopian to dystopian

00:29:04.917-->00:29:10.167
youth. Ah, yes, we don’t want to
hear about this future, by the
way, it didn’t just happen

00:29:10.167-->00:29:18.958
overnight, it was a time when
people decided maybe it’s too
ah, it’s too risky. To ah, to ah

00:29:18.958-->00:29:24.583
do space exploration, actually
it is too risky. You know, just
imagine in 1969, 1969 when

00:29:24.583-->00:29:30.792
Americans landed on the moon,
the entire computing power of
NASA was less, than any

00:29:30.792-->00:29:39.417
computing power of any device in
your pocket here. So, this
device is a thousand times more

00:29:39.417-->00:29:44.375
powerful than Cray’s
supercomputer forty years ago.
So just imagine for a moment how

00:29:44.375-->00:29:49.042
much power we carry with us and
how we use this. I’m not sure
that, this, that Apple, um

00:29:49.042-->00:29:54.375
iPhone 7 is the same as Apollo
7, this has the same effect. And
I think there are many great

00:29:54.375-->00:29:58.583
things that can happen if we
start looking for, you know,
just for the sky, for, for the

00:29:58.583-->00:30:02.708
stars again, Deep Ocean’s,
there’s so many great things we
can do. And again, we should

00:30:02.708-->00:30:09.208
realize that machines, they are
offering us an opportunity to
take larger risk. And um, I just

00:30:09.208-->00:30:20.917
wanted to end up on, on a
positive note. Is it positive?
[Laughter] Actually it is, now

00:30:20.917-->00:30:24.875
by the way, the, the picture in
the bottom, you know it’s not ah
Photoshop; it’s a real one, yes

00:30:24.875-->00:30:30.417
I was in the office of the
Terminator in 2003, yeah. Oh, he
loved the game of chess; his

00:30:30.417-->00:30:35.500
kids you know will have these
mandatory lessons. Yeah, we
played a game of chess ah; yes

00:30:35.500-->00:30:43.917
it ended in a draw very quickly
[Laughter] And he was I’m sure
he was so excited that six

00:30:43.917-->00:30:49.083
months later he ran for the
Governor of California.
[Laughter] And won [Laughter]

00:30:49.083-->00:30:55.542
Um, now you think why the
picture is there, why it’s, why
it’s, um why I call it positive,

00:30:55.542-->00:31:03.042
because you know, set aside the
first movie, in the rest ah of
the, of the series. Ah, it’s,

00:31:03.042-->00:31:13.167
it’s still just; you know,
Arnold who always is, always on
the winning you know old, but

00:31:13.167-->00:31:18.083
not obsolete, beating ah newer
machines. But actually it’s a
combination of what I described

00:31:18.083-->00:31:26.625
few minutes earlier; it’s a, ah,
human plus an old machine plus a
superior interface. Dominating

00:31:26.625-->00:31:32.583
newest machines [Laughter] so I
guess I, gives us a little of
you know just self-confidence

00:31:32.583-->00:31:38.667
that working with machines and
having the best interface, I’m
sure you know you guys are just

00:31:38.667-->00:31:44.583
the best in the world who can do
that, so this is how we move
forward. And, ah and then for

00:31:44.583-->00:31:49.208
those who say yes, but machines
will eventually get everything
done, so this is, no matter what

00:31:49.208-->00:31:53.917
else, they will calculate
everything, because machines
knows the all’s. They will

00:31:53.917-->00:31:59.292
calculate you know it’s not
about calculating everything; by
the way the game of chess for

00:31:59.292-->00:32:03.750
instance is technically called
mathematically infinite, ten to
the power of forty five, number

00:32:03.750-->00:32:08.958
of legal moves. That’s more than
enough for any computer in the
universe. Um, but the most

00:32:08.958-->00:32:12.833
important thing is this, again,
is in the games it’s all, you
know, it’s, it’s, it’s can be

00:32:12.833-->00:32:18.583
not calculated but machine can
be always be ahead of humans,
it’s all about playing by the

00:32:18.583-->00:32:23.333
rules and you know the rules are
fixed. You know that machine,
can you know just ah find the

00:32:23.333-->00:32:26.792
best, best path in, in, in, in
this jungle. But now if we move
into the, if we move into the

00:32:26.792-->00:32:34.667
just normal situation, now wives
are you sure machines can be
helpful all the time? Let’s look

00:32:34.667-->00:32:42.500
at a very ordinary situation,
mundane. You have your computer
running your budget and you are

00:32:42.500-->00:32:51.333
in the store, you buy a gift, an
expensive gift and machine
beeps, nah, you’re at your

00:32:51.333-->00:33:00.333
limit, machine knows the outs.
But one more, slight change, you
have your kid next to you and

00:33:00.333-->00:33:05.583
it’s his birthday or her
birthday. Now how does it change
the equation? It changes

00:33:05.583-->00:33:10.167
everything, it could be a
wedding gift or whatever, I can
start adding this little things

00:33:10.167-->00:33:17.458
that will change everything and
I don’t think you can simply,
you know incorporate it, into,

00:33:17.458-->00:33:21.542
into this equation. So,
definitely we have you know, we
have, we have a little room.

00:33:21.542-->00:33:27.125
It’s like asking the question,
it’s and because the situation
changes. And this is, this is

00:33:27.125-->00:33:30.958
something that you may call
ordinary but I had something for
in the movies, I have something

00:33:30.958-->00:33:39.667
more dramatic. Let’s look for
something that is, ah,
extraordinary. [Laughter] Empire

00:33:39.667-->00:33:46.750
strikes back, ah you remember
this, this little episode? Hans
Solo is, is, is, is, ah

00:33:46.750-->00:33:53.167
directing his ship into the
field of asteroids and cp3O ah,
cp30 is just aahhh, panic, the

00:33:53.167-->00:34:02.333
chances of surviving in this
field are 3720 to 1, never tell
me the odds. [Laughter] Now,

00:34:02.333-->00:34:09.500
this is interesting, it just, ah
just, you know from human’s part
to the other one, who was right?

00:34:09.500-->00:34:18.125
Technically, cp30 was right, the
chances of surviving were slim
to none and maybe technically,

00:34:18.125-->00:34:25.292
being caught by Imperial guard
was a better option, was it?
Because humans could recognize

00:34:25.292-->00:34:30.917
that even if technically for the
computer eyes, the chances of
being caught by, by Imperial

00:34:30.917-->00:34:37.625
guard, it’s all for the better,
better odds, that wasn’t an
option at all. So this is very

00:34:37.625-->00:34:43.667
important, that in many cases
again both simple ordinary and
extraordinary ah, highly

00:34:43.667-->00:34:48.708
unusual, so we still have room,
we still have room to move on
and just to, to make all the

00:34:48.708-->00:34:54.667
difference. Um, I’m saying that
human leadership is still
required, and sometimes,

00:34:54.667-->00:35:03.000
sometimes, it, that will mean,
going against ah the computer
recommendations. So, the essence

00:35:03.000-->00:35:07.333
of human leadership is not a
question of knowing the odds,
but a question of knowing what

00:35:07.333-->00:35:13.542
really matters. Not just for the
tomorrow, but for the distant
future. Call it, human guidance,

00:35:13.542-->00:35:21.292
or you may even call it, human
interference. Interference with
our intelligent machines and I

00:35:21.292-->00:35:27.875
believe that will set the course
for this century. Now it,
sometimes it surprises people,

00:35:27.875-->00:35:32.917
that ah, I’m such an optimist
ah, after about ah, intelligent
machines ah, considering my

00:35:32.917-->00:35:39.083
personal experience. But, I, I’m
an optimist, I’m a pure optimist
by nature, I have to say, and I

00:35:39.083-->00:35:44.708
believe that you all too are
optimist about the future of
humans and intelligent machines.

00:35:44.708-->00:35:51.125
Because we should remember our
technology is agnostic, it’s
neither good nor bad and it

00:35:51.125-->00:35:56.833
could be used for good or evil.
The machines will keep getting
smarter and more capable and

00:35:56.833-->00:36:08.083
it’s up to we humans, to do what
only humans can do, dream. And
dream big, so we can get the

00:36:08.083-->00:36:35.583
most out these amazing new
tools. Thank you. [Audience
applauses] Um, I [Inaudible

00:36:35.583-->00:36:43.958
comments] ten minutes, yes
exactly as planned, so. >> Hi,
Hi >> A decent player, yes

00:36:43.958-->00:36:53.000
managing time >> Hi, here, here
>> Gonna ask question now? >>
Here, I have one. Um, so I

00:36:53.000-->00:37:02.875
recently saw a reddit post,
about a composition that is
Stockfish couldn’t solve. Eh, is

00:37:02.875-->00:37:10.917
it possible eh, to create a
machine learning system that at
the next [Inaudible] what

00:37:10.917-->00:37:17.292
problems are more likely to be
solved for a human than any
computer? >> Did you hear the

00:37:17.292-->00:37:23.458
question? >> It’s the, the sound
is somehow, I don’t know why but
the sound is just ah, it’s quite

00:37:23.458-->00:37:29.333
ah, >> In your left, I mean in
your left >> It’s just again at
the Def Con conference and he >>

00:37:29.333-->00:37:36.458
You can ask your questions right
here >> Here, here >> Oh, yeah,
I can hear yes, okay >> Alright,

00:37:36.458-->00:37:43.875
so I saw a composition that
couldn’t be solved for a
Stockfish, so >> Yes, I

00:37:43.875-->00:37:48.292
mentioned Stockfish, but there
are many other problems >> Is
possible to create a machine

00:37:48.292-->00:37:58.250
learning classifier that, eh,
that takes what kind of position
is, easier to play for human or

00:37:58.250-->00:38:06.875
more likely to be played better
for human? >> Um, look it’s the,
it’s the first of all we don’t

00:38:06.875-->00:38:11.833
expect a machine to make a first
move and to announce a mate in
17 555 moves, so, ah and um, I

00:38:11.833-->00:38:20.000
think definitely we can we can
use machines just for, um, for
the best recommendations for

00:38:20.000-->00:38:24.708
specific styles and that’s by
the way what the top players are
doing. They always looking for

00:38:24.708-->00:38:31.833
machines as the, sort of, as
the, as the um, as the guides to
um to help them to get to the

00:38:31.833-->00:38:36.500
positions that they like most.
So, um because again, you have
to just recognize that ah,

00:38:36.500-->00:38:41.625
machine evaluation is in, nine
out of ten cases, is, is, far
superior to, to the humans. >>

00:38:41.625-->00:38:50.375
Alright, thanks >> Hello, ah
would you agree that real, hi >>
Yeah >> Ah, would you agree that

00:38:50.375-->00:38:58.583
real intelligence, requires free
will and free choices that only
humans can make and Deep blue

00:38:58.583-->00:39:04.708
and any computer program is
actually written by people and
when you lose to Deep blue, you

00:39:04.708-->00:39:11.542
don’t lose to machine, you lose
to a programmer, programmers of
those programs. >> Oh >> So, my

00:39:11.542-->00:39:17.708
question is, do you think we are
in any danger of, any kind of
intelligence, until computer can

00:39:17.708-->00:39:25.417
have free will? >> It’s ah,
yeah, we are moving now from
the, ah scientific domain to,

00:39:25.417-->00:39:32.292
to, to philosophy, ah. Ah, as
for Deep blue, is very clear, it
was a product of, of, of a great

00:39:32.292-->00:39:39.625
ward by, by humans. And I, you
know I in, in most of the cases,
we dealing even with AlphaGo and

00:39:39.625-->00:39:45.208
with Demis Hassabis team, it’s
still the result of the work of,
of, um, ah human intelligence.

00:39:45.208-->00:39:51.750
Now, um, whether machines can
have a free will or not, I don’t
know. Um, I used to; I used to

00:39:51.750-->00:39:59.042
believe that anything that we do
while knowing how we do that,
machines will do better. But,

00:39:59.042-->00:40:05.500
there are many things we do that
without knowing how we do that,
without even recognizing why

00:40:05.500-->00:40:09.458
it’s happening. And I don’t
think it will be easy for
machines, if possible at all, to

00:40:09.458-->00:40:16.667
grasp. So, for instance we have
purpose, but we don’t know what
purpose is. So that’s why I

00:40:16.667-->00:40:21.458
think it’s if you’re talking
about free will which is somehow
connected to the purpose. So, I

00:40:21.458-->00:40:28.875
think it’s ah; it might be very,
very distant future for machines
to ah get close to that. >>

00:40:28.875-->00:40:35.875
Thanks >> Over here >> Yeah >>
Um, what are your thoughts on
human characteristics, such as

00:40:35.875-->00:40:40.917
bravery and morality and the
decisions that artificial
intelligence can make, related

00:40:40.917-->00:40:45.958
to the, ah for example the
vehicle choosing to hit a child
or go off a cliff and kill a

00:40:45.958-->00:40:51.375
driver? >> Ah, that’s exactly
the state you may call it
passion because it’s the all

00:40:51.375-->00:40:56.667
different, you know, um human
characteristics that cannot be
quantified, at least easily

00:40:56.667-->00:41:01.375
quantified. And that’s the,
that’s why I use the Hans Solo
example because at the end of

00:41:01.375-->00:41:07.792
the day, when we talking about
bravery it’s, it’s very often
going against the odds. So, I

00:41:07.792-->00:41:12.917
think this, machines by
definition will not be able to
grasp it since, since they are

00:41:12.917-->00:41:18.708
basing, they, they, they based
on, on, on sort of the best,
finding the best patterns and so

00:41:18.708-->00:41:24.667
the best evaluations. And ah
being brave and being passionate
very often, in most of the cases

00:41:24.667-->00:41:32.875
goes against ah the, ah precise
calculation. >> Mr Kasparov, I
have a question a computer would

00:41:32.875-->00:41:41.792
not consider important. Ah,
what’s in your flask and may I
try some? [Laughter] What is

00:41:41.792-->00:41:47.667
contained in your flask? >> I
actually have the stoli that you
pulled out of your pocket; I

00:41:47.667-->00:41:56.542
think that’s what he wants to
know. >> My pocket? Stolichnaya
[Laughter] [Audience claps] >>

00:41:56.542-->00:42:00.833
No that’s not an advertising
[Laughter] you saw, you know, I
just dropped it. [Laughter] >>

00:42:00.833-->00:42:08.292
Who, who will be the next human
world champion? And do you think
the young Chinese player; Wei Yi

00:42:08.292-->00:42:13.667
has a chance to dethrone
Carlsen? >> Um, currently Magnus
Carlsen is, is the number one

00:42:13.667-->00:42:18.458
player, he’s not a world
champion; he’s still the
dominant player. Ah, he is 20;

00:42:18.458-->00:42:25.250
he’ll be turning 27 this year so
he’s still quite young, though
but not very young by the modern

00:42:25.250-->00:42:31.042
standards. I think Wei Li is 18
or 19, so um, I think Magnus
will be facing younger players.

00:42:31.042-->00:42:37.958
There are two young American’s
like ah Wesley So and Fabiano
Caruana ah, and Wei Li,

00:42:37.958-->00:42:42.875
definitely, you know, by
definition makes a potential
challenger. Though it’s again,

00:42:42.875-->00:42:48.417
being a champion requires more
than talent and being young and
energetic. Ah, ah, you know, you

00:42:48.417-->00:42:53.333
need probably the element of
luck, but Wei is definitely in
the category of those who can

00:42:53.333-->00:43:00.250
and most likely will challenge
Magnus Carlsen. >> Thank you >>
You discussed primarily the

00:43:00.250-->00:43:04.083
terministic algorithms or even
basic machine learning when you
were talking about using

00:43:04.083-->00:43:11.042
machines as tools to supplement
our intelligence. However, ah,
what do you say to the, immense

00:43:11.042-->00:43:17.125
amount of resources being
important to, creating a strong
AI or even ah, putting a human

00:43:17.125-->00:43:25.500
brain into a computer. >> Yeah,
but um, again I always have to
confess, you know, my ignorance

00:43:25.500-->00:43:30.125
and sometimes I’m just I’m not
sure I’m in a position to answer
a question, but something that

00:43:30.125-->00:43:36.458
always, you know, was I was
struggling to understand what
the human brains, let’s imagine

00:43:36.458-->00:43:41.375
you can just separated from our
bodies, whether it can function
separately, because I don’t know

00:43:41.375-->00:43:46.792
and this is probably, you guys
know better. So how do brains
functioning outside of the body,

00:43:46.792-->00:43:51.250
whether ,the, the, the fact that
it is moving so it is connected
to our body, makes it, makes it

00:43:51.250-->00:43:55.917
work the way it works, maybe
not. We don’t, we, we, again
this is the kind of experiment

00:43:55.917-->00:44:01.667
that definitely will see maybe
in, in, in the future. But my
view is that, it’s the, it’s,

00:44:01.667-->00:44:08.625
it’s the combination of the
movements and and other human
factors and emotions, create, um

00:44:08.625-->00:44:14.083
the mind that is just, it’s
bigger than just simply, you
know just taking the brain and

00:44:14.083-->00:44:23.000
freezing it and using it as the,
as the ah, um, device full of
neurons. >> Thank you >> Yep >>

00:44:23.000-->00:44:29.292
Hi, sorry, hi, um I was
wondering ah, in light of the
trend of machines eliminating

00:44:29.292-->00:44:33.500
human jobs, what are your
thoughts on the idea of
universal basic income? >>

00:44:33.500-->00:44:40.083
[Laughter] Yeah, it’s just this
again the >> Can you repeat the
question? >> Sorry okay ah, what

00:44:40.083-->00:44:47.458
are your thoughts on the idea of
machines, oh sorry, of universal
basic income? >> Um, no it’s

00:44:47.458-->00:44:52.792
the, I think it’s, it’s a very
important question, because
clearly we are moving at, moving

00:44:52.792-->00:44:59.875
ah, ah, reaching the point where
a lot of people will be just be
left behind. Since, um, it’s

00:44:59.875-->00:45:07.208
kind of a paradox of, of, um, of
the technological progress. On
one side, we have great new

00:45:07.208-->00:45:13.000
technologies, ah that make that,
that that gives huge competitive
advantage to younger people.

00:45:13.000-->00:45:20.083
Just, you know, every new
generation is far more
sophisticated, ah just by

00:45:20.083-->00:45:23.750
dealing with this, with this
devices. On other side, we have
the progress in medicine and the

00:45:23.750-->00:45:26.458
diet that, ah that helps people
do to live longer and just to,
to keep their, just you know

00:45:26.458-->00:45:32.292
ability to work for, for many,
for longer years. So, but
obviously, my generation, the

00:45:32.292-->00:45:37.000
fifties and of course sixties
and even the forties, it’s just
it’s, can, can hardly be

00:45:37.000-->00:45:43.000
competitive with, with the young
kids just moving in. So, ah, we
have to look for this, um, for

00:45:43.000-->00:45:49.167
this um, paradox and for, for
this growing gap. Because we
just have a gap that, that from

00:45:49.167-->00:45:55.000
history we know always led to
big explosions. A gap between
the social infrastructure of

00:45:55.000-->00:46:01.000
society and the technological
ah, ah progress and ah, what you
said is probably, it’s a part of

00:46:01.000-->00:46:05.583
the, part of the solution but
the problem is that the
politicians they are just trying

00:46:05.583-->00:46:11.083
to dump it, you know just to,
to, to, to the next, to the next
elections. Nobody wants to talk

00:46:11.083-->00:46:16.250
about it because it’s painful
because it basically challenges
the very foundation of the, of

00:46:16.250-->00:46:21.083
the sort of modern world order.
It’s much easier to do
[Inaudible] and keep printing

00:46:21.083-->00:46:25.625
money, so thinking that somebody
else will pay. So there are
many, many paradoxes that that

00:46:25.625-->00:46:30.833
make me feel uneasy because for
instance, the piling debt will
have to be paid by younger

00:46:30.833-->00:46:36.000
younger people. But will they be
willing to do it and keeping the
social guarantees for the older

00:46:36.000-->00:46:41.292
generation that made this debt?
I think we are just, you know,
[Audience claps] it’s, no it’s

00:46:41.292-->00:46:46.792
again I, I, I there are more
questions that I,I can ask than,
than, than, than answers that I

00:46:46.792-->00:46:52.125
can produce. Hopefully, I can
help us with that. But at the
end of the day, it’s, it’s, it’s

00:46:52.125-->00:46:59.958
very troubling that the
political class are in the free
world for ,for, for years, if

00:46:59.958-->00:47:03.750
not for decades, is trying to,
to, to ignore the problems that
we’re just discussing now.

00:47:03.750-->00:47:08.667
Because these problems will be,
are already manifesting, they’re
already on this plane and just,

00:47:08.667-->00:47:13.833
you know, ignoring the fact, you
know, we have this, this
technological progress, the, the

00:47:13.833-->00:47:19.500
huge development of, in many
areas, is inevitably changing
our lives. It’s, it’s extremely

00:47:19.500-->00:47:26.458
counterproductive and, and it’s
basically neglecting our future.
Thank you [Applause] >> Thanks

00:47:26.458-->00:47:30.417
>> That is all the time we have
for questions, thank you very
much everybody and thank you Mr

00:47:30.417-->00:00:00.000
Kasparov. [Applause]

