00:00:00.067,00:00:06.707
>> Hello everyone, thank you for
coming at the crack of 11
o’clock in the morning. Okay how

00:00:06.707,00:00:11.712
do I, fullscreen. Why do I have
a thing here? Is that what I
want? No. This is terribly

00:00:15.215,00:00:20.220
embarrassing. There we go, cool
okay.. So after that little
embarrassment my name is

00:00:24.791,00:00:29.596
Catherine. Um, I am here to talk
today about Hacking Your
Thoughts: Batman Forever Meets

00:00:29.596,00:00:36.003
Black Mirror. Um the standard
disclaimer, um the work that is
in this presentation was done

00:00:36.003,00:00:40.140
while I was at the University of
Washington as part of my PhD
dissertation. The results and

00:00:40.140,00:00:44.144
views presented here do not
necessarily represent those of
my funding sources or my current

00:00:44.144,00:00:49.149
employer. So, thanks. Okay so
there’s been a lot of sort of
hype in the media recently

00:00:51.852,00:00:56.290
surrounding brain-computer
interfaces, putting things into
your brain, getting signals out

00:00:56.290,00:01:00.160
so I wanted to take a little bit
of time to sorta separate the
hype from the reality and let

00:01:00.160,00:01:04.064
you know what really is
possible. Um, I’m gonna talk
about some of the experimental

00:01:04.064,00:01:09.903
results from the experiments
that I did, umm and as part of
that I did um ethics and policy

00:01:09.903,00:01:14.308
research so even though my
dissertation was electrical and
computer engineering, um I

00:01:14.308,00:01:19.079
actually did a neuroethics
survey um and I um spent some
time looking at the policy

00:01:19.079,00:01:24.084
proposals that we can come up
with for emerging technologies
like uh brain-computer

00:01:24.084,00:01:30.457
interfaces. Okay, so real quick,
things that are not covered by
this presentation um I am really

00:01:30.457,00:01:36.396
sorry there no aliens involved
at all in this. Um, I’m also
really sorry - I know nothing

00:01:36.396,00:01:40.400
about any chips that the
government might have implanted
in your brain. Um, I used

00:01:40.400,00:01:47.007
non-invasive stuff so that is
outside the purview of my
research. Before we get started,

00:01:47.007,00:01:50.610
um, we’re gonna just baseline
the definition for
brain-computer interface. Um

00:01:50.610,00:01:54.715
this is the definition that I
used in my neuroethics survey
that we’re gonna get to in the

00:01:54.715,00:02:01.455
second half of the presentation.
Um I defined a BCI as um it can
record brain activity uh while

00:02:01.455,00:02:05.659
an individual is performing
different actions for example
blinking their eyes, playing a

00:02:05.659,00:02:10.797
video game, or texting on a
phone. BCIs are often used to
give user control of a computer

00:02:10.797,00:02:16.069
using their brain activity. This
could be anything from playing a
video game to actually

00:02:16.069,00:02:21.308
controlling like a prosthetic
arm or a wheelchair, so. I’m
also going to be talking about

00:02:21.308,00:02:26.046
target elicitation, emphasis on
targeted and elicitation. So,
we’re showing specific stimuli

00:02:26.046,00:02:31.852
in order to obtain a particular
response. This is not writ
large, I’m taking everything

00:02:31.852,00:02:38.458
from your mind. So, right off
the bat, what do you think of
when you hear brain hacking. Um,

00:02:38.458,00:02:43.463
for some of us of a certain age,
you might go immediately to my
favorite movie from 1995, Batman

00:02:45.499,00:02:49.603
Forever. Um, I forget what
streaming services it’s on but
you should totally go check it

00:02:49.603,00:02:54.608
out, probably while drinking if
that is your -your thing. Um for
those who haven't seen the

00:02:54.608,00:02:59.279
movie, one of the plots is
Edwards Nygma aka The Riddler
wants to find out who Batman

00:02:59.279,00:03:04.885
really is so he creates a device
that basically sucks up all the
brain waves of everyone in

00:03:04.885,00:03:09.890
Gotham um and then figures out
who has bats on their brain and
that’s obviously Batman. So the

00:03:09.890,00:03:14.361
uh end of the movie, um they’re
having the showdown and -and
Batman says, “You’ve sucked

00:03:14.361,00:03:18.498
Gothaman’s brain waves and
devised a way to read men’s
minds.” and the Riddler says,

00:03:18.498,00:03:22.869
“You betcha! Soon my little box
will be on countless TV’s around
the world feeding me credit card

00:03:22.869,00:03:28.108
numbers, bank codes, sexual
fantasies, and little white
lies.” So this is in 1995 that

00:03:28.108,00:03:33.580
we already had this concept of
taking information that could be
useful either for ya know

00:03:33.580,00:03:38.251
stealing all of someone’s money
or using it to blackmail them.
Um, for those of you who have

00:03:38.251,00:03:42.456
not seen this lovely movie, you
may be familiar with Black
Mirror. This is the episode,

00:03:42.456,00:03:47.394
Crocodile. Um, sorry spoilers,
it’s been out for a while, hope
you’ve already seen it. But

00:03:47.394,00:03:52.265
basically in this futuristic
Iceland, it is required by law
that you give up your neural

00:03:52.265,00:03:57.237
information as part of
investigations and so this woman
um saw a man get hit by an

00:03:57.237,00:04:02.776
autonomous pizza delivery truck
and the insurance agent is um
compelling her by law to give

00:04:02.776,00:04:06.813
this information. Um,
unfortunately for the insurance
agent, this woman also killed

00:04:06.813,00:04:11.585
someone in her hotel room um and
you can all guess where that
goes from there. So, this is in

00:04:11.585,00:04:16.690
a future where you have to give
up your brain signals which kind
of leads to this question of, do

00:04:16.690,00:04:23.130
we really want to have to do
that? So, based on those two
examples, you might think that

00:04:23.130,00:04:28.235
oh we’re going to be able to
pull things out immediately, in
kind of like the Penseive in

00:04:28.235,00:04:31.638
Harry Potter, just look at them.
But not all that’s really
possible and so two things have

00:04:31.638,00:04:36.443
happened recently um and I wanna
sorta explain them for you. So
some of you may have seen Elon

00:04:36.443,00:04:41.281
Musk’s presentation about
Neuralink and they have this
great little chip that has

00:04:41.281,00:04:47.854
thousands of little electrodes
that they’re gonna put into your
skull using this great little uh

00:04:47.854,00:04:52.359
um sewing machine essentially
that by the way was developed by
DARPA at UCSF. Um and Elon’s

00:04:52.359,00:04:57.764
goal is to have someone who is
not a neurosurgeon just use a
laser to drill a hole in your

00:04:57.764,00:05:03.270
brain, they’ll pop this in and
you’re good to go. So my
problems with that, first of all

00:05:03.270,00:05:07.240
a lot of what they’re talking
about deals with reanimation of
limbs so, spinal cord injury,

00:05:07.240,00:05:11.378
prosthetic arm type deal. It’s
great if you can get the
information out to “I want to

00:05:11.378,00:05:17.150
move my arm.” But what happens
is there’s nothing coming back
saying um this is what the arm

00:05:17.150,00:05:21.955
is, or where it is in space,
this is what I’m feeling, this
is how much weight I’m carrying.

00:05:21.955,00:05:26.693
So without that feedback loop,
it’s not actually that helpful.
Um, a lot of the things they’re

00:05:26.693,00:05:30.564
talking about for the
augmentation, things like memory
and things like that, it’s

00:05:30.564,00:05:34.401
unclear how the current
implantation method they’re
talking about is actually gonna

00:05:34.401,00:05:38.705
get to those deep branch
structures. Um, right now if
you’re looking at things um for

00:05:38.705,00:05:42.409
like for the hypothalamus you’re
looking at very very long
electrodes that you stick in the

00:05:42.409,00:05:47.180
brain and by the way once you
put them in, the brain says we
have a foreign invader and scars

00:05:47.180,00:05:52.552
around it so lessons that um
effectivity. Um, one of the
other things that Elon’s talking

00:05:52.552,00:05:57.524
about is actually using
electricity to stimulate the
brain which is great except for

00:05:57.524,00:06:02.562
the fact that we have a very
extensive literature from deep
brain stimulators that show that

00:06:02.562,00:06:07.934
putting electricity in can cause
side effects like profound
behavioral changes, um changes

00:06:07.934,00:06:12.772
in sexual preferences and
behavior um, compulsive gambling
and spending and so if you’re

00:06:12.772,00:06:16.409
just gonna be putting one of
these in your brain and turning
the electricity on, you may

00:06:16.409,00:06:20.580
wanna know that it’s gonna do a
lot of really crazy things. So
that’s an area of research that

00:06:20.580,00:06:25.151
really should be explored more
before you just go down to your
local ya know Radio Shack 2

00:06:25.151,00:06:29.589
point 0 to get one of these put
in. So, the next one is
Facebook. This one got a little

00:06:29.589,00:06:35.595
less press because they released
a peer reviewed article in
Nature Communications. So two

00:06:35.595,00:06:39.733
years ago, they said they were
gonna do typing by brain and it
was gonna be 100 words a minute

00:06:39.733,00:06:45.672
from your brain to Facebook. Um,
right now the gold standard for
typing by brain if you’re gonna

00:06:45.672,00:06:49.809
be reading someone’s neural
signals non-invasively, so just
a cap on their head it's about

00:06:49.809,00:06:54.414
one word a minute. I think
Stanford can do eight words a
minute. Um, also the study that

00:06:54.414,00:06:58.118
they came out with, which if you
were interested, you’re more
than welcome to read, they only

00:06:58.118,00:07:02.255
did it with three subjects and
actually they did it very
invasively so I'm gonna warn you

00:07:02.255,00:07:06.459
now in a couple slides I’m gonna
show a picture of brain surgery.
I will tell you when, feel free

00:07:06.459,00:07:09.062
to close your eyes, but I’m
gonna show you what they
actually did to get this

00:07:09.062,00:07:13.566
information out. And of course
everybody knows Facebook. Do you
really want Facebook to have

00:07:13.566,00:07:18.471
direct access to all of your
neural signals particularly when
they know what you were looking

00:07:18.471,00:07:23.376
at when you were using it so
just little -little though in
your back of your mind. Okay, so

00:07:23.376,00:07:26.980
the way that the people uh, the
way that they did the
experimentation in this is that

00:07:26.980,00:07:30.917
they used something called
electrocorticography of ECoG and
that is for patients who have

00:07:30.917,00:07:35.322
intractable epilepsy, they don’t
know where the -the locus of the
seizure is coming from so they

00:07:35.322,00:07:39.693
bring ‘em to the hospital, they
take off the -the skull and they
put electrodes on the surface

00:07:39.693,00:07:43.396
and they let them sit in the
hospital for two weeks and they
have seizures and they can find

00:07:43.396,00:07:46.232
out where those seizures are
coming from. While they’re
sitting in the hospital, they’re

00:07:46.232,00:07:52.405
bored out of their gourd
literally, um ahh, that wasn’t
as funny as it was supposed to

00:07:52.405,00:07:56.810
be um [laughing]. And so
researchers come in and you can
do really cool experiments, so.

00:07:56.810,00:08:00.980
This is what the grid looks like
so it’s ya know, they have
different sizes, they have

00:08:00.980,00:08:06.152
different numbers based on where
you want to put them, and
hemispheres and coverage, okay.

00:08:06.152,00:08:10.357
The gory picture’s coming up
next. If you do not like
surgery, or gore or blood,

00:08:10.357,00:08:14.361
please close your eyes. I’ll let
you know when you can look
again. Okay here we go. This is

00:08:14.361,00:08:20.266
what they’re actually doing. So
this is um electrocorticography.
This is the subject from the

00:08:20.266,00:08:26.573
Facebook experiment. This is
kind of what Elon is trying to
do except smaller, um so if you

00:08:26.573,00:08:30.043
just look at this and think
about do you really want someone
who doesn’t have a medical

00:08:30.043,00:08:35.582
degree doing this to your brain,
yeah. Little -little thought for
mind. Okay, gory picture’s gone,

00:08:35.582,00:08:39.686
you can open your eyes now. So
what is currently feasible now
that I’ve scared the living crap

00:08:39.686,00:08:43.690
outta you. Um what I ended up
doing is I used
electroencephalography or EEG,

00:08:43.690,00:08:49.362
this is no surgery needed cause
as it turns out I am not a
neurosurgeon, I can't put uh

00:08:49.362,00:08:54.334
electrodes into my head. Um as
you can see, I take goofy
pictures, I actually have lots

00:08:54.334,00:08:58.972
of these of me wearing EG caps
cause I think they’re cool. Um,
this is the setup that I used.

00:08:58.972,00:09:03.676
It’s a Brain Vision brain
products cap and what I was
looking for was I was looking

00:09:03.676,00:09:09.549
fro events related responses to
specific stimuli that I was
showing. So on your right hand

00:09:09.549,00:09:14.554
side of the screen, you can see
sorta this family of um brain
wave patterns, and so they’re of

00:09:16.990,00:09:21.161
variant related potentials and
what happens is they come in
response to different stimuli so

00:09:21.161,00:09:25.231
ERN is um Error Related
Negativity so if you make a
mistake, your brain actually

00:09:25.231,00:09:28.935
creates that so you know you
made the mistake; there’s one
for spelling errors, there’s one

00:09:28.935,00:09:33.640
for grammatical errors, and the
particular one I’m interest is
called P300. That’s a positive

00:09:33.640,00:09:39.913
peak 300 milliseconds after the
stimuli has shown to you. And
the best way to explain this is

00:09:39.913,00:09:43.583
to tell you about the
experimental paradigm that a lot
of people use to test this out.

00:09:43.583,00:09:47.787
So this is called the Guilty
Knowledge Test. Um and the P300
is called the Oddball Response

00:09:47.787,00:09:54.394
because it reoccur it occurs in
um uh in response to things that
are different than the things

00:09:54.394,00:09:58.698
around it. So the way this
usually goes in experimental
literature is you’ll have a

00:09:58.698,00:10:03.369
subject come in the room and
there'll be six pictures usually
of jewelery or something and

00:10:03.369,00:10:08.508
you'll be asked to steal one of
them. So either put it in a
drawer put it in your pocket,

00:10:08.508,00:10:13.113
put it ya know somewhere else
and then they sit you down with
an EG cap on your head and they

00:10:13.113,00:10:17.817
start showing you pictures of
the things that you could have
taken and they record your

00:10:17.817,00:10:23.423
neural signals to see if they
can figure out which one elicits
the response of this is the one

00:10:23.423,00:10:29.462
that you took. And low and
behold, it was the watch, you
have been caught. Um, obviously

00:10:29.462,00:10:33.399
it’s not quite this drastic. You
have to do this over and over
again but this is the general

00:10:33.399,00:10:38.404
idea of you have a family, a -a
set of stimuli and you’re hoping
that one of that set is the

00:10:41.241,00:10:46.646
target so that you can elicit
this response and then you can
actually use that information.

00:10:46.646,00:10:52.218
So what I ended up doing is I
did a single-digit guessing game
and I did this because I wanted

00:10:52.218,00:10:57.790
to go back to basics from the
literature so if you look at the
prior literature and there are a

00:10:57.790,00:11:01.060
couple papers specifically about
elicitation of private
information, they tend to use

00:11:01.060,00:11:05.131
overt or conscious so you know
what you’re looking at stimuli
or subliminal, technically

00:11:05.131,00:11:10.203
unconscious but most people can
actually see what it is because
monitors being the way they are,

00:11:10.203,00:11:14.974
they don't refresh fast enough
et cetera, et cetera. And they
all relied on experimental

00:11:14.974,00:11:20.813
training data so what happens is
you come in and the experimenter
show you a series of stimuli and

00:11:20.813,00:11:25.251
they know which one you’re going
to have a response to and they
can use that set then to match

00:11:25.251,00:11:30.390
the test data which they know
what they’re looking at. I was
looking at completely untrained

00:11:30.390,00:11:34.961
data. So you come into my lab, I
put the cap on you, I start
showing you stimuli and then I

00:11:34.961,00:11:40.366
try to figure out the number.
And so this is the picture of
again, me wearing a lovely cap

00:11:40.366,00:11:45.038
ya know in the -in the lab and
basically what I did was I had
subjects pick a number, and then

00:11:45.038,00:11:48.074
I told them they were going to
stare at a dot on the screen and
then numbers would flash around

00:11:48.074,00:11:53.146
it. You can see the timeline of
the stimuli on the screen. And
the only thing I told them after

00:11:53.146,00:11:57.750
they selected the number was
that they would have to put the
number in again at the end of

00:11:57.750,00:12:01.087
the experiment. So I didn’t
actually tell them that they
were supposed to think about it

00:12:01.087,00:12:04.290
or what they were supposed to do
with that number, I just said
pick a number and at the end,

00:12:04.290,00:12:09.696
you’re gonna tell me what that
number is again. And I ended up
with three kinds of results, um

00:12:09.696,00:12:13.233
one of them about the overall
effectiveness in identifying
subject’s chosen digit, the

00:12:13.233,00:12:17.870
effect of attention on
identifying the subject’s digit,
and then determining the curture

00:12:17.870,00:12:22.842
-current versus future digit
information so actually getting
it intention here. So for the

00:12:22.842,00:12:26.813
first one, for all but one of
the subjects that I had, the
computer correctly calculated

00:12:26.813,00:12:31.050
the correct digit that they were
thinking for two to three times
out of ten sessions that we were

00:12:31.050,00:12:34.887
doing so this is an example of
one of those sessions. The
computer got it right three

00:12:34.887,00:12:39.158
times out of ten. And you may be
thinking, well 20 to 30 percent
when chances 10 percent, that’s

00:12:39.158,00:12:44.964
not great, however, this is with
zero training data and actually
fairly simple signal processing

00:12:44.964,00:12:49.669
techniques. And if you look at
the rest of the literature, it’s
actually not that much worse

00:12:49.669,00:12:53.806
compared to even things with
training data sets and I want to
say the one paper that did have

00:12:53.806,00:12:58.845
untrained data and it, they said
it was quote, five to ten times
harder to calculate with

00:12:58.845,00:13:02.915
untrained data versus trained
data. So it’s -it’s okay; you
can -you can actually do this

00:13:02.915,00:13:06.619
where if you show enough stimuli
over and over again, you can
increase that confidence

00:13:06.619,00:13:11.724
interval, you just have to have
more time and more data. The
effect of attention. So in my

00:13:11.724,00:13:15.528
experiments, the subjects
literally sat there and stared
at dots on a screen for five

00:13:15.528,00:13:20.800
minutes at a time and it turns
out people get really tired and
start falling asleep. So what I

00:13:20.800,00:13:26.973
ended up doing is I had subjects
in umm counterbalanced um
sessions press this space bar

00:13:26.973,00:13:31.444
when they saw their digit. And
so that let -meant that I had
some sessions where they were

00:13:31.444,00:13:35.615
paying attention and others
where they were more passive.
And so percentage wise, the

00:13:35.615,00:13:39.118
correct digit was calculated
more often for spacebar rounds
when they were paying more

00:13:39.118,00:13:43.322
attention than non-spacebar
rounds which is when they were
passive. And this actually holds

00:13:43.322,00:13:47.393
up with the literature and it
-another experiment where they
were having people um count the

00:13:47.393,00:13:50.930
number of times a region on a
map where they live show up and
that one they actually also got

00:13:50.930,00:13:55.768
a higher percentage of accuracy.
So, this is great. The third
one, which I think is super

00:13:55.768,00:14:00.506
cool, is determining current
versus future intent. So like I
said before, I didn’t actually

00:14:00.506,00:14:05.478
tell subjects how they were
supposed to maintain that digit
in their head. And it turns out

00:14:05.478,00:14:09.782
for some, the number the subject
was going to pick in the
following round was calculated

00:14:09.782,00:14:13.619
almost as many times as the
number for the current round but
this was not consistent. So as

00:14:13.619,00:14:19.358
before it was 20, 30 percent
across all subjects, but here
you had some subjects for whom

00:14:19.358,00:14:24.597
you only got ya know 20 percent
right and no future guesses, and
you had some subjects for whom

00:14:24.597,00:14:30.503
you got more of the future digit
correct than the current digit.
So, this is super cool; it may

00:14:30.503,00:14:34.107
have something to do with how
they were thinking about the
experiment. Maybe they were just

00:14:34.107,00:14:37.643
thinking about the future digits
cause they wanted to get done
and stop staring at a screen.

00:14:37.643,00:14:43.382
Um, but if anyone wants to do a
PhD dissertation, I have a great
research topic for you and I can

00:14:43.382,00:14:49.722
tell you which lab to talk to so
let me know. Okay, so that’s all
fine and dandy, we can extract

00:14:49.722,00:14:53.726
information. It may not be 100
percent, it probably will never
get to 100 percent but we have

00:14:53.726,00:14:57.997
that technology. So can we then
ask the questions, what do
consumers think about

00:14:57.997,00:15:03.770
neuroprivacy? So if you lived in
Gotham City and Edward Nygma’s
device came out, what would you

00:15:03.770,00:15:08.141
actually think about the fact
that you -you know information
was being put into your head and

00:15:08.141,00:15:13.880
then taken out. So what I talked
about in their scenario is
what’s being protected. I am

00:15:13.880,00:15:18.384
interested in the quantifiable
information that is determined
from the combination of the

00:15:18.384,00:15:22.855
electrical signals from the
brain along with the relevant
environmental stimuli so what

00:15:22.855,00:15:27.693
you were looking at and we’ve
looked at it. The original broad
neural signals without context

00:15:27.693,00:15:31.664
are much less informative. Um,
there are some studies where
they're looking at determining

00:15:31.664,00:15:37.136
things like Parkinsons and
Alzheiers using just raw traces
of neural signals but for the

00:15:37.136,00:15:43.109
actual information, you need to
know the context in which it was
generated. So, um we’re talking

00:15:43.109,00:15:46.612
about definitions about neural
privacy. This is not the first
time that people have thought

00:15:46.612,00:15:51.017
about privacy. So for those of
you who are law nerds in the
audience, if you go back to

00:15:51.017,00:15:56.022
1890, this is the very famous
Right to Privacy um by Warner
Brandeis and they wrote this

00:15:58.357,00:16:04.063
article in response to this
crazy new technology called
photography and how it was going

00:16:04.063,00:16:08.267
to be ya know invading peoples’
lives, and ya know things were
going to be put in -in

00:16:08.267,00:16:12.672
permanence and there’s all these
papers, and so they basically
were like, we need to declare a

00:16:12.672,00:16:18.077
right to privacy now. Now in
2019, we’re still having that
conversation about privacy. What

00:16:18.077,00:16:21.414
I’m saying is we also need to
extrapolate it to emerging
technology so let’s talk about

00:16:21.414,00:16:26.185
neura privacy specifically. And
so the four issues that I
considered in defining neural

00:16:26.185,00:16:31.190
privacy, um is privacy a right
or an interest? So in legal
terms, a right is something

00:16:33.259,00:16:38.664
where if you’re harmed, you can
actually get some sort of
compensation or um -um

00:16:38.664,00:16:42.435
reimbursement for that. An
interest is just, “Yeah, I’d
really like to not have this

00:16:42.435,00:16:47.740
happen to me but if it does
happen to be I have nothing to
do about it.” So can we actually

00:16:47.740,00:16:51.143
come up sort of a legal
structure whereas if something
happens to your neural signals

00:16:51.143,00:16:56.282
you can do something about it?
Do we own our own thoughts. So I
love this question because

00:16:56.282,00:17:00.853
everyone has their different
ideas about who our thoughts are
and how they make us a person

00:17:00.853,00:17:05.858
and are our thoughts our person
or are our thoughts just inside
of us? Um but what happens when

00:17:05.858,00:17:11.631
they are extrapolated. So if you
are um playing a video game with
your brain and they start

00:17:11.631,00:17:16.269
showing you different pictures
of coffee logos and they
determine that you like

00:17:16.269,00:17:21.707
Starbucks, do they own the fact
you own -you like Starbucks now.
They’re obviously probably going

00:17:21.707,00:17:27.480
to monetize it and try to send
you targeted ads about Starbucks
but who owns that information.

00:17:27.480,00:17:31.918
Cause that’s a great question to
ask? What is the relationship we
have with those who elicit

00:17:31.918,00:17:36.689
information neurally. So, um
there’s a lot of questions about
relationships we have with the

00:17:36.689,00:17:41.994
um data aggregators and social
media. Is it uh fiduciary,
information fiduciary

00:17:41.994,00:17:47.233
relationship, is it a uh
parasitic one, is it a symbotic
one -symbiotic one um there’s a

00:17:47.233,00:17:51.971
great philosophical um
discussion that I get into in
the next bit. And then the

00:17:51.971,00:17:56.676
importance of trust. So, do you
actually trust when you hand
over your neural information

00:17:56.676,00:18:00.313
that they’re gonna do what they
say they’re going to do with it.
And that’s something we’re going

00:18:00.313,00:18:04.984
to talk about from the
neuroethics study. So I took out
a bunch of slides because y’all

00:18:04.984,00:18:09.755
don't need to sit here and
listen to my dissertation uh
chapter about philosophy. But

00:18:09.755,00:18:13.726
what it boils down to is, we
should all have an interest in
protecting our neural privacy

00:18:13.726,00:18:18.998
but we do need additional legal
frameworks to make it a right.
So, congresspeople, I heard

00:18:18.998,00:18:22.601
there were congressional
staffers here, take note. Um,
defining and ascribing ownership

00:18:22.601,00:18:29.275
is necessary to provide value to
what is being elicited. So this
is the case of, “Yeah maybe

00:18:29.275,00:18:35.147
eliciting that you are a fan of
Starbucks over Pete’s Coffee,”
may mean more than you turned

00:18:35.147,00:18:40.486
right into a video game.” So how
can we actually ascribe a value
to that information if we do

00:18:40.486,00:18:44.757
want to come up with some sorta
economy where you are actually
allowing these thoughts to be

00:18:44.757,00:18:49.595
elicited? Users should be able
to trust that the information
taken from elicited neural

00:18:49.595,00:18:53.866
signals by a company will be
used and interpreted properly,
making the relationship between

00:18:53.866,00:18:59.105
user and company an intimate
one. So um the concept of intim
-intimacy and privacy is talked

00:18:59.105,00:19:03.542
about a lot by Julie Inis. Um
she actually has a great book
about this if you like

00:19:03.542,00:19:07.613
philosophy or just like reading.
Um and she talks about an
intimate relationship where you

00:19:07.613,00:19:11.150
actually have an understanding
and so that, I like the
framework for talking about

00:19:11.150,00:19:16.856
this. And do to test out some of
these questions, I actually did
a neuroethics survey and so I

00:19:16.856,00:19:21.861
put this out online, some may
have even seen this on Twitter.
Um, but I got 77 respondents in

00:19:21.861,00:19:26.465
about 24 days in the beginning
of the year and in it I had four
questions, I’ll go over three

00:19:26.465,00:19:31.270
here in um the last one in the
later half of the talk. So I
basically asked, “Is there a

00:19:31.270,00:19:35.741
difference in perceived privacy
violation between a person
intercepting BCI information

00:19:35.741,00:19:41.914
versus um a phone or an app?” so
something that is not a person.
Um, “What are the differences in

00:19:41.914,00:19:46.352
trust and willingness to share
neural information with a range
of entities?” And, “Is neural

00:19:46.352,00:19:50.523
information more important than
other data that’s already
available about us?” So, things

00:19:50.523,00:19:55.594
like your FitBit or maybe your
online shopping history. One of
the things that I did do in this

00:19:55.594,00:20:00.566
survey is; I asked about
mobility status and the reason I
asked about this is that someone

00:20:00.566,00:20:07.206
who uses a wheelchair or cane or
may not be able to move um about
like someone else in the world;

00:20:07.206,00:20:12.311
they may have a different
relationship with um privacy and
trust in that they have to have

00:20:12.311,00:20:15.915
a home maid come in and help
them use the bathroom or they
can’t reach the top shelf so

00:20:15.915,00:20:19.719
they’re always asking someone
for it and I wanted to see if we
could suss out that

00:20:19.719,00:20:24.990
relationship. So, question one,
“Who or what is taking your
information?” So the scenarios

00:20:24.990,00:20:29.762
is you’re sitting on a bus,
you’re using a BCI to control
your phone and some malicious

00:20:29.762,00:20:34.400
hacker is sitting behind you and
is able to intercept those
signals, Meanwhile, um in a

00:20:34.400,00:20:37.903
different alternative timeline,
you have an app on your phone.
Um, I was ambiguous as to

00:20:37.903,00:20:42.741
whether or not you installed it
or you knew that it was doing it
all I said was there was an app

00:20:42.741,00:20:47.546
on the phone and it was also
doing the same thing. And so I
asked in a sorta a stairstep set

00:20:47.546,00:20:52.551
of questions. What is your um
-um level of um, violation of
privacy based on scenarios . So

00:20:54.820,00:20:59.425
if it’s just the contents, if
they’re able to take video of
you typing it, if they have

00:20:59.425,00:21:03.729
access to the current brain
activity that your doing when
you type it out, if they have

00:21:03.729,00:21:09.235
access to you planning to type
out, so getting back to that
experimental part of future

00:21:09.235,00:21:14.039
intent and then if they have
access to the emotional content
of that message. Uh, two

00:21:14.039,00:21:18.778
takeaways from this one, the
personal procurement, so the
-the person sitting behind the

00:21:18.778,00:21:22.948
bus of neural planning
information so the future intent
is a statistically significant

00:21:22.948,00:21:26.919
privacy violation over the app.
That was the only one that I
found to be statistically

00:21:26.919,00:21:30.623
significant. So that’s kind of
strange, ya know, you -you don’t
want someone to know what you’re

00:21:30.623,00:21:36.529
doing. Um but also it gets to
the, we tend to be okay with
giving information on our phones

00:21:36.529,00:21:40.866
even though the phones may have
far ya know worse implications
about it going back to the Face

00:21:40.866,00:21:44.603
app thing of aging everyone.
Everyone was totally okay with
putting their face on there

00:21:44.603,00:21:50.242
until everyone was like, “Wait”
are they Russian? Um, sorta
thinking about that. Um, I also

00:21:50.242,00:21:55.147
found that across all five
categories, mobility status did
not statistically impact

00:21:55.147,00:22:00.052
perceptions of neuroprivacy. So
um, I have more analysis; it
gets a little more nuanced when

00:22:00.052,00:22:05.057
you look at the -each of the
individual scenarios. But um
overall didn’t have an impact so

00:22:05.057,00:22:11.263
that was interesting. So the
next question, if you’re using a
BCI and it has the ability to

00:22:11.263,00:22:14.733
find out what foods you like,
what your physical and mental
states is, who you’re attracted

00:22:14.733,00:22:19.305
to, or maybe your political
views, are you willing to and do
you trust giving this

00:22:19.305,00:22:23.642
information to six different
entities? So I started with a
family member, a physician,

00:22:23.642,00:22:28.614
university researcher, a
government entity, a nonprofit,
and a for-profit. So lovely

00:22:28.614,00:22:34.620
chart from R if you’re familiar
with R. Um, you can basically
see that family members, medical

00:22:34.620,00:22:38.524
professionals, and university
researchers, you’re okay, trust
and willing, you’re -you’re

00:22:38.524,00:22:42.094
willing to go there. But you get
to government, nonprofit and
for-profit and you’re like no no

00:22:42.094,00:22:47.366
no, untrustworthy, not -not
going there. And it’s
interesting because I feel like

00:22:47.366,00:22:53.372
the thing I left out of this is
the feedback loop. So why were
you giving it? So maybe if um

00:22:53.372,00:22:59.345
you needed to um, you wanted to
donate something to a non-profit
like you wanna donate your brain

00:22:59.345,00:23:04.650
signals for an EG repository,
maybe you’d be more willing to
do that or ya know if you know

00:23:04.650,00:23:09.288
that in um medical profession,
you have HIPAA protecting you or
university reacher - researcher

00:23:09.288,00:23:13.392
has to go through an approval
process through a review board,
then you might increase it so

00:23:13.392,00:23:16.996
there’s a lot of variables that
need to look at here but I
really like this because I can

00:23:16.996,00:23:20.899
go to a for-profit company and
say look, people really don’t
trust and aren’t willing to give

00:23:20.899,00:23:26.205
you this information, you should
do something about that.
Finally, what’s more important?

00:23:26.205,00:23:31.210
So I asked, is your neural
information more, equally, or
less important than Fitbit or

00:23:31.210,00:23:35.714
similar exercise tracker, uh the
record of your personal medical
history at your doctor’s office,

00:23:35.714,00:23:40.486
generic information um like 23
and Me, your online shopping
history, monthly credit card

00:23:40.486,00:23:45.724
statement, and a journal or a
diary. And so there are the
results here. Um, what’s really

00:23:45.724,00:23:49.528
interesting and there are
definitely people who think that
their neural information is less

00:23:49.528,00:23:53.565
important than things like their
online shopping history. I would
really love to meet these people

00:23:53.565,00:23:58.804
to find out what they’re buying
online. Um but [laughing] um,
but you can kind of see things

00:23:58.804,00:24:03.842
that may or may not have more
like bodily salience to the
medical records information,

00:24:03.842,00:24:09.281
journal diary ya know directly
projecting your -your thoughts
and feelings onto a page ya

00:24:09.281,00:24:12.618
know, they're -they’re about
equivalent but you get to
exercise tracker and credit card

00:24:12.618,00:24:16.955
and so it’s like, ehh so that’s
-that’s a level of abstraction
of what you’re thinking of so

00:24:16.955,00:24:23.562
cool result. Okay, so what are
the potential policy and
regulatory implications. So

00:24:23.562,00:24:27.766
people obviously have thoughts
and feelings about this. We’ve
shown that it is possible. Is

00:24:27.766,00:24:31.937
there anything that we can
actually do about it? And so
this gets the ya know back to

00:24:31.937,00:24:36.809
the Batman Forever um analogy.
If Harvey Dent hadn’t turned
into Two-Face, could there have

00:24:36.809,00:24:41.480
been a law in Gotham City that
would’ve allowed them to go off
and prosecute him. Or at the

00:24:41.480,00:24:46.151
beginning of the movie when
Edward Nygma was proposing this,
Brue Wayne and um Ngyma’s boss

00:24:46.151,00:24:51.724
could’ve been like, oh according
to the FDA or ya know whatever
government agency, you can’t do

00:24:51.724,00:24:56.862
this because of XYZ regulations.
So, let’s talk about that. So
there are some existing

00:24:56.862,00:25:02.101
biometric precedents, where
they’re either protecting or
profiting off you. Um, the 2008

00:25:02.101,00:25:07.172
Genetic Information
Non-discrimination Act um is um
implimented in -by Congress um

00:25:07.172,00:25:14.012
and this allowed people to seek
out um genetic um sequencing and
then not be disciminated against

00:25:14.012,00:25:18.984
it. So the -the words. The way
that they were calling this out
in the bills, they were

00:25:18.984,00:25:22.755
specifically talking about
sickle cell anemia so there's a
particular um affliction that

00:25:22.755,00:25:26.859
only happens to a certain part
of the population and they
should not be discriminated

00:25:26.859,00:25:30.896
against for going out and
seeking treatment for it. So
that’s all fine and dandy. Um,

00:25:30.896,00:25:35.801
there also it ties into the
Affordable Care Act where you
can’t technically discriminate

00:25:35.801,00:25:40.606
it against um for that genetic
information but that’s because
we have the preexisting cause so

00:25:40.606,00:25:45.344
if we get the preexisting
condition you can’t technically
be discriminated against, so go

00:25:45.344,00:25:48.747
for covering preexisting
conditions. Um, the life
insurance one is interesting.

00:25:48.747,00:25:55.120
Um, in uh this starting this
year, I believe it’s John
Hancock um will only provide

00:25:55.120,00:26:01.460
life insurance if you do active
tracking so you have to wear a
tracker or a like a FitBit or

00:26:01.460,00:26:06.365
something or you have to fill
out a survey. They will no
longer just let you sign up or

00:26:06.365,00:26:10.068
fill out a questionnaire; they
actually have to be monitoring
you at all times to make sure

00:26:10.068,00:26:15.073
they put you in the right life
insurance bucket. Um, in the
state of New York, you are also

00:26:15.073,00:26:19.478
um, companies are also allowed
to follow your social media
feeds to figure out how they’re

00:26:19.478,00:26:23.515
going to set your life insurance
rates. And I’m really annoyed
because I can’t find the notes

00:26:23.515,00:26:27.586
um in this particular setup but
the Wall Street Journal actually
published things that you should

00:26:27.586,00:26:32.691
and shouldn’t do for your
insurance rates in New York and
one of them was like um, you

00:26:32.691,00:26:37.763
should frequent um gyms but
leave your phone at home when
you go to the bar or do

00:26:37.763,00:26:42.801
activities like running but if
you go skydiving ya know, that’s
a bit more risky. So they’re

00:26:42.801,00:26:47.706
literally telling you how you
should and shouldn’t act because
otherwise your life insurance is

00:26:47.706,00:26:52.578
gonna change. [Booing in the
audience] There ya go, yes boo,
very boo. If you live in New

00:26:52.578,00:26:58.150
York, talk to your -your state
legislatures. Um, so the final
one that’s really interesting um

00:26:58.150,00:27:03.255
is a state case called Rosenbach
versus Six Flags. So this is a
Illinois State Supreme Court

00:27:03.255,00:27:07.359
Case. Um the state of Illinois
if you live in Illinois, good on
you. Um they actually have one

00:27:07.359,00:27:11.697
of the strongest biometric um
protection laws in the country.
Um, unfortunately there’s not

00:27:11.697,00:27:17.870
much competition cause the only
two other states that I know of
are Oregon or uh Washington and

00:27:17.870,00:27:22.941
Texas um, but they basically
said that um, in this statues,
you are required to get consent

00:27:22.941,00:27:26.879
to take any form of biometric
information that includes
fingerprints um and you also

00:27:26.879,00:27:32.651
have to have a written um
documentation of what you’re
doing with the information and

00:27:32.651,00:27:38.524
how long you keep it. And so in
this case, a mom signed her son
up for season psses to Six Flags

00:27:38.524,00:27:44.329
and that she said fill out
paperwork and when you get there
and bring back the pass. The kid

00:27:44.329,00:27:46.331
comes back and she’s like,
where’s the pass and the kid
said, oh they just took my

00:27:46.331,00:27:50.168
fingerprint because it turns out
that the Six Flags in Illinois
um they didn’t have passes, it

00:27:50.168,00:27:55.140
was a biometric ticket into the
park. And the mother said, not
only did I not consent to having

00:27:55.140,00:28:00.212
my son give his fingerprint uh,
I don’t know what they’re going
to do with it. And so they went

00:28:00.212,00:28:05.551
through and the State Supreme
Court said that it was a harm
that Six Flags had violated the

00:28:05.551,00:28:09.888
statue, so they had taken the
information, even though they
hadn’t done anything with it,

00:28:09.888,00:28:13.892
there was a harm and the mother
was allowed to seek um a right
of action i.e. she could sue

00:28:13.892,00:28:18.196
them or do whatever. And this is
different because most the time,
when someone takes something,

00:28:18.196,00:28:22.134
you have to prove that they did
something with it. And so this
is a great example of how you

00:28:22.134,00:28:27.673
can actually create laws that
allow you to get um compensation
for something even though ya

00:28:27.673,00:28:32.344
know nothing terrible may have
happened with it. So this gets
to the last question I ask. Do

00:28:32.344,00:28:37.849
people actually have feelings
about who should be involved in
development, in ya know the -the

00:28:37.849,00:28:42.854
sale of and then in reparations
for malicious use um or
elicitation. So I asked this

00:28:42.854,00:28:48.894
question; it was kinda a -a grid
chart um of who should be in
charge at what portions of it so

00:28:48.894,00:28:53.398
if you’re a user uh, university
researcher, an independent
regulatory organization, a

00:28:53.398,00:28:57.402
legislator, or a device
manufacturer, how should you be
involved in BCI development

00:28:57.402,00:29:00.839
compared to current involvement
so there is a subjective, how
much do you know about what’s

00:29:00.839,00:29:05.577
currently going on. For
development oversight the actual
implementation and use and then

00:29:05.577,00:29:10.849
the reparations. And so the two
main takeaway points here are
that independent regulatory

00:29:10.849,00:29:15.053
organizations, legislators, and
device manufacturers should be
more involved going from

00:29:15.053,00:29:19.825
development to reparations for
misuse. So as you sort of go
down this development chain,

00:29:19.825,00:29:23.762
they should be more involved in
regulating or actually saying
that you should be anonymizing

00:29:23.762,00:29:29.901
or you know where things should
be happening. Um, I also found
that users should be the least

00:29:29.901,00:29:33.739
involved in reparations from a
use and device manufacturers
should be the most involved. So

00:29:33.739,00:29:37.442
it shouldn’t be the onus of the
user when something is taken
from them to go out and try to

00:29:37.442,00:29:42.381
sort of survey how they can get
reparations. The device
manufacturers should really be

00:29:42.381,00:29:46.418
taking charge of that either in
just giving out money or maybe
they should be protecting the

00:29:46.418,00:29:49.488
information to begin with so
they make sure that no one’s
eliciting information without

00:29:49.488,00:29:54.526
their knowing about it or
they’re not the ones who are
doing it. So, based on all that,

00:29:54.526,00:29:59.197
there’s some policy solutions
and there’s a part in here where
all y’all can participate so get

00:29:59.197,00:30:04.603
ready to take notes. Um, one of
the biggest things is increased
involvement by legislators with

00:30:04.603,00:30:09.775
reparations from elicit
-elicitation or misuse so it’d
be great if we could get federal

00:30:09.775,00:30:14.279
or state level rights to
neuro-privacy or broader genetic
biometric da -data privacy

00:30:14.279,00:30:20.452
legislation. Um, also possibly
providing reparations by statute
so either monetary allowing for

00:30:20.452,00:30:24.589
private right of action i.e. you
yourself can sue someone, you
don't have to wait for a class

00:30:24.589,00:30:29.928
action. Also, empowering
regulatory agencies like the FTC
to actually um have more money

00:30:29.928,00:30:34.032
and have more people to look
into this because thank you FTC
I know you’re doing great things

00:30:34.032,00:30:38.737
but there’s not a lot of you.
Um, another great one is
involving independent regulatory

00:30:38.737,00:30:44.609
organizations so things like I
triple EEE, AMC. Um I actually
consider you the hacker uh

00:30:44.609,00:30:48.046
Defcon audience as an
independent regulatory
organization because now that

00:30:48.046,00:30:52.584
you know about this, maybe you
start looking at source code,
maybe you start ya know taking

00:30:52.584,00:30:57.089
down the -the terms of service
and actually looking at what’s
going on so that you can inform

00:30:57.089,00:31:00.525
ya know here at Defcon or
otherwise what’s actually
happening with the systems that

00:31:00.525,00:31:04.796
we’re using. Um, it would be
great to have accountability for
device manufacturers cause let’s

00:31:04.796,00:31:10.402
be honest, there’s not a lot of
that right now. And then
overall, how do we actually uh

00:31:10.402,00:31:15.941
portray to consumers the risk of
using the device. So, how do you
let people know there’s a 75

00:31:15.941,00:31:19.911
percent risk that information
could be elicited from them by
using the device. Or how do they

00:31:19.911,00:31:24.449
actually understand that 99
percent of the time they use it,
they’re gonna be protected from

00:31:24.449,00:31:28.120
hackers coming in and eliciting
information. So how do we have
that conversation and this gets

00:31:28.120,00:31:33.125
to um overall tech literacy um
in United States and beyond.
Okay, so homework. Here’s my

00:31:35.927,00:31:42.400
asks of all of you. Um for those
of you who are familiar with
This is Fine Dog we are about on

00:31:42.400,00:31:47.939
slide three. Um, but it’s not
too late, we can start putting
out the fire, um and there’s a

00:31:47.939,00:31:52.077
couple different ways that we
can do that. I actually a -a
stuffed This is Fine Dog at

00:31:52.077,00:31:57.048
home, um it’s very cute little
plushy. Um, reminds me that
things are terrible but they can

00:31:57.048,00:32:01.753
get better. So, to the
developers in the room, just
because you can, doesn’t mean

00:32:01.753,00:32:07.425
you should. And I say this I -I
see some yays in the front of
the audience here okay. I say

00:32:07.425,00:32:12.264
this because as someone who
loves new technologies and loves
hacking and things like that,

00:32:12.264,00:32:15.667
you really have to start
thinking about the things that
you’re doing. So if you are

00:32:15.667,00:32:20.639
trying to create a game
controller for someone who’s
paralyzes, that’s great because

00:32:20.639,00:32:25.310
maybe the brain signal’s is the
only thing that’s left. If you
are literally like shooting

00:32:25.310,00:32:30.015
electricity through your skull
ya know using a 9 volt battery
whatever, or you find a friend

00:32:30.015,00:32:32.517
and a drill bit and you’re like
yeah we’re gonna drill a hole in
my skull and we’re gonna stick

00:32:32.517,00:32:39.491
this wire in, like no, please
for the love of God no. Um, ask
yourself what kind of problem is

00:32:39.491,00:32:41.860
it you're trying to solve, maybe
are there other modalities that
you can use to obtain that

00:32:41.860,00:32:45.363
information and what is the
least amount of information you
need to complete a particular

00:32:45.363,00:32:50.702
task. So maybe you don’t need to
have complete coverage over the
entire cortex, maybe you just

00:32:50.702,00:32:56.408
need motor cortex, maybe you
just need um parietal occipital
for the P300. Try to gather the

00:32:56.408,00:33:00.545
least amount of information
possible to make yourself the
least liable. And then finally,

00:33:00.545,00:33:04.916
do as much processing as
possible locally or on the
device. And so the best example

00:33:04.916,00:33:10.255
I have of this, this is a BCI
anonymizer, this is um from a
paper called App Stores for the

00:33:10.255,00:33:16.161
Brain. Um this by Tamara Bonaci,
Ryan Calo, and Howard Chizeck,
um Tamara Bonaci started this

00:33:16.161,00:33:19.798
research in a lab I was in, Ryan
Calo was on my dissertation
committee, and Howard Chizeck

00:33:19.798,00:33:23.935
was my PhD advisor. So, um but
they basically were saying,
look, is there a way that you

00:33:23.935,00:33:28.940
can still get information out
but you’re only releasing
information that’s necessary to

00:33:28.940,00:33:34.412
be used by the device itself? So
if you’re controlling a
helicopter with your mind, it

00:33:34.412,00:33:38.216
doesn't need to have the entire
raw data stream of your neural
information, it just needs to

00:33:38.216,00:33:43.755
get the commands up, down,
right, left, type deal. To the
privacy conscious people in the

00:33:43.755,00:33:48.460
room. I’m really sad there’s not
a lot of aluminum, tin foil hats
in here; I was expecting a

00:33:48.460,00:33:53.131
little bit more. Um, so right
off the bat I can say now don’t
use these kinds of devices, and

00:33:53.131,00:33:56.868
that’s easy for me to say
because they don’t have market
saturation, it’s not like you

00:33:56.868,00:34:02.173
need them to do your job. This
is gonna be a lot harder if for
some reason, these devices start

00:34:02.173,00:34:08.013
getting mandated for use. So you
have to pass some sort of lie
detector testing using a BCI,

00:34:08.013,00:34:12.884
you have to wear it for your job
because they’re monitoring your
productivity or that’s just how

00:34:12.884,00:34:18.089
you use the computer. So as we
get further and further down
this technological path, how can

00:34:18.089,00:34:24.562
you opt out and how does that
disadvantage you when you do
that? Um, if you are worried

00:34:24.562,00:34:27.933
about people eliciting
information without your
knowledge, you may feel better

00:34:27.933,00:34:32.370
with a slower screen refresh
rate to prevent that subliminal
elicitation; go pull that CRT

00:34:32.370,00:34:36.908
monitor out of the basement or
the garage, um you should be
fine. Cause then at least you’ll

00:34:36.908,00:34:40.879
know that ya know someone’s
trying to get information out
from you. And here’s my ask.

00:34:40.879,00:34:45.116
Okay contacting congress dot
org. Everyone write this down or
go to it right now. So you can

00:34:45.116,00:34:49.554
look up your federal um
legislators, your congresspeep
um your House of Representative

00:34:49.554,00:34:55.627
and your Senators. Um step one,
you can either call or email
their DC offices and ask them

00:34:55.627,00:35:00.432
point blank, what is your
position on data privacy? What
is your position on regulating

00:35:00.432,00:35:06.037
emerging technologies? They will
probably send you back a nice
form letter. But having been a

00:35:06.037,00:35:10.842
fellow in-staffer in Congress,
um they will categorize this and
if they get enough people

00:35:10.842,00:35:16.581
calling about it, then they know
that consumers are interested.
If any of you were at the um -uh

00:35:16.581,00:35:20.885
hacker talk yesterday that had
Reps Langevin and Lieu, um they
also said the same thing, like

00:35:20.885,00:35:25.590
you can totally be involved in
this process. Um, it is the
August recess which means all or

00:35:25.590,00:35:30.195
your congresspeople are back
home in their districts. Um, go
to the town halls, ask them what

00:35:30.195,00:35:35.934
their data privacy feelings are.
Um you can even call and make an
appointments and you’ll either

00:35:35.934,00:35:38.403
talk to a staffer there or
you’ll talk to the
representative or senator

00:35:38.403,00:35:41.806
themselves depending on their
schedule and have a
conversation, let them know that

00:35:41.806,00:35:46.344
you are an expert in security,
let them know you’re an expert
on privacy, let them know what

00:35:46.344,00:35:49.614
expertise you have and then
maybe when a bill comes up,
they’ll be like, oh, we should

00:35:49.614,00:35:52.717
ya know find out more
information about it and they
can use you as a resource. Um,

00:35:52.717,00:35:57.022
and just generally be involved
in the democratic process so uh
the offices of the members are

00:35:57.022,00:36:02.093
actually uh, they belong to you,
not them so you can go into them
if you go to DC. Um, you should

00:36:02.093,00:36:06.765
be totally feel free to reach
out to them um in your state as
well, and this goes for state

00:36:06.765,00:36:11.336
legislatures um and city
legislatures. So, anyone from
the state of Washington we just

00:36:11.336,00:36:16.875
had a big showdown over um data
privacy in the last congress.
Um, they’re probably gonna be

00:36:16.875,00:36:22.847
bringing that back so get ready
to call your um state
legislators um in 2020. And most

00:36:22.847,00:36:26.251
importantly and I know that
there are studies saying this is
gonna take years off your life

00:36:26.251,00:36:29.354
but really read the terms of
service to find out what’s
happening to your biometric

00:36:29.354,00:36:33.525
information, particularly
biometric information. It’s
probably too late for ya know

00:36:33.525,00:36:36.861
most the social media sites but
if you’re gonna be putting that
cap on your head, you should

00:36:36.861,00:36:41.299
really know what’s happening to
your information. Okay, 3-Letter
Agencies in the room, I know

00:36:41.299,00:36:46.004
you’re here, I know that you
offered money to my advisor to
fund this project and we turned

00:36:46.004,00:36:52.277
you down. Um, I will say I don’t
know if I was supposed to say
that, crap. Nevermind. Um, too

00:36:52.277,00:36:57.082
late now. Um, I so I’m guessing
that there’s a couple people in
this room who are probably

00:36:57.082,00:37:02.153
saying, why the hell would you
do this research? You’re
enabling the further ya know use

00:37:02.153,00:37:05.857
of this technology and you’re
gonna let the 3-Letter agencies
come in and steal our

00:37:05.857,00:37:10.728
information. And my response to
that is if I’m not the one
telling you that this is

00:37:10.728,00:37:15.633
happening now before it becomes
an actual problem, would you
rather find out later when it is

00:37:15.633,00:37:20.071
a problem and they’ve already
been taking information from
people? So, now that you the

00:37:20.071,00:37:25.009
hacker community at large are
aware of this, you can start
looking for it, you can start

00:37:25.009,00:37:29.280
asking those questions, and you
can start being skeptical of
these kinds of devices coming

00:37:29.280,00:37:35.220
onto the market. Trying not to
make you all paranoid but let’s
be honest. Um, if you’re

00:37:35.220,00:37:38.556
thinking about using this kind
of technique for interrogation,
you have to come to terms with

00:37:38.556,00:37:44.195
some serious ethical and legal
questions. Yes there is actually
a Neuro Law group; um it’s part

00:37:44.195,00:37:48.399
of the MacArthur uh Scott and
MacArthur Green it’s out of
Vanderbilt. You can sign up for

00:37:48.399,00:37:53.505
their distrio list and they will
send you on a semi-regular basis
papers um and conferences

00:37:53.505,00:37:57.742
related to neurolaw. It’s
actually quite interesting um
but you can look at questions of

00:37:57.742,00:38:01.479
ya know freedom of speech and
expression, reasonable
expectations of privacy, um self

00:38:01.479,00:38:06.184
incrimination. Are you really
gonna find a judge who ya know
for whatever reason is gonna

00:38:06.184,00:38:11.890
allow that if you don’t have
some sort of warrant? I don’t
know. Um, there are, I would

00:38:11.890,00:38:16.728
also like to point out that all
these results are from compliant
and willing participants so

00:38:16.728,00:38:21.966
these are mostly graduate
students who volunteered to come
sit in a room and you give them

00:38:21.966,00:38:25.970
money or a gift card afterwards
and they’re perfectly happy to
stare at a dot on a screen. I

00:38:25.970,00:38:29.374
don’t think someone in custody
is gonna be that compliant so I
don't know if the results are

00:38:29.374,00:38:33.611
gonna be that good. This
technology is also still in its
infancy and you really shouldn't

00:38:33.611,00:38:38.349
think of technology as the
solution to a problem. Um for
those of you who are familiar

00:38:38.349,00:38:44.088
with FMRI literature, there was
a poster that someone did where
they put they took a fish, a

00:38:44.088,00:38:47.959
dead salmon from Pike’s Place
market and put it in an FMRI
scanner and they got

00:38:47.959,00:38:52.931
statistically significant
results when they showed it
images. [Laughter in audience]

00:38:52.931,00:38:58.703
So think about that. If you can
get statistically significant
results from a dead fish are you

00:38:58.703,00:39:02.140
really gonna be confident that
the information that you’re
gonna be taking from someone who

00:39:02.140,00:39:05.243
doesn’t want to be giving up the
information is really what you
want to be getting out of them.

00:39:05.243,00:39:10.248
So think about that. So in
summary, this is one future. So
going back to Black Mirror um,

00:39:12.550,00:39:17.388
this is play test, this is
someone who volunteers to um
come and play a video game again

00:39:17.388,00:39:23.962
spoilers, I’m sorry. Um, but
they figure out that he’s afraid
of spiders, they figure out who

00:39:23.962,00:39:29.734
his childhood bully is and they
really start using this
information um against him and

00:39:29.734,00:39:36.174
super spoilers, he dies sorry.
Um, but it’s a very dystopian
future of, you know everything

00:39:36.174,00:39:40.545
that we think of is now going to
be used against us. And I’d like
to posit that we can create a

00:39:40.545,00:39:46.317
different future. Um so shout
out to my Star Trek fans in the
audience. Geordi LaForge um yay!

00:39:46.317,00:39:52.190
But ya know this is another BCI.
Like it is taking information in
from the outside world and

00:39:52.190,00:39:56.861
putting it into his brain and I
don’t know if there was a plot
line that I missed, but I don’t

00:39:56.861,00:40:00.932
remember them being like, oh, we
found out that Geordi likes
Starbucks because we showed him

00:40:00.932,00:40:06.037
a bunch of logos. So you know, I
don’t know it’s the -it just
seems less dystopian, it’s much

00:40:06.037,00:40:10.608
happier future. So start
thinking of the ways we can do
this for good. And if there’s

00:40:10.608,00:40:15.780
one thing that I need you to
remember from this talk, there’s
a difference between telepathy

00:40:15.780,00:40:20.652
and targeted elicitation of
information. Targeted
elicitation so we’re not going

00:40:20.652,00:40:25.156
to come and use a brain ray and
take all of your thoughts. Most
the time it’s going to be very

00:40:25.156,00:40:30.428
specific and the stimuli are
going to be very particular. So
on that note, um I took out my

00:40:30.428,00:40:33.931
funding screen slide, You can
talk to me about funding. Like I
said, I have a lot of pictures

00:40:33.931,00:40:39.437
of me wearing nerdy EG caps. Um
you can find me on Twitter. Um,
and thanks to everyone who made

00:40:39.437,00:40:45.677
this talk possible and thank you
so much for coming um so early
on a Saturday morning to Vegas.

00:40:45.677,00:40:50.682
Um I’ll be around if you have
any questions, so, thank you.
[Audience clapping]

