00:00:00.033,00:00:06.139
>>Hello and welcome to my talk,
uh Don’t Red Team AI Like A
Chump. So uh here’s a brief

00:00:06.139,00:00:11.144
intro. Uh who am I? Um I am an
AI researcher and cofounder of
the AI village here and a grad

00:00:13.614,00:00:17.885
student at Harvard with a bit of
an axe to grind against the AI
hype train. Uh most of my

00:00:17.885,00:00:22.422
research so far has been on how
to operationalize theoretical
attacks that are proposed in the

00:00:22.422,00:00:26.760
academic literature against real
systems. Um and so this includes
things like attacking facial

00:00:26.760,00:00:31.598
recognition software to extract
personal data, uh attacking text
completion tools to steal credit

00:00:31.598,00:00:38.038
card numbers, um and also to
reverse engineer um
recommendation algorithms so

00:00:38.038,00:00:43.043
that you can recommend fake
products. Uh this is gonna be a
very practical talk um cause

00:00:45.078,00:00:48.916
it’s about how to do things
right when it comes to hacking
on AI systems. And it’s more or

00:00:48.916,00:00:53.654
less a knowledge dump of a lot
of the wisdom that I’ve picked
up over many many failures um

00:00:53.654,00:00:57.591
and some successes over the
years. Uh and I do hope that
it'll be interesting and useful

00:00:57.591,00:01:03.597
to you. So I’m gonna start off
this talk with a bit of a story.
Um in March uh Tencent security

00:01:03.597,00:01:06.900
lab came out with a really
awesome report that showed that
you can inject an adversarial

00:01:06.900,00:01:12.806
example on uh Tesla hardware and
then cause the car to drive into
traffic. Um and now contrast

00:01:12.806,00:01:17.010
this with another story uh where
back in November my friend and I
were driving in a Tesla and then

00:01:17.010,00:01:22.115
chatting about self driving cars
um and it was Boston and it was
cold outside and I had this

00:01:22.115,00:01:26.586
really dumb idea to see if we
could take the salt melt that
was in the trunk and then pour

00:01:26.586,00:01:31.558
it on the road to just see kind
of how robust this lane finding
algorithm would be. Um and the

00:01:31.558,00:01:35.862
result is that the Tesla
interpreted the salt as a line
um and it ended up driving us

00:01:35.862,00:01:41.802
into an oncoming lane, which was
fun. Um [laughs] and 1 of these
attacks is obviously much easier

00:01:41.802,00:01:45.172
to pull off from a practical
perspective, but 1 of these is
really sexy and it looks really

00:01:45.172,00:01:51.445
good in the media. Um so which
of these is more valuable to a
real attacker? And this attack-

00:01:51.445,00:01:54.381
this talk is about the
practicality of these kinds of
attacks so if you want the sexy

00:01:54.381,00:01:59.386
ones, the- the door is over
there. Uh so and also and in
case at some point you zone out

00:02:01.788,00:02:05.625
during this presentation because
it’s the morning, maybe you’re
hangover- hungover a bit, um

00:02:05.625,00:02:10.197
I’ve put the salient details up
for you right now. Um and really
if you only take 1 thing away

00:02:10.197,00:02:13.066
from this talk, it’s that you
need to focus on the threat
model of the system that you’re

00:02:13.066,00:02:18.639
attacking. And we’ll talk about
the other things during the
course of presentation. So uh

00:02:18.639,00:02:22.009
we’re gonna take a brief detour
to make sure that we’re all kind
of on the same page, I’m not

00:02:22.009,00:02:25.645
sure how many of you guys have
done AI stuff, or if you have
sometimes it’s nice to have a

00:02:25.645,00:02:30.217
refresher. Um so when we talk
about AI, we’re referring to a
particular class of algorithms

00:02:30.217,00:02:34.054
that we use to extract
actionable information from
data. Um and it’s actually quite

00:02:34.054,00:02:38.859
a very old field it came from
the post war 50’s and over the
years it’s transformed a bunch

00:02:38.859,00:02:42.829
and um it’s gone through booms
and busts as we discovered new
methods of extracting

00:02:42.829,00:02:49.403
information, with larger data
sets and we have better compute
power. Um and these days um the

00:02:49.403,00:02:52.873
current hype centers on deep
learning algorithms because
these have managed to solve a

00:02:52.873,00:02:57.811
lot of the problems that we uh
previously thought were not
really gonna be solvable in our

00:02:57.811,00:03:02.783
lifetime. Um and the principles
that I talk about in this talk
uh apply broadly to machine

00:03:02.783,00:03:06.920
learning algorithms including
deep learning algorithms um that
we see uh deployed across

00:03:06.920,00:03:13.160
sectors. And also in this talk
um AI is ML um cause that’s
about as close to any sort of

00:03:13.160,00:03:17.264
real artificial intelligence
that we’ve gotten yet, um even
if it is literally just pattern

00:03:17.264,00:03:22.803
matching under the hood. Uh so
when we look under the hood of
an AI algorithm um ok this is

00:03:22.803,00:03:26.573
also my favorite meme um
[laughs] so it’s a Scooby Doo
meme. I don’t know if you can,

00:03:26.573,00:03:31.011
you probably can’t read it, so
I’ll read it to you. Um so Fred
is like “OK gang! Let’s see what

00:03:31.011,00:03:35.615
deep learning really is!” and
there’s a deep neural network um
on that hood there, and so he

00:03:35.615,00:03:41.154
pulls the hood off and he’s like
“What? Convex optimization?”
[audience chuckles] Uh so what

00:03:41.154,00:03:45.092
we’re so when we look under the
hood of these algorithms what
we’re actually finding is math

00:03:45.092,00:03:50.263
for optimizing a function which
maps input data uh to output
predictions. So think like this

00:03:50.263,00:03:55.402
cluster of pixels uh probably
maps to this person's face or
this spot’s network activity

00:03:55.402,00:03:59.206
probably maps to this threat
actor. Um and don’t worry I’m
gonna keep the math to the

00:03:59.206,00:04:03.810
minimum here so you don’t fall
asleep if- if you haven't um
already [laughs] uh but I’m

00:04:03.810,00:04:08.815
happy to talk math after or off
line because math is great. Um
mk so here’s my AI 101 speil. At

00:04:11.384,00:04:15.689
an extremely basic level all
that learning is is just an
iterative process where we just

00:04:15.689,00:04:20.627
kind of shove some data into a
black box um then twiddle some
knobs that adjust how well this

00:04:20.627,00:04:25.398
function fits the data. And um
we tend to call this function
the model cause what we’re

00:04:25.398,00:04:29.336
trying to do is we’re trying to
build a model of the data. So
for example, say that we wanna

00:04:29.336,00:04:33.173
build a model for recognizing
objects in an image; uh what
we’re actually trying to do is

00:04:33.173,00:04:39.679
we’re trying to fit a function
that maps clusters of pixels to
object names. And mathematically

00:04:39.679,00:04:44.451
this translates into asking the
model to predict which pixel
clusters will reliably tell us

00:04:44.451,00:04:50.924
the correct object that’s in the
image. And the model after it
outputs a prediction value um we

00:04:50.924,00:04:54.895
then will look at it and compare
it with what we know to be the
true prediction value. Um so if

00:04:54.895,00:04:58.598
we have a picture of a boat and
the model says it’s a dog um we
know that there’s something not

00:04:58.598,00:05:02.235
quite right there. Uh so we need
to twiddle some knobs again and
then try again. I mean we use

00:05:02.235,00:05:07.774
this sort of comparison between
the prediction and the truth
value uh to help make our next

00:05:07.774,00:05:12.078
round of knob twiddling, which
we actually call parameter
tweaking, um more effective

00:05:12.078,00:05:17.250
until finally we get to some
point where like this is good
enough, let's go with it. Um and

00:05:17.250,00:05:23.790
we call um and then we call the
set of these uh data that’s uh
labeled with the true values the

00:05:23.790,00:05:27.861
training data set. Um and this
whole process is known as
training and it’s actually kind

00:05:27.861,00:05:32.299
of analogous to how we as humans
learn where uh we take in a
bunch of information um we try

00:05:32.299,00:05:36.703
to use it to make decisions, uh
we fail a bunch, uh we hopefully
learn from each of these

00:05:36.703,00:05:40.740
failures and eventually we end
up learning something new that
we can apply to situations we

00:05:40.740,00:05:46.713
haven’t experienced yet. And um
so after we finish training we
then want to check to make sure

00:05:46.713,00:05:53.086
that we’ve learned what we
intended to uh by using a
training data set. Um. sorry I

00:05:53.086,00:05:58.091
have to, ugh, ff. Um when- and
you can think of the t- the uh
testing data set um as being

00:06:01.494,00:06:05.565
similar to taking an exam in
school um where you haven’t seen
the, hopefully you haven't seen

00:06:05.565,00:06:09.836
the questions before, um and the
goal is to see if you can then
generalize what you’ve learned

00:06:09.836,00:06:14.841
in class to like the real world
sort of um. And we kind of did
the same thing with machine

00:06:17.177,00:06:21.381
learning, or AI algorithms. And
the reason we do this is because
we want to learn some

00:06:21.381,00:06:26.219
generalized uh representation of
the data um cause that means
that we can use it in the real

00:06:26.219,00:06:30.590
world a lot better than if it
was just trained on 1 data set
and that’s all it’s ever seen.

00:06:30.590,00:06:36.463
Um and we can also kind of think
of learning as being a bit of er
essentially a feedback loop. So

00:06:36.463,00:06:39.566
ok now that we have the
vocabulary and basic
understanding of what goes into

00:06:39.566,00:06:43.737
an AI um now let’s look at the
parts so that we can think of
different ways to break them. Um

00:06:43.737,00:06:48.575
and these are the parts. So we
have the data and the model, um
the data could be that the

00:06:48.575,00:06:52.312
training or the testing data
sets, or it could be the
deployed environment data. Um

00:06:52.312,00:06:57.317
the model uh is either like the
algorithm that you used to train
like a deep neural network or

00:06:57.317,00:07:01.454
logistic regression or 1 of
those things and the parameters
are what you’re tweaking um that

00:07:01.454,00:07:07.794
fits this algorithm to whatever
data set you’re working with. So
we can poison the data by

00:07:07.794,00:07:11.331
feeding inaccurate information
to the AI system, um which will
make it make incorrect

00:07:11.331,00:07:15.402
decisions. Um and in a way you
can actually think of this as
being a supply chain attack

00:07:15.402,00:07:19.639
where uh you, you don’t, in some
cases you don’t necessarily know
where the data comes from which

00:07:19.639,00:07:23.510
kind of gives you a level of
uncertainty. Um and an
interesting feature of modern AI

00:07:23.510,00:07:27.981
systems is that we need to label
the training data, um but
getting good labels is actually

00:07:27.981,00:07:32.152
kind of hard and expensive. Um
and even some techniques that
propose unsupervised or

00:07:32.152,00:07:38.124
semi-supervised learning methods
um will still rely to some
extent on human based or human

00:07:38.124,00:07:42.962
labeled training, human labeled
training data. Um so if you can
sneak in a bad dirt- version of

00:07:42.962,00:07:46.599
this training data into an AI
system then you can actually
accomplish a pretty good amount

00:07:46.599,00:07:53.006
of bad. Um and we can also do
this data poisoning during real
time deployment. Um if you’ve

00:07:53.006,00:07:56.843
heard the term adversarial
example or wild pattern uh it
refers to a data point where you

00:07:56.843,00:08:03.616
can show or that- that you can
show to a deployed AI system um
that it will misinterpret. And

00:08:03.616,00:08:07.320
there’s a lot of hullabaloo
around these adversarial example
things. Um mathematically you

00:08:07.320,00:08:10.924
can think of it as constructing
an optimization problem to find
a data point that lies along the

00:08:10.924,00:08:15.261
classification boundary, um in
such a way that the AI doesn't
quite know how to classify it

00:08:15.261,00:08:19.232
but will probably classify it in
the wrong direction. Um so I’ve
got this picture of these dogs

00:08:19.232,00:08:26.206
and these chicken, fried
chickens um and uh to your eyes
uh initially like it might not

00:08:26.206,00:08:30.343
be that easy to tell which 1 of
these is a dog and which 1 of
these is a chicken so in in some

00:08:30.343,00:08:35.348
way there’s kind of an analogy
between adversarial examples and
uh optical illusions um and

00:08:38.718,00:08:41.821
there’s just something about the
way that these images look that
kind of wig out your brain, and

00:08:41.821,00:08:48.027
turns out you can do something
similar with AI. We can do also
something along the lines of the

00:08:48.027,00:08:52.265
infamous Javascript MPM hack
last year. Um which is where
some bad eggs got ahold of the

00:08:52.265,00:08:56.770
MPM maintainers account or an
MPM maintainers account and then
used it to push a rogue version

00:08:56.770,00:09:02.409
of a popular programming tool um
that ended up scraping a bunch
of people’s MPM uh login tokens.

00:09:02.409,00:09:06.646
Um and since most AI developer
software is open source, like
scikit-learn uh it's possible

00:09:06.646,00:09:10.350
that a bad actor could inject
some bad versions of functions
that would alter the behavior of

00:09:10.350,00:09:15.855
an AI model during deployment
which is kinda scary. We can
also do a cool attack that’s

00:09:15.855,00:09:19.926
called model inversion which is
basically like taking the model
and then shaking it creatively

00:09:19.926,00:09:25.432
with some statistics. Um and
then making the training data
fall back out. Um so in ss- so

00:09:25.432,00:09:29.536
in this example we have this
picture of this dude just named
Bill um and it exists in this

00:09:29.536,00:09:33.473
training data set for a facial
recognition system. It turns out
you can recover a picture of

00:09:33.473,00:09:38.645
BIll by essentially asking the
model “Who is in this picture?”
um or instead of asking the

00:09:38.645,00:09:45.318
model “Who is in the picture?”
but to make it draw who it
thinks Bill is. So it’s like an

00:09:45.318,00:09:49.856
inversion kind of problem. Um we
can also uh do some things to
steal the parameters of a model

00:09:49.856,00:09:53.827
um through an attack called
model theft. Um and this model
or this attack usually involves

00:09:53.827,00:09:57.197
a surrogate model that’s trained
on the same data as the target
model that we use that we’re

00:09:57.197,00:10:01.835
trying to steal. Um and we can
take the output predictions from
the target model and then pair

00:10:01.835,00:10:07.073
them with the original data set
to make a new training data set
um which uh when we train our

00:10:07.073,00:10:10.376
stolen model, it’ll give us sort
of a similar output predictions
as the target we’re trying to

00:10:10.376,00:10:15.114
copy. So it’s basically just
stealing the model, using their
inputs and outputs, it’s a great

00:10:15.114,00:10:21.221
attack.Um ok so those are some
of the ways that you can attack
an AI system but how do we go

00:10:21.221,00:10:24.224
about designing 1 of these
attacks fresh? The principles
that are laid out in the

00:10:24.224,00:10:28.428
academic literature can kind of
be broken down into 3 questions
you can ask yourself. Uh so you

00:10:28.428,00:10:31.431
want to know like, what kind of
model are you attacking? Is it
deep learning? Or logistic

00:10:31.431,00:10:35.001
regression? Or a decision tree?
Um cause knowing what algorithm
you’re dealing with will give

00:10:35.001,00:10:40.139
you kind of an idea of where to
look in the academic literature
for prior art uh to try to help

00:10:40.139,00:10:46.779
you figure out maybe what you
should be doing to bend this
model to your will. Um and then

00:10:46.779,00:10:49.849
uh you want to kind of know
where the data comes from. And
you also want to know what

00:10:49.849,00:10:54.354
formats it- it’s in. Um so if we
know that the system deals with
images, that’s gonna be much

00:10:54.354,00:10:59.492
different than if it’s dealing
with strings or or text. And we
also want to know where the

00:10:59.492,00:11:05.365
predictions go and what data
overall does it output. Cause it
turns out um that a lot of

00:11:05.365,00:11:08.801
information that some of these
systems put out, you can take
advantage of and use to execute

00:11:08.801,00:11:13.306
some of these attacks. Um like
the the model in- 1 of the ways
that you can do a model

00:11:13.306,00:11:17.644
inversion attack is to use how
confident so some models that
are deployed in the cloud, ew,

00:11:17.644,00:11:22.949
f*ck, sorry [laughs]. Um some
models that are deployed in the
cloud will also spit out kind of

00:11:22.949,00:11:29.422
a confidence interval that says
like “I’m 60 percent certain
that this is a dog.” Um and you

00:11:29.422,00:11:34.427
can actually use that when
you’re doing your statistical,
aw f*ck sorry [chuckles]. Agh! I

00:11:37.330,00:11:42.335
scrolled on the wrong thing. Um
enjoy this preview for a second.
Ok. Um. Yes. F*ck what, where

00:11:47.974,00:11:54.647
was I? Um. Ok well point is pay
attention to where the
predictions go, um and what is

00:11:54.647,00:11:59.152
outputted because it means you
can potentially take advantage
of that to execute an attack

00:11:59.152,00:12:04.824
that affects peoples’ privacy in
terms of the training data.
Alright um so now let’s go

00:12:04.824,00:12:08.861
through 1 of these attacks as it
relates to fooling AI powered
video surveillance. Um and what

00:12:08.861,00:12:12.332
I’m going to talk about is
actually considered state of the
art, um. So the first thing that

00:12:12.332,00:12:16.703
we need to talk about is the
data pipeline. Um the common
system components of a lot of

00:12:16.703,00:12:22.508
these AI powered tools for video
surveillance um is there is uh a
camera system with a bunch of

00:12:22.508,00:12:26.179
sensors and stuff, um and you
have your detection system and
you have your recognition

00:12:26.179,00:12:31.417
system. So detection refers to
some sort of on premises model
that checks to see like each

00:12:31.417,00:12:37.557
still image in the video um has
a face in it and then frames
that have faces in it will just

00:12:37.557,00:12:41.160
forward to a recognition system.
And then wh- what the
recognition system does is it

00:12:41.160,00:12:45.698
then looks at all of these
images and then it checks that
against a database of known

00:12:45.698,00:12:50.770
faces that’s held by like a
private entity or maybe law
enforcement or something like

00:12:50.770,00:12:57.644
that. Um and then it will
identify people in like images
and video that way. Um and we’re

00:12:57.644,00:13:02.048
focusing on the detection system
because the further down the
pipeline you go in terms of data

00:13:02.048,00:13:05.518
processing, uh the more
processed the data gets, then
the harder it is to figure out

00:13:05.518,00:13:10.523
what’s actually going on. Um.
Alright so with this example
we’re going with we’re gonna

00:13:13.426,00:13:18.731
attack the YOLO model which I
did not name. It stands for You
Only Look Once. It’s very cool.

00:13:18.731,00:13:23.536
Um uh where does the data come
from? It- it comes from video
frames uh cause we’re pulling in

00:13:23.536,00:13:27.840
data from cameras. Um and then
the predictions are stored as a
set of flagged frames to then

00:13:27.840,00:13:34.313
forward on to our recognition
system somewhere offsite. Um ok
so this is the YOLO object

00:13:34.313,00:13:39.185
detection algorithm. Essentially
we’re cutting the image into a
grid for each and then for each

00:13:39.185,00:13:42.889
box in the grid we want to
compute a score for how likely
it is that that box contains an

00:13:42.889,00:13:47.560
object. And approximately which
object the algorithm thinks is
gonna be in that little box. Um

00:13:47.560,00:13:50.630
and there’s some bugs with this
algorithm like it- it can’t ha-
handle objects that are too

00:13:50.630,00:13:54.934
close together because there’s a
1 object per box rule, um
however it’s still pretty

00:13:54.934,00:13:59.605
popular for many deployed, uh
systems, which means it’s a
really likely target for any

00:13:59.605,00:14:05.244
sort of system that’s doing like
a object detection task, like
facial recognition. Or object

00:14:05.244,00:14:10.883
detection, face detection. Ok so
the state of the art right now
for attacking object detection

00:14:10.883,00:14:16.589
algorithms is to generate
stickers, patches, objects, um
like there- there’s some

00:14:16.589,00:14:20.460
examples of wearing some glasses
on your face that’s supposed to
obscure you from facial

00:14:20.460,00:14:24.330
recognition. Um there’s always a
really, I think my friend is
somewhere in here but he had a

00:14:24.330,00:14:28.134
really cool attack where you
could like 3D print a turtle and
then um you could make the

00:14:28.134,00:14:30.136
system think that it’s a gun. Um
and uh the way that these kinds
of systems work is that um it-

00:14:30.136,00:14:32.138
it messes up the statistics of
the image um by screaming. So
with this example um this

00:14:32.138,00:14:34.140
sticker is screaming like “I am
a toaster!” at the, at the model
and then the model is like “Ok,

00:14:34.140,00:14:36.142
sure you are.” Um [laughs] so
it, in that- that’s kind of how
these kind of stickers work. Um

00:14:36.142,00:14:38.778
and so the most rad attack so
far of these adversarial patches
um has been to try to take the

00:14:38.778,00:14:43.783
mathematics behind that and then
kind of twist it a little bit so
that you can end up making

00:14:50.022,00:14:55.027
yourself invisible. So instead
of classifying yourself as a
toaster, which might not

00:15:03.970,00:15:07.507
actually be useful if you’re
trying to attack facial
recognition, this way you can

00:15:07.507,00:15:12.712
scream like “I am just
background noise!” Um and then
it’ll look like you’re invisible

00:15:12.712,00:15:16.482
to the system. So I’m gonna show
you the demo video that’s
recorded by the researchers who

00:15:16.482,00:15:21.988
came up with this trick, cause I
think it’s a little bit more um
it’s a little more convincing at

00:15:21.988,00:15:25.825
highlighting an important point
that I’m gonna make a little bit
later. Um so AI research code is

00:15:25.825,00:15:31.097
often also extremely hard to
replicate without expertise, um
and even with expertise. Um and

00:15:31.097,00:15:36.669
it’s hideous and it’s not fun
for the uninitiated so um for
this attack um I’m releasing a

00:15:36.669,00:15:39.972
cleaned up version that’s a lot
easier to deal with so you can
play around with it cause it is

00:15:39.972,00:15:44.977
quite cool. Um and alright so
let’s play this video. How do I
do that? Oh here we go. Ok. So

00:15:52.385,00:15:57.223
notice that this the dude on the
right isn’t being detected but
the dude on the left is. The

00:15:57.223,00:16:02.762
dude on the right has this kind
of weird square crotch region
sticker thing and notice like he

00:16:02.762,00:16:06.465
doesn’t even need to move it
that far um and already the
attack kind of breaks a little

00:16:06.465,00:16:10.603
bit. Um since we’re a room full
of security researchers, um I
want you to notice that they

00:16:10.603,00:16:14.240
have to keep positioning that
patch in the right location
relative to the rest of their

00:16:14.240,00:16:19.512
bodies, um and- and to the
camera. Um he’s also going to
pass it over to his friend and

00:16:19.512,00:16:23.182
you’ll see j- just where it
starts to break and where it
starts to work for his- his

00:16:23.182,00:16:28.187
friend there. Oh there we go.
[audience member woo] Yeah you
go dude! Yeah ok, um so anyway

00:16:33.492,00:16:37.463
this is cool from a math
standpoint and also like it’s
cool as a demo, um but it’s

00:16:37.463,00:16:41.500
extremely fragile as a real
attack. Like if you were to try
to rob a bank using this to get

00:16:41.500,00:16:45.638
around the surveillance system,
people would be like, they would
definitely notice your awkward

00:16:45.638,00:16:49.742
side shuffle and then they’de
also be like “What the hell is
on your crotch?” [audience

00:16:53.779,00:16:57.516
laughter] Alright so this is the
part of the- aw f*ck, so this is
the part of the talk where I

00:16:57.516,00:17:01.754
rail on the status quo. So as
cool as these algorithm based
attacks are, there’s a huge

00:17:01.754,00:17:05.591
piece missing if you want your
tech to actually work in the
real world. Um and that’s the

00:17:05.591,00:17:10.296
threat model. Uh so red teaming
AI is often conflated with the
academic discipline of

00:17:10.296,00:17:14.400
adversarial machine learning. Um
and when we say adversarial
machine learning we mean like

00:17:14.400,00:17:18.838
cool ways to attack an AI model
with math. And when we say red
teaming AI we mean we want to

00:17:18.838,00:17:23.843
evaluate the security of an AI
system. And I made this meme
fresh for you so you’re welcome

00:17:27.280,00:17:30.650
[laughs]. Oh let me read this
meme in case you can’t read it,
uh cause I’m very proud of it.

00:17:30.650,00:17:35.955
Um it’s the steamed ham meme, um
it says: “You call this red
teaming AI despite the fact that

00:17:35.955,00:17:40.459
it’s obviously just adversarial
machine learning.” And it’s got
the sticker that um if you show

00:17:40.459,00:17:46.999
it to a classifier it’ll make it
think that it’s a toaster. Yeah
[laughs] anyway ok so um now

00:17:46.999,00:17:50.903
let’s think about this in terms
of an- an attack tree. So here’s
the adversarial machine learning

00:17:50.903,00:17:55.141
attack tree for fooling AI
powered video surveillance. So
our goal here is to make the

00:17:55.141,00:18:00.680
object detector ignore somebody
and so the way that this works
in the paper um is they try a

00:18:00.680,00:18:04.850
couple of different experiments.
1 of them is you wanna minimize
the specific class likelihood

00:18:04.850,00:18:08.754
which in this case means you
want to minimize um how likely
it is that you’re gonna be

00:18:08.754,00:18:13.225
classified as a person. And then
um the- there’s another way
where you could minimize the

00:18:13.225,00:18:19.665
objectyness which is how likely
is this uh model gonna classify
you as being an object of

00:18:19.665,00:18:23.402
interest at all. And then the
third option is to try to
minimize both. And in the paper

00:18:23.402,00:18:27.907
they show that if you minimize
this objectyness it tends to
work better which is kinda cool.

00:18:27.907,00:18:32.912
Um it’s very James Bond-esque.
Um ok so here’s the AI red team
attack tree though. Um so in

00:18:35.548,00:18:39.785
this case like we have the same
goal but the- the options on our
tree are a bit different where

00:18:39.785,00:18:43.255
we could get physical control of
the camera and then in that case
we would put a sticker over the

00:18:43.255,00:18:49.895
camera, or remove the power
source, or move the camera to a
different location um or uh we

00:18:49.895,00:18:55.067
can get access to the camera
network um and then play looping
footage or then use that to get

00:18:55.067,00:18:59.071
access to the facial detection
AI software, and then from there
do like this weird academic

00:18:59.071,00:19:05.644
attack. Um so also I want you to
notice um how far down the
adversarial machine learning

00:19:05.644,00:19:10.082
attack sub tree this is. Or I
want you to notice how far down
this adversarial machine

00:19:10.082,00:19:14.553
learning attack tree is on the
actual large red team attack
tree, um cause part of the

00:19:14.553,00:19:20.226
problem here is that the
literature for red teaming AI is
often conflated um and if there

00:19:20.226,00:19:25.297
is- there’s some stuff happening
in Hong Kong um and some very
creative solutions for some of

00:19:25.297,00:19:29.835
those things were instead of
generating bespoke adversarial
examples, um they just put

00:19:29.835,00:19:33.472
lasers on their hats and then
just shine them at the cameras
like that’s way easier than

00:19:33.472,00:19:37.743
having to do all of the math
involved in getting 1 of these
stickers or what not to work.

00:19:37.743,00:19:42.748
And you have to worry a lot less
about operational hazards. So
here is the 1 for the

00:19:44.750,00:19:50.222
adversarial- so here’s the
adversarial ML attack tree for
um the breaking self driving car

00:19:50.222,00:19:54.360
lane detection example. Um so
the goal here is we want to make
a self driving car think that a

00:19:54.360,00:19:58.931
lane isn’t a lane. And so in
this case we would just generate
an adversarial example, it’s

00:19:58.931,00:20:04.537
what you do. Um but if we want
to actually operationalize this,
um we have a couple of other

00:20:04.537,00:20:08.174
options where we could dump a
salt line on the road or we
could put stickers on the road.

00:20:08.174,00:20:14.880
In the Tencent security lab uh
paper that I mentioned um from
March uh they ended up putting

00:20:14.880,00:20:19.051
stickers on the road. That is
very similar to what I did with
salt so it’s not just me, it’s

00:20:19.051,00:20:24.457
like a problem. Um [laughs] and-
and then like your third option
is to get acc- physical access

00:20:24.457,00:20:28.027
to the car um and then after you
have that then you could get
access to the lane detection

00:20:28.027,00:20:32.264
system and then you can generate
your average joe example. But
that- that’s so much harder.

00:20:32.264,00:20:37.837
Cause in all of this uh we need
to remember that we’re looking
to be like AI security

00:20:37.837,00:20:42.208
researchers looking to exploit
AI systems and we’re not here to
write machine learning research

00:20:42.208,00:20:46.345
papers about the math necessary
to do a particular type of
algorithmic attack even though

00:20:46.345,00:20:51.584
that’s fun and I do recommend
it. Um so now that we have a
better view of what attacking an

00:20:51.584,00:20:56.155
AI system actually entails and
that it’s not all about the
model um or the algorithm that

00:20:56.155,00:21:00.693
you’re using let’s revise our
attacker guidelines. So uh what
system are we attacking rather

00:21:00.693,00:21:03.929
than what model are we
attacking. Um is it object
detection? What are the

00:21:03.929,00:21:10.469
components of this system that
we need to put together. Um and
uh what is the data processing

00:21:10.469,00:21:15.207
pipeline? Um so it often helps
to draw out a diagram of the
system that you’re targeting and

00:21:15.207,00:21:19.912
then focusing on answering each
of the next three questions for
each of those parts. So you

00:21:19.912,00:21:24.917
wanna know where the inc- the
inputs come from, and where the
outputs go. Um and it also helps

00:21:27.353,00:21:32.358
to know um like what is the data
representation that’s happening
inside this model. Um and the

00:21:35.594,00:21:38.664
last piece in designing an
attack on an AI system is to
determine what your threat model

00:21:38.664,00:21:43.035
is. Uh so how much access do you
have to each of the parts? And
what is the risk associated with

00:21:43.035,00:21:48.808
attacking each of these parts?
Uh so let’s try this out on our
AI powered recognition facial

00:21:48.808,00:21:53.612
recognition system or facial
detection system, um cause this
is Defcon, and we hate the man

00:21:53.612,00:21:57.449
um so here’s the diagram we had
earlier and we’re still gonna
focus on the detection system

00:21:57.449,00:22:01.353
because the- our goal as
attackers is ultimately to avoid
being seen, and so if you’re not

00:22:01.353,00:22:07.493
seen then you can’t be
recognized. Uh and we’re also
focusing on the detection

00:22:07.493,00:22:11.263
subsystem because it’s an
earlier part of the data
processing pipeline um and to

00:22:11.263,00:22:15.301
re- to reitercate- reiterate in
case you’ve fallen asleep
already um we want to focus

00:22:15.301,00:22:19.038
earlier in the data processing
pipeline because the further
through the system we go, um the

00:22:19.038,00:22:24.176
more processed the data gets and
the harder it is to figure out
what’s going on. Alright and so

00:22:24.176,00:22:27.646
the second, answering the second
question here, um let’s draw out
a high level view of what the

00:22:27.646,00:22:32.384
data pipeline is for the system
and then answer each- each of
these questions. So um an AI

00:22:32.384,00:22:36.922
system is taking in data um and
then it turns it into a simple
representation of it that it can

00:22:36.922,00:22:41.527
then use to make predictions. So
for a detection system, the
input is the initial image

00:22:41.527,00:22:45.831
capture from the camera. Um
lighting is an important
environmental variable because

00:22:45.831,00:22:49.368
uh sh*tty cameras use sh*tty
components that can’t deal with
extremes in lighting condition

00:22:49.368,00:22:54.139
hence the Hong Kong thing. Um
and the first stop in the
detection system is uh feature

00:22:54.139,00:22:58.310
extraction, um which is where
the AI system uh looks at the
image and then picks out the

00:22:58.310,00:23:02.414
parts of the image that it kinda
thinks match statistical
characteristics of something

00:23:02.414,00:23:07.219
that it figures might be a face.
Um and these things are f- eh
it’s important to note that the

00:23:07.219,00:23:10.856
features aren’t features like
eyes, nose, mouth or anything
like that it’s- it’s like little

00:23:10.856,00:23:15.861
clusters of pixels that tend to
map to being a face. And
lighting effects uh this quite a

00:23:17.997,00:23:21.734
lot because again if you have
blurry pictures and bad lighting
uh the system is going to be

00:23:21.734,00:23:27.840
confused and be more likely to
make mistakes. And the output is
the actual decision made by the

00:23:27.840,00:23:32.311
AI model here. So, is this or is
this not a face? And uh the data
representation is extremely

00:23:32.311,00:23:37.082
important here um because the
decision is entirely influenced
by the feature extraction and

00:23:37.082,00:23:40.853
the data the detection system
was trained on. And in many
cases it also helps to reason

00:23:40.853,00:23:46.091
about uh what data was used to
train the system. So here’s-
here’s an example scenario uh

00:23:46.091,00:23:49.194
that we can go through to kind
of illustrate this. Um so
imagine that there’s a subway

00:23:49.194,00:23:53.699
company uh that’s deployed a
facial recognition system to
presumably cut down on people

00:23:53.699,00:23:58.203
hopping turnstiles um and
they’re pretty eager to hop on
the AI hype train, so they put

00:23:58.203,00:24:02.341
their engineering team under
pretty heavy pressure um and so
from this fact alone we can

00:24:02.341,00:24:06.145
pretty reasonably guess that
these engineers uh probably used
a bunch of off the shelf

00:24:06.145,00:24:09.415
implementations they found on
GitHub and then just like
hammered the sh*t out of them

00:24:09.415,00:24:13.519
until they finally fit roughly
what their manager wanted. Um
and they probably also used a

00:24:13.519,00:24:17.890
very generic face data set to
train their AI uh cause most
companies tend to be a bit lazy

00:24:17.890,00:24:22.528
and they’ll use data that’s
roughly statistically similar uh
to publicly released data sets.

00:24:22.528,00:24:27.199
Um and we can also guess that
the cameras are probably pretty
sh*tty, uh cause if you’re

00:24:27.199,00:24:31.670
trying to avoid costs to avoid
hiring humans, um you’re
probably using a sh*tty AI and

00:24:31.670,00:24:35.607
sh*tty cameras cause either you
don’t have money or you don’t
want to spend money. Um and all

00:24:35.607,00:24:39.511
of this is extremely helpful
additional information about the
data supply chain um that can

00:24:39.511,00:24:45.551
help us design the specific
mechanics of- of our attack. Um
like with social engineering

00:24:45.551,00:24:49.421
it’s often easier to exploit the
human parts of the system. So it
helps to think through the human

00:24:49.421,00:24:55.627
factors of engineering design in
addition to the technical
components. Ok so now we have

00:24:55.627,00:24:59.531
all the parts of the system and
kind of an idea of how they all
fit together. Uh so now we’re

00:24:59.531,00:25:05.371
ready to take our- to make our
threat model. Um so here like
our- our goal again is to make

00:25:05.371,00:25:10.275
the facial detection system kind
of ignore somebody. And so we
have 2 options here where we can

00:25:10.275,00:25:14.480
get physical control of the
camera or we can get access to
the camera network. Um if we get

00:25:14.480,00:25:18.484
physical control of the thing,
we can put a sticker on the
camera, we can shine lasers at

00:25:18.484,00:25:24.490
it, um we can do all sorts of
stuff like that. Um we can also
move it to a different location

00:25:24.490,00:25:30.195
um and if we get access to the
network then we can like play
looping footage, we can move the

00:25:30.195,00:25:35.167
camera lense itself, um or we
can get access to the facial
detection AI system and in that

00:25:35.167,00:25:39.872
case um we could get- try to get
access to the training
procedure, um and in that case

00:25:39.872,00:25:43.876
well we can poison the data set
or we can force maybe a
backdoored function if we know

00:25:43.876,00:25:50.082
roughly what - what libraries
they’re using. Um or uh we could
use some like bespoke

00:25:50.082,00:25:52.985
adversarial example solution.
Like you might find in the
literature, because there are

00:25:52.985,00:25:59.391
some cases in which that is
valuable. Um uh in- in all of
this it’s important to think

00:25:59.391,00:26:03.796
about like how much access do
you have here and what are the
relative risks associated with

00:26:03.796,00:26:08.333
each of the steps. Um and the
algorithm level attacks carry
huge risks so if you somehow

00:26:08.333,00:26:12.371
snuck in here from the blue team
um this is important for you to
understand because most skids

00:26:12.371,00:26:15.974
aren’t gonna try to back door
the training function but if
you’re firm is the type to

00:26:15.974,00:26:20.245
attract ABT attention maybe um
they might try this against you
if you have a particular tasty

00:26:20.245,00:26:25.250
target. So be really smart about
calculating the risk of these
kinds of attacks. Alright so um

00:26:27.820,00:26:32.424
all and all there are 3 things
that I’m really hoping that you
get from this attack. So wake up

00:26:32.424,00:26:37.996
um uh so the the first thing is
you should go after the feature
extraction process because it’s

00:26:37.996,00:26:41.834
easiest, because it’s earliest
in the data processing pipeline.
Um so if you’ve read about the

00:26:41.834,00:26:45.370
attack against Cylance-
Cylance’s AI powered anti virus
system that happened a few weeks

00:26:45.370,00:26:50.142
ago um that’s a perfect example
of a real world attack on an AI
system that occurred during the

00:26:50.142,00:26:56.582
feature extraction process or
like very early in the data
processing process. Um and you

00:26:56.582,00:26:59.585
should also think about the data
supply chain, so where is the
data coming from? And who's

00:26:59.585,00:27:03.355
making it? Is there a
possibility that the data that
you’re using might be

00:27:03.355,00:27:06.525
compromisable in some way and
how might that affect the
predictions that your system is

00:27:06.525,00:27:10.429
going to make? Um and then the
last and most important thing is
that you should focus on the

00:27:10.429,00:27:15.300
threat model. So circling back
to the Tesla story I told at the
beginning, um there are attacks

00:27:15.300,00:27:19.204
that are sexy, and there are
attacks that are real, um and
the adversarial machine learning

00:27:19.204,00:27:22.040
literature is focused on
understanding the quirks of
learning. Um there’s sort of

00:27:22.040,00:27:26.578
like a knowledge debt around
like, why do AIs do these
things? How are they different

00:27:26.578,00:27:30.215
from people? Like these are all
fascinating questions but
they’re not for us right now,

00:27:30.215,00:27:35.420
cause we’re interested in fixing
things that are broken. Um and
there’s a lot of extremely cool

00:27:35.420,00:27:39.024
math involved with these and I
highly recommend reading that
kind of literature cause it can

00:27:39.024,00:27:45.030
help inspire you come- come up
with new ways of attacking these
things. Um but they’re not

00:27:45.030,00:27:50.502
actually useful for doing real
security work uh so don’t
confuse algorithmic attacks um

00:27:50.502,00:27:56.008
from academic research labs with
real security threats and please
don’t red team AI like a chump.

00:27:56.008,00:28:00.946
Thank you. [laughs] [audience
applause] [laughs] Do I have
questions now? I don’t actually

00:28:11.423,00:28:16.428
know what happens? Uh oh. What
do I do? >>Does anybody have any
questions? We. [indistinct

00:28:20.132,00:28:24.303
audience member] >>[speaker
laughs] Depends on how much
you’re paying. [laughter]

00:28:24.303,00:28:29.341
>>Otherwise uh we have a few
minutes so we’ll, you can uh
talk with adversarial over here

00:28:29.341,00:28:34.780
on the side and uh we’ll get set
up for the next 1. Thanks.
[audience claps] >>Thank you,

00:28:34.780,00:28:39.718
it’s been a pleasure. Oh!
[laughs]

