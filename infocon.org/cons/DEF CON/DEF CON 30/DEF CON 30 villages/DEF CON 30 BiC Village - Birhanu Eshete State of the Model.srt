1
00:00:02,240 --> 00:00:04,560
hi this is brihan shetty i'm an

2
00:00:04,560 --> 00:00:06,399
assistant professor at the university of

3
00:00:06,399 --> 00:00:08,480
michigan in dearborn

4
00:00:08,480 --> 00:00:10,639
i'm going to talk about

5
00:00:10,639 --> 00:00:13,120
machine learning trustworthiness so the

6
00:00:13,120 --> 00:00:15,759
title of my talk is

7
00:00:15,759 --> 00:00:17,840
state of the model promising steps and

8
00:00:17,840 --> 00:00:19,920
remaining challenges towards trustworthy

9
00:00:19,920 --> 00:00:22,240
micheler

10
00:00:22,240 --> 00:00:25,199
so to set the scene here

11
00:00:25,199 --> 00:00:27,680
let's consider this setting where we

12
00:00:27,680 --> 00:00:31,039
have a supervised learning task

13
00:00:31,039 --> 00:00:34,000
and the goal is to to train a model or a

14
00:00:34,000 --> 00:00:36,880
classifier such as a spam filter or

15
00:00:36,880 --> 00:00:38,640
marvel detector or an image

16
00:00:38,640 --> 00:00:40,239
classification model

17
00:00:40,239 --> 00:00:42,719
and the way this happens is through this

18
00:00:42,719 --> 00:00:44,640
pipeline typical

19
00:00:44,640 --> 00:00:47,120
pipelining machine learning which is

20
00:00:47,120 --> 00:00:49,039
where we have a training data which is

21
00:00:49,039 --> 00:00:52,480
labeled and we will run this through a

22
00:00:52,480 --> 00:00:55,520
typical training pipeline and produce a

23
00:00:55,520 --> 00:00:57,280
model f

24
00:00:57,280 --> 00:01:00,000
and in modern days this is typically a

25
00:01:00,000 --> 00:01:02,399
deep neural network uh and then we

26
00:01:02,399 --> 00:01:05,360
deploy this model as a as

27
00:01:05,360 --> 00:01:07,520
some sort of prediction api

28
00:01:07,520 --> 00:01:10,560
and users would send inputs to this

29
00:01:10,560 --> 00:01:12,799
model so the prediction api

30
00:01:12,799 --> 00:01:16,080
and get a an output um

31
00:01:16,080 --> 00:01:19,600
or a label of a specific input

32
00:01:19,600 --> 00:01:20,400
okay

33
00:01:20,400 --> 00:01:22,960
so that's the the setup we will consider

34
00:01:22,960 --> 00:01:24,960
um so when we talk about machine

35
00:01:24,960 --> 00:01:26,320
learning the progress in machine

36
00:01:26,320 --> 00:01:27,840
learning today

37
00:01:27,840 --> 00:01:29,840
we've got two sides of the story so

38
00:01:29,840 --> 00:01:31,840
there is this exciting site where

39
00:01:31,840 --> 00:01:33,280
machine learning

40
00:01:33,280 --> 00:01:35,439
does a lot of amazing things

41
00:01:35,439 --> 00:01:38,640
and we also have the opposite or somehow

42
00:01:38,640 --> 00:01:41,040
the darker side of machine learning

43
00:01:41,040 --> 00:01:42,399
where

44
00:01:42,399 --> 00:01:44,399
the machine learning model could do a

45
00:01:44,399 --> 00:01:46,240
lot of crazy things

46
00:01:46,240 --> 00:01:48,640
so if we start with the bright side we

47
00:01:48,640 --> 00:01:50,240
have seen progress where machine

48
00:01:50,240 --> 00:01:53,280
learning models these days are good at

49
00:01:53,280 --> 00:01:55,680
uh image classification tasks in

50
00:01:55,680 --> 00:01:57,200
computer vision

51
00:01:57,200 --> 00:01:58,719
or in

52
00:01:58,719 --> 00:02:01,360
uh domains safety's safety critical

53
00:02:01,360 --> 00:02:05,119
domains like autonomous vehicles uh or

54
00:02:05,119 --> 00:02:08,479
even in domains where we do our

55
00:02:08,479 --> 00:02:10,878
translation of languages uh in the

56
00:02:10,878 --> 00:02:13,440
medical setting we've also seen models

57
00:02:13,440 --> 00:02:15,760
which are pretty close to human accuracy

58
00:02:15,760 --> 00:02:17,599
in terms of detecting

59
00:02:17,599 --> 00:02:19,200
cancer

60
00:02:19,200 --> 00:02:20,319
and

61
00:02:20,319 --> 00:02:22,480
we've also seen examples where

62
00:02:22,480 --> 00:02:24,640
our models could beat you know

63
00:02:24,640 --> 00:02:26,959
professional uh gamers

64
00:02:26,959 --> 00:02:28,640
uh and

65
00:02:28,640 --> 00:02:30,319
we all

66
00:02:30,319 --> 00:02:33,280
have recently seen examples where uh

67
00:02:33,280 --> 00:02:35,120
audio assistants or voice based

68
00:02:35,120 --> 00:02:37,120
assistance could be used for

69
00:02:37,120 --> 00:02:39,440
a number of predictive tasks in our

70
00:02:39,440 --> 00:02:40,959
houses

71
00:02:40,959 --> 00:02:43,519
so that's the the nice side

72
00:02:43,519 --> 00:02:45,120
or the great side of

73
00:02:45,120 --> 00:02:47,120
the progress we see in machine learning

74
00:02:47,120 --> 00:02:49,120
the the flip side or what i call the

75
00:02:49,120 --> 00:02:53,360
worrisome side of machine learning is

76
00:02:53,360 --> 00:02:56,480
could be summarized with one question

77
00:02:56,480 --> 00:02:58,959
here which is in the pipeline that that

78
00:02:58,959 --> 00:03:01,120
i gave you earlier what could possibly

79
00:03:01,120 --> 00:03:04,800
go wrong if we replace the user here

80
00:03:04,800 --> 00:03:08,000
with some sort of adversary who has some

81
00:03:08,000 --> 00:03:09,680
malicious intent

82
00:03:09,680 --> 00:03:10,640
right

83
00:03:10,640 --> 00:03:13,200
so because the adversary could do a you

84
00:03:13,200 --> 00:03:15,360
know a great deal of manipulations of

85
00:03:15,360 --> 00:03:18,319
the input or maybe try to manipulate the

86
00:03:18,319 --> 00:03:21,280
training data uh for fulfilling the

87
00:03:21,280 --> 00:03:22,959
adversarial goals

88
00:03:22,959 --> 00:03:25,120
so um in in the sense of this

89
00:03:25,120 --> 00:03:26,720
adversarial setting

90
00:03:26,720 --> 00:03:28,720
where uh the adversary could come with a

91
00:03:28,720 --> 00:03:31,680
lot of intentions there are a couple of

92
00:03:31,680 --> 00:03:35,200
scenarios that we've now witnessed that

93
00:03:35,200 --> 00:03:37,840
you know um machine learning is not

94
00:03:37,840 --> 00:03:40,640
secure or safe

95
00:03:40,640 --> 00:03:42,879
so the first one is what we call data

96
00:03:42,879 --> 00:03:45,599
poisoning attacks so in in this setting

97
00:03:45,599 --> 00:03:48,239
the goal of the adversary is basically

98
00:03:48,239 --> 00:03:51,040
to inject some training samples so this

99
00:03:51,040 --> 00:03:53,519
sample should be carefully crafted so

100
00:03:53,519 --> 00:03:56,239
that the the model's decision would be

101
00:03:56,239 --> 00:03:58,319
skewed or somehow

102
00:03:58,319 --> 00:03:59,439
influenced

103
00:03:59,439 --> 00:04:02,560
uh towards uh producing an output that

104
00:04:02,560 --> 00:04:04,319
the adversary wants

105
00:04:04,319 --> 00:04:07,360
so the way this happens is when machine

106
00:04:07,360 --> 00:04:10,319
learning models such as sperm filters or

107
00:04:10,319 --> 00:04:12,239
recommendation systems or malware

108
00:04:12,239 --> 00:04:13,439
detectors

109
00:04:13,439 --> 00:04:16,320
that have to collect data from untrusted

110
00:04:16,320 --> 00:04:19,199
sources and merge that collected data

111
00:04:19,199 --> 00:04:22,400
into their original training set in in

112
00:04:22,400 --> 00:04:25,040
the hopes that they would improve uh the

113
00:04:25,040 --> 00:04:27,680
accuracy of the model so what what they

114
00:04:27,680 --> 00:04:29,280
merge

115
00:04:29,280 --> 00:04:32,080
into the original training set uh

116
00:04:32,080 --> 00:04:35,199
is or could be a

117
00:04:35,199 --> 00:04:38,160
poisonous kind of training data that is

118
00:04:38,160 --> 00:04:41,680
intentionally devised by adversaries to

119
00:04:41,680 --> 00:04:44,320
to reduce the accuracy of the model or

120
00:04:44,320 --> 00:04:47,040
maybe to bias the model to specific kind

121
00:04:47,040 --> 00:04:49,120
of labels

122
00:04:49,120 --> 00:04:51,280
in real life we have seen many examples

123
00:04:51,280 --> 00:04:53,919
where training data has been poisoned

124
00:04:53,919 --> 00:04:56,639
and back in 2016 there was this uh

125
00:04:56,639 --> 00:04:58,240
infamous

126
00:04:58,240 --> 00:05:01,360
chatbot from microsoft which was train

127
00:05:01,360 --> 00:05:03,919
retraining itself from data that it was

128
00:05:03,919 --> 00:05:07,039
getting from people online and it all

129
00:05:07,039 --> 00:05:09,520
within a span of hours

130
00:05:09,520 --> 00:05:12,080
it just turned to be a very racist uh

131
00:05:12,080 --> 00:05:13,840
chatbot because

132
00:05:13,840 --> 00:05:16,000
people were feeding uh this chat with a

133
00:05:16,000 --> 00:05:18,560
lot of crazy content which is motivated

134
00:05:18,560 --> 00:05:22,400
by race or politics and things like that

135
00:05:22,400 --> 00:05:24,240
so that's about

136
00:05:24,240 --> 00:05:27,440
poisoning the other common

137
00:05:27,440 --> 00:05:28,479
attack

138
00:05:28,479 --> 00:05:30,479
or threat that we see in the machine

139
00:05:30,479 --> 00:05:33,120
learning setup here is what we call

140
00:05:33,120 --> 00:05:34,880
adversarial examples so this is a very

141
00:05:34,880 --> 00:05:36,400
famous

142
00:05:36,400 --> 00:05:39,360
attack vector where the goal here is

143
00:05:39,360 --> 00:05:43,039
given an input x the adversary would

144
00:05:43,039 --> 00:05:45,600
perturb or modify this input in a

145
00:05:45,600 --> 00:05:48,800
non-random way in a very calculated way

146
00:05:48,800 --> 00:05:50,160
to produce

147
00:05:50,160 --> 00:05:53,199
what we call an adversarial example and

148
00:05:53,199 --> 00:05:55,520
that adversarial example is going to be

149
00:05:55,520 --> 00:05:58,560
misclassified by the model so the goal

150
00:05:58,560 --> 00:06:00,319
is essentially

151
00:06:00,319 --> 00:06:03,360
by performing minimal perturbations on a

152
00:06:03,360 --> 00:06:05,360
given input let's say an image and

153
00:06:05,360 --> 00:06:07,680
without changing the visual

154
00:06:07,680 --> 00:06:09,360
you know the visual appearance of the

155
00:06:09,360 --> 00:06:10,479
image

156
00:06:10,479 --> 00:06:14,639
the adversary would fool the model

157
00:06:14,639 --> 00:06:16,960
so for this we have seen more than

158
00:06:16,960 --> 00:06:19,199
enough examples so in image

159
00:06:19,199 --> 00:06:21,520
classification people have demonstrated

160
00:06:21,520 --> 00:06:25,919
this is possible uh even in more safety

161
00:06:25,919 --> 00:06:27,120
sensitive

162
00:06:27,120 --> 00:06:29,440
domains like healthcare we have seen

163
00:06:29,440 --> 00:06:33,039
models misclassifying a tumor to the

164
00:06:33,039 --> 00:06:34,560
wrong class which means basically

165
00:06:34,560 --> 00:06:37,360
misdiagnosing a patient

166
00:06:37,360 --> 00:06:39,759
and we have also seen cases where a stop

167
00:06:39,759 --> 00:06:42,000
sign could be misclassified as something

168
00:06:42,000 --> 00:06:44,479
like a yield sign which obviously has

169
00:06:44,479 --> 00:06:46,160
safety implications in autonomous

170
00:06:46,160 --> 00:06:47,600
vehicles

171
00:06:47,600 --> 00:06:49,199
and even in

172
00:06:49,199 --> 00:06:50,160
voice

173
00:06:50,160 --> 00:06:52,639
commands we have seen how to fool voice

174
00:06:52,639 --> 00:06:55,280
commands uh in voice assistance

175
00:06:55,280 --> 00:06:58,639
and beyond computer vision or the audio

176
00:06:58,639 --> 00:07:01,280
domain we have also seen cases where

177
00:07:01,280 --> 00:07:04,720
malware detectors could be fooled by

178
00:07:04,720 --> 00:07:07,199
just minimal changes made to let's say

179
00:07:07,199 --> 00:07:10,319
android apks or windows executables

180
00:07:10,319 --> 00:07:12,319
without changing the malicious behavior

181
00:07:12,319 --> 00:07:15,199
of the samples

182
00:07:15,199 --> 00:07:17,840
and the other kind of attack is what we

183
00:07:17,840 --> 00:07:20,080
call model extraction attack uh model

184
00:07:20,080 --> 00:07:21,840
extraction is uh

185
00:07:21,840 --> 00:07:24,160
is motivated by an adversary who wants

186
00:07:24,160 --> 00:07:27,039
to either steal a model for let's say

187
00:07:27,039 --> 00:07:30,080
reasons like intellectual property or

188
00:07:30,080 --> 00:07:32,160
national security

189
00:07:32,160 --> 00:07:35,199
secrets and so on so the idea here is by

190
00:07:35,199 --> 00:07:37,440
interacting with the model

191
00:07:37,440 --> 00:07:40,080
the adversary would use the model as an

192
00:07:40,080 --> 00:07:42,960
oracle to label you know a bunch of data

193
00:07:42,960 --> 00:07:44,800
items and then

194
00:07:44,800 --> 00:07:47,759
using this label dataset the adversary

195
00:07:47,759 --> 00:07:50,160
would train what we call a substitute

196
00:07:50,160 --> 00:07:51,280
model

197
00:07:51,280 --> 00:07:53,840
and this substitutive model is

198
00:07:53,840 --> 00:07:55,840
functionally equivalent to the original

199
00:07:55,840 --> 00:07:56,840
model

200
00:07:56,840 --> 00:08:01,039
uh model and in in doing this what the

201
00:08:01,039 --> 00:08:02,960
adversary achieves is the adversary

202
00:08:02,960 --> 00:08:06,240
basically gets uh an almost uh exact

203
00:08:06,240 --> 00:08:10,400
copy of the original model and now

204
00:08:10,400 --> 00:08:13,280
uh if if the original model was best

205
00:08:13,280 --> 00:08:15,759
trend based on let's say a huge amount

206
00:08:15,759 --> 00:08:17,919
of data that was collected over a number

207
00:08:17,919 --> 00:08:19,919
of years and that happens to be an

208
00:08:19,919 --> 00:08:22,879
intellectual property or let's say even

209
00:08:22,879 --> 00:08:24,879
were in the worst case scenario it is

210
00:08:24,879 --> 00:08:26,479
based trained based on let's say

211
00:08:26,479 --> 00:08:28,080
national security

212
00:08:28,080 --> 00:08:30,000
secret of a nation

213
00:08:30,000 --> 00:08:32,240
then the adversary effectively steals

214
00:08:32,240 --> 00:08:35,120
that knowledge of that

215
00:08:35,120 --> 00:08:37,519
model

216
00:08:38,080 --> 00:08:39,360
and

217
00:08:39,360 --> 00:08:41,279
in addition to the three attacks i

218
00:08:41,279 --> 00:08:43,919
described uh model uh

219
00:08:43,919 --> 00:08:46,160
machine learning models could also be

220
00:08:46,160 --> 00:08:48,640
vulnerable to what we call membership

221
00:08:48,640 --> 00:08:50,959
inference attacks these are privacy

222
00:08:50,959 --> 00:08:53,279
motivated attacks where

223
00:08:53,279 --> 00:08:56,000
the goal of the adversary is by simply

224
00:08:56,000 --> 00:08:58,640
inspecting the model predictions

225
00:08:58,640 --> 00:09:01,279
they want to probabilistically determine

226
00:09:01,279 --> 00:09:03,519
whether a sample was used to train the

227
00:09:03,519 --> 00:09:05,920
model or not so the intuition of the

228
00:09:05,920 --> 00:09:08,640
attack here is based on how machine

229
00:09:08,640 --> 00:09:11,040
learning models generalize when when

230
00:09:11,040 --> 00:09:12,720
they are trained so

231
00:09:12,720 --> 00:09:14,720
the common observation we have about

232
00:09:14,720 --> 00:09:16,480
machine learning models these days is

233
00:09:16,480 --> 00:09:18,800
that machine learning models tend to

234
00:09:18,800 --> 00:09:20,880
overfit on their training data which

235
00:09:20,880 --> 00:09:24,000
means a model would be more confident in

236
00:09:24,000 --> 00:09:26,880
its predictions when it sees

237
00:09:26,880 --> 00:09:29,200
members of its training set so

238
00:09:29,200 --> 00:09:32,000
by exploiting this behavior or

239
00:09:32,000 --> 00:09:34,399
statistical distinguishability of how

240
00:09:34,399 --> 00:09:37,040
the model behaves on members versus

241
00:09:37,040 --> 00:09:39,440
members an adversary could build a

242
00:09:39,440 --> 00:09:42,560
probabilistic or a threshold based uh

243
00:09:42,560 --> 00:09:44,399
you know identification or inference

244
00:09:44,399 --> 00:09:47,120
model that would tell it okay this looks

245
00:09:47,120 --> 00:09:50,880
like a member of the the training set

246
00:09:50,880 --> 00:09:53,680
versus this other sample which doesn't

247
00:09:53,680 --> 00:09:56,160
have a good score of being a member so

248
00:09:56,160 --> 00:09:58,480
the implication for this is since this

249
00:09:58,480 --> 00:10:00,320
is a privacy motivated attack the

250
00:10:00,320 --> 00:10:03,200
implication for this is that

251
00:10:03,200 --> 00:10:05,440
if a model is trained on let's say

252
00:10:05,440 --> 00:10:08,480
hospital dataset or medical records uh

253
00:10:08,480 --> 00:10:10,800
if the adversary is able to identify a

254
00:10:10,800 --> 00:10:13,440
person this is obviously a privacy

255
00:10:13,440 --> 00:10:15,440
breach

256
00:10:15,440 --> 00:10:18,480
so the consequence is about is going to

257
00:10:18,480 --> 00:10:20,399
be bad obviously

258
00:10:20,399 --> 00:10:23,200
all right so so i i kind of gave you an

259
00:10:23,200 --> 00:10:25,519
overview of the four different attack

260
00:10:25,519 --> 00:10:27,760
vectors that we have

261
00:10:27,760 --> 00:10:29,600
fairly understood at this point are

262
00:10:29,600 --> 00:10:32,000
pretty important in the machine learning

263
00:10:32,000 --> 00:10:33,360
pipeline

264
00:10:33,360 --> 00:10:35,680
so what i'm going to do in this the rest

265
00:10:35,680 --> 00:10:38,560
of the talk is i'll take two of these

266
00:10:38,560 --> 00:10:39,680
four

267
00:10:39,680 --> 00:10:41,920
namely adversarial examples and

268
00:10:41,920 --> 00:10:43,839
membership inference so adversarial

269
00:10:43,839 --> 00:10:45,839
examples for fooling the model

270
00:10:45,839 --> 00:10:48,160
membership inference for basically

271
00:10:48,160 --> 00:10:49,839
inferring the

272
00:10:49,839 --> 00:10:51,279
record of

273
00:10:51,279 --> 00:10:54,720
a record used to train a model

274
00:10:54,720 --> 00:10:57,360
so in the first part i will focus on

275
00:10:57,360 --> 00:10:59,600
this moving target defense that we've

276
00:10:59,600 --> 00:11:01,519
recently developed

277
00:11:01,519 --> 00:11:04,079
to improve upon existing defenses

278
00:11:04,079 --> 00:11:06,399
against artificial examples

279
00:11:06,399 --> 00:11:10,320
and in the second part i will

280
00:11:10,320 --> 00:11:12,880
move into this other defense that

281
00:11:12,880 --> 00:11:15,040
focuses on defending models against

282
00:11:15,040 --> 00:11:16,880
membership influenced tax and this is

283
00:11:16,880 --> 00:11:18,560
based on what we call preemptive

284
00:11:18,560 --> 00:11:21,120
exclusion of the data points

285
00:11:21,120 --> 00:11:22,320
and

286
00:11:22,320 --> 00:11:25,279
in the third part the final part

287
00:11:25,279 --> 00:11:27,040
i'm going to come back and try to tie

288
00:11:27,040 --> 00:11:28,800
back what whatever i'm going to talk

289
00:11:28,800 --> 00:11:31,440
about i was the title of my talk which

290
00:11:31,440 --> 00:11:34,000
is the state of the model so where do we

291
00:11:34,000 --> 00:11:36,079
stand in the state of the robustness or

292
00:11:36,079 --> 00:11:37,279
machine learning model or

293
00:11:37,279 --> 00:11:38,640
trustworthiness or machine learning

294
00:11:38,640 --> 00:11:39,760
model

295
00:11:39,760 --> 00:11:42,560
and i'll take a broader perspective

296
00:11:42,560 --> 00:11:45,200
uh beyond the two kinds of attacks that

297
00:11:45,200 --> 00:11:46,880
i'm gonna describe and you know the

298
00:11:46,880 --> 00:11:49,519
progress we have made uh in defending

299
00:11:49,519 --> 00:11:51,839
against that personal examples and also

300
00:11:51,839 --> 00:11:53,920
membership inference

301
00:11:53,920 --> 00:11:55,920
all right so let's jump into the first

302
00:11:55,920 --> 00:11:56,959
part

303
00:11:56,959 --> 00:11:59,360
so in this first part i'm going to talk

304
00:11:59,360 --> 00:12:02,000
about this recent work that we've done

305
00:12:02,000 --> 00:12:04,480
called morphine so the idea for more

306
00:12:04,480 --> 00:12:06,959
friends is basically to make the machine

307
00:12:06,959 --> 00:12:08,480
learning model

308
00:12:08,480 --> 00:12:10,880
a moving target in in the eyes of the

309
00:12:10,880 --> 00:12:12,800
adversary

310
00:12:12,800 --> 00:12:15,600
so before i get to morphine's um i it's

311
00:12:15,600 --> 00:12:17,920
it's fair to kind of assess where we

312
00:12:17,920 --> 00:12:19,760
stand in terms of the progress we've

313
00:12:19,760 --> 00:12:22,880
made in ampersand examples uh defense

314
00:12:22,880 --> 00:12:24,399
arms atmospheres right

315
00:12:24,399 --> 00:12:25,680
so

316
00:12:25,680 --> 00:12:27,600
adversarial examples are dated back to

317
00:12:27,600 --> 00:12:30,800
early 2000s and

318
00:12:30,800 --> 00:12:33,120
the landmark paper that kind of

319
00:12:33,120 --> 00:12:34,959
introduced diversity examples in the

320
00:12:34,959 --> 00:12:38,480
sense of deep learning is that 2013 40

321
00:12:38,480 --> 00:12:40,399
uh 14 uh

322
00:12:40,399 --> 00:12:43,680
paper uh from google that looks at you

323
00:12:43,680 --> 00:12:45,680
know these intriguing properties of

324
00:12:45,680 --> 00:12:47,519
fiber cell examples where they could be

325
00:12:47,519 --> 00:12:50,800
filled with simple preservations

326
00:12:50,800 --> 00:12:52,399
after that

327
00:12:52,399 --> 00:12:54,880
defenses have emerged in many directions

328
00:12:54,880 --> 00:12:58,160
so uh the early defenses looked at

329
00:12:58,160 --> 00:13:01,040
uh gradient-based um attacks

330
00:13:01,040 --> 00:13:04,480
uh and uh they um they they wanted to

331
00:13:04,480 --> 00:13:06,160
defend against this

332
00:13:06,160 --> 00:13:07,680
gradient-based attacks because the

333
00:13:07,680 --> 00:13:09,920
attacks were exploiting the gradient

334
00:13:09,920 --> 00:13:12,800
information so the the defenses were you

335
00:13:12,800 --> 00:13:15,920
know obviously based on

336
00:13:15,920 --> 00:13:18,959
doing things like gradient masking or

337
00:13:18,959 --> 00:13:21,360
refining or pruning the model or

338
00:13:21,360 --> 00:13:24,560
performing some transformations on the

339
00:13:24,560 --> 00:13:26,320
the inputs or data

340
00:13:26,320 --> 00:13:30,880
and then around 2017 there was this

341
00:13:30,880 --> 00:13:33,760
seminal paper by carolinian wagner which

342
00:13:33,760 --> 00:13:34,800
broke

343
00:13:34,800 --> 00:13:36,959
what was the state-of-the-art

344
00:13:36,959 --> 00:13:39,440
defense at the time uh defensive

345
00:13:39,440 --> 00:13:41,519
distillation and

346
00:13:41,519 --> 00:13:44,720
later a series of

347
00:13:44,720 --> 00:13:47,279
a series of attacks by carlini and

348
00:13:47,279 --> 00:13:50,480
others broke most of the defense that we

349
00:13:50,480 --> 00:13:52,079
were proposed against adversarial

350
00:13:52,079 --> 00:13:55,600
examples around 2017

351
00:13:55,600 --> 00:13:56,800
there was

352
00:13:56,800 --> 00:13:58,800
a new defense called adversary training

353
00:13:58,800 --> 00:14:00,880
so basically training the model or

354
00:14:00,880 --> 00:14:02,480
introducing the model to adversarial

355
00:14:02,480 --> 00:14:05,040
examples uh so that when it sees the

356
00:14:05,040 --> 00:14:07,680
these examples in the future it can uh

357
00:14:07,680 --> 00:14:10,079
correctly classify them into the right

358
00:14:10,079 --> 00:14:12,480
class instead of the the adversarial

359
00:14:12,480 --> 00:14:15,040
class that the the adversary wants the

360
00:14:15,040 --> 00:14:17,120
the sample to fit into

361
00:14:17,120 --> 00:14:18,399
um

362
00:14:18,399 --> 00:14:21,279
so as we speak unprecedented training um

363
00:14:21,279 --> 00:14:24,320
is fairly fairly good but it does it

364
00:14:24,320 --> 00:14:27,279
comes with its own costs which is

365
00:14:27,279 --> 00:14:29,680
when you train a model in an adversarial

366
00:14:29,680 --> 00:14:32,160
setting you are feeding the

367
00:14:32,160 --> 00:14:34,320
the training pipeline with adversarial

368
00:14:34,320 --> 00:14:36,240
examples in addition to the original

369
00:14:36,240 --> 00:14:39,600
training points uh so uh there is a risk

370
00:14:39,600 --> 00:14:43,360
of um you know penalizing the clean uh

371
00:14:43,360 --> 00:14:45,920
clean input accuracy of the market

372
00:14:45,920 --> 00:14:48,000
um around 2019

373
00:14:48,000 --> 00:14:50,560
and quite recently we have also seen a

374
00:14:50,560 --> 00:14:53,440
surge in this direction of certified

375
00:14:53,440 --> 00:14:54,880
defenses

376
00:14:54,880 --> 00:14:56,320
where

377
00:14:56,320 --> 00:14:58,320
the idea is to provide a minimum

378
00:14:58,320 --> 00:15:00,720
robustness guarantee under a specific

379
00:15:00,720 --> 00:15:04,079
setting for example the lp norm uh

380
00:15:04,079 --> 00:15:08,399
distance measure of images um so so that

381
00:15:08,399 --> 00:15:10,720
the the the user of the model would have

382
00:15:10,720 --> 00:15:12,560
you know the

383
00:15:12,560 --> 00:15:14,800
the lower bound on the accuracy of the

384
00:15:14,800 --> 00:15:17,760
model under attack so if if i guarantee

385
00:15:17,760 --> 00:15:20,560
that the model won't go beyond let's say

386
00:15:20,560 --> 00:15:24,079
55 accuracy under attack under a

387
00:15:24,079 --> 00:15:26,399
specific class of attacks then when you

388
00:15:26,399 --> 00:15:29,199
deploy the model you already understand

389
00:15:29,199 --> 00:15:31,759
that there is a certification uh up to a

390
00:15:31,759 --> 00:15:33,360
certain degree

391
00:15:33,360 --> 00:15:34,880
and you know

392
00:15:34,880 --> 00:15:36,959
it's up to you to deploy the model uh

393
00:15:36,959 --> 00:15:39,040
given the certification i gave you so

394
00:15:39,040 --> 00:15:42,320
it's basically giving a proof or

395
00:15:42,320 --> 00:15:44,079
telling the user of the model that you

396
00:15:44,079 --> 00:15:45,600
know um

397
00:15:45,600 --> 00:15:47,680
here is a guarantee i can give you but

398
00:15:47,680 --> 00:15:49,839
it's up to you to deploy it so this

399
00:15:49,839 --> 00:15:52,000
divide defenses come with that kind of

400
00:15:52,000 --> 00:15:54,399
guarantee but they do also have their

401
00:15:54,399 --> 00:15:56,720
own limitations in terms of scalability

402
00:15:56,720 --> 00:15:59,040
and covering you know different

403
00:15:59,040 --> 00:16:02,160
classes of attacks

404
00:16:02,240 --> 00:16:03,440
all right

405
00:16:03,440 --> 00:16:04,480
so

406
00:16:04,480 --> 00:16:07,360
beyond what i said about this different

407
00:16:07,360 --> 00:16:09,199
classes of defenses

408
00:16:09,199 --> 00:16:12,000
what is common to all defenses up to

409
00:16:12,000 --> 00:16:14,399
this point up to you know certified

410
00:16:14,399 --> 00:16:17,519
defenses is that they all treat the

411
00:16:17,519 --> 00:16:21,360
model as a fixed target model so if an

412
00:16:21,360 --> 00:16:24,000
adversary tries to attack your model

413
00:16:24,000 --> 00:16:24,880
once

414
00:16:24,880 --> 00:16:27,519
uh and the attack succeeds the adversary

415
00:16:27,519 --> 00:16:29,279
can come back and attack the model

416
00:16:29,279 --> 00:16:31,199
because the model doesn't move or the

417
00:16:31,199 --> 00:16:33,839
model is is not changing the decision

418
00:16:33,839 --> 00:16:35,600
boundary of the model is basically the

419
00:16:35,600 --> 00:16:37,920
same so that's that's where we we come

420
00:16:37,920 --> 00:16:41,279
in with this new defense technique uh

421
00:16:41,279 --> 00:16:44,800
called morphines which as i said earlier

422
00:16:44,800 --> 00:16:46,880
is based on moving target

423
00:16:46,880 --> 00:16:49,600
idea so the idea here is we're going to

424
00:16:49,600 --> 00:16:52,320
deploy a pool of models

425
00:16:52,320 --> 00:16:54,560
where the decision boundaries of these

426
00:16:54,560 --> 00:16:58,000
models are slightly different but the

427
00:16:58,000 --> 00:17:00,000
overall accuracy comparison between

428
00:17:00,000 --> 00:17:02,160
these models is the same

429
00:17:02,160 --> 00:17:04,480
so what we do is when the adversary

430
00:17:04,480 --> 00:17:07,439
comes in with an input x we pick one of

431
00:17:07,439 --> 00:17:09,599
these modems and that model would

432
00:17:09,599 --> 00:17:12,240
predict the output of

433
00:17:12,240 --> 00:17:15,039
the input and then when the adversary

434
00:17:15,039 --> 00:17:17,039
comes with another input or even the

435
00:17:17,039 --> 00:17:18,480
same input again

436
00:17:18,480 --> 00:17:21,599
what we do is we keep moving this models

437
00:17:21,599 --> 00:17:24,000
or picking a different model

438
00:17:24,000 --> 00:17:26,480
which is you know as accurate as any

439
00:17:26,480 --> 00:17:28,240
other model in the pool

440
00:17:28,240 --> 00:17:29,679
so that

441
00:17:29,679 --> 00:17:32,160
the adversary who is trying to establish

442
00:17:32,160 --> 00:17:34,160
let's say the decision boundary or the

443
00:17:34,160 --> 00:17:36,480
robust the sensitivity of the model

444
00:17:36,480 --> 00:17:38,559
against different preservations would be

445
00:17:38,559 --> 00:17:41,600
discouraged and eventually give up uh in

446
00:17:41,600 --> 00:17:44,000
terms of the attack that it is planning

447
00:17:44,000 --> 00:17:46,480
to launch against this model so that's

448
00:17:46,480 --> 00:17:48,799
the idea uh so to give you a little bit

449
00:17:48,799 --> 00:17:52,720
of context as to how our approach works

450
00:17:52,720 --> 00:17:54,960
here is a brief overview so given a

451
00:17:54,960 --> 00:17:57,280
model that is trained on a specific

452
00:17:57,280 --> 00:17:58,720
training data set

453
00:17:58,720 --> 00:18:01,280
the first step that we do is what we

454
00:18:01,280 --> 00:18:04,400
call seed pole generation so as i said

455
00:18:04,400 --> 00:18:06,640
we have we have a pool of models that

456
00:18:06,640 --> 00:18:09,120
will act as a moving target for for the

457
00:18:09,120 --> 00:18:11,520
adversary so what we do is given the

458
00:18:11,520 --> 00:18:13,360
weights of the model

459
00:18:13,360 --> 00:18:15,840
what we do is we just perturb these

460
00:18:15,840 --> 00:18:18,640
models in a very bounded way

461
00:18:18,640 --> 00:18:20,720
and generate what we call

462
00:18:20,720 --> 00:18:23,120
the initial set of models

463
00:18:23,120 --> 00:18:25,679
and this initial set of models since we

464
00:18:25,679 --> 00:18:28,080
perturbed the weights

465
00:18:28,080 --> 00:18:30,480
is bound to be less accurate compared to

466
00:18:30,480 --> 00:18:32,880
the original model therefore we have

467
00:18:32,880 --> 00:18:35,200
this second step what we that we call

468
00:18:35,200 --> 00:18:38,640
seed ball retraining which is basically

469
00:18:38,640 --> 00:18:41,120
aimed for two goals the first one is of

470
00:18:41,120 --> 00:18:43,679
course we want to gain accuracy back and

471
00:18:43,679 --> 00:18:45,039
the second is

472
00:18:45,039 --> 00:18:47,280
we have to also create sufficient

473
00:18:47,280 --> 00:18:49,679
diversity between uh the individual

474
00:18:49,679 --> 00:18:52,160
models that we forked from the the

475
00:18:52,160 --> 00:18:54,720
original model because in adversarial

476
00:18:54,720 --> 00:18:55,760
example

477
00:18:55,760 --> 00:18:58,080
literature there is this phenomenon

478
00:18:58,080 --> 00:19:01,120
called transferability of attacks so if

479
00:19:01,120 --> 00:19:03,120
if you attack one model with an

480
00:19:03,120 --> 00:19:04,799
adversarial example

481
00:19:04,799 --> 00:19:07,679
chances are the the attack would work on

482
00:19:07,679 --> 00:19:09,760
another model even if these models are

483
00:19:09,760 --> 00:19:11,840
architecturally different

484
00:19:11,840 --> 00:19:16,320
so to minimize that transferability

485
00:19:16,320 --> 00:19:19,120
we also apply different transformations

486
00:19:19,120 --> 00:19:22,080
on the different the the original data

487
00:19:22,080 --> 00:19:25,600
sets so the t1 t2 up to tn here indicate

488
00:19:25,600 --> 00:19:27,919
the unique transformations that are

489
00:19:27,919 --> 00:19:31,280
aimed for getting the diverse models

490
00:19:31,280 --> 00:19:33,840
uh and after doing that we will get a

491
00:19:33,840 --> 00:19:35,840
you know a fairly diverse model and we

492
00:19:35,840 --> 00:19:38,080
will also get a

493
00:19:38,080 --> 00:19:40,000
gain back the accuracy of the individual

494
00:19:40,000 --> 00:19:41,200
models

495
00:19:41,200 --> 00:19:43,679
and then in this third stage what we do

496
00:19:43,679 --> 00:19:45,760
is what we call selective adversarial

497
00:19:45,760 --> 00:19:47,760
training so as i told you earlier the

498
00:19:47,760 --> 00:19:49,760
third training is sort of the benchmark

499
00:19:49,760 --> 00:19:52,080
defense that we use because that's as

500
00:19:52,080 --> 00:19:52,960
far as

501
00:19:52,960 --> 00:19:55,120
we knew at the time of you know

502
00:19:55,120 --> 00:19:56,799
developing morphines this was the best

503
00:19:56,799 --> 00:19:59,760
defense so what we do here is instead of

504
00:19:59,760 --> 00:20:02,080
training all of the models in in

505
00:20:02,080 --> 00:20:04,480
adversarially what we do is we pick a

506
00:20:04,480 --> 00:20:07,360
subset of the models and train them

507
00:20:07,360 --> 00:20:09,520
adversarially so i apply adversarial

508
00:20:09,520 --> 00:20:11,120
training basically we introduce these

509
00:20:11,120 --> 00:20:13,360
models to adversarial examples

510
00:20:13,360 --> 00:20:15,440
and the reason why we are selective here

511
00:20:15,440 --> 00:20:17,120
is because

512
00:20:17,120 --> 00:20:21,039
if we train every model in the pool

513
00:20:21,039 --> 00:20:23,039
with adversarial examples they would be

514
00:20:23,039 --> 00:20:24,400
great at

515
00:20:24,400 --> 00:20:26,400
catching adversarial examples but they

516
00:20:26,400 --> 00:20:28,240
would penalize

517
00:20:28,240 --> 00:20:30,960
overall accuracy on clean examples so to

518
00:20:30,960 --> 00:20:32,840
balance for that

519
00:20:32,840 --> 00:20:37,200
we we keep the the remaining subset

520
00:20:37,200 --> 00:20:40,400
of the models as is to make sure that

521
00:20:40,400 --> 00:20:42,799
you know we're not using on uh clean

522
00:20:42,799 --> 00:20:45,840
example accuracy

523
00:20:46,320 --> 00:20:47,600
and finally

524
00:20:47,600 --> 00:20:48,960
we deploy this

525
00:20:48,960 --> 00:20:50,720
pool of models which is a mix of

526
00:20:50,720 --> 00:20:52,880
adversarially trend and also the

527
00:20:52,880 --> 00:20:55,760
originally uh improved models and then

528
00:20:55,760 --> 00:20:57,919
we have what we call a scheduler which

529
00:20:57,919 --> 00:21:00,960
basically accepts an input and then

530
00:21:00,960 --> 00:21:04,000
performs the moving target aspect of the

531
00:21:04,000 --> 00:21:04,799
whole

532
00:21:04,799 --> 00:21:07,520
the whole pipeline here uh

533
00:21:07,520 --> 00:21:08,640
so

534
00:21:08,640 --> 00:21:11,200
when we do scheduling uh we have to also

535
00:21:11,200 --> 00:21:13,200
make sure that the pull the model pull

536
00:21:13,200 --> 00:21:15,440
that we produce after passing through

537
00:21:15,440 --> 00:21:17,679
these three stages should also be

538
00:21:17,679 --> 00:21:19,679
replenished or it should be renewed

539
00:21:19,679 --> 00:21:22,159
because if the model pull remains in

540
00:21:22,159 --> 00:21:24,799
static again we're back to square one

541
00:21:24,799 --> 00:21:27,039
which is you know the adversary would

542
00:21:27,039 --> 00:21:29,840
would recover or discover some

543
00:21:29,840 --> 00:21:31,760
some information about the fact that

544
00:21:31,760 --> 00:21:32,640
okay

545
00:21:32,640 --> 00:21:35,280
it looks like i have kind of saturated

546
00:21:35,280 --> 00:21:38,559
this model cool so now i know that this

547
00:21:38,559 --> 00:21:41,679
is a pool that never changes so to

548
00:21:41,679 --> 00:21:44,320
to avoid that kind of risk what we want

549
00:21:44,320 --> 00:21:45,120
is

550
00:21:45,120 --> 00:21:46,480
we every

551
00:21:46,480 --> 00:21:48,480
maximum number of queries

552
00:21:48,480 --> 00:21:50,400
we want to just

553
00:21:50,400 --> 00:21:53,919
run the pipeline one two three here and

554
00:21:53,919 --> 00:21:55,440
produce another pool of models so

555
00:21:55,440 --> 00:21:57,280
another batch of models would be

556
00:21:57,280 --> 00:22:00,159
deployed and that way the

557
00:22:00,159 --> 00:22:01,679
the

558
00:22:01,679 --> 00:22:04,720
moving target aspect will continue by

559
00:22:04,720 --> 00:22:08,080
updating the model

560
00:22:08,320 --> 00:22:11,520
okay so that is basically how our system

561
00:22:11,520 --> 00:22:14,480
orphans operates to achieve the moving

562
00:22:14,480 --> 00:22:17,760
target aspect of the defense here

563
00:22:17,760 --> 00:22:20,000
so let me give you a bit of a highlight

564
00:22:20,000 --> 00:22:21,760
of how this

565
00:22:21,760 --> 00:22:23,120
defense

566
00:22:23,120 --> 00:22:25,679
performs and how it compares against the

567
00:22:25,679 --> 00:22:28,080
state of the art uh the time which is

568
00:22:28,080 --> 00:22:29,600
adversarial training

569
00:22:29,600 --> 00:22:32,080
so here uh we've got different kinds of

570
00:22:32,080 --> 00:22:34,240
attacks so fast grading sign method

571
00:22:34,240 --> 00:22:37,520
carlini wagner and spsa so fast gradient

572
00:22:37,520 --> 00:22:39,919
sign method and carlini wagner white box

573
00:22:39,919 --> 00:22:42,720
attacks where the adversary knows um you

574
00:22:42,720 --> 00:22:45,440
know the gradient and you know uh

575
00:22:45,440 --> 00:22:48,640
details of the model while spsea is

576
00:22:48,640 --> 00:22:51,679
an iterative black rocks attack so uh

577
00:22:51,679 --> 00:22:54,720
this is the accuracy of the the model uh

578
00:22:54,720 --> 00:22:55,679
when

579
00:22:55,679 --> 00:22:58,080
there is no attack so it's a you know a

580
00:22:58,080 --> 00:23:00,159
fairly good accuracy

581
00:23:00,159 --> 00:23:02,559
of the this is by the way for the mnist

582
00:23:02,559 --> 00:23:05,440
dataset um the benchmark uh

583
00:23:05,440 --> 00:23:08,480
toy dataset for image classification

584
00:23:08,480 --> 00:23:09,919
um

585
00:23:09,919 --> 00:23:12,159
and when we apply fast grid and sign

586
00:23:12,159 --> 00:23:13,520
method um

587
00:23:13,520 --> 00:23:15,760
you can see that the accuracy just drops

588
00:23:15,760 --> 00:23:18,640
to almost 10 percent from 99 percent

589
00:23:18,640 --> 00:23:20,799
karlini wagoner basically

590
00:23:20,799 --> 00:23:24,159
paralyzes the whole model so zero

591
00:23:24,159 --> 00:23:26,640
it's a very strong white-box attack and

592
00:23:26,640 --> 00:23:29,039
spsea also

593
00:23:29,039 --> 00:23:31,280
drops the accuracy up to

594
00:23:31,280 --> 00:23:33,120
roughly 30 percent

595
00:23:33,120 --> 00:23:34,720
then when you look at adversaries

596
00:23:34,720 --> 00:23:36,240
training

597
00:23:36,240 --> 00:23:38,159
obviously it's

598
00:23:38,159 --> 00:23:40,159
it improves up on you know

599
00:23:40,159 --> 00:23:42,720
the accuracy the accuracy under attack

600
00:23:42,720 --> 00:23:46,559
uh for example for fgsm up to 42

601
00:23:46,559 --> 00:23:50,559
but it fails to recover from caroline

602
00:23:50,559 --> 00:23:52,400
wagner and

603
00:23:52,400 --> 00:23:54,320
it fairly improves

604
00:23:54,320 --> 00:23:55,919
the spsc

605
00:23:55,919 --> 00:23:57,919
from the spsa attack

606
00:23:57,919 --> 00:23:59,919
but when you look at this last column

607
00:23:59,919 --> 00:24:02,880
which is our defense morphines you can

608
00:24:02,880 --> 00:24:05,039
see that morphines doesn't really incur

609
00:24:05,039 --> 00:24:08,480
any cost on clean label uh accuracy or

610
00:24:08,480 --> 00:24:10,880
clean data accuracy uh you would see

611
00:24:10,880 --> 00:24:13,200
that uh it is almost the same as the

612
00:24:13,200 --> 00:24:15,520
original the undefended model's accuracy

613
00:24:15,520 --> 00:24:17,279
so we're good on that

614
00:24:17,279 --> 00:24:19,360
and when you compare it

615
00:24:19,360 --> 00:24:22,000
against adversarial training on all

616
00:24:22,000 --> 00:24:24,320
these attacks you can see that more

617
00:24:24,320 --> 00:24:25,760
offense

618
00:24:25,760 --> 00:24:28,880
outperforms adversary training

619
00:24:28,880 --> 00:24:32,159
with a very large margin

620
00:24:32,159 --> 00:24:34,960
um we also tested morphines on another

621
00:24:34,960 --> 00:24:36,799
benchmark data set slightly more

622
00:24:36,799 --> 00:24:39,520
complicated than c well than mnist and

623
00:24:39,520 --> 00:24:42,240
this is called z14 uh

624
00:24:42,240 --> 00:24:43,200
so

625
00:24:43,200 --> 00:24:45,200
the conclusion here is that more fans

626
00:24:45,200 --> 00:24:47,440
again outperforms adversaries training

627
00:24:47,440 --> 00:24:48,240
uh

628
00:24:48,240 --> 00:24:53,360
in all cases as it can be seen here

629
00:24:53,520 --> 00:24:56,320
so the takeaway for um

630
00:24:56,320 --> 00:25:00,000
for the morphine's evaluation is that

631
00:25:00,000 --> 00:25:02,320
the the moving target aspect is really

632
00:25:02,320 --> 00:25:03,679
working

633
00:25:03,679 --> 00:25:06,720
so what it ensures is that it is much

634
00:25:06,720 --> 00:25:08,960
more robust than a fixed target model

635
00:25:08,960 --> 00:25:11,440
obviously

636
00:25:12,640 --> 00:25:14,799
it can also prevent falling victim to

637
00:25:14,799 --> 00:25:16,559
the same attack multiple times so we

638
00:25:16,559 --> 00:25:19,440
have you know details of evaluation in

639
00:25:19,440 --> 00:25:21,200
in the actual paper

640
00:25:21,200 --> 00:25:24,960
uh and the iterative query based attacks

641
00:25:24,960 --> 00:25:28,480
uh like spsa that we have evaluated

642
00:25:28,480 --> 00:25:29,919
they are very

643
00:25:29,919 --> 00:25:32,159
unlikely to succeed in the face of a

644
00:25:32,159 --> 00:25:35,520
moving target model so we've we've

645
00:25:35,520 --> 00:25:38,880
kind of understood or you know

646
00:25:38,880 --> 00:25:40,960
learned these lessons by

647
00:25:40,960 --> 00:25:44,000
applying this moving target strategy

648
00:25:44,000 --> 00:25:45,120
against

649
00:25:45,120 --> 00:25:48,320
adversarial examples

650
00:25:48,720 --> 00:25:50,640
so the second part i'm going to talk

651
00:25:50,640 --> 00:25:52,320
about is

652
00:25:52,320 --> 00:25:55,520
about membership inference defense so we

653
00:25:55,520 --> 00:25:58,480
have this upcoming paper uh called mia

654
00:25:58,480 --> 00:26:00,159
shield

655
00:26:00,159 --> 00:26:02,159
defending membership influencer tags

656
00:26:02,159 --> 00:26:05,279
using preemptive exclusion of the member

657
00:26:05,279 --> 00:26:07,039
data points

658
00:26:07,039 --> 00:26:08,080
so

659
00:26:08,080 --> 00:26:10,320
like i did for adversarial example i

660
00:26:10,320 --> 00:26:12,480
want to highlight a little bit about the

661
00:26:12,480 --> 00:26:15,039
context here because you know

662
00:26:15,039 --> 00:26:17,760
later on when i talk about our defense

663
00:26:17,760 --> 00:26:20,000
against membership influenced tax you

664
00:26:20,000 --> 00:26:22,000
would realize how

665
00:26:22,000 --> 00:26:24,960
what we are improving on so membership

666
00:26:24,960 --> 00:26:26,400
influence attack in the context of

667
00:26:26,400 --> 00:26:29,279
machine learning was um introduced in

668
00:26:29,279 --> 00:26:31,200
back in 2017

669
00:26:31,200 --> 00:26:34,159
um and after that there were you know

670
00:26:34,159 --> 00:26:37,200
range of defense techniques that were

671
00:26:37,200 --> 00:26:38,799
proposed

672
00:26:38,799 --> 00:26:41,200
spanning different strategies so there

673
00:26:41,200 --> 00:26:44,320
is this regularization-based

674
00:26:44,320 --> 00:26:45,679
defenses

675
00:26:45,679 --> 00:26:48,640
differential privacy is another one or

676
00:26:48,640 --> 00:26:51,200
masking the confidence of the model

677
00:26:51,200 --> 00:26:53,120
remember when i when i introduced

678
00:26:53,120 --> 00:26:55,039
membership inference i said the

679
00:26:55,039 --> 00:26:57,279
adversary exploits the confidence of the

680
00:26:57,279 --> 00:26:59,520
model on members versus members to

681
00:26:59,520 --> 00:27:02,240
differentiate members from non-members

682
00:27:02,240 --> 00:27:02,960
so

683
00:27:02,960 --> 00:27:05,279
if you do some sort of masking of the

684
00:27:05,279 --> 00:27:07,039
confidence without

685
00:27:07,039 --> 00:27:08,559
of course affecting the accuracy of the

686
00:27:08,559 --> 00:27:10,720
model or the label of the model then you

687
00:27:10,720 --> 00:27:12,320
know you can you can gain back some

688
00:27:12,320 --> 00:27:14,159
robustness

689
00:27:14,159 --> 00:27:16,080
and there's also another class of

690
00:27:16,080 --> 00:27:18,399
defenses called ensemble methods which

691
00:27:18,399 --> 00:27:20,640
is basically you know stacking machine

692
00:27:20,640 --> 00:27:22,720
learning models so as to confuse the

693
00:27:22,720 --> 00:27:25,039
adversary not to establish any

694
00:27:25,039 --> 00:27:27,360
any intuition about you know whether a

695
00:27:27,360 --> 00:27:29,200
sample is a member or not

696
00:27:29,200 --> 00:27:30,960
and then lately we've got what we call

697
00:27:30,960 --> 00:27:33,279
knowledge distillation which is uh based

698
00:27:33,279 --> 00:27:34,799
on making

699
00:27:34,799 --> 00:27:36,559
basically pruning the model so that the

700
00:27:36,559 --> 00:27:38,399
model doesn't leak too much information

701
00:27:38,399 --> 00:27:41,120
about members

702
00:27:43,200 --> 00:27:45,360
similar to what i said

703
00:27:45,360 --> 00:27:47,919
for adversarial example defenses what is

704
00:27:47,919 --> 00:27:50,480
common here among all these defenses is

705
00:27:50,480 --> 00:27:51,279
that

706
00:27:51,279 --> 00:27:53,600
they are based on this idea of masking

707
00:27:53,600 --> 00:27:55,600
or concealing the presence of a member

708
00:27:55,600 --> 00:27:57,120
so basically

709
00:27:57,120 --> 00:28:00,159
all techniques up to this point

710
00:28:00,159 --> 00:28:02,640
are what they're doing is the member

711
00:28:02,640 --> 00:28:03,440
that

712
00:28:03,440 --> 00:28:05,440
the member element is in the in the

713
00:28:05,440 --> 00:28:06,480
models

714
00:28:06,480 --> 00:28:09,039
so we want to protect let's say you know

715
00:28:09,039 --> 00:28:11,120
a patient's record uh that was used to

716
00:28:11,120 --> 00:28:13,600
train them on what we do is

717
00:28:13,600 --> 00:28:16,960
we do our best so that the adversary

718
00:28:16,960 --> 00:28:19,360
doesn't doesn't really

719
00:28:19,360 --> 00:28:21,440
you know discover that the

720
00:28:21,440 --> 00:28:24,080
item or the data point is uh not in the

721
00:28:24,080 --> 00:28:25,120
model

722
00:28:25,120 --> 00:28:28,559
uh so it's basically based on masking or

723
00:28:28,559 --> 00:28:31,279
concealing the fact that the

724
00:28:31,279 --> 00:28:34,960
data point is in the in the training set

725
00:28:34,960 --> 00:28:36,559
so what we say

726
00:28:36,559 --> 00:28:38,880
or the intuition we have for our defense

727
00:28:38,880 --> 00:28:41,279
technique is so

728
00:28:41,279 --> 00:28:43,200
it is well established that the presence

729
00:28:43,200 --> 00:28:45,279
of a data point offers a strong

730
00:28:45,279 --> 00:28:48,159
membership signal for inference

731
00:28:48,159 --> 00:28:50,720
so the question we ask is how about

732
00:28:50,720 --> 00:28:52,799
excluding the data points

733
00:28:52,799 --> 00:28:54,640
of course without compromising you know

734
00:28:54,640 --> 00:28:57,120
the accuracy of the model so that this

735
00:28:57,120 --> 00:29:00,080
signal the adversary gets would be weak

736
00:29:00,080 --> 00:29:02,399
and other results maybe the attack would

737
00:29:02,399 --> 00:29:03,200
fail

738
00:29:03,200 --> 00:29:04,159
okay

739
00:29:04,159 --> 00:29:06,399
uh so basically we're we're departing

740
00:29:06,399 --> 00:29:08,320
from what has been done in the

741
00:29:08,320 --> 00:29:11,360
literature in the sense that instead of

742
00:29:11,360 --> 00:29:14,240
masking we are basically excluding the

743
00:29:14,240 --> 00:29:16,240
item or the data point

744
00:29:16,240 --> 00:29:19,679
so that any probabilistic inference

745
00:29:19,679 --> 00:29:21,840
would be at best a false positive

746
00:29:21,840 --> 00:29:24,720
because if i remove the training point

747
00:29:24,720 --> 00:29:26,399
and i still give you the the same

748
00:29:26,399 --> 00:29:28,799
utility or accuracy of the model

749
00:29:28,799 --> 00:29:32,080
uh any any conclusion uh you make as an

750
00:29:32,080 --> 00:29:34,960
adversary uh would be a false conclusion

751
00:29:34,960 --> 00:29:37,840
because the the data point doesn't exist

752
00:29:37,840 --> 00:29:39,760
okay so there's that you know

753
00:29:39,760 --> 00:29:41,679
theoretical guarantee that we don't

754
00:29:41,679 --> 00:29:43,120
prove here but

755
00:29:43,120 --> 00:29:45,840
uh we'll see how how this approach turns

756
00:29:45,840 --> 00:29:47,919
out to be

757
00:29:47,919 --> 00:29:51,120
so here is a highlight of how mia shield

758
00:29:51,120 --> 00:29:53,520
or our approach defense of a

759
00:29:53,520 --> 00:29:56,000
membership inference works so as i said

760
00:29:56,000 --> 00:29:58,640
the idea is we want to preemptively

761
00:29:58,640 --> 00:30:00,559
exclude the member

762
00:30:00,559 --> 00:30:03,039
when we respond to predictions of the

763
00:30:03,039 --> 00:30:04,960
modern

764
00:30:04,960 --> 00:30:06,720
predictions of an input

765
00:30:06,720 --> 00:30:07,760
so

766
00:30:07,760 --> 00:30:10,240
the idea is given a sensitive data set

767
00:30:10,240 --> 00:30:12,960
let's say our patient record then say d

768
00:30:12,960 --> 00:30:16,159
what we do is we want to first

769
00:30:16,159 --> 00:30:17,279
split this

770
00:30:17,279 --> 00:30:20,480
data set into disjoint subsets let's say

771
00:30:20,480 --> 00:30:22,480
d1 to dn

772
00:30:22,480 --> 00:30:26,880
and we apply data augmentation to

773
00:30:26,880 --> 00:30:30,720
gain back accuracy that would be lost to

774
00:30:30,720 --> 00:30:33,200
this disjoint splitting of the

775
00:30:33,200 --> 00:30:34,960
the original data set

776
00:30:34,960 --> 00:30:37,840
and then we train the model

777
00:30:37,840 --> 00:30:40,559
uh the individual models uh after this

778
00:30:40,559 --> 00:30:42,480
data augmentation step

779
00:30:42,480 --> 00:30:45,200
and produce n models corresponding to

780
00:30:45,200 --> 00:30:47,600
the n disjoint uh

781
00:30:47,600 --> 00:30:49,600
data data sets

782
00:30:49,600 --> 00:30:51,600
or subsets

783
00:30:51,600 --> 00:30:54,480
and once we have these models what we do

784
00:30:54,480 --> 00:30:57,039
is when we get an input

785
00:30:57,039 --> 00:30:58,960
from an adversary

786
00:30:58,960 --> 00:31:01,039
we will leverage what we call an

787
00:31:01,039 --> 00:31:03,360
exclusion oracle

788
00:31:03,360 --> 00:31:05,840
which would first decide whether the

789
00:31:05,840 --> 00:31:07,919
input x is

790
00:31:07,919 --> 00:31:09,039
uh

791
00:31:09,039 --> 00:31:11,840
is or belongs to one of the training

792
00:31:11,840 --> 00:31:14,799
sets of this models f1 to f1

793
00:31:14,799 --> 00:31:17,679
if it belongs to one of these models

794
00:31:17,679 --> 00:31:20,480
what we do is we will exclude that model

795
00:31:20,480 --> 00:31:23,039
from the ensembl prediction that we are

796
00:31:23,039 --> 00:31:25,279
going to compute over here so y equals

797
00:31:25,279 --> 00:31:30,159
in symbol of f1 to fn so if fi model fi

798
00:31:30,159 --> 00:31:31,360
contains

799
00:31:31,360 --> 00:31:34,880
the input x we will exclude model fi

800
00:31:34,880 --> 00:31:37,120
and perform the ensembl prediction or

801
00:31:37,120 --> 00:31:40,320
aggregation on the rest of the models so

802
00:31:40,320 --> 00:31:42,799
basically what we're doing here is

803
00:31:42,799 --> 00:31:45,760
since the signal the model that carries

804
00:31:45,760 --> 00:31:48,320
the signal which is the the target data

805
00:31:48,320 --> 00:31:49,840
point x here

806
00:31:49,840 --> 00:31:50,960
uh

807
00:31:50,960 --> 00:31:53,120
is is identified through this exclusion

808
00:31:53,120 --> 00:31:56,320
oracle we exclude that model from

809
00:31:56,320 --> 00:32:00,000
participating in the predictions

810
00:32:00,000 --> 00:32:02,240
we still want to maintain the you know

811
00:32:02,240 --> 00:32:04,240
the accuracy of the body

812
00:32:04,240 --> 00:32:06,480
uh so the as you might imagine the

813
00:32:06,480 --> 00:32:10,080
challenge here is as to how we have to

814
00:32:10,080 --> 00:32:13,279
design this exclusion arc because that

815
00:32:13,279 --> 00:32:15,279
seems to be sort of the the ball to make

816
00:32:15,279 --> 00:32:17,760
here if the exclusion oracle gets things

817
00:32:17,760 --> 00:32:19,919
wrong um you know the whole pipeline or

818
00:32:19,919 --> 00:32:22,559
the whole defense strategy will fade

819
00:32:22,559 --> 00:32:24,799
so in the next slides what i'm going to

820
00:32:24,799 --> 00:32:26,960
do is i'm going to give you a highlight

821
00:32:26,960 --> 00:32:29,200
of you know different strategies that we

822
00:32:29,200 --> 00:32:31,679
explored for how to implement this

823
00:32:31,679 --> 00:32:33,440
exclusion arc

824
00:32:33,440 --> 00:32:35,279
okay so we've looked at five different

825
00:32:35,279 --> 00:32:36,880
techniques

826
00:32:36,880 --> 00:32:39,120
starting with you know

827
00:32:39,120 --> 00:32:42,559
a basic naive bayes line

828
00:32:42,559 --> 00:32:44,880
so that's so the first technique that we

829
00:32:44,880 --> 00:32:47,519
used is what we call

830
00:32:47,519 --> 00:32:49,840
model confidence based explosion so what

831
00:32:49,840 --> 00:32:51,519
we do here is

832
00:32:51,519 --> 00:32:53,279
to exclude

833
00:32:53,279 --> 00:32:55,360
one of this models from the ensembl

834
00:32:55,360 --> 00:32:56,480
prediction

835
00:32:56,480 --> 00:32:58,159
what we do is

836
00:32:58,159 --> 00:32:59,120
we

837
00:32:59,120 --> 00:33:02,000
exclude the most confident model on a

838
00:33:02,000 --> 00:33:05,039
given input so given an input we look at

839
00:33:05,039 --> 00:33:07,039
the confidence of all these models we

840
00:33:07,039 --> 00:33:09,039
exclude the most confident and the

841
00:33:09,039 --> 00:33:11,200
intuition here is that it goes back to

842
00:33:11,200 --> 00:33:13,600
the the original intuition of

843
00:33:13,600 --> 00:33:15,679
what is the root cause for membership in

844
00:33:15,679 --> 00:33:17,360
terms of tax

845
00:33:17,360 --> 00:33:18,960
it is the fact that models are more

846
00:33:18,960 --> 00:33:21,039
confident on their members

847
00:33:21,039 --> 00:33:24,080
so they must come by the same token this

848
00:33:24,080 --> 00:33:25,760
the most confident model in this

849
00:33:25,760 --> 00:33:28,559
scenario is likely the model that

850
00:33:28,559 --> 00:33:30,480
contains the input

851
00:33:30,480 --> 00:33:32,640
okay so that's the basis for using this

852
00:33:32,640 --> 00:33:34,240
strategy

853
00:33:34,240 --> 00:33:37,440
as it turns out uh the limitation

854
00:33:37,440 --> 00:33:39,200
of this technique is that the most

855
00:33:39,200 --> 00:33:42,000
confident model is not necessarily the

856
00:33:42,000 --> 00:33:44,080
most the model that is trained on the

857
00:33:44,080 --> 00:33:46,799
target data points so we've empirically

858
00:33:46,799 --> 00:33:49,600
uh validated that this although this is

859
00:33:49,600 --> 00:33:52,080
a good baseline or starting point as as

860
00:33:52,080 --> 00:33:55,279
a an oracle exclusion oracle

861
00:33:55,279 --> 00:33:57,279
it's not the best

862
00:33:57,279 --> 00:33:59,279
so as a result we have to explore other

863
00:33:59,279 --> 00:34:00,720
alternatives

864
00:34:00,720 --> 00:34:02,000
so the next

865
00:34:02,000 --> 00:34:04,320
alternative exclusion strategy we looked

866
00:34:04,320 --> 00:34:06,880
at is what we call exact matching based

867
00:34:06,880 --> 00:34:10,159
exclusion so the idea here is

868
00:34:10,159 --> 00:34:12,000
since we can compute

869
00:34:12,000 --> 00:34:14,239
you know hash values of like

870
00:34:14,239 --> 00:34:15,839
cryptographic hash values of the

871
00:34:15,839 --> 00:34:19,760
individual data points in each subset

872
00:34:19,760 --> 00:34:22,560
what we did was we com we we basically

873
00:34:22,560 --> 00:34:26,399
compare the inputs hash value to

874
00:34:26,399 --> 00:34:28,239
the hash values of the data points in

875
00:34:28,239 --> 00:34:29,520
each subset

876
00:34:29,520 --> 00:34:31,918
so whenever we find a match we exclude

877
00:34:31,918 --> 00:34:34,320
the model trend on a sample that exactly

878
00:34:34,320 --> 00:34:37,760
matches the inputs okay

879
00:34:37,760 --> 00:34:39,280
since this is an exact matching

880
00:34:39,280 --> 00:34:41,599
technique the limitation the obvious

881
00:34:41,599 --> 00:34:43,599
limitation for this is

882
00:34:43,599 --> 00:34:45,520
when you slightly manipulate an input

883
00:34:45,520 --> 00:34:47,679
specific especially an image

884
00:34:47,679 --> 00:34:49,520
that slide manipulation let's say one

885
00:34:49,520 --> 00:34:54,079
pixel change would mislead this um arc

886
00:34:54,079 --> 00:34:56,639
so it's not it's not a very effective

887
00:34:56,639 --> 00:34:58,880
technique it works when the exact margin

888
00:34:58,880 --> 00:35:02,400
is found but it it fails when

889
00:35:02,400 --> 00:35:03,119
the

890
00:35:03,119 --> 00:35:05,359
slide manipulation is there

891
00:35:05,359 --> 00:35:06,880
so

892
00:35:06,880 --> 00:35:09,839
so incrementally we went to the third

893
00:35:09,839 --> 00:35:12,000
alternative which is instead of exact

894
00:35:12,000 --> 00:35:13,680
matching why don't we do approximate

895
00:35:13,680 --> 00:35:16,800
matching based exclusion so

896
00:35:16,800 --> 00:35:18,800
the difference here is that we just

897
00:35:18,800 --> 00:35:21,040
exclude the model trend on a sample that

898
00:35:21,040 --> 00:35:23,440
approximately matches the input and one

899
00:35:23,440 --> 00:35:26,160
way to do this approximate matching is

900
00:35:26,160 --> 00:35:29,440
still on a hash value comparison or hash

901
00:35:29,440 --> 00:35:31,119
value lookup but

902
00:35:31,119 --> 00:35:32,160
the

903
00:35:32,160 --> 00:35:36,640
hash function or method we use is a

904
00:35:36,640 --> 00:35:39,200
perceptual hashing technique instead of

905
00:35:39,200 --> 00:35:40,079
the

906
00:35:40,079 --> 00:35:42,839
cryptographic hashing techniques

907
00:35:42,839 --> 00:35:44,400
um

908
00:35:44,400 --> 00:35:47,280
while this improves upon the exact

909
00:35:47,280 --> 00:35:49,839
matching technique because it catches

910
00:35:49,839 --> 00:35:52,640
you know uh certain

911
00:35:52,640 --> 00:35:56,079
images or model samples in within a

912
00:35:56,079 --> 00:35:59,440
threshold of a specific distance this

913
00:35:59,440 --> 00:36:01,839
the limitation is that data points that

914
00:36:01,839 --> 00:36:05,040
fall outside the distance threshold uh

915
00:36:05,040 --> 00:36:07,839
will be again missed by the org

916
00:36:07,839 --> 00:36:10,240
okay so

917
00:36:10,240 --> 00:36:11,680
next up is

918
00:36:11,680 --> 00:36:12,800
okay

919
00:36:12,800 --> 00:36:14,720
um so we've looked at this exact

920
00:36:14,720 --> 00:36:16,320
matching and approximate matching

921
00:36:16,320 --> 00:36:18,560
techniques but the

922
00:36:18,560 --> 00:36:19,920
the

923
00:36:19,920 --> 00:36:22,400
how about about looking at

924
00:36:22,400 --> 00:36:24,960
the exclusion itself as a classification

925
00:36:24,960 --> 00:36:26,640
problem right so probabilistic

926
00:36:26,640 --> 00:36:28,800
classification problem so what we do

927
00:36:28,800 --> 00:36:30,240
here is

928
00:36:30,240 --> 00:36:32,320
uh we want to predict

929
00:36:32,320 --> 00:36:34,720
the model to exclude so for which we

930
00:36:34,720 --> 00:36:37,599
have to train the oracle uh the model

931
00:36:37,599 --> 00:36:39,599
for the oracle itself right

932
00:36:39,599 --> 00:36:43,839
so what we do here is yeah so we get

933
00:36:43,839 --> 00:36:46,000
a portion of the the subsets of the

934
00:36:46,000 --> 00:36:51,680
datasets and we we use these datasets to

935
00:36:51,680 --> 00:36:54,160
generate feature vectors that would be

936
00:36:54,160 --> 00:36:56,960
used as the basis for

937
00:36:56,960 --> 00:36:59,119
establishing different classes of models

938
00:36:59,119 --> 00:37:00,800
so if you have n models we're going to

939
00:37:00,800 --> 00:37:02,400
have n

940
00:37:02,400 --> 00:37:03,440
labels

941
00:37:03,440 --> 00:37:05,680
uh on which we're going to train

942
00:37:05,680 --> 00:37:06,480
this

943
00:37:06,480 --> 00:37:09,599
classifier based exclusion strategy

944
00:37:09,599 --> 00:37:11,040
um the

945
00:37:11,040 --> 00:37:13,920
the natural limitation while it is you

946
00:37:13,920 --> 00:37:16,880
know it it improves upon the limitations

947
00:37:16,880 --> 00:37:19,760
on uh it or it solves the pro the the

948
00:37:19,760 --> 00:37:20,960
issues with

949
00:37:20,960 --> 00:37:23,760
the other two or three techniques i took

950
00:37:23,760 --> 00:37:25,760
the issue with this one is it can be

951
00:37:25,760 --> 00:37:28,240
over feed on members because

952
00:37:28,240 --> 00:37:31,040
you're training a model

953
00:37:31,040 --> 00:37:33,839
and we have already said that the the

954
00:37:33,839 --> 00:37:37,040
root cause for membership influence is

955
00:37:37,040 --> 00:37:38,640
you know the fact that models are over

956
00:37:38,640 --> 00:37:41,040
feed on their members so it might suffer

957
00:37:41,040 --> 00:37:42,880
or it might inherit you know the same

958
00:37:42,880 --> 00:37:44,079
issue

959
00:37:44,079 --> 00:37:46,480
okay so that's sort of the design you

960
00:37:46,480 --> 00:37:48,480
know the limitation by design but

961
00:37:48,480 --> 00:37:50,079
empirically

962
00:37:50,079 --> 00:37:52,160
as i will show later it's it's much

963
00:37:52,160 --> 00:37:54,640
better than the other ones

964
00:37:54,640 --> 00:37:56,480
and the other alternative

965
00:37:56,480 --> 00:37:58,079
that also

966
00:37:58,079 --> 00:38:00,000
you know speaks to the limitation of

967
00:38:00,000 --> 00:38:02,800
this classifier-based oracle is

968
00:38:02,800 --> 00:38:06,400
uh how about connecting or using what we

969
00:38:06,400 --> 00:38:09,119
call a chain of oracles so basically

970
00:38:09,119 --> 00:38:09,920
we

971
00:38:09,920 --> 00:38:12,240
we query exclusion oracles

972
00:38:12,240 --> 00:38:13,920
progressively instead of just picking

973
00:38:13,920 --> 00:38:15,440
one of them so

974
00:38:15,440 --> 00:38:18,400
we first start with the exact matching

975
00:38:18,400 --> 00:38:20,320
and when it doesn't find the match

976
00:38:20,320 --> 00:38:22,079
instead of giving up we go to the

977
00:38:22,079 --> 00:38:23,680
approximate matching

978
00:38:23,680 --> 00:38:26,320
and when it doesn't find a match again

979
00:38:26,320 --> 00:38:29,119
instead of giving up there we go to the

980
00:38:29,119 --> 00:38:31,200
classifier-based

981
00:38:31,200 --> 00:38:34,160
oracle or exclusion as a last resort

982
00:38:34,160 --> 00:38:35,200
okay

983
00:38:35,200 --> 00:38:37,839
so this also turns out to be a much

984
00:38:37,839 --> 00:38:39,440
better technique

985
00:38:39,440 --> 00:38:42,160
empirically speaking

986
00:38:42,160 --> 00:38:45,520
so let me uh give you a highlight of you

987
00:38:45,520 --> 00:38:47,599
know how this performs so i gave you

988
00:38:47,599 --> 00:38:48,400
like

989
00:38:48,400 --> 00:38:50,240
an overview of the five different

990
00:38:50,240 --> 00:38:53,359
techniques that we explored right so the

991
00:38:53,359 --> 00:38:56,240
model confidence based exclusion

992
00:38:56,240 --> 00:38:59,040
the exact signature based exclusion

993
00:38:59,040 --> 00:39:02,400
approximate signature based exclusion uh

994
00:39:02,400 --> 00:39:05,280
classifier-based exclusion and uh chain

995
00:39:05,280 --> 00:39:07,839
of exclusion oracle so these are the

996
00:39:07,839 --> 00:39:10,000
five um

997
00:39:10,000 --> 00:39:12,640
exclusion strategies that we have tested

998
00:39:12,640 --> 00:39:15,920
so what we have here is uh results

999
00:39:15,920 --> 00:39:18,880
of our defense or the different other

1000
00:39:18,880 --> 00:39:21,359
the performance of the different oracles

1001
00:39:21,359 --> 00:39:24,640
against the undefended model uh for two

1002
00:39:24,640 --> 00:39:27,520
data sets on the left is c10 data set

1003
00:39:27,520 --> 00:39:29,760
and on the right is chm list which is a

1004
00:39:29,760 --> 00:39:32,880
sort of a benchmark medical uh

1005
00:39:32,880 --> 00:39:35,200
image classification dataset

1006
00:39:35,200 --> 00:39:37,280
so on the x-axis we've got model

1007
00:39:37,280 --> 00:39:39,680
accuracy and on the y-axis we've got the

1008
00:39:39,680 --> 00:39:42,720
accuracy of the attack and the baseline

1009
00:39:42,720 --> 00:39:45,680
for the attack accuracy is 50

1010
00:39:45,680 --> 00:39:46,880
so if

1011
00:39:46,880 --> 00:39:49,760
if the adversary gets on average at 50

1012
00:39:49,760 --> 00:39:52,720
attack accuracy on a on a batch of let's

1013
00:39:52,720 --> 00:39:54,400
say inputs

1014
00:39:54,400 --> 00:39:56,160
it doesn't say much about the

1015
00:39:56,160 --> 00:39:58,960
effectiveness of the attack because it's

1016
00:39:58,960 --> 00:40:01,760
it's equal to random gas so that's why

1017
00:40:01,760 --> 00:40:05,119
we're uh fixing the the baseline at 50

1018
00:40:05,119 --> 00:40:08,160
so the closer the attack accuracy to 50

1019
00:40:08,160 --> 00:40:11,200
percent uh is

1020
00:40:11,200 --> 00:40:13,440
is good for the defense it indicates a

1021
00:40:13,440 --> 00:40:16,880
good defense uh and of course when you

1022
00:40:16,880 --> 00:40:18,880
talk about attack accuracy we have to

1023
00:40:18,880 --> 00:40:21,920
also look at it uh with respect to model

1024
00:40:21,920 --> 00:40:24,079
utility or model test accuracy so that's

1025
00:40:24,079 --> 00:40:25,920
why every time we measure the

1026
00:40:25,920 --> 00:40:28,720
effectiveness of a defense or even an

1027
00:40:28,720 --> 00:40:31,680
attack we have to look at

1028
00:40:31,680 --> 00:40:34,240
the trade-off between model accuracy and

1029
00:40:34,240 --> 00:40:35,280
attack

1030
00:40:35,280 --> 00:40:38,319
effectiveness so while keeping the model

1031
00:40:38,319 --> 00:40:40,160
the original model accuracy if the

1032
00:40:40,160 --> 00:40:42,880
attack could be dropped into uh the

1033
00:40:42,880 --> 00:40:45,200
baseline which is a 50 or randomness

1034
00:40:45,200 --> 00:40:47,599
then we can consider that defense to be

1035
00:40:47,599 --> 00:40:49,760
a good defense because without

1036
00:40:49,760 --> 00:40:53,040
compromising the utility of the model we

1037
00:40:53,040 --> 00:40:57,040
maintain that the attack won't succeed

1038
00:40:57,040 --> 00:40:58,960
so the the blue circle here is then

1039
00:40:58,960 --> 00:41:01,599
defended model so that is the accuracy

1040
00:41:01,599 --> 00:41:03,839
somewhere between 60 and 80

1041
00:41:03,839 --> 00:41:06,800
and as you can see all the other

1042
00:41:06,800 --> 00:41:10,160
points are our exclusion oracles which

1043
00:41:10,160 --> 00:41:13,119
are not too far in terms of

1044
00:41:13,119 --> 00:41:15,920
model test accuracy which is good news

1045
00:41:15,920 --> 00:41:17,119
and

1046
00:41:17,119 --> 00:41:19,119
interestingly they are almost on the

1047
00:41:19,119 --> 00:41:22,560
baseline or on the 50 percent randomness

1048
00:41:22,560 --> 00:41:24,400
attack accuracy which shows that the

1049
00:41:24,400 --> 00:41:26,800
attack is essentially failing so that

1050
00:41:26,800 --> 00:41:28,480
tells you that you know the defense is

1051
00:41:28,480 --> 00:41:29,359
super

1052
00:41:29,359 --> 00:41:31,280
uh effective

1053
00:41:31,280 --> 00:41:33,040
on the right hand side you would see the

1054
00:41:33,040 --> 00:41:35,200
same kind of story except that this is a

1055
00:41:35,200 --> 00:41:37,200
different data set which happens to be a

1056
00:41:37,200 --> 00:41:41,119
medical image classification

1057
00:41:41,119 --> 00:41:43,839
the next thing we did was okay so it

1058
00:41:43,839 --> 00:41:45,280
looks like

1059
00:41:45,280 --> 00:41:47,920
you know our defense mia shield is good

1060
00:41:47,920 --> 00:41:48,880
enough

1061
00:41:48,880 --> 00:41:51,119
in terms of

1062
00:41:51,119 --> 00:41:52,319
basically

1063
00:41:52,319 --> 00:41:53,920
mitigating the membership influence

1064
00:41:53,920 --> 00:41:55,599
attack and bringing

1065
00:41:55,599 --> 00:41:58,079
the accuracy of the attack down to

1066
00:41:58,079 --> 00:41:59,440
randomness

1067
00:41:59,440 --> 00:42:02,880
then we looked at the defenses that

1068
00:42:02,880 --> 00:42:04,800
were proposed over the years over the

1069
00:42:04,800 --> 00:42:06,800
last five years or so

1070
00:42:06,800 --> 00:42:09,119
and these defenses as i told you earlier

1071
00:42:09,119 --> 00:42:11,359
fall into different categories some are

1072
00:42:11,359 --> 00:42:13,200
based on differential privacy others are

1073
00:42:13,200 --> 00:42:14,560
based on

1074
00:42:14,560 --> 00:42:16,319
confidence masking while others are

1075
00:42:16,319 --> 00:42:18,720
based on model stacking etc and

1076
00:42:18,720 --> 00:42:20,720
regularization also

1077
00:42:20,720 --> 00:42:23,040
so uh we did the same kind of evaluation

1078
00:42:23,040 --> 00:42:25,119
so we tested the trade-off between model

1079
00:42:25,119 --> 00:42:27,280
accuracy and attack

1080
00:42:27,280 --> 00:42:29,119
effectiveness

1081
00:42:29,119 --> 00:42:31,839
against our defense miashield which is

1082
00:42:31,839 --> 00:42:35,119
the green square and all other defenses

1083
00:42:35,119 --> 00:42:36,240
like uh

1084
00:42:36,240 --> 00:42:38,480
dps gds this is differentially private

1085
00:42:38,480 --> 00:42:41,280
stochastic gradient descent pate is also

1086
00:42:41,280 --> 00:42:44,400
another ensemble based

1087
00:42:44,400 --> 00:42:45,839
differentially built differential

1088
00:42:45,839 --> 00:42:47,520
privacy-based technique and model

1089
00:42:47,520 --> 00:42:49,599
stacking memguard is a confidence

1090
00:42:49,599 --> 00:42:51,599
masking based model stacking is an

1091
00:42:51,599 --> 00:42:54,880
ensemble method and mmd mix-up is a

1092
00:42:54,880 --> 00:42:56,480
regularization with so as you can see

1093
00:42:56,480 --> 00:42:59,920
we've considered you know a span of

1094
00:42:59,920 --> 00:43:01,119
model

1095
00:43:01,119 --> 00:43:03,119
membership influence defense techniques

1096
00:43:03,119 --> 00:43:05,280
across the board over the last five or

1097
00:43:05,280 --> 00:43:06,400
so years

1098
00:43:06,400 --> 00:43:07,119
so

1099
00:43:07,119 --> 00:43:08,880
compared to the baseline which is the 50

1100
00:43:08,880 --> 00:43:11,040
percent as you can see

1101
00:43:11,040 --> 00:43:13,920
our defense mere shield is right here so

1102
00:43:13,920 --> 00:43:15,920
right on the line in terms of accuracy

1103
00:43:15,920 --> 00:43:17,760
which means it basically

1104
00:43:17,760 --> 00:43:19,440
mitigates the attack

1105
00:43:19,440 --> 00:43:22,400
and compared to the undefended model's

1106
00:43:22,400 --> 00:43:24,640
accuracy mia shield is almost aligned

1107
00:43:24,640 --> 00:43:26,960
with the undefended model's accuracy

1108
00:43:26,960 --> 00:43:30,240
which means without losing accuracy uh

1109
00:43:30,240 --> 00:43:32,960
significantly it is providing a

1110
00:43:32,960 --> 00:43:36,160
randomness kind of attack uh

1111
00:43:36,160 --> 00:43:39,119
attack protection here

1112
00:43:39,119 --> 00:43:40,880
there are some other techniques like

1113
00:43:40,880 --> 00:43:42,960
model stacking and vanguard which are

1114
00:43:42,960 --> 00:43:45,680
pretty close and also the mmd technique

1115
00:43:45,680 --> 00:43:47,520
here the regularization-based technique

1116
00:43:47,520 --> 00:43:50,079
which are close to mia shield but as you

1117
00:43:50,079 --> 00:43:53,280
can see uh their attack accuracy on each

1118
00:43:53,280 --> 00:43:55,599
of them is slightly higher

1119
00:43:55,599 --> 00:43:57,599
uh although they are pretty close to the

1120
00:43:57,599 --> 00:44:00,160
in terms of the model accuracy so

1121
00:44:00,160 --> 00:44:01,520
overall our

1122
00:44:01,520 --> 00:44:04,240
our defense still is the winner

1123
00:44:04,240 --> 00:44:06,400
and more interestingly when we look at

1124
00:44:06,400 --> 00:44:08,000
the differential privacy-based

1125
00:44:08,000 --> 00:44:11,200
techniques dpsgd and pate

1126
00:44:11,200 --> 00:44:13,760
they are super

1127
00:44:13,760 --> 00:44:15,680
comparable in

1128
00:44:15,680 --> 00:44:18,079
in attack

1129
00:44:18,079 --> 00:44:21,200
in in basically mitigating the attack

1130
00:44:21,200 --> 00:44:22,960
and so they're almost the same as mia

1131
00:44:22,960 --> 00:44:24,480
shield but

1132
00:44:24,480 --> 00:44:26,640
as you can see the gap between the blue

1133
00:44:26,640 --> 00:44:29,839
circles horizontally and this

1134
00:44:29,839 --> 00:44:31,680
green triangle and the red square which

1135
00:44:31,680 --> 00:44:33,760
is dpsgd and

1136
00:44:33,760 --> 00:44:35,119
and pate

1137
00:44:35,119 --> 00:44:37,599
you can see that differential privacy

1138
00:44:37,599 --> 00:44:39,200
based methods

1139
00:44:39,200 --> 00:44:41,520
they cost a lot on

1140
00:44:41,520 --> 00:44:43,680
the utility of the model so that's where

1141
00:44:43,680 --> 00:44:46,720
our defense is overall much better than

1142
00:44:46,720 --> 00:44:48,880
almost every every model that we have

1143
00:44:48,880 --> 00:44:51,520
tried here every uh defense we tried

1144
00:44:51,520 --> 00:44:53,839
here the same is true for the other kind

1145
00:44:53,839 --> 00:44:56,400
of data sets the hmnes stars the story

1146
00:44:56,400 --> 00:44:58,640
is more or less the same here except

1147
00:44:58,640 --> 00:45:01,359
minor differences

1148
00:45:01,359 --> 00:45:04,400
uh this the third thing we did uh in

1149
00:45:04,400 --> 00:45:06,640
addition to comparing um our defense

1150
00:45:06,640 --> 00:45:09,680
against existing defense is any defense

1151
00:45:09,680 --> 00:45:12,560
uh has to be tried or tested against you

1152
00:45:12,560 --> 00:45:14,960
know a possible adaptive attacks

1153
00:45:14,960 --> 00:45:15,839
so

1154
00:45:15,839 --> 00:45:16,800
one of

1155
00:45:16,800 --> 00:45:18,880
the the natural adaptive attacks that we

1156
00:45:18,880 --> 00:45:21,680
anticipate against our defense is

1157
00:45:21,680 --> 00:45:23,520
especially in the sense of

1158
00:45:23,520 --> 00:45:25,920
you know perceptual hashing

1159
00:45:25,920 --> 00:45:28,800
exclusion oracles is that an adversary

1160
00:45:28,800 --> 00:45:31,119
which uh keep just

1161
00:45:31,119 --> 00:45:33,680
keep perturbing or manipulating this uh

1162
00:45:33,680 --> 00:45:37,280
samples until you know the sample uh is

1163
00:45:37,280 --> 00:45:40,000
mis classified differently by the model

1164
00:45:40,000 --> 00:45:40,800
so

1165
00:45:40,800 --> 00:45:43,200
for that we've looked at a range of you

1166
00:45:43,200 --> 00:45:44,640
know

1167
00:45:44,640 --> 00:45:46,240
data augmentation or manipulation

1168
00:45:46,240 --> 00:45:47,760
techniques that the adversary would

1169
00:45:47,760 --> 00:45:49,599
likely perform

1170
00:45:49,599 --> 00:45:53,280
specifically by rotating or

1171
00:45:53,280 --> 00:45:56,240
translating inputs

1172
00:45:56,240 --> 00:45:57,839
so here we have got

1173
00:45:57,839 --> 00:46:00,800
results on the left for cfr 10 and the

1174
00:46:00,800 --> 00:46:04,000
right chm list so that the blue line is

1175
00:46:04,000 --> 00:46:07,920
our defense and the black line is

1176
00:46:07,920 --> 00:46:10,560
the baseline and the red line shows the

1177
00:46:10,560 --> 00:46:13,680
defended model's accuracy as

1178
00:46:13,680 --> 00:46:16,720
accuracy versus

1179
00:46:16,720 --> 00:46:20,319
accurate attack attack the the attack

1180
00:46:20,319 --> 00:46:22,880
the attack accuracy versus

1181
00:46:22,880 --> 00:46:25,119
this manipulation parameter so as you

1182
00:46:25,119 --> 00:46:29,839
can see our our method or our defense uh

1183
00:46:29,839 --> 00:46:32,720
maintains the attack accuracy while the

1184
00:46:32,720 --> 00:46:35,280
attack accuracy of the defended uh

1185
00:46:35,280 --> 00:46:38,240
against the undefended model is uh much

1186
00:46:38,240 --> 00:46:42,160
higher uh in both cases so this this

1187
00:46:42,160 --> 00:46:43,520
shows

1188
00:46:43,520 --> 00:46:44,480
that

1189
00:46:44,480 --> 00:46:47,119
adaptive attacks like this one uh

1190
00:46:47,119 --> 00:46:48,560
which are based on manipulation of

1191
00:46:48,560 --> 00:46:49,440
inputs

1192
00:46:49,440 --> 00:46:52,960
may not succeed overall

1193
00:46:52,960 --> 00:46:55,440
so as a takeaway

1194
00:46:55,440 --> 00:46:58,480
for this membership inference defense so

1195
00:46:58,480 --> 00:47:00,319
basically the membership inference

1196
00:47:00,319 --> 00:47:02,720
defense that we proposed which is based

1197
00:47:02,720 --> 00:47:04,880
on this elimination of the

1198
00:47:04,880 --> 00:47:05,920
signal

1199
00:47:05,920 --> 00:47:09,680
uh it it it works and

1200
00:47:09,680 --> 00:47:13,440
we have seen this across the board for

1201
00:47:13,440 --> 00:47:14,400
you know

1202
00:47:14,400 --> 00:47:16,160
multiple data sets

1203
00:47:16,160 --> 00:47:18,319
and also

1204
00:47:18,319 --> 00:47:21,119
compared it against existing defenses

1205
00:47:21,119 --> 00:47:24,319
and overall it it provides much better

1206
00:47:24,319 --> 00:47:27,440
utility privacy trade-off and

1207
00:47:27,440 --> 00:47:29,520
as i just showed you in the previous

1208
00:47:29,520 --> 00:47:32,960
slide it also remains resilient against

1209
00:47:32,960 --> 00:47:36,000
an adaptive adversary that basically

1210
00:47:36,000 --> 00:47:38,319
monetizes you know their knowledge about

1211
00:47:38,319 --> 00:47:41,040
how our exclusion oracles might have

1212
00:47:41,040 --> 00:47:43,119
been designed

1213
00:47:43,119 --> 00:47:46,160
all right so those are the two um

1214
00:47:46,160 --> 00:47:47,200
the two

1215
00:47:47,200 --> 00:47:49,920
lines of works that i kind of described

1216
00:47:49,920 --> 00:47:52,559
so how does this fit into the way i

1217
00:47:52,559 --> 00:47:54,160
started the talk which is you know the

1218
00:47:54,160 --> 00:47:56,319
state of the model you know what is what

1219
00:47:56,319 --> 00:47:58,240
are the progresses we've made and what

1220
00:47:58,240 --> 00:48:00,880
are the open you know problems we have

1221
00:48:00,880 --> 00:48:03,119
uh in terms of this

1222
00:48:03,119 --> 00:48:04,880
the grand scheme of making machine

1223
00:48:04,880 --> 00:48:07,760
learning models trustworthy

1224
00:48:07,760 --> 00:48:08,960
so

1225
00:48:08,960 --> 00:48:10,640
i took two

1226
00:48:10,640 --> 00:48:12,480
threads against machine learning models

1227
00:48:12,480 --> 00:48:14,480
adversarial examples and membership

1228
00:48:14,480 --> 00:48:17,359
inference and tried to kind of walk you

1229
00:48:17,359 --> 00:48:18,240
through

1230
00:48:18,240 --> 00:48:20,240
what has been done and what we have

1231
00:48:20,240 --> 00:48:22,240
added on top of uh

1232
00:48:22,240 --> 00:48:24,720
the existing uh state of the art for

1233
00:48:24,720 --> 00:48:25,920
defenses

1234
00:48:25,920 --> 00:48:26,880
okay

1235
00:48:26,880 --> 00:48:30,000
um so now for the last part of the talk

1236
00:48:30,000 --> 00:48:32,319
what i'm gonna do is i'm gonna take a

1237
00:48:32,319 --> 00:48:34,480
much broader take on what we call

1238
00:48:34,480 --> 00:48:36,319
trustworthy machine learning so i only

1239
00:48:36,319 --> 00:48:37,920
looked at the two dimensions and they're

1240
00:48:37,920 --> 00:48:40,079
saying robustness against adversarial

1241
00:48:40,079 --> 00:48:41,839
examples and

1242
00:48:41,839 --> 00:48:43,760
robustness against membership inference

1243
00:48:43,760 --> 00:48:46,400
attack which are important and we have

1244
00:48:46,400 --> 00:48:48,800
to we have to do this kind of work

1245
00:48:48,800 --> 00:48:49,680
but

1246
00:48:49,680 --> 00:48:51,599
trustworthiness of machine learning is

1247
00:48:51,599 --> 00:48:53,040
not all about

1248
00:48:53,040 --> 00:48:54,880
robustness against

1249
00:48:54,880 --> 00:48:57,520
this adversary inputs or robustness

1250
00:48:57,520 --> 00:49:00,000
against this privacy motivated attacks

1251
00:49:00,000 --> 00:49:02,240
like membership influence it's much

1252
00:49:02,240 --> 00:49:04,240
broader than that right

1253
00:49:04,240 --> 00:49:05,520
so

1254
00:49:05,520 --> 00:49:08,800
uh on this slide i'll just take a step

1255
00:49:08,800 --> 00:49:11,520
back and try to kind of summarize the

1256
00:49:11,520 --> 00:49:13,839
progress we've made in different

1257
00:49:13,839 --> 00:49:16,319
dimensions including the ones i just

1258
00:49:16,319 --> 00:49:17,760
discussed and

1259
00:49:17,760 --> 00:49:19,440
some open

1260
00:49:19,440 --> 00:49:23,280
issues that i consider are important

1261
00:49:23,280 --> 00:49:25,280
all right so in in the front of

1262
00:49:25,280 --> 00:49:27,680
adversary robustness so techniques like

1263
00:49:27,680 --> 00:49:30,240
adversaries training certified defenses

1264
00:49:30,240 --> 00:49:32,559
and the moving target strategy that i

1265
00:49:32,559 --> 00:49:33,839
just described

1266
00:49:33,839 --> 00:49:35,280
from our work

1267
00:49:35,280 --> 00:49:36,559
are super

1268
00:49:36,559 --> 00:49:38,319
useful and they are

1269
00:49:38,319 --> 00:49:39,680
they're good

1270
00:49:39,680 --> 00:49:41,280
progress

1271
00:49:41,280 --> 00:49:42,839
but on the flip

1272
00:49:42,839 --> 00:49:45,040
side what we are struggling with

1273
00:49:45,040 --> 00:49:46,720
especially in the sense of adversarial

1274
00:49:46,720 --> 00:49:49,760
examples uh these days is

1275
00:49:49,760 --> 00:49:51,359
that you know the adversarial example

1276
00:49:51,359 --> 00:49:53,760
literature didn't move much

1277
00:49:53,760 --> 00:49:55,280
uh so we're we're gaining some

1278
00:49:55,280 --> 00:49:57,760
robustness empirical robustness against

1279
00:49:57,760 --> 00:50:00,400
let's say the previous defense technique

1280
00:50:00,400 --> 00:50:02,160
and then somebody comes and breaks these

1281
00:50:02,160 --> 00:50:04,640
defenses and so that we're in that cycle

1282
00:50:04,640 --> 00:50:06,000
of you know

1283
00:50:06,000 --> 00:50:08,960
the classic attack defense arms right so

1284
00:50:08,960 --> 00:50:11,200
we i think we are at the time where we

1285
00:50:11,200 --> 00:50:12,400
have to rethink

1286
00:50:12,400 --> 00:50:14,640
what we call robustness especially in

1287
00:50:14,640 --> 00:50:16,559
the sense of personal example so we have

1288
00:50:16,559 --> 00:50:20,480
to broadly reason about robustness

1289
00:50:20,480 --> 00:50:22,079
and

1290
00:50:22,079 --> 00:50:24,000
get out of you know this

1291
00:50:24,000 --> 00:50:26,240
well-established robustness uh

1292
00:50:26,240 --> 00:50:28,880
assessment in the in the image domain

1293
00:50:28,880 --> 00:50:32,240
where you know the the the norms or the

1294
00:50:32,240 --> 00:50:34,640
distance metric is somehow limited to

1295
00:50:34,640 --> 00:50:36,800
the classical norms so

1296
00:50:36,800 --> 00:50:39,440
um so i'm not the first to to say this a

1297
00:50:39,440 --> 00:50:41,440
lot of people suggested this and i agree

1298
00:50:41,440 --> 00:50:43,839
with them

1299
00:50:44,079 --> 00:50:44,800
on

1300
00:50:44,800 --> 00:50:46,319
privacy

1301
00:50:46,319 --> 00:50:48,400
differential privacy has become sort of

1302
00:50:48,400 --> 00:50:50,800
the gold standard for

1303
00:50:50,800 --> 00:50:53,760
what we want a privacy definition to be

1304
00:50:53,760 --> 00:50:56,720
mathematically rigorous and provide some

1305
00:50:56,720 --> 00:50:57,520
you know

1306
00:50:57,520 --> 00:50:59,920
guaranteed

1307
00:50:59,920 --> 00:51:03,359
utility versus privacy guarantee and

1308
00:51:03,359 --> 00:51:06,800
that you can empirically verify and also

1309
00:51:06,800 --> 00:51:08,960
formally uh support

1310
00:51:08,960 --> 00:51:10,400
um but

1311
00:51:10,400 --> 00:51:12,000
beyond

1312
00:51:12,000 --> 00:51:14,480
what we call the average case metrics

1313
00:51:14,480 --> 00:51:16,640
that with which we are measuring attack

1314
00:51:16,640 --> 00:51:19,280
you know effectiveness

1315
00:51:19,280 --> 00:51:21,359
we have to also look at some realistic

1316
00:51:21,359 --> 00:51:25,119
scenarios for measuring privacy leakage

1317
00:51:25,119 --> 00:51:27,280
and we have to also reason about

1318
00:51:27,280 --> 00:51:30,079
cross-domain formulations of privacy

1319
00:51:30,079 --> 00:51:31,680
because

1320
00:51:31,680 --> 00:51:33,920
privacy leakage or implications of

1321
00:51:33,920 --> 00:51:36,319
privacy leakage in one domain may not

1322
00:51:36,319 --> 00:51:38,319
necessarily translate to another domain

1323
00:51:38,319 --> 00:51:39,440
for example

1324
00:51:39,440 --> 00:51:42,000
privacy leakage metrics for images

1325
00:51:42,000 --> 00:51:44,880
versus let's say language models may not

1326
00:51:44,880 --> 00:51:46,800
necessarily be the same because the

1327
00:51:46,800 --> 00:51:48,960
tokens we're dealing with are completely

1328
00:51:48,960 --> 00:51:52,079
different um and the semantics

1329
00:51:52,079 --> 00:51:53,119
of

1330
00:51:53,119 --> 00:51:56,079
privacy leakage is different as well

1331
00:51:56,079 --> 00:51:56,960
and

1332
00:51:56,960 --> 00:51:59,040
so the so the adversary robustness and

1333
00:51:59,040 --> 00:52:00,480
privacy are the two things that i

1334
00:52:00,480 --> 00:52:03,200
covered but beyond this as i said we

1335
00:52:03,200 --> 00:52:05,359
have to also look at trustworthiness

1336
00:52:05,359 --> 00:52:07,760
from a transparency point of view which

1337
00:52:07,760 --> 00:52:09,920
is

1338
00:52:10,000 --> 00:52:11,280
when machine learning models make

1339
00:52:11,280 --> 00:52:14,640
decisions on very important uh tasks

1340
00:52:14,640 --> 00:52:16,079
they we want

1341
00:52:16,079 --> 00:52:17,920
to we want to understand how the machine

1342
00:52:17,920 --> 00:52:19,760
learning models uh

1343
00:52:19,760 --> 00:52:22,559
got to that decision so there is this

1344
00:52:22,559 --> 00:52:26,240
you know explosive line of work in um

1345
00:52:26,240 --> 00:52:28,559
in literature in in the so-called

1346
00:52:28,559 --> 00:52:30,400
interpretable or explainable machine

1347
00:52:30,400 --> 00:52:31,599
learning which is

1348
00:52:31,599 --> 00:52:34,720
which is useful but i guess

1349
00:52:34,720 --> 00:52:37,200
what is challenging for

1350
00:52:37,200 --> 00:52:39,760
moving forward with interpretability or

1351
00:52:39,760 --> 00:52:41,440
explainability of machine learning

1352
00:52:41,440 --> 00:52:43,359
models is that

1353
00:52:43,359 --> 00:52:45,280
you know these machining models are keep

1354
00:52:45,280 --> 00:52:48,240
going in terms of size and getting more

1355
00:52:48,240 --> 00:52:51,359
and more complex so their black box

1356
00:52:51,359 --> 00:52:53,680
aspect is just getting more and more uh

1357
00:52:53,680 --> 00:52:54,960
bigger

1358
00:52:54,960 --> 00:52:55,839
and

1359
00:52:55,839 --> 00:52:58,720
it's becoming harder to scale up you

1360
00:52:58,720 --> 00:52:59,599
know

1361
00:52:59,599 --> 00:53:02,160
explainability frameworks to

1362
00:53:02,160 --> 00:53:04,319
the models that we are we're seeing

1363
00:53:04,319 --> 00:53:06,319
today which is got you know billions of

1364
00:53:06,319 --> 00:53:07,920
parameters

1365
00:53:07,920 --> 00:53:12,559
um so so that's about transparency and

1366
00:53:12,559 --> 00:53:14,480
the the picture won't be complete if we

1367
00:53:14,480 --> 00:53:16,640
don't bring in fairness and ethics into

1368
00:53:16,640 --> 00:53:18,240
the whole equation of trustworthy

1369
00:53:18,240 --> 00:53:20,240
machine learning here because

1370
00:53:20,240 --> 00:53:22,480
uh you might do a great job on making

1371
00:53:22,480 --> 00:53:24,800
the model uh robust against adversarial

1372
00:53:24,800 --> 00:53:27,040
manipulations or privacy motivated

1373
00:53:27,040 --> 00:53:29,040
attacks and you can still make it you

1374
00:53:29,040 --> 00:53:31,280
know somehow transparent about its

1375
00:53:31,280 --> 00:53:33,040
decisions and so on

1376
00:53:33,040 --> 00:53:34,160
but if it is

1377
00:53:34,160 --> 00:53:37,920
if it is not fair um to everyone or if

1378
00:53:37,920 --> 00:53:39,200
it is

1379
00:53:39,200 --> 00:53:42,640
somehow unfair to a specific long tail

1380
00:53:42,640 --> 00:53:45,119
or you know portion of the data set or a

1381
00:53:45,119 --> 00:53:47,760
population on which a model is trained

1382
00:53:47,760 --> 00:53:48,880
then

1383
00:53:48,880 --> 00:53:51,359
we are not doing things right so the

1384
00:53:51,359 --> 00:53:54,480
model is not trustworthy right

1385
00:53:54,480 --> 00:53:58,400
so the idea here is one of uh one of the

1386
00:53:58,400 --> 00:54:00,319
you know the progress we've

1387
00:54:00,319 --> 00:54:03,200
made in in the in the literature of you

1388
00:54:03,200 --> 00:54:05,440
know fairness and ethics and so on of

1389
00:54:05,440 --> 00:54:07,839
machine learning or ai in general is

1390
00:54:07,839 --> 00:54:10,400
this this tendency to quantify or

1391
00:54:10,400 --> 00:54:13,280
measure fairness and ethics

1392
00:54:13,280 --> 00:54:16,240
the question that might come naturally

1393
00:54:16,240 --> 00:54:17,280
is

1394
00:54:17,280 --> 00:54:18,240
is

1395
00:54:18,240 --> 00:54:20,400
is it is it okay or is it natural to

1396
00:54:20,400 --> 00:54:22,880
quantify fairness or is fairness and

1397
00:54:22,880 --> 00:54:24,720
ethics quantifiable in the first place

1398
00:54:24,720 --> 00:54:27,520
that's a very big question

1399
00:54:27,520 --> 00:54:29,839
that the ethics and fairness community

1400
00:54:29,839 --> 00:54:32,240
has to deal with

1401
00:54:32,240 --> 00:54:33,520
if we have to

1402
00:54:33,520 --> 00:54:35,760
formulate fairness and ethics

1403
00:54:35,760 --> 00:54:37,280
we have to have

1404
00:54:37,280 --> 00:54:39,440
some alignment between what humans

1405
00:54:39,440 --> 00:54:42,559
perceive as fair or ethical

1406
00:54:42,559 --> 00:54:46,720
and try to encode this human values into

1407
00:54:46,720 --> 00:54:48,720
the formulations of fairness and ethics

1408
00:54:48,720 --> 00:54:51,280
in machine learning

1409
00:54:51,280 --> 00:54:54,240
more importantly though

1410
00:54:54,240 --> 00:54:56,640
all these different dimensions of trash

1411
00:54:56,640 --> 00:54:59,119
closing machine learning are important

1412
00:54:59,119 --> 00:55:02,000
but what's more important is even

1413
00:55:02,000 --> 00:55:04,160
the dynamics between these different

1414
00:55:04,160 --> 00:55:06,720
properties that we expect from chilean

1415
00:55:06,720 --> 00:55:07,680
right

1416
00:55:07,680 --> 00:55:08,480
so

1417
00:55:08,480 --> 00:55:11,119
we have to also study um this is a very

1418
00:55:11,119 --> 00:55:13,599
underexplored or somehow overlooked

1419
00:55:13,599 --> 00:55:16,559
fairly overlooked area where we have to

1420
00:55:16,559 --> 00:55:18,960
look at for example accuracy versus all

1421
00:55:18,960 --> 00:55:20,480
these kinds of

1422
00:55:20,480 --> 00:55:22,480
properties that we expect how does

1423
00:55:22,480 --> 00:55:24,240
accuracy fair against

1424
00:55:24,240 --> 00:55:26,559
adversary robustness privacy

1425
00:55:26,559 --> 00:55:29,119
transparency fairness and so on

1426
00:55:29,119 --> 00:55:31,520
and then between this different if you

1427
00:55:31,520 --> 00:55:33,920
take a pair let's say robustness and

1428
00:55:33,920 --> 00:55:36,400
privacy or robustness and fairness or

1429
00:55:36,400 --> 00:55:38,960
privacy and transparency

1430
00:55:38,960 --> 00:55:41,200
some of them are seemingly conflicting

1431
00:55:41,200 --> 00:55:44,559
for example privacy and transparency

1432
00:55:44,559 --> 00:55:47,680
privacy is all about limiting leakage

1433
00:55:47,680 --> 00:55:49,680
in the sense of for example membership

1434
00:55:49,680 --> 00:55:50,720
inference

1435
00:55:50,720 --> 00:55:52,799
but at the same time you also want the

1436
00:55:52,799 --> 00:55:54,400
machine learning models to be

1437
00:55:54,400 --> 00:55:56,319
transparent about its decisions so how

1438
00:55:56,319 --> 00:55:58,960
do you fare to how do you balance you

1439
00:55:58,960 --> 00:56:00,640
know

1440
00:56:00,640 --> 00:56:02,640
privacy and transparency

1441
00:56:02,640 --> 00:56:05,359
um privacy and fairness is the same kind

1442
00:56:05,359 --> 00:56:07,839
of story here so um this is the way i

1443
00:56:07,839 --> 00:56:10,400
look into the the whole puzzle of

1444
00:56:10,400 --> 00:56:12,720
trustworthy machine learning in terms of

1445
00:56:12,720 --> 00:56:16,000
the progress we have made so far and

1446
00:56:16,000 --> 00:56:18,319
some of the open issues that i consider

1447
00:56:18,319 --> 00:56:20,079
are important

1448
00:56:20,079 --> 00:56:20,960
so

1449
00:56:20,960 --> 00:56:22,799
to conclude

1450
00:56:22,799 --> 00:56:25,440
the way i want to look into

1451
00:56:25,440 --> 00:56:27,839
trustworthy machine learning is

1452
00:56:27,839 --> 00:56:30,559
using this analogy of an umbrella

1453
00:56:30,559 --> 00:56:31,359
so

1454
00:56:31,359 --> 00:56:33,520
in in the normal sense of an umbrella

1455
00:56:33,520 --> 00:56:34,480
right

1456
00:56:34,480 --> 00:56:38,720
so you want your umbrella to be reliable

1457
00:56:38,720 --> 00:56:40,880
or trustworthy so that you know it

1458
00:56:40,880 --> 00:56:42,799
protects you from the things that you

1459
00:56:42,799 --> 00:56:45,280
anticipate an umbrella will protect you

1460
00:56:45,280 --> 00:56:46,480
against

1461
00:56:46,480 --> 00:56:47,280
so

1462
00:56:47,280 --> 00:56:49,680
if there is a uv ray or lights coming

1463
00:56:49,680 --> 00:56:51,440
from the sun you want your umbrella to

1464
00:56:51,440 --> 00:56:52,640
protect you

1465
00:56:52,640 --> 00:56:55,359
if there is a rain coming uh you have to

1466
00:56:55,359 --> 00:56:57,760
also expect or anticipate the umbrella

1467
00:56:57,760 --> 00:57:00,079
to protect you against the rain

1468
00:57:00,079 --> 00:57:03,040
or if there is a wind

1469
00:57:03,040 --> 00:57:05,280
unless it is a storm or something like

1470
00:57:05,280 --> 00:57:06,960
that you still

1471
00:57:06,960 --> 00:57:09,119
have some reasonable belief that

1472
00:57:09,119 --> 00:57:11,280
your umbrella will protect you

1473
00:57:11,280 --> 00:57:14,160
so you can replace all you know the the

1474
00:57:14,160 --> 00:57:17,520
uv rays the wind and the rain with you

1475
00:57:17,520 --> 00:57:20,720
know any threats or any uh properties

1476
00:57:20,720 --> 00:57:22,480
that you expect from a machine learning

1477
00:57:22,480 --> 00:57:24,720
model like adversary robustness privacy

1478
00:57:24,720 --> 00:57:26,559
preserving and so on

1479
00:57:26,559 --> 00:57:27,440
so

1480
00:57:27,440 --> 00:57:29,920
if if we consider

1481
00:57:29,920 --> 00:57:32,559
machine learning as an umbrella and we

1482
00:57:32,559 --> 00:57:34,640
want it to be trustworthy

1483
00:57:34,640 --> 00:57:36,559
this trustworthy machine learning

1484
00:57:36,559 --> 00:57:38,480
umbrella should include

1485
00:57:38,480 --> 00:57:40,480
of course robustness to adversarial

1486
00:57:40,480 --> 00:57:41,920
manipulations

1487
00:57:41,920 --> 00:57:44,880
it has to be privacy preserving

1488
00:57:44,880 --> 00:57:47,280
it has to be interpretable or you know

1489
00:57:47,280 --> 00:57:49,280
the explainability aspect should be

1490
00:57:49,280 --> 00:57:50,240
there

1491
00:57:50,240 --> 00:57:52,720
it shouldn't be biased to a specific

1492
00:57:52,720 --> 00:57:54,720
group of people

1493
00:57:54,720 --> 00:57:56,480
and it should

1494
00:57:56,480 --> 00:57:58,720
align with the ethical values that

1495
00:57:58,720 --> 00:58:00,160
humans

1496
00:58:00,160 --> 00:58:04,000
are reasonable humans would use

1497
00:58:04,319 --> 00:58:05,359
but

1498
00:58:05,359 --> 00:58:07,359
this umbrella

1499
00:58:07,359 --> 00:58:09,599
also needs some reinforcement right so

1500
00:58:09,599 --> 00:58:11,280
it has to

1501
00:58:11,280 --> 00:58:14,640
it has to have you know this united kind

1502
00:58:14,640 --> 00:58:17,359
of hands that will hold it together so

1503
00:58:17,359 --> 00:58:20,640
that it doesn't um flip around

1504
00:58:20,640 --> 00:58:22,720
so the

1505
00:58:22,720 --> 00:58:24,559
uh the way i look into this

1506
00:58:24,559 --> 00:58:26,480
reinforcement is

1507
00:58:26,480 --> 00:58:29,280
this triangular view of

1508
00:58:29,280 --> 00:58:30,079
the

1509
00:58:30,079 --> 00:58:32,559
synergy between academia doing basic

1510
00:58:32,559 --> 00:58:33,680
research

1511
00:58:33,680 --> 00:58:36,799
industry you know extending busy basic

1512
00:58:36,799 --> 00:58:40,640
research into products and services

1513
00:58:40,640 --> 00:58:43,520
and the public sector as

1514
00:58:43,520 --> 00:58:46,400
the mediator between the two because

1515
00:58:46,400 --> 00:58:48,960
the public sector has this authority of

1516
00:58:48,960 --> 00:58:52,079
doing you know oversight auditing and

1517
00:58:52,079 --> 00:58:53,839
also

1518
00:58:53,839 --> 00:58:56,480
regulatory aspects through legislations

1519
00:58:56,480 --> 00:59:00,000
and so on so the three entities

1520
00:59:00,000 --> 00:59:01,520
can work

1521
00:59:01,520 --> 00:59:02,880
in synergy

1522
00:59:02,880 --> 00:59:05,599
to reinforce what i call the trustworthy

1523
00:59:05,599 --> 00:59:08,799
machine learning umbrella which has to

1524
00:59:08,799 --> 00:59:09,920
uh

1525
00:59:09,920 --> 00:59:12,319
encompass or have all these properties

1526
00:59:12,319 --> 00:59:13,280
that i

1527
00:59:13,280 --> 00:59:16,480
i mentioned here on on the umbrella such

1528
00:59:16,480 --> 00:59:19,040
okay so with that i will start my talk

1529
00:59:19,040 --> 00:59:20,880
here and i will

1530
00:59:20,880 --> 00:59:25,000
look forward to your questions

