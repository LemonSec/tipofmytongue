1
00:00:02,440 --> 00:00:03,800
- So, hello everyone.

2
00:00:03,800 --> 00:00:05,530
Thank you for tuning in.

3
00:00:05,530 --> 00:00:07,940
Today we're gonna talk about Apache Spark.

4
00:00:07,940 --> 00:00:09,000
Now I will just assume that

5
00:00:09,000 --> 00:00:11,340
while some of you may be
familiar with the technology,

6
00:00:11,340 --> 00:00:13,900
a lot of you did not have actual chance

7
00:00:13,900 --> 00:00:14,770
to play with the cluster,

8
00:00:14,770 --> 00:00:16,983
so we'll take it right from the beginning.

9
00:00:18,130 --> 00:00:19,610
So we'll see how spark works,

10
00:00:19,610 --> 00:00:21,300
what makes it so popular,

11
00:00:21,300 --> 00:00:24,110
how to view some common
and default settings

12
00:00:24,110 --> 00:00:25,890
and obviously some forms along the way

13
00:00:25,890 --> 00:00:27,140
to bypass authentication

14
00:00:27,140 --> 00:00:29,520
and execute your code, right?

15
00:00:29,520 --> 00:00:30,680
So let's get going. Right?

16
00:00:30,680 --> 00:00:34,020
So our story takes us back back to 2008,

17
00:00:34,020 --> 00:00:37,510
when the economy was not
much better than today's.

18
00:00:37,510 --> 00:00:39,560
But Internet Explorer was still a thing

19
00:00:39,560 --> 00:00:42,490
and the big tech companies
were doing what they did best,

20
00:00:42,490 --> 00:00:44,630
quietly collecting the world's data.

21
00:00:44,630 --> 00:00:46,140
And when you gather that much data

22
00:00:46,140 --> 00:00:49,380
and I'm talking about terabytes
and terabytes of data,

23
00:00:49,380 --> 00:00:51,100
performing even the simplest calculations

24
00:00:51,100 --> 00:00:53,040
becomes quite challenging.

25
00:00:53,040 --> 00:00:54,110
Now you can either get a mainframe

26
00:00:54,110 --> 00:00:55,650
for half a million dollars

27
00:00:55,650 --> 00:00:57,860
plus whatever it takes to
get an intern to work on it,

28
00:00:57,860 --> 00:01:01,070
or you can actually distribute your files

29
00:01:01,070 --> 00:01:03,380
over thousands and thousands of servers,

30
00:01:03,380 --> 00:01:07,590
that way your computations
are small though multiplied,

31
00:01:07,590 --> 00:01:10,157
and that's one way to handle the load.

32
00:01:10,157 --> 00:01:14,010
And the standard way of
doing so I would say is

33
00:01:14,010 --> 00:01:16,520
by using the Apache Hadoop Framework.

34
00:01:16,520 --> 00:01:18,610
Now if you've ever worked with Hadoop,

35
00:01:18,610 --> 00:01:20,040
you must know that it's actually

36
00:01:20,040 --> 00:01:21,713
quite a hostile environment.

37
00:01:21,713 --> 00:01:24,350
It's a fairly complex environment.

38
00:01:24,350 --> 00:01:26,820
So first of all just to
give you a quick example,

39
00:01:26,820 --> 00:01:30,850
you need to fragment your files
using the HTFS file system.

40
00:01:30,850 --> 00:01:34,590
So you end up with multiple
fragments on multiple servers

41
00:01:34,590 --> 00:01:35,440
of these files,

42
00:01:35,440 --> 00:01:37,300
and then you need to do
coherent computations

43
00:01:37,300 --> 00:01:38,347
on these small fragments,

44
00:01:38,347 --> 00:01:40,370
you need the MapReduce Framework.

45
00:01:40,370 --> 00:01:42,610
But then you end up with
thousands of processes

46
00:01:42,610 --> 00:01:44,070
running on multiple machines.

47
00:01:44,070 --> 00:01:46,320
So you need a way to
schedule them into a yarn,

48
00:01:46,320 --> 00:01:47,707
but then you wanna do some SQL querying,

49
00:01:47,707 --> 00:01:50,830
and so you need Apache
Drill, et cetera, et cetera.

50
00:01:50,830 --> 00:01:53,130
So you end up with this plethora of tools

51
00:01:53,130 --> 00:01:54,220
each with their own latencies,

52
00:01:54,220 --> 00:01:58,470
each with their own
learning curve et cetera,

53
00:01:58,470 --> 00:02:00,470
just to cover all your
needs and data analytics.

54
00:02:00,470 --> 00:02:02,107
So just to give you an example,

55
00:02:02,107 --> 00:02:03,560
this is what it takes

56
00:02:03,560 --> 00:02:05,490
to do a simple word count on MapReduce.

57
00:02:05,490 --> 00:02:06,650
This is the first page of Java,

58
00:02:06,650 --> 00:02:08,580
this is the second page,

59
00:02:08,580 --> 00:02:09,980
which is downright outrageous

60
00:02:09,980 --> 00:02:12,323
for a data processing solution, right?

61
00:02:13,830 --> 00:02:14,663
In our story,

62
00:02:14,663 --> 00:02:16,820
we're around 2009

63
00:02:16,820 --> 00:02:19,760
and there's this guy named Matei Zaharia,

64
00:02:19,760 --> 00:02:22,490
who was at Berkeley I believe at the time,

65
00:02:22,490 --> 00:02:24,897
who I imagine looked
at this code and went,

66
00:02:24,897 --> 00:02:28,457
"Are you fucking crazy?
That's not gonna work.

67
00:02:28,457 --> 00:02:30,467
"A word count is a simple operation,

68
00:02:30,467 --> 00:02:34,480
"a word count should be
done in five lines of code."

69
00:02:34,480 --> 00:02:36,421
Apache Spark was born.

70
00:02:36,421 --> 00:02:37,830
Now this is not the official story,

71
00:02:37,830 --> 00:02:38,663
the official story is that

72
00:02:38,663 --> 00:02:39,750
he was working on something called Mesos,

73
00:02:39,750 --> 00:02:41,110
which is a resource manager.

74
00:02:41,110 --> 00:02:42,920
And he invented spark
as a proof of concept.

75
00:02:42,920 --> 00:02:45,340
But I like my version better.

76
00:02:45,340 --> 00:02:46,330
So anyway,

77
00:02:46,330 --> 00:02:48,737
needless to say we'll go
through the code later on,

78
00:02:48,737 --> 00:02:50,570
but right off the bat you can see that

79
00:02:50,570 --> 00:02:52,530
you can do pretty much the same processing

80
00:02:52,530 --> 00:02:53,870
that you would do in Apache Hadoop,

81
00:02:53,870 --> 00:02:57,173
except that you can do it at
10 times less code in spark.

82
00:02:58,070 --> 00:03:00,440
But it's also much faster.

83
00:03:00,440 --> 00:03:02,860
So it's three times faster
and 10 times less nodes,

84
00:03:02,860 --> 00:03:04,360
so it's 30 times faster.

85
00:03:04,360 --> 00:03:06,410
You see, Hadoop will
flush everything to disk

86
00:03:06,410 --> 00:03:07,570
whenever it gets a chance,

87
00:03:07,570 --> 00:03:10,260
whereas spark try to keep
everything in memory.

88
00:03:10,260 --> 00:03:11,580
It was developed in an era

89
00:03:11,580 --> 00:03:13,160
where memory was getting
cheaper and cheaper,

90
00:03:13,160 --> 00:03:14,640
so it kind of makes sense.

91
00:03:14,640 --> 00:03:16,460
But if you're running
in a cloud environment

92
00:03:16,460 --> 00:03:18,693
where you pay per the usage,

93
00:03:19,890 --> 00:03:21,740
having something that runs 30 times faster

94
00:03:21,740 --> 00:03:24,200
means that it's 30 times cheaper actually.

95
00:03:24,200 --> 00:03:26,800
And that partly explains

96
00:03:26,800 --> 00:03:30,470
the gigantic boom and popularity of spark

97
00:03:30,470 --> 00:03:33,284
amongst the big data companies.

98
00:03:33,284 --> 00:03:34,690
If you're serious about

99
00:03:34,690 --> 00:03:37,690
your big data processing and cluster font,

100
00:03:37,690 --> 00:03:41,420
you will be using spark for
at least some part of it.

101
00:03:41,420 --> 00:03:43,609
And so yeah, as spark boomed,

102
00:03:43,609 --> 00:03:45,660
it was adopted by the Apache foundation

103
00:03:45,660 --> 00:03:49,450
and it grew to encompass many
aspects of data analytics.

104
00:03:49,450 --> 00:03:50,880
If you can do SQL, streaming,

105
00:03:50,880 --> 00:03:54,400
machine learning, graph
processing, it got you covered.

106
00:03:54,400 --> 00:03:56,470
So it's developed half
in Java, half in Scala,

107
00:03:56,470 --> 00:03:59,090
but it also has some
connectors in Python and R.

108
00:03:59,090 --> 00:04:01,710
So, yeah awesome.

109
00:04:01,710 --> 00:04:06,540
So basically when you have
this powerful framework,

110
00:04:06,540 --> 00:04:08,360
suddenly everything looks like a nail.

111
00:04:08,360 --> 00:04:09,780
So if you're gonna do revenue prediction,

112
00:04:09,780 --> 00:04:11,890
you just hook spark to your, I dunno,

113
00:04:11,890 --> 00:04:14,500
Redshift cluster, download
those financial statements

114
00:04:14,500 --> 00:04:16,420
and then you can do yourself.

115
00:04:16,420 --> 00:04:18,630
If you wanna do fraud
analysis, user targeting,

116
00:04:18,630 --> 00:04:20,610
say you hook it up to
your Cassandra database,

117
00:04:20,610 --> 00:04:21,500
your extra buckets,

118
00:04:21,500 --> 00:04:24,710
you upload or download those log files

119
00:04:24,710 --> 00:04:26,520
and then you do your computation.

120
00:04:26,520 --> 00:04:29,310
So basically what I'm saying is that

121
00:04:29,310 --> 00:04:32,670
Apache Spark is often at the junction of

122
00:04:32,670 --> 00:04:35,980
almost every important data
store inside the company.

123
00:04:35,980 --> 00:04:38,130
And that's what made it
interesting, at least for me.

124
00:04:38,130 --> 00:04:41,410
Because, well if you can access this tool,

125
00:04:41,410 --> 00:04:46,410
you're pretty much open to
every database that's out there.

126
00:04:46,440 --> 00:04:48,390
So, you know, how is it protected?

127
00:04:48,390 --> 00:04:50,750
How does it work and what
is it's security et cetera?

128
00:04:50,750 --> 00:04:51,810
And so what I did,

129
00:04:51,810 --> 00:04:53,697
I did what any sensible person would do.

130
00:04:53,697 --> 00:04:56,020
I went to the documentation,
to their website,

131
00:04:56,020 --> 00:04:57,970
and I went to the security page.

132
00:04:57,970 --> 00:04:59,970
And lo and behold,

133
00:04:59,970 --> 00:05:02,420
from this magnificent sentence that says

134
00:05:02,420 --> 00:05:04,440
security in Spark is off by default,

135
00:05:04,440 --> 00:05:05,863
well fuck me.

136
00:05:05,863 --> 00:05:07,790
And what does it do again?

137
00:05:07,790 --> 00:05:10,640
Oh yeah, it's this gigantic framework,

138
00:05:10,640 --> 00:05:13,440
cluster of hundreds and
hundreds of machines

139
00:05:13,440 --> 00:05:16,430
that have access to every
database inside the company.

140
00:05:16,430 --> 00:05:17,280
And what does it do?

141
00:05:17,280 --> 00:05:19,590
Oh yeah, it does
distributed data processing.

142
00:05:19,590 --> 00:05:21,660
Or as I'd like to call it,

143
00:05:21,660 --> 00:05:24,000
it's really distributed
remote code execution,

144
00:05:24,000 --> 00:05:26,190
so I though this would be fun.

145
00:05:26,190 --> 00:05:28,010
And I hope I piqued your interest

146
00:05:28,010 --> 00:05:31,090
because really that's what
sparked the whole endeavor,

147
00:05:31,090 --> 00:05:32,103
no pun intended.

148
00:05:33,050 --> 00:05:35,970
And I hope I got you
interested enough to actually

149
00:05:35,970 --> 00:05:37,200
continue along this presentation

150
00:05:37,200 --> 00:05:39,250
so we can explore how it works and

151
00:05:40,840 --> 00:05:43,200
some worms that I found. Right?

152
00:05:43,200 --> 00:05:46,260
So yeah, in keeping
with DEF CON tradition,

153
00:05:46,260 --> 00:05:49,283
I think it's time for drinking.

154
00:05:50,810 --> 00:05:52,500
I should not do this online, but

155
00:05:52,500 --> 00:05:54,737
Oh well, what the heck?

156
00:05:54,737 --> 00:05:57,703
So to DEF CON Safe Mode. Cheers.

157
00:06:01,660 --> 00:06:02,920
Yeah, that will kick in later.

158
00:06:02,920 --> 00:06:04,220
Anyway,

159
00:06:04,220 --> 00:06:06,743
so what is Spark and how does it work?

160
00:06:07,670 --> 00:06:08,503
I hope it's gonna be one take,

161
00:06:08,503 --> 00:06:10,774
because I can not do that five times.

162
00:06:10,774 --> 00:06:12,050
So how does it work?

163
00:06:12,050 --> 00:06:15,060
So as in everything in
distributed processing basically,

164
00:06:15,060 --> 00:06:16,710
we start with a bunch of machines

165
00:06:16,710 --> 00:06:18,840
that we're gonna call workers.

166
00:06:18,840 --> 00:06:19,750
Now these workers

167
00:06:19,750 --> 00:06:23,730
are just basically brainless
machines that will execute

168
00:06:23,730 --> 00:06:24,707
whatever is sent their way.

169
00:06:24,707 --> 00:06:26,480
Now on each worker,

170
00:06:26,480 --> 00:06:29,175
you find a JVM process or a spark process

171
00:06:29,175 --> 00:06:30,760
that's called an executor

172
00:06:30,760 --> 00:06:34,460
and that's gonna do the actual execution.

173
00:06:34,460 --> 00:06:37,360
Now you can have many
executors on each worker

174
00:06:37,360 --> 00:06:41,030
and it's basically executer
that defines your power

175
00:06:41,030 --> 00:06:42,010
in terms of parallelism.

176
00:06:42,010 --> 00:06:45,540
So if you have 10 workers and each worker,

177
00:06:45,540 --> 00:06:47,390
or each machine basically
have three executors,

178
00:06:47,390 --> 00:06:49,683
so you can run 30 tasks in parallel.

179
00:06:51,190 --> 00:06:52,033
So yeah,

180
00:06:52,033 --> 00:06:53,770
you have some default http ports

181
00:06:53,770 --> 00:06:55,287
that gives a status of what's going on

182
00:06:55,287 --> 00:06:57,268
on the machine, not interesting.

183
00:06:57,268 --> 00:06:58,730
But you have the random RPC port

184
00:06:58,730 --> 00:07:01,480
that is set for each executer.

185
00:07:01,480 --> 00:07:02,927
And it's this one that

186
00:07:02,927 --> 00:07:04,680
the rest of the cluster contacts

187
00:07:04,680 --> 00:07:08,913
in order to send and receive
a status from the worker.

188
00:07:09,966 --> 00:07:11,310
Now the worker is fine,

189
00:07:11,310 --> 00:07:12,350
but the most important piece

190
00:07:12,350 --> 00:07:14,387
or one of the most important
pieces is the cluster manager,

191
00:07:14,387 --> 00:07:16,521
and that's our second piece.

192
00:07:16,521 --> 00:07:18,570
And the cluster manager's sole job is

193
00:07:18,570 --> 00:07:20,680
to actually schedule the applications

194
00:07:20,680 --> 00:07:22,907
so the work that is coming and say,

195
00:07:22,907 --> 00:07:24,627
"Oh yeah, this application is gonna run

196
00:07:24,627 --> 00:07:25,757
"on these two workers.

197
00:07:25,757 --> 00:07:27,480
"That one is gonna run on
the other three workers."

198
00:07:27,480 --> 00:07:30,390
So it has a status of the workers.

199
00:07:30,390 --> 00:07:31,223
What they're doing,

200
00:07:31,223 --> 00:07:32,610
are they online or not?

201
00:07:32,610 --> 00:07:34,730
Are they busy or not, et cetera. I know.

202
00:07:34,730 --> 00:07:36,330
Every component inside spark

203
00:07:36,330 --> 00:07:38,493
communicates using this spark RPC protocol

204
00:07:38,493 --> 00:07:40,890
that we will detail a little bit later on.

205
00:07:40,890 --> 00:07:42,360
And just to show you what it, you know--

206
00:07:42,360 --> 00:07:45,340
To demystify this spark cluster,

207
00:07:45,340 --> 00:07:46,960
this is what it looks like.

208
00:07:46,960 --> 00:07:50,600
This is the basic UI of the
spark master on port 80, 80.

209
00:07:50,600 --> 00:07:52,078
Oh, sorry about that.

210
00:07:52,078 --> 00:07:54,910
So yeah, you can see
that we have two workers.

211
00:07:54,910 --> 00:07:55,810
One which is dead.

212
00:07:55,810 --> 00:07:58,400
We have an application
that's running on two cores

213
00:07:58,400 --> 00:08:00,650
and since each worker has one core,

214
00:08:00,650 --> 00:08:02,500
so this application is
running on two cores.

215
00:08:02,500 --> 00:08:04,713
It's being distributed over two machines.

216
00:08:05,585 --> 00:08:10,360
But the most important
port really is port 7077,

217
00:08:10,360 --> 00:08:12,360
because that's the one
that we're gonna contact

218
00:08:12,360 --> 00:08:16,150
in order to send an application,
register it and schedule it

219
00:08:16,150 --> 00:08:20,750
to be executed in
parallel on these workers.

220
00:08:20,750 --> 00:08:22,640
And it's gonna be these basic hypothesis

221
00:08:22,640 --> 00:08:24,961
that we're gonna make
along this presentation

222
00:08:24,961 --> 00:08:29,496
that we have access to
this specific ports.

223
00:08:29,496 --> 00:08:30,427
And the third component,

224
00:08:30,427 --> 00:08:33,867
and indeed the most
important,  is the driver.

225
00:08:33,867 --> 00:08:36,730
And the driver is really the brain behind

226
00:08:36,730 --> 00:08:38,120
the application orchestration.

227
00:08:38,120 --> 00:08:39,980
It's the one that's gonna
slice up your application

228
00:08:39,980 --> 00:08:42,630
in small tasks, contact
the call center manager,

229
00:08:42,630 --> 00:08:45,650
receives the worker and then
send, distribute your workload

230
00:08:45,650 --> 00:08:47,329
to these worker and, you know,

231
00:08:47,329 --> 00:08:49,857
make sure that everything
is going smoothly

232
00:08:49,857 --> 00:08:51,293
and aggregate the result.

233
00:08:52,570 --> 00:08:54,490
And the driver in a typical case,

234
00:08:54,490 --> 00:08:56,840
or typical scenario is gonna
be running on your laptop.

235
00:08:56,840 --> 00:08:58,690
So you enter in that work,

236
00:08:58,690 --> 00:09:00,410
you have the cluster
manager running somewhere,

237
00:09:00,410 --> 00:09:02,730
you have the workers running someone else,

238
00:09:02,730 --> 00:09:04,530
and you're gonna boot
up the driver, right?

239
00:09:04,530 --> 00:09:07,400
To an obligation in Java,
Scala Python, or whatever,

240
00:09:07,400 --> 00:09:08,290
boot up the driver,

241
00:09:08,290 --> 00:09:11,430
and then send it to the cluster
manager to be scheduled on

242
00:09:11,430 --> 00:09:14,110
the worker's, hopefully
to execute some code.

243
00:09:14,110 --> 00:09:18,757
So that's one setup that's
kind of common, right?

244
00:09:20,060 --> 00:09:20,893
So recon,

245
00:09:20,893 --> 00:09:23,530
so let's take this a specific
scenario that I just listed.

246
00:09:23,530 --> 00:09:24,363
Let's say you end up in a network

247
00:09:24,363 --> 00:09:28,713
and you wanna hunt for some
spark in order to exploit it,

248
00:09:29,800 --> 00:09:31,230
how would you go about it?

249
00:09:31,230 --> 00:09:34,836
Well, the first thing
you wanna do I guess is

250
00:09:34,836 --> 00:09:37,337
basically if you are
in an AWS environment,

251
00:09:37,337 --> 00:09:38,830
you can simply use the API,

252
00:09:38,830 --> 00:09:41,235
if you have proper
access rights obviously,

253
00:09:41,235 --> 00:09:44,440
to look for every machine
that has Spark master

254
00:09:44,440 --> 00:09:46,300
or other keywords in their labels.

255
00:09:46,300 --> 00:09:48,210
Similarly, if you're
in a cube environment,

256
00:09:48,210 --> 00:09:49,870
Kubernetes, it's the same thing.

257
00:09:49,870 --> 00:09:51,150
But what if you end up in
a more traditional network

258
00:09:51,150 --> 00:09:53,890
where you have to N-Map
the shit out of things,

259
00:09:53,890 --> 00:09:56,120
how can you find that cluster?

260
00:09:56,120 --> 00:09:58,870
Cause you see N-Map does
not really support spark.

261
00:09:58,870 --> 00:10:01,460
So we're in the kind of a pickle here.

262
00:10:01,460 --> 00:10:02,883
So the first thing we wanna do

263
00:10:02,883 --> 00:10:06,170
is to be able to fingerprint
spark properly using N-Map.

264
00:10:06,170 --> 00:10:07,220
And to do that,

265
00:10:07,220 --> 00:10:09,880
I simply worked out a
spark lesson on my lab,

266
00:10:09,880 --> 00:10:12,520
I put some wireshark in the middle

267
00:10:12,520 --> 00:10:14,116
and I analyze the traffic.

268
00:10:14,116 --> 00:10:14,949
And if you do that,

269
00:10:14,949 --> 00:10:18,800
this is the blog that you're gonna see.

270
00:10:18,800 --> 00:10:21,090
And basically the way to decompose this

271
00:10:21,090 --> 00:10:24,300
is fairly simple and it's
quite repetitive actually,

272
00:10:24,300 --> 00:10:25,450
once you get to know spark RPC.

273
00:10:25,450 --> 00:10:28,230
So this is the spark
RPC I was talking about.

274
00:10:28,230 --> 00:10:30,627
It's always starts with the same things,

275
00:10:30,627 --> 00:10:33,760
the same header is gonna
be most of the time,

276
00:10:33,760 --> 00:10:36,370
21 bytes of data followed
by the payload, right?

277
00:10:36,370 --> 00:10:38,677
And this one bytes of data,

278
00:10:38,677 --> 00:10:42,356
a common header when you
send spark RPC commands

279
00:10:42,356 --> 00:10:44,980
is composed of seven bytes of null bytes,

280
00:10:44,980 --> 00:10:46,720
followed by two magic bites.

281
00:10:46,720 --> 00:10:47,600
I call them magic bites,

282
00:10:47,600 --> 00:10:49,810
but basically they depend on the RPC

283
00:10:49,810 --> 00:10:51,050
and one that you're trying to reach

284
00:10:51,050 --> 00:10:53,073
and the type of message
that you're sending.

285
00:10:53,073 --> 00:10:54,260
So in this case,

286
00:10:54,260 --> 00:10:58,420
I chose the example of verify endpoint,

287
00:10:58,420 --> 00:11:00,513
check existence message.

288
00:11:01,570 --> 00:11:04,096
And it's a magic bite is C305.

289
00:11:04,096 --> 00:11:07,780
I don't know if it's a worker
sending a heartbeat message

290
00:11:07,780 --> 00:11:11,810
to the cluster manager is
gonna be to 2B0F whatever,

291
00:11:11,810 --> 00:11:13,610
but in this case it's C305.

292
00:11:13,610 --> 00:11:15,390
But anyway it doesn't matter.

293
00:11:15,390 --> 00:11:17,059
Next we have eight bytes of data,

294
00:11:17,059 --> 00:11:17,892
like random data.

295
00:11:17,892 --> 00:11:19,720
You can put anything you want here,

296
00:11:19,720 --> 00:11:23,771
they'll just be echoed back
by the cluster manager.

297
00:11:23,771 --> 00:11:24,930
And finally we have,

298
00:11:24,930 --> 00:11:27,000
four bytes of the size of the data

299
00:11:27,000 --> 00:11:27,833
that's gonna follow,

300
00:11:27,833 --> 00:11:29,960
so the payload that will follow.

301
00:11:29,960 --> 00:11:31,560
what is this payload?

302
00:11:31,560 --> 00:11:32,650
Well, you have a bunch of data,

303
00:11:32,650 --> 00:11:34,248
IP addresses that we don't care about,

304
00:11:34,248 --> 00:11:35,178
but the most important thing really,

305
00:11:35,178 --> 00:11:38,050
the heart of the payload if you will,

306
00:11:38,050 --> 00:11:40,390
is the serialized object
that you can see here.

307
00:11:40,390 --> 00:11:43,894
And so it's a serialized JBM
object called Check Existence.

308
00:11:43,894 --> 00:11:44,920
And if you look at the source code,

309
00:11:44,920 --> 00:11:46,900
check existence is actually a class,

310
00:11:46,900 --> 00:11:48,690
well it's a Scala case class.

311
00:11:48,690 --> 00:11:50,360
But just think of it as a regular class,

312
00:11:50,360 --> 00:11:52,083
which is immutable.

313
00:11:52,083 --> 00:11:54,730
And it has some interesting
other properties in Scala

314
00:11:54,730 --> 00:11:56,250
that we simply don't give a fuck about.

315
00:11:56,250 --> 00:11:59,090
But yeah, it's a class.

316
00:11:59,090 --> 00:12:01,022
So Check Existence is a class

317
00:12:01,022 --> 00:12:02,700
and it has an attribute name.

318
00:12:02,700 --> 00:12:04,610
So basically the client or driver

319
00:12:04,610 --> 00:12:06,527
is sending this serialized class,

320
00:12:06,527 --> 00:12:08,470
to the cluster manager to see

321
00:12:08,470 --> 00:12:11,700
if there's an endpoint
bearing the name that it sent.

322
00:12:11,700 --> 00:12:12,795
And the name that it
sent is obviously master.

323
00:12:12,795 --> 00:12:17,795
So it wants to know if there
is a master RPC endpoint

324
00:12:18,030 --> 00:12:21,410
listening on the other
end of the communication.

325
00:12:21,410 --> 00:12:23,650
And if that is the case,

326
00:12:23,650 --> 00:12:25,890
the cluster manager will respond first

327
00:12:25,890 --> 00:12:27,830
with the 21 bytes of
data that you can see it

328
00:12:27,830 --> 00:12:29,443
echoed back what we said before, right?

329
00:12:29,443 --> 00:12:32,860
In a different magic RPC, magic two bytes.

330
00:12:32,860 --> 00:12:33,930
But anyway,

331
00:12:33,930 --> 00:12:36,160
and if there is indeed an end point,

332
00:12:36,160 --> 00:12:38,690
it sends a Boolean that is set to true.

333
00:12:38,690 --> 00:12:39,830
So obviously,

334
00:12:39,830 --> 00:12:41,490
I took this special exchange because

335
00:12:41,490 --> 00:12:44,152
you can use it in N-Map to
fingerprint the cluster manager.

336
00:12:44,152 --> 00:12:48,660
But you have to imagine that
all Spark RPCs are these

337
00:12:48,660 --> 00:12:50,690
exchange of serialized data

338
00:12:50,690 --> 00:12:53,340
that are prefixed with
some type of header.

339
00:12:53,340 --> 00:12:55,820
Either 21 bytes header, or 13 byte header

340
00:12:55,820 --> 00:12:57,720
that we will see a bit later on,

341
00:12:57,720 --> 00:13:00,880
but this is the main way that

342
00:13:00,880 --> 00:13:02,250
spark components
communicate with each other,

343
00:13:02,250 --> 00:13:03,653
just serialized objects.

344
00:13:04,940 --> 00:13:06,893
So once we have this
exchange all mapped out,

345
00:13:06,893 --> 00:13:08,190
then we can use it,

346
00:13:08,190 --> 00:13:12,600
to write a Lua script that
will replicate the same thing.

347
00:13:12,600 --> 00:13:14,290
Once you do that obviously,

348
00:13:14,290 --> 00:13:15,830
once you deconstruct it like this,

349
00:13:15,830 --> 00:13:17,160
it becomes quite obvious to do it.

350
00:13:17,160 --> 00:13:20,360
So I wrote it N-Map script
that's on my repo...

351
00:13:20,360 --> 00:13:22,690
Sorry, and I will push it
to the real N-Map people

352
00:13:22,690 --> 00:13:26,670
once people have used it
enough and debugged it enough.

353
00:13:26,670 --> 00:13:27,503
So yeah,

354
00:13:27,503 --> 00:13:28,970
now we are able to
identify a spark cluster.

355
00:13:28,970 --> 00:13:31,080
And you can see they've
been true to their promise,

356
00:13:31,080 --> 00:13:33,910
authentication is indeed
disabled by default.

357
00:13:33,910 --> 00:13:35,110
So this is pretty great.

358
00:13:36,060 --> 00:13:37,200
Now comes the interesting part.

359
00:13:37,200 --> 00:13:40,830
Now we know how to locate spark
clusters inside a network.

360
00:13:40,830 --> 00:13:44,020
How can we execute code? Right.

361
00:13:44,020 --> 00:13:46,017
Cause that's what's real interesting.

362
00:13:46,017 --> 00:13:48,880
Now the standard way of
submitting an application

363
00:13:48,880 --> 00:13:50,990
on to a spark cluster

364
00:13:50,990 --> 00:13:52,440
is basically you write your application

365
00:13:52,440 --> 00:13:54,553
in Java or Scala and what have you,

366
00:13:55,500 --> 00:13:57,350
you package it as a jar file,

367
00:13:57,350 --> 00:13:58,440
you boot up the driver

368
00:13:58,440 --> 00:14:01,690
and then you send that
jar file to be executed

369
00:14:02,610 --> 00:14:03,960
or parsed if you will,

370
00:14:03,960 --> 00:14:06,550
by the cluster manager and the workers.

371
00:14:06,550 --> 00:14:08,700
Now, the problem is this.

372
00:14:08,700 --> 00:14:12,740
I find writing Java boring, sorry.

373
00:14:12,740 --> 00:14:15,330
And Scala makes me wanna shoot myself,

374
00:14:15,330 --> 00:14:18,620
So I decided to do it in Python.

375
00:14:18,620 --> 00:14:20,960
And luckily Spark supports Python.

376
00:14:20,960 --> 00:14:24,132
In fact, they have an official
wrapper called pyspark,

377
00:14:24,132 --> 00:14:26,460
which actually takes care of

378
00:14:26,460 --> 00:14:28,090
all the nasty stuff that I just described.

379
00:14:28,090 --> 00:14:29,740
So it will put up the driver for you.

380
00:14:29,740 --> 00:14:33,400
It will serialize JVM
object into Python object

381
00:14:33,400 --> 00:14:34,233
and vice versa and send it.

382
00:14:34,233 --> 00:14:36,690
And it will take care of
all the nasty bits for you.

383
00:14:36,690 --> 00:14:38,993
So you just write a simple Python.

384
00:14:40,042 --> 00:14:41,336
And so yeah, pyspark,

385
00:14:41,336 --> 00:14:42,993
and if you pip install pyspark,

386
00:14:42,993 --> 00:14:43,826
it really is just a thin wrapper

387
00:14:43,826 --> 00:14:46,390
It downloads 200 megabytes
of Jar file behind.

388
00:14:46,390 --> 00:14:47,900
So, hilarious.

389
00:14:47,900 --> 00:14:49,194
Anyway so from pyspark,

390
00:14:49,194 --> 00:14:52,881
we import these two classes
per context per configuration.

391
00:14:52,881 --> 00:14:55,835
It's very straightforward,
very intuitive I would say.

392
00:14:55,835 --> 00:14:57,320
So you defined the configuration.

393
00:14:57,320 --> 00:14:59,420
We're gonna name our
application word count.

394
00:14:59,420 --> 00:15:02,687
We're gonna point to the
cluster, port 7077 obviously.

395
00:15:02,687 --> 00:15:05,930
Now you have to define
your IP, your public IP,

396
00:15:05,930 --> 00:15:07,300
that you're gonna bind on.

397
00:15:07,300 --> 00:15:08,500
We're gonna see why later,

398
00:15:08,500 --> 00:15:11,390
but if you forget this
line, it will not work.

399
00:15:11,390 --> 00:15:12,860
No what what does it say so in the doc,

400
00:15:12,860 --> 00:15:14,702
but if you forget this
line it will not work.

401
00:15:14,702 --> 00:15:16,119
So anyway,

402
00:15:16,119 --> 00:15:18,073
you define the spark context.

403
00:15:20,085 --> 00:15:22,260
The Spark content is gonna be the client

404
00:15:22,260 --> 00:15:23,093
that's gonna initiate the communication,

405
00:15:23,093 --> 00:15:26,630
handle the communication
with the cluster manager.

406
00:15:26,630 --> 00:15:27,463
And basically

407
00:15:27,463 --> 00:15:31,100
when the Python interpreter
comes to this line,

408
00:15:31,100 --> 00:15:33,880
it will boot up the JVM process
we talked about the driver

409
00:15:33,880 --> 00:15:36,400
and it will initiate communication
with the cluster manager.

410
00:15:36,400 --> 00:15:38,843
So quite naively and quite
intuitively, I would say,

411
00:15:38,843 --> 00:15:41,630
what we wanna do next is obviously

412
00:15:41,630 --> 00:15:44,573
just send the code that we want to,

413
00:15:45,840 --> 00:15:48,120
be serialized and we want to
be executed on the worker.

414
00:15:48,120 --> 00:15:51,633
So from sub-process P open the command ID.

415
00:15:52,690 --> 00:15:53,640
And if you do that,

416
00:15:53,640 --> 00:15:56,320
and this is really the
first thing that I did.

417
00:15:56,320 --> 00:15:58,070
If you do that, it actually works.

418
00:15:58,070 --> 00:16:00,847
I got this result back and I was like,

419
00:16:00,847 --> 00:16:03,947
"Well, hang on. I know
that user, that's me.

420
00:16:03,947 --> 00:16:07,227
"I have that user defined
on my local machine,

421
00:16:07,227 --> 00:16:09,187
"not on the worker or
the cluster machines,

422
00:16:09,187 --> 00:16:10,320
"what the fuck happened?"

423
00:16:10,320 --> 00:16:12,300
So it turns out I actually
just executed a code

424
00:16:12,300 --> 00:16:13,640
on my own machine.

425
00:16:13,640 --> 00:16:15,540
So if you look at wireshark what happened?

426
00:16:15,540 --> 00:16:17,240
'Cause I was curious
what the fuck happened.

427
00:16:17,240 --> 00:16:18,944
I thought this stuff was
supposed to serialize

428
00:16:18,944 --> 00:16:21,288
and send the command to be
executed on the workers.

429
00:16:21,288 --> 00:16:23,087
Well what happened was,

430
00:16:23,087 --> 00:16:26,330
first, we actually sent
this 21 bytes of data,

431
00:16:26,330 --> 00:16:27,330
check existence class,

432
00:16:27,330 --> 00:16:28,940
yada yada everything's all right.

433
00:16:28,940 --> 00:16:31,710
There is indeed a cluster
manager listening.

434
00:16:31,710 --> 00:16:34,690
Then we send the register
application class,

435
00:16:34,690 --> 00:16:36,450
which contains all the
information of the application

436
00:16:36,450 --> 00:16:38,940
that we're gonna run.

437
00:16:38,940 --> 00:16:40,826
So you can see that we send

438
00:16:40,826 --> 00:16:45,110
word count plus other properties, right?

439
00:16:45,110 --> 00:16:47,700
The Cassandra responded
with executed added class.

440
00:16:47,700 --> 00:16:51,680
So they gave us actually two workers,

441
00:16:51,680 --> 00:16:52,900
so this is great.

442
00:16:52,900 --> 00:16:53,733
What happened next?

443
00:16:53,733 --> 00:16:54,730
Well, the driver,

444
00:16:54,730 --> 00:16:56,210
so our instance of pyspark

445
00:16:56,210 --> 00:16:57,530
simply unregistered the application

446
00:16:57,530 --> 00:16:59,320
and called off the whole thing.

447
00:16:59,320 --> 00:17:00,520
So what the hell?

448
00:17:00,520 --> 00:17:02,273
Well to understand what happened actually,

449
00:17:02,273 --> 00:17:05,196
remember when I said that spark contacts

450
00:17:05,196 --> 00:17:06,780
pyspark will simply serialize

451
00:17:06,780 --> 00:17:08,880
whatever comes after
the spark context line.

452
00:17:08,880 --> 00:17:09,853
Yeah, I lied.

453
00:17:10,745 --> 00:17:11,962
Well the thing is,

454
00:17:11,962 --> 00:17:13,387
in order to understand what happened,

455
00:17:13,387 --> 00:17:15,770
we basically need to
talk about two concepts.

456
00:17:15,770 --> 00:17:17,160
First of all, spark APIs,

457
00:17:17,160 --> 00:17:18,000
and second,

458
00:17:18,000 --> 00:17:20,483
the notion of a DAG and
lazy evaluation in spark.

459
00:17:23,710 --> 00:17:25,030
Counter-intuitively I would say

460
00:17:25,030 --> 00:17:28,370
spark API's are not methods
they're data structures.

461
00:17:28,370 --> 00:17:29,620
So let's say you wanna open a file

462
00:17:29,620 --> 00:17:32,430
on Python or Java or Scala or whatever,

463
00:17:32,430 --> 00:17:33,450
you point to a file

464
00:17:33,450 --> 00:17:35,633
and then you call the
method, the open method.

465
00:17:35,633 --> 00:17:37,705
This will only give you an object,

466
00:17:37,705 --> 00:17:40,867
a Python object that refers to
a handle, a memory, whatever,

467
00:17:40,867 --> 00:17:43,750
but a single threaded object nonetheless.

468
00:17:43,750 --> 00:17:46,230
There's no parallels in there.

469
00:17:46,230 --> 00:17:48,140
And the proper way to do it in spark

470
00:17:48,140 --> 00:17:50,250
and still get that parallelism is actually

471
00:17:50,250 --> 00:17:53,840
to call text file method of
this spark context class.

472
00:17:53,840 --> 00:17:57,070
So here SC is an instance
of a spark context.

473
00:17:57,070 --> 00:17:58,450
So you point to a file

474
00:17:58,450 --> 00:17:59,330
you call it text file method,

475
00:17:59,330 --> 00:18:01,860
and text file method will grab that file,

476
00:18:01,860 --> 00:18:03,360
slice it in two fragments

477
00:18:03,360 --> 00:18:06,750
and then load those two
fragments in memory.

478
00:18:06,750 --> 00:18:09,320
Similarly, if you want to load a list,

479
00:18:09,320 --> 00:18:11,220
you just don't define a list like that.

480
00:18:11,220 --> 00:18:13,630
This will give you only a single
threaded object in memory.

481
00:18:13,630 --> 00:18:15,970
No, you need to call the paralyzed method,

482
00:18:15,970 --> 00:18:17,180
which will take that list,

483
00:18:17,180 --> 00:18:19,270
slice it in half or three or four,

484
00:18:19,270 --> 00:18:22,150
whatever spark deems necessary.

485
00:18:22,150 --> 00:18:24,340
And that way you will end up
with two fragments in file

486
00:18:24,340 --> 00:18:27,670
on which you can perform
work in a parallel fashion.

487
00:18:27,670 --> 00:18:30,352
Now these fragments are called partitions,

488
00:18:30,352 --> 00:18:31,890
and this is a very frequent term in spark

489
00:18:31,890 --> 00:18:32,910
that will come up a lot.

490
00:18:32,910 --> 00:18:37,830
Partitions are the main
unit of work inside spark.

491
00:18:37,830 --> 00:18:41,040
And these collection of partitions

492
00:18:42,087 --> 00:18:43,320
are called resilient distributed datasets,

493
00:18:43,320 --> 00:18:44,970
which is really the most
fundamental API in spark.

494
00:18:44,970 --> 00:18:46,158
There are others but they're ultimately

495
00:18:46,158 --> 00:18:50,470
all based on our other core.

496
00:18:50,470 --> 00:18:54,010
So once we have these partitions,

497
00:18:54,010 --> 00:18:56,370
then we can apply paralyzed work

498
00:18:56,370 --> 00:18:58,670
using something called
transformations in action.

499
00:18:58,670 --> 00:19:00,360
Now what's a transformation?

500
00:19:00,360 --> 00:19:01,870
A transformation.

501
00:19:01,870 --> 00:19:03,860
Now let's say I wanna multiply
every element on the list

502
00:19:03,860 --> 00:19:04,693
by 20 let's say.

503
00:19:04,693 --> 00:19:08,010
A concrete example will
actually make it better.

504
00:19:08,010 --> 00:19:09,700
So I define a method called multiply,

505
00:19:09,700 --> 00:19:13,580
which takes an element,
multiplies it by 20.

506
00:19:13,580 --> 00:19:15,810
And then I just called
a map transformation,

507
00:19:15,810 --> 00:19:18,730
which will iterate over every element

508
00:19:18,730 --> 00:19:20,940
in every partition and yield,

509
00:19:20,940 --> 00:19:22,070
the same number of elements

510
00:19:22,070 --> 00:19:23,400
in the same number of partitions.

511
00:19:23,400 --> 00:19:24,410
So that's a transformation.

512
00:19:24,410 --> 00:19:26,190
There are other types of transformation

513
00:19:26,190 --> 00:19:29,790
which may or may not yield
the same number of partitions

514
00:19:29,790 --> 00:19:30,930
and elements by the way.

515
00:19:30,930 --> 00:19:32,510
So yeah,

516
00:19:32,510 --> 00:19:35,410
each spark API basically
defines these transformations

517
00:19:35,410 --> 00:19:37,500
and actions to work on.

518
00:19:37,500 --> 00:19:39,560
And so what happens is,

519
00:19:39,560 --> 00:19:40,900
if I write this code,

520
00:19:40,900 --> 00:19:43,820
thinking that I just
multiplied every element by 20,

521
00:19:43,820 --> 00:19:45,840
actually nothing gets sent
to the workers just yet.

522
00:19:45,840 --> 00:19:46,690
Nothing happens.

523
00:19:46,690 --> 00:19:47,730
Nothing gets computed.

524
00:19:47,730 --> 00:19:50,043
I will have the same
results as I had earlier.

525
00:19:50,043 --> 00:19:51,653
In fact I will have nothing.

526
00:19:53,910 --> 00:19:55,970
Why? Because when the driver

527
00:19:56,940 --> 00:19:59,310
starts parsing this application,

528
00:19:59,310 --> 00:20:00,480
all it does is

529
00:20:00,480 --> 00:20:03,363
it actually builds an execution graph.

530
00:20:04,647 --> 00:20:06,140
And this execution graph is called

531
00:20:06,140 --> 00:20:07,460
the DAG and spark terminology.

532
00:20:07,460 --> 00:20:10,160
But basically all it does is

533
00:20:10,160 --> 00:20:12,340
it follows all these calls that we made.

534
00:20:12,340 --> 00:20:13,173
So, Oh yeah.

535
00:20:14,400 --> 00:20:15,789
It knows that it's gonna
call to parallelize,

536
00:20:15,789 --> 00:20:18,260
So it knows that it's gonna build an RTD,

537
00:20:18,260 --> 00:20:19,280
it doesn't care what's inside.

538
00:20:19,280 --> 00:20:20,980
It just knows that it's gonna build RTD

539
00:20:20,980 --> 00:20:23,010
with X number of partitions.

540
00:20:23,010 --> 00:20:23,843
Then Oh yeah,

541
00:20:23,843 --> 00:20:24,770
we're gonna call a map.

542
00:20:24,770 --> 00:20:26,140
It doesn't care what's inside the map.

543
00:20:26,140 --> 00:20:27,356
It doesn't care that we're gonna multiply

544
00:20:27,356 --> 00:20:28,480
every element by 20

545
00:20:28,480 --> 00:20:29,740
that's not the driver's job,

546
00:20:29,740 --> 00:20:32,450
the driver's job is only to follow

547
00:20:32,450 --> 00:20:33,990
to build this execution graph.

548
00:20:33,990 --> 00:20:35,930
So it knows that it's gonna apply a map.

549
00:20:35,930 --> 00:20:37,730
So given a set number of partition,

550
00:20:37,730 --> 00:20:40,650
it's gonna have and output
the same number of partition.

551
00:20:40,650 --> 00:20:42,841
It's gonna continue to
building up this tag.

552
00:20:42,841 --> 00:20:44,150
Maybe there's gonna be a filter map,

553
00:20:44,150 --> 00:20:46,614
which is another type of
transformation, et cetera.

554
00:20:46,614 --> 00:20:48,000
And it's gonna continue
building this graph

555
00:20:48,000 --> 00:20:49,300
until it hits an action,

556
00:20:49,300 --> 00:20:50,500
an action in spark is something

557
00:20:50,500 --> 00:20:53,660
that's gonna force it to
do the actual computation.

558
00:20:53,660 --> 00:20:55,150
So things like save file

559
00:20:55,150 --> 00:20:57,540
or collect or take sample.

560
00:20:57,540 --> 00:20:58,960
All these methods

561
00:20:58,960 --> 00:21:00,080
that you can call in an RTD

562
00:21:00,080 --> 00:21:02,951
that will force it to
actually have the right value.

563
00:21:02,951 --> 00:21:03,920
And once you do that,

564
00:21:03,920 --> 00:21:05,170
once you call a collect

565
00:21:05,170 --> 00:21:07,330
or an action in spark,

566
00:21:07,330 --> 00:21:09,330
then and only then will the graph

567
00:21:09,330 --> 00:21:11,117
be sent to the executors.

568
00:21:11,117 --> 00:21:13,360
The graph, mind you, not the code.

569
00:21:13,360 --> 00:21:15,100
Simply the graph.

570
00:21:15,100 --> 00:21:16,430
And it goes a little bit like this,

571
00:21:16,430 --> 00:21:18,550
or the driver will send to the workers,

572
00:21:18,550 --> 00:21:19,690
the graph of execution,

573
00:21:19,690 --> 00:21:22,480
the workers will go through this graph

574
00:21:22,480 --> 00:21:23,717
and they'll go, "Oh, I
need to parallelize it,

575
00:21:23,717 --> 00:21:24,617
"but I don't have that list.

576
00:21:24,617 --> 00:21:25,863
"Oh, you know what? Give me that list."

577
00:21:25,863 --> 00:21:28,950
And the driver will send
all these serialized objects

578
00:21:28,950 --> 00:21:30,397
one after the other.

579
00:21:30,397 --> 00:21:31,387
"I need to do a map.

580
00:21:31,387 --> 00:21:33,750
"Oh, but I don't have
the function multiplied."

581
00:21:33,750 --> 00:21:37,310
You know, they ask the driver
to serialize the function,

582
00:21:37,310 --> 00:21:39,540
the method and send it to
the workers, et cetera.

583
00:21:39,540 --> 00:21:40,957
And then they're gonna
apply those methods.

584
00:21:40,957 --> 00:21:44,480
And each worker will
only apply these methods,

585
00:21:44,480 --> 00:21:46,803
will only follow the DAG
on its own partition.

586
00:21:47,710 --> 00:21:48,904
So the worker number one

587
00:21:48,904 --> 00:21:50,477
will loop through the elements
of the first partition.

588
00:21:50,477 --> 00:21:52,748
So one, two, three multiply by 20,

589
00:21:52,748 --> 00:21:54,099
20, 40, 60,

590
00:21:54,099 --> 00:21:55,050
send back the result.

591
00:21:55,050 --> 00:21:56,607
The worker number two will do the same

592
00:21:56,607 --> 00:21:58,630
and the driver will aggregate the results

593
00:21:58,630 --> 00:22:01,730
and present it to the operator, us.

594
00:22:01,730 --> 00:22:03,583
So when you think about it this way,

595
00:22:03,583 --> 00:22:06,750
and you go back to our code
earlier with our sub-process,

596
00:22:06,750 --> 00:22:10,750
that was just sitting there
naked alone in the wilderness

597
00:22:10,750 --> 00:22:11,717
well, of course it didn't
make it to the workers.

598
00:22:11,717 --> 00:22:14,710
The workers are only
ever aware of the DAG.

599
00:22:14,710 --> 00:22:16,763
And so, if we want to execute code,

600
00:22:16,763 --> 00:22:19,033
we need to somehow
embed it inside the DAG.

601
00:22:20,130 --> 00:22:23,360
So we need to put it
inside a transformation,

602
00:22:23,360 --> 00:22:24,520
hence the following code.

603
00:22:24,520 --> 00:22:26,760
So here we can see we
define a lambda function,

604
00:22:26,760 --> 00:22:27,854
anonymous function,

605
00:22:27,854 --> 00:22:32,420
and inside we put our Popen
command execution stuff.

606
00:22:32,420 --> 00:22:34,780
And obviously we need to
follow it by an action.

607
00:22:34,780 --> 00:22:36,350
Otherwise, nothing gets sent.

608
00:22:36,350 --> 00:22:38,800
And this is the skeleton
that you need to follow,

609
00:22:38,800 --> 00:22:40,780
or the basic skeleton you need to follow

610
00:22:40,780 --> 00:22:43,560
in order to achieve
code execution on spark,

611
00:22:43,560 --> 00:22:45,773
across multiple machines.

612
00:22:45,773 --> 00:22:49,257
Now, if you don't want to
be bothered by creating

613
00:22:49,257 --> 00:22:51,620
RDD's handling parallelism and whatnot,

614
00:22:51,620 --> 00:22:53,354
I took this skeleton,

615
00:22:53,354 --> 00:22:55,077
put it in a fancier way
or something like that.

616
00:22:55,077 --> 00:22:57,610
And I incorporate it
in a tool called Sparky

617
00:22:57,610 --> 00:22:59,700
that will take care of
all this stuff for you.

618
00:22:59,700 --> 00:23:01,710
So you just point to a closer point

619
00:23:01,710 --> 00:23:04,273
to the IP that you want to target,

620
00:23:05,320 --> 00:23:06,153
send the command that you want,

621
00:23:06,153 --> 00:23:08,530
and then how many times,
on how many workers

622
00:23:08,530 --> 00:23:10,117
you want it to execute,

623
00:23:10,117 --> 00:23:11,570
and it will do that for you.

624
00:23:11,570 --> 00:23:13,649
It will build RDDs, build the APIs,

625
00:23:13,649 --> 00:23:15,100
call the transformations,

626
00:23:15,100 --> 00:23:17,050
the right ones in order
to achieve the code,

627
00:23:17,050 --> 00:23:19,250
the execution that you want.

628
00:23:19,250 --> 00:23:20,960
And this is what it looks like really.

629
00:23:20,960 --> 00:23:25,159
So again, I'm gonna publish
it on GitHub for everyone.

630
00:23:25,159 --> 00:23:27,510
So yeah, you will have
chance to look at the code.

631
00:23:27,510 --> 00:23:28,670
So you can see here, we
just point to the cluster,

632
00:23:28,670 --> 00:23:31,140
so we specify our public IP address,

633
00:23:31,140 --> 00:23:32,590
the command that we want to execute

634
00:23:32,590 --> 00:23:34,940
and the number of workers
that we wanna target.

635
00:23:37,092 --> 00:23:38,130
And you can see here that we have

636
00:23:38,130 --> 00:23:39,940
execution on three machines.

637
00:23:39,940 --> 00:23:41,010
So great.

638
00:23:41,010 --> 00:23:42,400
I'd also had some, you know,

639
00:23:42,400 --> 00:23:44,010
I embedded some scripts

640
00:23:44,010 --> 00:23:46,720
to make it easy to run
in an AWS environment.

641
00:23:46,720 --> 00:23:51,300
So like, dump some AWS
secrets if you want,

642
00:23:51,300 --> 00:23:55,220
or search for files that are
sometimes dumped by spark

643
00:23:57,170 --> 00:23:58,280
in some work folder.

644
00:23:58,280 --> 00:24:01,790
So you can search for AWS
keys or secrets or whatever.

645
00:24:01,790 --> 00:24:06,430
So it automates a lot of
the pentests in spark so...

646
00:24:09,824 --> 00:24:11,720
So that will get you started.

647
00:24:11,720 --> 00:24:12,553
Next.

648
00:24:12,553 --> 00:24:15,450
Now in this type of the
execution that I just showed,

649
00:24:15,450 --> 00:24:18,200
there are some problems
that are quite annoying.

650
00:24:18,200 --> 00:24:19,660
Let me go through them very quickly.

651
00:24:19,660 --> 00:24:20,850
First of all,

652
00:24:20,850 --> 00:24:22,600
the Python version on the driver

653
00:24:22,600 --> 00:24:24,937
needs to match the one on the worker,

654
00:24:24,937 --> 00:24:26,610
down to the minor version

655
00:24:26,610 --> 00:24:28,384
otherwise it will throw an error.

656
00:24:28,384 --> 00:24:29,248
Now there's a way...

657
00:24:29,248 --> 00:24:30,434
Yeah, so you can see

658
00:24:30,434 --> 00:24:33,590
even if you have a worker that's on 3.5,

659
00:24:33,590 --> 00:24:35,440
and your driver is on 3.7,
you're kind of screwed.

660
00:24:35,440 --> 00:24:37,930
So there's a way around it.

661
00:24:37,930 --> 00:24:40,440
Since the check is simply done
by checking the version info,

662
00:24:40,440 --> 00:24:42,030
you can just override it.

663
00:24:42,030 --> 00:24:44,293
So you can see here
that I put it like 3.5.

664
00:24:45,340 --> 00:24:46,173
And if you do that,

665
00:24:46,173 --> 00:24:49,050
it will work so long as it
is not too much of a gap

666
00:24:49,050 --> 00:24:51,620
between the two versions.

667
00:24:51,620 --> 00:24:52,453
So you can get away with 3.6 and 3.7

668
00:24:53,962 --> 00:24:57,524
because the serialized objects
are similar to some extent.

669
00:24:57,524 --> 00:24:59,193
But you cannot get away
with a 3.4 and a 3.7,

670
00:25:00,810 --> 00:25:02,420
there's too much of a gap.

671
00:25:02,420 --> 00:25:05,640
And the way around that is simply

672
00:25:05,640 --> 00:25:07,170
well to override the file,

673
00:25:07,170 --> 00:25:08,973
but also to take
advantage of the fact that

674
00:25:08,973 --> 00:25:10,723
a spark cluster or pyspark will use

675
00:25:11,817 --> 00:25:15,190
use pickle deserializer
to deserialize the object.

676
00:25:15,190 --> 00:25:16,520
And in pickle,

677
00:25:16,520 --> 00:25:18,260
you can specify a method

678
00:25:18,260 --> 00:25:22,030
if you define the method,
with use of an object,

679
00:25:22,030 --> 00:25:23,700
It will get executed first

680
00:25:23,700 --> 00:25:26,440
in the process of de-serialization.

681
00:25:26,440 --> 00:25:28,603
So before it fails,

682
00:25:29,456 --> 00:25:32,310
the de-serialization process later on,

683
00:25:32,310 --> 00:25:34,660
it will have executed the command reduce,

684
00:25:34,660 --> 00:25:36,220
which contains the execution command.

685
00:25:36,220 --> 00:25:39,240
And so, we simply need
to define the object

686
00:25:39,240 --> 00:25:40,750
and send the method as part of that object

687
00:25:40,750 --> 00:25:41,790
and will get you through.

688
00:25:41,790 --> 00:25:44,760
So even though the job submission fails,

689
00:25:44,760 --> 00:25:46,610
you still achieve code execution

690
00:25:46,610 --> 00:25:49,650
over however many workers you decided.

691
00:25:49,650 --> 00:25:52,330
And I did not incorporate it inside Sparky

692
00:25:52,330 --> 00:25:54,120
because it's just a hacky way.

693
00:25:54,120 --> 00:25:55,150
Really the easiest way is

694
00:25:55,150 --> 00:25:57,750
simply to align your version of of Python

695
00:25:57,750 --> 00:25:58,583
with that of the workers.

696
00:25:58,583 --> 00:26:00,257
But oh well,

697
00:26:00,257 --> 00:26:02,900
this is just a fun trick to share.

698
00:26:02,900 --> 00:26:05,730
But really the most annoying problem is

699
00:26:05,730 --> 00:26:07,340
actually a network problem.

700
00:26:07,340 --> 00:26:09,940
See what happens when we
just run the application,

701
00:26:09,940 --> 00:26:10,773
we send an applications,

702
00:26:10,773 --> 00:26:12,430
is we contact the driver...

703
00:26:13,846 --> 00:26:16,720
Sorry, the driver contacts
the cluster manager

704
00:26:16,720 --> 00:26:19,780
on an RPC port 7077 as we just saw,

705
00:26:19,780 --> 00:26:21,750
and will send a register
application class,

706
00:26:21,750 --> 00:26:25,850
along with the parameters, the
applications name, et cetera.

707
00:26:25,850 --> 00:26:27,547
The cluster manager will
receive that applications,

708
00:26:27,547 --> 00:26:29,847
"Oh yeah, I need three,
four workers whatever.

709
00:26:30,928 --> 00:26:31,837
"I'm gonna fetch the workers,

710
00:26:31,837 --> 00:26:35,720
"send them to the driver
along with some status data."

711
00:26:35,720 --> 00:26:36,766
But then just when you would think

712
00:26:36,766 --> 00:26:40,000
that it would be the driver's
job to contact the workers,

713
00:26:40,000 --> 00:26:42,300
actually it's the workers

714
00:26:42,300 --> 00:26:44,640
who initiate the communication
towards the driver.

715
00:26:44,640 --> 00:26:46,900
So they will be the one
contacting the driver

716
00:26:46,900 --> 00:26:48,900
on a random RPC port calling,

717
00:26:48,900 --> 00:26:52,520
scheduling port to say, "Hey,
we're ready." et cetera.

718
00:26:52,520 --> 00:26:54,040
And that's annoying because

719
00:26:54,040 --> 00:26:58,030
you don't necessarily have this
flow of communication open.

720
00:26:58,030 --> 00:26:58,953
Cause the driver is...

721
00:26:58,953 --> 00:27:01,492
your computer is usually
behind the NAT or whatever.

722
00:27:01,492 --> 00:27:03,540
So you've got a screwed, not only that,

723
00:27:03,540 --> 00:27:05,603
the workers who initiate
another communication

724
00:27:05,603 --> 00:27:07,170
on another port altogether

725
00:27:07,170 --> 00:27:09,470
to call him the block manager ports

726
00:27:09,470 --> 00:27:10,880
in order to receive some data blocks,

727
00:27:10,880 --> 00:27:13,214
in order to request
some serialized objects,

728
00:27:13,214 --> 00:27:14,780
broadcast some variables and whatnot.

729
00:27:14,780 --> 00:27:17,290
So that's annoying.

730
00:27:17,290 --> 00:27:21,540
So in sparky, basically
I default the settings to

731
00:27:21,540 --> 00:27:24,650
some common ports in order to evade or

732
00:27:24,650 --> 00:27:27,410
it works in some settings, but not all.

733
00:27:27,410 --> 00:27:29,500
But really the solution is to use

734
00:27:29,500 --> 00:27:31,290
what we call a cluster mode.

735
00:27:31,290 --> 00:27:33,230
And in a cluster mode,

736
00:27:33,230 --> 00:27:35,400
you don't run the driver on your laptop.

737
00:27:35,400 --> 00:27:39,330
You simply contact the cluster
management on port 7077.

738
00:27:39,330 --> 00:27:40,327
And then you tell it,

739
00:27:40,327 --> 00:27:42,047
"Oh, here's the application to execute.

740
00:27:42,047 --> 00:27:46,547
"And please also set up the
driver on your own cluster."

741
00:27:48,219 --> 00:27:49,450
And that works pretty well.

742
00:27:49,450 --> 00:27:50,300
So in this case,

743
00:27:50,300 --> 00:27:53,090
you only need access to port 7077.

744
00:27:53,090 --> 00:27:56,400
The downside is that
you cannot use pyspark

745
00:27:56,400 --> 00:27:58,505
you cannot use Python.

746
00:27:58,505 --> 00:28:00,390
So I had to redo that skeleton

747
00:28:00,390 --> 00:28:01,880
I showed you earlier in Python,

748
00:28:01,880 --> 00:28:04,080
I had to do it in Scala.

749
00:28:04,080 --> 00:28:07,440
I had to build a jar file
that will paralyze...

750
00:28:07,440 --> 00:28:10,410
That would execute whatever
command you give it

751
00:28:10,410 --> 00:28:13,620
across however many servers that you want.

752
00:28:13,620 --> 00:28:16,470
So yeah, the code is again on GitHub,

753
00:28:16,470 --> 00:28:18,333
but you don't need to touch this one,

754
00:28:18,333 --> 00:28:20,223
it's this compiled jar file.

755
00:28:21,666 --> 00:28:23,650
So you can just use it as it is.

756
00:28:23,650 --> 00:28:25,640
It's yeah.

757
00:28:25,640 --> 00:28:27,330
So here's a quick example.

758
00:28:27,330 --> 00:28:30,150
So you can see we host this
jar file that I just showed you

759
00:28:30,150 --> 00:28:31,983
on a server that is
reachable by the workers,

760
00:28:31,983 --> 00:28:34,710
and you simply use Sparky again,

761
00:28:34,710 --> 00:28:37,410
to build that command to
register this in question mode.

762
00:28:37,410 --> 00:28:40,620
So we just point to this jar file command

763
00:28:40,620 --> 00:28:41,655
we want to execute,

764
00:28:41,655 --> 00:28:44,830
and then we don't need
any network condition,

765
00:28:44,830 --> 00:28:48,047
except for that 7077 to reach the cluster.

766
00:28:48,047 --> 00:28:49,340
And it works pretty well.

767
00:28:49,340 --> 00:28:51,670
And it will even auto delete itself,

768
00:28:51,670 --> 00:28:53,260
so you don't have to worry about all that.

769
00:28:53,260 --> 00:28:56,830
So this gets us through the network thing.

770
00:28:56,830 --> 00:28:57,780
So these are the main ways

771
00:28:57,780 --> 00:29:00,773
to execute code on spark that I found

772
00:29:00,773 --> 00:29:03,830
and on a spark cluster
that has no authentication,

773
00:29:03,830 --> 00:29:06,480
which is enabled by default.

774
00:29:06,480 --> 00:29:07,313
So anyway,

775
00:29:07,313 --> 00:29:10,425
let's go explore some
other facets of spark.

776
00:29:10,425 --> 00:29:13,130
And there's this
interesting interface called

777
00:29:13,130 --> 00:29:14,760
well it's the rest API.

778
00:29:14,760 --> 00:29:16,670
And it is interesting because

779
00:29:16,670 --> 00:29:18,210
when you go through the documentation,

780
00:29:18,210 --> 00:29:21,160
you can see that it's
available on port 6066.

781
00:29:21,160 --> 00:29:22,960
And when you look at the documentation,

782
00:29:22,960 --> 00:29:25,960
it's really a read, okay, boring API.

783
00:29:25,960 --> 00:29:27,310
I was like, "Yeah, whatever."

784
00:29:27,310 --> 00:29:29,380
And I remember a few days later

785
00:29:29,380 --> 00:29:30,690
I was digging through the source code,

786
00:29:30,690 --> 00:29:33,796
like really lost inside
the source code amongst

787
00:29:33,796 --> 00:29:37,420
weird scholarship Funkster's
options and whatnot.

788
00:29:37,420 --> 00:29:40,189
And I remember I saw this
keyword with the the rest,

789
00:29:40,189 --> 00:29:41,457
I was like, "Oh my God, I know that is."

790
00:29:41,457 --> 00:29:43,690
And I grabbed it and I followed it.

791
00:29:43,690 --> 00:29:45,730
And I came across this piece of code

792
00:29:45,730 --> 00:29:47,177
that says something like,

793
00:29:47,177 --> 00:29:48,990
"Submit requests, create."

794
00:29:48,990 --> 00:29:50,527
I was like, "Wow, I like to submit stuff.

795
00:29:50,527 --> 00:29:51,810
"I like to create stuff."

796
00:29:51,810 --> 00:29:53,160
So what does this thing do?

797
00:29:54,443 --> 00:29:56,780
And you follow this
create submission request

798
00:29:56,780 --> 00:29:59,940
and there's talk about
app resource, main class.

799
00:29:59,940 --> 00:30:04,197
I'm like, "Well hang on, app
resources references a jar

800
00:30:04,197 --> 00:30:05,870
"usually in the spark code.

801
00:30:05,870 --> 00:30:08,377
"I thought this was a read only API,

802
00:30:08,377 --> 00:30:09,680
"what the hell is going on?"

803
00:30:09,680 --> 00:30:12,260
So I followed this
create submission request

804
00:30:12,260 --> 00:30:13,710
and I came across this

805
00:30:13,710 --> 00:30:16,260
app resources indeed and application jar.

806
00:30:16,260 --> 00:30:17,650
So basically the rest API,

807
00:30:17,650 --> 00:30:18,940
even though in the documentation

808
00:30:18,940 --> 00:30:20,690
it mentions it's only a read only API

809
00:30:20,690 --> 00:30:22,170
actually in the code

810
00:30:22,170 --> 00:30:24,150
it accepts a jar file
that you can send it.

811
00:30:24,150 --> 00:30:26,270
Which is kind of amazing.

812
00:30:26,270 --> 00:30:30,037
So all you have to do is
simply build a JSON file,

813
00:30:30,037 --> 00:30:31,810
takes up a JSON input.

814
00:30:31,810 --> 00:30:34,040
So you build a JSON file
with the right property.

815
00:30:34,040 --> 00:30:37,340
So the app resource, main class,
spawn properties and so on,

816
00:30:37,340 --> 00:30:39,760
and then you can send your jar file

817
00:30:39,760 --> 00:30:40,757
and achieve code execution.

818
00:30:40,757 --> 00:30:44,260
And we can take that Jar
file we used earlier,

819
00:30:44,260 --> 00:30:45,570
So simple add jar,

820
00:30:45,570 --> 00:30:46,470
send it a command arc.

821
00:30:46,470 --> 00:30:49,830
So it just accepts commands
to execute in base 64.

822
00:30:49,830 --> 00:30:52,050
So this is really, I think a, who am I?

823
00:30:52,050 --> 00:30:52,883
Or some shit like...

824
00:30:52,883 --> 00:30:53,716
No, not who am I

825
00:30:53,716 --> 00:30:56,487
but touch some file and some folder,

826
00:30:56,487 --> 00:30:59,095
maybe 64 it's very simple.

827
00:30:59,095 --> 00:30:59,980
But there's different properties

828
00:30:59,980 --> 00:31:02,910
and you can just curl it this payload,

829
00:31:02,910 --> 00:31:03,840
then it will execute code.

830
00:31:03,840 --> 00:31:05,413
It's as simple as that,

831
00:31:06,694 --> 00:31:08,894
This is just to show
you what it looks like.

832
00:31:09,800 --> 00:31:11,050
But yeah.

833
00:31:11,050 --> 00:31:13,600
But again, if you use this method,

834
00:31:13,600 --> 00:31:15,620
the driver, you don't need
it to run on your machine,

835
00:31:15,620 --> 00:31:17,210
its gonna run in the cluster.

836
00:31:17,210 --> 00:31:18,350
So it's gonna run in cluster mode.

837
00:31:18,350 --> 00:31:20,120
And one thing I will mention,

838
00:31:20,120 --> 00:31:21,870
if you decide to use the rest API,

839
00:31:21,870 --> 00:31:22,810
or Cluster-Mode in general,

840
00:31:22,810 --> 00:31:24,217
you can send whatever jar file you want.

841
00:31:24,217 --> 00:31:26,827
You don't have to use
my specific jar file.

842
00:31:26,827 --> 00:31:29,190
But you will get execution on one worker,

843
00:31:29,190 --> 00:31:30,643
but if you want to execute on 200, 300

844
00:31:30,643 --> 00:31:33,200
or all the workers inside the cluster,

845
00:31:33,200 --> 00:31:34,620
then you need to use spark API.

846
00:31:34,620 --> 00:31:38,691
So you need to use some
version of the spark,

847
00:31:38,691 --> 00:31:42,773
Like spark API I just showed
in the Scala code earlier,

848
00:31:43,770 --> 00:31:44,930
or a Java code earlier.

849
00:31:44,930 --> 00:31:47,710
So anyway, so this was fun.

850
00:31:47,710 --> 00:31:48,544
And I thought,

851
00:31:48,544 --> 00:31:50,220
"Well I wonder if anybody
blogged about it?"

852
00:31:50,220 --> 00:31:52,540
And I Googled create submission request,

853
00:31:52,540 --> 00:31:53,860
I called this and I came across

854
00:31:53,860 --> 00:31:56,490
the first results on
Google was this script.

855
00:31:56,490 --> 00:31:57,760
And it was already published.

856
00:31:57,760 --> 00:31:59,670
And I was like, "Fuck, so close."

857
00:31:59,670 --> 00:32:00,503
Not only that,

858
00:32:00,503 --> 00:32:04,350
there was actually an exploit
that was already published.

859
00:32:04,350 --> 00:32:06,760
It was actually a Metasploit model

860
00:32:06,760 --> 00:32:08,680
that was published by
these fine gentlemen.

861
00:32:08,680 --> 00:32:09,560
I was like, "Damnit."

862
00:32:09,560 --> 00:32:10,690
You know,

863
00:32:10,690 --> 00:32:13,310
sometimes you keep your head in the wheel

864
00:32:13,310 --> 00:32:15,110
and you forget to do basic research,

865
00:32:15,110 --> 00:32:17,970
but it was just, you know, months away.

866
00:32:17,970 --> 00:32:22,240
Anyway, let's get to
the real stuff. Right?

867
00:32:22,240 --> 00:32:24,390
So authentication.

868
00:32:24,390 --> 00:32:26,500
Let's say you come across a cluster

869
00:32:26,500 --> 00:32:29,190
that has all the right switches on, right?

870
00:32:29,190 --> 00:32:30,810
So it's enabled authentication,

871
00:32:30,810 --> 00:32:32,920
and the way to do that
in spark is fairly easy,

872
00:32:32,920 --> 00:32:35,280
you just enable authenticate to true.

873
00:32:35,280 --> 00:32:37,504
Then you define a secret
as you can see here,

874
00:32:37,504 --> 00:32:39,877
and yeah all the components you do...

875
00:32:39,877 --> 00:32:42,090
You need to do that on every
component inside the cluster.

876
00:32:42,090 --> 00:32:42,950
So every worker

877
00:32:42,950 --> 00:32:46,230
and on the cluster manager and the driver

878
00:32:46,230 --> 00:32:47,420
and that way, they all have

879
00:32:47,420 --> 00:32:48,393
a shared secret to communicate with.

880
00:32:48,393 --> 00:32:50,570
And if you have an authenticated cluster

881
00:32:50,570 --> 00:32:53,170
and you try to communicate with it,

882
00:32:53,170 --> 00:32:55,417
you will get this delightful
error that says, you know,

883
00:32:55,417 --> 00:32:57,950
You send this check systems class,

884
00:32:57,950 --> 00:33:00,800
and then you get this illegal
state exemption Sasl message.

885
00:33:00,800 --> 00:33:03,090
Now Sasl is an authentication protocol,

886
00:33:03,090 --> 00:33:04,490
challenge response protocol,

887
00:33:05,419 --> 00:33:09,133
based off a shared secret
as you might've guessed.

888
00:33:10,380 --> 00:33:11,213
But yeah basically

889
00:33:11,213 --> 00:33:12,377
the server sends nonce to the client

890
00:33:12,377 --> 00:33:14,630
and the client hashes that with the nonce,

891
00:33:14,630 --> 00:33:15,880
hashes the secrets with the nonce

892
00:33:15,880 --> 00:33:17,120
and then sends the response

893
00:33:17,120 --> 00:33:18,490
plus other parameters that we will see,

894
00:33:18,490 --> 00:33:20,420
but fairly simple.

895
00:33:20,420 --> 00:33:21,370
But the funny thing is that

896
00:33:21,370 --> 00:33:22,490
even though the driver

897
00:33:22,490 --> 00:33:24,530
you configure the driver to use the secret

898
00:33:24,530 --> 00:33:26,940
and you tell it to authenticate,

899
00:33:26,940 --> 00:33:29,970
it will first attempt an
unauthenticated session.

900
00:33:29,970 --> 00:33:31,060
So it will first

901
00:33:32,740 --> 00:33:34,450
try check existence,

902
00:33:34,450 --> 00:33:36,020
get slapped with this error

903
00:33:36,020 --> 00:33:38,294
and then it will try an
authenticated message.

904
00:33:38,294 --> 00:33:39,161
It's kind of weird.

905
00:33:39,161 --> 00:33:40,750
So anyway, what does it look like?

906
00:33:40,750 --> 00:33:42,100
An authenticated session?

907
00:33:42,100 --> 00:33:45,267
Well, first the driver
will send 21 bytes of data,

908
00:33:45,267 --> 00:33:46,980
the header, the usual Header.

909
00:33:46,980 --> 00:33:50,480
With the magic byte this time set to 2BO3

910
00:33:50,480 --> 00:33:52,710
as opposed to C3O5 earlier.

911
00:33:52,710 --> 00:33:54,817
Followed by eight random
bytes that we don't care about

912
00:33:54,817 --> 00:33:56,480
the size of the payload,

913
00:33:56,480 --> 00:33:58,110
which is definitely wrong in this case,

914
00:33:58,110 --> 00:33:59,770
because we're not talking.

915
00:33:59,770 --> 00:34:02,170
It's not 176 bytes.

916
00:34:02,170 --> 00:34:03,003
But anyway,

917
00:34:03,003 --> 00:34:04,500
the most important thing in
this authentication sequence

918
00:34:04,500 --> 00:34:07,530
is that it sends the spark Sasl user,

919
00:34:07,530 --> 00:34:10,110
which is gonna be the user used inside

920
00:34:10,110 --> 00:34:12,940
the challenge calculation.

921
00:34:12,940 --> 00:34:13,773
So anyway,

922
00:34:13,773 --> 00:34:15,890
the cluster manager responds
with 21 bytes of data as usual

923
00:34:15,890 --> 00:34:16,880
the header.

924
00:34:16,880 --> 00:34:20,870
And then it follows by the Sasl parameters

925
00:34:20,870 --> 00:34:22,690
to perform the challenge computation.

926
00:34:22,690 --> 00:34:27,260
So you have the nonce random
value realm QOP which is

927
00:34:27,260 --> 00:34:29,590
are we doing authentication
or encryption or both?

928
00:34:29,590 --> 00:34:31,790
These delightful algorithms to use

929
00:34:31,790 --> 00:34:34,150
in case we're doing Sasl encryption.

930
00:34:34,150 --> 00:34:35,418
Now just to reassure you

931
00:34:35,418 --> 00:34:39,980
spark does not use Sasl
encryption by default.

932
00:34:39,980 --> 00:34:41,920
It uses AES, but it's definitely possible

933
00:34:41,920 --> 00:34:44,010
to use it as a fallback.

934
00:34:44,010 --> 00:34:45,059
And the algorithm...

935
00:34:45,059 --> 00:34:48,420
And this one is indeed
used to calculate the hash.

936
00:34:48,420 --> 00:34:49,650
And the calculation of the hash,

937
00:34:49,650 --> 00:34:52,290
we're not gonna do it honestly.

938
00:34:52,290 --> 00:34:54,210
It's detailed and RFC 2831,

939
00:34:54,210 --> 00:34:56,910
but it take a bunch of
data, you hash it in MD5,

940
00:34:56,910 --> 00:34:57,743
you take the output

941
00:34:57,743 --> 00:34:58,890
you combine it with the
nonce and other stuff.

942
00:34:58,890 --> 00:35:01,510
And then you hashed a bunch...

943
00:35:01,510 --> 00:35:02,760
You hash other types of data

944
00:35:02,760 --> 00:35:05,410
and then you combine everything,

945
00:35:05,410 --> 00:35:07,810
which gives you the
response to the challenge.

946
00:35:07,810 --> 00:35:09,820
Again, if you want to know more about it,

947
00:35:09,820 --> 00:35:11,373
Google it, RFC 2831.

948
00:35:12,765 --> 00:35:16,053
The driver sends the response
to the cluster manager,

949
00:35:17,180 --> 00:35:18,013
if everything checks out

950
00:35:18,013 --> 00:35:19,250
the cluster manager acts.

951
00:35:21,207 --> 00:35:26,207
And then it can follow the
classic flow of communication.

952
00:35:26,290 --> 00:35:28,571
So basically, register class,

953
00:35:28,571 --> 00:35:32,692
register application, or
register with application

954
00:35:32,692 --> 00:35:35,670
executors added. Yada yada.

955
00:35:35,670 --> 00:35:36,970
Now here's what bothered me,

956
00:35:36,970 --> 00:35:37,830
or maybe not bothered me,

957
00:35:37,830 --> 00:35:40,020
what ticked me a little bit is that

958
00:35:40,020 --> 00:35:43,720
the dis-authentication
step was only ever done

959
00:35:43,720 --> 00:35:46,793
when there was an RPC
endpoint that was prefixed...

960
00:35:46,793 --> 00:35:49,720
A non RPC communication that was prefixed

961
00:35:49,720 --> 00:35:51,868
with those 21 bytes of data.

962
00:35:51,868 --> 00:35:54,370
But it turns out that is another header

963
00:35:54,370 --> 00:35:57,210
that is sometimes sent before payloads

964
00:35:58,133 --> 00:35:59,850
that prefixes payloads.

965
00:35:59,850 --> 00:36:02,556
And this header is only 13 bytes long.

966
00:36:02,556 --> 00:36:05,940
And is it's usually sent inside the middle

967
00:36:05,940 --> 00:36:07,480
inside the TCP stream.

968
00:36:07,480 --> 00:36:09,300
So it's inside the same TCP session.

969
00:36:09,300 --> 00:36:11,180
So I did not have much hope.

970
00:36:11,180 --> 00:36:13,487
I was like, "Well, maybe
the class actually remembers

971
00:36:13,487 --> 00:36:15,097
"that it's an authenticated session.

972
00:36:15,097 --> 00:36:17,180
"So that's why, et cetera."

973
00:36:17,180 --> 00:36:18,607
But nevertheless, I thought,

974
00:36:18,607 --> 00:36:19,567
"Well, you know what,

975
00:36:19,567 --> 00:36:21,057
"let's see what it looks like.

976
00:36:21,057 --> 00:36:24,450
"Let's see if we served the
communication from here,

977
00:36:24,450 --> 00:36:27,150
so we don't send this
check existence header

978
00:36:27,150 --> 00:36:29,490
which prompts the authentication message.

979
00:36:29,490 --> 00:36:30,720
Let's just send,

980
00:36:30,720 --> 00:36:32,120
start communication from here,

981
00:36:32,120 --> 00:36:33,700
by sending these 13 bytes of data

982
00:36:33,700 --> 00:36:35,970
followed by the register application,

983
00:36:35,970 --> 00:36:36,963
because that's what we
really care about. Right?

984
00:36:36,963 --> 00:36:39,450
We want to submit an application

985
00:36:39,450 --> 00:36:41,020
that's what we really want.

986
00:36:41,020 --> 00:36:44,850
And so if you begin
communication at this stage,

987
00:36:44,850 --> 00:36:46,923
you first sent 13 bytes of data.

988
00:36:47,970 --> 00:36:51,240
It looks awfully similar
to the 21 bytes header

989
00:36:51,240 --> 00:36:52,090
but basically...

990
00:36:52,090 --> 00:36:52,923
I'm sorry.

991
00:36:52,923 --> 00:36:55,280
You have four bytes in all bytes

992
00:36:55,280 --> 00:36:57,750
followed by the size of
the payload plus the header

993
00:36:57,750 --> 00:36:59,470
so the size plus 13,

994
00:36:59,470 --> 00:37:01,970
magic bite nine which didn't
change for some reason

995
00:37:01,970 --> 00:37:03,430
and four bytes of data,

996
00:37:03,430 --> 00:37:04,737
which are the actual size of the payload.

997
00:37:04,737 --> 00:37:06,710
And then you send the serialized

998
00:37:06,710 --> 00:37:08,704
register application
with all its attributes,

999
00:37:08,704 --> 00:37:11,269
applications name, yada yada.

1000
00:37:11,269 --> 00:37:14,120
And I sent this and I was
expecting a massive header,

1001
00:37:14,120 --> 00:37:16,400
you know, those JVM hundred line headers.

1002
00:37:16,400 --> 00:37:19,680
But instead I got a Header,

1003
00:37:19,680 --> 00:37:21,518
followed by register application class.

1004
00:37:21,518 --> 00:37:23,417
And I was like, "Is it me?

1005
00:37:23,417 --> 00:37:25,940
"Or is it just possible
to bypass authentication?"

1006
00:37:25,940 --> 00:37:28,900
I was like, "Hallelujah, it's amazing."

1007
00:37:28,900 --> 00:37:31,990
Now bypassing authentication is fine,

1008
00:37:31,990 --> 00:37:33,953
but what can we do with it?

1009
00:37:34,840 --> 00:37:36,050
Cause I was not gonna recreate

1010
00:37:36,050 --> 00:37:39,590
the 80 next messages that
were following along.

1011
00:37:39,590 --> 00:37:41,465
It's not possible.

1012
00:37:41,465 --> 00:37:44,030
So what can we do with just one message?

1013
00:37:44,030 --> 00:37:47,560
Well, if you look back
at spark, how it works,

1014
00:37:47,560 --> 00:37:48,947
the register application class

1015
00:37:48,947 --> 00:37:52,335
is the one that prompts all that

1016
00:37:52,335 --> 00:37:54,240
workload on the cluster manager

1017
00:37:54,240 --> 00:37:56,527
that's gonna contact the workers and say,

1018
00:37:56,527 --> 00:37:58,757
"Hey, I've got an application for you.

1019
00:37:58,757 --> 00:38:00,743
"You better get ready,
spawn those executor's

1020
00:38:00,743 --> 00:38:03,640
"and get ready to take the work."

1021
00:38:03,640 --> 00:38:04,880
So I went ahead to the workers

1022
00:38:04,880 --> 00:38:06,078
and I replied my message

1023
00:38:06,078 --> 00:38:08,510
hoping to see some
process spawn or whatever,

1024
00:38:08,510 --> 00:38:10,620
but I didn't see anything.

1025
00:38:10,620 --> 00:38:12,200
And the reason for it I figured it out

1026
00:38:12,200 --> 00:38:13,610
a couple of days later,

1027
00:38:13,610 --> 00:38:16,410
was because when you send this payload

1028
00:38:16,410 --> 00:38:17,920
serialized register application,

1029
00:38:17,920 --> 00:38:19,280
when you send it,

1030
00:38:19,280 --> 00:38:20,963
the cluster manager receives it.

1031
00:38:22,580 --> 00:38:24,650
I immediately shut down the connection,

1032
00:38:24,650 --> 00:38:25,483
the program terminated.

1033
00:38:25,483 --> 00:38:27,950
So the cluster manager
did not have enough time

1034
00:38:27,950 --> 00:38:29,850
to actually contact the worker,

1035
00:38:29,850 --> 00:38:32,733
spawn the process and let it do its thing.

1036
00:38:35,070 --> 00:38:36,467
I immediately shut down the connection,

1037
00:38:36,467 --> 00:38:38,650
So the way around it
in order to give time,

1038
00:38:38,650 --> 00:38:40,290
you simply need to do a sleep.

1039
00:38:40,290 --> 00:38:41,520
As simple as that,

1040
00:38:41,520 --> 00:38:42,910
you send the register application,

1041
00:38:42,910 --> 00:38:44,470
you sleep for five seconds.

1042
00:38:44,470 --> 00:38:45,860
That way you give enough time

1043
00:38:45,860 --> 00:38:48,572
for the cluster manager
to contact the worker,

1044
00:38:48,572 --> 00:38:50,807
spawn the executer and do its work.

1045
00:38:50,807 --> 00:38:52,670
And once you do that,
once you do that sleep,

1046
00:38:52,670 --> 00:38:54,859
I saw like I put a watcher on the worker

1047
00:38:54,859 --> 00:38:56,267
monitoring for processes

1048
00:38:56,267 --> 00:39:00,176
and I saw this process get inserted.

1049
00:39:00,176 --> 00:39:02,490
And the magnificent
thing about it was that

1050
00:39:02,490 --> 00:39:04,940
it took as parameters

1051
00:39:04,940 --> 00:39:07,160
something that I was defining

1052
00:39:07,160 --> 00:39:09,920
inside that serialized
register application,

1053
00:39:09,920 --> 00:39:11,980
the block manager port set to 8443.

1054
00:39:11,980 --> 00:39:12,813
And I know it was me

1055
00:39:12,813 --> 00:39:14,320
because I'm the one that set it

1056
00:39:14,320 --> 00:39:15,860
in order to have communication

1057
00:39:15,860 --> 00:39:17,887
come from the worker to the driver.

1058
00:39:19,015 --> 00:39:19,848
So I thought,

1059
00:39:19,848 --> 00:39:22,531
"Okay, authentication
bypass is a sure thing."

1060
00:39:22,531 --> 00:39:25,092
Remote code execution is a definite maybe.

1061
00:39:25,092 --> 00:39:26,643
Because you can influence parameters

1062
00:39:26,643 --> 00:39:28,510
that are being used to spawn a process.

1063
00:39:28,510 --> 00:39:30,360
So maybe there's some injection there.

1064
00:39:30,360 --> 00:39:32,860
So I turned to this
register application class

1065
00:39:32,860 --> 00:39:35,157
and saw like, "Okay, what's inside?

1066
00:39:35,157 --> 00:39:37,626
"What can we define as
parameters as options

1067
00:39:37,626 --> 00:39:38,782
"that we can take advantage of?"

1068
00:39:38,782 --> 00:39:40,270
So register application again I guess,

1069
00:39:40,270 --> 00:39:42,100
case class, which extends
the deploy message,

1070
00:39:42,100 --> 00:39:44,686
which implements the
trade de-serializable,

1071
00:39:44,686 --> 00:39:45,800
don't care about that.

1072
00:39:45,800 --> 00:39:47,850
Application description, what does it do?

1073
00:39:47,850 --> 00:39:49,645
Okay. So if you're like me,

1074
00:39:49,645 --> 00:39:51,370
you're gonna ignore all this shit

1075
00:39:51,370 --> 00:39:54,038
and go straight to command, right?

1076
00:39:54,038 --> 00:39:55,233
Because the command class, okay.

1077
00:39:55,233 --> 00:39:57,850
That command case class,
it has the main class,

1078
00:39:57,850 --> 00:39:59,900
which is the class of the
executer to be executed.

1079
00:39:59,900 --> 00:40:01,700
We can not override that,

1080
00:40:01,700 --> 00:40:04,320
have arguments environments,
a bunch of parameters.

1081
00:40:04,320 --> 00:40:06,376
Now, if you just want to inject

1082
00:40:06,376 --> 00:40:09,400
semi-colon and, or a pipe,

1083
00:40:09,400 --> 00:40:13,115
it will not work because these
parameters are sanitized.

1084
00:40:13,115 --> 00:40:16,040
So we need a more clever way

1085
00:40:16,040 --> 00:40:18,730
of hijacking the execution
and the way to do it...

1086
00:40:18,730 --> 00:40:20,630
I found the easiest way to do it at least,

1087
00:40:20,630 --> 00:40:22,770
is using these Java options.

1088
00:40:22,770 --> 00:40:24,188
Now, if you're familiar with the JVM,

1089
00:40:24,188 --> 00:40:26,240
you know that you can actually specify

1090
00:40:26,240 --> 00:40:28,170
or control the behavior of that JVM.

1091
00:40:28,170 --> 00:40:30,380
Like, I don't know which
garbage collector to use,

1092
00:40:30,380 --> 00:40:33,355
how should strings be compressed?

1093
00:40:33,355 --> 00:40:36,040
The amount of heat memory, et cetera.

1094
00:40:36,040 --> 00:40:36,873
Well it turns out,

1095
00:40:36,873 --> 00:40:40,410
if you dig into the hundreds
and hundreds of Java options,

1096
00:40:40,410 --> 00:40:43,470
there is one actually
that allow code execution.

1097
00:40:43,470 --> 00:40:45,900
So yeah, this is an
example of a JVM option.

1098
00:40:45,900 --> 00:40:47,830
So there is one that
allows code execution,

1099
00:40:47,830 --> 00:40:50,250
and it's called an on out of memory error.

1100
00:40:50,250 --> 00:40:52,750
And you just specify command
that will execute it.

1101
00:40:52,750 --> 00:40:53,770
The catch is that,

1102
00:40:53,770 --> 00:40:57,600
it only executes it if
there's an out of memory error

1103
00:40:57,600 --> 00:40:58,643
like its name said.

1104
00:40:59,610 --> 00:41:02,770
So if the executer or a process

1105
00:41:02,770 --> 00:41:04,423
fails to allocate heat memory,

1106
00:41:05,470 --> 00:41:07,660
then it will throw this error
and then it will execute code.

1107
00:41:07,660 --> 00:41:10,077
So how to make the executer
trigger this error?

1108
00:41:10,077 --> 00:41:11,780
Well a simple way is

1109
00:41:11,780 --> 00:41:14,560
simply to add another JVM option Xmx,

1110
00:41:14,560 --> 00:41:16,600
which will set the maximum amount memory

1111
00:41:16,600 --> 00:41:19,410
to a particular size like one
megabyte or two megabytes.

1112
00:41:19,410 --> 00:41:22,340
And once you combine these two properties,

1113
00:41:22,340 --> 00:41:24,630
then you have your code execution.

1114
00:41:24,630 --> 00:41:26,670
And so ideally would like
to do something like this.

1115
00:41:26,670 --> 00:41:28,570
Obviously we cannot do it inside the code

1116
00:41:28,570 --> 00:41:31,490
because while the driver
will attempt to authenticate,

1117
00:41:31,490 --> 00:41:32,323
which is no good,

1118
00:41:32,323 --> 00:41:34,950
and anyway you cannot set
the Xmx inside the drivers.

1119
00:41:34,950 --> 00:41:38,230
So we really need to
forge by hand basically

1120
00:41:38,230 --> 00:41:40,940
that serialized object,
embed these options

1121
00:41:40,940 --> 00:41:42,590
and have a properly serialized object

1122
00:41:42,590 --> 00:41:46,120
and send it to the cluster manager.

1123
00:41:46,120 --> 00:41:49,763
And again, easier way
to do it using Sparky.

1124
00:41:51,960 --> 00:41:53,330
So I just point a...

1125
00:41:53,330 --> 00:41:55,050
so this is just quick example to show you

1126
00:41:55,050 --> 00:41:56,300
how to exploit this form.

1127
00:41:57,470 --> 00:42:00,545
So this is the reverse shell
that we want to execute, right?

1128
00:42:00,545 --> 00:42:01,950
So I just point to a Sparky,

1129
00:42:01,950 --> 00:42:03,070
I wanna execute this code,

1130
00:42:03,070 --> 00:42:04,596
but as you can see,

1131
00:42:04,596 --> 00:42:07,713
the cluster requires authentication.

1132
00:42:09,920 --> 00:42:11,250
But if I add -b,

1133
00:42:11,250 --> 00:42:13,767
then it will take advantage of this form

1134
00:42:13,767 --> 00:42:15,187
and you can see it bypass authentication

1135
00:42:15,187 --> 00:42:17,300
and just execute the code directly.

1136
00:42:17,300 --> 00:42:18,133
And it's gonna execute it

1137
00:42:18,133 --> 00:42:20,913
on as many workers as requested.

1138
00:42:22,040 --> 00:42:23,570
So yeah, we just bypassed authentication,

1139
00:42:23,570 --> 00:42:25,573
which has executed code on workers.

1140
00:42:27,127 --> 00:42:27,960
So great.

1141
00:42:28,920 --> 00:42:29,753
So yeah.

1142
00:42:29,753 --> 00:42:32,920
So now that we understood
the hacker intuitive way

1143
00:42:32,920 --> 00:42:34,270
of finding this form,

1144
00:42:34,270 --> 00:42:35,620
let's go through the code.

1145
00:42:35,620 --> 00:42:37,030
Because I think it's interesting.

1146
00:42:37,030 --> 00:42:38,260
if you see,

1147
00:42:38,260 --> 00:42:40,410
I made the distinction
between the 21 bytes

1148
00:42:42,110 --> 00:42:44,000
payload that was prefixed
with 21 bytes of data

1149
00:42:44,000 --> 00:42:45,130
or prefixed with 13 bytes of data,

1150
00:42:45,130 --> 00:42:46,550
it turns out

1151
00:42:46,550 --> 00:42:48,900
there's actually a
difference in the dispatcher.

1152
00:42:48,900 --> 00:42:50,370
So the register application

1153
00:42:50,370 --> 00:42:53,050
is handled by the process one way message.

1154
00:42:53,050 --> 00:42:54,450
And a process one way message called

1155
00:42:54,450 --> 00:42:56,610
the receive method of
the RPC handler, right?

1156
00:42:56,610 --> 00:42:58,650
So it delegates to the receive.

1157
00:42:58,650 --> 00:43:00,550
And you can see that at no point

1158
00:43:00,550 --> 00:43:02,300
there is any authentication going on.

1159
00:43:02,300 --> 00:43:05,040
Here the delegate receive will
actually perform the work.

1160
00:43:05,040 --> 00:43:08,090
If you compare that with
the other receive method,

1161
00:43:08,090 --> 00:43:10,330
which actually asks for authentication,

1162
00:43:10,330 --> 00:43:13,850
you can see that it's
quite beefy and different.

1163
00:43:13,850 --> 00:43:17,390
So the correction
basically the patch was to

1164
00:43:17,390 --> 00:43:18,980
protect that same receive method,

1165
00:43:18,980 --> 00:43:21,360
and another one used in streaming,

1166
00:43:21,360 --> 00:43:23,000
which was affected also

1167
00:43:23,000 --> 00:43:27,360
by this same time types of checks.

1168
00:43:27,360 --> 00:43:30,790
And this was labeled CVE-2020-9480.

1169
00:43:30,790 --> 00:43:33,087
I disclosed them on 24th of the same...

1170
00:43:33,950 --> 00:43:36,940
which was very horrible, but anyway...

1171
00:43:37,930 --> 00:43:39,610
And yeah, it was fixed
by the Apache spark team.

1172
00:43:39,610 --> 00:43:41,800
So thank you very much for the
work that you guys have done,

1173
00:43:41,800 --> 00:43:42,633
it's amazing.

1174
00:43:44,520 --> 00:43:47,100
So in summary, what I wanna say is

1175
00:43:47,100 --> 00:43:48,430
spark is awesome.

1176
00:43:48,430 --> 00:43:50,501
I'm gonna say too bad
security's not taken seriously.

1177
00:43:50,501 --> 00:43:51,440
And I'm not talking about

1178
00:43:51,440 --> 00:43:53,350
the folks who manage security there.

1179
00:43:53,350 --> 00:43:55,460
I'm just referring to the fact that

1180
00:43:55,460 --> 00:43:57,310
security is off by default.

1181
00:43:57,310 --> 00:43:58,720
I don't think that should be something

1182
00:43:58,720 --> 00:44:00,040
that we should have in 2020,

1183
00:44:00,040 --> 00:44:01,720
especially for something that's

1184
00:44:01,720 --> 00:44:04,293
a framework that does data computation.

1185
00:44:05,300 --> 00:44:06,620
We all know how data is valuable.

1186
00:44:06,620 --> 00:44:10,570
So I say it in reference to that.

1187
00:44:10,570 --> 00:44:11,800
So that's...

1188
00:44:11,800 --> 00:44:14,330
Anyway, I hope that will change.

1189
00:44:14,330 --> 00:44:16,270
And finally, yeah, we only
covered spark standalone

1190
00:44:16,270 --> 00:44:19,123
where the cluster manager is actually

1191
00:44:19,123 --> 00:44:21,020
the one doing the work.

1192
00:44:21,020 --> 00:44:22,240
But it can have other setups

1193
00:44:22,240 --> 00:44:24,230
where it's actual yarn
that's doing the work.

1194
00:44:24,230 --> 00:44:28,090
Or I think even the Kubernetes
API server in spark 3.

1195
00:44:28,090 --> 00:44:30,410
So yeah, there's a lot of ground to cover.

1196
00:44:30,410 --> 00:44:32,543
So if you want to dig into it, go ahead.

1197
00:44:33,682 --> 00:44:34,820
The code to sparky,

1198
00:44:34,820 --> 00:44:37,141
if you wanna check out these spark API's

1199
00:44:37,141 --> 00:44:38,960
the serialized object that's being forced

1200
00:44:38,960 --> 00:44:39,880
not to bypass the...

1201
00:44:39,880 --> 00:44:41,860
All that stuff is there.

1202
00:44:41,860 --> 00:44:45,330
I hope you got enough of this talk

1203
00:44:45,330 --> 00:44:46,360
to actually dig into it,

1204
00:44:46,360 --> 00:44:48,200
because that's really the only way to

1205
00:44:48,200 --> 00:44:49,180
figure out how stuff works.

1206
00:44:49,180 --> 00:44:52,450
So yeah, your contribution
are most welcome.

1207
00:44:52,450 --> 00:44:54,760
And if you wanna hit me up on
Twitter to talk about it more,

1208
00:44:54,760 --> 00:44:55,960
please do not hesitate.

1209
00:44:55,960 --> 00:44:57,400
And looking forward to talk to you guys

1210
00:44:57,400 --> 00:44:59,680
and talk to you folks in the QA.

1211
00:44:59,680 --> 00:45:00,560
Thank you very much.

1212
00:45:00,560 --> 00:45:01,393
Bye bye.

