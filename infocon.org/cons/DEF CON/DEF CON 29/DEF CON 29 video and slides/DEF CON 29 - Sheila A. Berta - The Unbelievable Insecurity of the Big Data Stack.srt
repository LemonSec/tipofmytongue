1
00:00:04,880 --> 00:00:09,580
- Hi everyone, welcome to my
talk on big data insecurity.

2
00:00:09,580 --> 00:00:11,970
I will talk about how we can analyze

3
00:00:11,970 --> 00:00:14,060
these big data infrastructures

4
00:00:14,060 --> 00:00:15,883
from an offensive point of view.

5
00:00:16,820 --> 00:00:19,600
Before starting, let me
introduce myself briefly.

6
00:00:19,600 --> 00:00:22,530
My name is Sheila, I
work as Head of Research

7
00:00:22,530 --> 00:00:25,957
at Dreamlab Technologies,
a Swiss Infosec Company.

8
00:00:25,957 --> 00:00:29,060
I'm an Offensive Security Specialist

9
00:00:29,060 --> 00:00:31,560
with several years of experience.

10
00:00:31,560 --> 00:00:33,210
And in the last time I had focus

11
00:00:33,210 --> 00:00:35,390
on security in Cloud Environment,

12
00:00:35,390 --> 00:00:38,413
Cloud Native, Big Data and related stuff.

13
00:00:39,570 --> 00:00:42,610
Okay, so let's go to the important things.

14
00:00:43,610 --> 00:00:46,080
There are some key concepts
I will like to explain

15
00:00:46,080 --> 00:00:49,300
before jumping into the security part.

16
00:00:49,300 --> 00:00:51,910
Probably the first
thing that comes to mind

17
00:00:51,910 --> 00:00:53,734
when talking about big data

18
00:00:53,734 --> 00:00:57,530
is the challenge of storing
large volume of information,

19
00:00:57,530 --> 00:01:00,620
and the technology that
will take care of it.

20
00:01:00,620 --> 00:01:03,823
Although that's correct,
around the storage technology,

21
00:01:03,823 --> 00:01:06,290
there are many others of great importance

22
00:01:06,290 --> 00:01:08,193
that make up the ecosystem.

23
00:01:09,470 --> 00:01:12,210
When we design big data architectures,

24
00:01:12,210 --> 00:01:15,640
we must think about how the
data will be transported

25
00:01:15,640 --> 00:01:17,624
from the source to the storage,

26
00:01:17,624 --> 00:01:21,900
if the data requires some kind
of processing to be consumed

27
00:01:21,900 --> 00:01:25,163
and how the information
will be accessed, right?

28
00:01:26,240 --> 00:01:29,867
So the different processes
that the data go through

29
00:01:29,867 --> 00:01:33,120
are divided into four main layers

30
00:01:33,120 --> 00:01:35,323
that comprise the big data stack.

31
00:01:36,160 --> 00:01:38,490
We have the Data Ingestion,

32
00:01:38,490 --> 00:01:40,900
that is the transport of the information

33
00:01:40,900 --> 00:01:44,340
from the different origins
to the storage place.

34
00:01:44,340 --> 00:01:46,260
The storage itself,

35
00:01:46,260 --> 00:01:48,000
the Data Processing layer,

36
00:01:48,000 --> 00:01:51,730
because the most common is
to ingest raw information

37
00:01:51,730 --> 00:01:54,043
that later needs some kind of processing.

38
00:01:55,240 --> 00:01:57,440
And finally, the Data Access layer,

39
00:01:57,440 --> 00:02:01,373
basically how users will access
and consume the information.

40
00:02:02,450 --> 00:02:04,910
And let's add one more layer here

41
00:02:04,910 --> 00:02:07,460
that is not part of the big data stack,

42
00:02:07,460 --> 00:02:11,160
but we have this layer in all
the big data infrastructures.

43
00:02:11,160 --> 00:02:13,483
The Cluster Management
is really important.

44
00:02:15,390 --> 00:02:16,900
So for each of these layers,

45
00:02:16,900 --> 00:02:18,950
there is a wide variety of technologies

46
00:02:18,950 --> 00:02:22,150
that can be implemented
because the big data stack

47
00:02:22,150 --> 00:02:23,463
is hugely big.

48
00:02:24,300 --> 00:02:28,250
These ones are just a few
of the most popular ones.

49
00:02:28,250 --> 00:02:31,000
For example, Hadoop for the storage.

50
00:02:31,000 --> 00:02:34,090
Spark and Storm for processing.

51
00:02:34,090 --> 00:02:38,740
Impala, Presto, Drill for
accessing information.

52
00:02:38,740 --> 00:02:41,318
Flume, Sqoop for data ingestion.

53
00:02:41,318 --> 00:02:43,643
Zookeeper for management, for example.

54
00:02:44,870 --> 00:02:48,415
So when we analyze an entire
big data infrastructure,

55
00:02:48,415 --> 00:02:52,384
we can actually find many
different and complex technologies

56
00:02:52,384 --> 00:02:54,890
interacting with each other,

57
00:02:54,890 --> 00:02:57,060
that they meet different functions,

58
00:02:57,060 --> 00:02:59,010
according to the layer of the stack,

59
00:02:59,010 --> 00:03:00,723
where they are located, right?

60
00:03:02,800 --> 00:03:06,133
So let's see an example of a
real big data architecture.

61
00:03:07,186 --> 00:03:10,720
Here we have two different
clouds, one in AWS,

62
00:03:10,720 --> 00:03:13,526
and another one in any
other cloud provider,

63
00:03:13,526 --> 00:03:17,000
both are running in
some Kubernetes clusters

64
00:03:17,000 --> 00:03:19,670
that are serving different applications.

65
00:03:19,670 --> 00:03:21,850
And we want to store and analyze

66
00:03:21,850 --> 00:03:24,140
the logs of these applications.

67
00:03:24,140 --> 00:03:27,770
So we will use Fluent Bit to collect

68
00:03:27,770 --> 00:03:29,860
all the application logs.

69
00:03:29,860 --> 00:03:33,790
Bring them to Kafka for the first cloud

70
00:03:33,790 --> 00:03:37,270
and stream them using Flume and Kinesis

71
00:03:37,270 --> 00:03:39,613
to run on-prem, Hadoop cluster.

72
00:03:41,240 --> 00:03:43,840
So within the Hadoop cluster,

73
00:03:43,840 --> 00:03:46,770
the first component that
will receive the data

74
00:03:46,770 --> 00:03:48,763
is Spark Structure Streaming.

75
00:03:49,670 --> 00:03:53,170
This one will take care of
ingesting and also processing

76
00:03:53,170 --> 00:03:55,470
the information before dumping it

77
00:03:55,470 --> 00:03:57,330
into the Hadoop filesystem.

78
00:03:58,870 --> 00:04:01,940
So once we have our information here,

79
00:04:01,940 --> 00:04:03,650
we want to access it.

80
00:04:03,650 --> 00:04:06,120
So for that we could
implement, for example,

81
00:04:06,120 --> 00:04:09,630
Hive and Presto, or instead of Presto

82
00:04:09,630 --> 00:04:11,590
we could use Impala, Grid

83
00:04:11,590 --> 00:04:14,190
or any other technology
for interactive queries

84
00:04:14,190 --> 00:04:15,533
against Hadoop, right?

85
00:04:16,440 --> 00:04:19,150
And if we are developing our own software

86
00:04:19,150 --> 00:04:20,890
to visualize the information,

87
00:04:20,890 --> 00:04:23,140
we will probably have an API talking

88
00:04:23,140 --> 00:04:26,693
to the Presto coordinator
and a nice front end.

89
00:04:28,400 --> 00:04:29,233
And finally,

90
00:04:29,233 --> 00:04:31,953
we have the management layer
and here it's super common

91
00:04:31,953 --> 00:04:34,370
to find Apache's Zookeeper,

92
00:04:34,370 --> 00:04:37,733
to centralize the configuration
of all these components.

93
00:04:39,000 --> 00:04:41,860
And also, an administration
tool, like Ambari

94
00:04:41,860 --> 00:04:45,293
or a centralized lock system
for cluster monitoring.

95
00:04:46,700 --> 00:04:49,720
So this is an example of a
real big data architecture

96
00:04:49,720 --> 00:04:53,290
and how the components
interact with each other.

97
00:04:53,290 --> 00:04:54,329
So back to security,

98
00:04:54,329 --> 00:04:57,170
the question is how we can analyze

99
00:04:57,170 --> 00:04:59,053
these complex infrastructures.

100
00:05:00,185 --> 00:05:03,339
I would like to propose
some methodology for this,

101
00:05:03,339 --> 00:05:06,966
where the analysis of, is based of,

102
00:05:06,966 --> 00:05:09,333
of the different layers
of the big data stack.

103
00:05:10,280 --> 00:05:11,610
Because I think that the good way

104
00:05:11,610 --> 00:05:15,790
to analyze big data
infrastructures is to dissect them,

105
00:05:15,790 --> 00:05:19,433
analyze the security of the
components layer by layer.

106
00:05:20,540 --> 00:05:23,660
In this way we can make sure
that we are covering all

107
00:05:23,660 --> 00:05:26,470
the stages that the
information we want to protect

108
00:05:26,470 --> 00:05:27,773
to go through, right?

109
00:05:29,000 --> 00:05:33,680
So from now on, I will explain
different attack vectors

110
00:05:33,680 --> 00:05:35,900
that I've found throughout this research

111
00:05:35,900 --> 00:05:37,560
for each of the layers.

112
00:05:39,460 --> 00:05:42,053
Okay, so let's start with
the management layer.

113
00:05:44,443 --> 00:05:45,990
Zookeeper, as I said,

114
00:05:45,990 --> 00:05:50,990
is a widely use tool to
centralize the configuration

115
00:05:51,310 --> 00:05:54,470
of the different technologies
that make up the cluster.

116
00:05:55,529 --> 00:05:57,843
Its architecture is pretty simple.

117
00:05:57,843 --> 00:06:02,020
It runs a service on all
nodes and then a client,

118
00:06:02,020 --> 00:06:05,490
let's say a cluster
administrator, can connect to

119
00:06:05,490 --> 00:06:08,430
one of the nodes and
update the configuration.

120
00:06:08,430 --> 00:06:11,600
So when that happens Zookeeper
will automatically broadcast

121
00:06:11,600 --> 00:06:13,533
the change across all the nodes.

122
00:06:15,192 --> 00:06:18,450
So, with this kind of node of the cluster,

123
00:06:18,450 --> 00:06:23,450
we will find the ports 2181 and 7888 open.

124
00:06:23,620 --> 00:06:26,890
Because these ports belongs to Zookeeper,

125
00:06:26,890 --> 00:06:29,220
are opened by Zookeeper, basically.

126
00:06:29,220 --> 00:06:30,310
So the ports 2181 is,

127
00:06:30,310 --> 00:06:34,423
is the port that accepts
connection from client.

128
00:06:35,771 --> 00:06:38,080
Should we be able to connect to it?

129
00:06:38,080 --> 00:06:40,380
Well, according to the
official documentation

130
00:06:40,380 --> 00:06:42,760
of Ambari, a tool that is widely used

131
00:06:42,760 --> 00:06:45,990
for deploying on-prem big data clusters,

132
00:06:45,990 --> 00:06:48,530
Disable the firewall.
These are requirements

133
00:06:48,530 --> 00:06:50,990
for installing big data clusters.

134
00:06:50,990 --> 00:06:53,283
So we can probably connect to Zookeeper.

135
00:06:54,700 --> 00:06:56,760
How should we do it?

136
00:06:56,760 --> 00:06:58,300
We can download the Zookeeper client

137
00:06:58,300 --> 00:07:00,490
from the official website.

138
00:07:00,490 --> 00:07:02,580
Then it's just about running this command,

139
00:07:02,580 --> 00:07:06,733
specifying the node IP
address and the 2181 port.

140
00:07:07,680 --> 00:07:09,530
So once we connect,

141
00:07:09,530 --> 00:07:13,540
if we run the help command
there is a list of actions

142
00:07:13,540 --> 00:07:15,460
we can execute on relevant znodes.

143
00:07:15,460 --> 00:07:20,050
The znodes, or Zookeeper
nodes are the configurations

144
00:07:20,050 --> 00:07:23,653
that Zookeeper organizes in
a hierarchical structure.

145
00:07:26,150 --> 00:07:28,700
So with the ls and get commands,

146
00:07:28,700 --> 00:07:30,923
we can browse this hierarchical structure.

147
00:07:32,030 --> 00:07:34,780
We can find very interesting
information about

148
00:07:34,780 --> 00:07:37,370
the configuration of all the components

149
00:07:37,370 --> 00:07:39,230
that make up the cluster.

150
00:07:39,230 --> 00:07:43,890
Like Hadoop, Hive, HBase,
Kafka, whatever, right?

151
00:07:43,890 --> 00:07:46,333
And of course we could
use it for fire attacks.

152
00:07:48,080 --> 00:07:52,090
We can also create new
configurations, modify existing ones,

153
00:07:52,090 --> 00:07:54,250
delete configurations.

154
00:07:54,250 --> 00:07:56,820
These actually will be a
problem for the cluster.

155
00:07:56,820 --> 00:08:00,620
Some components might go down because,

156
00:08:00,620 --> 00:08:02,440
Zookeeper, for example, is commonly used

157
00:08:02,440 --> 00:08:05,160
to manage the Hadoop high availability.

158
00:08:05,160 --> 00:08:06,370
So if we delete everything,

159
00:08:06,370 --> 00:08:08,813
the cluster might run into troubles.

160
00:08:10,230 --> 00:08:12,500
So I wont run demos of these

161
00:08:12,500 --> 00:08:14,900
because it's a pretty simple attack.

162
00:08:14,900 --> 00:08:17,113
But is actually quite impactful.

163
00:08:18,870 --> 00:08:22,560
So what about Ambari,
this are pretty popular,

164
00:08:22,560 --> 00:08:26,390
open source tool to install
and manage big data clusters.

165
00:08:26,390 --> 00:08:30,340
And it has a dashboard from
which you can control everything

166
00:08:30,340 --> 00:08:33,563
whose default credential
are admin/admin, of course.

167
00:08:34,440 --> 00:08:38,020
But if they were changed,
there is a second door,

168
00:08:38,020 --> 00:08:40,480
absolutely really to check.

169
00:08:40,480 --> 00:08:44,260
Ambari uses a Postgres database
to store the statistics

170
00:08:44,260 --> 00:08:45,960
and information about the cluster.

171
00:08:46,820 --> 00:08:49,500
And in the default installation process,

172
00:08:49,500 --> 00:08:52,670
the Ambari wizard asks you
to change the credentials

173
00:08:52,670 --> 00:08:55,730
for this network but it
doesn't ask you to change

174
00:08:55,730 --> 00:08:57,943
the default credential for the database.

175
00:08:58,830 --> 00:09:02,790
So we could simply connect to
the Postgres ports directly

176
00:09:02,790 --> 00:09:05,310
using these default credentials.

177
00:09:05,310 --> 00:09:08,140
They are user: ambari, password: bigdata

178
00:09:09,490 --> 00:09:13,180
and explore this Ambari database.

179
00:09:13,180 --> 00:09:15,930
We will find here two tables.

180
00:09:15,930 --> 00:09:18,813
The user.authentication and users one.

181
00:09:20,490 --> 00:09:22,540
So if we want to get the username

182
00:09:22,540 --> 00:09:24,780
and authentication key at once,

183
00:09:24,780 --> 00:09:27,173
we need to do this inner join query

184
00:09:27,173 --> 00:09:29,003
between those two tables.

185
00:09:30,840 --> 00:09:33,200
The authentication key is salted hash.

186
00:09:33,200 --> 00:09:35,830
So the best thing that
we can do here is just

187
00:09:35,830 --> 00:09:39,193
update the key for the
admin user, for example.

188
00:09:40,340 --> 00:09:42,880
I log in to the Ambari source code

189
00:09:42,880 --> 00:09:44,533
to find Ambari salted hash.

190
00:09:45,570 --> 00:09:48,213
Here we have the hash
for the admin password.

191
00:09:49,210 --> 00:09:51,790
So now we can run an update query.

192
00:09:51,790 --> 00:09:54,860
And once done we can log
into the Ambari dashboard

193
00:09:54,860 --> 00:09:56,913
with the admin/admin credentials.

194
00:09:58,480 --> 00:10:00,870
Well, I know that if this
happen it's pretty stupid

195
00:10:00,870 --> 00:10:02,800
but it's actually worth to check

196
00:10:02,800 --> 00:10:05,600
because Ambari controls the whole cluster.

197
00:10:05,600 --> 00:10:07,670
If you can access this password,

198
00:10:07,670 --> 00:10:10,880
you can do whatever you
want over over the cluster.

199
00:10:10,880 --> 00:10:12,730
And as the default installation process

200
00:10:12,730 --> 00:10:16,130
doesn't ask for these
credentials, to change them,

201
00:10:16,130 --> 00:10:19,013
you can most likely
compromise them in this way.

202
00:10:21,440 --> 00:10:24,060
Good. So the important thing
in the cluster management layer

203
00:10:24,060 --> 00:10:27,380
is to analyze the security
of the administration

204
00:10:27,380 --> 00:10:29,840
and monitoring tools, right?

205
00:10:29,840 --> 00:10:32,173
So let's now talk about the storage layer.

206
00:10:35,470 --> 00:10:37,148
First, first of all, it is,

207
00:10:37,148 --> 00:10:41,640
it's good to understand how Hadoop works.

208
00:10:41,640 --> 00:10:45,980
It has a masters layer architecture
and two main components,

209
00:10:45,980 --> 00:10:50,540
the HDF, that means Hadoop
distributed filesystem

210
00:10:50,540 --> 00:10:51,373
and Yarn.

211
00:10:52,360 --> 00:10:55,610
So the HDF has two main components.

212
00:10:55,610 --> 00:10:58,688
The namenode that saves the metadata

213
00:10:58,688 --> 00:11:03,688
of the files stored in the
cluster, runs in the master node.

214
00:11:04,551 --> 00:11:08,480
And the datanode that
stores the actual data

215
00:11:08,480 --> 00:11:10,743
and runs in this slave nodes, right.

216
00:11:11,870 --> 00:11:13,400
And, on the other hand,

217
00:11:13,400 --> 00:11:16,740
Yarn consists of two components, as well.

218
00:11:16,740 --> 00:11:20,160
The resource manager
located on the master nodes.

219
00:11:20,160 --> 00:11:22,470
It controls all the processing resources

220
00:11:22,470 --> 00:11:24,620
in the Hadoop cluster.

221
00:11:24,620 --> 00:11:28,270
And the node manager
installed in the slaves nodes

222
00:11:28,270 --> 00:11:31,530
that takes care of tracking
processing resources

223
00:11:31,530 --> 00:11:34,643
on the slave node, among other tasks.

224
00:11:35,530 --> 00:11:39,249
But basically, what we have
to know is that the HDF,

225
00:11:39,249 --> 00:11:41,610
that is the Hadoop filesystem,

226
00:11:41,610 --> 00:11:44,420
is where the cluster
information is stored.

227
00:11:44,420 --> 00:11:47,550
And then Yarn is a service
that manage the resources

228
00:11:47,550 --> 00:11:49,000
for the processing jobs

229
00:11:49,000 --> 00:11:52,270
that are executed over
the information stored.

230
00:11:52,270 --> 00:11:54,580
Basically it's stored.

231
00:11:54,580 --> 00:11:56,360
So when it comes to the storage layer,

232
00:11:56,360 --> 00:11:59,210
we are interested in the
Hadoop filesystem, right?

233
00:11:59,210 --> 00:12:02,903
So let's hear how we could
remotely compromise it.

234
00:12:04,430 --> 00:12:08,370
Hadoop disposes an IPC port of 8020,

235
00:12:08,370 --> 00:12:12,630
that we should find
opened in Hadoop clusters.

236
00:12:12,630 --> 00:12:15,280
So if we can connect to it,

237
00:12:15,280 --> 00:12:19,913
we could execute Hadoop commands
and access the stored data.

238
00:12:20,800 --> 00:12:24,930
However, this is not as simple
as the Zookeeper example was.

239
00:12:24,930 --> 00:12:28,373
So managing to do this is a
little more complex, right?

240
00:12:29,310 --> 00:12:32,930
There are four configuration
files that Hadoop needs

241
00:12:32,930 --> 00:12:36,003
to perform operations over
the Hadoop filesystem.

242
00:12:37,310 --> 00:12:40,620
And if we take a look at
these files inside a namenode,

243
00:12:40,620 --> 00:12:42,510
we can say that they have dozens

244
00:12:42,510 --> 00:12:44,910
of configuration parameters.

245
00:12:44,910 --> 00:12:48,600
So when I saw that, I
wonder, if I'm an attacker

246
00:12:48,600 --> 00:12:51,640
and I don't have access to these files,

247
00:12:51,640 --> 00:12:54,563
how can I compromise the
filesystem in a remote way?

248
00:12:55,620 --> 00:12:57,980
So a part of this research was to find

249
00:12:57,980 --> 00:13:00,300
among those dozens of parameters

250
00:13:00,300 --> 00:13:03,204
which ones are, a
hundred percent requires,

251
00:13:03,204 --> 00:13:06,363
how we can get them remotely
from the information

252
00:13:06,363 --> 00:13:09,083
that Hadoop itself discloses by default.

253
00:13:09,960 --> 00:13:11,760
So I will explain now how we can

254
00:13:11,760 --> 00:13:14,303
manually craft these files, one-by-one.

255
00:13:15,951 --> 00:13:19,150
Let's start by the core-site XML file.

256
00:13:19,150 --> 00:13:22,340
The only information we
need to have for this file

257
00:13:22,340 --> 00:13:23,437
is the namespace.

258
00:13:24,887 --> 00:13:28,590
This is pretty easy to find,
Hadoop disposes by default

259
00:13:28,590 --> 00:13:33,480
a dashboard on the
namenodes, on port 50070.

260
00:13:33,480 --> 00:13:34,813
It's a pretty high port.

261
00:13:36,090 --> 00:13:38,850
We can actually see it
without authentication.

262
00:13:38,850 --> 00:13:42,850
So as you can see here, we
can find the namespace name.

263
00:13:42,850 --> 00:13:45,370
So I will hide my target cluster.

264
00:13:45,370 --> 00:13:47,413
And that's all we need for this file.

265
00:13:49,580 --> 00:13:53,550
Then we need to craft the hdfs-site file.

266
00:13:53,550 --> 00:13:56,300
It's necessary to know the
namespace that we already have

267
00:13:56,300 --> 00:13:57,947
from the previous file.

268
00:13:57,947 --> 00:14:02,947
And we also need the namenodes
IDs, and the DNS of them.

269
00:14:03,210 --> 00:14:06,610
So we could have one,
two or more namenodes.

270
00:14:06,610 --> 00:14:09,860
We need to provide the ID
and the DNS for all of them

271
00:14:09,860 --> 00:14:10,713
in these files.

272
00:14:11,760 --> 00:14:13,560
Where can we have these information?

273
00:14:14,601 --> 00:14:16,520
From the same dashboard?

274
00:14:16,520 --> 00:14:19,540
We have the namespace
here, the namenode ID,

275
00:14:19,540 --> 00:14:21,530
and the DNS, right?

276
00:14:21,530 --> 00:14:24,950
So we just need to access this
dashboard on each namenode.

277
00:14:24,950 --> 00:14:28,143
Remember that this is on port 50070.

278
00:14:29,381 --> 00:14:32,834
Another alternative is to
enter the datanode dashboard.

279
00:14:32,834 --> 00:14:33,751
Port 50075.

280
00:14:36,537 --> 00:14:39,963
And then we can see all
the namenodes at once.

281
00:14:43,030 --> 00:14:45,263
So the next file is the mapper-site one.

282
00:14:46,120 --> 00:14:48,590
Here we need the DNS of the namenode

283
00:14:48,590 --> 00:14:51,003
that hosted mapreduce-jobhistory.

284
00:14:53,263 --> 00:14:55,710
We can try to access the ports 19088

285
00:14:59,039 --> 00:15:00,372
on the namenode.

286
00:15:01,440 --> 00:15:03,780
If we can see this dashboard

287
00:15:03,780 --> 00:15:06,230
then that's the namenode
that we are looking for.

288
00:15:07,800 --> 00:15:11,373
We already know its DNS
from the previous dashboard.

289
00:15:12,670 --> 00:15:13,503
Right. So.

290
00:15:15,460 --> 00:15:19,353
Finally, we need to
craft the yarn-site file.

291
00:15:20,900 --> 00:15:23,630
Again, we need the namenode and DNS.

292
00:15:23,630 --> 00:15:27,350
In this case, the one that
hosts the yarn.resourcemanager.

293
00:15:27,350 --> 00:15:31,880
So we can try to access the board 8088.

294
00:15:31,880 --> 00:15:36,131
And as we see this dashboard
then that's the right node.

295
00:15:36,131 --> 00:15:38,363
And here we can get its DNS, of course.

296
00:15:40,060 --> 00:15:42,800
So all these dashboard
are exposed by default

297
00:15:42,800 --> 00:15:45,450
and don't require any authentication.

298
00:15:45,450 --> 00:15:47,920
But if for some reason we cannot see them,

299
00:15:47,920 --> 00:15:50,330
we can try to get these
required information

300
00:15:50,330 --> 00:15:54,950
through Zookeeper with the
attack I showed you earlier.

301
00:15:54,950 --> 00:15:59,763
Because Zookeeper also has
all these information, right?

302
00:16:02,170 --> 00:16:05,020
Cool. So once we have the
configuration files we need,

303
00:16:05,020 --> 00:16:08,080
the next step is to start
Hadoop in our local machine

304
00:16:08,080 --> 00:16:10,010
and provide it with those files

305
00:16:10,010 --> 00:16:12,443
to perform the remote communication.

306
00:16:13,650 --> 00:16:16,280
As I didn't want to start
Hadoop on my local machine,

307
00:16:16,280 --> 00:16:19,323
I build these Docker
file, feel free to use it.

308
00:16:19,323 --> 00:16:20,833
It's pretty comfortable.

309
00:16:21,720 --> 00:16:24,270
You should need to
change the Hadoop version

310
00:16:24,270 --> 00:16:27,313
to match the version of
your target cluster, right?

311
00:16:28,300 --> 00:16:29,570
So from now on,

312
00:16:29,570 --> 00:16:32,280
this is going to be our
Hadoop hacking container

313
00:16:32,280 --> 00:16:34,313
running on the attacker machine, right?

314
00:16:36,410 --> 00:16:38,873
Good. So let's run and
get the shell inside it.

315
00:16:40,120 --> 00:16:44,200
We can create the config
directory to place the XML files

316
00:16:44,200 --> 00:16:45,603
we have crafted before.

317
00:16:46,670 --> 00:16:50,280
And you also need to copy
this log property file,

318
00:16:50,280 --> 00:16:51,230
inside this folder.

319
00:16:52,370 --> 00:16:56,270
And another thing I did
was to delete the host file

320
00:16:56,270 --> 00:17:00,490
to write the result of this namenode DNS.

321
00:17:00,490 --> 00:17:04,400
You can actually use the IP
addresses on the XML files,

322
00:17:04,400 --> 00:17:08,133
but for some reason I had
better results in doing this.

323
00:17:10,220 --> 00:17:12,350
Okay, so we are ready to go.

324
00:17:12,350 --> 00:17:15,010
Just pass to Hadoop this config directory,

325
00:17:15,010 --> 00:17:19,020
and you can execute for
example, an ls command.

326
00:17:19,020 --> 00:17:22,280
So voila, we can see the
entire Hadoop filesystem

327
00:17:22,280 --> 00:17:24,683
from our remote attacker machine.

328
00:17:27,050 --> 00:17:28,990
But before jumping into a demo of this,

329
00:17:28,990 --> 00:17:31,100
I would like to mention that most likely

330
00:17:31,100 --> 00:17:34,840
we will need to impersonate HDF users.

331
00:17:34,840 --> 00:17:37,970
For example, if I tried
to create a new directory

332
00:17:37,970 --> 00:17:40,363
using the root user, I cannot.

333
00:17:41,260 --> 00:17:44,550
So we need to impersonate
a user that has privileges

334
00:17:44,550 --> 00:17:46,810
within the Hadoop filesystem.

335
00:17:46,810 --> 00:17:48,953
That means one of these ones.

336
00:17:50,230 --> 00:17:52,540
Fortunately, that's very easy to do.

337
00:17:52,540 --> 00:17:55,310
We just need to set these
environment variable

338
00:17:55,310 --> 00:18:00,170
with the Hadoop username before
the command. And that's all.

339
00:18:00,170 --> 00:18:03,340
That will allow us to
create directories and

340
00:18:03,340 --> 00:18:07,270
also will allow us to delete
directories and files.

341
00:18:07,270 --> 00:18:10,483
So we could wipe out the
entire cluster information.

342
00:18:11,580 --> 00:18:14,393
Okay. So let's see a demo of this.

343
00:18:15,570 --> 00:18:20,240
Here, I have my files, the
core-site with the namespace.

344
00:18:20,240 --> 00:18:23,370
I also have the HDF file.

345
00:18:23,370 --> 00:18:27,090
This is, has more
information. The namespace.

346
00:18:27,090 --> 00:18:29,000
And also the namenodes.

347
00:18:29,000 --> 00:18:31,380
For the namenodes, I need to specify

348
00:18:31,380 --> 00:18:32,800
the DNS.

349
00:18:32,800 --> 00:18:34,810
I have two namenodes in this case,

350
00:18:34,810 --> 00:18:37,163
so I need to specify the
DNS for both of them.

351
00:18:37,163 --> 00:18:39,820
And in the last property
was something I had to

352
00:18:39,820 --> 00:18:41,593
add for this specific cluster.

353
00:18:43,720 --> 00:18:45,807
The mapred-site has the DNS for the

354
00:18:45,807 --> 00:18:49,130
mapreduce.jobhistory address,

355
00:18:49,130 --> 00:18:51,453
the namenode that has this resource.

356
00:18:52,430 --> 00:18:55,020
For the yarn-site, I had
to specify the DNS as well

357
00:18:55,020 --> 00:18:57,750
for the resourcemanager node.

358
00:18:57,750 --> 00:18:59,200
So once we have those files,

359
00:18:59,200 --> 00:19:02,510
we are just ready to go an we
can execute Hadoop commands

360
00:19:02,510 --> 00:19:04,970
over the remote filesystem.

361
00:19:06,105 --> 00:19:10,913
If we check the help for
Hadoop for the fs commands,

362
00:19:12,180 --> 00:19:17,180
we can find a super common
command for any UNIX system

363
00:19:17,180 --> 00:19:21,193
to move, copy, delete,
move files, whatever.

364
00:19:22,336 --> 00:19:25,110
To impersonate we need to
specify this environment variable

365
00:19:25,110 --> 00:19:29,630
as we saw before. And here
we can create directories

366
00:19:30,700 --> 00:19:35,700
or we can modify files or delete
also any directory, right?

367
00:19:39,040 --> 00:19:39,873
Good.

368
00:19:49,290 --> 00:19:54,290
Good. So let's now talk
about the processing layer,

369
00:19:55,160 --> 00:19:57,987
and how we can abuse Yarn in this case.

370
00:19:59,840 --> 00:20:03,320
So back to the Hadoop
architecture, just to remember.

371
00:20:03,320 --> 00:20:06,113
Yarn task schedules
processing jobs over the data.

372
00:20:07,990 --> 00:20:11,422
So these jobs execute
code in the datanodes.

373
00:20:11,422 --> 00:20:14,680
So our mission here is
to try to find a way

374
00:20:14,680 --> 00:20:17,807
to remotely submitting
an application to Yarn

375
00:20:17,807 --> 00:20:19,986
that executes our code or a command

376
00:20:19,986 --> 00:20:23,267
that we want to execute
in the clusters node.

377
00:20:23,267 --> 00:20:27,660
Basically I achieve our remote
code execution through Yarn.

378
00:20:27,660 --> 00:20:30,020
We can use the Hadoop IPC that we are,

379
00:20:30,020 --> 00:20:32,860
we were using in the previous attack.

380
00:20:32,860 --> 00:20:35,180
It's just necessary to
improve a little bit

381
00:20:35,180 --> 00:20:36,763
our yarn-site file.

382
00:20:38,000 --> 00:20:42,173
We need to add the
yarn.application.classpath property.

383
00:20:43,560 --> 00:20:46,270
This path used to be the default path

384
00:20:46,270 --> 00:20:48,390
in Hadoop installation.

385
00:20:48,390 --> 00:20:52,430
So it should not be difficult
to obtain this information.

386
00:20:52,430 --> 00:20:54,904
In the example, here, we
can see the default path

387
00:20:54,904 --> 00:20:58,573
for installation using
the Hortonworks packages.

388
00:21:00,340 --> 00:21:03,140
Then these other properties optional.

389
00:21:03,140 --> 00:21:06,222
It will specify the
application output path

390
00:21:06,222 --> 00:21:07,960
in the Hadoop filesystem.

391
00:21:07,960 --> 00:21:10,790
It might useful for us
to easily find the output

392
00:21:10,790 --> 00:21:13,653
of our remote code execution,
but it's not necessary.

393
00:21:15,320 --> 00:21:16,720
And something I would like to mention

394
00:21:16,720 --> 00:21:18,770
that I didn't say before.

395
00:21:18,770 --> 00:21:21,450
If you can access these
panels that we have seen

396
00:21:22,440 --> 00:21:24,320
under the /conf,

397
00:21:24,320 --> 00:21:27,200
we can find all the
configuration parameters.

398
00:21:27,200 --> 00:21:30,110
But you cannot just
download and use that file.

399
00:21:30,110 --> 00:21:32,540
We still need to manually craft the files

400
00:21:32,540 --> 00:21:34,780
the way we were doing it.

401
00:21:34,780 --> 00:21:38,420
However, if something
is not working for you,

402
00:21:38,420 --> 00:21:40,303
here you might find what's missing.

403
00:21:41,510 --> 00:21:43,817
For example, here we have the,

404
00:21:43,817 --> 00:21:46,770
the path that we are looking for for the,

405
00:21:46,770 --> 00:21:49,423
the property we have to set in this case.

406
00:21:51,200 --> 00:21:52,900
Good. So okay.

407
00:21:52,900 --> 00:21:55,890
So now we have improve our Yarn file

408
00:21:55,890 --> 00:21:58,030
and we can submit the
application through Yarn.

409
00:21:58,030 --> 00:22:01,203
The question is, what
applications should we submit?

410
00:22:02,240 --> 00:22:05,100
Here our Hortonworks
provides a simple one.

411
00:22:05,100 --> 00:22:07,297
That is enough for us to achieve

412
00:22:07,297 --> 00:22:10,450
the remote code execution that we want.

413
00:22:10,450 --> 00:22:13,003
It had only three Java files.

414
00:22:14,150 --> 00:22:17,970
Because Yarn applications
are developed in Java,

415
00:22:17,970 --> 00:22:20,830
but there are a lot of
Hadoop libraries necessary

416
00:22:20,830 --> 00:22:22,420
to include and use.

417
00:22:22,420 --> 00:22:25,210
So it might not be so easy to develop

418
00:22:25,210 --> 00:22:27,260
a native Yarn application,

419
00:22:27,260 --> 00:22:30,580
but we can use this one for our purpose.

420
00:22:30,580 --> 00:22:33,360
It takes as parameter,
the command to be executed

421
00:22:33,360 --> 00:22:34,660
on the cluster nodes.

422
00:22:34,660 --> 00:22:36,790
And the number of instances,

423
00:22:36,790 --> 00:22:40,450
which is basically, on
how many nodes our command

424
00:22:40,450 --> 00:22:41,993
will be executed, right?

425
00:22:44,070 --> 00:22:46,070
So we will clone this repository

426
00:22:46,070 --> 00:22:47,940
in our Hadoop hacking container

427
00:22:47,940 --> 00:22:51,170
and proceed to compile
this shell application.

428
00:22:51,170 --> 00:22:54,850
We need to edit the pom.xml file

429
00:22:54,850 --> 00:22:56,850
and change the Hadoop version

430
00:22:56,850 --> 00:22:58,870
to match the version of our target.

431
00:22:58,870 --> 00:23:00,070
This is really important.

432
00:23:00,070 --> 00:23:03,170
Otherwise, this is not going to work.

433
00:23:03,170 --> 00:23:04,950
So once we do that,

434
00:23:04,950 --> 00:23:07,513
we can compile the application using mvn.

435
00:23:09,870 --> 00:23:10,720
Good.

436
00:23:10,720 --> 00:23:14,390
So the next step is to
copy the compiled Yarn

437
00:23:14,390 --> 00:23:16,330
into the remote Hadoop filesystem.

438
00:23:16,330 --> 00:23:20,343
We can do it using the
copyFromLocal HDF command.

439
00:23:21,340 --> 00:23:24,000
And after that, we are ready to go.

440
00:23:24,000 --> 00:23:27,600
In this way, we can submit
the application through Yarn.

441
00:23:27,600 --> 00:23:30,850
Passing as parameter the
command that we want to execute

442
00:23:30,850 --> 00:23:33,330
and the number of instances.

443
00:23:33,330 --> 00:23:36,690
Here, example, I have
executed the hostname command

444
00:23:36,690 --> 00:23:37,933
over three nodes.

445
00:23:40,182 --> 00:23:43,663
And we are going to, to
receive an application ID.

446
00:23:45,060 --> 00:23:47,270
It's important to take note of it,

447
00:23:47,270 --> 00:23:51,030
but it's even more important
to get this finished status

448
00:23:51,030 --> 00:23:53,780
because that's means that
our application was executed

449
00:23:53,780 --> 00:23:54,613
successfully.

450
00:23:56,210 --> 00:23:59,560
And now what, where can we
see the application outputs?

451
00:23:59,560 --> 00:24:01,793
It's what we are interested in, right?

452
00:24:03,000 --> 00:24:07,230
Well, we can use this command
path in the application ID

453
00:24:07,230 --> 00:24:09,003
we got in the previous step.

454
00:24:09,890 --> 00:24:13,013
And the output is going
to be something like this.

455
00:24:13,950 --> 00:24:16,943
We have executed this
command over three nodes.

456
00:24:17,840 --> 00:24:22,440
So we have three different
outputs for the hostname command.

457
00:24:22,440 --> 00:24:24,420
Of course, we can change
the hostname command

458
00:24:24,420 --> 00:24:25,713
for any other, right?

459
00:24:26,590 --> 00:24:28,553
So let's see a demo of this.

460
00:24:29,700 --> 00:24:31,983
Here, I have improved the Yarn file

461
00:24:31,983 --> 00:24:33,923
to have the path I need to add.

462
00:24:35,810 --> 00:24:39,583
I have my simple yarn
application from Hortonworks.

463
00:24:41,400 --> 00:24:45,593
And I already uploaded it
to the Hadoop filesystem.

464
00:24:46,530 --> 00:24:50,320
So remember, you can
simply copyFromLocal and

465
00:24:50,320 --> 00:24:54,253
just upload the jar to
the remote Hadoop target.

466
00:24:55,530 --> 00:24:56,560
And now with this command,

467
00:24:56,560 --> 00:25:00,620
we have to specify the
local path of the Yarn file

468
00:25:00,620 --> 00:25:02,880
and the command that we want to execute,

469
00:25:02,880 --> 00:25:04,590
and the number of instances,

470
00:25:04,590 --> 00:25:06,593
the nodes and the remote path.

471
00:25:08,180 --> 00:25:09,200
So with these commands,

472
00:25:09,200 --> 00:25:13,933
we are going to get our
application ID and the status.

473
00:25:16,930 --> 00:25:19,853
So now we need to use this application ID.

474
00:25:20,690 --> 00:25:22,610
In my case, I need to move the output

475
00:25:22,610 --> 00:25:24,943
from one directory to other one.

476
00:25:25,870 --> 00:25:27,730
Just to allow yarn to find the,

477
00:25:27,730 --> 00:25:29,870
the output in the next command.

478
00:25:29,870 --> 00:25:31,570
It might be not necessary for you.

479
00:25:32,981 --> 00:25:35,150
So with a yarn command,

480
00:25:35,150 --> 00:25:39,133
we can just get the output
of this application.

481
00:25:40,875 --> 00:25:44,403
So we are going to see the
outputs for the three nodes.

482
00:25:46,100 --> 00:25:51,100
We have the hostname output for
the hadoop1, the first node,

483
00:25:53,464 --> 00:25:54,297
hadoop2,

484
00:25:55,697 --> 00:25:56,563
and hadoop3.

485
00:25:59,620 --> 00:26:03,230
Good. So let me show you one more.

486
00:26:03,230 --> 00:26:06,090
I submit one more application before

487
00:26:06,090 --> 00:26:09,610
to dump a file of the nodes.

488
00:26:09,610 --> 00:26:13,440
In this case, the /etc/password file.

489
00:26:13,440 --> 00:26:16,480
So here we can see the password file

490
00:26:16,480 --> 00:26:19,030
for the three nodes, as well.

491
00:26:19,030 --> 00:26:21,903
So basically you can change these

492
00:26:21,903 --> 00:26:25,720
and execute whatever command you want.

493
00:26:25,720 --> 00:26:29,020
So that's pretty easy to use.

494
00:26:29,020 --> 00:26:29,853
Yep.

495
00:26:33,840 --> 00:26:36,060
It's also, should be quite simple

496
00:26:36,060 --> 00:26:39,430
to change these Yarn
application to execute

497
00:26:39,430 --> 00:26:41,723
perhaps a more complex command.

498
00:26:42,600 --> 00:26:46,500
Just keep in mind that any
changes must be made both

499
00:26:46,500 --> 00:26:49,123
in the application master file,

500
00:26:50,250 --> 00:26:51,500
as we can see here inside

501
00:26:52,683 --> 00:26:54,693
and and also in the client file, right?

502
00:26:55,840 --> 00:26:57,880
So for example, if we
want to get something

503
00:26:57,880 --> 00:27:02,340
like a reverse shell on the
cluster nodes, it's possible.

504
00:27:02,340 --> 00:27:03,450
But keep in mind that

505
00:27:03,450 --> 00:27:07,440
this is a shell that it starts finished.

506
00:27:07,440 --> 00:27:09,440
So we may need to use other alternatives,

507
00:27:09,440 --> 00:27:13,020
like backdooring the crontab
with the Yarn application,

508
00:27:13,020 --> 00:27:14,320
for example.

509
00:27:14,320 --> 00:27:19,320
So you can execute this command
with the Yarn application

510
00:27:19,390 --> 00:27:20,700
and then back through the crontab,

511
00:27:20,700 --> 00:27:25,580
and then you will have your
reverse shell on every cluster.

512
00:27:25,580 --> 00:27:27,893
Sorry, on every node of the cluster.

513
00:27:28,770 --> 00:27:29,603
Good.

514
00:27:30,750 --> 00:27:33,860
I can't help but talk about
Spark in this section.

515
00:27:33,860 --> 00:27:37,640
Spark is super popular,
widely implemented technology

516
00:27:37,640 --> 00:27:40,220
for processing data, as well.

517
00:27:40,220 --> 00:27:42,670
It's generally installed on top of Hadoop

518
00:27:42,670 --> 00:27:47,370
and developers make data
processing application for Spark.

519
00:27:47,370 --> 00:27:51,100
For example, in Python using PySpark.

520
00:27:51,100 --> 00:27:53,040
Because it's easier than developing

521
00:27:53,040 --> 00:27:56,900
a native application for Yarn.

522
00:27:56,900 --> 00:28:00,283
And also Spark has other
advantages over Yarn.

523
00:28:01,630 --> 00:28:03,080
So as we can see here,

524
00:28:03,080 --> 00:28:08,013
Spark has its own IPC
port on 7075, sorry, 7077.

525
00:28:10,810 --> 00:28:14,230
We can submit a Spark
application to be executed

526
00:28:14,230 --> 00:28:16,133
on the cluster through this port.

527
00:28:16,980 --> 00:28:21,980
Is easier than we can. And
here we have an example.

528
00:28:22,150 --> 00:28:25,020
This small code will
connect to the Spark master

529
00:28:25,020 --> 00:28:30,020
to execute the hostname
command on every cluster node.

530
00:28:31,433 --> 00:28:34,304
We should simply need
to specify the remote

531
00:28:34,304 --> 00:28:36,700
Spark master IP address,

532
00:28:36,700 --> 00:28:40,360
our own IP address, to receive
the output of the command

533
00:28:41,210 --> 00:28:42,733
and the command itself.

534
00:28:43,780 --> 00:28:46,680
And then we should run this
script from our machine.

535
00:28:46,680 --> 00:28:50,670
We don't need anything
else. It's quite simple.

536
00:28:50,670 --> 00:28:54,270
But I am going to talk in depth about this

537
00:28:54,270 --> 00:28:55,890
because there is already a talk

538
00:28:55,890 --> 00:28:57,703
a hundred percent dedicated to Spark.

539
00:28:57,703 --> 00:29:01,210
This was given at Defcon, last year.

540
00:29:01,210 --> 00:29:04,000
So I actually recommend
watching this talk.

541
00:29:04,000 --> 00:29:07,980
The speaker explains how to
achieve remote code execution

542
00:29:07,980 --> 00:29:09,647
via Spark IPC.

543
00:29:09,647 --> 00:29:12,353
That is the equivalent
of what we did with Yarn.

544
00:29:13,320 --> 00:29:16,340
So keep in mind that the
Spark may, or may not,

545
00:29:16,340 --> 00:29:18,650
be present to the cluster.

546
00:29:18,650 --> 00:29:22,690
While Yarn will always be
present in Hadoop installations.

547
00:29:22,690 --> 00:29:26,970
So it's good to know how to
achieve remote code execution,

548
00:29:26,970 --> 00:29:29,090
via Yarn and also via Spark.

549
00:29:29,090 --> 00:29:32,323
As we have the possibility to
abuse this technology as well.

550
00:29:34,460 --> 00:29:38,313
Awesome. So let's take a look
at the ingestion layer, now.

551
00:29:41,000 --> 00:29:44,160
If you remember from our big
data architecture example

552
00:29:44,160 --> 00:29:47,890
at the beginning of this
talk, we have sources of data

553
00:29:47,890 --> 00:29:51,670
and such data is ingested to our cluster

554
00:29:51,670 --> 00:29:54,373
using data ingestion technologies.

555
00:29:55,620 --> 00:29:56,983
There are several ones.

556
00:29:58,320 --> 00:30:02,550
We have some design for
streaming like Flume, Kafka,

557
00:30:02,550 --> 00:30:06,500
and Spark Structured Streaming,
that is a variant of Spark.

558
00:30:06,500 --> 00:30:11,500
And then others like Sqoop,
that ingest static information.

559
00:30:12,110 --> 00:30:15,920
For example, from one data
lake to other data lake.

560
00:30:15,920 --> 00:30:20,143
Or from one database to
a data lake and so on.

561
00:30:21,430 --> 00:30:23,620
So from a security point of view,

562
00:30:23,620 --> 00:30:26,870
we need to make sure that these channels,

563
00:30:26,870 --> 00:30:29,530
that the information go
through from the source

564
00:30:29,530 --> 00:30:32,860
to the storage are secure, right?

565
00:30:32,860 --> 00:30:36,520
Otherwise an attacker might
interfere those channels

566
00:30:36,520 --> 00:30:38,543
and ingest malicious data.

567
00:30:39,990 --> 00:30:42,213
Let's see how this could happen.

568
00:30:43,990 --> 00:30:45,530
This is how Spark Streaming

569
00:30:45,530 --> 00:30:48,130
or a Spark Structured Streaming works.

570
00:30:48,130 --> 00:30:51,040
It's a variant of Spark that ingest data,

571
00:30:51,040 --> 00:30:53,650
and also process it
before dumping everything

572
00:30:53,650 --> 00:30:55,810
into the Hadoop filesystem.

573
00:30:55,810 --> 00:30:58,343
So it's like two components in one.

574
00:30:59,320 --> 00:31:02,050
So Spark Structure Streaming or streaming

575
00:31:02,050 --> 00:31:07,050
can works with technologies
like Kafka, Flume and Kinesis

576
00:31:07,320 --> 00:31:09,970
to pull or receive the data.

577
00:31:09,970 --> 00:31:13,490
And also has the possibility
to just ingest data

578
00:31:13,490 --> 00:31:15,790
from TCP sockets.

579
00:31:15,790 --> 00:31:17,893
And that could be pretty dangerous.

580
00:31:19,220 --> 00:31:21,770
Here we have an example
of how the code looks like

581
00:31:21,770 --> 00:31:25,430
when the streaming input
issues a TCP socket.

582
00:31:25,430 --> 00:31:28,250
It basically binds a port to the machine.

583
00:31:28,250 --> 00:31:31,260
So abuse this is super easy.

584
00:31:31,260 --> 00:31:34,760
We can use Netcat or a favorite tool,

585
00:31:34,760 --> 00:31:39,720
and just send data over
the socket. And it works.

586
00:31:39,720 --> 00:31:42,200
What happens to the data that we ingested

587
00:31:42,200 --> 00:31:45,570
will depend on the
application that processes it.

588
00:31:45,570 --> 00:31:47,720
Most likely we will crash the application

589
00:31:47,720 --> 00:31:50,710
because we may be ingesting bytes

590
00:31:50,710 --> 00:31:53,883
that the application
doesn't know how to handle.

591
00:31:53,883 --> 00:31:57,950
Or a byte might end up
inside the Hadoop filesystem.

592
00:31:57,950 --> 00:31:59,063
That's also likely.

593
00:32:00,320 --> 00:32:02,560
So it's important to
check that the interfaces

594
00:32:02,560 --> 00:32:05,360
that are waiting for data to be ingested

595
00:32:05,360 --> 00:32:08,143
cannot be reached by an attacker, right?

596
00:32:09,840 --> 00:32:13,710
And regarding Hadoop, as I said,
it's more of a static data.

597
00:32:13,710 --> 00:32:16,130
It's commonly used to insert information

598
00:32:16,130 --> 00:32:19,753
from different SQL databases into Hadoop.

599
00:32:21,300 --> 00:32:25,260
Analyzing a Sqoop server
I found an API exposed

600
00:32:25,260 --> 00:32:27,743
by default on port 12000.

601
00:32:30,130 --> 00:32:33,650
We can get the Sqoop software
version, for example,

602
00:32:33,650 --> 00:32:35,260
using this query,

603
00:32:35,260 --> 00:32:38,362
but there is not so much
documentation about the API.

604
00:32:38,362 --> 00:32:40,660
And honestly, it's quite easier

605
00:32:40,660 --> 00:32:42,220
to, um...

606
00:32:42,220 --> 00:32:44,773
abuse this using the Sqoop client.

607
00:32:46,730 --> 00:32:49,310
So something important
is to download the same

608
00:32:49,310 --> 00:32:51,510
client version of the server.

609
00:32:51,510 --> 00:32:54,800
For example, this already is 1.99.7,

610
00:32:55,660 --> 00:32:58,530
we should download that
version of the client

611
00:32:58,530 --> 00:32:59,863
from this website.

612
00:33:01,560 --> 00:33:04,230
Good. So what can we do?

613
00:33:04,230 --> 00:33:07,220
Well, we could, for example,
ingest malicious data

614
00:33:07,220 --> 00:33:10,020
from that database that
belongs to the attacker

615
00:33:10,020 --> 00:33:11,833
into the target Hadoop filesystem.

616
00:33:12,870 --> 00:33:14,540
That takes some steps.

617
00:33:14,540 --> 00:33:17,170
We have to connect to
the remote Sqoop server,

618
00:33:17,170 --> 00:33:18,780
create some links.

619
00:33:18,780 --> 00:33:21,680
This is provide Sqoop with
the information to connect

620
00:33:21,680 --> 00:33:26,650
to the malicious database and
the target Hadoop filesystem.

621
00:33:26,650 --> 00:33:29,270
And then we have to create a Sqoop job

622
00:33:29,270 --> 00:33:32,377
specifying that we want to ingest data

623
00:33:32,377 --> 00:33:37,377
from this database link to this
other HDF link and store it.

624
00:33:39,673 --> 00:33:41,730
So this is quite easier to
understand with the demo.

625
00:33:41,730 --> 00:33:43,853
So let's see video demonstrate.

626
00:33:46,410 --> 00:33:49,010
So here I have my Sqoop client,

627
00:33:49,010 --> 00:33:52,230
and I connect to the remote Sqoop server.

628
00:33:52,230 --> 00:33:54,730
These are the connectors
we have available.

629
00:33:54,730 --> 00:33:58,633
We need to create a link
for the mySQL database,

630
00:34:00,090 --> 00:34:02,910
the remote attacker database.

631
00:34:02,910 --> 00:34:05,220
So I will specify the mySQL driver

632
00:34:05,220 --> 00:34:08,990
and the remote address of the database,

633
00:34:08,990 --> 00:34:11,810
some credentials to access with.

634
00:34:11,810 --> 00:34:14,303
And then most of the
parameters are optional.

635
00:34:15,430 --> 00:34:17,427
So I will just create it.

636
00:34:18,300 --> 00:34:22,637
And also we need to create
a link for the HDF target.

637
00:34:25,810 --> 00:34:27,460
Here we have to specify two parameters.

638
00:34:27,460 --> 00:34:31,410
The first one is the remote
IP address of the Hadoop IPC.

639
00:34:31,410 --> 00:34:34,130
In this case, it's in the ports 9000,

640
00:34:34,130 --> 00:34:39,130
but it's going to be most likely
in 8020, as we saw before.

641
00:34:39,130 --> 00:34:43,900
And the Conf directory is a
remote path, not a local one.

642
00:34:43,900 --> 00:34:45,630
It's a remote path. That by def...

643
00:34:45,630 --> 00:34:49,403
It's going to be the Hadoop
installation path by default.

644
00:34:50,560 --> 00:34:53,615
So now I hear on it, I specify on it,

645
00:34:53,615 --> 00:34:55,000
the path of DEXO machine,

646
00:34:55,000 --> 00:34:57,460
but it's going to be most likely lines,

647
00:34:57,460 --> 00:35:00,217
/etc/hadoop/conf.

648
00:35:01,070 --> 00:35:04,580
So good now, so now we
find, now we have the,

649
00:35:04,580 --> 00:35:07,580
the links we have to create a job

650
00:35:07,580 --> 00:35:09,730
and the job we are going to specify that

651
00:35:09,730 --> 00:35:14,730
we want to inject data from
the attacker mySQL database

652
00:35:14,930 --> 00:35:17,723
to the target HDF.

653
00:35:21,270 --> 00:35:24,420
So we need to specify
the name of the table

654
00:35:24,420 --> 00:35:26,810
we are going to ingest

655
00:35:26,810 --> 00:35:30,310
and then most of the
parameters are optionals.

656
00:35:30,310 --> 00:35:32,433
So I'll, I leave it blank.

657
00:35:36,840 --> 00:35:39,130
So once we create the job.

658
00:35:39,130 --> 00:35:42,080
Ah, also, here we have to
specify the output directory

659
00:35:42,080 --> 00:35:42,917
and that's also important.

660
00:35:42,917 --> 00:35:46,503
That's the remote directory
in the Hadoop filesystem.

661
00:35:47,950 --> 00:35:52,950
So, right. Now we have our
jobs, ingest-malicious-data job.

662
00:35:53,340 --> 00:35:55,023
We just need to start it.

663
00:35:57,480 --> 00:36:01,860
This is all we are going to
see in the attacker machine,

664
00:36:01,860 --> 00:36:06,440
but to show you that we actually
ingest the malicious data,

665
00:36:06,440 --> 00:36:08,713
I will login to the remote machine

666
00:36:08,713 --> 00:36:10,930
that has the Hadoop filesystem,

667
00:36:10,930 --> 00:36:11,763
just to show that the
data was actually ingested

668
00:36:14,654 --> 00:36:16,570
in the filesystem.

669
00:36:16,570 --> 00:36:18,653
Here we have the hacking Hadoop

670
00:36:18,653 --> 00:36:21,539
and the hello, blahblah, malicious data

671
00:36:21,539 --> 00:36:23,789
that was in my remote mySQL

672
00:36:24,897 --> 00:36:27,548
and I ingested it into this group,

673
00:36:27,548 --> 00:36:30,798
into their Hadoop filesystem via Sqoop.

674
00:36:32,122 --> 00:36:35,328
So keep in mind that you
can ingest malicious data,

675
00:36:35,328 --> 00:36:37,245
but you can also, um...

676
00:36:38,119 --> 00:36:39,037
export data

677
00:36:39,037 --> 00:36:41,670
because Sqoop allows you
to import and export.

678
00:36:41,670 --> 00:36:46,443
So you can do this in a reverse
way and still steal data

679
00:36:46,443 --> 00:36:50,900
from the Hadoop filesystem into
your remote mySQL database,

680
00:36:50,900 --> 00:36:51,733
for example.

681
00:36:53,810 --> 00:36:54,810
Good. So finally,

682
00:36:54,810 --> 00:36:57,953
let's talk a little bit
about the data access layer.

683
00:37:00,500 --> 00:37:03,170
Back to our architecture example,

684
00:37:03,170 --> 00:37:06,240
we saw that it's possible to
use different technologies

685
00:37:06,240 --> 00:37:08,300
for data access.

686
00:37:08,300 --> 00:37:12,240
In this example, we are using
Presto together with Hive,

687
00:37:12,240 --> 00:37:14,770
but there are many others.

688
00:37:14,770 --> 00:37:18,130
And when it comes to Hive and HBase,

689
00:37:18,130 --> 00:37:22,140
these are HDF based storage technologies,

690
00:37:22,140 --> 00:37:27,140
but they also provide interfaces
to access the information.

691
00:37:27,590 --> 00:37:30,420
For example, Presto
needs the Hive metastore

692
00:37:30,420 --> 00:37:33,323
to create information and
store in the Hadoop filesystem.

693
00:37:34,600 --> 00:37:38,410
So this technology expose
dashboard and interfaces

694
00:37:38,410 --> 00:37:40,290
that can be abused

695
00:37:41,630 --> 00:37:44,253
by an attacker if they
are not rightly protected.

696
00:37:46,950 --> 00:37:50,840
For example, Hive exposes
a dashboard on port 10002

697
00:37:50,840 --> 00:37:53,090
where we can get interesting information

698
00:37:53,090 --> 00:37:58,090
and also an idea how the data
is structured in the storage.

699
00:37:58,410 --> 00:38:02,340
The same for HBase. And regarding Presto

700
00:38:02,340 --> 00:38:05,450
I found this tedious login form

701
00:38:05,450 --> 00:38:07,360
where a password is not allowed.

702
00:38:07,360 --> 00:38:10,710
It's quite curious
because it's a login form,

703
00:38:10,710 --> 00:38:12,310
but you cannot enter a password.

704
00:38:13,630 --> 00:38:17,260
I know that you can set
up one by, by, button,

705
00:38:17,260 --> 00:38:20,223
but by default, it seems to be this way.

706
00:38:21,070 --> 00:38:25,240
So you can write admin
user there and enter.

707
00:38:25,240 --> 00:38:26,270
And there is a doc for,

708
00:38:26,270 --> 00:38:30,230
that show some information
about the indirected queries

709
00:38:30,230 --> 00:38:32,123
being executed against the cluster.

710
00:38:34,240 --> 00:38:35,640
Good.

711
00:38:35,640 --> 00:38:39,550
So as I said, this technology
expose several interfaces.

712
00:38:39,550 --> 00:38:42,963
It's common to find at least a JDBC one.

713
00:38:43,920 --> 00:38:47,697
For example, in Hive, we
can find it on port 10000

714
00:38:49,360 --> 00:38:51,348
and there are different
clients that we can use

715
00:38:51,348 --> 00:38:52,710
to connect to it.

716
00:38:52,710 --> 00:38:55,090
Like SQRL, for example,

717
00:38:55,090 --> 00:38:57,203
or even Hadoop includes Beeline.

718
00:38:58,042 --> 00:39:01,590
We can connect to the remote Hive servers

719
00:39:01,590 --> 00:39:04,630
just specifying the remote address.

720
00:39:04,630 --> 00:39:07,720
If no authentication is
required, of course, but yeah,

721
00:39:07,720 --> 00:39:09,593
there's usually nothing by default.

722
00:39:10,800 --> 00:39:12,863
And Hive has its own commands.

723
00:39:13,830 --> 00:39:16,863
We need to know them to
browse the information.

724
00:39:17,700 --> 00:39:19,160
With show databases,

725
00:39:19,160 --> 00:39:22,037
we can see the databases in the cluster.

726
00:39:22,037 --> 00:39:25,123
Select one and show its tables.

727
00:39:26,340 --> 00:39:27,830
And then we have syntaxes

728
00:39:27,830 --> 00:39:32,643
to insert, update, delete
like any other SQL database.

729
00:39:34,120 --> 00:39:35,960
Good. So I running out of time.

730
00:39:35,960 --> 00:39:40,913
So let us provide some
recommendations as conclusion.

731
00:39:42,890 --> 00:39:45,720
Many attacks that we
saw throughout this talk

732
00:39:45,720 --> 00:39:48,520
were based on exposed interfaces.

733
00:39:48,520 --> 00:39:52,360
And there are many dashboard
that are exposed by default.

734
00:39:52,360 --> 00:39:54,630
So, if they are not be being used,

735
00:39:54,630 --> 00:39:58,130
we should either remove them
or block the access to them

736
00:39:58,130 --> 00:40:00,240
using a firewall, for example.

737
00:40:01,200 --> 00:40:04,310
If some components need
to talk to each other

738
00:40:04,310 --> 00:40:06,090
without a firewall in the middle,

739
00:40:06,090 --> 00:40:09,223
then we should secure
the perimeter, at least.

740
00:40:10,140 --> 00:40:11,920
The firewall has to be present

741
00:40:11,920 --> 00:40:15,720
despite the official documentation
ask for disabling it.

742
00:40:15,720 --> 00:40:19,585
I believe that we can
investigate what ports needs

743
00:40:19,585 --> 00:40:21,590
to be allowed in our infrastructure

744
00:40:21,590 --> 00:40:25,263
and design a good firewall policy rules.

745
00:40:27,220 --> 00:40:29,870
Do remember also to change
all the default credentials.

746
00:40:29,870 --> 00:40:33,200
Implement any kind of authentication
in all the technologies

747
00:40:33,200 --> 00:40:34,053
being used.

748
00:40:35,310 --> 00:40:39,730
Hadoop support authentication for the HDF.

749
00:40:39,730 --> 00:40:41,330
It's actually possible to
implement authentication

750
00:40:41,330 --> 00:40:44,330
and authorization in
most of the technologies

751
00:40:44,330 --> 00:40:47,490
that we have seen. But we have to do it.

752
00:40:47,490 --> 00:40:50,373
Because by default, there
is nothing implemented.

753
00:40:52,030 --> 00:40:55,410
Finally, remember that in
the big data infrastructure,

754
00:40:55,410 --> 00:40:57,020
there are many different technologies

755
00:40:57,020 --> 00:40:58,750
communicating with each other.

756
00:40:58,750 --> 00:41:02,520
So make sure that those
communications are happening

757
00:41:02,520 --> 00:41:03,793
in a secure way.

758
00:41:05,520 --> 00:41:08,630
Good. So in the next
weeks I hope to be able

759
00:41:08,630 --> 00:41:11,230
to put some more resources

760
00:41:11,230 --> 00:41:14,830
about the practical implementation
of security measures.

761
00:41:14,830 --> 00:41:16,910
So for today, that's all.

762
00:41:16,910 --> 00:41:19,450
Thank you for watching my talk and here's

763
00:41:19,450 --> 00:41:22,700
my contact information in
case you have any questions.

764
00:41:22,700 --> 00:41:25,240
Please feel free to reach me out.

765
00:41:25,240 --> 00:41:27,163
Thank you so much. Bye, bye.

