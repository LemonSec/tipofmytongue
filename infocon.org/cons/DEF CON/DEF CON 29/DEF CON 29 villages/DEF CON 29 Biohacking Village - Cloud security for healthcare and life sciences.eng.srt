1
00:00:04,560 --> 00:00:06,240
welcome everyone joining us today for

2
00:00:06,240 --> 00:00:08,000
this critical conversation around the

3
00:00:08,000 --> 00:00:09,840
intersection of cloud computing and

4
00:00:09,840 --> 00:00:11,440
healthcare and life sciences

5
00:00:11,440 --> 00:00:13,360
specifically focused on the secure and

6
00:00:13,360 --> 00:00:14,799
sensitive nature of much of the

7
00:00:14,799 --> 00:00:16,640
information flow that happens in those

8
00:00:16,640 --> 00:00:18,560
types of missions this panel will

9
00:00:18,560 --> 00:00:20,400
discuss recent use cases highlighting

10
00:00:20,400 --> 00:00:22,000
best security practices for cloud

11
00:00:22,000 --> 00:00:23,279
computing and healthcare and life

12
00:00:23,279 --> 00:00:25,279
sciences today we're going to focus on

13
00:00:25,279 --> 00:00:27,279
three main main takeaways and our four

14
00:00:27,279 --> 00:00:29,039
panelists will provide insights on those

15
00:00:29,039 --> 00:00:31,359
areas and we'll also be doing live q a

16
00:00:31,359 --> 00:00:33,280
so roll up your sleeves and get engaged

17
00:00:33,280 --> 00:00:35,680
with us the areas we're covering today

18
00:00:35,680 --> 00:00:37,440
are the benefits of using cloud for

19
00:00:37,440 --> 00:00:39,040
healthcare and life science data and

20
00:00:39,040 --> 00:00:41,520
analysis including scalable resources

21
00:00:41,520 --> 00:00:44,320
ease of collaboration i got amalgamation

22
00:00:44,320 --> 00:00:48,000
of data the use of aiml applications and

23
00:00:48,000 --> 00:00:50,239
replicability of research the second

24
00:00:50,239 --> 00:00:51,920
we'll be focusing on is building

25
00:00:51,920 --> 00:00:53,840
security and privacy needs into

26
00:00:53,840 --> 00:00:55,760
platforms as well as every layer of

27
00:00:55,760 --> 00:00:58,000
cloud-based projects and the final one

28
00:00:58,000 --> 00:01:00,079
is the implications of security for the

29
00:01:00,079 --> 00:01:02,640
data and ip of individual biomedical

30
00:01:02,640 --> 00:01:05,519
project projects organizations not just

31
00:01:05,519 --> 00:01:07,439
specific products and capabilities but

32
00:01:07,439 --> 00:01:09,760
also the broader bioeconomy and

33
00:01:09,760 --> 00:01:12,320
biosecurity we are so lucky today

34
00:01:12,320 --> 00:01:14,240
because we have four of my favorite

35
00:01:14,240 --> 00:01:17,759
people matt has hazel from the fda fda

36
00:01:17,759 --> 00:01:21,360
it's cdrh is our fda regulatory guru

37
00:01:21,360 --> 00:01:24,720
welcome matt uh andrea that's

38
00:01:24,720 --> 00:01:28,320
master sean sorry is our law policy tech

39
00:01:28,320 --> 00:01:31,200
cyber uh guru from penn state and is

40
00:01:31,200 --> 00:01:32,479
really going to be talking to us today

41
00:01:32,479 --> 00:01:35,759
about legal and policy issues dan prieto

42
00:01:35,759 --> 00:01:38,000
us on cyber security from google is our

43
00:01:38,000 --> 00:01:40,240
cyber security guru and finally michelle

44
00:01:40,240 --> 00:01:41,680
holco

45
00:01:41,680 --> 00:01:44,960
hcls sme from google is our biomedical

46
00:01:44,960 --> 00:01:47,439
research guru so with that welcome to

47
00:01:47,439 --> 00:01:48,880
our session today and as you can tell

48
00:01:48,880 --> 00:01:50,399
we're excited we're just going to jump

49
00:01:50,399 --> 00:01:53,200
right in so michelle let's level set

50
00:01:53,200 --> 00:01:54,479
help me understand

51
00:01:54,479 --> 00:01:56,399
you know without going too much into the

52
00:01:56,399 --> 00:01:58,479
basics what is cloud computing and what

53
00:01:58,479 --> 00:01:59,759
are some of the benefits of cloud

54
00:01:59,759 --> 00:02:01,920
computing what are some of the trends

55
00:02:01,920 --> 00:02:03,920
around use in adoption in the healthcare

56
00:02:03,920 --> 00:02:07,119
and life sciences sector

57
00:02:07,280 --> 00:02:09,679
yeah absolutely thank you so much alexis

58
00:02:09,679 --> 00:02:11,599
and thank you um for all of the

59
00:02:11,599 --> 00:02:12,879
panelists for being here and for

60
00:02:12,879 --> 00:02:14,400
everybody who's joining us today for

61
00:02:14,400 --> 00:02:15,599
this discussion

62
00:02:15,599 --> 00:02:17,520
um so this this topic is near and dear

63
00:02:17,520 --> 00:02:19,599
to my heart because i have recently

64
00:02:19,599 --> 00:02:22,160
joined google cloud um and but have been

65
00:02:22,160 --> 00:02:23,280
in the healthcare and life sciences

66
00:02:23,280 --> 00:02:25,120
space for a long time and have actually

67
00:02:25,120 --> 00:02:28,239
used cloud-based uh platforms to do data

68
00:02:28,239 --> 00:02:30,879
and analytics in the biomedical space

69
00:02:30,879 --> 00:02:33,040
so basically you know cloud

70
00:02:33,040 --> 00:02:35,599
cloud-based computing is just leveraging

71
00:02:35,599 --> 00:02:37,599
on-demand compute system resources

72
00:02:37,599 --> 00:02:39,120
including storage

73
00:02:39,120 --> 00:02:41,360
as well as compute

74
00:02:41,360 --> 00:02:44,000
distributed teams sharing resources and

75
00:02:44,000 --> 00:02:45,680
being able to build in economies of

76
00:02:45,680 --> 00:02:46,720
scale

77
00:02:46,720 --> 00:02:48,239
and you know there are a lot of reasons

78
00:02:48,239 --> 00:02:49,120
why

79
00:02:49,120 --> 00:02:51,040
cloud computing is being added to the

80
00:02:51,040 --> 00:02:52,959
healthcare and life sciences space i'm

81
00:02:52,959 --> 00:02:54,239
going to talk through a few of the use

82
00:02:54,239 --> 00:02:55,680
cases that we'll go into a little bit

83
00:02:55,680 --> 00:02:57,920
more detail throughout this discussion

84
00:02:57,920 --> 00:03:00,080
um but some of the driving factors are

85
00:03:00,080 --> 00:03:02,480
you know number one cost um it's

86
00:03:02,480 --> 00:03:04,400
actually a lot easier for young

87
00:03:04,400 --> 00:03:06,319
organizations as they're getting started

88
00:03:06,319 --> 00:03:08,239
instead of investing in a lot of

89
00:03:08,239 --> 00:03:11,120
overhead a lot of you know equipment

90
00:03:11,120 --> 00:03:12,800
they can just kind of spin up some cloud

91
00:03:12,800 --> 00:03:14,159
environments

92
00:03:14,159 --> 00:03:16,000
another really big one is scalability

93
00:03:16,000 --> 00:03:17,360
especially when you're looking at

94
00:03:17,360 --> 00:03:19,599
specific data types that require a lot

95
00:03:19,599 --> 00:03:22,080
of size to store as well as to compute

96
00:03:22,080 --> 00:03:24,560
across including images and genomics

97
00:03:24,560 --> 00:03:25,519
data

98
00:03:25,519 --> 00:03:28,000
and then a third really important reason

99
00:03:28,000 --> 00:03:29,760
is the ease of collaboration

100
00:03:29,760 --> 00:03:31,760
you know one of the reasons

101
00:03:31,760 --> 00:03:34,879
one of the enablers for ai and ml

102
00:03:34,879 --> 00:03:36,400
machine learning and artificial

103
00:03:36,400 --> 00:03:38,480
intelligence based studies is you have

104
00:03:38,480 --> 00:03:40,000
to have a

105
00:03:40,000 --> 00:03:41,840
like a critical mass of data in order to

106
00:03:41,840 --> 00:03:44,239
make those types of studies possible and

107
00:03:44,239 --> 00:03:46,319
so being able to really bring all of

108
00:03:46,319 --> 00:03:48,720
those types of data together across

109
00:03:48,720 --> 00:03:50,480
different organizations across different

110
00:03:50,480 --> 00:03:52,879
laboratories um is really an enabling

111
00:03:52,879 --> 00:03:55,599
factor um and then you know going along

112
00:03:55,599 --> 00:03:56,879
with the collaboration piece is

113
00:03:56,879 --> 00:03:59,280
reproducibility of research um you know

114
00:03:59,280 --> 00:04:01,760
there's uh some types of crazy number

115
00:04:01,760 --> 00:04:04,239
where i think it's like 30 to 50 of

116
00:04:04,239 --> 00:04:06,799
research is actually not reproducible um

117
00:04:06,799 --> 00:04:09,040
and so cloud is actually enabling a lot

118
00:04:09,040 --> 00:04:11,200
of that uh that that type of a barrier

119
00:04:11,200 --> 00:04:14,080
where now you know if if you're if your

120
00:04:14,080 --> 00:04:16,238
data is in a cloud-based system that you

121
00:04:16,238 --> 00:04:18,238
know researchers and reviewers can

122
00:04:18,238 --> 00:04:21,199
access with the tools that you use um

123
00:04:21,199 --> 00:04:23,040
then they can go in and they can look at

124
00:04:23,040 --> 00:04:25,199
those they can tweak some things and see

125
00:04:25,199 --> 00:04:27,120
if you know your research results still

126
00:04:27,120 --> 00:04:29,040
make sense so those are just some of

127
00:04:29,040 --> 00:04:30,479
some of the reasons

128
00:04:30,479 --> 00:04:32,720
why this transition to the cloud why

129
00:04:32,720 --> 00:04:34,000
we're seeing an increased transition to

130
00:04:34,000 --> 00:04:35,120
the cloud

131
00:04:35,120 --> 00:04:36,720
so just to give you a few examples of

132
00:04:36,720 --> 00:04:38,240
what's in scope for this discussion

133
00:04:38,240 --> 00:04:40,400
because this it feels like a very broad

134
00:04:40,400 --> 00:04:43,680
space to a lot of us um and and so it's

135
00:04:43,680 --> 00:04:45,199
kind of comforting to think about well

136
00:04:45,199 --> 00:04:46,880
what are the what are the areas that

137
00:04:46,880 --> 00:04:48,479
we're gonna think about and consider

138
00:04:48,479 --> 00:04:50,880
today um so some of them are certainly

139
00:04:50,880 --> 00:04:52,880
the biomedical research space which i've

140
00:04:52,880 --> 00:04:54,240
already alluded to

141
00:04:54,240 --> 00:04:56,400
and this can be in the clinical

142
00:04:56,400 --> 00:04:58,080
epidemiological

143
00:04:58,080 --> 00:05:00,479
as well as biomedical or molecular space

144
00:05:00,479 --> 00:05:04,160
including omics data um so um

145
00:05:04,160 --> 00:05:06,720
you know again some of the reasons why

146
00:05:06,720 --> 00:05:08,479
this is important is because of the data

147
00:05:08,479 --> 00:05:11,120
types genomics and image data can be

148
00:05:11,120 --> 00:05:14,400
very compute heavy um as well as as

149
00:05:14,400 --> 00:05:15,600
as well as

150
00:05:15,600 --> 00:05:17,199
you know storage heavy

151
00:05:17,199 --> 00:05:19,600
um and then another is the real world

152
00:05:19,600 --> 00:05:21,919
data or personal uh personal generated

153
00:05:21,919 --> 00:05:23,039
data

154
00:05:23,039 --> 00:05:25,199
types and so these are oftentimes coming

155
00:05:25,199 --> 00:05:26,479
off of

156
00:05:26,479 --> 00:05:28,800
devices that are being used either in

157
00:05:28,800 --> 00:05:31,840
the health care phase in as medical

158
00:05:31,840 --> 00:05:34,560
devices or in the health adjacent space

159
00:05:34,560 --> 00:05:36,639
fitness trackers come to mind and these

160
00:05:36,639 --> 00:05:38,240
types of devices are instead of

161
00:05:38,240 --> 00:05:40,240
generating single point of data they're

162
00:05:40,240 --> 00:05:43,120
generating continuous streams of data so

163
00:05:43,120 --> 00:05:44,560
it's actually you know a very

164
00:05:44,560 --> 00:05:46,720
interesting space because there's a lot

165
00:05:46,720 --> 00:05:48,880
of discovery to happen in terms of how

166
00:05:48,880 --> 00:05:51,759
do those continuous data streams relate

167
00:05:51,759 --> 00:05:53,520
back to the physiological state of the

168
00:05:53,520 --> 00:05:54,720
individual

169
00:05:54,720 --> 00:05:56,800
um so there's a lot of interesting

170
00:05:56,800 --> 00:05:59,280
promise there um but it also means that

171
00:05:59,280 --> 00:06:01,120
it's a lot more important to make sure

172
00:06:01,120 --> 00:06:03,759
that those types of devices are secure

173
00:06:03,759 --> 00:06:06,000
um and that those data streams and as

174
00:06:06,000 --> 00:06:07,919
well the analytic algorithms that are

175
00:06:07,919 --> 00:06:09,759
being uh that are trans doing the

176
00:06:09,759 --> 00:06:12,080
translating of the continuous data into

177
00:06:12,080 --> 00:06:14,800
the real world um insights that those

178
00:06:14,800 --> 00:06:18,080
are those are also secure and protected

179
00:06:18,080 --> 00:06:19,759
um another area where we're seeing a lot

180
00:06:19,759 --> 00:06:21,360
of cloud adoption as i i think

181
00:06:21,360 --> 00:06:23,759
previously alluded to is biotech startup

182
00:06:23,759 --> 00:06:25,759
so new organizations coming out of

183
00:06:25,759 --> 00:06:28,400
academia or other companies

184
00:06:28,400 --> 00:06:29,680
that again they don't have a lot of

185
00:06:29,680 --> 00:06:31,680
overhead overhead

186
00:06:31,680 --> 00:06:34,000
overhead yet and and certainly we're

187
00:06:34,000 --> 00:06:35,680
seeing a lot in the times of covid where

188
00:06:35,680 --> 00:06:38,160
people are working from home um so maybe

189
00:06:38,160 --> 00:06:39,840
you know they're they have a work office

190
00:06:39,840 --> 00:06:41,199
space at home where they're they're

191
00:06:41,199 --> 00:06:42,720
doing a lot of their data analytics and

192
00:06:42,720 --> 00:06:44,800
so the cloud makes that really easy so

193
00:06:44,800 --> 00:06:47,120
you don't have to have a lot of hardware

194
00:06:47,120 --> 00:06:48,720
on-site

195
00:06:48,720 --> 00:06:51,599
and then finally hospital systems

196
00:06:51,599 --> 00:06:53,759
you know ehr data electronic health

197
00:06:53,759 --> 00:06:56,080
record data moving from paper-based

198
00:06:56,080 --> 00:06:58,240
systems into a fully

199
00:06:58,240 --> 00:07:00,720
digital type of a system and then all of

200
00:07:00,720 --> 00:07:03,919
the uh the areas of interoperability um

201
00:07:03,919 --> 00:07:06,000
this feeds then back into the research

202
00:07:06,000 --> 00:07:08,000
question because oftentimes you know

203
00:07:08,000 --> 00:07:09,759
it's different data types having to be

204
00:07:09,759 --> 00:07:11,520
combined in order to really have

205
00:07:11,520 --> 00:07:14,240
insights um so really being able to

206
00:07:14,240 --> 00:07:16,319
build interoperable

207
00:07:16,319 --> 00:07:18,000
systems for electronic health record

208
00:07:18,000 --> 00:07:19,680
data that can then be utilized with the

209
00:07:19,680 --> 00:07:22,560
biomedical and molecular data

210
00:07:22,560 --> 00:07:25,280
um and um yeah so that so that's just

211
00:07:25,280 --> 00:07:27,680
you know a basic summary of where where

212
00:07:27,680 --> 00:07:30,000
we're talking about you know in terms of

213
00:07:30,000 --> 00:07:31,520
where we're focusing our discussion on

214
00:07:31,520 --> 00:07:33,440
cloud security in this phase

215
00:07:33,440 --> 00:07:34,880
now i appreciate that and i think one of

216
00:07:34,880 --> 00:07:36,080
the things that i'm really hearing is

217
00:07:36,080 --> 00:07:37,919
that we have the potential you know to

218
00:07:37,919 --> 00:07:39,599
be sharing more information than ever

219
00:07:39,599 --> 00:07:41,280
really to move at light speed but we've

220
00:07:41,280 --> 00:07:43,280
got to be careful right as we do that

221
00:07:43,280 --> 00:07:44,639
for all of the reasons that you

222
00:07:44,639 --> 00:07:46,319
mentioned i want to push this to all of

223
00:07:46,319 --> 00:07:48,479
you so anyone jump in that that wants to

224
00:07:48,479 --> 00:07:50,240
but what are some of the specific

225
00:07:50,240 --> 00:07:52,479
security challenges at the intersection

226
00:07:52,479 --> 00:07:54,160
of cloud computing and healthcare and

227
00:07:54,160 --> 00:07:55,520
life sciences you know are there

228
00:07:55,520 --> 00:07:57,599
examples or you know what can we learn

229
00:07:57,599 --> 00:07:59,840
from these in that idea of moving to

230
00:07:59,840 --> 00:08:03,198
light speed but carefully

231
00:08:03,440 --> 00:08:06,240
dan here i think a couple things i think

232
00:08:06,240 --> 00:08:07,680
when you think about security as it

233
00:08:07,680 --> 00:08:10,639
relates to health care and life sciences

234
00:08:10,639 --> 00:08:12,639
a couple regimes come to play and i

235
00:08:12,639 --> 00:08:14,400
think this will be good setup for for

236
00:08:14,400 --> 00:08:17,199
matt and andrea in that

237
00:08:17,199 --> 00:08:18,479
you know when you think a lot of times

238
00:08:18,479 --> 00:08:19,680
about just plain old information

239
00:08:19,680 --> 00:08:21,120
technology let's not think health for a

240
00:08:21,120 --> 00:08:22,400
minute a lot of times you're thinking

241
00:08:22,400 --> 00:08:25,199
about computers and data but a lot of

242
00:08:25,199 --> 00:08:27,520
times that body of cyber security best

243
00:08:27,520 --> 00:08:29,039
practice can often be treated as

244
00:08:29,039 --> 00:08:32,479
separate from a privacy corpus

245
00:08:32,479 --> 00:08:34,080
of sort of rules and regulations

246
00:08:34,080 --> 00:08:35,679
particularly if you're not dealing with

247
00:08:35,679 --> 00:08:37,039
customer data

248
00:08:37,039 --> 00:08:38,880
in the health and life sciences space

249
00:08:38,880 --> 00:08:41,679
those two regimes automatically overlap

250
00:08:41,679 --> 00:08:43,679
and overlap significantly so you need to

251
00:08:43,679 --> 00:08:45,600
think about security and privacy at the

252
00:08:45,600 --> 00:08:47,680
same time that also then therefore

253
00:08:47,680 --> 00:08:49,360
brings to the fore how you think about

254
00:08:49,360 --> 00:08:51,200
the regulatory regimes there's hipaa

255
00:08:51,200 --> 00:08:53,360
there's sock two there's high trust and

256
00:08:53,360 --> 00:08:54,959
all these best practices that are out

257
00:08:54,959 --> 00:08:57,279
there um just as a general matter for

258
00:08:57,279 --> 00:08:59,360
anything health care related so i i'll

259
00:08:59,360 --> 00:09:01,360
leave further discussion of that to matt

260
00:09:01,360 --> 00:09:03,760
and andrea after that my second comment

261
00:09:03,760 --> 00:09:05,519
which is

262
00:09:05,519 --> 00:09:07,120
michelle did a really good job of

263
00:09:07,120 --> 00:09:09,120
pointing out the different business

264
00:09:09,120 --> 00:09:11,839
models in which cloud and therefore

265
00:09:11,839 --> 00:09:14,399
cloud security is implicated right there

266
00:09:14,399 --> 00:09:16,800
are traditional long-standing models

267
00:09:16,800 --> 00:09:18,560
health records for example

268
00:09:18,560 --> 00:09:20,959
where what is afoot on the technology

269
00:09:20,959 --> 00:09:22,399
side is still

270
00:09:22,399 --> 00:09:25,120
a long-term attempt to transform away

271
00:09:25,120 --> 00:09:26,880
from paper records so there's a huge

272
00:09:26,880 --> 00:09:29,600
piece that's related to digitization

273
00:09:29,600 --> 00:09:32,240
consuming the paper this mound of paper

274
00:09:32,240 --> 00:09:33,600
that you existed on previously and

275
00:09:33,600 --> 00:09:35,279
moving to a new model where you both

276
00:09:35,279 --> 00:09:36,880
have the old paper which is now

277
00:09:36,880 --> 00:09:38,959
digitized but also

278
00:09:38,959 --> 00:09:41,440
have structured sort of digital data

279
00:09:41,440 --> 00:09:43,279
from inception through more you know

280
00:09:43,279 --> 00:09:45,839
user interfaces patient self scheduling

281
00:09:45,839 --> 00:09:47,920
more electronic health records you're

282
00:09:47,920 --> 00:09:49,440
inserting in the middle of an old

283
00:09:49,440 --> 00:09:52,240
business model the cloud and so you need

284
00:09:52,240 --> 00:09:54,320
to ask the security question relative to

285
00:09:54,320 --> 00:09:55,680
what is going to be a long-standing

286
00:09:55,680 --> 00:09:57,600
decades long-standing set of security

287
00:09:57,600 --> 00:09:59,760
practices related to the old stuff

288
00:09:59,760 --> 00:10:01,040
right and so that is going to be an

289
00:10:01,040 --> 00:10:03,600
enormous culture change on other models

290
00:10:03,600 --> 00:10:05,120
where for example it's the internet of

291
00:10:05,120 --> 00:10:07,519
things it's new medical devices it's a

292
00:10:07,519 --> 00:10:09,839
you know adjacent thing like a fitbit or

293
00:10:09,839 --> 00:10:12,399
it's a you know glucose monitor that is

294
00:10:12,399 --> 00:10:14,320
constantly sending telemetry off so that

295
00:10:14,320 --> 00:10:16,560
your doctor your patient uh care

296
00:10:16,560 --> 00:10:18,720
provider and your nurse can see the data

297
00:10:18,720 --> 00:10:20,240
and help you manage for example your

298
00:10:20,240 --> 00:10:22,880
diabetes or some other chronic disease

299
00:10:22,880 --> 00:10:25,760
cloud is native in that business model

300
00:10:25,760 --> 00:10:26,560
right

301
00:10:26,560 --> 00:10:28,560
the device itself can't store the data

302
00:10:28,560 --> 00:10:29,680
so the

303
00:10:29,680 --> 00:10:31,680
so automatically that data needs to be

304
00:10:31,680 --> 00:10:33,760
securely sent to a repository in the

305
00:10:33,760 --> 00:10:36,240
cloud and read by a healthcare provider

306
00:10:36,240 --> 00:10:38,320
from that repository in the cloud so

307
00:10:38,320 --> 00:10:40,399
you're dealing with less legacy security

308
00:10:40,399 --> 00:10:42,160
architectures but now you're also not

309
00:10:42,160 --> 00:10:43,920
dealing with the security of the cloud

310
00:10:43,920 --> 00:10:45,360
where the data exists there's a whole

311
00:10:45,360 --> 00:10:46,800
host of other questions which people

312
00:10:46,800 --> 00:10:48,640
have focused on on the security of the

313
00:10:48,640 --> 00:10:50,079
device itself

314
00:10:50,079 --> 00:10:52,079
where was it made what countries was it

315
00:10:52,079 --> 00:10:54,079
made in where is the software inside of

316
00:10:54,079 --> 00:10:56,000
it coming from how do you do updates and

317
00:10:56,000 --> 00:10:57,839
things like how do you do updates of

318
00:10:57,839 --> 00:11:00,240
particular concern given the breaches

319
00:11:00,240 --> 00:11:02,560
we've seen on on the supply chain side

320
00:11:02,560 --> 00:11:05,279
software updates end up being the vector

321
00:11:05,279 --> 00:11:07,200
for malicious code to enter

322
00:11:07,200 --> 00:11:09,279
and when you get to devices like that

323
00:11:09,279 --> 00:11:11,040
it's no longer just an academic

324
00:11:11,040 --> 00:11:13,120
discussion not even academic it's beyond

325
00:11:13,120 --> 00:11:14,959
a discussion about is the data secure

326
00:11:14,959 --> 00:11:16,640
it's a question as to whether the device

327
00:11:16,640 --> 00:11:18,640
is actually doing the right thing

328
00:11:18,640 --> 00:11:21,440
because there's a risk if data is

329
00:11:21,440 --> 00:11:22,480
um

330
00:11:22,480 --> 00:11:25,440
tampered with or incorrect that all of a

331
00:11:25,440 --> 00:11:26,480
sudden

332
00:11:26,480 --> 00:11:28,399
you know a device that's doing dosing

333
00:11:28,399 --> 00:11:29,519
for you

334
00:11:29,519 --> 00:11:31,360
gives you the wrong dose or sets your

335
00:11:31,360 --> 00:11:33,279
heart rate if it's a heart uh like a

336
00:11:33,279 --> 00:11:35,120
pacemaker at the wrong rate so it's

337
00:11:35,120 --> 00:11:36,959
actually the equivalent for the physical

338
00:11:36,959 --> 00:11:39,040
body of the kind of

339
00:11:39,040 --> 00:11:40,079
um

340
00:11:40,079 --> 00:11:41,360
um

341
00:11:41,360 --> 00:11:43,519
uh control systems that we've seen be of

342
00:11:43,519 --> 00:11:46,079
concern as relates to utilities

343
00:11:46,079 --> 00:11:48,160
uh water utilities electric utilities

344
00:11:48,160 --> 00:11:49,519
the shutdown of this meat processing

345
00:11:49,519 --> 00:11:51,120
plant which started on the i.t side but

346
00:11:51,120 --> 00:11:52,720
you're starting to see concern about

347
00:11:52,720 --> 00:11:55,200
control systems and then the next model

348
00:11:55,200 --> 00:11:56,480
that michelle brought up again the

349
00:11:56,480 --> 00:11:58,560
security aspects are slightly different

350
00:11:58,560 --> 00:12:01,120
if you have researchers collaborating on

351
00:12:01,120 --> 00:12:03,519
a shared data set right how do you

352
00:12:03,519 --> 00:12:04,720
ensure that the people that are

353
00:12:04,720 --> 00:12:06,320
contributing to the data set using the

354
00:12:06,320 --> 00:12:08,720
data set copying portions of it have are

355
00:12:08,720 --> 00:12:10,720
the right people with the right access

356
00:12:10,720 --> 00:12:12,880
and it really raises the potential there

357
00:12:12,880 --> 00:12:14,800
to begin to implement things that have

358
00:12:14,800 --> 00:12:17,519
been at the forefront of late um

359
00:12:17,519 --> 00:12:19,839
with i.t security zero trust

360
00:12:19,839 --> 00:12:21,519
do i know who you are

361
00:12:21,519 --> 00:12:23,600
do i know what device you're using do i

362
00:12:23,600 --> 00:12:26,079
trust those things when i make an access

363
00:12:26,079 --> 00:12:27,920
decision for you to actually access the

364
00:12:27,920 --> 00:12:28,880
data

365
00:12:28,880 --> 00:12:30,720
do i authenticate properly and does all

366
00:12:30,720 --> 00:12:32,560
the data properly encrypted so it can't

367
00:12:32,560 --> 00:12:34,560
be tampered with or if it's exfiltrated

368
00:12:34,560 --> 00:12:36,880
it does not uh create a data spell that

369
00:12:36,880 --> 00:12:38,560
might be a privacy issue

370
00:12:38,560 --> 00:12:40,560
or overall jeopardize this sort of

371
00:12:40,560 --> 00:12:42,399
research agenda so i think the security

372
00:12:42,399 --> 00:12:43,920
pieces you look at

373
00:12:43,920 --> 00:12:45,839
very much will

374
00:12:45,839 --> 00:12:47,680
ebb and flow or at least be distinct and

375
00:12:47,680 --> 00:12:49,519
different depending on which business

376
00:12:49,519 --> 00:12:51,040
model and which value chain you're

377
00:12:51,040 --> 00:12:52,480
talking about because in each of those

378
00:12:52,480 --> 00:12:54,959
instances the cloud sits in a relatively

379
00:12:54,959 --> 00:12:56,399
different place to the other business

380
00:12:56,399 --> 00:12:57,920
models

381
00:12:57,920 --> 00:13:00,160
thank you for that matt will you want to

382
00:13:00,160 --> 00:13:02,399
follow on that

383
00:13:02,399 --> 00:13:05,279
sure um so definitely from fda's

384
00:13:05,279 --> 00:13:06,720
perspective when we're looking at

385
00:13:06,720 --> 00:13:09,279
medical devices we're looking at cyber

386
00:13:09,279 --> 00:13:11,279
security through the lens of safety and

387
00:13:11,279 --> 00:13:13,360
effectiveness that's our regulatory

388
00:13:13,360 --> 00:13:15,920
mandate that's the way we evaluate

389
00:13:15,920 --> 00:13:18,880
medical devices so when we're looking at

390
00:13:18,880 --> 00:13:21,920
medical devices that are incorporating a

391
00:13:21,920 --> 00:13:24,320
cloud environment we're looking at how

392
00:13:24,320 --> 00:13:26,639
the cloud environment can impact the

393
00:13:26,639 --> 00:13:28,560
safety and effectiveness of the end

394
00:13:28,560 --> 00:13:29,920
device

395
00:13:29,920 --> 00:13:32,240
so some of that is software that lives

396
00:13:32,240 --> 00:13:34,480
in the cloud there's software as a

397
00:13:34,480 --> 00:13:36,160
medical device so some of those are

398
00:13:36,160 --> 00:13:39,279
fully hosted um in a cloud environment

399
00:13:39,279 --> 00:13:41,680
some of it are implanted devices that

400
00:13:41,680 --> 00:13:42,639
are then

401
00:13:42,639 --> 00:13:45,360
communicating with a mobile application

402
00:13:45,360 --> 00:13:47,360
and then transmitting that data into the

403
00:13:47,360 --> 00:13:50,480
cloud for further processing potentially

404
00:13:50,480 --> 00:13:52,399
figuring out treatment parameters and

405
00:13:52,399 --> 00:13:54,000
sending that back

406
00:13:54,000 --> 00:13:57,760
so there's a broad diversity of how

407
00:13:57,760 --> 00:13:58,760
cloud

408
00:13:58,760 --> 00:14:01,120
implementations can be utilized by

409
00:14:01,120 --> 00:14:04,639
medical devices and you see a lot of

410
00:14:04,639 --> 00:14:06,959
medical device manufacturers adding

411
00:14:06,959 --> 00:14:09,120
cloud components into already fielded

412
00:14:09,120 --> 00:14:12,320
devices to increase their abilities for

413
00:14:12,320 --> 00:14:15,040
remote monitoring of the device itself

414
00:14:15,040 --> 00:14:16,880
the patient's

415
00:14:16,880 --> 00:14:19,040
information that's being transmitted and

416
00:14:19,040 --> 00:14:21,120
logged through the medical device you

417
00:14:21,120 --> 00:14:23,760
see it being used for remote support of

418
00:14:23,760 --> 00:14:25,839
medical devices so troubleshooting when

419
00:14:25,839 --> 00:14:27,519
there's an issue

420
00:14:27,519 --> 00:14:29,279
you're starting to see it in terms of

421
00:14:29,279 --> 00:14:30,959
delivering

422
00:14:30,959 --> 00:14:33,360
software updates or firmware updates to

423
00:14:33,360 --> 00:14:35,760
the medical devices themselves and you

424
00:14:35,760 --> 00:14:38,000
also see it in terms of

425
00:14:38,000 --> 00:14:41,360
leveraging the added

426
00:14:41,360 --> 00:14:44,639
computation capability so being able to

427
00:14:44,639 --> 00:14:47,519
implement more complex ai and machine

428
00:14:47,519 --> 00:14:48,959
learning and also

429
00:14:48,959 --> 00:14:51,760
more computational heavy things where

430
00:14:51,760 --> 00:14:54,240
you want to leverage the power of the

431
00:14:54,240 --> 00:14:57,120
cloud environment instead of having a

432
00:14:57,120 --> 00:15:00,000
larger system bedside um so there's a

433
00:15:00,000 --> 00:15:02,320
lot of different factors and the

434
00:15:02,320 --> 00:15:04,240
implications of

435
00:15:04,240 --> 00:15:07,839
the cloud have a very diverse

436
00:15:07,839 --> 00:15:10,079
set of risks that they have based off of

437
00:15:10,079 --> 00:15:12,560
how it's implemented in the medical

438
00:15:12,560 --> 00:15:14,160
device whether the

439
00:15:14,160 --> 00:15:16,240
it was something added to the medical

440
00:15:16,240 --> 00:15:18,720
device after the fact

441
00:15:18,720 --> 00:15:20,480
a lot of people think of these cloud

442
00:15:20,480 --> 00:15:22,639
environments as essentially just someone

443
00:15:22,639 --> 00:15:25,519
else hosting a server for them

444
00:15:25,519 --> 00:15:27,519
that exists in some

445
00:15:27,519 --> 00:15:29,360
ether

446
00:15:29,360 --> 00:15:31,759
when in reality when you look at some of

447
00:15:31,759 --> 00:15:33,360
these cloud environments they're

448
00:15:33,360 --> 00:15:35,519
incredibly complex in terms of the

449
00:15:35,519 --> 00:15:37,440
different services cloud service

450
00:15:37,440 --> 00:15:39,360
providers offer

451
00:15:39,360 --> 00:15:42,480
both in terms of security services as

452
00:15:42,480 --> 00:15:43,920
well as kind of the additional

453
00:15:43,920 --> 00:15:46,240
computation so you look at the

454
00:15:46,240 --> 00:15:49,199
key management services for potentially

455
00:15:49,199 --> 00:15:51,360
being part of authentication schemes or

456
00:15:51,360 --> 00:15:54,639
providing signed software updates

457
00:15:54,639 --> 00:15:57,120
all of these different factors are

458
00:15:57,120 --> 00:15:59,360
load balancing on the front end to make

459
00:15:59,360 --> 00:16:01,680
sure that your environment's protected

460
00:16:01,680 --> 00:16:03,759
all of these things have

461
00:16:03,759 --> 00:16:06,720
a lot of complexity and a lot of weight

462
00:16:06,720 --> 00:16:08,720
that needs to be considered so as you

463
00:16:08,720 --> 00:16:10,720
see this

464
00:16:10,720 --> 00:16:12,720
advent of

465
00:16:12,720 --> 00:16:15,120
new startups migrating into the cloud

466
00:16:15,120 --> 00:16:17,440
it's a very complex environment to

467
00:16:17,440 --> 00:16:19,199
actually

468
00:16:19,199 --> 00:16:23,440
integrate into a medical device platform

469
00:16:23,440 --> 00:16:25,199
i love the fact that you really you

470
00:16:25,199 --> 00:16:26,399
brought it home for me on a personal

471
00:16:26,399 --> 00:16:28,320
level i have two or i had two family

472
00:16:28,320 --> 00:16:29,920
members who were both dependent on

473
00:16:29,920 --> 00:16:32,720
medical device implants and so it was

474
00:16:32,720 --> 00:16:34,639
really interesting because this was uh

475
00:16:34,639 --> 00:16:36,079
you know unfortunately at a time where i

476
00:16:36,079 --> 00:16:37,759
think we didn't have the advancement or

477
00:16:37,759 --> 00:16:39,279
the computational power or even the

478
00:16:39,279 --> 00:16:41,279
security you know that we have now but

479
00:16:41,279 --> 00:16:43,360
you know their their lives were very

480
00:16:43,360 --> 00:16:44,800
dependent on those and i think you know

481
00:16:44,800 --> 00:16:46,959
everyone's really brought up why this is

482
00:16:46,959 --> 00:16:48,959
an exciting time to be moving at warp

483
00:16:48,959 --> 00:16:51,279
speed but but with that level of

484
00:16:51,279 --> 00:16:52,959
carefulness um

485
00:16:52,959 --> 00:16:54,720
you know andrea over to you you know

486
00:16:54,720 --> 00:16:56,320
let's talk about that let's talk about

487
00:16:56,320 --> 00:16:58,880
law ip what are the legal interests that

488
00:16:58,880 --> 00:17:01,600
exist in databases with pii sets what

489
00:17:01,600 --> 00:17:03,440
are the types of concerns people you

490
00:17:03,440 --> 00:17:04,959
know building and caring for these

491
00:17:04,959 --> 00:17:06,720
systems should think about and and what

492
00:17:06,720 --> 00:17:09,439
might keep them up at night

493
00:17:09,439 --> 00:17:12,959
well that's a really uh interesting and

494
00:17:12,959 --> 00:17:14,319
uh

495
00:17:14,319 --> 00:17:16,319
complicated question that will take

496
00:17:16,319 --> 00:17:18,880
decades to resolve for better or worse

497
00:17:18,880 --> 00:17:22,079
in minutia but the short version is that

498
00:17:22,079 --> 00:17:24,400
there is established law around what

499
00:17:24,400 --> 00:17:26,880
constitutes for example uh a

500
00:17:26,880 --> 00:17:29,039
copyrightable interest in in some of

501
00:17:29,039 --> 00:17:32,320
these uh kinds of situations so um let

502
00:17:32,320 --> 00:17:34,880
me just give kind of a little quick uh

503
00:17:34,880 --> 00:17:37,520
intro through the lens of uh medical

504
00:17:37,520 --> 00:17:39,600
devices so increasingly we're moving

505
00:17:39,600 --> 00:17:41,440
toward the world that you've already

506
00:17:41,440 --> 00:17:43,200
heard about i call it the internet of

507
00:17:43,200 --> 00:17:45,840
bodies the idea of devices not only

508
00:17:45,840 --> 00:17:48,000
being attached to human bodies but also

509
00:17:48,000 --> 00:17:51,440
implanted in human bodies or in the

510
00:17:51,440 --> 00:17:53,120
third generation

511
00:17:53,120 --> 00:17:55,440
talking to the cloud so right on point

512
00:17:55,440 --> 00:17:58,400
with this panel with um

513
00:17:58,400 --> 00:18:00,720
potentially live feeds that's the mental

514
00:18:00,720 --> 00:18:03,120
model that no pun intended uh some

515
00:18:03,120 --> 00:18:04,799
companies in the valley are using in

516
00:18:04,799 --> 00:18:06,000
building their

517
00:18:06,000 --> 00:18:08,559
uh recreational augmentation brain

518
00:18:08,559 --> 00:18:10,480
implants or that you see in medical

519
00:18:10,480 --> 00:18:12,960
contexts already in use and treatment of

520
00:18:12,960 --> 00:18:15,200
certain disease diseases

521
00:18:15,200 --> 00:18:18,559
or conditions such as parkinson's or

522
00:18:18,559 --> 00:18:20,799
interesting experiments underway

523
00:18:20,799 --> 00:18:22,880
to alleviate

524
00:18:22,880 --> 00:18:25,600
different kinds of other

525
00:18:25,600 --> 00:18:26,840
brain

526
00:18:26,840 --> 00:18:29,200
connected uh challenges that patients

527
00:18:29,200 --> 00:18:30,480
have with

528
00:18:30,480 --> 00:18:32,640
live data streams that are of course you

529
00:18:32,640 --> 00:18:34,720
know footnote reliant on internet access

530
00:18:34,720 --> 00:18:36,240
reliability among other things but

531
00:18:36,240 --> 00:18:38,160
that's a panel for another day

532
00:18:38,160 --> 00:18:40,799
so when we look at these databases

533
00:18:40,799 --> 00:18:42,400
whether they are

534
00:18:42,400 --> 00:18:44,799
local or remote and the actual

535
00:18:44,799 --> 00:18:47,200
information contained in these databases

536
00:18:47,200 --> 00:18:49,600
the way that a court would probably

537
00:18:49,600 --> 00:18:52,880
analyze these things as a first instance

538
00:18:52,880 --> 00:18:54,559
and this is where the law is going to

539
00:18:54,559 --> 00:18:57,520
develop definitely starts with the

540
00:18:57,520 --> 00:19:00,400
question of exactly what is protectable

541
00:19:00,400 --> 00:19:02,880
in these aggregations as a first matter

542
00:19:02,880 --> 00:19:06,000
so to answer that question the idea of

543
00:19:06,000 --> 00:19:07,840
copyright is that there is

544
00:19:07,840 --> 00:19:10,400
an idea embedded in a tangible medium so

545
00:19:10,400 --> 00:19:13,679
is code a tangible medium yes it is but

546
00:19:13,679 --> 00:19:16,160
the actual information starts to get a

547
00:19:16,160 --> 00:19:17,840
little dicey so

548
00:19:17,840 --> 00:19:20,559
facts themselves are not copyrightable

549
00:19:20,559 --> 00:19:23,679
so what you can copyright is potentially

550
00:19:23,679 --> 00:19:26,080
a creative arrangement but what does it

551
00:19:26,080 --> 00:19:28,480
mean to be creative enough well the

552
00:19:28,480 --> 00:19:30,480
leading case on point is one called

553
00:19:30,480 --> 00:19:32,160
feist publications ancient rural

554
00:19:32,160 --> 00:19:33,840
telephone and what it told us there is

555
00:19:33,840 --> 00:19:36,080
that the white pages for example even

556
00:19:36,080 --> 00:19:38,000
though it took a lot of effort to

557
00:19:38,000 --> 00:19:40,559
aggregate all of that information that

558
00:19:40,559 --> 00:19:43,440
extreme sweat of the brow that's not

559
00:19:43,440 --> 00:19:46,400
enough to by default out of the box give

560
00:19:46,400 --> 00:19:48,880
you a creative arrangement so

561
00:19:48,880 --> 00:19:51,679
when we look at you know say

562
00:19:51,679 --> 00:19:53,440
dropping all this information into a

563
00:19:53,440 --> 00:19:56,160
spreadsheet or into a searchable

564
00:19:56,160 --> 00:19:58,880
database that's probably in itself i

565
00:19:58,880 --> 00:20:02,080
would guess not going to be enough for

566
00:20:02,080 --> 00:20:03,760
courts to find that there's a

567
00:20:03,760 --> 00:20:05,280
protectable interest just in the

568
00:20:05,280 --> 00:20:07,520
information now as we start to talk

569
00:20:07,520 --> 00:20:09,600
about derived information with each

570
00:20:09,600 --> 00:20:11,840
subsequent layer of derivation i think

571
00:20:11,840 --> 00:20:14,080
arguably arguably you have a slightly

572
00:20:14,080 --> 00:20:16,159
stronger case for copyrightability of

573
00:20:16,159 --> 00:20:18,960
those derived databases that are then

574
00:20:18,960 --> 00:20:20,480
creatively arranged

575
00:20:20,480 --> 00:20:23,840
uh and used in those ways um but the

576
00:20:23,840 --> 00:20:26,080
other piece of this that is that i still

577
00:20:26,080 --> 00:20:28,960
see done wrong all the time in uh

578
00:20:28,960 --> 00:20:31,360
legal contexts even by otherwise

579
00:20:31,360 --> 00:20:33,200
sophisticated lawyers is that the

580
00:20:33,200 --> 00:20:36,240
transfer of use and the licenses of

581
00:20:36,240 --> 00:20:40,159
these copyrights needs to be written in

582
00:20:40,159 --> 00:20:42,720
a very particular way so there was a

583
00:20:42,720 --> 00:20:45,039
case uh called new york times v tasini

584
00:20:45,039 --> 00:20:48,159
that talked about the fact that the uh

585
00:20:48,159 --> 00:20:50,799
bundle of rights that are comprised

586
00:20:50,799 --> 00:20:53,760
within the notion of copyright have a

587
00:20:53,760 --> 00:20:55,840
special digital component and so for

588
00:20:55,840 --> 00:20:57,840
example the digital rights to use need

589
00:20:57,840 --> 00:21:01,039
to be transferred separately in any kind

590
00:21:01,039 --> 00:21:02,960
of copyright assignment and that's

591
00:21:02,960 --> 00:21:04,480
something that you know it's a little

592
00:21:04,480 --> 00:21:05,840
drafting point but people still get

593
00:21:05,840 --> 00:21:08,640
wrong all the time um so you'll have

594
00:21:08,640 --> 00:21:10,640
this battle over what is copyrightable

595
00:21:10,640 --> 00:21:12,159
and then you'll have a battle over

596
00:21:12,159 --> 00:21:14,480
whether the right to use has been

597
00:21:14,480 --> 00:21:16,960
appropriately granted and when we look

598
00:21:16,960 --> 00:21:18,720
at whether that right to use has been

599
00:21:18,720 --> 00:21:20,159
appropriately granted we will

600
00:21:20,159 --> 00:21:22,559
increasingly see battles over dignitary

601
00:21:22,559 --> 00:21:25,280
interests particularly in a world where

602
00:21:25,280 --> 00:21:27,919
end user license agreements are longer

603
00:21:27,919 --> 00:21:30,799
than war and peace see a fun project

604
00:21:30,799 --> 00:21:33,039
called eulas of despair that my penn

605
00:21:33,039 --> 00:21:34,880
state pilot lab um is currently

606
00:21:34,880 --> 00:21:37,679
finishing up um that creates lovely

607
00:21:37,679 --> 00:21:40,240
visual models of many of the uh

608
00:21:40,240 --> 00:21:41,280
uh

609
00:21:41,280 --> 00:21:44,640
most um shall we say expansive eula's on

610
00:21:44,640 --> 00:21:45,840
the internet

611
00:21:45,840 --> 00:21:48,960
um so when you get into the world where

612
00:21:48,960 --> 00:21:50,960
the licensing is predicated based on

613
00:21:50,960 --> 00:21:53,039
this fiction that people understand what

614
00:21:53,039 --> 00:21:54,960
they're agreeing to and then you have

615
00:21:54,960 --> 00:21:57,120
maximum repurposing of this information

616
00:21:57,120 --> 00:21:58,559
in ways that are not necessarily

617
00:21:58,559 --> 00:22:01,280
foreseeable to the person clicking yes

618
00:22:01,280 --> 00:22:03,039
and you have no proof that anyone's

619
00:22:03,039 --> 00:22:04,880
actually read it and it's longer than

620
00:22:04,880 --> 00:22:07,919
war and peace and and and we start to

621
00:22:07,919 --> 00:22:11,200
get into a situation where the law is

622
00:22:11,200 --> 00:22:13,520
gonna say okay brass tax

623
00:22:13,520 --> 00:22:15,360
nobody's reading this nobody understands

624
00:22:15,360 --> 00:22:17,520
what's going on nobody's meaningfully

625
00:22:17,520 --> 00:22:20,400
consenting to this and particularly in a

626
00:22:20,400 --> 00:22:23,360
medical device context and in a world

627
00:22:23,360 --> 00:22:25,440
where there's health-ish data being

628
00:22:25,440 --> 00:22:28,480
processed in the cloud you start to have

629
00:22:28,480 --> 00:22:30,720
real concerns that we're moving in those

630
00:22:30,720 --> 00:22:33,280
instances potentially to a world where

631
00:22:33,280 --> 00:22:34,880
we border on

632
00:22:34,880 --> 00:22:38,000
uh maybe doing less to help people and

633
00:22:38,000 --> 00:22:40,559
maybe sometimes doing more to

634
00:22:40,559 --> 00:22:42,559
put them at risk depending on what the

635
00:22:42,559 --> 00:22:44,640
terms are and how things play out and i

636
00:22:44,640 --> 00:22:47,120
should uh caveat all of this which i

637
00:22:47,120 --> 00:22:48,480
should do at the beginning that all of

638
00:22:48,480 --> 00:22:50,720
these opinions are mine and do not

639
00:22:50,720 --> 00:22:53,600
reflect any federal agencies with which

640
00:22:53,600 --> 00:22:56,159
i work um there are multiple agencies

641
00:22:56,159 --> 00:22:57,440
none of them have approved these

642
00:22:57,440 --> 00:22:59,600
comments this is all me being law

643
00:22:59,600 --> 00:23:01,440
professor engineering professor starting

644
00:23:01,440 --> 00:23:02,559
off okay

645
00:23:02,559 --> 00:23:05,039
um so that's the lay of the land and

646
00:23:05,039 --> 00:23:06,880
particularly as we get into a situation

647
00:23:06,880 --> 00:23:09,440
where there are multi-function devices

648
00:23:09,440 --> 00:23:11,440
where there are

649
00:23:11,440 --> 00:23:14,320
opportunities for companies to create

650
00:23:14,320 --> 00:23:16,240
devices that are

651
00:23:16,240 --> 00:23:17,679
medical

652
00:23:17,679 --> 00:23:20,799
in one use but not medical in another

653
00:23:20,799 --> 00:23:22,720
use or they claim it's not medical so

654
00:23:22,720 --> 00:23:25,440
like let's imagine a world where you're

655
00:23:25,440 --> 00:23:27,760
please don't let this device exist where

656
00:23:27,760 --> 00:23:30,559
your artificial pancreas also streams

657
00:23:30,559 --> 00:23:32,880
music to your ears

658
00:23:32,880 --> 00:23:33,840
okay

659
00:23:33,840 --> 00:23:36,640
one is clearly a medical use the other

660
00:23:36,640 --> 00:23:37,760
one

661
00:23:37,760 --> 00:23:40,720
is not a medical use

662
00:23:40,720 --> 00:23:43,039
and one that i really hope nobody has

663
00:23:43,039 --> 00:23:45,120
thought of so please do not make this

664
00:23:45,120 --> 00:23:47,520
device

665
00:23:48,320 --> 00:23:50,080
please don't please don't make this

666
00:23:50,080 --> 00:23:51,360
device but anyway so that's a

667
00:23:51,360 --> 00:23:53,440
non-medical use so we have their uh

668
00:23:53,440 --> 00:23:55,520
really dumb example of a multi-function

669
00:23:55,520 --> 00:23:57,360
device but in that world you have

670
00:23:57,360 --> 00:23:59,360
multiple kinds of data streams being

671
00:23:59,360 --> 00:24:01,200
aggregated together by a single provider

672
00:24:01,200 --> 00:24:03,200
potentially repurposed in various

673
00:24:03,200 --> 00:24:06,000
different ways and then when you have

674
00:24:06,000 --> 00:24:08,799
database mergers across fields of

675
00:24:08,799 --> 00:24:11,600
activity you start to see healthish data

676
00:24:11,600 --> 00:24:14,480
showing up in credit report-ish

677
00:24:14,480 --> 00:24:15,760
information

678
00:24:15,760 --> 00:24:18,960
and this notion of a social credit

679
00:24:18,960 --> 00:24:21,760
monitored society that's connected to

680
00:24:21,760 --> 00:24:24,720
devices embedded inside our bodies

681
00:24:24,720 --> 00:24:26,640
that's not

682
00:24:26,640 --> 00:24:28,799
not the best world we can build i'll

683
00:24:28,799 --> 00:24:30,960
leave it there and um we'll come back to

684
00:24:30,960 --> 00:24:32,960
this hopefully later on

685
00:24:32,960 --> 00:24:34,400
now i appreciate that and actually to

686
00:24:34,400 --> 00:24:36,400
your point my uh my family member

687
00:24:36,400 --> 00:24:38,159
actually was suffering with parkinson's

688
00:24:38,159 --> 00:24:40,080
and so the implant was and i remember

689
00:24:40,080 --> 00:24:42,080
very explicitly a long list of things

690
00:24:42,080 --> 00:24:44,960
for us to sign as as as they were going

691
00:24:44,960 --> 00:24:47,200
under that surgery and and to your point

692
00:24:47,200 --> 00:24:48,880
we had no idea we signed we just had

693
00:24:48,880 --> 00:24:51,039
hope right and so i do think it's really

694
00:24:51,039 --> 00:24:53,440
you know really critical dan over to you

695
00:24:53,440 --> 00:24:55,039
i think one of the things you know you

696
00:24:55,039 --> 00:24:57,279
kind of wet our appetite around you know

697
00:24:57,279 --> 00:24:58,559
a lot of the different elements of

698
00:24:58,559 --> 00:25:00,320
security but you know since we're on

699
00:25:00,320 --> 00:25:02,159
devices let's talk about the device as

700
00:25:02,159 --> 00:25:04,000
an element of the value chain that

701
00:25:04,000 --> 00:25:06,880
includes data source storage components

702
00:25:06,880 --> 00:25:09,039
what do we need to be intentional about

703
00:25:09,039 --> 00:25:11,440
when we think about the value chain and

704
00:25:11,440 --> 00:25:14,240
the role of devices

705
00:25:14,240 --> 00:25:15,360
you know i

706
00:25:15,360 --> 00:25:18,400
i think you need to be intentional about

707
00:25:18,400 --> 00:25:19,520
sort of the

708
00:25:19,520 --> 00:25:22,000
aspects of the security life side life

709
00:25:22,000 --> 00:25:24,720
cycle excuse me um we talked about zero

710
00:25:24,720 --> 00:25:26,320
trust and that really is how do you

711
00:25:26,320 --> 00:25:27,520
protect

712
00:25:27,520 --> 00:25:30,320
the device and protect the data

713
00:25:30,320 --> 00:25:31,919
right and that occurs through all the

714
00:25:31,919 --> 00:25:33,679
things we talked about in passing at the

715
00:25:33,679 --> 00:25:35,360
beginning which is basically

716
00:25:35,360 --> 00:25:37,279
strong identity multi-factor

717
00:25:37,279 --> 00:25:39,120
authentication

718
00:25:39,120 --> 00:25:40,799
role-based access

719
00:25:40,799 --> 00:25:42,480
not just role-based access for the

720
00:25:42,480 --> 00:25:44,240
person but their machine it is the

721
00:25:44,240 --> 00:25:46,400
device in good state what's the device

722
00:25:46,400 --> 00:25:47,760
posture

723
00:25:47,760 --> 00:25:49,279
then there's the encryption of the data

724
00:25:49,279 --> 00:25:50,320
itself

725
00:25:50,320 --> 00:25:53,679
um we also layer things on top uh like

726
00:25:53,679 --> 00:25:56,799
sort of default data loss prevention so

727
00:25:56,799 --> 00:25:58,799
if for example a device or a cloud

728
00:25:58,799 --> 00:26:01,120
environment is leaking sensitive data

729
00:26:01,120 --> 00:26:04,480
that's something we could notice quickly

730
00:26:04,480 --> 00:26:06,799
and and automate the blockage of it or

731
00:26:06,799 --> 00:26:08,320
the masking of it

732
00:26:08,320 --> 00:26:11,200
um and in addition you know for stuff

733
00:26:11,200 --> 00:26:13,840
that sits in google cloud i think i i

734
00:26:13,840 --> 00:26:15,520
would want people to have confidence in

735
00:26:15,520 --> 00:26:17,600
the fact that the infrastructure

736
00:26:17,600 --> 00:26:19,760
we've built it's a proprietary global

737
00:26:19,760 --> 00:26:22,480
network we're very attuned to

738
00:26:22,480 --> 00:26:24,720
um how we manage our own hardware and

739
00:26:24,720 --> 00:26:26,960
software supply chain

740
00:26:26,960 --> 00:26:29,440
we build a lot of our own infrastructure

741
00:26:29,440 --> 00:26:32,400
we lay our own undersea cables and so it

742
00:26:32,400 --> 00:26:34,799
sits all those things i mentioned

743
00:26:34,799 --> 00:26:37,360
strong identity encryption uh proper

744
00:26:37,360 --> 00:26:39,360
authorization and access sits on top of

745
00:26:39,360 --> 00:26:41,919
a strong global infrastructure so that's

746
00:26:41,919 --> 00:26:43,520
the protect piece

747
00:26:43,520 --> 00:26:46,880
i think security in the cloud also gives

748
00:26:46,880 --> 00:26:48,000
one

749
00:26:48,000 --> 00:26:49,760
other capabilities that are going to be

750
00:26:49,760 --> 00:26:52,559
distinct from traditional approaches

751
00:26:52,559 --> 00:26:54,880
you know decades-old approaches to i.t

752
00:26:54,880 --> 00:26:56,480
security around a traditional data

753
00:26:56,480 --> 00:26:58,640
center and one of them is really scale

754
00:26:58,640 --> 00:27:00,320
and analytics if you think about the

755
00:27:00,320 --> 00:27:03,039
proliferation of medical devices and

756
00:27:03,039 --> 00:27:05,279
other iot devices that are throwing off

757
00:27:05,279 --> 00:27:07,279
massive amounts of data

758
00:27:07,279 --> 00:27:09,679
for the most part a lot of organizations

759
00:27:09,679 --> 00:27:12,799
historically have not done a good job

760
00:27:12,799 --> 00:27:14,720
capturing storing

761
00:27:14,720 --> 00:27:16,880
analyzing getting insights and making

762
00:27:16,880 --> 00:27:19,279
decisions on all the

763
00:27:19,279 --> 00:27:21,200
machine data that their organizations

764
00:27:21,200 --> 00:27:23,919
are throwing off but with cloud it

765
00:27:23,919 --> 00:27:25,919
allows you really massive scale

766
00:27:25,919 --> 00:27:28,640
analytics to get end-to-end visibility

767
00:27:28,640 --> 00:27:31,360
um into machine data and so for example

768
00:27:31,360 --> 00:27:32,880
if you're a healthcare provider that's

769
00:27:32,880 --> 00:27:35,919
managing a deployed

770
00:27:35,919 --> 00:27:38,080
you know market of hundreds of thousands

771
00:27:38,080 --> 00:27:40,880
of deployed glucose meters

772
00:27:40,880 --> 00:27:43,200
heart monitors whatever it is

773
00:27:43,200 --> 00:27:45,039
you want to be able to

774
00:27:45,039 --> 00:27:47,679
see patterns in that data to interrogate

775
00:27:47,679 --> 00:27:50,720
that data identify anomalies

776
00:27:50,720 --> 00:27:53,600
look for trends and also proactively

777
00:27:53,600 --> 00:27:55,600
hunt in that data to see if there are

778
00:27:55,600 --> 00:27:57,840
bad actors or malicious code and a lot

779
00:27:57,840 --> 00:27:58,799
of that

780
00:27:58,799 --> 00:28:00,240
is beyond

781
00:28:00,240 --> 00:28:02,320
the normal capability of a traditional

782
00:28:02,320 --> 00:28:04,640
security workforce which tends to have

783
00:28:04,640 --> 00:28:06,640
an over proliferation of cyber security

784
00:28:06,640 --> 00:28:09,120
tools and not enough people so in those

785
00:28:09,120 --> 00:28:11,360
cases there's a flood of data

786
00:28:11,360 --> 00:28:13,760
in fact the tsunami of data but that

787
00:28:13,760 --> 00:28:15,520
more data doesn't actually make you

788
00:28:15,520 --> 00:28:17,120
better at doing security it actually

789
00:28:17,120 --> 00:28:18,799
makes you worse at doing security

790
00:28:18,799 --> 00:28:21,360
because it obfuscates things right so to

791
00:28:21,360 --> 00:28:22,640
counter that

792
00:28:22,640 --> 00:28:24,399
you have to be able to have that massive

793
00:28:24,399 --> 00:28:27,120
scale analytics to proactively look for

794
00:28:27,120 --> 00:28:29,679
bad things to baseline activities to

795
00:28:29,679 --> 00:28:32,159
look for things that are abnormal um and

796
00:28:32,159 --> 00:28:33,919
so the data actually becomes your friend

797
00:28:33,919 --> 00:28:35,440
and i think it's hard to do that with

798
00:28:35,440 --> 00:28:37,360
in-house data analytics platforms i

799
00:28:37,360 --> 00:28:39,520
think cloud capabilities that scale up

800
00:28:39,520 --> 00:28:41,520
scale down and can really make storage

801
00:28:41,520 --> 00:28:44,240
and analytics much more cost effective

802
00:28:44,240 --> 00:28:46,320
and democratize that kind of analytics

803
00:28:46,320 --> 00:28:47,919
is one of the things on the security

804
00:28:47,919 --> 00:28:49,919
side from a visibility standpoint that

805
00:28:49,919 --> 00:28:51,679
the cloud does for you

806
00:28:51,679 --> 00:28:53,039
i think the other piece and that's sort

807
00:28:53,039 --> 00:28:54,480
of the detect piece so we've gone over

808
00:28:54,480 --> 00:28:56,240
protect i've just walked through sort of

809
00:28:56,240 --> 00:28:57,440
the detect piece and then on the

810
00:28:57,440 --> 00:28:58,960
recovery side

811
00:28:58,960 --> 00:29:00,880
whether it's electronic health records

812
00:29:00,880 --> 00:29:03,360
or sort of you know a couple years of

813
00:29:03,360 --> 00:29:05,200
data getting thrown off by these medical

814
00:29:05,200 --> 00:29:08,399
devices or a massive research database

815
00:29:08,399 --> 00:29:10,840
we've seen a massive uptick in

816
00:29:10,840 --> 00:29:14,240
ransomware um and so there's the risk in

817
00:29:14,240 --> 00:29:16,080
general that people are afraid that that

818
00:29:16,080 --> 00:29:18,640
data is going to get um

819
00:29:18,640 --> 00:29:20,320
you know forcibly encrypted and they

820
00:29:20,320 --> 00:29:22,559
can't access it or can't go back to a

821
00:29:22,559 --> 00:29:24,840
sort of valid and known

822
00:29:24,840 --> 00:29:27,600
um accurate restore point and i think

823
00:29:27,600 --> 00:29:29,600
that that's another area where the cloud

824
00:29:29,600 --> 00:29:31,120
can help you really doing

825
00:29:31,120 --> 00:29:33,760
enterprise-wide data backup so that if

826
00:29:33,760 --> 00:29:35,120
that happens and you end up in a

827
00:29:35,120 --> 00:29:36,480
ransomware attack you don't have to pay

828
00:29:36,480 --> 00:29:38,480
the ransom because you're confident that

829
00:29:38,480 --> 00:29:40,480
you have an overarching data strategy

830
00:29:40,480 --> 00:29:42,159
that is tuned with an overarching

831
00:29:42,159 --> 00:29:44,320
security strategy so that you know and

832
00:29:44,320 --> 00:29:46,720
are confident that you can go back to

833
00:29:46,720 --> 00:29:48,880
secure points in time

834
00:29:48,880 --> 00:29:51,039
of dif earlier versions of your data

835
00:29:51,039 --> 00:29:52,960
that are clean from malware and haven't

836
00:29:52,960 --> 00:29:55,360
been tampered with it's basically strong

837
00:29:55,360 --> 00:29:57,440
and confident secure points so again

838
00:29:57,440 --> 00:29:59,520
i've sort of hit the protect

839
00:29:59,520 --> 00:30:01,520
detect and then that's the recover piece

840
00:30:01,520 --> 00:30:03,200
and i think the cloud helps on all those

841
00:30:03,200 --> 00:30:04,399
fronts

842
00:30:04,399 --> 00:30:05,440
dan i love that you're really

843
00:30:05,440 --> 00:30:07,200
introducing that that concept that we

844
00:30:07,200 --> 00:30:09,360
have to be uh we have to be more ready

845
00:30:09,360 --> 00:30:11,200
for more things to happen in the world

846
00:30:11,200 --> 00:30:12,640
right and we have to have kind of some

847
00:30:12,640 --> 00:30:14,960
of those break glass or from a military

848
00:30:14,960 --> 00:30:17,120
parlance kind of go bag right not just

849
00:30:17,120 --> 00:30:19,039
strategies but tactics to be able to

850
00:30:19,039 --> 00:30:21,679
respond and to not be in those positions

851
00:30:21,679 --> 00:30:23,440
i'm actually going to jump matt to you

852
00:30:23,440 --> 00:30:24,960
because i really want to continue this

853
00:30:24,960 --> 00:30:27,200
idea of you know the fact that there has

854
00:30:27,200 --> 00:30:29,520
been you know recent uh

855
00:30:29,520 --> 00:30:31,279
you know recent malware attacks and

856
00:30:31,279 --> 00:30:34,080
ransomware and things like that you know

857
00:30:34,080 --> 00:30:36,399
talk to me a little bit about you know

858
00:30:36,399 --> 00:30:38,399
what what that looks like what do you

859
00:30:38,399 --> 00:30:40,320
think about that from kind of an fda

860
00:30:40,320 --> 00:30:41,679
standpoint and then michelle i'm going

861
00:30:41,679 --> 00:30:43,520
to uh come to you and andrea i'm going

862
00:30:43,520 --> 00:30:46,080
to come to you for a little bit of a

863
00:30:46,080 --> 00:30:48,480
prod more into the idea of kind of that

864
00:30:48,480 --> 00:30:50,880
personal uh the personal impact of a lot

865
00:30:50,880 --> 00:30:52,559
of this but but matt talk to me a little

866
00:30:52,559 --> 00:30:54,559
bit about what what dan said and and

867
00:30:54,559 --> 00:30:56,720
also maybe a little bit about how how

868
00:30:56,720 --> 00:30:59,120
people make decisions around what cloud

869
00:30:59,120 --> 00:31:00,399
uh you know

870
00:31:00,399 --> 00:31:03,039
and and how they go about that

871
00:31:03,039 --> 00:31:06,240
sure um so definitely from

872
00:31:06,240 --> 00:31:08,720
the device standpoint as we start to see

873
00:31:08,720 --> 00:31:12,159
more reliance on cloud-based solutions

874
00:31:12,159 --> 00:31:15,679
and also just networking for that matter

875
00:31:15,679 --> 00:31:17,519
you have a lot more considerations

876
00:31:17,519 --> 00:31:19,600
around device availability so if there

877
00:31:19,600 --> 00:31:22,640
is a part of the device

878
00:31:22,640 --> 00:31:24,960
software that's operating solely in the

879
00:31:24,960 --> 00:31:27,120
cloud you need to start thinking about

880
00:31:27,120 --> 00:31:29,360
what happens with the device when that

881
00:31:29,360 --> 00:31:32,240
becomes unavailable um the different

882
00:31:32,240 --> 00:31:34,080
cloud service providers

883
00:31:34,080 --> 00:31:37,600
indicate that they have like 99 point

884
00:31:37,600 --> 00:31:40,880
whatever percent uptime um but as we

885
00:31:40,880 --> 00:31:43,279
talked about before with the complexity

886
00:31:43,279 --> 00:31:46,080
of those environments that's more of the

887
00:31:46,080 --> 00:31:49,440
face value up time not necessarily every

888
00:31:49,440 --> 00:31:52,159
crook and cranny that the device may be

889
00:31:52,159 --> 00:31:55,279
reliant upon in order to perform its

890
00:31:55,279 --> 00:31:57,120
intended use

891
00:31:57,120 --> 00:31:59,679
and deliver the therapy or perform the

892
00:31:59,679 --> 00:32:02,320
function that it's intended to do

893
00:32:02,320 --> 00:32:04,080
so when we look at things where

894
00:32:04,080 --> 00:32:06,880
ransomware in a cloud instance can

895
00:32:06,880 --> 00:32:08,399
impact the

896
00:32:08,399 --> 00:32:12,399
availability of a treatment device you

897
00:32:12,399 --> 00:32:14,159
start getting into real patient

898
00:32:14,159 --> 00:32:18,320
implications of delays of care um where

899
00:32:18,320 --> 00:32:19,360
that

900
00:32:19,360 --> 00:32:22,080
reliance on that cloud-based data and

901
00:32:22,080 --> 00:32:24,399
that cloud-based computation

902
00:32:24,399 --> 00:32:25,200
um

903
00:32:25,200 --> 00:32:27,120
when that's interrupted whether that

904
00:32:27,120 --> 00:32:29,440
comes from something in the cloud itself

905
00:32:29,440 --> 00:32:31,120
whether that comes from

906
00:32:31,120 --> 00:32:33,519
the hospital being impacted by

907
00:32:33,519 --> 00:32:35,519
ransomware and them severing their

908
00:32:35,519 --> 00:32:38,000
outward connections there's a lot of

909
00:32:38,000 --> 00:32:39,840
different considerations for these

910
00:32:39,840 --> 00:32:42,399
complex systems of systems

911
00:32:42,399 --> 00:32:44,720
medical devices where you have a lot of

912
00:32:44,720 --> 00:32:46,480
these reliances

913
00:32:46,480 --> 00:32:48,399
um where you need to start thinking

914
00:32:48,399 --> 00:32:50,960
about how the end device

915
00:32:50,960 --> 00:32:53,120
that's bedside with the patient that's

916
00:32:53,120 --> 00:32:55,360
implanted in the patient are going to

917
00:32:55,360 --> 00:32:57,360
respond to those scenarios so the

918
00:32:57,360 --> 00:32:59,440
incident response both on the

919
00:32:59,440 --> 00:33:01,440
manufacturer's side of how they're going

920
00:33:01,440 --> 00:33:04,559
to respond and address those situations

921
00:33:04,559 --> 00:33:07,279
as well as what those manufacturers are

922
00:33:07,279 --> 00:33:09,600
telling their users of what to do in the

923
00:33:09,600 --> 00:33:11,840
event and what backup or

924
00:33:11,840 --> 00:33:14,240
fail-safe capabilities the device will

925
00:33:14,240 --> 00:33:17,120
have when that extended function is no

926
00:33:17,120 --> 00:33:19,440
longer available is something that needs

927
00:33:19,440 --> 00:33:22,399
to be closely considered

928
00:33:22,399 --> 00:33:24,480
to your other question around

929
00:33:24,480 --> 00:33:27,760
how cloud service providers are selected

930
00:33:27,760 --> 00:33:30,240
um one common thing that we seem to be

931
00:33:30,240 --> 00:33:33,519
seeing a lot um is that the decision

932
00:33:33,519 --> 00:33:35,360
around which cloud service provider to

933
00:33:35,360 --> 00:33:38,960
use is being made at the c seat level um

934
00:33:38,960 --> 00:33:41,279
these decisions are being made of we

935
00:33:41,279 --> 00:33:42,320
want to

936
00:33:42,320 --> 00:33:44,720
use the technology that's

937
00:33:44,720 --> 00:33:46,880
grabby that everyone's using we want to

938
00:33:46,880 --> 00:33:51,600
use a cloud instance with our devices or

939
00:33:51,600 --> 00:33:53,760
we're going to use this one based off of

940
00:33:53,760 --> 00:33:56,960
some xyz factor so it's being decided by

941
00:33:56,960 --> 00:33:59,360
the c-suite whereas in traditional

942
00:33:59,360 --> 00:34:01,440
medical device development

943
00:34:01,440 --> 00:34:02,799
the

944
00:34:02,799 --> 00:34:05,279
device is set up you make your set of

945
00:34:05,279 --> 00:34:08,399
requirements you're identifying how to

946
00:34:08,399 --> 00:34:10,560
address those requirements with your

947
00:34:10,560 --> 00:34:13,359
device design whether that's using

948
00:34:13,359 --> 00:34:16,960
um software or an architecture that the

949
00:34:16,960 --> 00:34:19,040
manufacturer themselves provide or

950
00:34:19,040 --> 00:34:21,599
whether they go to a third party so much

951
00:34:21,599 --> 00:34:23,440
like using commercial off-the-shelf

952
00:34:23,440 --> 00:34:26,159
software in a medical device using a

953
00:34:26,159 --> 00:34:28,159
cloud service provider is really no

954
00:34:28,159 --> 00:34:30,239
different from fda's perspective in

955
00:34:30,239 --> 00:34:33,359
terms of the response and responsibility

956
00:34:33,359 --> 00:34:35,119
resting on the medical device

957
00:34:35,119 --> 00:34:37,280
manufacturer for what risks those are

958
00:34:37,280 --> 00:34:39,359
imposing on the system

959
00:34:39,359 --> 00:34:41,918
so when you have kind of this

960
00:34:41,918 --> 00:34:43,359
backwards

961
00:34:43,359 --> 00:34:45,280
driven

962
00:34:45,280 --> 00:34:48,960
kind of the ends justifying the means of

963
00:34:48,960 --> 00:34:50,800
you're being told which cloud service

964
00:34:50,800 --> 00:34:54,159
provider to use you're then trying to

965
00:34:54,159 --> 00:34:57,599
back architect the device to address

966
00:34:57,599 --> 00:34:59,680
whichever cloud instance you have

967
00:34:59,680 --> 00:35:01,520
because there's differences among all of

968
00:35:01,520 --> 00:35:03,359
them there's different

969
00:35:03,359 --> 00:35:05,280
capabilities different services

970
00:35:05,280 --> 00:35:06,960
different security

971
00:35:06,960 --> 00:35:09,040
services provided by all the different

972
00:35:09,040 --> 00:35:11,119
providers out there

973
00:35:11,119 --> 00:35:13,599
so really you're trying to

974
00:35:13,599 --> 00:35:16,079
make the solution that was selected work

975
00:35:16,079 --> 00:35:18,720
for you and then you're having to deal

976
00:35:18,720 --> 00:35:20,240
with the

977
00:35:20,240 --> 00:35:23,200
validation of that cloud environment on

978
00:35:23,200 --> 00:35:25,440
a much larger scale

979
00:35:25,440 --> 00:35:27,359
so there's a lot of different factors at

980
00:35:27,359 --> 00:35:30,160
play but from the regulatory perspective

981
00:35:30,160 --> 00:35:31,280
it's really

982
00:35:31,280 --> 00:35:33,200
no different than using an off-the-shelf

983
00:35:33,200 --> 00:35:37,759
software it's just a much more complex

984
00:35:38,000 --> 00:35:40,079
supplier agreement that you're entering

985
00:35:40,079 --> 00:35:42,400
in a much more complex

986
00:35:42,400 --> 00:35:44,640
set of risks that you're

987
00:35:44,640 --> 00:35:46,880
um needing to manage on the medical

988
00:35:46,880 --> 00:35:48,720
device manufacturer side

989
00:35:48,720 --> 00:35:50,079
well i love what you're introducing

990
00:35:50,079 --> 00:35:52,560
really too is this idea that as as more

991
00:35:52,560 --> 00:35:54,560
things become cloud native to dan's

992
00:35:54,560 --> 00:35:56,800
point that that not all clouds are the

993
00:35:56,800 --> 00:35:58,960
same right and that you know we might be

994
00:35:58,960 --> 00:36:00,960
seeing much more of a proliferation in

995
00:36:00,960 --> 00:36:02,720
this area faster than others around

996
00:36:02,720 --> 00:36:04,400
things like multi-cloud right because in

997
00:36:04,400 --> 00:36:06,720
some ways if i can be so bold you know

998
00:36:06,720 --> 00:36:08,960
why would we dumb down the device right

999
00:36:08,960 --> 00:36:11,440
or make it worse if just because we have

1000
00:36:11,440 --> 00:36:13,119
you know a cloud that in essence only

1001
00:36:13,119 --> 00:36:14,960
allows us to do so much

1002
00:36:14,960 --> 00:36:16,640
versus being able to kind of have a

1003
00:36:16,640 --> 00:36:17,920
multi-cloud approach and the

1004
00:36:17,920 --> 00:36:19,680
complexities may be that you know that

1005
00:36:19,680 --> 00:36:22,160
adds but to allow that device to be as

1006
00:36:22,160 --> 00:36:24,400
secure or sustainable or the information

1007
00:36:24,400 --> 00:36:26,880
or it's uptime to be as robust as you

1008
00:36:26,880 --> 00:36:28,720
want it i think it's a really a really

1009
00:36:28,720 --> 00:36:30,480
great point and i hadn't thought about

1010
00:36:30,480 --> 00:36:32,640
this kind of subject matter really being

1011
00:36:32,640 --> 00:36:34,800
at the forefront of kind of that that

1012
00:36:34,800 --> 00:36:36,560
multi-cloud or that cloud selection

1013
00:36:36,560 --> 00:36:38,480
process being quite critical right to

1014
00:36:38,480 --> 00:36:39,920
the performance of what someone actually

1015
00:36:39,920 --> 00:36:41,920
experiences every day i think that's

1016
00:36:41,920 --> 00:36:42,880
that's

1017
00:36:42,880 --> 00:36:45,200
spot on well andrea take us there you

1018
00:36:45,200 --> 00:36:47,040
know about what someone experiences

1019
00:36:47,040 --> 00:36:49,040
every day you know so what about those

1020
00:36:49,040 --> 00:36:51,520
of us who who use you know whose data is

1021
00:36:51,520 --> 00:36:53,520
in this database you know what do we

1022
00:36:53,520 --> 00:36:55,119
need to be thinking about when i when i

1023
00:36:55,119 --> 00:36:57,200
went back in time and was signing away

1024
00:36:57,200 --> 00:36:59,440
kind of medical device uh you know

1025
00:36:59,440 --> 00:37:01,040
paperwork and agreements you know on

1026
00:37:01,040 --> 00:37:02,560
behalf of one of my family members you

1027
00:37:02,560 --> 00:37:03,680
know

1028
00:37:03,680 --> 00:37:05,520
help me juxtapose what i should be

1029
00:37:05,520 --> 00:37:07,359
thinking about what should i be holding

1030
00:37:07,359 --> 00:37:10,960
an organization accountable for

1031
00:37:10,960 --> 00:37:14,800
so the questions of who is in the best

1032
00:37:14,800 --> 00:37:17,520
position to bear certain risks

1033
00:37:17,520 --> 00:37:19,760
that question is certainly at the core

1034
00:37:19,760 --> 00:37:22,400
of many of the decision-making processes

1035
00:37:22,400 --> 00:37:23,920
that we face

1036
00:37:23,920 --> 00:37:27,040
um when you are either a patient or

1037
00:37:27,040 --> 00:37:29,440
someone deciding on behalf of a patient

1038
00:37:29,440 --> 00:37:32,400
um or if you are a corporate decision

1039
00:37:32,400 --> 00:37:36,480
maker in thinking through your own risks

1040
00:37:36,480 --> 00:37:38,480
in terms of operating

1041
00:37:38,480 --> 00:37:41,119
business entity and so just to connect

1042
00:37:41,119 --> 00:37:43,119
quickly with something that matt pointed

1043
00:37:43,119 --> 00:37:44,320
out

1044
00:37:44,320 --> 00:37:48,400
the question of whether you have the

1045
00:37:48,400 --> 00:37:51,680
c-suite fully informed of the totality

1046
00:37:51,680 --> 00:37:52,960
of risks

1047
00:37:52,960 --> 00:37:55,839
that could exist

1048
00:37:55,839 --> 00:37:58,079
that's a key thing that not only

1049
00:37:58,079 --> 00:37:59,760
security professionals but the general

1050
00:37:59,760 --> 00:38:02,480
counsel and individual

1051
00:38:02,480 --> 00:38:05,200
constituents patients and other users of

1052
00:38:05,200 --> 00:38:06,320
products

1053
00:38:06,320 --> 00:38:08,079
can take time to

1054
00:38:08,079 --> 00:38:10,960
connect with the organization and inform

1055
00:38:10,960 --> 00:38:13,599
the organization about it so let me just

1056
00:38:13,599 --> 00:38:16,320
run through a few key points so one of

1057
00:38:16,320 --> 00:38:18,400
the common mistakes

1058
00:38:18,400 --> 00:38:21,119
or stories that i hear from c-suite

1059
00:38:21,119 --> 00:38:23,200
folks is well you know we have to pick

1060
00:38:23,200 --> 00:38:25,119
the most flexible cloud system so we can

1061
00:38:25,119 --> 00:38:27,599
maximally exploit the data in the future

1062
00:38:27,599 --> 00:38:30,240
in ways we have not determined yet

1063
00:38:30,240 --> 00:38:32,960
because we are bound by fiduciary duties

1064
00:38:32,960 --> 00:38:35,839
to maximally exploit short-term revenue

1065
00:38:35,839 --> 00:38:37,280
quarter to quarter

1066
00:38:37,280 --> 00:38:39,520
no not what fiduciary duties say

1067
00:38:39,520 --> 00:38:41,599
fiduciary duties say that you have to

1068
00:38:41,599 --> 00:38:44,000
think about the long-term best interests

1069
00:38:44,000 --> 00:38:44,839
of the

1070
00:38:44,839 --> 00:38:47,359
enterprise the long-term best interests

1071
00:38:47,359 --> 00:38:50,720
of the enterprise may not be aligned

1072
00:38:50,720 --> 00:38:53,839
with maximal short-term profit in any

1073
00:38:53,839 --> 00:38:55,280
given quarter

1074
00:38:55,280 --> 00:38:57,680
and they certainly aren't aligned with

1075
00:38:57,680 --> 00:38:59,680
skimping on say cloud security and

1076
00:38:59,680 --> 00:39:03,680
losing control of your entire set of

1077
00:39:03,680 --> 00:39:05,680
sensitive information connected to

1078
00:39:05,680 --> 00:39:08,240
humans who may suffer physical harm in

1079
00:39:08,240 --> 00:39:11,040
some cases as a result of your choice of

1080
00:39:11,040 --> 00:39:12,800
cloud provider

1081
00:39:12,800 --> 00:39:15,040
so usability is another key piece of

1082
00:39:15,040 --> 00:39:18,000
this what we see in cloud situations is

1083
00:39:18,000 --> 00:39:20,079
that security mistakes often happen

1084
00:39:20,079 --> 00:39:23,760
because of weak usability testing and

1085
00:39:23,760 --> 00:39:26,880
folks making good faith mistakes because

1086
00:39:26,880 --> 00:39:28,880
the design doesn't set them up to

1087
00:39:28,880 --> 00:39:30,240
succeed

1088
00:39:30,240 --> 00:39:31,599
so when

1089
00:39:31,599 --> 00:39:33,200
companies are thinking about cloud

1090
00:39:33,200 --> 00:39:35,839
providers and when users are thinking

1091
00:39:35,839 --> 00:39:36,800
about

1092
00:39:36,800 --> 00:39:39,359
which companies to do business with that

1093
00:39:39,359 --> 00:39:41,200
issue of whether

1094
00:39:41,200 --> 00:39:43,200
uh companies are setting folks up to

1095
00:39:43,200 --> 00:39:45,119
succeed um

1096
00:39:45,119 --> 00:39:46,960
should be part of the calculation and

1097
00:39:46,960 --> 00:39:50,240
and it's also a risk limitation factor

1098
00:39:50,240 --> 00:39:52,240
in terms of potential legal exposure

1099
00:39:52,240 --> 00:39:54,400
down the road if you can explain to a

1100
00:39:54,400 --> 00:39:56,880
court that you went through a careful

1101
00:39:56,880 --> 00:39:59,520
thoughtful process trying to set people

1102
00:39:59,520 --> 00:40:02,000
up to succeed and you identified key

1103
00:40:02,000 --> 00:40:03,680
mistakes that people interacting with

1104
00:40:03,680 --> 00:40:05,680
the system make and you took affirmative

1105
00:40:05,680 --> 00:40:07,839
mitigation steps that's a compelling

1106
00:40:07,839 --> 00:40:12,000
story talking about a careful entity

1107
00:40:12,000 --> 00:40:14,640
trying to make sure that harm does not

1108
00:40:14,640 --> 00:40:16,960
happen it's a very different story from

1109
00:40:16,960 --> 00:40:19,680
one where the usability testing is slim

1110
00:40:19,680 --> 00:40:23,119
to none and and the documents the legal

1111
00:40:23,119 --> 00:40:25,280
documents haven't been usability tested

1112
00:40:25,280 --> 00:40:27,359
there aren't regular audits not only in

1113
00:40:27,359 --> 00:40:29,200
terms of the nuts and bolts of technical

1114
00:40:29,200 --> 00:40:31,359
security controls but also in terms of

1115
00:40:31,359 --> 00:40:33,280
the way that humans interact with the

1116
00:40:33,280 --> 00:40:34,319
system

1117
00:40:34,319 --> 00:40:36,160
and when you can tell those good stories

1118
00:40:36,160 --> 00:40:39,040
it helps no insurance is one more point

1119
00:40:39,040 --> 00:40:41,760
that i'll i'll flag because uh companies

1120
00:40:41,760 --> 00:40:43,280
frequently think oh we'll just insure

1121
00:40:43,280 --> 00:40:45,040
our way out of this and that short

1122
00:40:45,040 --> 00:40:46,800
circuits the threat modeling process and

1123
00:40:46,800 --> 00:40:49,440
they they sometimes um

1124
00:40:49,440 --> 00:40:52,000
overly optimistically think that that

1125
00:40:52,000 --> 00:40:55,440
resolves the uh challenges of cloud

1126
00:40:55,440 --> 00:40:56,800
security

1127
00:40:56,800 --> 00:40:57,920
provision

1128
00:40:57,920 --> 00:40:59,680
it does not and in fact this is one of

1129
00:40:59,680 --> 00:41:01,280
the common mistakes that's happening

1130
00:41:01,280 --> 00:41:03,440
more frequently is that people don't

1131
00:41:03,440 --> 00:41:04,880
necessarily read their insurance

1132
00:41:04,880 --> 00:41:07,119
contracts well and that the defined

1133
00:41:07,119 --> 00:41:09,599
terms in some of these agreements leave

1134
00:41:09,599 --> 00:41:12,000
intentional ambiguity for the insurance

1135
00:41:12,000 --> 00:41:14,880
provider to subsequently refuse coverage

1136
00:41:14,880 --> 00:41:16,960
after an incident and just pull it into

1137
00:41:16,960 --> 00:41:20,800
litigation so a key strategy that exists

1138
00:41:20,800 --> 00:41:22,880
in some insurance

1139
00:41:22,880 --> 00:41:24,800
companies and in companies across

1140
00:41:24,800 --> 00:41:26,079
sectors this is not limited to the

1141
00:41:26,079 --> 00:41:27,440
insurance industry

1142
00:41:27,440 --> 00:41:29,839
is that when you have deep pockets your

1143
00:41:29,839 --> 00:41:32,240
goal is partially as a

1144
00:41:32,240 --> 00:41:34,480
an aggressive plaintiff an aggressive

1145
00:41:34,480 --> 00:41:37,520
defendant to bankrupt the plaintiff into

1146
00:41:37,520 --> 00:41:38,720
settling

1147
00:41:38,720 --> 00:41:39,680
so

1148
00:41:39,680 --> 00:41:41,520
if you uh

1149
00:41:41,520 --> 00:41:45,920
are not that deep pocket provider and

1150
00:41:45,920 --> 00:41:49,200
it's very important to not over trust

1151
00:41:49,200 --> 00:41:52,079
that your insurance will cover things uh

1152
00:41:52,079 --> 00:41:54,560
and the carve-outs really matter

1153
00:41:54,560 --> 00:41:56,640
and and so that's just a cautionary note

1154
00:41:56,640 --> 00:41:59,520
that i'll i'll flag um the last point

1155
00:41:59,520 --> 00:42:01,359
that goes i think most directly to your

1156
00:42:01,359 --> 00:42:03,280
question is that there's an inherent

1157
00:42:03,280 --> 00:42:06,079
tension in data curation that sometimes

1158
00:42:06,079 --> 00:42:07,920
exists between traditional social

1159
00:42:07,920 --> 00:42:10,160
science and medical methodology

1160
00:42:10,160 --> 00:42:12,240
in thinking through what kinds of

1161
00:42:12,240 --> 00:42:14,480
information will lead to the most

1162
00:42:14,480 --> 00:42:16,480
significant breakthroughs in terms of

1163
00:42:16,480 --> 00:42:17,760
research

1164
00:42:17,760 --> 00:42:20,319
and the norms

1165
00:42:20,319 --> 00:42:22,400
of some

1166
00:42:22,400 --> 00:42:27,119
machine learning systems and other

1167
00:42:28,240 --> 00:42:30,319
development processes sit in tension

1168
00:42:30,319 --> 00:42:32,319
with this where you just kind of throw

1169
00:42:32,319 --> 00:42:34,800
it all into the soup and stir it up and

1170
00:42:34,800 --> 00:42:36,240
see what happens

1171
00:42:36,240 --> 00:42:38,560
and so when you have two different

1172
00:42:38,560 --> 00:42:41,920
models one that's driven by high quality

1173
00:42:41,920 --> 00:42:44,000
reliable

1174
00:42:44,000 --> 00:42:46,960
slower research and one that's kind of

1175
00:42:46,960 --> 00:42:50,240
more on the fly with a little more

1176
00:42:50,240 --> 00:42:52,800
everything goes with outliers being

1177
00:42:52,800 --> 00:42:54,079
included in

1178
00:42:54,079 --> 00:42:56,319
you start to see

1179
00:42:56,319 --> 00:43:00,160
new kinds of risks um emerge

1180
00:43:00,160 --> 00:43:01,920
that are not necessarily fully

1181
00:43:01,920 --> 00:43:05,280
considered and the prior models of

1182
00:43:05,280 --> 00:43:07,599
differently curated data

1183
00:43:07,599 --> 00:43:09,280
don't adequately encompass because

1184
00:43:09,280 --> 00:43:11,920
they've been consciously risk mitigated

1185
00:43:11,920 --> 00:43:15,280
so legal liability will take that kind

1186
00:43:15,280 --> 00:43:17,680
of an analysis into account at the end

1187
00:43:17,680 --> 00:43:18,880
of the day

1188
00:43:18,880 --> 00:43:20,720
um so those are just some cautionary

1189
00:43:20,720 --> 00:43:22,560
notes that connect with this idea of the

1190
00:43:22,560 --> 00:43:24,480
balancing act between individuals and

1191
00:43:24,480 --> 00:43:26,560
the companies well this is a quick note

1192
00:43:26,560 --> 00:43:28,480
on on that

1193
00:43:28,480 --> 00:43:30,720
in a lot of cases you're starting to see

1194
00:43:30,720 --> 00:43:32,560
insurance companies since andre you

1195
00:43:32,560 --> 00:43:34,000
brought up insurance

1196
00:43:34,000 --> 00:43:35,920
balk at the idea of actually being

1197
00:43:35,920 --> 00:43:37,280
responsible

1198
00:43:37,280 --> 00:43:39,280
for certain types of cyber attacks

1199
00:43:39,280 --> 00:43:41,040
particularly the ones where the

1200
00:43:41,040 --> 00:43:43,920
malicious actor is a nation state uh and

1201
00:43:43,920 --> 00:43:45,280
you see

1202
00:43:45,280 --> 00:43:47,040
cases coming to court as to whether

1203
00:43:47,040 --> 00:43:48,400
those are sort of normal course of

1204
00:43:48,400 --> 00:43:50,160
business cyber things that they need to

1205
00:43:50,160 --> 00:43:52,720
pay for or those are basically aspects

1206
00:43:52,720 --> 00:43:54,880
of war between nation states that are

1207
00:43:54,880 --> 00:43:56,079
exempt

1208
00:43:56,079 --> 00:43:58,880
and so it raises a lot of larger issues

1209
00:43:58,880 --> 00:44:00,640
right about how risk is addressed in

1210
00:44:00,640 --> 00:44:03,680
this space or not

1211
00:44:03,680 --> 00:44:05,119
one of the things that you're all making

1212
00:44:05,119 --> 00:44:06,880
me think because each of you have

1213
00:44:06,880 --> 00:44:09,040
brought really an interesting angle to

1214
00:44:09,040 --> 00:44:10,400
this and one of the things i can tell

1215
00:44:10,400 --> 00:44:12,640
you is that i have a lot of empathy now

1216
00:44:12,640 --> 00:44:14,720
for our you know executives out there

1217
00:44:14,720 --> 00:44:16,640
who really are are having to become an

1218
00:44:16,640 --> 00:44:18,640
intentional intentionally kind of

1219
00:44:18,640 --> 00:44:21,119
digitally savvy right an entirely new

1220
00:44:21,119 --> 00:44:23,119
age whether those are our executives in

1221
00:44:23,119 --> 00:44:24,720
public sector whether those are device

1222
00:44:24,720 --> 00:44:26,800
manufacturers whether those are you know

1223
00:44:26,800 --> 00:44:29,440
our uh or other types of providers it's

1224
00:44:29,440 --> 00:44:30,800
it's really interesting if you think

1225
00:44:30,800 --> 00:44:33,040
about just in the session alone the

1226
00:44:33,040 --> 00:44:34,480
different insights and the the

1227
00:44:34,480 --> 00:44:36,400
complexity of the things that have to be

1228
00:44:36,400 --> 00:44:38,480
thought about and navigated huge amounts

1229
00:44:38,480 --> 00:44:40,880
of promise but also just huge amounts of

1230
00:44:40,880 --> 00:44:43,200
of you know of making this work i think

1231
00:44:43,200 --> 00:44:45,520
you know being savvy in this way um you

1232
00:44:45,520 --> 00:44:47,119
know quickly i'll go to you you know

1233
00:44:47,119 --> 00:44:48,880
michelle and dan and maybe a little bit

1234
00:44:48,880 --> 00:44:51,040
of a twofer here you know what are some

1235
00:44:51,040 --> 00:44:53,440
of the things that you know that that

1236
00:44:53,440 --> 00:44:55,119
google or that other groups you know

1237
00:44:55,119 --> 00:44:57,119
might provide in terms of like

1238
00:44:57,119 --> 00:44:58,400
healthcare and life sciences what does

1239
00:44:58,400 --> 00:45:00,400
this look like practically and how do

1240
00:45:00,400 --> 00:45:02,800
you make sure that they're secure that

1241
00:45:02,800 --> 00:45:04,079
that person you know that senior

1242
00:45:04,079 --> 00:45:06,480
executive can kind of sleep at night uh

1243
00:45:06,480 --> 00:45:07,599
you know after hopefully if they've

1244
00:45:07,599 --> 00:45:09,119
gotten all their insurance and other

1245
00:45:09,119 --> 00:45:11,280
complexities taken care of but but what

1246
00:45:11,280 --> 00:45:12,560
are the things that are out there that

1247
00:45:12,560 --> 00:45:14,720
that that help and that work and and how

1248
00:45:14,720 --> 00:45:16,240
do you how do you give someone that

1249
00:45:16,240 --> 00:45:19,839
ability to sleep at night

1250
00:45:20,400 --> 00:45:22,800
yeah absolutely so i can i can start and

1251
00:45:22,800 --> 00:45:25,119
then dan if you want to chime in you

1252
00:45:25,119 --> 00:45:27,200
know certainly having built-in

1253
00:45:27,200 --> 00:45:29,040
healthcare security and compliance tools

1254
00:45:29,040 --> 00:45:30,480
is critical

1255
00:45:30,480 --> 00:45:31,920
not only for the health care side of

1256
00:45:31,920 --> 00:45:33,680
things but also for the research data

1257
00:45:33,680 --> 00:45:36,160
side of things um and we definitely have

1258
00:45:36,160 --> 00:45:38,400
tools around privacy of research

1259
00:45:38,400 --> 00:45:40,560
subjects um but but also around the

1260
00:45:40,560 --> 00:45:42,160
healthcare data

1261
00:45:42,160 --> 00:45:44,800
and certainly another really interesting

1262
00:45:44,800 --> 00:45:46,720
and fun capability is

1263
00:45:46,720 --> 00:45:49,520
our tools to aid with the identification

1264
00:45:49,520 --> 00:45:51,440
so not only the identification of

1265
00:45:51,440 --> 00:45:53,760
records but also of

1266
00:45:53,760 --> 00:45:56,319
things like radiology images

1267
00:45:56,319 --> 00:45:58,160
and and that's really important because

1268
00:45:58,160 --> 00:46:00,079
you know certainly that's an area that

1269
00:46:00,079 --> 00:46:02,400
is ripe for research um but again you

1270
00:46:02,400 --> 00:46:04,800
know there's also the risk of patient

1271
00:46:04,800 --> 00:46:07,040
identification there um and we want to

1272
00:46:07,040 --> 00:46:09,440
make sure um to protect individuals and

1273
00:46:09,440 --> 00:46:10,960
you know this goes back to what andrea

1274
00:46:10,960 --> 00:46:12,560
was saying about you know the the

1275
00:46:12,560 --> 00:46:14,720
consent process is also a really

1276
00:46:14,720 --> 00:46:17,359
critical piece of this equation and

1277
00:46:17,359 --> 00:46:19,760
certainly from a research perspective

1278
00:46:19,760 --> 00:46:21,680
i've seen a lot of research studies do a

1279
00:46:21,680 --> 00:46:24,560
great job of building in

1280
00:46:24,560 --> 00:46:26,560
new ways of doing consent where people

1281
00:46:26,560 --> 00:46:28,319
actually understand what they're

1282
00:46:28,319 --> 00:46:30,880
consenting to and they're also able to

1283
00:46:30,880 --> 00:46:34,079
change their consent choices um over

1284
00:46:34,079 --> 00:46:36,079
time so i think that's a really great

1285
00:46:36,079 --> 00:46:38,240
innovation um and then going back to

1286
00:46:38,240 --> 00:46:39,920
what dan said earlier

1287
00:46:39,920 --> 00:46:42,240
the fact of the matter is this is a

1288
00:46:42,240 --> 00:46:44,160
because the technology is evolving so

1289
00:46:44,160 --> 00:46:45,839
quickly and because we're integrating

1290
00:46:45,839 --> 00:46:47,520
the technology into this space so

1291
00:46:47,520 --> 00:46:50,079
quickly we have to be really thoughtful

1292
00:46:50,079 --> 00:46:51,760
not just to be compliant with current

1293
00:46:51,760 --> 00:46:53,839
regulations but also to be forward

1294
00:46:53,839 --> 00:46:54,800
thinking

1295
00:46:54,800 --> 00:46:57,440
about what are the actual security risks

1296
00:46:57,440 --> 00:46:59,440
and how can we start to protect against

1297
00:46:59,440 --> 00:47:02,319
them before we have to you know so so

1298
00:47:02,319 --> 00:47:04,720
really taking that um taking that to

1299
00:47:04,720 --> 00:47:06,720
heart taking that uh that protection

1300
00:47:06,720 --> 00:47:08,839
piece to heart i think is really

1301
00:47:08,839 --> 00:47:11,440
critical i think another thing alexis is

1302
00:47:11,440 --> 00:47:12,560
to make sure

1303
00:47:12,560 --> 00:47:14,160
and i hope this goes without saying but

1304
00:47:14,160 --> 00:47:16,480
that non-technology and non-security

1305
00:47:16,480 --> 00:47:18,160
executives really need to get

1306
00:47:18,160 --> 00:47:19,359
comfortable

1307
00:47:19,359 --> 00:47:21,760
and understand a couple of things number

1308
00:47:21,760 --> 00:47:23,920
one is how does security affect the

1309
00:47:23,920 --> 00:47:25,680
mission and your overall business and

1310
00:47:25,680 --> 00:47:28,319
reputation right and take that to heart

1311
00:47:28,319 --> 00:47:29,680
that itself should be enough of a

1312
00:47:29,680 --> 00:47:32,400
motivator to not treat security as a

1313
00:47:32,400 --> 00:47:33,599
black box

1314
00:47:33,599 --> 00:47:35,520
but even going beyond that i think it's

1315
00:47:35,520 --> 00:47:37,040
important as they think about their

1316
00:47:37,040 --> 00:47:39,280
business models and how critical

1317
00:47:39,280 --> 00:47:41,200
technology and in many cases cloud

1318
00:47:41,200 --> 00:47:44,079
technology is in terms of enabling their

1319
00:47:44,079 --> 00:47:45,359
business model

1320
00:47:45,359 --> 00:47:47,440
they need to be very intentional and

1321
00:47:47,440 --> 00:47:49,760
very regularly be aware

1322
00:47:49,760 --> 00:47:52,079
of what their value chain and life cycle

1323
00:47:52,079 --> 00:47:54,400
of data looks like again whether it's

1324
00:47:54,400 --> 00:47:56,559
electronic health records or device or

1325
00:47:56,559 --> 00:47:59,520
research right they should understand

1326
00:47:59,520 --> 00:48:02,720
you know how data is born or required

1327
00:48:02,720 --> 00:48:04,800
how it is handled how it is protected

1328
00:48:04,800 --> 00:48:06,960
how it is augmented what metadata gets

1329
00:48:06,960 --> 00:48:08,800
created off of it how it is passed on to

1330
00:48:08,800 --> 00:48:11,839
other people how people interact with it

1331
00:48:11,839 --> 00:48:13,760
how it flows back and forth between them

1332
00:48:13,760 --> 00:48:15,440
and the customer how it flows back and

1333
00:48:15,440 --> 00:48:16,640
forth between employees and

1334
00:48:16,640 --> 00:48:18,400
collaborators right

1335
00:48:18,400 --> 00:48:21,200
and that actually

1336
00:48:21,200 --> 00:48:23,839
simply being intentional about that i

1337
00:48:23,839 --> 00:48:26,079
think takes security out of

1338
00:48:26,079 --> 00:48:27,920
what is too often a black box oh it's

1339
00:48:27,920 --> 00:48:29,599
the i.t guys it's over there but it

1340
00:48:29,599 --> 00:48:30,720
actually puts it in i'm going to ask

1341
00:48:30,720 --> 00:48:32,720
basic questions about how to change hand

1342
00:48:32,720 --> 00:48:35,280
changes hands and where it sits and when

1343
00:48:35,280 --> 00:48:36,880
it sits in certain places how is it

1344
00:48:36,880 --> 00:48:40,240
protected right you can ask very lay

1345
00:48:40,240 --> 00:48:41,359
questions

1346
00:48:41,359 --> 00:48:43,520
but everyone needs an end-to-end mental

1347
00:48:43,520 --> 00:48:45,119
model of how data flows through their

1348
00:48:45,119 --> 00:48:48,000
enterprise and if they do that again it

1349
00:48:48,000 --> 00:48:50,559
really collapses what is often too much

1350
00:48:50,559 --> 00:48:52,640
of an arms-length relationship or sort

1351
00:48:52,640 --> 00:48:54,160
of like a chasm between tech and

1352
00:48:54,160 --> 00:48:56,319
non-tech and if they take that kind of

1353
00:48:56,319 --> 00:48:58,319
ownership of it they should be able to

1354
00:48:58,319 --> 00:48:59,839
sleep at night because they're

1355
00:48:59,839 --> 00:49:02,319
constantly asking tough questions about

1356
00:49:02,319 --> 00:49:03,680
how all this stuff is handled and

1357
00:49:03,680 --> 00:49:06,000
protected number one number two they

1358
00:49:06,000 --> 00:49:08,000
have a mental model for how things flow

1359
00:49:08,000 --> 00:49:09,440
and how it relates to their business

1360
00:49:09,440 --> 00:49:10,559
success

1361
00:49:10,559 --> 00:49:13,119
and therefore by taking ownership of it

1362
00:49:13,119 --> 00:49:14,880
right it just arms them much more

1363
00:49:14,880 --> 00:49:17,200
readily to not be surprised

1364
00:49:17,200 --> 00:49:18,160
to

1365
00:49:18,160 --> 00:49:19,280
not be

1366
00:49:19,280 --> 00:49:21,920
overwhelmed by a technical security

1367
00:49:21,920 --> 00:49:24,160
update that often is filled with jargon

1368
00:49:24,160 --> 00:49:25,920
and it's a particular expertise but they

1369
00:49:25,920 --> 00:49:28,000
can ask questions and have

1370
00:49:28,000 --> 00:49:29,359
be confident that they're getting the

1371
00:49:29,359 --> 00:49:30,960
right answers because they have their

1372
00:49:30,960 --> 00:49:32,960
own language to deal with these security

1373
00:49:32,960 --> 00:49:34,559
things

1374
00:49:34,559 --> 00:49:35,760
i think i don't know if you could hear

1375
00:49:35,760 --> 00:49:37,680
it dan but i think the whole cadre of

1376
00:49:37,680 --> 00:49:39,760
public servant chief data officers are

1377
00:49:39,760 --> 00:49:41,680
cheering in the background right this

1378
00:49:41,680 --> 00:49:43,440
idea of really getting you know those

1379
00:49:43,440 --> 00:49:45,440
c-level executives to really care about

1380
00:49:45,440 --> 00:49:47,440
the data these stewards of data and

1381
00:49:47,440 --> 00:49:48,559
really in some ways you know what i'm

1382
00:49:48,559 --> 00:49:50,400
hearing from all of you is that in some

1383
00:49:50,400 --> 00:49:52,720
ways almost every organization is a data

1384
00:49:52,720 --> 00:49:54,880
company now right no matter what you're

1385
00:49:54,880 --> 00:49:56,240
doing no matter what you're delivering

1386
00:49:56,240 --> 00:49:58,640
this idea of being a data company and it

1387
00:49:58,640 --> 00:50:00,000
was actually funny someone said to me

1388
00:50:00,000 --> 00:50:01,760
the other day what is it that a public

1389
00:50:01,760 --> 00:50:04,240
servant does and you know i thought well

1390
00:50:04,240 --> 00:50:06,319
you know what i did was i i took in the

1391
00:50:06,319 --> 00:50:08,880
best information i could i had a great

1392
00:50:08,880 --> 00:50:10,480
privilege of trying to decide where

1393
00:50:10,480 --> 00:50:12,800
those resources would go right based on

1394
00:50:12,800 --> 00:50:14,400
that information to try to have have

1395
00:50:14,400 --> 00:50:15,760
great impact in the world and then

1396
00:50:15,760 --> 00:50:17,280
ultimately i looked at more information

1397
00:50:17,280 --> 00:50:19,119
to decide whether or not we had done it

1398
00:50:19,119 --> 00:50:20,960
right right and how we might tweak it

1399
00:50:20,960 --> 00:50:22,640
and so you know what i love that we kind

1400
00:50:22,640 --> 00:50:24,240
of brought brought together at the end

1401
00:50:24,240 --> 00:50:26,079
of this is all of you really helping you

1402
00:50:26,079 --> 00:50:27,599
know the folks out there understand that

1403
00:50:27,599 --> 00:50:29,599
this is about you know information

1404
00:50:29,599 --> 00:50:31,280
stewardship right this is about data

1405
00:50:31,280 --> 00:50:32,400
stewardship this is about that

1406
00:50:32,400 --> 00:50:34,960
intentionality that curiosity that we

1407
00:50:34,960 --> 00:50:37,040
really all just have to have you know to

1408
00:50:37,040 --> 00:50:39,280
be not only up to speed with all of the

1409
00:50:39,280 --> 00:50:41,359
things that you raised today each of you

1410
00:50:41,359 --> 00:50:42,960
but more importantly to be champions

1411
00:50:42,960 --> 00:50:44,800
right and to be pushing the envelope

1412
00:50:44,800 --> 00:50:46,960
with that curiosity are we doing it as

1413
00:50:46,960 --> 00:50:49,040
safely as we can as effectively as we

1414
00:50:49,040 --> 00:50:51,440
can you know to the best stewardship as

1415
00:50:51,440 --> 00:50:53,839
we can so i'd love to ask each of you

1416
00:50:53,839 --> 00:50:56,079
though to we've talked about a lot of

1417
00:50:56,079 --> 00:50:57,680
things people have to think about a lot

1418
00:50:57,680 --> 00:50:59,440
of concerns out there hopefully people

1419
00:50:59,440 --> 00:51:01,280
are going away with a lot of notes and

1420
00:51:01,280 --> 00:51:03,440
hopefully a lot of questions for our q a

1421
00:51:03,440 --> 00:51:05,280
that will be coming up soon but what i'd

1422
00:51:05,280 --> 00:51:07,359
like to ask each of you to leave and it

1423
00:51:07,359 --> 00:51:09,040
can just be you know a quick quick

1424
00:51:09,040 --> 00:51:10,720
response

1425
00:51:10,720 --> 00:51:12,160
groundness and the fact that this is

1426
00:51:12,160 --> 00:51:14,240
still amazing technology that's letting

1427
00:51:14,240 --> 00:51:16,640
us do amazing things so you know for

1428
00:51:16,640 --> 00:51:18,960
each of you what does getting this right

1429
00:51:18,960 --> 00:51:21,760
look like what might be different about

1430
00:51:21,760 --> 00:51:23,520
health biomedicine you know what might

1431
00:51:23,520 --> 00:51:26,400
those breakthroughs be in five years if

1432
00:51:26,400 --> 00:51:28,960
we're really harnessing you know

1433
00:51:28,960 --> 00:51:31,040
cloud in a way that is secure and

1434
00:51:31,040 --> 00:51:32,240
appropriate and that we're really

1435
00:51:32,240 --> 00:51:34,000
leveraging the power of cloud each of

1436
00:51:34,000 --> 00:51:35,680
you maybe give me you know a quick

1437
00:51:35,680 --> 00:51:37,280
summary of what's that thing you're

1438
00:51:37,280 --> 00:51:39,440
hoping will be different because we've

1439
00:51:39,440 --> 00:51:41,440
leveraged the power of cloud securely

1440
00:51:41,440 --> 00:51:43,359
and appropriately so i'm going to throw

1441
00:51:43,359 --> 00:51:46,240
that to michelle first

1442
00:51:46,240 --> 00:51:49,520
absolutely yeah so so for me success um

1443
00:51:49,520 --> 00:51:51,119
you know really came

1444
00:51:51,119 --> 00:51:52,960
very clearly over the last year when

1445
00:51:52,960 --> 00:51:55,280
before coming on board to google i was

1446
00:51:55,280 --> 00:51:57,200
working with cissa and the national risk

1447
00:51:57,200 --> 00:52:00,000
management center um to protect the the

1448
00:52:00,000 --> 00:52:01,920
assets around covid and developing the

1449
00:52:01,920 --> 00:52:03,440
vaccine

1450
00:52:03,440 --> 00:52:06,079
and and really the goal there was to

1451
00:52:06,079 --> 00:52:07,520
save lives

1452
00:52:07,520 --> 00:52:09,599
because we knew that any delay in

1453
00:52:09,599 --> 00:52:11,680
creating a vaccine was going to equal

1454
00:52:11,680 --> 00:52:13,920
loss of life and so it was a very it was

1455
00:52:13,920 --> 00:52:16,319
a very clear mission set where um you

1456
00:52:16,319 --> 00:52:18,240
know the more secure we are in the

1457
00:52:18,240 --> 00:52:20,240
healthcare and life sciences space the

1458
00:52:20,240 --> 00:52:23,200
more lives we will save um another um

1459
00:52:23,200 --> 00:52:24,880
another i have two three so that's

1460
00:52:24,880 --> 00:52:26,720
number one number two is precision

1461
00:52:26,720 --> 00:52:28,480
medicine i think that there is

1462
00:52:28,480 --> 00:52:30,640
tremendous opportunity there to really

1463
00:52:30,640 --> 00:52:32,160
crack that code and instead of doing

1464
00:52:32,160 --> 00:52:34,400
one-size-fits-all clinical practice to

1465
00:52:34,400 --> 00:52:36,960
be able to use genomics bioinformatics

1466
00:52:36,960 --> 00:52:40,720
metabolomics etc um to to really um

1467
00:52:40,720 --> 00:52:42,319
tailor clinical practice to each

1468
00:52:42,319 --> 00:52:44,800
individual and make people healthier um

1469
00:52:44,800 --> 00:52:46,559
so that they they not only live longer

1470
00:52:46,559 --> 00:52:47,839
lives but that they're healthier and

1471
00:52:47,839 --> 00:52:49,680
they have better quality quality of life

1472
00:52:49,680 --> 00:52:51,599
throughout their life um and then the

1473
00:52:51,599 --> 00:52:54,480
third is you know really protecting um

1474
00:52:54,480 --> 00:52:56,800
our nation's bio economy

1475
00:52:56,800 --> 00:52:58,880
this is a critical sector i think of

1476
00:52:58,880 --> 00:53:01,359
growth and um and really of national

1477
00:53:01,359 --> 00:53:03,680
security significance so um you know

1478
00:53:03,680 --> 00:53:05,599
making sure that we're able to protect

1479
00:53:05,599 --> 00:53:08,720
that and um and and create create nice

1480
00:53:08,720 --> 00:53:11,440
uh really amazing technology um you know

1481
00:53:11,440 --> 00:53:13,040
for our country security would be

1482
00:53:13,040 --> 00:53:15,119
amazing well michelle i know you're

1483
00:53:15,119 --> 00:53:16,559
going to do a lot to contribute to all

1484
00:53:16,559 --> 00:53:18,319
three of those things matt i'll go over

1485
00:53:18,319 --> 00:53:20,240
to you what what's your five-year hope

1486
00:53:20,240 --> 00:53:22,000
for us

1487
00:53:22,000 --> 00:53:25,200
oh i'd say that definitely in terms of

1488
00:53:25,200 --> 00:53:28,160
using the capabilities to be able to

1489
00:53:28,160 --> 00:53:30,160
more closely

1490
00:53:30,160 --> 00:53:32,480
monitor and

1491
00:53:32,480 --> 00:53:34,240
respond to

1492
00:53:34,240 --> 00:53:36,640
um the evolving

1493
00:53:36,640 --> 00:53:38,800
healthcare challenges that we have by

1494
00:53:38,800 --> 00:53:40,720
leveraging these capabilities definitely

1495
00:53:40,720 --> 00:53:42,880
the pandemic

1496
00:53:42,880 --> 00:53:45,440
drove a massive push into more remote

1497
00:53:45,440 --> 00:53:47,920
monitoring more remote care and the

1498
00:53:47,920 --> 00:53:50,800
cloud definitely is a important piece of

1499
00:53:50,800 --> 00:53:54,000
um enabling that moving forward

1500
00:53:54,000 --> 00:53:55,680
and i think just from

1501
00:53:55,680 --> 00:53:59,520
the larger fda perspective i think that

1502
00:53:59,520 --> 00:54:01,040
in terms of

1503
00:54:01,040 --> 00:54:03,520
reframing the cyber security

1504
00:54:03,520 --> 00:54:05,520
considerations of the cloud of it

1505
00:54:05,520 --> 00:54:08,160
extends beyond just

1506
00:54:08,160 --> 00:54:12,000
data security potential phi pii

1507
00:54:12,000 --> 00:54:15,119
data releases and data breaches that

1508
00:54:15,119 --> 00:54:17,040
patient safety and effectiveness is on

1509
00:54:17,040 --> 00:54:19,680
the line that this is a

1510
00:54:19,680 --> 00:54:22,960
central to the long term

1511
00:54:22,960 --> 00:54:24,480
healthcare delivery

1512
00:54:24,480 --> 00:54:26,960
in this country and across the world is

1513
00:54:26,960 --> 00:54:29,280
leveraging these systems

1514
00:54:29,280 --> 00:54:32,559
and doing so thoughtfully and

1515
00:54:32,559 --> 00:54:34,160
putting that careful

1516
00:54:34,160 --> 00:54:36,079
thought in consideration because it's

1517
00:54:36,079 --> 00:54:37,440
more than just

1518
00:54:37,440 --> 00:54:40,079
data on the line it's actual patient

1519
00:54:40,079 --> 00:54:41,839
safety and effectiveness

1520
00:54:41,839 --> 00:54:44,720
of the medical devices

1521
00:54:44,720 --> 00:54:46,319
well i know i speak for everyone here

1522
00:54:46,319 --> 00:54:48,240
that i am so glad that there are public

1523
00:54:48,240 --> 00:54:50,319
servants like you and your colleagues at

1524
00:54:50,319 --> 00:54:52,079
fda that are thinking about these things

1525
00:54:52,079 --> 00:54:54,000
and are making them bigger and helping

1526
00:54:54,000 --> 00:54:55,839
us think and be more curious about these

1527
00:54:55,839 --> 00:54:57,760
larger issues so thank you for the way

1528
00:54:57,760 --> 00:54:59,440
you're going to advance us in the next

1529
00:54:59,440 --> 00:55:03,359
five years as well dan over to you

1530
00:55:03,359 --> 00:55:06,400
um i i think i'd echo what everyone said

1531
00:55:06,400 --> 00:55:08,640
i think what's really on offer with the

1532
00:55:08,640 --> 00:55:10,400
cloud and new healthcare business models

1533
00:55:10,400 --> 00:55:12,240
whether it's devices or research and

1534
00:55:12,240 --> 00:55:15,520
collaboration or transforming sort of

1535
00:55:15,520 --> 00:55:18,079
you know medical records and the like

1536
00:55:18,079 --> 00:55:19,839
is really the fact that all of the

1537
00:55:19,839 --> 00:55:21,920
patient outcomes that we think about

1538
00:55:21,920 --> 00:55:24,720
really rely inherently on data the

1539
00:55:24,720 --> 00:55:26,640
quality of the data the ability to take

1540
00:55:26,640 --> 00:55:28,640
lessons off that data the ability to

1541
00:55:28,640 --> 00:55:30,400
treat better to make better decisions

1542
00:55:30,400 --> 00:55:33,280
and so when you think about data

1543
00:55:33,280 --> 00:55:35,760
again what cloud offers are velocity and

1544
00:55:35,760 --> 00:55:37,440
scale

1545
00:55:37,440 --> 00:55:39,119
and with velocity and scale again you

1546
00:55:39,119 --> 00:55:41,680
can get better patient outcomes and as i

1547
00:55:41,680 --> 00:55:43,280
said before the cloud sort of enables

1548
00:55:43,280 --> 00:55:45,200
that but it is a different

1549
00:55:45,200 --> 00:55:47,040
model for a lot of enterprises to be

1550
00:55:47,040 --> 00:55:48,720
managing data this way i mean we've been

1551
00:55:48,720 --> 00:55:50,160
in this transformation for a while but

1552
00:55:50,160 --> 00:55:51,680
it's still relatively new it's still in

1553
00:55:51,680 --> 00:55:53,200
the early innings if you're a baseball

1554
00:55:53,200 --> 00:55:54,400
fan right

1555
00:55:54,400 --> 00:55:56,240
and so for

1556
00:55:56,240 --> 00:55:57,520
patients

1557
00:55:57,520 --> 00:56:00,319
and caregivers to have confidence in

1558
00:56:00,319 --> 00:56:02,720
that and be confident that that that we

1559
00:56:02,720 --> 00:56:04,559
can live up to the promise of better

1560
00:56:04,559 --> 00:56:06,559
data-driven healthcare decisions in

1561
00:56:06,559 --> 00:56:08,880
patient care

1562
00:56:08,880 --> 00:56:12,000
we need to make sure and be stewards and

1563
00:56:12,000 --> 00:56:13,760
not just technology people need to be

1564
00:56:13,760 --> 00:56:15,839
stewards but all other executives who

1565
00:56:15,839 --> 00:56:17,520
are not technical also need to be

1566
00:56:17,520 --> 00:56:19,920
stewards of that in order for people to

1567
00:56:19,920 --> 00:56:21,920
be confident we have to secure things

1568
00:56:21,920 --> 00:56:23,040
properly

1569
00:56:23,040 --> 00:56:24,960
right we need to make sure that privacy

1570
00:56:24,960 --> 00:56:26,720
is protected that privacy data isn't

1571
00:56:26,720 --> 00:56:29,359
leaking that the risk of someone

1572
00:56:29,359 --> 00:56:31,280
tampering with data or tampering with

1573
00:56:31,280 --> 00:56:33,520
the instructions to an implanted medical

1574
00:56:33,520 --> 00:56:34,880
device that

1575
00:56:34,880 --> 00:56:36,480
that those things are bulletproof that

1576
00:56:36,480 --> 00:56:37,839
they're reliable

1577
00:56:37,839 --> 00:56:40,000
right and that everyone in that value

1578
00:56:40,000 --> 00:56:40,799
chain

1579
00:56:40,799 --> 00:56:42,799
is not just thinking about

1580
00:56:42,799 --> 00:56:45,040
the business outcome or business risk

1581
00:56:45,040 --> 00:56:46,400
everyone that's a business person in

1582
00:56:46,400 --> 00:56:47,920
that value chain is also a healthcare

1583
00:56:47,920 --> 00:56:50,079
provider right and so in my view all

1584
00:56:50,079 --> 00:56:52,240
those people also carry around with them

1585
00:56:52,240 --> 00:56:54,400
indirectly the hippocratic oath

1586
00:56:54,400 --> 00:56:57,040
right be stewards of the data create

1587
00:56:57,040 --> 00:56:58,880
confidence in the new models so that we

1588
00:56:58,880 --> 00:57:01,040
can all move forward and get the better

1589
00:57:01,040 --> 00:57:02,960
velocity the better scale the better

1590
00:57:02,960 --> 00:57:04,640
decision making the better insights that

1591
00:57:04,640 --> 00:57:06,559
come with that

1592
00:57:06,559 --> 00:57:07,760
well and then i think one of the things

1593
00:57:07,760 --> 00:57:09,359
i have to thank you for is i know how

1594
00:57:09,359 --> 00:57:11,119
much time you spend every day actually

1595
00:57:11,119 --> 00:57:12,960
helping a lot of those senior executives

1596
00:57:12,960 --> 00:57:15,040
be curious about these topics and really

1597
00:57:15,040 --> 00:57:16,480
investigate and really help them

1598
00:57:16,480 --> 00:57:18,000
understand kind of what are those

1599
00:57:18,000 --> 00:57:19,520
options what are those trade-offs so

1600
00:57:19,520 --> 00:57:21,040
thank you for getting us to where we're

1601
00:57:21,040 --> 00:57:23,200
going to be in five years and andrea

1602
00:57:23,200 --> 00:57:25,040
bring us home you have been so

1603
00:57:25,040 --> 00:57:26,960
interesting this whole panels i had to

1604
00:57:26,960 --> 00:57:29,839
give you the last word so please

1605
00:57:29,839 --> 00:57:33,040
five years tell me what to expect so uh

1606
00:57:33,040 --> 00:57:35,440
in five years i hope that we will have

1607
00:57:35,440 --> 00:57:36,559
been

1608
00:57:36,559 --> 00:57:38,720
more thoughtful in some ways than we

1609
00:57:38,720 --> 00:57:40,079
have been to this point about the

1610
00:57:40,079 --> 00:57:41,680
question of metrics

1611
00:57:41,680 --> 00:57:44,720
so we talk about innovation progress we

1612
00:57:44,720 --> 00:57:46,400
throw them around as buzzwords but

1613
00:57:46,400 --> 00:57:49,040
actually the definitions of whether we

1614
00:57:49,040 --> 00:57:52,799
have a shared view of what progress is

1615
00:57:52,799 --> 00:57:54,720
becomes relevant so legally the word

1616
00:57:54,720 --> 00:57:56,799
progress shows up in the constitution

1617
00:57:56,799 --> 00:57:59,599
but as a technology

1618
00:57:59,599 --> 00:58:01,839
sector broadly speaking

1619
00:58:01,839 --> 00:58:04,480
the question of what that next

1620
00:58:04,480 --> 00:58:07,200
generation better world looks like is

1621
00:58:07,200 --> 00:58:09,040
one that we don't spend enough time

1622
00:58:09,040 --> 00:58:10,799
talking about i don't think we're all on

1623
00:58:10,799 --> 00:58:12,880
the same page i think we're on very

1624
00:58:12,880 --> 00:58:14,559
different pages in some cases of what

1625
00:58:14,559 --> 00:58:17,119
that world looks like so progress the

1626
00:58:17,119 --> 00:58:18,480
word progress if you look at the

1627
00:58:18,480 --> 00:58:19,839
philosophy literature it has three

1628
00:58:19,839 --> 00:58:22,160
components the first is a normative

1629
00:58:22,160 --> 00:58:25,040
claim about what is that better life and

1630
00:58:25,040 --> 00:58:27,040
so we start there and then we get to the

1631
00:58:27,040 --> 00:58:29,040
social science claims that's the second

1632
00:58:29,040 --> 00:58:31,200
piece so what do we need to figure out

1633
00:58:31,200 --> 00:58:34,000
in order to build that world and the

1634
00:58:34,000 --> 00:58:35,599
third piece is the implementation

1635
00:58:35,599 --> 00:58:38,240
specifics and we need to focus on each

1636
00:58:38,240 --> 00:58:41,680
of those sets of questions in order to

1637
00:58:41,680 --> 00:58:44,559
achieve some sort of next generation

1638
00:58:44,559 --> 00:58:47,839
successful technology society um so

1639
00:58:47,839 --> 00:58:51,680
that's the first thing the secondary

1640
00:58:51,680 --> 00:58:53,920
concern that's included

1641
00:58:53,920 --> 00:58:55,040
is the question of whether we're

1642
00:58:55,040 --> 00:58:57,760
building technology for people to help

1643
00:58:57,760 --> 00:59:00,480
them achieve that good life or whether

1644
00:59:00,480 --> 00:59:02,799
we're building technologies that use

1645
00:59:02,799 --> 00:59:03,760
people

1646
00:59:03,760 --> 00:59:04,880
toward

1647
00:59:04,880 --> 00:59:07,040
other goals that are not necessarily

1648
00:59:07,040 --> 00:59:08,960
about building that better world

1649
00:59:08,960 --> 00:59:11,200
um and um

1650
00:59:11,200 --> 00:59:12,799
as a

1651
00:59:12,799 --> 00:59:15,280
kind of quick uh

1652
00:59:15,280 --> 00:59:17,839
shared homework assignment i i think we

1653
00:59:17,839 --> 00:59:19,839
might all have fun with uh

1654
00:59:19,839 --> 00:59:21,760
some beverages of our choice to start

1655
00:59:21,760 --> 00:59:23,760
re-watching episodes of the jetsons

1656
00:59:23,760 --> 00:59:25,680
which was one of my favorite

1657
00:59:25,680 --> 00:59:28,160
shows growing up and as i watch that

1658
00:59:28,160 --> 00:59:30,880
show now through the eyes of security i

1659
00:59:30,880 --> 00:59:34,079
realized that um what i remembered being

1660
00:59:34,079 --> 00:59:36,799
kind of a fun utopian next-gen tech

1661
00:59:36,799 --> 00:59:38,720
world is actually a little bit of a

1662
00:59:38,720 --> 00:59:42,000
dystopian hellscape where george jetson

1663
00:59:42,000 --> 00:59:44,400
gets sucked in by his treadmills and

1664
00:59:44,400 --> 00:59:46,400
robots malfunction and people get

1665
00:59:46,400 --> 00:59:48,400
ejected out of their flying cars all the

1666
00:59:48,400 --> 00:59:49,280
time

1667
00:59:49,280 --> 00:59:50,240
so

1668
00:59:50,240 --> 00:59:52,160
going back and looking at the lessons of

1669
00:59:52,160 --> 00:59:54,319
history and thinking through which

1670
00:59:54,319 --> 00:59:56,799
pieces of that

1671
00:59:56,799 --> 00:59:59,520
model were good and which pieces of our

1672
00:59:59,520 --> 01:00:02,960
historical models have been not so good

1673
01:00:02,960 --> 01:00:06,480
um and analyzing technologies with that

1674
01:00:06,480 --> 01:00:07,920
critical eye i think it's really

1675
01:00:07,920 --> 01:00:09,520
important um

1676
01:00:09,520 --> 01:00:12,000
super quick story 1933 world's fair on

1677
01:00:12,000 --> 01:00:13,680
one side of the fairway

1678
01:00:13,680 --> 01:00:14,799
there were

1679
01:00:14,799 --> 01:00:17,440
incubators that were privately funded

1680
01:00:17,440 --> 01:00:19,599
and the story is fascinating i go

1681
01:00:19,599 --> 01:00:21,359
through it in a future article um but

1682
01:00:21,359 --> 01:00:23,359
just um tldr

1683
01:00:23,359 --> 01:00:25,599
they were privately funded incubators

1684
01:00:25,599 --> 01:00:28,079
that charged admission to let people see

1685
01:00:28,079 --> 01:00:30,559
the babies but did a fantastic job

1686
01:00:30,559 --> 01:00:32,880
anonymizing the identities of the babies

1687
01:00:32,880 --> 01:00:34,160
to the point that some of the babies

1688
01:00:34,160 --> 01:00:36,079
when they grew up didn't even know that

1689
01:00:36,079 --> 01:00:38,079
they were in those incubators meanwhile

1690
01:00:38,079 --> 01:00:40,240
hospitals at the time did not want to

1691
01:00:40,240 --> 01:00:42,079
save those babies because they were

1692
01:00:42,079 --> 01:00:44,880
deemed to be not worth saving

1693
01:00:44,880 --> 01:00:48,240
so you had this doc this doctor question

1694
01:00:48,240 --> 01:00:52,240
mark his background is 2vd um saving

1695
01:00:52,240 --> 01:00:56,000
kids on a race-blind needs-blind basis

1696
01:00:56,000 --> 01:00:57,599
other side of the fairway

1697
01:00:57,599 --> 01:01:01,760
the eugenicists who subsequently were

1698
01:01:01,760 --> 01:01:04,240
commented on as inspiration by nazi

1699
01:01:04,240 --> 01:01:06,480
scientists for quantifying human bodies

1700
01:01:06,480 --> 01:01:08,640
in very particular ways

1701
01:01:08,640 --> 01:01:11,520
and leading to better baby contests and

1702
01:01:11,520 --> 01:01:14,160
other things that um history looks back

1703
01:01:14,160 --> 01:01:17,040
on with reproach and scorn

1704
01:01:17,040 --> 01:01:18,960
so um

1705
01:01:18,960 --> 01:01:21,359
uh i'll leave the social commentary

1706
01:01:21,359 --> 01:01:23,599
there um but just on a very concrete

1707
01:01:23,599 --> 01:01:26,319
legal point in terms of evolving law

1708
01:01:26,319 --> 01:01:27,920
which is the question that companies

1709
01:01:27,920 --> 01:01:29,359
always ask how do we anticipate where

1710
01:01:29,359 --> 01:01:30,640
the law is going

1711
01:01:30,640 --> 01:01:33,680
if you build with an eye on that better

1712
01:01:33,680 --> 01:01:36,480
society that better world and you try

1713
01:01:36,480 --> 01:01:39,760
not to skirt into the regulatory cracks

1714
01:01:39,760 --> 01:01:42,960
but instead just build in line with the

1715
01:01:42,960 --> 01:01:45,520
the paradigm that any regulator could

1716
01:01:45,520 --> 01:01:47,920
potentially cover you if you think about

1717
01:01:47,920 --> 01:01:50,319
it in terms of not causing harm the

1718
01:01:50,319 --> 01:01:53,440
medical ethic of do no harm if you live

1719
01:01:53,440 --> 01:01:56,559
by those words you won't have regulatory

1720
01:01:56,559 --> 01:01:58,400
problems in terms of the general

1721
01:01:58,400 --> 01:01:59,680
direction

1722
01:01:59,680 --> 01:02:00,559
and

1723
01:02:00,559 --> 01:02:02,799
uh decades later you know we have a very

1724
01:02:02,799 --> 01:02:04,319
different approach to say incubators

1725
01:02:04,319 --> 01:02:06,480
which are state of state-of-the-art uh

1726
01:02:06,480 --> 01:02:09,119
now and incorporated in regular settings

1727
01:02:09,119 --> 01:02:11,920
um and they're highly regulated by the

1728
01:02:11,920 --> 01:02:15,280
the fda but uh nevertheless uh you know

1729
01:02:15,280 --> 01:02:18,640
the the goal of saving those uh babies

1730
01:02:18,640 --> 01:02:20,960
and doing it in a way with

1731
01:02:20,960 --> 01:02:24,319
attention to detail um and care uh

1732
01:02:24,319 --> 01:02:26,240
remains uh even though the regulatory

1733
01:02:26,240 --> 01:02:27,760
system is very different today than it

1734
01:02:27,760 --> 01:02:31,119
was back in 1933

1735
01:02:31,920 --> 01:02:34,640
this was amazing thank you all i think

1736
01:02:34,640 --> 01:02:36,079
one of the things that i'm walking away

1737
01:02:36,079 --> 01:02:39,039
with andrea from your your talk is

1738
01:02:39,039 --> 01:02:41,039
the idea that we have to be curious we

1739
01:02:41,039 --> 01:02:43,920
have to be proactive but we have to we

1740
01:02:43,920 --> 01:02:45,599
have to do no harm right we've got it

1741
01:02:45,599 --> 01:02:48,079
we've got to be as good as we can be

1742
01:02:48,079 --> 01:02:50,799
together in all of this so thank you all

1743
01:02:50,799 --> 01:02:52,559
again for those of you watching i hope

1744
01:02:52,559 --> 01:02:54,559
you are just about to jump in with a

1745
01:02:54,559 --> 01:02:56,319
million and one questions for this panel

1746
01:02:56,319 --> 01:02:59,280
because they are awesome and they are

1747
01:02:59,280 --> 01:03:01,680
fearless as you've seen and happy to to

1748
01:03:01,680 --> 01:03:03,200
take on this subject because we've all

1749
01:03:03,200 --> 01:03:04,640
got to be curious and we've all got to

1750
01:03:04,640 --> 01:03:06,160
figure this out together and we all have

1751
01:03:06,160 --> 01:03:08,400
to do good so thank you all so much for

1752
01:03:08,400 --> 01:03:10,240
coming today thank you to the panelists

1753
01:03:10,240 --> 01:03:13,359
and we will see you soon for q a

1754
01:03:13,359 --> 01:03:16,359
bye

1755
01:03:21,280 --> 01:03:23,359
you

