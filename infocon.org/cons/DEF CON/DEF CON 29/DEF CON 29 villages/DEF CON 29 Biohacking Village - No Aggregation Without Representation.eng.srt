1
00:00:02,639 --> 00:00:03,679
hello defcon

2
00:00:03,679 --> 00:00:06,560
my name is andrea downing and i'm really

3
00:00:06,560 --> 00:00:08,880
excited and honored to be back

4
00:00:08,880 --> 00:00:10,960
here at the biohacking village for my

5
00:00:10,960 --> 00:00:11,920
third year

6
00:00:11,920 --> 00:00:15,120
um today i'm going to be talking to you

7
00:00:15,120 --> 00:00:17,279
about predictive algorithms and how they

8
00:00:17,279 --> 00:00:18,400
affect your health

9
00:00:18,400 --> 00:00:21,600
and your future the name of my talk is

10
00:00:21,600 --> 00:00:24,720
no aggregation without representation

11
00:00:24,720 --> 00:00:27,359
so let's get started my talk has five

12
00:00:27,359 --> 00:00:28,640
parts

13
00:00:28,640 --> 00:00:31,679
why me why should you listen to me uh

14
00:00:31,679 --> 00:00:33,200
what i'm going to talk about predictive

15
00:00:33,200 --> 00:00:35,040
power of health data harm and threat

16
00:00:35,040 --> 00:00:37,040
models hacking algorithms

17
00:00:37,040 --> 00:00:38,800
and how we think and act differently to

18
00:00:38,800 --> 00:00:40,960
protect patients

19
00:00:40,960 --> 00:00:43,600
i want to start by dedicating my talk to

20
00:00:43,600 --> 00:00:44,320
a friend

21
00:00:44,320 --> 00:00:47,200
and fellow disability rights activist

22
00:00:47,200 --> 00:00:48,559
aaron gilmer

23
00:00:48,559 --> 00:00:51,760
who took her own life not long ago

24
00:00:51,760 --> 00:00:55,039
while she can't be here today to

25
00:00:55,039 --> 00:00:57,039
raise her voice i thought i would start

26
00:00:57,039 --> 00:00:59,280
with her words

27
00:00:59,280 --> 00:01:02,960
she said this in 2014 i come from the

28
00:01:02,960 --> 00:01:03,680
privilege

29
00:01:03,680 --> 00:01:06,479
of being an educated white woman and a

30
00:01:06,479 --> 00:01:07,280
background

31
00:01:07,280 --> 00:01:09,119
in a culture where it's okay to question

32
00:01:09,119 --> 00:01:11,439
and fight for what i want

33
00:01:11,439 --> 00:01:13,920
i know the correct jargon and after so

34
00:01:13,920 --> 00:01:15,280
much time in hospitals

35
00:01:15,280 --> 00:01:18,159
i've have enough knowledge to ask for

36
00:01:18,159 --> 00:01:20,799
and fight for my care

37
00:01:20,799 --> 00:01:23,200
some patients die trying to get food

38
00:01:23,200 --> 00:01:25,520
medicine housing and medical care

39
00:01:25,520 --> 00:01:27,200
if you don't die along the way you

40
00:01:27,200 --> 00:01:28,720
honestly wish you could because it's

41
00:01:28,720 --> 00:01:30,479
also exhausting frustrating and

42
00:01:30,479 --> 00:01:33,039
degrading

43
00:01:34,079 --> 00:01:36,400
i may not speak for all patient

44
00:01:36,400 --> 00:01:38,320
communities when i talk about

45
00:01:38,320 --> 00:01:41,040
predicting your health and your future

46
00:01:41,040 --> 00:01:41,920
and i might

47
00:01:41,920 --> 00:01:44,799
might not represent the entire patient

48
00:01:44,799 --> 00:01:47,920
population or the diversity of it

49
00:01:47,920 --> 00:01:51,040
but my own origin story and path

50
00:01:51,040 --> 00:01:54,880
to becoming a hacker and starting

51
00:01:54,880 --> 00:01:58,799
on this road that got me to defcon

52
00:01:58,799 --> 00:02:01,520
began when i was very young and

53
00:02:01,520 --> 00:02:02,399
predicting my

54
00:02:02,399 --> 00:02:06,000
own future around cancer

55
00:02:06,000 --> 00:02:08,318
i have a lot to say about one gene and

56
00:02:08,318 --> 00:02:09,758
the history of that gene

57
00:02:09,758 --> 00:02:12,080
what it tells us about predicting your

58
00:02:12,080 --> 00:02:13,680
health

59
00:02:13,680 --> 00:02:16,319
i never wanted to be on this path i

60
00:02:16,319 --> 00:02:17,120
first

61
00:02:17,120 --> 00:02:19,599
learned about the power of data to

62
00:02:19,599 --> 00:02:20,160
predict

63
00:02:20,160 --> 00:02:23,520
my own future when i was 25 and i share

64
00:02:23,520 --> 00:02:24,640
this in every talk

65
00:02:24,640 --> 00:02:27,599
i have a bug in my code i have a brca1

66
00:02:27,599 --> 00:02:28,720
mutation

67
00:02:28,720 --> 00:02:32,480
what that means is my genetic counselor

68
00:02:32,480 --> 00:02:35,120
sat me down when i was 25 years old and

69
00:02:35,120 --> 00:02:36,080
told me that i had

70
00:02:36,080 --> 00:02:38,879
up to an 87 chance of developing breast

71
00:02:38,879 --> 00:02:39,760
cancer

72
00:02:39,760 --> 00:02:42,000
and up to a 60 chance of developing

73
00:02:42,000 --> 00:02:44,319
ovarian cancer in my lifetime

74
00:02:44,319 --> 00:02:46,959
that's me getting surgery one of seven

75
00:02:46,959 --> 00:02:47,599
surgeries

76
00:02:47,599 --> 00:02:50,400
up at the top left and my first foray

77
00:02:50,400 --> 00:02:52,160
into data sharing

78
00:02:52,160 --> 00:02:54,319
and disability rights starts at the

79
00:02:54,319 --> 00:02:55,920
bottom right here at the steps of the

80
00:02:55,920 --> 00:02:56,959
supreme court

81
00:02:56,959 --> 00:02:58,959
when i served as one of the spokespeople

82
00:02:58,959 --> 00:03:01,040
a media spokesperson for

83
00:03:01,040 --> 00:03:03,440
a plaintiff in a case that went to the

84
00:03:03,440 --> 00:03:05,360
supreme court on whether human genes

85
00:03:05,360 --> 00:03:07,120
could be patented

86
00:03:07,120 --> 00:03:10,720
the case and and the genes in question

87
00:03:10,720 --> 00:03:12,879
were brca 1 and 2 where i

88
00:03:12,879 --> 00:03:15,599
had a genetic mutation and the company

89
00:03:15,599 --> 00:03:16,480
that had done

90
00:03:16,480 --> 00:03:18,800
my genetic testing had a patent on those

91
00:03:18,800 --> 00:03:20,239
genes

92
00:03:20,239 --> 00:03:23,120
before that case over 40 percent of the

93
00:03:23,120 --> 00:03:25,120
human genome had been patented

94
00:03:25,120 --> 00:03:27,760
and after that case the access to

95
00:03:27,760 --> 00:03:29,200
genetic testing

96
00:03:29,200 --> 00:03:32,720
and genomic text technologies exploded

97
00:03:32,720 --> 00:03:35,360
and we really are still only at the tip

98
00:03:35,360 --> 00:03:37,599
of the iceberg when it comes to

99
00:03:37,599 --> 00:03:41,440
understanding the power of genomics

100
00:03:41,440 --> 00:03:45,280
my path took an interesting turn

101
00:03:45,280 --> 00:03:48,480
in 2018 after cambridge analytica

102
00:03:48,480 --> 00:03:51,120
where i became an accidental hacker

103
00:03:51,120 --> 00:03:52,000
asking myself

104
00:03:52,000 --> 00:03:54,560
a simple question about the privacy

105
00:03:54,560 --> 00:03:56,400
implications of having

106
00:03:56,400 --> 00:03:59,680
a support group of genetic mutation

107
00:03:59,680 --> 00:04:02,959
carriers on facebook

108
00:04:02,959 --> 00:04:05,200
that's a whole other talk my first talk

109
00:04:05,200 --> 00:04:07,280
at def con in 2019 if you want to go

110
00:04:07,280 --> 00:04:09,280
check it out it's there

111
00:04:09,280 --> 00:04:11,760
and all i want to say here is from that

112
00:04:11,760 --> 00:04:13,280
experience

113
00:04:13,280 --> 00:04:15,599
i found other patients and this

114
00:04:15,599 --> 00:04:17,199
incredible community here

115
00:04:17,199 --> 00:04:20,160
which i'm very thankful for the other

116
00:04:20,160 --> 00:04:22,639
patients around me who have

117
00:04:22,639 --> 00:04:24,960
you know sought a path forward are

118
00:04:24,960 --> 00:04:28,639
working to advance our rights but also

119
00:04:28,639 --> 00:04:31,759
fairer representation in the systems and

120
00:04:31,759 --> 00:04:32,720
technology

121
00:04:32,720 --> 00:04:35,040
and data that affects our lives our

122
00:04:35,040 --> 00:04:37,199
health and our future

123
00:04:37,199 --> 00:04:38,720
we are a non-profit called the light

124
00:04:38,720 --> 00:04:40,960
collective and i'll talk to you a little

125
00:04:40,960 --> 00:04:43,360
bit more about our work at the end

126
00:04:43,360 --> 00:04:46,639
but first let's go into uh what my talk

127
00:04:46,639 --> 00:04:47,759
is about

128
00:04:47,759 --> 00:04:50,400
predictive algorithms are everywhere not

129
00:04:50,400 --> 00:04:52,000
only in healthcare

130
00:04:52,000 --> 00:04:55,600
but increasingly it they exist in ways

131
00:04:55,600 --> 00:04:56,080
that

132
00:04:56,080 --> 00:04:57,919
impact our health that we may not

133
00:04:57,919 --> 00:04:59,759
understand or realize because

134
00:04:59,759 --> 00:05:03,199
the choices aren't apparent to us

135
00:05:03,199 --> 00:05:06,320
we don't truly understand as we walk

136
00:05:06,320 --> 00:05:09,440
through our day and on the internet some

137
00:05:09,440 --> 00:05:11,840
of the ways in which we can be targeted

138
00:05:11,840 --> 00:05:14,080
or how our identities and our futures

139
00:05:14,080 --> 00:05:18,159
can be affected by predictive algorithms

140
00:05:18,240 --> 00:05:20,960
increasingly the choices that we have

141
00:05:20,960 --> 00:05:23,280
and the way we're treated both inside

142
00:05:23,280 --> 00:05:24,000
and outside

143
00:05:24,000 --> 00:05:26,479
the walls of a clinic are dependent on

144
00:05:26,479 --> 00:05:28,240
the data that we generate

145
00:05:28,240 --> 00:05:30,160
and the choices that predictive

146
00:05:30,160 --> 00:05:32,880
algorithms make about us

147
00:05:32,880 --> 00:05:35,199
clinicians develop predictive algorithms

148
00:05:35,199 --> 00:05:36,800
by studying health outcomes

149
00:05:36,800 --> 00:05:39,280
of patient populations over time that

150
00:05:39,280 --> 00:05:40,080
would be the

151
00:05:40,080 --> 00:05:43,039
like five second explanation of

152
00:05:43,039 --> 00:05:45,039
predictive algorithms

153
00:05:45,039 --> 00:05:47,280
and you know that's very basic it's

154
00:05:47,280 --> 00:05:49,280
obviously a lot more complex than that

155
00:05:49,280 --> 00:05:50,320
when we think about

156
00:05:50,320 --> 00:05:52,880
informed medical decision making

157
00:05:52,880 --> 00:05:55,520
clinical grade diagnostics

158
00:05:55,520 --> 00:05:57,680
and making sure that those things are

159
00:05:57,680 --> 00:06:00,400
good and trustworthy for care

160
00:06:00,400 --> 00:06:02,240
when we're seeking knowledge to make

161
00:06:02,240 --> 00:06:03,919
informed decisions

162
00:06:03,919 --> 00:06:06,960
how are those choices really our own my

163
00:06:06,960 --> 00:06:08,800
view on this

164
00:06:08,800 --> 00:06:11,360
from the cheap seats after 15 years of

165
00:06:11,360 --> 00:06:12,560
lived experience

166
00:06:12,560 --> 00:06:16,479
navigating my own future and my destiny

167
00:06:16,479 --> 00:06:17,520
based on genetic

168
00:06:17,520 --> 00:06:20,400
information is that your choices should

169
00:06:20,400 --> 00:06:20,800
be

170
00:06:20,800 --> 00:06:23,919
your own and what you don't know may

171
00:06:23,919 --> 00:06:26,479
kill you

172
00:06:27,520 --> 00:06:30,160
i'm going to talk about black boxes

173
00:06:30,160 --> 00:06:31,759
black box predictions

174
00:06:31,759 --> 00:06:34,400
about our health are killing us in ways

175
00:06:34,400 --> 00:06:35,840
we don't know

176
00:06:35,840 --> 00:06:39,199
and the examples here emerge in four key

177
00:06:39,199 --> 00:06:39,919
themes

178
00:06:39,919 --> 00:06:43,039
patient safety inaccuracy and bias

179
00:06:43,039 --> 00:06:45,520
privacy and security and lack of digital

180
00:06:45,520 --> 00:06:47,039
rights

181
00:06:47,039 --> 00:06:50,400
let me start with emrs epic has

182
00:06:50,400 --> 00:06:53,039
about 20 proprietary algorithms designed

183
00:06:53,039 --> 00:06:54,639
to predict things like

184
00:06:54,639 --> 00:06:56,240
how long a patient might stay in the

185
00:06:56,240 --> 00:06:57,840
hospital

186
00:06:57,840 --> 00:07:00,880
how you might get care and one specific

187
00:07:00,880 --> 00:07:02,000
example of a predictive

188
00:07:02,000 --> 00:07:04,960
algorithm is your chances of developing

189
00:07:04,960 --> 00:07:08,560
a serious condition called sepsis

190
00:07:08,560 --> 00:07:11,199
well this example i'm calling out and i

191
00:07:11,199 --> 00:07:12,960
want to just say that

192
00:07:12,960 --> 00:07:15,199
epic isn't the only emr with predictive

193
00:07:15,199 --> 00:07:17,120
algorithms this is an industry-wide

194
00:07:17,120 --> 00:07:17,680
practice

195
00:07:17,680 --> 00:07:20,080
and this just happens to be an example

196
00:07:20,080 --> 00:07:22,720
that just hit the news so i'm

197
00:07:22,720 --> 00:07:26,160
giving that here an external validation

198
00:07:26,160 --> 00:07:28,880
cohort study looked at epic's

199
00:07:28,880 --> 00:07:29,759
proprietary

200
00:07:29,759 --> 00:07:32,720
algorithm for sepsis and they found that

201
00:07:32,720 --> 00:07:33,759
67

202
00:07:33,759 --> 00:07:36,960
of patients with sepsis were not

203
00:07:36,960 --> 00:07:38,479
identified based on this predictive

204
00:07:38,479 --> 00:07:39,759
algorithm looking at the wider

205
00:07:39,759 --> 00:07:41,599
population

206
00:07:41,599 --> 00:07:44,560
so that's very interesting when we have

207
00:07:44,560 --> 00:07:46,560
a black box making decisions about

208
00:07:46,560 --> 00:07:47,919
you know are you going to stay in the

209
00:07:47,919 --> 00:07:50,000
hospital longer or not or what's your

210
00:07:50,000 --> 00:07:52,879
risk of readmission or developing sepsis

211
00:07:52,879 --> 00:07:55,039
you might not be given the same care as

212
00:07:55,039 --> 00:07:56,800
somebody else based on the data that is

213
00:07:56,800 --> 00:07:58,479
generated about you

214
00:07:58,479 --> 00:08:00,960
what are the sources of that data how is

215
00:08:00,960 --> 00:08:01,680
it accurate

216
00:08:01,680 --> 00:08:04,400
and how is it worthy of decision making

217
00:08:04,400 --> 00:08:05,680
that is informed by

218
00:08:05,680 --> 00:08:09,120
you and your doctor another example on

219
00:08:09,120 --> 00:08:10,319
the right here

220
00:08:10,319 --> 00:08:13,280
is something many may be familiar with

221
00:08:13,280 --> 00:08:15,599
the opioid epidemic

222
00:08:15,599 --> 00:08:17,360
what a lot of people don't realize is

223
00:08:17,360 --> 00:08:19,520
the opioid epidemic over the last

224
00:08:19,520 --> 00:08:22,800
two decades has cost here in the united

225
00:08:22,800 --> 00:08:23,680
states

226
00:08:23,680 --> 00:08:27,280
600 000 lives how did it start

227
00:08:27,280 --> 00:08:30,080
aggressive marketing while underplaying

228
00:08:30,080 --> 00:08:30,879
the risk

229
00:08:30,879 --> 00:08:33,919
of overdose and addiction to drugs in

230
00:08:33,919 --> 00:08:34,640
the middle

231
00:08:34,640 --> 00:08:38,240
of the opioid crisis a silicon valley

232
00:08:38,240 --> 00:08:40,000
startup had a really fantastic

233
00:08:40,000 --> 00:08:42,399
business model idea they were called

234
00:08:42,399 --> 00:08:44,240
practice fusion

235
00:08:44,240 --> 00:08:47,360
and the company developed an electronic

236
00:08:47,360 --> 00:08:49,519
medical record system for doctors and

237
00:08:49,519 --> 00:08:50,160
instead of

238
00:08:50,160 --> 00:08:52,399
charging for the software like their

239
00:08:52,399 --> 00:08:53,600
competitors

240
00:08:53,600 --> 00:08:55,839
the company generated the bulk of its

241
00:08:55,839 --> 00:08:59,040
revenue by advertising to doctors

242
00:08:59,040 --> 00:09:01,839
specifically practice fusion solicited a

243
00:09:01,839 --> 00:09:03,360
one million dollar

244
00:09:03,360 --> 00:09:06,800
uh payment from opioid companies to

245
00:09:06,800 --> 00:09:09,120
create alert systems that would

246
00:09:09,120 --> 00:09:11,120
encourage doctors to prescribe

247
00:09:11,120 --> 00:09:14,160
extended release opioids with two

248
00:09:14,160 --> 00:09:15,440
patients

249
00:09:15,440 --> 00:09:18,320
this has since been busted and is in a

250
00:09:18,320 --> 00:09:19,120
huge

251
00:09:19,120 --> 00:09:21,600
criminal case the doj is involved it's

252
00:09:21,600 --> 00:09:23,360
really messy and ugly

253
00:09:23,360 --> 00:09:26,800
and the maybe the result of that here

254
00:09:26,800 --> 00:09:30,320
is maybe it's a bad idea to um

255
00:09:30,320 --> 00:09:33,040
have drug companies as a business model

256
00:09:33,040 --> 00:09:33,600
um

257
00:09:33,600 --> 00:09:36,640
share with dr c and choose or predict

258
00:09:36,640 --> 00:09:39,439
about a patient

259
00:09:40,240 --> 00:09:43,040
and that's all i have to say about that

260
00:09:43,040 --> 00:09:45,120
how are we solving

261
00:09:45,120 --> 00:09:48,000
the opioid crisis well here's a shining

262
00:09:48,000 --> 00:09:48,800
example

263
00:09:48,800 --> 00:09:51,440
you may be surprised you may not be

264
00:09:51,440 --> 00:09:53,200
predictive algorithms

265
00:09:53,200 --> 00:09:56,720
this is a fantastic piece by formerly

266
00:09:56,720 --> 00:09:57,440
political

267
00:09:57,440 --> 00:10:00,880
reporter mahonda mohanna ravindranav

268
00:10:00,880 --> 00:10:03,200
now the interesting thing here is it's

269
00:10:03,200 --> 00:10:05,360
not based on just clinical or emr

270
00:10:05,360 --> 00:10:08,480
or study data in an irb approved study

271
00:10:08,480 --> 00:10:10,640
without your consent what's happening is

272
00:10:10,640 --> 00:10:12,160
data aggregators

273
00:10:12,160 --> 00:10:16,240
are buying up uh data about the food you

274
00:10:16,240 --> 00:10:16,720
eat

275
00:10:16,720 --> 00:10:19,279
the time you spend watching tv um the

276
00:10:19,279 --> 00:10:20,959
kinds of medications you've been just

277
00:10:20,959 --> 00:10:25,200
prescribed before and and they are

278
00:10:25,200 --> 00:10:29,120
mixing that up into a soup of prediction

279
00:10:29,120 --> 00:10:32,160
and then selling this to

280
00:10:32,160 --> 00:10:34,720
providers in order to predict your risk

281
00:10:34,720 --> 00:10:36,320
this is unregulated

282
00:10:36,320 --> 00:10:38,560
and it is used without patient knowledge

283
00:10:38,560 --> 00:10:39,839
or consent

284
00:10:39,839 --> 00:10:41,680
this is tammy dobbs she lives in the

285
00:10:41,680 --> 00:10:43,120
state of arkansas

286
00:10:43,120 --> 00:10:46,240
and relies on out-of-home care for years

287
00:10:46,240 --> 00:10:47,760
the state of arkansas gave

288
00:10:47,760 --> 00:10:50,959
tammy 56 hours of care per week to do

289
00:10:50,959 --> 00:10:52,320
basic tasks like

290
00:10:52,320 --> 00:10:54,160
getting out of bed going to the bathroom

291
00:10:54,160 --> 00:10:56,240
making meals and cleaning

292
00:10:56,240 --> 00:10:58,959
then in 2016 the state implemented an

293
00:10:58,959 --> 00:11:01,600
algorithm that decided that tammy could

294
00:11:01,600 --> 00:11:02,320
only get

295
00:11:02,320 --> 00:11:05,760
36 hours of care she had to ration

296
00:11:05,760 --> 00:11:08,720
that care and decide and schedule when

297
00:11:08,720 --> 00:11:10,320
she was going to

298
00:11:10,320 --> 00:11:14,320
go to the bathroom and eat and in this

299
00:11:14,320 --> 00:11:16,160
specific case

300
00:11:16,160 --> 00:11:19,680
the the designed algorithm was so wildly

301
00:11:19,680 --> 00:11:22,160
irrational that not even the developer

302
00:11:22,160 --> 00:11:25,519
understood how and why the algorithm

303
00:11:25,519 --> 00:11:28,560
made the recommendations that it did

304
00:11:28,560 --> 00:11:31,680
and what does that leave patients when

305
00:11:31,680 --> 00:11:33,519
something like this happens

306
00:11:33,519 --> 00:11:36,000
it leaves us with loss of care loss of

307
00:11:36,000 --> 00:11:37,120
benefits

308
00:11:37,120 --> 00:11:40,240
uh inability to get food back to

309
00:11:40,240 --> 00:11:42,160
the the the words of aaron in the

310
00:11:42,160 --> 00:11:43,760
beginning of my talk

311
00:11:43,760 --> 00:11:46,959
um and just ways that

312
00:11:46,959 --> 00:11:50,240
these systems can be used to

313
00:11:50,240 --> 00:11:52,720
charge you the most amount of money in

314
00:11:52,720 --> 00:11:54,839
order to give you the least amount of

315
00:11:54,839 --> 00:11:57,680
care

316
00:11:57,680 --> 00:11:59,519
now let me take it back to my little

317
00:11:59,519 --> 00:12:01,440
domain that i know a lot about which is

318
00:12:01,440 --> 00:12:02,560
brca

319
00:12:02,560 --> 00:12:04,480
that happens to be one of the most

320
00:12:04,480 --> 00:12:07,279
studied genes on the human genome now

321
00:12:07,279 --> 00:12:10,480
if you remember i told you those 87

322
00:12:10,480 --> 00:12:13,440
and 60 like super high scary numbers uh

323
00:12:13,440 --> 00:12:14,880
back in the beginning when i was told my

324
00:12:14,880 --> 00:12:16,399
genetic risk

325
00:12:16,399 --> 00:12:18,160
and you have to keep in mind when we

326
00:12:18,160 --> 00:12:20,800
talk about informed medical decisions

327
00:12:20,800 --> 00:12:22,480
that was like a death sentence to me at

328
00:12:22,480 --> 00:12:24,000
25 years old

329
00:12:24,000 --> 00:12:26,800
and a lot of my path and my identity was

330
00:12:26,800 --> 00:12:27,360
has been

331
00:12:27,360 --> 00:12:30,560
shaped by that sense and what i've

332
00:12:30,560 --> 00:12:31,519
wanted

333
00:12:31,519 --> 00:12:34,160
are better options not only for myself

334
00:12:34,160 --> 00:12:36,079
and my future but my family and my

335
00:12:36,079 --> 00:12:37,680
community

336
00:12:37,680 --> 00:12:41,040
and since the the patents were

337
00:12:41,040 --> 00:12:43,279
overturned in 2013

338
00:12:43,279 --> 00:12:46,480
i mentioned that we've expanded access

339
00:12:46,480 --> 00:12:48,399
on genomic testing and all kinds of

340
00:12:48,399 --> 00:12:51,040
technologies and we're learning about

341
00:12:51,040 --> 00:12:53,040
new predictive algorithms that are very

342
00:12:53,040 --> 00:12:54,480
interesting but still need to be

343
00:12:54,480 --> 00:12:55,440
validated

344
00:12:55,440 --> 00:12:57,839
in order for them to be worthy of

345
00:12:57,839 --> 00:12:59,839
clinical grade decision making

346
00:12:59,839 --> 00:13:01,839
so these are in early stages when you

347
00:13:01,839 --> 00:13:03,680
think about

348
00:13:03,680 --> 00:13:06,560
those over 60 like scientists already

349
00:13:06,560 --> 00:13:06,880
now

350
00:13:06,880 --> 00:13:10,000
know there are 66 000

351
00:13:10,000 --> 00:13:13,040
variants on brca1 and 2 alone and the

352
00:13:13,040 --> 00:13:15,040
way that we classify those variants are

353
00:13:15,040 --> 00:13:16,160
some are

354
00:13:16,160 --> 00:13:19,120
harmful or gonna kill you or you might

355
00:13:19,120 --> 00:13:20,480
develop cancer i don't want to scare

356
00:13:20,480 --> 00:13:21,279
anybody

357
00:13:21,279 --> 00:13:23,839
some are totally benign and then in the

358
00:13:23,839 --> 00:13:24,959
middle there are variants

359
00:13:24,959 --> 00:13:27,920
of uncertain significance and those

360
00:13:27,920 --> 00:13:30,079
variants of uncertain significance

361
00:13:30,079 --> 00:13:32,399
some of them are so rare that we will

362
00:13:32,399 --> 00:13:34,240
never have enough data in the population

363
00:13:34,240 --> 00:13:34,720
to make

364
00:13:34,720 --> 00:13:36,399
accurate predictions but they can still

365
00:13:36,399 --> 00:13:38,240
be harmful the way that we are

366
00:13:38,240 --> 00:13:39,760
predicting these things

367
00:13:39,760 --> 00:13:43,440
are looking at for example in silico

368
00:13:43,440 --> 00:13:44,720
prior prediction

369
00:13:44,720 --> 00:13:47,440
um to be considered with other evidence

370
00:13:47,440 --> 00:13:48,160
in

371
00:13:48,160 --> 00:13:50,720
partnership with your clinician or

372
00:13:50,720 --> 00:13:52,639
thinly functional scores and these are

373
00:13:52,639 --> 00:13:53,279
all like

374
00:13:53,279 --> 00:13:55,600
where on the protein did the mutation

375
00:13:55,600 --> 00:13:56,800
happen or

376
00:13:56,800 --> 00:13:59,519
if you look at model organisms and that

377
00:13:59,519 --> 00:14:00,639
organism has

378
00:14:00,639 --> 00:14:03,360
the mutation did they all develop cancer

379
00:14:03,360 --> 00:14:05,199
like those are other ways that you can

380
00:14:05,199 --> 00:14:06,320
look at

381
00:14:06,320 --> 00:14:08,240
prediction of a health outcome without

382
00:14:08,240 --> 00:14:11,839
actually looking at the population

383
00:14:12,240 --> 00:14:16,000
this is dr lynette hammond jorito and

384
00:14:16,000 --> 00:14:17,600
valencia robinson

385
00:14:17,600 --> 00:14:19,680
lynette on the left the lindsay on the

386
00:14:19,680 --> 00:14:22,079
right not pictured is tia tomlin

387
00:14:22,079 --> 00:14:25,199
founder of my style matters in atlanta

388
00:14:25,199 --> 00:14:27,839
and i want to share this as a good

389
00:14:27,839 --> 00:14:29,600
example of

390
00:14:29,600 --> 00:14:33,120
patients representing themselves

391
00:14:33,120 --> 00:14:36,079
in partnership with a researcher to

392
00:14:36,079 --> 00:14:37,279
tackle the problem

393
00:14:37,279 --> 00:14:40,480
of genomic disparities in brca

394
00:14:40,480 --> 00:14:43,519
and data sharing what lynette

395
00:14:43,519 --> 00:14:45,920
has done in this initial study is look

396
00:14:45,920 --> 00:14:47,519
at the many layers of the cancer

397
00:14:47,519 --> 00:14:49,519
disparities problem

398
00:14:49,519 --> 00:14:52,160
we know from prior studies that african

399
00:14:52,160 --> 00:14:54,880
american women are 39 to 44

400
00:14:54,880 --> 00:14:56,800
more likely to die from breast cancer

401
00:14:56,800 --> 00:14:58,800
than white women

402
00:14:58,800 --> 00:15:01,040
well there are many layers to that

403
00:15:01,040 --> 00:15:04,320
problem including lack of access to care

404
00:15:04,320 --> 00:15:08,320
systemic racism in the healthcare system

405
00:15:08,320 --> 00:15:10,399
but i want to talk about how genetic

406
00:15:10,399 --> 00:15:11,600
testing doesn't work

407
00:15:11,600 --> 00:15:14,720
for people of color because those

408
00:15:14,720 --> 00:15:18,160
genes were patented in 2013 what

409
00:15:18,160 --> 00:15:21,600
we knew at the time what what clinicians

410
00:15:21,600 --> 00:15:24,839
and and the guidelines at the time were

411
00:15:24,839 --> 00:15:26,000
that

412
00:15:26,000 --> 00:15:28,880
brca mainly affects ashkenazi jewish

413
00:15:28,880 --> 00:15:30,000
women it's breast

414
00:15:30,000 --> 00:15:33,600
and ovarian cancer those are the um like

415
00:15:33,600 --> 00:15:36,639
very very extreme cases of cancer risk

416
00:15:36,639 --> 00:15:37,680
when it comes to

417
00:15:37,680 --> 00:15:41,519
the population therefore only

418
00:15:41,519 --> 00:15:44,800
people with those certain markers were

419
00:15:44,800 --> 00:15:47,759
able to get tested so back in 2005 there

420
00:15:47,759 --> 00:15:49,519
were just so many hubs that you had to

421
00:15:49,519 --> 00:15:51,040
jump through to get

422
00:15:51,040 --> 00:15:52,800
insurance coverage for a four thousand

423
00:15:52,800 --> 00:15:54,480
dollar test and

424
00:15:54,480 --> 00:15:56,959
to get into a genetic counselor that was

425
00:15:56,959 --> 00:15:59,279
a six week wait time

426
00:15:59,279 --> 00:16:02,079
now over time what happened was people

427
00:16:02,079 --> 00:16:04,480
with less access to care

428
00:16:04,480 --> 00:16:07,680
um were not part of that original data

429
00:16:07,680 --> 00:16:09,199
set or the development of those

430
00:16:09,199 --> 00:16:10,639
algorithms

431
00:16:10,639 --> 00:16:13,519
and now what we have is a problem where

432
00:16:13,519 --> 00:16:14,959
genomic

433
00:16:14,959 --> 00:16:17,839
testing genetic testing for brca

434
00:16:17,839 --> 00:16:19,440
literally does not work

435
00:16:19,440 --> 00:16:23,120
for black people and that is because

436
00:16:23,120 --> 00:16:25,519
the the systems were that you know the

437
00:16:25,519 --> 00:16:26,560
way that we have

438
00:16:26,560 --> 00:16:28,480
developed genetic testing is on

439
00:16:28,480 --> 00:16:31,120
reference genomes for

440
00:16:31,120 --> 00:16:34,480
white people so we need data

441
00:16:34,480 --> 00:16:38,320
and we need algorithms that represent

442
00:16:38,320 --> 00:16:41,199
diverse populations and my thesis here

443
00:16:41,199 --> 00:16:42,880
looking at this example

444
00:16:42,880 --> 00:16:45,279
is that needs to be led and supported

445
00:16:45,279 --> 00:16:47,040
institutional support

446
00:16:47,040 --> 00:16:49,440
for researchers and patient community

447
00:16:49,440 --> 00:16:50,399
advocates

448
00:16:50,399 --> 00:16:53,519
like lynette like valencia and lake tia

449
00:16:53,519 --> 00:16:56,320
leading the way because that's the only

450
00:16:56,320 --> 00:16:58,880
way we are going to earn and build trust

451
00:16:58,880 --> 00:17:02,839
this is their time to lead and it's very

452
00:17:02,839 --> 00:17:05,280
exciting

453
00:17:05,280 --> 00:17:07,599
i promised i talked about hectic hacking

454
00:17:07,599 --> 00:17:10,880
uh predictive algorithms so here you go

455
00:17:10,880 --> 00:17:12,559
it's really not that hard you think of

456
00:17:12,559 --> 00:17:14,160
it like oh my gosh

457
00:17:14,160 --> 00:17:16,559
you know you gotta do the thing and and

458
00:17:16,559 --> 00:17:17,760
get into the system

459
00:17:17,760 --> 00:17:20,559
and breach it um i don't even know if i

460
00:17:20,559 --> 00:17:22,000
said the right thing here but

461
00:17:22,000 --> 00:17:24,319
here's a beautiful example of simon

462
00:17:24,319 --> 00:17:25,280
weckert

463
00:17:25,280 --> 00:17:28,559
a a german artist who took a

464
00:17:28,559 --> 00:17:32,000
a wagon full of cell phones

465
00:17:32,000 --> 00:17:34,640
down the street in order to affect

466
00:17:34,640 --> 00:17:35,280
google's

467
00:17:35,280 --> 00:17:37,039
algorithm on whether or not there was

468
00:17:37,039 --> 00:17:38,400
traffic you know

469
00:17:38,400 --> 00:17:41,440
and i think this is a fantastic example

470
00:17:41,440 --> 00:17:42,799
well not in healthcare

471
00:17:42,799 --> 00:17:46,000
it applies these barriers to entry and

472
00:17:46,000 --> 00:17:47,919
how to hack a predictive algorithm are

473
00:17:47,919 --> 00:17:49,520
pretty low

474
00:17:49,520 --> 00:17:52,240
like you just give it you just feed a

475
00:17:52,240 --> 00:17:53,679
bad data

476
00:17:53,679 --> 00:17:57,120
there you go so up until now i've really

477
00:17:57,120 --> 00:17:58,480
focused on

478
00:17:58,480 --> 00:18:01,360
algorithmic bias where testing works and

479
00:18:01,360 --> 00:18:03,679
where it doesn't

480
00:18:03,679 --> 00:18:07,360
and when we lose care and we lose trust

481
00:18:07,360 --> 00:18:08,799
and we fall through the cracks of the

482
00:18:08,799 --> 00:18:10,480
health care system

483
00:18:10,480 --> 00:18:14,160
where do patients and the public go they

484
00:18:14,160 --> 00:18:16,640
go to social media

485
00:18:16,640 --> 00:18:18,400
when patients fall through the cracks of

486
00:18:18,400 --> 00:18:20,080
the health care system

487
00:18:20,080 --> 00:18:22,640
they turn to social media for help

488
00:18:22,640 --> 00:18:24,240
support and resources

489
00:18:24,240 --> 00:18:26,320
and i'm telling you this because i did

490
00:18:26,320 --> 00:18:28,320
it in 2013

491
00:18:28,320 --> 00:18:30,160
when i was going through my surgeries i

492
00:18:30,160 --> 00:18:32,720
found a beautiful brave support group

493
00:18:32,720 --> 00:18:34,720
before the days of disinformation and it

494
00:18:34,720 --> 00:18:36,480
was a lifeline for me

495
00:18:36,480 --> 00:18:38,960
finding people with a shared identity

496
00:18:38,960 --> 00:18:40,640
and a health condition

497
00:18:40,640 --> 00:18:43,600
is one of the most important parts it

498
00:18:43,600 --> 00:18:44,640
has been for me

499
00:18:44,640 --> 00:18:47,600
and many studies on this show that it is

500
00:18:47,600 --> 00:18:48,559
a really

501
00:18:48,559 --> 00:18:51,200
important part of the healing process

502
00:18:51,200 --> 00:18:51,600
and

503
00:18:51,600 --> 00:18:53,039
the question becomes how do we guide

504
00:18:53,039 --> 00:18:54,320
that and do it in a way that isn't

505
00:18:54,320 --> 00:18:55,520
exploiting people

506
00:18:55,520 --> 00:18:59,520
or harming and abusing their privacy

507
00:18:59,760 --> 00:19:01,600
and what happens outside the walls of a

508
00:19:01,600 --> 00:19:02,880
health care system

509
00:19:02,880 --> 00:19:04,960
that researchers and clinicians often

510
00:19:04,960 --> 00:19:06,320
don't think about when they don't have a

511
00:19:06,320 --> 00:19:08,840
tick tock account

512
00:19:08,840 --> 00:19:11,919
well i will start with this

513
00:19:11,919 --> 00:19:14,320
fantastic study that was done in 2019

514
00:19:14,320 --> 00:19:15,520
evaluating

515
00:19:15,520 --> 00:19:18,160
the predictability of medical conditions

516
00:19:18,160 --> 00:19:20,400
in social media posts

517
00:19:20,400 --> 00:19:23,440
and this was in place it was uh

518
00:19:23,440 --> 00:19:27,360
roughly a n of 1 000

519
00:19:27,360 --> 00:19:30,559
and the post analyzed through natural

520
00:19:30,559 --> 00:19:32,240
language processing was roughly a

521
00:19:32,240 --> 00:19:34,720
million posts 949 000.

522
00:19:34,720 --> 00:19:37,280
and so what they did was they took all

523
00:19:37,280 --> 00:19:38,400
the posts they

524
00:19:38,400 --> 00:19:42,480
looked at um different types of medical

525
00:19:42,480 --> 00:19:45,760
conditions and correlated with um

526
00:19:45,760 --> 00:19:48,960
a way to predict risk of diabetes

527
00:19:48,960 --> 00:19:51,440
pregnancy anxiety psychosis everything

528
00:19:51,440 --> 00:19:52,640
down to

529
00:19:52,640 --> 00:19:54,720
really personal things like whether

530
00:19:54,720 --> 00:19:56,480
you're not whether or not you're likely

531
00:19:56,480 --> 00:19:58,160
to abuse alcohol

532
00:19:58,160 --> 00:20:00,160
whether not you have a digestive

533
00:20:00,160 --> 00:20:02,640
disorder

534
00:20:03,120 --> 00:20:05,840
and you have to keep in mind this is an

535
00:20:05,840 --> 00:20:06,480
irb

536
00:20:06,480 --> 00:20:09,280
approved study with consent from the

537
00:20:09,280 --> 00:20:09,760
people

538
00:20:09,760 --> 00:20:13,280
involved and the power

539
00:20:13,280 --> 00:20:15,520
of these predictive models are very

540
00:20:15,520 --> 00:20:16,880
early

541
00:20:16,880 --> 00:20:19,039
but facebook has a lot more data than

542
00:20:19,039 --> 00:20:20,000
that we know

543
00:20:20,000 --> 00:20:23,280
that when

544
00:20:23,280 --> 00:20:26,240
predictive analytics are used in ad

545
00:20:26,240 --> 00:20:29,120
targeting on social media

546
00:20:29,120 --> 00:20:32,240
pharma companies are able to target you

547
00:20:32,240 --> 00:20:34,000
target you based on your health

548
00:20:34,000 --> 00:20:35,360
conditions

549
00:20:35,360 --> 00:20:37,200
one might think great more effective

550
00:20:37,200 --> 00:20:39,039
marketing you can get to the right

551
00:20:39,039 --> 00:20:39,840
patient

552
00:20:39,840 --> 00:20:41,600
at the right time and we're really

553
00:20:41,600 --> 00:20:43,520
getting effective about micro targeting

554
00:20:43,520 --> 00:20:45,679
these populations

555
00:20:45,679 --> 00:20:47,600
but you have to keep in mind that if you

556
00:20:47,600 --> 00:20:51,039
go back to 2018 we warned facebook about

557
00:20:51,039 --> 00:20:54,480
the huge problems with their

558
00:20:54,480 --> 00:20:56,400
group architecture and the way that

559
00:20:56,400 --> 00:20:59,120
those were feeding recommendations into

560
00:20:59,120 --> 00:21:04,239
uh group recommendations and interests

561
00:21:04,559 --> 00:21:06,720
those things were basically swept under

562
00:21:06,720 --> 00:21:09,520
the rug and then a pandemic happen

563
00:21:09,520 --> 00:21:11,039
i want you to think back to that

564
00:21:11,039 --> 00:21:13,600
practice fusion example

565
00:21:13,600 --> 00:21:15,440
where they had a business model to

566
00:21:15,440 --> 00:21:17,120
target ads to physicians

567
00:21:17,120 --> 00:21:20,000
now this is what i'm showing you outside

568
00:21:20,000 --> 00:21:21,760
the walls of a clinic where we just go

569
00:21:21,760 --> 00:21:24,000
around the physicians and target

570
00:21:24,000 --> 00:21:26,960
directly to the patient and the key here

571
00:21:26,960 --> 00:21:28,000
is to know

572
00:21:28,000 --> 00:21:30,320
that there's no governance no digital

573
00:21:30,320 --> 00:21:32,559
governance that is protecting

574
00:21:32,559 --> 00:21:35,280
populations vulnerable populations from

575
00:21:35,280 --> 00:21:35,919
being

576
00:21:35,919 --> 00:21:39,600
micro-targeted these three ads are just

577
00:21:39,600 --> 00:21:41,440
three examples of what

578
00:21:41,440 --> 00:21:44,480
my community sees when we go on social

579
00:21:44,480 --> 00:21:45,440
media

580
00:21:45,440 --> 00:21:48,080
and there are thousands more of it you

581
00:21:48,080 --> 00:21:50,080
know these are just three examples of

582
00:21:50,080 --> 00:21:51,280
thousands

583
00:21:51,280 --> 00:21:54,400
you can target ads bourbon ads to

584
00:21:54,400 --> 00:21:55,520
alcoholics

585
00:21:55,520 --> 00:21:58,080
you can target snake oil and fake

586
00:21:58,080 --> 00:21:59,600
treatments to us

587
00:21:59,600 --> 00:22:03,439
you can give a

588
00:22:03,520 --> 00:22:06,159
send out a scam of dna based life

589
00:22:06,159 --> 00:22:08,320
insurance

590
00:22:08,320 --> 00:22:10,559
to people with genetic mutations so that

591
00:22:10,559 --> 00:22:12,080
you can deny them care

592
00:22:12,080 --> 00:22:14,799
and and that is actually a real thing

593
00:22:14,799 --> 00:22:15,440
that

594
00:22:15,440 --> 00:22:19,120
happened between july and november of

595
00:22:19,120 --> 00:22:19,919
2020

596
00:22:19,919 --> 00:22:21,919
the health care and pharmaceutical

597
00:22:21,919 --> 00:22:23,960
industry in the united states spent

598
00:22:23,960 --> 00:22:27,120
198 million dollars on facebook

599
00:22:27,120 --> 00:22:28,559
advertising

600
00:22:28,559 --> 00:22:31,840
instagram followed with 151 million

601
00:22:31,840 --> 00:22:35,200
and ad investments have just exploded

602
00:22:35,200 --> 00:22:37,360
when it comes to healthcare

603
00:22:37,360 --> 00:22:39,600
and direct to consumer pharma ads on

604
00:22:39,600 --> 00:22:40,799
facebook and

605
00:22:40,799 --> 00:22:43,440
other places on social media but we need

606
00:22:43,440 --> 00:22:44,559
to stop just like

607
00:22:44,559 --> 00:22:46,480
shifting risks down the road and saying

608
00:22:46,480 --> 00:22:47,600
to people

609
00:22:47,600 --> 00:22:49,440
oh well they should have known better

610
00:22:49,440 --> 00:22:51,360
than putting their health information

611
00:22:51,360 --> 00:22:54,640
out there on the internet communities

612
00:22:54,640 --> 00:22:55,280
rely

613
00:22:55,280 --> 00:22:58,240
on old and outdated tactics when we're

614
00:22:58,240 --> 00:23:00,720
responding to misinformation

615
00:23:00,720 --> 00:23:02,799
you know i have witnessed it time and

616
00:23:02,799 --> 00:23:04,720
again from the world health organization

617
00:23:04,720 --> 00:23:05,200
to the

618
00:23:05,200 --> 00:23:09,039
cdc to hospitals

619
00:23:09,039 --> 00:23:11,600
putting up faqs on their website

620
00:23:11,600 --> 00:23:12,400
download this

621
00:23:12,400 --> 00:23:14,640
yet another app so you can learn about

622
00:23:14,640 --> 00:23:16,080
medical misinformation

623
00:23:16,080 --> 00:23:18,799
and get educated and yay we're going to

624
00:23:18,799 --> 00:23:20,320
be experts teaching you

625
00:23:20,320 --> 00:23:23,039
and that's not working obviously we're

626
00:23:23,039 --> 00:23:24,400
in a place where

627
00:23:24,400 --> 00:23:26,080
that's pretty much played out and we

628
00:23:26,080 --> 00:23:29,200
need to try some different tactics

629
00:23:29,200 --> 00:23:32,000
when i talk in just towards the end of

630
00:23:32,000 --> 00:23:33,760
this presentation i'm going to give you

631
00:23:33,760 --> 00:23:37,039
a few examples of what i think may work

632
00:23:37,039 --> 00:23:39,360
what we don't see is also a lot more

633
00:23:39,360 --> 00:23:40,880
dangerous

634
00:23:40,880 --> 00:23:43,679
i'm going to give you this very

635
00:23:43,679 --> 00:23:45,440
important example of

636
00:23:45,440 --> 00:23:48,159
clinical suicide risk versus social

637
00:23:48,159 --> 00:23:49,600
suicide risk

638
00:23:49,600 --> 00:23:51,520
and i'll point you to this fantastic

639
00:23:51,520 --> 00:23:53,840
piece from mason marks

640
00:23:53,840 --> 00:23:56,240
on artificial intelligence-based suicide

641
00:23:56,240 --> 00:23:57,440
prediction

642
00:23:57,440 --> 00:23:59,840
in the yale journal of health policy law

643
00:23:59,840 --> 00:24:01,840
and ethics

644
00:24:01,840 --> 00:24:03,360
we break down if you think about a

645
00:24:03,360 --> 00:24:05,600
clinical setting and traditional risk

646
00:24:05,600 --> 00:24:06,880
models for suicide

647
00:24:06,880 --> 00:24:08,799
something as intimate as predicting your

648
00:24:08,799 --> 00:24:10,559
risk and making an informed decision

649
00:24:10,559 --> 00:24:10,960
about

650
00:24:10,960 --> 00:24:13,279
how to navigate care that's got to be a

651
00:24:13,279 --> 00:24:14,799
really hard thing right

652
00:24:14,799 --> 00:24:18,000
um well unfortunately in

653
00:24:18,000 --> 00:24:21,120
the traditional clinical risk models the

654
00:24:21,120 --> 00:24:22,960
the the risk scores and algorithms that

655
00:24:22,960 --> 00:24:24,720
have been developed for

656
00:24:24,720 --> 00:24:27,919
suicide risk are notoriously inaccurate

657
00:24:27,919 --> 00:24:30,159
uh this study breaks down these

658
00:24:30,159 --> 00:24:31,120
sensitivity

659
00:24:31,120 --> 00:24:34,320
and specificity and level of evidence

660
00:24:34,320 --> 00:24:37,679
um for a a couple of a few dozen

661
00:24:37,679 --> 00:24:39,600
different predictive risk scores that

662
00:24:39,600 --> 00:24:41,120
have been developed with irb

663
00:24:41,120 --> 00:24:44,320
approval and you know the gist of it

664
00:24:44,320 --> 00:24:47,120
is the a lot more needs to be done

665
00:24:47,120 --> 00:24:48,880
before these are worthy

666
00:24:48,880 --> 00:24:52,559
of informed clinical decision making

667
00:24:52,559 --> 00:24:55,760
in a practice with your doctor

668
00:24:55,760 --> 00:24:57,520
and at the very least something like

669
00:24:57,520 --> 00:24:59,039
this should be

670
00:24:59,039 --> 00:25:01,600
in your own hands as a risk or that you

671
00:25:01,600 --> 00:25:02,640
could

672
00:25:02,640 --> 00:25:05,840
make decisions about on your own and be

673
00:25:05,840 --> 00:25:09,120
informed about rather than something

674
00:25:09,120 --> 00:25:10,799
that is

675
00:25:10,799 --> 00:25:15,360
a decision made about you

676
00:25:15,360 --> 00:25:18,720
well it's starting to be a thing that

677
00:25:18,720 --> 00:25:22,480
we are able to predict risk of suicide

678
00:25:22,480 --> 00:25:24,720
based on what you post and it doesn't

679
00:25:24,720 --> 00:25:27,600
have to be that you intend self-harm

680
00:25:27,600 --> 00:25:30,640
this is a study on machine learning

681
00:25:30,640 --> 00:25:31,520
approach for

682
00:25:31,520 --> 00:25:34,400
predicting uh suicidal ideation on

683
00:25:34,400 --> 00:25:36,240
social media data

684
00:25:36,240 --> 00:25:38,960
and it's it's there are a few studies

685
00:25:38,960 --> 00:25:39,919
like this right now

686
00:25:39,919 --> 00:25:41,919
in their early stages where this was

687
00:25:41,919 --> 00:25:43,279
just a participant

688
00:25:43,279 --> 00:25:46,480
group of 283 because

689
00:25:46,480 --> 00:25:50,640
the study was on twitter data

690
00:25:50,640 --> 00:25:53,600
they felt it was important that to

691
00:25:53,600 --> 00:25:54,880
outline there was no need

692
00:25:54,880 --> 00:25:57,120
for consent because the tweets were

693
00:25:57,120 --> 00:25:57,919
public

694
00:25:57,919 --> 00:26:01,120
and that's an important thing we know

695
00:26:01,120 --> 00:26:03,840
when we go on twitter any user knows

696
00:26:03,840 --> 00:26:05,919
there's no expectation of private

697
00:26:05,919 --> 00:26:08,960
privacy and what you broadcast is what

698
00:26:08,960 --> 00:26:10,080
you broadcast

699
00:26:10,080 --> 00:26:13,039
but what you may not know is that could

700
00:26:13,039 --> 00:26:15,279
predict even if you're talking about

701
00:26:15,279 --> 00:26:18,320
something totally unrelated the words

702
00:26:18,320 --> 00:26:19,760
you use

703
00:26:19,760 --> 00:26:22,480
those could be used to train machine

704
00:26:22,480 --> 00:26:24,240
learning algorithms on your risk of

705
00:26:24,240 --> 00:26:26,080
suicide and then

706
00:26:26,080 --> 00:26:28,000
who knows if they're accurate or not

707
00:26:28,000 --> 00:26:30,559
those proprietary black boxes can be

708
00:26:30,559 --> 00:26:31,520
sold off

709
00:26:31,520 --> 00:26:34,320
to make decisions about your employment

710
00:26:34,320 --> 00:26:34,960
your

711
00:26:34,960 --> 00:26:38,080
ability to get a mortgage or a loan

712
00:26:38,080 --> 00:26:40,480
a student loan who knows it's a black

713
00:26:40,480 --> 00:26:41,760
box and

714
00:26:41,760 --> 00:26:44,159
that's a problem and it begs the

715
00:26:44,159 --> 00:26:45,039
question

716
00:26:45,039 --> 00:26:47,120
should your risk of suicide be a trade

717
00:26:47,120 --> 00:26:48,559
secret at all

718
00:26:48,559 --> 00:26:50,559
or at your risk of any health

719
00:26:50,559 --> 00:26:53,360
information should it be a trade secret

720
00:26:53,360 --> 00:26:55,679
i get that companies have to build

721
00:26:55,679 --> 00:26:57,360
business models

722
00:26:57,360 --> 00:27:01,120
on uh the software that they develop but

723
00:27:01,120 --> 00:27:02,880
there are different ways to do this

724
00:27:02,880 --> 00:27:04,799
and as we think about paths forward and

725
00:27:04,799 --> 00:27:06,480
ethical ai

726
00:27:06,480 --> 00:27:10,240
this is a really important one and

727
00:27:10,240 --> 00:27:13,360
it scares me um because

728
00:27:13,360 --> 00:27:16,799
when we are

729
00:27:16,799 --> 00:27:20,720
aggregating the pain of people or

730
00:27:20,720 --> 00:27:23,520
making decisions about somebody without

731
00:27:23,520 --> 00:27:25,520
their knowledge or consent

732
00:27:25,520 --> 00:27:28,000
it hopefully i've said it over and over

733
00:27:28,000 --> 00:27:28,799
again

734
00:27:28,799 --> 00:27:32,000
is a complete breach of our human rights

735
00:27:32,000 --> 00:27:35,360
and our privacy and it shouldn't happen

736
00:27:35,360 --> 00:27:38,480
you should feel free to be vulnerable as

737
00:27:38,480 --> 00:27:39,279
you want to be

738
00:27:39,279 --> 00:27:42,720
on social media without thinking about

739
00:27:42,720 --> 00:27:44,159
what the implications are of what you

740
00:27:44,159 --> 00:27:46,480
post

741
00:27:47,600 --> 00:27:49,600
and i'm gonna bring up by the way that

742
00:27:49,600 --> 00:27:51,039
goes for any

743
00:27:51,039 --> 00:27:53,200
jerk on the internet who wants to share

744
00:27:53,200 --> 00:27:55,039
things that are

745
00:27:55,039 --> 00:27:58,159
um misinformed people need the freedom

746
00:27:58,159 --> 00:27:58,960
to learn

747
00:27:58,960 --> 00:28:03,120
and to disagree and be wrong

748
00:28:03,120 --> 00:28:06,640
and trust is completely broken maybe i

749
00:28:06,640 --> 00:28:07,600
have shared enough

750
00:28:07,600 --> 00:28:10,720
at this point to convince you that

751
00:28:10,720 --> 00:28:13,440
trust is broken in institutions in

752
00:28:13,440 --> 00:28:14,480
technologies

753
00:28:14,480 --> 00:28:19,120
and and patients the public know it

754
00:28:19,440 --> 00:28:21,120
i personally have an interesting

755
00:28:21,120 --> 00:28:23,440
definition of trust that comes from

756
00:28:23,440 --> 00:28:27,039
stanford professor lindridge greer

757
00:28:27,039 --> 00:28:29,039
and she studies teams and how teams

758
00:28:29,039 --> 00:28:30,159
behave

759
00:28:30,159 --> 00:28:32,080
one of the things she says is trust is

760
00:28:32,080 --> 00:28:33,600
not a feeling

761
00:28:33,600 --> 00:28:36,559
it is a measure of how accurately you

762
00:28:36,559 --> 00:28:37,520
can predict

763
00:28:37,520 --> 00:28:40,880
benefit or harm and

764
00:28:40,880 --> 00:28:43,200
often i think we look at patients as an

765
00:28:43,200 --> 00:28:44,720
abstraction

766
00:28:44,720 --> 00:28:46,880
we think of us as a market or as

767
00:28:46,880 --> 00:28:49,600
consumers or users with no rights

768
00:28:49,600 --> 00:28:53,760
and therefore our voices don't matter

769
00:28:53,760 --> 00:28:56,720
i think of of patients very differently

770
00:28:56,720 --> 00:28:57,679
as needing to

771
00:28:57,679 --> 00:29:00,720
be representative in the policies the

772
00:29:00,720 --> 00:29:02,720
technology and the field of cyber

773
00:29:02,720 --> 00:29:04,080
security

774
00:29:04,080 --> 00:29:07,200
i think e-patients patients like me who

775
00:29:07,200 --> 00:29:07,440
are

776
00:29:07,440 --> 00:29:09,919
activists are hackers too they just

777
00:29:09,919 --> 00:29:12,480
don't know it yet and if we build

778
00:29:12,480 --> 00:29:15,440
trust we have to do so in ways that are

779
00:29:15,440 --> 00:29:18,799
accurate and measurable

780
00:29:19,279 --> 00:29:22,320
what do we do and how do we do that

781
00:29:22,320 --> 00:29:26,080
as early adopters of technology

782
00:29:26,080 --> 00:29:29,200
the brca community and i have been

783
00:29:29,200 --> 00:29:29,760
hacked

784
00:29:29,760 --> 00:29:32,799
scammed sold patented by the last

785
00:29:32,799 --> 00:29:33,919
generation of

786
00:29:33,919 --> 00:29:36,480
innovators in healthcare and i think

787
00:29:36,480 --> 00:29:39,279
it's time for a change

788
00:29:39,279 --> 00:29:40,960
i hope i've scared you enough because

789
00:29:40,960 --> 00:29:42,080
now i'm going to get to some more

790
00:29:42,080 --> 00:29:46,080
lighthearted stuff and a path forward

791
00:29:46,159 --> 00:29:48,640
what is my call to arms well i'm going

792
00:29:48,640 --> 00:29:49,279
to channel

793
00:29:49,279 --> 00:29:52,399
rosalind franklin who was

794
00:29:52,399 --> 00:29:55,440
the original x-ray crystallographer who

795
00:29:55,440 --> 00:29:57,200
found the structure of dna and then add

796
00:29:57,200 --> 00:29:57,919
that

797
00:29:57,919 --> 00:30:02,159
co-opted by watson and crick

798
00:30:02,159 --> 00:30:05,360
and she said

799
00:30:05,360 --> 00:30:07,440
science and everyday life cannot and

800
00:30:07,440 --> 00:30:09,039
should not be separated

801
00:30:09,039 --> 00:30:11,440
she also died of ovarian cancer the

802
00:30:11,440 --> 00:30:12,320
truth is

803
00:30:12,320 --> 00:30:15,120
though when it comes to dismantling the

804
00:30:15,120 --> 00:30:16,399
problems that we

805
00:30:16,399 --> 00:30:18,960
share these are all problems that the

806
00:30:18,960 --> 00:30:21,520
second you go into the wells of a clinic

807
00:30:21,520 --> 00:30:23,279
these problems are going to affect you

808
00:30:23,279 --> 00:30:25,200
too

809
00:30:25,200 --> 00:30:29,919
and i think that we have to start

810
00:30:29,919 --> 00:30:32,320
bringing together the experts and

811
00:30:32,320 --> 00:30:34,880
institutions by supporting communities

812
00:30:34,880 --> 00:30:38,000
of patients and their leadership with

813
00:30:38,000 --> 00:30:41,520
governance digital governance

814
00:30:41,520 --> 00:30:44,399
in order to create a structure like this

815
00:30:44,399 --> 00:30:46,240
to address these problems

816
00:30:46,240 --> 00:30:47,840
i shared at the beginning we formed a

817
00:30:47,840 --> 00:30:50,640
non-profit to serve patient communities

818
00:30:50,640 --> 00:30:52,799
and represent our collective digital

819
00:30:52,799 --> 00:30:55,200
rights today we think of

820
00:30:55,200 --> 00:30:57,760
informed consent and okay if i look at a

821
00:30:57,760 --> 00:31:00,159
50-page document and i sign at the end

822
00:31:00,159 --> 00:31:00,799
well

823
00:31:00,799 --> 00:31:03,120
i am like about to go into the emergency

824
00:31:03,120 --> 00:31:04,799
room box checked

825
00:31:04,799 --> 00:31:07,760
i've given my consent well it just it

826
00:31:07,760 --> 00:31:08,640
that doesn't

827
00:31:08,640 --> 00:31:10,960
do justice to anybody and it is not

828
00:31:10,960 --> 00:31:12,080
sustainable

829
00:31:12,080 --> 00:31:14,480
and it is further enabling these systems

830
00:31:14,480 --> 00:31:15,679
of harm

831
00:31:15,679 --> 00:31:19,039
so our mission is to help peer support

832
00:31:19,039 --> 00:31:20,799
groups people with shared identities and

833
00:31:20,799 --> 00:31:22,240
health conditions

834
00:31:22,240 --> 00:31:24,480
to build capacity around digital

835
00:31:24,480 --> 00:31:26,240
government governance

836
00:31:26,240 --> 00:31:28,720
and foster healthy human connections

837
00:31:28,720 --> 00:31:30,559
when it comes to

838
00:31:30,559 --> 00:31:33,120
representing our identity and our rights

839
00:31:33,120 --> 00:31:34,720
in health data and technologies that

840
00:31:34,720 --> 00:31:37,200
affect our lives

841
00:31:37,200 --> 00:31:40,080
heidi larson vaccine anthropologist said

842
00:31:40,080 --> 00:31:41,440
it beautifully

843
00:31:41,440 --> 00:31:43,120
you know when we think about the problem

844
00:31:43,120 --> 00:31:45,200
of medical misinformation

845
00:31:45,200 --> 00:31:47,840
we should be looking at rumors as an

846
00:31:47,840 --> 00:31:48,799
ecosystem

847
00:31:48,799 --> 00:31:51,919
not unlike a microbiome when instead

848
00:31:51,919 --> 00:31:53,760
today we're just continuing to go to

849
00:31:53,760 --> 00:31:56,559
here go to this faq and educate yourself

850
00:31:56,559 --> 00:31:58,240
when people don't trust it it doesn't

851
00:31:58,240 --> 00:32:01,039
add anything to their experience

852
00:32:01,039 --> 00:32:04,640
so here's what i am doing take this

853
00:32:04,640 --> 00:32:07,440
for example is a picture of the brca

854
00:32:07,440 --> 00:32:08,559
community

855
00:32:08,559 --> 00:32:11,440
the breast cancer community brain tumor

856
00:32:11,440 --> 00:32:13,519
tumor community how we interact this is

857
00:32:13,519 --> 00:32:14,240
a

858
00:32:14,240 --> 00:32:17,840
network graph and i'm learning um

859
00:32:17,840 --> 00:32:20,720
how predictive algorithms work so that i

860
00:32:20,720 --> 00:32:21,120
might

861
00:32:21,120 --> 00:32:24,320
be a part of the solution and for the

862
00:32:24,320 --> 00:32:25,679
past year i've been working with this

863
00:32:25,679 --> 00:32:26,240
amazing

864
00:32:26,240 --> 00:32:28,559
amazing open source a project called

865
00:32:28,559 --> 00:32:29,840
project domino

866
00:32:29,840 --> 00:32:33,200
in fact leo mayarovic and cody webb

867
00:32:33,200 --> 00:32:36,480
are one of them maybe

868
00:32:36,480 --> 00:32:40,960
in our q a so keep an eye out for that

869
00:32:40,960 --> 00:32:44,080
but what we've been doing is working to

870
00:32:44,080 --> 00:32:45,600
understand how our networks are

871
00:32:45,600 --> 00:32:46,320
visualized

872
00:32:46,320 --> 00:32:48,720
and helping to inform the design of

873
00:32:48,720 --> 00:32:50,240
predictive algorithms when there are

874
00:32:50,240 --> 00:32:51,840
often

875
00:32:51,840 --> 00:32:54,799
positives and false negatives how do we

876
00:32:54,799 --> 00:32:56,399
become part of the solution and

877
00:32:56,399 --> 00:32:58,159
represent ourselves

878
00:32:58,159 --> 00:33:00,399
in the data or the algorithms that we're

879
00:33:00,399 --> 00:33:02,240
developing

880
00:33:02,240 --> 00:33:03,919
i think this lends to a more important

881
00:33:03,919 --> 00:33:05,679
question in my mind which is

882
00:33:05,679 --> 00:33:08,880
are patient activists when we think

883
00:33:08,880 --> 00:33:10,799
patients can be hackers too patients

884
00:33:10,799 --> 00:33:12,799
like me can be hackers too we just don't

885
00:33:12,799 --> 00:33:14,080
know it yet

886
00:33:14,080 --> 00:33:17,679
can we be a part of a digital community

887
00:33:17,679 --> 00:33:20,000
of health workers that are helping to

888
00:33:20,000 --> 00:33:21,360
counter the effects of medical

889
00:33:21,360 --> 00:33:23,279
misinformation and become

890
00:33:23,279 --> 00:33:26,320
stewards of fairer representation

891
00:33:26,320 --> 00:33:29,840
in technology um

892
00:33:29,840 --> 00:33:32,000
so little nerd out in eye candy this is

893
00:33:32,000 --> 00:33:32,880
my little

894
00:33:32,880 --> 00:33:36,080
jupiter notebook repository of

895
00:33:36,080 --> 00:33:39,440
brca tweets and here's a beautiful

896
00:33:39,440 --> 00:33:40,799
visualization of

897
00:33:40,799 --> 00:33:42,000
some of the things that we've been

898
00:33:42,000 --> 00:33:44,320
playing around with this is actually um

899
00:33:44,320 --> 00:33:46,159
looking at disinformation networks and

900
00:33:46,159 --> 00:33:47,760
bot networks and

901
00:33:47,760 --> 00:33:49,679
understanding how to identify them and

902
00:33:49,679 --> 00:33:51,279
when you're a patient community going

903
00:33:51,279 --> 00:33:52,000
through trauma

904
00:33:52,000 --> 00:33:54,240
and diagnosis you might not know the

905
00:33:54,240 --> 00:33:56,320
first thing about whether a bot network

906
00:33:56,320 --> 00:33:57,919
is targeting you or not or whether

907
00:33:57,919 --> 00:33:59,039
that's real

908
00:33:59,039 --> 00:34:01,039
and you might only see a small sliver of

909
00:34:01,039 --> 00:34:03,039
the problem where this is a way to think

910
00:34:03,039 --> 00:34:03,519
about

911
00:34:03,519 --> 00:34:06,720
of it as a living breathing ecosystem

912
00:34:06,720 --> 00:34:07,840
and how

913
00:34:07,840 --> 00:34:09,760
information and knowledge is shared in a

914
00:34:09,760 --> 00:34:12,399
visual way

915
00:34:12,399 --> 00:34:15,839
what is my call to arms i am

916
00:34:15,839 --> 00:34:18,399
asking those listening to understand

917
00:34:18,399 --> 00:34:20,000
that patient communities who

918
00:34:20,000 --> 00:34:23,119
are users of technology also have

919
00:34:23,119 --> 00:34:26,960
no rights when we are generating

920
00:34:26,960 --> 00:34:30,320
health data as the supply in the supply

921
00:34:30,320 --> 00:34:31,280
chain

922
00:34:31,280 --> 00:34:33,918
and those data and algorithms can either

923
00:34:33,918 --> 00:34:35,520
cure or kill us

924
00:34:35,520 --> 00:34:38,480
we need digital rights so this is a

925
00:34:38,480 --> 00:34:39,599
draft of six

926
00:34:39,599 --> 00:34:42,239
principles for rights that in 2019 we

927
00:34:42,239 --> 00:34:44,079
drafted with

928
00:34:44,079 --> 00:34:48,239
the the help of patient leaders

929
00:34:48,239 --> 00:34:51,119
disability rights activists e-patients

930
00:34:51,119 --> 00:34:51,679
and

931
00:34:51,679 --> 00:34:54,320
policy and technology experts i

932
00:34:54,320 --> 00:34:56,079
encourage you to go take a look at this

933
00:34:56,079 --> 00:34:57,280
on our website

934
00:34:57,280 --> 00:34:59,200
this is very much in the early stages

935
00:34:59,200 --> 00:35:02,000
it's just a first draft

936
00:35:02,000 --> 00:35:03,760
i have another call to arms and that is

937
00:35:03,760 --> 00:35:05,200
to the people

938
00:35:05,200 --> 00:35:07,520
not above me who think of themselves as

939
00:35:07,520 --> 00:35:08,960
experts representing

940
00:35:08,960 --> 00:35:12,079
institutions or elite hackers who

941
00:35:12,079 --> 00:35:13,839
are just talking about hacking all the

942
00:35:13,839 --> 00:35:15,520
things this is for

943
00:35:15,520 --> 00:35:18,240
the people beside me who are struggling

944
00:35:18,240 --> 00:35:20,480
to learn about the technologies that

945
00:35:20,480 --> 00:35:24,079
affect us and that we often struggle to

946
00:35:24,079 --> 00:35:25,040
trust

947
00:35:25,040 --> 00:35:27,000
we need to start building core

948
00:35:27,000 --> 00:35:28,160
competencies

949
00:35:28,160 --> 00:35:32,320
in technology as we organize communities

950
00:35:32,320 --> 00:35:33,599
on social media

951
00:35:33,599 --> 00:35:35,680
and help them to engage not only with

952
00:35:35,680 --> 00:35:37,359
experts in the healthcare system but

953
00:35:37,359 --> 00:35:40,560
advocate for our digital rights

954
00:35:40,560 --> 00:35:43,200
we are going to be piloting a civic

955
00:35:43,200 --> 00:35:45,280
trust this is a legal model

956
00:35:45,280 --> 00:35:47,280
and civic trust law is nothing new it's

957
00:35:47,280 --> 00:35:49,040
been around for a thousand years

958
00:35:49,040 --> 00:35:51,119
but when we think about having a

959
00:35:51,119 --> 00:35:53,680
fiduciary responsibility to shareholders

960
00:35:53,680 --> 00:35:54,640
instead of

961
00:35:54,640 --> 00:35:57,760
users or patients this is part of the

962
00:35:57,760 --> 00:35:59,920
fundamental misalignment of incentives

963
00:35:59,920 --> 00:36:01,920
in business models like practice fusion

964
00:36:01,920 --> 00:36:02,960
that i shared

965
00:36:02,960 --> 00:36:05,359
or in ad targeting on social media that

966
00:36:05,359 --> 00:36:06,880
can cause harm

967
00:36:06,880 --> 00:36:09,359
so if we think about patients and

968
00:36:09,359 --> 00:36:10,880
leaders and experts as

969
00:36:10,880 --> 00:36:13,680
data stewards that can be part of a new

970
00:36:13,680 --> 00:36:14,560
model

971
00:36:14,560 --> 00:36:17,119
and define the purpose of how data are

972
00:36:17,119 --> 00:36:18,079
used

973
00:36:18,079 --> 00:36:20,240
and communities as beneficiaries of

974
00:36:20,240 --> 00:36:21,440
those data

975
00:36:21,440 --> 00:36:24,000
i think we have an opportunity not only

976
00:36:24,000 --> 00:36:26,240
to create a lot of value in the design

977
00:36:26,240 --> 00:36:27,839
of better systems

978
00:36:27,839 --> 00:36:30,640
that improve health outcomes but also

979
00:36:30,640 --> 00:36:31,280
take a more

980
00:36:31,280 --> 00:36:35,520
networked approach to representation

981
00:36:35,520 --> 00:36:38,560
of communities and diverse identities

982
00:36:38,560 --> 00:36:44,078
that can be a part of a path forward

983
00:36:44,560 --> 00:36:47,440
finally i am i think i already said

984
00:36:47,440 --> 00:36:48,160
finally but

985
00:36:48,160 --> 00:36:51,359
finally again i am asking

986
00:36:51,359 --> 00:36:54,800
for you to partner with us and directly

987
00:36:54,800 --> 00:36:55,440
work with

988
00:36:55,440 --> 00:36:57,920
patients as colleagues and equals rather

989
00:36:57,920 --> 00:36:59,680
than research subjects

990
00:36:59,680 --> 00:37:02,960
and users i am asking you when you think

991
00:37:02,960 --> 00:37:05,040
about diversity on your team

992
00:37:05,040 --> 00:37:06,880
in cyber security or elsewhere in the

993
00:37:06,880 --> 00:37:08,960
design of your health care systems

994
00:37:08,960 --> 00:37:12,000
do you have patients representing

995
00:37:12,000 --> 00:37:15,440
your team if you don't you are part of

996
00:37:15,440 --> 00:37:16,400
the problem

997
00:37:16,400 --> 00:37:19,119
because we can help and we need to be

998
00:37:19,119 --> 00:37:21,200
treated not just as advocates being

999
00:37:21,200 --> 00:37:22,640
asked for feedback

1000
00:37:22,640 --> 00:37:25,040
we need to be treated as experts in our

1001
00:37:25,040 --> 00:37:26,400
own experience

1002
00:37:26,400 --> 00:37:28,880
and build the capacity and skills not

1003
00:37:28,880 --> 00:37:30,640
only in the past career path of cyber

1004
00:37:30,640 --> 00:37:31,520
security

1005
00:37:31,520 --> 00:37:33,920
but in other places because when we do

1006
00:37:33,920 --> 00:37:35,119
that there are

1007
00:37:35,119 --> 00:37:38,400
so many examples of where we have

1008
00:37:38,400 --> 00:37:40,400
changed things not only in the diabetes

1009
00:37:40,400 --> 00:37:42,720
community with we are not waiting and

1010
00:37:42,720 --> 00:37:43,200
opening

1011
00:37:43,200 --> 00:37:47,359
aps but also the think about the hiv and

1012
00:37:47,359 --> 00:37:49,520
aids movement and how that changed the

1013
00:37:49,520 --> 00:37:50,880
course of

1014
00:37:50,880 --> 00:37:53,920
their disease

1015
00:37:53,920 --> 00:37:57,200
so finally a third time finally

1016
00:37:57,200 --> 00:37:59,839
i'm asking you to join us in rebuilding

1017
00:37:59,839 --> 00:38:01,280
trust

1018
00:38:01,280 --> 00:38:03,440
uh you can follow twit on twitter be

1019
00:38:03,440 --> 00:38:04,320
like light

1020
00:38:04,320 --> 00:38:06,160
check out the light collective i'm going

1021
00:38:06,160 --> 00:38:08,000
to be sharing how to

1022
00:38:08,000 --> 00:38:10,800
get involved with us if you are on the

1023
00:38:10,800 --> 00:38:12,240
discord channel

1024
00:38:12,240 --> 00:38:14,800
and i'll be tweeting a few things out on

1025
00:38:14,800 --> 00:38:16,240
how to get involved

1026
00:38:16,240 --> 00:38:18,160
we are currently seeking applications

1027
00:38:18,160 --> 00:38:21,040
for our first cohort of partners

1028
00:38:21,040 --> 00:38:22,480
and we would love for you to be a part

1029
00:38:22,480 --> 00:38:25,119
of that finally i want to give a whole

1030
00:38:25,119 --> 00:38:26,640
bunch of thank yous

1031
00:38:26,640 --> 00:38:28,720
uh to the board of the light collective

1032
00:38:28,720 --> 00:38:30,320
who are e-patients who have been working

1033
00:38:30,320 --> 00:38:32,480
on this for two and a half years

1034
00:38:32,480 --> 00:38:35,200
advisors or legal advisors or clinical

1035
00:38:35,200 --> 00:38:37,359
experts and cyber security experts and

1036
00:38:37,359 --> 00:38:38,880
health data experts

1037
00:38:38,880 --> 00:38:40,720
i also want to thank the incredible

1038
00:38:40,720 --> 00:38:43,520
communities on social media like

1039
00:38:43,520 --> 00:38:47,040
bcsm with alicia staley i want to thank

1040
00:38:47,040 --> 00:38:49,040
robert with johnson foundation for

1041
00:38:49,040 --> 00:38:52,400
for supporting our work and um

1042
00:38:52,400 --> 00:38:54,800
brca sisterhood american living organ

1043
00:38:54,800 --> 00:38:55,680
donor fund

1044
00:38:55,680 --> 00:38:59,119
digital public my style matters and

1045
00:38:59,119 --> 00:39:02,240
so much more so thank you

1046
00:39:02,240 --> 00:39:07,200
and i'll see you in the q a

