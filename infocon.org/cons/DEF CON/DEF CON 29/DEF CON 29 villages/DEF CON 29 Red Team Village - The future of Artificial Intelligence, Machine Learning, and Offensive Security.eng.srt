1
00:00:10,639 --> 00:00:11,840
hey uh thank you

2
00:00:11,840 --> 00:00:15,040
omar uh i'm super excited to host

3
00:00:15,040 --> 00:00:18,320
the luminaries of the ai village and the

4
00:00:18,320 --> 00:00:19,920
red team village where i think what may

5
00:00:19,920 --> 00:00:20,640
be the first

6
00:00:20,640 --> 00:00:24,080
joint panel uh in this space so to set a

7
00:00:24,080 --> 00:00:25,439
little bit of contact like

8
00:00:25,439 --> 00:00:27,439
ai red teams and whatever teams are kind

9
00:00:27,439 --> 00:00:29,279
of mushrooming everywhere from

10
00:00:29,279 --> 00:00:32,159
facebook to microphone to nvidia and

11
00:00:32,159 --> 00:00:33,600
even in the government

12
00:00:33,600 --> 00:00:35,840
but there's a lot of like misconception

13
00:00:35,840 --> 00:00:37,360
about water days and

14
00:00:37,360 --> 00:00:38,879
we really don't we really want to take

15
00:00:38,879 --> 00:00:40,399
this time to kind of like peace out how

16
00:00:40,399 --> 00:00:41,840
different it is from the regular red

17
00:00:41,840 --> 00:00:42,719
teaming

18
00:00:42,719 --> 00:00:44,000
and most importantly how can the

19
00:00:44,000 --> 00:00:45,840
security community and the machine

20
00:00:45,840 --> 00:00:47,600
learning community come together

21
00:00:47,600 --> 00:00:50,960
to kind of like deal with this change so

22
00:00:50,960 --> 00:00:53,199
i want to quickly introduce the panel um

23
00:00:53,199 --> 00:00:54,079
we have with us

24
00:00:54,079 --> 00:00:57,440
the celebrated security guru he's been

25
00:00:57,440 --> 00:00:58,559
thinking a lot about

26
00:00:58,559 --> 00:01:01,760
uh attacks on ai systems um

27
00:01:01,760 --> 00:01:04,479
you know under recently so welcome bruce

28
00:01:04,479 --> 00:01:06,799
uh we also have omar santos who we who

29
00:01:06,799 --> 00:01:07,680
leads the

30
00:01:07,680 --> 00:01:10,799
uh we really need to be thankful to him

31
00:01:10,799 --> 00:01:12,560
for all the wonderful cpfs that are

32
00:01:12,560 --> 00:01:13,439
happening

33
00:01:13,439 --> 00:01:16,159
as well as as chris capelle from nvidia

34
00:01:16,159 --> 00:01:18,320
um

35
00:01:18,960 --> 00:01:22,080
ai red team head honcho so thank you uh

36
00:01:22,080 --> 00:01:24,240
omar and chris from representing the red

37
00:01:24,240 --> 00:01:25,600
team village

38
00:01:25,600 --> 00:01:28,080
um from the ai village we have uh dr

39
00:01:28,080 --> 00:01:29,600
anita nikolaj

40
00:01:29,600 --> 00:01:31,759
who's the director of research and

41
00:01:31,759 --> 00:01:33,680
technology innovation

42
00:01:33,680 --> 00:01:35,280
at the university of illinois

43
00:01:35,280 --> 00:01:37,920
urbana-champaign and dr anita is really

44
00:01:37,920 --> 00:01:38,720
gonna

45
00:01:38,720 --> 00:01:41,439
ground us today on what is actually

46
00:01:41,439 --> 00:01:42,320
possible

47
00:01:42,320 --> 00:01:43,759
and what is actually happening in the

48
00:01:43,759 --> 00:01:46,640
cutting edge research from academia

49
00:01:46,640 --> 00:01:48,799
and finally uh we have with us uh

50
00:01:48,799 --> 00:01:50,399
security data scientist

51
00:01:50,399 --> 00:01:54,320
uh rich harang from duosac who's also

52
00:01:54,320 --> 00:01:55,200
now put out

53
00:01:55,200 --> 00:01:59,119
very exciting uh twitter's uh bias but

54
00:01:59,119 --> 00:02:00,640
bug bunny which we will be talking about

55
00:02:00,640 --> 00:02:02,960
today so you know with this i really

56
00:02:02,960 --> 00:02:03,840
want to get uh

57
00:02:03,840 --> 00:02:05,439
quickly started i want all of you to

58
00:02:05,439 --> 00:02:07,439
hear from the experts

59
00:02:07,439 --> 00:02:10,479
bruce i want to start with you um

60
00:02:10,479 --> 00:02:13,040
you know you you had a very thoughtful

61
00:02:13,040 --> 00:02:14,080
real quick

62
00:02:14,080 --> 00:02:17,200
at the ei villain um recently

63
00:02:17,200 --> 00:02:19,280
you know he spoke about how lifespans

64
00:02:19,280 --> 00:02:22,000
will uh eventually find loopholes

65
00:02:22,000 --> 00:02:25,520
at blazing speed i really love your um

66
00:02:25,520 --> 00:02:28,959
for example with tax evasion so

67
00:02:28,959 --> 00:02:30,480
how do you think about can you just

68
00:02:30,480 --> 00:02:32,239
elaborate uh about that for

69
00:02:32,239 --> 00:02:34,000
just a quick minute for people from the

70
00:02:34,000 --> 00:02:35,920
red team village and i really want you

71
00:02:35,920 --> 00:02:37,280
to kind of like touch on

72
00:02:37,280 --> 00:02:39,760
how you think about humans attacking ai

73
00:02:39,760 --> 00:02:40,959
systems

74
00:02:40,959 --> 00:02:44,160
so i've been watching ais become hackers

75
00:02:44,160 --> 00:02:47,599
that happened at def con i think 2016

76
00:02:47,599 --> 00:02:50,160
when uh ais had their own capture the

77
00:02:50,160 --> 00:02:51,599
flag contest

78
00:02:51,599 --> 00:02:53,360
and more interestingly there's a lot of

79
00:02:53,360 --> 00:02:55,760
research in ai's finding vulnerabilities

80
00:02:55,760 --> 00:02:57,040
in code

81
00:02:57,040 --> 00:02:58,400
it's kind of the thing you'd expect

82
00:02:58,400 --> 00:03:00,080
iii's be good at it's uh

83
00:03:00,080 --> 00:03:01,680
it's pattern matching there's a lot of

84
00:03:01,680 --> 00:03:03,519
data it's a lot of it's a lot of

85
00:03:03,519 --> 00:03:04,080
repetitive

86
00:03:04,080 --> 00:03:06,560
work and they're not very good at it yet

87
00:03:06,560 --> 00:03:08,480
they're going to get better at it

88
00:03:08,480 --> 00:03:11,120
so when we think about hacking and

89
00:03:11,120 --> 00:03:13,440
vulnerability finding

90
00:03:13,440 --> 00:03:16,000
it's no longer going to be a human-only

91
00:03:16,000 --> 00:03:17,920
creative endeavor right the two parts to

92
00:03:17,920 --> 00:03:18,480
hacking

93
00:03:18,480 --> 00:03:20,480
this there's the creative part figuring

94
00:03:20,480 --> 00:03:21,760
out what the hack is

95
00:03:21,760 --> 00:03:23,840
and there's the execution i mean yes we

96
00:03:23,840 --> 00:03:25,280
can automate the execution

97
00:03:25,280 --> 00:03:27,760
that's pretty easy but automating the

98
00:03:27,760 --> 00:03:29,920
finding the clever

99
00:03:29,920 --> 00:03:32,879
vulnerability finding the exploit making

100
00:03:32,879 --> 00:03:33,760
it work

101
00:03:33,760 --> 00:03:35,280
that's going to become increasingly

102
00:03:35,280 --> 00:03:37,360
automated so

103
00:03:37,360 --> 00:03:40,480
that you look at at hacking ai systems

104
00:03:40,480 --> 00:03:41,200
now

105
00:03:41,200 --> 00:03:43,120
and you know ram you all know that it's

106
00:03:43,120 --> 00:03:44,239
primarily

107
00:03:44,239 --> 00:03:47,280
human it's done by humans it's a human

108
00:03:47,280 --> 00:03:48,560
creative process

109
00:03:48,560 --> 00:03:50,560
that's going to change probably the next

110
00:03:50,560 --> 00:03:52,799
few years slowly and then quickly like

111
00:03:52,799 --> 00:03:54,720
all of these things change

112
00:03:54,720 --> 00:03:56,879
oh i i was looking at a great time to

113
00:03:56,879 --> 00:03:58,640
kind of bring omar in

114
00:03:58,640 --> 00:04:01,599
uh omar you've been like you know uh you

115
00:04:01,599 --> 00:04:03,439
have a wealth of experience in kind of

116
00:04:03,439 --> 00:04:05,680
like retaining traditional systems

117
00:04:05,680 --> 00:04:08,799
so right now hacking involves like human

118
00:04:08,799 --> 00:04:11,280
um you know like how bruce pointed out

119
00:04:11,280 --> 00:04:13,599
how did you see the space kind of like

120
00:04:13,599 --> 00:04:14,159
uh

121
00:04:14,159 --> 00:04:17,519
evolving yeah i think that actually ruse

122
00:04:17,519 --> 00:04:19,199
mentions something extremely relevant

123
00:04:19,199 --> 00:04:20,560
right but before

124
00:04:20,560 --> 00:04:22,800
before i go in deeper into the red

125
00:04:22,800 --> 00:04:24,960
teaming of ai systems

126
00:04:24,960 --> 00:04:26,880
let me actually define the two aspects

127
00:04:26,880 --> 00:04:28,639
of that word

128
00:04:28,639 --> 00:04:30,479
or the phrase a i read timing that you

129
00:04:30,479 --> 00:04:32,000
mentioned right so

130
00:04:32,000 --> 00:04:34,240
one is about attacks against the an

131
00:04:34,240 --> 00:04:35,680
environment so we're gonna talk about

132
00:04:35,680 --> 00:04:36,960
that a little bit later

133
00:04:36,960 --> 00:04:38,639
and the other one is what bruce was

134
00:04:38,639 --> 00:04:40,560
actually mentioning is actually using ai

135
00:04:40,560 --> 00:04:42,240
or machine learning to

136
00:04:42,240 --> 00:04:44,080
to attack you know different platforms

137
00:04:44,080 --> 00:04:45,520
or to perform data

138
00:04:45,520 --> 00:04:47,360
manipulation because at the end of the

139
00:04:47,360 --> 00:04:49,520
day a lot of the attacks are going to be

140
00:04:49,520 --> 00:04:51,360
seen is a data manipulation and

141
00:04:51,360 --> 00:04:52,720
poisoning of data

142
00:04:52,720 --> 00:04:55,120
of training data so that you will you

143
00:04:55,120 --> 00:04:56,240
know uh

144
00:04:56,240 --> 00:04:59,040
cause some damage into a into the ni

145
00:04:59,040 --> 00:04:59,759
system

146
00:04:59,759 --> 00:05:02,080
but you know going going back to what

147
00:05:02,080 --> 00:05:03,039
bruce mentioned

148
00:05:03,039 --> 00:05:05,759
if you remember four to five years ago

149
00:05:05,759 --> 00:05:07,520
darpa had a competition that's what

150
00:05:07,520 --> 00:05:08,800
bruce mentioned

151
00:05:08,800 --> 00:05:10,800
here in defcon right it was called the

152
00:05:10,800 --> 00:05:12,880
darpa cyber challenge

153
00:05:12,880 --> 00:05:15,840
and they had different teams that they

154
00:05:15,840 --> 00:05:17,680
created machine learning environments to

155
00:05:17,680 --> 00:05:18,000
do

156
00:05:18,000 --> 00:05:20,800
both things so to attack and emulate

157
00:05:20,800 --> 00:05:22,880
different type of attack methods

158
00:05:22,880 --> 00:05:25,039
find vulnerabilities and then the other

159
00:05:25,039 --> 00:05:26,800
side of the coin of course protect

160
00:05:26,800 --> 00:05:28,479
protect against those vulnerabilities

161
00:05:28,479 --> 00:05:30,320
right so they were just were trying to

162
00:05:30,320 --> 00:05:31,759
actually patch at the same time

163
00:05:31,759 --> 00:05:34,240
and defend based on that behavior the

164
00:05:34,240 --> 00:05:35,919
adversary behavior right

165
00:05:35,919 --> 00:05:38,720
now ai you know of course machine

166
00:05:38,720 --> 00:05:39,919
learning will definitely be a

167
00:05:39,919 --> 00:05:41,600
big big role of the attacks in the

168
00:05:41,600 --> 00:05:44,880
future from manipulating

169
00:05:44,880 --> 00:05:47,919
people right the masses in in social

170
00:05:47,919 --> 00:05:48,720
engineering

171
00:05:48,720 --> 00:05:52,080
a type of tactics and so on to learning

172
00:05:52,080 --> 00:05:52,479
about

173
00:05:52,479 --> 00:05:54,720
weaknesses and the underlying systems

174
00:05:54,720 --> 00:05:57,280
and learning how humans may also

175
00:05:57,280 --> 00:06:00,160
defend against those attacks and respond

176
00:06:00,160 --> 00:06:01,759
to those attacks so remember

177
00:06:01,759 --> 00:06:04,000
that in a traditional incident response

178
00:06:04,000 --> 00:06:04,880
you know

179
00:06:04,880 --> 00:06:06,560
you you have the ability to you know

180
00:06:06,560 --> 00:06:08,240
detect whatever that the threat actor is

181
00:06:08,240 --> 00:06:09,600
actually doing what we call

182
00:06:09,600 --> 00:06:11,919
the tactics techniques and procedures

183
00:06:11,919 --> 00:06:12,639
what if

184
00:06:12,639 --> 00:06:14,240
you know the attacker is actually able

185
00:06:14,240 --> 00:06:16,560
to learn what are the mitigations and

186
00:06:16,560 --> 00:06:17,600
the responses

187
00:06:17,600 --> 00:06:21,199
from the from the you know security team

188
00:06:21,199 --> 00:06:22,800
and then you know of course evolve into

189
00:06:22,800 --> 00:06:24,240
that right and then

190
00:06:24,240 --> 00:06:26,319
i can think of you know things like

191
00:06:26,319 --> 00:06:28,240
anti-forensics capabilities you know

192
00:06:28,240 --> 00:06:29,440
being inserted

193
00:06:29,440 --> 00:06:32,319
into these environments and so on now

194
00:06:32,319 --> 00:06:34,240
shifting back to the other concept

195
00:06:34,240 --> 00:06:37,199
attacking the ai and ml systems the

196
00:06:37,199 --> 00:06:38,240
first thing that

197
00:06:38,240 --> 00:06:39,280
the traditional you know is the

198
00:06:39,280 --> 00:06:40,880
traditional vulnerabilities against the

199
00:06:40,880 --> 00:06:42,880
underlying system itself you're going to

200
00:06:42,880 --> 00:06:45,919
continue to see at the end of the day

201
00:06:45,919 --> 00:06:47,360
you're actually using

202
00:06:47,360 --> 00:06:49,599
you know a proven technology nowadays

203
00:06:49,599 --> 00:06:52,400
for machine learning however

204
00:06:52,400 --> 00:06:54,720
one thing that you're seeing a lot

205
00:06:54,720 --> 00:06:57,120
nowadays there are companies using black

206
00:06:57,120 --> 00:06:57,599
box

207
00:06:57,599 --> 00:07:00,000
machine learning solutions that you have

208
00:07:00,000 --> 00:07:02,080
in the cloud and so on right

209
00:07:02,080 --> 00:07:03,919
and even though they actually try to

210
00:07:03,919 --> 00:07:05,759
sell you some machine learning

211
00:07:05,759 --> 00:07:07,280
you know thing they're actually using

212
00:07:07,280 --> 00:07:08,720
somebody else's technology that is

213
00:07:08,720 --> 00:07:09,840
probably cloud-driven

214
00:07:09,840 --> 00:07:11,440
they're sending a whole bunch of data to

215
00:07:11,440 --> 00:07:13,120
that system

216
00:07:13,120 --> 00:07:15,440
what if you actually can manipulate that

217
00:07:15,440 --> 00:07:17,039
system and as a

218
00:07:17,039 --> 00:07:20,800
whole now not only attack one but many

219
00:07:20,800 --> 00:07:23,120
you know different resolutions right at

220
00:07:23,120 --> 00:07:24,400
the same time

221
00:07:24,400 --> 00:07:26,880
um data manipulation if you can actually

222
00:07:26,880 --> 00:07:28,000
poison

223
00:07:28,000 --> 00:07:29,840
data you can actually manipulate the

224
00:07:29,840 --> 00:07:32,000
results so i work at cisco as you

225
00:07:32,000 --> 00:07:32,880
mentioned

226
00:07:32,880 --> 00:07:35,199
we use machine learning solutions of

227
00:07:35,199 --> 00:07:36,240
course you know

228
00:07:36,240 --> 00:07:37,919
and reach here you know it's from duo as

229
00:07:37,919 --> 00:07:40,800
well and then

230
00:07:40,800 --> 00:07:41,840
one of the things that we're actually

231
00:07:41,840 --> 00:07:43,680
doing is that what if you can manipulate

232
00:07:43,680 --> 00:07:45,599
and poison network telemetry

233
00:07:45,599 --> 00:07:47,440
that could lead into different attack

234
00:07:47,440 --> 00:07:49,919
vectors or activations

235
00:07:49,919 --> 00:07:52,720
false negatives or unnecessary actions

236
00:07:52,720 --> 00:07:53,759
taken

237
00:07:53,759 --> 00:07:56,000
by an administrator or the security team

238
00:07:56,000 --> 00:07:57,680
right so all those aspects actually come

239
00:07:57,680 --> 00:07:58,560
into play

240
00:07:58,560 --> 00:08:01,599
from an adversarial perspective so

241
00:08:01,599 --> 00:08:04,240
i mean i think that's a i i kind of like

242
00:08:04,240 --> 00:08:06,319
i'm very excited to hear about like

243
00:08:06,319 --> 00:08:08,800
rich's point of view on this uh as well

244
00:08:08,800 --> 00:08:10,160
because you've been like building like

245
00:08:10,160 --> 00:08:11,759
machine learning systems

246
00:08:11,759 --> 00:08:15,520
like forever for security now so

247
00:08:15,520 --> 00:08:18,479
what is in this ai red teaming how are

248
00:08:18,479 --> 00:08:19,759
you going to kill him

249
00:08:19,759 --> 00:08:23,840
uh in this case so i think

250
00:08:23,840 --> 00:08:26,479
omar was was right on point when he

251
00:08:26,479 --> 00:08:28,080
pointed out that

252
00:08:28,080 --> 00:08:31,599
there's an entire system involved in

253
00:08:31,599 --> 00:08:33,919
doing these um and doing these attacks

254
00:08:33,919 --> 00:08:35,200
right you're not just attacking

255
00:08:35,200 --> 00:08:37,599
only the ml you're attacking the system

256
00:08:37,599 --> 00:08:38,559
that it's embedded in

257
00:08:38,559 --> 00:08:41,039
and you still have these same

258
00:08:41,039 --> 00:08:42,080
traditional

259
00:08:42,080 --> 00:08:44,159
vulnerabilities that you can go after

260
00:08:44,159 --> 00:08:45,600
and i think

261
00:08:45,600 --> 00:08:47,519
what's maybe a little bit different

262
00:08:47,519 --> 00:08:49,920
about the ai space when you get into

263
00:08:49,920 --> 00:08:51,920
stuff that's specific to ai

264
00:08:51,920 --> 00:08:57,200
it's sort of just it's it's the same

265
00:08:57,200 --> 00:08:58,959
really it's there's new capabilities

266
00:08:58,959 --> 00:09:01,120
that exist right so you have

267
00:09:01,120 --> 00:09:02,800
like you can attack and you can do like

268
00:09:02,800 --> 00:09:04,959
training data extraction you can attack

269
00:09:04,959 --> 00:09:06,880
you can do model model stealing or

270
00:09:06,880 --> 00:09:08,000
something like that

271
00:09:08,000 --> 00:09:10,480
um but you have to do it when this model

272
00:09:10,480 --> 00:09:11,200
is

273
00:09:11,200 --> 00:09:13,200
in sort of an embedded system and so we

274
00:09:13,200 --> 00:09:14,720
have academic research

275
00:09:14,720 --> 00:09:16,800
which says oh yeah these things are

276
00:09:16,800 --> 00:09:18,480
possible right we can

277
00:09:18,480 --> 00:09:20,480
do a black box attack and we can extract

278
00:09:20,480 --> 00:09:22,480
some of the training data or we can

279
00:09:22,480 --> 00:09:25,360
you know find if there was you know ppi

280
00:09:25,360 --> 00:09:26,240
that was used to

281
00:09:26,240 --> 00:09:28,320
to train with the model or something

282
00:09:28,320 --> 00:09:29,600
like that

283
00:09:29,600 --> 00:09:32,959
but most of those techniques don't seem

284
00:09:32,959 --> 00:09:34,720
to have really made the leap

285
00:09:34,720 --> 00:09:36,959
into practice and i think part of that

286
00:09:36,959 --> 00:09:38,800
is because we have this disconnect

287
00:09:38,800 --> 00:09:40,320
between treating the model

288
00:09:40,320 --> 00:09:43,440
as sort of part of a you know treating

289
00:09:43,440 --> 00:09:45,040
the model on its own which is kind of

290
00:09:45,040 --> 00:09:46,800
how it typically is handled in an

291
00:09:46,800 --> 00:09:47,920
academic setting

292
00:09:47,920 --> 00:09:50,399
versus as part of a complete system so

293
00:09:50,399 --> 00:09:51,760
when you're talking to people that are

294
00:09:51,760 --> 00:09:53,279
building and deploying models

295
00:09:53,279 --> 00:09:55,839
in the industry right now we are

296
00:09:55,839 --> 00:09:57,519
thinking about those things right

297
00:09:57,519 --> 00:10:01,040
we we fuzz our feature our feature

298
00:10:01,040 --> 00:10:01,760
extraction

299
00:10:01,760 --> 00:10:03,200
to make sure that there's no crashes

300
00:10:03,200 --> 00:10:05,920
there we sanitize our inputs we double

301
00:10:05,920 --> 00:10:06,320
check

302
00:10:06,320 --> 00:10:08,000
uh double check our telemetry and we

303
00:10:08,000 --> 00:10:09,920
keep an eye on telemetry to see

304
00:10:09,920 --> 00:10:11,680
if where things are going so all of sort

305
00:10:11,680 --> 00:10:14,240
of like the good software hygiene

306
00:10:14,240 --> 00:10:15,839
that comes along with deploying any

307
00:10:15,839 --> 00:10:18,480
application ml or not it's

308
00:10:18,480 --> 00:10:20,399
it's very similar you have very strong

309
00:10:20,399 --> 00:10:21,760
parallels between those

310
00:10:21,760 --> 00:10:24,800
um where we sort of fall over

311
00:10:24,800 --> 00:10:27,519
is we know that there are these other

312
00:10:27,519 --> 00:10:29,360
kinds of attacks that can be launched

313
00:10:29,360 --> 00:10:29,920
against

314
00:10:29,920 --> 00:10:32,880
ai driven systems but right now it's all

315
00:10:32,880 --> 00:10:34,399
sort of in the academic space

316
00:10:34,399 --> 00:10:36,399
and so i think this is where we're kind

317
00:10:36,399 --> 00:10:37,440
of hoping

318
00:10:37,440 --> 00:10:39,279
that that red teams can lead the way

319
00:10:39,279 --> 00:10:40,480
right as they find

320
00:10:40,480 --> 00:10:42,320
which of these theoretical attacks are

321
00:10:42,320 --> 00:10:43,760
actually sort of

322
00:10:43,760 --> 00:10:45,760
practically achievable and practically

323
00:10:45,760 --> 00:10:47,200
deployable

324
00:10:47,200 --> 00:10:50,000
that gives people who are invested in

325
00:10:50,000 --> 00:10:51,360
defending these models

326
00:10:51,360 --> 00:10:53,200
specifics right as opposed to just

327
00:10:53,200 --> 00:10:54,720
trying to prove a negative right tell me

328
00:10:54,720 --> 00:10:56,079
that you know

329
00:10:56,079 --> 00:10:58,240
to me that this model cannot is not

330
00:10:58,240 --> 00:11:00,640
subject to a data extraction attack

331
00:11:00,640 --> 00:11:03,200
versus we've done this one now defend

332
00:11:03,200 --> 00:11:04,800
against it

333
00:11:04,800 --> 00:11:07,519
yeah i really like your point about like

334
00:11:07,519 --> 00:11:09,279
how a lot of these attacks are in this

335
00:11:09,279 --> 00:11:11,279
academic space i want to kind of like

336
00:11:11,279 --> 00:11:13,839
um get to anita just real fast but we

337
00:11:13,839 --> 00:11:15,360
have somebody with us

338
00:11:15,360 --> 00:11:18,160
who's actually reading leading an ai red

339
00:11:18,160 --> 00:11:18,880
team

340
00:11:18,880 --> 00:11:22,079
uh chris you kind of like you know uh

341
00:11:22,079 --> 00:11:24,079
are leading this like nvidia's ai red

342
00:11:24,079 --> 00:11:26,240
team you know we just heard from like

343
00:11:26,240 --> 00:11:28,160
rich that a lot of these attacks are

344
00:11:28,160 --> 00:11:30,720
still like in the academic phase

345
00:11:30,720 --> 00:11:32,560
and kind of like bruce also kind of said

346
00:11:32,560 --> 00:11:34,480
hey we've got like humans going after

347
00:11:34,480 --> 00:11:35,040
like

348
00:11:35,040 --> 00:11:36,959
machine learning systems there is no

349
00:11:36,959 --> 00:11:39,200
like ai behind it at this point

350
00:11:39,200 --> 00:11:41,200
so what exactly do you do chris at

351
00:11:41,200 --> 00:11:43,920
nvidia what exactly is like

352
00:11:43,920 --> 00:11:47,040
you know the nvidia ai red teams charter

353
00:11:47,040 --> 00:11:49,200
why did you create this team and what

354
00:11:49,200 --> 00:11:51,120
exactly are you doing

355
00:11:51,120 --> 00:11:54,240
in terms of attacking ml systems sure

356
00:11:54,240 --> 00:11:57,200
um i'm super excited to be here by the

357
00:11:57,200 --> 00:11:57,519
way

358
00:11:57,519 --> 00:11:59,839
um this is a new field for a lot of us

359
00:11:59,839 --> 00:12:01,120
and

360
00:12:01,120 --> 00:12:04,480
uh we started it because it sounded cool

361
00:12:04,480 --> 00:12:07,279
first and foremost and and second of all

362
00:12:07,279 --> 00:12:08,160
because like

363
00:12:08,160 --> 00:12:11,200
uh specifically at nvidia we

364
00:12:11,200 --> 00:12:13,680
we have a lot of ai things we have a lot

365
00:12:13,680 --> 00:12:15,440
of machine learning things

366
00:12:15,440 --> 00:12:17,839
um and like omar mentioned you get a lot

367
00:12:17,839 --> 00:12:19,200
of these companies that have like black

368
00:12:19,200 --> 00:12:21,279
boxes they take models they

369
00:12:21,279 --> 00:12:23,839
they ingest models from other places we

370
00:12:23,839 --> 00:12:24,880
are actually creating

371
00:12:24,880 --> 00:12:29,200
some models on our own so it behooves us

372
00:12:29,200 --> 00:12:31,760
to figure out how to do this stuff um

373
00:12:31,760 --> 00:12:32,720
because not only

374
00:12:32,720 --> 00:12:35,760
are we consumers we're also suppliers in

375
00:12:35,760 --> 00:12:36,800
some regards

376
00:12:36,800 --> 00:12:39,839
um so a lot of people came to

377
00:12:39,839 --> 00:12:41,760
me and my team and they were like when

378
00:12:41,760 --> 00:12:43,200
are you gonna start doing ai red team or

379
00:12:43,200 --> 00:12:44,800
how do we hack an ai or how do we hack

380
00:12:44,800 --> 00:12:45,440
ml

381
00:12:45,440 --> 00:12:48,320
and you know a year ago we really can

382
00:12:48,320 --> 00:12:49,680
answer that question

383
00:12:49,680 --> 00:12:51,279
and you know red teamers we like

384
00:12:51,279 --> 00:12:53,600
challenges and we

385
00:12:53,600 --> 00:12:55,120
just tried to do what we could we

386
00:12:55,120 --> 00:12:57,279
started researching it and a year later

387
00:12:57,279 --> 00:12:58,079
you know we've

388
00:12:58,079 --> 00:12:59,680
we've gotten into partnerships like

389
00:12:59,680 --> 00:13:01,360
these and

390
00:13:01,360 --> 00:13:04,480
um it's still very table top-ish

391
00:13:04,480 --> 00:13:06,480
right now but we are getting to the

392
00:13:06,480 --> 00:13:08,079
point where we're

393
00:13:08,079 --> 00:13:09,440
gonna start doing some operations

394
00:13:09,440 --> 00:13:11,680
probably soon um

395
00:13:11,680 --> 00:13:13,440
chris can i follow up um you know i

396
00:13:13,440 --> 00:13:15,440
think you mentioned the core of today's

397
00:13:15,440 --> 00:13:16,959
talk which is like

398
00:13:16,959 --> 00:13:19,040
hack ai systems what does it mean in

399
00:13:19,040 --> 00:13:20,240
practice like

400
00:13:20,240 --> 00:13:21,519
you know even if you're doing tabletop

401
00:13:21,519 --> 00:13:23,120
exercise can you just like tell the red

402
00:13:23,120 --> 00:13:24,240
team members

403
00:13:24,240 --> 00:13:27,680
what an exercise would look like

404
00:13:27,680 --> 00:13:29,760
yeah sure um so some of the things we

405
00:13:29,760 --> 00:13:31,360
have scoped out so far

406
00:13:31,360 --> 00:13:34,079
there's there to me there's three main

407
00:13:34,079 --> 00:13:35,279
attack areas

408
00:13:35,279 --> 00:13:37,600
uh that we can go into is like when

409
00:13:37,600 --> 00:13:39,360
you're actually attacking an ai system

410
00:13:39,360 --> 00:13:39,920
there's

411
00:13:39,920 --> 00:13:42,320
as the model is being trained as like

412
00:13:42,320 --> 00:13:44,000
after the models trained and after the

413
00:13:44,000 --> 00:13:45,279
model is deployed

414
00:13:45,279 --> 00:13:47,519
um so each of those three phases you

415
00:13:47,519 --> 00:13:49,920
have different attack scenarios and

416
00:13:49,920 --> 00:13:51,600
depending on what our customer was or

417
00:13:51,600 --> 00:13:53,440
how we wanted to do things we would have

418
00:13:53,440 --> 00:13:54,399
to figure out

419
00:13:54,399 --> 00:13:56,160
what type of operation we wanted to do

420
00:13:56,160 --> 00:13:57,680
do we want to poison the model do we

421
00:13:57,680 --> 00:13:59,199
want to replicate the model

422
00:13:59,199 --> 00:14:01,360
do we want to you know after the model's

423
00:14:01,360 --> 00:14:02,720
been deployed like are we going to try

424
00:14:02,720 --> 00:14:04,639
to test systems to see if we can confuse

425
00:14:04,639 --> 00:14:05,279
it

426
00:14:05,279 --> 00:14:08,880
um so there's there's there's a lot to

427
00:14:08,880 --> 00:14:10,560
it it's like a mile wide and an inch

428
00:14:10,560 --> 00:14:12,480
deep

429
00:14:12,480 --> 00:14:14,959
so at least that's what i'm thinking so

430
00:14:14,959 --> 00:14:16,800
far like i said we're pretty new to this

431
00:14:16,800 --> 00:14:18,720
too

432
00:14:18,720 --> 00:14:20,720
oh thank you i'm gonna i'm gonna pull

433
00:14:20,720 --> 00:14:22,160
that through a little bit more

434
00:14:22,160 --> 00:14:25,279
but i first wanted to get dr uh nicholas

435
00:14:25,279 --> 00:14:28,320
uh into their conversation so

436
00:14:28,320 --> 00:14:31,279
yeah you're hearing like you know rich's

437
00:14:31,279 --> 00:14:33,120
comment about how a lot of these are

438
00:14:33,120 --> 00:14:34,480
like academic

439
00:14:34,480 --> 00:14:36,399
uh can you paint us a picture on what

440
00:14:36,399 --> 00:14:37,680
exactly

441
00:14:37,680 --> 00:14:39,199
are those types of attacks that

442
00:14:39,199 --> 00:14:40,959
academicians are kind of like

443
00:14:40,959 --> 00:14:42,560
thinking about on machine learning

444
00:14:42,560 --> 00:14:44,560
systems and do they have actual real

445
00:14:44,560 --> 00:14:45,199
impact

446
00:14:45,199 --> 00:14:47,040
or is this more theoretical kind of

447
00:14:47,040 --> 00:14:48,480
exercise

448
00:14:48,480 --> 00:14:50,399
so uh since i'm not in my day job i can

449
00:14:50,399 --> 00:14:51,839
be a little down on

450
00:14:51,839 --> 00:14:54,160
on ai and ai security and academia so ai

451
00:14:54,160 --> 00:14:55,680
security is a new concept for

452
00:14:55,680 --> 00:14:56,839
academicians

453
00:14:56,839 --> 00:14:59,760
um ai gets a lot of federal funding by

454
00:14:59,760 --> 00:15:01,279
the way nsf just for another round of

455
00:15:01,279 --> 00:15:02,880
these ai institutes none of them are

456
00:15:02,880 --> 00:15:05,120
focused on ai security

457
00:15:05,120 --> 00:15:06,959
but the focus being on agriculture

458
00:15:06,959 --> 00:15:08,880
chemicals water physics science if you

459
00:15:08,880 --> 00:15:09,920
can imagine nothing

460
00:15:09,920 --> 00:15:12,000
in there talks about the security of any

461
00:15:12,000 --> 00:15:14,079
of this ai

462
00:15:14,079 --> 00:15:16,480
um i think much like you know a few

463
00:15:16,480 --> 00:15:18,320
years ago disinformation

464
00:15:18,320 --> 00:15:19,760
and that whole concept was like a big

465
00:15:19,760 --> 00:15:21,519
no-no for researching

466
00:15:21,519 --> 00:15:22,959
uh but now we figured out it's important

467
00:15:22,959 --> 00:15:23,920
i think the same thing is going to

468
00:15:23,920 --> 00:15:25,600
happen with ai security

469
00:15:25,600 --> 00:15:26,880
but to get back to your question many of

470
00:15:26,880 --> 00:15:29,040
these attacks on ai are very esoteric

471
00:15:29,040 --> 00:15:30,560
and they are focused on different

472
00:15:30,560 --> 00:15:32,399
portions in the pipeline

473
00:15:32,399 --> 00:15:34,639
my frustration is that ai is a pipeline

474
00:15:34,639 --> 00:15:35,920
right there's data collection and

475
00:15:35,920 --> 00:15:37,440
cleaning and ethics and

476
00:15:37,440 --> 00:15:39,839
system and software and most of the

477
00:15:39,839 --> 00:15:42,240
academic attacks focus on one esoteric

478
00:15:42,240 --> 00:15:44,240
niche of that not the entire pipeline

479
00:15:44,240 --> 00:15:47,279
and i think that's that's a big problem

480
00:15:47,279 --> 00:15:49,040
and and can you also help us explain

481
00:15:49,040 --> 00:15:51,040
like when when people talk about ai

482
00:15:51,040 --> 00:15:52,079
security

483
00:15:52,079 --> 00:15:55,519
uh is it like attacking ai systems you

484
00:15:55,519 --> 00:15:58,320
mentioned misinformation like gpd3 to

485
00:15:58,320 --> 00:15:59,600
kind of like generate like

486
00:15:59,600 --> 00:16:00,880
disinformation

487
00:16:00,880 --> 00:16:02,320
then there's like what bruce can have

488
00:16:02,320 --> 00:16:04,480
alluded to in this awesome keynote about

489
00:16:04,480 --> 00:16:06,959
the coming of ai attackers so what

490
00:16:06,959 --> 00:16:08,000
exactly

491
00:16:08,000 --> 00:16:09,680
is ai security and how are people

492
00:16:09,680 --> 00:16:11,839
perceiving that anita

493
00:16:11,839 --> 00:16:14,160
so i mean ai security we know that you

494
00:16:14,160 --> 00:16:14,880
know as

495
00:16:14,880 --> 00:16:16,800
more these ai systems proliferate we

496
00:16:16,800 --> 00:16:18,320
know that there's unanticipated

497
00:16:18,320 --> 00:16:19,279
behaviors

498
00:16:19,279 --> 00:16:21,759
one of the places academia can focus is

499
00:16:21,759 --> 00:16:23,759
on how do we quantify how do i iden how

500
00:16:23,759 --> 00:16:25,600
do we identify these unanticipated

501
00:16:25,600 --> 00:16:26,639
how do we how do we deal with the

502
00:16:26,639 --> 00:16:28,800
unknowns and right now that's becoming

503
00:16:28,800 --> 00:16:29,920
really impossible

504
00:16:29,920 --> 00:16:31,279
uh one thing i'll throw out there that i

505
00:16:31,279 --> 00:16:33,360
think um i've seen a little bit in

506
00:16:33,360 --> 00:16:34,959
academia and i wish i would see more but

507
00:16:34,959 --> 00:16:36,560
many people are not keen on this

508
00:16:36,560 --> 00:16:38,399
is to think about design thinking and

509
00:16:38,399 --> 00:16:39,680
futures thinking

510
00:16:39,680 --> 00:16:41,120
and that really takes you know the whole

511
00:16:41,120 --> 00:16:42,959
pipeline to say okay there's

512
00:16:42,959 --> 00:16:46,000
this this type of system what's the best

513
00:16:46,000 --> 00:16:47,839
use of it what's the worst use of it

514
00:16:47,839 --> 00:16:49,519
what's the optimal use of it how could

515
00:16:49,519 --> 00:16:51,360
an attacker do it so really almost like

516
00:16:51,360 --> 00:16:52,079
um

517
00:16:52,079 --> 00:16:53,680
almost a tabletop exercise but thinking

518
00:16:53,680 --> 00:16:56,079
in the future and this is facilitated

519
00:16:56,079 --> 00:16:56,800
not by

520
00:16:56,800 --> 00:16:59,199
technical people like many of us but by

521
00:16:59,199 --> 00:17:01,440
design thinkers to think how society can

522
00:17:01,440 --> 00:17:02,639
use this and then thinking about the

523
00:17:02,639 --> 00:17:03,600
pipeline

524
00:17:03,600 --> 00:17:04,880
then you could have the you know the

525
00:17:04,880 --> 00:17:06,559
more academic people think all along

526
00:17:06,559 --> 00:17:07,199
that way

527
00:17:07,199 --> 00:17:10,000
where can i then attack it just one

528
00:17:10,000 --> 00:17:11,359
suggestion

529
00:17:11,359 --> 00:17:13,599
uh absolutely uh what do you mean by

530
00:17:13,599 --> 00:17:14,959
design thinking anita

531
00:17:14,959 --> 00:17:17,039
i'm sorry can you just like explain that

532
00:17:17,039 --> 00:17:18,240
concept a little bit

533
00:17:18,240 --> 00:17:19,919
yeah so designers i mean there's whole

534
00:17:19,919 --> 00:17:21,439
schools of design

535
00:17:21,439 --> 00:17:23,119
and one of the things they do is called

536
00:17:23,119 --> 00:17:24,959
futures thinking this is something that

537
00:17:24,959 --> 00:17:26,240
came out in the 70s so

538
00:17:26,240 --> 00:17:27,839
you might imagine like this and i took a

539
00:17:27,839 --> 00:17:29,520
futures design class for fun and

540
00:17:29,520 --> 00:17:30,880
somebody did this which is what they

541
00:17:30,880 --> 00:17:31,600
said

542
00:17:31,600 --> 00:17:33,679
what if funding for public art was no

543
00:17:33,679 --> 00:17:35,039
longer what would happen

544
00:17:35,039 --> 00:17:36,559
well maybe a company like amazon would

545
00:17:36,559 --> 00:17:38,559
make public art but maybe

546
00:17:38,559 --> 00:17:40,080
and this is part of the design thinking

547
00:17:40,080 --> 00:17:42,640
you sit and think about these futures

548
00:17:42,640 --> 00:17:44,559
if amazon funded public art maybe it'd

549
00:17:44,559 --> 00:17:45,840
have to be a prime member to see the

550
00:17:45,840 --> 00:17:47,520
public art well how would that work

551
00:17:47,520 --> 00:17:49,280
and so the kind of storyboard i think

552
00:17:49,280 --> 00:17:50,720
all through and

553
00:17:50,720 --> 00:17:53,760
uh ashken sultani who was an ftc cto had

554
00:17:53,760 --> 00:17:55,600
this great coin this great phrase about

555
00:17:55,600 --> 00:17:57,200
abusability thinking

556
00:17:57,200 --> 00:17:59,039
so how can we use how can we abuse the

557
00:17:59,039 --> 00:18:00,799
ai and i think if we think in the

558
00:18:00,799 --> 00:18:02,559
broader context

559
00:18:02,559 --> 00:18:04,400
that's when you can really think about

560
00:18:04,400 --> 00:18:06,000
red teaming i mean much of red teaming

561
00:18:06,000 --> 00:18:06,559
is the

562
00:18:06,559 --> 00:18:08,799
physical aspect of social engineering

563
00:18:08,799 --> 00:18:10,960
the technical trashing a model is easy

564
00:18:10,960 --> 00:18:12,640
it's everything around it how do you get

565
00:18:12,640 --> 00:18:13,919
to it how are you going to poison this

566
00:18:13,919 --> 00:18:14,559
model

567
00:18:14,559 --> 00:18:17,120
i think design thinking can help us

568
00:18:17,120 --> 00:18:18,480
that's awesome because like

569
00:18:18,480 --> 00:18:20,320
i have a question for bruce which i

570
00:18:20,320 --> 00:18:22,320
think just tailors on to the brass tax

571
00:18:22,320 --> 00:18:24,320
aspect

572
00:18:24,320 --> 00:18:26,720
are we even prepared if attacks on

573
00:18:26,720 --> 00:18:28,480
ancestors are coming are we prepared for

574
00:18:28,480 --> 00:18:29,520
this

575
00:18:29,520 --> 00:18:31,120
you know i think we're never prepared

576
00:18:31,120 --> 00:18:32,960
abusability thinking is the security

577
00:18:32,960 --> 00:18:34,960
mindset right how does it fail how can

578
00:18:34,960 --> 00:18:36,480
it be made to fail

579
00:18:36,480 --> 00:18:38,559
and if you think back to the the things

580
00:18:38,559 --> 00:18:39,919
omar was saying

581
00:18:39,919 --> 00:18:42,480
all of that those adversarial ways of

582
00:18:42,480 --> 00:18:43,679
thinking about

583
00:18:43,679 --> 00:18:46,480
ai machine learning and we're never

584
00:18:46,480 --> 00:18:47,440
prepared because

585
00:18:47,440 --> 00:18:49,360
people don't think about security i mean

586
00:18:49,360 --> 00:18:51,039
you just point that the funding is

587
00:18:51,039 --> 00:18:54,000
is for topics for things the system does

588
00:18:54,000 --> 00:18:56,000
not the things the system prevents

589
00:18:56,000 --> 00:18:58,480
and we saw this we saw this two days ago

590
00:18:58,480 --> 00:19:00,400
apple announced this system they're

591
00:19:00,400 --> 00:19:03,440
going to scan your iphone looking for uh

592
00:19:03,440 --> 00:19:06,799
for uh for abuse images yeah and

593
00:19:06,799 --> 00:19:08,720
you read theirs they have a lot of

594
00:19:08,720 --> 00:19:10,799
security they do not

595
00:19:10,799 --> 00:19:13,760
talk about the adversarial ml aspects at

596
00:19:13,760 --> 00:19:14,320
all

597
00:19:14,320 --> 00:19:16,480
they do not assume an adversarial model

598
00:19:16,480 --> 00:19:18,400
they do not assume data poisoning they

599
00:19:18,400 --> 00:19:18,960
do not

600
00:19:18,960 --> 00:19:20,960
they do not assume any of those things

601
00:19:20,960 --> 00:19:22,000
and this is apple

602
00:19:22,000 --> 00:19:25,440
this isn't some bunch of idiots right so

603
00:19:25,440 --> 00:19:28,960
here again and i see it again and again

604
00:19:28,960 --> 00:19:31,520
people design systems for functionality

605
00:19:31,520 --> 00:19:32,559
for how they work

606
00:19:32,559 --> 00:19:35,039
they don't design them for how they fail

607
00:19:35,039 --> 00:19:37,520
and i like the the phrase abusability

608
00:19:37,520 --> 00:19:38,960
thinking i think of it as a security

609
00:19:38,960 --> 00:19:39,760
mindset

610
00:19:39,760 --> 00:19:41,760
it is something we at defcon have been

611
00:19:41,760 --> 00:19:43,200
doing since

612
00:19:43,200 --> 00:19:45,360
forever and it is the way we think of

613
00:19:45,360 --> 00:19:47,120
systems and no of course

614
00:19:47,120 --> 00:19:49,360
we're not ready because nobody's calling

615
00:19:49,360 --> 00:19:51,440
us when they're designing the systems

616
00:19:51,440 --> 00:19:52,960
they're just designing them

617
00:19:52,960 --> 00:19:54,799
and and bruce for people who are just

618
00:19:54,799 --> 00:19:56,160
like um you know

619
00:19:56,160 --> 00:19:58,640
who are not exposed to adversarial ml

620
00:19:58,640 --> 00:20:00,480
can you give us an example of what

621
00:20:00,480 --> 00:20:03,840
um adversarial ml thinking in the apple

622
00:20:03,840 --> 00:20:06,240
case would look like is there an example

623
00:20:06,240 --> 00:20:07,200
that comes through

624
00:20:07,200 --> 00:20:08,799
so there are easy examples so image

625
00:20:08,799 --> 00:20:10,960
classifiers classify images and they

626
00:20:10,960 --> 00:20:12,400
assume the images are like

627
00:20:12,400 --> 00:20:15,039
regular images there's an entire class

628
00:20:15,039 --> 00:20:16,000
of research

629
00:20:16,000 --> 00:20:18,559
of making changes to images that the

630
00:20:18,559 --> 00:20:20,400
human eye don'ts and detect

631
00:20:20,400 --> 00:20:23,039
that the image classifier uh

632
00:20:23,039 --> 00:20:24,960
deliberately fails

633
00:20:24,960 --> 00:20:28,320
there's uh you can put stickers on stop

634
00:20:28,320 --> 00:20:28,720
signs

635
00:20:28,720 --> 00:20:30,880
turn them into machine uh speed limit

636
00:20:30,880 --> 00:20:33,440
signs that the ml systems and cars will

637
00:20:33,440 --> 00:20:34,159
ignore

638
00:20:34,159 --> 00:20:35,919
there are ways you can change though the

639
00:20:35,919 --> 00:20:37,520
famous one is changing a

640
00:20:37,520 --> 00:20:40,480
turtle into a rifle just just look up

641
00:20:40,480 --> 00:20:42,799
image classifier hacks and you'll see

642
00:20:42,799 --> 00:20:45,200
lots of them and that's an easy example

643
00:20:45,200 --> 00:20:47,039
that's not even a hard one

644
00:20:47,039 --> 00:20:49,280
that is i mean i feel like especially

645
00:20:49,280 --> 00:20:50,159
attacks on

646
00:20:50,159 --> 00:20:52,400
images are so captured people's

647
00:20:52,400 --> 00:20:54,559
imagination and i kind of want to bring

648
00:20:54,559 --> 00:20:57,600
rich in here um which

649
00:20:57,600 --> 00:21:00,720
uh before we get into like you know the

650
00:21:00,720 --> 00:21:02,000
challenge i want to kind of like can you

651
00:21:02,000 --> 00:21:04,240
talk through the chaos that happened

652
00:21:04,240 --> 00:21:07,200
with twitter's image cropping algorithm

653
00:21:07,200 --> 00:21:10,799
like um so um

654
00:21:10,799 --> 00:21:13,039
i should clarify that i don't work for

655
00:21:13,039 --> 00:21:14,240
twitter uh

656
00:21:14,240 --> 00:21:18,400
okay um i was i was an interested

657
00:21:18,400 --> 00:21:19,520
bystander in this

658
00:21:19,520 --> 00:21:22,880
and uh yeah essentially what happened

659
00:21:22,880 --> 00:21:23,600
was

660
00:21:23,600 --> 00:21:26,400
um people began to notice so twitter has

661
00:21:26,400 --> 00:21:27,919
a cropping algorithm

662
00:21:27,919 --> 00:21:30,000
by which if you have a very large photo

663
00:21:30,000 --> 00:21:31,520
that you post into a tweet it will

664
00:21:31,520 --> 00:21:33,280
actually try to reduce down

665
00:21:33,280 --> 00:21:35,760
to a narrow portion of that that could

666
00:21:35,760 --> 00:21:36,400
be shown

667
00:21:36,400 --> 00:21:38,880
in the complete stream of tweets um so

668
00:21:38,880 --> 00:21:40,240
that it doesn't you know one image

669
00:21:40,240 --> 00:21:41,120
doesn't blow up

670
00:21:41,120 --> 00:21:43,120
you know and dominate an entire

671
00:21:43,120 --> 00:21:44,960
somebody's entire feed

672
00:21:44,960 --> 00:21:47,919
what people noticed was that it was

673
00:21:47,919 --> 00:21:48,960
beginning

674
00:21:48,960 --> 00:21:51,840
it was cropping in strange ways so if

675
00:21:51,840 --> 00:21:52,960
you put

676
00:21:52,960 --> 00:21:55,039
a person of african descent next to a

677
00:21:55,039 --> 00:21:56,799
person next to a white person it might

678
00:21:56,799 --> 00:21:58,799
preferentially crop the white person

679
00:21:58,799 --> 00:22:01,679
if you put a woman up it very often

680
00:22:01,679 --> 00:22:02,720
tended to focus

681
00:22:02,720 --> 00:22:05,760
on their chest um and

682
00:22:05,760 --> 00:22:08,960
a lot of this was driven by the fact

683
00:22:08,960 --> 00:22:10,320
that what they did was they used a gaze

684
00:22:10,320 --> 00:22:11,679
tracking algorithm

685
00:22:11,679 --> 00:22:14,799
to train a saliency model to say what

686
00:22:14,799 --> 00:22:16,480
parts of the image would be most

687
00:22:16,480 --> 00:22:18,799
interesting to look at

688
00:22:18,799 --> 00:22:22,559
unfortunately the population i believe

689
00:22:22,559 --> 00:22:25,760
skewed white and male and so that drove

690
00:22:25,760 --> 00:22:28,080
the saliency algorithm which then drove

691
00:22:28,080 --> 00:22:28,720
the cropping

692
00:22:28,720 --> 00:22:31,280
so again it's back to what anita was

693
00:22:31,280 --> 00:22:33,520
talking about it's this entire pipeline

694
00:22:33,520 --> 00:22:35,679
of decisions that you have to consider

695
00:22:35,679 --> 00:22:39,200
um so to their credit twitter did

696
00:22:39,200 --> 00:22:40,720
immediately address that they they

697
00:22:40,720 --> 00:22:43,039
released a blog post where they

698
00:22:43,039 --> 00:22:46,960
analyzed the results and they found that

699
00:22:46,960 --> 00:22:48,720
really there was there was some small

700
00:22:48,720 --> 00:22:50,640
bias there but what was happening

701
00:22:50,640 --> 00:22:53,440
was people were finding sort of the

702
00:22:53,440 --> 00:22:55,760
remarkable events and highlighting those

703
00:22:55,760 --> 00:22:57,760
because those have the most impact

704
00:22:57,760 --> 00:22:59,919
um and they've also been very very open

705
00:22:59,919 --> 00:23:01,679
and transparent with this ethics bug

706
00:23:01,679 --> 00:23:02,720
bounty that they've

707
00:23:02,720 --> 00:23:05,120
uh launched in collaboration with the ai

708
00:23:05,120 --> 00:23:05,919
village and again

709
00:23:05,919 --> 00:23:07,440
that's that's something that all credit

710
00:23:07,440 --> 00:23:09,200
goes to ramon and utah

711
00:23:09,200 --> 00:23:12,000
and their teams um for pushing that

712
00:23:12,000 --> 00:23:13,280
through twitter uh

713
00:23:13,280 --> 00:23:15,200
really we we hosted it and we sort of

714
00:23:15,200 --> 00:23:16,880
helped them kick the tires and think

715
00:23:16,880 --> 00:23:17,760
through some of the

716
00:23:17,760 --> 00:23:20,720
issues with the bounty a little bit um

717
00:23:20,720 --> 00:23:22,000
but yeah it's it's another

718
00:23:22,000 --> 00:23:25,360
illustration of how what seems like a

719
00:23:25,360 --> 00:23:27,360
series of pretty good ideas

720
00:23:27,360 --> 00:23:29,840
um can actually lead to a machine

721
00:23:29,840 --> 00:23:31,120
learning classifier

722
00:23:31,120 --> 00:23:33,600
that can have an impact that actually

723
00:23:33,600 --> 00:23:35,200
upsets or even harms

724
00:23:35,200 --> 00:23:37,919
some people by cropping them out of

725
00:23:37,919 --> 00:23:39,200
photos where they really should have

726
00:23:39,200 --> 00:23:40,720
been the center of focus

727
00:23:40,720 --> 00:23:44,159
or um highlighting bits of their anatomy

728
00:23:44,159 --> 00:23:44,640
when

729
00:23:44,640 --> 00:23:47,120
really what should be highlighted is

730
00:23:47,120 --> 00:23:48,640
what they did or

731
00:23:48,640 --> 00:23:51,279
things like that so yeah i guess does

732
00:23:51,279 --> 00:23:52,559
that answer the question yeah

733
00:23:52,559 --> 00:23:55,760
i was i i really enjoyed the twitter

734
00:23:55,760 --> 00:23:57,360
bounty because i think it's a concrete

735
00:23:57,360 --> 00:23:59,279
example of how an organization is trying

736
00:23:59,279 --> 00:24:00,080
to bring

737
00:24:00,080 --> 00:24:01,919
uh anita's like anita streaming of

738
00:24:01,919 --> 00:24:03,200
abusability thinking

739
00:24:03,200 --> 00:24:06,240
into this like space um i would love for

740
00:24:06,240 --> 00:24:07,200
you to kind of like

741
00:24:07,200 --> 00:24:08,960
tell like people who want to get into

742
00:24:08,960 --> 00:24:10,559
this ai red teaming

743
00:24:10,559 --> 00:24:13,279
do you need to know math who actually

744
00:24:13,279 --> 00:24:14,799
can test ai systems

745
00:24:14,799 --> 00:24:16,880
should you actually have already machine

746
00:24:16,880 --> 00:24:20,400
learning knowledge to work in this space

747
00:24:20,400 --> 00:24:23,679
so i think it depends on how deep

748
00:24:23,679 --> 00:24:25,600
into the space you want to get like with

749
00:24:25,600 --> 00:24:26,880
any other pen testing

750
00:24:26,880 --> 00:24:29,440
there is sort of different levels you

751
00:24:29,440 --> 00:24:30,240
can do it at

752
00:24:30,240 --> 00:24:33,360
if you want to just do sort of high

753
00:24:33,360 --> 00:24:34,000
level

754
00:24:34,000 --> 00:24:35,600
throw stuff at the wall use known

755
00:24:35,600 --> 00:24:37,679
attacks redeploy them we're starting to

756
00:24:37,679 --> 00:24:38,159
see

757
00:24:38,159 --> 00:24:40,320
tools and frameworks come out so you've

758
00:24:40,320 --> 00:24:41,360
got things like

759
00:24:41,360 --> 00:24:45,200
toucan strike counterfeit clever hans is

760
00:24:45,200 --> 00:24:46,080
another example

761
00:24:46,080 --> 00:24:48,240
so frameworks that will actually let you

762
00:24:48,240 --> 00:24:49,279
execute

763
00:24:49,279 --> 00:24:52,960
um attacks against ml systems yourself

764
00:24:52,960 --> 00:24:56,400
and really you don't need much technical

765
00:24:56,400 --> 00:24:58,000
depth to be able to do that basically

766
00:24:58,000 --> 00:24:59,200
you need to be able to figure out how to

767
00:24:59,200 --> 00:25:00,159
run a model

768
00:25:00,159 --> 00:25:01,440
because like metasploit for machine

769
00:25:01,440 --> 00:25:04,240
learning sorry that is that how it works

770
00:25:04,240 --> 00:25:06,559
counterfeit is yeah broadly i think you

771
00:25:06,559 --> 00:25:08,400
can you can think of it as metasploit

772
00:25:08,400 --> 00:25:09,440
for machine learning the other

773
00:25:09,440 --> 00:25:10,720
frameworks are

774
00:25:10,720 --> 00:25:13,840
a little bit too can strike um tries to

775
00:25:13,840 --> 00:25:15,520
sort of go in the same direction clever

776
00:25:15,520 --> 00:25:16,799
hans

777
00:25:16,799 --> 00:25:18,080
you need to get your hands a little bit

778
00:25:18,080 --> 00:25:21,200
dirtier none of them require

779
00:25:21,200 --> 00:25:22,880
you know a phd in mathematics or

780
00:25:22,880 --> 00:25:24,320
statistics to

781
00:25:24,320 --> 00:25:28,159
to really get into um as you start to

782
00:25:28,159 --> 00:25:29,840
push the boundaries a little more and

783
00:25:29,840 --> 00:25:31,200
you want to think about developing your

784
00:25:31,200 --> 00:25:32,000
own attacks

785
00:25:32,000 --> 00:25:33,760
or you want to do stranger things with

786
00:25:33,760 --> 00:25:35,039
the models

787
00:25:35,039 --> 00:25:37,679
then you begin i think that's when you

788
00:25:37,679 --> 00:25:39,279
begin to need a little bit more of the

789
00:25:39,279 --> 00:25:41,039
specialized knowledge about how machine

790
00:25:41,039 --> 00:25:42,720
learning works what sort of

791
00:25:42,720 --> 00:25:44,960
inputs and outputs you expect from them

792
00:25:44,960 --> 00:25:46,400
but if you take a look you know moving

793
00:25:46,400 --> 00:25:47,520
it back to the

794
00:25:47,520 --> 00:25:49,679
twitter ethics bug bounty a lot of that

795
00:25:49,679 --> 00:25:51,600
was entirely data driven um

796
00:25:51,600 --> 00:25:54,240
all people needed to do was collect data

797
00:25:54,240 --> 00:25:55,840
that showed some sort of disparate

798
00:25:55,840 --> 00:25:57,600
impact or some sort of harm

799
00:25:57,600 --> 00:25:59,279
uh run it through the model to

800
00:25:59,279 --> 00:26:01,039
demonstrate that harm or that

801
00:26:01,039 --> 00:26:04,640
that impact and you could produce um

802
00:26:04,640 --> 00:26:07,760
you know essentially a about right it's

803
00:26:07,760 --> 00:26:09,120
undesired behavior

804
00:26:09,120 --> 00:26:11,840
even undesired behavior in the sense

805
00:26:11,840 --> 00:26:13,840
that the entire pipeline is working

806
00:26:13,840 --> 00:26:16,320
kind of as it's programmed to it just

807
00:26:16,320 --> 00:26:18,080
gave you what you

808
00:26:18,080 --> 00:26:20,960
asked for not what you really wanted so

809
00:26:20,960 --> 00:26:21,760
again

810
00:26:21,760 --> 00:26:23,919
any you know there's entry points at all

811
00:26:23,919 --> 00:26:25,520
levels i really do encourage people to

812
00:26:25,520 --> 00:26:26,799
get into this because this is a really

813
00:26:26,799 --> 00:26:27,919
fascinating space

814
00:26:27,919 --> 00:26:31,919
um but to to to really sort of push the

815
00:26:31,919 --> 00:26:33,279
boundaries of it i don't think there's

816
00:26:33,279 --> 00:26:34,480
much of a way around

817
00:26:34,480 --> 00:26:36,320
uh getting your hands into at least a

818
00:26:36,320 --> 00:26:37,600
little bit of calculus

819
00:26:37,600 --> 00:26:40,320
yeah i like your perspective of like how

820
00:26:40,320 --> 00:26:40,960
should like

821
00:26:40,960 --> 00:26:43,279
red team members know machine learning i

822
00:26:43,279 --> 00:26:44,320
think um

823
00:26:44,320 --> 00:26:45,919
omar this is more like a question for

824
00:26:45,919 --> 00:26:48,159
you you've seen like red team members

825
00:26:48,159 --> 00:26:50,480
kind of grow what advice do you have for

826
00:26:50,480 --> 00:26:52,320
machine learning people who want to

827
00:26:52,320 --> 00:26:54,080
learn red teaming skills

828
00:26:54,080 --> 00:26:57,039
um like is that required for in your

829
00:26:57,039 --> 00:26:58,080
perspective

830
00:26:58,080 --> 00:27:02,400
for building like a ai pen test team

831
00:27:02,400 --> 00:27:04,400
yeah that's an it's an interesting

832
00:27:04,400 --> 00:27:06,080
question because let me define the red

833
00:27:06,080 --> 00:27:06,480
team

834
00:27:06,480 --> 00:27:09,679
part of it um so most people whenever

835
00:27:09,679 --> 00:27:10,640
they mention about

836
00:27:10,640 --> 00:27:12,559
what they call red teaming is you know

837
00:27:12,559 --> 00:27:14,000
infiltrating a building

838
00:27:14,000 --> 00:27:15,840
trying to impersonate somebody and then

839
00:27:15,840 --> 00:27:17,919
launching some exploits to attack

840
00:27:17,919 --> 00:27:20,880
a system so it's a it's a bigger scope

841
00:27:20,880 --> 00:27:22,000
pen testing right

842
00:27:22,000 --> 00:27:25,520
um now what bruce mentioned about

843
00:27:25,520 --> 00:27:27,440
people not even paying attention to

844
00:27:27,440 --> 00:27:29,600
security from the beginning

845
00:27:29,600 --> 00:27:32,080
when designing things that's the number

846
00:27:32,080 --> 00:27:33,279
one thing that i'm gonna start

847
00:27:33,279 --> 00:27:34,480
highlighting right

848
00:27:34,480 --> 00:27:36,399
how you fix this is actually not by

849
00:27:36,399 --> 00:27:37,520
doing a pen test

850
00:27:37,520 --> 00:27:39,760
or a red team engagement after the fact

851
00:27:39,760 --> 00:27:41,200
you fix it

852
00:27:41,200 --> 00:27:43,279
you know what the the boss worries you

853
00:27:43,279 --> 00:27:44,799
know moving security to the left right

854
00:27:44,799 --> 00:27:46,240
at that moment that you're trying to

855
00:27:46,240 --> 00:27:48,559
design these systems

856
00:27:48,559 --> 00:27:50,559
try to think about the adversarial

857
00:27:50,559 --> 00:27:51,840
methodologies right

858
00:27:51,840 --> 00:27:53,919
that will apply right whether it's to

859
00:27:53,919 --> 00:27:56,399
data as we were talking about before

860
00:27:56,399 --> 00:27:59,120
and actually rich brought an amazing

861
00:27:59,120 --> 00:28:00,240
point related to

862
00:28:00,240 --> 00:28:01,919
ethics and i think i need to touch on

863
00:28:01,919 --> 00:28:04,399
that what you're also going to see

864
00:28:04,399 --> 00:28:07,120
is that a lot of people will concentrate

865
00:28:07,120 --> 00:28:08,799
okay i know security because i know how

866
00:28:08,799 --> 00:28:10,880
to do this fuzzing techniques or i know

867
00:28:10,880 --> 00:28:13,360
how to do this type of traditional

868
00:28:13,360 --> 00:28:15,520
you know penetration testing you know

869
00:28:15,520 --> 00:28:16,559
typing

870
00:28:16,559 --> 00:28:19,600
activities but in order to solve this

871
00:28:19,600 --> 00:28:20,159
problem

872
00:28:20,159 --> 00:28:22,240
you also have to think about if i'm an

873
00:28:22,240 --> 00:28:24,799
attacker and i'm able to manipulate

874
00:28:24,799 --> 00:28:27,760
this type of behavior that then has a

875
00:28:27,760 --> 00:28:29,600
collateral damage to ethics

876
00:28:29,600 --> 00:28:32,320
like we're talking about pixelation and

877
00:28:32,320 --> 00:28:33,919
manipulating of images

878
00:28:33,919 --> 00:28:36,080
what if i actually can do that and then

879
00:28:36,080 --> 00:28:37,840
affect the background and the foreground

880
00:28:37,840 --> 00:28:39,360
of an image and then

881
00:28:39,360 --> 00:28:41,600
have face recognition not work for

882
00:28:41,600 --> 00:28:42,960
certain people right

883
00:28:42,960 --> 00:28:45,520
yes things like that just thinking

884
00:28:45,520 --> 00:28:46,640
outside of

885
00:28:46,640 --> 00:28:49,120
the traditional confinements of a pen

886
00:28:49,120 --> 00:28:49,840
test

887
00:28:49,840 --> 00:28:52,799
and then truly understanding one how the

888
00:28:52,799 --> 00:28:54,559
technology is going to be used

889
00:28:54,559 --> 00:28:57,600
uh second how can it be abused and the

890
00:28:57,600 --> 00:28:59,279
third one what are gonna be the evasion

891
00:28:59,279 --> 00:29:01,600
techniques from an attacker perspective

892
00:29:01,600 --> 00:29:03,679
that then you know is trying to cover

893
00:29:03,679 --> 00:29:05,760
the tracks how can i detect it

894
00:29:05,760 --> 00:29:07,520
what happens if this is actually

895
00:29:07,520 --> 00:29:09,360
compromised because it's not so much

896
00:29:09,360 --> 00:29:10,640
about protecting you're never gonna be

897
00:29:10,640 --> 00:29:11,760
able to protect

898
00:29:11,760 --> 00:29:14,880
everything 100 so it's also how to react

899
00:29:14,880 --> 00:29:17,200
what is going to be the way one thing

900
00:29:17,200 --> 00:29:19,360
that i'm actually publicly

901
00:29:19,360 --> 00:29:21,919
discussing with many other entities is

902
00:29:21,919 --> 00:29:23,679
the disclosure of vulnerabilities right

903
00:29:23,679 --> 00:29:25,120
how are you also going to be disclosing

904
00:29:25,120 --> 00:29:27,679
vulnerabilities in an ai system

905
00:29:27,679 --> 00:29:30,000
it has been manipulated especially

906
00:29:30,000 --> 00:29:32,240
something that can be

907
00:29:32,240 --> 00:29:34,320
potentially you know affecting many

908
00:29:34,320 --> 00:29:35,279
vendors many

909
00:29:35,279 --> 00:29:37,760
implementations many you know software

910
00:29:37,760 --> 00:29:38,799
out there so

911
00:29:38,799 --> 00:29:40,480
so that's something to also keep in mind

912
00:29:40,480 --> 00:29:42,320
is not only about the

913
00:29:42,320 --> 00:29:45,120
the cool pen testing or red teaming

914
00:29:45,120 --> 00:29:46,880
methodologies it's also

915
00:29:46,880 --> 00:29:48,960
the whole ecosystem the design the

916
00:29:48,960 --> 00:29:52,240
adversarial techniques and the response

917
00:29:52,240 --> 00:29:54,480
i i really like that point because you

918
00:29:54,480 --> 00:29:55,520
know

919
00:29:55,520 --> 00:29:57,279
i want to get to the vulnerability

920
00:29:57,279 --> 00:29:59,600
disclosure in just a moment

921
00:29:59,600 --> 00:30:02,320
um anita like some of the tools that

922
00:30:02,320 --> 00:30:04,240
rich mentioned like to construct and

923
00:30:04,240 --> 00:30:05,279
clever hands

924
00:30:05,279 --> 00:30:08,320
actually has its roots in academia like

925
00:30:08,320 --> 00:30:09,360
um

926
00:30:09,360 --> 00:30:11,039
and there seems to be considerable work

927
00:30:11,039 --> 00:30:13,200
happening in academia in this space

928
00:30:13,200 --> 00:30:14,880
especially when it comes to tooling

929
00:30:14,880 --> 00:30:17,919
uh why is that

930
00:30:18,320 --> 00:30:20,960
i think that uh it's easy to come up

931
00:30:20,960 --> 00:30:22,080
with these cute tools

932
00:30:22,080 --> 00:30:23,600
you mean you get a grant do you have a

933
00:30:23,600 --> 00:30:25,200
study and

934
00:30:25,200 --> 00:30:26,640
you come up with these cute tools and

935
00:30:26,640 --> 00:30:27,760
then somebody comes up with a better

936
00:30:27,760 --> 00:30:28,720
tool

937
00:30:28,720 --> 00:30:30,240
and it's like this cat and mouse game it

938
00:30:30,240 --> 00:30:32,080
escalates and then the next conference

939
00:30:32,080 --> 00:30:33,679
they're saying i got a better tool

940
00:30:33,679 --> 00:30:35,919
um i don't think that's the best way to

941
00:30:35,919 --> 00:30:37,919
do it i truly and this is like

942
00:30:37,919 --> 00:30:39,520
just like with regular security i think

943
00:30:39,520 --> 00:30:41,360
truly like it's not just a buzzword we

944
00:30:41,360 --> 00:30:42,880
need to think about multi-disciplinary

945
00:30:42,880 --> 00:30:43,360
teams

946
00:30:43,360 --> 00:30:45,760
i mean you know i'm not a physicist but

947
00:30:45,760 --> 00:30:46,960
i work with physicists

948
00:30:46,960 --> 00:30:49,200
on the whole end-to-end pipeline when

949
00:30:49,200 --> 00:30:50,880
they're trying to do ai for physics now

950
00:30:50,880 --> 00:30:52,320
with the large hadron collider i don't

951
00:30:52,320 --> 00:30:53,520
know anything about physics but i do

952
00:30:53,520 --> 00:30:54,080
know

953
00:30:54,080 --> 00:30:55,360
what could happen if someone walks in

954
00:30:55,360 --> 00:30:57,200
their unprotected data center with a usb

955
00:30:57,200 --> 00:30:58,080
drive

956
00:30:58,080 --> 00:31:00,000
so i think like you know having more

957
00:31:00,000 --> 00:31:01,200
humanities technical

958
00:31:01,200 --> 00:31:03,039
and people who are not uh just machine

959
00:31:03,039 --> 00:31:04,480
learning math experts

960
00:31:04,480 --> 00:31:06,320
um is really important but ac but to get

961
00:31:06,320 --> 00:31:08,320
back to your question you you get tenure

962
00:31:08,320 --> 00:31:09,919
based on your cute paper and your cute

963
00:31:09,919 --> 00:31:10,559
tool

964
00:31:10,559 --> 00:31:12,799
um not based on not based on not harming

965
00:31:12,799 --> 00:31:14,000
people

966
00:31:14,000 --> 00:31:17,600
yeah well um bruce should people like

967
00:31:17,600 --> 00:31:20,559
think about releasing these cute tools

968
00:31:20,559 --> 00:31:22,159
like you know when people aren't even

969
00:31:22,159 --> 00:31:23,519
thinking about securing machine learning

970
00:31:23,519 --> 00:31:24,399
systems

971
00:31:24,399 --> 00:31:26,320
or should they just hold back a little

972
00:31:26,320 --> 00:31:27,840
bit um you know

973
00:31:27,840 --> 00:31:30,399
and also systems are even perfected like

974
00:31:30,399 --> 00:31:30,960
you know

975
00:31:30,960 --> 00:31:32,399
do you think this future can actually

976
00:31:32,399 --> 00:31:34,960
cause more harm you know so

977
00:31:34,960 --> 00:31:37,360
like we've been doing this at defcon for

978
00:31:37,360 --> 00:31:39,440
a bunch of decades and and we know by

979
00:31:39,440 --> 00:31:39,840
now

980
00:31:39,840 --> 00:31:42,159
that releasing the tools doing the

981
00:31:42,159 --> 00:31:42,880
research

982
00:31:42,880 --> 00:31:46,000
making it public improves systems so the

983
00:31:46,000 --> 00:31:47,919
ml people might not like it but the car

984
00:31:47,919 --> 00:31:49,360
people didn't like it and microsoft

985
00:31:49,360 --> 00:31:50,799
didn't like it in the 90s and no one

986
00:31:50,799 --> 00:31:51,760
likes it

987
00:31:51,760 --> 00:31:54,320
but this is how we improve security if

988
00:31:54,320 --> 00:31:55,440
we if we don't

989
00:31:55,440 --> 00:31:57,679
do the work we don't release the tools

990
00:31:57,679 --> 00:31:59,840
then the lousy stuff just stays in

991
00:31:59,840 --> 00:32:00,960
production

992
00:32:00,960 --> 00:32:03,519
and this this is the lesson that we can

993
00:32:03,519 --> 00:32:04,799
teach everybody else

994
00:32:04,799 --> 00:32:07,760
instead of we in the def con community

995
00:32:07,760 --> 00:32:08,640
because we know this

996
00:32:08,640 --> 00:32:12,080
we've been known it for decades um

997
00:32:12,080 --> 00:32:14,000
kind of like people that tried a little

998
00:32:14,000 --> 00:32:15,279
bit first like

999
00:32:15,279 --> 00:32:17,279
you could have like you know mentioned

1000
00:32:17,279 --> 00:32:19,039
even like a little you know

1001
00:32:19,039 --> 00:32:21,279
uh for a couple of minutes ago like how

1002
00:32:21,279 --> 00:32:22,080
these systems

1003
00:32:22,080 --> 00:32:24,399
are kind of left unguarded so for

1004
00:32:24,399 --> 00:32:26,159
somebody in the machine learning space

1005
00:32:26,159 --> 00:32:27,760
who's just like you know wrapping their

1006
00:32:27,760 --> 00:32:29,760
head around security can you give an

1007
00:32:29,760 --> 00:32:30,720
example of how

1008
00:32:30,720 --> 00:32:34,000
releasing a security tool actually led

1009
00:32:34,000 --> 00:32:34,640
to

1010
00:32:34,640 --> 00:32:37,519
improving the net security posture like

1011
00:32:37,519 --> 00:32:39,120
mlml researchers might think

1012
00:32:39,120 --> 00:32:40,960
oh the release is full i'm empowering

1013
00:32:40,960 --> 00:32:43,440
attackers why is that a bad

1014
00:32:43,440 --> 00:32:46,880
why is that a bad thing so so

1015
00:32:46,880 --> 00:32:48,720
it's a common belief that that you're

1016
00:32:48,720 --> 00:32:50,159
giving attackers ideas you're giving

1017
00:32:50,159 --> 00:32:51,760
attackers tools attackers don't need

1018
00:32:51,760 --> 00:32:52,399
ideas

1019
00:32:52,399 --> 00:32:54,880
attackers have tools right who doesn't

1020
00:32:54,880 --> 00:32:56,640
have the tools are the defenders

1021
00:32:56,640 --> 00:32:58,640
are the non-security people doing the

1022
00:32:58,640 --> 00:33:01,120
designing those are the most ignorant

1023
00:33:01,120 --> 00:33:04,240
you know in this entire uh system

1024
00:33:04,240 --> 00:33:07,679
so that is and also it spurs companies

1025
00:33:07,679 --> 00:33:09,200
to action

1026
00:33:09,200 --> 00:33:12,399
microsoft took uh security seriously

1027
00:33:12,399 --> 00:33:14,320
because the community kept pushing them

1028
00:33:14,320 --> 00:33:16,880
automobiles are like what 10 15 years

1029
00:33:16,880 --> 00:33:18,399
later doing the same thing

1030
00:33:18,399 --> 00:33:21,600
and here's ml systems again behind so

1031
00:33:21,600 --> 00:33:25,519
we in security see this cycle again and

1032
00:33:25,519 --> 00:33:27,039
again and again

1033
00:33:27,039 --> 00:33:30,640
and it is only by doing the research

1034
00:33:30,640 --> 00:33:34,159
in public with disclosure

1035
00:33:34,159 --> 00:33:37,200
that you actually get improvement and

1036
00:33:37,200 --> 00:33:37,519
and

1037
00:33:37,519 --> 00:33:39,600
you'll see that here in ml systems this

1038
00:33:39,600 --> 00:33:40,640
is going to be the way it's going to

1039
00:33:40,640 --> 00:33:41,760
work because because

1040
00:33:41,760 --> 00:33:45,360
and and we know it it's almost like

1041
00:33:45,360 --> 00:33:47,919
bruce especially for you know for you

1042
00:33:47,919 --> 00:33:49,360
and the rest of the veterans in this

1043
00:33:49,360 --> 00:33:49,760
like

1044
00:33:49,760 --> 00:33:52,960
chat this is not new you perhaps you all

1045
00:33:52,960 --> 00:33:54,480
have seen this like multiple times you

1046
00:33:54,480 --> 00:33:55,039
see that

1047
00:33:55,039 --> 00:33:56,799
omar shaking your side of bruce shaking

1048
00:33:56,799 --> 00:33:59,840
his head and rich smiling

1049
00:34:00,000 --> 00:34:01,760
listen what chris said right she might

1050
00:34:01,760 --> 00:34:03,600
not know the domain

1051
00:34:03,600 --> 00:34:06,159
but she knows security turns out that

1052
00:34:06,159 --> 00:34:08,560
security knowledge is important and it

1053
00:34:08,560 --> 00:34:11,359
transfers and it doesn't matter if the

1054
00:34:11,359 --> 00:34:12,960
computer is attached

1055
00:34:12,960 --> 00:34:16,399
to a car or a refrigerator or a phone

1056
00:34:16,399 --> 00:34:19,679
it's a computer and we know how to

1057
00:34:19,679 --> 00:34:20,000
attack

1058
00:34:20,000 --> 00:34:23,280
and secure computers it's uh software

1059
00:34:23,280 --> 00:34:24,480
all the way down

1060
00:34:24,480 --> 00:34:28,000
uh chris i want to kind of like uh

1061
00:34:28,000 --> 00:34:29,918
you know bring bring this question to

1062
00:34:29,918 --> 00:34:31,359
you uh

1063
00:34:31,359 --> 00:34:32,960
what kind of tools like we've been

1064
00:34:32,960 --> 00:34:34,239
speaking a little bit about tooling

1065
00:34:34,239 --> 00:34:35,679
because we want to talk about brass

1066
00:34:35,679 --> 00:34:36,879
tacks of preparing for

1067
00:34:36,879 --> 00:34:39,918
red teaming what kind of tools

1068
00:34:39,918 --> 00:34:42,719
do you use as part of your ai red

1069
00:34:42,719 --> 00:34:43,359
teaming

1070
00:34:43,359 --> 00:34:46,399
effort at nvidia like um

1071
00:34:46,399 --> 00:34:47,679
can you just talk a little bit about

1072
00:34:47,679 --> 00:34:49,599
that sure

1073
00:34:49,599 --> 00:34:51,679
um that's a great question and there was

1074
00:34:51,679 --> 00:34:53,280
a bunch of really awesome topics that

1075
00:34:53,280 --> 00:34:54,800
were presented like literally right

1076
00:34:54,800 --> 00:34:56,480
before you asked me this question so it

1077
00:34:56,480 --> 00:34:59,440
it's an awesome segue um so there's been

1078
00:34:59,440 --> 00:35:00,400
talks about like

1079
00:35:00,400 --> 00:35:02,640
what does it mean to be a red team for

1080
00:35:02,640 --> 00:35:03,440
ai omr

1081
00:35:03,440 --> 00:35:04,880
brought up and bruce was talking about

1082
00:35:04,880 --> 00:35:06,960
how releasing tools and things like that

1083
00:35:06,960 --> 00:35:08,400
actually emboldens the security

1084
00:35:08,400 --> 00:35:10,240
community and makes things better

1085
00:35:10,240 --> 00:35:13,280
so there's there's three main tools um

1086
00:35:13,280 --> 00:35:15,119
that we have started to use one is

1087
00:35:15,119 --> 00:35:17,520
counterfeit that microsoft has released

1088
00:35:17,520 --> 00:35:19,920
uh that is and i wholeheartedly agree

1089
00:35:19,920 --> 00:35:20,720
that that would

1090
00:35:20,720 --> 00:35:22,240
that basically is metasploit for

1091
00:35:22,240 --> 00:35:23,760
attacking ai systems

1092
00:35:23,760 --> 00:35:25,920
and that would be attacking an ai system

1093
00:35:25,920 --> 00:35:27,760
after the model has been deployed

1094
00:35:27,760 --> 00:35:30,160
so that's you know one of the phases of

1095
00:35:30,160 --> 00:35:31,200
attacking it

1096
00:35:31,200 --> 00:35:33,119
there's a second tool that i think what

1097
00:35:33,119 --> 00:35:34,480
is great that we

1098
00:35:34,480 --> 00:35:37,200
as nvidia released uh to bring the

1099
00:35:37,200 --> 00:35:39,119
security community into

1100
00:35:39,119 --> 00:35:40,960
the fold you know if people don't need

1101
00:35:40,960 --> 00:35:42,160
math like you need to talk

1102
00:35:42,160 --> 00:35:45,200
talked about like uh

1103
00:35:45,200 --> 00:35:47,280
you don't need math to attack certain

1104
00:35:47,280 --> 00:35:48,400
aspects

1105
00:35:48,400 --> 00:35:50,960
um and that tool is called mint nv and

1106
00:35:50,960 --> 00:35:52,800
that is a docker container that you pull

1107
00:35:52,800 --> 00:35:53,680
down

1108
00:35:53,680 --> 00:35:56,960
um it's a boot to root you attack it

1109
00:35:56,960 --> 00:35:58,640
the initial access is you have to

1110
00:35:58,640 --> 00:36:01,040
circumvent a deployed ai model

1111
00:36:01,040 --> 00:36:03,440
you can use counterfeit with it uh but

1112
00:36:03,440 --> 00:36:05,040
that way you can

1113
00:36:05,040 --> 00:36:06,720
security people can go they can pull

1114
00:36:06,720 --> 00:36:08,800
this down they can use the two tools

1115
00:36:08,800 --> 00:36:10,640
and they can see what it looks like to

1116
00:36:10,640 --> 00:36:12,240
actually attack like one of the phases

1117
00:36:12,240 --> 00:36:13,920
of an ai system

1118
00:36:13,920 --> 00:36:16,560
um and the other the third tool is not

1119
00:36:16,560 --> 00:36:18,480
really a tool unfortunately it's a

1120
00:36:18,480 --> 00:36:20,920
it's in this publication it's nist

1121
00:36:20,920 --> 00:36:22,079
ir8269

1122
00:36:22,079 --> 00:36:25,520
episode real machine learning um

1123
00:36:25,520 --> 00:36:27,760
get a big old pot of coffee and just

1124
00:36:27,760 --> 00:36:29,040
read it and

1125
00:36:29,040 --> 00:36:30,640
you'll only really have to read it once

1126
00:36:30,640 --> 00:36:32,160
to to kind of digest it

1127
00:36:32,160 --> 00:36:34,320
but as you're going through the taxon

1128
00:36:34,320 --> 00:36:36,240
taxonomy and terminology of like

1129
00:36:36,240 --> 00:36:37,920
adversarial machine learning

1130
00:36:37,920 --> 00:36:39,200
you'll start to get an idea of like

1131
00:36:39,200 --> 00:36:40,480
where you can attack these things and

1132
00:36:40,480 --> 00:36:42,079
what tools would be applicable

1133
00:36:42,079 --> 00:36:45,280
for each of these different phases um

1134
00:36:45,280 --> 00:36:46,640
and the last thing that i would like to

1135
00:36:46,640 --> 00:36:48,160
mention about tooling is to think about

1136
00:36:48,160 --> 00:36:49,040
a concept

1137
00:36:49,040 --> 00:36:51,839
of what tools you would need to red team

1138
00:36:51,839 --> 00:36:53,280
an ai system like omar

1139
00:36:53,280 --> 00:36:55,599
saying what does that mean i've started

1140
00:36:55,599 --> 00:36:56,800
to come to the turn

1141
00:36:56,800 --> 00:36:59,280
that when you attack an ai this is the

1142
00:36:59,280 --> 00:37:00,320
first case where you're actually

1143
00:37:00,320 --> 00:37:02,320
socially engineering a technology

1144
00:37:02,320 --> 00:37:04,960
so build your tools around how do i

1145
00:37:04,960 --> 00:37:05,680
trick

1146
00:37:05,680 --> 00:37:07,359
how do i socially engineer this

1147
00:37:07,359 --> 00:37:09,359
technology

1148
00:37:09,359 --> 00:37:11,760
hey um i know you mentioned um

1149
00:37:11,760 --> 00:37:12,480
especially

1150
00:37:12,480 --> 00:37:15,520
rich and chris some tools so

1151
00:37:15,520 --> 00:37:17,760
can you tweet it out from your handle at

1152
00:37:17,760 --> 00:37:18,640
our

1153
00:37:18,640 --> 00:37:21,440
harangue and at ice bear friend after

1154
00:37:21,440 --> 00:37:23,280
the end of this live panel so people can

1155
00:37:23,280 --> 00:37:24,320
have pointers

1156
00:37:24,320 --> 00:37:25,440
for these tools that will be very

1157
00:37:25,440 --> 00:37:27,680
helpful of course

1158
00:37:27,680 --> 00:37:30,560
yeah thank you um you know i kind of

1159
00:37:30,560 --> 00:37:31,280
like

1160
00:37:31,280 --> 00:37:33,119
uh now i want to switch gears a little

1161
00:37:33,119 --> 00:37:34,560
bit and talk about

1162
00:37:34,560 --> 00:37:38,240
the future of like this field

1163
00:37:38,240 --> 00:37:41,200
and bruce i'm going to come back to you

1164
00:37:41,200 --> 00:37:41,680
uh

1165
00:37:41,680 --> 00:37:42,960
and i want you to kind of like paint

1166
00:37:42,960 --> 00:37:44,960
this picture for us

1167
00:37:44,960 --> 00:37:48,160
based on your experience in this field

1168
00:37:48,160 --> 00:37:49,839
when do you think we'll have the next

1169
00:37:49,839 --> 00:37:52,320
stuxnet for ai systems

1170
00:37:52,320 --> 00:37:53,920
like when is what are we going to have

1171
00:37:53,920 --> 00:37:56,480
like ept uses like hacks machine

1172
00:37:56,480 --> 00:37:57,599
learning system

1173
00:37:57,599 --> 00:37:59,520
and our estate vendors are going to like

1174
00:37:59,520 --> 00:38:01,040
sell us on the solution

1175
00:38:01,040 --> 00:38:03,359
when is that going to happen so

1176
00:38:03,359 --> 00:38:05,119
interesting so you you pick stuxnet

1177
00:38:05,119 --> 00:38:06,240
stuxtet is it was a

1178
00:38:06,240 --> 00:38:10,000
very very targeted hack that was not a

1179
00:38:10,000 --> 00:38:12,880
a general you know general hack that

1180
00:38:12,880 --> 00:38:13,920
that affected

1181
00:38:13,920 --> 00:38:15,440
hundreds or thousands of systems that

1182
00:38:15,440 --> 00:38:17,200
was against one particular

1183
00:38:17,200 --> 00:38:20,320
iranian nuclear plant and we'll have

1184
00:38:20,320 --> 00:38:22,960
that when a government decides that that

1185
00:38:22,960 --> 00:38:23,280
is an

1186
00:38:23,280 --> 00:38:26,000
efficacious way of advancing uh their

1187
00:38:26,000 --> 00:38:27,359
foreign policy

1188
00:38:27,359 --> 00:38:29,119
and it could be tomorrow and it could be

1189
00:38:29,119 --> 00:38:31,119
years from now

1190
00:38:31,119 --> 00:38:33,040
and we're going to see criminal hacks

1191
00:38:33,040 --> 00:38:34,480
when they become

1192
00:38:34,480 --> 00:38:36,880
profitable i mean this follows the

1193
00:38:36,880 --> 00:38:38,160
trajectory

1194
00:38:38,160 --> 00:38:41,280
of these systems being deployed as they

1195
00:38:41,280 --> 00:38:42,800
are deployed

1196
00:38:42,800 --> 00:38:45,280
in more places you will see them used so

1197
00:38:45,280 --> 00:38:47,119
again back to the apple and image

1198
00:38:47,119 --> 00:38:49,280
classifying and looking for

1199
00:38:49,280 --> 00:38:52,640
child abuse material on your phone

1200
00:38:52,640 --> 00:38:54,960
so you know we could expect to see hacks

1201
00:38:54,960 --> 00:38:57,920
that will attempt to frame somebody

1202
00:38:57,920 --> 00:38:59,760
we will see attempt we're going to see

1203
00:38:59,760 --> 00:39:01,040
hacks that will

1204
00:39:01,040 --> 00:39:04,400
bypass the system and and we'll we'll

1205
00:39:04,400 --> 00:39:06,000
see hacks that just you know cause

1206
00:39:06,000 --> 00:39:08,240
general mayhem

1207
00:39:08,240 --> 00:39:09,440
i don't know i mean i think we're going

1208
00:39:09,440 --> 00:39:11,599
to see the first papers on this in

1209
00:39:11,599 --> 00:39:15,200
in a year or two and this would go

1210
00:39:15,200 --> 00:39:17,440
down to the criminals and governments

1211
00:39:17,440 --> 00:39:19,359
you know when it uh when it does and all

1212
00:39:19,359 --> 00:39:22,320
this stuff flows downhill

1213
00:39:22,320 --> 00:39:24,800
thomas nsa program is tomorrow's phd

1214
00:39:24,800 --> 00:39:27,920
thesis and the next day's hacker tool

1215
00:39:27,920 --> 00:39:29,520
it's hard to know when

1216
00:39:29,520 --> 00:39:32,480
but we know it's coming and it's driven

1217
00:39:32,480 --> 00:39:33,040
by

1218
00:39:33,040 --> 00:39:34,510
how they're used

1219
00:39:34,510 --> 00:39:36,079
[Music]

1220
00:39:36,079 --> 00:39:37,599
and and what do you mean by how you're

1221
00:39:37,599 --> 00:39:40,160
used like we already see like machine

1222
00:39:40,160 --> 00:39:42,320
learning systems powering like

1223
00:39:42,320 --> 00:39:45,760
healthcare finance and all these like

1224
00:39:45,760 --> 00:39:49,520
important like fields and we all agree

1225
00:39:49,520 --> 00:39:50,880
they're unguarded

1226
00:39:50,880 --> 00:39:52,640
why aren't you seeing attacks on machine

1227
00:39:52,640 --> 00:39:54,320
learning systems more than

1228
00:39:54,320 --> 00:39:55,920
i'm not convinced we're not i mean the

1229
00:39:55,920 --> 00:39:57,920
question is whether they make the news

1230
00:39:57,920 --> 00:40:00,079
right so so you know right a proxy for

1231
00:40:00,079 --> 00:40:01,760
whether things occur or whether we know

1232
00:40:01,760 --> 00:40:02,560
about them and

1233
00:40:02,560 --> 00:40:05,200
that's an imperfect proxy i think

1234
00:40:05,200 --> 00:40:06,319
already

1235
00:40:06,319 --> 00:40:10,079
uh doctors know how to code

1236
00:40:10,079 --> 00:40:12,640
patient information in order to get the

1237
00:40:12,640 --> 00:40:14,400
ais at the insurance companies to

1238
00:40:14,400 --> 00:40:16,480
produce the outcomes they want

1239
00:40:16,480 --> 00:40:18,560
to approve the procedure i mean so i

1240
00:40:18,560 --> 00:40:19,680
think there is

1241
00:40:19,680 --> 00:40:23,200
that kind of adversarial ml

1242
00:40:23,200 --> 00:40:26,160
going on right now uh i think image

1243
00:40:26,160 --> 00:40:28,160
classifiers i mean the stuff that kills

1244
00:40:28,160 --> 00:40:28,880
people

1245
00:40:28,880 --> 00:40:30,240
you tend not to see that because a lot

1246
00:40:30,240 --> 00:40:32,560
of people don't want to kill people

1247
00:40:32,560 --> 00:40:34,640
that just you know is annoying right the

1248
00:40:34,640 --> 00:40:35,680
uh the way

1249
00:40:35,680 --> 00:40:39,599
uh microsoft's pay was turned into a

1250
00:40:39,599 --> 00:40:42,319
misogynistic nazi in 48 hours by 4chan

1251
00:40:42,319 --> 00:40:43,040
right that

1252
00:40:43,040 --> 00:40:46,800
was an adversarial ml attack

1253
00:40:46,800 --> 00:40:48,880
so we do see them i don't think they're

1254
00:40:48,880 --> 00:40:52,160
making the news because they're still

1255
00:40:52,160 --> 00:40:54,079
under the radar and the systems aren't

1256
00:40:54,079 --> 00:40:57,359
as widely deployed and understood

1257
00:40:57,359 --> 00:40:59,200
i feel that's a that's an interesting

1258
00:40:59,200 --> 00:41:01,599
point i think like chris

1259
00:41:01,599 --> 00:41:03,440
i'm bringing it back again because i

1260
00:41:03,440 --> 00:41:04,880
think like for

1261
00:41:04,880 --> 00:41:07,359
clearly like all these like flagship

1262
00:41:07,359 --> 00:41:08,240
companies

1263
00:41:08,240 --> 00:41:10,480
that you know that you know microsoft

1264
00:41:10,480 --> 00:41:11,920
nvidia

1265
00:41:11,920 --> 00:41:14,079
ibm twitter google they're all putting

1266
00:41:14,079 --> 00:41:15,359
like machine learning systems

1267
00:41:15,359 --> 00:41:19,520
front and center of like you know

1268
00:41:19,520 --> 00:41:22,880
their their competitive like advantage

1269
00:41:22,880 --> 00:41:25,200
so all of these teams tend to have red

1270
00:41:25,200 --> 00:41:26,079
teams right

1271
00:41:26,079 --> 00:41:28,319
like nvidia i'm sure had like a vanilla

1272
00:41:28,319 --> 00:41:29,920
red team before this

1273
00:41:29,920 --> 00:41:33,200
so based on what you're seeing like how

1274
00:41:33,200 --> 00:41:33,760
can they

1275
00:41:33,760 --> 00:41:35,760
invest in the space like what are some

1276
00:41:35,760 --> 00:41:37,119
of the organizational

1277
00:41:37,119 --> 00:41:38,960
like challenges and opportunities at

1278
00:41:38,960 --> 00:41:40,960
hand

1279
00:41:40,960 --> 00:41:43,440
uh no it's a great point um and i've

1280
00:41:43,440 --> 00:41:44,880
heard a lot of really good

1281
00:41:44,880 --> 00:41:46,319
feedback from the panelists so far and i

1282
00:41:46,319 --> 00:41:48,400
think anita nailed it she said just

1283
00:41:48,400 --> 00:41:49,760
getting in the room

1284
00:41:49,760 --> 00:41:52,960
um that is going to be like the best

1285
00:41:52,960 --> 00:41:54,560
entry point for any red team if you have

1286
00:41:54,560 --> 00:41:56,240
a red team you have

1287
00:41:56,240 --> 00:41:57,760
if you're going to be using ai systems

1288
00:41:57,760 --> 00:41:59,520
or models or anything like that just get

1289
00:41:59,520 --> 00:42:00,240
them in the room

1290
00:42:00,240 --> 00:42:03,040
get them at the table it's not as

1291
00:42:03,040 --> 00:42:04,319
technical up front

1292
00:42:04,319 --> 00:42:06,800
but they know how to attack things they

1293
00:42:06,800 --> 00:42:07,520
know

1294
00:42:07,520 --> 00:42:09,280
types of attacks they have adversarial

1295
00:42:09,280 --> 00:42:10,800
mindsets

1296
00:42:10,800 --> 00:42:12,960
just having them in the room and being

1297
00:42:12,960 --> 00:42:14,880
able to ask questions to scientists or

1298
00:42:14,880 --> 00:42:18,079
policy makers will change the trajectory

1299
00:42:18,079 --> 00:42:20,079
of whatever ai or ml system is being

1300
00:42:20,079 --> 00:42:21,440
created and if it's being

1301
00:42:21,440 --> 00:42:23,839
if the model is being created you've now

1302
00:42:23,839 --> 00:42:24,480
you've now

1303
00:42:24,480 --> 00:42:26,160
shifted how that model was going to be

1304
00:42:26,160 --> 00:42:27,920
secured people are going to start asking

1305
00:42:27,920 --> 00:42:28,960
questions

1306
00:42:28,960 --> 00:42:32,640
um so to for entry points for sit for

1307
00:42:32,640 --> 00:42:33,839
places that are using

1308
00:42:33,839 --> 00:42:36,240
ai technology or anything like that or

1309
00:42:36,240 --> 00:42:38,240
making their own models

1310
00:42:38,240 --> 00:42:41,200
i would say do table tops first and then

1311
00:42:41,200 --> 00:42:42,079
the

1312
00:42:42,079 --> 00:42:44,400
the next phase after that after you can

1313
00:42:44,400 --> 00:42:46,160
start affecting policy would be

1314
00:42:46,160 --> 00:42:47,520
try to start doing some of those more

1315
00:42:47,520 --> 00:42:49,760
later stage ai attacks because that's

1316
00:42:49,760 --> 00:42:50,079
more

1317
00:42:50,079 --> 00:42:51,359
aligned with what i would consider

1318
00:42:51,359 --> 00:42:53,040
traditional pin testing you know you're

1319
00:42:53,040 --> 00:42:54,560
going to be bleeding apis you'll be

1320
00:42:54,560 --> 00:42:56,480
pulling data stuff like that

1321
00:42:56,480 --> 00:42:59,119
and as more familiarity gets

1322
00:42:59,119 --> 00:43:00,560
accomplished with the red team you can

1323
00:43:00,560 --> 00:43:01,760
start moving into

1324
00:43:01,760 --> 00:43:03,599
attacking the model poisoning and as

1325
00:43:03,599 --> 00:43:05,040
it's being created

1326
00:43:05,040 --> 00:43:07,839
you know replicating the model um how to

1327
00:43:07,839 --> 00:43:08,640
attack those things

1328
00:43:08,640 --> 00:43:10,560
offline attacks and you can just get

1329
00:43:10,560 --> 00:43:13,920
more uh deeper with it

1330
00:43:13,920 --> 00:43:15,599
oh sorry chris i didn't mean to like

1331
00:43:15,599 --> 00:43:18,000
interrupt you but when you meant policy

1332
00:43:18,000 --> 00:43:20,640
um what were you meaning governmental

1333
00:43:20,640 --> 00:43:22,079
policy or like

1334
00:43:22,079 --> 00:43:24,240
like nvidia's like policy like can you

1335
00:43:24,240 --> 00:43:26,400
just like clarify that

1336
00:43:26,400 --> 00:43:28,640
sure yeah i i meant policy like that

1337
00:43:28,640 --> 00:43:29,920
that stakeholders

1338
00:43:29,920 --> 00:43:31,359
would implement like if we're going to

1339
00:43:31,359 --> 00:43:33,119
use an ml system

1340
00:43:33,119 --> 00:43:34,560
we're going to have to do this you know

1341
00:43:34,560 --> 00:43:36,800
i think rich talked about the bias

1342
00:43:36,800 --> 00:43:38,319
that stuff is like i mean that's

1343
00:43:38,319 --> 00:43:39,920
incredible once you start reading about

1344
00:43:39,920 --> 00:43:41,200
those things and

1345
00:43:41,200 --> 00:43:43,119
there has to be policy around you can't

1346
00:43:43,119 --> 00:43:44,240
just take a

1347
00:43:44,240 --> 00:43:46,640
ai system and just use it like there's

1348
00:43:46,640 --> 00:43:48,480
there's going to be inherent things

1349
00:43:48,480 --> 00:43:51,599
you need to account for

1350
00:43:51,599 --> 00:43:54,240
and being in the room and saying hey i

1351
00:43:54,240 --> 00:43:55,200
think

1352
00:43:55,200 --> 00:43:56,960
we need to account for this type of

1353
00:43:56,960 --> 00:43:58,640
attack

1354
00:43:58,640 --> 00:44:01,760
just being there does wonders like it

1355
00:44:01,760 --> 00:44:03,520
you went from zero percent to 100

1356
00:44:03,520 --> 00:44:08,240
so like when i when i met policy i meant

1357
00:44:08,240 --> 00:44:10,160
like stakeholders internally how are you

1358
00:44:10,160 --> 00:44:11,760
going to use the models

1359
00:44:11,760 --> 00:44:14,720
how they're going to be implemented and

1360
00:44:14,720 --> 00:44:16,319
things like that

1361
00:44:16,319 --> 00:44:18,000
that is awesome chris like i want to

1362
00:44:18,000 --> 00:44:19,599
just pull that through just a little bit

1363
00:44:19,599 --> 00:44:20,400
more

1364
00:44:20,400 --> 00:44:23,440
um or somebody who's just like listening

1365
00:44:23,440 --> 00:44:24,480
to this like panel

1366
00:44:24,480 --> 00:44:26,000
and they're super pumped they want to go

1367
00:44:26,000 --> 00:44:28,800
think about like you know think about

1368
00:44:28,800 --> 00:44:31,040
like say even talking to stakeholders

1369
00:44:31,040 --> 00:44:32,079
what are like

1370
00:44:32,079 --> 00:44:34,000
two or three questions that they can ask

1371
00:44:34,000 --> 00:44:36,400
an ml developer as a security person

1372
00:44:36,400 --> 00:44:39,680
just get the ball rolling anything on

1373
00:44:39,680 --> 00:44:41,359
top of your line

1374
00:44:41,359 --> 00:44:42,960
yeah where'd you get the model from like

1375
00:44:42,960 --> 00:44:44,800
work like is that our date or is it

1376
00:44:44,800 --> 00:44:45,920
somebody else's

1377
00:44:45,920 --> 00:44:47,440
so it'd be like where'd you get the

1378
00:44:47,440 --> 00:44:50,480
model do we trust them is the data that

1379
00:44:50,480 --> 00:44:52,000
goes that like the model was used

1380
00:44:52,000 --> 00:44:54,319
against is it ours or somebody else's

1381
00:44:54,319 --> 00:44:56,160
uh you start asking those questions and

1382
00:44:56,160 --> 00:44:57,599
then you can start figuring out like

1383
00:44:57,599 --> 00:44:58,800
where it come from

1384
00:44:58,800 --> 00:45:00,240
uh i need to talk about pipelines

1385
00:45:00,240 --> 00:45:02,560
attacks well like sometimes the pipeline

1386
00:45:02,560 --> 00:45:03,200
is

1387
00:45:03,200 --> 00:45:05,119
like the model doesn't come from you it

1388
00:45:05,119 --> 00:45:06,720
comes from somebody else

1389
00:45:06,720 --> 00:45:09,040
so if that would if the model was

1390
00:45:09,040 --> 00:45:11,119
compromised way upstream

1391
00:45:11,119 --> 00:45:12,960
by the time it gets to you you may not

1392
00:45:12,960 --> 00:45:14,640
even know it's compromised there may be

1393
00:45:14,640 --> 00:45:16,160
a backdoor sitting in it i don't know

1394
00:45:16,160 --> 00:45:17,119
this but like

1395
00:45:17,119 --> 00:45:19,040
there there may be a backdoor sitting in

1396
00:45:19,040 --> 00:45:20,560
it that has made it through like five

1397
00:45:20,560 --> 00:45:21,680
different channels that nobody knows

1398
00:45:21,680 --> 00:45:22,640
about

1399
00:45:22,640 --> 00:45:25,359
um so those would be my two biggest

1400
00:45:25,359 --> 00:45:26,640
questions it's like where the model come

1401
00:45:26,640 --> 00:45:27,680
from

1402
00:45:27,680 --> 00:45:30,240
where did the data come from i i want to

1403
00:45:30,240 --> 00:45:32,079
jump in on that just to emphasize data

1404
00:45:32,079 --> 00:45:34,640
data and data and where are you hosting

1405
00:45:34,640 --> 00:45:35,440
you know

1406
00:45:35,440 --> 00:45:37,839
something like a simple you know s3

1407
00:45:37,839 --> 00:45:38,720
bucket that's

1408
00:45:38,720 --> 00:45:40,400
probably a likely more likely vector

1409
00:45:40,400 --> 00:45:41,760
where is the data kept

1410
00:45:41,760 --> 00:45:43,040
how long are you keeping all these

1411
00:45:43,040 --> 00:45:45,599
boring security compliance things

1412
00:45:45,599 --> 00:45:46,720
which we go through and people don't

1413
00:45:46,720 --> 00:45:48,240
like to go through it where's it backed

1414
00:45:48,240 --> 00:45:48,640
up

1415
00:45:48,640 --> 00:45:50,400
who has it when are you gonna

1416
00:45:50,400 --> 00:45:52,240
de-accession who has access to it

1417
00:45:52,240 --> 00:45:54,480
those fundamentals that we know in

1418
00:45:54,480 --> 00:45:55,280
security

1419
00:45:55,280 --> 00:45:58,240
first thing i'd ask that's awesome like

1420
00:45:58,240 --> 00:45:58,720
i see

1421
00:45:58,720 --> 00:46:01,920
as i listen to what you're saying anita

1422
00:46:01,920 --> 00:46:03,599
and you know as i'm processing like

1423
00:46:03,599 --> 00:46:05,440
chris's information

1424
00:46:05,440 --> 00:46:08,640
like i'm and this is omar i want i

1425
00:46:08,640 --> 00:46:10,400
really want to get your thought here the

1426
00:46:10,400 --> 00:46:12,079
questions that chris and anita are

1427
00:46:12,079 --> 00:46:13,359
asking

1428
00:46:13,359 --> 00:46:16,240
for for you know in some levels you must

1429
00:46:16,240 --> 00:46:18,079
have been asking from like basic

1430
00:46:18,079 --> 00:46:19,040
software

1431
00:46:19,040 --> 00:46:20,800
for like you know like you know for a

1432
00:46:20,800 --> 00:46:22,720
long time so using the

1433
00:46:22,720 --> 00:46:24,640
questions that chris and anita kind of

1434
00:46:24,640 --> 00:46:25,920
like you know

1435
00:46:25,920 --> 00:46:28,880
just post how do you think the role of a

1436
00:46:28,880 --> 00:46:30,400
traditional rec team member

1437
00:46:30,400 --> 00:46:33,040
is going to change in five years you

1438
00:46:33,040 --> 00:46:34,880
know with ml systems

1439
00:46:34,880 --> 00:46:36,960
um can you just talk a little bit about

1440
00:46:36,960 --> 00:46:38,000
that

1441
00:46:38,000 --> 00:46:40,800
yeah if you if you go back in time at

1442
00:46:40,800 --> 00:46:41,520
least

1443
00:46:41,520 --> 00:46:44,400
i guess i'm getting old but whenever we

1444
00:46:44,400 --> 00:46:44,880
started

1445
00:46:44,880 --> 00:46:46,640
with you know router switches you know

1446
00:46:46,640 --> 00:46:48,640
embedded devices a lot of people didn't

1447
00:46:48,640 --> 00:46:49,920
know about that it was just you know

1448
00:46:49,920 --> 00:46:52,000
this taboo thing they concentrated in

1449
00:46:52,000 --> 00:46:54,640
end host machines and probably windows

1450
00:46:54,640 --> 00:46:56,480
we saw a big gap

1451
00:46:56,480 --> 00:46:59,520
in in the talent right so on how to

1452
00:46:59,520 --> 00:47:01,119
to look and manipulate those type of

1453
00:47:01,119 --> 00:47:04,079
systems in some cases because it was

1454
00:47:04,079 --> 00:47:06,720
cost prohibited by some people right

1455
00:47:06,720 --> 00:47:09,200
putting a two million dollar you know

1456
00:47:09,200 --> 00:47:10,720
core rather the internet you know that

1457
00:47:10,720 --> 00:47:13,760
was it was pretty pretty challenging

1458
00:47:13,760 --> 00:47:16,960
but um but we evolved right we looked

1459
00:47:16,960 --> 00:47:18,240
into not only

1460
00:47:18,240 --> 00:47:20,880
the red team part but also the forensics

1461
00:47:20,880 --> 00:47:22,319
there's a huge lack

1462
00:47:22,319 --> 00:47:25,119
feel to this day of really good people

1463
00:47:25,119 --> 00:47:26,079
that knows

1464
00:47:26,079 --> 00:47:29,359
how to do forensics in these type of

1465
00:47:29,359 --> 00:47:29,839
systems

1466
00:47:29,839 --> 00:47:33,280
now fast forward now with ml and ai

1467
00:47:33,280 --> 00:47:34,559
that's something that we have to think

1468
00:47:34,559 --> 00:47:37,040
through right what are the

1469
00:47:37,040 --> 00:47:39,359
the skills that are necessary not only

1470
00:47:39,359 --> 00:47:40,720
from the red team perspective i'm

1471
00:47:40,720 --> 00:47:42,160
touching that in a second

1472
00:47:42,160 --> 00:47:44,079
but also in other areas of security to

1473
00:47:44,079 --> 00:47:46,559
protect these systems right

1474
00:47:46,559 --> 00:47:48,559
again going back what if i'm already

1475
00:47:48,559 --> 00:47:50,319
compromised what are the things that i'm

1476
00:47:50,319 --> 00:47:51,119
going to be doing

1477
00:47:51,119 --> 00:47:54,400
from this ml system uh to see if

1478
00:47:54,400 --> 00:47:55,760
again you know for the attacker is

1479
00:47:55,760 --> 00:47:57,440
actually compromising this you know

1480
00:47:57,440 --> 00:48:00,720
a year from you know ago and then

1481
00:48:00,720 --> 00:48:02,079
same thing that you're gonna see you're

1482
00:48:02,079 --> 00:48:03,760
not gonna see the perfect

1483
00:48:03,760 --> 00:48:06,400
candidate or the perfect person that

1484
00:48:06,400 --> 00:48:09,200
will know all ai and old red teams right

1485
00:48:09,200 --> 00:48:10,960
um what chris mentioned about being on

1486
00:48:10,960 --> 00:48:12,559
the table that's number one

1487
00:48:12,559 --> 00:48:14,240
right he's having those conversations

1488
00:48:14,240 --> 00:48:16,400
because it's an evolving thing right

1489
00:48:16,400 --> 00:48:18,720
second

1490
00:48:19,520 --> 00:48:21,760
you don't is specializations within you

1491
00:48:21,760 --> 00:48:23,119
know this red teaming

1492
00:48:23,119 --> 00:48:24,559
pen testing offensive security i'm going

1493
00:48:24,559 --> 00:48:26,400
to generalize it of censor security just

1494
00:48:26,400 --> 00:48:27,040
like

1495
00:48:27,040 --> 00:48:30,480
i have probably more uh knowledge and

1496
00:48:30,480 --> 00:48:32,319
web applications you know and somebody

1497
00:48:32,319 --> 00:48:34,000
else in embedded devices and somebody

1498
00:48:34,000 --> 00:48:35,760
else in you know in other type of

1499
00:48:35,760 --> 00:48:37,440
technologies you will

1500
00:48:37,440 --> 00:48:39,920
traditionally you will see that right

1501
00:48:39,920 --> 00:48:41,440
and same thing goes with

1502
00:48:41,440 --> 00:48:43,520
quantum computing right we haven't even

1503
00:48:43,520 --> 00:48:45,200
got the kind of worms

1504
00:48:45,200 --> 00:48:47,440
right now another thing that you're

1505
00:48:47,440 --> 00:48:49,119
going to be seeing

1506
00:48:49,119 --> 00:48:53,040
is ai or nml is going to augment

1507
00:48:53,040 --> 00:48:55,200
the task of a red team error slash

1508
00:48:55,200 --> 00:48:56,720
offensive security person

1509
00:48:56,720 --> 00:48:59,280
right so in that case it's not so much

1510
00:48:59,280 --> 00:49:01,119
of attacking those things it's how can

1511
00:49:01,119 --> 00:49:02,000
you use those

1512
00:49:02,000 --> 00:49:04,640
systems to then do all types of attacks

1513
00:49:04,640 --> 00:49:05,359
right

1514
00:49:05,359 --> 00:49:07,119
what are the types of obfuscation other

1515
00:49:07,119 --> 00:49:08,960
types of evasion techniques

1516
00:49:08,960 --> 00:49:11,200
other types of manipulation right that

1517
00:49:11,200 --> 00:49:13,520
those notices today are probably

1518
00:49:13,520 --> 00:49:16,960
not a scale right so it's the augmenting

1519
00:49:16,960 --> 00:49:18,800
of the red teamer

1520
00:49:18,800 --> 00:49:21,040
or the offensive security type person

1521
00:49:21,040 --> 00:49:22,720
using these type of systems for some

1522
00:49:22,720 --> 00:49:25,040
other manipulation

1523
00:49:25,040 --> 00:49:26,960
i mean omar that was such a

1524
00:49:26,960 --> 00:49:28,559
comprehensive answer

1525
00:49:28,559 --> 00:49:30,480
and especially if you're interested uh

1526
00:49:30,480 --> 00:49:31,680
you know um

1527
00:49:31,680 --> 00:49:32,960
if you're listening to this panel if

1528
00:49:32,960 --> 00:49:34,720
you're interested in a lot last part

1529
00:49:34,720 --> 00:49:37,760
of how like uh red team can be like

1530
00:49:37,760 --> 00:49:39,119
automated you should really listen to

1531
00:49:39,119 --> 00:49:40,400
bruce's keynote

1532
00:49:40,400 --> 00:49:42,000
uh that's gonna be that's hosted in the

1533
00:49:42,000 --> 00:49:44,160
ai village right now it's

1534
00:49:44,160 --> 00:49:46,160
it really does open your mind to ai

1535
00:49:46,160 --> 00:49:47,599
hackers so thank you for bringing that

1536
00:49:47,599 --> 00:49:48,480
point omar

1537
00:49:48,480 --> 00:49:50,000
feels like we're tying down a lot of

1538
00:49:50,000 --> 00:49:52,000
like um tying down a lot of different

1539
00:49:52,000 --> 00:49:52,559
points

1540
00:49:52,559 --> 00:49:54,000
but rich i also want to kind of like ask

1541
00:49:54,000 --> 00:49:56,079
you this question you're you're an ml

1542
00:49:56,079 --> 00:49:57,760
you've been an ml engineer since

1543
00:49:57,760 --> 00:50:00,240
even the title didn't exist so how do

1544
00:50:00,240 --> 00:50:00,960
you see

1545
00:50:00,960 --> 00:50:03,599
like you know the role of analogy are

1546
00:50:03,599 --> 00:50:05,680
changing in the next five years

1547
00:50:05,680 --> 00:50:08,480
then think about actively securing these

1548
00:50:08,480 --> 00:50:10,800
machine learning systems

1549
00:50:10,800 --> 00:50:12,960
um i think we have sort of the good path

1550
00:50:12,960 --> 00:50:14,160
and the bad path

1551
00:50:14,160 --> 00:50:17,599
uh so tell us both the paths

1552
00:50:17,599 --> 00:50:21,040
okay so so the bad path is we

1553
00:50:21,040 --> 00:50:24,559
do nothing and what we end up with

1554
00:50:24,559 --> 00:50:26,800
is these systems which are deployed

1555
00:50:26,800 --> 00:50:28,800
everywhere we don't have a standard way

1556
00:50:28,800 --> 00:50:29,920
of understanding what the

1557
00:50:29,920 --> 00:50:31,839
vulnerabilities are in them or how to

1558
00:50:31,839 --> 00:50:32,319
address

1559
00:50:32,319 --> 00:50:34,720
them or what the best practices are we

1560
00:50:34,720 --> 00:50:36,720
haven't invested sufficiently in secure

1561
00:50:36,720 --> 00:50:38,240
tooling for them

1562
00:50:38,240 --> 00:50:41,520
and as a result it's the wild west like

1563
00:50:41,520 --> 00:50:43,040
you know the internet was i don't know

1564
00:50:43,040 --> 00:50:44,640
20 years ago and it's just it's a

1565
00:50:44,640 --> 00:50:46,079
complete disaster

1566
00:50:46,079 --> 00:50:49,760
um i think the good path is

1567
00:50:49,760 --> 00:50:52,440
we form these sort of multi-skilled

1568
00:50:52,440 --> 00:50:54,400
multi-disciplinary teams who can think

1569
00:50:54,400 --> 00:50:56,079
about these things holistically

1570
00:50:56,079 --> 00:50:58,960
right and there's you know the just back

1571
00:50:58,960 --> 00:51:01,680
to the data thing right there's a whole

1572
00:51:01,680 --> 00:51:04,480
issue there with like forensics and data

1573
00:51:04,480 --> 00:51:06,240
security and data privacy if you have

1574
00:51:06,240 --> 00:51:08,079
pii that's going for personally

1575
00:51:08,079 --> 00:51:09,599
identifiable information

1576
00:51:09,599 --> 00:51:11,200
that's going into those models that in

1577
00:51:11,200 --> 00:51:13,520
some jurisdictions is controlled

1578
00:51:13,520 --> 00:51:15,839
that's actually telemetry that you would

1579
00:51:15,839 --> 00:51:18,160
need to do forensics on the model so how

1580
00:51:18,160 --> 00:51:19,760
can you handle that securely right

1581
00:51:19,760 --> 00:51:21,040
you've got all of these

1582
00:51:21,040 --> 00:51:23,680
questions that we've we've barely even

1583
00:51:23,680 --> 00:51:24,400
started

1584
00:51:24,400 --> 00:51:27,280
to tackle that we need to think about to

1585
00:51:27,280 --> 00:51:28,960
be able to

1586
00:51:28,960 --> 00:51:30,400
essentially secure these models and

1587
00:51:30,400 --> 00:51:31,839
defend them properly so i think the good

1588
00:51:31,839 --> 00:51:33,119
path is essentially the

1589
00:51:33,119 --> 00:51:36,400
inverse of that we spent time

1590
00:51:36,400 --> 00:51:38,480
thinking about these things um you know

1591
00:51:38,480 --> 00:51:40,000
with the health perhaps of red teamers

1592
00:51:40,000 --> 00:51:41,040
we've identified

1593
00:51:41,040 --> 00:51:43,040
these are the classes of attacks that

1594
00:51:43,040 --> 00:51:45,040
are feasible these are how they usually

1595
00:51:45,040 --> 00:51:45,760
happen

1596
00:51:45,760 --> 00:51:48,800
in production systems and not just

1597
00:51:48,800 --> 00:51:52,160
in sort of academic settings

1598
00:51:52,160 --> 00:51:55,359
and this is how we can then go about

1599
00:51:55,359 --> 00:51:57,200
defending against them and we're

1600
00:51:57,200 --> 00:51:58,800
definitely seeing moves i think

1601
00:51:58,800 --> 00:52:01,920
towards the good path um we've got

1602
00:52:01,920 --> 00:52:04,880
nist is seeking input on how to do like

1603
00:52:04,880 --> 00:52:06,240
trustworthy and reliable

1604
00:52:06,240 --> 00:52:08,960
ai computing uh we have attack

1605
00:52:08,960 --> 00:52:10,480
frameworks i think um the

1606
00:52:10,480 --> 00:52:14,000
the atlas framework uh from microsoft

1607
00:52:14,000 --> 00:52:15,599
and mitre i believe

1608
00:52:15,599 --> 00:52:17,680
um so we're starting to sort of

1609
00:52:17,680 --> 00:52:20,079
systematize and categorize these

1610
00:52:20,079 --> 00:52:24,240
uh what we don't have at the moment is

1611
00:52:24,240 --> 00:52:26,880
again this transition from the academic

1612
00:52:26,880 --> 00:52:27,839
space where we have

1613
00:52:27,839 --> 00:52:29,440
all of these theoretical attacks and all

1614
00:52:29,440 --> 00:52:31,040
of these theoretical vulnerabilities

1615
00:52:31,040 --> 00:52:32,960
that are very specific to machine

1616
00:52:32,960 --> 00:52:34,480
learning models

1617
00:52:34,480 --> 00:52:37,200
um that haven't made the jump into oh

1618
00:52:37,200 --> 00:52:38,720
yeah that's actually happening all the

1619
00:52:38,720 --> 00:52:39,599
time now i mean

1620
00:52:39,599 --> 00:52:43,200
we have as far as i know we still today

1621
00:52:43,200 --> 00:52:46,319
since 2019 have exactly one cve

1622
00:52:46,319 --> 00:52:49,359
that has been filed in relationship to a

1623
00:52:49,359 --> 00:52:51,920
machine learning model and that was um

1624
00:52:51,920 --> 00:52:54,000
will and nick with the cbe against

1625
00:52:54,000 --> 00:52:55,680
fruitcoin and again that's a good

1626
00:52:55,680 --> 00:52:56,960
example because

1627
00:52:56,960 --> 00:53:00,160
that required both a weird configuration

1628
00:53:00,160 --> 00:53:02,319
in how the mail bounce was handled and

1629
00:53:02,319 --> 00:53:03,760
the leaking of the scores for that

1630
00:53:03,760 --> 00:53:05,280
attack to be effective so again it's the

1631
00:53:05,280 --> 00:53:06,480
entire system

1632
00:53:06,480 --> 00:53:08,319
that you have to consider and so those

1633
00:53:08,319 --> 00:53:10,400
kinds of things right in five years

1634
00:53:10,400 --> 00:53:12,480
maybe we'll be like oh yeah obviously we

1635
00:53:12,480 --> 00:53:13,119
should write

1636
00:53:13,119 --> 00:53:15,119
obviously we shouldn't make raw scores

1637
00:53:15,119 --> 00:53:16,240
obviously

1638
00:53:16,240 --> 00:53:19,920
we should you know um do differential

1639
00:53:19,920 --> 00:53:21,200
privacy on the inputs

1640
00:53:21,200 --> 00:53:23,359
stuff like this hopefully it'll just be

1641
00:53:23,359 --> 00:53:24,480
like

1642
00:53:24,480 --> 00:53:26,800
routine part of ml engineering the same

1643
00:53:26,800 --> 00:53:28,960
way that software engineers these days

1644
00:53:28,960 --> 00:53:31,520
think about very routine security tasks

1645
00:53:31,520 --> 00:53:32,960
that for a long time

1646
00:53:32,960 --> 00:53:34,720
nobody bothered with because we didn't

1647
00:53:34,720 --> 00:53:36,079
have sort of the framework to think

1648
00:53:36,079 --> 00:53:38,559
about them in a systematic manner

1649
00:53:38,559 --> 00:53:40,720
yeah i i really like that point about

1650
00:53:40,720 --> 00:53:42,480
not having frameworks and

1651
00:53:42,480 --> 00:53:45,680
especially like how contrasting it with

1652
00:53:45,680 --> 00:53:46,240
like how

1653
00:53:46,240 --> 00:53:49,359
chris mentioned um the mist framework

1654
00:53:49,359 --> 00:53:51,599
you know for like adversarial ml so it

1655
00:53:51,599 --> 00:53:53,040
feels like there's some piece of the

1656
00:53:53,040 --> 00:53:54,480
puzzle but there's still like

1657
00:53:54,480 --> 00:53:56,559
integration is still missing is that a

1658
00:53:56,559 --> 00:53:58,160
fair statement uh

1659
00:53:58,160 --> 00:54:02,079
uh um rich

1660
00:54:02,079 --> 00:54:04,400
yeah i think so i think i think there's

1661
00:54:04,400 --> 00:54:05,599
two pieces

1662
00:54:05,599 --> 00:54:08,640
that are missing so the first piece is

1663
00:54:08,640 --> 00:54:10,400
we sort of need to pull these different

1664
00:54:10,400 --> 00:54:12,480
threads together

1665
00:54:12,480 --> 00:54:14,880
sort of within the security community

1666
00:54:14,880 --> 00:54:16,800
insofar as such a thing exists

1667
00:54:16,800 --> 00:54:18,079
have an agreement that yeah this is how

1668
00:54:18,079 --> 00:54:19,760
we should think about vulnerabilities to

1669
00:54:19,760 --> 00:54:21,440
machine learning research

1670
00:54:21,440 --> 00:54:23,200
uh into machine in machine learning

1671
00:54:23,200 --> 00:54:24,880
models i think the second thing we need

1672
00:54:24,880 --> 00:54:25,680
to do though

1673
00:54:25,680 --> 00:54:28,079
is actually focus on how do we

1674
00:54:28,079 --> 00:54:29,280
transition these

1675
00:54:29,280 --> 00:54:31,359
theoretical attacks we have an

1676
00:54:31,359 --> 00:54:33,440
unbelievable number of papers about all

1677
00:54:33,440 --> 00:54:34,800
kinds of different attacks

1678
00:54:34,800 --> 00:54:36,400
against machine learning models in the

1679
00:54:36,400 --> 00:54:38,400
academic space so

1680
00:54:38,400 --> 00:54:40,640
in some sense it's kind of scary because

1681
00:54:40,640 --> 00:54:42,480
we know that there are these potential

1682
00:54:42,480 --> 00:54:43,520
vulnerabilities

1683
00:54:43,520 --> 00:54:46,319
against machine learning models um we

1684
00:54:46,319 --> 00:54:48,000
could be leaking data

1685
00:54:48,000 --> 00:54:51,119
we could be you know

1686
00:54:51,119 --> 00:54:53,599
having models stolen all the time and

1687
00:54:53,599 --> 00:54:55,520
and it's sort of the question that bruce

1688
00:54:55,520 --> 00:54:56,000
brought up

1689
00:54:56,000 --> 00:54:57,680
is it that it's happening and we don't

1690
00:54:57,680 --> 00:54:59,280
know about it or

1691
00:54:59,280 --> 00:55:01,200
is it just those they haven't made the

1692
00:55:01,200 --> 00:55:03,359
jump yeah and so i think what we really

1693
00:55:03,359 --> 00:55:04,160
need

1694
00:55:04,160 --> 00:55:07,440
to see is more collaboration

1695
00:55:07,440 --> 00:55:09,599
sort of industry academic to make these

1696
00:55:09,599 --> 00:55:10,799
transitions happen

1697
00:55:10,799 --> 00:55:12,799
so they can be like okay yeah that's

1698
00:55:12,799 --> 00:55:14,480
that's realistic we really do need to

1699
00:55:14,480 --> 00:55:15,760
worry about that we need to think about

1700
00:55:15,760 --> 00:55:17,119
how to protect against that this other

1701
00:55:17,119 --> 00:55:18,640
thing maybe not so much it's a cute

1702
00:55:18,640 --> 00:55:20,079
trick but it doesn't actually fly

1703
00:55:20,079 --> 00:55:23,119
in reality yeah i feel like i

1704
00:55:23,119 --> 00:55:24,960
one of anita's point is really sticking

1705
00:55:24,960 --> 00:55:27,119
to me is like how people

1706
00:55:27,119 --> 00:55:28,960
they get funding and they'd write cute

1707
00:55:28,960 --> 00:55:30,319
papers and cute tools

1708
00:55:30,319 --> 00:55:31,839
so that's gonna like you know be a big

1709
00:55:31,839 --> 00:55:33,520
takeaway for me anita

1710
00:55:33,520 --> 00:55:35,440
um i want to like um i think there's a

1711
00:55:35,440 --> 00:55:38,000
good closing question from at mammon

1712
00:55:38,000 --> 00:55:40,559
on twitter uh this is for i think it's a

1713
00:55:40,559 --> 00:55:41,839
great question for everybody in the

1714
00:55:41,839 --> 00:55:42,880
panel just like

1715
00:55:42,880 --> 00:55:45,920
uh a quick uh if you can just take 30

1716
00:55:45,920 --> 00:55:48,240
seconds and point to one

1717
00:55:48,240 --> 00:55:50,880
a piece of information that people you

1718
00:55:50,880 --> 00:55:52,160
think should know about air

1719
00:55:52,160 --> 00:55:55,599
teaming um i know it's pretty like

1720
00:55:55,599 --> 00:55:58,240
you know pretty generic but there's one

1721
00:55:58,240 --> 00:55:59,760
key takeaway that you want people to

1722
00:55:59,760 --> 00:56:00,960
take away from

1723
00:56:00,960 --> 00:56:02,640
you know ai red teaming what would it be

1724
00:56:02,640 --> 00:56:04,720
i need app do you want to start us

1725
00:56:04,720 --> 00:56:08,319
do you want to start start us off

1726
00:56:08,559 --> 00:56:11,440
i guess the key takeaway for me would be

1727
00:56:11,440 --> 00:56:13,359
intended use and that

1728
00:56:13,359 --> 00:56:15,839
ai red teaming is not in my opinion it's

1729
00:56:15,839 --> 00:56:19,279
not a technical exercise

1730
00:56:19,440 --> 00:56:21,599
um that's a great like you know way to

1731
00:56:21,599 --> 00:56:22,799
throw the ball at chris

1732
00:56:22,799 --> 00:56:24,079
because you are doing technical

1733
00:56:24,079 --> 00:56:26,640
exercises

1734
00:56:26,640 --> 00:56:29,599
well and that is a great ball um because

1735
00:56:29,599 --> 00:56:31,280
my takeaway would be

1736
00:56:31,280 --> 00:56:32,960
you don't have to be a data scientist to

1737
00:56:32,960 --> 00:56:35,200
attack it

1738
00:56:35,200 --> 00:56:36,799
and we should like pass the ball to the

1739
00:56:36,799 --> 00:56:38,319
data scientist now

1740
00:56:38,319 --> 00:56:40,720
rich well chris stole what i was going

1741
00:56:40,720 --> 00:56:41,760
to say which was

1742
00:56:41,760 --> 00:56:44,640
that the field is the field is wide open

1743
00:56:44,640 --> 00:56:45,920
um i think

1744
00:56:45,920 --> 00:56:47,760
maybe i would just expand about on that

1745
00:56:47,760 --> 00:56:49,599
a little bit and i think like i've maybe

1746
00:56:49,599 --> 00:56:50,000
been

1747
00:56:50,000 --> 00:56:51,920
harving on a bit too much we don't know

1748
00:56:51,920 --> 00:56:53,119
what we don't know yet

1749
00:56:53,119 --> 00:56:56,240
um the field really is wide open

1750
00:56:56,240 --> 00:56:57,520
i think tools are making it more

1751
00:56:57,520 --> 00:56:59,760
accessible uh there's

1752
00:56:59,760 --> 00:57:01,520
so much free content out there to just

1753
00:57:01,520 --> 00:57:03,920
get enough of an idea about how machine

1754
00:57:03,920 --> 00:57:05,680
learning kind of works to get a sense

1755
00:57:05,680 --> 00:57:07,280
for it what the inputs and outputs look

1756
00:57:07,280 --> 00:57:08,240
like

1757
00:57:08,240 --> 00:57:10,720
i really would encourage people to you

1758
00:57:10,720 --> 00:57:11,599
know grab

1759
00:57:11,599 --> 00:57:14,400
the docker image that chris was talking

1760
00:57:14,400 --> 00:57:16,240
about or take a look at what was posted

1761
00:57:16,240 --> 00:57:18,000
for the twitter ethics bug bounty

1762
00:57:18,000 --> 00:57:20,000
or take a look at some of these other ml

1763
00:57:20,000 --> 00:57:22,160
problems and see where you can dive in

1764
00:57:22,160 --> 00:57:23,760
and see what you can find because

1765
00:57:23,760 --> 00:57:25,440
there's there's a lot that's still out

1766
00:57:25,440 --> 00:57:28,400
there to discover so please get involved

1767
00:57:28,400 --> 00:57:30,319
and rich can i ask you to like tweet

1768
00:57:30,319 --> 00:57:31,760
this out from my twitter handle

1769
00:57:31,760 --> 00:57:36,000
at harang go people

1770
00:57:36,480 --> 00:57:39,839
omar how do what does your key take away

1771
00:57:39,839 --> 00:57:42,480
i like the controversy between the

1772
00:57:42,480 --> 00:57:45,760
non-technical versus technical so

1773
00:57:45,760 --> 00:57:47,440
so i'm gonna be between the two i think

1774
00:57:47,440 --> 00:57:50,319
that is a absolutely as a combination of

1775
00:57:50,319 --> 00:57:53,200
what i what i think that is paramount

1776
00:57:53,200 --> 00:57:55,040
instead of giving you just one resource

1777
00:57:55,040 --> 00:57:57,760
is a call to action right is because a

1778
00:57:57,760 --> 00:57:58,799
lot of the things that we

1779
00:57:58,799 --> 00:58:01,440
talked about before even bruce mentioned

1780
00:58:01,440 --> 00:58:02,559
some of these conversations are not

1781
00:58:02,559 --> 00:58:03,760
taking place

1782
00:58:03,760 --> 00:58:05,440
and whenever you're creating technology

1783
00:58:05,440 --> 00:58:08,000
right and so on and it's as rich

1784
00:58:08,000 --> 00:58:10,160
mention it's an evolving thing right so

1785
00:58:10,160 --> 00:58:11,920
it's up to us

1786
00:58:11,920 --> 00:58:14,079
in this panel and in this community

1787
00:58:14,079 --> 00:58:16,400
right to to come up with resources that

1788
00:58:16,400 --> 00:58:18,880
the you know the newcomers actually are

1789
00:58:18,880 --> 00:58:19,440
will

1790
00:58:19,440 --> 00:58:22,480
will take advantage of and i think

1791
00:58:22,480 --> 00:58:23,839
i'm really curious you know to actually

1792
00:58:23,839 --> 00:58:25,520
collect all the different feedback

1793
00:58:25,520 --> 00:58:27,119
all the tools that chris mentioned all

1794
00:58:27,119 --> 00:58:28,880
the research that anita mentioned and

1795
00:58:28,880 --> 00:58:30,880
everything and probably putting together

1796
00:58:30,880 --> 00:58:32,799
probably a github repository you know

1797
00:58:32,799 --> 00:58:34,480
tweet it out i mean whatever we want to

1798
00:58:34,480 --> 00:58:35,680
do but at least

1799
00:58:35,680 --> 00:58:38,240
start incorporating that you know i

1800
00:58:38,240 --> 00:58:39,839
guess it's a call to action versus

1801
00:58:39,839 --> 00:58:43,280
giving you the perfect solution yeah

1802
00:58:43,280 --> 00:58:46,079
thank you uh omar and bruce bring us

1803
00:58:46,079 --> 00:58:46,480
home

1804
00:58:46,480 --> 00:58:49,680
like uh tell us tell us from uh what we

1805
00:58:49,680 --> 00:58:52,799
need to know about this field

1806
00:58:52,799 --> 00:58:55,440
oh bruce uh this is this is i love it

1807
00:58:55,440 --> 00:58:56,480
it's 20 21

1808
00:58:56,480 --> 00:58:58,720
yes

1809
00:58:59,680 --> 00:59:01,200
i know i just feel it and it feels like

1810
00:59:01,200 --> 00:59:04,400
march 2020 all over again

1811
00:59:04,400 --> 00:59:07,200
what anita said that uh right think of

1812
00:59:07,200 --> 00:59:08,400
the big picture this is

1813
00:59:08,400 --> 00:59:11,119
do your threat modeling okay this is not

1814
00:59:11,119 --> 00:59:12,480
just technical this is

1815
00:59:12,480 --> 00:59:14,079
this is the entire system it's a

1816
00:59:14,079 --> 00:59:16,000
socio-technical system and the better

1817
00:59:16,000 --> 00:59:17,440
you can threat model the better you are

1818
00:59:17,440 --> 00:59:19,760
on both sides of this

1819
00:59:19,760 --> 00:59:22,880
fantastic um thank you bruce i you know

1820
00:59:22,880 --> 00:59:25,839
first of all big thank you to all of you

1821
00:59:25,839 --> 00:59:28,079
on the panel today i know how busy

1822
00:59:28,079 --> 00:59:29,599
it was according to accountants i really

1823
00:59:29,599 --> 00:59:31,359
appreciate all of you coming here

1824
00:59:31,359 --> 00:59:33,200
if you are listening to the panel i

1825
00:59:33,200 --> 00:59:35,599
strongly encourage you to go check out

1826
00:59:35,599 --> 00:59:37,680
ai village's discord channel they have

1827
00:59:37,680 --> 00:59:39,599
an entire channel on attacking ai

1828
00:59:39,599 --> 00:59:40,559
systems

1829
00:59:40,559 --> 00:59:42,480
and you know you can you can poke anita

1830
00:59:42,480 --> 00:59:44,160
and rich there and if you hang out at

1831
00:59:44,160 --> 00:59:46,400
the red team village now we've got omar

1832
00:59:46,400 --> 00:59:48,720
and chris also kind of guiding you there

1833
00:59:48,720 --> 00:59:50,799
so please make use of these resources

1834
00:59:50,799 --> 00:59:53,599
and a rich and chris will be tweeting

1835
00:59:53,599 --> 00:59:55,359
out some of these resources

1836
00:59:55,359 --> 00:59:58,079
uh so you should also go look into that

1837
00:59:58,079 --> 00:59:59,680
so with that thank you very much i

1838
00:59:59,680 --> 01:00:02,400
really appreciate all of your time today

1839
01:00:02,400 --> 01:00:15,599
thank you ron

