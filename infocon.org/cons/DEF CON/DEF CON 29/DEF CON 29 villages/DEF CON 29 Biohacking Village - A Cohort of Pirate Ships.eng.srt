1
00:00:00,000 --> 00:00:03,600
all right all right hi everyone

2
00:00:03,600 --> 00:00:06,720
i'm alex pearlman i am a bioethicist and

3
00:00:06,720 --> 00:00:09,120
an independent journalist and researcher

4
00:00:09,120 --> 00:00:11,599
and i'm so excited to be here today with

5
00:00:11,599 --> 00:00:13,360
my colleague christy guarini

6
00:00:13,360 --> 00:00:16,079
um at the biohacking village at defcon

7
00:00:16,079 --> 00:00:17,680
so thanks so much for having us

8
00:00:17,680 --> 00:00:19,439
um christy why don't you introduce

9
00:00:19,439 --> 00:00:20,800
yourself sure

10
00:00:20,800 --> 00:00:22,960
hi everyone i'm christy um i'm an

11
00:00:22,960 --> 00:00:24,000
assistant professor

12
00:00:24,000 --> 00:00:25,519
in the center for medical ethics and

13
00:00:25,519 --> 00:00:27,599
health policy at baylor college of

14
00:00:27,599 --> 00:00:28,800
medicine and really

15
00:00:28,800 --> 00:00:31,920
happy to be here excited to share the

16
00:00:31,920 --> 00:00:33,280
results of the project that i've been

17
00:00:33,280 --> 00:00:34,960
working on with alex

18
00:00:34,960 --> 00:00:37,200
yeah we are so pumped to talk about this

19
00:00:37,200 --> 00:00:39,360
paper and what we really were looking

20
00:00:39,360 --> 00:00:39,920
for

21
00:00:39,920 --> 00:00:43,120
um was to ask whether oversight

22
00:00:43,120 --> 00:00:46,480
of ethics was something that community

23
00:00:46,480 --> 00:00:47,120
scientists

24
00:00:47,120 --> 00:00:50,320
biohackers are interested in and if it

25
00:00:50,320 --> 00:00:51,840
turns out that that's the case

26
00:00:51,840 --> 00:00:55,039
um what forms any kind of oversight

27
00:00:55,039 --> 00:00:58,399
might take um and so given that you know

28
00:00:58,399 --> 00:00:59,760
in these communities there are a lot of

29
00:00:59,760 --> 00:01:01,760
different opinions a lot of different

30
00:01:01,760 --> 00:01:04,319
um backgrounds and ways that folks

31
00:01:04,319 --> 00:01:06,240
self-identify we were really interested

32
00:01:06,240 --> 00:01:06,720
to see

33
00:01:06,720 --> 00:01:09,600
sort of what the attitudes were towards

34
00:01:09,600 --> 00:01:11,119
these questions

35
00:01:11,119 --> 00:01:13,680
um and then we're really excited

36
00:01:13,680 --> 00:01:14,560
specifically

37
00:01:14,560 --> 00:01:18,000
to be able to come and then talk about

38
00:01:18,000 --> 00:01:20,720
our findings um with this paper

39
00:01:20,720 --> 00:01:21,840
especially because

40
00:01:21,840 --> 00:01:24,640
we are really adamant in the research

41
00:01:24,640 --> 00:01:25,680
that we do

42
00:01:25,680 --> 00:01:28,320
that it's for the communities that we're

43
00:01:28,320 --> 00:01:29,759
listening to and that's really what

44
00:01:29,759 --> 00:01:31,119
we're doing we're just interested in

45
00:01:31,119 --> 00:01:32,880
what the attitudes are and then

46
00:01:32,880 --> 00:01:34,880
reporting that data back to the

47
00:01:34,880 --> 00:01:36,000
community

48
00:01:36,000 --> 00:01:38,400
um so i just wanted to kick off from the

49
00:01:38,400 --> 00:01:39,759
beginning christy can you tell us a

50
00:01:39,759 --> 00:01:40,000
little

51
00:01:40,000 --> 00:01:43,680
bit about how this project came about

52
00:01:43,680 --> 00:01:46,799
yeah sure uh so i

53
00:01:46,799 --> 00:01:50,720
i'm currently a grant funded researcher

54
00:01:50,720 --> 00:01:52,840
and my research does focus on this

55
00:01:52,840 --> 00:01:54,799
intersection of

56
00:01:54,799 --> 00:01:58,719
law policy ethics and health

57
00:01:58,719 --> 00:02:01,200
i i'm really interested in innovation in

58
00:02:01,200 --> 00:02:03,920
a past life i was a patent attorney

59
00:02:03,920 --> 00:02:07,200
and so when i came to baylor i did

60
00:02:07,200 --> 00:02:09,520
become aware of independent research

61
00:02:09,520 --> 00:02:10,318
communities

62
00:02:10,318 --> 00:02:12,959
uh and the the work that's being done in

63
00:02:12,959 --> 00:02:15,599
these spaces became very interested

64
00:02:15,599 --> 00:02:18,160
uh in that work and so i started

65
00:02:18,160 --> 00:02:20,959
investigating questions around ownership

66
00:02:20,959 --> 00:02:23,840
uh during the conversations i was having

67
00:02:23,840 --> 00:02:25,760
about ownership questions

68
00:02:25,760 --> 00:02:28,959
the the broader more general questions

69
00:02:28,959 --> 00:02:29,599
around

70
00:02:29,599 --> 00:02:31,680
ethics and the role of ethics oversight

71
00:02:31,680 --> 00:02:33,840
in the communities kept coming up

72
00:02:33,840 --> 00:02:37,280
uh so i went back to to get a little bit

73
00:02:37,280 --> 00:02:38,480
more grant funding to

74
00:02:38,480 --> 00:02:40,800
to drill down into those issues in

75
00:02:40,800 --> 00:02:41,840
particular

76
00:02:41,840 --> 00:02:43,360
and it seemed like the appropriate way

77
00:02:43,360 --> 00:02:45,519
to do so was through an interview study

78
00:02:45,519 --> 00:02:46,000
by

79
00:02:46,000 --> 00:02:48,800
uh listening to you and getting the

80
00:02:48,800 --> 00:02:50,239
perspectives of

81
00:02:50,239 --> 00:02:52,959
the individuals doing the work whose

82
00:02:52,959 --> 00:02:53,360
work

83
00:02:53,360 --> 00:02:57,360
would be subjected to ethics oversight

84
00:02:57,360 --> 00:03:00,800
yeah and i was so pleased and honored to

85
00:03:00,800 --> 00:03:03,040
be asked to join your research team

86
00:03:03,040 --> 00:03:05,519
as someone who had been reporting on and

87
00:03:05,519 --> 00:03:06,400
writing about

88
00:03:06,400 --> 00:03:09,440
these communities for so long um

89
00:03:09,440 --> 00:03:12,400
and so one of the things we heard in our

90
00:03:12,400 --> 00:03:13,599
interviews which

91
00:03:13,599 --> 00:03:16,400
definitely um was something that i had

92
00:03:16,400 --> 00:03:17,760
encountered before was a lot of

93
00:03:17,760 --> 00:03:20,159
different terms for self-identification

94
00:03:20,159 --> 00:03:22,400
so we ultimately go with this umbrella

95
00:03:22,400 --> 00:03:24,799
term in the paper that's biomedical

96
00:03:24,799 --> 00:03:27,120
citizen scientists but we also heard

97
00:03:27,120 --> 00:03:29,200
folks self-identify as biohackers

98
00:03:29,200 --> 00:03:31,440
diy biologists citizen scientists

99
00:03:31,440 --> 00:03:33,120
community scientists

100
00:03:33,120 --> 00:03:36,319
um independent scientists and so um

101
00:03:36,319 --> 00:03:39,760
you know we definitely had a diverse

102
00:03:39,760 --> 00:03:41,920
cross-section a lot of these different

103
00:03:41,920 --> 00:03:43,040
sort of subgroups

104
00:03:43,040 --> 00:03:45,599
of independent research communities but

105
00:03:45,599 --> 00:03:46,480
we also

106
00:03:46,480 --> 00:03:48,720
did a really deep dive into sort of who

107
00:03:48,720 --> 00:03:49,760
exactly

108
00:03:49,760 --> 00:03:52,159
were we interviewing and and who were

109
00:03:52,159 --> 00:03:52,959
our

110
00:03:52,959 --> 00:03:56,720
um study participants so we have a slide

111
00:03:56,720 --> 00:04:00,159
sure and i'm going to share my screen

112
00:04:00,159 --> 00:04:03,519
yeah so uh alex thanks for

113
00:04:03,519 --> 00:04:05,599
i mean that i think that captures well

114
00:04:05,599 --> 00:04:07,280
that we were trying to

115
00:04:07,280 --> 00:04:10,799
elicit sort of uh broad perspectives uh

116
00:04:10,799 --> 00:04:14,640
that were um sort of representative

117
00:04:14,640 --> 00:04:17,199
of various communities multiple uh

118
00:04:17,199 --> 00:04:18,160
communities and

119
00:04:18,160 --> 00:04:20,720
activities we ended up we did these

120
00:04:20,720 --> 00:04:21,759
interviews

121
00:04:21,759 --> 00:04:23,680
at two different conferences uh the

122
00:04:23,680 --> 00:04:25,440
first was biohack the planet and the

123
00:04:25,440 --> 00:04:27,520
second was the global community bio

124
00:04:27,520 --> 00:04:28,800
summit

125
00:04:28,800 --> 00:04:32,960
we spoke to 35 individuals and

126
00:04:32,960 --> 00:04:35,120
the this table which i've broken down on

127
00:04:35,120 --> 00:04:36,639
the right is reported

128
00:04:36,639 --> 00:04:40,000
in our larger paper reporting our

129
00:04:40,000 --> 00:04:41,199
findings which is

130
00:04:41,199 --> 00:04:43,680
publish open access in citizen science

131
00:04:43,680 --> 00:04:45,360
theory and practice

132
00:04:45,360 --> 00:04:48,400
so who are our interviewees well 60

133
00:04:48,400 --> 00:04:51,600
did identify as male the majority were

134
00:04:51,600 --> 00:04:52,240
between

135
00:04:52,240 --> 00:04:55,520
ages 30 and 50. they

136
00:04:55,520 --> 00:04:58,160
came from six different countries but

137
00:04:58,160 --> 00:05:00,240
the majority were based in the united

138
00:05:00,240 --> 00:05:00,880
states

139
00:05:00,880 --> 00:05:03,600
uh and specifically in the united states

140
00:05:03,600 --> 00:05:05,520
uh in the northeast

141
00:05:05,520 --> 00:05:07,520
as well as the west coast really

142
00:05:07,520 --> 00:05:08,880
california

143
00:05:08,880 --> 00:05:11,120
and then finally our interviewees were

144
00:05:11,120 --> 00:05:13,039
produ pretty evenly split

145
00:05:13,039 --> 00:05:15,680
uh between the two conferences uh that

146
00:05:15,680 --> 00:05:17,280
they attended

147
00:05:17,280 --> 00:05:20,479
yeah and we also you know were

148
00:05:20,479 --> 00:05:23,680
were really interested um in

149
00:05:23,680 --> 00:05:26,000
not only the practice of interviewing

150
00:05:26,000 --> 00:05:28,320
that's one of my favorite things to do

151
00:05:28,320 --> 00:05:30,639
um but then also you know we get all of

152
00:05:30,639 --> 00:05:32,240
this interview data from

153
00:05:32,240 --> 00:05:34,639
these amazing folks who take the time to

154
00:05:34,639 --> 00:05:36,080
sit down with us

155
00:05:36,080 --> 00:05:38,000
um and then what do we do with it right

156
00:05:38,000 --> 00:05:39,360
so we had this

157
00:05:39,360 --> 00:05:42,639
really um wonderful

158
00:05:42,639 --> 00:05:45,919
um qualitative research methodology

159
00:05:45,919 --> 00:05:48,400
that i thought was super interesting and

160
00:05:48,400 --> 00:05:49,440
very different from

161
00:05:49,440 --> 00:05:51,520
how journalists generally do research

162
00:05:51,520 --> 00:05:52,960
even though a lot of times we get to the

163
00:05:52,960 --> 00:05:53,919
same

164
00:05:53,919 --> 00:05:56,800
place so you know without an entire

165
00:05:56,800 --> 00:05:58,160
research team which is about

166
00:05:58,160 --> 00:05:59,840
five or so people at the beginning of

167
00:05:59,840 --> 00:06:01,199
the pandemic we

168
00:06:01,199 --> 00:06:04,400
went through a series of steps to sort

169
00:06:04,400 --> 00:06:05,759
of

170
00:06:05,759 --> 00:06:08,560
get to the heart of the data and the

171
00:06:08,560 --> 00:06:09,840
interviews so

172
00:06:09,840 --> 00:06:12,560
what is it that we did christy what was

173
00:06:12,560 --> 00:06:12,880
our

174
00:06:12,880 --> 00:06:15,280
qualitative research methods yeah sure

175
00:06:15,280 --> 00:06:18,160
happy to to summarize those steps um and

176
00:06:18,160 --> 00:06:20,000
some of you listening in may be familiar

177
00:06:20,000 --> 00:06:21,680
with these steps but we really

178
00:06:21,680 --> 00:06:24,560
followed a pretty standard approach um

179
00:06:24,560 --> 00:06:26,319
so first we did have the

180
00:06:26,319 --> 00:06:28,000
audio recordings professionally

181
00:06:28,000 --> 00:06:29,919
transcribed we then cleaned the

182
00:06:29,919 --> 00:06:31,440
transcripts

183
00:06:31,440 --> 00:06:34,319
and by that i mean that we took the

184
00:06:34,319 --> 00:06:36,160
transcripts back to the audio to make

185
00:06:36,160 --> 00:06:37,039
sure

186
00:06:37,039 --> 00:06:38,800
that all of the words on the page were

187
00:06:38,800 --> 00:06:40,639
faithful to the audio recording

188
00:06:40,639 --> 00:06:43,919
um because every every word is a data

189
00:06:43,919 --> 00:06:47,199
point right at that point we then

190
00:06:47,199 --> 00:06:48,880
turn to what's called coding which is

191
00:06:48,880 --> 00:06:50,400
the activity that links

192
00:06:50,400 --> 00:06:53,919
data collection to data interpretation

193
00:06:53,919 --> 00:06:57,120
uh so with coding we take the

194
00:06:57,120 --> 00:06:57,919
transcripts

195
00:06:57,919 --> 00:07:01,280
and we apply we identify and apply

196
00:07:01,280 --> 00:07:04,319
codes to different chunks

197
00:07:04,319 --> 00:07:08,560
of the transcripts the codes are really

198
00:07:08,560 --> 00:07:13,039
a way to identify patterns

199
00:07:13,039 --> 00:07:16,400
across the conversations uh because

200
00:07:16,400 --> 00:07:19,440
oftentimes interviewees will uh

201
00:07:19,440 --> 00:07:21,360
talk for a long time i mean the

202
00:07:21,360 --> 00:07:23,360
conversation can veer away

203
00:07:23,360 --> 00:07:25,680
from the question where it began right

204
00:07:25,680 --> 00:07:28,000
so we need a way to sort of understand

205
00:07:28,000 --> 00:07:29,360
and organize

206
00:07:29,360 --> 00:07:32,880
uh those conversations uh so in a very

207
00:07:32,880 --> 00:07:34,800
long and iterative process we don't

208
00:07:34,800 --> 00:07:36,160
develop a code book

209
00:07:36,160 --> 00:07:38,240
as a team we come together we identify

210
00:07:38,240 --> 00:07:40,720
codes we develop a preliminary code book

211
00:07:40,720 --> 00:07:42,880
we take it back to the transcripts we

212
00:07:42,880 --> 00:07:44,720
see if it works it usually doesn't it

213
00:07:44,720 --> 00:07:46,080
didn't in this case

214
00:07:46,080 --> 00:07:47,520
we went back to the code book we

215
00:07:47,520 --> 00:07:50,560
adjusted it etc until we had a final

216
00:07:50,560 --> 00:07:53,599
stable code book at that point

217
00:07:53,599 --> 00:07:55,520
we all went back to the transcripts and

218
00:07:55,520 --> 00:07:57,199
we coded all of the transcripts

219
00:07:57,199 --> 00:07:59,599
according to that final code book

220
00:07:59,599 --> 00:08:01,840
we then generated coding reports and

221
00:08:01,840 --> 00:08:04,000
then sat down with the coding reports

222
00:08:04,000 --> 00:08:07,840
in order to do the interpretation

223
00:08:07,840 --> 00:08:11,039
work and identify themes

224
00:08:11,039 --> 00:08:14,080
that we knew we wanted to report in our

225
00:08:14,080 --> 00:08:15,919
final publication

226
00:08:15,919 --> 00:08:18,400
yep and i'm gonna talk a little bit

227
00:08:18,400 --> 00:08:19,039
about

228
00:08:19,039 --> 00:08:22,080
what those themes actually were um

229
00:08:22,080 --> 00:08:25,759
and here i will share my screen um

230
00:08:25,759 --> 00:08:28,960
we did uh we did find ten

231
00:08:28,960 --> 00:08:32,479
sort of high-level ethical priorities um

232
00:08:32,479 --> 00:08:34,640
that folks noted as things that were

233
00:08:34,640 --> 00:08:35,919
important to them that they were

234
00:08:35,919 --> 00:08:37,120
thinking about

235
00:08:37,120 --> 00:08:39,440
and that they felt you know was really a

236
00:08:39,440 --> 00:08:40,880
part of conversations that were

237
00:08:40,880 --> 00:08:42,080
happening in the community

238
00:08:42,080 --> 00:08:45,360
and so those ten that we sort of

239
00:08:45,360 --> 00:08:48,080
picked out of our of our data as the

240
00:08:48,080 --> 00:08:49,040
priorities

241
00:08:49,040 --> 00:08:50,560
of folks in these communities were

242
00:08:50,560 --> 00:08:52,160
autonomy respect

243
00:08:52,160 --> 00:08:55,440
diversity safety community consent

244
00:08:55,440 --> 00:08:58,320
equality education altruism and good

245
00:08:58,320 --> 00:08:59,279
science

246
00:08:59,279 --> 00:09:02,560
and it seems you know kind of silly to

247
00:09:02,560 --> 00:09:03,760
say because these are all

248
00:09:03,760 --> 00:09:07,440
pretty high level ethics concepts

249
00:09:07,440 --> 00:09:09,360
and principles that are necessary for

250
00:09:09,360 --> 00:09:10,560
good science

251
00:09:10,560 --> 00:09:13,680
um but they don't actually map

252
00:09:13,680 --> 00:09:16,640
exactly onto the traditional sort of

253
00:09:16,640 --> 00:09:17,440
establishment

254
00:09:17,440 --> 00:09:20,880
ethical principles that um you know more

255
00:09:20,880 --> 00:09:24,640
traditional and establishment uh ethical

256
00:09:24,640 --> 00:09:25,519
oversight

257
00:09:25,519 --> 00:09:28,560
um models sort of used as guiding

258
00:09:28,560 --> 00:09:29,600
principles and so

259
00:09:29,600 --> 00:09:31,600
that was a little bit interesting but

260
00:09:31,600 --> 00:09:33,120
also

261
00:09:33,120 --> 00:09:35,440
it's really important to note that these

262
00:09:35,440 --> 00:09:37,440
conversations are already happening

263
00:09:37,440 --> 00:09:40,480
when we went to talk to folks about

264
00:09:40,480 --> 00:09:42,880
what their attitudes were about ethics

265
00:09:42,880 --> 00:09:44,560
it wasn't the first time that

266
00:09:44,560 --> 00:09:46,720
these conversations had come up and that

267
00:09:46,720 --> 00:09:48,240
it was it was very clear

268
00:09:48,240 --> 00:09:51,760
that a lot of thought is going on in the

269
00:09:51,760 --> 00:09:54,000
community and a lot of conversations

270
00:09:54,000 --> 00:09:57,200
center around these concepts of

271
00:09:57,200 --> 00:10:00,240
ethical oversight and how do we do good

272
00:10:00,240 --> 00:10:04,399
science well um and do it right so

273
00:10:04,399 --> 00:10:06,959
that's one of the things that we um

274
00:10:06,959 --> 00:10:08,880
definitely saw

275
00:10:08,880 --> 00:10:11,680
um and i also wanted to talk a little

276
00:10:11,680 --> 00:10:12,480
bit about

277
00:10:12,480 --> 00:10:15,120
um you know the fact that there's

278
00:10:15,120 --> 00:10:16,959
already this culture of ethics

279
00:10:16,959 --> 00:10:21,200
that exists in in these in these spaces

280
00:10:21,200 --> 00:10:25,120
um but there's not exactly a

281
00:10:25,120 --> 00:10:28,320
sort of centralized way that folks are

282
00:10:28,320 --> 00:10:29,360
using

283
00:10:29,360 --> 00:10:32,480
to actually put their principles

284
00:10:32,480 --> 00:10:35,760
into practice right um and so

285
00:10:35,760 --> 00:10:37,600
one of the things that we talk about in

286
00:10:37,600 --> 00:10:39,519
our paper are a few different

287
00:10:39,519 --> 00:10:42,640
models for ethical oversight that exist

288
00:10:42,640 --> 00:10:44,560
and that we have sort of

289
00:10:44,560 --> 00:10:47,680
seen examples of or know um

290
00:10:47,680 --> 00:10:51,120
that folks might be interested in

291
00:10:51,839 --> 00:10:54,480
and i'm happy to share the conversations

292
00:10:54,480 --> 00:10:57,680
around those oversight models i'm gonna

293
00:10:57,680 --> 00:11:00,800
share my screen and i am watching the

294
00:11:00,800 --> 00:11:02,640
clocks i'll i'll try and do this in just

295
00:11:02,640 --> 00:11:04,079
a couple of minutes

296
00:11:04,079 --> 00:11:06,959
um so as as probably everybody here

297
00:11:06,959 --> 00:11:08,399
knows the traditional

298
00:11:08,399 --> 00:11:10,800
uh mechanism for ethics oversight in the

299
00:11:10,800 --> 00:11:12,000
united states is called the

300
00:11:12,000 --> 00:11:14,000
institutional review board

301
00:11:14,000 --> 00:11:16,959
it is required for all uh federally

302
00:11:16,959 --> 00:11:19,279
funded research as well as fda

303
00:11:19,279 --> 00:11:22,640
regulated activities uh

304
00:11:22,640 --> 00:11:25,839
so what we were interested in is what

305
00:11:25,839 --> 00:11:26,399
our

306
00:11:26,399 --> 00:11:29,839
interviewees perspectives are on the

307
00:11:29,839 --> 00:11:32,880
feasibility and appropriateness of that

308
00:11:32,880 --> 00:11:33,440
model

309
00:11:33,440 --> 00:11:37,519
as well as some alternatives to it

310
00:11:37,519 --> 00:11:40,800
given that irbs may not be accessible

311
00:11:40,800 --> 00:11:42,880
to independent research communities

312
00:11:42,880 --> 00:11:44,399
either because they're not affiliated

313
00:11:44,399 --> 00:11:46,800
with institutions or they can't afford

314
00:11:46,800 --> 00:11:49,040
the fees that are charged by independent

315
00:11:49,040 --> 00:11:50,240
irbs

316
00:11:50,240 --> 00:11:52,959
uh so so we set them out here this is we

317
00:11:52,959 --> 00:11:55,440
call this our stair step figure

318
00:11:55,440 --> 00:11:57,920
where we've organized these models from

319
00:11:57,920 --> 00:12:00,320
uh sort of formal processes to informal

320
00:12:00,320 --> 00:12:01,279
processes

321
00:12:01,279 --> 00:12:04,639
uh uh an external review meaning

322
00:12:04,639 --> 00:12:05,680
external to

323
00:12:05,680 --> 00:12:08,880
uh the communities

324
00:12:08,880 --> 00:12:12,959
to very internal mechanisms

325
00:12:12,959 --> 00:12:15,360
and with the caveat that we were not

326
00:12:15,360 --> 00:12:16,320
able to discuss

327
00:12:16,320 --> 00:12:19,200
every mechanism with every interviewee

328
00:12:19,200 --> 00:12:20,880
due to time constraints

329
00:12:20,880 --> 00:12:23,279
but if we start over on the left we did

330
00:12:23,279 --> 00:12:24,000
discuss

331
00:12:24,000 --> 00:12:29,040
traditional irb review as well as

332
00:12:29,040 --> 00:12:31,600
some sort of experimentation that's

333
00:12:31,600 --> 00:12:32,639
that's taking place

334
00:12:32,639 --> 00:12:35,839
with irb review in some communities so

335
00:12:35,839 --> 00:12:37,519
some communities are

336
00:12:37,519 --> 00:12:40,880
registering their own irbs we discussed

337
00:12:40,880 --> 00:12:43,279
expert consultation models this is where

338
00:12:43,279 --> 00:12:45,040
outside experts

339
00:12:45,040 --> 00:12:46,959
by that i mean professional ethicists

340
00:12:46,959 --> 00:12:49,120
biosafety experts other scientific

341
00:12:49,120 --> 00:12:49,839
experts

342
00:12:49,839 --> 00:12:51,519
might make themselves available to

343
00:12:51,519 --> 00:12:53,680
communities to provide opinions and

344
00:12:53,680 --> 00:12:55,279
answer questions

345
00:12:55,279 --> 00:12:57,600
then there's a community review model

346
00:12:57,600 --> 00:13:01,680
which is a really a community built

347
00:13:01,680 --> 00:13:05,040
oversight committee that usually

348
00:13:05,040 --> 00:13:07,519
the idea is that they usually provide

349
00:13:07,519 --> 00:13:09,760
guidance

350
00:13:09,760 --> 00:13:12,720
to specific projects then there's this

351
00:13:12,720 --> 00:13:15,120
idea of crowdsource review which is a

352
00:13:15,120 --> 00:13:16,560
really interesting model that's been

353
00:13:16,560 --> 00:13:18,480
proposed in the literature and the idea

354
00:13:18,480 --> 00:13:19,200
is that

355
00:13:19,200 --> 00:13:22,560
an individual or individuals designated

356
00:13:22,560 --> 00:13:25,680
as citizen ethicists would provide their

357
00:13:25,680 --> 00:13:26,560
opinions

358
00:13:26,560 --> 00:13:29,839
on those projects there's

359
00:13:29,839 --> 00:13:32,800
the idea of systematized self-reflection

360
00:13:32,800 --> 00:13:34,480
this is a model that was used quite

361
00:13:34,480 --> 00:13:36,560
successfully by a group of quantified

362
00:13:36,560 --> 00:13:37,279
sulfurs

363
00:13:37,279 --> 00:13:38,959
and the idea is that project

364
00:13:38,959 --> 00:13:41,199
participants get together periodically

365
00:13:41,199 --> 00:13:42,320
and they reflect

366
00:13:42,320 --> 00:13:43,760
on the ethical issues that they're

367
00:13:43,760 --> 00:13:45,680
confronting in their work

368
00:13:45,680 --> 00:13:47,199
and then finally all the way to the

369
00:13:47,199 --> 00:13:48,800
right are codes of ethics which of

370
00:13:48,800 --> 00:13:50,959
course have been adopted

371
00:13:50,959 --> 00:13:54,399
in community bio and diy uh

372
00:13:54,399 --> 00:13:57,600
bio communities so we

373
00:13:57,600 --> 00:14:00,320
again we sought to understand you know

374
00:14:00,320 --> 00:14:02,560
perspectives around feasibility

375
00:14:02,560 --> 00:14:06,000
and appropriateness uh with these models

376
00:14:06,000 --> 00:14:08,160
yeah and i think you know there was a

377
00:14:08,160 --> 00:14:10,639
wide range of opinions

378
00:14:10,639 --> 00:14:12,880
about and like attitudes about each one

379
00:14:12,880 --> 00:14:14,560
of these and like you said we didn't get

380
00:14:14,560 --> 00:14:16,240
a chance to bring up each one to every

381
00:14:16,240 --> 00:14:16,959
person

382
00:14:16,959 --> 00:14:18,560
but we did ask a lot of people about a

383
00:14:18,560 --> 00:14:20,240
lot of different kinds of oversight

384
00:14:20,240 --> 00:14:21,120
models

385
00:14:21,120 --> 00:14:23,519
um and so there's definitely you know

386
00:14:23,519 --> 00:14:25,120
the biggest split that i saw

387
00:14:25,120 --> 00:14:28,639
was um the difference in

388
00:14:28,639 --> 00:14:30,800
opinion about bringing on experts and

389
00:14:30,800 --> 00:14:33,040
even consulting with outside experts

390
00:14:33,040 --> 00:14:33,680
like

391
00:14:33,680 --> 00:14:35,519
you know folks who are legal experts are

392
00:14:35,519 --> 00:14:37,600
regulated knowledgeable about regulatory

393
00:14:37,600 --> 00:14:38,560
schemes

394
00:14:38,560 --> 00:14:41,680
or bioethics or biosafety experts who

395
00:14:41,680 --> 00:14:43,440
are part of sort of establishment

396
00:14:43,440 --> 00:14:44,480
systems

397
00:14:44,480 --> 00:14:47,440
um there was a large group of folks that

398
00:14:47,440 --> 00:14:49,040
we interviewed who are more hesitant

399
00:14:49,040 --> 00:14:49,760
about that

400
00:14:49,760 --> 00:14:52,000
but then there are also those who are

401
00:14:52,000 --> 00:14:54,480
really really open to that idea

402
00:14:54,480 --> 00:14:56,880
um and are willing that are willing to

403
00:14:56,880 --> 00:14:57,519
engage

404
00:14:57,519 --> 00:14:58,959
in building bridges building

405
00:14:58,959 --> 00:15:00,800
relationships and having these sort of

406
00:15:00,800 --> 00:15:02,320
open conversations

407
00:15:02,320 --> 00:15:04,320
with folks in establishment spaces that

408
00:15:04,320 --> 00:15:05,760
i really am excited about

409
00:15:05,760 --> 00:15:09,839
so um i am too a big takeaway for me is

410
00:15:09,839 --> 00:15:11,279
it needs to be the right partner

411
00:15:11,279 --> 00:15:13,519
it needs to be a partner who is willing

412
00:15:13,519 --> 00:15:14,800
to invest time

413
00:15:14,800 --> 00:15:17,440
into understanding the activities and

414
00:15:17,440 --> 00:15:19,199
the priorities of the community

415
00:15:19,199 --> 00:15:21,680
that is asking them for help i thought

416
00:15:21,680 --> 00:15:23,600
it was very encouraging though as well

417
00:15:23,600 --> 00:15:26,560
yeah um so what what are some of the

418
00:15:26,560 --> 00:15:28,800
barriers to getting to these models that

419
00:15:28,800 --> 00:15:30,160
you see

420
00:15:30,160 --> 00:15:33,519
yeah so um what we heard is

421
00:15:33,519 --> 00:15:35,199
i mean as you said i mean lots of

422
00:15:35,199 --> 00:15:36,720
interest in uh

423
00:15:36,720 --> 00:15:38,720
trying out these different kinds of

424
00:15:38,720 --> 00:15:41,279
models uh but significant barriers in

425
00:15:41,279 --> 00:15:42,480
doing so

426
00:15:42,480 --> 00:15:44,320
and honestly it seems to come down to

427
00:15:44,320 --> 00:15:45,600
time and money

428
00:15:45,600 --> 00:15:47,360
uh especially if you look at sort of

429
00:15:47,360 --> 00:15:48,959
community review models

430
00:15:48,959 --> 00:15:51,440
um that that really will take an

431
00:15:51,440 --> 00:15:52,320
investment

432
00:15:52,320 --> 00:15:55,519
uh to to establish

433
00:15:55,519 --> 00:15:58,880
uh as well as to sustain

434
00:15:58,880 --> 00:16:01,839
so i think to me that's that's the next

435
00:16:01,839 --> 00:16:02,639
big question

436
00:16:02,639 --> 00:16:05,040
is how we can support communities um in

437
00:16:05,040 --> 00:16:06,000
it you know

438
00:16:06,000 --> 00:16:09,120
piloting these models um uh

439
00:16:09,120 --> 00:16:11,759
and helping them overcome those barriers

440
00:16:11,759 --> 00:16:13,120
what are your thoughts though well

441
00:16:13,120 --> 00:16:15,199
absolutely i mean i think that you know

442
00:16:15,199 --> 00:16:17,120
our role is just to sort of listen and

443
00:16:17,120 --> 00:16:19,279
sort of help if if we're asked

444
00:16:19,279 --> 00:16:21,199
but that ultimately you know one of the

445
00:16:21,199 --> 00:16:22,639
other things that we heard

446
00:16:22,639 --> 00:16:25,440
sort of across the board really was that

447
00:16:25,440 --> 00:16:26,720
any kind of ethics

448
00:16:26,720 --> 00:16:29,360
oversight model needs to be culture

449
00:16:29,360 --> 00:16:29,759
built

450
00:16:29,759 --> 00:16:31,600
from inside the community and it needs

451
00:16:31,600 --> 00:16:33,279
to be driven by members of these

452
00:16:33,279 --> 00:16:34,560
communities

453
00:16:34,560 --> 00:16:37,600
and not from people who are outside um

454
00:16:37,600 --> 00:16:39,600
and so that's that's basically what we

455
00:16:39,600 --> 00:16:41,279
were you know trying to do with this

456
00:16:41,279 --> 00:16:43,279
paper was just sort of listen

457
00:16:43,279 --> 00:16:46,079
and just sort of keep pushing this

458
00:16:46,079 --> 00:16:47,759
conversation along

459
00:16:47,759 --> 00:16:49,199
you know that that had already been

460
00:16:49,199 --> 00:16:50,720
happening and will continue to happen

461
00:16:50,720 --> 00:16:51,279
and

462
00:16:51,279 --> 00:16:54,000
um you know we're just really excited to

463
00:16:54,000 --> 00:16:54,480
to

464
00:16:54,480 --> 00:16:56,800
you know help continue to help and be

465
00:16:56,800 --> 00:16:57,519
there for

466
00:16:57,519 --> 00:16:59,600
for any advice or insight or whatever or

467
00:16:59,600 --> 00:17:01,440
more just more conversations

468
00:17:01,440 --> 00:17:05,760
at conferences because that's fun too

469
00:17:05,760 --> 00:17:08,240
so yeah i think we're probably at the

470
00:17:08,240 --> 00:17:08,799
end of

471
00:17:08,799 --> 00:17:13,439
time but um we also wanted to note that

472
00:17:13,439 --> 00:17:16,039
we we built a website called

473
00:17:16,039 --> 00:17:17,280
outlawbio.org

474
00:17:17,280 --> 00:17:20,160
and all of our research and a lot of

475
00:17:20,160 --> 00:17:21,119
christie's

476
00:17:21,119 --> 00:17:22,559
research and other members of our

477
00:17:22,559 --> 00:17:24,640
research team that is related

478
00:17:24,640 --> 00:17:28,079
um can all be found on that website and

479
00:17:28,079 --> 00:17:30,240
so we're really excited to sort of share

480
00:17:30,240 --> 00:17:32,400
all of this with the community and and

481
00:17:32,400 --> 00:17:34,960
build it out

482
00:17:34,960 --> 00:17:38,080
yeah so thank you for the opportunity to

483
00:17:38,080 --> 00:17:40,320
to share their project uh it's great

484
00:17:40,320 --> 00:17:41,919
talking with you

485
00:17:41,919 --> 00:17:43,679
to see you too christy and thanks

486
00:17:43,679 --> 00:17:45,600
everyone at defcon for

487
00:17:45,600 --> 00:17:47,200
listening and thank you especially to

488
00:17:47,200 --> 00:17:50,240
nina and the other biohacker village

489
00:17:50,240 --> 00:17:53,120
organizers for having us and um yeah

490
00:17:53,120 --> 00:17:53,760
hopefully

491
00:17:53,760 --> 00:17:56,559
we will see you next year in person one

492
00:17:56,559 --> 00:17:57,840
can only hope

493
00:17:57,840 --> 00:18:01,840
thank you so much everyone

