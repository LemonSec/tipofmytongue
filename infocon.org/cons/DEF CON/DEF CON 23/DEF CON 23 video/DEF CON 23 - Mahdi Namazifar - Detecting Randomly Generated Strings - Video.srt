00:00:00.000 --> 00:00:06.173
I'm Mahdi Namazifar and
currently I'm a senior data

00:00:06.173 --> 00:00:11.979
scientist with Twitter but
before that I was lucky enough

00:00:11.979 --> 00:00:15.616
to be a part of the amazing
Talos team with Cisco and this

00:00:15.616 --> 00:00:20.554
work I did was -- wow, that's
awesome. Was something that I

00:00:20.554 --> 00:00:26.560
did when I was with Talos. First
I want to give you the

00:00:32.599 --> 00:00:37.471
definition of the problem that
I'm trying to address here.

00:00:37.471 --> 00:00:41.208
Let's say that you're given a
random string and you want to

00:00:41.208 --> 00:00:45.379
decide whether this string is a
random sequence of characters.

00:00:45.379 --> 00:00:48.248
So one thing to note here that I
said random sequence of

00:00:48.248 --> 00:00:52.686
characters. And this work does
not address strings that are

00:00:52.686 --> 00:00:56.323
random sequence of dictionary
words. The other thing is that

00:00:56.323 --> 00:01:01.228
my focus is are on strings that
are at least 8 characters long

00:01:01.228 --> 00:01:04.097
and anything less than that is
very difficult for a human being

00:01:04.097 --> 00:01:07.968
to detect randomness. So I'm
focusing on strings of length 8

00:01:07.968 --> 00:01:13.974
or more. So why do we even look
at this problem? Our motivation

00:01:29.623 --> 00:01:33.894
for this was detecting domain
names that are generated using

00:01:33.894 --> 00:01:38.732
domain generation algorithms.
You know better than I do how

00:01:38.732 --> 00:01:44.905
these are used. And this is not
a new problem. This problem has

00:01:44.905 --> 00:01:48.675
been studied quite a lot.
There's rich literature around

00:01:48.675 --> 00:01:53.780
it, at least I'm aware of some
of them here and also a bunch of

00:01:53.780 --> 00:01:59.486
works that are done at Cisco.
Usually the way these works work

00:01:59.486 --> 00:02:04.424
is that they look at this
problem as a classification

00:02:04.424 --> 00:02:07.995
problem and they take a machine
learning classification approach

00:02:07.995 --> 00:02:15.635
to solve these problems. And but
here my approach is a little bit

00:02:15.635 --> 00:02:20.941
different. So I give you the big
picture of the approach, and

00:02:20.941 --> 00:02:26.947
then I'll go deeper into the
details of it. So the first

00:02:29.883 --> 00:02:35.889
thing I want to do -- out of
these, I want a word list. List

00:02:44.531 --> 00:02:51.671
of words, as many as possible.
And once I have this, I put them

00:02:51.671 --> 00:02:56.943
together, I call it the mega
dictionary. How do I use it? I

00:02:56.943 --> 00:03:01.381
basically take the arbitrary
string that I'm given, and I

00:03:01.381 --> 00:03:05.419
take substrings of it, and I
look them up in this mega

00:03:05.419 --> 00:03:08.622
dictionary. So based on the
number of dictionary hits that I

00:03:08.622 --> 00:03:12.726
find out of these substrings,
based on the length of these

00:03:12.726 --> 00:03:15.495
substrings and based on the
different languages that these

00:03:15.495 --> 00:03:19.199
substrings are from, I come up
with a randomness score. And

00:03:19.199 --> 00:03:25.205
based on this randomness score,
I determine whether or not this

00:03:32.312 --> 00:03:38.318
is a random string, and if you
notice here, the idea is that if

00:03:42.522 --> 00:03:46.526
you could see how the substrings
of this string could be covered

00:03:46.526 --> 00:03:50.063
by different words from
different languages, then we

00:03:50.063 --> 00:03:53.633
have an idea whether this is a
random string or not. So I'll

00:03:53.633 --> 00:03:59.873
get into some details here. So
the first part is that mega

00:03:59.873 --> 00:04:05.812
dictionary. First I try to find
as many dictionaries that I can,

00:04:09.316 --> 00:04:14.888
language-based dictionaries,
basically, from the Internet.

00:04:14.888 --> 00:04:20.060
These are almost 70 different
languages that I found different

00:04:20.060 --> 00:04:25.298
dictionaries for. Some of them
are constructed languages, some

00:04:25.298 --> 00:04:32.372
of them are English. I have
multiple versions of the English

00:04:32.372 --> 00:04:36.243
dictionary like British English
dictionaries, or American

00:04:36.243 --> 00:04:43.116
English dictionaries. So this is
the languages that I was able to

00:04:43.116 --> 00:04:49.122
find. So other than that, so
also I should note here that out

00:04:52.526 --> 00:04:57.464
of these dictionaries I only
want the key, not the value for

00:04:57.464 --> 00:05:00.901
each dictionaries. You have the
word and you have the definition

00:05:00.901 --> 00:05:03.203
of it. I don't care about the
definition, I just want the

00:05:03.203 --> 00:05:10.243
words. So that was the
languages. I also get some

00:05:10.243 --> 00:05:16.249
names. Based on the census data,
female names, maiden names,

00:05:18.785 --> 00:05:25.058
surnames. Also I get a list of
Scrabble words, words that are

00:05:25.058 --> 00:05:29.763
not necessarily in the English
dictionary, they could be

00:05:29.763 --> 00:05:36.603
acronyms here. And so these two
items were given to me by my

00:05:36.603 --> 00:05:40.840
good friend and former colleague
Adam Katz and they helped a lot,

00:05:40.840 --> 00:05:47.914
actually. The next thing is I
get the Alexa 1000 domain names,

00:05:47.914 --> 00:05:53.954
I add them to my word list. The
word Yelp, the word eBay might

00:05:53.954 --> 00:05:59.726
not be in any dictionary, but
these are actually important

00:05:59.726 --> 00:06:05.665
words. Some numbers, and also
got my hands on a dictionary of

00:06:09.869 --> 00:06:15.876
or a list of texting acronyms.
YOLO, BRB, things like that. So

00:06:19.879 --> 00:06:24.484
for some of these words, I need
to do some special treatments.

00:06:24.484 --> 00:06:28.355
For instance, the words that are
coming from eastern European

00:06:28.355 --> 00:06:34.127
languages I need to get rid of
accent on the characters. For

00:06:34.127 --> 00:06:41.101
the Mandarin language, I need to
get these characters, if I can

00:06:41.101 --> 00:06:44.771
find my pointer, these
characters, and basically

00:06:44.771 --> 00:06:51.678
translate them to Roman
characters. And for that I use

00:06:51.678 --> 00:06:58.051
the pin ying standard for
Russian and Ukrainian. It will

00:06:58.051 --> 00:07:03.990
take care of the fact that I and
Y are used interchangeably and a

00:07:10.096 --> 00:07:16.870
bunch of other special
treatments like that. So the

00:07:16.870 --> 00:07:21.474
next thing I need to note here
is that the same word might

00:07:21.474 --> 00:07:25.445
appear in multiple dictionaries.
The word book appears in at

00:07:25.445 --> 00:07:32.218
least these three dictionaries,
English, polish, Dutch. So to

00:07:32.218 --> 00:07:37.090
take care of this, I run a map
to find all the dictionaries

00:07:37.090 --> 00:07:43.196
that a given word appears in. So
the result of this looks like

00:07:43.196 --> 00:07:47.000
something like this for each
word, for each given word that I

00:07:47.000 --> 00:07:53.106
have in these dictionaries, I
have a list of dictionaries that

00:07:53.106 --> 00:07:59.112
that word appears in. So here in
this example, sui appears in the

00:07:59.112 --> 00:08:01.614
French dictionary, in the
Catalan dictionary, and a bunch

00:08:01.614 --> 00:08:07.620
of others. So this is at the end
what my mega dictionary is going

00:08:14.327 --> 00:08:20.967
to look like. The keys here are
words and the values here are

00:08:20.967 --> 00:08:26.706
lists of dictionary names. And
the look of complexity of this

00:08:26.706 --> 00:08:31.945
is constant, so it's pretty fast
to look up anything here. And at

00:08:31.945 --> 00:08:38.818
the end I have about 12 million
words in this mega dictionary.

00:08:38.818 --> 00:08:42.655
So that was the dictionary part.
Now how am I going to use this

00:08:42.655 --> 00:08:49.829
dictionary for detecting random
strings? So I find substrings of

00:08:49.829 --> 00:08:53.600
a given string and I look them
up in this dictionary. How do I

00:08:53.600 --> 00:08:59.606
do that? I do that by traversing
strings. I can traverse a string

00:09:02.175 --> 00:09:07.046
from left to right this way. I
have two indices, I fix them one

00:09:07.046 --> 00:09:10.784
at the end, one at the
beginning. If I'm traversing

00:09:10.784 --> 00:09:15.121
from left, I fix the right index
and I move the left index one at

00:09:15.121 --> 00:09:21.795
a time. And as a result, these
are the substrings that I find.

00:09:21.795 --> 00:09:27.300
If I find the substring the same
string from right to left, I get

00:09:27.300 --> 00:09:31.738
the substrings. It will be a
little bit more clear why I do

00:09:31.738 --> 00:09:35.175
it once from right to left, once
from left to right later on, but

00:09:35.175 --> 00:09:40.280
let's just fix the definition
here and go to the next slide.

00:09:40.280 --> 00:09:44.651
And look at an example. How do I
find the substrings of a given

00:09:44.651 --> 00:09:49.289
string? So let's say we are just
dealing with the English

00:09:49.289 --> 00:09:56.229
dictionary only. And let's say
that this is my given string. So

00:09:56.229 --> 00:09:59.999
I start traversing this string
and at each step and I'm doing

00:09:59.999 --> 00:10:06.005
it from left. And at each step I
find the substring and I look

00:10:06.005 --> 00:10:13.146
that up in my mega dictionary. I
continue until I find a hit. So

00:10:13.146 --> 00:10:17.050
none of these words appeared in
my English dictionary until I

00:10:17.050 --> 00:10:23.356
hit this word. That appeared in
my English dictionary. So I take

00:10:23.356 --> 00:10:28.495
that word, I take it out of my
string, put it aside, and now I

00:10:28.495 --> 00:10:35.635
am left with this substring. So
again, I start -- I reset the

00:10:35.635 --> 00:10:42.041
indexes and I start doing the
traverse from left again. And

00:10:42.041 --> 00:10:44.878
these are the substrings. None
of them are in the English

00:10:44.878 --> 00:10:50.350
dictionary, until I find this
word. And it's a hit. I take it

00:10:50.350 --> 00:10:54.888
out. I'm left with this
substring. And so on. So at the

00:10:54.888 --> 00:11:00.994
end I get these three words. Out
of this string that it's good to

00:11:00.994 --> 00:11:07.500
be here. So I did this once from
left, I do the same thing once

00:11:07.500 --> 00:11:13.206
from right to left. And at the
end from left to right I get

00:11:13.206 --> 00:11:15.208
this list and if I do it from
right to left I get this list.

00:11:18.878 --> 00:11:26.052
You see that? So these two, ones
from right, ones from left give

00:11:26.052 --> 00:11:31.558
me a higher chance to find the
right words in that given

00:11:31.558 --> 00:11:37.030
string. So I need to pick
between these two and because

00:11:37.030 --> 00:11:42.869
the minimum length of the words
that are in this list is four, I

00:11:42.869 --> 00:11:45.505
pick this one. Because the
longer words that you find in

00:11:45.505 --> 00:11:52.779
the substring, the higher is the
chance that this is not by

00:11:52.779 --> 00:11:59.819
chance. So that was just looking
up in the English dictionary.

00:11:59.819 --> 00:12:04.123
What about the case that we
have, well, we built the

00:12:04.123 --> 00:12:08.661
dictionary based on 70 different
languages. How do we use that?

00:12:08.661 --> 00:12:14.067
Let's say we are looking at this
string, and let's say that I

00:12:14.067 --> 00:12:16.836
found these substrings in this
string. Right? And these are all

00:12:16.836 --> 00:12:22.842
the dictionaries that each one
of them appears in. So the

00:12:28.615 --> 00:12:32.652
question here is, okay. So at
the end of the day, how many

00:12:32.652 --> 00:12:36.222
languages do I need to cover
these substrings that I found.

00:12:36.222 --> 00:12:43.896
If I take the union of these,
these dictionary lists, it's

00:12:43.896 --> 00:12:48.434
going to be way too many. If I
take the intersection of these,

00:12:48.434 --> 00:12:53.373
it's going to be zero. They
don't have any intersection. So

00:12:53.373 --> 00:12:59.078
I need to find the minimal set
of dictionaries or languages

00:12:59.078 --> 00:13:06.886
that cover these substrings. So
how do I do that? That's

00:13:06.886 --> 00:13:10.923
actually the problem of minimum
hitting set. It's a very

00:13:10.923 --> 00:13:14.927
well-known problem. Very
well-studied and this is

00:13:14.927 --> 00:13:17.964
basically the father of a bunch
of other unknown problems such

00:13:17.964 --> 00:13:22.468
as set covering problem and
stuff. This is the definition of

00:13:22.468 --> 00:13:26.272
the problem. I don't want to get
into the definition. You can

00:13:26.272 --> 00:13:31.511
always look it up. But envelope
this is a problem. But the good

00:13:31.511 --> 00:13:33.513
news is our sets are small
enough, if you look at it again,

00:13:33.513 --> 00:13:39.519
our sets are small enough that
even if I do a greedy search in

00:13:41.921 --> 00:13:49.495
the space of possible solutions,
I can find the headings set. So

00:13:49.495 --> 00:13:54.434
I exactly do that. I basically
have this very, very simple

00:13:54.434 --> 00:13:59.272
algorithm for finding a minimal
heading set problem. This is by

00:13:59.272 --> 00:14:06.779
know means an optimal or the
best greedy algorithm for this,

00:14:06.779 --> 00:14:14.287
but I don't care because it's
fast enough. It gives me what I

00:14:14.287 --> 00:14:20.293
need. That's good enough for me.
So back to our example. So we

00:14:23.696 --> 00:14:29.368
have these sets that we wanted
to find the minimum hitting set

00:14:29.368 --> 00:14:34.741
of. And by just applying a very
simple reading surf, I find

00:14:34.741 --> 00:14:40.747
these minimum hitting sets.
These are the -- the minimum

00:14:43.783 --> 00:14:48.721
hitting set number is 2. Meaning
that I need at least 2 different

00:14:48.721 --> 00:14:54.727
languages to cover these
substrings that I found. So

00:14:59.565 --> 00:15:04.303
based on this minimum hitting
set number, the length of the

00:15:04.303 --> 00:15:07.140
string itself, the percentage of
it that was covered by the

00:15:07.140 --> 00:15:12.178
substrings that I found, some of
the length of the words that are

00:15:12.178 --> 00:15:16.516
in the string, length of the
substrings themselves, I define

00:15:16.516 --> 00:15:21.687
a randomness score, and that
becomes my touchstone for

00:15:21.687 --> 00:15:29.095
detecting randomness. One last
thing to mention here is that I

00:15:29.095 --> 00:15:34.767
do it twice. I run it, the
string, first against the

00:15:34.767 --> 00:15:40.807
English language only. So
English language is universal.

00:15:40.807 --> 00:15:44.577
Many people use it. So I first
run it against that. If,

00:15:44.577 --> 00:15:48.247
according to the English
language I don't have a verdict

00:15:48.247 --> 00:15:52.685
about randomness of the string,
next I go to all the languages.

00:15:52.685 --> 00:15:58.691
And this is basically a
two-phase filter. So a bunch of

00:16:04.530 --> 00:16:11.637
other considerations here. If
you have a sequence of

00:16:11.637 --> 00:16:15.842
alternating vowels and
consonants, if you practice this

00:16:15.842 --> 00:16:21.547
yourself, put a sequence of
random alternating vowels and

00:16:21.547 --> 00:16:27.553
consonants, for this, also
another thing I consider is if I

00:16:37.797 --> 00:16:43.803
see a dash or underscore, it
means that there is some natural

00:16:46.505 --> 00:16:53.646
separation at that point. So
because otherwise why would be

00:16:53.646 --> 00:16:58.851
an underscore or a dash in the
middle of a string. So based on

00:16:58.851 --> 00:17:03.623
that I treat that as a
separation and I look at each

00:17:03.623 --> 00:17:07.260
one of these separate pieces
separately. So I'm going to get

00:17:07.260 --> 00:17:10.863
into some results here. Based on
the experiments that we ran and

00:17:10.863 --> 00:17:16.869
I look at both false positive
and false negative rates. For

00:17:23.976 --> 00:17:29.982
this, let's first look at false
negative. So I'm using 9 domain

00:17:32.318 --> 00:17:38.324
generation algorithms. These are
from known malwares and bot nets

00:17:40.393 --> 00:17:45.765
and these are reverse engineered
by the Talos team. So they gave

00:17:45.765 --> 00:17:51.203
me the code and I generated
basically these samples. So this

00:17:51.203 --> 00:17:53.506
is the number of samples that
comes from each one of these

00:17:53.506 --> 00:18:00.746
algorithms. And then this is the
number of strings that were

00:18:00.746 --> 00:18:07.620
generated by this specific
algorithm that were missed by my

00:18:07.620 --> 00:18:12.758
randomness detection. So if you
look at the missed percentages,

00:18:12.758 --> 00:18:19.498
I don't know how clear it is on
the screen, but basically the

00:18:19.498 --> 00:18:24.070
rate is pretty low. Across the
board it's probably around 1%,

00:18:24.070 --> 00:18:26.405
less than 1% false negative
rate. So how about false

00:18:26.405 --> 00:18:32.411
positive? For this I took Alexa
10 --> 000 domain names. I filtered

00:18:39.685 --> 00:18:46.158
out strings that are shorter
than 8 characters. And put them

00:18:46.158 --> 00:18:49.929
aside as I mentioned in the
first slide. So I'm left with

00:18:49.929 --> 00:18:55.935
5,400, almost 5,400 domain
names. And I run them through

00:18:58.904 --> 00:19:04.610
the code. So the rationale here
is that in the Alexa 10 --> 000,

00:19:04.610 --> 00:19:09.815
you're unlikely to see loss of
VGAs. So hopefully a lot of

00:19:09.815 --> 00:19:15.821
those are legitimate websites.
Out of this 5,400 domains that I

00:19:19.892 --> 00:19:25.464
checked, 412 of them were
depicted by my algorithm as

00:19:25.464 --> 00:19:32.104
being random. And this is the
whole list of them. Some of

00:19:32.104 --> 00:19:35.908
them, like for instance this
one. This one, if you showed it

00:19:35.908 --> 00:19:41.514
to me I would say this is a
random string. Or I don't know,

00:19:41.514 --> 00:19:45.918
like this one. This is pretty
random looking to me. But some

00:19:45.918 --> 00:19:49.455
other ones are not random. Like,
for instance, as a matter of

00:19:49.455 --> 00:19:54.326
fact I know this is a pretty
legitimate Turkish website, or

00:19:54.326 --> 00:20:00.332
this one is a legitimate Farsi
website. So but at the end, out

00:20:02.702 --> 00:20:08.708
of 5,400, we have 42. And we can
say that it's about 1% false

00:20:11.610 --> 00:20:18.484
positive. Which is not bad. So
if 1% for false positive, 1% for

00:20:18.484 --> 00:20:25.691
false negative, and overall I
think it's a good rate compared

00:20:25.691 --> 00:20:31.697
to other studies that I've seen
on this problem. And this would

00:20:31.697 --> 00:20:37.703
conclude my talk. Thank you very
much. (Applause.)

