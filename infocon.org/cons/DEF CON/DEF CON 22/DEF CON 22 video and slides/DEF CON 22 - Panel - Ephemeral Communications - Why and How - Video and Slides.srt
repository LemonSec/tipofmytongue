1
00:00:00,167 --> 00:00:06,167
I'd like to introduce Ryan
Lackey. Talking about Ephemeral
Communications: Why and how? >>

2
00:00:10,083 --> 00:00:16,083
Hi everyone I'm Ryan Lackey with
CloudFlare. And I'm pleased to
be back after a long time. I'm

3
00:00:19,958 --> 00:00:25,958
introducing my panel which is
Jon Callas with Silent Circle,
Elissa Shevinsky of Glimpse, and

4
00:00:34,125 --> 00:00:40,125
Nico Sel of Wickr. So we're
going to talk about Ephemeral
Communications Apps or

5
00:00:43,667 --> 00:00:48,792
applications that are short
lived, very short duration
storage and messages. The funny

6
00:00:48,792 --> 00:00:53,625
thing about this is it used to
be the default. It used to be
that storage was incredibly

7
00:00:53,625 --> 00:01:00,625
expensive. Disks were in the
multi‑tens of pounds, hundreds
of pounds range, big rack

8
00:01:00,625 --> 00:01:05,417
mounted devices. You were really
careful about what you saved,
you wouldn't just retain

9
00:01:05,417 --> 00:01:11,167
everything. And the fault was to
erase all of content.That got
solved I guess in the 80s and

10
00:01:11,167 --> 00:01:14,833
90s to the point where it wasn't
about deleting text information
and it got to the point where

11
00:01:14,833 --> 00:01:19,500
video and everything else was
sort of persistent. Then there
was too much and you had all

12
00:01:19,500 --> 00:01:25,375
this content and you could never
find anything. So until pretty
much search engines, web,

13
00:01:25,375 --> 00:01:29,292
ultavista,Google came about,
even if you had all this
information it would sort of

14
00:01:29,292 --> 00:01:34,333
disappear and you could pretty
much hope if the information
existed about you anywhere

15
00:01:34,333 --> 00:01:37,083
people wouldn't find it. At
least it wouldn't come
immediately to mind when you

16
00:01:37,083 --> 00:01:41,208
typed something into a web
browser. There's another
advantage that pretty much

17
00:01:41,208 --> 00:01:45,167
everybody using the internet
like a professional user,
academic people like that. That

18
00:01:45,167 --> 00:01:52,667
was actually true even through a
lot of the early two thousands.
You could assume people were

19
00:01:52,667 --> 00:01:56,917
using the internet for specific
purposes. Either finding
information or using it to do

20
00:01:56,917 --> 00:02:02,500
various things, but they weren't
using it every second of their
lives. But something changed.

21
00:02:02,500 --> 00:02:05,417
That's pretty much what
happened. Now everybody carries
around a super computer in their

22
00:02:05,417 --> 00:02:09,292
pocket constantly connected,
constantly storing data, and
using it to talk to everybody

23
00:02:09,292 --> 00:02:15,292
all the time. It makes it easy
to store everything, store
everything by default through

24
00:02:17,458 --> 00:02:25,208
Facebook, Twitter, all your
history, email lives everywhere.
It's crazy. And some people

25
00:02:25,208 --> 00:02:30,583
started to notice this a little
while ago. Fred Wilson from USC
had a great quote. Pretty much

26
00:02:30,583 --> 00:02:36,583
privacy by being the default was
the State of nature by the
internet then it went away due

27
00:02:39,958 --> 00:02:44,042
to technological change. Then
people started to think what
would it be like to go back to

28
00:02:44,042 --> 00:02:48,417
the day when everything wasn't
logged. A new generation of
users every day. There's always

29
00:02:48,417 --> 00:02:51,542
new people using the internet
and they have a different
perception of what things should

30
00:02:51,542 --> 00:02:57,542
be. So I'm going to let the
panel get started on what they
do and come back to this stuff

31
00:03:03,250 --> 00:03:08,625
later. Maybe start with Jon with
Silent Circle and let him
describe his application. >>

32
00:03:08,625 --> 00:03:15,375
Sure. Thank you very much, Ryan.
You can see up there is a screen
shot of how things work on

33
00:03:15,375 --> 00:03:21,375
Android and IOS. We started
building the chat system that
does silent text. As we were

34
00:03:24,417 --> 00:03:30,417
building it and already thinking
about this sort of thing, people
that we've been talking to had

35
00:03:32,792 --> 00:03:39,542
requests. One of the things I
thought was compelling was that
someone said that they had been

36
00:03:39,542 --> 00:03:45,917
texting with a co‑worker stuff
about work stuff. And the
co‑worker says can you come over

37
00:03:45,917 --> 00:03:50,708
to my office and talk about
something. So they come over to
the office and the co‑worker's

38
00:03:50,708 --> 00:03:56,708
phone is sitting on their desk
unlocked, texts up with someone
else. And the person said, so I

39
00:04:01,000 --> 00:04:07,000
can't really trust this
co‑worker with the rude comments
I might make about someone else.

40
00:04:07,000 --> 00:04:14,417
Because, you know, we all say
things that are intemperate. And
texting makes it really easy to

41
00:04:14,417 --> 00:04:20,833
sound a little more emotional
than you are. And, you know, to
have the tongue in cheek there

42
00:04:20,833 --> 00:04:28,667
and just enough Emojis to get
the right Emogian down. The
request was can you make these

43
00:04:28,667 --> 00:04:34,375
things go away. That's how we
got to burn notice. We said
yeah. Data destruction policy.

44
00:04:34,375 --> 00:04:39,708
There's no problem with data
destruction. It's one of those
things that is enshrined in WA

45
00:04:39,708 --> 00:04:45,708
and you make it so when you text
there is a default delete on it.
And there is, for example, a guy

46
00:04:48,333 --> 00:04:54,250
that I do an awful lot of
texting with who just started
texting me out of the blue. And

47
00:04:54,250 --> 00:05:01,333
I keep, like, a one‑day burn on
everything. So that, you know,
we talk about security and this,

48
00:05:01,333 --> 00:05:09,208
that and the other thing, and I
know that in general things will
get deleted after a day. >> So,

49
00:05:09,208 --> 00:05:15,875
maybe next, Nico can talk about
Wicker. >> Sure. So I like that
you brought up Snap Chat. A year

50
00:05:15,875 --> 00:05:23,500
ago at this conference Stros
Freeburg did a great assessment
of Snap Chat and Facebook,

51
00:05:23,500 --> 00:05:30,083
"poke". At that time, you could
Google and find Snap Chat's
masterkey online. Facebook

52
00:05:30,083 --> 00:05:36,750
didn't have any security on
there at all. Just looked like
they deleted it, I guess, and

53
00:05:36,750 --> 00:05:42,750
they left the stage saying
indeed Wickr leaves no trace. We
were really proud about that. We

54
00:05:45,375 --> 00:05:53,375
were founded before Snap Chat
had launched, but we came about
it from a really different

55
00:05:53,375 --> 00:05:59,375
place. And we essentially said
hey we've made this seamless
security protocol and we think

56
00:06:02,375 --> 00:06:07,458
that everyone in the world
should be using encryption. And
so how do we get everyone to use

57
00:06:07,458 --> 00:06:11,667
encryption? Let's give them
something that they've seen in
the movies, right. Everyone's

58
00:06:11,667 --> 00:06:17,917
wanted self‑destructing
messages. So that's how we got
to that equation. Wickr is the

59
00:06:17,917 --> 00:06:23,042
fuse from Mission Impossible and
that's where it came from. I
think its important to remember

60
00:06:23,042 --> 00:06:30,667
here that the use case here is
spy to spy. If you are sending a
message spy to an enemy, I don't

61
00:06:30,667 --> 00:06:35,083
care how good the tech is it's
not going to disappear. That's
kind of what's happened with

62
00:06:35,083 --> 00:06:40,292
Snap Chat. There's a lot of
education that needs to be done
that this is about trusted

63
00:06:40,292 --> 00:06:48,125
parties. Otherwise it's DRM. We
all know how that is. It's more
about betrayal generally happens

64
00:06:48,125 --> 00:06:53,500
after the fact and more just
about cleaning up your messages
on someone else's phone is

65
00:06:53,500 --> 00:06:59,500
really how I think about it. >>
Great. Maybe now Elissa will
talk about Glimpse. >> Glimpse

66
00:07:01,875 --> 00:07:09,625
was founded when Pat Stickinson
and I really just fell in love
with the idea of Ephemeral

67
00:07:09,625 --> 00:07:15,875
Messaging. We thought that it
was really fun and we also
really believed in the threat

68
00:07:15,875 --> 00:07:21,042
model of, like, people you trust
now. You know, like you trust
people now but you don't

69
00:07:21,042 --> 00:07:27,083
necessarily trust them in three
months or in six months. You
want things to disappear. But we

70
00:07:27,083 --> 00:07:33,083
didn't trust Snap Chat. And that
was proven out later with the
hacks and the various, you know,

71
00:07:35,750 --> 00:07:41,750
legal actions that have been
taken against them. So they
built Glimpse to make really

72
00:07:41,750 --> 00:07:47,750
easy to use privacy. And we did
it keeping in mind that Silent
Circle and Wicker were already

73
00:07:50,000 --> 00:07:56,792
in the market doing really,
really solid work on encryption.
Jon actually was kind enough to

74
00:07:56,792 --> 00:08:01,583
take time and meet with me
before we launched. Helped us
craft our privacy policy and

75
00:08:01,583 --> 00:08:06,042
were using Wicker to chat when
along with Silent text and we
were really just getting the

76
00:08:06,042 --> 00:08:11,458
company started. We wanted to be
in a really different place in
the market that wasn't directly

77
00:08:11,458 --> 00:08:17,667
competitive with them and was
more competitive with the
classic social media. So we're

78
00:08:17,667 --> 00:08:23,667
really, really proud to be I
think one of the only encrypted
networks primarily used by high

79
00:08:25,875 --> 00:08:31,000
school and sorority students.
And they don't necessarily care
that we're encrypted but they

80
00:08:31,000 --> 00:08:35,667
care we're private in the ways
that are meaningful to them.
Their model is they're hiding

81
00:08:35,667 --> 00:08:42,417
from their parents and teacher
and from drama on Facebook. So
we really work closely with high

82
00:08:42,417 --> 00:08:47,708
schoolers and with people in
sororities and other very
tight‑knit college groups to

83
00:08:47,708 --> 00:08:53,167
figure out features they really
love. So we have upload from
text which you can't do, I mean,

84
00:08:53,167 --> 00:08:59,875
upload from camera which you
can't do on Snap Chat. Like if I
want to send a cute picture of a

85
00:08:59,875 --> 00:09:06,042
bunny with his face in a tiny
little shopping cart full of
baby carrots I can't do that on

86
00:09:06,042 --> 00:09:10,583
Snap Chat unless I have the
bunny in front of me. But with
Glimpse I have send photos of

87
00:09:10,583 --> 00:09:17,000
bunnies and ghosts and all
things that are fun to me in an
ephemeral way. So that's the

88
00:09:17,000 --> 00:09:21,792
been the engine that's really
helped us take off with young
people. They have a really low

89
00:09:21,792 --> 00:09:26,667
switching cost. And something as
simple as being able to upload a
photo from camera and write a

90
00:09:26,667 --> 00:09:32,250
full screen of text has been
enough to get them to start
using glimpse. What's up next

91
00:09:32,250 --> 00:09:37,667
will be group messaging. So,
like, really, really easy to use
encrypted messaging. The one

92
00:09:37,667 --> 00:09:43,208
other thing that is really worth
mentioning, as you situate
Glimpse in contrast to Silent

93
00:09:43,208 --> 00:09:50,583
Circle and Wicker they are like
spy to spy. They really are
encrypted in this unique and

94
00:09:50,583 --> 00:09:57,792
special way. With Glimpse we
make security compromises they
wouldn't make. So we're able to

95
00:09:57,792 --> 00:10:03,792
have usability that wouldn't be
present otherwise. So we don't
encrypt your social graph. It's

96
00:10:03,792 --> 00:10:08,875
not a secret on Glimpse who you
are talking to. It is a secret
the horrible thing that you have

97
00:10:08,875 --> 00:10:14,958
said. (Laughter). >> And that's
exactly right because we don't
know who our users are. We have

98
00:10:14,958 --> 00:10:21,875
no idea. You can imagine trying
to talk to investors and giving
them to give you money and

99
00:10:21,875 --> 00:10:26,833
they're, like, tell me about
your users. >> Similarly we
don't require contact

100
00:10:26,833 --> 00:10:31,417
information. Which is one of the
interesting things we've talked
about Snap Chat a lot. I tried

101
00:10:31,417 --> 00:10:35,083
to get them to come. They
weren't willing to show up. They
don't really have a long history

102
00:10:35,083 --> 00:10:40,667
of engaging with the security
community. But they've done a
lot of great things. You can

103
00:10:40,667 --> 00:10:44,667
attack them for maybe making
some mistakes in how they
implemented things but they

104
00:10:44,667 --> 00:10:50,833
really did move the industry
from log everything again like
the mid‑two thousands to

105
00:10:50,833 --> 00:10:56,292
creating this new sort of
concept or going back to very
old concept of being able to

106
00:10:56,292 --> 00:11:01,333
chat with people directly. For
the users they have, which is
sort of a weird bi-model

107
00:11:01,333 --> 00:11:06,792
distribution of kids using it
for evading their parents then
people who should know better

108
00:11:06,792 --> 00:11:12,917
using it for things they
shouldn't really be doing, it
mostly meets with the first

109
00:11:12,917 --> 00:11:17,250
threat model. Maybe not so much
with the second. So you can't
pick any of these tools and say

110
00:11:17,250 --> 00:11:21,417
they're perfect for everybody.
You have to know what your prep
model is. And maybe you are

111
00:11:21,417 --> 00:11:26,000
willing to make sacrifices on
security for convenience on
certain applications. People

112
00:11:26,000 --> 00:11:31,708
don't have 50 character pass
code to unlock their phone every
time. People. Maybe I do but

113
00:11:31,708 --> 00:11:33,708
maybe other people don't. >>
What I think is really
interesting here too about it is

114
00:11:33,708 --> 00:11:36,750
the kind of minimal viable
product that's happening right
now. And with Yo for instance

115
00:11:36,750 --> 00:11:41,792
and Snap Chat and some of the
others coming out, is I wonder
if you can survive doing minimal

116
00:11:41,792 --> 00:11:47,792
viable product when you are
hosting massive amounts of very
sensitive customer data. Will be

117
00:11:55,333 --> 00:12:01,750
interesting to see as it goes on
taking a longer term view here
is you haven't ‑‑ to see who

118
00:12:01,750 --> 00:12:06,250
wins in the long run. But I
think we'll be left standing. I
want to know if others will. >>

119
00:12:06,250 --> 00:12:11,792
I don't think it's a minimal
viable product issue with Snap
Chat or Yo. The moment where

120
00:12:11,792 --> 00:12:19,125
Snap Chat had that massive hack
this past New Year's they had
enough money and time because

121
00:12:19,125 --> 00:12:23,042
the hackers had given them
plenty of notice to have solved
that. And they simply didn't

122
00:12:23,042 --> 00:12:30,542
because it wasn't important
enough to them. And with Yo
people think that Yo is just a

123
00:12:30,542 --> 00:12:38,458
really small app but actually Yo
had gotten in front of Robert
Scobal well before it got in the

124
00:12:38,458 --> 00:12:44,458
mass media attention. And as
anyone who has used Yo knows, Yo
was subject to a hack where all

125
00:12:46,667 --> 00:12:52,333
of your cell phone data ‑‑ it
was subject to really awful
hacks. I can't figure out how to

126
00:12:52,333 --> 00:12:56,833
delete Yo from my phone like in
a meaningful way, not just
delete the app but, like, to

127
00:12:56,833 --> 00:13:01,750
where my account is gone. This
isn't because Yo isn't well
funded at this point. They

128
00:13:01,750 --> 00:13:06,000
raised a million dollars. This
isn't because it isn't starting
to mature as a company. It's

129
00:13:06,000 --> 00:13:13,042
because they really don't care.
The investors, the founders
don't care. And, you know, where

130
00:13:13,042 --> 00:13:18,167
does that leave us in the
industry? >> I think that's an
important thing. Because most of

131
00:13:18,167 --> 00:13:23,458
the mistakes that we have seen
have been they don't care
mistakes. They didn't delete the

132
00:13:23,458 --> 00:13:27,125
files. They didn't ‑‑ they
renamed the files they didn't
actually delete them. You got

133
00:13:27,125 --> 00:13:34,042
told about a problem on your
network and you didn't do
anything about it. It is a

134
00:13:34,042 --> 00:13:40,042
negligence thing not even a
beginner slash competence thing.
>> I think glimpse is a really

135
00:13:44,125 --> 00:13:49,625
good example of what Snap Chat
could have done if they cared.
We had a much smaller team.

136
00:13:49,625 --> 00:13:56,792
We've only raised $200,000 to
date and we've had, like, four
of us, three developers and me.

137
00:13:56,792 --> 00:14:01,125
And we built something very
similar to Snap Chat but not
subject to any hacks they're

138
00:14:01,125 --> 00:14:07,042
subject to. And it didn't take
us all that long. Like, it took
us about two to three weeks to

139
00:14:07,042 --> 00:14:11,958
really securely build in the
system that uploads your
contacts, you know, and helps

140
00:14:11,958 --> 00:14:17,542
you find your friends through
contact. And we use rate
limiting and also enable you to

141
00:14:17,542 --> 00:14:23,167
be hidden. It's that simple.
Snap Chat would have just
improve their rate limiting and

142
00:14:23,167 --> 00:14:27,625
allow users to hide themselves
then Mark Zuckerburg wouldn't
have had his phone number

143
00:14:27,625 --> 00:14:33,042
leaked. >> It might be
interesting to bring up some of
the other ways to compare the

144
00:14:33,042 --> 00:14:38,417
applications. Security is one
and competence and negligence
but there's also how you use the

145
00:14:38,417 --> 00:14:44,208
things. The other applications
like Secret, Whisper, Tech
Secure, I mean, we can talk

146
00:14:44,208 --> 00:14:48,958
about how those applications fit
into some sort of category
system in what you used

147
00:14:48,958 --> 00:14:53,750
different ones for and what the
characteristics of various
applications. >> As a mom I

148
00:14:53,750 --> 00:15:00,583
worry about secret and whisper.
The whisper that's one of the
top in the app store right now.

149
00:15:00,583 --> 00:15:06,375
Because kids think that those
are anonymous. And my daughter
knows how to tell who secrets

150
00:15:06,375 --> 00:15:12,500
those are. >> Which I think is
perhaps part of the learning
experience. They aren't

151
00:15:12,500 --> 00:15:17,542
anonymous apps. I use both
secret and whisper and I like
them both. They have different

152
00:15:17,542 --> 00:15:24,208
feels to them but they're
un‑attributed. They aren't
anonymous at all. And in both

153
00:15:24,208 --> 00:15:31,583
cases, part of the fun is
guessing who it is. >> Do you
think they planned it that way?

154
00:15:31,583 --> 00:15:36,125
>> Yeah I believe they planned
it that way. Absolutely. >>
Yeah? Whisper was started as a

155
00:15:36,125 --> 00:15:42,125
suicide help. >> Yeah. >>
There's something interesting of
course how people behave when

156
00:15:47,083 --> 00:15:53,083
they have anonymity as we've
seen. If you go to any internet
forum where stuff is essentially

157
00:15:56,042 --> 00:16:00,750
anonymous or at least
un‑attributed you see different
emerging behaviors. You can have

158
00:16:00,750 --> 00:16:07,333
some where people very civil
conversations and just sort of
the setting and the context.

159
00:16:07,333 --> 00:16:13,250
It's not ‑‑ it might be informed
by the level of anonymity and
the kind of technology but it

160
00:16:13,250 --> 00:16:18,625
seems to be a community driven
thing as much as anything else.
>> That's part of what all of

161
00:16:18,625 --> 00:16:24,875
these have to do is figure out
what sort of community they
want. When I first started doing

162
00:16:24,875 --> 00:16:29,958
collaboration apps, I spent a
lot of time talking to Randy
Farmer who is definitely one of

163
00:16:29,958 --> 00:16:36,708
the real pioneers in this and
knows more about communities
than anyone else. It's

164
00:16:36,708 --> 00:16:44,208
interesting on Secret they've
now limited things so that only
friends of friends can comment

165
00:16:44,208 --> 00:16:51,625
on a thread. Anybody can see it,
but you have to be one hop or
two hops to be able to comment.

166
00:16:51,625 --> 00:16:57,625
And they say in the little box
that you type in "please be
civil." So they are limiting the

167
00:17:00,083 --> 00:17:07,583
amount of trolling that can go
on. And creating a social
environment where it's highly

168
00:17:07,583 --> 00:17:13,583
likely that the person you are
commenting on is someone you
know. >> I think the community

169
00:17:13,583 --> 00:17:19,458
that you are going after is
really important and really what
should define us. And I'm

170
00:17:19,458 --> 00:17:24,333
founder of Roots, that's at
DEF CON here as well and work a
lot with kids all the time and

171
00:17:24,333 --> 00:17:31,875
that was really one of the
reasons that we founded Wicker.
Was I felt like my daughters

172
00:17:31,875 --> 00:17:38,500
deserve the same level of
encryption that the spies were
using. Because in my opinion

173
00:17:38,500 --> 00:17:44,250
data brokers are way greater
threat to my friends and family
than Snowden. You've got the

174
00:17:44,250 --> 00:17:48,542
worst data breakers out there
like Experion that are selling
lists of rape victims and

175
00:17:48,542 --> 00:17:55,250
erectile sufferers and dementia
sufferers for seven cents a
piece. There's thousands and

176
00:17:55,250 --> 00:18:02,167
thousands of those out there.
And with our apps we've now, you
know, my crypto team was killing

177
00:18:02,167 --> 00:18:10,042
me because we made them add
graffiti, and there are stickers
in there. Why are we doing this?

178
00:18:10,042 --> 00:18:15,875
Because my daughters say it's
really important. Now I use all
those features myself every day.

179
00:18:15,875 --> 00:18:20,667
There's really something to it.
It's part of what Snap Chat
really did show is it is a

180
00:18:20,667 --> 00:18:25,458
new kind of messaging and
picturing and writing on
pictures is definitely a new way

181
00:18:25,458 --> 00:18:32,542
of communication that I've just
learned myself. >> We'd be
interested in bringing in the

182
00:18:32,542 --> 00:18:40,500
audience if you have any
questions for the panelists.
Things we can discuss. We've got

183
00:18:40,500 --> 00:18:46,500
more things to discuss so just
as you are getting ready. >> We
can chat all day. >> Yes. >> Go

184
00:18:49,583 --> 00:18:54,625
ahead. >> Sort of a two‑part
question having to do with not
knowing who your customers are.

185
00:18:54,625 --> 00:19:01,250
What happens when somebody who
has perhaps used the service for
a while decides or, you know,

186
00:19:01,250 --> 00:19:07,583
they lose their credentials and
they say, well, you know, I am
really unhappy if you can't

187
00:19:07,583 --> 00:19:11,667
reset my account. And you don't
have an email address or
anything like that. How do you

188
00:19:11,667 --> 00:19:18,583
handle that? Also, the question
of do you really not know who it
is when you have information

189
00:19:18,583 --> 00:19:25,917
about who their ‑‑ I haven't
used your app so I don't know,
but I presume there's some sort

190
00:19:25,917 --> 00:19:31,917
of social graph there where you
kind of know who these people
have in their address books, if

191
00:19:34,167 --> 00:19:39,417
you will, in your app. >> If
nothing else you have the IP
connection. >> Well maybe you

192
00:19:39,417 --> 00:19:45,458
don't have ‑‑ hopefully you
don't have the history with it
being an ephemeral app. >> Yes.

193
00:19:45,458 --> 00:19:52,667
>> I mean, in our case, what we
want to do is to permit you to
have as much privacy as you are

194
00:19:52,667 --> 00:19:58,667
comfortable with. It's something
that we have worked out and
tested that you could go get a

195
00:20:02,500 --> 00:20:08,500
gift card from a department
store. You know, a $100 gift
card will pay for one year of

196
00:20:11,875 --> 00:20:16,083
the account and we'll accept
that. And we test that to make
sure it continues to work. So

197
00:20:16,083 --> 00:20:23,417
you can create an amount of
unattributedness that you want.
We don't keep server logs on

198
00:20:23,417 --> 00:20:29,417
those things. That's one of the
things that is a commitment for
us. Particularly because I

199
00:20:29,417 --> 00:20:34,917
believe on the privacy end of
things, the less that you as a
service are keeping, the more

200
00:20:34,917 --> 00:20:40,042
that you will get dragged in to
fights that you really don't
want to be dragged in to. So

201
00:20:40,042 --> 00:20:46,458
your defense in these things is
to make sure that everything is
on the end points and you don't

202
00:20:46,458 --> 00:20:53,833
have it. Creating something
where people opt in is
different. If you create an

203
00:20:53,833 --> 00:20:58,750
account and you don't give me a
contact email address and don't
do anything and you lose your

204
00:20:58,750 --> 00:21:04,083
password, I'm terribly sorry,
you're stuck. Because you could
have done things to give

205
00:21:04,083 --> 00:21:09,458
yourself a safety line and you
chose not to. That's part of the
service that I am offering. Is

206
00:21:09,458 --> 00:21:15,458
that I am giving you a coil of
rope and you can tie a noose
with it if you really want to.

207
00:21:15,458 --> 00:21:20,083
>> I think that's right. I mean,
users don't want us to give
their accounts to other users.

208
00:21:20,083 --> 00:21:25,500
So we have to take, you know,
appropriate actions around that.
With glimpse, if you want to set

209
00:21:25,500 --> 00:21:31,167
up an account on a phone, a
second phone, then we're only
one phone per account right now

210
00:21:31,167 --> 00:21:36,375
because of the way our
encryption works. Then we say
you can log in to this device

211
00:21:36,375 --> 00:21:42,208
and you will lose everything
from your old account or you can
create a new account. And I

212
00:21:42,208 --> 00:21:47,083
think that's important if people
care about privacy they
understand that comes with

213
00:21:47,083 --> 00:21:52,500
certain usability issues around
things like making accounts too
easy to fake or too easy to

214
00:21:52,500 --> 00:21:57,208
recover. >> I would say this is
one of the beautiful things
about having an ephemeral

215
00:21:57,208 --> 00:22:02,792
messaging app. We don't have a
password reset. That would be
absolutely unacceptable on

216
00:22:02,792 --> 00:22:08,667
E‑mail I think. That wouldn't
work. But there's no password
reset right now and what do we

217
00:22:08,667 --> 00:22:13,375
tell people? You might have lost
five days of messaging but just
get a new account and start

218
00:22:13,375 --> 00:22:18,667
over. There's a password reset,
that means someone has your
password. If someone has your

219
00:22:18,667 --> 00:22:24,917
password, then bad guys and the
FBI and lots of people can get
your password. If we would have

220
00:22:24,917 --> 00:22:29,292
had passwords when the FBI came
to me and asked me for a back
door I wouldn't have been able

221
00:22:29,292 --> 00:22:35,292
to successfully say no. So as we
move to the future and we'll
have messages that live forever,

222
00:22:35,292 --> 00:22:41,250
we're really looking at
solutions there and how do we do
a password reset and things like

223
00:22:41,250 --> 00:22:46,208
giving people USB drive with
their password on it. Legally
speaking, if someone else has

224
00:22:46,208 --> 00:22:51,167
your password they are not
legally protected with it. They
must give it up. That's

225
00:22:51,167 --> 00:22:55,375
something that's really
important to think about as a
legal ramifications here. >>

226
00:22:55,375 --> 00:23:01,708
Also something with the way that
we built our system. It is a
zero knowledge system. So it

227
00:23:01,708 --> 00:23:07,042
took us a long time to figure
out how to connect people
together without us knowing who

228
00:23:07,042 --> 00:23:13,417
you are. And I think my tech
team came one a brilliant
solution. Instead of viral

229
00:23:13,417 --> 00:23:21,375
growth I say we've invented
bacterial growth. So more like
yogurt than a disease but it's

230
00:23:21,375 --> 00:23:25,417
beneficial to society. What we
do is if you allow us to, what
we'll do is do a crypto graphic

231
00:23:25,417 --> 00:23:31,917
hash of your address book to
send up to our servers. If
someone has you in their contact

232
00:23:31,917 --> 00:23:38,875
book we'll match the two of you
if you have opted in but we
still have no idea who you are

233
00:23:38,875 --> 00:23:43,333
and who is in your servers.
Growth is a little slower this
way because we're not forcing

234
00:23:43,333 --> 00:23:47,750
everyone into our system and
automatically uploading
everyone's contact. So one of

235
00:23:47,750 --> 00:23:54,208
the biggest complaints I get is
I can't find my friends. But we
think it is a better way to do

236
00:23:54,208 --> 00:23:58,500
it but definitely makes things
tricky. >> We do something
similar at glimpse we hash the

237
00:23:58,500 --> 00:24:06,167
user IDs. We do it for Facebook,
Twitter and for contacts. We do
the same thing where if both

238
00:24:06,167 --> 00:24:12,750
parties have opted in and we can
match you. But, again, we're
really different from Wicker and

239
00:24:12,750 --> 00:24:20,208
Silent Circle because we really
do have the social graph. And
that lets us be a lot faster and

240
00:24:20,208 --> 00:24:26,083
do a lot of things we otherwise
couldn't do. Like sending
messages is faster. Again,

241
00:24:26,083 --> 00:24:32,542
because on our app we're not
hiding the social graph. I
understand that metadata can be

242
00:24:32,542 --> 00:24:37,250
really useful and important.
It's not to say that metadata
doesn't matter. But it is to say

243
00:24:37,250 --> 00:24:43,250
that there has to be some kind
of privacy space and privacy app
in between Facebook and Snap

244
00:24:45,833 --> 00:24:51,958
Chat and in between apps
designed for spies. >> I don't
know PC magazine just did an

245
00:24:51,958 --> 00:24:57,958
extensive review of all the top
messages in the world and we
beat What's App and Snap Chat on

246
00:25:00,375 --> 00:25:04,417
features alone because we have
more filters, stickers I don't
know why, but maybe it's because

247
00:25:04,417 --> 00:25:08,083
we have more filters, stickers
and doodles than they do.
(Laughter). >> I don't think it

248
00:25:08,083 --> 00:25:12,917
works that way. It doesn't work
that way. I mean, you look at Yo
and they went to a viral and

249
00:25:12,917 --> 00:25:17,500
people loved it. It was just one
click. I used Wicker and I like
Wicker. And I respect you a

250
00:25:17,500 --> 00:25:22,625
tremendous amount. But users
want things that are easy and
simple and if adding more

251
00:25:22,625 --> 00:25:26,667
features was enough to make
users more happy, then, like,
Facebook and Snap Chat and all

252
00:25:26,667 --> 00:25:30,792
those companies that have tons
of developers, and money and
research would just be more

253
00:25:30,792 --> 00:25:37,667
featureish. It's not that
simple. >> No it's just that
we're a text messaging app. And

254
00:25:37,667 --> 00:25:43,250
by the way we're secure and
respect your data and don't
abuse or lose your personal

255
00:25:43,250 --> 00:25:49,250
information. >> Okay. Let's move
on to the next question. >>
Yeah. >> So this is I guess a

256
00:25:49,250 --> 00:25:54,792
different type of technical
question. You mentioned Snap
Chat and these other companies

257
00:25:54,792 --> 00:26:01,125
not fixing these potential leaks
because they were negligent. But
what if it isn't negligent but

258
00:26:01,125 --> 00:26:08,250
rather an economic misalignment?
They don't have the, sort of
impetus to actually fix this.

259
00:26:08,250 --> 00:26:14,250
How does your business model,
your economic model create the
drive to make sure that you

260
00:26:17,500 --> 00:26:23,500
maintain your security? >> Well
one answer is three letters FTC.
>> I think that's a really hard

261
00:26:30,125 --> 00:26:35,833
problem right. Like companies
aren't properly incentivized to
do security because it's cheaper

262
00:26:35,833 --> 00:26:41,375
to just apologize later and
because consumers aren't
necessarily making choices based

263
00:26:41,375 --> 00:26:46,875
on that. So I think you need to
be really initiative driven.
It's really obviously Silent

264
00:26:46,875 --> 00:26:53,042
Circle and Wicker are mission
driven. Glimpse is too. We find
ways to make the business models

265
00:26:53,042 --> 00:26:59,042
work really well. My perspective
is this is obviously a problem.
I'm going to are a long term

266
00:27:02,500 --> 00:27:07,708
business model instead of a
short‑term model. Here is one of
the tricky things. We've got a

267
00:27:07,708 --> 00:27:12,625
million dollar budget we spend
on hackers. And the largest bug
bounty in the world. I'm going

268
00:27:12,625 --> 00:27:18,750
to do hundreds of thousands of
dollars of pin tests constantly
while companies like frankly

269
00:27:18,750 --> 00:27:24,583
that compete with me instead
take that million dollars and
buy fake user downloads and look

270
00:27:24,583 --> 00:27:29,625
like they have a lot more users
and it pushes up the rankings
and that gives them more users.

271
00:27:29,625 --> 00:27:35,750
So it will be interesting to see
in the long run which of those
wins. But yeah there's a couple

272
00:27:35,750 --> 00:27:41,083
of different ways of doing it.
>> Okay. Next question. >> This
has to do with law enforcement

273
00:27:41,083 --> 00:27:47,917
security and privacy against
those people. When I created the
idea for the warrant canary I

274
00:27:47,917 --> 00:27:53,083
was hoping that it would find
kind of broad acceptance
especially by internet service

275
00:27:53,083 --> 00:27:58,000
providers, ephemeral, you know,
service providers, and that
hasn't happened. It especially

276
00:27:58,000 --> 00:28:02,333
hasn't happened even in the
companies that have adopted it
in the sense that they've only

277
00:28:02,333 --> 00:28:07,667
made sort of a broad blanket use
of the warrant canary if they've
received a warrant. Where my

278
00:28:07,667 --> 00:28:14,333
original idea was it could be
individually targeted so people
could inquire using the canary

279
00:28:14,333 --> 00:28:20,208
whether or not they were the
target of a particular warrant
or national security letter. So

280
00:28:20,208 --> 00:28:26,208
my question is why hasn't in
your opinion on the group really
found a home or use? >> I have

281
00:28:30,625 --> 00:28:36,625
no idea. >> I didn't hear it
either. >> The warrant canary
system where you post a posting

282
00:28:39,500 --> 00:28:44,417
on your website saying I've
never received an NSL >> We're
actually the first company in

283
00:28:44,417 --> 00:28:48,917
the world to do that. It was a
really stressful day. This is
when a lot Lavabit was having

284
00:28:48,917 --> 00:28:54,625
its issues. We were wondering
what to do. What if we get
served with a national security

285
00:28:54,625 --> 00:28:59,417
letter? Well we've got a zero
knowledge system to nothing
would really happen. We said

286
00:28:59,417 --> 00:29:05,417
let's go ahead and put in a
transparency report that we've
not received a national security

287
00:29:05,417 --> 00:29:11,042
letter and any other secret
orders and we do not have a back
door. Since then, Apple followed

288
00:29:11,042 --> 00:29:15,833
us and now there's a 11 other
companies they have identified
that have done this. It's

289
00:29:15,833 --> 00:29:21,083
unprecedented ground. Who knows
if it will work or not. But it
came from a really great place

290
00:29:21,083 --> 00:29:26,958
which was librarians in the
1970s. The FBI was going and
looking at book records to

291
00:29:26,958 --> 00:29:32,375
determine who was subversive and
who wasn't. Librarians take
privacy seriously. When you

292
00:29:32,375 --> 00:29:37,625
check out a book, when you are
done, they get rid of your
record. So what they did, they

293
00:29:37,625 --> 00:29:41,833
started hearing that the FBI was
doing this and all the
librarians that didn't have a

294
00:29:41,833 --> 00:29:47,458
gag order started putting a sign
outside their library that said
the FBI has not been here. And

295
00:29:47,458 --> 00:29:52,500
it seemed to work really well at
that point. Another part of that
actually is after I started

296
00:29:52,500 --> 00:29:58,583
Wicker my mom received notice
that the federal government had
accessed her library record. She

297
00:29:58,583 --> 00:30:05,292
called me up and said I don't
know, this is really strange and
I just got this letter and I

298
00:30:05,292 --> 00:30:12,542
wonder why that is. And talked
to librarians. They said if you
are checkbooks out online it is

299
00:30:12,542 --> 00:30:19,708
not ‑‑ it does not have the same
protection as if you are at the
library in person. Good thing to

300
00:30:19,708 --> 00:30:25,708
know. >> So there are a lot of
things you can do like warrant
canaries. The biggest thing you

301
00:30:28,875 --> 00:30:34,875
do is don't keep any data. Part
of that is also to make sure
that you have servers set up

302
00:30:41,375 --> 00:30:47,375
well. We have ours in Canada and
Switzerland intentionally
because of their history of

303
00:30:50,417 --> 00:30:56,708
respecting privacy and having
government officials who are
there to help you do this sort

304
00:30:56,708 --> 00:31:03,500
of thing. But having good
relationships with them where
you explain to them before they

305
00:31:03,500 --> 00:31:08,958
get upset about something that
this is what we do, this is the
service we're offering. This is

306
00:31:08,958 --> 00:31:14,250
the sort of data we have and no
we don't have that is one of the
ways that you don't get one of

307
00:31:14,250 --> 00:31:20,250
those is, you know, they don't
want to waste their time giving
you a subpoena for data that you

308
00:31:20,250 --> 00:31:26,708
don't have. So having a
relationship with people where
it's well‑known that you do

309
00:31:26,708 --> 00:31:33,125
things a certain way means that
you are extremely unlikely to
get a request for the data that

310
00:31:33,125 --> 00:31:36,917
you don't have. >> Exactly. And
actually one of the things I've
noticed with zero knowledge

311
00:31:36,917 --> 00:31:42,833
systems is the amount of
requests actually goes down over
time. So, if you are seeing

312
00:31:42,833 --> 00:31:48,125
transparency reports where the
requests are going up over time,
then that means they're giving

313
00:31:48,125 --> 00:31:52,000
the data. >> Right. >> Exactly.
>> I noticed when I ran an
anonymous emailer for a long

314
00:31:52,000 --> 00:31:56,875
time am I got a few contacts
from the secret service and FBI.
I told them what I had, exactly

315
00:31:56,875 --> 00:32:02,000
what I could do, what couldn't
be done technically and they
were pretty friendly and they

316
00:32:02,000 --> 00:32:06,167
stopped harassing me after that
point. It was pretty much the
right solution. So definitely

317
00:32:06,167 --> 00:32:13,875
spend your money on technical
solutions. >> And a lot of it's
very simple. If you put up a

318
00:32:13,875 --> 00:32:19,625
privacy policy that says, this
is my policy dealing with
disposal of logs, dealing with

319
00:32:19,625 --> 00:32:24,625
this, dealing with that, and you
can show that's what you have,
you have a contract with your

320
00:32:24,625 --> 00:32:28,292
customers that you are supposed
to do a certain thing.
Ironically the federal trade

321
00:32:28,292 --> 00:32:32,750
commission that we talked about
on the previous question would
punish you for handing things

322
00:32:32,750 --> 00:32:38,500
over or keeping those logs after
you said you weren't going to.
So this is also your best

323
00:32:38,500 --> 00:32:42,708
defense. You've made a
commitment to your customers
that you will behave in a

324
00:32:42,708 --> 00:32:50,292
certain way. And you are
contractually required to do
that. And that becomes your

325
00:32:50,292 --> 00:32:55,375
greatest defense. >> I think
that's right. I chatted with Jon
when we were setting up our

326
00:32:55,375 --> 00:33:01,542
privacy policy and it seems to
be the best defense of your
customers, right, if you have a

327
00:33:01,542 --> 00:33:06,375
privacy policy where you stated
your obligation to them and your
server architecture that

328
00:33:06,375 --> 00:33:12,750
supports that, then government
officials, you know, there's
only so much they can do. >> On

329
00:33:12,750 --> 00:33:18,625
the other end of it, Snap Chat,
Twitter, What's App, all of the
services, Skype, if you read

330
00:33:18,625 --> 00:33:23,083
their privacy policy pretty much
what you are granting is the
free transferrable worldwide

331
00:33:23,083 --> 00:33:28,125
license for eternity to your
contract. >> Snap Chat deletes
after 30 days and there's

332
00:33:28,125 --> 00:33:32,750
utility with that if they want
to deal with moderation issues.
I understand why they're doing

333
00:33:32,750 --> 00:33:37,750
30 days and not zero given,
like, their priorities and their
customer base. But, you know, if

334
00:33:37,750 --> 00:33:43,958
you are sending really sensitive
material 30 days is probably too
long for them to have your data.

335
00:33:43,958 --> 00:33:50,125
>> Maybe the next question. >>
Hi. So quick question.
Hopefully. Have you found any

336
00:33:50,125 --> 00:33:54,708
commonalities in user bases and
conversely have you found any
users that have surprised the

337
00:33:54,708 --> 00:34:00,250
hell out of you? >> Jon and I
don't know who our users are so
they're very common. >> I didn't

338
00:34:00,250 --> 00:34:04,250
hear the question. >>
Commonality in user base. I
mean, I know a lot of people who

339
00:34:04,250 --> 00:34:10,250
use both Silent Circle and
Wicker. I think we have a good
deal of overlap. But that comes

340
00:34:13,292 --> 00:34:20,708
from the people who are using
it. The real people who are
wanting the real anonymity are

341
00:34:20,708 --> 00:34:26,208
actually few and far between.
Remember these are communication
systems that you are using to

342
00:34:26,208 --> 00:34:31,583
talk to people that you know.
And no matter how you set it up
you are going to give yourself

343
00:34:31,583 --> 00:34:38,875
some sort of identifier, slash
handle and you are going to be
telling that, distributing it,

344
00:34:38,875 --> 00:34:46,167
whatever to people that you
know. If you go down the line,
Elissa is doing something that

345
00:34:46,167 --> 00:34:51,000
is very nice because she is
being upfront with her people
and saying this is the way we

346
00:34:51,000 --> 00:34:57,000
behave. We are making it so that
you and your friends can create
a social group where you can

347
00:34:59,250 --> 00:35:05,042
chatter among yourselves in a
way that is conducive to things
that you might do if you were in

348
00:35:05,042 --> 00:35:11,042
the same room. >> The other
direction to go would be
something like secure drop where

349
00:35:14,125 --> 00:35:19,833
you've got people submitting
anonymous content, focusing on
anonymity too. A third party.

350
00:35:19,833 --> 00:35:24,958
That's a totally separate kind
of system. >> With regard to
Snap Chat, there will always be

351
00:35:24,958 --> 00:35:31,125
the analog hole where people can
take a photograph of what is
being sent. Is there any

352
00:35:31,125 --> 00:35:35,958
technical solution for a screen
shot on the device preventing
those? >> I think your best

353
00:35:35,958 --> 00:35:41,792
solution is really a social one.
It's called plausible
deniability. Because a picture

354
00:35:41,792 --> 00:35:49,375
of a picture can always be
faked. >> That's true. >> I, you
know, there's no such thing as

355
00:35:49,375 --> 00:35:53,833
plausible deniability. There is
reasonable doubt. But there's no
such thing as plausible

356
00:35:53,833 --> 00:35:59,833
deniability. No. There is
nothing there. You have to have
a certain amount of trust of the

357
00:36:02,708 --> 00:36:10,333
person that you are sending
something to. Because most of us
now are carrying around four or

358
00:36:10,333 --> 00:36:17,500
five devices that's all have
cameras and they can be pointed
at each other. So really look at

359
00:36:17,500 --> 00:36:25,208
it like a data destruction
policy. Look at it like
something else. A constructed,

360
00:36:25,208 --> 00:36:31,125
fake looking message that
somebody made and lied about
that you had done might actually

361
00:36:31,125 --> 00:36:38,750
be more believable than the real
thing. >> I have a surprising
answer. Just that users don't

362
00:36:38,750 --> 00:36:45,708
care about screen shots. Glimpse
built a tool that worked really
effectively to prevent screen

363
00:36:45,708 --> 00:36:52,292
shots. It was an animation layer
that ran over your photo. So any
screen shot would capture a

364
00:36:52,292 --> 00:36:58,542
water mark on top of it. Because
of the way the frames work, if
you took a video of the

365
00:36:58,542 --> 00:37:05,792
animation it was really fuzzy.
Like the words were not readable
and the image became very hard

366
00:37:05,792 --> 00:37:11,375
to make out. But when you see
the image on your phone using
glimpse you would be like that's

367
00:37:11,375 --> 00:37:17,917
a naked person that's a really
interesting gossipy message. We
thought we just built like the

368
00:37:17,917 --> 00:37:22,583
world's greatest sexting app. We
were so proud. And we were
really surprised that discover

369
00:37:22,583 --> 00:37:27,250
that users didn't really want
this. It's interesting to talk
about screen shots.

370
00:37:27,250 --> 00:37:30,208
Intellectually interesting.
Because I'm not sure ‑‑ it's,
like, a feature people want to

371
00:37:30,208 --> 00:37:36,208
use day‑to‑day. >> We use to
have an anti‑tampering solution
that made it really difficult to

372
00:37:39,958 --> 00:37:45,958
take a screen shot because if
you moved the pixel it would go
away. And it worked really well.

373
00:37:48,792 --> 00:37:55,958
But it was a usability issue and
people didn't care. So we took
it out of there too. It wasn't

374
00:37:55,958 --> 00:38:00,000
that important it seemed. >> We
took out our screen shot
protection and no one noticed.

375
00:38:00,000 --> 00:38:06,625
>> Us too. No complaints. >>
Next question. >> I didn't know
you did that. That's cool to

376
00:38:06,625 --> 00:38:12,208
hear. >> So I'm actually not
familiar with the three
applications but from what I've

377
00:38:12,208 --> 00:38:17,000
gathered from the conversation
it seems that the development
teams behind them are relatively

378
00:38:17,000 --> 00:38:23,000
compact. So this is a bit of a
layered question. I guess, well
no I'll just jump right to it.

379
00:38:26,042 --> 00:38:32,042
How have the three of you
approached insider threat? >>
Very good question. I think

380
00:38:34,333 --> 00:38:39,083
again it comes down to, you
know, the social engineering
contest that happened here. It

381
00:38:39,083 --> 00:38:43,958
is 100 percent of the target,
100 percent of the time. This is
the number one threat we all

382
00:38:43,958 --> 00:38:50,292
need to look at. You can't have
any one person in charge of core
code. We've got several people

383
00:38:50,292 --> 00:38:55,542
that would have to change the
core code to do things. Because
for instance, you know, we don't

384
00:38:55,542 --> 00:39:01,750
want the threat of someone
kidnaps your child and says
change this. Or let me give you

385
00:39:01,750 --> 00:39:06,208
$100 million under the table to
change this. Those are all very
key threats you need to think

386
00:39:06,208 --> 00:39:12,208
about. >> There's two simple
tools that we use. Get and get
hub. That our developers are all

387
00:39:17,417 --> 00:39:22,833
checking things in and we know
who did them and periodically we
take a snapshot and put it up

388
00:39:22,833 --> 00:39:30,458
for review. So an insider who
made a change would end up going
through internal review and we

389
00:39:30,458 --> 00:39:36,833
know who is working on what. And
ultimately it will end up where
anybody can go and see it. So

390
00:39:36,833 --> 00:39:42,833
the idea that an insider can do
something insidious is greatly
limited. Well do something

391
00:39:44,958 --> 00:39:50,750
insidious and not get caught is
greatly limited. And so
publishing your source is what I

392
00:39:50,750 --> 00:39:55,000
would say. >> So the counter
argument to that, of course,
would be for instance true

393
00:39:55,000 --> 00:39:58,750
crypt. I am not saying there
have been any problems but, of
course, how long has it been

394
00:39:58,750 --> 00:40:05,500
before somebody decided to go
through a major audit of that
code base? >> That's a very long

395
00:40:05,500 --> 00:40:11,500
other debate. And, you know,
it's a very interesting one. But
it's not clear that it has

396
00:40:13,708 --> 00:40:18,458
anything to do with insider
threats. Which is what you
asked. >> Right. >> Thank you.

397
00:40:18,458 --> 00:40:23,292
>> Thanks. >> All right. Just
going to ask a question related
to the previous one. Are all

398
00:40:23,292 --> 00:40:29,667
three of your applications open
sourced. >> The EFF would say
they're not open sourced though.

399
00:40:29,667 --> 00:40:35,667
>> Depends on what you mean by
open source. If you mean, you
know, if you mean ‑‑ there are

400
00:40:39,500 --> 00:40:45,500
people who think they own the
word "open" and my source is
published under a variant BSD

401
00:40:49,250 --> 00:40:53,458
license that says you can do
anything with it except make
money. You can compile it

402
00:40:53,458 --> 00:40:57,833
yourself. You can use it. You
can give it to your friends.
But, you know, you can't build a

403
00:40:57,833 --> 00:41:03,417
competing system. This is not an
OSI compatible license. But you
can download the source and

404
00:41:03,417 --> 00:41:07,917
compile it and do whatever you
want with it except make money.
>> I think that's what glimpse

405
00:41:07,917 --> 00:41:13,667
is going to do. We're a really
baby company. You know, it's
actually like I'm really honored

406
00:41:13,667 --> 00:41:18,000
to be on stage with Wicker and
Silent Circle but we only
launched in March and we're only

407
00:41:18,000 --> 00:41:25,500
available for IOS. It's going to
be a good three months before we
market ourselves as a security

408
00:41:25,500 --> 00:41:31,917
app. We just need to spend more
time on that first. So we've
been thinking about how we want

409
00:41:31,917 --> 00:41:38,042
to handle this. We are not open
source but I think following
those models is what we'll want

410
00:41:38,042 --> 00:41:44,542
to do. >> We're wondering about
that saying do we do part open
source and part not. The main

411
00:41:44,542 --> 00:41:50,583
reason we haven't opened sourced
our code is we don't want a
bunch of copycat apps out there

412
00:41:50,583 --> 00:41:56,458
that have done interesting
things with it. There's also a
threat model there. But it's

413
00:41:56,458 --> 00:42:01,958
something that we continually
think about. But I think it's
important, I think within this

414
00:42:01,958 --> 00:42:08,708
community a lot of times people
equate open source with secure
and closed source with not

415
00:42:08,708 --> 00:42:14,292
secure. And that's a dogma. And
something that I think, you
know, there's a lot of ways to

416
00:42:14,292 --> 00:42:20,542
add trust and transparency. And
I think that's what we all need
to get better at every day. >>

417
00:42:20,542 --> 00:42:26,458
Well you can do code audits
right. >> Just today we actually
announced two more code audits

418
00:42:26,458 --> 00:42:32,958
from I tech and aspect. So we've
had the best groups of hackers
in the world. And months and

419
00:42:32,958 --> 00:42:38,250
months looking at our source
code every day. And two of them
today verified that indeed we

420
00:42:38,250 --> 00:42:44,167
use the strong crypto that we
say we do. It's been properly
implemented and they indeed

421
00:42:44,167 --> 00:42:50,250
found no back door. >> Let's try
to get the last two questions.
>> As a developer, do you have

422
00:42:50,250 --> 00:42:56,542
any advice for someone that's
looking to build ephemeral
communication apps or at least

423
00:42:56,542 --> 00:43:02,625
applications that have some
elements of this type of
communication? >> Advice? Well

424
00:43:02,625 --> 00:43:08,625
the first bit of advice is
actually care about what you are
doing. Because really the things

425
00:43:12,500 --> 00:43:19,333
we've seen that have gone wrong,
everybody makes mistakes. Fix
your mistakes, look at what you

426
00:43:19,333 --> 00:43:24,500
want to be your threat model.
Look at your privacy policy.
Look at how you are going to

427
00:43:24,500 --> 00:43:30,500
handle some of these things. And
genuinely live up to what your
own ideals are. That's the

428
00:43:32,667 --> 00:43:40,458
really big one. The technology
is relatively easy. It's the
execution that's hard. >> It is

429
00:43:40,458 --> 00:43:45,333
a really crowded space. When I
first decided to do this it was
a year and a half ago and Snap

430
00:43:45,333 --> 00:43:50,083
Chat wasn't on Android and
Facebook hadn't yet started to
get into the market. And line

431
00:43:50,083 --> 00:43:56,083
which is IPO in Japan is going
ephemeral. Facebook wants to do
this. It's becoming kind of

432
00:44:00,458 --> 00:44:04,833
normative. So I think if you
want to do an ephemeral app the
question is, okay, like,

433
00:44:04,833 --> 00:44:11,417
everyone is ephemeral now. How
are you different? >> There's a
communication sub strait company

434
00:44:11,417 --> 00:44:17,417
called Prodia and they let you
do a lot of programing in Java
script and they have a how to

435
00:44:19,958 --> 00:44:25,958
make ephemeral communication
system there. It really is
normative. >> Could we have the

436
00:44:25,958 --> 00:44:31,500
last question? >> So if
everyone's ephemeral and we
aren't protecting screen shots,

437
00:44:31,500 --> 00:44:38,333
what sort of features do you
think these apps should have
that they don't currently have

438
00:44:38,333 --> 00:44:43,292
and what are the most important?
>> In our apps? Right now. Sorry
I didn't understand the

439
00:44:43,292 --> 00:44:49,292
question. I'd love to find a way
to do password reset. >> I think
community is what is really

440
00:44:54,958 --> 00:45:00,167
interesting now. When I look at
new apps that come out. Because
there's so many communications

441
00:45:00,167 --> 00:45:08,083
apps out there and what can make
an app different surprisingly is
like what is the ethos on there.

442
00:45:08,083 --> 00:45:13,042
What kind of modernization is on
there? Who else, you know, which
friends of mine are on there.

443
00:45:13,042 --> 00:45:18,083
That's actually what's most
interesting to me right now. >>
So talk to me about password

444
00:45:18,083 --> 00:45:24,333
resets. That's easy. (Laughter)
There are ‑‑ the idea is what do
you want to solve? What sort of

445
00:45:24,333 --> 00:45:28,292
communities do you want to
support? What sort of
communications do you want to

446
00:45:28,292 --> 00:45:36,208
support? There are a lot of
things that a bunch of people
are over thinking. And if you

447
00:45:36,208 --> 00:45:42,792
don't try to do everything, say,
in the security layer, you do it
in the user layer, are a lot

448
00:45:42,792 --> 00:45:48,708
easier than if you try to prove
that you can always do these
things. I mean, for example with

449
00:45:48,708 --> 00:45:54,125
ephemerality as we've said you
are not going to get
100 percent. You are going to

450
00:45:54,125 --> 00:45:58,083
get something that is completely
and utterly usable and solves
99.99 percent of it and that's

451
00:45:58,083 --> 00:46:00,667
actually easy. The last nine,
you are not going to get that.
>> I'd like to thank the panel

452
00:46:00,667 --> 00:46:06,667
for contributing and working on
these great applications that I
use myself and there's a lot of

453
00:46:14,375 --> 00:46:20,375
people that use. And I'm really
excited to see where they're all
going to go in the feature. >>

454
00:46:24,583 --> 00:46:26,583
Thank you, Ryan. (Applause)

