00:00:00.033,00:00:05.038
>>How many of you flew here?
Alright now how many of you flew
here and needed your passport to

00:00:07.774,00:00:12.779
get here? Alright now, wouldn’t
it be great if you lost that and
you could still fly home? Um I

00:00:18.585,00:00:23.590
don’t know if that’s what this
talk is about but um -
[Laughter] >>So AV, can we their

00:00:26.493,00:00:31.498
slide deck up behind us? I don’t
think my voice is gonna get me
anywhere. But yeah in a couple

00:00:45.045,00:00:50.050
of minutes, we got Azeem and
JOhn who goes by Delta Zero.
Give them a warm round of

00:00:56.023,00:01:00.961
applause. [Applause] >>Thank
you. Ready to go? Awesome. >>You
got a couple of minutes of

00:01:08.101,00:01:13.040
downtime if you want to tell a
joke or two. >>Um, I don’t know.
Why did the chicken cross the

00:01:13.04-->000:01:19.012
road? >>Oh I don’t know?
>>Because I’m totally
unoriginal. I don’t know.

00:01:19.012,00:01:24.017
>>That’s a good one. >>Thanks.
>>Yeah. >>I try. [Laughter]
>>Cool. And silence. [Laughter]

00:01:30.958,00:01:37.831
>>Silence. >>Yeah, I couldn’t.
>>I don’t think I could tell a
joke up here because I’d

00:01:37.831,00:01:42.836
alienate half of the population.
Um. >>We can just go ahead and
start. >>Yeah, it’s about time.

00:01:48.342,00:01:55.248
Everybody if there’s a seat open
next to you, make a friend.
There’s still gonna be people

00:01:55.248,00:02:00.187
piling in and uh give them
another round of applause.
[Applause] >>So welcome

00:02:06.293,00:02:10.731
everybody to our talk, “Your
Voice is My Passport.” It has
nothing to do with physical

00:02:10.731,00:02:17.237
passports. Um my name is John
Seymour aka, Delta Zero. >>And I
am Azeem Aqil. >>And we both

00:02:17.237,00:02:23.276
work at Salesforce doing
detection and response
engineering. >>Okay so let’s get

00:02:23.276,00:02:29.916
into to. So these days we see
that voice is starting to
become, is starting to be used

00:02:29.916,00:02:33.420
as a means to authenticate
right? I’m using the word
authenticate a little loosely

00:02:33.42-->000:02:38.792
here, and we’ll see why. Now
when I say voice authentication
or speaker recognition, I think

00:02:38.792,00:02:42.763
the thing that comes immediately
to people’s mind is maybe um
Apple Siri or Google Assistant,

00:02:42.763,00:02:44.765
right? Both of these are
services that are signed up to
unlock not only a subset of

00:02:44.765,00:02:46.767
their features based on whether
their targets as a specific
sentence. So Hey Siri for Apple

00:02:46.767,00:02:48.769
or Okay Google, for Google,
right. Now I do want to mention
here that neither Apple nor

00:02:48.769,00:02:50.771
Google use the word
authenticate, authentication to
describe their service at least

00:02:50.771,00:02:52.773
we never uh came across that to
term. Uh we suspect it’s because
they’re aware that this is maybe

00:02:52.773,00:02:54.775
brittle, but we’ll see. So here
you have an of a financial
institution, Schwab Bank that

00:02:54.775,00:02:56.777
does indeed use authentication.
So, you can get into your
account just based on your

00:02:56.777,00:03:01.715
voice. You can unlimited access
to everything. And the way that
it works is um after you’ve

00:03:10.023,00:03:15.028
registered, uh you say the term
“at Schwab my voice is my
password,” to get into to the

00:03:37.017,00:03:42.022
account. Now the irony of that
sentence seems like it is
completely lost on them.

00:03:54.034,00:03:58.872
[Laughter] And now then finally,
here’s an example of Microsoft
Speech API which also claims to

00:03:58.872,00:04:03.810
do authentication and this is so
voice recognition or speaker
recognition as a service. Okay

00:04:08.882,00:04:15.355
so as you may have inferred by
now our goal is to break voice
authentication. And but we want

00:04:15.355,00:04:19.860
to do this with minimal effort.
Now let me be little more
specific. By breaking voice

00:04:19.86-->000:04:26.366
authentication I mean that we
want to be able to spoof a
specific target and get into a

00:04:26.366,00:04:32.305
service that’s deployed today.
That’s set up to let a person in
by using his voice. By minimal

00:04:32.305,00:04:37.310
effort, we actually mean three
things. So the first is, that
whatever solution we come up

00:04:39.613,00:04:45.118
with, should not require tons
and tons of compute. So, voice
authentication, speaker

00:04:45.118,00:04:49.956
recognition or machine learning
problems, and machine learning
and deep neural networks in

00:04:49.956,00:04:55.028
particular just tend to require
lots of compute. So I’m talking
of maybe a commodity server not

00:04:55.028,00:04:59.966
a server farm. Second uh it
should be realizable in some
reasonable time. So maybe days

00:05:02.836,00:05:09.376
or weeks, not months. And then
finally you should not require a
PhD in data science degree to be

00:05:09.376,00:05:15.649
able to implement this.
>>Alright so if you haven’t seen
the Hacker movie, Sneakers, or

00:05:15.649,00:05:21.288
the Hacker movie Sneakers um you
probably should. It’s a hacker
classic from the early 90’s and

00:05:21.288,00:05:26.526
it’s also quite a bit relevant
to our talk. It’s where the
title actually comes from. In it

00:05:26.526,00:05:31.498
the heroes actually need to
bypass a voice authentication
system and they do so by social

00:05:31.498,00:05:37.304
engineering their target to say
the specific words in the
passphrase. So let’s see if this

00:05:37.304,00:05:42.309
actually works. >>You are clear
all the way up to the mantra.
[Music playing] >>Hi, my name is

00:05:52.519,00:05:57.524
Winnefred. Hi my name is Warner
Brandos, my voice is my
passport. Verify me. >>Woo!

00:06:36.496,00:06:41.501
>>Yeah, cool. [Clapping] And so
here’s how you do that. Right um
so let’s go to the original idea

00:06:49.542,00:06:55.048
of Sneakers right. Um in
Sneakers they record the words
that the um for the passphrase

00:06:55.048,00:06:59.152
that the victim would say um by
using Social Engineering and
getting the target to actually

00:06:59.152,00:07:03.390
say those particular words. But
in real life this is actually
pretty difficult for three

00:07:03.39-->000:07:08.461
reasons, right? First, the
people that you’d normally want
to impersonate or spoof are say

00:07:08.461,00:07:12.766
like a CEO or a politician.
They’re normally pretty busy
people and may not want to speak

00:07:12.766,00:07:18.705
with you know normal people. Um
second, um it-it-it if you’ve
ever tried this in a

00:07:18.705,00:07:24.077
conversation, uh you should, uh
if you’ve ever said like hey you
should say like “Hey Siri” to

00:07:24.077,00:07:29.282
me, and I want to record it. Um
it-it’s something that’s gonna
get you know your target very

00:07:29.282,00:07:33.787
suspicious of you and say, “hey
why do you want me to say the
words’ ‘Hey Siri”? But even if

00:07:33.787,00:07:39.659
you were able to do those two
things, right? Um still actually
most voice authentication

00:07:39.659,00:07:43.730
systems are-are pretty smart and
sometimes they like change pass
you know the passphrase and

00:07:43.73-->000:07:48.735
things like that. So the actual
recording that you do, um might
actually be stale and useless by

00:07:48.735,00:07:53.173
the time you actually go and
authenticate. However luckily
there’s this thing called

00:07:53.173,00:07:58.645
text-to-speech, and it’s
actually pretty good. There’s an
entire area of research around

00:07:58.645,00:08:03.583
it. It’s got um basically a
workshop at NIPS dedicated to
it. So NIPS is a very

00:08:05.618,00:08:09.622
prestigious machine learning
conference. Um it’s machine
learning based so basically you

00:08:09.622,00:08:14.961
give the system a bunch of audio
and transcripts of that audio
and it-it produces new for you.

00:08:14.961,00:08:19.299
And it’s made a ton of
improvements of it lately. And
so it’s a very active research

00:08:19.299,00:08:24.304
area. Let’s try this. Alright so
let’s see if this one works.
>>This is a dangerous time.

00:08:42.255,00:08:47.360
Moving forward we need to be
more vigilant with what we trust
from the internet . It’s a time

00:08:47.36-->000:08:52.365
where we need to rely on trusted
news sources. >>Alright so the
actual audio lagged a bit there

00:08:55.535,00:09:01.408
just because of the network
here. But uh basically that was
Jordan Peele actually um and

00:09:01.408,00:09:06.012
BuzzFeed that made that video.
And it should convince you that
this technology is becoming

00:09:06.012,00:09:11.017
pretty widespread. Um just think
of for example what you could do
with a huge AI research lab

00:09:11.017,00:09:17.724
backing you. In our case, we’re
actually gonna focus on
exclusively on using it to

00:09:17.724,00:09:22.829
bypass voice authentication. As
such we really don't care about
the quality of the audio that’s

00:09:22.829,00:09:27.434
generated, we just care whether
it bypasses the service or not.
It could be complete and utter

00:09:27.434,00:09:32.439
garbage. This could be fun.
>>Okay so, John has already
mentioned that text to speech is

00:09:45.018,00:09:51.391
generally a machine learning
problem right. Uh, the essential
idea is that you give the

00:09:51.391,00:09:55.995
algorithm some text. Um
transcribe text to be specific
and it generates the equivalent

00:09:55.995,00:10:00.233
audio representation of that
text so for example, Mel
Spectrograms, which is just the

00:10:00.233,00:10:06.439
audio waveform corresponding to
some text. The model learns the
mapping between the transcript

00:10:06.439,00:10:12.378
and the audio or to be more
precise, character sequences and
the final output. And the way it

00:10:12.378,00:10:16.483
does this, is that you give it
labeled data by label I just
mean transcribe audio and you

00:10:16.483,00:10:21.020
feed it into a deep neural
network and after many many
interrations the model learns

00:10:21.02-->000:10:25.258
this association I’ve been
talking about. The association
between character sequences and

00:10:25.258,00:10:31.130
the final audio output. Now a
couple of things I want you guys
to note here. So generally uh

00:10:31.13-->000:10:37.570
deep learning models that are
focused on voice are trained on
a single person’s voice, right?

00:10:37.57-->000:10:42.675
This is starting to change and
you’ll see later in the talk why
but but it’s still a good thing

00:10:42.675,00:10:47.814
to keep in mind. The second
important thing is that deep
learning models in particular

00:10:47.814,00:10:53.786
and ones that have to do with
voice. Especially so, require
lots and lots of data-data to do

00:10:53.786,00:10:58.291
any kind of good work, right?
And the general consensus in the
academic community is that these

00:10:58.291,00:11:04.063
models require like around 24
hours of high quality labeled
audio to be able to do well.

00:11:04.063,00:11:10.904
Now, there are two very high
quality open source data
services that are available both

00:11:10.904,00:11:15.575
of them have over 24 hours of
data. The first one is Blizzard,
the second one is LJ Speech. The

00:11:15.575,00:11:18.444
only difference between the two
of these is that one is a
recording of a male and the

00:11:18.444,00:11:23.449
other of a female. All of these
you will see why this is
important. >>So, um basically uh

00:11:29.022,00:11:32.859
there’s this company called
Lyrebird, and they’re it’s
founded by several of the

00:11:32.859,00:11:38.998
pioneers in um Text to Speech
research. And one of their goals
is to bring awareness to what

00:11:38.998,00:11:43.670
all this technology can do. They
host of similar videos to the
Jordan Peele video that we

00:11:43.67-->000:11:48.808
showed you earlier. Um as a
demonstration to the general
public they’ve actually set up a

00:11:48.808,00:11:52.745
service where you can actually
record your own voice and
generate something from it. And

00:11:52.745,00:11:57.383
so the steps to do so are pretty
easy. All you do is you create
an account. You record 30

00:11:57.383,00:12:01.988
sentences which are actually
chosen by Lyrebird in advance
and are the same for all users

00:12:01.988,00:12:07.493
and basically after-after that,
that basically trains the model.
You then provide a target

00:12:07.493,00:12:12.065
sentence that Lyrebird would
generate. It’s actually pretty
simple, it only takes a few

00:12:12.065,00:12:17.270
minutes to generate audio but
there’s definitely degradation
in quality. It’s also finicky

00:12:17.27-->000:12:22.709
with a lot of different accents.
Um we actually did a proof of
concept with Siri and

00:12:22.709,00:12:27.213
Microsoft’s Speaker Recognition
Public Beta. Uh we didn’t
actually test with like Schwab

00:12:27.213,00:12:32.218
or Google voice. So first we
actually trained Siri or MSR to
recognize our own voices then we

00:12:34.787,00:12:39.225
generated the target passphrases
using Lyrebird and tested the
audio against the speaker

00:12:39.225,00:12:44.230
recognition authentication
software. >>My voice is stronger
than passwords. >>So this is us

00:12:54.407,00:12:59.412
actually training the service in
the first place. >>My voice is
stronger than passwords. My

00:13:04.684,00:13:09.889
voice is stronger than
passwords. >>Okay so now
Microsoft uh actually accepts

00:13:09.889,00:13:16.596
our our- >>My voice is stronger
than passwords. >>And notice
that was accepted. >>This is a

00:13:16.596,00:13:21.601
test and should be rejected.
>>Rejected is expected? >>My
voice is stronger than

00:13:23.87-->000:13:28.875
passwords. >>It rejects Azeem as
well. >>My voice is stronger
than passwords. >>And look it

00:13:36.182,00:13:41.187
accepts the generated audio that
we took from Lyrebird. So
basically actually um there is

00:13:49.295,00:13:54.300
some um there is some
limitations to basically using
Lyrebird as a service right. Um

00:13:56.869,00:14:02.442
for example, its effectiveness
varies greatly based on the
speaker. Um it worked very well

00:14:02.442,00:14:07.647
for me, it didn’t work for
Azeem. But aside from just
general finickiness right um,

00:14:07.647,00:14:12.218
Lyrebird requires specific
utterances, and it falls back to
a lot of the same issues that

00:14:12.218,00:14:17.757
the Sneakers video we showed
before um has as well. Right,
it’s simply unlikely that an

00:14:17.757,00:14:23.396
attacker could contain specific
recordings of a target. Though
this does mean the Lyrebird

00:14:23.396,00:14:28.267
database and um as well as voice
authentication databases in
general um might be a valuable

00:14:28.267,00:14:33.673
target for attackers. To
demonstrate how a real attack
might work, we actually turned

00:14:33.673,00:14:38.678
to the state of the art text to
speech generation. >>Okay so,
when I started this out I

00:14:41.881,00:14:46.385
mentioned that one of our goals
is to make this as easy as
possible. You should not require

00:14:46.385,00:14:51.824
data scientists degree in order
to implement this soutloon. So
naturally we return to open

00:14:51.824,00:14:53.826
source that are just readily
available, right. Now there are
several open source models. Two

00:14:53.826,00:14:58.831
of the most popular ones are
Tacotron which is by Google and
Wavenet. So Wavenet is perhaps

00:15:04.27-->000:15:08.975
maybe better known and it
generates very very realistic
human sounding output, however

00:15:08.975,00:15:13.980
the problem is that it needs to
be tuned significantly. So what
I mean by that is that Wavenet

00:15:13.98-->000:15:19.185
has lots of input parameters so
as examples of some of them that
those would be the fundamental

00:15:19.185,00:15:25.691
log frequency um the [inaudible]
linguistic features. All of
these things would need to be

00:15:25.691,00:15:30.163
tuned by a domain expert right?
This requires domain expertise
and kind of strays away from our

00:15:30.163,00:15:35.568
original goal for making this as
easy as possible. So now,
Tacotron simplifies this entire

00:15:35.568,00:15:39.238
process very much so right. It
takes the guessing out of it so
it no longer need to

00:15:39.238,00:15:45.478
individually choose features.
You can basically just give
Tacotron the audio as direct

00:15:45.478,00:15:52.051
import and it will figure out
what the best feature set for
that is. So this is an example

00:15:52.051,00:15:57.056
of Google Tacotron 2. Which is
Google’s latest and greatest
text to speech system. Now

00:15:59.258,00:16:02.361
Tacotron 2 is basically composed
of 2 steps. There’s this thing
at the bottom and the one at the

00:16:02.361,00:16:07.366
top. The thing at the bottom is
basically a recurring sequence
to sequence of um feature

00:16:10.203,00:16:16.876
prediction network which outputs
um Mel Spectrograms and the one
on the top is a modified wavenet

00:16:16.876,00:16:21.247
which is conditioned on the
previous Mel Spectrogram frame
and gener-generates the final

00:16:21.247,00:16:26.252
audio sequence. So um an easier
way to think about this is the
first network kind of determines

00:16:28.988,00:16:33.292
what the ideal feature set for
wavenet should which you can
think of as this visual

00:16:33.292,00:16:38.130
representation of sound
frequencies and wavenet then
takes those as inputs and

00:16:38.13-->000:16:44.704
finally gives you an output. Now
the good news here is that you
don’t need to know any of the

00:16:44.704,00:16:50.376
internal s of Tacotron to make
it work. This is available open
source and you can basically

00:16:50.376,00:16:56.515
just run this give it the actual
character sequences. Uh there
are some parameters that you can

00:16:56.515,00:17:02.388
tweak and make it better but we
did not. If you just leave those
things in default, it will work

00:17:02.388,00:17:07.393
very well. >>Alright, so we just
have a few comparisons of the
different audio for you. So this

00:17:12.698,00:17:19.105
should be um the audio from
Tacotron, audio generated from
Tacotron version 1, which Google

00:17:19.105,00:17:25.144
actually published in April of
2017 and there’s actually a
completely open source

00:17:25.144,00:17:30.149
implementation of it.
>>Scientists at the CERN
laboratory say they have

00:17:34.72-->000:17:39.392
discovered a new particle. >>So
you can actually kind of tell
that that was generated >>You’re

00:17:39.392,00:17:44.397
clear all the way up to the
mantra. >>Um this is fun. >>The
other video started playing.

00:17:53.739,00:17:58.744
>>We just really love Sneakers
here. [Laughter] Alright.
>>Generative adversarial

00:18:07.153,00:18:10.990
networker variational
autoencoder, generative
adversarial, generative.

00:18:10.99-->000:18:17.630
>>Alright um so that’s actually
audio generated by Tacotron
version 2. Which Google released

00:18:17.63-->000:18:22.635
in December of 2017 so we’re
talking April of 2017 to
December of 2017. Huge increase

00:18:25.071,00:18:30.409
in quality in a very short
period of time. For completion
purposes, here’s the audio

00:18:30.409,00:18:35.414
generated by Wavenet. I really
should just set the defaults for
this but- >>Man dying of liver

00:18:40.553,00:18:45.558
complaint lay on the cold stones
without or food to eat.
>>Alright, cool. >>Alright, so

00:18:55.534,00:18:59.605
that’s all well and good but in
order to actually spoof
somebody’s voice or train any

00:18:59.605,00:19:04.543
kind of model, you need data and
you needs lots of it. So given
that we want to impersonate a

00:19:06.579,00:19:10.850
specific target like where,
where might you get this data
from. So if you’re target is

00:19:10.85-->000:19:15.454
somebody that does lots of
public speaking like say John,
you can probably grab that audio

00:19:15.454,00:19:21.761
from YouTube uh or some public
source. But remember both the
quality and the quantity of the

00:19:21.761,00:19:26.766
audio is important. Then you
actually need to transcribe this
data because as I mentioned

00:19:28.801,00:19:33.873
earlier these models required
labeled data and labeled in this
sense in just a transcription.

00:19:33.873,00:19:39.078
Uh and then finally you need to
chunk this up. So you need to
chunk in up because these these

00:19:39.078,00:19:44.717
models expect sentences right
they expect you to be able to
give them chunks of audio and

00:19:44.717,00:19:50.790
then that’s how the network
runs. So when we started this
out, we thought that we could

00:19:50.79-->000:19:55.661
kill 2 birds with one stone and
use Google’s speech API. So the
speech API is supposed, what

00:19:55.661,00:20:00.533
it’s supposed to do is you give
it some audio and it gives you
both the transcript and the

00:20:00.533,00:20:05.538
start and the end time of each
word in that audio right. But
for whatever reason we could

00:20:07.973,00:20:12.311
never get it to work well
enough. Um we suspect it’s
because you know there was there

00:20:12.311,00:20:17.049
when you get audio from some
public service there's going to
be lots of noise in it and it

00:20:17.049,00:20:23.022
doesn’t tend to do very well
with that. Um it also does not
do very well with natural pauses

00:20:23.022,00:20:28.928
in human speech like um or I it
just tends tends to think that’s
some word. Now this is not a

00:20:28.928,00:20:33.632
ding on Google actually the
speech API does work very well
when you give it like good

00:20:33.632,00:20:38.037
quality audio but we think
that's unreasonable when you’re
going to impersonate some

00:20:38.037,00:20:43.442
specific target. So what we had
to do then is we ended up
manually transcribing our data

00:20:43.442,00:20:48.447
right. So remember John is the
target here and uh it’s not so
bad because we uh it took us

00:20:51.584,00:20:56.589
what an hour to transcribe that
data and if then chunking that
data up actually turned up to be

00:20:58.691,00:21:04.363
very easy right. So you just use
FFMPEG and split your audio by
7’s and that just conventinely

00:21:04.363,00:21:09.368
chunks it up by sentences.
Alright so. So I’ve mentioned
that both the quality and the

00:21:13.939,00:21:17.843
quantity of the data is
important right so when we get
this data from a public source

00:21:17.843,00:21:23.349
like uh a YouTube talk. A Lot of
the sentences in that talk are
actually not very useable right

00:21:23.349,00:21:28.621
so if youre target says lots of
ums and uhs that's very useful
uh the models not going to learn

00:21:28.621,00:21:34.460
anything from that. Uh there’s
also times when there’s applause
and that again will mess your

00:21:34.46-->000:21:39.532
sample up so what you and need
to do is you need to subsample,
select the highest quality sub

00:21:39.532,00:21:45.604
sample from your uh audio and
then use those. What we ended up
with was around like 5 to 10

00:21:45.604,00:21:51.243
minutes of good quality audio
and if you remember I mentioned
that you need 24 hours of data

00:21:51.243,00:21:56.816
and that is- this is just not
nearly enough to any kind of
like good training. Um now the

00:21:56.816,00:22:01.754
solution of that problem of very
limited data is something called
Data Augmentation. >>Right so

00:22:04.323,00:22:09.028
there's one side effect of
actually slowing down and
speeding up and that’s actually

00:22:09.028,00:22:15.034
that the pitch changes. Um so
you can abuse this to generate
new examples and you can add

00:22:15.034,00:22:20.940
these to your training sets. Uh
training examples. Um there’s
tons of libraries available for

00:22:20.94-->000:22:26.111
this, we used Pydub. Um but to
make this a little rigorous what
we did actually was we took an

00:22:26.111,00:22:31.417
original recording of me saying
“Hey Siri” and we slowed it down
and sped it up and we saw how

00:22:31.417,00:22:37.523
far we could actually do so and
the um the Siri actually still
recognized my voice and and

00:22:37.523,00:22:43.729
unlocked the phone. And so in
our case uh basically we were
able to slow it down to about

00:22:43.729,00:22:49.268
point 88 times um and speed it
up to about one point 21 times
and Siri would still recognize

00:22:49.268,00:22:54.139
that it was me speaking.
Obviously your mileage may vary
for the exact parameter here

00:22:54.139,00:22:59.411
right, um it’s probably
different for every single
person. Notice that this

00:22:59.411,00:23:03.182
actually fixes both of our
original issues right it
multiples our training data by

00:23:03.182,00:23:08.320
about 30 times. As well as you
only need to transcribe about
1/30th of the original training

00:23:08.32-->000:23:13.359
data but there is an issue
introduced by this. Um and
that’s the issue of overfitting

00:23:13.359,00:23:18.130
right. If you're only choosing
one subset of what the target
actually speaks, then you’re not

00:23:18.13-->000:23:23.602
getting a full representative
sample of all the different faux
names or things they might say.

00:23:23.602,00:23:28.474
Um so you still have to be
careful about this. Um so in
other words the model’s being

00:23:28.474,00:23:33.012
trained on a small subset of
what the target might say so
there might be some sounds that

00:23:33.012,00:23:38.017
it can’t generate very well. But
even if you consider like 30
times right, um basically that's

00:23:41.453,00:23:47.059
still not enough to actually
generate really good um really
good audio right, if you

00:23:47.059,00:23:53.032
actually do that math um 5 to 10
minutes times about 30 is still
nowhere near the 24 hours that

00:23:53.032,00:23:58.337
we originally um originally
needed. Uh so shifting pitch
ended up not getting us all the

00:23:58.337,00:24:03.542
way there. Um if we calculated
right uh we’d need at least 1
hour of high quality and that

00:24:03.542,00:24:08.514
actually still takes forever to
uh to transcribe unless that
this is not even considering the

00:24:08.514,00:24:13.519
issue of limited vocabulary. So
we actually turned to this idea
of domain adaptation or transfer

00:24:15.587,00:24:20.859
learning so uh how this actually
works. So you initially train on
a large open source data set

00:24:20.859,00:24:26.332
such as Blizzard or LJSpeech,
you get a decent model and you
actually stop training there.

00:24:26.332,00:24:31.770
You actually just simply swap uh
your target’s data into the
original training data and you

00:24:31.77-->000:24:36.842
just continue training the model
and eventually you'll get a
model which actually sounds more

00:24:36.842,00:24:42.915
like the original target. And so
what we think this is the model
initially learns how to speak

00:24:42.915,00:24:47.753
using the Blizzard and LJSpeech
data and then it learns
basically, it adjust pitch and

00:24:47.753,00:24:53.792
accent based on the targets. We
think this is actually because
of the sort of layered approach

00:24:53.792,00:24:59.965
of neural nets. Um so we think
sort of the lower layers are
more um um more useful for

00:24:59.965,00:25:04.303
basically understanding the
basics of language and
translating you know characters

00:25:04.303,00:25:10.776
and words into sentences and
into audio. And then the higher
layers just determine pitch and

00:25:10.776,00:25:15.848
accent and things like that. And
furthermore there’s still a lot
of variance in affectiveness

00:25:15.848,00:25:22.621
here. Um it’s it’s very finicky
sometimes it converges in one
ePOP which is just one iteration

00:25:22.621,00:25:28.394
over all of your training data
over your target. Sometimes it
actually takes a couple days to

00:25:28.394,00:25:33.399
train. So we have a simple demo
here of basically, this is, we
trained our Blizzard model for

00:25:38.137,00:25:43.142
not for very long so it’s not
great audio quality. >>I’m going
to make him an offer he can not

00:25:51.283,00:25:56.288
refuse.I am- >>So it-it still
sounds, thats-thats sounds alot
like the Blizzard um person. But

00:25:58.39-->000:26:03.061
it-it’s still sort of choppy and
you can hear it. That's an
artifact of us using uh Tacotron

00:26:03.061,00:26:09.468
V1 we expect the quality ot get
better. But then when we
actually use Transfer Learning.

00:26:09.468,00:26:13.872
>>I am going to make him an
offer he can not refuse. >>So.
>>I am going to make him an

00:26:13.872,00:26:18.710
offer. >>That’s actually, that
sounds a lot more like my voice
and that was completely

00:26:18.71-->000:26:23.715
generated right. So basically um
EPOX vary, this one took about 3
days to train and then an

00:26:29.922,00:26:35.727
overnight to actually do the
transfer over.Um this actually
is good enough to start breakin

00:26:35.727,00:26:42.668
APIs right? Um the approach
works, it’s not very its not as
consistent even as Lyrebird but

00:26:42.668,00:26:47.573
it doesn't require any specified
speech at all. What we did is we
scraped audio from YouTube to

00:26:47.573,00:26:53.479
generate that. Um the overall
effort is also very very low it
took us about a month from

00:26:53.479,00:26:58.851
conception to completion. Uh
more effort would obviously make
the audio quality for example

00:26:58.851,00:27:03.789
much better or make it alot you
know higher probability of
actually being accepted by the

00:27:06.658,00:27:11.797
uh 2 APIs that we demonstrated
earlier. Um there's many more
parameters we could have tweak

00:27:11.797,00:27:18.003
and so much data we could have
transcribed for example. Um but
the fact that the overall effort

00:27:18.003,00:27:24.409
is so low should be pretty
scary. >>Okay, so we may have
thrown quite a bit of

00:27:24.409,00:27:29.047
information your way. So let me
just take a step back and put
everything back together right,

00:27:29.047,00:27:33.185
so the steps you would need to
take in order to spoofs
somebody's voice. It’s really

00:27:33.185,00:27:37.923
not that much. So you start off
by scraping data from the
target, some public source maybe

00:27:37.923,00:27:44.096
YouTube. You subsample, you only
select the high quality samples
from your audio. You need to

00:27:44.096,00:27:49.268
then transcribe and chunk that
audio. Uh at this point you need
to do it manually but there is

00:27:49.268,00:27:55.140
no reason to believe that the
speech API is not going to get
very good very quickly. Then you

00:27:55.14-->000:28:00.245
need to augment your audio by
shifting pitch. Uh the second
augmentation is 2 steps. So

00:28:00.245,00:28:04.616
first you need to train a
general text to speech model on
any open source dataset and then

00:28:04.616,00:28:08.720
you replace your general model
training data with the target
data and then you finish

00:28:08.72-->000:28:12.991
training. At this point you
should be able to successfully
synthesize your target’s voice.

00:28:16.862,00:28:22.067
Okay so I-I kinda want to put
our work in perspective now,
give uh people a flavor of

00:28:22.067,00:28:27.673
everything uh machine learning
for offense related. So what
we’ve done here is we’ve grouped

00:28:27.673,00:28:33.779
um prior work into to these two
arguably very broad buckets. So
there's attacks on machine

00:28:33.779,00:28:38.483
learning systems and then
attacks using machine learning
systems and our work is kind of

00:28:38.483,00:28:44.122
squarely in the middle. So let’s
first start with attacks on
machine learning systems. Uh now

00:28:44.122,00:28:47.092
adversary attacks are one of the
hottest topics in uh machine
learning research right now. In

00:28:47.092,00:28:50.062
fact these two words are
sometimes just used
synonymously. So the basic idea

00:28:50.062,00:28:55.067
be-behind adversary attacks is
that you have to carefully craft
your input to machine learning

00:29:01.94-->000:29:06.945
model in such a way that the
model ends up misclassifying
your uh your import right. So as

00:29:09.214,00:29:15.120
an example. Think of an image
recognition system and a picture
of a dog right? Now you would

00:29:15.12-->000:29:20.158
carefully tweak those some
pixels in that picture in such a
way that the model would

00:29:20.158,00:29:26.264
misclassify that as a giraffe or
a panda or something. Now,
this-this might sound cute but

00:29:26.264,00:29:29.635
there are like security
implications here. So the
canonical example that people

00:29:29.635,00:29:35.674
give is think of a self driving
car, uh a self driving system,
and a stop sign right. So if

00:29:35.674,00:29:40.912
somebody does something to that
stop sign, uh when the stop sign
is still very much a stop sign

00:29:40.912,00:29:45.017
to a human the altered and
unaltered pictures are
indistinguishable to the human

00:29:45.017,00:29:50.889
eye. The system is going to
misclassify that as say a yield
sign or something. Now the most

00:29:50.889,00:29:55.894
of the prior work on adversarial
attacks for white systems have
focused on hiding hidden

00:29:57.996,00:30:03.969
commands in benign sounding
audio. So some password
basically showed how you can uh

00:30:03.969,00:30:10.142
have a benign sounding system
like have Okay, Google, uh turn
on the lights. Which would in

00:30:10.142,00:30:15.914
fact Google would interpret that
as something like send an email
or some such thing. Now this

00:30:15.914,00:30:22.788
method is pretty cool right but
the con right is that it's
currently very brittle. Then

00:30:22.788,00:30:27.125
there’s this idea of poisoning
the well. So with poisoning the
well, similar to adversaily

00:30:27.125,00:30:32.130
attacks you carefully craft your
input but your uh your aim now
is to uh corrupt the model.

00:30:36.702,00:30:40.172
Differential privacy is kind of
the inverse so, you-you
carefully observe the output of

00:30:40.172,00:30:42.174
your model in the hope that this
will tell you something about
the actual that we use to train

00:30:42.174,00:30:47.179
it. >>Cool. And so, again we’ve
sort of bucketed these things in
2 different categories just to

00:30:51.55-->000:30:56.555
make them simpler to understand.
Um but also um we have this idea
of attacks of using machine

00:31:01.526,00:31:06.264
learning systems right. And so
for example, earlier this
year-year we actually sae the

00:31:06.264,00:31:10.902
first what we consider
widespread machine learning
based attack in the form of deep

00:31:10.902,00:31:15.774
fakes. Um so if you don't know
about deep fakes, is its just
basically this app where oyo can

00:31:15.774,00:31:22.114
transplant let’s say a photo or
a head of one person onto
another in a picture or a video

00:31:22.114,00:31:26.485
and what we've seen is basically
is this ends up being used
mostly for pornographic

00:31:26.485,00:31:31.490
purposes.There’s actually also a
whole host [Laughs] A whole host
of other ways machine learning

00:31:34.025,00:31:40.065
systems can be used to actually
attack people right. And so one
um primary example is phishing,

00:31:40.065,00:31:44.703
um you can actually scrape data
of a target off of YouTube or
Twitter or something like that

00:31:44.703,00:31:50.142
and generate a phishing post
specifically tailored to their
own interests. Uh the final

00:31:50.142,00:31:54.112
thing that we want to call out
in this space is robotics and
social engineering. And if you

00:31:54.112,00:32:00.018
haven't seen it, there’s a
really cool talk by Sarah Jane
Turp and Straith and Windy Knox

00:32:00.018,00:32:06.958
on that. >>Okay so, we’re hoping
at this point we’ve convinced
you how relatively easy it is to

00:32:06.958,00:32:13.298
spoof somebody’s voice. Um so
there-there are other issues
with like voice as it means to

00:32:13.298,00:32:20.138
authenticate right. Uh you could
have some kind of passphrase uh
that you use but the problem is

00:32:20.138,00:32:25.911
that is it’s difficult to keep
passphrases secret if you have
to say them out loud. The other

00:32:25.911,00:32:30.515
problem is that you could
require an unknown vocabulary
and John talked about this

00:32:30.515,00:32:34.753
earlier but actually speaker
recognition with unknown
vocabulary is a harder problem

00:32:34.753,00:32:40.692
than with speaker recognition
with a known vocabulary. So what
we want to stress here is that

00:32:40.692,00:32:46.164
speaker recognition and speaker
authentication are two separate
problems and they should be

00:32:46.164,00:32:51.970
treated as such, right? Uh what
we suggest is that you use
speaker recognition as a weak

00:32:51.97-->000:32:57.676
signal on top of a multi factor
authentication system. So now
think of an MFA system that

00:32:57.676,00:33:02.581
requires tokens. So what you
would end up doing is saying
those tokens to the system

00:33:02.581,00:33:07.586
instead of typing it out maybe.
And that does indeed provide
another weak signal on top. So

00:33:10.355,00:33:15.794
let’s talk about detection. So
over here we’ve thrown together
two examples of something that

00:33:15.794,00:33:20.265
you could use to detect this. So
on the left is an example of
something that attempts to

00:33:20.265,00:33:26.404
detect computer generated audio.
On the right hand side, you have
the inverse where this device

00:33:26.404,00:33:31.610
which tries to detect certain
neural muscular features. So the
idea being that if it detects

00:33:31.61-->000:33:37.015
something the voice the the
sound must have come from a
human. Now treat these with

00:33:37.015,00:33:42.020
skepticism because we expect
these to be an arms race.
>>Cool. So just to sort of

00:33:45.69-->000:33:50.161
reiterate of what all we’re
trying to raise awareness for
and what all we think um based

00:33:50.161,00:33:55.433
on our own experiments right? Um
so first off what we’d like you
to takeaway, is that speaker

00:33:55.433,00:34:00.105
authentication and speaker
recognition are two different
problems, completely.

00:34:00.105,00:34:05.677
Recognition should only be
treated as a weak signal for
authenticating. Um the second

00:34:05.677,00:34:10.081
takeaway is that speaker
authentication can easily be
broken if the attacker has

00:34:10.081,00:34:15.253
speech data of the target and
knows the authentication prompt.
And third although most text to

00:34:15.253,00:34:19.991
speech systems require about 24
hours of speech to train,
transfer learning is actually a

00:34:19.991,00:34:25.463
very effective method to reduce
that time um to an amount
realistic for an attacker today

00:34:25.463,00:34:30.201
to abuse. In fact transfer
learning is very effective
technique that you can use it in

00:34:30.201,00:34:35.206
a very large number of machine
learning use cases. But in
conclusion, it’s relatively easy

00:34:38.843,00:34:45.417
at this time to spoof someone's
voice, and it’s only going to
get easier over time. And just

00:34:45.417,00:34:50.689
as a final note, um even after
we submitted this to DefCon um
some researchers at Google

00:34:50.689,00:34:56.428
published um this paper back in
June of this year. Um so the
idea there is tech uh transfer

00:34:56.428,00:35:00.598
learning from Speaker
Verification to Multispeaker
text to speech Synthesis. We

00:35:00.598,00:35:04.769
just want to note that this is a
very active area of research
generally and we’re not the only

00:35:04.769,00:35:10.909
ones looking into this. Um
basically this entire field is
going to grow at a very alarming

00:35:10.909,00:35:16.114
rate and we should figure out
how to deal with it now. And
with that, that’s actually the

00:35:16.114,00:35:21.119
end of out talk so if anybody
has any questions, definitely
feel free. [Applause]

