1
00:00:00,000 --> 00:00:08,800
[Music]

2
00:00:08,800 --> 00:00:10,000
hello everyone

3
00:00:10,000 --> 00:00:14,080
thank you for joining today to our 23rd

4
00:00:14,080 --> 00:00:18,000
session of hackaday quantum computing

5
00:00:18,000 --> 00:00:19,279
class

6
00:00:19,279 --> 00:00:22,160
as many of you who have been here before

7
00:00:22,160 --> 00:00:23,039
know that

8
00:00:23,039 --> 00:00:26,480
in september we started

9
00:00:26,480 --> 00:00:31,039
the lecture guest lectures so

10
00:00:31,039 --> 00:00:33,760
as usual i would remind people that all

11
00:00:33,760 --> 00:00:34,880
our

12
00:00:34,880 --> 00:00:38,559
content is held on hackaday

13
00:00:38,559 --> 00:00:41,520
projects platform if you search quantum

14
00:00:41,520 --> 00:00:43,600
computing through comics you'll find

15
00:00:43,600 --> 00:00:47,039
our class it's been uh quite a lot of

16
00:00:47,039 --> 00:00:48,960
months that we've been doing this almost

17
00:00:48,960 --> 00:00:50,239
every sunday

18
00:00:50,239 --> 00:00:53,600
we talk about a new pro a new topic

19
00:00:53,600 --> 00:00:56,879
for 30 minutes to an hour and

20
00:00:56,879 --> 00:01:00,480
we also have been using the quantum

21
00:01:00,480 --> 00:01:03,920
documentation from microsoft and uh

22
00:01:03,920 --> 00:01:06,479
the quantum catalyst where we had

23
00:01:06,479 --> 00:01:08,799
several classes that we did coding

24
00:01:08,799 --> 00:01:12,159
through the uh quantum contest exercises

25
00:01:12,159 --> 00:01:12,799
that we have

26
00:01:12,799 --> 00:01:16,240
at microsoft so as i

27
00:01:16,240 --> 00:01:19,360
mentioned we have been having some guest

28
00:01:19,360 --> 00:01:20,799
lectures

29
00:01:20,799 --> 00:01:24,960
and um today we have uh dr maria

30
00:01:24,960 --> 00:01:28,400
shout and we have two more classes

31
00:01:28,400 --> 00:01:31,759
afterwards and i think that would be the

32
00:01:31,759 --> 00:01:34,479
probably we can wind down the class for

33
00:01:34,479 --> 00:01:36,400
this year

34
00:01:36,400 --> 00:01:40,560
so let me introduce you dr mario shout

35
00:01:40,560 --> 00:01:44,000
very quick she is a senior

36
00:01:44,000 --> 00:01:48,000
researcher at cenado i i was actually

37
00:01:48,000 --> 00:01:50,720
lucky to meet some folks in xanadu when

38
00:01:50,720 --> 00:01:51,200
i was

39
00:01:51,200 --> 00:01:54,560
in the conference q2c very nice group of

40
00:01:54,560 --> 00:01:55,520
people there

41
00:01:55,520 --> 00:01:57,360
they are a toronto based quantum

42
00:01:57,360 --> 00:01:58,799
computing startup

43
00:01:58,799 --> 00:02:01,439
doing some very exciting work and maria

44
00:02:01,439 --> 00:02:02,240
is also

45
00:02:02,240 --> 00:02:05,280
part of the big data and uh informatics

46
00:02:05,280 --> 00:02:07,360
flagship hello folks i don't seem to

47
00:02:07,360 --> 00:02:08,318
hear anything

48
00:02:08,318 --> 00:02:11,200
is that oh you won't hear anything um

49
00:02:11,200 --> 00:02:13,040
what about others

50
00:02:13,040 --> 00:02:16,400
i can hear you perfectly fine yeah i can

51
00:02:16,400 --> 00:02:17,599
hear fine

52
00:02:17,599 --> 00:02:19,760
so other people can hear so maybe it's

53
00:02:19,760 --> 00:02:20,840
on your side

54
00:02:20,840 --> 00:02:24,319
sorry uh let's

55
00:02:24,319 --> 00:02:26,400
yeah i'll move on if you missed anything

56
00:02:26,400 --> 00:02:28,560
we have recorded this you can go back to

57
00:02:28,560 --> 00:02:29,440
it

58
00:02:29,440 --> 00:02:32,160
um so maria is also part of the

59
00:02:32,160 --> 00:02:33,120
university

60
00:02:33,120 --> 00:02:36,480
of waterloo natto in durban

61
00:02:36,480 --> 00:02:40,080
in south africa and she also

62
00:02:40,080 --> 00:02:43,920
is a author not very different from the

63
00:02:43,920 --> 00:02:44,560
book i

64
00:02:44,560 --> 00:02:48,080
published but maria wrote a

65
00:02:48,080 --> 00:02:50,560
textbook supervised learning with

66
00:02:50,560 --> 00:02:52,720
quantum computers

67
00:02:52,720 --> 00:02:55,280
and she also has been working on the

68
00:02:55,280 --> 00:02:57,680
penny lane software framework for

69
00:02:57,680 --> 00:02:59,840
quantum machine learning which is a

70
00:02:59,840 --> 00:03:02,159
xanadu

71
00:03:02,159 --> 00:03:06,319
framework using python library

72
00:03:06,319 --> 00:03:08,640
so we're very excited to have her here

73
00:03:08,640 --> 00:03:10,080
today and

74
00:03:10,080 --> 00:03:11,840
quantum machine learning is definitely a

75
00:03:11,840 --> 00:03:13,599
very exciting topic

76
00:03:13,599 --> 00:03:15,519
both quantum computing and machine

77
00:03:15,519 --> 00:03:17,280
learning are developing fields

78
00:03:17,280 --> 00:03:19,040
so it will be very interesting to see

79
00:03:19,040 --> 00:03:21,760
how these two fuse together

80
00:03:21,760 --> 00:03:25,360
so before i give the

81
00:03:25,360 --> 00:03:29,680
slide to maria microsoft also has a

82
00:03:29,680 --> 00:03:31,440
quantum machine learning library

83
00:03:31,440 --> 00:03:34,000
if you go to the documentation you'll be

84
00:03:34,000 --> 00:03:36,000
able to find it

85
00:03:36,000 --> 00:03:39,440
very easily in the q sharp library

86
00:03:39,440 --> 00:03:43,599
list so i'm super excited interested to

87
00:03:43,599 --> 00:03:46,720
hear and learn from maria today and

88
00:03:46,720 --> 00:03:49,200
and then look at our own documentation

89
00:03:49,200 --> 00:03:50,239
and

90
00:03:50,239 --> 00:03:54,000
compare the methods uh also a reminder

91
00:03:54,000 --> 00:03:54,560
that we

92
00:03:54,560 --> 00:03:57,040
just published a new set of learning

93
00:03:57,040 --> 00:03:57,840
modules

94
00:03:57,840 --> 00:04:01,439
on ms learn so this i highly recommend

95
00:04:01,439 --> 00:04:03,360
everyone who's coming to this class to

96
00:04:03,360 --> 00:04:04,879
take these

97
00:04:04,879 --> 00:04:08,400
courses these are for you to

98
00:04:08,400 --> 00:04:12,480
start practically using quantum computer

99
00:04:12,480 --> 00:04:15,280
so we start with no prerequisites and we

100
00:04:15,280 --> 00:04:16,880
have a learning path for you to get

101
00:04:16,880 --> 00:04:17,918
hands-on with

102
00:04:17,918 --> 00:04:20,959
the qdk and q-sharp

103
00:04:20,959 --> 00:04:23,919
and gradually you build up your skills

104
00:04:23,919 --> 00:04:26,960
to doing some group research

105
00:04:26,960 --> 00:04:29,840
exercises this is in addition to the

106
00:04:29,840 --> 00:04:31,919
cutters we have been using and this is a

107
00:04:31,919 --> 00:04:33,040
step-by-step

108
00:04:33,040 --> 00:04:36,080
course material that we curated

109
00:04:36,080 --> 00:04:38,160
and we are constantly adding more

110
00:04:38,160 --> 00:04:40,000
materials

111
00:04:40,000 --> 00:04:42,320
for anything any question about this

112
00:04:42,320 --> 00:04:43,280
class

113
00:04:43,280 --> 00:04:45,600
during the week any time if you have

114
00:04:45,600 --> 00:04:46,560
questions

115
00:04:46,560 --> 00:04:50,240
post them in the hackaday project

116
00:04:50,240 --> 00:04:52,880
in the comment section and also

117
00:04:52,880 --> 00:04:54,479
frequently ask the question

118
00:04:54,479 --> 00:04:58,320
we record the class and it will be on

119
00:04:58,320 --> 00:04:59,040
youtube

120
00:04:59,040 --> 00:05:02,000
shortly after this class also

121
00:05:02,000 --> 00:05:03,919
announcement for next week

122
00:05:03,919 --> 00:05:06,800
so we were scheduling professor chris

123
00:05:06,800 --> 00:05:07,840
ferry

124
00:05:07,840 --> 00:05:10,960
for october 25th but then

125
00:05:10,960 --> 00:05:13,919
we had some schedule change and uh 18th

126
00:05:13,919 --> 00:05:14,479
is

127
00:05:14,479 --> 00:05:17,600
open now so i can't wait to hear chris

128
00:05:17,600 --> 00:05:18,000
talk

129
00:05:18,000 --> 00:05:20,160
so i asked him to come speak to us

130
00:05:20,160 --> 00:05:21,280
earlier so

131
00:05:21,280 --> 00:05:24,800
chris will be here on october 18th

132
00:05:24,800 --> 00:05:26,800
and please note the time change it's

133
00:05:26,800 --> 00:05:29,039
different from today it will be later

134
00:05:29,039 --> 00:05:32,400
so if you use pacific time it's 2pm

135
00:05:32,400 --> 00:05:35,919
so please make a note of that you will

136
00:05:35,919 --> 00:05:38,160
if you show up at this time you won't

137
00:05:38,160 --> 00:05:39,120
see anyone

138
00:05:39,120 --> 00:05:41,520
but we gotta move it to a later time so

139
00:05:41,520 --> 00:05:44,720
the topic will be on quantum tomography

140
00:05:44,720 --> 00:05:47,280
and then the week after on the 25th we

141
00:05:47,280 --> 00:05:48,320
will take a break

142
00:05:48,320 --> 00:05:49,919
no class then but then we'll have

143
00:05:49,919 --> 00:05:52,160
another one on november 2nd

144
00:05:52,160 --> 00:05:55,840
all right thank you maria for joining us

145
00:05:55,840 --> 00:06:00,318
i would let you take the stage

146
00:06:01,600 --> 00:06:05,039
thank you let me try to share my screen

147
00:06:05,039 --> 00:06:07,440
and if you can't hear me for some reason

148
00:06:07,440 --> 00:06:09,600
if south african internet is too bad

149
00:06:09,600 --> 00:06:11,039
then just let me know and

150
00:06:11,039 --> 00:06:14,240
we'll try to fix it um okay cool you can

151
00:06:14,240 --> 00:06:14,800
see my

152
00:06:14,800 --> 00:06:17,840
screen i guess if you don't complain yes

153
00:06:17,840 --> 00:06:21,759
we see you now

154
00:06:21,759 --> 00:06:25,199
great okay cool so um

155
00:06:25,199 --> 00:06:28,080
first of all um thanks for joining i was

156
00:06:28,080 --> 00:06:28,800
here already

157
00:06:28,800 --> 00:06:32,080
last week sunday like just uh 20 past

158
00:06:32,080 --> 00:06:33,520
eight or something i was sitting on my

159
00:06:33,520 --> 00:06:35,120
couch and i thought like oh my gosh

160
00:06:35,120 --> 00:06:36,960
i've got to give a talk now and then i

161
00:06:36,960 --> 00:06:38,479
realized actually this week which might

162
00:06:38,479 --> 00:06:40,479
be very happy

163
00:06:40,479 --> 00:06:42,800
um so i want to speak about quantum

164
00:06:42,800 --> 00:06:44,000
machine learning being

165
00:06:44,000 --> 00:06:45,680
very cognizant that the audience might

166
00:06:45,680 --> 00:06:48,000
not be full of people who had quantum

167
00:06:48,000 --> 00:06:49,840
mechanics 101

168
00:06:49,840 --> 00:06:52,160
and i guess from the course that you

169
00:06:52,160 --> 00:06:53,680
have seen before what you

170
00:06:53,680 --> 00:06:55,680
expect now is me to go through an

171
00:06:55,680 --> 00:06:57,360
algorithm or two with lots of direct

172
00:06:57,360 --> 00:06:59,520
notation lots of circuit diagrams and

173
00:06:59,520 --> 00:07:01,759
convince you that quantum computing and

174
00:07:01,759 --> 00:07:03,360
this algorithm in particular can now

175
00:07:03,360 --> 00:07:05,440
speed up machine learning in some sense

176
00:07:05,440 --> 00:07:08,160
and this is not what we'll do today

177
00:07:08,160 --> 00:07:09,039
partially because

178
00:07:09,039 --> 00:07:11,199
um my personal research over the last

179
00:07:11,199 --> 00:07:12,880
like seven years or eight years has

180
00:07:12,880 --> 00:07:14,560
brought me quite far away from this

181
00:07:14,560 --> 00:07:16,319
question of like how can we speed up

182
00:07:16,319 --> 00:07:18,160
quantum or how can we speed up machine

183
00:07:18,160 --> 00:07:20,080
learning with quantum computers

184
00:07:20,080 --> 00:07:22,400
um what i want to talk about is a

185
00:07:22,400 --> 00:07:24,319
question that the community has now i

186
00:07:24,319 --> 00:07:24,800
mean

187
00:07:24,800 --> 00:07:26,960
predominantly accepted as like a really

188
00:07:26,960 --> 00:07:28,560
really interesting research question and

189
00:07:28,560 --> 00:07:30,160
a lot of resources are like spent on on

190
00:07:30,160 --> 00:07:31,680
this question which is

191
00:07:31,680 --> 00:07:33,440
if we take a quantum computer very

192
00:07:33,440 --> 00:07:35,120
generic quantum computer

193
00:07:35,120 --> 00:07:36,880
on quantum computation as a machine

194
00:07:36,880 --> 00:07:38,960
learning model what happens how can we

195
00:07:38,960 --> 00:07:40,319
train it how can we

196
00:07:40,319 --> 00:07:42,880
predict with with this model how can we

197
00:07:42,880 --> 00:07:44,560
understand what actually mathematically

198
00:07:44,560 --> 00:07:46,000
is happening

199
00:07:46,000 --> 00:07:49,120
so um so today

200
00:07:49,120 --> 00:07:51,599
try to shade a bit your ideas of like

201
00:07:51,599 --> 00:07:53,199
always wanting speed ups today we're

202
00:07:53,199 --> 00:07:55,520
talking a bit more about like um

203
00:07:55,520 --> 00:07:57,280
where does quantum computing actually

204
00:07:57,280 --> 00:07:59,199
fit into this big pipeline in this big

205
00:07:59,199 --> 00:08:00,160
market and this big

206
00:08:00,160 --> 00:08:03,120
um theory and theoretical construct of

207
00:08:03,120 --> 00:08:04,720
machine learning

208
00:08:04,720 --> 00:08:07,759
let me just check yeah

209
00:08:07,759 --> 00:08:09,360
what i want to convince you of so my

210
00:08:09,360 --> 00:08:12,319
basic message is the following and um

211
00:08:12,319 --> 00:08:14,960
yeah i try to explain this with not much

212
00:08:14,960 --> 00:08:16,639
math and this is a bit challenging but

213
00:08:16,639 --> 00:08:18,240
but i hope we get it right and we will

214
00:08:18,240 --> 00:08:20,000
see the topic that comes out of this

215
00:08:20,000 --> 00:08:21,360
and what i want to try to convince you

216
00:08:21,360 --> 00:08:23,759
of is that what makes quantum computing

217
00:08:23,759 --> 00:08:25,919
super interesting if we understand it as

218
00:08:25,919 --> 00:08:27,440
a machine learning model

219
00:08:27,440 --> 00:08:29,360
is that it's kind of a hybrid it's

220
00:08:29,360 --> 00:08:30,879
something that is very similar to neural

221
00:08:30,879 --> 00:08:32,000
networks in that it's

222
00:08:32,000 --> 00:08:34,559
composable it's a trainable model that

223
00:08:34,559 --> 00:08:36,240
fits into all the pipelines that you

224
00:08:36,240 --> 00:08:37,200
know tensorflow

225
00:08:37,200 --> 00:08:39,760
and pytorch and them have already built

226
00:08:39,760 --> 00:08:41,599
but on the other hand what's going on

227
00:08:41,599 --> 00:08:43,440
theoretically is very much like the

228
00:08:43,440 --> 00:08:45,440
methods that people analyzed in the 90s

229
00:08:45,440 --> 00:08:46,800
that machine learning has beautiful

230
00:08:46,800 --> 00:08:48,640
theory for so this is the point i want

231
00:08:48,640 --> 00:08:50,160
to make

232
00:08:50,160 --> 00:08:53,360
um check okay cool so we have to

233
00:08:53,360 --> 00:08:54,800
very briefly talk about machine learning

234
00:08:54,800 --> 00:08:56,240
i just want to collect the things that i

235
00:08:56,240 --> 00:08:58,160
need to make this argument basically

236
00:08:58,160 --> 00:08:59,839
so machine learning to me always has

237
00:08:59,839 --> 00:09:01,519
three ingredients obviously it's a big

238
00:09:01,519 --> 00:09:02,880
landscape but i'm dumping

239
00:09:02,880 --> 00:09:06,160
this down like into a few words here um

240
00:09:06,160 --> 00:09:08,959
we have data um usually modeled as a

241
00:09:08,959 --> 00:09:10,720
data distribution from which we can draw

242
00:09:10,720 --> 00:09:11,680
data

243
00:09:11,680 --> 00:09:13,519
we've got a model the model is the thing

244
00:09:13,519 --> 00:09:15,600
that actually solves the task that we

245
00:09:15,600 --> 00:09:16,240
want to

246
00:09:16,240 --> 00:09:18,560
do in machine learning so for example

247
00:09:18,560 --> 00:09:20,320
the model could be a predictor it takes

248
00:09:20,320 --> 00:09:23,200
data points and predicts for example

249
00:09:23,200 --> 00:09:24,959
if the data is a picture it predicts

250
00:09:24,959 --> 00:09:26,320
what's on the picture it could also be a

251
00:09:26,320 --> 00:09:27,600
generative model that doesn't

252
00:09:27,600 --> 00:09:29,600
necessarily take

253
00:09:29,600 --> 00:09:32,000
inputs of that form but they produce

254
00:09:32,000 --> 00:09:32,880
data

255
00:09:32,880 --> 00:09:35,040
it could also be a translation system

256
00:09:35,040 --> 00:09:37,440
whatever you know you want to make it

257
00:09:37,440 --> 00:09:39,279
and the third ingredient is a cost and

258
00:09:39,279 --> 00:09:40,560
the cost is basically

259
00:09:40,560 --> 00:09:43,120
a function that measures how well does

260
00:09:43,120 --> 00:09:43,920
the model do

261
00:09:43,920 --> 00:09:46,800
on the data the cost is also like a

262
00:09:46,800 --> 00:09:47,920
whole

263
00:09:47,920 --> 00:09:49,839
you know modular thing that you can

264
00:09:49,839 --> 00:09:51,360
change you can analyze what different

265
00:09:51,360 --> 00:09:52,640
cost functions do you can

266
00:09:52,640 --> 00:09:53,920
you can construct the cost with

267
00:09:53,920 --> 00:09:55,600
different terms regularization like

268
00:09:55,600 --> 00:09:56,720
those of you who work in machine

269
00:09:56,720 --> 00:09:57,279
learning

270
00:09:57,279 --> 00:09:59,760
know the pain of this so now what is the

271
00:09:59,760 --> 00:10:01,519
task in machine learning and i think

272
00:10:01,519 --> 00:10:03,519
in early quantum machine learning we or

273
00:10:03,519 --> 00:10:04,720
like a lot of people in the community

274
00:10:04,720 --> 00:10:06,240
got this a little bit wrong

275
00:10:06,240 --> 00:10:10,000
the task is use data samples so examples

276
00:10:10,000 --> 00:10:12,160
a small example set of data to construct

277
00:10:12,160 --> 00:10:14,160
a model that minimizes the cost on

278
00:10:14,160 --> 00:10:15,600
unseen data and this is important

279
00:10:15,600 --> 00:10:17,040
because what we got wrong or what many

280
00:10:17,040 --> 00:10:18,560
people get wrong in quantum computing

281
00:10:18,560 --> 00:10:19,920
when they just start to engage with the

282
00:10:19,920 --> 00:10:20,640
topic

283
00:10:20,640 --> 00:10:22,320
they think machine learning is to use

284
00:10:22,320 --> 00:10:24,160
data samples to construct a model that

285
00:10:24,160 --> 00:10:26,480
minimizes the cost on the data samples

286
00:10:26,480 --> 00:10:28,320
so on the small data data set on the

287
00:10:28,320 --> 00:10:30,959
training set that i basically get given

288
00:10:30,959 --> 00:10:32,560
but the task of machine learning is not

289
00:10:32,560 --> 00:10:34,000
the optimization itself

290
00:10:34,000 --> 00:10:36,320
it is the generalization afterwards so

291
00:10:36,320 --> 00:10:37,839
you want to build a model

292
00:10:37,839 --> 00:10:39,120
that does not necessarily only perform

293
00:10:39,120 --> 00:10:41,200
well on the data you have but it has to

294
00:10:41,200 --> 00:10:42,800
perform more on data you've never seen

295
00:10:42,800 --> 00:10:43,120
so

296
00:10:43,120 --> 00:10:45,279
if you've got a translation system if

297
00:10:45,279 --> 00:10:46,240
you train it with

298
00:10:46,240 --> 00:10:48,480
200 words you wanted to also start

299
00:10:48,480 --> 00:10:49,279
understanding

300
00:10:49,279 --> 00:10:51,600
language that it hasn't heard or even a

301
00:10:51,600 --> 00:10:53,360
language it has never been trained on

302
00:10:53,360 --> 00:10:56,959
so generalization is like the big thing

303
00:10:56,959 --> 00:10:59,440
um when you open then a textbook like

304
00:10:59,440 --> 00:11:00,959
the the beginning is always like

305
00:11:00,959 --> 00:11:02,959
uh linear models although linear models

306
00:11:02,959 --> 00:11:04,560
i mean linear models are something that

307
00:11:04,560 --> 00:11:06,160
we know from statistics and also in

308
00:11:06,160 --> 00:11:06,959
physics like

309
00:11:06,959 --> 00:11:09,519
for hundreds of years but however even

310
00:11:09,519 --> 00:11:11,120
if you go to proofs of

311
00:11:11,120 --> 00:11:13,519
hectic deep learning models and in crazy

312
00:11:13,519 --> 00:11:14,399
papers

313
00:11:14,399 --> 00:11:15,920
often people actually go back to the

314
00:11:15,920 --> 00:11:17,519
linear model so they're quite a fruit

315
00:11:17,519 --> 00:11:19,440
fly example for machine learning

316
00:11:19,440 --> 00:11:20,959
what happens here i probably don't have

317
00:11:20,959 --> 00:11:22,560
to explain to to many people

318
00:11:22,560 --> 00:11:24,720
is like that linear models basically

319
00:11:24,720 --> 00:11:26,160
just think of

320
00:11:26,160 --> 00:11:29,120
a mathematical function that takes the

321
00:11:29,120 --> 00:11:29,519
input

322
00:11:29,519 --> 00:11:32,240
think of it as a big data vector and

323
00:11:32,240 --> 00:11:33,279
multiplies it

324
00:11:33,279 --> 00:11:34,880
with a weight vector and this weight

325
00:11:34,880 --> 00:11:37,200
vector can be trained or can be chosen

326
00:11:37,200 --> 00:11:39,120
and what happens basically in the space

327
00:11:39,120 --> 00:11:40,720
of data here we now have just two

328
00:11:40,720 --> 00:11:42,320
dimensional data so think of this as

329
00:11:42,320 --> 00:11:42,720
being

330
00:11:42,720 --> 00:11:45,680
two pixels of an image um what this

331
00:11:45,680 --> 00:11:47,200
linear model will do it will create a

332
00:11:47,200 --> 00:11:49,040
linear decision boundary so

333
00:11:49,040 --> 00:11:50,560
everything in the data space that's on

334
00:11:50,560 --> 00:11:52,399
the left side of this of this

335
00:11:52,399 --> 00:11:54,399
hyperplane i mean in higher dimension

336
00:11:54,399 --> 00:11:55,600
spaces

337
00:11:55,600 --> 00:11:58,160
will be for example classified as is one

338
00:11:58,160 --> 00:12:00,240
class if i put a step function on the

339
00:12:00,240 --> 00:12:01,839
result of this model for example and use

340
00:12:01,839 --> 00:12:03,200
this for classification and the other

341
00:12:03,200 --> 00:12:04,560
thing will be classified as the other

342
00:12:04,560 --> 00:12:05,279
cars

343
00:12:05,279 --> 00:12:07,360
so now what's the important point here

344
00:12:07,360 --> 00:12:08,720
those of you have worked with linear

345
00:12:08,720 --> 00:12:09,680
models before know

346
00:12:09,680 --> 00:12:11,760
that if i want to optimize a linear

347
00:12:11,760 --> 00:12:14,160
model i have a really beautiful and very

348
00:12:14,160 --> 00:12:16,399
simple optimization task which is convex

349
00:12:16,399 --> 00:12:17,519
optimization

350
00:12:17,519 --> 00:12:18,959
so without i like going into

351
00:12:18,959 --> 00:12:20,720
mathematical details here

352
00:12:20,720 --> 00:12:22,639
comics optimization you can simply think

353
00:12:22,639 --> 00:12:24,720
of an optimization landscape

354
00:12:24,720 --> 00:12:25,839
something where you want to find the

355
00:12:25,839 --> 00:12:28,160
minimum where you have a global minimum

356
00:12:28,160 --> 00:12:30,079
and you don't have local minima so

357
00:12:30,079 --> 00:12:31,680
basically if you go down you will always

358
00:12:31,680 --> 00:12:33,279
like kind of find your beautiful global

359
00:12:33,279 --> 00:12:34,560
minimum

360
00:12:34,560 --> 00:12:36,399
and what you can also do is you can find

361
00:12:36,399 --> 00:12:38,720
an equation so a closed form equation of

362
00:12:38,720 --> 00:12:41,360
this w vector here that um you know

363
00:12:41,360 --> 00:12:42,560
optimizes this model

364
00:12:42,560 --> 00:12:45,440
so basically theory is very beautiful

365
00:12:45,440 --> 00:12:47,440
computationally

366
00:12:47,440 --> 00:12:50,000
it's so-and-so so um this closed form

367
00:12:50,000 --> 00:12:51,440
equation you can solve on a computer

368
00:12:51,440 --> 00:12:52,160
quite nicely

369
00:12:52,160 --> 00:12:54,560
however the way to solve it for those of

370
00:12:54,560 --> 00:12:56,079
you who remember it

371
00:12:56,079 --> 00:12:58,240
is that you construct a big data matrix

372
00:12:58,240 --> 00:13:00,000
um out of your data and then you have to

373
00:13:00,000 --> 00:13:02,000
invert that matrix and matrix inversion

374
00:13:02,000 --> 00:13:02,480
is

375
00:13:02,480 --> 00:13:04,800
you know it's efficient so it has a

376
00:13:04,800 --> 00:13:06,560
polynomial runtime algorithm obviously

377
00:13:06,560 --> 00:13:06,880
like

378
00:13:06,880 --> 00:13:08,720
quadratic in the number of data or a bit

379
00:13:08,720 --> 00:13:10,959
more than quadratic actually

380
00:13:10,959 --> 00:13:12,560
but for machine learning standards like

381
00:13:12,560 --> 00:13:14,160
being quadratic in the

382
00:13:14,160 --> 00:13:15,600
in the number and size of data is

383
00:13:15,600 --> 00:13:17,200
actually quite big especially when we

384
00:13:17,200 --> 00:13:19,600
when we go to bigger data

385
00:13:19,600 --> 00:13:21,200
so how did machine learning solve this

386
00:13:21,200 --> 00:13:22,800
and i still need like all of this

387
00:13:22,800 --> 00:13:24,639
narrative basically to like say where

388
00:13:24,639 --> 00:13:26,959
quantum computing fits in

389
00:13:26,959 --> 00:13:29,680
um oh sorry not how they solved it they

390
00:13:29,680 --> 00:13:30,560
haven't solved it

391
00:13:30,560 --> 00:13:33,120
that's the wrong uh line here what i

392
00:13:33,120 --> 00:13:34,000
want to say is

393
00:13:34,000 --> 00:13:36,240
um so now we obviously have a problem

394
00:13:36,240 --> 00:13:38,000
that linear models can only

395
00:13:38,000 --> 00:13:39,760
classify data in linear decision

396
00:13:39,760 --> 00:13:41,279
managers so they're rather stupid models

397
00:13:41,279 --> 00:13:42,720
so they can't do very much people have

398
00:13:42,720 --> 00:13:44,320
figured that out probably in the 40s and

399
00:13:44,320 --> 00:13:45,680
50s

400
00:13:45,680 --> 00:13:48,079
it was the famous xor problem if people

401
00:13:48,079 --> 00:13:48,880
remember

402
00:13:48,880 --> 00:13:53,040
and um so but what happens is that you

403
00:13:53,040 --> 00:13:54,079
can actually give this

404
00:13:54,079 --> 00:13:57,040
very simple model a lot of power by

405
00:13:57,040 --> 00:13:57,600
mapping

406
00:13:57,600 --> 00:13:59,680
the data with a linear map into a really

407
00:13:59,680 --> 00:14:01,519
high dimensional space

408
00:14:01,519 --> 00:14:02,800
and that means that in your original

409
00:14:02,800 --> 00:14:04,240
space you can actually have models that

410
00:14:04,240 --> 00:14:05,680
are more complicated which means

411
00:14:05,680 --> 00:14:07,360
decision boundaries if we think about

412
00:14:07,360 --> 00:14:09,120
classification for now that are very

413
00:14:09,120 --> 00:14:10,399
complex

414
00:14:10,399 --> 00:14:12,639
um the beauty of it we're still doing

415
00:14:12,639 --> 00:14:14,639
convex optimization because i don't

416
00:14:14,639 --> 00:14:17,760
have a non-linearity in my data um

417
00:14:17,760 --> 00:14:19,680
so this means we are doing this context

418
00:14:19,680 --> 00:14:20,959
optimization in a very very high

419
00:14:20,959 --> 00:14:22,880
dimensional space now

420
00:14:22,880 --> 00:14:24,320
and this high dimensional space in this

421
00:14:24,320 --> 00:14:25,839
high dimensional space we're still doing

422
00:14:25,839 --> 00:14:27,199
something linear but in the original

423
00:14:27,199 --> 00:14:29,839
space we're not

424
00:14:29,839 --> 00:14:31,519
now computationally we're now doing

425
00:14:31,519 --> 00:14:33,199
something you know that was already not

426
00:14:33,199 --> 00:14:34,720
so easy and now in a really high

427
00:14:34,720 --> 00:14:36,000
dimensional space so we're actually

428
00:14:36,000 --> 00:14:37,199
getting more and more problems

429
00:14:37,199 --> 00:14:39,120
and the beauty of kernel methods in the

430
00:14:39,120 --> 00:14:41,440
90s i won't go into why they call kernel

431
00:14:41,440 --> 00:14:44,240
methods is a bit more complicated but is

432
00:14:44,240 --> 00:14:46,399
to find out that actually this context

433
00:14:46,399 --> 00:14:48,399
optimization problem can be solved in

434
00:14:48,399 --> 00:14:49,760
the original space

435
00:14:49,760 --> 00:14:51,279
so i don't have to go to these very high

436
00:14:51,279 --> 00:14:52,639
dimensional spaces it's just a

437
00:14:52,639 --> 00:14:54,320
mathematical construction in the end i

438
00:14:54,320 --> 00:14:56,160
can do all my computation

439
00:14:56,160 --> 00:14:59,199
in the original data space so to

440
00:14:59,199 --> 00:15:00,880
summarize here we made a more

441
00:15:00,880 --> 00:15:02,320
interesting mod a much more interesting

442
00:15:02,320 --> 00:15:03,040
model

443
00:15:03,040 --> 00:15:06,320
beautiful optimization um kind of easy

444
00:15:06,320 --> 00:15:07,920
to solve i still have this problem that

445
00:15:07,920 --> 00:15:08,240
it's

446
00:15:08,240 --> 00:15:09,839
quadratic in the number of data and not

447
00:15:09,839 --> 00:15:11,519
linear so

448
00:15:11,519 --> 00:15:14,399
um yeah doable but maybe not optimal or

449
00:15:14,399 --> 00:15:14,800
not

450
00:15:14,800 --> 00:15:16,320
like super cool so there's not

451
00:15:16,320 --> 00:15:18,880
bottlenecks another last step that i

452
00:15:18,880 --> 00:15:20,000
need here um

453
00:15:20,000 --> 00:15:22,160
is now deep learning so this was

454
00:15:22,160 --> 00:15:23,920
basically the thing that

455
00:15:23,920 --> 00:15:26,079
took over from kernel methods maybe from

456
00:15:26,079 --> 00:15:28,160
a similar paper like in 2006 but

457
00:15:28,160 --> 00:15:28,800
probably

458
00:15:28,800 --> 00:15:31,279
2010 like things started to get rolling

459
00:15:31,279 --> 00:15:32,639
properly

460
00:15:32,639 --> 00:15:34,800
and what we have here is usually also

461
00:15:34,800 --> 00:15:36,720
quite a particular type of data so we've

462
00:15:36,720 --> 00:15:39,199
got big data we've got lots of data

463
00:15:39,199 --> 00:15:42,000
we often have data of a specific kind so

464
00:15:42,000 --> 00:15:43,920
deep learning started off with

465
00:15:43,920 --> 00:15:45,519
image recognition which was the first

466
00:15:45,519 --> 00:15:48,079
problem it really solved

467
00:15:48,079 --> 00:15:51,440
the models that we use um well

468
00:15:51,440 --> 00:15:52,560
most of you know there are neural

469
00:15:52,560 --> 00:15:54,639
networks so they are basically um

470
00:15:54,639 --> 00:15:57,199
produced by taking this linear model and

471
00:15:57,199 --> 00:15:58,240
not taking pic

472
00:15:58,240 --> 00:16:00,079
taking the non-linearity out of it so

473
00:16:00,079 --> 00:16:02,320
basically now we are non-linear also in

474
00:16:02,320 --> 00:16:02,880
the

475
00:16:02,880 --> 00:16:06,160
w and then we compose this block over

476
00:16:06,160 --> 00:16:07,360
and over again

477
00:16:07,360 --> 00:16:09,120
and so i actually don't even have to

478
00:16:09,120 --> 00:16:10,720
talk about deep learning

479
00:16:10,720 --> 00:16:11,839
by saying something about neural

480
00:16:11,839 --> 00:16:14,160
networks i mean most people do but

481
00:16:14,160 --> 00:16:16,000
what makes deep learning so crazy or so

482
00:16:16,000 --> 00:16:17,360
cool is that

483
00:16:17,360 --> 00:16:19,759
what it uses as models is a model type

484
00:16:19,759 --> 00:16:21,680
that's composable so you can stack them

485
00:16:21,680 --> 00:16:22,480
you can

486
00:16:22,480 --> 00:16:24,480
make an image recognition system stack

487
00:16:24,480 --> 00:16:26,480
it to translation system and do crazy

488
00:16:26,480 --> 00:16:28,399
crazy things

489
00:16:28,399 --> 00:16:31,360
and the model is differentiable okay i

490
00:16:31,360 --> 00:16:32,480
have to explain this like

491
00:16:32,480 --> 00:16:34,240
what i mean by differentiable here i

492
00:16:34,240 --> 00:16:35,600
mean models

493
00:16:35,600 --> 00:16:37,120
in some sense are differentiable if i

494
00:16:37,120 --> 00:16:38,720
can compute um

495
00:16:38,720 --> 00:16:40,800
how the output changes with respect to

496
00:16:40,800 --> 00:16:42,079
parameters so in this

497
00:16:42,079 --> 00:16:44,639
case like here these w's um but what i

498
00:16:44,639 --> 00:16:45,759
mean here is actually that it's very

499
00:16:45,759 --> 00:16:47,120
efficiently differentiable

500
00:16:47,120 --> 00:16:48,720
and this has been turned back

501
00:16:48,720 --> 00:16:50,320
propagation in your network so

502
00:16:50,320 --> 00:16:51,759
this just means there's an algorithm

503
00:16:51,759 --> 00:16:53,440
that can really really efficiently like

504
00:16:53,440 --> 00:16:54,959
optimize these models

505
00:16:54,959 --> 00:16:57,839
and this algorithm is in general a

506
00:16:57,839 --> 00:16:59,279
gradient based method so a gradient

507
00:16:59,279 --> 00:17:00,959
descent based method

508
00:17:00,959 --> 00:17:03,040
why do i need this like really good

509
00:17:03,040 --> 00:17:04,160
optimization method

510
00:17:04,160 --> 00:17:05,439
and why do i actually need gradient

511
00:17:05,439 --> 00:17:07,359
descent here is because there's

512
00:17:07,359 --> 00:17:08,880
non-linearity switched now

513
00:17:08,880 --> 00:17:12,400
and it includes the weights we now don't

514
00:17:12,400 --> 00:17:14,160
have this beautiful convex optimization

515
00:17:14,160 --> 00:17:16,000
problem anymore we have a very difficult

516
00:17:16,000 --> 00:17:18,000
non-convex optimization problem if our

517
00:17:18,000 --> 00:17:19,599
data is very high dimensional

518
00:17:19,599 --> 00:17:21,679
we have a lot of weights in our model

519
00:17:21,679 --> 00:17:23,679
it's a really really hectic optimization

520
00:17:23,679 --> 00:17:25,039
problem that's non-convex

521
00:17:25,039 --> 00:17:27,119
non-convex poor for this talk just means

522
00:17:27,119 --> 00:17:28,880
difficult

523
00:17:28,880 --> 00:17:30,640
and now what enable deep learning really

524
00:17:30,640 --> 00:17:32,480
is this type of model

525
00:17:32,480 --> 00:17:34,799
this type of algorithm to optimize this

526
00:17:34,799 --> 00:17:36,240
model although the cost function looks

527
00:17:36,240 --> 00:17:37,600
really horrible the landscape is really

528
00:17:37,600 --> 00:17:39,440
horrible to optimize

529
00:17:39,440 --> 00:17:41,520
but also these two things here which is

530
00:17:41,520 --> 00:17:42,799
high performance hardware so we're

531
00:17:42,799 --> 00:17:44,799
talking about gpus we're talking about

532
00:17:44,799 --> 00:17:47,520
big high performance computer and so

533
00:17:47,520 --> 00:17:48,240
also like

534
00:17:48,240 --> 00:17:50,000
server farms that sit somewhere and

535
00:17:50,000 --> 00:17:51,679
optimize your models

536
00:17:51,679 --> 00:17:54,480
and what really also enabled this field

537
00:17:54,480 --> 00:17:54,799
is

538
00:17:54,799 --> 00:17:56,240
special purpose software and i'm talking

539
00:17:56,240 --> 00:17:57,919
here about the likes of of tensorflow

540
00:17:57,919 --> 00:17:59,120
and pytorch and

541
00:17:59,120 --> 00:18:02,880
mxnet and jax or whatever you want

542
00:18:02,880 --> 00:18:06,000
this is software that is written

543
00:18:06,000 --> 00:18:08,799
to basically compose models that are

544
00:18:08,799 --> 00:18:10,320
into indifferentiable

545
00:18:10,320 --> 00:18:12,559
and then differentiate them so basically

546
00:18:12,559 --> 00:18:14,000
you just have to write an algorithm

547
00:18:14,000 --> 00:18:16,160
which is your model you can write also a

548
00:18:16,160 --> 00:18:17,840
cost function on top you write like your

549
00:18:17,840 --> 00:18:19,520
entire algorithm in a language

550
00:18:19,520 --> 00:18:21,520
such that your computer knows exactly

551
00:18:21,520 --> 00:18:23,360
how to compute derivatives of any

552
00:18:23,360 --> 00:18:24,720
parameter you used anywhere in your

553
00:18:24,720 --> 00:18:25,679
algorithm

554
00:18:25,679 --> 00:18:27,280
of the output with respect to any of

555
00:18:27,280 --> 00:18:28,720
those parameters

556
00:18:28,720 --> 00:18:30,320
so this is what makes deep learning

557
00:18:30,320 --> 00:18:31,760
successful

558
00:18:31,760 --> 00:18:34,240
oh by the way and people are you know

559
00:18:34,240 --> 00:18:35,760
wondering at the moment why this whole

560
00:18:35,760 --> 00:18:36,559
thing works and

561
00:18:36,559 --> 00:18:38,240
um the only thing in the last few years

562
00:18:38,240 --> 00:18:39,679
that people can say really is

563
00:18:39,679 --> 00:18:41,520
it's an interplay of all three methods

564
00:18:41,520 --> 00:18:43,039
all three things you have to have the

565
00:18:43,039 --> 00:18:44,559
data you have to have this

566
00:18:44,559 --> 00:18:46,000
type of model and you have to have this

567
00:18:46,000 --> 00:18:47,440
gradient descent algorithm you can't

568
00:18:47,440 --> 00:18:48,480
really

569
00:18:48,480 --> 00:18:50,000
make up a theory that only depends on

570
00:18:50,000 --> 00:18:52,160
one of them

571
00:18:52,160 --> 00:18:54,000
okay cool now okay i've spoken a lot

572
00:18:54,000 --> 00:18:55,120
about machine learning but how can

573
00:18:55,120 --> 00:18:56,880
actually quantum computing help

574
00:18:56,880 --> 00:18:58,480
and again there are lots of different

575
00:18:58,480 --> 00:19:00,240
ways so for example the the

576
00:19:00,240 --> 00:19:03,440
first like ideas maybe in the 2000s were

577
00:19:03,440 --> 00:19:05,200
thinking very information theoretically

578
00:19:05,200 --> 00:19:06,320
about this

579
00:19:06,320 --> 00:19:08,240
for example and there's a beautiful

580
00:19:08,240 --> 00:19:10,400
review summary of this work is like

581
00:19:10,400 --> 00:19:13,120
maybe if you had kind of quantum oracles

582
00:19:13,120 --> 00:19:14,160
maybe you could have

583
00:19:14,160 --> 00:19:15,679
you know you can do something with query

584
00:19:15,679 --> 00:19:16,960
complexity i don't know if you've heard

585
00:19:16,960 --> 00:19:18,000
this term but

586
00:19:18,000 --> 00:19:19,360
what it translates to in machine

587
00:19:19,360 --> 00:19:21,520
learning means i don't have to query my

588
00:19:21,520 --> 00:19:23,520
data oracle very often so maybe i can

589
00:19:23,520 --> 00:19:24,000
learn from

590
00:19:24,000 --> 00:19:27,200
you from fewer data samples there's also

591
00:19:27,200 --> 00:19:28,480
since the beginning of quantum machine

592
00:19:28,480 --> 00:19:29,440
learning um

593
00:19:29,440 --> 00:19:31,600
which is a term that has been only used

594
00:19:31,600 --> 00:19:33,280
probably for five or six years

595
00:19:33,280 --> 00:19:34,960
now properly i mean before it was more

596
00:19:34,960 --> 00:19:37,280
like scattered papers that looked into

597
00:19:37,280 --> 00:19:38,960
this but there's also this like kind of

598
00:19:38,960 --> 00:19:41,360
idea to process quantum data

599
00:19:41,360 --> 00:19:43,520
i was chatting about it but earlier what

600
00:19:43,520 --> 00:19:45,280
i mean by this quantum data is a word

601
00:19:45,280 --> 00:19:46,720
people use it for lots of different

602
00:19:46,720 --> 00:19:48,720
things but what i mean by this is have

603
00:19:48,720 --> 00:19:49,840
actually um

604
00:19:49,840 --> 00:19:51,280
quantum systems that carry quantum

605
00:19:51,280 --> 00:19:53,600
information and use now a quantum

606
00:19:53,600 --> 00:19:55,200
algorithm on them for example a

607
00:19:55,200 --> 00:19:57,120
photon send it into a quantum computer

608
00:19:57,120 --> 00:19:59,200
that's photonic and start experimenting

609
00:19:59,200 --> 00:19:59,600
with

610
00:19:59,600 --> 00:20:01,600
this data and start to analyze it and

611
00:20:01,600 --> 00:20:04,320
why i put it as a reference for claw is

612
00:20:04,320 --> 00:20:05,679
lots of people talk about this all the

613
00:20:05,679 --> 00:20:07,760
time it might be the coolest application

614
00:20:07,760 --> 00:20:09,120
of quantum machine learning

615
00:20:09,120 --> 00:20:11,120
there are some papers that also like you

616
00:20:11,120 --> 00:20:13,919
know have some ideas but i think

617
00:20:13,919 --> 00:20:16,400
something that is really like proper so

618
00:20:16,400 --> 00:20:18,320
this this part is not really gone yet i

619
00:20:18,320 --> 00:20:19,600
think

620
00:20:19,600 --> 00:20:22,000
okay second idea the cost side of things

621
00:20:22,000 --> 00:20:23,600
and the optimization side of things we

622
00:20:23,600 --> 00:20:25,120
can try to improve it and that was

623
00:20:25,120 --> 00:20:26,720
the beginning of quantum machinery a few

624
00:20:26,720 --> 00:20:28,480
years back people tried to

625
00:20:28,480 --> 00:20:30,400
basically just speed up the optimization

626
00:20:30,400 --> 00:20:32,400
part so for example this paper here was

627
00:20:32,400 --> 00:20:33,520
one of the very first one

628
00:20:33,520 --> 00:20:35,200
nathan bieber also like worked in the

629
00:20:35,200 --> 00:20:36,720
microsoft group for a long time

630
00:20:36,720 --> 00:20:40,880
and um so he just took um a linear model

631
00:20:40,880 --> 00:20:42,720
remember the matrix inversion that i was

632
00:20:42,720 --> 00:20:44,320
talking about and did it on the linear

633
00:20:44,320 --> 00:20:46,159
model the second paper took the second

634
00:20:46,159 --> 00:20:47,520
model the kernel method

635
00:20:47,520 --> 00:20:49,039
a very specific one a support vector

636
00:20:49,039 --> 00:20:50,799
machine and did the kernel

637
00:20:50,799 --> 00:20:53,360
sorry did the inversion problem here

638
00:20:53,360 --> 00:20:54,799
also on a quantum computer

639
00:20:54,799 --> 00:20:57,280
or proposed algorithms that did that

640
00:20:57,280 --> 00:20:59,039
using you might have heard of these like

641
00:20:59,039 --> 00:20:59,360
these

642
00:20:59,360 --> 00:21:02,480
hhl type algorithms for matrix inversion

643
00:21:02,480 --> 00:21:03,840
there's a lot of controversy here

644
00:21:03,840 --> 00:21:06,640
because some of these algorithms use

645
00:21:06,640 --> 00:21:08,640
techniques where you need oracles for

646
00:21:08,640 --> 00:21:09,760
data loading so

647
00:21:09,760 --> 00:21:11,360
if they can really speed up things

648
00:21:11,360 --> 00:21:13,200
exponentially like claimed in the paper

649
00:21:13,200 --> 00:21:15,280
is very tricky to tell and it's it's

650
00:21:15,280 --> 00:21:16,480
hard in machine learning because we

651
00:21:16,480 --> 00:21:17,360
don't only want

652
00:21:17,360 --> 00:21:18,960
things to be fast we want them to make

653
00:21:18,960 --> 00:21:20,720
sense and be useful as well and this

654
00:21:20,720 --> 00:21:22,240
usefulness is very hard to quantify

655
00:21:22,240 --> 00:21:23,360
scientifically

656
00:21:23,360 --> 00:21:25,600
and then also put like another um idea

657
00:21:25,600 --> 00:21:27,120
early idea here to

658
00:21:27,120 --> 00:21:30,320
use um quantum annealers to train

659
00:21:30,320 --> 00:21:33,120
a model called boltzmann machines

660
00:21:33,120 --> 00:21:33,840
another one

661
00:21:33,840 --> 00:21:35,280
i just put this in because i found this

662
00:21:35,280 --> 00:21:36,400
really fascinating but i don't know a

663
00:21:36,400 --> 00:21:37,679
single paper that has really thought

664
00:21:37,679 --> 00:21:39,200
about this maybe there's also quantum

665
00:21:39,200 --> 00:21:40,240
computers could find

666
00:21:40,240 --> 00:21:42,240
better solutions and better solutions

667
00:21:42,240 --> 00:21:44,159
are solutions like models

668
00:21:44,159 --> 00:21:46,559
in the end that generalize better but i

669
00:21:46,559 --> 00:21:48,000
haven't seen like structural work on

670
00:21:48,000 --> 00:21:48,320
this

671
00:21:48,320 --> 00:21:51,120
tell me if you have i want to talk about

672
00:21:51,120 --> 00:21:51,919
this one here

673
00:21:51,919 --> 00:21:54,400
so we tackle the model and obviously

674
00:21:54,400 --> 00:21:55,360
again like

675
00:21:55,360 --> 00:21:56,799
everyone in quantum computing loves

676
00:21:56,799 --> 00:21:58,960
thinking about speed up speed ups so you

677
00:21:58,960 --> 00:21:59,840
can think of like

678
00:21:59,840 --> 00:22:01,919
let's speed up existing models so there

679
00:22:01,919 --> 00:22:03,120
were lots of early there

680
00:22:03,120 --> 00:22:04,720
really a lot of papers on this so this

681
00:22:04,720 --> 00:22:06,880
is just a very random selection

682
00:22:06,880 --> 00:22:08,400
and the one i want to speak about is

683
00:22:08,400 --> 00:22:10,000
actually now as i said not

684
00:22:10,000 --> 00:22:11,440
how can i take a neural network and just

685
00:22:11,440 --> 00:22:12,880
make it faster i want to actually talk

686
00:22:12,880 --> 00:22:13,520
about

687
00:22:13,520 --> 00:22:15,440
how can we design or how can quantum

688
00:22:15,440 --> 00:22:17,120
computing give rise to a completely new

689
00:22:17,120 --> 00:22:18,960
class of models if we understand it as a

690
00:22:18,960 --> 00:22:20,080
model a circuit

691
00:22:20,080 --> 00:22:24,480
what model is that um i think the first

692
00:22:24,480 --> 00:22:25,760
okay you can't never say the first but

693
00:22:25,760 --> 00:22:27,919
like the first time i've seen this type

694
00:22:27,919 --> 00:22:29,280
of thinking is in this paper where

695
00:22:29,280 --> 00:22:30,720
someone said

696
00:22:30,720 --> 00:22:32,880
you know if a boltzmann machine is

697
00:22:32,880 --> 00:22:34,240
something like an icing model

698
00:22:34,240 --> 00:22:35,760
which is like a typical physics model

699
00:22:35,760 --> 00:22:37,440
and we have quantum icing models why

700
00:22:37,440 --> 00:22:38,799
don't we just invent quantum multiple

701
00:22:38,799 --> 00:22:39,520
machines

702
00:22:39,520 --> 00:22:40,799
and then the second paper here is a

703
00:22:40,799 --> 00:22:42,720
summary of this approach that i will

704
00:22:42,720 --> 00:22:43,679
talk about now

705
00:22:43,679 --> 00:22:45,600
or a review paper on that which is this

706
00:22:45,600 --> 00:22:48,000
idea of using a certain type of circuit

707
00:22:48,000 --> 00:22:50,320
okay now let me so the rest of this talk

708
00:22:50,320 --> 00:22:52,240
i will just draw this argument now why

709
00:22:52,240 --> 00:22:53,679
are quantum models interesting or

710
00:22:53,679 --> 00:22:57,280
quantum circuits interesting as models

711
00:22:58,080 --> 00:23:01,280
so first of all there's a realization

712
00:23:01,280 --> 00:23:03,520
there is a certain type of approach to

713
00:23:03,520 --> 00:23:05,039
quantum computing that comes out of

714
00:23:05,039 --> 00:23:07,360
nisk devices so the devices that we use

715
00:23:07,360 --> 00:23:08,400
at the moment

716
00:23:08,400 --> 00:23:10,080
there was basically borne out of

717
00:23:10,080 --> 00:23:12,080
desperation because they can't run

718
00:23:12,080 --> 00:23:14,320
any feasible algorithms not even like 20

719
00:23:14,320 --> 00:23:15,840
gates and then the noise just drowns

720
00:23:15,840 --> 00:23:16,880
everything

721
00:23:16,880 --> 00:23:18,640
so the idea and it came very much from

722
00:23:18,640 --> 00:23:20,559
quantum chemistry is um

723
00:23:20,559 --> 00:23:22,240
why don't we just like train our

724
00:23:22,240 --> 00:23:24,080
circuits a bit like in quantum control

725
00:23:24,080 --> 00:23:26,080
experiments when we have feedback loops

726
00:23:26,080 --> 00:23:26,480
to

727
00:23:26,480 --> 00:23:30,320
find good gates or so um the idea is

728
00:23:30,320 --> 00:23:31,679
take a quantum circuit and just make a

729
00:23:31,679 --> 00:23:34,240
couple of gates trainable so

730
00:23:34,240 --> 00:23:36,400
have them depend on parameters and this

731
00:23:36,400 --> 00:23:38,559
um whole approach to quantum computing

732
00:23:38,559 --> 00:23:40,960
is called variation of circuits

733
00:23:40,960 --> 00:23:42,720
so now the idea of using a variation

734
00:23:42,720 --> 00:23:44,400
circuit as a machine learning model is

735
00:23:44,400 --> 00:23:46,400
absolutely interesting because why

736
00:23:46,400 --> 00:23:47,600
because they are composable and

737
00:23:47,600 --> 00:23:49,279
differentiable and

738
00:23:49,279 --> 00:23:52,400
i will tell you exactly why um in a few

739
00:23:52,400 --> 00:23:55,679
seconds but this means that we can

740
00:23:55,679 --> 00:23:57,440
actually just plug them into the entire

741
00:23:57,440 --> 00:23:59,279
pipeline of deep learning because

742
00:23:59,279 --> 00:24:00,880
we can treat them like new networks we

743
00:24:00,880 --> 00:24:02,799
can do gradient descent on them

744
00:24:02,799 --> 00:24:05,520
we can um i mean so the hardware we're

745
00:24:05,520 --> 00:24:06,799
building is like actually the quantum

746
00:24:06,799 --> 00:24:08,320
hardware here but we can also make use

747
00:24:08,320 --> 00:24:10,000
of the special purpose software so we

748
00:24:10,000 --> 00:24:11,120
can build

749
00:24:11,120 --> 00:24:13,360
libraries for quantum machine learning

750
00:24:13,360 --> 00:24:14,320
that plug into

751
00:24:14,320 --> 00:24:16,480
tensorflow and and pytorch and just

752
00:24:16,480 --> 00:24:18,960
extend them by quantum computing so

753
00:24:18,960 --> 00:24:21,679
this idea really makes quantum computing

754
00:24:21,679 --> 00:24:23,360
ready for machine learning on a much

755
00:24:23,360 --> 00:24:25,360
bigger scale than just coming up with

756
00:24:25,360 --> 00:24:27,679
a sped up a support vector machine this

757
00:24:27,679 --> 00:24:28,480
is like a very

758
00:24:28,480 --> 00:24:30,080
big way of thinking about quantum

759
00:24:30,080 --> 00:24:32,799
computing and machine learning together

760
00:24:32,799 --> 00:24:34,320
let me tell you why i say that these

761
00:24:34,320 --> 00:24:35,919
variation circuits which are just these

762
00:24:35,919 --> 00:24:37,200
trainable quantum circuits are

763
00:24:37,200 --> 00:24:39,279
composable and differentiable

764
00:24:39,279 --> 00:24:41,039
the composable part is probably quite

765
00:24:41,039 --> 00:24:42,480
easy to stomach because

766
00:24:42,480 --> 00:24:44,240
already you know that quantum circuits

767
00:24:44,240 --> 00:24:46,720
are composed of gates so we can just

768
00:24:46,720 --> 00:24:48,480
stack aids or we can repeat

769
00:24:48,480 --> 00:24:51,440
a subroutine or we can um branch out

770
00:24:51,440 --> 00:24:53,760
with a subroutine so

771
00:24:53,760 --> 00:24:55,440
you know like what we do in circuits is

772
00:24:55,440 --> 00:24:57,520
we compose circuits so that's maybe not

773
00:24:57,520 --> 00:24:59,520
so difficult

774
00:24:59,520 --> 00:25:01,279
but why are they differentiable and this

775
00:25:01,279 --> 00:25:02,640
is um

776
00:25:02,640 --> 00:25:04,480
actually not so so clear in the first

777
00:25:04,480 --> 00:25:05,840
moment so let me try to explain this

778
00:25:05,840 --> 00:25:06,480
here so

779
00:25:06,480 --> 00:25:08,559
this is a circuit diagram of you know

780
00:25:08,559 --> 00:25:10,240
just a typical variation of quantum

781
00:25:10,240 --> 00:25:11,679
circuit

782
00:25:11,679 --> 00:25:13,679
if i have an input i encode this input

783
00:25:13,679 --> 00:25:14,880
first so this is this

784
00:25:14,880 --> 00:25:16,960
idea of state preparation in traditional

785
00:25:16,960 --> 00:25:17,919
quantum computing

786
00:25:17,919 --> 00:25:19,279
they've got a couple of gates some of

787
00:25:19,279 --> 00:25:21,200
them depend on parameters i use the same

788
00:25:21,200 --> 00:25:22,799
here this has no meaning i just didn't

789
00:25:22,799 --> 00:25:24,000
want to clutter things and then i

790
00:25:24,000 --> 00:25:25,679
measure

791
00:25:25,679 --> 00:25:28,320
once so if i now plug this in into

792
00:25:28,320 --> 00:25:29,600
tensorflow into a machine learning

793
00:25:29,600 --> 00:25:31,440
pipeline the machine learning pipeline

794
00:25:31,440 --> 00:25:32,960
will go to this computation at some

795
00:25:32,960 --> 00:25:35,039
stage and ask i want to do gradient

796
00:25:35,039 --> 00:25:36,640
descent i need a gradient of your

797
00:25:36,640 --> 00:25:38,559
computation so i need to know what's the

798
00:25:38,559 --> 00:25:40,480
partial derivative of this output here

799
00:25:40,480 --> 00:25:41,919
with respect to any of these

800
00:25:41,919 --> 00:25:45,120
inputs on these parameters here

801
00:25:45,120 --> 00:25:46,720
but it's not i mean you can write this

802
00:25:46,720 --> 00:25:48,559
down on paper but it's not entirely

803
00:25:48,559 --> 00:25:48,960
clear

804
00:25:48,960 --> 00:25:50,960
that these partial derivatives are

805
00:25:50,960 --> 00:25:52,240
things you can actually compute on a

806
00:25:52,240 --> 00:25:53,919
quantum computer

807
00:25:53,919 --> 00:25:57,120
and this was um kind of like um

808
00:25:57,120 --> 00:25:59,679
a whole kind of idea or question that a

809
00:25:59,679 --> 00:26:01,520
lot of papers in succession worked out

810
00:26:01,520 --> 00:26:02,960
more and more and more

811
00:26:02,960 --> 00:26:05,840
um by the way these these two references

812
00:26:05,840 --> 00:26:07,200
here are just from

813
00:26:07,200 --> 00:26:08,799
basically this was actually a paper from

814
00:26:08,799 --> 00:26:10,559
my time at microsoft just

815
00:26:10,559 --> 00:26:13,520
just in reference to kitty um these were

816
00:26:13,520 --> 00:26:13,919
kind of

817
00:26:13,919 --> 00:26:16,400
uh as far as i understand the first two

818
00:26:16,400 --> 00:26:17,360
papers that

819
00:26:17,360 --> 00:26:18,880
i had this idea of using quantum

820
00:26:18,880 --> 00:26:20,480
circuits in machine learning in this way

821
00:26:20,480 --> 00:26:21,440
so i put them here

822
00:26:21,440 --> 00:26:23,919
but now back to differentiation so there

823
00:26:23,919 --> 00:26:25,520
was a whole string of research that

824
00:26:25,520 --> 00:26:27,840
that showed that if we put a quantum

825
00:26:27,840 --> 00:26:29,120
computation as part of a big

826
00:26:29,120 --> 00:26:30,960
computational pipeline that at some

827
00:26:30,960 --> 00:26:32,480
stage asks for gradients

828
00:26:32,480 --> 00:26:34,640
so the quantum node i call now the

829
00:26:34,640 --> 00:26:35,760
quantum computation

830
00:26:35,760 --> 00:26:38,320
has a forward mode so it just like makes

831
00:26:38,320 --> 00:26:40,880
the computation and sends it further

832
00:26:40,880 --> 00:26:42,799
we can show that if it gets asked for

833
00:26:42,799 --> 00:26:44,960
gradients that the same quantum note

834
00:26:44,960 --> 00:26:46,720
just makes a computation

835
00:26:46,720 --> 00:26:49,679
twice and it shifts the gate where the

836
00:26:49,679 --> 00:26:51,679
parameter enters with which i want to

837
00:26:51,679 --> 00:26:53,200
with respect to which i want to

838
00:26:53,200 --> 00:26:55,440
differentiate it just shifts it once to

839
00:26:55,440 --> 00:26:57,440
the one side and one's to the other side

840
00:26:57,440 --> 00:26:59,760
and then it takes the difference between

841
00:26:59,760 --> 00:27:01,200
these classical outcomes of these

842
00:27:01,200 --> 00:27:02,080
circuits

843
00:27:02,080 --> 00:27:03,760
an outcome of the circuit for me is

844
00:27:03,760 --> 00:27:05,440
always an expectation value so

845
00:27:05,440 --> 00:27:07,120
physically this means i run my circuit a

846
00:27:07,120 --> 00:27:08,559
lot of times and then i take some

847
00:27:08,559 --> 00:27:09,360
average

848
00:27:09,360 --> 00:27:12,559
so it's a number basically um this looks

849
00:27:12,559 --> 00:27:14,000
a bit like finite difference rule if you

850
00:27:14,000 --> 00:27:15,360
can still follow but it's uh it's

851
00:27:15,360 --> 00:27:17,200
actually a macroscopic shift so this is

852
00:27:17,200 --> 00:27:19,360
depends on the gate what the shift is

853
00:27:19,360 --> 00:27:21,520
we don't know if every gate quantum gate

854
00:27:21,520 --> 00:27:22,559
has a shift but the

855
00:27:22,559 --> 00:27:25,840
the common ones have um but this allows

856
00:27:25,840 --> 00:27:27,600
us to basically by just running the same

857
00:27:27,600 --> 00:27:28,640
quantitation twice

858
00:27:28,640 --> 00:27:30,320
to get a gradient out of the quantum

859
00:27:30,320 --> 00:27:32,399
node

860
00:27:32,399 --> 00:27:34,399
and this allowed us now to compose

861
00:27:34,399 --> 00:27:36,399
software so for example kitty was saying

862
00:27:36,399 --> 00:27:37,279
i'm i'm

863
00:27:37,279 --> 00:27:38,799
involved in like developing a quantum

864
00:27:38,799 --> 00:27:40,240
machine learning special purpose

865
00:27:40,240 --> 00:27:41,919
software called pennylane

866
00:27:41,919 --> 00:27:43,520
and the idea of this quantum software as

867
00:27:43,520 --> 00:27:45,200
i said is really to plug into machine

868
00:27:45,200 --> 00:27:46,720
learning pipelines so on the left here

869
00:27:46,720 --> 00:27:48,720
you see code of written in pi touch it

870
00:27:48,720 --> 00:27:50,320
could also be tensorflow or

871
00:27:50,320 --> 00:27:52,000
even numpy so there are different ways

872
00:27:52,000 --> 00:27:53,840
you can you can do this where we just

873
00:27:53,840 --> 00:27:55,039
exchanged

874
00:27:55,039 --> 00:27:57,440
the model with a penny lane a quantum

875
00:27:57,440 --> 00:27:58,799
model basically

876
00:27:58,799 --> 00:28:00,840
and so you see you can use the entire

877
00:28:00,840 --> 00:28:03,200
optimization pipeline here

878
00:28:03,200 --> 00:28:05,919
written in in really in pytorch but what

879
00:28:05,919 --> 00:28:07,440
you optimize is now the quantum model so

880
00:28:07,440 --> 00:28:08,559
we don't have to invent all

881
00:28:08,559 --> 00:28:10,000
the wheel again we just use deep

882
00:28:10,000 --> 00:28:12,240
learning to optimize our models which is

883
00:28:12,240 --> 00:28:14,320
really really powerful

884
00:28:14,320 --> 00:28:16,559
okay and now the last little argument

885
00:28:16,559 --> 00:28:18,480
that i want to give you and this is now

886
00:28:18,480 --> 00:28:20,559
this is a bit um maybe my colleagues

887
00:28:20,559 --> 00:28:22,080
would stop here and go into another

888
00:28:22,080 --> 00:28:23,520
direction

889
00:28:23,520 --> 00:28:25,440
but what i find extremely interesting is

890
00:28:25,440 --> 00:28:27,440
the following observation

891
00:28:27,440 --> 00:28:30,399
um let us say now we just like do

892
00:28:30,399 --> 00:28:32,240
something like a duncan experiment so we

893
00:28:32,240 --> 00:28:32,720
just like

894
00:28:32,720 --> 00:28:35,440
um play but around here we had this

895
00:28:35,440 --> 00:28:36,960
circuit here that was the variational

896
00:28:36,960 --> 00:28:38,720
trainable circuit in the middle

897
00:28:38,720 --> 00:28:40,960
but now i interpret the circuit by just

898
00:28:40,960 --> 00:28:42,559
like changing my measurement

899
00:28:42,559 --> 00:28:44,159
and this is like actually something that

900
00:28:44,159 --> 00:28:45,600
happens in the lab all the time

901
00:28:45,600 --> 00:28:47,200
when you do a measurement i mean you

902
00:28:47,200 --> 00:28:48,799
write it on paper as an m

903
00:28:48,799 --> 00:28:50,960
or it's an operator it's an observable

904
00:28:50,960 --> 00:28:52,640
but in the lab you probably have only a

905
00:28:52,640 --> 00:28:53,760
certain type of measurement that a

906
00:28:53,760 --> 00:28:55,120
computer can do

907
00:28:55,120 --> 00:28:56,320
and how do you make a different

908
00:28:56,320 --> 00:28:57,679
measurement out of this very simple

909
00:28:57,679 --> 00:28:59,360
measurement is you just apply a circuit

910
00:28:59,360 --> 00:29:00,240
before and some

911
00:29:00,240 --> 00:29:02,720
post-processing probably after so the

912
00:29:02,720 --> 00:29:04,000
circuit before the measurement can be

913
00:29:04,000 --> 00:29:05,520
interpreted as changing the measurement

914
00:29:05,520 --> 00:29:06,799
basis

915
00:29:06,799 --> 00:29:08,799
so i kind of like just call this entire

916
00:29:08,799 --> 00:29:10,799
like a purple thing that i had before

917
00:29:10,799 --> 00:29:11,200
just

918
00:29:11,200 --> 00:29:14,320
m but if this circuit is trainable it's

919
00:29:14,320 --> 00:29:15,120
actually

920
00:29:15,120 --> 00:29:18,159
now the measurement is trainable and

921
00:29:18,159 --> 00:29:20,399
um why i'm doing this is because now my

922
00:29:20,399 --> 00:29:22,320
algorithm only consists of the so this

923
00:29:22,320 --> 00:29:23,679
is my machine learning algorithm now

924
00:29:23,679 --> 00:29:24,559
right it

925
00:29:24,559 --> 00:29:26,320
consists of data encoding and a

926
00:29:26,320 --> 00:29:27,840
measurement

927
00:29:27,840 --> 00:29:29,840
and now let us try to i mean i'm just

928
00:29:29,840 --> 00:29:31,120
giving you a picture because i wanted to

929
00:29:31,120 --> 00:29:31,760
avoid

930
00:29:31,760 --> 00:29:34,799
too much math but um what is

931
00:29:34,799 --> 00:29:36,240
a data encoding and a measurement

932
00:29:36,240 --> 00:29:38,960
actually doing data encoding basically

933
00:29:38,960 --> 00:29:40,000
takes

934
00:29:40,000 --> 00:29:41,919
classical data for example an image like

935
00:29:41,919 --> 00:29:43,840
all the pixels of an image these values

936
00:29:43,840 --> 00:29:45,840
and maps it to quantum states so it maps

937
00:29:45,840 --> 00:29:48,080
it to a very high dimensional

938
00:29:48,080 --> 00:29:50,720
space somehow and the measurement does

939
00:29:50,720 --> 00:29:51,679
something that is

940
00:29:51,679 --> 00:29:53,200
very similar to a linear decision

941
00:29:53,200 --> 00:29:55,200
boundary actually

942
00:29:55,200 --> 00:29:57,679
so and from just this image you will

943
00:29:57,679 --> 00:29:59,600
remember like the image before of

944
00:29:59,600 --> 00:30:01,120
of the kernel methods where you said

945
00:30:01,120 --> 00:30:02,720
like we map data into high dimensional

946
00:30:02,720 --> 00:30:04,480
spaces and we do something linear and

947
00:30:04,480 --> 00:30:05,520
something very simple

948
00:30:05,520 --> 00:30:07,039
so linear there's like the terms and

949
00:30:07,039 --> 00:30:09,039
conditions come in here where

950
00:30:09,039 --> 00:30:11,440
there's a quadratic factor somewhere so

951
00:30:11,440 --> 00:30:12,799
just like if you are very

952
00:30:12,799 --> 00:30:16,000
um pedantic like you're really shout but

953
00:30:16,000 --> 00:30:17,840
still the math that's going on here with

954
00:30:17,840 --> 00:30:19,279
state preparation and measurement

955
00:30:19,279 --> 00:30:20,880
is very very similar to these kernel

956
00:30:20,880 --> 00:30:22,559
methods and

957
00:30:22,559 --> 00:30:24,720
um so let me just go back these two

958
00:30:24,720 --> 00:30:26,320
pictures these two papers came out

959
00:30:26,320 --> 00:30:27,919
so this one is from us from xanadu this

960
00:30:27,919 --> 00:30:29,520
one's from ibm came also out in very

961
00:30:29,520 --> 00:30:30,960
close succession and

962
00:30:30,960 --> 00:30:33,360
noticed that actually these these data

963
00:30:33,360 --> 00:30:34,960
encoding things in a quantum computer

964
00:30:34,960 --> 00:30:36,720
are basically just a feature map into a

965
00:30:36,720 --> 00:30:38,880
high dimensional space and then we can

966
00:30:38,880 --> 00:30:40,640
you know apply the entire operators of

967
00:30:40,640 --> 00:30:42,240
kernel methods here

968
00:30:42,240 --> 00:30:44,720
and this is how i want to finish and so

969
00:30:44,720 --> 00:30:46,880
as i said this is like kind of

970
00:30:46,880 --> 00:30:48,720
the the argument i want to give why

971
00:30:48,720 --> 00:30:50,640
quantum computing is really interesting

972
00:30:50,640 --> 00:30:52,000
and is more interesting than just

973
00:30:52,000 --> 00:30:53,679
speeding up machine learning

974
00:30:53,679 --> 00:30:55,600
as we know it it can actually create new

975
00:30:55,600 --> 00:30:56,880
things

976
00:30:56,880 --> 00:30:58,480
and just to summarize this argument

977
00:30:58,480 --> 00:31:00,559
again is if we take this variation

978
00:31:00,559 --> 00:31:02,559
circuits as models and we interpret it

979
00:31:02,559 --> 00:31:03,360
as just like

980
00:31:03,360 --> 00:31:05,760
you know we map data you know to a

981
00:31:05,760 --> 00:31:06,880
quantum state

982
00:31:06,880 --> 00:31:10,000
and then we perform a measurement and

983
00:31:10,000 --> 00:31:12,080
the measurement or even the data map

984
00:31:12,080 --> 00:31:13,760
actually both can be

985
00:31:13,760 --> 00:31:16,240
composable and trainable that means that

986
00:31:16,240 --> 00:31:18,000
this model is now in the framework

987
00:31:18,000 --> 00:31:20,640
of deep learning so we can use all the

988
00:31:20,640 --> 00:31:22,799
ideas of non-convex optimization that i

989
00:31:22,799 --> 00:31:24,080
showed you with penny lane

990
00:31:24,080 --> 00:31:27,120
but it is also a model that has the

991
00:31:27,120 --> 00:31:28,960
whole theory of this very beautiful

992
00:31:28,960 --> 00:31:30,320
convex optimization

993
00:31:30,320 --> 00:31:32,000
so we're basically non-convex

994
00:31:32,000 --> 00:31:33,760
optimization is just like basically

995
00:31:33,760 --> 00:31:35,679
think of it as a lazy proxy that we use

996
00:31:35,679 --> 00:31:37,760
because we don't know about this theory

997
00:31:37,760 --> 00:31:39,440
yet we just close our eyes as a black

998
00:31:39,440 --> 00:31:40,799
box we just use it

999
00:31:40,799 --> 00:31:42,480
but if we use this theory of that this

1000
00:31:42,480 --> 00:31:44,480
model is actually a kernel method in

1001
00:31:44,480 --> 00:31:45,440
some sentence

1002
00:31:45,440 --> 00:31:47,120
we can start thinking about how can we

1003
00:31:47,120 --> 00:31:49,120
use utilize these ideas of convex

1004
00:31:49,120 --> 00:31:50,320
optimization

1005
00:31:50,320 --> 00:31:51,679
and maybe the last thing and this is

1006
00:31:51,679 --> 00:31:53,760
maybe more for people who who are a bit

1007
00:31:53,760 --> 00:31:56,320
of experts already in machine learning

1008
00:31:56,320 --> 00:31:58,480
um this idea of doing convex

1009
00:31:58,480 --> 00:32:00,480
optimization and just inverting a matrix

1010
00:32:00,480 --> 00:32:02,320
a specific matrix so this was this trick

1011
00:32:02,320 --> 00:32:04,640
that people used here to still keep on

1012
00:32:04,640 --> 00:32:06,640
doing convex optimization and tractable

1013
00:32:06,640 --> 00:32:08,159
on your computer

1014
00:32:08,159 --> 00:32:10,240
we can do this with the quantum computer

1015
00:32:10,240 --> 00:32:12,000
not in quadratic time this was this

1016
00:32:12,000 --> 00:32:13,519
problem that we before had if we have

1017
00:32:13,519 --> 00:32:15,200
big data quadratic time and the number

1018
00:32:15,200 --> 00:32:16,559
of data is really bad

1019
00:32:16,559 --> 00:32:17,919
but on a quantum computer we can do it

1020
00:32:17,919 --> 00:32:20,559
in linear time and maybe this closes a

1021
00:32:20,559 --> 00:32:21,600
bit the circle of why

1022
00:32:21,600 --> 00:32:23,519
personally i stopped being so interested

1023
00:32:23,519 --> 00:32:25,120
in like only having exponential speed

1024
00:32:25,120 --> 00:32:26,640
ups everywhere because if we can just

1025
00:32:26,640 --> 00:32:28,000
invert a matrix in

1026
00:32:28,000 --> 00:32:30,240
linear time instead of quadratic time we

1027
00:32:30,240 --> 00:32:31,279
remove one of the biggest

1028
00:32:31,279 --> 00:32:34,159
bottlenecks of the methods in the 90s

1029
00:32:34,159 --> 00:32:35,600
from doing something like

1030
00:32:35,600 --> 00:32:37,760
with big data so we really have a model

1031
00:32:37,760 --> 00:32:39,600
that can do both or is a hybrid of the

1032
00:32:39,600 --> 00:32:40,640
two

1033
00:32:40,640 --> 00:32:43,120
yes and i think this is um where i

1034
00:32:43,120 --> 00:32:43,679
finish

1035
00:32:43,679 --> 00:32:45,919
i hope i haven't stepped over my time

1036
00:32:45,919 --> 00:32:47,360
and uh if you have questions i'm very

1037
00:32:47,360 --> 00:32:48,320
happy to

1038
00:32:48,320 --> 00:32:52,240
speak to you about thank you very much

1039
00:32:52,240 --> 00:32:54,080
maria

1040
00:32:54,080 --> 00:32:57,440
is it possible to kind of quickly

1041
00:32:57,440 --> 00:33:02,000
show us how penny lane works

1042
00:33:02,640 --> 00:33:04,240
no that's not a problem let me stop

1043
00:33:04,240 --> 00:33:06,159
sharing quickly

1044
00:33:06,159 --> 00:33:07,770
and um

1045
00:33:07,770 --> 00:33:09,360
[Music]

1046
00:33:09,360 --> 00:33:11,840
just like

1047
00:33:12,880 --> 00:33:15,600
and i'm taking your time free to post

1048
00:33:15,600 --> 00:33:17,120
your questions in the chat

1049
00:33:17,120 --> 00:33:21,439
or i'll mute yourselves if you have any

1050
00:33:22,840 --> 00:33:25,360
questions

1051
00:33:25,360 --> 00:33:29,918
okay one second

1052
00:33:31,200 --> 00:33:34,240
um cool i think you can see now my

1053
00:33:34,240 --> 00:33:35,519
entire desktop so

1054
00:33:35,519 --> 00:33:38,880
um so this is uh

1055
00:33:38,880 --> 00:33:42,080
this is the landing page oh sure

1056
00:33:42,080 --> 00:33:45,440
yeah cool um uh very briefly

1057
00:33:45,440 --> 00:33:47,120
um if you want to learn more about this

1058
00:33:47,120 --> 00:33:48,399
way of thinking of quantum machine

1059
00:33:48,399 --> 00:33:50,159
learning you find a lot of resources

1060
00:33:50,159 --> 00:33:52,399
um on the quantum machine learning tab

1061
00:33:52,399 --> 00:33:54,399
so my internet seems to be slow

1062
00:33:54,399 --> 00:33:55,840
if it does a lot of things at the same

1063
00:33:55,840 --> 00:33:58,000
time

1064
00:33:58,000 --> 00:33:58,540
um

1065
00:33:58,540 --> 00:34:01,879
[Music]

1066
00:34:02,399 --> 00:34:05,440
um so especially what what you find

1067
00:34:05,440 --> 00:34:06,080
there is

1068
00:34:06,080 --> 00:34:08,000
um a lot of demos so what we're trying

1069
00:34:08,000 --> 00:34:09,760
to do alexander do a lot is

1070
00:34:09,760 --> 00:34:11,599
because what we want to do we want to

1071
00:34:11,599 --> 00:34:13,599
get lots of people ready to play with us

1072
00:34:13,599 --> 00:34:15,040
right so this is the idea give it

1073
00:34:15,040 --> 00:34:16,560
into the hands of the community and see

1074
00:34:16,560 --> 00:34:18,960
what they do with it so what we're

1075
00:34:18,960 --> 00:34:21,918
often doing is we kind of like um so we

1076
00:34:21,918 --> 00:34:23,679
have got like basic introductions into

1077
00:34:23,679 --> 00:34:25,119
quantum machine learning here and code

1078
00:34:25,119 --> 00:34:26,320
but we also like

1079
00:34:26,320 --> 00:34:28,239
code up a lot of research papers so if

1080
00:34:28,239 --> 00:34:30,320
someone comes up with a research paper

1081
00:34:30,320 --> 00:34:32,719
we kind of um make a tutorial about it

1082
00:34:32,719 --> 00:34:34,000
so that you can actually implement

1083
00:34:34,000 --> 00:34:37,280
it in penny lane yourself

1084
00:34:37,280 --> 00:34:40,079
um secondly what i want to show is

1085
00:34:40,079 --> 00:34:41,119
pennylane is not

1086
00:34:41,119 --> 00:34:43,199
itself something like kiskit or qsharp

1087
00:34:43,199 --> 00:34:44,639
that it's its own language

1088
00:34:44,639 --> 00:34:46,399
only but it's more a manager of

1089
00:34:46,399 --> 00:34:48,480
languages so it can actually

1090
00:34:48,480 --> 00:34:50,480
it uses for quantum computations either

1091
00:34:50,480 --> 00:34:52,079
or in simulators

1092
00:34:52,079 --> 00:34:54,239
or a lot of devices so and you see here

1093
00:34:54,239 --> 00:34:56,000
we even have a microsoft device so what

1094
00:34:56,000 --> 00:34:57,520
happens is pennylane says okay i need

1095
00:34:57,520 --> 00:34:59,440
this computation it sends it off to

1096
00:34:59,440 --> 00:35:01,520
you know like your local microsoft

1097
00:35:01,520 --> 00:35:03,119
installation of your shop

1098
00:35:03,119 --> 00:35:05,119
or maybe a hardware device so you can

1099
00:35:05,119 --> 00:35:06,640
also run stuff on hardware

1100
00:35:06,640 --> 00:35:09,200
it gets the results back and then

1101
00:35:09,200 --> 00:35:10,960
processes them

1102
00:35:10,960 --> 00:35:13,280
and then lastly here you find the

1103
00:35:13,280 --> 00:35:14,480
documentation

1104
00:35:14,480 --> 00:35:16,880
which is um you know gives you like kind

1105
00:35:16,880 --> 00:35:18,079
of an introduction of how

1106
00:35:18,079 --> 00:35:20,560
the language works since there are many

1107
00:35:20,560 --> 00:35:22,560
yes and if any one of you is from the

1108
00:35:22,560 --> 00:35:24,720
microsoft community and wants to help us

1109
00:35:24,720 --> 00:35:25,680
to develop the

1110
00:35:25,680 --> 00:35:27,440
q sharp plugin further because i think

1111
00:35:27,440 --> 00:35:29,119
it's you know it has been quoted up like

1112
00:35:29,119 --> 00:35:30,560
a year ago would really like to update

1113
00:35:30,560 --> 00:35:32,320
it as well with the latest stuff in in

1114
00:35:32,320 --> 00:35:33,119
microsoft

1115
00:35:33,119 --> 00:35:36,480
in cu shop then just let us know

1116
00:35:36,480 --> 00:35:44,240
cool let me stop sharing

1117
00:35:44,240 --> 00:35:47,200
in the chat uh i guess where you also

1118
00:35:47,200 --> 00:35:48,160
have already

1119
00:35:48,160 --> 00:35:51,520
put a lot of papers in your presentation

1120
00:35:51,520 --> 00:35:53,280
so this one is asking

1121
00:35:53,280 --> 00:35:55,119
if you have any recommendations of

1122
00:35:55,119 --> 00:35:58,960
papers related to qml

1123
00:35:58,960 --> 00:36:03,040
um especially in the optimization

1124
00:36:03,040 --> 00:36:04,079
section

1125
00:36:04,079 --> 00:36:07,920
here yeah yeah i would go

1126
00:36:07,920 --> 00:36:10,480
um so maybe i should actually not have

1127
00:36:10,480 --> 00:36:12,320
stopped sharing my slides that wasn't

1128
00:36:12,320 --> 00:36:15,200
stupid wait one second and afterwards

1129
00:36:15,200 --> 00:36:16,560
you can also

1130
00:36:16,560 --> 00:36:18,960
send me the slides and i will be able to

1131
00:36:18,960 --> 00:36:21,040
upload it to the hackaday page

1132
00:36:21,040 --> 00:36:24,240
so people can also look at the reference

1133
00:36:24,240 --> 00:36:25,920
great the only thing i wanted to quickly

1134
00:36:25,920 --> 00:36:27,280
say is um

1135
00:36:27,280 --> 00:36:29,760
so this one here in my mind uh so

1136
00:36:29,760 --> 00:36:30,320
patrick

1137
00:36:30,320 --> 00:36:31,920
raymond told us it was lloyd and mussini

1138
00:36:31,920 --> 00:36:33,920
was like really kick starting quantum

1139
00:36:33,920 --> 00:36:35,280
machine learning for the first time

1140
00:36:35,280 --> 00:36:38,160
and um that had this idea of doing the

1141
00:36:38,160 --> 00:36:38,960
inversion

1142
00:36:38,960 --> 00:36:41,920
um really fast like based also on ideas

1143
00:36:41,920 --> 00:36:43,359
that have been developed here

1144
00:36:43,359 --> 00:36:46,079
so look at this paper and what what gets

1145
00:36:46,079 --> 00:36:47,760
cited here and you find a whole string

1146
00:36:47,760 --> 00:36:49,200
of ideas you have like

1147
00:36:49,200 --> 00:36:51,520
more and more elaborate idea uh ideas of

1148
00:36:51,520 --> 00:36:52,960
like this exponential

1149
00:36:52,960 --> 00:36:54,960
speed up claim of like matrix inversion

1150
00:36:54,960 --> 00:36:57,599
in one place or the other

1151
00:36:57,599 --> 00:37:00,320
and then as many of you know there was a

1152
00:37:00,320 --> 00:37:01,280
bit of a you know

1153
00:37:01,280 --> 00:37:03,839
community shock because then a couple of

1154
00:37:03,839 --> 00:37:04,960
years ago

1155
00:37:04,960 --> 00:37:07,119
a young student came even tongue and she

1156
00:37:07,119 --> 00:37:08,720
just dequantized

1157
00:37:08,720 --> 00:37:10,320
some of the algorithms not all of them

1158
00:37:10,320 --> 00:37:11,920
and showed that actually if there's a

1159
00:37:11,920 --> 00:37:12,400
quantum

1160
00:37:12,400 --> 00:37:14,560
exponential algorithm sorry expansion

1161
00:37:14,560 --> 00:37:16,880
speed up we can also find a classical

1162
00:37:16,880 --> 00:37:18,800
algorithm that has the same like input

1163
00:37:18,800 --> 00:37:19,839
oracle

1164
00:37:19,839 --> 00:37:23,200
and um yeah um

1165
00:37:23,200 --> 00:37:25,000
you can also send me an email at maria

1166
00:37:25,000 --> 00:37:27,520
xander.i then i'll send you an overview

1167
00:37:27,520 --> 00:37:28,720
paper of this type of work and then

1168
00:37:28,720 --> 00:37:30,160
lastly what i want to say the other

1169
00:37:30,160 --> 00:37:31,839
strain of optimization was a lot with

1170
00:37:31,839 --> 00:37:33,280
quantum unit you find a lot of quantum

1171
00:37:33,280 --> 00:37:34,480
machine learning in early days on

1172
00:37:34,480 --> 00:37:36,000
quantum annealing and there it's always

1173
00:37:36,000 --> 00:37:37,760
a bit tricky obviously to have um

1174
00:37:37,760 --> 00:37:39,440
proven speed up so there's a lot of

1175
00:37:39,440 --> 00:37:41,040
experiments here

1176
00:37:41,040 --> 00:37:44,400
yeah thank you

1177
00:37:44,400 --> 00:37:47,680
i see simon has

1178
00:37:48,000 --> 00:37:50,960
yes yes uh good evening uh mania i have

1179
00:37:50,960 --> 00:37:53,280
a question regarding penny lane

1180
00:37:53,280 --> 00:37:56,880
do you hear me yeah yes yeah yeah

1181
00:37:56,880 --> 00:37:59,760
sure so the goal of pennylane is to

1182
00:37:59,760 --> 00:38:00,720
design

1183
00:38:00,720 --> 00:38:04,000
or achieve a common let's say uh

1184
00:38:04,000 --> 00:38:07,280
standard framework to enable to

1185
00:38:07,280 --> 00:38:09,040
to bind all the dots between the

1186
00:38:09,040 --> 00:38:10,720
different frameworks that currently

1187
00:38:10,720 --> 00:38:14,720
already exist on the market including a

1188
00:38:15,119 --> 00:38:18,160
sharp queue or uh or a

1189
00:38:18,160 --> 00:38:21,280
torch or all the let's say the others

1190
00:38:21,280 --> 00:38:23,119
that according to the market

1191
00:38:23,119 --> 00:38:25,040
this is the original the original

1192
00:38:25,040 --> 00:38:28,079
purpose behind the penny lane

1193
00:38:28,079 --> 00:38:33,119
uh yes so um yes and no

1194
00:38:33,119 --> 00:38:34,960
yes you can you can do you can do

1195
00:38:34,960 --> 00:38:36,320
everything in penny lane

1196
00:38:36,320 --> 00:38:38,640
with the native framework so you can

1197
00:38:38,640 --> 00:38:40,320
also i don't know if you see the slide

1198
00:38:40,320 --> 00:38:41,119
here but

1199
00:38:41,119 --> 00:38:42,720
where you change the device or what's

1200
00:38:42,720 --> 00:38:43,839
the back end that actually does the

1201
00:38:43,839 --> 00:38:45,440
computation if it's the hardware device

1202
00:38:45,440 --> 00:38:45,760
or

1203
00:38:45,760 --> 00:38:48,079
software or like a simulator is changed

1204
00:38:48,079 --> 00:38:49,839
here and you see there's a default qubit

1205
00:38:49,839 --> 00:38:51,040
and we've got a couple of default

1206
00:38:51,040 --> 00:38:52,880
devices that ship with penny lane

1207
00:38:52,880 --> 00:38:54,400
so pennylane also does the quantum

1208
00:38:54,400 --> 00:38:56,320
computation if you want but if you want

1209
00:38:56,320 --> 00:38:57,839
to use something else because you want

1210
00:38:57,839 --> 00:38:58,960
to use google's

1211
00:38:58,960 --> 00:39:01,280
new hardware or whatever i don't think

1212
00:39:01,280 --> 00:39:03,040
it's like public yet but like uh you can

1213
00:39:03,040 --> 00:39:03,599
connect

1214
00:39:03,599 --> 00:39:06,079
to those as well and then on the other

1215
00:39:06,079 --> 00:39:07,119
side there are the machine learning

1216
00:39:07,119 --> 00:39:08,079
frameworks that's

1217
00:39:08,079 --> 00:39:09,920
pi torch tensorflow and we've got

1218
00:39:09,920 --> 00:39:12,000
autograd which is a numpy

1219
00:39:12,000 --> 00:39:13,440
extension to do this like kind of

1220
00:39:13,440 --> 00:39:15,359
machine learning stuff um

1221
00:39:15,359 --> 00:39:17,200
so there you also have the choice to use

1222
00:39:17,200 --> 00:39:19,440
those three and the easiest one is numpy

1223
00:39:19,440 --> 00:39:21,440
and that just looks like plain python

1224
00:39:21,440 --> 00:39:23,520
so you can use pennylane without ever

1225
00:39:23,520 --> 00:39:24,960
knowing about anything else that penny

1226
00:39:24,960 --> 00:39:29,839
lane but you can also use it to connect

1227
00:39:30,720 --> 00:39:34,079
i hope that helps yes thank you so much

1228
00:39:34,079 --> 00:39:36,720
thank you very much

1229
00:39:36,720 --> 00:39:38,959
cool

1230
00:39:41,200 --> 00:39:44,560
uh another question in the chat

1231
00:39:44,560 --> 00:39:46,960
how is the quantum memory being thought

1232
00:39:46,960 --> 00:39:48,480
of in terms of

1233
00:39:48,480 --> 00:39:51,599
old explosion of learning for general

1234
00:39:51,599 --> 00:39:52,720
purpose ai

1235
00:39:52,720 --> 00:39:55,760
or nn because there's

1236
00:39:55,760 --> 00:39:58,240
neural network because that's where i

1237
00:39:58,240 --> 00:39:58,960
see its

1238
00:39:58,960 --> 00:40:02,560
biggest impact any thoughts

1239
00:40:03,200 --> 00:40:06,480
okay that is a bit um what do you mean

1240
00:40:06,480 --> 00:40:08,160
by quantum memory so quantum memory

1241
00:40:08,160 --> 00:40:08,960
could mean

1242
00:40:08,960 --> 00:40:12,000
um a quantum system that stores data as

1243
00:40:12,000 --> 00:40:13,520
quantum information

1244
00:40:13,520 --> 00:40:15,200
and in that case obviously that is

1245
00:40:15,200 --> 00:40:16,880
actually our sole spot so this is like

1246
00:40:16,880 --> 00:40:18,000
not very clear why

1247
00:40:18,000 --> 00:40:19,920
why this is an advantage because so

1248
00:40:19,920 --> 00:40:21,280
first of all first of all you have to

1249
00:40:21,280 --> 00:40:22,880
always take the data and load it into

1250
00:40:22,880 --> 00:40:24,560
quantum memory so you already create an

1251
00:40:24,560 --> 00:40:25,280
overhead

1252
00:40:25,280 --> 00:40:27,119
even if you can then read it out of that

1253
00:40:27,119 --> 00:40:28,880
quantum area very quickly

1254
00:40:28,880 --> 00:40:30,079
but i'm not too sure if you're talking

1255
00:40:30,079 --> 00:40:32,240
about a different use of the term

1256
00:40:32,240 --> 00:40:35,118
quantum memory here

1257
00:40:35,440 --> 00:40:36,880
maybe you want to unmute yourself and

1258
00:40:36,880 --> 00:40:39,839
ask

1259
00:40:41,200 --> 00:40:46,078
yeah do you want clarifies in it

1260
00:40:49,040 --> 00:40:52,800
maybe their microphone is not working

1261
00:40:55,280 --> 00:40:58,319
hi i'm from canada

1262
00:40:58,319 --> 00:41:00,160
and um just to give a little background

1263
00:41:00,160 --> 00:41:02,240
i'm a blockchain developer

1264
00:41:02,240 --> 00:41:05,200
and there's been a huge buzz in the

1265
00:41:05,200 --> 00:41:05,760
whole

1266
00:41:05,760 --> 00:41:09,200
mining faction uh to use quantum

1267
00:41:09,200 --> 00:41:09,839
computing

1268
00:41:09,839 --> 00:41:12,319
so would what are your thoughts on that

1269
00:41:12,319 --> 00:41:14,640
and because uh basically if

1270
00:41:14,640 --> 00:41:17,599
to give it to you in like 30 seconds um

1271
00:41:17,599 --> 00:41:20,000
they use

1272
00:41:20,000 --> 00:41:22,160
huge gpus to basically solve an

1273
00:41:22,160 --> 00:41:24,240
algorithm so that they can go ahead and

1274
00:41:24,240 --> 00:41:24,720
get

1275
00:41:24,720 --> 00:41:28,079
a reward so how do you think this

1276
00:41:28,079 --> 00:41:30,319
will affect that and do you think it

1277
00:41:30,319 --> 00:41:32,800
will help in any way or form

1278
00:41:32,800 --> 00:41:35,839
yeah i think i'm i'm not um qualified to

1279
00:41:35,839 --> 00:41:36,960
speak about this so but

1280
00:41:36,960 --> 00:41:38,400
one of my students she's really into

1281
00:41:38,400 --> 00:41:40,319
blockchain and she gives me like

1282
00:41:40,319 --> 00:41:42,400
a lot of information on this and as far

1283
00:41:42,400 --> 00:41:43,760
as i understand

1284
00:41:43,760 --> 00:41:45,280
you obviously and i'm sure people are

1285
00:41:45,280 --> 00:41:46,880
doing this right you have to

1286
00:41:46,880 --> 00:41:49,040
find an algorithm like that you can do

1287
00:41:49,040 --> 00:41:50,800
this mining with where you have like the

1288
00:41:50,800 --> 00:41:53,040
right balance of like hardness and

1289
00:41:53,040 --> 00:41:56,319
uh but still get rewards or like right i

1290
00:41:56,319 --> 00:41:57,280
don't know what you're getting

1291
00:41:57,280 --> 00:42:00,560
yeah and then prove all the um you know

1292
00:42:00,560 --> 00:42:00,960
like

1293
00:42:00,960 --> 00:42:02,640
the hotness proves that like that this

1294
00:42:02,640 --> 00:42:04,079
can't be cracked and stuff like that

1295
00:42:04,079 --> 00:42:05,839
so i really don't know i'm sure there's

1296
00:42:05,839 --> 00:42:07,280
a very active community so

1297
00:42:07,280 --> 00:42:09,280
i think anything uh anything

1298
00:42:09,280 --> 00:42:10,720
computational you put quantum in front

1299
00:42:10,720 --> 00:42:12,640
of you will find this community but yeah

1300
00:42:12,640 --> 00:42:15,359
i would i wouldn't know this i'm sorry

1301
00:42:15,359 --> 00:42:18,640
no problem thank you so much

1302
00:42:18,800 --> 00:42:22,240
uh next question relates to a question i

1303
00:42:22,240 --> 00:42:23,440
had

1304
00:42:23,440 --> 00:42:27,280
so they're asking uh if

1305
00:42:27,280 --> 00:42:30,560
penny lane has any interface with d-wave

1306
00:42:30,560 --> 00:42:32,240
quantum computers

1307
00:42:32,240 --> 00:42:35,680
and it also deals with optimization with

1308
00:42:35,680 --> 00:42:38,480
which potentially helping cost reduction

1309
00:42:38,480 --> 00:42:40,000
so i had a similar question

1310
00:42:40,000 --> 00:42:43,280
it's like yeah other places optimization

1311
00:42:43,280 --> 00:42:45,040
and lower cost

1312
00:42:45,040 --> 00:42:47,520
is quantum annealing or generally

1313
00:42:47,520 --> 00:42:49,359
speaking

1314
00:42:49,359 --> 00:42:52,480
quantum inspire optimization how so

1315
00:42:52,480 --> 00:42:54,800
it's different method but then is there

1316
00:42:54,800 --> 00:42:55,520
any

1317
00:42:55,520 --> 00:42:58,560
relation between them or

1318
00:42:58,560 --> 00:43:01,599
how if not how how are they comparing

1319
00:43:01,599 --> 00:43:04,960
yes so um no pen doesn't have an

1320
00:43:04,960 --> 00:43:06,400
interface to quantum annealing yet at

1321
00:43:06,400 --> 00:43:08,079
the moment it only has two computation

1322
00:43:08,079 --> 00:43:10,400
models that it supports the one is a

1323
00:43:10,400 --> 00:43:12,079
circuit based quantum computing so the

1324
00:43:12,079 --> 00:43:14,400
normal ones you know with qubits

1325
00:43:14,400 --> 00:43:15,839
and then it also supports continuous

1326
00:43:15,839 --> 00:43:17,040
variable quantum computing which is

1327
00:43:17,040 --> 00:43:18,240
something we're very actively

1328
00:43:18,240 --> 00:43:19,839
investigating axanar do

1329
00:43:19,839 --> 00:43:21,440
uh so you know photonics is very

1330
00:43:21,440 --> 00:43:23,040
natively like written in that language

1331
00:43:23,040 --> 00:43:24,880
but at the moment we don't have um

1332
00:43:24,880 --> 00:43:26,319
plugins to annealers

1333
00:43:26,319 --> 00:43:28,079
this might be a question of time i think

1334
00:43:28,079 --> 00:43:29,920
why it's not a priority at the moment is

1335
00:43:29,920 --> 00:43:30,400
that

1336
00:43:30,400 --> 00:43:32,880
although quantum annealing was very was

1337
00:43:32,880 --> 00:43:34,800
really important actually even d-wave

1338
00:43:34,800 --> 00:43:36,560
themselves with their research team were

1339
00:43:36,560 --> 00:43:38,319
very important for the early days of

1340
00:43:38,319 --> 00:43:40,000
quantum machine learning

1341
00:43:40,000 --> 00:43:41,920
people have gone a bit away from this

1342
00:43:41,920 --> 00:43:44,720
and i'm not entirely sure why maybe it's

1343
00:43:44,720 --> 00:43:46,400
partially because it was so hard to

1344
00:43:46,400 --> 00:43:47,280
prove that things

1345
00:43:47,280 --> 00:43:49,599
are better so people still do a lot of

1346
00:43:49,599 --> 00:43:50,560
experiments like on

1347
00:43:50,560 --> 00:43:52,960
on d-wave but um but also obviously

1348
00:43:52,960 --> 00:43:54,480
because the d-wave is solving a very

1349
00:43:54,480 --> 00:43:55,839
specific problem which is the

1350
00:43:55,839 --> 00:43:57,440
quadratic unconstrained optimization

1351
00:43:57,440 --> 00:44:00,079
problem although you can put machine

1352
00:44:00,079 --> 00:44:01,599
learning into that framework it's not a

1353
00:44:01,599 --> 00:44:03,200
very native one so if you think about

1354
00:44:03,200 --> 00:44:04,319
deep learning

1355
00:44:04,319 --> 00:44:05,760
deep learning doesn't necessarily want

1356
00:44:05,760 --> 00:44:07,520
to solve that particular problem you

1357
00:44:07,520 --> 00:44:08,960
have to kick it quite hard

1358
00:44:08,960 --> 00:44:12,000
to get into that shape so i think that

1359
00:44:12,000 --> 00:44:13,200
um

1360
00:44:13,200 --> 00:44:14,560
for some reason in the community

1361
00:44:14,560 --> 00:44:16,800
interest has has dropped a bit in that

1362
00:44:16,800 --> 00:44:18,240
so this is where probably we don't have

1363
00:44:18,240 --> 00:44:19,119
it but

1364
00:44:19,119 --> 00:44:20,640
but if if you know something interesting

1365
00:44:20,640 --> 00:44:24,640
pops up then we easily extended that way

1366
00:44:24,640 --> 00:44:28,240
yeah we we have a very big effort on

1367
00:44:28,240 --> 00:44:31,040
uh quantum optimization or economy

1368
00:44:31,040 --> 00:44:32,319
expired

1369
00:44:32,319 --> 00:44:36,880
um that's also true but then obviously

1370
00:44:36,880 --> 00:44:38,880
you need no quantum computer yeah so

1371
00:44:38,880 --> 00:44:41,680
then yeah

1372
00:44:41,680 --> 00:44:43,680
yeah that's that's the fun thing about

1373
00:44:43,680 --> 00:44:44,800
it is that you can

1374
00:44:44,800 --> 00:44:48,079
improve classical algorithms and run on

1375
00:44:48,079 --> 00:44:50,560
classical hardware and this is

1376
00:44:50,560 --> 00:44:52,400
this is also why i'm sometimes not you

1377
00:44:52,400 --> 00:44:54,079
know i'm not even so bothered about

1378
00:44:54,079 --> 00:44:55,760
speedups because just this way of

1379
00:44:55,760 --> 00:44:57,200
thinking about computation is so

1380
00:44:57,200 --> 00:44:58,160
different and it

1381
00:44:58,160 --> 00:45:00,400
you know it really opens like a couple

1382
00:45:00,400 --> 00:45:02,319
of doors also to think of

1383
00:45:02,319 --> 00:45:04,560
a computation that's probabilistic by

1384
00:45:04,560 --> 00:45:06,880
itself right like it's

1385
00:45:06,880 --> 00:45:08,480
you know like quantum computing the one

1386
00:45:08,480 --> 00:45:10,240
thing that i wish someone would have

1387
00:45:10,240 --> 00:45:11,440
told me right from the start i would

1388
00:45:11,440 --> 00:45:13,520
have understood it a lot easier is

1389
00:45:13,520 --> 00:45:15,680
that it is not the same as deterministic

1390
00:45:15,680 --> 00:45:17,200
computation what you get out is a

1391
00:45:17,200 --> 00:45:18,400
probability distribution

1392
00:45:18,400 --> 00:45:19,839
and especially for machine learning this

1393
00:45:19,839 --> 00:45:21,359
is super interesting because a lot of

1394
00:45:21,359 --> 00:45:22,640
machine learning is

1395
00:45:22,640 --> 00:45:26,079
in probabilistic theory and yeah

1396
00:45:26,079 --> 00:45:29,119
yeah so in terms of the

1397
00:45:29,119 --> 00:45:32,240
quantum memory earlier submit means

1398
00:45:32,240 --> 00:45:35,359
some general purpose ai in terms of data

1399
00:45:35,359 --> 00:45:37,119
explosion

1400
00:45:37,119 --> 00:45:40,319
oh okay yeah i mean well

1401
00:45:40,319 --> 00:45:41,760
it depends a bit also on how you

1402
00:45:41,760 --> 00:45:43,520
formulate your problem right if you

1403
00:45:43,520 --> 00:45:46,079
um if you have this interface or you

1404
00:45:46,079 --> 00:45:47,760
have a data set that's classical

1405
00:45:47,760 --> 00:45:49,760
and then you want to feed it into a

1406
00:45:49,760 --> 00:45:51,359
quantum computer

1407
00:45:51,359 --> 00:45:52,640
so this is this kind of like big

1408
00:45:52,640 --> 00:45:55,040
controversy about the early like speed

1409
00:45:55,040 --> 00:45:55,839
up

1410
00:45:55,839 --> 00:45:58,880
algorithms of this matrix inversion kind

1411
00:45:58,880 --> 00:46:00,720
it's like colleagues always said yeah

1412
00:46:00,720 --> 00:46:02,400
but i can invent this like i can assume

1413
00:46:02,400 --> 00:46:04,480
this oracle that just picks data super

1414
00:46:04,480 --> 00:46:06,480
efficiently somehow but in my mind if my

1415
00:46:06,480 --> 00:46:08,000
problem is formulated as

1416
00:46:08,000 --> 00:46:09,760
given a classical memory with classical

1417
00:46:09,760 --> 00:46:11,520
data the first thing i have to do is

1418
00:46:11,520 --> 00:46:13,440
touch all my data i mean i can sub

1419
00:46:13,440 --> 00:46:14,960
sample but then i don't have the

1420
00:46:14,960 --> 00:46:16,480
original data i just have a subset of

1421
00:46:16,480 --> 00:46:17,520
data right

1422
00:46:17,520 --> 00:46:19,359
so i will always have to like take all

1423
00:46:19,359 --> 00:46:22,000
my data and load it into something

1424
00:46:22,000 --> 00:46:25,119
once i've done that i already have an

1425
00:46:25,119 --> 00:46:26,640
algorithm that's linear in the time of

1426
00:46:26,640 --> 00:46:27,280
the data

1427
00:46:27,280 --> 00:46:29,200
so i won't get an exponential speed up

1428
00:46:29,200 --> 00:46:30,319
and your network

1429
00:46:30,319 --> 00:46:32,400
training is linear in the amount of data

1430
00:46:32,400 --> 00:46:33,440
already so

1431
00:46:33,440 --> 00:46:35,680
i find it very hard to argue like that

1432
00:46:35,680 --> 00:46:37,520
if you formulate the problem this way

1433
00:46:37,520 --> 00:46:39,200
that there's a speed up the moment you

1434
00:46:39,200 --> 00:46:41,200
have quantum data so you already have a

1435
00:46:41,200 --> 00:46:42,160
quantum experiment

1436
00:46:42,160 --> 00:46:44,560
and then you get a quantum information

1437
00:46:44,560 --> 00:46:45,280
out somehow

1438
00:46:45,280 --> 00:46:46,880
then immediately you have to speed up

1439
00:46:46,880 --> 00:46:48,160
but then you don't even need a paper

1440
00:46:48,160 --> 00:46:49,599
about this this is like follows

1441
00:46:49,599 --> 00:46:50,960
trivially

1442
00:46:50,960 --> 00:46:55,359
yeah another question

1443
00:46:55,359 --> 00:46:59,200
with regards to notes in new networks

1444
00:46:59,200 --> 00:47:01,760
how do you see this playing out in

1445
00:47:01,760 --> 00:47:02,560
future

1446
00:47:02,560 --> 00:47:06,880
as individual quantum notes

1447
00:47:07,240 --> 00:47:10,079
specializing specific feature or same as

1448
00:47:10,079 --> 00:47:12,079
classical

1449
00:47:12,079 --> 00:47:16,079
um so there's also here different ways

1450
00:47:16,079 --> 00:47:17,280
where you can plug your quantum

1451
00:47:17,280 --> 00:47:19,040
algorithm in

1452
00:47:19,040 --> 00:47:20,960
you can throw away the entire neural

1453
00:47:20,960 --> 00:47:22,319
network and replace it by quantum

1454
00:47:22,319 --> 00:47:23,359
algorithm or

1455
00:47:23,359 --> 00:47:24,800
as you say if you talk about nodes you

1456
00:47:24,800 --> 00:47:26,480
probably mean neurons or

1457
00:47:26,480 --> 00:47:28,000
i don't know if you can think of a

1458
00:47:28,000 --> 00:47:29,359
neuron as a quantum one which you

1459
00:47:29,359 --> 00:47:30,640
probably could also but

1460
00:47:30,640 --> 00:47:32,559
um what people do quite a lot is they

1461
00:47:32,559 --> 00:47:35,920
take a layer out of a neural network so

1462
00:47:35,920 --> 00:47:37,920
for example and then replace it by

1463
00:47:37,920 --> 00:47:38,960
quantum computation

1464
00:47:38,960 --> 00:47:41,359
so for example um there are experiments

1465
00:47:41,359 --> 00:47:42,720
actually if you go to penny lane and you

1466
00:47:42,720 --> 00:47:44,640
look at the transfer learning tutorial

1467
00:47:44,640 --> 00:47:45,599
there's one like really

1468
00:47:45,599 --> 00:47:47,680
very popular tutorial where you take

1469
00:47:47,680 --> 00:47:49,520
resnet which is an image classification

1470
00:47:49,520 --> 00:47:50,640
your network

1471
00:47:50,640 --> 00:47:52,640
you can take it preload it download it

1472
00:47:52,640 --> 00:47:54,160
from i think it's

1473
00:47:54,160 --> 00:47:56,880
written in tensorflow and then you just

1474
00:47:56,880 --> 00:47:58,720
chop up the last layer and replace it by

1475
00:47:58,720 --> 00:47:59,920
quantum circuit and then you see what

1476
00:47:59,920 --> 00:48:00,480
happens

1477
00:48:00,480 --> 00:48:03,359
and um so this is very since neural

1478
00:48:03,359 --> 00:48:04,559
networks are modular

1479
00:48:04,559 --> 00:48:06,559
and quantum computation is modular you

1480
00:48:06,559 --> 00:48:07,680
can really like start

1481
00:48:07,680 --> 00:48:11,279
mixing and matching quite interestingly

1482
00:48:12,160 --> 00:48:15,440
cool i guess let's take the last

1483
00:48:15,440 --> 00:48:16,480
question

1484
00:48:16,480 --> 00:48:20,160
is more not about the technical is more

1485
00:48:20,160 --> 00:48:24,079
about how much you would need

1486
00:48:24,079 --> 00:48:26,240
in terms of quantum computing quantum

1487
00:48:26,240 --> 00:48:28,319
physics for one to

1488
00:48:28,319 --> 00:48:31,760
know to use uml that's actually a nice

1489
00:48:31,760 --> 00:48:32,960
last question um

1490
00:48:32,960 --> 00:48:35,040
i'm a bit biased here and also

1491
00:48:35,040 --> 00:48:36,480
colleagues would answer this differently

1492
00:48:36,480 --> 00:48:38,319
i personally think the less you know the

1493
00:48:38,319 --> 00:48:40,240
better because everyone who's in quantum

1494
00:48:40,240 --> 00:48:42,319
machinery comes from quantum physics

1495
00:48:42,319 --> 00:48:44,400
and we have these old ways of thinking

1496
00:48:44,400 --> 00:48:46,559
that have been practiced in the last 30

1497
00:48:46,559 --> 00:48:48,160
years and

1498
00:48:48,160 --> 00:48:50,240
i think what what you really need to be

1499
00:48:50,240 --> 00:48:52,079
good at is machine learning because

1500
00:48:52,079 --> 00:48:54,400
the quantum part you could probably

1501
00:48:54,400 --> 00:48:55,200
learn

1502
00:48:55,200 --> 00:48:56,559
quite quickly because we're talking

1503
00:48:56,559 --> 00:48:58,000
about these very generic models you

1504
00:48:58,000 --> 00:48:59,040
don't have to know anything about

1505
00:48:59,040 --> 00:49:00,640
implementation you probably don't even

1506
00:49:00,640 --> 00:49:01,760
have to know about

1507
00:49:01,760 --> 00:49:03,359
noise models or something like that just

1508
00:49:03,359 --> 00:49:05,920
to like understand the basics

1509
00:49:05,920 --> 00:49:07,680
but since actually this question of

1510
00:49:07,680 --> 00:49:09,839
generalization is really actually very

1511
00:49:09,839 --> 00:49:11,680
non-trivial this is

1512
00:49:11,680 --> 00:49:13,280
there's a lot of theory about this when

1513
00:49:13,280 --> 00:49:14,960
does a model generalize i think this is

1514
00:49:14,960 --> 00:49:16,079
what we actually started

1515
00:49:16,079 --> 00:49:19,520
to start to get skill sets in so yeah

1516
00:49:19,520 --> 00:49:21,599
i think you don't need to know too much

1517
00:49:21,599 --> 00:49:23,359
about quantum computing there

1518
00:49:23,359 --> 00:49:26,880
right keep an open mind

1519
00:49:26,880 --> 00:49:30,160
okay so let's

1520
00:49:30,160 --> 00:49:32,319
call it a day thank you so much maria

1521
00:49:32,319 --> 00:49:34,480
for joining us

1522
00:49:34,480 --> 00:49:37,680
it's also your evening now so i hope

1523
00:49:37,680 --> 00:49:41,520
i hope you enjoyed this as well

1524
00:49:41,920 --> 00:49:45,040
okay thank you everyone for joining i i

1525
00:49:45,040 --> 00:49:46,880
did take notes but yes very much

1526
00:49:46,880 --> 00:49:49,119
thank you very much didn't do a lot of

1527
00:49:49,119 --> 00:49:49,920
drawing today

1528
00:49:49,920 --> 00:49:53,040
but we're trying to make something to

1529
00:49:53,040 --> 00:49:53,520
post

1530
00:49:53,520 --> 00:49:55,599
next week i'm excited i'm looking

1531
00:49:55,599 --> 00:49:57,680
forward to what's coming out of it i

1532
00:49:57,680 --> 00:49:59,760
need to first convert some of the

1533
00:49:59,760 --> 00:50:03,760
uh functions into cats

1534
00:50:03,920 --> 00:50:05,839
yeah yeah i didn't mention much cats

1535
00:50:05,839 --> 00:50:07,119
actually usually people do machine

1536
00:50:07,119 --> 00:50:07,680
learning on

1537
00:50:07,680 --> 00:50:10,319
cats and dogs but even that was replaced

1538
00:50:10,319 --> 00:50:11,599
by ants and bees so

1539
00:50:11,599 --> 00:50:13,119
sorry about that that's a good

1540
00:50:13,119 --> 00:50:16,160
inspiration i'll try that

1541
00:50:16,160 --> 00:50:20,160
thank you so much for joining us maria

1542
00:50:20,800 --> 00:50:26,960
next time bye thank you everyone

1543
00:50:26,960 --> 00:50:29,040
you

