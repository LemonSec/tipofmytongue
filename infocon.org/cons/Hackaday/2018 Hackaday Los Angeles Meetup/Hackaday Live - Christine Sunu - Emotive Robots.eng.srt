1
00:00:00,000 --> 00:00:03,570
hey she has spoken at a super con went

2
00:00:03,570 --> 00:00:06,180
last year which was amazing you've done

3
00:00:06,180 --> 00:00:07,890
a ton of great things too even on

4
00:00:07,890 --> 00:00:09,750
BuzzFeed and like oh my god

5
00:00:09,750 --> 00:00:12,290
so many cool things I could go on and on

6
00:00:12,290 --> 00:00:15,540
and she's also one of our upcoming

7
00:00:15,540 --> 00:00:17,369
residents starting on Monday so I'm

8
00:00:17,369 --> 00:00:18,720
really excited to spend some more time

9
00:00:18,720 --> 00:00:21,029
with her and really see what your

10
00:00:21,029 --> 00:00:24,390
project is all about and without further

11
00:00:24,390 --> 00:00:29,550
ado here is Christine hi oh my gosh

12
00:00:29,550 --> 00:00:32,880
applause Wow

13
00:00:32,880 --> 00:00:36,180
so I am really excited to be here I'm

14
00:00:36,180 --> 00:00:38,250
also excited to follow like a bio talk I

15
00:00:38,250 --> 00:00:40,350
was talking to Richie beforehand and I

16
00:00:40,350 --> 00:00:42,809
was just like you know what I am I used

17
00:00:42,809 --> 00:00:44,250
to be in genetics and I was like I never

18
00:00:44,250 --> 00:00:45,989
thought that I would miss going to lab

19
00:00:45,989 --> 00:00:48,780
meeting like this was so nice except oh

20
00:00:48,780 --> 00:00:50,340
my god right the things they're doing

21
00:00:50,340 --> 00:00:51,600
with imaging I was just sitting there

22
00:00:51,600 --> 00:00:54,809
like you know I I felt like I felt like

23
00:00:54,809 --> 00:00:56,309
the way that my grandpa feels when he

24
00:00:56,309 --> 00:00:57,570
talks to me I mean it's only been like

25
00:00:57,570 --> 00:00:59,640
10 ish years since I was in genetics and

26
00:00:59,640 --> 00:01:02,039
I was still like when I was in genetics

27
00:01:02,039 --> 00:01:05,010
we had to section the mouse brain to get

28
00:01:05,010 --> 00:01:10,020
an image that good uphill both ways but

29
00:01:10,020 --> 00:01:10,860
yeah that's not what I'm gonna talk

30
00:01:10,860 --> 00:01:12,439
about today I'm going to talk about

31
00:01:12,439 --> 00:01:15,119
emotive robots so a little bit about me

32
00:01:15,119 --> 00:01:17,040
my name is Christine I'm a former writer

33
00:01:17,040 --> 00:01:18,990
researcher and medical person and now I

34
00:01:18,990 --> 00:01:21,270
work as a designer and a technologist in

35
00:01:21,270 --> 00:01:23,729
emerging fields so I would do rapid

36
00:01:23,729 --> 00:01:25,560
prototyping and product development for

37
00:01:25,560 --> 00:01:27,960
hardware and emotive tech at flash bang

38
00:01:27,960 --> 00:01:30,270
and I also post these DIY tutorials

39
00:01:30,270 --> 00:01:32,490
about design and technology at hack

40
00:01:32,490 --> 00:01:37,439
pretty calm yeah and I love connected

41
00:01:37,439 --> 00:01:39,570
tech I used to run the developer

42
00:01:39,570 --> 00:01:41,640
community at particle which is a startup

43
00:01:41,640 --> 00:01:43,380
that did hardware software the Internet

44
00:01:43,380 --> 00:01:45,299
of Things but I found in my pursuits of

45
00:01:45,299 --> 00:01:46,890
the connected that would really

46
00:01:46,890 --> 00:01:48,689
interested me what always interested in

47
00:01:48,689 --> 00:01:52,110
me was not the technological connections

48
00:01:52,110 --> 00:01:53,909
we could forge but the human connections

49
00:01:53,909 --> 00:01:57,180
that underlie them so my research of my

50
00:01:57,180 --> 00:01:58,619
work frequently focuses on a motive

51
00:01:58,619 --> 00:02:01,259
technology tech that makes us feel tech

52
00:02:01,259 --> 00:02:02,850
that makes us feel living emotions about

53
00:02:02,850 --> 00:02:04,619
on alive things whether that build is a

54
00:02:04,619 --> 00:02:06,030
motive robots and that's what this talk

55
00:02:06,030 --> 00:02:08,128
is about robots that make us feel

56
00:02:08,128 --> 00:02:11,550
feelings and some of you may question

57
00:02:11,550 --> 00:02:13,500
how that's even possible

58
00:02:13,500 --> 00:02:15,480
how can an object make us feel a deep

59
00:02:15,480 --> 00:02:17,640
living emotion typically associated with

60
00:02:17,640 --> 00:02:19,790
living things like a person or a pet

61
00:02:19,790 --> 00:02:22,980
well we already feel strong emotions

62
00:02:22,980 --> 00:02:24,750
towards a variety of objects that aren't

63
00:02:24,750 --> 00:02:27,780
alive at all frequently the objects

64
00:02:27,780 --> 00:02:30,959
reflect memories and stories primarily

65
00:02:30,959 --> 00:02:33,570
concerned with our relationships with

66
00:02:33,570 --> 00:02:36,200
other people involved in those stories

67
00:02:36,200 --> 00:02:38,760
so while we have relationships with

68
00:02:38,760 --> 00:02:41,820
objects and those objects cannot love us

69
00:02:41,820 --> 00:02:44,220
back our relationship is ultimately not

70
00:02:44,220 --> 00:02:46,739
with this object at all our relationship

71
00:02:46,739 --> 00:02:49,020
is with the people associated with the

72
00:02:49,020 --> 00:02:52,680
objects this one-sided relationship with

73
00:02:52,680 --> 00:02:55,350
an object is a proxy for a very real set

74
00:02:55,350 --> 00:02:57,180
of feelings that we have for each other

75
00:02:57,180 --> 00:03:00,120
in other words objects mediate

76
00:03:00,120 --> 00:03:02,250
relationships they represent them they

77
00:03:02,250 --> 00:03:06,959
facilitate them teller stories so robots

78
00:03:06,959 --> 00:03:08,220
are ultimately not really different

79
00:03:08,220 --> 00:03:10,380
except that as objects go they're far

80
00:03:10,380 --> 00:03:12,900
more confusing for us it's really easy

81
00:03:12,900 --> 00:03:14,640
to look at a robot and wonder if it's an

82
00:03:14,640 --> 00:03:16,380
object at all or if it's more like a

83
00:03:16,380 --> 00:03:18,180
living thing that relates back to us and

84
00:03:18,180 --> 00:03:20,910
this perception of robots can be really

85
00:03:20,910 --> 00:03:22,700
confusing for our simple human brains

86
00:03:22,700 --> 00:03:25,080
especially since we anthropomorphize and

87
00:03:25,080 --> 00:03:26,700
zoom amorphize things at the drop of the

88
00:03:26,700 --> 00:03:29,780
Hat if it has a face it might be alive

89
00:03:29,780 --> 00:03:33,510
if it moves in a particular way it might

90
00:03:33,510 --> 00:03:36,000
be alive and this is my favorite I'm

91
00:03:36,000 --> 00:03:45,180
gonna wait until it does this and then

92
00:03:45,180 --> 00:03:47,940
also if it seems to need something from

93
00:03:47,940 --> 00:03:53,130
us we think it might be alive right our

94
00:03:53,130 --> 00:03:55,890
instinct is to assume life over object

95
00:03:55,890 --> 00:03:57,600
if we see something that implies life

96
00:03:57,600 --> 00:03:59,760
and that's really wonderful in some way

97
00:03:59,760 --> 00:04:01,560
then we as humans try to bond with and

98
00:04:01,560 --> 00:04:02,940
care for things that might not be alive

99
00:04:02,940 --> 00:04:04,620
at all it's kind of you know like just

100
00:04:04,620 --> 00:04:09,600
in case and oh it's moving this is my

101
00:04:09,600 --> 00:04:10,799
favorite example of this this was posted

102
00:04:10,799 --> 00:04:12,480
up on reddit a while ago as a cross post

103
00:04:12,480 --> 00:04:14,220
from tumblr this is a person who

104
00:04:14,220 --> 00:04:15,630
believed that because their Roomba

105
00:04:15,630 --> 00:04:17,339
malfunctioned after a thunderclap that

106
00:04:17,339 --> 00:04:18,870
it must have been afraid of the storm so

107
00:04:18,870 --> 00:04:19,918
they picked it up and they put it in

108
00:04:19,918 --> 00:04:22,680
their lap to comfort it yeah which is

109
00:04:22,680 --> 00:04:24,650
weird and also kind of beautiful I guess

110
00:04:24,650 --> 00:04:26,480
it just kind of goes

111
00:04:26,480 --> 00:04:28,370
show that our instinct for empathy is so

112
00:04:28,370 --> 00:04:30,470
strong that we will bond with almost

113
00:04:30,470 --> 00:04:32,420
anything even though we know it's not

114
00:04:32,420 --> 00:04:36,050
alive to go that further this thread was

115
00:04:36,050 --> 00:04:37,820
full of comments from people who had

116
00:04:37,820 --> 00:04:39,860
also bonded with their Roombas this was

117
00:04:39,860 --> 00:04:43,040
one of my favorites I used to joke that

118
00:04:43,040 --> 00:04:46,250
the Roomba was my son every time the

119
00:04:46,250 --> 00:04:47,960
Roomba bumped into something I would say

120
00:04:47,960 --> 00:04:50,840
in a stern voice he is not my son he is

121
00:04:50,840 --> 00:04:52,690
a disappointment

122
00:04:52,690 --> 00:04:54,620
recently the Roomba broke and all I

123
00:04:54,620 --> 00:04:57,440
could think was I was too hard on him

124
00:04:57,440 --> 00:05:00,020
only now that he is gone can I say that

125
00:05:00,020 --> 00:05:02,810
I was proud of him I ended up getting it

126
00:05:02,810 --> 00:05:03,740
fixed even though it would have been

127
00:05:03,740 --> 00:05:05,480
cheaper to buy a new one and now I'm

128
00:05:05,480 --> 00:05:09,920
always nice to the Roomba so this is a

129
00:05:09,920 --> 00:05:12,080
person who started by making a joke

130
00:05:12,080 --> 00:05:14,570
about the relationship of the Roomba but

131
00:05:14,570 --> 00:05:16,010
their feelings committed to the story

132
00:05:16,010 --> 00:05:19,790
they felt it when the Roomba died so

133
00:05:19,790 --> 00:05:22,010
much so that it influenced a purchasing

134
00:05:22,010 --> 00:05:24,110
decision they could have had that money

135
00:05:24,110 --> 00:05:26,090
for them a living person and then

136
00:05:26,090 --> 00:05:27,950
instead they spent it on the robot which

137
00:05:27,950 --> 00:05:30,320
cannot feel any feelings at all so you

138
00:05:30,320 --> 00:05:31,520
know we can make these jokes about

139
00:05:31,520 --> 00:05:33,020
things we can logically understand that

140
00:05:33,020 --> 00:05:35,260
something's not alive but it can still

141
00:05:35,260 --> 00:05:37,940
influence these like real decisions

142
00:05:37,940 --> 00:05:39,770
right that we make our feelings can

143
00:05:39,770 --> 00:05:40,970
surprise us in that way

144
00:05:40,970 --> 00:05:42,860
and I think it's worth mentioning that

145
00:05:42,860 --> 00:05:44,990
iRobot did not set out to create an

146
00:05:44,990 --> 00:05:46,970
emotive robot quite the opposite they

147
00:05:46,970 --> 00:05:48,890
tried to make a robot that no one would

148
00:05:48,890 --> 00:05:51,860
bond to and people still did this is the

149
00:05:51,860 --> 00:05:54,260
power of our instinct to assume life

150
00:05:54,260 --> 00:05:57,320
when we see robots this is insane they

151
00:05:57,320 --> 00:05:59,540
tried to make a robot that nobody would

152
00:05:59,540 --> 00:06:01,370
like in terms of it being a creature and

153
00:06:01,370 --> 00:06:03,680
people adopted it into their family they

154
00:06:03,680 --> 00:06:07,280
named it like 99.9 percent of them and I

155
00:06:07,280 --> 00:06:09,770
want to go back and remember this we do

156
00:06:09,770 --> 00:06:11,780
not naturally relate to objects objects

157
00:06:11,780 --> 00:06:13,610
mediate and represent our relationships

158
00:06:13,610 --> 00:06:16,700
with others but we don't see it that way

159
00:06:16,700 --> 00:06:18,860
when it comes to robots we see the robot

160
00:06:18,860 --> 00:06:21,350
as the end point of a relationship we

161
00:06:21,350 --> 00:06:23,420
see the robot as the entity upon which

162
00:06:23,420 --> 00:06:25,460
we should confer the same personhood and

163
00:06:25,460 --> 00:06:30,350
respect that we would a living thing but

164
00:06:30,350 --> 00:06:32,240
a robots not a living thing and when we

165
00:06:32,240 --> 00:06:33,890
throw our emotions out into an

166
00:06:33,890 --> 00:06:36,010
emotionless algorithm out into the void

167
00:06:36,010 --> 00:06:37,630
sort of the

168
00:06:37,630 --> 00:06:38,950
did things happen and we've known this

169
00:06:38,950 --> 00:06:42,100
for a while this is kismet a robot built

170
00:06:42,100 --> 00:06:44,440
in the 1990s I sent the a Brazil at the

171
00:06:44,440 --> 00:06:46,900
MIT Media Lab and kismet was one of the

172
00:06:46,900 --> 00:06:48,730
first sociable robots but with the

173
00:06:48,730 --> 00:06:51,610
intention of interacting with humans so

174
00:06:51,610 --> 00:06:52,900
kids Matt's a capable of making all

175
00:06:52,900 --> 00:06:54,610
these incredible expressions playful

176
00:06:54,610 --> 00:06:57,310
serious angry sad and it really opened

177
00:06:57,310 --> 00:06:59,020
up all of these interesting avenues for

178
00:06:59,020 --> 00:07:00,580
research into how people interact with

179
00:07:00,580 --> 00:07:04,870
robots so one of the big researchers in

180
00:07:04,870 --> 00:07:06,790
this space is sherry Turkle she's a

181
00:07:06,790 --> 00:07:08,380
professor in research at MIT and she

182
00:07:08,380 --> 00:07:10,450
focuses on the impact of technology on

183
00:07:10,450 --> 00:07:12,580
people in society and back in the day

184
00:07:12,580 --> 00:07:14,380
she was researching the impact of robots

185
00:07:14,380 --> 00:07:16,900
like kismet on children she talked about

186
00:07:16,900 --> 00:07:18,250
this recently in a Washington Post

187
00:07:18,250 --> 00:07:21,700
article and she said the children took

188
00:07:21,700 --> 00:07:24,840
the robots behavior to signify feelings

189
00:07:24,840 --> 00:07:27,070
and at first were like okay well that

190
00:07:27,070 --> 00:07:29,350
seems like what I was supposed to do but

191
00:07:29,350 --> 00:07:31,260
then we think more about it we go oh

192
00:07:31,260 --> 00:07:35,560
wait the children took the not alive

193
00:07:35,560 --> 00:07:38,410
robots behavior to signify living

194
00:07:38,410 --> 00:07:41,170
feelings that affected them that

195
00:07:41,170 --> 00:07:43,120
affected their self-perception that

196
00:07:43,120 --> 00:07:44,650
affected their self-esteem and their

197
00:07:44,650 --> 00:07:48,040
state of mind if a robot interacted with

198
00:07:48,040 --> 00:07:49,750
them the kids thought the robot likes

199
00:07:49,750 --> 00:07:52,540
them but if it didn't work on cue they

200
00:07:52,540 --> 00:07:54,220
assumed that it was because the robot

201
00:07:54,220 --> 00:07:56,590
didn't like them for a variety of

202
00:07:56,590 --> 00:07:58,450
reasons it brought up all kinds of

203
00:07:58,450 --> 00:08:00,310
feelings you know some kids would try to

204
00:08:00,310 --> 00:08:01,810
comfort the broken robots they'd say

205
00:08:01,810 --> 00:08:03,790
please don't be scared and others would

206
00:08:03,790 --> 00:08:06,400
be really angry or really sad there was

207
00:08:06,400 --> 00:08:08,410
a child who tried to force-feed the

208
00:08:08,410 --> 00:08:10,000
robot his pen

209
00:08:10,000 --> 00:08:11,470
there was another who said he was sure

210
00:08:11,470 --> 00:08:13,300
that the room wasn't interacting with

211
00:08:13,300 --> 00:08:16,620
him because it liked his brothers better

212
00:08:16,620 --> 00:08:19,180
so we have a decidedly not alive

213
00:08:19,180 --> 00:08:21,370
interface avoiding these deep emotions

214
00:08:21,370 --> 00:08:23,320
that kids normally have based on their

215
00:08:23,320 --> 00:08:26,260
interactions with sentient things and

216
00:08:26,260 --> 00:08:27,790
when she spoke of this research dr.

217
00:08:27,790 --> 00:08:31,930
Terkel said this we were led to wonder

218
00:08:31,930 --> 00:08:36,479
whether a broken robot can break a child

219
00:08:36,479 --> 00:08:39,700
think about that these were kids who

220
00:08:39,700 --> 00:08:41,320
were led into a research setting to

221
00:08:41,320 --> 00:08:42,880
interact with creatures that they knew

222
00:08:42,880 --> 00:08:46,480
were robotic and yet still they felt

223
00:08:46,480 --> 00:08:48,970
care they felt anger they felt sadness

224
00:08:48,970 --> 00:08:50,590
they felt rejection

225
00:08:50,590 --> 00:08:53,380
felt human emotions so deeply that it

226
00:08:53,380 --> 00:08:55,540
affected the way that they felt and

227
00:08:55,540 --> 00:08:59,110
behaved we were led to wonder whether a

228
00:08:59,110 --> 00:09:02,980
broken robot can break a child or an

229
00:09:02,980 --> 00:09:09,130
adult so when it comes to robots our

230
00:09:09,130 --> 00:09:12,250
instinct for a life over machine is so

231
00:09:12,250 --> 00:09:16,090
strong that it's nearly involuntary it

232
00:09:16,090 --> 00:09:18,280
doesn't matter that we know we

233
00:09:18,280 --> 00:09:19,780
absolutely know that the robot is an

234
00:09:19,780 --> 00:09:21,730
object and we know that the robot is a

235
00:09:21,730 --> 00:09:24,130
robot that doesn't matter emotionally we

236
00:09:24,130 --> 00:09:27,900
still parse the robot as its own person

237
00:09:27,900 --> 00:09:30,880
this is tremendously powerful and it's

238
00:09:30,880 --> 00:09:32,650
really crazy that this happens

239
00:09:32,650 --> 00:09:34,690
I used to talk to people about this

240
00:09:34,690 --> 00:09:35,950
during the rise of all the digital

241
00:09:35,950 --> 00:09:37,240
assistants and I felt like people were

242
00:09:37,240 --> 00:09:39,130
really getting it they would be like no

243
00:09:39,130 --> 00:09:41,530
I'm the one who's in control I'm the one

244
00:09:41,530 --> 00:09:42,730
who decided to name the group out a

245
00:09:42,730 --> 00:09:45,100
Roomba and call it my son so you know I

246
00:09:45,100 --> 00:09:46,360
decided I needed to build a demo that

247
00:09:46,360 --> 00:09:48,640
showed just how simple and powerful

248
00:09:48,640 --> 00:09:53,760
emotive tech could be this is the firm

249
00:09:53,760 --> 00:09:55,990
the firm one is really simple it's just

250
00:09:55,990 --> 00:09:57,640
three servos and a microcontroller a

251
00:09:57,640 --> 00:09:59,740
little sound player if you squish him he

252
00:09:59,740 --> 00:10:01,150
struggles he struggles like he wants to

253
00:10:01,150 --> 00:10:03,190
get away the harder you squish him the

254
00:10:03,190 --> 00:10:05,200
more he struggles and what happens is

255
00:10:05,200 --> 00:10:07,660
our human emotions you know and our

256
00:10:07,660 --> 00:10:09,610
brains fill in the blanks when we see

257
00:10:09,610 --> 00:10:10,720
something that looks like it wants to

258
00:10:10,720 --> 00:10:12,580
live if we say if it wants to live it

259
00:10:12,580 --> 00:10:16,780
must be alive so I did a talk where I

260
00:10:16,780 --> 00:10:18,640
not only explained how the code was

261
00:10:18,640 --> 00:10:20,500
working I also explained every little

262
00:10:20,500 --> 00:10:21,790
node a trick that I was using to get

263
00:10:21,790 --> 00:10:22,930
people attached to the audience

264
00:10:22,930 --> 00:10:25,270
logically understood what was going on

265
00:10:25,270 --> 00:10:27,220
they logically got exactly how it worked

266
00:10:27,220 --> 00:10:28,930
to the point where if logic was the only

267
00:10:28,930 --> 00:10:30,700
dominant thing nobody should have been

268
00:10:30,700 --> 00:10:33,250
fooled by this so this was the end of

269
00:10:33,250 --> 00:10:38,620
the talk this is me sort of talking

270
00:10:38,620 --> 00:10:40,660
about this and you know like holding the

271
00:10:40,660 --> 00:10:42,340
worm and then I held it at arm's laying

272
00:10:42,340 --> 00:10:45,250
in pain and I think it's a good question

273
00:10:45,250 --> 00:10:47,260
to ask whether we're ever gonna be able

274
00:10:47,260 --> 00:10:49,180
to see something that appears alive and

275
00:10:49,180 --> 00:10:51,550
feel nothing to watch it struggle and

276
00:10:51,550 --> 00:10:56,950
feel nothing to watch it die and feel

277
00:10:56,950 --> 00:10:59,329
nothing

278
00:10:59,329 --> 00:11:02,029
living things need other living things

279
00:11:02,029 --> 00:11:15,480
and we are alive aren't we I'm pretty

280
00:11:15,480 --> 00:11:17,189
sure they added the applause at the end

281
00:11:17,189 --> 00:11:20,459
there because I remember walking out the

282
00:11:20,459 --> 00:11:22,439
stage to gasping followed by stunned

283
00:11:22,439 --> 00:11:26,279
silence and you know this is something

284
00:11:26,279 --> 00:11:28,559
that was oh and I also did actually have

285
00:11:28,559 --> 00:11:30,389
the words this is a robot behind me the

286
00:11:30,389 --> 00:11:33,919
whole time you know just for safety

287
00:11:33,919 --> 00:11:36,179
there's a cognitive scientist who came

288
00:11:36,179 --> 00:11:38,459
up to me after the talk and she told me

289
00:11:38,459 --> 00:11:40,679
that even though she understood every

290
00:11:40,679 --> 00:11:42,779
trick that I was using to make the worm

291
00:11:42,779 --> 00:11:43,709
seem alive

292
00:11:43,709 --> 00:11:46,499
she still cried when I broke the robot

293
00:11:46,499 --> 00:11:50,699
in half she cried so this is not a

294
00:11:50,699 --> 00:11:52,709
reaction based on logic it's based on

295
00:11:52,709 --> 00:11:54,660
instinct it's based on our human

296
00:11:54,660 --> 00:11:58,319
instinct for empathy it's based on this

297
00:11:58,319 --> 00:12:00,869
instinct we have to care and love each

298
00:12:00,869 --> 00:12:02,609
other right it's based on our

299
00:12:02,609 --> 00:12:04,910
interesting to assume life over not life

300
00:12:04,910 --> 00:12:07,139
and that's sort of beautiful but it's

301
00:12:07,139 --> 00:12:08,699
also like scary when you apply it to

302
00:12:08,699 --> 00:12:10,589
robots because of the way that it

303
00:12:10,589 --> 00:12:14,850
affects living people so you know even

304
00:12:14,850 --> 00:12:16,350
if we were trying to make a well

305
00:12:16,350 --> 00:12:18,149
intended robot right like our feelings

306
00:12:18,149 --> 00:12:20,129
have consequences and emotive robots can

307
00:12:20,129 --> 00:12:21,209
be dangerous if they're used without

308
00:12:21,209 --> 00:12:23,009
forethought so like let's imagine a

309
00:12:23,009 --> 00:12:24,299
robot that's about for a nice friendly

310
00:12:24,299 --> 00:12:26,189
purpose like it works in a factory it's

311
00:12:26,189 --> 00:12:27,239
meant to generate the sense of

312
00:12:27,239 --> 00:12:29,610
camaraderie among its human peers to

313
00:12:29,610 --> 00:12:32,069
really like make the workplace a happier

314
00:12:32,069 --> 00:12:34,259
more wonderful place to be every day and

315
00:12:34,259 --> 00:12:37,079
that seems okay at first but what

316
00:12:37,079 --> 00:12:39,149
happens when the robot malfunctions on

317
00:12:39,149 --> 00:12:42,269
the line and we need to pull the plug in

318
00:12:42,269 --> 00:12:44,220
the two seconds that it takes us to

319
00:12:44,220 --> 00:12:46,079
reclassify our Robo friend as a machine

320
00:12:46,079 --> 00:12:48,389
will it harm someone who is actually

321
00:12:48,389 --> 00:12:52,679
alive and I mean that's not even going

322
00:12:52,679 --> 00:12:54,480
into this potential danger which is

323
00:12:54,480 --> 00:12:56,910
perhaps more salient remember this

324
00:12:56,910 --> 00:12:58,399
structure the structure of objects

325
00:12:58,399 --> 00:13:00,809
emotive objects mediate and support

326
00:13:00,809 --> 00:13:03,600
human relationships and a robot is an

327
00:13:03,600 --> 00:13:07,799
emotive object which means there's

328
00:13:07,799 --> 00:13:09,869
someone behind every emotive object

329
00:13:09,869 --> 00:13:11,220
there's someone behind every emotive

330
00:13:11,220 --> 00:13:11,930
robot there

331
00:13:11,930 --> 00:13:13,040
someone who built it there's someone who

332
00:13:13,040 --> 00:13:15,080
controls it so who is there who is that

333
00:13:15,080 --> 00:13:17,899
person right there what can they do what

334
00:13:17,899 --> 00:13:21,080
is their intention on the answer that is

335
00:13:21,080 --> 00:13:22,430
actually sometimes really wonderful

336
00:13:22,430 --> 00:13:23,899
things so let's lighten this up for a

337
00:13:23,899 --> 00:13:28,300
second and talk about therapy robots

338
00:13:28,570 --> 00:13:32,480
this is huggable huggable was built as a

339
00:13:32,480 --> 00:13:34,520
support robot in hospital settings so

340
00:13:34,520 --> 00:13:35,990
basically this gives you the option to

341
00:13:35,990 --> 00:13:37,880
have a teddy bear come in and talk to a

342
00:13:37,880 --> 00:13:39,410
kid who is otherwise having just a

343
00:13:39,410 --> 00:13:41,209
terrible time being stuck in an

344
00:13:41,209 --> 00:13:43,040
unfamiliar and unfriendly place day

345
00:13:43,040 --> 00:13:45,350
after day after day huggable allows you

346
00:13:45,350 --> 00:13:47,330
to wear an entirely different face an

347
00:13:47,330 --> 00:13:49,220
entirely different form the form of a

348
00:13:49,220 --> 00:13:50,870
creature who doesn't know or care that

349
00:13:50,870 --> 00:13:53,570
you're sick a living thing that doesn't

350
00:13:53,570 --> 00:13:55,430
wear white coat doesn't betray these

351
00:13:55,430 --> 00:13:57,290
subtle motions of like pity or

352
00:13:57,290 --> 00:13:59,600
awkwardness a living thing that truly

353
00:13:59,600 --> 00:14:03,320
reacts to you and you know the nice

354
00:14:03,320 --> 00:14:04,970
thing about this before you get like too

355
00:14:04,970 --> 00:14:06,380
scared about like a hypothetical

356
00:14:06,380 --> 00:14:08,510
algorithm controlling this or excited

357
00:14:08,510 --> 00:14:10,820
about that one of my favorite parts of

358
00:14:10,820 --> 00:14:12,320
huggable is that this barrier is

359
00:14:12,320 --> 00:14:14,149
controlled by a person this is

360
00:14:14,149 --> 00:14:15,680
essentially remote puppetry but

361
00:14:15,680 --> 00:14:17,300
specifically a kind of remote puppetry

362
00:14:17,300 --> 00:14:20,450
that you know takes into account an

363
00:14:20,450 --> 00:14:22,220
existing situation at an existing

364
00:14:22,220 --> 00:14:23,450
relationship with somebody who already

365
00:14:23,450 --> 00:14:25,220
knows this kid with somebody who can

366
00:14:25,220 --> 00:14:26,900
it's somebody who can react to the kid

367
00:14:26,900 --> 00:14:28,520
in real time who's working with the

368
00:14:28,520 --> 00:14:29,959
providers who's working with the team

369
00:14:29,959 --> 00:14:31,760
who could actually prevent like

370
00:14:31,760 --> 00:14:34,339
disastrous movements or snafus that

371
00:14:34,339 --> 00:14:36,980
could you know cause a problem if the

372
00:14:36,980 --> 00:14:39,529
robot - if the robot were to malfunction

373
00:14:39,529 --> 00:14:41,959
but the main thing is this the robot is

374
00:14:41,959 --> 00:14:44,120
actually augmenting an existing

375
00:14:44,120 --> 00:14:46,310
relationship right between two actual

376
00:14:46,310 --> 00:14:48,650
people - actual living humans maybe

377
00:14:48,650 --> 00:14:50,000
you're wearing a different face but

378
00:14:50,000 --> 00:14:51,350
you're still like a human person on the

379
00:14:51,350 --> 00:14:53,510
other side of the line when we ask who's

380
00:14:53,510 --> 00:14:55,190
there someone answers and it's someone

381
00:14:55,190 --> 00:15:00,200
human being able to augment a

382
00:15:00,200 --> 00:15:01,850
relationship doing people is really huge

383
00:15:01,850 --> 00:15:03,770
and when you can achieve therapeutically

384
00:15:03,770 --> 00:15:06,230
is really incredible this is keep on one

385
00:15:06,230 --> 00:15:07,970
of my favorite emotive robots I'm just

386
00:15:07,970 --> 00:15:10,339
gonna let this loop keep on is

387
00:15:10,339 --> 00:15:11,810
frequently discussed in the context of

388
00:15:11,810 --> 00:15:13,520
teaching children particularly kids on

389
00:15:13,520 --> 00:15:15,200
the autism spectrum attention and

390
00:15:15,200 --> 00:15:16,850
emotion as expressed through simple

391
00:15:16,850 --> 00:15:18,800
movement so it allows the provider to

392
00:15:18,800 --> 00:15:20,300
wear a different face a simpler face a

393
00:15:20,300 --> 00:15:22,700
less confusing face while teaching a

394
00:15:22,700 --> 00:15:25,760
really essential concept so much like

395
00:15:25,760 --> 00:15:28,010
keep on was not built with an automated

396
00:15:28,010 --> 00:15:30,020
interface immediately right it was built

397
00:15:30,020 --> 00:15:31,520
with the idea of having a human in the

398
00:15:31,520 --> 00:15:35,240
loop and keeping a human in the loop is

399
00:15:35,240 --> 00:15:36,950
really important not just an emotive

400
00:15:36,950 --> 00:15:41,330
technology like you know technology like

401
00:15:41,330 --> 00:15:42,920
we don't like self-driving cars go

402
00:15:42,920 --> 00:15:44,300
around without the option for human

403
00:15:44,300 --> 00:15:46,340
intervention and we wouldn't build like

404
00:15:46,340 --> 00:15:48,200
an automated sentry gun or something

405
00:15:48,200 --> 00:15:50,090
without that without the ability for

406
00:15:50,090 --> 00:15:51,470
humans you switch it off or at least

407
00:15:51,470 --> 00:15:53,840
confirm what it's doing but when it

408
00:15:53,840 --> 00:15:56,510
comes to emotive robots we kind of just

409
00:15:56,510 --> 00:15:58,340
fully automate them frequently we will

410
00:15:58,340 --> 00:16:00,230
fully automate one side of a highly

411
00:16:00,230 --> 00:16:01,820
emotional conversation just let people

412
00:16:01,820 --> 00:16:04,400
deal with the fallout from that so let's

413
00:16:04,400 --> 00:16:07,070
talk about a famous example of fallout

414
00:16:07,070 --> 00:16:09,770
from a non robotic automated interface

415
00:16:09,770 --> 00:16:11,840
right it was Microsoft's chat BOTS I

416
00:16:11,840 --> 00:16:13,190
think this was online for like one or

417
00:16:13,190 --> 00:16:15,890
two days only right and in that short

418
00:16:15,890 --> 00:16:17,540
span of time people realize they could

419
00:16:17,540 --> 00:16:19,070
train tey to have different opinions and

420
00:16:19,070 --> 00:16:24,250
say different things and so they did I

421
00:16:24,250 --> 00:16:27,650
imagine that Microsoft meant Forte to be

422
00:16:27,650 --> 00:16:29,510
an automated interface that inspired

423
00:16:29,510 --> 00:16:32,350
people with the social potential of AI

424
00:16:32,350 --> 00:16:35,330
but you know without a human person in

425
00:16:35,330 --> 00:16:37,700
the loop mediating the algorithm and

426
00:16:37,700 --> 00:16:40,070
working to make it more human there's a

427
00:16:40,070 --> 00:16:42,140
lot of dangers right in general

428
00:16:42,140 --> 00:16:43,550
imagine the equivalent of Taiba in

429
00:16:43,550 --> 00:16:45,010
physical form and in your house

430
00:16:45,010 --> 00:16:47,270
interacting with your most emotionally

431
00:16:47,270 --> 00:16:49,580
vulnerable relative and that's kind of

432
00:16:49,580 --> 00:16:52,040
you know where we are now or like where

433
00:16:52,040 --> 00:16:53,330
we could get to hypothetically you're

434
00:16:53,330 --> 00:16:55,130
removing a human from the loop how good

435
00:16:55,130 --> 00:16:57,830
is that probably not good to be safe I

436
00:16:57,830 --> 00:16:59,090
think having a human in the loop is a

437
00:16:59,090 --> 00:17:01,940
really good idea and having this

438
00:17:01,940 --> 00:17:03,470
relationship reflect an actual

439
00:17:03,470 --> 00:17:05,000
relationship between two humans I think

440
00:17:05,000 --> 00:17:08,839
is also a good idea but you know what's

441
00:17:08,839 --> 00:17:10,490
more common as you see this no humans in

442
00:17:10,490 --> 00:17:12,800
the loop algorithmic stuff relationship

443
00:17:12,800 --> 00:17:14,449
with an entity that you like don't know

444
00:17:14,449 --> 00:17:16,160
at all and you believe that the

445
00:17:16,160 --> 00:17:17,990
relationship ends here that stops at the

446
00:17:17,990 --> 00:17:20,930
robot but it doesn't as an obvious

447
00:17:20,930 --> 00:17:23,180
example thirty nine million people on

448
00:17:23,180 --> 00:17:25,130
Amazon echoes I think probably at this

449
00:17:25,130 --> 00:17:27,109
point it's more than 39 million but

450
00:17:27,109 --> 00:17:29,720
people are convinced that the echo is

451
00:17:29,720 --> 00:17:31,700
endpoint of their relationship even if

452
00:17:31,700 --> 00:17:33,410
they logically know that Amazon is out

453
00:17:33,410 --> 00:17:34,970
there they're emotionally convinced by

454
00:17:34,970 --> 00:17:38,000
Alexa this worked so well that Amazon

455
00:17:38,000 --> 00:17:39,320
was able to make a super

456
00:17:39,320 --> 00:17:41,540
full commercial about how much better it

457
00:17:41,540 --> 00:17:43,310
is to talk to Alexa than an actual

458
00:17:43,310 --> 00:17:47,870
person or a celebrity but Alexa is not a

459
00:17:47,870 --> 00:17:49,520
person and the relationship does not

460
00:17:49,520 --> 00:17:51,470
stop with the robot it goes on your

461
00:17:51,470 --> 00:17:53,630
relationship with this emotive object is

462
00:17:53,630 --> 00:17:55,130
representing and maintaining your

463
00:17:55,130 --> 00:17:59,240
relationship with amazon.com so you are

464
00:17:59,240 --> 00:18:01,190
not asking Alexa for dating advice

465
00:18:01,190 --> 00:18:04,630
you are asking Amazon for dating advice

466
00:18:04,630 --> 00:18:07,430
you are not letting Alexa teacher kid

467
00:18:07,430 --> 00:18:10,910
manners your kid is asking amazon.com a

468
00:18:10,910 --> 00:18:12,860
business with business priorities for

469
00:18:12,860 --> 00:18:15,620
etiquette lessons you're not putting a

470
00:18:15,620 --> 00:18:17,480
friendly robot in your kid's room you're

471
00:18:17,480 --> 00:18:19,400
putting an Amazon point of sale in your

472
00:18:19,400 --> 00:18:23,600
kid's room so though we think that the

473
00:18:23,600 --> 00:18:26,120
relationship ends with the robot it

474
00:18:26,120 --> 00:18:33,320
doesn't so when you relate to or help a

475
00:18:33,320 --> 00:18:35,060
robot write you think that you're

476
00:18:35,060 --> 00:18:37,310
helping the robot do what it wants to do

477
00:18:37,310 --> 00:18:39,590
you think you're helping it accomplish

478
00:18:39,590 --> 00:18:42,650
its goals but that's not really true is

479
00:18:42,650 --> 00:18:44,150
it you know you're doing what the

480
00:18:44,150 --> 00:18:45,950
technologist or the company wants you to

481
00:18:45,950 --> 00:18:50,390
do to accomplish their goals so please

482
00:18:50,390 --> 00:18:51,860
ask this question every time you

483
00:18:51,860 --> 00:18:53,320
encounter an emotive robot who's there

484
00:18:53,320 --> 00:18:56,990
is it someone who cares about you or not

485
00:18:56,990 --> 00:19:00,650
is it someone you know is it someone

486
00:19:00,650 --> 00:19:02,030
with good intentions is someone with

487
00:19:02,030 --> 00:19:06,080
whose goals are aligned with yours is it

488
00:19:06,080 --> 00:19:07,880
a person you know well is it a company

489
00:19:07,880 --> 00:19:10,970
that you don't know at all is it

490
00:19:10,970 --> 00:19:14,620
possible for just you to have control

491
00:19:16,789 --> 00:19:20,010
when someone is in this position what do

492
00:19:20,010 --> 00:19:22,919
they do with this power if the system is

493
00:19:22,919 --> 00:19:25,260
automated how did they build algorithm

494
00:19:25,260 --> 00:19:28,440
what is and was their goal what is the

495
00:19:28,440 --> 00:19:31,320
system for what are they selling are

496
00:19:31,320 --> 00:19:33,090
they selling other goods on a website

497
00:19:33,090 --> 00:19:35,309
are they selling a cloud subscription

498
00:19:35,309 --> 00:19:39,659
service it's great to have someone in

499
00:19:39,659 --> 00:19:41,429
the loop but you better hope it's

500
00:19:41,429 --> 00:19:45,570
someone you trust and you know there's

501
00:19:45,570 --> 00:19:48,090
sort of this other issue where if you

502
00:19:48,090 --> 00:19:49,230
know you do trust them you can have

503
00:19:49,230 --> 00:19:50,549
really great effects obviously a

504
00:19:50,549 --> 00:19:51,929
wonderful therapy from a person who

505
00:19:51,929 --> 00:19:55,020
knows you but if not this relationship

506
00:19:55,020 --> 00:19:55,950
starts to feel kind of uncomfortable

507
00:19:55,950 --> 00:19:57,990
it's like a person dressed as a robot

508
00:19:57,990 --> 00:20:00,240
and if you don't trust anybody to be in

509
00:20:00,240 --> 00:20:02,460
the loop you start to have the question

510
00:20:02,460 --> 00:20:04,980
can it just be me is it possible for you

511
00:20:04,980 --> 00:20:06,539
to just put yourself there is it

512
00:20:06,539 --> 00:20:08,250
possible for you to be the person with

513
00:20:08,250 --> 00:20:10,980
full control of this powerful technology

514
00:20:10,980 --> 00:20:12,690
that's likely to alter your emotional

515
00:20:12,690 --> 00:20:15,120
state can you just be the person who's

516
00:20:15,120 --> 00:20:18,260
in control the whole time so I argue yes

517
00:20:18,260 --> 00:20:21,210
I think as we've already seen logic

518
00:20:21,210 --> 00:20:22,770
doesn't really matter with this one you

519
00:20:22,770 --> 00:20:23,909
know when it comes to a mode of robots

520
00:20:23,909 --> 00:20:25,230
we can look behind the curtain of The

521
00:20:25,230 --> 00:20:26,730
Wizard of Oz and still be really stunned

522
00:20:26,730 --> 00:20:30,690
and convinced by the display so why

523
00:20:30,690 --> 00:20:31,799
don't we just heat control ourselves

524
00:20:31,799 --> 00:20:33,809
why don't we tune the emotions and the

525
00:20:33,809 --> 00:20:35,010
emotional responses to help us

526
00:20:35,010 --> 00:20:37,260
accomplish things that we want to do we

527
00:20:37,260 --> 00:20:38,610
could use this technology to really like

528
00:20:38,610 --> 00:20:40,169
augment our abilities to relate to

529
00:20:40,169 --> 00:20:42,179
ourselves to improve ourselves to do

530
00:20:42,179 --> 00:20:44,279
things that you know we might not feel

531
00:20:44,279 --> 00:20:45,870
encouraged to do otherwise make it all

532
00:20:45,870 --> 00:20:49,320
our choice so here's an extremely simple

533
00:20:49,320 --> 00:20:51,080
example I mean this is like really easy

534
00:20:51,080 --> 00:20:52,860
everybody can build this this is one of

535
00:20:52,860 --> 00:20:54,149
the hacks I did when I was doing the

536
00:20:54,149 --> 00:20:55,649
video series everyday BOTS on the verge

537
00:20:55,649 --> 00:20:57,659
this is the healthy eating fridge magnet

538
00:20:57,659 --> 00:21:00,390
I had a lot of people especially those

539
00:21:00,390 --> 00:21:01,890
who work from home telling me that they

540
00:21:01,890 --> 00:21:04,230
either forgot to eat or they would like

541
00:21:04,230 --> 00:21:05,490
go to a kitchen they couldn't stop

542
00:21:05,490 --> 00:21:06,720
themselves from going to the fridge over

543
00:21:06,720 --> 00:21:07,799
and over you know even though they

544
00:21:07,799 --> 00:21:08,940
didn't want to so the healthy eating

545
00:21:08,940 --> 00:21:11,309
fridge magnet was a way for you to

546
00:21:11,309 --> 00:21:13,230
extrude your own conscience and put it

547
00:21:13,230 --> 00:21:15,000
as a glanceable emotive reminder on the

548
00:21:15,000 --> 00:21:16,710
fridge so if it was time for you to eat

549
00:21:16,710 --> 00:21:19,529
and you needed to be reminded you might

550
00:21:19,529 --> 00:21:22,640
see just how hungry he and you are

551
00:21:22,640 --> 00:21:25,020
keeping snacking law and you like really

552
00:21:25,020 --> 00:21:27,990
didn't want to be you might get mad

553
00:21:27,990 --> 00:21:29,820
and if you keep opening the fridge over

554
00:21:29,820 --> 00:21:31,200
and over again even though you had sworn

555
00:21:31,200 --> 00:21:32,730
that you weren't gonna he might get

556
00:21:32,730 --> 00:21:37,260
matter all right this is not a face

557
00:21:37,260 --> 00:21:38,580
though right let's acknowledge that this

558
00:21:38,580 --> 00:21:40,680
is like two years in some paper but we

559
00:21:40,680 --> 00:21:42,930
parse it as a face and it affects us

560
00:21:42,930 --> 00:21:44,370
like it's a face it affects us and

561
00:21:44,370 --> 00:21:45,870
there's a weird glanceable emotion away

562
00:21:45,870 --> 00:21:51,420
and that's kind of amazing and the key

563
00:21:51,420 --> 00:21:53,280
here is this is a hack that wouldn't

564
00:21:53,280 --> 00:21:55,200
work if you weren't in control of the

565
00:21:55,200 --> 00:21:57,270
system I mean first of all everybody's

566
00:21:57,270 --> 00:21:58,680
dietary knees are totally different so

567
00:21:58,680 --> 00:22:01,440
you need control there but also the way

568
00:22:01,440 --> 00:22:03,720
people are motivated is very different

569
00:22:03,720 --> 00:22:05,970
I've had people request robots that are

570
00:22:05,970 --> 00:22:07,740
happy when they're sad so that they get

571
00:22:07,740 --> 00:22:09,180
cheered hop up and I've had people

572
00:22:09,180 --> 00:22:11,010
request robots that are sad when they're

573
00:22:11,010 --> 00:22:12,270
sad because then they feel like

574
00:22:12,270 --> 00:22:13,770
someone's there feeling what they're

575
00:22:13,770 --> 00:22:17,280
feeling so what we want to be motivated

576
00:22:17,280 --> 00:22:19,560
for and how we're motivated is very

577
00:22:19,560 --> 00:22:21,780
nuanced it vary significantly from

578
00:22:21,780 --> 00:22:26,850
person to person so it's very important

579
00:22:26,850 --> 00:22:28,950
for you know this to actually be custom

580
00:22:28,950 --> 00:22:31,740
let's give me a really nice system it

581
00:22:31,740 --> 00:22:32,910
can be a system where you're in control

582
00:22:32,910 --> 00:22:34,530
when you call it into the void asking

583
00:22:34,530 --> 00:22:36,480
who's there you're calling through a

584
00:22:36,480 --> 00:22:39,000
robot back to yourself you're connecting

585
00:22:39,000 --> 00:22:41,070
with and improving you know yourself

586
00:22:41,070 --> 00:22:42,480
you're improving this relationship with

587
00:22:42,480 --> 00:22:44,760
yourself not with some Rando who's in

588
00:22:44,760 --> 00:22:46,140
control of the system manipulating your

589
00:22:46,140 --> 00:22:48,720
emotions etc the only person here doing

590
00:22:48,720 --> 00:22:51,210
the manipulation is really you it's also

591
00:22:51,210 --> 00:22:52,560
a weird thing it means that improving

592
00:22:52,560 --> 00:22:53,910
your relationship with a robot isn't

593
00:22:53,910 --> 00:22:55,140
improving your relationship with an

594
00:22:55,140 --> 00:22:56,340
external party it's improving your

595
00:22:56,340 --> 00:22:58,170
relationship to yourself so it'll be

596
00:22:58,170 --> 00:22:59,640
cool if more people built robots like

597
00:22:59,640 --> 00:23:00,840
this they try to do this as much as

598
00:23:00,840 --> 00:23:03,360
possible but it's it's Ritter to see it

599
00:23:03,360 --> 00:23:05,820
these days really but just because the

600
00:23:05,820 --> 00:23:06,990
spirit doesn't mean that you as a

601
00:23:06,990 --> 00:23:08,430
consumer shouldn't be able to have it

602
00:23:08,430 --> 00:23:11,100
shouldn't be able to demand it as a side

603
00:23:11,100 --> 00:23:14,670
bar I'm just gonna say this please know

604
00:23:14,670 --> 00:23:16,260
who is behind your robot if you're like

605
00:23:16,260 --> 00:23:18,180
going out and getting an emotive robot

606
00:23:18,180 --> 00:23:19,560
don't make a relationship with anyone

607
00:23:19,560 --> 00:23:22,460
you don't want to also good life advice

608
00:23:22,460 --> 00:23:24,510
this should go double for vulnerable

609
00:23:24,510 --> 00:23:26,700
populations also you know companies are

610
00:23:26,700 --> 00:23:28,050
frequently creating these emotional

611
00:23:28,050 --> 00:23:29,970
support robots for children often

612
00:23:29,970 --> 00:23:31,500
children with emotional developmental

613
00:23:31,500 --> 00:23:33,390
issues and for the elderly frequently

614
00:23:33,390 --> 00:23:34,950
the elderly who are affected by dementia

615
00:23:34,950 --> 00:23:37,620
and you know you're affected by your

616
00:23:37,620 --> 00:23:39,540
Roomba consider how it might feel to be

617
00:23:39,540 --> 00:23:41,610
elderly and bad with technology

618
00:23:41,610 --> 00:23:43,910
and infected by Roma or young and

619
00:23:43,910 --> 00:23:46,260
unclear on the difference between alive

620
00:23:46,260 --> 00:23:48,480
and not alive and healthy and unhealthy

621
00:23:48,480 --> 00:23:50,549
emotional states and affected by an

622
00:23:50,549 --> 00:23:54,840
emotive robot so regardless I think

623
00:23:54,840 --> 00:23:56,670
you're likely to see just like a ton of

624
00:23:56,670 --> 00:23:58,260
emotive robots in a mode of chat come

625
00:23:58,260 --> 00:24:00,059
out on the market there's a lot out

626
00:24:00,059 --> 00:24:02,429
there already I'm not gonna tell you

627
00:24:02,429 --> 00:24:03,630
it's bad and I'm gonna tell you how to

628
00:24:03,630 --> 00:24:06,360
buy it but I would encourage you to

629
00:24:06,360 --> 00:24:08,730
consider the power of the technology how

630
00:24:08,730 --> 00:24:11,160
it defies our logical brains and I would

631
00:24:11,160 --> 00:24:13,679
encourage you to ask this who's behind

632
00:24:13,679 --> 00:24:23,640
it who controls it who's there that's

633
00:24:23,640 --> 00:24:27,860
about it for me any questions

634
00:24:27,870 --> 00:24:35,360
[Applause]

635
00:24:47,570 --> 00:24:49,730
mirrored feedback loops can spiral in

636
00:24:49,730 --> 00:24:51,309
the opposite direction as well we've had

637
00:24:51,309 --> 00:24:53,929
mass shootings and such in this country

638
00:24:53,929 --> 00:24:56,900
because people have on their own kind of

639
00:24:56,900 --> 00:25:01,669
decided that what most interests them is

640
00:25:01,669 --> 00:25:03,260
best for everyone

641
00:25:03,260 --> 00:25:07,340
right how would you give such a

642
00:25:07,340 --> 00:25:15,470
reflective robot or mirror to so I guess

643
00:25:15,470 --> 00:25:16,640
the question is sort of you know

644
00:25:16,640 --> 00:25:18,110
especially when you're talking about

645
00:25:18,110 --> 00:25:19,520
people who are emotionally vulnerable or

646
00:25:19,520 --> 00:25:21,020
her trying to find their footing people

647
00:25:21,020 --> 00:25:22,309
where you might give them a mode of

648
00:25:22,309 --> 00:25:25,850
robots there's a very there's very easy

649
00:25:25,850 --> 00:25:27,440
ways for them to as their programming

650
00:25:27,440 --> 00:25:28,760
and themselves create unhealthy

651
00:25:28,760 --> 00:25:30,049
behaviors or encourage unhealthy

652
00:25:30,049 --> 00:25:32,090
behaviors that would be bad for them and

653
00:25:32,090 --> 00:25:33,740
for everybody else and how do you

654
00:25:33,740 --> 00:25:35,450
prevent that so I really only talked

655
00:25:35,450 --> 00:25:37,340
about like half of kind of emotive tech

656
00:25:37,340 --> 00:25:38,570
here I talked about how you connect it

657
00:25:38,570 --> 00:25:40,070
back to yourself and how some of the

658
00:25:40,070 --> 00:25:42,470
structures that we see are like a little

659
00:25:42,470 --> 00:25:45,799
achy and I I think that there's a whole

660
00:25:45,799 --> 00:25:47,899
other side of this right and the side is

661
00:25:47,899 --> 00:25:49,970
how we connect to each other using these

662
00:25:49,970 --> 00:25:51,860
interfaces and how you make sure that

663
00:25:51,860 --> 00:25:53,779
what you're doing is you're establishing

664
00:25:53,779 --> 00:25:55,309
community and you're establishing care

665
00:25:55,309 --> 00:25:58,700
rather than having an object to be you

666
00:25:58,700 --> 00:26:02,270
know sort of a an algorithmic feedback

667
00:26:02,270 --> 00:26:05,179
loop having enough not just necessarily

668
00:26:05,179 --> 00:26:06,649
you in the loop if you need support but

669
00:26:06,649 --> 00:26:08,539
having real human support in the loop

670
00:26:08,539 --> 00:26:11,029
that can help you with that so that's

671
00:26:11,029 --> 00:26:16,029
sort of my answer there other questions

672
00:26:18,970 --> 00:26:22,820
save the walking so this reminded me

673
00:26:22,820 --> 00:26:26,779
that I heard a story of how following on

674
00:26:26,779 --> 00:26:31,149
from Google's example of having their

675
00:26:31,149 --> 00:26:34,220
automated system to book you know

676
00:26:34,220 --> 00:26:35,990
appointments for you I think we've all

677
00:26:35,990 --> 00:26:38,740
seen that there was another reference to

678
00:26:38,740 --> 00:26:41,179
something happening in China where

679
00:26:41,179 --> 00:26:43,429
apparently they're already doing this

680
00:26:43,429 --> 00:26:47,210
and they described an approach a use

681
00:26:47,210 --> 00:26:48,950
case which was very similar to this

682
00:26:48,950 --> 00:26:51,020
emotive support type thing and I'm just

683
00:26:51,020 --> 00:26:53,419
curious if you are aware of what's going

684
00:26:53,419 --> 00:26:55,820
on over there and how that might feed

685
00:26:55,820 --> 00:26:56,840
into what you

686
00:26:56,840 --> 00:26:59,210
been talking tonight so did you hear the

687
00:26:59,210 --> 00:27:01,130
the story I'm talking about I haven't

688
00:27:01,130 --> 00:27:02,690
read that particular story I do

689
00:27:02,690 --> 00:27:05,240
frequently hear about Check Into anyway

690
00:27:05,240 --> 00:27:09,980
it was particularly about how someone

691
00:27:09,980 --> 00:27:13,250
would call the person that's not someone

692
00:27:13,250 --> 00:27:16,400
and putting a person in there this

693
00:27:16,400 --> 00:27:19,159
robotic entity whatever this AI would

694
00:27:19,159 --> 00:27:21,529
call someone to support them through a

695
00:27:21,529 --> 00:27:23,960
hard time and they it sounded very

696
00:27:23,960 --> 00:27:26,150
similar to what you were really

697
00:27:26,150 --> 00:27:28,010
describing here and I mean people have

698
00:27:28,010 --> 00:27:30,559
talked about sort of the idea of robots

699
00:27:30,559 --> 00:27:32,360
for support for like a really long time

700
00:27:32,360 --> 00:27:34,210
I don't remember what the first

701
00:27:34,210 --> 00:27:37,700
psychotherapy like program was Eliza I

702
00:27:37,700 --> 00:27:38,750
was trying to remember when that

703
00:27:38,750 --> 00:27:42,649
happened and like the 70s or the 80s

704
00:27:42,649 --> 00:27:44,750
right it's like a really it's a it's a

705
00:27:44,750 --> 00:27:46,340
much older robot but it was basically

706
00:27:46,340 --> 00:27:48,440
like you would write to this interface

707
00:27:48,440 --> 00:27:50,240
and you would type like it would be like

708
00:27:50,240 --> 00:27:51,350
you know what's up like what's wrong

709
00:27:51,350 --> 00:27:53,570
today you'd be like I'm really mad at

710
00:27:53,570 --> 00:27:55,520
blank like I'm really mad at my dog you

711
00:27:55,520 --> 00:27:57,220
know be like tell me more about your dog

712
00:27:57,220 --> 00:27:59,690
supposed to be like select last words

713
00:27:59,690 --> 00:28:01,340
said you know like say tell me more

714
00:28:01,340 --> 00:28:03,710
about blank and people found this like

715
00:28:03,710 --> 00:28:06,409
ridiculously effective and you know

716
00:28:06,409 --> 00:28:07,549
people actually felt that they were

717
00:28:07,549 --> 00:28:09,649
deriving good feedback from this and

718
00:28:09,649 --> 00:28:11,539
it's really interesting how something

719
00:28:11,539 --> 00:28:13,039
that's simple can be really helpful for

720
00:28:13,039 --> 00:28:14,299
us but I think one of the things that's

721
00:28:14,299 --> 00:28:15,980
important throughout is what is the

722
00:28:15,980 --> 00:28:18,049
framing of that because there's also you

723
00:28:18,049 --> 00:28:19,130
know you have to be conscious when

724
00:28:19,130 --> 00:28:20,240
you're working with an interface like

725
00:28:20,240 --> 00:28:22,190
that that it is not a person that what

726
00:28:22,190 --> 00:28:23,270
you're essentially doing here is

727
00:28:23,270 --> 00:28:25,309
journaling you know you're doing like

728
00:28:25,309 --> 00:28:27,679
journaling with with the augmentation of

729
00:28:27,679 --> 00:28:30,350
a piece of technology and so I think

730
00:28:30,350 --> 00:28:31,970
that framing is really important just so

731
00:28:31,970 --> 00:28:34,070
that we don't you know get further

732
00:28:34,070 --> 00:28:37,658
separated from ourselves in each other

733
00:28:42,279 --> 00:28:46,880
do you or Google why or why not okay so

734
00:28:46,880 --> 00:28:51,260
we have one Amazon echo in the house we

735
00:28:51,260 --> 00:28:54,289
bought an echo in order to do an

736
00:28:54,289 --> 00:28:56,659
interface experiment which I will now

737
00:28:56,659 --> 00:28:58,139
talk about

738
00:28:58,139 --> 00:29:01,330
okay so I am I'm not really like

739
00:29:01,330 --> 00:29:03,370
interested in having an Amazon echo for

740
00:29:03,370 --> 00:29:04,389
like all the reasons you just talked

741
00:29:04,389 --> 00:29:07,120
about but I am really interested in how

742
00:29:07,120 --> 00:29:09,730
much people like Alexa and you know how

743
00:29:09,730 --> 00:29:11,139
convincing this interface is for people

744
00:29:11,139 --> 00:29:12,820
and I think that they did a fantastic

745
00:29:12,820 --> 00:29:14,769
job with the emotive design frankly I

746
00:29:14,769 --> 00:29:16,570
mean it's like in its own body you know

747
00:29:16,570 --> 00:29:18,370
and it's so it's not on your phone and

748
00:29:18,370 --> 00:29:19,480
it has a voice and you're only allowed

749
00:29:19,480 --> 00:29:21,070
to call it certain things and it has

750
00:29:21,070 --> 00:29:23,470
like a communication style that makes it

751
00:29:23,470 --> 00:29:25,120
very compelling so there's all these

752
00:29:25,120 --> 00:29:26,110
things that they did that I thought were

753
00:29:26,110 --> 00:29:29,769
really masterful but we got one because

754
00:29:29,769 --> 00:29:31,480
we were curious what happened when you

755
00:29:31,480 --> 00:29:33,129
changed up where the voice was coming

756
00:29:33,129 --> 00:29:35,440
from so you know normally you talk to

757
00:29:35,440 --> 00:29:36,730
the echo and it's just like it's the

758
00:29:36,730 --> 00:29:38,950
voice of some like anonymous ghost that

759
00:29:38,950 --> 00:29:41,649
comes in you know like if you say Alexa

760
00:29:41,649 --> 00:29:42,820
then it's just like oh you know what's

761
00:29:42,820 --> 00:29:44,259
up like it's just suddenly out of the

762
00:29:44,259 --> 00:29:45,759
air and so you know that's not really

763
00:29:45,759 --> 00:29:47,649
what you want from I guess your digital

764
00:29:47,649 --> 00:29:51,129
servant basically so we got this so I

765
00:29:51,129 --> 00:29:52,570
mean so this is something that Richard

766
00:29:52,570 --> 00:29:55,570
built back there so we we got this old

767
00:29:55,570 --> 00:29:58,929
phone it's like you know like an old

768
00:29:58,929 --> 00:30:00,279
phone then you would see them answer the

769
00:30:00,279 --> 00:30:02,019
phone and like Mary Poppins you know

770
00:30:02,019 --> 00:30:05,669
like you have to pick it up you know so

771
00:30:05,669 --> 00:30:08,289
so you know this old phone and Richard

772
00:30:08,289 --> 00:30:10,179
put the killed all the mics on the the

773
00:30:10,179 --> 00:30:11,740
amazonica Hardware muted it to that

774
00:30:11,740 --> 00:30:13,899
telephone and now when you want to talk

775
00:30:13,899 --> 00:30:15,639
to Alexa you have to pick up the

776
00:30:15,639 --> 00:30:17,289
receiver you have to actually like talk

777
00:30:17,289 --> 00:30:19,200
into the phone and then put it back so

778
00:30:19,200 --> 00:30:21,639
there's a crazy interface experience it

779
00:30:21,639 --> 00:30:22,960
kind of started as like you know we

780
00:30:22,960 --> 00:30:24,309
thought this would be funny but then the

781
00:30:24,309 --> 00:30:26,169
way people interact with a phone is very

782
00:30:26,169 --> 00:30:27,309
different than the way they might

783
00:30:27,309 --> 00:30:29,139
interact with the Amazon echo they are

784
00:30:29,139 --> 00:30:32,230
more polite to the telephone because

785
00:30:32,230 --> 00:30:34,240
it's it's closer to something right that

786
00:30:34,240 --> 00:30:36,279
we were used to of close to a context or

787
00:30:36,279 --> 00:30:37,299
we're used to like talking to people

788
00:30:37,299 --> 00:30:39,639
it's not this like anonymous you know oh

789
00:30:39,639 --> 00:30:41,200
computer please turn the lights on and

790
00:30:41,200 --> 00:30:43,690
off it's like oh hey Alexa what's the

791
00:30:43,690 --> 00:30:45,460
weather gonna be like like people will

792
00:30:45,460 --> 00:30:48,340
wait to hear the full response before

793
00:30:48,340 --> 00:30:49,600
they hang up and then sometimes they're

794
00:30:49,600 --> 00:30:53,129
like thank you

795
00:30:53,340 --> 00:30:55,529
which is like crazy but yeah that's what

796
00:30:55,529 --> 00:30:57,809
so we have one and it's it's super

797
00:30:57,809 --> 00:30:59,820
busted because we blew up all the

798
00:30:59,820 --> 00:31:02,460
microphones except for that one and it's

799
00:31:02,460 --> 00:31:04,019
in a phone yeah

800
00:31:04,019 --> 00:31:06,809
other questions not so much of a

801
00:31:06,809 --> 00:31:08,340
question but you you kind of made me

802
00:31:08,340 --> 00:31:10,799
think of something on this voice

803
00:31:10,799 --> 00:31:14,999
assistant tip I I realized so I I'm a

804
00:31:14,999 --> 00:31:16,919
tech enthusiast I have a couple google

805
00:31:16,919 --> 00:31:18,840
homes at home and one of the things that

806
00:31:18,840 --> 00:31:20,220
I always made me mad was like why can't

807
00:31:20,220 --> 00:31:22,350
I change the thing that engages this

808
00:31:22,350 --> 00:31:25,440
thing and I realized I don't maybe this

809
00:31:25,440 --> 00:31:28,590
is I'm a designer so I never thought

810
00:31:28,590 --> 00:31:31,379
about the intent of why you addressed a

811
00:31:31,379 --> 00:31:34,409
Google home system as OK Google but

812
00:31:34,409 --> 00:31:36,210
hearing you say that and thinking about

813
00:31:36,210 --> 00:31:38,820
making robots work for you I do I've

814
00:31:38,820 --> 00:31:40,649
done a couple projects with Google I you

815
00:31:40,649 --> 00:31:42,480
know I'm a fan of their products and I

816
00:31:42,480 --> 00:31:44,399
realized that they do have a maker reset

817
00:31:44,399 --> 00:31:45,690
where you can they did a paper

818
00:31:45,690 --> 00:31:47,730
experiment with the designer that I

819
00:31:47,730 --> 00:31:48,690
follow in New York where you could

820
00:31:48,690 --> 00:31:50,340
basically program these robots to do

821
00:31:50,340 --> 00:31:54,679
things and they have a hole but I guess

822
00:31:54,679 --> 00:31:59,730
not not to assign good or bad to these

823
00:31:59,730 --> 00:32:01,950
big mega corporations but any thoughts

824
00:32:01,950 --> 00:32:04,919
on that and the intentionality of you

825
00:32:04,919 --> 00:32:06,929
know making somebody say ok Google is

826
00:32:06,929 --> 00:32:09,570
not that intermediary it's like I'd

827
00:32:09,570 --> 00:32:11,309
never thought about the like oh I'm

828
00:32:11,309 --> 00:32:13,049
talking to Google I'm not talking to

829
00:32:13,049 --> 00:32:16,769
some proxy a fake person or name that's

830
00:32:16,769 --> 00:32:24,600
interesting is it is it the brand or I

831
00:32:24,600 --> 00:32:26,789
mean so based on what you said this it's

832
00:32:26,789 --> 00:32:28,710
I didn't think about the relationship

833
00:32:28,710 --> 00:32:30,869
there or how I'm engaging with that

834
00:32:30,869 --> 00:32:33,389
brand and it's it's interesting yeah I

835
00:32:33,389 --> 00:32:36,029
mean I think that regardless of the

836
00:32:36,029 --> 00:32:38,309
buzzword that you call the object I do

837
00:32:38,309 --> 00:32:39,749
think that when you have to call it a

838
00:32:39,749 --> 00:32:41,039
person's name it makes a big difference

839
00:32:41,039 --> 00:32:43,440
it changes the framing because we're

840
00:32:43,440 --> 00:32:45,049
very influenced by what the framing is

841
00:32:45,049 --> 00:32:47,129
you know one of the things that I talk

842
00:32:47,129 --> 00:32:48,149
about when I talk about how to build

843
00:32:48,149 --> 00:32:49,619
these interfaces that a big part of it

844
00:32:49,619 --> 00:32:51,149
is the storytelling it's a big part of

845
00:32:51,149 --> 00:32:54,690
it is the context and so you know the

846
00:32:54,690 --> 00:32:56,220
difference between saying OK Google and

847
00:32:56,220 --> 00:32:58,289
saying Alexa is like significant you

848
00:32:58,289 --> 00:33:00,090
know in the way that we parse it

849
00:33:00,090 --> 00:33:04,320
instantly yeah it's it is it is really

850
00:33:04,320 --> 00:33:05,850
crazy I guess your question you had a

851
00:33:05,850 --> 00:33:06,480
question about

852
00:33:06,480 --> 00:33:11,580
like what the like the intentionality of

853
00:33:11,580 --> 00:33:16,230
large brands yeah so you know the thing

854
00:33:16,230 --> 00:33:18,299
is that I think that a lot of people are

855
00:33:18,299 --> 00:33:20,070
not trying to do this on purpose there's

856
00:33:20,070 --> 00:33:22,320
this like tricky thing where as humans

857
00:33:22,320 --> 00:33:24,270
we just naturally parse a lot of

858
00:33:24,270 --> 00:33:25,770
interfaces and a lot of interactive

859
00:33:25,770 --> 00:33:27,419
interfaces is alive like this is not

860
00:33:27,419 --> 00:33:28,980
something that you can really help like

861
00:33:28,980 --> 00:33:31,290
we're essentially animals and so you

862
00:33:31,290 --> 00:33:32,880
know we're on the lookout for like okay

863
00:33:32,880 --> 00:33:34,950
it moved on its own do I need to help it

864
00:33:34,950 --> 00:33:36,270
is it a part of my tribe or is it about

865
00:33:36,270 --> 00:33:37,710
to kill me and those are like basically

866
00:33:37,710 --> 00:33:40,590
the two categories you know and so

867
00:33:40,590 --> 00:33:42,450
whenever you have an automated interface

868
00:33:42,450 --> 00:33:43,950
when I have something that like moves or

869
00:33:43,950 --> 00:33:45,630
speaks like seemingly on its own and

870
00:33:45,630 --> 00:33:48,900
it's clearly not the wind you know it's

871
00:33:48,900 --> 00:33:51,540
it's you just get people starting to

872
00:33:51,540 --> 00:33:53,340
think of it as alive and you get fallout

873
00:33:53,340 --> 00:33:55,080
from that and you know it's it's not

874
00:33:55,080 --> 00:33:57,150
really a surprise to me that like you

875
00:33:57,150 --> 00:33:59,190
know the the CEO of iRobot is just like

876
00:33:59,190 --> 00:34:00,630
oh my god we didn't know this was gonna

877
00:34:00,630 --> 00:34:01,860
happen we tried to prevent this from

878
00:34:01,860 --> 00:34:03,570
happening and that Amazon is like we're

879
00:34:03,570 --> 00:34:05,250
so surprised by how much people love

880
00:34:05,250 --> 00:34:07,830
Alexa you have a lot of brands that are

881
00:34:07,830 --> 00:34:09,239
gonna in the future just be doing this

882
00:34:09,239 --> 00:34:10,859
entirely by accident and I think that

883
00:34:10,859 --> 00:34:11,790
one of the things that's going to become

884
00:34:11,790 --> 00:34:14,190
very important is for us to be really

885
00:34:14,190 --> 00:34:16,739
mindful of the humanity of ourselves and

886
00:34:16,739 --> 00:34:18,480
of others both like designers and as

887
00:34:18,480 --> 00:34:19,800
people who work in these businesses and

888
00:34:19,800 --> 00:34:35,219
consumers robots usually connect us to

889
00:34:35,219 --> 00:34:36,600
other people and you really try to tie

890
00:34:36,600 --> 00:34:39,330
it back to exactly as you're picturing

891
00:34:39,330 --> 00:34:42,629
here talking also about yourself and a

892
00:34:42,629 --> 00:34:44,310
lot of ways and how it interfaces with

893
00:34:44,310 --> 00:34:46,409
you is there a really big difference

894
00:34:46,409 --> 00:34:49,020
when kind of these robotic technologies

895
00:34:49,020 --> 00:34:50,730
are actually trying to make you look

896
00:34:50,730 --> 00:34:52,619
back at yourself kind of like the Eliza

897
00:34:52,619 --> 00:34:54,000
example I believe you brought the

898
00:34:54,000 --> 00:34:58,250
psychotherapy time yes so I think that I

899
00:34:58,250 --> 00:35:00,210
think it's there's a really big

900
00:35:00,210 --> 00:35:02,520
difference between when you as a as a

901
00:35:02,520 --> 00:35:04,710
user have an understanding that this is

902
00:35:04,710 --> 00:35:07,020
a robot and that these are all of like

903
00:35:07,020 --> 00:35:08,430
the places where the data goes and the

904
00:35:08,430 --> 00:35:10,050
data is stored and like this is you're

905
00:35:10,050 --> 00:35:11,340
the person in control in these specific

906
00:35:11,340 --> 00:35:13,109
ways you know I think that's really

907
00:35:13,109 --> 00:35:15,480
different from this is a robot and you

908
00:35:15,480 --> 00:35:17,400
should just play with it because it's a

909
00:35:17,400 --> 00:35:18,470
fun robot

910
00:35:18,470 --> 00:35:20,570
and by the way the side benefit to us is

911
00:35:20,570 --> 00:35:21,830
like all of these other things that are

912
00:35:21,830 --> 00:35:23,500
invisible to you and you have no control

913
00:35:23,500 --> 00:35:25,640
so I think that and there's a lot of

914
00:35:25,640 --> 00:35:26,810
interesting researchers who are now

915
00:35:26,810 --> 00:35:28,730
working on different interfaces that

916
00:35:28,730 --> 00:35:31,520
allow us to see that allow us to see

917
00:35:31,520 --> 00:35:34,220
ourselves that allow us to really probe

918
00:35:34,220 --> 00:35:35,740
and examine kind of what we're feeling

919
00:35:35,740 --> 00:35:38,150
through using technologies in augment

920
00:35:38,150 --> 00:35:39,410
and I think that that's really the key

921
00:35:39,410 --> 00:35:41,000
thing here using technology as an

922
00:35:41,000 --> 00:35:44,150
augment that what you want to do with

923
00:35:44,150 --> 00:35:45,680
robotics and when you want to do with a

924
00:35:45,680 --> 00:35:48,560
mode of robotics is not necessarily make

925
00:35:48,560 --> 00:35:50,450
something automatic particularly when it

926
00:35:50,450 --> 00:35:51,950
comes to emotions not to say like oh

927
00:35:51,950 --> 00:35:53,180
it's just gonna respond in this way

928
00:35:53,180 --> 00:35:54,920
every time but to say like this is going

929
00:35:54,920 --> 00:35:56,690
to help you do what you want to do it's

930
00:35:56,690 --> 00:35:58,040
going to help you understand what you

931
00:35:58,040 --> 00:36:00,170
want to understand it's going to augment

932
00:36:00,170 --> 00:36:02,000
the ability of a provider to do that for

933
00:36:02,000 --> 00:36:03,230
you it's going to augment your ability

934
00:36:03,230 --> 00:36:06,010
to do that for yourself

935
00:36:08,480 --> 00:36:13,210
[Applause]

