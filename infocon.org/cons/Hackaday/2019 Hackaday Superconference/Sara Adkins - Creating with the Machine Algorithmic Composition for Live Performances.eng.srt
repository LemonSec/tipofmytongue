1
00:00:00,030 --> 00:00:01,740
please welcome to the super consta

2
00:00:01,740 --> 00:00:12,929
Adkins okay uh hi everyone thank you my

3
00:00:12,929 --> 00:00:14,940
name is Sarah Adkins and today I'm going

4
00:00:14,940 --> 00:00:16,949
to be talking about my project creating

5
00:00:16,949 --> 00:00:19,470
with the machine which is a set of

6
00:00:19,470 --> 00:00:21,660
compositions I wrote for human and

7
00:00:21,660 --> 00:00:24,980
machine musicians so first off

8
00:00:24,980 --> 00:00:27,180
algorithmic composition what does that

9
00:00:27,180 --> 00:00:29,670
mean by that I basically just mean an

10
00:00:29,670 --> 00:00:31,740
algorithm that takes in some input and

11
00:00:31,740 --> 00:00:34,890
output some musical output by

12
00:00:34,890 --> 00:00:37,350
Interactive algorithmic composition I

13
00:00:37,350 --> 00:00:40,680
mean that the input is a performance

14
00:00:40,680 --> 00:00:43,680
from a live musician and the algorithm

15
00:00:43,680 --> 00:00:47,940
is able to output in real time so the

16
00:00:47,940 --> 00:00:49,590
basic interaction model for this system

17
00:00:49,590 --> 00:00:51,600
is we want a closed loop between the

18
00:00:51,600 --> 00:00:55,079
human performer and the algorithm so the

19
00:00:55,079 --> 00:00:57,660
idea is we have a musician that's

20
00:00:57,660 --> 00:00:59,609
improvising they're kind of making up

21
00:00:59,609 --> 00:01:02,430
audio on the spot and then we have an

22
00:01:02,430 --> 00:01:04,049
algorithm that's interpreting what

23
00:01:04,049 --> 00:01:06,119
they're playing and coming up with some

24
00:01:06,119 --> 00:01:08,130
sort of way to play music back to them

25
00:01:08,130 --> 00:01:09,750
so they're kind of both playing off each

26
00:01:09,750 --> 00:01:10,680
other

27
00:01:10,680 --> 00:01:13,500
it's common for human musicians to

28
00:01:13,500 --> 00:01:16,439
improvise with each other so the idea

29
00:01:16,439 --> 00:01:18,180
here is to replace one of the humans

30
00:01:18,180 --> 00:01:20,580
with an algorithm but the big question

31
00:01:20,580 --> 00:01:22,740
is how do we get these two things to

32
00:01:22,740 --> 00:01:25,470
communicate effectively because clearly

33
00:01:25,470 --> 00:01:28,200
a musician and an algorithm are going to

34
00:01:28,200 --> 00:01:31,950
speak really different languages and

35
00:01:31,950 --> 00:01:34,470
also why do we even want to do this I

36
00:01:34,470 --> 00:01:36,509
mean what's the point of introducing

37
00:01:36,509 --> 00:01:38,220
algorithms into music I mean that's

38
00:01:38,220 --> 00:01:40,680
supposed to be a creative thing that

39
00:01:40,680 --> 00:01:43,079
humans are good at and computers we

40
00:01:43,079 --> 00:01:45,630
don't want to involve them obviously

41
00:01:45,630 --> 00:01:47,549
humans are better musicians than

42
00:01:47,549 --> 00:01:50,130
computers computers are more accurate

43
00:01:50,130 --> 00:01:52,470
they're faster they're better better at

44
00:01:52,470 --> 00:01:55,049
multitasking they can keep track of more

45
00:01:55,049 --> 00:01:57,659
information so maybe we can kind of

46
00:01:57,659 --> 00:01:59,189
combine the strengths of both these

47
00:01:59,189 --> 00:02:01,530
things and create compelling new music

48
00:02:01,530 --> 00:02:04,020
we want to explore the idea of making

49
00:02:04,020 --> 00:02:06,899
random nishan and automation useful to a

50
00:02:06,899 --> 00:02:09,060
musicians creative process and hopefully

51
00:02:09,060 --> 00:02:11,030
help them improvise in a way

52
00:02:11,030 --> 00:02:15,440
that they're not used to but how do we

53
00:02:15,440 --> 00:02:18,200
do that so I'm first gonna talk about

54
00:02:18,200 --> 00:02:20,900
how we can get data from a musician's

55
00:02:20,900 --> 00:02:24,170
instrument into the algorithm so I'll

56
00:02:24,170 --> 00:02:26,870
start with the simplest one just a

57
00:02:26,870 --> 00:02:29,810
waveform so if you Mike a musician this

58
00:02:29,810 --> 00:02:33,350
is common at almost every concert you

59
00:02:33,350 --> 00:02:35,959
have an accurate sample stream of what

60
00:02:35,959 --> 00:02:38,270
they're playing the issue with that is

61
00:02:38,270 --> 00:02:40,940
you can't really get many musical

62
00:02:40,940 --> 00:02:42,709
features just from looking at that audio

63
00:02:42,709 --> 00:02:46,850
wave but if we take an FFT and we

64
00:02:46,850 --> 00:02:48,709
convert that audio wave into the

65
00:02:48,709 --> 00:02:51,260
frequency domain suddenly we can see a

66
00:02:51,260 --> 00:02:54,530
lot more musical information so in this

67
00:02:54,530 --> 00:02:56,150
spectrogram I have over on the right you

68
00:02:56,150 --> 00:02:59,750
can see the x-axis is time and the

69
00:02:59,750 --> 00:03:02,060
y-axis is frequency so we can see what

70
00:03:02,060 --> 00:03:05,120
frequencies are prominent in the

71
00:03:05,120 --> 00:03:07,550
performance at each time slice so from

72
00:03:07,550 --> 00:03:09,739
here we can do pitch detection detecting

73
00:03:09,739 --> 00:03:12,800
what pitch the musician is playing but

74
00:03:12,800 --> 00:03:14,480
more than that we can determine more

75
00:03:14,480 --> 00:03:18,230
things about the tone of their sound so

76
00:03:18,230 --> 00:03:19,760
different instruments will sound

77
00:03:19,760 --> 00:03:21,200
different even if they're playing the

78
00:03:21,200 --> 00:03:22,880
same note and that's because there's

79
00:03:22,880 --> 00:03:24,799
something called Tambor basically

80
00:03:24,799 --> 00:03:26,480
they're playing the same base frequency

81
00:03:26,480 --> 00:03:28,690
but there are a lot of other harmonic

82
00:03:28,690 --> 00:03:30,829
frequencies that are playing above that

83
00:03:30,829 --> 00:03:33,170
that kind of create the voice of each

84
00:03:33,170 --> 00:03:34,940
individual instrument so from a

85
00:03:34,940 --> 00:03:38,650
spectrogram we can get that information

86
00:03:39,019 --> 00:03:41,299
another option for representing musical

87
00:03:41,299 --> 00:03:44,540
data is called MIDI this is just a

88
00:03:44,540 --> 00:03:47,120
commuter communication protocol it

89
00:03:47,120 --> 00:03:50,000
represents exact notes when a musician

90
00:03:50,000 --> 00:03:53,000
starts playing a note when they stop and

91
00:03:53,000 --> 00:03:55,100
how loud it is you need special

92
00:03:55,100 --> 00:03:57,650
instruments to be able to use MIDI but

93
00:03:57,650 --> 00:03:59,299
it is pretty common in musical

94
00:03:59,299 --> 00:04:04,609
performances um and now let's talk about

95
00:04:04,609 --> 00:04:06,950
if maybe we can get some weirder

96
00:04:06,950 --> 00:04:09,320
information from a musician's

97
00:04:09,320 --> 00:04:11,450
performance than just the audio they're

98
00:04:11,450 --> 00:04:14,420
playing musicians often use a lot of

99
00:04:14,420 --> 00:04:15,859
body movements when they're playing

100
00:04:15,859 --> 00:04:18,459
because they really get into their songs

101
00:04:18,459 --> 00:04:20,048
maybe we could get some information

102
00:04:20,048 --> 00:04:22,810
about their performance from that we

103
00:04:22,810 --> 00:04:24,850
could also think about how can we get

104
00:04:24,850 --> 00:04:27,280
information from their breath are they

105
00:04:27,280 --> 00:04:29,020
holding their breath at certain moments

106
00:04:29,020 --> 00:04:30,460
are they breathing more intensely or

107
00:04:30,460 --> 00:04:34,419
faster at others and also we want to

108
00:04:34,419 --> 00:04:36,490
consider gestures maybe what we want to

109
00:04:36,490 --> 00:04:39,520
give the musician control of various

110
00:04:39,520 --> 00:04:41,770
parts of the algorithm but we don't want

111
00:04:41,770 --> 00:04:43,900
to distract them too much from actually

112
00:04:43,900 --> 00:04:46,780
playing their instrument so using things

113
00:04:46,780 --> 00:04:49,600
like accelerometers or IR sensors we can

114
00:04:49,600 --> 00:04:52,389
get information like phrasing and subtle

115
00:04:52,389 --> 00:04:54,729
control that might not be so apparent

116
00:04:54,729 --> 00:04:58,620
from just the raw audio wave some

117
00:04:58,620 --> 00:05:00,760
technologies that I like to use for this

118
00:05:00,760 --> 00:05:02,949
are a Kinect which has really good limb

119
00:05:02,949 --> 00:05:05,139
tracking or elite motion which is really

120
00:05:05,139 --> 00:05:08,430
good at tracking someone's fingers

121
00:05:08,430 --> 00:05:12,070
alright so we have some data from our

122
00:05:12,070 --> 00:05:15,490
musician how do we use that to actually

123
00:05:15,490 --> 00:05:18,539
generate new music to play back to them

124
00:05:18,539 --> 00:05:21,400
so the main point is that we don't want

125
00:05:21,400 --> 00:05:24,490
our musician to have to worry about how

126
00:05:24,490 --> 00:05:26,199
they're interacting with the algorithm

127
00:05:26,199 --> 00:05:28,090
it should be intuitive for them they

128
00:05:28,090 --> 00:05:30,340
should be worrying about making music

129
00:05:30,340 --> 00:05:32,830
not worrying about do did I just play

130
00:05:32,830 --> 00:05:34,060
something that's gonna break the

131
00:05:34,060 --> 00:05:36,460
algorithm so that's the main concern

132
00:05:36,460 --> 00:05:38,349
here we don't want them to get fed up

133
00:05:38,349 --> 00:05:40,330
with the technology we want them to want

134
00:05:40,330 --> 00:05:43,360
to use it so there are a couple of

135
00:05:43,360 --> 00:05:45,250
different options here the simplest is

136
00:05:45,250 --> 00:05:47,949
just to use rule-based algorithms you

137
00:05:47,949 --> 00:05:49,750
could say something like when the

138
00:05:49,750 --> 00:05:52,479
musician does X the algorithm should do

139
00:05:52,479 --> 00:05:56,440
Y or we want to map parameter a from the

140
00:05:56,440 --> 00:05:59,020
musicians performance to parameter B of

141
00:05:59,020 --> 00:06:02,770
the algorithm another option is to use

142
00:06:02,770 --> 00:06:05,440
Markov chains basically what that is is

143
00:06:05,440 --> 00:06:07,240
it's a way to represent a bunch of

144
00:06:07,240 --> 00:06:09,580
different states and probabilities for

145
00:06:09,580 --> 00:06:12,789
transitioning between those states so

146
00:06:12,789 --> 00:06:14,979
the example you see here is for a chord

147
00:06:14,979 --> 00:06:19,030
progression each chord is a state and

148
00:06:19,030 --> 00:06:21,130
the arrows between them represent the

149
00:06:21,130 --> 00:06:23,530
probability of moving from one chord to

150
00:06:23,530 --> 00:06:25,360
each other

151
00:06:25,360 --> 00:06:29,210
so for rule-based and Markov chains

152
00:06:29,210 --> 00:06:30,800
I think something really important is

153
00:06:30,800 --> 00:06:32,770
that music theory is your friend here

154
00:06:32,770 --> 00:06:35,930
music theory is just the rules of how

155
00:06:35,930 --> 00:06:38,270
music works they're made to be broken

156
00:06:38,270 --> 00:06:41,060
but it's a good starting point for

157
00:06:41,060 --> 00:06:44,720
writing an algorithm if you don't care

158
00:06:44,720 --> 00:06:46,730
about music theory and you don't want to

159
00:06:46,730 --> 00:06:48,650
learn that luckily we have neural

160
00:06:48,650 --> 00:06:50,420
networks now so you don't actually need

161
00:06:50,420 --> 00:06:52,880
to learn anything yourself but that's

162
00:06:52,880 --> 00:06:56,270
not true but yeah you can also if you

163
00:06:56,270 --> 00:06:58,490
have a bunch of example MIDI inputs our

164
00:06:58,490 --> 00:07:00,380
example audio waves you can train a

165
00:07:00,380 --> 00:07:03,620
neural network to output new music in a

166
00:07:03,620 --> 00:07:06,770
similar style but for all of these the

167
00:07:06,770 --> 00:07:08,510
most important thing is just to make

168
00:07:08,510 --> 00:07:11,360
sure you're using your ears you can have

169
00:07:11,360 --> 00:07:13,340
a really good idea for an algorithm but

170
00:07:13,340 --> 00:07:15,050
if you implement it and it doesn't sound

171
00:07:15,050 --> 00:07:16,550
good

172
00:07:16,550 --> 00:07:18,080
then it's not really useful as an

173
00:07:18,080 --> 00:07:20,510
algorithmic composition so as you're

174
00:07:20,510 --> 00:07:21,980
designing something it's important to

175
00:07:21,980 --> 00:07:23,780
iterate by actually listening to make

176
00:07:23,780 --> 00:07:25,490
sure that you're coming up as something

177
00:07:25,490 --> 00:07:27,470
that's pleasing to you maybe other

178
00:07:27,470 --> 00:07:29,240
people won't like it but it's only

179
00:07:29,240 --> 00:07:33,040
important that you like what you make so

180
00:07:33,040 --> 00:07:36,770
okay now I'm gonna get into a little bit

181
00:07:36,770 --> 00:07:38,630
about some compositions that I've

182
00:07:38,630 --> 00:07:40,940
written I'll go into the technical

183
00:07:40,940 --> 00:07:42,860
details of how any of them and I'll also

184
00:07:42,860 --> 00:07:45,020
have a couple of audio recording

185
00:07:45,020 --> 00:07:50,510
examples so the first piece I'm gonna

186
00:07:50,510 --> 00:07:53,300
talk about is called breathe this is an

187
00:07:53,300 --> 00:07:56,450
example of a rule based algorithm and

188
00:07:56,450 --> 00:07:58,850
this is a composition that's for solo

189
00:07:58,850 --> 00:08:02,150
electric guitar so there's this big

190
00:08:02,150 --> 00:08:04,610
flowchart here and let's break it down a

191
00:08:04,610 --> 00:08:07,610
little bit so we have our guitar input

192
00:08:07,610 --> 00:08:10,430
the code tourist is starts off this

193
00:08:10,430 --> 00:08:13,550
piece by improvising any key they want

194
00:08:13,550 --> 00:08:16,670
doesn't matter their input then gets

195
00:08:16,670 --> 00:08:19,820
sent to a granular synthesizer if you're

196
00:08:19,820 --> 00:08:21,590
not familiar with what that is it's

197
00:08:21,590 --> 00:08:23,840
basically you take an audio wave and you

198
00:08:23,840 --> 00:08:25,700
chop it up into a bunch of tiny parts

199
00:08:25,700 --> 00:08:27,650
could be as small as like 10

200
00:08:27,650 --> 00:08:30,590
milliseconds up to a second and then you

201
00:08:30,590 --> 00:08:32,690
play them back slightly scrambled and

202
00:08:32,690 --> 00:08:34,690
overlapping to create an interest

203
00:08:34,690 --> 00:08:37,090
effect so that's one thing that's going

204
00:08:37,090 --> 00:08:39,940
on in the algorithm the other is the

205
00:08:39,940 --> 00:08:43,870
guitarist is able to control an FFT

206
00:08:43,870 --> 00:08:46,210
using a foot pedal so they play a note

207
00:08:46,210 --> 00:08:50,290
they press their foot pedal and we get

208
00:08:50,290 --> 00:08:52,390
an FFT snapshot of what they're playing

209
00:08:52,390 --> 00:08:55,120
so now we can figure out what pitch did

210
00:08:55,120 --> 00:08:57,160
they just play and more than just the

211
00:08:57,160 --> 00:08:59,100
base frequency we can also figure out

212
00:08:59,100 --> 00:09:01,750
what harmonics they were playing that

213
00:09:01,750 --> 00:09:05,320
made up that tone once we have that we

214
00:09:05,320 --> 00:09:07,510
can then play each harmonic back as a

215
00:09:07,510 --> 00:09:09,610
sine wave drone and this kind of creates

216
00:09:09,610 --> 00:09:12,400
like a background for the piece and the

217
00:09:12,400 --> 00:09:14,560
guitarist is able to layer a couple of

218
00:09:14,560 --> 00:09:17,020
these and the oldest gets deleted as it

219
00:09:17,020 --> 00:09:20,200
makes a new one so that's the guitar

220
00:09:20,200 --> 00:09:24,520
input and then we also have a microphone

221
00:09:24,520 --> 00:09:26,200
that's taped to the performer under

222
00:09:26,200 --> 00:09:27,790
their nose and we're using this to

223
00:09:27,790 --> 00:09:31,240
capture their breath signal so basically

224
00:09:31,240 --> 00:09:33,520
as they breathe out or they breathe in

225
00:09:33,520 --> 00:09:35,800
the microphone will get hot and the

226
00:09:35,800 --> 00:09:37,420
amplitude will increase as they're

227
00:09:37,420 --> 00:09:39,610
holding their breath it'll decrease so

228
00:09:39,610 --> 00:09:42,130
we kind of have this amplitude and both

229
00:09:42,130 --> 00:09:45,670
of their breaths that we're creating and

230
00:09:45,670 --> 00:09:47,440
this is what we're using to map to

231
00:09:47,440 --> 00:09:50,890
parameter changes in the algorithm so as

232
00:09:50,890 --> 00:09:53,350
the amplitude of the breath signal

233
00:09:53,350 --> 00:09:56,260
increases it first affects the granular

234
00:09:56,260 --> 00:10:00,790
synthesis the greens get faster and more

235
00:10:00,790 --> 00:10:04,630
overlapping so it starts to sound a

236
00:10:04,630 --> 00:10:06,660
little bit more scattered and chaotic

237
00:10:06,660 --> 00:10:09,070
and then we're also going to start

238
00:10:09,070 --> 00:10:11,860
changing the volumes of the different

239
00:10:11,860 --> 00:10:15,520
drones so as I said before what makes up

240
00:10:15,520 --> 00:10:17,890
the tone quality of an instrument is the

241
00:10:17,890 --> 00:10:20,620
ratio in which you play the harmonics so

242
00:10:20,620 --> 00:10:22,300
if you vary the amplitude of those you

243
00:10:22,300 --> 00:10:26,230
start to create different tones so as

244
00:10:26,230 --> 00:10:28,600
the amplitude of the person's breath

245
00:10:28,600 --> 00:10:31,000
increases these drones start to move

246
00:10:31,000 --> 00:10:34,540
around faster cool that's the technical

247
00:10:34,540 --> 00:10:37,480
part I also have a couple short audio

248
00:10:37,480 --> 00:10:40,660
recordings of parts of the piece this is

249
00:10:40,660 --> 00:10:42,820
a picture of me performing it

250
00:10:42,820 --> 00:10:44,920
on the right with the guitar and then on

251
00:10:44,920 --> 00:10:47,500
the left that's my professor wearing a

252
00:10:47,500 --> 00:10:53,290
wizard robe so in this first excerpt I

253
00:10:53,290 --> 00:10:55,390
think you should the best thing to pay

254
00:10:55,390 --> 00:10:59,160
attention to is listening to the drones

255
00:11:00,210 --> 00:11:05,690
[Music]

256
00:11:08,850 --> 00:11:18,440
[Music]

257
00:11:20,830 --> 00:11:26,129
[Music]

258
00:11:26,129 --> 00:11:27,870
and then in the second part you're going

259
00:11:27,870 --> 00:11:29,939
to hear more of the granular synthesis

260
00:11:29,939 --> 00:11:32,449
in the piece

261
00:11:33,710 --> 00:11:52,309
[Music]

262
00:11:59,800 --> 00:12:08,099
[Music]

263
00:12:10,160 --> 00:12:14,850
alright so next piece this next piece is

264
00:12:14,850 --> 00:12:17,310
called recurrent neural networks on Bach

265
00:12:17,310 --> 00:12:19,530
and as you might guess from the title

266
00:12:19,530 --> 00:12:21,780
this is using neural networks in the

267
00:12:21,780 --> 00:12:26,280
algorithm so like 350 years ago there's

268
00:12:26,280 --> 00:12:27,180
this dude

269
00:12:27,180 --> 00:12:29,790
his name was Bach and he wrote a lot of

270
00:12:29,790 --> 00:12:34,890
music a lot of it was good yeah alright

271
00:12:34,890 --> 00:12:37,740
so basically what I did here is I trains

272
00:12:37,740 --> 00:12:43,890
a 3 a 3 layer LS TM on 405 Bach Corral's

273
00:12:43,890 --> 00:12:46,650
so what is an LS TM so it's basically a

274
00:12:46,650 --> 00:12:49,200
neural network where the output is fed

275
00:12:49,200 --> 00:12:51,660
back to the input so it's able to

276
00:12:51,660 --> 00:12:53,400
remember information from previous

277
00:12:53,400 --> 00:12:56,070
States long term this is especially

278
00:12:56,070 --> 00:12:58,320
useful in music because this you can

279
00:12:58,320 --> 00:13:00,540
imagine what a musician plays next is

280
00:13:00,540 --> 00:13:02,400
very dependent on what happened

281
00:13:02,400 --> 00:13:04,620
previously so if we're going to try to

282
00:13:04,620 --> 00:13:07,020
learn how to do that with it with a

283
00:13:07,020 --> 00:13:08,820
neural network we want to make sure that

284
00:13:08,820 --> 00:13:10,470
we're feeding back the output and that

285
00:13:10,470 --> 00:13:13,290
we're being recurrent and the output of

286
00:13:13,290 --> 00:13:15,570
the neural network is a floor voice

287
00:13:15,570 --> 00:13:19,170
Corral in the style of Bach a chorale is

288
00:13:19,170 --> 00:13:21,750
kind of like a church hymn it's a short

289
00:13:21,750 --> 00:13:24,570
song piece with a soprano and Alto a

290
00:13:24,570 --> 00:13:27,630
tenor and a bass it's used usually

291
00:13:27,630 --> 00:13:29,880
rhythmically pretty simple but the

292
00:13:29,880 --> 00:13:32,610
harmonies are complex and a lot of the

293
00:13:32,610 --> 00:13:34,590
music that Bach wrote is kind of the

294
00:13:34,590 --> 00:13:38,420
basis of tonal harmony and music theory

295
00:13:38,420 --> 00:13:41,310
so I trained up this network using

296
00:13:41,310 --> 00:13:41,940
tensorflow

297
00:13:41,940 --> 00:13:44,310
I took about three days but the

298
00:13:44,310 --> 00:13:46,350
interesting thing here that I did was I

299
00:13:46,350 --> 00:13:49,350
would save a snapshot of all the model

300
00:13:49,350 --> 00:13:51,810
weights every half hour during training

301
00:13:51,810 --> 00:13:55,590
so basically I had this whole folder of

302
00:13:55,590 --> 00:14:00,180
how the algorithm was performing as it

303
00:14:00,180 --> 00:14:02,940
learned so the first snapshot it knows

304
00:14:02,940 --> 00:14:06,260
nothing and it just outputs random notes

305
00:14:06,260 --> 00:14:09,570
if you look at a checkpoint like a day

306
00:14:09,570 --> 00:14:11,820
into training it's kind of starting to

307
00:14:11,820 --> 00:14:13,290
get the idea and if you look at a

308
00:14:13,290 --> 00:14:16,110
checkpoint at the end of training it's

309
00:14:16,110 --> 00:14:16,880
actually

310
00:14:16,880 --> 00:14:22,100
generating some pretty cool harmonies so

311
00:14:22,100 --> 00:14:24,710
that's how I generated kind of the

312
00:14:24,710 --> 00:14:26,450
content for this piece but the

313
00:14:26,450 --> 00:14:30,440
performance is kind of another loop so

314
00:14:30,440 --> 00:14:34,280
basically ah okay Mike basically the

315
00:14:34,280 --> 00:14:37,040
performer has access to a MIDI

316
00:14:37,040 --> 00:14:40,880
controller that's able to determine how

317
00:14:40,880 --> 00:14:43,460
fast the algorithm is going to move

318
00:14:43,460 --> 00:14:45,560
through the checkpoints so we always

319
00:14:45,560 --> 00:14:48,730
start by playing music that's generated

320
00:14:48,730 --> 00:14:51,080
randomly before the network has learned

321
00:14:51,080 --> 00:14:53,240
anything and we keep moving until we get

322
00:14:53,240 --> 00:14:55,220
to a point where the algorithm has

323
00:14:55,220 --> 00:14:57,740
learned to generate tonal harmony so

324
00:14:57,740 --> 00:15:00,110
what's interesting here is the performer

325
00:15:00,110 --> 00:15:02,420
actually doesn't have any control over

326
00:15:02,420 --> 00:15:05,210
the exact notes that are played they're

327
00:15:05,210 --> 00:15:07,940
able to control what checkpoint we

328
00:15:07,940 --> 00:15:11,030
generate notes from but they're not able

329
00:15:11,030 --> 00:15:13,520
to control what those notes are they can

330
00:15:13,520 --> 00:15:15,800
pause on a note if they like it or they

331
00:15:15,800 --> 00:15:18,380
can speed past a section if they want to

332
00:15:18,380 --> 00:15:22,490
get past it but that's about as much

333
00:15:22,490 --> 00:15:24,830
control as they get so the tensor flow

334
00:15:24,830 --> 00:15:27,830
model will output a phrase the performer

335
00:15:27,830 --> 00:15:30,530
will hear it back and can choose how

336
00:15:30,530 --> 00:15:32,300
fast to move through it they also have

337
00:15:32,300 --> 00:15:35,360
some other controls for shaping effects

338
00:15:35,360 --> 00:15:40,100
of the piece and I'll play a couple

339
00:15:40,100 --> 00:15:42,920
examples from this at various stages of

340
00:15:42,920 --> 00:15:45,680
the model so the first this is when the

341
00:15:45,680 --> 00:15:49,040
neural network has just started training

342
00:15:49,040 --> 00:15:51,710
so it's it's basically outputting random

343
00:15:51,710 --> 00:15:54,670
notes at this point

344
00:15:57,410 --> 00:16:02,329
or maybe the audio won't work oh there

345
00:16:02,329 --> 00:16:03,890
we go

346
00:16:03,890 --> 00:16:07,299
[Music]

347
00:16:15,990 --> 00:16:18,640
yeah so it sounds kind of creepy and

348
00:16:18,640 --> 00:16:21,130
eerie but it's just Brando notes the

349
00:16:21,130 --> 00:16:23,860
network knows nothing at this point now

350
00:16:23,860 --> 00:16:27,070
let's look at a section from the middle

351
00:16:27,070 --> 00:16:28,900
of training hopefully it's starting to

352
00:16:28,900 --> 00:16:30,880
get a better idea of harmony at this

353
00:16:30,880 --> 00:16:32,500
point

354
00:16:32,500 --> 00:16:46,229
[Music]

355
00:16:53,809 --> 00:16:56,100
so at this point we're starting here

356
00:16:56,100 --> 00:16:58,230
that it's playing actual chords that

357
00:16:58,230 --> 00:17:00,959
don't sound awful but there isn't really

358
00:17:00,959 --> 00:17:03,629
much flow between the chords it doesn't

359
00:17:03,629 --> 00:17:05,459
really feel like it played an actual

360
00:17:05,459 --> 00:17:08,130
phrase so finally let's listen to an

361
00:17:08,130 --> 00:17:10,079
example from the end of the piece where

362
00:17:10,079 --> 00:17:13,668
the network has been fully trained

363
00:17:14,900 --> 00:17:18,259
[Music]

364
00:17:23,940 --> 00:17:27,180
[Music]

365
00:17:28,569 --> 00:17:31,610
yes that's more what we want like there

366
00:17:31,610 --> 00:17:34,880
is a cadence it resolved it might not be

367
00:17:34,880 --> 00:17:38,600
as good as Bach but it's it's getting

368
00:17:38,600 --> 00:17:44,480
there cool so the last piece I'm going

369
00:17:44,480 --> 00:17:47,870
to talk about is called machine cycle so

370
00:17:47,870 --> 00:17:51,470
this is an ensemble piece but it kind of

371
00:17:51,470 --> 00:17:54,230
has a soloist in it that is playing a

372
00:17:54,230 --> 00:17:56,899
MIDI keyboard the reason for this is I

373
00:17:56,899 --> 00:17:58,490
didn't want to deal with having to do

374
00:17:58,490 --> 00:18:00,950
pitch detection MIDI is an easy way

375
00:18:00,950 --> 00:18:04,250
around that so how this piece works is

376
00:18:04,250 --> 00:18:06,620
the MIDI keyboard player will be playing

377
00:18:06,620 --> 00:18:09,500
phrases and improvising and as they play

378
00:18:09,500 --> 00:18:11,929
the algorithm is gonna randomly grab

379
00:18:11,929 --> 00:18:14,809
phrases from them and then from that

380
00:18:14,809 --> 00:18:16,250
phrase they're gonna create a Markov

381
00:18:16,250 --> 00:18:17,870
chain so it's going to add some

382
00:18:17,870 --> 00:18:20,330
potential embellishments to their phrase

383
00:18:20,330 --> 00:18:22,759
that'll be maybe notes that could be

384
00:18:22,759 --> 00:18:24,879
between jumps maybe some rhythmic

385
00:18:24,879 --> 00:18:28,610
alterations when the phrase starch does

386
00:18:28,610 --> 00:18:30,470
it start at an upbeat or a downbeat

387
00:18:30,470 --> 00:18:34,340
maybe adding in more notes and then

388
00:18:34,340 --> 00:18:37,220
it'll save that in a bank and as the

389
00:18:37,220 --> 00:18:40,009
musician continues to play more change

390
00:18:40,009 --> 00:18:41,570
will be added to the bank and will

391
00:18:41,570 --> 00:18:42,950
delete the oldest so we're always

392
00:18:42,950 --> 00:18:44,899
keeping track of phrases that the

393
00:18:44,899 --> 00:18:48,350
musician has played most recently and

394
00:18:48,350 --> 00:18:50,629
then during the piece the algorithm will

395
00:18:50,629 --> 00:18:52,909
output phrases that are generated from a

396
00:18:52,909 --> 00:18:53,840
Markov chain

397
00:18:53,840 --> 00:18:56,529
so the musician will be hearing back

398
00:18:56,529 --> 00:18:59,059
phrases similar to what they played but

399
00:18:59,059 --> 00:19:01,129
with some embellishments so there's kind

400
00:19:01,129 --> 00:19:02,600
of the closed loop there they're playing

401
00:19:02,600 --> 00:19:04,460
off what the algorithm has played back

402
00:19:04,460 --> 00:19:07,789
to them also in this piece there is a

403
00:19:07,789 --> 00:19:09,710
conductor who is leading the other

404
00:19:09,710 --> 00:19:12,379
unsampled members and controlling some

405
00:19:12,379 --> 00:19:14,230
different parameters of the algorithm

406
00:19:14,230 --> 00:19:18,710
like how often phrases are altered how

407
00:19:18,710 --> 00:19:20,840
often do we skip notes how often do we

408
00:19:20,840 --> 00:19:24,259
play it half or double tempo basically

409
00:19:24,259 --> 00:19:26,029
controlling how different the phrase

410
00:19:26,029 --> 00:19:28,730
that's played back is from the original

411
00:19:28,730 --> 00:19:33,830
phrase and on the left here you'll see a

412
00:19:33,830 --> 00:19:35,659
picture of the score that I wrote for

413
00:19:35,659 --> 00:19:37,809
this piece

414
00:19:37,809 --> 00:19:40,340
unlike a traditional score it's not

415
00:19:40,340 --> 00:19:43,039
dictating like exact notes that all of

416
00:19:43,039 --> 00:19:45,500
the performers have to play basically

417
00:19:45,500 --> 00:19:47,480
the top line is for the keyboardist and

418
00:19:47,480 --> 00:19:49,159
it's kind of telling them when to

419
00:19:49,159 --> 00:19:51,409
improvise when to pause when they should

420
00:19:51,409 --> 00:19:55,340
build dynamics the control is for the

421
00:19:55,340 --> 00:19:57,590
conductor what they should change about

422
00:19:57,590 --> 00:20:00,409
the algorithm there's also a guitarist

423
00:20:00,409 --> 00:20:01,940
that's kind of acting as a counter

424
00:20:01,940 --> 00:20:03,710
melody and the drones that are just

425
00:20:03,710 --> 00:20:06,830
filling out the background so it's just

426
00:20:06,830 --> 00:20:08,480
to give the musicians a structural

427
00:20:08,480 --> 00:20:11,240
outline of the piece and they're free to

428
00:20:11,240 --> 00:20:14,299
improvise the specific notes and the

429
00:20:14,299 --> 00:20:16,340
conductor is responsible for leading the

430
00:20:16,340 --> 00:20:17,900
musicians through the piece and

431
00:20:17,900 --> 00:20:21,440
controlling the parameters and so now we

432
00:20:21,440 --> 00:20:23,000
can listen to an excerpt from a machine

433
00:20:23,000 --> 00:20:25,330
cycle

434
00:20:28,550 --> 00:21:33,740
[Music]

435
00:21:36,210 --> 00:21:40,179
all right so those are the pieces and I

436
00:21:40,179 --> 00:21:42,160
obviously did not write all this from

437
00:21:42,160 --> 00:21:43,900
scratch so I just wanted to take a

438
00:21:43,900 --> 00:21:46,420
minute to talk about the tech that I use

439
00:21:46,420 --> 00:21:49,450
to build these so for recurrent neural

440
00:21:49,450 --> 00:21:52,090
networks on Bach I use tensor flow which

441
00:21:52,090 --> 00:21:54,160
is a popular machine learning framework

442
00:21:54,160 --> 00:21:57,760
for building deep neural networks Google

443
00:21:57,760 --> 00:22:00,220
in particular has a cool open-source

444
00:22:00,220 --> 00:22:03,100
project called magenta that is dedicated

445
00:22:03,100 --> 00:22:06,429
to creating art and music using machine

446
00:22:06,429 --> 00:22:09,660
learning so I use their tool chain a lot

447
00:22:09,660 --> 00:22:11,950
most of the code I wrote was programmed

448
00:22:11,950 --> 00:22:14,080
in Python and they have a lot of useful

449
00:22:14,080 --> 00:22:17,800
packages for music the MIDI library MIT

450
00:22:17,800 --> 00:22:20,380
oh and if you're going to be working

451
00:22:20,380 --> 00:22:22,990
with MIDI and generating MIDI you

452
00:22:22,990 --> 00:22:24,940
definitely want to be using a scheduling

453
00:22:24,940 --> 00:22:26,890
library so you're not continuously

454
00:22:26,890 --> 00:22:30,280
writing sleep statements in your code so

455
00:22:30,280 --> 00:22:32,940
sched is the library that I use for that

456
00:22:32,940 --> 00:22:35,920
something that I also use a lot is

457
00:22:35,920 --> 00:22:38,860
graphical programming languages so it's

458
00:22:38,860 --> 00:22:40,900
basically if you've never worked with

459
00:22:40,900 --> 00:22:43,480
one before functions are kind of like

460
00:22:43,480 --> 00:22:46,570
blocks and you draw wires to connect

461
00:22:46,570 --> 00:22:48,429
them and that represents like a signal

462
00:22:48,429 --> 00:22:50,470
flow chain so when you're working with

463
00:22:50,470 --> 00:22:52,840
an audio algorithm it's a really nice

464
00:22:52,840 --> 00:22:56,140
way to represent that I use something

465
00:22:56,140 --> 00:22:59,470
called max/msp which is a paid software

466
00:22:59,470 --> 00:23:01,630
it's a couple hundred dollars but

467
00:23:01,630 --> 00:23:03,820
there's also a open-source version of it

468
00:23:03,820 --> 00:23:06,640
called pure data that's really nice and

469
00:23:06,640 --> 00:23:08,470
both of those have a lot of externals

470
00:23:08,470 --> 00:23:09,809
that people have written for them

471
00:23:09,809 --> 00:23:13,450
sigmund and fiddle in particular I used

472
00:23:13,450 --> 00:23:16,690
for a pitch detection in breathe and

473
00:23:16,690 --> 00:23:18,730
then the last thing I'll mention is

474
00:23:18,730 --> 00:23:20,740
message passing so if you're working

475
00:23:20,740 --> 00:23:22,179
with a lot of different pieces of

476
00:23:22,179 --> 00:23:23,559
hardware or a lot of different

477
00:23:23,559 --> 00:23:26,679
applications open sound control is

478
00:23:26,679 --> 00:23:27,850
something I use a lot

479
00:23:27,850 --> 00:23:30,309
it's a UDP we messaging protocol it's

480
00:23:30,309 --> 00:23:34,630
just really useful for speedily passing

481
00:23:34,630 --> 00:23:39,100
information around and my name is Sarah

482
00:23:39,100 --> 00:23:41,380
Adkins I have more my music on my

483
00:23:41,380 --> 00:23:41,890
website

484
00:23:41,890 --> 00:23:44,440
as well as full audio recordings of all

485
00:23:44,440 --> 00:23:47,620
the pieces I talked about I have my code

486
00:23:47,620 --> 00:23:49,630
on my github it's kind of

487
00:23:49,630 --> 00:23:53,049
well-documented some of it and there's

488
00:23:53,049 --> 00:23:54,970
my email feel free to email me with any

489
00:23:54,970 --> 00:23:57,280
questions or just come and talk to me

490
00:23:57,280 --> 00:24:01,350
later in the conference thank you

