1
00:00:04,319 --> 00:00:08,480
russ heir from the reg team

2
00:00:06,480 --> 00:00:10,160
our next talk today comes from senior

3
00:00:08,480 --> 00:00:13,440
consultant and researcher

4
00:00:10,160 --> 00:00:15,759
speaker and blogger gavin klondike

5
00:00:13,440 --> 00:00:16,560
gavin's talk is titled hacking with

6
00:00:15,759 --> 00:00:20,000
skynet

7
00:00:16,560 --> 00:00:21,759
how ai is empowering adversaries

8
00:00:20,000 --> 00:00:23,680
at the end of the talk a link will be

9
00:00:21,760 --> 00:00:26,880
displayed so that you can join in

10
00:00:23,680 --> 00:00:27,759
on a live q a session with gavin all you

11
00:00:26,880 --> 00:00:29,679
need to do is

12
00:00:27,760 --> 00:00:31,760
click the link and you'll be taken over

13
00:00:29,679 --> 00:00:34,800
to the live q a channel

14
00:00:31,760 --> 00:00:36,960
so without further delay gavin over to

15
00:00:34,800 --> 00:00:36,959
you

16
00:00:40,160 --> 00:00:44,319
hello cactus con and welcome to hacking

17
00:00:43,440 --> 00:00:47,760
with skynet

18
00:00:44,320 --> 00:00:49,520
how ai is empowering adversaries

19
00:00:47,760 --> 00:00:50,960
now before we get started i want to

20
00:00:49,520 --> 00:00:53,120
share with you something that

21
00:00:50,960 --> 00:00:54,320
may be a little scary but really hits on

22
00:00:53,120 --> 00:00:56,800
what we're going to be talking about

23
00:00:54,320 --> 00:00:56,800
today

24
00:00:57,039 --> 00:01:00,800
late last november there was a top

25
00:00:58,879 --> 00:01:02,079
iranian nuclear scientist that was

26
00:01:00,800 --> 00:01:05,119
assassinated

27
00:01:02,079 --> 00:01:08,560
by what some have called an ai powered

28
00:01:05,119 --> 00:01:10,000
gun now there's conflicting reports as

29
00:01:08,560 --> 00:01:12,159
to what exactly happened

30
00:01:10,000 --> 00:01:14,560
but this isn't too far outside the realm

31
00:01:12,159 --> 00:01:16,720
of possibility

32
00:01:14,560 --> 00:01:19,520
there's already tutorials online on how

33
00:01:16,720 --> 00:01:23,520
to create an autonomous century turret

34
00:01:19,520 --> 00:01:24,798
and that uses lidar for object detection

35
00:01:23,520 --> 00:01:26,640
but what if we add a little bit more

36
00:01:24,799 --> 00:01:28,080
brains into the mix

37
00:01:26,640 --> 00:01:29,680
what if we added in some facial

38
00:01:28,080 --> 00:01:32,240
recognition or

39
00:01:29,680 --> 00:01:32,880
deep learning object detection tracking

40
00:01:32,240 --> 00:01:36,079
like

41
00:01:32,880 --> 00:01:38,560
yolo or you only look once now we're

42
00:01:36,079 --> 00:01:40,639
starting to get somewhere

43
00:01:38,560 --> 00:01:41,680
anyways that's your bottom line up front

44
00:01:40,640 --> 00:01:43,360
what we're going to be talking about

45
00:01:41,680 --> 00:01:43,920
today and why this presentation is

46
00:01:43,360 --> 00:01:47,920
called

47
00:01:43,920 --> 00:01:50,799
hacking with skynet my name is gavin

48
00:01:47,920 --> 00:01:51,840
i am known online as gt klondike i am a

49
00:01:50,799 --> 00:01:54,079
security researcher

50
00:01:51,840 --> 00:01:55,680
and ai village officer and a senior

51
00:01:54,079 --> 00:01:57,279
security consultant

52
00:01:55,680 --> 00:01:59,280
i also run a passion project called

53
00:01:57,280 --> 00:02:00,799
netsec explain where i explain

54
00:01:59,280 --> 00:02:03,439
advanced level computer and network

55
00:02:00,799 --> 00:02:05,680
security topics in an easy to understand

56
00:02:03,439 --> 00:02:05,679
way

57
00:02:07,040 --> 00:02:11,200
so for the uninitiated uh you've

58
00:02:09,679 --> 00:02:12,879
probably heard machine learning

59
00:02:11,200 --> 00:02:15,440
artificial intelligence and deep

60
00:02:12,879 --> 00:02:17,679
learning used interchangeably

61
00:02:15,440 --> 00:02:19,200
but there's a difference and we'll go

62
00:02:17,680 --> 00:02:21,599
over that

63
00:02:19,200 --> 00:02:22,799
so artificial intelligence is a big

64
00:02:21,599 --> 00:02:25,200
umbrella term

65
00:02:22,800 --> 00:02:26,800
that covers not only machine learning

66
00:02:25,200 --> 00:02:27,920
but other types of artificial

67
00:02:26,800 --> 00:02:30,720
intelligence like

68
00:02:27,920 --> 00:02:31,518
rule-based artificial intelligence and

69
00:02:30,720 --> 00:02:34,959
traditional

70
00:02:31,519 --> 00:02:37,200
or classical object recognition

71
00:02:34,959 --> 00:02:38,160
underneath that umbrella we have machine

72
00:02:37,200 --> 00:02:40,238
learning

73
00:02:38,160 --> 00:02:41,519
now machine learning uses statistics and

74
00:02:40,239 --> 00:02:44,319
data to come

75
00:02:41,519 --> 00:02:45,120
to inferences or decisions based on the

76
00:02:44,319 --> 00:02:47,200
data

77
00:02:45,120 --> 00:02:48,640
so the algorithms will stay the same but

78
00:02:47,200 --> 00:02:50,399
the data is what

79
00:02:48,640 --> 00:02:52,000
allows the machine to make the decisions

80
00:02:50,400 --> 00:02:53,840
that it's making

81
00:02:52,000 --> 00:02:55,360
and then as a subset of machine learning

82
00:02:53,840 --> 00:02:57,120
we have deep learning

83
00:02:55,360 --> 00:02:59,680
this is where we have deep neural

84
00:02:57,120 --> 00:02:59,680
networks

85
00:03:01,200 --> 00:03:04,560
so think of this presentation as being

86
00:03:03,680 --> 00:03:06,720
really to

87
00:03:04,560 --> 00:03:08,080
smaller talks so in the first section

88
00:03:06,720 --> 00:03:11,200
we're going to be talking about

89
00:03:08,080 --> 00:03:14,319
offensive ai tools what's out there

90
00:03:11,200 --> 00:03:15,518
what's possible and what exists today

91
00:03:14,319 --> 00:03:17,119
and then in the second part we're going

92
00:03:15,519 --> 00:03:18,239
to talk about adversarial machine

93
00:03:17,120 --> 00:03:21,120
learning this is

94
00:03:18,239 --> 00:03:22,879
hacking or attacking machine learning

95
00:03:21,120 --> 00:03:26,400
algorithms and machine learning based

96
00:03:22,879 --> 00:03:28,560
software so why is that important

97
00:03:26,400 --> 00:03:29,920
well there's no question we've already

98
00:03:28,560 --> 00:03:31,200
seen the effects of machine learning in

99
00:03:29,920 --> 00:03:33,200
the security industry

100
00:03:31,200 --> 00:03:35,359
we see how it can help us with log

101
00:03:33,200 --> 00:03:37,679
detection anomaly detection

102
00:03:35,360 --> 00:03:38,720
and sifting through massive amounts of

103
00:03:37,680 --> 00:03:41,519
data

104
00:03:38,720 --> 00:03:43,359
it's very valuable but there's another

105
00:03:41,519 --> 00:03:46,159
side to this coin

106
00:03:43,360 --> 00:03:48,799
how is this allowing attackers to

107
00:03:46,159 --> 00:03:52,079
improve their own capabilities

108
00:03:48,799 --> 00:03:56,159
so let's jump into it by talking about

109
00:03:52,080 --> 00:03:56,159
offensive ai tools

110
00:03:56,640 --> 00:04:00,238
so why do we even want offensive ai

111
00:03:58,480 --> 00:04:02,000
tools well

112
00:04:00,239 --> 00:04:03,920
what this is going to allow us to do is

113
00:04:02,000 --> 00:04:06,159
explore the target attack surface

114
00:04:03,920 --> 00:04:08,238
very quickly especially in today's day

115
00:04:06,159 --> 00:04:09,599
and age where we have a lot of iot

116
00:04:08,239 --> 00:04:12,480
devices we have

117
00:04:09,599 --> 00:04:14,000
billions with a b lines of code on

118
00:04:12,480 --> 00:04:16,159
millions of devices

119
00:04:14,000 --> 00:04:18,478
stamped into a chip and sent across the

120
00:04:16,160 --> 00:04:20,160
world

121
00:04:18,478 --> 00:04:22,000
i can't even wrap my head around the

122
00:04:20,160 --> 00:04:24,560
idea of billions of lines of code

123
00:04:22,000 --> 00:04:26,639
let alone the potential vulnerabilities

124
00:04:24,560 --> 00:04:29,199
that could be associated with that

125
00:04:26,639 --> 00:04:30,080
so how can we operate at a speed and

126
00:04:29,199 --> 00:04:31,840
scale

127
00:04:30,080 --> 00:04:34,479
that will allow us to handle this with

128
00:04:31,840 --> 00:04:36,000
our current workforce

129
00:04:34,479 --> 00:04:37,680
and that's exactly what offensive tools

130
00:04:36,000 --> 00:04:39,440
will allow us to do it'll allow us to

131
00:04:37,680 --> 00:04:41,120
automate some of the penetration testing

132
00:04:39,440 --> 00:04:43,840
and vulnerability assessments

133
00:04:41,120 --> 00:04:44,479
but it also allows adversaries the bad

134
00:04:43,840 --> 00:04:46,638
guys

135
00:04:44,479 --> 00:04:49,120
to attack us in many different and

136
00:04:46,639 --> 00:04:51,280
unthought of ways

137
00:04:49,120 --> 00:04:52,400
so this is going to allow us to move at

138
00:04:51,280 --> 00:04:55,119
a speed and scale

139
00:04:52,400 --> 00:04:56,638
that we have not been able to do before

140
00:04:55,120 --> 00:04:59,919
and it's going to allow us to

141
00:04:56,639 --> 00:05:00,960
automate our manual analysis plus the

142
00:04:59,919 --> 00:05:02,320
added caveat

143
00:05:00,960 --> 00:05:04,239
where we're going to be able to uncover

144
00:05:02,320 --> 00:05:06,560
some hidden blind spots in our defensive

145
00:05:04,240 --> 00:05:11,680
tools and software

146
00:05:06,560 --> 00:05:14,880
and so can they so how realistic is this

147
00:05:11,680 --> 00:05:15,360
well in 2016 darpa initiated the cyber

148
00:05:14,880 --> 00:05:17,120
grand

149
00:05:15,360 --> 00:05:18,960
challenge where it pitted teams of

150
00:05:17,120 --> 00:05:22,800
researchers against each other

151
00:05:18,960 --> 00:05:25,440
in a fully ai or autonomous

152
00:05:22,800 --> 00:05:26,160
capture the flag challenge so as part of

153
00:05:25,440 --> 00:05:29,120
this challenge

154
00:05:26,160 --> 00:05:30,320
the ai or the automated applications and

155
00:05:29,120 --> 00:05:31,919
systems that they built

156
00:05:30,320 --> 00:05:34,000
had to perform its own vulnerability

157
00:05:31,919 --> 00:05:37,280
discovery exploitation

158
00:05:34,000 --> 00:05:39,120
post exploitation or patching and

159
00:05:37,280 --> 00:05:40,559
data theft it would then take the

160
00:05:39,120 --> 00:05:42,560
exploits that it found in its own

161
00:05:40,560 --> 00:05:43,199
systems and launch them against other

162
00:05:42,560 --> 00:05:45,680
systems

163
00:05:43,199 --> 00:05:48,080
automatically everything was air-gapped

164
00:05:45,680 --> 00:05:50,840
and the only people the only humans

165
00:05:48,080 --> 00:05:52,000
on that side of the fence were the

166
00:05:50,840 --> 00:05:54,239
referees

167
00:05:52,000 --> 00:05:55,039
so it was absolutely crazy and if you

168
00:05:54,240 --> 00:05:56,800
haven't i

169
00:05:55,039 --> 00:05:58,318
highly recommend that you check it out

170
00:05:56,800 --> 00:06:01,199
because some of the results

171
00:05:58,319 --> 00:06:02,720
were insane for example one of the

172
00:06:01,199 --> 00:06:05,280
researchers or one of the

173
00:06:02,720 --> 00:06:07,199
the ai units actually found

174
00:06:05,280 --> 00:06:07,758
vulnerabilities in a challenge that the

175
00:06:07,199 --> 00:06:11,120
author

176
00:06:07,759 --> 00:06:14,479
didn't initially intend to have in there

177
00:06:11,120 --> 00:06:17,840
but another one identified

178
00:06:14,479 --> 00:06:19,520
and patched that same vulnerability so

179
00:06:17,840 --> 00:06:20,880
this really gave us a glimpse on the

180
00:06:19,520 --> 00:06:23,359
world stage of

181
00:06:20,880 --> 00:06:24,560
what ai can do and how it can be applied

182
00:06:23,360 --> 00:06:26,960
into the security

183
00:06:24,560 --> 00:06:26,960
sphere

184
00:06:28,160 --> 00:06:32,160
so some applications of offensive ai

185
00:06:30,639 --> 00:06:33,520
unfortunately i'm not going to be able

186
00:06:32,160 --> 00:06:35,680
to get into all of them

187
00:06:33,520 --> 00:06:37,039
but i'm going to be doing a whirlwind

188
00:06:35,680 --> 00:06:37,600
tour to kind of show you what's out

189
00:06:37,039 --> 00:06:39,280
there

190
00:06:37,600 --> 00:06:40,880
and what advancements are being made

191
00:06:39,280 --> 00:06:42,719
today so

192
00:06:40,880 --> 00:06:44,719
right now applications of offensive ai

193
00:06:42,720 --> 00:06:46,560
we have social engineering

194
00:06:44,720 --> 00:06:47,919
we already know that we can use ai to

195
00:06:46,560 --> 00:06:51,360
perform

196
00:06:47,919 --> 00:06:53,680
defensive detection but what if we're a

197
00:06:51,360 --> 00:06:55,199
malware researcher or a malware engineer

198
00:06:53,680 --> 00:06:56,880
and we want to identify whether or not

199
00:06:55,199 --> 00:06:59,199
something is in a sandbox

200
00:06:56,880 --> 00:07:00,719
so how can we find ways to bypass

201
00:06:59,199 --> 00:07:04,160
intrusion detection systems

202
00:07:00,720 --> 00:07:05,360
and even antivirus systems and evade

203
00:07:04,160 --> 00:07:07,199
those or shut down

204
00:07:05,360 --> 00:07:08,479
if we detect that we're located on one

205
00:07:07,199 --> 00:07:11,039
of them

206
00:07:08,479 --> 00:07:12,560
we can use or adversaries can use

207
00:07:11,039 --> 00:07:14,159
artificial intelligence in order to

208
00:07:12,560 --> 00:07:16,000
evaluate data leaks so

209
00:07:14,160 --> 00:07:18,080
massive password dumps or credential

210
00:07:16,000 --> 00:07:18,880
dumps in order to see who's connected to

211
00:07:18,080 --> 00:07:22,318
what

212
00:07:18,880 --> 00:07:24,080
and how things are related uh automated

213
00:07:22,319 --> 00:07:25,440
network exploitation both internal

214
00:07:24,080 --> 00:07:27,599
network exploitation and

215
00:07:25,440 --> 00:07:29,599
external network exploitation and then

216
00:07:27,599 --> 00:07:31,520
automated software exploitation

217
00:07:29,599 --> 00:07:33,199
so this will take fuzzing to a whole new

218
00:07:31,520 --> 00:07:35,758
level develop

219
00:07:33,199 --> 00:07:38,400
new and interesting exploits and proof

220
00:07:35,759 --> 00:07:38,400
of concepts

221
00:07:39,599 --> 00:07:44,800
so let's start off talking about

222
00:07:42,880 --> 00:07:46,960
social engineering and applications for

223
00:07:44,800 --> 00:07:50,160
that so the first one i want to bring up

224
00:07:46,960 --> 00:07:51,919
is stylegan stylegan is a project by

225
00:07:50,160 --> 00:07:55,120
nvidia and what it allows

226
00:07:51,919 --> 00:07:58,960
us to do is create ai generated

227
00:07:55,120 --> 00:08:00,879
human faces that look realistic

228
00:07:58,960 --> 00:08:03,039
so how do we apply this to social

229
00:08:00,879 --> 00:08:05,520
engineering social engineering

230
00:08:03,039 --> 00:08:06,560
uh usually what you'll have is a persona

231
00:08:05,520 --> 00:08:07,758
and so this will be for the social

232
00:08:06,560 --> 00:08:09,039
engineering contest

233
00:08:07,759 --> 00:08:10,879
and this will be for red teams you'll

234
00:08:09,039 --> 00:08:11,520
develop a persona somebody that seems

235
00:08:10,879 --> 00:08:13,120
plausible

236
00:08:11,520 --> 00:08:14,719
usually this comes with a name and a

237
00:08:13,120 --> 00:08:16,319
face

238
00:08:14,720 --> 00:08:18,160
traditionally what people would do is

239
00:08:16,319 --> 00:08:20,000
they'll go to the 10th page of google or

240
00:08:18,160 --> 00:08:22,800
they'll look up a stock photo

241
00:08:20,000 --> 00:08:23,520
grab that and post that as a profile

242
00:08:22,800 --> 00:08:25,919
picture

243
00:08:23,520 --> 00:08:26,960
and so i as a 32 year old male can

244
00:08:25,919 --> 00:08:29,840
impersonate

245
00:08:26,960 --> 00:08:31,039
becky a 20-something female who's

246
00:08:29,840 --> 00:08:31,758
looking at getting started in the

247
00:08:31,039 --> 00:08:33,199
industry

248
00:08:31,759 --> 00:08:35,519
and then i can perform my social

249
00:08:33,200 --> 00:08:37,519
engineering tasks from there i can send

250
00:08:35,519 --> 00:08:38,399
emails to everybody in hr accounting

251
00:08:37,519 --> 00:08:40,560
sales

252
00:08:38,399 --> 00:08:41,440
whatever in order to gather more

253
00:08:40,559 --> 00:08:45,119
information

254
00:08:41,440 --> 00:08:45,839
from a company but a clever security

255
00:08:45,120 --> 00:08:48,240
researcher

256
00:08:45,839 --> 00:08:49,519
or network defender will be able to see

257
00:08:48,240 --> 00:08:51,040
that image

258
00:08:49,519 --> 00:08:52,959
if it doesn't pass the sniff test

259
00:08:51,040 --> 00:08:55,120
they'll grab it reverse image search

260
00:08:52,959 --> 00:08:56,800
and they'll find that exact same person

261
00:08:55,120 --> 00:09:00,080
as i don't know

262
00:08:56,800 --> 00:09:03,199
catherine in new york new york as

263
00:09:00,080 --> 00:09:05,040
a image that i had stolen so

264
00:09:03,200 --> 00:09:06,839
things are going to match up and

265
00:09:05,040 --> 00:09:10,399
immediately flagged as

266
00:09:06,839 --> 00:09:12,640
false but why steal somebody's identity

267
00:09:10,399 --> 00:09:14,000
when i can create a whole new identity

268
00:09:12,640 --> 00:09:15,680
style again will let me generate a

269
00:09:14,000 --> 00:09:17,440
profile picture and there's other tools

270
00:09:15,680 --> 00:09:18,640
out there that'll let me generate social

271
00:09:17,440 --> 00:09:21,600
media content

272
00:09:18,640 --> 00:09:22,800
long form blog posts and even my own

273
00:09:21,600 --> 00:09:24,240
voice

274
00:09:22,800 --> 00:09:26,160
now before i show you the voice part i

275
00:09:24,240 --> 00:09:27,680
kind of want to show you uh

276
00:09:26,160 --> 00:09:29,600
some interesting things about the way

277
00:09:27,680 --> 00:09:30,800
that stylegan is generated and some

278
00:09:29,600 --> 00:09:32,399
things that you can use

279
00:09:30,800 --> 00:09:34,079
to identify and see if somebody has

280
00:09:32,399 --> 00:09:35,600
already done this

281
00:09:34,080 --> 00:09:37,440
so if you notice that all of the images

282
00:09:35,600 --> 00:09:39,040
they're all looking straight

283
00:09:37,440 --> 00:09:41,200
uh another thing that you might notice

284
00:09:39,040 --> 00:09:43,120
and this isn't true for all images but

285
00:09:41,200 --> 00:09:44,480
you'll see artifacts happening in the

286
00:09:43,120 --> 00:09:46,480
background the background is really

287
00:09:44,480 --> 00:09:48,399
blurry but if you see things that are

288
00:09:46,480 --> 00:09:49,440
supposed to be vertical or in a straight

289
00:09:48,399 --> 00:09:52,240
line like

290
00:09:49,440 --> 00:09:55,600
trees or fence posts you can kind of see

291
00:09:52,240 --> 00:09:57,360
some distortion or artifacts

292
00:09:55,600 --> 00:09:59,440
an easier way to demonstrate this is

293
00:09:57,360 --> 00:10:01,839
with ai-generated cats

294
00:09:59,440 --> 00:10:03,200
so all the images here are also

295
00:10:01,839 --> 00:10:04,959
generated by stylegan

296
00:10:03,200 --> 00:10:06,560
of cat pictures and you can see the

297
00:10:04,959 --> 00:10:08,000
level of detail that they have in the

298
00:10:06,560 --> 00:10:10,239
fur and the whiskers

299
00:10:08,000 --> 00:10:11,600
but artifacts do happen their paws

300
00:10:10,240 --> 00:10:14,560
aren't rendered properly

301
00:10:11,600 --> 00:10:15,200
and you can even see some weird like wtf

302
00:10:14,560 --> 00:10:18,880
stuff

303
00:10:15,200 --> 00:10:21,279
going on over here

304
00:10:18,880 --> 00:10:22,640
so let's talk about voice what if it's

305
00:10:21,279 --> 00:10:24,880
not enough for me to

306
00:10:22,640 --> 00:10:26,399
communicate over chat and somebody wants

307
00:10:24,880 --> 00:10:29,120
to actually conduct an interview

308
00:10:26,399 --> 00:10:29,920
or get on the phone with me how can i

309
00:10:29,120 --> 00:10:33,200
impersonate

310
00:10:29,920 --> 00:10:35,439
becky well i can use a friend of mine

311
00:10:33,200 --> 00:10:36,320
or i can use a synthetic voice that will

312
00:10:35,440 --> 00:10:38,560
be able to

313
00:10:36,320 --> 00:10:39,839
put in the right suggestions that i need

314
00:10:38,560 --> 00:10:42,319
in order to

315
00:10:39,839 --> 00:10:43,279
ask the questions of part of my social

316
00:10:42,320 --> 00:10:46,079
engineering campaign

317
00:10:43,279 --> 00:10:47,120
and get those trophies so i'm going to

318
00:10:46,079 --> 00:10:50,000
show you a couple

319
00:10:47,120 --> 00:10:50,720
voice clips from lyra bird and tacotron

320
00:10:50,000 --> 00:10:53,839
lyrebird

321
00:10:50,720 --> 00:10:57,360
is a a real organization

322
00:10:53,839 --> 00:10:58,640
and it's pitched as a reverse

323
00:10:57,360 --> 00:11:00,800
transcription service

324
00:10:58,640 --> 00:11:02,319
so you type in whatever it is you want

325
00:11:00,800 --> 00:11:02,959
to say and then you send it off to

326
00:11:02,320 --> 00:11:04,240
lyrebird

327
00:11:02,959 --> 00:11:07,359
and they will give you a human

328
00:11:04,240 --> 00:11:10,000
synthesized voice saying those things

329
00:11:07,360 --> 00:11:12,399
and then tacotron is a google project

330
00:11:10,000 --> 00:11:15,120
google project

331
00:11:12,399 --> 00:11:15,440
that does a similar thing so let's go

332
00:11:15,120 --> 00:11:17,279
ahead

333
00:11:15,440 --> 00:11:20,640
check out the original voice and these

334
00:11:17,279 --> 00:11:20,640
first two are from lyrebird

335
00:11:21,680 --> 00:11:26,640
in another moment down went alice after

336
00:11:24,000 --> 00:11:30,160
it never once did she consider how in

337
00:11:26,640 --> 00:11:30,160
the world she was to get out again

338
00:11:30,800 --> 00:11:34,319
awesome and so the synthetic voice based

339
00:11:33,760 --> 00:11:38,079
off of

340
00:11:34,320 --> 00:11:41,839
that sample image or that sample clip

341
00:11:38,079 --> 00:11:42,560
oops my voice might be generated by a

342
00:11:41,839 --> 00:11:44,880
computer

343
00:11:42,560 --> 00:11:46,719
but i think it sounds pretty human i

344
00:11:44,880 --> 00:11:49,760
don't know exactly how they made it but

345
00:11:46,720 --> 00:11:49,760
i'm really impressed

346
00:11:50,560 --> 00:11:55,439
very nice so

347
00:11:53,600 --> 00:11:56,959
what we can do is type up a script or

348
00:11:55,440 --> 00:11:59,680
type up a series of scripts

349
00:11:56,959 --> 00:12:00,560
and will react to question answer

350
00:11:59,680 --> 00:12:03,680
segments

351
00:12:00,560 --> 00:12:05,839
and then use that for social engineering

352
00:12:03,680 --> 00:12:07,920
but you notice that it sounds a little

353
00:12:05,839 --> 00:12:09,600
synthetic or it just sounds a little off

354
00:12:07,920 --> 00:12:11,040
you can explain that away as saying uh

355
00:12:09,600 --> 00:12:12,560
it's just part of a bad connection or

356
00:12:11,040 --> 00:12:14,079
i'm using wi-fi calling or something of

357
00:12:12,560 --> 00:12:15,279
that nature

358
00:12:14,079 --> 00:12:17,120
so the last one that i want to show you

359
00:12:15,279 --> 00:12:19,120
here is tacotron and there's been a lot

360
00:12:17,120 --> 00:12:21,920
of advancements since i originally

361
00:12:19,120 --> 00:12:23,040
pulled this audio clip but this one what

362
00:12:21,920 --> 00:12:23,760
i want you to do is while you're

363
00:12:23,040 --> 00:12:27,839
listening to it

364
00:12:23,760 --> 00:12:29,839
listen to the breaths and species

365
00:12:27,839 --> 00:12:31,680
normally in human speech we'll have

366
00:12:29,839 --> 00:12:33,920
where there's a comma or a period

367
00:12:31,680 --> 00:12:35,760
a breath or a space something that kind

368
00:12:33,920 --> 00:12:37,360
of fills in that gap

369
00:12:35,760 --> 00:12:40,240
another thing and this isn't part of

370
00:12:37,360 --> 00:12:43,680
this research is they'll introduce

371
00:12:40,240 --> 00:12:46,399
filler words like um and uh so and and

372
00:12:43,680 --> 00:12:49,120
so anyways take a listen to this clip

373
00:12:46,399 --> 00:12:50,720
listen for the breaths

374
00:12:49,120 --> 00:12:53,120
only the photograph really showed how

375
00:12:50,720 --> 00:12:54,880
much time had passed 10 years ago

376
00:12:53,120 --> 00:12:56,639
there had been lots of pictures of what

377
00:12:54,880 --> 00:12:58,160
looked like a large pink beach ball

378
00:12:56,639 --> 00:13:00,399
wearing different colored bonnets

379
00:12:58,160 --> 00:13:02,000
but dudley dursley was no longer a baby

380
00:13:00,399 --> 00:13:04,639
and now the photograph showed a large

381
00:13:02,000 --> 00:13:06,320
blonde boy riding his first bicycle

382
00:13:04,639 --> 00:13:08,240
on a carousel at the fair playing a

383
00:13:06,320 --> 00:13:10,079
computer game with his father being

384
00:13:08,240 --> 00:13:12,710
hugged and kissed by his mother

385
00:13:10,079 --> 00:13:14,959
by his mother

386
00:13:12,710 --> 00:13:16,240
[Music]

387
00:13:14,959 --> 00:13:18,000
and these are technologies that are

388
00:13:16,240 --> 00:13:21,600
advancing all the time

389
00:13:18,000 --> 00:13:22,079
so soon we might have to keep an eye out

390
00:13:21,600 --> 00:13:24,240
for

391
00:13:22,079 --> 00:13:25,519
synthetically generated voice that is

392
00:13:24,240 --> 00:13:30,000
trying to social engineer

393
00:13:25,519 --> 00:13:33,519
our organizations or us as individuals

394
00:13:30,000 --> 00:13:34,560
so let's move on next we have sandbox

395
00:13:33,519 --> 00:13:36,639
detection

396
00:13:34,560 --> 00:13:37,920
so for malware if you're a malware

397
00:13:36,639 --> 00:13:40,000
engineer

398
00:13:37,920 --> 00:13:41,439
what you want to do is try and avoid

399
00:13:40,000 --> 00:13:43,519
sandbox detection

400
00:13:41,440 --> 00:13:45,760
as long as possible the reason is

401
00:13:43,519 --> 00:13:47,600
because

402
00:13:45,760 --> 00:13:50,720
antivirus companies such as norton

403
00:13:47,600 --> 00:13:53,040
kaspersky mcafee what have you

404
00:13:50,720 --> 00:13:54,720
they try and grab malware samples or

405
00:13:53,040 --> 00:13:55,040
potential malware samples they throw

406
00:13:54,720 --> 00:13:57,360
them

407
00:13:55,040 --> 00:13:58,079
in a special environment and then they

408
00:13:57,360 --> 00:14:01,279
just

409
00:13:58,079 --> 00:14:02,319
let them run and see what they do so as

410
00:14:01,279 --> 00:14:04,160
a malware engineer

411
00:14:02,320 --> 00:14:05,360
i want my malware to go undetected for

412
00:14:04,160 --> 00:14:07,600
as long as possible

413
00:14:05,360 --> 00:14:08,720
which means that i need to find a way to

414
00:14:07,600 --> 00:14:11,199
get past

415
00:14:08,720 --> 00:14:13,440
this sandbox or this this phase of

416
00:14:11,199 --> 00:14:15,120
analysis that they perform

417
00:14:13,440 --> 00:14:16,880
so i'm going to perform sandbox

418
00:14:15,120 --> 00:14:17,279
detection and implement that into my

419
00:14:16,880 --> 00:14:20,240
alpha

420
00:14:17,279 --> 00:14:21,839
into my malware so the way that sandbox

421
00:14:20,240 --> 00:14:23,600
detection some of the researchers over

422
00:14:21,839 --> 00:14:26,480
at silent break security

423
00:14:23,600 --> 00:14:27,279
what they did was they identified what

424
00:14:26,480 --> 00:14:29,680
is something

425
00:14:27,279 --> 00:14:30,959
that will identify or what what are some

426
00:14:29,680 --> 00:14:34,319
uh processes

427
00:14:30,959 --> 00:14:36,319
that we'll see inside of a legitimate

428
00:14:34,320 --> 00:14:36,800
operating system that is being used by a

429
00:14:36,320 --> 00:14:38,800
user

430
00:14:36,800 --> 00:14:39,920
and processes that are just in there by

431
00:14:38,800 --> 00:14:41,760
default on

432
00:14:39,920 --> 00:14:44,959
some of the well-known sandboxes like

433
00:14:41,760 --> 00:14:47,279
fireeye and cuckoo

434
00:14:44,959 --> 00:14:48,880
what they were able to do is use process

435
00:14:47,279 --> 00:14:51,279
lists user count

436
00:14:48,880 --> 00:14:52,639
and other types of information on the

437
00:14:51,279 --> 00:14:55,279
host operating system

438
00:14:52,639 --> 00:14:57,199
as data points and they said based on

439
00:14:55,279 --> 00:14:58,800
those data points that we've identified

440
00:14:57,199 --> 00:15:00,479
we're going to set a threshold and based

441
00:14:58,800 --> 00:15:02,160
on that threshold we're going to

442
00:15:00,480 --> 00:15:03,199
determine whether or not we're in a

443
00:15:02,160 --> 00:15:06,240
sandbox or

444
00:15:03,199 --> 00:15:07,279
on a legitimate user a legitimate user

445
00:15:06,240 --> 00:15:09,120
system

446
00:15:07,279 --> 00:15:10,959
if we're on a legitimate user system go

447
00:15:09,120 --> 00:15:12,320
ahead execute no problem

448
00:15:10,959 --> 00:15:15,680
if we have identified that we're on a

449
00:15:12,320 --> 00:15:18,880
sandbox then shut down delete itself

450
00:15:15,680 --> 00:15:22,479
and it can't be analyzed so for this

451
00:15:18,880 --> 00:15:23,519
uh every ai or machine learning

452
00:15:22,480 --> 00:15:25,279
algorithm

453
00:15:23,519 --> 00:15:26,959
you need to set a threshold so it gives

454
00:15:25,279 --> 00:15:28,320
you a certain confidence level based on

455
00:15:26,959 --> 00:15:30,479
the data points that you give and then

456
00:15:28,320 --> 00:15:33,759
you set a certain threshold

457
00:15:30,480 --> 00:15:35,440
so in this case we have hosts a c and f

458
00:15:33,759 --> 00:15:37,519
that were determined to be part of a

459
00:15:35,440 --> 00:15:40,399
sandbox and that's because they didn't

460
00:15:37,519 --> 00:15:43,360
reach this threshold of 168

461
00:15:40,399 --> 00:15:45,120
which is the host average score so if

462
00:15:43,360 --> 00:15:46,880
you didn't reach the host average score

463
00:15:45,120 --> 00:15:50,480
then i'm going to consider you a sandbox

464
00:15:46,880 --> 00:15:50,480
and just shut down

465
00:15:52,160 --> 00:15:55,920
so the next part that i want to talk

466
00:15:53,440 --> 00:15:58,399
about is automatic

467
00:15:55,920 --> 00:15:59,680
network exploitation and these are some

468
00:15:58,399 --> 00:16:01,920
tools that were released a couple years

469
00:15:59,680 --> 00:16:05,120
ago that are still being updated today

470
00:16:01,920 --> 00:16:07,199
uh deep exploit and goython they work

471
00:16:05,120 --> 00:16:08,160
very similar but they automate the

472
00:16:07,199 --> 00:16:10,000
network intro

473
00:16:08,160 --> 00:16:11,279
uh the network penetration testing

474
00:16:10,000 --> 00:16:13,120
process

475
00:16:11,279 --> 00:16:14,560
so where they'll start is they'll begin

476
00:16:13,120 --> 00:16:17,440
with an nmap scan

477
00:16:14,560 --> 00:16:19,119
or a series of analyzers so maybe a nesa

478
00:16:17,440 --> 00:16:20,160
scan or openvas or something of that

479
00:16:19,120 --> 00:16:21,600
nature

480
00:16:20,160 --> 00:16:23,279
and then they'll follow it up with

481
00:16:21,600 --> 00:16:26,079
identifying certain ports

482
00:16:23,279 --> 00:16:27,680
so if port 80 is open or 443 then we're

483
00:16:26,079 --> 00:16:28,719
going to perform web crawling using

484
00:16:27,680 --> 00:16:30,399
scrappy

485
00:16:28,720 --> 00:16:31,839
based on the information that we pulled

486
00:16:30,399 --> 00:16:33,759
from the nmap scan

487
00:16:31,839 --> 00:16:36,320
nessus and scrappy we're going to be

488
00:16:33,759 --> 00:16:37,120
able to identify and enumerate the type

489
00:16:36,320 --> 00:16:39,680
of service

490
00:16:37,120 --> 00:16:40,560
that's running on the system so are they

491
00:16:39,680 --> 00:16:43,599
running apache

492
00:16:40,560 --> 00:16:46,160
nginx are they running jboss

493
00:16:43,600 --> 00:16:46,880
are they running jenkins things like

494
00:16:46,160 --> 00:16:48,399
that

495
00:16:46,880 --> 00:16:50,000
then they're going to port it over to

496
00:16:48,399 --> 00:16:53,120
metasploit and see what existing

497
00:16:50,000 --> 00:16:55,680
metasploit modules there are and

498
00:16:53,120 --> 00:16:57,440
try and kick off the types of metasploit

499
00:16:55,680 --> 00:16:58,959
modules that'll target that specific

500
00:16:57,440 --> 00:17:00,480
type of system

501
00:16:58,959 --> 00:17:01,839
they'll run through the gamut see what

502
00:17:00,480 --> 00:17:03,440
connects back and then they'll be able

503
00:17:01,839 --> 00:17:05,679
to print out a nice report

504
00:17:03,440 --> 00:17:06,640
that will tell you what was exploitable

505
00:17:05,679 --> 00:17:10,240
what was found

506
00:17:06,640 --> 00:17:10,240
and what was actually exploited

507
00:17:12,480 --> 00:17:17,599
my favorite one in the network intrusion

508
00:17:15,919 --> 00:17:18,559
or the network penetration testing

509
00:17:17,599 --> 00:17:21,599
section

510
00:17:18,559 --> 00:17:22,959
is this application called death star

511
00:17:21,599 --> 00:17:24,879
now death star is one of my favorites

512
00:17:22,959 --> 00:17:26,720
just because of the way that it works

513
00:17:24,880 --> 00:17:28,400
it seems like it's just a bunch of if

514
00:17:26,720 --> 00:17:30,400
statements but it's a lot more

515
00:17:28,400 --> 00:17:33,520
complicated than that

516
00:17:30,400 --> 00:17:34,480
so here we start off saying uh going

517
00:17:33,520 --> 00:17:37,120
from a regular

518
00:17:34,480 --> 00:17:38,640
domain user account to try and escalate

519
00:17:37,120 --> 00:17:40,719
up to domain admin

520
00:17:38,640 --> 00:17:42,880
and we already see attack graphs like

521
00:17:40,720 --> 00:17:44,400
this kind of being generated by tools

522
00:17:42,880 --> 00:17:46,799
like bloodhound

523
00:17:44,400 --> 00:17:48,720
but this one's a little unique so it

524
00:17:46,799 --> 00:17:50,799
starts off in a start state and says

525
00:17:48,720 --> 00:17:52,480
okay do i already have domain admin

526
00:17:50,799 --> 00:17:54,720
credentials or do i already have

527
00:17:52,480 --> 00:17:56,320
a domain admin user it goes down one

528
00:17:54,720 --> 00:17:57,760
track if we have the user it goes down

529
00:17:56,320 --> 00:18:01,360
another track if we have the user

530
00:17:57,760 --> 00:18:03,120
and the credentials and then it ends uh

531
00:18:01,360 --> 00:18:04,719
if not then we're just going to go ahead

532
00:18:03,120 --> 00:18:08,159
and start the next loop

533
00:18:04,720 --> 00:18:10,720
and we go down this middle spine column

534
00:18:08,160 --> 00:18:11,840
if you will so it says okay let's spawn

535
00:18:10,720 --> 00:18:13,120
a new agent

536
00:18:11,840 --> 00:18:15,360
now do we know where the domain

537
00:18:13,120 --> 00:18:16,879
controllers are yes all right let's

538
00:18:15,360 --> 00:18:19,918
enumerate the domain controllers

539
00:18:16,880 --> 00:18:20,559
so we're going to use get net domain

540
00:18:19,919 --> 00:18:23,600
controller

541
00:18:20,559 --> 00:18:25,678
get net group member invoke user hunter

542
00:18:23,600 --> 00:18:28,080
and try and identify all the paths that

543
00:18:25,679 --> 00:18:31,360
we can to get to a

544
00:18:28,080 --> 00:18:32,720
domain admin now

545
00:18:31,360 --> 00:18:35,039
i should have said this earlier but this

546
00:18:32,720 --> 00:18:38,799
is built on powershell empire

547
00:18:35,039 --> 00:18:40,720
3 and so some of these things may work

548
00:18:38,799 --> 00:18:41,600
may not work i know powershell empire

549
00:18:40,720 --> 00:18:45,360
kind of has some

550
00:18:41,600 --> 00:18:46,320
criticisms against it but uh besides

551
00:18:45,360 --> 00:18:48,879
that the idea

552
00:18:46,320 --> 00:18:50,559
is still there eventually it'll go down

553
00:18:48,880 --> 00:18:52,400
it'll find domain admin or it'll find a

554
00:18:50,559 --> 00:18:53,918
new windows system and it'll say hey

555
00:18:52,400 --> 00:18:55,760
let's enumerate the processes on the

556
00:18:53,919 --> 00:18:56,720
system and then let's identify the

557
00:18:55,760 --> 00:18:59,039
operating system

558
00:18:56,720 --> 00:19:00,960
are we running windows 7 yeah let's run

559
00:18:59,039 --> 00:19:02,080
mimikatz on it are we running windows 8

560
00:19:00,960 --> 00:19:04,799
windows 10

561
00:19:02,080 --> 00:19:05,840
let's try other exploitation techniques

562
00:19:04,799 --> 00:19:07,918
and then if there's

563
00:19:05,840 --> 00:19:08,879
other users on the system that we don't

564
00:19:07,919 --> 00:19:10,480
already have in our list

565
00:19:08,880 --> 00:19:12,880
let's add it to our list and try the

566
00:19:10,480 --> 00:19:16,240
whole process again

567
00:19:12,880 --> 00:19:16,720
so this is very cool and i'm excited to

568
00:19:16,240 --> 00:19:20,000
see

569
00:19:16,720 --> 00:19:23,360
where this will go but we might see

570
00:19:20,000 --> 00:19:25,120
some adversaries or even apts

571
00:19:23,360 --> 00:19:27,199
start to use this kind of automation in

572
00:19:25,120 --> 00:19:28,239
some of their tools in order to traverse

573
00:19:27,200 --> 00:19:31,600
through the network

574
00:19:28,240 --> 00:19:34,000
secretly and quickly so be on the

575
00:19:31,600 --> 00:19:37,199
lookout

576
00:19:34,000 --> 00:19:40,320
anyways that's all i have for the

577
00:19:37,200 --> 00:19:42,080
offensive ai tools portion but i don't

578
00:19:40,320 --> 00:19:43,600
want to leave you empty-handed on here

579
00:19:42,080 --> 00:19:45,360
if you want to develop your own

580
00:19:43,600 --> 00:19:48,240
offensive ai tools

581
00:19:45,360 --> 00:19:49,439
here's my recommendations first start

582
00:19:48,240 --> 00:19:52,080
with a pipeline

583
00:19:49,440 --> 00:19:53,039
so something that is repeatable and

584
00:19:52,080 --> 00:19:55,678
automated

585
00:19:53,039 --> 00:19:57,440
or wrote so for example you'll start

586
00:19:55,679 --> 00:19:59,679
with an nmap scan

587
00:19:57,440 --> 00:20:01,679
okay cool with an nmap scan what can we

588
00:19:59,679 --> 00:20:02,720
do with this oh well i see port 80 or

589
00:20:01,679 --> 00:20:05,120
443

590
00:20:02,720 --> 00:20:06,320
let me go ahead and perform some

591
00:20:05,120 --> 00:20:08,479
analysis on that

592
00:20:06,320 --> 00:20:09,760
we saw with goythan and deep exploit

593
00:20:08,480 --> 00:20:14,799
that they're using

594
00:20:09,760 --> 00:20:16,559
scrappy and so we can also use gobuster

595
00:20:14,799 --> 00:20:18,720
dirtbuster something of that nature and

596
00:20:16,559 --> 00:20:20,158
then we can enumerate

597
00:20:18,720 --> 00:20:22,080
all the different directories we can use

598
00:20:20,159 --> 00:20:23,600
aquatone to grab screenshots

599
00:20:22,080 --> 00:20:26,480
so now we have a lot of information

600
00:20:23,600 --> 00:20:28,000
about this potential web application

601
00:20:26,480 --> 00:20:30,320
we can continue further down our

602
00:20:28,000 --> 00:20:31,039
analysis we can try identify a login

603
00:20:30,320 --> 00:20:34,240
page

604
00:20:31,039 --> 00:20:35,679
try and log in as a potential user or as

605
00:20:34,240 --> 00:20:38,720
a

606
00:20:35,679 --> 00:20:41,200
using a password list and

607
00:20:38,720 --> 00:20:43,760
so this is the the way that you develop

608
00:20:41,200 --> 00:20:45,440
the framework for building ai tools

609
00:20:43,760 --> 00:20:47,679
you just take a process that you do

610
00:20:45,440 --> 00:20:49,520
repeatedly build a pipeline out of it

611
00:20:47,679 --> 00:20:51,120
and then turn that into data to be

612
00:20:49,520 --> 00:20:54,799
passed on to the next process

613
00:20:51,120 --> 00:20:54,799
and or the next pipeline

614
00:20:55,120 --> 00:20:58,479
so the next thing that i want to talk

615
00:20:56,320 --> 00:21:01,200
about is adversarial

616
00:20:58,480 --> 00:21:02,400
machine learning adversarial machine

617
00:21:01,200 --> 00:21:05,440
learning is actually

618
00:21:02,400 --> 00:21:07,440
attacking the ai algorithms

619
00:21:05,440 --> 00:21:09,840
and this is specifically machine

620
00:21:07,440 --> 00:21:09,840
learning

621
00:21:10,960 --> 00:21:15,600
so there are really four primary ways to

622
00:21:13,600 --> 00:21:18,639
attack a machine learning model

623
00:21:15,600 --> 00:21:20,240
first is to look at the model robustness

624
00:21:18,640 --> 00:21:22,640
which isn't really attacking the model

625
00:21:20,240 --> 00:21:25,039
itself so much as attacking the software

626
00:21:22,640 --> 00:21:27,200
around the model

627
00:21:25,039 --> 00:21:28,158
all of machine learning as advanced as

628
00:21:27,200 --> 00:21:30,080
it may sound

629
00:21:28,159 --> 00:21:31,200
it's just software and it's vulnerable

630
00:21:30,080 --> 00:21:32,720
to the exact same software

631
00:21:31,200 --> 00:21:36,400
vulnerabilities that we see day in

632
00:21:32,720 --> 00:21:40,000
and day out uh the three others we have

633
00:21:36,400 --> 00:21:40,880
model evasion so how can we develop some

634
00:21:40,000 --> 00:21:43,440
sort of

635
00:21:40,880 --> 00:21:45,280
adversarial example that'll bypass the

636
00:21:43,440 --> 00:21:47,280
model so if the model has intrusion

637
00:21:45,280 --> 00:21:48,720
detection or something of that nature

638
00:21:47,280 --> 00:21:51,280
how can we actually use the intrusion

639
00:21:48,720 --> 00:21:52,400
detection built into that model to

640
00:21:51,280 --> 00:21:56,480
bypass it

641
00:21:52,400 --> 00:21:58,880
and be considered benign model poisoning

642
00:21:56,480 --> 00:21:59,840
is how do we actually influence the way

643
00:21:58,880 --> 00:22:03,039
that the model

644
00:21:59,840 --> 00:22:05,199
learns in order to build

645
00:22:03,039 --> 00:22:07,520
new adversarial examples without being

646
00:22:05,200 --> 00:22:08,480
detected or to just completely tear down

647
00:22:07,520 --> 00:22:11,360
the model

648
00:22:08,480 --> 00:22:12,640
and uh ruin it and then we have data

649
00:22:11,360 --> 00:22:14,639
leakage so

650
00:22:12,640 --> 00:22:16,000
maybe we have access to the model in

651
00:22:14,640 --> 00:22:17,360
some way shape or form

652
00:22:16,000 --> 00:22:19,280
and then we can ask it targeted

653
00:22:17,360 --> 00:22:20,320
questions in order to leak data and

654
00:22:19,280 --> 00:22:22,879
information

655
00:22:20,320 --> 00:22:24,158
that the model was trained on so this

656
00:22:22,880 --> 00:22:26,880
can be really dangerous

657
00:22:24,159 --> 00:22:29,280
for top secret information or hipaa

658
00:22:26,880 --> 00:22:31,120
information right phi data so if a model

659
00:22:29,280 --> 00:22:34,080
is trained on phi data

660
00:22:31,120 --> 00:22:34,399
and it's vulnerable to data leakage then

661
00:22:34,080 --> 00:22:37,039
we

662
00:22:34,400 --> 00:22:38,559
as adversaries can just extract that phi

663
00:22:37,039 --> 00:22:41,200
data out of the model

664
00:22:38,559 --> 00:22:43,440
it's very hard to do but definitely a

665
00:22:41,200 --> 00:22:43,440
thing

666
00:22:43,679 --> 00:22:47,280
so the first thing i want to talk about

667
00:22:45,039 --> 00:22:50,720
is model robustness and

668
00:22:47,280 --> 00:22:52,320
sometimes the software is just broken

669
00:22:50,720 --> 00:22:54,640
if it's not a robust model it's not a

670
00:22:52,320 --> 00:22:57,678
good model and that is true

671
00:22:54,640 --> 00:22:59,760
absolutely true for machine learning

672
00:22:57,679 --> 00:23:00,799
so a really good example would be

673
00:22:59,760 --> 00:23:02,720
self-driving cars

674
00:23:00,799 --> 00:23:04,799
self-driving cars you'll see all these

675
00:23:02,720 --> 00:23:06,480
cool hacks that are happening

676
00:23:04,799 --> 00:23:08,639
security researcher will find them

677
00:23:06,480 --> 00:23:11,039
they'll submit them off to

678
00:23:08,640 --> 00:23:12,400
toyota or jeep or whatever company and

679
00:23:11,039 --> 00:23:13,280
then the company will come back and say

680
00:23:12,400 --> 00:23:16,559
oh

681
00:23:13,280 --> 00:23:17,280
that's not a security problem why not

682
00:23:16,559 --> 00:23:18,639
what happened

683
00:23:17,280 --> 00:23:20,320
like i was able to influence your

684
00:23:18,640 --> 00:23:21,840
self-driving car

685
00:23:20,320 --> 00:23:23,600
well this is where we talk about model

686
00:23:21,840 --> 00:23:25,840
robustness uh

687
00:23:23,600 --> 00:23:27,439
the model needs to be able to exist in

688
00:23:25,840 --> 00:23:30,559
its environment

689
00:23:27,440 --> 00:23:32,320
without having problems that's way

690
00:23:30,559 --> 00:23:33,840
before we get to the security section

691
00:23:32,320 --> 00:23:35,520
where the model can exist in its

692
00:23:33,840 --> 00:23:38,158
environment but we

693
00:23:35,520 --> 00:23:39,918
as adversaries can try and influence it

694
00:23:38,159 --> 00:23:42,080
or attack it and make it to do something

695
00:23:39,919 --> 00:23:43,919
that it wasn't expecting to do

696
00:23:42,080 --> 00:23:45,279
so in this example actually one of my

697
00:23:43,919 --> 00:23:48,159
favorite examples is

698
00:23:45,279 --> 00:23:49,440
called the summoning circle attack where

699
00:23:48,159 --> 00:23:52,240
somebody can just paint

700
00:23:49,440 --> 00:23:52,960
what seem like road lines out of salt

701
00:23:52,240 --> 00:23:56,000
onto

702
00:23:52,960 --> 00:23:57,760
a road or concrete or asphalt

703
00:23:56,000 --> 00:23:59,279
and uh as soon as a self-driving car

704
00:23:57,760 --> 00:24:01,440
goes into it it gets stuck

705
00:23:59,279 --> 00:24:02,720
because it knows that it's not legally

706
00:24:01,440 --> 00:24:05,919
allowed to cross

707
00:24:02,720 --> 00:24:07,679
that that intersection or that street

708
00:24:05,919 --> 00:24:09,760
but it also can't back up it can't go

709
00:24:07,679 --> 00:24:12,720
off to the side so it gets very confused

710
00:24:09,760 --> 00:24:14,640
and has no idea what it's supposed to do

711
00:24:12,720 --> 00:24:15,760
another example of failed model

712
00:24:14,640 --> 00:24:18,080
robustness with

713
00:24:15,760 --> 00:24:20,400
self-driving cars is a team of

714
00:24:18,080 --> 00:24:22,559
researchers working on tesla

715
00:24:20,400 --> 00:24:23,440
was actually able to get a tesla car to

716
00:24:22,559 --> 00:24:26,559
switch lanes

717
00:24:23,440 --> 00:24:28,080
into potentially oncoming traffic now

718
00:24:26,559 --> 00:24:29,279
the reason why this isn't really a

719
00:24:28,080 --> 00:24:31,199
security concern

720
00:24:29,279 --> 00:24:33,120
is because what they were able to do is

721
00:24:31,200 --> 00:24:36,240
just put little lidar stickers

722
00:24:33,120 --> 00:24:40,080
on the asphalt and so they just did the

723
00:24:36,240 --> 00:24:41,919
wiley coyote version of drawing

724
00:24:40,080 --> 00:24:43,840
lane lines over into the oncoming

725
00:24:41,919 --> 00:24:46,480
traffic so

726
00:24:43,840 --> 00:24:47,918
not really a security concern more they

727
00:24:46,480 --> 00:24:52,799
just need to work on their model

728
00:24:47,919 --> 00:24:52,799
for their self-driving car let's move on

729
00:24:53,120 --> 00:24:57,360
so this is where we start getting into

730
00:24:54,640 --> 00:24:59,679
the actual adversarial approach

731
00:24:57,360 --> 00:25:01,120
right so when you're going to attack or

732
00:24:59,679 --> 00:25:03,600
when somebody is going to attack

733
00:25:01,120 --> 00:25:04,399
a machine learning model you have either

734
00:25:03,600 --> 00:25:07,120
a white box

735
00:25:04,400 --> 00:25:08,320
or a black box the white box is fairly

736
00:25:07,120 --> 00:25:10,479
self-explanatory

737
00:25:08,320 --> 00:25:12,399
uh white box here we have the source

738
00:25:10,480 --> 00:25:13,919
code or the data that the model was

739
00:25:12,400 --> 00:25:15,840
originally trained on we have the way

740
00:25:13,919 --> 00:25:17,600
that the model was created

741
00:25:15,840 --> 00:25:19,760
and so we can just do a code review and

742
00:25:17,600 --> 00:25:22,719
we can rebuild the model and we can

743
00:25:19,760 --> 00:25:23,360
try and find little uh points in that

744
00:25:22,720 --> 00:25:26,159
model

745
00:25:23,360 --> 00:25:28,320
that are vulnerable the black box model

746
00:25:26,159 --> 00:25:28,720
is a little bit more complicated instead

747
00:25:28,320 --> 00:25:30,799
of

748
00:25:28,720 --> 00:25:32,960
trying to attack it like a web

749
00:25:30,799 --> 00:25:34,960
application from a black box perspective

750
00:25:32,960 --> 00:25:36,320
what we try to do is ask the model a

751
00:25:34,960 --> 00:25:38,240
series of questions right

752
00:25:36,320 --> 00:25:40,320
millions and millions of questions so

753
00:25:38,240 --> 00:25:42,400
that we can get as many data points as

754
00:25:40,320 --> 00:25:45,760
we can to train our own

755
00:25:42,400 --> 00:25:47,200
surrogate model now if you're familiar

756
00:25:45,760 --> 00:25:49,919
with cryptography and some of the

757
00:25:47,200 --> 00:25:51,919
the crypto games that are done there uh

758
00:25:49,919 --> 00:25:52,400
we can use that surrogate model to act

759
00:25:51,919 --> 00:25:55,279
as an

760
00:25:52,400 --> 00:25:56,559
oracle where we can try and attack the

761
00:25:55,279 --> 00:25:59,520
oracle

762
00:25:56,559 --> 00:26:01,600
and whatever works on the oracle should

763
00:25:59,520 --> 00:26:03,760
work on the original model

764
00:26:01,600 --> 00:26:06,158
this is because of a phenomenon known as

765
00:26:03,760 --> 00:26:08,879
attack transferability

766
00:26:06,159 --> 00:26:09,200
the phenomenon that's been identified is

767
00:26:08,880 --> 00:26:11,840
if

768
00:26:09,200 --> 00:26:12,799
an attack works on one specific model

769
00:26:11,840 --> 00:26:15,039
then it'll work

770
00:26:12,799 --> 00:26:16,400
on many other types of models it doesn't

771
00:26:15,039 --> 00:26:18,480
even have to be made with the same

772
00:26:16,400 --> 00:26:21,760
algorithm or the same data

773
00:26:18,480 --> 00:26:22,799
it's the same type it's the same type of

774
00:26:21,760 --> 00:26:24,559
features

775
00:26:22,799 --> 00:26:25,840
that really kind of messes with the

776
00:26:24,559 --> 00:26:27,279
model

777
00:26:25,840 --> 00:26:30,240
and we'll see why this is important a

778
00:26:27,279 --> 00:26:30,240
little later on

779
00:26:31,440 --> 00:26:34,720
so before we can begin attacking an ai

780
00:26:34,240 --> 00:26:36,400
model

781
00:26:34,720 --> 00:26:38,159
first or a machine learning model first

782
00:26:36,400 --> 00:26:38,960
we need to understand what is a

783
00:26:38,159 --> 00:26:41,600
realistic

784
00:26:38,960 --> 00:26:42,400
attack surface now i'm sure that you've

785
00:26:41,600 --> 00:26:44,879
seen

786
00:26:42,400 --> 00:26:45,520
image of a panda you manipulate some

787
00:26:44,880 --> 00:26:48,640
pixels

788
00:26:45,520 --> 00:26:51,360
and then it identifies it as a bird

789
00:26:48,640 --> 00:26:52,720
that's not really realistic because the

790
00:26:51,360 --> 00:26:55,600
only way that you can

791
00:26:52,720 --> 00:26:57,200
attack that panda image and modify the

792
00:26:55,600 --> 00:26:59,840
pixels is if you have some sort of

793
00:26:57,200 --> 00:27:01,840
physical access to the model itself

794
00:26:59,840 --> 00:27:03,760
and at that point why modify pixels in a

795
00:27:01,840 --> 00:27:07,279
really convoluted way when you can just

796
00:27:03,760 --> 00:27:10,158
change them so it is a picture of a bird

797
00:27:07,279 --> 00:27:11,919
it's a little weird so here's the

798
00:27:10,159 --> 00:27:13,520
generic machine learning model and the

799
00:27:11,919 --> 00:27:16,400
process that it goes through

800
00:27:13,520 --> 00:27:17,600
and then as an example so that it makes

801
00:27:16,400 --> 00:27:18,880
a lot more sense to

802
00:27:17,600 --> 00:27:20,719
those who aren't familiar with machine

803
00:27:18,880 --> 00:27:22,399
learning i have a network intrusion

804
00:27:20,720 --> 00:27:24,399
detection system

805
00:27:22,399 --> 00:27:27,360
built on machine learning to kind of

806
00:27:24,399 --> 00:27:29,918
illustrate what different points

807
00:27:27,360 --> 00:27:31,600
what means what at different points so

808
00:27:29,919 --> 00:27:33,440
here we have a physical object which is

809
00:27:31,600 --> 00:27:35,760
the observed event

810
00:27:33,440 --> 00:27:37,679
so this could be a road sign this could

811
00:27:35,760 --> 00:27:38,799
be an image of a road sign i should say

812
00:27:37,679 --> 00:27:41,039
this should be

813
00:27:38,799 --> 00:27:42,639
distance to an object in our network

814
00:27:41,039 --> 00:27:43,120
intrusion detection system we have the

815
00:27:42,640 --> 00:27:45,440
attack

816
00:27:43,120 --> 00:27:46,959
traffic itself then what we're going to

817
00:27:45,440 --> 00:27:48,480
do is digitize

818
00:27:46,960 --> 00:27:49,919
that information and we're going to

819
00:27:48,480 --> 00:27:51,279
transform it into something that the

820
00:27:49,919 --> 00:27:52,640
model can use

821
00:27:51,279 --> 00:27:54,480
so we're going to take that image we're

822
00:27:52,640 --> 00:27:55,919
going to transform it into bytes or

823
00:27:54,480 --> 00:27:57,440
we're going to take that attack traffic

824
00:27:55,919 --> 00:27:58,640
and we're going to transform it into a

825
00:27:57,440 --> 00:28:00,960
pcap using

826
00:27:58,640 --> 00:28:02,480
tcp dump or t-sharp or something of that

827
00:28:00,960 --> 00:28:05,360
nature

828
00:28:02,480 --> 00:28:06,399
and then we can take that pull the

829
00:28:05,360 --> 00:28:07,918
features out of it

830
00:28:06,399 --> 00:28:09,439
and put it into our machine learning

831
00:28:07,919 --> 00:28:11,039
model now

832
00:28:09,440 --> 00:28:12,880
our machine learning model is kind of a

833
00:28:11,039 --> 00:28:13,520
box in and of itself it takes input

834
00:28:12,880 --> 00:28:16,559
features

835
00:28:13,520 --> 00:28:17,679
and it provides an output so that output

836
00:28:16,559 --> 00:28:20,799
is always going to be

837
00:28:17,679 --> 00:28:22,159
some sort of score or some sort of scale

838
00:28:20,799 --> 00:28:23,840
and so in this case we're going to have

839
00:28:22,159 --> 00:28:26,399
our packet metadata

840
00:28:23,840 --> 00:28:28,240
and then it's going to output our attack

841
00:28:26,399 --> 00:28:30,239
probability what is the probability that

842
00:28:28,240 --> 00:28:32,480
what we're looking at right now is a

843
00:28:30,240 --> 00:28:34,480
network attack

844
00:28:32,480 --> 00:28:35,520
and then at that point it goes on to the

845
00:28:34,480 --> 00:28:37,760
rest of the software

846
00:28:35,520 --> 00:28:39,760
that makes a decision it says okay if

847
00:28:37,760 --> 00:28:41,919
you reach this threshold

848
00:28:39,760 --> 00:28:43,200
anything above it is attack traffic

849
00:28:41,919 --> 00:28:46,480
anything below it

850
00:28:43,200 --> 00:28:48,960
is benign and that's just a binary

851
00:28:46,480 --> 00:28:50,320
classifier a one or a two

852
00:28:48,960 --> 00:28:52,480
there are many other types of

853
00:28:50,320 --> 00:28:54,480
classifiers for example object detection

854
00:28:52,480 --> 00:28:56,640
i know imagenet can detect i believe

855
00:28:54,480 --> 00:28:58,399
it's 200 different types of images is it

856
00:28:56,640 --> 00:28:59,520
a dog is it a cat is it a plane is it a

857
00:28:58,399 --> 00:29:02,719
train

858
00:28:59,520 --> 00:29:06,158
uh that's all based on this decision

859
00:29:02,720 --> 00:29:07,840
piece so let's see how this works in

860
00:29:06,159 --> 00:29:09,520
practice

861
00:29:07,840 --> 00:29:12,080
first we're going to talk about model

862
00:29:09,520 --> 00:29:14,639
evasion in model evasion uh

863
00:29:12,080 --> 00:29:16,799
or i should explain this diagram first

864
00:29:14,640 --> 00:29:18,080
we have a massive amount of theoretical

865
00:29:16,799 --> 00:29:20,960
space this is all the

866
00:29:18,080 --> 00:29:21,360
possible objects in the world but we

867
00:29:20,960 --> 00:29:23,039
can't

868
00:29:21,360 --> 00:29:24,879
look at all the possible objects in the

869
00:29:23,039 --> 00:29:26,240
world so what we do is we collect as

870
00:29:24,880 --> 00:29:28,880
much data as we can

871
00:29:26,240 --> 00:29:29,520
and this is our training and testing

872
00:29:28,880 --> 00:29:30,960
data

873
00:29:29,520 --> 00:29:32,879
so we have all the data that we have

874
00:29:30,960 --> 00:29:35,919
it's clean it's labeled

875
00:29:32,880 --> 00:29:37,679
it's clear and so we split it up usually

876
00:29:35,919 --> 00:29:41,279
it's 80 20 or

877
00:29:37,679 --> 00:29:44,159
70 30 where the model is trained on

878
00:29:41,279 --> 00:29:45,120
80 or 70 of the data and then it's

879
00:29:44,159 --> 00:29:48,320
tested on

880
00:29:45,120 --> 00:29:49,279
that 20 or 30 that's left over so the

881
00:29:48,320 --> 00:29:51,918
reason we do this

882
00:29:49,279 --> 00:29:54,000
is the same reason uh the same way

883
00:29:51,919 --> 00:29:56,640
actually that we teach students in

884
00:29:54,000 --> 00:29:57,600
elementary school right we teach them as

885
00:29:56,640 --> 00:29:59,919
much as we can

886
00:29:57,600 --> 00:30:01,678
on different mathematical principles and

887
00:29:59,919 --> 00:30:02,799
we use many different examples of what

888
00:30:01,679 --> 00:30:04,559
math looks like

889
00:30:02,799 --> 00:30:07,679
and then we test them to see if they

890
00:30:04,559 --> 00:30:10,080
actually understand what's going on

891
00:30:07,679 --> 00:30:12,399
so here's where we get into our

892
00:30:10,080 --> 00:30:14,399
adversarial space adversarial space is

893
00:30:12,399 --> 00:30:16,799
where we as attackers

894
00:30:14,399 --> 00:30:18,158
are actually able to try and hide and

895
00:30:16,799 --> 00:30:20,158
blend in

896
00:30:18,159 --> 00:30:21,600
so this is outside of their training and

897
00:30:20,159 --> 00:30:24,000
testing data but we need to have some

898
00:30:21,600 --> 00:30:26,320
sort of knowledge of what they used

899
00:30:24,000 --> 00:30:28,000
in order to perform some sort of

900
00:30:26,320 --> 00:30:29,840
malicious action

901
00:30:28,000 --> 00:30:31,120
so for model evasion our adversarial

902
00:30:29,840 --> 00:30:34,080
space is also kind of

903
00:30:31,120 --> 00:30:34,639
technically part of our testing space

904
00:30:34,080 --> 00:30:37,760
where

905
00:30:34,640 --> 00:30:39,039
we try to hide in the blind spots so we

906
00:30:37,760 --> 00:30:39,760
try and make the things that it's

907
00:30:39,039 --> 00:30:42,640
looking at

908
00:30:39,760 --> 00:30:43,600
as benign but the things that it's not

909
00:30:42,640 --> 00:30:48,000
looking at

910
00:30:43,600 --> 00:30:50,240
as uh malicious that'll get through

911
00:30:48,000 --> 00:30:51,279
so a perfect example of this would be a

912
00:30:50,240 --> 00:30:52,880
naive bayesian

913
00:30:51,279 --> 00:30:54,640
spam filter and if you want more

914
00:30:52,880 --> 00:30:57,120
information on how

915
00:30:54,640 --> 00:30:58,320
a spam filter like this works or how the

916
00:30:57,120 --> 00:31:00,320
machine learning process

917
00:30:58,320 --> 00:31:02,559
works take a look at my workshop that's

918
00:31:00,320 --> 00:31:05,840
going on later today

919
00:31:02,559 --> 00:31:07,360
so how do we defeat a naive bayesian

920
00:31:05,840 --> 00:31:11,678
spam filter just any

921
00:31:07,360 --> 00:31:15,439
uh regular machine learning spam filter

922
00:31:11,679 --> 00:31:18,960
well here we have an email that it flags

923
00:31:15,440 --> 00:31:21,039
on certain keywords so the probability

924
00:31:18,960 --> 00:31:23,440
of a regular email having the word

925
00:31:21,039 --> 00:31:26,080
viagra is pretty low but the probability

926
00:31:23,440 --> 00:31:27,039
that a spam email has viagra is pretty

927
00:31:26,080 --> 00:31:29,360
high

928
00:31:27,039 --> 00:31:31,279
and so it looks at all the words it

929
00:31:29,360 --> 00:31:32,080
scores all those words and says okay

930
00:31:31,279 --> 00:31:34,080
here's a bundle

931
00:31:32,080 --> 00:31:35,760
of potentially bad words here's a bundle

932
00:31:34,080 --> 00:31:37,918
of potentially good words

933
00:31:35,760 --> 00:31:39,120
if we add them all together then we can

934
00:31:37,919 --> 00:31:41,519
see where we fit

935
00:31:39,120 --> 00:31:42,479
on this little scale so in this case

936
00:31:41,519 --> 00:31:46,240
here's an email

937
00:31:42,480 --> 00:31:49,600
that the spam filter is 99 sure

938
00:31:46,240 --> 00:31:51,120
is spam so it throws it out how can we

939
00:31:49,600 --> 00:31:53,360
evade it

940
00:31:51,120 --> 00:31:54,719
well one clever way of doing this is

941
00:31:53,360 --> 00:31:56,080
looking at the way that emails are

942
00:31:54,720 --> 00:31:59,519
constructed

943
00:31:56,080 --> 00:32:02,720
emails are typically html files

944
00:31:59,519 --> 00:32:04,399
now a typical spam filter or a

945
00:32:02,720 --> 00:32:07,600
regular spam filter and this is exactly

946
00:32:04,399 --> 00:32:09,279
what they used to use back in the day

947
00:32:07,600 --> 00:32:10,959
only looks at the keywords it can't

948
00:32:09,279 --> 00:32:12,880
determine whether or not it's a word

949
00:32:10,960 --> 00:32:15,600
that's being shown to the user

950
00:32:12,880 --> 00:32:16,559
so what we can do is go to wikipedia

951
00:32:15,600 --> 00:32:19,678
grab

952
00:32:16,559 --> 00:32:23,519
the entire article for a horse and

953
00:32:19,679 --> 00:32:25,440
put that in as a html comment

954
00:32:23,519 --> 00:32:27,360
so to the user's perspective it doesn't

955
00:32:25,440 --> 00:32:28,000
see any difference in the spam email or

956
00:32:27,360 --> 00:32:30,559
in the regular

957
00:32:28,000 --> 00:32:31,840
email but from a machine learning

958
00:32:30,559 --> 00:32:33,840
perspective it takes

959
00:32:31,840 --> 00:32:35,918
all of the words from that wikipedia

960
00:32:33,840 --> 00:32:38,000
article into account

961
00:32:35,919 --> 00:32:40,000
and it says hey you have a lot more good

962
00:32:38,000 --> 00:32:43,360
boy points than bad boy points

963
00:32:40,000 --> 00:32:46,240
you get to go right on through so that's

964
00:32:43,360 --> 00:32:49,918
an example of model evasion in action

965
00:32:46,240 --> 00:32:52,080
let's see what more we can do with it

966
00:32:49,919 --> 00:32:53,919
so the researchers over at skylight

967
00:32:52,080 --> 00:32:54,639
saber actually did something very

968
00:32:53,919 --> 00:32:57,360
similar

969
00:32:54,640 --> 00:32:58,799
with the silent antivirus and this

970
00:32:57,360 --> 00:32:59,678
wonderful article i highly recommend

971
00:32:58,799 --> 00:33:02,000
checking out called

972
00:32:59,679 --> 00:33:02,960
silent i kill you where what they were

973
00:33:02,000 --> 00:33:06,320
able to do is

974
00:33:02,960 --> 00:33:09,679
identify different features that

975
00:33:06,320 --> 00:33:10,000
the the the silence antivirus was able

976
00:33:09,679 --> 00:33:12,880
to look

977
00:33:10,000 --> 00:33:14,799
at and perform on they did a little bit

978
00:33:12,880 --> 00:33:18,159
of reverse engineering and they found

979
00:33:14,799 --> 00:33:22,080
that there are certain uh applications

980
00:33:18,159 --> 00:33:24,240
that are white listed inside of silence

981
00:33:22,080 --> 00:33:26,320
so in this case they looked at online

982
00:33:24,240 --> 00:33:28,960
video games

983
00:33:26,320 --> 00:33:30,080
so online video games they found certain

984
00:33:28,960 --> 00:33:32,320
things were whitelisted

985
00:33:30,080 --> 00:33:33,279
so they just grabbed as many as they

986
00:33:32,320 --> 00:33:35,519
could and

987
00:33:33,279 --> 00:33:37,679
put those into their antivirus or into

988
00:33:35,519 --> 00:33:40,320
their malware samples

989
00:33:37,679 --> 00:33:41,279
so here we can see the score before

990
00:33:40,320 --> 00:33:44,240
making the change

991
00:33:41,279 --> 00:33:44,640
and the scores after making the change

992
00:33:44,240 --> 00:33:46,320
so

993
00:33:44,640 --> 00:33:48,480
the one that uh the one that entertains

994
00:33:46,320 --> 00:33:52,559
me the most is actually the zeus

995
00:33:48,480 --> 00:33:55,360
malware where it is at negative 997

996
00:33:52,559 --> 00:33:56,720
one of the worst malware that it was

997
00:33:55,360 --> 00:33:58,240
able to identify

998
00:33:56,720 --> 00:34:00,399
and then after padding it with a whole

999
00:33:58,240 --> 00:34:03,519
bunch of good boy points

1000
00:34:00,399 --> 00:34:05,199
it now became 997 positive which is one

1001
00:34:03,519 --> 00:34:06,960
of the best

1002
00:34:05,200 --> 00:34:08,879
examples that cy lance was able to

1003
00:34:06,960 --> 00:34:11,440
identify

1004
00:34:08,879 --> 00:34:12,560
so now that they're actually able to

1005
00:34:11,440 --> 00:34:15,679
disclose

1006
00:34:12,560 --> 00:34:17,119
what what game it was it actually turns

1007
00:34:15,679 --> 00:34:20,159
out it was rocket league

1008
00:34:17,119 --> 00:34:22,079
so they took words or features sets

1009
00:34:20,159 --> 00:34:23,440
from rocket league and put them into

1010
00:34:22,079 --> 00:34:24,800
their malware samples

1011
00:34:23,440 --> 00:34:28,000
but they also tried other games like

1012
00:34:24,800 --> 00:34:28,000
fortnite league of legends

1013
00:34:28,879 --> 00:34:34,319
so how do we prevent model evasion well

1014
00:34:32,000 --> 00:34:36,239
there's a few different ways

1015
00:34:34,320 --> 00:34:37,760
what we can do is actually perform

1016
00:34:36,239 --> 00:34:40,078
adversarial training

1017
00:34:37,760 --> 00:34:40,800
so the idea is that we know we're going

1018
00:34:40,079 --> 00:34:42,879
to come across

1019
00:34:40,800 --> 00:34:44,720
people who are going to try and

1020
00:34:42,879 --> 00:34:47,040
manipulate our model

1021
00:34:44,719 --> 00:34:49,118
so what if we put those kinds of

1022
00:34:47,040 --> 00:34:51,520
manipulations in our training set

1023
00:34:49,119 --> 00:34:52,879
so that we can identify them this kind

1024
00:34:51,520 --> 00:34:55,040
of runs on the same principle

1025
00:34:52,879 --> 00:34:57,118
as what is called chaos engineering or

1026
00:34:55,040 --> 00:34:59,040
engineering chaos

1027
00:34:57,119 --> 00:35:00,800
which is hey we know that we have a

1028
00:34:59,040 --> 00:35:02,160
network our server is going to go down

1029
00:35:00,800 --> 00:35:03,680
at some point where something's going to

1030
00:35:02,160 --> 00:35:05,920
happen hard drive is going to die

1031
00:35:03,680 --> 00:35:07,118
power's going to go out how do we

1032
00:35:05,920 --> 00:35:10,079
account for that

1033
00:35:07,119 --> 00:35:10,640
so engineering chaos what they'll do is

1034
00:35:10,079 --> 00:35:13,440
they'll have

1035
00:35:10,640 --> 00:35:14,879
a cso or somebody of that nature walk

1036
00:35:13,440 --> 00:35:16,480
into the data center and just start

1037
00:35:14,880 --> 00:35:18,560
unplugging servers

1038
00:35:16,480 --> 00:35:19,680
right we know that there's a data we

1039
00:35:18,560 --> 00:35:20,560
know that there's going to be an outage

1040
00:35:19,680 --> 00:35:22,240
at some point

1041
00:35:20,560 --> 00:35:24,320
how do we handle that how do we protect

1042
00:35:22,240 --> 00:35:25,759
against it so if you go in start

1043
00:35:24,320 --> 00:35:27,040
unplugging servers then what you're

1044
00:35:25,760 --> 00:35:28,480
going to do is start to develop

1045
00:35:27,040 --> 00:35:30,800
processes and policies

1046
00:35:28,480 --> 00:35:32,240
around those potential out those

1047
00:35:30,800 --> 00:35:34,400
potential outages

1048
00:35:32,240 --> 00:35:35,439
so it's not a matter of if it's a matter

1049
00:35:34,400 --> 00:35:37,280
of when and

1050
00:35:35,440 --> 00:35:38,880
you know what types of things are going

1051
00:35:37,280 --> 00:35:41,440
to happen

1052
00:35:38,880 --> 00:35:43,200
so you begin to train your model on

1053
00:35:41,440 --> 00:35:45,359
adversarial examples or you begin to

1054
00:35:43,200 --> 00:35:49,359
train your data center or your processes

1055
00:35:45,359 --> 00:35:49,839
on malicious insider or power outages

1056
00:35:49,359 --> 00:35:54,000
and

1057
00:35:49,839 --> 00:35:57,680
other unexpected things another way to

1058
00:35:54,000 --> 00:35:58,480
prevent model evasion is what's called

1059
00:35:57,680 --> 00:36:01,118
defensive

1060
00:35:58,480 --> 00:36:02,640
distillation so defensive distillation

1061
00:36:01,119 --> 00:36:06,160
is just really a fancy term

1062
00:36:02,640 --> 00:36:09,598
of smoothing a curve

1063
00:36:06,160 --> 00:36:11,598
so in this case instead of

1064
00:36:09,599 --> 00:36:12,800
the model looking at the email right the

1065
00:36:11,599 --> 00:36:14,160
email example that we had

1066
00:36:12,800 --> 00:36:15,599
instead of looking at the email and

1067
00:36:14,160 --> 00:36:16,799
saying okay there's a bunch of bad point

1068
00:36:15,599 --> 00:36:18,240
points bad boy points

1069
00:36:16,800 --> 00:36:20,320
oh here's a bunch of good boy points and

1070
00:36:18,240 --> 00:36:22,399
it goes straight down what it does is it

1071
00:36:20,320 --> 00:36:26,079
smooths the curve so it can't do

1072
00:36:22,400 --> 00:36:27,760
a radical change so that

1073
00:36:26,079 --> 00:36:29,680
isn't entirely foolproof but it

1074
00:36:27,760 --> 00:36:31,119
definitely helps make the model a lot

1075
00:36:29,680 --> 00:36:34,240
more resilient

1076
00:36:31,119 --> 00:36:34,960
against adversarial examples and then

1077
00:36:34,240 --> 00:36:37,359
the last one

1078
00:36:34,960 --> 00:36:38,079
is called uh monotonic classification

1079
00:36:37,359 --> 00:36:40,640
monotonic

1080
00:36:38,079 --> 00:36:43,040
also another fancy term for the graph

1081
00:36:40,640 --> 00:36:46,240
only grows in one direction

1082
00:36:43,040 --> 00:36:46,880
so in this case we see a non-monotonic

1083
00:36:46,240 --> 00:36:48,959
example

1084
00:36:46,880 --> 00:36:50,880
where it'll dip down below the decision

1085
00:36:48,960 --> 00:36:51,359
boundary it'll dip back up as soon as it

1086
00:36:50,880 --> 00:36:54,560
sees

1087
00:36:51,359 --> 00:36:56,160
all of the the words in the article for

1088
00:36:54,560 --> 00:36:58,480
a horse

1089
00:36:56,160 --> 00:37:00,240
and then in the monotonic example we see

1090
00:36:58,480 --> 00:37:01,920
that it only goes in one direction

1091
00:37:00,240 --> 00:37:03,839
it starts accumulating bad boy points

1092
00:37:01,920 --> 00:37:05,599
and once it hits a certain threshold

1093
00:37:03,839 --> 00:37:08,799
it doesn't matter what you do jesus

1094
00:37:05,599 --> 00:37:13,440
won't save you you're bad

1095
00:37:08,800 --> 00:37:13,440
so those are some evasion defenses

1096
00:37:13,520 --> 00:37:17,119
next we have model poisoning this is

1097
00:37:16,720 --> 00:37:18,879
where

1098
00:37:17,119 --> 00:37:21,280
an adversary is actually able to

1099
00:37:18,880 --> 00:37:23,200
influence the model itself

1100
00:37:21,280 --> 00:37:24,800
now this is really unique because it

1101
00:37:23,200 --> 00:37:27,919
almost exclusively

1102
00:37:24,800 --> 00:37:30,480
works on what are called online learners

1103
00:37:27,920 --> 00:37:32,880
machine learning models stop learning as

1104
00:37:30,480 --> 00:37:36,320
soon as they're done being trained

1105
00:37:32,880 --> 00:37:39,520
to work on that engineers have developed

1106
00:37:36,320 --> 00:37:42,720
online models where they will classify

1107
00:37:39,520 --> 00:37:43,680
images or data points objects data

1108
00:37:42,720 --> 00:37:46,640
objects

1109
00:37:43,680 --> 00:37:47,919
uh as benign malicious or whatever

1110
00:37:46,640 --> 00:37:49,118
different classes they have and then

1111
00:37:47,920 --> 00:37:51,760
they'll use the data

1112
00:37:49,119 --> 00:37:52,720
from the stuff that it had classified to

1113
00:37:51,760 --> 00:37:55,440
retrain

1114
00:37:52,720 --> 00:37:56,078
itself so a really good example of this

1115
00:37:55,440 --> 00:37:58,079
would be

1116
00:37:56,079 --> 00:38:00,800
network intrusion detection where it

1117
00:37:58,079 --> 00:38:02,560
needs to learn how your network operates

1118
00:38:00,800 --> 00:38:04,960
what is baseline what is stuff that's

1119
00:38:02,560 --> 00:38:05,680
normally seen so for example in company

1120
00:38:04,960 --> 00:38:08,560
a

1121
00:38:05,680 --> 00:38:09,680
telnet might be something normal it's

1122
00:38:08,560 --> 00:38:10,400
probably something the security team

1123
00:38:09,680 --> 00:38:12,399
doesn't like

1124
00:38:10,400 --> 00:38:14,240
and they want to get rid of but for the

1125
00:38:12,400 --> 00:38:16,960
time being it's normal

1126
00:38:14,240 --> 00:38:18,560
but in company b telnet traffic is

1127
00:38:16,960 --> 00:38:21,839
immediately a red flag

1128
00:38:18,560 --> 00:38:22,720
because it's all clear text so for an

1129
00:38:21,839 --> 00:38:25,359
online learner

1130
00:38:22,720 --> 00:38:27,759
what we want to do is add adversarial

1131
00:38:25,359 --> 00:38:29,359
examples into the adversarial space and

1132
00:38:27,760 --> 00:38:32,000
then move that information

1133
00:38:29,359 --> 00:38:33,920
into the training space so the model is

1134
00:38:32,000 --> 00:38:37,040
slowly moving the line

1135
00:38:33,920 --> 00:38:37,040
to what is acceptable

1136
00:38:37,839 --> 00:38:41,279
so a couple examples of model poisoning

1137
00:38:40,320 --> 00:38:45,280
would be

1138
00:38:41,280 --> 00:38:46,000
the microsoft tay ai tai if you're not

1139
00:38:45,280 --> 00:38:48,880
familiar

1140
00:38:46,000 --> 00:38:49,280
was a microsoft twitter chat bot and i

1141
00:38:48,880 --> 00:38:52,960
use

1142
00:38:49,280 --> 00:38:55,119
was because people over at 4chan

1143
00:38:52,960 --> 00:38:56,160
decided that they wanted to try and take

1144
00:38:55,119 --> 00:38:58,480
it down

1145
00:38:56,160 --> 00:39:00,160
tai was pitched as a twitter bot that

1146
00:38:58,480 --> 00:39:01,200
will learn from conversations that it

1147
00:39:00,160 --> 00:39:02,879
has with people

1148
00:39:01,200 --> 00:39:05,040
and it becomes more intelligent over

1149
00:39:02,880 --> 00:39:06,560
time so the people over at 4chan we're

1150
00:39:05,040 --> 00:39:09,680
trying to decide

1151
00:39:06,560 --> 00:39:11,359
what's the worst that we can do

1152
00:39:09,680 --> 00:39:13,520
it took only about 16 hours before

1153
00:39:11,359 --> 00:39:15,920
microsoft permanently shut it down

1154
00:39:13,520 --> 00:39:17,759
which is terrifying because imagine how

1155
00:39:15,920 --> 00:39:20,079
many engineers how many

1156
00:39:17,760 --> 00:39:21,760
uh software developers were behind this

1157
00:39:20,079 --> 00:39:25,440
in order to develop the system

1158
00:39:21,760 --> 00:39:26,880
that is now just permanently shut down

1159
00:39:25,440 --> 00:39:29,040
so the way they were able to do that was

1160
00:39:26,880 --> 00:39:32,320
just uh treat it

1161
00:39:29,040 --> 00:39:34,079
with some conversation pieces and it

1162
00:39:32,320 --> 00:39:36,000
would learn over time

1163
00:39:34,079 --> 00:39:37,760
what was acceptable and what was not so

1164
00:39:36,000 --> 00:39:39,359
it only took about 16 hours before it

1165
00:39:37,760 --> 00:39:42,560
started saying things like

1166
00:39:39,359 --> 00:39:46,480
hitler did nothing wrong or

1167
00:39:42,560 --> 00:39:49,680
other racial slurs another form

1168
00:39:46,480 --> 00:39:53,119
is a jacobian map saliency attack or

1169
00:39:49,680 --> 00:39:56,399
jmsa and this is where we kind of modify

1170
00:39:53,119 --> 00:39:58,079
certain pixels so it's very technical

1171
00:39:56,400 --> 00:40:00,240
and academic and not something that i

1172
00:39:58,079 --> 00:40:03,200
would really consider realistic

1173
00:40:00,240 --> 00:40:05,040
uh in a real world setting but

1174
00:40:03,200 --> 00:40:07,359
definitely something to be aware of

1175
00:40:05,040 --> 00:40:10,240
so in this example we have a seven eight

1176
00:40:07,359 --> 00:40:11,839
kilometer per hour sign

1177
00:40:10,240 --> 00:40:14,319
change some pixels and then it drops

1178
00:40:11,839 --> 00:40:17,279
down to 30 kilometers per hour

1179
00:40:14,319 --> 00:40:18,400
so we may be able to influence a

1180
00:40:17,280 --> 00:40:20,960
self-driving car

1181
00:40:18,400 --> 00:40:21,920
by being able to change some of this

1182
00:40:20,960 --> 00:40:24,400
stuff

1183
00:40:21,920 --> 00:40:25,680
but remember what i said it's almost

1184
00:40:24,400 --> 00:40:27,839
exclusively

1185
00:40:25,680 --> 00:40:29,118
for online learners and self-driving

1186
00:40:27,839 --> 00:40:32,240
cars don't really do

1187
00:40:29,119 --> 00:40:35,200
online learning that much so just some

1188
00:40:32,240 --> 00:40:37,680
examples to be aware of

1189
00:40:35,200 --> 00:40:38,319
so how do we prevent model poisoning

1190
00:40:37,680 --> 00:40:40,560
well

1191
00:40:38,319 --> 00:40:41,599
there's a few different ways we can

1192
00:40:40,560 --> 00:40:44,560
train the model

1193
00:40:41,599 --> 00:40:46,079
for uh longer periods of time or have

1194
00:40:44,560 --> 00:40:47,040
longer periods of time between the

1195
00:40:46,079 --> 00:40:49,359
trainings

1196
00:40:47,040 --> 00:40:52,000
so what that'll do is allow the model to

1197
00:40:49,359 --> 00:40:54,078
accumulate a lot more examples

1198
00:40:52,000 --> 00:40:55,520
that it can be used to train the next

1199
00:40:54,079 --> 00:40:57,839
version of that model

1200
00:40:55,520 --> 00:41:00,079
so it makes it a lot harder for the

1201
00:40:57,839 --> 00:41:00,880
adversaries to influence enough of the

1202
00:41:00,079 --> 00:41:03,839
data

1203
00:41:00,880 --> 00:41:05,680
for it to be statistically significant

1204
00:41:03,839 --> 00:41:06,880
another way that we can do is analyzing

1205
00:41:05,680 --> 00:41:09,040
longer periods of data

1206
00:41:06,880 --> 00:41:10,319
it's kind of the same thing and then we

1207
00:41:09,040 --> 00:41:12,240
can just minimize the impact of

1208
00:41:10,319 --> 00:41:14,560
adversarial training examples

1209
00:41:12,240 --> 00:41:16,240
by incorporating our own adversarial

1210
00:41:14,560 --> 00:41:18,880
training examples or

1211
00:41:16,240 --> 00:41:20,000
limiting what the adversaries are able

1212
00:41:18,880 --> 00:41:22,319
to get through or

1213
00:41:20,000 --> 00:41:23,839
what data points that the model

1214
00:41:22,319 --> 00:41:25,520
collected

1215
00:41:23,839 --> 00:41:28,400
how many of those actually go into

1216
00:41:25,520 --> 00:41:28,400
retraining the model

1217
00:41:29,839 --> 00:41:33,359
so the last one we have here is data

1218
00:41:32,240 --> 00:41:35,759
leakage

1219
00:41:33,359 --> 00:41:37,598
data leakage i mentioned earlier can be

1220
00:41:35,760 --> 00:41:39,200
really devastating if you're creating a

1221
00:41:37,599 --> 00:41:42,319
top secret model or if you're

1222
00:41:39,200 --> 00:41:44,799
creating a model that is built on

1223
00:41:42,319 --> 00:41:46,800
personal health care data or phi or

1224
00:41:44,800 --> 00:41:48,400
hipaa compliant data

1225
00:41:46,800 --> 00:41:50,800
also privacy especially if you're

1226
00:41:48,400 --> 00:41:54,640
concerned with gdpr

1227
00:41:50,800 --> 00:41:57,119
so the idea here is models being trained

1228
00:41:54,640 --> 00:41:58,879
if they are what's called overfit will

1229
00:41:57,119 --> 00:42:01,359
store some of the information or it'll

1230
00:41:58,880 --> 00:42:03,680
memorize some of the information

1231
00:42:01,359 --> 00:42:05,440
so in this example the headlines were

1232
00:42:03,680 --> 00:42:08,640
saying oh my god this google ai

1233
00:42:05,440 --> 00:42:10,400
it memorized the test and so it was able

1234
00:42:08,640 --> 00:42:13,040
to cheat

1235
00:42:10,400 --> 00:42:15,119
that's not really true this is a really

1236
00:42:13,040 --> 00:42:17,279
well known phenomenon called overfitting

1237
00:42:15,119 --> 00:42:21,040
overfitting is just the ai model

1238
00:42:17,280 --> 00:42:24,720
memorizes certain things instead of uh

1239
00:42:21,040 --> 00:42:25,040
generalizing so if two plus two is two

1240
00:42:24,720 --> 00:42:27,359
or

1241
00:42:25,040 --> 00:42:29,040
if two plus two is four every single

1242
00:42:27,359 --> 00:42:29,920
time and then you say okay well what's

1243
00:42:29,040 --> 00:42:32,480
one

1244
00:42:29,920 --> 00:42:33,839
eight plus five it'll say four because

1245
00:42:32,480 --> 00:42:35,599
it only memorized

1246
00:42:33,839 --> 00:42:37,359
certain answers it's a really poorly

1247
00:42:35,599 --> 00:42:38,160
performing model and part of a whole

1248
00:42:37,359 --> 00:42:40,880
other problem

1249
00:42:38,160 --> 00:42:42,399
but overfit models do go into production

1250
00:42:40,880 --> 00:42:44,240
sometimes

1251
00:42:42,400 --> 00:42:46,079
so usually we'll see this when models

1252
00:42:44,240 --> 00:42:48,319
are too good to be true

1253
00:42:46,079 --> 00:42:49,359
because it memorized the answers that it

1254
00:42:48,319 --> 00:42:51,200
needs to memorize

1255
00:42:49,359 --> 00:42:53,040
which means that it will usually perform

1256
00:42:51,200 --> 00:42:55,200
really badly when it's given a question

1257
00:42:53,040 --> 00:42:56,720
that it hasn't seen before

1258
00:42:55,200 --> 00:42:58,720
it could leak private or proprietary

1259
00:42:56,720 --> 00:43:00,799
data and then it could also allow

1260
00:42:58,720 --> 00:43:02,000
model theft by competitors as i was

1261
00:43:00,800 --> 00:43:04,960
saying with the

1262
00:43:02,000 --> 00:43:05,760
black box testing paradigm where you

1263
00:43:04,960 --> 00:43:07,520
take a model

1264
00:43:05,760 --> 00:43:10,079
you ask it a series of questions and

1265
00:43:07,520 --> 00:43:13,200
then you create your own surrogate model

1266
00:43:10,079 --> 00:43:16,880
it's not technically stealing but

1267
00:43:13,200 --> 00:43:20,399
it's a nearly identical model and so

1268
00:43:16,880 --> 00:43:20,400
that's stolen proprietary data

1269
00:43:21,440 --> 00:43:24,880
so one more thing i want to leave you

1270
00:43:23,680 --> 00:43:27,440
with is a

1271
00:43:24,880 --> 00:43:29,359
conversation around adversarial stickers

1272
00:43:27,440 --> 00:43:32,000
so adversarial stickers they look like

1273
00:43:29,359 --> 00:43:33,598
melted crayons in my opinion but they

1274
00:43:32,000 --> 00:43:35,680
were all the rage especially when

1275
00:43:33,599 --> 00:43:37,920
uh some of this research first came out

1276
00:43:35,680 --> 00:43:39,839
where uh headlines really grabbed on to

1277
00:43:37,920 --> 00:43:40,800
the idea of putting adversarial stickers

1278
00:43:39,839 --> 00:43:42,160
next to things

1279
00:43:40,800 --> 00:43:44,400
maybe you can put something on a stop

1280
00:43:42,160 --> 00:43:47,118
sign and it looks like a

1281
00:43:44,400 --> 00:43:50,640
guild sign or a crosswalk or a speed up

1282
00:43:47,119 --> 00:43:53,119
to 100 miles an hour sign

1283
00:43:50,640 --> 00:43:54,319
that's not really the case so

1284
00:43:53,119 --> 00:43:56,960
adversarial stickers

1285
00:43:54,319 --> 00:43:57,680
they look cool but they suffer from a

1286
00:43:56,960 --> 00:44:00,079
very

1287
00:43:57,680 --> 00:44:01,680
critical problem and that problem is

1288
00:44:00,079 --> 00:44:04,079
attack transferability

1289
00:44:01,680 --> 00:44:05,520
they're not transferable so if you had

1290
00:44:04,079 --> 00:44:08,560
two models that operated

1291
00:44:05,520 --> 00:44:09,520
nearly identically right so in this case

1292
00:44:08,560 --> 00:44:11,599
we have

1293
00:44:09,520 --> 00:44:13,520
an image classifier that can identify

1294
00:44:11,599 --> 00:44:15,920
bananas and other images

1295
00:44:13,520 --> 00:44:17,520
you can have one model that you create

1296
00:44:15,920 --> 00:44:19,280
an adversarial sticker for

1297
00:44:17,520 --> 00:44:21,920
put it next to a banana it thinks it's a

1298
00:44:19,280 --> 00:44:23,520
toaster the other model if you use that

1299
00:44:21,920 --> 00:44:25,359
same adversarial sticker

1300
00:44:23,520 --> 00:44:27,839
it may not trap it may not pick it up it

1301
00:44:25,359 --> 00:44:29,839
may not consider that a toaster it may

1302
00:44:27,839 --> 00:44:32,160
still consider it a banana with a really

1303
00:44:29,839 --> 00:44:34,799
weird thing next to it

1304
00:44:32,160 --> 00:44:35,598
so adversarial stickers they look cool

1305
00:44:34,800 --> 00:44:39,680
but

1306
00:44:35,599 --> 00:44:41,440
not realistic in a lot of ways

1307
00:44:39,680 --> 00:44:43,598
if you want more information on this i

1308
00:44:41,440 --> 00:44:46,880
highly recommend checking out the defcon

1309
00:44:43,599 --> 00:44:48,400
2018 talk by sven it's 30 minutes he

1310
00:44:46,880 --> 00:44:49,680
talks about adversarial stickers and the

1311
00:44:48,400 --> 00:44:52,480
problems with them

1312
00:44:49,680 --> 00:44:53,839
then i also recommend defcon 2019 by

1313
00:44:52,480 --> 00:44:55,200
rich haring

1314
00:44:53,839 --> 00:44:57,520
talking about defeating facial

1315
00:44:55,200 --> 00:44:59,359
recognition algorithms

1316
00:44:57,520 --> 00:45:01,200
so both of them kind of talk about this

1317
00:44:59,359 --> 00:45:03,920
attack transferability problem

1318
00:45:01,200 --> 00:45:06,879
and why even though it looks cool it's

1319
00:45:03,920 --> 00:45:06,880
not realistic

1320
00:45:07,280 --> 00:45:12,800
so to recap how has ai empower

1321
00:45:10,480 --> 00:45:14,720
adversaries or how will advancements in

1322
00:45:12,800 --> 00:45:16,480
ai empower adversaries

1323
00:45:14,720 --> 00:45:18,640
well it's going to let them operate at

1324
00:45:16,480 --> 00:45:20,960
the same speed and scale as machines

1325
00:45:18,640 --> 00:45:23,118
so now we're going to have ai defense

1326
00:45:20,960 --> 00:45:25,760
tools against ai attack tools

1327
00:45:23,119 --> 00:45:27,760
at some point in the near future and

1328
00:45:25,760 --> 00:45:29,200
this is also the very beginning of what

1329
00:45:27,760 --> 00:45:31,920
ai can bring

1330
00:45:29,200 --> 00:45:32,960
so you remember back to the 80s and 90s

1331
00:45:31,920 --> 00:45:35,359
where

1332
00:45:32,960 --> 00:45:36,000
anti-virus systems intrusion detections

1333
00:45:35,359 --> 00:45:38,960
they were all

1334
00:45:36,000 --> 00:45:40,319
basic scripts or very large but still

1335
00:45:38,960 --> 00:45:41,920
basic scripts

1336
00:45:40,319 --> 00:45:44,000
and now they've both turned into

1337
00:45:41,920 --> 00:45:46,480
multi-billion dollar industries

1338
00:45:44,000 --> 00:45:47,440
so i see ai and security being able to

1339
00:45:46,480 --> 00:45:50,240
do the same thing

1340
00:45:47,440 --> 00:45:52,480
for both the attackers and the defenders

1341
00:45:50,240 --> 00:45:55,759
and i think that we're on the very edge

1342
00:45:52,480 --> 00:45:57,359
of where this transition will start

1343
00:45:55,760 --> 00:45:58,880
we're also already starting to see some

1344
00:45:57,359 --> 00:46:00,240
of these adversarial attacks against

1345
00:45:58,880 --> 00:46:02,319
real world models

1346
00:46:00,240 --> 00:46:03,439
for example the self-driving cars that i

1347
00:46:02,319 --> 00:46:05,440
showed earlier

1348
00:46:03,440 --> 00:46:07,280
those are real world models that

1349
00:46:05,440 --> 00:46:11,599
researchers were able to

1350
00:46:07,280 --> 00:46:14,079
get to kind of trap themselves

1351
00:46:11,599 --> 00:46:16,079
the adversarial sticker that is a real

1352
00:46:14,079 --> 00:46:18,960
model but it's not something that can

1353
00:46:16,079 --> 00:46:19,520
work in the real world against other

1354
00:46:18,960 --> 00:46:22,560
models

1355
00:46:19,520 --> 00:46:24,720
because of attack transferability we saw

1356
00:46:22,560 --> 00:46:26,480
silence how silence could be bypassed

1357
00:46:24,720 --> 00:46:28,799
we saw how spam filters could be

1358
00:46:26,480 --> 00:46:30,400
bypassed so a lot of this is what we're

1359
00:46:28,800 --> 00:46:31,599
starting to see with researchers

1360
00:46:30,400 --> 00:46:34,880
and some of these are what we're

1361
00:46:31,599 --> 00:46:36,640
starting to see in the wild

1362
00:46:34,880 --> 00:46:37,920
anyways that's all i have for you guys

1363
00:46:36,640 --> 00:46:39,598
today if you

1364
00:46:37,920 --> 00:46:41,599
didn't have a chance to ask questions in

1365
00:46:39,599 --> 00:46:42,560
the discord server here's my contact

1366
00:46:41,599 --> 00:46:44,160
information

1367
00:46:42,560 --> 00:46:45,839
if you think of something later on go

1368
00:46:44,160 --> 00:46:48,240
ahead reach out to me

1369
00:46:45,839 --> 00:46:49,759
and of course check out my youtube video

1370
00:46:48,240 --> 00:46:54,640
net tech explained

1371
00:46:49,760 --> 00:46:56,400
and my slides are up on slideshare.net

1372
00:46:54,640 --> 00:46:57,839
so the slides unfortunately i wasn't

1373
00:46:56,400 --> 00:46:59,440
able to cover everything

1374
00:46:57,839 --> 00:47:01,680
but of the things that i wasn't able to

1375
00:46:59,440 --> 00:47:03,359
cover i have up there on the slides and

1376
00:47:01,680 --> 00:47:04,720
of course you can reach out to me and we

1377
00:47:03,359 --> 00:47:06,400
can talk about them

1378
00:47:04,720 --> 00:47:11,839
anyways thank you for your time and

1379
00:47:06,400 --> 00:47:11,839
attention and have a good day

1380
00:47:30,480 --> 00:47:32,559
you

