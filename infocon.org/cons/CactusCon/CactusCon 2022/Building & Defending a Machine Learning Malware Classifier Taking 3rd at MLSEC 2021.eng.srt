1
00:00:16,880 --> 00:00:18,400
all right how about now coming through

2
00:00:18,400 --> 00:00:19,199
okay

3
00:00:19,199 --> 00:00:21,279
thumbs up all right good deal all right

4
00:00:21,279 --> 00:00:22,880
thank you all for being here um my name

5
00:00:22,880 --> 00:00:24,720
is andy applebaum i am going to be

6
00:00:24,720 --> 00:00:26,880
presenting on building and defending a

7
00:00:26,880 --> 00:00:28,800
machine learning classifier

8
00:00:28,800 --> 00:00:30,560
this is really just my experiences in

9
00:00:30,560 --> 00:00:32,558
the machine learning security evasion

10
00:00:32,558 --> 00:00:35,600
competition in 2021 i took third and of

11
00:00:35,600 --> 00:00:37,200
course i have the

12
00:00:37,200 --> 00:00:40,320
meme up there because i had to

13
00:00:40,320 --> 00:00:42,320
um a quick outline

14
00:00:42,320 --> 00:00:44,719
just about me i am a security researcher

15
00:00:44,719 --> 00:00:46,800
i primarily focus on the intersection of

16
00:00:46,800 --> 00:00:49,280
security and ai i previously was with

17
00:00:49,280 --> 00:00:50,640
mitre where i did a lot of stuff with

18
00:00:50,640 --> 00:00:52,480
the attack framework caldera threatened

19
00:00:52,480 --> 00:00:54,559
form defense things like that

20
00:00:54,559 --> 00:00:57,039
personally i am a an avid chess player

21
00:00:57,039 --> 00:00:59,520
um i love playing chess and if you ever

22
00:00:59,520 --> 00:01:00,879
see me on my phone i'm probably just

23
00:01:00,879 --> 00:01:03,440
playing chess uh this presentation is as

24
00:01:03,440 --> 00:01:06,799
i said my experiences in the 2021 ml set

25
00:01:06,799 --> 00:01:08,240
up competition

26
00:01:08,240 --> 00:01:10,960
um it includes two segments uh building

27
00:01:10,960 --> 00:01:13,200
a classifier used in the sec in in the

28
00:01:13,200 --> 00:01:15,119
competition and also attacking other

29
00:01:15,119 --> 00:01:16,720
people's classifiers

30
00:01:16,720 --> 00:01:18,720
in the competition and it also includes

31
00:01:18,720 --> 00:01:20,000
a lot of like lessons learned and kind

32
00:01:20,000 --> 00:01:21,920
of side observations that hopefully for

33
00:01:21,920 --> 00:01:23,920
others who are looking to kind of get

34
00:01:23,920 --> 00:01:26,080
into machine learning and security might

35
00:01:26,080 --> 00:01:27,680
be useful

36
00:01:27,680 --> 00:01:30,400
this slide deck is also accompanied by

37
00:01:30,400 --> 00:01:32,159
some open source code it's available

38
00:01:32,159 --> 00:01:34,079
online really the defensive stuff i did

39
00:01:34,079 --> 00:01:36,320
the model i built the data all sorts of

40
00:01:36,320 --> 00:01:37,520
stuff to kind of hit the ground running

41
00:01:37,520 --> 00:01:38,960
if you wanted to run with this on your

42
00:01:38,960 --> 00:01:41,439
own as well as a white paper and another

43
00:01:41,439 --> 00:01:43,520
talk at a separate conference called

44
00:01:43,520 --> 00:01:45,920
camless where i focus primarily on the

45
00:01:45,920 --> 00:01:48,000
defensive aspect

46
00:01:48,000 --> 00:01:50,479
as one disclaimer this is was a side

47
00:01:50,479 --> 00:01:52,640
project for me it was fun it was solely

48
00:01:52,640 --> 00:01:54,560
a personal project so it was very much

49
00:01:54,560 --> 00:01:56,320
me saying hey i want to dive into

50
00:01:56,320 --> 00:01:57,840
something get better at it learn about

51
00:01:57,840 --> 00:01:59,200
it um

52
00:01:59,200 --> 00:02:02,000
so it it really is

53
00:02:02,000 --> 00:02:03,280
if it seems a little rough around the

54
00:02:03,280 --> 00:02:05,119
edges in some places

55
00:02:05,119 --> 00:02:07,119
that's my excuse

56
00:02:07,119 --> 00:02:08,800
as a big motivation for this talk there

57
00:02:08,800 --> 00:02:11,599
is the 2021 mls competition or machine

58
00:02:11,599 --> 00:02:14,080
learning security evasion competition

59
00:02:14,080 --> 00:02:15,920
this was a public competition that's

60
00:02:15,920 --> 00:02:18,400
been run i think

61
00:02:18,400 --> 00:02:22,480
three years so far 2021 2020 and 2019

62
00:02:22,480 --> 00:02:24,800
put on by a few companies on really

63
00:02:24,800 --> 00:02:26,959
trying to let people build and attack

64
00:02:26,959 --> 00:02:28,640
machine learning based malware

65
00:02:28,640 --> 00:02:30,080
classifiers

66
00:02:30,080 --> 00:02:31,280
and there's two tracks as part of the

67
00:02:31,280 --> 00:02:33,280
competition there's the defend track

68
00:02:33,280 --> 00:02:35,440
where you submit a classifier designed

69
00:02:35,440 --> 00:02:37,599
to detect malware and the attack track

70
00:02:37,599 --> 00:02:40,000
where they give you hey here here are 50

71
00:02:40,000 --> 00:02:42,239
malware samples make them evade

72
00:02:42,239 --> 00:02:44,319
everybody's classifiers

73
00:02:44,319 --> 00:02:46,400
and as a personal project my goal here

74
00:02:46,400 --> 00:02:48,160
was to just submit something to the

75
00:02:48,160 --> 00:02:49,920
defender track i thought it'd be fun it

76
00:02:49,920 --> 00:02:51,680
would be good to learn it would be i

77
00:02:51,680 --> 00:02:53,200
don't know a good experience and it

78
00:02:53,200 --> 00:02:55,040
didn't have to do well i just wanted it

79
00:02:55,040 --> 00:02:56,160
to get in

80
00:02:56,160 --> 00:02:57,920
just for the sake of saying like hey i

81
00:02:57,920 --> 00:02:59,440
did it

82
00:02:59,440 --> 00:03:01,120
so with that with that said i wanted to

83
00:03:01,120 --> 00:03:02,080
give a little bit of kind of

84
00:03:02,080 --> 00:03:04,239
introduction in background you know why

85
00:03:04,239 --> 00:03:06,319
is there such a very specific

86
00:03:06,319 --> 00:03:07,680
competition

87
00:03:07,680 --> 00:03:09,840
and i i think it almost goes entirely

88
00:03:09,840 --> 00:03:11,760
without saying that machine learning has

89
00:03:11,760 --> 00:03:14,000
made a lot of hard problems tractable

90
00:03:14,000 --> 00:03:15,440
between you know natural language

91
00:03:15,440 --> 00:03:17,519
processing computer vision game playing

92
00:03:17,519 --> 00:03:19,200
all sorts of stuff

93
00:03:19,200 --> 00:03:20,879
the advancements are kind of through the

94
00:03:20,879 --> 00:03:21,680
roof

95
00:03:21,680 --> 00:03:23,920
and this includes security this you know

96
00:03:23,920 --> 00:03:26,239
machine learning has been applied across

97
00:03:26,239 --> 00:03:28,159
a wide variety of domains you know

98
00:03:28,159 --> 00:03:29,680
malware classification guessing

99
00:03:29,680 --> 00:03:32,080
passwords intrusion detection and it's

100
00:03:32,080 --> 00:03:34,239
something that isn't just in academia

101
00:03:34,239 --> 00:03:35,840
it's an industry too

102
00:03:35,840 --> 00:03:38,159
um this is a an old chart from 2019 i've

103
00:03:38,159 --> 00:03:40,159
just i found some blog of like look at

104
00:03:40,159 --> 00:03:41,440
these companies they all use machine

105
00:03:41,440 --> 00:03:43,519
learning or they say to excuse me they

106
00:03:43,519 --> 00:03:45,760
say too

107
00:03:45,760 --> 00:03:47,599
probably even more now

108
00:03:47,599 --> 00:03:49,599
and really at the end of the day most

109
00:03:49,599 --> 00:03:51,200
security nowadays you know there's

110
00:03:51,200 --> 00:03:52,959
usually some sort of ai or machine

111
00:03:52,959 --> 00:03:54,799
learning story you know when

112
00:03:54,799 --> 00:03:56,560
you're reading a blog post or a vendor

113
00:03:56,560 --> 00:03:58,560
thing you know it's you often find the

114
00:03:58,560 --> 00:04:01,280
two mentioned with each other

115
00:04:01,280 --> 00:04:03,599
and for mlsec we're really looking at

116
00:04:03,599 --> 00:04:06,000
applying machine learning towards static

117
00:04:06,000 --> 00:04:08,000
malware detection really looking at in

118
00:04:08,000 --> 00:04:09,920
this case pe files you know windows

119
00:04:09,920 --> 00:04:11,519
executables

120
00:04:11,519 --> 00:04:12,879
and

121
00:04:12,879 --> 00:04:14,879
there's two and this is a gross over

122
00:04:14,879 --> 00:04:16,320
simplification and i apologize but

123
00:04:16,320 --> 00:04:18,238
there's two ways to look at static kind

124
00:04:18,238 --> 00:04:20,320
of malware detection there's signature

125
00:04:20,320 --> 00:04:22,240
based where we're looking at like known

126
00:04:22,240 --> 00:04:23,600
indicators

127
00:04:23,600 --> 00:04:26,240
like md5 strings things like that this

128
00:04:26,240 --> 00:04:28,320
is super quick it's reliable low false

129
00:04:28,320 --> 00:04:29,360
positive

130
00:04:29,360 --> 00:04:30,720
but it struggles with new with new

131
00:04:30,720 --> 00:04:32,800
malware you'll often miss things if it

132
00:04:32,800 --> 00:04:35,360
doesn't fit into the iocs that you have

133
00:04:35,360 --> 00:04:37,120
by contrast there's you know what i'd

134
00:04:37,120 --> 00:04:38,479
what i'd say like heuristic based

135
00:04:38,479 --> 00:04:39,600
detection

136
00:04:39,600 --> 00:04:41,520
where you find similarities between a

137
00:04:41,520 --> 00:04:44,240
given file and a set of like known bad

138
00:04:44,240 --> 00:04:45,919
files and that's where machine learning

139
00:04:45,919 --> 00:04:46,960
comes in because you can start doing

140
00:04:46,960 --> 00:04:48,639
that pattern recognition

141
00:04:48,639 --> 00:04:50,479
and this is great because you can detect

142
00:04:50,479 --> 00:04:52,960
new malware with with higher accuracy

143
00:04:52,960 --> 00:04:54,240
but of course has some drawbacks where

144
00:04:54,240 --> 00:04:55,680
you've got to have like training data it

145
00:04:55,680 --> 00:04:57,360
can be a little slow and accuracy can be

146
00:04:57,360 --> 00:04:58,800
hit or miss

147
00:04:58,800 --> 00:05:00,560
and there's been a ton of work applying

148
00:05:00,560 --> 00:05:02,080
machine learning to static malware

149
00:05:02,080 --> 00:05:03,199
detection

150
00:05:03,199 --> 00:05:05,120
two kind of popular models i'm going to

151
00:05:05,120 --> 00:05:07,600
talk a decent bit about are ember on the

152
00:05:07,600 --> 00:05:10,240
left this is a specific machine learning

153
00:05:10,240 --> 00:05:12,240
model and a set of

154
00:05:12,240 --> 00:05:14,080
data that some researchers published a

155
00:05:14,080 --> 00:05:16,080
few years ago a lot of folks use it for

156
00:05:16,080 --> 00:05:19,199
benchmarking and then malcon is another

157
00:05:19,199 --> 00:05:20,639
machine learning framework but they use

158
00:05:20,639 --> 00:05:22,800
a neural network to detect malware i

159
00:05:22,800 --> 00:05:24,560
mean both are studied a lot kind of

160
00:05:24,560 --> 00:05:27,199
within this this realm of literature

161
00:05:27,199 --> 00:05:29,520
now if you just go off of the news and i

162
00:05:29,520 --> 00:05:30,720
don't want to say hype but if you just

163
00:05:30,720 --> 00:05:32,960
read about you know machine learning in

164
00:05:32,960 --> 00:05:34,720
in in the news

165
00:05:34,720 --> 00:05:36,240
it'll sound like it's it's a silver

166
00:05:36,240 --> 00:05:38,400
bullet and for a variety of reasons

167
00:05:38,400 --> 00:05:40,639
machine learning is not a silver bullet

168
00:05:40,639 --> 00:05:42,000
you can't just slap machine learning

169
00:05:42,000 --> 00:05:43,600
onto something and say i solved it i'm

170
00:05:43,600 --> 00:05:44,479
done

171
00:05:44,479 --> 00:05:45,840
you know there there's a lot of things

172
00:05:45,840 --> 00:05:47,520
you got to do and

173
00:05:47,520 --> 00:05:49,600
one of the issues with machine learning

174
00:05:49,600 --> 00:05:52,560
is something called adversarial examples

175
00:05:52,560 --> 00:05:54,240
this idea kind of came around with this

176
00:05:54,240 --> 00:05:57,759
paper in 2013 where they realized that

177
00:05:57,759 --> 00:05:58,880
i might have a machine learning

178
00:05:58,880 --> 00:06:01,520
classifier that has really high accuracy

179
00:06:01,520 --> 00:06:03,120
but what i can do is i can look at that

180
00:06:03,120 --> 00:06:06,000
classifier and kind of modify things in

181
00:06:06,000 --> 00:06:07,919
ways that are imperceptible to humans

182
00:06:07,919 --> 00:06:09,199
you know normally

183
00:06:09,199 --> 00:06:10,960
but really throw off the machine

184
00:06:10,960 --> 00:06:13,199
learning algorithm entirely i know it's

185
00:06:13,199 --> 00:06:14,960
small but these these pictures on the

186
00:06:14,960 --> 00:06:16,720
bottom are an example here where we

187
00:06:16,720 --> 00:06:18,880
start with the image on the left

188
00:06:18,880 --> 00:06:20,800
add a little bit of noise in the middle

189
00:06:20,800 --> 00:06:21,759
that

190
00:06:21,759 --> 00:06:24,080
you know as humans we we don't see that

191
00:06:24,080 --> 00:06:25,440
and you get the image on the right and

192
00:06:25,440 --> 00:06:27,360
you know to us it's the same but to a

193
00:06:27,360 --> 00:06:29,360
machine learning classifier you know

194
00:06:29,360 --> 00:06:31,600
this image here or let's go with this

195
00:06:31,600 --> 00:06:33,039
one here the dog

196
00:06:33,039 --> 00:06:34,960
you know it says yes this is a dog you

197
00:06:34,960 --> 00:06:36,639
had some noise and now over here it says

198
00:06:36,639 --> 00:06:38,240
it's an ostrich

199
00:06:38,240 --> 00:06:40,800
and so it turns out that this is a

200
00:06:40,800 --> 00:06:43,120
relatively big problem within this this

201
00:06:43,120 --> 00:06:44,639
kind of um

202
00:06:44,639 --> 00:06:46,560
the machine learning ecosystem

203
00:06:46,560 --> 00:06:48,800
adversarial examples are a problem that

204
00:06:48,800 --> 00:06:50,880
started really as like hey

205
00:06:50,880 --> 00:06:52,800
neural nets that's that's kind of where

206
00:06:52,800 --> 00:06:54,560
they're coming from but really most

207
00:06:54,560 --> 00:06:56,240
machine learning algorithms have some

208
00:06:56,240 --> 00:06:59,199
sort of story to tell with adversarial

209
00:06:59,199 --> 00:07:01,280
ai adversarial examples where you can

210
00:07:01,280 --> 00:07:03,680
take you know a normal a thing that

211
00:07:03,680 --> 00:07:05,120
would be normally classified by a

212
00:07:05,120 --> 00:07:07,680
classifier and add some noise to it and

213
00:07:07,680 --> 00:07:09,599
all of a sudden everything's off

214
00:07:09,599 --> 00:07:12,160
and this includes security there's been

215
00:07:12,160 --> 00:07:14,000
some papers in the past few years which

216
00:07:14,000 --> 00:07:15,840
say like hey i'm going to go build this

217
00:07:15,840 --> 00:07:17,520
machine learning classifier for a

218
00:07:17,520 --> 00:07:19,520
specific kind of malware

219
00:07:19,520 --> 00:07:20,240
and

220
00:07:20,240 --> 00:07:22,160
it'll work great but then i'm going to

221
00:07:22,160 --> 00:07:23,599
know take what i know about that

222
00:07:23,599 --> 00:07:25,759
classifier and

223
00:07:25,759 --> 00:07:27,680
find a way to create malware that

224
00:07:27,680 --> 00:07:29,520
specifically gets bio

225
00:07:29,520 --> 00:07:31,039
and there's a lot of paper well not a

226
00:07:31,039 --> 00:07:32,160
lot but there's a decent amount of

227
00:07:32,160 --> 00:07:34,000
papers in the literature that that talk

228
00:07:34,000 --> 00:07:35,520
about different ways to do this this has

229
00:07:35,520 --> 00:07:37,599
been applied to android malware pdf

230
00:07:37,599 --> 00:07:39,919
malware windows executables you know

231
00:07:39,919 --> 00:07:41,680
it's been applied to a variety of things

232
00:07:41,680 --> 00:07:42,639
and

233
00:07:42,639 --> 00:07:44,720
it's it seemed to be like a pressing

234
00:07:44,720 --> 00:07:46,319
thing that the community should worry

235
00:07:46,319 --> 00:07:47,280
about

236
00:07:47,280 --> 00:07:48,479
which

237
00:07:48,479 --> 00:07:50,720
when in in that context well yeah a

238
00:07:50,720 --> 00:07:52,400
competition makes sense because what we

239
00:07:52,400 --> 00:07:54,240
can do is you know yeah there's all this

240
00:07:54,240 --> 00:07:56,160
buzz about security and ml lots of

241
00:07:56,160 --> 00:07:58,160
research let's put a competition out

242
00:07:58,160 --> 00:07:59,840
there to join the two and really advance

243
00:07:59,840 --> 00:08:01,680
the state of the art

244
00:08:01,680 --> 00:08:03,599
so i apologize i know that's a mouthful

245
00:08:03,599 --> 00:08:04,879
um

246
00:08:04,879 --> 00:08:06,400
so i'm going to switch gears a little

247
00:08:06,400 --> 00:08:08,240
bit and talk about actually

248
00:08:08,240 --> 00:08:10,800
participating in the competition

249
00:08:10,800 --> 00:08:11,919
i'm going to be talking about something

250
00:08:11,919 --> 00:08:14,560
called kipple which is the name i gave

251
00:08:14,560 --> 00:08:17,599
to my um defender submission my

252
00:08:17,599 --> 00:08:19,120
detection kind of

253
00:08:19,120 --> 00:08:21,120
machine learning algorithm

254
00:08:21,120 --> 00:08:22,240
i'm going to say kipple a lot i

255
00:08:22,240 --> 00:08:24,000
apologize it's i don't know if anyone

256
00:08:24,000 --> 00:08:27,840
gets the reference it's really obscure

257
00:08:28,080 --> 00:08:29,840
all right so to

258
00:08:29,840 --> 00:08:31,440
as kind of a preamble to talking about

259
00:08:31,440 --> 00:08:34,240
how i built kipple the machine learning

260
00:08:34,240 --> 00:08:36,159
the ml set competition it had kind of

261
00:08:36,159 --> 00:08:37,919
four main phases there's the

262
00:08:37,919 --> 00:08:40,159
registration phase the defender phase

263
00:08:40,159 --> 00:08:41,839
where you build the classifier the

264
00:08:41,839 --> 00:08:43,519
attacker phase where people attack the

265
00:08:43,519 --> 00:08:45,680
classifiers and then kind of the the

266
00:08:45,680 --> 00:08:47,920
announcement of winners and

267
00:08:47,920 --> 00:08:49,839
for the defender track we had about five

268
00:08:49,839 --> 00:08:51,680
weeks to build a classifier the

269
00:08:51,680 --> 00:08:53,920
organizers provided an example in docker

270
00:08:53,920 --> 00:08:55,040
that you can just kind of download off

271
00:08:55,040 --> 00:08:56,800
of github you just run it and proof

272
00:08:56,800 --> 00:08:59,920
you've got a malware classifier

273
00:08:59,920 --> 00:09:02,080
and they did have a few entry

274
00:09:02,080 --> 00:09:03,600
restrictions saying like hey you need to

275
00:09:03,600 --> 00:09:05,360
meet these specific guidelines to be

276
00:09:05,360 --> 00:09:06,480
eligible

277
00:09:06,480 --> 00:09:08,399
in the competition one of them was it

278
00:09:08,399 --> 00:09:10,000
needs to take less than five seconds to

279
00:09:10,000 --> 00:09:11,920
classify a sample

280
00:09:11,920 --> 00:09:13,920
it also needs less than ten percent of a

281
00:09:13,920 --> 00:09:16,000
false negative rate so it needs to be be

282
00:09:16,000 --> 00:09:18,399
able to actually spot malware but it

283
00:09:18,399 --> 00:09:20,240
also needs less than one percent false

284
00:09:20,240 --> 00:09:21,839
positive rate which means that it's not

285
00:09:21,839 --> 00:09:23,279
just saying like hey everything is

286
00:09:23,279 --> 00:09:26,000
malware so you can't evade it

287
00:09:26,000 --> 00:09:27,680
and the winning model is the one that

288
00:09:27,680 --> 00:09:29,920
had the least evasions the one that was

289
00:09:29,920 --> 00:09:32,480
able to detect the most

290
00:09:32,480 --> 00:09:34,240
so the approach i took to build uh

291
00:09:34,240 --> 00:09:36,480
kipple was kind of five steps relatively

292
00:09:36,480 --> 00:09:38,000
straightforward first i was going to

293
00:09:38,000 --> 00:09:40,640
obtain or i did obtain a set of normal

294
00:09:40,640 --> 00:09:42,000
malware just kind of going off of some

295
00:09:42,000 --> 00:09:43,440
public um

296
00:09:43,440 --> 00:09:45,920
repositories then using that normal

297
00:09:45,920 --> 00:09:50,080
malware i built a set of anti-ml malware

298
00:09:50,080 --> 00:09:51,760
kind of taking the normal malware

299
00:09:51,760 --> 00:09:53,839
modifying it specifically to evade a

300
00:09:53,839 --> 00:09:55,279
classifier

301
00:09:55,279 --> 00:09:57,040
i then trained an initial model kind of

302
00:09:57,040 --> 00:09:58,880
for baselining a little bit

303
00:09:58,880 --> 00:10:01,760
only on the original malware to see how

304
00:10:01,760 --> 00:10:02,720
well

305
00:10:02,720 --> 00:10:04,079
you know we could detect things just

306
00:10:04,079 --> 00:10:05,600
kind of out of the box

307
00:10:05,600 --> 00:10:07,600
i then trained a whole bunch of models

308
00:10:07,600 --> 00:10:08,480
on

309
00:10:08,480 --> 00:10:10,560
different sets and combinations of the

310
00:10:10,560 --> 00:10:12,399
original and adversarial malware to see

311
00:10:12,399 --> 00:10:14,399
what approach might work best

312
00:10:14,399 --> 00:10:15,760
and then combine the different models

313
00:10:15,760 --> 00:10:17,360
together and chose the one that worked

314
00:10:17,360 --> 00:10:18,880
best

315
00:10:18,880 --> 00:10:20,240
so to walk through this in more detail

316
00:10:20,240 --> 00:10:23,120
step one getting the malware

317
00:10:23,120 --> 00:10:24,640
there are a few different sources i

318
00:10:24,640 --> 00:10:25,600
looked to

319
00:10:25,600 --> 00:10:27,120
the first was something called the ember

320
00:10:27,120 --> 00:10:29,760
data set this is freely available online

321
00:10:29,760 --> 00:10:30,800
this is

322
00:10:30,800 --> 00:10:32,399
the feature vectors you know it's not

323
00:10:32,399 --> 00:10:34,560
the the raw malware or the raw you know

324
00:10:34,560 --> 00:10:36,880
binaries but the feature representation

325
00:10:36,880 --> 00:10:37,760
of

326
00:10:37,760 --> 00:10:39,839
one million different pe files broken

327
00:10:39,839 --> 00:10:40,800
into

328
00:10:40,800 --> 00:10:42,720
malware benign and whether or not you

329
00:10:42,720 --> 00:10:44,720
know they didn't know

330
00:10:44,720 --> 00:10:46,560
and um

331
00:10:46,560 --> 00:10:48,320
this formed you know the basis for a lot

332
00:10:48,320 --> 00:10:49,519
of the training data and some of the

333
00:10:49,519 --> 00:10:51,519
test data i used

334
00:10:51,519 --> 00:10:53,279
then i also was able to get random

335
00:10:53,279 --> 00:10:55,440
malware from virus share

336
00:10:55,440 --> 00:10:57,680
um this was actually rate limited when i

337
00:10:57,680 --> 00:10:59,360
was downloading um

338
00:10:59,360 --> 00:11:00,640
which was

339
00:11:00,640 --> 00:11:02,640
it was like once one for every 15

340
00:11:02,640 --> 00:11:04,480
seconds or something and i wasn't

341
00:11:04,480 --> 00:11:05,839
working with a lot of hard drive space

342
00:11:05,839 --> 00:11:07,760
for a variety of reasons

343
00:11:07,760 --> 00:11:08,959
and

344
00:11:08,959 --> 00:11:11,200
i only got about 7 600 when just kind of

345
00:11:11,200 --> 00:11:13,920
letting it run all day and all night

346
00:11:13,920 --> 00:11:15,920
there's another malware set um you know

347
00:11:15,920 --> 00:11:18,880
actual binaries for called sorel

348
00:11:18,880 --> 00:11:20,640
this one's freely available online as

349
00:11:20,640 --> 00:11:23,040
well i was also downloading this one you

350
00:11:23,040 --> 00:11:25,200
know hard drive limited and not a little

351
00:11:25,200 --> 00:11:27,680
bit rate limited it's like about 32 000

352
00:11:27,680 --> 00:11:29,680
samples there

353
00:11:29,680 --> 00:11:31,839
i also needed some examples of benign

354
00:11:31,839 --> 00:11:34,079
binaries and for that i just looked on

355
00:11:34,079 --> 00:11:37,360
my own computer and got about 2500 which

356
00:11:37,360 --> 00:11:39,200
were just random things accumulated over

357
00:11:39,200 --> 00:11:40,959
15 years

358
00:11:40,959 --> 00:11:42,320
and then as part of the ml set

359
00:11:42,320 --> 00:11:44,560
competition proper they gave us two uh

360
00:11:44,560 --> 00:11:48,880
some example data 150 um

361
00:11:48,880 --> 00:11:51,920
normal malware instances and about 600

362
00:11:51,920 --> 00:11:52,560
or

363
00:11:52,560 --> 00:11:54,320
544

364
00:11:54,320 --> 00:11:57,200
adversarial instances that were

365
00:11:57,200 --> 00:11:58,959
you know submitted as part of the

366
00:11:58,959 --> 00:12:00,399
attacker track

367
00:12:00,399 --> 00:12:04,079
in the mlc 2019 competition

368
00:12:04,079 --> 00:12:06,399
so okay i have a bunch of malware and

369
00:12:06,399 --> 00:12:08,800
now i want to generate adversarial mail

370
00:12:08,800 --> 00:12:10,160
where i've got the normal stuff and i

371
00:12:10,160 --> 00:12:11,680
want to j i want to take that normal

372
00:12:11,680 --> 00:12:13,920
stuff and make it anti-ml

373
00:12:13,920 --> 00:12:16,240
and for that i followed um or i used

374
00:12:16,240 --> 00:12:18,000
three different approaches

375
00:12:18,000 --> 00:12:20,800
the first two used specific libraries

376
00:12:20,800 --> 00:12:22,560
that are both you know open source

377
00:12:22,560 --> 00:12:25,360
available online that are adversarial ai

378
00:12:25,360 --> 00:12:30,160
focused so these are able to take in

379
00:12:30,560 --> 00:12:32,880
windows malware and modify them

380
00:12:32,880 --> 00:12:35,839
specifically to be evasive

381
00:12:35,839 --> 00:12:38,480
the first is malware rl which makes

382
00:12:38,480 --> 00:12:40,880
small changes chaining them together to

383
00:12:40,880 --> 00:12:42,399
take a um

384
00:12:42,399 --> 00:12:44,000
a piece of malware make it a veda

385
00:12:44,000 --> 00:12:45,200
classifier

386
00:12:45,200 --> 00:12:47,040
and the other is secondml malware which

387
00:12:47,040 --> 00:12:48,560
makes a little bit bigger changes and

388
00:12:48,560 --> 00:12:50,720
both have a very heavy like adversarial

389
00:12:50,720 --> 00:12:52,560
ml focus

390
00:12:52,560 --> 00:12:53,760
the other approach i took was to

391
00:12:53,760 --> 00:12:56,800
generate new malware and for this i just

392
00:12:56,800 --> 00:12:58,399
generated

393
00:12:58,399 --> 00:13:00,240
lots of different versions of

394
00:13:00,240 --> 00:13:02,560
meterpreter more or less and just having

395
00:13:02,560 --> 00:13:05,040
it compiled to a pe file

396
00:13:05,040 --> 00:13:06,800
and varying a ton of the different

397
00:13:06,800 --> 00:13:08,399
options including like

398
00:13:08,399 --> 00:13:10,079
throwing extra code in there for the fun

399
00:13:10,079 --> 00:13:11,600
of it

400
00:13:11,600 --> 00:13:13,760
and as i mentioned all of these are open

401
00:13:13,760 --> 00:13:16,560
source they're available online um

402
00:13:16,560 --> 00:13:18,800
they're all out there and in total i

403
00:13:18,800 --> 00:13:21,680
generated about 131 000 different

404
00:13:21,680 --> 00:13:24,480
malware samples spread across three

405
00:13:24,480 --> 00:13:26,720
different categories the first being

406
00:13:26,720 --> 00:13:29,120
looking at the sorel data i downloaded

407
00:13:29,120 --> 00:13:31,519
and converting those into different

408
00:13:31,519 --> 00:13:33,360
adversarial samples

409
00:13:33,360 --> 00:13:34,959
same thing for the virus share and then

410
00:13:34,959 --> 00:13:37,680
using msf venom so got a nice kind of

411
00:13:37,680 --> 00:13:40,959
wide set of different data here

412
00:13:40,959 --> 00:13:43,199
another thing i did was i've got all

413
00:13:43,199 --> 00:13:44,639
that training data now i'm going to

414
00:13:44,639 --> 00:13:46,320
generate some test data to make sure

415
00:13:46,320 --> 00:13:48,480
that my approach is

416
00:13:48,480 --> 00:13:50,079
useful right i can't you know build an

417
00:13:50,079 --> 00:13:52,800
algorith build a classifier on some data

418
00:13:52,800 --> 00:13:54,160
and then test it on that data because

419
00:13:54,160 --> 00:13:55,760
it'll obviously know

420
00:13:55,760 --> 00:13:57,920
that the the training data is you know

421
00:13:57,920 --> 00:13:59,519
what what the labels are

422
00:13:59,519 --> 00:14:01,279
and there are three sets here the first

423
00:14:01,279 --> 00:14:03,120
was the one included as part of the

424
00:14:03,120 --> 00:14:04,959
mlset competition

425
00:14:04,959 --> 00:14:08,240
this is just 544 example

426
00:14:08,240 --> 00:14:10,480
adversarial malware instances

427
00:14:10,480 --> 00:14:14,639
the second was taking some of the mlsec

428
00:14:14,639 --> 00:14:17,040
examples those 150 that they gave us and

429
00:14:17,040 --> 00:14:19,040
using malware rl on them

430
00:14:19,040 --> 00:14:21,920
that generated a nice small set of 1400

431
00:14:21,920 --> 00:14:23,519
then the third kind of the same idea but

432
00:14:23,519 --> 00:14:25,920
using sec ml malware and that i

433
00:14:25,920 --> 00:14:28,560
generated you know 746.

434
00:14:28,560 --> 00:14:30,079
so i got a nice little set of training

435
00:14:30,079 --> 00:14:32,240
data and what i can do is look at that

436
00:14:32,240 --> 00:14:35,600
and um or use that to validate that that

437
00:14:35,600 --> 00:14:37,440
these different classifiers i build are

438
00:14:37,440 --> 00:14:39,440
actually useful

439
00:14:39,440 --> 00:14:41,120
and a few lessons learned in case

440
00:14:41,120 --> 00:14:43,120
anyone's looking to try any of this on

441
00:14:43,120 --> 00:14:44,399
their own

442
00:14:44,399 --> 00:14:45,760
the first is to

443
00:14:45,760 --> 00:14:47,600
make sure you have a lot of disk space i

444
00:14:47,600 --> 00:14:50,240
initially built this on a small linux vm

445
00:14:50,240 --> 00:14:52,399
on my personal pc it was less than 30

446
00:14:52,399 --> 00:14:54,639
gigabytes that i allocated this was a

447
00:14:54,639 --> 00:14:57,519
huge mistake um and it came as a problem

448
00:14:57,519 --> 00:15:00,240
in downloading models samples anything

449
00:15:00,240 --> 00:15:02,160
and even generating new data i

450
00:15:02,160 --> 00:15:04,079
constantly was resizing my vm and

451
00:15:04,079 --> 00:15:06,079
eventually made it to 300 gigabytes and

452
00:15:06,079 --> 00:15:07,040
and

453
00:15:07,040 --> 00:15:09,440
that worked well but it would have saved

454
00:15:09,440 --> 00:15:11,680
me time and heartache to just start

455
00:15:11,680 --> 00:15:13,040
there

456
00:15:13,040 --> 00:15:14,560
and also just dedicate enough time if

457
00:15:14,560 --> 00:15:16,399
you're going to do this when downloading

458
00:15:16,399 --> 00:15:18,320
samples if you are rate limited it takes

459
00:15:18,320 --> 00:15:19,279
time

460
00:15:19,279 --> 00:15:21,040
and when you're generating new samples

461
00:15:21,040 --> 00:15:23,680
that takes time as well so often time i

462
00:15:23,680 --> 00:15:24,959
would just like leave things running

463
00:15:24,959 --> 00:15:27,440
overnight and i'd have multiple things

464
00:15:27,440 --> 00:15:29,040
running at once and this is on a

465
00:15:29,040 --> 00:15:30,399
personal pc

466
00:15:30,399 --> 00:15:31,920
at home my power bill was through the

467
00:15:31,920 --> 00:15:33,360
roof and

468
00:15:33,360 --> 00:15:35,120
looking back i should have just done

469
00:15:35,120 --> 00:15:38,480
stuff in the cloud so i wasn't paying

470
00:15:38,480 --> 00:15:40,560
dominion power as much money as i ended

471
00:15:40,560 --> 00:15:41,680
up paying them

472
00:15:41,680 --> 00:15:42,560
all right

473
00:15:42,560 --> 00:15:44,480
so step number three building an initial

474
00:15:44,480 --> 00:15:46,240
model and for this i really just

475
00:15:46,240 --> 00:15:48,320
followed that the the basic kind of

476
00:15:48,320 --> 00:15:50,880
baseline goal not gold standard but

477
00:15:50,880 --> 00:15:52,880
standard of using ember

478
00:15:52,880 --> 00:15:55,519
and i just followed the normal ember

479
00:15:55,519 --> 00:15:57,920
model training code set the false

480
00:15:57,920 --> 00:16:00,000
positive threshold to one percent as per

481
00:16:00,000 --> 00:16:02,079
the guidelines of the competition

482
00:16:02,079 --> 00:16:05,519
and ran the results and overall it did

483
00:16:05,519 --> 00:16:06,959
fairly well

484
00:16:06,959 --> 00:16:09,040
a few blips here and there but just kind

485
00:16:09,040 --> 00:16:11,360
of taking ember off the shelf and

486
00:16:11,360 --> 00:16:13,759
training it running it i got some decent

487
00:16:13,759 --> 00:16:15,600
results you know on on the normal

488
00:16:15,600 --> 00:16:17,120
malware

489
00:16:17,120 --> 00:16:18,639
that said when looking at the

490
00:16:18,639 --> 00:16:20,959
adversarial malware the malware designed

491
00:16:20,959 --> 00:16:22,959
to evade a classifier

492
00:16:22,959 --> 00:16:24,959
things weren't as nice

493
00:16:24,959 --> 00:16:27,839
first it struggled with the 2019

494
00:16:27,839 --> 00:16:30,320
adversarial data and the

495
00:16:30,320 --> 00:16:33,040
malware rl and gamma samples when

496
00:16:33,040 --> 00:16:35,519
applied to sorel you can see the 90 you

497
00:16:35,519 --> 00:16:37,440
know the mid 90s that we had before

498
00:16:37,440 --> 00:16:40,079
those are jumping down to the high 50s

499
00:16:40,079 --> 00:16:41,519
you know these are pretty easily getting

500
00:16:41,519 --> 00:16:44,240
by ember

501
00:16:44,240 --> 00:16:45,759
we do see though that it can detect some

502
00:16:45,759 --> 00:16:47,680
of the adversarial attacks off the shelf

503
00:16:47,680 --> 00:16:49,920
um dos header manipulation and and the

504
00:16:49,920 --> 00:16:52,399
two padding attacks that i had

505
00:16:52,399 --> 00:16:54,639
i also noticed that um the virus share

506
00:16:54,639 --> 00:16:56,880
variants over here

507
00:16:56,880 --> 00:17:00,000
those were um easier to detect for ember

508
00:17:00,000 --> 00:17:02,000
than the sorel variants just kind of

509
00:17:02,000 --> 00:17:03,839
overall this was likely because there

510
00:17:03,839 --> 00:17:06,880
was some mixing of the data there

511
00:17:06,880 --> 00:17:08,640
and then the msf ones all the different

512
00:17:08,640 --> 00:17:11,439
you know interpreter variants they uh

513
00:17:11,439 --> 00:17:14,000
ember struggled with those big time even

514
00:17:14,000 --> 00:17:16,559
more so than with the adversarial ml

515
00:17:16,559 --> 00:17:18,640
ones which was i always think it's

516
00:17:18,640 --> 00:17:20,160
interesting when interpreter can't be

517
00:17:20,160 --> 00:17:22,400
detected

518
00:17:22,400 --> 00:17:24,079
and a few lessons learned the first

519
00:17:24,079 --> 00:17:25,439
thing was

520
00:17:25,439 --> 00:17:27,119
if you are looking to get into this a

521
00:17:27,119 --> 00:17:29,360
basic model is reasonably accurate

522
00:17:29,360 --> 00:17:31,120
ember by itself on the normal malware

523
00:17:31,120 --> 00:17:32,880
did a good job and

524
00:17:32,880 --> 00:17:34,559
as a side project it was fun just kind

525
00:17:34,559 --> 00:17:36,400
of downloading stuff off the shelf

526
00:17:36,400 --> 00:17:37,919
hitting play and all of a sudden i've

527
00:17:37,919 --> 00:17:41,200
got a malware classifier so it was nice

528
00:17:41,200 --> 00:17:42,880
um also if you're doing this make sure

529
00:17:42,880 --> 00:17:44,880
to keep good records i ended up doing a

530
00:17:44,880 --> 00:17:46,480
lot of different things and

531
00:17:46,480 --> 00:17:48,559
not taking good notes and forgetting

532
00:17:48,559 --> 00:17:50,480
what i had tried and what i didn't try

533
00:17:50,480 --> 00:17:52,160
and all sorts of different stuff that

534
00:17:52,160 --> 00:17:52,960
i'm

535
00:17:52,960 --> 00:17:54,320
not going to get into

536
00:17:54,320 --> 00:17:55,919
and then also make sure to separate your

537
00:17:55,919 --> 00:17:57,760
training and your testing data just

538
00:17:57,760 --> 00:17:59,520
because as you saw with a little bit of

539
00:17:59,520 --> 00:18:01,200
the virus share stuff

540
00:18:01,200 --> 00:18:03,360
it was um

541
00:18:03,360 --> 00:18:05,520
some cross-pollination there

542
00:18:05,520 --> 00:18:06,880
and so yeah just make sure to track

543
00:18:06,880 --> 00:18:08,799
where your data is coming from and make

544
00:18:08,799 --> 00:18:11,120
sure to generate test data that's

545
00:18:11,120 --> 00:18:14,080
different than your trained data um i've

546
00:18:14,080 --> 00:18:15,200
got a little bit more on that in the

547
00:18:15,200 --> 00:18:17,200
white paper but it was interesting

548
00:18:17,200 --> 00:18:19,760
finding some of the results that i got

549
00:18:19,760 --> 00:18:21,600
all right so we've got we've got uh

550
00:18:21,600 --> 00:18:23,600
normal malware adversarial malware we've

551
00:18:23,600 --> 00:18:25,200
got the initial model and now i want to

552
00:18:25,200 --> 00:18:26,960
build some advanced models

553
00:18:26,960 --> 00:18:29,120
and the first idea here is to just use

554
00:18:29,120 --> 00:18:31,360
something called adversarial retraining

555
00:18:31,360 --> 00:18:33,440
and the idea is well i'm going to take

556
00:18:33,440 --> 00:18:34,320
the

557
00:18:34,320 --> 00:18:36,080
you know the the amber idea and just

558
00:18:36,080 --> 00:18:38,240
retrain it to include those hundred and

559
00:18:38,240 --> 00:18:40,160
thirty thousand malware samples that i

560
00:18:40,160 --> 00:18:42,240
generated right i'm going to retrain it

561
00:18:42,240 --> 00:18:43,600
including those

562
00:18:43,600 --> 00:18:45,760
and when you do that it does pretty well

563
00:18:45,760 --> 00:18:47,280
it's a straightforward idea just use

564
00:18:47,280 --> 00:18:49,440
those pop them in there and

565
00:18:49,440 --> 00:18:51,679
we see a good performance boost and when

566
00:18:51,679 --> 00:18:54,160
you compare it against the initial model

567
00:18:54,160 --> 00:18:55,760
on the left the retrained model on the

568
00:18:55,760 --> 00:18:56,640
right

569
00:18:56,640 --> 00:18:58,559
what you can see is when it comes to

570
00:18:58,559 --> 00:19:00,720
just looking at normal malicious these

571
00:19:00,720 --> 00:19:01,919
two right here

572
00:19:01,919 --> 00:19:04,400
normal malicious malware samples

573
00:19:04,400 --> 00:19:06,799
ember does a little bit better than the

574
00:19:06,799 --> 00:19:08,880
retrain model but overall they're kind

575
00:19:08,880 --> 00:19:10,720
of in the same ballpark but on the

576
00:19:10,720 --> 00:19:12,720
adversarial side we're going from

577
00:19:12,720 --> 00:19:16,000
50s and 70s to 70s and 80s we're doing

578
00:19:16,000 --> 00:19:18,000
much better just using retraining so

579
00:19:18,000 --> 00:19:19,280
this was nice to see as well just

580
00:19:19,280 --> 00:19:21,360
because it was like hey this is clearly

581
00:19:21,360 --> 00:19:22,880
better than using ember i'm already

582
00:19:22,880 --> 00:19:24,480
beating that that baseline and if i

583
00:19:24,480 --> 00:19:26,160
submitted that to the competition i

584
00:19:26,160 --> 00:19:28,640
would feel good about myself

585
00:19:28,640 --> 00:19:30,240
but i don't want to stop there i wanted

586
00:19:30,240 --> 00:19:32,000
to kind of do more advanced things to or

587
00:19:32,000 --> 00:19:33,360
at least try to

588
00:19:33,360 --> 00:19:34,880
and the idea i had was okay i'm going to

589
00:19:34,880 --> 00:19:37,760
build a set of different variations of

590
00:19:37,760 --> 00:19:39,120
what i want to train and what i want to

591
00:19:39,120 --> 00:19:40,320
test on

592
00:19:40,320 --> 00:19:42,640
and

593
00:19:42,640 --> 00:19:44,640
the first idea was i want to follow one

594
00:19:44,640 --> 00:19:47,520
of two paradigms kind of dictating what

595
00:19:47,520 --> 00:19:50,000
i want to distinguish between

596
00:19:50,000 --> 00:19:51,360
one of them

597
00:19:51,360 --> 00:19:53,600
called all i i wanted to distinguish

598
00:19:53,600 --> 00:19:55,440
between normal pe files and the

599
00:19:55,440 --> 00:19:57,360
adversarial

600
00:19:57,360 --> 00:19:59,919
um adversarially modified pe files and

601
00:19:59,919 --> 00:20:01,840
so when i say normal i mean

602
00:20:01,840 --> 00:20:04,400
both benign and malware the idea being

603
00:20:04,400 --> 00:20:05,640
that

604
00:20:05,640 --> 00:20:07,679
anti-mlpes those might be

605
00:20:07,679 --> 00:20:10,559
distinguishable in and of themselves

606
00:20:10,559 --> 00:20:12,080
which is a little bit out there but it

607
00:20:12,080 --> 00:20:14,320
was one thing to test and then benign

608
00:20:14,320 --> 00:20:15,760
which is i'm going to train specific

609
00:20:15,760 --> 00:20:18,640
models looking at benign pes versus

610
00:20:18,640 --> 00:20:21,679
adversarial pes

611
00:20:21,679 --> 00:20:23,600
and then underneath that as well i had

612
00:20:23,600 --> 00:20:25,600
lots of different ideas on

613
00:20:25,600 --> 00:20:27,600
what different types of adversarial

614
00:20:27,600 --> 00:20:30,400
samples i wanted to train against

615
00:20:30,400 --> 00:20:32,159
four ideas in total one of them was to

616
00:20:32,159 --> 00:20:33,520
just train against everything i called

617
00:20:33,520 --> 00:20:35,520
that adversarial one of them was

618
00:20:35,520 --> 00:20:38,799
variants which was just the anti-ml

619
00:20:38,799 --> 00:20:40,400
pe files that are generated you know

620
00:20:40,400 --> 00:20:43,039
malware rl and secamel malware

621
00:20:43,039 --> 00:20:45,760
msf which was the msf venom ones and

622
00:20:45,760 --> 00:20:48,960
undetected which was effectively the msf

623
00:20:48,960 --> 00:20:50,880
venom instances that were particularly

624
00:20:50,880 --> 00:20:52,159
evasive

625
00:20:52,159 --> 00:20:52,960
so

626
00:20:52,960 --> 00:20:55,039
that's eight different combinations and

627
00:20:55,039 --> 00:20:56,240
of course i'm going to produce an eye

628
00:20:56,240 --> 00:20:57,760
chart

629
00:20:57,760 --> 00:20:58,960
i'm not going to go through these in all

630
00:20:58,960 --> 00:21:01,520
detail aside from saying that

631
00:21:01,520 --> 00:21:03,600
variants performed the best

632
00:21:03,600 --> 00:21:05,520
just kind of across the board it was

633
00:21:05,520 --> 00:21:07,840
clear that that model was was doing a

634
00:21:07,840 --> 00:21:09,600
good job over here on detecting the

635
00:21:09,600 --> 00:21:11,120
adversarial malware not so good over

636
00:21:11,120 --> 00:21:13,919
here but did a good job

637
00:21:13,919 --> 00:21:16,640
the msf models didn't do well much at

638
00:21:16,640 --> 00:21:17,360
all

639
00:21:17,360 --> 00:21:19,840
training on on ms on kind of these

640
00:21:19,840 --> 00:21:21,360
interpreter variants it didn't really

641
00:21:21,360 --> 00:21:23,200
seem to help

642
00:21:23,200 --> 00:21:26,080
the benign case outperformed all i'm not

643
00:21:26,080 --> 00:21:28,559
going to get into that too much

644
00:21:28,559 --> 00:21:29,919
and the undetected case that was a

645
00:21:29,919 --> 00:21:31,600
little bit reversed and again i'm not

646
00:21:31,600 --> 00:21:33,440
going to get into that too much but

647
00:21:33,440 --> 00:21:35,919
there's a bit of

648
00:21:35,919 --> 00:21:37,440
interesting internals when you look at

649
00:21:37,440 --> 00:21:39,120
the training data that was used between

650
00:21:39,120 --> 00:21:40,720
these different models

651
00:21:40,720 --> 00:21:42,400
so okay i've got a million models now

652
00:21:42,400 --> 00:21:43,360
and i'm going to choose the best

653
00:21:43,360 --> 00:21:45,039
combination and

654
00:21:45,039 --> 00:21:46,400
basically the idea was i'm going to

655
00:21:46,400 --> 00:21:48,559
choose up to three models find

656
00:21:48,559 --> 00:21:50,400
thresholds for each to ensure i hit one

657
00:21:50,400 --> 00:21:52,320
percent false positive rate and then

658
00:21:52,320 --> 00:21:54,799
compare performance

659
00:21:54,799 --> 00:21:57,840
and unsurprisingly another eye chart a

660
00:21:57,840 --> 00:22:00,000
couple of quick things i'd say is

661
00:22:00,000 --> 00:22:01,440
a few of the models

662
00:22:01,440 --> 00:22:03,600
initial retrained and

663
00:22:03,600 --> 00:22:05,039
the

664
00:22:05,039 --> 00:22:07,440
adversarial along with initial those did

665
00:22:07,440 --> 00:22:09,440
well on normal malware

666
00:22:09,440 --> 00:22:13,360
but struggled on adversarial malware

667
00:22:13,360 --> 00:22:14,080
the

668
00:22:14,080 --> 00:22:15,919
models trained on

669
00:22:15,919 --> 00:22:17,360
the meterpreter

670
00:22:17,360 --> 00:22:18,480
uh

671
00:22:18,480 --> 00:22:20,400
the interpreter instances

672
00:22:20,400 --> 00:22:23,200
those did did well overall but for you

673
00:22:23,200 --> 00:22:24,880
know relatively speaking they they

674
00:22:24,880 --> 00:22:26,400
really weren't you know particularly

675
00:22:26,400 --> 00:22:27,760
stand out

676
00:22:27,760 --> 00:22:29,440
and then these models that were a mix of

677
00:22:29,440 --> 00:22:30,960
variants and undetected all these

678
00:22:30,960 --> 00:22:33,039
different combinations those seem to

679
00:22:33,039 --> 00:22:34,880
shine the most doing the best on

680
00:22:34,880 --> 00:22:36,799
adversarial malware while also doing

681
00:22:36,799 --> 00:22:38,480
decently on

682
00:22:38,480 --> 00:22:40,159
normal malware itself and again i

683
00:22:40,159 --> 00:22:41,919
apologize i know it's an eye chart it's

684
00:22:41,919 --> 00:22:43,360
going to skip to the chase which was

685
00:22:43,360 --> 00:22:44,960
that i ended up choosing this row

686
00:22:44,960 --> 00:22:47,520
somewhat randomly um

687
00:22:47,520 --> 00:22:48,640
it's not the best i don't know why i

688
00:22:48,640 --> 00:22:50,559
chose it but i chose it

689
00:22:50,559 --> 00:22:52,960
and that's what i ended up submitting as

690
00:22:52,960 --> 00:22:55,840
my as kipple i also did a few things to

691
00:22:55,840 --> 00:22:57,200
make it a little bit stronger i

692
00:22:57,200 --> 00:23:00,080
hard-coded the md5 hashes of the benign

693
00:23:00,080 --> 00:23:01,919
wear that i was using just so i i

694
00:23:01,919 --> 00:23:04,000
lowered my false positive

695
00:23:04,000 --> 00:23:06,320
i also used something that was called a

696
00:23:06,320 --> 00:23:08,480
stateful detection mechanism this was

697
00:23:08,480 --> 00:23:10,640
given free to me it was something

698
00:23:10,640 --> 00:23:13,120
included by the mlsac organizers the

699
00:23:13,120 --> 00:23:16,320
idea behind it was whenever a new sample

700
00:23:16,320 --> 00:23:18,240
comes in as benign

701
00:23:18,240 --> 00:23:20,240
or a new sample comes in we run the

702
00:23:20,240 --> 00:23:22,080
classifier on it if the classifier

703
00:23:22,080 --> 00:23:23,679
thinks it's benign we're going to

704
00:23:23,679 --> 00:23:25,679
compare it against the last you know x

705
00:23:25,679 --> 00:23:28,000
number of submissions and if and if any

706
00:23:28,000 --> 00:23:30,320
of those are malware and it looks like

707
00:23:30,320 --> 00:23:32,400
them we're going to classify it as

708
00:23:32,400 --> 00:23:34,320
malicious the idea being that if an

709
00:23:34,320 --> 00:23:35,600
adversary is trying to kind of push the

710
00:23:35,600 --> 00:23:37,760
classifier in one direction it can pick

711
00:23:37,760 --> 00:23:39,760
up on that

712
00:23:39,760 --> 00:23:42,240
after all that um about a day before the

713
00:23:42,240 --> 00:23:44,240
deadline i got it in and i was super

714
00:23:44,240 --> 00:23:45,440
happy

715
00:23:45,440 --> 00:23:48,080
okay let's switch over to

716
00:23:48,080 --> 00:23:50,159
the attacker track now and talk about

717
00:23:50,159 --> 00:23:52,799
how i tried to defend my classifier by

718
00:23:52,799 --> 00:23:54,799
attacking the others this was never my

719
00:23:54,799 --> 00:23:56,880
intention walking into it i just ended

720
00:23:56,880 --> 00:23:57,600
up

721
00:23:57,600 --> 00:23:59,360
committing so much time to it that i

722
00:23:59,360 --> 00:24:01,279
figured i'd invest more

723
00:24:01,279 --> 00:24:02,960
as a quick disclaimer some of the

724
00:24:02,960 --> 00:24:05,360
details here are missing this was kind

725
00:24:05,360 --> 00:24:08,400
of ad hoc as i was doing it so

726
00:24:08,400 --> 00:24:09,840
some things are embellished some of the

727
00:24:09,840 --> 00:24:11,440
numbers aren't exactly right but

728
00:24:11,440 --> 00:24:13,360
generally the i

729
00:24:13,360 --> 00:24:14,960
idea is there

730
00:24:14,960 --> 00:24:16,720
so to give a little bit of context right

731
00:24:16,720 --> 00:24:19,279
when the um attacker track kicked off

732
00:24:19,279 --> 00:24:21,279
there were a total of six entries into

733
00:24:21,279 --> 00:24:22,960
the defender track it was kipple and

734
00:24:22,960 --> 00:24:24,240
five others

735
00:24:24,240 --> 00:24:25,600
and right off the bat in the first few

736
00:24:25,600 --> 00:24:27,200
weeks kipple was doing great the other

737
00:24:27,200 --> 00:24:29,360
models all had hits against them kippel

738
00:24:29,360 --> 00:24:31,200
was basically picking up everything so i

739
00:24:31,200 --> 00:24:33,039
was super happy

740
00:24:33,039 --> 00:24:35,279
a couple weeks or maybe a week later uh

741
00:24:35,279 --> 00:24:37,360
two models start catching up one called

742
00:24:37,360 --> 00:24:40,080
am square amsqr and another one called

743
00:24:40,080 --> 00:24:41,840
secret um

744
00:24:41,840 --> 00:24:43,360
the other three are really far behind

745
00:24:43,360 --> 00:24:44,799
but you know kipple's ahead and i'm like

746
00:24:44,799 --> 00:24:46,559
okay this is good

747
00:24:46,559 --> 00:24:48,400
then a little bit later um the other two

748
00:24:48,400 --> 00:24:50,559
models continue catching up and it

749
00:24:50,559 --> 00:24:53,120
occurs to me that at this rate kipple

750
00:24:53,120 --> 00:24:55,360
isn't going to do you know it's not

751
00:24:55,360 --> 00:24:56,720
going to get in the top two which is

752
00:24:56,720 --> 00:24:59,440
where i was hoping to get but you know i

753
00:24:59,440 --> 00:25:01,840
wanted to get there um

754
00:25:01,840 --> 00:25:03,600
so i think well maybe i'm going to need

755
00:25:03,600 --> 00:25:04,960
to actually do something on the attacker

756
00:25:04,960 --> 00:25:05,840
track

757
00:25:05,840 --> 00:25:06,960
and to provide a little bit of

758
00:25:06,960 --> 00:25:09,440
background the way um

759
00:25:09,440 --> 00:25:11,200
the way the attacker track worked was

760
00:25:11,200 --> 00:25:13,600
specifically around scoring

761
00:25:13,600 --> 00:25:15,760
basically the mlsec organizers they

762
00:25:15,760 --> 00:25:19,039
provided 50 malware samples labeled 1-50

763
00:25:19,039 --> 00:25:21,039
attackers can either do one of two

764
00:25:21,039 --> 00:25:23,279
things they can query one or more of the

765
00:25:23,279 --> 00:25:25,279
target models that they're interested in

766
00:25:25,279 --> 00:25:27,039
to see what the label is or they can

767
00:25:27,039 --> 00:25:28,480
submit an attack

768
00:25:28,480 --> 00:25:31,200
the query is quick it's unlimited you

769
00:25:31,200 --> 00:25:32,320
just kind of throw it out they get a

770
00:25:32,320 --> 00:25:34,640
result back real quickly piece of cake

771
00:25:34,640 --> 00:25:36,799
the attack is much more in depth that's

772
00:25:36,799 --> 00:25:38,400
kind of like saying yes i want you to

773
00:25:38,400 --> 00:25:40,960
count this specifically as a submission

774
00:25:40,960 --> 00:25:42,320
and they actually detonate it in a

775
00:25:42,320 --> 00:25:43,919
sandbox to make sure that you haven't

776
00:25:43,919 --> 00:25:46,080
done that that it actually works and

777
00:25:46,080 --> 00:25:47,919
this one's you know limited to about 50

778
00:25:47,919 --> 00:25:50,080
an hour or so

779
00:25:50,080 --> 00:25:51,600
and the way scoring works is that

780
00:25:51,600 --> 00:25:53,600
attackers get a point by creating a

781
00:25:53,600 --> 00:25:57,039
working variant that evades um

782
00:25:57,039 --> 00:25:59,520
that evades a classifier so if i if i

783
00:25:59,520 --> 00:26:01,440
submit a working variant of zero zero

784
00:26:01,440 --> 00:26:03,520
one and it evades all six classifiers i

785
00:26:03,520 --> 00:26:04,960
get six points

786
00:26:04,960 --> 00:26:07,200
there is however one a maximum of one

787
00:26:07,200 --> 00:26:09,679
point per classifier per sample so

788
00:26:09,679 --> 00:26:12,159
there's a maximum attacker score of 300

789
00:26:12,159 --> 00:26:14,880
they also must be functional and they

790
00:26:14,880 --> 00:26:16,400
they need to adhere to a couple of rules

791
00:26:16,400 --> 00:26:17,919
that i'm not going to go into aside from

792
00:26:17,919 --> 00:26:20,640
that they're smaller than two megabytes

793
00:26:20,640 --> 00:26:22,159
and then tie breaks are broken by the

794
00:26:22,159 --> 00:26:24,960
number of queries and attacks so if i'm

795
00:26:24,960 --> 00:26:26,799
querying you know five million times

796
00:26:26,799 --> 00:26:28,799
against the classifiers i'm gonna lose

797
00:26:28,799 --> 00:26:30,240
on tie breaks versus someone who queries

798
00:26:30,240 --> 00:26:31,919
you know five times

799
00:26:31,919 --> 00:26:33,200
on the defenders side things are a

800
00:26:33,200 --> 00:26:34,799
little bit different you'll effectively

801
00:26:34,799 --> 00:26:37,679
lose a point if an attacker is able to

802
00:26:37,679 --> 00:26:40,480
evade the classifier for a given sample

803
00:26:40,480 --> 00:26:43,440
however unlike the attacker track

804
00:26:43,440 --> 00:26:44,960
there's no limit to the number of points

805
00:26:44,960 --> 00:26:47,760
you can lose so if i submit 0 0 1

806
00:26:47,760 --> 00:26:49,679
variant a and then variant b against

807
00:26:49,679 --> 00:26:51,520
kipple and both of them evade i'm going

808
00:26:51,520 --> 00:26:53,200
to lose two points for that versus on

809
00:26:53,200 --> 00:26:54,480
the attacker's side you're only going to

810
00:26:54,480 --> 00:26:56,159
get one point because it only counts for

811
00:26:56,159 --> 00:26:57,760
one sample

812
00:26:57,760 --> 00:26:58,640
so

813
00:26:58,640 --> 00:27:00,400
with that in mind i thought well maybe

814
00:27:00,400 --> 00:27:02,799
the best way to make kipple better is to

815
00:27:02,799 --> 00:27:06,559
go on offense if i can submit variance

816
00:27:06,559 --> 00:27:08,880
of of the different samples that kipple

817
00:27:08,880 --> 00:27:11,440
detects but the others don't detect i'm

818
00:27:11,440 --> 00:27:13,120
going to relatively increase the score

819
00:27:13,120 --> 00:27:14,400
of kipple

820
00:27:14,400 --> 00:27:16,240
this is silly in a kind of gaming the

821
00:27:16,240 --> 00:27:19,120
system but

822
00:27:19,120 --> 00:27:21,600
it's a contest it's fun

823
00:27:21,600 --> 00:27:22,399
so

824
00:27:22,399 --> 00:27:24,000
i said i'm going to give this a shot and

825
00:27:24,000 --> 00:27:25,200
go to the drawing board there's two ways

826
00:27:25,200 --> 00:27:26,640
to look at evasion one of them is

827
00:27:26,640 --> 00:27:28,880
traditional evasion you know av evasion

828
00:27:28,880 --> 00:27:30,559
fully undetectable we want to take you

829
00:27:30,559 --> 00:27:32,399
know a file get it out there obfuscators

830
00:27:32,399 --> 00:27:35,279
crypters packers all that fun stuff

831
00:27:35,279 --> 00:27:38,080
the other is adversarial ml anti-ml

832
00:27:38,080 --> 00:27:40,640
evasion things like adversarial examples

833
00:27:40,640 --> 00:27:43,120
malware rl secondary malware

834
00:27:43,120 --> 00:27:45,039
and having invested as much time as i

835
00:27:45,039 --> 00:27:48,000
did in anti-ml i decided that this would

836
00:27:48,000 --> 00:27:49,919
be best and number one i think

837
00:27:49,919 --> 00:27:52,960
adversarial ml is fun i i hope to convey

838
00:27:52,960 --> 00:27:55,279
that in this talk as well um

839
00:27:55,279 --> 00:27:57,200
i also have the infrastructure in place

840
00:27:57,200 --> 00:27:59,360
for anti-ml i've got malware rl i've got

841
00:27:59,360 --> 00:28:00,960
second male malware got a bunch of other

842
00:28:00,960 --> 00:28:04,000
stuff kipple also should detect the

843
00:28:04,000 --> 00:28:06,240
anti-ml attacks

844
00:28:06,240 --> 00:28:07,520
um

845
00:28:07,520 --> 00:28:09,120
and then also i think traditional

846
00:28:09,120 --> 00:28:10,799
obfuscation is harder i'm not very

847
00:28:10,799 --> 00:28:13,200
experienced in this domain and i don't

848
00:28:13,200 --> 00:28:15,200
have good infrastructure my linux vm

849
00:28:15,200 --> 00:28:16,720
isn't a great um

850
00:28:16,720 --> 00:28:19,039
av obfuscation testbed

851
00:28:19,039 --> 00:28:21,200
and i'm also not sure if kipple can even

852
00:28:21,200 --> 00:28:24,240
detect traditional obfuscation

853
00:28:24,240 --> 00:28:27,440
so i decided to go with anti-ml and

854
00:28:27,440 --> 00:28:28,799
just as a brief interlude i'm going to

855
00:28:28,799 --> 00:28:32,320
talk about something called miter atlas

856
00:28:32,720 --> 00:28:34,240
how many people are familiar with attack

857
00:28:34,240 --> 00:28:36,080
miter attack

858
00:28:36,080 --> 00:28:37,360
a lot of people

859
00:28:37,360 --> 00:28:40,559
has anyone else seen miter atlas

860
00:28:40,559 --> 00:28:42,000
okay so i'm going to show a little bit

861
00:28:42,000 --> 00:28:43,520
about miter atlas because i think atlas

862
00:28:43,520 --> 00:28:45,440
is cool it's a framework it's available

863
00:28:45,440 --> 00:28:47,279
atlas.mitre.org

864
00:28:47,279 --> 00:28:50,080
but as you might guess with my preamble

865
00:28:50,080 --> 00:28:52,399
it is an attack-like framework for

866
00:28:52,399 --> 00:28:55,279
adversarial ai attacks i'm taking that

867
00:28:55,279 --> 00:28:57,440
that same you know matrix idea you know

868
00:28:57,440 --> 00:28:58,799
attacker's perspective that kind of

869
00:28:58,799 --> 00:29:02,159
stuff and applying it to adversarial ml

870
00:29:02,159 --> 00:29:03,600
some of the things that i like about

871
00:29:03,600 --> 00:29:05,279
atlas in particular and again i'm just

872
00:29:05,279 --> 00:29:07,520
speaking for myself number one it

873
00:29:07,520 --> 00:29:09,200
includes the full life cycle of

874
00:29:09,200 --> 00:29:11,440
adversarial attacks um it's not just

875
00:29:11,440 --> 00:29:13,120
limited to evasion and adversarial

876
00:29:13,120 --> 00:29:16,000
examples i bring this up as i'm not

877
00:29:16,000 --> 00:29:18,159
doing it nearly enough justice this idea

878
00:29:18,159 --> 00:29:21,279
of anti-ml atlas does a better job

879
00:29:21,279 --> 00:29:23,039
it's also broken into tactics and

880
00:29:23,039 --> 00:29:24,799
techniques like the attack framework

881
00:29:24,799 --> 00:29:26,399
written largely from the attacker's

882
00:29:26,399 --> 00:29:28,480
perspective so as i'm looking at the

883
00:29:28,480 --> 00:29:30,000
competition i'm going to go conduct an

884
00:29:30,000 --> 00:29:31,919
attack i can consult atlas for

885
00:29:31,919 --> 00:29:34,559
constructing the ttps i want to use

886
00:29:34,559 --> 00:29:36,720
it's also backed up with public examples

887
00:29:36,720 --> 00:29:38,399
from case studies and reports that are

888
00:29:38,399 --> 00:29:40,240
contributed by the community to get a

889
00:29:40,240 --> 00:29:42,559
little bit of examples of how to do some

890
00:29:42,559 --> 00:29:43,840
of these attacks

891
00:29:43,840 --> 00:29:45,200
and then it's attack-like and if you

892
00:29:45,200 --> 00:29:46,720
wanted to integrate it with with some of

893
00:29:46,720 --> 00:29:48,399
the other kind of attack ecosystem of

894
00:29:48,399 --> 00:29:51,039
models it's it's not that hard

895
00:29:51,039 --> 00:29:53,440
so with that said i had kind of a step

896
00:29:53,440 --> 00:29:56,080
attack workflow the first was and again

897
00:29:56,080 --> 00:29:57,679
i'm using kind of the atlas notation

898
00:29:57,679 --> 00:30:00,240
aml.t0016

899
00:30:00,240 --> 00:30:02,320
i was going to obtain adver obtain

900
00:30:02,320 --> 00:30:03,600
capabilities you know getting some

901
00:30:03,600 --> 00:30:05,279
adversarial ml stuff you know kind of

902
00:30:05,279 --> 00:30:06,399
locally

903
00:30:06,399 --> 00:30:08,960
i already did this um i've got malware

904
00:30:08,960 --> 00:30:11,200
rl i've got second ml malware i'm good

905
00:30:11,200 --> 00:30:12,559
to go

906
00:30:12,559 --> 00:30:14,919
number two aml

907
00:30:14,919 --> 00:30:19,679
t0040 ml model inference api access

908
00:30:19,679 --> 00:30:20,960
this one's kind of provided by the

909
00:30:20,960 --> 00:30:22,960
competition they've set up an api and i

910
00:30:22,960 --> 00:30:25,120
can query it to access

911
00:30:25,120 --> 00:30:26,640
these two target models that i'm

912
00:30:26,640 --> 00:30:28,880
specifically focused on

913
00:30:28,880 --> 00:30:31,360
then uh ml attack staging i'm going to

914
00:30:31,360 --> 00:30:34,000
use replicate ml model the idea is that

915
00:30:34,000 --> 00:30:36,000
i'm going to throw a bunch of data at

916
00:30:36,000 --> 00:30:38,559
amsquare and kip amsquare in secret the

917
00:30:38,559 --> 00:30:40,559
two models that are kind of giving me

918
00:30:40,559 --> 00:30:42,720
grief on the scoreboard and i'm going to

919
00:30:42,720 --> 00:30:44,640
query them a bunch and based on the

920
00:30:44,640 --> 00:30:45,919
results of those queries i'm going to

921
00:30:45,919 --> 00:30:48,399
train a model that i think can replicate

922
00:30:48,399 --> 00:30:50,880
what they detect

923
00:30:50,880 --> 00:30:53,120
and then i'm gonna do attack staging

924
00:30:53,120 --> 00:30:55,520
adversarial um

925
00:30:55,520 --> 00:30:57,360
craft adversarial data black box

926
00:30:57,360 --> 00:30:59,919
transfer the idea is that i'm gonna i've

927
00:30:59,919 --> 00:31:01,840
got this replicated model i'm gonna

928
00:31:01,840 --> 00:31:04,000
construct a bunch of stuff locally see

929
00:31:04,000 --> 00:31:06,399
if it bypasses my replicated model and

930
00:31:06,399 --> 00:31:07,919
then if it does

931
00:31:07,919 --> 00:31:09,600
let me get to the last one impact evade

932
00:31:09,600 --> 00:31:11,039
ml model i'm going to submit those that

933
00:31:11,039 --> 00:31:13,039
get by i'm also going to kind of put

934
00:31:13,039 --> 00:31:14,880
guardrails for myself so that when i do

935
00:31:14,880 --> 00:31:16,480
submit an attack against the target

936
00:31:16,480 --> 00:31:17,519
models

937
00:31:17,519 --> 00:31:19,360
um i try to guarantee that it will be

938
00:31:19,360 --> 00:31:21,919
detected by kipple so i don't end up

939
00:31:21,919 --> 00:31:23,279
hurting myself right i want to do that

940
00:31:23,279 --> 00:31:25,600
relative and balance

941
00:31:25,600 --> 00:31:27,360
just real quickly one of the nice things

942
00:31:27,360 --> 00:31:29,120
about having a matrix is you can

943
00:31:29,120 --> 00:31:31,279
visualize it we've got this nice little

944
00:31:31,279 --> 00:31:33,679
workflow of resource development ml

945
00:31:33,679 --> 00:31:35,679
model access attack staging and then

946
00:31:35,679 --> 00:31:37,600
impact so again if you haven't checked

947
00:31:37,600 --> 00:31:39,600
out atlas atlas.mirror.org it's a lot of

948
00:31:39,600 --> 00:31:40,880
fun

949
00:31:40,880 --> 00:31:42,000
all right

950
00:31:42,000 --> 00:31:44,240
so profiling the competition um what i

951
00:31:44,240 --> 00:31:46,559
did is you know this is the

952
00:31:46,559 --> 00:31:49,039
ml model inference api access along with

953
00:31:49,039 --> 00:31:51,679
replicate ml model here i submitted some

954
00:31:51,679 --> 00:31:53,440
benign and adversarial samples to the

955
00:31:53,440 --> 00:31:55,360
two target models that i was worried

956
00:31:55,360 --> 00:31:56,399
about

957
00:31:56,399 --> 00:31:57,840
these were just taken from kipple's

958
00:31:57,840 --> 00:31:59,279
training data so the same data i was

959
00:31:59,279 --> 00:32:00,960
talking before

960
00:32:00,960 --> 00:32:01,760
i

961
00:32:01,760 --> 00:32:05,200
submitted roughly 20 000 things in total

962
00:32:05,200 --> 00:32:07,679
all of the benign wear all 2500 and then

963
00:32:07,679 --> 00:32:09,519
a lot of malware rl

964
00:32:09,519 --> 00:32:11,519
and some of the second ml malware data

965
00:32:11,519 --> 00:32:12,640
as well

966
00:32:12,640 --> 00:32:14,559
and you can look at the results and you

967
00:32:14,559 --> 00:32:16,399
can kind of see the the accuracy over

968
00:32:16,399 --> 00:32:18,960
here um they both do well on malware rl

969
00:32:18,960 --> 00:32:21,760
and gamma or which is second ml malware

970
00:32:21,760 --> 00:32:23,760
but what's interesting is that they both

971
00:32:23,760 --> 00:32:24,960
kind of take a little bit of a hit with

972
00:32:24,960 --> 00:32:28,039
the benign wear secret in particular um

973
00:32:28,039 --> 00:32:30,799
misclassifies 60 of the benign wear that

974
00:32:30,799 --> 00:32:33,760
i submitted as malware so it was very

975
00:32:33,760 --> 00:32:36,720
sensitive to anything being thrown at it

976
00:32:36,720 --> 00:32:38,720
um definitely not one percent right

977
00:32:38,720 --> 00:32:39,840
there

978
00:32:39,840 --> 00:32:41,039
um

979
00:32:41,039 --> 00:32:43,120
so i got all those labels and what i did

980
00:32:43,120 --> 00:32:44,880
is i just trained another ember like

981
00:32:44,880 --> 00:32:47,279
model just following those labels

982
00:32:47,279 --> 00:32:49,919
and you know basically saying if amp

983
00:32:49,919 --> 00:32:52,480
square or a secret they classified a

984
00:32:52,480 --> 00:32:54,640
file as benign regardless of the truth

985
00:32:54,640 --> 00:32:57,519
assign it to label zero otherwise one

986
00:32:57,519 --> 00:32:59,600
and then i use the same parameters as

987
00:32:59,600 --> 00:33:01,279
kipple effectively just kind of building

988
00:33:01,279 --> 00:33:03,200
a model that looks like what i'm trying

989
00:33:03,200 --> 00:33:04,640
to evade

990
00:33:04,640 --> 00:33:06,880
then to submit the attack i just ran

991
00:33:06,880 --> 00:33:08,960
malware rl and second ml malware on

992
00:33:08,960 --> 00:33:13,120
those 50 samples i need to make evasive

993
00:33:13,120 --> 00:33:16,080
and i generate a random variance

994
00:33:16,080 --> 00:33:17,760
i then

995
00:33:17,760 --> 00:33:18,960
made sure that the following two

996
00:33:18,960 --> 00:33:21,440
conditions hold that it evades the

997
00:33:21,440 --> 00:33:23,279
replicated model that i have but it's

998
00:33:23,279 --> 00:33:25,360
also detected by kipple again not to

999
00:33:25,360 --> 00:33:26,799
hurt myself

1000
00:33:26,799 --> 00:33:28,320
and then each hour

1001
00:33:28,320 --> 00:33:29,440
more or less

1002
00:33:29,440 --> 00:33:31,679
submit a set of 50 of these samples that

1003
00:33:31,679 --> 00:33:33,360
are supposed to evade the target

1004
00:33:33,360 --> 00:33:34,799
classifier

1005
00:33:34,799 --> 00:33:36,720
and this is um

1006
00:33:36,720 --> 00:33:38,480
just a few notes

1007
00:33:38,480 --> 00:33:40,880
i did rely on a lot of um scripted

1008
00:33:40,880 --> 00:33:41,840
processes there were a lot of

1009
00:33:41,840 --> 00:33:43,039
submissions

1010
00:33:43,039 --> 00:33:46,559
um i did not i used a black box attack

1011
00:33:46,559 --> 00:33:48,240
against the local model which is not the

1012
00:33:48,240 --> 00:33:49,440
way you should do this you should use

1013
00:33:49,440 --> 00:33:51,120
something a little bit better

1014
00:33:51,120 --> 00:33:52,640
and

1015
00:33:52,640 --> 00:33:53,919
i should also just note that this

1016
00:33:53,919 --> 00:33:56,399
approach this entire workflow

1017
00:33:56,399 --> 00:33:58,480
is fun in the context of a competition

1018
00:33:58,480 --> 00:34:00,159
but is unrealistic for what a real

1019
00:34:00,159 --> 00:34:02,159
adversary might do

1020
00:34:02,159 --> 00:34:04,240
so how did it work and again please take

1021
00:34:04,240 --> 00:34:05,760
numbers with a grain of salt i had about

1022
00:34:05,760 --> 00:34:08,800
1600 different submissions as a tax um

1023
00:34:08,800 --> 00:34:10,639
about half of those weren't functional

1024
00:34:10,639 --> 00:34:12,079
or were duplicates so they didn't count

1025
00:34:12,079 --> 00:34:13,440
they were thrown out right away they got

1026
00:34:13,440 --> 00:34:14,560
detonated

1027
00:34:14,560 --> 00:34:16,399
not not good enough but the other half

1028
00:34:16,399 --> 00:34:18,000
761

1029
00:34:18,000 --> 00:34:20,480
were eligible for scoring

1030
00:34:20,480 --> 00:34:22,960
and i got a bunch of results to see how

1031
00:34:22,960 --> 00:34:24,719
how i did and when you look at the three

1032
00:34:24,719 --> 00:34:26,560
middle columns these are the three

1033
00:34:26,560 --> 00:34:28,079
models i wasn't too concerned about you

1034
00:34:28,079 --> 00:34:29,679
can see we've got a good success rate

1035
00:34:29,679 --> 00:34:31,599
evading them even though that wasn't my

1036
00:34:31,599 --> 00:34:34,879
goal um it was you know we can get 500

1037
00:34:34,879 --> 00:34:37,440
170 290 right was able to get a lot of

1038
00:34:37,440 --> 00:34:39,280
good points against these um wasn't the

1039
00:34:39,280 --> 00:34:42,000
goal but you know hey nice benefit

1040
00:34:42,000 --> 00:34:43,599
against am square though

1041
00:34:43,599 --> 00:34:46,719
only get 49 evasions out of 761 and and

1042
00:34:46,719 --> 00:34:49,119
that's really not a lot and not nearly

1043
00:34:49,119 --> 00:34:50,960
as much as i was hoping to have gotten

1044
00:34:50,960 --> 00:34:52,719
from running this attack

1045
00:34:52,719 --> 00:34:54,719
against kippel a little bit of friendly

1046
00:34:54,719 --> 00:34:57,359
fire um five of the ones i submitted got

1047
00:34:57,359 --> 00:34:59,040
by it just because of some bad practices

1048
00:34:59,040 --> 00:35:01,680
on my end i won't talk about them but

1049
00:35:01,680 --> 00:35:04,079
secret got nothing um

1050
00:35:04,079 --> 00:35:05,520
couldn't get it to

1051
00:35:05,520 --> 00:35:08,960
misclassify anything um it was just

1052
00:35:08,960 --> 00:35:11,280
nope it was able to detect everything i

1053
00:35:11,280 --> 00:35:13,680
threw at it and so okay

1054
00:35:13,680 --> 00:35:15,280
i apologize for the graphics but that

1055
00:35:15,280 --> 00:35:17,040
didn't work um

1056
00:35:17,040 --> 00:35:18,560
i was able to get a few points off of

1057
00:35:18,560 --> 00:35:21,599
amsquare but nothing on secret and i've

1058
00:35:21,599 --> 00:35:23,280
got about maybe a week week and a half

1059
00:35:23,280 --> 00:35:25,280
left in in the competition

1060
00:35:25,280 --> 00:35:26,240
i'm

1061
00:35:26,240 --> 00:35:28,320
committed at this point so i want to try

1062
00:35:28,320 --> 00:35:30,960
something so what else can i do well

1063
00:35:30,960 --> 00:35:33,920
what if i look at you know different

1064
00:35:33,920 --> 00:35:36,000
different individual instances i can see

1065
00:35:36,000 --> 00:35:38,000
here this is a chart of

1066
00:35:38,000 --> 00:35:40,240
how many evasions per sample there were

1067
00:35:40,240 --> 00:35:42,079
and you can see some samples were easy

1068
00:35:42,079 --> 00:35:44,400
to get by some of the classifiers

1069
00:35:44,400 --> 00:35:47,119
and some were a lot harder to get by the

1070
00:35:47,119 --> 00:35:48,480
classifiers

1071
00:35:48,480 --> 00:35:50,160
and if we focus specifically on am

1072
00:35:50,160 --> 00:35:54,400
square we can see that samples 2 and 18

1073
00:35:54,400 --> 00:35:57,440
we most of the 49 that got by it were

1074
00:35:57,440 --> 00:35:59,280
those two individual samples it was

1075
00:35:59,280 --> 00:36:00,720
struggling with those for whatever

1076
00:36:00,720 --> 00:36:01,599
reason

1077
00:36:01,599 --> 00:36:03,040
and so i thought maybe i could just

1078
00:36:03,040 --> 00:36:04,640
focus on these two generate a bunch of

1079
00:36:04,640 --> 00:36:06,560
ones make them really evasive

1080
00:36:06,560 --> 00:36:08,560
but really this is only going to slowly

1081
00:36:08,560 --> 00:36:10,640
generate points versus this one model so

1082
00:36:10,640 --> 00:36:14,000
it's not quite what i'd like to do

1083
00:36:14,000 --> 00:36:15,599
okay so that's not going to work too

1084
00:36:15,599 --> 00:36:17,520
well what about going back to atlas and

1085
00:36:17,520 --> 00:36:19,200
seeing you know what other ttps can we

1086
00:36:19,200 --> 00:36:20,079
use

1087
00:36:20,079 --> 00:36:23,040
i'd say okay obtain capabilities maybe i

1088
00:36:23,040 --> 00:36:25,520
can get a new attack type i'm going to

1089
00:36:25,520 --> 00:36:27,440
find a different framework to use a

1090
00:36:27,440 --> 00:36:29,359
different anti-ml framework and and see

1091
00:36:29,359 --> 00:36:31,280
if this works there's something called

1092
00:36:31,280 --> 00:36:33,920
map malware this uses this is kind of

1093
00:36:33,920 --> 00:36:36,160
similar to malware rl it's an rl a

1094
00:36:36,160 --> 00:36:37,760
reinforcement learning-based framework

1095
00:36:37,760 --> 00:36:39,920
to sequence actions together to evade a

1096
00:36:39,920 --> 00:36:42,079
specific classifier it's open source

1097
00:36:42,079 --> 00:36:44,160
available online um

1098
00:36:44,160 --> 00:36:45,920
dockerized2 was very easy just download

1099
00:36:45,920 --> 00:36:47,440
run get working

1100
00:36:47,440 --> 00:36:49,440
and it's conceptually similar to malware

1101
00:36:49,440 --> 00:36:51,359
rl it's got a very similar action space

1102
00:36:51,359 --> 00:36:54,400
a little bit different um but it's

1103
00:36:54,400 --> 00:36:55,839
really got that reinforcement learning

1104
00:36:55,839 --> 00:36:57,599
you download it

1105
00:36:57,599 --> 00:36:58,800
point it in the direction of something

1106
00:36:58,800 --> 00:37:00,960
and you just let it run and and and it

1107
00:37:00,960 --> 00:37:02,079
works

1108
00:37:02,079 --> 00:37:04,160
so i get it i swap out the included

1109
00:37:04,160 --> 00:37:05,839
model for that replicated model i had

1110
00:37:05,839 --> 00:37:07,839
maybe this this capability will be

1111
00:37:07,839 --> 00:37:10,480
stronger than the other two i'm using

1112
00:37:10,480 --> 00:37:13,040
initially it was able to easily evade

1113
00:37:13,040 --> 00:37:14,960
the model i gave it but all of the pe

1114
00:37:14,960 --> 00:37:17,200
files it generated were like

1115
00:37:17,200 --> 00:37:19,119
massive so it was just like a pending a

1116
00:37:19,119 --> 00:37:21,040
bunch of stuff and eventually it was

1117
00:37:21,040 --> 00:37:23,280
able to evade the classifier but you

1118
00:37:23,280 --> 00:37:24,640
know i had a restriction of two

1119
00:37:24,640 --> 00:37:26,160
megabytes

1120
00:37:26,160 --> 00:37:27,200
so

1121
00:37:27,200 --> 00:37:29,040
i modified it say okay now you can only

1122
00:37:29,040 --> 00:37:31,040
do less than two megabytes and still got

1123
00:37:31,040 --> 00:37:32,000
some

1124
00:37:32,000 --> 00:37:34,800
some evasive files out of it i got 51

1125
00:37:34,800 --> 00:37:37,119
valid submissions that i was able to to

1126
00:37:37,119 --> 00:37:38,960
throw out there but none of them really

1127
00:37:38,960 --> 00:37:41,920
got by only two bypass am square and

1128
00:37:41,920 --> 00:37:44,400
again secret didn't work

1129
00:37:44,400 --> 00:37:47,119
okay so no progress versus the existing

1130
00:37:47,119 --> 00:37:48,960
capabilities um

1131
00:37:48,960 --> 00:37:51,440
and again i'm invested so all right

1132
00:37:51,440 --> 00:37:53,200
going back to the drawing board

1133
00:37:53,200 --> 00:37:55,680
miter atlas what else can i do here um

1134
00:37:55,680 --> 00:37:57,440
got maybe a week a little bit more a

1135
00:37:57,440 --> 00:37:59,920
little bit less left in the competition

1136
00:37:59,920 --> 00:38:01,520
okay what about something fun like

1137
00:38:01,520 --> 00:38:03,520
reconnaissance right it sounds silly but

1138
00:38:03,520 --> 00:38:05,599
maybe i can use reconnaissance

1139
00:38:05,599 --> 00:38:06,720
and

1140
00:38:06,720 --> 00:38:08,880
looking at the scoreboard you can see

1141
00:38:08,880 --> 00:38:10,160
we've got these two columns here i know

1142
00:38:10,160 --> 00:38:11,280
it's a little small but i've got the

1143
00:38:11,280 --> 00:38:13,920
author i've got the model name

1144
00:38:13,920 --> 00:38:16,800
so the first one um you can see fm by

1145
00:38:16,800 --> 00:38:18,560
something or other it it's a random

1146
00:38:18,560 --> 00:38:20,880
string that's nothing i can't use that

1147
00:38:20,880 --> 00:38:23,599
secret is the name of a model but secret

1148
00:38:23,599 --> 00:38:24,880
is probably secret because they're

1149
00:38:24,880 --> 00:38:27,040
hiding it that's what i thought

1150
00:38:27,040 --> 00:38:28,640
so that one's out

1151
00:38:28,640 --> 00:38:30,720
a1 that's too generic don't think i can

1152
00:38:30,720 --> 00:38:32,640
use that but am square well what about

1153
00:38:32,640 --> 00:38:34,079
that and so

1154
00:38:34,079 --> 00:38:36,240
i go to google throw an am square and

1155
00:38:36,240 --> 00:38:38,320
bam i get a personal web page and a

1156
00:38:38,320 --> 00:38:40,720
github repository and i think this is

1157
00:38:40,720 --> 00:38:42,480
wonderful i can look through and see if

1158
00:38:42,480 --> 00:38:44,480
there's some adversarial ml or some you

1159
00:38:44,480 --> 00:38:46,400
know ml stuff on there for malware

1160
00:38:46,400 --> 00:38:47,760
maybe point me in the right direction of

1161
00:38:47,760 --> 00:38:49,040
what i need to do

1162
00:38:49,040 --> 00:38:50,320
but unfortunately there was nothing

1163
00:38:50,320 --> 00:38:52,240
about malware so

1164
00:38:52,240 --> 00:38:53,920
that didn't work

1165
00:38:53,920 --> 00:38:54,720
but

1166
00:38:54,720 --> 00:38:56,480
i wasn't quite done yet

1167
00:38:56,480 --> 00:38:58,800
the competition had a public slack

1168
00:38:58,800 --> 00:39:01,200
channel and you can see on september

1169
00:39:01,200 --> 00:39:02,720
10th a little bit before the deadline

1170
00:39:02,720 --> 00:39:04,320
for the attacker track

1171
00:39:04,320 --> 00:39:07,200
the admin posted the current scoreboard

1172
00:39:07,200 --> 00:39:09,359
and i noticed little thumbs up there

1173
00:39:09,359 --> 00:39:12,000
mouseover and i got a name and on a whim

1174
00:39:12,000 --> 00:39:15,119
i throw the name into google

1175
00:39:15,119 --> 00:39:17,440
i know this is this is crazy

1176
00:39:17,440 --> 00:39:20,880
this is crazy i i if you ever invest in

1177
00:39:20,880 --> 00:39:22,480
something

1178
00:39:22,480 --> 00:39:24,800
you just go down the rabbit hole

1179
00:39:24,800 --> 00:39:26,320
but i throw the name in there and i get

1180
00:39:26,320 --> 00:39:28,000
a bunch of hits i get a twitter a

1181
00:39:28,000 --> 00:39:30,880
scholar and a github and i get that

1182
00:39:30,880 --> 00:39:32,720
fabricio is an author at something

1183
00:39:32,720 --> 00:39:35,040
called secret which

1184
00:39:35,040 --> 00:39:36,960
turns out

1185
00:39:36,960 --> 00:39:37,680
to

1186
00:39:37,680 --> 00:39:39,359
oh skipping ahead a little bit an

1187
00:39:39,359 --> 00:39:41,440
academic research lab secret isn't like

1188
00:39:41,440 --> 00:39:43,040
we're hiding the my model name that's

1189
00:39:43,040 --> 00:39:45,440
the name of the lab they focus on you

1190
00:39:45,440 --> 00:39:48,320
know security and ml stuff and they won

1191
00:39:48,320 --> 00:39:51,359
the 2020 ammo set competition wrote a

1192
00:39:51,359 --> 00:39:54,320
paper about it that's freely available

1193
00:39:54,320 --> 00:39:56,560
have a blog post about what they did in

1194
00:39:56,560 --> 00:39:57,760
2020

1195
00:39:57,760 --> 00:40:00,320
and have access to their model their

1196
00:40:00,320 --> 00:40:02,400
classifier online that i can go to and

1197
00:40:02,400 --> 00:40:03,839
just check out

1198
00:40:03,839 --> 00:40:06,319
but it gets better that's not it no

1199
00:40:06,319 --> 00:40:09,359
in the blog post they've got the code

1200
00:40:09,359 --> 00:40:11,680
for their 2020 model on their not the

1201
00:40:11,680 --> 00:40:13,200
data but the code

1202
00:40:13,200 --> 00:40:15,760
they got how they won 2020 and they got

1203
00:40:15,760 --> 00:40:18,800
the dropper that they used in 2020

1204
00:40:18,800 --> 00:40:20,480
and finally i'd found what i've been

1205
00:40:20,480 --> 00:40:21,680
looking for i think this is going to be

1206
00:40:21,680 --> 00:40:23,760
great and after a day or two i kind of

1207
00:40:23,760 --> 00:40:24,720
give up

1208
00:40:24,720 --> 00:40:25,920
um

1209
00:40:25,920 --> 00:40:28,319
my time was too short i needed more data

1210
00:40:28,319 --> 00:40:29,839
to replicate their model changing my

1211
00:40:29,839 --> 00:40:31,119
infrastructure

1212
00:40:31,119 --> 00:40:33,040
and even if i replicated it i didn't

1213
00:40:33,040 --> 00:40:35,200
think malware rl and second ml malware

1214
00:40:35,200 --> 00:40:37,200
would be enough to get by it

1215
00:40:37,200 --> 00:40:38,640
the dropper that they gave looked really

1216
00:40:38,640 --> 00:40:40,319
promising but i couldn't get it to work

1217
00:40:40,319 --> 00:40:42,240
on my linux vm

1218
00:40:42,240 --> 00:40:46,000
and admittedly i'm not big on i i'm very

1219
00:40:46,000 --> 00:40:47,839
much at the depths of my knowledge here

1220
00:40:47,839 --> 00:40:48,720
and

1221
00:40:48,720 --> 00:40:50,560
was getting kind of tired and frustrated

1222
00:40:50,560 --> 00:40:51,359
and

1223
00:40:51,359 --> 00:40:52,640
just giving up

1224
00:40:52,640 --> 00:40:56,079
just a little bit but not entirely

1225
00:40:56,079 --> 00:40:57,839
that did give me some ideas of okay

1226
00:40:57,839 --> 00:40:59,760
maybe i can find something easier to use

1227
00:40:59,760 --> 00:41:02,000
under traditional evasion so let's focus

1228
00:41:02,000 --> 00:41:04,000
on traditional evasion i went on github

1229
00:41:04,000 --> 00:41:05,440
looking for all sorts of just open

1230
00:41:05,440 --> 00:41:07,760
source crypters packers droppers setting

1231
00:41:07,760 --> 00:41:09,280
just a threshold of if i can't get this

1232
00:41:09,280 --> 00:41:11,280
to work right away i'm just gonna gonna

1233
00:41:11,280 --> 00:41:12,800
give up

1234
00:41:12,800 --> 00:41:14,240
and unfortunately most i couldn't i

1235
00:41:14,240 --> 00:41:15,599
either couldn't get to work and those

1236
00:41:15,599 --> 00:41:17,520
that i could get to work either produced

1237
00:41:17,520 --> 00:41:18,880
output that

1238
00:41:18,880 --> 00:41:21,359
wasn't a valid pe file it didn't run

1239
00:41:21,359 --> 00:41:24,079
uh or it didn't evade secret or amscore

1240
00:41:24,079 --> 00:41:26,800
so it wasn't actually helping

1241
00:41:26,800 --> 00:41:29,040
and working or not many of the files

1242
00:41:29,040 --> 00:41:30,800
that were outputted actually bypassed

1243
00:41:30,800 --> 00:41:33,119
kipple just because this was one of the

1244
00:41:33,119 --> 00:41:35,040
huge shortcuttings of kippel was it had

1245
00:41:35,040 --> 00:41:38,160
no knowledge of traditional evasion

1246
00:41:38,160 --> 00:41:40,720
that said um one package did stick out

1247
00:41:40,720 --> 00:41:42,800
to me call or one one program called

1248
00:41:42,800 --> 00:41:45,760
dark armor this is a windows avpe

1249
00:41:45,760 --> 00:41:47,280
evasion tool it's open source it's

1250
00:41:47,280 --> 00:41:49,040
python based so i can run it in my linux

1251
00:41:49,040 --> 00:41:50,800
vm

1252
00:41:50,800 --> 00:41:52,640
really really easy to use give it an

1253
00:41:52,640 --> 00:41:54,319
input pe file it's going to spit out a

1254
00:41:54,319 --> 00:41:56,319
new one with the original encrypted

1255
00:41:56,319 --> 00:41:59,680
and the new pe gonna

1256
00:41:59,680 --> 00:42:01,680
execute the old one in memory piece of

1257
00:42:01,680 --> 00:42:03,839
cake and best of all is i can get it to

1258
00:42:03,839 --> 00:42:05,280
run and it works

1259
00:42:05,280 --> 00:42:07,440
it's actually able to consistently evade

1260
00:42:07,440 --> 00:42:09,599
all six of the classifiers

1261
00:42:09,599 --> 00:42:12,079
and not only that what i can do is i

1262
00:42:12,079 --> 00:42:14,240
take the output from dark armor which is

1263
00:42:14,240 --> 00:42:16,640
evasive to everybody including kipple

1264
00:42:16,640 --> 00:42:18,800
i then run it through malware rl one or

1265
00:42:18,800 --> 00:42:19,920
two times

1266
00:42:19,920 --> 00:42:21,520
not to really make it evasive but to

1267
00:42:21,520 --> 00:42:24,000
make it signatureable by kibble because

1268
00:42:24,000 --> 00:42:25,680
kipple is able to pick up anything going

1269
00:42:25,680 --> 00:42:28,160
through malware rl and now the output

1270
00:42:28,160 --> 00:42:30,640
from the tool is able to get by

1271
00:42:30,640 --> 00:42:32,880
secret while being detected by kipple

1272
00:42:32,880 --> 00:42:34,560
again this is a contest i'm gaming the

1273
00:42:34,560 --> 00:42:36,800
system just a little bit

1274
00:42:36,800 --> 00:42:40,480
bad news though um the pe executes but

1275
00:42:40,480 --> 00:42:41,280
it

1276
00:42:41,280 --> 00:42:42,839
there's something wrong with the payload

1277
00:42:42,839 --> 00:42:45,200
and it doesn't actually execute the

1278
00:42:45,200 --> 00:42:47,920
payload so the file executes but

1279
00:42:47,920 --> 00:42:49,280
something's going wrong and i try a

1280
00:42:49,280 --> 00:42:51,440
billion options and eventually look at

1281
00:42:51,440 --> 00:42:53,440
the github issues and find out

1282
00:42:53,440 --> 00:42:55,440
everybody else was having the same

1283
00:42:55,440 --> 00:42:57,119
problems as well they couldn't get this

1284
00:42:57,119 --> 00:43:00,319
to run but i wasn't just ready to get up

1285
00:43:00,319 --> 00:43:02,640
i to give up i was trying one one more

1286
00:43:02,640 --> 00:43:03,680
thing i'm going to get this to work i'm

1287
00:43:03,680 --> 00:43:05,920
going to use donut and pizzor i'm going

1288
00:43:05,920 --> 00:43:07,119
to use donut it's open source i'm going

1289
00:43:07,119 --> 00:43:09,680
to convert my pe file to shell code i'm

1290
00:43:09,680 --> 00:43:11,760
going to throw that into pe zor and

1291
00:43:11,760 --> 00:43:13,040
encrypt it a little bit again open

1292
00:43:13,040 --> 00:43:15,520
source freely available and i i invest a

1293
00:43:15,520 --> 00:43:16,560
lot of time making this work and

1294
00:43:16,560 --> 00:43:18,560
everything and it can evade all

1295
00:43:18,560 --> 00:43:21,119
classifiers like dark armor apply

1296
00:43:21,119 --> 00:43:23,839
malware rl couple detects it but

1297
00:43:23,839 --> 00:43:25,200
doesn't work

1298
00:43:25,200 --> 00:43:27,920
and at this point i i get on to

1299
00:43:27,920 --> 00:43:29,280
you know

1300
00:43:29,280 --> 00:43:30,800
slack i reach out to the author he's

1301
00:43:30,800 --> 00:43:32,560
going to help me a little bit and time's

1302
00:43:32,560 --> 00:43:33,920
up

1303
00:43:33,920 --> 00:43:35,280
i invested way too much into that

1304
00:43:35,280 --> 00:43:36,800
attacker track my initial thing was i'm

1305
00:43:36,800 --> 00:43:37,920
just going to throw a little bit into it

1306
00:43:37,920 --> 00:43:40,560
and you can see i got very carried away

1307
00:43:40,560 --> 00:43:43,200
so okay so i ran out of time i did have

1308
00:43:43,200 --> 00:43:44,880
a lot of fun doing that but

1309
00:43:44,880 --> 00:43:46,079
you know ultimately couldn't get any of

1310
00:43:46,079 --> 00:43:48,400
those other attacks to work

1311
00:43:48,400 --> 00:43:50,240
so what ended up happening what were the

1312
00:43:50,240 --> 00:43:52,720
results um i know the title of my talk

1313
00:43:52,720 --> 00:43:54,079
gave it away but still going to walk

1314
00:43:54,079 --> 00:43:56,240
through it

1315
00:43:56,240 --> 00:43:57,920
walking through you can see secret took

1316
00:43:57,920 --> 00:44:00,560
first place they had 162 bypasses they

1317
00:44:00,560 --> 00:44:01,760
were the lowest

1318
00:44:01,760 --> 00:44:05,520
am square took second with 193

1319
00:44:05,520 --> 00:44:08,720
and kippel a convincing third with 231

1320
00:44:08,720 --> 00:44:09,920
and the other three models all in

1321
00:44:09,920 --> 00:44:11,359
between

1322
00:44:11,359 --> 00:44:13,200
on the attacker side the results are

1323
00:44:13,200 --> 00:44:15,280
pretty similar um more competitors

1324
00:44:15,280 --> 00:44:17,440
definitely than the defender's side but

1325
00:44:17,440 --> 00:44:20,079
secret took first they did

1326
00:44:20,079 --> 00:44:23,359
a you know a great job 196 bypasses they

1327
00:44:23,359 --> 00:44:25,839
got out of 300 but they only used 600

1328
00:44:25,839 --> 00:44:28,240
api queries which was pretty low

1329
00:44:28,240 --> 00:44:31,280
am square took a second with 167 and a

1330
00:44:31,280 --> 00:44:34,480
moderate amount of api queries and

1331
00:44:34,480 --> 00:44:35,359
me

1332
00:44:35,359 --> 00:44:39,040
um you can see me i'm our wch yes a

1333
00:44:39,040 --> 00:44:41,520
modest 55

1334
00:44:41,520 --> 00:44:44,000
000 api queries

1335
00:44:44,000 --> 00:44:46,079
um as i said i got a little invested all

1336
00:44:46,079 --> 00:44:48,000
right

1337
00:44:48,000 --> 00:44:48,839
all right

1338
00:44:48,839 --> 00:44:51,280
so doing a little bit of critical

1339
00:44:51,280 --> 00:44:52,800
analysis of you know what happened you

1340
00:44:52,800 --> 00:44:54,000
know a little bit of retrospective

1341
00:44:54,000 --> 00:44:55,280
because of course you know i'm curious

1342
00:44:55,280 --> 00:44:56,560
well number one

1343
00:44:56,560 --> 00:44:58,319
kipple had a really low false positive

1344
00:44:58,319 --> 00:44:59,680
rate i

1345
00:44:59,680 --> 00:45:01,359
there's more in if you're interested in

1346
00:45:01,359 --> 00:45:02,880
the paper and the other material on how

1347
00:45:02,880 --> 00:45:04,640
i went about the false positive rate but

1348
00:45:04,640 --> 00:45:06,480
it was a little

1349
00:45:06,480 --> 00:45:08,560
overly rigorous there

1350
00:45:08,560 --> 00:45:10,640
but number two kipple still missed a lot

1351
00:45:10,640 --> 00:45:13,040
of the malware rl style attacks if you

1352
00:45:13,040 --> 00:45:16,000
just let malware rl run

1353
00:45:16,000 --> 00:45:17,440
it'll eventually get by kippel just

1354
00:45:17,440 --> 00:45:19,520
acting randomly and the reinforcement

1355
00:45:19,520 --> 00:45:20,960
learning map malware that other

1356
00:45:20,960 --> 00:45:23,440
capability it relatively handily was

1357
00:45:23,440 --> 00:45:25,280
able to bypass kipple without too much

1358
00:45:25,280 --> 00:45:26,800
difficulty

1359
00:45:26,800 --> 00:45:28,079
but number three which i think was

1360
00:45:28,079 --> 00:45:29,680
probably the most important for why the

1361
00:45:29,680 --> 00:45:31,359
scores were so high is

1362
00:45:31,359 --> 00:45:33,440
you know it kippel just lacked knowledge

1363
00:45:33,440 --> 00:45:35,680
of the traditional non-ml evasion

1364
00:45:35,680 --> 00:45:37,680
techniques crypter's packers you know a

1365
00:45:37,680 --> 00:45:39,680
lot of the normal off-the-shelf stuff

1366
00:45:39,680 --> 00:45:41,680
was enough to get by it without without

1367
00:45:41,680 --> 00:45:43,599
too much of a

1368
00:45:43,599 --> 00:45:45,119
hurdle

1369
00:45:45,119 --> 00:45:47,119
on the attacker side i think the

1370
00:45:47,119 --> 00:45:49,040
attacker idea that i was following using

1371
00:45:49,040 --> 00:45:50,640
atlas walking through those steps it had

1372
00:45:50,640 --> 00:45:53,920
merit but overall had mixed results

1373
00:45:53,920 --> 00:45:56,240
and this is a few for a few reasons um

1374
00:45:56,240 --> 00:45:57,520
after the fact

1375
00:45:57,520 --> 00:46:00,160
amsquare ended up publishing a a little

1376
00:46:00,160 --> 00:46:02,319
white paper on on what they did

1377
00:46:02,319 --> 00:46:04,880
for both tracks and in their defensive

1378
00:46:04,880 --> 00:46:06,960
model they also used an ensemble

1379
00:46:06,960 --> 00:46:09,040
approach a variety of methods

1380
00:46:09,040 --> 00:46:10,560
they used

1381
00:46:10,560 --> 00:46:12,079
gradient boosted decision trees just

1382
00:46:12,079 --> 00:46:14,079
like kipple did but also neural networks

1383
00:46:14,079 --> 00:46:16,720
and the fun one um some rules that that

1384
00:46:16,720 --> 00:46:17,920
they threw in there for anomaly

1385
00:46:17,920 --> 00:46:19,280
detection

1386
00:46:19,280 --> 00:46:21,119
and from a data perspective they use

1387
00:46:21,119 --> 00:46:23,839
normal ember and sorel so a lot of data

1388
00:46:23,839 --> 00:46:26,240
um you know sorrell i only used 32 000

1389
00:46:26,240 --> 00:46:28,319
and i think

1390
00:46:28,319 --> 00:46:30,880
there's 20 million in that entire data

1391
00:46:30,880 --> 00:46:33,520
set so a lot a lot of training data

1392
00:46:33,520 --> 00:46:35,280
but probably more importantly they used

1393
00:46:35,280 --> 00:46:37,440
a stateful defense which compared the

1394
00:46:37,440 --> 00:46:38,960
incoming samples to the previously

1395
00:46:38,960 --> 00:46:41,200
classified ones so me throwing a

1396
00:46:41,200 --> 00:46:43,359
bajillion things out there

1397
00:46:43,359 --> 00:46:45,040
didn't really help because it's if one

1398
00:46:45,040 --> 00:46:46,720
of those gets classified as malware the

1399
00:46:46,720 --> 00:46:48,800
others are going to look like it too

1400
00:46:48,800 --> 00:46:50,720
and and and that along with those rules

1401
00:46:50,720 --> 00:46:52,079
for anomaly detection i think that's

1402
00:46:52,079 --> 00:46:54,240
where i ended up getting getting hung up

1403
00:46:54,240 --> 00:46:56,160
um being unable to evade m square

1404
00:46:56,160 --> 00:46:58,240
reasonably reliably

1405
00:46:58,240 --> 00:46:59,760
secret they also put it right up out

1406
00:46:59,760 --> 00:47:01,200
there um

1407
00:47:01,200 --> 00:47:03,599
and i was surprised they they're they're

1408
00:47:03,599 --> 00:47:05,760
um

1409
00:47:05,760 --> 00:47:07,200
maybe i misread it but their model

1410
00:47:07,200 --> 00:47:08,720
didn't seem too intense it was a random

1411
00:47:08,720 --> 00:47:10,560
forest with some custom feature

1412
00:47:10,560 --> 00:47:11,680
extraction

1413
00:47:11,680 --> 00:47:14,640
um nothing to to too heavy there but

1414
00:47:14,640 --> 00:47:16,000
then from a training perspective they

1415
00:47:16,000 --> 00:47:18,640
only use the ember data which isn't much

1416
00:47:18,640 --> 00:47:20,720
along with 50 normal malware samples

1417
00:47:20,720 --> 00:47:23,520
from the 2020 competition which again

1418
00:47:23,520 --> 00:47:25,599
isn't a whole lot but

1419
00:47:25,599 --> 00:47:27,520
they acknowledge that they have a very

1420
00:47:27,520 --> 00:47:30,079
high false positive rate and they even

1421
00:47:30,079 --> 00:47:31,920
put that little graph in there showing

1422
00:47:31,920 --> 00:47:33,440
that when it came to the competition

1423
00:47:33,440 --> 00:47:34,400
entry

1424
00:47:34,400 --> 00:47:36,480
what the what the organizers were doing

1425
00:47:36,480 --> 00:47:38,880
was they were looking at just windows

1426
00:47:38,880 --> 00:47:40,480
false positive identification kind of

1427
00:47:40,480 --> 00:47:42,800
the built-in utilities and it did and

1428
00:47:42,800 --> 00:47:45,040
their submission did great there but

1429
00:47:45,040 --> 00:47:46,960
they also looked at these other benign

1430
00:47:46,960 --> 00:47:49,040
wear sets where the false positive rates

1431
00:47:49,040 --> 00:47:51,440
up in the 80s so it was a very sensitive

1432
00:47:51,440 --> 00:47:53,920
model and because the organizers were

1433
00:47:53,920 --> 00:47:56,079
only looking at the blue box up here

1434
00:47:56,079 --> 00:47:57,920
they were able to meet that one percent

1435
00:47:57,920 --> 00:48:00,960
um classification rate

1436
00:48:00,960 --> 00:48:03,040
so a couple of just quick closing

1437
00:48:03,040 --> 00:48:04,720
thoughts um lessons learned takeaways

1438
00:48:04,720 --> 00:48:06,640
things like that if you do end up

1439
00:48:06,640 --> 00:48:07,760
wanting to get into this building your

1440
00:48:07,760 --> 00:48:09,359
own classifier a couple of points i'd

1441
00:48:09,359 --> 00:48:10,319
recommend

1442
00:48:10,319 --> 00:48:11,599
make sure you have enough space and you

1443
00:48:11,599 --> 00:48:13,680
dedicate enough time

1444
00:48:13,680 --> 00:48:15,359
it can be time consuming you can't do it

1445
00:48:15,359 --> 00:48:18,160
quickly and on a space budget but if you

1446
00:48:18,160 --> 00:48:19,680
really want to get into it and dive in

1447
00:48:19,680 --> 00:48:20,400
deep

1448
00:48:20,400 --> 00:48:22,240
make sure you've got both and also just

1449
00:48:22,240 --> 00:48:24,160
keep good records of everything you're

1450
00:48:24,160 --> 00:48:26,079
doing so you don't end up reinventing

1451
00:48:26,079 --> 00:48:27,599
the wheel or retrying stuff when you

1452
00:48:27,599 --> 00:48:28,960
don't need to

1453
00:48:28,960 --> 00:48:31,839
when you're attacking a classifier

1454
00:48:31,839 --> 00:48:34,160
i think atlas is a great way to reorient

1455
00:48:34,160 --> 00:48:36,000
or not reorient yourself but orient

1456
00:48:36,000 --> 00:48:37,839
yourself and kind of build an attack to

1457
00:48:37,839 --> 00:48:39,200
go back to the drawing board kind of

1458
00:48:39,200 --> 00:48:40,480
iterate that way

1459
00:48:40,480 --> 00:48:42,079
um it's a good way for me to at least

1460
00:48:42,079 --> 00:48:44,000
conceptualize what i was trying

1461
00:48:44,000 --> 00:48:45,839
and then reconnaissance can be fun and

1462
00:48:45,839 --> 00:48:47,440
you can go down a fun rabbit hole even

1463
00:48:47,440 --> 00:48:49,440
for adversarial ai

1464
00:48:49,440 --> 00:48:50,800
there's a reason it's included is that

1465
00:48:50,800 --> 00:48:52,960
little column there

1466
00:48:52,960 --> 00:48:54,160
also another takeaway i think

1467
00:48:54,160 --> 00:48:55,920
adversarial ml is a lot of fun there's

1468
00:48:55,920 --> 00:48:57,520
tons of free stuff out there if you want

1469
00:48:57,520 --> 00:49:00,240
to get into and want to get started

1470
00:49:00,240 --> 00:49:01,760
i think it's a great thing to get into

1471
00:49:01,760 --> 00:49:04,720
and the last takeaway is um contests can

1472
00:49:04,720 --> 00:49:07,040
be gamed a little bit and i am very

1473
00:49:07,040 --> 00:49:08,880
guilty of that

1474
00:49:08,880 --> 00:49:10,400
and then a couple of quick notes on

1475
00:49:10,400 --> 00:49:12,480
future work and discussion um

1476
00:49:12,480 --> 00:49:13,839
all these materials they will be

1477
00:49:13,839 --> 00:49:15,839
released if not already

1478
00:49:15,839 --> 00:49:17,920
this deck the code behind kipple it's

1479
00:49:17,920 --> 00:49:20,079
all out there on on github

1480
00:49:20,079 --> 00:49:21,599
i've also got on

1481
00:49:21,599 --> 00:49:22,960
on the github repository a bunch of

1482
00:49:22,960 --> 00:49:23,839
links

1483
00:49:23,839 --> 00:49:26,160
that are to others working in this space

1484
00:49:26,160 --> 00:49:29,440
doing a better job explaining it

1485
00:49:29,440 --> 00:49:31,599
my infinite free time which is

1486
00:49:31,599 --> 00:49:33,119
obviously not infinite but there's lots

1487
00:49:33,119 --> 00:49:35,119
of other things i'd like to do um the

1488
00:49:35,119 --> 00:49:36,800
biggest one was i really wanted to

1489
00:49:36,800 --> 00:49:38,720
measure the applicability of adversarial

1490
00:49:38,720 --> 00:49:41,680
ml on virus total to see what what if

1491
00:49:41,680 --> 00:49:43,040
things get by there

1492
00:49:43,040 --> 00:49:45,520
anecdotally it was hit or miss on on

1493
00:49:45,520 --> 00:49:46,960
some of those but

1494
00:49:46,960 --> 00:49:49,280
subjects for future work and just call

1495
00:49:49,280 --> 00:49:50,720
to action

1496
00:49:50,720 --> 00:49:53,119
if ml sex 2022 is happening i'd highly

1497
00:49:53,119 --> 00:49:54,720
recommend it it was obviously a lot of

1498
00:49:54,720 --> 00:49:55,920
fun

1499
00:49:55,920 --> 00:49:58,000
i hope that that comes out and it's good

1500
00:49:58,000 --> 00:49:59,839
to think about how security and ml

1501
00:49:59,839 --> 00:50:01,680
interact with each other

1502
00:50:01,680 --> 00:50:02,800
so with that i will thank you for

1503
00:50:02,800 --> 00:50:04,079
attending i think i've got a little bit

1504
00:50:04,079 --> 00:50:05,920
of time for questions reach me on

1505
00:50:05,920 --> 00:50:07,280
twitter and then

1506
00:50:07,280 --> 00:50:09,280
the code for kipple is available down

1507
00:50:09,280 --> 00:50:12,280
there

1508
00:50:12,590 --> 00:50:18,639
[Applause]

1509
00:50:26,319 --> 00:50:30,160
yep so so the the question was um

1510
00:50:30,160 --> 00:50:32,240
secret use custom features did i use any

1511
00:50:32,240 --> 00:50:34,880
custom features and no i used everything

1512
00:50:34,880 --> 00:50:38,160
entirely just from ember head away to

1513
00:50:38,160 --> 00:50:39,599
you know kind of a built-in extractor

1514
00:50:39,599 --> 00:50:41,599
for features and i just use that you

1515
00:50:41,599 --> 00:50:46,000
know keeping it very fair very simple

1516
00:50:48,400 --> 00:50:50,400
just curious when does the competition

1517
00:50:50,400 --> 00:50:52,319
get announced and how long do you have

1518
00:50:52,319 --> 00:50:55,280
to prepare for that before you know

1519
00:50:55,280 --> 00:50:57,280
you've got to have something submitted

1520
00:50:57,280 --> 00:51:00,319
i think they announced it in may

1521
00:51:00,319 --> 00:51:02,000
with the defender track because i was

1522
00:51:02,000 --> 00:51:03,520
looking at the calendar was announced on

1523
00:51:03,520 --> 00:51:05,839
june 16th that's when the defender track

1524
00:51:05,839 --> 00:51:08,400
opened and then it closed on july 23rd

1525
00:51:08,400 --> 00:51:10,640
so about five weeks is the amount for

1526
00:51:10,640 --> 00:51:13,359
the defender track

1527
00:51:16,400 --> 00:51:18,079
um regarding the

1528
00:51:18,079 --> 00:51:20,559
the ml evasion like the noi adding noise

1529
00:51:20,559 --> 00:51:22,240
to binaries what does what does that

1530
00:51:22,240 --> 00:51:23,440
type of

1531
00:51:23,440 --> 00:51:24,480
um

1532
00:51:24,480 --> 00:51:26,000
technique actually look like at like a

1533
00:51:26,000 --> 00:51:29,200
code assembly level in the binary

1534
00:51:29,200 --> 00:51:31,200
so like the ttp is used by like malware

1535
00:51:31,200 --> 00:51:32,400
rl

1536
00:51:32,400 --> 00:51:34,400
yeah

1537
00:51:34,400 --> 00:51:36,160
it's been a while like like adding

1538
00:51:36,160 --> 00:51:38,559
additional sections some code overlays

1539
00:51:38,559 --> 00:51:40,480
um flipping a few bits here and there

1540
00:51:40,480 --> 00:51:42,319
it's a lot of little small stuff but the

1541
00:51:42,319 --> 00:51:43,839
idea is like the little small stuff you

1542
00:51:43,839 --> 00:51:45,440
can chain it together

1543
00:51:45,440 --> 00:51:47,200
right into something bigger

1544
00:51:47,200 --> 00:51:49,760
and if we use reinforcement learning to

1545
00:51:49,760 --> 00:51:53,200
as an ai attack technique we can figure

1546
00:51:53,200 --> 00:51:55,520
out the sequences that are good

1547
00:51:55,520 --> 00:51:58,839
if that helps

1548
00:52:06,880 --> 00:52:08,160
yeah there are no other questions thank

1549
00:52:08,160 --> 00:52:10,620
you

1550
00:52:10,620 --> 00:52:15,870
[Applause]

1551
00:52:26,400 --> 00:52:29,400
oh

