1
00:00:02,240 --> 00:00:08,930
please welcome to the stage lead chin

2
00:00:05,700 --> 00:00:08,930
and La Villita

3
00:00:10,240 --> 00:00:18,380
[Music]

4
00:00:15,750 --> 00:00:22,650
[Applause]

5
00:00:18,380 --> 00:00:25,288
hi everyone thanks for being coming to

6
00:00:22,650 --> 00:00:27,598
blue hat almost at black hat we need

7
00:00:25,289 --> 00:00:30,930
black hat too so we're going to talk to

8
00:00:27,599 --> 00:00:33,360
you about sort of a Western take on a

9
00:00:30,930 --> 00:00:36,300
story on ransomware detection using

10
00:00:33,360 --> 00:00:38,040
machine learning my name is Ravi Sarita

11
00:00:36,300 --> 00:00:41,220
worked in inter labs in security and

12
00:00:38,040 --> 00:00:43,710
privacy research hi my name is Li Chen I

13
00:00:41,220 --> 00:00:46,699
am a deploying research scientist in the

14
00:00:43,710 --> 00:00:50,640
security privacy lab at Inter Labs okay

15
00:00:46,699 --> 00:00:51,870
so you know the following the legal talk

16
00:00:50,640 --> 00:00:56,160
we have to have the legal disclaimers

17
00:00:51,870 --> 00:00:57,959
right so the so this is just a quick

18
00:00:56,160 --> 00:00:59,849
outline of what topics we're going to

19
00:00:57,960 --> 00:01:01,620
cover today on the ransomware detection

20
00:00:59,850 --> 00:01:05,339
we will walk you through some of our

21
00:01:01,620 --> 00:01:07,140
experiences doing this case study in in

22
00:01:05,339 --> 00:01:08,759
the labs and our lab works on sort of

23
00:01:07,140 --> 00:01:11,280
exploit prevention capabilities as well

24
00:01:08,759 --> 00:01:13,920
as machine learning used for exploit

25
00:01:11,280 --> 00:01:16,800
prevention so you know following the

26
00:01:13,920 --> 00:01:18,630
sort of the Western theme will walk you

27
00:01:16,800 --> 00:01:20,250
through like you know what are the good

28
00:01:18,630 --> 00:01:23,089
parts about using machine learning for

29
00:01:20,250 --> 00:01:25,939
this particular use case amongst others

30
00:01:23,090 --> 00:01:28,200
what are the bad parts right that any

31
00:01:25,939 --> 00:01:29,939
researcher or practitioner needs to be

32
00:01:28,200 --> 00:01:32,549
aware of when using machine learning for

33
00:01:29,939 --> 00:01:34,139
this use case or other and then what are

34
00:01:32,549 --> 00:01:37,140
the really ugly parts that we need to

35
00:01:34,140 --> 00:01:39,420
also think about and leave and go

36
00:01:37,140 --> 00:01:41,850
through those in lots of gory detail so

37
00:01:39,420 --> 00:01:45,479
you know get ready with your sort of put

38
00:01:41,850 --> 00:01:48,030
your math hat on as well and then I'll

39
00:01:45,479 --> 00:01:49,950
also cover some of the ideas that we can

40
00:01:48,030 --> 00:01:52,530
apply here from a platform perspective

41
00:01:49,950 --> 00:01:54,930
to build an overall system where you

42
00:01:52,530 --> 00:01:57,329
know we can improve the robustness of

43
00:01:54,930 --> 00:02:03,420
this kind of an approach right so it's

44
00:01:57,329 --> 00:02:06,119
start with that so let's touch a little

45
00:02:03,420 --> 00:02:07,590
bit on ransomware and I know people here

46
00:02:06,119 --> 00:02:10,169
are familiar with it and I think this

47
00:02:07,590 --> 00:02:12,180
speaks to Alex this point also in the

48
00:02:10,169 --> 00:02:13,950
keynote today morning is that this is

49
00:02:12,180 --> 00:02:16,829
one area where the tools that we have

50
00:02:13,950 --> 00:02:20,339
built for better cryptography you know

51
00:02:16,829 --> 00:02:22,230
new instructions for cryptography SDKs

52
00:02:20,340 --> 00:02:24,450
for cryptography you know making it

53
00:02:22,230 --> 00:02:26,190
easier for developers that sort of gets

54
00:02:24,450 --> 00:02:28,649
used against us right so obviously

55
00:02:26,190 --> 00:02:30,480
cryptography has is a great tool it's

56
00:02:28,650 --> 00:02:32,909
awesome for privacy

57
00:02:30,480 --> 00:02:34,890
access control but it also gets misused

58
00:02:32,909 --> 00:02:37,890
when when somebody uses it as a

59
00:02:34,890 --> 00:02:39,599
ransomware right and it's not

60
00:02:37,890 --> 00:02:41,010
hypothetical anymore it's sort of been

61
00:02:39,599 --> 00:02:43,140
in the news for a long time I mean

62
00:02:41,010 --> 00:02:45,120
imagine like worst case scenario like

63
00:02:43,140 --> 00:02:47,790
you go into a hospital with a medical

64
00:02:45,120 --> 00:02:49,590
condition and you know you can't get the

65
00:02:47,790 --> 00:02:51,000
diagnosis or the treatment because the

66
00:02:49,590 --> 00:02:52,560
systems are down because of a ransomware

67
00:02:51,000 --> 00:02:54,750
attack right and this is not

68
00:02:52,560 --> 00:02:56,879
hypothetical - you know anymore if you

69
00:02:54,750 --> 00:02:58,079
just do a Google search today morning

70
00:02:56,879 --> 00:03:00,870
you're going to see an incident that

71
00:02:58,079 --> 00:03:03,390
happened that was reported by PBS for a

72
00:03:00,870 --> 00:03:05,220
medical institute and it's also schools

73
00:03:03,390 --> 00:03:07,619
government organizations everywhere

74
00:03:05,220 --> 00:03:09,090
right so in this talk we're going to

75
00:03:07,620 --> 00:03:11,489
focus more on the crypto ransomware

76
00:03:09,090 --> 00:03:13,140
which uses encryption to essentially you

77
00:03:11,489 --> 00:03:17,310
know prevent access to your resources

78
00:03:13,140 --> 00:03:19,319
and essentially extort from there and

79
00:03:17,310 --> 00:03:21,450
again this is this is sort of a flow I'm

80
00:03:19,319 --> 00:03:24,869
sure most people here are familiar with

81
00:03:21,450 --> 00:03:26,690
right there's different aspects to to

82
00:03:24,870 --> 00:03:30,000
ransomware in terms of its life cycle

83
00:03:26,690 --> 00:03:32,060
starting from distribution getting

84
00:03:30,000 --> 00:03:34,290
actually on the platform to execute

85
00:03:32,060 --> 00:03:36,480
communicating to its servers potentially

86
00:03:34,290 --> 00:03:38,929
to you know use the best form of crypto

87
00:03:36,480 --> 00:03:40,679
they can of the strongest form of Keys

88
00:03:38,930 --> 00:03:41,910
enumerated in the files they want to

89
00:03:40,680 --> 00:03:43,650
encrypt encrypting the file actually

90
00:03:41,910 --> 00:03:46,410
right which is the area that we will

91
00:03:43,650 --> 00:03:48,989
focus on in this case study and then

92
00:03:46,410 --> 00:03:50,280
leading to extortion right so we this

93
00:03:48,989 --> 00:03:51,660
case study from a machine learning

94
00:03:50,280 --> 00:03:53,940
perspective will focus on the encryption

95
00:03:51,660 --> 00:03:56,849
aspects and use that from a detection

96
00:03:53,940 --> 00:04:00,060
perspective let's look a little bit of

97
00:03:56,849 --> 00:04:01,530
when we set set in our mind to do this

98
00:04:00,060 --> 00:04:03,780
case that he liked what kind of data

99
00:04:01,530 --> 00:04:07,019
sets that we focused on right and this

100
00:04:03,780 --> 00:04:09,840
was just sort of a sort of a point in

101
00:04:07,019 --> 00:04:11,700
time right so the the ransomware you

102
00:04:09,840 --> 00:04:13,230
know you know has evolved from that

103
00:04:11,700 --> 00:04:14,879
point on and there's new forms of

104
00:04:13,230 --> 00:04:16,680
ransomware but when he started this

105
00:04:14,879 --> 00:04:18,089
study we sort of looked at what

106
00:04:16,680 --> 00:04:21,690
transferred families were sort of the

107
00:04:18,089 --> 00:04:23,969
most prevalent at that time and we used

108
00:04:21,690 --> 00:04:25,729
that that data set from sort of standard

109
00:04:23,970 --> 00:04:28,740
sources like virustotal

110
00:04:25,729 --> 00:04:30,870
there was a limited amount of time you

111
00:04:28,740 --> 00:04:31,830
know the ransomware can get executed and

112
00:04:30,870 --> 00:04:34,740
I'll talk about some of the limitations

113
00:04:31,830 --> 00:04:37,349
of and the issues that that causes later

114
00:04:34,740 --> 00:04:38,760
on and we uses you'd stand sort of

115
00:04:37,349 --> 00:04:40,740
standard approaches here we use decoy

116
00:04:38,760 --> 00:04:42,780
files to check that you know the

117
00:04:40,740 --> 00:04:45,150
ransomware was actually execute

118
00:04:42,780 --> 00:04:46,979
right and we saw a low activation rate

119
00:04:45,150 --> 00:04:49,679
right and I'm sure many practitioners

120
00:04:46,980 --> 00:04:50,670
you all have seen similar issues but

121
00:04:49,680 --> 00:04:52,800
this is just just to give sort of a

122
00:04:50,670 --> 00:04:56,280
baseline of what data said we we started

123
00:04:52,800 --> 00:04:57,990
with the data acquisition we did why was

124
00:04:56,280 --> 00:05:00,989
Wireless and worked system that we built

125
00:04:57,990 --> 00:05:02,430
in in our lab it's basically a sort of a

126
00:05:00,990 --> 00:05:04,590
you know dedicated system and this is

127
00:05:02,430 --> 00:05:05,610
like a small a small sort of fragments

128
00:05:04,590 --> 00:05:08,909
and apps out of it right

129
00:05:05,610 --> 00:05:11,010
where as much as possible we tried to

130
00:05:08,910 --> 00:05:12,690
you know avoid the you know even if

131
00:05:11,010 --> 00:05:15,450
malware things like that by using bare

132
00:05:12,690 --> 00:05:16,800
metal systems and that obviously is you

133
00:05:15,450 --> 00:05:18,450
know a lot of lot of pain to sort of

134
00:05:16,800 --> 00:05:21,480
configure those right and and make those

135
00:05:18,450 --> 00:05:23,820
repeatable etc and then we use some new

136
00:05:21,480 --> 00:05:25,260
capabilities in the labs like doing

137
00:05:23,820 --> 00:05:27,120
interesting things on like drive

138
00:05:25,260 --> 00:05:29,219
firmwares to essentially do like you

139
00:05:27,120 --> 00:05:30,510
know very fast checkpointing so that we

140
00:05:29,220 --> 00:05:32,340
could like make the system as it's

141
00:05:30,510 --> 00:05:34,380
beautiful and robust as possible right

142
00:05:32,340 --> 00:05:39,419
and that's how we acquired most of our

143
00:05:34,380 --> 00:05:42,810
data set the reader set that we

144
00:05:39,420 --> 00:05:44,340
collected was essentially we said let's

145
00:05:42,810 --> 00:05:45,720
let's sort of keep it simple to begin

146
00:05:44,340 --> 00:05:48,060
with right so we use a lot of the file

147
00:05:45,720 --> 00:05:50,610
i/o telemetry that is available from

148
00:05:48,060 --> 00:05:52,500
systems today and as I said we focus on

149
00:05:50,610 --> 00:05:55,830
the the encryption stage of the

150
00:05:52,500 --> 00:05:57,450
lifecycle of the ransomware so we

151
00:05:55,830 --> 00:05:58,800
collected sort of standard events like

152
00:05:57,450 --> 00:06:02,190
you know what type of file of events are

153
00:05:58,800 --> 00:06:03,900
happening what file regions is a

154
00:06:02,190 --> 00:06:05,520
targeting right is the system

155
00:06:03,900 --> 00:06:07,530
directories it's going to the user space

156
00:06:05,520 --> 00:06:09,270
directories Mesa Kalia where the data is

157
00:06:07,530 --> 00:06:10,320
that the user cares about and also the

158
00:06:09,270 --> 00:06:12,750
entropy of the files as they are

159
00:06:10,320 --> 00:06:14,159
changing right and those those pieces of

160
00:06:12,750 --> 00:06:17,760
information are essentially combined

161
00:06:14,160 --> 00:06:21,810
that in and then use in our in our deep

162
00:06:17,760 --> 00:06:24,150
learning system right so with that let's

163
00:06:21,810 --> 00:06:26,460
start diving into the the good parts of

164
00:06:24,150 --> 00:06:28,710
of machine learning and Lee will walk us

165
00:06:26,460 --> 00:06:30,030
through this journey of like hey how do

166
00:06:28,710 --> 00:06:32,729
we start off building a machine learning

167
00:06:30,030 --> 00:06:35,130
model and then we look at the platform

168
00:06:32,730 --> 00:06:37,490
aspects after we complete that part so

169
00:06:35,130 --> 00:06:37,490
that

170
00:06:38,120 --> 00:06:44,320
so let's examine first the aspect of the

171
00:06:42,080 --> 00:06:48,169
good part for machine learning at

172
00:06:44,320 --> 00:06:50,449
recognizing attacks so Robbie talked

173
00:06:48,169 --> 00:06:52,760
about what the event logs are like and

174
00:06:50,449 --> 00:06:54,500
before we fit into the model let's first

175
00:06:52,760 --> 00:06:59,030
do some feature engineering feature

176
00:06:54,500 --> 00:07:02,030
extraction and we try to ping the events

177
00:06:59,030 --> 00:07:04,520
and then in terms of the file i/o

178
00:07:02,030 --> 00:07:07,219
activities and also the entropy change

179
00:07:04,520 --> 00:07:09,849
and then map that into categorical

180
00:07:07,220 --> 00:07:12,740
integer values and of course some of the

181
00:07:09,850 --> 00:07:14,720
the logs are of different lengths so we

182
00:07:12,740 --> 00:07:17,330
also use zero padding to make sure

183
00:07:14,720 --> 00:07:20,030
they're all of the same size so we can

184
00:07:17,330 --> 00:07:22,820
treat train machine learning models on

185
00:07:20,030 --> 00:07:25,760
top of the data sets so this is what our

186
00:07:22,820 --> 00:07:28,820
features look like so each log is a

187
00:07:25,760 --> 00:07:34,219
representation of those integer values

188
00:07:28,820 --> 00:07:37,550
from 0 to 9 in sequence and then we do

189
00:07:34,220 --> 00:07:41,150
our training on the data set where we

190
00:07:37,550 --> 00:07:45,560
have the training test but at 80 20 %

191
00:07:41,150 --> 00:07:48,530
split and we use 7 different machine

192
00:07:45,560 --> 00:07:51,229
learning models and evaluates a lot of

193
00:07:48,530 --> 00:07:54,020
performance metrics on the test set and

194
00:07:51,229 --> 00:07:58,760
our dimensionality originally we set it

195
00:07:54,020 --> 00:08:00,190
to an equals the 3000 and we can use

196
00:07:58,760 --> 00:08:05,240
additional dimension reduction

197
00:08:00,190 --> 00:08:07,310
methodologies to Train on the models so

198
00:08:05,240 --> 00:08:10,880
we train those seven different

199
00:08:07,310 --> 00:08:13,250
classifiers including Texian n x gb

200
00:08:10,880 --> 00:08:15,620
linear discriminant analysis random

201
00:08:13,250 --> 00:08:18,050
forest naive bayes support vector

202
00:08:15,620 --> 00:08:20,599
machines with two different kernels and

203
00:08:18,050 --> 00:08:23,389
what we have observed is that Texian and

204
00:08:20,599 --> 00:08:27,860
actually has the best performance on

205
00:08:23,389 --> 00:08:30,139
this data set and we look at the results

206
00:08:27,860 --> 00:08:32,450
one thing we thought about is okay why

207
00:08:30,139 --> 00:08:35,089
don't we also use Texian and as a

208
00:08:32,450 --> 00:08:37,459
feature extractor and then retrain the

209
00:08:35,089 --> 00:08:39,979
rest of the classifiers in that feature

210
00:08:37,458 --> 00:08:43,699
space and can we observe a performance

211
00:08:39,979 --> 00:08:45,740
boost and what do we mean by that so

212
00:08:43,700 --> 00:08:48,960
let's have a very quick overview about

213
00:08:45,740 --> 00:08:52,910
what tech CN n is so essentially this is

214
00:08:48,960 --> 00:08:55,290
one dimensional filter sized a

215
00:08:52,910 --> 00:08:58,410
convolution on your network it has been

216
00:08:55,290 --> 00:09:02,099
- has been shown to perform very well

217
00:08:58,410 --> 00:09:06,149
for sentence classification and here is

218
00:09:02,100 --> 00:09:08,399
a architecture picture for what this

219
00:09:06,149 --> 00:09:11,279
neural network look like and what do we

220
00:09:08,399 --> 00:09:13,560
mean by the feature space and here we're

221
00:09:11,279 --> 00:09:15,390
considering the layer before the fully

222
00:09:13,560 --> 00:09:17,430
connected layer as the feature space

223
00:09:15,390 --> 00:09:20,520
where we are extracting the

224
00:09:17,430 --> 00:09:23,849
representation in this layer and we want

225
00:09:20,520 --> 00:09:25,860
to because of the prior better superior

226
00:09:23,850 --> 00:09:29,370
performance of Texian and we want to

227
00:09:25,860 --> 00:09:31,800
return the other classifiers so before

228
00:09:29,370 --> 00:09:34,860
we do that we also did a visualization

229
00:09:31,800 --> 00:09:37,109
on the distribution between benign and

230
00:09:34,860 --> 00:09:39,029
ransomware in the Texian and subspace

231
00:09:37,110 --> 00:09:41,700
and from looking at the distribution

232
00:09:39,029 --> 00:09:43,770
plotted in red as in the malicious

233
00:09:41,700 --> 00:09:46,020
samples and blue the benign samples

234
00:09:43,770 --> 00:09:48,620
we're seeing the features are well

235
00:09:46,020 --> 00:09:51,779
separated in this feature space and

236
00:09:48,620 --> 00:09:53,970
hints here is the results where we have

237
00:09:51,779 --> 00:09:56,370
the compositions of two different

238
00:09:53,970 --> 00:09:58,920
classifiers where the subsequent one is

239
00:09:56,370 --> 00:10:01,110
trained on the feature space of Texian n

240
00:09:58,920 --> 00:10:04,229
and we're seeing all of the classifiers

241
00:10:01,110 --> 00:10:06,060
have a performance boost in accuracy

242
00:10:04,230 --> 00:10:09,930
false positive rate through positive

243
00:10:06,060 --> 00:10:12,779
rate F score and area under the curve so

244
00:10:09,930 --> 00:10:14,579
this is really great and this is the

245
00:10:12,779 --> 00:10:18,480
good row of machine learning being

246
00:10:14,580 --> 00:10:22,410
effective and accurate at detecting

247
00:10:18,480 --> 00:10:24,990
attacks and we also evaluated accuracy

248
00:10:22,410 --> 00:10:27,900
false positive rate precision recall f1

249
00:10:24,990 --> 00:10:29,820
scores but are these things enough when

250
00:10:27,900 --> 00:10:32,880
the machine learning algorithms are used

251
00:10:29,820 --> 00:10:36,300
in security critical applications so

252
00:10:32,880 --> 00:10:39,779
with that question we go to the next

253
00:10:36,300 --> 00:10:41,790
part the bad parts essentially how we

254
00:10:39,779 --> 00:10:45,180
wanted to look at is trying to adjust

255
00:10:41,790 --> 00:10:48,300
opposed the good cop versus the bad cop

256
00:10:45,180 --> 00:10:51,959
row of machine learning and how they can

257
00:10:48,300 --> 00:10:54,270
perform and behave and in this analysis

258
00:10:51,959 --> 00:10:56,670
what we're trying to do is we're trying

259
00:10:54,270 --> 00:10:59,310
to identifying the vulnerabilities and

260
00:10:56,670 --> 00:11:00,339
blind-spot of those highly effective

261
00:10:59,310 --> 00:11:03,609
machine learning

262
00:11:00,340 --> 00:11:06,010
and furthermore we're actually using an

263
00:11:03,610 --> 00:11:08,200
other machine learning algorithms in

264
00:11:06,010 --> 00:11:13,150
order to automatically hack these

265
00:11:08,200 --> 00:11:16,630
effective mo systems so before going

266
00:11:13,150 --> 00:11:19,810
into our proposed overview of techniques

267
00:11:16,630 --> 00:11:22,540
let's first talk about a virtual machine

268
00:11:19,810 --> 00:11:25,150
learning in the vision space so a

269
00:11:22,540 --> 00:11:28,390
virtual machine learning is a relatively

270
00:11:25,150 --> 00:11:30,430
newer emerging field in machine learning

271
00:11:28,390 --> 00:11:32,860
that examines the robustness and

272
00:11:30,430 --> 00:11:34,900
vulnerability resiliency of machine

273
00:11:32,860 --> 00:11:37,300
learning algorithms and what I have

274
00:11:34,900 --> 00:11:39,760
shown here is the adversary machine

275
00:11:37,300 --> 00:11:43,420
learning used in computer vision where

276
00:11:39,760 --> 00:11:45,700
you can have a very highly effective DNN

277
00:11:43,420 --> 00:11:47,949
algorithm that have very high accuracy

278
00:11:45,700 --> 00:11:49,810
at computer vision classification

279
00:11:47,950 --> 00:11:54,370
algorithms are classification results

280
00:11:49,810 --> 00:11:56,949
however you can just add very few

281
00:11:54,370 --> 00:12:00,310
perturbations that's imperceptible to

282
00:11:56,950 --> 00:12:04,120
the human eyes but the DNN algorithm

283
00:12:00,310 --> 00:12:07,449
will miss classify them so as you can

284
00:12:04,120 --> 00:12:09,370
see their cost can be high because a

285
00:12:07,450 --> 00:12:11,740
stop sign that's originally classified

286
00:12:09,370 --> 00:12:14,460
as a stop sign with some perturbation

287
00:12:11,740 --> 00:12:18,930
could be misclassified as a max speed

288
00:12:14,460 --> 00:12:22,030
100 in addition to image classification

289
00:12:18,930 --> 00:12:24,189
these type of attacks can also happen in

290
00:12:22,030 --> 00:12:27,280
object detection as we add some

291
00:12:24,190 --> 00:12:31,270
perturbations the sheep are being

292
00:12:27,280 --> 00:12:35,530
recognized as cars so how are we mapping

293
00:12:31,270 --> 00:12:38,230
this into the malware space and our core

294
00:12:35,530 --> 00:12:40,750
idea is we're also thinking of some new

295
00:12:38,230 --> 00:12:43,390
ideas some new algorithm called the

296
00:12:40,750 --> 00:12:46,990
generative adversarial Network so we

297
00:12:43,390 --> 00:12:49,720
wanted to employ the idea again

298
00:12:46,990 --> 00:12:52,209
generated adversary network into

299
00:12:49,720 --> 00:12:54,730
synthesizing some of the adversarial

300
00:12:52,209 --> 00:12:58,779
perturbations to spoof the ransomware

301
00:12:54,730 --> 00:13:02,200
detection system so what is a generative

302
00:12:58,779 --> 00:13:04,930
adversarial Network so it consists of

303
00:13:02,200 --> 00:13:08,110
two parts or you can consider two neural

304
00:13:04,930 --> 00:13:09,489
networks one is the generator and one is

305
00:13:08,110 --> 00:13:13,420
the disk

306
00:13:09,490 --> 00:13:18,590
so the generator would generate fake

307
00:13:13,420 --> 00:13:21,079
samples to fool the discriminator on the

308
00:13:18,590 --> 00:13:24,170
other hand the discriminator will

309
00:13:21,080 --> 00:13:27,260
differentiate or classifies the rios

310
00:13:24,170 --> 00:13:31,360
from the fake so that is so over time

311
00:13:27,260 --> 00:13:34,640
the generator get much better as

312
00:13:31,360 --> 00:13:37,880
creating fakes that look alike the reals

313
00:13:34,640 --> 00:13:40,910
but over time the discriminator will

314
00:13:37,880 --> 00:13:44,720
also get better at classifying wheels

315
00:13:40,910 --> 00:13:47,660
from the fake so essentially this is

316
00:13:44,720 --> 00:13:49,430
what the idea of ganon comes from is

317
00:13:47,660 --> 00:13:51,920
essentially two neural networks are

318
00:13:49,430 --> 00:13:53,660
playing a two-player game and you can

319
00:13:51,920 --> 00:13:57,680
consider the generator being a

320
00:13:53,660 --> 00:14:00,079
counterfeiter that's trying to make some

321
00:13:57,680 --> 00:14:01,699
counterfeits and the discriminator is

322
00:14:00,080 --> 00:14:05,810
the police and both are getting better

323
00:14:01,700 --> 00:14:08,870
by playing this arms race and it has

324
00:14:05,810 --> 00:14:11,900
been adopted and used primarily first

325
00:14:08,870 --> 00:14:14,930
focused a first proposed in images as we

326
00:14:11,900 --> 00:14:17,750
can see here initially the eminence

327
00:14:14,930 --> 00:14:22,300
digits generated by Gann were looking

328
00:14:17,750 --> 00:14:26,060
like noise however after some iterations

329
00:14:22,300 --> 00:14:31,579
it becomes better and more crisp like

330
00:14:26,060 --> 00:14:34,400
the original amnesty data set so this is

331
00:14:31,580 --> 00:14:38,450
the idea core idea of how we wanted to

332
00:14:34,400 --> 00:14:43,550
build this machine learning based system

333
00:14:38,450 --> 00:14:46,850
to spoof ml so what we wanted to propose

334
00:14:43,550 --> 00:14:49,939
is we also have this generator and

335
00:14:46,850 --> 00:14:52,340
discriminator but we will have the

336
00:14:49,940 --> 00:14:54,320
generator to generate the dynamic

337
00:14:52,340 --> 00:14:57,920
execution of the sequences that we

338
00:14:54,320 --> 00:15:00,410
showed earlier in our data set and we'll

339
00:14:57,920 --> 00:15:03,620
have the discriminator in order to try

340
00:15:00,410 --> 00:15:07,010
to differentiate the real and the fake

341
00:15:03,620 --> 00:15:08,930
and over time we would hope to get a

342
00:15:07,010 --> 00:15:11,750
very good generator that can synthesize

343
00:15:08,930 --> 00:15:14,089
these type of adversarial ransomware

344
00:15:11,750 --> 00:15:16,860
sequences that can spoof machine

345
00:15:14,090 --> 00:15:19,590
learning algorithms

346
00:15:16,860 --> 00:15:22,200
and furthermore notes that we are

347
00:15:19,590 --> 00:15:25,440
proposing this technique not in the

348
00:15:22,200 --> 00:15:27,950
vision space but in a malware space so

349
00:15:25,440 --> 00:15:30,960
typically again when you're proposing

350
00:15:27,950 --> 00:15:33,450
generating images in a vision space the

351
00:15:30,960 --> 00:15:35,730
human eyes can be one of the metrics to

352
00:15:33,450 --> 00:15:38,280
stop training as long as it looks

353
00:15:35,730 --> 00:15:40,440
somewhat like the digits or it looks

354
00:15:38,280 --> 00:15:43,939
somewhat like a real human being you can

355
00:15:40,440 --> 00:15:47,520
stop the training however in our case

356
00:15:43,940 --> 00:15:49,590
the human eyes is not applicable here

357
00:15:47,520 --> 00:15:52,290
what we wanted to do is we want to make

358
00:15:49,590 --> 00:15:54,660
sure the average your examples we argue

359
00:15:52,290 --> 00:15:58,410
we have generated from ghin actually

360
00:15:54,660 --> 00:16:01,160
possess some malicious maliciousness so

361
00:15:58,410 --> 00:16:03,959
we actually proposed two sets of

362
00:16:01,160 --> 00:16:06,569
adversarial quality assessment metric

363
00:16:03,960 --> 00:16:09,660
one is sample based one is batch based

364
00:16:06,570 --> 00:16:10,380
so that we evaluate that the adversarial

365
00:16:09,660 --> 00:16:14,850
samples

366
00:16:10,380 --> 00:16:18,720
truly possess maliciousness so given

367
00:16:14,850 --> 00:16:21,390
that here is our overview methodology so

368
00:16:18,720 --> 00:16:24,540
we have those samples that's collected

369
00:16:21,390 --> 00:16:27,870
from our sequence ransomware logs and

370
00:16:24,540 --> 00:16:30,719
then we train particularly a acan that's

371
00:16:27,870 --> 00:16:33,390
also not only con fake and real but also

372
00:16:30,720 --> 00:16:37,860
conditioned on benign and malicious

373
00:16:33,390 --> 00:16:40,230
labels and once we have trained the

374
00:16:37,860 --> 00:16:42,270
neural network at test time we would

375
00:16:40,230 --> 00:16:44,880
generate the adversary machine malicious

376
00:16:42,270 --> 00:16:48,329
samples and we incorporate this quality

377
00:16:44,880 --> 00:16:50,880
assessment check on the general

378
00:16:48,330 --> 00:16:53,040
adversary examples if it passes the

379
00:16:50,880 --> 00:16:56,460
assessment check we'll put them in our

380
00:16:53,040 --> 00:16:59,310
set for later to bypass the mo systems

381
00:16:56,460 --> 00:17:02,880
if it doesn't pass that we will go back

382
00:16:59,310 --> 00:17:06,510
to regenerate more of the adversarial

383
00:17:02,880 --> 00:17:09,000
samples and finally with all the

384
00:17:06,510 --> 00:17:11,790
generated adversarial samples we will

385
00:17:09,000 --> 00:17:14,220
send them into the highly effective

386
00:17:11,790 --> 00:17:16,530
machine learning classifiers that we

387
00:17:14,220 --> 00:17:18,810
that I have shown earlier and then

388
00:17:16,530 --> 00:17:22,490
understand how well they can detect

389
00:17:18,810 --> 00:17:22,490
those adversarial examples

390
00:17:23,589 --> 00:17:28,809
and here is some more details in how we

391
00:17:26,499 --> 00:17:30,970
train typically when training and

392
00:17:28,809 --> 00:17:33,070
there's some old collapsing issues and

393
00:17:30,970 --> 00:17:34,779
also when we stop because the loss

394
00:17:33,070 --> 00:17:37,570
function is a combination of the

395
00:17:34,779 --> 00:17:39,580
generator and the discriminator and so

396
00:17:37,570 --> 00:17:42,330
we have some more technical details in

397
00:17:39,580 --> 00:17:45,730
this aspect but test time we will

398
00:17:42,330 --> 00:17:48,580
generate 5,000 malicious samples and

399
00:17:45,730 --> 00:17:51,549
test whether those highly effective ml

400
00:17:48,580 --> 00:17:53,620
algorithms can detect them and all of

401
00:17:51,549 --> 00:17:57,009
those samples actually do pass it pass

402
00:17:53,620 --> 00:18:02,918
our assessment adversarial quality

403
00:17:57,009 --> 00:18:05,919
assessment so here is the result as we

404
00:18:02,919 --> 00:18:09,610
can see that out of all of the samples

405
00:18:05,919 --> 00:18:12,779
we have fed into the mo system only one

406
00:18:09,610 --> 00:18:15,969
classifier support vector machine with a

407
00:18:12,779 --> 00:18:18,429
normal linear kernel in the Tech's inner

408
00:18:15,970 --> 00:18:21,429
space does not get fooled by the

409
00:18:18,429 --> 00:18:23,529
adversary examples versus all these

410
00:18:21,429 --> 00:18:26,499
other highly effective classifiers

411
00:18:23,529 --> 00:18:29,139
actually fail to detect the majority of

412
00:18:26,499 --> 00:18:31,480
those adversarial samples and this

413
00:18:29,139 --> 00:18:35,850
clearly indicates a very broad attack

414
00:18:31,480 --> 00:18:35,850
surface for machine learning systems

415
00:18:36,360 --> 00:18:44,008
hence here comes our the bass summary so

416
00:18:42,070 --> 00:18:46,539
what we have proposed here is that

417
00:18:44,009 --> 00:18:47,919
machine learning algorithms can

418
00:18:46,539 --> 00:18:50,200
automatically hack

419
00:18:47,919 --> 00:18:53,649
other machine learning systems as we

420
00:18:50,200 --> 00:18:56,110
have demonstrated here the generative

421
00:18:53,649 --> 00:18:58,570
adversarial Network can serve as an

422
00:18:56,110 --> 00:19:03,899
intelligent hacker to bypass those

423
00:18:58,570 --> 00:19:05,700
highly effective mo systems so from this

424
00:19:03,899 --> 00:19:08,340
case study

425
00:19:05,700 --> 00:19:11,970
it definitely demonstrates that

426
00:19:08,340 --> 00:19:15,399
robustness and resiliency are equally

427
00:19:11,970 --> 00:19:18,249
equally important as accuracy false

428
00:19:15,399 --> 00:19:20,258
positive reposition recall f1 scores

429
00:19:18,249 --> 00:19:22,539
especially when we're building ml

430
00:19:20,259 --> 00:19:25,840
classifiers for security critical

431
00:19:22,539 --> 00:19:29,259
applications now one of our goal is to

432
00:19:25,840 --> 00:19:33,009
try to understand why this happened so

433
00:19:29,259 --> 00:19:36,020
we dive deep to investigate what happens

434
00:19:33,009 --> 00:19:39,380
in a boundary decision such that in

435
00:19:36,020 --> 00:19:41,570
original feature space these samples are

436
00:19:39,380 --> 00:19:44,059
very malicious very close to the

437
00:19:41,570 --> 00:19:45,530
malicious real samples and they do

438
00:19:44,059 --> 00:19:48,559
possess maliciousness

439
00:19:45,530 --> 00:19:50,720
however the classifiers who are highly

440
00:19:48,559 --> 00:19:54,139
effective just really failed to detect

441
00:19:50,720 --> 00:19:57,290
them so here is our ugly part which

442
00:19:54,140 --> 00:19:59,120
consists of the investigation and also

443
00:19:57,290 --> 00:20:03,170
some other aspects that we should

444
00:19:59,120 --> 00:20:07,760
address and consider when we build

445
00:20:03,170 --> 00:20:10,580
machine learning algorithms so we

446
00:20:07,760 --> 00:20:13,190
investigate while the generate samples

447
00:20:10,580 --> 00:20:16,280
can bypass those machine learning

448
00:20:13,190 --> 00:20:19,460
systems so we have a 2d visualization

449
00:20:16,280 --> 00:20:23,059
here where the red corresponds to the

450
00:20:19,460 --> 00:20:26,330
ransomware cluster and the blue is the

451
00:20:23,059 --> 00:20:29,860
benign where cluster and we also plotted

452
00:20:26,330 --> 00:20:34,220
the average here examples in this 2d

453
00:20:29,860 --> 00:20:37,790
plot so the average your examples are in

454
00:20:34,220 --> 00:20:41,210
the dark red small cluster as we can see

455
00:20:37,790 --> 00:20:44,690
and they lie pretty closely to the

456
00:20:41,210 --> 00:20:47,840
boundary but much closer to the real

457
00:20:44,690 --> 00:20:52,970
benign particularly in this text CNN

458
00:20:47,840 --> 00:20:55,850
feature space so recall that all these

459
00:20:52,970 --> 00:20:59,600
samples actually passed our adversarial

460
00:20:55,850 --> 00:21:01,909
assessment quality check but when they

461
00:20:59,600 --> 00:21:04,399
are mapped in this highly effective

462
00:21:01,910 --> 00:21:12,170
feature space they actually get mapped

463
00:21:04,400 --> 00:21:16,370
from the red part to the blue part on

464
00:21:12,170 --> 00:21:20,870
the other hand we consider what about

465
00:21:16,370 --> 00:21:23,530
other type of boundaries so if you

466
00:21:20,870 --> 00:21:26,629
remember in one of the seven classifiers

467
00:21:23,530 --> 00:21:31,070
support vector machine with the non

468
00:21:26,630 --> 00:21:33,260
linear boundary actually was able to

469
00:21:31,070 --> 00:21:36,230
detect a hundred percent of the

470
00:21:33,260 --> 00:21:40,070
adversary samples so we plotted the

471
00:21:36,230 --> 00:21:43,490
nonlinear boundary and then understand

472
00:21:40,070 --> 00:21:43,779
where the average your examples lie and

473
00:21:43,490 --> 00:21:46,359
is

474
00:21:43,779 --> 00:21:49,229
case what we're seeing is that the

475
00:21:46,359 --> 00:21:52,080
nonlinear boundary actually do

476
00:21:49,229 --> 00:21:55,929
demonstrate some level of robustness

477
00:21:52,080 --> 00:21:58,449
against ever serial examples and this

478
00:21:55,929 --> 00:22:01,349
indicates they seem to have a smaller

479
00:21:58,450 --> 00:22:01,349
blind spot

480
00:22:05,440 --> 00:22:10,720
but there are other issues that we

481
00:22:07,300 --> 00:22:14,260
should consider so for instance concept

482
00:22:10,720 --> 00:22:18,130
drift when your samples are varying and

483
00:22:14,260 --> 00:22:20,160
changing over time how can you make your

484
00:22:18,130 --> 00:22:22,660
machine learning system more robust

485
00:22:20,160 --> 00:22:26,620
against the samples that are morphing

486
00:22:22,660 --> 00:22:29,890
over time and another aspect is time

487
00:22:26,620 --> 00:22:32,969
variance for instance if your ransomware

488
00:22:29,890 --> 00:22:35,950
take much longer to encrypt the data

489
00:22:32,970 --> 00:22:42,450
where your machine learning algorithm be

490
00:22:35,950 --> 00:22:42,450
able to capture that prolonged behavior

491
00:22:45,600 --> 00:22:53,919
hence there comes our ugly summary so we

492
00:22:51,280 --> 00:22:57,070
have investigated the machine learning

493
00:22:53,920 --> 00:23:00,190
boundaries and understand why the

494
00:22:57,070 --> 00:23:03,429
average here examples are easily being

495
00:23:00,190 --> 00:23:06,010
classified as benign and we show that in

496
00:23:03,430 --> 00:23:10,030
the original feature space the adversary

497
00:23:06,010 --> 00:23:12,790
examples are indeed malicious however in

498
00:23:10,030 --> 00:23:14,860
the feature space when it when it maps

499
00:23:12,790 --> 00:23:17,080
to the feature space it's actually

500
00:23:14,860 --> 00:23:20,310
closer to the boundary decision that's

501
00:23:17,080 --> 00:23:24,750
why they get misclassified and we also

502
00:23:20,310 --> 00:23:26,909
examined the newly linearity and the

503
00:23:24,750 --> 00:23:29,650
non-linearity of the boundaries and

504
00:23:26,910 --> 00:23:31,890
things like the non linear boundary

505
00:23:29,650 --> 00:23:39,030
decision will show better resiliency

506
00:23:31,890 --> 00:23:44,950
against adversarial examples okay so

507
00:23:39,030 --> 00:23:47,440
next I'm gonna pass the presentation to

508
00:23:44,950 --> 00:23:50,170
Ravi to talk about how our platform

509
00:23:47,440 --> 00:23:54,930
capabilities can improve detection to

510
00:23:50,170 --> 00:23:58,270
make the attackers job harder thank you

511
00:23:54,930 --> 00:24:00,070
alright so hopefully with Lee walking

512
00:23:58,270 --> 00:24:01,600
through like the the three phases that

513
00:24:00,070 --> 00:24:05,200
we went through in the case study it

514
00:24:01,600 --> 00:24:07,179
gives you a good idea of how the you

515
00:24:05,200 --> 00:24:09,700
know using this this technique called

516
00:24:07,180 --> 00:24:12,370
generative adversarial machine learning

517
00:24:09,700 --> 00:24:14,420
is it's quite powerful to essentially

518
00:24:12,370 --> 00:24:15,949
like undo

519
00:24:14,420 --> 00:24:17,810
you know the intelligence that is

520
00:24:15,950 --> 00:24:20,480
embedded when you train a machine

521
00:24:17,810 --> 00:24:22,610
learning model what I wanted to sort of

522
00:24:20,480 --> 00:24:24,440
consider or put some ideas forward here

523
00:24:22,610 --> 00:24:26,840
on what are the platform capabilities

524
00:24:24,440 --> 00:24:28,630
when you think of ransomware detector as

525
00:24:26,840 --> 00:24:31,159
a world system as a defensive mechanism

526
00:24:28,630 --> 00:24:35,090
what platform capabilities can we apply

527
00:24:31,160 --> 00:24:37,760
to improve that detection further so

528
00:24:35,090 --> 00:24:40,669
let's consider how you know as we

529
00:24:37,760 --> 00:24:43,670
pointed out unlike image based

530
00:24:40,670 --> 00:24:45,260
classification where you know some pixel

531
00:24:43,670 --> 00:24:47,840
perturbation may not be really

532
00:24:45,260 --> 00:24:50,710
observable to the human eye and that

533
00:24:47,840 --> 00:24:53,240
that's sort of easy to think about from

534
00:24:50,710 --> 00:24:55,580
when you generate perturbed logs or

535
00:24:53,240 --> 00:24:58,040
modified logs from using this gain based

536
00:24:55,580 --> 00:24:59,870
approach for an attacker to successfully

537
00:24:58,040 --> 00:25:02,000
use that they obviously have to go

538
00:24:59,870 --> 00:25:03,739
through a number of other steps to make

539
00:25:02,000 --> 00:25:07,520
sure that that log can actually be used

540
00:25:03,740 --> 00:25:10,160
as you know as a tool to actually bypass

541
00:25:07,520 --> 00:25:11,830
the you know circumvent the the model

542
00:25:10,160 --> 00:25:14,780
when the ransomware is executing right

543
00:25:11,830 --> 00:25:16,340
so we sort of looked at like how would

544
00:25:14,780 --> 00:25:18,170
an attacker go about using those

545
00:25:16,340 --> 00:25:21,080
generated logs right so one approach

546
00:25:18,170 --> 00:25:24,830
could be that the generated log is used

547
00:25:21,080 --> 00:25:27,260
by by the by the attacker to essentially

548
00:25:24,830 --> 00:25:29,240
take the original ransomware and combine

549
00:25:27,260 --> 00:25:32,120
this log essentially as input into a

550
00:25:29,240 --> 00:25:34,010
into a state machine that essentially

551
00:25:32,120 --> 00:25:36,590
finds interesting sequences in the

552
00:25:34,010 --> 00:25:38,150
generated log where it can M the ransom

553
00:25:36,590 --> 00:25:40,340
it can embed the sequent interesting

554
00:25:38,150 --> 00:25:41,900
sequences that they need for the ransom

555
00:25:40,340 --> 00:25:43,790
to actually perform right because a lot

556
00:25:41,900 --> 00:25:46,160
of the ransomware is essentially a loop

557
00:25:43,790 --> 00:25:48,139
of you know renaming the files or moving

558
00:25:46,160 --> 00:25:50,660
the files encrypting it and deleting the

559
00:25:48,140 --> 00:25:53,480
original right so if you can find within

560
00:25:50,660 --> 00:25:55,280
the generated gann generated log those

561
00:25:53,480 --> 00:25:57,740
interesting repeating patterns you can

562
00:25:55,280 --> 00:25:59,960
essentially embed spurious operations in

563
00:25:57,740 --> 00:26:02,570
them to essentially still run the

564
00:25:59,960 --> 00:26:03,470
ransomware but to the system or to the

565
00:26:02,570 --> 00:26:06,020
ml detector

566
00:26:03,470 --> 00:26:08,210
it fits the the the pattern of this

567
00:26:06,020 --> 00:26:09,620
adversary lock right which would mean

568
00:26:08,210 --> 00:26:11,540
that you would circumvent the system

569
00:26:09,620 --> 00:26:15,080
completely because that activity would

570
00:26:11,540 --> 00:26:16,790
look normal right so really the only

571
00:26:15,080 --> 00:26:19,550
defense against those kinds of

572
00:26:16,790 --> 00:26:22,159
approaches is either to essentially make

573
00:26:19,550 --> 00:26:24,230
the attackers job essentially by adding

574
00:26:22,160 --> 00:26:26,150
more interesting differentiating

575
00:26:24,230 --> 00:26:27,630
features right which make it more

576
00:26:26,150 --> 00:26:29,760
complex for the attacker

577
00:26:27,630 --> 00:26:32,789
to take that log and actually map that

578
00:26:29,760 --> 00:26:34,020
into a binary with the from the original

579
00:26:32,789 --> 00:26:36,480
binary to make sure it still actually

580
00:26:34,020 --> 00:26:38,429
does the job that the attacker wants to

581
00:26:36,480 --> 00:26:41,190
do right so that's that's kind of the

582
00:26:38,429 --> 00:26:43,559
bar we need to raise along while because

583
00:26:41,190 --> 00:26:46,049
we are using the the the information

584
00:26:43,559 --> 00:26:48,809
extracted from the system behavior we

585
00:26:46,049 --> 00:26:51,450
can add that other nuances that we're

586
00:26:48,809 --> 00:26:53,970
looking for that make the the job of the

587
00:26:51,450 --> 00:26:55,710
attacker harder the one other standard

588
00:26:53,970 --> 00:26:57,270
approach people use is once you once you

589
00:26:55,710 --> 00:26:58,530
have these adverts or logs you can just

590
00:26:57,270 --> 00:27:00,418
use address for retraining to

591
00:26:58,530 --> 00:27:02,370
essentially tell your machine learning

592
00:27:00,419 --> 00:27:04,559
model that these sequences are still

593
00:27:02,370 --> 00:27:06,030
malicious right so that's sort of a

594
00:27:04,559 --> 00:27:08,190
standard technique that's been used to

595
00:27:06,030 --> 00:27:10,470
address addressable approaches but

596
00:27:08,190 --> 00:27:12,570
because this domain is different from an

597
00:27:10,470 --> 00:27:14,010
image classification domain you can do

598
00:27:12,570 --> 00:27:17,330
additional things on the system to make

599
00:27:14,010 --> 00:27:19,799
the job of the attacker much more harder

600
00:27:17,330 --> 00:27:21,780
some other ideas as we were looking at

601
00:27:19,799 --> 00:27:23,250
sort of this this track of like and what

602
00:27:21,780 --> 00:27:26,639
can we do on the system to make the

603
00:27:23,250 --> 00:27:28,590
attackers job or harder is using other

604
00:27:26,640 --> 00:27:31,049
sources of telemetry on the platform

605
00:27:28,590 --> 00:27:32,610
right so we extract for this case

606
00:27:31,049 --> 00:27:35,940
studying the extracted telemetry from

607
00:27:32,610 --> 00:27:37,770
the OS subsystem for file IO right there

608
00:27:35,940 --> 00:27:39,900
is other telemetry that you know the

609
00:27:37,770 --> 00:27:41,789
processor can can give you or other

610
00:27:39,900 --> 00:27:43,320
subsystems can give you for example

611
00:27:41,789 --> 00:27:45,570
through I provider that's completely

612
00:27:43,320 --> 00:27:47,490
outside the scope of the their attackers

613
00:27:45,570 --> 00:27:49,678
purview that can give you additional

614
00:27:47,490 --> 00:27:53,190
sideband information that can be used by

615
00:27:49,679 --> 00:27:54,870
the defense to essentially you know give

616
00:27:53,190 --> 00:27:56,280
you an upper hand right and then and

617
00:27:54,870 --> 00:27:58,168
give you sort of an asymmetric

618
00:27:56,280 --> 00:28:01,168
information advantage or what the

619
00:27:58,169 --> 00:28:03,510
attacker can can can play with right at

620
00:28:01,169 --> 00:28:05,429
one time an Intel processor trace is

621
00:28:03,510 --> 00:28:08,460
just one one example of such a telemetry

622
00:28:05,429 --> 00:28:10,620
where you can extract execution logs of

623
00:28:08,460 --> 00:28:12,870
what the CPU is actually executing for a

624
00:28:10,620 --> 00:28:14,908
particular application so as the as the

625
00:28:12,870 --> 00:28:17,039
code is executing you can basically tell

626
00:28:14,909 --> 00:28:19,200
the processor to give you like a running

627
00:28:17,039 --> 00:28:21,750
audit log of where execution is

628
00:28:19,200 --> 00:28:23,730
transitioning to in a particular you

629
00:28:21,750 --> 00:28:26,309
know kernel or application space so you

630
00:28:23,730 --> 00:28:28,350
can actually profile you know exactly

631
00:28:26,309 --> 00:28:30,059
what system calls were being executed by

632
00:28:28,350 --> 00:28:31,678
an application and this can help you

633
00:28:30,059 --> 00:28:34,168
differentiate between a spoofed log

634
00:28:31,679 --> 00:28:37,679
versus a log that tells you that truly

635
00:28:34,169 --> 00:28:39,659
this was actually executed the other

636
00:28:37,679 --> 00:28:40,890
keep some sort of technologies or

637
00:28:39,659 --> 00:28:41,490
building blocks we've looked at is

638
00:28:40,890 --> 00:28:43,290
checkpoint

639
00:28:41,490 --> 00:28:45,290
this is a good tool and I will describe

640
00:28:43,290 --> 00:28:47,970
that sort of use case in the next slide

641
00:28:45,290 --> 00:28:50,430
the third area is trusted execution

642
00:28:47,970 --> 00:28:53,820
right so trusted execution gets used as

643
00:28:50,430 --> 00:28:55,680
a generic term what I mean here is a lot

644
00:28:53,820 --> 00:28:57,720
of the cases it's important to actually

645
00:28:55,680 --> 00:28:59,220
protect the the weights of the model

646
00:28:57,720 --> 00:29:00,990
that you're using right let's say you

647
00:28:59,220 --> 00:29:03,540
you deploy a model on an end point and

648
00:29:00,990 --> 00:29:05,580
if an attacker can tweak the tweak the

649
00:29:03,540 --> 00:29:07,680
weights of that model they don't really

650
00:29:05,580 --> 00:29:09,330
need to go to any lens to like do any

651
00:29:07,680 --> 00:29:11,640
address and attacks right they can just

652
00:29:09,330 --> 00:29:13,590
like chain the chains few weights of the

653
00:29:11,640 --> 00:29:16,470
model and cause the model to just miss

654
00:29:13,590 --> 00:29:17,820
classify right so so but there are

655
00:29:16,470 --> 00:29:19,200
capabilities on the platform that can be

656
00:29:17,820 --> 00:29:21,000
used to actually protect the model at

657
00:29:19,200 --> 00:29:22,440
runtime so that once your model is

658
00:29:21,000 --> 00:29:24,570
deployed you have a way to ensure that

659
00:29:22,440 --> 00:29:25,520
it doesn't get tampered with right so

660
00:29:24,570 --> 00:29:28,200
you're not as much worried about

661
00:29:25,520 --> 00:29:30,210
confidentiality here we are more worried

662
00:29:28,200 --> 00:29:33,420
more about the integrity of execution

663
00:29:30,210 --> 00:29:35,330
right so I'm sort of marketing a bunch

664
00:29:33,420 --> 00:29:37,230
of things and a trusted execution right

665
00:29:35,330 --> 00:29:39,389
and then there are new storage

666
00:29:37,230 --> 00:29:40,920
mechanisms like persistent memory that

667
00:29:39,390 --> 00:29:42,540
are you know coming online you know

668
00:29:40,920 --> 00:29:44,880
starting more from the from the server

669
00:29:42,540 --> 00:29:47,280
side and hopefully becoming available on

670
00:29:44,880 --> 00:29:49,800
clients as well and those give us like

671
00:29:47,280 --> 00:29:52,020
completely new avenues to to access

672
00:29:49,800 --> 00:29:54,210
control storage right the way you think

673
00:29:52,020 --> 00:29:56,160
about it today a lot of the ransomware

674
00:29:54,210 --> 00:29:57,690
works because you have sort of a you

675
00:29:56,160 --> 00:29:59,460
know common choke point in the kernel

676
00:29:57,690 --> 00:30:01,560
where you really don't know who the

677
00:29:59,460 --> 00:30:04,140
actor is that's modifying a particular

678
00:30:01,560 --> 00:30:05,940
you know block storage device right but

679
00:30:04,140 --> 00:30:08,850
with persistent memory and new modes of

680
00:30:05,940 --> 00:30:11,490
like direct access to to storage through

681
00:30:08,850 --> 00:30:12,659
through memory that gives us new

682
00:30:11,490 --> 00:30:14,520
opportunities for building access

683
00:30:12,660 --> 00:30:15,930
control models where maybe we can wipe

684
00:30:14,520 --> 00:30:18,540
out certain classes of ransomware

685
00:30:15,930 --> 00:30:21,980
completely right so it behooves us to

686
00:30:18,540 --> 00:30:25,110
look at those those new models as well

687
00:30:21,980 --> 00:30:27,690
so this is just one slide I sort of I

688
00:30:25,110 --> 00:30:29,790
picked on as a way of like talking about

689
00:30:27,690 --> 00:30:31,980
checkpointing coming back to my earlier

690
00:30:29,790 --> 00:30:34,139
hypothetical use case of like equal to

691
00:30:31,980 --> 00:30:37,920
hospital and you know the system is down

692
00:30:34,140 --> 00:30:39,750
because of a ransomware attack right so

693
00:30:37,920 --> 00:30:40,920
one one place we can start with this

694
00:30:39,750 --> 00:30:43,140
obviously we can protect the integrity

695
00:30:40,920 --> 00:30:44,550
of the ransomware detector or the

696
00:30:43,140 --> 00:30:46,170
machine learning capability on the

697
00:30:44,550 --> 00:30:48,810
platform itself to make sure it doesn't

698
00:30:46,170 --> 00:30:50,640
get tampered with in the first place

699
00:30:48,810 --> 00:30:52,500
the second one is as we are collecting

700
00:30:50,640 --> 00:30:54,270
telemetry on the platform which which

701
00:30:52,500 --> 00:30:55,140
guides our machine learning system we

702
00:30:54,270 --> 00:30:56,790
can ensure that

703
00:30:55,140 --> 00:30:58,560
use you know sideband trusted

704
00:30:56,790 --> 00:31:00,180
information or in bank trusted

705
00:30:58,560 --> 00:31:02,429
information to make sure that that

706
00:31:00,180 --> 00:31:03,750
information is not being spoofed by an

707
00:31:02,430 --> 00:31:05,640
attacker because that's an obvious

708
00:31:03,750 --> 00:31:07,710
attack point if I can just modify the

709
00:31:05,640 --> 00:31:10,760
data you're classifying on at one time

710
00:31:07,710 --> 00:31:13,650
then I can just change the decision path

711
00:31:10,760 --> 00:31:15,300
and then then using the machine learning

712
00:31:13,650 --> 00:31:17,250
techniques of the sorts that you know

713
00:31:15,300 --> 00:31:18,480
that let me describe you know you can

714
00:31:17,250 --> 00:31:20,520
use you can build early detection

715
00:31:18,480 --> 00:31:22,980
systems now know machine learning system

716
00:31:20,520 --> 00:31:24,780
is perfect you know that we are striving

717
00:31:22,980 --> 00:31:26,880
to build perfect machine learning

718
00:31:24,780 --> 00:31:29,190
systems but it's sort of it's all it's a

719
00:31:26,880 --> 00:31:31,170
journey right so but we can now use

720
00:31:29,190 --> 00:31:33,330
these machine learning systems as very

721
00:31:31,170 --> 00:31:34,980
good or early detectors as long as we

722
00:31:33,330 --> 00:31:36,510
have you know capabilities on the

723
00:31:34,980 --> 00:31:39,120
platform which can be used to drive

724
00:31:36,510 --> 00:31:41,400
those those decisions and fill that gap

725
00:31:39,120 --> 00:31:43,459
of waiting of the portions where the

726
00:31:41,400 --> 00:31:46,440
machine learning cannot give you a

727
00:31:43,460 --> 00:31:47,460
deterministic you know a solid outcome

728
00:31:46,440 --> 00:31:50,160
right where there's still some

729
00:31:47,460 --> 00:31:51,390
uncertainty and so you can combine this

730
00:31:50,160 --> 00:31:53,790
is sort of an example of how you can

731
00:31:51,390 --> 00:31:55,500
combine system based capabilities with

732
00:31:53,790 --> 00:31:57,030
machine learning capabilities to

733
00:31:55,500 --> 00:31:59,040
essentially build an overall more robust

734
00:31:57,030 --> 00:32:01,500
system right so if the system thinks

735
00:31:59,040 --> 00:32:02,670
that there is a you know you know

736
00:32:01,500 --> 00:32:05,160
potential ransomware

737
00:32:02,670 --> 00:32:07,710
attack and you can tune it such that you

738
00:32:05,160 --> 00:32:09,720
get very very low false positives then

739
00:32:07,710 --> 00:32:11,550
you can respond quickly by having local

740
00:32:09,720 --> 00:32:14,310
backups and hopefully bring the critical

741
00:32:11,550 --> 00:32:17,220
systems back up in in in an operational

742
00:32:14,310 --> 00:32:21,720
State and not affect users right more

743
00:32:17,220 --> 00:32:23,520
widely I also wanted to call out this

744
00:32:21,720 --> 00:32:25,290
just make a plug for one of the tools

745
00:32:23,520 --> 00:32:26,639
that we've been working on within sort

746
00:32:25,290 --> 00:32:29,760
of the open source face and with

747
00:32:26,640 --> 00:32:31,260
universities call ml flight so with this

748
00:32:29,760 --> 00:32:33,240
is some some research we did with

749
00:32:31,260 --> 00:32:35,580
Georgia Tech and it's an open source

750
00:32:33,240 --> 00:32:38,730
sort of openly available tool that can

751
00:32:35,580 --> 00:32:40,409
be used by ml researchers squarely

752
00:32:38,730 --> 00:32:42,840
focusing on some of these addresses all

753
00:32:40,410 --> 00:32:46,170
aspects as applied to Mouser research

754
00:32:42,840 --> 00:32:48,300
right so ms flight is a capability where

755
00:32:46,170 --> 00:32:50,550
you know researchers can come in they

756
00:32:48,300 --> 00:32:52,110
can upload samples or datasets and you

757
00:32:50,550 --> 00:32:55,230
can essentially do like a Red Team Blue

758
00:32:52,110 --> 00:32:56,939
Team like interaction on that on that

759
00:32:55,230 --> 00:32:58,800
system where you can apply research

760
00:32:56,940 --> 00:33:01,080
functions to bypass other researchers

761
00:32:58,800 --> 00:33:02,909
can learn from those those bypasses and

762
00:33:01,080 --> 00:33:04,439
then compare results and then improve

763
00:33:02,910 --> 00:33:07,590
the improve the model right and

764
00:33:04,440 --> 00:33:08,899
hopefully get get you know common sort

765
00:33:07,590 --> 00:33:13,399
of ideas that

766
00:33:08,899 --> 00:33:15,468
can be applied more broadly so with that

767
00:33:13,399 --> 00:33:19,248
we would like to summarize and I would

768
00:33:15,469 --> 00:33:21,469
like to call Li back up on the stage but

769
00:33:19,249 --> 00:33:23,779
these sort of key points right that we

770
00:33:21,469 --> 00:33:26,059
want to call out obviously ml is a great

771
00:33:23,779 --> 00:33:28,369
tool especially to avoid sort of the

772
00:33:26,059 --> 00:33:31,940
brute force feature engineering and get

773
00:33:28,369 --> 00:33:33,799
to you know a good enough solid solution

774
00:33:31,940 --> 00:33:35,239
pretty quickly for for things like

775
00:33:33,799 --> 00:33:37,749
ransomware and other we just chose

776
00:33:35,239 --> 00:33:40,460
ransomware as as an important case study

777
00:33:37,749 --> 00:33:41,989
but at the same time that same ml

778
00:33:40,460 --> 00:33:44,929
techniques that we use can very easily

779
00:33:41,989 --> 00:33:46,909
be used by attackers as well and as

780
00:33:44,929 --> 00:33:50,029
shown by the by the case study here and

781
00:33:46,909 --> 00:33:52,580
that has to believe this that's super

782
00:33:50,029 --> 00:33:54,859
critical in this kinds of use cases as

783
00:33:52,580 --> 00:33:57,199
opposed to like you know something like

784
00:33:54,859 --> 00:33:58,879
you know maybe like a photo based system

785
00:33:57,200 --> 00:34:00,679
where it may not be critical if you know

786
00:33:58,879 --> 00:34:03,080
your photo base system did not identify

787
00:34:00,679 --> 00:34:04,369
like a person correctly you know as long

788
00:34:03,080 --> 00:34:06,619
as you're not using it in a security

789
00:34:04,369 --> 00:34:09,168
application it's fine but in a security

790
00:34:06,619 --> 00:34:12,529
application we cannot tolerate you know

791
00:34:09,168 --> 00:34:14,388
these kinds of address cell attacks and

792
00:34:12,529 --> 00:34:16,039
there are other complexities we didn't

793
00:34:14,389 --> 00:34:17,990
dive into today that lightly touched

794
00:34:16,039 --> 00:34:20,510
upon like you have to sort of make sure

795
00:34:17,989 --> 00:34:22,219
the model stays up today it is

796
00:34:20,510 --> 00:34:23,690
recognizing if there is any poisoning

797
00:34:22,219 --> 00:34:25,609
attacks coming in based on the data that

798
00:34:23,690 --> 00:34:28,129
you are using how is the model drifting

799
00:34:25,609 --> 00:34:30,589
over time and then you know we feel like

800
00:34:28,129 --> 00:34:32,629
there's a very important aspect of

801
00:34:30,589 --> 00:34:34,699
combining especially for these kinds of

802
00:34:32,629 --> 00:34:36,980
security problems combining platform

803
00:34:34,699 --> 00:34:39,020
capabilities along with machine learning

804
00:34:36,980 --> 00:34:42,319
based detectors which can give us a much

805
00:34:39,020 --> 00:34:44,059
more overall robust system so with that

806
00:34:42,319 --> 00:34:46,520
we'd like to conclude and we'll be in

807
00:34:44,059 --> 00:34:48,730
the speaker section if people have more

808
00:34:46,520 --> 00:34:48,730
questions

809
00:34:51,940 --> 00:34:54,000
you

