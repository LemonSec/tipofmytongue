1
00:00:02,240 --> 00:00:10,060
we welcome to the stage Anitra data

2
00:00:05,870 --> 00:00:18,760
Majumdar and honorable

3
00:00:10,060 --> 00:00:22,000
[Music]

4
00:00:18,760 --> 00:00:25,780
almost there almost there so this is the

5
00:00:22,000 --> 00:00:27,040
last talk and after that it's all open

6
00:00:25,780 --> 00:00:30,910
for fun and games

7
00:00:27,040 --> 00:00:33,010
so today myself and a noob of both of us

8
00:00:30,910 --> 00:00:35,460
from the LinkedIn house security team

9
00:00:33,010 --> 00:00:37,960
would be talking about building

10
00:00:35,460 --> 00:00:40,840
securement a machine learning pipelines

11
00:00:37,960 --> 00:00:49,750
the challenges and the security patterns

12
00:00:40,840 --> 00:00:51,070
that we have been following okay so we

13
00:00:49,750 --> 00:00:53,080
would like to acknowledge the

14
00:00:51,070 --> 00:00:56,650
contributions from the various internal

15
00:00:53,080 --> 00:00:59,050
teams in LinkedIn and also of course the

16
00:00:56,650 --> 00:01:01,420
house security team the larger team that

17
00:00:59,050 --> 00:01:03,970
we are a part of for spearheading some

18
00:01:01,420 --> 00:01:05,860
of the initiatives and programs for

19
00:01:03,970 --> 00:01:15,759
securing the machine learning pipelines

20
00:01:05,860 --> 00:01:19,420
at LinkedIn so the talk outline we will

21
00:01:15,759 --> 00:01:21,940
be covering the major use cases of

22
00:01:19,420 --> 00:01:24,850
machine learning at LinkedIn and

23
00:01:21,940 --> 00:01:27,970
basically the business use cases the

24
00:01:24,850 --> 00:01:30,550
various phases involved in machine

25
00:01:27,970 --> 00:01:32,259
learning pipelines and some of the

26
00:01:30,550 --> 00:01:35,369
nuances of the distributed

27
00:01:32,259 --> 00:01:38,110
infrastructure that backs those phases

28
00:01:35,369 --> 00:01:40,990
we will then be going into some of the

29
00:01:38,110 --> 00:01:45,130
security risks and security solution

30
00:01:40,990 --> 00:01:48,539
patterns and eventually we also touch

31
00:01:45,130 --> 00:01:51,399
upon some of the scalability aspects

32
00:01:48,539 --> 00:01:54,340
that we have issues that we have

33
00:01:51,399 --> 00:02:02,440
observed while applying the security

34
00:01:54,340 --> 00:02:05,110
controls to secure these pipelines okay

35
00:02:02,440 --> 00:02:07,899
so the major machine learning use cases

36
00:02:05,110 --> 00:02:12,069
at LinkedIn so we use machine learning

37
00:02:07,899 --> 00:02:15,610
to help members build network on the

38
00:02:12,069 --> 00:02:18,880
LinkedIn platform machine learning is

39
00:02:15,610 --> 00:02:23,980
used to predict relevant jobs to the

40
00:02:18,880 --> 00:02:26,079
various members on our platform machine

41
00:02:23,980 --> 00:02:29,640
learning is used heavily to predict and

42
00:02:26,080 --> 00:02:31,220
recommend feed content things like

43
00:02:29,640 --> 00:02:34,730
articles

44
00:02:31,220 --> 00:02:37,010
to our members on the platform and of

45
00:02:34,730 --> 00:02:39,829
course last not but not the least we use

46
00:02:37,010 --> 00:02:42,739
machine learning to detect abuse like a

47
00:02:39,830 --> 00:02:47,660
count takeover fake accounts harassment

48
00:02:42,740 --> 00:02:49,490
and spamming on our platform so what are

49
00:02:47,660 --> 00:02:51,710
some of the high level phases involved

50
00:02:49,490 --> 00:02:54,830
in machine learning we have the

51
00:02:51,710 --> 00:02:57,620
experimentation phase where the most of

52
00:02:54,830 --> 00:02:59,450
the time is spent by data scientists and

53
00:02:57,620 --> 00:03:03,230
performing feature engineering and

54
00:02:59,450 --> 00:03:05,660
problem formulation we have the model

55
00:03:03,230 --> 00:03:07,489
training and evaluation phase where the

56
00:03:05,660 --> 00:03:11,090
data that is prepped in the

57
00:03:07,490 --> 00:03:13,280
experimentation phase is fed in and the

58
00:03:11,090 --> 00:03:16,970
model that has been authored undergoes

59
00:03:13,280 --> 00:03:20,480
training and then the best model is code

60
00:03:16,970 --> 00:03:23,480
and selected in the model deployment

61
00:03:20,480 --> 00:03:24,679
phase the models that have been the best

62
00:03:23,480 --> 00:03:28,670
model that is selected in the previous

63
00:03:24,680 --> 00:03:31,550
phase is actually deployed into the

64
00:03:28,670 --> 00:03:34,790
infrastructure production infrastructure

65
00:03:31,550 --> 00:03:40,100
usually for the purpose of model serving

66
00:03:34,790 --> 00:03:41,870
and inference which is the last phase so

67
00:03:40,100 --> 00:03:43,880
currently there is an initiative at

68
00:03:41,870 --> 00:03:47,300
LinkedIn called productive machine

69
00:03:43,880 --> 00:03:51,079
learning or tro ml where all these

70
00:03:47,300 --> 00:03:54,770
various phases are tied together through

71
00:03:51,080 --> 00:03:58,850
integrated workflows and tooling and

72
00:03:54,770 --> 00:04:02,720
libraries and the main goal behind this

73
00:03:58,850 --> 00:04:04,640
initiative is to boost the productivity

74
00:04:02,720 --> 00:04:08,450
of the data scientists at the company

75
00:04:04,640 --> 00:04:10,570
because currently various teams within

76
00:04:08,450 --> 00:04:13,750
the data science organization use

77
00:04:10,570 --> 00:04:16,880
different methods for each of the phases

78
00:04:13,750 --> 00:04:19,640
so the main goal of this initiative is

79
00:04:16,880 --> 00:04:21,980
to bring all those processes and tooling

80
00:04:19,640 --> 00:04:25,820
and pipeline components under a single

81
00:04:21,980 --> 00:04:29,810
pane of glass and of course the major

82
00:04:25,820 --> 00:04:31,400
security concern is is our member data

83
00:04:29,810 --> 00:04:34,550
protecting our member data on our

84
00:04:31,400 --> 00:04:38,549
platform from data exfiltration and

85
00:04:34,550 --> 00:04:41,069
unauthorized data access

86
00:04:38,550 --> 00:04:44,190
so how does the ml flow logical

87
00:04:41,069 --> 00:04:46,650
architecture look like so we have

88
00:04:44,190 --> 00:04:49,919
offline components in our infrastructure

89
00:04:46,650 --> 00:04:52,258
that primarily performs the

90
00:04:49,919 --> 00:04:56,219
experimentation and training phases and

91
00:04:52,259 --> 00:04:59,099
by offline components I mean these are

92
00:04:56,220 --> 00:05:04,050
jobs that are batched or scheduled in

93
00:04:59,099 --> 00:05:06,620
nature we have components that bridge

94
00:05:04,050 --> 00:05:09,270
the offline and the online world and

95
00:05:06,620 --> 00:05:11,970
these are mostly in the form of event

96
00:05:09,270 --> 00:05:15,060
bus like in the case of LinkedIn it is

97
00:05:11,970 --> 00:05:17,639
Kafka and also there there is a model

98
00:05:15,060 --> 00:05:20,639
artifact store which is used to store

99
00:05:17,639 --> 00:05:22,860
and retrieve models and of course the

100
00:05:20,639 --> 00:05:26,460
online components which are basically

101
00:05:22,860 --> 00:05:30,090
applications and they communicate with

102
00:05:26,460 --> 00:05:34,080
one another over rest interfaces and as

103
00:05:30,090 --> 00:05:37,830
a part of this online components we have

104
00:05:34,080 --> 00:05:42,690
the model deployment phase and the model

105
00:05:37,830 --> 00:05:46,650
serving phase that run so what are some

106
00:05:42,690 --> 00:05:48,599
of the security risks and challenges in

107
00:05:46,650 --> 00:05:51,719
each of these phases that I just

108
00:05:48,599 --> 00:05:56,280
described in the experimentation phase

109
00:05:51,720 --> 00:06:01,320
the major challenges unauthorized data

110
00:05:56,280 --> 00:06:03,448
access and the data leakage so as I was

111
00:06:01,320 --> 00:06:06,479
mentioning like in the experimentation

112
00:06:03,449 --> 00:06:08,639
phase various data scientists in the

113
00:06:06,479 --> 00:06:12,389
organization have a need to do feature

114
00:06:08,639 --> 00:06:16,349
engineering so during the course of such

115
00:06:12,389 --> 00:06:20,090
an action this like unauthorized data

116
00:06:16,349 --> 00:06:22,889
access is a risk that we have seen and

117
00:06:20,090 --> 00:06:25,590
also during that process there might be

118
00:06:22,889 --> 00:06:29,090
a leakage of sensitive data into the

119
00:06:25,590 --> 00:06:33,359
less secure areas of our infrastructure

120
00:06:29,090 --> 00:06:36,090
in the model training phase the

121
00:06:33,360 --> 00:06:38,280
important concern is generation of

122
00:06:36,090 --> 00:06:40,888
intermediate datasets through automatic

123
00:06:38,280 --> 00:06:44,099
flows which could be sensitive in nature

124
00:06:40,889 --> 00:06:46,949
and which might not have any associated

125
00:06:44,099 --> 00:06:49,050
audit trail and may be flowing into the

126
00:06:46,949 --> 00:06:52,260
areas of the infrastructure that is less

127
00:06:49,050 --> 00:06:55,340
secure in the model

128
00:06:52,260 --> 00:06:58,830
women face the main concerns our

129
00:06:55,340 --> 00:07:00,989
unintended model actions like model

130
00:06:58,830 --> 00:07:04,229
publication to production or test

131
00:07:00,990 --> 00:07:06,600
environments and in some of the cases

132
00:07:04,230 --> 00:07:09,420
the models themselves might be

133
00:07:06,600 --> 00:07:12,180
personalized and might be leaking some

134
00:07:09,420 --> 00:07:14,820
containing some member data in that case

135
00:07:12,180 --> 00:07:17,250
we don't want the models to leak member

136
00:07:14,820 --> 00:07:20,700
PII into less secure areas of the

137
00:07:17,250 --> 00:07:23,400
environment and finally in the inference

138
00:07:20,700 --> 00:07:25,400
phase the major concerns our security

139
00:07:23,400 --> 00:07:28,380
misconfigurations

140
00:07:25,400 --> 00:07:30,419
which could lead to a potential denial

141
00:07:28,380 --> 00:07:35,040
of service attack due to resource

142
00:07:30,420 --> 00:07:37,680
exhaustion and also using serve ulnar

143
00:07:35,040 --> 00:07:40,350
about software components there are

144
00:07:37,680 --> 00:07:42,870
lesser known theoretical attacks

145
00:07:40,350 --> 00:07:46,740
possible as well like member inference

146
00:07:42,870 --> 00:07:50,340
attacks where an attacker is able to

147
00:07:46,740 --> 00:07:56,790
guess the datasets used for training the

148
00:07:50,340 --> 00:07:59,310
model so what are some of the controls

149
00:07:56,790 --> 00:08:02,190
and the pattern security controls and

150
00:07:59,310 --> 00:08:05,940
patterns we have adopted for the

151
00:08:02,190 --> 00:08:09,990
experimentation phase the major controls

152
00:08:05,940 --> 00:08:12,210
are access control and by access control

153
00:08:09,990 --> 00:08:15,780
I mean both rule-based

154
00:08:12,210 --> 00:08:17,969
and role based access control in the

155
00:08:15,780 --> 00:08:20,789
rule based access control is usually

156
00:08:17,970 --> 00:08:23,400
based on the attribute of the target

157
00:08:20,790 --> 00:08:25,590
that is being accessed and the role

158
00:08:23,400 --> 00:08:27,239
based access control is on the attribute

159
00:08:25,590 --> 00:08:31,739
of the source that is trying to access

160
00:08:27,240 --> 00:08:34,979
the target and the rule based access

161
00:08:31,740 --> 00:08:38,430
control is usually done in some sort of

162
00:08:34,979 --> 00:08:41,220
a uniform access layer and where the

163
00:08:38,429 --> 00:08:45,780
rules can be enforced in an integrated

164
00:08:41,220 --> 00:08:49,230
manner for the encryption control we use

165
00:08:45,780 --> 00:08:52,020
both encryption of data at rest as well

166
00:08:49,230 --> 00:08:57,090
as encryption of data in transit for

167
00:08:52,020 --> 00:08:59,100
encryption in data at rest we have seen

168
00:08:57,090 --> 00:09:01,620
cases where we absolutely need to

169
00:08:59,100 --> 00:09:04,380
decrypt the data during the process of

170
00:09:01,620 --> 00:09:09,420
model training so in that case

171
00:09:04,380 --> 00:09:11,490
what we try to do is reduce the data in

172
00:09:09,420 --> 00:09:14,689
the encrypted state or in the decrepit

173
00:09:11,490 --> 00:09:18,270
state as much as possible so that the

174
00:09:14,690 --> 00:09:24,360
exposure of the sensitive decrypted data

175
00:09:18,270 --> 00:09:28,650
is reduced and the other control that we

176
00:09:24,360 --> 00:09:30,330
have been using is in the as a part of

177
00:09:28,650 --> 00:09:34,439
the rule based access control is also

178
00:09:30,330 --> 00:09:37,260
like enforce rules so that privacy

179
00:09:34,440 --> 00:09:40,610
doesn't privacy of the datasets doesn't

180
00:09:37,260 --> 00:09:44,189
get leaked and what I mean by that when

181
00:09:40,610 --> 00:09:47,430
multiple non PII fields are combined it

182
00:09:44,190 --> 00:09:51,270
shouldn't be leaking some PII field and

183
00:09:47,430 --> 00:09:54,750
this is usually done in that data access

184
00:09:51,270 --> 00:09:57,240
layer that I just mentioned through some

185
00:09:54,750 --> 00:10:02,370
rule-based throw through the enforcement

186
00:09:57,240 --> 00:10:05,490
of some rules and the last one is the

187
00:10:02,370 --> 00:10:07,710
feature sensitivity annotation where we

188
00:10:05,490 --> 00:10:09,990
do have an architectural component in

189
00:10:07,710 --> 00:10:13,920
our environment called the AI metadata

190
00:10:09,990 --> 00:10:16,440
hub where various features used for

191
00:10:13,920 --> 00:10:20,370
during the training phase are annotated

192
00:10:16,440 --> 00:10:22,980
for sensitivity and it is contacted by

193
00:10:20,370 --> 00:10:27,410
various applications and tooling in the

194
00:10:22,980 --> 00:10:27,410
pipeline during each of the phases

195
00:10:27,470 --> 00:10:33,840
during the model training phase we have

196
00:10:31,680 --> 00:10:36,060
the encryption as I mentioned in the as

197
00:10:33,840 --> 00:10:40,830
in the case of an experimentation phase

198
00:10:36,060 --> 00:10:43,290
and and most of the training flows are

199
00:10:40,830 --> 00:10:46,890
more automated compared to the

200
00:10:43,290 --> 00:10:49,079
experimentation phase and they were the

201
00:10:46,890 --> 00:10:53,850
thing to look out for in this phase is

202
00:10:49,080 --> 00:10:57,030
the leaking of PII during these

203
00:10:53,850 --> 00:10:59,990
intermediate flows especially in the

204
00:10:57,030 --> 00:11:03,800
case of a complex multistage model and

205
00:10:59,990 --> 00:11:07,230
we have also developed some in-house

206
00:11:03,800 --> 00:11:11,969
machine learning based classifiers to go

207
00:11:07,230 --> 00:11:13,830
and detect for PII x' in areas of the

208
00:11:11,970 --> 00:11:14,920
infrastructure that it's not supposed to

209
00:11:13,830 --> 00:11:19,540
be in

210
00:11:14,920 --> 00:11:23,920
and also do some monitoring or alerting

211
00:11:19,540 --> 00:11:27,040
based on that detection in the model

212
00:11:23,920 --> 00:11:30,009
deployment phase we used controls like

213
00:11:27,040 --> 00:11:32,009
dual verification which is live very

214
00:11:30,009 --> 00:11:35,019
similar to the to a code review process

215
00:11:32,009 --> 00:11:38,319
like me the code cannot be checked in

216
00:11:35,019 --> 00:11:40,749
without a reviewers shipit so similarly

217
00:11:38,319 --> 00:11:43,868
in the case of a model a model cannot be

218
00:11:40,749 --> 00:11:47,769
deployed to prod unless it has been

219
00:11:43,869 --> 00:11:51,339
reviewed and okayed by a second person

220
00:11:47,769 --> 00:11:54,429
and usually these model deployment

221
00:11:51,339 --> 00:11:56,499
systems are accessed through a new eye

222
00:11:54,429 --> 00:12:00,220
and we enforce multi-factor

223
00:11:56,499 --> 00:12:04,269
authentication on that the other control

224
00:12:00,220 --> 00:12:07,569
that we have started looking into is the

225
00:12:04,269 --> 00:12:10,600
model randomization control and this is

226
00:12:07,569 --> 00:12:14,469
applicable more to personalized models

227
00:12:10,600 --> 00:12:17,499
which could leak member PII data let's

228
00:12:14,470 --> 00:12:22,299
say in the test environment and of

229
00:12:17,499 --> 00:12:25,419
course during the model deployment phase

230
00:12:22,299 --> 00:12:27,970
in the testing infrastructure we must

231
00:12:25,419 --> 00:12:35,350
ensure that synthetic data is used

232
00:12:27,970 --> 00:12:38,470
instead of actual member PII for the

233
00:12:35,350 --> 00:12:42,129
model inference phase the major concern

234
00:12:38,470 --> 00:12:45,629
is security miss configuration and the

235
00:12:42,129 --> 00:12:48,279
way we proactively address this is

236
00:12:45,629 --> 00:12:51,509
through the running of security

237
00:12:48,279 --> 00:12:54,309
benchmarks that can detect low-hanging

238
00:12:51,509 --> 00:12:59,649
security miss configuration issues and

239
00:12:54,309 --> 00:13:01,539
then making sure it's configured

240
00:12:59,649 --> 00:13:05,379
according to the best practices in the

241
00:13:01,539 --> 00:13:08,319
benchmark and also the other controllers

242
00:13:05,379 --> 00:13:11,619
we typically want to segregate workloads

243
00:13:08,319 --> 00:13:13,689
in production based on the sensitivity

244
00:13:11,619 --> 00:13:17,919
level of the data that the model is

245
00:13:13,689 --> 00:13:21,480
handling and the model health assurance

246
00:13:17,919 --> 00:13:24,970
which is a part of the pro ml initiative

247
00:13:21,480 --> 00:13:27,489
helps us look into the model health or

248
00:13:24,970 --> 00:13:28,269
observe the model health over the course

249
00:13:27,489 --> 00:13:31,480
of time

250
00:13:28,269 --> 00:13:34,209
to look for performance degradation like

251
00:13:31,480 --> 00:13:35,860
accuracy degradation issues over a

252
00:13:34,209 --> 00:13:39,008
course of time which could suggest

253
00:13:35,860 --> 00:13:42,850
potential denial of service attacks that

254
00:13:39,009 --> 00:13:44,589
the model would be could be under now

255
00:13:42,850 --> 00:13:46,660
going into some of the key challenges

256
00:13:44,589 --> 00:13:51,759
that we have been facing in our

257
00:13:46,660 --> 00:13:55,809
environment so basically had using

258
00:13:51,759 --> 00:13:58,350
heterogeneous controls in our offline

259
00:13:55,809 --> 00:14:01,089
and online environment like

260
00:13:58,350 --> 00:14:04,629
heterogeneous authentication controls in

261
00:14:01,089 --> 00:14:07,600
this case so our offline environment

262
00:14:04,629 --> 00:14:11,019
which is where all the offline training

263
00:14:07,600 --> 00:14:13,809
jobs run are mostly backed by things

264
00:14:11,019 --> 00:14:19,660
like the Hadoop HDFS Hadoop MapReduce

265
00:14:13,809 --> 00:14:22,749
job spark etc so the main method of

266
00:14:19,660 --> 00:14:24,610
authentication in this offline

267
00:14:22,749 --> 00:14:27,279
environment is through the use of

268
00:14:24,610 --> 00:14:29,769
Kerberos delegation tokens and block

269
00:14:27,279 --> 00:14:32,230
access tokens using which all the

270
00:14:29,769 --> 00:14:34,029
compute jobs is able to distribute the

271
00:14:32,230 --> 00:14:39,399
workload and access data from the

272
00:14:34,029 --> 00:14:41,709
storage in the online infrastructure all

273
00:14:39,399 --> 00:14:45,369
our services communicate with one

274
00:14:41,709 --> 00:14:49,888
another over HTTP based connection which

275
00:14:45,369 --> 00:14:53,769
uses mutual TLS so now when a online

276
00:14:49,889 --> 00:14:57,160
service needs to contact an offline job

277
00:14:53,769 --> 00:14:59,740
of to get data or for whatever purpose

278
00:14:57,160 --> 00:15:01,839
as a part of a flow that spans both the

279
00:14:59,740 --> 00:15:06,429
online and offline world there is a

280
00:15:01,839 --> 00:15:09,369
translation problem so the way that we

281
00:15:06,429 --> 00:15:12,189
are have solved this problem is through

282
00:15:09,369 --> 00:15:15,720
the introduction of this service called

283
00:15:12,189 --> 00:15:19,110
the translator service which goes and

284
00:15:15,720 --> 00:15:22,509
consults an identity management system

285
00:15:19,110 --> 00:15:24,850
to get the entitlements of the

286
00:15:22,509 --> 00:15:27,970
Authenticator that it gets from the

287
00:15:24,850 --> 00:15:32,920
service online service and in this case

288
00:15:27,970 --> 00:15:37,329
it is an x.509 certificate and then it

289
00:15:32,920 --> 00:15:40,300
decides whether to grant access to that

290
00:15:37,329 --> 00:15:43,660
principle or not and generates the token

291
00:15:40,300 --> 00:15:46,390
and that token is used to complete rest

292
00:15:43,660 --> 00:15:49,530
of the flow to go and contact the

293
00:15:46,390 --> 00:15:52,660
offline jobs and the offline data and

294
00:15:49,530 --> 00:15:55,360
through this means a full user

295
00:15:52,660 --> 00:15:58,270
attribution or a full service

296
00:15:55,360 --> 00:16:00,070
attribution is established that spans

297
00:15:58,270 --> 00:16:04,750
across both the online and offline

298
00:16:00,070 --> 00:16:08,140
environment another challenge that we

299
00:16:04,750 --> 00:16:10,690
have faced in our on-prem environment is

300
00:16:08,140 --> 00:16:14,140
due to the tight coupling of the compute

301
00:16:10,690 --> 00:16:15,850
and the data tiers like usually all

302
00:16:14,140 --> 00:16:19,120
these machine learning phases are

303
00:16:15,850 --> 00:16:22,600
heavily distributed environments so it

304
00:16:19,120 --> 00:16:25,510
is this such a tight coupling can

305
00:16:22,600 --> 00:16:28,750
actually introduce some challenges when

306
00:16:25,510 --> 00:16:31,620
we are trying to apply security controls

307
00:16:28,750 --> 00:16:34,810
for authentication and authorization and

308
00:16:31,620 --> 00:16:39,150
what I mean by tight coupling is let's

309
00:16:34,810 --> 00:16:43,209
say there is a compute cluster that

310
00:16:39,150 --> 00:16:46,420
needs to go and contact a cluster that

311
00:16:43,210 --> 00:16:50,050
stores data during the flow of a

312
00:16:46,420 --> 00:16:53,949
training job so the compute cluster

313
00:16:50,050 --> 00:16:57,000
relies on some host level identity to

314
00:16:53,950 --> 00:16:59,680
identify the storage cluster and then

315
00:16:57,000 --> 00:17:02,500
and then goes and completes the

316
00:16:59,680 --> 00:17:05,829
transaction so this introduces the tight

317
00:17:02,500 --> 00:17:09,940
coupling the way we would be solving

318
00:17:05,829 --> 00:17:12,879
this problem is through moving to the

319
00:17:09,940 --> 00:17:15,880
cloud Azure through an internal project

320
00:17:12,880 --> 00:17:18,430
called blue shift so as a part of that

321
00:17:15,880 --> 00:17:21,220
project all these storage services are

322
00:17:18,430 --> 00:17:23,820
already available as azure services as

323
00:17:21,220 --> 00:17:27,310
well as the compute services are also

324
00:17:23,819 --> 00:17:30,220
encapsulated as Azure services so the

325
00:17:27,310 --> 00:17:34,020
main job over here would be to come come

326
00:17:30,220 --> 00:17:36,550
up with an access layer in which all the

327
00:17:34,020 --> 00:17:39,010
controls for authentication and

328
00:17:36,550 --> 00:17:41,260
authorization can be introduced in a

329
00:17:39,010 --> 00:17:45,280
scalable manner and then the access that

330
00:17:41,260 --> 00:17:47,560
can consult the identity layer to go and

331
00:17:45,280 --> 00:17:50,220
check for entitlements of the various

332
00:17:47,560 --> 00:17:50,220
principles

333
00:17:51,080 --> 00:17:58,379
moving over to deep learning pipelines

334
00:17:54,030 --> 00:18:01,230
so we we have use cases and LinkedIn

335
00:17:58,380 --> 00:18:03,530
like each of those ml use cases that I

336
00:18:01,230 --> 00:18:07,350
was mentioning in the beginning

337
00:18:03,530 --> 00:18:09,240
currently there is a trend and that this

338
00:18:07,350 --> 00:18:11,580
is primarily to improve the model

339
00:18:09,240 --> 00:18:15,150
accuracy for each of those use cases

340
00:18:11,580 --> 00:18:18,720
that data scientists are using deep

341
00:18:15,150 --> 00:18:21,750
learning and this is because on the

342
00:18:18,720 --> 00:18:25,830
LinkedIn platform there is a lot of text

343
00:18:21,750 --> 00:18:29,520
data like member activity member

344
00:18:25,830 --> 00:18:32,939
economic graph member profiles so all

345
00:18:29,520 --> 00:18:36,000
these data attacks data are taken and

346
00:18:32,940 --> 00:18:39,720
then vectorized through a technique

347
00:18:36,000 --> 00:18:41,790
called embeddings and fed into deep

348
00:18:39,720 --> 00:18:46,100
learning network like convolutional

349
00:18:41,790 --> 00:18:48,960
neural network and such an operation is

350
00:18:46,100 --> 00:18:54,179
computationally intensive both in terms

351
00:18:48,960 --> 00:18:57,660
of hardware and software so in order to

352
00:18:54,180 --> 00:19:01,140
scale those curve to that computational

353
00:18:57,660 --> 00:19:04,440
demands there is a common pattern where

354
00:19:01,140 --> 00:19:06,300
we are seeing that there is a use of a

355
00:19:04,440 --> 00:19:09,690
lot of open-source containerized

356
00:19:06,300 --> 00:19:14,190
software to boost the innovation and

357
00:19:09,690 --> 00:19:16,620
scale of use and with that having said

358
00:19:14,190 --> 00:19:19,230
the context of using containerized

359
00:19:16,620 --> 00:19:22,620
pipelines i will hand over rest of the

360
00:19:19,230 --> 00:19:28,830
talk to my co-presenter an above thank

361
00:19:22,620 --> 00:19:30,689
you so how does a machine learning

362
00:19:28,830 --> 00:19:32,189
container pipeline looks like it just

363
00:19:30,690 --> 00:19:36,600
looks like any other application

364
00:19:32,190 --> 00:19:39,540
container pipeline the only fancy thing

365
00:19:36,600 --> 00:19:43,679
here is for deep learning we are using

366
00:19:39,540 --> 00:19:47,940
GPU cluster instead of a CPU cluster so

367
00:19:43,680 --> 00:19:50,550
what are the phases so in build phase a

368
00:19:47,940 --> 00:19:52,530
container images build for a specific

369
00:19:50,550 --> 00:19:55,230
machine learning workflow for building a

370
00:19:52,530 --> 00:19:57,030
container image the base image as well

371
00:19:55,230 --> 00:19:59,970
as system dependencies and any

372
00:19:57,030 --> 00:20:01,050
application artifact is it's packaged

373
00:19:59,970 --> 00:20:03,240
together

374
00:20:01,050 --> 00:20:05,430
and which builds our container image

375
00:20:03,240 --> 00:20:08,670
once you have a container image you will

376
00:20:05,430 --> 00:20:11,130
push it to a image repository from there

377
00:20:08,670 --> 00:20:14,700
a container deployment deployer such an

378
00:20:11,130 --> 00:20:17,420
Orchestrator will deploy the container

379
00:20:14,700 --> 00:20:20,790
on the container runtime environment

380
00:20:17,420 --> 00:20:23,520
once the container is running container

381
00:20:20,790 --> 00:20:25,649
management tool will will manage the

382
00:20:23,520 --> 00:20:27,780
container for the availability as well

383
00:20:25,650 --> 00:20:31,410
as scale as well as continuous

384
00:20:27,780 --> 00:20:32,940
monitoring as well as continuous

385
00:20:31,410 --> 00:20:36,270
monitoring as well as from anyone

386
00:20:32,940 --> 00:20:39,540
ability scanning so on a high level

387
00:20:36,270 --> 00:20:42,840
let's go through offline workflow here a

388
00:20:39,540 --> 00:20:45,360
data scientist authors training service

389
00:20:42,840 --> 00:20:48,060
model training service on its laptop

390
00:20:45,360 --> 00:20:50,699
then pushes it to a code repository then

391
00:20:48,060 --> 00:20:54,330
builds a container image using Jenkins

392
00:20:50,700 --> 00:20:58,290
or any internal image tool pushes that

393
00:20:54,330 --> 00:21:00,629
container image builds our container

394
00:20:58,290 --> 00:21:02,909
image using the like base image as well

395
00:21:00,630 --> 00:21:05,670
as any system dependencies and the code

396
00:21:02,910 --> 00:21:07,770
heap that data scientist has pushed to

397
00:21:05,670 --> 00:21:09,710
the code repository and push that

398
00:21:07,770 --> 00:21:12,889
container image to the image repository

399
00:21:09,710 --> 00:21:16,920
then to spin up a kubernetes cluster

400
00:21:12,890 --> 00:21:19,260
training cluster the data scientist can

401
00:21:16,920 --> 00:21:22,770
use a cool CTL job and will trigger a

402
00:21:19,260 --> 00:21:25,590
API call to API server which will turn

403
00:21:22,770 --> 00:21:27,570
bring up the tensor flow training

404
00:21:25,590 --> 00:21:29,939
cluster which will fetch the image from

405
00:21:27,570 --> 00:21:33,060
the image repository will train the

406
00:21:29,940 --> 00:21:36,270
model and based on the data fetch from

407
00:21:33,060 --> 00:21:38,100
the SD FS cluster and once the model is

408
00:21:36,270 --> 00:21:41,940
generated it will be pushed to the model

409
00:21:38,100 --> 00:21:44,969
artifact store in case of online

410
00:21:41,940 --> 00:21:46,620
workflow now the model has been trained

411
00:21:44,970 --> 00:21:48,690
now it's ready to be promoted to the

412
00:21:46,620 --> 00:21:51,629
production environment they will be a

413
00:21:48,690 --> 00:21:54,260
kafka event generated the model

414
00:21:51,630 --> 00:21:58,160
deployment component of the pro ml

415
00:21:54,260 --> 00:22:00,960
workflow will consume that kafka event

416
00:21:58,160 --> 00:22:03,060
that the model deploy executor will

417
00:22:00,960 --> 00:22:05,790
fetch the model from the model deploy

418
00:22:03,060 --> 00:22:07,919
artifact store and will stage the model

419
00:22:05,790 --> 00:22:10,080
on the kubernetes cluster which will

420
00:22:07,920 --> 00:22:12,320
pull the service image from the image

421
00:22:10,080 --> 00:22:12,320
repository

422
00:22:16,970 --> 00:22:25,170
so what are the security challenges

423
00:22:19,650 --> 00:22:30,720
which we see in container environment we

424
00:22:25,170 --> 00:22:32,850
enable data scientists to use images as

425
00:22:30,720 --> 00:22:35,160
well as system dependencies which they

426
00:22:32,850 --> 00:22:38,580
can download from external environment

427
00:22:35,160 --> 00:22:40,710
but they may pose a threat

428
00:22:38,580 --> 00:22:43,320
they include vulnerabilities and they

429
00:22:40,710 --> 00:22:46,380
can lower down the security posture of

430
00:22:43,320 --> 00:22:49,200
our environment the other challenge is

431
00:22:46,380 --> 00:22:52,110
related to the insecure access

432
00:22:49,200 --> 00:22:56,190
insufficient authentication as well as

433
00:22:52,110 --> 00:22:59,340
authorization an example would be a cube

434
00:22:56,190 --> 00:23:04,560
API server allowing anonymous access as

435
00:22:59,340 --> 00:23:08,850
well as Orchestrator allowing access not

436
00:23:04,560 --> 00:23:12,240
based on role just wide open access the

437
00:23:08,850 --> 00:23:16,580
other risk is inter container network

438
00:23:12,240 --> 00:23:19,380
traffic by default any traffic within

439
00:23:16,580 --> 00:23:23,129
between the hosts between the containers

440
00:23:19,380 --> 00:23:26,040
within our hosts is not restricted that

441
00:23:23,130 --> 00:23:30,090
means any container on that host can can

442
00:23:26,040 --> 00:23:33,090
see the packets all the packets at the

443
00:23:30,090 --> 00:23:36,120
container level these are more related

444
00:23:33,090 --> 00:23:37,770
to the runtime vulnerabilities any new

445
00:23:36,120 --> 00:23:40,050
components and in your vulnerabilities

446
00:23:37,770 --> 00:23:43,170
found on any of the containers which are

447
00:23:40,050 --> 00:23:46,080
running at the moment and the last one

448
00:23:43,170 --> 00:23:50,370
is insecure configuration of container

449
00:23:46,080 --> 00:23:52,800
runtime this is more specific to how we

450
00:23:50,370 --> 00:23:58,530
are configuring the container runtime

451
00:23:52,800 --> 00:24:00,899
like a container de or a darker which is

452
00:23:58,530 --> 00:24:04,760
running on the talker which is running

453
00:24:00,900 --> 00:24:08,490
the containers an example would be

454
00:24:04,760 --> 00:24:12,300
allowing direct as such within the

455
00:24:08,490 --> 00:24:15,780
container these allowing containers to

456
00:24:12,300 --> 00:24:18,480
run as a privileged root user host

457
00:24:15,780 --> 00:24:20,879
allowing containers to to mount

458
00:24:18,480 --> 00:24:24,050
sensitive directories so these are some

459
00:24:20,880 --> 00:24:24,050
of the risks we see

460
00:24:24,450 --> 00:24:33,850
so what are the control patterns at the

461
00:24:29,110 --> 00:24:36,580
image level start from basics make sure

462
00:24:33,850 --> 00:24:39,250
you certify the base images any system

463
00:24:36,580 --> 00:24:42,280
dependencies as well as application

464
00:24:39,250 --> 00:24:44,710
artifacts by certified I mean perform

465
00:24:42,280 --> 00:24:47,710
benchmark checks against the base images

466
00:24:44,710 --> 00:24:49,720
perform vulnerability scanning if you

467
00:24:47,710 --> 00:24:51,820
are downloading perform vulnerability

468
00:24:49,720 --> 00:24:53,770
scanning against any external libraries

469
00:24:51,820 --> 00:24:57,280
which you are downloading from external

470
00:24:53,770 --> 00:24:59,800
sources any application artifacts should

471
00:24:57,280 --> 00:25:02,830
go through a security review download

472
00:24:59,800 --> 00:25:05,379
the Excel external libraries as well as

473
00:25:02,830 --> 00:25:08,889
images from trusted sources trusted

474
00:25:05,380 --> 00:25:12,670
sources here being your trusted vendors

475
00:25:08,890 --> 00:25:15,340
as well as partners at the registry

476
00:25:12,670 --> 00:25:18,250
level makes sure to encrypt all the

477
00:25:15,340 --> 00:25:21,580
communications to Eric to registry make

478
00:25:18,250 --> 00:25:22,720
sure to enable authentication based on a

479
00:25:21,580 --> 00:25:26,980
certificate or token-based

480
00:25:22,720 --> 00:25:31,420
authentication use a scrub or a bus to

481
00:25:26,980 --> 00:25:34,150
scrub and still images or any vulnerable

482
00:25:31,420 --> 00:25:35,590
images by still images I mean any images

483
00:25:34,150 --> 00:25:37,000
which are which hasn't been used for a

484
00:25:35,590 --> 00:25:41,260
long time and may contain some

485
00:25:37,000 --> 00:25:45,160
vulnerable components at the

486
00:25:41,260 --> 00:25:47,260
orchestrator level segregate the cluster

487
00:25:45,160 --> 00:25:49,360
based on the workflow sensivity makes

488
00:25:47,260 --> 00:25:53,050
sure to create virtual networks for

489
00:25:49,360 --> 00:25:56,199
different sensitivity workflows enable

490
00:25:53,050 --> 00:25:59,379
audit log in on all the user generated

491
00:25:56,200 --> 00:26:04,210
events as well as request log it

492
00:25:59,380 --> 00:26:05,710
remotely compare the configuration of an

493
00:26:04,210 --> 00:26:08,470
Orchestrator with the benchmark check

494
00:26:05,710 --> 00:26:11,650
like these are the like start from

495
00:26:08,470 --> 00:26:15,040
basics you do the basics correctly then

496
00:26:11,650 --> 00:26:17,740
you can build on top of that use the

497
00:26:15,040 --> 00:26:21,070
least privileged access model by least

498
00:26:17,740 --> 00:26:23,650
privileged access model is provision the

499
00:26:21,070 --> 00:26:25,158
access for the users based on their

500
00:26:23,650 --> 00:26:28,429
roles and

501
00:26:25,159 --> 00:26:30,979
and don't just widely allow access to

502
00:26:28,429 --> 00:26:33,440
all like all the containers or in on a

503
00:26:30,979 --> 00:26:38,029
host or all the services in an

504
00:26:33,440 --> 00:26:41,179
Orchestrator at the container side

505
00:26:38,029 --> 00:26:45,769
certified runtime configuration make

506
00:26:41,179 --> 00:26:48,470
sure you apply resource isolation ensure

507
00:26:45,769 --> 00:26:50,239
SSD is not running on the container and

508
00:26:48,470 --> 00:26:54,859
show the container is not running as a

509
00:26:50,239 --> 00:26:57,229
privileged user apply user and the last

510
00:26:54,859 --> 00:26:59,570
one is user attribution all the actions

511
00:26:57,229 --> 00:27:03,799
on an container should be attributed to

512
00:26:59,570 --> 00:27:07,928
a specific user so let's revisit the

513
00:27:03,799 --> 00:27:10,879
same diagram with some controls here now

514
00:27:07,929 --> 00:27:13,249
now you can see that any images which

515
00:27:10,879 --> 00:27:15,799
are being built goes through a benchmark

516
00:27:13,249 --> 00:27:17,330
image benchmark check as well as

517
00:27:15,799 --> 00:27:21,049
security and legal vulnerability

518
00:27:17,330 --> 00:27:23,869
scanning and it's being logged all the

519
00:27:21,049 --> 00:27:26,330
communication is over TLS as well as

520
00:27:23,869 --> 00:27:30,349
certificate authentication all the

521
00:27:26,330 --> 00:27:31,849
secrets are if you are using at CD for

522
00:27:30,349 --> 00:27:33,799
managing some other secrets should be

523
00:27:31,849 --> 00:27:36,289
those should be encrypted otherwise use

524
00:27:33,799 --> 00:27:39,379
a key management system to store any

525
00:27:36,289 --> 00:27:41,869
secrets for application between

526
00:27:39,379 --> 00:27:44,349
kubernetes cluster and as DFS its

527
00:27:41,869 --> 00:27:49,330
Kerberos token-based authentication and

528
00:27:44,349 --> 00:27:53,710
use a remote log or for example cousteau

529
00:27:49,330 --> 00:27:56,509
in case of online services the Kafka

530
00:27:53,710 --> 00:27:58,729
online workflow for machine learning the

531
00:27:56,509 --> 00:28:01,429
Kafka event which was generated needs to

532
00:27:58,729 --> 00:28:03,440
go through dual verification which means

533
00:28:01,429 --> 00:28:06,529
it needs to be peer-reviewed validated

534
00:28:03,440 --> 00:28:09,799
and user attributed all other

535
00:28:06,529 --> 00:28:14,679
communications are rest over TLS as well

536
00:28:09,799 --> 00:28:14,679
as certificate authentication is applied

537
00:28:15,729 --> 00:28:20,929
ok

538
00:28:18,039 --> 00:28:22,520
so what are the key takeaways for

539
00:28:20,929 --> 00:28:26,440
building a machine-learning security

540
00:28:22,520 --> 00:28:29,389
patterns segregation of infrastructure

541
00:28:26,440 --> 00:28:32,360
machine learning workloads are very are

542
00:28:29,390 --> 00:28:34,220
highly distributed in nature so it's

543
00:28:32,360 --> 00:28:37,010
very important to apply security

544
00:28:34,220 --> 00:28:39,049
controls in a scalable manner which

545
00:28:37,010 --> 00:28:41,720
means we have to define some security

546
00:28:39,049 --> 00:28:47,240
boundaries once we define the security

547
00:28:41,720 --> 00:28:50,299
boundaries we can apply sorry we can

548
00:28:47,240 --> 00:28:52,340
apply security controls on those

549
00:28:50,299 --> 00:28:54,740
specific boundaries which we have

550
00:28:52,340 --> 00:28:57,320
defined the high level guideline here

551
00:28:54,740 --> 00:28:59,090
would be to divide or segregate the

552
00:28:57,320 --> 00:29:03,668
machine learning pipeline environment

553
00:28:59,090 --> 00:29:06,770
into following three phases three

554
00:29:03,669 --> 00:29:10,850
environment based on storage and

555
00:29:06,770 --> 00:29:13,789
computation tier based on your based on

556
00:29:10,850 --> 00:29:17,600
control plane and data plane components

557
00:29:13,789 --> 00:29:19,820
as well as based on on those workload

558
00:29:17,600 --> 00:29:21,740
sensitivity once you have segregated

559
00:29:19,820 --> 00:29:24,620
your environment or your infrastructure

560
00:29:21,740 --> 00:29:27,080
based on these three tiers then you can

561
00:29:24,620 --> 00:29:29,570
apply security controls only specific to

562
00:29:27,080 --> 00:29:34,360
that specific tier and it's easy to

563
00:29:29,570 --> 00:29:39,590
manage and scaleable a metadata system

564
00:29:34,360 --> 00:29:41,870
machine learning the risk associated

565
00:29:39,590 --> 00:29:46,309
with machine learning are very specific

566
00:29:41,870 --> 00:29:48,949
to models for example model performance

567
00:29:46,309 --> 00:29:53,418
degradation as well as a model learning

568
00:29:48,950 --> 00:29:56,240
PII during training phase so it becomes

569
00:29:53,419 --> 00:29:59,000
very important to understand what sort

570
00:29:56,240 --> 00:30:01,669
of what's what is the purpose of a model

571
00:29:59,000 --> 00:30:04,070
is what sort of access that model has

572
00:30:01,669 --> 00:30:06,590
like what sort of access what sort of

573
00:30:04,070 --> 00:30:09,139
features that model is accessing as well

574
00:30:06,590 --> 00:30:11,299
as the data and it becomes very

575
00:30:09,140 --> 00:30:15,409
important to maintain a meta data system

576
00:30:11,299 --> 00:30:18,980
which will have Association of all this

577
00:30:15,409 --> 00:30:21,500
information as well as sensitivity and

578
00:30:18,980 --> 00:30:25,549
rotation of each and every feature used

579
00:30:21,500 --> 00:30:28,010
by that model so that controls can be

580
00:30:25,549 --> 00:30:30,429
designed as well as segregated based on

581
00:30:28,010 --> 00:30:30,429
that information

582
00:30:30,840 --> 00:30:36,730
monitoring continuous monitoring is

583
00:30:33,250 --> 00:30:39,400
important make sure you whenever you're

584
00:30:36,730 --> 00:30:45,640
designing your mo a pipeline security

585
00:30:39,400 --> 00:30:47,560
patterns you are applying the concept of

586
00:30:45,640 --> 00:30:49,300
continued continuous monitoring that

587
00:30:47,560 --> 00:30:52,570
means you are scanning your environment

588
00:30:49,300 --> 00:30:54,960
regularly for any vulnerabilities many

589
00:30:52,570 --> 00:30:57,790
monitoring the infrastructure for any

590
00:30:54,960 --> 00:31:00,580
specific performance degradation because

591
00:30:57,790 --> 00:31:02,409
that's one of the major risk in machine

592
00:31:00,580 --> 00:31:05,439
learning pipelines as well as collecting

593
00:31:02,410 --> 00:31:10,720
as many security metrics as you can do

594
00:31:05,440 --> 00:31:16,390
to design controls security

595
00:31:10,720 --> 00:31:18,700
infrastructure in ml in pipeline there

596
00:31:16,390 --> 00:31:22,600
are two key security infrastructure

597
00:31:18,700 --> 00:31:25,690
components one is Identity and Access

598
00:31:22,600 --> 00:31:28,750
Management and the other is your key

599
00:31:25,690 --> 00:31:30,070
management system so make sure whenever

600
00:31:28,750 --> 00:31:32,440
you are designing your security

601
00:31:30,070 --> 00:31:38,230
infrastructure that should be scalable

602
00:31:32,440 --> 00:31:41,640
and highly available any any any design

603
00:31:38,230 --> 00:31:44,320
should incorporate the concept of

604
00:31:41,640 --> 00:31:46,210
automation that's where the security

605
00:31:44,320 --> 00:31:48,730
control scaling comes into picture all

606
00:31:46,210 --> 00:31:51,070
the controls we have spoken about or

607
00:31:48,730 --> 00:31:53,920
what ana mitra has spoken up about

608
00:31:51,070 --> 00:31:59,980
should be automated with supports the

609
00:31:53,920 --> 00:32:03,700
concept of scaling with that thank you

610
00:31:59,980 --> 00:32:06,220
and if you want to contact us email

611
00:32:03,700 --> 00:32:10,300
address is here otherwise we will be

612
00:32:06,220 --> 00:32:13,090
available at the speaker cover corner we

613
00:32:10,300 --> 00:32:18,250
also have a team channel which is

614
00:32:13,090 --> 00:32:21,030
blackhat 2019 Li ml so contact us over

615
00:32:18,250 --> 00:32:21,030
there thank you

616
00:32:21,240 --> 00:32:23,300
you

