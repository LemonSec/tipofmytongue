1
00:00:01,040 --> 00:00:16,640
[Music]

2
00:00:15,200 --> 00:00:19,038
johannes will talk about

3
00:00:16,640 --> 00:00:20,880
human rights in the age of artificial

4
00:00:19,039 --> 00:00:23,119
intelligence

5
00:00:20,880 --> 00:00:24,480
johannes is a volunteer at amnesty

6
00:00:23,119 --> 00:00:26,240
international

7
00:00:24,480 --> 00:00:28,160
he is a member of the amnesty

8
00:00:26,240 --> 00:00:28,720
international expert group on human

9
00:00:28,160 --> 00:00:32,159
rights

10
00:00:28,720 --> 00:00:35,600
in the digital age

11
00:00:32,159 --> 00:00:37,199
professionally he works on the topic of

12
00:00:35,600 --> 00:00:40,480
effects of algorithms

13
00:00:37,200 --> 00:00:43,440
on digital platforms

14
00:00:40,480 --> 00:00:45,680
if you would like to post questions for

15
00:00:43,440 --> 00:00:47,839
the q and a session afterwards

16
00:00:45,680 --> 00:00:49,559
you can post them on twitter under

17
00:00:47,840 --> 00:00:53,120
hashtag

18
00:00:49,559 --> 00:00:57,519
rc3 oru in one word in small

19
00:00:53,120 --> 00:01:02,000
or in the irc channel under rc3

20
00:00:57,520 --> 00:01:05,840
minus oyo now a warm welcome to

21
00:01:02,000 --> 00:01:08,240
johannes and enjoy the talk

22
00:01:05,840 --> 00:01:09,760
welcome to our talk human rights in the

23
00:01:08,240 --> 00:01:13,439
age of ai

24
00:01:09,760 --> 00:01:16,320
dystopia or shining future

25
00:01:13,439 --> 00:01:17,360
tonight i want to take you on a

26
00:01:16,320 --> 00:01:20,399
top-level

27
00:01:17,360 --> 00:01:24,000
overview tour through this vast

28
00:01:20,400 --> 00:01:26,880
and possibly endless seeming field

29
00:01:24,000 --> 00:01:27,920
my goal is to give an introduction to

30
00:01:26,880 --> 00:01:30,560
the topic that is

31
00:01:27,920 --> 00:01:33,280
accessible to beginners but also to

32
00:01:30,560 --> 00:01:36,320
enrich this talk

33
00:01:33,280 --> 00:01:41,840
by information that makes it valuable

34
00:01:36,320 --> 00:01:45,039
and worthwhile for advanced audiences

35
00:01:41,840 --> 00:01:46,560
my name is johannes walter and i am

36
00:01:45,040 --> 00:01:48,000
speaking to you tonight as a

37
00:01:46,560 --> 00:01:51,759
representative

38
00:01:48,000 --> 00:01:54,399
of amnesty international

39
00:01:51,759 --> 00:01:56,079
let me try to explain in four bullet

40
00:01:54,399 --> 00:01:58,320
points or less

41
00:01:56,079 --> 00:02:00,320
who amnesty international is and what we

42
00:01:58,320 --> 00:02:05,360
do just so you know

43
00:02:00,320 --> 00:02:08,720
who is talking to you and why

44
00:02:05,360 --> 00:02:11,680
so maybe in one sentence

45
00:02:08,720 --> 00:02:13,440
amnesty's mission is to campaign for a

46
00:02:11,680 --> 00:02:17,440
world where human rights

47
00:02:13,440 --> 00:02:20,319
are enjoyed by everyone

48
00:02:17,440 --> 00:02:21,040
we are a non-governmental organization

49
00:02:20,319 --> 00:02:25,599
that is

50
00:02:21,040 --> 00:02:28,079
independent of any political ideology

51
00:02:25,599 --> 00:02:30,720
or economic interest or any type of

52
00:02:28,080 --> 00:02:33,519
religious belief

53
00:02:30,720 --> 00:02:34,640
so what is it that we're actually doing

54
00:02:33,519 --> 00:02:37,599
in order to achieve

55
00:02:34,640 --> 00:02:39,839
our goal of living in a world where

56
00:02:37,599 --> 00:02:42,640
everybody enjoys human rights

57
00:02:39,840 --> 00:02:43,200
very broadly speaking very broadly

58
00:02:42,640 --> 00:02:47,279
speaking

59
00:02:43,200 --> 00:02:50,640
it is two things we do for one

60
00:02:47,280 --> 00:02:51,760
we are a lobby group we lobby

61
00:02:50,640 --> 00:02:55,359
governments

62
00:02:51,760 --> 00:02:57,679
and corporations companies

63
00:02:55,360 --> 00:02:58,959
such that they stick to their promises

64
00:02:57,680 --> 00:03:02,239
and that they

65
00:02:58,959 --> 00:03:03,120
respect international law amnesty

66
00:03:02,239 --> 00:03:06,400
globally has

67
00:03:03,120 --> 00:03:10,000
several million members and

68
00:03:06,400 --> 00:03:14,000
we are leveraging that human power

69
00:03:10,000 --> 00:03:15,280
in order to document and uncover human

70
00:03:14,000 --> 00:03:19,120
rights violations

71
00:03:15,280 --> 00:03:23,280
all over the world and then to

72
00:03:19,120 --> 00:03:26,959
use our ability to create

73
00:03:23,280 --> 00:03:29,760
publicity to build up pressure on

74
00:03:26,959 --> 00:03:32,480
governments and corporations

75
00:03:29,760 --> 00:03:34,319
to make sure that they um respect human

76
00:03:32,480 --> 00:03:38,079
rights

77
00:03:34,319 --> 00:03:39,359
and then the second thing we generally

78
00:03:38,080 --> 00:03:43,040
do

79
00:03:39,360 --> 00:03:45,920
is we try to keep the public

80
00:03:43,040 --> 00:03:47,599
informed about human rights related

81
00:03:45,920 --> 00:03:50,480
topics

82
00:03:47,599 --> 00:03:51,440
because we believe that the best

83
00:03:50,480 --> 00:03:55,040
outcomes

84
00:03:51,440 --> 00:03:59,920
for a society are achieved

85
00:03:55,040 --> 00:04:00,720
when that society is engaging in a

86
00:03:59,920 --> 00:04:04,000
debate

87
00:04:00,720 --> 00:04:08,159
in a discussion on

88
00:04:04,000 --> 00:04:10,720
how to solve any kind of problem really

89
00:04:08,159 --> 00:04:11,519
and we believe that the results that

90
00:04:10,720 --> 00:04:14,239
come out of

91
00:04:11,519 --> 00:04:15,040
these debates are the better the better

92
00:04:14,239 --> 00:04:18,478
informed

93
00:04:15,040 --> 00:04:19,199
the public is and that is also the

94
00:04:18,478 --> 00:04:24,000
reason why

95
00:04:19,199 --> 00:04:24,000
i am speaking to you tonight

96
00:04:24,479 --> 00:04:32,000
now i feel like it is warranted to start

97
00:04:28,479 --> 00:04:32,400
any type of presentation that throws the

98
00:04:32,000 --> 00:04:35,759
term

99
00:04:32,400 --> 00:04:38,880
ai around by clearly

100
00:04:35,759 --> 00:04:39,759
stating and clearly defining what is

101
00:04:38,880 --> 00:04:42,320
meant

102
00:04:39,759 --> 00:04:43,199
by artificial intelligence such a

103
00:04:42,320 --> 00:04:46,159
definition

104
00:04:43,199 --> 00:04:48,479
is crucial for a couple of reasons

105
00:04:46,160 --> 00:04:52,240
really

106
00:04:48,479 --> 00:04:54,080
for one the

107
00:04:52,240 --> 00:04:55,360
ethical assessment of the moral

108
00:04:54,080 --> 00:04:59,198
challenges that

109
00:04:55,360 --> 00:05:01,440
come about with ai hinge critically

110
00:04:59,199 --> 00:05:02,320
on the definition you get the definition

111
00:05:01,440 --> 00:05:05,919
wrong and

112
00:05:02,320 --> 00:05:08,320
the discussion turns into

113
00:05:05,919 --> 00:05:10,719
science fiction in the best case in the

114
00:05:08,320 --> 00:05:12,719
worst case it's a distraction from the

115
00:05:10,720 --> 00:05:16,000
actual problem

116
00:05:12,720 --> 00:05:18,400
but then there is also this phenomenon

117
00:05:16,000 --> 00:05:20,240
and we talk about this topic a lot to

118
00:05:18,400 --> 00:05:21,919
people

119
00:05:20,240 --> 00:05:24,400
there is this phenomenon that mentioning

120
00:05:21,919 --> 00:05:27,440
the term ai by this point really causes

121
00:05:24,400 --> 00:05:29,919
a mental chain reaction

122
00:05:27,440 --> 00:05:30,719
that is going on in the heads of people

123
00:05:29,919 --> 00:05:33,039
for some

124
00:05:30,720 --> 00:05:33,919
mentioning ai causes them to be

125
00:05:33,039 --> 00:05:37,039
immediately

126
00:05:33,919 --> 00:05:39,840
annoyed and turned off because they are

127
00:05:37,039 --> 00:05:41,680
worn down and dulled by the constant

128
00:05:39,840 --> 00:05:45,599
overuse of the term

129
00:05:41,680 --> 00:05:47,039
in meaningless marketing like settings

130
00:05:45,600 --> 00:05:49,520
and then on the other end of the

131
00:05:47,039 --> 00:05:52,560
spectrum you have people who are

132
00:05:49,520 --> 00:05:54,240
super excited and thrilled

133
00:05:52,560 --> 00:05:56,080
as soon as they hear the term ai and

134
00:05:54,240 --> 00:05:57,160
they're ready to embark on a discussion

135
00:05:56,080 --> 00:06:00,719
about the

136
00:05:57,160 --> 00:06:04,840
singularity and superhuman ai

137
00:06:00,720 --> 00:06:09,680
and i think a scientifically sound

138
00:06:04,840 --> 00:06:09,679
approach and one that is also closest to

139
00:06:09,759 --> 00:06:13,199
the results that the leading i.t

140
00:06:12,479 --> 00:06:16,318
companies

141
00:06:13,199 --> 00:06:17,759
are achieving these days could be the

142
00:06:16,319 --> 00:06:20,800
following

143
00:06:17,759 --> 00:06:25,039
say ai is software

144
00:06:20,800 --> 00:06:28,479
that uses statistical algorithms

145
00:06:25,039 --> 00:06:32,000
to search and find patterns

146
00:06:28,479 --> 00:06:34,159
in large amounts of data

147
00:06:32,000 --> 00:06:35,919
and then it's it's using these learn

148
00:06:34,160 --> 00:06:39,360
correlations in the data

149
00:06:35,919 --> 00:06:41,280
to make predictions about data points

150
00:06:39,360 --> 00:06:44,720
that it hasn't seen yet

151
00:06:41,280 --> 00:06:46,559
and such a software of course can also

152
00:06:44,720 --> 00:06:51,360
run on hardware so this would

153
00:06:46,560 --> 00:06:51,360
include robotics obviously just as well

154
00:06:51,759 --> 00:06:54,000
now

155
00:06:55,120 --> 00:07:00,319
with this method

156
00:06:58,240 --> 00:07:02,000
companies over the course of the last

157
00:07:00,319 --> 00:07:05,280
five or maybe even already

158
00:07:02,000 --> 00:07:06,639
eight years have achieved tremendous

159
00:07:05,280 --> 00:07:08,400
results and

160
00:07:06,639 --> 00:07:10,240
and those results are the reasons really

161
00:07:08,400 --> 00:07:13,840
why we are in a

162
00:07:10,240 --> 00:07:17,520
in a up wave in a in a a.i

163
00:07:13,840 --> 00:07:20,719
boom um these days so

164
00:07:17,520 --> 00:07:24,080
computers can nowadays reliably see

165
00:07:20,720 --> 00:07:27,680
and speak and listen

166
00:07:24,080 --> 00:07:29,919
react in intelligent ways and

167
00:07:27,680 --> 00:07:30,960
in the last two years or so also we

168
00:07:29,919 --> 00:07:34,479
started

169
00:07:30,960 --> 00:07:37,120
seeing ai's pop up that can really

170
00:07:34,479 --> 00:07:40,240
start to generate and create their own

171
00:07:37,120 --> 00:07:40,240
creative content

172
00:07:40,479 --> 00:07:47,919
and um yeah

173
00:07:44,240 --> 00:07:48,800
and and that's why um it makes sense to

174
00:07:47,919 --> 00:07:50,799
talk about this

175
00:07:48,800 --> 00:07:52,479
now i know that um some of you might be

176
00:07:50,800 --> 00:07:54,080
thinking that the definition i just gave

177
00:07:52,479 --> 00:07:57,039
you is closer to

178
00:07:54,080 --> 00:07:57,758
um what is typically meant by machine

179
00:07:57,039 --> 00:08:00,560
learning

180
00:07:57,759 --> 00:08:02,960
and i'm aware that usually ai is an

181
00:08:00,560 --> 00:08:04,960
umbrella term that is

182
00:08:02,960 --> 00:08:07,280
including but not limited to machine

183
00:08:04,960 --> 00:08:08,638
learning but

184
00:08:07,280 --> 00:08:11,198
as you will see throughout the

185
00:08:08,639 --> 00:08:14,160
presentation

186
00:08:11,199 --> 00:08:16,080
this definition will serve us in the

187
00:08:14,160 --> 00:08:20,000
scope of this presentation just fine

188
00:08:16,080 --> 00:08:20,000
and therefore i will run with it

189
00:08:21,280 --> 00:08:24,559
now in what ways does artificial

190
00:08:23,440 --> 00:08:28,000
intelligence

191
00:08:24,560 --> 00:08:30,080
hurt us already today and in what ways

192
00:08:28,000 --> 00:08:34,320
can it possibly develop

193
00:08:30,080 --> 00:08:37,279
into an even bigger threat in the future

194
00:08:34,320 --> 00:08:38,320
one um research article that really

195
00:08:37,279 --> 00:08:41,439
kick-started

196
00:08:38,320 --> 00:08:44,959
the whole a wider debate about the

197
00:08:41,440 --> 00:08:47,680
ethical repercussions of ai is

198
00:08:44,959 --> 00:08:51,199
the one from two years ago from uh bro

199
00:08:47,680 --> 00:08:51,199
la vinnie and gabriel

200
00:08:51,440 --> 00:08:58,560
in which the they looked at

201
00:08:55,120 --> 00:09:00,720
facial recognition algorithms so

202
00:08:58,560 --> 00:09:03,599
two years ago at the time they took

203
00:09:00,720 --> 00:09:06,640
three of the most widely used commercial

204
00:09:03,600 --> 00:09:09,120
facial recognition algorithms one was

205
00:09:06,640 --> 00:09:11,279
microsoft and i forgot the other two but

206
00:09:09,120 --> 00:09:13,760
the big it companies

207
00:09:11,279 --> 00:09:14,560
and what they did was they were trying

208
00:09:13,760 --> 00:09:17,920
to assess

209
00:09:14,560 --> 00:09:20,959
the accuracy of the algorithm but

210
00:09:17,920 --> 00:09:24,000
breaking that accuracy down for

211
00:09:20,959 --> 00:09:26,959
different demographics

212
00:09:24,000 --> 00:09:28,959
and what they found was quite striking

213
00:09:26,959 --> 00:09:32,640
so

214
00:09:28,959 --> 00:09:35,839
for the group of light-skinned males

215
00:09:32,640 --> 00:09:37,600
the algorithm worked almost perfectly

216
00:09:35,839 --> 00:09:41,200
the error rate was

217
00:09:37,600 --> 00:09:42,640
not 0.8 percent but for dark skinned

218
00:09:41,200 --> 00:09:46,080
women

219
00:09:42,640 --> 00:09:47,519
the arrow rate was more than 40 times

220
00:09:46,080 --> 00:09:52,160
worse

221
00:09:47,519 --> 00:09:56,240
so if the algorithm was trying to

222
00:09:52,160 --> 00:09:59,519
if it sees if it saw a new picture of a

223
00:09:56,240 --> 00:10:02,160
people of a female people of color then

224
00:09:59,519 --> 00:10:03,440
it would in almost 35 percent of the

225
00:10:02,160 --> 00:10:06,319
cases

226
00:10:03,440 --> 00:10:08,640
misclassify that person as male for

227
00:10:06,320 --> 00:10:11,920
example

228
00:10:08,640 --> 00:10:13,680
now these algorithms

229
00:10:11,920 --> 00:10:15,519
were already used at the time so we're

230
00:10:13,680 --> 00:10:18,399
not talking about something that lies in

231
00:10:15,519 --> 00:10:22,160
the future in fact

232
00:10:18,399 --> 00:10:25,839
harm by such algorithms is happening

233
00:10:22,160 --> 00:10:29,920
right now 2020 saw the first

234
00:10:25,839 --> 00:10:33,920
case where an american citizen

235
00:10:29,920 --> 00:10:36,640
robert williams was wrongfully arrested

236
00:10:33,920 --> 00:10:38,160
due to a mismatch by a facial

237
00:10:36,640 --> 00:10:41,839
recognition algorithm

238
00:10:38,160 --> 00:10:44,160
that the police were running

239
00:10:41,839 --> 00:10:45,040
the story would be kind of entertaining

240
00:10:44,160 --> 00:10:48,319
if it wasn't

241
00:10:45,040 --> 00:10:49,439
that unfortunate and sad because as he

242
00:10:48,320 --> 00:10:52,160
as he states

243
00:10:49,440 --> 00:10:53,760
he was working a normal shift when he

244
00:10:52,160 --> 00:10:55,839
got a call from the local police

245
00:10:53,760 --> 00:10:59,120
department

246
00:10:55,839 --> 00:11:01,760
asking him to turn himself

247
00:10:59,120 --> 00:11:02,480
in for jail time so what was happened

248
00:11:01,760 --> 00:11:05,600
was

249
00:11:02,480 --> 00:11:08,560
the um police found

250
00:11:05,600 --> 00:11:10,720
um or the the police was invested was

251
00:11:08,560 --> 00:11:14,399
investigating a case

252
00:11:10,720 --> 00:11:18,160
of a minor robbery of a local store

253
00:11:14,399 --> 00:11:20,079
and the cctv the video footage of that

254
00:11:18,160 --> 00:11:22,640
store recorded a

255
00:11:20,079 --> 00:11:24,079
face of a black man the police ran that

256
00:11:22,640 --> 00:11:24,720
facial recognition algorithm and the

257
00:11:24,079 --> 00:11:28,319
match

258
00:11:24,720 --> 00:11:33,040
spit out this man robert

259
00:11:28,320 --> 00:11:36,079
williams and he even ended up

260
00:11:33,040 --> 00:11:38,719
doing jail time even though later

261
00:11:36,079 --> 00:11:40,719
it was of course then discovered that he

262
00:11:38,720 --> 00:11:42,880
was not responsible

263
00:11:40,720 --> 00:11:45,120
and he receives an apology from the

264
00:11:42,880 --> 00:11:49,600
police it is an interesting case

265
00:11:45,120 --> 00:11:49,600
as it is the first account we know of

266
00:11:50,720 --> 00:11:56,720
um so

267
00:11:53,760 --> 00:11:58,720
like we've seen in these examples

268
00:11:56,720 --> 00:12:01,360
algorithms

269
00:11:58,720 --> 00:12:02,880
can show discriminatory behavior and

270
00:12:01,360 --> 00:12:05,600
that comes maybe as a

271
00:12:02,880 --> 00:12:07,600
surprise to some because naively you

272
00:12:05,600 --> 00:12:12,560
could think that

273
00:12:07,600 --> 00:12:16,240
computers are these hyper rational

274
00:12:12,560 --> 00:12:18,719
machines that are strangers to

275
00:12:16,240 --> 00:12:21,200
any kind of emotional bias and therefore

276
00:12:18,720 --> 00:12:24,320
discrimination shouldn't be a problem

277
00:12:21,200 --> 00:12:26,160
but as as we just saw it is and so the

278
00:12:24,320 --> 00:12:27,440
question is how can that how can that

279
00:12:26,160 --> 00:12:29,519
happen

280
00:12:27,440 --> 00:12:31,120
and of course as many of you probably

281
00:12:29,519 --> 00:12:34,720
already know

282
00:12:31,120 --> 00:12:38,399
one way biases can be introduced into ai

283
00:12:34,720 --> 00:12:42,079
is by using bad training data and

284
00:12:38,399 --> 00:12:47,279
one particularly striking example

285
00:12:42,079 --> 00:12:50,160
is the story of inyoluva raji

286
00:12:47,279 --> 00:12:52,639
this young woman nigerian born but now

287
00:12:50,160 --> 00:12:55,839
living in the u.s

288
00:12:52,639 --> 00:12:58,959
in was an intern did an internship at an

289
00:12:55,839 --> 00:13:02,399
ai at the ai company

290
00:12:58,959 --> 00:13:06,399
clarify and

291
00:13:02,399 --> 00:13:08,480
what she was working on there was a

292
00:13:06,399 --> 00:13:11,600
facial recognition algorithm

293
00:13:08,480 --> 00:13:15,440
that was supposed to help

294
00:13:11,600 --> 00:13:19,279
clients flag inappropriate images

295
00:13:15,440 --> 00:13:22,720
as not safe for work

296
00:13:19,279 --> 00:13:25,600
what she soon realized was that

297
00:13:22,720 --> 00:13:26,240
images that contained people of color

298
00:13:25,600 --> 00:13:28,320
were

299
00:13:26,240 --> 00:13:29,440
deemed inappropriate at a much higher

300
00:13:28,320 --> 00:13:35,120
rate than

301
00:13:29,440 --> 00:13:38,320
imagery that contained only white people

302
00:13:35,120 --> 00:13:41,360
and so she started to investigate and

303
00:13:38,320 --> 00:13:43,760
what she curiously found out was

304
00:13:41,360 --> 00:13:45,279
the problem was in the way the ai was

305
00:13:43,760 --> 00:13:48,240
trained

306
00:13:45,279 --> 00:13:49,279
so the the ai learned inappropriate

307
00:13:48,240 --> 00:13:52,800
contract

308
00:13:49,279 --> 00:13:53,760
from pornography footage and appropriate

309
00:13:52,800 --> 00:13:57,120
content

310
00:13:53,760 --> 00:13:57,760
from looking at stock photos as it turns

311
00:13:57,120 --> 00:14:00,959
out

312
00:13:57,760 --> 00:14:02,079
pawn is much more diverse in terms of

313
00:14:00,959 --> 00:14:06,160
skin colors

314
00:14:02,079 --> 00:14:07,839
then is stock footage which contains

315
00:14:06,160 --> 00:14:10,719
mostly white people

316
00:14:07,839 --> 00:14:12,000
so the algorithm learned to associate

317
00:14:10,720 --> 00:14:15,440
black skin

318
00:14:12,000 --> 00:14:18,399
with inappropriate content

319
00:14:15,440 --> 00:14:18,959
interestingly when she when she raised

320
00:14:18,399 --> 00:14:22,320
this

321
00:14:18,959 --> 00:14:23,920
finding um to when she made it aware

322
00:14:22,320 --> 00:14:26,079
when she brought it to the awareness of

323
00:14:23,920 --> 00:14:28,399
her managers

324
00:14:26,079 --> 00:14:30,880
they were in fact not doing anything

325
00:14:28,399 --> 00:14:33,920
about it the sentiment was

326
00:14:30,880 --> 00:14:35,279
it is difficult enough to find good

327
00:14:33,920 --> 00:14:38,240
training data

328
00:14:35,279 --> 00:14:40,000
or training data at a large scale at all

329
00:14:38,240 --> 00:14:41,880
so we're not going to worry too much

330
00:14:40,000 --> 00:14:44,399
about

331
00:14:41,880 --> 00:14:46,639
representativeness representatives for

332
00:14:44,399 --> 00:14:46,639
now

333
00:14:48,880 --> 00:14:53,279
okay so so much about bad training data

334
00:14:51,839 --> 00:14:57,120
but there are other ways

335
00:14:53,279 --> 00:15:00,480
on um in which ai's can be biased

336
00:14:57,120 --> 00:15:05,040
as well and

337
00:15:00,480 --> 00:15:08,720
one important other reason is if you

338
00:15:05,040 --> 00:15:10,719
tell the ai the wrong thing to do

339
00:15:08,720 --> 00:15:12,399
if you're not careful about how to

340
00:15:10,720 --> 00:15:16,000
specify the target

341
00:15:12,399 --> 00:15:19,199
objective objective function of the

342
00:15:16,000 --> 00:15:22,399
algorithm so i want to share

343
00:15:19,199 --> 00:15:24,000
a very interesting story at least in my

344
00:15:22,399 --> 00:15:26,720
opinion

345
00:15:24,000 --> 00:15:28,560
and that is the story of how two

346
00:15:26,720 --> 00:15:31,759
researchers found

347
00:15:28,560 --> 00:15:35,359
a gender bias in ad algorithms

348
00:15:31,759 --> 00:15:38,800
on facebook so

349
00:15:35,360 --> 00:15:42,079
what the authors did was they ran

350
00:15:38,800 --> 00:15:45,680
an ad campaign for stem

351
00:15:42,079 --> 00:15:46,880
degrees on facebook stem of course being

352
00:15:45,680 --> 00:15:49,920
science technology

353
00:15:46,880 --> 00:15:53,439
engineering and mathematics and

354
00:15:49,920 --> 00:15:56,560
what would happen is they they just

355
00:15:53,440 --> 00:15:57,519
went into the regular ad advertisement

356
00:15:56,560 --> 00:16:00,719
um

357
00:15:57,519 --> 00:16:03,360
advertising way on facebook and

358
00:16:00,720 --> 00:16:04,160
people would be shown this ad and when

359
00:16:03,360 --> 00:16:07,199
they click on it

360
00:16:04,160 --> 00:16:08,079
it would take them on a website that

361
00:16:07,199 --> 00:16:11,279
would inform them

362
00:16:08,079 --> 00:16:14,319
about the advantages of studying stem

363
00:16:11,279 --> 00:16:17,439
and about you know finding out job

364
00:16:14,320 --> 00:16:20,480
opportunities and the like

365
00:16:17,440 --> 00:16:20,880
so they ran this for a couple of weeks

366
00:16:20,480 --> 00:16:23,120
and

367
00:16:20,880 --> 00:16:25,279
when the campaign was done they analyzed

368
00:16:23,120 --> 00:16:28,560
the data and what they

369
00:16:25,279 --> 00:16:32,480
saw was that the algorithm

370
00:16:28,560 --> 00:16:37,599
chose to show this ad much more often

371
00:16:32,480 --> 00:16:40,959
to male audiences than to female ones

372
00:16:37,600 --> 00:16:43,519
now um if we are making an

373
00:16:40,959 --> 00:16:44,880
ad now for example for any type of

374
00:16:43,519 --> 00:16:47,279
consumer product

375
00:16:44,880 --> 00:16:48,480
that might not be a problem but if we're

376
00:16:47,279 --> 00:16:51,199
talking about com

377
00:16:48,480 --> 00:16:52,240
painting advertising for something that

378
00:16:51,199 --> 00:16:55,680
is

379
00:16:52,240 --> 00:16:56,560
having further implications for the

380
00:16:55,680 --> 00:16:59,920
society

381
00:16:56,560 --> 00:17:00,239
like who studies what and why then maybe

382
00:16:59,920 --> 00:17:02,639
we

383
00:17:00,240 --> 00:17:04,559
want to drill into the reason for why

384
00:17:02,639 --> 00:17:05,839
the algorithm chose to discriminate

385
00:17:04,559 --> 00:17:08,079
between genders here

386
00:17:05,839 --> 00:17:08,958
and the authors first thought that well

387
00:17:08,079 --> 00:17:12,159
okay

388
00:17:08,959 --> 00:17:15,439
maybe men are just more interested

389
00:17:12,160 --> 00:17:17,039
in that in this ad and are more likely

390
00:17:15,439 --> 00:17:18,559
to click on it

391
00:17:17,039 --> 00:17:20,559
more likely than women anyways and

392
00:17:18,559 --> 00:17:22,240
therefore it would be somewhat justified

393
00:17:20,559 --> 00:17:25,280
to show it more often to men

394
00:17:22,240 --> 00:17:27,919
but when they did the analysis they uh

395
00:17:25,280 --> 00:17:28,639
to their surprise found out that the uh

396
00:17:27,919 --> 00:17:30,880
chances

397
00:17:28,640 --> 00:17:33,520
for men and women to click on this ad

398
00:17:30,880 --> 00:17:35,280
were basically exactly the same

399
00:17:33,520 --> 00:17:38,080
so now it's getting really interesting

400
00:17:35,280 --> 00:17:38,080
right why

401
00:17:38,160 --> 00:17:41,600
if that is the case then it really seems

402
00:17:39,840 --> 00:17:43,918
like the algorithm is discriminating

403
00:17:41,600 --> 00:17:46,399
women here

404
00:17:43,919 --> 00:17:48,400
when they drilled further they found out

405
00:17:46,400 --> 00:17:52,880
that the reason lies in the

406
00:17:48,400 --> 00:17:56,080
way the target for the ai is defined

407
00:17:52,880 --> 00:17:56,880
so the algorithm was told or the way the

408
00:17:56,080 --> 00:18:00,480
algorithm is

409
00:17:56,880 --> 00:18:03,600
coded is to maximize

410
00:18:00,480 --> 00:18:08,400
the ratio between

411
00:18:03,600 --> 00:18:11,439
impact and cost so that

412
00:18:08,400 --> 00:18:11,440
it would show the ad

413
00:18:11,919 --> 00:18:15,360
yeah that it would maximize this ratio

414
00:18:14,799 --> 00:18:18,879
and

415
00:18:15,360 --> 00:18:22,159
now it turns out that

416
00:18:18,880 --> 00:18:24,640
female eyeballs having a contact with an

417
00:18:22,160 --> 00:18:26,480
ad impression

418
00:18:24,640 --> 00:18:28,480
is actually more valuable for

419
00:18:26,480 --> 00:18:32,160
advertisers than

420
00:18:28,480 --> 00:18:35,760
showing ads to men on average

421
00:18:32,160 --> 00:18:36,720
all things equal because as it turns out

422
00:18:35,760 --> 00:18:40,000
at least in the us

423
00:18:36,720 --> 00:18:42,160
and i don't doubt that it is very

424
00:18:40,000 --> 00:18:44,960
similar in europe as it turns out

425
00:18:42,160 --> 00:18:45,520
women are making the most decisions

426
00:18:44,960 --> 00:18:48,960
about

427
00:18:45,520 --> 00:18:52,000
what to buy big ticketed items

428
00:18:48,960 --> 00:18:53,120
and all the way down to everyday grocery

429
00:18:52,000 --> 00:18:56,000
shopping

430
00:18:53,120 --> 00:18:57,760
and because of that it is more enticing

431
00:18:56,000 --> 00:18:58,799
and interesting for advertisers to reach

432
00:18:57,760 --> 00:19:00,400
women

433
00:18:58,799 --> 00:19:02,320
because it's more valuable it's also

434
00:19:00,400 --> 00:19:05,039
more expensive

435
00:19:02,320 --> 00:19:06,960
now because the probability for men and

436
00:19:05,039 --> 00:19:07,840
women was more or less the same to click

437
00:19:06,960 --> 00:19:11,039
on the ad

438
00:19:07,840 --> 00:19:13,600
but women were more expensive it was

439
00:19:11,039 --> 00:19:16,000
optimal for the algorithm to show it

440
00:19:13,600 --> 00:19:19,360
more often to men

441
00:19:16,000 --> 00:19:20,640
now when they when the authors find out

442
00:19:19,360 --> 00:19:23,039
about that result

443
00:19:20,640 --> 00:19:24,160
their immediate first reaction would be

444
00:19:23,039 --> 00:19:26,160
was indeed

445
00:19:24,160 --> 00:19:27,200
to go to facebook and say hey facebook

446
00:19:26,160 --> 00:19:28,880
please

447
00:19:27,200 --> 00:19:30,840
we're aware of this problem here the

448
00:19:28,880 --> 00:19:32,240
algorithm seems to discriminate

449
00:19:30,840 --> 00:19:35,199
unjustifiedly

450
00:19:32,240 --> 00:19:36,640
please make sure that you show this ad

451
00:19:35,200 --> 00:19:39,919
in equal proportions to men

452
00:19:36,640 --> 00:19:41,760
and women but quite ironically exactly

453
00:19:39,919 --> 00:19:45,520
that is not possible

454
00:19:41,760 --> 00:19:47,679
under the current rules on facebook

455
00:19:45,520 --> 00:19:49,039
exactly in order to prevent

456
00:19:47,679 --> 00:19:52,240
discrimination

457
00:19:49,039 --> 00:19:55,840
based on gender so that story

458
00:19:52,240 --> 00:19:59,120
really is a nice example also of how

459
00:19:55,840 --> 00:20:02,320
we might have to rethink certain rules

460
00:19:59,120 --> 00:20:05,199
um now with the emergence of ai there's

461
00:20:02,320 --> 00:20:09,600
a wide red technology

462
00:20:05,200 --> 00:20:12,880
i want to share another example story of

463
00:20:09,600 --> 00:20:15,678
how a bad objective objective

464
00:20:12,880 --> 00:20:16,080
function can cause problems and that is

465
00:20:15,679 --> 00:20:18,720
from

466
00:20:16,080 --> 00:20:20,879
a study that was very nicely published

467
00:20:18,720 --> 00:20:23,120
in science

468
00:20:20,880 --> 00:20:24,640
what they did they looked at an

469
00:20:23,120 --> 00:20:28,158
algorithm that was used

470
00:20:24,640 --> 00:20:30,080
in the american healthcare system

471
00:20:28,159 --> 00:20:31,280
and the job of the algorithm was to

472
00:20:30,080 --> 00:20:35,199
support

473
00:20:31,280 --> 00:20:38,320
doctors of medicine it would

474
00:20:35,200 --> 00:20:41,600
make a suggestion of who should receive

475
00:20:38,320 --> 00:20:43,678
further intensive care

476
00:20:41,600 --> 00:20:45,918
which patients should receive more care

477
00:20:43,679 --> 00:20:48,080
and which are okay with

478
00:20:45,919 --> 00:20:50,080
receiving a little bit less intensive

479
00:20:48,080 --> 00:20:51,918
care

480
00:20:50,080 --> 00:20:53,120
when they looked at this algorithm again

481
00:20:51,919 --> 00:20:56,960
they found

482
00:20:53,120 --> 00:20:59,520
that for patients in the same conditions

483
00:20:56,960 --> 00:21:01,360
black patients were recommended at much

484
00:20:59,520 --> 00:21:03,840
lower rates for intensive care than

485
00:21:01,360 --> 00:21:03,840
white patients

486
00:21:04,159 --> 00:21:08,960
and what they found out was the

487
00:21:07,120 --> 00:21:12,239
algorithm was told

488
00:21:08,960 --> 00:21:12,240
to proxy

489
00:21:12,320 --> 00:21:16,240
medical the need for medical intensive

490
00:21:15,280 --> 00:21:19,918
care

491
00:21:16,240 --> 00:21:21,120
by how much money the health care system

492
00:21:19,919 --> 00:21:24,159
spends

493
00:21:21,120 --> 00:21:26,799
on a certain type of patient

494
00:21:24,159 --> 00:21:27,679
now because the american healthcare

495
00:21:26,799 --> 00:21:31,400
system

496
00:21:27,679 --> 00:21:34,799
is structured is structurally

497
00:21:31,400 --> 00:21:37,280
disadvantaging black people

498
00:21:34,799 --> 00:21:38,240
there is less money spent in the

499
00:21:37,280 --> 00:21:40,960
healthcare system

500
00:21:38,240 --> 00:21:43,039
already over the last decades on black

501
00:21:40,960 --> 00:21:44,640
people than on white people so

502
00:21:43,039 --> 00:21:47,039
again with the same conditions black

503
00:21:44,640 --> 00:21:47,679
people would decided by humans now

504
00:21:47,039 --> 00:21:50,320
receive

505
00:21:47,679 --> 00:21:52,080
less care and less money the algorithm

506
00:21:50,320 --> 00:21:55,360
seeing this data would infer

507
00:21:52,080 --> 00:21:56,960
that black people are more healthy and

508
00:21:55,360 --> 00:22:01,280
don't need as much care

509
00:21:56,960 --> 00:22:05,120
which is of course bringing this whole

510
00:22:01,280 --> 00:22:05,120
argument ad absurder

511
00:22:06,240 --> 00:22:12,320
so we've seen now how

512
00:22:10,000 --> 00:22:13,760
ais can discriminate and for what

513
00:22:12,320 --> 00:22:16,399
reasons that is

514
00:22:13,760 --> 00:22:17,120
now i want to talk a little bit about

515
00:22:16,400 --> 00:22:21,760
another

516
00:22:17,120 --> 00:22:25,280
important way in which ai could be

517
00:22:21,760 --> 00:22:29,280
detrimental to our societies and that is

518
00:22:25,280 --> 00:22:31,918
talking about deep fakes now

519
00:22:29,280 --> 00:22:33,600
without delving into the technicalities

520
00:22:31,919 --> 00:22:37,200
too much

521
00:22:33,600 --> 00:22:40,320
deep fakes are manipulated video audio

522
00:22:37,200 --> 00:22:42,159
or images and

523
00:22:40,320 --> 00:22:44,480
they have been manipulated by so-called

524
00:22:42,159 --> 00:22:47,679
deep neural networks

525
00:22:44,480 --> 00:22:48,480
and in effect what that means is we can

526
00:22:47,679 --> 00:22:51,919
now

527
00:22:48,480 --> 00:22:55,600
create videos that

528
00:22:51,919 --> 00:23:00,240
can be altered at an unprecedented ease

529
00:22:55,600 --> 00:23:03,678
and at almost this is close to zero cost

530
00:23:00,240 --> 00:23:06,159
and it is easy to imagine how that can

531
00:23:03,679 --> 00:23:10,000
be dangerous

532
00:23:06,159 --> 00:23:13,840
for example i've seen a paper recently

533
00:23:10,000 --> 00:23:16,960
that introduced an ai that is capable of

534
00:23:13,840 --> 00:23:18,320
removing people or objects out of a

535
00:23:16,960 --> 00:23:21,760
video entire

536
00:23:18,320 --> 00:23:22,158
entirely without living leaving almost

537
00:23:21,760 --> 00:23:26,320
any

538
00:23:22,159 --> 00:23:26,320
artifacts in the in the image

539
00:23:28,240 --> 00:23:35,520
we've seen the last two u.s elections

540
00:23:32,240 --> 00:23:36,480
we've seen brexit we've seen over the

541
00:23:35,520 --> 00:23:39,600
last nine months

542
00:23:36,480 --> 00:23:42,880
the debate going on going on about

543
00:23:39,600 --> 00:23:47,199
covet 19 and it is really easy

544
00:23:42,880 --> 00:23:49,520
to to see how in the 2020s decade that

545
00:23:47,200 --> 00:23:52,640
is lying ahead of us

546
00:23:49,520 --> 00:23:55,600
our democratic discourse

547
00:23:52,640 --> 00:23:56,080
can be negatively influenced by bringing

548
00:23:55,600 --> 00:23:59,360
about

549
00:23:56,080 --> 00:24:00,320
fake news deep fakes into into the

550
00:23:59,360 --> 00:24:02,399
discussion

551
00:24:00,320 --> 00:24:04,399
especially considering that there are

552
00:24:02,400 --> 00:24:05,600
internationally actors that have a

553
00:24:04,400 --> 00:24:09,279
vested interest

554
00:24:05,600 --> 00:24:12,080
in interrupting a smooth democratic

555
00:24:09,279 --> 00:24:14,080
process in western countries

556
00:24:12,080 --> 00:24:15,918
but as it turns out it's not only

557
00:24:14,080 --> 00:24:16,879
western countries that are concerned

558
00:24:15,919 --> 00:24:19,760
about deep fakes

559
00:24:16,880 --> 00:24:20,799
so china's internet regulator for

560
00:24:19,760 --> 00:24:23,840
example

561
00:24:20,799 --> 00:24:26,000
announced um a ban of

562
00:24:23,840 --> 00:24:28,399
fake news that have been created by deep

563
00:24:26,000 --> 00:24:32,080
fakes and they even

564
00:24:28,400 --> 00:24:35,760
discussed to ban the

565
00:24:32,080 --> 00:24:38,480
deep fake technology altogether

566
00:24:35,760 --> 00:24:39,520
and then on the other side of the earth

567
00:24:38,480 --> 00:24:42,000
in the u.s

568
00:24:39,520 --> 00:24:43,760
california has already taken action

569
00:24:42,000 --> 00:24:46,640
against deep fakes

570
00:24:43,760 --> 00:24:47,520
such that since last year it is now

571
00:24:46,640 --> 00:24:50,080
illegal

572
00:24:47,520 --> 00:24:51,200
to use the neural networks to alter

573
00:24:50,080 --> 00:24:54,559
images

574
00:24:51,200 --> 00:24:58,159
that were or video that would um

575
00:24:54,559 --> 00:25:01,200
bias the way a politician's

576
00:24:58,159 --> 00:25:03,760
action or words are received by a wider

577
00:25:01,200 --> 00:25:03,760
audience

578
00:25:05,679 --> 00:25:08,720
so i've talked now about discrimination

579
00:25:07,919 --> 00:25:11,360
and about

580
00:25:08,720 --> 00:25:12,480
deep fakes a little bit in greater

581
00:25:11,360 --> 00:25:15,918
detail

582
00:25:12,480 --> 00:25:16,400
because discrimination by ai is really a

583
00:25:15,919 --> 00:25:18,799
topic

584
00:25:16,400 --> 00:25:19,919
that is that has seen a lot of attention

585
00:25:18,799 --> 00:25:23,120
by policy makers

586
00:25:19,919 --> 00:25:24,559
and researchers and because

587
00:25:23,120 --> 00:25:26,959
deep fake's becoming more and more

588
00:25:24,559 --> 00:25:29,279
prevalent but of course

589
00:25:26,960 --> 00:25:31,520
there are many other ways in which ai

590
00:25:29,279 --> 00:25:35,520
can be problematic for us

591
00:25:31,520 --> 00:25:38,720
and i just want to list a couple of ways

592
00:25:35,520 --> 00:25:42,240
and i want to embed that in a

593
00:25:38,720 --> 00:25:46,080
by by adding a couple of words to

594
00:25:42,240 --> 00:25:49,520
the question of do we need

595
00:25:46,080 --> 00:25:50,799
new human rights do we need digital

596
00:25:49,520 --> 00:25:52,960
human rights

597
00:25:50,799 --> 00:25:54,799
possibly in order to deal with these

598
00:25:52,960 --> 00:25:58,240
problems and i said there is an ongoing

599
00:25:54,799 --> 00:26:02,639
debate and it is far from being settled

600
00:25:58,240 --> 00:26:05,520
but at least speaking for our

601
00:26:02,640 --> 00:26:07,840
group at amnesty i think it is safe to

602
00:26:05,520 --> 00:26:11,840
say that there is a tendency forming

603
00:26:07,840 --> 00:26:14,399
to say that no in fact we do not need

604
00:26:11,840 --> 00:26:17,039
new human rights in order to cover all

605
00:26:14,400 --> 00:26:20,240
these problems that i've talked about

606
00:26:17,039 --> 00:26:22,240
but the ones that we already have

607
00:26:20,240 --> 00:26:23,840
just need to be applied in the

608
00:26:22,240 --> 00:26:26,080
appropriate manner but of course this

609
00:26:23,840 --> 00:26:29,360
discussion is far from being over

610
00:26:26,080 --> 00:26:31,360
and just to sort the

611
00:26:29,360 --> 00:26:34,080
the cases the examples i've talked about

612
00:26:31,360 --> 00:26:35,678
so far and to give a little bit of a

613
00:26:34,080 --> 00:26:37,600
taste for what other problems

614
00:26:35,679 --> 00:26:39,360
are being out there and how they relate

615
00:26:37,600 --> 00:26:42,240
to human rights

616
00:26:39,360 --> 00:26:43,840
if we look at the human rights as

617
00:26:42,240 --> 00:26:47,360
defined by the universal

618
00:26:43,840 --> 00:26:49,840
declaration of human rights we can go

619
00:26:47,360 --> 00:26:50,719
through a couple and of course i'm aware

620
00:26:49,840 --> 00:26:53,520
that the

621
00:26:50,720 --> 00:26:55,919
universal declaration is not legally

622
00:26:53,520 --> 00:26:59,039
binding as it isn't

623
00:26:55,919 --> 00:26:59,520
a contract of international law but of

624
00:26:59,039 --> 00:27:03,520
course

625
00:26:59,520 --> 00:27:06,240
most if not all rights have been

626
00:27:03,520 --> 00:27:07,918
implemented into legally binding very

627
00:27:06,240 --> 00:27:11,600
much legally binding national

628
00:27:07,919 --> 00:27:14,000
law and so for example the

629
00:27:11,600 --> 00:27:16,000
case about robert williams that i've

630
00:27:14,000 --> 00:27:19,120
mentioned a couple of minutes ago

631
00:27:16,000 --> 00:27:20,679
would fall into the domain of article 2

632
00:27:19,120 --> 00:27:23,678
which is the right to

633
00:27:20,679 --> 00:27:23,679
non-discrimination

634
00:27:23,760 --> 00:27:29,120
another another field about which

635
00:27:27,279 --> 00:27:30,640
we could do an entire presentation is

636
00:27:29,120 --> 00:27:32,639
predictive policing

637
00:27:30,640 --> 00:27:34,399
which falls in the domain of this

638
00:27:32,640 --> 00:27:36,880
article too

639
00:27:34,399 --> 00:27:37,439
and of course there is article 3 the

640
00:27:36,880 --> 00:27:40,399
right

641
00:27:37,440 --> 00:27:41,039
the right to life and liberty and here

642
00:27:40,399 --> 00:27:42,959
of course

643
00:27:41,039 --> 00:27:45,600
we have to mention autonomous weapon

644
00:27:42,960 --> 00:27:48,720
systems which is basically

645
00:27:45,600 --> 00:27:52,158
killer robots that have been

646
00:27:48,720 --> 00:27:55,600
deployed with some kind of um

647
00:27:52,159 --> 00:27:57,600
for example facial recognition ai or

648
00:27:55,600 --> 00:27:59,360
an ai that allows you to make the

649
00:27:57,600 --> 00:28:02,879
decision

650
00:27:59,360 --> 00:28:05,199
of whether to go forth with a

651
00:28:02,880 --> 00:28:07,440
lethal strike without a human in the

652
00:28:05,200 --> 00:28:07,440
loop

653
00:28:07,600 --> 00:28:12,240
then of course there is article 12 the

654
00:28:10,480 --> 00:28:14,960
right to privacy

655
00:28:12,240 --> 00:28:16,000
and i've talked in great detail about

656
00:28:14,960 --> 00:28:19,039
facial recognition by

657
00:28:16,000 --> 00:28:20,559
now but of course here we could also

658
00:28:19,039 --> 00:28:23,760
talk about this

659
00:28:20,559 --> 00:28:26,639
system of data surveillance that the big

660
00:28:23,760 --> 00:28:30,000
it companies are basically

661
00:28:26,640 --> 00:28:32,399
putting us all into um

662
00:28:30,000 --> 00:28:33,200
article 20 the freedom of assembly could

663
00:28:32,399 --> 00:28:36,399
be

664
00:28:33,200 --> 00:28:39,120
endangered for example by

665
00:28:36,399 --> 00:28:40,559
facial recognition ai because some

666
00:28:39,120 --> 00:28:43,678
people might

667
00:28:40,559 --> 00:28:44,080
choose not to go to a demonstration if

668
00:28:43,679 --> 00:28:48,080
they

669
00:28:44,080 --> 00:28:51,199
are afraid that the police might

670
00:28:48,080 --> 00:28:54,399
identify them individually and

671
00:28:51,200 --> 00:28:55,039
that this is far from a dystopian in a

672
00:28:54,399 --> 00:28:58,080
future

673
00:28:55,039 --> 00:29:00,320
lying problem we have seen

674
00:28:58,080 --> 00:29:02,399
at the protests in hong kong over the

675
00:29:00,320 --> 00:29:05,200
last years

676
00:29:02,399 --> 00:29:05,678
of course article 18 freedom of thought

677
00:29:05,200 --> 00:29:08,799
could be

678
00:29:05,679 --> 00:29:11,520
endangered by for example the

679
00:29:08,799 --> 00:29:13,440
problem of deep fakes poisoning our

680
00:29:11,520 --> 00:29:16,720
democratic

681
00:29:13,440 --> 00:29:19,840
discussion and

682
00:29:16,720 --> 00:29:21,279
even all the way down to people being

683
00:29:19,840 --> 00:29:24,320
discriminated based

684
00:29:21,279 --> 00:29:26,159
on protected attributes we have seen for

685
00:29:24,320 --> 00:29:28,639
example

686
00:29:26,159 --> 00:29:30,559
gender and race but of course there are

687
00:29:28,640 --> 00:29:34,080
many other that could be

688
00:29:30,559 --> 00:29:37,840
in question here so

689
00:29:34,080 --> 00:29:41,520
um this this just to like i said

690
00:29:37,840 --> 00:29:42,158
give you a glimpse of how far-reaching

691
00:29:41,520 --> 00:29:45,440
this is

692
00:29:42,159 --> 00:29:46,320
but the title is called dystopia or

693
00:29:45,440 --> 00:29:48,640
shining future

694
00:29:46,320 --> 00:29:50,240
so i also want to talk a little bit

695
00:29:48,640 --> 00:29:53,679
about

696
00:29:50,240 --> 00:29:56,720
how ai can be used as a force

697
00:29:53,679 --> 00:29:59,840
for good

698
00:29:56,720 --> 00:30:02,399
and um

699
00:29:59,840 --> 00:30:03,840
there is good reason to to be hopeful

700
00:30:02,399 --> 00:30:08,320
and to believe that ai

701
00:30:03,840 --> 00:30:11,840
can be helpful as well so for example a

702
00:30:08,320 --> 00:30:13,200
ai image recognition algorithms have

703
00:30:11,840 --> 00:30:16,879
been used to

704
00:30:13,200 --> 00:30:20,159
document human rights violations in

705
00:30:16,880 --> 00:30:24,399
yemen in syria and amnesty international

706
00:30:20,159 --> 00:30:27,520
for example has used it to document

707
00:30:24,399 --> 00:30:30,799
human rights violations in

708
00:30:27,520 --> 00:30:32,080
darfur which is a western region of

709
00:30:30,799 --> 00:30:36,080
sudan

710
00:30:32,080 --> 00:30:39,120
and what was happening there the the

711
00:30:36,080 --> 00:30:43,120
region of darfur wants

712
00:30:39,120 --> 00:30:45,039
more participation in the national uh

713
00:30:43,120 --> 00:30:46,399
in the national political affairs of the

714
00:30:45,039 --> 00:30:49,760
state

715
00:30:46,399 --> 00:30:50,320
and so the the conflict escalated and

716
00:30:49,760 --> 00:30:53,679
the

717
00:30:50,320 --> 00:30:56,960
government was fighting against rebels

718
00:30:53,679 --> 00:30:57,840
and amnesty is accusing the national

719
00:30:56,960 --> 00:31:00,880
government

720
00:30:57,840 --> 00:31:03,840
to use chemical weapons

721
00:31:00,880 --> 00:31:05,120
against the population and now in order

722
00:31:03,840 --> 00:31:08,240
to gather evidence

723
00:31:05,120 --> 00:31:11,678
of these crimes what amnesty did was

724
00:31:08,240 --> 00:31:14,960
looking at satellite images on

725
00:31:11,679 --> 00:31:18,799
before and after such a chemical attack

726
00:31:14,960 --> 00:31:22,960
because these chemical attacks would

727
00:31:18,799 --> 00:31:26,240
expel the population of certain villages

728
00:31:22,960 --> 00:31:28,559
and of course what

729
00:31:26,240 --> 00:31:30,399
we could have done is using drawing on a

730
00:31:28,559 --> 00:31:31,120
large amount of volunteers who would

731
00:31:30,399 --> 00:31:34,000
then

732
00:31:31,120 --> 00:31:36,719
classify these images by hand but of

733
00:31:34,000 --> 00:31:39,919
course it is much more efficient

734
00:31:36,720 --> 00:31:44,000
and faster and impactful to use ai

735
00:31:39,919 --> 00:31:45,279
in this context and in a very similar

736
00:31:44,000 --> 00:31:48,320
vein

737
00:31:45,279 --> 00:31:50,159
amnesty is running the toxic twitter

738
00:31:48,320 --> 00:31:53,039
project

739
00:31:50,159 --> 00:31:54,080
so that is now switching subjects we're

740
00:31:53,039 --> 00:31:56,799
now

741
00:31:54,080 --> 00:31:57,760
talking no longer about human rights

742
00:31:56,799 --> 00:32:00,799
violations

743
00:31:57,760 --> 00:32:05,519
in in countries but about the problem

744
00:32:00,799 --> 00:32:05,519
of violent sexualized

745
00:32:05,840 --> 00:32:14,559
hate speech against women on twitter

746
00:32:10,640 --> 00:32:18,399
and what amnesty is doing here is again

747
00:32:14,559 --> 00:32:20,399
trying to document this problem and to

748
00:32:18,399 --> 00:32:21,678
build up pressure and force twitter to

749
00:32:20,399 --> 00:32:25,439
take action

750
00:32:21,679 --> 00:32:28,720
such that everyone feels

751
00:32:25,440 --> 00:32:33,440
safe and secure in this social space

752
00:32:28,720 --> 00:32:37,279
that twitter is that twitter is nowadays

753
00:32:33,440 --> 00:32:40,960
and again what we're doing is

754
00:32:37,279 --> 00:32:44,000
now we use text nlp

755
00:32:40,960 --> 00:32:45,600
text analyzing algorithms that help us

756
00:32:44,000 --> 00:32:48,960
classify

757
00:32:45,600 --> 00:32:51,199
millions of tweets into

758
00:32:48,960 --> 00:32:52,240
dangerous hate speech or into

759
00:32:51,200 --> 00:32:56,640
appropriate

760
00:32:52,240 --> 00:33:02,240
content and um

761
00:32:56,640 --> 00:33:05,519
for example doxing is a large problem

762
00:33:02,240 --> 00:33:07,519
that is for example the the act of

763
00:33:05,519 --> 00:33:10,080
publishing private information about

764
00:33:07,519 --> 00:33:14,320
someone online such that then

765
00:33:10,080 --> 00:33:16,799
others can uh go and be uh

766
00:33:14,320 --> 00:33:18,399
go and use that information to make

767
00:33:16,799 --> 00:33:22,559
death threats in real life

768
00:33:18,399 --> 00:33:25,678
or so on um

769
00:33:22,559 --> 00:33:29,279
these are two very precise examples

770
00:33:25,679 --> 00:33:29,279
of what we did

771
00:33:29,919 --> 00:33:33,200
but of course amnesty is not the only

772
00:33:31,840 --> 00:33:36,639
one there is

773
00:33:33,200 --> 00:33:37,440
has great work has been done to use ai

774
00:33:36,640 --> 00:33:40,720
to recognize

775
00:33:37,440 --> 00:33:45,039
displaced people or to use ai to

776
00:33:40,720 --> 00:33:50,000
analyze the background of

777
00:33:45,039 --> 00:33:53,279
child pornography videos

778
00:33:50,000 --> 00:33:54,159
such that then similar backgrounds could

779
00:33:53,279 --> 00:33:56,720
be

780
00:33:54,159 --> 00:33:58,799
an indication that it was filmed by the

781
00:33:56,720 --> 00:33:59,760
same group or individual which would be

782
00:33:58,799 --> 00:34:02,879
a hint that

783
00:33:59,760 --> 00:34:02,879
helps the police to

784
00:34:02,960 --> 00:34:09,839
find the criminals who made these

785
00:34:06,720 --> 00:34:12,800
this video and therefore breaking

786
00:34:09,839 --> 00:34:14,719
um child pornography and sex trafficking

787
00:34:12,800 --> 00:34:18,320
rings

788
00:34:14,719 --> 00:34:21,759
but then there is also

789
00:34:18,320 --> 00:34:26,159
this very fundamental hope that

790
00:34:21,760 --> 00:34:29,679
ai as a general purpose technology

791
00:34:26,159 --> 00:34:34,000
can have tremendous positive

792
00:34:29,679 --> 00:34:37,200
effects on humanity on a global

793
00:34:34,000 --> 00:34:39,679
scale even so what i mean by saying

794
00:34:37,199 --> 00:34:43,839
general purpose technology

795
00:34:39,679 --> 00:34:47,679
is that ai is really considered to be

796
00:34:43,839 --> 00:34:50,159
not just any other new innovation

797
00:34:47,679 --> 00:34:51,918
but it is considered to be an innovation

798
00:34:50,159 --> 00:34:54,960
that

799
00:34:51,918 --> 00:34:57,598
is impactful for

800
00:34:54,960 --> 00:34:59,119
in basically all domains of human lives

801
00:34:57,599 --> 00:35:02,240
that

802
00:34:59,119 --> 00:35:04,720
like in a domino effect causes

803
00:35:02,240 --> 00:35:07,439
new innovations and discoveries that

804
00:35:04,720 --> 00:35:10,959
improve the living conditions

805
00:35:07,440 --> 00:35:12,599
just like the uh just like electricity

806
00:35:10,960 --> 00:35:16,560
did

807
00:35:12,599 --> 00:35:19,760
140 years ago

808
00:35:16,560 --> 00:35:20,560
ai for example could be used in the

809
00:35:19,760 --> 00:35:23,760
context

810
00:35:20,560 --> 00:35:27,200
of fighting climate change we could

811
00:35:23,760 --> 00:35:30,560
for example use it to monitor

812
00:35:27,200 --> 00:35:34,160
the biodiversity and

813
00:35:30,560 --> 00:35:36,560
climate conditions heat in remote areas

814
00:35:34,160 --> 00:35:37,200
in the world it could be used to improve

815
00:35:36,560 --> 00:35:39,119
the

816
00:35:37,200 --> 00:35:40,319
climate the predictive power of the

817
00:35:39,119 --> 00:35:43,359
climate models

818
00:35:40,320 --> 00:35:45,839
such that we can adjust our behavior

819
00:35:43,359 --> 00:35:45,839
accordingly

820
00:35:46,400 --> 00:35:50,240
then not only in the fight against

821
00:35:48,160 --> 00:35:54,000
climate change it could also be used

822
00:35:50,240 --> 00:35:57,118
in the domain of health

823
00:35:54,000 --> 00:36:00,320
and one noteworthy example here is

824
00:35:57,119 --> 00:36:03,359
google's alpha fold which

825
00:36:00,320 --> 00:36:05,760
is a discovery that or achievement

826
00:36:03,359 --> 00:36:07,598
that some of you might have heard a very

827
00:36:05,760 --> 00:36:10,640
recent one just last month

828
00:36:07,599 --> 00:36:12,480
and one that i think did not actually

829
00:36:10,640 --> 00:36:13,920
receive the media attention that it

830
00:36:12,480 --> 00:36:16,800
deserves because

831
00:36:13,920 --> 00:36:17,599
what this group around this ai achieved

832
00:36:16,800 --> 00:36:20,560
is to

833
00:36:17,599 --> 00:36:23,520
they solve the protein folding problem

834
00:36:20,560 --> 00:36:27,040
which was one of the fundamental

835
00:36:23,520 --> 00:36:30,960
problems of the last 50 years in

836
00:36:27,040 --> 00:36:32,320
molecular biology meaning that the ai

837
00:36:30,960 --> 00:36:35,359
can now predict

838
00:36:32,320 --> 00:36:38,480
the way in which protein

839
00:36:35,359 --> 00:36:40,000
folds up and that allows us to much

840
00:36:38,480 --> 00:36:42,400
faster and much cheaper

841
00:36:40,000 --> 00:36:44,160
devise new materials materials which

842
00:36:42,400 --> 00:36:45,599
then again could be used in the fight

843
00:36:44,160 --> 00:36:47,839
against climate change

844
00:36:45,599 --> 00:36:48,640
because they are more energy efficient

845
00:36:47,839 --> 00:36:52,160
or new

846
00:36:48,640 --> 00:36:54,560
or new proteins that could allow for

847
00:36:52,160 --> 00:36:56,720
better and more efficient medication and

848
00:36:54,560 --> 00:37:00,799
then of course

849
00:36:56,720 --> 00:37:00,799
in an economic sense ai could be

850
00:37:00,880 --> 00:37:05,760
hopefully used to improve the

851
00:37:03,760 --> 00:37:09,760
productivity

852
00:37:05,760 --> 00:37:11,839
and to boost global living standards

853
00:37:09,760 --> 00:37:13,280
and that is important of course because

854
00:37:11,839 --> 00:37:16,960
human rights are now

855
00:37:13,280 --> 00:37:17,280
not limited uh to these political rights

856
00:37:16,960 --> 00:37:19,839
that

857
00:37:17,280 --> 00:37:20,560
you might be typically thinking of as

858
00:37:19,839 --> 00:37:22,320
we've seen

859
00:37:20,560 --> 00:37:24,799
freedom of assembly freedom of speech

860
00:37:22,320 --> 00:37:27,920
and so on but of course human rights

861
00:37:24,800 --> 00:37:29,119
and compass nowadays also socioeconomic

862
00:37:27,920 --> 00:37:33,119
rights

863
00:37:29,119 --> 00:37:36,000
and if

864
00:37:33,119 --> 00:37:37,920
we make the best out of that technology

865
00:37:36,000 --> 00:37:39,599
we can be hopeful that

866
00:37:37,920 --> 00:37:42,720
all these achievements come into

867
00:37:39,599 --> 00:37:42,720
fruition in the future

868
00:37:45,280 --> 00:37:51,119
but in order to achieve that we have to

869
00:37:47,280 --> 00:37:54,880
make sure that artificial intelligence

870
00:37:51,119 --> 00:37:58,160
does actually behave in a safe manner

871
00:37:54,880 --> 00:38:02,079
and so how would we go about

872
00:37:58,160 --> 00:38:05,279
to do that policy makers

873
00:38:02,079 --> 00:38:08,560
and researchers have really started to

874
00:38:05,280 --> 00:38:11,839
think in detail about this problem so

875
00:38:08,560 --> 00:38:14,000
um you see uh the

876
00:38:11,839 --> 00:38:15,759
expert boards popping up in the last

877
00:38:14,000 --> 00:38:16,400
couple of years that are dealing with

878
00:38:15,760 --> 00:38:19,280
this

879
00:38:16,400 --> 00:38:20,079
problem of safe ai all over the place

880
00:38:19,280 --> 00:38:22,240
there is the

881
00:38:20,079 --> 00:38:24,720
ai high level expert group of the

882
00:38:22,240 --> 00:38:28,160
european commission there is the german

883
00:38:24,720 --> 00:38:30,799
data ethics commission and basically

884
00:38:28,160 --> 00:38:31,759
any kind of company that thinks of

885
00:38:30,800 --> 00:38:35,280
itself as i.t

886
00:38:31,760 --> 00:38:37,200
company has set up an ai ethics

887
00:38:35,280 --> 00:38:40,240
or has at least published an ethics

888
00:38:37,200 --> 00:38:40,240
paper about ai

889
00:38:40,720 --> 00:38:46,000
for example to the left you see a graph

890
00:38:44,400 --> 00:38:48,400
from the report of the german data

891
00:38:46,000 --> 00:38:51,680
ethics commission and what they say

892
00:38:48,400 --> 00:38:54,640
is well we can divide

893
00:38:51,680 --> 00:38:55,359
ai according to their potential harm

894
00:38:54,640 --> 00:38:57,759
they call it

895
00:38:55,359 --> 00:38:59,520
according to their potential criticality

896
00:38:57,760 --> 00:39:03,200
and the base of this

897
00:38:59,520 --> 00:39:04,160
triangle in green is the vast amount of

898
00:39:03,200 --> 00:39:07,839
ai

899
00:39:04,160 --> 00:39:12,000
algorithms that is unproblematic

900
00:39:07,839 --> 00:39:15,680
and they say these algorithms would

901
00:39:12,000 --> 00:39:18,400
not cross the threshold in order to

902
00:39:15,680 --> 00:39:19,520
for the to have the knee to be regulated

903
00:39:18,400 --> 00:39:20,320
and then on the other end of the

904
00:39:19,520 --> 00:39:23,759
triangle

905
00:39:20,320 --> 00:39:26,800
you have this red tip which would be

906
00:39:23,760 --> 00:39:29,599
very few ai's but these really should

907
00:39:26,800 --> 00:39:32,400
not be allowed to be used at all

908
00:39:29,599 --> 00:39:33,040
so for example um in the in the green

909
00:39:32,400 --> 00:39:35,920
field

910
00:39:33,040 --> 00:39:37,759
you could think of an algorithm that

911
00:39:35,920 --> 00:39:39,680
identifies whether the coin that is

912
00:39:37,760 --> 00:39:42,000
thrown into a vending machine

913
00:39:39,680 --> 00:39:43,200
is actually the appropriate amount of

914
00:39:42,000 --> 00:39:46,800
money

915
00:39:43,200 --> 00:39:50,319
and the an example for an ai that

916
00:39:46,800 --> 00:39:52,720
should be forbidden entirely could be

917
00:39:50,320 --> 00:39:54,320
one out of the field of autonomous

918
00:39:52,720 --> 00:39:57,359
weapon systems

919
00:39:54,320 --> 00:39:58,320
but of course the interesting the

920
00:39:57,359 --> 00:40:01,359
interesting debate

921
00:39:58,320 --> 00:40:04,400
is going on in this yellow to orange

922
00:40:01,359 --> 00:40:04,400
field in the middle

923
00:40:06,800 --> 00:40:13,200
then there is also a report that amnesty

924
00:40:10,560 --> 00:40:16,400
has published with access now called the

925
00:40:13,200 --> 00:40:20,319
toronto declaration and in it

926
00:40:16,400 --> 00:40:24,079
amnesty is demanding that

927
00:40:20,319 --> 00:40:27,119
public and private actors who employ ai

928
00:40:24,079 --> 00:40:30,480
systems are being held accountable

929
00:40:27,119 --> 00:40:34,319
that they ensure a safe development

930
00:40:30,480 --> 00:40:36,319
of ai and a couple of concrete

931
00:40:34,319 --> 00:40:36,960
suggestions for example to make sure

932
00:40:36,319 --> 00:40:41,759
that the

933
00:40:36,960 --> 00:40:45,280
developer team of an ai is

934
00:40:41,760 --> 00:40:48,240
diverse in many senses so thinking back

935
00:40:45,280 --> 00:40:50,400
to the example of the story of inyoluva

936
00:40:48,240 --> 00:40:54,000
raji

937
00:40:50,400 --> 00:40:56,880
you remember that her managers

938
00:40:54,000 --> 00:40:58,960
did not actually care even after she

939
00:40:56,880 --> 00:41:01,680
brought the problem to their attention

940
00:40:58,960 --> 00:41:03,119
and having a diverse team that is

941
00:41:01,680 --> 00:41:05,200
possibly even affected by the

942
00:41:03,119 --> 00:41:08,319
detrimental effects of ai

943
00:41:05,200 --> 00:41:10,879
could help out here

944
00:41:08,319 --> 00:41:12,400
what all of these suggestions to ensure

945
00:41:10,880 --> 00:41:15,839
safe ai have in common

946
00:41:12,400 --> 00:41:18,880
is that they're calling for a element of

947
00:41:15,839 --> 00:41:22,480
human oversight and

948
00:41:18,880 --> 00:41:24,960
for a way in which we can make sure that

949
00:41:22,480 --> 00:41:27,760
humans can understand how the ai is

950
00:41:24,960 --> 00:41:31,920
coming to its decisions

951
00:41:27,760 --> 00:41:36,640
and while that is desirable it is also

952
00:41:31,920 --> 00:41:36,640
extremely difficult for two reasons so

953
00:41:37,040 --> 00:41:41,200
in contrast to traditional code you

954
00:41:39,119 --> 00:41:41,760
can't just look at the source code and

955
00:41:41,200 --> 00:41:43,919
do a

956
00:41:41,760 --> 00:41:45,680
code audit in order to find out the

957
00:41:43,920 --> 00:41:48,560
flaws in the program

958
00:41:45,680 --> 00:41:50,000
ai's are so called black boxes you see

959
00:41:48,560 --> 00:41:53,839
the input that goes in

960
00:41:50,000 --> 00:41:56,400
and you observe the output but in these

961
00:41:53,839 --> 00:41:58,480
billions of parameter large neural

962
00:41:56,400 --> 00:42:00,880
networks it is impossible

963
00:41:58,480 --> 00:42:02,000
even for the developers to determine how

964
00:42:00,880 --> 00:42:04,640
the ai arrives

965
00:42:02,000 --> 00:42:05,359
at a certain result and the second

966
00:42:04,640 --> 00:42:09,520
problem is

967
00:42:05,359 --> 00:42:13,359
that unlike for example um

968
00:42:09,520 --> 00:42:15,359
auditing to make sure that a car runs

969
00:42:13,359 --> 00:42:18,560
safely

970
00:42:15,359 --> 00:42:20,880
ai's are changing in such a frequent or

971
00:42:18,560 --> 00:42:24,799
possibly even continuous manner

972
00:42:20,880 --> 00:42:30,000
that the auditing process should also be

973
00:42:24,800 --> 00:42:31,920
somewhat made continuously now

974
00:42:30,000 --> 00:42:33,440
like i said there is a lot of research

975
00:42:31,920 --> 00:42:36,880
going on about this

976
00:42:33,440 --> 00:42:40,079
and there are um and ideas exist

977
00:42:36,880 --> 00:42:41,920
about how to tackle these problems so

978
00:42:40,079 --> 00:42:44,480
what all of these possible solutions

979
00:42:41,920 --> 00:42:47,280
have in common is a

980
00:42:44,480 --> 00:42:49,040
kind of what like what i would call a

981
00:42:47,280 --> 00:42:52,240
crowd or expert based

982
00:42:49,040 --> 00:42:55,520
ai challenging system so what that means

983
00:42:52,240 --> 00:42:56,479
is you circumvent this black black box

984
00:42:55,520 --> 00:43:00,480
problem

985
00:42:56,480 --> 00:43:03,680
by um feeding the ai

986
00:43:00,480 --> 00:43:07,119
with input and trying

987
00:43:03,680 --> 00:43:10,879
to feed it with input that brings the ai

988
00:43:07,119 --> 00:43:14,000
to to make to commit a mistake and then

989
00:43:10,880 --> 00:43:17,200
you can infer so to say

990
00:43:14,000 --> 00:43:18,560
where the problem area of an ai really

991
00:43:17,200 --> 00:43:20,799
lies

992
00:43:18,560 --> 00:43:20,799
and

993
00:43:22,880 --> 00:43:26,560
it is also of course important to ensure

994
00:43:25,359 --> 00:43:30,720
that the

995
00:43:26,560 --> 00:43:33,920
consideration for safe ai is

996
00:43:30,720 --> 00:43:36,959
in the mind of the developers from

997
00:43:33,920 --> 00:43:39,200
point one of the development so that we

998
00:43:36,960 --> 00:43:42,640
can do these challenging processes

999
00:43:39,200 --> 00:43:44,160
not just after the ai has been deployed

1000
00:43:42,640 --> 00:43:46,480
and affected possibly

1001
00:43:44,160 --> 00:43:47,598
millions or billions of people but

1002
00:43:46,480 --> 00:43:49,280
already

1003
00:43:47,599 --> 00:43:53,040
that there is an internal auditing

1004
00:43:49,280 --> 00:43:56,560
process that defines clearly

1005
00:43:53,040 --> 00:43:59,520
steps and document documents these steps

1006
00:43:56,560 --> 00:44:00,799
of what decision is made in order to

1007
00:43:59,520 --> 00:44:03,680
develop this ai

1008
00:44:00,800 --> 00:44:05,599
and how such that in the end there is a

1009
00:44:03,680 --> 00:44:08,720
accountability report

1010
00:44:05,599 --> 00:44:10,480
that can possibly

1011
00:44:08,720 --> 00:44:12,879
already take the biggest kinds of

1012
00:44:10,480 --> 00:44:16,000
problems out of the ai before it

1013
00:44:12,880 --> 00:44:18,480
is even reaching a larger audience

1014
00:44:16,000 --> 00:44:20,160
so it's with the examples i gave you

1015
00:44:18,480 --> 00:44:22,640
from earlier it is easy to

1016
00:44:20,160 --> 00:44:23,359
see how it would have been possible to

1017
00:44:22,640 --> 00:44:26,480
spot

1018
00:44:23,359 --> 00:44:28,640
a problem in the in the facial

1019
00:44:26,480 --> 00:44:30,800
recognition algorithms for example

1020
00:44:28,640 --> 00:44:32,640
by just making sure that the training

1021
00:44:30,800 --> 00:44:36,960
data is actually representative

1022
00:44:32,640 --> 00:44:40,240
of the general u.s population

1023
00:44:36,960 --> 00:44:43,440
that brings me to my conclusion so

1024
00:44:40,240 --> 00:44:45,680
are we headed for a dystopia or are we

1025
00:44:43,440 --> 00:44:49,119
headed for a shining future now i

1026
00:44:45,680 --> 00:44:52,799
could make my life easy and say

1027
00:44:49,119 --> 00:44:55,040
we're going for middle ground but

1028
00:44:52,800 --> 00:44:58,400
i want to be there here and i think

1029
00:44:55,040 --> 00:45:00,640
there is good reason to be optimistic

1030
00:44:58,400 --> 00:45:02,480
of course i've talked a lot about

1031
00:45:00,640 --> 00:45:05,118
problems that we already have today and

1032
00:45:02,480 --> 00:45:07,200
about potential problems in the future

1033
00:45:05,119 --> 00:45:08,880
and of course with every type of new

1034
00:45:07,200 --> 00:45:10,960
technology

1035
00:45:08,880 --> 00:45:12,800
regulation and supervision is always

1036
00:45:10,960 --> 00:45:16,400
trailing a little bit behind

1037
00:45:12,800 --> 00:45:20,240
but as you have also seen researchers

1038
00:45:16,400 --> 00:45:21,839
and makers have been become aware of the

1039
00:45:20,240 --> 00:45:25,759
potential problems

1040
00:45:21,839 --> 00:45:26,480
and with the potential of this

1041
00:45:25,760 --> 00:45:28,880
technology

1042
00:45:26,480 --> 00:45:31,280
if we make sure that we continue on a

1043
00:45:28,880 --> 00:45:34,079
good trajectory into the future

1044
00:45:31,280 --> 00:45:35,440
i think we are actually headed more for

1045
00:45:34,079 --> 00:45:38,640
the shining future

1046
00:45:35,440 --> 00:45:38,640
than for the dystopia

1047
00:45:38,960 --> 00:45:42,640
let me end the talk by just pointing out

1048
00:45:41,760 --> 00:45:45,280
a few

1049
00:45:42,640 --> 00:45:46,000
things about the literature so these are

1050
00:45:45,280 --> 00:45:48,720
my sources

1051
00:45:46,000 --> 00:45:49,359
and all of these except for the last one

1052
00:45:48,720 --> 00:45:53,040
here

1053
00:45:49,359 --> 00:45:56,480
should be accessible for free

1054
00:45:53,040 --> 00:46:00,000
um also

1055
00:45:56,480 --> 00:46:00,720
um timnit uh gabriel who is the author

1056
00:46:00,000 --> 00:46:04,079
of the

1057
00:46:00,720 --> 00:46:07,040
third paper um

1058
00:46:04,079 --> 00:46:08,480
has some interesting developments going

1059
00:46:07,040 --> 00:46:10,000
on about her if you want to follow her

1060
00:46:08,480 --> 00:46:14,079
on twitter that is interesting

1061
00:46:10,000 --> 00:46:16,240
also the last bullet point here um

1062
00:46:14,079 --> 00:46:18,400
is a shooting star of the ethical ai

1063
00:46:16,240 --> 00:46:19,919
scene it's also worth following her on

1064
00:46:18,400 --> 00:46:23,119
twitter

1065
00:46:19,920 --> 00:46:26,160
and the rest of the sources

1066
00:46:23,119 --> 00:46:26,960
also except for the first one all

1067
00:46:26,160 --> 00:46:30,319
available

1068
00:46:26,960 --> 00:46:31,040
for free online i want to thank the

1069
00:46:30,319 --> 00:46:34,000
awesome and

1070
00:46:31,040 --> 00:46:35,759
tell and talented photographers who were

1071
00:46:34,000 --> 00:46:39,599
kind enough to allow me to

1072
00:46:35,760 --> 00:46:42,800
use their stock images for free

1073
00:46:39,599 --> 00:46:43,520
and i want to end by saying that if you

1074
00:46:42,800 --> 00:46:45,200
are interested

1075
00:46:43,520 --> 00:46:47,280
in any of the things that i have

1076
00:46:45,200 --> 00:46:49,040
mentioned today

1077
00:46:47,280 --> 00:46:51,119
especially if you're interested in some

1078
00:46:49,040 --> 00:46:51,759
of the topics i have merely touched upon

1079
00:46:51,119 --> 00:46:54,240
like

1080
00:46:51,760 --> 00:46:56,079
predictive policing or data surveillance

1081
00:46:54,240 --> 00:46:58,879
then please don't hesitate

1082
00:46:56,079 --> 00:47:00,079
to get in touch with our expert group

1083
00:46:58,880 --> 00:47:02,000
visit our homepage

1084
00:47:00,079 --> 00:47:03,839
or if you have questions directly about

1085
00:47:02,000 --> 00:47:06,880
this talk then

1086
00:47:03,839 --> 00:47:08,640
get in touch with me directly but of

1087
00:47:06,880 --> 00:47:09,280
course i'm also looking forward to see

1088
00:47:08,640 --> 00:47:12,799
you now

1089
00:47:09,280 --> 00:47:14,880
in the q a session and

1090
00:47:12,800 --> 00:47:17,119
take your questions there thank you very

1091
00:47:14,880 --> 00:47:17,119
much

1092
00:47:18,240 --> 00:47:22,720
political decision makers on a broader

1093
00:47:21,839 --> 00:47:25,119
level

1094
00:47:22,720 --> 00:47:26,720
have an awareness about the problem or

1095
00:47:25,119 --> 00:47:29,040
do you think this is really

1096
00:47:26,720 --> 00:47:30,879
uh just tied to some experts for the

1097
00:47:29,040 --> 00:47:34,240
moment

1098
00:47:30,880 --> 00:47:36,720
i think we begin to see that

1099
00:47:34,240 --> 00:47:38,558
the awareness for the problem trickles

1100
00:47:36,720 --> 00:47:42,078
down to the

1101
00:47:38,559 --> 00:47:45,119
um for general uh political

1102
00:47:42,079 --> 00:47:46,960
into a general political sphere so i i

1103
00:47:45,119 --> 00:47:47,440
would imagine that during the next 10

1104
00:47:46,960 --> 00:47:50,480
years

1105
00:47:47,440 --> 00:47:52,400
we as a society in general will start

1106
00:47:50,480 --> 00:47:56,400
discussing this problem on a much wider

1107
00:47:52,400 --> 00:47:56,400
scale so i'm optimistic about that

1108
00:48:00,640 --> 00:48:08,319
okay and um so

1109
00:48:04,319 --> 00:48:13,520
going on the more positive side um

1110
00:48:08,319 --> 00:48:16,079
if there was to be a shining future

1111
00:48:13,520 --> 00:48:20,079
what possible obstacles are there to

1112
00:48:16,079 --> 00:48:22,960
overcome still

1113
00:48:20,079 --> 00:48:24,160
um i think uh one problem and there are

1114
00:48:22,960 --> 00:48:27,040
a lot of talks uh

1115
00:48:24,160 --> 00:48:27,839
during this rc3 that are concerned with

1116
00:48:27,040 --> 00:48:30,400
that

1117
00:48:27,839 --> 00:48:31,520
is getting the big it companies in check

1118
00:48:30,400 --> 00:48:35,359
we will

1119
00:48:31,520 --> 00:48:39,119
have to find uh one way or another to

1120
00:48:35,359 --> 00:48:42,319
find a way to deal with

1121
00:48:39,119 --> 00:48:43,599
the big tech monopolies

1122
00:48:42,319 --> 00:48:46,480
because they are the ones who are

1123
00:48:43,599 --> 00:48:50,319
employing the most cutting-edge

1124
00:48:46,480 --> 00:48:53,359
ai technology and if we succeed in that

1125
00:48:50,319 --> 00:48:57,119
um then i think we can be uh

1126
00:48:53,359 --> 00:48:59,680
also optimistic about um leveraging the

1127
00:48:57,119 --> 00:49:03,280
technology to the full potential and to

1128
00:48:59,680 --> 00:49:04,558
so that it actually does good

1129
00:49:03,280 --> 00:49:06,720
and i mean that has a lot to do with

1130
00:49:04,559 --> 00:49:09,440
your first question so i mean of course

1131
00:49:06,720 --> 00:49:11,279
none of this is um out of our control if

1132
00:49:09,440 --> 00:49:14,559
there is enough political will

1133
00:49:11,280 --> 00:49:14,559
then it's feasible

1134
00:49:15,040 --> 00:49:18,960
okay so what would your opinion be

1135
00:49:17,599 --> 00:49:21,920
actually on

1136
00:49:18,960 --> 00:49:22,559
seals um in german the term is guitars

1137
00:49:21,920 --> 00:49:26,319
eagle

1138
00:49:22,559 --> 00:49:29,040
uh on uh approval um

1139
00:49:26,319 --> 00:49:30,160
that and discussion at the moment for ai

1140
00:49:29,040 --> 00:49:32,000
to ensure

1141
00:49:30,160 --> 00:49:34,078
safe technology can you say anything

1142
00:49:32,000 --> 00:49:37,040
about seals

1143
00:49:34,079 --> 00:49:38,880
so like i try to to point out in my talk

1144
00:49:37,040 --> 00:49:42,000
there is a lot of talks going on

1145
00:49:38,880 --> 00:49:44,640
about the fact that we have to um audit

1146
00:49:42,000 --> 00:49:46,640
in one way or another ai

1147
00:49:44,640 --> 00:49:48,879
but nobody is really going into the

1148
00:49:46,640 --> 00:49:53,040
specifics of how to do that and

1149
00:49:48,880 --> 00:49:54,000
um attaching a seal onto an ai like the

1150
00:49:53,040 --> 00:49:56,720
way you would

1151
00:49:54,000 --> 00:49:58,720
attach a seal on a car when you send it

1152
00:49:56,720 --> 00:49:59,839
to the tuffy in germany after it got

1153
00:49:58,720 --> 00:50:03,598
checked

1154
00:49:59,839 --> 00:50:05,440
is probably a poor analogy um

1155
00:50:03,599 --> 00:50:07,200
like i said in the talk you have a

1156
00:50:05,440 --> 00:50:10,640
problem that ai's are

1157
00:50:07,200 --> 00:50:12,879
changing constantly and

1158
00:50:10,640 --> 00:50:15,040
you can't just like open the hood of the

1159
00:50:12,880 --> 00:50:17,359
car and look at the motor as they are

1160
00:50:15,040 --> 00:50:19,119
these black boxes so we will have to

1161
00:50:17,359 --> 00:50:22,319
find new ways to do

1162
00:50:19,119 --> 00:50:25,200
um to do these audits and i think a zeal

1163
00:50:22,319 --> 00:50:26,800
that only ever confirms at a certain

1164
00:50:25,200 --> 00:50:29,439
point in time that the ai

1165
00:50:26,800 --> 00:50:31,040
isn't misbehaving is a fundamentally

1166
00:50:29,440 --> 00:50:33,280
flawed concept

1167
00:50:31,040 --> 00:50:34,400
but there's a lot of research going on

1168
00:50:33,280 --> 00:50:36,720
in this field right now

1169
00:50:34,400 --> 00:50:39,520
and i'm i guess we will see new

1170
00:50:36,720 --> 00:50:43,118
approaches in the next years

1171
00:50:39,520 --> 00:50:46,880
we have to i mean there is no other way

1172
00:50:43,119 --> 00:50:47,440
yeah is there actually anything that we

1173
00:50:46,880 --> 00:50:50,880
can

1174
00:50:47,440 --> 00:50:54,720
do as individuals to take action as like

1175
00:50:50,880 --> 00:50:57,920
non-researchers and non-experts

1176
00:50:54,720 --> 00:50:59,520
um i think well that's a difficult

1177
00:50:57,920 --> 00:51:01,839
question i mean

1178
00:50:59,520 --> 00:51:01,839
um

1179
00:51:02,559 --> 00:51:08,880
probably on on a general uh note it is

1180
00:51:06,400 --> 00:51:09,680
important that the public is aware of

1181
00:51:08,880 --> 00:51:12,480
the problem

1182
00:51:09,680 --> 00:51:13,598
and that people are informed enough

1183
00:51:12,480 --> 00:51:16,480
about

1184
00:51:13,599 --> 00:51:18,559
the the details so that they can come to

1185
00:51:16,480 --> 00:51:21,280
a

1186
00:51:18,559 --> 00:51:24,160
useful judgment about their everyday

1187
00:51:21,280 --> 00:51:27,280
live use of technology that employs ai

1188
00:51:24,160 --> 00:51:31,359
so i mean for example

1189
00:51:27,280 --> 00:51:33,280
when we use youtube or other

1190
00:51:31,359 --> 00:51:35,598
social media that are using recommender

1191
00:51:33,280 --> 00:51:39,040
systems and we grow aware that

1192
00:51:35,599 --> 00:51:42,319
um that there are problems like

1193
00:51:39,040 --> 00:51:43,200
echo chambers that are arising then we

1194
00:51:42,319 --> 00:51:45,200
need to channel

1195
00:51:43,200 --> 00:51:48,000
our frustration with that into a

1196
00:51:45,200 --> 00:51:48,000
constructive

1197
00:51:48,160 --> 00:51:52,078
form for example like talk to your

1198
00:51:51,119 --> 00:51:55,520
representative

1199
00:51:52,079 --> 00:51:58,240
in the uh in your national parliament um

1200
00:51:55,520 --> 00:51:58,880
and or call them write them about this

1201
00:51:58,240 --> 00:52:01,279
problem

1202
00:51:58,880 --> 00:52:02,000
so that we can then use the political

1203
00:52:01,280 --> 00:52:05,520
power to

1204
00:52:02,000 --> 00:52:06,880
uh ensure a safe regulation otherwise i

1205
00:52:05,520 --> 00:52:09,680
don't know it's difficult to

1206
00:52:06,880 --> 00:52:11,520
um on a individual level of course but

1207
00:52:09,680 --> 00:52:14,399
yeah together

1208
00:52:11,520 --> 00:52:14,800
leveraging that force could do something

1209
00:52:14,400 --> 00:52:17,599
but

1210
00:52:14,800 --> 00:52:18,640
raising awareness is uh always a very

1211
00:52:17,599 --> 00:52:20,720
good first step

1212
00:52:18,640 --> 00:52:22,558
can i actually ask how did you

1213
00:52:20,720 --> 00:52:23,040
personally get interested in this topic

1214
00:52:22,559 --> 00:52:27,119
or how

1215
00:52:23,040 --> 00:52:27,119
did you first become aware of it

1216
00:52:27,200 --> 00:52:34,960
um so i

1217
00:52:30,319 --> 00:52:37,520
have been working on digital um

1218
00:52:34,960 --> 00:52:38,400
on the problem of how algorithms affect

1219
00:52:37,520 --> 00:52:41,599
the society

1220
00:52:38,400 --> 00:52:43,040
for one and a half years now in my job

1221
00:52:41,599 --> 00:52:46,640
and i've been

1222
00:52:43,040 --> 00:52:48,240
a member of the amnesty expert group on

1223
00:52:46,640 --> 00:52:50,480
human rights in the digital age for

1224
00:52:48,240 --> 00:52:53,759
about uh two years now

1225
00:52:50,480 --> 00:52:56,079
and in fact i mean that's

1226
00:52:53,760 --> 00:52:57,040
it's also a new field for us at msd so

1227
00:52:56,079 --> 00:53:00,400
we are

1228
00:52:57,040 --> 00:53:02,079
um basically this presentation is also a

1229
00:53:00,400 --> 00:53:03,920
report about the work in progress that

1230
00:53:02,079 --> 00:53:06,000
we're doing wrestling with um

1231
00:53:03,920 --> 00:53:07,839
coming up with concepts on on how to

1232
00:53:06,000 --> 00:53:08,559
work on the problem of ai and human

1233
00:53:07,839 --> 00:53:10,720
rights

1234
00:53:08,559 --> 00:53:14,880
um yeah so so i grew into that over the

1235
00:53:10,720 --> 00:53:18,359
last two years or so

1236
00:53:14,880 --> 00:53:21,520
okay um so since uh

1237
00:53:18,359 --> 00:53:24,558
2024 uh many reasons has been

1238
00:53:21,520 --> 00:53:26,160
a challenging year um but uh um

1239
00:53:24,559 --> 00:53:30,800
regarding the topic that you're working

1240
00:53:26,160 --> 00:53:30,799
on what are your wishes for 2021

1241
00:53:31,119 --> 00:53:36,640
um well it would be cool if um

1242
00:53:34,720 --> 00:53:38,319
right from a research perspective it

1243
00:53:36,640 --> 00:53:41,359
would be cool if

1244
00:53:38,319 --> 00:53:43,440
some large it companies

1245
00:53:41,359 --> 00:53:46,078
open up their source code for for

1246
00:53:43,440 --> 00:53:49,520
example ai models they no longer use

1247
00:53:46,079 --> 00:53:53,839
to allow the research community to

1248
00:53:49,520 --> 00:53:56,880
to have a deep dive look at that

1249
00:53:53,839 --> 00:53:59,200
and in the research community also

1250
00:53:56,880 --> 00:54:00,880
for the like in a similar vein that

1251
00:53:59,200 --> 00:54:03,118
researchers start sharing

1252
00:54:00,880 --> 00:54:04,160
the code they produce with their papers

1253
00:54:03,119 --> 00:54:07,200
um

1254
00:54:04,160 --> 00:54:09,520
for everyone which is shockingly um not

1255
00:54:07,200 --> 00:54:12,319
the case in many in many uh

1256
00:54:09,520 --> 00:54:13,759
for many papers so yeah there needs to

1257
00:54:12,319 --> 00:54:16,400
be a

1258
00:54:13,760 --> 00:54:17,680
shift in mindset and we see that

1259
00:54:16,400 --> 00:54:19,280
beginning already

1260
00:54:17,680 --> 00:54:22,240
and that would be a cool trend to

1261
00:54:19,280 --> 00:54:24,000
continue for 2021.

1262
00:54:22,240 --> 00:54:25,520
great well i hope the right people were

1263
00:54:24,000 --> 00:54:28,319
listening just now

1264
00:54:25,520 --> 00:54:30,240
um well thank you johannes very much for

1265
00:54:28,319 --> 00:54:33,440
this interesting talk if uh

1266
00:54:30,240 --> 00:54:35,919
you and um everyone uh at home at

1267
00:54:33,440 --> 00:54:37,359
your screens would like to continue the

1268
00:54:35,920 --> 00:54:40,160
discussion then

1269
00:54:37,359 --> 00:54:41,759
please join johannes in the gypsy room

1270
00:54:40,160 --> 00:54:46,160
you can find that

1271
00:54:41,760 --> 00:54:46,960
under discussion point rc 3.0 point

1272
00:54:46,160 --> 00:54:50,879
social

1273
00:54:46,960 --> 00:54:53,520
i repeat discussion point rc 3.0 point

1274
00:54:50,880 --> 00:54:55,200
social thank you very much and see you

1275
00:54:53,520 --> 00:55:07,839
there

1276
00:54:55,200 --> 00:55:07,839
thank you

1277
00:55:12,210 --> 00:55:18,319
[Music]

1278
00:55:16,240 --> 00:55:18,319
you

