1
00:00:01,500 --> 00:00:10,520
[Music]

2
00:00:07,350 --> 00:00:10,520
[Applause]

3
00:00:13,040 --> 00:00:17,279
and

4
00:00:13,840 --> 00:00:19,920
the talk that is about to begin now is

5
00:00:17,279 --> 00:00:21,920
by christoph schmon from eff and

6
00:00:19,920 --> 00:00:23,439
elizabeth from accessnow

7
00:00:21,920 --> 00:00:25,359
and they were talking about this

8
00:00:23,439 --> 00:00:26,240
probably maybe the biggest legislative

9
00:00:25,359 --> 00:00:28,400
initiative

10
00:00:26,240 --> 00:00:30,479
since the gdpr in brussels it's called

11
00:00:28,400 --> 00:00:32,879
the digital services act and onto

12
00:00:30,480 --> 00:00:32,880
youtube

13
00:00:38,559 --> 00:00:43,040
muted i realized hello uh

14
00:00:41,600 --> 00:00:44,960
first of all thank you very much for

15
00:00:43,040 --> 00:00:47,360
having us today it's a great pleasure

16
00:00:44,960 --> 00:00:48,000
to be the part of this event i think for

17
00:00:47,360 --> 00:00:49,600
both of us

18
00:00:48,000 --> 00:00:51,039
it's the first time we are actually

19
00:00:49,600 --> 00:00:53,120
joining this year

20
00:00:51,039 --> 00:00:54,399
so it's fantastic to be a part of this

21
00:00:53,120 --> 00:00:57,440
great community

22
00:00:54,399 --> 00:00:59,440
um and today we are going to talk about

23
00:00:57,440 --> 00:01:01,039
the legislative proposal still a

24
00:00:59,440 --> 00:01:04,319
proposal which caused

25
00:01:01,039 --> 00:01:06,320
a lot of noise all around europe but not

26
00:01:04,319 --> 00:01:08,399
only in europe but also beyond

27
00:01:06,320 --> 00:01:10,479
and that's the digital services act

28
00:01:08,400 --> 00:01:12,080
legislative package that today we

29
00:01:10,479 --> 00:01:14,320
already know that this legislative

30
00:01:12,080 --> 00:01:16,720
package actually consists of two acts

31
00:01:14,320 --> 00:01:17,758
digital services act and a digital

32
00:01:16,720 --> 00:01:20,000
market act

33
00:01:17,759 --> 00:01:20,960
and both of them will significantly

34
00:01:20,000 --> 00:01:22,960
change the

35
00:01:20,960 --> 00:01:25,439
regulation of online platforms with a

36
00:01:22,960 --> 00:01:27,919
specific focus on very large online

37
00:01:25,439 --> 00:01:30,320
platforms also often referred to as

38
00:01:27,920 --> 00:01:31,920
gatekeepers so those who actually hold a

39
00:01:30,320 --> 00:01:34,720
lot of economic dominance

40
00:01:31,920 --> 00:01:36,479
but also a lot of influence and control

41
00:01:34,720 --> 00:01:37,679
over users rights and the public

42
00:01:36,479 --> 00:01:40,079
discourse

43
00:01:37,680 --> 00:01:40,960
and so i'm going to start with giving

44
00:01:40,079 --> 00:01:43,279
you a quick

45
00:01:40,960 --> 00:01:45,119
introduction into what's the fuss about

46
00:01:43,280 --> 00:01:47,200
what is actually the essay all about why

47
00:01:45,119 --> 00:01:48,640
we are also interested in it and why we

48
00:01:47,200 --> 00:01:50,399
keep talking about it

49
00:01:48,640 --> 00:01:51,680
and why this legislation will keep us

50
00:01:50,399 --> 00:01:54,640
preoccupied

51
00:01:51,680 --> 00:01:55,680
for the years to come the essay was

52
00:01:54,640 --> 00:01:57,840
already announced

53
00:01:55,680 --> 00:01:58,960
two years ago as a part of european

54
00:01:57,840 --> 00:02:02,159
union uh

55
00:01:58,960 --> 00:02:05,039
digital strategy and

56
00:02:02,159 --> 00:02:06,960
it was appointed as one of those actions

57
00:02:05,040 --> 00:02:08,800
that the digital strategy will be

58
00:02:06,960 --> 00:02:10,959
actually consisting of

59
00:02:08,800 --> 00:02:12,480
um and it was the promise that the

60
00:02:10,959 --> 00:02:14,080
european commission gave us

61
00:02:12,480 --> 00:02:16,640
already at that time to create the

62
00:02:14,080 --> 00:02:19,360
systemic regulation of online platforms

63
00:02:16,640 --> 00:02:20,238
that actually places hopefully the users

64
00:02:19,360 --> 00:02:22,800
and their rights

65
00:02:20,239 --> 00:02:24,560
into the center of this upcoming

66
00:02:22,800 --> 00:02:27,360
legislation

67
00:02:24,560 --> 00:02:29,280
so the promise behind the esa is that

68
00:02:27,360 --> 00:02:31,200
this ad based internet bill

69
00:02:29,280 --> 00:02:33,040
and i'm speaking now about aztec and

70
00:02:31,200 --> 00:02:35,040
online targeting the internet as we

71
00:02:33,040 --> 00:02:36,319
actually knew will be actually replaced

72
00:02:35,040 --> 00:02:38,799
by something that puts

73
00:02:36,319 --> 00:02:40,079
user and uses controls and uses right as

74
00:02:38,800 --> 00:02:42,239
a priority

75
00:02:40,080 --> 00:02:44,000
so both of these legislations if

76
00:02:42,239 --> 00:02:46,239
implemented and drafted right

77
00:02:44,000 --> 00:02:48,000
should be actually achieving that goal

78
00:02:46,239 --> 00:02:51,120
in the future

79
00:02:48,000 --> 00:02:51,840
now previously before dsa actually was

80
00:02:51,120 --> 00:02:53,680
drafted

81
00:02:51,840 --> 00:02:55,280
there was some so-called e-commerce

82
00:02:53,680 --> 00:02:56,160
directive in place that actually

83
00:02:55,280 --> 00:02:58,080
established

84
00:02:56,160 --> 00:02:59,599
the basic principles especially in the

85
00:02:58,080 --> 00:03:02,319
field of content governance

86
00:02:59,599 --> 00:03:03,599
i won't go into details uh on that

87
00:03:02,319 --> 00:03:05,599
because i don't want to make this too

88
00:03:03,599 --> 00:03:06,399
legalistic but ultimately the sa

89
00:03:05,599 --> 00:03:08,879
legislation

90
00:03:06,400 --> 00:03:10,640
is supposed to not completely replace

91
00:03:08,879 --> 00:03:12,560
but build up on the top of these

92
00:03:10,640 --> 00:03:14,799
legislation that actually created the

93
00:03:12,560 --> 00:03:16,480
ground and the main legal regime for

94
00:03:14,800 --> 00:03:19,200
almost 20 years in europe

95
00:03:16,480 --> 00:03:19,679
to regulate the user-generated content

96
00:03:19,200 --> 00:03:22,238
so

97
00:03:19,680 --> 00:03:24,400
a dsa and dma as the legislation will

98
00:03:22,239 --> 00:03:26,560
seek to harmonize the rules addressing

99
00:03:24,400 --> 00:03:27,840
the problems such as online hate speech

100
00:03:26,560 --> 00:03:30,720
disinformation

101
00:03:27,840 --> 00:03:32,879
but it also puts emphasis finally on

102
00:03:30,720 --> 00:03:33,519
increased meaningful transparency in

103
00:03:32,879 --> 00:03:35,599
online

104
00:03:33,519 --> 00:03:37,440
advertising the way how the content is

105
00:03:35,599 --> 00:03:40,798
actually being distributed across

106
00:03:37,440 --> 00:03:42,959
platforms and and also

107
00:03:40,799 --> 00:03:43,920
will develop a specific enforcement

108
00:03:42,959 --> 00:03:46,080
mechanism

109
00:03:43,920 --> 00:03:48,319
that will be actually looking into it

110
00:03:46,080 --> 00:03:50,879
now before i will actually go into the

111
00:03:48,319 --> 00:03:52,560
details on the essay and why dsa matters

112
00:03:50,879 --> 00:03:54,480
and do we actually need such a

113
00:03:52,560 --> 00:03:56,319
big new legislative reform that is

114
00:03:54,480 --> 00:03:58,079
coming from the european commission

115
00:03:56,319 --> 00:04:00,399
i want to just unpack it for you a

116
00:03:58,080 --> 00:04:00,959
little bit uh this dsa legislative

117
00:04:00,400 --> 00:04:02,959
package

118
00:04:00,959 --> 00:04:05,519
actually consists of so as i already

119
00:04:02,959 --> 00:04:07,280
mentioned two regulations so regulation

120
00:04:05,519 --> 00:04:09,200
the strongest legal instrument

121
00:04:07,280 --> 00:04:10,480
european commission actually hacks in

122
00:04:09,200 --> 00:04:12,319
has in its hands

123
00:04:10,480 --> 00:04:14,319
which is supposed to achieve the highest

124
00:04:12,319 --> 00:04:14,958
level of harmonization across the member

125
00:04:14,319 --> 00:04:16,560
states

126
00:04:14,959 --> 00:04:18,798
and we all can imagine how difficult

127
00:04:16,560 --> 00:04:21,040
that will be to achieve especially

128
00:04:18,798 --> 00:04:22,799
in the realm of freedom of expression

129
00:04:21,040 --> 00:04:24,720
and particular categories of user

130
00:04:22,800 --> 00:04:26,960
generated content which is so deeply

131
00:04:24,720 --> 00:04:28,720
complex dependent

132
00:04:26,960 --> 00:04:30,880
all of those related to content

133
00:04:28,720 --> 00:04:33,360
moderation and content curation

134
00:04:30,880 --> 00:04:34,000
will be mainly in the realm of digital

135
00:04:33,360 --> 00:04:36,160
services

136
00:04:34,000 --> 00:04:38,400
act and then the second regulation the

137
00:04:36,160 --> 00:04:40,240
digital market act will be specifically

138
00:04:38,400 --> 00:04:40,880
looking at the dominance of the online

139
00:04:40,240 --> 00:04:42,960
platforms

140
00:04:40,880 --> 00:04:44,159
economic dominance competitive

141
00:04:42,960 --> 00:04:46,159
environment for

142
00:04:44,160 --> 00:04:47,280
smaller players the fairness in the

143
00:04:46,160 --> 00:04:49,520
competition

144
00:04:47,280 --> 00:04:51,440
and and it will also establish the list

145
00:04:49,520 --> 00:04:53,120
of do's and don'ts for gatekeepers

146
00:04:51,440 --> 00:04:55,680
platform so something that

147
00:04:53,120 --> 00:04:57,360
uh so the platforms that actually hold

148
00:04:55,680 --> 00:05:00,160
that relevant dominance

149
00:04:57,360 --> 00:05:01,120
and now based on these new proposals we

150
00:05:00,160 --> 00:05:03,120
know that these

151
00:05:01,120 --> 00:05:04,800
platforms are mainly called as very

152
00:05:03,120 --> 00:05:06,639
large online platforms so this is

153
00:05:04,800 --> 00:05:08,400
exactly how the legislation refers to

154
00:05:06,639 --> 00:05:11,120
gatekeepers now

155
00:05:08,400 --> 00:05:12,159
um now i think one more point that i

156
00:05:11,120 --> 00:05:15,360
want to make is

157
00:05:12,160 --> 00:05:17,919
that uh it the dsa and dma were launched

158
00:05:15,360 --> 00:05:20,080
on the 15th of december 2020 so

159
00:05:17,919 --> 00:05:21,758
it was literally a christmas present

160
00:05:20,080 --> 00:05:23,919
given to the digital rights community by

161
00:05:21,759 --> 00:05:27,520
the european commission along long

162
00:05:23,919 --> 00:05:29,840
time anticipated one the work on the

163
00:05:27,520 --> 00:05:31,840
essays started however much earlier

164
00:05:29,840 --> 00:05:33,840
electronic frontiers foundations as much

165
00:05:31,840 --> 00:05:35,599
as access now together with edri

166
00:05:33,840 --> 00:05:38,000
were working very hard to come up with

167
00:05:35,600 --> 00:05:39,520
the priorities and recommendations what

168
00:05:38,000 --> 00:05:41,360
we would like to see within these

169
00:05:39,520 --> 00:05:43,359
legislations to be unfriend

170
00:05:41,360 --> 00:05:45,039
because from the beginning we understood

171
00:05:43,360 --> 00:05:47,280
the importance and the far-reaching

172
00:05:45,039 --> 00:05:49,199
consequences this legislation will have

173
00:05:47,280 --> 00:05:50,559
not only inside of the european union

174
00:05:49,199 --> 00:05:52,080
but also beyond

175
00:05:50,560 --> 00:05:53,840
and that brings me to the final

176
00:05:52,080 --> 00:05:56,159
introductory point that i want to make

177
00:05:53,840 --> 00:05:59,119
before i will hand over to chris which

178
00:05:56,160 --> 00:06:01,600
is why and do we actually need the dsa

179
00:05:59,120 --> 00:06:03,680
we strongly believe that there is a big

180
00:06:01,600 --> 00:06:06,000
justification and good reason to

181
00:06:03,680 --> 00:06:08,479
actually establish a systemic regulation

182
00:06:06,000 --> 00:06:10,160
of online platforms in order to secure

183
00:06:08,479 --> 00:06:12,240
users fundamental rights

184
00:06:10,160 --> 00:06:13,840
empower them and also to protect our

185
00:06:12,240 --> 00:06:15,840
democratic discourse

186
00:06:13,840 --> 00:06:18,000
and this is due to the fact that for

187
00:06:15,840 --> 00:06:20,799
many years we're witnessing this space

188
00:06:18,000 --> 00:06:22,080
a quite bad regulatory practices in

189
00:06:20,800 --> 00:06:23,919
platform governance

190
00:06:22,080 --> 00:06:25,840
that are coming from member states and

191
00:06:23,919 --> 00:06:27,280
chris will provide for a very concrete

192
00:06:25,840 --> 00:06:28,880
examples in that regard

193
00:06:27,280 --> 00:06:30,318
but also coming from the european

194
00:06:28,880 --> 00:06:33,360
commission itself

195
00:06:30,319 --> 00:06:35,280
mainly the proposed online terrorist

196
00:06:33,360 --> 00:06:36,960
content regulation for instance

197
00:06:35,280 --> 00:06:39,198
or we all remember the story of

198
00:06:36,960 --> 00:06:40,560
copyright that crystal will discuss a

199
00:06:39,199 --> 00:06:43,039
little bit further

200
00:06:40,560 --> 00:06:43,600
we saw how not only the member states

201
00:06:43,039 --> 00:06:46,159
but also

202
00:06:43,600 --> 00:06:48,000
european commission or european union in

203
00:06:46,160 --> 00:06:50,639
order to actually um

204
00:06:48,000 --> 00:06:51,120
uh establish some order in the digital

205
00:06:50,639 --> 00:06:53,280
space

206
00:06:51,120 --> 00:06:54,880
they started pushing the state's

207
00:06:53,280 --> 00:06:56,960
obligation and especially states

208
00:06:54,880 --> 00:07:00,080
positive obligation to protect users

209
00:06:56,960 --> 00:07:02,560
human rights in hands of online private

210
00:07:00,080 --> 00:07:05,840
platforms that started replacing

211
00:07:02,560 --> 00:07:07,520
uh state actors and public authorities

212
00:07:05,840 --> 00:07:09,280
within the online space

213
00:07:07,520 --> 00:07:10,880
they started assessing the content the

214
00:07:09,280 --> 00:07:13,039
legality of the content

215
00:07:10,880 --> 00:07:14,479
deciding under very short time frames

216
00:07:13,039 --> 00:07:15,360
what should stay online and what should

217
00:07:14,479 --> 00:07:17,758
go offline

218
00:07:15,360 --> 00:07:20,000
with no public scrutiny or transparency

219
00:07:17,759 --> 00:07:22,479
about practices that they kept deploying

220
00:07:20,000 --> 00:07:25,120
and they keep deploying to this day

221
00:07:22,479 --> 00:07:26,479
uh of course platforms under the threat

222
00:07:25,120 --> 00:07:29,680
of legal liability

223
00:07:26,479 --> 00:07:31,599
uh often uh had to rely and still have

224
00:07:29,680 --> 00:07:33,759
to rely on the content recognition

225
00:07:31,599 --> 00:07:34,960
technologies for removing user-generated

226
00:07:33,759 --> 00:07:37,039
content

227
00:07:34,960 --> 00:07:38,719
a typical example could be also avial

228
00:07:37,039 --> 00:07:39,919
law which will be still mentioned today

229
00:07:38,720 --> 00:07:42,000
during the presentation

230
00:07:39,919 --> 00:07:43,359
and the typical time frames are usually

231
00:07:42,000 --> 00:07:46,000
those that

232
00:07:43,360 --> 00:07:48,080
extend from one hour to 24 hours which

233
00:07:46,000 --> 00:07:49,680
is extremely short especially if any

234
00:07:48,080 --> 00:07:50,479
users would like to appeal such a

235
00:07:49,680 --> 00:07:53,759
decision

236
00:07:50,479 --> 00:07:55,280
or seek an effective remedy at the same

237
00:07:53,759 --> 00:07:58,960
time due to the

238
00:07:55,280 --> 00:08:01,039
lack of harmonization and lack of proper

239
00:07:58,960 --> 00:08:02,799
set of responsibilities that should lie

240
00:08:01,039 --> 00:08:04,560
in hands of these online platforms

241
00:08:02,800 --> 00:08:06,400
there was a lack of legal certainty

242
00:08:04,560 --> 00:08:07,120
which would only reinforce that vicious

243
00:08:06,400 --> 00:08:09,280
circle

244
00:08:07,120 --> 00:08:11,520
of removing more and more of online

245
00:08:09,280 --> 00:08:13,280
content in order to escape any possible

246
00:08:11,520 --> 00:08:15,840
liability

247
00:08:13,280 --> 00:08:17,440
and at the end to this day due to the

248
00:08:15,840 --> 00:08:19,840
lack of transparency

249
00:08:17,440 --> 00:08:20,960
we lack any evidence or research based

250
00:08:19,840 --> 00:08:22,960
policy making

251
00:08:20,960 --> 00:08:25,039
because platforms do not want to share

252
00:08:22,960 --> 00:08:26,000
or inform the public authorities what

253
00:08:25,039 --> 00:08:28,318
they actually do with

254
00:08:26,000 --> 00:08:30,240
the content how they moderate and those

255
00:08:28,319 --> 00:08:32,080
transparency information that we receive

256
00:08:30,240 --> 00:08:34,080
within their transparency reports are

257
00:08:32,080 --> 00:08:34,880
usually quantity oriented instead of

258
00:08:34,080 --> 00:08:36,959
quality

259
00:08:34,880 --> 00:08:39,279
so they focus on how much content is

260
00:08:36,958 --> 00:08:42,000
actually being removed and how fast

261
00:08:39,279 --> 00:08:43,599
which is not enough in order to create

262
00:08:42,000 --> 00:08:45,920
laws that can actually provide

263
00:08:43,599 --> 00:08:48,000
any more sustainable solutions and

264
00:08:45,920 --> 00:08:50,800
ultimately as we all agree

265
00:08:48,000 --> 00:08:52,720
um the core issue doesn't lie that much

266
00:08:50,800 --> 00:08:54,479
with how the content is being moderated

267
00:08:52,720 --> 00:08:56,000
but how content is being distributed

268
00:08:54,480 --> 00:08:58,320
across platforms

269
00:08:56,000 --> 00:09:00,320
um within the core of their business

270
00:08:58,320 --> 00:09:02,880
models that actually stands on a

271
00:09:00,320 --> 00:09:05,519
on a attention economy and on a way how

272
00:09:02,880 --> 00:09:08,160
sensational content is often being

273
00:09:05,519 --> 00:09:08,560
amplified in order to actually prolong

274
00:09:08,160 --> 00:09:10,399
that

275
00:09:08,560 --> 00:09:12,479
attention span of users that visit

276
00:09:10,399 --> 00:09:14,480
platforms on regular basis

277
00:09:12,480 --> 00:09:16,560
i'm at back i'm unpacking quite a few

278
00:09:14,480 --> 00:09:18,880
issues here and this is supposed to be

279
00:09:16,560 --> 00:09:20,959
just a quick introductory remark

280
00:09:18,880 --> 00:09:22,560
uh i will now hand over to christoph who

281
00:09:20,959 --> 00:09:24,239
will elaborate on all this point a

282
00:09:22,560 --> 00:09:24,719
little bit further and then we take a

283
00:09:24,240 --> 00:09:27,040
look and

284
00:09:24,720 --> 00:09:27,839
unpack few quite important parts of the

285
00:09:27,040 --> 00:09:30,480
essay

286
00:09:27,839 --> 00:09:30,959
that we feel should be prioritizing this

287
00:09:30,480 --> 00:09:35,040
debate

288
00:09:30,959 --> 00:09:38,319
at the moment chris over to you

289
00:09:35,040 --> 00:09:39,519
thanks alicia hi everybody um i'm quite

290
00:09:38,320 --> 00:09:41,040
sure that

291
00:09:39,519 --> 00:09:43,120
many of you have noticed that there's a

292
00:09:41,040 --> 00:09:44,319
growing appetite from the side of the

293
00:09:43,120 --> 00:09:46,160
european union to

294
00:09:44,320 --> 00:09:48,480
regulate the internet by using online

295
00:09:46,160 --> 00:09:50,959
platforms as their helping hands

296
00:09:48,480 --> 00:09:52,959
to monitor and sensor with users can say

297
00:09:50,959 --> 00:09:54,959
share do online

298
00:09:52,959 --> 00:09:56,719
as you see on the slide the first

299
00:09:54,959 --> 00:09:58,959
highlight of this

300
00:09:56,720 --> 00:10:00,640
growing appetite was corporate upload

301
00:09:58,959 --> 00:10:02,959
filters which are supposed to spot

302
00:10:00,640 --> 00:10:04,880
copyright protected content online

303
00:10:02,959 --> 00:10:06,560
thousands of people od and young went on

304
00:10:04,880 --> 00:10:09,120
the streets to demonstrate for

305
00:10:06,560 --> 00:10:10,800
free internet and to demonstrate against

306
00:10:09,120 --> 00:10:11,680
technical measures to turn the internet

307
00:10:10,800 --> 00:10:14,240
into some sort of

308
00:10:11,680 --> 00:10:15,920
censorship machine we've made a point

309
00:10:14,240 --> 00:10:16,959
then and we continue making the point

310
00:10:15,920 --> 00:10:19,680
now that

311
00:10:16,959 --> 00:10:21,599
upload filters are prone to error that

312
00:10:19,680 --> 00:10:23,599
upload filters cannot understand context

313
00:10:21,600 --> 00:10:25,360
that they are unaffordable by all but

314
00:10:23,600 --> 00:10:26,880
the largest tech companies

315
00:10:25,360 --> 00:10:30,079
and which happen to be all based in the

316
00:10:26,880 --> 00:10:32,079
united states but as you well know

317
00:10:30,079 --> 00:10:34,160
policymakers would not listen and

318
00:10:32,079 --> 00:10:35,920
article 13 of the copyright directive

319
00:10:34,160 --> 00:10:38,319
was adopted by a small

320
00:10:35,920 --> 00:10:39,839
small margin in the european parliament

321
00:10:38,320 --> 00:10:41,120
also because some members of the open

322
00:10:39,839 --> 00:10:43,360
parliament had troubles to press the

323
00:10:41,120 --> 00:10:45,760
right buttons

324
00:10:43,360 --> 00:10:47,040
but i think it's important for us to

325
00:10:45,760 --> 00:10:49,120
understand that the fight is

326
00:10:47,040 --> 00:10:50,880
the fight is far from over the member

327
00:10:49,120 --> 00:10:52,640
states must now implement the directive

328
00:10:50,880 --> 00:10:53,600
in a way that is not at odds with

329
00:10:52,640 --> 00:10:56,000
fundamental rights

330
00:10:53,600 --> 00:10:57,600
and we argue that mandated automated

331
00:10:56,000 --> 00:10:59,360
removal technologies are

332
00:10:57,600 --> 00:11:01,680
always in conflict with fundamental

333
00:10:59,360 --> 00:11:02,560
rights and this includes data protection

334
00:11:01,680 --> 00:11:04,640
rights

335
00:11:02,560 --> 00:11:06,479
it is a data protection right not to be

336
00:11:04,640 --> 00:11:08,000
made subject to automated decision

337
00:11:06,480 --> 00:11:08,880
making online if it involves your

338
00:11:08,000 --> 00:11:10,560
personal data

339
00:11:08,880 --> 00:11:12,079
and if such decision making has a

340
00:11:10,560 --> 00:11:13,439
negative effect

341
00:11:12,079 --> 00:11:15,199
but if you leave all those legal

342
00:11:13,440 --> 00:11:17,360
arguments aside um

343
00:11:15,200 --> 00:11:19,040
i think the most boring experience with

344
00:11:17,360 --> 00:11:21,040
upload filters is that they had a

345
00:11:19,040 --> 00:11:23,279
spillover effect

346
00:11:21,040 --> 00:11:24,880
a spillover effect to other initiatives

347
00:11:23,279 --> 00:11:26,800
sure if it works for copyright protected

348
00:11:24,880 --> 00:11:28,880
content it may well work for other type

349
00:11:26,800 --> 00:11:31,120
of content right

350
00:11:28,880 --> 00:11:32,640
except that it doesn't many considered

351
00:11:31,120 --> 00:11:34,880
now a clever idea that

352
00:11:32,640 --> 00:11:36,480
platforms should proactively monitor and

353
00:11:34,880 --> 00:11:38,240
check all sorts of user

354
00:11:36,480 --> 00:11:39,920
content may just be communication

355
00:11:38,240 --> 00:11:41,519
pictures or videos

356
00:11:39,920 --> 00:11:43,040
and they should use filters to take it

357
00:11:41,519 --> 00:11:43,760
down or to filters to prevent the

358
00:11:43,040 --> 00:11:47,439
re-upload

359
00:11:43,760 --> 00:11:49,279
of such content an example of such um

360
00:11:47,440 --> 00:11:51,040
spillover effect and english as i

361
00:11:49,279 --> 00:11:53,200
mentioned is the huge draft regulation

362
00:11:51,040 --> 00:11:55,199
on terrorist related content

363
00:11:53,200 --> 00:11:56,639
it took a huge joint effort of civil

364
00:11:55,200 --> 00:11:58,240
society groups and some members of

365
00:11:56,639 --> 00:12:02,000
parliament

366
00:11:58,240 --> 00:12:05,200
to reject the worst of all texts

367
00:12:02,000 --> 00:12:07,360
we had recent negotiations going on and

368
00:12:05,200 --> 00:12:08,480
at least we managed to get out the

369
00:12:07,360 --> 00:12:10,399
requirement to

370
00:12:08,480 --> 00:12:13,120
use upload failures but there's still a

371
00:12:10,399 --> 00:12:14,959
24 hours removal obligation that may not

372
00:12:13,120 --> 00:12:17,440
platforms to employ those filters

373
00:12:14,959 --> 00:12:20,000
nevertheless

374
00:12:17,440 --> 00:12:21,519
and we see that particularly national

375
00:12:20,000 --> 00:12:22,240
states are very fond of the idea that

376
00:12:21,519 --> 00:12:24,000
platforms

377
00:12:22,240 --> 00:12:26,160
rather than independent judges should be

378
00:12:24,000 --> 00:12:27,680
the new law enforcers

379
00:12:26,160 --> 00:12:29,439
there are several states in the european

380
00:12:27,680 --> 00:12:30,959
union that have adopted laws that would

381
00:12:29,440 --> 00:12:32,959
either oblige

382
00:12:30,959 --> 00:12:34,160
or nudge platforms to monitor use of

383
00:12:32,959 --> 00:12:37,279
speech online

384
00:12:34,160 --> 00:12:39,199
first up was the german net dg which set

385
00:12:37,279 --> 00:12:40,720
out systematic duties for platforms then

386
00:12:39,200 --> 00:12:42,720
we had the french lavia

387
00:12:40,720 --> 00:12:44,639
which copy based at the nasdg and made

388
00:12:42,720 --> 00:12:46,399
it worse and last we have the austrian

389
00:12:44,639 --> 00:12:48,560
hate speech bill which is a mix of both

390
00:12:46,399 --> 00:12:50,240
the german and the french proposal

391
00:12:48,560 --> 00:12:52,638
they all go much beyond copyright

392
00:12:50,240 --> 00:12:54,320
content but focus on hate speech and all

393
00:12:52,639 --> 00:12:55,519
sorts of content that may be considered

394
00:12:54,320 --> 00:12:58,160
problematic or

395
00:12:55,519 --> 00:13:00,160
in those respective countries not

396
00:12:58,160 --> 00:13:02,639
necessarily in other countries

397
00:13:00,160 --> 00:13:04,399
and this brings me to the next problem

398
00:13:02,639 --> 00:13:06,079
how do we deal with content that is

399
00:13:04,399 --> 00:13:07,920
illegal in one country but legally in

400
00:13:06,079 --> 00:13:09,519
another

401
00:13:07,920 --> 00:13:11,360
a recent court of justice ruling had

402
00:13:09,519 --> 00:13:12,320
confirmed that a court of a small state

403
00:13:11,360 --> 00:13:15,120
like austria

404
00:13:12,320 --> 00:13:17,040
can order platforms not only to take

405
00:13:15,120 --> 00:13:18,079
down deformatory content globally but

406
00:13:17,040 --> 00:13:20,240
also to take down

407
00:13:18,079 --> 00:13:22,160
identical or equivalent material using

408
00:13:20,240 --> 00:13:24,800
automated technologies

409
00:13:22,160 --> 00:13:26,639
for us this is a terrible outcome as

410
00:13:24,800 --> 00:13:28,479
this will lead to race to the bottom

411
00:13:26,639 --> 00:13:30,639
bottom where the countries with the

412
00:13:28,480 --> 00:13:31,839
least freedom of speech friendly laws

413
00:13:30,639 --> 00:13:33,760
can superimpose

414
00:13:31,839 --> 00:13:35,120
their laws on every other state in the

415
00:13:33,760 --> 00:13:37,360
world

416
00:13:35,120 --> 00:13:38,560
we really believe that all this nonsense

417
00:13:37,360 --> 00:13:40,000
has to stop

418
00:13:38,560 --> 00:13:42,239
it's time to acknowledge that the

419
00:13:40,000 --> 00:13:44,000
internet is a global space a place of

420
00:13:42,240 --> 00:13:45,360
exchange of creativity in the place

421
00:13:44,000 --> 00:13:47,440
where civil liberties are supposed to

422
00:13:45,360 --> 00:13:49,360
exist

423
00:13:47,440 --> 00:13:50,880
so we are fighting now against all those

424
00:13:49,360 --> 00:13:53,040
national initiatives

425
00:13:50,880 --> 00:13:55,120
we had the great first victory um when

426
00:13:53,040 --> 00:13:58,399
we had to bring down the french

427
00:13:55,120 --> 00:14:00,000
lora via the avia bill that had imposed

428
00:13:58,399 --> 00:14:02,240
a duty for platforms to check

429
00:14:00,000 --> 00:14:04,639
and remove potential potentially legal

430
00:14:02,240 --> 00:14:06,639
content within 24 hours

431
00:14:04,639 --> 00:14:08,399
before they can say constitutional the

432
00:14:06,639 --> 00:14:10,320
french supreme court we had argued that

433
00:14:08,399 --> 00:14:12,000
this would push platforms to constantly

434
00:14:10,320 --> 00:14:15,360
check what users post

435
00:14:12,000 --> 00:14:17,279
and if platforms face high fines

436
00:14:15,360 --> 00:14:19,360
of course steve would be rather

437
00:14:17,279 --> 00:14:20,880
motivated to block as much contestable

438
00:14:19,360 --> 00:14:23,120
content as possible

439
00:14:20,880 --> 00:14:24,480
you've made a point that this would be

440
00:14:23,120 --> 00:14:26,079
against the shadow fundamental rights

441
00:14:24,480 --> 00:14:27,839
including freedom of information and

442
00:14:26,079 --> 00:14:30,959
freedom of expression

443
00:14:27,839 --> 00:14:33,360
and it was a great victory for us that

444
00:14:30,959 --> 00:14:35,439
the french supreme court has struck down

445
00:14:33,360 --> 00:14:39,199
the french avia bill and followed our

446
00:14:35,440 --> 00:14:40,800
argument as you see it on the slide

447
00:14:39,199 --> 00:14:42,560
we also see that now that there's a push

448
00:14:40,800 --> 00:14:43,599
back at least against the update of the

449
00:14:42,560 --> 00:14:45,119
german netsdg

450
00:14:43,600 --> 00:14:47,440
which would have provided new access

451
00:14:45,120 --> 00:14:49,120
rights for law enforcement authorities

452
00:14:47,440 --> 00:14:50,720
this and other provisions are considered

453
00:14:49,120 --> 00:14:51,440
unconstitutional and as far as i

454
00:14:50,720 --> 00:14:54,000
understand

455
00:14:51,440 --> 00:14:55,199
and that's listeners can correct me the

456
00:14:54,000 --> 00:14:57,600
german president has

457
00:14:55,199 --> 00:15:00,000
refused to sign the bill and the

458
00:14:57,600 --> 00:15:02,639
austrian bill goes a similar pathway

459
00:15:00,000 --> 00:15:04,160
got recently a red light from brussels

460
00:15:02,639 --> 00:15:06,079
the commission considered

461
00:15:04,160 --> 00:15:07,199
considers it in conflict with your

462
00:15:06,079 --> 00:15:09,279
principles

463
00:15:07,199 --> 00:15:10,479
also thanks to joint effort by epicenter

464
00:15:09,279 --> 00:15:13,279
works

465
00:15:10,480 --> 00:15:14,880
and this shows that something positive

466
00:15:13,279 --> 00:15:15,839
is going on is that positive development

467
00:15:14,880 --> 00:15:18,880
the pushback

468
00:15:15,839 --> 00:15:20,160
against our midfielder technologies

469
00:15:18,880 --> 00:15:21,920
but it's important to understand that

470
00:15:20,160 --> 00:15:23,600
those national initiatives are not just

471
00:15:21,920 --> 00:15:24,880
purely national attempt to regulate hate

472
00:15:23,600 --> 00:15:27,120
speech

473
00:15:24,880 --> 00:15:28,240
it's an attempt of an eu member state to

474
00:15:27,120 --> 00:15:30,480
make their own bills

475
00:15:28,240 --> 00:15:32,880
as badly as they are some sort of a

476
00:15:30,480 --> 00:15:35,360
prototype for you white legislation

477
00:15:32,880 --> 00:15:36,880
a prototype for the digital services act

478
00:15:35,360 --> 00:15:38,720
and as you know national member states

479
00:15:36,880 --> 00:15:40,079
have the same new law making

480
00:15:38,720 --> 00:15:41,519
their voices are represented in the

481
00:15:40,079 --> 00:15:42,479
council of the eu and the european

482
00:15:41,519 --> 00:15:44,800
commission

483
00:15:42,480 --> 00:15:46,320
which will will be disincentivized to

484
00:15:44,800 --> 00:15:47,839
propose anything that would be voted

485
00:15:46,320 --> 00:15:49,360
down by council

486
00:15:47,839 --> 00:15:51,199
i think that's a nice takeaway from

487
00:15:49,360 --> 00:15:52,560
today that law making and national

488
00:15:51,199 --> 00:15:54,800
member states

489
00:15:52,560 --> 00:15:57,920
are not an isolated event it's always

490
00:15:54,800 --> 00:16:01,279
political it's always nets politic

491
00:15:57,920 --> 00:16:03,120
um the good news is that as far as the

492
00:16:01,279 --> 00:16:05,279
ump commission proposal for the digital

493
00:16:03,120 --> 00:16:07,120
services act is concerned

494
00:16:05,279 --> 00:16:08,399
that it has not followed the footsteps

495
00:16:07,120 --> 00:16:11,120
of those bad

496
00:16:08,399 --> 00:16:13,040
badly designed and misguided bills it

497
00:16:11,120 --> 00:16:14,800
has respected our input the input from

498
00:16:13,040 --> 00:16:17,599
access now from eff from the

499
00:16:14,800 --> 00:16:18,399
network from academics and many others

500
00:16:17,600 --> 00:16:20,000
that

501
00:16:18,399 --> 00:16:22,079
some key principles should not be

502
00:16:20,000 --> 00:16:24,000
removed like that liability for speed

503
00:16:22,079 --> 00:16:26,479
should rest with the speaker

504
00:16:24,000 --> 00:16:28,240
the dsa gives also red light to channel

505
00:16:26,480 --> 00:16:29,839
monitoring of user content

506
00:16:28,240 --> 00:16:31,519
and there are no short deadlines in

507
00:16:29,839 --> 00:16:32,639
there to remove content that might be

508
00:16:31,519 --> 00:16:34,079
illegal

509
00:16:32,639 --> 00:16:35,839
instead the commission gives more

510
00:16:34,079 --> 00:16:37,680
selective platforms to take down posts

511
00:16:35,839 --> 00:16:41,440
in good faith

512
00:16:37,680 --> 00:16:43,599
um which is or what we which we call the

513
00:16:41,440 --> 00:16:45,120
eu style good samaritan clause

514
00:16:43,600 --> 00:16:46,959
looking through the global lenses of law

515
00:16:45,120 --> 00:16:48,240
making it's very fascinating to see that

516
00:16:46,959 --> 00:16:49,920
that whilst the united states is

517
00:16:48,240 --> 00:16:51,680
flirting with the idea to move away from

518
00:16:49,920 --> 00:16:54,079
the good samaritan principle in section

519
00:16:51,680 --> 00:16:55,839
230 of the communications decency act

520
00:16:54,079 --> 00:16:57,839
so the idea that platforms can

521
00:16:55,839 --> 00:16:59,440
voluntarily remove content without being

522
00:16:57,839 --> 00:17:01,519
held liable for it

523
00:16:59,440 --> 00:17:02,480
the european union flirts with the idea

524
00:17:01,519 --> 00:17:05,760
to introduce it

525
00:17:02,480 --> 00:17:07,599
to give more options to platforms to act

526
00:17:05,760 --> 00:17:09,439
that being said the major differences

527
00:17:07,599 --> 00:17:11,760
between the us and the eu is that

528
00:17:09,439 --> 00:17:13,280
in europe platforms could be held liable

529
00:17:11,760 --> 00:17:15,119
the moment they become aware of the

530
00:17:13,280 --> 00:17:17,520
illegality of content

531
00:17:15,119 --> 00:17:19,280
and that's an issue because the services

532
00:17:17,520 --> 00:17:20,559
act has now introduced a relatively

533
00:17:19,280 --> 00:17:23,280
sophisticated system

534
00:17:20,559 --> 00:17:24,399
for user notification complete mechanism

535
00:17:23,280 --> 00:17:27,520
dispute resolution

536
00:17:24,400 --> 00:17:29,360
options which all lead to such awareness

537
00:17:27,520 --> 00:17:30,480
about illegality or could lead to such

538
00:17:29,360 --> 00:17:31,840
awareness

539
00:17:30,480 --> 00:17:33,520
and it's not quite clear for us how

540
00:17:31,840 --> 00:17:35,918
platforms will make use of voluntary

541
00:17:33,520 --> 00:17:38,000
measures to remove content

542
00:17:35,919 --> 00:17:39,280
that being said um we think that the

543
00:17:38,000 --> 00:17:40,080
commission's proposal could have been

544
00:17:39,280 --> 00:17:41,520
much worse

545
00:17:40,080 --> 00:17:43,918
and the parliament's reports on the

546
00:17:41,520 --> 00:17:45,760
digital services act have demonstrated

547
00:17:43,919 --> 00:17:47,360
that the new parliament is a bit better

548
00:17:45,760 --> 00:17:49,200
than the old one they have a lot of

549
00:17:47,360 --> 00:17:50,799
respect for fundamental rights

550
00:17:49,200 --> 00:17:52,320
the many members of parliament that are

551
00:17:50,799 --> 00:17:55,280
quite fond of the idea

552
00:17:52,320 --> 00:17:56,080
to protect civil liberties online but we

553
00:17:55,280 --> 00:17:58,399
know that this was

554
00:17:56,080 --> 00:17:59,520
only the start and we know that we need

555
00:17:58,400 --> 00:18:01,360
another joint effort

556
00:17:59,520 --> 00:18:02,799
to ensure that users are not monitored

557
00:18:01,360 --> 00:18:04,159
and not at the mercy of algorithmic

558
00:18:02,799 --> 00:18:05,360
decision making

559
00:18:04,160 --> 00:18:08,720
and the think alicia is now going to

560
00:18:05,360 --> 00:18:08,719
explain a bit more about all this

561
00:18:10,000 --> 00:18:14,000
thank you thank you very much chris um

562
00:18:12,320 --> 00:18:17,360
so we can actually move

563
00:18:14,000 --> 00:18:19,200
now further and unpack few

564
00:18:17,360 --> 00:18:20,399
relevant provisions for everything that

565
00:18:19,200 --> 00:18:21,760
has already been mentioned

566
00:18:20,400 --> 00:18:23,679
mainly in the realm of content

567
00:18:21,760 --> 00:18:26,160
moderation and content curation

568
00:18:23,679 --> 00:18:27,200
which ultimately lies in the core of

569
00:18:26,160 --> 00:18:29,679
digital services

570
00:18:27,200 --> 00:18:30,960
act and maybe not to make it all so

571
00:18:29,679 --> 00:18:32,880
abstract um

572
00:18:30,960 --> 00:18:34,240
i also have the printed version of the

573
00:18:32,880 --> 00:18:36,640
law here with me

574
00:18:34,240 --> 00:18:38,080
and if you look at it it's quite a

575
00:18:36,640 --> 00:18:39,760
impressive piece of work that the

576
00:18:38,080 --> 00:18:42,639
european commission did there

577
00:18:39,760 --> 00:18:43,200
and i have to say that even though it's

578
00:18:42,640 --> 00:18:46,160
a

579
00:18:43,200 --> 00:18:47,600
great start it still uh contains a lot

580
00:18:46,160 --> 00:18:49,679
of imperfections

581
00:18:47,600 --> 00:18:51,760
and and i will try to summarize those

582
00:18:49,679 --> 00:18:53,360
now uh for you especially in the light

583
00:18:51,760 --> 00:18:56,480
of our arm positioning

584
00:18:53,360 --> 00:18:57,199
uh and as i mean uh civil societies in

585
00:18:56,480 --> 00:18:58,799
general

586
00:18:57,200 --> 00:19:00,559
because i believe that on all those

587
00:18:58,799 --> 00:19:03,039
points we have a pretty solid uh

588
00:19:00,559 --> 00:19:04,639
agreement among each other um and what

589
00:19:03,039 --> 00:19:05,679
we were actually hoping that digital

590
00:19:04,640 --> 00:19:08,480
services act

591
00:19:05,679 --> 00:19:09,200
will do what it actually does and where

592
00:19:08,480 --> 00:19:11,120
we see

593
00:19:09,200 --> 00:19:12,960
that uh we will need to actually

594
00:19:11,120 --> 00:19:14,320
continue working very closely

595
00:19:12,960 --> 00:19:16,320
especially with the members of the

596
00:19:14,320 --> 00:19:17,918
european parliament in the future

597
00:19:16,320 --> 00:19:19,520
once the draft will actually enter

598
00:19:17,919 --> 00:19:21,120
european parliament which should happen

599
00:19:19,520 --> 00:19:24,080
relatively soon

600
00:19:21,120 --> 00:19:24,879
so as it already as i already mentioned

601
00:19:24,080 --> 00:19:28,000
at the beginning

602
00:19:24,880 --> 00:19:30,080
quite briefly is how actually dsa

603
00:19:28,000 --> 00:19:31,760
distinguishes between online platforms

604
00:19:30,080 --> 00:19:32,639
which are defined within the scope of

605
00:19:31,760 --> 00:19:35,039
the law

606
00:19:32,640 --> 00:19:35,840
and uh between very large online

607
00:19:35,039 --> 00:19:37,760
platforms

608
00:19:35,840 --> 00:19:40,080
which is exactly that scope where all

609
00:19:37,760 --> 00:19:41,520
large online gatekeepers fall into

610
00:19:40,080 --> 00:19:43,799
the essay specifically then

611
00:19:41,520 --> 00:19:46,160
distinguishes between obligations or

612
00:19:43,799 --> 00:19:48,080
responsibilities of these actors

613
00:19:46,160 --> 00:19:50,080
some assigning to all of them or

614
00:19:48,080 --> 00:19:52,320
including online platforms and some

615
00:19:50,080 --> 00:19:54,000
being extended specifically due to the

616
00:19:52,320 --> 00:19:54,799
dominance and power these online

617
00:19:54,000 --> 00:19:57,919
gatekeepers

618
00:19:54,799 --> 00:19:59,520
hold uh this is mainly then the case

619
00:19:57,919 --> 00:20:01,360
when we discuss the requirements for

620
00:19:59,520 --> 00:20:03,520
transparency there is a set of

621
00:20:01,360 --> 00:20:05,199
requirements for transparency that apply

622
00:20:03,520 --> 00:20:06,000
to online platforms but then there is

623
00:20:05,200 --> 00:20:07,840
still specific

624
00:20:06,000 --> 00:20:10,559
additional set of transparency

625
00:20:07,840 --> 00:20:13,199
requirements for larger online platforms

626
00:20:10,559 --> 00:20:14,879
what dsa does especially and this is the

627
00:20:13,200 --> 00:20:16,880
bid which is extremely relevant for the

628
00:20:14,880 --> 00:20:19,440
content moderation practices

629
00:20:16,880 --> 00:20:20,000
it attempts to establish a harmonized

630
00:20:19,440 --> 00:20:21,679
model for

631
00:20:20,000 --> 00:20:23,600
notice and action procedure for

632
00:20:21,679 --> 00:20:25,760
allegedly illegal content

633
00:20:23,600 --> 00:20:26,879
one of our red lines before i go into

634
00:20:25,760 --> 00:20:30,240
the details on this

635
00:20:26,880 --> 00:20:32,880
uh was that dsa will be touching only

636
00:20:30,240 --> 00:20:34,159
uh or trying to regulate only allegedly

637
00:20:32,880 --> 00:20:36,640
illegal content

638
00:20:34,159 --> 00:20:38,640
and stay away from vaguely defined

639
00:20:36,640 --> 00:20:40,799
content categories such as potentially

640
00:20:38,640 --> 00:20:42,960
harmful but legal content

641
00:20:40,799 --> 00:20:44,960
there are other ways how the legislation

642
00:20:42,960 --> 00:20:46,880
can tackle these content mainly through

643
00:20:44,960 --> 00:20:48,799
the

644
00:20:46,880 --> 00:20:50,240
meaningful transparency requirements

645
00:20:48,799 --> 00:20:53,120
accountability

646
00:20:50,240 --> 00:20:54,720
tackling issues within the um open

647
00:20:53,120 --> 00:20:56,158
content recommender systems and

648
00:20:54,720 --> 00:20:57,840
algorithmic curation

649
00:20:56,159 --> 00:20:59,919
but we didn't want this specific

650
00:20:57,840 --> 00:21:01,760
category of the content to be included

651
00:20:59,919 --> 00:21:03,919
within the scope of the essay

652
00:21:01,760 --> 00:21:05,760
this is due to the fact that vaguely

653
00:21:03,919 --> 00:21:06,720
defined terms that find their way into

654
00:21:05,760 --> 00:21:09,440
legislation

655
00:21:06,720 --> 00:21:10,559
always lead to human rights abuse in the

656
00:21:09,440 --> 00:21:12,799
future

657
00:21:10,559 --> 00:21:15,120
i could give you examples from europe

658
00:21:12,799 --> 00:21:17,918
such as the concept of online harms

659
00:21:15,120 --> 00:21:19,439
within the uk but also as a global

660
00:21:17,919 --> 00:21:21,200
organizations both of us

661
00:21:19,440 --> 00:21:23,120
we actually often see how wake

662
00:21:21,200 --> 00:21:25,440
terminology can quickly lead

663
00:21:23,120 --> 00:21:27,678
to even over criminalization of speech

664
00:21:25,440 --> 00:21:30,080
or suppressing the descent

665
00:21:27,679 --> 00:21:31,679
now if we go back to harmonize notice an

666
00:21:30,080 --> 00:21:32,960
action procedure what that actually

667
00:21:31,679 --> 00:21:35,280
means in practice

668
00:21:32,960 --> 00:21:37,280
as christoph already mentioned europe

669
00:21:35,280 --> 00:21:39,120
has so-called conditional model of

670
00:21:37,280 --> 00:21:41,520
intermediate reliability

671
00:21:39,120 --> 00:21:43,520
which is being provided already and

672
00:21:41,520 --> 00:21:45,200
established by the initial legal regime

673
00:21:43,520 --> 00:21:47,600
which is the e-commerce directive under

674
00:21:45,200 --> 00:21:50,960
article 14 of e-commerce directive

675
00:21:47,600 --> 00:21:52,879
which actually uh states that um

676
00:21:50,960 --> 00:21:54,400
unless the platform holds the actual

677
00:21:52,880 --> 00:21:55,440
knowledge and according to the wording

678
00:21:54,400 --> 00:21:57,840
of the essay now

679
00:21:55,440 --> 00:21:59,600
it's the actual knowledge or awareness

680
00:21:57,840 --> 00:22:01,360
about the presence of illegal content on

681
00:21:59,600 --> 00:22:03,600
their platform

682
00:22:01,360 --> 00:22:04,719
they cannot be held liable for such a

683
00:22:03,600 --> 00:22:07,039
content

684
00:22:04,720 --> 00:22:07,760
now we were asking for a harmonized

685
00:22:07,039 --> 00:22:09,440
procedure

686
00:22:07,760 --> 00:22:11,360
regarding notice and action across the

687
00:22:09,440 --> 00:22:13,039
eu for a while precisely

688
00:22:11,360 --> 00:22:14,399
because we wanted to see reinforced

689
00:22:13,039 --> 00:22:16,720
legal certainty

690
00:22:14,400 --> 00:22:17,919
lack of legal certainty often translated

691
00:22:16,720 --> 00:22:19,919
into over-removal

692
00:22:17,919 --> 00:22:22,640
of even legitimate content from the

693
00:22:19,919 --> 00:22:25,440
platforms with no public scrutiny

694
00:22:22,640 --> 00:22:26,559
um dsa does a good job it's a good

695
00:22:25,440 --> 00:22:28,000
starting point that

696
00:22:26,559 --> 00:22:29,840
actually it tries to attempt to

697
00:22:28,000 --> 00:22:30,720
establish such a harmonized procedure

698
00:22:29,840 --> 00:22:33,039
but it's still

699
00:22:30,720 --> 00:22:34,159
lacking behind on many aspects that we

700
00:22:33,039 --> 00:22:35,760
consider important

701
00:22:34,159 --> 00:22:37,679
in order to strengthen protection of

702
00:22:35,760 --> 00:22:39,440
fundamental rights of users

703
00:22:37,679 --> 00:22:41,360
one of them is for instance that

704
00:22:39,440 --> 00:22:41,679
harmonized notice and action procedure

705
00:22:41,360 --> 00:22:43,918
as

706
00:22:41,679 --> 00:22:44,960
envisioned by dsa is not specifically

707
00:22:43,919 --> 00:22:47,280
tailored to

708
00:22:44,960 --> 00:22:49,039
different types of uh categories of

709
00:22:47,280 --> 00:22:51,760
user-generated content

710
00:22:49,039 --> 00:22:53,520
and as we know there are some or many

711
00:22:51,760 --> 00:22:54,000
categories of content that are deeply

712
00:22:53,520 --> 00:22:57,120
context

713
00:22:54,000 --> 00:22:59,200
dependent linked to the historical

714
00:22:57,120 --> 00:23:00,639
and socio-political context of member

715
00:22:59,200 --> 00:23:03,200
state in question

716
00:23:00,640 --> 00:23:05,200
um and due to their of due to their

717
00:23:03,200 --> 00:23:07,679
reliance on automated measures that are

718
00:23:05,200 --> 00:23:09,440
usually context blind we are worried

719
00:23:07,679 --> 00:23:11,600
that if notice an action doesn't reflect

720
00:23:09,440 --> 00:23:13,760
this aspect in any further ways

721
00:23:11,600 --> 00:23:15,760
we will end up again with over removals

722
00:23:13,760 --> 00:23:18,080
of the content

723
00:23:15,760 --> 00:23:19,280
what is probably another huge issue that

724
00:23:18,080 --> 00:23:21,678
we are currently lacking

725
00:23:19,280 --> 00:23:23,600
in the draft even though dsa is trying

726
00:23:21,679 --> 00:23:25,280
to create a proper appeal and

727
00:23:23,600 --> 00:23:28,480
enforcement mechanisms

728
00:23:25,280 --> 00:23:29,120
um and also appeal mechanisms for users

729
00:23:28,480 --> 00:23:30,799
and different

730
00:23:29,120 --> 00:23:32,959
alternative dispute settlement that the

731
00:23:30,799 --> 00:23:36,240
law draft currently contains

732
00:23:32,960 --> 00:23:38,400
there is no possibility for a

733
00:23:36,240 --> 00:23:39,280
content provider so a user that uploads

734
00:23:38,400 --> 00:23:42,320
the filter

735
00:23:39,280 --> 00:23:44,960
uh sorry that was a nice uh

736
00:23:42,320 --> 00:23:46,960
friday and sleep there um the youth for

737
00:23:44,960 --> 00:23:49,760
a user that actually upload the content

738
00:23:46,960 --> 00:23:50,159
to appeal to that to directly actually

739
00:23:49,760 --> 00:23:52,480
uh

740
00:23:50,159 --> 00:23:54,480
use the counter notification about that

741
00:23:52,480 --> 00:23:55,200
notified content that belongs to that

742
00:23:54,480 --> 00:23:57,760
user

743
00:23:55,200 --> 00:23:59,520
nor platforms are obliged to actually

744
00:23:57,760 --> 00:24:01,679
send a notification to a user

745
00:23:59,520 --> 00:24:04,320
prior to any action that is being taken

746
00:24:01,679 --> 00:24:07,440
against that particular notified content

747
00:24:04,320 --> 00:24:09,200
these are for us a procedural safeguards

748
00:24:07,440 --> 00:24:10,480
for fairness that users should have

749
00:24:09,200 --> 00:24:12,960
and currently they are not being

750
00:24:10,480 --> 00:24:15,919
reflected in the draft

751
00:24:12,960 --> 00:24:18,240
um however this is a good start and it's

752
00:24:15,919 --> 00:24:20,080
something that we were pushing for but i

753
00:24:18,240 --> 00:24:21,360
think there are many more aspects that

754
00:24:20,080 --> 00:24:23,600
these notice and action

755
00:24:21,360 --> 00:24:24,399
procedures will need to contain in order

756
00:24:23,600 --> 00:24:27,760
to truly

757
00:24:24,400 --> 00:24:30,000
put users at first now

758
00:24:27,760 --> 00:24:31,840
the notice and action procedure is

759
00:24:30,000 --> 00:24:33,520
mainly focusing on the illegal content

760
00:24:31,840 --> 00:24:35,279
but there are ways in the draft where

761
00:24:33,520 --> 00:24:37,120
potentially harmful content

762
00:24:35,279 --> 00:24:38,720
which is still legal so the content that

763
00:24:37,120 --> 00:24:39,840
actually violates the terms of service

764
00:24:38,720 --> 00:24:41,520
of platforms

765
00:24:39,840 --> 00:24:43,039
is being mentioned throughout the draft

766
00:24:41,520 --> 00:24:45,200
so for us it's not at the moment

767
00:24:43,039 --> 00:24:46,480
exactly clear how that will work in

768
00:24:45,200 --> 00:24:49,760
practice

769
00:24:46,480 --> 00:24:52,880
uh so that's why we

770
00:24:49,760 --> 00:24:54,799
we often use this phrase that is also

771
00:24:52,880 --> 00:24:57,039
put on the slide good intention with

772
00:24:54,799 --> 00:24:58,639
imperfect solutions

773
00:24:57,039 --> 00:25:00,240
however i want to emphasize again that

774
00:24:58,640 --> 00:25:02,240
this is just the beginning and we will

775
00:25:00,240 --> 00:25:03,840
still have time and space to work very

776
00:25:02,240 --> 00:25:06,640
hard on this

777
00:25:03,840 --> 00:25:08,320
uh another kind of novel aspect that dsa

778
00:25:06,640 --> 00:25:08,799
actually brings about is already

779
00:25:08,320 --> 00:25:11,200
mentioned

780
00:25:08,799 --> 00:25:13,360
good samaritan clause and i tend to call

781
00:25:11,200 --> 00:25:15,840
it the eu model or eu

782
00:25:13,360 --> 00:25:18,158
version of good samaritan clause good

783
00:25:15,840 --> 00:25:20,799
samaritan clause originates in section

784
00:25:18,159 --> 00:25:22,480
230 of communication decency act as

785
00:25:20,799 --> 00:25:25,600
christophe already mentioned

786
00:25:22,480 --> 00:25:27,440
but uh within the european realm it goes

787
00:25:25,600 --> 00:25:29,678
hand in hand with this conditional model

788
00:25:27,440 --> 00:25:31,440
of liability which is being preserved

789
00:25:29,679 --> 00:25:33,600
within the dsa

790
00:25:31,440 --> 00:25:35,760
legal draft that was also one of our

791
00:25:33,600 --> 00:25:37,120
main ask to preserve this conditional

792
00:25:35,760 --> 00:25:38,799
model of liability

793
00:25:37,120 --> 00:25:41,120
and it's great that this time european

794
00:25:38,799 --> 00:25:42,799
commission really listened

795
00:25:41,120 --> 00:25:44,399
why we consider the good summer written

796
00:25:42,799 --> 00:25:47,120
clause being important

797
00:25:44,400 --> 00:25:48,799
uh in the past when such a security was

798
00:25:47,120 --> 00:25:50,799
an enshrined in the law

799
00:25:48,799 --> 00:25:52,400
uh but it was just somehow wiggly

800
00:25:50,799 --> 00:25:54,080
promised to the commission that if the

801
00:25:52,400 --> 00:25:57,919
platform will proactively

802
00:25:54,080 --> 00:25:59,678
uh you know uh deploy measures uh to

803
00:25:57,919 --> 00:26:01,919
fight against the spread of illegal

804
00:25:59,679 --> 00:26:04,159
content they won't be held liable

805
00:26:01,919 --> 00:26:06,400
without acknowledging that through such

806
00:26:04,159 --> 00:26:08,080
use of so-called proactive measures the

807
00:26:06,400 --> 00:26:09,760
platform could in theory

808
00:26:08,080 --> 00:26:11,840
gain the actual knowledge about the

809
00:26:09,760 --> 00:26:12,720
existence of such a type of content on

810
00:26:11,840 --> 00:26:14,639
its platform

811
00:26:12,720 --> 00:26:15,760
which would immediately trigger legal

812
00:26:14,640 --> 00:26:17,919
liability

813
00:26:15,760 --> 00:26:19,760
this threat of liability often pushed

814
00:26:17,919 --> 00:26:21,919
platforms to the corner so they would

815
00:26:19,760 --> 00:26:24,158
rather remove the content very quickly

816
00:26:21,919 --> 00:26:26,240
than to face uh more serious

817
00:26:24,159 --> 00:26:29,120
consequences later on

818
00:26:26,240 --> 00:26:30,880
uh that's why we see the importance

819
00:26:29,120 --> 00:26:33,120
within the good summer written clause or

820
00:26:30,880 --> 00:26:33,679
the european model of good samaritan

821
00:26:33,120 --> 00:26:35,840
clause

822
00:26:33,679 --> 00:26:38,640
and we are glad that it's currently

823
00:26:35,840 --> 00:26:40,879
being the part of the draft

824
00:26:38,640 --> 00:26:41,919
one of the biggest downfalls or one of

825
00:26:40,880 --> 00:26:44,960
the biggest

826
00:26:41,919 --> 00:26:48,159
uh disappointments when dsa finally

827
00:26:44,960 --> 00:26:50,400
uh came out on the 15th of december

828
00:26:48,159 --> 00:26:52,480
for us was to see that it's still online

829
00:26:50,400 --> 00:26:54,400
platforms that will remain in charge

830
00:26:52,480 --> 00:26:56,400
when it comes to assessing the legality

831
00:26:54,400 --> 00:26:57,760
of the content and deciding

832
00:26:56,400 --> 00:26:59,840
what content should be actually

833
00:26:57,760 --> 00:27:02,320
restricted and removed from a platform

834
00:26:59,840 --> 00:27:04,959
and what should be available

835
00:27:02,320 --> 00:27:05,678
we often emphasize that it's very

836
00:27:04,960 --> 00:27:08,240
important

837
00:27:05,679 --> 00:27:09,200
that uh the legality of the content is

838
00:27:08,240 --> 00:27:11,200
being uh

839
00:27:09,200 --> 00:27:12,480
assessed by the independent judicial

840
00:27:11,200 --> 00:27:14,320
authorities

841
00:27:12,480 --> 00:27:16,880
as in line with their rule of law

842
00:27:14,320 --> 00:27:19,279
principles we also do understand that

843
00:27:16,880 --> 00:27:20,480
that such a solution creates a big

844
00:27:19,279 --> 00:27:23,039
burden on the

845
00:27:20,480 --> 00:27:24,559
judicial structure of member states many

846
00:27:23,039 --> 00:27:26,480
member states see that as a very

847
00:27:24,559 --> 00:27:27,200
expensive solutions they don't always

848
00:27:26,480 --> 00:27:29,919
want to

849
00:27:27,200 --> 00:27:30,960
create a special network of course or

850
00:27:29,919 --> 00:27:34,000
e-course or

851
00:27:30,960 --> 00:27:35,919
other forms of judicial

852
00:27:34,000 --> 00:27:37,200
review of the illegal or allegedly

853
00:27:35,919 --> 00:27:39,679
illegal content

854
00:27:37,200 --> 00:27:40,559
but we still wanted to see more public

855
00:27:39,679 --> 00:27:42,480
scrutiny

856
00:27:40,559 --> 00:27:44,879
because for us this is truly just a

857
00:27:42,480 --> 00:27:45,440
reaffirmation of already existing status

858
00:27:44,880 --> 00:27:47,360
quo

859
00:27:45,440 --> 00:27:49,279
as at the moment under many

860
00:27:47,360 --> 00:27:50,320
jurisdictions within the eu and in the

861
00:27:49,279 --> 00:27:52,080
eu itself

862
00:27:50,320 --> 00:27:54,240
it's still online platform that will

863
00:27:52,080 --> 00:27:56,080
call the final shots

864
00:27:54,240 --> 00:27:57,440
uh what on the other hand is a positive

865
00:27:56,080 --> 00:27:59,760
outcome that we were also

866
00:27:57,440 --> 00:28:02,159
hardly pushing for are uh the

867
00:27:59,760 --> 00:28:04,559
requirements for meaningful transparency

868
00:28:02,159 --> 00:28:06,399
so to understand better what platforms

869
00:28:04,559 --> 00:28:07,440
actually do with the individual pieces

870
00:28:06,399 --> 00:28:09,840
of content

871
00:28:07,440 --> 00:28:10,559
that are being shared on these platforms

872
00:28:09,840 --> 00:28:13,039
and

873
00:28:10,559 --> 00:28:15,039
and how actually transparency can then

874
00:28:13,039 --> 00:28:17,120
ultimately empower user

875
00:28:15,039 --> 00:28:19,120
now i want to emphasize this because

876
00:28:17,120 --> 00:28:20,799
this is still ongoing debate

877
00:28:19,120 --> 00:28:22,320
and we will touch upon those issues in a

878
00:28:20,799 --> 00:28:24,480
minute but uh

879
00:28:22,320 --> 00:28:27,279
we don't see transparency as a silver

880
00:28:24,480 --> 00:28:29,600
bullet to the issues such as

881
00:28:27,279 --> 00:28:30,480
amplification of potentially harmful

882
00:28:29,600 --> 00:28:32,959
content

883
00:28:30,480 --> 00:28:34,799
or in general that transparency will be

884
00:28:32,960 --> 00:28:37,440
enough to actually hold platforms

885
00:28:34,799 --> 00:28:39,120
accountable absolutely not it will never

886
00:28:37,440 --> 00:28:41,120
be enough but it's a

887
00:28:39,120 --> 00:28:42,158
precondition for us to actually seek

888
00:28:41,120 --> 00:28:44,799
such solutions

889
00:28:42,159 --> 00:28:47,039
uh in the future the sa contains

890
00:28:44,799 --> 00:28:48,320
specific requirements for transparency

891
00:28:47,039 --> 00:28:50,480
as i already mentioned

892
00:28:48,320 --> 00:28:52,399
a set of requirements that will be

893
00:28:50,480 --> 00:28:53,279
applicable largely to all online

894
00:28:52,399 --> 00:28:54,959
platforms

895
00:28:53,279 --> 00:28:56,720
and then still specific set of

896
00:28:54,960 --> 00:28:58,880
requirements on the top of it

897
00:28:56,720 --> 00:29:00,880
that will be applicable only to very

898
00:28:58,880 --> 00:29:02,080
large online platforms so the online

899
00:29:00,880 --> 00:29:04,159
gatekeepers

900
00:29:02,080 --> 00:29:05,918
we appreciate the effort we see that the

901
00:29:04,159 --> 00:29:07,679
list is very promising

902
00:29:05,919 --> 00:29:09,039
but we still think it could be more

903
00:29:07,679 --> 00:29:11,840
ambitious both

904
00:29:09,039 --> 00:29:13,440
uh eff and access now put forward a

905
00:29:11,840 --> 00:29:15,039
specific set of requirements for

906
00:29:13,440 --> 00:29:17,360
meaningful transparency

907
00:29:15,039 --> 00:29:18,080
that are in our positions and so did

908
00:29:17,360 --> 00:29:20,799
edri and

909
00:29:18,080 --> 00:29:22,720
other civil society or digital rights

910
00:29:20,799 --> 00:29:25,440
activists in this space

911
00:29:22,720 --> 00:29:26,640
um and final point that i'm going to

912
00:29:25,440 --> 00:29:28,320
make is the

913
00:29:26,640 --> 00:29:30,399
so-called pandora box of online

914
00:29:28,320 --> 00:29:32,080
targeting and recommender systems

915
00:29:30,399 --> 00:29:33,760
why do i refer to these two as

916
00:29:32,080 --> 00:29:37,360
pandorabox

917
00:29:33,760 --> 00:29:39,679
when a european parliament published its

918
00:29:37,360 --> 00:29:40,959
initiative reports on the essay there

919
00:29:39,679 --> 00:29:43,600
are two reports

920
00:29:40,960 --> 00:29:44,159
one being a table by yuri committee and

921
00:29:43,600 --> 00:29:46,799
another

922
00:29:44,159 --> 00:29:49,679
one another one by inco especially the

923
00:29:46,799 --> 00:29:51,760
yuri report contained paragraph 17

924
00:29:49,679 --> 00:29:53,360
which calls out for a better regulation

925
00:29:51,760 --> 00:29:54,480
of online targeting and online

926
00:29:53,360 --> 00:29:56,799
advertisements

927
00:29:54,480 --> 00:29:58,159
and specifically calling for a ban of

928
00:29:56,799 --> 00:30:00,000
online targeting and

929
00:29:58,159 --> 00:30:01,360
including the face out that will then

930
00:30:00,000 --> 00:30:04,480
lead to a ban

931
00:30:01,360 --> 00:30:04,799
we supported this paragraph which at the

932
00:30:04,480 --> 00:30:07,039
end

933
00:30:04,799 --> 00:30:08,240
was voted for and is the part of the

934
00:30:07,039 --> 00:30:10,799
final report

935
00:30:08,240 --> 00:30:12,720
nevertheless we also do understand that

936
00:30:10,799 --> 00:30:14,799
this wording of the article has to be

937
00:30:12,720 --> 00:30:17,039
more nuanced in the future

938
00:30:14,799 --> 00:30:18,320
uh before i go into the details there i

939
00:30:17,039 --> 00:30:20,720
just want to say that

940
00:30:18,320 --> 00:30:22,639
this part has never made it to the essay

941
00:30:20,720 --> 00:30:24,799
so there is no ban on online targeting

942
00:30:22,640 --> 00:30:26,960
or online advertisement of any sorts

943
00:30:24,799 --> 00:30:29,039
which to us to some extent it was

944
00:30:26,960 --> 00:30:32,000
certainly disappointing too

945
00:30:29,039 --> 00:30:33,919
we specifically would call for a much

946
00:30:32,000 --> 00:30:35,360
more stricter approach when it comes to

947
00:30:33,919 --> 00:30:38,320
behavioral targeting

948
00:30:35,360 --> 00:30:39,199
as well as cross-site tracking of online

949
00:30:38,320 --> 00:30:42,000
users

950
00:30:39,200 --> 00:30:43,760
and but unfortunately and as we

951
00:30:42,000 --> 00:30:45,200
eventually also heard from commissioner

952
00:30:43,760 --> 00:30:48,320
west again that was simply

953
00:30:45,200 --> 00:30:50,399
like a will or maybe too much pressure

954
00:30:48,320 --> 00:30:52,399
from other law lobbyists in brussels

955
00:30:50,399 --> 00:30:54,879
and this provision never found its way

956
00:30:52,399 --> 00:30:56,959
uh to the final draft of the sa

957
00:30:54,880 --> 00:30:58,559
that's the current state of art we will

958
00:30:56,960 --> 00:31:00,799
see what we will manage to achieve

959
00:30:58,559 --> 00:31:02,320
once the dsa will enter the european

960
00:31:00,799 --> 00:31:05,760
parliament

961
00:31:02,320 --> 00:31:07,439
and finally uh the law also contains a

962
00:31:05,760 --> 00:31:10,399
specific a provision on

963
00:31:07,440 --> 00:31:12,159
recommender system so uh the way how the

964
00:31:10,399 --> 00:31:15,600
content is being distributed

965
00:31:12,159 --> 00:31:18,559
uh across platform and uh how the data

966
00:31:15,600 --> 00:31:20,559
of users are being abused for such a

967
00:31:18,559 --> 00:31:23,120
distribution and personalization of user

968
00:31:20,559 --> 00:31:24,879
generated content

969
00:31:23,120 --> 00:31:26,799
in both cases whether it's online

970
00:31:24,880 --> 00:31:27,760
targeting and recommender systems within

971
00:31:26,799 --> 00:31:29,918
the dsa

972
00:31:27,760 --> 00:31:31,480
the essay goes as far as the

973
00:31:29,919 --> 00:31:33,360
transparency requirements the

974
00:31:31,480 --> 00:31:35,679
explainability

975
00:31:33,360 --> 00:31:37,678
but it does very little for returning

976
00:31:35,679 --> 00:31:38,320
that control and empowerment back to the

977
00:31:37,679 --> 00:31:40,799
user

978
00:31:38,320 --> 00:31:43,039
so how use whether user can obtain or

979
00:31:40,799 --> 00:31:44,000
opt out from these algorithmic curation

980
00:31:43,039 --> 00:31:46,640
models

981
00:31:44,000 --> 00:31:48,480
uh how it can actually optimize if they

982
00:31:46,640 --> 00:31:50,640
decide to optimize it

983
00:31:48,480 --> 00:31:51,760
all of that is at the moment very much

984
00:31:50,640 --> 00:31:54,799
left outside

985
00:31:51,760 --> 00:31:57,279
of the scope of the sa and so does the

986
00:31:54,799 --> 00:31:58,559
issue of interoperability which is

987
00:31:57,279 --> 00:32:01,039
definitely a

988
00:31:58,559 --> 00:32:02,320
one of the key issues being currently

989
00:32:01,039 --> 00:32:05,519
discussed and main

990
00:32:02,320 --> 00:32:07,519
kind of possible hopes in the future for

991
00:32:05,519 --> 00:32:09,360
returning that control and empowerment

992
00:32:07,519 --> 00:32:11,279
back to the user and i keep repeating

993
00:32:09,360 --> 00:32:13,439
this as a mantra but it's truly

994
00:32:11,279 --> 00:32:14,880
the main driving force behind all our

995
00:32:13,440 --> 00:32:17,200
initiatives and work

996
00:32:14,880 --> 00:32:18,720
we do in these fields so the user and

997
00:32:17,200 --> 00:32:20,399
their fundamental rights

998
00:32:18,720 --> 00:32:21,919
and on that note i would like to hand

999
00:32:20,399 --> 00:32:24,479
over back to chris who will

1000
00:32:21,919 --> 00:32:26,559
explain the issue of interoperability

1001
00:32:24,480 --> 00:32:27,120
and how to actually empower you as a

1002
00:32:26,559 --> 00:32:28,720
user

1003
00:32:27,120 --> 00:32:30,879
and to strengthen the protection of

1004
00:32:28,720 --> 00:32:33,600
fundamental rights further chris

1005
00:32:30,880 --> 00:32:33,600
it's yours now

1006
00:32:35,519 --> 00:32:39,120
thank you um

1007
00:32:36,750 --> 00:32:41,840
[Music]

1008
00:32:39,120 --> 00:32:44,639
i think we all know i feel that the

1009
00:32:41,840 --> 00:32:46,480
internet has seen better times

1010
00:32:44,640 --> 00:32:48,640
if you look back over the last 20 years

1011
00:32:46,480 --> 00:32:50,480
we have seen that a transformation was

1012
00:32:48,640 --> 00:32:53,440
going on from an open internet

1013
00:32:50,480 --> 00:32:54,960
towards a more closed one monopolization

1014
00:32:53,440 --> 00:32:56,000
big platforms have built entire

1015
00:32:54,960 --> 00:32:58,559
ecosystems and

1016
00:32:56,000 --> 00:33:00,159
it seems that they alone decide who gets

1017
00:32:58,559 --> 00:33:02,080
to use them

1018
00:33:00,159 --> 00:33:04,000
those platforms have strong network

1019
00:33:02,080 --> 00:33:05,678
effects that have pushed platforms

1020
00:33:04,000 --> 00:33:07,360
or those platforms into gatekeeper

1021
00:33:05,679 --> 00:33:09,600
position which made it so easy for them

1022
00:33:07,360 --> 00:33:11,360
to avoid any real competition

1023
00:33:09,600 --> 00:33:13,918
this especially is especially true when

1024
00:33:11,360 --> 00:33:16,000
we think of social media platforms

1025
00:33:13,919 --> 00:33:17,519
this year we celebrate the 20th birthday

1026
00:33:16,000 --> 00:33:19,200
of the e-commerce directive that alicia

1027
00:33:17,519 --> 00:33:20,559
had mentioned the internet bill that

1028
00:33:19,200 --> 00:33:23,200
will now be replaced by the digital

1029
00:33:20,559 --> 00:33:24,080
services act and we believe it's a very

1030
00:33:23,200 --> 00:33:26,080
good time now to

1031
00:33:24,080 --> 00:33:27,918
think and make a choice should we give

1032
00:33:26,080 --> 00:33:28,879
even more power to the big platforms

1033
00:33:27,919 --> 00:33:30,720
that have created

1034
00:33:28,880 --> 00:33:32,480
a lot of the mess in the first place or

1035
00:33:30,720 --> 00:33:35,440
should we give the power to the users

1036
00:33:32,480 --> 00:33:37,519
give the power back to the people for us

1037
00:33:35,440 --> 00:33:39,600
the answer is clear

1038
00:33:37,519 --> 00:33:41,840
big tech companies already employ a wide

1039
00:33:39,600 --> 00:33:44,879
array of technical measures they monitor

1040
00:33:41,840 --> 00:33:46,799
they remove they disrespect user privacy

1041
00:33:44,880 --> 00:33:49,679
and the idea to turn them into the

1042
00:33:46,799 --> 00:33:51,918
internet police with a special license

1043
00:33:49,679 --> 00:33:53,679
of censoring the speech of users will

1044
00:33:51,919 --> 00:33:56,720
only solidify the dominance

1045
00:33:53,679 --> 00:33:57,039
so we wouldn't like that what we like is

1046
00:33:56,720 --> 00:33:58,799
to

1047
00:33:57,039 --> 00:34:00,559
put users in charge over the online

1048
00:33:58,799 --> 00:34:03,679
experience

1049
00:34:00,559 --> 00:34:05,279
users should if we had a say choose for

1050
00:34:03,679 --> 00:34:07,039
themselves which kind of content they

1051
00:34:05,279 --> 00:34:09,119
can see what services they can use to

1052
00:34:07,039 --> 00:34:10,719
talk to their friends and families

1053
00:34:09,119 --> 00:34:12,240
and we believe it's perhaps time to

1054
00:34:10,719 --> 00:34:14,399
break up those

1055
00:34:12,239 --> 00:34:16,158
silos those big platforms have become to

1056
00:34:14,399 --> 00:34:18,480
end the dominance of the data

1057
00:34:16,159 --> 00:34:20,000
one element to achieve this would be to

1058
00:34:18,480 --> 00:34:22,240
tackle the targeted ads industry

1059
00:34:20,000 --> 00:34:23,918
uh silicek mentioned it perhaps to give

1060
00:34:22,239 --> 00:34:26,239
an actual right to users not to be

1061
00:34:23,918 --> 00:34:28,000
subject to targeted ads

1062
00:34:26,239 --> 00:34:29,520
or to give more choice to use to decide

1063
00:34:28,000 --> 00:34:31,679
which content they would like to see

1064
00:34:29,520 --> 00:34:32,879
or not to see in the digital services

1065
00:34:31,679 --> 00:34:34,639
act the commission

1066
00:34:32,879 --> 00:34:36,719
went for transparency when it comes to

1067
00:34:34,639 --> 00:34:38,480
ads and better options for users users

1068
00:34:36,719 --> 00:34:41,439
to decide on recommended content which

1069
00:34:38,480 --> 00:34:43,599
is a stuff we can work with that

1070
00:34:41,440 --> 00:34:45,119
another important element to achieve

1071
00:34:43,599 --> 00:34:46,720
user autonomy over data is

1072
00:34:45,119 --> 00:34:48,159
interoperability

1073
00:34:46,719 --> 00:34:50,319
if the european union really wants to

1074
00:34:48,159 --> 00:34:51,119
break the power of those data-driven

1075
00:34:50,320 --> 00:34:52,879
platforms

1076
00:34:51,119 --> 00:34:54,960
that monopolize the internet it needs

1077
00:34:52,879 --> 00:34:56,799
regulations that enables users to be in

1078
00:34:54,960 --> 00:34:58,480
control over the data

1079
00:34:56,800 --> 00:35:00,720
we believe that users should be able to

1080
00:34:58,480 --> 00:35:02,640
access data to download data to move

1081
00:35:00,720 --> 00:35:04,959
manipulate their data as they see

1082
00:35:02,640 --> 00:35:07,040
fit and part of that control is to board

1083
00:35:04,960 --> 00:35:08,800
data from one place to another

1084
00:35:07,040 --> 00:35:11,200
but data portability which we have under

1085
00:35:08,800 --> 00:35:12,800
the gdpr is not good enough

1086
00:35:11,200 --> 00:35:14,480
and we see from the gdpr that it's not

1087
00:35:12,800 --> 00:35:16,480
working in practice

1088
00:35:14,480 --> 00:35:18,560
users should be able to communicate with

1089
00:35:16,480 --> 00:35:20,240
friends across platform boundaries or to

1090
00:35:18,560 --> 00:35:20,640
be able to follow their favorite content

1091
00:35:20,240 --> 00:35:22,479
across

1092
00:35:20,640 --> 00:35:24,160
different platforms without having to

1093
00:35:22,480 --> 00:35:26,880
create several accounts

1094
00:35:24,160 --> 00:35:28,879
or to put it in other terms if you're

1095
00:35:26,880 --> 00:35:30,000
upset with the absence of privacy and

1096
00:35:28,880 --> 00:35:32,480
facebook or how the content

1097
00:35:30,000 --> 00:35:34,000
is moderated on facebook you should be

1098
00:35:32,480 --> 00:35:36,400
able to just take your data with you

1099
00:35:34,000 --> 00:35:37,920
using portability options and move to an

1100
00:35:36,400 --> 00:35:38,800
alternative platform that is a better

1101
00:35:37,920 --> 00:35:40,640
fit

1102
00:35:38,800 --> 00:35:42,079
and this without losing touch with your

1103
00:35:40,640 --> 00:35:44,560
friends who stay behind who have not

1104
00:35:42,079 --> 00:35:46,960
left the incumbent big platform

1105
00:35:44,560 --> 00:35:48,000
so what we did for digital services act

1106
00:35:46,960 --> 00:35:50,000
is to argue for

1107
00:35:48,000 --> 00:35:51,680
mandatory interoperability options that

1108
00:35:50,000 --> 00:35:54,560
would force facebook to maintain

1109
00:35:51,680 --> 00:35:55,598
apis that let users on other platforms

1110
00:35:54,560 --> 00:35:58,720
exchange

1111
00:35:55,599 --> 00:36:00,800
messages and content with facebook users

1112
00:35:58,720 --> 00:36:02,240
however however if you look in the dsa

1113
00:36:00,800 --> 00:36:04,480
we see that the commission completely

1114
00:36:02,240 --> 00:36:06,319
missed the mark on interoperability

1115
00:36:04,480 --> 00:36:07,680
which is supposed to be dealt with by a

1116
00:36:06,320 --> 00:36:09,920
related legal act and now it gets

1117
00:36:07,680 --> 00:36:13,839
complicated it's the digital markets act

1118
00:36:09,920 --> 00:36:15,920
the dma another beautiful acronym

1119
00:36:13,839 --> 00:36:18,078
the digital market act wants to tackle

1120
00:36:15,920 --> 00:36:19,760
certain harmful businesses practices by

1121
00:36:18,079 --> 00:36:21,920
those gatekeeper platforms they're very

1122
00:36:19,760 --> 00:36:24,320
large tech companies that control

1123
00:36:21,920 --> 00:36:26,000
what is called core services a core

1124
00:36:24,320 --> 00:36:27,359
service is a search engine a social

1125
00:36:26,000 --> 00:36:29,920
networking service a

1126
00:36:27,359 --> 00:36:32,480
messaging services operating systems and

1127
00:36:29,920 --> 00:36:34,560
online intermediation services

1128
00:36:32,480 --> 00:36:36,320
like think of how amazon controls access

1129
00:36:34,560 --> 00:36:38,160
to customers for merchants that sell on

1130
00:36:36,320 --> 00:36:40,480
its platforms or how the android and

1131
00:36:38,160 --> 00:36:42,879
iphone app stores

1132
00:36:40,480 --> 00:36:44,480
us chokepoints and delivering mobile

1133
00:36:42,880 --> 00:36:45,839
software

1134
00:36:44,480 --> 00:36:47,680
there are many things we like in the new

1135
00:36:45,839 --> 00:36:48,720
proposal that proposal on the digital

1136
00:36:47,680 --> 00:36:51,279
markets act

1137
00:36:48,720 --> 00:36:52,240
for example there's a ban on mixing data

1138
00:36:51,280 --> 00:36:54,560
in there

1139
00:36:52,240 --> 00:36:56,399
the dma wants to ban gatekeepers from

1140
00:36:54,560 --> 00:36:58,799
mixing data from data brokers with the

1141
00:36:56,400 --> 00:37:02,079
data they collect on the customers

1142
00:36:58,800 --> 00:37:03,680
another rule is to ban cross tying so

1143
00:37:02,079 --> 00:37:05,520
the practice that

1144
00:37:03,680 --> 00:37:07,680
end users must sign up for ancillary

1145
00:37:05,520 --> 00:37:08,079
services so you should be able to use

1146
00:37:07,680 --> 00:37:09,839
android

1147
00:37:08,079 --> 00:37:12,079
without having to get an email account

1148
00:37:09,839 --> 00:37:12,799
for example we believe that this is all

1149
00:37:12,079 --> 00:37:16,079
good

1150
00:37:12,800 --> 00:37:17,599
but the dma like the dsa is very weak on

1151
00:37:16,079 --> 00:37:20,480
interoperability

1152
00:37:17,599 --> 00:37:21,040
what it does is to focus on real-time

1153
00:37:20,480 --> 00:37:23,200
data

1154
00:37:21,040 --> 00:37:25,040
ability instead so instead of having

1155
00:37:23,200 --> 00:37:26,480
interoperable services users will only

1156
00:37:25,040 --> 00:37:27,680
be able to send their data from one

1157
00:37:26,480 --> 00:37:30,560
service to another

1158
00:37:27,680 --> 00:37:31,839
like from facebook to diaspora meaning

1159
00:37:30,560 --> 00:37:33,440
that you would end up having two

1160
00:37:31,839 --> 00:37:35,759
accounts instead of one

1161
00:37:33,440 --> 00:37:37,359
or the quote corey doctor who spoke

1162
00:37:35,760 --> 00:37:39,359
yesterday already

1163
00:37:37,359 --> 00:37:41,598
users would still be subject to the

1164
00:37:39,359 --> 00:37:42,480
sprawling garbage novella of abusive

1165
00:37:41,599 --> 00:37:44,720
legalese

1166
00:37:42,480 --> 00:37:46,000
facebook lovably calls its terms of

1167
00:37:44,720 --> 00:37:49,040
service

1168
00:37:46,000 --> 00:37:52,560
we believe that this is not good enough

1169
00:37:49,040 --> 00:37:54,480
um in the last slide you see a a quote

1170
00:37:52,560 --> 00:37:56,560
from margaret vestaia

1171
00:37:54,480 --> 00:37:59,040
um who made a very good statement last

1172
00:37:56,560 --> 00:38:01,279
month that we need trustworthy services

1173
00:37:59,040 --> 00:38:02,560
fair use of data and free speech and

1174
00:38:01,280 --> 00:38:05,280
interoperable internet

1175
00:38:02,560 --> 00:38:07,040
we fully agree on that and in the next

1176
00:38:05,280 --> 00:38:08,720
months and years we will work on this to

1177
00:38:07,040 --> 00:38:10,480
actually happen

1178
00:38:08,720 --> 00:38:12,720
however you can imagine it would not be

1179
00:38:10,480 --> 00:38:14,320
easy we already see that european union

1180
00:38:12,720 --> 00:38:16,000
member states

1181
00:38:14,320 --> 00:38:18,160
followed a trend that platforms should

1182
00:38:16,000 --> 00:38:20,880
systematically check undesirable and

1183
00:38:18,160 --> 00:38:22,399
insightful content and share those data

1184
00:38:20,880 --> 00:38:23,680
with enforcement authorities which is

1185
00:38:22,400 --> 00:38:25,599
even worse

1186
00:38:23,680 --> 00:38:27,200
we see an international trend going on

1187
00:38:25,599 --> 00:38:28,640
to move away from the immunity of

1188
00:38:27,200 --> 00:38:30,839
platform for user content

1189
00:38:28,640 --> 00:38:32,960
towards a more active stance of those

1190
00:38:30,839 --> 00:38:35,359
platforms and we see that

1191
00:38:32,960 --> 00:38:37,359
recent terror terror attacks have fewer

1192
00:38:35,359 --> 00:38:38,078
ideas that monitoring is a good idea and

1193
00:38:37,359 --> 00:38:41,119
that end

1194
00:38:38,079 --> 00:38:42,720
to end decryption is a problem

1195
00:38:41,119 --> 00:38:44,240
so whatever will be the result you can

1196
00:38:42,720 --> 00:38:45,520
bet the european union will want to make

1197
00:38:44,240 --> 00:38:47,598
the digital services act

1198
00:38:45,520 --> 00:38:48,960
and the digital markets act another

1199
00:38:47,599 --> 00:38:50,400
export model

1200
00:38:48,960 --> 00:38:52,640
so this time we want to have numbers

1201
00:38:50,400 --> 00:38:53,760
right in parliament and the council

1202
00:38:52,640 --> 00:38:55,759
and we want to help members of

1203
00:38:53,760 --> 00:38:58,320
parliament to press the right buttons

1204
00:38:55,760 --> 00:38:59,839
and for all this we will need your help

1205
00:38:58,320 --> 00:39:01,680
even if it means to learn yet another

1206
00:38:59,839 --> 00:39:03,200
acronym or several acronyms after the

1207
00:39:01,680 --> 00:39:05,118
gdpr

1208
00:39:03,200 --> 00:39:08,560
and that's it from our side we are

1209
00:39:05,119 --> 00:39:08,560
looking forward to the discussion thank

1210
00:39:08,880 --> 00:39:11,939
[Music]

1211
00:39:12,839 --> 00:39:18,880
you

1212
00:39:15,359 --> 00:39:21,279
okay thank you uh and um

1213
00:39:18,880 --> 00:39:22,320
and christoph um there are questions

1214
00:39:21,280 --> 00:39:25,599
from the internet

1215
00:39:22,320 --> 00:39:28,320
and the the first one is basically

1216
00:39:25,599 --> 00:39:30,320
be just hard and as you mentioned in

1217
00:39:28,320 --> 00:39:32,240
your slide sequence of the

1218
00:39:30,320 --> 00:39:33,680
copyright in the digital single market

1219
00:39:32,240 --> 00:39:37,118
with

1220
00:39:33,680 --> 00:39:39,839
both accountability and

1221
00:39:37,119 --> 00:39:41,839
liability provisions you also briefly

1222
00:39:39,839 --> 00:39:45,040
mentioned

1223
00:39:41,839 --> 00:39:46,720
i think even the evidence proposal also

1224
00:39:45,040 --> 00:39:48,720
how do all these proposals relate to

1225
00:39:46,720 --> 00:39:50,319
each other especially for a lay person

1226
00:39:48,720 --> 00:39:53,520
that is not into

1227
00:39:50,320 --> 00:39:53,520
all the brussels jargon

1228
00:39:57,290 --> 00:40:03,599
[Music]

1229
00:40:00,720 --> 00:40:04,160
unintentionally but yeah i kind of did

1230
00:40:03,599 --> 00:40:07,359
um i

1231
00:40:04,160 --> 00:40:10,399
i can start and then let you kristoff to

1232
00:40:07,359 --> 00:40:13,680
to step in um yeah that's a very

1233
00:40:10,400 --> 00:40:15,280
very good question um and this is

1234
00:40:13,680 --> 00:40:17,759
specifically due to the fact that

1235
00:40:15,280 --> 00:40:19,839
when you mention especially online

1236
00:40:17,760 --> 00:40:22,400
terrorist content regulation but also

1237
00:40:19,839 --> 00:40:23,119
a recently proposed interim regulation

1238
00:40:22,400 --> 00:40:26,319
on

1239
00:40:23,119 --> 00:40:28,640
child sexual abuse they

1240
00:40:26,319 --> 00:40:30,079
all these we call them sector

1241
00:40:28,640 --> 00:40:31,680
legislation so

1242
00:40:30,079 --> 00:40:33,440
kind of a little bit departing from

1243
00:40:31,680 --> 00:40:36,480
these horizontal approach

1244
00:40:33,440 --> 00:40:39,359
meaning um an approach that tackles all

1245
00:40:36,480 --> 00:40:41,520
categories of illegal content in one way

1246
00:40:39,359 --> 00:40:43,040
uh instead of going after specific

1247
00:40:41,520 --> 00:40:43,599
categories such as online terrorist

1248
00:40:43,040 --> 00:40:45,040
content

1249
00:40:43,599 --> 00:40:46,800
in the in the separate way so it's a

1250
00:40:45,040 --> 00:40:48,640
little bit paradoxical thing

1251
00:40:46,800 --> 00:40:50,720
uh what is currently also happening at

1252
00:40:48,640 --> 00:40:52,240
the eu level because on one hand we were

1253
00:40:50,720 --> 00:40:54,640
promised this systemic

1254
00:40:52,240 --> 00:40:56,959
regulation that will once for all

1255
00:40:54,640 --> 00:40:59,279
established harmonized approach to

1256
00:40:56,960 --> 00:41:00,000
combating illegal content online and at

1257
00:40:59,280 --> 00:41:02,720
the same time

1258
00:41:00,000 --> 00:41:04,000
which is specifically bsa the digital

1259
00:41:02,720 --> 00:41:05,439
services act

1260
00:41:04,000 --> 00:41:08,000
and at the same time we still see

1261
00:41:05,440 --> 00:41:10,079
european commission allowing for these

1262
00:41:08,000 --> 00:41:11,520
fundamental rights harmful legislative

1263
00:41:10,079 --> 00:41:14,000
proposals happening

1264
00:41:11,520 --> 00:41:15,440
in these specific sectors such as

1265
00:41:14,000 --> 00:41:17,040
proposed online terrorist content

1266
00:41:15,440 --> 00:41:20,160
regulation

1267
00:41:17,040 --> 00:41:22,000
or other legislative act seeking to

1268
00:41:20,160 --> 00:41:23,759
somehow regulate specific categories of

1269
00:41:22,000 --> 00:41:25,599
user-generated content

1270
00:41:23,760 --> 00:41:27,920
uh this is quite puzzling for us as a

1271
00:41:25,599 --> 00:41:30,640
digital rights activist too

1272
00:41:27,920 --> 00:41:32,560
very often actually and and so i would

1273
00:41:30,640 --> 00:41:33,040
maybe separate the essay from this for a

1274
00:41:32,560 --> 00:41:35,119
moment

1275
00:41:33,040 --> 00:41:36,960
and say that all of these sector

1276
00:41:35,119 --> 00:41:39,280
legislations what they have in common

1277
00:41:36,960 --> 00:41:41,280
is first of all continuing these

1278
00:41:39,280 --> 00:41:43,359
negative legislative trends that we

1279
00:41:41,280 --> 00:41:44,800
already described and that we constantly

1280
00:41:43,359 --> 00:41:46,640
observe in practice

1281
00:41:44,800 --> 00:41:49,119
such as shifting more and more

1282
00:41:46,640 --> 00:41:51,118
responsibility on online platforms

1283
00:41:49,119 --> 00:41:52,960
and at the same time what is also very

1284
00:41:51,119 --> 00:41:55,440
interesting what they have in common is

1285
00:41:52,960 --> 00:41:57,599
the legal basis that they stand on and

1286
00:41:55,440 --> 00:42:00,480
that's the legal basis that is rather

1287
00:41:57,599 --> 00:42:02,079
connected to the cooperation within the

1288
00:42:00,480 --> 00:42:04,480
digital single market

1289
00:42:02,079 --> 00:42:05,680
where even though they seek to tackle a

1290
00:42:04,480 --> 00:42:08,160
very particular

1291
00:42:05,680 --> 00:42:09,919
particular type of category of content

1292
00:42:08,160 --> 00:42:13,279
category which is manifestly

1293
00:42:09,920 --> 00:42:15,520
illegal so logically if they should

1294
00:42:13,280 --> 00:42:17,520
have that appropriate legal ground it

1295
00:42:15,520 --> 00:42:19,359
should be something more close to police

1296
00:42:17,520 --> 00:42:21,759
and judicial cooperation

1297
00:42:19,359 --> 00:42:22,640
which we don't see happening in practice

1298
00:42:21,760 --> 00:42:24,720
specifically

1299
00:42:22,640 --> 00:42:27,200
because there is this idea that

1300
00:42:24,720 --> 00:42:29,118
platforms are the best suited to decide

1301
00:42:27,200 --> 00:42:30,480
how the illegal content will be tackled

1302
00:42:29,119 --> 00:42:32,720
in the online space

1303
00:42:30,480 --> 00:42:34,480
they can be the fastest they can be the

1304
00:42:32,720 --> 00:42:36,078
most effective so they should actually

1305
00:42:34,480 --> 00:42:39,040
have that main decision making

1306
00:42:36,079 --> 00:42:40,720
powers and forced into taking those

1307
00:42:39,040 --> 00:42:42,640
responsibilities which however

1308
00:42:40,720 --> 00:42:43,680
ultimately according to the rule of law

1309
00:42:42,640 --> 00:42:46,000
principle

1310
00:42:43,680 --> 00:42:48,000
should and have to be in hands of the

1311
00:42:46,000 --> 00:42:51,040
state and public authorities

1312
00:42:48,000 --> 00:42:54,240
as preferably judicial authorities so

1313
00:42:51,040 --> 00:42:56,160
um i would say they are all

1314
00:42:54,240 --> 00:42:58,640
bad news for fundamental rights

1315
00:42:56,160 --> 00:43:01,680
protection of online users

1316
00:42:58,640 --> 00:43:03,920
civil rights organizations uh

1317
00:43:01,680 --> 00:43:05,440
all of us that are on the scope today

1318
00:43:03,920 --> 00:43:07,280
were fighting very hard

1319
00:43:05,440 --> 00:43:09,599
also against the online service content

1320
00:43:07,280 --> 00:43:10,480
regulation there was a lot of damage

1321
00:43:09,599 --> 00:43:13,040
control done

1322
00:43:10,480 --> 00:43:14,000
especially with the first report that

1323
00:43:13,040 --> 00:43:16,480
was tabled

1324
00:43:14,000 --> 00:43:18,000
by the european parliament and also now

1325
00:43:16,480 --> 00:43:19,680
during the last trilog

1326
00:43:18,000 --> 00:43:22,079
since the negotiation seems to be

1327
00:43:19,680 --> 00:43:24,399
concluded and the outcome is not great

1328
00:43:22,079 --> 00:43:26,480
it's far from ideal and i'm worried that

1329
00:43:24,400 --> 00:43:28,400
with other sector legislative attempts

1330
00:43:26,480 --> 00:43:30,720
coming from the european commission

1331
00:43:28,400 --> 00:43:32,000
we might see the same outcome it will be

1332
00:43:30,720 --> 00:43:33,520
very interesting to see

1333
00:43:32,000 --> 00:43:35,920
how that will actually then play

1334
00:43:33,520 --> 00:43:36,560
together with the digital services act

1335
00:43:35,920 --> 00:43:38,319
which is

1336
00:43:36,560 --> 00:43:40,640
trying to do the exact opposite to

1337
00:43:38,319 --> 00:43:43,680
actually fix this negative

1338
00:43:40,640 --> 00:43:44,640
uh legislative efforts that we see at

1339
00:43:43,680 --> 00:43:46,560
the eu level

1340
00:43:44,640 --> 00:43:48,240
with the sector legislation but also

1341
00:43:46,560 --> 00:43:49,040
with the member states at the national

1342
00:43:48,240 --> 00:43:51,200
level

1343
00:43:49,040 --> 00:43:53,040
um i could also mention the european

1344
00:43:51,200 --> 00:43:54,560
commission reaction to the nets to some

1345
00:43:53,040 --> 00:43:56,319
national legislative proposals but

1346
00:43:54,560 --> 00:43:59,440
christoph i will leave that to you

1347
00:43:56,319 --> 00:43:59,440
and please step in

1348
00:44:00,400 --> 00:44:05,280
i think you explained it perfectly the

1349
00:44:02,240 --> 00:44:06,959
only thing i can supplement here is that

1350
00:44:05,280 --> 00:44:09,040
if you look at this move from sector

1351
00:44:06,960 --> 00:44:11,599
legislation asylum legislation to

1352
00:44:09,040 --> 00:44:14,319
horizontal legislation now back to

1353
00:44:11,599 --> 00:44:15,680
sector legislation it's a problem it's a

1354
00:44:14,319 --> 00:44:19,119
mess

1355
00:44:15,680 --> 00:44:23,598
first the those files are very good

1356
00:44:19,119 --> 00:44:25,839
coordinated which brings in um

1357
00:44:23,599 --> 00:44:27,839
troubles for legal certainty it makes it

1358
00:44:25,839 --> 00:44:28,480
very troublesome for platforms to follow

1359
00:44:27,839 --> 00:44:30,720
up

1360
00:44:28,480 --> 00:44:31,839
and it's problematic for us for us in

1361
00:44:30,720 --> 00:44:33,598
the space

1362
00:44:31,839 --> 00:44:35,040
we are some sort of lobbyist as well

1363
00:44:33,599 --> 00:44:36,960
just for public interest

1364
00:44:35,040 --> 00:44:38,560
but if you have to deal with copyright

1365
00:44:36,960 --> 00:44:41,599
we see some with steric with

1366
00:44:38,560 --> 00:44:43,119
end-to-end encryption dsa dma and 15

1367
00:44:41,599 --> 00:44:46,240
other files that will pop up

1368
00:44:43,119 --> 00:44:48,240
content by content it's very hard to

1369
00:44:46,240 --> 00:44:49,839
manage to have the capacity ready

1370
00:44:48,240 --> 00:44:51,919
to be early in the debate and it's so

1371
00:44:49,839 --> 00:44:53,359
important to be early in the debate to

1372
00:44:51,920 --> 00:44:54,880
prevent the verse from happening and i

1373
00:44:53,359 --> 00:44:56,880
think that's a huge challenge for us to

1374
00:44:54,880 --> 00:44:58,640
that perhaps something for us to reflect

1375
00:44:56,880 --> 00:45:00,000
during the next days how can we join

1376
00:44:58,640 --> 00:45:01,680
forces better

1377
00:45:00,000 --> 00:45:02,720
in a more systematic way in order to

1378
00:45:01,680 --> 00:45:05,040
really follow up on all those

1379
00:45:02,720 --> 00:45:08,240
initiatives and that's for me a

1380
00:45:05,040 --> 00:45:12,000
a very problematic development

1381
00:45:08,240 --> 00:45:14,000
so in summary it's it's a mess

1382
00:45:12,000 --> 00:45:15,760
so it is related but we can't explain

1383
00:45:14,000 --> 00:45:17,359
how because it's such a mess

1384
00:45:15,760 --> 00:45:20,880
fair enough i have another question for

1385
00:45:17,359 --> 00:45:23,598
you elishka and that is

1386
00:45:20,880 --> 00:45:25,040
someone was asking how the proposed good

1387
00:45:23,599 --> 00:45:27,200
samaritan clause

1388
00:45:25,040 --> 00:45:28,960
works compared to how it currently works

1389
00:45:27,200 --> 00:45:30,480
in germany

1390
00:45:28,960 --> 00:45:32,400
but i think it's a bit unreasonable to

1391
00:45:30,480 --> 00:45:33,599
expect everyone to know how it works in

1392
00:45:32,400 --> 00:45:36,160
germany

1393
00:45:33,599 --> 00:45:38,240
i would rephrase it as how does this

1394
00:45:36,160 --> 00:45:40,720
proposed good samaritan clause work

1395
00:45:38,240 --> 00:45:43,680
compared to how it is now under the

1396
00:45:40,720 --> 00:45:43,680
e-commerce directive

1397
00:45:44,240 --> 00:45:50,319
thank you very much um yeah so uh

1398
00:45:47,359 --> 00:45:50,960
great question again i think the first

1399
00:45:50,319 --> 00:45:53,520
uh

1400
00:45:50,960 --> 00:45:54,079
if we put it into the context of the eu

1401
00:45:53,520 --> 00:45:56,079
law and

1402
00:45:54,079 --> 00:45:58,240
apologies that i cannot really answer

1403
00:45:56,079 --> 00:46:00,000
how you know compare the german context

1404
00:45:58,240 --> 00:46:01,680
i really don't dare to i'm not a german

1405
00:46:00,000 --> 00:46:03,040
lawyer so i wouldn't like to stop those

1406
00:46:01,680 --> 00:46:06,240
voters but

1407
00:46:03,040 --> 00:46:08,079
um first of all there is no good

1408
00:46:06,240 --> 00:46:10,000
samaritan clause per se

1409
00:46:08,079 --> 00:46:11,520
within the scope of e-commerce directive

1410
00:46:10,000 --> 00:46:14,640
it did not really

1411
00:46:11,520 --> 00:46:16,560
exist within the and i'm using the

1412
00:46:14,640 --> 00:46:18,240
the past sentence now because dsa is

1413
00:46:16,560 --> 00:46:20,960
trying to change that so

1414
00:46:18,240 --> 00:46:21,598
that level of legal uncertainty was not

1415
00:46:20,960 --> 00:46:24,079
really

1416
00:46:21,599 --> 00:46:25,359
really there for the platforms uh there

1417
00:46:24,079 --> 00:46:26,880
was the conditional model of the

1418
00:46:25,359 --> 00:46:28,240
liability which is still preserved

1419
00:46:26,880 --> 00:46:30,400
within the regulation

1420
00:46:28,240 --> 00:46:31,279
but the if we think of a good samaritan

1421
00:46:30,400 --> 00:46:33,440
clause as we know

1422
00:46:31,280 --> 00:46:35,599
it from the section 230 or let's use

1423
00:46:33,440 --> 00:46:37,200
that good samaritan clause as an example

1424
00:46:35,599 --> 00:46:39,119
because also e-commerce directive was

1425
00:46:37,200 --> 00:46:41,520
actually drafted as a response to

1426
00:46:39,119 --> 00:46:43,200
communication decency act that was the

1427
00:46:41,520 --> 00:46:44,240
legislation that puts things into

1428
00:46:43,200 --> 00:46:47,118
motions

1429
00:46:44,240 --> 00:46:48,078
um um so that's the that's the first

1430
00:46:47,119 --> 00:46:50,480
ultimate point

1431
00:46:48,079 --> 00:46:51,839
um i explained at the beginning in my

1432
00:46:50,480 --> 00:46:54,480
presentation

1433
00:46:51,839 --> 00:46:56,480
uh what was then happening in the space

1434
00:46:54,480 --> 00:46:57,359
of combating illegal content at the eu

1435
00:46:56,480 --> 00:47:00,160
level

1436
00:46:57,359 --> 00:47:01,680
um and especially i would refer to the

1437
00:47:00,160 --> 00:47:03,279
communication that the european

1438
00:47:01,680 --> 00:47:04,879
commission published i think back in

1439
00:47:03,280 --> 00:47:08,000
2018

1440
00:47:04,880 --> 00:47:09,920
where it actually encouraged and called

1441
00:47:08,000 --> 00:47:11,760
on online platforms to proactively

1442
00:47:09,920 --> 00:47:15,040
engage with

1443
00:47:11,760 --> 00:47:17,040
illegal content and use this proactive

1444
00:47:15,040 --> 00:47:19,680
measure to actually seek and adequate

1445
00:47:17,040 --> 00:47:21,920
responses to lead to illegal content

1446
00:47:19,680 --> 00:47:23,279
now to mix that with this conditional

1447
00:47:21,920 --> 00:47:25,440
model of liability

1448
00:47:23,280 --> 00:47:26,960
uh which is of course defined by the

1449
00:47:25,440 --> 00:47:29,520
obtaining actual knowledge by the

1450
00:47:26,960 --> 00:47:31,839
platform that created a perfect storm

1451
00:47:29,520 --> 00:47:34,160
that i already explained so the

1452
00:47:31,839 --> 00:47:36,240
platforms knew that they are kind of

1453
00:47:34,160 --> 00:47:38,078
pushed by the legislator to actually

1454
00:47:36,240 --> 00:47:38,959
seek these active responses to illegal

1455
00:47:38,079 --> 00:47:41,200
content

1456
00:47:38,960 --> 00:47:43,599
often deploying automated measures but

1457
00:47:41,200 --> 00:47:45,759
they didn't have any legal certainty

1458
00:47:43,599 --> 00:47:48,319
or security on their side that if they

1459
00:47:45,760 --> 00:47:50,160
do so they won't end up ultimately being

1460
00:47:48,319 --> 00:47:52,160
held legally liable and face legal

1461
00:47:50,160 --> 00:47:53,359
consequences as a result of obtaining

1462
00:47:52,160 --> 00:47:55,200
actual knowledge

1463
00:47:53,359 --> 00:47:57,200
through those proactive measures that

1464
00:47:55,200 --> 00:47:59,359
were kind of the the tool how they could

1465
00:47:57,200 --> 00:48:03,200
possibly actually obtain that knowledge

1466
00:47:59,359 --> 00:48:05,920
um now uh what dsa does

1467
00:48:03,200 --> 00:48:07,598
it specifically actually simply states

1468
00:48:05,920 --> 00:48:09,440
and i think it's article 6

1469
00:48:07,599 --> 00:48:12,400
in the digital services act if i'm not

1470
00:48:09,440 --> 00:48:15,040
mistaken and i can even open it

1471
00:48:12,400 --> 00:48:16,319
it specifically basically says that that

1472
00:48:15,040 --> 00:48:19,440
platforms can

1473
00:48:16,319 --> 00:48:20,079
use these proactive measures or you know

1474
00:48:19,440 --> 00:48:23,440
uh

1475
00:48:20,079 --> 00:48:27,119
continue um using some tools that

1476
00:48:23,440 --> 00:48:28,800
uh actually uh seek to provide some

1477
00:48:27,119 --> 00:48:30,880
responses to this type of content

1478
00:48:28,800 --> 00:48:32,559
without the fear of being held liable

1479
00:48:30,880 --> 00:48:34,800
so it's a it's an article which has

1480
00:48:32,559 --> 00:48:37,280
approximately i think two paragraphs

1481
00:48:34,800 --> 00:48:39,119
um but it's finally in the legislation

1482
00:48:37,280 --> 00:48:41,520
and that means that it will help to

1483
00:48:39,119 --> 00:48:43,680
reinforce the level of legal certainty

1484
00:48:41,520 --> 00:48:45,599
i would also emphasize that very often

1485
00:48:43,680 --> 00:48:47,118
in europe when we discuss good samaritan

1486
00:48:45,599 --> 00:48:48,079
clause and good samaritan is actually a

1487
00:48:47,119 --> 00:48:50,880
very unfortunate

1488
00:48:48,079 --> 00:48:52,319
term because it's very much connected to

1489
00:48:50,880 --> 00:48:55,040
the american legal

1490
00:48:52,319 --> 00:48:57,119
tradition but when it's being mixed up

1491
00:48:55,040 --> 00:48:58,558
with the conditional model of liability

1492
00:48:57,119 --> 00:49:00,000
and with the prohibition of general

1493
00:48:58,559 --> 00:49:02,559
monitoring which is still

1494
00:49:00,000 --> 00:49:04,559
upheld and these are the main principles

1495
00:49:02,559 --> 00:49:05,040
of the european intermediary reliability

1496
00:49:04,559 --> 00:49:06,880
law

1497
00:49:05,040 --> 00:49:08,319
and the regime that is applicable within

1498
00:49:06,880 --> 00:49:09,920
the eu

1499
00:49:08,319 --> 00:49:12,000
such a safeguard can be actually

1500
00:49:09,920 --> 00:49:14,960
beneficial and it won't

1501
00:49:12,000 --> 00:49:16,079
lead hopefully to these blanket blanket

1502
00:49:14,960 --> 00:49:18,559
immunity for online

1503
00:49:16,079 --> 00:49:20,160
platforms or to this idea that platforms

1504
00:49:18,559 --> 00:49:21,680
will be able to do whatever they want

1505
00:49:20,160 --> 00:49:22,799
with the illegal content without any

1506
00:49:21,680 --> 00:49:23,919
public scrutiny

1507
00:49:22,800 --> 00:49:25,839
because there are other measures

1508
00:49:23,920 --> 00:49:27,520
safeguards and principles in place as a

1509
00:49:25,839 --> 00:49:29,200
part of conditional model of liability

1510
00:49:27,520 --> 00:49:31,119
that we have here in europe

1511
00:49:29,200 --> 00:49:33,279
um so i'm sorry maybe that was too

1512
00:49:31,119 --> 00:49:36,559
complicated legalistic explanation there

1513
00:49:33,280 --> 00:49:38,480
um but this how this is how these

1514
00:49:36,559 --> 00:49:39,680
provisions should work in practice we of

1515
00:49:38,480 --> 00:49:41,760
course have to wait

1516
00:49:39,680 --> 00:49:43,680
uh for the implementation of the law and

1517
00:49:41,760 --> 00:49:45,760
see how that will turn out

1518
00:49:43,680 --> 00:49:48,078
but the main purpose is that this legal

1519
00:49:45,760 --> 00:49:50,160
certainty that was lacking until now can

1520
00:49:48,079 --> 00:49:52,319
finally come to its existence

1521
00:49:50,160 --> 00:49:54,480
which should help us to prevent over

1522
00:49:52,319 --> 00:49:57,279
removal of legitimate speech from

1523
00:49:54,480 --> 00:49:57,280
online platforms

1524
00:49:57,599 --> 00:50:01,119
okay thank you um i've two other

1525
00:50:00,559 --> 00:50:03,359
questions

1526
00:50:01,119 --> 00:50:04,559
from the internet about interoperability

1527
00:50:03,359 --> 00:50:07,920
and i suppose

1528
00:50:04,559 --> 00:50:10,319
i should look at crystal for them um

1529
00:50:07,920 --> 00:50:11,760
the the last one i'm going to ask first

1530
00:50:10,319 --> 00:50:14,400
is

1531
00:50:11,760 --> 00:50:15,920
would such interoperability make it's

1532
00:50:14,400 --> 00:50:18,559
much more difficult

1533
00:50:15,920 --> 00:50:19,760
to ha to combat harassment and stalking

1534
00:50:18,559 --> 00:50:21,359
on the internet

1535
00:50:19,760 --> 00:50:24,079
how do you police that kind of

1536
00:50:21,359 --> 00:50:26,160
misbehavior if it's across

1537
00:50:24,079 --> 00:50:28,720
different platforms were forced to

1538
00:50:26,160 --> 00:50:31,759
interoperate and also

1539
00:50:28,720 --> 00:50:34,640
be conduits for such bad behavior

1540
00:50:31,760 --> 00:50:36,240
and i'll come to the earlier question

1541
00:50:34,640 --> 00:50:38,640
after you've answered this question uh

1542
00:50:36,240 --> 00:50:40,959
christoph

1543
00:50:38,640 --> 00:50:42,240
it's a good question um first to

1544
00:50:40,960 --> 00:50:43,440
understand our vision on

1545
00:50:42,240 --> 00:50:47,118
interoperability

1546
00:50:43,440 --> 00:50:49,680
um is to understand that we

1547
00:50:47,119 --> 00:50:50,559
would like to have it between platforms

1548
00:50:49,680 --> 00:50:53,598
that are in power

1549
00:50:50,559 --> 00:50:57,599
large platforms and the right

1550
00:50:53,599 --> 00:50:59,359
of smaller platforms actually to um

1551
00:50:57,599 --> 00:51:01,760
make use of interoperability so it

1552
00:50:59,359 --> 00:51:04,640
should not be among the big platforms

1553
00:51:01,760 --> 00:51:06,640
so small platforms should be able to

1554
00:51:04,640 --> 00:51:08,078
connect to the big platforms and second

1555
00:51:06,640 --> 00:51:10,640
we believe

1556
00:51:08,079 --> 00:51:11,200
it will help and not make it worse

1557
00:51:10,640 --> 00:51:13,520
because

1558
00:51:11,200 --> 00:51:15,359
we have not a problem of hate speech of

1559
00:51:13,520 --> 00:51:16,720
with not a problem of a lack of privacy

1560
00:51:15,359 --> 00:51:20,880
we have now the problem

1561
00:51:16,720 --> 00:51:21,439
that um of the attention industry that

1562
00:51:20,880 --> 00:51:24,160
works

1563
00:51:21,440 --> 00:51:26,079
with um you know certain pictures are

1564
00:51:24,160 --> 00:51:29,200
putting certain frames to trigger the

1565
00:51:26,079 --> 00:51:30,240
attention of users um because users

1566
00:51:29,200 --> 00:51:31,919
don't have a choice of a content

1567
00:51:30,240 --> 00:51:33,598
moderation practices users don't have a

1568
00:51:31,920 --> 00:51:36,319
choice to see which kind of content

1569
00:51:33,599 --> 00:51:37,200
will be shown and users don't have

1570
00:51:36,319 --> 00:51:40,400
options to

1571
00:51:37,200 --> 00:51:42,558
regulate the privacy the idea of more

1572
00:51:40,400 --> 00:51:44,240
competitors would be exactly that i can

1573
00:51:42,559 --> 00:51:48,880
move to a space where i'm not

1574
00:51:44,240 --> 00:51:51,759
harassed and not be made subject to

1575
00:51:48,880 --> 00:51:52,720
to certain um content that hurt my

1576
00:51:51,760 --> 00:51:55,599
feelings right

1577
00:51:52,720 --> 00:51:56,879
and that moment i got to get control i

1578
00:51:55,599 --> 00:51:59,359
can choose

1579
00:51:56,880 --> 00:52:01,119
a provider that gives me those options

1580
00:51:59,359 --> 00:52:02,160
and we would like even to go a step

1581
00:52:01,119 --> 00:52:04,240
further

1582
00:52:02,160 --> 00:52:05,440
um back end of interoperability is

1583
00:52:04,240 --> 00:52:07,279
faster start

1584
00:52:05,440 --> 00:52:08,800
we believe if users want to they should

1585
00:52:07,280 --> 00:52:10,480
be able to delegate a third-party

1586
00:52:08,800 --> 00:52:12,240
company or a piece of a third-party

1587
00:52:10,480 --> 00:52:13,359
software to interact with a platform on

1588
00:52:12,240 --> 00:52:15,200
their behalf

1589
00:52:13,359 --> 00:52:16,640
so users would have the option to see a

1590
00:52:15,200 --> 00:52:18,640
news feed in different order

1591
00:52:16,640 --> 00:52:20,000
calibrate their own filters on

1592
00:52:18,640 --> 00:52:22,240
misinformation

1593
00:52:20,000 --> 00:52:24,079
so in this sense interoperability can be

1594
00:52:22,240 --> 00:52:26,240
a great tool actually to tackle

1595
00:52:24,079 --> 00:52:28,079
um hate speech and those sort of

1596
00:52:26,240 --> 00:52:29,520
negative developments

1597
00:52:28,079 --> 00:52:32,720
of course there's a risk to it i think

1598
00:52:29,520 --> 00:52:34,960
the risk comes from rather from the data

1599
00:52:32,720 --> 00:52:36,959
um industry side again that we need to

1600
00:52:34,960 --> 00:52:39,440
take care not to place one

1601
00:52:36,960 --> 00:52:41,359
or another data selling industry on the

1602
00:52:39,440 --> 00:52:43,200
one that we already face

1603
00:52:41,359 --> 00:52:44,480
um but for this we have options as well

1604
00:52:43,200 --> 00:52:46,160
to avoid that from happening

1605
00:52:44,480 --> 00:52:48,559
but to answer the question we believe

1606
00:52:46,160 --> 00:52:50,960
interoperability is a tool actually do

1607
00:52:48,559 --> 00:52:52,880
to escape from the negative developments

1608
00:52:50,960 --> 00:52:57,040
you had mentioned

1609
00:52:52,880 --> 00:52:58,880
um critical counter question for me dan

1610
00:52:57,040 --> 00:53:01,040
aren't you actually advocating for just

1611
00:52:58,880 --> 00:53:03,760
all your own recommendation engines

1612
00:53:01,040 --> 00:53:07,200
to be able to do so and can't you

1613
00:53:03,760 --> 00:53:10,240
achieve that without interoperability

1614
00:53:07,200 --> 00:53:11,680
sure um recount a question do you think

1615
00:53:10,240 --> 00:53:13,200
and ever choose i can accomplish that

1616
00:53:11,680 --> 00:53:16,960
quite easily

1617
00:53:13,200 --> 00:53:18,960
um you know like when we look at

1618
00:53:16,960 --> 00:53:20,720
the internet through the lenses of

1619
00:53:18,960 --> 00:53:23,040
market competition then we see that it

1620
00:53:20,720 --> 00:53:26,399
is the dominance of platforms over data

1621
00:53:23,040 --> 00:53:28,079
that have created those um those spaces

1622
00:53:26,400 --> 00:53:29,520
those bullet gardens where users have to

1623
00:53:28,079 --> 00:53:30,400
feeling their trap you cannot escape

1624
00:53:29,520 --> 00:53:31,920
from

1625
00:53:30,400 --> 00:53:34,160
and there are so many alternative

1626
00:53:31,920 --> 00:53:36,079
options that cannot get off ground

1627
00:53:34,160 --> 00:53:37,598
because users

1628
00:53:36,079 --> 00:53:39,920
feel trapped don't want to leave their

1629
00:53:37,599 --> 00:53:41,599
friends behind and don't have options

1630
00:53:39,920 --> 00:53:42,400
actually do you have a better moderation

1631
00:53:41,599 --> 00:53:44,720
system

1632
00:53:42,400 --> 00:53:46,800
of course you can be creative and you

1633
00:53:44,720 --> 00:53:47,200
know use plugins on whatever you see fit

1634
00:53:46,800 --> 00:53:48,640
but

1635
00:53:47,200 --> 00:53:50,558
you need to stay within the platform

1636
00:53:48,640 --> 00:53:51,440
barriers what we would like to enable

1637
00:53:50,559 --> 00:53:53,520
users is to

1638
00:53:51,440 --> 00:53:54,559
actually leave the wallet garden go to

1639
00:53:53,520 --> 00:53:56,720
another place

1640
00:53:54,559 --> 00:53:58,720
but still stay in touch with friends who

1641
00:53:56,720 --> 00:53:59,680
have made a choice to remain there

1642
00:53:58,720 --> 00:54:03,040
and i think that's perhaps the

1643
00:53:59,680 --> 00:54:04,640
difference to what you had in mind

1644
00:54:03,040 --> 00:54:06,079
i have a follow-up question well another

1645
00:54:04,640 --> 00:54:07,359
question from the internet regarding iso

1646
00:54:06,079 --> 00:54:11,280
mobility

1647
00:54:07,359 --> 00:54:12,960
and that is historically speaking

1648
00:54:11,280 --> 00:54:14,960
as soon as the big players get involved

1649
00:54:12,960 --> 00:54:18,559
in setting standards they tend to also

1650
00:54:14,960 --> 00:54:20,960
shape policy by

1651
00:54:18,559 --> 00:54:22,240
being involved in that how would that be

1652
00:54:20,960 --> 00:54:24,720
different

1653
00:54:22,240 --> 00:54:26,078
in the case of interoperability and uh

1654
00:54:24,720 --> 00:54:27,520
it's specifically mentioned

1655
00:54:26,079 --> 00:54:29,760
by the uh the person who asked the

1656
00:54:27,520 --> 00:54:31,520
question that mastodon probably fuller

1657
00:54:29,760 --> 00:54:34,319
issues because nobody else was involved

1658
00:54:31,520 --> 00:54:37,359
in setting that standard

1659
00:54:34,319 --> 00:54:37,839
it's an excellent question and we

1660
00:54:37,359 --> 00:54:40,000
struggle the

1661
00:54:37,839 --> 00:54:41,680
the question of standards ourselves um

1662
00:54:40,000 --> 00:54:43,599
now a policy paper

1663
00:54:41,680 --> 00:54:45,598
which is our recommendations for the

1664
00:54:43,599 --> 00:54:47,119
european union to enact

1665
00:54:45,599 --> 00:54:48,720
certain provisions in the judicial

1666
00:54:47,119 --> 00:54:53,839
services act

1667
00:54:48,720 --> 00:54:53,839
um we abstain from asking to

1668
00:54:54,000 --> 00:54:57,680
establish new standards like api

1669
00:54:55,760 --> 00:55:00,000
standards we believe it's a bad idea

1670
00:54:57,680 --> 00:55:01,040
to regulate technology like that what we

1671
00:55:00,000 --> 00:55:03,280
want to do is that

1672
00:55:01,040 --> 00:55:04,160
big platforms just offer

1673
00:55:03,280 --> 00:55:05,760
interoperability

1674
00:55:04,160 --> 00:55:07,359
however they see fit we don't want to

1675
00:55:05,760 --> 00:55:10,960
have a standard that can be

1676
00:55:07,359 --> 00:55:12,078
either again monopolized or lobbied by

1677
00:55:10,960 --> 00:55:14,000
the big platforms

1678
00:55:12,079 --> 00:55:15,440
because then we end up with the

1679
00:55:14,000 --> 00:55:17,920
standards we already see which

1680
00:55:15,440 --> 00:55:19,520
which we don't like but it's a good

1681
00:55:17,920 --> 00:55:21,280
question and

1682
00:55:19,520 --> 00:55:23,040
what we did is with our policy

1683
00:55:21,280 --> 00:55:24,480
principles on interoperability to give

1684
00:55:23,040 --> 00:55:25,440
kind of a food for thought how we

1685
00:55:24,480 --> 00:55:28,720
believe

1686
00:55:25,440 --> 00:55:30,559
the end version should look like um

1687
00:55:28,720 --> 00:55:32,399
but there are many questions that remain

1688
00:55:30,559 --> 00:55:35,440
and we don't know exactly

1689
00:55:32,400 --> 00:55:37,280
how to go there

1690
00:55:35,440 --> 00:55:39,119
yeah and i'm sorry i'm sticking to the

1691
00:55:37,280 --> 00:55:40,799
topic of interoperability because

1692
00:55:39,119 --> 00:55:42,160
most questions are actually about that

1693
00:55:40,799 --> 00:55:44,640
one of the other questions is

1694
00:55:42,160 --> 00:55:46,160
um how do we prevent this from be

1695
00:55:44,640 --> 00:55:46,799
getting messed up like it happens if

1696
00:55:46,160 --> 00:55:49,520
peers do

1697
00:55:46,799 --> 00:55:51,040
psd psd2 and for the audience that don't

1698
00:55:49,520 --> 00:55:54,160
know about psd2

1699
00:55:51,040 --> 00:55:55,920
pg2 is a directive that forced banks to

1700
00:55:54,160 --> 00:55:57,040
open up the apis to other financial

1701
00:55:55,920 --> 00:55:58,960
service providers

1702
00:55:57,040 --> 00:56:00,480
which is also interoperability between

1703
00:55:58,960 --> 00:56:01,440
platforms in this case for banking

1704
00:56:00,480 --> 00:56:03,359
platforms

1705
00:56:01,440 --> 00:56:04,960
which comes with all sorts of privacy

1706
00:56:03,359 --> 00:56:06,558
questions that parents

1707
00:56:04,960 --> 00:56:08,160
completely thought through when that

1708
00:56:06,559 --> 00:56:10,000
legislation came about

1709
00:56:08,160 --> 00:56:11,440
sorry for having this long-winded

1710
00:56:10,000 --> 00:56:13,280
interaction in the crystal that's

1711
00:56:11,440 --> 00:56:16,640
i think it was needed for for people

1712
00:56:13,280 --> 00:56:18,079
that don't know what psd2 means

1713
00:56:16,640 --> 00:56:20,640
it's a good question interestingly we

1714
00:56:18,079 --> 00:56:21,760
never use psd2 or the telecommunications

1715
00:56:20,640 --> 00:56:23,359
act because both have

1716
00:56:21,760 --> 00:56:25,040
interoperability options as those

1717
00:56:23,359 --> 00:56:27,359
negative examples we always use it as

1718
00:56:25,040 --> 00:56:29,279
examples that hey it's already

1719
00:56:27,359 --> 00:56:31,359
possible so you don't have an excuse to

1720
00:56:29,280 --> 00:56:32,720
say it's impossible to put it in the law

1721
00:56:31,359 --> 00:56:34,480
what is true is that there's a lot of

1722
00:56:32,720 --> 00:56:37,759
mess around it and the question

1723
00:56:34,480 --> 00:56:40,720
of how do volumes is a question of

1724
00:56:37,760 --> 00:56:42,160
um that's politically again um so the

1725
00:56:40,720 --> 00:56:43,759
question of whether policymakers are

1726
00:56:42,160 --> 00:56:45,520
actually listening listening to us

1727
00:56:43,760 --> 00:56:47,520
or listening to industry lobbyists so

1728
00:56:45,520 --> 00:56:49,280
the the one who raised their questions

1729
00:56:47,520 --> 00:56:50,880
absolutely right there's a huge risk for

1730
00:56:49,280 --> 00:56:52,559
every topic we talk about but it's

1731
00:56:50,880 --> 00:56:55,280
interoperability whether it's

1732
00:56:52,559 --> 00:56:55,839
user control over content and targeted

1733
00:56:55,280 --> 00:56:57,760
ads

1734
00:56:55,839 --> 00:56:59,200
liability everything that we believe

1735
00:56:57,760 --> 00:57:02,240
should be in law of course

1736
00:56:59,200 --> 00:57:04,558
could be hijacked could be

1737
00:57:02,240 --> 00:57:06,319
redesigned in a way that it will lead to

1738
00:57:04,559 --> 00:57:09,760
more problems than fewer problems

1739
00:57:06,319 --> 00:57:11,520
um so indeed for every policy question

1740
00:57:09,760 --> 00:57:13,200
we raise we need to ask ourselves is it

1741
00:57:11,520 --> 00:57:15,599
worth the fight to be risk open in the

1742
00:57:13,200 --> 00:57:19,520
box of the pandora do we make it worse

1743
00:57:15,599 --> 00:57:22,079
would be sad as um on that front

1744
00:57:19,520 --> 00:57:23,440
we are happy to make pressure and what

1745
00:57:22,079 --> 00:57:24,640
we need to do in the next series is to

1746
00:57:23,440 --> 00:57:26,160
convince them that we are the right

1747
00:57:24,640 --> 00:57:27,440
person to talk to

1748
00:57:26,160 --> 00:57:29,200
and that's perhaps a challenge how to

1749
00:57:27,440 --> 00:57:32,640
make tech explicable

1750
00:57:29,200 --> 00:57:34,078
to um policymakers so those who ask the

1751
00:57:32,640 --> 00:57:35,839
questions i think those should

1752
00:57:34,079 --> 00:57:38,079
should help us to come to process to the

1753
00:57:35,839 --> 00:57:40,400
parliament and tell energies how it's

1754
00:57:38,079 --> 00:57:44,960
going to work

1755
00:57:40,400 --> 00:57:44,960
on that note question to both of you

1756
00:57:45,119 --> 00:57:51,040
you said citizen involvement i prefer

1757
00:57:48,240 --> 00:57:53,200
the term citizen of users

1758
00:57:51,040 --> 00:57:55,359
would it be helpful to push for a

1759
00:57:53,200 --> 00:57:59,200
maintenance in the parliament

1760
00:57:55,359 --> 00:58:02,480
for at least the the targeting

1761
00:57:59,200 --> 00:58:09,598
points you both mentioned before and if

1762
00:58:02,480 --> 00:58:11,760
so how so um uh i guess that i will

1763
00:58:09,599 --> 00:58:14,880
start just so christoph can

1764
00:58:11,760 --> 00:58:16,640
rest a little um so the question was

1765
00:58:14,880 --> 00:58:18,400
whether it would be useful to push for

1766
00:58:16,640 --> 00:58:21,920
those amendments was that

1767
00:58:18,400 --> 00:58:23,200
for amendments that cover targeting of

1768
00:58:21,920 --> 00:58:26,000
of citizens

1769
00:58:23,200 --> 00:58:27,118
absolutely um so there is of course

1770
00:58:26,000 --> 00:58:29,920
short and long answer

1771
00:58:27,119 --> 00:58:30,880
as to every question and so the short

1772
00:58:29,920 --> 00:58:34,000
answer would be

1773
00:58:30,880 --> 00:58:36,960
yes but given that the wording of such

1774
00:58:34,000 --> 00:58:38,720
an amendment will be precise and nuanced

1775
00:58:36,960 --> 00:58:40,480
uh we are still working out our

1776
00:58:38,720 --> 00:58:41,359
positioning on online targeting and i

1777
00:58:40,480 --> 00:58:43,839
think we all

1778
00:58:41,359 --> 00:58:45,598
uh no one can name those practices that

1779
00:58:43,839 --> 00:58:47,200
we don't want to see being deployed by

1780
00:58:45,599 --> 00:58:50,000
platforms and where we can actually

1781
00:58:47,200 --> 00:58:52,000
imagine a proper ban on such practices

1782
00:58:50,000 --> 00:58:53,920
we have recently published one of our

1783
00:58:52,000 --> 00:58:56,960
blog posts where we actually

1784
00:58:53,920 --> 00:58:59,440
um unfold the way of thinking

1785
00:58:56,960 --> 00:59:00,799
uh that access now currently you know

1786
00:58:59,440 --> 00:59:01,839
how we are brainstorming about this

1787
00:59:00,799 --> 00:59:04,000
whole issue

1788
00:59:01,839 --> 00:59:06,240
and as i said especially those that's

1789
00:59:04,000 --> 00:59:09,119
targeting that uses behavioral data

1790
00:59:06,240 --> 00:59:10,879
of users citizens then maybe let's go

1791
00:59:09,119 --> 00:59:12,480
for individuals because states are also

1792
00:59:10,880 --> 00:59:14,480
obliged to protect rights of

1793
00:59:12,480 --> 00:59:16,160
individuals that are not their citizens

1794
00:59:14,480 --> 00:59:19,440
um

1795
00:59:16,160 --> 00:59:21,200
so i uh that that's definitely one form

1796
00:59:19,440 --> 00:59:23,119
where we can definitely see and will be

1797
00:59:21,200 --> 00:59:24,879
supporting the ban and possible face out

1798
00:59:23,119 --> 00:59:27,920
that will actually lead to a ban

1799
00:59:24,880 --> 00:59:29,280
the same goes for the uh cross-site

1800
00:59:27,920 --> 00:59:32,559
tracking of users

1801
00:59:29,280 --> 00:59:35,440
and uh due to the fact how users

1802
00:59:32,559 --> 00:59:36,640
data are being abused again as it's

1803
00:59:35,440 --> 00:59:38,400
being the integral

1804
00:59:36,640 --> 00:59:40,799
integral part of the business models of

1805
00:59:38,400 --> 00:59:42,720
this platform and so on and so forth

1806
00:59:40,799 --> 00:59:44,960
um so that's one of the direction we

1807
00:59:42,720 --> 00:59:46,959
will be definitely taking and again

1808
00:59:44,960 --> 00:59:50,160
we are inviting all of you to help us

1809
00:59:46,960 --> 00:59:52,160
out to brainstorm together with us to

1810
00:59:50,160 --> 00:59:53,920
assess different options directions that

1811
00:59:52,160 --> 00:59:57,040
we should take into consideration

1812
00:59:53,920 --> 00:59:57,440
and not forget about uh but i personally

1813
00:59:57,040 --> 00:59:58,799
think

1814
00:59:57,440 --> 01:00:01,760
that this will be one of the main

1815
00:59:58,799 --> 01:00:04,160
battles when it comes to dsa

1816
01:00:01,760 --> 01:00:06,160
where we will definitely need to be on

1817
01:00:04,160 --> 01:00:08,160
the same page and harmonize and join the

1818
01:00:06,160 --> 01:00:10,399
forces

1819
01:00:08,160 --> 01:00:12,558
because dsa gives us a good ground at

1820
01:00:10,400 --> 01:00:15,920
the moment but it doesn't go far enough

1821
01:00:12,559 --> 01:00:17,760
and so uh yes definitely the answer is

1822
01:00:15,920 --> 01:00:19,599
yes but given that we will have a very

1823
01:00:17,760 --> 01:00:21,280
nuanced position so we know what we are

1824
01:00:19,599 --> 01:00:22,720
asking for and we are taking into

1825
01:00:21,280 --> 01:00:24,960
consideration

1826
01:00:22,720 --> 01:00:25,839
also those other aspects that could

1827
01:00:24,960 --> 01:00:29,040
eventually

1828
01:00:25,839 --> 01:00:30,720
play out uh badly in practice so

1829
01:00:29,040 --> 01:00:33,440
good intentions are not enough when it

1830
01:00:30,720 --> 01:00:33,439
comes to dsa

1831
01:00:34,559 --> 01:00:38,880
thank you we sli running slightly over

1832
01:00:37,440 --> 01:00:40,160
time but i've been told beforehand

1833
01:00:38,880 --> 01:00:43,359
that's okay that they do so

1834
01:00:40,160 --> 01:00:44,879
for a few minutes and there's three

1835
01:00:43,359 --> 01:00:47,200
questions that are open one of

1836
01:00:44,880 --> 01:00:48,319
one of them i will answer myself that is

1837
01:00:47,200 --> 01:00:50,319
basically half the member states

1838
01:00:48,319 --> 01:00:51,680
responded and the answer to that is no

1839
01:00:50,319 --> 01:00:52,480
the member states have not taken any

1840
01:00:51,680 --> 01:00:54,640
position

1841
01:00:52,480 --> 01:00:56,240
and two others i think are quite

1842
01:00:54,640 --> 01:00:57,839
interestingly important from our techie

1843
01:00:56,240 --> 01:01:00,720
perspective

1844
01:00:57,839 --> 01:01:02,400
one is is there anything you see that

1845
01:01:00,720 --> 01:01:04,959
might affect current decentralized

1846
01:01:02,400 --> 01:01:08,880
platforms like the fedivers macedon

1847
01:01:04,960 --> 01:01:11,520
and the other is will

1848
01:01:08,880 --> 01:01:13,280
any review of the data protection uh

1849
01:01:11,520 --> 01:01:16,240
sorry the database

1850
01:01:13,280 --> 01:01:20,160
uh protection directive uh affect meta

1851
01:01:16,240 --> 01:01:20,160
search engines and interactivities again

1852
01:01:21,520 --> 01:01:25,040
perhaps i jump in english can you take

1853
01:01:23,359 --> 01:01:26,558
over um

1854
01:01:25,040 --> 01:01:28,558
first member states have given opinion

1855
01:01:26,559 --> 01:01:30,000
actually on the dsa

1856
01:01:28,559 --> 01:01:32,160
they've been one two official

1857
01:01:30,000 --> 01:01:34,240
submissions plus joint letters

1858
01:01:32,160 --> 01:01:35,839
plus discussions in council and where

1859
01:01:34,240 --> 01:01:39,279
the dsa was presented

1860
01:01:35,839 --> 01:01:40,880
i have some nice protocols which show

1861
01:01:39,280 --> 01:01:42,480
the different attitude of member states

1862
01:01:40,880 --> 01:01:44,960
towards it um

1863
01:01:42,480 --> 01:01:47,200
so for us it also means we need to work

1864
01:01:44,960 --> 01:01:50,559
straight away with the council

1865
01:01:47,200 --> 01:01:53,919
to ensure that the package would be good

1866
01:01:50,559 --> 01:01:54,160
what was the question i guess um i think

1867
01:01:53,920 --> 01:01:55,920
the

1868
01:01:54,160 --> 01:01:58,879
the answer to the question depends on

1869
01:01:55,920 --> 01:02:00,480
the um what lawyers call materials cope

1870
01:01:58,880 --> 01:02:02,559
applications whether it would apply to

1871
01:02:00,480 --> 01:02:04,319
those platform models at all

1872
01:02:02,559 --> 01:02:06,160
and perhaps initially can help me out

1873
01:02:04,319 --> 01:02:07,839
here we have always criticized

1874
01:02:06,160 --> 01:02:10,720
for the e-commerce directive that it was

1875
01:02:07,839 --> 01:02:14,078
not quite clear how it would relate to

1876
01:02:10,720 --> 01:02:15,520
first non-for-profit uh platforms and

1877
01:02:14,079 --> 01:02:16,480
many of those alternative platforms are

1878
01:02:15,520 --> 01:02:18,319
like that

1879
01:02:16,480 --> 01:02:20,240
because there was this issue of

1880
01:02:18,319 --> 01:02:21,440
providing a service against remuneration

1881
01:02:20,240 --> 01:02:23,038
and it was not quite clear what it means

1882
01:02:21,440 --> 01:02:24,640
would it apply to wikipedia if you get

1883
01:02:23,039 --> 01:02:25,359
like donation would it apply to a

1884
01:02:24,640 --> 01:02:27,839
blogger

1885
01:02:25,359 --> 01:02:28,960
if you have like pop-ups of you know ads

1886
01:02:27,839 --> 01:02:31,359
or something like that

1887
01:02:28,960 --> 01:02:33,200
so that's i think one huge question and

1888
01:02:31,359 --> 01:02:36,160
the second question is in as much those

1889
01:02:33,200 --> 01:02:39,598
new due diligence obligation

1890
01:02:36,160 --> 01:02:41,440
would force alternative platforms um

1891
01:02:39,599 --> 01:02:42,880
governance models to redesign the

1892
01:02:41,440 --> 01:02:45,200
interfaces

1893
01:02:42,880 --> 01:02:46,960
and those are open question for us we

1894
01:02:45,200 --> 01:02:47,439
have not analyzed that in detail but we

1895
01:02:46,960 --> 01:02:49,440
see that

1896
01:02:47,440 --> 01:02:50,960
we have worried that it would not only

1897
01:02:49,440 --> 01:02:52,480
impact the large platforms but many

1898
01:02:50,960 --> 01:02:55,760
others as well

1899
01:02:52,480 --> 01:02:57,119
what do you think alicia yeah i can i

1900
01:02:55,760 --> 01:02:59,760
can only agree

1901
01:02:57,119 --> 01:03:00,480
um especially regarding the non-profit

1902
01:02:59,760 --> 01:03:03,920
question

1903
01:03:00,480 --> 01:03:06,160
um yeah this is also or was and always

1904
01:03:03,920 --> 01:03:09,520
been one of our main asks for

1905
01:03:06,160 --> 01:03:11,200
non-profit organizations um and actually

1906
01:03:09,520 --> 01:03:13,440
it's not quite clear how that will play

1907
01:03:11,200 --> 01:03:14,879
out now in practice with the way how dsa

1908
01:03:13,440 --> 01:03:16,720
is standing because at the moment it

1909
01:03:14,880 --> 01:03:18,240
actually speaks about online platforms

1910
01:03:16,720 --> 01:03:20,959
and then it speaks about very large

1911
01:03:18,240 --> 01:03:20,959
online platform

1912
01:03:28,319 --> 01:03:32,400
but what it will be and how it will

1913
01:03:30,319 --> 01:03:34,079
impact non-profit organizations whether

1914
01:03:32,400 --> 01:03:35,920
these are broker bloggers or

1915
01:03:34,079 --> 01:03:37,440
organizations like civil rights

1916
01:03:35,920 --> 01:03:40,880
organizations

1917
01:03:37,440 --> 01:03:43,440
um uh that remain to be to be

1918
01:03:40,880 --> 01:03:44,920
seen and i also know that the european

1919
01:03:43,440 --> 01:03:47,119
court of human rights and its

1920
01:03:44,920 --> 01:03:49,039
jurisprudence tried to

1921
01:03:47,119 --> 01:03:50,720
establish some principles for the

1922
01:03:49,039 --> 01:03:53,039
non-profit uh

1923
01:03:50,720 --> 01:03:53,839
since delphi vs estonia and then with

1924
01:03:53,039 --> 01:03:56,960
the

1925
01:03:53,839 --> 01:03:58,480
mte decision and the swedish case that

1926
01:03:56,960 --> 01:04:00,720
followed afterwards

1927
01:03:58,480 --> 01:04:02,480
um but i'm not sure how well it was

1928
01:04:00,720 --> 01:04:04,160
actually elaborated later on

1929
01:04:02,480 --> 01:04:05,839
um but this is something we will be

1930
01:04:04,160 --> 01:04:06,720
definitely looking at and working on

1931
01:04:05,839 --> 01:04:09,599
further

1932
01:04:06,720 --> 01:04:10,720
um and regarding the impact on the

1933
01:04:09,599 --> 01:04:13,760
smaller players

1934
01:04:10,720 --> 01:04:15,839
and the also the

1935
01:04:13,760 --> 01:04:16,960
the interface idea this is still

1936
01:04:15,839 --> 01:04:20,000
something we are

1937
01:04:16,960 --> 01:04:23,039
actually also wondering about um

1938
01:04:20,000 --> 01:04:24,240
and thinking how this will turn out in

1939
01:04:23,039 --> 01:04:26,960
practice and

1940
01:04:24,240 --> 01:04:28,720
um we are hoping to actually develop our

1941
01:04:26,960 --> 01:04:29,599
positioning further on these issues as

1942
01:04:28,720 --> 01:04:32,319
well because

1943
01:04:29,599 --> 01:04:32,799
um we actually started working all of us

1944
01:04:32,319 --> 01:04:35,279
uh

1945
01:04:32,799 --> 01:04:37,119
on dsa and on our recommendations

1946
01:04:35,280 --> 01:04:38,319
already i think a year ago maybe a year

1947
01:04:37,119 --> 01:04:40,000
and a half ago

1948
01:04:38,319 --> 01:04:42,000
uh when we were working just with the

1949
01:04:40,000 --> 01:04:43,920
leagues that political or other

1950
01:04:42,000 --> 01:04:45,839
uh media platforms in brussels were

1951
01:04:43,920 --> 01:04:47,280
actually sharing with us and we were

1952
01:04:45,839 --> 01:04:49,359
working with the bits and pieces and

1953
01:04:47,280 --> 01:04:51,039
trying to put our thinking together

1954
01:04:49,359 --> 01:04:52,480
now we have the draft and i think we

1955
01:04:51,039 --> 01:04:54,880
need to do another round of very

1956
01:04:52,480 --> 01:04:57,200
detailed thinking and

1957
01:04:54,880 --> 01:04:58,960
what will be ultimately our position or

1958
01:04:57,200 --> 01:05:00,640
what will be those recommendations and

1959
01:04:58,960 --> 01:05:02,240
amendments in the european parliament we

1960
01:05:00,640 --> 01:05:04,720
will be supporting and pushing for

1961
01:05:02,240 --> 01:05:05,839
so it's a it's a period of a hard work

1962
01:05:04,720 --> 01:05:09,680
for all of us

1963
01:05:05,839 --> 01:05:12,160
not mentioning that i always say that um

1964
01:05:09,680 --> 01:05:13,520
you know we stand against a huge lobby

1965
01:05:12,160 --> 01:05:16,160
power

1966
01:05:13,520 --> 01:05:18,319
uh that is going to be you know and is

1967
01:05:16,160 --> 01:05:21,680
constantly being exercised by these

1968
01:05:18,319 --> 01:05:23,520
uh private actors and but i also have to

1969
01:05:21,680 --> 01:05:25,038
say that we had a very good cooperation

1970
01:05:23,520 --> 01:05:26,400
with the european commissions throughout

1971
01:05:25,039 --> 01:05:28,000
the process and i think that i can say

1972
01:05:26,400 --> 01:05:29,599
that on behalf of all of us

1973
01:05:28,000 --> 01:05:31,200
that we feel that european commission

1974
01:05:29,599 --> 01:05:33,680
really listen this time

1975
01:05:31,200 --> 01:05:36,240
so um yeah more question marks than

1976
01:05:33,680 --> 01:05:37,759
answers here from me i think

1977
01:05:36,240 --> 01:05:40,000
that's okay with me not knowing

1978
01:05:37,760 --> 01:05:41,920
something is fine i think you

1979
01:05:40,000 --> 01:05:43,520
definitely run out of time thank you

1980
01:05:41,920 --> 01:05:47,039
both for being here

1981
01:05:43,520 --> 01:05:50,880
and um well enjoy the the 2d

1982
01:05:47,039 --> 01:05:53,200
online board as a congress thank you

1983
01:05:50,880 --> 01:05:55,200
that wraps up this session of us thank

1984
01:05:53,200 --> 01:06:05,839
you all

1985
01:05:55,200 --> 01:06:05,839
thank you very much bye

1986
01:06:30,839 --> 01:06:33,839
wow

1987
01:06:36,520 --> 01:06:39,520
bye

