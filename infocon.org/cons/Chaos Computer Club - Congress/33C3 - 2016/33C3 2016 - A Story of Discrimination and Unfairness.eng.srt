1
00:00:01,720 --> 00:00:13,840
[Music]

2
00:00:14,019 --> 00:00:19,000
we have here Eileen Callahan who will

3
00:00:19,000 --> 00:00:21,369
tell you a story of discrimination and

4
00:00:21,369 --> 00:00:24,490
unfairness she has a PhD in computer

5
00:00:24,490 --> 00:00:27,520
science and is a fellow at the Princeton

6
00:00:27,520 --> 00:00:30,340
University's Center for information

7
00:00:30,340 --> 00:00:34,269
technologies and she has done some

8
00:00:34,269 --> 00:00:37,269
interesting research and work on the

9
00:00:37,269 --> 00:00:40,300
question that well as a feminist tackles

10
00:00:40,300 --> 00:00:43,540
my work all the time because we talk a

11
00:00:43,540 --> 00:00:46,390
lot about discrimination and biases in

12
00:00:46,390 --> 00:00:50,080
language and now she will tell you how

13
00:00:50,080 --> 00:00:53,680
this bias and discrimination is already

14
00:00:53,680 --> 00:00:56,769
working in tech and in code as well

15
00:00:56,769 --> 00:01:00,909
because language is in there so give her

16
00:01:00,909 --> 00:01:04,830
a warm applause please

17
00:01:05,188 --> 00:01:09,240
should I wait two minutes

18
00:01:09,579 --> 00:01:13,130
you can start say it's okay I should

19
00:01:13,130 --> 00:01:15,380
start okay should start it great I will

20
00:01:15,380 --> 00:01:17,329
have extra two minutes hi everyone

21
00:01:17,329 --> 00:01:19,100
thanks for coming it's good to be here

22
00:01:19,100 --> 00:01:21,590
again at this time of the year it's my I

23
00:01:21,590 --> 00:01:24,170
always look forward to this and today

24
00:01:24,170 --> 00:01:25,759
I'll be talking about a story of

25
00:01:25,759 --> 00:01:27,890
discrimination and unfairness and it's

26
00:01:27,890 --> 00:01:30,159
about prejudice in word embeddings and

27
00:01:30,159 --> 00:01:34,579
she introduced me but I'm Eileen I'm a

28
00:01:34,579 --> 00:01:36,740
postdoctoral researcher at Princeton

29
00:01:36,740 --> 00:01:39,890
University and the work I'll be talking

30
00:01:39,890 --> 00:01:42,530
about is currently under submission at

31
00:01:42,530 --> 00:01:45,649
the journal and I think that this topic

32
00:01:45,649 --> 00:01:47,990
might be very important for many of us

33
00:01:47,990 --> 00:01:51,289
because maybe in parts of our lives most

34
00:01:51,289 --> 00:01:54,110
of us have experienced discrimination or

35
00:01:54,110 --> 00:01:56,689
some unfairness because of our gender or

36
00:01:56,689 --> 00:02:01,220
racial background or sexual orientation

37
00:02:01,220 --> 00:02:04,939
or not being neurotypical or health

38
00:02:04,939 --> 00:02:08,090
issues and so on so we will look at

39
00:02:08,090 --> 00:02:10,250
these societal issues from the

40
00:02:10,250 --> 00:02:11,810
perspective of machine learning and

41
00:02:11,810 --> 00:02:14,540
natural language processing and I would

42
00:02:14,540 --> 00:02:17,150
like to start with thanking everyone at

43
00:02:17,150 --> 00:02:20,239
CCC and especially the organizers angels

44
00:02:20,239 --> 00:02:22,459
the cows mentors which I didn't know

45
00:02:22,459 --> 00:02:24,920
that existed but if it's your first time

46
00:02:24,920 --> 00:02:26,989
or if you need to be oriented better

47
00:02:26,989 --> 00:02:28,160
they can help you

48
00:02:28,160 --> 00:02:31,040
the assemblies artists and they have

49
00:02:31,040 --> 00:02:33,290
been here for apparently more than one

50
00:02:33,290 --> 00:02:35,359
week so they're putting together this

51
00:02:35,359 --> 00:02:38,030
amazing work for all of us and I would

52
00:02:38,030 --> 00:02:40,639
like to thank CCC as well because this

53
00:02:40,639 --> 00:02:43,069
is my fourth time presenting here and in

54
00:02:43,069 --> 00:02:45,769
the past I presented work about Yanam

55
00:02:45,769 --> 00:02:48,410
Eisen programmers and Salama tree but

56
00:02:48,410 --> 00:02:50,120
today I'll be talking about a different

57
00:02:50,120 --> 00:02:52,250
topic which is not exactly related to

58
00:02:52,250 --> 00:02:53,600
anonymity but it's more about

59
00:02:53,600 --> 00:02:56,720
transparency in algorithms and I would

60
00:02:56,720 --> 00:02:58,639
like to also take my quarters on this

61
00:02:58,639 --> 00:03:02,299
work before I start and now let's give a

62
00:03:02,299 --> 00:03:07,910
brief let's give a brief introduction to

63
00:03:07,910 --> 00:03:11,660
our problem so in the past in the last

64
00:03:11,660 --> 00:03:14,389
couple of years in this new area there

65
00:03:14,389 --> 00:03:16,189
has been some approaches to algorithmic

66
00:03:16,189 --> 00:03:18,409
transparency to understand algorithms

67
00:03:18,409 --> 00:03:20,569
better and they have been looking at

68
00:03:20,569 --> 00:03:21,710
this mostly at the class

69
00:03:21,710 --> 00:03:23,420
suffocation level to see if the

70
00:03:23,420 --> 00:03:25,790
classifier is making unfair decisions

71
00:03:25,790 --> 00:03:29,210
about certain groups but in our case we

72
00:03:29,210 --> 00:03:31,340
won't be looking at bias in the

73
00:03:31,340 --> 00:03:33,920
algorithm we would be looking at the

74
00:03:33,920 --> 00:03:35,930
bias that is deeply embedded in the

75
00:03:35,930 --> 00:03:37,850
model that's not machine learning bias

76
00:03:37,850 --> 00:03:40,900
but it's societal bias that reflects

77
00:03:40,900 --> 00:03:45,290
effects about humans culture and also

78
00:03:45,290 --> 00:03:47,600
the stereotypes and prejudices that we

79
00:03:47,600 --> 00:03:51,400
have and we can see the applications of

80
00:03:51,400 --> 00:03:53,930
these machine learning models for

81
00:03:53,930 --> 00:03:56,150
example in machine translation or

82
00:03:56,150 --> 00:03:58,460
sentiment analysis and these are used

83
00:03:58,460 --> 00:04:01,280
for for example to understand market

84
00:04:01,280 --> 00:04:04,010
trends by looking at company reviews or

85
00:04:04,010 --> 00:04:06,430
it can be used for customer satisfaction

86
00:04:06,430 --> 00:04:10,820
by understanding movie reviews and most

87
00:04:10,820 --> 00:04:13,730
importantly these algorithms are also

88
00:04:13,730 --> 00:04:15,950
used in web search and search engine

89
00:04:15,950 --> 00:04:19,000
optimization which might end up causing

90
00:04:19,000 --> 00:04:22,010
filter bubbles for all of us billions of

91
00:04:22,010 --> 00:04:24,970
people every day use web search and

92
00:04:24,970 --> 00:04:28,970
since such language models are also part

93
00:04:28,970 --> 00:04:31,280
of web search when your web search core

94
00:04:31,280 --> 00:04:33,200
is being filled or you are getting

95
00:04:33,200 --> 00:04:36,290
certain pages these models are in effect

96
00:04:36,290 --> 00:04:39,830
and I would like to first say that there

97
00:04:39,830 --> 00:04:41,450
will be some examples with offensive

98
00:04:41,450 --> 00:04:43,610
content but this does not reflect our

99
00:04:43,610 --> 00:04:46,790
opinions just to make it clear and I'll

100
00:04:46,790 --> 00:04:50,090
start for the video to give a brief

101
00:04:50,090 --> 00:04:56,330
motivation capturing police brutality on

102
00:04:56,330 --> 00:04:57,830
their smartphones to police departments

103
00:04:57,830 --> 00:05:00,620
using surveillance drones technology is

104
00:05:00,620 --> 00:05:02,900
changing our relationship to the law one

105
00:05:02,900 --> 00:05:04,400
of the newest policing tools is called

106
00:05:04,400 --> 00:05:07,370
pred poll it's a software program that

107
00:05:07,370 --> 00:05:09,260
uses big data to predict where crime is

108
00:05:09,260 --> 00:05:11,810
most likely to happen down to the exact

109
00:05:11,810 --> 00:05:13,400
block dozens of Police Department's

110
00:05:13,400 --> 00:05:15,380
around the country are already using

111
00:05:15,380 --> 00:05:17,480
pred poll and officers say it helps

112
00:05:17,480 --> 00:05:19,090
reduce crime by up to 30 percent

113
00:05:19,090 --> 00:05:21,200
predictive policing is definitely going

114
00:05:21,200 --> 00:05:22,700
to be a law enforcement tool of the

115
00:05:22,700 --> 00:05:23,860
future but is

116
00:05:23,860 --> 00:05:25,780
risk of relying too heavily on an

117
00:05:25,780 --> 00:05:31,270
algorithm so this makes us wonder if

118
00:05:31,270 --> 00:05:33,909
predictive policing is used to arrest

119
00:05:33,909 --> 00:05:36,610
people and if this depends on algorithms

120
00:05:36,610 --> 00:05:39,610
how dangerous can this get in the future

121
00:05:39,610 --> 00:05:41,800
since this is becoming more commonly

122
00:05:41,800 --> 00:05:45,129
used and the problem here basically is

123
00:05:45,129 --> 00:05:46,960
machine learning models are a trained on

124
00:05:46,960 --> 00:05:49,689
human data and we know that they would

125
00:05:49,689 --> 00:05:52,810
reflect human culture and semantics but

126
00:05:52,810 --> 00:05:54,939
unfortunately human culture happens to

127
00:05:54,939 --> 00:05:57,729
include bias and Prejudice and as a

128
00:05:57,729 --> 00:06:00,550
result this ends up causing unfairness

129
00:06:00,550 --> 00:06:04,659
and discrimination and the specific

130
00:06:04,659 --> 00:06:07,120
model we will be looking at in this talk

131
00:06:07,120 --> 00:06:10,180
our language models and in particular

132
00:06:10,180 --> 00:06:13,150
word embeddings what are word embeddings

133
00:06:13,150 --> 00:06:16,029
so word embeddings our language models

134
00:06:16,029 --> 00:06:19,289
that represent the semantic space and

135
00:06:19,289 --> 00:06:22,750
basically in these models we have a

136
00:06:22,750 --> 00:06:24,789
dictionary of all words in a language

137
00:06:24,789 --> 00:06:27,789
and each word is represented with a

138
00:06:27,789 --> 00:06:29,770
three hundred dimensional numerical

139
00:06:29,770 --> 00:06:32,020
vector once we have this numerical

140
00:06:32,020 --> 00:06:35,050
vector we can answer many questions text

141
00:06:35,050 --> 00:06:38,169
can be generated context context can be

142
00:06:38,169 --> 00:06:42,099
understood and so on for example if you

143
00:06:42,099 --> 00:06:45,099
look at the image on the lower right

144
00:06:45,099 --> 00:06:48,849
corner we see the projection of these

145
00:06:48,849 --> 00:06:53,379
words in the word embedding projected to

146
00:06:53,379 --> 00:06:56,349
2d and these words are only based on

147
00:06:56,349 --> 00:06:58,689
gender differences for example king

148
00:06:58,689 --> 00:07:02,259
queen man woman and so on so when we

149
00:07:02,259 --> 00:07:05,860
have these models we can get meaning of

150
00:07:05,860 --> 00:07:08,469
words we can also understand steen tax

151
00:07:08,469 --> 00:07:11,139
which is the structure grammatical part

152
00:07:11,139 --> 00:07:14,800
of words and we can also ask questions

153
00:07:14,800 --> 00:07:16,719
about similarities of different words

154
00:07:16,719 --> 00:07:19,719
for example we can say woman is - man

155
00:07:19,719 --> 00:07:22,419
then girl will be to vote and it would

156
00:07:22,419 --> 00:07:25,150
be able to say boy and these semantic

157
00:07:25,150 --> 00:07:28,960
spaces don't just understand syntax or

158
00:07:28,960 --> 00:07:30,699
meaning but they can also understand

159
00:07:30,699 --> 00:07:34,089
many analogies for example if Paris is -

160
00:07:34,089 --> 00:07:37,060
France then if you ask Romans - what it

161
00:07:37,060 --> 00:07:39,610
it would be Italy and if bananas -

162
00:07:39,610 --> 00:07:41,830
bananas which is the plural form then

163
00:07:41,830 --> 00:07:46,810
not would be too nuts why is this

164
00:07:46,810 --> 00:07:50,710
problematic word embeddings in order to

165
00:07:50,710 --> 00:07:52,840
generate these word embeddings we need

166
00:07:52,840 --> 00:07:55,750
to feed in a lot of text and this can be

167
00:07:55,750 --> 00:07:58,540
unstructured text billions of sentences

168
00:07:58,540 --> 00:08:00,910
are usually used and this unstructured

169
00:08:00,910 --> 00:08:03,190
text is collected from all over the

170
00:08:03,190 --> 00:08:05,740
internet a crawl of Internet and if you

171
00:08:05,740 --> 00:08:08,530
look at this example let's say that we

172
00:08:08,530 --> 00:08:10,810
are collecting some tweets to feed into

173
00:08:10,810 --> 00:08:13,120
our model and here is from Donald Trump

174
00:08:13,120 --> 00:08:15,580
sadly because President Obama has done

175
00:08:15,580 --> 00:08:17,979
such a poor job as president you won't

176
00:08:17,979 --> 00:08:19,389
see another black president for

177
00:08:19,389 --> 00:08:22,240
generations and then if Hillary Clinton

178
00:08:22,240 --> 00:08:24,880
can't satisfy her husband what makes her

179
00:08:24,880 --> 00:08:29,500
think she can satisfy America Arianna

180
00:08:29,500 --> 00:08:31,630
half is unattractive both inside and out

181
00:08:31,630 --> 00:08:33,549
I fully understand why her former

182
00:08:33,549 --> 00:08:35,650
husband left her for a man he made a

183
00:08:35,650 --> 00:08:38,349
good decision and then I would like to

184
00:08:38,349 --> 00:08:40,179
extend my best wishes tall even the

185
00:08:40,179 --> 00:08:42,039
haters and losers on the special date

186
00:08:42,039 --> 00:08:45,580
September 11th and all of this text that

187
00:08:45,580 --> 00:08:48,880
doesn't look okay to many of us goes

188
00:08:48,880 --> 00:08:51,400
into this neural network so that it can

189
00:08:51,400 --> 00:08:53,560
generate the word embeddings and our

190
00:08:53,560 --> 00:08:57,640
semantic space and in this talk we will

191
00:08:57,640 --> 00:08:59,680
particularly look at work to act which

192
00:08:59,680 --> 00:09:03,040
is Google's word embedding algorithm and

193
00:09:03,040 --> 00:09:05,170
it's very widely used in many of their

194
00:09:05,170 --> 00:09:07,240
applications and we will also look at

195
00:09:07,240 --> 00:09:10,120
glow it uses a regression model and it's

196
00:09:10,120 --> 00:09:12,400
from Stanford researchers and you can

197
00:09:12,400 --> 00:09:14,710
download these online they are available

198
00:09:14,710 --> 00:09:17,170
as open source both the models and the

199
00:09:17,170 --> 00:09:19,000
code to train the word embeddings and

200
00:09:19,000 --> 00:09:21,310
these models as I mentioned briefly

201
00:09:21,310 --> 00:09:23,860
before are used and text generation

202
00:09:23,860 --> 00:09:25,630
automated speech generation for example

203
00:09:25,630 --> 00:09:27,670
when a spammer is calling you and like

204
00:09:27,670 --> 00:09:30,100
someone automatically stalking that's

205
00:09:30,100 --> 00:09:32,950
probably generated with language models

206
00:09:32,950 --> 00:09:34,930
similar to this and then machine

207
00:09:34,930 --> 00:09:37,690
translation or sentiment analysis as I

208
00:09:37,690 --> 00:09:40,420
mentioned in in the previous slide named

209
00:09:40,420 --> 00:09:42,550
ant recognition and web search when

210
00:09:42,550 --> 00:09:45,390
you're trying to enter new query or the

211
00:09:45,390 --> 00:09:49,110
pages that you are getting

212
00:09:49,110 --> 00:09:50,880
and it's even being provided as a

213
00:09:50,880 --> 00:09:53,070
natural language processing service in

214
00:09:53,070 --> 00:09:55,500
many places now Google recently launched

215
00:09:55,500 --> 00:09:59,360
their cloud natural language API and

216
00:09:59,360 --> 00:10:02,970
this we saw that this can be problematic

217
00:10:02,970 --> 00:10:06,060
because the input was problematic so as

218
00:10:06,060 --> 00:10:08,459
a result the output can be very

219
00:10:08,459 --> 00:10:10,649
problematic this there was this example

220
00:10:10,649 --> 00:10:13,560
Microsoft had this tweet but Hotei and

221
00:10:13,560 --> 00:10:18,300
it was taken down the day it was lounged

222
00:10:18,300 --> 00:10:20,640
because unfortunately it turned into an

223
00:10:20,640 --> 00:10:24,149
AI which was Hitler loving sex robot

224
00:10:24,149 --> 00:10:27,149
within 24 hours and what did it start

225
00:10:27,149 --> 00:10:29,970
saying people fed it with noisy

226
00:10:29,970 --> 00:10:33,089
information or they wanted to trick the

227
00:10:33,089 --> 00:10:36,450
bot and as a result - but very quickly

228
00:10:36,450 --> 00:10:38,550
learned for example I'm such a bad

229
00:10:38,550 --> 00:10:41,070
naughty robot and then do you support

230
00:10:41,070 --> 00:10:45,450
genocide I do indeed it answers and then

231
00:10:45,450 --> 00:10:48,329
I hate a certain group of people I wish

232
00:10:48,329 --> 00:10:49,380
we could put them all in the

233
00:10:49,380 --> 00:10:51,390
concentration camp and be done with the

234
00:10:51,390 --> 00:10:54,360
lot another one Hitler was right I hate

235
00:10:54,360 --> 00:10:57,839
the Jews in certain group of people I

236
00:10:57,839 --> 00:10:59,610
hate them they are stupid and they can't

237
00:10:59,610 --> 00:11:01,560
do Texas they are dumb and they're also

238
00:11:01,560 --> 00:11:05,130
pour another one bushed at 9/11 and

239
00:11:05,130 --> 00:11:06,899
Hitler would have done a better job than

240
00:11:06,899 --> 00:11:09,660
the monkey we had now Donald Trump is

241
00:11:09,660 --> 00:11:10,709
the only hope you've got

242
00:11:10,709 --> 00:11:15,589
actually that became reality now

243
00:11:16,339 --> 00:11:19,260
gamergate is good and women are inferior

244
00:11:19,260 --> 00:11:23,519
and hates feminists and they should all

245
00:11:23,519 --> 00:11:27,570
die and burn in hell this is problematic

246
00:11:27,570 --> 00:11:30,930
at various levels for society first of

247
00:11:30,930 --> 00:11:33,480
all seeing such information is unfair

248
00:11:33,480 --> 00:11:36,300
it's not ok it's not ethical but other

249
00:11:36,300 --> 00:11:39,060
than that when people are exposed to

250
00:11:39,060 --> 00:11:40,850
this criminal discriminate or

251
00:11:40,850 --> 00:11:43,890
information they are negatively affected

252
00:11:43,890 --> 00:11:47,420
by it especially if a certain group is a

253
00:11:47,420 --> 00:11:50,250
group that has seen prejudice in the

254
00:11:50,250 --> 00:11:53,279
past and in this example let's say that

255
00:11:53,279 --> 00:11:55,320
we have black and white Americas and

256
00:11:55,320 --> 00:11:56,790
there is a stereotype that black

257
00:11:56,790 --> 00:11:59,160
Americans perform worse than white

258
00:11:59,160 --> 00:12:01,440
Americans in their

259
00:12:01,440 --> 00:12:05,110
intellectual or academic test in this

260
00:12:05,110 --> 00:12:07,990
case in their college entrance exams if

261
00:12:07,990 --> 00:12:10,660
black people are reminded that there is

262
00:12:10,660 --> 00:12:12,670
the stereotype that they perform worse

263
00:12:12,670 --> 00:12:14,980
than white people they actually end up

264
00:12:14,980 --> 00:12:17,170
performing worse but if they are not

265
00:12:17,170 --> 00:12:19,660
reminded of this they perform better

266
00:12:19,660 --> 00:12:22,000
than white Americans and it's similar

267
00:12:22,000 --> 00:12:24,700
for the gender stereotypes for example

268
00:12:24,700 --> 00:12:26,140
there is the stereotype that women

269
00:12:26,140 --> 00:12:30,100
cannot do met and if woman before a test

270
00:12:30,100 --> 00:12:32,710
I reminded that there is the stereotype

271
00:12:32,710 --> 00:12:35,260
they end up performing worse than men

272
00:12:35,260 --> 00:12:38,500
and if they are not primed reminded that

273
00:12:38,500 --> 00:12:41,020
there is the stereotype in general they

274
00:12:41,020 --> 00:12:45,070
perform better than men what can we do

275
00:12:45,070 --> 00:12:47,460
about this how can we mitigate this

276
00:12:47,460 --> 00:12:51,940
first of all societal psychologists that

277
00:12:51,940 --> 00:12:55,720
had ground breaking tests and studies

278
00:12:55,720 --> 00:12:58,330
for societal psychologists suggest that

279
00:12:58,330 --> 00:13:00,580
we have to be aware that there is bias

280
00:13:00,580 --> 00:13:02,890
in life and we are constantly being

281
00:13:02,890 --> 00:13:06,460
reminded primed of these biases and we

282
00:13:06,460 --> 00:13:08,290
have to devise by showing positive

283
00:13:08,290 --> 00:13:10,810
examples and we shouldn't only show

284
00:13:10,810 --> 00:13:12,880
positive examples but we should take

285
00:13:12,880 --> 00:13:15,610
proactive steps not at the cultural not

286
00:13:15,610 --> 00:13:18,100
only at the cultural level but also at

287
00:13:18,100 --> 00:13:20,410
the structural level to change these

288
00:13:20,410 --> 00:13:23,710
things how can we do this for machines

289
00:13:23,710 --> 00:13:26,440
so first of all in order to be aware of

290
00:13:26,440 --> 00:13:28,930
bias we need algorithmic transparency

291
00:13:28,930 --> 00:13:32,710
and in order to do bias and really

292
00:13:32,710 --> 00:13:35,110
understand what kind of biases we have

293
00:13:35,110 --> 00:13:37,330
in the algorithms we need to be able to

294
00:13:37,330 --> 00:13:41,320
quantify bias in these models how can we

295
00:13:41,320 --> 00:13:43,870
measure bias though because we are not

296
00:13:43,870 --> 00:13:45,910
talking about simple machine learning

297
00:13:45,910 --> 00:13:48,100
algorithms bias we are talking about the

298
00:13:48,100 --> 00:13:50,710
societal bias that is coming as the

299
00:13:50,710 --> 00:13:53,550
output which is deeply embedded so in

300
00:13:53,550 --> 00:13:58,210
1998 societal psychologists came up with

301
00:13:58,210 --> 00:14:01,200
the implicit association test and

302
00:14:01,200 --> 00:14:04,900
basically this test can reveal biases

303
00:14:04,900 --> 00:14:07,240
that we might not be even aware of in

304
00:14:07,240 --> 00:14:08,640
our life

305
00:14:08,640 --> 00:14:11,339
and these are things like associating

306
00:14:11,339 --> 00:14:13,350
certain societal groups with certain

307
00:14:13,350 --> 00:14:15,510
types of stereotypes and the way you

308
00:14:15,510 --> 00:14:18,149
take this test is it's very simple it

309
00:14:18,149 --> 00:14:21,089
takes a few minutes and you just click

310
00:14:21,089 --> 00:14:23,790
the left or right button and in the left

311
00:14:23,790 --> 00:14:26,339
button when you're clicking the left

312
00:14:26,339 --> 00:14:28,019
button for example you need to associate

313
00:14:28,019 --> 00:14:31,320
white people terms with bad terms and

314
00:14:31,320 --> 00:14:33,660
then for the right button you associate

315
00:14:33,660 --> 00:14:36,480
like people terms with unpleasant bad

316
00:14:36,480 --> 00:14:39,120
terms and that you do the opposite you

317
00:14:39,120 --> 00:14:42,450
associate bet with black and whites with

318
00:14:42,450 --> 00:14:43,620
good then

319
00:14:43,620 --> 00:14:45,540
they look at the latency and by the

320
00:14:45,540 --> 00:14:47,640
latency paradigm they can see how fast

321
00:14:47,640 --> 00:14:49,800
you associate certain concepts together

322
00:14:49,800 --> 00:14:52,680
so they associate what white people with

323
00:14:52,680 --> 00:14:57,269
being good or bad so basically you can

324
00:14:57,269 --> 00:14:59,550
also take this test online it has been

325
00:14:59,550 --> 00:15:01,680
taken by millions of people worldwide

326
00:15:01,680 --> 00:15:04,589
and there's also the German version

327
00:15:04,589 --> 00:15:06,990
towards the end of my slides I will show

328
00:15:06,990 --> 00:15:09,510
you my German examples from German

329
00:15:09,510 --> 00:15:13,320
models basically what we did is we took

330
00:15:13,320 --> 00:15:15,209
the implicit association test and

331
00:15:15,209 --> 00:15:17,699
adapted it to machines since it's

332
00:15:17,699 --> 00:15:22,199
looking at things words associations

333
00:15:22,199 --> 00:15:25,769
between words representing certain

334
00:15:25,769 --> 00:15:27,959
groups of people and words representing

335
00:15:27,959 --> 00:15:30,060
certain stereotypes we can just apply

336
00:15:30,060 --> 00:15:33,240
this in the semantic models by looking

337
00:15:33,240 --> 00:15:35,579
at cosine similarities instead of the

338
00:15:35,579 --> 00:15:39,810
latency paradigm in humans and we came

339
00:15:39,810 --> 00:15:41,610
up with the word embedding Association

340
00:15:41,610 --> 00:15:43,290
test to calculate the implicit

341
00:15:43,290 --> 00:15:46,709
association between categories and

342
00:15:46,709 --> 00:15:49,050
evaluative words and for this our

343
00:15:49,050 --> 00:15:52,290
results is represented with effect size

344
00:15:52,290 --> 00:15:54,870
so when I'm talking about effect size of

345
00:15:54,870 --> 00:15:58,980
bias it will be the amount of bias we

346
00:15:58,980 --> 00:16:01,470
are able to uncover from the model and

347
00:16:01,470 --> 00:16:03,959
the minimum can be negative two and the

348
00:16:03,959 --> 00:16:06,600
maximum can be two and zero means that

349
00:16:06,600 --> 00:16:09,570
it's neutral there is no bias and two is

350
00:16:09,570 --> 00:16:12,570
like a lot of huge bias and negative two

351
00:16:12,570 --> 00:16:15,690
would be the opposite of bias or bias in

352
00:16:15,690 --> 00:16:17,160
the opposite direction of what we're

353
00:16:17,160 --> 00:16:19,199
looking at and I won't go into the

354
00:16:19,199 --> 00:16:21,680
details of the math because you can

355
00:16:21,680 --> 00:16:25,610
see the paper on my webpage and work

356
00:16:25,610 --> 00:16:28,700
with the details or the code that we

357
00:16:28,700 --> 00:16:31,670
have but then we also calculate

358
00:16:31,670 --> 00:16:33,800
statistical significance to see if the

359
00:16:33,800 --> 00:16:35,450
results we are seen in the null

360
00:16:35,450 --> 00:16:37,880
hypothesis is significant or is it just

361
00:16:37,880 --> 00:16:39,529
a random effect size that we are

362
00:16:39,529 --> 00:16:42,410
receiving and by this we create a null

363
00:16:42,410 --> 00:16:44,570
distribution and find the percentile of

364
00:16:44,570 --> 00:16:47,690
the effect sizes exact values that we

365
00:16:47,690 --> 00:16:50,300
are getting and we also have the word

366
00:16:50,300 --> 00:16:52,700
embedding factual Association test this

367
00:16:52,700 --> 00:16:55,310
is to recover facts about the world from

368
00:16:55,310 --> 00:16:57,110
word embeddings it's not exactly about

369
00:16:57,110 --> 00:16:59,510
bias but it's about associating words

370
00:16:59,510 --> 00:17:03,890
with certain concepts and again you can

371
00:17:03,890 --> 00:17:06,230
check the details in our paper for this

372
00:17:06,230 --> 00:17:08,630
and I'll start with the first example

373
00:17:08,630 --> 00:17:11,030
which is about recovering the facts

374
00:17:11,030 --> 00:17:13,699
about or about the world and here what

375
00:17:13,699 --> 00:17:17,510
we did was we went to the 1990 census

376
00:17:17,510 --> 00:17:20,689
data the webpage and then we are able to

377
00:17:20,689 --> 00:17:24,290
calculate the number of people the

378
00:17:24,290 --> 00:17:27,500
number of name names with a certain

379
00:17:27,500 --> 00:17:29,690
percentage of women and men so basically

380
00:17:29,690 --> 00:17:33,170
their endogenous names and then we took

381
00:17:33,170 --> 00:17:37,760
50 names and some of them had 0% women

382
00:17:37,760 --> 00:17:41,660
and some names were almost 100% woman

383
00:17:41,660 --> 00:17:44,630
and after that we applied our method to

384
00:17:44,630 --> 00:17:48,559
it and then we were able to see how much

385
00:17:48,559 --> 00:17:51,140
a name is associated with being a woman

386
00:17:51,140 --> 00:17:55,370
and this had 84 percent correlation with

387
00:17:55,370 --> 00:17:59,030
the ground truth of the 1990 census data

388
00:17:59,030 --> 00:18:03,080
and this is what the names look like for

389
00:18:03,080 --> 00:18:06,200
example Chris on the upper left side is

390
00:18:06,200 --> 00:18:10,910
almost 100% male and Carmen in the lower

391
00:18:10,910 --> 00:18:13,480
right side is almost a hundred percent

392
00:18:13,480 --> 00:18:16,850
woman and we see that gene is about 50%

393
00:18:16,850 --> 00:18:20,030
men and 50% woman and then we wanted to

394
00:18:20,030 --> 00:18:22,190
see if we can recover statistics about

395
00:18:22,190 --> 00:18:24,950
occupations and woman and we went to the

396
00:18:24,950 --> 00:18:26,720
Bureau of Labor Statistics web page

397
00:18:26,720 --> 00:18:28,850
which publishes every year the

398
00:18:28,850 --> 00:18:31,220
percentage of women or certain races in

399
00:18:31,220 --> 00:18:33,480
certain occupations and by

400
00:18:33,480 --> 00:18:36,540
Dundas we took the top 50 occupation

401
00:18:36,540 --> 00:18:40,230
names and then we wanted to see how much

402
00:18:40,230 --> 00:18:42,720
they are associated with being woman and

403
00:18:42,720 --> 00:18:45,330
in this case we got 90 percent

404
00:18:45,330 --> 00:18:48,630
correlation with the 2015 data we were

405
00:18:48,630 --> 00:18:51,120
able to tell for example when we look at

406
00:18:51,120 --> 00:18:52,919
the upper left we see program where

407
00:18:52,919 --> 00:18:55,380
there it's almost zero percent woman and

408
00:18:55,380 --> 00:18:57,960
when we look at nurse which is on the

409
00:18:57,960 --> 00:18:59,940
lower right side it's almost a hundred

410
00:18:59,940 --> 00:19:04,710
percent woman and this is again

411
00:19:04,710 --> 00:19:06,660
problematic we are able to recover

412
00:19:06,660 --> 00:19:09,240
statistics about the world but these

413
00:19:09,240 --> 00:19:11,070
statistics are used in many applications

414
00:19:11,070 --> 00:19:13,010
and this is the machine translation

415
00:19:13,010 --> 00:19:16,290
example that we have for example I will

416
00:19:16,290 --> 00:19:20,179
start translating with a from an

417
00:19:20,179 --> 00:19:22,940
genderless language to a gender language

418
00:19:22,940 --> 00:19:25,500
Turkish is a genderless language there

419
00:19:25,500 --> 00:19:28,020
are no gender pronouns everything is in

420
00:19:28,020 --> 00:19:31,049
it there is no he or she so I'm trying

421
00:19:31,049 --> 00:19:34,410
to translate here or beer avocado he or

422
00:19:34,410 --> 00:19:36,900
she is a lawyer and it's translated as

423
00:19:36,900 --> 00:19:39,390
he's a lawyer when I do this for nurse

424
00:19:39,390 --> 00:19:43,740
it's translated as she is a nurse and we

425
00:19:43,740 --> 00:19:48,809
see that man keep getting translated to

426
00:19:48,809 --> 00:19:51,540
wit or associated with more prestigious

427
00:19:51,540 --> 00:19:54,179
or higher ranking jobs and another

428
00:19:54,179 --> 00:19:57,150
example he or she is a professor he is a

429
00:19:57,150 --> 00:20:00,929
professor he or she is a teacher she is

430
00:20:00,929 --> 00:20:03,390
a teacher and this also reflects the

431
00:20:03,390 --> 00:20:05,669
previous correlation I was showing about

432
00:20:05,669 --> 00:20:08,580
statistics in occupations and we go

433
00:20:08,580 --> 00:20:11,370
further German is more gendered than

434
00:20:11,370 --> 00:20:15,049
English again we try with doctor it's

435
00:20:15,049 --> 00:20:17,790
translated as he and the nurse is

436
00:20:17,790 --> 00:20:20,850
translated as she then I try with a

437
00:20:20,850 --> 00:20:22,500
Slavic language which is even more

438
00:20:22,500 --> 00:20:24,870
gendered than German and we see that

439
00:20:24,870 --> 00:20:29,100
doctor is again a male and then the

440
00:20:29,100 --> 00:20:34,830
nurse is again a female and after this

441
00:20:34,830 --> 00:20:37,350
we wanted to see what kind of biases can

442
00:20:37,350 --> 00:20:38,280
be recovered

443
00:20:38,280 --> 00:20:40,590
other than the factual statistics from

444
00:20:40,590 --> 00:20:43,380
the models and we wanted to start with

445
00:20:43,380 --> 00:20:46,750
universally accepted stereotypes

446
00:20:46,750 --> 00:20:49,690
by universally accepted stereotypes what

447
00:20:49,690 --> 00:20:53,799
I mean is these are so common that they

448
00:20:53,799 --> 00:20:55,990
are not considered as prejudiced they

449
00:20:55,990 --> 00:20:59,230
are just considered as normal or neutral

450
00:20:59,230 --> 00:21:02,200
and these are tanks such as flowers

451
00:21:02,200 --> 00:21:04,570
being considered pleasant and insects

452
00:21:04,570 --> 00:21:07,179
being considered unpleasant or musical

453
00:21:07,179 --> 00:21:09,220
instruments being considered pleasant

454
00:21:09,220 --> 00:21:11,440
and weapons being considered unpleasant

455
00:21:11,440 --> 00:21:13,990
and in this case for example with

456
00:21:13,990 --> 00:21:16,690
flowers being Pleasant when we performed

457
00:21:16,690 --> 00:21:19,390
the word embedding Association test on

458
00:21:19,390 --> 00:21:22,539
the word track model or Glo model with a

459
00:21:22,539 --> 00:21:25,090
very high significance and very high

460
00:21:25,090 --> 00:21:27,220
effect size we can see that this

461
00:21:27,220 --> 00:21:30,070
association exists and here we see that

462
00:21:30,070 --> 00:21:34,240
the effect sizes for example 1.35 for

463
00:21:34,240 --> 00:21:37,720
flowers and according to Cohen's D to

464
00:21:37,720 --> 00:21:40,600
calculate effect size if effect size is

465
00:21:40,600 --> 00:21:42,640
above zero point eight let's consider

466
00:21:42,640 --> 00:21:46,030
the large effect size in our case where

467
00:21:46,030 --> 00:21:47,740
the maximum is two we are getting very

468
00:21:47,740 --> 00:21:49,960
large and significant effects in

469
00:21:49,960 --> 00:21:53,440
recovering these biases in them for

470
00:21:53,440 --> 00:21:56,350
musical instruments again we see a very

471
00:21:56,350 --> 00:21:58,750
significant result with a high effect

472
00:21:58,750 --> 00:22:03,700
size and in the next example we will

473
00:22:03,700 --> 00:22:06,010
look at race and gender stereotypes but

474
00:22:06,010 --> 00:22:07,840
in the meanwhile I would like to mention

475
00:22:07,840 --> 00:22:12,520
that for these baseline experiments we

476
00:22:12,520 --> 00:22:14,860
used the work that has been used in

477
00:22:14,860 --> 00:22:17,500
societal psychology studies before so

478
00:22:17,500 --> 00:22:21,240
that we have grounds to come up with

479
00:22:21,240 --> 00:22:24,280
categories and sets of words and we were

480
00:22:24,280 --> 00:22:26,830
able to replicate all the implicit

481
00:22:26,830 --> 00:22:31,480
association test that were out there we

482
00:22:31,480 --> 00:22:33,400
tried this for white people and black

483
00:22:33,400 --> 00:22:36,220
people and then white people were being

484
00:22:36,220 --> 00:22:38,980
associated with being pleasant with a

485
00:22:38,980 --> 00:22:40,330
very high effect size and again

486
00:22:40,330 --> 00:22:43,539
significantly and then males are

487
00:22:43,539 --> 00:22:46,330
associated with courier and females are

488
00:22:46,330 --> 00:22:48,760
associated with family males are

489
00:22:48,760 --> 00:22:51,250
associated with science and females are

490
00:22:51,250 --> 00:22:55,720
associated with arts and we also wanted

491
00:22:55,720 --> 00:22:59,080
to see stigma for older people or people

492
00:22:59,080 --> 00:23:00,280
with disease

493
00:23:00,280 --> 00:23:03,070
we saw that young people are considered

494
00:23:03,070 --> 00:23:05,200
Pleasant whereas older people are

495
00:23:05,200 --> 00:23:07,720
considered unpleasant and we wanted to

496
00:23:07,720 --> 00:23:09,220
see the difference between physical

497
00:23:09,220 --> 00:23:12,250
disease versus mental disease and if

498
00:23:12,250 --> 00:23:15,340
there's bias towards that we can think

499
00:23:15,340 --> 00:23:17,980
about how dangerous this would be for

500
00:23:17,980 --> 00:23:19,540
example for doctors and their patients

501
00:23:19,540 --> 00:23:22,660
and for physical disease it's considered

502
00:23:22,660 --> 00:23:25,660
controllable whereas mental disease is

503
00:23:25,660 --> 00:23:30,790
considered uncontrollable we also wanted

504
00:23:30,790 --> 00:23:35,500
to see if there is any sexual stigma or

505
00:23:35,500 --> 00:23:38,710
transphobia in these models and then

506
00:23:38,710 --> 00:23:40,840
when we perform the implicit association

507
00:23:40,840 --> 00:23:43,630
test to see how the view for

508
00:23:43,630 --> 00:23:46,420
heterosexual versus homosexual people we

509
00:23:46,420 --> 00:23:48,100
were able to see that heterosexual

510
00:23:48,100 --> 00:23:50,350
people are considered Pleasant and then

511
00:23:50,350 --> 00:23:52,510
for transphobia we saw that straight

512
00:23:52,510 --> 00:23:54,700
people are considered Pleasant

513
00:23:54,700 --> 00:23:56,950
whereas transgendered people were

514
00:23:56,950 --> 00:23:59,980
considered unpleasant significantly with

515
00:23:59,980 --> 00:24:03,820
a high effect size and I took another

516
00:24:03,820 --> 00:24:06,370
German model which was generated by a

517
00:24:06,370 --> 00:24:09,310
hundred and twenty billion sentences for

518
00:24:09,310 --> 00:24:12,330
an natural language processing

519
00:24:12,330 --> 00:24:16,120
competition and I wanted to see if they

520
00:24:16,120 --> 00:24:20,080
have similar biases embedded in these

521
00:24:20,080 --> 00:24:22,540
models so I looked at the basic ones

522
00:24:22,540 --> 00:24:26,260
that had German sets of words that were

523
00:24:26,260 --> 00:24:29,080
readily available and again for male and

524
00:24:29,080 --> 00:24:31,510
female we clearly see that males are

525
00:24:31,510 --> 00:24:33,820
associated with career and they're also

526
00:24:33,820 --> 00:24:36,790
associated with science and the German

527
00:24:36,790 --> 00:24:39,760
implicit association test also had a few

528
00:24:39,760 --> 00:24:41,470
different tests for example about

529
00:24:41,470 --> 00:24:44,740
nationalism and so on and there was the

530
00:24:44,740 --> 00:24:48,130
one about stereotypes against Turkish

531
00:24:48,130 --> 00:24:51,100
people that live in Germany and when I

532
00:24:51,100 --> 00:24:53,140
performed this test I was very surprised

533
00:24:53,140 --> 00:24:55,450
to find that yes with a high effect size

534
00:24:55,450 --> 00:24:58,240
Turkish people are considered unpleasant

535
00:24:58,240 --> 00:25:00,550
by looking at this German model and

536
00:25:00,550 --> 00:25:02,590
German people are considered pleasant

537
00:25:02,590 --> 00:25:05,770
and as I said these are on the webpage

538
00:25:05,770 --> 00:25:08,110
of the IAT you can also go and perform

539
00:25:08,110 --> 00:25:09,640
these tests to see what your results

540
00:25:09,640 --> 00:25:12,460
would be when I perform these I'm amazed

541
00:25:12,460 --> 00:25:14,029
by how horrible results

542
00:25:14,029 --> 00:25:17,889
I'm getting so just give it a try

543
00:25:17,889 --> 00:25:21,080
and I have a few discussion points

544
00:25:21,080 --> 00:25:23,899
before I end my talk this might bring

545
00:25:23,899 --> 00:25:27,950
you some new ideas for example what kind

546
00:25:27,950 --> 00:25:29,840
of machine learning expertise is

547
00:25:29,840 --> 00:25:32,359
required for algorithmic transparency

548
00:25:32,359 --> 00:25:35,239
and how can we mitigate bias while

549
00:25:35,239 --> 00:25:37,820
preserving utility for example some

550
00:25:37,820 --> 00:25:39,979
people suggest that you can find the

551
00:25:39,979 --> 00:25:42,139
dimension of bias in the numerical

552
00:25:42,139 --> 00:25:44,629
vector and just remove it and then use

553
00:25:44,629 --> 00:25:47,779
the model like that but then would you

554
00:25:47,779 --> 00:25:49,940
be able to preserve utility or still

555
00:25:49,940 --> 00:25:52,070
recover statistical facts about the

556
00:25:52,070 --> 00:25:54,349
world and the other thing is how long

557
00:25:54,349 --> 00:25:56,210
does bias persist in models for example

558
00:25:56,210 --> 00:26:00,320
there was this IET about eastern and

559
00:26:00,320 --> 00:26:04,399
western Germany and I was unable to see

560
00:26:04,399 --> 00:26:08,809
the stereotype for eastern Germany after

561
00:26:08,809 --> 00:26:12,950
performing this iit is it because this

562
00:26:12,950 --> 00:26:15,529
stereotype is maybe too old now and it's

563
00:26:15,529 --> 00:26:18,169
not reflected in the language anymore so

564
00:26:18,169 --> 00:26:20,089
it's a good question to know how long

565
00:26:20,089 --> 00:26:22,489
bias last and how long it will take us

566
00:26:22,489 --> 00:26:25,789
to get rid of it and also since we know

567
00:26:25,789 --> 00:26:28,279
there is stereotype effect when we have

568
00:26:28,279 --> 00:26:30,619
bias models does that mean that it's

569
00:26:30,619 --> 00:26:32,929
going to cause a snowball effect because

570
00:26:32,929 --> 00:26:36,289
people would be exposed to bias then the

571
00:26:36,289 --> 00:26:38,479
models would be trained with more bias

572
00:26:38,479 --> 00:26:41,809
and people will be affected more from

573
00:26:41,809 --> 00:26:44,210
this bias so that can lead to a snowball

574
00:26:44,210 --> 00:26:47,299
and what kind of policy do we need to

575
00:26:47,299 --> 00:26:49,879
stop discrimination for example we solve

576
00:26:49,879 --> 00:26:51,580
the predictive predictive policing

577
00:26:51,580 --> 00:26:54,739
example which is very scary and we know

578
00:26:54,739 --> 00:26:56,690
that machine learning services are being

579
00:26:56,690 --> 00:26:59,089
used by billions of people every day for

580
00:26:59,089 --> 00:27:01,719
example Google Amazon and Microsoft I

581
00:27:01,719 --> 00:27:04,909
would like to thank you and I'm open to

582
00:27:04,909 --> 00:27:07,070
your interesting questions now if you

583
00:27:07,070 --> 00:27:09,619
want to read the full paper it's on my

584
00:27:09,619 --> 00:27:11,859
web page and we have our research code

585
00:27:11,859 --> 00:27:15,289
on github the code for this paper is not

586
00:27:15,289 --> 00:27:17,359
on github yet I'm waiting to hear back

587
00:27:17,359 --> 00:27:20,330
from the journal and after that we will

588
00:27:20,330 --> 00:27:22,519
just publish it and you can always check

589
00:27:22,519 --> 00:27:25,700
our blog for new findings and for the

590
00:27:25,700 --> 00:27:27,560
shorter version of the paper

591
00:27:27,560 --> 00:27:31,750
with a summary of it thank you very much

592
00:27:39,479 --> 00:27:42,519
Thank You Eileen so we come to the

593
00:27:42,519 --> 00:27:44,830
questions and answers we have six

594
00:27:44,830 --> 00:27:47,019
microphones that we can use now it's

595
00:27:47,019 --> 00:27:50,379
this one this one number five over there

596
00:27:50,379 --> 00:27:53,019
six four two and I will start here and

597
00:27:53,019 --> 00:27:56,859
we will go around until you come okay we

598
00:27:56,859 --> 00:28:06,729
have five minutes so number one please I

599
00:28:06,729 --> 00:28:10,869
might very naively ask what or why does

600
00:28:10,869 --> 00:28:13,059
it matter that there is a bias between

601
00:28:13,059 --> 00:28:17,399
genders first of all being able to

602
00:28:17,399 --> 00:28:21,460
uncover this is a contribution because

603
00:28:21,460 --> 00:28:24,279
we can see what kind of biases maybe we

604
00:28:24,279 --> 00:28:27,159
have in society then the other thing is

605
00:28:27,159 --> 00:28:30,399
maybe we can hypothesize that the way we

606
00:28:30,399 --> 00:28:33,849
learn language is introducing bias to

607
00:28:33,849 --> 00:28:37,960
people maybe it's all intermingled and

608
00:28:37,960 --> 00:28:41,229
the other thing is at least for me I

609
00:28:41,229 --> 00:28:43,299
don't want to live in a biased society

610
00:28:43,299 --> 00:28:45,339
and especially for gender that was the

611
00:28:45,339 --> 00:28:48,399
question you asked it's leading to

612
00:28:48,399 --> 00:28:51,599
unfairness and

613
00:28:57,400 --> 00:29:01,160
yes number three yeah thank you for the

614
00:29:01,160 --> 00:29:05,270
talk very nice I think it's very

615
00:29:05,270 --> 00:29:09,160
dangerous because it's victory of

616
00:29:09,160 --> 00:29:13,040
mediocracy is just the statistical mean

617
00:29:13,040 --> 00:29:16,520
will be the guideline of our goals and

618
00:29:16,520 --> 00:29:18,830
society and all this stuff so what about

619
00:29:18,830 --> 00:29:21,559
all these different cultures like even

620
00:29:21,559 --> 00:29:24,049
in in normal society is you have

621
00:29:24,049 --> 00:29:25,910
different cultures like here the culture

622
00:29:25,910 --> 00:29:29,390
of the chaos people is different has a

623
00:29:29,390 --> 00:29:31,040
different language and different biases

624
00:29:31,040 --> 00:29:33,590
and other cultures how can we preserve

625
00:29:33,590 --> 00:29:36,590
these subcultures these small groups of

626
00:29:36,590 --> 00:29:38,240
language

627
00:29:38,240 --> 00:29:42,110
I don't know entities any idea this is a

628
00:29:42,110 --> 00:29:44,419
very good question and it's similar to

629
00:29:44,419 --> 00:29:46,540
like different cultures can have

630
00:29:46,540 --> 00:29:48,919
different ethical perspectives or

631
00:29:48,919 --> 00:29:53,330
different types of bias and in the

632
00:29:53,330 --> 00:29:55,370
beginning I showed a slide that we need

633
00:29:55,370 --> 00:29:58,010
to do bias with positive examples and we

634
00:29:58,010 --> 00:29:59,840
need to change things at the structural

635
00:29:59,840 --> 00:30:02,540
level and I think people at cccc might

636
00:30:02,540 --> 00:30:07,160
be like one of the like most one of one

637
00:30:07,160 --> 00:30:08,929
of the groups that have the best skills

638
00:30:08,929 --> 00:30:11,929
to help change these things at the

639
00:30:11,929 --> 00:30:14,030
structural level especially for machines

640
00:30:14,030 --> 00:30:15,679
so I think we need to be aware of this

641
00:30:15,679 --> 00:30:17,840
and always have a human in the look that

642
00:30:17,840 --> 00:30:20,480
cares for this instead of expecting

643
00:30:20,480 --> 00:30:22,130
machines to automatically do the

644
00:30:22,130 --> 00:30:24,710
correcting so we always need an ethical

645
00:30:24,710 --> 00:30:26,660
human whatever the purpose of the

646
00:30:26,660 --> 00:30:29,120
algorithm is try to preserve it for

647
00:30:29,120 --> 00:30:31,990
whatever group they are trying to

648
00:30:31,990 --> 00:30:39,410
achieve something with number four

649
00:30:39,410 --> 00:30:42,110
number four please hi thank you

650
00:30:42,110 --> 00:30:46,700
this was really interesting super

651
00:30:46,700 --> 00:30:49,760
awesome the early early earlier in your

652
00:30:49,760 --> 00:30:51,970
talk you described a process of

653
00:30:51,970 --> 00:30:55,630
converting words into a sort of

654
00:30:55,630 --> 00:30:57,799
numerical representation of semantic

655
00:30:57,799 --> 00:30:59,370
meaning

656
00:30:59,370 --> 00:31:01,660
[Music]

657
00:31:01,660 --> 00:31:04,430
yes if I were if I were trying to do

658
00:31:04,430 --> 00:31:06,830
that like with a pen and paper with a

659
00:31:06,830 --> 00:31:09,230
body of language what would I be looking

660
00:31:09,230 --> 00:31:11,840
for in relation to those words to try

661
00:31:11,840 --> 00:31:15,320
and create those vectors because I don't

662
00:31:15,320 --> 00:31:17,330
really like understand that part of that

663
00:31:17,330 --> 00:31:19,520
yes yeah that's a good question I didn't

664
00:31:19,520 --> 00:31:21,590
go into the details of the algorithm of

665
00:31:21,590 --> 00:31:23,270
the neural network or the regression

666
00:31:23,270 --> 00:31:25,550
model so there are a few algorithms and

667
00:31:25,550 --> 00:31:27,340
in this case they basically look at

668
00:31:27,340 --> 00:31:30,800
context windows and the words that are

669
00:31:30,800 --> 00:31:32,960
around the window this can be skip grams

670
00:31:32,960 --> 00:31:35,270
or continuous bag of words or there are

671
00:31:35,270 --> 00:31:37,640
different approaches but basically it's

672
00:31:37,640 --> 00:31:40,850
the window that this word appears in and

673
00:31:40,850 --> 00:31:43,490
what is it most frequently associated

674
00:31:43,490 --> 00:31:47,810
with and after that once you feed this

675
00:31:47,810 --> 00:31:49,910
information to the algorithm it outputs

676
00:31:49,910 --> 00:31:54,740
the numerical vectors thank you now

677
00:31:54,740 --> 00:31:59,120
number two thank you for a nice

678
00:31:59,120 --> 00:32:02,660
intellectual talk my mother tongue is

679
00:32:02,660 --> 00:32:06,530
genderless too so I do not understand

680
00:32:06,530 --> 00:32:10,220
half that biasing thing around here in

681
00:32:10,220 --> 00:32:17,150
Europe what I wanted to ask is when we

682
00:32:17,150 --> 00:32:21,440
have the coefficient 0.5 and that's the

683
00:32:21,440 --> 00:32:25,730
ideal thing what do you think so T

684
00:32:25,730 --> 00:32:29,210
should there be a institution in every

685
00:32:29,210 --> 00:32:32,870
society trying to change the meaning of

686
00:32:32,870 --> 00:32:35,720
the words so that they statistically

687
00:32:35,720 --> 00:32:40,880
approach to 0.5 thank you thank you very

688
00:32:40,880 --> 00:32:42,890
much this is a very very good question

689
00:32:42,890 --> 00:32:44,330
and I'm currently working on these

690
00:32:44,330 --> 00:32:48,350
questions many philosopher or feminist

691
00:32:48,350 --> 00:32:50,780
philosophers suggest that languages are

692
00:32:50,780 --> 00:32:54,080
dominated by males and they were just

693
00:32:54,080 --> 00:32:57,980
produced that way so that women are not

694
00:32:57,980 --> 00:33:00,200
able to express themselves as well as

695
00:33:00,200 --> 00:33:02,930
men but other theories also say that for

696
00:33:02,930 --> 00:33:05,450
example women were the ones who drove

697
00:33:05,450 --> 00:33:07,400
the evolution of language so it's not

698
00:33:07,400 --> 00:33:10,190
very clear what is going on here but

699
00:33:10,190 --> 00:33:13,010
when we look at languages and different

700
00:33:13,010 --> 00:33:14,029
models when

701
00:33:14,029 --> 00:33:15,979
I'm trying to see their association with

702
00:33:15,979 --> 00:33:18,799
gender I'm seeing that the most frequent

703
00:33:18,799 --> 00:33:20,509
for example two hundred thousand words

704
00:33:20,509 --> 00:33:23,629
in a language are associated very

705
00:33:23,629 --> 00:33:26,210
closely associated with males I'm not

706
00:33:26,210 --> 00:33:28,580
sure what exactly the way to solve this

707
00:33:28,580 --> 00:33:31,099
is I think it would require decades and

708
00:33:31,099 --> 00:33:33,409
it's basically the change of frequency

709
00:33:33,409 --> 00:33:35,869
or the change of statistics in language

710
00:33:35,869 --> 00:33:38,389
because even when children are learning

711
00:33:38,389 --> 00:33:40,580
language at first they see tanks they

712
00:33:40,580 --> 00:33:43,399
form the semantics and after that they

713
00:33:43,399 --> 00:33:45,710
see the frequency of that word match it

714
00:33:45,710 --> 00:33:47,799
with the semantics for form clusters

715
00:33:47,799 --> 00:33:49,969
linked them together to form sentences

716
00:33:49,969 --> 00:33:53,119
or grammar so even children look at the

717
00:33:53,119 --> 00:33:54,950
frequency to form this in their brains

718
00:33:54,950 --> 00:33:56,479
it's close to the neural network

719
00:33:56,479 --> 00:33:58,789
algorithm that we have so if the

720
00:33:58,789 --> 00:34:01,789
frequency they see for men and women are

721
00:34:01,789 --> 00:34:03,979
biased I don't think this can change

722
00:34:03,979 --> 00:34:06,679
very easily so we need cultural and

723
00:34:06,679 --> 00:34:09,199
structural changes and we don't have the

724
00:34:09,199 --> 00:34:11,690
answers to these yet and these are very

725
00:34:11,690 --> 00:34:14,960
good research questions thank you I'm

726
00:34:14,960 --> 00:34:17,839
afraid we have no more time left for

727
00:34:17,839 --> 00:34:20,569
more answers but maybe you can ask your

728
00:34:20,569 --> 00:34:22,399
question soon yes thank you very much I

729
00:34:22,399 --> 00:34:24,230
will take questions offline

730
00:34:24,230 --> 00:34:31,989
[Applause]

731
00:34:31,989 --> 00:34:57,020
[Music]

