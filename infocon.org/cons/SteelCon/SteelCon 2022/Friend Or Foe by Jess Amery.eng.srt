1
00:00:03,360 --> 00:00:04,960
right hello everyone

2
00:00:04,960 --> 00:00:07,359
thank you very much for coming along and

3
00:00:07,359 --> 00:00:09,519
i really mean that because there is some

4
00:00:09,519 --> 00:00:11,440
incredible people talking today and some

5
00:00:11,440 --> 00:00:14,080
amazing talks so the fact that you all

6
00:00:14,080 --> 00:00:16,079
took the time out of your day to come

7
00:00:16,079 --> 00:00:18,160
and see me talk today genuinely

8
00:00:18,160 --> 00:00:20,720
means quite a lot and this presentation

9
00:00:20,720 --> 00:00:22,800
was marked as pg when i was writing it i

10
00:00:22,800 --> 00:00:24,720
realized some things might not be

11
00:00:24,720 --> 00:00:27,199
entirely pg and so i have put a little

12
00:00:27,199 --> 00:00:29,599
content warning in there if anyone wants

13
00:00:29,599 --> 00:00:32,159
to to leave now i would not be offended

14
00:00:32,159 --> 00:00:35,120
there will be um themes such as online

15
00:00:35,120 --> 00:00:36,960
crime extortion

16
00:00:36,960 --> 00:00:40,239
um suicide etc mentioned during this

17
00:00:40,239 --> 00:00:44,719
presentation nothing too graphic though

18
00:00:44,719 --> 00:00:46,079
um right so what are we going to be

19
00:00:46,079 --> 00:00:48,559
covering off today um i'm first of all

20
00:00:48,559 --> 00:00:51,680
going to look a little bit um into

21
00:00:51,680 --> 00:00:55,120
the kind of current threat landscape um

22
00:00:55,120 --> 00:00:57,520
current um kind of kind of ongoings

23
00:00:57,520 --> 00:01:00,480
online in terms of how fake profiles are

24
00:01:00,480 --> 00:01:03,280
being used um in the wild and we're then

25
00:01:03,280 --> 00:01:04,640
going to play a little game of spot the

26
00:01:04,640 --> 00:01:06,799
difference so everyone get your arms at

27
00:01:06,799 --> 00:01:10,320
the ready um to get involved in that one

28
00:01:10,320 --> 00:01:12,479
about spotting the difference between a

29
00:01:12,479 --> 00:01:14,960
real and an artificially generated face

30
00:01:14,960 --> 00:01:16,400
sounds easy we'll see how we get on

31
00:01:16,400 --> 00:01:17,759
though and we're then going to have a

32
00:01:17,759 --> 00:01:19,680
look at um what we know is gans so for

33
00:01:19,680 --> 00:01:21,520
anyone who doesn't know that generative

34
00:01:21,520 --> 00:01:23,759
adversarial networks um

35
00:01:23,759 --> 00:01:25,520
basically how do we create these fake

36
00:01:25,520 --> 00:01:27,040
faces that we're going to be looking at

37
00:01:27,040 --> 00:01:28,560
then we're going to look at a solution

38
00:01:28,560 --> 00:01:29,360
so

39
00:01:29,360 --> 00:01:31,840
is it possible computationally to tell

40
00:01:31,840 --> 00:01:33,280
the difference between

41
00:01:33,280 --> 00:01:35,840
a legitimate or a artificially generated

42
00:01:35,840 --> 00:01:37,680
face

43
00:01:37,680 --> 00:01:40,479
so before we move on um who am i i think

44
00:01:40,479 --> 00:01:42,399
there's a few familiar faces in the

45
00:01:42,399 --> 00:01:44,240
audience and but for those who don't

46
00:01:44,240 --> 00:01:46,960
know me my name is jess not jessica and

47
00:01:46,960 --> 00:01:48,560
if you call me jessica i'll

48
00:01:48,560 --> 00:01:51,360
probably look away um

49
00:01:51,360 --> 00:01:52,640
if you want to follow me on twitter my

50
00:01:52,640 --> 00:01:55,600
twitter is there um i know that

51
00:01:55,600 --> 00:01:57,119
there might be some stuff here where

52
00:01:57,119 --> 00:01:58,640
people want to follow up or just want to

53
00:01:58,640 --> 00:02:01,119
have a chat follow me there um currently

54
00:02:01,119 --> 00:02:02,719
i am the

55
00:02:02,719 --> 00:02:05,680
interim sock lead and um threat intel

56
00:02:05,680 --> 00:02:08,239
lead at the weir group again for anyone

57
00:02:08,239 --> 00:02:10,639
who doesn't know um the the best way to

58
00:02:10,639 --> 00:02:12,640
describe what weird does is we take big

59
00:02:12,640 --> 00:02:14,959
rocks out the ground and mine them and

60
00:02:14,959 --> 00:02:17,120
build them into big things mined an

61
00:02:17,120 --> 00:02:19,440
engineering company and started their

62
00:02:19,440 --> 00:02:21,280
last year but this presentation in no

63
00:02:21,280 --> 00:02:23,520
way um kind of usual disclaimer this is

64
00:02:23,520 --> 00:02:25,280
my views not my employers

65
00:02:25,280 --> 00:02:27,599
um 2020 ethical hacking graduate at

66
00:02:27,599 --> 00:02:31,200
abertay in dundee um

67
00:02:31,200 --> 00:02:33,360
where security is formed if anyone's

68
00:02:33,360 --> 00:02:35,360
ever been to security another great

69
00:02:35,360 --> 00:02:38,640
conference and on kind of shameless plug

70
00:02:38,640 --> 00:02:40,640
there and i was the ex-president up

71
00:02:40,640 --> 00:02:42,319
there of the society of organized

72
00:02:42,319 --> 00:02:44,400
security so maybe slightly biased on

73
00:02:44,400 --> 00:02:45,599
that one

74
00:02:45,599 --> 00:02:48,879
privacy advocate lover of dash hounds

75
00:02:48,879 --> 00:02:51,280
nothing like a good dog and also a

76
00:02:51,280 --> 00:02:53,760
disclaimer here not machine um learning

77
00:02:53,760 --> 00:02:57,200
expert just like machine learning

78
00:02:57,200 --> 00:02:59,360
there's definitely people who are so

79
00:02:59,360 --> 00:03:01,200
experienced in this field and understand

80
00:03:01,200 --> 00:03:04,080
all the the background maths and and the

81
00:03:04,080 --> 00:03:06,959
kind of complex beneath the surface

82
00:03:06,959 --> 00:03:08,400
and

83
00:03:08,400 --> 00:03:10,640
kind of calculations and

84
00:03:10,640 --> 00:03:12,239
code that that's part of these

85
00:03:12,239 --> 00:03:14,720
algorithms this for me was a bit more of

86
00:03:14,720 --> 00:03:16,720
a passion project to be honest and the

87
00:03:16,720 --> 00:03:18,239
reason i put that photo at the side

88
00:03:18,239 --> 00:03:20,480
there is because even though i graduated

89
00:03:20,480 --> 00:03:23,519
in 2020 and thank you to covid's i

90
00:03:23,519 --> 00:03:25,440
actually only got my graduation ceremony

91
00:03:25,440 --> 00:03:27,120
last week so

92
00:03:27,120 --> 00:03:29,200
felt like i needed to shout that out

93
00:03:29,200 --> 00:03:33,360
thank you thank you thank you

94
00:03:33,360 --> 00:03:36,080
um and on that point a story of this

95
00:03:36,080 --> 00:03:38,239
research so this research was actually

96
00:03:38,239 --> 00:03:40,239
um the work that i completed as part of

97
00:03:40,239 --> 00:03:42,959
my undergraduate thesis um in my final

98
00:03:42,959 --> 00:03:44,560
year so

99
00:03:44,560 --> 00:03:46,959
describe the tale and pictures um i was

100
00:03:46,959 --> 00:03:49,120
at uni having the best time with my

101
00:03:49,120 --> 00:03:50,879
friends my best friend lucy's here today

102
00:03:50,879 --> 00:03:52,159
um

103
00:03:52,159 --> 00:03:53,680
having a great time going out on a

104
00:03:53,680 --> 00:03:55,599
tuesday night smiling and then all of a

105
00:03:55,599 --> 00:03:56,799
sudden

106
00:03:56,799 --> 00:04:00,400
coveted um during my final year my final

107
00:04:00,400 --> 00:04:02,480
term and we all went to working from

108
00:04:02,480 --> 00:04:04,480
home so i completed uni working from

109
00:04:04,480 --> 00:04:06,400
home and suddenly what was going to be a

110
00:04:06,400 --> 00:04:08,560
really cool and interesting final year

111
00:04:08,560 --> 00:04:10,799
um turned into

112
00:04:10,799 --> 00:04:12,000
not what i thought it was going to be

113
00:04:12,000 --> 00:04:14,480
anyway and i um submitted my

114
00:04:14,480 --> 00:04:16,478
dissertation from my bedroom at my mom's

115
00:04:16,478 --> 00:04:17,279
house

116
00:04:17,279 --> 00:04:19,279
but we moved we got our we got our

117
00:04:19,279 --> 00:04:20,798
dissertation done now and we finally

118
00:04:20,798 --> 00:04:22,320
graduated which is great um special

119
00:04:22,320 --> 00:04:24,960
thanks before we move on as well to dr

120
00:04:24,960 --> 00:04:27,199
natalie cool and dr

121
00:04:27,199 --> 00:04:29,759
saunas cavanaugh and abertay university

122
00:04:29,759 --> 00:04:31,199
as well and

123
00:04:31,199 --> 00:04:33,520
they really supported this project so we

124
00:04:33,520 --> 00:04:34,960
wouldn't have happened at all without

125
00:04:34,960 --> 00:04:36,160
them

126
00:04:36,160 --> 00:04:37,360
so

127
00:04:37,360 --> 00:04:40,320
background and how can fake profiles use

128
00:04:40,320 --> 00:04:41,680
be used online

129
00:04:41,680 --> 00:04:43,680
probably sounds like quite a simple

130
00:04:43,680 --> 00:04:46,240
question especially for everyone here um

131
00:04:46,240 --> 00:04:49,680
but nevertheless um so daniel perry

132
00:04:49,680 --> 00:04:51,440
17 years old

133
00:04:51,440 --> 00:04:53,040
amanda todd

134
00:04:53,040 --> 00:04:54,880
15 years old

135
00:04:54,880 --> 00:04:56,560
ronan hughes

136
00:04:56,560 --> 00:04:58,639
17 years old

137
00:04:58,639 --> 00:05:00,240
what do these

138
00:05:00,240 --> 00:05:02,240
very young people have in common and

139
00:05:02,240 --> 00:05:04,160
these young people um according to a

140
00:05:04,160 --> 00:05:05,840
study that was completed

141
00:05:05,840 --> 00:05:07,520
um by the crimes against children

142
00:05:07,520 --> 00:05:09,520
research center

143
00:05:09,520 --> 00:05:12,320
and the university of new hampshire

144
00:05:12,320 --> 00:05:14,479
found that these um three young people

145
00:05:14,479 --> 00:05:17,440
were all subject um

146
00:05:17,440 --> 00:05:19,440
to

147
00:05:19,440 --> 00:05:21,039
to what we can only be described as very

148
00:05:21,039 --> 00:05:24,080
sophisticated um social engineering so

149
00:05:24,080 --> 00:05:26,000
all three of these teenagers were and

150
00:05:26,000 --> 00:05:28,960
befriended by individuals online that

151
00:05:28,960 --> 00:05:31,360
were not who they said they were

152
00:05:31,360 --> 00:05:32,800
these people were

153
00:05:32,800 --> 00:05:35,280
pressured into um

154
00:05:35,280 --> 00:05:38,080
or lured rather into sharing images and

155
00:05:38,080 --> 00:05:41,199
and videos of themselves and they were

156
00:05:41,199 --> 00:05:43,600
then threatened and

157
00:05:43,600 --> 00:05:45,840
by those who befriended them

158
00:05:45,840 --> 00:05:48,320
that if they did not pay quite a

159
00:05:48,320 --> 00:05:49,759
substantial sum of money that these

160
00:05:49,759 --> 00:05:53,840
images would be um shared publicly

161
00:05:53,840 --> 00:05:56,240
all three of these teenagers very sadly

162
00:05:56,240 --> 00:05:58,319
commit suicide as a result of these

163
00:05:58,319 --> 00:05:59,840
events

164
00:05:59,840 --> 00:06:02,080
that's the reality of extortion online

165
00:06:02,080 --> 00:06:03,440
this is what is happening with these

166
00:06:03,440 --> 00:06:05,520
fake profiles and and what this study

167
00:06:05,520 --> 00:06:10,880
found is that 55 of participants um who

168
00:06:10,880 --> 00:06:11,840
who

169
00:06:11,840 --> 00:06:13,360
kind of were subject to these sort of

170
00:06:13,360 --> 00:06:15,840
acts stated that those who they were

171
00:06:15,840 --> 00:06:18,000
speaking to online and gave them a false

172
00:06:18,000 --> 00:06:20,319
impression of who they were so these are

173
00:06:20,319 --> 00:06:22,319
not real people for the most part these

174
00:06:22,319 --> 00:06:24,560
are these are people who are falsifying

175
00:06:24,560 --> 00:06:26,800
their images they are

176
00:06:26,800 --> 00:06:28,160
pretending to be younger pretending to

177
00:06:28,160 --> 00:06:32,240
be older changing their profile photos

178
00:06:33,280 --> 00:06:35,440
i've got a quick video here um if it

179
00:06:35,440 --> 00:06:38,240
decides to work

180
00:06:38,319 --> 00:06:39,680
the sound might not be it should have

181
00:06:39,680 --> 00:06:42,000
sir

182
00:06:42,080 --> 00:06:44,000
but

183
00:06:44,000 --> 00:06:46,000
created this fake video of president

184
00:06:46,000 --> 00:06:48,160
obama to demonstrate how easy it is to

185
00:06:48,160 --> 00:06:50,319
put words in someone else's mouth moving

186
00:06:50,319 --> 00:06:52,240
forward we need to be more vigilant with

187
00:06:52,240 --> 00:06:54,080
what we trust from the internet not

188
00:06:54,080 --> 00:06:55,919
everyone bought it but the technology

189
00:06:55,919 --> 00:06:58,080
behind such frauds is rapidly improving

190
00:06:58,080 --> 00:06:59,759
even as worries increase about their

191
00:06:59,759 --> 00:07:01,520
potential for harm this is your

192
00:07:01,520 --> 00:07:04,880
bloomberg quick take on deep fakes

193
00:07:04,880 --> 00:07:06,800
deep fakes or realistic looking fake

194
00:07:06,800 --> 00:07:08,960
videos and audio gained popularity as a

195
00:07:08,960 --> 00:07:10,720
means of adding famous actresses into

196
00:07:10,720 --> 00:07:12,720
porn scenes despite bans on major

197
00:07:12,720 --> 00:07:14,720
websites they remain easy to make and

198
00:07:14,720 --> 00:07:15,759
find

199
00:07:15,759 --> 00:07:17,199
they're named for the deep learning

200
00:07:17,199 --> 00:07:19,199
artificial intelligence algorithms that

201
00:07:19,199 --> 00:07:21,599
make them possible input real audio or

202
00:07:21,599 --> 00:07:24,000
video of a specific person the more the

203
00:07:24,000 --> 00:07:25,599
better then the software tries to

204
00:07:25,599 --> 00:07:27,280
recognize patterns in speech and

205
00:07:27,280 --> 00:07:29,280
movement introduce a new element like

206
00:07:29,280 --> 00:07:31,440
someone else's face or voice and a deep

207
00:07:31,440 --> 00:07:33,120
fake is born it's actually extremely

208
00:07:33,120 --> 00:07:35,120
easy to make one of these things there

209
00:07:35,120 --> 00:07:36,160
were just some

210
00:07:36,160 --> 00:07:37,440
supposed you know breakthroughs from

211
00:07:37,440 --> 00:07:39,759
academic researchers who

212
00:07:39,759 --> 00:07:41,039
work with this particular kind of

213
00:07:41,039 --> 00:07:42,960
machine learning in the past few weeks

214
00:07:42,960 --> 00:07:44,800
which would drastically reduce the

215
00:07:44,800 --> 00:07:46,400
amount of video you need actually to

216
00:07:46,400 --> 00:07:48,000
create one of these programs like fake

217
00:07:48,000 --> 00:07:49,759
app the most popular and widely

218
00:07:49,759 --> 00:07:51,759
available for making deep fakes need

219
00:07:51,759 --> 00:07:53,599
dozens of hours of human assistance to

220
00:07:53,599 --> 00:07:56,160
create a video that looks like this

221
00:07:56,160 --> 00:07:57,759
rather than this

222
00:07:57,759 --> 00:07:59,840
in august researchers at carnegie mellon

223
00:07:59,840 --> 00:08:01,440
revealed software that accurately

224
00:08:01,440 --> 00:08:03,840
rendered not just facial features but

225
00:08:03,840 --> 00:08:05,840
changing weather patterns

226
00:08:05,840 --> 00:08:07,520
and flowers in bloom

227
00:08:07,520 --> 00:08:09,520
this advance is not yet available to the

228
00:08:09,520 --> 00:08:11,919
public but with increasing capability

229
00:08:11,919 --> 00:08:14,080
comes increasing concern you know this

230
00:08:14,080 --> 00:08:15,599
is kind of fake news on steroids

231
00:08:15,599 --> 00:08:18,400
potentially um we do not know of a case

232
00:08:18,400 --> 00:08:20,400
yet where someone has tried to use this

233
00:08:20,400 --> 00:08:23,360
to perpetrate a kind of fraud or an

234
00:08:23,360 --> 00:08:26,319
information warfare campaign or or for

235
00:08:26,319 --> 00:08:28,639
that matter to really damage someone's

236
00:08:28,639 --> 00:08:30,080
reputation

237
00:08:30,080 --> 00:08:31,919
but it's the danger that everyone is

238
00:08:31,919 --> 00:08:33,919
really afraid of in a world where fakes

239
00:08:33,919 --> 00:08:36,479
are easy to create authenticity also

240
00:08:36,479 --> 00:08:38,799
becomes easier to deny people caught

241
00:08:38,799 --> 00:08:40,559
doing genuinely objectionable things

242
00:08:40,559 --> 00:08:42,000
could claim evidence against them is

243
00:08:42,000 --> 00:08:43,039
bogus

244
00:08:43,039 --> 00:08:45,120
fake videos can also be difficult to

245
00:08:45,120 --> 00:08:46,880
detect though researchers around the

246
00:08:46,880 --> 00:08:48,399
world and at the u.s department of

247
00:08:48,399 --> 00:08:49,839
defense have said they're working on

248
00:08:49,839 --> 00:08:52,080
ways to counter them deep fakes do

249
00:08:52,080 --> 00:08:54,399
however have some positive uses take

250
00:08:54,399 --> 00:08:56,480
sarah proc a firm that creates digital

251
00:08:56,480 --> 00:08:58,240
voices for people who lose theirs from

252
00:08:58,240 --> 00:09:00,320
disease speech synthesis is the

253
00:09:00,320 --> 00:09:03,360
artificial production of human speech

254
00:09:03,360 --> 00:09:05,120
there are also applications that could

255
00:09:05,120 --> 00:09:08,080
be considered either good or bad like

256
00:09:08,080 --> 00:09:10,160
the many many deep fakes that exist

257
00:09:10,160 --> 00:09:11,839
solely to turn as many movies as

258
00:09:11,839 --> 00:09:16,320
possible into nicholas cage movies

259
00:09:17,600 --> 00:09:19,519
right okay so i i thought it was quite

260
00:09:19,519 --> 00:09:21,920
important to to put that in there and

261
00:09:21,920 --> 00:09:23,600
i'm sure we've all seen

262
00:09:23,600 --> 00:09:25,920
deep fakes somewhere right whether

263
00:09:25,920 --> 00:09:26,880
that's

264
00:09:26,880 --> 00:09:30,160
your best friend your mum your dad

265
00:09:30,160 --> 00:09:32,800
sharing a video on facebook of nicholas

266
00:09:32,800 --> 00:09:35,440
cage or barack obama

267
00:09:35,440 --> 00:09:38,800
doing something um that they

268
00:09:38,800 --> 00:09:41,839
might not usually right um

269
00:09:41,839 --> 00:09:44,320
so moving on from there

270
00:09:44,320 --> 00:09:46,000
beyond deep faith so deep fakes

271
00:09:46,000 --> 00:09:48,240
themselves traditionally right now we're

272
00:09:48,240 --> 00:09:49,440
looking at

273
00:09:49,440 --> 00:09:50,399
altering

274
00:09:50,399 --> 00:09:53,120
images that exist of legitimate

275
00:09:53,120 --> 00:09:55,920
characters and quite often popular

276
00:09:55,920 --> 00:09:58,160
characters and politicians movie stars

277
00:09:58,160 --> 00:10:00,160
etc but let me introduce you to my

278
00:10:00,160 --> 00:10:01,839
friend katie jones here

279
00:10:01,839 --> 00:10:03,200
hello katie

280
00:10:03,200 --> 00:10:05,360
so katie m has a degree in russian

281
00:10:05,360 --> 00:10:08,320
studies from michigan university um

282
00:10:08,320 --> 00:10:12,399
katie is employed at the center and for

283
00:10:12,399 --> 00:10:14,399
strategic international studies and

284
00:10:14,399 --> 00:10:17,519
katie has a growing linkedin um network

285
00:10:17,519 --> 00:10:20,240
currently sitting at 49 connections

286
00:10:20,240 --> 00:10:21,920
but what if i told you that katie jones

287
00:10:21,920 --> 00:10:24,720
didn't actually exist

288
00:10:24,720 --> 00:10:27,360
so um in 2019 the associated press

289
00:10:27,360 --> 00:10:29,040
published an article reporting that miss

290
00:10:29,040 --> 00:10:30,640
katy jones here

291
00:10:30,640 --> 00:10:32,959
um

292
00:10:32,959 --> 00:10:35,680
was actually an artificially generated

293
00:10:35,680 --> 00:10:37,839
image and katie jones's face does not

294
00:10:37,839 --> 00:10:39,920
exist kay jones is not a person cajuns

295
00:10:39,920 --> 00:10:42,000
it's not human um

296
00:10:42,000 --> 00:10:45,279
but katie jones had um started to build

297
00:10:45,279 --> 00:10:47,120
up quite an impressive network on

298
00:10:47,120 --> 00:10:49,360
linkedin connecting with prominent

299
00:10:49,360 --> 00:10:53,120
members of dc think tanks um including

300
00:10:53,120 --> 00:10:55,680
the former deputy director of president

301
00:10:55,680 --> 00:10:58,880
donald trump's domestic policy council

302
00:10:58,880 --> 00:11:02,640
so while there's a lot of speculation um

303
00:11:02,640 --> 00:11:04,640
about miss katie jones and and what her

304
00:11:04,640 --> 00:11:05,760
aim was

305
00:11:05,760 --> 00:11:09,200
um it's likely that this was part of um

306
00:11:09,200 --> 00:11:11,120
a kind of wider scale cyber espionage

307
00:11:11,120 --> 00:11:13,440
campaign and this is in the wild one of

308
00:11:13,440 --> 00:11:15,519
the first examples that we have have

309
00:11:15,519 --> 00:11:17,920
seen or first publicly known examples

310
00:11:17,920 --> 00:11:20,640
where an artificially generated face is

311
00:11:20,640 --> 00:11:22,640
being used as part of social engineering

312
00:11:22,640 --> 00:11:24,880
campaigns

313
00:11:24,880 --> 00:11:26,640
so like i said get your get your arms at

314
00:11:26,640 --> 00:11:27,920
the ready so now we've introduced you to

315
00:11:27,920 --> 00:11:30,399
katie let's see how we would um respond

316
00:11:30,399 --> 00:11:32,320
when it comes to spotting the difference

317
00:11:32,320 --> 00:11:34,640
between a legitimate face and on our

318
00:11:34,640 --> 00:11:37,600
officially january face so

319
00:11:37,600 --> 00:11:40,320
right hands up if you think this

320
00:11:40,320 --> 00:11:42,959
face is real if you think that this lady

321
00:11:42,959 --> 00:11:46,239
is real hands up

322
00:11:46,320 --> 00:11:47,760
right

323
00:11:47,760 --> 00:11:50,079
okay right and hands up if you think

324
00:11:50,079 --> 00:11:51,760
she's fake

325
00:11:51,760 --> 00:11:53,839
okay so more people think she's real

326
00:11:53,839 --> 00:11:56,000
bye

327
00:11:56,000 --> 00:11:59,040
she's fake she she she's she's not a

328
00:11:59,040 --> 00:12:01,360
real person um

329
00:12:01,360 --> 00:12:02,320
and

330
00:12:02,320 --> 00:12:06,079
some of this is actually getting beyond

331
00:12:06,079 --> 00:12:08,320
what it was the technology that um is

332
00:12:08,320 --> 00:12:09,920
behind generating these images is

333
00:12:09,920 --> 00:12:12,240
improving at such a fast rate previously

334
00:12:12,240 --> 00:12:13,920
you might not be able to include things

335
00:12:13,920 --> 00:12:17,279
like um earrings or sunglasses

336
00:12:17,279 --> 00:12:20,480
that that's rapidly changing

337
00:12:20,480 --> 00:12:25,279
so let's move on to our next one

338
00:12:25,279 --> 00:12:28,000
take a yes right hands up if you think

339
00:12:28,000 --> 00:12:30,560
that this lovely lady here is

340
00:12:30,560 --> 00:12:32,880
real

341
00:12:33,680 --> 00:12:35,120
okay now we're not so sure we're not too

342
00:12:35,120 --> 00:12:36,720
sure right okay and hands up you think

343
00:12:36,720 --> 00:12:39,120
she's fake and hands up if you just

344
00:12:39,120 --> 00:12:41,200
don't know if you're not sure

345
00:12:41,200 --> 00:12:43,279
okay right

346
00:12:43,279 --> 00:12:47,680
she is fake she has a fake face

347
00:12:50,320 --> 00:12:53,360
this little girl right real

348
00:12:53,360 --> 00:12:55,920
hands up

349
00:12:56,720 --> 00:12:58,800
fake

350
00:12:58,800 --> 00:13:00,639
not so sure

351
00:13:00,639 --> 00:13:03,360
most people are not so sure yep

352
00:13:03,360 --> 00:13:05,360
she's actually fake and as part of the

353
00:13:05,360 --> 00:13:07,040
research that i'd done i looked at how

354
00:13:07,040 --> 00:13:10,079
people responded to these faces and what

355
00:13:10,079 --> 00:13:12,079
we've done is and we took

356
00:13:12,079 --> 00:13:14,639
quite a large number of participants and

357
00:13:14,639 --> 00:13:16,639
we provided them a series of images two

358
00:13:16,639 --> 00:13:18,240
faces at once and got them to select

359
00:13:18,240 --> 00:13:19,839
which one they thought was real which

360
00:13:19,839 --> 00:13:21,760
they thought was fake um

361
00:13:21,760 --> 00:13:23,760
and we will come on to the results of

362
00:13:23,760 --> 00:13:26,320
that shortly however what we did find is

363
00:13:26,320 --> 00:13:28,880
that quite often when it came to

364
00:13:28,880 --> 00:13:31,200
um kind of children uh and those that

365
00:13:31,200 --> 00:13:32,560
were maybe a wee bit younger or

366
00:13:32,560 --> 00:13:34,480
specifically those who are a little bit

367
00:13:34,480 --> 00:13:36,160
older people find it harder to tell the

368
00:13:36,160 --> 00:13:37,839
difference right because in your head

369
00:13:37,839 --> 00:13:40,720
you're going well surely not surely not

370
00:13:40,720 --> 00:13:42,959
it's almost hard to believe that that

371
00:13:42,959 --> 00:13:46,959
that is not a real human face right

372
00:13:47,680 --> 00:13:48,880
next one

373
00:13:48,880 --> 00:13:49,920
real

374
00:13:49,920 --> 00:13:51,279
hands up

375
00:13:51,279 --> 00:13:53,920
what do you think

376
00:13:54,000 --> 00:13:56,399
fake

377
00:13:56,399 --> 00:13:58,959
okay right not got clay

378
00:13:58,959 --> 00:14:00,560
people right yeah we're still we're

379
00:14:00,560 --> 00:14:02,320
still a little bit unsure that's a real

380
00:14:02,320 --> 00:14:03,920
man

381
00:14:03,920 --> 00:14:06,320
this is a real man um and i would like

382
00:14:06,320 --> 00:14:08,000
to find out that all the um real faces

383
00:14:08,000 --> 00:14:09,360
that are going to be used during in this

384
00:14:09,360 --> 00:14:11,199
presentation were taken from a

385
00:14:11,199 --> 00:14:12,880
legitimate online repository where you

386
00:14:12,880 --> 00:14:14,240
do have permission to take them i'm not

387
00:14:14,240 --> 00:14:15,120
just

388
00:14:15,120 --> 00:14:17,600
a strange person who's taking photos off

389
00:14:17,600 --> 00:14:19,519
people's facebook profiles but

390
00:14:19,519 --> 00:14:23,519
moving on right our next man

391
00:14:23,519 --> 00:14:26,000
what do we think real fake i can see a

392
00:14:26,000 --> 00:14:27,600
little bit of discussion talk amongst

393
00:14:27,600 --> 00:14:29,680
yourselves if you like you should

394
00:14:29,680 --> 00:14:32,880
right let's hands up for real

395
00:14:34,000 --> 00:14:36,399
hands up for fake

396
00:14:36,399 --> 00:14:37,519
okay we're starting to do a little bit

397
00:14:37,519 --> 00:14:38,560
better

398
00:14:38,560 --> 00:14:40,800
not not too sure

399
00:14:40,800 --> 00:14:43,760
okay yep so this man is actually is

400
00:14:43,760 --> 00:14:45,120
actually fake so he's done he's done

401
00:14:45,120 --> 00:14:46,720
quite well in that one and i don't know

402
00:14:46,720 --> 00:14:48,320
if there was any kind of telltale signs

403
00:14:48,320 --> 00:14:51,519
that anyone wants to point out

404
00:14:53,279 --> 00:14:54,240
yep

405
00:14:54,240 --> 00:14:56,000
anyway you wouldn't your eyes get drawn

406
00:14:56,000 --> 00:14:59,040
to your side the neck

407
00:14:59,680 --> 00:15:02,870
[Music]

408
00:15:07,839 --> 00:15:08,959
feel like we're all going to be coming

409
00:15:08,959 --> 00:15:10,560
out this talk very and kind of

410
00:15:10,560 --> 00:15:13,199
criticizing everyone's face

411
00:15:13,199 --> 00:15:15,040
looking at their nostrils and but yep

412
00:15:15,040 --> 00:15:16,639
correct this this man is indeed a fake

413
00:15:16,639 --> 00:15:18,480
face and the interesting um point to

414
00:15:18,480 --> 00:15:19,519
make with

415
00:15:19,519 --> 00:15:22,320
with this image um i specifically took

416
00:15:22,320 --> 00:15:24,560
this using the

417
00:15:24,560 --> 00:15:27,120
new generator that's available so it's

418
00:15:27,120 --> 00:15:28,079
supposed to look a little bit more

419
00:15:28,079 --> 00:15:30,079
realistic and compared to the past

420
00:15:30,079 --> 00:15:32,720
images kind of a little bit more i don't

421
00:15:32,720 --> 00:15:34,800
want to say disheveled and that's that's

422
00:15:34,800 --> 00:15:36,800
going to be quite harsh um but less kind

423
00:15:36,800 --> 00:15:38,959
of airbrushed more legit amid human

424
00:15:38,959 --> 00:15:40,720
human kind of features on their facial

425
00:15:40,720 --> 00:15:44,160
hair wrinkles and so on

426
00:15:44,160 --> 00:15:45,440
okay so

427
00:15:45,440 --> 00:15:46,480
when we look at the results of the

428
00:15:46,480 --> 00:15:49,279
surgery that was taken 42

429
00:15:49,279 --> 00:15:51,759
of survey participants they found a

430
00:15:51,759 --> 00:15:54,000
differentiating these images difficult i

431
00:15:54,000 --> 00:15:55,279
don't blame them

432
00:15:55,279 --> 00:15:57,519
most people did find it difficult and if

433
00:15:57,519 --> 00:15:59,519
we go and look into that in a bit more

434
00:15:59,519 --> 00:16:03,600
detail and these participants got asked

435
00:16:03,600 --> 00:16:05,440
and what their technical competency was

436
00:16:05,440 --> 00:16:08,320
so based on technical competency how did

437
00:16:08,320 --> 00:16:11,279
people find it and if we look at um

438
00:16:11,279 --> 00:16:13,199
on average um

439
00:16:13,199 --> 00:16:14,959
those with lower technical competency

440
00:16:14,959 --> 00:16:16,880
are those who assess themselves as

441
00:16:16,880 --> 00:16:19,199
having lower technical competency they

442
00:16:19,199 --> 00:16:22,000
found it harder to tell the images apart

443
00:16:22,000 --> 00:16:25,040
apart from one novice user who claimed

444
00:16:25,040 --> 00:16:26,800
it was really super easy but spoiler

445
00:16:26,800 --> 00:16:30,160
they actually got them all wrong so

446
00:16:30,639 --> 00:16:33,680
that tells a nice story there um

447
00:16:33,680 --> 00:16:35,600
general trend though

448
00:16:35,600 --> 00:16:38,959
technical competency did not have

449
00:16:38,959 --> 00:16:41,519
much of a sway on on how people were

450
00:16:41,519 --> 00:16:43,680
actually able to classify these images

451
00:16:43,680 --> 00:16:46,480
so let's look at results so success rate

452
00:16:46,480 --> 00:16:49,199
as you can see across the board not a

453
00:16:49,199 --> 00:16:52,320
lot of difference and you've got kind of

454
00:16:52,320 --> 00:16:54,560
your highest success rate sitting about

455
00:16:54,560 --> 00:16:57,360
46 percent and your lowest success rate

456
00:16:57,360 --> 00:17:00,079
they're sitting about 39

457
00:17:00,079 --> 00:17:02,720
so so it's not it's not high right and

458
00:17:02,720 --> 00:17:04,400
that's your success rate so more often

459
00:17:04,400 --> 00:17:06,400
than not people were getting these these

460
00:17:06,400 --> 00:17:10,000
questions wrong people were struggling

461
00:17:10,000 --> 00:17:10,720
so

462
00:17:10,720 --> 00:17:13,119
in summary it's really hard

463
00:17:13,119 --> 00:17:14,880
it's really really hard we've got to

464
00:17:14,880 --> 00:17:16,559
remember that the technology behind

465
00:17:16,559 --> 00:17:18,480
creating these images is vastly

466
00:17:18,480 --> 00:17:21,280
improving it's becoming more complex

467
00:17:21,280 --> 00:17:23,199
so while it's hard now

468
00:17:23,199 --> 00:17:24,400
tomorrow and the next day it's going to

469
00:17:24,400 --> 00:17:27,199
be even harder right

470
00:17:27,199 --> 00:17:29,840
so let's look a little bit about um at

471
00:17:29,840 --> 00:17:31,679
the how so

472
00:17:31,679 --> 00:17:35,760
for those who aren't aware of this site

473
00:17:35,760 --> 00:17:38,559
browse to it have a play about

474
00:17:38,559 --> 00:17:42,320
it's quite fun and you can generate a

475
00:17:42,320 --> 00:17:44,480
legitimate face and with a click of a

476
00:17:44,480 --> 00:17:48,080
button so this person does not exist.com

477
00:17:48,080 --> 00:17:51,280
and this person does not exist.com is

478
00:17:51,280 --> 00:17:53,760
powered by an algorithm that we know is

479
00:17:53,760 --> 00:17:56,240
stylegam

480
00:17:56,799 --> 00:17:58,880
so what do we miss telegram what is a

481
00:17:58,880 --> 00:18:00,960
generative adversarial network

482
00:18:00,960 --> 00:18:03,440
so dr jason brownlee once said and he's

483
00:18:03,440 --> 00:18:05,280
a very very smart man

484
00:18:05,280 --> 00:18:07,760
phd and does a lot of kind of open

485
00:18:07,760 --> 00:18:09,120
source writing about machine learning

486
00:18:09,120 --> 00:18:11,280
helping and very inexperienced

487
00:18:11,280 --> 00:18:13,120
programmers like me try to understand

488
00:18:13,120 --> 00:18:15,280
the complexities that are going on

489
00:18:15,280 --> 00:18:17,360
in the background and he once said that

490
00:18:17,360 --> 00:18:19,520
generative adversarial networks are an

491
00:18:19,520 --> 00:18:21,600
approach to generative modelling using

492
00:18:21,600 --> 00:18:23,200
deep learning methods such as neural

493
00:18:23,200 --> 00:18:25,280
networks

494
00:18:25,280 --> 00:18:28,160
right lots of big words in there that i

495
00:18:28,160 --> 00:18:30,080
did not understand at first so let's

496
00:18:30,080 --> 00:18:31,280
break it down

497
00:18:31,280 --> 00:18:34,000
once again so generative modeling itself

498
00:18:34,000 --> 00:18:35,760
is an unsupervised learning task so it's

499
00:18:35,760 --> 00:18:38,320
a task that we can leave a computer to

500
00:18:38,320 --> 00:18:40,160
perform by itself

501
00:18:40,160 --> 00:18:42,080
and it involves discovering and

502
00:18:42,080 --> 00:18:45,120
detecting patterns to form new examples

503
00:18:45,120 --> 00:18:47,520
so what we're saying is if we feed a

504
00:18:47,520 --> 00:18:49,760
generative adversarial network

505
00:18:49,760 --> 00:18:51,919
a large data set what it will do is it

506
00:18:51,919 --> 00:18:53,120
will go through

507
00:18:53,120 --> 00:18:54,720
that data whatever that is whether it's

508
00:18:54,720 --> 00:18:56,960
images whether it's text whatever in

509
00:18:56,960 --> 00:18:58,720
this case we're talking images

510
00:18:58,720 --> 00:19:00,640
we'll look at these images and it will

511
00:19:00,640 --> 00:19:02,799
detect patterns abnormalities and it

512
00:19:02,799 --> 00:19:04,080
will use that

513
00:19:04,080 --> 00:19:06,799
to create new images right so

514
00:19:06,799 --> 00:19:09,200
these guns are performed and or made up

515
00:19:09,200 --> 00:19:11,120
rather of two components and the

516
00:19:11,120 --> 00:19:14,559
generator and the discriminator so the

517
00:19:14,559 --> 00:19:17,679
generator that's what we train so

518
00:19:17,679 --> 00:19:20,240
we train this generator to make new

519
00:19:20,240 --> 00:19:22,320
examples right

520
00:19:22,320 --> 00:19:24,320
and the discriminator of what it will do

521
00:19:24,320 --> 00:19:27,840
is it tries to classify if these images

522
00:19:27,840 --> 00:19:30,960
in this case are real so from the the

523
00:19:30,960 --> 00:19:33,360
domain that we provided or fake come

524
00:19:33,360 --> 00:19:35,280
from the generator

525
00:19:35,280 --> 00:19:36,720
and what we do and we're training

526
00:19:36,720 --> 00:19:38,480
training again is we say once that

527
00:19:38,480 --> 00:19:40,960
success rate is a is above kind of

528
00:19:40,960 --> 00:19:43,440
50 so we're able to trick that that

529
00:19:43,440 --> 00:19:46,000
discriminator network more than 50 of

530
00:19:46,000 --> 00:19:47,039
the time

531
00:19:47,039 --> 00:19:48,720
no it's not too bad it's pretty much

532
00:19:48,720 --> 00:19:50,160
trained right and the closer we can get

533
00:19:50,160 --> 00:19:52,640
that's 100 the closer we can get

534
00:19:52,640 --> 00:19:54,080
to tricking

535
00:19:54,080 --> 00:19:55,760
that discriminator network into not

536
00:19:55,760 --> 00:19:57,520
being sure over what's part of the

537
00:19:57,520 --> 00:19:59,200
original data set what's part of the new

538
00:19:59,200 --> 00:20:02,559
data set that's what we want right

539
00:20:03,200 --> 00:20:05,840
a meme for your for your pleasure i felt

540
00:20:05,840 --> 00:20:07,200
quite impressed myself when i started to

541
00:20:07,200 --> 00:20:09,360
understand machine learning memes

542
00:20:09,360 --> 00:20:10,960
and so

543
00:20:10,960 --> 00:20:12,559
when you're training again you do kind

544
00:20:12,559 --> 00:20:14,480
of feel a bit like the overlord because

545
00:20:14,480 --> 00:20:16,559
you're going ha ha

546
00:20:16,559 --> 00:20:19,280
i am playing both of you so that we all

547
00:20:19,280 --> 00:20:23,918
do well here we all do well um

548
00:20:24,000 --> 00:20:25,760
if we go on to look a little bit about

549
00:20:25,760 --> 00:20:28,320
um style gun so still gun specifically

550
00:20:28,320 --> 00:20:31,760
it was developed by nvidia in 2019 and

551
00:20:31,760 --> 00:20:34,400
it demonstrates the unsupervised

552
00:20:34,400 --> 00:20:36,880
learning of high-level human attributes

553
00:20:36,880 --> 00:20:40,400
with stochastic and variation so what we

554
00:20:40,400 --> 00:20:43,440
mean by that is high-level attributes

555
00:20:43,440 --> 00:20:45,440
by and by right i know we're probably

556
00:20:45,440 --> 00:20:46,640
all going to come out here and be

557
00:20:46,640 --> 00:20:48,400
looking very in detail at each other's

558
00:20:48,400 --> 00:20:51,200
faces but um high level human attributes

559
00:20:51,200 --> 00:20:54,000
we mean every face has a nose every face

560
00:20:54,000 --> 00:20:56,159
has an eye a set of eyes every face has

561
00:20:56,159 --> 00:20:58,480
ears so on and so forth by stochastic

562
00:20:58,480 --> 00:21:01,039
variation what we mean as um some people

563
00:21:01,039 --> 00:21:03,360
might have freckles right some people

564
00:21:03,360 --> 00:21:04,559
might have green eyes some people might

565
00:21:04,559 --> 00:21:05,679
have blue eye

566
00:21:05,679 --> 00:21:07,039
some people might have bushy eyebrows

567
00:21:07,039 --> 00:21:08,960
some people might have an eyebrow

568
00:21:08,960 --> 00:21:10,159
depending

569
00:21:10,159 --> 00:21:12,480
um

570
00:21:12,480 --> 00:21:14,240
so that's what stylegum does it takes

571
00:21:14,240 --> 00:21:16,080
these high level attributes and it

572
00:21:16,080 --> 00:21:17,760
implements differences in them so you

573
00:21:17,760 --> 00:21:20,400
get a different face with different

574
00:21:20,400 --> 00:21:23,039
kind of known features every single time

575
00:21:23,039 --> 00:21:25,039
readily available technology like i said

576
00:21:25,039 --> 00:21:28,400
before um not only does it power this

577
00:21:28,400 --> 00:21:30,799
person does not exist.com but you can

578
00:21:30,799 --> 00:21:32,960
also go and have a play about with it on

579
00:21:32,960 --> 00:21:35,360
um github i would really recommend that

580
00:21:35,360 --> 00:21:36,880
if anyone comes out this presentation

581
00:21:36,880 --> 00:21:37,840
thinks this is actually really

582
00:21:37,840 --> 00:21:40,080
interesting the research is all there

583
00:21:40,080 --> 00:21:41,919
and they're very open about how how

584
00:21:41,919 --> 00:21:43,360
they've done it you can go in you can

585
00:21:43,360 --> 00:21:45,600
run the script yourself you can create

586
00:21:45,600 --> 00:21:47,120
your own fake face you can look at the

587
00:21:47,120 --> 00:21:49,120
code that they've used to do it so put a

588
00:21:49,120 --> 00:21:51,440
little link in there for anyone who is

589
00:21:51,440 --> 00:21:54,000
interested

590
00:21:54,000 --> 00:21:57,520
so let's look at kind of at a high level

591
00:21:57,520 --> 00:22:00,799
what the research says is this formula

592
00:22:00,799 --> 00:22:03,200
right x equals gen

593
00:22:03,200 --> 00:22:05,440
z and i don't mean gen z in there gen of

594
00:22:05,440 --> 00:22:06,159
z

595
00:22:06,159 --> 00:22:07,120
um

596
00:22:07,120 --> 00:22:09,600
and what we're saying there is where x

597
00:22:09,600 --> 00:22:11,280
equals the image is created so that's

598
00:22:11,280 --> 00:22:12,480
our output

599
00:22:12,480 --> 00:22:14,960
and g equals our lovely generator

600
00:22:14,960 --> 00:22:17,679
network that we have spoke through and z

601
00:22:17,679 --> 00:22:20,080
equals the latent features of the images

602
00:22:20,080 --> 00:22:21,919
created so hypothetically what we're

603
00:22:21,919 --> 00:22:23,840
saying is that combining the output of

604
00:22:23,840 --> 00:22:26,000
the generator um against the latent

605
00:22:26,000 --> 00:22:28,799
features of the images and generated

606
00:22:28,799 --> 00:22:32,000
you'll form a new data set

607
00:22:32,960 --> 00:22:34,320
so let's look at the proposed solution

608
00:22:34,320 --> 00:22:36,159
like i say we've got we've got a bit of

609
00:22:36,159 --> 00:22:38,159
a problem here because

610
00:22:38,159 --> 00:22:40,640
this technology that that's behind

611
00:22:40,640 --> 00:22:42,480
generating these images it's

612
00:22:42,480 --> 00:22:44,799
improving at such a vast rate and it's

613
00:22:44,799 --> 00:22:47,520
only a matter of time as um

614
00:22:47,520 --> 00:22:49,679
that the video that i showed earlier

615
00:22:49,679 --> 00:22:52,480
alluded to that this is going to be used

616
00:22:52,480 --> 00:22:55,120
in malice we have seen some images in

617
00:22:55,120 --> 00:22:56,880
the wild so far but it's only about our

618
00:22:56,880 --> 00:22:58,960
time before people click on to

619
00:22:58,960 --> 00:23:00,880
why would i use barack obama's face when

620
00:23:00,880 --> 00:23:02,480
i can create an entirely new persona

621
00:23:02,480 --> 00:23:04,640
something that comes to mind to me and

622
00:23:04,640 --> 00:23:06,480
apologies if this is incredibly high

623
00:23:06,480 --> 00:23:08,640
level for folks i don't know if anyone

624
00:23:08,640 --> 00:23:12,000
remembers the tv show catfish by mtv

625
00:23:12,000 --> 00:23:12,799
yeah

626
00:23:12,799 --> 00:23:13,679
yeah

627
00:23:13,679 --> 00:23:14,880
and then they would sit there and they

628
00:23:14,880 --> 00:23:17,280
would reverse image search the

629
00:23:17,280 --> 00:23:19,840
profile photo and it would come up right

630
00:23:19,840 --> 00:23:22,159
because they more often than not would

631
00:23:22,159 --> 00:23:24,880
take photos from someone else um

632
00:23:24,880 --> 00:23:26,880
and that was quite a telltale self sign

633
00:23:26,880 --> 00:23:29,200
for them on whether or not that profile

634
00:23:29,200 --> 00:23:32,159
was legitimate if you used a fake human

635
00:23:32,159 --> 00:23:34,480
face one that does not exist does not

636
00:23:34,480 --> 00:23:36,640
have an online persona and could create

637
00:23:36,640 --> 00:23:39,360
a new one at the click of a button

638
00:23:39,360 --> 00:23:41,120
coupled with the deep fake technology

639
00:23:41,120 --> 00:23:43,279
that we've now got to merge those human

640
00:23:43,279 --> 00:23:46,320
faces onto videos why would you not

641
00:23:46,320 --> 00:23:48,080
right like why would you use something

642
00:23:48,080 --> 00:23:49,679
that you might get caught

643
00:23:49,679 --> 00:23:51,440
versus something that's readily

644
00:23:51,440 --> 00:23:53,679
available

645
00:23:53,679 --> 00:23:55,679
so what i've tried to do within this

646
00:23:55,679 --> 00:23:57,760
research is build up quite a lightweight

647
00:23:57,760 --> 00:23:59,120
model that

648
00:23:59,120 --> 00:24:01,120
detects between what is real and what's

649
00:24:01,120 --> 00:24:02,400
fake

650
00:24:02,400 --> 00:24:05,039
so a little overview of it here and it

651
00:24:05,039 --> 00:24:08,880
was written in python um using the keras

652
00:24:08,880 --> 00:24:11,279
deep learning api

653
00:24:11,279 --> 00:24:12,640
the reason that we use this is because

654
00:24:12,640 --> 00:24:15,039
it provides a whole host of native

655
00:24:15,039 --> 00:24:17,200
features and that

656
00:24:17,200 --> 00:24:19,039
is easily accessible for those who are

657
00:24:19,039 --> 00:24:22,400
not familiar with the deep workings of

658
00:24:22,400 --> 00:24:24,240
machine learning and deep learning

659
00:24:24,240 --> 00:24:25,919
something that to be quite honest with

660
00:24:25,919 --> 00:24:28,000
you is quite good for beginners um

661
00:24:28,000 --> 00:24:29,520
something that's quite good if you don't

662
00:24:29,520 --> 00:24:32,159
want to spend too much time having to

663
00:24:32,159 --> 00:24:33,760
build out your own functions and it's

664
00:24:33,760 --> 00:24:34,880
something that's readily available it's

665
00:24:34,880 --> 00:24:36,480
open source it's free

666
00:24:36,480 --> 00:24:40,159
um vgg16 as a base model and that was

667
00:24:40,159 --> 00:24:43,200
pre-trained using the imagenet and

668
00:24:43,200 --> 00:24:44,960
database so the reason i use the base

669
00:24:44,960 --> 00:24:46,400
model in this case

670
00:24:46,400 --> 00:24:47,919
um

671
00:24:47,919 --> 00:24:51,279
trading your own deep learning model for

672
00:24:51,279 --> 00:24:52,720
for like an image classification would

673
00:24:52,720 --> 00:24:55,039
take millions of images i did not have

674
00:24:55,039 --> 00:24:56,880
millions of images so

675
00:24:56,880 --> 00:24:58,640
what i done was that i took a

676
00:24:58,640 --> 00:25:01,120
pre-trained base model but i looked at

677
00:25:01,120 --> 00:25:02,720
building a new classifier on the top of

678
00:25:02,720 --> 00:25:03,600
it so

679
00:25:03,600 --> 00:25:06,720
imagenet um is essentially a classifier

680
00:25:06,720 --> 00:25:10,559
that's built off of um wordnet where

681
00:25:10,559 --> 00:25:12,000
phrases and words are split into

682
00:25:12,000 --> 00:25:14,080
meaningful categories and imagenet

683
00:25:14,080 --> 00:25:15,520
assigns

684
00:25:15,520 --> 00:25:17,679
images to these categories

685
00:25:17,679 --> 00:25:19,440
and then these categories are basically

686
00:25:19,440 --> 00:25:22,159
sorted and labeled and then used to

687
00:25:22,159 --> 00:25:25,120
train a base model

688
00:25:25,120 --> 00:25:28,559
training set i had forty thousand images

689
00:25:28,559 --> 00:25:30,320
for trainings that was twenty thousand

690
00:25:30,320 --> 00:25:32,799
real twenty thousand fake um test and

691
00:25:32,799 --> 00:25:35,360
set of four thousand again split in half

692
00:25:35,360 --> 00:25:38,159
and an evaluation set of one thousand

693
00:25:38,159 --> 00:25:41,279
again split in half and use style gun to

694
00:25:41,279 --> 00:25:43,600
generate the fake images so

695
00:25:43,600 --> 00:25:46,400
um i referenced the github site earlier

696
00:25:46,400 --> 00:25:49,120
on there and just ran the scripts

697
00:25:49,120 --> 00:25:51,120
managed to get as many fake images as i

698
00:25:51,120 --> 00:25:52,799
would like um

699
00:25:52,799 --> 00:25:55,120
flickr faces hq for

700
00:25:55,120 --> 00:25:57,279
real faces and like i say if anyone

701
00:25:57,279 --> 00:25:59,760
wants to

702
00:25:59,760 --> 00:26:00,720
um

703
00:26:00,720 --> 00:26:02,559
volunteer their facebook profile picture

704
00:26:02,559 --> 00:26:05,039
i would be open but um no it is really

705
00:26:05,039 --> 00:26:07,200
really very difficult to get

706
00:26:07,200 --> 00:26:10,159
large data sets um that are kind of

707
00:26:10,159 --> 00:26:13,120
signed off by university ethics to use

708
00:26:13,120 --> 00:26:16,480
as part of um training in this case

709
00:26:16,480 --> 00:26:18,640
um trained on a macbook pro

710
00:26:18,640 --> 00:26:20,720
and i'll come on to that a little bit

711
00:26:20,720 --> 00:26:22,799
later and this was supposed to be

712
00:26:22,799 --> 00:26:25,919
trained on a far larger more powerful

713
00:26:25,919 --> 00:26:28,400
computer um that

714
00:26:28,400 --> 00:26:30,559
became unavailable because of covedon

715
00:26:30,559 --> 00:26:32,240
working from home so it was trained on

716
00:26:32,240 --> 00:26:34,240
my macbook pro bear that in mind when

717
00:26:34,240 --> 00:26:36,000
you look at the results

718
00:26:36,000 --> 00:26:37,039
um

719
00:26:37,039 --> 00:26:39,679
and how the image and classifier very

720
00:26:39,679 --> 00:26:42,559
high level worked uh worked from

721
00:26:42,559 --> 00:26:44,400
extracting features for images and then

722
00:26:44,400 --> 00:26:47,760
applying binary labels as an output

723
00:26:47,760 --> 00:26:49,600
some dogs because when i was writing

724
00:26:49,600 --> 00:26:51,440
this um presentation at this point i

725
00:26:51,440 --> 00:26:52,720
felt

726
00:26:52,720 --> 00:26:55,679
we might need some some dogs so just

727
00:26:55,679 --> 00:26:57,600
um

728
00:26:57,600 --> 00:27:00,400
for no other reason no other reason than

729
00:27:00,400 --> 00:27:01,840
um just

730
00:27:01,840 --> 00:27:04,000
just cause are they real

731
00:27:04,000 --> 00:27:06,320
they are they're real dogs i i promise i

732
00:27:06,320 --> 00:27:08,320
promise you and

733
00:27:08,320 --> 00:27:10,720
we've got lola and then we've got marley

734
00:27:10,720 --> 00:27:13,760
and the dashing so yeah these are all

735
00:27:13,760 --> 00:27:16,799
real dogs i promise

736
00:27:16,799 --> 00:27:19,919
so let's look at um the solution that we

737
00:27:19,919 --> 00:27:22,399
decided to build so i ended up building

738
00:27:22,399 --> 00:27:24,399
two solutions and the first being

739
00:27:24,399 --> 00:27:27,360
optimized to run on a cpu

740
00:27:27,360 --> 00:27:29,840
um so like i say these classifiers they

741
00:27:29,840 --> 00:27:31,760
leveraged feature extraction and

742
00:27:31,760 --> 00:27:35,760
pre-trained on the vgd 16 base and

743
00:27:35,760 --> 00:27:39,279
neural network model

744
00:27:39,679 --> 00:27:41,279
and essentially what the feature

745
00:27:41,279 --> 00:27:44,960
extraction consisted of in this instance

746
00:27:44,960 --> 00:27:47,600
was using representations learned from

747
00:27:47,600 --> 00:27:49,840
that previous network to

748
00:27:49,840 --> 00:27:52,559
form some new samples and and to look at

749
00:27:52,559 --> 00:27:54,159
reclassifying the images that it was

750
00:27:54,159 --> 00:27:55,279
being fed

751
00:27:55,279 --> 00:27:57,200
and

752
00:27:57,200 --> 00:28:00,480
we then passed that new data set through

753
00:28:00,480 --> 00:28:02,640
the base and built that classifier on

754
00:28:02,640 --> 00:28:05,600
the top and the extracted features of

755
00:28:05,600 --> 00:28:09,440
the images were then stored and used to

756
00:28:09,440 --> 00:28:12,640
identify what is a typical real and what

757
00:28:12,640 --> 00:28:14,240
is a typical

758
00:28:14,240 --> 00:28:17,039
fake face

759
00:28:17,039 --> 00:28:18,240
sounds pretty

760
00:28:18,240 --> 00:28:20,320
reasonable right um

761
00:28:20,320 --> 00:28:22,159
this is what the

762
00:28:22,159 --> 00:28:24,720
the results look like um so that very

763
00:28:24,720 --> 00:28:28,840
thick blue line um at the top was

764
00:28:28,840 --> 00:28:32,240
what the model performed at during

765
00:28:32,240 --> 00:28:33,919
training so

766
00:28:33,919 --> 00:28:36,159
performed quite well it achieved

767
00:28:36,159 --> 00:28:38,320
accuracy of about

768
00:28:38,320 --> 00:28:42,000
not .97 which is as close 100 as you're

769
00:28:42,000 --> 00:28:43,360
really gonna get

770
00:28:43,360 --> 00:28:44,640
however

771
00:28:44,640 --> 00:28:47,440
that comes with a bit of a caveat um

772
00:28:47,440 --> 00:28:50,640
when we fed that the testing data um it

773
00:28:50,640 --> 00:28:54,799
dropped by 20 on unseen data and

774
00:28:54,799 --> 00:28:56,640
my current hypothesis is working that if

775
00:28:56,640 --> 00:28:59,039
i fed even more data that would continue

776
00:28:59,039 --> 00:29:02,559
to drop and the model's overfitting um

777
00:29:02,559 --> 00:29:03,360
and

778
00:29:03,360 --> 00:29:05,919
overfitting in machine learning terms is

779
00:29:05,919 --> 00:29:06,880
when

780
00:29:06,880 --> 00:29:09,520
the model learns the detail and the

781
00:29:09,520 --> 00:29:12,720
noise of the training data too much it

782
00:29:12,720 --> 00:29:14,320
becomes too familiar with that training

783
00:29:14,320 --> 00:29:15,600
set you've provided it that when you

784
00:29:15,600 --> 00:29:17,600
provide it new data it just kind of goes

785
00:29:17,600 --> 00:29:18,480
ah

786
00:29:18,480 --> 00:29:22,159
don't know what to do with this um

787
00:29:22,480 --> 00:29:24,720
but that likely came about like i said

788
00:29:24,720 --> 00:29:27,600
as a result of our limited data set um

789
00:29:27,600 --> 00:29:30,080
if i had more images this possibly could

790
00:29:30,080 --> 00:29:32,159
work a little bit better

791
00:29:32,159 --> 00:29:33,360
but i had to go back to the drawing

792
00:29:33,360 --> 00:29:34,159
board

793
00:29:34,159 --> 00:29:37,919
so in the absence of more images

794
00:29:37,919 --> 00:29:39,840
i had to rethink um and i came up with

795
00:29:39,840 --> 00:29:40,720
this

796
00:29:40,720 --> 00:29:42,080
so something that was a bit more

797
00:29:42,080 --> 00:29:43,919
optimized to run on a gpu something that

798
00:29:43,919 --> 00:29:46,960
needed a little bit more power and

799
00:29:46,960 --> 00:29:48,240
worked in

800
00:29:48,240 --> 00:29:50,640
a similar way and so using feature

801
00:29:50,640 --> 00:29:53,679
extraction the same pre-trained and base

802
00:29:53,679 --> 00:29:55,440
model and it's probably worth pointing

803
00:29:55,440 --> 00:29:58,399
out here as well that vgg 16 is

804
00:29:58,399 --> 00:30:00,799
potentially a slightly outdated model

805
00:30:00,799 --> 00:30:03,200
it's it's not very very recent however

806
00:30:03,200 --> 00:30:05,760
it has been documented as yielding

807
00:30:05,760 --> 00:30:08,080
very um

808
00:30:08,080 --> 00:30:09,840
good results which is why we used in

809
00:30:09,840 --> 00:30:11,840
this case and it was also um something

810
00:30:11,840 --> 00:30:13,279
that has a lot of documentation behind

811
00:30:13,279 --> 00:30:14,159
it

812
00:30:14,159 --> 00:30:16,159
but they do have vgg 19 as well which is

813
00:30:16,159 --> 00:30:18,559
a little bit a little bit newer so

814
00:30:18,559 --> 00:30:19,919
um

815
00:30:19,919 --> 00:30:22,399
this model itself will use data

816
00:30:22,399 --> 00:30:24,480
augmentation so

817
00:30:24,480 --> 00:30:26,399
with feature extraction partnered with

818
00:30:26,399 --> 00:30:28,720
the augmentation what we mean by that is

819
00:30:28,720 --> 00:30:31,279
if we take our images and we apply

820
00:30:31,279 --> 00:30:35,039
slight different tweaks on them instead

821
00:30:35,039 --> 00:30:36,640
of getting new images so if we take an

822
00:30:36,640 --> 00:30:38,640
image and we go right let's flip it

823
00:30:38,640 --> 00:30:41,440
let's crop part of it out let's turn it

824
00:30:41,440 --> 00:30:43,279
right around on its head let's warp it a

825
00:30:43,279 --> 00:30:44,799
little bit suddenly you've got a new

826
00:30:44,799 --> 00:30:47,600
data set but without actually gathering

827
00:30:47,600 --> 00:30:49,039
more images

828
00:30:49,039 --> 00:30:52,320
um this model aimed to account for the

829
00:30:52,320 --> 00:30:54,240
small data set that we had

830
00:30:54,240 --> 00:30:57,600
um and again extracted features and used

831
00:30:57,600 --> 00:30:59,919
the binary labels that we previously

832
00:30:59,919 --> 00:31:02,799
identified to classify what is a typical

833
00:31:02,799 --> 00:31:06,399
real and what is a typical fake face

834
00:31:06,399 --> 00:31:08,799
these were the results

835
00:31:08,799 --> 00:31:11,600
which i will explain so

836
00:31:11,600 --> 00:31:14,559
in the training training portion um of

837
00:31:14,559 --> 00:31:19,519
this this research we got about 0.83

838
00:31:19,519 --> 00:31:23,039
ish accuracy rates it's not bad um

839
00:31:23,039 --> 00:31:25,519
testing

840
00:31:25,519 --> 00:31:26,880
unsteady

841
00:31:26,880 --> 00:31:29,519
to say the least um so that line that's

842
00:31:29,519 --> 00:31:31,519
going kind of up and down and up and

843
00:31:31,519 --> 00:31:33,120
down and up and down

844
00:31:33,120 --> 00:31:34,480
um

845
00:31:34,480 --> 00:31:37,760
it wasn't enough to to say that this

846
00:31:37,760 --> 00:31:39,120
model

847
00:31:39,120 --> 00:31:42,159
would work but we we're confident that

848
00:31:42,159 --> 00:31:43,440
if given

849
00:31:43,440 --> 00:31:45,360
more kind of

850
00:31:45,360 --> 00:31:47,679
resource so if i trained this on a

851
00:31:47,679 --> 00:31:49,760
computer that was better than my

852
00:31:49,760 --> 00:31:52,159
six-year-old macvic pro and this would

853
00:31:52,159 --> 00:31:53,600
have likely yielded better results

854
00:31:53,600 --> 00:31:55,600
however what we can see

855
00:31:55,600 --> 00:31:56,960
from these results is that there's less

856
00:31:56,960 --> 00:31:58,320
overfitting

857
00:31:58,320 --> 00:32:01,840
so that model that gap between

858
00:32:01,840 --> 00:32:04,080
kind of what

859
00:32:04,080 --> 00:32:06,559
was um the testing and what was the

860
00:32:06,559 --> 00:32:09,440
validation is kind of slowly starting to

861
00:32:09,440 --> 00:32:12,000
to go down and while we can see that

862
00:32:12,000 --> 00:32:15,440
it's unsteady the model does demonstrate

863
00:32:15,440 --> 00:32:17,360
less evidence of overfit and those lines

864
00:32:17,360 --> 00:32:20,158
are closer together

865
00:32:22,240 --> 00:32:24,480
again right so the ga the gan

866
00:32:24,480 --> 00:32:27,200
performance and there's

867
00:32:27,200 --> 00:32:30,559
a lot of different factors that go into

868
00:32:30,559 --> 00:32:31,840
building

869
00:32:31,840 --> 00:32:34,720
a deep learning model right you can

870
00:32:34,720 --> 00:32:37,120
there's probably boundless and endless

871
00:32:37,120 --> 00:32:38,960
numbers of features and functions you

872
00:32:38,960 --> 00:32:40,799
can tweak you can change you can play

873
00:32:40,799 --> 00:32:42,480
about this

874
00:32:42,480 --> 00:32:45,679
to get that model to as close to 100 as

875
00:32:45,679 --> 00:32:48,159
what you can get

876
00:32:48,159 --> 00:32:51,279
but this is what i focused on right so

877
00:32:51,279 --> 00:32:53,679
long in the short is that it took a lot

878
00:32:53,679 --> 00:32:56,880
of tuning um a lot of a lot of

879
00:32:56,880 --> 00:32:58,960
iterations and but the three kind of

880
00:32:58,960 --> 00:33:00,000
main

881
00:33:00,000 --> 00:33:02,240
variables that i focused on changing

882
00:33:02,240 --> 00:33:04,640
where the number of epochs so

883
00:33:04,640 --> 00:33:06,000
what that means is

884
00:33:06,000 --> 00:33:09,039
essentially the number of iterations ran

885
00:33:09,039 --> 00:33:10,960
through during training

886
00:33:10,960 --> 00:33:12,320
so

887
00:33:12,320 --> 00:33:14,720
that is the number of times that the

888
00:33:14,720 --> 00:33:16,880
algorithm will work through

889
00:33:16,880 --> 00:33:19,440
the entire sample set during training

890
00:33:19,440 --> 00:33:22,080
and this was altered with each prototype

891
00:33:22,080 --> 00:33:24,880
until an optimal solution was achieved

892
00:33:24,880 --> 00:33:26,799
or as close to optimal as we could we

893
00:33:26,799 --> 00:33:30,000
could get um with the model showing as

894
00:33:30,000 --> 00:33:33,039
limited um overfitting or under fitting

895
00:33:33,039 --> 00:33:35,120
as possible

896
00:33:35,120 --> 00:33:39,679
batch size um so batch size refers to

897
00:33:39,679 --> 00:33:41,600
the number of images that are passed

898
00:33:41,600 --> 00:33:44,240
through the model during an iteration

899
00:33:44,240 --> 00:33:47,440
essentially so

900
00:33:47,600 --> 00:33:50,399
a small batch size in this case for this

901
00:33:50,399 --> 00:33:52,640
specific research worked a little bit

902
00:33:52,640 --> 00:33:56,640
better um a small batch size takes less

903
00:33:56,640 --> 00:33:59,919
memory um and typically resulted in

904
00:33:59,919 --> 00:34:02,399
faster training times as the weights

905
00:34:02,399 --> 00:34:05,120
were updated on the model that little

906
00:34:05,120 --> 00:34:07,840
bit quicker and

907
00:34:07,840 --> 00:34:09,839
and the learning rate um arguably in

908
00:34:09,839 --> 00:34:11,918
this case um

909
00:34:11,918 --> 00:34:13,839
the learning rate was definitely the

910
00:34:13,839 --> 00:34:15,679
most important variable

911
00:34:15,679 --> 00:34:16,879
um

912
00:34:16,879 --> 00:34:18,879
note there very important glad i put

913
00:34:18,879 --> 00:34:20,320
that on the slide because it is very

914
00:34:20,320 --> 00:34:22,480
very important and so it controls how

915
00:34:22,480 --> 00:34:24,399
much the model can be

916
00:34:24,399 --> 00:34:26,719
changed in response to error

917
00:34:26,719 --> 00:34:28,719
so we mentioned earlier about those two

918
00:34:28,719 --> 00:34:30,079
kind of generating discriminator

919
00:34:30,079 --> 00:34:31,199
networks that are almost playing a bit

920
00:34:31,199 --> 00:34:32,879
of a cat and mouse game

921
00:34:32,879 --> 00:34:34,560
what we don't want

922
00:34:34,560 --> 00:34:36,879
is if the model goes no no this is wrong

923
00:34:36,879 --> 00:34:38,399
or this is right

924
00:34:38,399 --> 00:34:40,560
for them to suddenly jump up or down

925
00:34:40,560 --> 00:34:42,320
either end of the scale we want only

926
00:34:42,320 --> 00:34:45,520
tiny little changes to be made

927
00:34:45,520 --> 00:34:49,280
to the um the weights and the classifier

928
00:34:49,280 --> 00:34:50,960
each iteration we don't want it to be

929
00:34:50,960 --> 00:34:52,960
jumping up jumping down because that's

930
00:34:52,960 --> 00:34:54,480
not productive for anyone right then

931
00:34:54,480 --> 00:34:55,839
it's just gonna end up jumping up and

932
00:34:55,839 --> 00:34:57,359
down the scale and never really yielding

933
00:34:57,359 --> 00:35:00,079
um very very high results so

934
00:35:00,079 --> 00:35:01,040
um

935
00:35:01,040 --> 00:35:03,760
we used again keras like i mentioned

936
00:35:03,760 --> 00:35:05,520
earlier provides a lot of native

937
00:35:05,520 --> 00:35:07,839
features that are great for easily

938
00:35:07,839 --> 00:35:11,119
implementing um

939
00:35:11,200 --> 00:35:13,119
things like adaptive learning so

940
00:35:13,119 --> 00:35:15,440
adaptive learning and like i say in this

941
00:35:15,440 --> 00:35:17,200
case refers to

942
00:35:17,200 --> 00:35:19,359
the model being able to respond better

943
00:35:19,359 --> 00:35:22,400
to changes within that data set so for

944
00:35:22,400 --> 00:35:24,720
example if we had an image come in where

945
00:35:24,720 --> 00:35:27,359
someone was wearing sunglasses or a

946
00:35:27,359 --> 00:35:31,359
woman had earrings in or um we had very

947
00:35:31,359 --> 00:35:33,520
brightly colored hair we want them to

948
00:35:33,520 --> 00:35:36,480
not treat that as a differentiator but

949
00:35:36,480 --> 00:35:40,880
more integrate that into into our set

950
00:35:41,359 --> 00:35:44,400
in this case we used rms prop um as the

951
00:35:44,400 --> 00:35:47,119
optimizer

952
00:35:47,920 --> 00:35:51,359
so um limitations again

953
00:35:51,359 --> 00:35:53,119
do not ever

954
00:35:53,119 --> 00:35:55,520
try um to train an image classification

955
00:35:55,520 --> 00:35:57,520
model on the macbook pro

956
00:35:57,520 --> 00:35:59,760
i learned the hard way and my macbook

957
00:35:59,760 --> 00:36:01,680
has never been the same

958
00:36:01,680 --> 00:36:04,320
i pretty much fried it um but

959
00:36:04,320 --> 00:36:07,599
it loves to tell the tale um

960
00:36:07,599 --> 00:36:09,359
so like i've mentioned i feel like i

961
00:36:09,359 --> 00:36:12,800
keep i'm probably harping on

962
00:36:12,800 --> 00:36:15,440
the data set was extremely limited

963
00:36:15,440 --> 00:36:17,359
um so

964
00:36:17,359 --> 00:36:18,560
this model

965
00:36:18,560 --> 00:36:21,359
is restricted and the fact that it

966
00:36:21,359 --> 00:36:23,440
demonstrates a novel proof of concept

967
00:36:23,440 --> 00:36:26,400
that is computationally possible to tell

968
00:36:26,400 --> 00:36:28,320
the difference between these images

969
00:36:28,320 --> 00:36:30,560
however in order to validate those

970
00:36:30,560 --> 00:36:33,839
results we would need more images

971
00:36:33,839 --> 00:36:36,240
it is also specific to one algorithm and

972
00:36:36,240 --> 00:36:37,839
this is only looking at style and

973
00:36:37,839 --> 00:36:40,000
generated images and there are more

974
00:36:40,000 --> 00:36:41,920
algorithms out there and i'm sure as

975
00:36:41,920 --> 00:36:44,240
this research progresses there will be

976
00:36:44,240 --> 00:36:46,320
even more and that number will continue

977
00:36:46,320 --> 00:36:48,960
to grow and how

978
00:36:48,960 --> 00:36:51,359
this specific model would respond

979
00:36:51,359 --> 00:36:54,000
to images generated using um different

980
00:36:54,000 --> 00:36:56,000
algorithms

981
00:36:56,000 --> 00:36:58,160
i don't know um i would love to know and

982
00:36:58,160 --> 00:36:59,520
it's definitely an area for this work to

983
00:36:59,520 --> 00:37:02,320
move into in the future

984
00:37:02,320 --> 00:37:03,599
and like i say it was definitely trained

985
00:37:03,599 --> 00:37:05,280
in an environment absolutely not fit for

986
00:37:05,280 --> 00:37:08,880
purpose but we gave it a go

987
00:37:08,880 --> 00:37:09,839
and

988
00:37:09,839 --> 00:37:10,800
so

989
00:37:10,800 --> 00:37:12,880
advancements and style gun 2 has been

990
00:37:12,880 --> 00:37:14,000
released

991
00:37:14,000 --> 00:37:15,520
on this next slide i've got some useful

992
00:37:15,520 --> 00:37:17,920
resources for anyone who

993
00:37:17,920 --> 00:37:19,280
after this wants to do some reading

994
00:37:19,280 --> 00:37:22,320
about this technology how it works um

995
00:37:22,320 --> 00:37:23,760
and some of them really really

996
00:37:23,760 --> 00:37:25,280
interesting academic papers that there

997
00:37:25,280 --> 00:37:28,400
are out there however style gantu and it

998
00:37:28,400 --> 00:37:30,400
demonstrates the ability to

999
00:37:30,400 --> 00:37:33,599
make these images even more lifelike um

1000
00:37:33,599 --> 00:37:35,040
and that quality like i say is

1001
00:37:35,040 --> 00:37:36,720
continuing to improve

1002
00:37:36,720 --> 00:37:39,359
and deep fake technology and image

1003
00:37:39,359 --> 00:37:42,480
manipulation again continuing to advance

1004
00:37:42,480 --> 00:37:43,920
um

1005
00:37:43,920 --> 00:37:44,880
and the long and the short is that

1006
00:37:44,880 --> 00:37:47,359
detection technology and as security

1007
00:37:47,359 --> 00:37:49,280
professionals you must do better to look

1008
00:37:49,280 --> 00:37:52,640
at solutions on how to counteract this

1009
00:37:52,640 --> 00:37:54,400
useful resources and i'll leave this up

1010
00:37:54,400 --> 00:37:56,800
on the screen if anyone wants to

1011
00:37:56,800 --> 00:37:58,720
take some photos and i'm not sure if the

1012
00:37:58,720 --> 00:38:00,480
slides will be going out somewhere i'll

1013
00:38:00,480 --> 00:38:02,400
try to put them somewhere after

1014
00:38:02,400 --> 00:38:04,880
but yep resources are here and

1015
00:38:04,880 --> 00:38:08,320
um like i say i put my twitter i'll

1016
00:38:08,320 --> 00:38:09,760
leave this up for a minute but i have

1017
00:38:09,760 --> 00:38:11,040
put my twitter on the next slide so if

1018
00:38:11,040 --> 00:38:12,560
anyone does ever have any questions or

1019
00:38:12,560 --> 00:38:13,920
want to speak about this research in

1020
00:38:13,920 --> 00:38:15,839
more detail please let me know i could

1021
00:38:15,839 --> 00:38:17,520
probably speak about all day if i if i

1022
00:38:17,520 --> 00:38:20,560
was given the opportunity um perfect i

1023
00:38:20,560 --> 00:38:21,440
will

1024
00:38:21,440 --> 00:38:22,800
move on to questions i know we finished

1025
00:38:22,800 --> 00:38:25,280
up a little bit early there but if

1026
00:38:25,280 --> 00:38:28,240
anyone does have any questions happy to

1027
00:38:28,240 --> 00:38:31,399
know yep

1028
00:38:32,240 --> 00:38:34,560
most of the pictures that you see on

1029
00:38:34,560 --> 00:38:38,119
our social medias

1030
00:38:43,700 --> 00:38:46,759
[Music]

1031
00:38:51,119 --> 00:38:52,480
to be quite honest i think it would

1032
00:38:52,480 --> 00:38:54,960
depend on how good or bad your photoshop

1033
00:38:54,960 --> 00:38:57,599
skills were to be honest um

1034
00:38:57,599 --> 00:38:59,200
so quite a lot of the telltale signs

1035
00:38:59,200 --> 00:39:00,640
when it comes to

1036
00:39:00,640 --> 00:39:02,720
artificially generated images comes to

1037
00:39:02,720 --> 00:39:04,400
facial features and so i know we were

1038
00:39:04,400 --> 00:39:06,160
talking about nostrils earlier um

1039
00:39:06,160 --> 00:39:07,359
nostrils are definitely a good one

1040
00:39:07,359 --> 00:39:08,800
because they tend to be

1041
00:39:08,800 --> 00:39:10,480
slightly maybe warped or pulled to one

1042
00:39:10,480 --> 00:39:13,200
side ears another one and generating

1043
00:39:13,200 --> 00:39:15,119
ears because as much as we think an ear

1044
00:39:15,119 --> 00:39:17,440
is an ear and he was very hard to kind

1045
00:39:17,440 --> 00:39:18,720
of reproduce

1046
00:39:18,720 --> 00:39:21,920
um so if your photoshop skills involved

1047
00:39:21,920 --> 00:39:24,880
kind of tweaking areas of of that kind

1048
00:39:24,880 --> 00:39:26,560
of face that we might typically

1049
00:39:26,560 --> 00:39:29,119
associate with an artificially generated

1050
00:39:29,119 --> 00:39:31,839
face then yes it might end up going into

1051
00:39:31,839 --> 00:39:33,920
the the wrong bucket but if it was just

1052
00:39:33,920 --> 00:39:36,720
standard kind of covering up a spot or

1053
00:39:36,720 --> 00:39:37,920
making your hair look a little bit

1054
00:39:37,920 --> 00:39:39,680
blonder and your tan like a little bit

1055
00:39:39,680 --> 00:39:41,280
darker

1056
00:39:41,280 --> 00:39:44,240
it would probably be fine

1057
00:39:45,119 --> 00:39:48,119
yes

1058
00:39:58,240 --> 00:39:59,599
do you know what i think it depends

1059
00:39:59,599 --> 00:40:01,040
because if you

1060
00:40:01,040 --> 00:40:03,200
hash the image of

1061
00:40:03,200 --> 00:40:04,640
you really need to be looking at the

1062
00:40:04,640 --> 00:40:07,280
characteristics of the image itself

1063
00:40:07,280 --> 00:40:08,560
while it'd be great if we could have

1064
00:40:08,560 --> 00:40:10,800
kind of a central repository where i'm

1065
00:40:10,800 --> 00:40:12,640
assuming you mean as in we have a

1066
00:40:12,640 --> 00:40:14,560
central yeah people could go up and

1067
00:40:14,560 --> 00:40:15,440
search

1068
00:40:15,440 --> 00:40:18,000
this hashed image but

1069
00:40:18,000 --> 00:40:19,599
they're so easy to reproduce that if

1070
00:40:19,599 --> 00:40:21,200
someone went oh my

1071
00:40:21,200 --> 00:40:23,520
fake faces appeared on a

1072
00:40:23,520 --> 00:40:26,240
known site of gan images oh i need to

1073
00:40:26,240 --> 00:40:28,079
create another one all you need to do is

1074
00:40:28,079 --> 00:40:29,839
refresh a web page so

1075
00:40:29,839 --> 00:40:30,800
yep

1076
00:40:30,800 --> 00:40:32,880
absolutely it could definitely feed into

1077
00:40:32,880 --> 00:40:34,880
counteracting it but because the

1078
00:40:34,880 --> 00:40:36,960
technology now is so accessible i think

1079
00:40:36,960 --> 00:40:38,480
that if someone saw their image come up

1080
00:40:38,480 --> 00:40:39,839
there they would just simply create a

1081
00:40:39,839 --> 00:40:42,000
new one

1082
00:40:42,000 --> 00:40:44,560
yes sorry

1083
00:40:44,720 --> 00:40:46,640
so you're looking at human faces to

1084
00:40:46,640 --> 00:40:49,759
discriminate differences

1085
00:40:51,680 --> 00:40:54,160
a reverse engineer the current algorithm

1086
00:40:54,160 --> 00:40:55,680
for the face to look at how they do it

1087
00:40:55,680 --> 00:40:59,119
with the characteristics of the data to

1088
00:40:59,119 --> 00:41:01,040
just generate a huge data set using the

1089
00:41:01,040 --> 00:41:02,560
current methods

1090
00:41:02,560 --> 00:41:06,880
and shove it through same model so that

1091
00:41:13,359 --> 00:41:16,400
so i did consider this um

1092
00:41:16,400 --> 00:41:18,839
within the kind of research model

1093
00:41:18,839 --> 00:41:22,079
however the conclusion that i came to is

1094
00:41:22,079 --> 00:41:24,079
that because there's so many algorithms

1095
00:41:24,079 --> 00:41:25,119
out there

1096
00:41:25,119 --> 00:41:27,119
that if you went down that route it

1097
00:41:27,119 --> 00:41:29,520
would be very very specific to an

1098
00:41:29,520 --> 00:41:31,839
algorithm and while i can hypothesize

1099
00:41:31,839 --> 00:41:34,160
about the classifier that i built may

1100
00:41:34,160 --> 00:41:36,240
not respond as well

1101
00:41:36,240 --> 00:41:38,800
when fed other algorithms if it can

1102
00:41:38,800 --> 00:41:41,040
classify yes this is absolutely a human

1103
00:41:41,040 --> 00:41:42,000
face

1104
00:41:42,000 --> 00:41:43,440
it'll put it it's still into the right

1105
00:41:43,440 --> 00:41:45,599
bracket whereas that kind of middle

1106
00:41:45,599 --> 00:41:48,000
ground of how it'll respond with them

1107
00:41:48,000 --> 00:41:50,000
images generated using new algorithms is

1108
00:41:50,000 --> 00:41:51,920
a bit of a great area but at least we

1109
00:41:51,920 --> 00:41:53,599
can say for certain that yes it

1110
00:41:53,599 --> 00:41:55,680
classifies a human face correctly does

1111
00:41:55,680 --> 00:41:59,078
that make sense

1112
00:42:10,880 --> 00:42:12,880
the algorithm may just focus on the you

1113
00:42:12,880 --> 00:42:15,200
know the fidelity of the human face

1114
00:42:15,200 --> 00:42:16,480
symmetry and

1115
00:42:16,480 --> 00:42:17,839
abnormalities or whatever they want to

1116
00:42:17,839 --> 00:42:19,599
do but they may not focus too much on

1117
00:42:19,599 --> 00:42:22,160
the lighting to the same extent for

1118
00:42:22,160 --> 00:42:25,040
human fibroblasts

1119
00:42:27,359 --> 00:42:29,839
yeah so the algorithm that i put

1120
00:42:29,839 --> 00:42:31,440
together as in the classifier it did

1121
00:42:31,440 --> 00:42:33,839
look at the entire photo it didn't just

1122
00:42:33,839 --> 00:42:35,839
kind of specifically look at

1123
00:42:35,839 --> 00:42:38,240
the eyes the nose it did look at the

1124
00:42:38,240 --> 00:42:40,160
entire scene because what we found was

1125
00:42:40,160 --> 00:42:42,800
that more often than not in artificially

1126
00:42:42,800 --> 00:42:44,400
generated images the background's very

1127
00:42:44,400 --> 00:42:46,560
warped and it's it's very rare that

1128
00:42:46,560 --> 00:42:48,720
you'll be able to get an image that has

1129
00:42:48,720 --> 00:42:51,200
a perfect scenic backdrop that's

1130
00:42:51,200 --> 00:42:54,079
generated by a computer um

1131
00:42:54,079 --> 00:42:58,880
so yep that is entirely kind of in

1132
00:43:10,560 --> 00:43:13,440
using a break and then realized oh it's

1133
00:43:13,440 --> 00:43:16,160
actually a real person

1134
00:43:16,160 --> 00:43:16,880
so

1135
00:43:16,880 --> 00:43:20,000
as in someone has generated a face using

1136
00:43:20,000 --> 00:43:22,480
the algorithm then done a reverse search

1137
00:43:22,480 --> 00:43:24,480
and then realizes an actual person out

1138
00:43:24,480 --> 00:43:27,680
there no i don't i don't think so um as

1139
00:43:27,680 --> 00:43:29,680
far as as far as i know because

1140
00:43:29,680 --> 00:43:32,960
obviously the people don't exist and if

1141
00:43:32,960 --> 00:43:34,480
there is one it would be quite

1142
00:43:34,480 --> 00:43:36,400
interesting to see if someone was

1143
00:43:36,400 --> 00:43:38,319
refreshing the page and they suddenly

1144
00:43:38,319 --> 00:43:40,319
saw my doppelganger that would be pretty

1145
00:43:40,319 --> 00:43:41,920
cool

1146
00:43:41,920 --> 00:43:44,319
yes if we created a way of being able to

1147
00:43:44,319 --> 00:43:47,119
detect these fake photos that i'm

1148
00:43:47,119 --> 00:43:48,319
assuming that would allow us to build a

1149
00:43:48,319 --> 00:43:52,839
better classifier and therefore a better

1150
00:43:52,839 --> 00:43:55,040
photo generator

1151
00:43:55,040 --> 00:43:56,000
yes

1152
00:43:56,000 --> 00:43:57,839
how how do you see that problem being

1153
00:43:57,839 --> 00:43:58,640
sold

1154
00:43:58,640 --> 00:44:01,040
i think that's a problem that comes with

1155
00:44:01,040 --> 00:44:03,359
anything in security isn't it when when

1156
00:44:03,359 --> 00:44:05,359
we work out how to detect uh strain and

1157
00:44:05,359 --> 00:44:07,119
malware for example or we develop a

1158
00:44:07,119 --> 00:44:09,040
really good threat hunt within an

1159
00:44:09,040 --> 00:44:10,720
environment your actors are just going

1160
00:44:10,720 --> 00:44:12,319
to circumvent it

1161
00:44:12,319 --> 00:44:14,640
but again we're relying right now anyway

1162
00:44:14,640 --> 00:44:16,240
because the research is so limited on

1163
00:44:16,240 --> 00:44:18,319
that low hanging fruit on that initial

1164
00:44:18,319 --> 00:44:20,480
kind of classification but yeah

1165
00:44:20,480 --> 00:44:22,160
absolutely definitely definitely a

1166
00:44:22,160 --> 00:44:23,920
problem

1167
00:44:23,920 --> 00:44:27,040
yes i i saw um on twitter i saw the sign

1168
00:44:27,040 --> 00:44:29,599
but began images specifically from

1169
00:44:29,599 --> 00:44:31,200
this person does not exist and they

1170
00:44:31,200 --> 00:44:33,040
showed all of their li eyes were in a

1171
00:44:33,040 --> 00:44:34,400
line they had three pictures in the top

1172
00:44:34,400 --> 00:44:36,319
and then three in the bottom and it

1173
00:44:36,319 --> 00:44:38,079
matched this way and it matched this way

1174
00:44:38,079 --> 00:44:40,640
on all of the pictures exactly the eye

1175
00:44:40,640 --> 00:44:42,480
placement did you have the same issue

1176
00:44:42,480 --> 00:44:44,079
when you were doing this when you were

1177
00:44:44,079 --> 00:44:45,680
generating the fake images where the

1178
00:44:45,680 --> 00:44:47,680
eyes would all line up

1179
00:44:47,680 --> 00:44:51,119
it's in a weirdly symmetric way yeah so

1180
00:44:51,119 --> 00:44:53,760
i did try to

1181
00:44:53,760 --> 00:44:56,319
kind of sense check the faces that were

1182
00:44:56,319 --> 00:44:59,200
coming out of the generator and to make

1183
00:44:59,200 --> 00:45:00,480
sure that they were at least of

1184
00:45:00,480 --> 00:45:02,720
reasonable quality because some that i

1185
00:45:02,720 --> 00:45:04,880
put up they had like an ear for like a

1186
00:45:04,880 --> 00:45:07,359
nose and stuff and

1187
00:45:07,359 --> 00:45:09,599
which was not great and so i did do some

1188
00:45:09,599 --> 00:45:11,520
sense checking but yeah without a doubt

1189
00:45:11,520 --> 00:45:12,800
because the data set was so big i

1190
00:45:12,800 --> 00:45:13,839
couldn't manually go through every

1191
00:45:13,839 --> 00:45:16,720
single one but i didn't find too many

1192
00:45:16,720 --> 00:45:18,720
issues within it and to be honest i

1193
00:45:18,720 --> 00:45:20,800
think it depends on what algorithm used

1194
00:45:20,800 --> 00:45:22,880
to generate them what machine you

1195
00:45:22,880 --> 00:45:24,480
generate them on

1196
00:45:24,480 --> 00:45:25,760
um

1197
00:45:25,760 --> 00:45:27,520
and a lot of those kind of

1198
00:45:27,520 --> 00:45:29,680
hyper parameters that we discussed

1199
00:45:29,680 --> 00:45:31,119
there's so many variations that you can

1200
00:45:31,119 --> 00:45:33,280
do so i didn't specifically have that

1201
00:45:33,280 --> 00:45:34,240
issue

1202
00:45:34,240 --> 00:45:36,319
but i can see how people absolutely

1203
00:45:36,319 --> 00:45:38,079
would

1204
00:45:38,079 --> 00:45:39,280
yes sir

1205
00:45:39,280 --> 00:45:40,240
um

1206
00:45:40,240 --> 00:45:41,680
given that you were able to get some

1207
00:45:41,680 --> 00:45:43,599
success on the macbook

1208
00:45:43,599 --> 00:45:45,280
is there any technical limitations that

1209
00:45:45,280 --> 00:45:47,599
would spot like social media companies

1210
00:45:47,599 --> 00:45:49,119
and trying to solve this

1211
00:45:49,119 --> 00:45:51,119
um because i'm thinking

1212
00:45:51,119 --> 00:45:55,079
if profile of flagship

1213
00:46:02,160 --> 00:46:03,760
i think that it's something that is such

1214
00:46:03,760 --> 00:46:05,680
new and in the grand scheme of things

1215
00:46:05,680 --> 00:46:07,520
it's such new technology

1216
00:46:07,520 --> 00:46:08,319
that

1217
00:46:08,319 --> 00:46:10,319
for me personally it would be

1218
00:46:10,319 --> 00:46:12,160
computationally possible that was the

1219
00:46:12,160 --> 00:46:13,920
aim of this research was to prove it

1220
00:46:13,920 --> 00:46:16,160
would be computationally possible and

1221
00:46:16,160 --> 00:46:18,800
my answer is yes you can so

1222
00:46:18,800 --> 00:46:20,960
yep absolutely i i think it's something

1223
00:46:20,960 --> 00:46:24,319
that social media companies and

1224
00:46:24,319 --> 00:46:27,040
financial industries etc should be

1225
00:46:27,040 --> 00:46:29,520
should be looking at and without a doubt

1226
00:46:29,520 --> 00:46:32,640
um yep

1227
00:46:32,880 --> 00:46:35,880
yes

1228
00:46:54,960 --> 00:46:56,800
that will always make sure he's a fake

1229
00:46:56,800 --> 00:46:58,480
person even though the person is

1230
00:46:58,480 --> 00:46:59,839
absolute

1231
00:46:59,839 --> 00:47:01,920
how do we handle that for example

1232
00:47:01,920 --> 00:47:03,280
if your

1233
00:47:03,280 --> 00:47:05,359
social profile says always first of them

1234
00:47:05,359 --> 00:47:07,760
fake how do you as a person

1235
00:47:07,760 --> 00:47:10,800
will authenticate against the same

1236
00:47:10,800 --> 00:47:13,200
model over and over again fell to prove

1237
00:47:13,200 --> 00:47:15,119
that you're actually lost i was going to

1238
00:47:15,119 --> 00:47:16,480
make a joke about mfa they're about to

1239
00:47:16,480 --> 00:47:18,800
publish it

1240
00:47:18,800 --> 00:47:20,800
no it's one of those problems right and

1241
00:47:20,800 --> 00:47:23,680
i think that without a doubt if

1242
00:47:23,680 --> 00:47:25,599
there's no way that major social media

1243
00:47:25,599 --> 00:47:26,880
companies are going to take my research

1244
00:47:26,880 --> 00:47:28,000
on and go yep this is how we're going to

1245
00:47:28,000 --> 00:47:30,400
do it um however

1246
00:47:30,400 --> 00:47:32,000
if they were

1247
00:47:32,000 --> 00:47:34,400
to kind of use a similar model it just

1248
00:47:34,400 --> 00:47:37,440
means tuning so whatever that bias is

1249
00:47:37,440 --> 00:47:40,000
and we identify okay right these types

1250
00:47:40,000 --> 00:47:41,839
of profiles are getting flagged folks

1251
00:47:41,839 --> 00:47:43,040
with these specific facial

1252
00:47:43,040 --> 00:47:44,480
characteristics are getting put into the

1253
00:47:44,480 --> 00:47:47,680
wrong bracket go back rinse and repeat i

1254
00:47:47,680 --> 00:47:48,720
think that's always the same with

1255
00:47:48,720 --> 00:47:51,280
machine learning no

1256
00:47:51,280 --> 00:47:53,839
it also matter an effective

1257
00:47:53,839 --> 00:47:55,920
training size or your actual use size if

1258
00:47:55,920 --> 00:47:59,520
you have a model that's like 99.99

1259
00:47:59,520 --> 00:48:02,079
fine that's great but for profile atlas

1260
00:48:02,079 --> 00:48:04,480
for platform let's say facebook that's

1261
00:48:04,480 --> 00:48:07,359
still thousands

1262
00:48:12,240 --> 00:48:13,760
no i think there's never going to be

1263
00:48:13,760 --> 00:48:15,760
such a thing as a perfect model what i

1264
00:48:15,760 --> 00:48:17,440
tried to do to kind of circumvent some

1265
00:48:17,440 --> 00:48:18,880
of those problems was to make sure that

1266
00:48:18,880 --> 00:48:21,119
the data set was diverse so making sure

1267
00:48:21,119 --> 00:48:23,440
you had different ethnics ethnicities in

1268
00:48:23,440 --> 00:48:25,680
there different genders different ages

1269
00:48:25,680 --> 00:48:27,599
to try and overcome

1270
00:48:27,599 --> 00:48:30,319
that bias if it was all

1271
00:48:30,319 --> 00:48:33,359
blond-haired 24 year old females

1272
00:48:33,359 --> 00:48:35,520
it would have definitely been biased um

1273
00:48:35,520 --> 00:48:37,920
however i did try to counteract that a

1274
00:48:37,920 --> 00:48:38,960
little bit with what i done but i

1275
00:48:38,960 --> 00:48:40,880
imagine like i say the larger the data

1276
00:48:40,880 --> 00:48:43,200
set is and the more that you function it

1277
00:48:43,200 --> 00:48:46,640
the lower that margin for error would be

1278
00:48:46,640 --> 00:48:48,720
of course of uh yeah no so that time i

1279
00:48:48,720 --> 00:48:50,079
think yes sir

1280
00:48:50,079 --> 00:48:52,079
you talked about how the technology

1281
00:48:52,079 --> 00:48:53,760
could generate you just getting better

1282
00:48:53,760 --> 00:48:54,960
all the time

1283
00:48:54,960 --> 00:48:56,720
did you do any testing or are you aware

1284
00:48:56,720 --> 00:48:58,640
of anyone doing any testing taking a

1285
00:48:58,640 --> 00:49:00,319
classifier and saying

1286
00:49:00,319 --> 00:49:01,920
this is the performance degradation when

1287
00:49:01,920 --> 00:49:03,760
you say take a really old

1288
00:49:03,760 --> 00:49:06,400
method to generate debates versus the

1289
00:49:06,400 --> 00:49:07,359
later

1290
00:49:07,359 --> 00:49:08,640
better ones

1291
00:49:08,640 --> 00:49:09,520
and

1292
00:49:09,520 --> 00:49:11,359
how how how much worse did the

1293
00:49:11,359 --> 00:49:15,799
classifiers perform versus

1294
00:49:16,400 --> 00:49:17,280
i

1295
00:49:17,280 --> 00:49:20,000
considered it as part of my research i

1296
00:49:20,000 --> 00:49:22,160
couldn't implement it and but no i'm not

1297
00:49:22,160 --> 00:49:24,160
aware of anyone doing that research at

1298
00:49:24,160 --> 00:49:25,440
the moment i did try and see if there

1299
00:49:25,440 --> 00:49:27,920
was anything i could reference and many

1300
00:49:27,920 --> 00:49:30,400
material there but not currently but

1301
00:49:30,400 --> 00:49:32,079
definitely like i kind of touched on

1302
00:49:32,079 --> 00:49:33,359
there was one of the limitations of the

1303
00:49:33,359 --> 00:49:35,760
model that was specific to one

1304
00:49:35,760 --> 00:49:37,680
generator but yeah absolutely something

1305
00:49:37,680 --> 00:49:38,880
that needs to be done in the future to

1306
00:49:38,880 --> 00:49:41,599
look at over time how better it performs

1307
00:49:41,599 --> 00:49:43,359
is its specific images even what would

1308
00:49:43,359 --> 00:49:45,200
happen potentially if you

1309
00:49:45,200 --> 00:49:47,200
took images generated from different

1310
00:49:47,200 --> 00:49:50,079
algorithms put them in one data set and

1311
00:49:50,079 --> 00:49:52,960
then try and train that way potentially

1312
00:49:52,960 --> 00:49:54,079
um

1313
00:49:54,079 --> 00:49:57,079
yes

1314
00:50:10,240 --> 00:50:12,959
so you

1315
00:50:21,040 --> 00:50:21,760
so

1316
00:50:21,760 --> 00:50:23,680
part of the research

1317
00:50:23,680 --> 00:50:25,599
was looking at how

1318
00:50:25,599 --> 00:50:27,920
individuals respond to artificially

1319
00:50:27,920 --> 00:50:30,240
generated faces and based on the sample

1320
00:50:30,240 --> 00:50:32,160
set that was taken people were actually

1321
00:50:32,160 --> 00:50:33,680
unable to classify the difference

1322
00:50:33,680 --> 00:50:35,200
between an artificially generated face

1323
00:50:35,200 --> 00:50:37,040
and a human face and i don't know what

1324
00:50:37,040 --> 00:50:39,119
would happen if you kind of tried to i

1325
00:50:39,119 --> 00:50:41,920
don't know crop ears eyes his noses and

1326
00:50:41,920 --> 00:50:44,400
my soft people and put it all together

1327
00:50:44,400 --> 00:50:46,079
but no this research was specifically

1328
00:50:46,079 --> 00:50:47,280
focused on

1329
00:50:47,280 --> 00:50:51,040
pure artificially generated faces

1330
00:50:51,359 --> 00:50:54,759
any other questions

1331
00:51:08,690 --> 00:51:11,749
[Music]

1332
00:51:12,640 --> 00:51:14,400
and it's also just on that note it's

1333
00:51:14,400 --> 00:51:16,319
maybe worth to

1334
00:51:16,319 --> 00:51:17,680
remembering that as much as i'm focused

1335
00:51:17,680 --> 00:51:19,760
on the negativity right now of this

1336
00:51:19,760 --> 00:51:21,440
technology with anything it does have a

1337
00:51:21,440 --> 00:51:22,800
positive side as well there's a lot of

1338
00:51:22,800 --> 00:51:25,760
power and anonymity right so

1339
00:51:25,760 --> 00:51:28,880
just something as maybe a

1340
00:51:28,880 --> 00:51:30,880
uh um

1341
00:51:30,880 --> 00:51:32,880
a note to remember within this i'm

1342
00:51:32,880 --> 00:51:34,319
definitely looking at how it can be used

1343
00:51:34,319 --> 00:51:36,240
in a negative sense but with any

1344
00:51:36,240 --> 00:51:37,839
technology there's

1345
00:51:37,839 --> 00:51:39,760
definitely power for it to be used and

1346
00:51:39,760 --> 00:51:41,599
good as well um if there's one more

1347
00:51:41,599 --> 00:51:44,000
question so i will finish up

1348
00:51:44,000 --> 00:51:46,240
there

