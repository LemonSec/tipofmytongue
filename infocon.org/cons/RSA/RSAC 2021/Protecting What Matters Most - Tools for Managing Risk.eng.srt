1
00:00:01,140 --> 00:00:01,972
- Good morning.

2
00:00:01,973 --> 00:00:04,840
And thank you for joining our panel today

3
00:00:04,840 --> 00:00:08,170
to talk a little bit more about
protecting what matters most

4
00:00:08,170 --> 00:00:11,440
for your organizations and
the tool for managing risks.

5
00:00:11,440 --> 00:00:12,719
Before we get started with the panel,

6
00:00:12,720 --> 00:00:15,170
I wanted to take a moment to thank C Forum

7
00:00:15,170 --> 00:00:17,960
for allowing us to
participate in today's panel.

8
00:00:17,960 --> 00:00:19,320
And for those of you that aren't familiar

9
00:00:19,320 --> 00:00:24,320
with C Forum, C Forum is
a forum that was started

10
00:00:25,030 --> 00:00:27,570
from the NIST cybersecurity
framework workshops.

11
00:00:27,570 --> 00:00:30,630
And it was an online
community that helps enable

12
00:00:30,630 --> 00:00:33,940
the continued communications
that those workshops started.

13
00:00:33,940 --> 00:00:35,440
It's a free online forum

14
00:00:35,440 --> 00:00:37,760
that's available to any
security practitioner

15
00:00:37,760 --> 00:00:40,980
or implemented to understand
just best practices

16
00:00:40,980 --> 00:00:43,997
in cybersecurity and share
in the dialogue there

17
00:00:43,997 --> 00:00:48,080
and the form is available at
cyber.securityframework.org.

18
00:00:48,080 --> 00:00:52,489
Okay. Now, without further
ado, let's get onto the panel.

19
00:00:52,490 --> 00:00:56,880
I'd like to take a moment
to introduce our panelists.

20
00:00:56,880 --> 00:00:59,070
At first, we have Greg Witte.

21
00:00:59,070 --> 00:01:01,100
Greg Witte is a senior security engineer

22
00:01:01,100 --> 00:01:03,630
with Huntington Ingles Industry.

23
00:01:03,630 --> 00:01:08,540
Greg started his cybersecurity
experience in the workforce

24
00:01:08,540 --> 00:01:12,200
in 1993 and has spent
most of his time working

25
00:01:12,200 --> 00:01:14,810
with both government and
commercial organizations

26
00:01:14,810 --> 00:01:17,570
improving their cybersecurity program.

27
00:01:17,570 --> 00:01:21,589
Greg most notably has spent
15 years supporting NIST

28
00:01:21,590 --> 00:01:24,790
in the development of cybersecurity
standards, guidelines,

29
00:01:24,790 --> 00:01:27,930
and most notably the
cybersecurity framework.

30
00:01:27,930 --> 00:01:32,230
Greg also works with
ISACA and teaches several

31
00:01:32,230 --> 00:01:34,550
of the certification programs there

32
00:01:34,550 --> 00:01:36,380
and brings a unique perspective

33
00:01:36,380 --> 00:01:38,070
from both his NIST experience

34
00:01:38,070 --> 00:01:41,083
and from working with
ISACA to the panel today.

35
00:01:41,920 --> 00:01:44,670
Up next, we have John Manchester.

36
00:01:44,670 --> 00:01:46,840
John is a cybersecurity program manager

37
00:01:46,840 --> 00:01:49,830
at John Hopkins Bloomberg
School of Public Health

38
00:01:49,830 --> 00:01:52,110
with over 25 years of industry experience

39
00:01:52,110 --> 00:01:56,600
spread across managing enterprise,
systems and cybersecurity

40
00:01:56,600 --> 00:02:00,410
at both K through 12 and
higher education institutes.

41
00:02:00,410 --> 00:02:04,360
Most notably and most
recently John's been working

42
00:02:04,360 --> 00:02:07,260
to help build a security program

43
00:02:07,260 --> 00:02:09,610
aligned to FISMA requirements

44
00:02:09,610 --> 00:02:12,700
as well as improving cyber maturity

45
00:02:12,700 --> 00:02:16,179
within his organization with ISACA's

46
00:02:16,180 --> 00:02:18,900
CMI cyber maturity platform.

47
00:02:18,900 --> 00:02:20,380
And then rounding out the panel today,

48
00:02:20,380 --> 00:02:22,423
we have Leilani Lauger.

49
00:02:22,423 --> 00:02:25,210
Leilani is a cybersecurity consultant

50
00:02:25,210 --> 00:02:27,500
with over 20 years of experience.

51
00:02:27,500 --> 00:02:30,900
She's a former CISSO or Chief
Information Security Officer

52
00:02:30,900 --> 00:02:35,900
at the University of Chicago
and Ren ISACA board member.

53
00:02:36,320 --> 00:02:38,250
Most recently, Leilani has been working

54
00:02:38,250 --> 00:02:40,970
with Northeastern University in Boston

55
00:02:40,970 --> 00:02:44,480
and helping them with their CMMC

56
00:02:44,480 --> 00:02:46,160
or Cybersecurity Maturity Model

57
00:02:46,160 --> 00:02:48,650
Certification Compliance Initiatives.

58
00:02:48,650 --> 00:02:51,090
And she brings a unique
perspective on risk management

59
00:02:51,090 --> 00:02:55,800
for us today in how to combine
business risk objectives

60
00:02:57,170 --> 00:02:59,119
with compliance objectives.

61
00:02:59,120 --> 00:03:01,150
Okay. So very excited and thank you all

62
00:03:01,150 --> 00:03:04,100
for joining us today on the panel.

63
00:03:04,100 --> 00:03:05,090
And as we get started on the,

64
00:03:05,090 --> 00:03:07,850
please feel free to ask
questions throughout

65
00:03:07,850 --> 00:03:10,750
and the panelists will be
happy to answer your questions

66
00:03:10,750 --> 00:03:13,143
in addition to the
questions within the panel.

67
00:03:14,490 --> 00:03:18,213
So to help us get started,
Greg, let's start with you.

68
00:03:19,350 --> 00:03:22,260
And could you help us, tell us
a little bit more about some

69
00:03:22,260 --> 00:03:25,399
of the work that you've
been doing recently at NIS

70
00:03:25,400 --> 00:03:27,420
specifically with
enterprise risk management

71
00:03:27,420 --> 00:03:30,519
and your perspective on
enterprise risk management?

72
00:03:30,520 --> 00:03:31,353
- Oh, sure.

73
00:03:31,353 --> 00:03:32,487
Thanks Tom.

74
00:03:32,487 --> 00:03:34,297
I think one of the things
that's really exciting

75
00:03:34,297 --> 00:03:36,520
for me is working with
the brilliant people

76
00:03:36,520 --> 00:03:40,560
at NIST as a contractor, I'm
able to get a broad perspective

77
00:03:40,560 --> 00:03:42,850
of what many of their
scientists are working on.

78
00:03:42,850 --> 00:03:46,030
And while the NIST team
has been really working

79
00:03:46,030 --> 00:03:50,530
for decades on an enterprise
based approach over the last

80
00:03:50,530 --> 00:03:53,230
I guess maybe five to 10 years,

81
00:03:53,230 --> 00:03:55,010
the people who've been
applying those models

82
00:03:55,010 --> 00:03:58,190
have really been kind of focusing
at the system level more.

83
00:03:58,190 --> 00:03:59,060
We lost a little bit

84
00:03:59,060 --> 00:04:02,050
of the perspective of the
enterprise level view.

85
00:04:02,050 --> 00:04:04,030
And what we found was there was this need

86
00:04:04,030 --> 00:04:06,450
to kind of remind the public one more time

87
00:04:06,450 --> 00:04:09,269
that there needs to be
ways that we can do all

88
00:04:09,270 --> 00:04:11,940
of these important things
at the system level

89
00:04:11,940 --> 00:04:14,350
in the cybersecurity risk management field

90
00:04:14,350 --> 00:04:17,990
but to never forget that the
guidance for how we do that

91
00:04:17,990 --> 00:04:20,720
and the results of what we
do need to be integrated

92
00:04:20,720 --> 00:04:22,580
with the enterprise risk management.

93
00:04:22,580 --> 00:04:23,530
So we've been working

94
00:04:23,530 --> 00:04:28,530
on a NIST inter-agency report
called number NIST 8286

95
00:04:30,040 --> 00:04:30,873
and it's all

96
00:04:30,873 --> 00:04:33,190
about how to integrate
cybersecurity risk management

97
00:04:33,190 --> 00:04:35,350
with enterprise risk management.

98
00:04:35,350 --> 00:04:38,520
And it goes through a workflow
to understand how we start

99
00:04:38,520 --> 00:04:40,310
with an understanding of what's important

100
00:04:40,310 --> 00:04:43,880
to the organization, some of
the NIST special publications

101
00:04:43,880 --> 00:04:45,640
have really focused on that over the years

102
00:04:45,640 --> 00:04:49,140
but it's just a reminder
that the risk appetite,

103
00:04:49,140 --> 00:04:51,729
risk tolerance risk
direction, needs to be part

104
00:04:51,730 --> 00:04:53,290
of an enterprise strategy.

105
00:04:53,290 --> 00:04:55,080
And then we wanna try
to take an understanding

106
00:04:55,080 --> 00:04:58,289
of that and bring it back
through recording our results

107
00:04:58,290 --> 00:05:00,020
for example, in a risk register

108
00:05:00,020 --> 00:05:02,010
and making sure that those
are well communicated.

109
00:05:02,010 --> 00:05:03,900
So really just focusing

110
00:05:03,900 --> 00:05:07,520
on improving the way we measure
and report and communicate

111
00:05:07,520 --> 00:05:09,969
about risk at all the
levels of the organization.

112
00:05:11,930 --> 00:05:13,750
- Greg has some great points about that.

113
00:05:13,750 --> 00:05:16,350
And I can't tell you how
much I appreciate that

114
00:05:16,350 --> 00:05:18,440
there's guidance like that out there.

115
00:05:18,440 --> 00:05:20,000
There's been times in my career

116
00:05:20,000 --> 00:05:21,760
like early on when you kind of think that

117
00:05:21,760 --> 00:05:24,340
you know it all, and you're just gonna try

118
00:05:24,340 --> 00:05:26,900
and develop your own program
based on your own expertise

119
00:05:26,900 --> 00:05:30,500
but having guidance
that's really developed

120
00:05:30,500 --> 00:05:34,560
by experts in a collaborative
process is helpful.

121
00:05:34,560 --> 00:05:35,570
And I think that our role is

122
00:05:35,570 --> 00:05:38,650
as service security professionals is

123
00:05:38,650 --> 00:05:40,080
to help with that focus.

124
00:05:40,080 --> 00:05:42,481
Like you said about taking a bigger focus

125
00:05:42,481 --> 00:05:47,090
on the overall picture
of helping to do that.

126
00:05:47,090 --> 00:05:49,340
And it kind of interpreting the guidance

127
00:05:49,340 --> 00:05:51,530
that is developed by NIST

128
00:05:51,530 --> 00:05:55,313
and other similar organizations
and understanding it

129
00:05:55,314 --> 00:05:58,500
and then translating the
essence of that guidance too

130
00:05:58,500 --> 00:06:00,610
and share that information
with the business leaders

131
00:06:00,610 --> 00:06:03,910
and stakeholders in our own organizations.

132
00:06:03,910 --> 00:06:06,160
Do you have any thoughts
about this, Leilani?

133
00:06:07,038 --> 00:06:10,820
- Yes. So I think the enterprise
risk management process

134
00:06:10,820 --> 00:06:15,820
is really important to educating the board

135
00:06:16,480 --> 00:06:19,170
and also the other leaders
of an organization.

136
00:06:19,170 --> 00:06:24,170
So it's important that
cybersecurity doesn't appear as

137
00:06:24,420 --> 00:06:27,300
a line item in the risk register

138
00:06:27,300 --> 00:06:31,150
and that it's really
integrated into all the areas

139
00:06:31,150 --> 00:06:31,983
of the business.

140
00:06:31,983 --> 00:06:33,250
And this may require

141
00:06:33,250 --> 00:06:38,250
that the security risk
management leaders work

142
00:06:39,640 --> 00:06:44,159
with the other leaders
to identify the areas

143
00:06:44,160 --> 00:06:47,150
of cybersecurity risk in
their own organizations

144
00:06:47,150 --> 00:06:48,929
so that it's integrated

145
00:06:48,930 --> 00:06:51,290
into that enterprise
risk management process.

146
00:06:51,290 --> 00:06:55,920
And I think that 8286 tools
that Greg has been working on

147
00:06:55,920 --> 00:06:59,353
are really important tools here.

148
00:07:00,430 --> 00:07:01,263
- Thank you.

149
00:07:02,360 --> 00:07:04,400
- Excellent. Well with
that we understand just,

150
00:07:04,400 --> 00:07:06,849
understanding risk
within the organization,

151
00:07:06,850 --> 00:07:08,440
taking a 10 enterprise level to make sure

152
00:07:08,440 --> 00:07:11,930
that cybersecurity principles
are understood, are important.

153
00:07:11,930 --> 00:07:13,480
That didn't in a, as we mentioned.

154
00:07:13,480 --> 00:07:17,500
So it's not just a line
item in your risk register.

155
00:07:17,500 --> 00:07:20,560
So with that, and maybe
John, you can help us

156
00:07:20,560 --> 00:07:23,490
understand how do you
translate cybersecurity risks

157
00:07:23,490 --> 00:07:25,823
into business risks for your stakeholders?

158
00:07:28,230 --> 00:07:30,190
- Yeah, so we talk a little bit

159
00:07:30,190 --> 00:07:32,652
about what matters most, and I think

160
00:07:32,652 --> 00:07:36,219
it's a very subjective term
because what matters most

161
00:07:36,220 --> 00:07:38,540
to different stakeholders
are gonna be different.

162
00:07:38,540 --> 00:07:41,650
And there's a tendency to sometimes focus

163
00:07:41,650 --> 00:07:45,370
on what matters most to you
as the most important thing.

164
00:07:45,370 --> 00:07:46,330
And on the flip side of that,

165
00:07:46,330 --> 00:07:49,390
we often recognize that business leaders

166
00:07:49,390 --> 00:07:51,800
and other stakeholders have
a different perspective

167
00:07:51,800 --> 00:07:53,970
and what matters most
to them is different.

168
00:07:53,970 --> 00:07:56,890
And I think sometimes we can
be tempted to fill that gap

169
00:07:56,890 --> 00:07:59,870
with our own understanding
of what we think matters most

170
00:07:59,870 --> 00:08:03,210
to them, and then building
a program based on that.

171
00:08:03,210 --> 00:08:06,919
But I think that we need
to instead kind of engage

172
00:08:06,920 --> 00:08:10,240
with our stakeholders and
business leaders and understand

173
00:08:10,240 --> 00:08:14,950
from them what matters most to
them from their perspective.

174
00:08:14,950 --> 00:08:17,610
And then we have a better,
you're starting off

175
00:08:17,610 --> 00:08:20,650
the conversation and
engaging your stakeholders

176
00:08:20,650 --> 00:08:24,640
in a way that can lead
to a better partnership

177
00:08:24,640 --> 00:08:27,090
and you can allow them
to understand the risks

178
00:08:27,090 --> 00:08:28,349
and the threats that are out there

179
00:08:28,350 --> 00:08:29,750
that they might not understand.

180
00:08:29,750 --> 00:08:32,549
And we can understand their
perspective of what matters most

181
00:08:32,549 --> 00:08:34,640
to them and kind of merge that together

182
00:08:34,640 --> 00:08:37,082
into a much richer cybersecurity program.

183
00:08:38,500 --> 00:08:41,080
- That's a really great point, John,

184
00:08:41,080 --> 00:08:43,539
the education component,

185
00:08:43,539 --> 00:08:47,030
board education is becoming
increasingly important

186
00:08:47,030 --> 00:08:49,420
and this really applies across the board

187
00:08:49,420 --> 00:08:52,560
to even the leadership in an enterprise

188
00:08:52,560 --> 00:08:57,560
but educating everybody
how to evaluate risk

189
00:08:57,800 --> 00:09:02,439
and how to have some common
terms around addressing risk

190
00:09:02,440 --> 00:09:04,950
and risk mitigation,
that education component

191
00:09:04,950 --> 00:09:07,900
allows people to use
the expertise they have

192
00:09:07,900 --> 00:09:10,949
in their own field and apply it

193
00:09:10,950 --> 00:09:14,520
to this risk discussion,
which is really key,

194
00:09:14,520 --> 00:09:18,230
I think, so having board
of education sessions

195
00:09:18,230 --> 00:09:20,590
with your board, especially
if it's a large board,

196
00:09:20,590 --> 00:09:22,750
maybe with your audit committee

197
00:09:22,750 --> 00:09:25,720
and as well as with your
various leadership groups

198
00:09:25,720 --> 00:09:27,963
across campus and governance groups.

199
00:09:29,270 --> 00:09:32,090
- Well, I think that point
about what matters most

200
00:09:32,090 --> 00:09:34,930
I think both of you really
highlighted the fact that

201
00:09:34,930 --> 00:09:37,859
when I came into security
in the early 90s,

202
00:09:37,860 --> 00:09:40,690
I came in as a networking
guy who was ready

203
00:09:40,690 --> 00:09:43,653
to just throw buzzwords
at technical stuff at it.

204
00:09:43,653 --> 00:09:46,440
And one of my mentors, Kim
Jones just kept drilling

205
00:09:46,440 --> 00:09:48,560
into me, speak the language of business.

206
00:09:48,560 --> 00:09:51,020
Don't come in here talking
about ports and protocols,

207
00:09:51,020 --> 00:09:53,140
definitely have to do that
with my security colleagues.

208
00:09:53,140 --> 00:09:55,540
But when we're talking with
our business colleagues

209
00:09:55,540 --> 00:09:58,920
we need to understand what
they think matters most

210
00:09:58,920 --> 00:10:01,060
and how we can make sure
we're enabling them,

211
00:10:01,060 --> 00:10:03,550
one of the lessons I learned
the hard way was that,

212
00:10:03,550 --> 00:10:06,280
being one of those six
security practitioners

213
00:10:06,280 --> 00:10:07,250
for a long time, it seemed

214
00:10:07,250 --> 00:10:09,883
like our job was to say,
no, we were the no police.

215
00:10:10,780 --> 00:10:12,666
And as we worked with our stakeholders

216
00:10:12,667 --> 00:10:14,910
and as they helped us to understand

217
00:10:14,910 --> 00:10:16,350
and it's a model that we've used

218
00:10:16,350 --> 00:10:18,260
in that ISACA COBIT model too,

219
00:10:18,260 --> 00:10:20,939
not that they're the only ones that do it

220
00:10:20,940 --> 00:10:23,320
but it just reminded us that

221
00:10:23,320 --> 00:10:25,660
we're here to add value
to the organization.

222
00:10:25,660 --> 00:10:27,459
We're an enabling function.

223
00:10:27,460 --> 00:10:30,280
Security makes it
possible for us to do even

224
00:10:30,280 --> 00:10:32,800
remote conferences like
the one we're having today.

225
00:10:32,800 --> 00:10:35,030
So we needed to learn how to say yes

226
00:10:35,030 --> 00:10:38,040
and here's a way to do it
securely rather than no

227
00:10:38,040 --> 00:10:39,959
there was no way to do it securely.

228
00:10:39,960 --> 00:10:41,820
So really focusing on the business,

229
00:10:41,820 --> 00:10:43,330
focusing on those business goals

230
00:10:43,330 --> 00:10:45,390
and priorities made it possible

231
00:10:45,390 --> 00:10:48,720
for us in the security side
of things, to be an enabler

232
00:10:48,720 --> 00:10:50,394
to be contributing to the bottom line

233
00:10:50,394 --> 00:10:54,506
of the organization instead
of draining from it.

234
00:10:54,506 --> 00:10:56,002
- That's a great point.

235
00:10:57,290 --> 00:11:00,400
It's an in terms of
getting people involved

236
00:11:00,400 --> 00:11:04,242
in that process, your
governance model is also

237
00:11:04,243 --> 00:11:05,810
a really important function.

238
00:11:05,810 --> 00:11:10,140
You wanna get your, have
your risk discussions deep

239
00:11:10,140 --> 00:11:11,810
and wide in your organization.

240
00:11:11,810 --> 00:11:16,109
So you wanna have your
governance groups set up

241
00:11:16,110 --> 00:11:18,680
so that you've got people who
are very close to the work

242
00:11:18,680 --> 00:11:19,829
because they're also very good

243
00:11:19,830 --> 00:11:21,273
at identifying the risks,

244
00:11:22,120 --> 00:11:26,630
insider threats or just
business process risk.

245
00:11:26,630 --> 00:11:30,560
And having that depth and breadth

246
00:11:30,560 --> 00:11:32,880
in your governance process allows you

247
00:11:32,880 --> 00:11:35,960
to really integrate into the business

248
00:11:35,960 --> 00:11:40,170
and understand and get your message across

249
00:11:40,170 --> 00:11:43,193
and get people actively
involved in evaluating risk.

250
00:11:45,570 --> 00:11:46,500
- Yeah, very good comments.

251
00:11:46,500 --> 00:11:49,240
And actually just to kind of reinforce,

252
00:11:49,240 --> 00:11:51,020
completely agree with everybody

253
00:11:51,020 --> 00:11:54,150
with understanding what matters
most and understanding those

254
00:11:54,150 --> 00:11:56,082
stakeholder goals and needs.

255
00:11:56,082 --> 00:11:57,020
(crackling drowns out speaker)

256
00:11:57,020 --> 00:11:58,650
they call it the goals cascade,

257
00:11:58,650 --> 00:12:01,930
being able to take those
stakeholders inputs understanding

258
00:12:01,930 --> 00:12:02,859
and then as John mentioned

259
00:12:02,860 --> 00:12:04,670
translating it into something that's real

260
00:12:04,670 --> 00:12:06,240
so that we understand what are the risks

261
00:12:06,240 --> 00:12:07,780
and not necessarily what
do I think of the risks

262
00:12:07,780 --> 00:12:09,650
of the organization, but
what are the executive,

263
00:12:09,650 --> 00:12:11,329
what are the business
leaders thinks the risk is

264
00:12:11,330 --> 00:12:12,653
to their organization?

265
00:12:13,630 --> 00:12:15,150
So with that, Leilani,

266
00:12:15,150 --> 00:12:18,160
I think you're probably in
a unique situation, right?

267
00:12:18,160 --> 00:12:19,900
We've been talking about
how important it is

268
00:12:19,900 --> 00:12:21,630
to understand business risk,

269
00:12:21,630 --> 00:12:23,400
understand the stakeholder goals,

270
00:12:23,400 --> 00:12:26,340
but you also are challenged
with trying to align those

271
00:12:26,340 --> 00:12:28,080
with compliance requirements

272
00:12:28,080 --> 00:12:29,710
where in this case,

273
00:12:29,710 --> 00:12:33,200
the federal government DOD is
coming in and telling you some

274
00:12:33,200 --> 00:12:35,490
of the expectations that you have to meet

275
00:12:35,490 --> 00:12:40,140
in addition to mainly maintaining
the business operations.

276
00:12:40,140 --> 00:12:42,230
So how do you align business risks

277
00:12:42,230 --> 00:12:44,530
while meeting those
compliance goals together?

278
00:12:46,054 --> 00:12:48,350
- That is a really,

279
00:12:48,350 --> 00:12:50,660
it's been a really interesting challenge

280
00:12:50,660 --> 00:12:54,480
because compliance can really feel like

281
00:12:54,480 --> 00:12:56,500
checking the boxes at times.

282
00:12:56,500 --> 00:12:57,803
And it can,

283
00:13:00,995 --> 00:13:02,800
there are situations where

284
00:13:02,800 --> 00:13:07,229
being very strict about meeting
a compliance goal can feel

285
00:13:07,230 --> 00:13:08,860
like it's interfering with operations.

286
00:13:08,860 --> 00:13:12,920
So I've spent many years
now in higher education

287
00:13:12,920 --> 00:13:15,750
and so I'm very focused

288
00:13:15,750 --> 00:13:19,853
on the risks of things like
research and compliance can,

289
00:13:22,020 --> 00:13:24,410
if you're compliance focused

290
00:13:24,410 --> 00:13:27,620
without that risk point of
view, you can ignore the fact

291
00:13:27,620 --> 00:13:30,930
that you may have risks
to your very operations

292
00:13:30,930 --> 00:13:32,719
in the case of higher education,

293
00:13:32,720 --> 00:13:35,520
maybe it's research, another
organization that could be

294
00:13:35,520 --> 00:13:40,520
at other critical operations
of the organization.

295
00:13:40,665 --> 00:13:43,189
And you wanna be sure

296
00:13:43,190 --> 00:13:46,430
that that component is also taken

297
00:13:46,430 --> 00:13:51,130
into account when you're
evaluating the risk,

298
00:13:51,130 --> 00:13:53,100
your total risk goals.

299
00:13:53,100 --> 00:13:56,050
And so it's, I have definitely seen

300
00:13:57,630 --> 00:13:59,340
and in this project with CMMC,

301
00:13:59,340 --> 00:14:02,470
I've definitely seen situations where

302
00:14:02,470 --> 00:14:03,810
there was some concern

303
00:14:05,001 --> 00:14:08,939
that the compliance
with goals were onerous

304
00:14:10,344 --> 00:14:12,540
or that the requirements were onerous

305
00:14:12,540 --> 00:14:14,939
and would slow down the research

306
00:14:14,940 --> 00:14:18,790
and there really isn't a better
time than now with COVID,

307
00:14:18,790 --> 00:14:20,699
if we think about this

308
00:14:20,700 --> 00:14:25,090
where would we be if
research were slowed down

309
00:14:25,090 --> 00:14:28,390
and we didn't have the
technology we have today

310
00:14:28,390 --> 00:14:31,000
to create the vaccines that we have.

311
00:14:31,000 --> 00:14:36,000
So the other important
that we're doing research

312
00:14:36,390 --> 00:14:37,650
across so many

313
00:14:37,650 --> 00:14:42,650
important academic subjects,
but it's important to think

314
00:14:44,250 --> 00:14:48,740
about the, what are you
trying to accomplish?

315
00:14:48,740 --> 00:14:49,573
What is this

316
00:14:49,573 --> 00:14:52,069
and what does this
control try to accomplish

317
00:14:52,070 --> 00:14:56,723
and take taking into account
the business operations,

318
00:15:00,770 --> 00:15:03,390
but compliance is also an enabler

319
00:15:03,390 --> 00:15:06,880
in the sense that

320
00:15:06,880 --> 00:15:09,780
there are some technical controls

321
00:15:09,780 --> 00:15:13,329
or policy controls that you
may not have an opportunity

322
00:15:13,330 --> 00:15:16,420
to get move forward in your
organization without compliance.

323
00:15:16,420 --> 00:15:18,860
And so it's an opportunity

324
00:15:18,860 --> 00:15:20,260
to move those things forward.

325
00:15:20,260 --> 00:15:22,450
You've got there are obvious risks

326
00:15:22,450 --> 00:15:27,080
with not being compliant,
financial, public relations

327
00:15:27,080 --> 00:15:28,280
et cetera, reputational risks.

328
00:15:28,280 --> 00:15:33,280
So those are, those are goals
that can help you as well.

329
00:15:36,151 --> 00:15:37,300
- Leilani just listened to you talk

330
00:15:37,300 --> 00:15:40,266
about it kind of drives
home the point about

331
00:15:40,266 --> 00:15:44,130
how complicated the whole landscape is,

332
00:15:44,130 --> 00:15:46,500
not to date myself, but I remember a time

333
00:15:46,500 --> 00:15:48,640
and if you had a good antivirus software

334
00:15:48,640 --> 00:15:50,300
and a rudimentary firewall,

335
00:15:50,300 --> 00:15:53,550
you effectively had a
cybersecurity program,

336
00:15:53,550 --> 00:15:57,449
but it's so much more complicated today.

337
00:15:57,450 --> 00:15:59,030
And looking at that intersection

338
00:15:59,030 --> 00:16:01,720
between risk and compliance, it's almost

339
00:16:01,720 --> 00:16:03,770
like looking at it at a Venn diagram

340
00:16:04,690 --> 00:16:06,170
and different stakeholders

341
00:16:06,170 --> 00:16:08,140
will look at just their own circles.

342
00:16:08,140 --> 00:16:11,040
And sometimes it's tempting just to focus

343
00:16:11,040 --> 00:16:13,829
on the intersection of
those and figure, well,

344
00:16:13,830 --> 00:16:16,460
that's the low hanging
fruit, but the reality is

345
00:16:16,460 --> 00:16:19,380
that we have to look
at all of the circles,

346
00:16:19,380 --> 00:16:22,010
but I think we're the people who sit

347
00:16:22,010 --> 00:16:25,140
in the middle of that
Venn diagram just based

348
00:16:25,140 --> 00:16:27,210
on the fact that Tom is
asking us the question

349
00:16:27,210 --> 00:16:30,790
about it and reach out to
those stakeholders who are

350
00:16:30,790 --> 00:16:34,099
in each of those individual
circles and understand

351
00:16:34,100 --> 00:16:36,900
from them where they're
coming from, but also explain

352
00:16:36,900 --> 00:16:41,020
to them the risks and the requirements

353
00:16:41,020 --> 00:16:43,793
of addressing the needs
for the other circles.

354
00:16:44,850 --> 00:16:46,430
One of the things that I worked closely

355
00:16:46,430 --> 00:16:48,420
with our institutional review board

356
00:16:48,420 --> 00:16:50,920
and researchers who are very much focused

357
00:16:50,920 --> 00:16:53,969
on protecting data
regarding human subjects

358
00:16:53,970 --> 00:16:55,230
and human subjects research

359
00:16:55,230 --> 00:16:58,230
and data about human subjects,

360
00:16:58,230 --> 00:17:00,300
but may not necessarily be focused

361
00:17:00,300 --> 00:17:03,760
on the overall protection of data.

362
00:17:03,760 --> 00:17:08,690
And there's other
components to their data,

363
00:17:08,690 --> 00:17:11,900
institutional intellectual
property, for example.

364
00:17:11,900 --> 00:17:14,030
And they don't always look at
all of the different aspects

365
00:17:14,030 --> 00:17:16,349
like the triad of security
that we talk about.

366
00:17:16,349 --> 00:17:18,810
And there's been situations
where people will talk

367
00:17:18,810 --> 00:17:19,923
about the storage system

368
00:17:19,923 --> 00:17:21,990
that they're using to keep their data

369
00:17:21,990 --> 00:17:24,339
and they'll have all of the
security pieces in place.

370
00:17:24,339 --> 00:17:25,530
And then I'll ask them a question.

371
00:17:25,530 --> 00:17:26,940
Well, what if somebody just walks in here

372
00:17:26,940 --> 00:17:28,850
and picks up that device and walks

373
00:17:28,850 --> 00:17:31,110
out of here with it, and they're horrified

374
00:17:31,110 --> 00:17:33,929
at the thought of it, but it's our job

375
00:17:33,930 --> 00:17:35,610
to think about all of those risks

376
00:17:35,610 --> 00:17:38,540
and all of the pieces of the circles

377
00:17:38,540 --> 00:17:40,810
in the Venn diagram and
work with stakeholders

378
00:17:40,810 --> 00:17:44,072
on addressing all of
it, which is difficult.

379
00:17:44,073 --> 00:17:45,680
It's a complicated environment

380
00:17:47,290 --> 00:17:50,180
- Right, or they don't
think about the day.

381
00:17:50,180 --> 00:17:51,730
They think about where the data is stored

382
00:17:51,730 --> 00:17:54,478
but not how it's transmitted or

383
00:17:54,478 --> 00:17:57,613
put into the system or
taken out or processed.

384
00:17:59,720 --> 00:18:00,553
- Well, absolutely.

385
00:18:00,553 --> 00:18:03,500
And I can share from someone that's worked

386
00:18:03,500 --> 00:18:06,390
with the team at NIST is
that I see both sides of it.

387
00:18:06,390 --> 00:18:10,620
Like you said, John, it's
a Venn diagram, too often,

388
00:18:10,620 --> 00:18:12,810
we see people that go into
one ditch or the other

389
00:18:12,810 --> 00:18:15,030
whether thinking either they've got

390
00:18:15,030 --> 00:18:18,950
this compliance centered mindset
where they're just thinking

391
00:18:18,950 --> 00:18:21,100
about checking boxes and
not really seeing the forest

392
00:18:21,100 --> 00:18:25,260
for the trees or they're
minimizing that compliance need

393
00:18:25,260 --> 00:18:26,680
and ignoring the fact

394
00:18:26,680 --> 00:18:29,340
that we've made promises
to our constituents

395
00:18:29,340 --> 00:18:32,929
and to our contract
teams, to our regulators,

396
00:18:32,930 --> 00:18:33,763
so many others.

397
00:18:33,763 --> 00:18:36,250
So I think we've got to try
to find that middle ground

398
00:18:36,250 --> 00:18:37,280
just like you said, John.

399
00:18:37,280 --> 00:18:39,860
I think one of the things
that makes me smile

400
00:18:39,860 --> 00:18:43,129
about is one of my friends
from the CMMI Institute,

401
00:18:43,130 --> 00:18:44,570
Doug Grind Staff, he used to say

402
00:18:44,570 --> 00:18:47,159
that it's time for a Copernican shift.

403
00:18:47,160 --> 00:18:50,420
He used to say that compliance
is the exhaust that comes

404
00:18:50,420 --> 00:18:52,340
out of a well-working engine.

405
00:18:52,340 --> 00:18:54,740
If we're doing the right
things in the right way

406
00:18:54,740 --> 00:18:57,350
with the right tools
and the right processes,

407
00:18:57,350 --> 00:19:02,219
then compliance will be something
that just naturally comes

408
00:19:02,220 --> 00:19:04,090
out of that good working system.

409
00:19:04,090 --> 00:19:05,770
Whereas if we're chasing compliance

410
00:19:05,770 --> 00:19:07,900
we can end up going down the wrong trail.

411
00:19:07,900 --> 00:19:10,590
So I think it's absolutely critical.

412
00:19:10,590 --> 00:19:12,970
And yet as we've seen,

413
00:19:12,970 --> 00:19:17,000
NIST recently updated our
special publication, 853,

414
00:19:17,000 --> 00:19:19,580
our control catalog with a reminder

415
00:19:19,580 --> 00:19:21,879
that these things need to be done

416
00:19:21,880 --> 00:19:24,660
as part of a holistic
way to treat an agency

417
00:19:24,660 --> 00:19:26,423
or enterprise system.

418
00:19:27,260 --> 00:19:29,110
And we should be doing good things

419
00:19:29,110 --> 00:19:32,219
with good controls and good
plans and good monitoring.

420
00:19:32,220 --> 00:19:33,630
And by the way,

421
00:19:33,630 --> 00:19:36,080
we should also review to
make sure that those controls

422
00:19:36,080 --> 00:19:37,899
are in place and effective and efficient.

423
00:19:37,900 --> 00:19:40,390
So we've got to make sure
that compliance is there

424
00:19:40,390 --> 00:19:42,850
but it needs to be an outcome and not

425
00:19:42,850 --> 00:19:45,459
the end only goal.

426
00:19:45,460 --> 00:19:46,293
- That's great.

427
00:19:46,293 --> 00:19:47,126
I'll look, Greg

428
00:19:48,323 --> 00:19:49,900
- Completely agree.

429
00:19:49,900 --> 00:19:51,540
I like that.

430
00:19:51,540 --> 00:19:53,090
Compliance should be the exhaust

431
00:19:53,090 --> 00:19:54,520
of your cybersecurity program

432
00:19:54,520 --> 00:19:56,370
and managing risk appropriately.

433
00:19:56,370 --> 00:19:58,290
So with that, Greg, let's come back

434
00:19:58,290 --> 00:20:00,810
to you with that and
say from your experience

435
00:20:00,810 --> 00:20:04,679
in working with NIST and
specifically on the NIST 8286

436
00:20:04,680 --> 00:20:06,540
for enterprise risk management,

437
00:20:06,540 --> 00:20:08,580
can you help us understand
a little bit more

438
00:20:08,580 --> 00:20:11,449
on how organizations can
integrate cybersecurity risks

439
00:20:11,450 --> 00:20:15,030
into their enterprise risk
management framework vice

440
00:20:15,030 --> 00:20:18,649
just thinking of cybersecurity
as the risk itself?

441
00:20:18,650 --> 00:20:22,350
- Sure. Well, I think, and
that goes back to exactly

442
00:20:22,350 --> 00:20:24,179
what John and Leilani had mentioned before

443
00:20:24,180 --> 00:20:26,113
about understanding what matters most.

444
00:20:26,980 --> 00:20:30,260
I just worked with a customer
who had become accustomed

445
00:20:30,260 --> 00:20:33,600
to just running scans and they were,

446
00:20:33,600 --> 00:20:35,040
they're running the scans all the time.

447
00:20:35,040 --> 00:20:36,430
They're getting data in

448
00:20:36,430 --> 00:20:40,390
and stuff was going into their
security integration tools.

449
00:20:40,390 --> 00:20:42,610
And yet they weren't
really understanding what

450
00:20:42,610 --> 00:20:45,110
their key stakeholders expected.

451
00:20:45,110 --> 00:20:47,240
What is it that matters
most to their organization?

452
00:20:47,240 --> 00:20:49,340
How can they enable innovation?

453
00:20:49,340 --> 00:20:51,429
If you think about what
Leilani was just saying

454
00:20:51,430 --> 00:20:55,150
about that research and
how do we enable the secure

455
00:20:55,150 --> 00:21:00,150
and available and reliable
privacy related research

456
00:21:00,500 --> 00:21:01,770
that needs to happen,

457
00:21:01,770 --> 00:21:05,010
how can we make those things
happen in a way that is secure

458
00:21:05,010 --> 00:21:08,280
and aligns with the
organizations management model?

459
00:21:08,280 --> 00:21:11,500
So it really comes back to
understanding, first of all,

460
00:21:11,500 --> 00:21:14,210
what are the expectations
of our leadership?

461
00:21:14,210 --> 00:21:16,707
What are the mission objectives
for our organization?

462
00:21:16,707 --> 00:21:19,360
And that goals cascade
that you mentioned, Tom,

463
00:21:19,360 --> 00:21:21,290
how do we take what the company

464
00:21:21,290 --> 00:21:24,129
or the organization or the
agency, or the university

465
00:21:24,130 --> 00:21:26,550
or whoever it might be,
how do we take those needs

466
00:21:26,550 --> 00:21:28,850
and translate those into business goals?

467
00:21:28,850 --> 00:21:30,409
How do we turn those business goals

468
00:21:30,410 --> 00:21:33,640
into technical security opportunities?

469
00:21:33,640 --> 00:21:35,210
And then how do we turn those

470
00:21:35,210 --> 00:21:36,890
into things that we can measure?

471
00:21:36,890 --> 00:21:39,510
How do we determine how to measure

472
00:21:39,510 --> 00:21:41,710
whether we're achieving
the risk appetite goals,

473
00:21:41,710 --> 00:21:45,340
how do we understand how to
interpret that risk tolerance

474
00:21:45,340 --> 00:21:46,810
at the organizational level

475
00:21:46,810 --> 00:21:49,750
maybe a division or branch office level?

476
00:21:49,750 --> 00:21:52,330
How do we take those
directives translate those

477
00:21:52,330 --> 00:21:55,169
into something that
means something for me.

478
00:21:55,170 --> 00:21:58,330
And then we can build our security plans

479
00:21:58,330 --> 00:22:01,580
and shared controls and shared processes.

480
00:22:01,580 --> 00:22:03,730
Then we can start to
build those around it.

481
00:22:04,600 --> 00:22:07,310
The best part is then we
can start to understand

482
00:22:07,310 --> 00:22:10,610
how those results are
enabling the organization

483
00:22:10,610 --> 00:22:14,629
to be successful, how we're
contributing to avoiding fines,

484
00:22:14,630 --> 00:22:19,630
avoiding losses, improving
customer expectations,

485
00:22:20,520 --> 00:22:22,030
how we're meeting those needs.

486
00:22:22,030 --> 00:22:25,940
So by turning those risks,

487
00:22:25,940 --> 00:22:29,550
things that we have to
do into opportunities,

488
00:22:29,550 --> 00:22:31,216
and especially looking at positive risk,

489
00:22:31,217 --> 00:22:32,500
and you're trying to understand

490
00:22:32,500 --> 00:22:34,300
how can we enable positive things

491
00:22:35,200 --> 00:22:38,820
besides going after chasing
those negative risks

492
00:22:38,820 --> 00:22:42,033
that can really be an
enabler for the organization.

493
00:22:42,034 --> 00:22:44,940
- I think to that point, Greg,
chasing those negative risks.

494
00:22:44,940 --> 00:22:46,960
I think that's the motivator for a lot

495
00:22:46,960 --> 00:22:49,020
of organizations in the
work that they have done

496
00:22:49,020 --> 00:22:50,200
up until this point,

497
00:22:50,200 --> 00:22:52,470
they'll react to something that happened

498
00:22:52,470 --> 00:22:54,290
in the past that caused them harm

499
00:22:54,290 --> 00:22:57,889
or caused them risk or some
business leader is listening

500
00:22:57,890 --> 00:22:59,280
to the radio in the morning, they hear

501
00:22:59,280 --> 00:23:02,260
about the latest thing that's
happening and they'll come in

502
00:23:02,260 --> 00:23:04,620
and direct their
cybersecurity team to react

503
00:23:04,620 --> 00:23:05,689
in some way to it.

504
00:23:05,690 --> 00:23:07,410
But the the result of that

505
00:23:07,410 --> 00:23:10,480
is a lot of cybersecurity
programs that I would say

506
00:23:10,480 --> 00:23:13,380
they're ad hoc and chaotic

507
00:23:13,380 --> 00:23:15,220
and not well formed.

508
00:23:15,220 --> 00:23:18,260
But it's looking at those guidance views

509
00:23:18,260 --> 00:23:20,320
that come from NIST and ISACAA

510
00:23:20,320 --> 00:23:22,179
and other organizations that kind of

511
00:23:22,180 --> 00:23:25,730
helped us to put a
structured approach on it.

512
00:23:25,730 --> 00:23:30,287
And I was reading a part of
the intro to the NIST 8286

513
00:23:31,510 --> 00:23:32,440
and there was a section

514
00:23:32,440 --> 00:23:34,350
in there that says that
most enterprises do not

515
00:23:34,350 --> 00:23:37,100
communicate their
cybersecurity risk guidance

516
00:23:37,100 --> 00:23:40,639
or risk responses in
consistent repeatable ways.

517
00:23:40,640 --> 00:23:43,010
And I think that kind of
summarizes the problem

518
00:23:43,010 --> 00:23:44,520
that we have to overcome.

519
00:23:44,520 --> 00:23:48,500
We have put some structure
around our programs

520
00:23:48,500 --> 00:23:52,120
and address all of it in a meaningful way.

521
00:23:52,120 --> 00:23:54,399
Otherwise we're just
leaving ourselves exposed

522
00:23:54,400 --> 00:23:57,090
in one area or another, and we're gonna,

523
00:23:57,090 --> 00:23:58,280
it's gonna cause us harm

524
00:23:58,280 --> 00:23:59,980
at some point in the future again.

525
00:24:01,400 --> 00:24:02,233
- And it's not

526
00:24:02,233 --> 00:24:07,050
that we won't have to be
reactive to emerging situations.

527
00:24:08,000 --> 00:24:10,190
But having that repeatable way

528
00:24:10,190 --> 00:24:12,950
that you communicate is so important.

529
00:24:12,950 --> 00:24:15,040
When I was at the University of Chicago,

530
00:24:15,040 --> 00:24:17,530
we had a whole process
for anytime there was

531
00:24:17,530 --> 00:24:21,570
a major vulnerability
or a significant attack,

532
00:24:21,570 --> 00:24:24,980
that sort of thing, how
we communicated that

533
00:24:24,980 --> 00:24:27,690
to our organization was both structured

534
00:24:27,690 --> 00:24:30,170
and coordinated with various stakeholders

535
00:24:30,170 --> 00:24:33,060
on campus and at our hospital system.

536
00:24:33,060 --> 00:24:36,639
So we had, so everybody was
communicating in the same way.

537
00:24:36,640 --> 00:24:38,200
They knew what to expect.

538
00:24:38,200 --> 00:24:40,730
They knew they would get an email with,

539
00:24:40,730 --> 00:24:42,440
here's what the situation is.

540
00:24:42,440 --> 00:24:43,730
Here's how we're responding.

541
00:24:43,730 --> 00:24:46,470
And here's what you should
do to protect yourself

542
00:24:46,470 --> 00:24:49,770
and protect our organizational assets.

543
00:24:49,770 --> 00:24:54,639
And you can have multiple
even have processes

544
00:24:54,640 --> 00:24:58,160
for your general
organizational population.

545
00:24:58,160 --> 00:25:00,510
And then it also doesn't hurt to get

546
00:25:00,510 --> 00:25:02,310
in front of this with your leadership.

547
00:25:02,310 --> 00:25:06,510
So if there's an emerging
situation to be regularly

548
00:25:06,510 --> 00:25:07,960
in communication, so they know that

549
00:25:07,960 --> 00:25:10,630
if they read about
something or hear something

550
00:25:10,630 --> 00:25:11,870
on the news that they're gonna hear

551
00:25:11,870 --> 00:25:15,560
from you about that as well,
and that inspires a lot

552
00:25:15,560 --> 00:25:18,800
of confidence and it gives you a lot

553
00:25:18,800 --> 00:25:21,090
of opportunity to have good conversations

554
00:25:21,090 --> 00:25:23,143
with your leadership and your board.

555
00:25:24,160 --> 00:25:25,290
- When those things happen,

556
00:25:25,290 --> 00:25:27,040
Leilani, there's nothing better

557
00:25:27,040 --> 00:25:29,470
than somebody in a leadership
position hearing something

558
00:25:29,470 --> 00:25:31,870
on the radio and then
telling you to react to it.

559
00:25:31,870 --> 00:25:34,560
And you being able to say,
well, we're already covered.

560
00:25:34,560 --> 00:25:35,610
And that goes back

561
00:25:35,610 --> 00:25:37,580
to the exhaust that
Greg was talking about.

562
00:25:37,580 --> 00:25:39,460
If you have a structured program

563
00:25:39,460 --> 00:25:42,040
and you're already following
the things that you need to do,

564
00:25:42,040 --> 00:25:42,909
it's most likely

565
00:25:42,910 --> 00:25:45,100
that you've already mitigated the risks

566
00:25:45,100 --> 00:25:47,360
that are being announced
on the radio that morning.

567
00:25:47,360 --> 00:25:48,689
So you're already ahead of it.

568
00:25:48,690 --> 00:25:50,260
And it's it just feels so good to be

569
00:25:50,260 --> 00:25:53,050
in that position sometimes
because we all have been

570
00:25:53,050 --> 00:25:55,463
in the other side where
it doesn't feel so good.

571
00:25:58,060 --> 00:26:01,139
- Okay. One of the things that
we saw is in the development

572
00:26:01,140 --> 00:26:04,580
of the cybersecurity
workforce, we asked people,

573
00:26:04,580 --> 00:26:07,320
not the workforce, but the
cybersecurity framework,

574
00:26:07,320 --> 00:26:08,980
we asked what's working and what's not.

575
00:26:08,980 --> 00:26:12,830
And one of the key factors
in whether an organization

576
00:26:12,830 --> 00:26:16,639
was successful in their
cyber security program was

577
00:26:16,640 --> 00:26:18,820
whether they had just an engagement

578
00:26:18,820 --> 00:26:21,720
between the senior leaders
and the operational team

579
00:26:21,720 --> 00:26:24,010
and the business leaders in between.

580
00:26:24,010 --> 00:26:27,220
So just having that
dialogue, just knowing that

581
00:26:27,220 --> 00:26:30,070
there's a dialogue that's
happening, communications going on

582
00:26:30,070 --> 00:26:32,620
can make such a big difference
for an organization.

583
00:26:34,660 --> 00:26:35,700
(crackling drowns out speaker)

584
00:26:35,700 --> 00:26:38,140
- Hearing you talk about

585
00:26:38,140 --> 00:26:39,820
just the communication that's required,

586
00:26:39,820 --> 00:26:41,110
understanding the stakeholder goals

587
00:26:41,110 --> 00:26:44,090
that we started today's discussion

588
00:26:44,090 --> 00:26:47,209
with translating it into
what needs to be happened.

589
00:26:47,210 --> 00:26:49,830
Very important that we're
communicating effectively.

590
00:26:49,830 --> 00:26:54,830
So with that, maybe as one
of our last questions here,

591
00:26:55,000 --> 00:26:56,910
we can help the audience understand

592
00:26:56,910 --> 00:26:58,330
when it comes to those communications

593
00:26:58,330 --> 00:27:01,939
and working with the executives
and the operations teams,

594
00:27:01,940 --> 00:27:05,690
what metrics have you found that help,

595
00:27:05,690 --> 00:27:08,380
what metrics and reporting
formats have you helped

596
00:27:08,380 --> 00:27:11,520
or found that work best
for those communications?

597
00:27:11,520 --> 00:27:12,470
I'll just open that up

598
00:27:12,470 --> 00:27:16,030
to the panel here from your experience.

599
00:27:16,030 --> 00:27:19,200
- Well, I'd like to, with
the board, I think you know

600
00:27:19,200 --> 00:27:22,080
the ERM reporting format is really

601
00:27:22,941 --> 00:27:24,960
where things are going now.

602
00:27:24,960 --> 00:27:26,540
And I think that's a good format

603
00:27:26,540 --> 00:27:29,420
for your board and consistent

604
00:27:29,420 --> 00:27:32,333
with what other parts of the
organization will be doing.

605
00:27:35,340 --> 00:27:38,340
I think having metrics

606
00:27:38,340 --> 00:27:42,060
on the more technical controls
are more audience specific.

607
00:27:42,060 --> 00:27:44,570
So you wanna get the metrics

608
00:27:44,570 --> 00:27:47,439
on your vulnerability management results

609
00:27:47,440 --> 00:27:49,860
into the hands of the
system administrators,

610
00:27:49,860 --> 00:27:51,979
the system managers and owners,

611
00:27:51,980 --> 00:27:56,980
so that they can be empowered
to react to those things.

612
00:27:58,360 --> 00:28:00,320
And then that can be reported

613
00:28:02,520 --> 00:28:03,770
in a more narrative format

614
00:28:03,770 --> 00:28:06,683
maybe to your board
via your risk register.

615
00:28:09,047 --> 00:28:10,010
And I really like

616
00:28:10,010 --> 00:28:12,030
in those situations to use scorecards.

617
00:28:12,030 --> 00:28:14,590
So and the scorecards don't need to go

618
00:28:14,590 --> 00:28:19,590
to the board, but having that
scorecard format really helps.

619
00:28:19,770 --> 00:28:22,280
Nobody wants to be last in my experience

620
00:28:23,240 --> 00:28:27,593
or nobody wants to have a very a low bar

621
00:28:29,980 --> 00:28:33,350
where everybody else is
exceeding the expectations.

622
00:28:33,350 --> 00:28:36,750
And so it gamifies it a little bit

623
00:28:36,750 --> 00:28:38,190
and it's really effective with things

624
00:28:38,190 --> 00:28:40,950
like vulnerability management,
security awareness,

625
00:28:40,950 --> 00:28:45,440
training compliance,
anything where wide adoption

626
00:28:45,440 --> 00:28:48,430
or compliance is necessary.

627
00:28:48,430 --> 00:28:51,820
And then with the board, I
also recommend developing

628
00:28:51,820 --> 00:28:55,139
from your risk registers
a heat map of some sort

629
00:28:55,140 --> 00:28:57,610
not with every single risk on it

630
00:28:57,610 --> 00:29:00,479
but whatever the organization has decided

631
00:29:00,480 --> 00:29:04,330
are your top five say risks,

632
00:29:04,330 --> 00:29:06,290
have that heat map and show over time.

633
00:29:06,290 --> 00:29:07,280
It's really important to show

634
00:29:07,280 --> 00:29:11,040
over time how you're
reducing the risks, right?

635
00:29:11,040 --> 00:29:13,920
So, and you may not get the risk

636
00:29:13,920 --> 00:29:15,350
from red to green.

637
00:29:15,350 --> 00:29:19,659
However you wanna display
that kind of maybe

638
00:29:19,660 --> 00:29:20,670
in a heat map format,

639
00:29:20,670 --> 00:29:25,070
but how you've moved
the bar, reduced risk,

640
00:29:25,070 --> 00:29:27,439
you're moving in the right direction.

641
00:29:29,223 --> 00:29:30,450
- I think I'll jump in and say,

642
00:29:30,450 --> 00:29:32,513
I fully agree with you, Leilani,

643
00:29:33,865 --> 00:29:35,340
that having those reports targeted

644
00:29:35,340 --> 00:29:38,800
to the right audience is
critical and measuring things

645
00:29:38,800 --> 00:29:40,320
I think is even more important because

646
00:29:40,320 --> 00:29:42,950
if it really you're doing
something and not measuring it,

647
00:29:42,950 --> 00:29:45,250
you're probably just not
doing it well at all.

648
00:29:46,200 --> 00:29:48,890
And getting them like
some of my examples are,

649
00:29:48,890 --> 00:29:51,520
you mentioned cybersecurity
awareness training.

650
00:29:51,520 --> 00:29:53,340
I've been working hard
on that this past year

651
00:29:53,340 --> 00:29:55,850
and actually getting
some good traction on it.

652
00:29:55,850 --> 00:29:57,199
And mostly as a result

653
00:29:57,200 --> 00:29:59,380
of things that have
happened that were harmful

654
00:29:59,380 --> 00:30:01,920
to us and people felt the
pain, and now they're willing

655
00:30:01,920 --> 00:30:04,310
to pay a little bit more attention to it.

656
00:30:04,310 --> 00:30:05,600
But one of the things I do

657
00:30:05,600 --> 00:30:07,899
with reporting is
sending quarterly reports

658
00:30:07,900 --> 00:30:10,280
to the departmental leaders in each

659
00:30:10,280 --> 00:30:13,180
of the departments kind of
gauging their statistics

660
00:30:13,180 --> 00:30:16,090
and in annual reports,
I'll report that overall

661
00:30:16,090 --> 00:30:18,760
so they can see compared
results to other departments.

662
00:30:18,760 --> 00:30:21,310
Like you said, nobody wants to be last

663
00:30:21,310 --> 00:30:22,850
but other targeted things like

664
00:30:23,861 --> 00:30:26,520
the threat intelligence
information is very important

665
00:30:26,520 --> 00:30:29,460
to me because I think as
cybersecurity experts,

666
00:30:29,460 --> 00:30:31,030
we need to really be on the front edge

667
00:30:31,030 --> 00:30:33,740
of what's going on and
knowing what's out there

668
00:30:34,730 --> 00:30:37,600
but that's a lot of
information to weed through.

669
00:30:37,600 --> 00:30:39,070
So I start each day

670
00:30:39,070 --> 00:30:41,980
by weeding through as much
of that information as I can

671
00:30:41,980 --> 00:30:44,180
and summarizing it into
a meaningful report

672
00:30:44,180 --> 00:30:46,320
and sharing that with stakeholders so that

673
00:30:46,320 --> 00:30:49,580
we don't all have to weed
through all of the information,

674
00:30:49,580 --> 00:30:52,399
but yet the necessary information is

675
00:30:52,400 --> 00:30:54,540
in the hands of the people
that need to have it.

676
00:30:54,540 --> 00:30:57,420
So like you said, targeting
of that information

677
00:30:57,420 --> 00:30:59,193
to the right audiences is critical.

678
00:31:00,690 --> 00:31:01,730
- That's a great point, John.

679
00:31:01,730 --> 00:31:04,170
I think one more thing I would add is,

680
00:31:04,170 --> 00:31:06,330
for a long, long time that
we in the security world

681
00:31:06,330 --> 00:31:08,399
we're trying to teach the board

682
00:31:08,400 --> 00:31:09,810
of directors and the leadership

683
00:31:09,810 --> 00:31:12,490
in the organization to be security people,

684
00:31:12,490 --> 00:31:15,040
I found I needed to go back to
school myself and learn more

685
00:31:15,040 --> 00:31:17,610
about the business side of
things, I needed to understand,

686
00:31:17,610 --> 00:31:20,429
what's the impact of
the cybersecurity things

687
00:31:20,430 --> 00:31:22,870
on operating expenses
and capital expenses.

688
00:31:22,870 --> 00:31:25,379
Then I got an inside view

689
00:31:25,380 --> 00:31:29,191
into my company's SEC
filings and how we had

690
00:31:29,191 --> 00:31:31,780
to describe risk factors
that could have a bearing

691
00:31:31,780 --> 00:31:33,610
on even our share prices.

692
00:31:33,610 --> 00:31:36,949
So I needed to go back myself
and understand what metrics

693
00:31:36,950 --> 00:31:39,520
and measures would really be meaningful

694
00:31:39,520 --> 00:31:41,900
to people who are used to being
in that side of the world.

695
00:31:41,900 --> 00:31:44,250
And it really changed how I look

696
00:31:44,250 --> 00:31:46,510
at key performance indicators
and risk indicators.

697
00:31:46,510 --> 00:31:49,280
It's not that they're
significantly different

698
00:31:49,280 --> 00:31:50,413
but at the way I frame them

699
00:31:50,413 --> 00:31:51,950
and the way I communicate

700
00:31:51,950 --> 00:31:54,150
with those stakeholders changed based

701
00:31:54,150 --> 00:31:55,570
on my better understanding

702
00:31:55,570 --> 00:31:58,280
of what they do from the
business perspective.

703
00:31:58,280 --> 00:32:00,570
- I love that point,
Greg, about understanding

704
00:32:00,570 --> 00:32:02,879
or about reeducating ourselves
about what's important

705
00:32:02,880 --> 00:32:05,170
to them because both Leilani

706
00:32:05,170 --> 00:32:08,210
and I had mentioned
cybersecurity awareness is us

707
00:32:08,210 --> 00:32:09,980
trying to educate other
people and getting them

708
00:32:09,980 --> 00:32:13,090
to understand what's important to us.

709
00:32:13,090 --> 00:32:14,689
So that's a great point
that we need to really

710
00:32:14,690 --> 00:32:16,490
understand what's important to them

711
00:32:16,490 --> 00:32:20,000
and educate ourselves in
knowing more about it.

712
00:32:20,000 --> 00:32:22,150
So that's a good takeaway
for me from this.

713
00:32:24,870 --> 00:32:26,639
- Excellent, no, I agree.

714
00:32:26,640 --> 00:32:28,910
I think, yeah, every
organization is different.

715
00:32:28,910 --> 00:32:30,010
Our risks are different.

716
00:32:30,010 --> 00:32:31,750
Being able to understand
those communications,

717
00:32:31,750 --> 00:32:35,750
being able to translate it
back into terms that make sense

718
00:32:35,750 --> 00:32:38,940
for your specific organization
are very, very key.

719
00:32:38,940 --> 00:32:40,729
And yeah, learning from your business

720
00:32:40,729 --> 00:32:44,380
will help enable you
to have those messages,

721
00:32:44,380 --> 00:32:46,230
those conversations so that those messages

722
00:32:46,230 --> 00:32:48,010
are clearly understood

723
00:32:48,010 --> 00:32:50,710
and the specific risks

724
00:32:50,710 --> 00:32:53,560
within a cybersecurity
program or the specific risks

725
00:32:53,560 --> 00:32:55,990
to the business that the
cybersecurity program

726
00:32:55,990 --> 00:32:59,110
can help address are
considered and improved upon.

727
00:32:59,110 --> 00:33:02,040
So with that, I wanted to thank each

728
00:33:02,040 --> 00:33:04,920
of you for joining us today on the panel

729
00:33:04,920 --> 00:33:07,530
and ask that you stick
around for a little bit

730
00:33:07,530 --> 00:33:10,610
for a few moments more, and
we'll take questions from

731
00:33:10,610 --> 00:33:13,929
or additional questions from
any of the audience today

732
00:33:13,930 --> 00:33:15,660
and thank you.

733
00:33:15,660 --> 00:33:17,590
- Sure, thanks, Tom.

734
00:33:17,590 --> 00:33:19,790
- Thanks for the
opportunity to be with you.

