1
00:00:01,110 --> 00:00:01,943
- Hey everybody.

2
00:00:01,943 --> 00:00:03,480
I'm a Jules Polonetsky.

3
00:00:03,480 --> 00:00:05,810
I'm CEO of the Future of Privacy Forum,

4
00:00:05,810 --> 00:00:08,309
a think tank working with privacy leaders

5
00:00:08,310 --> 00:00:09,810
and executives around the world

6
00:00:09,810 --> 00:00:12,040
on trusted data practices.

7
00:00:12,040 --> 00:00:15,030
And I'm really excited
to be with you at RSA

8
00:00:15,030 --> 00:00:18,145
with two fabulous people, leaders, giants

9
00:00:18,145 --> 00:00:23,145
in the field of privacy,
ethics, policy, legal issues,

10
00:00:23,230 --> 00:00:24,219
data protection.

11
00:00:24,219 --> 00:00:27,340
With me is Julie Brill,
Corporate Vice President

12
00:00:27,340 --> 00:00:31,400
for Global Privacy and
Regulatory Affairs at Microsoft.

13
00:00:31,400 --> 00:00:33,864
Julie plays an incredibly central role

14
00:00:33,865 --> 00:00:37,612
in helping shape Microsoft's practices

15
00:00:37,612 --> 00:00:40,020
on a range of policy issues

16
00:00:40,020 --> 00:00:43,210
and plays a leading role in the industry,

17
00:00:43,210 --> 00:00:45,030
frankly, in working with stakeholders

18
00:00:45,030 --> 00:00:47,920
and advocates and academics
and regulators around the world

19
00:00:47,920 --> 00:00:49,810
when trying to move the ball forward

20
00:00:49,810 --> 00:00:51,419
in this really frothy environment

21
00:00:51,419 --> 00:00:55,199
where companies are trying to
use data advanced technology,

22
00:00:55,199 --> 00:00:57,144
deal with society's challenges

23
00:00:57,145 --> 00:01:01,140
while grappling with the
regulatory and policy risks

24
00:01:01,140 --> 00:01:03,840
that we all are gonna talk about today.

25
00:01:03,840 --> 00:01:07,539
I'm also so excited that
professor Danielle Citron,

26
00:01:07,539 --> 00:01:12,539
now at the University
of Virginia is with me.

27
00:01:12,673 --> 00:01:15,470
Danielle has a long and storied career

28
00:01:15,470 --> 00:01:17,510
working on some of the most interesting

29
00:01:17,510 --> 00:01:21,285
and important privacy and
data protection scholarship,

30
00:01:21,285 --> 00:01:24,970
and has been a mentor
to a generation of us

31
00:01:24,970 --> 00:01:26,640
who've grown up,

32
00:01:26,640 --> 00:01:28,180
grown up she's thought that senior,

33
00:01:28,180 --> 00:01:30,879
but who who've lived the last years

34
00:01:30,879 --> 00:01:33,926
really being shaped by her work.

35
00:01:33,926 --> 00:01:37,385
Julie was at the Federal Trade Commission

36
00:01:37,385 --> 00:01:41,540
before Microsoft helped lead
the privacy security practice

37
00:01:41,540 --> 00:01:43,309
at the global law firm, Hogan Lovells,

38
00:01:43,309 --> 00:01:45,250
worked at one point

39
00:01:45,250 --> 00:01:49,626
for the Attorney General of Vermont and--

40
00:01:49,626 --> 00:01:51,418
- North Carolina, just saying.

41
00:01:51,418 --> 00:01:52,960
- North Carolina.

42
00:01:52,960 --> 00:01:54,838
Both Vermont and North Carolina.

43
00:01:54,838 --> 00:01:55,728
- There you go.

44
00:01:55,728 --> 00:01:56,561
All right.

45
00:01:56,561 --> 00:01:58,460
We got to make sure those states

46
00:01:58,460 --> 00:01:59,976
that they're check boxes.

47
00:01:59,977 --> 00:02:00,990
- Oh yeah.

48
00:02:00,990 --> 00:02:01,823
- So let's jump in.

49
00:02:01,823 --> 00:02:05,184
It's been an incredible year,
obviously for everybody.

50
00:02:05,184 --> 00:02:07,310
And those of us who work

51
00:02:07,310 --> 00:02:09,408
at the intersection of data and technology

52
00:02:09,408 --> 00:02:11,725
obviously have been grappling

53
00:02:11,725 --> 00:02:14,859
with the incredible forces of change,

54
00:02:14,860 --> 00:02:16,470
the digital acceleration,

55
00:02:16,470 --> 00:02:18,440
but also the intense need for data

56
00:02:18,440 --> 00:02:21,863
to track COVID for exposure notification,

57
00:02:21,863 --> 00:02:25,932
to deal with bias and
diversity in machine learning.

58
00:02:25,932 --> 00:02:29,149
I think people living at the
intersection of these issues

59
00:02:29,150 --> 00:02:30,850
have just seen an onslaught

60
00:02:30,850 --> 00:02:32,920
of issues each more
important than the other.

61
00:02:32,920 --> 00:02:34,209
So let's jump in.

62
00:02:34,210 --> 00:02:37,192
One are the big questions that
a lot of us have been asked

63
00:02:37,192 --> 00:02:40,620
is are we giving up
privacy because of COVID,

64
00:02:40,620 --> 00:02:44,520
because we need data clearly saving lives,

65
00:02:44,520 --> 00:02:46,583
battling a pandemic is paramount.

66
00:02:46,583 --> 00:02:48,210
Question after question

67
00:02:48,210 --> 00:02:50,460
that I've been asked by
interviewer or others

68
00:02:51,420 --> 00:02:52,867
aren't we trading away our privacy?

69
00:02:52,867 --> 00:02:54,295
Is it worth it?

70
00:02:54,295 --> 00:02:56,130
So, Julie, let me start with you.

71
00:02:56,130 --> 00:03:00,120
Did we give up privacy
where we're doing education,

72
00:03:00,120 --> 00:03:03,578
where we're doing
research with health data.

73
00:03:03,578 --> 00:03:07,177
Did COVID lead to a world
where we gave up our privacy

74
00:03:07,177 --> 00:03:09,882
because we had to for the better good?

75
00:03:10,862 --> 00:03:15,862
- The pandemic has absolutely
changed, I believe forever,

76
00:03:17,040 --> 00:03:20,609
people's relationship with technology.

77
00:03:20,610 --> 00:03:25,570
We've learned that we can or
many of us can work from home,

78
00:03:25,570 --> 00:03:27,726
be deeply productive,

79
00:03:27,726 --> 00:03:32,726
learn from home and learn quite a bit,

80
00:03:32,920 --> 00:03:35,248
socialize online.

81
00:03:35,248 --> 00:03:38,390
That the connectivity that we have,

82
00:03:38,390 --> 00:03:40,040
the connections that we make

83
00:03:40,040 --> 00:03:42,378
to work, to people, to community

84
00:03:42,378 --> 00:03:47,378
is bolstered by and for a year
has been completely supported

85
00:03:48,650 --> 00:03:52,182
by connectivity and digital technology.

86
00:03:52,182 --> 00:03:56,280
So we've undergone a massive
digital transformation.

87
00:03:56,280 --> 00:03:58,107
I mean, what would normally
take years and years

88
00:03:58,107 --> 00:04:02,169
has been condensed to months and months.

89
00:04:02,169 --> 00:04:04,688
Enormous growth in digital platforms,

90
00:04:04,688 --> 00:04:06,750
we experienced that at Microsoft

91
00:04:06,750 --> 00:04:08,741
and people experienced that elsewhere.

92
00:04:08,741 --> 00:04:12,995
And, I think that there's a
heavy reliance on technology

93
00:04:12,995 --> 00:04:17,995
to sort of run our lives,
work, school, social, family

94
00:04:20,470 --> 00:04:25,170
has also, I think raised
consumers expectations

95
00:04:25,170 --> 00:04:27,330
around how their data is going to be used

96
00:04:27,330 --> 00:04:29,983
and frankly how it should be used.

97
00:04:29,983 --> 00:04:33,460
People are saying more and more

98
00:04:33,460 --> 00:04:37,620
that they're concerned about
how their data is being used

99
00:04:37,620 --> 00:04:40,730
and that they want more privacy.

100
00:04:40,730 --> 00:04:42,670
They want companies to do more

101
00:04:42,670 --> 00:04:44,700
and they want governments to do more

102
00:04:44,700 --> 00:04:48,420
to ensure that their
data is well-protected.

103
00:04:48,420 --> 00:04:52,770
You know what we have right now

104
00:04:52,770 --> 00:04:54,707
in the United States in particular,

105
00:04:54,707 --> 00:04:57,421
but in other places
around the globe as well

106
00:04:57,422 --> 00:05:01,360
is individuals really are
shouldering the burden of privacy

107
00:05:01,360 --> 00:05:02,997
and they're feeling that.

108
00:05:02,997 --> 00:05:07,997
And they want to engage,
they wants to be online,

109
00:05:08,320 --> 00:05:11,694
they want to interact to work, to learn

110
00:05:11,694 --> 00:05:14,902
and to be productive and
to be with their families.

111
00:05:14,902 --> 00:05:19,902
But they don't want to have
to shoulder this burden

112
00:05:19,980 --> 00:05:21,040
all themselves.

113
00:05:21,040 --> 00:05:25,740
And frankly the pandemic
should not be the reason

114
00:05:25,740 --> 00:05:29,270
why people are being asked
to give up their privacy.

115
00:05:29,270 --> 00:05:31,609
So it should be the case

116
00:05:31,609 --> 00:05:34,710
that companies that are
providing these tools,

117
00:05:34,710 --> 00:05:37,229
schools that are using these tools,

118
00:05:37,230 --> 00:05:38,760
companies that are using the tools

119
00:05:38,760 --> 00:05:41,376
and churches and temples
and community groups

120
00:05:41,376 --> 00:05:42,679
need to be thinking

121
00:05:42,680 --> 00:05:46,307
about ensuring they're
using trusted technology,

122
00:05:46,307 --> 00:05:49,720
need to ensure that parents
know what their kids are doing.

123
00:05:49,720 --> 00:05:51,964
I mean, we need to lean in on these issues

124
00:05:51,964 --> 00:05:55,560
so people engage with
controls that they have.

125
00:05:55,560 --> 00:05:57,471
That companies need to do more.

126
00:05:57,471 --> 00:05:59,137
Companies need to make sure

127
00:05:59,137 --> 00:06:03,900
that even in the absence
of a comprehensive law,

128
00:06:03,900 --> 00:06:07,460
which is still what we're
experiencing in the United States

129
00:06:07,460 --> 00:06:09,510
that they are providing
the tools that people need

130
00:06:09,511 --> 00:06:12,190
so that they can trust these technologies,

131
00:06:12,190 --> 00:06:15,480
because otherwise trust
in them will dissipate.

132
00:06:15,480 --> 00:06:18,730
We do know people will be
working more from home,

133
00:06:18,730 --> 00:06:20,458
we will have a hybrid environment,

134
00:06:20,458 --> 00:06:22,950
people want to work from home,

135
00:06:22,950 --> 00:06:26,770
but they want to learn
and work and socialize

136
00:06:26,770 --> 00:06:27,880
through trusted technology.

137
00:06:27,880 --> 00:06:29,363
So companies need to do more.

138
00:06:30,434 --> 00:06:31,500
- Now from where you sit,

139
00:06:31,500 --> 00:06:34,820
you've recently written
a influential paper

140
00:06:34,820 --> 00:06:39,820
of really helping try to
define what harms actually are.

141
00:06:39,930 --> 00:06:43,140
Has this been a year of
privacy harms with, I dunno,

142
00:06:43,140 --> 00:06:44,780
we're talking about vaccine passports.

143
00:06:44,780 --> 00:06:46,309
There are so many issues

144
00:06:46,310 --> 00:06:49,308
that seem to have been flapping
up and creating concerns.

145
00:06:49,308 --> 00:06:52,800
Has this been a year of
privacy concerns for you?

146
00:06:52,800 --> 00:06:56,257
- It's interesting that the thought that,

147
00:06:56,257 --> 00:06:59,450
we've always been under
surveillance by companies

148
00:06:59,450 --> 00:07:01,490
in ways that we can't even fathom.

149
00:07:01,490 --> 00:07:04,390
But I think really important
to underscore Julie's point

150
00:07:04,390 --> 00:07:06,070
that now they're salient, right?

151
00:07:06,070 --> 00:07:08,565
We can't feel data when we give it up.

152
00:07:08,565 --> 00:07:11,219
It doesn't ping, it doesn't sound.

153
00:07:11,220 --> 00:07:13,201
There are all of these ways
in which we are always,

154
00:07:13,201 --> 00:07:14,710
or information is being,

155
00:07:14,710 --> 00:07:16,840
the most intimate
information about our lives

156
00:07:16,840 --> 00:07:18,760
is being collected, used and shared,

157
00:07:18,760 --> 00:07:22,281
resold and resold again in
ways that we can't control,

158
00:07:22,281 --> 00:07:23,989
can't even comprehend,

159
00:07:23,989 --> 00:07:27,659
and couldn't, even if we're
given the right to say,

160
00:07:27,660 --> 00:07:29,440
hey don't use it, don't sell it,

161
00:07:29,440 --> 00:07:30,880
we just couldn't possibly contact

162
00:07:30,880 --> 00:07:32,019
the thousands of companies.

163
00:07:32,019 --> 00:07:33,970
And so I think that's right.

164
00:07:33,970 --> 00:07:35,547
It's given us a bird's eye view

165
00:07:35,547 --> 00:07:39,040
into the way in which
our lives are surveilled.

166
00:07:39,040 --> 00:07:42,183
And so just then to return
to the harms question,

167
00:07:43,760 --> 00:07:45,480
we have such a problem in the law

168
00:07:45,480 --> 00:07:48,130
with figuring out what the harm is.

169
00:07:48,130 --> 00:07:50,027
And in part it's 'cause the privacy torts

170
00:07:50,027 --> 00:07:52,180
just are not well suited

171
00:07:52,180 --> 00:07:53,763
to the problems of the 21st century.

172
00:07:53,763 --> 00:07:55,770
It's like it was made for an age

173
00:07:55,770 --> 00:07:58,890
where we had mass media
publishing stories about people

174
00:07:58,891 --> 00:08:02,640
and advertisers taking someone's face

175
00:08:02,640 --> 00:08:04,070
and then taking it as its own.

176
00:08:04,070 --> 00:08:06,380
And you had sort of the
one to many distribution.

177
00:08:06,380 --> 00:08:08,967
And now so many of our
21st century problems

178
00:08:08,967 --> 00:08:11,560
are about the collection,

179
00:08:11,560 --> 00:08:13,590
the use and the sale of information,

180
00:08:13,590 --> 00:08:16,421
not giving up publicity
to the world at large.

181
00:08:16,421 --> 00:08:19,305
But it's storage, it's collection.

182
00:08:19,305 --> 00:08:22,970
And so the proxy torts just
literally provide us no help.

183
00:08:22,970 --> 00:08:24,770
And the difficulty with the harm problem

184
00:08:24,770 --> 00:08:26,840
is then plaintiffs do whatever they can.

185
00:08:26,840 --> 00:08:29,390
They're like, all right
I'll use 86 other torts

186
00:08:29,390 --> 00:08:30,296
to give it a whirl.

187
00:08:30,296 --> 00:08:32,179
But tort law and civil claims

188
00:08:32,179 --> 00:08:36,120
either haven't quite caught up

189
00:08:36,120 --> 00:08:38,740
and courts insist upon financial,

190
00:08:38,740 --> 00:08:42,938
like really tangible harms that
are financial and physical,

191
00:08:42,938 --> 00:08:44,530
and they haven't caught up with,

192
00:08:44,530 --> 00:08:46,589
so Dan Solav and I wrote this paper

193
00:08:46,590 --> 00:08:49,881
in which we go through like a taxonomy

194
00:08:49,881 --> 00:08:51,890
of different privacy harms

195
00:08:51,890 --> 00:08:55,069
that aren't captured by,

196
00:08:55,070 --> 00:08:57,530
sort of vested and visceral
interests that we can feel

197
00:08:57,530 --> 00:08:58,819
and in the here and the now

198
00:08:58,820 --> 00:09:00,250
and you can calculate them right now,

199
00:09:00,250 --> 00:09:03,655
'cause it's identity theft and
what it precisely costs you.

200
00:09:03,655 --> 00:09:07,310
But it includes our thwarted expectations.

201
00:09:07,310 --> 00:09:09,359
It includes our autonomy harms.

202
00:09:09,360 --> 00:09:12,810
It includes our relationship harms

203
00:09:12,810 --> 00:09:16,890
and not only breach of
confidentiality, but also the trust.

204
00:09:16,890 --> 00:09:18,620
This is Julie's important point.

205
00:09:18,620 --> 00:09:21,480
The trust that we have
in those relationships

206
00:09:21,480 --> 00:09:22,892
in the longer term.

207
00:09:22,892 --> 00:09:24,530
It's discrimination harm.

208
00:09:24,530 --> 00:09:25,439
So a lot of my work

209
00:09:25,440 --> 00:09:30,311
has been about the civil rights
implications of online life.

210
00:09:30,311 --> 00:09:34,010
And my newest book is about,
it's called "The Privacy Myth"

211
00:09:34,010 --> 00:09:36,939
but it's about how we turned
our intimate lives into data

212
00:09:36,940 --> 00:09:39,104
and how we need to protect it.

213
00:09:39,104 --> 00:09:41,530
The implications for women and minorities

214
00:09:41,530 --> 00:09:42,704
are just different, right?

215
00:09:42,705 --> 00:09:45,950
When our information is
collected, used, sorted, sold

216
00:09:45,950 --> 00:09:47,320
and algorithmically stored.

217
00:09:47,320 --> 00:09:51,071
And so we have a long way to go.

218
00:09:51,071 --> 00:09:54,037
We don't understand
discrimination as a privacy harm

219
00:09:54,037 --> 00:09:55,390
and we should.

220
00:09:55,390 --> 00:09:58,553
We don't understand thwarted
expectations as a privacy harm

221
00:09:58,553 --> 00:09:59,745
and we should.

222
00:09:59,745 --> 00:10:03,260
And so we do have a long way to go

223
00:10:03,260 --> 00:10:07,900
and sort of resting on
control is almost impossible

224
00:10:07,900 --> 00:10:10,720
because vis-a-vis thousands of companies,

225
00:10:10,720 --> 00:10:12,179
I can't do it.

226
00:10:12,179 --> 00:10:14,240
And it is heartening though.

227
00:10:14,240 --> 00:10:16,104
I have to say, as someone
who writes in this field,

228
00:10:16,104 --> 00:10:19,500
seeing Julie go from state AGs office

229
00:10:19,500 --> 00:10:21,080
to commissioner at the FTC

230
00:10:21,080 --> 00:10:24,530
and now basically running
Microsoft's privacy,

231
00:10:24,530 --> 00:10:28,130
when Julie tweets like
privacy is a human right,

232
00:10:28,130 --> 00:10:29,329
get your act together as folks

233
00:10:29,329 --> 00:10:31,333
I'm like, man this is a dream.

234
00:10:31,333 --> 00:10:35,228
Like if every company had at the helm

235
00:10:35,229 --> 00:10:38,403
I think self-regulation
would mean something.

236
00:10:39,470 --> 00:10:44,336
And that was the ask in the late '90s

237
00:10:44,336 --> 00:10:49,336
When Sergey Brin and Larry
Paige met with the state AGs.

238
00:10:50,170 --> 00:10:53,199
They said, trust us, we're
gonna take care of this data.

239
00:10:53,200 --> 00:10:54,980
They didn't realize they
needed to monetize it

240
00:10:54,980 --> 00:10:56,690
Or they hadn't quite figured that out yet.

241
00:10:56,690 --> 00:11:00,430
And there was that bargain
struct of self-regulation

242
00:11:00,430 --> 00:11:02,315
and the deal of self-policing

243
00:11:02,316 --> 00:11:04,103
is there has to be self-policing.

244
00:11:05,710 --> 00:11:07,080
And if everyone's not Julie,

245
00:11:07,080 --> 00:11:08,970
I'm very worried about
what that world looks like.

246
00:11:08,970 --> 00:11:13,530
So I can end there, but.

247
00:11:13,530 --> 00:11:15,220
- You listed such a range of issues,

248
00:11:15,220 --> 00:11:19,575
civil rights issues, and intimate harms,

249
00:11:19,575 --> 00:11:22,850
and we hear a lot about dark patterns,

250
00:11:22,850 --> 00:11:24,640
which are deception

251
00:11:24,640 --> 00:11:27,055
that really is part of
consumer protection law.

252
00:11:27,055 --> 00:11:29,709
And we know that automakers want data

253
00:11:29,710 --> 00:11:31,600
for autonomous driving,

254
00:11:31,600 --> 00:11:35,283
and that healthcare researchers
want data for health.

255
00:11:35,283 --> 00:11:37,481
Is privacy law,

256
00:11:37,481 --> 00:11:40,260
and we don't have a comprehensive
privacy law in the US

257
00:11:40,260 --> 00:11:43,530
and Julie and you and
others are working as are we

258
00:11:43,530 --> 00:11:44,550
to try to get one,

259
00:11:44,550 --> 00:11:46,877
but is it reasonable to think

260
00:11:46,878 --> 00:11:49,490
that the issues of the world,

261
00:11:49,490 --> 00:11:51,120
every problem in the world

262
00:11:51,120 --> 00:11:54,814
seems to be played out now
with technology, with data.

263
00:11:54,814 --> 00:11:59,814
Isn't it quite a burden
to somehow ask privacy law

264
00:12:00,240 --> 00:12:03,293
to solve all these problems of society?

265
00:12:03,293 --> 00:12:06,440
In Europe, perhaps the GDPR,

266
00:12:06,440 --> 00:12:08,185
where independent data regulators

267
00:12:08,185 --> 00:12:12,347
really get to decide is the
data needed for healthcare?

268
00:12:12,347 --> 00:12:14,860
No, that is too much.

269
00:12:14,860 --> 00:12:15,700
It's not necessary.

270
00:12:15,700 --> 00:12:16,804
It's not proportionate.

271
00:12:16,804 --> 00:12:18,520
And they need to be experts

272
00:12:18,520 --> 00:12:22,140
in this incredible range of domains

273
00:12:22,140 --> 00:12:23,780
because they're telling companies,

274
00:12:23,780 --> 00:12:25,870
no, that's too much telemetry.

275
00:12:25,870 --> 00:12:28,513
No, that's not enough.

276
00:12:30,778 --> 00:12:34,133
Do you see, Julie you do
business around the world

277
00:12:34,133 --> 00:12:35,893
on the Microsoft side.

278
00:12:36,760 --> 00:12:39,740
Are the Europeans handling
this differently and better

279
00:12:39,740 --> 00:12:41,580
because they have a comprehensive law?

280
00:12:41,580 --> 00:12:44,110
Are the states, Virginia
has just passed the law,

281
00:12:44,110 --> 00:12:45,027
California passed the law,

282
00:12:45,027 --> 00:12:47,343
Washington state is wrestling.

283
00:12:49,878 --> 00:12:52,927
The audience watching
this at RSA is global.

284
00:12:54,070 --> 00:12:56,800
Are the issues too wide to
be handled by privacy law

285
00:12:56,800 --> 00:12:58,640
and are different regions of the world

286
00:12:58,640 --> 00:12:59,740
handling this differently?

287
00:12:59,740 --> 00:13:02,100
Are we all gonna adopt
some version of the GDPR?

288
00:13:02,100 --> 00:13:04,380
What's gonna happen in the US?

289
00:13:04,380 --> 00:13:05,213
What do you say?

290
00:13:06,590 --> 00:13:10,188
- Great series of about 25
questions packed in there.

291
00:13:10,188 --> 00:13:11,810
(laughs)

292
00:13:11,810 --> 00:13:14,959
I'd love to address all.

293
00:13:14,960 --> 00:13:19,960
And listen on some level
I think there is a hope

294
00:13:21,130 --> 00:13:24,713
that privacy law can
address a number of issues.

295
00:13:24,713 --> 00:13:26,786
Danielle talked about some of them

296
00:13:26,786 --> 00:13:29,449
is you point out there
are some privacy laws

297
00:13:29,450 --> 00:13:31,406
that focus on use of data

298
00:13:31,406 --> 00:13:35,010
in discriminatory ways in
breach of civil rights laws.

299
00:13:35,010 --> 00:13:37,910
We also know that GDPR,

300
00:13:37,910 --> 00:13:40,610
the European General Data
Protection Regulation

301
00:13:40,610 --> 00:13:42,050
talks about portability,

302
00:13:42,050 --> 00:13:43,979
which on some level is about privacy

303
00:13:43,980 --> 00:13:45,957
but it's also actually about competition.

304
00:13:45,957 --> 00:13:47,600
So there are a lot of,

305
00:13:47,600 --> 00:13:50,830
I'll say hopes and
aspirations if you will,

306
00:13:50,830 --> 00:13:51,667
for privacy laws.

307
00:13:51,667 --> 00:13:53,528
But I think there's a reason for that.

308
00:13:53,528 --> 00:13:58,528
I think because people's
relationship to who they are

309
00:14:00,820 --> 00:14:02,742
and how they want to be portrayed

310
00:14:02,742 --> 00:14:05,060
has often been framed

311
00:14:05,060 --> 00:14:10,060
in the context of control,
empowerment, and engagement.

312
00:14:11,749 --> 00:14:15,080
And at bottom, when you
really think about it

313
00:14:15,080 --> 00:14:17,630
that's what privacy laws are about.

314
00:14:17,630 --> 00:14:20,403
They're about choosing
when you're going to engage

315
00:14:20,403 --> 00:14:22,530
and how you will engage.

316
00:14:22,530 --> 00:14:26,174
Not having that defined
for you unnecessarily

317
00:14:26,174 --> 00:14:31,174
but having you have the ability to say no,

318
00:14:31,630 --> 00:14:34,520
I don't want my data used in that way,

319
00:14:34,520 --> 00:14:36,790
or I want to really understand

320
00:14:36,790 --> 00:14:38,502
what you're gonna do with my data.

321
00:14:38,502 --> 00:14:41,335
So I think a lot of
these other aspirations

322
00:14:41,336 --> 00:14:43,162
for privacy law

323
00:14:43,162 --> 00:14:46,120
actually sort of makes sense

324
00:14:46,120 --> 00:14:49,363
because of how fundamental privacy is.

325
00:14:50,540 --> 00:14:53,180
I often do say exactly what Danielle said.

326
00:14:53,180 --> 00:14:54,769
I say that privacy is a fundamental right

327
00:14:54,769 --> 00:14:58,160
and I'm very proud that
Microsoft believes that

328
00:14:58,160 --> 00:14:59,680
and actualizes it.

329
00:14:59,680 --> 00:15:00,849
But in some ways it's also

330
00:15:00,850 --> 00:15:02,490
a foundational fundamental right.

331
00:15:02,490 --> 00:15:03,738
It's like without privacy

332
00:15:03,738 --> 00:15:06,537
how do you do so many other things?

333
00:15:06,537 --> 00:15:08,260
How do you travel?

334
00:15:08,260 --> 00:15:09,860
How do you engage in speech?

335
00:15:09,860 --> 00:15:12,438
How do you exercise
all these other rights?

336
00:15:12,438 --> 00:15:16,180
So bringing that to some of
your other points, Jules,

337
00:15:16,180 --> 00:15:17,410
just really quickly,

338
00:15:17,410 --> 00:15:20,620
look we definitely see that regulators

339
00:15:20,620 --> 00:15:22,550
and actually I should say parliaments

340
00:15:22,550 --> 00:15:24,569
and legislators around the globe

341
00:15:24,570 --> 00:15:27,896
are enacting more privacy laws.

342
00:15:27,896 --> 00:15:28,949
They're doing that

343
00:15:28,950 --> 00:15:32,236
because the Europeans are
encouraging them to do that.

344
00:15:32,236 --> 00:15:35,640
there's a part of GDPR
that basically says,

345
00:15:35,640 --> 00:15:36,528
if you're like us

346
00:15:36,528 --> 00:15:40,320
you will be able to send
European citizens data

347
00:15:40,320 --> 00:15:41,749
to your jurisdiction.

348
00:15:41,749 --> 00:15:44,160
Good for the Europeans for figuring out

349
00:15:44,160 --> 00:15:47,069
that that was a good way
to sort of raise the bar,

350
00:15:47,070 --> 00:15:50,570
have everyone around the
world have a race to the top.

351
00:15:50,570 --> 00:15:53,030
And I think you're seeing
that race to the top

352
00:15:53,030 --> 00:15:53,863
around the globe.

353
00:15:53,863 --> 00:15:58,280
You are seeing Australia,
New Zealand, Hong Kong,

354
00:15:58,280 --> 00:16:03,280
enacting laws, and amending
laws to make them even stronger.

355
00:16:03,810 --> 00:16:05,140
And to your point, Jules,

356
00:16:05,140 --> 00:16:07,220
we're seeing in the United States,

357
00:16:07,220 --> 00:16:09,410
there's a lot of conversations
in the United States

358
00:16:09,410 --> 00:16:12,170
about a comprehensive federal law.

359
00:16:12,170 --> 00:16:15,540
And those are very important
conversations, Jules,

360
00:16:15,540 --> 00:16:16,569
as you point out.

361
00:16:16,570 --> 00:16:19,200
I'm very involved, you're
involved and Yale is involved.

362
00:16:19,200 --> 00:16:22,950
We're all thinking, what
should that law look like?

363
00:16:22,950 --> 00:16:25,150
We needed it yesterday

364
00:16:25,150 --> 00:16:29,140
but how do we make it happen
as soon as possible tomorrow?

365
00:16:29,140 --> 00:16:31,883
But in the meantime,
the states are acting.

366
00:16:33,070 --> 00:16:38,070
The laboratories of democracy
are working at full steam.

367
00:16:38,660 --> 00:16:42,160
California has adopted
not just one, but two,

368
00:16:42,160 --> 00:16:44,839
one through initiative,
one through the legislature

369
00:16:44,840 --> 00:16:47,460
and now Virginia has passed its law

370
00:16:47,460 --> 00:16:48,390
and other states,

371
00:16:48,390 --> 00:16:49,740
I don't know., I think by last count,

372
00:16:49,740 --> 00:16:53,900
there's about 40 different
privacy laws being discussed

373
00:16:53,900 --> 00:16:55,814
at various levels in the states.

374
00:16:55,814 --> 00:16:57,901
What this says to me

375
00:16:57,901 --> 00:17:02,901
is that there's a real
need that people feel.

376
00:17:05,720 --> 00:17:10,240
The pandemic and COVID has
accelerated this feeling,

377
00:17:10,240 --> 00:17:12,400
has sharpened this feeling

378
00:17:12,400 --> 00:17:14,973
but it was something that
existed before the pandemic.

379
00:17:14,973 --> 00:17:16,649
And these efforts

380
00:17:16,650 --> 00:17:19,034
are going to continue to
sweep around the globe.

381
00:17:19,034 --> 00:17:21,349
Technology, as I said,

382
00:17:21,349 --> 00:17:24,741
has has demonstrated how
important it is in our lives,

383
00:17:24,741 --> 00:17:28,290
how central it is to staying connected,

384
00:17:28,290 --> 00:17:30,270
but also that raises the concerns

385
00:17:30,270 --> 00:17:32,340
about am I really in control

386
00:17:32,340 --> 00:17:34,456
and is this company really to be trusted?

387
00:17:34,456 --> 00:17:38,659
So I think we're gonna see
more laws around the world.

388
00:17:38,660 --> 00:17:41,810
I think many of them will
be inspired by Europe,

389
00:17:41,810 --> 00:17:45,290
and we can maybe talk a
little bit in a little while

390
00:17:45,290 --> 00:17:48,710
about sort of the distinctions
between the US and Europe.

391
00:17:48,710 --> 00:17:50,100
But I think we will see more of this

392
00:17:50,100 --> 00:17:52,727
because I think it's just
a fundamental human need

393
00:17:52,728 --> 00:17:57,728
that policy makers need to
fulfill because people need it,

394
00:17:58,230 --> 00:18:00,450
and people are asking for it.

395
00:18:00,450 --> 00:18:02,200
- Danielle, you were awarded

396
00:18:02,200 --> 00:18:04,960
one of the famous MacArthur Genius Awards.

397
00:18:04,960 --> 00:18:07,770
And I think what was being recognized

398
00:18:07,770 --> 00:18:09,040
in addition to the work that you've done

399
00:18:09,040 --> 00:18:10,510
on privacy and data protection

400
00:18:10,510 --> 00:18:14,170
is some of the work that
you've done on intimate harms,

401
00:18:14,170 --> 00:18:17,110
on sharing of non-consensual photo

402
00:18:17,110 --> 00:18:19,526
or non-consensual sharing of photos

403
00:18:19,527 --> 00:18:22,670
on some of the Section 230 issues,

404
00:18:22,670 --> 00:18:24,980
when can and should companies be liable

405
00:18:24,980 --> 00:18:27,513
to take things down that
can be causing harm.

406
00:18:28,480 --> 00:18:29,880
There's I think some awareness

407
00:18:29,880 --> 00:18:32,230
that the pandemic year
has been a challenging one

408
00:18:32,230 --> 00:18:34,220
for issues like domestic violence.

409
00:18:34,220 --> 00:18:37,850
People are cooped up, tensions
are high, bad things happen.

410
00:18:37,850 --> 00:18:40,540
What's the year meant for you

411
00:18:40,540 --> 00:18:43,129
looking at these issues legally,

412
00:18:43,130 --> 00:18:48,130
and has this exacerbated
some of these intimate harms

413
00:18:48,350 --> 00:18:51,389
that you've argued need
broader legal protection

414
00:18:51,390 --> 00:18:52,840
than we have today?

415
00:18:52,840 --> 00:18:54,000
- Yeah, sadly so.

416
00:18:54,000 --> 00:18:56,910
I feel like I say this with
my colleague Marianne Franks,

417
00:18:56,910 --> 00:18:58,780
who together, she's the president

418
00:18:58,780 --> 00:19:01,170
I'm the vice president of the
Cyber Civil Rights Initiative.

419
00:19:01,170 --> 00:19:03,793
Like our work is a growth industry, sadly,

420
00:19:05,135 --> 00:19:07,435
violations of intimate
privacy are on the rise,

421
00:19:07,435 --> 00:19:10,091
and by quadruple fold.

422
00:19:10,092 --> 00:19:12,121
That is like in Australia,

423
00:19:12,121 --> 00:19:16,429
the complaints for any type
of non-consensual taking,

424
00:19:16,429 --> 00:19:21,000
sharing, publicizing of intimate imagery

425
00:19:21,000 --> 00:19:24,790
is up by 250% in the first
six months of the pandemic.

426
00:19:24,790 --> 00:19:26,706
We've seen it at CCRI.

427
00:19:26,706 --> 00:19:31,706
All sextortion is tripling it's
occurrence in South Africa,

428
00:19:33,690 --> 00:19:34,523
in France.

429
00:19:34,523 --> 00:19:36,039
So it's not just the United States

430
00:19:36,039 --> 00:19:37,414
it's uniquely experiencing,

431
00:19:37,414 --> 00:19:40,110
in Australia uniquely experiencing a rise

432
00:19:40,110 --> 00:19:43,750
of the exploitation of weaponization

433
00:19:43,750 --> 00:19:45,500
of your intimate information.

434
00:19:45,500 --> 00:19:47,610
And so I feel like every day

435
00:19:47,610 --> 00:19:51,729
I got another email from someone, help,

436
00:19:51,730 --> 00:19:53,410
who do I go to?

437
00:19:53,410 --> 00:19:56,570
And I think one encouraging
thing is that with CCRI

438
00:19:56,570 --> 00:19:59,600
we've had law firms start
to volunteer to work with us

439
00:19:59,600 --> 00:20:00,782
for pro bono counsel.

440
00:20:00,782 --> 00:20:03,770
And there's nothing better
than when someone writes to you

441
00:20:03,770 --> 00:20:06,862
to be able to say, I think
I have a lawyer for you

442
00:20:06,862 --> 00:20:08,719
because for years and years,

443
00:20:08,719 --> 00:20:11,420
we just pulling our hair out

444
00:20:11,420 --> 00:20:14,280
doing our best to change
the laws around the country

445
00:20:14,280 --> 00:20:17,672
and to get law enforcement
to enforce them.

446
00:20:17,672 --> 00:20:20,158
And so it's been a real struggle.

447
00:20:20,158 --> 00:20:25,158
And so often what the victims
want is for it to stop.

448
00:20:25,266 --> 00:20:28,050
Their lives are so upended.

449
00:20:28,050 --> 00:20:30,790
It feels so insecure and unsafe.

450
00:20:30,790 --> 00:20:34,060
And even searching yourself is a nightmare

451
00:20:34,060 --> 00:20:37,631
because when will another
intimate photo show up

452
00:20:37,632 --> 00:20:39,260
and where on what site

453
00:20:39,260 --> 00:20:40,800
and how do I figure out to take it down?

454
00:20:40,800 --> 00:20:45,620
And unfortunately, too many
sites that's their business.

455
00:20:45,620 --> 00:20:48,409
And so there are over 9,500 sites

456
00:20:48,410 --> 00:20:52,610
whose business model is intimate imagery.

457
00:20:52,610 --> 00:20:55,310
Peeping Toms, deep fake sex videos.

458
00:20:55,310 --> 00:20:56,200
It's depressing.

459
00:20:56,200 --> 00:20:59,110
And they won't take it down because why?

460
00:20:59,110 --> 00:21:01,276
It's like their clicks and
shares and subscribers.

461
00:21:01,277 --> 00:21:02,647
For 19.99 a month,

462
00:21:02,647 --> 00:21:06,790
their subscribers are expecting a flow

463
00:21:06,790 --> 00:21:08,139
of non-consensual intimate images.

464
00:21:08,139 --> 00:21:12,620
And so I'm hoping that
this helps lawmakers,

465
00:21:12,620 --> 00:21:14,649
I've been working to a
bunch of work on the Hill,

466
00:21:14,650 --> 00:21:15,880
to get them to see

467
00:21:15,880 --> 00:21:18,970
that we need some sections 230 reform.

468
00:21:18,970 --> 00:21:21,020
We shouldn't get rid of it, but we do.

469
00:21:21,020 --> 00:21:23,440
We need to have the immunity conditioned

470
00:21:23,440 --> 00:21:24,900
on being a good Samaritan

471
00:21:24,900 --> 00:21:27,960
and bad Samaritans certainly
shouldn't enjoy it.

472
00:21:27,960 --> 00:21:29,410
But it's sadly true

473
00:21:29,410 --> 00:21:31,037
that the more we're in
front of our computers,

474
00:21:31,037 --> 00:21:36,037
the more we are seeing
criminals, domestic abusers

475
00:21:37,522 --> 00:21:40,157
take advantage of these tools in ways,

476
00:21:40,157 --> 00:21:42,059
so it's both strangers and people we know,

477
00:21:42,059 --> 00:21:44,800
people we trust, betray our trust,

478
00:21:44,800 --> 00:21:46,870
and it's been sort of devastating,

479
00:21:46,870 --> 00:21:48,899
I suppose helpful in
writing a book about it

480
00:21:48,900 --> 00:21:50,830
and why it's a privacy matters

481
00:21:50,830 --> 00:21:51,990
because as Julie says so, well,

482
00:21:51,990 --> 00:21:54,754
it is a precondition for law.

483
00:21:54,754 --> 00:21:58,650
Privacy is the precondition,
it's the oxygen

484
00:21:58,651 --> 00:22:03,450
for self-development and
authentic identities and dignity.

485
00:22:03,450 --> 00:22:07,310
And so without that, who
are we but shallow people.

486
00:22:07,310 --> 00:22:10,120
So we've got some work to do

487
00:22:10,120 --> 00:22:11,879
but I'm always super gratified

488
00:22:11,880 --> 00:22:14,040
that FPF has been so supportive

489
00:22:14,040 --> 00:22:16,550
of the work that we do at the
Cyber Civil Rights Initiative

490
00:22:16,550 --> 00:22:17,570
and same with Microsoft,

491
00:22:17,570 --> 00:22:22,570
so that we have some important
champions behind the issue.

492
00:22:23,400 --> 00:22:24,477
But it's gonna take us,

493
00:22:24,477 --> 00:22:25,590
it's like what do they say,

494
00:22:25,590 --> 00:22:28,699
it's a slow march with
social and cultural forces

495
00:22:28,700 --> 00:22:30,850
which are behind all this stuff.

496
00:22:30,850 --> 00:22:32,205
- Let's be clear for the audience.

497
00:22:32,205 --> 00:22:36,370
The challenge is that companies
don't wanna be responsible

498
00:22:36,371 --> 00:22:41,371
for having to review every
comment or every post in advance.

499
00:22:41,432 --> 00:22:44,191
That just doesn't scale and--

500
00:22:44,191 --> 00:22:46,956
- And they won't respond
to it also, Jules.

501
00:22:46,956 --> 00:22:49,461
They don't give a, sorry, bleep.

502
00:22:49,461 --> 00:22:51,650
And they're like too bad, so sad,

503
00:22:51,650 --> 00:22:53,608
actually, this is how we make our money.

504
00:22:53,608 --> 00:22:57,600
So it's not just the pre-screening costs,

505
00:22:57,600 --> 00:23:00,689
but it's the ex-post cost at all

506
00:23:00,690 --> 00:23:01,790
they're not interested in

507
00:23:01,790 --> 00:23:03,080
because that's how they make their money.

508
00:23:03,080 --> 00:23:05,114
And it's true there are some
companies who actually care

509
00:23:05,114 --> 00:23:06,963
and it's scale it's hard.

510
00:23:08,060 --> 00:23:10,804
So working with Twitter and
Facebook as I have for 12 years

511
00:23:10,805 --> 00:23:13,743
they've been trying when
it's in their self-interest.

512
00:23:15,340 --> 00:23:18,290
But at least they've been trying.

513
00:23:18,290 --> 00:23:20,610
- Well, it's clear congress
wants to turn up the pressure

514
00:23:20,610 --> 00:23:23,379
not only in this area,
but in the range of areas.

515
00:23:23,380 --> 00:23:24,950
The challenge just seems to be

516
00:23:24,950 --> 00:23:27,760
that the Democrats want companies

517
00:23:27,760 --> 00:23:29,460
to take down much more

518
00:23:29,460 --> 00:23:33,035
because of hate speech and
election misinformation

519
00:23:33,035 --> 00:23:35,330
and conservatives and Republicans

520
00:23:35,330 --> 00:23:37,899
think that they're being overmoderated

521
00:23:37,900 --> 00:23:39,070
for having their views

522
00:23:39,070 --> 00:23:41,240
and the companies are
sort of in the middle

523
00:23:41,240 --> 00:23:43,730
being yelled at by both sides

524
00:23:43,730 --> 00:23:45,760
and whether or not reasonable compromises

525
00:23:45,760 --> 00:23:47,536
like some of that you
have advanced and others

526
00:23:47,536 --> 00:23:49,640
are feasible in this heated environment

527
00:23:49,640 --> 00:23:53,245
I think is the challenge.

528
00:23:53,245 --> 00:23:54,892
Let me raise a different issue.

529
00:23:55,824 --> 00:23:58,750
The current White House,

530
00:23:58,750 --> 00:24:01,290
and frankly, governments around the world

531
00:24:01,290 --> 00:24:05,680
are focused on how to unlock data.

532
00:24:05,680 --> 00:24:08,750
The European Union has a big strategy.

533
00:24:08,750 --> 00:24:11,663
They'd like to catch up and compete.

534
00:24:11,663 --> 00:24:13,830
They want data available for AI,

535
00:24:13,830 --> 00:24:16,810
they want data
collaboratives, data altruism.

536
00:24:16,810 --> 00:24:18,810
The Biden administration here in the US

537
00:24:18,810 --> 00:24:22,946
has said we wanna unlock
data to unleash innovation.

538
00:24:22,946 --> 00:24:26,206
We want data to train AI

539
00:24:26,206 --> 00:24:30,729
but we also wanna understand
who did COVID impact?

540
00:24:30,730 --> 00:24:34,870
Did it impact one class of
people, one ethnicity, one race,

541
00:24:34,870 --> 00:24:36,489
which kids missed out.

542
00:24:36,490 --> 00:24:37,660
And that means we want data

543
00:24:37,660 --> 00:24:39,160
about some very sensitive things,

544
00:24:39,160 --> 00:24:41,104
your health, your students.

545
00:24:41,104 --> 00:24:46,104
How do we build fair
machine learning models?

546
00:24:46,898 --> 00:24:48,350
How do we have the data

547
00:24:48,350 --> 00:24:50,476
so that we know whether or
not we're discriminating?

548
00:24:50,477 --> 00:24:53,251
How do we have the data when
it's really sensitive data?

549
00:24:53,251 --> 00:24:55,500
It's the highest risk data.

550
00:24:55,500 --> 00:24:57,344
It's race, it's sexual orientation.

551
00:24:57,344 --> 00:24:59,903
Are we also at sort of a catch 20?

552
00:24:59,903 --> 00:25:01,420
We want data more than ever,

553
00:25:01,420 --> 00:25:04,012
we understand discrimination
is a big problem

554
00:25:04,012 --> 00:25:07,178
but it's actually that
data and opening it up

555
00:25:07,178 --> 00:25:10,270
that is gonna risk some of these problems.

556
00:25:10,270 --> 00:25:14,129
Julie, is federal legislation
gonna somehow solve this?

557
00:25:14,130 --> 00:25:15,454
How do we deal with the fact that

558
00:25:15,454 --> 00:25:17,899
we want data for equitable purposes,

559
00:25:17,900 --> 00:25:21,269
but we also are worried
about the downsides.

560
00:25:21,269 --> 00:25:26,060
- One of the things that
I think inhibited the US

561
00:25:26,060 --> 00:25:28,110
from having a really robust,

562
00:25:28,111 --> 00:25:31,142
early response to the pandemic

563
00:25:31,142 --> 00:25:33,823
was the fact that we had lots of data,

564
00:25:34,965 --> 00:25:38,209
but companies didn't know how to use it.

565
00:25:38,210 --> 00:25:42,710
And especially companies
outside of a HIPAA context

566
00:25:44,320 --> 00:25:46,010
that governs health information

567
00:25:46,010 --> 00:25:48,470
when it's held by sort of
traditional institutions,

568
00:25:48,470 --> 00:25:51,210
hospitals, insurance
companies, and whatnot.

569
00:25:51,210 --> 00:25:54,270
But others which have a lot of health

570
00:25:54,270 --> 00:25:55,870
and health-related data

571
00:25:55,870 --> 00:25:59,330
felt that they weren't sure
what the guard rails were.

572
00:25:59,330 --> 00:26:02,699
They weren't sure what lines
they could cross or not cross.

573
00:26:02,700 --> 00:26:06,420
And as a result we did not really know

574
00:26:06,420 --> 00:26:09,220
like where are the communities
that are deeply impacted?

575
00:26:09,220 --> 00:26:11,115
How do we reach those individuals?

576
00:26:11,115 --> 00:26:15,540
How do we find out who needs help?

577
00:26:16,684 --> 00:26:21,600
Maybe wants to mask up or
wants to stay home, but can't

578
00:26:21,600 --> 00:26:23,264
because of their economic situation

579
00:26:23,265 --> 00:26:25,610
or a different situation.

580
00:26:25,610 --> 00:26:28,036
We were really hamstrung.

581
00:26:28,037 --> 00:26:30,072
So I do think having a law

582
00:26:30,072 --> 00:26:32,179
that will make clear for companies

583
00:26:32,180 --> 00:26:34,090
what these guard rails are,

584
00:26:34,090 --> 00:26:36,740
when they can appropriately use data,

585
00:26:36,740 --> 00:26:40,200
when they need to
de-identify, to aggregate,

586
00:26:40,200 --> 00:26:44,970
to really do what they can
to take personal identifiers

587
00:26:44,970 --> 00:26:47,154
but still get meaningful
information out of it,

588
00:26:47,154 --> 00:26:49,162
all of those kinds of guard rails

589
00:26:49,162 --> 00:26:53,363
I think would have helped us
have a much more agile response

590
00:26:53,363 --> 00:26:57,073
to the crisis.

591
00:26:59,010 --> 00:27:01,730
You're raising another
very important issue though

592
00:27:01,730 --> 00:27:03,715
that I think that we should talk about,

593
00:27:03,715 --> 00:27:08,715
which is the fact is that
what we saw over the past year

594
00:27:10,431 --> 00:27:13,210
is something that has
been true in our society

595
00:27:13,210 --> 00:27:14,910
for a long time,

596
00:27:14,910 --> 00:27:17,898
but this is something that
was deeply exacerbated.

597
00:27:17,898 --> 00:27:20,860
The pandemic laid bare

598
00:27:20,860 --> 00:27:25,860
that we need to address
systemic racial inequalities

599
00:27:25,971 --> 00:27:29,214
that have been in our
society for a very long time

600
00:27:29,214 --> 00:27:32,528
and have shaped our
nation for way too long.

601
00:27:32,528 --> 00:27:34,532
Data's gonna be critical,

602
00:27:34,532 --> 00:27:36,799
not just to solve health crises

603
00:27:36,799 --> 00:27:40,129
but also to understand how is it

604
00:27:40,130 --> 00:27:41,300
that certain communities,

605
00:27:41,300 --> 00:27:42,389
communities of color,

606
00:27:42,390 --> 00:27:46,109
communities of black and
African-American people,

607
00:27:46,109 --> 00:27:48,629
Asian, Pacific Islanders,

608
00:27:48,630 --> 00:27:53,330
whatever groups that we
need to be focusing on,

609
00:27:53,330 --> 00:27:54,687
and there are many of them,

610
00:27:54,688 --> 00:27:57,650
how to understand how
they are being impacted

611
00:27:57,650 --> 00:28:02,650
economically, socially,
religious reasons, et cetera.

612
00:28:02,846 --> 00:28:05,454
We need to be able to understand

613
00:28:05,454 --> 00:28:10,454
how this pandemic has disparate
impact on these communities

614
00:28:11,990 --> 00:28:13,653
and that's gonna take data.

615
00:28:13,653 --> 00:28:14,857
And so once again

616
00:28:14,857 --> 00:28:18,800
if we don't have clear rules of the road

617
00:28:18,800 --> 00:28:19,990
where people say, okay, yeah,

618
00:28:19,990 --> 00:28:21,632
I'm gonna use this data for good,

619
00:28:21,632 --> 00:28:24,950
and I want communities to understand

620
00:28:24,950 --> 00:28:27,304
that they're not gonna
be harmed all over again.

621
00:28:27,304 --> 00:28:31,586
The health care system has
not necessarily done well,

622
00:28:31,586 --> 00:28:33,660
that many people in these communities

623
00:28:33,660 --> 00:28:35,073
have not necessarily done well

624
00:28:35,073 --> 00:28:38,610
by the healthcare system historically

625
00:28:38,610 --> 00:28:39,848
over the past decades.

626
00:28:39,848 --> 00:28:42,970
So these communities
need to have confidence,

627
00:28:42,970 --> 00:28:45,320
people in these communities
need to have confidence

628
00:28:45,320 --> 00:28:46,720
that when their data is being used,

629
00:28:46,720 --> 00:28:47,778
it's being used for good

630
00:28:47,778 --> 00:28:52,600
and to help them come out of a pandemic

631
00:28:52,600 --> 00:28:56,050
to help them escape poverty,

632
00:28:56,050 --> 00:28:57,831
to help understand

633
00:28:57,831 --> 00:29:00,879
how we can bring connectivity
to their communities

634
00:29:00,879 --> 00:29:04,219
in ways that will allow them to learn

635
00:29:04,220 --> 00:29:05,930
and allow their kids to work from home.

636
00:29:05,930 --> 00:29:08,420
There's just so many issues
that we need to be focused on.

637
00:29:08,420 --> 00:29:10,790
Data is gonna be critically important

638
00:29:10,790 --> 00:29:12,482
to driving solutions.

639
00:29:12,482 --> 00:29:16,670
We need, when we're
thinking about a federal law

640
00:29:16,670 --> 00:29:20,903
to say if you're gonna
violate the civil rights laws,

641
00:29:21,780 --> 00:29:24,379
if you're gonna engage
in biased use of data,

642
00:29:24,380 --> 00:29:26,193
that that is also gonna be a problem

643
00:29:26,193 --> 00:29:29,639
with respect to this
comprehensive privacy law.

644
00:29:29,639 --> 00:29:32,689
We believe, I believe
and Microsoft believes

645
00:29:32,690 --> 00:29:37,670
that federal privacy law
ought to address civil rights.

646
00:29:37,670 --> 00:29:42,290
It ought to recognize that
data can be discriminatory

647
00:29:42,290 --> 00:29:45,600
so that we ensure that
companies, or it can be,

648
00:29:45,600 --> 00:29:48,843
excuse me, data can be used
in a discriminatory way.

649
00:29:48,844 --> 00:29:52,360
And if data's gonna be used
in a discriminatory way,

650
00:29:52,360 --> 00:29:55,040
we need to ensure that
companies understand

651
00:29:55,040 --> 00:29:57,040
that that's not okay.

652
00:29:57,040 --> 00:29:58,327
Those are the kinds of guard rails.

653
00:29:58,327 --> 00:30:00,748
How to robustly use data,

654
00:30:00,748 --> 00:30:02,036
what are the guard rails

655
00:30:02,036 --> 00:30:03,838
to make sure that you're not engaged

656
00:30:03,838 --> 00:30:06,129
in bias and discrimination

657
00:30:06,130 --> 00:30:07,970
so that we can actually use data

658
00:30:07,970 --> 00:30:09,453
to help some of these communities

659
00:30:09,453 --> 00:30:12,218
that have been so deeply
harmed for decades,

660
00:30:12,218 --> 00:30:15,670
but also especially over the past year.

661
00:30:15,670 --> 00:30:18,525
- Danielle, are you optimistic or worried

662
00:30:18,525 --> 00:30:22,531
when you hear me and others say

663
00:30:22,531 --> 00:30:24,659
we need that sensitive data,

664
00:30:24,660 --> 00:30:27,182
because we're gonna do good with it.

665
00:30:27,182 --> 00:30:29,340
We need it for research.

666
00:30:29,340 --> 00:30:34,203
We need it to find and
rid out discrimination.

667
00:30:35,688 --> 00:30:40,330
Are the civil society actors worried

668
00:30:40,330 --> 00:30:44,280
that this will lead to new abuses

669
00:30:44,280 --> 00:30:45,960
if there's law in place?

670
00:30:45,960 --> 00:30:47,540
Does that set the guard rails?

671
00:30:47,540 --> 00:30:51,240
How do you see sort of this call

672
00:30:51,240 --> 00:30:52,985
that's coming from I think
a lot of stakeholders,

673
00:30:52,985 --> 00:30:54,270
and it's not just companies,

674
00:30:54,270 --> 00:30:56,320
and researchers who are
saying, I need data.

675
00:30:56,320 --> 00:31:01,320
Advocates for the climate say we need data

676
00:31:01,870 --> 00:31:03,929
and what we need is more important

677
00:31:03,930 --> 00:31:07,535
than worrying about
that little privacy risk

678
00:31:07,535 --> 00:31:10,635
because we're solving this big problem.

679
00:31:10,635 --> 00:31:15,635
There's some skepticism
sometimes from civil society

680
00:31:15,753 --> 00:31:18,159
that the balance will work out well.

681
00:31:18,160 --> 00:31:19,433
What's your reaction?

682
00:31:20,860 --> 00:31:23,030
- So I think the important
question always should be

683
00:31:23,030 --> 00:31:25,649
what's the problem we're trying to solve.

684
00:31:25,650 --> 00:31:29,500
And we often see AI or
automated sort of programs

685
00:31:29,500 --> 00:31:31,170
used in ways that it's,

686
00:31:31,170 --> 00:31:32,676
we're not trying to solve a problem.

687
00:31:32,676 --> 00:31:34,780
It seems like snake oil.

688
00:31:34,780 --> 00:31:36,610
There isn't really proof of concept,

689
00:31:36,610 --> 00:31:37,969
whether it's higher view

690
00:31:37,970 --> 00:31:42,970
uses these sort of
employment, hiring programs

691
00:31:43,077 --> 00:31:45,880
that claim to be able to use data

692
00:31:45,880 --> 00:31:48,610
to make important suggestions

693
00:31:48,610 --> 00:31:50,837
that are free from bias, absurd.

694
00:31:50,837 --> 00:31:52,120
There are all these ways

695
00:31:52,120 --> 00:31:56,840
which I think the argument
for data is a strong one,

696
00:31:56,840 --> 00:31:59,000
particularly in a health context.

697
00:31:59,000 --> 00:32:00,319
Here are the ways in which we think,

698
00:32:00,319 --> 00:32:02,750
gosh, that trade-off is worth it

699
00:32:02,750 --> 00:32:06,139
so long as, as Julie underscores,
we have the guard rails

700
00:32:06,139 --> 00:32:07,439
and protections.

701
00:32:07,440 --> 00:32:09,337
But there are other times in which we are,

702
00:32:09,337 --> 00:32:11,570
we just have a data collection imperative

703
00:32:11,570 --> 00:32:14,899
so we collect everything
and then throw AI at it

704
00:32:14,900 --> 00:32:17,230
as if we're solving something important.

705
00:32:17,230 --> 00:32:18,660
And it's almost absurd.

706
00:32:18,660 --> 00:32:22,805
So I think we've got to figure
out what the problems are,

707
00:32:22,805 --> 00:32:27,805
see if AI and different
algorithmic systems can weigh in

708
00:32:29,190 --> 00:32:31,920
and in an effective way
have a proof of concept

709
00:32:31,920 --> 00:32:33,708
and then set up the guard rails.

710
00:32:33,708 --> 00:32:35,455
But right now it seems like

711
00:32:35,455 --> 00:32:39,120
every startup and their
grandfather, a brother

712
00:32:39,120 --> 00:32:41,032
is like the using AI for stupid reasons.

713
00:32:41,032 --> 00:32:43,499
And guess what, the risks are potent

714
00:32:43,499 --> 00:32:46,616
and individuals will never
know the jobs they don't get,

715
00:32:46,616 --> 00:32:48,769
the ads they do or don't see,

716
00:32:48,769 --> 00:32:53,760
the apartments that they're denied or not.

717
00:32:53,760 --> 00:32:56,980
So individuals can
remotely protect themselves

718
00:32:56,980 --> 00:32:59,062
from these sort of structural problems.

719
00:32:59,062 --> 00:33:02,443
So I'm with you, if it's
a good problem Jules,

720
00:33:03,465 --> 00:33:04,669
that we have to solve.
- You said the word had,

721
00:33:04,670 --> 00:33:07,160
so let's close with what has been

722
00:33:07,160 --> 00:33:09,730
perhaps the challenge of having

723
00:33:09,730 --> 00:33:13,010
serious data protection
conversations for 20 some odd years

724
00:33:13,010 --> 00:33:14,657
since my early days at DoubleClick.

725
00:33:14,657 --> 00:33:17,842
We are worried about data

726
00:33:17,843 --> 00:33:20,660
because it might be used
for behavioral advertising

727
00:33:20,660 --> 00:33:21,940
that could part get me

728
00:33:21,940 --> 00:33:24,010
or make me feel like I'm being surveilled

729
00:33:24,010 --> 00:33:25,680
or do things that are discriminatory

730
00:33:25,680 --> 00:33:29,232
or give me only the low pay jobs.

731
00:33:29,232 --> 00:33:33,330
Concerns about and angst
about the privacy and security

732
00:33:33,330 --> 00:33:36,199
of the current ad tech ecosystem

733
00:33:36,200 --> 00:33:40,497
have sort of been the behind
the scenes legitimate issue

734
00:33:42,071 --> 00:33:44,665
but it's been sometimes
hard to have issues

735
00:33:44,665 --> 00:33:46,338
about can we fix healthcare?

736
00:33:46,338 --> 00:33:48,949
Well, the data might be used for...

737
00:33:48,949 --> 00:33:50,630
And now there's changing.

738
00:33:50,630 --> 00:33:51,650
There's change happening.

739
00:33:51,650 --> 00:33:53,400
Microsoft's Internet Explorer

740
00:33:53,400 --> 00:33:55,600
has been blocking third
party cookies for awhile.

741
00:33:55,600 --> 00:33:56,792
Now Google is gonna do it.

742
00:33:56,792 --> 00:33:58,467
Safari has been doing it.

743
00:33:58,468 --> 00:34:03,468
Apple is changing the way
tracking can work on iOS devices.

744
00:34:05,162 --> 00:34:07,840
At the same time, ad tech provides revenue

745
00:34:07,840 --> 00:34:10,259
for lots of publishers,
advertisers compete,

746
00:34:10,260 --> 00:34:11,810
that's how they find customers.

747
00:34:12,850 --> 00:34:16,259
Are the platforms and the new laws

748
00:34:16,260 --> 00:34:19,105
that give people up doubts of sale,

749
00:34:19,105 --> 00:34:22,050
are we radically changing
the ad tech ecosystem

750
00:34:22,050 --> 00:34:24,363
and is it gonna be for the better?

751
00:34:25,960 --> 00:34:27,590
Julie, I'll let you both take that

752
00:34:27,590 --> 00:34:28,490
and we'll wrap up.

753
00:34:30,969 --> 00:34:35,592
- I think that we have the
opportunity to invent our future.

754
00:34:37,025 --> 00:34:42,025
There are concerns about
online behavioral advertising

755
00:34:42,489 --> 00:34:43,799
that looks at what people do

756
00:34:43,800 --> 00:34:48,159
as they are going from website to website

757
00:34:48,159 --> 00:34:50,210
and surfacing ads to them.

758
00:34:50,210 --> 00:34:51,534
Some people think it's creepy,

759
00:34:51,534 --> 00:34:54,507
other people think that
it's just inappropriate.

760
00:34:54,507 --> 00:34:58,270
There are conversations in
the United States about it

761
00:34:58,270 --> 00:34:59,746
and there are conversations in Europe

762
00:34:59,746 --> 00:35:01,773
that are actually quite different.

763
00:35:03,137 --> 00:35:05,060
It's often, if you sit down

764
00:35:05,060 --> 00:35:07,580
with a privacy regulator in Europe,

765
00:35:07,580 --> 00:35:09,227
a data protection authority,

766
00:35:09,227 --> 00:35:12,259
they will say like why do we need it?

767
00:35:12,260 --> 00:35:15,650
What is the purpose of advertising?

768
00:35:15,650 --> 00:35:20,360
Why can't I just look
at an ad that's random?

769
00:35:20,360 --> 00:35:23,470
And I think what's
important is to understand

770
00:35:23,470 --> 00:35:27,236
that actually what advertising does,

771
00:35:27,236 --> 00:35:29,370
not behavioral necessarily,

772
00:35:29,370 --> 00:35:34,057
but what advertising does is
it allows companies, startups,

773
00:35:34,057 --> 00:35:38,229
mom and pop whatevers to find customers.

774
00:35:38,230 --> 00:35:42,259
And we want a thriving economy.

775
00:35:42,259 --> 00:35:47,259
We don't want just like the
really big guys and gals

776
00:35:47,530 --> 00:35:48,800
who everybody knows

777
00:35:48,800 --> 00:35:51,390
to be the ones who succeed in our economy.

778
00:35:51,390 --> 00:35:53,901
We want small players
to be able to compete.

779
00:35:53,901 --> 00:35:57,723
And so I think the key to
this future of advertising

780
00:35:57,723 --> 00:36:02,723
is figuring out how do
we invent our future

781
00:36:03,000 --> 00:36:06,011
so that small, medium players

782
00:36:06,011 --> 00:36:09,661
can participate in an
advertising ecosystem

783
00:36:09,661 --> 00:36:14,661
so that journalism and
publishers will thrive

784
00:36:15,616 --> 00:36:19,170
because we've learned over
the past several years

785
00:36:19,170 --> 00:36:23,420
how important journalism
is to our democracy.

786
00:36:23,420 --> 00:36:25,327
These are the kinds of elements,

787
00:36:25,327 --> 00:36:29,673
all of which are part of an
advertising pecker system.

788
00:36:30,698 --> 00:36:34,129
Newspapers, that's how
they make their money.

789
00:36:34,130 --> 00:36:36,086
Publishers, that's how
they make their money.

790
00:36:36,086 --> 00:36:39,892
So I think that we have
the opportunity to invent

791
00:36:39,893 --> 00:36:44,310
a respectful future for advertising

792
00:36:44,310 --> 00:36:49,310
where we will have robust
competition, thriving journalism,

793
00:36:49,840 --> 00:36:53,640
and people won't feel creeped out.

794
00:36:53,640 --> 00:36:55,498
They will be able to make choices.

795
00:36:55,498 --> 00:36:57,320
It's not impossible.

796
00:36:57,320 --> 00:36:59,850
We do have that future that we can invent,

797
00:36:59,850 --> 00:37:01,071
but we have to be committed

798
00:37:01,071 --> 00:37:05,450
and understand that there
are a lot of different pieces

799
00:37:05,450 --> 00:37:06,826
that are part of this puzzle,

800
00:37:06,826 --> 00:37:10,500
and we want to try to create a system

801
00:37:10,500 --> 00:37:14,563
where everyone can thrive,
not just the really big guys.

802
00:37:15,640 --> 00:37:19,120
And we wanna make sure that
journalism and small players

803
00:37:19,120 --> 00:37:20,807
are really doing well too.

804
00:37:20,807 --> 00:37:24,200
- Danielle, a very
brief final word to you.

805
00:37:24,200 --> 00:37:25,272
- So I think,

806
00:37:26,173 --> 00:37:28,732
God how can you disagree
with Julie on this one here?

807
00:37:28,733 --> 00:37:31,105
A measured approach I think is important.

808
00:37:31,105 --> 00:37:36,105
And I don't wanna see all
the great things go away too.

809
00:37:36,210 --> 00:37:40,020
But I think the fact that we see churn

810
00:37:40,020 --> 00:37:41,620
that involves data brokers,

811
00:37:41,620 --> 00:37:44,299
there's a way in which I
think limits need to be set.

812
00:37:44,300 --> 00:37:45,431
So I'm agreeing with Julie,

813
00:37:45,431 --> 00:37:49,463
and also echoing her insight
about have there be guardrails.

814
00:37:50,550 --> 00:37:53,200
- Thanks to you both for a
really great conversation.

815
00:37:54,930 --> 00:37:55,763
- Thank you.

816
00:37:55,763 --> 00:37:56,596
- Thank you.

817
00:37:56,596 --> 00:37:57,500
Thanks for having us.

818
00:37:57,500 --> 00:38:00,190
And it was wonderful to
be here with both of you.

819
00:38:00,190 --> 00:38:02,150
- Same, it was wonderful.

