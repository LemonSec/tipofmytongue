1
00:00:01,120 --> 00:00:02,070
- Hey, everyone!

2
00:00:02,070 --> 00:00:04,180
Who's ready for some multicloud monitoring

3
00:00:04,180 --> 00:00:06,000
and digital forensics?

4
00:00:06,000 --> 00:00:08,640
Yeah, I know that sounds
pretty complicated,

5
00:00:08,640 --> 00:00:10,750
but I promise it's really not nearly

6
00:00:10,750 --> 00:00:12,620
as complicated as it sounds.

7
00:00:12,620 --> 00:00:15,660
In 2020, IBM and the
Panama Institute found

8
00:00:15,660 --> 00:00:20,463
that the average cost of a
data breach was $3.86 million.

9
00:00:21,350 --> 00:00:22,980
As terrible as that sounds,

10
00:00:22,980 --> 00:00:25,420
it actually gets worse
when you think about it,

11
00:00:25,420 --> 00:00:27,330
that it is average number,

12
00:00:27,330 --> 00:00:30,250
meaning that there are
many smaller scale breaches

13
00:00:30,250 --> 00:00:32,009
that drove this number down.

14
00:00:32,009 --> 00:00:34,500
If you work for a larger organization,

15
00:00:34,500 --> 00:00:37,580
this could easily cost you nine figures.

16
00:00:37,580 --> 00:00:39,670
So how are we going to
save your organization

17
00:00:39,670 --> 00:00:42,130
from the fate of a massive breach?

18
00:00:42,130 --> 00:00:44,710
Welcome to Multi-cloud Anomaly Detection:

19
00:00:44,710 --> 00:00:47,600
finding threats among us
in the big three clouds.

20
00:00:47,600 --> 00:00:49,130
My name is Brandon Evans,

21
00:00:49,130 --> 00:00:52,000
I am a certified instructor
for the SANS Institute.

22
00:00:52,000 --> 00:00:56,076
I am the lead author of SANS
SEC510, Public Cloud Security,

23
00:00:56,076 --> 00:00:58,423
AWS, Azure, and GCP.

24
00:00:58,423 --> 00:00:59,570
A course that explores

25
00:00:59,570 --> 00:01:01,870
many of the topics we will discuss today

26
00:01:01,870 --> 00:01:05,149
and which has a brand
new GEC certification

27
00:01:05,150 --> 00:01:09,700
called a GEC public
cloud security or GPCS.

28
00:01:09,700 --> 00:01:13,160
Like most SANS instructors,
SANS is not my day job.

29
00:01:13,160 --> 00:01:16,770
Most days I am a practitioner
of security in the field.

30
00:01:16,770 --> 00:01:20,190
I am proud to have recently
joined Zoom Video Communications

31
00:01:20,190 --> 00:01:22,050
where I lead the internal application

32
00:01:22,050 --> 00:01:23,560
security training program

33
00:01:23,560 --> 00:01:25,600
for one of the most
consequential companies

34
00:01:25,600 --> 00:01:27,229
in the new world we live in.

35
00:01:27,230 --> 00:01:30,130
I hope that this presentation
will deliver happiness to you

36
00:01:30,130 --> 00:01:32,899
by equipping you with the
tools and skills you need

37
00:01:32,900 --> 00:01:35,200
in order to protect
your organization's data

38
00:01:35,200 --> 00:01:36,870
and assets in the cloud.

39
00:01:36,870 --> 00:01:39,610
So if we know that there
are countless threats

40
00:01:39,610 --> 00:01:43,420
in our cloud virtual networks,
why are they so hard to find?

41
00:01:43,420 --> 00:01:47,130
Unfortunately, there are
many imposters among us.

42
00:01:47,130 --> 00:01:50,250
The traffic and logs they
generate look identical

43
00:01:50,250 --> 00:01:51,840
to that of normal users.

44
00:01:51,840 --> 00:01:54,950
But behind their facade, they
are pilfering your resources

45
00:01:54,950 --> 00:01:57,020
and exfiltrating your data.

46
00:01:57,020 --> 00:01:59,700
The less sophisticated
attacker of yesteryear

47
00:01:59,700 --> 00:02:00,900
would transfer your data

48
00:02:00,900 --> 00:02:04,530
over relatively conspicuous
channels, such as FTP.

49
00:02:04,530 --> 00:02:08,090
Today they can use seemingly
innocuous protocols

50
00:02:08,090 --> 00:02:12,170
that aren't designed for mass
data transfer, such as DNS.

51
00:02:12,170 --> 00:02:16,000
Worse yet, if the attacker can
export your cloud credentials

52
00:02:16,000 --> 00:02:19,500
they can pull your data from
outside your private network

53
00:02:19,500 --> 00:02:23,600
by making valid requests
to your cloud's APIs.

54
00:02:23,600 --> 00:02:26,710
Is it possible to find a
few malicious communications

55
00:02:26,710 --> 00:02:28,980
in an ocean of legitimate ones?

56
00:02:28,980 --> 00:02:32,640
The answer is yes, but
it's also very difficult.

57
00:02:32,640 --> 00:02:35,179
So we throw millions of dollars

58
00:02:35,180 --> 00:02:38,410
at intrusion detection and
data loss protection tools.

59
00:02:38,410 --> 00:02:39,870
Now don't get me wrong,

60
00:02:39,870 --> 00:02:43,220
the cost of getting breached
far outweighs the cost

61
00:02:43,220 --> 00:02:44,900
of procuring these tools.

62
00:02:44,900 --> 00:02:46,810
But what if there was a third option?

63
00:02:46,810 --> 00:02:48,090
What if I told you

64
00:02:48,090 --> 00:02:51,150
that you could detect
intrusions and prevent data loss

65
00:02:51,150 --> 00:02:53,870
with your cloud providers
and native services?

66
00:02:53,870 --> 00:02:54,703
Think about it.

67
00:02:54,703 --> 00:02:57,840
The cloud generates so much data,

68
00:02:57,840 --> 00:03:01,450
nearly every action taken
in the cloud can be logged.

69
00:03:01,450 --> 00:03:04,089
If we can detect anomalies in these logs,

70
00:03:04,090 --> 00:03:07,190
we can identify indicators of compromise.

71
00:03:07,190 --> 00:03:08,780
The big three cloud providers

72
00:03:08,780 --> 00:03:11,500
give us all the tools we need to do so.

73
00:03:11,500 --> 00:03:14,990
Unfortunately, if organizations
fail to use these tools

74
00:03:14,990 --> 00:03:18,630
at their disposal, they
are unfortunately useless.

75
00:03:18,630 --> 00:03:19,990
If you aren't using these tools

76
00:03:19,990 --> 00:03:21,810
or don't know what I'm talking about,

77
00:03:21,810 --> 00:03:23,360
you're in the right place.

78
00:03:23,360 --> 00:03:24,960
This session will teach defenders

79
00:03:24,960 --> 00:03:28,490
how to unlock the full
potential of the audit logs

80
00:03:28,490 --> 00:03:30,980
generated by the big
three cloud providers,

81
00:03:30,980 --> 00:03:33,619
Amazon Web Services, Microsoft Azure,

82
00:03:33,620 --> 00:03:35,190
and the Google Cloud Platform.

83
00:03:35,190 --> 00:03:37,890
We will focus on two types of audit logs,

84
00:03:37,890 --> 00:03:41,549
cloud API activity logs
and network flow logs.

85
00:03:41,550 --> 00:03:43,760
Using a real cloud environment,

86
00:03:43,760 --> 00:03:45,290
we will demonstrate how to detect

87
00:03:45,290 --> 00:03:47,269
two types of attack signatures

88
00:03:47,270 --> 00:03:49,780
by looking for anomalies in these logs.

89
00:03:49,780 --> 00:03:53,930
While manual log analysis is
helpful, it is also expensive.

90
00:03:53,930 --> 00:03:55,360
So we will also discuss

91
00:03:55,360 --> 00:03:57,780
how to generate alerts from these logs.

92
00:03:57,780 --> 00:04:01,040
These alerts can tip off
our incident response teams

93
00:04:01,040 --> 00:04:03,760
of potential indicators of compromise.

94
00:04:03,760 --> 00:04:05,790
Finally, while each cloud provider

95
00:04:05,790 --> 00:04:09,530
has relatively similar
logging services and concepts,

96
00:04:09,530 --> 00:04:13,160
there are some key differences
that you must be aware of

97
00:04:13,160 --> 00:04:15,180
in order to use them properly.

98
00:04:15,180 --> 00:04:17,953
We will explore these
differences in depth.

99
00:04:19,000 --> 00:04:22,113
So let's start by talking about
cloud network flow logging.

100
00:04:23,010 --> 00:04:24,390
What is flow logging?

101
00:04:24,390 --> 00:04:27,620
It is a powerful tool
for monitoring a network.

102
00:04:27,620 --> 00:04:29,980
flow logging captures metadata

103
00:04:29,980 --> 00:04:33,540
about the traffic flowing
to and from a network.

104
00:04:33,540 --> 00:04:35,540
We specify that this is metadata

105
00:04:35,540 --> 00:04:39,700
because flow logging does not
actually capture the contents

106
00:04:39,700 --> 00:04:41,060
of the network traffic,

107
00:04:41,060 --> 00:04:42,530
which can be highly sensitive,

108
00:04:42,530 --> 00:04:46,010
like credit card information
or social security numbers.

109
00:04:46,010 --> 00:04:48,230
Instead, as we will see shortly,

110
00:04:48,230 --> 00:04:51,440
flow logs just tell us where
traffic is coming from,

111
00:04:51,440 --> 00:04:52,620
where it is going,

112
00:04:52,620 --> 00:04:54,550
how much of the data is being transferred,

113
00:04:54,550 --> 00:04:56,880
and a couple other pieces of data.

114
00:04:56,880 --> 00:04:59,469
This alone can be very useful.

115
00:04:59,470 --> 00:05:01,510
If a particular set of IP addresses

116
00:05:01,510 --> 00:05:03,650
are sending anomalous amounts of data

117
00:05:03,650 --> 00:05:06,070
or are probing unexpected ports,

118
00:05:06,070 --> 00:05:08,440
this is a good sign
that these IP addresses

119
00:05:08,440 --> 00:05:11,410
might be controlled by threat actors.

120
00:05:11,410 --> 00:05:13,820
If we see a large amount
of data being sent

121
00:05:13,820 --> 00:05:16,260
from our network to a particular host,

122
00:05:16,260 --> 00:05:19,240
this might indicate that our
data is being exfiltrated.

123
00:05:19,240 --> 00:05:22,970
We can glean a lot of
information from this data.

124
00:05:22,970 --> 00:05:26,320
Now, flow logging is a
common practice on premises.

125
00:05:26,320 --> 00:05:27,719
So it shouldn't be surprising

126
00:05:27,720 --> 00:05:30,630
that each of the cloud
providers support it as well.

127
00:05:30,630 --> 00:05:32,070
Here's the catch.

128
00:05:32,070 --> 00:05:34,540
None of the big three cloud providers

129
00:05:34,540 --> 00:05:37,610
have flow logging enabled by default.

130
00:05:37,610 --> 00:05:40,700
The customer must explicitly opt in

131
00:05:40,700 --> 00:05:43,090
and define a log retention policy.

132
00:05:43,090 --> 00:05:45,789
So if your organization
hasn't done this already,

133
00:05:45,790 --> 00:05:49,710
make sure you enable
flow logging immediately.

134
00:05:49,710 --> 00:05:51,840
Once flow logging is enabled,

135
00:05:51,840 --> 00:05:54,799
we can persist, view and analyze the logs

136
00:05:54,800 --> 00:05:57,140
using a variety of cloud services.

137
00:05:57,140 --> 00:05:59,150
We will discuss each related service

138
00:05:59,150 --> 00:06:02,280
for each of the cloud providers in scope.

139
00:06:02,280 --> 00:06:05,159
Thankfully, our friends at the
Center for Internet Security

140
00:06:05,160 --> 00:06:07,830
provide guidance on this exact topic.

141
00:06:07,830 --> 00:06:09,150
For each cloud provider,

142
00:06:09,150 --> 00:06:11,299
they detail how to properly use

143
00:06:11,300 --> 00:06:13,540
their cloud flow logging services.

144
00:06:13,540 --> 00:06:16,990
For AWS, we must ensure
that flow logging is enabled

145
00:06:16,990 --> 00:06:20,360
for each virtual private cloud or VPC.

146
00:06:20,360 --> 00:06:22,230
In their view, it is sufficient

147
00:06:22,230 --> 00:06:25,870
to only capture metadata about
packets that are rejected

148
00:06:25,870 --> 00:06:29,100
by your security groups and
network access control lists.

149
00:06:29,100 --> 00:06:30,400
For our purposes,

150
00:06:30,400 --> 00:06:34,219
it is hard to understand what
anomalous traffic looks like

151
00:06:34,220 --> 00:06:36,350
without establishing a baseline.

152
00:06:36,350 --> 00:06:39,860
Therefore, we recommend that
you capture all flow logs

153
00:06:39,860 --> 00:06:41,300
for all packets.

154
00:06:41,300 --> 00:06:44,290
Now, regardless, we have to define

155
00:06:44,290 --> 00:06:47,130
how long we want to retain these logs.

156
00:06:47,130 --> 00:06:51,620
Sadly, on average, it takes 210 days

157
00:06:51,620 --> 00:06:54,860
for an organization to discover
that it has been breached.

158
00:06:54,860 --> 00:06:57,290
Therefore, the CIS suggests

159
00:06:57,290 --> 00:06:59,370
that organizations retain their flow logs

160
00:06:59,370 --> 00:07:01,540
for at least one year.

161
00:07:01,540 --> 00:07:05,000
But as you can imagine,
this can get very expensive.

162
00:07:05,000 --> 00:07:07,540
Amazon CloudWatch, one
of the best services

163
00:07:07,540 --> 00:07:10,030
for analyzing logs in AWS

164
00:07:10,030 --> 00:07:13,760
is also the service with
the highest storage cost.

165
00:07:13,760 --> 00:07:16,760
Luckily, you don't necessarily
need to store your logs

166
00:07:16,760 --> 00:07:18,690
in CloudWatch for one year.

167
00:07:18,690 --> 00:07:21,150
Instead, you can retain
your logs in CloudWatch

168
00:07:21,150 --> 00:07:23,020
for a smaller period of time

169
00:07:23,020 --> 00:07:26,440
before archiving them into
cheaper storage services

170
00:07:26,440 --> 00:07:28,023
like S3 and Glacier.

171
00:07:29,230 --> 00:07:32,940
In AWS, the services used
to manage internal traffic,

172
00:07:32,940 --> 00:07:37,090
aggregate flow logs and
analyze logs are all different.

173
00:07:37,090 --> 00:07:40,179
This slide shows how these
services work together.

174
00:07:40,180 --> 00:07:41,920
As mentioned on the previous slide,

175
00:07:41,920 --> 00:07:45,360
we must enable flow
logging on individual VPCs.

176
00:07:45,360 --> 00:07:48,040
Alternatively, we could
enable them on subnets

177
00:07:48,040 --> 00:07:49,370
within the VPC

178
00:07:49,370 --> 00:07:53,850
or in an individual elastic
network interface or ENI.

179
00:07:53,850 --> 00:07:55,250
To analyze the logs,

180
00:07:55,250 --> 00:07:57,860
we will need to use
Amazon CloudWatch logs,

181
00:07:57,860 --> 00:08:00,030
which provides a general purpose mechanism

182
00:08:00,030 --> 00:08:01,380
for querying logs.

183
00:08:01,380 --> 00:08:03,340
We will use CloudWatch extensively

184
00:08:03,340 --> 00:08:05,010
throughout this presentation.

185
00:08:05,010 --> 00:08:07,662
And there is a delay between the traffic

186
00:08:07,662 --> 00:08:10,530
before the traffic is aggregated.

187
00:08:10,530 --> 00:08:13,030
There is a delay between
when traffic is received.

188
00:08:13,030 --> 00:08:16,599
By default flow logs are
aggregated on a 10-minute interval

189
00:08:16,600 --> 00:08:19,930
and sent to CloudWatch on
a five-minute interval.

190
00:08:19,930 --> 00:08:23,930
You can reduce these intervals
to one and five respectively.

191
00:08:23,930 --> 00:08:25,350
With that configuration,

192
00:08:25,350 --> 00:08:27,720
you can confidently rely on your logs

193
00:08:27,720 --> 00:08:30,280
showing up between one and six minutes

194
00:08:30,280 --> 00:08:34,102
after the traffic has arrived
to your network interface.

195
00:08:35,059 --> 00:08:39,260
Let's talk about a sample
flow log captured in a VPC

196
00:08:39,260 --> 00:08:41,850
and query through CloudWatch logs.

197
00:08:41,850 --> 00:08:43,960
There's a lot of metadata
that we can see here,

198
00:08:43,960 --> 00:08:47,720
but remember the actual
data is not persisted.

199
00:08:47,720 --> 00:08:51,130
Instead, we can just learn where
the traffic is coming from,

200
00:08:51,130 --> 00:08:53,570
where it's going, how
is it being delivered,

201
00:08:53,570 --> 00:08:55,030
how large the message is,

202
00:08:55,030 --> 00:08:56,952
and whether or not the data was accepted

203
00:08:56,952 --> 00:09:00,393
by our network rules or rejected.

204
00:09:01,720 --> 00:09:05,770
We can query and visualize
these flow logs in a few ways.

205
00:09:05,770 --> 00:09:07,100
For automation purposes,

206
00:09:07,100 --> 00:09:09,250
it is important to know how to query them

207
00:09:09,250 --> 00:09:11,380
from the command line interface.

208
00:09:11,380 --> 00:09:13,200
You can see the syntax for doing this

209
00:09:13,200 --> 00:09:15,760
on the left hand side of the slide.

210
00:09:15,760 --> 00:09:18,370
It is a two-part process.

211
00:09:18,370 --> 00:09:20,410
First, you initialize the query

212
00:09:20,410 --> 00:09:22,160
and then you get the results.

213
00:09:22,160 --> 00:09:24,339
AWS splits these steps up

214
00:09:24,340 --> 00:09:27,200
because these queries can
take a long time to run,

215
00:09:27,200 --> 00:09:29,810
and you might want to see
the partial set of results

216
00:09:29,810 --> 00:09:32,150
before the query is completed.

217
00:09:32,150 --> 00:09:35,012
Now for the folks who are
more visually inclined,

218
00:09:35,012 --> 00:09:37,869
AWS also provides a
graphical user interface

219
00:09:37,870 --> 00:09:40,720
for running these queries
in the AWS console.

220
00:09:40,720 --> 00:09:41,813
An additional perk of this method is

221
00:09:41,813 --> 00:09:45,000
that you can visualize
the traffic patterns.

222
00:09:45,000 --> 00:09:47,810
In this example, we are
looking for rejected traffic

223
00:09:47,810 --> 00:09:49,420
from one of our virtual machines.

224
00:09:49,420 --> 00:09:51,640
An analyst can view the time-lapse bar

225
00:09:51,640 --> 00:09:54,120
and find periods when the rejected traffic

226
00:09:54,120 --> 00:09:56,603
spikes or dipped from the baseline.

227
00:09:58,060 --> 00:10:00,300
The CIS provides very similar guidelines

228
00:10:00,300 --> 00:10:02,099
for flow logging in Azure.

229
00:10:02,100 --> 00:10:05,110
In Azure, the network resources
are logically isolated

230
00:10:05,110 --> 00:10:07,570
into virtual networks or VNets

231
00:10:07,570 --> 00:10:10,490
instead of VPCs, but
it's the same concept.

232
00:10:10,490 --> 00:10:12,170
Traffic within a VNet is controlled

233
00:10:12,170 --> 00:10:14,870
by network security groups or NSGs,

234
00:10:14,870 --> 00:10:18,510
we enable flow logs at
the NSG level in Azure.

235
00:10:18,510 --> 00:10:20,540
Azure flow logs, when enabled,

236
00:10:20,540 --> 00:10:23,939
always captures both accepted
and rejected packets.

237
00:10:23,940 --> 00:10:25,060
So we don't have to worry about

238
00:10:25,060 --> 00:10:28,729
that particular configuration
like we did for Amazon.

239
00:10:28,730 --> 00:10:31,230
Still, we need to ensure
that we persist the logs

240
00:10:31,230 --> 00:10:33,400
for a sufficient amount of time.

241
00:10:33,400 --> 00:10:37,800
Oddly enough, the CIS differs
from the guidance for AWS

242
00:10:37,800 --> 00:10:41,500
by recommending a retention
period of at least 90 days.

243
00:10:41,500 --> 00:10:43,780
For consistency, we are going to recommend

244
00:10:43,780 --> 00:10:46,329
that you persist them
all for at least one year

245
00:10:46,330 --> 00:10:48,307
just like we said for AWS.

246
00:10:49,920 --> 00:10:52,900
Like with AWS, Azure
requires multiple services

247
00:10:52,900 --> 00:10:54,390
to use flow logs.

248
00:10:54,390 --> 00:10:57,100
There are a lot of moving parts here.

249
00:10:57,100 --> 00:11:00,490
First, you need a VNet with an NSG.

250
00:11:00,490 --> 00:11:03,530
Then you need to enable
the network watcher service

251
00:11:03,530 --> 00:11:06,040
in your VNet to start sending flow logs

252
00:11:06,040 --> 00:11:08,010
to an Azure Storage account.

253
00:11:08,010 --> 00:11:10,410
For those who aren't
familiar, Azure Storage

254
00:11:10,410 --> 00:11:13,890
is Azure's general purpose
file storage service

255
00:11:13,890 --> 00:11:16,220
like Amazon S3.

256
00:11:16,220 --> 00:11:19,020
Azure uses Azure Storage behind the scenes

257
00:11:19,020 --> 00:11:21,590
all across their service catalog.

258
00:11:21,590 --> 00:11:24,160
Now, technically you
could read the flow logs

259
00:11:24,160 --> 00:11:28,160
directly from Azure
Storage, but it's not query.

260
00:11:28,160 --> 00:11:30,230
It's not pretty. You can't query it.

261
00:11:30,230 --> 00:11:32,910
It is much easier to analyze them

262
00:11:32,910 --> 00:11:36,339
using an Azure log analytics workspace.

263
00:11:36,340 --> 00:11:39,430
This interface exposes
the logs to be queried

264
00:11:39,430 --> 00:11:43,310
through the Kusto Query Language or KQL.

265
00:11:43,310 --> 00:11:44,699
Like with AWS,

266
00:11:44,700 --> 00:11:48,180
Azure flow logs are not
available for query immediately.

267
00:11:48,180 --> 00:11:51,140
You can aggregate them to
your log analytics workspace,

268
00:11:51,140 --> 00:11:54,430
either every 10 minutes or 60 minutes.

269
00:11:54,430 --> 00:11:56,630
So when compared to the other providers,

270
00:11:56,630 --> 00:11:59,293
these are the slowest aggregation options.

271
00:12:00,740 --> 00:12:02,180
Now for the longest time,

272
00:12:02,180 --> 00:12:04,839
I thought that log analytics
could only be queried

273
00:12:04,840 --> 00:12:06,550
via the Azure portal.

274
00:12:06,550 --> 00:12:09,500
Eventually I figured out that
you could install an extension

275
00:12:09,500 --> 00:12:11,750
for the Azure CLI.

276
00:12:11,750 --> 00:12:13,800
If you want a visualization though,

277
00:12:13,800 --> 00:12:15,870
the portal is the way to go.

278
00:12:15,870 --> 00:12:18,380
The first time I queried
Azure's flow logs,

279
00:12:18,380 --> 00:12:20,600
I assumed that I did something wrong.

280
00:12:20,600 --> 00:12:24,770
I was getting far less results
than I was getting in AWS.

281
00:12:24,770 --> 00:12:28,829
And as Ben Allen, another course
author at SANS often says,

282
00:12:28,830 --> 00:12:32,487
when it comes to flow logs,
quote "quiet is actionable.

283
00:12:32,487 --> 00:12:34,877
"Either you've lost internet connectivity,

284
00:12:34,877 --> 00:12:37,180
"or your monitoring tools have failed."

285
00:12:37,180 --> 00:12:38,219
End of quote.

286
00:12:38,220 --> 00:12:40,830
There's no way that my
VM was only receiving

287
00:12:40,830 --> 00:12:43,460
a few messages every hour.

288
00:12:43,460 --> 00:12:44,750
I eventually realized

289
00:12:44,750 --> 00:12:48,300
that Azure's flow log records
aggregate the traffic.

290
00:12:48,300 --> 00:12:51,260
A single record can
represent tens or hundreds

291
00:12:51,260 --> 00:12:52,910
of similar messages.

292
00:12:52,910 --> 00:12:54,850
So make sure to examine each record

293
00:12:54,850 --> 00:12:56,910
to determine the flow count.

294
00:12:56,910 --> 00:13:00,100
In this example, we are graphing
the flow count associated

295
00:13:00,100 --> 00:13:01,683
with the query over time.

296
00:13:02,910 --> 00:13:06,645
Here's an example of how we
can use Azure log analytic's

297
00:13:06,645 --> 00:13:09,280
CLI extension to query flow logs.

298
00:13:09,280 --> 00:13:12,276
The data here is very similar
to what we have in AWS.

299
00:13:13,790 --> 00:13:15,760
Finally, onto GCP.

300
00:13:15,760 --> 00:13:19,640
Like AWS, they call their
private networks VPCs.

301
00:13:19,640 --> 00:13:21,319
We must enable flow logging

302
00:13:21,320 --> 00:13:24,330
at the sub network or sub-net level.

303
00:13:24,330 --> 00:13:26,900
Now, one thing to be aware of with GCP is

304
00:13:26,900 --> 00:13:28,530
that they have a sampling rate.

305
00:13:28,530 --> 00:13:31,079
You can sample a
percentage of the flow logs

306
00:13:31,080 --> 00:13:34,130
instead of all of the flow
logs from a certain type.

307
00:13:34,130 --> 00:13:37,400
This might be useful in
reducing storage costs

308
00:13:37,400 --> 00:13:40,870
but at the same time, you
might regret having missed data

309
00:13:40,870 --> 00:13:42,550
in the event of a breach.

310
00:13:42,550 --> 00:13:46,483
So we recommend sampling 100% of the logs.

311
00:13:47,900 --> 00:13:50,360
Now, if you configure Google cloud logging

312
00:13:50,360 --> 00:13:52,890
to include all metadata and its flow logs,

313
00:13:52,890 --> 00:13:55,189
you can get a lot of information.

314
00:13:55,190 --> 00:13:58,290
And we could talk about several
of the items on this slide.

315
00:13:58,290 --> 00:14:02,199
But what is most important is
what the logs don't include.

316
00:14:02,200 --> 00:14:05,830
An indication of whether or
not the traffic was accepted

317
00:14:05,830 --> 00:14:08,410
or rejected by the firewall.

318
00:14:08,410 --> 00:14:10,600
You might be thinking, wow

319
00:14:10,600 --> 00:14:12,690
that's a huge one oversight from Google.

320
00:14:12,690 --> 00:14:14,600
When are they going to fix this bug?

321
00:14:14,600 --> 00:14:17,230
Well, the answer is, it's not a bug.

322
00:14:17,230 --> 00:14:18,460
It's a feature.

323
00:14:18,460 --> 00:14:20,350
This is by design.

324
00:14:20,350 --> 00:14:24,960
Why? Because of the GCP
flow logs for the VPC

325
00:14:24,960 --> 00:14:29,450
cover traffic from, quote,
"The perspective of the VM."

326
00:14:29,450 --> 00:14:31,690
This means that packets will be logged

327
00:14:31,690 --> 00:14:34,380
as they enter and exit a virtual machine,

328
00:14:34,380 --> 00:14:37,390
regardless of how they are
handled by the firewall.

329
00:14:37,390 --> 00:14:40,270
So we will lose all of the messages

330
00:14:40,270 --> 00:14:42,390
that are blocked by the firewall inbound

331
00:14:42,390 --> 00:14:44,069
and we will have no indication

332
00:14:44,070 --> 00:14:47,490
of whether or not the traffic
is accepted, outbound.

333
00:14:47,490 --> 00:14:49,610
The outbound flow logs will contain

334
00:14:49,610 --> 00:14:50,820
a lot of extra data

335
00:14:50,820 --> 00:14:53,980
and we have zero visibility
into the inbound traffic

336
00:14:53,980 --> 00:14:57,490
because those packets will
never make it to the VM.

337
00:14:57,490 --> 00:14:58,930
This is a huge distinction

338
00:14:58,930 --> 00:15:01,670
between GCP and the other two providers.

339
00:15:01,670 --> 00:15:03,000
Now, for this reason,

340
00:15:03,000 --> 00:15:05,490
you might want to consider also enabling

341
00:15:05,490 --> 00:15:07,460
firewall rules logging.

342
00:15:07,460 --> 00:15:09,860
This allows you to audit
your firewall rules

343
00:15:09,860 --> 00:15:13,710
and detect traffic that is
denied at the firewall level.

344
00:15:13,710 --> 00:15:16,760
You can read more about
this feature at this URL.

345
00:15:16,760 --> 00:15:18,720
But hopefully you now understand

346
00:15:18,720 --> 00:15:21,120
the different types of
flow logging and GCP

347
00:15:21,120 --> 00:15:24,033
and which types are appropriate
for your organization.

348
00:15:25,480 --> 00:15:29,090
Now, GCP is unique in that
it is the only provider

349
00:15:29,090 --> 00:15:32,750
that uses a single service for
persisting and querying logs:

350
00:15:32,750 --> 00:15:33,920
Google cloud logging.

351
00:15:33,920 --> 00:15:34,849
It's convenient,

352
00:15:34,850 --> 00:15:37,440
you don't have to manage
multiple services here.

353
00:15:37,440 --> 00:15:40,810
This service can be queried
via the CLI and the GUI,

354
00:15:40,810 --> 00:15:42,520
as shown here.

355
00:15:42,520 --> 00:15:43,910
In our first demonstration,

356
00:15:43,910 --> 00:15:45,910
we are going to audit our flow logs

357
00:15:45,910 --> 00:15:48,280
in each cloud environment
in order to reveal

358
00:15:48,280 --> 00:15:52,100
that our organization has been
compromised from the inside.

359
00:15:52,100 --> 00:15:53,380
Before we get into it,

360
00:15:53,380 --> 00:15:56,189
let's talk about supply chain attacks.

361
00:15:56,190 --> 00:15:59,560
We've all heard about the
solar winds supply chain attack

362
00:15:59,560 --> 00:16:01,119
that happened last year.

363
00:16:01,120 --> 00:16:03,520
It is scary to know that the services used

364
00:16:03,520 --> 00:16:05,960
in our environment can be compromised.

365
00:16:05,960 --> 00:16:08,320
What scares me even more is the thought

366
00:16:08,320 --> 00:16:11,980
about an open source
component being compromised.

367
00:16:11,980 --> 00:16:13,970
Modern applications are composed

368
00:16:13,970 --> 00:16:16,990
of thousands of third-party components.

369
00:16:16,990 --> 00:16:20,610
How can we know that each of
these components are clean?

370
00:16:20,610 --> 00:16:22,500
The answer? We can't.

371
00:16:22,500 --> 00:16:26,270
Our industry has failed
time and time again

372
00:16:26,270 --> 00:16:29,900
to this unsophisticated,
but powerful attack.

373
00:16:29,900 --> 00:16:32,699
One particularly incredible
example of this to me

374
00:16:32,700 --> 00:16:35,850
was in July of 2018 when ESLint,

375
00:16:35,850 --> 00:16:38,220
a popular and simple utility

376
00:16:38,220 --> 00:16:41,530
for cleaning up node.JS
code was compromised.

377
00:16:41,530 --> 00:16:44,290
The attacker embedded
malware into the library

378
00:16:44,290 --> 00:16:46,800
that downloaded another snippet of malware

379
00:16:46,800 --> 00:16:48,609
from pastebin.com.

380
00:16:48,610 --> 00:16:50,490
It would then execute this malware

381
00:16:50,490 --> 00:16:53,160
every time ESLint ran a scan.

382
00:16:53,160 --> 00:16:55,550
At minimum, this resulted in the theft

383
00:16:55,550 --> 00:16:59,329
of countless access tokens for npmjs.com.

384
00:16:59,330 --> 00:17:02,560
It seems like the attacker
wanted to get these credentials

385
00:17:02,560 --> 00:17:06,379
to publish even more malware
and spread endlessly.

386
00:17:06,380 --> 00:17:09,700
Fortunately, npmjs.com quickly revoked

387
00:17:09,700 --> 00:17:13,619
all tokens generated on or
before the date of compromise.

388
00:17:13,619 --> 00:17:16,159
Unfortunately, this also
broke a lot of builds

389
00:17:16,160 --> 00:17:19,369
and made a lot of developers
very unhappy that day.

390
00:17:19,369 --> 00:17:21,260
Speaking from experience.

391
00:17:21,260 --> 00:17:24,510
This slide just is an example
of the different types

392
00:17:24,510 --> 00:17:27,410
of supply chain repositories out there.

393
00:17:27,410 --> 00:17:30,230
In our presentation, we will
examine a piece of malware

394
00:17:30,230 --> 00:17:34,810
from npmjs.com that has found
its way into our clouds.

395
00:17:34,810 --> 00:17:37,379
You might have noticed that
earlier in this presentation,

396
00:17:37,380 --> 00:17:41,422
there were a couple signs
of malware in our flow logs.

397
00:17:41,422 --> 00:17:43,660
If you noticed some anomalies

398
00:17:43,660 --> 00:17:45,840
in the screenshots we provided,

399
00:17:45,840 --> 00:17:50,189
congratulations, you have
the attention to detail

400
00:17:50,190 --> 00:17:52,250
and the curiosity that is necessary

401
00:17:52,250 --> 00:17:54,743
in order to detect things like this.

402
00:17:55,690 --> 00:17:57,380
So are you ready to log?

403
00:17:57,380 --> 00:17:59,150
Let's get into it.

404
00:17:59,150 --> 00:18:02,720
In our demo environment,
we have a real application

405
00:18:02,720 --> 00:18:06,420
for a fake medical company
named Nimbus Inmutable.

406
00:18:06,420 --> 00:18:11,420
This application has been
deployed to AWS, Azure and GCP.

407
00:18:12,070 --> 00:18:14,010
Each version of the application

408
00:18:14,010 --> 00:18:18,170
uses the cloud native
services for that provider.

409
00:18:18,170 --> 00:18:22,410
Now these applications
were written in node.JS,

410
00:18:22,410 --> 00:18:27,240
a popular runtime for running
JavaScript on the backend.

411
00:18:27,240 --> 00:18:30,630
And for those who are not
familiar with node.JS,

412
00:18:30,630 --> 00:18:35,020
its package manager or its
supply chain is called NPM,

413
00:18:35,020 --> 00:18:36,780
the node package manager.

414
00:18:36,780 --> 00:18:38,470
And if we want to look at the packages

415
00:18:38,470 --> 00:18:40,290
used by this application,

416
00:18:40,290 --> 00:18:43,960
we can simply look at
the package.JSON file.

417
00:18:43,960 --> 00:18:46,450
Now, if you look at this file for a bit,

418
00:18:46,450 --> 00:18:48,050
you'll notice that one of these things

419
00:18:48,050 --> 00:18:49,270
is not like the other,

420
00:18:49,270 --> 00:18:52,160
one of these packages
looks very suspicious.

421
00:18:52,160 --> 00:18:56,110
Check read stream really
awesome totally secure, I think.

422
00:18:56,110 --> 00:19:00,590
This is a real package that
we deployed to npmjs.com.

423
00:19:00,590 --> 00:19:02,689
You could download it right now.

424
00:19:02,690 --> 00:19:03,840
I wouldn't recommend it

425
00:19:03,840 --> 00:19:06,520
because it's doing something nefarious.

426
00:19:06,520 --> 00:19:07,780
If we look at the source code,

427
00:19:07,780 --> 00:19:09,370
we'll see that initially

428
00:19:09,370 --> 00:19:11,679
it looks like this code
is actually useful,

429
00:19:11,680 --> 00:19:13,990
but at the very end, we'll
see that it's requiring

430
00:19:13,990 --> 00:19:16,690
this other file named innocuous.

431
00:19:16,690 --> 00:19:19,880
Is the innocuous file actually innocuous?

432
00:19:19,880 --> 00:19:21,040
Of course not.

433
00:19:21,040 --> 00:19:24,300
It has some obfuscated malware in it.

434
00:19:24,300 --> 00:19:29,210
This was the attacker trying
to pull one over on us.

435
00:19:29,210 --> 00:19:33,230
And we can't tell what this
code is actually doing,

436
00:19:33,230 --> 00:19:35,280
that's the purpose of obfuscation,

437
00:19:35,280 --> 00:19:38,530
but perhaps we could figure
out what this code is doing

438
00:19:38,530 --> 00:19:42,070
by looking at the network
traffic that it is generating.

439
00:19:42,070 --> 00:19:46,300
So we have our flow logs
enabled in our cloud providers.

440
00:19:46,300 --> 00:19:48,786
Let's take a look at AWS first.

441
00:19:48,786 --> 00:19:50,140
First here we are running a query

442
00:19:50,140 --> 00:19:53,470
that is looking at the traffic

443
00:19:53,470 --> 00:19:56,460
from our AWS ec2 instance.

444
00:19:56,460 --> 00:20:00,000
And it's looking for rejected
traffic, specifically.

445
00:20:00,000 --> 00:20:01,650
So we're going to run this query,

446
00:20:02,820 --> 00:20:05,870
after it's done we can
look at the results,

447
00:20:05,870 --> 00:20:08,000
and we'll notice that
there's a lot of traffic

448
00:20:08,000 --> 00:20:11,180
being sent on port 9999,

449
00:20:11,180 --> 00:20:15,760
which is not a typical port
for sending or receiving data.

450
00:20:15,760 --> 00:20:18,310
Now, if we look at the same thing in GCP,

451
00:20:18,310 --> 00:20:21,040
this is a little bit more
of a complicated query,

452
00:20:21,040 --> 00:20:24,930
but we are able to detect
the same kind of data.

453
00:20:24,930 --> 00:20:29,380
But as mentioned earlier,
VPC flow logs do not tell you

454
00:20:29,380 --> 00:20:32,860
whether or not a packet
was accepted or rejected.

455
00:20:32,860 --> 00:20:36,870
That's at the firewall
level, not at the VM level.

456
00:20:36,870 --> 00:20:38,550
So to do the same basic thing,

457
00:20:38,550 --> 00:20:40,680
we're going to look for traffic

458
00:20:40,680 --> 00:20:42,480
over ports that we are blocking,

459
00:20:42,480 --> 00:20:45,310
that we know is not going to
make it through the firewall.

460
00:20:45,310 --> 00:20:49,370
on port 80, 443, and 3306.

461
00:20:49,370 --> 00:20:51,429
So once we run this query,

462
00:20:51,430 --> 00:20:53,410
we're going to see some similar results

463
00:20:53,410 --> 00:20:55,480
to what we had before.

464
00:20:55,480 --> 00:21:00,180
A lot of traffic being sent over port 9999

465
00:21:00,180 --> 00:21:04,520
So flow logs only shows
you metadata about traffic.

466
00:21:04,520 --> 00:21:07,100
It doesn't show you the
contents of the traffic.

467
00:21:07,100 --> 00:21:11,449
To do that, we could go to
the virtual machine over SSH,

468
00:21:11,450 --> 00:21:14,200
and we could run a utility like TCP dump,

469
00:21:14,200 --> 00:21:16,400
which is going to show us all the traffic

470
00:21:16,400 --> 00:21:20,170
over port 9999, outbound.

471
00:21:20,170 --> 00:21:21,400
And in a short moment,

472
00:21:21,400 --> 00:21:23,873
we're going to see something suspicious.

473
00:21:24,990 --> 00:21:29,030
So here, we can see some
traffic that looks suspicious.

474
00:21:29,030 --> 00:21:30,570
This is a DNS query,

475
00:21:30,570 --> 00:21:32,970
but it doesn't look
like a normal DNS query.

476
00:21:32,970 --> 00:21:35,750
It looks like this
message might be encoded.

477
00:21:35,750 --> 00:21:39,793
So if I copy this message right here,

478
00:21:41,300 --> 00:21:43,139
maybe, just, maybe.

479
00:21:43,140 --> 00:21:45,443
I'm going to do this
again, sorry about that.

480
00:21:51,290 --> 00:21:55,330
So after a while, we will see
this DNS query right here.

481
00:21:55,330 --> 00:21:57,830
This doesn't look like a normal DNS query,

482
00:21:57,830 --> 00:22:00,730
it looks like potentially
an encoded message.

483
00:22:00,730 --> 00:22:03,800
Perhaps using base32 encoding.

484
00:22:03,800 --> 00:22:05,919
So if I copy this message

485
00:22:07,070 --> 00:22:09,179
and I join it with this message

486
00:22:09,180 --> 00:22:14,030
because DNS queries could
only carry 63 characters,

487
00:22:14,030 --> 00:22:17,660
then I uppercase it
and I base32 decode it,

488
00:22:17,660 --> 00:22:21,100
we will see that this message
contains the Mac addresses

489
00:22:21,100 --> 00:22:24,240
for the Nimbus Inmutable virtual machine.

490
00:22:24,240 --> 00:22:26,860
Now Mac addresses are private.

491
00:22:26,860 --> 00:22:30,800
You can't determine what a Mac
address is from the outside.

492
00:22:30,800 --> 00:22:33,000
But it's also innocuous,

493
00:22:33,000 --> 00:22:35,420
this is something that
we're doing for an example.

494
00:22:35,420 --> 00:22:39,470
Imagine if instead we were
getting credit card numbers,

495
00:22:39,470 --> 00:22:42,320
social security numbers, API keys,

496
00:22:42,320 --> 00:22:46,820
other data that is only accessible
by your virtual machine.

497
00:22:46,820 --> 00:22:48,050
This goes to show

498
00:22:48,050 --> 00:22:51,460
that data could be
exfiltrated via malware,

499
00:22:51,460 --> 00:22:52,930
like the one that we demonstrated,

500
00:22:52,930 --> 00:22:54,930
and that we can catch things like this

501
00:22:54,930 --> 00:22:57,860
by looking for anomalies in our flow logs.

502
00:22:57,860 --> 00:23:02,590
So we've seen the three cloud
providers' logging solutions.

503
00:23:02,590 --> 00:23:04,500
What are the core takeaways

504
00:23:04,500 --> 00:23:07,730
in terms of the differences
between these three solutions?

505
00:23:07,730 --> 00:23:09,380
The first thing that
you need to be aware of

506
00:23:09,380 --> 00:23:10,630
is the similarity.

507
00:23:10,630 --> 00:23:14,050
None of these logging solutions
are enabled by default.

508
00:23:14,050 --> 00:23:16,560
So if you take nothing else
away from this section,

509
00:23:16,560 --> 00:23:18,300
enable them today.

510
00:23:18,300 --> 00:23:22,159
We've talked about the delay
between triggering the logs

511
00:23:22,160 --> 00:23:25,200
and getting the logs
into a queryable format.

512
00:23:25,200 --> 00:23:26,700
GCP is the fastest.

513
00:23:26,700 --> 00:23:28,250
Azure is the slowest.

514
00:23:28,250 --> 00:23:31,110
In terms of retention
period, AWS and Azure,

515
00:23:31,110 --> 00:23:34,010
they allow you to have your
logs stored indefinitely.

516
00:23:34,010 --> 00:23:35,370
Of course they do.

517
00:23:35,370 --> 00:23:36,669
They can charge money for it.

518
00:23:36,670 --> 00:23:39,320
So I don't know why they
would have any restrictions.

519
00:23:39,320 --> 00:23:42,020
Interestingly enough, GCP
does have a restriction

520
00:23:42,020 --> 00:23:43,760
but it's absurdly high.

521
00:23:43,760 --> 00:23:45,320
It's 10 years.

522
00:23:45,320 --> 00:23:47,870
So I think you're good
to go after 10 years.

523
00:23:47,870 --> 00:23:50,139
I don't think this is really
a functional difference.

524
00:23:50,140 --> 00:23:51,100
After 10 years,

525
00:23:51,100 --> 00:23:53,570
your flow logs are
probably not useful to you.

526
00:23:53,570 --> 00:23:56,720
And if they are back them
up to an external service.

527
00:23:56,720 --> 00:23:58,450
In terms of command line support,

528
00:23:58,450 --> 00:24:00,540
all three of them
support the command line,

529
00:24:00,540 --> 00:24:03,000
but Azure requires an extension.

530
00:24:03,000 --> 00:24:05,560
And finally, the most
important distinction

531
00:24:05,560 --> 00:24:09,300
in terms of analyzing
the logs is AWS and Azure

532
00:24:09,300 --> 00:24:13,139
allow you to look at
blocked ingress traffic,

533
00:24:13,140 --> 00:24:17,450
whereas GCP, unless you're
using firewall flow logging,

534
00:24:17,450 --> 00:24:20,430
you will not see any data
about the ingress traffic

535
00:24:20,430 --> 00:24:21,700
that has been rejected.

536
00:24:21,700 --> 00:24:24,490
So very critical difference here.

537
00:24:24,490 --> 00:24:26,863
Now let's talk about cloud API logging.

538
00:24:28,270 --> 00:24:31,170
There are a lot of services in the cloud.

539
00:24:31,170 --> 00:24:33,010
We need to interact with these services.

540
00:24:33,010 --> 00:24:37,170
Sometimes as administrators
and other times as users.

541
00:24:37,170 --> 00:24:39,620
The rules that dictate how we interact

542
00:24:39,620 --> 00:24:41,399
with cloud services are defined

543
00:24:41,400 --> 00:24:45,363
in cloud identity and access
management services, or IAM.

544
00:24:46,399 --> 00:24:48,919
IAM services allow you
to grant permissions

545
00:24:48,920 --> 00:24:52,690
to both individual users
and to cloud infrastructure.

546
00:24:52,690 --> 00:24:54,290
For an example of the latter,

547
00:24:54,290 --> 00:24:56,659
when you want to grant
your application access

548
00:24:56,660 --> 00:25:00,060
to a cloud resource,
such as Amazon DynamoDB,

549
00:25:00,060 --> 00:25:03,560
you can provide the VM running
the application permissions

550
00:25:03,560 --> 00:25:06,570
with what is called the
instance profile role.

551
00:25:06,570 --> 00:25:09,169
The diagram on this slide
illustrates the flow

552
00:25:09,170 --> 00:25:10,760
from start to finish.

553
00:25:10,760 --> 00:25:13,180
We're going to make a request to DynamoDB

554
00:25:13,180 --> 00:25:16,470
using a request signed
with our IAM credentials.

555
00:25:16,470 --> 00:25:19,820
We are going to then
analyze, does this entity,

556
00:25:19,820 --> 00:25:23,510
does this principal have
permission to perform this action

557
00:25:23,510 --> 00:25:24,960
on this resource?

558
00:25:24,960 --> 00:25:27,980
If the answer is yes, we're
going to get the results.

559
00:25:27,980 --> 00:25:30,533
If not, we are going to throw an error.

560
00:25:31,680 --> 00:25:33,730
So it is important to understand

561
00:25:33,730 --> 00:25:36,660
that if the credentials
used in the background

562
00:25:36,660 --> 00:25:39,660
with the cloud IAM
service are compromised,

563
00:25:39,660 --> 00:25:42,410
the attacker will have all
of the privileges granted

564
00:25:42,410 --> 00:25:44,070
to that IAM principal.

565
00:25:44,070 --> 00:25:46,350
An attack just like this happened

566
00:25:46,350 --> 00:25:49,330
to Capital One in July of 2019.

567
00:25:49,330 --> 00:25:50,710
A sophisticated attacker

568
00:25:50,710 --> 00:25:53,160
who had previously worked for AWS

569
00:25:53,160 --> 00:25:56,830
knew that it was possible to
extract the IAM credentials

570
00:25:56,830 --> 00:26:00,260
for a VM hosted on an Amazon ec2 instance

571
00:26:00,260 --> 00:26:03,490
that is hosting an open forward proxy.

572
00:26:03,490 --> 00:26:07,280
Normally a proxy will
accept requests to users

573
00:26:07,280 --> 00:26:11,580
to fetch a particular
resource on their behalf.

574
00:26:11,580 --> 00:26:14,050
If the proxy accepts any URL,

575
00:26:14,050 --> 00:26:16,730
it is possible to fetch private resources

576
00:26:16,730 --> 00:26:19,150
within the VM's private network.

577
00:26:19,150 --> 00:26:23,570
This is called a server side
request forgery vulnerability,

578
00:26:23,570 --> 00:26:25,980
or an SSRF vulnerability.

579
00:26:25,980 --> 00:26:27,540
In Capital One's case,

580
00:26:27,540 --> 00:26:29,730
the attacker was able to
use such a vulnerability

581
00:26:29,730 --> 00:26:31,390
to have the proxy server

582
00:26:31,390 --> 00:26:34,360
access the internal
instance metadata service

583
00:26:34,360 --> 00:26:36,419
on the ec2 instance,

584
00:26:36,420 --> 00:26:39,910
which contains the
instance's IAM credentials.

585
00:26:39,910 --> 00:26:41,670
Thinking that this is a valid request,

586
00:26:41,670 --> 00:26:45,010
because the application has
no way of distinguishing

587
00:26:45,010 --> 00:26:48,510
between fetching a legitimate
file and an illegitimate file,

588
00:26:48,510 --> 00:26:51,670
the proxy returned the
credentials to the attacker.

589
00:26:51,670 --> 00:26:53,490
From there, they were able to access

590
00:26:53,490 --> 00:26:57,270
all the resources that ec2
instance had access to.

591
00:26:57,270 --> 00:27:00,070
Unfortunately, this included S3 buckets

592
00:27:00,070 --> 00:27:03,730
containing 100 million
credit card applications.

593
00:27:03,730 --> 00:27:07,060
So there are a lot of ways
to prevent attacks like this.

594
00:27:07,060 --> 00:27:09,399
You could eliminate
the SSRF vulnerability.

595
00:27:09,400 --> 00:27:10,970
You could reduce the permissions

596
00:27:10,970 --> 00:27:13,300
so that the ec2 instance
doesn't have access

597
00:27:13,300 --> 00:27:14,950
to all this excess data.

598
00:27:14,950 --> 00:27:18,300
There's a lot of things you can
do to prevent these attacks,

599
00:27:18,300 --> 00:27:21,230
but this is a discussion on monitoring.

600
00:27:21,230 --> 00:27:24,510
Our focus will be on
monitoring our cloud API usage

601
00:27:24,510 --> 00:27:27,010
for anomalies that could indicate

602
00:27:27,010 --> 00:27:28,840
that credentials have been stolen

603
00:27:28,840 --> 00:27:31,023
or are otherwise being abused.

604
00:27:32,080 --> 00:27:35,530
Now in AWS, we can audit
our API access logs

605
00:27:35,530 --> 00:27:37,870
using a service called CloudTrail.

606
00:27:37,870 --> 00:27:40,739
On this slide, we are enabling
the CloudTrail service

607
00:27:40,740 --> 00:27:42,930
using Terraform code.

608
00:27:42,930 --> 00:27:45,800
Terraform is an
infrastructure as code tool

609
00:27:45,800 --> 00:27:48,580
that we can use to configure
our cloud environments.

610
00:27:48,580 --> 00:27:51,439
Its syntax is a fairly straightforward way

611
00:27:51,440 --> 00:27:53,740
of describing our cloud configuration

612
00:27:53,740 --> 00:27:57,263
so we will use it to do so
in a few of these slides.

613
00:27:58,180 --> 00:28:00,150
Now, Amazon is very strict

614
00:28:00,150 --> 00:28:02,510
about how they handle access management,

615
00:28:02,510 --> 00:28:07,510
so much so that CloudTrail
can't even rise to an S3 bucket

616
00:28:07,550 --> 00:28:11,500
on your behalf, unless you
authorize it with an IAM policy.

617
00:28:11,500 --> 00:28:14,170
So here's an example of
such a policy provided

618
00:28:14,170 --> 00:28:16,050
by AWS's documentation

619
00:28:16,050 --> 00:28:19,883
allowing for CloudTrail to
write logs to your S3 bucket.

620
00:28:21,140 --> 00:28:25,140
Now CloudTrails can be persistent
to both S3 and CloudWatch.

621
00:28:25,140 --> 00:28:27,210
One of the advantages of using CloudWatch

622
00:28:27,210 --> 00:28:30,630
is now we can query our API audit logs

623
00:28:30,630 --> 00:28:34,040
using the same techniques
that we did for flow logs.

624
00:28:34,040 --> 00:28:36,399
So here's an example of that right here.

625
00:28:36,400 --> 00:28:38,550
We are looking for decryption events

626
00:28:38,550 --> 00:28:42,253
in our AWS environment,
using a particular key.

627
00:28:43,830 --> 00:28:44,990
Onto Azure.

628
00:28:44,990 --> 00:28:48,570
Unlike AWS, where we can
enable all cloud API logs

629
00:28:48,570 --> 00:28:50,210
with a single resource,

630
00:28:50,210 --> 00:28:52,190
in Azure, we must enable logging

631
00:28:52,190 --> 00:28:54,280
on a service by service basis

632
00:28:54,280 --> 00:28:55,720
by creating what is called

633
00:28:55,720 --> 00:28:58,413
an Azure monitor diagnostic setting.

634
00:28:59,780 --> 00:29:01,420
Google is like Azure

635
00:29:01,420 --> 00:29:04,710
in that you must enable
logging for each service.

636
00:29:04,710 --> 00:29:07,940
In addition, you can
choose to only log actions

637
00:29:07,940 --> 00:29:10,260
that read data or write data.

638
00:29:10,260 --> 00:29:14,860
However, by default GCP logs
have what are considered

639
00:29:14,860 --> 00:29:18,419
administrative actions
logged at all times.

640
00:29:18,420 --> 00:29:20,600
These logs cannot be disabled.

641
00:29:20,600 --> 00:29:23,449
So if you're using GCP and you haven't yet

642
00:29:23,450 --> 00:29:25,950
audited your cloud logging configuration,

643
00:29:25,950 --> 00:29:28,730
rest assured that you at
least have these critical

644
00:29:28,730 --> 00:29:30,400
logs at your disposal.

645
00:29:30,400 --> 00:29:31,820
But it's far from what you need

646
00:29:31,820 --> 00:29:33,973
in order to catch a potential attack.

647
00:29:35,130 --> 00:29:37,420
We're now going to show a potential attack

648
00:29:37,420 --> 00:29:40,260
or abuse of cryptographic keys

649
00:29:40,260 --> 00:29:44,340
and how to detect that abuse
using cloud API logging.

650
00:29:44,340 --> 00:29:46,669
In this demonstration
we're going to show you

651
00:29:46,670 --> 00:29:49,760
how to audit your cloud API logs

652
00:29:49,760 --> 00:29:53,080
to find a misuse of a cryptographic key.

653
00:29:53,080 --> 00:29:54,800
A little bit of background here,

654
00:29:54,800 --> 00:29:56,210
the development team has decided

655
00:29:56,210 --> 00:29:58,380
to create their own custom solution

656
00:29:58,380 --> 00:30:00,290
for backing up their databases.

657
00:30:00,290 --> 00:30:02,500
They're not going to use
the cloud native solutions

658
00:30:02,500 --> 00:30:03,600
that are provided.

659
00:30:03,600 --> 00:30:07,490
I wish they did, but instead
they are using a manual process

660
00:30:07,490 --> 00:30:08,930
with my SQL dump.

661
00:30:08,930 --> 00:30:11,770
They're going to encrypt
the database with KMS

662
00:30:11,770 --> 00:30:14,110
and they're going to store it in S3.

663
00:30:14,110 --> 00:30:15,699
Same concept with Azure,

664
00:30:15,700 --> 00:30:17,570
they're going to dump the database,

665
00:30:17,570 --> 00:30:21,429
they are going to encrypt it
using the Azure key vault,

666
00:30:21,430 --> 00:30:25,450
and then they're going to put
the database in Azure Storage.

667
00:30:25,450 --> 00:30:28,180
And finally with GCP, same thing,

668
00:30:28,180 --> 00:30:30,400
dump the database, KMS encrypt,

669
00:30:30,400 --> 00:30:33,160
and copy it to Google cloud storage.

670
00:30:33,160 --> 00:30:36,720
In this setup, we have run
these scripts one time.

671
00:30:36,720 --> 00:30:38,780
So there shouldn't be
a whole lot of activity

672
00:30:38,780 --> 00:30:40,889
with these cryptographic keys.

673
00:30:40,890 --> 00:30:43,510
Unfortunately, that is not the case.

674
00:30:43,510 --> 00:30:47,680
If we look at our key
usage using Azure metrics,

675
00:30:47,680 --> 00:30:50,160
specifically our key vault metrics,

676
00:30:50,160 --> 00:30:53,130
we will see there are a lot of events

677
00:30:53,130 --> 00:30:55,800
that are being triggered
by our cryptographic keys.

678
00:30:55,800 --> 00:30:58,149
Six within the past hour.

679
00:30:58,150 --> 00:31:01,380
If we look at the same thing
with Google cloud storage,

680
00:31:01,380 --> 00:31:04,640
and we're going to specifically
look for the backup key

681
00:31:04,640 --> 00:31:06,210
that we're using here,

682
00:31:06,210 --> 00:31:09,740
we will see a lot of traffic here as well,

683
00:31:09,740 --> 00:31:12,030
much more traffic than
we would have expected

684
00:31:12,030 --> 00:31:14,710
given our usage of the key.

685
00:31:14,710 --> 00:31:16,670
So why is this?

686
00:31:16,670 --> 00:31:19,340
Will it turns out that a data scientist

687
00:31:19,340 --> 00:31:21,780
has actually been running
a script of their own

688
00:31:21,780 --> 00:31:25,180
in order to decrypt the
data in the database

689
00:31:25,180 --> 00:31:28,140
and run a variety of analytics.

690
00:31:28,140 --> 00:31:29,890
Now, this is unfortunately something

691
00:31:29,890 --> 00:31:31,670
that I have personally seen

692
00:31:31,670 --> 00:31:33,380
where data scientists

693
00:31:33,380 --> 00:31:36,820
not given the data that they
need to use to do their job

694
00:31:36,820 --> 00:31:39,189
will find the data one way or the other.

695
00:31:39,190 --> 00:31:44,030
And oftentimes this will involve
a lot of manual processes.

696
00:31:44,030 --> 00:31:46,360
They're going to run
a variety of commands,

697
00:31:46,360 --> 00:31:48,370
potentially on their workstation,

698
00:31:48,370 --> 00:31:50,419
and they're going to do some processing

699
00:31:50,420 --> 00:31:52,890
instead of perhaps getting the same data

700
00:31:52,890 --> 00:31:57,110
from an API or an SDK
and using it securely.

701
00:31:57,110 --> 00:31:58,379
So in this instance,

702
00:31:58,380 --> 00:32:02,270
they are decrypting data
from their workstation,

703
00:32:02,270 --> 00:32:05,570
and that's probably
something we'd like to avoid.

704
00:32:05,570 --> 00:32:08,929
We would like to keep the
usage of our cryptographic keys

705
00:32:08,930 --> 00:32:11,850
to our virtual private network.

706
00:32:11,850 --> 00:32:13,480
So this is something

707
00:32:13,480 --> 00:32:16,080
that we were able to find
using our audit logs,

708
00:32:16,080 --> 00:32:18,710
but the solution is to lock down our keys

709
00:32:18,710 --> 00:32:22,380
such that they are not
able to be used outside

710
00:32:22,380 --> 00:32:26,360
of our network and not by the wrong users.

711
00:32:26,360 --> 00:32:29,520
So just like before we
should compare the services

712
00:32:29,520 --> 00:32:30,480
at play here.

713
00:32:30,480 --> 00:32:34,730
AWS and Azure do not
log anything by default

714
00:32:34,730 --> 00:32:37,350
when it comes to cloud API audit logs.

715
00:32:37,350 --> 00:32:40,980
GCP also does contain a lot of data

716
00:32:40,980 --> 00:32:42,660
that is useful to your organization,

717
00:32:42,660 --> 00:32:45,080
but at least they log what they consider

718
00:32:45,080 --> 00:32:47,240
to be admin actions.

719
00:32:47,240 --> 00:32:50,390
None of them log user actions by default,

720
00:32:50,390 --> 00:32:55,110
and the scope of these logs
vary from provider to provider.

721
00:32:55,110 --> 00:32:59,129
The AWS logs you can enable
for every single service

722
00:32:59,130 --> 00:33:00,830
with a single configuration.

723
00:33:00,830 --> 00:33:02,409
For Azure you have to enable it

724
00:33:02,410 --> 00:33:04,280
on a service by service basis.

725
00:33:04,280 --> 00:33:06,770
And then GCP gets even more granular.

726
00:33:06,770 --> 00:33:08,639
You have to configure each service

727
00:33:08,640 --> 00:33:10,950
for a particular type of log.

728
00:33:10,950 --> 00:33:13,540
So this can help you save costs

729
00:33:13,540 --> 00:33:15,170
by limiting what you're logging,

730
00:33:15,170 --> 00:33:18,070
but it can also be more
operationally expensive

731
00:33:18,070 --> 00:33:20,909
to enable all of these
logs that you care about.

732
00:33:20,910 --> 00:33:22,740
Before we wrap up this conversation,

733
00:33:22,740 --> 00:33:25,830
let's talk about how we can
use these metrics and logs

734
00:33:25,830 --> 00:33:29,340
in order to feed our alerting systems.

735
00:33:29,340 --> 00:33:32,520
Logs are valuable but only
if they are monitored.

736
00:33:32,520 --> 00:33:35,570
It is expensive to have
human beings analyze

737
00:33:35,570 --> 00:33:37,510
all of your logs at all times.

738
00:33:37,510 --> 00:33:39,710
Potentially impossible depending

739
00:33:39,710 --> 00:33:41,480
on the size of your organization

740
00:33:41,480 --> 00:33:43,560
and the amount of data that you have.

741
00:33:43,560 --> 00:33:46,690
Luckily we can help our
incident response teams focus

742
00:33:46,690 --> 00:33:49,960
on interesting anomalies using alarms.

743
00:33:49,960 --> 00:33:53,230
Amazon CloudWatch has a service
called CloudWatch Alarms

744
00:33:53,230 --> 00:33:55,610
for exactly this purpose.

745
00:33:55,610 --> 00:33:57,820
CloudWatch Alarms will perform an action

746
00:33:57,820 --> 00:33:59,649
when a CloudWatch metric

747
00:33:59,650 --> 00:34:02,640
exceeds a particular set of criteria.

748
00:34:02,640 --> 00:34:05,210
It is possible to use CloudWatch logs

749
00:34:05,210 --> 00:34:07,830
to create CloudWatch metrics.

750
00:34:07,830 --> 00:34:10,980
We can create a metric
that counts every log

751
00:34:10,980 --> 00:34:13,650
that matches a particular pattern.

752
00:34:13,650 --> 00:34:15,909
From there, we can sound an alarm

753
00:34:15,909 --> 00:34:18,569
if this log occurs at a different rate

754
00:34:18,570 --> 00:34:20,239
than what we'd expect.

755
00:34:20,239 --> 00:34:22,819
Azure and GCP have similar capabilities

756
00:34:22,820 --> 00:34:26,250
to create an alert off
of log based metrics.

757
00:34:26,250 --> 00:34:27,440
But for the sake of time,

758
00:34:27,440 --> 00:34:29,757
we'll just show this example with AWS.

759
00:34:30,610 --> 00:34:32,429
This alarm will go off

760
00:34:32,429 --> 00:34:34,810
if on two separate occasions

761
00:34:34,810 --> 00:34:37,940
there are more than 80 decryption events

762
00:34:37,940 --> 00:34:42,250
in a period of 120 seconds.

763
00:34:42,250 --> 00:34:46,000
When the alarm goes off,
we can deliver an alert

764
00:34:46,000 --> 00:34:50,270
to a variety of platforms
via an SNS topic.

765
00:34:50,270 --> 00:34:52,340
By creating a variety of alarms

766
00:34:52,340 --> 00:34:54,570
for our cloud logs and metrics,

767
00:34:54,570 --> 00:34:56,699
we can sound the emergency alarm

768
00:34:56,699 --> 00:35:00,350
when things dramatically
differ from the norm.

769
00:35:00,350 --> 00:35:02,640
So what are your next steps?

770
00:35:02,640 --> 00:35:05,810
If you take nothing else
away from this talk,

771
00:35:05,810 --> 00:35:08,330
please make sure that your organization

772
00:35:08,330 --> 00:35:11,790
is capturing cloud API
and network flow logs

773
00:35:11,790 --> 00:35:14,460
for each of the cloud
providers that you use.

774
00:35:14,460 --> 00:35:15,760
It is impossible

775
00:35:15,760 --> 00:35:18,580
to start properly monitoring
these environments

776
00:35:18,580 --> 00:35:21,140
unless you have data to analyze.

777
00:35:21,140 --> 00:35:24,670
Start collecting that data today.

778
00:35:24,670 --> 00:35:27,040
Next, take a look at
what your organization

779
00:35:27,040 --> 00:35:29,060
is currently using for monitoring.

780
00:35:29,060 --> 00:35:31,820
Maybe you're already using
cloud-based solutions.

781
00:35:31,820 --> 00:35:35,110
Great. Maybe you're using
third-party solutions.

782
00:35:35,110 --> 00:35:38,410
Perhaps these third-party
solutions are still useful

783
00:35:38,410 --> 00:35:41,270
in addition to cloud-based solutions.

784
00:35:41,270 --> 00:35:42,500
Maybe not.

785
00:35:42,500 --> 00:35:44,240
That's why you should use both

786
00:35:44,240 --> 00:35:48,140
in the median term over
the next three months.

787
00:35:48,140 --> 00:35:51,069
These cloud-based solutions
can provide insights

788
00:35:51,070 --> 00:35:53,250
that you can't get otherwise.

789
00:35:53,250 --> 00:35:56,090
Combine your tools to maximize the chances

790
00:35:56,090 --> 00:35:58,720
that you'll find indicators of compromise.

791
00:35:58,720 --> 00:36:01,649
Long-term you will
discover several weaknesses

792
00:36:01,650 --> 00:36:04,420
in your cloud infrastructure
and configuration.

793
00:36:04,420 --> 00:36:06,610
Work with engineering to
harden your permissions

794
00:36:06,610 --> 00:36:08,840
using the principle of least privilege.

795
00:36:08,840 --> 00:36:12,700
If we can block attacks
altogether, we absolutely should.

796
00:36:12,700 --> 00:36:16,460
However, monitoring and alerting
will always be necessary

797
00:36:16,460 --> 00:36:20,900
to find the weaknesses we have
not yet identified and fixed.

798
00:36:20,900 --> 00:36:22,820
Additionally, if you've determined

799
00:36:22,820 --> 00:36:25,720
that these cloud-based monitoring
solutions are sufficient

800
00:36:25,720 --> 00:36:27,220
for your use cases,

801
00:36:27,220 --> 00:36:28,689
you might want to consider

802
00:36:28,690 --> 00:36:31,790
decommissioning redundant
third party solutions.

803
00:36:31,790 --> 00:36:35,300
Finally, while we can find
a lot of anomalies simply

804
00:36:35,300 --> 00:36:38,240
by crafting log queries
to find these anomalies,

805
00:36:38,240 --> 00:36:40,640
this will require some
effort from your security

806
00:36:40,640 --> 00:36:42,109
and operations teams,

807
00:36:42,110 --> 00:36:45,210
to understand the baseline,
understand what is normal

808
00:36:45,210 --> 00:36:48,110
and then craft queries
that can find differences

809
00:36:48,110 --> 00:36:50,150
from those baselines.

810
00:36:50,150 --> 00:36:54,090
We can augment those efforts
using machine learning.

811
00:36:54,090 --> 00:36:56,600
Dave Holser, a faculty fellow at SANS,

812
00:36:56,600 --> 00:36:58,589
recently did a free live stream

813
00:36:58,590 --> 00:37:01,700
called applying machine
learning to network anomalies,

814
00:37:01,700 --> 00:37:03,629
which I've linked to here.

815
00:37:03,630 --> 00:37:06,770
So in summary, there are so many logs

816
00:37:06,770 --> 00:37:08,960
that your cloud providers can generate

817
00:37:08,960 --> 00:37:10,710
about your environment,

818
00:37:10,710 --> 00:37:12,770
but these logs are useless

819
00:37:12,770 --> 00:37:16,540
unless you are storing
them and monitoring them.

820
00:37:16,540 --> 00:37:19,220
And in the process of
storing and monitoring them,

821
00:37:19,220 --> 00:37:21,120
there are several key differences

822
00:37:21,120 --> 00:37:23,069
between the big three cloud providers,

823
00:37:23,070 --> 00:37:24,650
which you now understand

824
00:37:24,650 --> 00:37:27,470
and can help yourself navigate through.

825
00:37:27,470 --> 00:37:30,589
So I hope that you have
enjoyed this presentation

826
00:37:30,590 --> 00:37:32,740
and I hope that this has provided value

827
00:37:32,740 --> 00:37:34,160
to your organization.

828
00:37:34,160 --> 00:37:36,819
If you'd like to learn
more about me and my teams,

829
00:37:36,820 --> 00:37:41,660
feel free to follow me and my
teams on Twitter and LinkedIn.

830
00:37:41,660 --> 00:37:44,100
Wrapping up, I'd like to
acknowledge a few folks

831
00:37:44,100 --> 00:37:46,100
who contributed to this presentation.

832
00:37:46,100 --> 00:37:48,890
First and foremost, thank
you to Eric Johnson,

833
00:37:48,890 --> 00:37:50,960
the other author of SEC510,

834
00:37:50,960 --> 00:37:55,030
without whom this course
simply would not exist.

835
00:37:55,030 --> 00:37:57,780
Thanks to Frank Kim, my mentor at SANS

836
00:37:57,780 --> 00:37:58,890
who has been instrumental

837
00:37:58,890 --> 00:38:00,810
in the development of this content.

838
00:38:00,810 --> 00:38:02,880
Make sure to check out
Frank's presentation

839
00:38:02,880 --> 00:38:05,410
here at RSAC 2021.

840
00:38:05,410 --> 00:38:07,149
And thanks to everyone from Zoom

841
00:38:07,150 --> 00:38:08,900
who reviewed my presentation.

842
00:38:08,900 --> 00:38:12,820
I am proud that there are two
other presentations at RSAC

843
00:38:12,820 --> 00:38:14,610
this year from Zoomies.

844
00:38:14,610 --> 00:38:18,360
Make sure to check out Max Crone
and Adam Rotterman's talks.

845
00:38:18,360 --> 00:38:21,240
And finally, this slide
wouldn't be complete

846
00:38:21,240 --> 00:38:23,680
without me thanking you for listening.

847
00:38:23,680 --> 00:38:27,149
You had many high quality
talks to choose from today

848
00:38:27,150 --> 00:38:29,610
and I really appreciate that
you took the opportunity

849
00:38:29,610 --> 00:38:30,543
to attend to mine.

850
00:38:31,570 --> 00:38:32,440
Thank you so much.

851
00:38:32,440 --> 00:38:34,990
And I hope you have a great
rest of the conference.

