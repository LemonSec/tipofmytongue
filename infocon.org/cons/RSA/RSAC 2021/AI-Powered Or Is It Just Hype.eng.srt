1
00:00:01,330 --> 00:00:03,180
- Hi, thank you for joining us today.

2
00:00:03,180 --> 00:00:04,620
My name is Anne Townsend,

3
00:00:04,620 --> 00:00:06,470
I'm a part of the MITRE Corporation

4
00:00:06,470 --> 00:00:09,350
and I'm joined by my colleague,
Dr. Mike Hadjimichael,

5
00:00:09,350 --> 00:00:11,580
also a part of the MITRE Corporation.

6
00:00:11,580 --> 00:00:13,700
And we're here today to
talk to you about this term,

7
00:00:13,700 --> 00:00:16,210
this idea of AI powered.

8
00:00:16,210 --> 00:00:19,483
Are things really AI
powered or is it just hype?

9
00:00:20,590 --> 00:00:22,430
So now that cybersecurity conferences

10
00:00:22,430 --> 00:00:24,850
like today being at RSA,

11
00:00:24,850 --> 00:00:27,040
when we walk out to the expo floor

12
00:00:27,040 --> 00:00:29,759
we have a wealth of
resources we can use to talk

13
00:00:29,760 --> 00:00:32,920
to these cybersecurity
vendors about the product

14
00:00:32,920 --> 00:00:35,120
that they're providing to us.

15
00:00:35,120 --> 00:00:36,980
If I wanted to know how they're securing

16
00:00:36,980 --> 00:00:38,260
their product themselves,

17
00:00:38,260 --> 00:00:39,580
I could look to something like

18
00:00:39,580 --> 00:00:42,800
the NIST 800-53 security control.

19
00:00:42,800 --> 00:00:46,190
And I could find out like, how
are they securing their data,

20
00:00:46,190 --> 00:00:48,699
I could ask them questions
about user access controls,

21
00:00:48,700 --> 00:00:51,280
user privileges, but I could
really start to understand

22
00:00:51,280 --> 00:00:54,810
about the security that they
put on their device themselves.

23
00:00:54,810 --> 00:00:57,450
So, if I was to be an
adopter of their tool

24
00:00:57,450 --> 00:00:59,180
and I bring this into my enterprise,

25
00:00:59,180 --> 00:01:00,890
I realized that

26
00:01:02,450 --> 00:01:04,420
what kind of security implications

27
00:01:04,420 --> 00:01:06,920
I could be bringing into my enterprise.

28
00:01:06,920 --> 00:01:10,060
On the same accord, I could
also ask them a question

29
00:01:11,170 --> 00:01:13,720
about what kind of capability
they're providing me.

30
00:01:13,720 --> 00:01:16,887
I could say, "Would you be
able to help me identify

31
00:01:16,887 --> 00:01:19,027
"vulnerabilities within my enterprise?

32
00:01:19,027 --> 00:01:23,317
"Or could you help me better
protect my enterprise?

33
00:01:23,317 --> 00:01:25,767
"Or I have no ability to recover,

34
00:01:25,767 --> 00:01:28,699
"are you able to help
me with this at all?"

35
00:01:28,700 --> 00:01:31,170
If you're familiar with the
NIST cybersecurity framework

36
00:01:31,170 --> 00:01:33,330
those are the functions that
are within their framework.

37
00:01:33,330 --> 00:01:35,429
And it really helps me understand

38
00:01:35,430 --> 00:01:38,200
really what they're gonna provide for me.

39
00:01:38,200 --> 00:01:39,560
What's great about this dialogue

40
00:01:39,560 --> 00:01:41,690
is it really lets vendors understand

41
00:01:41,690 --> 00:01:44,810
as a potential tool adopter,
what I'm looking for,

42
00:01:44,810 --> 00:01:46,890
what I need, and it can help them to drive

43
00:01:46,890 --> 00:01:49,750
how they produce their
cyber security products.

44
00:01:49,750 --> 00:01:51,230
Conversely, as the adopter

45
00:01:51,230 --> 00:01:53,080
when I'm asking these kinds of questions

46
00:01:53,080 --> 00:01:55,429
I can kind of start to understand
what I'm really getting

47
00:01:55,430 --> 00:01:57,700
by adopting this tool.

48
00:01:57,700 --> 00:02:00,600
The other great thing about
this wealth of resources

49
00:02:00,600 --> 00:02:03,100
I have to ask him these questions is,

50
00:02:03,100 --> 00:02:06,410
it doesn't really reveal
any intellectual property.

51
00:02:06,410 --> 00:02:07,789
I'm not asking questions

52
00:02:07,790 --> 00:02:10,350
that a vendor should really
be uncomfortable in answering

53
00:02:10,350 --> 00:02:13,680
'cause they might be
revealing through secrets.

54
00:02:13,680 --> 00:02:16,370
But at these conferences
in the last few years,

55
00:02:16,370 --> 00:02:19,130
you'll notice that there's
been a lot of buzzwords

56
00:02:19,130 --> 00:02:20,850
about AI being dropped, right?

57
00:02:20,850 --> 00:02:25,373
AI-Enabled, AI-Powered, we
use AI, AI for the better.

58
00:02:26,330 --> 00:02:29,000
But there's this question
of those that dialogue

59
00:02:29,000 --> 00:02:31,090
that we're having about cybersecurity,

60
00:02:31,090 --> 00:02:36,090
does it really translate to
understand that AI component?

61
00:02:36,150 --> 00:02:37,340
So I wanna put a pen there.

62
00:02:37,340 --> 00:02:39,400
I want that question to
kind of sink in your mind

63
00:02:39,400 --> 00:02:40,280
for a minute.

64
00:02:40,280 --> 00:02:43,543
And I'd like to move to
the idea of AI, next.

65
00:02:45,550 --> 00:02:48,460
So this slide, I could
go down a rabbit hole,

66
00:02:48,460 --> 00:02:51,080
we could talk in the world of
adversarial machine learning

67
00:02:51,080 --> 00:02:53,490
and it's fascinating, a fascinating world,

68
00:02:53,490 --> 00:02:56,370
but I need to just bring it up one level

69
00:02:56,370 --> 00:02:58,820
to kind of illustrate the
idea that there's still a lot

70
00:02:58,820 --> 00:03:02,040
to learn with AI especially
when we wanna talk

71
00:03:02,040 --> 00:03:04,790
about AI and its consequences.

72
00:03:04,790 --> 00:03:07,510
So, if you look at these
images, the ones that almost

73
00:03:07,510 --> 00:03:10,170
is in the middle of the screen,

74
00:03:10,170 --> 00:03:12,690
I, as a human, I look at this image

75
00:03:12,690 --> 00:03:16,170
and I know that this is a
pig, it's labeled as a pig.

76
00:03:16,170 --> 00:03:17,899
In the world of adversarial
machine learning,

77
00:03:17,900 --> 00:03:20,460
there's what is called innovation attack.

78
00:03:20,460 --> 00:03:22,020
Into this digital image,

79
00:03:22,020 --> 00:03:24,150
there's a slight perturbation

80
00:03:24,150 --> 00:03:25,870
which is what this equation is about,

81
00:03:25,870 --> 00:03:29,053
adding this little gradient
kind of noise to the image.

82
00:03:30,020 --> 00:03:31,660
The human eye, can't see it

83
00:03:31,660 --> 00:03:34,600
but it's been placed within
the digital image itself

84
00:03:34,600 --> 00:03:36,989
and as a result, we get
this picture that's over

85
00:03:36,990 --> 00:03:39,250
on the other side of the equal sign.

86
00:03:39,250 --> 00:03:41,840
Again, as a human I look at this picture

87
00:03:41,840 --> 00:03:43,930
and I know that this is a pig.

88
00:03:43,930 --> 00:03:47,810
But for this example, this
image classifier saw this pig

89
00:03:47,810 --> 00:03:48,793
as an airliner.

90
00:03:49,851 --> 00:03:51,572
What that means is the
left side of the screen

91
00:03:51,572 --> 00:03:53,410
and we talk about this, stop sign.

92
00:03:53,410 --> 00:03:55,920
This one there's a perturbation,
but obviously this is

93
00:03:55,920 --> 00:03:56,899
in the physical world.

94
00:03:56,900 --> 00:03:58,620
So I can see this as a human,

95
00:03:58,620 --> 00:04:00,040
I know it's still a stop sign,

96
00:04:00,040 --> 00:04:02,620
but I see that there's graffiti on it.

97
00:04:02,620 --> 00:04:05,850
If I'm driving my car and I'm
approaching an intersection

98
00:04:05,850 --> 00:04:09,850
and I see the stop sign, I
know I should stop my car

99
00:04:09,850 --> 00:04:12,880
regardless of the fact that
there's graffiti on it.

100
00:04:12,880 --> 00:04:16,029
But in this example,
this image classifier,

101
00:04:16,029 --> 00:04:17,700
when it approaches the stop sign,

102
00:04:17,700 --> 00:04:22,500
it sees the stop sign as a
45 mile per hour speed limit.

103
00:04:22,500 --> 00:04:24,990
Think of the consequence of
approaching that intersection

104
00:04:24,990 --> 00:04:26,780
and proceeding at 45 miles

105
00:04:28,500 --> 00:04:29,333
Next.

106
00:04:32,590 --> 00:04:35,099
So, I wanna put these
two thoughts together.

107
00:04:35,100 --> 00:04:37,120
So the last slide about AI,

108
00:04:37,120 --> 00:04:38,750
I picked on adversarial machine learning.

109
00:04:38,750 --> 00:04:40,600
I talked about security.

110
00:04:40,600 --> 00:04:42,750
But as an AI component besides security,

111
00:04:42,750 --> 00:04:45,000
there's other things I'm gonna wanna know

112
00:04:45,000 --> 00:04:47,830
when it's a critical part of an AI

113
00:04:47,830 --> 00:04:51,729
or a cybersecurity type of product.

114
00:04:51,730 --> 00:04:56,200
So some thoughts would be,
with this AI component,

115
00:04:56,200 --> 00:04:58,777
is it really doing what
it should be doing?

116
00:04:58,777 --> 00:05:01,200
And is it doing a good job at that?

117
00:05:01,200 --> 00:05:03,530
That's really a high level question.

118
00:05:03,530 --> 00:05:05,969
There's some heavier
questions I'll wanna answer

119
00:05:05,970 --> 00:05:09,110
to kind of dig into that space.

120
00:05:09,110 --> 00:05:10,940
But it's also the idea of, you know,

121
00:05:10,940 --> 00:05:13,730
this cybersecurity tool
could have an AI component

122
00:05:13,730 --> 00:05:16,560
but is that AI component really doing

123
00:05:16,560 --> 00:05:19,970
any of the cyber security functionality?

124
00:05:19,970 --> 00:05:23,320
If I'm being told their
cybersecurity tool is AI powered

125
00:05:23,320 --> 00:05:25,814
I would hope that it is, but is it?

126
00:05:25,814 --> 00:05:27,390
And there's also the idea of cost,

127
00:05:27,390 --> 00:05:29,183
and cost will immediately get to the idea,

128
00:05:29,183 --> 00:05:32,450
I think jumped to finances, right?

129
00:05:32,450 --> 00:05:34,849
And financial is a huge
impact that could be there.

130
00:05:34,850 --> 00:05:36,120
I think about my last slide,

131
00:05:36,120 --> 00:05:39,310
there's a cost when it comes
to cyber security itself.

132
00:05:39,310 --> 00:05:41,420
Is it possible that this AI component

133
00:05:41,420 --> 00:05:45,500
is really introducing
vulnerabilities or security issues

134
00:05:45,500 --> 00:05:47,063
into my enterprise?

135
00:05:48,000 --> 00:05:49,810
Okay so, the first slide I took you

136
00:05:49,810 --> 00:05:52,800
kind of on a mental field
trip onto the expo floor

137
00:05:52,800 --> 00:05:56,710
now I wanna take you on a
mental trip like back in time.

138
00:05:56,710 --> 00:05:57,543
Next.

139
00:05:59,180 --> 00:06:02,730
So, rewind time five, 10, 15 years ago,

140
00:06:02,730 --> 00:06:04,470
cybersecurity is advances.

141
00:06:04,470 --> 00:06:06,040
Again, and we have these questions

142
00:06:06,040 --> 00:06:09,570
that are kind of coming up
along the side of cybersecurity.

143
00:06:09,570 --> 00:06:13,640
The first one we could think
of was, or as an example,

144
00:06:13,640 --> 00:06:18,099
do we understand adversary
attacks and techniques, right?

145
00:06:18,100 --> 00:06:20,010
If we could really understand adversaries,

146
00:06:20,010 --> 00:06:22,700
tax and techniques, we
could really as a result

147
00:06:22,700 --> 00:06:24,890
have better cyber security.

148
00:06:24,890 --> 00:06:26,880
So a lot of research went into this,

149
00:06:26,880 --> 00:06:29,280
a big initiative went
into this, but as a result

150
00:06:29,280 --> 00:06:31,462
we came out with a tool such as Attack.

151
00:06:32,570 --> 00:06:35,480
On a similar note, we can
approach cyber security

152
00:06:35,480 --> 00:06:36,620
with the question of,

153
00:06:36,620 --> 00:06:40,510
is there a way that I could
make cybersecurity decisions

154
00:06:40,510 --> 00:06:42,810
but through risk management approach?

155
00:06:42,810 --> 00:06:44,890
And again, a lot of research went on here.

156
00:06:44,890 --> 00:06:47,500
A lot of community involvement went on,

157
00:06:47,500 --> 00:06:49,950
but NIST came out with a
cybersecurity framework

158
00:06:49,950 --> 00:06:52,289
to address those concerns.

159
00:06:52,290 --> 00:06:54,550
In more recent years
we've had the questions of

160
00:06:54,550 --> 00:06:57,660
can we capture and organize what we learn

161
00:06:57,660 --> 00:07:00,480
about active defense
and adversary engagement

162
00:07:00,480 --> 00:07:01,880
for better defense?

163
00:07:01,880 --> 00:07:04,340
Again, this is another huge
research initiative, right?

164
00:07:04,340 --> 00:07:07,260
But it resulted in
something like MITRE shield.

165
00:07:07,260 --> 00:07:09,430
And then taking that idea
of the NIST cybersecurity

166
00:07:09,430 --> 00:07:12,340
framework and pivoting
into the idea of privacy,

167
00:07:12,340 --> 00:07:14,830
we could also talk about
a risk management approach

168
00:07:14,830 --> 00:07:16,530
but addressing the privacy space.

169
00:07:16,530 --> 00:07:19,330
And we have a framework like
the NIST privacy framework.

170
00:07:20,180 --> 00:07:22,360
Again, we had these
questions kind of paralleling

171
00:07:22,360 --> 00:07:23,950
the world of cybersecurity.

172
00:07:23,950 --> 00:07:26,710
And as a result of putting
research in the initiative

173
00:07:26,710 --> 00:07:28,549
into it, we came out with frameworks

174
00:07:28,550 --> 00:07:31,713
or tools or resources to help
us address those questions.

175
00:07:32,590 --> 00:07:33,423
Next.

176
00:07:35,700 --> 00:07:38,990
So at the National Cybersecurity,
Federally Funded Research

177
00:07:38,990 --> 00:07:42,400
and Development Center,
which is operated by MITRE,

178
00:07:42,400 --> 00:07:44,390
we've started on a research initiative

179
00:07:44,390 --> 00:07:46,580
to really kind of address those questions

180
00:07:46,580 --> 00:07:50,750
about the AI component
of a cybersecurity tool.

181
00:07:50,750 --> 00:07:52,900
Our goal in this research
initiative is really

182
00:07:52,900 --> 00:07:56,409
to offer a better understanding
of the AI component.

183
00:07:56,410 --> 00:07:58,810
We wanna be able to
jump into that dialogue

184
00:07:58,810 --> 00:08:02,510
with cybersecurity vendors and
understand the cybersecurity

185
00:08:02,510 --> 00:08:04,570
but also start to get into

186
00:08:04,570 --> 00:08:07,810
what's going on with the
AI component as well.

187
00:08:07,810 --> 00:08:10,200
The way we've addressed or approached

188
00:08:10,200 --> 00:08:13,150
this research initiative
is we've started off

189
00:08:13,150 --> 00:08:14,650
focusing on cybersecurity.

190
00:08:14,650 --> 00:08:18,830
So something that's a cybersecurity
tool that's AI enabled.

191
00:08:18,830 --> 00:08:21,830
But in reality, the way
we're approaching it is that

192
00:08:21,830 --> 00:08:23,479
this framework that we're talking about

193
00:08:23,480 --> 00:08:25,550
could be used in other dimensions as well.

194
00:08:25,550 --> 00:08:30,000
It could be used in health IT
systems that are AI powered.

195
00:08:30,000 --> 00:08:32,490
And again, back to the
beginning, we wanna make sure

196
00:08:32,490 --> 00:08:34,850
that the way that we're
asking these questions,

197
00:08:34,850 --> 00:08:36,210
we're keeping with that dialogue

198
00:08:36,210 --> 00:08:37,760
in which a vendor would be comfortable

199
00:08:37,760 --> 00:08:39,510
in helping and answering it.

200
00:08:39,510 --> 00:08:40,980
So we're not interested in getting

201
00:08:40,980 --> 00:08:42,710
into intellectual property

202
00:08:42,710 --> 00:08:46,910
or doing any type of reveal
of proprietary development.

203
00:08:46,910 --> 00:08:49,430
Again, the goal is really to
continue to help facilitate

204
00:08:49,430 --> 00:08:51,489
that better dialogue between the vendor

205
00:08:51,490 --> 00:08:54,230
and the potential tool
adopter for both sides

206
00:08:54,230 --> 00:08:56,930
to when the really
gonna understand what is

207
00:08:56,930 --> 00:09:00,910
the tool adopter interested
in, what are their concerns?

208
00:09:00,910 --> 00:09:03,370
And then the tool adopter
themselves can really

209
00:09:03,370 --> 00:09:05,030
understand what are they getting,

210
00:09:05,030 --> 00:09:08,150
should they decide to adopt
something that's AI enabled

211
00:09:08,150 --> 00:09:10,160
into their enterprise?

212
00:09:10,160 --> 00:09:12,819
So ultimately our goal is to really assist

213
00:09:12,820 --> 00:09:15,980
in developing a tool that
an organization can use

214
00:09:15,980 --> 00:09:17,720
to really start to understand,

215
00:09:17,720 --> 00:09:20,400
is something re really AI enabled

216
00:09:20,400 --> 00:09:22,449
or is it really just hype?

217
00:09:22,450 --> 00:09:25,010
I'm gonna go ahead and turn
it over to Mike at this point.

218
00:09:25,010 --> 00:09:27,450
And he's gonna jump
into talking a lot more

219
00:09:27,450 --> 00:09:30,140
about the framework that
we're looking to develop.

220
00:09:30,140 --> 00:09:32,020
But along the way, if you're interested

221
00:09:32,020 --> 00:09:34,960
in asking us any questions,
we do have the chat running.

222
00:09:34,960 --> 00:09:37,430
You're welcome to drop
any questions in the chat

223
00:09:37,430 --> 00:09:39,829
as the presentation proceeds.

224
00:09:39,830 --> 00:09:40,913
Mike, over to you.

225
00:09:42,940 --> 00:09:43,840
- Thank you, Anne.

226
00:09:45,370 --> 00:09:48,320
So now I'm gonna present our RX framework.

227
00:09:48,320 --> 00:09:49,570
RX of course stands for

228
00:09:49,570 --> 00:09:53,200
AI relevance, competence, cost score.

229
00:09:53,200 --> 00:09:55,600
And this is the tool that
we're proposing today.

230
00:09:56,700 --> 00:09:59,620
The goal of RX is really
to develop a methodology

231
00:09:59,620 --> 00:10:04,620
for assessing an AI enabled product, okay?

232
00:10:04,820 --> 00:10:08,430
Framework is organized around
a structured set of questions,

233
00:10:08,430 --> 00:10:09,949
answers to these questions,

234
00:10:09,950 --> 00:10:12,300
provide a score and assessment score

235
00:10:12,300 --> 00:10:14,963
and guidance for an acquisition decision.

236
00:10:16,090 --> 00:10:17,860
So as we thought about this, we realized

237
00:10:17,860 --> 00:10:20,690
that there are essentially
three questions that

238
00:10:20,690 --> 00:10:25,550
sort of drive this, you
know, is AI necessary,

239
00:10:25,550 --> 00:10:29,579
is it functional, is it cost-effective

240
00:10:29,580 --> 00:10:33,280
in this particular tool,
in this particular case?

241
00:10:33,280 --> 00:10:35,370
And these three questions kind of lead us

242
00:10:35,370 --> 00:10:37,220
to three dimensions.

243
00:10:37,220 --> 00:10:39,717
So first of all, we think about, you know,

244
00:10:41,290 --> 00:10:44,223
how necessary and appropriate
is the AI in this component?

245
00:10:45,150 --> 00:10:47,720
And that's the relevance dimension.

246
00:10:47,720 --> 00:10:50,620
We know that sometimes
you can have AI that's,

247
00:10:50,620 --> 00:10:55,020
it's there but it's not
really crucial to the system.

248
00:10:55,020 --> 00:10:57,113
So is this a good place for AI or not?

249
00:10:57,950 --> 00:11:00,550
We're also interested in the competence.

250
00:11:00,550 --> 00:11:02,479
How well does it do what it claims?

251
00:11:02,480 --> 00:11:04,450
That's our second dimension.

252
00:11:04,450 --> 00:11:08,250
And our third dimension really
is the cost benefit analysis.

253
00:11:08,250 --> 00:11:10,720
So we have relevance, competence and cost

254
00:11:10,720 --> 00:11:13,340
as there three dimensions of interest.

255
00:11:13,340 --> 00:11:17,350
Clearly there are a lot of
other ways of assessing systems.

256
00:11:17,350 --> 00:11:18,700
These aren't all the questions,

257
00:11:18,700 --> 00:11:21,163
or all the ways of assessing an AI system.

258
00:11:22,000 --> 00:11:23,460
but these are the ones that focus

259
00:11:23,460 --> 00:11:26,730
on an initial screening for
the acquisition process.

260
00:11:26,730 --> 00:11:28,990
If you're really
considering adopting a tool

261
00:11:28,990 --> 00:11:30,790
but you've got to start somewhere,

262
00:11:30,790 --> 00:11:33,790
this framework provides a set of questions

263
00:11:33,790 --> 00:11:38,790
that will help you kickstart
that assessment process, okay?

264
00:11:38,930 --> 00:11:41,699
The goal is to be accessible.

265
00:11:41,700 --> 00:11:43,790
We're not relying on proprietary

266
00:11:43,790 --> 00:11:45,810
or deep technical knowledge.

267
00:11:45,810 --> 00:11:47,420
We're looking for accessibility

268
00:11:47,420 --> 00:11:49,762
and a quick screening tool to get started.

269
00:11:53,090 --> 00:11:55,090
So I'm gonna address
now the three dimensions

270
00:11:55,090 --> 00:11:59,270
and the first dimension of
these is relevance, okay?

271
00:11:59,270 --> 00:12:03,000
So is AI the right tool
to solve this problem?

272
00:12:03,000 --> 00:12:05,313
And is it the right kind of AI in fact?

273
00:12:07,170 --> 00:12:10,689
If you're using machine learning
system, you need to know

274
00:12:10,690 --> 00:12:13,820
is there relevant data
to support this tool?

275
00:12:13,820 --> 00:12:15,540
If you're doing an expert system

276
00:12:15,540 --> 00:12:18,360
is it in a field where sufficient

277
00:12:18,360 --> 00:12:20,760
subject matter expertise exists?

278
00:12:20,760 --> 00:12:22,160
And is the data that you have,

279
00:12:22,160 --> 00:12:23,760
is it the right data?

280
00:12:23,760 --> 00:12:25,990
All these things will help you understand

281
00:12:25,990 --> 00:12:28,000
if AI is the right tool.

282
00:12:28,000 --> 00:12:32,303
And if it's the right kind
of AI for this application.

283
00:12:34,470 --> 00:12:37,000
Centrality is another
place we're looking at.

284
00:12:37,000 --> 00:12:40,780
So is the AI central
insignificant to the system?

285
00:12:40,780 --> 00:12:44,410
Imagine an AI component that fails.

286
00:12:44,410 --> 00:12:46,490
If it fails does the entire tool fail?

287
00:12:46,490 --> 00:12:49,500
And in which case it was
probably pretty central.

288
00:12:49,500 --> 00:12:54,000
If the AI fails and the tool
is still quite functional,

289
00:12:54,000 --> 00:12:54,833
then you say, okay,

290
00:12:54,833 --> 00:12:57,300
it was not the particularly
central to the system.

291
00:12:58,630 --> 00:13:00,080
So we're looking at how much

292
00:13:00,080 --> 00:13:03,120
of the overall
functionality depends on AI.

293
00:13:03,120 --> 00:13:06,820
Think about an AI driven
chat bot, for example

294
00:13:06,820 --> 00:13:09,000
does natural language processing.

295
00:13:09,000 --> 00:13:13,200
It understands the users
questions in an interface

296
00:13:13,200 --> 00:13:14,640
but all it's doing perhaps is

297
00:13:14,640 --> 00:13:17,080
pulling up frequently asked questions.

298
00:13:17,080 --> 00:13:20,480
If that's the case, it's
not really bringing a lot

299
00:13:20,480 --> 00:13:25,223
of extra functionality or extra value

300
00:13:25,223 --> 00:13:28,750
compared to just having a
frequently asked questions list.

301
00:13:28,750 --> 00:13:31,150
And finally, for relevance,
we look at the AI.

302
00:13:31,150 --> 00:13:32,810
Is the AI necessary

303
00:13:32,810 --> 00:13:35,380
and is it necessary at the right level

304
00:13:35,380 --> 00:13:37,080
that's been implemented?

305
00:13:37,080 --> 00:13:40,660
Sometimes AI can just
speak to it as you know,

306
00:13:40,660 --> 00:13:43,600
for example, did you need the chat box,

307
00:13:43,600 --> 00:13:46,710
the chat bot or is it gratuitous?

308
00:13:46,710 --> 00:13:50,090
Consider a situation you
know, in this trivial example,

309
00:13:50,090 --> 00:13:51,910
if I need the floor swept

310
00:13:51,910 --> 00:13:56,850
I can put in a floor sweeping
robot as an illustration

311
00:13:56,850 --> 00:14:00,430
or I can go full on
with a walking, talking

312
00:14:00,430 --> 00:14:02,193
automatic robotic made.

313
00:14:04,760 --> 00:14:07,430
Clearly there are various levels of AI

314
00:14:07,430 --> 00:14:08,870
and you need to choose the one that's

315
00:14:08,870 --> 00:14:10,443
appropriate to the project.

316
00:14:11,490 --> 00:14:15,040
You don't want to get more
or less than you need,

317
00:14:15,040 --> 00:14:16,173
no more, no less.

318
00:14:19,550 --> 00:14:22,372
So after relevance, we look at competence.

319
00:14:23,260 --> 00:14:24,900
Measuring competence of course,

320
00:14:24,900 --> 00:14:26,740
when you don't have a lot of information,

321
00:14:26,740 --> 00:14:30,140
you don't have insight into
the system is challenging.

322
00:14:30,140 --> 00:14:33,260
Usually one thinks about
accuracy, precision recall

323
00:14:33,260 --> 00:14:34,810
and other such metrics.

324
00:14:34,810 --> 00:14:36,790
But in this case, when
we don't have that kind

325
00:14:36,790 --> 00:14:39,250
of information, we're interested in more

326
00:14:39,250 --> 00:14:41,810
in the comparative
performance of the system.

327
00:14:41,810 --> 00:14:44,020
Comparisons to industry standards

328
00:14:44,020 --> 00:14:46,610
or typical industry performance.

329
00:14:46,610 --> 00:14:48,700
Various fields have different metrics

330
00:14:48,700 --> 00:14:51,960
so you know, you have to
choose the right metric

331
00:14:51,960 --> 00:14:53,710
for the field you're interested in.

332
00:14:54,780 --> 00:14:58,970
But without looking at the
inside of the code itself

333
00:14:58,970 --> 00:15:01,890
all we can do is look for
comparative performance.

334
00:15:01,890 --> 00:15:03,040
And that's our standard

335
00:15:04,250 --> 00:15:08,283
for understanding performance
at the competence dimension.

336
00:15:09,740 --> 00:15:13,523
So when you take a system
out of a lab, however,

337
00:15:14,632 --> 00:15:17,729
whatever benchmarks or
scores you got in the lab

338
00:15:17,730 --> 00:15:20,450
may be different in the real world.

339
00:15:20,450 --> 00:15:22,640
What we really need to think about is,

340
00:15:22,640 --> 00:15:26,319
when the operational issues
you deal with when you bring

341
00:15:26,320 --> 00:15:29,790
something into operational
context in the real world.

342
00:15:29,790 --> 00:15:33,120
So for example, things like a model drift

343
00:15:33,120 --> 00:15:35,990
and retraining requirements.

344
00:15:35,990 --> 00:15:39,980
It does happen that a system
that's uses machine learning

345
00:15:39,980 --> 00:15:41,803
that was trained on one set of data,

346
00:15:43,190 --> 00:15:44,940
starts performing well in the beginning,

347
00:15:44,940 --> 00:15:47,453
and as that data changes for some reason,

348
00:15:48,470 --> 00:15:51,350
the model no longer performs accurately.

349
00:15:51,350 --> 00:15:55,320
So does the system that
you're considering,

350
00:15:55,320 --> 00:15:59,170
does that system accommodate
for that kind of drift?

351
00:15:59,170 --> 00:16:02,510
And if it does, does it
require retraining or not?

352
00:16:02,510 --> 00:16:04,490
So these are the issues that come about

353
00:16:04,490 --> 00:16:07,520
when you bring the system
into the real world.

354
00:16:07,520 --> 00:16:09,333
And finally transparency.

355
00:16:10,370 --> 00:16:13,470
We wanted to know if an AI
system provides the transparency

356
00:16:13,470 --> 00:16:15,723
and explainability into its output.

357
00:16:16,620 --> 00:16:19,740
So that, you know, the
system has to reveal behavior

358
00:16:19,740 --> 00:16:21,700
in a manner that lets you monitor

359
00:16:21,700 --> 00:16:23,563
and improve it's performance.

360
00:16:25,300 --> 00:16:27,420
These are the three areas of competence

361
00:16:27,420 --> 00:16:30,430
that we're gonna look
at within the framework.

362
00:16:30,430 --> 00:16:32,890
So after relevance and competence,

363
00:16:32,890 --> 00:16:36,490
we understand that the
AI belongs where it is.

364
00:16:36,490 --> 00:16:39,340
It's doing hopefully a good job at it,

365
00:16:39,340 --> 00:16:41,480
but finally there's one last consideration

366
00:16:41,480 --> 00:16:43,620
and that is the cost, okay?

367
00:16:43,620 --> 00:16:46,713
And costs obviously is more
than dollars and cents.

368
00:16:47,810 --> 00:16:51,709
Integrating a system involves
tuning or specialization.

369
00:16:51,710 --> 00:16:53,330
Think about the case where you have

370
00:16:53,330 --> 00:16:55,730
a natural language processing system.

371
00:16:55,730 --> 00:16:58,800
It does understanding,
but it's been trained on,

372
00:16:58,800 --> 00:17:01,709
say data scraped off the web, okay?

373
00:17:01,710 --> 00:17:05,190
It's gonna have a very general ability

374
00:17:05,190 --> 00:17:08,280
to understand the English
language for example.

375
00:17:08,280 --> 00:17:11,153
Now bring that into your operation

376
00:17:13,569 --> 00:17:16,020
where you have jargon, okay?

377
00:17:16,020 --> 00:17:19,129
It's no longer gonna be
functioning the same way.

378
00:17:19,130 --> 00:17:21,930
So if you bring it into something
like air traffic control

379
00:17:21,930 --> 00:17:25,690
or even cybersecurity, that
might have a lot of jargon,

380
00:17:25,690 --> 00:17:27,569
the system may no longer understand

381
00:17:27,569 --> 00:17:29,143
and perform as advertised.

382
00:17:32,370 --> 00:17:35,040
AI may also increase the
accuracy of your system,

383
00:17:35,040 --> 00:17:36,450
which sounds really good.

384
00:17:36,450 --> 00:17:40,440
But that increase accuracy
may come at a cost

385
00:17:40,440 --> 00:17:42,140
of increased processing time

386
00:17:42,140 --> 00:17:44,750
or increase memory or other resources.

387
00:17:44,750 --> 00:17:47,890
So there's a cost benefit
analysis there as well.

388
00:17:47,890 --> 00:17:51,373
Did the increased accuracy,
slow down your system?

389
00:17:53,080 --> 00:17:56,220
So finally, we're
interested in the security,

390
00:17:56,220 --> 00:17:58,700
particularly in this context.

391
00:17:58,700 --> 00:18:01,700
Understanding that adding AI may

392
00:18:01,700 --> 00:18:04,480
introduce unmitigated vulnerabilities.

393
00:18:04,480 --> 00:18:06,880
Anne gave a nice description earlier

394
00:18:06,880 --> 00:18:10,440
of how adversarial machine
learning can be used

395
00:18:10,440 --> 00:18:12,373
to attacks an AI system.

396
00:18:13,260 --> 00:18:14,830
In fact adversarial machine learning

397
00:18:14,830 --> 00:18:16,699
is increasingly democratized.

398
00:18:16,700 --> 00:18:19,920
It's in the hands of anybody
who is interested now.

399
00:18:19,920 --> 00:18:24,010
There are well-documented
attack modes, libraries

400
00:18:24,010 --> 00:18:27,190
and software, and so
on that make it easier

401
00:18:27,190 --> 00:18:31,090
for adversaries to attack
systems in one way or the other.

402
00:18:31,090 --> 00:18:33,439
Many machine learning
systems now are trained

403
00:18:33,440 --> 00:18:35,800
on publicly available data sets.

404
00:18:35,800 --> 00:18:39,149
And those data sets might be
corrupted by an adversary.

405
00:18:39,150 --> 00:18:42,270
Poisoned is the term we use.

406
00:18:42,270 --> 00:18:45,660
And if that poisoned data
gets into the training system

407
00:18:45,660 --> 00:18:49,950
in the trained model, then you
have another vulnerability.

408
00:18:49,950 --> 00:18:52,030
So understanding that, you know

409
00:18:52,030 --> 00:18:54,870
to get the advantages
of AI, you also have to

410
00:18:54,870 --> 00:18:56,729
consider the vulnerabilities
that you're getting

411
00:18:56,730 --> 00:18:57,693
at the same time.

412
00:19:02,860 --> 00:19:04,959
So it turns out there's not usually enough

413
00:19:04,960 --> 00:19:08,620
information available to give
really definitive answers

414
00:19:08,620 --> 00:19:11,530
to some of these questions, okay?

415
00:19:11,530 --> 00:19:12,870
The question is of relevance,

416
00:19:12,870 --> 00:19:16,689
competence and costs are important.

417
00:19:16,690 --> 00:19:19,180
We try to get as much
information as we can

418
00:19:19,180 --> 00:19:22,030
but sometimes the information
isn't always there

419
00:19:22,030 --> 00:19:25,763
or it's not in necessarily
trustworthy sources.

420
00:19:27,150 --> 00:19:28,970
So how do we deal with that?

421
00:19:28,970 --> 00:19:32,780
But what we do is we
make our best estimate

422
00:19:32,780 --> 00:19:36,000
and then we wait the result,
according to our confidence.

423
00:19:36,000 --> 00:19:38,170
And we measure our confidence
according to the quality

424
00:19:38,170 --> 00:19:40,260
and the quantity of the
supporting information,

425
00:19:40,260 --> 00:19:42,470
the supporting materials.

426
00:19:42,470 --> 00:19:44,410
So if you consider
something like transparency

427
00:19:44,410 --> 00:19:46,840
into the model and the supporting data,

428
00:19:46,840 --> 00:19:49,720
this is something that we
can look at and understand

429
00:19:49,720 --> 00:19:51,770
a useful source of that
kind of information

430
00:19:51,770 --> 00:19:54,970
is even something called
model cards and data sheets.

431
00:19:54,970 --> 00:19:58,010
These were developed that Google and

432
00:20:00,497 --> 00:20:02,940
provide documentation about the model

433
00:20:02,940 --> 00:20:05,730
where it's appropriate to
be used, how it was trained,

434
00:20:05,730 --> 00:20:10,320
what was trained on, what
biases or flaws it might have.

435
00:20:10,320 --> 00:20:15,320
And data sheets, documented data sets,

436
00:20:15,320 --> 00:20:18,379
how they were developed, what
they're appropriately used for

437
00:20:18,380 --> 00:20:22,370
what pre-processing of
de-identification has been done.

438
00:20:22,370 --> 00:20:25,290
So there is information
out there, or they could be

439
00:20:25,290 --> 00:20:28,733
that provides that kind
of needed transparency.

440
00:20:30,210 --> 00:20:31,490
Publications and patents,

441
00:20:31,490 --> 00:20:34,460
especially peer reviewed publications

442
00:20:34,460 --> 00:20:37,540
are fairly trustworthy
sources of information.

443
00:20:37,540 --> 00:20:40,750
And if those are available,
those can give you

444
00:20:40,750 --> 00:20:43,660
the answers you need with a
high degree of confidence.

445
00:20:43,660 --> 00:20:45,670
White papers and publicity materials,

446
00:20:45,670 --> 00:20:47,630
of course, if they're not peer reviewed

447
00:20:51,350 --> 00:20:52,969
you may have less confidence in those,

448
00:20:52,970 --> 00:20:55,940
although they may also
provide some information.

449
00:20:55,940 --> 00:20:58,780
So we take all this into
account to understand

450
00:20:58,780 --> 00:21:01,010
exactly how confident
we are in the answers

451
00:21:01,010 --> 00:21:05,103
that we generated for the
three dimensions of trust.

452
00:21:07,800 --> 00:21:10,210
So once we consider all this,

453
00:21:10,210 --> 00:21:13,570
we have relevance, competence and cost.

454
00:21:13,570 --> 00:21:15,899
We aggregate this all and weighted

455
00:21:15,900 --> 00:21:17,900
by the confidence in the answers we got.

456
00:21:19,570 --> 00:21:21,562
And we end up with the RX tool.

457
00:21:22,860 --> 00:21:25,023
So what's in the box in the RX tool?

458
00:21:25,920 --> 00:21:29,910
RX provides a hierarchically
organized list of questions.

459
00:21:29,910 --> 00:21:32,560
So in the upper left there,
you see the questionnaire

460
00:21:33,420 --> 00:21:35,993
stored in a spreadsheet at this time.

461
00:21:36,950 --> 00:21:39,520
These questions are really the questions

462
00:21:39,520 --> 00:21:43,483
that reflect the features
that the tool is assessing.

463
00:21:44,860 --> 00:21:47,250
You also get a scoring
system that tells you

464
00:21:47,250 --> 00:21:51,010
for each answer, how it's scored,

465
00:21:51,010 --> 00:21:52,483
what value to assign to it.

466
00:21:53,540 --> 00:21:56,300
The inference method is the part that

467
00:21:56,300 --> 00:21:58,770
takes all those scores and aggregates them

468
00:21:58,770 --> 00:22:00,283
to give you a final score.

469
00:22:01,830 --> 00:22:05,730
And last and certainly not
least, because of its importance

470
00:22:05,730 --> 00:22:10,730
the guidance, to how to use the system,

471
00:22:11,260 --> 00:22:14,060
what questions to ask,
how to do the scoring,

472
00:22:14,060 --> 00:22:16,483
but also and very importantly,

473
00:22:17,690 --> 00:22:20,380
if you don't have the answers you need

474
00:22:20,380 --> 00:22:22,510
to answer these questions,

475
00:22:22,510 --> 00:22:24,500
guidance is what tells you

476
00:22:24,500 --> 00:22:26,713
these are the questions to ask the vendor.

477
00:22:28,200 --> 00:22:31,603
So if you're considering acquiring a tool,

478
00:22:32,700 --> 00:22:35,670
you have now questions to answer.

479
00:22:35,670 --> 00:22:37,440
When you can't answer them, you can go

480
00:22:37,440 --> 00:22:40,000
with those questions in
hand to the vendor and say,

481
00:22:40,000 --> 00:22:42,120
this is the information that I need

482
00:22:42,120 --> 00:22:45,632
in order to make a purchasing
or acquisition decision, okay?

483
00:22:51,370 --> 00:22:56,030
So to wrap up, we've
provided a tool that lets you

484
00:22:56,030 --> 00:23:00,720
in a consistent way, assess the system

485
00:23:00,720 --> 00:23:02,890
and provide a score that you can use

486
00:23:02,890 --> 00:23:04,290
to compare to other systems.

487
00:23:04,290 --> 00:23:06,600
It's important, very important.

488
00:23:06,600 --> 00:23:10,879
Nobody wants to end up
disappointed and regretting

489
00:23:10,880 --> 00:23:12,920
an expensive acquisition.

490
00:23:12,920 --> 00:23:17,670
So by using something
like our RX tool, you know

491
00:23:17,670 --> 00:23:21,290
you can start to understand
if you got what you paid for

492
00:23:21,290 --> 00:23:23,090
and if you're getting what you need.

493
00:23:28,041 --> 00:23:30,040
So finally, yeah.

494
00:23:30,040 --> 00:23:31,810
What have you learned today?

495
00:23:31,810 --> 00:23:34,050
First of all, it's important
to take a structure

496
00:23:34,050 --> 00:23:37,629
to post assessing AI
enabled tool acquisitions.

497
00:23:37,630 --> 00:23:40,360
And then you have to be able
to ask the right questions

498
00:23:40,360 --> 00:23:41,709
and ask them consistently

499
00:23:41,710 --> 00:23:44,350
across all tools under consideration.

500
00:23:44,350 --> 00:23:46,560
And to be able then to compare those tools

501
00:23:46,560 --> 00:23:48,460
within that common framework.

502
00:23:48,460 --> 00:23:51,220
Not every AI enabled claim is equivalent.

503
00:23:51,220 --> 00:23:53,620
And if you have the same metric to use

504
00:23:53,620 --> 00:23:57,409
against all AI claims, you'll
be in a better position

505
00:23:57,410 --> 00:23:59,553
for making good acquisitions.

506
00:24:01,670 --> 00:24:04,610
Longer term down the road purchasers,

507
00:24:04,610 --> 00:24:08,020
if you are looking to
acquire tools, you know

508
00:24:08,020 --> 00:24:12,310
this RX tool will help you to
actually make that assessment.

509
00:24:12,310 --> 00:24:14,520
So you should be able to learn how to use

510
00:24:14,520 --> 00:24:18,290
and apply this tool and
your feedback in fact,

511
00:24:18,290 --> 00:24:19,693
will help improve that tool.

512
00:24:21,040 --> 00:24:24,270
Vendors, product developers, if you use RX

513
00:24:24,270 --> 00:24:28,139
you can see how your tool
rates, how your product rates.

514
00:24:28,140 --> 00:24:30,600
And in fact, you can also look and see

515
00:24:31,900 --> 00:24:34,070
the results of your assessment can drive

516
00:24:34,070 --> 00:24:35,870
your public documentation.

517
00:24:35,870 --> 00:24:39,560
If you see that you haven't
provided the information yet

518
00:24:39,560 --> 00:24:42,460
to the public that helps
them make their decision,

519
00:24:42,460 --> 00:24:47,113
Maybe this will drive a
better documentation for you.

520
00:24:48,290 --> 00:24:50,050
So we are really excited to get this

521
00:24:50,050 --> 00:24:51,653
out into people's hands.

522
00:24:52,870 --> 00:24:55,169
If you are interested in working with us,

523
00:24:55,170 --> 00:24:59,840
please reach out if you're
a vendor and you want us

524
00:24:59,840 --> 00:25:00,899
to take a look at your tool

525
00:25:00,900 --> 00:25:03,460
or work with us to help improve your tool.

526
00:25:03,460 --> 00:25:07,100
If you are looking to
acquire a tool in general

527
00:25:07,100 --> 00:25:09,510
and you want us to help you out,

528
00:25:09,510 --> 00:25:12,950
please feel free to contact
us at the email on the screen.

529
00:25:12,950 --> 00:25:16,120
We're really excited to work
with the public on this.

530
00:25:16,120 --> 00:25:18,820
Thank you very much for your
time and attention today.

