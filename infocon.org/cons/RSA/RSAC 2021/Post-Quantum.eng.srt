1
00:00:08,350 --> 00:00:10,730
- Hello, my name is Thomas Prest,

2
00:00:10,730 --> 00:00:14,697
and this is joint work with
James Howe and Daniel Apron.

3
00:00:14,697 --> 00:00:17,619
And James and I will talk to you about:

4
00:00:17,620 --> 00:00:21,433
How (not) to design and implement
post-quantum cryptography.

5
00:00:22,500 --> 00:00:25,210
So, post-quantum
cryptography is a vast field

6
00:00:25,210 --> 00:00:26,900
with thousands of papers.

7
00:00:26,900 --> 00:00:28,720
It is also a very diverse field,

8
00:00:28,720 --> 00:00:31,610
because you can build
post-quantum cryptographic schemes

9
00:00:31,610 --> 00:00:36,013
from lattices, codes, isogenies,
hash functions and so on.

10
00:00:37,200 --> 00:00:41,310
And there are already many
good surveys out there,

11
00:00:41,310 --> 00:00:44,120
but they usually focus on one family

12
00:00:44,120 --> 00:00:46,860
or one aspect of one family.

13
00:00:46,860 --> 00:00:50,250
And our goal is not to
subsume existing works,

14
00:00:50,250 --> 00:00:52,070
and this would be infeasible.

15
00:00:52,070 --> 00:00:54,580
Our goal is rather to
complement these works.

16
00:00:54,580 --> 00:00:58,278
And the way we hope to achieve that is by

17
00:00:58,278 --> 00:01:03,278
realizing the transversal
approach in the sense that,

18
00:01:03,470 --> 00:01:08,140
instead of focus on each family,
we abstract away the family

19
00:01:08,140 --> 00:01:10,620
and we focus on the fourth process

20
00:01:10,620 --> 00:01:13,770
which is formulating an assumption,

21
00:01:13,770 --> 00:01:15,810
designing your cryptographic scheme,

22
00:01:15,810 --> 00:01:19,243
implementing the scheme
and deploying the scheme.

23
00:01:21,950 --> 00:01:26,120
And this talk for time constraints

24
00:01:26,120 --> 00:01:27,800
will only focus on two steps,

25
00:01:27,800 --> 00:01:30,320
which is defining your
scheme and implementing them.

26
00:01:30,320 --> 00:01:32,350
But then in the CT-RSA paper,

27
00:01:32,350 --> 00:01:34,419
we also discuss the assumptions,

28
00:01:34,420 --> 00:01:38,263
and in the e-print, we
discuss all four steps.

29
00:01:39,240 --> 00:01:42,759
And we believe that this approach allows

30
00:01:42,760 --> 00:01:45,650
to highlight trends and patterns,

31
00:01:45,650 --> 00:01:49,360
and also common methodologies,
common paradigms,

32
00:01:49,360 --> 00:01:50,960
and common threats.

33
00:01:50,960 --> 00:01:53,039
We hope it make it easier to navigate

34
00:01:53,040 --> 00:01:55,703
the front times daunting amount of work

35
00:01:55,703 --> 00:01:58,000
that is published on the subject.

36
00:01:58,000 --> 00:01:59,580
And we also hope that it allows

37
00:01:59,580 --> 00:02:02,810
to foster reusability of ideas,

38
00:02:02,810 --> 00:02:05,250
so that if you see a paper in a domain

39
00:02:05,250 --> 00:02:08,699
which is not the domain
you would usually work on,

40
00:02:08,699 --> 00:02:11,130
then you can identify
what are the core ideas,

41
00:02:11,130 --> 00:02:15,222
and hopefully reuse this
idea in your own domain.

42
00:02:17,010 --> 00:02:17,843
And...

43
00:02:19,010 --> 00:02:21,530
And yeah, this work is half the survey

44
00:02:21,530 --> 00:02:24,793
and half lesson learned paper.

45
00:02:26,130 --> 00:02:30,743
So first, I will discuss the
theory and design of schemes.

46
00:02:32,370 --> 00:02:35,590
So if you look at classical schemes

47
00:02:35,590 --> 00:02:39,360
for providing signature
and key establishment,

48
00:02:39,360 --> 00:02:44,320
well, all four were, all schemes can be...

49
00:02:44,320 --> 00:02:49,320
Casted as instantiations
of one of four paradigms,

50
00:02:49,430 --> 00:02:54,070
which is either Fiat-Shamir or
Hash-then-Sign for signature,

51
00:02:54,070 --> 00:02:56,430
and Diffie-Hellman or
Public Key Encryption

52
00:02:56,430 --> 00:02:58,130
for key establishment.

53
00:02:58,130 --> 00:02:59,540
And the good news is that,

54
00:02:59,540 --> 00:03:01,500
if you look at post-quantum cryptography,

55
00:03:01,500 --> 00:03:03,930
even though there are much more schemes,

56
00:03:03,930 --> 00:03:07,190
and the landscape can
seem very complicated

57
00:03:07,190 --> 00:03:08,850
if not confusing at times,

58
00:03:08,850 --> 00:03:11,560
then almost all post-quantum schemes,

59
00:03:11,560 --> 00:03:14,820
they can also be casted in
one of these four families.

60
00:03:14,820 --> 00:03:17,577
So by looking at that, one might think,

61
00:03:17,577 --> 00:03:19,460
"Okay, then it's really simple

62
00:03:19,460 --> 00:03:21,150
to design post-quantum cryptography.

63
00:03:21,150 --> 00:03:23,910
You just take one of these four paradigms,

64
00:03:23,910 --> 00:03:26,340
and you apply them to
your favorite problem."

65
00:03:26,340 --> 00:03:30,230
But unfortunately it's not so
simple and the reason is that,

66
00:03:30,230 --> 00:03:33,810
if we try to replicate
previous constructions,

67
00:03:33,810 --> 00:03:36,400
experience has shown that you can end up

68
00:03:36,400 --> 00:03:40,110
with either an inefficient scheme,

69
00:03:40,110 --> 00:03:42,770
a broken assumption or both.

70
00:03:42,770 --> 00:03:46,270
And again, looking at existing works,

71
00:03:46,270 --> 00:03:49,600
the approach which seems the most fruitful

72
00:03:50,860 --> 00:03:55,470
is to tweak the paradigms and the problem

73
00:03:55,470 --> 00:03:57,030
to fit the assumption,

74
00:03:57,030 --> 00:03:59,640
and that the important thing to preserve

75
00:03:59,640 --> 00:04:01,343
is a meaningful security proof.

76
00:04:02,900 --> 00:04:06,600
And in our paper, we highlight
a few examples where,

77
00:04:06,600 --> 00:04:11,600
indeed the latter approach
seems more successful

78
00:04:11,720 --> 00:04:14,020
than the prior approach.

79
00:04:14,020 --> 00:04:15,760
But in this presentation,

80
00:04:15,760 --> 00:04:18,170
I will only focus on one single example.

81
00:04:20,420 --> 00:04:22,460
And this example is the example

82
00:04:22,460 --> 00:04:24,150
of full-domain hash signatures.

83
00:04:24,150 --> 00:04:26,270
So because we are at CT-RSA,

84
00:04:26,270 --> 00:04:29,683
it makes sense to talk to
you about RSA signatures.

85
00:04:29,683 --> 00:04:33,040
And RSA signatures, they can be formalized

86
00:04:34,000 --> 00:04:36,680
as an instantiation of what is called

87
00:04:36,680 --> 00:04:38,450
full-domain hash signature.

88
00:04:38,450 --> 00:04:40,820
So you start with a message

89
00:04:40,820 --> 00:04:42,370
and you pass it into a hash function,

90
00:04:42,370 --> 00:04:44,280
then you obtain the challenge C.

91
00:04:44,280 --> 00:04:46,327
And then what you have is that

92
00:04:46,327 --> 00:04:48,920
you have a pair of functions, F and G.

93
00:04:48,920 --> 00:04:53,920
So G takes as input the
secret key plus the challenge,

94
00:04:54,400 --> 00:04:56,770
and it allows you to
compute the signature.

95
00:04:56,770 --> 00:05:00,530
And then, if you have the
public key from the signature,

96
00:05:00,530 --> 00:05:03,359
you can compute the challenge

97
00:05:03,360 --> 00:05:05,670
and check that it matches
the hash of the message,

98
00:05:05,670 --> 00:05:10,670
and this is how RSA
signatures work for example.

99
00:05:10,710 --> 00:05:15,280
And there are few theoretical
works that tells us that

100
00:05:15,280 --> 00:05:19,289
if F and G define what is
called trapdoor permutation

101
00:05:19,290 --> 00:05:21,830
then the scheme that you obtain,

102
00:05:21,830 --> 00:05:24,193
achieve meaningful security guarantees.

103
00:05:27,290 --> 00:05:29,130
And so a natural question is that,

104
00:05:29,130 --> 00:05:32,920
can we obtain the same for
post-quantum cryptography?

105
00:05:32,920 --> 00:05:34,890
Well, unfortunately it's not as simple,

106
00:05:34,890 --> 00:05:37,289
because we don't know how to realize

107
00:05:39,321 --> 00:05:41,860
trapdoor permutations
in a post-quantum world.

108
00:05:41,860 --> 00:05:43,920
So there is a work of CFS01

109
00:05:44,960 --> 00:05:47,810
that comes close to achieving
such a notion using code,

110
00:05:47,810 --> 00:05:52,810
but unfortunately, it has
a non-scalable parameter,

111
00:05:53,980 --> 00:05:55,400
and then there are two works

112
00:05:55,400 --> 00:05:59,159
that tried initially to
achieve that using lattices,

113
00:05:59,160 --> 00:06:01,850
but it realized a weaker notion,

114
00:06:01,850 --> 00:06:04,550
and the security code
it no longer applied,

115
00:06:04,550 --> 00:06:07,210
and indeed the schemes got broken.

116
00:06:07,210 --> 00:06:10,659
And a solution that was provided in 2008

117
00:06:10,660 --> 00:06:14,050
is the notion of Trapdoor
Preimage Sampleable Functions

118
00:06:14,050 --> 00:06:19,050
or TPSF, and this notion
is just weak enough

119
00:06:19,330 --> 00:06:21,469
that it can be instantiated from lattices,

120
00:06:21,470 --> 00:06:23,200
but it's still strong enough

121
00:06:23,200 --> 00:06:25,500
that it still allows you to achieve

122
00:06:25,500 --> 00:06:26,920
a meaningful security proof.

123
00:06:26,920 --> 00:06:30,033
So it hits just the right balance

124
00:06:30,033 --> 00:06:33,266
between being weak and
being strong enough.

125
00:06:33,266 --> 00:06:37,590
And interestingly
enough, one decade later,

126
00:06:37,590 --> 00:06:40,909
another relaxation called
Average TPSF was provided,

127
00:06:40,910 --> 00:06:42,850
and the good news is that it allows

128
00:06:42,850 --> 00:06:46,600
to have even more efficient
instantiations from lattices.

129
00:06:46,600 --> 00:06:48,670
And in addition, it also allows you

130
00:06:48,670 --> 00:06:51,750
to be instantiated from codes.

131
00:06:51,750 --> 00:06:56,070
And two prominent examples
of schemes derived

132
00:06:56,070 --> 00:06:59,090
from these new paradigms are Falcon

133
00:06:59,090 --> 00:07:01,979
which is based on TPSF and lattices,

134
00:07:01,980 --> 00:07:05,850
and Wave based on ATPSF and codes.

135
00:07:05,850 --> 00:07:09,343
And this concludes my
example and my section.

136
00:07:11,310 --> 00:07:14,280
- Yeah, so the second half
of our paper and our talk

137
00:07:14,280 --> 00:07:17,203
outlines the issues we've seen in the past

138
00:07:17,204 --> 00:07:21,110
with implementing these
post-quantum schemes.

139
00:07:21,110 --> 00:07:23,060
And I think one thing we can say for sure

140
00:07:23,060 --> 00:07:26,620
is that the transition to
post-quantum will be difficult.

141
00:07:26,620 --> 00:07:29,070
And we've seen that
with some of the issues

142
00:07:29,070 --> 00:07:30,219
mentioned in the paper,

143
00:07:31,600 --> 00:07:34,450
and we know that
post-quantum brings many new

144
00:07:34,450 --> 00:07:37,420
and unique challenges to
implementing cryptography.

145
00:07:37,420 --> 00:07:40,270
We see new operations
like gashing sampling,

146
00:07:40,270 --> 00:07:43,120
rejection sampling and
decryption failures,

147
00:07:43,120 --> 00:07:45,570
something that we've not
really seen in the past.

148
00:07:47,350 --> 00:07:49,830
One of the things we observed

149
00:07:49,830 --> 00:07:52,950
is that many initial
post-quantum implementations

150
00:07:52,950 --> 00:07:55,930
failed in some key areas which possibly

151
00:07:55,930 --> 00:07:58,420
wouldn't have happened if
secure coding practices

152
00:07:58,420 --> 00:07:59,780
have been used.

153
00:07:59,780 --> 00:08:03,140
For example, ensuring
that no leakage is present

154
00:08:03,140 --> 00:08:05,719
from data-dependent branching,

155
00:08:05,720 --> 00:08:09,243
which could have been bypassed
by using things like CT Bind.

156
00:08:10,420 --> 00:08:12,310
And we've also seen recent examples

157
00:08:12,310 --> 00:08:16,563
of implementation attacks on
the Fujisaki-Okamoto transform,

158
00:08:17,720 --> 00:08:20,853
which is used in most, if
not all post-quantum schemes.

159
00:08:21,900 --> 00:08:24,150
One of the attacks exploits the use of

160
00:08:24,150 --> 00:08:27,239
a non-constant time comparison

161
00:08:27,240 --> 00:08:30,320
and also another one looked at the errors

162
00:08:31,777 --> 00:08:34,273
in the domain separation of our operation.

163
00:08:37,309 --> 00:08:39,390
Yeah, we also in the paper,

164
00:08:39,390 --> 00:08:42,559
looked at things on side-channel analysis.

165
00:08:42,559 --> 00:08:43,569
So...

166
00:08:46,010 --> 00:08:48,560
We consider things like power analysis

167
00:08:48,560 --> 00:08:53,020
on the core multiplication
methods required for each family

168
00:08:54,177 --> 00:08:55,010
within post quantum.

169
00:08:55,010 --> 00:08:58,150
So for example, matrix multiplication.

170
00:08:58,150 --> 00:09:00,449
And we also consider
things like fault attacks,

171
00:09:00,450 --> 00:09:03,910
cold-boots, key reuse attacks and...

172
00:09:05,311 --> 00:09:10,311
We also discussed things
like how side channel hints

173
00:09:10,600 --> 00:09:13,550
can be incorporated in
the security evaluation

174
00:09:13,550 --> 00:09:14,382
of the schemes.

175
00:09:14,383 --> 00:09:18,960
So there's papers that show
this for lattice reduction,

176
00:09:18,960 --> 00:09:22,403
and on codes using the ISD method.

177
00:09:24,830 --> 00:09:26,330
And as well as side-channel analysis,

178
00:09:26,330 --> 00:09:27,490
we looked at the countermeasures.

179
00:09:27,490 --> 00:09:31,083
So we're looking at masking
and hiding in post-quantum,

180
00:09:32,470 --> 00:09:35,480
and we obviously look at this

181
00:09:35,480 --> 00:09:39,700
so that we can put these
post-quantum schemes in the wild.

182
00:09:39,700 --> 00:09:42,879
And it's another important
area to consider.

183
00:09:42,879 --> 00:09:46,750
We've only really seen
the start of this though,

184
00:09:46,750 --> 00:09:50,350
so, I think we'll see a lot more of this.

185
00:09:50,350 --> 00:09:52,290
So yeah, for post-quantum,

186
00:09:52,290 --> 00:09:55,400
we have three of the next candidates

187
00:09:55,400 --> 00:09:59,040
that have a masking scheme proposed.

188
00:09:59,040 --> 00:10:00,906
Kyber was really recent (exhales),

189
00:10:02,450 --> 00:10:04,480
and I think in general,

190
00:10:04,480 --> 00:10:06,903
post-quantum is also another challenge,

191
00:10:08,130 --> 00:10:11,189
masking is also another
challenge for post-quantum.

192
00:10:11,190 --> 00:10:16,190
We see a lot of new operations
like rounding and comparisons

193
00:10:17,420 --> 00:10:21,553
and things that make the
masking awkward or difficult.

194
00:10:23,150 --> 00:10:24,470
And we've also seen recent results

195
00:10:24,470 --> 00:10:25,870
that show that these countermeasures

196
00:10:25,870 --> 00:10:28,050
are by no means a guarantee,

197
00:10:28,050 --> 00:10:32,750
something we've seen a lot
in the past for RSA and AES.

198
00:10:32,750 --> 00:10:35,713
We'll likely see a lot more of
these in the future as well.

199
00:10:37,713 --> 00:10:41,339
And one thing we also look
at is this hedging technique

200
00:10:41,340 --> 00:10:44,120
for mitigating fault attacks
in Fiat-Shamir signatures,

201
00:10:44,120 --> 00:10:47,023
which is another interesting
like avenue of research.

202
00:10:48,850 --> 00:10:50,690
Finally, we look at the...

203
00:10:50,690 --> 00:10:51,900
We look at benchmarking,

204
00:10:51,900 --> 00:10:55,930
like the projects that
benchmark these schemes.

205
00:10:55,930 --> 00:11:00,930
So NIST in their call,
asked for evaluation,

206
00:11:01,100 --> 00:11:04,260
asked for our performances
in software and hardware,

207
00:11:04,260 --> 00:11:06,590
and side channels in order to add

208
00:11:06,590 --> 00:11:10,933
some of evaluation criteria
to this competition.

209
00:11:11,840 --> 00:11:14,157
They decided on the ARM Cortex M4

210
00:11:14,157 --> 00:11:16,603
and the Xilinx Artex 7 FPGA.

211
00:11:17,940 --> 00:11:20,210
There's plenty of repositories online

212
00:11:20,210 --> 00:11:24,330
for software benchmarking on
the Cortex M4 and other CPUs.

213
00:11:25,670 --> 00:11:28,630
One thing to note on the M4 is that

214
00:11:28,630 --> 00:11:33,610
up to 50% of the time of all,

215
00:11:33,610 --> 00:11:35,970
not all, but a lot of these schemes

216
00:11:35,970 --> 00:11:38,460
are spent on seed expanding (sniffs).

217
00:11:38,460 --> 00:11:40,910
And I think this actually goes up to 70%

218
00:11:40,910 --> 00:11:43,130
for some of the signatures (exhales).

219
00:11:43,130 --> 00:11:46,490
And we also collate a lot
of the hardware designs

220
00:11:46,490 --> 00:11:50,213
that we've seen over the years
on these candidates as well.

221
00:11:52,560 --> 00:11:54,930
So yeah, that's really the summary

222
00:11:54,930 --> 00:11:56,670
of the two chapters of the four.

223
00:11:56,670 --> 00:11:59,069
So, here's the e-print link

224
00:11:59,070 --> 00:12:00,770
if you want to look at the others.

225
00:12:03,400 --> 00:12:05,340
And yeah, thank you for listening.

226
00:12:09,480 --> 00:12:11,220
- Hello and welcome to this talk about:

227
00:12:11,220 --> 00:12:13,430
Dual lattice attacks
for closest vector calls

228
00:12:13,430 --> 00:12:15,000
with preprocessing.

229
00:12:15,000 --> 00:12:17,810
This is joint work with Michael
Walter from IST Austria,

230
00:12:17,810 --> 00:12:19,239
and I'm Thijs Laarhoven

231
00:12:19,240 --> 00:12:22,180
from The Eindhoven
University of Technology.

232
00:12:22,180 --> 00:12:23,760
So this talk will be about lattices.

233
00:12:23,760 --> 00:12:25,650
So just to start easy what's a lattice?

234
00:12:25,650 --> 00:12:29,068
So given base vectors,
in this case, b1 and b2,

235
00:12:29,068 --> 00:12:30,020
we get a lattice from this

236
00:12:30,020 --> 00:12:31,910
by taking all integer linear combination

237
00:12:31,910 --> 00:12:33,780
of these basics vectors,

238
00:12:33,780 --> 00:12:35,620
which in this case gives us infinite scale

239
00:12:35,620 --> 00:12:38,400
of points depicted in gray.

240
00:12:38,400 --> 00:12:41,000
You can use lattices
to build cryptography,

241
00:12:41,000 --> 00:12:42,470
and this cryptography is only secure

242
00:12:42,470 --> 00:12:45,350
if certain lattice forms actually are,

243
00:12:45,350 --> 00:12:47,410
such as the Closest Vector Problem,

244
00:12:47,410 --> 00:12:49,969
which asks to find the
closest lattice point

245
00:12:49,970 --> 00:12:52,720
to a target point, in
this case, point T here,

246
00:12:52,720 --> 00:12:56,308
and the closest point
would be this point V here.

247
00:12:56,308 --> 00:12:58,610
And in this talk, I will focus
on the Closest Vector Problem

248
00:12:58,610 --> 00:12:59,830
with preprocessing.

249
00:12:59,830 --> 00:13:02,400
Which says that, okay, I
first give you a description

250
00:13:02,400 --> 00:13:05,433
of a lattice, again, a basis, b1 and b2.

251
00:13:05,433 --> 00:13:06,550
Now, you can do anything to it.

252
00:13:06,550 --> 00:13:09,020
So, for instance, you
can find a nicer basis

253
00:13:09,020 --> 00:13:10,480
for the same lattice, r1, r2.

254
00:13:11,540 --> 00:13:13,839
Maybe you want to enumerate
all short vectors,

255
00:13:13,840 --> 00:13:15,680
I mean, you can do whatever you want,

256
00:13:15,680 --> 00:13:17,229
and restore them in some data structure.

257
00:13:17,230 --> 00:13:19,690
But then when you say, "Okay, I'm done,"

258
00:13:19,690 --> 00:13:21,900
then I will give you a target vector or,

259
00:13:21,900 --> 00:13:23,420
in this case, this point T here,

260
00:13:23,420 --> 00:13:25,420
and then again you have to
find the closest point to it,

261
00:13:25,420 --> 00:13:26,800
this case, V.

262
00:13:26,800 --> 00:13:28,290
Hopefully you can do this faster than

263
00:13:28,290 --> 00:13:30,040
if you didn't do any preprocessing.

264
00:13:31,919 --> 00:13:35,670
So the talk, so our paper
is about dual attacks,

265
00:13:35,671 --> 00:13:38,857
(indistinct) to complex our
first considered primal attacks,

266
00:13:38,857 --> 00:13:40,273
and to cover them briefly.

267
00:13:41,300 --> 00:13:43,689
So maybe the most natural
courses you can think of

268
00:13:43,690 --> 00:13:47,060
for solving this problem in
high dimensions or any dimension

269
00:13:47,060 --> 00:13:50,410
is Voronoi cells because Voronoi
cells exactly tell you that

270
00:13:50,410 --> 00:13:51,900
if you are in one of these cells,

271
00:13:51,900 --> 00:13:55,209
then the points in the middle of the cell

272
00:13:55,210 --> 00:13:56,190
is actually the closest point.

273
00:13:56,190 --> 00:13:59,340
So, this case here, like if
you are in one of these cells

274
00:13:59,340 --> 00:14:01,150
then you know that a
lattice point in this cell

275
00:14:01,150 --> 00:14:03,160
is actually the closest lattice point

276
00:14:03,160 --> 00:14:04,459
out of all lattice points.

277
00:14:05,370 --> 00:14:07,353
But okay, if you want to use
these in higher dimensions,

278
00:14:07,353 --> 00:14:08,390
like the first question is

279
00:14:08,390 --> 00:14:11,150
how do we actually store
these Voronoi cells?

280
00:14:11,150 --> 00:14:12,540
Well you can implicitly store them

281
00:14:12,540 --> 00:14:14,589
by storing the element vectors

282
00:14:14,590 --> 00:14:16,707
which are in this case, vectors r1 to r6.

283
00:14:16,707 --> 00:14:18,957
We define the boundaries
of the (indistinct).

284
00:14:21,022 --> 00:14:22,740
And then you can also use
these relevant vectors

285
00:14:22,740 --> 00:14:25,040
to build an algorithm for scoring

286
00:14:25,040 --> 00:14:27,219
both the Closest Vector Problem.

287
00:14:27,220 --> 00:14:29,100
But also, if we have a target T here,

288
00:14:29,100 --> 00:14:32,280
this is orange the star on the right.

289
00:14:32,280 --> 00:14:34,860
Then, okay, in this
case, the closest point

290
00:14:34,860 --> 00:14:36,917
is this orange point in the right,

291
00:14:36,917 --> 00:14:39,060
and it is in the cell here.

292
00:14:39,060 --> 00:14:39,959
What we are going to do is

293
00:14:39,960 --> 00:14:41,837
we're going to add relevant
vectors to make it er

294
00:14:41,837 --> 00:14:46,319
and er until we end up in
the Voronoi cell origin.

295
00:14:46,320 --> 00:14:47,770
Is just to show what this means,

296
00:14:47,770 --> 00:14:50,970
is that we just, yeah, we
add these vectors one by one,

297
00:14:50,970 --> 00:14:52,990
shifting it in the same
codes of the lattice

298
00:14:52,990 --> 00:14:56,420
until we end up in the
Voronoi cells origin.

299
00:14:56,420 --> 00:14:58,150
In this case, we cannot reuse it anymore,

300
00:14:58,150 --> 00:14:59,720
so we know that now the origin

301
00:14:59,720 --> 00:15:03,700
is actually the closest
point to this shifter target,

302
00:15:03,700 --> 00:15:06,282
and that also tells us what
is the closest lattice point

303
00:15:06,282 --> 00:15:09,040
to the original lattice vector.

304
00:15:09,040 --> 00:15:11,870
So this is actually the
exact Voronoi cell portion,

305
00:15:11,870 --> 00:15:13,880
but this does not really
work well in practice,

306
00:15:13,880 --> 00:15:15,417
because the number of Voronoi cells

307
00:15:15,417 --> 00:15:17,160
with the number of relevant vectors

308
00:15:17,160 --> 00:15:19,400
is way too big to use this.

309
00:15:19,400 --> 00:15:21,300
So in this case, we have
six relevant vectors,

310
00:15:21,300 --> 00:15:23,550
but in end dimensions,
you have two DDN vectors

311
00:15:23,550 --> 00:15:26,920
to describe the boundaries
of this Voronoi cell,

312
00:15:26,920 --> 00:15:29,719
and it is just way too much.

313
00:15:29,720 --> 00:15:31,010
What you actually do in practice,

314
00:15:31,010 --> 00:15:32,890
like what is the
(indistinct) the others use

315
00:15:32,890 --> 00:15:34,240
of approximate Voronoi cells,

316
00:15:34,240 --> 00:15:36,256
where you say, "Okay I'm not going to use

317
00:15:36,256 --> 00:15:38,010
the exact sort of relevant vectors.

318
00:15:38,010 --> 00:15:42,637
I'm just going to use a
small set of short vectors."

319
00:15:43,475 --> 00:15:45,930
So in this case, if you want
up to to fifth for instance,

320
00:15:45,930 --> 00:15:48,680
which again define some
kind of salvage looks

321
00:15:48,680 --> 00:15:49,973
like the Voronoi cell.

322
00:15:50,827 --> 00:15:53,440
And then using this
approximate Voronoi cell,

323
00:15:53,440 --> 00:15:55,730
you can again use the
same approach as before

324
00:15:55,730 --> 00:15:57,987
where you reduce the targets all the way

325
00:15:57,987 --> 00:16:00,663
until you get into the cell diversion.

326
00:16:01,540 --> 00:16:03,709
But now, because this
approximate Voronoi cell

327
00:16:03,710 --> 00:16:05,400
is not and except one,

328
00:16:05,400 --> 00:16:08,459
we may not actually find a
solution because in this case,

329
00:16:08,460 --> 00:16:10,900
the solution that we think we have

330
00:16:10,900 --> 00:16:13,083
is actually not a closest lattice point,

331
00:16:14,190 --> 00:16:15,950
but actually if you
randomize this procedure

332
00:16:15,950 --> 00:16:17,730
of reproducing a number of times,

333
00:16:17,730 --> 00:16:20,570
then you may actually find
the rise of (indistinct)

334
00:16:20,570 --> 00:16:22,490
in this approximate Voronoi cell origin

335
00:16:22,490 --> 00:16:25,504
and then you do find a
solution that you have to do.

336
00:16:25,504 --> 00:16:26,357
Then there's a number of times,

337
00:16:26,357 --> 00:16:29,170
and then actually you will
do find the solutions.

338
00:16:29,171 --> 00:16:31,490
When you use approximate presentation,

339
00:16:31,490 --> 00:16:36,490
on approximate Voronoi cell
when you use that and stuff.

340
00:16:36,638 --> 00:16:38,850
But okay, so our previous was
actually about the dual attack

341
00:16:38,850 --> 00:16:41,903
and analyzing these and comparing
it to the primal attack.

342
00:16:43,060 --> 00:16:46,400
So just to recall briefly,
so what is the dual lattice?

343
00:16:46,400 --> 00:16:48,590
So dual lattice is find
to us all points in space

344
00:16:48,590 --> 00:16:51,313
always dot-product at
primal vector integer.

345
00:16:52,370 --> 00:16:55,300
And that again forms
lattice, so in this example,

346
00:16:55,300 --> 00:16:56,973
two dimensional example,

347
00:16:56,974 --> 00:17:00,933
this grid of pink points
is the dual lattice,

348
00:17:02,220 --> 00:17:04,060
and a dual to dual is a kind of primal

349
00:17:04,060 --> 00:17:05,960
which also means that the primal lattice

350
00:17:05,960 --> 00:17:08,810
can be defined as all points
in space always the dot-product

351
00:17:08,810 --> 00:17:11,973
with the dual vector, dual
lattice vector as an integer.

352
00:17:13,040 --> 00:17:15,230
And this dot-product has to be an integer

353
00:17:15,230 --> 00:17:17,829
for any dual vector, also if
we take for instance this,

354
00:17:17,829 --> 00:17:19,990
dual vector d1 here then we know that

355
00:17:19,990 --> 00:17:24,520
all primal lattice vectors
must have a dot-product

356
00:17:24,520 --> 00:17:25,869
which is an integer, which means that

357
00:17:25,869 --> 00:17:28,520
it must lie on one of these lines here,

358
00:17:28,520 --> 00:17:30,500
because d1 is a short dual vector.

359
00:17:30,500 --> 00:17:33,170
These lines are pretty far apart

360
00:17:33,170 --> 00:17:36,150
and that is the key
priority for the dual attack

361
00:17:36,150 --> 00:17:37,890
which is the observation here

362
00:17:37,890 --> 00:17:39,330
that if we now have a target vector

363
00:17:39,330 --> 00:17:41,070
which does not lie on the lattice,

364
00:17:41,070 --> 00:17:43,347
on the primal lattice, but
close to the primal lattice

365
00:17:43,347 --> 00:17:46,930
and also it will lie close
to one of these lines.

366
00:17:46,930 --> 00:17:50,090
So if we take the dot-product
with a short target vector,

367
00:17:50,090 --> 00:17:52,159
and we compute it modular one

368
00:17:52,160 --> 00:17:54,190
to see if we are close
to one of these lines

369
00:17:54,190 --> 00:17:57,280
then we actually can distinguish
between target which line

370
00:17:57,280 --> 00:17:59,930
far from lattice and which
line close to the lattice.

371
00:18:01,220 --> 00:18:02,630
So okay, here you,

372
00:18:02,630 --> 00:18:04,780
so if you have a target
vector you can compute

373
00:18:04,780 --> 00:18:07,639
these dot-products with dual vectors,

374
00:18:07,640 --> 00:18:09,960
compute a modular one
and then you combine them

375
00:18:09,960 --> 00:18:12,583
in a certain way for lots of dual vectors.

376
00:18:13,610 --> 00:18:14,949
And if you do that in this way here,

377
00:18:14,950 --> 00:18:17,387
for instance with this close end point,

378
00:18:17,387 --> 00:18:20,389
then you see that the output
value tells you exactly

379
00:18:20,390 --> 00:18:21,450
how close you're at a lattice,

380
00:18:21,450 --> 00:18:23,640
because like if you are
very close to lattice,

381
00:18:23,640 --> 00:18:25,860
if T is close to lattice at this,

382
00:18:25,860 --> 00:18:30,229
if the sum of all dual vectors
is going to be very large

383
00:18:30,230 --> 00:18:34,810
which is depicted by a
dark of shade of red here,

384
00:18:34,810 --> 00:18:36,879
we'll have to the sum is kind of

385
00:18:36,880 --> 00:18:39,170
a small like the target
is kind of eminence-based

386
00:18:39,170 --> 00:18:41,010
and you get like a white color.

387
00:18:41,010 --> 00:18:44,680
So you can tell from the
value of this infamous sum

388
00:18:44,680 --> 00:18:47,210
when you are close to lattice or not,

389
00:18:47,210 --> 00:18:49,820
but you have to take this
infamous sum of all dual vectors

390
00:18:49,820 --> 00:18:52,960
which is not really what
you can do in practice.

391
00:18:52,960 --> 00:18:54,030
But you can actually do in practice

392
00:18:54,030 --> 00:18:56,760
if you just take a number
of short dual vectors

393
00:18:56,760 --> 00:18:58,530
and you again, compute such a sum

394
00:18:58,530 --> 00:19:01,520
and then you hope that it
also helps you distinguish

395
00:19:01,520 --> 00:19:03,127
between targets which are close to lattice

396
00:19:03,127 --> 00:19:05,450
and which are not close to lattice.

397
00:19:05,450 --> 00:19:08,037
So let's get if we take
these four points here,

398
00:19:08,037 --> 00:19:11,070
and the shape is not exactly
like this nice gaussian shape

399
00:19:11,070 --> 00:19:11,903
around the lattice points

400
00:19:11,903 --> 00:19:14,333
which you get like (indistinct) shape.

401
00:19:15,278 --> 00:19:17,160
And we can use this to distinguish,

402
00:19:17,160 --> 00:19:19,680
you can also build a search
algorithm on top of this

403
00:19:19,680 --> 00:19:21,440
by doing a gradient to send a approach

404
00:19:21,440 --> 00:19:24,380
on this kind of Haley landscape
with all these functions

405
00:19:24,380 --> 00:19:26,233
around lattice points.

406
00:19:27,440 --> 00:19:29,170
So you move in the
direction of the gradient

407
00:19:29,170 --> 00:19:31,700
of this sum of both sines,

408
00:19:31,700 --> 00:19:35,159
or just to illustrate this if
we have this targets T here,

409
00:19:35,159 --> 00:19:37,820
and we shift like we do a
small step in the direction

410
00:19:37,820 --> 00:19:41,100
of the gradient which means
that we get this point T,

411
00:19:41,100 --> 00:19:44,209
T prime here, then we do
this a number of times

412
00:19:44,210 --> 00:19:46,620
until we get very close to lattice,

413
00:19:46,620 --> 00:19:49,590
and then we get our estimate
for the closest lattice point

414
00:19:49,590 --> 00:19:52,120
to the original target vector V.

415
00:19:52,120 --> 00:19:53,780
And this case, we don't
actually find the solution

416
00:19:53,780 --> 00:19:56,170
because we only have an
approximate representation

417
00:19:56,170 --> 00:19:58,130
of the exact line gaussian shape,

418
00:19:58,130 --> 00:20:00,480
'cause we only have some
short of dual vectors.

419
00:20:01,820 --> 00:20:04,919
That is kind of the basic
idea of the dual attack,

420
00:20:04,920 --> 00:20:07,350
and that is what we
analyze, but distinguishing

421
00:20:07,350 --> 00:20:10,570
and the search outcome is
gradient ascent approach.

422
00:20:10,570 --> 00:20:14,710
Okay, so just to illustrate the
main results from our paper.

423
00:20:14,710 --> 00:20:17,420
So here we have on the horizontal axis,

424
00:20:17,420 --> 00:20:19,670
we have kind of different columns settings

425
00:20:19,670 --> 00:20:21,230
where either we have a target

426
00:20:21,230 --> 00:20:22,550
which lies very close to lattice

427
00:20:22,550 --> 00:20:26,250
which is the last part of
this graph, BD setting,

428
00:20:26,250 --> 00:20:28,250
the Bounded Distance Decoding setting.

429
00:20:28,250 --> 00:20:30,240
On the right part, we
have the column setting

430
00:20:30,240 --> 00:20:33,900
where we have a target which
may be kind of random in space,

431
00:20:33,900 --> 00:20:38,540
but we are happy if we find
any close-ish lattice point,

432
00:20:38,540 --> 00:20:40,430
like we don't have to
actually find the closest one,

433
00:20:40,430 --> 00:20:42,360
this something which should like,

434
00:20:42,360 --> 00:20:43,810
the most effective for further

435
00:20:43,810 --> 00:20:45,840
than the actual closest vector
(indistinct) of solution,

436
00:20:45,840 --> 00:20:48,919
which is on the right part of this box.

437
00:20:49,990 --> 00:20:53,860
And then on the vertical axis,
we have to create complexity

438
00:20:53,860 --> 00:20:55,959
like after preprocessing
what is actually the cost

439
00:20:55,960 --> 00:20:59,963
of solving one, two pieces
instance with this approach.

440
00:21:00,860 --> 00:21:02,639
So in with that, we have the primal attack

441
00:21:02,640 --> 00:21:05,200
which we can see that it works pretty well

442
00:21:05,200 --> 00:21:08,610
for approximate CVP settings.

443
00:21:08,610 --> 00:21:11,860
Like if we want to solve
approximate CVP problems,

444
00:21:11,860 --> 00:21:13,850
then actually a primal attack
is what you want to use

445
00:21:13,850 --> 00:21:17,100
because the dual attack is
not really going to help you

446
00:21:17,100 --> 00:21:20,169
so much, but if you want
to solve a BDD instance,

447
00:21:20,170 --> 00:21:22,750
then actually the dual
attack is much better,

448
00:21:22,750 --> 00:21:25,110
which is what the analyzer will be shows.

449
00:21:25,110 --> 00:21:27,250
This is also kind of solving a problem

450
00:21:27,250 --> 00:21:29,510
from the (indistinct)
on the primal attack,

451
00:21:29,510 --> 00:21:33,344
where we saw that the
primal attack worked well

452
00:21:33,344 --> 00:21:36,280
for approximate CVPP but not for BDDP,

453
00:21:36,280 --> 00:21:38,097
which is kind of weird, and we were like,

454
00:21:38,097 --> 00:21:41,670
"Okay, maybe this is a
problem with primal attack,

455
00:21:41,670 --> 00:21:43,850
but now we showed actually the
dual attack is much better,

456
00:21:43,850 --> 00:21:46,996
and a dual attack doesn't
work well for approximates.

457
00:21:48,025 --> 00:21:51,710
But kind of these primal and dual attacks

458
00:21:51,710 --> 00:21:53,090
are kind of dual to one another,

459
00:21:53,090 --> 00:21:56,720
not just by using one of
the primalism on the dualism

460
00:21:56,720 --> 00:21:58,730
but also one word 12 for one column

461
00:21:58,730 --> 00:22:00,003
and one word 12 for the other column,

462
00:22:00,003 --> 00:22:04,403
which is kind of a nice
result from our paper.

463
00:22:05,390 --> 00:22:07,040
So this is kind of the simplest result

464
00:22:07,040 --> 00:22:09,853
that I can present in this short time.

465
00:22:11,070 --> 00:22:14,750
So just to briefly concludes
the main contributions

466
00:22:14,750 --> 00:22:17,460
of our paper and of this talk.

467
00:22:17,460 --> 00:22:19,790
So, primal attacks approach shows that

468
00:22:19,790 --> 00:22:23,182
you use a list of short primal
lattice vector like this,

469
00:22:23,182 --> 00:22:25,630
Voronoi cells, approximate Voronoi cells.

470
00:22:25,630 --> 00:22:29,140
And this works well for solving
approximate CVP and CVPP,

471
00:22:29,140 --> 00:22:31,490
but not for Bounded
Distance Decoding problems.

472
00:22:32,450 --> 00:22:35,570
Well, a dual attack uses a list
of short dual lattice vector

473
00:22:35,570 --> 00:22:38,889
either to distinguish using
the sum of cosine values

474
00:22:38,890 --> 00:22:40,510
or to actually find a solution

475
00:22:40,510 --> 00:22:43,090
using this gradient ascent approach.

476
00:22:43,090 --> 00:22:45,530
And this works well for
Bounded Distance Decoding

477
00:22:45,530 --> 00:22:49,622
the BDD setting, but
not for approximate CVP

478
00:22:49,622 --> 00:22:51,320
by the primal lattice vector.

479
00:22:51,320 --> 00:22:53,470
And this work is actually
like our contribution

480
00:22:53,470 --> 00:22:55,770
is a complete heuristic
average-case analysis

481
00:22:55,770 --> 00:22:59,760
of the dual attack, which had
not really been done before

482
00:22:59,760 --> 00:23:03,883
for CVP but mostly done for
(indistinct) for instance.

483
00:23:04,760 --> 00:23:07,260
And we also did experiments
with this approach

484
00:23:07,260 --> 00:23:09,950
and these experiments closely match

485
00:23:09,950 --> 00:23:12,760
our heuristic predictions
by (indistinct) given.

486
00:23:12,760 --> 00:23:14,390
Just kind of closest really,

487
00:23:14,390 --> 00:23:16,930
we can almost exactly or
what you want us to cost

488
00:23:16,930 --> 00:23:18,860
or like dimension (indistinct),

489
00:23:18,860 --> 00:23:21,050
like a theory is really very close

490
00:23:21,050 --> 00:23:23,576
to all heuristic predictions.

491
00:23:23,576 --> 00:23:27,000
So this is just a kind
of the high level results

492
00:23:27,000 --> 00:23:29,830
in my paper, so, I will stop here.

493
00:23:29,830 --> 00:23:31,783
So yeah, thank you for watching.

494
00:23:35,510 --> 00:23:38,690
- Hello and thank you for
attending this presentation.

495
00:23:38,690 --> 00:23:39,940
I'm called Corentin Jeudy

496
00:23:39,940 --> 00:23:44,940
and I'm a Research Intern at
Univ Rennes CNRS and IRISA

497
00:23:46,080 --> 00:23:48,320
I will be presenting today a joint work

498
00:23:48,320 --> 00:23:51,283
with Katharina Boudgoust, Adeline Langlois

499
00:23:51,284 --> 00:23:52,650
and Weiqiang Wen.

500
00:23:52,650 --> 00:23:54,950
On the Hardest of Module
Learning With Errors

501
00:23:54,950 --> 00:23:56,423
with Binary Secrets.

502
00:23:58,080 --> 00:24:01,409
Our work is focused on
proving the hardness

503
00:24:01,410 --> 00:24:03,470
of the Module Learning With Errors problem

504
00:24:03,470 --> 00:24:05,700
when using Binary Secrets.

505
00:24:05,700 --> 00:24:10,330
And we do so by providing
a reduction from an LWE

506
00:24:10,330 --> 00:24:12,082
to its binary secret variance.

507
00:24:13,130 --> 00:24:17,240
The problem as well as its
binary variance are fundamental

508
00:24:17,240 --> 00:24:20,550
in that is based cryptography
because their structure allows

509
00:24:20,550 --> 00:24:25,050
for a nice balance between
efficiency and security.

510
00:24:25,050 --> 00:24:28,230
The purpose of this work more precisely

511
00:24:28,230 --> 00:24:29,910
is to get a better understanding

512
00:24:29,910 --> 00:24:31,840
of the Module Learning
With Errors problem,

513
00:24:31,840 --> 00:24:34,490
and more specifically of the differences

514
00:24:34,490 --> 00:24:37,660
between theoretical and practical hardness

515
00:24:37,660 --> 00:24:39,820
when using small secrets.

516
00:24:39,820 --> 00:24:44,270
The small secret case is
particularly interesting

517
00:24:44,270 --> 00:24:45,660
for various constructions,

518
00:24:45,660 --> 00:24:48,160
especially fully homomorphic encryption

519
00:24:48,160 --> 00:24:50,630
as well as for efficiency reasons.

520
00:24:50,630 --> 00:24:53,230
And it is also valuable in the proof

521
00:24:53,230 --> 00:24:55,053
of some other theoretical results.

522
00:24:55,990 --> 00:24:57,980
Now, as far as the parameters go,

523
00:24:57,980 --> 00:25:00,170
I will just mention that
the main restriction

524
00:25:00,170 --> 00:25:02,160
is about the module rank

525
00:25:02,160 --> 00:25:04,183
which needs to be super-logarithmic.

526
00:25:05,060 --> 00:25:07,300
We will see in a second,
what it refers to,

527
00:25:07,300 --> 00:25:09,980
but it is the most
challenging part when trying

528
00:25:09,980 --> 00:25:13,383
to close the gap between
theory and practice.

529
00:25:14,930 --> 00:25:17,100
But before, let me do a quick reminder

530
00:25:17,100 --> 00:25:19,632
on what this problem actually is.

531
00:25:20,823 --> 00:25:23,770
So you're given a
uniformly random matrix A

532
00:25:23,770 --> 00:25:25,810
with M rows and K columns.

533
00:25:25,810 --> 00:25:28,020
M is called the number of samples,

534
00:25:28,020 --> 00:25:30,653
and K is the so-called module rank.

535
00:25:31,961 --> 00:25:34,610
You're also given a vector B of size M

536
00:25:34,610 --> 00:25:38,280
and the M-LWE problem asks you to decide

537
00:25:38,280 --> 00:25:40,610
whether B is uniformly random

538
00:25:40,610 --> 00:25:42,909
without any specific structure to it,

539
00:25:42,910 --> 00:25:45,670
or if it is up the form AS plus E

540
00:25:45,670 --> 00:25:49,220
or S is a uniformly random secret vector

541
00:25:49,220 --> 00:25:52,673
and E is an error vector
following a Gaussian distribution.

542
00:25:54,060 --> 00:25:56,740
As opposed to a standard LWE

543
00:25:56,740 --> 00:25:58,710
where we work over the integers,

544
00:25:58,710 --> 00:26:01,540
in M-LWE, we work over ring R.

545
00:26:01,540 --> 00:26:05,210
R here can be seen as the
ring of integer polynomials

546
00:26:05,210 --> 00:26:06,960
where degree less than N,

547
00:26:06,960 --> 00:26:09,990
where asymptotic parameter N is the degree

548
00:26:09,990 --> 00:26:12,237
of the defining polynomial file.

549
00:26:13,470 --> 00:26:17,630
Popular choice of N for both
theory and implementations

550
00:26:17,630 --> 00:26:20,470
is when N equals the power of two,

551
00:26:20,470 --> 00:26:23,240
because it simplifies the form of file

552
00:26:23,240 --> 00:26:25,320
making it quite easy to work with.

553
00:26:25,320 --> 00:26:29,070
And it also has additional
interesting properties

554
00:26:29,070 --> 00:26:30,503
to work with.

555
00:26:32,520 --> 00:26:36,480
The final parameter that I
didn't mention is the modules Q,

556
00:26:36,480 --> 00:26:38,790
all of the major season vectors

557
00:26:38,790 --> 00:26:41,200
are composed of elements of RQ,

558
00:26:41,200 --> 00:26:44,070
which can be seen as inter-tripolynomials

559
00:26:44,070 --> 00:26:46,280
of degree less than N still,

560
00:26:46,280 --> 00:26:50,562
with each coefficient is
reduced modular the integer Q.

561
00:26:52,300 --> 00:26:55,320
But what interests us
today more specifically

562
00:26:55,320 --> 00:26:57,889
is the binary version of this problem.

563
00:26:57,890 --> 00:27:00,830
The only difference is
that instead of choosing

564
00:27:00,830 --> 00:27:05,750
the secret vector S from
RQ, we choose it from R2,

565
00:27:05,750 --> 00:27:08,210
which essentially is the set of vectors

566
00:27:08,210 --> 00:27:09,750
for which each coefficient

567
00:27:09,750 --> 00:27:12,083
is a binary polynomial
of degree less than N.

568
00:27:14,200 --> 00:27:17,380
Now the reason why M-LWE is so interesting

569
00:27:17,380 --> 00:27:19,930
from a theoretical standpoint is because

570
00:27:19,930 --> 00:27:22,323
it subsumes 2 of its popular variants.

571
00:27:23,160 --> 00:27:26,010
First, the original LWE problem

572
00:27:26,010 --> 00:27:28,920
can be seen as a special case of M-LWE

573
00:27:28,920 --> 00:27:30,550
when setting N equals to 1

574
00:27:30,550 --> 00:27:33,190
because then we fall back on the integers.

575
00:27:33,190 --> 00:27:36,000
The second is the ring LWE problem

576
00:27:36,000 --> 00:27:39,990
which was the first
structured variance of LWE

577
00:27:39,990 --> 00:27:42,483
and which is quite popular
for its efficiency.

578
00:27:43,590 --> 00:27:46,770
And here, ring LWE corresponds to the case

579
00:27:46,770 --> 00:27:49,340
when the module rank K is equal to 1.

580
00:27:51,030 --> 00:27:54,500
But, why else do we care
so much about this problem?

581
00:27:54,500 --> 00:27:56,350
Well, from standard LWE,

582
00:27:56,350 --> 00:27:59,199
we are able to construct
several cryptographic primitives

583
00:27:59,200 --> 00:28:02,280
ranging from public-key
encryption to signatures,

584
00:28:02,280 --> 00:28:04,590
up to the more advanced ones,

585
00:28:04,590 --> 00:28:06,342
like fully homomorphic encryption.

586
00:28:07,430 --> 00:28:09,440
And all of these schemes are secure,

587
00:28:09,440 --> 00:28:11,280
they lack of efficiency.

588
00:28:11,280 --> 00:28:15,070
Now on the contrary, the
scheme is based on ring LWE

589
00:28:15,070 --> 00:28:18,480
are more efficient but
we still have some doubts

590
00:28:18,480 --> 00:28:20,510
about their security.

591
00:28:20,510 --> 00:28:24,160
And this is where M-LWE
becomes interesting,

592
00:28:24,160 --> 00:28:26,610
because it allows us
to tune the parameters

593
00:28:26,610 --> 00:28:30,142
to trade concrete security
for efficiency and vice versa.

594
00:28:31,490 --> 00:28:35,290
So much so that three of
the third round finalists

595
00:28:35,290 --> 00:28:39,450
in this competition use it
as their hardness assumption.

596
00:28:39,450 --> 00:28:43,160
The crystal suites for
example, bases its security

597
00:28:43,160 --> 00:28:46,990
directly on the hardness of Module-LWE,

598
00:28:46,990 --> 00:28:49,250
while the key encapsulation
mechanism Saber

599
00:28:49,250 --> 00:28:52,150
is based on Module Learning With Rounding,

600
00:28:52,150 --> 00:28:54,853
which is a deterministic
variance of M-LWE.

601
00:28:56,070 --> 00:29:00,020
And according to the NIST,
these structured lattice schemes

602
00:29:00,020 --> 00:29:01,879
seem the most promising candidates

603
00:29:01,880 --> 00:29:04,250
for the post-quantum
cryptography standard,

604
00:29:04,250 --> 00:29:07,477
as far as KEM and digital
signatures scheme.

605
00:29:08,420 --> 00:29:10,900
And in all of that, the binary version

606
00:29:10,900 --> 00:29:15,200
is mostly helpful because
these candidates prefer

607
00:29:15,200 --> 00:29:18,650
to use small parameters to make
the schemes more efficient.

608
00:29:18,650 --> 00:29:21,270
And this is why we need to investigate

609
00:29:21,270 --> 00:29:24,473
the theoretical framework
behind it to back that up.

610
00:29:26,300 --> 00:29:29,139
So we can now dive into
the core of our work,

611
00:29:29,140 --> 00:29:31,740
and here's an overview
of the different steps

612
00:29:31,740 --> 00:29:32,893
of R reduction.

613
00:29:33,990 --> 00:29:37,500
We first reduce M-LWE to a variant called

614
00:29:37,500 --> 00:29:40,760
first-is-errorless M-LWE,

615
00:29:40,760 --> 00:29:42,520
it is exactly the same problem

616
00:29:42,520 --> 00:29:47,520
except for an extra hints
on the arrow vector E

617
00:29:47,660 --> 00:29:50,550
which is simply the first
coefficient of the vector E.

618
00:29:51,650 --> 00:29:55,830
Then we reduced to another
variant called the Extended-M-LWE

619
00:29:55,830 --> 00:29:58,860
where we generalize
the possible hints on E

620
00:29:59,716 --> 00:30:02,753
by allowing any binary vectors.

621
00:30:03,820 --> 00:30:06,889
We then go to a matrix
version of the same problems,

622
00:30:06,890 --> 00:30:10,570
so we have a secret matrix
and an arrow matrix N,

623
00:30:10,570 --> 00:30:12,639
but the rest is exactly the same,

624
00:30:12,640 --> 00:30:16,793
and finally, we reduce to
M-LWE with binary secrets.

625
00:30:17,950 --> 00:30:20,190
So in the remaining time
of the presentation,

626
00:30:20,190 --> 00:30:23,000
I want to focus on two steps here.

627
00:30:23,000 --> 00:30:26,250
The first one being the
reduction from first-is-errorless

628
00:30:26,250 --> 00:30:27,583
to extended-M-LWE.

629
00:30:30,410 --> 00:30:33,650
In this step, we need
to explicitly construct

630
00:30:33,650 --> 00:30:35,830
a matrix over the ring.

631
00:30:35,830 --> 00:30:39,800
For each binary vector Z,
we must construct a matrix U

632
00:30:39,800 --> 00:30:40,899
that is invertible RQ,

633
00:30:41,870 --> 00:30:44,889
and such that all of its
columns except for the first one

634
00:30:44,890 --> 00:30:47,233
are orthogonal to the vectors.

635
00:30:48,310 --> 00:30:51,179
We also need to minimize its spectral norm

636
00:30:51,180 --> 00:30:53,860
because it is what
influences the noise growth

637
00:30:53,860 --> 00:30:57,513
of the reduction so we want to
keep it as small as possible.

638
00:30:58,950 --> 00:31:01,360
So we provided this construction here

639
00:31:01,360 --> 00:31:03,050
which looks rather simple.

640
00:31:03,050 --> 00:31:05,330
The orthogonality is quite trivial

641
00:31:06,390 --> 00:31:10,030
due to this plus negative
pattern of the columns,

642
00:31:10,030 --> 00:31:14,440
and the spectral norm can be
bounded quite easily as well

643
00:31:14,440 --> 00:31:16,840
because of the diagonal
structure of the matrix.

644
00:31:17,870 --> 00:31:21,679
The three key one though, was
the invertibility condition

645
00:31:21,680 --> 00:31:24,530
because it requires
every diagonal elements

646
00:31:24,530 --> 00:31:26,993
to be invertible in the caution ring.

647
00:31:28,040 --> 00:31:31,730
Fortunately for us, we were
able to use a recent results

648
00:31:31,730 --> 00:31:35,030
by Lyubashevsky Seiler from 2018,

649
00:31:35,030 --> 00:31:39,100
that gives explicit conditions on Q

650
00:31:39,100 --> 00:31:43,760
to provide us with the
invertibility of small norm elements

651
00:31:43,760 --> 00:31:48,403
which we can use for
binary elements as well.

652
00:31:49,530 --> 00:31:53,473
And this is what completed
our construction.

653
00:31:54,700 --> 00:31:58,140
The second step that I
want to focus on today

654
00:31:58,140 --> 00:32:00,957
is the final reduction to binary M-LWE.

655
00:32:02,390 --> 00:32:04,990
In this step, we use a lossy argument

656
00:32:04,990 --> 00:32:08,190
which consists in replacing the matrix A

657
00:32:08,190 --> 00:32:11,690
by M-LWE instance BC plus N

658
00:32:11,690 --> 00:32:14,440
where C acts as a secret matrix

659
00:32:14,440 --> 00:32:16,513
and N as a Gaussian error matrix.

660
00:32:17,980 --> 00:32:21,530
And the goal here is to
prove that the top two pairs

661
00:32:21,530 --> 00:32:24,763
are indistinguishable to prove
that the problem is hard.

662
00:32:26,220 --> 00:32:30,520
So we start by decomposing the arrow E

663
00:32:30,520 --> 00:32:33,070
into negative N times Z plus e hat,

664
00:32:33,070 --> 00:32:35,592
where both N and e hat are Gaussians.

665
00:32:37,060 --> 00:32:40,560
And then we replaced A by BC plus N.

666
00:32:40,560 --> 00:32:42,480
Now, the reason why we need

667
00:32:42,480 --> 00:32:45,560
the extended-M-LWE hardness assumption

668
00:32:45,560 --> 00:32:47,520
is because we have some information

669
00:32:47,520 --> 00:32:49,870
about N time Z in the previous pair.

670
00:32:49,870 --> 00:32:52,189
So we need to include this hint

671
00:32:52,190 --> 00:32:54,053
within our hardness assumption.

672
00:32:55,360 --> 00:32:58,290
After that we use the a ring version

673
00:32:58,290 --> 00:33:02,190
of the leftover hash lemma
to argue that C times Z

674
00:33:02,190 --> 00:33:04,780
is indistinguishable
from a smaller vector S

675
00:33:05,630 --> 00:33:07,630
which is uniform (indistinct),

676
00:33:07,630 --> 00:33:10,760
and this is where our
super-logarithmic rank condition

677
00:33:10,760 --> 00:33:11,593
is needed.

678
00:33:12,860 --> 00:33:17,100
Then BS plus e hat is in M-LWE instance,

679
00:33:17,100 --> 00:33:19,379
so assuming that M-LWE is hard,

680
00:33:19,380 --> 00:33:23,120
we can say that it is indistinguishable

681
00:33:23,120 --> 00:33:24,503
from a uniform vector B.

682
00:33:25,730 --> 00:33:28,840
And finally, we map A hat back to A

683
00:33:28,840 --> 00:33:33,260
using a similar assumption
which concludes this whole chain

684
00:33:33,260 --> 00:33:37,373
of indistinguishability
proving the results.

685
00:33:39,240 --> 00:33:44,130
So to briefly conclude, I
first want to mention that

686
00:33:44,130 --> 00:33:47,960
our work extends the results
from the original paper

687
00:33:47,960 --> 00:33:50,880
by Brakerski, Langlois,
Peikert, Regev and Stehle

688
00:33:50,880 --> 00:33:54,963
from 2013, which deals with standard LME.

689
00:33:55,970 --> 00:33:58,760
So if we set N equals to 1 in our setting

690
00:33:58,760 --> 00:34:00,810
we end up with the same result as theirs.

691
00:34:02,210 --> 00:34:06,370
We also provided another
reduction at Asia Scribe last year

692
00:34:06,370 --> 00:34:10,139
which achieved a similar
rank in modules conditions,

693
00:34:10,139 --> 00:34:13,542
but the noise growth
was a little bit larger.

694
00:34:13,543 --> 00:34:15,690
So here we managed to improve that

695
00:34:15,690 --> 00:34:18,112
by a factor of square root N times D,

696
00:34:19,250 --> 00:34:22,449
also making it independent
of the number of samples M

697
00:34:22,449 --> 00:34:24,592
which is quite satisfactory as well.

698
00:34:25,790 --> 00:34:27,949
There's still some open questions,

699
00:34:27,949 --> 00:34:30,210
the main one being about the existence

700
00:34:30,210 --> 00:34:33,150
of such a reduction for smaller ranks.

701
00:34:33,150 --> 00:34:36,020
We required the module rank
to be super-logarithmic

702
00:34:36,020 --> 00:34:38,429
because of our leftover hash lemma,

703
00:34:38,429 --> 00:34:40,790
but it would be interesting to have

704
00:34:40,790 --> 00:34:43,909
a similar results for constant ranks,

705
00:34:43,909 --> 00:34:47,859
because it would encompass
most practical constructions,

706
00:34:47,860 --> 00:34:50,823
including the NIST candidates
that I mentioned earlier.

707
00:34:51,889 --> 00:34:55,460
The case D equals 1 would also
be particularly interesting

708
00:34:55,460 --> 00:34:58,190
as it would provide the first results,

709
00:34:58,190 --> 00:35:01,250
to the first hardness results for ring LWE

710
00:35:01,250 --> 00:35:02,303
with binary secrets.

711
00:35:04,320 --> 00:35:08,170
Finally, even though other
number fields are not common

712
00:35:08,170 --> 00:35:10,210
in cryptography constructions,

713
00:35:10,210 --> 00:35:13,000
it could be interesting
to extend our result

714
00:35:13,000 --> 00:35:16,300
to other number fields than cyclotomics,

715
00:35:16,300 --> 00:35:18,383
but just out of pure curiosity.

716
00:35:20,370 --> 00:35:21,660
Thank you for your attention,

717
00:35:21,660 --> 00:35:23,830
and I'll be there in
the interactive session

718
00:35:23,830 --> 00:35:25,799
if you have any questions.

719
00:35:25,799 --> 00:35:26,632
Thank you.

