1
00:00:04,120 --> 00:00:06,540
- In 1805, Admiral Lord Nelson

2
00:00:06,540 --> 00:00:08,700
changed naval warfare forever.

3
00:00:08,700 --> 00:00:11,320
And this is how naval warfare used to be,

4
00:00:11,320 --> 00:00:14,550
two fleets opposed to each
other, firing point blank

5
00:00:14,550 --> 00:00:18,750
until one gives up and
retires from the field.

6
00:00:18,750 --> 00:00:22,190
But that all changed in 1805.

7
00:00:22,190 --> 00:00:23,510
At the Battle of Trafalgar,

8
00:00:23,510 --> 00:00:26,360
Lord Admiral Nelson
ordered his entire fleet

9
00:00:26,360 --> 00:00:29,200
to dive straight at the enemy.

10
00:00:29,200 --> 00:00:31,169
You know what happens
when you send one fleet

11
00:00:31,170 --> 00:00:33,043
into another in naval warfare?

12
00:00:34,120 --> 00:00:37,970
A hot mess, chaos ensued,
friend and foe mixed together,

13
00:00:37,970 --> 00:00:40,839
it's hard to know what to do when,

14
00:00:40,840 --> 00:00:43,010
which is why Nelson invented

15
00:00:43,010 --> 00:00:46,099
an innovation authentication
method for this battle.

16
00:00:46,100 --> 00:00:47,660
He painted all of his ships

17
00:00:47,660 --> 00:00:50,750
in this black and white paint scheme,

18
00:00:50,750 --> 00:00:52,620
and that worked great
because of two reasons,

19
00:00:52,620 --> 00:00:56,280
one, it was accurate, only his
ships were painted like this,

20
00:00:56,280 --> 00:00:57,640
and it was easy to use.

21
00:00:57,640 --> 00:01:00,550
At a single glance,
every ship, every crew,

22
00:01:00,550 --> 00:01:02,500
every sailor could instantly see friend

23
00:01:02,500 --> 00:01:04,690
or foe and take action.

24
00:01:04,690 --> 00:01:06,840
But immediately after the battle,

25
00:01:06,840 --> 00:01:09,900
challenges started to arise
to this technique, right?

26
00:01:09,900 --> 00:01:13,760
As various navies started
impersonating the paint scheme,

27
00:01:13,760 --> 00:01:15,330
causing accuracy issues,

28
00:01:15,330 --> 00:01:18,260
and actually it was
too easy to use, right?

29
00:01:18,260 --> 00:01:20,700
As soon as an opponent knew the scheme,

30
00:01:20,700 --> 00:01:21,820
they could use the same thing

31
00:01:21,820 --> 00:01:24,500
and a single glance would give
them the same information.

32
00:01:24,500 --> 00:01:27,550
You couldn't restrict it to
only those people he wanted

33
00:01:27,550 --> 00:01:29,910
to be able to use his method.

34
00:01:29,910 --> 00:01:32,750
Now, facial recognition
in the last decade or so

35
00:01:32,750 --> 00:01:35,860
has been on a similar curve.

36
00:01:35,860 --> 00:01:40,560
It's accurate and easy to use
when it works well, right?

37
00:01:40,560 --> 00:01:44,320
A single glance has
given us authentication.

38
00:01:44,320 --> 00:01:47,210
And paired with the
rise of mobile devices,

39
00:01:47,210 --> 00:01:49,300
that's a very powerful combination

40
00:01:49,300 --> 00:01:51,080
because it's not out of band, right?

41
00:01:51,080 --> 00:01:51,913
What do you do?

42
00:01:51,913 --> 00:01:53,450
You pick up your phone, you look at it,

43
00:01:53,450 --> 00:01:57,600
and instantly you're strongly
authenticated locally

44
00:01:57,600 --> 00:02:00,559
to that device, which can
drive other innovations,

45
00:02:00,560 --> 00:02:03,380
such as serving
underprivileged populations

46
00:02:03,380 --> 00:02:05,020
who don't have as much access

47
00:02:05,020 --> 00:02:07,369
to financial products or insurance,

48
00:02:07,370 --> 00:02:10,190
and with or facial recognition on board,

49
00:02:10,190 --> 00:02:15,130
that can give them access to
new types of infrastructure.

50
00:02:15,130 --> 00:02:17,810
But just like the Nelson checker,

51
00:02:17,810 --> 00:02:20,580
this innovation's facing challenges.

52
00:02:20,580 --> 00:02:24,520
We've seen issues with accuracy
and bias being revealed.

53
00:02:24,520 --> 00:02:27,640
And in fact, it's too easy to use.

54
00:02:27,640 --> 00:02:30,890
There are problems with just
restricting facial recognition

55
00:02:30,890 --> 00:02:35,829
and to those people that you
want to do it on your behalf.

56
00:02:35,830 --> 00:02:37,710
Over the last several years,

57
00:02:37,710 --> 00:02:40,730
the Algorithmic Justice League has shown,

58
00:02:40,730 --> 00:02:42,399
along with the NIST and others,

59
00:02:42,400 --> 00:02:46,250
that there is bias in these
algorithms and these models,

60
00:02:46,250 --> 00:02:49,340
that minority populations are underserved

61
00:02:49,340 --> 00:02:52,620
and they are discriminated
against as a result.

62
00:02:52,620 --> 00:02:56,080
And that bias is fine on its
own and problematic on its own,

63
00:02:56,080 --> 00:02:58,680
but it's even worse when governments

64
00:02:58,680 --> 00:03:00,620
and law enforcement start to take it

65
00:03:00,620 --> 00:03:02,820
and wield it as part of their process.

66
00:03:02,820 --> 00:03:06,670
Russia, China, the U.K.
all have CCTV systems

67
00:03:06,670 --> 00:03:09,980
that can use facial recognition
to monitor citizens,

68
00:03:09,980 --> 00:03:12,899
Australia is using it
for COVID-19 tracking,

69
00:03:12,900 --> 00:03:15,650
and the U.S. government law enforcement

70
00:03:15,650 --> 00:03:18,300
has used video surveillance alone

71
00:03:18,300 --> 00:03:23,300
to arrest no fewer than
three black male suspects.

72
00:03:23,590 --> 00:03:25,770
Now, with that bias baked in,

73
00:03:25,770 --> 00:03:28,150
that becomes particularly dangerous.

74
00:03:28,150 --> 00:03:29,840
And then, there's the other side, right?

75
00:03:29,840 --> 00:03:33,020
The ease of use being too easy to use.

76
00:03:33,020 --> 00:03:34,960
In the last year or so, we've learned

77
00:03:34,960 --> 00:03:36,730
that there are companies who are going out

78
00:03:36,730 --> 00:03:38,730
and taking photos that
people have uploaded

79
00:03:38,730 --> 00:03:40,970
to Facebook, Google, Instagram,

80
00:03:40,970 --> 00:03:44,730
and scraping those off and
building their own repository,

81
00:03:44,730 --> 00:03:47,450
and then selling facial
recognition databases

82
00:03:47,450 --> 00:03:49,420
that result to law enforcement.

83
00:03:49,420 --> 00:03:52,130
It's hard to keep just those who you want

84
00:03:52,130 --> 00:03:53,540
to have access to it.

85
00:03:53,540 --> 00:03:56,590
And even if they're not
selling it to law enforcement,

86
00:03:56,590 --> 00:04:00,670
anyone keeping a database
of facial information

87
00:04:00,670 --> 00:04:04,179
is under danger of a breach,

88
00:04:04,180 --> 00:04:06,330
like the Customs and Border patrol

89
00:04:06,330 --> 00:04:09,170
had happened in 2019 in the United States.

90
00:04:09,170 --> 00:04:10,959
Now, governments and legislatures

91
00:04:10,960 --> 00:04:13,370
are trying to catch up, right?

92
00:04:13,370 --> 00:04:16,610
The United States has in
Illinois, the state of Illinois

93
00:04:16,610 --> 00:04:21,610
has a law preventing
biometric misuse of data,

94
00:04:21,660 --> 00:04:24,130
and Canada and Sweden
among other countries

95
00:04:24,130 --> 00:04:26,219
are working to make
sure that organizations

96
00:04:26,220 --> 00:04:29,250
such as Clearview AI are
not allowed to operate

97
00:04:29,250 --> 00:04:34,250
in their jurisdiction, but
that's more reactive, right?

98
00:04:34,990 --> 00:04:38,330
What this calls for, if our
ethics are to be defended

99
00:04:38,330 --> 00:04:40,490
is something a bit more proactive.

100
00:04:40,490 --> 00:04:44,300
And that's where adversarial
research comes into play.

101
00:04:44,300 --> 00:04:46,090
Now, over the past several years,

102
00:04:46,090 --> 00:04:49,380
we've seen adversarial
research address the issue

103
00:04:49,380 --> 00:04:50,610
of bias, right?

104
00:04:50,610 --> 00:04:52,440
To figure out where it's flawed,

105
00:04:52,440 --> 00:04:56,070
where the model was flawed
and improve that accuracy.

106
00:04:56,070 --> 00:04:58,380
But in the last six months to a year,

107
00:04:58,380 --> 00:05:01,140
there's been a whole
adversarial research expansion

108
00:05:01,140 --> 00:05:03,960
into the ease of use aspect,

109
00:05:03,960 --> 00:05:08,450
trying to restrict facial
data, facial recognition,

110
00:05:08,450 --> 00:05:12,700
to be used only by those that
people authorize them to do,

111
00:05:12,700 --> 00:05:15,430
in other words, providing
some individuals some kind

112
00:05:15,430 --> 00:05:18,740
of agency, even as they
upload their photographs

113
00:05:18,740 --> 00:05:22,240
or their snapshots to
these social media sites

114
00:05:22,240 --> 00:05:26,283
to give them some form of
protection against misuse.

115
00:05:27,690 --> 00:05:29,530
And that's what we'll talk about today.

116
00:05:29,530 --> 00:05:32,159
We'll explore the latest research,

117
00:05:32,160 --> 00:05:34,100
the pros and cons of how they work.

118
00:05:34,100 --> 00:05:36,610
And then we'll introduce
an open source proof

119
00:05:36,610 --> 00:05:39,990
of concept mobile app that
uses some of these techniques

120
00:05:39,990 --> 00:05:41,040
to do just that,

121
00:05:41,040 --> 00:05:45,460
to give people some agency in
protecting their own privacy.

122
00:05:45,460 --> 00:05:47,760
Now, looking back at
the adversarial approach

123
00:05:47,760 --> 00:05:48,593
that have taken place,

124
00:05:48,593 --> 00:05:52,300
this kinda started somewhat back in 2001,

125
00:05:52,300 --> 00:05:55,940
where there was a geometric
approach where it changed pixels

126
00:05:55,940 --> 00:05:59,080
within these photographs and
forded facial recognition,

127
00:05:59,080 --> 00:06:00,800
this worked surprisingly well.

128
00:06:00,800 --> 00:06:02,500
It was cheap, it was fast,

129
00:06:02,500 --> 00:06:05,860
but it was static and facial
recognition quickly caught up

130
00:06:05,860 --> 00:06:10,300
in the cat and mouse game
that is adversarial research.

131
00:06:10,300 --> 00:06:14,300
Then came general adversarial networks,

132
00:06:14,300 --> 00:06:17,420
which are actually two different
machine learning models

133
00:06:17,420 --> 00:06:20,250
that are competing and
kind of working together.

134
00:06:20,250 --> 00:06:22,870
There's a generator and
the discriminator, right?

135
00:06:22,870 --> 00:06:27,030
And the generator is
creating face-based images

136
00:06:27,030 --> 00:06:29,349
that is trying to make
as real as possible,

137
00:06:29,350 --> 00:06:30,450
and then the discriminator,

138
00:06:30,450 --> 00:06:32,650
which has been trained on real images,

139
00:06:32,650 --> 00:06:35,599
is trying to differentiate
between the reals and the fakes.

140
00:06:35,600 --> 00:06:39,020
These compete with each other,
and it's a feedback loop.

141
00:06:39,020 --> 00:06:41,849
You get more and more realistic images,

142
00:06:41,850 --> 00:06:44,310
and you get things like these.

143
00:06:44,310 --> 00:06:48,770
You may have seen this on
thispersondoesnotexist.com,

144
00:06:48,770 --> 00:06:51,580
and the high level
attributes are attained,

145
00:06:51,580 --> 00:06:53,609
the low level attributes are modified.

146
00:06:53,610 --> 00:06:57,040
These are wholly synthetic people often.

147
00:06:57,040 --> 00:07:00,280
And this works pretty great,
except for a few problems.

148
00:07:00,280 --> 00:07:02,070
One, it's static, in other words,

149
00:07:02,070 --> 00:07:04,050
you could use this as your avatar,

150
00:07:04,050 --> 00:07:05,790
as your replacement face online,

151
00:07:05,790 --> 00:07:08,350
but then it would be
one-to-one tied back to you

152
00:07:08,350 --> 00:07:09,867
and you'd be back to square one,

153
00:07:09,867 --> 00:07:12,823
the protection will be
static, and it's expensive,

154
00:07:13,821 --> 00:07:17,490
and you require a processing
power that you may not have,

155
00:07:17,490 --> 00:07:19,640
and it wouldn't be as easy as on a,

156
00:07:19,640 --> 00:07:21,772
let's say face ID on a mobile device.

157
00:07:23,060 --> 00:07:25,920
Now, July of last year,

158
00:07:25,920 --> 00:07:29,600
researchers from Australia
introduced Camera Adversaria.

159
00:07:29,600 --> 00:07:31,800
And what this does is it takes images

160
00:07:31,800 --> 00:07:35,080
and it introduces procedural noise,

161
00:07:35,080 --> 00:07:38,130
natural looking noise into an image.

162
00:07:38,130 --> 00:07:41,070
And it disturbs the classification system.

163
00:07:41,070 --> 00:07:43,310
It's an Android app, it's a camera app,

164
00:07:43,310 --> 00:07:46,040
and so, it's in the flow
and it, like I said,

165
00:07:46,040 --> 00:07:47,740
it perturbs the classification.

166
00:07:47,740 --> 00:07:49,573
Let's see this in action.

167
00:07:51,350 --> 00:07:54,310
The Camera Adversaria
app is an Android app,

168
00:07:54,310 --> 00:07:57,290
obviously, that uses the on-device camera.

169
00:07:57,290 --> 00:07:59,510
Taking a photo or, as in this case,

170
00:07:59,510 --> 00:08:01,370
selecting a preexisting photo

171
00:08:01,370 --> 00:08:03,310
from the dedicated collection,

172
00:08:03,310 --> 00:08:06,460
results in the image being
displayed in the main view.

173
00:08:06,460 --> 00:08:09,950
A classifier runs on the
device itself mobile net based,

174
00:08:09,950 --> 00:08:14,260
and gives the top
classification of the image.

175
00:08:14,260 --> 00:08:16,800
Simplex noise supplied in a filter

176
00:08:16,800 --> 00:08:19,660
via the GPU image library

177
00:08:19,660 --> 00:08:22,070
and then the object is reclassified

178
00:08:22,070 --> 00:08:24,450
after the image is redrawn.

179
00:08:24,450 --> 00:08:26,330
Here you can see that by moving the slider

180
00:08:26,330 --> 00:08:30,719
to a higher setting, the
noise begins to reveal itself

181
00:08:30,720 --> 00:08:33,653
from the image and distortion results.

182
00:08:35,169 --> 00:08:37,209
Leatherback turtle, chain mail,

183
00:08:37,210 --> 00:08:39,470
and other options are identified

184
00:08:39,470 --> 00:08:41,850
as the noise affects the classifier

185
00:08:41,850 --> 00:08:46,770
and the top result changes
due to the perturbations.

186
00:08:46,770 --> 00:08:49,130
So, that's a great approach.

187
00:08:49,130 --> 00:08:52,530
It's quick, it's simple, it's
direct, it's a mobile app.

188
00:08:52,530 --> 00:08:53,990
Around the same time,

189
00:08:53,990 --> 00:08:57,230
a new method appeared from
the University of Chicago,

190
00:08:57,230 --> 00:09:00,270
and this was called
Fawkes after Guy Fawkes.

191
00:09:00,270 --> 00:09:03,490
And what this does is it
takes your original images

192
00:09:03,490 --> 00:09:05,920
that you've taken or that
you have, your faces,

193
00:09:05,920 --> 00:09:08,540
and then it combines
them with a collection

194
00:09:08,540 --> 00:09:10,490
of faces from a particular target,

195
00:09:10,490 --> 00:09:12,970
a different individual that's not you.

196
00:09:12,970 --> 00:09:14,540
The feature sets are combined.

197
00:09:14,540 --> 00:09:18,110
You have cloaked images,
which are then made available

198
00:09:18,110 --> 00:09:20,340
to these facial recognition systems,

199
00:09:20,340 --> 00:09:23,230
which take them in as training data.

200
00:09:23,230 --> 00:09:25,920
And then, when your real
photos are out there,

201
00:09:25,920 --> 00:09:29,680
it misclassified you as
the target individual.

202
00:09:29,680 --> 00:09:32,579
It uses a loss function to optimize.

203
00:09:32,580 --> 00:09:36,750
It's a Mac and Windows application
that's been open-sourced.

204
00:09:36,750 --> 00:09:37,970
And so, what you kind of have to do

205
00:09:37,970 --> 00:09:39,600
is you have to either download the binary

206
00:09:39,600 --> 00:09:41,850
or download it and compile it yourself.

207
00:09:41,850 --> 00:09:43,800
So, there's a bit of a usability issue.

208
00:09:43,800 --> 00:09:45,770
But let's take a look here,

209
00:09:45,770 --> 00:09:48,460
and what it actually does, as I said,

210
00:09:48,460 --> 00:09:52,020
is it tries to shift the feature set

211
00:09:52,020 --> 00:09:54,660
so that your feature set
actually gets intermingled

212
00:09:54,660 --> 00:09:57,439
and miss-identified and classified wrong.

213
00:09:57,440 --> 00:10:01,240
Keep in mind that kind of
like the Perlin noise option,

214
00:10:01,240 --> 00:10:03,250
what they're trying to
do is modify the photos

215
00:10:03,250 --> 00:10:06,180
so to a human it looks
realistic, it looks normal,

216
00:10:06,180 --> 00:10:08,069
but to a facial recognition system,

217
00:10:08,070 --> 00:10:10,073
it presents great difficulty.

218
00:10:12,740 --> 00:10:14,730
It's always helpful to
see these tools in action.

219
00:10:14,730 --> 00:10:16,960
And so, I'm gonna use the IntelliJ IDEA,

220
00:10:16,960 --> 00:10:20,730
and I've downloaded the
Fawkes open-source tool.

221
00:10:20,730 --> 00:10:23,160
And we'll look through the
code, see, kind of get a sense

222
00:10:23,160 --> 00:10:25,670
of what it's doing, and
then we'll actually see it

223
00:10:25,670 --> 00:10:29,589
in action and what it produces over time.

224
00:10:29,590 --> 00:10:34,590
Most of the actual meat
of the tool takes place

225
00:10:35,690 --> 00:10:37,750
in this Fawkes object, of course.

226
00:10:37,750 --> 00:10:41,650
It's got some initialization,
looking for GPU,

227
00:10:41,650 --> 00:10:43,270
setting up TensorFlow.

228
00:10:43,270 --> 00:10:45,910
The tool is designed for TensorFlow One.

229
00:10:45,910 --> 00:10:46,742
And so, you'll see a lot

230
00:10:46,743 --> 00:10:50,250
of backwards compatibility
in the TensorFlow commands.

231
00:10:50,250 --> 00:10:53,030
Note that what it's gonna
use to detect the faces

232
00:10:53,030 --> 00:10:54,569
and for feature extraction

233
00:10:54,570 --> 00:10:57,720
and some of the comparison is MTCNN.

234
00:10:57,720 --> 00:11:01,600
Of course, it's that Multi-task
Cascaded Neural Network,

235
00:11:01,600 --> 00:11:04,003
mouthful which is why
they have the acronym.

236
00:11:05,190 --> 00:11:06,500
And then of course,

237
00:11:06,500 --> 00:11:11,500
the tool has multiple
levels from min to ultra.

238
00:11:11,830 --> 00:11:15,820
And this impacts how
dissimilar the resulting image,

239
00:11:15,820 --> 00:11:20,780
how much protection is
embedded in the final image.

240
00:11:20,780 --> 00:11:23,490
It's setting things like
the similarity threshold,

241
00:11:23,490 --> 00:11:25,710
which is that TH variable you see there,

242
00:11:25,710 --> 00:11:28,507
and then the step size
and the learning rate

243
00:11:28,507 --> 00:11:32,900
for the training that will take place.

244
00:11:32,900 --> 00:11:35,870
What it does first is it goes
ahead and detects the faces,

245
00:11:35,870 --> 00:11:38,840
crops them, aligns them if it's desired,

246
00:11:38,840 --> 00:11:41,600
it doesn't have as much of
an impact as you might think.

247
00:11:41,600 --> 00:11:46,600
So, the alignment is
turned off by default.

248
00:11:47,170 --> 00:11:51,610
Once it gets those images,
it's gonna go through

249
00:11:51,610 --> 00:11:54,630
and find the target embedding
for all of those images

250
00:11:54,630 --> 00:11:57,480
and kinda batch them up together,

251
00:11:57,480 --> 00:12:01,010
because what it really wants
to do is it wants to go

252
00:12:01,010 --> 00:12:04,390
and find what target to use.

253
00:12:04,390 --> 00:12:07,600
And what it's doing is it's
looking for another image

254
00:12:07,600 --> 00:12:09,870
to then use that other image's feature set

255
00:12:09,870 --> 00:12:12,620
and make the image that
you want to protect

256
00:12:12,620 --> 00:12:14,470
or the face that you want to cloak,

257
00:12:14,470 --> 00:12:18,770
make the feature set result
be a lot more similar

258
00:12:18,770 --> 00:12:21,160
to the feature set of that target image.

259
00:12:21,160 --> 00:12:22,810
And so to do that,

260
00:12:22,810 --> 00:12:27,280
it actually calls the select target label.

261
00:12:27,280 --> 00:12:29,790
And so, what it does is it goes out

262
00:12:29,790 --> 00:12:34,790
and it picks out a random
set of potential images

263
00:12:36,570 --> 00:12:39,830
from a hosted data set

264
00:12:39,830 --> 00:12:43,380
out on the University
of Chicago's servers,

265
00:12:43,380 --> 00:12:46,010
and what it does is it
measures the distance

266
00:12:48,010 --> 00:12:51,370
for each of them that is most dissimilar

267
00:12:51,370 --> 00:12:54,870
to all the target images taken as a whole.

268
00:12:54,870 --> 00:12:59,050
Then it actually goes ahead
and sets up the protector.

269
00:12:59,050 --> 00:13:03,740
If we go look at that, it's
got the variables in here,

270
00:13:03,740 --> 00:13:08,740
it is also converting things to 10H space.

271
00:13:09,760 --> 00:13:10,650
And then down here,

272
00:13:10,650 --> 00:13:15,650
it's using the SSIM to do the difference,

273
00:13:17,200 --> 00:13:19,060
the user perceived difference

274
00:13:19,060 --> 00:13:21,357
between the real image
and the modified image.

275
00:13:21,357 --> 00:13:22,920
And so, what it's doing

276
00:13:22,920 --> 00:13:25,640
that's the structural dissimilarity index,

277
00:13:25,640 --> 00:13:28,540
and so what they're trying
to do is they are taking it

278
00:13:28,540 --> 00:13:33,540
so the images are different
than what they originally were,

279
00:13:33,660 --> 00:13:36,660
especially to the machine vision aspect,

280
00:13:36,660 --> 00:13:40,560
but then not so different that you look

281
00:13:40,560 --> 00:13:42,709
like a totally different
person to a human.

282
00:13:44,260 --> 00:13:45,180
And so, what they're gonna do

283
00:13:45,180 --> 00:13:48,132
is they're gonna set up
their losses down here,

284
00:13:49,320 --> 00:13:52,450
that they're going to optimize

285
00:13:52,450 --> 00:13:57,450
using add a delta stochastic
gradient descent method.

286
00:13:57,780 --> 00:14:00,319
And basically, they're
baking into that loss

287
00:14:00,320 --> 00:14:04,750
that human perceivability component

288
00:14:04,750 --> 00:14:07,330
to make sure they're not too far afield

289
00:14:07,330 --> 00:14:09,253
with the resulting image.

290
00:14:10,770 --> 00:14:15,069
So, after all that takes place
and everything is set up,

291
00:14:15,070 --> 00:14:18,730
actually the attack method
is eventually called.

292
00:14:18,730 --> 00:14:20,380
And that basically sets it up

293
00:14:20,380 --> 00:14:23,680
to where it's gonna go through
each of the images in turn,

294
00:14:23,680 --> 00:14:28,680
and generate a cloak, hand
that off to attack batch,

295
00:14:29,290 --> 00:14:32,920
which is actually going
to do the actual session

296
00:14:32,920 --> 00:14:34,209
and the training.

297
00:14:34,210 --> 00:14:36,780
And so, what results is you get a set

298
00:14:36,780 --> 00:14:41,780
of images that have modified
faces that are trying

299
00:14:41,850 --> 00:14:45,530
to protect those faces from
facial recognition, obviously,

300
00:14:45,530 --> 00:14:48,150
and eventually the faces
are emerged back in,

301
00:14:48,150 --> 00:14:50,170
and they're labeled and
they're written out.

302
00:14:50,170 --> 00:14:51,002
Now, I'm gonna go ahead

303
00:14:51,003 --> 00:14:53,670
and run the tool on the minimum level,

304
00:14:53,670 --> 00:14:55,640
just to give you a sense
of what it looks like

305
00:14:55,640 --> 00:14:57,550
and how it goes.

306
00:14:57,550 --> 00:15:01,180
And so, when we run it,
well, you can see it load up.

307
00:15:01,180 --> 00:15:03,810
And then, what it will
first do is it will go

308
00:15:03,810 --> 00:15:06,869
and find the images that
it wants to protect,

309
00:15:06,870 --> 00:15:08,570
which is an a default directory.

310
00:15:08,570 --> 00:15:10,530
So, the way you would do
this is you would run it

311
00:15:10,530 --> 00:15:13,130
on images that are already
on your hard drive,

312
00:15:13,130 --> 00:15:16,630
and then apply the cloak
protection, and then upload those,

313
00:15:16,630 --> 00:15:18,230
but just kind of inconvenient

314
00:15:18,230 --> 00:15:20,620
and just why we made a
mobile app for the other.

315
00:15:20,620 --> 00:15:22,430
You can see it find the target

316
00:15:22,430 --> 00:15:24,992
that it wants to use to
shift the feature set.

317
00:15:25,860 --> 00:15:30,860
You can see it here, setting
up the mass generation function

318
00:15:32,090 --> 00:15:34,670
to set up the TensorFlow information

319
00:15:34,670 --> 00:15:36,770
and the functions it's gonna use.

320
00:15:36,770 --> 00:15:41,640
And now, it starts to go
through each of the images,

321
00:15:41,640 --> 00:15:44,010
and it's training, it's developing a cloak

322
00:15:44,010 --> 00:15:47,960
and applying that cloak
for one image at a time.

323
00:15:47,960 --> 00:15:49,490
And we'll go ahead and speed this up

324
00:15:49,490 --> 00:15:51,443
so you don't have to
watch the whole thing.

325
00:15:52,910 --> 00:15:55,110
So, you can see it took about 83 seconds

326
00:15:55,110 --> 00:15:57,307
to do those four images.

327
00:15:57,307 --> 00:16:00,070
And this gets a lot longer depending

328
00:16:00,070 --> 00:16:02,580
on the quality of cloaking

329
00:16:02,580 --> 00:16:06,600
or what level you want
to defend your images.

330
00:16:06,600 --> 00:16:09,690
Looking at what those
images actually look like,

331
00:16:09,690 --> 00:16:13,650
we see the original
image that's unmodified,

332
00:16:13,650 --> 00:16:15,640
and then we can cycle through.

333
00:16:15,640 --> 00:16:19,260
And you can see that the minimum image

334
00:16:19,260 --> 00:16:21,010
has a little bit of modification,

335
00:16:21,010 --> 00:16:22,970
especially around some of the features,

336
00:16:22,970 --> 00:16:25,050
but it's especially in small format,

337
00:16:25,050 --> 00:16:27,069
it's not that noticeable.

338
00:16:27,070 --> 00:16:29,200
This one took 88 seconds or so,

339
00:16:29,200 --> 00:16:30,230
about a minute and a half.

340
00:16:30,230 --> 00:16:32,570
When we flip up to low protection,

341
00:16:32,570 --> 00:16:35,030
you see the artifacts
start to get stronger.

342
00:16:35,030 --> 00:16:38,770
And this was almost a
three minute runtime.

343
00:16:38,770 --> 00:16:41,600
When we go up to the
mid level of protection,

344
00:16:41,600 --> 00:16:44,340
now you're talking in
the order of 10 minutes

345
00:16:45,618 --> 00:16:48,350
for four images in this case,

346
00:16:48,350 --> 00:16:50,030
but it still kinda gets prohibitive.

347
00:16:50,030 --> 00:16:53,470
And then of course, the high
level modifies it greatly.

348
00:16:53,470 --> 00:16:57,370
I kinda resemble a cartoon
villain from the old days.

349
00:16:57,370 --> 00:17:00,610
And for four images, it took 22 minutes.

350
00:17:00,610 --> 00:17:03,950
So, five minutes per
image on the top level.

351
00:17:03,950 --> 00:17:06,550
Usability then is a key problem here

352
00:17:06,550 --> 00:17:08,349
because that's gonna not only do you have

353
00:17:08,349 --> 00:17:10,030
to download the tool and run it,

354
00:17:10,030 --> 00:17:13,403
but it's a time consuming process.

355
00:17:14,380 --> 00:17:15,839
Looking at the effectiveness

356
00:17:15,839 --> 00:17:19,010
as Fawkes's larger numbers
here are better obviously,

357
00:17:19,010 --> 00:17:19,950
they tested against

358
00:17:19,950 --> 00:17:22,319
the three large facial
recognition systems,

359
00:17:22,319 --> 00:17:25,348
Microsoft's, Amazon's, and Face++.

360
00:17:25,348 --> 00:17:28,919
And you can see they did the
best against Microsoft's.

361
00:17:28,920 --> 00:17:32,390
Now, it's a balance between
usability and protection.

362
00:17:32,390 --> 00:17:34,110
And in January, it should be noted

363
00:17:34,110 --> 00:17:36,500
that the authors went back and noticed

364
00:17:36,500 --> 00:17:38,740
that the data has changed against Azure.

365
00:17:38,740 --> 00:17:40,630
And they think that Microsoft

366
00:17:40,630 --> 00:17:42,700
may have trained their model slightly

367
00:17:42,700 --> 00:17:46,660
to improve it's defense
against something like Fawkes.

368
00:17:46,660 --> 00:17:50,480
Now at the same time,
Lowkey was being worked on.

369
00:17:50,480 --> 00:17:52,770
And this was from the
University of Maryland.

370
00:17:52,770 --> 00:17:53,863
It was announced earlier this year it

371
00:17:53,863 --> 00:17:55,830
will be presented in May.

372
00:17:55,830 --> 00:17:59,010
It's using a similar process
to do an adversarial attack

373
00:17:59,010 --> 00:18:01,629
on image, pollute the training data.

374
00:18:01,630 --> 00:18:05,460
It uses a slightly
different loss function,

375
00:18:05,460 --> 00:18:09,600
and one that incorporates
not just classification

376
00:18:09,600 --> 00:18:13,060
of the image but also
detection in the whole process.

377
00:18:13,060 --> 00:18:15,379
Keep in mind that this is not open source.

378
00:18:15,380 --> 00:18:18,680
It's a hosted web service
where you submit your pictures,

379
00:18:18,680 --> 00:18:20,900
and then you get them emailed back to you.

380
00:18:20,900 --> 00:18:23,360
So, it's kind of out of band.

381
00:18:23,360 --> 00:18:25,580
The effectiveness, it's important to note

382
00:18:25,580 --> 00:18:29,100
that here it is lower
numbers that are better,

383
00:18:29,100 --> 00:18:31,020
because what they're
doing is they're using it

384
00:18:31,020 --> 00:18:32,270
like law enforcement would.

385
00:18:32,270 --> 00:18:34,870
In other words, they're not
doing a one-to-one match.

386
00:18:34,870 --> 00:18:35,760
They're looking and saying,

387
00:18:35,760 --> 00:18:37,690
I've got an image of a suspect

388
00:18:37,690 --> 00:18:39,060
that I want to know who it is,

389
00:18:39,060 --> 00:18:41,700
return to me the top N suspects

390
00:18:41,700 --> 00:18:43,350
that I need to go in and investigate.

391
00:18:43,350 --> 00:18:44,189
And so, you can see

392
00:18:44,190 --> 00:18:47,520
that Lowkey has a higher
resolution and it performs better

393
00:18:47,520 --> 00:18:52,520
with their algorithm and
their testing than Fawkes did.

394
00:18:52,970 --> 00:18:54,090
Now, looking at all of these,

395
00:18:54,090 --> 00:18:57,350
you can begin to categorize
and summarize some of them

396
00:18:57,350 --> 00:18:59,480
and their are different approaches.

397
00:18:59,480 --> 00:19:02,700
And the open source app that
we're going to propose tries

398
00:19:02,700 --> 00:19:05,010
to use the best of all of these worlds.

399
00:19:05,010 --> 00:19:08,160
We want a mobile application format

400
00:19:08,160 --> 00:19:10,510
because that helps with
ease of use, right?

401
00:19:10,510 --> 00:19:12,690
If it's not on a film,
people won't use it.

402
00:19:12,690 --> 00:19:15,150
If it's not in the flow
of taking a picture,

403
00:19:15,150 --> 00:19:16,640
people won't use it.

404
00:19:16,640 --> 00:19:19,770
Along with that, we
want dynamic protection.

405
00:19:19,770 --> 00:19:21,940
We want a protection for
each image on its own,

406
00:19:21,940 --> 00:19:24,850
not just a static approach
that can be countermanded.

407
00:19:24,850 --> 00:19:27,850
We want it to look like
the person to your human,

408
00:19:27,850 --> 00:19:28,770
to be useful.

409
00:19:28,770 --> 00:19:31,440
We want it to be cheap
because of that mobile format.

410
00:19:31,440 --> 00:19:32,370
And then ideally,

411
00:19:32,370 --> 00:19:34,800
we want to protect against
not just classification

412
00:19:34,800 --> 00:19:39,080
but also detection, the other
part of facial recognition.

413
00:19:39,080 --> 00:19:41,149
And so, that's what we're doing.

414
00:19:41,150 --> 00:19:44,130
We're building a mobile app
here that tries to be useful.

415
00:19:44,130 --> 00:19:48,350
It's usable as a camera
app to normal people,

416
00:19:48,350 --> 00:19:50,290
which also means that
we're limited to mobile,

417
00:19:50,290 --> 00:19:52,360
which means that we don't have TensorFlow.

418
00:19:52,360 --> 00:19:54,550
We only have TensorFlow Lite,

419
00:19:54,550 --> 00:19:57,430
which means that training
is very limited for now

420
00:19:57,430 --> 00:19:58,650
on mobile devices.

421
00:19:58,650 --> 00:20:00,790
And we're gonna release this on GitHub,

422
00:20:00,790 --> 00:20:02,070
but again, for usability,

423
00:20:02,070 --> 00:20:04,700
we would push this like
the Camera Adversaria app

424
00:20:04,700 --> 00:20:07,500
into app stores for download and usage.

425
00:20:07,500 --> 00:20:09,940
The flow of the application
is in three phases.

426
00:20:09,940 --> 00:20:13,960
The first is we'll take the
image and we'll apply a style

427
00:20:13,960 --> 00:20:16,930
to it using arbitrary fast style transfer.

428
00:20:16,930 --> 00:20:18,490
Once we have the stylized image,

429
00:20:18,490 --> 00:20:20,480
then we'll inject some procedural noise

430
00:20:20,480 --> 00:20:22,170
to disrupt the classification.

431
00:20:22,170 --> 00:20:23,240
Then we'll have a final image

432
00:20:23,240 --> 00:20:25,280
and we'll do a sanity check
to see how we're doing

433
00:20:25,280 --> 00:20:27,780
against facial detection and recognition.

434
00:20:27,780 --> 00:20:29,550
And then, we will save it to an album

435
00:20:29,550 --> 00:20:32,110
or allow the user to upload it from there.

436
00:20:32,110 --> 00:20:34,860
The first phase is arbitrary
fast style transfer,

437
00:20:34,860 --> 00:20:39,780
and what this does is it applies
a style to a content image.

438
00:20:39,780 --> 00:20:41,980
You have a content
image and a style image,

439
00:20:41,980 --> 00:20:45,100
and actually two different inferences run,

440
00:20:45,100 --> 00:20:47,010
the first is a style prediction model

441
00:20:47,010 --> 00:20:48,820
which has been trained on other styles.

442
00:20:48,820 --> 00:20:53,820
It extracts the top layer of
that model into a bottleneck.

443
00:20:54,050 --> 00:20:57,210
And now, you have the style
from that style image,

444
00:20:57,210 --> 00:20:58,480
which you can then apply

445
00:20:58,480 --> 00:21:01,490
on the content image with
the style transform model.

446
00:21:01,490 --> 00:21:04,040
It's kinda a paint by numbers technique.

447
00:21:04,040 --> 00:21:07,310
Now, if we run both the style
image and the content image

448
00:21:07,310 --> 00:21:09,550
through that first style prediction model,

449
00:21:09,550 --> 00:21:11,899
now we have the styles of the style image

450
00:21:11,900 --> 00:21:13,100
and the content image,

451
00:21:13,100 --> 00:21:14,629
and we can begin to play with it

452
00:21:14,630 --> 00:21:17,993
and choose how we want it to
appear in the final image.

453
00:21:20,250 --> 00:21:21,880
We're gonna use a Jupyter notebook

454
00:21:21,880 --> 00:21:25,730
to show you the algorithmic
flow that takes place

455
00:21:25,730 --> 00:21:29,180
as we go through the core
of the open source app

456
00:21:29,180 --> 00:21:30,450
that I've developed.

457
00:21:30,450 --> 00:21:32,350
And so, really here
what you can already see

458
00:21:32,350 --> 00:21:34,520
is we're just importing
the table stakes, right?

459
00:21:34,520 --> 00:21:36,812
Python 38 and TensorFlow 24.

460
00:21:37,780 --> 00:21:39,990
In the actual iOS app of course,

461
00:21:39,990 --> 00:21:41,810
we'll be using TensorFlow Lite

462
00:21:41,810 --> 00:21:45,770
and we'll use a nightly build
with the metal parameter

463
00:21:45,770 --> 00:21:49,530
to make sure we can get
the GPU acceleration

464
00:21:49,530 --> 00:21:53,562
on the iOS app itself, that
we're building an X Code.

465
00:21:54,410 --> 00:21:56,120
Next step is to include a couple

466
00:21:56,120 --> 00:21:59,739
of libraries that will be
helpful to do some math,

467
00:21:59,740 --> 00:22:02,910
some time calculations,
so forth and so on.

468
00:22:02,910 --> 00:22:06,570
Some of these are available
in X Code, not all of them,

469
00:22:06,570 --> 00:22:08,689
so some of it we'll have
to do some hand rolling

470
00:22:08,690 --> 00:22:10,480
of functions ourselves.

471
00:22:10,480 --> 00:22:12,630
What we'll also do is we'll go out

472
00:22:12,630 --> 00:22:16,090
and grab a couple of things
for the deministration.

473
00:22:16,090 --> 00:22:19,860
The first is the face,
the picture of the person

474
00:22:19,860 --> 00:22:21,629
that we want to manipulate,

475
00:22:21,630 --> 00:22:24,560
to seek to protect in
an adversarial fashion,

476
00:22:24,560 --> 00:22:28,129
and the other is the style
that we want to transfer

477
00:22:28,130 --> 00:22:30,610
onto that person's face.

478
00:22:30,610 --> 00:22:33,709
And then finally, we will go

479
00:22:33,710 --> 00:22:38,390
and get both the prediction model

480
00:22:38,390 --> 00:22:40,810
as well as the transfer model.

481
00:22:40,810 --> 00:22:43,409
Both of these have been
trained, and we'll use these,

482
00:22:43,410 --> 00:22:46,870
since we have a GPU
available to us on iOS,

483
00:22:46,870 --> 00:22:49,100
we use a float 16 version of these,

484
00:22:49,100 --> 00:22:52,179
and that will give us a faster transfer.

485
00:22:52,180 --> 00:22:55,610
Note that the style and the models

486
00:22:55,610 --> 00:22:58,179
are all resident in the app.

487
00:22:58,180 --> 00:23:00,570
There are a couple of
sample faces on the iOS app,

488
00:23:00,570 --> 00:23:02,950
but also you can use anything
from the camera roll,

489
00:23:02,950 --> 00:23:04,760
and you can also just take a picture live

490
00:23:04,760 --> 00:23:06,260
as you normally would.

491
00:23:06,260 --> 00:23:10,070
So, the next step is to
pre-process the images

492
00:23:10,070 --> 00:23:12,250
that we want to use,
both content and style

493
00:23:12,250 --> 00:23:14,340
to prepare them for the model.

494
00:23:14,340 --> 00:23:15,850
They need to be RGB images

495
00:23:15,850 --> 00:23:18,879
with pixel values being float 32 numbers

496
00:23:18,880 --> 00:23:23,850
between zero to one and a particular size.

497
00:23:23,850 --> 00:23:26,840
The style image has to be 256 by 256.

498
00:23:26,840 --> 00:23:30,959
And the content image must be 384 by 384.

499
00:23:30,960 --> 00:23:34,083
Luckily, TensorFlow gives us a
lot of these methods already.

500
00:23:35,040 --> 00:23:38,090
Like for example, the
decode image you see there,

501
00:23:38,090 --> 00:23:41,560
and converting just between data types.

502
00:23:41,560 --> 00:23:42,760
Some of these will be available

503
00:23:42,760 --> 00:23:45,270
but a lot of these we'll have to use

504
00:23:45,270 --> 00:23:50,070
with Swift's own UI handling
and graphics processing.

505
00:23:50,070 --> 00:23:52,649
Finally, we print out
the shape of the images

506
00:23:52,650 --> 00:23:54,460
just to make sure we're ready to go

507
00:23:54,460 --> 00:23:56,710
and they're ready to
be used with the model,

508
00:23:56,710 --> 00:23:58,343
and you can see the
appropriate shape there

509
00:23:58,343 --> 00:23:59,883
that models expect.

510
00:24:01,805 --> 00:24:04,740
The next step here is just to print out

511
00:24:04,740 --> 00:24:07,950
or visually display in
the notebook the content

512
00:24:07,950 --> 00:24:10,763
and the style image
that's been pre-processed.

513
00:24:10,763 --> 00:24:12,419
Just make sure it's a sanity check.

514
00:24:12,420 --> 00:24:13,620
Make sure it looks like we think

515
00:24:13,620 --> 00:24:16,322
before we actually run
them through the model.

516
00:24:17,870 --> 00:24:19,820
The next step is pretty straightforward.

517
00:24:19,820 --> 00:24:23,080
What we do is we take our
pre-processed style image

518
00:24:23,080 --> 00:24:26,129
and place it on the input tensor

519
00:24:26,130 --> 00:24:28,960
for our style prediction model.

520
00:24:28,960 --> 00:24:31,320
We run it through the model,
we run in prints on it.

521
00:24:31,320 --> 00:24:35,290
And that reduces that style
image down to a bottleneck

522
00:24:35,290 --> 00:24:38,370
that we can use in our transfer model

523
00:24:38,370 --> 00:24:42,280
to apply that style to our content image.

524
00:24:42,280 --> 00:24:44,590
Now that we have our style bottleneck

525
00:24:44,590 --> 00:24:47,689
and we already have our
pre-processed content image,

526
00:24:47,690 --> 00:24:49,560
we can now feed these

527
00:24:49,560 --> 00:24:54,080
into the input tensors
for our style transform

528
00:24:54,080 --> 00:24:55,850
or transform model,

529
00:24:55,850 --> 00:25:00,750
and we actually get a
stylized content image.

530
00:25:00,750 --> 00:25:03,790
It's 384 by 384 as a result.

531
00:25:03,790 --> 00:25:07,670
And it was relatively fast,
which is what we're looking for.

532
00:25:07,670 --> 00:25:10,070
And that's good, but we can do better,

533
00:25:10,070 --> 00:25:13,282
because if we run the content image

534
00:25:13,282 --> 00:25:14,250
that's been pre-processed

535
00:25:14,250 --> 00:25:17,680
through that style predict model,

536
00:25:17,680 --> 00:25:20,610
we can also get the bottleneck
for the content image.

537
00:25:20,610 --> 00:25:23,330
And then, if we have the
bottlenecks for the content image

538
00:25:23,330 --> 00:25:27,000
and for the style image, now
we can play with the ratio

539
00:25:27,000 --> 00:25:31,937
between those two and decide
how much style input we want

540
00:25:32,900 --> 00:25:35,160
and how much content input we want.

541
00:25:35,160 --> 00:25:37,210
And we can have a whole range

542
00:25:37,210 --> 00:25:42,210
of stylized images from very,
very dislike the original

543
00:25:42,600 --> 00:25:45,429
to very, very close to the original,

544
00:25:45,430 --> 00:25:47,480
albeit slightly modified.

545
00:25:47,480 --> 00:25:49,620
And this is what's going to give us

546
00:25:49,620 --> 00:25:52,639
the dynamic filter effect,

547
00:25:52,640 --> 00:25:56,040
that every time you take an
image and you stylize it,

548
00:25:56,040 --> 00:25:59,899
not only are you changing the
style that you're applying,

549
00:25:59,900 --> 00:26:03,540
but you're also changing
the impact of that style,

550
00:26:03,540 --> 00:26:05,810
which means you have different

551
00:26:05,810 --> 00:26:08,830
and a wider variety of resulting images,

552
00:26:08,830 --> 00:26:12,100
and you can also adjust
the strength of that filter

553
00:26:12,100 --> 00:26:16,360
until it provides the protection
that you're looking for

554
00:26:16,360 --> 00:26:18,500
for the image that you have just taken

555
00:26:18,500 --> 00:26:19,703
or are concerned with.

556
00:26:20,950 --> 00:26:23,340
And here you can see the range of styles.

557
00:26:23,340 --> 00:26:26,149
and how it applies, and the impact it has

558
00:26:26,150 --> 00:26:28,870
as you shift that slider
that you'll see here

559
00:26:28,870 --> 00:26:30,709
in the demo in a bit.

560
00:26:30,710 --> 00:26:34,030
Now, the app is designed
to be isolated mode.

561
00:26:34,030 --> 00:26:36,310
In other words, you don't
need to be on a network.

562
00:26:36,310 --> 00:26:39,020
And so, there are 32 onboard styles.

563
00:26:39,020 --> 00:26:42,110
If you do happen to be
connected to the internet,

564
00:26:42,110 --> 00:26:44,209
then you have 80,000 more styles

565
00:26:44,210 --> 00:26:46,710
at your disposal to download and use.

566
00:26:46,710 --> 00:26:47,830
The more you rotate these,

567
00:26:47,830 --> 00:26:50,850
the more dynamic the protection will be.

568
00:26:50,850 --> 00:26:53,389
The phase two is procedural
noise generation.

569
00:26:53,390 --> 00:26:54,230
And this is where we're going

570
00:26:54,230 --> 00:26:57,170
to like the Camera
Adversaria app inject noise

571
00:26:57,170 --> 00:26:58,150
into the image.

572
00:26:58,150 --> 00:26:59,780
And we're using simplex noise

573
00:26:59,780 --> 00:27:02,639
because we know it impacts classification.

574
00:27:02,640 --> 00:27:06,720
It's relatively computationally
cheap, it's dynamic,

575
00:27:06,720 --> 00:27:09,360
and can be uniquely
generated for every image.

576
00:27:09,360 --> 00:27:11,919
And if you do it right, it's
not that obvious to the viewer.

577
00:27:11,920 --> 00:27:15,210
And we can just use GPU
image to apply this filter.

578
00:27:15,210 --> 00:27:16,650
And there's an open source algorithm.

579
00:27:16,650 --> 00:27:19,020
So, it's fairly straightforward.

580
00:27:19,020 --> 00:27:20,850
Then phase three is evaluation.

581
00:27:20,850 --> 00:27:22,409
We check to see how we've done.

582
00:27:22,410 --> 00:27:23,720
We look, in this case,

583
00:27:23,720 --> 00:27:26,620
for a facial detection using Google ML kit

584
00:27:26,620 --> 00:27:30,003
just to see how it has protected or not.

585
00:27:31,750 --> 00:27:33,030
And now, for a quick demo

586
00:27:33,030 --> 00:27:35,629
of the current open source app.

587
00:27:35,630 --> 00:27:38,290
Currently, the app is
installed on the iPhone

588
00:27:38,290 --> 00:27:42,510
through Apple's X Code
application, but longterm,

589
00:27:42,510 --> 00:27:43,810
my hope is that will be available

590
00:27:43,810 --> 00:27:47,000
for free from the official app store.

591
00:27:47,000 --> 00:27:48,750
And now, just to orient you

592
00:27:48,750 --> 00:27:51,370
to the app interface a little bit here,

593
00:27:51,370 --> 00:27:53,409
the top row allows you to cycle

594
00:27:53,410 --> 00:27:56,330
through a sampling of faces to modify

595
00:27:56,330 --> 00:27:59,129
with the next image link there.

596
00:27:59,130 --> 00:28:01,380
The option of selecting an existing photo

597
00:28:01,380 --> 00:28:05,170
from your camera roll
or to take a new photo

598
00:28:05,170 --> 00:28:07,530
with the built-in camera facility,

599
00:28:07,530 --> 00:28:09,899
the video icon you see
there is for future use,

600
00:28:09,900 --> 00:28:12,400
but it's not supported at this time.

601
00:28:12,400 --> 00:28:13,610
The center view shows you

602
00:28:13,610 --> 00:28:16,929
the current image you're working
with in its current status,

603
00:28:16,930 --> 00:28:20,540
and the middle tabs just
beneath that allow you to cycle

604
00:28:20,540 --> 00:28:22,100
between the original photo,

605
00:28:22,100 --> 00:28:25,399
the style that you're applying
to that original photo,

606
00:28:25,400 --> 00:28:30,270
and the images that result
from the style transfer,

607
00:28:30,270 --> 00:28:33,860
and the resulting simplex
noise perturbation

608
00:28:33,860 --> 00:28:34,790
that's injected in,

609
00:28:34,790 --> 00:28:38,129
which you shouldn't be
able to see in most cases.

610
00:28:38,130 --> 00:28:40,650
The style, I will note is also displayed

611
00:28:40,650 --> 00:28:43,020
at the square at the bottom right

612
00:28:43,020 --> 00:28:44,840
with a change button you'll see here

613
00:28:44,840 --> 00:28:48,790
in action in a little bit that
will let you select styles.

614
00:28:48,790 --> 00:28:51,300
Now, there's also a
sliding scale right next

615
00:28:51,300 --> 00:28:54,230
to that style image that allows a user

616
00:28:54,230 --> 00:28:58,790
to adjust the impact between
the original content image

617
00:28:58,790 --> 00:29:01,960
and the style image as it is applied,

618
00:29:01,960 --> 00:29:03,700
more like the style image,

619
00:29:03,700 --> 00:29:06,223
more like the original content image,

620
00:29:07,300 --> 00:29:09,000
you can slide back and forth,

621
00:29:09,000 --> 00:29:11,780
and once you apply the style,
you'll see that impact.

622
00:29:11,780 --> 00:29:13,910
There's also a toggle switch to use

623
00:29:13,910 --> 00:29:15,790
or not use GPU acceleration.

624
00:29:15,790 --> 00:29:19,350
And this is a relatively recent
addition to TensorFlow Lite,

625
00:29:19,350 --> 00:29:21,159
and for performance reasons,

626
00:29:21,160 --> 00:29:23,300
obviously it's best just leave that on,

627
00:29:23,300 --> 00:29:26,770
but it's there just for comparison sake.

628
00:29:26,770 --> 00:29:28,970
Finally, at the bottom, there are buttons

629
00:29:28,970 --> 00:29:32,300
to left to right, obfuscate the image,

630
00:29:32,300 --> 00:29:34,913
by which I mean apply the style,

631
00:29:35,930 --> 00:29:39,060
options to detect faces
or attempt to detect faces

632
00:29:39,060 --> 00:29:41,270
with Google's ML kit,

633
00:29:41,270 --> 00:29:44,129
to clear any facial detection markings

634
00:29:44,130 --> 00:29:46,930
that you may have caused
to appear on the image,

635
00:29:46,930 --> 00:29:50,950
and finally, to save the
current image as it's viewed

636
00:29:50,950 --> 00:29:55,520
to a dedicated album in the
photo roll for the device.

637
00:29:55,520 --> 00:29:57,160
Let's run through a sample use case

638
00:29:57,160 --> 00:29:59,210
with a phone in the local park set

639
00:29:59,210 --> 00:30:02,650
to airplane mode disconnected
from the internet.

640
00:30:02,650 --> 00:30:03,990
The user starts up the app,

641
00:30:03,990 --> 00:30:05,850
and then selects to take a new photo.

642
00:30:05,850 --> 00:30:08,320
Keep in mind, by the way
that the same process

643
00:30:08,320 --> 00:30:09,800
after image capture would apply

644
00:30:09,800 --> 00:30:13,139
to a photo selected from
the device's photo roll.

645
00:30:13,140 --> 00:30:16,470
So, a rather handsome devil,
if I may say so myself.

646
00:30:16,470 --> 00:30:18,480
Now, after he's captured the image,

647
00:30:18,480 --> 00:30:20,630
you should already notice that
the app has center cropped

648
00:30:20,630 --> 00:30:23,360
the image in preparation
for pre-processing.

649
00:30:23,360 --> 00:30:24,719
It's going to be adjusted down

650
00:30:24,720 --> 00:30:28,500
to a 384 by 384 format eventually.

651
00:30:28,500 --> 00:30:31,100
So, it's less jarring
for the user to go ahead

652
00:30:31,100 --> 00:30:35,020
and convert it now to the
square format at capture time.

653
00:30:35,020 --> 00:30:38,650
At this point, the user
can opt to change the style

654
00:30:38,650 --> 00:30:39,760
that they wish to apply.

655
00:30:39,760 --> 00:30:41,730
It's good to switch this up regularly

656
00:30:41,730 --> 00:30:44,733
to make sure each image
has a unique protection.

657
00:30:45,720 --> 00:30:47,100
At any point in the process,

658
00:30:47,100 --> 00:30:48,909
the user can opt to detect faces

659
00:30:48,910 --> 00:30:51,180
with the onboard detection feature.

660
00:30:51,180 --> 00:30:53,580
Not surprisingly, the unmodified face

661
00:30:53,580 --> 00:30:55,830
from the photo is easily identified.

662
00:30:55,830 --> 00:30:58,070
The user clears the
face detection markings

663
00:30:58,070 --> 00:31:00,730
and moves the slider up
to be highly stylized

664
00:31:00,730 --> 00:31:02,270
by the style image.

665
00:31:02,270 --> 00:31:03,190
This protects the image

666
00:31:03,190 --> 00:31:05,550
of the potential cost of
human intelligibility.

667
00:31:05,550 --> 00:31:07,629
It is clearly a modified.

668
00:31:07,630 --> 00:31:08,990
Running face detection again,

669
00:31:08,990 --> 00:31:11,010
however, shows that the face in the image

670
00:31:11,010 --> 00:31:13,860
has now avoided detection
by Google's algorithm.

671
00:31:13,860 --> 00:31:16,260
The user can then adjust
the slider judging

672
00:31:16,260 --> 00:31:18,250
for the best combination of protection

673
00:31:18,250 --> 00:31:20,820
and usability for their
particular use case.

674
00:31:20,820 --> 00:31:23,659
Shifting the slider closer
to the content image results

675
00:31:23,660 --> 00:31:26,410
in the face being detected once more.

676
00:31:26,410 --> 00:31:28,760
Note that the app only
provides an evaluation

677
00:31:28,760 --> 00:31:31,540
of face detection evasion.

678
00:31:31,540 --> 00:31:33,870
Later versions hope to
include an estimation

679
00:31:33,870 --> 00:31:36,810
of how well a face can
avoid classification

680
00:31:36,810 --> 00:31:38,940
after successful detection.

681
00:31:38,940 --> 00:31:41,330
Finally, users can opt to save any

682
00:31:41,330 --> 00:31:43,750
of the images appearing
in the center view,

683
00:31:43,750 --> 00:31:46,290
particularly the ones
that have been stylized,

684
00:31:46,290 --> 00:31:48,450
to the system's photo roll.

685
00:31:48,450 --> 00:31:51,650
The images are placed into
a dedicated photo album,

686
00:31:51,650 --> 00:31:54,410
thus ensuring that the end user can trust

687
00:31:54,410 --> 00:31:57,570
that the photos that they
choose to upload to social media

688
00:31:57,570 --> 00:32:01,562
or other online services have
some measure of protection.

689
00:32:02,680 --> 00:32:04,620
So, let's talk about that protection

690
00:32:04,620 --> 00:32:06,669
and how well it performs.

691
00:32:06,670 --> 00:32:08,890
This is not a rigorous
testing like you saw

692
00:32:08,890 --> 00:32:12,590
with Fawkes and with Lowkey,
this is 50 individuals taken

693
00:32:12,590 --> 00:32:15,629
from PubFig, a publicly
available database,

694
00:32:15,630 --> 00:32:19,500
and you can see that as you
up the impact the style has,

695
00:32:19,500 --> 00:32:21,400
facial detection, faces are actually

696
00:32:21,400 --> 00:32:23,830
just not detected eventually over time

697
00:32:23,830 --> 00:32:26,250
at the compromise of some usability

698
00:32:26,250 --> 00:32:29,110
or human intelligibility, obviously.

699
00:32:29,110 --> 00:32:30,669
Classification, in other words,

700
00:32:30,670 --> 00:32:34,323
is it identifying the
person as the right person?

701
00:32:35,240 --> 00:32:39,040
Again, the same kind of
50 small sample data set,

702
00:32:39,040 --> 00:32:41,062
and it's a little bit more spread out.

703
00:32:42,210 --> 00:32:45,860
The perturbation and the
style does have an impact.

704
00:32:45,860 --> 00:32:48,179
More formal testing is necessary,

705
00:32:48,180 --> 00:32:50,320
and we'll talk about
that in just a minute,

706
00:32:50,320 --> 00:32:52,720
but at least this is
proof of life that shows

707
00:32:52,720 --> 00:32:56,440
that both detection and
classification are impacted.

708
00:32:56,440 --> 00:32:59,770
And so, the project is
worth pursuing further.

709
00:32:59,770 --> 00:33:02,240
And that's really brings
me to my next point,

710
00:33:02,240 --> 00:33:04,240
which is the future plans here.

711
00:33:04,240 --> 00:33:07,130
Formal testing is required obviously,

712
00:33:07,130 --> 00:33:09,270
with probably the FaceScrub data set,

713
00:33:09,270 --> 00:33:11,210
we can test against the big three,

714
00:33:11,210 --> 00:33:14,770
Microsoft, Amazon, and Face++.

715
00:33:14,770 --> 00:33:17,500
In addition, it would be helpful
to have onboard the device

716
00:33:17,500 --> 00:33:20,130
not just a test for face detection,

717
00:33:20,130 --> 00:33:23,020
but one for face classification.

718
00:33:23,020 --> 00:33:26,540
And we could use a MobileNet
v2 based model kind

719
00:33:26,540 --> 00:33:28,800
of train on your own personal face,

720
00:33:28,800 --> 00:33:31,110
and to know how far off you've gone

721
00:33:31,110 --> 00:33:34,979
from it classifying you correctly
as kind of a smoke test.

722
00:33:34,980 --> 00:33:36,580
We also need to make scale adjustments

723
00:33:36,580 --> 00:33:37,870
to that procedural noise.

724
00:33:37,870 --> 00:33:40,659
You can choose just like
the Camera Adversaria app

725
00:33:40,660 --> 00:33:42,520
how much impact it should have.

726
00:33:42,520 --> 00:33:44,570
Ideally, we could do a
higher res capability.

727
00:33:44,570 --> 00:33:47,500
You've seen the images, they're smaller.

728
00:33:47,500 --> 00:33:50,480
Long-term, we may be able to
stitch together larger pieces.

729
00:33:50,480 --> 00:33:51,680
And then finally, there are things

730
00:33:51,680 --> 00:33:56,680
like Web Assembly where you
can offload some of this work

731
00:33:57,400 --> 00:34:00,670
to say a secure enclave where you

732
00:34:00,670 --> 00:34:02,530
could do some of that processing

733
00:34:02,530 --> 00:34:05,060
and offload some of that mobile work,

734
00:34:05,060 --> 00:34:07,389
so you could use some of the
more complex applications

735
00:34:07,390 --> 00:34:10,650
like Fawkes or Lowkey, and the like.

736
00:34:10,650 --> 00:34:13,677
You should be asking yourself
at this point, "Mike,

737
00:34:13,677 --> 00:34:18,199
"is this practical," to which
I'd say yes and no, right?

738
00:34:18,199 --> 00:34:22,270
And on one hand, no,
consumers are still limited

739
00:34:22,270 --> 00:34:24,030
by those mobile devices.

740
00:34:24,030 --> 00:34:26,380
And as a result, the quality is limited,

741
00:34:26,380 --> 00:34:27,969
performance is limited,

742
00:34:27,969 --> 00:34:30,649
and quality is part of ease of use, right?

743
00:34:30,650 --> 00:34:32,889
If I don't get an image that I think looks

744
00:34:32,889 --> 00:34:34,850
like I want it to look and show the world,

745
00:34:34,850 --> 00:34:36,321
I'm not gonna use it.

746
00:34:36,321 --> 00:34:39,159
If it's not as easy as
just looking at my phone

747
00:34:39,159 --> 00:34:41,469
or going through whatever normal process,

748
00:34:41,469 --> 00:34:44,569
I'm not gonna be as
invested as a normal person

749
00:34:44,570 --> 00:34:49,090
in protecting my own privacy
and my own facial data.

750
00:34:49,090 --> 00:34:51,020
So, there's work to be done there.

751
00:34:51,020 --> 00:34:53,980
And on other hand, yes,

752
00:34:53,980 --> 00:34:55,810
some of these techniques could be used

753
00:34:55,810 --> 00:34:58,070
by organizations and enterprises

754
00:34:58,070 --> 00:35:00,280
to protect the facial data they have.

755
00:35:00,280 --> 00:35:02,500
Much like hashing private data,

756
00:35:02,500 --> 00:35:05,570
you could apply masks,
you could apply cloaking,

757
00:35:05,570 --> 00:35:08,860
you could apply procedural noise

758
00:35:08,860 --> 00:35:10,790
to these images when they're at rest.

759
00:35:10,790 --> 00:35:14,570
So, you could remove it, use
the images and restore them.

760
00:35:14,570 --> 00:35:16,120
And this would provide some measure

761
00:35:16,120 --> 00:35:20,609
of protection for individuals

762
00:35:20,610 --> 00:35:24,200
if you're happen to have
these facial biometric data

763
00:35:24,200 --> 00:35:25,569
in your data set.

764
00:35:25,570 --> 00:35:27,810
Regardless, it's a team sport, right?

765
00:35:27,810 --> 00:35:30,380
It's not just organizations
and enterprises.

766
00:35:30,380 --> 00:35:32,540
It's not just individuals.

767
00:35:32,540 --> 00:35:33,910
And what this project is doing

768
00:35:33,910 --> 00:35:38,049
is trying to give individuals
agency to participate

769
00:35:38,050 --> 00:35:40,630
in the overall process.

770
00:35:40,630 --> 00:35:42,340
And so, as you leave today,

771
00:35:42,340 --> 00:35:44,410
what I'd like you to think about

772
00:35:44,410 --> 00:35:48,670
is what facial recognition
use cases you have going

773
00:35:48,670 --> 00:35:50,610
in your organization.

774
00:35:50,610 --> 00:35:53,440
Whether you're outsourcing
it or keeping it internal,

775
00:35:53,440 --> 00:35:57,893
contemplate how it's being used
and the ethics behind that.

776
00:35:58,800 --> 00:36:01,540
Over time, in the next few months,

777
00:36:01,540 --> 00:36:04,120
think about potential
protections you might be able

778
00:36:04,120 --> 00:36:09,120
to either ask for or apply to
your facial biometric data,

779
00:36:09,460 --> 00:36:10,610
whether it's your customers,

780
00:36:10,610 --> 00:36:12,760
your clients, or your employees.

781
00:36:12,760 --> 00:36:14,040
This might be particularly

782
00:36:14,040 --> 00:36:17,290
of interest to those
organizations involved

783
00:36:17,290 --> 00:36:20,509
with avatars or social media,

784
00:36:20,510 --> 00:36:22,270
but it also could apply to things

785
00:36:22,270 --> 00:36:25,020
like an employee directory
that is vulnerable,

786
00:36:25,020 --> 00:36:28,430
susceptible to theft or misuse.

787
00:36:28,430 --> 00:36:31,660
And then, within a timeframe,
six months to a year,

788
00:36:31,660 --> 00:36:35,520
start working on and putting
those protections in place.

789
00:36:35,520 --> 00:36:36,353
'Cause like I said,

790
00:36:36,353 --> 00:36:40,080
this is not just one organization's job,

791
00:36:40,080 --> 00:36:44,750
it's not just one individual's
job, it's everyone's job.

792
00:36:44,750 --> 00:36:48,160
And if we're all pulling
together in the same fleet,

793
00:36:48,160 --> 00:36:51,339
so to speak, then we can
raise the signal just

794
00:36:51,340 --> 00:36:54,950
as Lord Admiral Nelson did, as he steamed,

795
00:36:54,950 --> 00:36:59,069
as he sailed, excuse me,
into the Battle of Trafalgar

796
00:36:59,070 --> 00:37:00,327
where he raised a flag and said,

797
00:37:00,327 --> 00:37:05,040
"It is expected that every
person will do their duty."

798
00:37:05,040 --> 00:37:08,810
Every organization enabled,
every enterprise enabled,

799
00:37:08,810 --> 00:37:13,810
every individual enabled
to invest in privacy.

800
00:37:14,200 --> 00:37:16,109
And that's the only hope we have

801
00:37:16,110 --> 00:37:19,760
of our ethics not being
outstripped by new innovation

802
00:37:19,760 --> 00:37:24,760
and new technology, like
facial recognition, thank you.

