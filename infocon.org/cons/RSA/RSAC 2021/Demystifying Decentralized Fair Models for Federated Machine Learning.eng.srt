1
00:00:01,130 --> 00:00:02,870
- Hello, everyone.

2
00:00:02,870 --> 00:00:05,180
In this session, we are going
to talk about demystifying

3
00:00:05,180 --> 00:00:08,709
decentralized fair models
for federated learning.

4
00:00:08,710 --> 00:00:10,060
My name is Subarna.

5
00:00:10,060 --> 00:00:12,820
I'm a Manager Data Science
here at Publicis Sapient.

6
00:00:12,820 --> 00:00:14,470
And I have Sharmistha with me,

7
00:00:14,470 --> 00:00:17,220
she's a Senior Manager Data
Science at Sapient as well.

8
00:00:18,410 --> 00:00:20,330
So in this session, we
are going to talk a lot

9
00:00:20,330 --> 00:00:23,049
about fair models and federated learning.

10
00:00:23,050 --> 00:00:26,610
And also just one side note
that if you have any questions,

11
00:00:26,610 --> 00:00:28,800
you can just ping us in chat

12
00:00:28,800 --> 00:00:30,950
and we'll take those questions

13
00:00:30,950 --> 00:00:33,210
during the session, thank you.

14
00:00:33,210 --> 00:00:35,970
So in the next slide, we are going to talk

15
00:00:35,970 --> 00:00:38,350
about the agenda and the
agenda looks like this.

16
00:00:38,350 --> 00:00:40,630
We will be starting with
a brief introduction

17
00:00:40,630 --> 00:00:43,880
about federated learning and how it works.

18
00:00:43,880 --> 00:00:46,390
Then we'll be talking
about fair clustering

19
00:00:46,390 --> 00:00:47,570
what is fair clustering

20
00:00:47,570 --> 00:00:50,010
and how we can incorporate fair clustering

21
00:00:50,010 --> 00:00:51,530
in federated learning.

22
00:00:51,530 --> 00:00:54,310
And lastly, we'll be talking
about the architectures

23
00:00:54,310 --> 00:00:56,457
that go behind federated learning

24
00:00:56,457 --> 00:00:58,810
and how you can leverage them.

25
00:00:58,810 --> 00:01:01,143
And finally, we have the appendix section.

26
00:01:02,320 --> 00:01:05,269
So let's dive in and the first thing

27
00:01:05,269 --> 00:01:07,720
is we wanted to let this know

28
00:01:07,720 --> 00:01:10,080
that what are the things that you can do

29
00:01:10,080 --> 00:01:11,700
to learn more about federated learning

30
00:01:11,700 --> 00:01:13,715
and where will you be?

31
00:01:13,715 --> 00:01:15,040
So in the next week,

32
00:01:15,040 --> 00:01:18,220
I think you will be able to
identify those applications

33
00:01:18,220 --> 00:01:21,020
which are actually using
federated learning.

34
00:01:21,020 --> 00:01:24,080
There are many applications
and in the first three months,

35
00:01:24,080 --> 00:01:25,840
you'll be able to understand properly

36
00:01:25,840 --> 00:01:27,840
what are the different
phases of federated learning,

37
00:01:27,840 --> 00:01:30,060
how you can increase the privacy

38
00:01:30,060 --> 00:01:33,600
and security as you go with those phases.

39
00:01:33,600 --> 00:01:36,440
And by the end of six months, hopefully,

40
00:01:36,440 --> 00:01:37,830
you will be able to create

41
00:01:37,830 --> 00:01:40,710
your own federated learning system

42
00:01:40,710 --> 00:01:44,323
where you will be able to do
your machine learning tasks.

43
00:01:45,200 --> 00:01:48,640
So before moving forward in this section,

44
00:01:48,640 --> 00:01:51,230
I have two questions for the audience.

45
00:01:51,230 --> 00:01:54,080
I just want you guys to think about it.

46
00:01:54,080 --> 00:01:57,203
The first question is, are
the company's data safe?

47
00:01:58,670 --> 00:02:01,720
And the obvious answer is probably no

48
00:02:01,720 --> 00:02:03,710
because we have so many instances

49
00:02:03,710 --> 00:02:07,610
where we see that people are, you know,

50
00:02:07,610 --> 00:02:09,210
there's data breach happening.

51
00:02:09,210 --> 00:02:10,360
Like the Facebook case,

52
00:02:10,360 --> 00:02:13,350
even recently there was
a Facebook data breach

53
00:02:13,350 --> 00:02:17,640
happened where Facebook
had to pay a lot of fines.

54
00:02:17,640 --> 00:02:20,470
So apart from that, we
have so many big names

55
00:02:20,470 --> 00:02:23,570
from who are not able to
protect their customers' data

56
00:02:23,570 --> 00:02:26,459
like British Airways or Uber

57
00:02:26,460 --> 00:02:29,690
even the WhatsApp case
is quite interesting

58
00:02:29,690 --> 00:02:32,320
where they notified the users

59
00:02:32,320 --> 00:02:35,590
that they will be not encrypting
the business messages,

60
00:02:35,590 --> 00:02:38,050
but people took it in a very different way

61
00:02:38,050 --> 00:02:40,230
and they thought they
will be not encrypting

62
00:02:40,230 --> 00:02:43,440
the private messages and
a lot of users migrated

63
00:02:43,440 --> 00:02:47,500
from WhatsApp to Signal
just to have the privacy.

64
00:02:47,500 --> 00:02:50,330
So hence it is not really safe.

65
00:02:50,330 --> 00:02:52,830
The other question, which
I want you to think about

66
00:02:52,830 --> 00:02:56,240
is, is the masking of data is enough?

67
00:02:56,240 --> 00:03:01,190
So masking of data is enough,
the point is probably not,

68
00:03:01,190 --> 00:03:03,180
because there are so many cases

69
00:03:03,180 --> 00:03:07,640
where the researchers
are able to redo the data

70
00:03:07,640 --> 00:03:09,920
and come to actual data, which was formed

71
00:03:09,920 --> 00:03:13,123
even with anonymizing and
sanitization of the data.

72
00:03:13,123 --> 00:03:15,840
The famous case, I think
is the Netflix case

73
00:03:15,840 --> 00:03:17,640
where Netflix released a data set

74
00:03:17,640 --> 00:03:22,559
which was about the movie
ratings by different users.

75
00:03:22,560 --> 00:03:26,370
And in this case, the movies were masked

76
00:03:26,370 --> 00:03:29,460
as well as the users were masked as well.

77
00:03:29,460 --> 00:03:32,970
But there was University
of Texas researchers

78
00:03:32,970 --> 00:03:34,930
who were able to come back

79
00:03:34,930 --> 00:03:38,710
to the original data,
just by looking at IMDB

80
00:03:38,710 --> 00:03:42,190
and then trying to correlate
between the Netflix data

81
00:03:42,190 --> 00:03:43,480
and IMDB data.

82
00:03:43,480 --> 00:03:44,530
This is quite interesting

83
00:03:44,530 --> 00:03:45,400
but at the same time,

84
00:03:45,400 --> 00:03:49,373
it also means that masking
the data is not enough.

85
00:03:50,290 --> 00:03:54,100
So moving on to the next section,

86
00:03:54,100 --> 00:03:55,131
probably we need to talk

87
00:03:55,131 --> 00:03:57,990
about how the machine
learning system works.

88
00:03:57,990 --> 00:04:00,080
And then we'll try to connect the privacy

89
00:04:00,080 --> 00:04:03,300
and machine learning
into federated learning.

90
00:04:03,300 --> 00:04:06,230
So a typical machine learning
system looks like this.

91
00:04:06,230 --> 00:04:07,670
We have the different devices

92
00:04:07,670 --> 00:04:09,690
and we have the data collection

93
00:04:09,690 --> 00:04:11,890
that goes into a central server.

94
00:04:11,890 --> 00:04:14,920
So from the key point is all the data

95
00:04:14,920 --> 00:04:16,570
is going to a central server,

96
00:04:16,570 --> 00:04:18,360
which is the data collection part

97
00:04:18,360 --> 00:04:20,910
and then we have model training happening.

98
00:04:20,910 --> 00:04:24,730
And finally, an API is exposed, right?

99
00:04:24,730 --> 00:04:26,980
The endpoint is exposed as an API.

100
00:04:26,980 --> 00:04:29,000
And now if the device is worn

101
00:04:29,000 --> 00:04:31,040
when the device is worn, the predictions,

102
00:04:31,040 --> 00:04:34,830
they can connect to that
API and get the output.

103
00:04:34,830 --> 00:04:36,950
Typical case is the Google Maps case,

104
00:04:36,950 --> 00:04:40,190
where you put location from A to B

105
00:04:40,190 --> 00:04:43,130
and you will be able to
see how much time it takes.

106
00:04:43,130 --> 00:04:46,000
So for that, Google is
collecting all of your data

107
00:04:46,000 --> 00:04:49,680
and then they are trying to
improve, build those models,

108
00:04:49,680 --> 00:04:53,310
which is basically telling
you the estimate of the time.

109
00:04:53,310 --> 00:04:56,450
Now, the challenge with all of this

110
00:04:56,450 --> 00:04:59,300
is the data is being accessible

111
00:04:59,300 --> 00:05:02,410
by the data controller,
as well as the processor.

112
00:05:02,410 --> 00:05:04,670
And we don't really want that in our case.

113
00:05:04,670 --> 00:05:08,850
So we want the data to be
with the data generator

114
00:05:08,850 --> 00:05:10,353
and not with anyone else.

115
00:05:11,211 --> 00:05:14,483
So that is a way to preserve the privacy.

116
00:05:15,480 --> 00:05:18,970
Now, this is a very cool slide, I feel.

117
00:05:18,970 --> 00:05:21,200
This is about introduction
to federated learning,

118
00:05:21,200 --> 00:05:24,400
what we are talking about
here are the different cases

119
00:05:24,400 --> 00:05:25,989
why do we need federated learning?

120
00:05:25,990 --> 00:05:27,210
What is federated learning?

121
00:05:27,210 --> 00:05:31,250
And a bit of applications
that go behind it.

122
00:05:31,250 --> 00:05:33,400
So why we need federated learning,

123
00:05:33,400 --> 00:05:35,179
I think we have touched upon that

124
00:05:35,180 --> 00:05:37,950
but there are some more
things that why it is needed.

125
00:05:37,950 --> 00:05:40,260
For example, there are
several data regulations

126
00:05:40,260 --> 00:05:41,730
that are now in place,

127
00:05:41,730 --> 00:05:44,820
which restricts the usage
of user data in some way.

128
00:05:44,820 --> 00:05:46,930
So we have the HIPAA guidelines.

129
00:05:46,930 --> 00:05:51,320
We have the GDPR, which is
quite famous these days.

130
00:05:51,320 --> 00:05:55,085
And also the third point,
which is also important.

131
00:05:55,085 --> 00:05:58,210
I think companies don't really
want to reveal their data

132
00:05:58,210 --> 00:06:02,058
even if it is for the
betterment of the society.

133
00:06:02,058 --> 00:06:03,540
So a lot of times this happens

134
00:06:03,540 --> 00:06:05,597
because they want to protect the data

135
00:06:05,597 --> 00:06:10,539
and they don't really want to
give the data to anyone else

136
00:06:10,540 --> 00:06:14,060
even if it is for say medical
reasons or any other reasons.

137
00:06:14,060 --> 00:06:16,030
So in that case also federated learning

138
00:06:16,030 --> 00:06:17,998
can play a very crucial role

139
00:06:17,999 --> 00:06:22,140
for the betterment of the
society, we'll look into that.

140
00:06:22,140 --> 00:06:23,587
So what is federated learning?

141
00:06:23,588 --> 00:06:25,810
We have been talking so much
about federated learning today.

142
00:06:25,810 --> 00:06:28,150
So federated learning is a technique

143
00:06:28,150 --> 00:06:30,760
where you are able to do the modeling,

144
00:06:30,760 --> 00:06:33,120
you are able to expose
the API and everything

145
00:06:33,120 --> 00:06:36,610
but without actually seeing the data.

146
00:06:36,610 --> 00:06:37,850
So how do you do that?

147
00:06:37,850 --> 00:06:40,340
Is that instead of sending the data

148
00:06:40,340 --> 00:06:43,380
to the central server, like
we saw in the previous slide,

149
00:06:43,380 --> 00:06:45,219
now we are sending the model

150
00:06:45,220 --> 00:06:47,490
to the individual devices.

151
00:06:47,490 --> 00:06:51,040
So how do we do that is we
have the different weights

152
00:06:51,040 --> 00:06:54,430
that basically define a model,
a machine learning model.

153
00:06:54,430 --> 00:06:58,490
And we are trying to find
out what is the change

154
00:06:58,490 --> 00:07:00,620
in weights from the individual devices

155
00:07:00,620 --> 00:07:02,923
by sending the model to the devices.

156
00:07:04,450 --> 00:07:07,450
And we'll look a bit more on that

157
00:07:07,450 --> 00:07:09,633
in the later part of the section,

158
00:07:10,660 --> 00:07:13,400
also let's look at a
couple of applications.

159
00:07:13,400 --> 00:07:16,010
So some of the applications
are the next word prediction.

160
00:07:16,010 --> 00:07:19,670
So in your Android phone,
if you type messages,

161
00:07:19,670 --> 00:07:21,950
you'll be able to see that
there is some recommendation

162
00:07:21,950 --> 00:07:24,800
of the next word that
is also an application

163
00:07:24,800 --> 00:07:25,633
of federated learning.

164
00:07:25,633 --> 00:07:27,570
There are some more applications

165
00:07:27,570 --> 00:07:29,452
like the predictive health suggestions,

166
00:07:29,452 --> 00:07:30,735
predictive car maintenance.

167
00:07:30,735 --> 00:07:33,340
So predictive car maintenance is again

168
00:07:33,340 --> 00:07:36,210
where it's about the different companies

169
00:07:36,210 --> 00:07:38,806
have different car manufacturers.

170
00:07:38,806 --> 00:07:42,960
So you may want to have a
proper maintenance system

171
00:07:42,960 --> 00:07:45,669
for the users where they
will be able to find out

172
00:07:45,670 --> 00:07:48,260
whether their car will
be breaking down or not

173
00:07:48,260 --> 00:07:49,349
in the near future.

174
00:07:49,350 --> 00:07:53,040
But at the same time,
you may want to have data

175
00:07:53,040 --> 00:07:56,300
from different manufacturing
companies as well.

176
00:07:56,300 --> 00:07:59,463
So here also federated learning
can play a crucial role.

177
00:08:00,760 --> 00:08:02,840
Now let's look at the different levels

178
00:08:02,840 --> 00:08:03,919
of federated learning.

179
00:08:03,920 --> 00:08:06,050
So the different levels
of federated learning

180
00:08:06,050 --> 00:08:07,483
is defined by the way

181
00:08:08,510 --> 00:08:11,690
you add in the privacy
regarding safeguarding.

182
00:08:11,690 --> 00:08:14,050
So the first thing is
federated learning basic.

183
00:08:14,050 --> 00:08:16,180
So in this case, we are just talking

184
00:08:16,180 --> 00:08:19,653
about the data is now with
the devices and the model

185
00:08:19,653 --> 00:08:21,363
is sent to the devices.

186
00:08:22,670 --> 00:08:25,110
Now, if we can somehow add in

187
00:08:25,110 --> 00:08:28,510
a bit of more cryptograph into that,

188
00:08:28,510 --> 00:08:31,349
and a bit of more encryption

189
00:08:31,350 --> 00:08:33,330
to the whole weights and everything,

190
00:08:33,330 --> 00:08:36,419
then we will reach to the second
level of federated learning

191
00:08:36,419 --> 00:08:38,919
which is adding the secret sharing

192
00:08:38,919 --> 00:08:40,809
and fixed precision and coding.

193
00:08:40,809 --> 00:08:42,059
We'll talk more on that.

194
00:08:42,059 --> 00:08:44,369
And the final one is if you're really want

195
00:08:44,370 --> 00:08:47,550
to safe guard your
privacy with everything,

196
00:08:47,550 --> 00:08:50,849
then you can add in
differential privacy as well,

197
00:08:50,850 --> 00:08:52,570
which is basically adding a bit

198
00:08:52,570 --> 00:08:53,663
of noise to the data.

199
00:08:55,640 --> 00:08:57,810
Now let's look at
federated learning basic.

200
00:08:57,810 --> 00:09:01,089
I think I touched upon that,
but this is how it looks like.

201
00:09:01,090 --> 00:09:02,760
So we have a central server

202
00:09:02,760 --> 00:09:07,680
and the model is basically being sent

203
00:09:07,680 --> 00:09:09,800
to the individual devices.

204
00:09:09,800 --> 00:09:12,310
And then the updates are, again,

205
00:09:12,310 --> 00:09:14,329
going back to the central server.

206
00:09:14,330 --> 00:09:16,490
And finally, the aggregation is happening

207
00:09:16,490 --> 00:09:17,770
in the central server

208
00:09:17,770 --> 00:09:21,569
which is a intelligent and smart model.

209
00:09:21,570 --> 00:09:26,230
So here all the data is
being with the devices

210
00:09:26,230 --> 00:09:28,593
and the model is being
sent to the devices.

211
00:09:30,000 --> 00:09:31,350
Here is a bit of code,

212
00:09:31,350 --> 00:09:33,400
which basically tells you how to do this.

213
00:09:34,680 --> 00:09:35,910
The code is written in PySyft

214
00:09:35,910 --> 00:09:40,060
and is also available in
PySyft library tutorial.

215
00:09:40,060 --> 00:09:42,040
So here, what we are talking about

216
00:09:42,040 --> 00:09:45,290
is we have two devices, Elizabeth and Sam,

217
00:09:45,290 --> 00:09:48,660
and if you look at this section,

218
00:09:48,660 --> 00:09:51,650
then the model is training
separately on device one.

219
00:09:51,650 --> 00:09:55,069
And the model is training
separately on device two.

220
00:09:55,070 --> 00:09:58,790
And finally, all of the
change in the weights

221
00:09:58,790 --> 00:10:00,780
are being sent to the central server,

222
00:10:00,780 --> 00:10:02,689
let's call this as the central server

223
00:10:02,690 --> 00:10:05,950
and here again the averaging is happening.

224
00:10:05,950 --> 00:10:09,823
It can be any form of creation,
depends on your use case.

225
00:10:11,190 --> 00:10:13,220
So the disadvantage to this

226
00:10:13,220 --> 00:10:15,990
is that it can be re-engineered.

227
00:10:15,990 --> 00:10:19,180
So if someone looks at
the change in weights,

228
00:10:19,180 --> 00:10:20,620
they can still come back

229
00:10:20,620 --> 00:10:24,330
and see that how the data looks like.

230
00:10:24,330 --> 00:10:26,840
So in this case, let's
move on to the next level.

231
00:10:26,840 --> 00:10:27,780
How do we do that?

232
00:10:27,780 --> 00:10:30,449
Is you add encryption.

233
00:10:30,450 --> 00:10:32,440
So the way you add encryption

234
00:10:32,440 --> 00:10:36,756
is that there's a very famous
secret sharing technique,

235
00:10:36,756 --> 00:10:38,010
additive secret sharing.

236
00:10:38,010 --> 00:10:39,960
So in this, what is happening

237
00:10:39,960 --> 00:10:42,710
is that you're trying to
encrypt the weight itself

238
00:10:42,710 --> 00:10:44,810
so that even if someone sees the weight,

239
00:10:44,810 --> 00:10:49,329
they are not able to re-engineer
it to reach to the data.

240
00:10:49,330 --> 00:10:51,620
Now, the advantage of this

241
00:10:51,620 --> 00:10:53,820
is you want something which is encrypted

242
00:10:53,820 --> 00:10:57,100
and also at the same time,
you can do aggregation on it.

243
00:10:57,100 --> 00:11:00,550
So for that additive secret
sharing is being used

244
00:11:00,550 --> 00:11:05,229
and fixed precision and
coding is just a layer on top

245
00:11:05,230 --> 00:11:06,220
just to make sure

246
00:11:06,220 --> 00:11:08,400
that the additive secret
sharing is working

247
00:11:08,400 --> 00:11:09,900
on the floating point numbers.

248
00:11:11,020 --> 00:11:12,540
Again, in terms of code,

249
00:11:12,540 --> 00:11:14,430
it looks pretty simple actually.

250
00:11:14,430 --> 00:11:17,061
And PySyft, it is quite easy to do.

251
00:11:17,062 --> 00:11:21,200
We have this function called
the dot fixed precision

252
00:11:21,200 --> 00:11:26,143
which basically allows you to
incorporate the encryption.

253
00:11:28,010 --> 00:11:29,500
This is all good, actually.

254
00:11:29,500 --> 00:11:31,840
So far in level two,
if you have level two,

255
00:11:31,840 --> 00:11:33,680
this is all working fine,

256
00:11:33,680 --> 00:11:35,900
but if you want extra level of security,

257
00:11:35,900 --> 00:11:38,090
you can add in differential privacy.

258
00:11:38,090 --> 00:11:40,440
So differential privacy
is basically adding

259
00:11:40,440 --> 00:11:42,440
a bit of noise to the data itself

260
00:11:42,440 --> 00:11:46,630
so that even if someone at
somehow reaches to the data

261
00:11:46,630 --> 00:11:49,630
but they will still
not be able to find out

262
00:11:49,630 --> 00:11:51,030
the complete data.

263
00:11:51,030 --> 00:11:53,230
So they will not be able to do that.

264
00:11:53,230 --> 00:11:55,629
How it works is that I have this example

265
00:11:55,629 --> 00:11:57,910
in the bottom right corner,

266
00:11:57,910 --> 00:11:59,219
which I think makes it very clear.

267
00:11:59,220 --> 00:12:01,220
So we have two images.

268
00:12:01,220 --> 00:12:03,860
And if you look at the top, first image,

269
00:12:03,860 --> 00:12:05,610
one image is changed.

270
00:12:05,610 --> 00:12:08,500
So let's call that a bit of
noise to the data itself.

271
00:12:08,500 --> 00:12:11,572
And this is what I was talking about

272
00:12:11,572 --> 00:12:13,640
in terms of differential privacy.

273
00:12:13,640 --> 00:12:16,490
So in differential privacy,
we have two kinds of noise,

274
00:12:16,490 --> 00:12:18,497
Laplacian and Gaussian noise

275
00:12:18,497 --> 00:12:23,410
and it is basically measured
by a metrical epsilon.

276
00:12:23,410 --> 00:12:26,890
So as you already probably know

277
00:12:26,890 --> 00:12:29,263
that if we add more noise,

278
00:12:31,100 --> 00:12:33,220
so here the privacy is increasing

279
00:12:33,220 --> 00:12:37,000
but at the same time, our
accuracy, we are trading off.

280
00:12:37,000 --> 00:12:41,770
So this is something which
we need to keep in mind.

281
00:12:41,770 --> 00:12:45,140
Lastly, let's look at the
benefits and challenges

282
00:12:45,140 --> 00:12:46,390
of federated learning.

283
00:12:46,390 --> 00:12:49,040
So there are a lot of benefits.

284
00:12:49,041 --> 00:12:52,210
So first thing is definitely
we are ensuring privacy

285
00:12:52,210 --> 00:12:54,410
because the data is with the device

286
00:12:54,410 --> 00:12:57,060
where it is being
generated, so nowhere else.

287
00:12:57,060 --> 00:12:59,250
The second key thing is lower latency

288
00:12:59,250 --> 00:13:02,910
because the model is now
in the individual devices,

289
00:13:02,910 --> 00:13:05,040
so there's low latency and data,

290
00:13:05,040 --> 00:13:08,550
also, the data is not being
sent to the central server.

291
00:13:08,550 --> 00:13:10,000
So there you are saving time.

292
00:13:11,170 --> 00:13:14,120
The third thing is smarter models.

293
00:13:14,120 --> 00:13:16,430
So you are able to
access the data directly

294
00:13:16,430 --> 00:13:18,609
as soon as it is generated.

295
00:13:18,610 --> 00:13:22,300
So it's not something that
someone is working on the data

296
00:13:22,300 --> 00:13:24,550
and massaging the data, manipulating it

297
00:13:24,550 --> 00:13:25,630
and then the model is being built.

298
00:13:25,630 --> 00:13:29,860
So theoretically, this
should give the smart model.

299
00:13:29,860 --> 00:13:32,210
And the last point is the
less power consumption.

300
00:13:32,210 --> 00:13:34,470
This is from the person

301
00:13:34,470 --> 00:13:37,430
who is from the center
server's point of view

302
00:13:37,430 --> 00:13:39,660
because we are distributing
the model itself,

303
00:13:39,660 --> 00:13:44,060
all the computation are happening
in the individual devices

304
00:13:44,060 --> 00:13:45,939
rather than the central server,

305
00:13:45,940 --> 00:13:49,400
so hence we have less power consumption.

306
00:13:49,400 --> 00:13:51,569
Now let's look at some of the challenges.

307
00:13:51,570 --> 00:13:54,680
The first challenge is
expensive communication.

308
00:13:54,680 --> 00:13:57,858
So we have so much of
communication happening here.

309
00:13:57,858 --> 00:14:00,810
It can be really expensive

310
00:14:00,810 --> 00:14:03,037
between the devices and central server.

311
00:14:03,038 --> 00:14:05,640
The second point is lesser control.

312
00:14:05,640 --> 00:14:09,449
So for the person who is
actually creating all this system

313
00:14:09,450 --> 00:14:13,370
he or she probably will not
have that much of control

314
00:14:13,370 --> 00:14:15,237
because everything is encrypted

315
00:14:15,237 --> 00:14:17,550
and even data is not viewable,

316
00:14:17,550 --> 00:14:20,420
model weights or not
viewable, nothing is visible.

317
00:14:21,750 --> 00:14:24,340
The third point is fewer frameworks.

318
00:14:24,340 --> 00:14:27,123
So, federated learning is
kind of new to the market,

319
00:14:27,123 --> 00:14:29,605
there's only a couple of frameworks

320
00:14:29,605 --> 00:14:30,683
which are really famous.

321
00:14:30,683 --> 00:14:34,143
One is PySyft the other one
is TensorFlow encrypted.

322
00:14:34,143 --> 00:14:35,407
Both of them are really good,

323
00:14:35,407 --> 00:14:37,143
but there's only two as of now.

324
00:14:38,410 --> 00:14:40,680
The last point is
statistical heterogeneity.

325
00:14:40,680 --> 00:14:43,380
This point basically means
that individual devices

326
00:14:43,380 --> 00:14:45,270
will have their own noise

327
00:14:45,270 --> 00:14:47,530
in terms of the model building.

328
00:14:47,530 --> 00:14:50,810
So for example, when you type messages,

329
00:14:50,810 --> 00:14:53,640
you type in a specific way.

330
00:14:53,640 --> 00:14:56,380
So for example, you may
write instead of LOL

331
00:14:56,380 --> 00:15:00,449
you may use a smiley where
someone else can use LOL.

332
00:15:00,449 --> 00:15:02,839
So in that case,

333
00:15:02,840 --> 00:15:06,660
what happens that if I
have millions of devices

334
00:15:06,660 --> 00:15:09,810
and there are some biases
happening within the devices,

335
00:15:09,810 --> 00:15:14,760
then the whole model goes
to a biased output state.

336
00:15:14,760 --> 00:15:18,013
So that is what these
statistical heterogeneity means.

337
00:15:19,815 --> 00:15:21,820
So, far we have spoken

338
00:15:21,820 --> 00:15:24,703
about federated learning and
the challenges and benefits.

339
00:15:24,703 --> 00:15:27,079
Now let's move on to the next section

340
00:15:27,080 --> 00:15:29,290
which is about fair clustering.

341
00:15:29,290 --> 00:15:32,593
And Sharmistha is going
to be talking about that.

342
00:15:32,593 --> 00:15:34,020
Over to you Sharmistha.

343
00:15:34,020 --> 00:15:36,160
- Thank you Subarna for
the wonderful insights

344
00:15:36,160 --> 00:15:39,120
and giving an introduction
of federated learning,

345
00:15:39,120 --> 00:15:40,890
its benefits and challenges.

346
00:15:40,890 --> 00:15:43,290
Now, I'm going to talk
about fair clustering

347
00:15:43,290 --> 00:15:46,390
in federated learning, and
that's one of the main aspects

348
00:15:46,390 --> 00:15:49,520
of ethical AI in federated models.

349
00:15:49,520 --> 00:15:50,890
So in this section,

350
00:15:50,890 --> 00:15:54,280
we will mostly concentrate
on incorporating AI ethics

351
00:15:54,280 --> 00:15:57,060
in the required use cases
that you may want to do

352
00:15:57,060 --> 00:15:58,819
when you go back to your organization

353
00:15:58,820 --> 00:16:01,310
so that you are able to
build fair able models

354
00:16:01,310 --> 00:16:03,140
and define the appropriate metrics,

355
00:16:03,140 --> 00:16:07,170
what is used to judge that
the model is fair and private.

356
00:16:07,170 --> 00:16:08,280
And along with that,

357
00:16:08,280 --> 00:16:11,077
when we talk about ethics
and interpretability,

358
00:16:11,077 --> 00:16:12,510
we will also see an architecture,

359
00:16:12,510 --> 00:16:14,233
how it could fit the entire model.

360
00:16:15,090 --> 00:16:16,230
Now, what are the problems

361
00:16:16,230 --> 00:16:18,801
with federated learning with clustering?

362
00:16:18,801 --> 00:16:20,470
And why we talk about clustering?

363
00:16:20,470 --> 00:16:22,670
So clustering is one of the mechanisms

364
00:16:22,670 --> 00:16:25,860
how we incorporate fairness
in federated models.

365
00:16:25,860 --> 00:16:29,680
So basic problems lies in
federated learning non-independent

366
00:16:29,680 --> 00:16:31,939
and identically distributed data,

367
00:16:31,940 --> 00:16:34,020
actually are found to
degrade the performance

368
00:16:34,020 --> 00:16:35,449
of machine learning models.

369
00:16:35,450 --> 00:16:37,620
And that's why researchers have come up

370
00:16:37,620 --> 00:16:40,850
with combining fairness
objectives with privacy.

371
00:16:40,850 --> 00:16:42,740
And that's again, another challenge

372
00:16:42,740 --> 00:16:44,600
and it is a new area of research.

373
00:16:44,600 --> 00:16:47,080
Like how do we combine fairness, privacy,

374
00:16:47,080 --> 00:16:50,940
so that the computation costs
is also less on the devices.

375
00:16:50,940 --> 00:16:53,030
So one of the approaches
that we have followed.

376
00:16:53,030 --> 00:16:56,280
So different groups of users
might have their own objectives

377
00:16:56,280 --> 00:16:58,467
data aggregation, so that all the data

378
00:16:58,467 --> 00:17:01,600
are from different clusters are aggregated

379
00:17:01,600 --> 00:17:03,407
from all the different sets of devices

380
00:17:03,407 --> 00:17:05,310
and which devices behave similarly

381
00:17:05,310 --> 00:17:08,270
are aggregated and represented
to proper clusters.

382
00:17:08,270 --> 00:17:10,480
How do we ensure that
the clustered membership

383
00:17:10,480 --> 00:17:13,290
of different incomes are
proportionally represented?

384
00:17:13,290 --> 00:17:15,839
and how we do that in
a distributed framework

385
00:17:15,839 --> 00:17:18,709
so that we are able to
deploy the production setup.

386
00:17:18,710 --> 00:17:19,960
And why do we do that?

387
00:17:19,960 --> 00:17:23,359
Our objective is to attain
accuracy of performance

388
00:17:23,359 --> 00:17:26,339
as close as conventional federated model.

389
00:17:26,339 --> 00:17:28,850
So that'd were are not
degrading performance

390
00:17:28,850 --> 00:17:32,959
and also we can leverage the
best clustering objective

391
00:17:32,960 --> 00:17:34,180
for each of the clusters.

392
00:17:34,180 --> 00:17:37,630
Here you can see that you can
predict disease mortality,

393
00:17:37,630 --> 00:17:40,810
ICU conditions from different patients

394
00:17:40,810 --> 00:17:43,440
coming from three different hospitals.

395
00:17:43,440 --> 00:17:45,950
And each of these hospitals may have

396
00:17:45,950 --> 00:17:48,580
say one or two different clusters.

397
00:17:48,580 --> 00:17:51,409
And we go against the
conventional assumption

398
00:17:51,410 --> 00:17:55,100
that all the data from
coming all the devices

399
00:17:55,100 --> 00:17:57,389
are equally distributed

400
00:17:57,390 --> 00:17:59,760
or have similar machine learning

401
00:17:59,760 --> 00:18:01,660
or similar statistical distributions.

402
00:18:01,660 --> 00:18:03,760
So we try to correct that assumption

403
00:18:03,760 --> 00:18:06,930
and instead in clusters of devices

404
00:18:06,930 --> 00:18:09,760
and put the devices in those clusters

405
00:18:09,760 --> 00:18:12,370
which behave similarly and our objective

406
00:18:12,370 --> 00:18:13,379
at the end of the day

407
00:18:13,380 --> 00:18:17,450
is not to ensure only fairness
or privacy or security,

408
00:18:17,450 --> 00:18:19,660
but how do we make the clusters dynamic

409
00:18:19,660 --> 00:18:21,500
so that the clusters can correlate,

410
00:18:21,500 --> 00:18:23,640
means the clusters can unite

411
00:18:23,640 --> 00:18:25,790
or the clusters if needed can be broken

412
00:18:25,790 --> 00:18:28,060
into different clusters
and trying to do that

413
00:18:28,060 --> 00:18:32,070
in a federated environment,
in a decentralized manner.

414
00:18:32,070 --> 00:18:33,730
Now, what are the risks?

415
00:18:33,730 --> 00:18:35,867
What are the risks with federated
learning with clustering?

416
00:18:35,867 --> 00:18:37,870
There are data privacy breach.

417
00:18:37,870 --> 00:18:40,479
There are fixed shape of
clusters as we have talked about

418
00:18:40,480 --> 00:18:45,200
and even the client data
coming from differences

419
00:18:45,200 --> 00:18:48,250
may vary with that timing
and characteristics.

420
00:18:48,250 --> 00:18:51,440
And it is the non-adaptive
nature of the clusters.

421
00:18:51,440 --> 00:18:53,280
So what are the solutions for that?

422
00:18:53,280 --> 00:18:55,860
So the solutions we have
in trying to incorporate

423
00:18:55,860 --> 00:18:59,919
is including data compression
techniques to auto encoder.

424
00:18:59,920 --> 00:19:03,360
So dynamic cluster adaptation
while preserving privacy

425
00:19:03,360 --> 00:19:05,810
and also merging the different clusters

426
00:19:05,810 --> 00:19:06,760
as we have talked about

427
00:19:06,760 --> 00:19:09,810
so that the clusters have
an optimal select score

428
00:19:09,810 --> 00:19:11,520
that is one of the main characteristics

429
00:19:11,520 --> 00:19:14,530
of measuring clustering and
I'll be trying to do that

430
00:19:14,530 --> 00:19:17,060
if we have to do it in
a multitask environment

431
00:19:17,060 --> 00:19:20,159
in federated learning so
that there are no adversaries

432
00:19:20,160 --> 00:19:24,018
who can disturb or break the privacy

433
00:19:24,018 --> 00:19:25,638
of the in-test settings.

434
00:19:25,638 --> 00:19:27,058
So what are the different parameters

435
00:19:27,058 --> 00:19:28,690
or metrics that you would like to monitor?

436
00:19:28,690 --> 00:19:30,510
So one of them is the privacy budget,

437
00:19:30,510 --> 00:19:32,900
so that it tries to give a measurement

438
00:19:32,900 --> 00:19:35,040
that how private is the data.

439
00:19:35,040 --> 00:19:38,810
Like if you remove one single record

440
00:19:38,810 --> 00:19:41,159
from a dataset that
Subarna has talked about,

441
00:19:41,160 --> 00:19:43,348
there should not be any output,

442
00:19:43,348 --> 00:19:46,669
or that model outcome should
not be distinguishable

443
00:19:46,670 --> 00:19:48,480
from the two results.

444
00:19:48,480 --> 00:19:51,540
So one data record is removed
and the model outcomes

445
00:19:51,540 --> 00:19:53,680
from the two different
machine learning models

446
00:19:53,680 --> 00:19:54,513
should be similar.

447
00:19:54,513 --> 00:19:57,010
So there is no way the
attacker could distinguish

448
00:19:57,010 --> 00:19:59,887
or go back to that original data,

449
00:19:59,887 --> 00:20:01,540
and we also have sensitivity.

450
00:20:01,540 --> 00:20:03,030
How do we measure the sensitivity?

451
00:20:03,030 --> 00:20:05,450
It is the amount of noise that we add,

452
00:20:05,450 --> 00:20:08,990
and it is measured by the
one norm of a distance

453
00:20:08,990 --> 00:20:10,760
between the two data sets

454
00:20:10,760 --> 00:20:12,990
that refer in a single element

455
00:20:12,990 --> 00:20:15,740
that we have seen in differential
privacy, what we have.

456
00:20:15,740 --> 00:20:18,170
So now fairness, what are the metrics

457
00:20:18,170 --> 00:20:19,810
that we need to measure for fairness?

458
00:20:19,810 --> 00:20:20,822
So the protected and
the non-protected loss.

459
00:20:20,823 --> 00:20:25,110
So for protected loss
or non-protected loss,

460
00:20:25,110 --> 00:20:26,719
we take into different distance parameters

461
00:20:26,720 --> 00:20:28,320
like Euclidean distance

462
00:20:28,320 --> 00:20:30,560
or Manhattan distance between centralized

463
00:20:30,560 --> 00:20:33,129
and protected or non-protected attributes.

464
00:20:33,130 --> 00:20:35,220
Now, the most important thing is Lambda.

465
00:20:35,220 --> 00:20:36,773
As Subarna has talked about

466
00:20:36,773 --> 00:20:39,310
that we have to make a compromise

467
00:20:39,310 --> 00:20:41,860
or a trade-off between the actual accuracy

468
00:20:41,860 --> 00:20:45,639
as well as the fairness
of privacy metrics.

469
00:20:45,640 --> 00:20:47,480
So how do we control trade-off

470
00:20:47,480 --> 00:20:50,380
between the clustering objective
and the fairness penalty.

471
00:20:52,159 --> 00:20:53,090
When we do that,

472
00:20:53,090 --> 00:20:54,610
what are the conventional model metrics

473
00:20:54,610 --> 00:20:56,820
that we should also
take into consideration?

474
00:20:56,820 --> 00:21:00,889
These are accuracy, precision,
recall, and F1 score.

475
00:21:00,890 --> 00:21:03,090
Now let's look at how we generate,

476
00:21:03,090 --> 00:21:05,090
at what we have learned
from the beginning.

477
00:21:05,090 --> 00:21:08,250
So first we have seen how
vanilla federated learning works.

478
00:21:08,250 --> 00:21:12,130
So it's nothing like a central
server pushes a basic model

479
00:21:12,130 --> 00:21:15,570
to the devices, devices
have their own local data.

480
00:21:15,570 --> 00:21:18,426
They try to learn and send it
back to the central server.

481
00:21:18,426 --> 00:21:20,750
We have sending waves.

482
00:21:20,750 --> 00:21:24,060
We can also add encrypted based DB,

483
00:21:24,060 --> 00:21:25,092
moving to the second phase

484
00:21:25,093 --> 00:21:28,090
that is encryption and
DB-enabled federated learning.

485
00:21:28,090 --> 00:21:30,776
So here we see more of the encryption,

486
00:21:30,777 --> 00:21:32,180
including the weights

487
00:21:32,180 --> 00:21:35,740
and then on top of it,
we can also add privacy

488
00:21:35,740 --> 00:21:38,500
at the application level
of the individual devices.

489
00:21:38,500 --> 00:21:41,560
And then ultimately the model gets pushed

490
00:21:41,560 --> 00:21:42,490
to the central server

491
00:21:42,490 --> 00:21:44,860
and central server does the averaging.

492
00:21:44,860 --> 00:21:48,024
Then after that comes the third phase

493
00:21:48,024 --> 00:21:49,469
that is the encryption

494
00:21:49,470 --> 00:21:52,360
and DB-enabled clustered
federated learning.

495
00:21:52,360 --> 00:21:53,770
And what happens during this phase

496
00:21:53,770 --> 00:21:55,830
is that all the previous two phases stays,

497
00:21:55,830 --> 00:21:57,020
the second phase rather,

498
00:21:57,020 --> 00:22:00,680
stays and instead of on
top of it, we add clusters

499
00:22:00,680 --> 00:22:02,590
where similar devices are grouped

500
00:22:02,590 --> 00:22:03,639
into different clusters

501
00:22:03,640 --> 00:22:06,787
and once a model is changed for cluster,

502
00:22:06,787 --> 00:22:08,690
with the differential privacy

503
00:22:08,690 --> 00:22:11,506
and that model per cluster
is sent to the central server

504
00:22:11,506 --> 00:22:13,520
and the central server averages it

505
00:22:13,520 --> 00:22:16,040
and pushes the model to
their different clusters

506
00:22:16,040 --> 00:22:19,220
so that the objective remains
that every demographic group

507
00:22:19,220 --> 00:22:22,310
and minorities say the
black versus the white,

508
00:22:22,310 --> 00:22:24,350
where everyone is
proportionally represented

509
00:22:24,350 --> 00:22:25,370
in each question.

510
00:22:25,370 --> 00:22:27,053
And that's what we are trying to achieve

511
00:22:27,054 --> 00:22:28,660
with the clustering assignment

512
00:22:28,660 --> 00:22:31,010
or later objective function.

513
00:22:31,010 --> 00:22:33,170
There are clustered
perturbation techniques,

514
00:22:33,170 --> 00:22:34,810
like when we talk about
differential privacy

515
00:22:34,810 --> 00:22:35,860
we do add noise.

516
00:22:35,860 --> 00:22:38,773
And ultimately we do
have to control that data

517
00:22:38,773 --> 00:22:42,500
between privacy and fairness or accuracy.

518
00:22:42,500 --> 00:22:44,290
Now, how do we do in a distributed setup?

519
00:22:44,290 --> 00:22:46,288
So there maybe a problem of scalability

520
00:22:46,288 --> 00:22:47,730
because a lot of points are there.

521
00:22:47,730 --> 00:22:50,290
So what is the objective
we try to achieve?

522
00:22:50,290 --> 00:22:52,590
We try the small weighted point set

523
00:22:52,590 --> 00:22:54,830
or such that for any K-subset,

524
00:22:54,830 --> 00:22:57,960
we are trying to optimize
a small subset of data

525
00:22:57,960 --> 00:23:01,600
from the whole dataset and
with a fairness constraint.

526
00:23:01,600 --> 00:23:03,639
And then we try to apply that

527
00:23:03,640 --> 00:23:05,890
with cluster perturbation
techniques fairness

528
00:23:05,890 --> 00:23:07,199
and then try to deploy that

529
00:23:07,199 --> 00:23:10,970
in the largest setup
to ensure scalability.

530
00:23:10,970 --> 00:23:12,550
And repeatedly, we can use that

531
00:23:12,550 --> 00:23:16,230
and see where the cluster
performance is maximum

532
00:23:16,230 --> 00:23:20,300
and that's to accelerate
the computation time,

533
00:23:20,300 --> 00:23:22,100
save the storage device, sorry,

534
00:23:22,100 --> 00:23:23,209
space of the devices,

535
00:23:23,210 --> 00:23:26,010
as well as there are
different distributions

536
00:23:26,010 --> 00:23:26,843
that we can compare.

537
00:23:26,843 --> 00:23:28,580
The first iteration we have that,

538
00:23:28,580 --> 00:23:29,832
second iteration we have that,

539
00:23:29,833 --> 00:23:30,923
then we compare that.

540
00:23:30,923 --> 00:23:33,810
Next step we'll enable the fairness term,

541
00:23:33,810 --> 00:23:35,780
that we can use in achieving that.

542
00:23:35,780 --> 00:23:38,340
So we have talked about this
example, I've just put it here.

543
00:23:38,340 --> 00:23:40,797
You can go through that in
the appendix and later on

544
00:23:40,797 --> 00:23:42,660
you can get in touch with us,

545
00:23:42,660 --> 00:23:45,810
like how there's a paper on
it, variational clustering.

546
00:23:45,810 --> 00:23:48,210
So we have taken IoT data
set for male and female

547
00:23:48,210 --> 00:23:50,850
and try to achieve the clustering results

548
00:23:50,850 --> 00:23:55,060
add different values of
the fairness objective.

549
00:23:55,060 --> 00:23:57,700
Now, how do we deploy
the federated learning

550
00:23:57,700 --> 00:23:58,990
in IoT edge networks

551
00:23:58,990 --> 00:24:01,700
so that the processing happens
at different components?

552
00:24:01,700 --> 00:24:05,570
So here you see an automobile scenario

553
00:24:05,570 --> 00:24:08,250
where we can want to apply
the federated learning

554
00:24:08,250 --> 00:24:10,660
with GPS coordinates, speed,
strength, transportation

555
00:24:10,660 --> 00:24:12,590
or electric vehicles.

556
00:24:12,590 --> 00:24:13,770
And when we try to do that,

557
00:24:13,770 --> 00:24:16,970
we see that the finest
granular feature like GPS,

558
00:24:16,970 --> 00:24:20,520
we are able to add noise and
get back the actual location.

559
00:24:20,520 --> 00:24:23,650
So we have to be very conscious
the way we add the noise

560
00:24:23,650 --> 00:24:28,130
because if a person is walking
around say a city in a road,

561
00:24:28,130 --> 00:24:32,690
if we add a wrong noise, 0.01
seconds of North or East,

562
00:24:32,690 --> 00:24:35,610
may end up into us by like 10 meters ahead

563
00:24:35,610 --> 00:24:37,419
and it may end up in a pool.

564
00:24:37,420 --> 00:24:40,350
So we have to be very careful
when we talk about ethics

565
00:24:40,350 --> 00:24:43,602
that helps us to comply by data contrast,

566
00:24:43,603 --> 00:24:45,420
compliance and ethics.

567
00:24:45,420 --> 00:24:49,710
And there is also a structured
or architecture available

568
00:24:49,710 --> 00:24:54,020
with federated localization
framework which uses clustering.

569
00:24:54,020 --> 00:24:55,950
So at the end of the day,

570
00:24:55,950 --> 00:24:58,560
we are having better performance
with fairness privacy.

571
00:24:58,560 --> 00:25:00,190
We are building robust models,

572
00:25:00,190 --> 00:25:01,680
which have fine print features

573
00:25:01,680 --> 00:25:03,370
as well as we are building data.

574
00:25:03,370 --> 00:25:06,350
We have moved from building
models, but see no data

575
00:25:06,350 --> 00:25:09,629
to non-independent and
identically distributed data.

576
00:25:09,630 --> 00:25:10,750
So that's the objective.

577
00:25:10,750 --> 00:25:11,720
And you can also go through

578
00:25:11,720 --> 00:25:14,710
that federated localization
framework, how it works.

579
00:25:14,710 --> 00:25:16,950
Now we have talked about
clustering, fairness, ethics,

580
00:25:16,950 --> 00:25:17,783
so many things.

581
00:25:17,783 --> 00:25:20,280
And one of the primary applications

582
00:25:20,280 --> 00:25:21,930
of this is the healthcare system.

583
00:25:21,930 --> 00:25:23,430
So here we get data

584
00:25:23,430 --> 00:25:25,570
from different industrial IoT applications

585
00:25:25,570 --> 00:25:28,000
and we try to train the data
with differential privacy

586
00:25:28,000 --> 00:25:29,807
as well as clustering.

587
00:25:29,807 --> 00:25:30,640
And what does it mean?

588
00:25:30,640 --> 00:25:33,030
Here you can see that
we have a presentation

589
00:25:33,030 --> 00:25:33,863
saying that black Americans

590
00:25:33,863 --> 00:25:37,080
are more likely to have prostate cancer.

591
00:25:37,080 --> 00:25:38,389
So how do we audit it?

592
00:25:38,390 --> 00:25:41,080
Or how do we see that this,
the result, is verifiable?

593
00:25:41,080 --> 00:25:44,260
So unless, and until
we have sufficient data

594
00:25:44,260 --> 00:25:47,350
for the minorities or black
people, we cannot say that.

595
00:25:47,350 --> 00:25:50,600
And that's the objective where
we have talked about bringing

596
00:25:50,600 --> 00:25:53,010
the concept of clustered
federated learning

597
00:25:53,010 --> 00:25:55,280
in distributed differentiated
private system,

598
00:25:55,280 --> 00:25:57,149
so that non-discriminatory predictors

599
00:25:57,150 --> 00:25:59,300
for protected groups of race or gender

600
00:25:59,300 --> 00:26:02,590
are proportionally represented
with equal error rates

601
00:26:02,590 --> 00:26:04,439
for either minority or majority.

602
00:26:04,440 --> 00:26:06,620
And when we think about doing that

603
00:26:06,620 --> 00:26:09,097
in a distributed architecture,
we have three layers finally.

604
00:26:09,097 --> 00:26:10,120
One is the client.

605
00:26:10,120 --> 00:26:12,560
So the client is able to send the models,

606
00:26:12,560 --> 00:26:15,520
the clusters are able to
send the models to the edge.

607
00:26:15,520 --> 00:26:18,520
So edge offloads part of
the task from the client

608
00:26:18,520 --> 00:26:20,200
so that it is doing, helping the clients

609
00:26:20,200 --> 00:26:21,690
not to process every task.

610
00:26:21,690 --> 00:26:23,520
And ultimately the bar from the edge,

611
00:26:23,520 --> 00:26:25,210
it gets to the central server

612
00:26:25,210 --> 00:26:27,980
who does the aggregation and
send back the local updates

613
00:26:27,980 --> 00:26:30,670
to agents as well as the client.

614
00:26:31,620 --> 00:26:33,179
So, that's the thing here,

615
00:26:33,180 --> 00:26:36,030
if you see this architecture

616
00:26:36,030 --> 00:26:38,560
on which we have put it in
a horizontal ship like that.

617
00:26:38,560 --> 00:26:39,679
Client is more engaged

618
00:26:39,680 --> 00:26:41,930
with the feature extraction,
feature distillation.

619
00:26:41,930 --> 00:26:43,940
One of the features
which have been important

620
00:26:43,940 --> 00:26:45,430
followed by noising techniques,

621
00:26:45,430 --> 00:26:48,170
where it does add feature perturbation.

622
00:26:48,170 --> 00:26:51,180
And the edge networks are more concerned

623
00:26:51,180 --> 00:26:52,840
about the heavy processing.

624
00:26:52,840 --> 00:26:55,510
So if the client is doing
the conventional layers

625
00:26:55,510 --> 00:26:57,680
which are extraction
where age is concerned

626
00:26:57,680 --> 00:27:00,550
with gradient loss and
stochastic gradients.

627
00:27:00,550 --> 00:27:02,370
So that through deep neural networks

628
00:27:02,370 --> 00:27:04,270
so that the optimization is achieved

629
00:27:04,270 --> 00:27:07,027
and that's pushed back to the
server and the server learns

630
00:27:07,027 --> 00:27:10,130
and averages all this and
pushes this back to the client

631
00:27:10,130 --> 00:27:10,963
at the edge.

632
00:27:10,963 --> 00:27:12,680
So that task gets distributed.

633
00:27:12,680 --> 00:27:14,850
And as we have seen that we now know

634
00:27:14,850 --> 00:27:17,909
into a totally new phase
where we have security,

635
00:27:17,910 --> 00:27:20,310
security multi-participant encryption,

636
00:27:20,310 --> 00:27:23,080
followed by encryption with
fixed position encoding,

637
00:27:23,080 --> 00:27:25,340
cluster sensitive data, where we ensure

638
00:27:25,340 --> 00:27:27,780
the clusters are fair
with proper select score,

639
00:27:27,780 --> 00:27:29,780
differential privacy and that model

640
00:27:29,780 --> 00:27:32,520
is ultimately pushed
from the global server.

641
00:27:32,520 --> 00:27:35,590
Now we have seen so many
things so hope it helps you.

642
00:27:35,590 --> 00:27:38,109
And when you go back, you
can apply those concepts

643
00:27:38,109 --> 00:27:41,319
you have learned about federated
learning, basic concepts.

644
00:27:41,319 --> 00:27:44,210
And we have gone into deep dive

645
00:27:44,210 --> 00:27:46,230
into federated learning evolution phases

646
00:27:46,230 --> 00:27:50,560
and how to select which use
case and which application

647
00:27:50,560 --> 00:27:52,139
will cater to which use case.

648
00:27:52,140 --> 00:27:54,420
Then we have seen also the architecture

649
00:27:54,420 --> 00:27:55,800
and the deployment systems

650
00:27:55,800 --> 00:27:58,850
that how you can incorporate
privacy, security and fairness.

651
00:27:58,850 --> 00:28:01,409
So there are a lot of resources
already mentioned the PPT.

652
00:28:01,410 --> 00:28:02,640
And if you want to get in touch,

653
00:28:02,640 --> 00:28:04,380
get in touch with Subarna and me

654
00:28:04,380 --> 00:28:06,120
and also you can tweet us.

655
00:28:06,120 --> 00:28:08,020
Before we close the session,

656
00:28:08,020 --> 00:28:09,889
just the final summary

657
00:28:09,890 --> 00:28:13,370
of how do you build a
complete federated model?

658
00:28:13,370 --> 00:28:15,830
So better you build a private fair

659
00:28:15,830 --> 00:28:20,399
or a fully ethical system
with interpretability in it.

660
00:28:20,400 --> 00:28:22,410
You are in a situation

661
00:28:22,410 --> 00:28:24,490
that you should be able
to deploy that model

662
00:28:24,490 --> 00:28:28,440
either on-premise native
cloud or in a hybrid setup.

663
00:28:28,440 --> 00:28:32,550
And it's not about only
about building privacy

664
00:28:32,550 --> 00:28:35,110
and fairness into the ML part of it,

665
00:28:35,110 --> 00:28:37,050
but you should also have a big,

666
00:28:37,050 --> 00:28:39,379
fairly-secured, big data pipeline

667
00:28:39,380 --> 00:28:44,380
and fair enough API privacy
built in into that architecture.

668
00:28:44,490 --> 00:28:46,160
Otherwise there will be adversity

669
00:28:46,160 --> 00:28:50,020
and that step that can
compromise your systems.

670
00:28:50,020 --> 00:28:51,660
And when we address federated learning,

671
00:28:51,660 --> 00:28:54,930
we are addressing privacy
issues with centralized data.

672
00:28:54,930 --> 00:28:56,450
So in a decentralized manner

673
00:28:57,370 --> 00:29:00,800
this use case can also largely be taken up

674
00:29:00,800 --> 00:29:04,310
with social recommendation
systems, GPS, trajectory, data

675
00:29:04,310 --> 00:29:05,914
and sensitive health data.

676
00:29:05,914 --> 00:29:09,470
Now, when we build the
systems, we have to be aware

677
00:29:09,470 --> 00:29:11,560
that the system is scalable

678
00:29:11,560 --> 00:29:15,139
and it does have
auto-protection enabled security

679
00:29:15,140 --> 00:29:17,940
and ethical ML framework built into it

680
00:29:17,940 --> 00:29:21,880
so that we are evolving every
day as we learn with new data.

681
00:29:21,880 --> 00:29:23,880
So if you see the cycle

682
00:29:23,880 --> 00:29:27,130
that the main characteristics of the setup

683
00:29:27,130 --> 00:29:29,010
should be identify the use cases,

684
00:29:29,010 --> 00:29:32,320
apply it measure the metrics,
which we have talked about,

685
00:29:32,320 --> 00:29:35,580
scale and improve all done
in a scalable agile manner.

686
00:29:35,580 --> 00:29:37,179
And we have the appendix

687
00:29:37,180 --> 00:29:39,330
and include some of the
research results here.

688
00:29:39,330 --> 00:29:44,127
You can go to it and later on contact us

689
00:29:44,127 --> 00:29:47,496
saying how did we do this
and how did we apply this.

690
00:29:47,497 --> 00:29:50,280
The GitHub link will
be shared in the chat.

691
00:29:50,280 --> 00:29:51,113
Thank you.

