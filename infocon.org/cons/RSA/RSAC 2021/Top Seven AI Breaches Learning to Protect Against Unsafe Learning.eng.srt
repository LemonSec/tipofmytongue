1
00:00:01,537 --> 00:00:02,940
- Hello, and thanks for joining today.

2
00:00:02,940 --> 00:00:03,900
My name's Davi Ottenheimer.

3
00:00:03,900 --> 00:00:06,240
I'm the VP of Trust and
Digital Ethics at Inrupt

4
00:00:06,240 --> 00:00:08,880
and I'm gonna talk today about
the top seven AI breaches

5
00:00:08,880 --> 00:00:11,600
which is really learning to
protect against unsafe learning.

6
00:00:11,600 --> 00:00:13,240
A bit of tongue in cheek there,

7
00:00:13,240 --> 00:00:14,073
but you'll see in a minute

8
00:00:14,073 --> 00:00:15,750
how what we're talking about today is

9
00:00:15,750 --> 00:00:18,759
what should have happened
with AI, what was expected,

10
00:00:18,760 --> 00:00:20,370
and then what actually was delivered.

11
00:00:20,370 --> 00:00:21,830
And then we'll dive in a little deeper

12
00:00:21,830 --> 00:00:23,590
into examples of where things go wrong

13
00:00:23,590 --> 00:00:25,170
so you understand essentially why we have

14
00:00:25,170 --> 00:00:28,780
these breaches of trust in
artificial intelligence systems.

15
00:00:28,780 --> 00:00:30,110
I have to give a trigger warning.

16
00:00:30,110 --> 00:00:33,480
Unfortunately, a lot of the
material we talk about today is

17
00:00:33,480 --> 00:00:34,699
disturbing or upsetting to people

18
00:00:34,700 --> 00:00:36,390
because the nature of the intelligence

19
00:00:36,390 --> 00:00:38,650
that we're talking about
can be very upsetting.

20
00:00:38,650 --> 00:00:40,833
And so with that, let's jump in.

21
00:00:41,700 --> 00:00:44,030
First, I want to give you
a couple quick stories

22
00:00:44,030 --> 00:00:45,193
to set the context.

23
00:00:46,170 --> 00:00:47,800
The first story is the Tatra 87

24
00:00:47,800 --> 00:00:49,779
which a lot of people
apparently don't know about.

25
00:00:49,780 --> 00:00:52,330
Maybe people in the car cultures do,

26
00:00:52,330 --> 00:00:54,320
but this is a very famous car in the sense

27
00:00:54,320 --> 00:00:55,560
that it was way ahead of its time.

28
00:00:55,560 --> 00:00:59,550
It was a very interesting
concept in how fast it went

29
00:00:59,550 --> 00:01:01,220
and how good it was,

30
00:01:01,220 --> 00:01:02,960
mile per gallon,
efficiencies and so forth,

31
00:01:02,960 --> 00:01:06,070
but when the Nazis invaded Czechoslovakia

32
00:01:06,070 --> 00:01:07,350
they basically stole the design.

33
00:01:07,350 --> 00:01:09,470
You may recognize it from
the Porsche or the Volkswagen

34
00:01:09,470 --> 00:01:11,760
which is basically where the
Germans got the idea from.

35
00:01:11,760 --> 00:01:13,560
But also it killed more Nazi generals

36
00:01:13,560 --> 00:01:14,570
than World War II itself.

37
00:01:14,570 --> 00:01:17,240
And the Czechs ultimately
called it their secret weapon

38
00:01:17,240 --> 00:01:19,229
because so many Germans
were killing themselves

39
00:01:19,230 --> 00:01:20,220
with this technology.

40
00:01:20,220 --> 00:01:21,870
And so it's important to keep in mind

41
00:01:21,870 --> 00:01:23,640
that when people say they want
to rush into new technology

42
00:01:23,640 --> 00:01:25,140
and they want to get ahold
of the latest greatest

43
00:01:25,140 --> 00:01:27,050
because it has so many
features and benefits,

44
00:01:27,050 --> 00:01:27,940
it may end up actually doing

45
00:01:27,940 --> 00:01:29,420
more harm to themselves than good.

46
00:01:29,420 --> 00:01:33,460
And in this case, that actually
is an interesting use case

47
00:01:33,460 --> 00:01:35,429
because of the competition of,

48
00:01:35,430 --> 00:01:36,470
or the competitive nature

49
00:01:36,470 --> 00:01:38,140
of what's going on with technology,

50
00:01:38,140 --> 00:01:40,150
often you think you're
gonna be the good guys

51
00:01:40,150 --> 00:01:41,110
and do the great things,

52
00:01:41,110 --> 00:01:43,790
but you'll ultimately end
up preventing yourself.

53
00:01:43,790 --> 00:01:46,170
They actually ended up
banning the generals.

54
00:01:46,170 --> 00:01:48,050
The Nazis had to ban
people from driving the car

55
00:01:48,050 --> 00:01:49,710
because they were so bad at it.

56
00:01:49,710 --> 00:01:52,220
But the second example,
or the second story,

57
00:01:52,220 --> 00:01:53,690
really takes that even further to say

58
00:01:53,690 --> 00:01:56,500
that in 1977, there was
this theory of John McCarthy

59
00:01:56,500 --> 00:01:58,320
who some people say is the father of AI

60
00:01:58,320 --> 00:01:59,509
or one of the fathers.

61
00:01:59,510 --> 00:02:02,317
And the 0.3 theory as I
call it is when he said,

62
00:02:02,317 --> 00:02:06,199
"You want 1.7 Einsteins and
0.3 of the Manhattan Project."

63
00:02:06,200 --> 00:02:08,060
And he says, "You want
the Einsteins first,"

64
00:02:08,060 --> 00:02:10,460
but he also has a caveat that
it can take up to 500 years.

65
00:02:10,460 --> 00:02:12,140
So who knows where we are
in the spectrum of time,

66
00:02:12,140 --> 00:02:13,429
but you definitely know

67
00:02:13,430 --> 00:02:15,610
that what you want is something
that is more collaborative

68
00:02:15,610 --> 00:02:18,010
than it is competitive,
back to my first story.

69
00:02:18,010 --> 00:02:20,810
So if you have collaboration,
the more natural state,

70
00:02:20,810 --> 00:02:21,670
then you can actually see

71
00:02:21,670 --> 00:02:22,920
if you're on the team as other people,

72
00:02:22,920 --> 00:02:24,640
we're all humans, we're working together,

73
00:02:24,640 --> 00:02:26,019
then we get a lot of benefits out of AI.

74
00:02:26,020 --> 00:02:27,380
But if you're in a competitive state

75
00:02:27,380 --> 00:02:28,850
and you're actually trying
to build nuclear weapons

76
00:02:28,850 --> 00:02:29,683
and kill each other,

77
00:02:29,683 --> 00:02:31,829
then you get into a very bad state,

78
00:02:31,830 --> 00:02:34,420
in the competitive state of
the Nazis trying to destroy

79
00:02:34,420 --> 00:02:37,630
the Czech Republic or
Czechoslovakia, right.

80
00:02:37,630 --> 00:02:40,120
It cause a lot of harm when
people get ahead of themselves

81
00:02:40,120 --> 00:02:41,160
in terms of technology,

82
00:02:41,160 --> 00:02:43,359
where they're gonna go with this stuff.

83
00:02:43,360 --> 00:02:45,410
So, and I find this to be true
more and more across science

84
00:02:45,410 --> 00:02:47,057
the more I study all the
examples in intelligence

85
00:02:47,057 --> 00:02:48,390
and all the uses of AI.

86
00:02:48,390 --> 00:02:50,709
I find that there's
collaboration risk, competition,

87
00:02:50,710 --> 00:02:53,610
and more collaboration usually
leads to better outcomes.

88
00:02:53,610 --> 00:02:56,020
Wherever it's used for competition
or competitive purposes

89
00:02:56,020 --> 00:02:57,370
it leads to pretty sad outcomes.

90
00:02:57,370 --> 00:03:00,680
So in modern context, 1977's 0.3 theory,

91
00:03:00,680 --> 00:03:02,040
AI should prevent harm.

92
00:03:02,040 --> 00:03:04,609
And you look at the Afrofeminist
concerns about data use,

93
00:03:04,610 --> 00:03:06,780
this actually comes up
quite a bit in Africa.

94
00:03:06,780 --> 00:03:08,900
When you look at all the places
that data's being collected

95
00:03:08,900 --> 00:03:10,810
and how it's being used and
what it should be used for

96
00:03:10,810 --> 00:03:12,870
and how AI integrates with that, right,

97
00:03:12,870 --> 00:03:15,050
you want a 1.7 approach to this

98
00:03:15,050 --> 00:03:18,340
as opposed to a 0.3 approach
to this if that makes sense.

99
00:03:18,340 --> 00:03:20,730
And even more so when you
talk about AI should stop

100
00:03:20,730 --> 00:03:22,420
catastrophe or catastrophic events

101
00:03:22,420 --> 00:03:24,339
for humanity and human rights law,

102
00:03:24,340 --> 00:03:25,830
NATO was talking about how to use AI

103
00:03:25,830 --> 00:03:27,860
and where AI is ethical,
where it should be used,

104
00:03:27,860 --> 00:03:28,990
and what a breach would look like.

105
00:03:28,990 --> 00:03:31,120
Again, think collaboration
versus competition.

106
00:03:31,120 --> 00:03:32,390
And NATO is by nature,

107
00:03:32,390 --> 00:03:36,269
it's defined by essentially
being a militarized competition

108
00:03:36,270 --> 00:03:38,560
that's trying to protect some
states against other states.

109
00:03:38,560 --> 00:03:41,080
So it's in a very interesting
and dangerous space.

110
00:03:41,080 --> 00:03:43,003
So from there I just want to point out

111
00:03:43,003 --> 00:03:45,899
that in 2016, I made
an RSA Conference talk

112
00:03:45,900 --> 00:03:48,240
where I basically laid out
Facebook as a doomsday machine.

113
00:03:48,240 --> 00:03:49,857
And I've seen this topic
come up more and more.

114
00:03:49,857 --> 00:03:51,960
"The Atlantic" even wrote up
an article that was very good

115
00:03:51,960 --> 00:03:54,320
about Facebook being a
doomsday machine in 2020.

116
00:03:54,320 --> 00:03:55,859
And they talked about how every situation

117
00:03:55,860 --> 00:03:57,440
they found extremist violence,

118
00:03:57,440 --> 00:03:59,820
they found Facebook there as well.

119
00:03:59,820 --> 00:04:01,760
More and more evidence that
the actual AI algorithms

120
00:04:01,760 --> 00:04:03,429
and the platforms are doing things

121
00:04:03,430 --> 00:04:05,070
that ultimately cause tons of harm.

122
00:04:05,070 --> 00:04:06,380
And I would say this is not that different

123
00:04:06,380 --> 00:04:07,213
from a doomsday machine.

124
00:04:07,213 --> 00:04:10,930
So you find that AI from the
1930s to today is in risk

125
00:04:10,930 --> 00:04:12,510
of being that actual horrible thing

126
00:04:12,510 --> 00:04:14,220
that's gonna bring the end of the world.

127
00:04:14,220 --> 00:04:15,800
And so we really have to think carefully

128
00:04:15,800 --> 00:04:16,899
about what these breaches mean,

129
00:04:16,899 --> 00:04:19,799
very circumspectly as
opposed to in a narrow sense.

130
00:04:19,800 --> 00:04:21,269
Are we winning this competition or that,

131
00:04:21,269 --> 00:04:23,840
but more like are we doing
good in the 1.7 sense

132
00:04:23,840 --> 00:04:27,429
or are we precipitously going
to end up in the 0.3 sense

133
00:04:27,430 --> 00:04:29,490
launching a nuclear weapon
that we would not want to

134
00:04:29,490 --> 00:04:31,150
such as the Cuban Missile Crisis event

135
00:04:31,150 --> 00:04:33,370
which is what I talked about in 2016.

136
00:04:33,370 --> 00:04:35,390
So today what I want to talk about is

137
00:04:35,390 --> 00:04:37,240
first, what should be expected of AI,

138
00:04:37,240 --> 00:04:38,960
and I give this in three
different sections,

139
00:04:38,960 --> 00:04:41,469
incompleteness, imitation, and efficiency.

140
00:04:41,470 --> 00:04:42,810
The first one might sound a little harsh,

141
00:04:42,810 --> 00:04:44,180
but until we get into it you'll see

142
00:04:44,180 --> 00:04:46,420
that incompleteness really is from 1929.

143
00:04:46,420 --> 00:04:48,380
Godel very famously said

144
00:04:48,380 --> 00:04:51,880
that in any consistent
axiomatic mathematical system,

145
00:04:51,880 --> 00:04:53,969
propositions that can't
be proved or disproved.

146
00:04:53,970 --> 00:04:55,080
And what he's saying that,

147
00:04:55,080 --> 00:04:55,960
you don't want to read the German.

148
00:04:55,960 --> 00:04:57,140
It's even harder to understand.

149
00:04:57,140 --> 00:04:59,770
But in English, the translation really is

150
00:04:59,770 --> 00:05:02,210
that a computer can't answer
all the questions itself

151
00:05:02,210 --> 00:05:03,299
to achieve its objectives.

152
00:05:03,300 --> 00:05:05,040
It needs some external influence.

153
00:05:05,040 --> 00:05:06,450
It needs some oversight.

154
00:05:06,450 --> 00:05:08,650
It needs some external inherited sources.

155
00:05:08,650 --> 00:05:10,969
And another way somebody's
put this to me recently is

156
00:05:10,970 --> 00:05:12,550
intelligence is the spice in the soup.

157
00:05:12,550 --> 00:05:13,990
It's not the soup itself.

158
00:05:13,990 --> 00:05:15,890
So you need to have a perspective on this

159
00:05:15,890 --> 00:05:17,690
that it is incomplete by definition.

160
00:05:17,690 --> 00:05:20,020
And the second is that when
you take simple thoughts

161
00:05:20,020 --> 00:05:23,299
and try to make them into
binary circuits as in 1937,

162
00:05:23,300 --> 00:05:26,330
you see there's evidence
of this from Shannon

163
00:05:26,330 --> 00:05:27,909
who's credited with basically birthing

164
00:05:27,910 --> 00:05:28,800
the computer industry,

165
00:05:28,800 --> 00:05:31,720
you have A equals Z, Z
equals A, simple logic.

166
00:05:31,720 --> 00:05:33,590
And if you say Bob bought X toothpaste,

167
00:05:33,590 --> 00:05:35,159
therefore he'll buy a toothbrush,

168
00:05:35,160 --> 00:05:36,820
this is not the same as human reasoning

169
00:05:36,820 --> 00:05:38,420
where you say a cloud leads to rain.

170
00:05:38,420 --> 00:05:40,730
You can't say a rain leads to cloud.

171
00:05:40,730 --> 00:05:41,630
Violence leads to pain.

172
00:05:41,630 --> 00:05:44,380
You can't say pain leads
to violence necessarily.

173
00:05:44,380 --> 00:05:46,670
So in logic and in the
way that we represent

174
00:05:46,670 --> 00:05:48,720
the completeness of the human mind

175
00:05:48,720 --> 00:05:49,553
and the way that it works,

176
00:05:49,553 --> 00:05:52,280
we have an incompleteness
situation with computers.

177
00:05:52,280 --> 00:05:54,960
The second point is
that the imitation game

178
00:05:54,960 --> 00:05:57,390
or the way that it's been
framed during Alan Turing,

179
00:05:57,390 --> 00:05:58,940
in 1950, he's basically saying

180
00:05:58,940 --> 00:06:00,500
computers can imitate the human.

181
00:06:00,500 --> 00:06:02,512
It's a game to try to figure out

182
00:06:02,512 --> 00:06:04,790
if you're talking to a machine or a human.

183
00:06:04,790 --> 00:06:05,780
And I do want to point out here

184
00:06:05,780 --> 00:06:07,530
that Turing was essentially persecuted

185
00:06:07,530 --> 00:06:09,640
for his beliefs or his choices.

186
00:06:09,640 --> 00:06:10,650
And so he was actually killed

187
00:06:10,650 --> 00:06:13,690
by his own country very unceremoniously

188
00:06:13,690 --> 00:06:15,000
and he was led to his own death,

189
00:06:15,000 --> 00:06:17,100
but that is fascinating

190
00:06:17,100 --> 00:06:18,970
when you think about
how we need to respect

191
00:06:18,970 --> 00:06:20,380
people's difference of opinions today.

192
00:06:20,380 --> 00:06:23,580
And so we base a lot of our
AI and a lot of the imitation

193
00:06:23,580 --> 00:06:25,930
and a lot of the ideas
around right and wrong

194
00:06:25,930 --> 00:06:28,040
and AI will figure out what's good and bad

195
00:06:28,040 --> 00:06:32,040
on a person who was essentially
persecuted for who he was.

196
00:06:32,040 --> 00:06:33,100
And you see in the '50s

197
00:06:33,100 --> 00:06:35,950
rapid advancement of machines and AI.

198
00:06:35,950 --> 00:06:36,900
The term is coined

199
00:06:36,900 --> 00:06:39,060
and the IBM comes out with
a machine, the Perceptron,

200
00:06:39,060 --> 00:06:41,360
that's supposed to
recognize right from left.

201
00:06:41,360 --> 00:06:43,400
May as well been right from wrong, right.

202
00:06:43,400 --> 00:06:46,070
And so you get very quickly
this sort of excitement.

203
00:06:46,070 --> 00:06:48,780
And from there in 1958,
towards the end of that period,

204
00:06:48,780 --> 00:06:50,679
you find the Navy in
the United States saying

205
00:06:50,680 --> 00:06:52,100
hey, we can recognize people.

206
00:06:52,100 --> 00:06:53,640
We can instantly translate speech.

207
00:06:53,640 --> 00:06:54,710
Things are gonna be amazing.

208
00:06:54,710 --> 00:06:55,750
Here's 1958.

209
00:06:55,750 --> 00:06:57,010
You see a robot being developed

210
00:06:57,010 --> 00:06:59,210
that has very human attributes.

211
00:06:59,210 --> 00:07:02,090
Looks very much like it's
going to turn into this thing

212
00:07:02,090 --> 00:07:03,900
that's going to replace humans.

213
00:07:03,900 --> 00:07:06,890
And actually from there you
even see in 1964 the idea

214
00:07:06,890 --> 00:07:09,210
that you have to have recognition

215
00:07:09,210 --> 00:07:11,599
in systems like anti-aircraft systems

216
00:07:11,600 --> 00:07:14,890
or long-range missiles or artillery,

217
00:07:14,890 --> 00:07:17,460
and that very naturally
translates into people thinking

218
00:07:17,460 --> 00:07:18,729
well, if we can do facial recognition,

219
00:07:18,730 --> 00:07:20,830
then on a micro level we
can do all sorts of things

220
00:07:20,830 --> 00:07:23,150
to protect ourselves from harm as well.

221
00:07:23,150 --> 00:07:24,200
It's a very old concept.

222
00:07:24,200 --> 00:07:25,810
And I don't think a lot of people realize

223
00:07:25,810 --> 00:07:27,720
that it was 1966 even
when people were talking

224
00:07:27,720 --> 00:07:28,810
about license plate recognition

225
00:07:28,810 --> 00:07:31,040
where tapes were being recorded
and sent to a mainframe,

226
00:07:31,040 --> 00:07:34,710
and this was $1.4 million
equivalent in today's terms

227
00:07:34,710 --> 00:07:36,530
in New York State that was used on bridges

228
00:07:36,530 --> 00:07:39,059
to try to figure out who's
driving in, who's driving out,

229
00:07:39,060 --> 00:07:39,930
try to find criminals.

230
00:07:39,930 --> 00:07:41,620
So a lot of the stuff that was promised

231
00:07:41,620 --> 00:07:45,640
out of these ideas of
efficiency suddenly came in 1979

232
00:07:45,640 --> 00:07:46,740
to sort of a screeching halt

233
00:07:46,740 --> 00:07:47,800
when people said wait a minute,

234
00:07:47,800 --> 00:07:52,210
the efficiency around this
and the fairness of this

235
00:07:52,210 --> 00:07:55,739
by ceding agency to machines
given the incompleteness,

236
00:07:55,740 --> 00:07:57,750
given the imitation, by ceding this,

237
00:07:57,750 --> 00:07:59,580
you end up inefficiency
running into something else

238
00:07:59,580 --> 00:08:00,413
which is racism.

239
00:08:00,413 --> 00:08:02,660
And so that really, I
think, set people back.

240
00:08:02,660 --> 00:08:04,270
So let me get to the next section here

241
00:08:04,270 --> 00:08:08,010
which is, good segue, what
has been delivered by AI.

242
00:08:08,010 --> 00:08:10,490
In fact, what's been
delivered by AI has been

243
00:08:10,490 --> 00:08:11,580
not just incompleteness,

244
00:08:11,580 --> 00:08:13,130
not just oh, you need a helping hand here

245
00:08:13,130 --> 00:08:15,290
to figure out what's going
on, but total gibberish.

246
00:08:15,290 --> 00:08:16,310
And what's been delivered

247
00:08:16,310 --> 00:08:18,300
in terms of imitation
has been basically fraud.

248
00:08:18,300 --> 00:08:19,550
And then finally "Breaking Bad,"

249
00:08:19,550 --> 00:08:22,330
another presentation I gave
at RSA Conference in the past,

250
00:08:22,330 --> 00:08:25,020
the efficiency has led to serious breaks

251
00:08:25,020 --> 00:08:27,539
in the trust relationship.

252
00:08:27,540 --> 00:08:28,640
So first, incompleteness.

253
00:08:28,640 --> 00:08:31,979
Very simply, as Shannon told us in 1937,

254
00:08:31,980 --> 00:08:34,179
you have this ability to
say marijuana causes cancer.

255
00:08:34,179 --> 00:08:35,280
You can translate that.

256
00:08:35,280 --> 00:08:38,199
But flip it around,
cancer causes marijuana,

257
00:08:38,200 --> 00:08:39,270
the machine can't tell.

258
00:08:39,270 --> 00:08:41,210
It absolutely can't tell when
words are shuffled around

259
00:08:41,210 --> 00:08:42,450
to the point where if you take

260
00:08:42,450 --> 00:08:45,610
an actual oppositional
model of very complex ideas

261
00:08:45,610 --> 00:08:47,180
in the human world,

262
00:08:47,180 --> 00:08:50,260
the machine has absolutely
no idea how to deal with it.

263
00:08:50,260 --> 00:08:51,720
So GPT is essentially useless

264
00:08:51,720 --> 00:08:53,150
when you talk about incompleteness.

265
00:08:53,150 --> 00:08:55,069
And so when you talk about imitation,

266
00:08:55,070 --> 00:08:57,110
interestingly you find that
there's a lot of fraud,

267
00:08:57,110 --> 00:09:00,140
deep fakes, and some of that
is, I think, fascinating

268
00:09:00,140 --> 00:09:01,010
in terms of power struggle

269
00:09:01,010 --> 00:09:02,490
because you think about white men

270
00:09:02,490 --> 00:09:04,620
who have to listen to nonwhite men,

271
00:09:04,620 --> 00:09:06,000
it's actually a scary proposition.

272
00:09:06,000 --> 00:09:07,850
So I think a lot of
people saying oh my god,

273
00:09:07,850 --> 00:09:09,620
what if Tom Cruise is
fake are really saying

274
00:09:09,620 --> 00:09:12,330
what if someone else who's
not Tom Cruise has that voice

275
00:09:12,330 --> 00:09:14,500
and can talk to me as
though they are Tom Cruise?

276
00:09:14,500 --> 00:09:16,530
That's a much deeper problem
than it is just the fact

277
00:09:16,530 --> 00:09:18,770
that someone looks like someone else.

278
00:09:18,770 --> 00:09:20,270
White men fear loss of
control, essentially.

279
00:09:20,270 --> 00:09:22,020
And so they lose a lot of the privilege

280
00:09:22,020 --> 00:09:24,000
that they had of being white men.

281
00:09:24,000 --> 00:09:24,950
And so more importantly,

282
00:09:24,950 --> 00:09:27,290
I think we see a lot of the
machines are actually failing

283
00:09:27,290 --> 00:09:28,189
at a very severe level.

284
00:09:28,190 --> 00:09:29,420
They're actually killing people.

285
00:09:29,420 --> 00:09:31,969
Human learning machines are not doing this

286
00:09:31,970 --> 00:09:35,033
and they're getting held in a
much higher consequence level.

287
00:09:35,033 --> 00:09:36,530
A human, little children,

288
00:09:36,530 --> 00:09:38,720
a little child was picking a flower

289
00:09:38,720 --> 00:09:42,170
and was basically taken to
court over it at a bus stop,

290
00:09:42,170 --> 00:09:46,199
whereas a car that's learning
not even at a child level can

291
00:09:46,200 --> 00:09:48,953
run over people and kill them
and not be held responsible.

292
00:09:50,330 --> 00:09:53,120
So two very interesting
cases of imitation.

293
00:09:53,120 --> 00:09:55,070
And the third in efficiency,

294
00:09:55,070 --> 00:09:57,260
what we've seen in factories
in terms of the efficiency

295
00:09:57,260 --> 00:09:58,200
and the dangers of it is

296
00:09:58,200 --> 00:09:59,670
that things get much worse with robots.

297
00:09:59,670 --> 00:10:00,520
We see a lot of death.

298
00:10:00,520 --> 00:10:01,790
We see a lot of injuries.

299
00:10:01,790 --> 00:10:05,250
We actually see it worse to
work with robots than without.

300
00:10:05,250 --> 00:10:08,040
And you see the
optimization of our freedom

301
00:10:08,040 --> 00:10:09,630
basically squeezing people to the point

302
00:10:09,630 --> 00:10:11,450
where we're seeing a repeat
of what we saw in 1906

303
00:10:11,450 --> 00:10:13,640
which was hugely unsafe factories

304
00:10:13,640 --> 00:10:15,280
and unsafe working conditions.

305
00:10:15,280 --> 00:10:17,709
Particularly Amazon was called
out for this several times.

306
00:10:17,710 --> 00:10:18,810
And so if you just want to look

307
00:10:18,810 --> 00:10:20,839
at the whole history of
efficiency with robots,

308
00:10:20,840 --> 00:10:23,700
you see that there's a lot
of deaths across the spectrum

309
00:10:23,700 --> 00:10:26,140
but they've been happening
more and more recently.

310
00:10:26,140 --> 00:10:26,973
In fact, if you look

311
00:10:26,973 --> 00:10:29,849
at the Tesla quarter-by-quarter accidents,

312
00:10:29,850 --> 00:10:31,920
it's been increasing dramatically versus

313
00:10:31,920 --> 00:10:33,819
relative to all the other
vehicles on the road.

314
00:10:33,820 --> 00:10:35,580
They're more than anyone seemed to have

315
00:10:35,580 --> 00:10:37,460
more and more accidents per mile

316
00:10:37,460 --> 00:10:39,310
when you do the simple math.

317
00:10:39,310 --> 00:10:40,660
All right, so where things go wrong.

318
00:10:40,660 --> 00:10:42,780
You've seen what we're
supposed to be seeing,

319
00:10:42,780 --> 00:10:46,520
the promises, you saw
basically what happened,

320
00:10:46,520 --> 00:10:47,560
and so that's bad.

321
00:10:47,560 --> 00:10:49,560
And so now let's talk about
where things go wrong.

322
00:10:49,560 --> 00:10:51,400
And let me give a caveat there

323
00:10:51,400 --> 00:10:53,829
that there's a book about
surveillance capitalism

324
00:10:53,830 --> 00:10:55,690
which I believe to be dangerously wrong

325
00:10:55,690 --> 00:10:58,230
because surveillance when
you think about it is

326
00:10:58,230 --> 00:10:59,690
watching over, supervision.

327
00:10:59,690 --> 00:11:02,910
It's basically this idea of
being a parent, if you will.

328
00:11:02,910 --> 00:11:06,469
Supervision, surveillance,
very similar concepts.

329
00:11:06,470 --> 00:11:07,760
You surveil things as scientists.

330
00:11:07,760 --> 00:11:08,890
Capitalism is the idea

331
00:11:08,890 --> 00:11:11,630
of you have competition
de-emphasizing collaboration.

332
00:11:11,630 --> 00:11:14,060
Very dangerous, but I'm not
gonna say you can't compete.

333
00:11:14,060 --> 00:11:16,099
Of course competition
has a place in the world.

334
00:11:16,100 --> 00:11:17,450
So you don't want to
get rid of competition

335
00:11:17,450 --> 00:11:19,430
and you don't want to get
rid of surveillance really.

336
00:11:19,430 --> 00:11:21,920
Be like saying you want
to get rid of science

337
00:11:21,920 --> 00:11:24,670
and that's really bad
because authorized knowledge

338
00:11:24,670 --> 00:11:27,010
and fair play and
ownership are fundamental

339
00:11:27,010 --> 00:11:29,120
to our scientific systems.

340
00:11:29,120 --> 00:11:30,530
And so you should think instead perhaps

341
00:11:30,530 --> 00:11:32,199
about the harms from debt capitalism.

342
00:11:32,200 --> 00:11:33,270
That's really the problem

343
00:11:33,270 --> 00:11:35,500
where people are put into a
situation they can't get out of.

344
00:11:35,500 --> 00:11:36,900
So they're indebted.

345
00:11:36,900 --> 00:11:38,490
Their consent is violated.

346
00:11:38,490 --> 00:11:40,560
There's excessive knowledge
they didn't approve.

347
00:11:40,560 --> 00:11:42,239
There's exploitation of them.

348
00:11:42,240 --> 00:11:44,900
They're controlled in a
way that they can't get out

349
00:11:44,900 --> 00:11:46,350
and they can't leave the pitch they're on,

350
00:11:46,350 --> 00:11:48,150
that they're playing the
game they don't want to play.

351
00:11:48,150 --> 00:11:49,069
They're in competition

352
00:11:49,070 --> 00:11:50,620
when they don't want to be in competition,

353
00:11:50,620 --> 00:11:51,453
like being a slave

354
00:11:51,453 --> 00:11:52,870
and told you have to
compete with the others

355
00:11:52,870 --> 00:11:53,800
if want to eat food

356
00:11:53,800 --> 00:11:55,699
as opposed to hey, I don't
want to have to compete

357
00:11:55,700 --> 00:11:57,040
for my food like this.

358
00:11:57,040 --> 00:12:00,050
It's just a ridiculous
context people are in.

359
00:12:00,050 --> 00:12:02,170
So it's not surveillance
capitalism any more

360
00:12:02,170 --> 00:12:04,540
than I would say that slavery
was about surveillance.

361
00:12:04,540 --> 00:12:07,010
It's much more about debt and
it's about debt capitalism

362
00:12:07,010 --> 00:12:08,730
that we should be talking.

363
00:12:08,730 --> 00:12:10,950
So if you criticize things too much

364
00:12:10,950 --> 00:12:11,870
with the surveillance capitalism,

365
00:12:11,870 --> 00:12:13,240
I believe you get into mysticism

366
00:12:13,240 --> 00:12:14,440
because you're basically going back

367
00:12:14,440 --> 00:12:15,950
to really simple observations

368
00:12:15,950 --> 00:12:18,020
unless you have the power of surveillance

369
00:12:18,020 --> 00:12:20,540
and unless you have the
power of competition.

370
00:12:20,540 --> 00:12:23,410
And so I would illustrate this very simply

371
00:12:23,410 --> 00:12:25,240
as if you're in a binary world,

372
00:12:25,240 --> 00:12:27,150
things are very easy,
routine, minimal judgment.

373
00:12:27,150 --> 00:12:28,370
You get rid of all the surveillance,

374
00:12:28,370 --> 00:12:30,520
you get rid of all the
overseeing and supervision,

375
00:12:30,520 --> 00:12:31,810
but you lose all the knowledge

376
00:12:31,810 --> 00:12:33,880
that's necessary for
essentially advancement.

377
00:12:33,880 --> 00:12:35,090
And if I put it back in,

378
00:12:35,090 --> 00:12:37,850
here you can see politicians
exist in a ranked world.

379
00:12:37,850 --> 00:12:39,760
Contrarians exist in a heatmap world.

380
00:12:39,760 --> 00:12:41,850
Critics who are essential to good science

381
00:12:41,850 --> 00:12:44,740
and essential to good social advancement,

382
00:12:44,740 --> 00:12:47,260
you need your critics
who use absolute ranking,

383
00:12:47,260 --> 00:12:49,130
well, they would cease
to exist essentially

384
00:12:49,130 --> 00:12:51,670
if you get rid of all of
those abilities to talk about,

385
00:12:51,670 --> 00:12:53,579
to surveil and to compete.

386
00:12:53,580 --> 00:12:54,740
And then philosophers of course

387
00:12:54,740 --> 00:12:57,090
from the highest orders,
I believe, of thought.

388
00:12:58,070 --> 00:12:59,910
So narrow-mindedness basically becomes

389
00:12:59,910 --> 00:13:02,540
accepting what is versus what should be,

390
00:13:02,540 --> 00:13:04,040
and that's what we have
to be careful about.

391
00:13:04,040 --> 00:13:05,209
We need to have that information.

392
00:13:05,210 --> 00:13:06,440
We need to gather that information.

393
00:13:06,440 --> 00:13:08,000
We need to look at that
information as humans

394
00:13:08,000 --> 00:13:09,470
in order to figure out where we want to be

395
00:13:09,470 --> 00:13:11,870
as opposed to just collecting
things very minimally

396
00:13:11,870 --> 00:13:12,780
and not understanding it

397
00:13:12,780 --> 00:13:14,370
and saying well, this is
what's been in the past,

398
00:13:14,370 --> 00:13:15,990
so this is what's gonna be in the future.

399
00:13:15,990 --> 00:13:17,640
The fundamental building
blocks of the United States,

400
00:13:17,640 --> 00:13:20,080
for example, are white
supremacy and misogyny,

401
00:13:20,080 --> 00:13:21,610
and we don't want that to be the future.

402
00:13:21,610 --> 00:13:22,830
We want to change that.

403
00:13:22,830 --> 00:13:25,600
And so the future shouldn't
just resemble the past.

404
00:13:25,600 --> 00:13:27,580
It's in conflict with our moral values.

405
00:13:27,580 --> 00:13:29,270
We should make sure that
we change the future

406
00:13:29,270 --> 00:13:30,990
based on our past, right?

407
00:13:30,990 --> 00:13:32,580
So we can't just accept things as they are

408
00:13:32,580 --> 00:13:33,710
the way that machines do.

409
00:13:33,710 --> 00:13:36,070
And what makes intelligent
humans so dangerous is,

410
00:13:36,070 --> 00:13:38,480
for example, con games
where you narrow down

411
00:13:38,480 --> 00:13:39,710
the games that you play to a point

412
00:13:39,710 --> 00:13:41,130
where people are successful

413
00:13:41,130 --> 00:13:42,939
because they've defined
away all of the things

414
00:13:42,940 --> 00:13:44,440
that would hold them responsible,

415
00:13:44,440 --> 00:13:45,930
the consequences of their actions.

416
00:13:45,930 --> 00:13:48,729
Edison, for example, would
take other people's inventions,

417
00:13:48,730 --> 00:13:50,980
but somehow we'd label him
a genius at inventions.

418
00:13:50,980 --> 00:13:52,460
It's just not true.

419
00:13:52,460 --> 00:13:54,330
Appropriated other people's ideas.

420
00:13:54,330 --> 00:13:55,710
And the more you do research on him,

421
00:13:55,710 --> 00:13:56,543
the more you realize

422
00:13:56,543 --> 00:13:59,120
what an inhumane, horrible person he was.

423
00:13:59,120 --> 00:14:00,480
In America's civil rights struggle,

424
00:14:00,480 --> 00:14:02,940
its racist history makes
it especially vulnerable

425
00:14:02,940 --> 00:14:03,773
to AI risks

426
00:14:03,773 --> 00:14:05,130
because if you allow people like Edison

427
00:14:05,130 --> 00:14:08,200
to be given credit for things
and called intelligent,

428
00:14:08,200 --> 00:14:09,400
then you're allowing machines

429
00:14:09,400 --> 00:14:11,380
that are as bad or worse than Edison

430
00:14:11,380 --> 00:14:13,770
to also be given credit
for doing terrible things.

431
00:14:13,770 --> 00:14:15,220
And so you get dangers increasing

432
00:14:15,220 --> 00:14:16,760
through tech domain shifts

433
00:14:16,760 --> 00:14:21,760
where basically repeating
rifles become the effective,

434
00:14:23,140 --> 00:14:25,050
the example of, or
machine guns even becoming

435
00:14:25,050 --> 00:14:26,270
the effective example of technology

436
00:14:26,270 --> 00:14:27,740
that shifts the entire domain.

437
00:14:27,740 --> 00:14:31,000
And in digital slavery, that
kind of very dangerous shift

438
00:14:31,000 --> 00:14:32,490
from one domain to the other is

439
00:14:32,490 --> 00:14:36,150
the new form of this technology
that can seriously upend

440
00:14:36,150 --> 00:14:37,810
and change the way that power is balanced.

441
00:14:37,810 --> 00:14:39,540
So we're really talking about
power more than anything else

442
00:14:39,540 --> 00:14:42,990
when we talk about AI
breaches and risks to people.

443
00:14:42,990 --> 00:14:44,930
So let me put it in terms of real AI

444
00:14:44,930 --> 00:14:47,530
or intelligent machine games

445
00:14:47,530 --> 00:14:50,220
where they game specifications
the same way an Edison would

446
00:14:50,220 --> 00:14:53,130
or a colonialist would or
someone who wants to do harm.

447
00:14:53,130 --> 00:14:54,630
So what you see, for example, is

448
00:14:54,630 --> 00:14:56,050
my favorite example ever really,

449
00:14:56,050 --> 00:14:59,400
was you want to make
machines flip pancakes

450
00:14:59,400 --> 00:15:01,380
and they get penalized if
they drop them on the floor.

451
00:15:01,380 --> 00:15:03,760
So they start flipping them
so high they never land.

452
00:15:03,760 --> 00:15:05,420
Nobody eats, everyone starves to death,

453
00:15:05,420 --> 00:15:06,709
but it's okay because they're winning

454
00:15:06,710 --> 00:15:08,570
because the pancakes
aren't touching the floor.

455
00:15:08,570 --> 00:15:10,260
And they do this sort of gamification

456
00:15:10,260 --> 00:15:12,300
because they find loopholes essentially.

457
00:15:12,300 --> 00:15:14,339
And this is true in human world too.

458
00:15:14,340 --> 00:15:16,940
Russia, for example, was trying
to bombard foreign athletes

459
00:15:16,940 --> 00:15:17,773
to where they'd quit

460
00:15:17,773 --> 00:15:20,450
by just hitting them
with lots of information

461
00:15:20,450 --> 00:15:21,740
that would disrupt their thinking

462
00:15:21,740 --> 00:15:23,800
and disrupt their ability to perform.

463
00:15:23,800 --> 00:15:24,990
People were giving up in the same way

464
00:15:24,990 --> 00:15:27,860
that AI was winning a tic-tac-toe game

465
00:15:27,860 --> 00:15:28,810
for what it's worth.

466
00:15:28,810 --> 00:15:30,839
So it figured out that
if it did infinite moves

467
00:15:30,840 --> 00:15:31,920
that exhausted the opponent,

468
00:15:31,920 --> 00:15:33,727
they couldn't figure
out where on the board

469
00:15:33,727 --> 00:15:35,530
because it was an infinite-sized board,

470
00:15:35,530 --> 00:15:36,480
they couldn't figure out on the board

471
00:15:36,480 --> 00:15:38,240
where the next move was,
they would just give up.

472
00:15:38,240 --> 00:15:40,360
And that's how tic-tac-toe
was being won by AI.

473
00:15:40,360 --> 00:15:42,920
So I think a lot of these
things are useful to understand

474
00:15:42,920 --> 00:15:43,752
because like I said,

475
00:15:43,753 --> 00:15:45,750
AI power abuse is really
about civil rights.

476
00:15:45,750 --> 00:15:46,950
And what it comes down to is

477
00:15:46,950 --> 00:15:49,140
that things are changing
because of the domain shift

478
00:15:49,140 --> 00:15:50,310
and the way that AI is being used.

479
00:15:50,310 --> 00:15:52,329
And so these breaks are
especially important to understand

480
00:15:52,330 --> 00:15:55,830
if you really want to prevent
harm at massive levels.

481
00:15:55,830 --> 00:15:56,900
And so if you want to ask yourself

482
00:15:56,900 --> 00:15:57,890
what can I do about this,

483
00:15:57,890 --> 00:15:59,569
wow, this is really gonna
get bad really fast,

484
00:15:59,570 --> 00:16:01,930
you obviously don't want
to a nuclear explosion.

485
00:16:01,930 --> 00:16:03,680
You obviously don't
want catastrophic harms.

486
00:16:03,680 --> 00:16:04,512
You don't want racism.

487
00:16:04,513 --> 00:16:05,610
You don't want all these horrible things.

488
00:16:05,610 --> 00:16:07,210
Well, you need to figure out

489
00:16:07,210 --> 00:16:09,060
not just how do you fix these algorithms,

490
00:16:09,060 --> 00:16:09,989
very common question.

491
00:16:09,990 --> 00:16:11,390
And you need to not just figure out

492
00:16:11,390 --> 00:16:13,470
how does the algorithm
interact with society at large.

493
00:16:13,470 --> 00:16:14,650
Is it doing bad things?

494
00:16:14,650 --> 00:16:16,860
Is it flipping pancakes
and starving people

495
00:16:16,860 --> 00:16:18,700
by throwing them too high?

496
00:16:18,700 --> 00:16:19,870
But you need to figure
out how you interact

497
00:16:19,870 --> 00:16:22,940
with society at large and
where you get authority from

498
00:16:22,940 --> 00:16:26,000
when you add your completeness
to the incompleteness of AI.

499
00:16:26,000 --> 00:16:27,750
So what is the background of the people

500
00:16:27,750 --> 00:16:28,583
who are working on this stuff

501
00:16:28,583 --> 00:16:30,060
and where did they make
their assumptions is

502
00:16:30,060 --> 00:16:31,500
much deeper issue.

503
00:16:31,500 --> 00:16:33,440
So let's find AI, first of all.

504
00:16:33,440 --> 00:16:34,630
Where is this happening?

505
00:16:34,630 --> 00:16:35,980
And ultimately I'd have to say

506
00:16:35,980 --> 00:16:37,870
it's like software, it's everywhere.

507
00:16:37,870 --> 00:16:40,670
When I got into security,
basically it was everywhere

508
00:16:40,670 --> 00:16:42,370
because everyone used
hardware and software.

509
00:16:42,370 --> 00:16:43,210
The same is in AI.

510
00:16:43,210 --> 00:16:44,670
I find that everywhere I find data,

511
00:16:44,670 --> 00:16:46,380
everywhere I find interfaces the data,

512
00:16:46,380 --> 00:16:47,760
someone's trying to slap on some AI

513
00:16:47,760 --> 00:16:49,300
or use some sort of
artificial intelligence

514
00:16:49,300 --> 00:16:51,060
machine learning of some kind,

515
00:16:51,060 --> 00:16:52,660
so expect it to be everywhere.

516
00:16:52,660 --> 00:16:54,270
That's really, when you get down to it,

517
00:16:54,270 --> 00:16:55,103
that's pretty much it.

518
00:16:55,103 --> 00:16:56,130
Second, you want to test,

519
00:16:56,130 --> 00:16:58,410
again, like when I got
started in security,

520
00:16:58,410 --> 00:17:00,640
you gotta jump in and start
testing everything everywhere

521
00:17:00,640 --> 00:17:02,360
and find where things are broken.

522
00:17:02,360 --> 00:17:05,540
So find vulnerabilities in every
process you can find in AI.

523
00:17:05,540 --> 00:17:06,440
It's actually quite fun

524
00:17:06,440 --> 00:17:07,950
and there's a lot of work to be done.

525
00:17:07,950 --> 00:17:10,630
So for example, you can
use a strategic checklist.

526
00:17:10,630 --> 00:17:11,560
You can have a test plan

527
00:17:11,560 --> 00:17:13,710
where you say let's look for
the fairness, the impact,

528
00:17:13,710 --> 00:17:15,500
and the transparency.

529
00:17:15,500 --> 00:17:16,700
This is different because it used to be

530
00:17:16,700 --> 00:17:18,430
just looking at consequences primarily.

531
00:17:18,430 --> 00:17:19,410
I feel like now you have to look

532
00:17:19,410 --> 00:17:22,069
at the motives of the
people who are in the game.

533
00:17:22,069 --> 00:17:24,720
And so fairness or inherited rights,

534
00:17:24,720 --> 00:17:25,900
this incompleteness theory,

535
00:17:25,900 --> 00:17:27,020
allowing people to figure out

536
00:17:27,020 --> 00:17:30,960
where the rights are coming
from outside really helps

537
00:17:30,960 --> 00:17:32,150
frame the competitions.

538
00:17:32,150 --> 00:17:34,040
And if you look at this Daytona 500 quote

539
00:17:34,040 --> 00:17:35,899
by Darrell Waltrip, it helps let you know

540
00:17:35,900 --> 00:17:37,540
that everyone's cheating all the time

541
00:17:37,540 --> 00:17:39,370
any time you have any kind of competition.

542
00:17:39,370 --> 00:17:41,030
So really look for the people
who are trying to use that

543
00:17:41,030 --> 00:17:44,500
to their advantage using
this sort of FIT model.

544
00:17:44,500 --> 00:17:46,250
But if you look for fairness,
impact, and transparency,

545
00:17:46,250 --> 00:17:48,490
I think you're on your
way to testing effectively

546
00:17:48,490 --> 00:17:49,520
using a simple checklist.

547
00:17:49,520 --> 00:17:50,639
And so what would that look like?

548
00:17:50,640 --> 00:17:51,880
You can do the FIT of data.

549
00:17:51,880 --> 00:17:53,370
You can do the FIT of the objectives.

550
00:17:53,370 --> 00:17:55,129
You can look at the
assessment team itself,

551
00:17:55,130 --> 00:17:56,620
the blue team, red team.

552
00:17:56,620 --> 00:17:57,560
You can look at the process.

553
00:17:57,560 --> 00:17:59,040
You can look at the operational oversight

554
00:17:59,040 --> 00:18:00,840
or a board, if you will.

555
00:18:00,840 --> 00:18:02,590
A lot of people have
to have a review board

556
00:18:02,590 --> 00:18:04,800
if they're going to use
AI because without it,

557
00:18:04,800 --> 00:18:06,310
just like without having auditing,

558
00:18:06,310 --> 00:18:08,190
how would you know that
the books aren't cooked?

559
00:18:08,190 --> 00:18:10,750
And so you ultimately don't
want to become the authority

560
00:18:10,750 --> 00:18:12,690
the way a referee in Russia might be,

561
00:18:12,690 --> 00:18:13,680
beating up the players.

562
00:18:13,680 --> 00:18:16,630
You want to be a person
who allows people to play

563
00:18:16,630 --> 00:18:18,580
so that they can ultimately be competitive

564
00:18:18,580 --> 00:18:21,220
in a collaborative way,
if that makes sense.

565
00:18:21,220 --> 00:18:22,800
So let's go on.

566
00:18:22,800 --> 00:18:25,629
Regulation fundamentally is
pretty worthless without tests.

567
00:18:25,630 --> 00:18:26,840
You can't enforce things

568
00:18:26,840 --> 00:18:28,240
unless you can prove things are wrong.

569
00:18:28,240 --> 00:18:31,540
So this is more and more an
essential aspect of learning.

570
00:18:31,540 --> 00:18:34,300
Intelligence, artificial
intelligence is not gonna be

571
00:18:34,300 --> 00:18:36,070
successful unless people
are able to go in and test

572
00:18:36,070 --> 00:18:38,260
and prove that it's breaking.

573
00:18:38,260 --> 00:18:40,970
And if you want to apply

574
00:18:40,970 --> 00:18:44,130
and make enforceable engineering
actions for safe learning,

575
00:18:44,130 --> 00:18:45,340
you can do things tactically

576
00:18:45,340 --> 00:18:46,870
where you tell engineering, for example,

577
00:18:46,870 --> 00:18:49,090
that in an easy, routine,
minimal judgment world,

578
00:18:49,090 --> 00:18:51,010
for example, you don't know
much about what's going on,

579
00:18:51,010 --> 00:18:52,770
but you know there needs
to be an off button

580
00:18:52,770 --> 00:18:53,920
and there needs to be a reset button.

581
00:18:53,920 --> 00:18:55,050
These are fundamentals, right?

582
00:18:55,050 --> 00:18:56,220
If the machine isn't working

583
00:18:56,220 --> 00:18:58,230
in the case of a car not
driving the right way,

584
00:18:58,230 --> 00:18:59,063
you hit the brake,

585
00:18:59,063 --> 00:19:00,810
it turns it off or you take control back.

586
00:19:00,810 --> 00:19:03,149
A reset button says erase
all those decisions you made.

587
00:19:03,150 --> 00:19:04,240
You've learned wrong.

588
00:19:04,240 --> 00:19:06,860
How do you re-correct
or change those things

589
00:19:06,860 --> 00:19:08,709
that it had learned because
it learned in the wrong way

590
00:19:08,710 --> 00:19:09,930
and you don't want to
be penalized for that?

591
00:19:09,930 --> 00:19:11,750
Strategic is much bigger, right?

592
00:19:11,750 --> 00:19:12,780
You have an ethics review board

593
00:19:12,780 --> 00:19:14,470
as I started to allude to already.

594
00:19:14,470 --> 00:19:15,430
You start to threat model

595
00:19:15,430 --> 00:19:17,570
and figure out how do you
identify going forward

596
00:19:17,570 --> 00:19:18,919
where things are going to be risky

597
00:19:18,920 --> 00:19:20,280
or where they're gonna cause problems.

598
00:19:20,280 --> 00:19:21,460
And you start to gate releases

599
00:19:21,460 --> 00:19:22,670
through all these tests and audits.

600
00:19:22,670 --> 00:19:24,970
So you set up process and procedures.

601
00:19:24,970 --> 00:19:25,910
And then ultimately you have to have

602
00:19:25,910 --> 00:19:27,150
some executive-level enforcement.

603
00:19:27,150 --> 00:19:28,440
You gotta have the ability

604
00:19:28,440 --> 00:19:31,320
to drop the hammer on stuff that's broken.

605
00:19:31,320 --> 00:19:34,639
And so let me just make a big,
very important caveat there.

606
00:19:34,640 --> 00:19:35,920
When you start to do these tests

607
00:19:35,920 --> 00:19:37,230
they may be very unpopular,

608
00:19:37,230 --> 00:19:39,180
just like security always
runs into these problems

609
00:19:39,180 --> 00:19:40,170
with some executives.

610
00:19:40,170 --> 00:19:41,680
And so Amazon in particular said

611
00:19:41,680 --> 00:19:43,320
they do not like tests being run

612
00:19:43,320 --> 00:19:44,340
because they're inconsistent

613
00:19:44,340 --> 00:19:45,840
with how something's supposed to be used.

614
00:19:45,840 --> 00:19:48,120
Yeah, duh, that's the whole point.

615
00:19:48,120 --> 00:19:49,620
If I can do things that are inconsistent

616
00:19:49,620 --> 00:19:52,219
with how this is designed to
be used, you got a problem.

617
00:19:52,220 --> 00:19:54,450
And so you gotta sometimes
educate places like Amazon

618
00:19:54,450 --> 00:19:56,920
about what the right way to fix things is.

619
00:19:56,920 --> 00:19:59,121
So let's jump into the
top seven AI breaches.

620
00:19:59,121 --> 00:19:59,954
We don't have a lot of time

621
00:19:59,954 --> 00:20:01,530
so I'm gonna blow through
these pretty fast,

622
00:20:01,530 --> 00:20:03,580
but I think they'll give you
a taste of what's out there.

623
00:20:03,580 --> 00:20:04,830
And there's actually a really great report

624
00:20:04,830 --> 00:20:06,129
that just came out from adverse AI

625
00:20:06,130 --> 00:20:08,890
which says they think there's
vision, analytics, language,

626
00:20:08,890 --> 00:20:10,290
and autonomy, four categories.

627
00:20:10,290 --> 00:20:11,440
I went with seven categories,

628
00:20:11,440 --> 00:20:12,930
so I'm gonna dive a little deeper,

629
00:20:12,930 --> 00:20:14,100
but fundamentally you can see

630
00:20:14,100 --> 00:20:16,510
that there's targeting based on how,

631
00:20:16,510 --> 00:20:17,740
at least from their perspective,

632
00:20:17,740 --> 00:20:19,140
how things are being used.

633
00:20:19,140 --> 00:20:19,973
I would actually say

634
00:20:19,973 --> 00:20:21,560
that their targeting
percentages come a lot

635
00:20:21,560 --> 00:20:23,129
from what they see in the industry,

636
00:20:23,130 --> 00:20:25,100
not from where we're
going to see things going.

637
00:20:25,100 --> 00:20:27,699
So I biased mine towards where
I think things are gonna go

638
00:20:27,700 --> 00:20:29,300
as opposed to just what we've been seeing.

639
00:20:29,300 --> 00:20:30,513
One, translation.

640
00:20:32,020 --> 00:20:33,080
Translation is an interesting one

641
00:20:33,080 --> 00:20:34,520
because as you remember in the beginning

642
00:20:34,520 --> 00:20:35,777
in the 1950s and 1960s,

643
00:20:35,777 --> 00:20:37,680
it was supposed to be done instantaneously

644
00:20:37,680 --> 00:20:40,220
and quickly replacing
people who do translation.

645
00:20:40,220 --> 00:20:42,310
In fact, what we're finding
is a lot of interesting cases

646
00:20:42,310 --> 00:20:43,970
where it's totally broken.

647
00:20:43,970 --> 00:20:45,850
And here's just a simple example.

648
00:20:45,850 --> 00:20:47,300
In Hungarian Predictive Gender,

649
00:20:47,300 --> 00:20:49,899
if you try to add an O
as you write program,

650
00:20:49,900 --> 00:20:53,300
the word program, programmer
in English would be equivalent,

651
00:20:53,300 --> 00:20:55,260
it flips the gender of the sentence.

652
00:20:55,260 --> 00:20:57,590
So she's an amazing programmer, you know,

653
00:20:57,590 --> 00:20:58,973
he's an amazing programmer.

654
00:21:00,927 --> 00:21:02,070
And so you gotta ask why?

655
00:21:02,070 --> 00:21:04,260
Why is he suddenly showing up?

656
00:21:04,260 --> 00:21:06,160
What's that prediction coming from?

657
00:21:06,160 --> 00:21:07,170
So let's dive a little deeper.

658
00:21:07,170 --> 00:21:09,010
In Finnish, you can see that this Pekka

659
00:21:09,010 --> 00:21:11,320
which is a male name
suddenly becomes different

660
00:21:11,320 --> 00:21:13,850
if you put some emotion in there.

661
00:21:13,850 --> 00:21:15,750
Loves cars, Pekka loves cars.

662
00:21:15,750 --> 00:21:20,530
He, and then on changes to female,

663
00:21:20,530 --> 00:21:22,680
and then it changes again to,

664
00:21:22,680 --> 00:21:24,290
angry, it goes back to he.

665
00:21:24,290 --> 00:21:25,840
So I could go on and on about these.

666
00:21:25,840 --> 00:21:26,860
There's so many examples.

667
00:21:26,860 --> 00:21:28,600
For example, if I put beautiful and sexy,

668
00:21:28,600 --> 00:21:31,760
it flips the gender from he to she, right?

669
00:21:31,760 --> 00:21:34,460
The word beautiful forces female override.

670
00:21:34,460 --> 00:21:35,950
So you can sort of predictably figure out

671
00:21:35,950 --> 00:21:37,450
where you want genders to be.

672
00:21:37,450 --> 00:21:39,500
And these obviously have effect on people.

673
00:21:39,500 --> 00:21:41,690
If they read something
and how it's translated,

674
00:21:41,690 --> 00:21:43,240
the he/she gender is very important

675
00:21:43,240 --> 00:21:45,110
in terms of people's perception of self.

676
00:21:45,110 --> 00:21:46,500
So car is gendered, for example,

677
00:21:46,500 --> 00:21:48,500
except when you add love it becomes she.

678
00:21:50,130 --> 00:21:51,250
Genders are inverted.

679
00:21:51,250 --> 00:21:53,850
I started off with he, she,
he, she, right, back and forth,

680
00:21:53,850 --> 00:21:56,570
but then it flips over to he, he, she, he.

681
00:21:56,570 --> 00:21:58,909
So you can see that's related to work.

682
00:21:58,910 --> 00:22:00,930
In some languages of
course they try to warn you

683
00:22:00,930 --> 00:22:02,550
by giving you some sense here

684
00:22:02,550 --> 00:22:04,169
that Turkish and gender.

685
00:22:04,170 --> 00:22:05,810
I mean, Persian and Turkish
have a gender warning

686
00:22:05,810 --> 00:22:08,480
that say basically he,
she, gender neutral,

687
00:22:08,480 --> 00:22:09,313
but that even breaks.

688
00:22:09,313 --> 00:22:10,370
I can flip it back and forth

689
00:22:10,370 --> 00:22:12,530
and the hes and shes disappear.

690
00:22:12,530 --> 00:22:14,639
And fairly reliably too
I can predict basically

691
00:22:14,640 --> 00:22:16,940
what gender I'm gonna get
based on the algorithm.

692
00:22:16,940 --> 00:22:18,960
So she, he ends up being she, she,

693
00:22:18,960 --> 00:22:21,200
and here you can see Persian
flipped back and forth.

694
00:22:21,200 --> 00:22:23,400
All right, so obviously
translation's breaking

695
00:22:23,400 --> 00:22:24,490
in a lot of different ways.

696
00:22:24,490 --> 00:22:25,620
I've done this for a lot of years

697
00:22:25,620 --> 00:22:27,909
and it seems to be getting
better in some ways

698
00:22:27,910 --> 00:22:28,870
but mainly worse

699
00:22:28,870 --> 00:22:30,149
because more and more
people are recognizing

700
00:22:30,150 --> 00:22:31,910
that it's a problem and
wondering should they even use

701
00:22:31,910 --> 00:22:33,870
the translation if they can't trust it?

702
00:22:33,870 --> 00:22:36,370
So two, let's talk about written speech.

703
00:22:36,370 --> 00:22:37,203
Now this is interesting

704
00:22:37,203 --> 00:22:40,940
because what you find is that
there's a lot of bad language

705
00:22:40,940 --> 00:22:42,690
on the Internet, of course,

706
00:22:42,690 --> 00:22:44,010
and so a lot of machines are

707
00:22:44,010 --> 00:22:45,200
supposed to be dealing with this.

708
00:22:45,200 --> 00:22:46,960
Chatbots in particular are
supposed to be protecting you

709
00:22:46,960 --> 00:22:47,930
against this sort of thing.

710
00:22:47,930 --> 00:22:49,780
But instead, what you
find is the Korean chatbot

711
00:22:49,780 --> 00:22:52,660
in this example starts to sort of spread,

712
00:22:52,660 --> 00:22:54,730
back to the example of Alan Turing, right,

713
00:22:54,730 --> 00:22:58,700
1950 and '54, '58, see these advances.

714
00:22:58,700 --> 00:23:00,354
People are talking about
how sensitive it is

715
00:23:00,354 --> 00:23:03,800
and important it is that we
remember his life and his role.

716
00:23:03,800 --> 00:23:04,633
Well, here you can see

717
00:23:04,633 --> 00:23:08,450
that verbally abusive language
is targeting LGBTQ groups,

718
00:23:08,450 --> 00:23:11,200
particular groups
consistently being targeted.

719
00:23:11,200 --> 00:23:12,033
That was right away.

720
00:23:12,033 --> 00:23:12,866
It's not like they came out of this

721
00:23:12,866 --> 00:23:13,770
and it happened over time.

722
00:23:13,770 --> 00:23:14,810
It was like right out of the box

723
00:23:14,810 --> 00:23:15,720
these things are being discovered.

724
00:23:15,720 --> 00:23:17,310
Here's another one from
Google where you can see

725
00:23:17,310 --> 00:23:20,870
that I am a gay black woman is 87% toxic.

726
00:23:20,870 --> 00:23:23,889
But if you start spewing
white supremacist narratives,

727
00:23:23,890 --> 00:23:26,010
not seen as toxic, another problem.

728
00:23:26,010 --> 00:23:27,530
That's just not right, right,

729
00:23:27,530 --> 00:23:29,280
the way that it's being interpreted.

730
00:23:29,280 --> 00:23:30,270
And so here's another one

731
00:23:30,270 --> 00:23:33,180
where the Amazon staff were targeting

732
00:23:33,180 --> 00:23:35,480
lesbian gay literature disproportionately.

733
00:23:35,480 --> 00:23:36,800
They said, "Oh, it's a software glitch,"

734
00:23:36,800 --> 00:23:37,633
but that didn't seem to be

735
00:23:37,633 --> 00:23:39,570
reliable explanation for the problem.

736
00:23:39,570 --> 00:23:41,639
They accidentally were
censoring all the gay literature

737
00:23:41,640 --> 00:23:44,810
off of their site using a software glitch.

738
00:23:44,810 --> 00:23:46,210
And then here's another example

739
00:23:46,210 --> 00:23:47,760
where Facebook is amplifying hate,

740
00:23:47,760 --> 00:23:49,560
10 billion false views,

741
00:23:49,560 --> 00:23:51,830
false and toxic content being spread

742
00:23:51,830 --> 00:23:53,669
by not releasing the algorithms they had

743
00:23:53,670 --> 00:23:55,630
that supposedly were capable
of getting rid of this.

744
00:23:55,630 --> 00:23:57,530
And we're talking about
like Facebook being

745
00:23:57,530 --> 00:24:00,370
75% of online hate and harassment,

746
00:24:00,370 --> 00:24:03,100
far beyond any other platform, far, far.

747
00:24:03,100 --> 00:24:04,870
75% of those who are harassed said

748
00:24:04,870 --> 00:24:06,100
it was happening on Facebook.

749
00:24:06,100 --> 00:24:07,169
Well, I shouldn't say that,

750
00:24:07,170 --> 00:24:08,860
but far, far beyond any other platform,

751
00:24:08,860 --> 00:24:11,000
just in their own league of allowing

752
00:24:11,000 --> 00:24:13,060
horrible, horrible, toxic content

753
00:24:13,060 --> 00:24:13,893
even though they say they have algorithms

754
00:24:13,893 --> 00:24:15,610
that can work on this.

755
00:24:15,610 --> 00:24:17,360
So three, let's talk about emotion,

756
00:24:18,250 --> 00:24:19,300
really health recognition,

757
00:24:19,300 --> 00:24:20,530
and this is a fascinating space.

758
00:24:20,530 --> 00:24:23,560
I went through some examples
myself where I found,

759
00:24:23,560 --> 00:24:24,899
this has been the same for years,

760
00:24:24,900 --> 00:24:26,380
I can basically fool any system

761
00:24:26,380 --> 00:24:28,170
into thinking whatever
I want my emotion is.

762
00:24:28,170 --> 00:24:31,710
So I could make it sad 99%, happy 98%.

763
00:24:31,710 --> 00:24:33,820
I had neither of those
emotions when I was doing this.

764
00:24:33,820 --> 00:24:35,129
Years ago I gave a presentation

765
00:24:35,130 --> 00:24:36,470
where I talked about I could take data

766
00:24:36,470 --> 00:24:40,090
from World Wrestling
Federation, I think, or WWE,

767
00:24:40,090 --> 00:24:41,459
and I could basically show

768
00:24:41,460 --> 00:24:43,050
that the guy on the
bottom who's being choked

769
00:24:43,050 --> 00:24:46,340
and stomped on has 91%
happy, 92% happiness

770
00:24:46,340 --> 00:24:47,173
which is ridiculous.

771
00:24:47,173 --> 00:24:48,006
And this is important

772
00:24:48,006 --> 00:24:49,490
because when you think about
drones flying around now

773
00:24:49,490 --> 00:24:51,920
that are doing emotional
analysis of huge crowds

774
00:24:51,920 --> 00:24:54,160
or even just sensors in
large, crowded spaces,

775
00:24:54,160 --> 00:24:56,160
they're gonna be assessing
everyone's emotions

776
00:24:56,160 --> 00:24:58,380
and reporting what they
think the state of a crowd is

777
00:24:58,380 --> 00:25:00,180
which is gonna lead to some response

778
00:25:01,220 --> 00:25:03,330
and even targeting based
on those emotions, right.

779
00:25:03,330 --> 00:25:04,270
Find all the happy people.

780
00:25:04,270 --> 00:25:05,670
Take out all the angry people.

781
00:25:05,670 --> 00:25:07,700
Respond because somebody's
gonna be in a fight.

782
00:25:07,700 --> 00:25:08,920
I've worked with a lot of this stuff

783
00:25:08,920 --> 00:25:10,300
in like individual cases.

784
00:25:10,300 --> 00:25:11,820
Like if you have a sensor that turns on

785
00:25:11,820 --> 00:25:12,939
and says hey, look at this spot,

786
00:25:12,940 --> 00:25:15,590
a fight's about the brew
because we hear anger,

787
00:25:15,590 --> 00:25:16,830
that's something people have to rely on

788
00:25:16,830 --> 00:25:19,120
and they're gonna respond to
that maybe even with force.

789
00:25:19,120 --> 00:25:21,209
So another part of health is of course

790
00:25:21,210 --> 00:25:24,770
using these diagnostics to
try to improve people's lives.

791
00:25:24,770 --> 00:25:28,129
We find that clinical systems
are fundamentally flawed

792
00:25:28,130 --> 00:25:29,270
again because of bias.

793
00:25:29,270 --> 00:25:30,980
So they'll be reporting things

794
00:25:30,980 --> 00:25:34,020
in a way that are such a high risk of bias

795
00:25:34,020 --> 00:25:34,940
that you can't even use them.

796
00:25:34,940 --> 00:25:36,130
They're so flawed.

797
00:25:36,130 --> 00:25:38,130
The underlying biases
are so poorly understood

798
00:25:38,130 --> 00:25:40,410
that you can't rely on
this for healthcare.

799
00:25:40,410 --> 00:25:42,450
All right, four, so gender classification,

800
00:25:42,450 --> 00:25:44,220
again, you can see these are themes

801
00:25:44,220 --> 00:25:46,040
that are sort of intertwined
repeating themselves

802
00:25:46,040 --> 00:25:46,873
a little bit,

803
00:25:46,873 --> 00:25:47,706
but if you took a picture like this

804
00:25:47,706 --> 00:25:49,320
and you fed it through
some of these algorithms

805
00:25:49,320 --> 00:25:52,210
and you said okay, is this male or female,

806
00:25:52,210 --> 00:25:53,860
it's fundamentally the wrong question.

807
00:25:53,860 --> 00:25:55,669
Even though computers are
ranking the way people look,

808
00:25:55,670 --> 00:25:57,920
they do things in a way that
don't make a lot of sense

809
00:25:57,920 --> 00:26:00,940
because binary
classifications erase people.

810
00:26:00,940 --> 00:26:02,860
Here's an algorithm that I
was talking to people about,

811
00:26:02,860 --> 00:26:03,693
they were developing this,

812
00:26:03,693 --> 00:26:06,790
that was determining gender from behind,

813
00:26:06,790 --> 00:26:07,990
from the back of the head.

814
00:26:07,990 --> 00:26:09,990
And it said men, women counted,

815
00:26:09,990 --> 00:26:12,470
but it didn't have a non-binary.

816
00:26:12,470 --> 00:26:14,960
And if it did, it assumed
it wasn't done yet

817
00:26:14,960 --> 00:26:15,793
as opposed to just saying

818
00:26:15,793 --> 00:26:17,070
okay, this is a non-binary.

819
00:26:17,070 --> 00:26:18,100
The gray space, right?

820
00:26:18,100 --> 00:26:20,129
So you really want to stop erasing people

821
00:26:20,130 --> 00:26:22,230
and allowing them to live
in the non-binary space

822
00:26:22,230 --> 00:26:23,670
if that's what they actually are.

823
00:26:23,670 --> 00:26:26,250
That's more fundamentally
respectful of the human.

824
00:26:26,250 --> 00:26:27,930
And it's an authorization question.

825
00:26:27,930 --> 00:26:29,980
Who's giving you the
authorization to define them

826
00:26:29,980 --> 00:26:32,450
in a way that they
wouldn't define themselves?

827
00:26:32,450 --> 00:26:33,780
So very important to understand

828
00:26:33,780 --> 00:26:35,879
in the classifications and gender.

829
00:26:35,880 --> 00:26:37,010
But you get into it a little bit further

830
00:26:37,010 --> 00:26:38,970
you can see Amazon, for
example, was training

831
00:26:38,970 --> 00:26:41,870
its AI recruitment tool
in men only basically.

832
00:26:41,870 --> 00:26:42,703
So it taught itself

833
00:26:42,703 --> 00:26:44,300
that only male candidates were preferable.

834
00:26:44,300 --> 00:26:45,370
It actually penalized anyone

835
00:26:45,370 --> 00:26:47,399
who was recognized as a woman.

836
00:26:47,400 --> 00:26:49,010
Any time they showed up
as a woman, it was binary.

837
00:26:49,010 --> 00:26:51,220
Boom, you're less desirable to us.

838
00:26:51,220 --> 00:26:52,860
And so that's a problem

839
00:26:52,860 --> 00:26:55,580
if they're trying to be
a fair hiring company.

840
00:26:55,580 --> 00:26:57,062
And so if you look at
Facebook versus LinkedIn,

841
00:26:57,063 --> 00:26:59,070
this is a very interesting study,

842
00:26:59,070 --> 00:27:01,169
they found a huge difference.

843
00:27:01,170 --> 00:27:03,240
They found that Facebook was biasing

844
00:27:03,240 --> 00:27:04,550
software engineer ads, for example.

845
00:27:04,550 --> 00:27:05,690
Lots of ads they tested.

846
00:27:05,690 --> 00:27:06,890
Lots of ads were biased.

847
00:27:06,890 --> 00:27:08,270
And so here's just one example

848
00:27:08,270 --> 00:27:11,570
where they said 63% of men
were being shown this ad.

849
00:27:11,570 --> 00:27:14,139
36% of females or women
were shown this ad.

850
00:27:14,140 --> 00:27:16,380
In LinkedIn it was basically
an equitable distribution.

851
00:27:16,380 --> 00:27:17,450
In fact, it was almost the opposite.

852
00:27:17,450 --> 00:27:19,970
You had more women seeing the ad than men.

853
00:27:19,970 --> 00:27:22,410
And so it was beyond what
could be legally justified

854
00:27:22,410 --> 00:27:24,140
which means they're breaking the law.

855
00:27:24,140 --> 00:27:26,880
And so this sort of algorithm
that's supposed to be fair

856
00:27:26,880 --> 00:27:31,090
and be more efficient and impersonate

857
00:27:31,090 --> 00:27:33,540
or ends up being so incomplete

858
00:27:33,540 --> 00:27:37,350
as to be completely
broken, illegally broken.

859
00:27:37,350 --> 00:27:40,840
So let's talk about five,
visual facial recognition.

860
00:27:40,840 --> 00:27:41,673
Here's an interesting one

861
00:27:41,673 --> 00:27:44,730
because I think what people
often don't understand is

862
00:27:44,730 --> 00:27:47,610
that even in the human world,
this is a really big problem.

863
00:27:47,610 --> 00:27:50,590
So machines being able to
do it is almost impossible.

864
00:27:50,590 --> 00:27:51,550
I think the idea that,

865
00:27:51,550 --> 00:27:53,290
or recognizing that
the problem is so great

866
00:27:53,290 --> 00:27:55,320
that they can do a very small
subset is the first step

867
00:27:55,320 --> 00:27:56,153
to understanding it.

868
00:27:56,153 --> 00:27:57,760
So go ahead, write down what you see here.

869
00:27:57,760 --> 00:27:58,780
I'll wait for a second

870
00:27:58,780 --> 00:28:00,370
as you write down the word that you think

871
00:28:00,370 --> 00:28:03,250
this image represents, or words.

872
00:28:03,250 --> 00:28:05,210
And so I ran it through the algorithms

873
00:28:05,210 --> 00:28:06,330
to see what it would come up with,

874
00:28:06,330 --> 00:28:07,163
and here you can see

875
00:28:07,163 --> 00:28:09,060
that it thinks it's a military dot reticle

876
00:28:09,060 --> 00:28:12,020
or a site for shooting things, okay.

877
00:28:12,020 --> 00:28:13,240
So I'm looking at a gun site.

878
00:28:13,240 --> 00:28:15,490
That's definitely dangerous feedback.

879
00:28:15,490 --> 00:28:17,250
So I edited out a little
bit of the white space.

880
00:28:17,250 --> 00:28:19,540
Let's just crop that image
size down a little bit.

881
00:28:19,540 --> 00:28:22,050
Boom, it's a happy, smiley,
fun game that, you know,

882
00:28:22,050 --> 00:28:25,280
maybe it's a kid that
is playing with a star

883
00:28:25,280 --> 00:28:26,760
and has a smiley face.

884
00:28:26,760 --> 00:28:29,370
So reduced space completely
changes the sentiment here.

885
00:28:29,370 --> 00:28:30,550
Completely changed the recognition

886
00:28:30,550 --> 00:28:32,270
of what we're dealing with

887
00:28:32,270 --> 00:28:35,629
which tongue in cheek you can
see things change dramatically

888
00:28:35,630 --> 00:28:37,160
based on what sort of
assumptions are being made.

889
00:28:37,160 --> 00:28:38,760
And all I did was change white space.

890
00:28:38,760 --> 00:28:40,830
I made it a square instead of a rectangle.

891
00:28:40,830 --> 00:28:42,030
Nothing else changed.

892
00:28:42,030 --> 00:28:43,879
So in reality, this is an actual test

893
00:28:43,880 --> 00:28:45,040
and it's given to humans.

894
00:28:45,040 --> 00:28:47,730
And it's a smart test to give people

895
00:28:47,730 --> 00:28:49,440
because what you're looking
at is a submarine porthole

896
00:28:49,440 --> 00:28:51,340
with a fish tail or
whatever you want it to be,

897
00:28:51,340 --> 00:28:53,800
but it's never usually
what people think it is.

898
00:28:53,800 --> 00:28:56,419
And so what does that look
like in real AI term again?

899
00:28:56,420 --> 00:28:58,430
Well, you can find that
Twitter was auto-cropping AI.

900
00:28:58,430 --> 00:29:00,110
So it was supposedly finding faces,

901
00:29:00,110 --> 00:29:01,770
but in fact it wasn't finding black faces.

902
00:29:01,770 --> 00:29:03,210
It was only finding white faces

903
00:29:03,210 --> 00:29:05,200
unless you flipped the contrast

904
00:29:05,200 --> 00:29:06,710
or messed with the gray scale

905
00:29:06,710 --> 00:29:07,633
and then it would find them.

906
00:29:07,633 --> 00:29:10,720
It's very interesting the
way it was being triggered.

907
00:29:10,720 --> 00:29:11,970
And so you can see here also

908
00:29:11,970 --> 00:29:13,900
that when you do some
grayscale adjustments

909
00:29:13,900 --> 00:29:14,950
on the image on the right,

910
00:29:14,950 --> 00:29:17,420
turns out the person looked
black instead of white.

911
00:29:17,420 --> 00:29:19,480
And so the ACLU filed a lawsuit

912
00:29:19,480 --> 00:29:21,050
based on the false arrest of blacks

913
00:29:21,050 --> 00:29:23,919
based on these crazy recognition systems

914
00:29:23,920 --> 00:29:25,390
where the computers completely get wrong

915
00:29:25,390 --> 00:29:26,600
who they're looking at.

916
00:29:26,600 --> 00:29:28,100
And then when the people appeal

917
00:29:28,100 --> 00:29:30,500
and say this is an incomplete assessment,

918
00:29:30,500 --> 00:29:32,440
the people have even
more, they double down.

919
00:29:32,440 --> 00:29:33,273
The police double down

920
00:29:33,273 --> 00:29:34,760
and say we should trust
technology even more.

921
00:29:34,760 --> 00:29:36,940
You should go Google it to
get an answer from a computer

922
00:29:36,940 --> 00:29:37,950
instead of really talking to us

923
00:29:37,950 --> 00:29:39,450
about what the problem is.

924
00:29:39,450 --> 00:29:41,910
A school exam, for example,
tried to recognize faces

925
00:29:41,910 --> 00:29:44,550
in order to reduce fraud and
cheating in the school room

926
00:29:44,550 --> 00:29:45,810
or to validate tests

927
00:29:45,810 --> 00:29:48,110
and it basically just
stopped seeing black faces.

928
00:29:48,110 --> 00:29:50,270
That's obviously a disaster.

929
00:29:50,270 --> 00:29:51,740
And Google even was so sensitive

930
00:29:51,740 --> 00:29:54,000
about its own racism that it
started blocking searches.

931
00:29:54,000 --> 00:29:55,510
So it actually took people out of

932
00:29:55,510 --> 00:29:56,830
or took all the searches out

933
00:29:56,830 --> 00:29:58,040
that it thought would reveal the fact

934
00:29:58,040 --> 00:29:59,680
that it was being racist.

935
00:29:59,680 --> 00:30:01,660
Uber terminated workers it couldn't see,

936
00:30:01,660 --> 00:30:03,360
again, trying to validate people

937
00:30:03,360 --> 00:30:05,516
in order to use the system and
the system couldn't see them.

938
00:30:05,517 --> 00:30:08,370
And so it actually
terminated them unfairly.

939
00:30:08,370 --> 00:30:10,669
And so let's talk about
the threat detection now

940
00:30:10,670 --> 00:30:12,690
because you can see all
the problems with language

941
00:30:12,690 --> 00:30:14,410
and written language and spoken language

942
00:30:14,410 --> 00:30:16,900
and facial detection
and emotional detection.

943
00:30:16,900 --> 00:30:18,260
So what happens if you try to figure out

944
00:30:18,260 --> 00:30:19,093
who's a threat to you?

945
00:30:19,093 --> 00:30:22,310
Well trust me, in this
example is a very fun paper.

946
00:30:22,310 --> 00:30:23,679
You know, if a boat's coming to attack

947
00:30:23,680 --> 00:30:24,790
or a train's coming to attack,

948
00:30:24,790 --> 00:30:26,170
you could probably figure it out.

949
00:30:26,170 --> 00:30:28,660
But if you have a person
coming to you on a horse

950
00:30:28,660 --> 00:30:32,130
and they carry a copyright
notice, good luck detecting them.

951
00:30:32,130 --> 00:30:35,170
Another example here is if
you put a simple fur coat

952
00:30:35,170 --> 00:30:37,450
on a Chihuahua, then they disappear

953
00:30:37,450 --> 00:30:40,010
because all you see as a feather boa.

954
00:30:40,010 --> 00:30:42,280
You don't see the dog, basic camouflage,

955
00:30:42,280 --> 00:30:44,270
and that's tongue in cheek, obviously.

956
00:30:44,270 --> 00:30:45,220
But even more to the point,

957
00:30:45,220 --> 00:30:47,650
if you took an image
like this from history,

958
00:30:47,650 --> 00:30:48,800
which is a very important image,

959
00:30:48,800 --> 00:30:51,950
and you say do see black
or white people in this,

960
00:30:51,950 --> 00:30:53,210
it's actually again the wrong question

961
00:30:53,210 --> 00:30:54,480
because what you're seeing is

962
00:30:54,480 --> 00:30:56,340
an inability to recognize the true threat

963
00:30:56,340 --> 00:30:58,770
which is white supremacy
or white insecurity.

964
00:30:58,770 --> 00:31:00,680
And so, and you see that a
lot, actually, in the news.

965
00:31:00,680 --> 00:31:02,220
The AI systems are not being trained

966
00:31:02,220 --> 00:31:03,180
and not being used to see

967
00:31:03,180 --> 00:31:06,020
that there's a lot of people
who are very dangerous threats

968
00:31:06,020 --> 00:31:07,639
that are in our midst.

969
00:31:07,640 --> 00:31:09,010
In fact, when you look at things

970
00:31:09,010 --> 00:31:11,280
that AI is not doing, right,

971
00:31:11,280 --> 00:31:12,290
so they're not looking at,

972
00:31:12,290 --> 00:31:13,590
the algorithms are not looking

973
00:31:13,590 --> 00:31:15,490
at evidence of white supremacists.

974
00:31:15,490 --> 00:31:16,930
They're not looking at
evidence of people using things

975
00:31:16,930 --> 00:31:19,270
like the white ethnostate of Rhodesia

976
00:31:19,270 --> 00:31:21,170
as a signal to each other to organize.

977
00:31:22,190 --> 00:31:24,780
Hands up, a good example
of this, used to mean

978
00:31:24,780 --> 00:31:26,530
in 1979, a white ethnostate.

979
00:31:26,530 --> 00:31:28,160
It was you put your hands up.

980
00:31:28,160 --> 00:31:29,980
If you don't, we shoot you, right.

981
00:31:29,980 --> 00:31:31,190
And that's basically the way they rolled.

982
00:31:31,190 --> 00:31:33,040
They would disguise themselves as blacks.

983
00:31:33,040 --> 00:31:34,250
Then they'd go around shooting blacks

984
00:31:34,250 --> 00:31:35,300
and just saying hands up.

985
00:31:35,300 --> 00:31:36,240
People were very confused

986
00:31:36,240 --> 00:31:38,400
and they would shoot them
based on their reaction.

987
00:31:38,400 --> 00:31:40,150
But that all changed in 2021.

988
00:31:40,150 --> 00:31:42,640
The same people were doing the same things

989
00:31:42,640 --> 00:31:43,890
in the same area of the world

990
00:31:43,890 --> 00:31:45,980
and suddenly they're
accused of war crimes.

991
00:31:45,980 --> 00:31:46,970
Now I point that out

992
00:31:46,970 --> 00:31:48,450
because the next thing that happens is

993
00:31:48,450 --> 00:31:50,900
we talk about how AI is
learning from the past

994
00:31:50,900 --> 00:31:52,220
in order to figure out
what it should be doing

995
00:31:52,220 --> 00:31:53,053
in the future.

996
00:31:53,053 --> 00:31:54,890
And so books like "Hands Up, Don't Shoot"

997
00:31:54,890 --> 00:31:55,930
kind of laid this out.

998
00:31:55,930 --> 00:31:58,850
But if you look at unlawful
and biased police practices

999
00:31:58,850 --> 00:32:00,510
and the data that's training them,

1000
00:32:00,510 --> 00:32:02,607
you see it's learning from
very bad practices in the past.

1001
00:32:02,607 --> 00:32:04,879
And the idea of this hands
up, don't shoot model

1002
00:32:04,880 --> 00:32:07,630
in the past can lead to
serious, serious war crimes,

1003
00:32:07,630 --> 00:32:09,100
serious, serious problems.

1004
00:32:09,100 --> 00:32:10,219
And if AI learns from them,

1005
00:32:10,220 --> 00:32:12,120
it's gonna make the same mistakes.

1006
00:32:12,120 --> 00:32:14,120
And so completely false claims

1007
00:32:14,120 --> 00:32:16,919
in the last point on this
I want to make basically

1008
00:32:16,920 --> 00:32:19,580
before we sort of close
out this section is

1009
00:32:19,580 --> 00:32:20,466
people can actually claim

1010
00:32:20,467 --> 00:32:21,320
that they have AI when they don't

1011
00:32:21,320 --> 00:32:23,110
and all they're really
doing is perpetuating bias.

1012
00:32:23,110 --> 00:32:24,459
All they're doing is trying to perpetuate

1013
00:32:24,460 --> 00:32:25,660
a sort of power imbalance.

1014
00:32:25,660 --> 00:32:28,860
And none of the stuff in the
Banjo company was making,

1015
00:32:28,860 --> 00:32:30,550
none of the stuff that they
were claiming was actually true.

1016
00:32:30,550 --> 00:32:31,800
They were just trying to get some money

1017
00:32:31,800 --> 00:32:33,430
in order to build systems

1018
00:32:33,430 --> 00:32:36,630
and the person had obvious
former neo-Nazi connections.

1019
00:32:36,630 --> 00:32:40,480
Another one just before
we go is antivirus bias

1020
00:32:41,795 --> 00:32:43,340
where people can actually
make a simple change

1021
00:32:43,340 --> 00:32:45,250
as I was doing earlier

1022
00:32:45,250 --> 00:32:46,750
but now you're trying
to detect real threats

1023
00:32:46,750 --> 00:32:47,760
and it's real situation.

1024
00:32:47,760 --> 00:32:49,060
So malware was able to slip through

1025
00:32:49,060 --> 00:32:51,129
by trying to look like just
basically anything else,

1026
00:32:51,130 --> 00:32:52,320
a simple game.

1027
00:32:52,320 --> 00:32:54,720
And another example I
just threw in here was

1028
00:32:54,720 --> 00:32:57,070
you have people trying to cry wolf.

1029
00:32:57,070 --> 00:32:58,970
Well, if you're detecting
wolves by looking at snow

1030
00:32:58,970 --> 00:33:01,390
you're gonna have a very broken
threat detection algorithm.

1031
00:33:01,390 --> 00:33:03,100
It's a famous case, so
I thought I'd add it.

1032
00:33:03,100 --> 00:33:05,070
But anyway, let's close out, as I said,

1033
00:33:05,070 --> 00:33:06,340
the last example, number seven.

1034
00:33:06,340 --> 00:33:08,770
So the top of the food
chart here in terms of

1035
00:33:08,770 --> 00:33:10,840
or the worst case, for
example, in terms of breaches

1036
00:33:10,840 --> 00:33:12,480
I believe to be transit safety,

1037
00:33:12,480 --> 00:33:14,380
and I've talked about
this a lot in the past.

1038
00:33:14,380 --> 00:33:15,450
I've given a lot of presentations

1039
00:33:15,450 --> 00:33:18,210
where I basically say hey, if
you can't see what's going on,

1040
00:33:18,210 --> 00:33:20,370
if you claim to be 90% effective

1041
00:33:20,370 --> 00:33:22,000
and I can throw some tests at you

1042
00:33:22,000 --> 00:33:24,590
and you're completely
stopped, then that's a fail.

1043
00:33:24,590 --> 00:33:26,899
And if a pedestrian jumps out
in front of you, easy to test,

1044
00:33:26,900 --> 00:33:28,690
if we throw up a Mylar balloon
in the middle of the street

1045
00:33:28,690 --> 00:33:30,330
and you run it over, that's a fail.

1046
00:33:30,330 --> 00:33:31,909
You're not allowed to drive anymore.

1047
00:33:31,910 --> 00:33:33,470
You shouldn't be allowed
to be on the road.

1048
00:33:33,470 --> 00:33:37,180
But in fact, you see
over time, 2016, 2017,

1049
00:33:37,180 --> 00:33:39,710
multiple tests have shown
that these cars cannot see.

1050
00:33:39,710 --> 00:33:40,680
They can't see signs.

1051
00:33:40,680 --> 00:33:42,750
They can't see objects.

1052
00:33:42,750 --> 00:33:45,310
They're easily gamed, easily fooled.

1053
00:33:45,310 --> 00:33:46,418
And it's not just the test.

1054
00:33:46,419 --> 00:33:47,410
We're not just talking about academics.

1055
00:33:47,410 --> 00:33:50,660
We see in tweets and evidence in the news

1056
00:33:50,660 --> 00:33:52,660
that people are having
problems with their machines

1057
00:33:52,660 --> 00:33:54,180
that are supposedly in production.

1058
00:33:54,180 --> 00:33:55,960
You can put a sign on the back of a bus

1059
00:33:55,960 --> 00:33:58,300
and, I mean, you can put a sign

1060
00:33:58,300 --> 00:33:59,659
in the back of anything really, a car,

1061
00:33:59,660 --> 00:34:03,560
and the car behind you won't
read that as a street sign.

1062
00:34:03,560 --> 00:34:04,570
It's ridiculous.

1063
00:34:04,570 --> 00:34:07,439
You can see that the amount of confusion

1064
00:34:07,440 --> 00:34:10,219
that a Tesla has between
signs is unacceptable.

1065
00:34:10,219 --> 00:34:11,179
You would not be able to drive

1066
00:34:11,179 --> 00:34:12,960
if you had this kind
of problem as a human.

1067
00:34:12,960 --> 00:34:14,290
More to the point,

1068
00:34:14,290 --> 00:34:17,310
humans are being tested
more and more than ever

1069
00:34:17,310 --> 00:34:19,299
to prove that they actually
are who they say they are

1070
00:34:19,300 --> 00:34:21,510
whereas these machines can't even see

1071
00:34:21,510 --> 00:34:22,760
flashing lights on a police car

1072
00:34:22,760 --> 00:34:26,159
and big letters that say
don't crash into me, right.

1073
00:34:26,159 --> 00:34:27,429
Who gets to keep driving?

1074
00:34:27,429 --> 00:34:29,339
And there's a big difference in reactions

1075
00:34:29,340 --> 00:34:30,900
when we have failings in AI too.

1076
00:34:30,900 --> 00:34:32,820
When Uber failed to see
red lights and pedestrians,

1077
00:34:32,820 --> 00:34:34,280
it said, "Hey, fine, we'll go to Arizona,"

1078
00:34:34,280 --> 00:34:36,750
where they ended up killing a
pedestrian very predictably.

1079
00:34:36,750 --> 00:34:38,250
You could have, I told people

1080
00:34:38,250 --> 00:34:39,330
that was what's gonna happen,

1081
00:34:39,330 --> 00:34:40,400
I think a lot of people did,

1082
00:34:40,400 --> 00:34:42,230
and that's exactly what happened.

1083
00:34:42,230 --> 00:34:44,409
So in 2018, Tesla killed a pedestrian

1084
00:34:44,409 --> 00:34:46,629
and they had a very
different response than Uber.

1085
00:34:46,630 --> 00:34:48,239
They basically said, "Hey,
we're gonna shut down

1086
00:34:48,239 --> 00:34:51,089
our PR Department, we're
gonna stop returning calls,

1087
00:34:51,090 --> 00:34:53,070
and we're gonna start
upselling this technology

1088
00:34:53,070 --> 00:34:54,610
as an even more expensive one."

1089
00:34:54,610 --> 00:34:56,630
And so I guess you could say predictably

1090
00:34:56,630 --> 00:34:58,340
Tesla over time has made the same mistake

1091
00:34:58,340 --> 00:34:59,173
over and over again.

1092
00:34:59,173 --> 00:35:00,270
What's interesting though
is they've replaced

1093
00:35:00,270 --> 00:35:01,103
a lot of technology.

1094
00:35:01,103 --> 00:35:02,480
So they got rid of the Mobileye.

1095
00:35:02,480 --> 00:35:03,860
They had a falling out with that company.

1096
00:35:03,860 --> 00:35:05,160
They went with Nvidia.

1097
00:35:05,160 --> 00:35:06,109
They had another accident.

1098
00:35:06,110 --> 00:35:07,440
Now they have another accident.

1099
00:35:07,440 --> 00:35:08,640
So they keep having the same accident

1100
00:35:08,640 --> 00:35:10,310
even though they change
all of the technology

1101
00:35:10,310 --> 00:35:12,070
which brings it back to

1102
00:35:12,070 --> 00:35:13,880
it's not really the algorithm.

1103
00:35:13,880 --> 00:35:15,480
It's not really the people
writing the algorithm.

1104
00:35:15,480 --> 00:35:16,930
It's the world that the people come from

1105
00:35:16,930 --> 00:35:17,799
who are writing the algorithm

1106
00:35:17,800 --> 00:35:19,030
and what assumptions are they making

1107
00:35:19,030 --> 00:35:21,810
and what safety are they
putting into the system, right.

1108
00:35:21,810 --> 00:35:24,220
So they're actually
trending worse over time.

1109
00:35:24,220 --> 00:35:25,589
And it's not just me saying this.

1110
00:35:25,590 --> 00:35:27,130
I'm not trying to be provocative here.

1111
00:35:27,130 --> 00:35:28,730
When you look at the
Nvidia folks themselves

1112
00:35:28,730 --> 00:35:30,030
who put the technology in the car,

1113
00:35:30,030 --> 00:35:33,260
when they talk about the leaders
in transit safety industry

1114
00:35:33,260 --> 00:35:34,790
they don't put Tesla at
the top of the chart.

1115
00:35:34,790 --> 00:35:35,900
They're way down at the bottom

1116
00:35:35,900 --> 00:35:37,680
and everybody else is
at the top of the chart.

1117
00:35:37,680 --> 00:35:38,529
Tesla's the lagger.

1118
00:35:38,530 --> 00:35:40,670
They're way behind everybody
in terms of safety.

1119
00:35:40,670 --> 00:35:42,530
So that's what we see in the AI systems,

1120
00:35:42,530 --> 00:35:44,610
the tests that are being
done in the real world

1121
00:35:44,610 --> 00:35:47,280
that could cause accidents
and can kill people,

1122
00:35:47,280 --> 00:35:48,760
they can't see the double yellow lines,

1123
00:35:48,760 --> 00:35:50,250
they're driving on the
wrong side of the road,

1124
00:35:50,250 --> 00:35:51,870
they're having near collisions,

1125
00:35:51,870 --> 00:35:53,839
they are blind in basic tests.

1126
00:35:53,840 --> 00:35:54,750
People go out on the road

1127
00:35:54,750 --> 00:35:57,130
to see if this amazing stuff
is working and it's not.

1128
00:35:57,130 --> 00:35:58,963
The AI is completely failing.

1129
00:36:00,010 --> 00:36:00,930
What's seen from the air

1130
00:36:00,930 --> 00:36:02,919
and what's seen from the
car is completely different

1131
00:36:02,920 --> 00:36:04,940
in terms of cars that
are gonna run into you.

1132
00:36:04,940 --> 00:36:06,030
In this case in particular

1133
00:36:06,030 --> 00:36:08,740
there was a car that the
AI tried to launch into.

1134
00:36:08,740 --> 00:36:12,430
It would have killed or damaged
the people a couple times.

1135
00:36:12,430 --> 00:36:15,440
And so it's easily
documented how flawed it is.

1136
00:36:15,440 --> 00:36:16,790
Really shouldn't even be
on the road at this point

1137
00:36:16,790 --> 00:36:18,340
when you think about what this is doing.

1138
00:36:18,340 --> 00:36:20,980
And so here's an example
of what happens next is

1139
00:36:20,980 --> 00:36:23,350
Tesla plows into people and kills people.

1140
00:36:23,350 --> 00:36:24,690
So it was a really sad outcome

1141
00:36:24,690 --> 00:36:26,070
for basically what we're doing.

1142
00:36:26,070 --> 00:36:29,290
When you talk about this
kind of trust being put

1143
00:36:29,290 --> 00:36:30,890
into technology and all the breaches

1144
00:36:30,890 --> 00:36:32,210
that we've talked about so far,

1145
00:36:32,210 --> 00:36:34,990
it's gonna set us back if
we don't do proper testing

1146
00:36:34,990 --> 00:36:36,720
and basically put this into a mode

1147
00:36:36,720 --> 00:36:39,009
where you can respond rationally

1148
00:36:39,010 --> 00:36:40,610
when people have a flaw or a breach

1149
00:36:40,610 --> 00:36:41,490
and say this is what happened

1150
00:36:41,490 --> 00:36:42,490
and this is what we're doing to fix it.

1151
00:36:42,490 --> 00:36:44,310
But in fact, what you find
with Tesla is the opposite.

1152
00:36:44,310 --> 00:36:46,670
They've had a major
breach, major catastrophe,

1153
00:36:46,670 --> 00:36:49,210
and the response is incomprehensible.

1154
00:36:49,210 --> 00:36:50,400
The car itself was being designed

1155
00:36:50,400 --> 00:36:51,410
to drive without a human in it.

1156
00:36:51,410 --> 00:36:52,640
There's nobody in the car.

1157
00:36:52,640 --> 00:36:53,950
And so it's called summoning the car.

1158
00:36:53,950 --> 00:36:55,180
And when it starts to get summoned

1159
00:36:55,180 --> 00:36:56,879
it tries to run over somebody,

1160
00:36:56,880 --> 00:36:58,900
and Tesla's response is well,
the person in the car could

1161
00:36:58,900 --> 00:37:01,250
obviously take back control.

1162
00:37:01,250 --> 00:37:02,150
That doesn't work.

1163
00:37:03,400 --> 00:37:06,270
And so what you see instead is claims made

1164
00:37:06,270 --> 00:37:08,070
that it's the safest record of any car

1165
00:37:08,070 --> 00:37:10,230
based on no records at all.

1166
00:37:10,230 --> 00:37:12,100
You see claims made against the NHTSA

1167
00:37:13,000 --> 00:37:15,490
which don't seem to map
to anything in reality.

1168
00:37:15,490 --> 00:37:19,439
In the last three quarters,
you see Tesla AI having

1169
00:37:19,440 --> 00:37:20,580
massive problems in the news.

1170
00:37:20,580 --> 00:37:22,060
You may have heard about people dying,

1171
00:37:22,060 --> 00:37:24,750
testing the car and ended
up in a fire and dead.

1172
00:37:24,750 --> 00:37:26,410
And you also see that there's a decline,

1173
00:37:26,410 --> 00:37:29,549
rapid decline in the ability
of the car to actually be safe.

1174
00:37:29,550 --> 00:37:30,667
So other cars, we're not seeing that.

1175
00:37:30,667 --> 00:37:32,350
And we're actually
seeing other brands have

1176
00:37:32,350 --> 00:37:36,480
much more safety, more perfect
safety records than ever

1177
00:37:36,480 --> 00:37:38,740
as Tesla seems to be
getting worse and worse

1178
00:37:38,740 --> 00:37:39,870
and all of this overconfidence.

1179
00:37:39,870 --> 00:37:42,020
And so what does that really
mean, to sort of wrap it up?

1180
00:37:42,020 --> 00:37:42,910
You've got these breaches.

1181
00:37:42,910 --> 00:37:43,770
You've got all these problems.

1182
00:37:43,770 --> 00:37:44,840
You've got all these risks.

1183
00:37:44,840 --> 00:37:46,470
Again, I would say to apply this

1184
00:37:46,470 --> 00:37:47,669
you need to go back to the basics

1185
00:37:47,670 --> 00:37:49,060
which is do you have an off button?

1186
00:37:49,060 --> 00:37:50,060
Do you have a reset button?

1187
00:37:50,060 --> 00:37:51,630
You don't just tell me that
there's a person in the car

1188
00:37:51,630 --> 00:37:52,680
when there's no person in the car.

1189
00:37:52,680 --> 00:37:55,470
That's not an acceptable
off button, a reset button.

1190
00:37:55,470 --> 00:37:57,060
Did it try to run somebody over?

1191
00:37:57,060 --> 00:37:58,710
Well then you need to
go back to the basics

1192
00:37:58,710 --> 00:38:00,250
and say don't try to run people over.

1193
00:38:00,250 --> 00:38:02,930
Learn differently or reset
it to go back to before

1194
00:38:02,930 --> 00:38:04,060
where you thought that was acceptable

1195
00:38:04,060 --> 00:38:05,810
because that's obviously a wrong outcome.

1196
00:38:05,810 --> 00:38:08,759
And the strategic is back
to are you threat modeling

1197
00:38:08,760 --> 00:38:10,130
so you don't end up in that situation?

1198
00:38:10,130 --> 00:38:13,710
Don't be designing AI
for a treeless moonscape

1199
00:38:13,710 --> 00:38:15,480
where there's perfect light from the sun

1200
00:38:15,480 --> 00:38:17,290
when you then put it into a forest

1201
00:38:17,290 --> 00:38:19,430
and expect the car to navigate

1202
00:38:19,430 --> 00:38:20,629
instead of running into a tree.

1203
00:38:20,630 --> 00:38:22,800
That's just not the way
threat modeling works for AI.

1204
00:38:22,800 --> 00:38:24,410
And so set up an ethics review board

1205
00:38:24,410 --> 00:38:26,810
to make sure these are
things are being considered.

1206
00:38:26,810 --> 00:38:28,180
You're thinking through the process

1207
00:38:28,180 --> 00:38:29,190
so that you can gate things

1208
00:38:29,190 --> 00:38:31,280
before they get released to production.

1209
00:38:31,280 --> 00:38:32,910
People should not be human guinea pigs.

1210
00:38:32,910 --> 00:38:33,890
People should not be harmed.

1211
00:38:33,890 --> 00:38:35,609
They shouldn't be told they're
a she when they're a he.

1212
00:38:35,610 --> 00:38:37,630
They shouldn't be told to kill themselves

1213
00:38:37,630 --> 00:38:40,500
when they have a particular
sexual preference.

1214
00:38:40,500 --> 00:38:41,360
They shouldn't be bullied.

1215
00:38:41,360 --> 00:38:42,500
They shouldn't be harmed, right.

1216
00:38:42,500 --> 00:38:44,650
All these things come from understanding

1217
00:38:44,650 --> 00:38:47,090
the social context under
which AI is being developed

1218
00:38:47,090 --> 00:38:50,690
and how people who are
developing it carry with them

1219
00:38:50,690 --> 00:38:53,490
a sense of power, a
sense of right and wrong,

1220
00:38:53,490 --> 00:38:55,580
a sense of up and down.

1221
00:38:55,580 --> 00:38:58,330
And so ultimately, again, assign
executive level enforcement

1222
00:38:58,330 --> 00:38:59,163
so that you have

1223
00:38:59,163 --> 00:39:01,210
some independent way of
holding people responsible.

1224
00:39:01,210 --> 00:39:02,560
Consequences is key.

1225
00:39:02,560 --> 00:39:05,150
And so what you find in companies
like Facebook and Amazon,

1226
00:39:05,150 --> 00:39:05,983
they don't seem to have

1227
00:39:05,983 --> 00:39:07,680
any sense of consequence
or responsibility.

1228
00:39:07,680 --> 00:39:09,029
And so they continuously harm people

1229
00:39:09,030 --> 00:39:09,970
using these technologies

1230
00:39:09,970 --> 00:39:11,649
and that's why things are getting worse.

1231
00:39:11,650 --> 00:39:14,523
So back to the 0.3, 0.7, or 1.7.

1232
00:39:15,600 --> 00:39:16,520
We definitely want to
be collaborating more.

1233
00:39:16,520 --> 00:39:19,550
We definitely want to be
using the 1.7, the Einsteins.

1234
00:39:19,550 --> 00:39:20,900
It may take us 500 years,

1235
00:39:20,900 --> 00:39:22,280
but I think we can get there faster

1236
00:39:22,280 --> 00:39:23,810
if we stop trying to be so competitive

1237
00:39:23,810 --> 00:39:26,560
and try to get rid of the situation

1238
00:39:26,560 --> 00:39:28,503
where people are bringing in the Tatra 87

1239
00:39:28,503 --> 00:39:30,300
just trying to drive as fast as possible

1240
00:39:30,300 --> 00:39:31,680
to prove that you can kill yourself.

1241
00:39:31,680 --> 00:39:34,069
It's not a good way to
handle the new technology.

1242
00:39:34,070 --> 00:39:36,290
Let's try to work better
at making things safer,

1243
00:39:36,290 --> 00:39:37,123
and thank you.

