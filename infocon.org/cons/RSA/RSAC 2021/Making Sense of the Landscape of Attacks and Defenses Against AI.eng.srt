1
00:00:01,360 --> 00:00:04,190
- Hello everybody, I am Paul Rowe,

2
00:00:04,190 --> 00:00:06,210
I am a Principal Cybersecurity Engineer

3
00:00:06,210 --> 00:00:07,670
at the MITRE Corporation,

4
00:00:07,670 --> 00:00:09,210
and I'll be giving a presentation today

5
00:00:09,210 --> 00:00:10,817
with my colleague Elham Tabassi,

6
00:00:10,817 --> 00:00:11,860
Elham, would you like
to introduce yourself?

7
00:00:11,860 --> 00:00:14,480
- Elham Tabassi, hello everyone.

8
00:00:14,480 --> 00:00:17,320
I'm Chief of Staff at
Information Technology Laboratory

9
00:00:17,320 --> 00:00:20,403
and leading the NIST AI
program, glad to be here.

10
00:00:21,810 --> 00:00:23,720
- And today we're gonna be talking about,

11
00:00:23,720 --> 00:00:26,000
Making Sense of the Landscape of Attacks

12
00:00:26,000 --> 00:00:27,512
and Defenses Against AI.

13
00:00:28,860 --> 00:00:30,960
Here's a brief agenda of what we'll cover.

14
00:00:31,840 --> 00:00:35,070
My colleague, Elham will talk about NIST

15
00:00:35,070 --> 00:00:37,310
and their focus on trustworthy AI.

16
00:00:37,310 --> 00:00:40,210
And I will then talk
about sort of an overview

17
00:00:40,210 --> 00:00:42,600
of attacks and defenses that exist.

18
00:00:42,600 --> 00:00:47,600
I'll present the NCCoE's
Securing AI Testbed architecture.

19
00:00:48,610 --> 00:00:49,930
And I'll talk a little bit about

20
00:00:49,930 --> 00:00:51,720
evaluating algorithms and context,

21
00:00:51,720 --> 00:00:54,723
and we'll talk about some
future opportunities.

22
00:00:56,540 --> 00:00:59,839
So with that, I'll hand it over to Elham.

23
00:00:59,840 --> 00:01:01,010
- Thank you, Paul.

24
00:01:01,010 --> 00:01:04,920
So we're talking about Attacks
and Defenses Against AI

25
00:01:04,920 --> 00:01:07,400
but I wanna keep a sort
of a broader context

26
00:01:07,400 --> 00:01:10,970
on how this work fits-in within our work.

27
00:01:10,970 --> 00:01:13,789
We all know that AI is rapidly
transforming our world,

28
00:01:13,790 --> 00:01:17,630
still the development and use
of new AI based technologies

29
00:01:17,630 --> 00:01:21,220
are not without technical
challenges and societal risks.

30
00:01:21,220 --> 00:01:23,760
Optimistic, we always that
AI will change our lives

31
00:01:23,760 --> 00:01:25,910
for better in ways that we don't know yet,

32
00:01:25,910 --> 00:01:29,179
the pessimistic view is that
AI can be a threat to humanity.

33
00:01:29,180 --> 00:01:32,760
And we all know that the truth
lies somewhere in between.

34
00:01:32,760 --> 00:01:35,120
While AI offers many positive benefits,

35
00:01:35,120 --> 00:01:38,310
it can also lead to significant
unintended consequences

36
00:01:38,310 --> 00:01:41,970
for individuals,
organizations, or societies.

37
00:01:41,970 --> 00:01:44,880
So what we are trying to do
is try to understand AI risks

38
00:01:44,880 --> 00:01:45,713
and their driver.

39
00:01:45,713 --> 00:01:48,380
And one challenge is that
despite the consensus

40
00:01:48,380 --> 00:01:51,420
that there are several types
of risks associated with AI,

41
00:01:51,420 --> 00:01:53,430
there has yet to be a concerted effort

42
00:01:53,430 --> 00:01:56,600
around establishing the
taxonomy of AI risks.

43
00:01:56,600 --> 00:01:59,390
The taxonomy that describes
the various type of risks,

44
00:01:59,390 --> 00:02:00,670
and how they can be managed,

45
00:02:00,670 --> 00:02:03,330
is necessary for future efforts to test AI

46
00:02:03,330 --> 00:02:06,610
and eventually build human trust in AI.

47
00:02:06,610 --> 00:02:08,800
The effort that we have in understanding

48
00:02:08,800 --> 00:02:10,520
the vulnerabilities of AI,

49
00:02:10,520 --> 00:02:15,520
and evaluations for securing
AI fits within that effort.

50
00:02:15,790 --> 00:02:17,239
Next slide please.

51
00:02:17,240 --> 00:02:20,150
So NIST has been engaging
a public and private sector

52
00:02:20,150 --> 00:02:22,400
in discussions about building
blocks for trustworthy

53
00:02:22,400 --> 00:02:25,210
AI systems and associated
measurement methods,

54
00:02:25,210 --> 00:02:28,360
tools standards to implement those.

55
00:02:28,360 --> 00:02:30,470
Our efforts at NIST has two main trust.

56
00:02:30,470 --> 00:02:33,650
One of them is Foundational
Research in AI,

57
00:02:33,650 --> 00:02:36,100
to understand and measure
an enhanced trustworthy

58
00:02:37,270 --> 00:02:38,882
and responsible AI.

59
00:02:38,882 --> 00:02:43,882
But our effort also includes,
Use-inspired Research,

60
00:02:44,700 --> 00:02:47,010
applying AI to measurement problems,

61
00:02:47,010 --> 00:02:49,290
being tackled in a NIST Laboratories

62
00:02:49,290 --> 00:02:51,239
to advance the specific research,

63
00:02:51,240 --> 00:02:53,270
as well as to gain deeper understanding

64
00:02:53,270 --> 00:02:56,435
and insight about AI
capabilities and limitations.

65
00:02:56,435 --> 00:03:00,060
We also participate in
development of standards

66
00:03:00,060 --> 00:03:02,350
and guidelines to further AI innovation

67
00:03:02,350 --> 00:03:05,523
and trust in systems
that use AI technologies.

68
00:03:06,420 --> 00:03:08,250
Next slide please.

69
00:03:08,250 --> 00:03:11,700
So what we're trying to do,
so to advance trustworthy

70
00:03:11,700 --> 00:03:14,130
and responsible AI, the
first thing we need to do

71
00:03:14,130 --> 00:03:16,390
and that's the bottom
layer in this pyramid,

72
00:03:16,390 --> 00:03:19,320
is we need to know the
technical requirements

73
00:03:19,320 --> 00:03:21,970
or building blocks for AI.

74
00:03:21,970 --> 00:03:23,480
This is similar to the taxonomy

75
00:03:23,480 --> 00:03:25,623
of risk that I just talked about.

76
00:03:25,623 --> 00:03:28,290
We welcome global discussions

77
00:03:28,290 --> 00:03:31,400
around trustworthy AI and
principles for trustworthy AI.

78
00:03:31,400 --> 00:03:34,670
Examples are OACD and National Policies

79
00:03:34,670 --> 00:03:37,470
and OMB Regulatory Memo for use of AI.

80
00:03:37,470 --> 00:03:40,270
The challenge and opportunity
for us to the community,

81
00:03:40,270 --> 00:03:42,520
is that these principles are aspirational.

82
00:03:42,520 --> 00:03:46,360
So we need to translate
these aspirational principles

83
00:03:46,360 --> 00:03:48,500
to technical requirements
and characteristics

84
00:03:48,500 --> 00:03:51,223
that were clear
implementable and testable.

85
00:03:52,140 --> 00:03:54,170
In other words we're trying to address

86
00:03:54,170 --> 00:03:57,579
the question of what
constitutes trust in AI

87
00:03:57,580 --> 00:03:59,543
or finding that taxonomy of risk.

88
00:04:00,590 --> 00:04:02,410
The next step, which is the second layer

89
00:04:02,410 --> 00:04:04,549
from the bottom on this pyramid,

90
00:04:04,550 --> 00:04:08,030
is that for each of these
core building blocks,

91
00:04:08,030 --> 00:04:12,900
we need to come up with a
terminology and taxonomy concepts,

92
00:04:12,900 --> 00:04:15,220
we need to have a shared understanding

93
00:04:15,220 --> 00:04:16,730
of what we mean by that.

94
00:04:16,730 --> 00:04:18,660
So the technical requirements

95
00:04:18,660 --> 00:04:21,029
are core building blocks
that we have come up with,

96
00:04:21,029 --> 00:04:23,219
across consultation with
the community so far,

97
00:04:23,220 --> 00:04:27,340
are accuracy, reliability,
safety, security, and resilience

98
00:04:27,340 --> 00:04:29,977
which is a focus of this talk, robustness,

99
00:04:29,977 --> 00:04:34,270
explainability, privacy
and mitigation from bias.

100
00:04:34,270 --> 00:04:36,440
There are also factors
such as fairness and

101
00:04:36,440 --> 00:04:38,230
accountability that should be considered

102
00:04:38,230 --> 00:04:40,253
particularly doing deployment.

103
00:04:42,210 --> 00:04:44,380
So I talked about that for each of these,

104
00:04:44,380 --> 00:04:48,140
we need to have a good, clear
and common set of vocabulary

105
00:04:48,140 --> 00:04:49,340
to express each of them,

106
00:04:49,340 --> 00:04:52,900
and this is something
that's right now missing,

107
00:04:52,900 --> 00:04:57,900
and this layer, that's part
of work address the question

108
00:04:58,330 --> 00:05:00,669
of what to measure.

109
00:05:00,670 --> 00:05:03,040
Once we know what it is
that we want to measure,

110
00:05:03,040 --> 00:05:05,530
we can work towards
development of the metrics,

111
00:05:05,530 --> 00:05:08,450
testbed for benchmarking
and performance evaluations

112
00:05:08,450 --> 00:05:11,180
of each of these aspects
of the trustworthy AI,

113
00:05:11,180 --> 00:05:13,800
that's addressing the
question of how to measure,

114
00:05:13,800 --> 00:05:15,512
and we talk about measurements

115
00:05:15,512 --> 00:05:19,440
and evaluations for
vulnerable of AI today.

116
00:05:19,440 --> 00:05:22,410
Developing such a solid
scientific foundations

117
00:05:22,410 --> 00:05:24,260
about what to measure, and how to measure,

118
00:05:24,260 --> 00:05:25,840
is essential for development

119
00:05:25,840 --> 00:05:29,169
of clear technically sound
vulnerability standards.

120
00:05:29,170 --> 00:05:31,260
And once we have all of these pieces,

121
00:05:31,260 --> 00:05:33,690
the scientific foundation and
technically sound standard

122
00:05:33,690 --> 00:05:36,940
of what constitute
trustworthy, responsible AI

123
00:05:36,940 --> 00:05:39,280
or risk to AI, how to measure them,

124
00:05:39,280 --> 00:05:42,527
what data to use for tests,
benchmark performance

125
00:05:42,527 --> 00:05:45,800
and compromise assessment,
standard, et cetera in place,

126
00:05:45,800 --> 00:05:48,920
then we can work towards
risk management approaches,

127
00:05:48,920 --> 00:05:51,980
where developers or users
for each application decide

128
00:05:51,980 --> 00:05:54,020
the appropriate level
for each of the aspects

129
00:05:54,020 --> 00:05:58,332
of trustworthiness and managing
interactions among them.

130
00:06:00,190 --> 00:06:03,030
The point here is that there
is no one size fits all.

131
00:06:03,030 --> 00:06:07,010
And again we're doing the work,

132
00:06:07,010 --> 00:06:09,120
on the different aspects
of the trustworthiness,

133
00:06:09,120 --> 00:06:12,183
and today we are presenting our
work for the security of AI.

134
00:06:13,610 --> 00:06:14,960
The next layer of our desk

135
00:06:14,960 --> 00:06:18,120
it gets to the governance and
evidence-based policy making,

136
00:06:18,120 --> 00:06:21,360
and the emphasis here is that

137
00:06:21,360 --> 00:06:25,260
we need to build a sound
scientific foundation,

138
00:06:25,260 --> 00:06:27,099
to get to this higher level.

139
00:06:27,100 --> 00:06:29,000
Next slide please.

140
00:06:29,000 --> 00:06:31,140
So I keep talking about
the core building blocks

141
00:06:31,140 --> 00:06:34,310
or technical requirements
or characteristics,

142
00:06:34,310 --> 00:06:35,143
and I mentioned them,

143
00:06:35,143 --> 00:06:36,869
but I just wanted to put them in slides

144
00:06:36,870 --> 00:06:38,360
where you guys look at it,

145
00:06:38,360 --> 00:06:42,270
that what can or should be
measured to give credibility

146
00:06:42,270 --> 00:06:44,270
and confidence to machine
learning algorithms?

147
00:06:44,270 --> 00:06:46,260
So this is what we have come up with,

148
00:06:46,260 --> 00:06:51,260
in consultation with the
community that of course accuracy

149
00:06:51,560 --> 00:06:53,800
we want the systems to be accurate,

150
00:06:53,800 --> 00:06:56,850
but there are other
important aspects such as,

151
00:06:56,850 --> 00:06:59,210
as you can see here,
reliability, robustness

152
00:06:59,210 --> 00:07:02,359
security, safety, privacy, and bias free.

153
00:07:02,360 --> 00:07:04,210
And then we have this question mark here,

154
00:07:04,210 --> 00:07:06,520
because we are not sure that
we have covered everything,

155
00:07:06,520 --> 00:07:08,472
so this is an ongoing discussions.

156
00:07:11,070 --> 00:07:13,310
Next slide please.

157
00:07:13,310 --> 00:07:18,160
So while we have built research
programs for bias in AI,

158
00:07:18,160 --> 00:07:21,270
explainable AI, but also security of AI,

159
00:07:21,270 --> 00:07:25,740
so we have a, NISTIR
published about a year ago,

160
00:07:25,740 --> 00:07:27,180
on taxonomy and terminology

161
00:07:27,180 --> 00:07:30,670
for adversarial machine learning,

162
00:07:30,670 --> 00:07:34,630
this is sort of the classification
that we had in the paper

163
00:07:34,630 --> 00:07:38,390
and then we received some good input

164
00:07:38,390 --> 00:07:39,740
and feedback from the community.

165
00:07:39,740 --> 00:07:41,570
And the continuation of that work,

166
00:07:41,570 --> 00:07:43,890
is the work that Paul is gonna talk about,

167
00:07:43,890 --> 00:07:46,150
on sort of building a testbed

168
00:07:46,150 --> 00:07:49,986
for understanding and measuring
the vulnerability of AI,

169
00:07:49,987 --> 00:07:52,990
the attacks and defenses
against AI systems.

170
00:07:52,990 --> 00:07:54,790
Paul, back to you.

171
00:07:54,790 --> 00:07:56,580
- Thank you Elham.

172
00:07:56,580 --> 00:08:01,580
So I'm gonna start with a
brief overview of the space of

173
00:08:01,600 --> 00:08:04,140
adversarial machine learning
and give you a sense of what

174
00:08:04,140 --> 00:08:05,729
attacks and defenses are out there

175
00:08:05,730 --> 00:08:07,973
and sort of the scope of the problem.

176
00:08:09,300 --> 00:08:13,150
So in that NISTIR that Elham mentioned,

177
00:08:13,150 --> 00:08:15,370
there's a high level classification

178
00:08:15,370 --> 00:08:18,280
of attack types into three categories.

179
00:08:18,280 --> 00:08:20,719
This may not be the only
way to classify things,

180
00:08:20,720 --> 00:08:22,450
but it's a very useful one.

181
00:08:22,450 --> 00:08:23,640
Those three categories are,

182
00:08:23,640 --> 00:08:26,460
Poisoning, Evasion and Oracle.

183
00:08:26,460 --> 00:08:29,260
Now poisoning attacks
are those that tamper

184
00:08:29,260 --> 00:08:31,130
with the training data itself.

185
00:08:31,130 --> 00:08:34,470
And so that can affect
what the model learns.

186
00:08:34,470 --> 00:08:37,710
So it can it can cause a model to sort of

187
00:08:37,710 --> 00:08:41,333
learn wrong associations among the data.

188
00:08:42,650 --> 00:08:44,757
Evasion attacks happened
at a different part

189
00:08:44,757 --> 00:08:46,170
of the life cycle.

190
00:08:46,170 --> 00:08:50,360
They typically add noise, or
they alter an image coming in,

191
00:08:50,360 --> 00:08:51,623
at inference time,

192
00:08:53,346 --> 00:08:55,220
and those alterations
are done in such a way

193
00:08:55,220 --> 00:08:58,253
as to prompting a
misclassifications of the data.

194
00:08:59,340 --> 00:09:03,980
Typically, those are alterations are,

195
00:09:03,980 --> 00:09:06,240
they're often done in a
way that is meant to be

196
00:09:06,240 --> 00:09:07,900
imperceptible to humans,

197
00:09:07,900 --> 00:09:10,453
but not always as we'll see shortly.

198
00:09:11,330 --> 00:09:13,050
Finally Oracle attacks are typically

199
00:09:13,050 --> 00:09:15,439
sort of black box queries,
against the model,

200
00:09:15,440 --> 00:09:17,990
with the goal of perhaps
replicating its behavior,

201
00:09:17,990 --> 00:09:20,590
to sort of learn what the
associations it knows,

202
00:09:20,590 --> 00:09:23,040
or perhaps learning about information

203
00:09:23,040 --> 00:09:24,569
about the training data behind it.

204
00:09:24,570 --> 00:09:27,693
So this would be some
sort of privacy leakage.

205
00:09:29,440 --> 00:09:31,840
So the Testbed that we are building,

206
00:09:31,840 --> 00:09:34,960
for the moment we focused
on image classification,

207
00:09:34,960 --> 00:09:38,616
because that's where a lot of
the information in the open,

208
00:09:38,616 --> 00:09:40,540
in the academic literature

209
00:09:40,540 --> 00:09:44,540
and open source
implementations are available.

210
00:09:44,540 --> 00:09:46,719
So here's just a sample of
what some of these attacks

211
00:09:46,720 --> 00:09:48,423
look like in that domain.

212
00:09:49,330 --> 00:09:54,330
So on the top left, we have an
example of an evasion attack

213
00:09:54,610 --> 00:09:59,310
where the adversary creates
the specially designed

214
00:09:59,310 --> 00:10:02,536
sort of colorful psychedelic patch

215
00:10:02,536 --> 00:10:04,689
that they attach to an image.

216
00:10:04,690 --> 00:10:08,930
And that's carefully
designed, to cause a model

217
00:10:08,930 --> 00:10:12,130
to always classify
images with such a patch

218
00:10:12,130 --> 00:10:14,160
as in this case, a Toaster.

219
00:10:14,160 --> 00:10:16,079
So to the human it's clearly,

220
00:10:16,080 --> 00:10:19,130
a picture of a dog with a
weird look and circle on it,

221
00:10:19,130 --> 00:10:24,130
but the model is completely
fooled by this kind of change

222
00:10:24,730 --> 00:10:26,090
to the image.

223
00:10:26,090 --> 00:10:27,630
Perhaps more subtly on the right,

224
00:10:27,630 --> 00:10:30,026
we have a noisy evasion attack,

225
00:10:30,026 --> 00:10:33,380
where instead of concentrating
all of the changes

226
00:10:33,380 --> 00:10:34,663
in one part of the image,

227
00:10:38,567 --> 00:10:39,400
they're much more spread
out and much smaller

228
00:10:41,660 --> 00:10:42,492
and typically less perceptible to humans.

229
00:10:42,493 --> 00:10:43,820
So in this case the sports
car gets misclassified

230
00:10:43,820 --> 00:10:44,940
as a purse.

231
00:10:44,940 --> 00:10:49,940
And depending on the quality
of this video on your screen,

232
00:10:50,070 --> 00:10:52,120
you may, or may not be able
to perceive that difference,

233
00:10:52,120 --> 00:10:54,330
so you might just sort of see
it as a little bit fuzzier,

234
00:10:54,330 --> 00:10:56,853
but again, the model is totally confused.

235
00:10:58,830 --> 00:11:01,140
A different class of attack,
is this poisoning attack.

236
00:11:01,140 --> 00:11:05,670
In this case, we have an
original image of an apple,

237
00:11:05,670 --> 00:11:07,540
and then we have this
target image of a currant

238
00:11:07,540 --> 00:11:10,400
and then the training data gets augmented

239
00:11:10,400 --> 00:11:12,180
with this blended image,

240
00:11:12,180 --> 00:11:15,670
which contains features of
both the currant and the apple.

241
00:11:15,670 --> 00:11:17,479
And now this image to
us looks like an apple,

242
00:11:17,480 --> 00:11:19,440
so we would typically label it as such,

243
00:11:19,440 --> 00:11:21,990
but this may have downstream consequences

244
00:11:21,990 --> 00:11:24,750
of the model that is trained,

245
00:11:24,750 --> 00:11:27,315
essentially classifying
currants as apples,

246
00:11:27,315 --> 00:11:30,480
because it's learned to
associate features of a currant

247
00:11:30,480 --> 00:11:31,383
with an apple.

248
00:11:32,690 --> 00:11:36,040
So this is by far not a
comprehensive sampling,

249
00:11:36,040 --> 00:11:37,089
but I wanted to just give you

250
00:11:37,090 --> 00:11:39,980
a sense of what these look like.

251
00:11:39,980 --> 00:11:42,560
On the defensive side, again,

252
00:11:42,560 --> 00:11:46,439
there's an equal amount of sort of variety

253
00:11:46,440 --> 00:11:48,830
and defenses as there are two attacks.

254
00:11:48,830 --> 00:11:51,020
Some sample examples are,

255
00:11:51,020 --> 00:11:54,470
data sanitization to help protect
against poisoning attacks,

256
00:11:54,470 --> 00:11:58,390
that attempt to either detect
train poison training samples

257
00:11:58,390 --> 00:12:00,740
and remove them before
a training, for example.

258
00:12:02,210 --> 00:12:05,360
There are techniques for robust training,

259
00:12:05,360 --> 00:12:09,170
so one can add these
adversarially evasion examples

260
00:12:09,170 --> 00:12:10,920
to the training model, or excuse me,

261
00:12:10,920 --> 00:12:12,729
to the to the training data,

262
00:12:12,730 --> 00:12:16,500
so that the model sort
of learns to ignore them,

263
00:12:16,500 --> 00:12:20,773
and so there's sort of a more
robust model that comes out.

264
00:12:21,650 --> 00:12:23,360
There's other kinds of defenses

265
00:12:23,360 --> 00:12:24,900
that are maybe image pre-processing.

266
00:12:24,900 --> 00:12:29,079
So at inference time, there'll
will be a pre-processing step

267
00:12:29,080 --> 00:12:32,740
to hopefully squeeze out any
adversarial manipulations

268
00:12:32,740 --> 00:12:33,573
to the images,

269
00:12:34,580 --> 00:12:38,160
and so that is sometimes
effective and sometimes

270
00:12:38,160 --> 00:12:40,193
not as effective as we'll see.

271
00:12:41,470 --> 00:12:43,411
So what this looks like here on the left

272
00:12:43,411 --> 00:12:47,569
is an example of an image
pre-processing defense.

273
00:12:47,570 --> 00:12:50,040
This is a feature squeezing defense where

274
00:12:50,040 --> 00:12:53,410
it squeezes information out of channels,

275
00:12:53,410 --> 00:12:56,560
so that the original picture
has some number of bits

276
00:12:56,560 --> 00:12:58,729
of information in three
different color channels,

277
00:12:58,730 --> 00:13:00,530
and it squeezes it down to one bit

278
00:13:00,530 --> 00:13:02,140
in each of those channels.

279
00:13:02,140 --> 00:13:05,220
As you can see, this is
likely to sort of wash out

280
00:13:05,220 --> 00:13:09,240
any adversarial perturbations
that might exist in the image,

281
00:13:09,240 --> 00:13:12,580
but it has the the sort
of unfortunate consequence

282
00:13:12,580 --> 00:13:14,300
that it might make it harder to perform

283
00:13:14,300 --> 00:13:16,900
correct classifications on
unaltered images as well.

284
00:13:18,290 --> 00:13:21,410
The right is an example of what
data sanitization might do.

285
00:13:21,410 --> 00:13:26,170
So here is a poisoning attack,
where if you notice on the

286
00:13:26,170 --> 00:13:28,410
on the upper nine, there
are these four little pixels

287
00:13:28,410 --> 00:13:29,760
in the bottom right corner.

288
00:13:30,820 --> 00:13:35,820
This attack forces the model
to learn to associate images

289
00:13:36,050 --> 00:13:39,620
with those four pixels with
let's say the label too,

290
00:13:39,620 --> 00:13:42,030
so it always learns out
whenever it see those pixels

291
00:13:42,030 --> 00:13:43,560
that's a sure sign that it's a two

292
00:13:43,560 --> 00:13:45,739
which of course is not the case here.

293
00:13:45,740 --> 00:13:49,890
And so data sanitization
techniques attempt to identify

294
00:13:49,890 --> 00:13:51,560
these sort of poisoned examples

295
00:13:51,560 --> 00:13:53,560
and either remove the
feature that is the poison,

296
00:13:53,560 --> 00:13:56,040
or perhaps just remove the entire image,

297
00:13:56,040 --> 00:13:58,833
from the training set.

298
00:14:01,770 --> 00:14:04,900
So while this is far from
comprehensive overview,

299
00:14:04,900 --> 00:14:08,360
hopefully this gives you a
sense of really the complexity

300
00:14:08,360 --> 00:14:10,690
and the scope of the problem
that we're dealing with.

301
00:14:10,690 --> 00:14:13,180
So not only is it natural to ask,

302
00:14:13,180 --> 00:14:15,949
okay, is this attack good
against this defense?

303
00:14:15,950 --> 00:14:20,000
But with a number of
combinations possible,

304
00:14:20,000 --> 00:14:22,150
the number of combinations just,

305
00:14:22,150 --> 00:14:24,747
the combinatorial
explosion makes this a very

306
00:14:24,748 --> 00:14:27,180
intractable problem.

307
00:14:27,180 --> 00:14:28,050
What makes it worse,

308
00:14:28,050 --> 00:14:30,349
is that some of the
effectiveness that will depend

309
00:14:30,350 --> 00:14:33,140
not just on what attack
or defense it's against

310
00:14:33,140 --> 00:14:35,390
but what dataset it's using,

311
00:14:35,390 --> 00:14:38,848
perhaps the architecture
model that you're looking

312
00:14:38,849 --> 00:14:40,099
at possibly more as well.

313
00:14:40,950 --> 00:14:41,783
So what this means is,

314
00:14:41,783 --> 00:14:44,900
it's really difficult
to generalize results,

315
00:14:44,900 --> 00:14:48,970
when you get results in specific
narrow testing conditions.

316
00:14:48,970 --> 00:14:51,860
It's unclear if those results depend on

317
00:14:51,860 --> 00:14:54,283
aspects of the data set
or aspects of the model.

318
00:14:55,130 --> 00:14:59,120
And so that's an issue that
we're trying to address

319
00:14:59,120 --> 00:15:00,623
with this Testbed.

320
00:15:02,980 --> 00:15:05,720
So the challenge is
really to begin to attain

321
00:15:05,720 --> 00:15:08,230
this wild landscape of
attacks and defenses,

322
00:15:08,230 --> 00:15:11,420
to begin to get a handle
on what we can measure,

323
00:15:11,420 --> 00:15:15,040
regarding the security of
machine learning algorithms

324
00:15:15,040 --> 00:15:17,522
in this adversarial environment.

325
00:15:19,184 --> 00:15:21,069
And so our approach in this setting

326
00:15:21,070 --> 00:15:23,470
is on the sort of use inspired,

327
00:15:23,470 --> 00:15:26,790
its very an empirical approach
in building the Testbed.

328
00:15:26,790 --> 00:15:29,089
We want to help enable,

329
00:15:29,090 --> 00:15:31,800
people to experiment with
attacks and defenses,

330
00:15:31,800 --> 00:15:33,449
we really want to lower the barrier

331
00:15:33,450 --> 00:15:35,460
to entry as much as possible.

332
00:15:35,460 --> 00:15:36,800
So to that end,

333
00:15:36,800 --> 00:15:41,030
we've designed the Testbed
to have a modular design.

334
00:15:41,030 --> 00:15:44,069
The idea is that that will
hopefully enable people to swap

335
00:15:44,070 --> 00:15:47,870
in and out these modular pieces to really

336
00:15:47,870 --> 00:15:50,100
make it easy to alter one experiment,

337
00:15:50,100 --> 00:15:52,900
to create a new sort of related experiment

338
00:15:52,900 --> 00:15:57,220
that is very easy to run and requires

339
00:15:57,220 --> 00:15:59,240
not as much expertise to set up,

340
00:15:59,240 --> 00:16:00,453
something entirely new.

341
00:16:01,490 --> 00:16:02,400
We also have a goal

342
00:16:02,400 --> 00:16:05,040
with this Testbed of
engaging with the community.

343
00:16:05,040 --> 00:16:06,875
We'd like it to be a focal point

344
00:16:06,875 --> 00:16:10,540
for talking with the community at large,

345
00:16:10,540 --> 00:16:14,770
that is you, and we anticipate that

346
00:16:14,770 --> 00:16:19,500
there will be users that come
with a diverse set of needs,

347
00:16:19,500 --> 00:16:22,200
and we're very interested
in hearing about those,

348
00:16:22,200 --> 00:16:24,930
and the modular design
helps us be prepared

349
00:16:24,930 --> 00:16:27,410
to accommodate some of those needs

350
00:16:27,410 --> 00:16:29,060
that we may not have anticipated.

351
00:16:30,950 --> 00:16:33,880
We think this Testbed does provide

352
00:16:33,880 --> 00:16:37,680
some new opportunities,
one for researchers,

353
00:16:37,680 --> 00:16:41,770
we think that it can be
a platform to help enable

354
00:16:41,770 --> 00:16:44,300
more robust evaluation of new proposals

355
00:16:44,300 --> 00:16:45,573
for attacks or defenses.

356
00:16:46,580 --> 00:16:48,960
Perhaps more importantly,
or more interestingly,

357
00:16:48,960 --> 00:16:52,550
it offers the ability to replicate

358
00:16:52,550 --> 00:16:55,740
and repeat results from others.

359
00:16:55,740 --> 00:16:57,950
I think we're, many of us are aware

360
00:16:57,950 --> 00:17:02,570
of issues of repeatability
in other sciences,

361
00:17:02,570 --> 00:17:07,480
and we wanna make sure that
doesn't sort of propagate

362
00:17:07,480 --> 00:17:08,913
into this area as well.

363
00:17:09,810 --> 00:17:11,710
Looking a little bit
further down the road,

364
00:17:11,710 --> 00:17:13,470
we can envision the test testbed

365
00:17:13,470 --> 00:17:17,069
being used by first party
developers of of machine learning

366
00:17:17,069 --> 00:17:19,200
or AI enabled products,

367
00:17:19,200 --> 00:17:22,220
perhaps incorporating this
into a DevSecOps pipeline

368
00:17:23,140 --> 00:17:25,970
to really put their
algorithms through their paces

369
00:17:25,970 --> 00:17:27,283
as part of testing.

370
00:17:29,290 --> 00:17:31,780
We can also think of
second party consumers

371
00:17:31,780 --> 00:17:32,613
of those products,

372
00:17:32,613 --> 00:17:34,480
who may not be aware of the testing

373
00:17:34,480 --> 00:17:37,190
that went into it beforehand,

374
00:17:37,190 --> 00:17:39,170
that would like to really understand,

375
00:17:39,170 --> 00:17:41,450
how these products behave,

376
00:17:41,450 --> 00:17:44,240
under various adversarial
settings, perhaps.

377
00:17:44,240 --> 00:17:46,420
And so this Testbed could provide a way

378
00:17:46,420 --> 00:17:47,920
for such second party testing.

379
00:17:49,113 --> 00:17:51,130
In terms of serving as a focal point

380
00:17:51,130 --> 00:17:53,660
for community engagement,

381
00:17:53,660 --> 00:17:58,310
we are interested in discussing
potential challenge problems

382
00:17:58,310 --> 00:17:59,950
that we might be able
to leverage this Testbed

383
00:17:59,950 --> 00:18:01,963
as an evaluation platform.

384
00:18:03,680 --> 00:18:06,390
So we're very interested in that,

385
00:18:06,390 --> 00:18:09,760
so, if that's something
that appeals to you,

386
00:18:09,760 --> 00:18:10,873
please stay in touch.

387
00:18:12,340 --> 00:18:13,929
And of course, although the focus

388
00:18:13,930 --> 00:18:16,550
for today is really on security,

389
00:18:16,550 --> 00:18:21,550
this does support the broader trust

390
00:18:21,840 --> 00:18:24,120
that Elham was talking
about, of trustworthy ML,

391
00:18:24,120 --> 00:18:28,760
so nothing about this Testbed
is limited to just security.

392
00:18:28,760 --> 00:18:32,090
So we really do hope
that it could be useful

393
00:18:32,090 --> 00:18:35,939
for investigating other
aspects for trustworthy MLs

394
00:18:35,940 --> 00:18:37,543
which is bias and transparency.

395
00:18:39,490 --> 00:18:40,530
Okay, so I'm giving you

396
00:18:40,530 --> 00:18:41,810
a little bit of a scope of the problem,

397
00:18:41,810 --> 00:18:43,679
I'd now I would like to
tell you a little bit about

398
00:18:43,680 --> 00:18:46,030
the Architecture of the Testbed,

399
00:18:46,030 --> 00:18:48,500
and what it is sort of like to use it

400
00:18:48,500 --> 00:18:49,513
and program with it.

401
00:18:50,690 --> 00:18:52,930
So as I said, our guiding principle

402
00:18:52,930 --> 00:18:54,620
has really been modularity.

403
00:18:54,620 --> 00:18:55,870
We really wanna embrace this,

404
00:18:55,870 --> 00:18:59,010
or choose your own adventure attitude,

405
00:18:59,010 --> 00:19:02,730
where here we have what are
essentially notional columns

406
00:19:02,730 --> 00:19:05,810
of I can pick, an Architecture, a Dataset,

407
00:19:05,810 --> 00:19:09,360
an Attack, a Defense, a
Metric, and I can pick,

408
00:19:09,360 --> 00:19:11,629
one or maybe two from
even some of these columns

409
00:19:11,630 --> 00:19:14,500
and put them together in novel ways,

410
00:19:14,500 --> 00:19:17,130
and the Testbed should be set up,

411
00:19:17,130 --> 00:19:20,780
so that all I really need to
do is identify which pieces

412
00:19:20,780 --> 00:19:23,320
I wanna put together, kind of identify

413
00:19:23,320 --> 00:19:24,540
how they should be wired together

414
00:19:24,540 --> 00:19:27,053
and the rest should be
as automatic as possible.

415
00:19:28,590 --> 00:19:30,879
And so this really gives it,

416
00:19:30,880 --> 00:19:34,040
makes it a lot easier to do
a sort of plug and play swap

417
00:19:34,040 --> 00:19:35,800
in and out components.

418
00:19:35,800 --> 00:19:39,810
So let me just give you an
example of one thing that we did

419
00:19:39,810 --> 00:19:40,950
along these lines.

420
00:19:40,950 --> 00:19:43,430
So here a colleague of mine

421
00:19:43,430 --> 00:19:44,830
performed the following experiment.

422
00:19:44,830 --> 00:19:47,433
He was looking at a poisoning attack,

423
00:19:48,620 --> 00:19:50,110
and this poisoning attack required

424
00:19:50,110 --> 00:19:53,520
first to train a proxy model,

425
00:19:53,520 --> 00:19:55,373
and then on that proxy model,

426
00:19:56,520 --> 00:20:01,210
the attack would generate poison sample,

427
00:20:01,210 --> 00:20:04,330
poison images, that
would then be augmented,

428
00:20:04,330 --> 00:20:06,730
that would then open the training data,

429
00:20:06,730 --> 00:20:09,230
in a follow on model training steps.

430
00:20:09,230 --> 00:20:11,200
So in that model, in the second step,

431
00:20:11,200 --> 00:20:13,910
what comes out is a poisoned model.

432
00:20:13,910 --> 00:20:17,350
And so this experiment
then, test the efficacy

433
00:20:17,350 --> 00:20:21,667
of this poisoning attack
on that test data.

434
00:20:25,680 --> 00:20:29,180
Now it should be hopefully
easy to then quickly say, okay

435
00:20:29,180 --> 00:20:31,850
if we have already implemented a defense,

436
00:20:31,850 --> 00:20:33,120
like the feature squeezing one,

437
00:20:33,120 --> 00:20:34,550
I discussed before,

438
00:20:34,550 --> 00:20:36,470
we should be able to add that in

439
00:20:36,470 --> 00:20:39,800
to our experimentation
pipeline and say, okay,

440
00:20:39,800 --> 00:20:41,610
well how do the results differ

441
00:20:41,610 --> 00:20:43,639
if we insert this feature squeezing

442
00:20:43,640 --> 00:20:46,850
into the model inference stage?

443
00:20:46,850 --> 00:20:51,230
And so that should just
be a matter of saying

444
00:20:51,230 --> 00:20:52,800
include this in that stage,

445
00:20:52,800 --> 00:20:56,370
and now run and everything
else happens automatically.

446
00:20:56,370 --> 00:20:58,689
Similarly, we might say, okay, well

447
00:20:58,690 --> 00:21:03,400
what if we replaced that
initial poisoning attack,

448
00:21:03,400 --> 00:21:05,070
with a slightly different poisoning attack

449
00:21:05,070 --> 00:21:07,290
that doesn't have to first
do that model training?

450
00:21:07,290 --> 00:21:08,530
We'd like to be able to just swap

451
00:21:08,530 --> 00:21:11,010
in and out and see if the results differ,

452
00:21:11,010 --> 00:21:16,010
if they're the same, how much
they change and so forth.

453
00:21:17,880 --> 00:21:19,311
I think, the point here

454
00:21:19,311 --> 00:21:21,470
is to start to get your
imagination running

455
00:21:21,470 --> 00:21:25,780
as to how many combinations there could be

456
00:21:25,780 --> 00:21:28,023
and how easy it would
be to swap in and out.

457
00:21:30,060 --> 00:21:31,909
So this is a high level view

458
00:21:31,910 --> 00:21:34,720
of the architecture of the Testbed.

459
00:21:34,720 --> 00:21:36,900
I wanna point out that it's really

460
00:21:36,900 --> 00:21:38,390
a microservices architecture.

461
00:21:38,390 --> 00:21:42,840
So each of those blue boxes,
is a separate docker container

462
00:21:42,840 --> 00:21:44,909
that can live on a single machine,

463
00:21:44,910 --> 00:21:47,380
or it can be deployed across many machines

464
00:21:47,380 --> 00:21:49,130
and like an on premises deployment.

465
00:21:50,660 --> 00:21:55,590
And really the heart of this architecture,

466
00:21:55,590 --> 00:21:57,360
is that RESTful API at the center

467
00:21:57,360 --> 00:21:59,699
that really coordinates the communication

468
00:21:59,700 --> 00:22:01,670
among all the other pieces.

469
00:22:01,670 --> 00:22:04,370
So the way an experiment
gets run is that the user

470
00:22:04,370 --> 00:22:07,560
submits an experiment to the API,

471
00:22:07,560 --> 00:22:10,822
which then sort of records it
in the backend data storage.

472
00:22:11,860 --> 00:22:15,389
And then the user submits
a particular job query,

473
00:22:15,390 --> 00:22:17,740
says, please run this particular job,

474
00:22:17,740 --> 00:22:20,930
one of those like teal boxes
from the previous slide.

475
00:22:20,930 --> 00:22:23,390
And so the API sends
that to a redis queue,

476
00:22:23,390 --> 00:22:24,223
and in the bottom right,

477
00:22:24,223 --> 00:22:26,230
there's some worker containers,

478
00:22:26,230 --> 00:22:28,430
that have dependencies built in

479
00:22:29,383 --> 00:22:32,090
for machine learning frameworks
like Tensorflow and Py Torch

480
00:22:32,090 --> 00:22:34,409
that are constantly
looking for jobs to run

481
00:22:34,410 --> 00:22:35,590
from that redis queue,

482
00:22:35,590 --> 00:22:39,860
so they'll do all of the
more powerful computation

483
00:22:39,860 --> 00:22:41,830
that required for training and so forth.

484
00:22:41,830 --> 00:22:44,429
And then they'll record
the results in the storage,

485
00:22:44,430 --> 00:22:45,610
and they'll also coordinate

486
00:22:45,610 --> 00:22:47,240
with this MlFlow Tracking Service

487
00:22:47,240 --> 00:22:50,880
which is an open source service
for essentially organizing

488
00:22:50,880 --> 00:22:53,783
and keeping track of machine
learning experiments.

489
00:22:54,740 --> 00:22:57,330
And then the user can query the database

490
00:22:57,330 --> 00:22:58,439
through the RESTful API,

491
00:22:58,440 --> 00:23:01,593
or perhaps even through
the MLFlow service itself

492
00:23:01,593 --> 00:23:03,820
because MLFlow provides
a nice dashboard view

493
00:23:03,820 --> 00:23:05,463
so the're several options there.

494
00:23:07,918 --> 00:23:09,870
So that's high level how the architecture

495
00:23:09,870 --> 00:23:13,226
processes these things and as a user,

496
00:23:13,226 --> 00:23:16,420
your job is to define these experiments.

497
00:23:16,420 --> 00:23:20,870
And so the Testbed comes equipped already

498
00:23:20,870 --> 00:23:23,199
with several entry points,

499
00:23:23,200 --> 00:23:25,680
and entry points themselves
are made up of these

500
00:23:25,680 --> 00:23:29,320
smaller functional units that
we're calling task plugins.

501
00:23:29,320 --> 00:23:31,860
So there's really sort of three
levels of modularity here,

502
00:23:31,860 --> 00:23:33,550
the experiment level,

503
00:23:33,550 --> 00:23:36,340
the entry point level, and the task level.

504
00:23:36,340 --> 00:23:41,340
So as we'll see, we think
that this will enable

505
00:23:41,700 --> 00:23:44,060
various levels of users to
interact with the platform.

506
00:23:44,060 --> 00:23:46,960
Someone who can just knows,
just how to run experiments

507
00:23:46,960 --> 00:23:48,275
can do that.

508
00:23:48,275 --> 00:23:51,480
A more advanced user can sort
of define new entry points

509
00:23:51,480 --> 00:23:53,950
and wire them together in new ways.

510
00:23:53,950 --> 00:23:55,800
And a more advanced
user can sort of rewrite

511
00:23:55,800 --> 00:23:59,253
their own task plugins,
and put those together.

512
00:24:02,340 --> 00:24:04,669
So we really envisioned
these four level of users,

513
00:24:04,670 --> 00:24:07,950
the newcomer who hopefully
doesn't really need much

514
00:24:07,950 --> 00:24:08,980
programming experience,

515
00:24:08,980 --> 00:24:11,870
or even a machine learning
experience at all.

516
00:24:11,870 --> 00:24:15,560
They should be able to get
it installed and deployed,

517
00:24:15,560 --> 00:24:17,379
and then they should just
be able to run the demos,

518
00:24:17,380 --> 00:24:19,160
and ultra parameters just to kind of see

519
00:24:19,160 --> 00:24:22,940
how they react and get a
sense for what's possible

520
00:24:22,940 --> 00:24:25,230
with these attacks and defenses,

521
00:24:25,230 --> 00:24:27,260
and sort of how we're
thinking about measuring

522
00:24:27,260 --> 00:24:28,660
some of the aspects of them.

523
00:24:29,630 --> 00:24:33,060
A slightly more advanced
user would perhaps,

524
00:24:33,060 --> 00:24:35,399
wire together entry points in new ways,

525
00:24:35,400 --> 00:24:40,400
or create new entry points
out of existing task plugins.

526
00:24:40,490 --> 00:24:41,740
So this is slightly more advanced,

527
00:24:41,740 --> 00:24:42,840
but still we would hope

528
00:24:42,840 --> 00:24:46,580
that it would be a pretty
formulaic programming.

529
00:24:46,580 --> 00:24:49,090
In fact, we envisioned the ability

530
00:24:49,090 --> 00:24:50,760
to sort of do this through,

531
00:24:50,760 --> 00:24:52,740
more of just like a
graphical user interface,

532
00:24:52,740 --> 00:24:53,863
that seems possible.

533
00:24:55,190 --> 00:24:56,120
Someone like a researcher

534
00:24:56,120 --> 00:24:58,790
who is really developing
new attacks and defenses,

535
00:24:58,790 --> 00:25:01,750
would really start to have to
do some more serious coding

536
00:25:01,750 --> 00:25:03,850
like they're already doing for themselves,

537
00:25:03,850 --> 00:25:07,100
but by integrating with this Testbed,

538
00:25:07,100 --> 00:25:08,750
we provide hooks to make it much easier

539
00:25:08,750 --> 00:25:11,540
to use and reuse those
in new and novel ways

540
00:25:12,790 --> 00:25:16,840
to really broaden the set
of experiments possible.

541
00:25:16,840 --> 00:25:18,300
And of course we envisioned developers

542
00:25:18,300 --> 00:25:21,010
who might be able to
contribute to the backend,

543
00:25:21,010 --> 00:25:22,351
and really just kind of contribute back

544
00:25:22,351 --> 00:25:23,893
to the project itself.

545
00:25:26,690 --> 00:25:29,590
So here's just a very high
level kind of sampling

546
00:25:29,590 --> 00:25:30,850
of what the code might look like

547
00:25:30,850 --> 00:25:34,110
for various levels on the
left, is maybe a level two user

548
00:25:34,110 --> 00:25:36,770
who's doing very formulaic code,

549
00:25:36,770 --> 00:25:39,970
they're really just kind of
defining inputs and outputs

550
00:25:39,970 --> 00:25:42,430
and then, putting things
together that way.

551
00:25:42,430 --> 00:25:43,580
On the right is maybe

552
00:25:44,527 --> 00:25:47,360
looks more like a typical
software engineers code,

553
00:25:47,360 --> 00:25:48,780
and that would be look
more like level three.

554
00:25:48,780 --> 00:25:50,800
So we wanna be able to accommodate,

555
00:25:50,800 --> 00:25:53,530
both the ease of use and flexibility

556
00:25:53,530 --> 00:25:54,873
for more advanced users.

557
00:25:57,750 --> 00:26:01,980
Okay, so that's really a rundown
of the architecture itself.

558
00:26:01,980 --> 00:26:04,180
I wanna give you before leaving the stage,

559
00:26:04,180 --> 00:26:07,180
a little bit of a sense of some
of the things we've learned,

560
00:26:07,180 --> 00:26:11,507
as far as what is important to measure

561
00:26:11,507 --> 00:26:15,210
and what are some, gotchas
and things to keep in mind

562
00:26:15,210 --> 00:26:16,970
when it comes to measurement

563
00:26:16,970 --> 00:26:18,743
in the space of security for AI.

564
00:26:20,040 --> 00:26:21,550
So the first thing to note is that

565
00:26:21,550 --> 00:26:24,870
there is really no shortage
of things to measure.

566
00:26:24,870 --> 00:26:29,870
There's always new ways,
to generate new numbers,

567
00:26:30,571 --> 00:26:32,330
and that's good and bad, right?

568
00:26:32,330 --> 00:26:36,970
So, there's never gonna
be an end all metric

569
00:26:36,970 --> 00:26:39,573
for security or artificial intelligence.

570
00:26:41,580 --> 00:26:44,399
Typically we wanna look at
things like the effectiveness,

571
00:26:44,400 --> 00:26:45,904
the effectiveness of an attack,

572
00:26:45,904 --> 00:26:49,280
how much does an attack
decrease the accuracy of a model

573
00:26:49,280 --> 00:26:50,113
for example?

574
00:26:51,710 --> 00:26:53,980
On the other side we
might look at things like,

575
00:26:53,980 --> 00:26:56,690
the time and resources
required to generate

576
00:26:56,690 --> 00:26:58,360
an adversarial sample.

577
00:26:58,360 --> 00:27:00,870
That's information that
might be incorporated

578
00:27:00,870 --> 00:27:03,669
in understanding, what's
the real risk involved,

579
00:27:03,670 --> 00:27:06,480
what real risk am I
facing for such an attack,

580
00:27:06,480 --> 00:27:07,720
given the deployment scenario?

581
00:27:07,720 --> 00:27:09,970
And I'll say something
more about that later.

582
00:27:11,240 --> 00:27:13,713
But one thing I do wanna point out is,

583
00:27:14,850 --> 00:27:16,413
we've become aware that it's,

584
00:27:18,160 --> 00:27:20,230
you do have to be a little bit careful

585
00:27:20,230 --> 00:27:23,600
about how you interpret the
metrics that do come out,

586
00:27:23,600 --> 00:27:27,550
and what they really mean
for the security of a system.

587
00:27:27,550 --> 00:27:31,649
So there is a really
interesting area of research,

588
00:27:31,650 --> 00:27:33,750
in adversarial machine learning,

589
00:27:33,750 --> 00:27:35,150
that seeks to provide

590
00:27:35,150 --> 00:27:37,920
very strong mathematical guarantees

591
00:27:37,920 --> 00:27:39,510
of robustness for models.

592
00:27:39,510 --> 00:27:41,410
So in this bottom picture on the left,

593
00:27:42,360 --> 00:27:44,550
they can say, look for this model,

594
00:27:44,550 --> 00:27:47,419
if you're given an image at this point

595
00:27:47,420 --> 00:27:49,220
in the classification space,

596
00:27:49,220 --> 00:27:53,640
I can guarantee that, it's
classification won't change

597
00:27:53,640 --> 00:27:57,050
unless you perturbed by
more than someone out.

598
00:27:57,050 --> 00:27:59,210
So that's really good
that those are really good

599
00:27:59,210 --> 00:28:00,410
and interesting results.

600
00:28:01,250 --> 00:28:04,590
A slightly skeptical user
might say, okay, that's good,

601
00:28:04,590 --> 00:28:09,590
but maybe what I really care
about as far as that distance,

602
00:28:09,870 --> 00:28:12,050
is whether or not a human can notice it.

603
00:28:12,050 --> 00:28:15,280
And so, that distance
may or may not correspond

604
00:28:15,280 --> 00:28:16,287
to human perceptibility.

605
00:28:16,287 --> 00:28:19,020
And so there's different ways
to achieve those distances

606
00:28:19,020 --> 00:28:20,867
as you see in the bottom middle.

607
00:28:20,867 --> 00:28:22,870
It can be spread out over the whole image,

608
00:28:22,870 --> 00:28:24,919
it can be concentrated in one area,

609
00:28:24,920 --> 00:28:26,023
these are things that,

610
00:28:26,023 --> 00:28:29,082
are not necessarily
taken into consideration.

611
00:28:30,350 --> 00:28:32,490
Also, perceptibility

612
00:28:32,490 --> 00:28:35,380
or a distance may not
really be a constraint

613
00:28:35,380 --> 00:28:37,320
that an adversary is particularly worried

614
00:28:37,320 --> 00:28:38,950
about in the bottom right,

615
00:28:38,950 --> 00:28:40,140
in this sort of patch attack,

616
00:28:40,140 --> 00:28:41,850
an adversary is perfectly willing

617
00:28:41,850 --> 00:28:44,105
to make very large changes.

618
00:28:44,105 --> 00:28:47,010
And there are certain attack scenarios,

619
00:28:47,010 --> 00:28:49,610
where that's reasonable to assume,

620
00:28:49,610 --> 00:28:53,760
so these guarantees might not
apply to to those situations.

621
00:28:53,760 --> 00:28:57,629
So understanding how to
generalize results and metrics,

622
00:28:57,630 --> 00:29:02,150
is not a simple task, and
we're hoping that this Testbed,

623
00:29:02,150 --> 00:29:05,093
helps foster conversations in that area.

624
00:29:06,850 --> 00:29:09,520
Let me just give, a couple more examples.

625
00:29:09,520 --> 00:29:11,150
So there's other aspects

626
00:29:11,150 --> 00:29:13,190
like characteristics of the data set,

627
00:29:13,190 --> 00:29:16,130
that can that can affect
how effective something is.

628
00:29:16,130 --> 00:29:19,980
So, as we saw earlier,
this poisoning attack on,

629
00:29:19,980 --> 00:29:22,680
with mixing a currant with an apple,

630
00:29:22,680 --> 00:29:25,890
it's pretty easy to see
how those can get mixed up

631
00:29:25,890 --> 00:29:29,130
and might not be noticed by a human doing

632
00:29:29,130 --> 00:29:31,873
a sort of review of the training data.

633
00:29:33,490 --> 00:29:36,380
But a much simpler data set
like handwritten digits,

634
00:29:36,380 --> 00:29:39,440
provides much less
opportunity for an adversary

635
00:29:39,440 --> 00:29:41,620
to perform this kind of attack.

636
00:29:41,620 --> 00:29:43,489
On the top the adversary might get lucky

637
00:29:43,490 --> 00:29:46,190
and find some samples that look similar,

638
00:29:46,190 --> 00:29:47,550
that are already different classes

639
00:29:47,550 --> 00:29:50,110
to blend in just the right way,

640
00:29:50,110 --> 00:29:52,449
but, on the bottom, it's
probably, you're not

641
00:29:52,450 --> 00:29:55,330
likely to find a lot of
samples of one and five

642
00:29:55,330 --> 00:29:57,629
that you can blend that
wouldn't be noticeable

643
00:29:58,520 --> 00:30:01,990
to a human or perhaps even to a computer.

644
00:30:01,990 --> 00:30:06,100
So in essence what this means is that

645
00:30:06,100 --> 00:30:09,419
certain kinds of let's say
data sanitization techniques,

646
00:30:09,420 --> 00:30:11,080
are much more likely to be effected

647
00:30:11,080 --> 00:30:13,010
against this on the simple data set,

648
00:30:13,010 --> 00:30:14,460
than on the more complex one.

649
00:30:16,850 --> 00:30:18,030
And finally, I wanna make sure

650
00:30:18,030 --> 00:30:20,780
that people come away from this talk,

651
00:30:20,780 --> 00:30:22,840
thinking not just about
the machine learning,

652
00:30:22,840 --> 00:30:25,362
but about the systems that they support.

653
00:30:26,370 --> 00:30:29,449
Because AI technology does
not exist in isolation,

654
00:30:29,450 --> 00:30:34,240
it does exist to support larger goals.

655
00:30:34,240 --> 00:30:36,216
And so we have to ask ourselves,

656
00:30:36,216 --> 00:30:39,520
in this larger system, what is it doing,

657
00:30:39,520 --> 00:30:41,280
what does an adversary's true objective?

658
00:30:41,280 --> 00:30:43,540
It's not just to cause a misclassification

659
00:30:43,540 --> 00:30:44,960
but it's because of misclassification

660
00:30:44,960 --> 00:30:48,493
we'll have some follow on
effects that really matter.

661
00:30:49,630 --> 00:30:51,590
And so in this whole systems thinking,

662
00:30:51,590 --> 00:30:53,800
we wanna look at, what
are the dependencies

663
00:30:53,800 --> 00:30:55,810
among the various things,

664
00:30:55,810 --> 00:31:00,030
and does that provide new opportunities

665
00:31:00,030 --> 00:31:02,053
for defenses or for attacks?

666
00:31:03,340 --> 00:31:04,490
So to bring this point home,

667
00:31:04,490 --> 00:31:07,620
let me just give you a very
concrete set of examples.

668
00:31:07,620 --> 00:31:10,189
So on the left, we have
a security checkpoint

669
00:31:10,190 --> 00:31:12,490
that is doing facial recognition

670
00:31:13,511 --> 00:31:16,190
to generate a boarding pass, for example.

671
00:31:16,190 --> 00:31:17,920
So this is a very controlled environment.

672
00:31:17,920 --> 00:31:18,910
The guard standing there,

673
00:31:18,910 --> 00:31:20,460
is not gonna let someone hold up

674
00:31:20,460 --> 00:31:22,940
a very colorful around
patch to their face.

675
00:31:22,940 --> 00:31:25,123
That's just not gonna be allowed.

676
00:31:26,460 --> 00:31:29,730
And the images that it's
creating are generated freshly.

677
00:31:29,730 --> 00:31:31,130
So there's very little opportunity

678
00:31:31,130 --> 00:31:34,590
for an adversary to perform manipulations

679
00:31:34,590 --> 00:31:36,500
on the actual pixels themselves,

680
00:31:36,500 --> 00:31:40,930
unless they get into the system itself.

681
00:31:40,930 --> 00:31:42,450
So this is a very controlled environment,

682
00:31:42,450 --> 00:31:43,677
and that what this means is that

683
00:31:43,677 --> 00:31:45,939
there's certain classes of attacks

684
00:31:45,940 --> 00:31:48,990
that we might be less worried about.

685
00:31:48,990 --> 00:31:51,350
Research has shown that
you can do the patch thing,

686
00:31:51,350 --> 00:31:53,639
you can kind of put
those things on glasses,

687
00:31:53,640 --> 00:31:55,700
I'm not saying that you
don't have to worry,

688
00:31:55,700 --> 00:31:58,990
but I'm saying that the threat
landscape sort of changes

689
00:31:58,990 --> 00:32:01,100
due to the deployment model.

690
00:32:01,100 --> 00:32:03,209
If you look at the
automated driving setting,

691
00:32:03,210 --> 00:32:05,690
there, the images are
also generated freshly.

692
00:32:05,690 --> 00:32:08,300
There's little opportunity
to do those pixel attacks.

693
00:32:08,300 --> 00:32:10,840
But the environment is
open, it's very easy

694
00:32:10,840 --> 00:32:15,080
for an adversary to slap a
colorful patch on a stop sign,

695
00:32:15,080 --> 00:32:16,340
and there's nothing we can really do

696
00:32:16,340 --> 00:32:19,419
to monitor the environment,
to help prevent that.

697
00:32:19,420 --> 00:32:21,800
And finally, the sort of a good example,

698
00:32:21,800 --> 00:32:23,129
or maybe a more open than that,

699
00:32:23,130 --> 00:32:25,480
is image forensics use case,

700
00:32:25,480 --> 00:32:29,770
where you don't really have
control over the images on,

701
00:32:29,770 --> 00:32:32,639
let's say, someone's
laptop that you're trying

702
00:32:33,540 --> 00:32:36,030
look through it to find
incriminating images.

703
00:32:36,030 --> 00:32:37,840
And furthermore, they're also stale.

704
00:32:37,840 --> 00:32:41,889
So there's plenty of
opportunity for an adversary

705
00:32:41,890 --> 00:32:44,235
to have manipulated
them to be misclassified

706
00:32:44,235 --> 00:32:46,983
by your machine learning algorithm.

707
00:32:49,070 --> 00:32:53,040
So I wanna leave you with
this sense that while security

708
00:32:53,040 --> 00:32:55,270
and measuring the aspects
of the machine learning

709
00:32:55,270 --> 00:32:57,562
is important, it's not the whole story.

710
00:32:59,580 --> 00:33:00,800
So that's about everything

711
00:33:00,800 --> 00:33:02,500
we wanted to share with you today.

712
00:33:04,470 --> 00:33:07,410
If you do one thing in the next week,

713
00:33:07,410 --> 00:33:10,150
I encourage you to please
try out the Testbed.

714
00:33:10,150 --> 00:33:14,650
It should be available
on GitHub, if it is not,

715
00:33:14,650 --> 00:33:15,700
at the time of the conference

716
00:33:15,700 --> 00:33:17,910
it will be very shortly thereafter.

717
00:33:17,910 --> 00:33:21,580
And so please take a
look and give it a try.

718
00:33:21,580 --> 00:33:24,000
And, and we're happy to have your feedback

719
00:33:24,000 --> 00:33:26,120
because this is an ongoing project,

720
00:33:26,120 --> 00:33:28,090
and so we're looking to help improve it

721
00:33:29,180 --> 00:33:30,393
with community feedback.

722
00:33:31,960 --> 00:33:33,280
I also wanna encourage you,

723
00:33:33,280 --> 00:33:34,600
if you are in organizations

724
00:33:34,600 --> 00:33:39,600
that are using ML enabled
products or processes,

725
00:33:40,500 --> 00:33:44,400
think about what this landscape
of attacks really means

726
00:33:44,400 --> 00:33:45,770
for your organization.

727
00:33:45,770 --> 00:33:48,040
Identify those processes
that might be affected

728
00:33:48,040 --> 00:33:50,740
if they were attacked in such
ways as you've seen today,

729
00:33:50,740 --> 00:33:52,380
and maybe begin to make a plan

730
00:33:52,380 --> 00:33:54,590
for what you would do to evaluate those

731
00:33:54,590 --> 00:33:57,290
to see the degree to which
the robust to such attacks.

732
00:33:58,190 --> 00:34:01,080
And within six months,
hopefully you might actually

733
00:34:01,080 --> 00:34:04,750
be able to implement such
plans and leverage our Testbed.

734
00:34:04,750 --> 00:34:08,960
And so we really encourage you to interact

735
00:34:08,960 --> 00:34:12,810
and for those developers of you out there,

736
00:34:12,810 --> 00:34:15,350
we encourage you to contribute
back to the project.

737
00:34:15,350 --> 00:34:16,306
So thank you.

