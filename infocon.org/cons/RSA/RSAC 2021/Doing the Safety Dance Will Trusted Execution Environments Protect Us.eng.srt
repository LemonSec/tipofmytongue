1
00:00:01,680 --> 00:00:03,729
- And hello everyone,

2
00:00:03,730 --> 00:00:08,420
and welcome to the RSA
panel where we're discussing

3
00:00:08,420 --> 00:00:10,500
how to do the Safety Dance.

4
00:00:10,500 --> 00:00:14,410
So it is my super pleasure let's be clear,

5
00:00:14,410 --> 00:00:19,410
to work with and be able to
introduce to you our panelists,

6
00:00:20,270 --> 00:00:23,820
as well as introduce to
you, some of you, possibly,

7
00:00:23,820 --> 00:00:28,820
the Confidential Computing Consortium.

8
00:00:29,660 --> 00:00:33,830
So, and also more broadly,
confidential computing.

9
00:00:33,830 --> 00:00:36,809
So let's talk first about
confidential computing

10
00:00:36,810 --> 00:00:38,680
as a definition.

11
00:00:38,680 --> 00:00:43,090
So more than 30 different companies

12
00:00:43,090 --> 00:00:45,120
around confidential computing

13
00:00:45,120 --> 00:00:48,110
and having an interest in
confidential computing,

14
00:00:48,110 --> 00:00:52,310
have gone ahead and built
a definition for this.

15
00:00:52,310 --> 00:00:54,240
So the groups that participated

16
00:00:54,240 --> 00:00:56,490
included CSP and Silicon vendors

17
00:00:56,490 --> 00:00:59,860
and Cloud vendors and
hardware manufacturers,

18
00:00:59,860 --> 00:01:02,010
service companies, et cetera.

19
00:01:02,010 --> 00:01:03,730
And they spent more than six months

20
00:01:03,730 --> 00:01:06,750
defining what confidential computing is

21
00:01:06,750 --> 00:01:10,340
in order to build for our industry,

22
00:01:10,340 --> 00:01:13,760
a good solid standard to have our future

23
00:01:13,760 --> 00:01:16,180
well and sufficiently supported

24
00:01:16,180 --> 00:01:19,020
with confidence and confidentiality.

25
00:01:19,020 --> 00:01:22,830
So what is this actual definition?

26
00:01:22,830 --> 00:01:26,929
And what, broadly speaking,
confidential computing

27
00:01:26,930 --> 00:01:29,800
is the protection of data in use.

28
00:01:29,800 --> 00:01:32,149
This is the big difference here in use,

29
00:01:32,150 --> 00:01:35,963
using hardware based trusted
execution environments.

30
00:01:37,330 --> 00:01:39,140
And now you ask what is a TEE.

31
00:01:39,140 --> 00:01:41,890
What is a trusted execution environment?

32
00:01:41,890 --> 00:01:44,740
It's a hardware based TEE,

33
00:01:44,740 --> 00:01:46,839
using hardware backed techniques

34
00:01:46,840 --> 00:01:51,450
to provide increased security guarantees,

35
00:01:51,450 --> 00:01:54,900
for the execution of code
and protection of data

36
00:01:54,900 --> 00:01:56,410
within that environment.

37
00:01:56,410 --> 00:01:58,460
Again, data in use.

38
00:01:58,460 --> 00:02:01,080
So now that we have a basic definition

39
00:02:01,080 --> 00:02:03,750
thanks to the Confidential
Computing Consortium

40
00:02:03,750 --> 00:02:06,500
of what confidential computing is,

41
00:02:06,500 --> 00:02:09,389
we now get to talk about
how this can help you

42
00:02:09,389 --> 00:02:11,489
as end users and customers

43
00:02:11,490 --> 00:02:14,410
and how this really moves
our industry forward.

44
00:02:14,410 --> 00:02:18,950
So let me do quick
introductions for our panelists.

45
00:02:18,950 --> 00:02:22,589
Today, we're going to get
to talk with Aeva Black,

46
00:02:22,590 --> 00:02:24,310
who works at Microsoft,

47
00:02:24,310 --> 00:02:27,920
working in the Azure
Confidential Compute group.

48
00:02:27,920 --> 00:02:31,280
We get to speak with Nelly
Porter who works for Google,

49
00:02:31,280 --> 00:02:34,550
also working on continental computing

50
00:02:34,550 --> 00:02:36,270
you may see a theme here.

51
00:02:36,270 --> 00:02:40,000
And now, we also have get
to speak with Mike Burcell,

52
00:02:40,000 --> 00:02:42,490
who was a long time architect at Red Hat,

53
00:02:42,490 --> 00:02:46,320
and also, oddly enough works
on confidential computing.

54
00:02:46,320 --> 00:02:48,600
So if they have other
things they want to add

55
00:02:48,600 --> 00:02:51,739
in their little, in their introductions,

56
00:02:51,740 --> 00:02:53,080
or as way of introductions,

57
00:02:53,080 --> 00:02:56,070
we can have them add them
as we ask some questions.

58
00:02:56,070 --> 00:02:59,040
So let's start with,
we gave the definition,

59
00:02:59,040 --> 00:03:02,822
let's start with the basic question of,

60
00:03:03,820 --> 00:03:06,100
who should care about
confidential computing.

61
00:03:06,100 --> 00:03:08,329
Is this something everyone
should be interested in,

62
00:03:08,330 --> 00:03:11,320
what are the use cases
that are deployable today

63
00:03:11,320 --> 00:03:14,910
and what role do we think
open source plays in this?

64
00:03:14,910 --> 00:03:17,400
So let's start with Aeva today.

65
00:03:17,400 --> 00:03:19,090
So Aeva, if you have anything more to add

66
00:03:19,090 --> 00:03:19,923
to your introduction,

67
00:03:19,923 --> 00:03:22,290
and then talk about who
you think should care

68
00:03:22,290 --> 00:03:24,019
about confidential computing.

69
00:03:24,020 --> 00:03:27,320
- Sure, so, in addition to being involved

70
00:03:27,320 --> 00:03:29,329
in the Confidential Computing Consortium,

71
00:03:29,330 --> 00:03:31,900
I'm also involved in a couple
other open source foundations

72
00:03:31,900 --> 00:03:34,440
I've been around Cloud computing,

73
00:03:34,440 --> 00:03:37,270
gosh, for awhile now I
guess more than 10 years.

74
00:03:37,270 --> 00:03:39,843
I worked a bunch of OpenStack
on hardware security.

75
00:03:40,900 --> 00:03:42,580
And before that, other stuff.

76
00:03:42,580 --> 00:03:44,500
And I think really everyone should care

77
00:03:44,500 --> 00:03:46,030
about digital privacy.

78
00:03:46,030 --> 00:03:48,230
And you might notice that theme,

79
00:03:48,230 --> 00:03:50,310
I really care a lot about digital privacy.

80
00:03:50,310 --> 00:03:51,880
And I think a lot of people today

81
00:03:51,880 --> 00:03:53,170
do care deeply about it

82
00:03:53,170 --> 00:03:55,510
enough that several
governments around the world

83
00:03:55,510 --> 00:03:56,500
have begun passing

84
00:03:56,500 --> 00:03:59,293
or looking into passing data privacy laws.

85
00:04:00,190 --> 00:04:01,270
And to support this,

86
00:04:01,270 --> 00:04:04,250
I think every company
also needs to care, right?

87
00:04:04,250 --> 00:04:05,600
Listen to your users,

88
00:04:05,600 --> 00:04:09,130
especially those that handle
sensitive data, personal data,

89
00:04:09,130 --> 00:04:11,120
or operate in regulated industries.

90
00:04:13,360 --> 00:04:17,290
- So, Mike, I know you often
have differing opinions

91
00:04:18,178 --> 00:04:19,199
you want to share with--
- I like that,

92
00:04:19,199 --> 00:04:20,870
we have complimentary opinions--

93
00:04:20,870 --> 00:04:22,118
- Complimentary is a great way to discuss.

94
00:04:22,118 --> 00:04:24,180
- A greater way to ask
quite often how about that.

95
00:04:24,180 --> 00:04:26,070
Yeah, so firstly, a little about me

96
00:04:26,070 --> 00:04:29,219
I'm part of the office
of the CTO at Red Hat,

97
00:04:29,220 --> 00:04:30,320
a Chief Security Architect,

98
00:04:30,320 --> 00:04:32,340
but I'm also co-founder
of the NRT project,

99
00:04:32,340 --> 00:04:35,299
which is an open source project

100
00:04:35,300 --> 00:04:36,800
around confidential computing.

101
00:04:37,970 --> 00:04:42,970
So, the I think one answer
to who should care about

102
00:04:43,085 --> 00:04:44,585
combination computing is CSOs.

103
00:04:45,610 --> 00:04:48,860
CSOs should care about
combination computing,

104
00:04:48,860 --> 00:04:50,750
not just commercial companies,

105
00:04:50,750 --> 00:04:53,720
but government, charitable,
healthcare, pretty much everyone

106
00:04:53,720 --> 00:04:56,420
because all of those, as Aeva said,

107
00:04:56,420 --> 00:05:01,420
have data or algorithms
or workloads or programs

108
00:05:03,560 --> 00:05:05,740
that are sensitive

109
00:05:05,740 --> 00:05:08,180
and that needs to be
protected in some way or that.

110
00:05:08,180 --> 00:05:11,320
So, I mean, architects
need to learn about it

111
00:05:11,320 --> 00:05:12,710
and the opportunities it offers,

112
00:05:12,710 --> 00:05:15,520
and developers should think
about getting their hands dirty.

113
00:05:15,520 --> 00:05:18,810
But if the CSOs are on the call

114
00:05:18,810 --> 00:05:22,170
don't understand the change it
can make to the organizations

115
00:05:22,170 --> 00:05:24,440
that they lead or they're parts of,

116
00:05:24,440 --> 00:05:26,190
then they're not just missing out

117
00:05:26,190 --> 00:05:28,060
on an important new technology,

118
00:05:28,060 --> 00:05:30,690
but also really, I
think to a change to how

119
00:05:30,690 --> 00:05:34,200
we're going to think about
deploying applications

120
00:05:34,200 --> 00:05:36,110
and our risk profiles.

121
00:05:36,110 --> 00:05:37,110
And that's really important.

122
00:05:37,110 --> 00:05:38,690
And you also asked about open source.

123
00:05:38,690 --> 00:05:42,044
From my point of view, open
source is vital in this

124
00:05:42,044 --> 00:05:46,680
because you can't trust what,
we love open source, yeah.

125
00:05:46,680 --> 00:05:49,510
If you can't trust what you're running,

126
00:05:49,510 --> 00:05:51,800
then how can you trust that
it's been protected properly.

127
00:05:51,800 --> 00:05:53,840
And you can't trust it
unless it's open source,

128
00:05:53,840 --> 00:05:55,919
and you can audit it, the
community can audit it.

129
00:05:55,920 --> 00:05:57,460
We'll probably come back
to this bill later on,

130
00:05:57,460 --> 00:06:00,919
but I think open sources
is vital in this context.

131
00:06:00,920 --> 00:06:02,420
Nelly.
- Awesome.

132
00:06:02,420 --> 00:06:03,253
Nelly, yeah.

133
00:06:03,253 --> 00:06:05,050
- Thank you so much, Mike and Sarah.

134
00:06:05,050 --> 00:06:08,890
So again, adding a few words about me

135
00:06:08,890 --> 00:06:11,940
there's Google cloud security and Google,

136
00:06:11,940 --> 00:06:16,469
and all of this time was working
in a confidential computing

137
00:06:16,470 --> 00:06:20,090
leading confidential
computing efforts in Google.

138
00:06:20,090 --> 00:06:22,419
And trying to help industry

139
00:06:22,420 --> 00:06:24,960
to drive interoperability technology.

140
00:06:24,960 --> 00:06:27,859
So all of us would be
able to work together

141
00:06:27,860 --> 00:06:30,830
in harmony if it's ever possible.

142
00:06:30,830 --> 00:06:35,530
But again, who cares about
confidential computing?

143
00:06:35,530 --> 00:06:37,460
I absolutely agree with both of you.

144
00:06:37,460 --> 00:06:41,349
It needs to be from CSOs, but also again,

145
00:06:41,350 --> 00:06:45,960
cloud out means a IT teams, DevOps,

146
00:06:45,960 --> 00:06:50,159
everybody is touching
infrastructure, touching workloads

147
00:06:50,160 --> 00:06:54,540
and trying to make them a
protected when they're running.

148
00:06:54,540 --> 00:06:57,940
Because we all, a lot of these necessity

149
00:06:57,940 --> 00:07:02,300
of protecting data in transit and in rest.

150
00:07:02,300 --> 00:07:05,400
But somehow we forgot
completely about the fact

151
00:07:05,400 --> 00:07:07,690
that we need to do
something with this data.

152
00:07:07,690 --> 00:07:08,980
We need to index it.

153
00:07:08,980 --> 00:07:12,910
We need criticize this data,
really train on these data.

154
00:07:12,910 --> 00:07:17,110
Then it means confidential
computing here to fill this hole.

155
00:07:17,110 --> 00:07:19,190
One that is not things that you ask Sarah

156
00:07:19,190 --> 00:07:22,000
about use cases and it's
kind of our ongoing.

157
00:07:22,000 --> 00:07:26,410
So use cases, I think
it's a lot of use cases.

158
00:07:26,410 --> 00:07:27,500
When we started

159
00:07:27,500 --> 00:07:30,910
and we had so many conversations
with customers and users.

160
00:07:30,910 --> 00:07:34,100
It was kind of very
limited side of, Oh my God

161
00:07:34,100 --> 00:07:36,170
how do Google protect my secrets?

162
00:07:36,170 --> 00:07:39,210
What I will do with my
certificate, et cetera.

163
00:07:39,210 --> 00:07:41,070
But as we moving along

164
00:07:41,070 --> 00:07:43,760
it is the ability of
confidential computing to

165
00:07:43,760 --> 00:07:48,760
cover incredibly large
workloads becoming necessity.

166
00:07:49,500 --> 00:07:53,520
Our customers talking about,
I want to break any database

167
00:07:53,520 --> 00:07:57,190
from post-grads to MySQL to MongoDB.

168
00:07:57,190 --> 00:08:00,780
I want to protect my
employees records utilizing

169
00:08:00,780 --> 00:08:01,902
those databases.

170
00:08:01,903 --> 00:08:04,530
Or how I will do analytics.

171
00:08:04,530 --> 00:08:06,729
And that means Spark and Hadoop

172
00:08:06,730 --> 00:08:09,480
and name it and how they'll do all

173
00:08:09,480 --> 00:08:12,510
of those simulation and
preferably not only on CPU

174
00:08:12,510 --> 00:08:15,402
but give me GPU, and the FPG's.

175
00:08:16,372 --> 00:08:18,120
And this, why is the
confidence of computing

176
00:08:18,120 --> 00:08:21,810
consortium is all Silicon industries

177
00:08:21,810 --> 00:08:23,100
is so critically important

178
00:08:23,100 --> 00:08:27,000
because we all have the same goals

179
00:08:27,000 --> 00:08:30,850
but you need to have very
clear again, requirements

180
00:08:30,850 --> 00:08:34,600
but in how we will
implement those goals, Mike

181
00:08:34,600 --> 00:08:35,880
you raising your hand.

182
00:08:35,880 --> 00:08:38,439
- Yeah, Nelly, You touched
on an important point

183
00:08:38,440 --> 00:08:41,960
which I want to bring
out, which is, it's not

184
00:08:41,960 --> 00:08:45,260
just necessarily the data
your you're looking after.

185
00:08:45,260 --> 00:08:48,130
You sometimes wanna protect
the programs themselves.

186
00:08:48,130 --> 00:08:52,260
If you think about, sort of AI or ML,

187
00:08:52,260 --> 00:08:56,300
the those programs themselves,
or, trading applications

188
00:08:56,300 --> 00:09:00,099
or a farmer or voting
applications, all of these things.

189
00:09:00,100 --> 00:09:01,640
In fact, sometimes it's not

190
00:09:01,640 --> 00:09:04,590
the data is sometimes even subsidiary

191
00:09:04,590 --> 00:09:06,950
to the algorithm that's running it.

192
00:09:06,950 --> 00:09:11,000
And TEEs can protect those algorithms

193
00:09:11,000 --> 00:09:13,710
the workloads themselves
and not just the data.

194
00:09:13,710 --> 00:09:14,543
And that's one

195
00:09:14,543 --> 00:09:16,980
of the really exciting
things about this technology.

196
00:09:16,980 --> 00:09:19,480
Sometimes we see them
as sort of separate to,

197
00:09:19,480 --> 00:09:22,910
sort of homomorphic encryption
or multi-party computation.

198
00:09:22,910 --> 00:09:25,209
But actually that really
complimentary, sorry, Ava

199
00:09:25,210 --> 00:09:26,180
you wanna say something?
- Aeva.

200
00:09:26,180 --> 00:09:28,099
- Yeah, I wanna add that.

201
00:09:28,100 --> 00:09:30,330
We've mentioned TPMS and HSMs a little bit

202
00:09:30,330 --> 00:09:32,760
and I think most folks are
probably familiar with those.

203
00:09:32,760 --> 00:09:34,560
They're pretty widely distributed now.

204
00:09:34,560 --> 00:09:37,642
The evolution here is
adding marketability.

205
00:09:38,549 --> 00:09:43,260
TPM or an HSM protects very
specific parts of a system.

206
00:09:43,260 --> 00:09:45,550
Either letting you attest the integrity

207
00:09:45,550 --> 00:09:46,829
of hardware components

208
00:09:46,830 --> 00:09:48,990
or software components or protect a key.

209
00:09:48,990 --> 00:09:52,400
A TEE protects the entire
application and its memory.

210
00:09:52,400 --> 00:09:53,970
That's the shift here.

211
00:09:53,970 --> 00:09:57,090
And why Sarah gave us in the intro

212
00:09:57,090 --> 00:10:00,850
the consortium spent so much
time defining what is a TEE

213
00:10:00,850 --> 00:10:01,990
- And you don't,

214
00:10:01,990 --> 00:10:05,950
you can write applications to run on HSMs

215
00:10:05,950 --> 00:10:08,660
but it's really difficult,
very difficult to deploy.

216
00:10:08,660 --> 00:10:10,280
It's easy to get it wrong.

217
00:10:10,280 --> 00:10:13,060
And tees are much more
general compute computation.

218
00:10:13,060 --> 00:10:13,892
- Exactly.

219
00:10:15,141 --> 00:10:18,050
- And to your point,
about who should care.

220
00:10:18,050 --> 00:10:20,430
This also then extends
it to the whole developer

221
00:10:20,430 --> 00:10:23,819
anyone who is developing
and using data, oddly enough

222
00:10:23,820 --> 00:10:25,170
that seems to be everybody.

223
00:10:26,157 --> 00:10:27,300
- Yep.
- Awesome.

224
00:10:27,300 --> 00:10:30,140
- So we're going to be
now see a future where

225
00:10:30,140 --> 00:10:32,210
everyone needs to think about this,

226
00:10:32,210 --> 00:10:34,680
but of course we don't
have a future where all

227
00:10:34,680 --> 00:10:37,199
of this technology is evenly distributed.

228
00:10:37,200 --> 00:10:41,930
So given there will be a
future where everything

229
00:10:41,930 --> 00:10:45,089
where continental computing
is widely available.

230
00:10:45,090 --> 00:10:47,510
At that point, we hear
lots of questions about

231
00:10:47,510 --> 00:10:50,569
how much a customer needs to
trust their cloud provider.

232
00:10:50,570 --> 00:10:51,403
So in the future

233
00:10:51,403 --> 00:10:54,490
when we have confidential
computing everywhere

234
00:10:54,490 --> 00:10:57,590
will your customers still have
to trust your cloud provider?

235
00:10:57,590 --> 00:10:59,740
And how does that really fit?

236
00:10:59,740 --> 00:11:02,233
Nelly, I think you were
first up on this one.

237
00:11:03,170 --> 00:11:06,599
- Absolutely as cloud
provider representative here.

238
00:11:06,600 --> 00:11:11,600
So I think this is very
complicated and very

239
00:11:11,910 --> 00:11:16,350
multifaceted question because
it is a question of trust.

240
00:11:16,350 --> 00:11:20,000
And again, if I will kind of compare it

241
00:11:20,000 --> 00:11:24,660
to our life as consumers,
when we using our ISP,

242
00:11:24,660 --> 00:11:29,660
or when we are using our
devices, we have to assume

243
00:11:30,190 --> 00:11:34,320
that our providers, ISP
providers, manufacturers

244
00:11:34,320 --> 00:11:38,400
of our devices and those apart
do not violate our trust,

245
00:11:38,400 --> 00:11:42,610
because otherwise we probably
will never use this services.

246
00:11:42,610 --> 00:11:44,760
So I think cloud providers today

247
00:11:44,760 --> 00:11:47,870
all of them Google is not exception.

248
00:11:47,870 --> 00:11:51,190
It's taking this
responsibility very seriously

249
00:11:51,190 --> 00:11:55,110
and offering services, managed services.

250
00:11:55,110 --> 00:11:59,717
It will simplify
customers again lives with

251
00:12:00,670 --> 00:12:04,110
helping them to manage
this tremendous workloads

252
00:12:04,110 --> 00:12:08,550
they bring into the cloud
in the way that it's managed

253
00:12:08,550 --> 00:12:13,550
and doesn't require it for
dependency on a cloud provider.

254
00:12:14,220 --> 00:12:17,960
But think very realistic
and pragmatic as I am.

255
00:12:17,960 --> 00:12:20,440
I think even every single
confidential computing

256
00:12:20,440 --> 00:12:22,810
building place, it will be areas

257
00:12:23,828 --> 00:12:27,578
of when we a building
more necessity of trust

258
00:12:29,830 --> 00:12:33,240
but probably still will have some areas

259
00:12:33,240 --> 00:12:34,710
as its cloud providers

260
00:12:34,710 --> 00:12:38,520
but not be able to open
source every single line

261
00:12:38,520 --> 00:12:41,890
of code that they using to
provide those managed services.

262
00:12:41,890 --> 00:12:46,330
So short of this, we do need
to provide full transparency

263
00:12:46,330 --> 00:12:50,070
on modification paging,
and how's those binary.

264
00:12:50,070 --> 00:12:51,760
So this services.

265
00:12:51,760 --> 00:12:56,760
This applications that cloud
provider offering is maintain

266
00:12:58,410 --> 00:12:59,660
what is a provenance

267
00:12:59,660 --> 00:13:01,850
and both of the processes around that.

268
00:13:01,850 --> 00:13:06,160
So I think it's will
evolved, is it some trust?

269
00:13:06,160 --> 00:13:08,100
Does it need to be in place?

270
00:13:08,100 --> 00:13:09,170
But I still believe

271
00:13:09,170 --> 00:13:12,729
that some areas would
require some level of trust.

272
00:13:12,730 --> 00:13:14,720
We sometimes internally joking

273
00:13:14,720 --> 00:13:19,720
that the whole concept of
zero trust is fantastic dream

274
00:13:20,490 --> 00:13:23,100
but if we will get to absolute trust

275
00:13:23,100 --> 00:13:26,280
is and very very close to zero.

276
00:13:26,280 --> 00:13:29,829
We actually celebrate
and call this weekend.

277
00:13:29,830 --> 00:13:32,183
It's my story.

278
00:13:34,250 --> 00:13:36,140
- Mike, I think you have a slightly

279
00:13:36,140 --> 00:13:36,980
different thought on that

280
00:13:36,980 --> 00:13:40,600
- Yeah, so I mean, I would
take it the opposite way around

281
00:13:40,600 --> 00:13:44,230
and say that not only
should they not need to

282
00:13:44,230 --> 00:13:45,850
trust their CSP people

283
00:13:45,850 --> 00:13:48,853
they must not trust their
CSP, that cloud provider.

284
00:13:49,830 --> 00:13:53,350
And when, I'm a big open source person.

285
00:13:53,350 --> 00:13:54,750
I think the cloud providers should

286
00:13:54,750 --> 00:13:57,720
open-source every single
piece of what they do

287
00:13:57,720 --> 00:14:00,410
because that's the way we
can have the highest trust.

288
00:14:00,410 --> 00:14:03,829
And, even then you shouldn't need to

289
00:14:03,830 --> 00:14:06,150
but I want to be a bit
more nuanced about that.

290
00:14:06,150 --> 00:14:08,310
There will always be
areas where you need to

291
00:14:08,310 --> 00:14:09,709
trust your cloud provider

292
00:14:09,710 --> 00:14:14,710
whether it's it to provide,
resource availability

293
00:14:15,680 --> 00:14:19,060
that's access to the CPU, it's access to,

294
00:14:19,060 --> 00:14:22,119
to networking, or soft storage, whatever.

295
00:14:22,120 --> 00:14:27,120
But I think in the context
of TEE trust execution firms.

296
00:14:27,120 --> 00:14:31,090
I wanna be removing the
requirement to trust the CSP

297
00:14:31,090 --> 00:14:33,330
for setting it up and running it.

298
00:14:33,330 --> 00:14:37,760
And so I want there to be
no way that the CSP can look

299
00:14:37,760 --> 00:14:42,760
at any parts of my supply
chain, my data, my workload.

300
00:14:44,290 --> 00:14:47,160
So I think we come to it from
different points of view.

301
00:14:47,160 --> 00:14:48,900
I think we're probably hopefully converged

302
00:14:48,900 --> 00:14:51,770
on the same, same answer,
but that's kind of where,

303
00:14:51,770 --> 00:14:54,563
I'd be starting rather
than finishing hows that.

304
00:14:55,550 --> 00:14:58,660
Sounds great, how about you, Ava?

305
00:14:58,660 --> 00:15:01,689
You also are part of a
cloud service provider.

306
00:15:01,690 --> 00:15:03,440
We have two cloud service providers

307
00:15:03,440 --> 00:15:07,120
and one open source company now purchased

308
00:15:07,120 --> 00:15:08,703
by a cloud service provider.

309
00:15:09,960 --> 00:15:10,793
- It's true.

310
00:15:10,793 --> 00:15:12,250
I am part of a cloud
service provider, but also.

311
00:15:12,250 --> 00:15:15,350
I'm also an open-source
program manager here at Azure.

312
00:15:15,350 --> 00:15:19,140
So, my role is a lot kind
of similar to Mike's.

313
00:15:19,140 --> 00:15:21,670
We kind of between the two
of like helping the cloud

314
00:15:21,670 --> 00:15:25,520
but also advancing our
open source initiatives

315
00:15:25,520 --> 00:15:28,020
and building trust with
customers through open source

316
00:15:28,020 --> 00:15:30,160
and even long before joining Azure.

317
00:15:30,160 --> 00:15:30,992
And I've been involved

318
00:15:30,993 --> 00:15:33,110
in open source almost my entire career

319
00:15:33,110 --> 00:15:35,313
probably 15 out of 20 years.

320
00:15:36,410 --> 00:15:37,350
And a big chunk

321
00:15:37,350 --> 00:15:39,400
of that was working on
OpenStack and Kubernetes.

322
00:15:39,400 --> 00:15:41,550
So, I really agree with
Mike it's possible to

323
00:15:41,550 --> 00:15:44,010
have a fully open source cloud stack

324
00:15:44,010 --> 00:15:45,600
even down to open-source hardware

325
00:15:45,600 --> 00:15:48,900
open farm firmware
open-source boot loaders.

326
00:15:48,900 --> 00:15:51,590
However even doing that

327
00:15:51,590 --> 00:15:54,750
doesn't entirely solve
software security problems.

328
00:15:54,750 --> 00:15:56,550
Customers are not
necessarily going to read all

329
00:15:56,550 --> 00:15:59,719
the source code and software
supply chain attacks against

330
00:15:59,720 --> 00:16:03,950
both open-source and against
hardware are increasing

331
00:16:03,950 --> 00:16:06,640
whether it's increasing
in awareness threats.

332
00:16:06,640 --> 00:16:09,990
So back to the question
in a perfect world,

333
00:16:09,990 --> 00:16:11,230
I don't think a customer

334
00:16:11,230 --> 00:16:12,930
would have to trust their
cloud provider at all.

335
00:16:12,930 --> 00:16:15,550
They'd be able to rely on a third party

336
00:16:15,550 --> 00:16:19,079
or independent verification
to attest the integrity

337
00:16:19,080 --> 00:16:20,220
of the services.

338
00:16:20,220 --> 00:16:23,080
Detect, if the provider was
doing anything nefarious

339
00:16:23,080 --> 00:16:25,190
or had been compromised in some way.

340
00:16:25,190 --> 00:16:26,750
But we live in the real world

341
00:16:26,750 --> 00:16:28,650
and we're not at that ideal state yet.

342
00:16:29,530 --> 00:16:32,300
And after spending several
years doing bare metal

343
00:16:32,300 --> 00:16:35,609
provisioning and bare metal
security within cloud providers

344
00:16:35,610 --> 00:16:37,330
trying to ensure that the cloud provider

345
00:16:37,330 --> 00:16:39,313
can trust their own hardware.

346
00:16:40,210 --> 00:16:41,400
That's, hard enough.

347
00:16:41,400 --> 00:16:44,630
What we're trying to do now
is provide even more on top

348
00:16:44,630 --> 00:16:46,980
of that layer of auditability
and detectability

349
00:16:46,980 --> 00:16:49,453
to compromise those
and workload isolation.

350
00:16:50,440 --> 00:16:51,860
And so while we might have that

351
00:16:51,860 --> 00:16:53,370
in some specific cases today

352
00:16:53,370 --> 00:16:55,750
I don't think anyone is fully there yet

353
00:16:55,750 --> 00:16:56,900
or where we want to be.

354
00:16:58,460 --> 00:17:00,190
- Can I add something to that?

355
00:17:00,190 --> 00:17:01,640
- Please do now.

356
00:17:01,640 --> 00:17:05,500
- I think again, and I
agree with both of you.

357
00:17:05,500 --> 00:17:07,700
I think we've seen

358
00:17:07,700 --> 00:17:12,700
that open-sourcing
everything it doesn't mean

359
00:17:13,280 --> 00:17:15,129
is that we provide security obsolete.

360
00:17:16,601 --> 00:17:18,960
And more than that we also understand

361
00:17:18,960 --> 00:17:22,579
that all our workloads running on silica

362
00:17:22,579 --> 00:17:26,159
and former is it also has to be validated

363
00:17:26,160 --> 00:17:28,369
and ensures that customers trust it.

364
00:17:28,369 --> 00:17:32,030
I do believe that in cloud provider fault

365
00:17:32,030 --> 00:17:37,030
unilateral cloud ownership
will be eliminated as a risk

366
00:17:38,760 --> 00:17:43,760
and necessity of multiple
companies being auditors

367
00:17:43,980 --> 00:17:48,980
or being convert vendors have
to collude to somehow expose

368
00:17:49,520 --> 00:17:52,200
or leap customers data.

369
00:17:52,200 --> 00:17:55,370
And we won't to do insurers
that unilaterally based

370
00:17:55,370 --> 00:17:59,669
on whatever reasons, none of
the companies would have access

371
00:17:59,670 --> 00:18:03,090
and control of sensitive customers data.

372
00:18:03,090 --> 00:18:06,129
Absolute goal across everything

373
00:18:06,130 --> 00:18:08,910
but what I'm trying to be
realistic, it's why I started

374
00:18:08,910 --> 00:18:11,810
with that is there is definitely times

375
00:18:11,810 --> 00:18:14,639
when it will be possible in some areas

376
00:18:14,640 --> 00:18:19,140
and some use cases where it
completely eliminate the need

377
00:18:19,140 --> 00:18:23,920
of Amy trust, but still
you have to run on silica.

378
00:18:23,920 --> 00:18:26,205
You have to process.

379
00:18:26,205 --> 00:18:27,038
(laughs)

380
00:18:27,038 --> 00:18:29,260
And as a result of that,
then trust is moving

381
00:18:29,260 --> 00:18:34,260
to it is even less visible,
less open source, et cetera.

382
00:18:34,350 --> 00:18:35,500
I'm trying to think about that.

383
00:18:35,500 --> 00:18:40,090
And I'm sure how everybody
in this world tries to think

384
00:18:40,090 --> 00:18:42,399
into big concepts.

385
00:18:42,400 --> 00:18:46,750
So and do not over promise
what will not be possible.

386
00:18:46,750 --> 00:18:50,540
And trying to say that you have
to trust something somewhere

387
00:18:50,540 --> 00:18:53,700
and this why zero trust
is looking interesting

388
00:18:53,700 --> 00:18:57,540
but it's sounds like a
little over reaching.

389
00:18:57,540 --> 00:19:01,420
So it's, my opinion on that.

390
00:19:01,420 --> 00:19:04,009
I'm sure Mike has something to add.

391
00:19:04,009 --> 00:19:07,200
- I wanted to bring up
a point you just made

392
00:19:07,200 --> 00:19:09,400
which is that we need to bring out trust.

393
00:19:09,400 --> 00:19:11,460
I, one of the things that worries me even

394
00:19:11,460 --> 00:19:12,760
about the phrase zero trust

395
00:19:12,760 --> 00:19:14,870
is that there is actually trust in there.

396
00:19:14,870 --> 00:19:17,960
I want to say explicit trust.

397
00:19:17,961 --> 00:19:20,717
- Absolutely Mike absolutely.

398
00:19:20,717 --> 00:19:22,610
- But you're pulling out assumptions

399
00:19:22,610 --> 00:19:23,810
and, what's going on.

400
00:19:23,810 --> 00:19:25,389
I'm much happier with that as a concept.

401
00:19:25,390 --> 00:19:26,223
And you're quite right Nelly.

402
00:19:26,223 --> 00:19:27,370
And over, you said

403
00:19:28,910 --> 00:19:31,970
- I would love to echo what
Nelly said about zero access.

404
00:19:31,970 --> 00:19:35,720
We need to get to a state
where for specific workloads up

405
00:19:35,720 --> 00:19:38,520
to the consumer or the
tenant choice, the cloud

406
00:19:38,520 --> 00:19:41,950
provider has zero access even in use.

407
00:19:41,950 --> 00:19:43,300
That is absolutely essential

408
00:19:43,300 --> 00:19:46,639
for maintaining trust and
operating in certain markets.

409
00:19:46,640 --> 00:19:50,213
- And understands the
decision what they have

410
00:19:51,280 --> 00:19:55,570
and who has access explicit
trust, fantastic term.

411
00:19:55,570 --> 00:19:57,266
I love it.
- It needs to be programmatic.

412
00:19:57,267 --> 00:20:01,000
- And zero access, I
think those two, we going.

413
00:20:01,000 --> 00:20:02,580
- Exactly.
- It needs to be programmatic.

414
00:20:02,580 --> 00:20:04,699
It needs to be business.

415
00:20:04,700 --> 00:20:05,533
It needs to be legal.

416
00:20:05,533 --> 00:20:09,620
All of those SLS, all the
different layers need to reflect.

417
00:20:09,620 --> 00:20:12,100
- I really want to see
us reach the state where

418
00:20:12,100 --> 00:20:15,520
our business agreements, our trust written

419
00:20:15,520 --> 00:20:20,020
in contracts today between
providers or custodians

420
00:20:20,020 --> 00:20:23,000
of data can actually be encoded in policy

421
00:20:23,000 --> 00:20:27,050
and enforced somehow
cryptographically or programmatically

422
00:20:27,050 --> 00:20:29,243
not just through a Yaml file.

423
00:20:30,130 --> 00:20:31,630
- And we know that's difficult

424
00:20:33,180 --> 00:20:37,073
we'd go back to that trophy and
then practical cryptography.

425
00:20:38,120 --> 00:20:40,822
- It's not an easy
problem to be, not at all.

426
00:20:41,710 --> 00:20:44,610
- We can look on this for the
entire next hour and a half.

427
00:20:47,910 --> 00:20:50,388
- But I need to bridge
to the next question.

428
00:20:50,388 --> 00:20:52,629
- Yes, Sarah go.

429
00:20:52,630 --> 00:20:53,710
- So I think you're actually

430
00:20:53,710 --> 00:20:55,280
you're talking about amazing stuff

431
00:20:55,280 --> 00:20:57,730
because trust is the underpinning of this.

432
00:20:57,730 --> 00:21:01,700
And we're seeing this
inflection point in the industry

433
00:21:01,700 --> 00:21:06,020
as the industry moves from
a model where our end users

434
00:21:06,020 --> 00:21:08,879
and customers didn't have expertise

435
00:21:08,880 --> 00:21:10,700
and so wanted to be told

436
00:21:10,700 --> 00:21:13,940
and that trust was then earned through.

437
00:21:13,940 --> 00:21:15,860
Having told people, telling people

438
00:21:15,860 --> 00:21:18,790
and customers how to manage
their fully integrated stack

439
00:21:18,790 --> 00:21:20,700
top to bottom, as opposed
to what we have now

440
00:21:20,700 --> 00:21:24,450
which is opinions from our
end users, and composability

441
00:21:24,450 --> 00:21:27,880
because they are the
subject matter experts.

442
00:21:27,880 --> 00:21:30,830
And so we need them to be just as active

443
00:21:30,830 --> 00:21:34,790
in their privacy and their
trust engagements with people

444
00:21:34,790 --> 00:21:36,379
with their cloud providers

445
00:21:36,380 --> 00:21:39,750
with their service
providers, with everyone.

446
00:21:39,750 --> 00:21:43,020
And so in that, we're
actually going way back

447
00:21:43,020 --> 00:21:46,950
to some of the questions
that many of us last

448
00:21:46,950 --> 00:21:49,270
maybe not many of us were
last looked at in college

449
00:21:49,270 --> 00:21:52,360
but that have been coming
up over and over and over.

450
00:21:52,360 --> 00:21:57,080
The philosophical questions
of ethical and policy in this.

451
00:21:57,080 --> 00:22:00,629
So are there ethical
and policy implications?

452
00:22:00,630 --> 00:22:02,190
And, we've got lots

453
00:22:02,190 --> 00:22:04,920
of recent events that bring up security

454
00:22:04,920 --> 00:22:09,920
and bring our attention deeply
and laser light to security.

455
00:22:11,050 --> 00:22:12,629
How is that influencing this work?

456
00:22:12,630 --> 00:22:13,940
What are the policy implications?

457
00:22:13,940 --> 00:22:16,370
What are the ethical implications?

458
00:22:16,370 --> 00:22:17,939
Aeva, I think you were at first.

459
00:22:17,940 --> 00:22:21,170
- I think there are
definitely both implications

460
00:22:21,170 --> 00:22:23,900
and historical things that
are informing this work.

461
00:22:23,900 --> 00:22:25,140
In the last several years,

462
00:22:25,140 --> 00:22:27,630
in particular, you mentioned
in an inflection point

463
00:22:27,630 --> 00:22:30,010
I might point at the
Cambridge Analytica incident

464
00:22:30,010 --> 00:22:31,712
there have been others as well.

465
00:22:31,712 --> 00:22:33,890
I think we've had a global
awakening to the fact

466
00:22:33,890 --> 00:22:36,070
that our personal and our private data is

467
00:22:36,070 --> 00:22:38,260
in the hands of companies who may

468
00:22:38,260 --> 00:22:41,140
or may not be using it in the
best interest of those people.

469
00:22:41,140 --> 00:22:43,760
And on the other side of the same coin

470
00:22:43,760 --> 00:22:45,480
more companies are prioritizing

471
00:22:45,480 --> 00:22:48,370
and investing in or
increasing their efforts to

472
00:22:48,370 --> 00:22:50,379
protect all personal information

473
00:22:50,380 --> 00:22:52,493
even outside a regulated industry.

474
00:22:54,260 --> 00:22:57,000
- Yeah, I'd go beyond just companies.

475
00:22:57,000 --> 00:22:59,970
I think we need to talk
about governments as well.

476
00:22:59,970 --> 00:23:02,860
We were living in a, in a period where

477
00:23:04,210 --> 00:23:08,480
the way that we trust governments
or don't, is changing.

478
00:23:08,480 --> 00:23:11,430
We're seeing some possible
erosion of civil liberties,

479
00:23:11,430 --> 00:23:14,860
even in Western societies,
which have strictly had,

480
00:23:14,860 --> 00:23:18,020
quite strong protections there.

481
00:23:18,020 --> 00:23:19,960
And, I think we need to be aware

482
00:23:19,960 --> 00:23:24,380
that protecting our data as individuals

483
00:23:24,380 --> 00:23:26,870
or as companies operating
in different jurisdictions

484
00:23:26,870 --> 00:23:29,723
and staff becomes increasingly important.

485
00:23:30,677 --> 00:23:34,199
And I think the competence
competing is not the answer

486
00:23:34,200 --> 00:23:38,460
but it's part of the
jigsaw that can provide

487
00:23:38,460 --> 00:23:40,690
provided an answer as we go through.

488
00:23:40,690 --> 00:23:41,523
- I'll jump back in.

489
00:23:41,523 --> 00:23:45,110
I would like to get to the,
a place with technology,

490
00:23:45,110 --> 00:23:46,889
I as a consumer of technology.

491
00:23:46,890 --> 00:23:49,220
I buy a phone, I use a computer

492
00:23:49,220 --> 00:23:50,860
and I traveled around the world back

493
00:23:50,860 --> 00:23:52,449
in the times when we could.

494
00:23:52,450 --> 00:23:54,100
Help to, again, someday

495
00:23:54,100 --> 00:23:57,260
I'd like to get to the place
where I might have some sort

496
00:23:57,260 --> 00:23:59,010
of a physical hardware token,

497
00:23:59,010 --> 00:24:01,140
like the stuff that is built into my phone

498
00:24:01,140 --> 00:24:03,010
that can protect my data no matter

499
00:24:03,010 --> 00:24:05,710
where it moves in the world.

500
00:24:05,710 --> 00:24:08,600
If it's on a social media site or

501
00:24:08,600 --> 00:24:12,139
if it follows me sort of as a last layer

502
00:24:12,140 --> 00:24:15,120
in edge computing for
localized translation services

503
00:24:15,120 --> 00:24:16,250
I'd love to see that kind

504
00:24:16,250 --> 00:24:19,010
of workload be able to follow
me around across providers

505
00:24:19,010 --> 00:24:22,963
around the world and still be
encrypted back to my token.

506
00:24:23,830 --> 00:24:27,750
- So that makes me think
of self-sovereign identity.

507
00:24:27,750 --> 00:24:28,946
- Yes, exactly.

508
00:24:28,946 --> 00:24:30,416
- I was gonna say the same thing.

509
00:24:30,416 --> 00:24:31,720
- I think, one of the interesting things

510
00:24:31,720 --> 00:24:33,640
around here is that, we've been told

511
00:24:33,640 --> 00:24:35,510
with blockchain that there
is no need for trust.

512
00:24:35,510 --> 00:24:37,270
That's absolutely wrong.

513
00:24:37,270 --> 00:24:39,260
Trust needs to be explicit.

514
00:24:39,260 --> 00:24:42,760
And in fact, as we move to more
complex blockchain use cases

515
00:24:42,760 --> 00:24:44,727
beyond just cryptocurrency

516
00:24:44,727 --> 00:24:47,316
and some permission blockchains,
or shared blockchains

517
00:24:47,316 --> 00:24:51,540
actually there are aspects
of that that really do need

518
00:24:51,540 --> 00:24:53,180
the sort of protections

519
00:24:53,180 --> 00:24:55,190
that confidential computing provides.

520
00:24:55,190 --> 00:24:56,950
- Absolute.
- So we were aware that

521
00:24:56,950 --> 00:24:59,110
it may not be obvious to
people from the outside,

522
00:24:59,110 --> 00:25:00,580
but those of us who've been involved

523
00:25:00,580 --> 00:25:02,290
in the competence computer consortium.

524
00:25:02,290 --> 00:25:04,610
We spoke to other people from
the blockchain community.

525
00:25:04,610 --> 00:25:06,610
You're saying, actually,
this is really interesting.

526
00:25:06,610 --> 00:25:08,820
We definitely have ways of using this.

527
00:25:08,820 --> 00:25:10,320
We want to know more about it.

528
00:25:12,940 --> 00:25:15,710
- Fantastic, I was just
going to say, we also

529
00:25:15,710 --> 00:25:19,560
need the individual accountability
in this trust version

530
00:25:19,560 --> 00:25:21,419
which then we got with
self-sovereign identity

531
00:25:21,420 --> 00:25:25,220
which makes me so happy
that we had that as a topic.

532
00:25:25,220 --> 00:25:29,200
So back to confidential
computing, as a big, big pictures

533
00:25:29,200 --> 00:25:31,270
of hosts, our specific,
how does a customer use it?

534
00:25:31,270 --> 00:25:32,850
How does a cloud provider use it?

535
00:25:32,850 --> 00:25:34,649
How does an end user use it?

536
00:25:34,650 --> 00:25:35,810
What are the use cases there?

537
00:25:35,810 --> 00:25:37,480
So big picture.

538
00:25:37,480 --> 00:25:40,830
What is one piece of
information or maybe two

539
00:25:40,830 --> 00:25:43,210
if you're all moving quickly

540
00:25:43,210 --> 00:25:46,280
that you feel would be essential
for someone that is new

541
00:25:46,280 --> 00:25:49,020
to confidential computing,
maybe it's a technical tidbit.

542
00:25:49,020 --> 00:25:51,750
Maybe it's something a little
bit longer what's brandnew

543
00:25:51,750 --> 00:25:53,880
and or someone for someone brand new

544
00:25:53,880 --> 00:25:56,883
what would you like to
share with them, Nelly.

545
00:25:59,870 --> 00:26:03,189
- One thing about trusted
execution environment,

546
00:26:03,190 --> 00:26:06,780
I guess was all this
cave to normal humans.

547
00:26:06,780 --> 00:26:11,780
Because it was all of us part
of very technical conversation

548
00:26:12,070 --> 00:26:14,370
around trusted platform modules,

549
00:26:14,370 --> 00:26:19,209
OR, TEEO again, not pointing
to our, by any means

550
00:26:19,210 --> 00:26:23,720
but it was always kind of, when we talking

551
00:26:23,720 --> 00:26:27,360
about TEEs as in normal
human started to fade away,

552
00:26:27,360 --> 00:26:30,459
because it's becoming so
complicated so quickly.

553
00:26:30,460 --> 00:26:34,760
So my one bit one month
tips that I would give

554
00:26:34,760 --> 00:26:37,280
to every single customer
when you're talking

555
00:26:37,280 --> 00:26:41,110
about confidential computing,
think about simplicity.

556
00:26:41,110 --> 00:26:45,760
Think about ease of use
performance and scale.

557
00:26:45,760 --> 00:26:48,956
And it's what we want to
bring to our customers.

558
00:26:48,956 --> 00:26:50,659
We want to give them opportunity

559
00:26:50,660 --> 00:26:53,270
do not say anything about
confidential computing

560
00:26:53,270 --> 00:26:58,270
utilizing it very easily for
every single sayings that say

561
00:26:58,670 --> 00:27:02,590
feel comfortable to share
the data, to share the algos,

562
00:27:02,590 --> 00:27:05,350
to share both of them with is anyone.

563
00:27:05,350 --> 00:27:08,409
It can be cloud providers, it
can be hosted environments.

564
00:27:08,410 --> 00:27:11,400
It can be again, social media platform.

565
00:27:11,400 --> 00:27:16,400
So there is a necessity for
us to think about usability

566
00:27:16,450 --> 00:27:20,130
because without usability,
there's no security

567
00:27:20,980 --> 00:27:25,500
So usability, performance, and scale.

568
00:27:25,500 --> 00:27:27,660
And I think the goal for us

569
00:27:27,660 --> 00:27:31,570
for there's computing
consortium, to give it all

570
00:27:31,570 --> 00:27:34,352
to our customers, it's
about to be going after.

571
00:27:37,229 --> 00:27:39,766
Mike, you have the ticket.

572
00:27:39,766 --> 00:27:42,230
- We, often say follow the money.

573
00:27:42,230 --> 00:27:45,550
I watched too many crime
programs not have picked

574
00:27:45,550 --> 00:27:48,560
up on that in this context,
I'd say, follow the trust.

575
00:27:48,560 --> 00:27:52,129
So, what entities, what
components need to trust?

576
00:27:52,130 --> 00:27:54,370
What other pieces of a system to do what?

577
00:27:54,370 --> 00:27:56,070
What, are those press relationships?

578
00:27:56,070 --> 00:27:57,929
And we need to be caring about those.

579
00:27:57,930 --> 00:28:00,670
What assurances can we gather through

580
00:28:00,670 --> 00:28:03,890
to the design or runtime
mechanisms that we're using?

581
00:28:03,890 --> 00:28:06,750
We haven't talked about
a key word actually

582
00:28:06,750 --> 00:28:09,440
in confidential meeting,
which is attestation.

583
00:28:09,440 --> 00:28:13,140
So attestation is about how
you can gain assurances as

584
00:28:13,140 --> 00:28:16,110
to whether what you're doing
is what you expect to be doing

585
00:28:16,110 --> 00:28:17,080
running, where it should be

586
00:28:17,080 --> 00:28:18,935
et cetera, et cetera, et cetera.

587
00:28:18,935 --> 00:28:21,669
So my technical tip is
learn about attestation.

588
00:28:21,670 --> 00:28:24,550
If you're, if you're a
low level person, then,

589
00:28:24,550 --> 00:28:25,649
learn all the protocol stuff.

590
00:28:25,650 --> 00:28:27,797
But if you're not learn
what it can give you

591
00:28:27,797 --> 00:28:30,320
and what it, can't, how it works for you

592
00:28:30,320 --> 00:28:31,720
how it works against you.

593
00:28:31,720 --> 00:28:35,190
Because until you understand attestation,

594
00:28:35,190 --> 00:28:36,700
that at least at the high level

595
00:28:36,700 --> 00:28:39,430
you can't understand
confidential computing

596
00:28:40,290 --> 00:28:42,322
- Ava, thoughts?

597
00:28:44,360 --> 00:28:46,080
- Yeah, I kind of mentioned this earlier

598
00:28:46,080 --> 00:28:48,860
but I think the biggest
takeaway is that trust today

599
00:28:48,860 --> 00:28:51,560
between businesses is
represented in contracts.

600
00:28:51,560 --> 00:28:53,903
And those often vary between companies.

601
00:28:55,560 --> 00:28:58,399
We're, assuming that
they're actually gonna get

602
00:28:58,400 --> 00:28:59,470
properly implemented.

603
00:28:59,470 --> 00:29:02,347
And this has the opportunity
to really change that

604
00:29:02,347 --> 00:29:05,020
and to invert our trust model

605
00:29:05,020 --> 00:29:07,540
when we are outsourcing computing

606
00:29:07,540 --> 00:29:08,730
that's fundamentally what a cloud is.

607
00:29:08,730 --> 00:29:10,610
Or you're outsourcing
that to somebody else.

608
00:29:10,610 --> 00:29:11,939
And you're trusting them.

609
00:29:11,940 --> 00:29:14,070
This lets us invert that model

610
00:29:14,070 --> 00:29:16,200
but it's not gonna be a magic bullet.

611
00:29:16,200 --> 00:29:18,500
It's not going to solve
all your security problems.

612
00:29:18,500 --> 00:29:20,900
Think of this as one more
layer of Swiss cheese.

613
00:29:22,220 --> 00:29:24,840
- Can I add one additional
point before Mike.

614
00:29:24,840 --> 00:29:26,360
- Jump on it.

615
00:29:26,360 --> 00:29:27,760
- Before Mike, yes go Nelly.

616
00:29:29,130 --> 00:29:29,983
- So it's fine.

617
00:29:29,983 --> 00:29:33,483
I'm trying to give you an
opportunity to answer maybe both.

618
00:29:34,350 --> 00:29:36,449
As attestation is absolutely must

619
00:29:36,450 --> 00:29:37,800
for confidential computing.

620
00:29:37,800 --> 00:29:41,389
It's a very how we well prove to you

621
00:29:41,390 --> 00:29:45,040
our consumer or our customer
is that you actually

622
00:29:45,040 --> 00:29:47,889
writing in confidential environments.

623
00:29:47,890 --> 00:29:50,670
But what I'm also born in a little bit

624
00:29:50,670 --> 00:29:55,380
at attestation exactly as it,
was very complicated topic

625
00:29:55,380 --> 00:29:58,060
a vendor talking about
protocols, but they're talking

626
00:29:58,060 --> 00:30:01,030
about claims and all of
this wonderful world.

627
00:30:01,030 --> 00:30:02,540
It's super confusing.

628
00:30:02,540 --> 00:30:05,139
I think we as confidential
computing consortium

629
00:30:05,140 --> 00:30:06,980
and we started working on that.

630
00:30:06,980 --> 00:30:10,520
We form specific dedicated seek

631
00:30:10,520 --> 00:30:13,500
seek attestation to bring usability,

632
00:30:13,500 --> 00:30:16,637
to bring clarity to this efforts

633
00:30:16,637 --> 00:30:20,129
and help our customers and all of us.

634
00:30:20,130 --> 00:30:23,750
To arrive to the survey, how
customers would be able to

635
00:30:23,750 --> 00:30:26,560
validate confidential
computing environments.

636
00:30:26,560 --> 00:30:28,531
Because there's different silicones

637
00:30:28,531 --> 00:30:29,860
there's different platforms.

638
00:30:29,860 --> 00:30:34,709
We have different security
model for attestation.

639
00:30:34,710 --> 00:30:38,060
So it's time for us to
figure out how to unify that.

640
00:30:38,060 --> 00:30:41,899
So customers don't have
to, again really learn

641
00:30:41,900 --> 00:30:45,503
every single variant
that back to you Mike.

642
00:30:46,637 --> 00:30:48,370
- I'm not gonna add
anything while we've got

643
00:30:48,370 --> 00:30:49,600
we don't have a huge amount of time.

644
00:30:49,600 --> 00:30:52,723
Let's just carry on
with loads of it ready.

645
00:30:53,682 --> 00:30:54,670
(laughs)

646
00:30:54,670 --> 00:30:55,993
- Aeva did you want to add a bite?

647
00:30:55,993 --> 00:30:58,669
- A real quick bite to
what Nelly just said

648
00:30:58,670 --> 00:31:02,220
we've got the attestation
working group in the CCC.

649
00:31:02,220 --> 00:31:06,220
There's also a whole bunch
of projects in the CNCF

650
00:31:06,220 --> 00:31:10,110
that are trying to build
in or have already built in

651
00:31:10,110 --> 00:31:11,800
TPMS and HSM.

652
00:31:11,800 --> 00:31:13,600
And we'd love to see and collaborate

653
00:31:13,600 --> 00:31:17,713
with those projects or others
to add TEE support as well.

654
00:31:18,758 --> 00:31:19,770
That's all.

655
00:31:19,770 --> 00:31:22,129
- Cool, so we've got sort
of the, what we can do today

656
00:31:22,130 --> 00:31:24,370
what you need to learn
to get started on this.

657
00:31:24,370 --> 00:31:28,669
Now let's jump to the
future, because the future

658
00:31:28,670 --> 00:31:31,970
is what we all love to
look at as technologists.

659
00:31:31,970 --> 00:31:33,940
And of course Aeva teased us with the,

660
00:31:33,940 --> 00:31:37,410
this isn't a confidential
computing, isn't a silver bullet.

661
00:31:37,410 --> 00:31:39,060
So we now know that.

662
00:31:39,060 --> 00:31:42,190
So what in five to 10
years does confidential

663
00:31:42,190 --> 00:31:43,470
computing look like?

664
00:31:43,470 --> 00:31:45,780
And what's it not gonna solve?

665
00:31:45,780 --> 00:31:47,960
Let's see, eeny, meeny, miny, Moe.

666
00:31:47,960 --> 00:31:50,260
Why don't we start with Mike?

667
00:31:50,260 --> 00:31:53,010
- Yeah, well flying cars,
flying cars is always

668
00:31:53,010 --> 00:31:54,460
the answer to five, 10 years.

669
00:31:55,950 --> 00:31:58,620
- But is confidential
computing going to make.

670
00:31:58,620 --> 00:31:59,453
- No, I don't think so.

671
00:31:59,453 --> 00:32:04,453
So, I think that what
I hope is that the box

672
00:32:04,470 --> 00:32:07,050
that says security, when you're building

673
00:32:07,050 --> 00:32:09,080
ratification the box that says security

674
00:32:09,080 --> 00:32:11,350
will default to ticked,

675
00:32:11,350 --> 00:32:13,000
when we're building parts of the system.

676
00:32:13,000 --> 00:32:15,770
I think instead of saying,
you know, Oh, we need to turn

677
00:32:15,770 --> 00:32:18,610
on security for this
component of the system.

678
00:32:18,610 --> 00:32:21,000
We're moving toward a world
where developers will be

679
00:32:21,000 --> 00:32:24,870
required to explain, to make
the case for why they want to

680
00:32:24,870 --> 00:32:28,389
turn a security off for
a particular component.

681
00:32:28,390 --> 00:32:30,940
Maybe for performance
reasons, whatever it will be.

682
00:32:30,940 --> 00:32:33,500
Confidential computing, won't
fix human error and it doesn't

683
00:32:33,500 --> 00:32:34,950
fix the supply chain either.

684
00:32:34,950 --> 00:32:38,893
It does offer the ability,
in the future, it should,

685
00:32:39,900 --> 00:32:42,860
to make one or more specific links

686
00:32:42,860 --> 00:32:46,617
in that supply chain much, much stronger.

687
00:32:46,617 --> 00:32:49,820
And I think that that's having it

688
00:32:49,820 --> 00:32:51,733
as default is where we want to be.

689
00:32:55,510 --> 00:32:57,330
- Aeva, is up next in our prep

690
00:32:57,330 --> 00:33:00,429
but I know is gonna want to
say something in a moment.

691
00:33:00,430 --> 00:33:02,970
So it doesn't hold Nelly?

692
00:33:02,970 --> 00:33:03,967
- It feels cold.

693
00:33:03,967 --> 00:33:06,384
(chartering)

694
00:33:07,220 --> 00:33:10,910
- So I really wanna sort of
double down on this office

695
00:33:10,910 --> 00:33:13,380
supply chain comments
that Mike was just making.

696
00:33:13,380 --> 00:33:15,180
This is a, it's a really big initiative.

697
00:33:15,180 --> 00:33:18,067
I've been sort of sticking
my fingers in the open SSF

698
00:33:18,067 --> 00:33:21,080
the open sort of security foundation,

699
00:33:21,080 --> 00:33:22,810
looking at some of those things.

700
00:33:22,810 --> 00:33:23,941
I think first of all, competence competing

701
00:33:23,941 --> 00:33:28,630
is not gonna address that really directly,

702
00:33:28,630 --> 00:33:32,110
but it does indirectly through
attestation and signing.

703
00:33:32,110 --> 00:33:36,020
And the ability to sort of enforce that

704
00:33:36,020 --> 00:33:39,570
the application you built
has not changed in any way

705
00:33:39,570 --> 00:33:41,139
as it moves through different environments

706
00:33:41,140 --> 00:33:41,973
and be the container image

707
00:33:41,973 --> 00:33:43,379
moves between different registries

708
00:33:43,380 --> 00:33:44,670
is deployed in different environments.

709
00:33:44,670 --> 00:33:48,240
Like if you can enforce that
integrity at a hardware level

710
00:33:48,240 --> 00:33:50,100
it helps with software
supply chain security.

711
00:33:50,100 --> 00:33:51,100
It doesn't solve it.

712
00:33:53,200 --> 00:33:56,030
So holistic practices,
holistic security practices

713
00:33:56,030 --> 00:33:57,930
are gonna continue to be important.

714
00:33:57,930 --> 00:33:59,970
No matter we have flying cars or not.

715
00:33:59,970 --> 00:34:02,370
And part of that is ensuring
that you have a good diversity

716
00:34:02,370 --> 00:34:06,370
program, that everything, all
people feel welcome and safe

717
00:34:06,370 --> 00:34:08,799
in your company or your project.

718
00:34:08,800 --> 00:34:10,389
Because, if people don't feel safe to

719
00:34:10,389 --> 00:34:12,290
be themselves, that is a security risk

720
00:34:13,310 --> 00:34:14,719
- To be themselves as well

721
00:34:14,719 --> 00:34:17,370
as to step up and say,
there's a problem here.

722
00:34:17,370 --> 00:34:20,330
If someone is feeling
uncomfortable in an organization

723
00:34:20,330 --> 00:34:22,429
they're less likely to do the right thing

724
00:34:22,429 --> 00:34:25,020
even though they may
feel that they need to.

725
00:34:25,020 --> 00:34:29,143
Nelly I know you had an opinion
about Mike because you were.

726
00:34:32,489 --> 00:34:35,533
- I think again, let's,
answer, your question first.

727
00:34:36,530 --> 00:34:40,370
In five years, I hope it
will not take us 10 years

728
00:34:40,370 --> 00:34:43,940
five years, we will see
that confidential computing

729
00:34:43,940 --> 00:34:45,610
it'll become utility.

730
00:34:45,610 --> 00:34:46,760
And what I mean

731
00:34:46,760 --> 00:34:50,940
by that every single cloud
provider in the world.

732
00:34:50,940 --> 00:34:52,400
And I'm sure it will be more

733
00:34:52,400 --> 00:34:56,159
than few will be offering
the managed services only

734
00:34:56,159 --> 00:34:58,600
normally in confidential environments

735
00:34:58,600 --> 00:35:02,630
and to accomplish that
they cannot sacrifice

736
00:35:02,630 --> 00:35:04,910
in usability scale and with performance,

737
00:35:04,910 --> 00:35:07,250
and this is what am trying to do.

738
00:35:07,250 --> 00:35:10,750
Again be a little bit more assertive.

739
00:35:10,750 --> 00:35:12,740
Performance has to be resolved.

740
00:35:12,740 --> 00:35:17,040
We should be able to
demonstrate the performance

741
00:35:17,040 --> 00:35:20,740
is not the way while you
build disabled security.

742
00:35:20,740 --> 00:35:25,500
But again, talking back to
why confidential computing

743
00:35:25,500 --> 00:35:29,995
will be really everywhere.

744
00:35:29,995 --> 00:35:34,995
The analogy I will usually
use it's HTTPS five years ago,

745
00:35:35,520 --> 00:35:40,460
we all believes that HTTPS is
probably for a few websites,

746
00:35:40,460 --> 00:35:41,930
because you'll have to figure out

747
00:35:41,930 --> 00:35:44,500
how to deal with PKI and certificate

748
00:35:44,500 --> 00:35:47,970
and who for God's sake
will understand PKI.

749
00:35:47,970 --> 00:35:52,140
So now five years later, if
your website doesn't have

750
00:35:52,140 --> 00:35:55,990
certificate and doesn't offer HTTPS,

751
00:35:55,990 --> 00:35:58,910
it's becoming the exception from the rule.

752
00:35:58,910 --> 00:36:01,740
It's where confidential
computing will become.

753
00:36:01,740 --> 00:36:05,790
And this is why I act
on actually my exponent.

754
00:36:05,790 --> 00:36:08,120
That may be some use cases without fee.

755
00:36:08,120 --> 00:36:11,880
Maybe are some again, sayings
that you need to opt out,

756
00:36:11,880 --> 00:36:16,880
but by definition, I'd say
99.99% of customers that occupied

757
00:36:19,310 --> 00:36:22,200
cloud as they'll bring
the workloads to the cloud

758
00:36:22,200 --> 00:36:23,866
would use confidential computing.

759
00:36:23,867 --> 00:36:28,270
But every single thing they
do and today's conversation

760
00:36:28,270 --> 00:36:29,470
is those customers.

761
00:36:29,470 --> 00:36:32,602
And they asking one
basic question, why not?

762
00:36:33,722 --> 00:36:37,050
Is there any these, I don't
want to use confidential.

763
00:36:37,050 --> 00:36:40,942
And I'm saying sort of zero
reason if you can, please do.

764
00:36:40,942 --> 00:36:43,450
And this is what we
will see in five years.

765
00:36:43,450 --> 00:36:45,109
- I think that's what makes it different

766
00:36:45,110 --> 00:36:48,960
from all of the work I did
in previous like hardware,

767
00:36:48,960 --> 00:36:53,030
infrastructure, security
around TPMS and remote boot.

768
00:36:53,030 --> 00:36:54,960
The audience for that was so small.

769
00:36:54,960 --> 00:36:56,830
And the tech was so hard to use.

770
00:36:56,830 --> 00:36:58,430
This is so much easier

771
00:36:58,430 --> 00:37:02,040
and will be so much more
widely available and important.

772
00:37:02,040 --> 00:37:03,560
- Yep.

773
00:37:03,560 --> 00:37:05,020
- This has been fantastic.

774
00:37:05,020 --> 00:37:08,060
We have like a minute and a half left.

775
00:37:08,060 --> 00:37:11,180
Is there one thing that we didn't mention

776
00:37:12,318 --> 00:37:16,580
that you leave as an exercise
to our readers to go look up.

777
00:37:17,950 --> 00:37:19,839
- I've got a quick one, which is that

778
00:37:19,840 --> 00:37:22,400
the confidential computer consortium

779
00:37:22,400 --> 00:37:25,070
is all those meetings open.

780
00:37:25,070 --> 00:37:27,000
Come along, whether you're an end user,

781
00:37:27,000 --> 00:37:28,030
whether you're a techie.

782
00:37:28,030 --> 00:37:31,020
You wanna get involved, in
sort of outreach and stuff,

783
00:37:31,020 --> 00:37:34,573
it's open, go to the, it's
a confidential computer.io.

784
00:37:35,670 --> 00:37:37,690
We'd love to have you don't need to pay

785
00:37:37,690 --> 00:37:39,360
to be a member just to attend meetings

786
00:37:39,360 --> 00:37:41,510
and have your voice heard, come along.

787
00:37:41,510 --> 00:37:42,950
Please would love to see.

788
00:37:42,950 --> 00:37:44,740
- Yeah, can I add one additional thing

789
00:37:44,740 --> 00:37:47,580
if its possible to that flying cars.

790
00:37:47,580 --> 00:37:50,210
'cause it would need
confidential computing.

791
00:37:50,210 --> 00:37:51,804
- Yes.

792
00:37:51,804 --> 00:37:54,204
- We need to know where
to go (indistinct) data.

793
00:37:55,890 --> 00:37:57,020
So forget about that.

794
00:37:57,020 --> 00:38:00,490
If you need flying cars, do you
need confidential computing?

795
00:38:00,490 --> 00:38:03,609
- I think we end there because we're gonna

796
00:38:03,610 --> 00:38:07,140
bring flying cars to secure flying parts.

797
00:38:07,140 --> 00:38:08,930
Let's be clear to the world

798
00:38:08,930 --> 00:38:12,049
in five to 10 years through
confidential computing.

799
00:38:12,050 --> 00:38:14,770
Have a great rest of the
event thank you three

800
00:38:14,770 --> 00:38:16,233
for joining me on this channel

801
00:38:16,233 --> 00:38:18,077
- Thank you Sarah.
- It's been great fun.

802
00:38:19,475 --> 00:38:21,142
- Thank you so much.

