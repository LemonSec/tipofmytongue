1
00:00:01,770 --> 00:00:02,950
- Hi everyone.

2
00:00:02,950 --> 00:00:03,977
This is the session,

3
00:00:03,977 --> 00:00:05,370
"We're All in this Together."

4
00:00:05,370 --> 00:00:08,670
It's a session about
"Technology Abuse and Design."

5
00:00:08,670 --> 00:00:10,170
My name is Erica Olsen.

6
00:00:10,170 --> 00:00:11,960
I am the Director of
the Safety Net Project

7
00:00:11,960 --> 00:00:14,120
at the National Network
to End Domestic Violence.

8
00:00:14,120 --> 00:00:16,530
And I'll be moderating this panel.

9
00:00:16,530 --> 00:00:19,070
Before we start I'm gonna
turn it over to our panelists

10
00:00:19,070 --> 00:00:20,970
so they can each introduce themselves.

11
00:00:22,010 --> 00:00:23,900
- Hi everyone. My name
is Jennifer Landhuis.

12
00:00:23,900 --> 00:00:25,540
I'm the Director of SPARC which is the

13
00:00:25,540 --> 00:00:28,380
Stalking Prevention Awareness
and Resource Center.

14
00:00:28,380 --> 00:00:30,119
We are a project of Aequitas.

15
00:00:30,120 --> 00:00:32,650
And we provide training
and technical assistance

16
00:00:32,650 --> 00:00:35,699
all over of the U.S. on
the issue of stalking.

17
00:00:35,700 --> 00:00:37,190
Hi, my name is Natalie Dolci.

18
00:00:37,190 --> 00:00:39,919
I'm a Senior Violence Prevention
and Response Specialist

19
00:00:39,920 --> 00:00:42,430
at the University of
Washington Safe Campus

20
00:00:42,430 --> 00:00:43,540
and a Co-Founder

21
00:00:43,540 --> 00:00:46,129
of the Technology-Enabled
Coercive Control Working Group

22
00:00:46,130 --> 00:00:47,790
in Seattle.

23
00:00:47,790 --> 00:00:49,519
- Hi everybody, I'm Tom Ristenpart.

24
00:00:49,520 --> 00:00:51,550
I'm a Associate Professor at Cornell Tech

25
00:00:51,550 --> 00:00:53,919
and Cornell University
in Computer Science.

26
00:00:53,920 --> 00:00:55,870
And I co-lead our research lab

27
00:00:55,870 --> 00:00:58,129
on understanding technology abuse

28
00:00:58,130 --> 00:00:59,600
and intimate partner violence,

29
00:00:59,600 --> 00:01:02,180
as well as help run our
Clinic to End Tech Abuse

30
00:01:02,180 --> 00:01:07,170
which does direct survivor
support on technology issues.

31
00:01:07,170 --> 00:01:08,640
- Great, thank you all so much.

32
00:01:08,640 --> 00:01:09,810
And thank you for being here.

33
00:01:09,810 --> 00:01:13,180
I'm really excited to have
this discussion today.

34
00:01:13,180 --> 00:01:14,630
So for those of you joining us

35
00:01:14,630 --> 00:01:16,899
this session will provide an overview

36
00:01:16,900 --> 00:01:19,290
of technology abuse and the importance

37
00:01:19,290 --> 00:01:22,550
of trauma-informed technology
design for survivors

38
00:01:22,550 --> 00:01:24,810
to increase safety and privacy.

39
00:01:24,810 --> 00:01:26,590
This will be a discussion
between advocates,

40
00:01:26,590 --> 00:01:29,020
technologists, and
clinic service providers.

41
00:01:29,020 --> 00:01:32,080
For those who are not as
familiar with anti-abuse work,

42
00:01:32,080 --> 00:01:33,929
I just want to take a
quick moment and provide

43
00:01:33,930 --> 00:01:37,690
some context and definitions
for words that we'll be using.

44
00:01:37,690 --> 00:01:41,350
We may use the terms
"victim" and "survivor"

45
00:01:41,350 --> 00:01:43,640
interchangeably throughout this talk.

46
00:01:43,640 --> 00:01:45,700
This session is focusing on victims

47
00:01:45,700 --> 00:01:47,250
of intimate partner abuse

48
00:01:47,250 --> 00:01:50,450
which includes much more
than physical violence.

49
00:01:50,450 --> 00:01:53,720
Intimate partner abuse
includes a number of behaviors

50
00:01:53,720 --> 00:01:55,940
that are perpetrated by someone

51
00:01:55,940 --> 00:01:59,920
to maintain power and
control over another person.

52
00:01:59,920 --> 00:02:03,870
Abuse may consist of financial, emotional,

53
00:02:03,870 --> 00:02:06,737
sexual or physical abuse.

54
00:02:06,737 --> 00:02:10,229
And it can be a pattern
of stalking and harassment

55
00:02:10,229 --> 00:02:14,100
of coercive control and what
we'll be focusing on today,

56
00:02:14,100 --> 00:02:16,730
technology, technological abuse.

57
00:02:16,730 --> 00:02:19,560
On average, more than one in three women,

58
00:02:19,560 --> 00:02:21,440
and one in four men in the US

59
00:02:21,440 --> 00:02:23,720
will experience rape, physical violence,

60
00:02:23,720 --> 00:02:26,220
and/or stalking by an intimate partner.

61
00:02:26,220 --> 00:02:27,830
Professionals who work with survivors

62
00:02:27,830 --> 00:02:30,640
are usually called Advocates
or Victim Service Providers.

63
00:02:30,640 --> 00:02:33,670
And you'll hear those
terms frequently as well.

64
00:02:33,670 --> 00:02:34,940
During the session,

65
00:02:34,940 --> 00:02:37,750
we will discuss best
practices for incorporating

66
00:02:37,750 --> 00:02:41,170
survivor-centered approaches
to technological design,

67
00:02:41,170 --> 00:02:45,549
anti-abuse testing and
related policy development.

68
00:02:45,550 --> 00:02:47,300
So with that is our intro,

69
00:02:47,300 --> 00:02:50,000
so everyone has a basic understanding

70
00:02:50,000 --> 00:02:51,770
of what we're talking about today.

71
00:02:51,770 --> 00:02:54,810
We'll move into defining these
issues a little bit more.

72
00:02:54,810 --> 00:02:58,570
I'll dive in by providing a
brief overview of tech abuse,

73
00:02:58,570 --> 00:03:00,440
some examples of what this looks like,

74
00:03:00,440 --> 00:03:04,380
and the importance of security
for survivors of abuse.

75
00:03:04,380 --> 00:03:07,670
Now, as technology has become
woven into our daily lives

76
00:03:07,670 --> 00:03:09,730
it has also become a common tactic

77
00:03:09,730 --> 00:03:11,769
within intimate partner abuse.

78
00:03:11,770 --> 00:03:15,820
Stalking, sexual violence,
and online harassment.

79
00:03:15,820 --> 00:03:18,090
Abusers misuse technology in order

80
00:03:18,090 --> 00:03:21,920
to commit and cover up
those behaviors and crimes.

81
00:03:21,920 --> 00:03:24,299
We see technology misused and used

82
00:03:24,300 --> 00:03:28,280
to monitor, track,
surveil, harass, threaten,

83
00:03:28,280 --> 00:03:30,420
and isolate survivors.

84
00:03:30,420 --> 00:03:33,309
We also see it misused
to impersonate survivors

85
00:03:33,310 --> 00:03:35,140
and damage a reputations,

86
00:03:35,140 --> 00:03:38,750
often impacting their employment
and their relationships.

87
00:03:38,750 --> 00:03:40,240
As technology evolves,

88
00:03:40,240 --> 00:03:43,530
we see the tactics of a misuse evolve too.

89
00:03:43,530 --> 00:03:46,020
An example of this is somebody accessing

90
00:03:46,020 --> 00:03:51,020
IoT home devices remotely to
control thermostats and lights

91
00:03:51,380 --> 00:03:53,850
to purposely impact the victim's bills,

92
00:03:53,850 --> 00:03:58,650
using technology as a tactic
to further financial abuse.

93
00:03:58,650 --> 00:03:59,670
On top of this,

94
00:03:59,670 --> 00:04:02,019
the vast amount of data

95
00:04:02,020 --> 00:04:05,620
that is available about all of us online,

96
00:04:05,620 --> 00:04:09,730
it can continually compromises
survivors' privacy.

97
00:04:09,730 --> 00:04:11,738
For victims of abuse,

98
00:04:11,738 --> 00:04:16,250
their privacy is closely
linked to their safety.

99
00:04:16,250 --> 00:04:19,670
It is critical for technologists
and security professionals

100
00:04:19,670 --> 00:04:22,180
to understand how
technologies can facilitate

101
00:04:22,180 --> 00:04:26,720
stalking, abuse, coercive
control, harassment, and violence

102
00:04:26,720 --> 00:04:29,140
both online and offline.

103
00:04:29,140 --> 00:04:32,240
Current anti-abuse strategies
don't always fully account

104
00:04:32,240 --> 00:04:34,800
for the dynamics of intimate partner abuse

105
00:04:34,800 --> 00:04:36,890
and the serious impact that tech abuse

106
00:04:36,890 --> 00:04:38,810
can have on survivors.

107
00:04:38,810 --> 00:04:42,070
Strategies need to include
robust, survivor-centric,

108
00:04:42,070 --> 00:04:45,659
and trauma-informed approaches
to technology design

109
00:04:45,660 --> 00:04:48,420
and relevant policy development.

110
00:04:48,420 --> 00:04:50,250
So with that, I'm gonna
turn it over to Jen

111
00:04:50,250 --> 00:04:53,110
to talk a little bit
about stalking and tech

112
00:04:53,110 --> 00:04:55,343
as a tool for spying and monitoring.

113
00:04:56,200 --> 00:04:57,380
- Thanks Erica.

114
00:04:57,380 --> 00:05:01,010
So we kind of see survivors
present in two different ways

115
00:05:01,010 --> 00:05:02,789
when it comes to tech abuse.

116
00:05:02,790 --> 00:05:05,500
We have, on one hand
we've got the survivors

117
00:05:05,500 --> 00:05:08,930
whose partner might be misusing
a legitimate technology.

118
00:05:08,930 --> 00:05:11,360
So one that wasn't designed to surveil

119
00:05:11,360 --> 00:05:12,750
or monitor somebody else,

120
00:05:12,750 --> 00:05:14,750
but being manipulated to do so.

121
00:05:14,750 --> 00:05:17,660
So for example, things like Find My iPhone

122
00:05:17,660 --> 00:05:20,290
or some other types of location apps.

123
00:05:20,290 --> 00:05:24,180
So what we know is like
many aspects of technology

124
00:05:24,180 --> 00:05:27,010
they're often designed
in a way that assumes

125
00:05:27,010 --> 00:05:29,030
that people who share an account together

126
00:05:29,030 --> 00:05:32,330
or share a home together
are safe with each other.

127
00:05:32,330 --> 00:05:35,359
And so one of the aspects that
we have to be thinking about

128
00:05:35,360 --> 00:05:39,100
is for those that want to try
to opt out of that technology.

129
00:05:39,100 --> 00:05:40,130
So say for instance,

130
00:05:40,130 --> 00:05:43,290
we have a victim whose partner
might be monitoring them

131
00:05:43,290 --> 00:05:45,430
through something like Find my iPhone.

132
00:05:45,430 --> 00:05:47,230
To turn that technology off,

133
00:05:47,230 --> 00:05:49,220
they might not even know how to do so.

134
00:05:49,220 --> 00:05:51,960
They might not have permissions to do so.

135
00:05:51,960 --> 00:05:53,260
And if they do so,

136
00:05:53,260 --> 00:05:55,240
it would be obvious to the other person.

137
00:05:55,240 --> 00:05:58,250
So that might increase the likelihood

138
00:05:58,250 --> 00:06:01,380
that harm or danger could
happen to them as well.

139
00:06:01,380 --> 00:06:04,140
So you have those examples of
those who are being monitored

140
00:06:04,140 --> 00:06:06,030
through a legitimate technology

141
00:06:06,030 --> 00:06:08,059
and then that's just being misused

142
00:06:08,060 --> 00:06:09,650
by the stalker or the offender.

143
00:06:09,650 --> 00:06:12,030
And then you have the other category

144
00:06:12,030 --> 00:06:14,250
where people are being
monitored or surveilled

145
00:06:14,250 --> 00:06:16,540
through technology that's designed

146
00:06:16,540 --> 00:06:18,380
for those particular purposes.

147
00:06:18,380 --> 00:06:21,380
So what we sometimes see
is things like stalkerware.

148
00:06:21,380 --> 00:06:24,380
So for instance, we just
got a call last week

149
00:06:24,380 --> 00:06:27,790
from somebody who had
left an abusive partner,

150
00:06:27,790 --> 00:06:30,200
had gone to stay with a family member.

151
00:06:30,200 --> 00:06:33,260
And the call came into an advocacy center

152
00:06:33,260 --> 00:06:35,469
saying that they were certain

153
00:06:35,470 --> 00:06:37,720
that there were some kind of way

154
00:06:37,720 --> 00:06:40,310
that the offender was
"hacking into their phone."

155
00:06:40,310 --> 00:06:42,310
That's the language
that we hear oftentimes

156
00:06:42,310 --> 00:06:43,970
from victims and survivors.

157
00:06:43,970 --> 00:06:46,330
And in this case, the victim
would show up certain places

158
00:06:46,330 --> 00:06:48,190
and the offender would show up there

159
00:06:48,190 --> 00:06:50,370
often enough that it wasn't a coincidence.

160
00:06:50,370 --> 00:06:53,390
And the victim also
believed that this offender

161
00:06:53,390 --> 00:06:55,080
was reading the text messages

162
00:06:55,080 --> 00:06:57,770
that she was sending back
and forth to her family.

163
00:06:57,770 --> 00:07:00,090
So she had reached out
to an advocacy agency

164
00:07:00,090 --> 00:07:01,330
at first for help.

165
00:07:01,330 --> 00:07:02,719
And what we have to understand

166
00:07:02,720 --> 00:07:06,290
is there's not a ton of
training and education

167
00:07:06,290 --> 00:07:07,928
for folks who might be going on

168
00:07:07,928 --> 00:07:11,410
and advocating on behalf of survivors.

169
00:07:11,410 --> 00:07:14,250
And so you might have
individuals who aren't equipped

170
00:07:14,250 --> 00:07:16,360
to be able to deal with
these types of issues.

171
00:07:16,360 --> 00:07:19,650
Then she was referred to
a law enforcement agency

172
00:07:19,650 --> 00:07:21,120
who told her essentially

173
00:07:21,120 --> 00:07:22,610
if she didn't want to report a crime

174
00:07:22,610 --> 00:07:24,831
which she would, did not,

175
00:07:24,831 --> 00:07:27,130
basically there was nothing
they could do for her.

176
00:07:27,130 --> 00:07:30,290
So then her next strategy
was to try to reach out

177
00:07:30,290 --> 00:07:31,447
to the phone company to say,

178
00:07:31,447 --> 00:07:34,730
"I believe that somebody is
monitoring and surveilling me."

179
00:07:34,730 --> 00:07:36,080
She wasn't the account holder.

180
00:07:36,080 --> 00:07:37,419
So there was no way for her

181
00:07:37,420 --> 00:07:39,770
to change anything with the phone.

182
00:07:39,770 --> 00:07:41,859
And frankly, she faced disbelief

183
00:07:41,860 --> 00:07:43,967
by the phone company who said,

184
00:07:43,967 --> 00:07:45,950
"There's no way this
kind of stuff can happen.

185
00:07:45,950 --> 00:07:48,381
Are you sure it's not just
a coincidence?" et cetera.

186
00:07:48,381 --> 00:07:50,679
So we see survivors kind of fall

187
00:07:50,680 --> 00:07:52,110
into two different categories.

188
00:07:52,110 --> 00:07:55,030
Those that are being monitored
through legitimate use apps

189
00:07:55,030 --> 00:07:57,190
and then those who are being monitored

190
00:07:57,190 --> 00:07:59,900
through apps that are designed with,

191
00:07:59,900 --> 00:08:02,840
you know, not quite as
legitimate purposes.

192
00:08:02,840 --> 00:08:04,950
And then what we often see happen

193
00:08:04,950 --> 00:08:08,140
is regardless of what the technology is

194
00:08:08,140 --> 00:08:10,130
that survivors are facing roadblocks

195
00:08:10,130 --> 00:08:11,840
in trying to access help.

196
00:08:11,840 --> 00:08:14,560
They're facing roadblocks in privacy.

197
00:08:14,560 --> 00:08:16,450
They're facing roadblocks in people

198
00:08:16,450 --> 00:08:18,409
who are responding to them,

199
00:08:18,410 --> 00:08:21,236
so the criminal justice
system or advocacy agencies.

200
00:08:21,236 --> 00:08:24,190
And then roadblocks with
the tech companies on not,

201
00:08:24,190 --> 00:08:25,750
you know, if they try to access help

202
00:08:25,750 --> 00:08:27,010
or call somebody for help

203
00:08:27,010 --> 00:08:29,580
rarely are they met with
a voice at the other end.

204
00:08:29,580 --> 00:08:31,768
And so there's lots of different aspects.

205
00:08:31,768 --> 00:08:33,720
Not that we'll get to all of these

206
00:08:33,720 --> 00:08:35,409
but at least give you an idea

207
00:08:35,409 --> 00:08:37,313
of what we see happening in real time.

208
00:08:38,200 --> 00:08:39,820
- Thank you, Jen.

209
00:08:39,820 --> 00:08:43,320
I love the clear examples
and I think that's so helpful

210
00:08:43,320 --> 00:08:45,840
for people to wrap their
head around this issue.

211
00:08:45,840 --> 00:08:48,430
Natalie, could you talk a
little bit about your work

212
00:08:48,430 --> 00:08:50,693
and the campus intersection
and prevention?

213
00:08:51,610 --> 00:08:52,660
- Sure.

214
00:08:52,660 --> 00:08:55,819
Our office supports students, faculty,

215
00:08:55,820 --> 00:08:57,830
and staff at the University of Washington

216
00:08:57,830 --> 00:09:01,510
who are experiencing a
variety of safety concerns.

217
00:09:01,510 --> 00:09:03,360
When we're working on tech abuse

218
00:09:03,360 --> 00:09:06,500
we see a couple of
different threat models.

219
00:09:06,500 --> 00:09:09,710
We see the threat model
of folks who are targeted

220
00:09:09,710 --> 00:09:12,200
because they're an intimate
partner violence situation,

221
00:09:12,200 --> 00:09:14,990
a stalking situation, sexual harassment.

222
00:09:14,990 --> 00:09:17,690
We also see the threat
model of researchers

223
00:09:17,690 --> 00:09:20,030
who are targeted for ideological reason.

224
00:09:20,030 --> 00:09:22,140
Perhaps they study racial justice,

225
00:09:22,140 --> 00:09:23,040
climate justice,

226
00:09:23,040 --> 00:09:24,342
gender justice issues

227
00:09:24,342 --> 00:09:27,066
and are experiencing harassment and doxing

228
00:09:27,066 --> 00:09:29,653
as a result of that.

229
00:09:30,510 --> 00:09:32,120
There's a couple of things

230
00:09:32,120 --> 00:09:35,040
I'd like to uplift in
this discussion today

231
00:09:35,040 --> 00:09:37,290
which is that it's really paramount

232
00:09:37,290 --> 00:09:38,709
that we start to integrate

233
00:09:38,710 --> 00:09:40,740
the threat model of
intimate partner violence

234
00:09:40,740 --> 00:09:45,010
and to the design processes of technology.

235
00:09:45,010 --> 00:09:48,050
We need to consider coercive
control and potential abuse

236
00:09:48,050 --> 00:09:49,589
in product development.

237
00:09:49,590 --> 00:09:52,650
But also we need to
consider a response plan.

238
00:09:52,650 --> 00:09:55,660
I think particularly what
Jennifer was just talking about

239
00:09:55,660 --> 00:09:59,650
really highlights that when
people are trying to seek relief

240
00:09:59,650 --> 00:10:02,600
it seems that there hasn't
been much effort spent

241
00:10:02,600 --> 00:10:05,140
on what that response process looks like

242
00:10:05,140 --> 00:10:08,323
or the user experience for
those who've encountered harm.

243
00:10:09,300 --> 00:10:13,540
I think an example would
be a case study where

244
00:10:13,540 --> 00:10:17,410
a PhD student had an ex who
was sending intimate images

245
00:10:17,410 --> 00:10:20,360
to her department and her doctoral advisor

246
00:10:20,360 --> 00:10:22,083
using a spoofed email.

247
00:10:23,170 --> 00:10:25,300
He was obviously embarrassed

248
00:10:25,300 --> 00:10:27,130
in front of the doctoral advisor

249
00:10:27,130 --> 00:10:29,980
but also found other
professional consequences

250
00:10:29,980 --> 00:10:32,070
such as conferences she had signed up for

251
00:10:32,070 --> 00:10:33,960
were also targeted by this stalker

252
00:10:33,960 --> 00:10:36,113
and were asking her to
no longer participate.

253
00:10:37,250 --> 00:10:39,010
The far too common response path

254
00:10:39,010 --> 00:10:41,500
is that she is increasingly isolated.

255
00:10:41,500 --> 00:10:44,500
The email platform does
nothing to assist her

256
00:10:44,500 --> 00:10:47,530
and the perpetuation of
this crime that's occurring

257
00:10:47,530 --> 00:10:50,760
and that her doctoral
advisor felt alienated

258
00:10:50,760 --> 00:10:53,610
and no longer wanted
to meet with her alone

259
00:10:53,610 --> 00:10:57,960
feeling just nervous
about the whole situation.

260
00:10:57,960 --> 00:10:59,930
And I'd invite the audience to consider

261
00:10:59,930 --> 00:11:02,349
an alternative path where instead

262
00:11:02,350 --> 00:11:04,480
that same survivor was put in touch

263
00:11:04,480 --> 00:11:07,170
with specialized victim advocacy services

264
00:11:07,170 --> 00:11:11,780
that cooperated with the
university's CISO Office.

265
00:11:11,780 --> 00:11:15,137
Considering the opportunity
for an email platform

266
00:11:15,137 --> 00:11:18,339
that had a specially trained response team

267
00:11:18,340 --> 00:11:19,350
that could work with people

268
00:11:19,350 --> 00:11:21,550
who were experienced in
the unlawful distribution

269
00:11:21,550 --> 00:11:22,890
of their intimate images

270
00:11:22,890 --> 00:11:25,300
and prying for that behavior quickly

271
00:11:25,300 --> 00:11:28,030
and considering a conference
that had protocols in place

272
00:11:28,030 --> 00:11:30,350
to protect speakers and privacy

273
00:11:30,350 --> 00:11:34,180
rather than participating in
attrition or a brain drain.

274
00:11:34,180 --> 00:11:37,370
And I think that when we fail to consider

275
00:11:37,370 --> 00:11:39,000
appropriate prevention and response,

276
00:11:39,000 --> 00:11:41,890
we actually allow our
platforms and our workplaces

277
00:11:41,890 --> 00:11:44,260
to be weaponized by the abuser

278
00:11:44,260 --> 00:11:46,350
rather than supporting the survivor.

279
00:11:46,350 --> 00:11:49,990
And that there is a real
opportunity to flip the script

280
00:11:49,990 --> 00:11:52,330
and make those needed changes.

281
00:11:52,330 --> 00:11:53,867
- Thank you so much for all of that.

282
00:11:53,867 --> 00:11:56,652
And Tom, Leslie, would
you dive in and say,

283
00:11:56,653 --> 00:12:00,230
what does this mean for
designing threat models?

284
00:12:00,230 --> 00:12:01,140
- Yeah, yeah.

285
00:12:01,140 --> 00:12:04,060
It's a great question.

286
00:12:04,060 --> 00:12:04,892
It's a really hard one.

287
00:12:04,893 --> 00:12:06,700
So we've been doing research in this space

288
00:12:06,700 --> 00:12:09,020
for like maybe five or six years

289
00:12:09,020 --> 00:12:10,319
and definitely just want to echo

290
00:12:10,320 --> 00:12:13,310
some of the things that Jen
and Natalie were both saying,

291
00:12:13,310 --> 00:12:15,150
which then when we started talking

292
00:12:15,150 --> 00:12:17,550
to IPV support professionals,

293
00:12:17,550 --> 00:12:19,430
this issue of technology abuse came up

294
00:12:19,430 --> 00:12:21,120
over and over again.

295
00:12:21,120 --> 00:12:21,953
But at the same time,

296
00:12:21,953 --> 00:12:23,810
there were not a lot of technologists

297
00:12:23,810 --> 00:12:27,329
out there who could help and
assist with these issues.

298
00:12:27,330 --> 00:12:29,020
And certainly there was
a lot of frustration,

299
00:12:29,020 --> 00:12:31,410
palpable frustration with
how the tech industry,

300
00:12:31,410 --> 00:12:33,890
you know, tech companies specifically,

301
00:12:33,890 --> 00:12:35,850
we're not necessarily dealing well

302
00:12:35,850 --> 00:12:36,870
with the type of threat models

303
00:12:36,870 --> 00:12:40,853
that victim/survivors
are facing every day.

304
00:12:42,775 --> 00:12:43,608
And so when I think about this,

305
00:12:43,608 --> 00:12:46,040
I think there's a really
big landscape of things

306
00:12:46,040 --> 00:12:48,240
that we need to make progress on, right?

307
00:12:48,240 --> 00:12:51,250
One is how do we and
perhaps most urgently,

308
00:12:51,250 --> 00:12:53,730
how do we help survivors
in the short term, right?

309
00:12:53,730 --> 00:12:56,730
Dealing with the existing
technology landscape that we have.

310
00:12:57,710 --> 00:12:59,200
And for this,

311
00:12:59,200 --> 00:13:01,970
we need those type of tailored responses

312
00:13:01,970 --> 00:13:03,820
that Natalie was mentioning, right?

313
00:13:03,820 --> 00:13:07,350
I think having people have
very difficult situations.

314
00:13:07,350 --> 00:13:10,320
There is not a one-size-fits-all solution

315
00:13:10,320 --> 00:13:14,633
in terms of helping
out any, all survivors.

316
00:13:15,529 --> 00:13:16,362
And so we've been working a lot

317
00:13:16,362 --> 00:13:18,449
on this type of tailored
advocacy kind of model

318
00:13:18,450 --> 00:13:21,870
where you kind of understand
the situation of the client,

319
00:13:21,870 --> 00:13:24,610
just as a concrete example of the issues

320
00:13:24,610 --> 00:13:25,600
that Jen was mentioning,

321
00:13:25,600 --> 00:13:27,460
vis-a-vis like spyware, stalkerware

322
00:13:27,460 --> 00:13:29,310
being installed on devices, right?

323
00:13:29,310 --> 00:13:31,069
Sometimes it's just not a good idea

324
00:13:31,070 --> 00:13:34,090
to remove those apps, right?

325
00:13:34,090 --> 00:13:37,430
because that might trigger
the abuser to physically,

326
00:13:37,430 --> 00:13:40,260
to escalate to physical violence
in these kinds of things.

327
00:13:40,260 --> 00:13:43,660
So that's like one
really, really important

328
00:13:43,660 --> 00:13:45,449
set of work that we need to do.

329
00:13:45,450 --> 00:13:48,400
Another is how do we help
companies basically do better

330
00:13:48,400 --> 00:13:50,350
on these dual use or misuse issues.

331
00:13:50,350 --> 00:13:52,620
And I think for that designing
against these threat models,

332
00:13:52,620 --> 00:13:53,820
is really important.

333
00:13:53,820 --> 00:13:55,457
Which tangibly for companies means,

334
00:13:55,457 --> 00:13:57,790
"Hey can we actually do design reviews?"

335
00:13:57,790 --> 00:13:58,622
Where you're saying,

336
00:13:58,623 --> 00:14:00,000
"Hey, how is this interface gonna play out

337
00:14:00,000 --> 00:14:01,680
in a domestic violence situation?"

338
00:14:01,680 --> 00:14:04,010
Right? Like when you have
a bad actor in the home

339
00:14:04,010 --> 00:14:06,530
who knows the passwords
of the of the victim

340
00:14:06,530 --> 00:14:09,160
who can get into their
accounts, et cetera.

341
00:14:09,160 --> 00:14:10,990
Then there's holding these kind

342
00:14:10,990 --> 00:14:12,400
of worst bad actors accountable.

343
00:14:12,400 --> 00:14:13,990
People who are making spyware

344
00:14:13,990 --> 00:14:17,340
and advertising for abuse
cases is a big problem.

345
00:14:17,340 --> 00:14:18,490
And then finally, I just want to mention,

346
00:14:18,490 --> 00:14:20,410
I think a really understudied area

347
00:14:20,410 --> 00:14:22,490
that is understanding
how to do prevention,

348
00:14:22,490 --> 00:14:25,260
how do we kind of intervene
with potential abusers

349
00:14:25,260 --> 00:14:28,090
and kind of steer them
towards healthier paths

350
00:14:28,090 --> 00:14:31,123
on the technology use in relationships?

351
00:14:32,590 --> 00:14:34,340
- Thank you all so much.

352
00:14:34,340 --> 00:14:36,640
There's so much to discuss about this.

353
00:14:36,640 --> 00:14:40,160
I'm pretty sure this
could be an all-day event

354
00:14:40,160 --> 00:14:42,699
but I'd like to walk through
a couple of the questions

355
00:14:42,700 --> 00:14:43,680
that we've talked about.

356
00:14:43,680 --> 00:14:45,300
And, and we know that we,

357
00:14:45,300 --> 00:14:49,401
that all of us frequently
get from working with people.

358
00:14:49,401 --> 00:14:52,349
First, I wanted to go
back to you, Natalie.

359
00:14:52,350 --> 00:14:55,570
Could you talk a little
bit about stalkerware?

360
00:14:55,570 --> 00:14:56,930
We've already mentioned
it a couple of times

361
00:14:56,930 --> 00:14:59,160
and I know it's something
that we get asked

362
00:14:59,160 --> 00:15:00,610
about all the time.

363
00:15:00,610 --> 00:15:04,690
And it's also a technology that's secured,

364
00:15:04,690 --> 00:15:06,080
several security companies

365
00:15:06,080 --> 00:15:09,169
have documented significant increases

366
00:15:09,169 --> 00:15:11,329
throughout the pandemic.

367
00:15:11,330 --> 00:15:13,660
So I'd love for the audience
to hear a little bit more

368
00:15:13,660 --> 00:15:15,459
about what stalkerware is

369
00:15:15,460 --> 00:15:18,160
and how we're seeing it misused

370
00:15:18,160 --> 00:15:22,510
and why it's such an invasive
type of misuse for survivors.

371
00:15:22,510 --> 00:15:23,881
- Sure.

372
00:15:23,881 --> 00:15:26,880
I think, I really agree with
everything Jennifer said

373
00:15:26,880 --> 00:15:28,920
as far as you know,

374
00:15:28,920 --> 00:15:32,300
when we think about technology
abuse is sort of a pyramid.

375
00:15:32,300 --> 00:15:35,742
I think that broadest band
at the bottom of the pyramid

376
00:15:35,742 --> 00:15:38,300
is lower tech forms of abuse.

377
00:15:38,300 --> 00:15:41,510
Where perhaps the abuser
knows the survivor's password,

378
00:15:41,510 --> 00:15:44,223
perhaps the abuser, you know,

379
00:15:44,223 --> 00:15:45,500
they were logged in on the
same computer at some point,

380
00:15:45,500 --> 00:15:48,020
they guessed their security questions.

381
00:15:48,020 --> 00:15:51,880
So it's typically not the
most high tech forms of abuse

382
00:15:51,880 --> 00:15:53,860
but sometimes they evolve

383
00:15:53,860 --> 00:15:55,570
or there are some more expert abusers

384
00:15:55,570 --> 00:15:57,210
who use more sophisticated things

385
00:15:57,210 --> 00:16:01,490
such as stalkerware
which is really designed

386
00:16:01,490 --> 00:16:05,410
to surveil someone's device remotely.

387
00:16:05,410 --> 00:16:09,060
And that's something that can
be downloaded onto the device

388
00:16:09,060 --> 00:16:11,729
and not even have an
app necessarily appear

389
00:16:11,730 --> 00:16:13,010
on the home screen.

390
00:16:13,010 --> 00:16:16,950
And it doesn't necessarily
provide a persistent notification

391
00:16:16,950 --> 00:16:19,330
to the individual whose device it is.

392
00:16:19,330 --> 00:16:23,130
So this can obviously create a feeling

393
00:16:23,130 --> 00:16:25,890
of 24-hour persistent uncertainty

394
00:16:25,890 --> 00:16:29,319
when it seems all your
communications are being monitored.

395
00:16:29,320 --> 00:16:32,090
So, though our research indicated

396
00:16:32,090 --> 00:16:33,980
that the majority of the time

397
00:16:33,980 --> 00:16:36,400
it is sort of those dual use apps

398
00:16:36,400 --> 00:16:38,699
or a lack of robust password management

399
00:16:38,700 --> 00:16:41,040
that is leading to
these security breaches,

400
00:16:41,040 --> 00:16:42,790
since the pandemic,

401
00:16:42,790 --> 00:16:44,800
there have been some disturbing reports

402
00:16:44,800 --> 00:16:46,990
that have come out such
as from Malwarebytes,

403
00:16:46,990 --> 00:16:49,660
that found that they
detected a 1000% increase

404
00:16:49,660 --> 00:16:51,350
in the stalkerware with their scanning.

405
00:16:51,350 --> 00:16:53,500
And that's obviously, you know,

406
00:16:53,500 --> 00:16:55,403
people who are using that service.

407
00:16:56,300 --> 00:17:00,729
But I think that it's a higher
risk time for stalkerware

408
00:17:00,730 --> 00:17:02,930
to be on your device because before,

409
00:17:02,930 --> 00:17:04,960
if you were in a domestic
violence situation

410
00:17:04,960 --> 00:17:07,900
you would be meeting face to
face with your DV advocate.

411
00:17:07,900 --> 00:17:09,980
You wouldn't need to
meet with them by Zoom

412
00:17:09,980 --> 00:17:13,150
or have all of your communications
going across a device.

413
00:17:13,150 --> 00:17:15,869
You might have an in-person
meeting with an attorney

414
00:17:15,869 --> 00:17:17,339
about your divorce plan,

415
00:17:17,339 --> 00:17:19,859
or you know, custody issues.

416
00:17:19,859 --> 00:17:21,899
But if it's all happening via Zoom

417
00:17:21,900 --> 00:17:23,819
and it's all being monitored,

418
00:17:23,819 --> 00:17:27,149
then you can really see how the abuser

419
00:17:27,150 --> 00:17:30,950
is surveilling every aspect
of their communication.

420
00:17:30,950 --> 00:17:34,330
So I think what we've learned
is that this is an area

421
00:17:34,330 --> 00:17:36,159
that absolutely demands more study,

422
00:17:36,160 --> 00:17:38,713
and there is a Coalition
Against Stalkerware

423
00:17:38,713 --> 00:17:40,840
that is looking at some recommendations.

424
00:17:40,840 --> 00:17:42,649
And I know some folks on the
panel are involved with that.

425
00:17:42,650 --> 00:17:44,810
So I'd invite them to jump in.

426
00:17:44,810 --> 00:17:47,590
But I think that
multi-disciplinary partnership

427
00:17:47,590 --> 00:17:50,040
of the coalition is a
really inspiring model

428
00:17:50,040 --> 00:17:53,080
for efforts in this stage going forward.

429
00:17:53,080 --> 00:17:55,072
- Yeah, I absolutely agree.

430
00:17:55,072 --> 00:17:56,540
Thank you for that.

431
00:17:56,540 --> 00:17:59,639
Jen, could you talk a
little bit about another

432
00:17:59,640 --> 00:18:01,670
common question that we always get

433
00:18:01,670 --> 00:18:03,470
is about the laws?

434
00:18:03,470 --> 00:18:05,090
Tell us a little bit about what tech,

435
00:18:05,090 --> 00:18:07,820
how technology abuse
is addressed by US laws

436
00:18:07,820 --> 00:18:10,110
and what gaps may exist.

437
00:18:10,110 --> 00:18:12,189
- You know, I think when we look at laws

438
00:18:12,190 --> 00:18:14,550
trying to address technology,

439
00:18:14,550 --> 00:18:16,110
it's like we're always playing catch up.

440
00:18:16,110 --> 00:18:17,790
We're always behind the
eight ball a little bit

441
00:18:17,790 --> 00:18:20,190
because we have some
really antiquated laws

442
00:18:20,190 --> 00:18:22,250
that we're using to try to address

443
00:18:22,250 --> 00:18:24,150
some really up-to-date technologies.

444
00:18:24,150 --> 00:18:26,640
And then the strategy in lots of states

445
00:18:26,640 --> 00:18:28,380
is to try to pass new laws

446
00:18:28,380 --> 00:18:30,060
that by the time they get passed

447
00:18:30,060 --> 00:18:31,750
are now out of date as well.

448
00:18:31,750 --> 00:18:34,610
So there's always that difficulty.

449
00:18:34,610 --> 00:18:37,179
And I think the other
difficulty we have is really

450
00:18:37,180 --> 00:18:40,160
just kind of demystifying
the use of technology.

451
00:18:40,160 --> 00:18:41,960
I think one of the things that we hear

452
00:18:41,960 --> 00:18:43,240
from survivors all the time

453
00:18:43,240 --> 00:18:47,520
is if there's in-person things happening

454
00:18:47,520 --> 00:18:50,040
or very real threats made by somebody,

455
00:18:50,040 --> 00:18:52,600
you know, as they're
across the room from them,

456
00:18:52,600 --> 00:18:53,433
that's one thing.

457
00:18:53,433 --> 00:18:56,379
But if it happens just
through the technology

458
00:18:56,380 --> 00:18:57,800
or via social media,

459
00:18:57,800 --> 00:18:59,330
then it's not taken as seriously.

460
00:18:59,330 --> 00:19:00,919
And so I think one of the strategies

461
00:19:00,920 --> 00:19:02,650
that we really have to focus on

462
00:19:02,650 --> 00:19:04,840
is understanding the harm that can happen

463
00:19:04,840 --> 00:19:06,679
from use of secure technologies.

464
00:19:06,680 --> 00:19:09,590
And even if there is an in-person contact,

465
00:19:09,590 --> 00:19:11,399
it's still as life-changing,

466
00:19:11,400 --> 00:19:12,450
it's still as scary,

467
00:19:12,450 --> 00:19:14,020
it's still as threatening to those people

468
00:19:14,020 --> 00:19:15,060
who are experiencing it.

469
00:19:15,060 --> 00:19:17,230
So I think some of our strategies

470
00:19:17,230 --> 00:19:18,290
have to be kind of two-fold.

471
00:19:18,290 --> 00:19:21,705
Number one, demystifying the
technology, first of all.

472
00:19:21,705 --> 00:19:23,909
We also see that in response systems,

473
00:19:23,910 --> 00:19:27,810
people who are less likely to
go forward with the tech case

474
00:19:27,810 --> 00:19:30,120
because it seems so
hard to get the evidence

475
00:19:30,120 --> 00:19:31,419
or more difficult

476
00:19:31,420 --> 00:19:34,230
and not knowing who to
reach out to for help.

477
00:19:34,230 --> 00:19:35,857
And then we have the strategy of,

478
00:19:35,857 --> 00:19:37,620
"Okay so if we have the information,

479
00:19:37,620 --> 00:19:39,830
what kind of laws do we
have at our disposal?"

480
00:19:39,830 --> 00:19:41,429
And then of course,

481
00:19:41,430 --> 00:19:45,680
jump into, if you have a company
that's not inside the US,

482
00:19:45,680 --> 00:19:48,910
do they have to obey court
orders and subpoenas, et cetera

483
00:19:48,910 --> 00:19:49,860
that are US-based?

484
00:19:49,860 --> 00:19:50,693
No, they don't.

485
00:19:50,693 --> 00:19:52,370
So that can be difficult as well.

486
00:19:52,370 --> 00:19:55,780
So all those different
strategies can play into it

487
00:19:55,780 --> 00:19:56,780
but I think definitely,

488
00:19:56,780 --> 00:19:59,340
always feeling like we're playing catch up

489
00:19:59,340 --> 00:20:01,480
in that particular response.

490
00:20:01,480 --> 00:20:03,070
- Absolutely.

491
00:20:03,070 --> 00:20:07,687
So Tom, digging more into
the various threat models

492
00:20:07,688 --> 00:20:09,080
and the gaps that exist,

493
00:20:09,080 --> 00:20:11,360
can you talk a little bit more about how

494
00:20:11,360 --> 00:20:12,821
intimate partner abuse

495
00:20:12,821 --> 00:20:16,399
that the threat models
for intimate partner abuse

496
00:20:16,400 --> 00:20:19,390
differ from the from the ones that focus

497
00:20:19,390 --> 00:20:22,083
on state surveillance or
broader privacy issues?

498
00:20:23,530 --> 00:20:24,363
- Yeah, for sure.

499
00:20:24,363 --> 00:20:26,350
And I think this is a
really important point,

500
00:20:26,350 --> 00:20:28,080
particularly at RSA,

501
00:20:28,080 --> 00:20:30,810
which is a conference that's built around

502
00:20:30,810 --> 00:20:32,363
the computer security industry.

503
00:20:34,130 --> 00:20:35,670
I think we can, everyone,

504
00:20:35,670 --> 00:20:36,910
I'll come to some other examples

505
00:20:36,910 --> 00:20:39,290
but the stalkerware, spyware example

506
00:20:39,290 --> 00:20:42,670
is an easy one to highlight
this difference, right?

507
00:20:42,670 --> 00:20:45,470
You know, up until a
couple of, few years ago

508
00:20:45,470 --> 00:20:47,980
most of these spyware, stalkerware apps

509
00:20:47,980 --> 00:20:50,960
were not flagged by
antivirus tools, right?

510
00:20:50,960 --> 00:20:52,643
And we've seen a nice shift in that,

511
00:20:52,643 --> 00:20:54,180
over the last couple of years

512
00:20:55,047 --> 00:20:56,870
as companies have started flagging that.

513
00:20:56,870 --> 00:20:58,679
And I think that really
emanates out of the mindset

514
00:20:58,680 --> 00:21:01,080
of like, "Oh, what is antivirus
supposed to be preventing?"

515
00:21:01,080 --> 00:21:03,587
Right? Anti-virus is
supposed to prevent malware

516
00:21:03,587 --> 00:21:06,863
that's you know, worms or ransomware,

517
00:21:07,729 --> 00:21:09,929
or kind of this other
commercially motivated

518
00:21:11,289 --> 00:21:15,180
criminal enterprises
that is profit-motivated.

519
00:21:15,180 --> 00:21:17,910
And this issue of like
intimate partner abuse

520
00:21:17,910 --> 00:21:21,320
or actually other forms of
kind of interpersonal attacks

521
00:21:21,320 --> 00:21:23,102
were pretty much off the radar.

522
00:21:24,620 --> 00:21:26,679
And so I think that it's something

523
00:21:26,680 --> 00:21:27,847
we need to shift a
little bit thinking like,

524
00:21:27,847 --> 00:21:29,160
"Hey, there's a lot of other interesting

525
00:21:29,160 --> 00:21:30,250
threat models out here

526
00:21:30,250 --> 00:21:33,970
that are really directly
affecting people's wellbeing

527
00:21:33,970 --> 00:21:37,090
and IPV is a significant one."

528
00:21:37,090 --> 00:21:38,720
Similarly, I think you can look at,

529
00:21:38,720 --> 00:21:39,870
other things like on social media

530
00:21:39,870 --> 00:21:43,852
and misinformation and
abuse and harassment online.

531
00:21:45,100 --> 00:21:48,340
It's really hard to catch that type of,

532
00:21:48,340 --> 00:21:50,980
catch the type of issues

533
00:21:50,980 --> 00:21:52,640
that are facing survivors

534
00:21:52,640 --> 00:21:54,420
because it's such an
intimate context, right?

535
00:21:54,420 --> 00:21:56,007
Like we hear from survivors about like,

536
00:21:56,007 --> 00:21:58,210
"Hey, you know, I reported.

537
00:21:58,210 --> 00:22:00,810
My abuser sent me a picture of a gun,

538
00:22:00,810 --> 00:22:03,750
which I know in this context
means he's threatening me.

539
00:22:03,750 --> 00:22:07,400
Right? And I report it
to Facebook or whomever."

540
00:22:07,400 --> 00:22:09,112
And you know, they're like,

541
00:22:09,112 --> 00:22:10,283
"Well, it's just the gun."

542
00:22:10,283 --> 00:22:11,379
Like, you know, the
context is missing, right?

543
00:22:11,380 --> 00:22:13,703
Such a really challenging issue

544
00:22:13,703 --> 00:22:18,350
to deal with the very nuanced
context-specific issues

545
00:22:18,350 --> 00:22:20,243
in intimate partner violence.

546
00:22:21,160 --> 00:22:23,570
So I think in general
there's a big gap here

547
00:22:23,570 --> 00:22:25,189
between abuse threat models

548
00:22:26,358 --> 00:22:28,500
and regular computer security models,

549
00:22:28,500 --> 00:22:30,594
which is actually one of the reasons

550
00:22:30,594 --> 00:22:31,427
I was so fascinated by this area.

551
00:22:31,427 --> 00:22:32,360
I mean, I think it's really interesting

552
00:22:32,360 --> 00:22:34,240
but we need a lot more focus and attention

553
00:22:34,240 --> 00:22:36,020
thinking cleverly about how we can build

554
00:22:36,020 --> 00:22:39,160
into our design processes
like reviews for abuse,

555
00:22:39,160 --> 00:22:44,160
or like as I mentioned before
for abuse of user interfaces

556
00:22:44,430 --> 00:22:46,710
and these types of things and just think

557
00:22:46,710 --> 00:22:48,980
more thoughtfully about this.

558
00:22:48,980 --> 00:22:50,400
- Thank you.

559
00:22:50,400 --> 00:22:51,830
Now Jen and Natalie,

560
00:22:51,830 --> 00:22:53,110
coming back to you both,

561
00:22:53,110 --> 00:22:56,100
how have you both seen these
issues with threat models

562
00:22:56,100 --> 00:22:59,520
play out for survivors
who are being abused

563
00:22:59,520 --> 00:23:03,170
and are trying to address
that by regaining control

564
00:23:03,170 --> 00:23:04,110
over their devices,

565
00:23:04,110 --> 00:23:05,520
seeking legal resources,

566
00:23:05,520 --> 00:23:08,463
or just maintaining their
privacy in order to feel safe?

567
00:23:10,040 --> 00:23:11,203
- Yeah, I think,

568
00:23:12,600 --> 00:23:15,909
when a person enters looking the process,

569
00:23:15,910 --> 00:23:16,930
looking for accountability

570
00:23:16,930 --> 00:23:18,830
through the criminal legal system,

571
00:23:18,830 --> 00:23:21,260
there are sort of barriers at every turn

572
00:23:21,260 --> 00:23:24,340
when tech abuse is the primary concern.

573
00:23:24,340 --> 00:23:28,020
Speaking of the criminal
path locally where I am,

574
00:23:28,020 --> 00:23:30,800
the majority of these
crimes are misdemeanors

575
00:23:30,800 --> 00:23:33,399
which means they typically
do not get assigned

576
00:23:33,400 --> 00:23:37,450
to a detective and there is
not investigative support.

577
00:23:37,450 --> 00:23:40,810
So to unmask a perpetrator in these cases

578
00:23:40,810 --> 00:23:43,120
often requires multiple warrants

579
00:23:43,120 --> 00:23:46,550
which is not ever going to
happen at the patrol level.

580
00:23:46,550 --> 00:23:48,330
So survivors might find

581
00:23:48,330 --> 00:23:50,820
that their electronic
protection order violations,

582
00:23:50,820 --> 00:23:52,189
meaning if they have a protection order

583
00:23:52,190 --> 00:23:55,400
and a person is sending, you know,

584
00:23:55,400 --> 00:23:57,260
encrypted messages or you know,

585
00:23:57,260 --> 00:23:58,200
creating new accounts,

586
00:23:58,200 --> 00:24:00,590
if there's no investigation into that

587
00:24:00,590 --> 00:24:03,050
then there is not going
to be any accountability

588
00:24:03,050 --> 00:24:05,020
for that perpetrator.

589
00:24:05,020 --> 00:24:07,290
So that's the criminal piece.

590
00:24:07,290 --> 00:24:09,200
Looking at the civil path

591
00:24:09,200 --> 00:24:12,080
if someone's pursuing a
Civil Protection Order

592
00:24:12,080 --> 00:24:16,120
we see that the courts have
not caught up whatsoever.

593
00:24:16,120 --> 00:24:18,756
We see survivors putting a lot of labor

594
00:24:18,757 --> 00:24:22,220
and making digital evidence analog.

595
00:24:22,220 --> 00:24:27,220
There may not be any way to
upload video or audio files

596
00:24:28,010 --> 00:24:29,190
to the court records,

597
00:24:29,190 --> 00:24:30,540
so they're transcribing it.

598
00:24:31,470 --> 00:24:34,653
They're trying to get
photo stills of videos.

599
00:24:35,500 --> 00:24:37,660
They're trying to educate the judge

600
00:24:37,660 --> 00:24:39,560
about social media platforms

601
00:24:39,560 --> 00:24:42,139
that the judge or commissioner
may not be familiar with.

602
00:24:42,140 --> 00:24:44,830
So you can see some real
equity issues happening here

603
00:24:44,830 --> 00:24:47,730
where the survivors were in
a position to project manage

604
00:24:47,730 --> 00:24:50,720
their own case might be more successful

605
00:24:50,720 --> 00:24:54,480
in getting a Protection Order
than a survivor who's not.

606
00:24:54,480 --> 00:24:56,730
And unfortunately,

607
00:24:56,730 --> 00:25:00,830
we also see abusers manipulate the courts

608
00:25:00,830 --> 00:25:02,560
to their own advantage.

609
00:25:02,560 --> 00:25:06,379
I've seen multiple cases
where the abuser will file

610
00:25:06,380 --> 00:25:08,970
intimate images into the court record

611
00:25:08,970 --> 00:25:12,020
and they are not filed under a seal.

612
00:25:12,020 --> 00:25:15,520
So now we have used the
court to perpetuate the harm

613
00:25:15,520 --> 00:25:17,980
of the distribution of intimate images.

614
00:25:17,980 --> 00:25:20,830
So there is the need to improve capacity

615
00:25:20,830 --> 00:25:22,929
of cross-systems in terms of understanding

616
00:25:22,930 --> 00:25:25,160
how to investigate these types of harm.

617
00:25:25,160 --> 00:25:26,840
But there's also the need to have

618
00:25:26,840 --> 00:25:29,540
considered processes that can allow

619
00:25:29,540 --> 00:25:31,623
for as much privacy as possible.

620
00:25:33,099 --> 00:25:35,216
- If I can only add to
what Natalie has said,

621
00:25:35,217 --> 00:25:36,280
in the long and short of it

622
00:25:36,280 --> 00:25:39,070
it is expensive and it's time-consuming

623
00:25:39,070 --> 00:25:41,070
and it's not always accessible to folks.

624
00:25:41,070 --> 00:25:42,330
And if you think about it,

625
00:25:42,330 --> 00:25:45,730
it's one of those times
where we essentially rely

626
00:25:45,730 --> 00:25:49,203
on victims to project manage
or build their own cases.

627
00:25:50,158 --> 00:25:52,929
And what the solution people
often recommend to survivors

628
00:25:52,930 --> 00:25:55,210
is to just stop using the technology

629
00:25:55,210 --> 00:25:58,250
which isn't obviously victim-centered.

630
00:25:58,250 --> 00:26:00,210
It's not a possibility

631
00:26:00,210 --> 00:26:02,320
and it's not something that
survivors are gonna do.

632
00:26:02,320 --> 00:26:05,159
And so thinking about that as you know

633
00:26:05,160 --> 00:26:06,820
the tongue-in-cheek people thing,

634
00:26:06,820 --> 00:26:09,105
people say to victims all the time

635
00:26:09,105 --> 00:26:10,080
is, "Well, just stop
using the technology."

636
00:26:10,080 --> 00:26:11,889
It's not realistic and it's not fair.

637
00:26:11,890 --> 00:26:14,640
And so how do we help
survivors have that right

638
00:26:14,640 --> 00:26:19,640
to continue to access that
technology but in a safer way?

639
00:26:19,730 --> 00:26:20,870
- Yeah, thank you both.

640
00:26:20,870 --> 00:26:23,520
And that is such a great point
that you ended on there too.

641
00:26:23,520 --> 00:26:26,960
We are big advocates
of empowering survivors

642
00:26:26,960 --> 00:26:29,760
to take control of their technology

643
00:26:29,760 --> 00:26:32,660
and to know how to increase their privacy

644
00:26:32,660 --> 00:26:36,027
and safety on their
devices and their accounts

645
00:26:36,027 --> 00:26:39,280
`coz taking away their
technology does nothing

646
00:26:39,280 --> 00:26:41,320
to hold a perpetrator accountable.

647
00:26:41,320 --> 00:26:44,050
It does nothing to even increase safety.

648
00:26:44,050 --> 00:26:46,050
Abusers don't go away that easily.

649
00:26:46,050 --> 00:26:47,370
And we know that.

650
00:26:47,370 --> 00:26:48,632
So thank you for that.

651
00:26:49,510 --> 00:26:52,700
So do any of you have some examples

652
00:26:52,700 --> 00:26:55,420
of best practices that you would encourage

653
00:26:55,420 --> 00:26:57,980
for broad industry adoption

654
00:26:57,980 --> 00:26:59,760
and any thoughts about the challenges

655
00:26:59,760 --> 00:27:01,427
that the industry faces

656
00:27:01,427 --> 00:27:03,127
and really addressing this better?

657
00:27:04,410 --> 00:27:06,169
- I guess I could just start.

658
00:27:06,169 --> 00:27:07,780
I mean, I think one,

659
00:27:07,780 --> 00:27:10,210
this has to get on the
radar at companies, right?

660
00:27:10,210 --> 00:27:13,220
I mean, most companies hire security teams

661
00:27:13,220 --> 00:27:16,350
and they have people
dedicated to computer security

662
00:27:16,350 --> 00:27:19,179
and you need to invest also in anti-abuse

663
00:27:19,180 --> 00:27:21,200
in Trust and Safety, right?

664
00:27:21,200 --> 00:27:22,990
I think that's something
that across the board

665
00:27:22,990 --> 00:27:26,170
companies are becoming more recognizing of

666
00:27:26,170 --> 00:27:28,750
but this isn't just Trust and Safety

667
00:27:28,750 --> 00:27:30,010
for your average user, right?

668
00:27:30,010 --> 00:27:31,802
This is Trust and Safety

669
00:27:31,802 --> 00:27:35,141
for particular targeted people

670
00:27:35,142 --> 00:27:38,300
which based on the statistics
that you were giving earlier,

671
00:27:38,300 --> 00:27:40,200
that's a large fraction
of your users, right?

672
00:27:40,200 --> 00:27:43,164
One out of four women and one out of

673
00:27:43,164 --> 00:27:46,550
or one out of three women
and one out of four men.

674
00:27:46,550 --> 00:27:48,850
So, you know, hiring people

675
00:27:48,850 --> 00:27:50,360
to actually think about these problems,

676
00:27:50,360 --> 00:27:52,959
review your designs,

677
00:27:52,960 --> 00:27:54,850
understand that every
time you open up interface

678
00:27:54,850 --> 00:27:56,699
that's used to communicate
between two of your users

679
00:27:56,700 --> 00:27:58,070
that will be abused.

680
00:27:58,070 --> 00:28:00,550
And so you need to think about
how it's gonna be abused,

681
00:28:00,550 --> 00:28:02,899
what, you know, how to try to add friction

682
00:28:02,900 --> 00:28:04,210
to those abuse use cases

683
00:28:04,210 --> 00:28:06,450
and how to add the recovery mechanisms

684
00:28:06,450 --> 00:28:08,010
for what happens when abuse occurs

685
00:28:08,010 --> 00:28:10,670
and what recourse our users can have.

686
00:28:10,670 --> 00:28:12,470
There's a lot of challenges here

687
00:28:12,470 --> 00:28:15,200
and you need to invest in it.

688
00:28:15,200 --> 00:28:16,670
- Yeah. I think there's a real opportunity

689
00:28:16,670 --> 00:28:20,480
to move from compliance to culture change.

690
00:28:20,480 --> 00:28:23,240
I think it's not sufficient
to have a legal department

691
00:28:23,240 --> 00:28:25,900
that will at some point
respond to a subpoena,

692
00:28:25,900 --> 00:28:27,930
but instead if you have people

693
00:28:27,930 --> 00:28:30,540
who have given you their data,

694
00:28:30,540 --> 00:28:32,680
who are your customers

695
00:28:32,680 --> 00:28:35,370
who are consistently having
these issues on your platform,

696
00:28:35,370 --> 00:28:39,080
there needs to be some kind of
way to provide that feedback.

697
00:28:39,080 --> 00:28:42,461
And I think having
gender-based violence experts

698
00:28:42,461 --> 00:28:45,940
in a design process as
like an advisory council

699
00:28:45,940 --> 00:28:47,120
would be a really great idea

700
00:28:47,120 --> 00:28:48,590
because these are gonna be the people,

701
00:28:48,590 --> 00:28:51,209
whether they're survivors
or they're advocates

702
00:28:51,210 --> 00:28:53,227
who are really going to
be able to flag like,

703
00:28:53,227 --> 00:28:55,440
"Hey, that's a concern."

704
00:28:55,440 --> 00:28:58,010
You know, I think a recent
example and you know

705
00:28:58,010 --> 00:29:00,510
this is a platform I
love and I use, you know,

706
00:29:00,510 --> 00:29:03,260
I use Signal but I've known some advocates

707
00:29:03,260 --> 00:29:05,160
who have thought about
maybe starting to use that

708
00:29:05,160 --> 00:29:07,100
with clients but then they were concerned

709
00:29:07,100 --> 00:29:09,889
because then your contact
section gets a ping

710
00:29:09,890 --> 00:29:11,430
if you've joined Signal.

711
00:29:11,430 --> 00:29:12,507
So then they were worried,

712
00:29:12,507 --> 00:29:14,590
"Okay, well then their abuser might know

713
00:29:14,590 --> 00:29:15,750
that they're using this other

714
00:29:15,750 --> 00:29:17,710
separate private encrypted platform

715
00:29:17,710 --> 00:29:19,567
to speak with someone on."

716
00:29:20,501 --> 00:29:23,767
I think if we had ways to communicate that

717
00:29:23,767 --> 00:29:27,470
"Hey, this actually can
disrupt the kind of privacy

718
00:29:27,470 --> 00:29:28,320
we're seeking,"

719
00:29:28,320 --> 00:29:31,020
I think those would be really
productive relationship.

720
00:29:32,300 --> 00:29:33,240
- Thank you so much.

721
00:29:33,240 --> 00:29:36,188
And this speaks to, this
goes back to something

722
00:29:36,188 --> 00:29:37,021
you had mentioned earlier

723
00:29:37,021 --> 00:29:40,180
to know about the importance
of people within the industry

724
00:29:40,180 --> 00:29:41,670
and all outside of the industry

725
00:29:41,670 --> 00:29:42,740
to really work together

726
00:29:42,740 --> 00:29:45,691
and the importance of
those collaborations.

727
00:29:45,691 --> 00:29:47,360
We're a founding member

728
00:29:47,360 --> 00:29:49,449
of the Coalition Against
Stalkerware you mentioned.

729
00:29:49,450 --> 00:29:52,890
And I know all of us on the
panel here have worked closely

730
00:29:52,890 --> 00:29:55,200
with a number of tech
companies over the years

731
00:29:55,200 --> 00:29:56,710
and continue to,

732
00:29:56,710 --> 00:29:59,320
and being part of those conversations

733
00:30:00,613 --> 00:30:04,590
about both the design of the platforms,

734
00:30:04,590 --> 00:30:05,870
the reporting flows,

735
00:30:05,870 --> 00:30:07,020
all of that,

736
00:30:07,020 --> 00:30:08,870
and the options for survivors,

737
00:30:08,870 --> 00:30:09,939
the notifications,

738
00:30:09,940 --> 00:30:12,143
how sensitive, how informative they are,

739
00:30:13,111 --> 00:30:14,780
all of that is incredibly important.

740
00:30:14,780 --> 00:30:18,160
And I think it's a big part of our work

741
00:30:18,160 --> 00:30:21,820
that we've really enjoyed
and have learned a lot about.

742
00:30:21,820 --> 00:30:23,909
And we definitely need
to see more and more

743
00:30:23,910 --> 00:30:25,560
of those kinds of collaborations.

744
00:30:26,920 --> 00:30:31,340
Before we turn to the live Q and A,

745
00:30:31,340 --> 00:30:33,750
I think it's always important
to just take a quick moment

746
00:30:33,750 --> 00:30:37,060
to talk about a few
things that you can do.

747
00:30:37,060 --> 00:30:39,990
If anyone who's listening to this session,

748
00:30:39,990 --> 00:30:42,850
if you know somebody who
could be experiencing abuse

749
00:30:42,850 --> 00:30:44,679
or someone discloses abuse to you,

750
00:30:44,680 --> 00:30:46,807
what would you do in those situations?

751
00:30:46,807 --> 00:30:49,960
`Coz it can be very hard
to know exactly what to say

752
00:30:49,960 --> 00:30:51,710
or where to point somebody.

753
00:30:51,710 --> 00:30:53,580
But considering the statistics on abuse,

754
00:30:53,580 --> 00:30:54,413
as we've mentioned,

755
00:30:54,413 --> 00:30:57,110
we should all be prepared
for this conversation.

756
00:30:57,110 --> 00:30:58,429
It could be someone in your family,

757
00:30:58,430 --> 00:31:00,920
a colleague or a customer who is concerned

758
00:31:00,920 --> 00:31:04,590
about your products and their
privacy because of abuse.

759
00:31:04,590 --> 00:31:05,709
So I always tell people

760
00:31:05,710 --> 00:31:07,820
that one of the most
important things you can do

761
00:31:07,820 --> 00:31:11,925
is to not judge the
person or the situation,

762
00:31:11,925 --> 00:31:14,169
and to trust the person's instincts.

763
00:31:14,170 --> 00:31:17,900
They know that person and
know the situation better

764
00:31:17,900 --> 00:31:19,330
than you ever will,

765
00:31:19,330 --> 00:31:21,270
better than any of us ever will.

766
00:31:21,270 --> 00:31:24,960
So if they believe that
it is unsafe for them,

767
00:31:24,960 --> 00:31:26,110
then it probably is.

768
00:31:26,110 --> 00:31:29,379
If they believe that they're
being monitored by someone

769
00:31:29,380 --> 00:31:31,630
it's definitely possible that they are.

770
00:31:31,630 --> 00:31:35,160
Even if it seems
unbelievable to us, you know?

771
00:31:35,160 --> 00:31:36,467
I often hear people say like,

772
00:31:36,467 --> 00:31:39,640
"Oh, I don't think that
person was that techie."

773
00:31:39,640 --> 00:31:40,800
And unfortunately,

774
00:31:40,800 --> 00:31:43,240
we know you don't have to be that techie

775
00:31:43,240 --> 00:31:47,500
to misuse and take advantage
of many of the technologies

776
00:31:47,500 --> 00:31:51,140
that are out there that
abusers are frequently using

777
00:31:51,140 --> 00:31:52,760
as a tactic of abuse.

778
00:31:52,760 --> 00:31:54,050
It's always helpful to point people

779
00:31:54,050 --> 00:31:56,000
to the National Domestic Violence Hotline

780
00:31:56,000 --> 00:31:57,410
so they can talk to an advocate

781
00:31:57,410 --> 00:32:00,170
and find programs that are local to them.

782
00:32:00,170 --> 00:32:04,090
So, any other words of
advice from the panelists

783
00:32:04,090 --> 00:32:05,939
that you might have for the audience?

784
00:32:07,532 --> 00:32:09,429
- I think the only ones
I would add to that

785
00:32:09,430 --> 00:32:11,170
is to echo the, you know,

786
00:32:11,170 --> 00:32:13,690
validate and believe
what people have to say

787
00:32:13,690 --> 00:32:16,620
as well as make sure you're
respecting the victim's privacy.

788
00:32:16,620 --> 00:32:18,229
Don't go advocating on their behalf

789
00:32:18,230 --> 00:32:21,280
without their permission
is really important.

790
00:32:21,280 --> 00:32:23,570
And obviously, as I mentioned earlier

791
00:32:24,580 --> 00:32:27,506
don't tell them to just stop
using the tech. (laughs)

792
00:32:27,507 --> 00:32:29,750
Not a good strategy. (laughs)

793
00:32:29,750 --> 00:32:31,460
- Yes! Very important.

794
00:32:31,460 --> 00:32:32,690
Anything else?

795
00:32:32,690 --> 00:32:35,670
- Yeah. Just a caution against
being too prescriptive.

796
00:32:35,670 --> 00:32:37,730
What might increase
one's survivor's safety

797
00:32:37,730 --> 00:32:40,420
would put another survivor in more danger.

798
00:32:40,420 --> 00:32:44,020
For example, suggesting that
they just block that person.

799
00:32:44,020 --> 00:32:46,680
They may want the information
if that individual

800
00:32:46,680 --> 00:32:48,760
is escalating or threatening to come over

801
00:32:48,760 --> 00:32:50,600
and that may not be what's safe for them.

802
00:32:50,600 --> 00:32:52,879
So just discuss options,

803
00:32:52,880 --> 00:32:54,823
but be a little wary of advice.

804
00:32:55,670 --> 00:32:57,850
- Yeah. You took the words
right out of my mouth.

805
00:32:57,850 --> 00:33:02,030
I was gonna say be careful
about taking actions,

806
00:33:02,030 --> 00:33:04,247
particularly if you're a
computer IT person you're like,

807
00:33:04,247 --> 00:33:07,310
"Oh, I can fix that for you
`coz I know how to do that."

808
00:33:07,310 --> 00:33:08,560
Without proper safety planning

809
00:33:08,560 --> 00:33:10,040
that could be really dangerous for people.

810
00:33:10,040 --> 00:33:12,409
So, getting them in touch
with support services

811
00:33:12,410 --> 00:33:13,820
and then also providing information

812
00:33:13,820 --> 00:33:16,279
about how they can empower themselves

813
00:33:16,279 --> 00:33:17,410
vis-a-vis their technology.

814
00:33:17,410 --> 00:33:18,373
Super important.

815
00:33:20,280 --> 00:33:21,113
- Thank you.

816
00:33:21,113 --> 00:33:23,610
All those are all incredibly
important points to end on.

817
00:33:23,610 --> 00:33:24,669
So I really appreciate it.

818
00:33:24,670 --> 00:33:26,830
I wanna take a moment
just to say thank you

819
00:33:26,830 --> 00:33:30,470
to all of our panelists for
such a great conversation.

820
00:33:30,470 --> 00:33:33,700
And please can you get
the audience members

821
00:33:33,700 --> 00:33:35,680
can ask them questions in the chat,

822
00:33:35,680 --> 00:33:39,830
and we'd be happy to answer
anything individually

823
00:33:39,830 --> 00:33:42,250
through this and then after this

824
00:33:42,250 --> 00:33:45,070
we all have our contact information
available on the website

825
00:33:45,070 --> 00:33:46,935
and we hope to hear from you

826
00:33:46,935 --> 00:33:48,437
if you have any other questions.

