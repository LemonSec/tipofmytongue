1
00:00:01,600 --> 00:00:04,970
- Hi, everyone. Welcome
to RSA conference 2021.

2
00:00:04,970 --> 00:00:07,770
This is, Lessons drawn from Cybersecurity

3
00:00:07,770 --> 00:00:09,490
in the rise of Privacy Tech.

4
00:00:09,490 --> 00:00:11,460
We'll start with speaker introductions

5
00:00:11,460 --> 00:00:13,870
and then do some background information

6
00:00:13,870 --> 00:00:17,060
And baseline definitions
in privacy and security,

7
00:00:17,060 --> 00:00:18,500
before we dig deeper

8
00:00:18,500 --> 00:00:21,380
into the transferable
lessons in cybersecurity,

9
00:00:21,380 --> 00:00:22,980
and then wrap it up.

10
00:00:22,980 --> 00:00:24,140
I'm Lourdes Turrecha,

11
00:00:24,140 --> 00:00:26,060
I'm the founder of The
Rise of Privacy Tech.

12
00:00:26,060 --> 00:00:28,599
Our mission is to fuel privacy innovation.

13
00:00:28,600 --> 00:00:31,970
We do this by bridging the
tech capital expertise gaps

14
00:00:31,970 --> 00:00:34,610
in privacy tech and
bringing together founders,

15
00:00:34,610 --> 00:00:38,290
investors and experts in this very nascent

16
00:00:38,290 --> 00:00:39,660
privacy tech landscape.

17
00:00:39,660 --> 00:00:40,940
In my individual capacity,

18
00:00:40,940 --> 00:00:43,297
I advise and consult with
some cybersecurity startups

19
00:00:43,297 --> 00:00:45,019
and privacy tech startups.

20
00:00:45,020 --> 00:00:47,350
And I'm also an adjunct professor

21
00:00:47,350 --> 00:00:49,890
and fellow at Santa Clara Law,

22
00:00:49,890 --> 00:00:51,810
where I teach an experiential course,

23
00:00:51,810 --> 00:00:53,115
Privacy and Technology.

24
00:00:53,115 --> 00:00:55,890
Where my students work with,

25
00:00:55,890 --> 00:00:59,640
for very exciting privacy tech startups.

26
00:00:59,640 --> 00:01:02,930
In my past work lives, I worked
at a cybersecurity startup

27
00:01:02,930 --> 00:01:04,319
I had not started my work company,

28
00:01:04,319 --> 00:01:06,696
called Alto Networks on privacy

29
00:01:06,697 --> 00:01:10,200
and also worked in big
law with a portfolio

30
00:01:10,200 --> 00:01:13,540
of more than a hundred
clients on everything privacy

31
00:01:13,540 --> 00:01:16,013
from GDPR to incident response,

32
00:01:17,450 --> 00:01:20,570
With me today are two amazing veterans

33
00:01:20,570 --> 00:01:23,119
and in privacy and cybersecurity,

34
00:01:23,120 --> 00:01:25,083
Michelle Dennedy and Melanie Ensign.

35
00:01:26,720 --> 00:01:28,160
Micky?

36
00:01:28,160 --> 00:01:30,110
- Hi everyone Michelle Dennedy here.

37
00:01:30,110 --> 00:01:31,720
I am trained as an attorney.

38
00:01:31,720 --> 00:01:33,789
So I come at it from
that side of the house.

39
00:01:33,790 --> 00:01:35,830
I was the chief privacy officer

40
00:01:35,830 --> 00:01:37,940
at a lot of the big
infrastructure companies

41
00:01:37,940 --> 00:01:41,220
on Oracle McAfee, Intel, Cisco

42
00:01:41,220 --> 00:01:43,990
and then dove off into
the world of startup land.

43
00:01:43,990 --> 00:01:46,640
Yeah, and I am currently acting as the CEO

44
00:01:46,640 --> 00:01:50,340
of a stealth privacy company
for the developer community

45
00:01:50,340 --> 00:01:53,500
as well as a consulting
practice called Privatus

46
00:01:53,500 --> 00:01:57,473
where we deal with wicked privacy
and all of its complexity.

47
00:01:58,810 --> 00:02:00,363
- Thanks Micky, Melanie.

48
00:02:01,350 --> 00:02:02,183
- Hi everyone.

49
00:02:02,183 --> 00:02:03,020
I'm Melanie Ensign.

50
00:02:03,020 --> 00:02:06,160
I am the founder and CEO of discernible.

51
00:02:06,160 --> 00:02:09,900
We are a specialized
communications consulting firm

52
00:02:10,930 --> 00:02:13,330
dedicated specifically to security,

53
00:02:13,330 --> 00:02:15,140
privacy and risk organization.

54
00:02:15,140 --> 00:02:17,170
So everything from incident response

55
00:02:17,170 --> 00:02:20,030
to application security,
bug bounty programs

56
00:02:21,110 --> 00:02:25,640
various privacy issues as well as

57
00:02:26,880 --> 00:02:30,000
product development guidance
and things like that.

58
00:02:30,000 --> 00:02:31,920
Particularly in terms of
how do you communicate

59
00:02:31,920 --> 00:02:34,839
about security and privacy to consumers

60
00:02:34,840 --> 00:02:37,660
and to stakeholders who
are not domain experts?

61
00:02:37,660 --> 00:02:41,146
So everything from website
copy to in-app copy

62
00:02:41,146 --> 00:02:44,279
in product experiences
and things like that.

63
00:02:44,280 --> 00:02:47,140
So we are specifically focused on helping

64
00:02:47,140 --> 00:02:49,535
security and privacy organizations

65
00:02:49,535 --> 00:02:52,530
operationalize more effectively

66
00:02:52,530 --> 00:02:55,400
and to become better
communicators themselves.

67
00:02:55,400 --> 00:02:57,300
And in my personal capacity,

68
00:02:57,300 --> 00:03:00,240
I lead the press department for DEF CON

69
00:03:00,240 --> 00:03:01,570
which many of you probably know

70
00:03:01,570 --> 00:03:04,549
it is the world's largest
hacker conference.

71
00:03:04,550 --> 00:03:07,480
So, I have spent the last 15 or so years

72
00:03:07,480 --> 00:03:11,053
working in security and
privacy at some big companies.

73
00:03:11,053 --> 00:03:14,800
Earned a few battle scars along the way,

74
00:03:14,800 --> 00:03:16,700
convinced me to try entrepreneurship.

75
00:03:16,700 --> 00:03:19,859
And now I'm helping both
large and small companies

76
00:03:19,860 --> 00:03:22,460
with their security and
privacy communication needs.

77
00:03:23,580 --> 00:03:25,010
- Thank you for that Mel.

78
00:03:25,010 --> 00:03:27,760
I think we should just dig deep into..

79
00:03:27,760 --> 00:03:29,620
get into the definitions, right?

80
00:03:29,620 --> 00:03:34,510
So two very distinct
domains, privacy and security

81
00:03:34,510 --> 00:03:36,079
but they do intersect,

82
00:03:36,080 --> 00:03:39,660
how do you ladies think about the two?

83
00:03:39,660 --> 00:03:42,843
These two domains separately
and where they intersect?

84
00:03:44,250 --> 00:03:45,083
Mel?

85
00:03:45,083 --> 00:03:46,000
- You wanna start with security

86
00:03:46,000 --> 00:03:47,193
It's big.

87
00:03:50,340 --> 00:03:54,380
- So I'll start just from a
communications perspective.

88
00:03:54,380 --> 00:03:59,210
I have to simultaneously see
them as the same and different

89
00:03:59,210 --> 00:04:01,130
and that is because of the way

90
00:04:01,130 --> 00:04:03,810
that various stakeholders
think about these issues.

91
00:04:03,810 --> 00:04:06,630
So, a lot of consumers
and a lot of regulators

92
00:04:06,630 --> 00:04:10,109
and government officials
sees security and privacy,

93
00:04:10,110 --> 00:04:12,220
as well as other elements of safety

94
00:04:12,220 --> 00:04:15,337
as being just multiple
sides of the same coin

95
00:04:15,337 --> 00:04:18,680
or sometimes even equivalent concepts.

96
00:04:18,680 --> 00:04:19,940
Particularly when we're talking about them

97
00:04:19,940 --> 00:04:21,140
in the abstract sense, right?

98
00:04:21,140 --> 00:04:23,349
Like online safety kind of encompasses

99
00:04:23,350 --> 00:04:25,990
all of those things in
terms of the experience

100
00:04:25,990 --> 00:04:27,350
and expectations that you have

101
00:04:27,350 --> 00:04:30,190
when you are interacting online

102
00:04:30,190 --> 00:04:31,890
with a company or other individuals.

103
00:04:31,890 --> 00:04:33,469
And so from that perspective,

104
00:04:33,470 --> 00:04:36,210
I have to be mindful that there is

105
00:04:36,210 --> 00:04:39,770
an entire very large
population of the world

106
00:04:39,770 --> 00:04:41,729
that sees them as one and the same

107
00:04:41,730 --> 00:04:44,620
in the way that they talk about
them in their expectations.

108
00:04:44,620 --> 00:04:47,350
However, internally within an organization

109
00:04:47,350 --> 00:04:49,470
we have to think about them differently

110
00:04:49,470 --> 00:04:52,873
in order to operationalize
them appropriately.

111
00:04:52,873 --> 00:04:57,140
we need dedicated teams
focused on security.

112
00:04:57,140 --> 00:05:00,270
We need dedicated teams
focused exclusively on privacy.

113
00:05:00,270 --> 00:05:01,799
We need teams that are looking

114
00:05:01,800 --> 00:05:05,340
at things like anti abuse
and other forms of safety.

115
00:05:05,340 --> 00:05:07,640
And we actually need
them to be specialized

116
00:05:07,640 --> 00:05:09,380
and focused on those areas,

117
00:05:09,380 --> 00:05:12,590
but then come together
and work collaboratively

118
00:05:12,590 --> 00:05:16,609
to create a holistic and safe
experience for consumers.

119
00:05:16,610 --> 00:05:19,080
And so I have to think, like I said

120
00:05:19,080 --> 00:05:21,460
simultaneously along both of those tracks,

121
00:05:21,460 --> 00:05:25,000
but as experts when we are
working on these problems,

122
00:05:25,000 --> 00:05:27,270
we have to be able to separate them.

123
00:05:27,270 --> 00:05:30,450
Otherwise we can't ensure
that each perspective

124
00:05:30,450 --> 00:05:32,150
is getting its due diligence

125
00:05:32,150 --> 00:05:36,429
and the dedicated thought process

126
00:05:36,430 --> 00:05:38,283
that it really deserves and needs.

127
00:05:39,490 --> 00:05:42,900
- See, there's a reason for
the communicator who went first

128
00:05:42,900 --> 00:05:44,131
because it takes..

129
00:05:44,131 --> 00:05:46,030
- I'm setting the tone, Michelle.

130
00:05:46,030 --> 00:05:49,099
Now you may follow
- It's good.

131
00:05:49,100 --> 00:05:51,640
And I think it's so
important to get that tone in

132
00:05:51,640 --> 00:05:56,640
to understand in any sort of development,

133
00:05:56,750 --> 00:05:58,980
since this is, the RSI Crowd,

134
00:05:58,980 --> 00:06:01,020
you break down those elements.

135
00:06:01,020 --> 00:06:02,430
So get the contextual picture.

136
00:06:02,430 --> 00:06:04,810
As Mel has said, there's an output

137
00:06:04,810 --> 00:06:08,910
of fairness of treating
human beings with respect

138
00:06:08,910 --> 00:06:11,410
of the human rights aspect of privacy,

139
00:06:11,410 --> 00:06:13,380
which is every one of us

140
00:06:13,380 --> 00:06:16,350
has an individual story online, offline

141
00:06:16,350 --> 00:06:17,420
and long after we're gone.

142
00:06:17,420 --> 00:06:20,620
Our stories are the thing that endures.

143
00:06:20,620 --> 00:06:25,530
And so the integrity of that
through line is privacy.

144
00:06:25,530 --> 00:06:28,070
I think the content now that said

145
00:06:28,070 --> 00:06:30,570
that sounds like either it's impossible

146
00:06:30,570 --> 00:06:34,210
because the surveillance
capitalist society is here to spy

147
00:06:34,210 --> 00:06:35,989
on us and the board is real

148
00:06:35,990 --> 00:06:38,129
and we are we in the
matrix, are we not are up?

149
00:06:38,129 --> 00:06:40,700
If you start to freak out and say

150
00:06:40,700 --> 00:06:42,680
there's security versus privacy,

151
00:06:42,680 --> 00:06:43,970
and you start to define it

152
00:06:43,970 --> 00:06:47,660
as security is the lockdown
of certain categories

153
00:06:47,660 --> 00:06:51,240
of certain subsets of
certain jurisdictions of data

154
00:06:51,240 --> 00:06:54,010
you start to play this whack-a-mole game.

155
00:06:54,010 --> 00:06:55,469
when you bring it all together again,

156
00:06:55,470 --> 00:06:58,190
and you say privacy if you look at it

157
00:06:58,190 --> 00:06:59,623
through the user story,

158
00:06:59,623 --> 00:07:02,250
as you're looking at an
architectural flow diagram

159
00:07:02,250 --> 00:07:03,950
or activity diagram

160
00:07:03,950 --> 00:07:07,000
as privacy is really the authorized

161
00:07:07,880 --> 00:07:09,430
and that is throughout its lifestyle.

162
00:07:09,430 --> 00:07:13,280
Imagine that sticking that
policy, sticking to your story

163
00:07:13,280 --> 00:07:15,359
the authorized processing

164
00:07:15,360 --> 00:07:18,910
of personally identifiable
or personal data,

165
00:07:18,910 --> 00:07:22,363
according to moral, ethical, legal,

166
00:07:22,363 --> 00:07:24,760
and sustainable principles.

167
00:07:24,760 --> 00:07:27,110
And this is where security comes back in

168
00:07:27,110 --> 00:07:30,690
because morality is a decision
of, should we be safe?

169
00:07:30,690 --> 00:07:31,719
When should we be safe?

170
00:07:31,720 --> 00:07:33,320
Who should be allowed in ?

171
00:07:33,320 --> 00:07:35,099
Ethics are your brand.

172
00:07:35,100 --> 00:07:36,986
This is why you hire Discernible.

173
00:07:36,986 --> 00:07:38,840
What is your story?

174
00:07:38,840 --> 00:07:40,039
What is your preference?

175
00:07:40,040 --> 00:07:41,620
How much should I trust you

176
00:07:41,620 --> 00:07:45,461
with what kind of sensitive
information or storyline?

177
00:07:45,461 --> 00:07:49,520
Legal is usually the first
thing that you reach to

178
00:07:49,520 --> 00:07:52,490
and you see a cadre of
privacy lawyers out there

179
00:07:52,490 --> 00:07:54,170
talking about legal.

180
00:07:54,170 --> 00:07:55,220
They need more Lord as

181
00:07:55,220 --> 00:07:58,030
as their professors to say,
it's an experiential thing.

182
00:07:58,030 --> 00:08:00,919
It's not starting with,
what are the limitations

183
00:08:00,920 --> 00:08:02,560
and what can I get away with?

184
00:08:02,560 --> 00:08:07,400
What's the minimum viable,
legal thing that I can do today?

185
00:08:07,400 --> 00:08:09,310
And then finally, sustainability,

186
00:08:09,310 --> 00:08:12,910
I think, is this something
that your brand wants

187
00:08:12,910 --> 00:08:15,000
to continue contributing to?

188
00:08:15,000 --> 00:08:17,850
You don't run up and like
get compliant with GDPR?

189
00:08:17,850 --> 00:08:21,200
Is this a topic or is
this a type of output

190
00:08:21,200 --> 00:08:23,060
that's going to be valuable.

191
00:08:23,060 --> 00:08:25,790
If you're a bank, I'm assuming
you want that depositor

192
00:08:25,790 --> 00:08:27,430
to put in more deposits

193
00:08:27,430 --> 00:08:29,450
or different variety of types of deposits

194
00:08:29,450 --> 00:08:30,610
or as they get more

195
00:08:30,610 --> 00:08:33,929
and more offloaded put more and
more capital into your bank.

196
00:08:33,929 --> 00:08:37,159
All of that output goes into that stack

197
00:08:37,159 --> 00:08:38,760
of what is authorized

198
00:08:38,760 --> 00:08:43,760
what information is personally
identifiable and personal.

199
00:08:44,070 --> 00:08:46,550
And then how do we apply these principles?

200
00:08:46,550 --> 00:08:48,319
So design it, if you can imagine

201
00:08:48,320 --> 00:08:50,990
like I'm waving my arms around like

202
00:08:50,990 --> 00:08:53,760
as if it was a flow diagram.

203
00:08:53,760 --> 00:08:55,498
And so when we're talking about technical

204
00:08:55,498 --> 00:08:59,170
the personal and the
storyline in front of that

205
00:08:59,170 --> 00:09:01,880
you're constantly looking
across that soft system

206
00:09:01,880 --> 00:09:04,320
and saying, where do humans
interact with machines?

207
00:09:04,320 --> 00:09:06,670
Where does policy and expectation live?

208
00:09:06,670 --> 00:09:08,599
And then how do you run that system

209
00:09:08,600 --> 00:09:10,520
as a symphony together rather than one

210
00:09:10,520 --> 00:09:13,180
or two beeping instruments going off

211
00:09:13,180 --> 00:09:14,209
and playing their own tune?

212
00:09:14,210 --> 00:09:16,120
So it's a long-winded answer

213
00:09:16,120 --> 00:09:18,380
but I think the getting
down to brass tacks

214
00:09:18,380 --> 00:09:22,090
about what is security, how
are we operationalizing that?

215
00:09:22,090 --> 00:09:23,470
What is privacy?

216
00:09:23,470 --> 00:09:25,390
How are we operationalizing that?

217
00:09:25,390 --> 00:09:28,110
And importantly, what
substance are we using

218
00:09:28,110 --> 00:09:30,023
to glue these capabilities together?

219
00:09:31,430 --> 00:09:34,270
- It was such an elegant way
to explain that distinction

220
00:09:34,270 --> 00:09:37,800
Micky, I love that framework
that you shared with us,

221
00:09:37,800 --> 00:09:42,800
none of the usual boring
proxies just security.

222
00:09:43,690 --> 00:09:44,986
Privacy is dead.

223
00:09:44,986 --> 00:09:49,410
security is better than privacy or whatnot

224
00:09:49,410 --> 00:09:50,242
- Exhausting.

225
00:09:50,243 --> 00:09:55,243
- Exactly, so, I do appreciate
that the elegant explanation

226
00:09:55,530 --> 00:09:57,327
that you so kindly shared with us.

227
00:09:57,327 --> 00:10:01,420
So, let's dig deep then
into what is privacy tech

228
00:10:01,420 --> 00:10:03,300
and full disclosure at
the rise of privacy tech.

229
00:10:03,300 --> 00:10:05,080
We are doing that work this year of

230
00:10:05,081 --> 00:10:09,410
of defining the landscape
and with these ladies

231
00:10:09,410 --> 00:10:11,170
as members of the working group

232
00:10:11,170 --> 00:10:15,349
and looking at them first,
defining what privacy tickets

233
00:10:15,350 --> 00:10:19,558
and then categorizing the
different startups in this space.

234
00:10:19,558 --> 00:10:21,370
If we look at cybersecurity

235
00:10:21,370 --> 00:10:23,760
it's such a mature landscape, right?

236
00:10:23,760 --> 00:10:26,370
Like you have your have your firewalls

237
00:10:26,370 --> 00:10:28,400
your virtual firewalls, your endpoints,

238
00:10:28,400 --> 00:10:31,453
your network, your cloud
security, or container security.

239
00:10:32,630 --> 00:10:36,046
And so what is privacy tech then?

240
00:10:36,046 --> 00:10:38,327
I'd love to hear about

241
00:10:38,327 --> 00:10:40,100
I know you've done this work already,

242
00:10:40,100 --> 00:10:41,370
but I'd love to hear.

243
00:10:41,370 --> 00:10:42,690
I'd love to have you share it

244
00:10:42,690 --> 00:10:46,733
with the audience of what
you think of as privacy tech.

245
00:10:48,960 --> 00:10:50,312
- I'll kick us off on this one.

246
00:10:50,312 --> 00:10:52,080
I think it will stick

247
00:10:52,080 --> 00:10:55,199
to my activity flow
diagram and architectural.

248
00:10:55,199 --> 00:10:57,872
If you look at the
players in the marketplace

249
00:10:57,872 --> 00:11:00,849
self-identifying as privacy tech today

250
00:11:00,850 --> 00:11:03,610
most of them are in the legal category.

251
00:11:03,610 --> 00:11:07,510
What have we been spending on
to become or remain compliant

252
00:11:07,510 --> 00:11:11,750
with the big sticker shock of GDPR, LGPD

253
00:11:11,750 --> 00:11:13,490
and all the California Shaizer

254
00:11:13,490 --> 00:11:15,440
and there's more stuff coming, right?

255
00:11:15,440 --> 00:11:19,750
So it's like we knew for
the last 20, 25 years

256
00:11:19,750 --> 00:11:21,860
we were supposed to be
doing something outside

257
00:11:21,860 --> 00:11:23,140
and commercial world and the government.

258
00:11:23,140 --> 00:11:24,400
They were supposed to be doing stuff

259
00:11:24,400 --> 00:11:26,250
for the last 70 years

260
00:11:26,250 --> 00:11:29,080
now there's a big fine on your customers.

261
00:11:29,080 --> 00:11:31,890
It's gonna have some of
that sustainability impact.

262
00:11:31,890 --> 00:11:33,980
So most of the privacy tech
that you're seeing today

263
00:11:33,980 --> 00:11:36,570
is automating things that lawyers buy .

264
00:11:36,570 --> 00:11:39,380
Services, they buy reporting services,

265
00:11:39,380 --> 00:11:42,040
tracking services, and metrics.

266
00:11:42,040 --> 00:11:44,579
So we're not a desperate hunt for metrics.

267
00:11:44,580 --> 00:11:46,160
Where's my data?

268
00:11:46,160 --> 00:11:47,540
What is it doing?

269
00:11:47,540 --> 00:11:49,459
I think the overall landscape,

270
00:11:49,460 --> 00:11:51,120
you should think about privacy tech

271
00:11:51,120 --> 00:11:55,060
as a wide open blue ocean of innovation.

272
00:11:55,060 --> 00:11:56,540
If you cover that whole spectrum

273
00:11:56,540 --> 00:11:59,329
of what is authorization
and how does it stick

274
00:11:59,330 --> 00:12:01,420
to micro stories that people are telling

275
00:12:01,420 --> 00:12:04,760
with more and more and more
capabilities for surveillance.

276
00:12:04,760 --> 00:12:06,600
That doesn't mean privacy goes away.

277
00:12:06,600 --> 00:12:09,310
That means come on in little innovator

278
00:12:09,310 --> 00:12:12,699
your problem just got more
delicious and more impactful

279
00:12:12,700 --> 00:12:13,650
and more important.

280
00:12:14,580 --> 00:12:17,210
what's processing, Oh gosh

281
00:12:17,210 --> 00:12:19,340
we've been in a pandemic
for the last year.

282
00:12:19,340 --> 00:12:21,610
No one should be saying anything about

283
00:12:21,610 --> 00:12:24,830
we might get into cloud
you're in the cloud baby.

284
00:12:24,830 --> 00:12:27,395
So how does that alter and how does..

285
00:12:27,395 --> 00:12:31,090
what are the tools that we have
to apply to that storyline?

286
00:12:31,090 --> 00:12:33,456
And then how are we actually tracking

287
00:12:33,456 --> 00:12:37,220
and marking that we're
using this data efficiently?

288
00:12:37,220 --> 00:12:40,484
The old fashioned marketing

289
00:12:40,484 --> 00:12:44,250
I don't even know what a
tome or I think a fallacy

290
00:12:44,250 --> 00:12:47,690
was that everyone knows that
50% of marketing is wasted.

291
00:12:47,690 --> 00:12:49,660
We just don't know which 50%.

292
00:12:49,660 --> 00:12:52,530
And I think that story has to go away

293
00:12:52,530 --> 00:12:55,449
in this age of massive data.

294
00:12:55,450 --> 00:12:57,610
And you have to figure out where along..

295
00:12:57,610 --> 00:12:58,670
What are the tools we need

296
00:12:58,670 --> 00:13:00,719
to understand what we're collecting?

297
00:13:00,720 --> 00:13:02,830
Are we collecting it in
our own best interest?

298
00:13:02,830 --> 00:13:04,790
Are we collecting it from the user story

299
00:13:04,790 --> 00:13:07,719
of the individual whose
story is being kept

300
00:13:07,720 --> 00:13:09,257
or held in fiduciary care?

301
00:13:09,257 --> 00:13:11,180
And why?

302
00:13:11,180 --> 00:13:13,630
Like, don't be afraid to ask the question

303
00:13:13,630 --> 00:13:16,070
why do we have all this crap?

304
00:13:16,070 --> 00:13:18,570
And how do we get rid of
it when we don't need it?

305
00:13:20,550 --> 00:13:21,979
- Thanks Mickey. Mel?

306
00:13:21,980 --> 00:13:23,907
- Well, yeah, this is why I went first

307
00:13:23,907 --> 00:13:26,863
on the first question
following Michelle just sucks.

308
00:13:29,535 --> 00:13:32,270
But the one thing that I will say

309
00:13:32,270 --> 00:13:33,850
and I mean truthfully,

310
00:13:33,850 --> 00:13:36,100
I'm not as generous as as Mickey

311
00:13:36,100 --> 00:13:39,890
with some of the inclusiveness,

312
00:13:39,890 --> 00:13:42,800
I guess in terms of how I
think about privacy tech

313
00:13:44,040 --> 00:13:48,535
because I think a lot of those
legal workflow technologies

314
00:13:48,535 --> 00:13:51,990
aren't actually enabling
or improving privacy

315
00:13:51,990 --> 00:13:54,580
it is a workflow technology
that could just as

316
00:13:54,580 --> 00:13:56,880
easily be applied to any
other legal requirement

317
00:13:56,880 --> 00:13:59,050
that needs to be tracked and measured.

318
00:13:59,050 --> 00:14:02,020
And so what I really focus on

319
00:14:02,020 --> 00:14:06,590
is where is the technology that
is actually raising the bar?

320
00:14:06,590 --> 00:14:09,440
The technology that is
actually putting privacy first

321
00:14:09,440 --> 00:14:12,070
in terms of why it exists, right?

322
00:14:12,070 --> 00:14:14,760
We have a lot of workflow
solutions that exist

323
00:14:14,760 --> 00:14:16,260
because laws exist.

324
00:14:16,260 --> 00:14:20,944
I'm looking for technology
that exists to enable privacy.

325
00:14:20,945 --> 00:14:25,945
You know, that it's primary
purpose for being in the market

326
00:14:26,900 --> 00:14:30,780
is actually to protect
personally identifiable

327
00:14:30,780 --> 00:14:34,500
or personal data or to
give people more control

328
00:14:34,500 --> 00:14:35,920
over their data.

329
00:14:35,920 --> 00:14:39,760
And so, when I think about
what's actually privacy tech

330
00:14:39,760 --> 00:14:43,890
and the reason I'm so specific
and somewhat exclusive

331
00:14:43,890 --> 00:14:47,990
is because I really want
to fuel that innovation.

332
00:14:47,990 --> 00:14:51,040
And I really want to help
elevate those companies

333
00:14:51,040 --> 00:14:53,000
that are doing something different.

334
00:14:53,000 --> 00:14:57,830
We've had automated workflow
software for a gazillion years.

335
00:14:57,830 --> 00:15:01,570
We just now have new
laws that we apply it to.

336
00:15:01,570 --> 00:15:04,700
And so what I'm really excited about,

337
00:15:04,700 --> 00:15:07,030
in terms of privacy tech is,

338
00:15:07,030 --> 00:15:09,670
Mickey, like you said those
innovators that are coming in

339
00:15:09,670 --> 00:15:13,300
and actually solving new problems.

340
00:15:13,300 --> 00:15:16,532
where they're not just
putting a privacy sticker on,

341
00:15:17,750 --> 00:15:20,469
orchestration software
that's existed for 20 years

342
00:15:20,469 --> 00:15:22,420
but actually focused on

343
00:15:22,420 --> 00:15:25,160
what can we build that
makes privacy better?

344
00:15:25,160 --> 00:15:26,420
That makes it more accessible?

345
00:15:26,420 --> 00:15:28,550
That makes it easier for people to use?

346
00:15:28,550 --> 00:15:33,550
And that actually enables
the safe processing of data,

347
00:15:35,043 --> 00:15:37,420
the safe analytics of data

348
00:15:37,420 --> 00:15:41,500
and being able to do something different.

349
00:15:41,500 --> 00:15:43,570
And it's one of the things that

350
00:15:43,570 --> 00:15:46,130
when we even just talk about innovation

351
00:15:46,130 --> 00:15:49,180
the word innovation almost
has no meaning at this point,

352
00:15:49,180 --> 00:15:52,069
because anything people launch

353
00:15:53,040 --> 00:15:54,719
they're like this was so innovative,

354
00:15:54,720 --> 00:15:56,190
we changed the font color.

355
00:15:56,190 --> 00:15:59,880
And now you have 17 new
boxes that you have to check

356
00:15:59,880 --> 00:16:01,640
in order to get any type of privacy

357
00:16:01,640 --> 00:16:03,620
from this new feature that we've lost.

358
00:16:03,620 --> 00:16:07,830
Like that it's not privacy
tech and it's not innovation.

359
00:16:07,830 --> 00:16:11,120
So I, I look specifically
for the technologies

360
00:16:11,120 --> 00:16:16,120
that are changing the way
that we operate businesses

361
00:16:16,510 --> 00:16:18,550
changing the way that consumers interact

362
00:16:18,550 --> 00:16:21,130
with technology and changing the control

363
00:16:21,130 --> 00:16:23,480
and choice that consumers
have over their data.

364
00:16:24,904 --> 00:16:25,737
- I love this so much.

365
00:16:25,737 --> 00:16:28,709
And it's like if we were
in a real room together

366
00:16:28,710 --> 00:16:32,250
it'd be so good because just
these two women together

367
00:16:32,250 --> 00:16:35,380
I don't think people
recognize how innovative

368
00:16:35,380 --> 00:16:36,783
that thinking is Melanie.

369
00:16:38,207 --> 00:16:40,850
- That's so depressing.
That's so depressing.

370
00:16:40,850 --> 00:16:44,290
So depressing that this
is not common sense.

371
00:16:44,290 --> 00:16:46,880
- Why is this so exciting,
its not common sense exactly

372
00:16:46,880 --> 00:16:49,320
because you're a hundred percent right.

373
00:16:49,320 --> 00:16:54,090
And I think, I don't know
how many technological slaps

374
00:16:54,090 --> 00:16:58,150
to the head we need to
recognize here we are.

375
00:16:58,150 --> 00:17:01,360
I mean, the internet,
the commercialization

376
00:17:01,360 --> 00:17:05,109
from about 1995 onward
of the internet itself

377
00:17:05,109 --> 00:17:08,550
should have been a
technological wake up call

378
00:17:08,550 --> 00:17:10,859
calling for international treaties.

379
00:17:10,859 --> 00:17:13,879
We did this once with piracy in the 1500's

380
00:17:13,880 --> 00:17:16,819
our laws have not gotten
up for the internet

381
00:17:16,819 --> 00:17:20,189
up to the speed of 1500 shipping laws.

382
00:17:20,190 --> 00:17:23,420
Folks, innovation needs to happen there

383
00:17:23,420 --> 00:17:25,349
from a technological perspective.

384
00:17:25,349 --> 00:17:27,369
We've tried to play this cat and mouse

385
00:17:27,369 --> 00:17:30,630
with stickers saying, Oh,
here's some privacy for you.

386
00:17:30,631 --> 00:17:32,320
Hey, guess what?

387
00:17:32,320 --> 00:17:34,110
We have a transparency law, OMG

388
00:17:34,110 --> 00:17:37,260
you can send an email and get a log dump.

389
00:17:37,260 --> 00:17:39,520
That is not privacy.

390
00:17:39,520 --> 00:17:41,430
That is not control.

391
00:17:41,430 --> 00:17:44,020
consenting to things over
which you have zero control

392
00:17:44,020 --> 00:17:45,530
is not privacy.

393
00:17:45,530 --> 00:17:46,830
It's a show game.

394
00:17:46,830 --> 00:17:48,169
So the real innovation,

395
00:17:48,170 --> 00:17:51,210
I think the good news and the bad news,

396
00:17:51,210 --> 00:17:53,880
the good news is there's a lot of room

397
00:17:53,880 --> 00:17:57,040
for true innovation and
fresh thinking here.

398
00:17:57,040 --> 00:17:59,090
The bad news is we'll get into

399
00:17:59,090 --> 00:18:01,689
I'm sure with Lourdes
is where's the market.

400
00:18:01,690 --> 00:18:04,260
Who's gonna be the buyer of this new tech

401
00:18:04,260 --> 00:18:07,150
because with the advent of quantum

402
00:18:07,150 --> 00:18:10,040
and it's not just the buzz
word of quantum computing

403
00:18:10,040 --> 00:18:14,389
and it's not just big data,
and it's not just analytics

404
00:18:14,390 --> 00:18:18,380
it's truly every piece of silica that is

405
00:18:18,380 --> 00:18:21,280
in the data centers today will be replaced

406
00:18:21,280 --> 00:18:23,320
in the next seven to 10 years

407
00:18:23,320 --> 00:18:26,439
simply because things are
gonna have to get cooler.

408
00:18:26,440 --> 00:18:28,240
Things are gonna have
to be more efficient.

409
00:18:28,240 --> 00:18:30,540
We're gonna have more and
more people on the grid

410
00:18:30,540 --> 00:18:34,370
on the electrical grid, as
well as on the compute grid.

411
00:18:34,370 --> 00:18:38,040
And we are so dependent as humanity

412
00:18:38,040 --> 00:18:41,330
upon data for our day to day lives.

413
00:18:41,330 --> 00:18:42,935
So our commerce has changed.

414
00:18:42,935 --> 00:18:46,690
Our culture has changed, our
governments are changing.

415
00:18:46,690 --> 00:18:49,650
And so the data stories that rise up

416
00:18:49,650 --> 00:18:52,280
from the 7 billion souls
running around on the planet

417
00:18:52,280 --> 00:18:55,860
at any given time are demanding a change.

418
00:18:55,860 --> 00:18:58,260
And that sounds very ooey-gooey-fooey.

419
00:18:58,260 --> 00:19:01,730
But the reality is when you
have the texture of a cubit

420
00:19:01,730 --> 00:19:04,560
and you're able to simultaneously process

421
00:19:04,560 --> 00:19:07,179
much larger chunks of data,

422
00:19:07,180 --> 00:19:10,390
can you imagine walking up to
that data center and going,

423
00:19:10,390 --> 00:19:13,428
hi, my password is Cordy one, two, three,

424
00:19:13,428 --> 00:19:15,440
tell me everything you know,

425
00:19:15,440 --> 00:19:16,590
it's not gonna work that way.

426
00:19:16,590 --> 00:19:19,446
So we have to start thinking it more as..

427
00:19:19,446 --> 00:19:22,850
like the the ancient physicians did

428
00:19:22,850 --> 00:19:25,080
at first, you just had an external body.

429
00:19:25,080 --> 00:19:26,639
And then they started
looking inside and saying,

430
00:19:26,640 --> 00:19:28,940
Oh there's different pieces here.

431
00:19:28,940 --> 00:19:32,100
And then, Oh, they do
different things in here.

432
00:19:32,100 --> 00:19:33,399
Every single piece

433
00:19:33,400 --> 00:19:37,180
of your virtual data
center needs to be upgraded

434
00:19:37,180 --> 00:19:39,020
how you're doing consent management

435
00:19:39,020 --> 00:19:41,070
how you're doing proportionality control

436
00:19:41,070 --> 00:19:43,409
what you're storing, how you're sharing it

437
00:19:43,410 --> 00:19:46,440
with whom you're sharing
and under what conditions

438
00:19:46,440 --> 00:19:49,670
all of those questions
are operational questions.

439
00:19:49,670 --> 00:19:52,300
Some of them will be automated.

440
00:19:52,300 --> 00:19:54,550
Others will become so boring

441
00:19:54,550 --> 00:19:56,460
and rogue that we can use things

442
00:19:56,460 --> 00:19:59,410
like MLAI to extend their,

443
00:19:59,410 --> 00:20:03,130
at least their reach if not their efficacy

444
00:20:03,130 --> 00:20:06,280
and really thinking through
on an executive level,

445
00:20:06,280 --> 00:20:07,830
what is this worth?

446
00:20:07,830 --> 00:20:09,129
What is the asset?

447
00:20:09,130 --> 00:20:11,240
If we can do this for currency exchange

448
00:20:11,240 --> 00:20:13,310
we can do it for data stories.

449
00:20:13,310 --> 00:20:16,550
And, but we just simply have
to invest in the technology

450
00:20:16,550 --> 00:20:19,710
and you have to let go of this fantasy

451
00:20:19,710 --> 00:20:22,130
that somehow we're gonna
reveal the data centers

452
00:20:22,130 --> 00:20:24,510
of the Facebooks and the
Amazons of the world.

453
00:20:24,510 --> 00:20:26,140
And suddenly we're gonna be healed.

454
00:20:26,140 --> 00:20:28,280
We're gonna be terrified
when we actually see

455
00:20:28,280 --> 00:20:31,110
what's going on under the
covers, but it's not too late.

456
00:20:31,110 --> 00:20:32,030
That's the exciting thing

457
00:20:32,030 --> 00:20:34,590
for a real innovator is
you run into the fire.

458
00:20:34,590 --> 00:20:35,939
You don't run away from it.

459
00:20:37,639 --> 00:20:40,699
- And there's so much
fire to address right now

460
00:20:40,700 --> 00:20:43,540
in front of bringing it all together.

461
00:20:43,540 --> 00:20:45,540
It sounds like this,

462
00:20:45,540 --> 00:20:47,570
at the highest level, privacy tech

463
00:20:48,862 --> 00:20:53,862
is the term that we've been
using to label this new tools

464
00:20:54,600 --> 00:20:57,820
and startups that are
solving for privacy problems.

465
00:20:57,820 --> 00:21:00,649
May they be the workflow,
automation, compliance

466
00:21:00,650 --> 00:21:03,610
privacy, data protection,
type of problems.

467
00:21:03,610 --> 00:21:08,120
May they be the, the
processing, data analytics,

468
00:21:08,120 --> 00:21:10,469
data mapping this source type of problems.

469
00:21:10,470 --> 00:21:13,240
And then you have the B2B
versus B2C distinction

470
00:21:13,240 --> 00:21:15,840
that Melanie was alluding to earlier,

471
00:21:15,840 --> 00:21:17,080
are you really solving

472
00:21:17,080 --> 00:21:20,447
for privacy individual privacy problems?

473
00:21:20,447 --> 00:21:25,070
And we are seeing some
B2C type of startups

474
00:21:25,070 --> 00:21:27,790
address that even though the B2B space

475
00:21:27,790 --> 00:21:30,031
is way ahead B2C right now.

476
00:21:30,031 --> 00:21:35,031
And then I guess you have the legal stuff,

477
00:21:35,760 --> 00:21:38,500
the compliance stuff, the mapping,

478
00:21:38,500 --> 00:21:41,500
and Mickey beyond and then
there's a security, right?

479
00:21:41,500 --> 00:21:44,040
There's the overlap between
cybersecurity and privacy

480
00:21:44,040 --> 00:21:47,594
where there are some tools
in the security space

481
00:21:47,594 --> 00:21:49,930
that have great use cases

482
00:21:49,930 --> 00:21:53,370
for privacy, homomorphic encryption

483
00:21:53,370 --> 00:21:54,793
and those sorts of things.

484
00:21:56,627 --> 00:21:58,610
Micky and Melanie, what are other areas?

485
00:21:58,610 --> 00:22:01,350
What are other categories that
you're most excited about?

486
00:22:01,350 --> 00:22:04,370
And I particularly wanna
talk bout shifting left

487
00:22:04,370 --> 00:22:07,100
when it comes to privacy
and what that looks like

488
00:22:07,100 --> 00:22:08,449
for developers given the people

489
00:22:08,450 --> 00:22:10,560
that are listening to us today.

490
00:22:10,560 --> 00:22:12,210
- Yeah. This is a softball court.

491
00:22:14,490 --> 00:22:16,812
I have a small bias.

492
00:22:16,812 --> 00:22:18,449
That is a massive bias

493
00:22:18,450 --> 00:22:23,450
is that I'm really focused
on developer tools.

494
00:22:24,050 --> 00:22:26,680
And I don't just mean the current people

495
00:22:26,680 --> 00:22:29,790
who are our code jockeys or dev ops.

496
00:22:29,790 --> 00:22:33,310
People who are implementing
things are quality people

497
00:22:33,310 --> 00:22:34,270
or even architects.

498
00:22:34,270 --> 00:22:37,410
I mean, when I say developers,
please think expansively

499
00:22:37,410 --> 00:22:40,470
and include, please include
your privacy officers

500
00:22:40,470 --> 00:22:41,650
even if they're legal people

501
00:22:41,650 --> 00:22:44,810
and they are intimidated by technology,

502
00:22:44,810 --> 00:22:45,810
bring them into the room

503
00:22:45,810 --> 00:22:50,520
and think about the development
from the scoping workshop.

504
00:22:50,520 --> 00:22:52,430
So if you think about early on,

505
00:22:52,430 --> 00:22:56,270
in compute before you put
any code into production

506
00:22:56,270 --> 00:22:58,100
there was quite a bit of thinking about

507
00:22:58,100 --> 00:22:59,459
what were the business rules

508
00:22:59,460 --> 00:23:01,080
what were the goals and the outcomes,

509
00:23:01,080 --> 00:23:02,800
what was a failure?

510
00:23:02,800 --> 00:23:05,280
And then you designed
accordingly because you had,

511
00:23:05,280 --> 00:23:07,990
the punch cards, you kind
of had a one-shot thing

512
00:23:07,990 --> 00:23:09,113
or it was gonna be weeks and weeks

513
00:23:09,113 --> 00:23:12,100
or years and years in the future.

514
00:23:12,100 --> 00:23:15,639
Now we're grabbing
stuff off stack overflow

515
00:23:15,640 --> 00:23:19,100
and sticking them in and hoping
that the twinky lights go on

516
00:23:19,100 --> 00:23:22,044
because we're making these
innovative moves saying,

517
00:23:22,045 --> 00:23:24,760
Ooh Twinkie light turned bright blue

518
00:23:24,760 --> 00:23:28,560
rather than saying does this
piece of data have integrity

519
00:23:28,560 --> 00:23:30,679
throughout its life cycle?

520
00:23:30,680 --> 00:23:35,650
So, and is it approved
from bad guys touching it

521
00:23:35,650 --> 00:23:38,830
and bad guys can be lazy
guys who let it hang out

522
00:23:38,830 --> 00:23:40,760
or negligent people who let it hang out.

523
00:23:40,760 --> 00:23:43,270
So how, how are you doing
that security Colonel

524
00:23:44,384 --> 00:23:49,100
around that privacy thing,
but then also, should you?

525
00:23:49,100 --> 00:23:53,659
That's the big, hard
question with privacy tech

526
00:23:53,660 --> 00:23:55,290
should you have that data?

527
00:23:55,290 --> 00:23:59,379
So building that gateway
into your scoping workshops,

528
00:23:59,380 --> 00:24:01,100
figuring out how does it actually

529
00:24:01,100 --> 00:24:03,090
go through your code development,

530
00:24:03,090 --> 00:24:05,030
so that at the end of this,

531
00:24:05,030 --> 00:24:08,430
you should have your hand
raised saying I have data

532
00:24:08,430 --> 00:24:10,770
so that your your mapping
technologies can catch it

533
00:24:10,770 --> 00:24:14,010
and say, aha now I can do
mapping instead of saying,

534
00:24:14,010 --> 00:24:16,575
yo, Mel you got any data?

535
00:24:16,575 --> 00:24:19,160
and why don't you fill
it into this wizard here

536
00:24:19,160 --> 00:24:20,660
And we'll pretend that's real.

537
00:24:21,780 --> 00:24:23,070
We have metadata.

538
00:24:23,070 --> 00:24:25,340
Why aren't we reading the
metadata for our mapping?

539
00:24:25,340 --> 00:24:27,740
Hello? And if you're putting more data

540
00:24:27,740 --> 00:24:30,320
into the system and you're
enabling that to happen

541
00:24:30,320 --> 00:24:32,610
that's the time to put
those markers in there.

542
00:24:32,610 --> 00:24:33,979
So that's what we're working on

543
00:24:33,980 --> 00:24:35,730
at my stealth software company right now

544
00:24:35,730 --> 00:24:39,880
is how do we have tools to
repeatedly do a better job

545
00:24:39,880 --> 00:24:42,800
of actually building code, checking code

546
00:24:42,800 --> 00:24:44,250
making sure it has quality

547
00:24:44,250 --> 00:24:46,310
and making sure it has that hand up saying

548
00:24:46,310 --> 00:24:49,233
pick me, pick me, I'm the
data you're looking for.

549
00:24:50,550 --> 00:24:52,770
- And that's one of those lessons that,

550
00:24:52,770 --> 00:24:56,010
we saw the past few
years in security, right?

551
00:24:56,010 --> 00:24:59,010
That let's build security early on

552
00:24:59,010 --> 00:25:00,750
not just like tack it later.

553
00:25:00,750 --> 00:25:02,270
And it's certainly true for parks.

554
00:25:02,270 --> 00:25:03,650
And that's one of those things that

555
00:25:03,650 --> 00:25:06,320
we are trying to implement
into privacy tech space

556
00:25:06,320 --> 00:25:09,653
early on with with startups like Mickey,

557
00:25:10,500 --> 00:25:13,840
Mel, are there areas
that you are most excited

558
00:25:13,840 --> 00:25:16,379
about in categories in this space?

559
00:25:16,380 --> 00:25:21,330
- Yeah. So similar to
Mickey I'm really excited

560
00:25:21,330 --> 00:25:24,350
about the technologies
that enable us to do better

561
00:25:24,350 --> 00:25:27,392
from the beginning in terms
of building things better.

562
00:25:28,240 --> 00:25:31,380
You know, I'm excited to
see capabilities where,

563
00:25:31,380 --> 00:25:35,410
you can actually scan the
entire history of your code base

564
00:25:35,410 --> 00:25:37,610
not just current code, but
the entire history of it

565
00:25:37,610 --> 00:25:42,189
understand the patterns
get it to self identify

566
00:25:42,190 --> 00:25:45,470
that it's touching personal information

567
00:25:45,470 --> 00:25:48,449
and then being able to actually automate

568
00:25:48,449 --> 00:25:50,690
the workflows once you
actually know what's happening

569
00:25:50,690 --> 00:25:52,120
with the data, right?

570
00:25:52,120 --> 00:25:53,979
Like you can loop in
whichever stakeholders

571
00:25:53,980 --> 00:25:55,303
and teams you need to,

572
00:25:56,250 --> 00:25:59,780
but being able to actually identify

573
00:25:59,780 --> 00:26:01,889
like which tools are exposing.

574
00:26:01,890 --> 00:26:04,200
Do you have an API that's exposing PII?

575
00:26:04,200 --> 00:26:07,090
Most companies don't know
the answer to that question.

576
00:26:07,090 --> 00:26:08,189
It should be very simple.

577
00:26:08,190 --> 00:26:10,540
And it's the type of thing

578
00:26:10,540 --> 00:26:12,389
that is technically written into laws

579
00:26:12,390 --> 00:26:16,070
but nobody's really doing it.

580
00:26:16,070 --> 00:26:18,585
And so it just kind of
flies under the radar.

581
00:26:18,585 --> 00:26:23,400
So, being able to actually
understand what the code is doing

582
00:26:23,400 --> 00:26:27,070
and being able to anticipate
how the code will react

583
00:26:27,070 --> 00:26:28,810
before you push it into production, right?

584
00:26:28,810 --> 00:26:31,399
Like that has immense security benefits.

585
00:26:31,400 --> 00:26:33,523
It has immense privacy benefits,

586
00:26:36,120 --> 00:26:38,389
random programmers who've
been at your company

587
00:26:38,390 --> 00:26:40,830
for a couple of weeks
should not be making,

588
00:26:40,830 --> 00:26:44,082
material changes to your
authentication systems.

589
00:26:45,377 --> 00:26:47,440
And especially if it's connected to

590
00:26:48,742 --> 00:26:52,082
personal data or customer data,

591
00:26:54,100 --> 00:26:55,959
in being able to actually
have that visibility.

592
00:26:55,960 --> 00:26:58,000
And the reason why these
tools are so exciting

593
00:26:58,000 --> 00:26:59,809
to me is because the programmers

594
00:26:59,809 --> 00:27:03,560
or the traditional developer
teams don't actually

595
00:27:03,560 --> 00:27:05,159
have to get involved

596
00:27:05,160 --> 00:27:06,790
you don't have to disrupt their workflow.

597
00:27:06,790 --> 00:27:09,870
You don't have to try to
force another tool onto them.

598
00:27:09,870 --> 00:27:13,000
This is just something
that arms the privacy

599
00:27:13,000 --> 00:27:15,250
and security experts
with all the visibility

600
00:27:15,250 --> 00:27:16,840
and information that they need.

601
00:27:16,840 --> 00:27:20,360
And then whatever cultural relationship

602
00:27:20,360 --> 00:27:22,889
organizational product
changes need to be made.

603
00:27:22,890 --> 00:27:25,588
You can have that
conversation in the right way.

604
00:27:25,588 --> 00:27:30,588
I have a client that
I've been working with

605
00:27:33,080 --> 00:27:35,909
where, just I'm so excited to see

606
00:27:35,910 --> 00:27:38,570
the fact that their number one kind

607
00:27:38,570 --> 00:27:41,620
of mantra is don't give
the developers more stuff,

608
00:27:41,620 --> 00:27:44,100
giving them one more tool

609
00:27:44,100 --> 00:27:45,919
giving them one more step in the process

610
00:27:45,920 --> 00:27:49,100
is just setting yourselves up to fail.

611
00:27:49,100 --> 00:27:50,796
And so similar to Mickey,
I'm really excited

612
00:27:50,797 --> 00:27:53,790
about the things that it's
not just moving things left

613
00:27:53,790 --> 00:27:55,460
but actually putting security and privacy

614
00:27:55,460 --> 00:27:57,660
at the beginning of the process,

615
00:27:57,660 --> 00:27:59,690
not just further left than we've been

616
00:27:59,690 --> 00:28:04,014
but at the very origin
of code being written.

617
00:28:04,014 --> 00:28:08,179
And but again, I also
have to agree with Mickey

618
00:28:08,180 --> 00:28:10,480
that the big, I think,
elephant in the room

619
00:28:10,480 --> 00:28:13,960
from both the technical
perspective and the ethical

620
00:28:13,960 --> 00:28:15,910
and kind of regulatory perspective is

621
00:28:15,910 --> 00:28:18,060
should we have this data to begin with?

622
00:28:18,060 --> 00:28:20,720
And that's something that at
least in the United States

623
00:28:20,720 --> 00:28:24,893
our privacy laws have
fallen very short of,

624
00:28:26,070 --> 00:28:28,100
we've known insecurity
for a very long time.

625
00:28:28,100 --> 00:28:30,379
You don't have to protect
stuff you don't have.

626
00:28:30,380 --> 00:28:33,914
And we have yet to really adopt that

627
00:28:33,914 --> 00:28:37,765
as an ethos on the privacy side to say,

628
00:28:37,765 --> 00:28:40,780
we don't actually need to
have all these different ways

629
00:28:40,780 --> 00:28:43,000
for people to access
and delete their data.

630
00:28:43,000 --> 00:28:45,500
If it's data we shouldn't
have in the first place.

631
00:28:46,669 --> 00:28:47,650
- Thanks Mel.

632
00:28:47,650 --> 00:28:50,560
So we talked about shifting
left as one of the lessons

633
00:28:50,560 --> 00:28:52,750
both in privacy and
security that we've drawn

634
00:28:52,750 --> 00:28:55,450
from cyber security and
can apply in privacy tech.

635
00:28:55,450 --> 00:28:58,020
I do want us to dig into the comms

636
00:28:58,020 --> 00:29:02,360
and PR lessons that
Mel you've worked with,

637
00:29:02,360 --> 00:29:06,250
so in the cybersecurity
space for more than a decade.

638
00:29:06,250 --> 00:29:11,160
So what are some of these
lessons that you are imparting

639
00:29:11,160 --> 00:29:14,410
to your privacy tech
clients given privacy tech

640
00:29:14,410 --> 00:29:15,743
as such a nascent space?

641
00:29:17,050 --> 00:29:19,310
- Sure. So there's three
key things that I look at

642
00:29:19,310 --> 00:29:22,250
that we learned the hard way
from the security experience

643
00:29:22,250 --> 00:29:24,520
and that we're trying to impart

644
00:29:24,520 --> 00:29:27,664
to privacy tux so they don't
have to learn it the hard way.

645
00:29:27,664 --> 00:29:30,330
The first is earning credibility, right?

646
00:29:30,330 --> 00:29:32,810
Talking points do not equal trust.

647
00:29:32,810 --> 00:29:36,000
Having a laundry list of user
controls buried somewhere

648
00:29:36,000 --> 00:29:38,310
in your app does not mean

649
00:29:38,310 --> 00:29:40,723
that you are going to earn public trust.

650
00:29:41,970 --> 00:29:43,330
It is a great talking point

651
00:29:43,330 --> 00:29:44,470
for a regulator to say,

652
00:29:44,470 --> 00:29:46,700
Hey look at all these
things that we've built

653
00:29:46,700 --> 00:29:49,160
but if it's not actually
part of the user experience

654
00:29:49,160 --> 00:29:51,420
if the user doesn't understand the context

655
00:29:51,420 --> 00:29:52,900
as they're using your product or service

656
00:29:52,900 --> 00:29:55,826
in terms of what's happening
with their data in real time

657
00:29:55,826 --> 00:29:57,730
or what things are being used for

658
00:29:57,730 --> 00:29:59,100
when they're asked for consent.

659
00:29:59,100 --> 00:30:02,235
Consent is a whole nother
problem in terms of

660
00:30:02,235 --> 00:30:06,062
how we ask for it and what we use it for.

661
00:30:07,370 --> 00:30:11,010
But earning credibility as a
privacy tech company is crucial

662
00:30:11,010 --> 00:30:13,530
because you have to walk the walk.

663
00:30:13,530 --> 00:30:15,020
You can't sell privacy.

664
00:30:15,020 --> 00:30:17,483
If you yourself suck at privacy.

665
00:30:18,370 --> 00:30:21,699
The second thing is to
embrace accountability.

666
00:30:21,700 --> 00:30:24,960
And this is true for both
privacy tech startups

667
00:30:24,960 --> 00:30:28,360
but as well as large
companies that are trying to

668
00:30:28,360 --> 00:30:30,159
get better at privacy is,

669
00:30:30,160 --> 00:30:33,884
you need to learn how to
apologize and actually mean it.

670
00:30:33,884 --> 00:30:36,430
That is something that a
lot of companies struggle

671
00:30:36,430 --> 00:30:39,340
with where they may recognize
that now is the moment

672
00:30:39,340 --> 00:30:41,230
for an apology, but they don't mean it.

673
00:30:41,230 --> 00:30:42,970
In which case your apology, actually

674
00:30:42,970 --> 00:30:47,030
people can tell when it's a fake apology.

675
00:30:47,030 --> 00:30:49,500
And so the second piece
of that is to actually,

676
00:30:49,500 --> 00:30:53,540
once you issue an apology
to actually change, right?

677
00:30:53,540 --> 00:30:56,500
make the changes that
people are asking you for.

678
00:30:56,500 --> 00:30:58,990
And for startups, they're very fortunate

679
00:30:58,990 --> 00:31:01,980
that they don't have
decades of legacy systems.

680
00:31:01,980 --> 00:31:06,600
And years and years of
executive personality precedent

681
00:31:06,600 --> 00:31:08,889
that they are gonna
have to clean up after.

682
00:31:08,890 --> 00:31:11,504
You can start now to do this better.

683
00:31:11,504 --> 00:31:14,699
We have lots and lots of case studies

684
00:31:14,700 --> 00:31:17,770
from companies who have gone before.

685
00:31:17,770 --> 00:31:20,010
There's no reason to be repeating history.

686
00:31:20,010 --> 00:31:22,580
You can actually avoid those landmines.

687
00:31:22,580 --> 00:31:24,888
So you're not issuing as many apologies

688
00:31:24,888 --> 00:31:27,000
as your predecessors.

689
00:31:27,000 --> 00:31:30,193
And then the third thing is
to actually advance the field.

690
00:31:30,193 --> 00:31:32,170
And I don't just mean advanced the fields

691
00:31:32,170 --> 00:31:35,120
in terms of your technological
in that advancement

692
00:31:36,000 --> 00:31:39,190
or capabilities that that
comes with the territory

693
00:31:39,190 --> 00:31:42,130
of what you're building
as an entrepreneur.

694
00:31:42,130 --> 00:31:44,150
What I'm really talking about here

695
00:31:44,150 --> 00:31:47,560
is elevating the
profession and the industry

696
00:31:47,560 --> 00:31:50,669
from a marketing and
comms and PR perspective.

697
00:31:50,670 --> 00:31:53,240
We as communicators need to hold ourselves

698
00:31:53,240 --> 00:31:55,820
to a pretty high standard in terms

699
00:31:55,820 --> 00:31:59,090
of what we are willing to say
on behalf of an organization.

700
00:31:59,090 --> 00:32:01,980
I read some of the statements in the news

701
00:32:01,980 --> 00:32:05,240
from various folks, people,
and I'm just flabbergasted

702
00:32:05,240 --> 00:32:07,080
that they can sleep at night

703
00:32:07,080 --> 00:32:09,810
because they're either
lying through their teeth

704
00:32:09,810 --> 00:32:11,090
or they're dumb as rocks.

705
00:32:11,090 --> 00:32:12,669
And don't understand
that their own company

706
00:32:12,670 --> 00:32:13,987
is lying to them.

707
00:32:13,987 --> 00:32:17,640
And so that is really important

708
00:32:17,640 --> 00:32:22,000
that we're not just elevating
the technical capabilities

709
00:32:22,000 --> 00:32:23,440
that are advancing in privacy,

710
00:32:23,440 --> 00:32:25,560
but that we are looking at our profession

711
00:32:25,560 --> 00:32:26,700
as communicators to say,

712
00:32:26,700 --> 00:32:29,420
how can we hold ourselves to
a higher ethical standard?

713
00:32:29,420 --> 00:32:31,960
How can we hold our organizations
to a higher standard

714
00:32:31,960 --> 00:32:33,490
and help them see around the corner

715
00:32:33,490 --> 00:32:35,640
and avoid those problems
in the first place

716
00:32:36,560 --> 00:32:38,730
- Walking the privacy
talk is certainly one

717
00:32:38,730 --> 00:32:41,263
of the main messages that we share

718
00:32:41,263 --> 00:32:43,300
with startups that we work with

719
00:32:43,300 --> 00:32:45,476
at the rise of privacy
tech and come to us.

720
00:32:45,477 --> 00:32:47,900
So, thank you for that message Mel.

721
00:32:47,900 --> 00:32:49,060
Mickey before we wrap up,

722
00:32:49,060 --> 00:32:51,970
I'd love to hear about your perspective

723
00:32:51,970 --> 00:32:54,980
and how the privacy tech
landscape can do better

724
00:32:54,980 --> 00:32:58,070
when it comes to selling or
marketing privacy products.

725
00:32:58,070 --> 00:33:01,419
So we see CSOs budgets and they're

726
00:33:04,551 --> 00:33:07,330
just 10, 12 ,20 times more
than privacy offices budgets.

727
00:33:07,330 --> 00:33:09,210
How are we going to fuel the privacy tech

728
00:33:09,210 --> 00:33:13,010
landscape when there's this
huge discrepancy in budget?

729
00:33:13,010 --> 00:33:15,150
What are some of the
lessons that you think

730
00:33:15,150 --> 00:33:18,220
the privacy profession and
the privacy tech startups

731
00:33:18,220 --> 00:33:21,640
who are selling can draw
from cybersecurity here?

732
00:33:21,640 --> 00:33:24,240
- Yeah, I think one of the things,

733
00:33:24,240 --> 00:33:25,590
particularly since this is our sec,

734
00:33:25,590 --> 00:33:27,889
we're mostly getting
people that at least care

735
00:33:28,840 --> 00:33:31,870
about security if not embedded
in the security world.

736
00:33:31,870 --> 00:33:33,330
And if you are a privacy person,

737
00:33:33,330 --> 00:33:35,780
who's wandered here
and you're not embedded

738
00:33:35,780 --> 00:33:38,250
get embedded and get embedded fast

739
00:33:38,250 --> 00:33:41,360
because it's not a 10
X, my last corporate gig

740
00:33:41,360 --> 00:33:44,575
it was an 88, zero to one ratio

741
00:33:44,576 --> 00:33:48,570
of my budget to security budget.

742
00:33:48,570 --> 00:33:50,830
So you can collect with abandon.

743
00:33:50,830 --> 00:33:54,639
You can have 165 different countries

744
00:33:54,640 --> 00:33:58,670
that you've got to be in
simultaneous compliance,

745
00:33:58,670 --> 00:34:02,270
and yet you have a band-aid.

746
00:34:02,270 --> 00:34:03,790
So you're gonna have to figure out

747
00:34:03,790 --> 00:34:07,129
how do we leverage the
CSOs budgets to make sure

748
00:34:07,130 --> 00:34:08,600
that their goals are achieved.

749
00:34:08,600 --> 00:34:10,909
And remember there's 11 other things

750
00:34:10,909 --> 00:34:12,509
that privacy people are caring about

751
00:34:12,510 --> 00:34:15,737
proportionality, minimization , transfer

752
00:34:15,737 --> 00:34:18,909
and the security aspect is one of 11

753
00:34:18,909 --> 00:34:22,310
but it's a really important 11th element

754
00:34:22,310 --> 00:34:25,380
of fare processing to
get that data secured.

755
00:34:25,380 --> 00:34:27,440
So to the extent you can make the case

756
00:34:27,440 --> 00:34:30,920
if you're a privacy person that
this relates to an elevates

757
00:34:30,920 --> 00:34:32,870
the goal and the perspective

758
00:34:32,870 --> 00:34:36,679
and the insight for the
CSO, it's a mutual win.

759
00:34:36,679 --> 00:34:37,980
So this is where looking

760
00:34:37,980 --> 00:34:40,530
at wicked methodologies
and wicked problem solving

761
00:34:40,530 --> 00:34:44,100
looks at almost infinite
complexity and stakeholders.

762
00:34:44,100 --> 00:34:47,536
And so if you approach security or privacy

763
00:34:47,536 --> 00:34:49,909
as a problem to be solved,

764
00:34:49,909 --> 00:34:51,759
you will never have enough budget.

765
00:34:51,760 --> 00:34:53,139
You will never have enough people.

766
00:34:53,139 --> 00:34:55,060
You will never have enough skill.

767
00:34:55,060 --> 00:34:56,639
If you instead look at this

768
00:34:56,639 --> 00:35:01,640
as think about it as a
sloping curve to optimal

769
00:35:01,860 --> 00:35:05,400
you're starting here at ad
hoc, always the most expensive.

770
00:35:05,400 --> 00:35:08,350
Now you're competing a land war of like,

771
00:35:08,350 --> 00:35:10,130
can I get a dollar to analyze this law

772
00:35:10,130 --> 00:35:12,080
versus can I get a firewall.

773
00:35:12,080 --> 00:35:14,009
once you start to have rigor out

774
00:35:14,010 --> 00:35:16,899
I'm now pointing that firewall in front

775
00:35:16,899 --> 00:35:21,420
of my most secure assets,
which ones need to be secured.

776
00:35:21,420 --> 00:35:23,380
It used to be IP.

777
00:35:23,380 --> 00:35:25,930
And I used to be an IP litigator.

778
00:35:25,930 --> 00:35:28,299
Your system may not really
understand the value

779
00:35:28,300 --> 00:35:29,820
of what IP is.

780
00:35:29,820 --> 00:35:32,890
losing a $7 million patent
case is nothing compared

781
00:35:32,890 --> 00:35:36,160
to a $7 billion privacy fine.

782
00:35:36,160 --> 00:35:39,759
So it's not a tit for tat
figuring out what your data is

783
00:35:39,760 --> 00:35:42,860
and where your data is,
is critical for privacy.

784
00:35:42,860 --> 00:35:45,370
So data mapping technologies probably

785
00:35:45,370 --> 00:35:48,819
will help your OPSEC
people with app control.

786
00:35:48,820 --> 00:35:51,260
It probably will make their
reporting more efficient

787
00:35:51,260 --> 00:35:54,650
for how are we keeping out the
bad guys, reducing incidents,

788
00:35:54,650 --> 00:35:56,430
making sure we have the
coverage that we need

789
00:35:56,430 --> 00:35:58,513
to continue our cyber insurance

790
00:35:58,513 --> 00:36:01,390
that the privacy person now has leveraged.

791
00:36:01,390 --> 00:36:03,252
So you've got to really take this sort of

792
00:36:03,253 --> 00:36:07,300
this integrated approach
and make sure that everybody

793
00:36:07,300 --> 00:36:09,580
is not sort of patting the head

794
00:36:09,580 --> 00:36:13,690
and doing the favor of giving
the privacy person some budget

795
00:36:13,690 --> 00:36:14,910
which has been the practice

796
00:36:14,910 --> 00:36:18,069
over the last 20 years of tin cup favors.

797
00:36:18,070 --> 00:36:20,290
No, this is an integrated approach where

798
00:36:20,290 --> 00:36:22,410
everyone is elevating the data assets

799
00:36:22,410 --> 00:36:24,799
of the company you're protecting risk.

800
00:36:24,800 --> 00:36:25,880
Yes, sure.

801
00:36:25,880 --> 00:36:27,970
Great. Use all those combat metaphors.

802
00:36:27,970 --> 00:36:31,080
But what you're really doing
is inviting in asset value

803
00:36:31,080 --> 00:36:35,259
and tangible access to
have leverage and controls

804
00:36:35,260 --> 00:36:38,660
over that data so that your
consumers can control more.

805
00:36:38,660 --> 00:36:40,790
Your governance, people can control more.

806
00:36:40,790 --> 00:36:44,880
Your reporting has a
higher level of integrity

807
00:36:44,880 --> 00:36:46,910
and everybody wins.

808
00:36:46,910 --> 00:36:48,950
So make sure if you're a privacy person

809
00:36:48,950 --> 00:36:50,259
I know you don't have budget.

810
00:36:50,260 --> 00:36:51,870
Pick everybody else's pockets.

811
00:36:51,870 --> 00:36:53,960
If you're a security or tech person

812
00:36:53,960 --> 00:36:55,910
this is not just a tech solution.

813
00:36:55,910 --> 00:36:57,310
It's a wicked problem.

814
00:36:57,310 --> 00:37:00,270
It's an integrated management
soft systems problem.

815
00:37:00,270 --> 00:37:02,875
Figure out your soft system,
weaknesses and volumes

816
00:37:02,875 --> 00:37:06,210
and then get together collectively.

817
00:37:06,210 --> 00:37:07,240
And I guarantee you

818
00:37:07,240 --> 00:37:09,569
that little privacy
person that's been asking

819
00:37:09,570 --> 00:37:11,933
for favors is going to do you a big one.

820
00:37:13,100 --> 00:37:15,620
- I love that last point
because it leads us

821
00:37:15,620 --> 00:37:17,406
to the other point though,
we don't have a lot of time

822
00:37:17,407 --> 00:37:19,365
we have a minute to discuss it.

823
00:37:19,365 --> 00:37:21,720
Getting embedded just gets us

824
00:37:21,720 --> 00:37:24,799
to the point of bridging
gaps and collaborating

825
00:37:24,800 --> 00:37:27,570
because of the cross-functional
nature of privacy, right?

826
00:37:27,570 --> 00:37:29,330
So you can't do everything

827
00:37:29,330 --> 00:37:30,880
on your own as a privacy professional

828
00:37:30,880 --> 00:37:33,220
and privacy tech founders
need to understand

829
00:37:33,220 --> 00:37:34,970
that you're not just
selling to the lawyers here

830
00:37:34,970 --> 00:37:37,730
or you might be selling
to the data analytics team

831
00:37:37,730 --> 00:37:42,720
or the dev ops team, or this
GRC security PR professional.

832
00:37:42,720 --> 00:37:45,200
I do wanna give us some time to wrap up.

833
00:37:45,200 --> 00:37:49,337
So from my end, thank you
everyone for listening in

834
00:37:49,338 --> 00:37:52,410
and you can find the rights
of privacy tech online

835
00:37:52,410 --> 00:37:54,680
and Twitter and LinkedIn
@privacytechwriters

836
00:37:54,680 --> 00:37:57,100
or our website rightsofprivacytech.com.

837
00:37:57,100 --> 00:37:59,114
If you're a founder, investor

838
00:37:59,114 --> 00:38:02,347
or expert in the privacy tech
space, please come to us.

839
00:38:02,347 --> 00:38:05,500
We have a great event coming up on June 23

840
00:38:05,500 --> 00:38:07,931
at the rise of privacy tech virtual summit

841
00:38:07,931 --> 00:38:09,180
come join us.

842
00:38:09,180 --> 00:38:10,779
And we'd love to have you there.

843
00:38:12,850 --> 00:38:16,410
Mickey, Mel any wrap up closing remarks?

844
00:38:16,410 --> 00:38:19,020
- The thing I'll say
is that for any privacy

845
00:38:19,020 --> 00:38:21,380
or security professional,
your most important skill

846
00:38:21,380 --> 00:38:23,400
that you can develop is
the ability to influence.

847
00:38:23,400 --> 00:38:25,130
You've got to get people who don't report

848
00:38:25,130 --> 00:38:27,530
to you to do the thing
that you need them to do.

849
00:38:28,570 --> 00:38:29,840
- Amen

850
00:38:29,840 --> 00:38:34,290
- Yeah, and stop where
Mel's profession begins

851
00:38:34,290 --> 00:38:35,509
is to be a storyteller

852
00:38:35,510 --> 00:38:37,270
understand why you're in the room,

853
00:38:37,270 --> 00:38:39,000
understand what your objectives are.

854
00:38:39,000 --> 00:38:40,190
This is not a war.

855
00:38:40,190 --> 00:38:41,720
It's not a competition.

856
00:38:41,720 --> 00:38:43,959
You're not going to like save your job

857
00:38:43,960 --> 00:38:47,530
so that Aceso loses his,
forget about all of that.

858
00:38:47,530 --> 00:38:52,530
This is the absolute bit
flip of data as asset

859
00:38:54,220 --> 00:38:57,169
Data is your currency
protect it like you would

860
00:38:57,170 --> 00:38:58,970
your very own dollars

861
00:38:58,970 --> 00:39:01,310
and you're gonna do really well.

862
00:39:01,310 --> 00:39:02,820
- Thanks everyone for joining us.

863
00:39:02,820 --> 00:39:03,860
And we look forward

864
00:39:03,860 --> 00:39:06,567
to maybe seeing you next
year, live in person.

865
00:39:07,482 --> 00:39:08,315
- Hope so

