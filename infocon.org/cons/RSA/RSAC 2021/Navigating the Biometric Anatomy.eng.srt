1
00:00:01,390 --> 00:00:02,223
- Hello everyone.

2
00:00:02,223 --> 00:00:04,290
Welcome to the Biometric Legal Session.

3
00:00:04,290 --> 00:00:06,859
I'll give you a brief
introduction to who I am.

4
00:00:06,860 --> 00:00:07,930
I'm Chris Hydak.

5
00:00:07,930 --> 00:00:09,770
I'm an attorney at Microsoft,

6
00:00:09,770 --> 00:00:12,250
I'm in Privacy and Regulatory Affairs.

7
00:00:12,250 --> 00:00:14,240
And I so that means I mainly do privacy

8
00:00:14,240 --> 00:00:16,360
and data protection work.

9
00:00:16,360 --> 00:00:19,210
And one of the areas I
work on is biometrics.

10
00:00:19,210 --> 00:00:23,450
And so this is gonna be a kind
of a Biometric Legal Session.

11
00:00:23,450 --> 00:00:27,220
I emphasize legal because this
is gonna be kind of focused

12
00:00:27,220 --> 00:00:29,090
on something that we've done recently

13
00:00:29,090 --> 00:00:31,110
which is kinda designing a review process

14
00:00:31,110 --> 00:00:35,140
for biometric solutions that
try to enable those solutions

15
00:00:35,140 --> 00:00:36,750
while mitigating risk.

16
00:00:36,750 --> 00:00:40,210
So I'm not gonna delve into
the security pros and cons

17
00:00:40,210 --> 00:00:42,790
of biometric solutions
versus other solutions.

18
00:00:42,790 --> 00:00:44,710
I'm not gonna try to sell anyone

19
00:00:44,710 --> 00:00:47,600
on using any Microsoft biometrics.

20
00:00:47,600 --> 00:00:49,380
Rather, hopefully this
session will help folks

21
00:00:49,380 --> 00:00:51,800
devise a framework for reviewing

22
00:00:51,800 --> 00:00:54,040
and tailoring biometric solutions

23
00:00:54,040 --> 00:00:56,839
so that you can launch
them wherever you work

24
00:00:56,840 --> 00:00:58,330
with a reasonable degree of risk.

25
00:00:58,330 --> 00:01:00,010
And I say reasonable degree of risk

26
00:01:00,010 --> 00:01:02,349
because in the current legal landscape,

27
00:01:02,350 --> 00:01:06,580
as I'm sure many of you know,
there aren't many scenarios

28
00:01:06,580 --> 00:01:08,643
that are gonna involve no risk.

29
00:01:09,720 --> 00:01:13,870
So just to talk about
the agenda for a second

30
00:01:13,870 --> 00:01:15,260
I'm gonna talk about a little bit

31
00:01:15,260 --> 00:01:17,600
of the current legal regimes.

32
00:01:17,600 --> 00:01:20,830
Many of you probably know
about many of these regimes

33
00:01:20,830 --> 00:01:22,320
but I thought it'd be
helpful to set a base

34
00:01:22,320 --> 00:01:25,449
on the primary biometric legal regimes.

35
00:01:25,450 --> 00:01:27,480
And then the meat of the presentation

36
00:01:27,480 --> 00:01:29,620
we'll be talking about the next two topics

37
00:01:29,620 --> 00:01:32,820
which are kinda strategies
for compliance across regimes,

38
00:01:32,820 --> 00:01:35,580
how to create a scalable and
global biometric solution,

39
00:01:35,580 --> 00:01:39,450
and what's happening in
courtrooms and legislatures today.

40
00:01:39,450 --> 00:01:42,303
And what's likely to
happen over the next year.

41
00:01:43,200 --> 00:01:48,100
So I will start with the
Current Legal Regimes

42
00:01:48,100 --> 00:01:49,610
in the United States,

43
00:01:49,610 --> 00:01:53,140
and there are currently
three biometric specific laws

44
00:01:53,140 --> 00:01:54,840
of general applicability,

45
00:01:54,840 --> 00:01:57,800
Washington State, Texas, and Illinois.

46
00:01:57,800 --> 00:01:59,360
And I sit in Washington State.

47
00:01:59,360 --> 00:02:02,380
So I'll start with Washington
State, and Washington State

48
00:02:02,380 --> 00:02:04,100
the biometric law in Washington State

49
00:02:04,100 --> 00:02:09,100
there's a few, but the main
one of general applicability

50
00:02:09,180 --> 00:02:11,530
prohibits enrolling a biometric identifier

51
00:02:11,530 --> 00:02:14,070
in a database for a commercial purpose

52
00:02:14,070 --> 00:02:17,060
without obtaining consent
or providing notice

53
00:02:17,060 --> 00:02:19,970
or instituting a mechanism
to prevent subsequent reuse

54
00:02:19,970 --> 00:02:21,240
of the identifier.

55
00:02:21,240 --> 00:02:24,980
Not gonna get into all the
nuances of the definition,

56
00:02:24,980 --> 00:02:27,670
but a few keys of the law, but a few keys,

57
00:02:27,670 --> 00:02:29,500
first of all as I try to emphasize

58
00:02:29,500 --> 00:02:31,881
it's flexible on notice versus consent.

59
00:02:31,881 --> 00:02:34,760
And it explicitly provides
that there's a choice

60
00:02:34,760 --> 00:02:35,870
between notice and consent

61
00:02:35,870 --> 00:02:38,093
and that choice is context dependent.

62
00:02:39,150 --> 00:02:40,450
Second, the law is triggered

63
00:02:40,450 --> 00:02:44,209
only when a biometric identifier
is enrolled in a database.

64
00:02:44,210 --> 00:02:46,950
There's a lot of use cases
that don't involve enrollment.

65
00:02:46,950 --> 00:02:49,792
And we'll talk about
more about them later.

66
00:02:50,750 --> 00:02:53,530
Third, Washington State Biometrical Law

67
00:02:53,530 --> 00:02:54,900
doesn't have a private right of action.

68
00:02:54,900 --> 00:02:57,620
And there has been
limited if any enforcement

69
00:02:57,620 --> 00:03:00,690
by the Washington
attorney general to date.

70
00:03:00,690 --> 00:03:02,850
So there's few resources
for interpretation

71
00:03:02,850 --> 00:03:04,283
outside the line itself.

72
00:03:05,260 --> 00:03:07,720
Then we'll move to Texas next

73
00:03:07,720 --> 00:03:10,570
as I a former resident of Texas.

74
00:03:10,570 --> 00:03:13,269
But the Texas laws is somewhat
similar to the Illinois law

75
00:03:13,270 --> 00:03:14,480
which I'll talk about in a second,

76
00:03:14,480 --> 00:03:17,519
but the Texas law, it says
that you can't capture

77
00:03:17,520 --> 00:03:20,650
a biometric identifier
for a commercial purpose

78
00:03:20,650 --> 00:03:22,410
without notice and consent.

79
00:03:22,410 --> 00:03:24,740
So a little different from Washington

80
00:03:24,740 --> 00:03:27,290
and then once possessed,
you cannot disclose it

81
00:03:27,290 --> 00:03:30,859
to a third party without
consent or an exception.

82
00:03:30,860 --> 00:03:34,970
There's few exceptions
that are generally helpful

83
00:03:34,970 --> 00:03:38,960
but there is one that is useful
for financial institutions.

84
00:03:38,960 --> 00:03:41,110
If it's the biometric
identifiers to complete

85
00:03:41,110 --> 00:03:42,590
a financial transaction.

86
00:03:42,590 --> 00:03:46,150
The other exceptions include
kinda unique scenarios

87
00:03:46,150 --> 00:03:49,150
like disclosing biometric data
in response to a court order.

88
00:03:50,890 --> 00:03:52,738
Again, I'm not gonna get
too many of the specifics

89
00:03:52,739 --> 00:03:54,320
of the Texas law, but it's brief

90
00:03:54,320 --> 00:03:55,980
and it doesn't define many of its terms.

91
00:03:55,980 --> 00:03:57,850
For example, there's no
definition of capture

92
00:03:57,850 --> 00:04:01,030
like what is capturing
a biometric identifier

93
00:04:02,320 --> 00:04:05,359
but the Texas law doesn't
define biometric identifier

94
00:04:05,360 --> 00:04:08,040
to include a record of
hand or face geometry

95
00:04:09,150 --> 00:04:10,670
which is similar to BIPA,
which we'll talk about

96
00:04:10,670 --> 00:04:13,750
in a second, but it doesn't
explain or provide context

97
00:04:13,750 --> 00:04:15,970
about what a record of
hand or face geometry is

98
00:04:15,970 --> 00:04:17,970
or what that encompasses.

99
00:04:17,970 --> 00:04:19,817
So is that a picture of a hand?

100
00:04:19,817 --> 00:04:23,649
Is it, what about notes that
fingers are a certain length?

101
00:04:23,649 --> 00:04:25,640
What about, or is it only
something that allows

102
00:04:25,640 --> 00:04:28,210
for automated comparison
for identification

103
00:04:28,210 --> 00:04:29,293
or authentication.

104
00:04:30,130 --> 00:04:31,260
Similar to Washington,

105
00:04:31,260 --> 00:04:34,349
the Texas law does not have
a private right of action.

106
00:04:34,350 --> 00:04:38,090
We haven't seen public
enforcement of the Texas law

107
00:04:38,090 --> 00:04:42,400
beyond a few articles that
the Texas attorney general

108
00:04:42,400 --> 00:04:44,140
was investigating Facebook

109
00:04:44,140 --> 00:04:47,550
in the wake of Facebook's
biometric settlement

110
00:04:47,550 --> 00:04:49,480
class action settlement, settlement.

111
00:04:49,480 --> 00:04:52,590
But the articles have just
indicated investigating

112
00:04:52,590 --> 00:04:55,810
and at least publicly, we
don't know the conclusion

113
00:04:55,810 --> 00:04:57,210
of those or at least I know.

114
00:04:58,320 --> 00:04:59,860
The one everyone talks about

115
00:04:59,860 --> 00:05:01,050
in the United States is Illinois.

116
00:05:01,050 --> 00:05:02,820
The Biometric Information Privacy Act

117
00:05:02,820 --> 00:05:05,253
or BIPA as it's known for short.

118
00:05:06,202 --> 00:05:07,293
I'm just gonna a little
bit more time on BIPA

119
00:05:07,293 --> 00:05:10,020
just because it's the
one with the most chief

120
00:05:10,020 --> 00:05:12,630
in the United States at a high level

121
00:05:12,630 --> 00:05:14,620
it requires a written
consent for the collection

122
00:05:14,620 --> 00:05:17,870
of biometric identifiers
and biometric information.

123
00:05:17,870 --> 00:05:19,120
It creates two definitions,

124
00:05:19,120 --> 00:05:21,960
biometric identifiers and
biometric information.

125
00:05:21,960 --> 00:05:25,690
And all of the obligations in
BIPA apply equally to both.

126
00:05:25,690 --> 00:05:29,363
So the need for two definitions
is somewhat unclear.

127
00:05:30,480 --> 00:05:33,900
But getting into them for a
second, biometric identifiers

128
00:05:33,900 --> 00:05:35,169
BIPA provides an exhaustive list.

129
00:05:35,170 --> 00:05:38,350
And that includes retina
or iris scan, fingerprint,

130
00:05:38,350 --> 00:05:42,540
voiceprint, and a scan
of hand or face geometry.

131
00:05:42,540 --> 00:05:45,670
There's numerous exceptions,
including things to derive

132
00:05:45,670 --> 00:05:48,120
from writing samples and from photographs.

133
00:05:48,120 --> 00:05:50,570
And that biometric information as compared

134
00:05:50,570 --> 00:05:52,840
to biometric identifier is any info

135
00:05:52,840 --> 00:05:55,010
based on a biometric identifier

136
00:05:55,010 --> 00:05:57,510
and use to identify an individual.

137
00:05:57,510 --> 00:06:00,980
And importantly, the definition
of biometric information

138
00:06:00,980 --> 00:06:04,700
includes the phrase and use
to identify an individual

139
00:06:04,700 --> 00:06:08,750
but the definition of
biometric identifier does not.

140
00:06:08,750 --> 00:06:10,620
So one of the questions that has arisen

141
00:06:10,620 --> 00:06:14,260
in litigation and just
for practitioners is

142
00:06:14,260 --> 00:06:16,960
does the law intentionally
create two definitions

143
00:06:16,960 --> 00:06:19,890
and pose obligations on them equally,

144
00:06:19,890 --> 00:06:22,479
but require unique
identification or authentication

145
00:06:22,480 --> 00:06:24,420
for just one of the definitions.

146
00:06:24,420 --> 00:06:28,290
And maybe some plaintiffs
are arguing just that.

147
00:06:28,290 --> 00:06:31,000
And defendants of course
are asserting the opposite

148
00:06:31,000 --> 00:06:34,690
and defendant's arguments,
their main arguments

149
00:06:34,690 --> 00:06:39,690
have kinda fallen into two
camps which is the first is,

150
00:06:40,070 --> 00:06:42,840
the term itself is biometric identifier.

151
00:06:42,840 --> 00:06:46,830
So how could identifiers
in the defined term

152
00:06:46,830 --> 00:06:49,849
how could it apply to non
identifiable information?

153
00:06:49,850 --> 00:06:53,340
And then the other arguments
that defendants are making

154
00:06:53,340 --> 00:06:56,700
is that the legislative
intent section of the statute

155
00:06:57,910 --> 00:06:59,470
seems to only be concerned

156
00:06:59,470 --> 00:07:02,510
with uniquely identifiable
biometric information.

157
00:07:02,510 --> 00:07:05,947
For example, the legislative
intent section states that

158
00:07:05,947 --> 00:07:07,797
"Unlike other unique identifiers

159
00:07:07,797 --> 00:07:09,847
"unlike other unique identifiers,

160
00:07:09,847 --> 00:07:12,426
"biometric identifiers
are biologically unique

161
00:07:12,427 --> 00:07:13,817
"to the individual."

162
00:07:13,817 --> 00:07:15,830
And that seems to suggest
that the legislature

163
00:07:15,830 --> 00:07:18,590
was concerned predominantly
with biometrics

164
00:07:18,590 --> 00:07:20,222
for unique identification.

165
00:07:21,320 --> 00:07:23,810
Let's talk about the applicability
of BIPA for a second.

166
00:07:23,810 --> 00:07:26,490
It applies to private entities

167
00:07:26,490 --> 00:07:30,170
and all requirements apply
equally to all private entities.

168
00:07:30,170 --> 00:07:32,750
So there's no distinction
between controllers,

169
00:07:32,750 --> 00:07:36,000
or processors, or entities,
with first party relationships

170
00:07:36,000 --> 00:07:38,820
and service providers or platforms.

171
00:07:38,820 --> 00:07:42,599
So talking about backing up
to the requirements of BIPA,

172
00:07:42,600 --> 00:07:44,770
one of the main requirements
obtain written consent.

173
00:07:44,770 --> 00:07:46,159
So it can be tucked to make sense

174
00:07:46,160 --> 00:07:48,380
of who should obtain consent in scenarios

175
00:07:48,380 --> 00:07:53,380
where there is a more
multiple entities involved

176
00:07:53,880 --> 00:07:57,820
in the processing of the
biometric information.

177
00:07:57,820 --> 00:08:00,650
And of course you can
spell that out via contract

178
00:08:01,510 --> 00:08:04,289
between the entities that are all involved

179
00:08:04,290 --> 00:08:08,890
in the processing, entity
X should obtain consent,

180
00:08:08,890 --> 00:08:11,510
but in the event of a
class action, that contract

181
00:08:11,510 --> 00:08:13,349
between the relevant entities

182
00:08:13,350 --> 00:08:15,760
isn't gonna help the party
escape the class action.

183
00:08:15,760 --> 00:08:17,719
All it will do is provide some recourse

184
00:08:17,720 --> 00:08:21,060
for that entity that didn't
have the contractual obligation

185
00:08:21,060 --> 00:08:22,970
to obtain consent, provide some recourse

186
00:08:22,970 --> 00:08:26,520
for that entity to recoup
damages from the entity

187
00:08:26,520 --> 00:08:30,022
that was supposed to obtain
consent after the class action.

188
00:08:31,630 --> 00:08:34,789
Comprehensive privacy legislation
is becoming more common

189
00:08:34,789 --> 00:08:35,622
in the United States.

190
00:08:35,623 --> 00:08:37,409
It started with the CCPA

191
00:08:37,409 --> 00:08:40,589
and the CCPA has a definition
of biometric information.

192
00:08:40,590 --> 00:08:42,630
And it's an interesting definition

193
00:08:42,630 --> 00:08:44,350
and I'll just kinda read it for a second

194
00:08:44,350 --> 00:08:47,790
so people can hear it, but
it's biometric information

195
00:08:47,790 --> 00:08:50,339
means that individuals
physiological, biological,

196
00:08:50,340 --> 00:08:52,000
or behavioral characteristics

197
00:08:52,000 --> 00:08:55,990
including an individual's
DNA that can be used singly

198
00:08:55,990 --> 00:08:57,820
or in combination with each other

199
00:08:57,820 --> 00:08:59,900
or with other identifying
data to establish

200
00:08:59,900 --> 00:09:01,540
individual identity.

201
00:09:01,540 --> 00:09:04,209
And then it provides a list of examples

202
00:09:04,210 --> 00:09:05,890
of biometric information.

203
00:09:05,890 --> 00:09:07,640
And it says biometric information includes

204
00:09:07,640 --> 00:09:10,180
but not limited to imagery
of iris, retina, fingerprint,

205
00:09:10,180 --> 00:09:12,500
face, hand, palm, vein
patterns, voice recordings,

206
00:09:12,500 --> 00:09:15,640
from which an identifier
template such as a face print,

207
00:09:15,640 --> 00:09:19,033
a minutia template, or a
voiceprint can be extracted.

208
00:09:20,030 --> 00:09:25,030
And so this use of can be
ex-used and can be extracted

209
00:09:25,260 --> 00:09:27,613
language is really interesting in CCPA.

210
00:09:28,610 --> 00:09:30,900
And so if you can create a facial template

211
00:09:30,900 --> 00:09:33,340
from a photo, but you don't,

212
00:09:33,340 --> 00:09:36,693
the CCPA seems to suggest that
that is still biometric data.

213
00:09:37,790 --> 00:09:42,780
And the teeth of the CCPA
is a little bit unknown

214
00:09:42,780 --> 00:09:44,750
because it creates this
definition of biometric data

215
00:09:44,750 --> 00:09:46,980
but then it doesn't have any requirements

216
00:09:46,980 --> 00:09:50,120
on biometric data
outside of personal data.

217
00:09:50,120 --> 00:09:53,520
So whether that definition
has any real word impact yet

218
00:09:53,520 --> 00:09:58,120
we're not sure, but then
Virginia following California,

219
00:09:58,120 --> 00:09:59,750
implemented a comprehensive privacy law

220
00:09:59,750 --> 00:10:01,550
which probably many of you are aware of.

221
00:10:01,550 --> 00:10:03,630
And it has a different definition

222
00:10:03,630 --> 00:10:06,100
of biometric data from the CCPA.

223
00:10:06,100 --> 00:10:08,860
And the Virginia definition
is biometric data,

224
00:10:08,860 --> 00:10:11,720
it means data generated
by automatic measurements

225
00:10:11,720 --> 00:10:13,910
of an individual's
biological characteristics

226
00:10:13,910 --> 00:10:18,910
such as a fingerprint,
voiceprint, eyes retinas, irises,

227
00:10:19,160 --> 00:10:21,630
or rather unique biological
patterns or characteristics

228
00:10:21,630 --> 00:10:23,850
that are used to identify
specific individual.

229
00:10:23,850 --> 00:10:27,120
So personally I liked the
Virginia definition a lot better

230
00:10:27,120 --> 00:10:27,953
for two reasons.

231
00:10:27,953 --> 00:10:30,270
The first is it requires
automatic measurements.

232
00:10:30,270 --> 00:10:34,160
So things like we had the
capacity to do something

233
00:10:34,160 --> 00:10:36,209
but didn't are not biometric data.

234
00:10:36,210 --> 00:10:38,970
And also things like
John DOE walked slowly

235
00:10:38,970 --> 00:10:40,510
which couldn't be
considered biometric data

236
00:10:40,510 --> 00:10:43,950
under the CCPA is not biometric
data under the Virginia law

237
00:10:43,950 --> 00:10:46,594
because there's no automatic measurements.

238
00:10:46,594 --> 00:10:49,090
And it avoids that can be used language,

239
00:10:49,090 --> 00:10:51,610
it avoids that can be
the capacity argument.

240
00:10:51,610 --> 00:10:53,050
So rather to be biometric data

241
00:10:53,050 --> 00:10:55,959
the data must be used for
unique identification.

242
00:10:55,960 --> 00:10:57,080
This takes a lot of the squishing

243
00:10:57,080 --> 00:10:59,590
it's about capability
capacity out of the equation.

244
00:10:59,590 --> 00:11:03,920
It focuses on the reality of
the actual and current use.

245
00:11:03,920 --> 00:11:07,939
And this capacity versus
present use debate is something

246
00:11:07,940 --> 00:11:09,470
I don't know how many people here

247
00:11:09,470 --> 00:11:11,280
privacy lawyers, a data protection lawyers

248
00:11:11,280 --> 00:11:12,959
but it's something we've
seen in other contexts.

249
00:11:12,960 --> 00:11:13,793
So if anyone's dealt

250
00:11:13,793 --> 00:11:15,800
with the Telephone Consumer Protection Act

251
00:11:15,800 --> 00:11:17,410
over the last 15 years, you
know there's been tension

252
00:11:17,410 --> 00:11:19,199
regarding the definition of an autodialer.

253
00:11:19,200 --> 00:11:22,230
Is an autodialer something
that has the capacity

254
00:11:22,230 --> 00:11:24,660
to generate and dial random numbers

255
00:11:24,660 --> 00:11:27,209
even if it's not used in that capacity?

256
00:11:27,210 --> 00:11:30,350
Or something autodialer only
if it's used to generate

257
00:11:30,350 --> 00:11:31,460
and dial random number.

258
00:11:31,460 --> 00:11:33,730
So we're seeing in that
same kinda capacity

259
00:11:33,730 --> 00:11:37,253
versus present use argument
come up in biometric laws.

260
00:11:38,210 --> 00:11:41,210
Many cities and states also
have an active biometric laws.

261
00:11:41,210 --> 00:11:42,750
I won't get into them in detail

262
00:11:42,750 --> 00:11:45,520
but most of them focus only on government

263
00:11:45,520 --> 00:11:48,949
and law enforcement use of
biometrics or facial recognition.

264
00:11:48,950 --> 00:11:51,670
Many of them are focused
on facial recognition

265
00:11:51,670 --> 00:11:53,579
at least one applies
to the private sector.

266
00:11:53,580 --> 00:11:55,970
However, and that's the
law in Portland, Oregon.

267
00:11:55,970 --> 00:11:58,040
And Portland's law interestingly,

268
00:11:58,040 --> 00:12:01,219
bans the facial recognition
tech in places of public

269
00:12:01,220 --> 00:12:03,963
and places of public accommodation.

270
00:12:05,040 --> 00:12:07,810
And similar to BIPA, it contains
a private right of action

271
00:12:07,810 --> 00:12:09,800
in statutory damages.

272
00:12:09,800 --> 00:12:13,490
So we may see a lot of
litigation or that law as well.

273
00:12:13,490 --> 00:12:16,653
It's only been inactive
in the last since January.

274
00:12:17,818 --> 00:12:19,410
And consent interestingly,
is not an exception

275
00:12:19,410 --> 00:12:22,120
or a basis for processing the biometrics.

276
00:12:22,120 --> 00:12:25,179
Unlike BIPA, Portland's law is a ban.

277
00:12:25,179 --> 00:12:26,670
Now there are a few exceptions

278
00:12:26,670 --> 00:12:30,170
such as using facial recognition
for verification purposes

279
00:12:30,170 --> 00:12:35,170
for BYOD or using work issued
device, but it's a ban.

280
00:12:35,879 --> 00:12:37,350
It's not a requirement to get consent.

281
00:12:37,350 --> 00:12:38,663
Consent is irrelevant.

282
00:12:39,680 --> 00:12:41,870
And so next, I'll talk about

283
00:12:41,870 --> 00:12:44,930
the Current Legal Regime in Europe.

284
00:12:44,930 --> 00:12:48,120
Of course, the GDPR which has been around

285
00:12:48,120 --> 00:12:51,570
for a few years now,
addresses biometric data.

286
00:12:51,570 --> 00:12:53,230
And it has a definition which is different

287
00:12:53,230 --> 00:12:55,600
from the CCPA, and
different from Virginia,

288
00:12:55,600 --> 00:12:57,670
and different from BIPA,
and different from Texas,

289
00:12:57,670 --> 00:12:59,719
and different from Washington.

290
00:12:59,720 --> 00:13:03,430
And it states the biometric
data is personal data

291
00:13:03,430 --> 00:13:05,819
resulting from specific
technical processing

292
00:13:05,820 --> 00:13:07,540
relating to the physical physiological,

293
00:13:07,540 --> 00:13:10,280
or behavioral characteristics
of a natural person

294
00:13:10,280 --> 00:13:11,882
which allow or confirm
the unique identification

295
00:13:11,883 --> 00:13:15,510
of that natural persons,
such as facial images or DNA.

296
00:13:15,510 --> 00:13:18,667
So very similar to the concept
of automatic measurements

297
00:13:18,667 --> 00:13:21,280
in the Virginia law, the
GDPR requires some type

298
00:13:21,280 --> 00:13:23,800
of specific technical processing.

299
00:13:23,800 --> 00:13:26,209
So our example of John Doe walks slowly,

300
00:13:26,210 --> 00:13:27,280
isn't biometric data.

301
00:13:27,280 --> 00:13:31,930
There's no specific technical
processing in that data.

302
00:13:31,930 --> 00:13:34,739
Different DPA's have issued guidance

303
00:13:34,740 --> 00:13:36,963
about processing biometric
data under the GDPR

304
00:13:36,963 --> 00:13:39,599
perhaps the most interesting

305
00:13:39,600 --> 00:13:42,540
and maybe the one that's
given the most thought

306
00:13:42,540 --> 00:13:47,355
and linked to it, is the
German DPA's through the DSK

307
00:13:47,355 --> 00:13:49,873
which is kinda the body
of all the German DPA's.

308
00:13:51,310 --> 00:13:54,719
I'm not gonna go in depth on the paper,

309
00:13:54,720 --> 00:13:58,060
but they provides a lot
of scenarios with examples

310
00:13:58,060 --> 00:14:00,880
and then the German DPA's
guidance on those examples.

311
00:14:00,880 --> 00:14:04,220
And it's really interesting
if you haven't read it.

312
00:14:04,220 --> 00:14:07,400
And another interesting
aspect to it was two or three

313
00:14:07,400 --> 00:14:11,530
of the German DPA's did
not vote to issue it.

314
00:14:11,530 --> 00:14:12,970
They don't issue dissenting statements

315
00:14:12,970 --> 00:14:15,730
or anything like that, but
it wasn't unanimous paper

316
00:14:15,730 --> 00:14:18,050
by the German DPA's, which is interesting

317
00:14:19,020 --> 00:14:22,220
on we know what presumably
there's some conclusions

318
00:14:22,220 --> 00:14:25,123
in there that two or three of
the DPA's did not agree with.

319
00:14:26,970 --> 00:14:28,770
Then getting outside of Europe.

320
00:14:28,770 --> 00:14:30,750
I'll talk about the kinda
the rest of the world.

321
00:14:30,750 --> 00:14:32,540
I talked about U.S. and
Europe a little bit.

322
00:14:32,540 --> 00:14:34,680
I'm just gonna kinda briefly touch

323
00:14:34,680 --> 00:14:35,870
on some of the interesting developments

324
00:14:35,870 --> 00:14:37,360
outside U.S. and rest of the world.

325
00:14:37,360 --> 00:14:39,010
I'm not gonna try to
provide the legal regimes

326
00:14:39,010 --> 00:14:39,843
everywhere else.

327
00:14:39,843 --> 00:14:42,610
But India's most recent
Data Protection Bill

328
00:14:42,610 --> 00:14:44,280
bans processing biometric data

329
00:14:44,280 --> 00:14:46,300
without rule by the government.

330
00:14:46,300 --> 00:14:48,939
It's exactly how that ban will work.

331
00:14:48,940 --> 00:14:51,480
If this law gets passed or
the exact biometric data

332
00:14:51,480 --> 00:14:54,320
is somewhat unclear, but it's
really interesting position

333
00:14:54,320 --> 00:14:56,123
in India Data Protection Bill,

334
00:14:57,520 --> 00:15:00,260
which has been in the
works for a few years now.

335
00:15:00,260 --> 00:15:03,240
The Canadian DPA's recently released

336
00:15:03,240 --> 00:15:06,730
a really interesting joint
investigation into clear view.

337
00:15:06,730 --> 00:15:08,620
A Clearview as you may
have heard had been accused

338
00:15:08,620 --> 00:15:11,110
of creating facial templates that people

339
00:15:11,110 --> 00:15:13,660
via publicly available
pictures on the internet

340
00:15:13,660 --> 00:15:15,380
and Clearview provided a service

341
00:15:15,380 --> 00:15:16,689
that allowed law enforcement

342
00:15:16,690 --> 00:15:19,040
and certain other governmental
actors to submit a picture

343
00:15:19,040 --> 00:15:21,680
of an individual and
Clearview would provide back

344
00:15:21,680 --> 00:15:23,500
other images of the
individual that Clearview

345
00:15:23,500 --> 00:15:25,870
found on the internet
plus metadata associated

346
00:15:25,870 --> 00:15:27,060
with the pictures.

347
00:15:27,060 --> 00:15:31,209
And I'm just gonna have one
really interesting sentence

348
00:15:31,210 --> 00:15:34,580
from the joint investigation
by the Canadian DPA's.

349
00:15:34,580 --> 00:15:36,430
And it was Clearview should have obtained

350
00:15:36,430 --> 00:15:40,510
express opt-in consent before
it collected the images

351
00:15:40,510 --> 00:15:42,450
of any individual in Canada.

352
00:15:42,450 --> 00:15:45,380
So note that the report doesn't say

353
00:15:45,380 --> 00:15:47,158
that Clearview should
have obtained consent

354
00:15:47,158 --> 00:15:50,240
before creating a facial
vector or a facial template.

355
00:15:50,240 --> 00:15:51,487
It says Clearview should
have obtained consent

356
00:15:51,487 --> 00:15:53,400
before even obtained the images,

357
00:15:53,400 --> 00:15:56,730
before it clicked download on the images

358
00:15:56,730 --> 00:16:00,100
that it is scraped from
publicly available websites.

359
00:16:00,100 --> 00:16:01,950
And this seems to blur any distinctions

360
00:16:01,950 --> 00:16:06,930
between raw data images, video, audio

361
00:16:06,930 --> 00:16:10,849
of people talking and
derived biometric data

362
00:16:10,850 --> 00:16:14,000
such as facial vectors
or facial templates.

363
00:16:14,000 --> 00:16:17,050
But perhaps I'm taking this one paragraph

364
00:16:17,050 --> 00:16:21,120
of the investigation and
I'm reading too much into it

365
00:16:21,120 --> 00:16:22,510
and taking a little bit of out of context.

366
00:16:22,510 --> 00:16:24,157
But I thought that was a
very interesting paragraph

367
00:16:24,157 --> 00:16:25,773
from the Canadians.

368
00:16:26,756 --> 00:16:30,800
So this is mainly what I'm
gonna try walk through.

369
00:16:30,801 --> 00:16:32,930
We're going to kinda
get to what we've done

370
00:16:32,930 --> 00:16:36,469
over the past I don't know, a
year or so to try to reconcile

371
00:16:36,470 --> 00:16:39,330
all these biometric laws
and provide some method

372
00:16:39,330 --> 00:16:43,230
for reviewing solutions, and products,

373
00:16:43,230 --> 00:16:45,683
and services that
Microsoft tries to create.

374
00:16:46,982 --> 00:16:49,130
And again, this is
focused on the legal side.

375
00:16:49,130 --> 00:16:51,420
So we've also made a
bunch of policy decisions

376
00:16:51,420 --> 00:16:54,900
like whether to sell facial
recognition tech to police

377
00:16:54,900 --> 00:16:57,079
and I'm not gonna get into
those policy decisions.

378
00:16:57,080 --> 00:16:58,131
I'm not on the policy team

379
00:16:58,131 --> 00:17:00,280
and I haven't been involved
in those discussions,

380
00:17:00,280 --> 00:17:03,252
so I wouldn't have anything
intelligent or helpful to say.

381
00:17:04,260 --> 00:17:08,629
But so step one, what we
did first was we looked

382
00:17:08,630 --> 00:17:10,349
at our existing review systems

383
00:17:10,349 --> 00:17:12,109
and frameworks and just
looked for the Delta.

384
00:17:12,109 --> 00:17:15,859
So if you work in an org
of moderate size or bigger,

385
00:17:15,859 --> 00:17:17,829
you likely have a
framework around processing

386
00:17:17,829 --> 00:17:21,089
personal data that you created
in response to the GDPR,

387
00:17:21,089 --> 00:17:25,500
the CCPA, or whatever relevant
law in your jurisdiction.

388
00:17:25,500 --> 00:17:29,830
But if not, it's certainly
very feasible to start fresh.

389
00:17:29,830 --> 00:17:32,370
And I don't think starting fresh is bad,

390
00:17:32,370 --> 00:17:33,909
or it means you're late.

391
00:17:33,910 --> 00:17:35,580
It likely means you've
had other priorities

392
00:17:35,580 --> 00:17:38,199
or you've had got different circumstances.

393
00:17:38,200 --> 00:17:40,770
And now you have the opportunity to create

394
00:17:40,770 --> 00:17:43,639
something relevant for
your size, your market,

395
00:17:43,640 --> 00:17:45,560
your processing, or
the other circumstances

396
00:17:45,560 --> 00:17:47,820
that surround you.

397
00:17:47,820 --> 00:17:50,290
So what I discovered versus
that we needed a definition

398
00:17:50,290 --> 00:17:54,080
of biometric data to
enable the fact-finding.

399
00:17:54,080 --> 00:17:58,580
So rather than start
with engineering and ask,

400
00:17:58,580 --> 00:18:01,230
Hey, what biometric
data are you processing?

401
00:18:01,230 --> 00:18:04,838
We needed to give engineering
teams a definition

402
00:18:04,838 --> 00:18:07,680
that we said this is biometric data,

403
00:18:07,680 --> 00:18:09,260
can you tell us what all the places

404
00:18:09,260 --> 00:18:11,460
where you're processing
this biometric data.

405
00:18:13,190 --> 00:18:16,650
And it was a little bit odd to me

406
00:18:16,650 --> 00:18:19,550
'cause I thought, oh, we'll
just start with asking people,

407
00:18:19,550 --> 00:18:21,010
everyone what they're doing.

408
00:18:21,010 --> 00:18:24,820
And it quickly became untenable,

409
00:18:24,820 --> 00:18:28,300
because different engineering
groups have different ideas

410
00:18:28,300 --> 00:18:30,440
of what biometric data is.

411
00:18:30,440 --> 00:18:33,350
So we needed a definition
to drive the fact finding.

412
00:18:33,350 --> 00:18:35,439
And the definition was tricky to create,

413
00:18:35,440 --> 00:18:37,290
due to the difference in
the laws we've discussed

414
00:18:37,290 --> 00:18:42,290
like CCPA has this
capacity component to it.

415
00:18:42,670 --> 00:18:44,710
And most of the other laws do not.

416
00:18:44,710 --> 00:18:47,430
So we were slightly overbroad,

417
00:18:47,430 --> 00:18:49,450
and I think it's likely worth
being slightly overbroad

418
00:18:49,450 --> 00:18:51,590
to your definition, particularly due to

419
00:18:51,590 --> 00:18:53,617
the sharp teeth of BIPA.

420
00:18:53,617 --> 00:18:56,600
But it may be tolerable
to your risk appetite

421
00:18:56,600 --> 00:18:58,620
depending on who you are to
exclude certain processing

422
00:18:58,620 --> 00:19:00,469
that could include biometric data.

423
00:19:00,470 --> 00:19:02,210
For example, as I just
talked about the CCPA

424
00:19:02,210 --> 00:19:03,750
has this capacity component,

425
00:19:03,750 --> 00:19:07,080
but you might not include
the capacity component

426
00:19:07,080 --> 00:19:09,800
in your definition and
just worry about places

427
00:19:09,800 --> 00:19:13,330
where you're actually creating a vector.

428
00:19:13,330 --> 00:19:14,800
You're actually driving something

429
00:19:14,800 --> 00:19:16,840
from that raw underlying data.

430
00:19:16,840 --> 00:19:21,490
And until this capacity
component of CCPA's definition

431
00:19:21,490 --> 00:19:23,420
gains more force either via regs

432
00:19:23,420 --> 00:19:26,240
or laws or judicial decisions.

433
00:19:26,240 --> 00:19:29,570
It might be tolerable to exclude
that from your definition.

434
00:19:29,570 --> 00:19:32,520
But I advise against
absorbing any BIPA risks

435
00:19:32,520 --> 00:19:34,310
at the definitional stage.

436
00:19:34,310 --> 00:19:37,560
It's just make sure your
definition captures all of BIPA.

437
00:19:37,560 --> 00:19:39,280
You can take other BIPA risks later.

438
00:19:39,280 --> 00:19:41,210
You may want to take
other BIPA risks later on.

439
00:19:41,210 --> 00:19:43,820
You may not want to, but I
would advise try to avoid

440
00:19:43,820 --> 00:19:46,370
taking a BIPA risk, it's
just this like first step.

441
00:19:47,870 --> 00:19:49,770
And so the next thing we did

442
00:19:49,770 --> 00:19:52,910
after creating this definition
was to discover our uses.

443
00:19:52,910 --> 00:19:54,340
And when I say discovery uses,

444
00:19:54,340 --> 00:19:55,949
I don't mean like we had
no idea what we're doing.

445
00:19:55,950 --> 00:19:58,500
Like we have privacy
teams they're reviewing

446
00:19:59,820 --> 00:20:01,790
all processing, all
personal data processing,

447
00:20:01,790 --> 00:20:03,940
but we had this like
special project created

448
00:20:03,940 --> 00:20:07,190
to when biometric litigation
was really taken off

449
00:20:07,190 --> 00:20:08,750
we had the special project, it was like,

450
00:20:08,750 --> 00:20:11,410
let's make sure that
we got a good handle on

451
00:20:11,410 --> 00:20:12,457
of our biometric data processing

452
00:20:12,457 --> 00:20:14,464
and we know what we're doing.

453
00:20:14,464 --> 00:20:17,520
So, circulate the definition
to engineering teams

454
00:20:17,520 --> 00:20:20,320
after feedback, encourage
questions and feedback.

455
00:20:20,320 --> 00:20:21,929
The engineering team should
send back descriptions.

456
00:20:21,930 --> 00:20:24,350
There's areas that meet the definition.

457
00:20:24,350 --> 00:20:26,469
And then, so you read the
description and scenarios.

458
00:20:26,470 --> 00:20:29,570
This likely, this involves
some back and forth,

459
00:20:29,570 --> 00:20:30,760
and be prepared to spend some time

460
00:20:30,760 --> 00:20:32,090
understanding what the team is doing

461
00:20:32,090 --> 00:20:33,919
and why they're doing it.

462
00:20:33,920 --> 00:20:35,476
But after reviewing your scenarios,

463
00:20:35,476 --> 00:20:37,560
you'll have a much better idea

464
00:20:37,560 --> 00:20:39,879
of the processing entity engaged in.

465
00:20:39,880 --> 00:20:41,748
Maybe it's just fingerprint scanning tech,

466
00:20:41,748 --> 00:20:44,170
provided by a vendor to
access secure building.

467
00:20:44,170 --> 00:20:46,540
Maybe that's all the biometric
processing you're doing.

468
00:20:46,540 --> 00:20:47,540
Maybe you're at the forefront

469
00:20:47,540 --> 00:20:49,010
of creating a biometric techniques

470
00:20:49,010 --> 00:20:51,100
across the gamut of use cases.

471
00:20:51,100 --> 00:20:54,139
But neither event you'll
have a really good idea

472
00:20:54,140 --> 00:20:55,220
of the amount of work you have

473
00:20:55,220 --> 00:20:56,510
after seeing the brief descriptions

474
00:20:56,510 --> 00:20:58,810
of all the scenarios and
meet your definition.

475
00:21:00,210 --> 00:21:04,750
So then after you get your use cases

476
00:21:04,750 --> 00:21:06,730
what we did is create some categories

477
00:21:06,730 --> 00:21:10,150
and of environment of use
cases that were common.

478
00:21:10,150 --> 00:21:14,880
And so, for example, and like the gamut

479
00:21:14,880 --> 00:21:18,220
of facial techniques, you
start with facial recognition

480
00:21:18,220 --> 00:21:21,410
which is verifying that Chris is Chris

481
00:21:21,410 --> 00:21:24,730
or John is John, or Jane is Jane.

482
00:21:24,730 --> 00:21:26,390
That's comparing a facial template

483
00:21:26,390 --> 00:21:29,030
against another facial
template to identify

484
00:21:29,030 --> 00:21:32,810
or authenticate that person,
but that's on the one end.

485
00:21:32,810 --> 00:21:35,360
And on the other end of
the spectrum is technology,

486
00:21:35,360 --> 00:21:38,040
that involves what a lot of people refer

487
00:21:38,040 --> 00:21:40,570
to as facial detection,
where facial detection

488
00:21:40,570 --> 00:21:43,990
is just biometric processing to determine

489
00:21:43,990 --> 00:21:46,478
whether that's a human
face, but it has no idea.

490
00:21:46,478 --> 00:21:49,890
So it's taking measurements and
there's technical processing

491
00:21:49,890 --> 00:21:53,830
of the image, but it's
not identifying the person

492
00:21:53,830 --> 00:21:55,439
or figuring out, it can't even distinguish

493
00:21:55,440 --> 00:21:58,140
between person A and person B, all it does

494
00:21:58,140 --> 00:22:00,700
is specific annual
processing to figure out

495
00:22:00,700 --> 00:22:02,080
that there's a face.

496
00:22:02,080 --> 00:22:03,379
And you see this all over the place.

497
00:22:03,380 --> 00:22:07,310
Like if you pull out your smartphone

498
00:22:07,310 --> 00:22:09,470
and open up whatever camera app you use,

499
00:22:09,470 --> 00:22:12,400
then you try to take a picture,
you'll see a bounding box

500
00:22:12,400 --> 00:22:14,670
around the people's faces in the picture

501
00:22:14,670 --> 00:22:15,570
you're about to snap.

502
00:22:15,570 --> 00:22:19,139
And that bounding box is using
facial detection technology

503
00:22:19,140 --> 00:22:20,390
to detect those faces.

504
00:22:20,390 --> 00:22:24,970
And it might focus in on those
faces, adjust the lighting,

505
00:22:24,970 --> 00:22:26,540
because often when people
are taking pictures,

506
00:22:26,540 --> 00:22:29,129
they want the faces of the people

507
00:22:29,130 --> 00:22:33,020
they're taking a picture
of to be well-focused,

508
00:22:33,020 --> 00:22:35,135
be the highlight of the picture.

509
00:22:35,135 --> 00:22:38,090
But there's a whole gamut of
activities on that spectrum

510
00:22:38,090 --> 00:22:40,139
between facial recognition,
or verification,

511
00:22:40,140 --> 00:22:41,940
or identification on one end

512
00:22:41,940 --> 00:22:43,630
and facial detection on the other end.

513
00:22:43,630 --> 00:22:44,950
These are things like facial character,

514
00:22:44,950 --> 00:22:46,090
but in the middle there's things like

515
00:22:46,090 --> 00:22:49,889
facial characterization
which is one step beyond

516
00:22:49,890 --> 00:22:52,250
facial detection which is okay, well,

517
00:22:52,250 --> 00:22:54,480
there's a specific technical
processing on the image

518
00:22:54,480 --> 00:22:56,320
to detect that the human face.

519
00:22:56,320 --> 00:22:57,960
And then the next step is,

520
00:22:57,960 --> 00:23:00,430
well, what are some
characteristics of that human face?

521
00:23:00,430 --> 00:23:02,891
Like, is this a male or a female?

522
00:23:02,891 --> 00:23:07,891
Is this a person in their
20s or a person in their 70s?

523
00:23:07,920 --> 00:23:10,100
Does this person look angry or sad

524
00:23:10,100 --> 00:23:11,860
or what's their emotional state?

525
00:23:11,860 --> 00:23:14,870
And companies might use
facial characterization

526
00:23:14,870 --> 00:23:16,949
for any number of things,
like the classic example

527
00:23:16,950 --> 00:23:19,730
is in the retail context, a retailer

528
00:23:19,730 --> 00:23:22,700
might wanna understand
the aggregate demographics

529
00:23:22,700 --> 00:23:24,463
of their customers.

530
00:23:25,455 --> 00:23:27,590
Doesn't really care who came in

531
00:23:27,590 --> 00:23:31,280
but the retailer might
be very interested if 80%

532
00:23:31,280 --> 00:23:36,220
of their customers are angry
when they walk into the store.

533
00:23:36,220 --> 00:23:38,470
Like why are our customers
so angry when they walk into,

534
00:23:38,470 --> 00:23:40,010
oh, why they're so angry when do they walk

535
00:23:40,010 --> 00:23:41,853
into this one part of the store?

536
00:23:43,560 --> 00:23:47,190
And that spectrum for
different facial technologies

537
00:23:47,190 --> 00:23:51,490
is not unique to face, it's
in the voice context as well.

538
00:23:51,490 --> 00:23:56,010
So voice detection, we're
on a Zoom meeting right now.

539
00:23:56,010 --> 00:24:00,680
I don't know anything about
Zoom's tech, but it very well

540
00:24:00,680 --> 00:24:04,060
a lot of voice software
detects the human voice

541
00:24:04,060 --> 00:24:05,760
and it may detect the human voice

542
00:24:05,760 --> 00:24:07,950
applies in specific technical processing

543
00:24:07,950 --> 00:24:12,920
to detect that voice to
make the tech better,

544
00:24:12,920 --> 00:24:15,130
whatever kind of product
or service they're offering

545
00:24:15,130 --> 00:24:17,010
to minimize ambient noise.

546
00:24:17,010 --> 00:24:18,750
If there's a dog barking
in the background,

547
00:24:18,750 --> 00:24:21,360
to minimize that noise and maximize

548
00:24:21,360 --> 00:24:23,303
and focus on the human voice.

549
00:24:24,740 --> 00:24:26,850
And then of course,
there's voice recognition

550
00:24:26,850 --> 00:24:28,790
and voice authentication, and verification

551
00:24:28,790 --> 00:24:29,777
on the other end of the spectrum,

552
00:24:29,777 --> 00:24:31,797
"like my voice is my password."

553
00:24:32,700 --> 00:24:35,130
And in the middle there's
speaker diarization,

554
00:24:35,130 --> 00:24:39,020
which is grouping people by their voice.

555
00:24:39,020 --> 00:24:42,370
So there's a broad spectrum of things

556
00:24:42,370 --> 00:24:44,068
and you wanna create subcategories

557
00:24:44,068 --> 00:24:48,562
for the different biometric
scenarios you're involved in.

558
00:24:50,720 --> 00:24:55,180
And so the next step is to
create varying requirements

559
00:24:57,210 --> 00:24:59,790
for different buckets,
based on the sophistication,

560
00:24:59,790 --> 00:25:02,550
and identifiability of
your biometric scenarios.

561
00:25:02,550 --> 00:25:05,470
You may only need one
bucket, or you may need 20,

562
00:25:05,470 --> 00:25:09,050
but the buckets will be separated
by a handful of factors.

563
00:25:09,050 --> 00:25:13,300
So the biggest factor most
likely is unique identification

564
00:25:13,300 --> 00:25:15,300
yes or no. And as we just talked about

565
00:25:15,300 --> 00:25:17,100
think about face rep facial recognition

566
00:25:17,100 --> 00:25:19,530
versus facial detection,
facial recognition

567
00:25:19,530 --> 00:25:21,740
which involves a unique
identification is gonna fall

568
00:25:21,740 --> 00:25:24,630
in a higher risk bucket
than facial detection.

569
00:25:24,630 --> 00:25:26,620
But even within facial recognition

570
00:25:26,620 --> 00:25:29,080
you may want different buckets.

571
00:25:29,080 --> 00:25:31,389
If the facial recognition
is one-to-one matching,

572
00:25:31,390 --> 00:25:32,530
or one-to-end matching.

573
00:25:32,530 --> 00:25:37,530
So one-to-one matching is
like using facial recognition

574
00:25:37,599 --> 00:25:39,510
to log into a device,

575
00:25:39,510 --> 00:25:41,270
the device only stores
one facial template,

576
00:25:41,270 --> 00:25:42,590
the owner of the device.

577
00:25:42,590 --> 00:25:44,399
And all it does is match

578
00:25:44,400 --> 00:25:47,485
whoever tries to look at
that device to unlock it,

579
00:25:47,485 --> 00:25:50,010
does this facial template
match the facial template

580
00:25:50,010 --> 00:25:51,410
we have stored on the device,

581
00:25:51,410 --> 00:25:53,410
that's just one-to-one matching.

582
00:25:53,410 --> 00:25:54,810
But there's also one to end matching,

583
00:25:54,810 --> 00:25:58,210
which could be a camera on a sidewalk

584
00:25:58,210 --> 00:25:59,810
that's taking a facial
template of everybody

585
00:25:59,810 --> 00:26:02,379
that walks by to match
against the database

586
00:26:02,380 --> 00:26:04,740
of 10,000 criminals.

587
00:26:04,740 --> 00:26:09,640
And in those scenarios, a
riskier scenario, of course,

588
00:26:09,640 --> 00:26:12,603
unlawful in a lot of places about,

589
00:26:13,520 --> 00:26:16,879
you're matching every single data subject

590
00:26:16,880 --> 00:26:18,750
against the database of 10,000 templates,

591
00:26:18,750 --> 00:26:21,330
as opposed to just one template.

592
00:26:21,330 --> 00:26:24,399
And then of course the
geography you operate in

593
00:26:24,400 --> 00:26:25,470
or where you sell your product,

594
00:26:25,470 --> 00:26:29,990
or where you're located
is another a fact pivot

595
00:26:29,990 --> 00:26:32,700
that is gonna factor
into your risk buckets.

596
00:26:32,700 --> 00:26:35,690
And if you operate only in Ohio,

597
00:26:35,690 --> 00:26:37,270
your requirements gonna
look a lot different

598
00:26:37,270 --> 00:26:38,973
than if you're global.

599
00:26:40,120 --> 00:26:42,649
Another risk factors, are you a controller

600
00:26:42,650 --> 00:26:45,340
or a processor as we've discussed though,

601
00:26:45,340 --> 00:26:48,530
a lot of U.S. laws don't
recognize this distinction.

602
00:26:48,530 --> 00:26:52,899
So I think that that is
an important distinction

603
00:26:52,900 --> 00:26:55,262
and will have impact on your risk buckets

604
00:26:55,262 --> 00:26:58,583
but it's not explicitly provided for

605
00:26:58,584 --> 00:27:01,317
in every law regarding
biometrics in the U.S..

606
00:27:03,170 --> 00:27:05,900
So the next step is to
design your requirements

607
00:27:05,900 --> 00:27:08,710
along those risk pivots
we just talked about.

608
00:27:08,710 --> 00:27:12,850
So the first requirement that
we did was a lawful basis.

609
00:27:12,850 --> 00:27:14,909
And so if you're doing
unique identification,

610
00:27:14,910 --> 00:27:16,780
you're likely gonna need to use consent.

611
00:27:16,780 --> 00:27:20,260
Now, there are exceptions in
various places around the world

612
00:27:21,680 --> 00:27:23,730
but generally for unique identification,

613
00:27:23,730 --> 00:27:27,960
consent is gonna be the
most common lawful basis.

614
00:27:27,960 --> 00:27:29,890
Other lawful basis are gonna be available

615
00:27:29,890 --> 00:27:32,110
in other situations, though, for example

616
00:27:32,110 --> 00:27:34,429
legitimate interests might work

617
00:27:34,430 --> 00:27:37,400
in situations that do not
involve unique identification.

618
00:27:37,400 --> 00:27:39,350
Like let's say you have a camera app

619
00:27:39,350 --> 00:27:41,520
for mobile devices and
the camera app detects

620
00:27:41,520 --> 00:27:43,220
human faces using automated measures

621
00:27:43,220 --> 00:27:45,530
of specific technical
processing, automated measures

622
00:27:45,530 --> 00:27:48,320
again from the Virginia law
specific technical processing

623
00:27:48,320 --> 00:27:51,040
from the GDPR to put a
rectangle around the faces

624
00:27:51,040 --> 00:27:53,110
and highlight the faces,
adjust the lighting, et cetera.

625
00:27:53,110 --> 00:27:55,469
We just talked about
the app doesn't identify

626
00:27:55,470 --> 00:27:57,280
just detects human faces.

627
00:27:57,280 --> 00:27:59,720
Legitimate interest may very
well be a very reasonable basis

628
00:27:59,720 --> 00:28:01,290
to use in that circumstance return rather

629
00:28:01,290 --> 00:28:04,278
than trying to get consent
from whomever is in the picture

630
00:28:04,278 --> 00:28:07,330
to consent to the boundary
box around their face.

631
00:28:07,330 --> 00:28:10,039
Another requirement you're
gonna have to deal with

632
00:28:10,039 --> 00:28:12,879
with biometrics is
retention and destruction.

633
00:28:12,880 --> 00:28:17,660
Many biometric laws have
peculiar retention requirements.

634
00:28:17,660 --> 00:28:19,090
And so you're gonna need requirements

635
00:28:19,090 --> 00:28:21,483
for how long you can
retain the biometric data.

636
00:28:23,760 --> 00:28:25,590
A lot of processing, though, of course

637
00:28:25,590 --> 00:28:27,280
you don't need to retain
any of the biometric data.

638
00:28:27,280 --> 00:28:30,470
For example, face
detecting for a camera app.

639
00:28:30,470 --> 00:28:32,120
You don't need to retain anything

640
00:28:34,010 --> 00:28:36,360
after the picture is taken.

641
00:28:36,360 --> 00:28:38,199
Another requirement
you'll likely wanna have

642
00:28:38,200 --> 00:28:39,700
is this disclosure to the third party.

643
00:28:39,700 --> 00:28:41,760
Under BIPA, even disclosures to vendors

644
00:28:41,760 --> 00:28:43,795
who act on your behalf
may require consent.

645
00:28:43,795 --> 00:28:45,830
It's not clear that there is an exception

646
00:28:45,830 --> 00:28:47,795
to the consent requirement under BIPA

647
00:28:47,796 --> 00:28:50,100
to even pass biometrics to a vendor

648
00:28:50,100 --> 00:28:53,120
who's operating strictly on your behalf.

649
00:28:53,120 --> 00:28:56,840
But again, try to avoid
redundant or duplicative

650
00:28:56,840 --> 00:28:58,550
or contradictory requirements as compared

651
00:28:58,550 --> 00:29:01,159
to your normal policies or
practices on personal data.

652
00:29:01,160 --> 00:29:03,580
So, like I said a little bit earlier

653
00:29:03,580 --> 00:29:07,060
as you probably already got some framework

654
00:29:07,060 --> 00:29:09,732
for how to evaluate
personal data processing.

655
00:29:09,732 --> 00:29:11,530
You don't need to create a second...

656
00:29:11,530 --> 00:29:14,580
You don't need to create
a redundant framework.

657
00:29:14,580 --> 00:29:16,990
You just wanna address the delta

658
00:29:16,990 --> 00:29:20,653
for biometrical requirements
that you may already have.

659
00:29:22,730 --> 00:29:27,730
So let's talk about some of
the risks that might arise.

660
00:29:28,780 --> 00:29:31,310
There's a numerous of them.

661
00:29:31,310 --> 00:29:33,870
And I'll discuss a few
of the common risks.

662
00:29:33,870 --> 00:29:37,810
So, as we've talked about,
particularly in the U.S.,

663
00:29:37,810 --> 00:29:40,139
many of these state
laws do not distinguish

664
00:29:40,140 --> 00:29:42,470
between processors and controllers.

665
00:29:42,470 --> 00:29:45,340
As we discussed earlier,
this can make compliance plan

666
00:29:45,340 --> 00:29:47,459
really difficult for processors.

667
00:29:47,460 --> 00:29:48,440
The lack of distinction

668
00:29:48,440 --> 00:29:51,720
doesn't really have a
significant impact on controllers

669
00:29:51,720 --> 00:29:53,640
or more accurately the
party with the relationship

670
00:29:53,640 --> 00:29:55,100
with the data subject.

671
00:29:55,100 --> 00:29:57,610
The controller generally is in
a position to obtain consent

672
00:29:57,610 --> 00:29:59,909
or whatever is required
from the data subject.

673
00:29:59,910 --> 00:30:03,830
But the processor is in a
much more difficult situation.

674
00:30:03,830 --> 00:30:05,500
As we discussed, the processor may be able

675
00:30:05,500 --> 00:30:08,540
to shift responsibility to
the controller via contract,

676
00:30:08,540 --> 00:30:10,520
but that shifting won't help the processor

677
00:30:10,520 --> 00:30:11,940
when a motion to dismiss in the face

678
00:30:11,940 --> 00:30:13,770
of the consumer class action

679
00:30:13,770 --> 00:30:15,920
you will merely provide
a hook to recoup damages

680
00:30:15,920 --> 00:30:18,170
and costs from the
controller after the fact.

681
00:30:19,450 --> 00:30:21,340
Another common risk is bystanders

682
00:30:21,340 --> 00:30:24,790
or data subjects who are
not the intended target

683
00:30:24,790 --> 00:30:27,050
of the biometric solution.

684
00:30:27,050 --> 00:30:28,310
If you're deploying biometrics

685
00:30:28,310 --> 00:30:31,129
outside of a controlled space,
like outside of a warehouse

686
00:30:31,130 --> 00:30:33,560
where you count money or something.

687
00:30:33,560 --> 00:30:35,840
The risk of bystanders will
almost always be present.

688
00:30:35,840 --> 00:30:38,010
And if you think about a photo
app that allows consumers

689
00:30:38,010 --> 00:30:41,900
to quickly sort photos using
facial grouping technology

690
00:30:41,900 --> 00:30:44,000
the tech allows the user to quickly see

691
00:30:44,000 --> 00:30:46,360
all the faces that match person A.

692
00:30:46,360 --> 00:30:49,270
Perhaps the user spouse, parent, or child.

693
00:30:49,270 --> 00:30:50,760
But what about all the pictures of person

694
00:30:50,760 --> 00:30:53,420
at the Beach Museum or Times Square?

695
00:30:53,420 --> 00:30:55,230
The photo apps have to
create a facial template

696
00:30:55,230 --> 00:30:58,749
of everyone in the picture in
order to find all the pictures

697
00:30:58,749 --> 00:30:59,870
of person A in the album.

698
00:30:59,870 --> 00:31:02,610
The facial templates of the
bystanders, they are a femoral,

699
00:31:02,610 --> 00:31:04,889
the app doesn't store them
for more than a second

700
00:31:04,890 --> 00:31:07,460
to compare against the template person A,

701
00:31:07,460 --> 00:31:09,540
but they are created.

702
00:31:09,540 --> 00:31:12,740
And so do you get
consent for those people?

703
00:31:12,740 --> 00:31:14,980
And that's a very different category

704
00:31:14,980 --> 00:31:17,080
from someone who intentionally
submits themselves

705
00:31:17,080 --> 00:31:19,210
to a fingerprint scanner,
or a pommery, or the clock

706
00:31:19,210 --> 00:31:20,750
in a pay, if you have
a fingerprint scanner,

707
00:31:20,750 --> 00:31:23,090
the clock in a pay,
somebody could come use it.

708
00:31:23,090 --> 00:31:25,310
That's not the intended person

709
00:31:25,310 --> 00:31:28,580
that is not the intended data
subject that hasn't consented

710
00:31:28,580 --> 00:31:30,870
but that person
intentionally went up there

711
00:31:30,870 --> 00:31:32,659
and use the fingerprint scanner.

712
00:31:32,660 --> 00:31:34,560
These folks were just at a museum.

713
00:31:34,560 --> 00:31:35,590
And the example we're just talking about,

714
00:31:35,590 --> 00:31:37,290
these folks are just
at a museum or a beach

715
00:31:37,290 --> 00:31:39,629
or time square, and
someone unknown to them

716
00:31:39,630 --> 00:31:41,700
captured them in a photo.

717
00:31:41,700 --> 00:31:44,170
That person's photo app
then created a facial,

718
00:31:44,170 --> 00:31:47,863
an ephemeral facial template
of all the bystanders.

719
00:31:48,880 --> 00:31:52,630
So the photo app could push
the consent requirement

720
00:31:52,630 --> 00:31:54,830
to the user via terms and conditions.

721
00:31:54,830 --> 00:31:57,750
Don't use this facial grouping feature

722
00:31:57,750 --> 00:32:01,005
unless you get consent from
everybody in the photo.

723
00:32:01,005 --> 00:32:04,580
And that may be the
most feasible solution.

724
00:32:04,580 --> 00:32:07,050
At least if we want the
technology to be enabled.

725
00:32:07,050 --> 00:32:09,090
But as a user really
gonna go obtain consent

726
00:32:09,090 --> 00:32:11,169
for everyone in the photo,
now, no, one's gonna walk

727
00:32:11,170 --> 00:32:14,443
around times square and
ask everyone to consent.

728
00:32:15,310 --> 00:32:17,379
And perhaps consent is not something

729
00:32:17,380 --> 00:32:20,650
we think should be
required from these folks,

730
00:32:20,650 --> 00:32:22,290
but at the picture is taken in Illinois,

731
00:32:22,290 --> 00:32:25,077
some replace Times Square
with Michigan Avenue,

732
00:32:25,077 --> 00:32:27,210
or perhaps some other jurisdictions.

733
00:32:27,210 --> 00:32:29,560
There's certainly risks without consent

734
00:32:29,560 --> 00:32:30,993
from the individuals.

735
00:32:32,660 --> 00:32:34,240
There's of course, other
risks that you'll discover

736
00:32:34,240 --> 00:32:35,970
but it might be worth talking
about some of the actions

737
00:32:35,970 --> 00:32:38,530
that you can use to mitigate those risks.

738
00:32:38,530 --> 00:32:42,020
And so one of the biggest ones
is device-side processing.

739
00:32:42,020 --> 00:32:44,620
As much as possible perform
the biometric functions

740
00:32:44,620 --> 00:32:46,209
on the edge of the user's device.

741
00:32:46,210 --> 00:32:48,090
If you've got a camera in a warehouse

742
00:32:48,090 --> 00:32:50,820
performing some course
biometrics for security purposes,

743
00:32:50,820 --> 00:32:53,070
keep the biometric processing
and the results local

744
00:32:53,070 --> 00:32:55,720
on the camera, on the edge
device in the warehouse,

745
00:32:55,720 --> 00:32:59,200
don't upload, or store, or use,
the biometrics in the cloud,

746
00:32:59,200 --> 00:33:00,033
if at all possible.

747
00:33:00,033 --> 00:33:01,530
And obviously gonna depend on your tech

748
00:33:01,530 --> 00:33:03,470
and whoever your vendors tech is,

749
00:33:03,470 --> 00:33:06,263
or if you're the vendor, it's
gonna depend on your tech.

750
00:33:07,120 --> 00:33:08,840
But in the context of the photos app,

751
00:33:08,840 --> 00:33:11,740
we just talked about the risk
is likely a whole lot lower

752
00:33:11,740 --> 00:33:15,803
if the facial templates never
leave that user's device.

753
00:33:17,300 --> 00:33:19,730
Use courts techniques,
don't use facial recognition

754
00:33:19,730 --> 00:33:22,620
when facial detection will
suffice for your objectives.

755
00:33:22,620 --> 00:33:25,330
Many of these go without saying
but don't retain templates.

756
00:33:25,330 --> 00:33:28,030
If you don't need them, don't
retain any biometric data

757
00:33:28,030 --> 00:33:29,051
if you don't need it.

758
00:33:29,051 --> 00:33:31,270
But even consider dropping
aggregated learnings

759
00:33:31,270 --> 00:33:34,190
if have no minimal value,
particularly in Illinois,

760
00:33:34,190 --> 00:33:36,000
I mean, if it defines
biometric information

761
00:33:36,000 --> 00:33:38,570
to collude any info
based on an individual's

762
00:33:38,570 --> 00:33:39,963
biometric identifier.

763
00:33:41,210 --> 00:33:44,764
I can't intelligently get
into conflicts of laws issues,

764
00:33:44,764 --> 00:33:48,510
but geofencing likely is an option

765
00:33:48,510 --> 00:33:50,030
for some of your products.

766
00:33:50,030 --> 00:33:52,090
And geofencing doesn't
necessarily mean wholly

767
00:33:52,090 --> 00:33:54,949
excluding riskier
jurisdictions, such as Illinois.

768
00:33:54,950 --> 00:33:56,560
It may mean having different settings

769
00:33:56,560 --> 00:33:59,240
or consent experiences for
different jurisdictions.

770
00:33:59,240 --> 00:34:01,990
So just adjusting a toggle,

771
00:34:01,990 --> 00:34:05,300
what the default state of a
toggle in certain jurisdictions

772
00:34:05,300 --> 00:34:06,783
versus other jurisdictions.

773
00:34:08,360 --> 00:34:10,670
The next step is determined reviewers.

774
00:34:10,670 --> 00:34:13,254
More than likely there's
gonna be residual risk

775
00:34:13,254 --> 00:34:16,400
despite whatever input
mitigations you implement

776
00:34:16,400 --> 00:34:18,404
with some perspective,
many of your solutions.

777
00:34:18,404 --> 00:34:20,489
The reviewers will need
to assess that risk

778
00:34:20,489 --> 00:34:22,572
and give a go or no-go determination.

779
00:34:24,719 --> 00:34:27,929
So let's talk a little bit
about BIPA Litigation Trends

780
00:34:27,929 --> 00:34:30,960
getting kind of back to
away from the process

781
00:34:30,960 --> 00:34:32,610
of creating this framework for reviewing

782
00:34:32,610 --> 00:34:34,060
your biometric solutions

783
00:34:34,060 --> 00:34:37,860
and talk about specific
of litigation trends.

784
00:34:37,860 --> 00:34:39,840
So now that we're done
with the, how to section

785
00:34:39,840 --> 00:34:41,355
I thought I'd give some thoughts

786
00:34:41,355 --> 00:34:42,210
on some current litigation trends

787
00:34:42,210 --> 00:34:44,632
and make some probably shaky predictions.

788
00:34:45,989 --> 00:34:48,189
Although BIPA litigation has exploded

789
00:34:48,190 --> 00:34:49,850
over the past five years or so,

790
00:34:49,850 --> 00:34:52,799
we're still somewhat in the
infancy of the litigation.

791
00:34:52,800 --> 00:34:55,550
There's lots of opinion
than decisions on standing

792
00:34:55,550 --> 00:34:57,500
but there's still a
relative dearth of decisions

793
00:34:57,500 --> 00:35:00,480
interpreting the substantive
requirements of the statute.

794
00:35:00,480 --> 00:35:01,967
Like what are biometric identifiers

795
00:35:01,967 --> 00:35:04,130
and biometric information?

796
00:35:04,130 --> 00:35:05,740
Many of the lawsuits concern items

797
00:35:05,740 --> 00:35:08,399
clearly within the definition,
such as fingerprint scanners

798
00:35:08,400 --> 00:35:11,060
to clock employees in and
out, but we're starting

799
00:35:11,060 --> 00:35:13,820
to see cases that may help
define the outer boundaries

800
00:35:13,820 --> 00:35:15,160
of the terms.

801
00:35:15,160 --> 00:35:17,069
For example, there have been in our cases

802
00:35:17,070 --> 00:35:19,150
percolating regarding voice prints

803
00:35:19,150 --> 00:35:21,500
which are considered biometric
identifiers under BIPA,

804
00:35:21,500 --> 00:35:23,880
but BIPA doesn't define
or provide any context

805
00:35:23,880 --> 00:35:25,760
regarding what's a voiceprint.

806
00:35:25,760 --> 00:35:28,240
So we're seeing cases against
voice activated assistants

807
00:35:28,240 --> 00:35:30,220
alleging that their tech
involves voiceprint.

808
00:35:30,220 --> 00:35:32,372
So these likely will turn on the facts

809
00:35:32,373 --> 00:35:35,230
for the specific assistance,
but they could be helpful

810
00:35:35,230 --> 00:35:37,110
in determining what is a voiceprint.

811
00:35:37,110 --> 00:35:40,080
Similarly for scans of
face and hand geometry

812
00:35:40,080 --> 00:35:43,549
also included in the term
biometric identifier and BIPA,

813
00:35:43,550 --> 00:35:46,270
we're starting to see cases
that test these definitions.

814
00:35:46,270 --> 00:35:48,040
So we're starting to see
cases alleging technology

815
00:35:48,040 --> 00:35:51,750
like facial grouping is a
scan of facial geometry.

816
00:35:51,750 --> 00:35:53,490
Facial grouping of the
tech we just talked about

817
00:35:53,490 --> 00:35:58,479
that allows users to group
images of people by face.

818
00:35:58,480 --> 00:36:00,420
The app creates an ephemeral template

819
00:36:00,420 --> 00:36:01,780
of everyone in the user's album

820
00:36:01,780 --> 00:36:03,520
and then shows only photos that match

821
00:36:03,520 --> 00:36:05,759
the subject the user want to see.

822
00:36:05,760 --> 00:36:08,080
The app has no idea who the person is.

823
00:36:08,080 --> 00:36:09,900
Although I might let the
user label the person

824
00:36:09,900 --> 00:36:11,810
but the label could be anything.

825
00:36:11,810 --> 00:36:13,220
But the app does create a template

826
00:36:13,220 --> 00:36:15,439
of every person in the album.

827
00:36:15,440 --> 00:36:17,720
Is that a scan, a facial geometry,

828
00:36:17,720 --> 00:36:20,410
or does BIPA require some
level of identification

829
00:36:20,410 --> 00:36:23,299
or knowledge of who the data subject is?

830
00:36:23,300 --> 00:36:26,110
And some of you, particularly
in Europe might assert

831
00:36:26,110 --> 00:36:28,860
that any automated comparison
of facial templates

832
00:36:28,860 --> 00:36:31,278
regardless of whether the
templates are processed

833
00:36:31,278 --> 00:36:33,799
ephemerally, or the
company can make more info

834
00:36:33,800 --> 00:36:35,410
about the data from that template

835
00:36:35,410 --> 00:36:36,827
does unique identification.

836
00:36:36,827 --> 00:36:39,980
And I think most of European
DPA's have that view

837
00:36:39,980 --> 00:36:42,720
based on the EDPD guidance
on processing video data

838
00:36:42,720 --> 00:36:44,330
for 2020.

839
00:36:44,331 --> 00:36:46,690
So let's take the example
one step further away

840
00:36:46,690 --> 00:36:47,830
from identification.

841
00:36:47,830 --> 00:36:50,090
What about facial characterization?

842
00:36:50,090 --> 00:36:51,273
The BBB guidance indicates

843
00:36:51,273 --> 00:36:53,860
that that is not unique identification,

844
00:36:53,860 --> 00:36:56,090
but it would be considered
a scan of facial geometry

845
00:36:56,090 --> 00:36:58,310
under BIPA, these types of questions

846
00:36:58,310 --> 00:37:00,110
takes some time to resolve via litigation,

847
00:37:00,110 --> 00:37:01,580
but there's beginning to be cases

848
00:37:01,580 --> 00:37:04,259
that will help folks
provide substantive guidance

849
00:37:04,260 --> 00:37:06,440
on those types of questions.

850
00:37:06,440 --> 00:37:08,550
Another interesting line
of cases is the validity

851
00:37:08,550 --> 00:37:11,510
of the exclusions of
photographs from the definition

852
00:37:11,510 --> 00:37:14,370
of biometric information,
a biometric identifiers.

853
00:37:14,370 --> 00:37:16,730
A handful of district courts have held,

854
00:37:16,730 --> 00:37:19,870
some of you may think this has
done this question is over,

855
00:37:19,870 --> 00:37:21,120
but I don't think so.

856
00:37:21,120 --> 00:37:23,310
I handful of district courts
have held this exclusion

857
00:37:23,310 --> 00:37:26,430
does not apply to techniques
to derive biometrics

858
00:37:26,430 --> 00:37:28,180
from online or digital photos,

859
00:37:28,180 --> 00:37:30,069
but there's certainly textual support

860
00:37:30,070 --> 00:37:33,280
for the argument and
defendants continue to make it.

861
00:37:33,280 --> 00:37:36,550
So a favorable decision for
defendants from four defendants

862
00:37:36,550 --> 00:37:38,270
are favorable for defendants, of course,

863
00:37:38,270 --> 00:37:40,070
from a higher court could really damp it

864
00:37:40,070 --> 00:37:44,050
by major class actions
focused on online photos.

865
00:37:44,050 --> 00:37:47,090
We've talked briefly
about service providers,

866
00:37:47,090 --> 00:37:48,200
vendors, and processors

867
00:37:48,200 --> 00:37:50,509
and whether they are subject
to BIPA requirements.

868
00:37:50,509 --> 00:37:52,610
It's a hot question for many suppliers

869
00:37:52,610 --> 00:37:54,340
of biometric technology.

870
00:37:54,340 --> 00:37:56,180
As BIPA it doesn't
contain that distinction

871
00:37:56,180 --> 00:37:58,000
between controllers and processors.

872
00:37:58,000 --> 00:38:00,030
There's certainly an
argument that processors

873
00:38:00,030 --> 00:38:02,840
are responsible for obtaining
consent from data subjects

874
00:38:02,840 --> 00:38:04,760
even if the processors
have no relationship

875
00:38:04,760 --> 00:38:06,840
with the data subjects and
simply take instructions

876
00:38:06,840 --> 00:38:08,238
from the controller.

877
00:38:08,238 --> 00:38:10,580
The early decisions we've seen so far

878
00:38:10,580 --> 00:38:14,640
from district courts seem to
focus probably unsurprisingly

879
00:38:14,640 --> 00:38:16,620
on the text of the statute.

880
00:38:16,620 --> 00:38:18,770
For example, the consent
obligation or BIPA

881
00:38:18,770 --> 00:38:21,829
applies on a private
entity collects, captures,

882
00:38:21,829 --> 00:38:25,140
purchases, received through trade

883
00:38:25,140 --> 00:38:28,290
or otherwise attains biometric data.

884
00:38:28,290 --> 00:38:29,920
At least one district court has focused

885
00:38:29,920 --> 00:38:31,830
on whether the vendor took an active step,

886
00:38:31,830 --> 00:38:35,100
an active step is not in the text of BIPA,

887
00:38:35,100 --> 00:38:38,569
but trying to figure out what
that may interpret that means

888
00:38:38,570 --> 00:38:40,390
the court tried to look
at whether the vendor

889
00:38:40,390 --> 00:38:42,879
took an active step to
obtain use of biometric data

890
00:38:42,880 --> 00:38:45,357
as opposed to receiving it passively.

891
00:38:45,357 --> 00:38:46,820
And another record indicated

892
00:38:46,820 --> 00:38:48,930
that service provider
could comply with BIPA

893
00:38:48,930 --> 00:38:51,419
by contractually requiring
its customer to obtain consent

894
00:38:51,420 --> 00:38:52,950
from data subjects.

895
00:38:52,950 --> 00:38:55,430
But I don't think that
actually insurance compliance

896
00:38:55,430 --> 00:38:57,136
it just provides an
opportunity for recourse

897
00:38:57,137 --> 00:39:00,060
as we talked about earlier.

898
00:39:00,060 --> 00:39:03,710
In any event, I would draw
too many conclusions yet,

899
00:39:03,710 --> 00:39:06,660
other than I think it's
unlikely U.S. courts will draw

900
00:39:06,660 --> 00:39:09,450
a bright line controller
and processor distinction.

901
00:39:09,450 --> 00:39:11,470
I think that's a solution
that would have to come

902
00:39:11,470 --> 00:39:14,832
from the legislature as
opposed to the courts.

903
00:39:16,630 --> 00:39:19,548
So to talk about a few
forward looking developments

904
00:39:19,548 --> 00:39:21,370
I won't spend too much time here

905
00:39:21,370 --> 00:39:22,549
but I thought it'd be worthwhile to note

906
00:39:22,550 --> 00:39:24,563
a few forward looking developments.

907
00:39:25,470 --> 00:39:27,990
Biometrics used to be the
purview of trial attorneys.

908
00:39:27,990 --> 00:39:29,910
Sure, there was the odd guidance

909
00:39:29,910 --> 00:39:32,029
from European and DPA here or there,

910
00:39:32,030 --> 00:39:33,970
but Clearview changed that.

911
00:39:33,970 --> 00:39:36,270
Governments and regulators
are getting heavily involved

912
00:39:36,270 --> 00:39:37,731
with around the world.

913
00:39:37,731 --> 00:39:39,670
I don't wanna spend too
much more time talking

914
00:39:39,670 --> 00:39:41,852
about facial grouping,
which I've already talked

915
00:39:41,852 --> 00:39:44,410
about that a lot, but a
recent FTC enforcement action

916
00:39:44,410 --> 00:39:47,460
I think against every
album demonstrates the FTCs

917
00:39:47,460 --> 00:39:49,410
increasing attention toward biometrics.

918
00:39:49,410 --> 00:39:51,899
And one of the very interesting components

919
00:39:51,900 --> 00:39:54,980
of that settlement as reiterated
in certain commissioners

920
00:39:54,980 --> 00:39:56,760
company statements was that every album

921
00:39:56,760 --> 00:39:59,000
was required to delete models

922
00:39:59,000 --> 00:40:02,070
derived from the allegedly
unlawful biometric processing.

923
00:40:02,070 --> 00:40:04,290
It wasn't just required
to delete the raw data

924
00:40:04,290 --> 00:40:06,860
or the biometric data, it was
required to delete its IP,

925
00:40:06,860 --> 00:40:08,760
its models for creating, using,

926
00:40:08,760 --> 00:40:10,837
and processing biometric data.

927
00:40:10,837 --> 00:40:14,220
This requirement I
think will gain traction

928
00:40:14,220 --> 00:40:15,200
and future settlements,

929
00:40:15,200 --> 00:40:17,669
particularly those
involving biometric data.

930
00:40:17,670 --> 00:40:20,070
And it could be much more
impactful to companies

931
00:40:21,000 --> 00:40:23,603
than a monetary fine of almost any amount.

932
00:40:24,615 --> 00:40:27,910
There had been a flurry of
state laws at a high level.

933
00:40:27,910 --> 00:40:29,339
The laws come in two forms.

934
00:40:29,340 --> 00:40:31,800
Those that specifically
regulate biometrics

935
00:40:31,800 --> 00:40:34,080
and those that are
comprehensive privacy laws

936
00:40:34,080 --> 00:40:36,283
like the GDPR, CCPA or Virginia.

937
00:40:37,340 --> 00:40:39,650
Personally, I like
comprehensive privacy laws.

938
00:40:39,650 --> 00:40:41,010
I think they're preferable

939
00:40:41,010 --> 00:40:42,920
for ease of understanding
and accessibility.

940
00:40:42,920 --> 00:40:45,080
So people can go find
the laws and read them,

941
00:40:45,080 --> 00:40:46,113
and interpret them.

942
00:40:47,390 --> 00:40:49,190
And private and comprehensive privacy laws

943
00:40:49,190 --> 00:40:51,900
can always have provisions
that apply only to biometrics

944
00:40:51,900 --> 00:40:55,130
or only to special categories
or sensitive personal data.

945
00:40:55,130 --> 00:40:56,780
But despite my personal preference

946
00:40:56,780 --> 00:40:58,870
I think we're gonna continue
to see both types of laws

947
00:40:58,870 --> 00:41:00,053
proposed and enacted.

948
00:41:01,207 --> 00:41:04,529
Despite the intense focus
on the CCPA and BIPA,

949
00:41:04,530 --> 00:41:06,910
there are competing
models for comprehensive

950
00:41:06,910 --> 00:41:09,080
and biometric focused privacy laws.

951
00:41:09,080 --> 00:41:12,196
And I think we may see
some of competing models

952
00:41:12,196 --> 00:41:14,180
advanced but enacted.

953
00:41:14,180 --> 00:41:15,220
I mean, Virginia enacted

954
00:41:15,220 --> 00:41:16,750
a comprehensive privacy law recently

955
00:41:16,750 --> 00:41:18,980
and Washington State has been
pushing a comprehensive law

956
00:41:18,980 --> 00:41:20,820
for years, but thus far at least

957
00:41:20,820 --> 00:41:22,620
it's died in the legislature every year.

958
00:41:22,620 --> 00:41:24,970
And on the biometrics specific side, Texas

959
00:41:24,970 --> 00:41:28,029
and Washington State both have
biometrics specific statutes.

960
00:41:28,030 --> 00:41:29,430
They don't get as much attention as BIPA,

961
00:41:29,430 --> 00:41:31,690
because BIPA has far more litigation.

962
00:41:31,690 --> 00:41:34,240
But in debating between which model enact,

963
00:41:34,240 --> 00:41:37,240
legislators should look past
the headlines for statutes

964
00:41:37,240 --> 00:41:40,009
that protect individual privacy
while providing companies

965
00:41:40,010 --> 00:41:41,800
with clear rules to follow

966
00:41:41,800 --> 00:41:45,411
and opacity is not good for
data subjects or businesses.

967
00:41:45,411 --> 00:41:47,590
Voluminous litigation can be assigned

968
00:41:47,590 --> 00:41:49,200
the data subjects are
enforcing their rights

969
00:41:49,200 --> 00:41:51,589
against companies that are
disregarding those rights

970
00:41:51,590 --> 00:41:53,770
or voluminous litigation can be a sign

971
00:41:53,770 --> 00:41:56,153
of an opaque statute with sharp teeth.

972
00:41:57,560 --> 00:42:00,259
A higher level debate that's
being had in some policy

973
00:42:00,260 --> 00:42:02,390
and governmental arenas
is whether government

974
00:42:02,390 --> 00:42:04,779
should permit biometrics at all,

975
00:42:04,780 --> 00:42:07,210
at least biometrics that
involve unique identification.

976
00:42:07,210 --> 00:42:09,230
And I think the opening two sentences

977
00:42:09,230 --> 00:42:11,290
in former commissioner Chopra's statement,

978
00:42:11,290 --> 00:42:12,680
accompanying the Emory album's settlement

979
00:42:12,680 --> 00:42:14,567
are really noteworthy, and he wrote

980
00:42:14,567 --> 00:42:16,671
"Today's best facial
recognition technology

981
00:42:16,671 --> 00:42:20,747
"is fundamentally flawed and
reinforces harmful biases.

982
00:42:20,747 --> 00:42:22,707
"I support efforts to enact moratoria

983
00:42:22,707 --> 00:42:25,520
"or otherwise severely restrict its use."

984
00:42:25,520 --> 00:42:27,120
These sentences have nothing
to do with the alleged

985
00:42:27,120 --> 00:42:30,759
deceptive conduct underpinning
the FTCs enforcement action.

986
00:42:30,760 --> 00:42:32,790
Rather they were called a ban,

987
00:42:32,790 --> 00:42:36,150
probably temporarily facial
recognition technology

988
00:42:36,150 --> 00:42:38,960
as a whole, and former
commissioner Trover is not

989
00:42:38,960 --> 00:42:41,610
the only person who
feels this way, many do.

990
00:42:41,610 --> 00:42:43,610
And there's a debate happening around

991
00:42:43,610 --> 00:42:46,190
whether biometrical
should simply impose a ban

992
00:42:46,190 --> 00:42:47,440
on their use until some
indeterminate point

993
00:42:47,440 --> 00:42:50,890
in the future when policy
makers or the public

994
00:42:50,890 --> 00:42:54,900
are assuage that the benefits
of the biometric technology

995
00:42:54,900 --> 00:42:57,773
facial recognition particularly
outweigh their costs.

996
00:42:59,150 --> 00:43:01,160
This is just kinda some highlights

997
00:43:01,160 --> 00:43:04,272
of relevant mitigation and
enforcement that we've seen.

998
00:43:06,321 --> 00:43:08,783
One of the highlights,
some of the headlines,

999
00:43:10,440 --> 00:43:13,180
many of you are probably familiar
with Facebook $650 million

1000
00:43:13,180 --> 00:43:14,373
biometric settlement.

1001
00:43:15,292 --> 00:43:20,292
We talked about the Alvarez
album, FTC enforcement action,

1002
00:43:20,300 --> 00:43:22,040
the Rosenbach v. Six Flags.

1003
00:43:22,040 --> 00:43:24,450
That's a reference to the
Illinois Supreme Court decision

1004
00:43:24,450 --> 00:43:27,399
under BIPA, which held that
a violation of the statute

1005
00:43:27,400 --> 00:43:30,474
is a cognizable harm that can be sued for.

1006
00:43:30,474 --> 00:43:33,783
The plaintiffs don't have to
show some additional injury.

1007
00:43:35,540 --> 00:43:40,540
And the bottom two screenshots show

1008
00:43:41,000 --> 00:43:44,503
kind of the diverging
paths that are of some,

1009
00:43:44,503 --> 00:43:47,680
the results in cases against
service providers or vendors

1010
00:43:47,680 --> 00:43:50,872
that don't have the direct
relationship with data subjects.

1011
00:43:50,872 --> 00:43:54,290
So you're seeing like the
BIPA class claims advance

1012
00:43:54,290 --> 00:43:55,710
against the company that
provides biometric try

1013
00:43:55,710 --> 00:43:58,410
and keep a diverse timeframe
advice to employers

1014
00:43:58,410 --> 00:44:01,649
that was against Kronos Data
subjects sued the provider

1015
00:44:01,650 --> 00:44:04,240
of the timekeeping device directly.

1016
00:44:04,240 --> 00:44:06,350
And Kronos lost a motion to dismiss

1017
00:44:06,350 --> 00:44:09,310
and then kinda contradict early Amazon

1018
00:44:09,310 --> 00:44:11,640
and pin drop motions
to dismiss were granted

1019
00:44:11,640 --> 00:44:12,690
in a voiceprint suit.

1020
00:44:13,810 --> 00:44:16,134
Now the reason it's not the courts,

1021
00:44:16,135 --> 00:44:19,290
there's different facts, but
the Amazon suit was dismissed

1022
00:44:19,290 --> 00:44:23,550
because it didn't have
sufficient connection to Illinois

1023
00:44:23,550 --> 00:44:28,240
but Amazon was providing a
voice recognition service

1024
00:44:28,240 --> 00:44:32,160
as a processor, as a vendor to
another company John Hancock

1025
00:44:32,160 --> 00:44:35,230
and Amazon won a motion to dismiss.

1026
00:44:35,230 --> 00:44:38,340
So just kind of a highlight
of the differences

1027
00:44:38,340 --> 00:44:41,457
that are happening at the
district court level right now

1028
00:44:41,457 --> 00:44:43,859
but things are moving to the higher courts

1029
00:44:43,860 --> 00:44:46,995
and BIPA litigation against
vendors and service providers.

1030
00:44:46,995 --> 00:44:50,319
And just lastly before we wrap up,

1031
00:44:50,320 --> 00:44:53,121
just wanted to give you a
summary of like citations.

1032
00:44:53,121 --> 00:44:54,760
I think almost everything I referenced

1033
00:44:54,760 --> 00:44:59,760
in this presentation is linked
to and highlighted here.

1034
00:45:00,230 --> 00:45:03,540
But if I mentioned something that was not,

1035
00:45:03,540 --> 00:45:05,440
feel free to reach out to me

1036
00:45:06,470 --> 00:45:09,122
and I will be happy to provide a site.

1037
00:45:10,080 --> 00:45:14,650
Oh yeah, so I see one
possible amendments to BIPA.

1038
00:45:14,650 --> 00:45:16,400
So there have been a couple

1039
00:45:16,400 --> 00:45:20,680
of legislative proposals to change BIPA

1040
00:45:20,680 --> 00:45:24,450
and one of the ones that I
think got the most attention

1041
00:45:24,450 --> 00:45:28,169
was a proposal that would
have kind of reigned

1042
00:45:28,170 --> 00:45:33,170
in the litigation, the
class action nature of BIPA.

1043
00:45:33,170 --> 00:45:35,700
My understanding, and this is not

1044
00:45:35,700 --> 00:45:40,259
from any kind of discussions
with legislators in Illinois

1045
00:45:41,530 --> 00:45:42,363
or anything.

1046
00:45:42,363 --> 00:45:44,660
My understanding is those
proposals have all died

1047
00:45:44,660 --> 00:45:46,529
on the vine, and it's unlikely,

1048
00:45:46,530 --> 00:45:48,730
at least in this legislative session

1049
00:45:48,730 --> 00:45:52,453
that we're going to see any
significant amendments to BIPA.

1050
00:45:53,378 --> 00:45:56,580
I'm not saying that they
won't happen in the future

1051
00:45:56,580 --> 00:45:59,330
but I think it's unlikely
that they're going to happen

1052
00:45:59,330 --> 00:46:02,232
at this legislative session in Illinois.

1053
00:46:03,700 --> 00:46:05,439
But that's just based on reading articles,

1054
00:46:05,440 --> 00:46:08,030
not on any kind of inside information

1055
00:46:08,030 --> 00:46:10,060
from Illinois legislators.

1056
00:46:10,060 --> 00:46:11,790
Yeah, that's a great question.

1057
00:46:11,790 --> 00:46:13,509
So one question, one of
the other questions is

1058
00:46:13,510 --> 00:46:15,448
do you think there could be a trend

1059
00:46:15,448 --> 00:46:17,740
for those laws to be closer to each other

1060
00:46:17,740 --> 00:46:19,542
as the use of biometrics is growing?

1061
00:46:22,360 --> 00:46:23,840
I do think that's possible.

1062
00:46:23,840 --> 00:46:28,840
So to the extent, and I think this is good

1063
00:46:31,060 --> 00:46:33,090
for everyone is to have a clear definition

1064
00:46:33,090 --> 00:46:35,597
of what falls within
biometrics and what doesn't.

1065
00:46:35,597 --> 00:46:37,578
I think it's good for plaintiffs

1066
00:46:37,578 --> 00:46:38,742
and I think its good for defenders.

1067
00:46:38,742 --> 00:46:39,920
I think it's good for data subjects.

1068
00:46:39,920 --> 00:46:40,753
And I think it's good for companies

1069
00:46:40,753 --> 00:46:41,910
that are processing biometric data.

1070
00:46:41,910 --> 00:46:45,970
And hopefully this is
just my personal opinion

1071
00:46:45,970 --> 00:46:47,870
about how that definition will happen.

1072
00:46:47,870 --> 00:46:49,029
But I think there is a room

1073
00:46:49,030 --> 00:46:52,138
because there aren't too too
many competing definitions yet.

1074
00:46:52,138 --> 00:46:55,940
And I think some of
them are much preferable

1075
00:46:55,940 --> 00:47:00,640
to others in the minds of a
large portion of the populace.

1076
00:47:00,640 --> 00:47:05,640
So the CCPA definition I think
is troublesome to implement

1077
00:47:07,350 --> 00:47:11,839
because it has this capacity, a component,

1078
00:47:11,840 --> 00:47:14,709
which is like can you do
something even if you don't?

1079
00:47:14,709 --> 00:47:19,109
And I think hopefully
that does not catch on.

1080
00:47:19,110 --> 00:47:22,042
And the CCPA, there's the CPRA

1081
00:47:22,042 --> 00:47:25,373
and there's gonna be the California board.

1082
00:47:25,373 --> 00:47:27,810
I think that definition could be amended

1083
00:47:27,810 --> 00:47:30,515
to get rid of that capacity component.

1084
00:47:30,515 --> 00:47:34,400
And maybe that capacity
component wasn't even intended.

1085
00:47:34,400 --> 00:47:36,240
So I think there's an ability to like,

1086
00:47:36,240 --> 00:47:39,750
I do think there is some
possibility that these definitions

1087
00:47:39,750 --> 00:47:41,970
will kinda streamline into more something

1088
00:47:41,970 --> 00:47:44,529
that looks more like the
GDPR or the Virginia law

1089
00:47:44,530 --> 00:47:48,550
which requires some type
of automated processing,

1090
00:47:48,550 --> 00:47:51,320
some type of specific tactical processing.

1091
00:47:51,320 --> 00:47:55,600
And whether there's a unique
identification component or not

1092
00:47:55,600 --> 00:47:57,970
I think is gonna be a
little bit up in the air.

1093
00:47:57,970 --> 00:47:59,980
I don't know if the laws
will coalesce around that,

1094
00:47:59,980 --> 00:48:00,970
they may coalesce around

1095
00:48:00,970 --> 00:48:03,060
and there doesn't have to
be a unique identification.

1096
00:48:03,060 --> 00:48:05,940
I know that GDPR language kinda suggests

1097
00:48:05,940 --> 00:48:07,461
there should be unique identification

1098
00:48:07,461 --> 00:48:11,736
but then all the DPA guidance
from the individual DPA's

1099
00:48:11,737 --> 00:48:15,500
and EPB states that things
like facial characterization

1100
00:48:15,500 --> 00:48:18,180
are biometric processing

1101
00:48:18,180 --> 00:48:21,129
but there's no unique
identification in that scenario.

1102
00:48:21,130 --> 00:48:22,280
Okay, I see another question came in.

1103
00:48:22,280 --> 00:48:24,420
Do I see proceeding federal
legislation on biometrics?

1104
00:48:24,420 --> 00:48:26,033
No I don't.

1105
00:48:27,981 --> 00:48:28,894
And I don't.

1106
00:48:28,894 --> 00:48:33,857
Other than some situations
specific biometrics,

1107
00:48:33,858 --> 00:48:36,470
potentially there could
be some about police

1108
00:48:36,470 --> 00:48:38,165
or about governmental use
or something like that,

1109
00:48:38,166 --> 00:48:42,930
but I don't foresee any kind
of broad biomedical legislation

1110
00:48:42,930 --> 00:48:44,960
from the federal government applicable

1111
00:48:44,960 --> 00:48:46,721
to the private sector.

1112
00:48:46,721 --> 00:48:51,510
I think probably if asked me
that question six months ago,

1113
00:48:51,510 --> 00:48:54,050
or maybe a year ago, I thought
there was a good chance

1114
00:48:54,050 --> 00:48:56,146
for some kind of federal
Comprehensive Privacy Legislation

1115
00:48:56,146 --> 00:48:59,450
that could have included
a biometrics component.

1116
00:48:59,450 --> 00:49:01,029
But I feel like that's died.

1117
00:49:01,030 --> 00:49:06,030
I feel like both parties
have other priorities now

1118
00:49:07,220 --> 00:49:11,319
and federal privacy legislation
has taken a little bit

1119
00:49:11,320 --> 00:49:15,610
of a backseat and I don't
think it's a priority

1120
00:49:15,610 --> 00:49:17,810
a top priority of either party right now.

1121
00:49:17,810 --> 00:49:21,049
Now obviously if president
Biden came out tomorrow

1122
00:49:21,050 --> 00:49:23,720
and said Federal Privacy Legislation

1123
00:49:23,720 --> 00:49:27,529
is my number one priority,
that would change overnight.

1124
00:49:27,530 --> 00:49:30,140
But I think that's unlikely.

1125
00:49:30,140 --> 00:49:32,270
So I think we're probably waiting a while

1126
00:49:32,270 --> 00:49:33,990
for any type of Federal
Privacy Legislation

1127
00:49:33,990 --> 00:49:37,720
including Federal Privacy
Legislation focused on biometrics.

1128
00:49:37,720 --> 00:49:39,386
Somebody asked me, "you know
what's my personal approach

1129
00:49:39,386 --> 00:49:42,017
"to using my own devices,
using biometric tech

1130
00:49:42,017 --> 00:49:44,857
"on my own devices in terms of security?"

1131
00:49:46,398 --> 00:49:47,933
I use it.

1132
00:49:50,550 --> 00:49:53,120
Generally, I prefer, I know I'm certainly

1133
00:49:53,120 --> 00:49:57,210
no security expert, but I
generally try to use biometrics

1134
00:49:57,210 --> 00:49:59,100
for things that post passwords.

1135
00:49:59,100 --> 00:50:00,009
And I have that opportunity.

1136
00:50:00,010 --> 00:50:05,010
And when it's relatively
reputable company, I log in

1137
00:50:07,050 --> 00:50:10,433
to my phone with my face, used
to log in with my thumbprint.

1138
00:50:11,930 --> 00:50:14,960
I don't have, and maybe
this is just because

1139
00:50:15,864 --> 00:50:19,440
I'm not as much, I'm
not a social media user

1140
00:50:19,440 --> 00:50:21,840
or I don't take a ton of pictures.

1141
00:50:21,840 --> 00:50:25,130
I don't do a lot of the
photo stuff with biometrics

1142
00:50:25,130 --> 00:50:27,880
but that's not so much that
hesitancy towards the biometrics

1143
00:50:27,880 --> 00:50:30,713
as it is just kind of like personal use.

1144
00:50:32,580 --> 00:50:35,100
Have there been dialogue
about controlling biometrics

1145
00:50:35,100 --> 00:50:37,370
in the context of greater
accountability for police?

1146
00:50:37,370 --> 00:50:41,500
Yeah, I think there certainly has been.

1147
00:50:41,500 --> 00:50:43,350
And sorry, maybe I'm
misinterpreting this question

1148
00:50:43,350 --> 00:50:46,930
but there are a lot of laws percolating

1149
00:50:46,930 --> 00:50:48,440
particularly the state and local level

1150
00:50:48,440 --> 00:50:49,550
and some have already been enacted

1151
00:50:49,550 --> 00:50:51,900
about the police use of biometrics.

1152
00:50:51,900 --> 00:50:53,370
So Washington State, for example

1153
00:50:53,370 --> 00:50:58,370
passed a law last year on
police and governmental use

1154
00:50:58,770 --> 00:51:01,779
of biometrics and restricting its use.

1155
00:51:01,779 --> 00:51:04,880
It is not directly applicable
on the private sector,

1156
00:51:04,880 --> 00:51:06,130
but of course, if the private sector

1157
00:51:06,130 --> 00:51:09,870
is gonna try to sell
some biometric solution

1158
00:51:09,870 --> 00:51:12,000
to a government or a police

1159
00:51:12,000 --> 00:51:13,220
then there are only gonna be interested

1160
00:51:13,220 --> 00:51:15,879
in solutions that can comply
with the Washington State Law

1161
00:51:15,880 --> 00:51:17,210
and the Washington
State's not alone in that,

1162
00:51:17,210 --> 00:51:19,660
there's other localities in
states that are passing laws

1163
00:51:19,660 --> 00:51:21,319
like that all the time.

1164
00:51:21,320 --> 00:51:22,153
All right, well, thanks everyone.

1165
00:51:22,153 --> 00:51:23,653
I really appreciate your time.

1166
00:51:24,810 --> 00:51:26,040
Again, feel free to reach out to me

1167
00:51:26,040 --> 00:51:28,750
if you have any questions,
chris.hydak@microsoft.com

1168
00:51:28,750 --> 00:51:31,466
or if you just wanna chat about anything,

1169
00:51:31,466 --> 00:51:35,460
Hydak is H-Y-D-A-K have
a great rest of your day.

1170
00:51:35,460 --> 00:51:36,665
I appreciate your time.

1171
00:51:36,665 --> 00:51:37,498
Bye.

