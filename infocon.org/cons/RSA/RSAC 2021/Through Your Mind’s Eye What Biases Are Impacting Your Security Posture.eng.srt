1
00:00:01,550 --> 00:00:03,490
- Welcome to RSA.

2
00:00:03,490 --> 00:00:06,043
I'm Linda Grindstaff,
Vice President McAfee

3
00:00:06,043 --> 00:00:08,530
in our office of the CTO.

4
00:00:08,530 --> 00:00:11,827
And I'm excited to be here
with you today to discuss

5
00:00:11,827 --> 00:00:14,710
"Through Your Mind's Eye:
What Biases Are Impacting

6
00:00:14,710 --> 00:00:16,550
Your Security Posture?"

7
00:00:16,550 --> 00:00:18,950
Be sure to enter your questions in chat

8
00:00:18,950 --> 00:00:22,150
as I'll be answering questions
throughout the presentation.

9
00:00:22,150 --> 00:00:23,189
Let's get started.

10
00:00:23,190 --> 00:00:24,520
(mouse clicking)

11
00:00:24,520 --> 00:00:26,919
Cyber security personnel
such as yourselves

12
00:00:26,920 --> 00:00:30,890
have many things on your plates
to address every single day.

13
00:00:30,890 --> 00:00:33,140
You may be feeling like this
woman here in the picture

14
00:00:33,140 --> 00:00:37,030
as you're dealing with what
alerts need to be investigated,

15
00:00:37,030 --> 00:00:38,920
what systems need to be patched

16
00:00:38,920 --> 00:00:41,030
for the latest vulnerabilities,

17
00:00:41,030 --> 00:00:43,420
to what do you need to tell
your board of directors

18
00:00:43,420 --> 00:00:45,290
in that upcoming meeting?

19
00:00:45,290 --> 00:00:48,839
We all make thousands
of decisions every day.

20
00:00:48,840 --> 00:00:51,870
However, our brains
don't give each decision

21
00:00:51,870 --> 00:00:53,260
equal attention.

22
00:00:53,260 --> 00:00:55,599
We take mental shortcuts.

23
00:00:55,600 --> 00:00:58,870
These mental shortcuts are known as biases

24
00:00:58,870 --> 00:01:02,793
and allows us to be able to
react quickly to situations.

25
00:01:03,690 --> 00:01:06,520
Security and biases are not
topics that are typically

26
00:01:06,520 --> 00:01:07,990
discussed together.

27
00:01:07,990 --> 00:01:11,940
However, we all have biases
that shape who we are.

28
00:01:11,940 --> 00:01:15,320
As a result, our decisions
in and out of security

29
00:01:15,320 --> 00:01:16,979
can be impacted.

30
00:01:16,980 --> 00:01:20,770
So let me ask you, "Do you
feel that you are biased?"

31
00:01:20,770 --> 00:01:22,820
No need to answer out loud.

32
00:01:22,820 --> 00:01:26,830
But, the biases that we do
have have influenced and shaped

33
00:01:26,830 --> 00:01:30,163
who we are today in
both good and bad ways.

34
00:01:31,196 --> 00:01:32,029
(mouse clicking)

35
00:01:32,029 --> 00:01:34,800
So what do you see when
you see these pictures?

36
00:01:34,800 --> 00:01:37,700
Do you see vases or faces?

37
00:01:37,700 --> 00:01:40,480
Do you see a young
woman or an older woman?

38
00:01:40,480 --> 00:01:42,780
Or a duck or a rabbit?

39
00:01:42,780 --> 00:01:45,840
I know many of you have seen
these types of pictures before.

40
00:01:45,840 --> 00:01:47,330
And my point of showing you

41
00:01:47,330 --> 00:01:50,370
is that while we're all
looking at the same picture,

42
00:01:50,370 --> 00:01:54,350
you may see one or both of
the objects in the pictures.

43
00:01:54,350 --> 00:01:58,009
This is representative of
what happens in real life.

44
00:01:58,010 --> 00:02:00,240
Many of us are at the
same meetings together

45
00:02:00,240 --> 00:02:02,330
but we leave with different perspectives

46
00:02:02,330 --> 00:02:05,380
about the discussion based on our biases.

47
00:02:05,380 --> 00:02:09,002
And these cognitive biases
influence us in many ways.

48
00:02:10,090 --> 00:02:12,560
For those of you that
don't know a cognitive bias

49
00:02:12,560 --> 00:02:15,580
is a result of our brains
attempt to simplify

50
00:02:15,580 --> 00:02:17,363
processing of information.

51
00:02:18,200 --> 00:02:20,637
The formal definition
in the dictionary says,

52
00:02:20,637 --> 00:02:24,770
"It's a systematic pattern
of deviation from norms

53
00:02:24,770 --> 00:02:26,490
in judgment."

54
00:02:26,490 --> 00:02:30,400
But we as individuals create
our own subjective reality

55
00:02:30,400 --> 00:02:34,360
from the perception of
those inputs that we have.

56
00:02:34,360 --> 00:02:38,460
And our construction of reality,
not the objective inputs

57
00:02:38,460 --> 00:02:41,800
are gonna dictate our
behavior in the world.

58
00:02:41,800 --> 00:02:44,810
So let's look at the different
types of cognitive biases

59
00:02:44,810 --> 00:02:48,090
that could be affecting your
company's security posture.

60
00:02:48,090 --> 00:02:50,730
And as I go through these, I'll
close with some tips for you

61
00:02:50,730 --> 00:02:53,480
on how to overcome those biases.

62
00:02:53,480 --> 00:02:56,239
One side note as we go through
the biases that you're going

63
00:02:56,240 --> 00:02:59,340
to see is that they're
not mutually exclusive.

64
00:02:59,340 --> 00:03:02,610
You'll see many overlaps and
things that you may hopefully

65
00:03:02,610 --> 00:03:05,710
relate to you in your
day to day activities.

66
00:03:05,710 --> 00:03:06,543
(mouse clicking)

67
00:03:06,543 --> 00:03:10,400
The first area is availability bias.

68
00:03:10,400 --> 00:03:14,350
Availability bias is a mental
shortcut that our brains use

69
00:03:14,350 --> 00:03:16,430
based on past examples,

70
00:03:16,430 --> 00:03:19,350
relating to information
that's available to us

71
00:03:19,350 --> 00:03:22,180
on a specific topic, event or decision.

72
00:03:22,180 --> 00:03:25,550
This information could come
from things that we saw

73
00:03:25,550 --> 00:03:29,890
in the news, we heard from
a friend, something we read,

74
00:03:29,890 --> 00:03:32,809
something we experienced, for instance.

75
00:03:32,810 --> 00:03:34,500
Basically it's something that's recalled

76
00:03:34,500 --> 00:03:36,130
in our memory quickly.

77
00:03:36,130 --> 00:03:38,690
And when we hear that
information frequently,

78
00:03:38,690 --> 00:03:40,900
we are able to recall it quickly

79
00:03:40,900 --> 00:03:44,160
thus our brains feel that
it's important to us.

80
00:03:44,160 --> 00:03:47,040
And then our brains think,
"Well information that we recall

81
00:03:47,040 --> 00:03:52,040
fast must be important, more
important than alternatives."

82
00:03:52,240 --> 00:03:54,427
And with all those urgent
interrupts that we have

83
00:03:54,427 --> 00:03:57,660
and the volume of things and
decisions that we have to,

84
00:03:57,660 --> 00:03:59,640
to think about and to figure out

85
00:03:59,640 --> 00:04:03,320
that need to be made by all
of us on the executive levels,

86
00:04:03,320 --> 00:04:06,829
in our CISOs and our administration,

87
00:04:06,830 --> 00:04:09,420
it's very easy to get caught
up in the decision-making

88
00:04:09,420 --> 00:04:13,329
based on the past information
that's available to us

89
00:04:13,330 --> 00:04:16,079
or the present information
that we see in the news

90
00:04:16,079 --> 00:04:17,880
or things like that.

91
00:04:17,880 --> 00:04:20,658
And so how does this bias impact security?

92
00:04:20,658 --> 00:04:22,630
For risk assessment.

93
00:04:22,630 --> 00:04:25,480
Availability bias may show up when

94
00:04:25,480 --> 00:04:28,020
your company board of
directors looks for an updated

95
00:04:28,020 --> 00:04:29,680
risk assessment.

96
00:04:29,680 --> 00:04:32,260
Rather than focusing
on the entire company,

97
00:04:32,260 --> 00:04:35,860
data could be presented
with respect to an area

98
00:04:35,860 --> 00:04:38,360
for which another company had a breach.

99
00:04:38,360 --> 00:04:42,470
For example, we've seen SolarWinds
in the news a lot lately,

100
00:04:42,470 --> 00:04:44,360
in the first part of this year,

101
00:04:44,360 --> 00:04:46,020
And the risk assessments,

102
00:04:46,020 --> 00:04:48,570
had they been looking at
all aspects of the business

103
00:04:48,570 --> 00:04:52,010
in depth and not just
focusing on supply chain

104
00:04:52,010 --> 00:04:54,670
because that's what we had
heard about in the news.

105
00:04:54,670 --> 00:04:56,820
Are there issues in your
environment that require

106
00:04:56,820 --> 00:04:58,990
more attention than what is trending

107
00:04:58,990 --> 00:05:01,890
that you see in the news or
that people are talking about?

108
00:05:03,030 --> 00:05:07,000
Risk assessment is also related
to overall preparedness.

109
00:05:07,000 --> 00:05:10,820
We see availability bias
play out in organizations

110
00:05:10,820 --> 00:05:13,540
preparing for things such as a high impact

111
00:05:13,540 --> 00:05:15,690
but low probability events,

112
00:05:15,690 --> 00:05:19,400
instead of preparing for
high probability events.

113
00:05:19,400 --> 00:05:23,289
High impact and low probability
events are things like

114
00:05:23,290 --> 00:05:28,260
an airplane crash or a volcano
eruption or shark attacks

115
00:05:28,260 --> 00:05:30,710
or even a meteorite hitting earth.

116
00:05:30,710 --> 00:05:34,659
These events have a high
impact but a low probability

117
00:05:34,660 --> 00:05:36,350
to occur.

118
00:05:36,350 --> 00:05:38,880
We tend to remember
those events more though

119
00:05:38,880 --> 00:05:40,890
than higher probability events.

120
00:05:40,890 --> 00:05:44,300
For example, when was the
last time that we heard about

121
00:05:44,300 --> 00:05:45,580
a ransomware attack?

122
00:05:45,580 --> 00:05:49,000
Or the last time someone's PII was stolen?

123
00:05:49,000 --> 00:05:52,420
Those are more of a low
impact or excuse me,

124
00:05:52,420 --> 00:05:55,020
high impact but low probability events.

125
00:05:55,020 --> 00:05:57,299
But we tend to not remember those as much

126
00:05:57,300 --> 00:06:00,170
as the high probability ones.

127
00:06:00,170 --> 00:06:04,520
And so what happens is that
that our environments and our,

128
00:06:04,520 --> 00:06:09,520
our administration and
people in our organizations

129
00:06:09,670 --> 00:06:13,190
they tend to prefer to
prepare for the wrong things.

130
00:06:13,190 --> 00:06:16,690
And if you listen to Steve
Grumman's keynote earlier,

131
00:06:16,690 --> 00:06:19,510
you saw that what we
should be worrying about

132
00:06:19,510 --> 00:06:21,932
doesn't always align with
what we do worry about

133
00:06:21,932 --> 00:06:26,120
and we're focused on the wrong
things as an organization.

134
00:06:26,120 --> 00:06:28,560
In the areas of decision-making,

135
00:06:28,560 --> 00:06:32,380
your CISO or the cyber security
analyst may make decisions

136
00:06:32,380 --> 00:06:34,483
in favor of hot topics in the news.

137
00:06:35,660 --> 00:06:38,630
And these topics may
overshadow other information

138
00:06:38,630 --> 00:06:42,280
that they know but aren't
exposed to as much.

139
00:06:42,280 --> 00:06:45,919
So as a result, decisions
are made that aren't well,

140
00:06:45,920 --> 00:06:47,530
well-rounded.

141
00:06:47,530 --> 00:06:51,130
For example, if there was
a recent IOT related issue

142
00:06:51,130 --> 00:06:55,270
similar to what we had back
in 2016 with the dine attacks,

143
00:06:55,270 --> 00:06:59,099
your analysts may over-focus
on IOT related items

144
00:06:59,100 --> 00:07:02,670
and neglect things like investing
in new security solutions

145
00:07:02,670 --> 00:07:04,363
for say your mobile devices.

146
00:07:05,870 --> 00:07:08,080
Around incident response.

147
00:07:08,080 --> 00:07:11,380
During an incident,
emotions are running high.

148
00:07:11,380 --> 00:07:14,730
And typically we need to
address the incident quickly.

149
00:07:14,730 --> 00:07:18,010
Availability bias may show up
in that we are not actually

150
00:07:18,010 --> 00:07:20,940
looking at the bigger
picture as to what's going on

151
00:07:20,940 --> 00:07:22,600
during the incident.

152
00:07:22,600 --> 00:07:25,670
And focus on scene only
where the incident occurred

153
00:07:25,670 --> 00:07:28,570
without ensuring we don't
have another issue waiting

154
00:07:28,570 --> 00:07:30,150
in the wings.

155
00:07:30,150 --> 00:07:33,250
Let's say somebody broke into
your home through a window.

156
00:07:33,250 --> 00:07:35,980
Your first thought may be
to secure all of the windows

157
00:07:35,980 --> 00:07:37,120
quickly.

158
00:07:37,120 --> 00:07:40,080
But, if you didn't look at
all of your security risk

159
00:07:40,080 --> 00:07:42,950
you may forget that you
can shake your garage door

160
00:07:42,950 --> 00:07:44,830
and it'll pop right open.

161
00:07:44,830 --> 00:07:47,560
And these are just a few examples

162
00:07:47,560 --> 00:07:49,730
where when you're looking
at incident response

163
00:07:49,730 --> 00:07:51,730
that you really need
to focus on everything

164
00:07:51,730 --> 00:07:52,603
that's happening.

165
00:07:53,880 --> 00:07:57,159
And our analysts are typically
exploring data thoroughly.

166
00:07:57,160 --> 00:07:59,910
However, executives may not always have

167
00:07:59,910 --> 00:08:01,710
in-depth information.

168
00:08:01,710 --> 00:08:03,210
So if you're at the executive level,

169
00:08:03,210 --> 00:08:05,239
I would definitely recommend
that you're reviewing

170
00:08:05,240 --> 00:08:08,960
all the facts beyond what is
available quickly in your mind

171
00:08:08,960 --> 00:08:11,840
so you can get the full
picture of the incident.

172
00:08:11,840 --> 00:08:14,810
How prepared are you for those risks?

173
00:08:14,810 --> 00:08:18,110
If you're an analyst or in
a position of influence,

174
00:08:18,110 --> 00:08:20,410
I would recommend summarizing the facts

175
00:08:20,410 --> 00:08:23,550
so that they accurately
reflect the probability

176
00:08:23,550 --> 00:08:25,240
of those events occurring

177
00:08:25,240 --> 00:08:28,343
as well as considering
all possible events.

178
00:08:30,110 --> 00:08:30,943
(mouse clicking)

179
00:08:30,943 --> 00:08:33,520
Another bias that we see in cybersecurity

180
00:08:33,520 --> 00:08:35,750
is confirmation bias.

181
00:08:35,750 --> 00:08:39,600
Confirmation bias is when you
look for things to confirm

182
00:08:39,600 --> 00:08:40,850
your own beliefs.

183
00:08:40,850 --> 00:08:44,450
Or you were, may remember
things that only confirm

184
00:08:44,450 --> 00:08:45,360
to your beliefs.

185
00:08:45,360 --> 00:08:49,180
Similar to what we just
discussed with availability bias.

186
00:08:49,180 --> 00:08:53,520
For example, your newsfeed
maybe full of things related

187
00:08:53,520 --> 00:08:56,590
to your political beliefs
based on what articles

188
00:08:56,590 --> 00:08:59,510
you've been clicking on, liked or sharing.

189
00:08:59,510 --> 00:09:02,120
Chances are it's not filled
with things that oppose

190
00:09:02,120 --> 00:09:03,033
your beliefs.

191
00:09:04,000 --> 00:09:07,870
Where we tend to see confirmation
bias in cyber security is

192
00:09:07,870 --> 00:09:09,790
in decision-making.

193
00:09:09,790 --> 00:09:12,810
Are you considering all of
the different points of view?

194
00:09:12,810 --> 00:09:15,270
Or are you just looking
into your close group

195
00:09:15,270 --> 00:09:18,406
of trusted advisors
who may think like you?

196
00:09:19,386 --> 00:09:22,439
Are you willing to push and
challenge your own beliefs

197
00:09:22,440 --> 00:09:25,270
to ensure that you're doing the
right thing for your company

198
00:09:25,270 --> 00:09:27,653
and making the best decisions possible?

199
00:09:28,900 --> 00:09:31,730
What's your company's security hygiene?

200
00:09:31,730 --> 00:09:34,530
Are you diligent about updating systems?

201
00:09:34,530 --> 00:09:37,290
Or do you believe that it
won't happen to you based on,

202
00:09:37,290 --> 00:09:38,892
because it hasn't happened yet?

203
00:09:39,870 --> 00:09:43,240
Are you using an XDR
solution in your environment?

204
00:09:43,240 --> 00:09:45,950
Or you do you feel that that's
just the latest buzzword

205
00:09:45,950 --> 00:09:47,980
of the industry and you
don't need it because

206
00:09:47,980 --> 00:09:51,363
your current systems are
serving your needs just fine?

207
00:09:52,430 --> 00:09:55,239
Do you feel that you're more
secure when you're in the cloud

208
00:09:55,240 --> 00:09:59,403
versus on-prem despite
human error can affect both?

209
00:10:00,290 --> 00:10:02,920
For risk assessment and preparedness,

210
00:10:02,920 --> 00:10:06,069
how do you approach
cybersecurity preparedness?

211
00:10:06,070 --> 00:10:09,103
Are you passive, reactive or progressive?

212
00:10:10,070 --> 00:10:13,650
Similarly to hygiene, do you
feel an incident won't happen

213
00:10:13,650 --> 00:10:18,090
to me so you look past
the data to confirm that?

214
00:10:18,090 --> 00:10:21,220
Or are you looking for that
data and so you're feeling

215
00:10:21,220 --> 00:10:23,810
like you may have a repeat
incident if you don't do

216
00:10:23,810 --> 00:10:25,122
everything possible?

217
00:10:26,220 --> 00:10:27,640
Whatever data you're looking for,

218
00:10:27,640 --> 00:10:29,290
is it confirming your beliefs?

219
00:10:29,290 --> 00:10:32,410
Or really focusing on
the data that's showing

220
00:10:32,410 --> 00:10:34,443
all of the facts and the evidence.

221
00:10:35,990 --> 00:10:39,590
Are you looking at everything
that you personally know well

222
00:10:39,590 --> 00:10:41,400
from when you were early in your career?

223
00:10:41,400 --> 00:10:43,610
Or are you looking at
what's actually happening

224
00:10:43,610 --> 00:10:45,230
in your environment?

225
00:10:45,230 --> 00:10:48,040
I've seen some analysts
ignore some of the alerts

226
00:10:48,040 --> 00:10:50,719
that come in because they
weren't quite sure how to deal

227
00:10:50,720 --> 00:10:52,480
with those newer alerts.

228
00:10:52,480 --> 00:10:54,990
So they just end up falling
back to what they know

229
00:10:54,990 --> 00:10:56,760
from early on in their career,

230
00:10:56,760 --> 00:10:58,819
or really just looking at the information

231
00:10:58,820 --> 00:11:01,640
that's readily available
instead of actually looking at

232
00:11:01,640 --> 00:11:02,990
everything that's possible.

233
00:11:04,370 --> 00:11:05,633
Around pen testing.

234
00:11:06,540 --> 00:11:10,069
Sometimes companies may
hire third-party companies

235
00:11:10,070 --> 00:11:13,170
or they themselves may have
penetration testing performed

236
00:11:13,170 --> 00:11:15,120
in their environments.

237
00:11:15,120 --> 00:11:17,950
When you define the scope
of your pen testing,

238
00:11:17,950 --> 00:11:20,240
are you looking for the gaps or the holes?

239
00:11:20,240 --> 00:11:22,410
Or are you just focusing on the weaknesses

240
00:11:22,410 --> 00:11:24,130
and the strengths?

241
00:11:24,130 --> 00:11:27,090
When the results come
in from the analysis,

242
00:11:27,090 --> 00:11:29,370
do you address everything
that is recommended?

243
00:11:29,370 --> 00:11:32,540
Or are you only focusing on
the items that you believe

244
00:11:32,540 --> 00:11:35,392
will impact just you and your business?

245
00:11:36,450 --> 00:11:39,040
I know it's hard to look
beyond what we believe

246
00:11:39,040 --> 00:11:41,490
because in our eyes that's ground truth.

247
00:11:41,490 --> 00:11:43,916
Our beliefs are all we have
and that's what we feel

248
00:11:43,916 --> 00:11:45,580
is the right thing.

249
00:11:45,580 --> 00:11:48,580
But it's important when you're
making security decisions

250
00:11:48,580 --> 00:11:52,240
that we're looking beyond
what we want to hear or see

251
00:11:52,240 --> 00:11:55,440
and not only looking at what
shows up in our news feeds

252
00:11:55,440 --> 00:11:57,940
but also looking at the
full complete picture

253
00:11:57,940 --> 00:11:59,440
of the facts and the evidence,

254
00:12:01,300 --> 00:12:02,390
(mouse clicking)

255
00:12:02,390 --> 00:12:06,870
Unconscious or implicit bias
are where social stereotypes

256
00:12:06,870 --> 00:12:10,420
about certain groups of
people that we form outside

257
00:12:10,420 --> 00:12:12,740
our own conscious awareness.

258
00:12:12,740 --> 00:12:16,040
So just as you see this
picture here on the left,

259
00:12:16,040 --> 00:12:17,459
that is similar to our mind

260
00:12:17,460 --> 00:12:19,630
and our mind is like an iceberg

261
00:12:19,630 --> 00:12:22,720
where our conscious mind is
what we can recall quickly

262
00:12:22,720 --> 00:12:24,100
and what we are aware of.

263
00:12:24,100 --> 00:12:26,340
So that would be similar to the iceberg

264
00:12:26,340 --> 00:12:27,890
that's sticking out of the water

265
00:12:27,890 --> 00:12:30,460
of what you can see and what you know.

266
00:12:30,460 --> 00:12:32,990
But just like with an iceberg, only 10%

267
00:12:32,990 --> 00:12:36,050
sticks out of the water and
the bulk of it is underneath.

268
00:12:36,050 --> 00:12:38,180
That's very similar to our brains

269
00:12:38,180 --> 00:12:41,930
in that the subconscious mind
is what's slightly underneath

270
00:12:41,930 --> 00:12:44,500
the water speaking with
our conscious mind.

271
00:12:44,500 --> 00:12:48,090
And that our subconscious
mind stores our beliefs,

272
00:12:48,090 --> 00:12:51,700
our previous experiences,
our memories, et cetera.

273
00:12:51,700 --> 00:12:54,610
And so when you have an
idea or maybe a memory

274
00:12:54,610 --> 00:12:58,040
from the past, that's coming
from the subconscious area

275
00:12:58,040 --> 00:13:01,839
of the brain that our
conscious mind asked for.

276
00:13:01,840 --> 00:13:04,740
But our unconscious mind
is deep inside the brain

277
00:13:04,740 --> 00:13:08,800
so just similar to an iceberg
it's a very deep down inside

278
00:13:08,800 --> 00:13:10,699
the, of the ocean.

279
00:13:10,700 --> 00:13:13,560
Everyone holds unconscious
beliefs about various

280
00:13:13,560 --> 00:13:16,449
social groups, activities, or things.

281
00:13:16,450 --> 00:13:19,230
And these biases stem
from our upbringings,

282
00:13:19,230 --> 00:13:22,590
they stem from the
influences that we've had

283
00:13:22,590 --> 00:13:24,230
in our lifetime.

284
00:13:24,230 --> 00:13:28,530
And so therefore we had the
tendency to organize our world

285
00:13:28,530 --> 00:13:31,720
by categorizing them quickly.

286
00:13:31,720 --> 00:13:36,040
And it allows us to often
think in just a very fast way

287
00:13:36,040 --> 00:13:37,750
and to make quick judgements.

288
00:13:37,750 --> 00:13:41,560
And this is true for all of
us as humans around the world.

289
00:13:41,560 --> 00:13:44,699
Unfortunately, unconscious
bias is often associated

290
00:13:44,700 --> 00:13:48,220
in a negative way, but it
could also be positive.

291
00:13:48,220 --> 00:13:51,940
And just with availability
bias or confirmation bias

292
00:13:51,940 --> 00:13:55,964
we see unconscious bias
arise in cyber as well.

293
00:13:55,964 --> 00:13:56,797
(mouse clicking)

294
00:13:56,797 --> 00:14:00,330
In decision-making, I've
seen executive blindly trust

295
00:14:00,330 --> 00:14:04,320
the IT team because they're
perceived as being the experts.

296
00:14:04,320 --> 00:14:07,720
And we all know that there
has been a cyber security

297
00:14:07,720 --> 00:14:10,290
skills shortage over the years.

298
00:14:10,290 --> 00:14:13,810
No disrespect to my IT or InfoSec friends.

299
00:14:13,810 --> 00:14:16,650
However, I recommend reviewing the data

300
00:14:16,650 --> 00:14:20,350
and asking for a second
opinion before blindly trusting

301
00:14:20,350 --> 00:14:23,680
or jumping on the bandwagon
to agree or disagree

302
00:14:23,680 --> 00:14:25,140
with something.

303
00:14:25,140 --> 00:14:27,340
Often it's easy to go with the majority

304
00:14:27,340 --> 00:14:29,240
to not rock the boat.

305
00:14:29,240 --> 00:14:32,590
But if you feel that
something needs to be changed

306
00:14:32,590 --> 00:14:34,970
or needs to be addressed differently,

307
00:14:34,970 --> 00:14:37,970
don't be afraid to go against the flow.

308
00:14:37,970 --> 00:14:40,687
Mark Twain is, has been quoted as saying,

309
00:14:40,687 --> 00:14:44,480
"Whenever you find yourself
on the side of the majority,

310
00:14:44,480 --> 00:14:47,130
it's time to pause and reflect."

311
00:14:47,130 --> 00:14:49,030
And that's something to think about is,

312
00:14:49,030 --> 00:14:50,720
is the majority always right?

313
00:14:50,720 --> 00:14:52,660
Sometimes it is and sometimes it isn't,

314
00:14:52,660 --> 00:14:54,760
but it's always good to ask that question.

315
00:14:55,740 --> 00:14:58,440
With respect to risk assessment,

316
00:14:58,440 --> 00:15:01,550
some people feel that older
workers are more of a risk

317
00:15:01,550 --> 00:15:03,490
to a company than younger workers.

318
00:15:03,490 --> 00:15:05,460
Because older workers aren't up to date

319
00:15:05,460 --> 00:15:07,460
with newer technologies.

320
00:15:07,460 --> 00:15:10,440
But some people feel that
younger people are more of a risk

321
00:15:10,440 --> 00:15:13,110
because they engage in risky behaviors

322
00:15:13,110 --> 00:15:15,410
like sharing passwords with friends

323
00:15:15,410 --> 00:15:18,770
or sharing the same passwords
across applications.

324
00:15:18,770 --> 00:15:21,319
And as a result, security
analysts may focus

325
00:15:21,320 --> 00:15:24,900
on the wrong area as the
source of a security risk

326
00:15:24,900 --> 00:15:27,240
or issue based on their biases.

327
00:15:27,240 --> 00:15:30,193
And not actually again,
looking at the data at hand.

328
00:15:31,610 --> 00:15:33,490
For incident response.

329
00:15:33,490 --> 00:15:36,750
If there was an incident,
how would you respond?

330
00:15:36,750 --> 00:15:39,770
Would you blame the
unsecure IT environment

331
00:15:39,770 --> 00:15:41,250
that you feel you have?

332
00:15:41,250 --> 00:15:44,310
Or would you feel that, that
the incompetent end users

333
00:15:44,310 --> 00:15:45,930
were the problem?

334
00:15:45,930 --> 00:15:48,689
Or would you look at the
facts and the evidence

335
00:15:48,690 --> 00:15:50,840
inside and outside of your beliefs

336
00:15:50,840 --> 00:15:53,063
to determine what exactly happened?

337
00:15:53,910 --> 00:15:57,360
How would you and your team
respond to the incident?

338
00:15:57,360 --> 00:15:59,970
If your security operations
team felt that IT

339
00:15:59,970 --> 00:16:02,600
hadn't done their part
prior to the incident,

340
00:16:02,600 --> 00:16:04,950
you may actually be
looking in the wrong area

341
00:16:04,950 --> 00:16:07,653
for the source of the, of
the incident that happened.

342
00:16:08,580 --> 00:16:12,600
Many of us have heard of the
acronym called PEBKAC before.

343
00:16:12,600 --> 00:16:14,800
And for those of you that
don't know what it means it's

344
00:16:14,800 --> 00:16:18,880
the Problem Exists Between
the Keyboard And Chair.

345
00:16:18,880 --> 00:16:21,140
And a lot of times we tend to default that

346
00:16:21,140 --> 00:16:23,760
the problem is PEBKAC but,

347
00:16:23,760 --> 00:16:25,960
or is it really lie somewhere else

348
00:16:25,960 --> 00:16:27,740
in one of our responsible areas?

349
00:16:27,740 --> 00:16:30,633
And it's not actually
the end users fault here.

350
00:16:31,900 --> 00:16:33,939
For policies and procedures.

351
00:16:33,940 --> 00:16:36,850
When was the last time
your cybersecurity policies

352
00:16:36,850 --> 00:16:38,850
and procedures were thoroughly reviewed?

353
00:16:39,800 --> 00:16:42,609
Let's say that you feel
that your SOC analyst

354
00:16:42,610 --> 00:16:44,130
is just amazing.

355
00:16:44,130 --> 00:16:46,660
And you trust everything that they say.

356
00:16:46,660 --> 00:16:49,600
Well because you're implicitly
trusting your analyst,

357
00:16:49,600 --> 00:16:52,600
you may not think to
dive into the details.

358
00:16:52,600 --> 00:16:55,510
And as a result, you could
have a firewall running

359
00:16:55,510 --> 00:16:58,290
without defined rules if
you've actually never checked

360
00:16:58,290 --> 00:16:59,122
that data.

361
00:17:00,390 --> 00:17:03,680
For IAM, the identity
and access management.

362
00:17:03,680 --> 00:17:06,909
Security analysts and software developers

363
00:17:06,910 --> 00:17:09,700
may blame users for
issues and fail to look

364
00:17:09,700 --> 00:17:11,750
at their internal infrastructure

365
00:17:11,750 --> 00:17:14,530
or maybe their own code for the problem.

366
00:17:14,530 --> 00:17:17,583
And instead be focusing
again on a wrong area.

367
00:17:18,710 --> 00:17:22,280
To overcome unconscious and implicit bias,

368
00:17:22,280 --> 00:17:25,240
be sure to look that or look and make sure

369
00:17:25,240 --> 00:17:26,680
that you're sticking with facts

370
00:17:26,680 --> 00:17:28,810
and asking all the stakeholders,

371
00:17:28,810 --> 00:17:33,010
including those you may not
like or agree with for inputs.

372
00:17:33,010 --> 00:17:35,320
It's important to get the full picture

373
00:17:35,320 --> 00:17:37,360
and sometimes people that
you may not agree with

374
00:17:37,360 --> 00:17:40,909
may actually have a good
opinion and a good possibility

375
00:17:40,910 --> 00:17:43,730
of somewhere you need
to look and investigate.

376
00:17:43,730 --> 00:17:46,480
And also be sure to look
in the mirror at yourself

377
00:17:46,480 --> 00:17:50,050
because we all carry biases
whether we like it or not.

378
00:17:50,050 --> 00:17:53,480
Did you make a mistake or are
you excusing your behavior

379
00:17:53,480 --> 00:17:55,660
instead of actually facing it?

380
00:17:55,660 --> 00:17:58,340
Don't be afraid to follow
in the words of Mark Twain

381
00:17:58,340 --> 00:18:00,580
and pause and reflect to ensure

382
00:18:00,580 --> 00:18:02,780
that you're not actually
just going with the majority

383
00:18:02,780 --> 00:18:04,870
because that's the safe option.

384
00:18:04,870 --> 00:18:06,969
Make sure you're hiring the right person

385
00:18:06,970 --> 00:18:09,950
with the right skills,
making the correct decisions

386
00:18:09,950 --> 00:18:12,390
and then addressing the
incidents in the correct way

387
00:18:12,390 --> 00:18:14,270
using the data to help guide you

388
00:18:14,270 --> 00:18:16,883
and not following on your
gut instincts always.

389
00:18:17,895 --> 00:18:19,040
(mouse clicking)

390
00:18:19,040 --> 00:18:22,220
Safety bias is another bias that we see.

391
00:18:22,220 --> 00:18:25,260
And safety bias is focusing
on the shortcomings

392
00:18:25,260 --> 00:18:26,873
so as not to take a risk.

393
00:18:27,740 --> 00:18:30,590
Many studies have shown that
we as humans would prefer

394
00:18:30,590 --> 00:18:33,062
not to lose money even though,

395
00:18:33,062 --> 00:18:36,580
even more than we prefer to gain money.

396
00:18:36,580 --> 00:18:38,600
You may have heard about experiments where

397
00:18:38,600 --> 00:18:41,010
people were offered
various amounts of money.

398
00:18:41,010 --> 00:18:42,960
And for example what if I said,

399
00:18:42,960 --> 00:18:45,900
you know I was gonna
offer you $100 dollars now

400
00:18:45,900 --> 00:18:49,700
or I'll give you $500
if you wait two years.

401
00:18:49,700 --> 00:18:51,950
Most people will take the sure thing now

402
00:18:51,950 --> 00:18:55,430
rather than waiting to
get more money later.

403
00:18:55,430 --> 00:18:58,320
But what we've seen in
studies is this changes

404
00:18:58,320 --> 00:19:01,270
when people are faced
with a loss decision.

405
00:19:01,270 --> 00:19:06,270
For instance, if I said you
would definitely lose $100 now

406
00:19:06,360 --> 00:19:11,350
or you could have a 50%
chance of losing only $1,000

407
00:19:11,350 --> 00:19:14,699
most would say they'll take
the option of losing the risk,

408
00:19:14,700 --> 00:19:16,840
actually of losing that $1,000

409
00:19:16,840 --> 00:19:18,929
instead of going for the sure thing.

410
00:19:18,930 --> 00:19:22,720
And so what we see is
safety biases slows us down

411
00:19:22,720 --> 00:19:24,830
in progress of decision-making

412
00:19:24,830 --> 00:19:28,840
and hold us back from
healthy forms of risk-taking.

413
00:19:28,840 --> 00:19:33,370
And so, where we see this
insecurity is around our DevOps.

414
00:19:33,370 --> 00:19:36,629
Is your DevOps team applying
traditional network controls

415
00:19:36,630 --> 00:19:38,680
to your cloud activities?

416
00:19:38,680 --> 00:19:40,500
Or are they looking at how

417
00:19:40,500 --> 00:19:43,260
they can refactor your
applications for the cloud

418
00:19:43,260 --> 00:19:45,923
to take your organization
to the next level?

419
00:19:46,780 --> 00:19:48,750
For your risk assessment.

420
00:19:48,750 --> 00:19:51,607
When was the last time you
reviewed your security products

421
00:19:51,607 --> 00:19:53,550
and their capabilities?

422
00:19:53,550 --> 00:19:55,960
Are you keeping with what you
have because you've already

423
00:19:55,960 --> 00:19:59,750
purchased those solutions and
they're working just fine?

424
00:19:59,750 --> 00:20:02,140
For example, do you know
if your current solutions

425
00:20:02,140 --> 00:20:05,100
have a vulnerability
scanner that can identify

426
00:20:05,100 --> 00:20:06,620
advanced vulnerabilities?

427
00:20:06,620 --> 00:20:08,100
Have you ever checked?

428
00:20:08,100 --> 00:20:10,110
Because things are working
just fine you may never

429
00:20:10,110 --> 00:20:11,330
think to check.

430
00:20:11,330 --> 00:20:13,810
And because it's not
talked about in the news,

431
00:20:13,810 --> 00:20:16,260
you just keep going with
what you currently have.

432
00:20:17,190 --> 00:20:19,790
For your policies and procedures.

433
00:20:19,790 --> 00:20:24,080
Remember my 100 and $500
example, that same thinking

434
00:20:24,080 --> 00:20:28,060
causes companies to invest in
solutions that may be overkill

435
00:20:28,060 --> 00:20:32,320
to address their specific high
impact and low probability

436
00:20:32,320 --> 00:20:33,639
risk factors.

437
00:20:33,640 --> 00:20:36,990
So they may be having
way, way extra products

438
00:20:36,990 --> 00:20:40,000
that they don't actually need
because they're pre-planning

439
00:20:40,000 --> 00:20:42,670
to prepare for that next
shark attack that you know,

440
00:20:42,670 --> 00:20:46,023
has a low probability of,
of actually happening.

441
00:20:47,000 --> 00:20:49,000
For decision making.

442
00:20:49,000 --> 00:20:52,060
When you have a lack of
information or ambiguity,

443
00:20:52,060 --> 00:20:54,490
you can see system owners
are being reluctant

444
00:20:54,490 --> 00:20:56,580
to upgrade their systems.

445
00:20:56,580 --> 00:20:59,360
They don't necessarily want
to apply the latest patches.

446
00:20:59,360 --> 00:21:02,879
They may have an unwillingness
of end users to configure

447
00:21:02,880 --> 00:21:04,210
safety features.

448
00:21:04,210 --> 00:21:06,590
And there could be a lack
of interest from developers

449
00:21:06,590 --> 00:21:11,399
to add in new security features
to existing applications.

450
00:21:11,400 --> 00:21:14,760
And as a result system owners
err on the side of caution

451
00:21:14,760 --> 00:21:17,330
because they don't wanna
break anything since again,

452
00:21:17,330 --> 00:21:19,040
they don't see a bigger impact.

453
00:21:19,040 --> 00:21:20,800
You know, we haven't
had those problems yet

454
00:21:20,800 --> 00:21:23,570
so why do I need to put
those latest patches?

455
00:21:23,570 --> 00:21:26,230
Or are they error on the siding of costs

456
00:21:26,230 --> 00:21:28,570
and saying you know what,
we can't afford to add in

457
00:21:28,570 --> 00:21:31,320
those new, new upgrades or new products.

458
00:21:31,320 --> 00:21:33,360
And so again, they just don't do anything

459
00:21:33,360 --> 00:21:34,560
and take the safe route.

460
00:21:35,700 --> 00:21:37,990
For identity and access management.

461
00:21:37,990 --> 00:21:41,310
As you move from your on-prem
to your cloud solutions,

462
00:21:41,310 --> 00:21:43,620
have you considered what
software applications

463
00:21:43,620 --> 00:21:47,399
need to be refactored
to run as cloud native?

464
00:21:47,400 --> 00:21:51,070
What new identity analytics
solutions need to be put

465
00:21:51,070 --> 00:21:53,530
in place so you're
preparing for the future?

466
00:21:53,530 --> 00:21:55,290
Have you thought about
all of those or again,

467
00:21:55,290 --> 00:21:57,840
are you just kind of going
with what you have today

468
00:21:57,840 --> 00:22:00,199
because it's the safe route?

469
00:22:00,200 --> 00:22:04,550
Some of our social scientists
have also called the ostrich,

470
00:22:04,550 --> 00:22:07,889
ostrich effect here with safety bias.

471
00:22:07,890 --> 00:22:11,470
And if we think of it, you know
the ostrich effect is based

472
00:22:11,470 --> 00:22:14,581
on this myth that ostriches
bury their heads in the sand

473
00:22:14,581 --> 00:22:16,780
when they sense danger.

474
00:22:16,780 --> 00:22:19,340
And so is your team burying
their head in the sand

475
00:22:19,340 --> 00:22:21,360
when they need to make a risky decision

476
00:22:21,360 --> 00:22:23,120
because they don't wanna take that risk?

477
00:22:23,120 --> 00:22:26,340
So instead they just keep
going the way it's been done

478
00:22:26,340 --> 00:22:30,020
for years and years because
again, it's the safe route and

479
00:22:30,020 --> 00:22:32,120
you haven't had a breach yet.

480
00:22:32,120 --> 00:22:34,709
So to overcome safety bias,

481
00:22:34,710 --> 00:22:36,780
one of the things that I do recommend is,

482
00:22:36,780 --> 00:22:39,200
is to get some distance
between you and the decision

483
00:22:39,200 --> 00:22:40,220
being made.

484
00:22:40,220 --> 00:22:43,820
So for example, imagine a
past self already having made

485
00:22:43,820 --> 00:22:47,409
the choice successfully
to you know, make a change

486
00:22:47,410 --> 00:22:49,760
that there won't be a
loss in that environment.

487
00:22:50,680 --> 00:22:53,712
Another idea is if you feel
that there is something

488
00:22:53,712 --> 00:22:57,690
happening in your environment
related to safety bias

489
00:22:57,690 --> 00:23:00,720
is that you can also balance
out your team members

490
00:23:00,720 --> 00:23:03,020
with more risk taking team members.

491
00:23:03,020 --> 00:23:05,379
Those who are more comfortable
with taking that risk

492
00:23:05,380 --> 00:23:08,190
or even bring in a third
party that can help you

493
00:23:08,190 --> 00:23:10,220
provide that unbiased opinion

494
00:23:10,220 --> 00:23:12,840
and take a risk for you
and look at and help you

495
00:23:12,840 --> 00:23:14,253
move to that next level.

496
00:23:15,629 --> 00:23:16,870
(mouse clicking)

497
00:23:16,870 --> 00:23:19,330
There's many, many
biases that are out there

498
00:23:19,330 --> 00:23:21,980
and I know that we've spent
time going through some.

499
00:23:21,980 --> 00:23:24,910
And I just wanna touch
briefly on a few others

500
00:23:24,910 --> 00:23:27,380
that could arise in your environment.

501
00:23:27,380 --> 00:23:30,240
And remember like none of
these are mutually exclusive.

502
00:23:30,240 --> 00:23:33,670
They all definitely overlap and,

503
00:23:33,670 --> 00:23:35,883
and relate to each other in many ways.

504
00:23:36,760 --> 00:23:39,370
So if we look at the, the framing effect.

505
00:23:39,370 --> 00:23:43,939
The framing effect actually
is very similar to safety bias

506
00:23:43,940 --> 00:23:45,050
in a lot of ways.

507
00:23:45,050 --> 00:23:48,870
And because the framing effect
is how something is framed

508
00:23:48,870 --> 00:23:51,040
and how it's positioned to you.

509
00:23:51,040 --> 00:23:54,670
So for example, if something
is worded in a negative way

510
00:23:54,670 --> 00:23:57,210
to emphasize the potential for loss,

511
00:23:57,210 --> 00:23:59,990
then you may be afraid to take a risk.

512
00:23:59,990 --> 00:24:03,810
So you may have seen commercials
for services that say

513
00:24:03,810 --> 00:24:06,030
one in five companies lost their data

514
00:24:06,030 --> 00:24:08,090
while using another service.

515
00:24:08,090 --> 00:24:10,649
But instead of focusing that
there was four companies

516
00:24:10,650 --> 00:24:12,270
that didn't lose their data,

517
00:24:12,270 --> 00:24:15,510
they focused on the one
that did lose their data.

518
00:24:15,510 --> 00:24:17,940
So that way you would think
about them to protect you

519
00:24:17,940 --> 00:24:19,600
instead of another company.

520
00:24:19,600 --> 00:24:22,510
But if they positioned
it as, four companies

521
00:24:22,510 --> 00:24:24,830
didn't lose their data
with that other service

522
00:24:24,830 --> 00:24:27,409
that's not going to sell or sound,

523
00:24:27,410 --> 00:24:29,470
make their company sound in great light

524
00:24:29,470 --> 00:24:32,570
and then you might not be
as reluctant to use them.

525
00:24:32,570 --> 00:24:35,889
Another example is let's say
that you needed an operation

526
00:24:35,890 --> 00:24:38,327
and a doctor came in and told you that,

527
00:24:38,327 --> 00:24:41,490
"You know you have an
80% chance of recovery."

528
00:24:41,490 --> 00:24:43,367
But what if the doctor came in and said,

529
00:24:43,367 --> 00:24:46,450
"You actually only have
a 20% chance of death

530
00:24:46,450 --> 00:24:48,000
if you have this operation."

531
00:24:48,000 --> 00:24:50,740
It's still the same
operation, it's the same data

532
00:24:50,740 --> 00:24:53,190
but it's all about how
it was framed to you.

533
00:24:53,190 --> 00:24:56,470
80% chance of recovery
or 20% chance of death.

534
00:24:56,470 --> 00:24:58,510
They still equal a hundred percent.

535
00:24:58,510 --> 00:25:01,129
But would you think differently
about how you approach

536
00:25:01,130 --> 00:25:03,817
the operation if that
doctor came in and said,

537
00:25:03,817 --> 00:25:06,690
"You only had a 20% chance of death?"

538
00:25:06,690 --> 00:25:09,240
Pay attention to how
statements are phrased

539
00:25:09,240 --> 00:25:12,740
and that way you can
overcome any kind of instinct

540
00:25:12,740 --> 00:25:14,803
or reactions in making that decision.

541
00:25:16,160 --> 00:25:18,190
Affinity bias.

542
00:25:18,190 --> 00:25:21,220
Affinity bias is gravitating
towards what we know

543
00:25:21,220 --> 00:25:22,890
or what we are comfortable with.

544
00:25:22,890 --> 00:25:26,760
So an example of this is where outside of,

545
00:25:26,760 --> 00:25:30,730
of work or environment, is
let's say that you see somebody,

546
00:25:30,730 --> 00:25:33,360
you're out and about maybe
when you travel or something

547
00:25:33,360 --> 00:25:35,030
and you're far away from home,

548
00:25:35,030 --> 00:25:38,560
and you see somebody with your
college alma mater shirt on

549
00:25:38,560 --> 00:25:40,070
and you're in another city.

550
00:25:40,070 --> 00:25:42,040
You instantly feel a connection to them

551
00:25:42,040 --> 00:25:44,760
even though you may have
never met this person.

552
00:25:44,760 --> 00:25:47,879
And what this does, is it
creates an in-group bias.

553
00:25:47,880 --> 00:25:52,000
Because instantly you know,
hey, we went to the same school,

554
00:25:52,000 --> 00:25:54,380
we experienced a lot of the same things.

555
00:25:54,380 --> 00:25:58,060
And so what happens in, in
cyber related to this is

556
00:25:58,060 --> 00:26:00,560
it could be around new product offerings.

557
00:26:00,560 --> 00:26:02,540
Are you still using those same solutions

558
00:26:02,540 --> 00:26:04,409
that you've been using
for the last 20 years

559
00:26:04,410 --> 00:26:07,080
because they're familiar to
you and they're comfortable

560
00:26:07,080 --> 00:26:09,929
to you and you like maybe your sales rep?

561
00:26:09,930 --> 00:26:13,420
Or are you using the latest
solutions that are new to you

562
00:26:13,420 --> 00:26:15,110
and are you taking that risk?

563
00:26:15,110 --> 00:26:17,592
But, again it may not
feel comfortable to you.

564
00:26:18,550 --> 00:26:21,200
You also may see this in cyber in that,

565
00:26:21,200 --> 00:26:23,480
you may feel that your direct team alone

566
00:26:23,480 --> 00:26:25,340
has all of the right answers

567
00:26:25,340 --> 00:26:28,030
and no one else knows how to
secure your environment better.

568
00:26:28,030 --> 00:26:31,070
So you don't need to get
a third party perspective

569
00:26:31,070 --> 00:26:33,790
or opinion because nobody
knows your environment better

570
00:26:33,790 --> 00:26:35,240
than your team does.

571
00:26:35,240 --> 00:26:37,890
But again, that's where
your affinity start to come

572
00:26:37,890 --> 00:26:40,780
into play and I would
challenge you to look at

573
00:26:40,780 --> 00:26:42,870
your environment and think
about where that might be

574
00:26:42,870 --> 00:26:43,703
popping up.

575
00:26:44,780 --> 00:26:46,470
Similarity bias.

576
00:26:46,470 --> 00:26:51,470
Similarity bias occurs, is
when we have or us as humans,

577
00:26:51,880 --> 00:26:54,280
we're highly motivated to see ourselves

578
00:26:54,280 --> 00:26:57,560
and those who are similar
to us in a favorable light.

579
00:26:57,560 --> 00:27:00,970
So similar to the affinity
bias, we create those in-groups

580
00:27:00,970 --> 00:27:02,350
and those out-groups.

581
00:27:02,350 --> 00:27:04,870
So those who went to
the same college as us,

582
00:27:04,870 --> 00:27:08,260
those that live in the same
city, same state, same country,

583
00:27:08,260 --> 00:27:11,550
et cetera, may you know,
we create those similarly,

584
00:27:11,550 --> 00:27:13,700
similarity biases with them.

585
00:27:13,700 --> 00:27:16,100
And we may treat them
differently or we may,

586
00:27:16,100 --> 00:27:19,169
may trust their opinion or
distrust their opinions.

587
00:27:19,170 --> 00:27:21,860
And so as you're looking
at the people on your team,

588
00:27:21,860 --> 00:27:24,750
do you know what do they look
like in terms of similarities?

589
00:27:24,750 --> 00:27:27,760
Are they similar to what you
have in terms of thinking?

590
00:27:27,760 --> 00:27:29,980
And what kind of diversity do you have?

591
00:27:29,980 --> 00:27:33,120
Or are you looking for the
skills and the individuals

592
00:27:33,120 --> 00:27:36,040
and the products in areas
where you need to be

593
00:27:36,040 --> 00:27:37,450
in one to two years?

594
00:27:37,450 --> 00:27:39,250
Or are you still looking at what you have?

595
00:27:39,250 --> 00:27:42,060
Because again, it's
similar, it's comfortable

596
00:27:42,060 --> 00:27:44,523
and it feels right for
you because it's safe.

597
00:27:45,670 --> 00:27:47,980
As we look to a loss aversion.

598
00:27:47,980 --> 00:27:51,470
This again is related to a sunk cost.

599
00:27:51,470 --> 00:27:54,260
And this is observed where
companies have already invested

600
00:27:54,260 --> 00:27:56,290
in their traditional IT infrastructure,

601
00:27:56,290 --> 00:27:58,290
so why move to the cloud?

602
00:27:58,290 --> 00:27:59,210
This takes time.

603
00:27:59,210 --> 00:28:00,720
It takes resources.

604
00:28:00,720 --> 00:28:03,110
So instead of doing that,
companies will keep buying

605
00:28:03,110 --> 00:28:06,060
the same new servers over
and over and spending

606
00:28:06,060 --> 00:28:09,310
a lot of money to keep those
on-prem solutions running.

607
00:28:09,310 --> 00:28:11,956
Instead of being able to look and say,

608
00:28:11,957 --> 00:28:14,630
"Hey, maybe we need to cut
our losses and move to a new,

609
00:28:14,630 --> 00:28:15,707
new solution."

610
00:28:16,910 --> 00:28:18,200
And distance bias.

611
00:28:18,200 --> 00:28:21,030
This is prioritizing what
is nearby whether it's in

612
00:28:21,030 --> 00:28:24,230
physical space, time or other domains.

613
00:28:24,230 --> 00:28:27,090
And so prior to the pandemic
when we were all working

614
00:28:27,090 --> 00:28:29,300
in conference rooms and in our office

615
00:28:29,300 --> 00:28:31,370
and we had conversations together,

616
00:28:31,370 --> 00:28:33,649
how many times did you see
that people in the room

617
00:28:33,650 --> 00:28:36,270
failed to gather inputs
from their remote colleagues

618
00:28:36,270 --> 00:28:37,620
on the phone?

619
00:28:37,620 --> 00:28:40,739
Or did you make a decision
based on what you needed to do

620
00:28:40,740 --> 00:28:43,880
right now instead of considering
those long-term effects

621
00:28:43,880 --> 00:28:45,930
on what was best for the company?

622
00:28:45,930 --> 00:28:47,920
Did you say, "Well you know
what, we don't have the money

623
00:28:47,920 --> 00:28:50,850
this year, so we're gonna,
gonna shut this particular

624
00:28:50,850 --> 00:28:52,000
service down?"

625
00:28:52,000 --> 00:28:55,260
Instead of understanding what
those long-term impacts are

626
00:28:55,260 --> 00:28:58,023
to the company and doing
their full analysis.

627
00:29:00,058 --> 00:29:01,010
(mouse clicking)

628
00:29:01,010 --> 00:29:03,610
So tips on, on removing bias.

629
00:29:03,610 --> 00:29:06,760
And as you saw in each of the
biases that I highlighted,

630
00:29:06,760 --> 00:29:08,780
they're not mutually exclusive.

631
00:29:08,780 --> 00:29:11,700
There's many overlaps between
all of the various types

632
00:29:11,700 --> 00:29:13,480
of cognitive biases.

633
00:29:13,480 --> 00:29:15,620
And I wanna give you some
more tips on how to remove

634
00:29:15,620 --> 00:29:17,239
those biases.

635
00:29:17,240 --> 00:29:19,240
The first one is acknowledge.

636
00:29:19,240 --> 00:29:22,260
Acknowledge that the biases exist.

637
00:29:22,260 --> 00:29:25,570
Security is not just one
product as you all know,

638
00:29:25,570 --> 00:29:29,460
it's a culmination of
products, processes, technology

639
00:29:29,460 --> 00:29:32,300
and all of that depends on human behavior.

640
00:29:32,300 --> 00:29:35,960
And unfortunately, human
behavior lends itself to biases.

641
00:29:35,960 --> 00:29:37,593
That's just a fact of life.

642
00:29:38,980 --> 00:29:42,550
The second area is seek
and review all of your data

643
00:29:42,550 --> 00:29:45,320
objectively before making a decision.

644
00:29:45,320 --> 00:29:48,970
Don't base that decision on
a split second gut feeling.

645
00:29:48,970 --> 00:29:51,500
Don't base it on what was done previously,

646
00:29:51,500 --> 00:29:54,070
on the opinion of an expert.

647
00:29:54,070 --> 00:29:56,970
Consider how all those
options were framed.

648
00:29:56,970 --> 00:29:58,760
When looking at that, that's gonna address

649
00:29:58,760 --> 00:30:01,470
your availability, your confirmation bias,

650
00:30:01,470 --> 00:30:03,173
as well as that framing effect.

651
00:30:04,470 --> 00:30:07,447
The third area, include
everyone that needs to be

652
00:30:07,447 --> 00:30:11,489
an inputter on a decision or
even how to handle an incident,

653
00:30:11,490 --> 00:30:14,090
including those who you may not like.

654
00:30:14,090 --> 00:30:16,510
This is gonna address
your confirmation bias

655
00:30:16,510 --> 00:30:19,003
as well as your unconscious bias.

656
00:30:20,600 --> 00:30:24,260
Your fourth area is to
utilize third-party companies

657
00:30:24,260 --> 00:30:27,160
to help evaluate in an unbiased way.

658
00:30:27,160 --> 00:30:30,140
By having a third party company come in,

659
00:30:30,140 --> 00:30:32,270
they can look at your situation.

660
00:30:32,270 --> 00:30:33,690
They can look at your posture.

661
00:30:33,690 --> 00:30:34,920
They can do your pen testing.

662
00:30:34,920 --> 00:30:37,230
They can look at things that you know,

663
00:30:37,230 --> 00:30:40,150
aren't necessarily things that
you may have thought about.

664
00:30:40,150 --> 00:30:42,800
And then you can make the
decision based on the data.

665
00:30:43,760 --> 00:30:44,640
(mouse clicking)

666
00:30:44,640 --> 00:30:47,100
The fifth way, is really
looking to the future

667
00:30:47,100 --> 00:30:49,330
without attachment to the past.

668
00:30:49,330 --> 00:30:52,590
So this addresses your safety
bias, your loss aversion,

669
00:30:52,590 --> 00:30:55,230
affinity and similarity bias.

670
00:30:55,230 --> 00:30:58,280
So ensuring that you're
also using monitoring tools

671
00:30:58,280 --> 00:31:01,820
that have capabilities to
understand human weaknesses

672
00:31:01,820 --> 00:31:03,639
and provide those proper analysis

673
00:31:03,640 --> 00:31:05,110
based on the user behavior.

674
00:31:05,110 --> 00:31:08,810
So look for things like UBA
or user behavior analytics

675
00:31:08,810 --> 00:31:10,118
to help with that.

676
00:31:10,118 --> 00:31:11,090
(mouse clicking)

677
00:31:11,090 --> 00:31:15,070
And the last tip is, don't
group human behaviors together.

678
00:31:15,070 --> 00:31:18,530
Instead, really look more
at individual behaviors

679
00:31:18,530 --> 00:31:20,110
including your own.

680
00:31:20,110 --> 00:31:23,959
Educate your employees that
cyber issues are often due to

681
00:31:23,960 --> 00:31:25,610
cognitive biases.

682
00:31:25,610 --> 00:31:28,620
And attackers target this in combination

683
00:31:28,620 --> 00:31:31,610
with technical flaws to exploit systems.

684
00:31:31,610 --> 00:31:34,740
We often hear that fishing
is still one of the main ways

685
00:31:34,740 --> 00:31:37,610
that attackers get into an environment.

686
00:31:37,610 --> 00:31:39,770
And that's true because
they play on our weaknesses

687
00:31:39,770 --> 00:31:40,650
as humans.

688
00:31:40,650 --> 00:31:43,630
They play on our biases and
they get us to click on those

689
00:31:43,630 --> 00:31:45,780
links which start to download
things that we definitely,

690
00:31:45,780 --> 00:31:47,830
definitely don't want in our environment.

691
00:31:49,065 --> 00:31:49,898
(mouse clicking)

692
00:31:49,898 --> 00:31:51,622
So applying what you have learned.

693
00:31:52,910 --> 00:31:57,060
Next week, acknowledge that
those possibility of biases

694
00:31:57,060 --> 00:31:59,760
and flaws in your environment exist.

695
00:31:59,760 --> 00:32:03,110
Examine where you may have
biases influencing your secure,

696
00:32:03,110 --> 00:32:05,000
cyber security posture.

697
00:32:05,000 --> 00:32:08,420
This is going to require
personal insight and empathy.

698
00:32:08,420 --> 00:32:10,210
It's not gonna be easy,

699
00:32:10,210 --> 00:32:12,080
because again it takes a lot to admit

700
00:32:12,080 --> 00:32:14,820
that we all have those biases
and that they all are there

701
00:32:14,820 --> 00:32:16,919
and we may have made the wrong decision.

702
00:32:16,920 --> 00:32:18,380
But it's better to make the wrong,

703
00:32:18,380 --> 00:32:21,730
correct that wrong decision
than find your company name

704
00:32:21,730 --> 00:32:24,970
ending up in the newspaper
because of a cyber security

705
00:32:24,970 --> 00:32:26,970
incident, we definitely don't want that.

706
00:32:27,828 --> 00:32:28,661
(mouse clicking)

707
00:32:28,661 --> 00:32:31,840
Over the next three months,
you want to educate others

708
00:32:31,840 --> 00:32:34,169
where and how biases could be impacting

709
00:32:34,170 --> 00:32:36,410
that cybersecurity posture.

710
00:32:36,410 --> 00:32:38,860
There is a blog on this topic,

711
00:32:38,860 --> 00:32:41,729
out on our mcafee.com blog site.

712
00:32:41,730 --> 00:32:44,210
Just search for the title
and you'll see it out there.

713
00:32:44,210 --> 00:32:47,330
And you can share that with
your colleagues to educate them

714
00:32:47,330 --> 00:32:50,330
on all the topics that
we've touched on today.

715
00:32:50,330 --> 00:32:54,560
You also be sure to review
your cyber security posture

716
00:32:54,560 --> 00:32:56,500
and adjust it as necessary.

717
00:32:56,500 --> 00:32:57,980
Things change,

718
00:32:57,980 --> 00:33:00,110
techniques change, attackers change,

719
00:33:00,110 --> 00:33:01,959
our environments have changed.

720
00:33:01,960 --> 00:33:03,940
When was the last time you reviewed that?

721
00:33:03,940 --> 00:33:05,980
And be sure that you're
keeping up to date.

722
00:33:05,980 --> 00:33:08,610
It's never too late to start
if you haven't looked at it

723
00:33:08,610 --> 00:33:09,443
in a while.

724
00:33:10,560 --> 00:33:14,030
And within six months,
be sure to build habits

725
00:33:14,030 --> 00:33:16,810
across your teams to ensure
that you're consciously

726
00:33:16,810 --> 00:33:18,149
removing biases.

727
00:33:18,150 --> 00:33:20,260
This is not a one and done thing,

728
00:33:20,260 --> 00:33:22,060
because biases don't go away.

729
00:33:22,060 --> 00:33:24,220
They continue over and over.

730
00:33:24,220 --> 00:33:27,410
And as soon as you have new
team players come into play

731
00:33:27,410 --> 00:33:30,610
and new challenges and new
things that you need to consider

732
00:33:30,610 --> 00:33:33,320
in your environment, you
always want to rebuild

733
00:33:33,320 --> 00:33:35,629
those habits and keep revisiting them.

734
00:33:35,630 --> 00:33:38,100
Because you wanna make sure
that your cyber security posture

735
00:33:38,100 --> 00:33:39,648
is the strongest that can be.

736
00:33:39,648 --> 00:33:40,550
(mouse clicking)

737
00:33:40,550 --> 00:33:44,810
And so in summary, awareness
is just one of the cognitive,

738
00:33:44,810 --> 00:33:47,870
sorry, awareness of your cognitive biases

739
00:33:47,870 --> 00:33:50,239
is the first step of it.

740
00:33:50,240 --> 00:33:53,850
And being sure that your
teams know what's going on

741
00:33:53,850 --> 00:33:55,260
and know what is out there

742
00:33:55,260 --> 00:33:57,580
and making sure that you're not at risk.

743
00:33:57,580 --> 00:33:59,990
Our adversaries are well aware that humans

744
00:33:59,990 --> 00:34:03,160
have these weaknesses and
will try to exploit them.

745
00:34:03,160 --> 00:34:07,310
So what can you do to remove
biases as much as possible

746
00:34:07,310 --> 00:34:11,639
inside your organization and
improve across all levels?

747
00:34:11,639 --> 00:34:14,560
We definitely don't want
to see your company name

748
00:34:14,560 --> 00:34:15,460
in the news.

749
00:34:15,460 --> 00:34:17,270
We definitely want to help you.

750
00:34:17,270 --> 00:34:19,110
And so please take this to heart

751
00:34:19,110 --> 00:34:22,480
and how can, you can remove
biases from your environment.

752
00:34:22,480 --> 00:34:24,460
Thank you very much and I look forward to

753
00:34:24,460 --> 00:34:26,110
answering your questions in chat.

