1
00:00:01,420 --> 00:00:04,380
- Welcome to our presentation today.

2
00:00:04,380 --> 00:00:05,720
My name is Beat Beusser,

3
00:00:05,720 --> 00:00:09,500
and I'm presenting today
here with Abigail Goldsteen,

4
00:00:09,500 --> 00:00:10,960
we are both developers and maintainers

5
00:00:10,960 --> 00:00:13,660
of the Adversarial Robustness Toolbox,

6
00:00:13,660 --> 00:00:16,070
and we are working as
research staff members

7
00:00:16,070 --> 00:00:17,670
for IBM Research.

8
00:00:17,670 --> 00:00:20,860
Our talk today is titled
Evasion, Poisoning, Extraction

9
00:00:20,860 --> 00:00:24,372
and Inference: The Tools
to Defend and Evaluate.

10
00:00:26,627 --> 00:00:27,610
What we will cover today

11
00:00:27,610 --> 00:00:30,320
is an introduction to
the adversarial threats

12
00:00:30,320 --> 00:00:32,633
against AI and machine learning,

13
00:00:33,690 --> 00:00:36,470
and we will introduce
the open-source project,

14
00:00:36,470 --> 00:00:40,129
the Adversarial Robustness
Toolbox, or short, ART.

15
00:00:40,130 --> 00:00:41,200
In the second half,

16
00:00:41,200 --> 00:00:43,850
we will be presenting privacy attacks

17
00:00:43,850 --> 00:00:45,290
against machine learning models,

18
00:00:45,290 --> 00:00:48,060
including an actual
demonstration of the tools,

19
00:00:48,060 --> 00:00:48,997
there's a focus on attribute

20
00:00:48,997 --> 00:00:51,690
and membership inference attacks,

21
00:00:51,690 --> 00:00:53,080
and we will show how you

22
00:00:53,080 --> 00:00:56,309
can defend using differential privacy,

23
00:00:56,310 --> 00:01:00,623
including a demonstration of
a related open-source project.

24
00:01:02,730 --> 00:01:05,470
The emerging adversarial
threats against AI

25
00:01:05,470 --> 00:01:08,050
and machine learning can be grouped

26
00:01:08,050 --> 00:01:10,473
into four different main threats,

27
00:01:12,209 --> 00:01:16,010
which include evasion, poisoning,

28
00:01:16,010 --> 00:01:18,220
extraction and inference,

29
00:01:18,220 --> 00:01:21,300
and visualized on this simple
machine learning pipeline,

30
00:01:21,300 --> 00:01:23,090
consisting of training data

31
00:01:23,090 --> 00:01:25,700
and the machine learning
that is trained on it,

32
00:01:25,700 --> 00:01:29,550
it can be attacked by
evasion by the process

33
00:01:29,550 --> 00:01:32,960
of modifying input to
influence the models behavior,

34
00:01:32,960 --> 00:01:36,130
to, for example, misclassify an input.

35
00:01:36,130 --> 00:01:39,030
Poisoning describes the task

36
00:01:39,030 --> 00:01:41,540
of modifying the training data,

37
00:01:41,540 --> 00:01:43,490
with the goal to add, for example,

38
00:01:43,490 --> 00:01:47,919
a backdoor in the model
trained on this training data,

39
00:01:47,920 --> 00:01:49,610
which can later be exploited

40
00:01:49,610 --> 00:01:53,433
to influence the machine
learning model's behavior.

41
00:01:54,320 --> 00:01:58,429
Extraction is the process of
actually stealing a model,

42
00:01:58,430 --> 00:02:02,070
which is very important if
the model is proprietary

43
00:02:02,070 --> 00:02:03,179
and very valuable,

44
00:02:03,180 --> 00:02:05,800
but also it's often an entrance

45
00:02:05,800 --> 00:02:08,173
during the reconnaissance
phase of an attack,

46
00:02:09,020 --> 00:02:11,770
which we'll come back in later slides to,

47
00:02:11,770 --> 00:02:15,280
and very important for
today's presentation

48
00:02:15,280 --> 00:02:18,400
in the second part today
is the inference threat,

49
00:02:18,400 --> 00:02:20,530
which is the process of going

50
00:02:20,530 --> 00:02:22,833
through a trained machine learning model,

51
00:02:22,833 --> 00:02:25,380
with the goal to learn information

52
00:02:25,380 --> 00:02:28,109
about sensitive private information

53
00:02:28,110 --> 00:02:30,630
that is contained in the training data,

54
00:02:30,630 --> 00:02:33,563
solely by accessing a trained
machine learning model.

55
00:02:34,430 --> 00:02:36,460
There's a great and detailed overview

56
00:02:36,460 --> 00:02:38,340
that has been recently published,

57
00:02:38,340 --> 00:02:39,173
available on GitHub,

58
00:02:39,173 --> 00:02:41,970
and I'm showing the
link here on the bottom,

59
00:02:41,970 --> 00:02:44,980
and I would like to encourage everybody

60
00:02:44,980 --> 00:02:47,220
to take a closer look there,

61
00:02:47,220 --> 00:02:49,723
if you would like to dive
deeper into this topic.

62
00:02:50,737 --> 00:02:53,460
These are not just theoretical threats,

63
00:02:53,460 --> 00:02:55,100
these are also,

64
00:02:55,100 --> 00:02:58,530
we see an increasing number
of real-world exploits

65
00:02:58,530 --> 00:03:00,290
of these adversarial threats,

66
00:03:00,290 --> 00:03:05,290
for example, evasion attacks
against classification models

67
00:03:05,428 --> 00:03:08,320
that are used in antivirus products,

68
00:03:08,320 --> 00:03:10,700
the threat there is, of course,

69
00:03:10,700 --> 00:03:13,226
undetected ransomwares can install

70
00:03:13,227 --> 00:03:15,130
and encrypt your computers,

71
00:03:15,130 --> 00:03:18,153
and create very, very extensive damages,

72
00:03:19,588 --> 00:03:20,822
there're also physical,

73
00:03:20,822 --> 00:03:24,431
real-world adversarial
attacks using patches

74
00:03:24,431 --> 00:03:27,213
for evasion attacks on self-driving cars,

75
00:03:28,480 --> 00:03:32,630
for example, putting
stickers on driving lanes

76
00:03:32,630 --> 00:03:34,890
and traffic signs,

77
00:03:34,890 --> 00:03:36,750
with the goal to influence the behavior

78
00:03:36,750 --> 00:03:38,460
of self-driving car,

79
00:03:38,460 --> 00:03:41,340
so resulting then in losing control

80
00:03:41,340 --> 00:03:43,110
of autonomous vehicles

81
00:03:43,110 --> 00:03:46,923
and leading to grave damages
and injuries and chaos.

82
00:03:50,340 --> 00:03:53,080
The third example is, for example,

83
00:03:53,080 --> 00:03:55,280
the extraction threat,

84
00:03:55,280 --> 00:03:56,570
which has been demonstrated

85
00:03:56,570 --> 00:04:00,820
that classification models
contained in applications

86
00:04:00,820 --> 00:04:05,820
can be extracted and then
used to stage evasion attacks,

87
00:04:05,890 --> 00:04:08,559
for example, against
email protection systems,

88
00:04:08,560 --> 00:04:12,670
and this allows really
bypassing these security systems

89
00:04:12,670 --> 00:04:16,092
to increase the chances of
phishing attacks, for example.

90
00:04:18,860 --> 00:04:21,370
The fourth example that
we would like to show

91
00:04:21,370 --> 00:04:26,370
is revealing, based on
the inference threat,

92
00:04:27,760 --> 00:04:30,500
that it could reveal
that you're a person who

93
00:04:30,500 --> 00:04:32,260
is suffering from a certain disease,

94
00:04:32,260 --> 00:04:34,590
based on a membership inference attack

95
00:04:34,590 --> 00:04:37,710
on a model that is used for classification

96
00:04:37,710 --> 00:04:40,936
of a certain disease,

97
00:04:40,937 --> 00:04:43,020
and here the really big threat

98
00:04:43,020 --> 00:04:45,710
is the leaking of sensitive
private information,

99
00:04:45,710 --> 00:04:48,489
which is regulated and protected by laws

100
00:04:48,490 --> 00:04:52,830
and also by large funds,

101
00:04:52,830 --> 00:04:54,243
if such leaks occur.

102
00:04:55,910 --> 00:04:57,280
Now, as we see,

103
00:04:57,280 --> 00:05:00,969
there's an increasing number
of these real-world threats,

104
00:05:00,970 --> 00:05:03,370
there's also the combination
of these threats,

105
00:05:03,370 --> 00:05:05,510
and here we would like
to just quickly point

106
00:05:05,510 --> 00:05:08,650
on the vulnerability note
that we are showing here,

107
00:05:08,650 --> 00:05:10,510
and there'll be a
listing with three links,

108
00:05:10,510 --> 00:05:13,099
which provide much more
detailed information,

109
00:05:13,100 --> 00:05:13,991
but the reason

110
00:05:13,991 --> 00:05:17,789
why we find this vulnerability
note so interesting is

111
00:05:17,790 --> 00:05:20,010
because it's demonstrating the combination

112
00:05:20,010 --> 00:05:24,770
of two threats of extraction and evasion

113
00:05:24,770 --> 00:05:26,349
to increase the impact

114
00:05:26,350 --> 00:05:29,840
and feasibility of adversarial attacks,

115
00:05:29,840 --> 00:05:34,840
and here, attacking this application,

116
00:05:36,580 --> 00:05:38,640
an application that is leaking information

117
00:05:38,640 --> 00:05:40,099
on an internal classification

118
00:05:40,099 --> 00:05:43,460
or it's classification decisions,

119
00:05:43,460 --> 00:05:45,859
allows, in a first step,

120
00:05:45,859 --> 00:05:49,620
to create a data set of
input labels and pairs

121
00:05:49,620 --> 00:05:52,930
by querying the application repeatedly,

122
00:05:52,930 --> 00:05:55,530
and then train a surrogate
classification model

123
00:05:55,530 --> 00:05:57,666
on this new data set,

124
00:05:57,666 --> 00:05:59,150
this is kind of this first step

125
00:05:59,150 --> 00:06:01,280
of extracting the classification model,

126
00:06:01,280 --> 00:06:05,042
that is part of the attacked application.

127
00:06:05,886 --> 00:06:07,070
The second step then

128
00:06:07,070 --> 00:06:10,230
contains applying this
classification model

129
00:06:10,230 --> 00:06:14,310
and used the insight on the
applications classification

130
00:06:14,310 --> 00:06:16,950
to actually craft adversarial examples,

131
00:06:16,950 --> 00:06:19,580
and then use these adversarial examples

132
00:06:19,580 --> 00:06:21,140
to achieve the attack's goal,

133
00:06:21,140 --> 00:06:22,270
which can be, for example,

134
00:06:22,270 --> 00:06:24,693
the misclassification of spam emails,

135
00:06:28,090 --> 00:06:33,090
and here we would like, then,

136
00:06:33,800 --> 00:06:37,333
to introduce the Adversarial
Robustness Toolbox, ART.

137
00:06:37,333 --> 00:06:39,090
So ART is an open-source library

138
00:06:39,090 --> 00:06:40,710
for machine learning security,

139
00:06:40,710 --> 00:06:44,370
and it's hosted by the Linux
Foundation for AI and Data,

140
00:06:44,370 --> 00:06:48,150
and it's available on GitHub
in the Trusted-AI workspace,

141
00:06:48,150 --> 00:06:51,219
and the goal of ART is to provide tools

142
00:06:51,220 --> 00:06:53,530
to developers and researchers,

143
00:06:53,530 --> 00:06:55,207
to enable them to evaluate

144
00:06:55,207 --> 00:06:58,198
and defend machine learning
models and applications

145
00:06:58,198 --> 00:07:01,743
against these four threats
that we have just introduced.

146
00:07:03,290 --> 00:07:07,490
Now, ART started as an image
classification toolbox,

147
00:07:08,530 --> 00:07:12,770
but has since then grown and been extended

148
00:07:12,770 --> 00:07:15,180
to all possible machine learning tasks,

149
00:07:15,180 --> 00:07:16,650
including object detection

150
00:07:16,650 --> 00:07:18,840
and automated speech recognition,

151
00:07:18,840 --> 00:07:21,003
and it's still growing into other tasks.

152
00:07:22,780 --> 00:07:25,099
ART is also framework independent,

153
00:07:25,100 --> 00:07:28,080
so it support all popular frameworks,

154
00:07:28,080 --> 00:07:32,419
including TensorFlow, PyTorch,
Scikit-learn and many more,

155
00:07:32,420 --> 00:07:36,520
and as ART started with images,

156
00:07:36,520 --> 00:07:40,750
it has pretty soon then grown
into supporting tabled data,

157
00:07:40,750 --> 00:07:44,350
and recently also in
supporting audio and video data

158
00:07:44,350 --> 00:07:48,210
and even multi-modal data combinations.

159
00:07:48,210 --> 00:07:52,150
So while ART is providing attack tools,

160
00:07:52,150 --> 00:07:55,400
we see ART as a white hat tool,

161
00:07:55,400 --> 00:07:58,539
so the attacks that are provided in ART,

162
00:07:58,540 --> 00:08:01,230
we aim to make them state of the art,

163
00:08:01,230 --> 00:08:04,730
but we see them as tools

164
00:08:04,730 --> 00:08:06,744
to evaluate the machine learning models

165
00:08:06,744 --> 00:08:10,980
in red team-like approaches,

166
00:08:10,980 --> 00:08:13,440
so that we can really
evaluate the strengths

167
00:08:13,440 --> 00:08:16,880
and become aware of the
possible vulnerability

168
00:08:16,880 --> 00:08:20,520
of AI and ML based applications,

169
00:08:20,520 --> 00:08:23,870
so in that sense, we see the
tools as evaluation tools

170
00:08:23,870 --> 00:08:28,210
for poising, inference and
extraction and evasion threats,

171
00:08:28,210 --> 00:08:32,380
but ART also provides blue team tools,

172
00:08:32,380 --> 00:08:36,870
which the selection as shown here,

173
00:08:36,870 --> 00:08:40,230
which examples for these tools are,

174
00:08:40,230 --> 00:08:42,760
for example, tools for
adversarial training,

175
00:08:42,760 --> 00:08:46,180
so these are special training algorithms

176
00:08:46,180 --> 00:08:48,870
to make machine learning
models more robust

177
00:08:48,870 --> 00:08:50,440
against evasion attacks,

178
00:08:51,320 --> 00:08:53,750
ART also includes detection tools

179
00:08:53,750 --> 00:08:55,572
for poison and evasion,

180
00:08:55,572 --> 00:08:59,330
and also and more recently,

181
00:08:59,330 --> 00:09:00,480
ART has been extended

182
00:09:00,480 --> 00:09:04,180
to provide also certification
and verification tools,

183
00:09:04,180 --> 00:09:05,530
which allow it to certify,

184
00:09:05,530 --> 00:09:08,552
or to verify trained
machine learning models

185
00:09:08,552 --> 00:09:11,770
on their robustness or vulnerability

186
00:09:11,770 --> 00:09:14,593
to these four adversarial threats.

187
00:09:18,330 --> 00:09:21,510
There's also a growing
ecosystem around ART,

188
00:09:21,510 --> 00:09:25,439
and we would like to
mention three examples,

189
00:09:25,440 --> 00:09:28,140
the first one is Armory,

190
00:09:28,140 --> 00:09:31,020
it's an adversarial robustness
evaluation test bed,

191
00:09:31,020 --> 00:09:33,800
which builds on the tools of ART,

192
00:09:33,800 --> 00:09:37,349
it allows you to run
evaluations with ART locally

193
00:09:37,350 --> 00:09:40,330
or scaled in the cloud
using Docker containers,

194
00:09:40,330 --> 00:09:43,290
and it's available on GitHub

195
00:09:43,290 --> 00:09:47,150
and developed by our
colleagues at Two Six Labs,

196
00:09:47,150 --> 00:09:50,980
and we are working
closely to work together

197
00:09:50,980 --> 00:09:54,060
and bring Armory and ART closer together.

198
00:09:54,060 --> 00:09:55,109
A very interesting

199
00:09:55,110 --> 00:09:59,240
and very exciting new open-source
project is Counterfit,

200
00:09:59,240 --> 00:10:01,453
which is soon to be open-source,

201
00:10:03,172 --> 00:10:05,130
it's a command line tool

202
00:10:05,130 --> 00:10:08,260
to simplify running evaluations with ART

203
00:10:08,260 --> 00:10:10,200
in your local terminal,

204
00:10:10,200 --> 00:10:13,097
and we have seen some previews,

205
00:10:13,097 --> 00:10:15,430
and we find them very, very exciting,

206
00:10:15,430 --> 00:10:17,824
and so be on the out look on GitHub

207
00:10:17,824 --> 00:10:19,207
when it will be released soon.

208
00:10:20,050 --> 00:10:23,920
The second is ai-privacy-toolkit,

209
00:10:23,920 --> 00:10:26,683
which will possibly soon
available open-source,

210
00:10:27,896 --> 00:10:30,010
it will provide tools for privacy

211
00:10:30,010 --> 00:10:32,450
and compliance of AI models,

212
00:10:32,450 --> 00:10:35,170
and provide end-to-end privacy,

213
00:10:35,170 --> 00:10:38,173
evaluation and mitigation
of privacy risks,

214
00:10:38,173 --> 00:10:41,697
and it will be available in
the IBM workspace on GitHub.

215
00:10:44,560 --> 00:10:49,560
Now, the tools of ART can be
grouped into six sub-modules,

216
00:10:51,927 --> 00:10:53,630
and just to give you a quick overview

217
00:10:53,630 --> 00:10:55,670
of the organization of the ART library,

218
00:10:55,670 --> 00:10:58,410
I would just like to
highlight these modules

219
00:10:58,410 --> 00:11:00,040
to give you an overview,

220
00:11:00,040 --> 00:11:03,050
so the most important one
is the attacks module,

221
00:11:03,050 --> 00:11:08,050
which contains all the
black-box and in-between attacks

222
00:11:09,630 --> 00:11:11,663
and also physical attacks with patches,

223
00:11:12,570 --> 00:11:13,930
then we have an estimate,

224
00:11:13,930 --> 00:11:15,213
ART as an estimators module,

225
00:11:15,213 --> 00:11:17,420
which contains the abstractions

226
00:11:17,420 --> 00:11:18,770
of your machine learning model

227
00:11:18,770 --> 00:11:20,550
that you would like to evaluate,

228
00:11:20,550 --> 00:11:24,680
and make it compatible with
the attacks and the defenses,

229
00:11:24,680 --> 00:11:26,089
there's an evaluations module

230
00:11:26,090 --> 00:11:28,310
which contains higher level tools

231
00:11:28,310 --> 00:11:29,959
for evaluation of robustness

232
00:11:29,960 --> 00:11:34,573
that automate to a certain
degree, the attacks,

233
00:11:35,572 --> 00:11:37,780
there's a metrics module
which provides tools

234
00:11:37,780 --> 00:11:41,060
for quantifying robustness
and similar metrics,

235
00:11:41,060 --> 00:11:42,252
there's a defenses module

236
00:11:42,252 --> 00:11:46,229
that contains the
defenses against attacks,

237
00:11:46,230 --> 00:11:48,260
including adversarial training,

238
00:11:48,260 --> 00:11:50,910
pre-processing and
post-processing defenses,

239
00:11:50,910 --> 00:11:52,100
with the defense module,

240
00:11:52,100 --> 00:11:54,310
we always would like add the disclaimer

241
00:11:54,310 --> 00:11:58,270
that most or all of these
defenses have been broken

242
00:11:58,270 --> 00:11:59,230
to a certain degree,

243
00:11:59,230 --> 00:12:01,580
so be careful when you apply them

244
00:12:01,580 --> 00:12:04,719
to follow the recent
evaluation literature,

245
00:12:04,719 --> 00:12:07,493
and then we also have a
pre-processing module,

246
00:12:09,188 --> 00:12:11,590
which allows you to
implement any modification

247
00:12:11,590 --> 00:12:12,910
of your input data,

248
00:12:12,910 --> 00:12:15,223
and one of the most exciting tools

249
00:12:15,223 --> 00:12:18,689
that we provide is Expectation
over Transformations,

250
00:12:18,690 --> 00:12:23,420
which allows you to train your model

251
00:12:23,420 --> 00:12:26,280
on random perturbations,

252
00:12:26,280 --> 00:12:27,300
to make it more robust

253
00:12:27,300 --> 00:12:31,079
and also the attacks much stronger.

254
00:12:31,080 --> 00:12:35,210
Now, I'm highlighting two
sub-modules in pink here,

255
00:12:35,210 --> 00:12:37,237
so these are the inference attacks

256
00:12:37,237 --> 00:12:39,535
and the classification estimators,

257
00:12:39,535 --> 00:12:41,591
and these are the two sub-modules

258
00:12:41,591 --> 00:12:45,630
that Abigail is now going to present

259
00:12:45,630 --> 00:12:47,790
in the second part of the presentation,

260
00:12:47,790 --> 00:12:49,689
and I'm happy to hand over to Abigail.

261
00:12:51,060 --> 00:12:53,479
- Okay, so I'm going to focus specifically

262
00:12:53,480 --> 00:12:55,670
on two types of inference attacks,

263
00:12:55,670 --> 00:12:58,990
namely, attribute inference
and membership inference,

264
00:12:58,990 --> 00:13:01,030
and anyone interested in other types

265
00:13:01,030 --> 00:13:02,900
of attacks is welcome to check

266
00:13:02,900 --> 00:13:04,933
out the inference module in ART.

267
00:13:05,910 --> 00:13:07,449
So the first attack I'm going to talk

268
00:13:07,450 --> 00:13:09,580
about is called attribute inference,

269
00:13:09,580 --> 00:13:12,220
and here we assume that an attacker knows

270
00:13:12,220 --> 00:13:14,279
that a specific sample was part

271
00:13:14,279 --> 00:13:17,060
of the training data of a model,

272
00:13:17,060 --> 00:13:19,569
and they know, they
have partial information

273
00:13:19,570 --> 00:13:20,660
about that sample,

274
00:13:20,660 --> 00:13:24,800
and they want to learn the
value of some other unknown,

275
00:13:24,800 --> 00:13:26,632
potentially sensitive feature,

276
00:13:28,000 --> 00:13:32,770
and there are two modes in which
this attack can be applied,

277
00:13:32,770 --> 00:13:35,170
in the black-box setting,

278
00:13:35,170 --> 00:13:37,530
we assume that the
attacker only has access

279
00:13:37,530 --> 00:13:39,970
to the output of a machine learning model,

280
00:13:39,970 --> 00:13:41,500
and in the white-box setting,

281
00:13:41,500 --> 00:13:42,680
we assume that the attacker

282
00:13:42,680 --> 00:13:45,900
also knows internal
details about the model,

283
00:13:45,900 --> 00:13:48,763
such as its architecture
and weights, et cetera.

284
00:13:50,400 --> 00:13:54,500
The threat here is obviously
leaking the information

285
00:13:54,500 --> 00:13:56,830
about these sensitive features,

286
00:13:56,830 --> 00:13:58,590
which can be a person's salary

287
00:13:58,590 --> 00:14:01,500
or their sexual orientation, et cetera,

288
00:14:01,500 --> 00:14:06,500
and the way that this attack works,

289
00:14:06,520 --> 00:14:09,620
is that usually the known features,

290
00:14:09,620 --> 00:14:11,410
as well as the model's predictions,

291
00:14:11,410 --> 00:14:15,540
are used to infer the value
of the missing feature,

292
00:14:15,540 --> 00:14:17,160
and one way this can be done

293
00:14:17,160 --> 00:14:20,370
is by varying the value
of the target feature,

294
00:14:20,370 --> 00:14:24,493
and checking which value causes
the most similar outcome.

295
00:14:26,120 --> 00:14:28,150
In ART, there are three implementations

296
00:14:28,150 --> 00:14:29,990
of attribute inference attacks,

297
00:14:29,990 --> 00:14:33,020
two white-box attacks
for decision tree models,

298
00:14:33,020 --> 00:14:34,853
and one black-box attacks.

299
00:14:38,100 --> 00:14:40,030
Now, I'm going to show an example

300
00:14:40,030 --> 00:14:41,923
of how to use one of those attacks.

301
00:14:44,030 --> 00:14:48,833
So I'm using the publicly
available nursery dataset,

302
00:14:49,710 --> 00:14:54,710
it's used for deciding on
nursery placement for children,

303
00:14:55,510 --> 00:14:57,823
based on characteristics of their family.

304
00:14:59,070 --> 00:15:02,810
It has eight features,
all of them categorical,

305
00:15:02,810 --> 00:15:04,229
and the feature that I'm going

306
00:15:04,230 --> 00:15:06,620
to try to attack today is called social,

307
00:15:06,620 --> 00:15:09,320
and it represents the
social health of the family,

308
00:15:09,320 --> 00:15:12,003
so obviously this is very
sensitive information.

309
00:15:13,090 --> 00:15:15,560
Originally, this is a categorical feature

310
00:15:15,560 --> 00:15:17,223
with three possible values,

311
00:15:18,070 --> 00:15:21,630
in order to increase the chances
of success of the attack,

312
00:15:21,630 --> 00:15:23,850
we turned this into a binary feature

313
00:15:23,850 --> 00:15:26,633
by combining two of the
values to a single value,

314
00:15:27,810 --> 00:15:31,142
and the data has also been
one-hot and coded and scaled.

315
00:15:32,390 --> 00:15:34,900
So this is what the data looks like,

316
00:15:34,900 --> 00:15:36,620
and this is the social feature

317
00:15:36,620 --> 00:15:38,353
that we're going to try to attack,

318
00:15:39,330 --> 00:15:42,420
you can see that the zeros
and ones have been replaced

319
00:15:42,420 --> 00:15:46,060
by some other floating
point values due to scaling,

320
00:15:46,060 --> 00:15:47,479
but still it's a binary feature,

321
00:15:47,480 --> 00:15:49,323
it only has two possible values.

322
00:15:52,080 --> 00:15:52,913
Here we're training

323
00:15:52,913 --> 00:15:56,920
a regular Scikit-learn decision tree model

324
00:15:56,920 --> 00:15:58,640
with our training data,

325
00:15:58,640 --> 00:16:01,860
and in order to use this model within ART

326
00:16:01,860 --> 00:16:04,500
and be able to apply attacks to it,

327
00:16:04,500 --> 00:16:06,380
we need to create this wrapper class

328
00:16:06,380 --> 00:16:09,620
called ScikitlearnDecisionTreeClassifier,

329
00:16:09,620 --> 00:16:12,880
and we pass as input the
model that we've just trained.

330
00:16:14,660 --> 00:16:19,050
So the black-box attack
consists of an attack model

331
00:16:19,050 --> 00:16:21,770
that's trained on the
seven remaining features

332
00:16:21,770 --> 00:16:23,760
along with the model's prediction,

333
00:16:23,760 --> 00:16:26,810
to try to infer the value
of the attacked feature,

334
00:16:26,810 --> 00:16:28,483
in this case, the social feature.

335
00:16:30,120 --> 00:16:32,220
It's important to train the attack

336
00:16:32,220 --> 00:16:35,750
on a different data set than
it's going to be evaluated on,

337
00:16:35,750 --> 00:16:37,050
and since, in this case,

338
00:16:37,050 --> 00:16:38,541
we want to evaluate the attack

339
00:16:38,542 --> 00:16:41,820
on the original model's training data,

340
00:16:41,820 --> 00:16:45,770
we're going to train it on the
original model's test data,

341
00:16:45,770 --> 00:16:47,350
another option would've been

342
00:16:47,350 --> 00:16:51,210
to take the training data and
just divide it into two sets,

343
00:16:51,210 --> 00:16:53,240
and use one for training the attack

344
00:16:53,240 --> 00:16:55,233
and the other one for evaluating it.

345
00:16:57,440 --> 00:17:00,390
Here we're going to do
the inference phase,

346
00:17:00,390 --> 00:17:03,080
and for that we need to prepare a dataset

347
00:17:03,080 --> 00:17:05,060
that does not contain
the attacked feature,

348
00:17:05,060 --> 00:17:06,929
so that's what we're doing here,

349
00:17:06,930 --> 00:17:10,109
we're deleting the attack
feature from the training data,

350
00:17:10,109 --> 00:17:13,050
and we need the model's predictions,

351
00:17:13,050 --> 00:17:18,050
and we're also passing this
other parameter called values,

352
00:17:18,430 --> 00:17:20,079
it's the actual floating point values

353
00:17:20,079 --> 00:17:21,960
of the missing feature,

354
00:17:21,960 --> 00:17:26,030
so that the attack can
translate it's predictions back

355
00:17:26,030 --> 00:17:26,863
to those values,

356
00:17:26,863 --> 00:17:29,683
just so it'll be easier to
compare to the original data,

357
00:17:30,950 --> 00:17:33,420
and here we can see that the accuracy

358
00:17:33,420 --> 00:17:36,430
of the attack is around 70%,

359
00:17:36,430 --> 00:17:38,660
however, it's important
to note that when looking

360
00:17:38,660 --> 00:17:41,677
at the frequency of the different values

361
00:17:41,677 --> 00:17:43,250
in the original data set,

362
00:17:43,250 --> 00:17:45,220
the most frequent value
is actually present

363
00:17:45,220 --> 00:17:47,620
in 66% of the data,

364
00:17:47,620 --> 00:17:50,533
so this is only a little bit
better than random guessing.

365
00:17:52,460 --> 00:17:54,450
There's also a white-box attack,

366
00:17:54,450 --> 00:17:56,810
and I'm not going to go
into too many details,

367
00:17:56,810 --> 00:17:59,909
but I just want to mention
that, in this case,

368
00:17:59,910 --> 00:18:04,700
since there's no internal
model used within the attack,

369
00:18:04,700 --> 00:18:06,240
there's no need to fit it

370
00:18:06,240 --> 00:18:09,760
and you can just directly
call the infer method,

371
00:18:09,760 --> 00:18:10,940
and you can also see here

372
00:18:10,940 --> 00:18:14,423
that the accuracy is slightly
higher in the white-box case.

373
00:18:18,100 --> 00:18:21,129
This is a short recap of what
we just saw in the notebook

374
00:18:21,130 --> 00:18:23,413
and the results that we
were able to achieve,

375
00:18:26,630 --> 00:18:28,630
and now I'm going to
talk about a second type

376
00:18:28,630 --> 00:18:30,980
of attack called membership inference.

377
00:18:30,980 --> 00:18:32,770
Here we assume that the attacker

378
00:18:32,770 --> 00:18:35,930
has complete knowledge
of a specific sample,

379
00:18:35,930 --> 00:18:37,082
and what they wanna find out

380
00:18:37,082 --> 00:18:39,480
is whether it was part of the training set

381
00:18:39,480 --> 00:18:40,903
of a model or not.

382
00:18:41,830 --> 00:18:43,020
In this case, again,

383
00:18:43,020 --> 00:18:45,760
there are two possible modes of attacking,

384
00:18:45,760 --> 00:18:48,113
black-box and white-box, same as before,

385
00:18:49,200 --> 00:18:51,110
in ART at the moment,

386
00:18:51,110 --> 00:18:52,913
there are only black-box attacks.

387
00:18:54,280 --> 00:18:55,970
Here the threat is,

388
00:18:55,970 --> 00:18:57,670
in cases where knowledge

389
00:18:57,670 --> 00:19:00,670
of whether someone was
in the training data

390
00:19:00,670 --> 00:19:03,690
of a model is in itself sensitive,

391
00:19:03,690 --> 00:19:05,900
so take as an example a model

392
00:19:05,900 --> 00:19:07,790
that's trying to predict something related

393
00:19:07,790 --> 00:19:09,649
to a specific disease,

394
00:19:09,650 --> 00:19:11,502
then the fact that someone participated

395
00:19:11,502 --> 00:19:14,409
in the training of this
model could be an indication

396
00:19:14,410 --> 00:19:15,810
that they have this disease,

397
00:19:17,190 --> 00:19:20,000
and the way that these
attacks usually work,

398
00:19:20,000 --> 00:19:22,770
is they try to distinguish
between the behavior

399
00:19:22,770 --> 00:19:26,170
of the model on samples that
it has seen during training,

400
00:19:26,170 --> 00:19:28,490
and samples that it hasn't seen,

401
00:19:28,490 --> 00:19:31,170
in the case of a black-box attacks,

402
00:19:31,170 --> 00:19:33,750
we do this based on the
output of the model,

403
00:19:33,750 --> 00:19:36,820
so this can be either
a probability vector,

404
00:19:36,820 --> 00:19:39,040
a single predicted label,

405
00:19:39,040 --> 00:19:41,600
the loss of the model, et cetera,

406
00:19:41,600 --> 00:19:44,820
and in ART there are four
different implementations

407
00:19:44,820 --> 00:19:46,760
of membership inference attacks,

408
00:19:46,760 --> 00:19:48,480
all of them are black-box attacks

409
00:19:48,480 --> 00:19:50,113
and these are their names,

410
00:19:51,930 --> 00:19:54,323
and now I'm going to
demonstrate one of those.

411
00:19:55,610 --> 00:19:59,010
So I'm using the same data set
as before, the nursery data,

412
00:19:59,010 --> 00:20:01,960
this time I didn't need to
manipulate the social feature

413
00:20:01,960 --> 00:20:03,160
because it's not needed,

414
00:20:04,040 --> 00:20:06,413
and again it's one-hot encoded and scaled.

415
00:20:07,890 --> 00:20:09,470
In the case of this attack,

416
00:20:09,470 --> 00:20:11,330
I need a balanced data set,

417
00:20:11,330 --> 00:20:16,330
so it needs to be 50% training
data and 50% test data,

418
00:20:17,400 --> 00:20:19,800
and in addition I'm purposefully

419
00:20:19,800 --> 00:20:23,110
making the training set pretty small,

420
00:20:23,110 --> 00:20:23,943
this is, again,

421
00:20:23,943 --> 00:20:27,062
to increase the success
rate of the attack.

422
00:20:28,980 --> 00:20:29,813
So we're going

423
00:20:29,813 --> 00:20:33,843
to train a just regular logistic
regression model this time,

424
00:20:35,090 --> 00:20:36,459
and same as before,

425
00:20:36,460 --> 00:20:38,250
I need to create this ART wrapper

426
00:20:38,250 --> 00:20:40,160
called ScikitlearnLogisticRegression

427
00:20:40,160 --> 00:20:42,970
and pass it the model
that I've just trained,

428
00:20:42,970 --> 00:20:46,933
and this is the original
accuracy of the model, 91%.

429
00:20:50,550 --> 00:20:52,230
The black-box attack here,

430
00:20:52,230 --> 00:20:55,890
is similar to the one
that we showed earlier,

431
00:20:55,890 --> 00:20:59,930
in that it also trains an attack model,

432
00:20:59,930 --> 00:21:01,650
and in this case,

433
00:21:01,650 --> 00:21:05,790
we're going to pass for
the training of the attack,

434
00:21:05,790 --> 00:21:07,840
half of the original training data

435
00:21:07,840 --> 00:21:10,169
and half of the original test data,

436
00:21:10,170 --> 00:21:12,210
and we're going to use
the remaining halves

437
00:21:12,210 --> 00:21:14,003
for the evaluation of the attack.

438
00:21:15,290 --> 00:21:16,649
When creating this attack,

439
00:21:16,650 --> 00:21:19,200
we can also supply this parameter

440
00:21:19,200 --> 00:21:22,010
that tells it what type
of attack model to use,

441
00:21:22,010 --> 00:21:24,510
and in this case, we're
going to use random first,

442
00:21:26,160 --> 00:21:27,990
and here we're applying the attack

443
00:21:27,990 --> 00:21:29,750
and checking the accuracy,

444
00:21:29,750 --> 00:21:33,770
and we can see that the attack
achieves an accuracy of 54%,

445
00:21:33,770 --> 00:21:34,760
which is, again,

446
00:21:34,760 --> 00:21:36,873
a little bit better than random guessing,

447
00:21:41,640 --> 00:21:43,030
and again here's a recap

448
00:21:43,030 --> 00:21:45,730
of what we just saw in the notebook,

449
00:21:45,730 --> 00:21:49,243
and the results that we
were able to achieve.

450
00:21:52,540 --> 00:21:55,360
Now, I'm going to present
one possible defense

451
00:21:55,360 --> 00:21:57,120
against this type of attack,

452
00:21:57,120 --> 00:21:59,070
which is based on differential privacy.

453
00:22:00,050 --> 00:22:00,980
In differential privacy,

454
00:22:00,980 --> 00:22:02,130
a small amount of noise

455
00:22:02,130 --> 00:22:04,370
is added during the training process,

456
00:22:04,370 --> 00:22:06,239
such that in the resulting model,

457
00:22:06,240 --> 00:22:07,970
it will be very difficult to distinguish

458
00:22:07,970 --> 00:22:09,600
whether a specific sample was part

459
00:22:09,600 --> 00:22:11,053
of the training set or not.

460
00:22:12,240 --> 00:22:14,370
Here you can see the
mathematical definition

461
00:22:14,370 --> 00:22:16,129
of differential privacy,

462
00:22:16,130 --> 00:22:19,310
it uses two parameters, epsilon and delta,

463
00:22:19,310 --> 00:22:21,810
that determine the level
of the privacy guarantee,

464
00:22:23,210 --> 00:22:25,320
the smaller these values are,

465
00:22:25,320 --> 00:22:26,572
the better the privacy,

466
00:22:27,570 --> 00:22:28,620
and I'm going to show you how

467
00:22:28,620 --> 00:22:31,209
to build a differentially private model,

468
00:22:31,210 --> 00:22:34,570
using an open-source-library
called Diffprivlib,

469
00:22:34,570 --> 00:22:36,320
of course, there are other options,

470
00:22:37,460 --> 00:22:39,023
now let's look at the example.

471
00:22:42,540 --> 00:22:45,110
So training a differentially private model

472
00:22:45,110 --> 00:22:48,090
is pretty similar to
training a regular model,

473
00:22:48,090 --> 00:22:51,693
we just need to pass it
two additional parameters,

474
00:22:53,090 --> 00:22:54,350
the first is epsilon,

475
00:22:54,350 --> 00:22:57,870
which is the privacy parameter
that I just talked about,

476
00:22:57,870 --> 00:23:00,459
in this case, there is no delta,

477
00:23:00,460 --> 00:23:02,340
and the second is the data norm,

478
00:23:02,340 --> 00:23:06,740
which is the maximum L2 norm
of any record in the dataset,

479
00:23:06,740 --> 00:23:08,820
and this is used to calibrate the noise

480
00:23:08,820 --> 00:23:10,270
that's added during training.

481
00:23:11,500 --> 00:23:14,010
So again, we train the model,

482
00:23:14,010 --> 00:23:15,983
and then we create the art wrapper,

483
00:23:16,900 --> 00:23:18,460
and we can see that the accuracy

484
00:23:18,460 --> 00:23:21,290
is very low this time, it's 56%,

485
00:23:21,290 --> 00:23:23,340
and I'll get back to this in a moment,

486
00:23:23,340 --> 00:23:26,149
but I just wanna show what
happens with the attack,

487
00:23:26,150 --> 00:23:28,530
so I'm running the same attack as before,

488
00:23:28,530 --> 00:23:30,680
the black-box membership inference attack

489
00:23:30,680 --> 00:23:32,740
with the same model type,

490
00:23:32,740 --> 00:23:34,530
and we can see that both the accuracy

491
00:23:34,530 --> 00:23:36,040
and the precision have been reduced

492
00:23:36,040 --> 00:23:39,633
to 51% from 54% earlier,

493
00:23:40,870 --> 00:23:42,860
but still we've we've lost most

494
00:23:42,860 --> 00:23:45,070
of the original accuracy of the model,

495
00:23:45,070 --> 00:23:47,720
so we wanna see if we can
find a better compromise

496
00:23:47,720 --> 00:23:49,833
between model accuracy and privacy,

497
00:23:51,070 --> 00:23:55,830
so here I'm trying out different
possible values of epsilon,

498
00:23:55,830 --> 00:23:56,980
and for each one I'm going

499
00:23:56,980 --> 00:23:59,700
to train the differentially private model,

500
00:23:59,700 --> 00:24:02,763
train my attack and check
the accuracy of the attack,

501
00:24:03,650 --> 00:24:06,550
and so let's see what happens
with these different values.

502
00:24:07,470 --> 00:24:09,180
So when looking at the model accuracy,

503
00:24:09,180 --> 00:24:12,130
we can see that as epsilon increases,

504
00:24:12,130 --> 00:24:13,730
the model accuracy also increases

505
00:24:13,730 --> 00:24:16,883
until it reaches the original
accuracy of the model,

506
00:24:18,380 --> 00:24:20,450
and when looking at the attack accuracy,

507
00:24:20,450 --> 00:24:22,140
we see a similar trend,

508
00:24:22,140 --> 00:24:25,170
where increasing epsilon
increases the accuracy

509
00:24:25,170 --> 00:24:26,551
of the attack,

510
00:24:26,551 --> 00:24:29,730
there's a few spikes here
and there in both directions

511
00:24:29,730 --> 00:24:32,943
and this is due to the
randomness of the noise edition,

512
00:24:34,040 --> 00:24:35,860
so we can see that we have a trade-off

513
00:24:35,860 --> 00:24:39,020
between the model accuracy
and the attack accuracy,

514
00:24:39,020 --> 00:24:41,350
but we can see that around this area

515
00:24:41,350 --> 00:24:43,770
with epsilon equals 25,

516
00:24:43,770 --> 00:24:46,633
looks like a good option in both cases.

517
00:24:47,470 --> 00:24:49,360
So I've chosen this value

518
00:24:49,360 --> 00:24:51,699
and I ran the whole process again,

519
00:24:51,700 --> 00:24:53,080
and we can see that we're able

520
00:24:53,080 --> 00:24:55,770
to achieve reasonable model accuracy,

521
00:24:55,770 --> 00:24:58,720
while still reducing the attack accuracy,

522
00:24:58,720 --> 00:25:00,018
and even though 25

523
00:25:00,018 --> 00:25:03,730
may be considered a
relatively high epsilon value,

524
00:25:03,730 --> 00:25:05,290
in this specific case,

525
00:25:05,290 --> 00:25:07,663
it still provides a decent defense,

526
00:25:13,080 --> 00:25:16,092
and again, we have a
recap of what we just saw.

527
00:25:20,260 --> 00:25:23,200
So I demonstrated the attacks using ART

528
00:25:23,200 --> 00:25:27,080
and the use of differential
privacy using Diffprivlib,

529
00:25:27,080 --> 00:25:30,850
and here we show a few other
options of open-source tools,

530
00:25:30,850 --> 00:25:33,790
both for attacking and assessing models,

531
00:25:33,790 --> 00:25:37,220
as well as defending
against these attacks,

532
00:25:37,220 --> 00:25:40,100
some of these defenses also
use differential privacy,

533
00:25:40,100 --> 00:25:43,283
and some of them use
other privacy mechanisms.

534
00:25:46,800 --> 00:25:48,370
So this is what we suggest you do

535
00:25:48,370 --> 00:25:51,072
in the next weeks and
months following this talk,

536
00:25:52,040 --> 00:25:53,240
so in the short range,

537
00:25:53,240 --> 00:25:55,990
we recommend finding the relevant models,

538
00:25:55,990 --> 00:25:59,410
meaning those that were
trained on personal data,

539
00:25:59,410 --> 00:26:01,850
as well as trying to raise the awareness

540
00:26:01,850 --> 00:26:03,949
of the relevant people
in your organization

541
00:26:03,950 --> 00:26:05,993
regarding these potential risks,

542
00:26:07,200 --> 00:26:10,780
in the medium range, we
propose to download ART

543
00:26:10,780 --> 00:26:12,670
and start learning how to use it,

544
00:26:12,670 --> 00:26:15,733
using the examples and
notebooks provided in the tool,

545
00:26:16,930 --> 00:26:17,840
in addition, you can try

546
00:26:17,840 --> 00:26:21,240
to identify the most relevant
attacks for your use case,

547
00:26:21,240 --> 00:26:24,093
for example, black-box versus white-box,

548
00:26:26,772 --> 00:26:27,980
and in the longer range,

549
00:26:27,980 --> 00:26:30,750
you should identify your
high-risk models based

550
00:26:30,750 --> 00:26:33,210
on how they're deployed or published,

551
00:26:33,210 --> 00:26:37,020
and vulnerability to one
or more of the attacks,

552
00:26:37,020 --> 00:26:39,713
and then start applying
mitigation strategies.

553
00:26:40,840 --> 00:26:44,115
One option is to use open-source
tools like Diffprivlib

554
00:26:44,115 --> 00:26:46,860
and ai-privacy-toolkit and more,

555
00:26:46,860 --> 00:26:49,260
that provide defenses for models

556
00:26:49,260 --> 00:26:51,820
against these types of attacks,

557
00:26:51,820 --> 00:26:55,550
another option is to try to
change your training data,

558
00:26:55,550 --> 00:26:58,745
such that it will contain
less personal information,

559
00:26:58,746 --> 00:27:01,330
if, for example, you can use public data

560
00:27:01,330 --> 00:27:04,080
instead of personal
information, that's great,

561
00:27:04,080 --> 00:27:06,449
if not, at least try to abstain

562
00:27:06,450 --> 00:27:09,030
from using the most sensitive features

563
00:27:09,030 --> 00:27:10,943
unless they're really required,

564
00:27:12,100 --> 00:27:13,689
and the third thing that can be done,

565
00:27:13,690 --> 00:27:16,060
is trying to change the model,

566
00:27:16,060 --> 00:27:20,389
so you can try out different
architectures and model types

567
00:27:20,390 --> 00:27:21,800
and evaluate each of them

568
00:27:21,800 --> 00:27:24,350
to try to find the one
that best suits your needs,

569
00:27:24,350 --> 00:27:25,954
both from a privacy perspective,

570
00:27:25,954 --> 00:27:30,062
and accuracy and other
relevant perspectives.

571
00:27:33,090 --> 00:27:34,540
Thank you very much for listening

572
00:27:34,540 --> 00:27:36,597
and we're here to answer any questions.

