1
00:00:01,720 --> 00:00:03,947
- Hi everyone and welcome
to our session about

2
00:00:03,947 --> 00:00:05,950
"Navigating the Unknowable:

3
00:00:05,950 --> 00:00:09,620
Creating Resilience through
Security Chaos Engineering."

4
00:00:09,620 --> 00:00:12,329
We wanna first of all
thank RSA for hosting us

5
00:00:12,330 --> 00:00:14,370
and having us at this big event

6
00:00:14,370 --> 00:00:17,490
and for showcasing the work
that we've done in this space.

7
00:00:17,490 --> 00:00:19,500
We are really honored to be here today

8
00:00:19,500 --> 00:00:22,400
and are honored that you
chose to watch our session.

9
00:00:22,400 --> 00:00:24,680
My name is Jamie Dicken
and I am the Director

10
00:00:24,680 --> 00:00:28,410
of Security Assurance at
National Resilience Inc.

11
00:00:28,410 --> 00:00:31,580
I lead up GRC audit and security awareness

12
00:00:31,580 --> 00:00:35,010
and I have a huge focus on
continuous security validation

13
00:00:35,010 --> 00:00:38,220
and a lot of the concepts we're
gonna talk about here today.

14
00:00:38,220 --> 00:00:41,150
My co-speaker is none
other than Aaron Rinehart,

15
00:00:41,150 --> 00:00:45,680
the CTO of Verika, pioneer of
Security Chaos Engineering,

16
00:00:45,680 --> 00:00:48,850
leader of ChaoSlingr back
at UnitedHealth Group

17
00:00:48,850 --> 00:00:51,160
and co-author of a new O'Reilly report

18
00:00:51,160 --> 00:00:53,403
on Security Chaos Engineering.

19
00:00:54,460 --> 00:00:55,980
We could not be more excited

20
00:00:55,980 --> 00:00:58,059
to present this talk to you today.

21
00:00:58,060 --> 00:01:00,390
We are gonna discuss the challenges that

22
00:01:00,390 --> 00:01:03,110
we as an industry have been facing

23
00:01:03,110 --> 00:01:06,340
and how Security Chaos
Engineering is really the future

24
00:01:06,340 --> 00:01:09,610
that can answer a lot
of our tough problems.

25
00:01:09,610 --> 00:01:12,710
We will also give examples
of real world case studies

26
00:01:12,710 --> 00:01:16,429
that we both have been a part
of and then we will help you

27
00:01:16,430 --> 00:01:19,380
on your security chaos journey as well.

28
00:01:19,380 --> 00:01:20,449
If you're watching live,

29
00:01:20,450 --> 00:01:22,180
please make sure to use the chat window

30
00:01:22,180 --> 00:01:23,830
and Aaron and I will be monitoring

31
00:01:23,830 --> 00:01:26,263
and answering your questions in real time.

32
00:01:27,110 --> 00:01:29,033
And with that, let's get started.

33
00:01:30,420 --> 00:01:33,603
As we all know, systems
engineering is messy.

34
00:01:34,440 --> 00:01:35,273
In the beginning,

35
00:01:35,273 --> 00:01:37,850
we have these beautifully
simplistic representations

36
00:01:37,850 --> 00:01:39,770
either of what we want to build

37
00:01:39,770 --> 00:01:41,960
or what we think that we did build.

38
00:01:41,960 --> 00:01:44,140
But it doesn't take long for us to sour

39
00:01:44,140 --> 00:01:47,030
on our initially perfect creations.

40
00:01:47,030 --> 00:01:48,710
And it's not always our fault.

41
00:01:48,710 --> 00:01:49,970
Throughout a system's life,

42
00:01:49,970 --> 00:01:52,323
complexity has a way of sneaking in.

43
00:01:53,300 --> 00:01:55,280
And as time goes on,

44
00:01:55,280 --> 00:01:57,710
the problems compound as new changes

45
00:01:57,710 --> 00:01:59,699
introduced into the system

46
00:01:59,700 --> 00:02:03,050
or even as our teams or
our processes change.

47
00:02:03,050 --> 00:02:06,590
The reality is that after
years of complexity,

48
00:02:06,590 --> 00:02:09,930
our systems become almost unrecognizable

49
00:02:09,930 --> 00:02:14,180
and the outages and the
incidents that we face

50
00:02:14,180 --> 00:02:15,920
just show that we aren't really keeping up

51
00:02:15,920 --> 00:02:18,839
with an understanding of our own systems.

52
00:02:18,840 --> 00:02:22,450
What we need is a radical
new way to stabilize them

53
00:02:22,450 --> 00:02:25,299
and secure them if we're ever
going to get ahead of this.

54
00:02:26,520 --> 00:02:27,520
And if you think about it,

55
00:02:27,520 --> 00:02:30,330
most times when you are changing a system,

56
00:02:30,330 --> 00:02:34,340
you're adding complexity
whether you intend to or not,

57
00:02:34,340 --> 00:02:37,550
and complexity itself isn't
necessarily a bad thing.

58
00:02:37,550 --> 00:02:38,990
It could be really necessary

59
00:02:38,990 --> 00:02:42,035
for the use cases that
you're trying to accomplish.

60
00:02:42,035 --> 00:02:45,340
But when you increase your complexity,

61
00:02:45,340 --> 00:02:48,360
you also increase the number
of potential failure points

62
00:02:48,360 --> 00:02:49,333
that you have too.

63
00:02:50,600 --> 00:02:54,070
Complexity, it's more than
just an engineering problem

64
00:02:54,070 --> 00:02:56,019
or an operations problem.

65
00:02:56,020 --> 00:02:59,503
It's also a security problem
and a reliability problem too.

66
00:03:00,440 --> 00:03:02,760
As your system complexity increases,

67
00:03:02,760 --> 00:03:05,880
so does that opportunity
for mistakes and accidents

68
00:03:05,880 --> 00:03:08,873
and misconfigurations
that can lead to a breach.

69
00:03:10,210 --> 00:03:12,820
Year after year, for the last 10 years,

70
00:03:12,820 --> 00:03:15,180
the data and the "Cost
of a Data Breach Report"

71
00:03:15,180 --> 00:03:17,610
has remained relatively unchanged

72
00:03:17,610 --> 00:03:19,660
and we're still facing the cold hard fact

73
00:03:19,660 --> 00:03:21,859
that almost half of data breaches

74
00:03:21,860 --> 00:03:24,493
are caused by human error
and system glitches.

75
00:03:25,510 --> 00:03:28,750
Now, the good news is that
these causes are preventable

76
00:03:28,750 --> 00:03:31,713
if we have a way of knowing
that they've already happened.

77
00:03:32,620 --> 00:03:34,930
So therefore that question really becomes,

78
00:03:34,930 --> 00:03:38,413
how can we proactively find
these before an adversary does?

79
00:03:40,110 --> 00:03:41,920
In the past, we, as an industry,

80
00:03:41,920 --> 00:03:44,420
we relied really heavily on system design

81
00:03:44,420 --> 00:03:47,130
and documentation to identify

82
00:03:47,130 --> 00:03:49,370
potential security vulnerabilities.

83
00:03:49,370 --> 00:03:52,020
We take these documents and
we begin our threat modeling

84
00:03:52,020 --> 00:03:54,310
and identifying single points of failure

85
00:03:54,310 --> 00:03:57,006
and opportunities for latency.

86
00:03:57,006 --> 00:04:00,870
Unfortunately, in the world of
complex distributed systems,

87
00:04:00,870 --> 00:04:03,750
this method alone, isn't sufficient.

88
00:04:03,750 --> 00:04:06,370
It's still very valuable
to do threat modeling.

89
00:04:06,370 --> 00:04:10,313
But relying on outdated documentation,

90
00:04:11,870 --> 00:04:14,480
relying on that alone it's old school.

91
00:04:14,480 --> 00:04:16,959
Now, there are two real
big problems with it.

92
00:04:16,959 --> 00:04:20,029
So first your documentation
is never going to tell you

93
00:04:20,029 --> 00:04:22,422
that a human being made a mistake.

94
00:04:23,350 --> 00:04:25,480
But next, think about when

95
00:04:25,480 --> 00:04:28,560
and how our documentation
is typically made.

96
00:04:28,560 --> 00:04:31,280
Maybe it was made in the
beginning of a project

97
00:04:31,280 --> 00:04:33,840
before a single line of code was written.

98
00:04:33,840 --> 00:04:36,599
If that's the case, then
your documentation matches

99
00:04:36,600 --> 00:04:38,940
the system that was in an engineer's head

100
00:04:38,940 --> 00:04:40,380
but maybe doesn't even match

101
00:04:40,380 --> 00:04:42,623
the system that was
deployed to production.

102
00:04:43,690 --> 00:04:47,010
Or maybe that documentation
does match what was deployed

103
00:04:47,010 --> 00:04:50,772
but was it maintained
throughout the assistant's life?

104
00:04:51,910 --> 00:04:55,930
The old school approach of
only using documentation fails

105
00:04:55,930 --> 00:04:58,360
because the assumption is inherent

106
00:04:58,360 --> 00:05:00,700
that that documentation is correct,

107
00:05:00,700 --> 00:05:03,250
that it's updated and that it's complete

108
00:05:03,250 --> 00:05:05,560
and I'm sure all of us have
seen plenty of examples

109
00:05:05,560 --> 00:05:07,423
where that assumption doesn't hold.

110
00:05:08,400 --> 00:05:11,010
If your process of evaluating a system

111
00:05:11,010 --> 00:05:14,800
for security vulnerabilities
or reliability issues

112
00:05:14,800 --> 00:05:17,330
is completely dependent on an outdated

113
00:05:17,330 --> 00:05:19,960
or incomplete representation
of your system,

114
00:05:19,960 --> 00:05:23,270
your evaluation is
ultimately going to fail

115
00:05:23,270 --> 00:05:24,919
and there are going to
be plenty of problems

116
00:05:24,920 --> 00:05:26,373
that you don't anticipate.

117
00:05:28,160 --> 00:05:29,020
I like this quote.

118
00:05:29,020 --> 00:05:31,219
Complexity Scientist
David Snowden reminds us

119
00:05:31,220 --> 00:05:33,920
that it's actually impossible for a human

120
00:05:33,920 --> 00:05:38,020
to document and model a complex system.

121
00:05:38,020 --> 00:05:41,460
The only way to understand it
then is to interact with it

122
00:05:41,460 --> 00:05:44,010
and that's going to be the core solution

123
00:05:44,010 --> 00:05:46,450
that we're talking about today.

124
00:05:46,450 --> 00:05:49,479
The answer to this challenge
is not to just rely on our

125
00:05:49,480 --> 00:05:52,240
own recollection of our systems but

126
00:05:52,240 --> 00:05:55,800
to use empirical data and learn from them.

127
00:05:55,800 --> 00:05:58,500
And that's really the
heart of chaos engineering,

128
00:05:58,500 --> 00:06:01,640
instrumenting experiments that
clearly at the end of the day

129
00:06:01,640 --> 00:06:04,820
remind us what our real
system landscapes look like

130
00:06:04,820 --> 00:06:07,830
so that we can tease out
those false assumptions.

131
00:06:07,830 --> 00:06:09,780
So Aaron, I'm gonna turn
it to you to tell us

132
00:06:09,780 --> 00:06:11,760
how to do things differently.

133
00:06:11,761 --> 00:06:12,594
- Right.

134
00:06:13,957 --> 00:06:15,270
I love that.

135
00:06:15,270 --> 00:06:16,520
I always like to say that

136
00:06:17,787 --> 00:06:18,679
within engineering, we
don't believe in two things.

137
00:06:18,680 --> 00:06:20,620
We don't believe in hope and luck, right?

138
00:06:20,620 --> 00:06:23,340
We believe in goodish
mentation of feedback loops

139
00:06:23,340 --> 00:06:26,130
and that's what this is all about.

140
00:06:26,130 --> 00:06:29,560
What it's about is it's about
a new approach to learning

141
00:06:29,560 --> 00:06:31,930
and being proactive while
proactively discovering,

142
00:06:31,930 --> 00:06:34,980
like Jamie said that we
made a mistake or accident

143
00:06:34,980 --> 00:06:36,960
or that we've drifted beyond.

144
00:06:36,960 --> 00:06:37,900
Our security controls

145
00:06:37,900 --> 00:06:39,729
are no longer as effective
as we thought they were

146
00:06:39,730 --> 00:06:41,520
because the system has drifted

147
00:06:41,520 --> 00:06:43,849
beyond the securities context.

148
00:06:43,850 --> 00:06:45,970
And so it's about being
proactive and learning.

149
00:06:45,970 --> 00:06:47,310
So it's not about...

150
00:06:47,310 --> 00:06:49,308
So a lot of people think

151
00:06:49,308 --> 00:06:52,190
the way forward is

152
00:06:52,190 --> 00:06:53,830
really more about continuous fixing.

153
00:06:53,830 --> 00:06:55,849
It's not really about
traditionally fixing,

154
00:06:55,850 --> 00:06:59,160
it's about learning about
how the system really works

155
00:06:59,160 --> 00:07:02,820
versus how it works versus
how we thought it works.

156
00:07:02,820 --> 00:07:05,460
And that's really in terms of like

157
00:07:05,460 --> 00:07:07,539
how we learn traditionally

158
00:07:07,540 --> 00:07:10,503
about how our system didn't really work

159
00:07:10,503 --> 00:07:11,740
the way we thought it did.

160
00:07:11,740 --> 00:07:14,690
It's through a series of
outages, incidents or breaches

161
00:07:14,690 --> 00:07:18,230
and that's not a good time to learn this.

162
00:07:18,230 --> 00:07:21,090
So Chaos Engineering
is a proactive exercise

163
00:07:21,090 --> 00:07:22,530
and it allows us to actually learn

164
00:07:22,530 --> 00:07:24,123
instead of just continuously
fixing the system

165
00:07:24,123 --> 00:07:26,099
when there's a problem.

166
00:07:26,100 --> 00:07:27,480
So how does the system become stable?

167
00:07:27,480 --> 00:07:30,890
So I like I was describing
before is that currently,

168
00:07:30,890 --> 00:07:34,169
how did that legacy
system become so stable?

169
00:07:34,170 --> 00:07:35,960
How did the engineers become so competent

170
00:07:35,960 --> 00:07:37,859
or feel so comfortable with the system?

171
00:07:37,860 --> 00:07:40,530
Why does it have very few outages anymore?

172
00:07:40,530 --> 00:07:44,330
And why is it so quote unquote, stable?

173
00:07:44,330 --> 00:07:46,159
Was it always that way?

174
00:07:46,160 --> 00:07:47,440
Is kind of the question.

175
00:07:47,440 --> 00:07:49,010
Is that

176
00:07:49,010 --> 00:07:51,240
most likely it was not, right?

177
00:07:51,240 --> 00:07:53,580
We learn about our system
slowly become stable

178
00:07:53,580 --> 00:07:54,690
and well known

179
00:07:54,690 --> 00:07:56,440
through a series of surprises.

180
00:07:56,440 --> 00:07:59,010
If it wasn't a surprise, you
would have fixed it, right?

181
00:07:59,010 --> 00:08:00,260
Or you would have taken
care of it already.

182
00:08:00,260 --> 00:08:02,940
The thing is like, we typically learn

183
00:08:02,940 --> 00:08:06,170
how the system really
worked to those events.

184
00:08:06,170 --> 00:08:08,620
And then we earn, okay?

185
00:08:08,620 --> 00:08:09,453
Although we take that data,

186
00:08:09,453 --> 00:08:12,539
we go back and we remediate
and then we run the system.

187
00:08:12,540 --> 00:08:13,600
Over time, we could learn

188
00:08:13,600 --> 00:08:16,200
or we would understand the system better.

189
00:08:16,200 --> 00:08:19,219
But the problem with that process is that

190
00:08:19,220 --> 00:08:22,780
it comes at a very expensive
costs, especially the customer.

191
00:08:22,780 --> 00:08:25,363
And with chaos engineering,

192
00:08:25,363 --> 00:08:30,363
it's a proactive exercise of
accelerating that kind of...

193
00:08:31,170 --> 00:08:32,700
Giving that kind of feedback

194
00:08:32,700 --> 00:08:35,673
without impacting the
customer or the business.

195
00:08:37,549 --> 00:08:39,909
It's also important to recognize that

196
00:08:39,909 --> 00:08:41,959
when there's an incident or an outage

197
00:08:41,960 --> 00:08:45,270
or war room being launched,
people operate differently.

198
00:08:45,270 --> 00:08:48,560
If you're not in security,
you're probably freaking out

199
00:08:48,560 --> 00:08:52,180
because people are worried about
the blame name, shame game.

200
00:08:52,180 --> 00:08:53,630
And really people worry about

201
00:08:55,383 --> 00:08:57,860
being named for causing it,

202
00:08:57,860 --> 00:08:59,460
I know I shouldn't have pushed that code,

203
00:08:59,460 --> 00:09:02,850
or I knew I should've made that change,

204
00:09:02,850 --> 00:09:04,820
and then being blamed for it.

205
00:09:04,820 --> 00:09:08,610
And it's just not a good
environment for learning.

206
00:09:08,610 --> 00:09:12,636
It's people are, their cognitive state

207
00:09:12,636 --> 00:09:14,780
is overloaded already.

208
00:09:14,780 --> 00:09:17,459
So chaos engineering, we like to....

209
00:09:17,460 --> 00:09:19,250
So I like to break down,

210
00:09:19,250 --> 00:09:20,290
let's say

211
00:09:20,290 --> 00:09:23,939
instrumentation into sort of two buckets.

212
00:09:23,940 --> 00:09:26,800
There's testing and
there's experimentation.

213
00:09:26,800 --> 00:09:29,050
So testing is the verification

214
00:09:29,050 --> 00:09:32,120
of something we already
know to be true or false.

215
00:09:32,120 --> 00:09:35,470
So in our world, it's either
like a CV or attack pattern.

216
00:09:35,470 --> 00:09:38,290
We know what we're looking for
before we go looking for it.

217
00:09:38,290 --> 00:09:40,349
Whereas experimentation,

218
00:09:40,350 --> 00:09:42,520
we're trying to derive new information

219
00:09:42,520 --> 00:09:44,350
that we previously did not know.

220
00:09:44,350 --> 00:09:45,834
You could think of

221
00:09:45,834 --> 00:09:48,021
that's why the experiments in the form of

222
00:09:48,022 --> 00:09:51,060
the scientific method hypothesis

223
00:09:51,060 --> 00:09:53,410
is that we're trying
to discover the unknown

224
00:09:53,410 --> 00:09:55,360
sort of unknown behavior in the system.

225
00:09:56,360 --> 00:09:58,390
So here's the definition
of chaos engineering.

226
00:09:58,390 --> 00:10:01,250
Chaos engineering is the
discipline of experimenting

227
00:10:01,250 --> 00:10:05,010
on the distributed system by
injecting turbulent conditions

228
00:10:05,010 --> 00:10:06,450
to learn how the system responds.

229
00:10:06,450 --> 00:10:08,230
So another way of saying this is that

230
00:10:08,230 --> 00:10:09,480
it's sort of the discipline

231
00:10:09,480 --> 00:10:11,490
of proactively introducing
tripling conditions

232
00:10:11,490 --> 00:10:13,970
into a system to try to
determine the conditions

233
00:10:13,970 --> 00:10:16,210
by which a system will fail

234
00:10:16,210 --> 00:10:20,080
before it actually fails so
we can learn and remediate it

235
00:10:20,080 --> 00:10:23,050
and the problem never actually manifests.

236
00:10:23,050 --> 00:10:25,000
So it really is a proactive,

237
00:10:25,000 --> 00:10:26,880
that's what really makes
this unique is that,

238
00:10:26,880 --> 00:10:29,530
it's not about meantime to
detect meantime to respond.

239
00:10:29,530 --> 00:10:33,810
We're proactively doing
these exercises when

240
00:10:33,810 --> 00:10:35,170
there is no real problem.

241
00:10:35,170 --> 00:10:37,390
When we have the opportunity

242
00:10:37,390 --> 00:10:41,080
to learn without word playing
the blame named shame game

243
00:10:41,080 --> 00:10:42,470
'cause we can't learn during an accident.

244
00:10:42,470 --> 00:10:44,831
It's an active incident 'cause it's about

245
00:10:44,831 --> 00:10:46,200
get that thing back up and running,

246
00:10:46,200 --> 00:10:48,770
or like we have an ongoing
attack, we gotta...

247
00:10:48,770 --> 00:10:50,350
it's about the mindset.

248
00:10:50,350 --> 00:10:53,200
The column load is not there
for you to be able to learn.

249
00:10:54,670 --> 00:10:57,199
So, security chaos engineering,

250
00:10:57,200 --> 00:10:58,970
really what we're trying to do is,

251
00:10:58,970 --> 00:11:03,970
is that trying to proactively
understand and discover.

252
00:11:04,020 --> 00:11:06,610
So in modern security
we have this issue where

253
00:11:06,610 --> 00:11:09,360
a modern engineering per se was that

254
00:11:09,360 --> 00:11:11,500
as a builder, I need the
flexibility and convenience

255
00:11:11,500 --> 00:11:12,990
to constantly change something.

256
00:11:12,990 --> 00:11:15,090
I'm constantly changing,
just because I'm attempting

257
00:11:15,090 --> 00:11:18,090
to deliver on business
requirements, deliver customer value

258
00:11:18,090 --> 00:11:19,920
so constantly changing or to do that.

259
00:11:19,920 --> 00:11:22,160
Well, security's a context
dependent component.

260
00:11:22,160 --> 00:11:23,730
So as security

261
00:11:23,730 --> 00:11:27,550
you must understand the object
and the context about it

262
00:11:27,550 --> 00:11:29,589
in order to know what needs to be secured.

263
00:11:29,590 --> 00:11:34,087
The problem is is that as we're delivering

264
00:11:34,087 --> 00:11:35,990
and building from an
engineering perspective

265
00:11:35,990 --> 00:11:37,110
towards customer value,

266
00:11:37,110 --> 00:11:40,160
the controls are often
not moving along with it.

267
00:11:40,160 --> 00:11:41,420
You start to diverge.

268
00:11:41,420 --> 00:11:43,760
And what we're trying to do
with chaos engineering is

269
00:11:43,760 --> 00:11:46,939
expose that drift before an

270
00:11:46,940 --> 00:11:49,315
adversary can actually
take advantage of it.

271
00:11:49,315 --> 00:11:51,773
And that's really what
we're trying to achieve.

272
00:11:54,470 --> 00:11:57,220
So, we often actually misremember

273
00:11:57,220 --> 00:11:58,610
what our systems really are.

274
00:11:58,610 --> 00:12:00,670
And like Jamie said, there's just...

275
00:12:00,670 --> 00:12:03,461
over time we don't recognize it because

276
00:12:03,461 --> 00:12:07,039
we can't rock all that
information in our heads

277
00:12:07,039 --> 00:12:09,520
like all the changes and
all the different teams

278
00:12:09,520 --> 00:12:10,730
and all the different pieces of the system

279
00:12:10,730 --> 00:12:11,823
moving and changing,

280
00:12:13,900 --> 00:12:15,100
it's opportunity for mistakes.

281
00:12:15,100 --> 00:12:16,800
I mean, we're human we're going to error

282
00:12:16,800 --> 00:12:18,740
that was always part of the equation.

283
00:12:18,740 --> 00:12:21,920
But like it increases as
the more changes we make

284
00:12:21,920 --> 00:12:25,050
to the system, the opportunity
for accidents, mistake,

285
00:12:25,050 --> 00:12:26,819
cognitive misalignment

286
00:12:26,820 --> 00:12:29,370
is there.

287
00:12:29,370 --> 00:12:30,670
So really one of the areas that

288
00:12:30,670 --> 00:12:32,760
I know Jamie and I have
this in common is that

289
00:12:32,760 --> 00:12:34,540
we focused on chaos
engineering is really about

290
00:12:34,540 --> 00:12:35,896
continuous verification.

291
00:12:35,896 --> 00:12:38,329
Continuously verifying
the system works the way

292
00:12:38,330 --> 00:12:39,570
it's supposed to.

293
00:12:39,570 --> 00:12:41,180
We're not attacking it,

294
00:12:41,180 --> 00:12:42,560
what we're trying to do is

295
00:12:42,560 --> 00:12:44,800
we're introducing questions in the system.

296
00:12:44,800 --> 00:12:47,020
Hey, I believe in our X conditions,

297
00:12:47,020 --> 00:12:49,780
Y is what security's supposed to do.

298
00:12:49,780 --> 00:12:51,360
So we're introducing that

299
00:12:51,360 --> 00:12:53,370
failure mode of that fault into the system

300
00:12:53,370 --> 00:12:56,370
to verify that patrol works
the way it's supposed to.

301
00:12:56,370 --> 00:12:57,810
And we never do a chaos experiment

302
00:12:57,810 --> 00:13:00,553
we know is going to fail because
we won't learn a new thing.

303
00:13:01,530 --> 00:13:04,060
So here's some brief use cases actually

304
00:13:04,060 --> 00:13:05,859
in the overall report that
we're gonna get a link to

305
00:13:05,860 --> 00:13:07,510
at the very end,

306
00:13:07,510 --> 00:13:09,510
that Jamie and I are
both a part of it goes

307
00:13:09,510 --> 00:13:11,830
into more depth with these use cases.

308
00:13:11,830 --> 00:13:14,610
But some great use cases
are instant response

309
00:13:14,610 --> 00:13:16,680
security observability,

310
00:13:16,680 --> 00:13:18,300
control validation is also...

311
00:13:18,300 --> 00:13:21,729
I know Jamie and I both experimented,

312
00:13:21,730 --> 00:13:23,530
we've got a lot of value in that.

313
00:13:23,530 --> 00:13:24,670
But it's also important to remember

314
00:13:24,670 --> 00:13:27,189
that every cast experiment,
whether it's for security

315
00:13:27,190 --> 00:13:30,490
or availability has compliance value.

316
00:13:30,490 --> 00:13:33,300
So make sure that you keep the outputs

317
00:13:33,300 --> 00:13:34,500
in a high integrity way.

318
00:13:35,420 --> 00:13:36,479
So there are some differences.

319
00:13:36,480 --> 00:13:38,655
So a lot of questions
that Jamie and I get,

320
00:13:38,655 --> 00:13:41,130
a lot of people in the security
chaos engineering space get.

321
00:13:41,130 --> 00:13:42,720
Is what is the difference
between red-teaming

322
00:13:42,720 --> 00:13:44,680
and sort of some of the
traditional exercises

323
00:13:44,680 --> 00:13:46,620
like red-teaming,
purple-teaming, pen testing,

324
00:13:46,620 --> 00:13:47,810
and breach attack simulation

325
00:13:47,810 --> 00:13:49,569
and security chaos engineering.

326
00:13:49,570 --> 00:13:50,403
It is

327
00:13:52,120 --> 00:13:55,480
one that there's a wide definition
of what those things are.

328
00:13:55,480 --> 00:13:57,240
And, but often pen testing

329
00:13:57,240 --> 00:13:59,280
red-teaming, purple-teaming are

330
00:13:59,280 --> 00:14:04,280
most often done at the top 5%
of the application portfolio.

331
00:14:04,780 --> 00:14:08,780
And they're very all these
things are very valuable.

332
00:14:08,780 --> 00:14:10,400
Do not misconstrue what we're saying.

333
00:14:10,400 --> 00:14:11,233
What we're saying is

334
00:14:11,233 --> 00:14:14,060
there's a difference in approach and value

335
00:14:14,060 --> 00:14:15,449
in that we're not trying to attack it,

336
00:14:15,450 --> 00:14:16,340
we're not trying to get it,

337
00:14:16,340 --> 00:14:19,520
we're not trying to assimilate some kind

338
00:14:19,520 --> 00:14:20,920
of attack in the system.

339
00:14:20,920 --> 00:14:22,860
We are taking into account

340
00:14:22,860 --> 00:14:26,440
the complex nature and
all the interacting parts

341
00:14:26,440 --> 00:14:28,880
of a distributed system
into what we're doing.

342
00:14:28,880 --> 00:14:31,910
And because if you start
sending attack information

343
00:14:31,910 --> 00:14:33,770
you're sending a lot of
information to the system.

344
00:14:33,770 --> 00:14:35,890
It's hard to figure out
what worked, what didn't

345
00:14:35,890 --> 00:14:38,750
with all the noise of
the signal to noise ratio

346
00:14:38,750 --> 00:14:39,730
is quite large.

347
00:14:39,730 --> 00:14:41,650
What we're doing with
Security Chaos Engineering,

348
00:14:41,650 --> 00:14:44,030
and choosing very small perturbations

349
00:14:44,030 --> 00:14:45,630
our fault in the system.

350
00:14:45,630 --> 00:14:48,080
One at a time trying to determine,

351
00:14:48,080 --> 00:14:49,400
did it work with a supposed to?

352
00:14:49,400 --> 00:14:52,160
That way we can look at broader lens

353
00:14:52,160 --> 00:14:54,160
and understand what worked, what didn't,

354
00:14:55,495 --> 00:14:58,403
and then repeat and continually
run the experiments.

355
00:14:59,260 --> 00:15:01,360
And if you're looking for more information

356
00:15:01,360 --> 00:15:03,253
on the differences,

357
00:15:04,250 --> 00:15:06,820
there's just great articles
out there as well as

358
00:15:06,820 --> 00:15:07,653
in the book.

359
00:15:10,220 --> 00:15:11,350
Jamie, back to you.

360
00:15:11,350 --> 00:15:12,183
- All right.

361
00:15:12,183 --> 00:15:13,510
Well, thank you, Aaron.

362
00:15:13,510 --> 00:15:15,819
So there are several companies that are

363
00:15:15,820 --> 00:15:18,750
implementing Security
Chaos Engineering today

364
00:15:18,750 --> 00:15:21,270
and we are continuing to
see that number increase

365
00:15:21,270 --> 00:15:23,439
which is fantastic.

366
00:15:23,440 --> 00:15:25,060
Cardinal Health, where I used to work

367
00:15:25,060 --> 00:15:26,619
until March of this year

368
00:15:26,620 --> 00:15:29,370
was among those that began
implementing the discipline

369
00:15:29,370 --> 00:15:31,030
about two years ago.

370
00:15:31,030 --> 00:15:33,640
And so of the four use
cases that Aaron described

371
00:15:33,640 --> 00:15:36,280
the one that really drove our
adoption and our investment

372
00:15:36,280 --> 00:15:38,480
in the team was security
control validation.

373
00:15:39,315 --> 00:15:41,360
At the end of the day,

374
00:15:41,360 --> 00:15:43,580
we like so many other companies

375
00:15:43,580 --> 00:15:45,910
had really put our faith in the idea

376
00:15:45,910 --> 00:15:48,130
that our tools, our technical designs,

377
00:15:48,130 --> 00:15:51,880
and our technology standards
that we had kept us secure.

378
00:15:51,880 --> 00:15:53,930
And it's not that we
didn't trust our people

379
00:15:53,930 --> 00:15:56,912
but look at the data that
we see year after year.

380
00:15:58,190 --> 00:16:00,140
Our leadership at Cardinal health realized

381
00:16:00,140 --> 00:16:02,939
that we needed a new approach to security.

382
00:16:02,940 --> 00:16:04,900
One that focused on building

383
00:16:04,900 --> 00:16:07,090
and maintaining security controls,

384
00:16:07,090 --> 00:16:09,630
but also validating that
they stayed in place

385
00:16:09,630 --> 00:16:12,120
but they didn't degrade over time.

386
00:16:12,120 --> 00:16:14,750
And also a way to get after that 48%

387
00:16:14,750 --> 00:16:17,323
which we felt was very preventable.

388
00:16:18,600 --> 00:16:21,130
So the summer of 2019
was when Cardinal Health

389
00:16:21,130 --> 00:16:25,500
invested in a team to build
security control validation

390
00:16:25,500 --> 00:16:29,000
and to implement a
Security Chaos Engineering.

391
00:16:29,000 --> 00:16:31,200
So I came from a software
development background

392
00:16:31,200 --> 00:16:32,960
and I was brought in to

393
00:16:32,960 --> 00:16:36,690
build a team of
multi-disciplined engineers

394
00:16:36,690 --> 00:16:39,940
that had diverse technical backgrounds.

395
00:16:39,940 --> 00:16:41,850
And that to us was really key

396
00:16:41,850 --> 00:16:44,333
that everybody came from
a different background.

397
00:16:45,470 --> 00:16:47,730
The reason was that we
wanted to be able to

398
00:16:47,730 --> 00:16:49,450
make good guesses about

399
00:16:49,450 --> 00:16:53,270
where some of our existing
security control gaps were

400
00:16:53,270 --> 00:16:56,210
so that we can prove value right away

401
00:16:56,210 --> 00:16:58,430
start working on our mediations.

402
00:16:58,430 --> 00:17:00,819
And that's why when
the team first launched

403
00:17:00,820 --> 00:17:02,470
we had collective expertise,

404
00:17:02,470 --> 00:17:04,450
not only in software development,

405
00:17:04,450 --> 00:17:06,960
but security architecture,

406
00:17:06,960 --> 00:17:10,800
systems architecture and
design systems, administration

407
00:17:10,800 --> 00:17:14,373
network engineering, cloud
engineering, risk, and privacy.

408
00:17:15,670 --> 00:17:18,510
So a lot of times what
we ended up doing was

409
00:17:18,510 --> 00:17:20,099
we would build some of our own

410
00:17:20,099 --> 00:17:22,810
security control validations ourselves.

411
00:17:22,810 --> 00:17:25,780
A lot of common tools
on the market today have

412
00:17:25,780 --> 00:17:28,139
APIs that are publicly available

413
00:17:28,140 --> 00:17:32,243
that you can actually query
for its configurations.

414
00:17:33,090 --> 00:17:36,189
Other times, we would take
a look at either open source

415
00:17:36,190 --> 00:17:39,250
or commercial office shelf
products if we could not

416
00:17:39,250 --> 00:17:41,163
write a script that met our needs.

417
00:17:42,500 --> 00:17:44,470
But we got to make some
of those really good

418
00:17:44,470 --> 00:17:46,550
build versus buy decisions
to do what was right

419
00:17:46,550 --> 00:17:47,483
for the company.

420
00:17:48,590 --> 00:17:50,159
Ultimately, the end of the day

421
00:17:50,160 --> 00:17:53,690
our goal was to identify
unknown technical security gaps

422
00:17:53,690 --> 00:17:56,330
and partner with the
organization to remediate them

423
00:17:56,330 --> 00:17:58,673
before an adversary found them for us.

424
00:17:59,830 --> 00:18:02,750
I wanna talk a little bit
more about how we did it.

425
00:18:02,750 --> 00:18:05,380
So as exciting as it
would have been to just,

426
00:18:05,380 --> 00:18:08,450
get access to every system
and every project and go nuts

427
00:18:08,450 --> 00:18:10,393
looking for technical security gaps,

428
00:18:12,106 --> 00:18:13,629
we knew that we needed a disciplined

429
00:18:13,630 --> 00:18:16,543
and repeatable process that
accomplished three things.

430
00:18:17,480 --> 00:18:18,750
So first is that

431
00:18:18,750 --> 00:18:21,760
our process had to identify indisputably

432
00:18:21,760 --> 00:18:24,260
critically important deficiencies.

433
00:18:24,260 --> 00:18:28,000
It didn't do us any good
if my team went after

434
00:18:28,000 --> 00:18:30,330
a bunch of gaps that at the end of the day

435
00:18:30,330 --> 00:18:33,210
the business decided wasn't
worth fixing due to the risk

436
00:18:33,210 --> 00:18:35,960
or they weren't a high enough priority.

437
00:18:35,960 --> 00:18:39,490
So we knew we had to
establish some benchmarks

438
00:18:39,490 --> 00:18:42,570
that were relevant to our
company and to our systems.

439
00:18:42,570 --> 00:18:45,419
This way, when we established benchmarks

440
00:18:45,420 --> 00:18:46,900
the gaps that we found,

441
00:18:46,900 --> 00:18:49,720
we wanted identify them
just on an arbitrary basis.

442
00:18:49,720 --> 00:18:52,570
And these benchmarks
wouldn't be just merely

443
00:18:52,570 --> 00:18:54,610
theoretic best practices but

444
00:18:55,860 --> 00:18:59,159
in reality to turned out
to be too lofty to achieve,

445
00:18:59,160 --> 00:19:00,163
so that was one.

446
00:19:01,080 --> 00:19:03,439
Second was that our process
needed to be big enough

447
00:19:03,440 --> 00:19:08,100
to see the big picture of
the technical security gap

448
00:19:08,100 --> 00:19:11,669
in detail where we saw this.

449
00:19:11,670 --> 00:19:14,130
The challenge we had was that previously

450
00:19:14,130 --> 00:19:17,170
frontline engineers in different areas

451
00:19:17,170 --> 00:19:19,330
either hypothesize that we had gaps

452
00:19:19,330 --> 00:19:21,399
or they had seen evidence,

453
00:19:21,400 --> 00:19:23,790
but what they really lacked was that

454
00:19:23,790 --> 00:19:26,740
visibility to how widespread something was

455
00:19:26,740 --> 00:19:30,080
and all of the detail that
was necessary to document it

456
00:19:30,080 --> 00:19:31,699
and drive it to remediation.

457
00:19:31,700 --> 00:19:34,713
And so we knew we needed to
do something differently.

458
00:19:35,770 --> 00:19:38,980
And then finally we
needed to make sure that

459
00:19:38,980 --> 00:19:40,940
whatever technical gaps we identify

460
00:19:40,940 --> 00:19:42,780
and drove to remediation,

461
00:19:42,780 --> 00:19:45,340
weren't just unknowingly
reopened in the future.

462
00:19:45,340 --> 00:19:49,330
We wanted to make sure that
if we were fixing things now

463
00:19:49,330 --> 00:19:52,983
that we didn't find ourselves
in the same place, come later.

464
00:19:54,090 --> 00:19:55,800
So with those goals in mind,

465
00:19:55,800 --> 00:19:58,470
we created a process that we called

466
00:19:58,470 --> 00:20:02,570
continuous verification and
validation or CVB for short.

467
00:20:02,570 --> 00:20:05,310
It's simply put on a regular basis,

468
00:20:05,310 --> 00:20:07,889
we wanted to continuously verify

469
00:20:07,890 --> 00:20:10,260
that our technical security
controls were in place

470
00:20:10,260 --> 00:20:12,760
and validate that they
were implemented correctly.

471
00:20:13,920 --> 00:20:17,020
And that process really
included five steps.

472
00:20:17,020 --> 00:20:18,840
First, we obviously had to identify

473
00:20:18,840 --> 00:20:21,040
which security control
we wanted to validate

474
00:20:21,040 --> 00:20:22,540
which one we were going after.

475
00:20:23,490 --> 00:20:26,290
Second was that we had to
establish the benchmarks

476
00:20:26,290 --> 00:20:28,860
by which gaps would be identified.

477
00:20:28,860 --> 00:20:31,399
So to give authority to our findings,

478
00:20:31,400 --> 00:20:34,410
we largely use the standards
that were set forth by

479
00:20:34,410 --> 00:20:37,490
security architecture and
our Cloud architecture teams

480
00:20:37,490 --> 00:20:40,083
and that were approved
by our senior leadership.

481
00:20:41,010 --> 00:20:45,270
If those standards didn't yet
exist, we still could proceed.

482
00:20:45,270 --> 00:20:48,810
We would still partner with
all our relevant stakeholders

483
00:20:48,810 --> 00:20:52,530
and start to brainstorm what
our recommendations would be.

484
00:20:52,530 --> 00:20:54,740
And after that, we would socialize those.

485
00:20:54,740 --> 00:20:56,830
And so what was key about that

486
00:20:56,830 --> 00:21:00,800
was that we started to set the requirement

487
00:21:00,800 --> 00:21:02,899
that if we found gaps,

488
00:21:02,900 --> 00:21:05,650
either those had to be remediated,

489
00:21:05,650 --> 00:21:08,140
or we needed to get
visibility to our leadership

490
00:21:08,140 --> 00:21:09,940
so that they could accept the risk.

491
00:21:09,940 --> 00:21:13,680
But this gave us a treasure trove of data

492
00:21:13,680 --> 00:21:16,943
that could help move the
needle on our security posture.

493
00:21:18,310 --> 00:21:20,879
The third step was when
we got to do the fun part

494
00:21:20,880 --> 00:21:22,690
the Security Chaos Engineering part

495
00:21:22,690 --> 00:21:25,250
of implementing the continuous

496
00:21:25,250 --> 00:21:27,650
checks to make sure that
our systems were adhering

497
00:21:27,650 --> 00:21:29,560
to our benchmarks.

498
00:21:29,560 --> 00:21:34,260
Again, either we would write
our own scripts using APIs,

499
00:21:34,260 --> 00:21:37,673
or we would look for tooling of some kind.

500
00:21:38,730 --> 00:21:41,930
After that we created a
dashboard that illustrated

501
00:21:41,930 --> 00:21:45,010
the real time compliance
with our benchmarks.

502
00:21:45,010 --> 00:21:48,190
And this was really key
because it did two things.

503
00:21:48,190 --> 00:21:50,660
So one is it, let us on demand,

504
00:21:50,660 --> 00:21:53,773
pull up have an understanding
of where we stood

505
00:21:53,773 --> 00:21:57,679
with that security control
across the enterprise,

506
00:21:57,680 --> 00:22:00,250
and second is it actually fed some reports

507
00:22:00,250 --> 00:22:02,310
that we would show to our
senior leaders so that

508
00:22:02,310 --> 00:22:05,250
they were aware of what our
real security posture was

509
00:22:05,250 --> 00:22:08,423
and they could make decisions
that were based in real data.

510
00:22:09,660 --> 00:22:11,180
And then finally, in our process,

511
00:22:11,180 --> 00:22:13,810
if at any point we had
identified that adherence

512
00:22:13,810 --> 00:22:15,429
to our benchmarks decreased,

513
00:22:15,430 --> 00:22:18,550
that's when we would place an
issue in our risk register.

514
00:22:18,550 --> 00:22:21,169
And we had some really
good governance processes

515
00:22:21,170 --> 00:22:23,133
that would drive that to remediation.

516
00:22:24,400 --> 00:22:26,970
So that's that process in a nutshell

517
00:22:26,970 --> 00:22:28,610
but the beauty of it was that

518
00:22:30,102 --> 00:22:32,600
this process wasn't just done in a vacuum.

519
00:22:32,600 --> 00:22:35,850
It wasn't fun and experimentation
just for the sake of fun

520
00:22:35,850 --> 00:22:37,459
and experimentation,

521
00:22:37,460 --> 00:22:40,000
Security Chaos Engineering became

522
00:22:40,000 --> 00:22:44,700
a means to drive real
security posture enhancement.

523
00:22:44,700 --> 00:22:47,550
So what we did was we fit
ourselves into a process

524
00:22:47,550 --> 00:22:50,740
where once we identified
mistakes or misconfigurations

525
00:22:50,740 --> 00:22:54,780
we would partner with
the relevant IP teams

526
00:22:54,780 --> 00:22:58,790
and with the enterprise to get
a remediation plan in place

527
00:22:58,790 --> 00:23:00,960
and to actually execute on that.

528
00:23:00,960 --> 00:23:02,970
And then after that not only did our

529
00:23:02,970 --> 00:23:05,060
security posture increase

530
00:23:05,060 --> 00:23:07,100
but then we had that continuous monitoring

531
00:23:07,100 --> 00:23:09,290
in place afterwards to make sure that

532
00:23:09,290 --> 00:23:11,993
our security controls
didn't degrade over time.

533
00:23:13,060 --> 00:23:14,300
So for Cardinal Health,

534
00:23:14,300 --> 00:23:17,810
Security Chaos Engineering
became the start to creating

535
00:23:17,810 --> 00:23:21,023
long-term competence in
our security posture.

536
00:23:22,320 --> 00:23:24,159
So that is the Cardinal Health story.

537
00:23:24,160 --> 00:23:26,850
So, Aaron, I'm gonna turn it over to you,

538
00:23:26,850 --> 00:23:28,663
to talk about your work with ChaoSlingr.

539
00:23:29,672 --> 00:23:30,505
- Thank you, Jamie.

540
00:23:30,505 --> 00:23:32,410
I always love hearing the
Cardinal Health story.

541
00:23:32,410 --> 00:23:35,053
It's such a great story of transformation.

542
00:23:36,440 --> 00:23:39,030
So ChaoSlingr was the
first open source tool

543
00:23:39,030 --> 00:23:40,300
for UnitedHealth Group.

544
00:23:40,300 --> 00:23:43,030
It was actually the first ever sort of

545
00:23:43,030 --> 00:23:44,980
implementation of
Netflix's Chaos Engineering

546
00:23:44,980 --> 00:23:46,490
to cyber security.

547
00:23:46,490 --> 00:23:50,603
And so the main example
ChaoSlingr when we released it was

548
00:23:50,603 --> 00:23:52,620
what we call PortSlingr.

549
00:23:52,620 --> 00:23:53,540
I would need an example that

550
00:23:53,540 --> 00:23:54,810
kind of everybody could understand

551
00:23:54,810 --> 00:23:59,010
and misconfigured or unauthorized
port changes are a common

552
00:23:59,010 --> 00:24:00,390
thing even today.

553
00:24:00,390 --> 00:24:02,353
And most people kind of
understand what a firewall is,

554
00:24:02,353 --> 00:24:05,730
and kind of what you
do in those processes.

555
00:24:05,730 --> 00:24:08,480
So our expectation was

556
00:24:08,480 --> 00:24:12,040
with PortSlingr was what a
misconfigured port would be

557
00:24:13,630 --> 00:24:15,170
inserted into the system is

558
00:24:15,170 --> 00:24:17,550
that our firewall would immediately detect

559
00:24:17,550 --> 00:24:21,320
and block the change would be a non issue.

560
00:24:21,320 --> 00:24:22,570
Well, what we were finding

561
00:24:22,570 --> 00:24:26,480
so we built chaos layer to
actually proactively inject

562
00:24:26,480 --> 00:24:27,970
either an open or closed port

563
00:24:27,970 --> 00:24:30,480
where that wasn't already open or closed

564
00:24:30,480 --> 00:24:33,760
into our Amazon, each two security groups.

565
00:24:33,760 --> 00:24:35,700
And what was interesting was is

566
00:24:35,700 --> 00:24:36,960
that the firewall only detects

567
00:24:36,960 --> 00:24:39,420
and locked into my 60% of the time.

568
00:24:39,420 --> 00:24:41,490
And what's support remember is that

569
00:24:41,490 --> 00:24:44,470
there was no issue to
war room, no incident.

570
00:24:44,470 --> 00:24:45,990
We were able to proactively

571
00:24:48,228 --> 00:24:49,370
do this and learn that,

572
00:24:49,370 --> 00:24:51,879
oh, there was a drift issue
between our non-commercial

573
00:24:51,880 --> 00:24:54,260
and commercial software environments.

574
00:24:54,260 --> 00:24:56,410
Okay, let's we fixed it non-issue 'cause

575
00:24:56,410 --> 00:24:57,317
no customer was impacted because of this

576
00:24:57,317 --> 00:25:00,220
'cause we fix it before
it became a problem.

577
00:25:00,220 --> 00:25:01,140
The second thing we learned was that

578
00:25:01,140 --> 00:25:02,530
the configuration management tool,

579
00:25:02,530 --> 00:25:04,180
the Cloud native one that we were using

580
00:25:04,180 --> 00:25:05,610
caught (indistinct) worked it every time.

581
00:25:05,610 --> 00:25:07,800
So it was like, "Wow, we
didn't expect that to happen."

582
00:25:07,800 --> 00:25:09,220
So that was the second learning.

583
00:25:09,220 --> 00:25:12,550
The third learning was is that
we had a homegrown kind of

584
00:25:12,550 --> 00:25:15,450
think of it as like a SIM
where we built our own tool

585
00:25:15,450 --> 00:25:17,850
for like security logging.

586
00:25:17,850 --> 00:25:20,060
And I was not very confident

587
00:25:20,060 --> 00:25:22,290
that we're actually gonna
be able to derive alerts

588
00:25:22,290 --> 00:25:24,220
'cause we were going into
the Cloud at the time.

589
00:25:24,220 --> 00:25:25,110
But that actually happened.

590
00:25:25,110 --> 00:25:26,389
We actually drove an alert

591
00:25:26,390 --> 00:25:28,880
to the SOC, the SOC got the alert,

592
00:25:28,880 --> 00:25:30,000
but the analyst didn't know

593
00:25:30,000 --> 00:25:31,447
which shady booths account it came from,

594
00:25:31,447 --> 00:25:34,784
and know what account came from now.

595
00:25:34,785 --> 00:25:35,717
It is as an engineer, you can say,

596
00:25:35,717 --> 00:25:37,340
"Well, can map back to IP address

597
00:25:37,340 --> 00:25:38,330
and find out where it came."

598
00:25:38,330 --> 00:25:40,010
Yeah, you can, but that
can take 15 minutes.

599
00:25:40,010 --> 00:25:42,129
It could take 30 minutes, right?

600
00:25:42,130 --> 00:25:46,520
If ESnet is applied you could
take longer a lot longer.

601
00:25:46,520 --> 00:25:51,300
And if one minute of downtime
is $10,000, a million dollars,

602
00:25:51,300 --> 00:25:52,899
that gets very expensive.

603
00:25:52,900 --> 00:25:53,770
But guess what?

604
00:25:53,770 --> 00:25:57,517
There was no loss, we
are the product we say,

605
00:25:57,517 --> 00:25:58,460
"Hey, all we have to do is

606
00:25:58,460 --> 00:26:01,800
add metadata pointers to
the alert and we're good.

607
00:26:01,800 --> 00:26:03,409
Then we'll know like, it's like,

608
00:26:03,410 --> 00:26:05,060
we fixed all these things

609
00:26:05,060 --> 00:26:07,010
because we're able to learn proactively

610
00:26:07,850 --> 00:26:11,689
and not have to respond and
react to an ongoing problem.

611
00:26:11,690 --> 00:26:13,230
We're able to fix it
before it came a problem."

612
00:26:13,230 --> 00:26:14,710
That's the key part of this.

613
00:26:14,710 --> 00:26:17,040
And one of the things I'm
gonna add to this is that,

614
00:26:17,040 --> 00:26:19,909
we weren't simply reading a
config in what we're doing

615
00:26:19,910 --> 00:26:21,310
with Jamie and I are talking about,

616
00:26:21,310 --> 00:26:22,550
we're exercising it.

617
00:26:22,550 --> 00:26:24,780
Which is a big difference.

618
00:26:24,780 --> 00:26:27,370
- So as you can see Aaron
and I obviously believe

619
00:26:27,370 --> 00:26:30,850
very strongly in Security
Chaos Engineering.

620
00:26:30,850 --> 00:26:32,510
And we believe that the more companies

621
00:26:32,510 --> 00:26:33,730
that adopt this practice,

622
00:26:33,730 --> 00:26:36,330
the more resilient all
of us are going to be

623
00:26:36,330 --> 00:26:39,710
as we'll be proactively
identifying security

624
00:26:39,710 --> 00:26:43,970
and reliability issues
before they become incidents.

625
00:26:43,970 --> 00:26:46,110
So we'd like to turn the conversation

626
00:26:46,110 --> 00:26:49,010
at this point into how
everyone in the audience can

627
00:26:49,010 --> 00:26:50,243
implement this practice.

628
00:26:51,270 --> 00:26:53,170
So first, if you're
thinking that what we're

629
00:26:53,170 --> 00:26:57,080
discussing is just too cutting
edge for your organization.

630
00:26:57,080 --> 00:26:59,780
First, I wanna encourage you
to adjust your mindset here

631
00:26:59,780 --> 00:27:02,470
and think about this almost as you would

632
00:27:02,470 --> 00:27:04,430
just like standard testing.

633
00:27:04,430 --> 00:27:09,200
Yes, there is an experimentation
element to it too.

634
00:27:09,200 --> 00:27:11,920
But for the sake of adopting
and moving towards it,

635
00:27:11,920 --> 00:27:14,600
let's first think in that testing lens.

636
00:27:14,600 --> 00:27:17,040
So with my background in
normal software development

637
00:27:17,040 --> 00:27:19,810
what really attracted me to
Security Chaos Engineering

638
00:27:19,810 --> 00:27:22,669
in the first place was
that the parallels between

639
00:27:22,670 --> 00:27:24,750
Security Chaos Engineering and testing

640
00:27:24,750 --> 00:27:26,543
were just too profound to ignore.

641
00:27:27,440 --> 00:27:30,500
So you can think about
Security Chaos Engineering,

642
00:27:30,500 --> 00:27:33,240
or think about the
Cardinal Health CVV process

643
00:27:33,240 --> 00:27:36,270
as basically the systems level equivalent

644
00:27:36,270 --> 00:27:39,930
to a regularly scheduled
automated test suite.

645
00:27:39,930 --> 00:27:42,270
It's just that instead of verifying that

646
00:27:42,270 --> 00:27:44,350
your code meets the
functional requirements

647
00:27:44,350 --> 00:27:45,389
of the system,

648
00:27:45,390 --> 00:27:49,050
your testing that the
system meets its security

649
00:27:49,050 --> 00:27:52,060
and its reliability requirements.

650
00:27:52,060 --> 00:27:56,050
So both sets of tests, you
can run on a continuous basis.

651
00:27:56,050 --> 00:27:58,720
You can run them and prod
or non-prod depending

652
00:27:58,720 --> 00:28:00,360
on your scenario.

653
00:28:00,360 --> 00:28:02,129
But you could also implement it

654
00:28:02,130 --> 00:28:05,500
even in like a test
driven development model.

655
00:28:05,500 --> 00:28:08,120
As new systems are coming on board,

656
00:28:08,120 --> 00:28:10,969
you can start to identify an engineer,

657
00:28:10,970 --> 00:28:13,060
what experiments what you're going to run.

658
00:28:13,060 --> 00:28:16,100
And as that system matures
and you start to see

659
00:28:16,100 --> 00:28:17,669
your experiments passing

660
00:28:17,670 --> 00:28:19,720
and giving you the results that you want,

661
00:28:20,710 --> 00:28:22,920
that's how you know that the security

662
00:28:22,920 --> 00:28:25,453
and resilience requirements
are going to be met.

663
00:28:26,790 --> 00:28:28,930
And so when you think about it this way

664
00:28:28,930 --> 00:28:32,260
and you think about it kinda
from that testing lens,

665
00:28:32,260 --> 00:28:33,770
Security Chaos Engineering

666
00:28:33,770 --> 00:28:37,570
ceases to be that esoteric practice that's

667
00:28:38,406 --> 00:28:42,562
too cutting edge for a
modern enterprise to adopt.

668
00:28:43,630 --> 00:28:46,213
And it really becomes
logical and practical.

669
00:28:47,320 --> 00:28:50,530
I like to think about my
days as a software developer

670
00:28:50,530 --> 00:28:53,430
because I never would have shipped code

671
00:28:53,430 --> 00:28:55,780
but didn't go through a slew of testing.

672
00:28:55,780 --> 00:28:58,490
I would have written my own tests,

673
00:28:58,490 --> 00:29:00,520
that exercise that functionality,

674
00:29:00,520 --> 00:29:02,100
I would have made sure that all the tests

675
00:29:02,100 --> 00:29:04,510
and my CIC pipeline passed,

676
00:29:04,510 --> 00:29:07,360
I would test it manually or even

677
00:29:07,360 --> 00:29:10,209
in a automated way in
a non-prod environment.

678
00:29:10,210 --> 00:29:13,030
And then even after I
deployed the code to prod,

679
00:29:13,030 --> 00:29:16,350
I would smoke test and
do what was safe there.

680
00:29:16,350 --> 00:29:17,860
And

681
00:29:17,860 --> 00:29:20,000
that became a standard practice.

682
00:29:20,000 --> 00:29:21,110
If you think about,

683
00:29:21,110 --> 00:29:24,120
and what really made me
wonder when I first joined

684
00:29:24,120 --> 00:29:27,000
Security Chaos Engineering and
started implementing it was

685
00:29:27,000 --> 00:29:29,820
why should the systems
world be any different?

686
00:29:29,820 --> 00:29:33,929
Why should we not go through
those same types of tests?

687
00:29:33,930 --> 00:29:36,130
And so, as I see it just like the world of

688
00:29:36,130 --> 00:29:39,250
software development first
adopted these practices,

689
00:29:39,250 --> 00:29:42,443
the systems engineering world
is going have to do the same.

690
00:29:44,020 --> 00:29:45,830
So once you've gotten past that mindset

691
00:29:45,830 --> 00:29:47,110
and you're ready to experiment

692
00:29:47,110 --> 00:29:49,649
it really is possible to start small.

693
00:29:49,650 --> 00:29:51,050
I think one of the biggest myths about

694
00:29:51,050 --> 00:29:53,669
Security Chaos Engineering
is that you need this

695
00:29:53,670 --> 00:29:56,720
super complex system wide

696
00:29:56,720 --> 00:29:58,660
highly disruptive experiment

697
00:29:58,660 --> 00:30:00,930
that you wreak havoc in production.

698
00:30:00,930 --> 00:30:03,790
And therefore you need to
have a business case needs

699
00:30:03,790 --> 00:30:06,353
VP approval and all of those other things.

700
00:30:08,177 --> 00:30:11,570
That's a myth you can start
a lot smaller than that.

701
00:30:11,570 --> 00:30:14,260
So let's reflect on what
Aaron had said earlier.

702
00:30:14,260 --> 00:30:15,650
Some of the primary use cases

703
00:30:15,650 --> 00:30:19,100
for Security Chaos Engineering
include incident response

704
00:30:19,100 --> 00:30:21,399
security control validation,

705
00:30:21,400 --> 00:30:23,750
compliance and observability.

706
00:30:23,750 --> 00:30:25,200
You can test those

707
00:30:25,200 --> 00:30:29,950
without inflicting mass
chaos on your own company.

708
00:30:29,950 --> 00:30:33,560
So for example, say
you have a control that

709
00:30:33,560 --> 00:30:35,649
watches for public storage buckets

710
00:30:35,650 --> 00:30:37,293
and closes them automatically.

711
00:30:38,220 --> 00:30:40,440
Go ahead and perform that experiment,

712
00:30:40,440 --> 00:30:43,880
open up a public bucket
and see what happens.

713
00:30:43,880 --> 00:30:47,740
Do your controls close it
like you think that it should,

714
00:30:47,740 --> 00:30:48,573
what's the timeframe?

715
00:30:48,573 --> 00:30:50,763
Does that match what you have documented?

716
00:30:52,320 --> 00:30:55,290
Say you have a detection
in place that discovers

717
00:30:55,290 --> 00:30:57,250
rogue access points.

718
00:30:57,250 --> 00:30:59,270
What happens if you
connect one to the network?

719
00:30:59,270 --> 00:31:02,580
One that's clean that you
own, you know is safe.

720
00:31:02,580 --> 00:31:04,490
But does it get detected?

721
00:31:04,490 --> 00:31:06,710
Is there enough information
available in the logs

722
00:31:06,710 --> 00:31:09,253
so that a human can see
it and respond to it?

723
00:31:10,410 --> 00:31:11,640
If you have a playbook

724
00:31:11,640 --> 00:31:14,650
for when an unapproved
firewalls rule is open

725
00:31:14,650 --> 00:31:16,870
or when any protection systems

726
00:31:16,870 --> 00:31:19,600
stop being configured to protect,

727
00:31:19,600 --> 00:31:20,909
go ahead and do this.

728
00:31:20,910 --> 00:31:22,400
Now, do it carefully

729
00:31:22,400 --> 00:31:24,630
in an environment that's
pretty low risk to start

730
00:31:24,630 --> 00:31:27,090
and make sure to revert what you did

731
00:31:27,090 --> 00:31:30,149
but test that you see
this happen in your logs,

732
00:31:30,150 --> 00:31:32,770
test that an alert actually does fire,

733
00:31:32,770 --> 00:31:36,220
test that somebody
actually responds to it.

734
00:31:36,220 --> 00:31:39,540
All of those are security
chaos experiments.

735
00:31:39,540 --> 00:31:40,530
And if you're thinking that

736
00:31:40,530 --> 00:31:43,510
even that experimentation
is a little bit too advanced

737
00:31:43,510 --> 00:31:47,040
for your company and your
company's appetite just yet,

738
00:31:47,040 --> 00:31:50,399
then start to do something
even lower impact.

739
00:31:50,400 --> 00:31:51,910
You could do something very similar

740
00:31:51,910 --> 00:31:54,389
to the Cardinal Health CVV process.

741
00:31:54,389 --> 00:31:58,580
So just start by querying
your security configurations

742
00:31:58,580 --> 00:32:00,260
and figure out if they match

743
00:32:00,260 --> 00:32:02,700
what you believe that they should.

744
00:32:02,700 --> 00:32:04,790
So, for example if you have

745
00:32:04,790 --> 00:32:07,379
endpoint agents that are deployed to all

746
00:32:07,380 --> 00:32:12,000
of your employees laptops,
are they deployed everywhere?

747
00:32:12,000 --> 00:32:15,000
Are they actually running
and in a healthy state?

748
00:32:15,000 --> 00:32:16,890
Are they on the right version or at least,

749
00:32:16,890 --> 00:32:17,833
and minus one?

750
00:32:18,830 --> 00:32:20,399
If they have security policies,

751
00:32:20,400 --> 00:32:22,020
are they the policies that you believe

752
00:32:22,020 --> 00:32:23,153
shouldn't be in place?

753
00:32:24,460 --> 00:32:27,290
These types of mini experiments at least

754
00:32:27,290 --> 00:32:30,879
peek at what's out there and
have virtually no impact.

755
00:32:30,880 --> 00:32:33,463
So they're great places to get started.

756
00:32:35,080 --> 00:32:36,730
So where do you begin?

757
00:32:36,730 --> 00:32:39,530
First things first, Aaron
and I have both talking.

758
00:32:39,530 --> 00:32:42,760
There is a brand new O'Reilly report,

759
00:32:42,760 --> 00:32:44,930
that was published this past fall

760
00:32:44,930 --> 00:32:47,790
all about Security Chaos
Engineering and the link is here.

761
00:32:47,790 --> 00:32:51,180
The eBook is free, so go
ahead and download that.

762
00:32:51,180 --> 00:32:56,180
It was written jointly by
Aaron and also Kelly Shortridg.

763
00:32:56,570 --> 00:32:58,439
And it covers not only the basics

764
00:32:58,440 --> 00:33:00,600
but it has additional information

765
00:33:00,600 --> 00:33:02,590
on the Cardinal Health use case,

766
00:33:02,590 --> 00:33:05,030
and has a ton of contributing authors.

767
00:33:05,030 --> 00:33:06,730
So you can hear stories from

768
00:33:06,730 --> 00:33:08,220
other really, really smart people

769
00:33:08,220 --> 00:33:10,540
at companies like Verika and Google

770
00:33:10,540 --> 00:33:13,835
and Capital One has a great
case study in there as well.

771
00:33:13,835 --> 00:33:16,250
So it is really a wealth of knowledge,

772
00:33:16,250 --> 00:33:17,780
it's only 90 pages long.

773
00:33:17,780 --> 00:33:20,420
So it's a short read, it's a great read.

774
00:33:20,420 --> 00:33:23,233
Read through that and you
will understand the basics.

775
00:33:24,390 --> 00:33:25,780
Next, after that

776
00:33:25,780 --> 00:33:28,780
start to look for areas
in your security programs

777
00:33:28,780 --> 00:33:30,860
where competence is critical

778
00:33:30,860 --> 00:33:33,159
and start thinking through
how you would instrument

779
00:33:33,160 --> 00:33:35,240
experiments there.

780
00:33:35,240 --> 00:33:38,220
Maybe even start implementing
a couple of experiments,

781
00:33:38,220 --> 00:33:39,670
maybe using some of the examples

782
00:33:39,670 --> 00:33:43,350
on the previous slide or
anything that's really low risk.

783
00:33:43,350 --> 00:33:46,770
I do recommend starting low
risks that you get the feel

784
00:33:46,770 --> 00:33:50,050
for the discipline and
how do you instrument,

785
00:33:50,050 --> 00:33:52,060
how do you set up an experiment

786
00:33:52,990 --> 00:33:56,120
and don't run the risk
of holding your progress

787
00:33:56,120 --> 00:33:58,239
before you really even get started,

788
00:33:58,240 --> 00:34:02,380
but learn the basics and you'll
be able to grow from there.

789
00:34:02,380 --> 00:34:04,230
After just a couple of experiments,

790
00:34:04,230 --> 00:34:05,670
I can almost guarantee you that

791
00:34:05,670 --> 00:34:07,050
you are going to have findings,

792
00:34:07,050 --> 00:34:08,620
and that you are going to learn new things

793
00:34:08,620 --> 00:34:11,960
about the way that your
systems actually function.

794
00:34:11,960 --> 00:34:12,940
And that's great.

795
00:34:12,940 --> 00:34:15,679
It's great to learn now,
like Aaron said earlier

796
00:34:15,679 --> 00:34:17,560
in a time where you're not really worried

797
00:34:17,560 --> 00:34:19,239
about being named, blamed, or shamed

798
00:34:19,239 --> 00:34:20,572
and you can be proactive.

799
00:34:21,639 --> 00:34:23,830
So once you have those learnings

800
00:34:23,830 --> 00:34:26,810
that's gonna let you do
things like prove value in

801
00:34:26,810 --> 00:34:28,710
Security Chaos Engineering

802
00:34:28,710 --> 00:34:32,400
maybe create a business
case to expand the practice.

803
00:34:32,400 --> 00:34:34,880
Maybe it'll even get you
some additional access

804
00:34:34,880 --> 00:34:37,270
to do some experiments
in other areas where

805
00:34:37,270 --> 00:34:40,540
you wouldn't normally be
able to have that access.

806
00:34:40,540 --> 00:34:43,029
And at that point, you can
start to increase the breadth

807
00:34:43,030 --> 00:34:45,570
and your complexity of
your experiments as well.

808
00:34:48,052 --> 00:34:48,885
And there you have it.

809
00:34:48,885 --> 00:34:50,060
So as you can see,

810
00:34:50,060 --> 00:34:52,610
Security Chaos Engineering is a field that

811
00:34:52,610 --> 00:34:55,370
it may seem esoteric and out there

812
00:34:55,370 --> 00:34:57,870
but at its core, it's really logical.

813
00:34:57,870 --> 00:34:59,759
And it's really simple.

814
00:34:59,760 --> 00:35:02,180
Instead of allowing the vast complexity

815
00:35:02,180 --> 00:35:05,069
of our systems to overwhelm us and create

816
00:35:05,070 --> 00:35:07,230
so many opportunities for accidents,

817
00:35:07,230 --> 00:35:09,520
mistakes, and misconfigurations,.

818
00:35:09,520 --> 00:35:12,640
really the answer is to
flip the model on its head,

819
00:35:12,640 --> 00:35:15,100
instead of relying on our documentation

820
00:35:15,100 --> 00:35:16,980
and our preconceived notions,

821
00:35:16,980 --> 00:35:20,230
we can go in eyes wide
open, ready to learn,

822
00:35:20,230 --> 00:35:23,810
ready to experiment and
learn from our systems

823
00:35:23,810 --> 00:35:25,480
and see what they really are,

824
00:35:25,480 --> 00:35:27,200
and not what we hope they would be

825
00:35:27,200 --> 00:35:31,520
or what the pictures in our
documentation say that they are.

826
00:35:31,520 --> 00:35:33,970
And through that testing
and that experimentation

827
00:35:34,830 --> 00:35:38,259
we get to identify opportunities to

828
00:35:38,260 --> 00:35:42,230
proactively secure and stabilize
our systems and get us away

829
00:35:42,230 --> 00:35:44,320
from the reactionary fire drills

830
00:35:44,320 --> 00:35:46,733
and the engineering
practices we have today.

831
00:35:48,270 --> 00:35:49,320
In case you missed it,

832
00:35:49,320 --> 00:35:51,460
here's a link to the book one last time

833
00:35:51,460 --> 00:35:55,070
it's verica.io/sce-book,

834
00:35:55,070 --> 00:35:56,303
and it is free.

835
00:35:57,140 --> 00:35:59,779
And we really hope that this
session was valuable for you.

836
00:35:59,780 --> 00:36:01,360
And that you learned something.

837
00:36:01,360 --> 00:36:03,230
Here, you're gonna find
our contact information

838
00:36:03,230 --> 00:36:05,740
if you wanna continue the conversation.

839
00:36:05,740 --> 00:36:08,000
Thank you again to RSA for hosting us.

840
00:36:08,000 --> 00:36:10,330
This has been a fantastic experience.

841
00:36:10,330 --> 00:36:12,750
We always love coming here

842
00:36:12,750 --> 00:36:15,150
and thank you to everybody
who listened live

843
00:36:15,150 --> 00:36:17,230
and enjoy the rest of the conference.

