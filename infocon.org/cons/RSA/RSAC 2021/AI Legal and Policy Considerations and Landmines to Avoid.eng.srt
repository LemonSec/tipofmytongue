1
00:00:01,340 --> 00:00:02,180
- Hello everyone.

2
00:00:02,180 --> 00:00:04,810
I'm Behnam Dayanim with Paul Hastings,

3
00:00:04,810 --> 00:00:06,979
and I'm really happy to be here today,

4
00:00:06,980 --> 00:00:10,530
to talk about AI, Legal
and Policy Considerations

5
00:00:10,530 --> 00:00:12,469
and Landmines to Avoid.

6
00:00:12,470 --> 00:00:15,150
Before I begin, and lest I forget,

7
00:00:15,150 --> 00:00:16,810
I want to make sure everyone knows

8
00:00:16,810 --> 00:00:18,070
that you can leave questions

9
00:00:18,070 --> 00:00:19,960
in the question chat box

10
00:00:19,960 --> 00:00:21,690
and I'd be happy to answer them

11
00:00:21,690 --> 00:00:23,880
or at the end of the presentation.

12
00:00:23,880 --> 00:00:25,259
Okay, so where do we go?

13
00:00:25,260 --> 00:00:26,780
First question is what's the big deal?

14
00:00:26,780 --> 00:00:29,880
Why do we care about AI
and landmines to avoid?

15
00:00:29,880 --> 00:00:31,509
Well, as I'm sure all of you know

16
00:00:31,510 --> 00:00:34,230
you wouldn't be here today,
listening to this session.

17
00:00:34,230 --> 00:00:36,599
We have an increasing
reliance on machine learning

18
00:00:36,600 --> 00:00:40,730
and Artificial Intelligence
in all aspects of our lives.

19
00:00:40,730 --> 00:00:42,620
Here on the slide are just a few credit,

20
00:00:42,620 --> 00:00:44,800
financial benefits and decision-making,

21
00:00:44,800 --> 00:00:48,160
employment decisions,
transportation and others.

22
00:00:48,160 --> 00:00:51,370
And so as we increasingly
rely on these types

23
00:00:52,340 --> 00:00:54,320
on automated decisions
in these types of areas,

24
00:00:54,320 --> 00:00:56,870
it becomes increasingly
important to understand

25
00:00:56,870 --> 00:00:58,010
how we're doing that

26
00:00:58,010 --> 00:01:02,260
and what the various considerations
costs benefits might be.

27
00:01:02,260 --> 00:01:06,570
So first though, it's worth
asking ourselves what is AI?

28
00:01:06,570 --> 00:01:09,419
And I took these definition on this slide

29
00:01:09,420 --> 00:01:11,940
the quite lengthy set of
definitions on this slide

30
00:01:11,940 --> 00:01:15,240
from the future of
artificial intelligence act

31
00:01:15,240 --> 00:01:16,360
which is pending in congress.

32
00:01:16,360 --> 00:01:17,770
And we'll talk about a little bit later

33
00:01:17,770 --> 00:01:19,720
because I think it really is a useful way

34
00:01:19,720 --> 00:01:21,450
to frame the issue.

35
00:01:21,450 --> 00:01:24,530
It defines an AI initially as,

36
00:01:24,530 --> 00:01:26,720
artificial systems that perform tasks

37
00:01:26,720 --> 00:01:28,810
under a variety of circumstances

38
00:01:28,810 --> 00:01:31,050
without significant human oversight

39
00:01:31,050 --> 00:01:33,160
or that can learn from their experience.

40
00:01:33,160 --> 00:01:38,080
And in general, the more
human-like the intelligence,

41
00:01:38,080 --> 00:01:39,610
the more it can be said to be used

42
00:01:39,610 --> 00:01:42,290
or it could be said to use
Artificial Intelligence.

43
00:01:42,290 --> 00:01:43,180
Later on the statue,

44
00:01:43,180 --> 00:01:46,120
the bill says, systems
that think like humans,

45
00:01:46,120 --> 00:01:48,600
systems that act like humans.

46
00:01:48,600 --> 00:01:50,869
A set of techniques
that seek to approximate

47
00:01:50,870 --> 00:01:52,690
some cognitive task.

48
00:01:52,690 --> 00:01:55,080
So systems that act rationally.

49
00:01:55,080 --> 00:01:58,520
Perhaps implying that thinking like humans

50
00:01:58,520 --> 00:02:00,190
somehow equates to acting rationally,

51
00:02:00,190 --> 00:02:01,880
which may or may not be true in real life,

52
00:02:01,880 --> 00:02:03,470
but that's how the statue,

53
00:02:03,470 --> 00:02:06,520
the bill conceived of
Artificial Intelligence.

54
00:02:06,520 --> 00:02:07,660
And it also talks about

55
00:02:07,660 --> 00:02:10,710
general Artificial
Intelligence versus narrow,

56
00:02:10,710 --> 00:02:12,530
and that's also an important concept.

57
00:02:12,530 --> 00:02:17,170
General AI, is a notional
future AI system,

58
00:02:17,170 --> 00:02:19,690
that exhibits apparently
intelligent behavior

59
00:02:19,690 --> 00:02:21,680
at least as advanced as a person.

60
00:02:21,680 --> 00:02:23,850
Again, the human reference point,

61
00:02:23,850 --> 00:02:25,109
I'm gonna come back to that.

62
00:02:25,110 --> 00:02:27,260
Across the range of cognitive emotional

63
00:02:27,260 --> 00:02:28,530
and social behaviors.

64
00:02:28,530 --> 00:02:29,900
Whereas a narrow AI,

65
00:02:29,900 --> 00:02:33,130
is one that addresses
specific applications,

66
00:02:33,130 --> 00:02:35,960
such as playing games, driving a car,

67
00:02:35,960 --> 00:02:39,710
recognizing a face, very discrete tasks.

68
00:02:39,710 --> 00:02:43,220
Okay, so as I've already
emphasized a couple of times,

69
00:02:43,220 --> 00:02:45,200
there seems to be an equation

70
00:02:45,200 --> 00:02:48,769
between intelligence and human, right.

71
00:02:48,770 --> 00:02:52,380
We want our AI system
to be as good as humans

72
00:02:52,380 --> 00:02:53,519
or like humans.

73
00:02:53,520 --> 00:02:55,550
So is that really what we want?

74
00:02:55,550 --> 00:02:57,320
Right, or do we actually
be on something better?

75
00:02:57,320 --> 00:02:59,090
And I think in thinking about that,

76
00:02:59,090 --> 00:03:01,790
the example of self-driving
vehicles is a great one.

77
00:03:01,790 --> 00:03:06,102
Because when we think
about self-driving cars,

78
00:03:07,100 --> 00:03:09,920
no one is promoting self-driving vehicles,

79
00:03:09,920 --> 00:03:12,929
on the premise that they will
be as safe as human drivers.

80
00:03:12,930 --> 00:03:15,120
In fact, if they were
as same as human drivers

81
00:03:15,120 --> 00:03:18,301
I think no one would have
interest in self-driving vehicles.

82
00:03:18,301 --> 00:03:21,510
It's sort of like the
backseat driver phenomenon.

83
00:03:21,510 --> 00:03:22,560
When you sit in the back seat,

84
00:03:22,560 --> 00:03:23,590
you're always a better driver

85
00:03:23,590 --> 00:03:24,650
than the person in front of you

86
00:03:24,650 --> 00:03:26,810
because you have that perhaps illusory

87
00:03:26,810 --> 00:03:30,370
but nonetheless perceived
feeling of control

88
00:03:30,370 --> 00:03:31,480
because you're the one at the wheel,

89
00:03:31,480 --> 00:03:32,579
whereas when you're in the backseat,

90
00:03:32,580 --> 00:03:34,630
you're depending upon that other person.

91
00:03:34,630 --> 00:03:38,030
Similarly, if AI could just
drive as well as we could,

92
00:03:38,030 --> 00:03:39,600
we would not tolerate that.

93
00:03:39,600 --> 00:03:41,950
In fact, AI is being sold in the context

94
00:03:41,950 --> 00:03:43,299
of self-driving cars,

95
00:03:43,300 --> 00:03:45,010
as being better than human

96
00:03:45,010 --> 00:03:49,570
as bringing accident rates and
mortality rates further down,

97
00:03:49,570 --> 00:03:53,010
but look below what they are
today with human drivers.

98
00:03:53,010 --> 00:03:54,299
So really when you think about it,

99
00:03:54,300 --> 00:03:58,190
and again we'll stick with
the car context for a minute.

100
00:03:58,190 --> 00:03:59,790
What are we really trying to do?

101
00:03:59,790 --> 00:04:00,790
What does safety really mean?

102
00:04:00,790 --> 00:04:02,750
Because it isn't really enough simply

103
00:04:02,750 --> 00:04:04,490
to say we want better than human.

104
00:04:04,490 --> 00:04:05,960
Because what does that mean,

105
00:04:05,960 --> 00:04:08,060
for whom, under what circumstances?

106
00:04:08,060 --> 00:04:11,260
And in the context of autonomous vehicles,

107
00:04:11,260 --> 00:04:16,260
MIT has a really fascinating
site called Moralmachine.net,

108
00:04:16,820 --> 00:04:18,500
where they ask that question

109
00:04:18,500 --> 00:04:21,920
and they attempt essentially
to crowdsource ethics.

110
00:04:21,920 --> 00:04:25,650
The crowdsource, the
programming of AI in this area.

111
00:04:25,650 --> 00:04:28,140
And so what they do is
they present scenarios

112
00:04:29,070 --> 00:04:31,159
that would confront a self-driving vehicle

113
00:04:31,160 --> 00:04:33,700
and they ask the users as us

114
00:04:33,700 --> 00:04:36,710
the public to select what we think

115
00:04:36,710 --> 00:04:38,320
the self-driving car should do.

116
00:04:38,320 --> 00:04:40,810
And I have reflected on the slide here

117
00:04:40,810 --> 00:04:42,770
at the very basic first scenario

118
00:04:42,770 --> 00:04:46,000
which is a car driving
with five occupants.

119
00:04:46,000 --> 00:04:49,260
And you'll see on the slide,
on the picture on the left.

120
00:04:49,260 --> 00:04:51,330
If the car proceeds straight ahead,

121
00:04:51,330 --> 00:04:52,740
it will hit a big concrete barrier,

122
00:04:52,740 --> 00:04:56,440
killing all of the applicant
occupants we're told.

123
00:04:56,440 --> 00:04:59,030
Or, if it swerves to avoid the barrier,

124
00:04:59,030 --> 00:05:01,080
it will hit five pedestrians,

125
00:05:01,080 --> 00:05:03,050
killing those five pedestrians.

126
00:05:03,050 --> 00:05:06,130
So what will you have the
self-driving vehicle do?

127
00:05:06,130 --> 00:05:07,909
Now actually, as a point of interest,

128
00:05:07,910 --> 00:05:11,120
that scenario is actually one that is,

129
00:05:11,120 --> 00:05:16,120
I believe fairly one sided in
favor of hitting the barrier,

130
00:05:16,400 --> 00:05:18,960
the prejudice there being
in action versus action

131
00:05:18,960 --> 00:05:21,560
as opposed to swerving and
hitting the pedestrians

132
00:05:21,560 --> 00:05:24,330
although other scenarios,

133
00:05:24,330 --> 00:05:25,163
Oh, firstly I'm not sure

134
00:05:25,163 --> 00:05:26,560
if that is what a person
would do in real life.

135
00:05:26,560 --> 00:05:27,720
But setting that aside.

136
00:05:27,720 --> 00:05:29,900
Of other scenarios are quite complicated.

137
00:05:29,900 --> 00:05:31,950
And what the Moral Machine does is,

138
00:05:31,950 --> 00:05:34,440
it will provide different
mixes of pedestrians

139
00:05:34,440 --> 00:05:36,810
and different mixes of
occupants in the car.

140
00:05:36,810 --> 00:05:39,210
And some cases there'll be children,

141
00:05:39,210 --> 00:05:40,460
in some cases there'll be elderly,

142
00:05:40,460 --> 00:05:42,359
in some cases there'll be pregnant women,

143
00:05:42,360 --> 00:05:45,340
in some cases there'll be
Nobel prize winning scientists,

144
00:05:45,340 --> 00:05:48,060
in other cases they might
be convicted criminals.

145
00:05:48,060 --> 00:05:50,930
In some cases the pedestrians
will be walking with the light

146
00:05:50,930 --> 00:05:53,240
and other cases there'll be
walking against the light,

147
00:05:53,240 --> 00:05:58,240
and some cases like the not doing anything

148
00:05:59,340 --> 00:06:01,489
we'll hit the pedestrian as
opposed to getting the barrier.

149
00:06:01,490 --> 00:06:02,940
So the various scenarios

150
00:06:02,940 --> 00:06:06,490
that the Moral Machine
proposes or provides,

151
00:06:06,490 --> 00:06:08,470
and then it takes the results

152
00:06:08,470 --> 00:06:12,950
and comes up with what the
crowd believes should happen.

153
00:06:12,950 --> 00:06:16,500
And attempts to analyze it by nationality

154
00:06:16,500 --> 00:06:17,490
will tell you, you know what,

155
00:06:17,490 --> 00:06:19,210
folks in the United States generally say

156
00:06:19,210 --> 00:06:21,080
what people in Spain and other countries,

157
00:06:21,080 --> 00:06:22,080
it's very interesting.

158
00:06:22,080 --> 00:06:24,169
But really the question is that how

159
00:06:24,170 --> 00:06:25,830
we want to develop our AI?

160
00:06:25,830 --> 00:06:26,919
And in fact by the way,

161
00:06:26,920 --> 00:06:29,880
if we did use the one machine approach,

162
00:06:29,880 --> 00:06:31,330
we'd find that more often than not,

163
00:06:31,330 --> 00:06:32,590
you end up killing the elderly

164
00:06:32,590 --> 00:06:34,950
because elderly lives are valued

165
00:06:34,950 --> 00:06:37,760
at a lower level than others,

166
00:06:37,760 --> 00:06:40,070
consistently actually across nationalities

167
00:06:40,070 --> 00:06:41,820
in the Moral Machine scenarios.

168
00:06:41,820 --> 00:06:43,000
So it's an interesting question

169
00:06:43,000 --> 00:06:45,620
as to how we should approach this.

170
00:06:45,620 --> 00:06:46,960
And I'm a lawyer,

171
00:06:46,960 --> 00:06:48,799
so what I frequently will ask

172
00:06:48,800 --> 00:06:50,220
when I'm not sure about
something like that.

173
00:06:50,220 --> 00:06:52,450
Yes, but what does the law say?

174
00:06:52,450 --> 00:06:53,560
Well, in the United States,

175
00:06:53,560 --> 00:06:55,910
there is very little law on this topic.

176
00:06:55,910 --> 00:06:58,610
The closest we get to these broader

177
00:06:58,610 --> 00:07:00,750
issues of ethics and values

178
00:07:00,750 --> 00:07:02,060
when it comes to AI,

179
00:07:02,060 --> 00:07:03,590
is the federal trade commission

180
00:07:03,590 --> 00:07:06,679
which in 2020 updated its AI guidance

181
00:07:06,680 --> 00:07:09,860
and included within that
guidance certain key principles.

182
00:07:09,860 --> 00:07:12,280
Can principles of transparency disclosing

183
00:07:12,280 --> 00:07:13,989
when you use data that you will collect

184
00:07:13,990 --> 00:07:15,750
and how you will use it,

185
00:07:15,750 --> 00:07:17,780
clear explanations for the system

186
00:07:17,780 --> 00:07:19,710
for the factors that go into it.

187
00:07:19,710 --> 00:07:22,460
Fairness, looking at both
the inputs that go in

188
00:07:22,460 --> 00:07:24,419
and the outcomes, are the inputs fair,

189
00:07:24,420 --> 00:07:26,330
are they being gathered in a fair way

190
00:07:26,330 --> 00:07:29,330
and are the outcomes that
are being generated fair?

191
00:07:29,330 --> 00:07:31,479
User rights, rights to understand the data

192
00:07:31,480 --> 00:07:34,200
that's being provided and
how it's being processed.

193
00:07:34,200 --> 00:07:37,469
Robustness, competence, has
got to be done in a capable way

194
00:07:37,470 --> 00:07:38,930
and accountability at the end of the day

195
00:07:38,930 --> 00:07:40,810
you have to be accountable
for the results.

196
00:07:40,810 --> 00:07:42,340
But they're very high level concepts,

197
00:07:42,340 --> 00:07:45,659
and it's just guidance, not
enforcement, not a regulation,

198
00:07:45,660 --> 00:07:48,422
not enforcement scheme.

199
00:07:48,422 --> 00:07:50,840
Beyond that, the United States

200
00:07:50,840 --> 00:07:54,080
at any United States
activity in the area that AI,

201
00:07:54,080 --> 00:07:57,729
has really focused on
increasing investment in AI

202
00:07:57,730 --> 00:07:59,890
and competitiveness in AI.

203
00:07:59,890 --> 00:08:01,840
So in February of 2019,

204
00:08:01,840 --> 00:08:03,989
then President Trump
issued an executive order,

205
00:08:03,990 --> 00:08:06,350
establishing the American AI Initiative.

206
00:08:06,350 --> 00:08:09,360
That really was about investment,

207
00:08:09,360 --> 00:08:10,980
very brief mentions of privacy,

208
00:08:10,980 --> 00:08:14,220
civil liberty and safety
concerns, but no direction.

209
00:08:14,220 --> 00:08:16,260
The office of management and budget

210
00:08:16,260 --> 00:08:19,250
and executive branch agency in late 2020

211
00:08:19,250 --> 00:08:22,970
warned against approaches
that would stifle AI adoption

212
00:08:22,970 --> 00:08:26,200
and emphasized what they call
10 stewardship principles

213
00:08:26,200 --> 00:08:27,550
that are there on the slide.

214
00:08:27,550 --> 00:08:29,410
You can see some of those principles

215
00:08:29,410 --> 00:08:32,640
do a touch on the ethical
and moral questions

216
00:08:32,640 --> 00:08:33,880
that we've been discussing.

217
00:08:33,880 --> 00:08:38,240
Fairness, non-discrimination,
disclosure, transparency.

218
00:08:38,240 --> 00:08:42,100
But again, the main
objective of that guidance

219
00:08:42,100 --> 00:08:44,300
was to encourage, to stimulate,

220
00:08:44,300 --> 00:08:46,689
to drive AI adoption.

221
00:08:46,690 --> 00:08:48,130
In January 2021

222
00:08:48,130 --> 00:08:50,280
in the waning days of the administration,

223
00:08:50,280 --> 00:08:52,663
the department of transportation issued,

224
00:08:53,620 --> 00:08:55,830
oops, issued comprehensive.

225
00:08:55,830 --> 00:08:58,720
I've got to go back, I'm sorry about that.

226
00:08:58,720 --> 00:08:59,800
Bear with me for a sec.

227
00:08:59,800 --> 00:09:03,729
Issued comprehensive guidance
on automated vehicles,

228
00:09:03,730 --> 00:09:08,010
again, prioritizing
safety and speed to market

229
00:09:08,010 --> 00:09:10,700
and leaving really sort of
key questions are answered.

230
00:09:10,700 --> 00:09:14,110
In January also, the same month of 2021,

231
00:09:14,110 --> 00:09:15,770
the National Artificial Intelligence

232
00:09:15,770 --> 00:09:18,120
Initiative Office was established.

233
00:09:18,120 --> 00:09:21,270
And then in March 2021, I'll
do the bike administration

234
00:09:21,270 --> 00:09:23,270
but the completing a process

235
00:09:23,270 --> 00:09:26,380
that was really conducted
under the prior administration,

236
00:09:26,380 --> 00:09:28,810
the National Security Commission
on Artificial Intelligence

237
00:09:28,810 --> 00:09:30,540
issued its report on AI.

238
00:09:30,540 --> 00:09:32,339
And again, all of these efforts

239
00:09:32,340 --> 00:09:34,393
are geared toward competitiveness,

240
00:09:35,650 --> 00:09:40,199
geared toward moving
the ball forward on AI.

241
00:09:40,200 --> 00:09:43,920
Okay, all right.

242
00:09:43,920 --> 00:09:46,189
Again, with the exception
of the FTC guidance,

243
00:09:46,190 --> 00:09:47,720
that a really big focus

244
00:09:47,720 --> 00:09:50,020
really has been an accelerating R and D.

245
00:09:50,020 --> 00:09:51,010
And that's also been true

246
00:09:51,010 --> 00:09:53,040
of most legislative
initiatives in congress,

247
00:09:53,040 --> 00:09:54,870
which are gonna about funding AI

248
00:09:54,870 --> 00:09:58,580
or encouraging executive
branch agencies to pursue AI.

249
00:09:58,580 --> 00:10:01,770
I'm not really focused for
the most part on ethical

250
00:10:01,770 --> 00:10:04,699
or discrimination or
other kinds of questions.

251
00:10:04,700 --> 00:10:08,010
So what's ahead now under
the new administration.

252
00:10:08,010 --> 00:10:10,920
Well, firstly, the Biden administration

253
00:10:10,920 --> 00:10:13,199
and its office of science
and technology policy

254
00:10:13,200 --> 00:10:15,490
has a new deputy director Alondra Nelson.

255
00:10:15,490 --> 00:10:17,580
She happens to be a sociologist

256
00:10:17,580 --> 00:10:20,380
who's written about the
societal impacts of AI

257
00:10:20,380 --> 00:10:22,560
emphasizing the need for accountability,

258
00:10:22,560 --> 00:10:25,010
which may portend an increased focus

259
00:10:25,010 --> 00:10:26,470
on those kind of issues.

260
00:10:26,470 --> 00:10:28,570
Trying to save us from being locked

261
00:10:28,570 --> 00:10:31,540
in the trapped, in the
cell of an amoral AI,

262
00:10:31,540 --> 00:10:33,689
and the illustration (indistinct)

263
00:10:33,690 --> 00:10:36,210
which shameless plug for
my daughter is an artist

264
00:10:36,210 --> 00:10:37,557
is her a piece of work.

265
00:10:37,557 --> 00:10:40,030
And increasing the
emphasis on potential harms

266
00:10:40,030 --> 00:10:42,002
and risks associated with AI.

267
00:10:43,190 --> 00:10:45,950
Abroad looking at a multimodal
multilateral context,

268
00:10:45,950 --> 00:10:48,430
we have seen some focus on these issues.

269
00:10:48,430 --> 00:10:53,426
The OECD has issued Principals
and Artificial Intelligence.

270
00:10:53,427 --> 00:10:57,270
The OECD principles are
back to over 40 nations.

271
00:10:57,270 --> 00:10:58,530
They're not legally binding

272
00:10:58,530 --> 00:11:01,020
but they're intended to
influence legislation.

273
00:11:01,020 --> 00:11:01,939
And you can see there,

274
00:11:01,940 --> 00:11:05,540
the principles are very
similar to the FTC guidelines,

275
00:11:05,540 --> 00:11:07,620
not surprisingly, not coincidentally.

276
00:11:07,620 --> 00:11:09,910
Things like inclusive
and sustainable growth

277
00:11:09,910 --> 00:11:12,469
and well-being, human centered values.

278
00:11:12,470 --> 00:11:14,600
Again, bring us back to this idea

279
00:11:14,600 --> 00:11:16,520
that we wanna replicate something

280
00:11:16,520 --> 00:11:18,721
that's human about us and the AI.

281
00:11:18,721 --> 00:11:20,660
Transparency and explainability,

282
00:11:20,660 --> 00:11:22,892
robustness and safety and accountability.

283
00:11:23,890 --> 00:11:26,790
The G20 and its Ministerial
Statement on Trade

284
00:11:26,790 --> 00:11:27,963
and Digital Economy,

285
00:11:29,360 --> 00:11:32,330
also issued similar principles calling

286
00:11:32,330 --> 00:11:35,500
for a Human-centered future society.

287
00:11:35,500 --> 00:11:36,800
The AI principles,

288
00:11:36,800 --> 00:11:38,829
again, that are reflective,
administerial statement

289
00:11:38,830 --> 00:11:41,960
are very similar to the OECD guidelines.

290
00:11:41,960 --> 00:11:46,300
Now, in terms of legislation
pending in congress right now,

291
00:11:46,300 --> 00:11:49,380
the only bill that really speaks
to these kind of questions

292
00:11:49,380 --> 00:11:51,370
is the one that I mentioned at the outset

293
00:11:51,370 --> 00:11:52,700
of my presentation,

294
00:11:52,700 --> 00:11:55,913
the feature of Artificial
Intelligence act of 2020.

295
00:11:57,085 --> 00:12:00,798
It was introduced by Senator
Maria Cantwell from Washington.

296
00:12:00,798 --> 00:12:03,860
With GOP co-sponsor
Sandra Young from Indiana,

297
00:12:03,860 --> 00:12:07,720
there is no worthy because
prior legislative initiatives

298
00:12:07,720 --> 00:12:10,230
that attempted to raise
these kinds of questions

299
00:12:10,230 --> 00:12:11,740
had generally been sponsored

300
00:12:11,740 --> 00:12:14,520
by Democratic members
on Republican members.

301
00:12:14,520 --> 00:12:17,500
This bill would establish a
Federal advisory committee

302
00:12:17,500 --> 00:12:19,090
under the department of Commerce

303
00:12:19,090 --> 00:12:21,620
to advise on these kinds of questions.

304
00:12:21,620 --> 00:12:22,980
Accountability legal rights,

305
00:12:22,980 --> 00:12:25,250
bias, civil rights, things of that nature,

306
00:12:25,250 --> 00:12:28,290
really emphasizing
those ethical questions.

307
00:12:28,290 --> 00:12:30,980
Now, it's important to not overstate

308
00:12:30,980 --> 00:12:34,020
that there is the impact of
this bill even if it is enacted

309
00:12:34,020 --> 00:12:36,630
because it really is establishing
an advisory committee.

310
00:12:36,630 --> 00:12:41,060
It's not creating new legal
norms or requirements.

311
00:12:41,060 --> 00:12:43,239
Unlike the prior bill

312
00:12:43,240 --> 00:12:44,990
that was in the last congress,

313
00:12:44,990 --> 00:12:47,490
the algorithmic accountability act,

314
00:12:47,490 --> 00:12:49,140
which did not pass

315
00:12:49,140 --> 00:12:51,250
and really had far more
(indistinct) to it.

316
00:12:51,250 --> 00:12:53,420
This was a bill that was
sponsored in the house

317
00:12:53,420 --> 00:12:56,339
by 32 Democratic members and of the Senate

318
00:12:56,340 --> 00:12:58,800
by senators Wyden and Booker from Oregon

319
00:12:58,800 --> 00:13:00,150
and New Jersey respectively.

320
00:13:00,150 --> 00:13:01,670
Both of them were also Democrats.

321
00:13:01,670 --> 00:13:04,209
And it would have
mandated the FTC to issue

322
00:13:04,210 --> 00:13:08,720
regulations requiring Automated
decision impact assessments

323
00:13:08,720 --> 00:13:11,030
which are those of you

324
00:13:11,030 --> 00:13:13,750
in the privacy world will
recognize that concept

325
00:13:13,750 --> 00:13:16,570
very similar to data
protection, impact assessments

326
00:13:16,570 --> 00:13:18,960
which also will be
mandated under the bill.

327
00:13:18,960 --> 00:13:21,450
It will cover all businesses that exceed

328
00:13:21,450 --> 00:13:24,060
the thresholds that are stated in that

329
00:13:24,060 --> 00:13:26,689
on a slide over $50 million in revenue.

330
00:13:26,690 --> 00:13:28,990
Because senior controlling
personal information of more

331
00:13:28,990 --> 00:13:32,980
than a million consumers or
devices and data brokers,

332
00:13:32,980 --> 00:13:37,980
and we treat violations
of the law as unfair

333
00:13:39,910 --> 00:13:42,240
or deceptive acts or practices,
which are penalties that are

334
00:13:42,240 --> 00:13:46,500
within the province of the FTC to enforce.

335
00:13:46,500 --> 00:13:48,150
It also would have granted States

336
00:13:48,150 --> 00:13:49,579
the power to bring civil suits

337
00:13:49,580 --> 00:13:51,110
against entities that violated the act.

338
00:13:51,110 --> 00:13:53,240
Now, this was a bill, as I said,

339
00:13:53,240 --> 00:13:55,560
that was introduced in 2019.

340
00:13:55,560 --> 00:13:56,540
It did not pass.

341
00:13:56,540 --> 00:13:58,790
And with the new Congress,
it's no longer a bill.

342
00:13:58,790 --> 00:14:00,839
We need to be re-introduced.

343
00:14:00,840 --> 00:14:02,480
And to my knowledge, as of the time

344
00:14:02,480 --> 00:14:05,280
that I'm recording this, it
has not yet been introduced.

345
00:14:06,160 --> 00:14:08,670
So we got to see what happens to it.

346
00:14:08,670 --> 00:14:11,120
Now, the States in the
meantime are not really sitting

347
00:14:11,120 --> 00:14:14,850
around waiting for the
Feds to take action.

348
00:14:14,850 --> 00:14:18,677
Like in privacy, States have
moved forward in this space.

349
00:14:18,677 --> 00:14:22,220
No state has passed legislation as yet,

350
00:14:22,220 --> 00:14:25,930
but there are bills pending
in at least three California,

351
00:14:25,930 --> 00:14:28,689
Washington state and Maryland.

352
00:14:28,690 --> 00:14:31,900
The Washington and Maryland
bills would require government

353
00:14:31,900 --> 00:14:35,790
agencies to assess the
discriminatory impacts

354
00:14:35,790 --> 00:14:39,520
of automated decision systems
before implementing them.

355
00:14:39,520 --> 00:14:42,810
California bill would
apply to any business

356
00:14:42,810 --> 00:14:46,430
in California that utilizes automated

357
00:14:46,430 --> 00:14:48,620
decision systems provides the person

358
00:14:48,620 --> 00:14:50,320
with a program that uses automation

359
00:14:50,320 --> 00:14:51,360
automated decision systems.

360
00:14:51,360 --> 00:14:52,438
Now there's a few interesting things

361
00:14:52,438 --> 00:14:55,510
about these state bills
that are worth noting

362
00:14:55,510 --> 00:14:59,080
just in terms of the politics

363
00:14:59,080 --> 00:15:01,600
that surround them and
their potential scope.

364
00:15:01,600 --> 00:15:02,700
And the first case.

365
00:15:02,700 --> 00:15:04,290
One thing you might notice that all three

366
00:15:04,290 --> 00:15:06,610
of these States are what
are called blue States.

367
00:15:06,610 --> 00:15:07,889
I don't think that's surprising.

368
00:15:07,889 --> 00:15:10,040
I think that's where
we've seen most interest

369
00:15:10,040 --> 00:15:12,120
in these kinds of
measures, at least so far.

370
00:15:12,120 --> 00:15:15,450
I don't think that will
always be the case.

371
00:15:15,450 --> 00:15:18,100
In addition, the Washington
and Maryland bills

372
00:15:18,100 --> 00:15:21,610
are more modest in
objective or an aspiration

373
00:15:21,610 --> 00:15:23,990
they would apply to
agencies, government agencies

374
00:15:23,990 --> 00:15:26,170
not to private businesses.

375
00:15:26,170 --> 00:15:29,810
And even the California
bill, unlike privacy laws,

376
00:15:29,810 --> 00:15:32,630
which tend to apply to any business

377
00:15:32,630 --> 00:15:34,200
that collects personal information

378
00:15:34,200 --> 00:15:37,050
about a person in that state.

379
00:15:37,050 --> 00:15:40,229
This bill applies to
businesses in the state.

380
00:15:40,230 --> 00:15:42,710
So a business outside the
California, that's dealing

381
00:15:42,710 --> 00:15:45,610
with Californians at least
would not appear to fall

382
00:15:45,610 --> 00:15:47,670
within the scope of the
bill as currently drafted.

383
00:15:47,670 --> 00:15:50,750
That's a material
difference between this kind

384
00:15:50,750 --> 00:15:53,190
of legislation and privacy legislation,

385
00:15:53,190 --> 00:15:56,500
and it will be interesting
to see if that continues.

386
00:15:56,500 --> 00:15:58,080
Okay, so in the meantime,

387
00:15:58,080 --> 00:16:01,878
without any actual law at
the federal or state level,

388
00:16:01,878 --> 00:16:05,170
what are ways to think
about approaching AI?

389
00:16:05,170 --> 00:16:07,229
If you're attempting
to deploy an AI system

390
00:16:07,230 --> 00:16:09,390
or developing a system in your enterprise?

391
00:16:09,390 --> 00:16:12,020
So there are what I like
to call four pillars

392
00:16:12,020 --> 00:16:13,819
of Artificial Intelligence
governance that I

393
00:16:13,820 --> 00:16:15,230
think makes sense to think about.

394
00:16:15,230 --> 00:16:18,310
And they'll track, you'll
see that the FTC guidelines

395
00:16:18,310 --> 00:16:21,819
the OACD, statement
principles, the G20 principles

396
00:16:21,820 --> 00:16:24,400
they really are not
particularly innovative

397
00:16:25,330 --> 00:16:29,490
or revolutionary, but I think
they're fairly intuitive.

398
00:16:29,490 --> 00:16:31,860
The first is fairness or effectiveness,

399
00:16:31,860 --> 00:16:33,700
implementing procedures and standards

400
00:16:33,700 --> 00:16:36,010
to ensure systems are meeting
their intended purpose

401
00:16:36,010 --> 00:16:40,420
without bias, competence,
specifying the knowledge

402
00:16:40,420 --> 00:16:42,219
and skills required of the developers

403
00:16:42,220 --> 00:16:44,331
and operators, so that
you can promote trust

404
00:16:44,331 --> 00:16:47,390
and mitigate the risk of bad outcomes.

405
00:16:47,390 --> 00:16:50,010
Transparency, what it sounds like,

406
00:16:50,010 --> 00:16:52,100
revealing appropriate
amounts of information.

407
00:16:52,100 --> 00:16:54,020
I'll come back to that in a little bit,

408
00:16:54,020 --> 00:16:57,699
so that users and the
public can have trust

409
00:16:57,700 --> 00:17:00,560
in what you're doing and in the system.

410
00:17:00,560 --> 00:17:03,239
Accountability, making sure that someone

411
00:17:03,240 --> 00:17:04,730
is responsible for implementing

412
00:17:04,730 --> 00:17:07,970
and for outcomes associated
with the AI system,

413
00:17:07,970 --> 00:17:09,839
which requires in turn measurability,

414
00:17:09,839 --> 00:17:12,030
you have to have some measure
way to measure outcomes,

415
00:17:12,030 --> 00:17:13,780
so you can be accountable for them.

416
00:17:14,670 --> 00:17:18,380
Now, not all AI applications
require equally strong

417
00:17:18,380 --> 00:17:20,690
or similarly structured
governance programs.

418
00:17:20,690 --> 00:17:21,550
For example,

419
00:17:21,550 --> 00:17:23,277
some AI's will be doing things that far

420
00:17:23,277 --> 00:17:25,230
are more important than others

421
00:17:25,230 --> 00:17:27,920
that will involve human life or Liberty.

422
00:17:27,920 --> 00:17:30,650
And those things, those
types of applications

423
00:17:30,650 --> 00:17:33,980
you might want to
require stronger programs

424
00:17:33,980 --> 00:17:36,270
from then those that
maybe are less important.

425
00:17:36,270 --> 00:17:38,790
Like those that determine
what Netflix movie

426
00:17:38,790 --> 00:17:40,909
to recommend for you to watch, right?

427
00:17:40,910 --> 00:17:43,420
Similarly, strengths in
one pillar may reduce

428
00:17:43,420 --> 00:17:44,950
the need for strength and others.

429
00:17:44,950 --> 00:17:48,770
These are not all for equal things.

430
00:17:48,770 --> 00:17:53,010
And so, one, having
answered strengthened one

431
00:17:53,010 --> 00:17:56,033
might allow you to have somewhat
less strength in the other.

432
00:17:56,870 --> 00:17:59,500
Now, let's dive down a little
bit into these pillars.

433
00:17:59,500 --> 00:18:02,653
So Fairness and Effectiveness,
what does that entail?

434
00:18:03,770 --> 00:18:05,700
Measuring effectiveness
because you don't know

435
00:18:05,700 --> 00:18:07,890
whether something's effective
unless you measure it?

436
00:18:07,890 --> 00:18:10,250
There are a few different ways to do that.

437
00:18:10,250 --> 00:18:14,030
One, if one way might be
single system validation

438
00:18:14,030 --> 00:18:15,960
where you have one app, but you do a lot

439
00:18:15,960 --> 00:18:18,890
of sampling and review of data to measure

440
00:18:18,890 --> 00:18:22,170
the outcomes and determine
their effectiveness.

441
00:18:22,170 --> 00:18:23,930
And know there might be
multi-system benchmarking

442
00:18:23,930 --> 00:18:25,540
where you've got comparative evaluation,

443
00:18:25,540 --> 00:18:27,420
you have two systems,
there's three or four

444
00:18:27,420 --> 00:18:28,253
and you can...

445
00:18:28,253 --> 00:18:30,260
Are all designed to
serve the same objective

446
00:18:30,260 --> 00:18:32,780
and you can measure
comparative performance.

447
00:18:32,780 --> 00:18:34,879
So, neither one is right or wrong.

448
00:18:34,880 --> 00:18:37,806
There are different ways of
accomplishing the same thing.

449
00:18:37,806 --> 00:18:39,800
When defining those
metrics of course, you have

450
00:18:39,800 --> 00:18:43,210
to use metrics that make
sense and your context.

451
00:18:43,210 --> 00:18:44,043
What does that mean?

452
00:18:44,043 --> 00:18:46,100
Well, they have to meet
your objectives, whether

453
00:18:46,100 --> 00:18:48,760
those are commercial objectives
or research objectives.

454
00:18:48,760 --> 00:18:50,060
And it's important, I think

455
00:18:50,060 --> 00:18:51,649
to be inclusive and defining them

456
00:18:51,650 --> 00:18:54,030
by seeking input from
various stakeholders,

457
00:18:54,030 --> 00:18:55,790
to make sure that you're
not overlooking things

458
00:18:55,790 --> 00:18:59,690
that are important or
assigning a disproportionate

459
00:18:59,690 --> 00:19:02,333
value or lack of value
to particular metrics.

460
00:19:03,450 --> 00:19:05,980
Again, robust human-led
sampling and auditing

461
00:19:05,980 --> 00:19:07,560
as I mentioned is important.

462
00:19:07,560 --> 00:19:08,730
And it's important also to know

463
00:19:08,730 --> 00:19:10,750
that bigger data is
not always better data.

464
00:19:10,750 --> 00:19:13,700
You have to decide what
data really is necessary,

465
00:19:13,700 --> 00:19:16,200
what data is appropriate,
what data is relevant.

466
00:19:16,200 --> 00:19:17,710
Perhaps there's a better way of saying it

467
00:19:17,710 --> 00:19:21,710
and that's the data that you
want to collect and analyze.

468
00:19:21,710 --> 00:19:22,543
Okay.

469
00:19:23,630 --> 00:19:25,130
It's also very important
when you're dealing

470
00:19:25,130 --> 00:19:28,210
with effectiveness to Combat Bias

471
00:19:28,210 --> 00:19:29,450
and Algorithmic Decision-making.

472
00:19:29,450 --> 00:19:31,250
That's both an Ethical
imperative and it can't

473
00:19:31,250 --> 00:19:33,230
be illegal one in certain
contexts, for example

474
00:19:33,230 --> 00:19:34,843
in employment related decisions.

475
00:19:35,750 --> 00:19:37,670
And so, in doing that, is important

476
00:19:37,670 --> 00:19:38,840
to make sure you're focusing

477
00:19:38,840 --> 00:19:41,970
on the demographic profile of the dataset.

478
00:19:41,970 --> 00:19:43,270
To make sure that the data set

479
00:19:43,270 --> 00:19:45,480
is appropriately representative,

480
00:19:45,480 --> 00:19:48,380
but also that your AI
team itself is diverse

481
00:19:48,380 --> 00:19:49,366
to make sure you've got

482
00:19:49,366 --> 00:19:54,030
a diversity of perspectives,
of backgrounds,

483
00:19:54,030 --> 00:19:57,770
of disciplines, so that
you are taking into account

484
00:19:57,770 --> 00:19:59,129
and looking at the issue holistically.

485
00:19:59,130 --> 00:20:01,510
Because again, if the
objective is at least

486
00:20:01,510 --> 00:20:04,270
is to create a human, or
a human approximation,

487
00:20:04,270 --> 00:20:05,850
or really as we discussed before

488
00:20:05,850 --> 00:20:07,659
a better than human approximation,

489
00:20:07,660 --> 00:20:10,300
then you want that system
to have the benefit

490
00:20:10,300 --> 00:20:13,510
of as holistic a set
of inputs as possible.

491
00:20:13,510 --> 00:20:15,970
You also want to consider equality

492
00:20:15,970 --> 00:20:18,440
of treatment versus equality impact.

493
00:20:18,440 --> 00:20:19,737
What is it that you're striving for?

494
00:20:19,737 --> 00:20:21,070
Are you striving for both?

495
00:20:21,070 --> 00:20:23,320
Are you striving for one or the other?

496
00:20:23,320 --> 00:20:24,750
And there's no one answer to that.

497
00:20:24,750 --> 00:20:27,260
It really will depend
on the circumstances.

498
00:20:27,260 --> 00:20:29,660
And once you've decided

499
00:20:29,660 --> 00:20:32,360
that with the appropriate
input from the team

500
00:20:32,360 --> 00:20:35,300
and from the constituents
and stakeholders,

501
00:20:35,300 --> 00:20:36,800
then you can design toward it.

502
00:20:38,210 --> 00:20:40,020
There's also a question of competence.

503
00:20:40,020 --> 00:20:42,690
Your (indistinct) your AI team
should be confident, right?

504
00:20:42,690 --> 00:20:45,090
That almost sounds obvious.

505
00:20:45,090 --> 00:20:48,199
It is obvious, but what competence mean

506
00:20:48,200 --> 00:20:50,610
in this context is not necessarily just

507
00:20:50,610 --> 00:20:52,100
that they're good programmers, right?

508
00:20:52,100 --> 00:20:53,550
Being a programmer isn't going

509
00:20:53,550 --> 00:20:55,780
to answer the question of what to program.

510
00:20:55,780 --> 00:21:00,280
And so having the right set
of disciplines represented,

511
00:21:00,280 --> 00:21:02,090
is gonna be incredibly important.

512
00:21:02,090 --> 00:21:04,520
There's a question whether
within the regulatory standards

513
00:21:04,520 --> 00:21:06,210
and surround that, I
don't know the answer.

514
00:21:06,210 --> 00:21:07,240
I don't have any answer.

515
00:21:07,240 --> 00:21:09,110
I think there are
arguments for and against.

516
00:21:09,110 --> 00:21:11,540
I am not sure what a regulatory
standard would say today,

517
00:21:11,540 --> 00:21:13,110
but perhaps down the road

518
00:21:13,110 --> 00:21:15,159
there'll be enough definition to do that.

519
00:21:15,160 --> 00:21:16,930
But in the meantime, it's
important to be thoughtful

520
00:21:16,930 --> 00:21:19,030
about the set of skills
that you're looking for

521
00:21:19,030 --> 00:21:20,860
when you're putting together a team

522
00:21:22,090 --> 00:21:24,687
to design and operate an AI system.

523
00:21:25,630 --> 00:21:27,050
In addition there needs to be safeguards

524
00:21:27,050 --> 00:21:29,520
against improper use or
operation of a system.

525
00:21:29,520 --> 00:21:31,129
And really importantly,

526
00:21:31,130 --> 00:21:34,020
there has to be a set
of defined circumstances

527
00:21:34,020 --> 00:21:37,376
where the system operator
should override the system.

528
00:21:37,376 --> 00:21:39,760
This should not be an
ad hoc determination.

529
00:21:39,760 --> 00:21:40,626
You should give a proactive thought

530
00:21:40,626 --> 00:21:45,460
to when that should
happen and document that,

531
00:21:45,460 --> 00:21:47,050
so that the operator knows

532
00:21:47,050 --> 00:21:49,730
when override needs to take place.

533
00:21:49,730 --> 00:21:52,523
And you should, could be
continually re-evaluating,

534
00:21:53,400 --> 00:21:56,960
continually gauging the
effectiveness of the system

535
00:21:56,960 --> 00:21:59,980
and making adjustments as appropriate.

536
00:21:59,980 --> 00:22:00,813
Transparency.

537
00:22:00,813 --> 00:22:02,210
Transparency is incredibly important

538
00:22:02,210 --> 00:22:03,980
as I've already described.

539
00:22:03,980 --> 00:22:08,980
You have to think about
in practicing transparency

540
00:22:09,110 --> 00:22:11,659
what you really are want to share,

541
00:22:11,660 --> 00:22:14,400
is doesn't mean that you
share your confidential IP.

542
00:22:14,400 --> 00:22:16,830
You don't put your source
code out there necessarily

543
00:22:16,830 --> 00:22:17,730
but you need to think about

544
00:22:17,730 --> 00:22:20,790
how can I communicate what we are doing

545
00:22:20,790 --> 00:22:22,970
in a way that's
understandable and digestible

546
00:22:22,970 --> 00:22:27,340
and accurate, without revealing
sensitive, proprietary IP.

547
00:22:27,340 --> 00:22:29,750
And in doing that, I think
it's really important

548
00:22:29,750 --> 00:22:33,560
for there to also be
whistleblower protection

549
00:22:33,560 --> 00:22:36,120
to individuals who offer information,

550
00:22:36,120 --> 00:22:38,229
the system is not being operated properly

551
00:22:38,230 --> 00:22:39,830
or if we're using improper results,

552
00:22:39,830 --> 00:22:41,830
because sometimes the best information,

553
00:22:41,830 --> 00:22:44,740
the best feedback you can get
is coming from those kinds

554
00:22:44,740 --> 00:22:47,500
of sources, not from
your regular processes.

555
00:22:47,500 --> 00:22:52,500
And as is the case with
escalation or override,

556
00:22:52,680 --> 00:22:54,530
it's also important to have a human led

557
00:22:54,530 --> 00:22:57,920
appellate process for
algorithmic decisions.

558
00:22:57,920 --> 00:23:01,090
And, having periodic transparency reports

559
00:23:01,090 --> 00:23:03,050
can be very useful as
well, where you put out

560
00:23:03,050 --> 00:23:05,870
annually or biannually
or quarterly reports

561
00:23:05,870 --> 00:23:09,199
that summarize how you
were doing very similar

562
00:23:09,200 --> 00:23:12,770
to how some of the large ISP,
for example, put out reports

563
00:23:12,770 --> 00:23:14,600
about how many times they
get government subpoenas

564
00:23:14,600 --> 00:23:15,830
or respond to government requests

565
00:23:15,830 --> 00:23:17,159
national security letters.

566
00:23:17,160 --> 00:23:18,980
The idea is they're
trying to build confidence

567
00:23:18,980 --> 00:23:20,750
by providing information to the public,

568
00:23:20,750 --> 00:23:23,100
they're there legally constrained
in what they can share.

569
00:23:23,100 --> 00:23:25,179
So often it's very high level information

570
00:23:25,180 --> 00:23:27,170
here that will often be the case.

571
00:23:27,170 --> 00:23:29,630
And so companies can make
their decision themselves.

572
00:23:29,630 --> 00:23:31,950
But providing that kind
of reporting proactively

573
00:23:31,950 --> 00:23:33,670
can be very helpful.

574
00:23:33,670 --> 00:23:35,230
Accountability.

575
00:23:35,230 --> 00:23:36,700
Having well-documented lines

576
00:23:36,700 --> 00:23:39,650
of responsibility among
those who operate the system,

577
00:23:39,650 --> 00:23:41,673
who is in charge, who they report,

578
00:23:43,089 --> 00:23:44,600
where do those people stepped in,

579
00:23:44,600 --> 00:23:47,360
all very important training is critical.

580
00:23:47,360 --> 00:23:49,370
An internal oversight mechanisms

581
00:23:49,370 --> 00:23:51,253
and audits are important.

582
00:23:52,895 --> 00:23:55,670
And making sure that
there's an understanding

583
00:23:55,670 --> 00:23:57,690
of the legal and ethical
standards to which you

584
00:23:57,690 --> 00:23:59,980
are hearing is also critical.

585
00:23:59,980 --> 00:24:03,110
And as of today, often the
case that will be the case

586
00:24:03,110 --> 00:24:05,110
that will be mostly
ethical as opposed to legal

587
00:24:05,110 --> 00:24:08,129
because there isn't a lot of while there

588
00:24:08,130 --> 00:24:10,170
although in some cases, as I
mentioned, there will be laws

589
00:24:10,170 --> 00:24:12,370
against discrimination
in the employment context

590
00:24:12,370 --> 00:24:13,889
and the credit context and others,

591
00:24:13,890 --> 00:24:15,930
but often it will be ethical questions

592
00:24:15,930 --> 00:24:17,270
that you're going to ethical standards

593
00:24:17,270 --> 00:24:19,000
that you're gonna want to establish.

594
00:24:19,000 --> 00:24:20,740
So how do these pillars translate

595
00:24:20,740 --> 00:24:23,290
into legal liability in the
case of the self-driving

596
00:24:23,290 --> 00:24:25,600
vehicle or the employment
or credit decision

597
00:24:25,600 --> 00:24:27,550
or the AI directed medical treatment

598
00:24:27,550 --> 00:24:32,060
or even God-forbid legal
advocacy, the AI lawyer?

599
00:24:32,060 --> 00:24:33,923
It's really not clear, again in some cases

600
00:24:33,923 --> 00:24:38,090
that there is generally
applicable legal guidance,

601
00:24:38,090 --> 00:24:39,409
or legal, or generally applicable

602
00:24:39,410 --> 00:24:41,140
legal requirements like in the context

603
00:24:41,140 --> 00:24:43,360
of non-discrimination and
employment and credit.

604
00:24:43,360 --> 00:24:44,919
But in other cases, there aren't

605
00:24:44,920 --> 00:24:46,200
yet legal standards.

606
00:24:46,200 --> 00:24:47,510
And it's almost, you know,

607
00:24:47,510 --> 00:24:49,420
there's a (indistinct) technology hurdle

608
00:24:49,420 --> 00:24:50,570
developing the technology

609
00:24:50,570 --> 00:24:52,000
to do what we want it to do.

610
00:24:52,000 --> 00:24:53,880
And there's a legal hurdle, developing

611
00:24:53,880 --> 00:24:57,100
the legal scaffolding to
surround that technology.

612
00:24:57,100 --> 00:24:59,580
And it's not those
things aren't built yet.

613
00:24:59,580 --> 00:25:00,939
Neither one of those is built.

614
00:25:00,940 --> 00:25:02,400
And as I mentioned at the outset,

615
00:25:02,400 --> 00:25:03,700
there's this underlying question

616
00:25:03,700 --> 00:25:06,350
that we all still have and we expect more

617
00:25:06,350 --> 00:25:08,149
from the AI than we do from each other.

618
00:25:08,150 --> 00:25:09,917
I think the answer is yes, we do.

619
00:25:09,917 --> 00:25:13,260
And the autonomous vehicles,
just one example of it.

620
00:25:13,260 --> 00:25:16,340
But, I think throughout
AI, we are building AI

621
00:25:16,340 --> 00:25:18,449
because we want it to
be better than we are.

622
00:25:18,450 --> 00:25:20,540
And the question becomes
what does better mean

623
00:25:20,540 --> 00:25:22,570
because it's not always so obvious.

624
00:25:22,570 --> 00:25:25,840
And when you have the
ability to be better,

625
00:25:25,840 --> 00:25:27,310
that presents choices that didn't

626
00:25:27,310 --> 00:25:28,990
present themselves previously.

627
00:25:28,990 --> 00:25:30,700
So we never had to think about them.

628
00:25:30,700 --> 00:25:32,040
Do we smash into the barrier

629
00:25:32,040 --> 00:25:34,440
or do we swerve and hit the pedestrians?

630
00:25:34,440 --> 00:25:36,350
It doesn't matter who
those pedestrians are.

631
00:25:36,350 --> 00:25:37,659
Those are questions we've never really

632
00:25:37,660 --> 00:25:39,840
had to grapple with as a society.

633
00:25:39,840 --> 00:25:42,080
But if we're going to program systems

634
00:25:42,080 --> 00:25:44,070
to govern these actions, we need to.

635
00:25:44,070 --> 00:25:46,300
And we don't want to leave
it just to a programmer.

636
00:25:46,300 --> 00:25:47,649
We need to think about more broadly

637
00:25:47,650 --> 00:25:49,639
how we're going to have them developed.

638
00:25:49,639 --> 00:25:50,610
So what do we do?

639
00:25:50,610 --> 00:25:53,469
So, in the meantime, and
then, and they hear it now,

640
00:25:53,470 --> 00:25:55,320
I think firstly, it's clear that we focus

641
00:25:55,320 --> 00:25:57,960
on the narrow AI, not the general AI.

642
00:25:57,960 --> 00:25:59,500
I think those are the applications

643
00:25:59,500 --> 00:26:03,200
that are the eminent applications.

644
00:26:03,200 --> 00:26:05,420
We need to think about AI similarly

645
00:26:05,420 --> 00:26:07,860
to the way we think about data privacy.

646
00:26:07,860 --> 00:26:09,830
The AI impact assessment,
which as you may recall

647
00:26:09,830 --> 00:26:14,389
was a requirement of the 2019 bill

648
00:26:14,390 --> 00:26:15,930
that did not become law.

649
00:26:15,930 --> 00:26:17,810
I think (indistinct) is good practice

650
00:26:17,810 --> 00:26:20,610
because impact assessments
forced us to think

651
00:26:20,610 --> 00:26:23,229
proactively about what
we're doing before we start.

652
00:26:23,230 --> 00:26:24,910
And to build those considerations

653
00:26:24,910 --> 00:26:26,950
into the design of the product.

654
00:26:26,950 --> 00:26:28,920
We need to have a well-defined purpose.

655
00:26:28,920 --> 00:26:30,793
What are we trying to accomplish?

656
00:26:32,000 --> 00:26:35,350
That purpose thoroughly
through an inclusive

657
00:26:35,350 --> 00:26:37,112
and diverse set of stakeholders.

658
00:26:38,480 --> 00:26:39,610
Make sure we have the right

659
00:26:39,610 --> 00:26:41,530
competencies represented, do we have

660
00:26:41,530 --> 00:26:43,610
the right skill sets,
the right disciplines?

661
00:26:43,610 --> 00:26:46,149
Again, it's not just
about coding, engineering.

662
00:26:46,150 --> 00:26:48,530
We're not just trying to
build a better mouse trap.

663
00:26:48,530 --> 00:26:52,270
We're trying to create
the mouse as that analogy.

664
00:26:52,270 --> 00:26:54,660
We're trying to create the mind behind it.

665
00:26:54,660 --> 00:26:56,753
And when we're trying to create a mind,

666
00:26:57,920 --> 00:27:00,110
our minds are holistic.

667
00:27:00,110 --> 00:27:02,939
Our minds include liberal arts, not just

668
00:27:02,940 --> 00:27:06,400
technical arts, not just
science, not just STEM.

669
00:27:06,400 --> 00:27:09,920
If we want an AI to,
again, approximate a human,

670
00:27:09,920 --> 00:27:12,420
it needs to have those
kinds of inputs as well.

671
00:27:12,420 --> 00:27:15,230
The things that go into our
ethics include religion.

672
00:27:15,230 --> 00:27:16,490
They include literature.

673
00:27:16,490 --> 00:27:19,360
They include a range of values that inform

674
00:27:19,360 --> 00:27:23,240
our thinking in subconscious ways.

675
00:27:23,240 --> 00:27:25,440
But we need to bring those
to the conscious level

676
00:27:25,440 --> 00:27:28,100
when we're thinking
about how we program AI.

677
00:27:28,100 --> 00:27:30,570
We also need to be able
to articulate clearly

678
00:27:30,570 --> 00:27:32,693
and plainly how it works.

679
00:27:33,630 --> 00:27:36,300
It doesn't mean putting
out the source code

680
00:27:36,300 --> 00:27:37,620
that I wouldn't understand the source code

681
00:27:37,620 --> 00:27:39,110
if you put it out to me anyway.

682
00:27:39,110 --> 00:27:40,669
We need to explain what we're trying

683
00:27:40,670 --> 00:27:42,050
to do with that source code.

684
00:27:42,050 --> 00:27:45,440
What are the values that have
been programmed into the AI?

685
00:27:45,440 --> 00:27:48,120
How will the react in
the various scenarios?

686
00:27:48,120 --> 00:27:49,550
And if those...

687
00:27:49,550 --> 00:27:53,760
If that level of transparency
can't withstand the scrutiny,

688
00:27:53,760 --> 00:27:55,940
then that should cause us
to have to re-evaluate.

689
00:27:55,940 --> 00:27:57,220
Are we doing it the right way?

690
00:27:57,220 --> 00:27:59,860
And look, in some cases
there may be no right way

691
00:27:59,860 --> 00:28:02,500
because no matter what we
pick, it may be controversial,

692
00:28:02,500 --> 00:28:05,010
maybe disputed because
in many cases, society

693
00:28:05,010 --> 00:28:07,620
hasn't had to grapple
with these questions.

694
00:28:07,620 --> 00:28:09,919
But that's the process
that we have to undergo,

695
00:28:09,920 --> 00:28:11,300
if we wanna reach a place

696
00:28:11,300 --> 00:28:13,310
and our society where
people are comfortable

697
00:28:13,310 --> 00:28:15,720
ultimately with AI systems

698
00:28:15,720 --> 00:28:17,939
and whatever function
it is that we're doing.

699
00:28:17,939 --> 00:28:20,120
And why, and lastly, is there a feel safe?

700
00:28:20,120 --> 00:28:21,560
Is there a human appeal?

701
00:28:21,560 --> 00:28:23,649
I don't think that any of us

702
00:28:23,650 --> 00:28:25,370
for the foreseeable
future, are we comfortable

703
00:28:25,370 --> 00:28:27,800
with a situation where
an important situation

704
00:28:27,800 --> 00:28:30,899
an important scenarios, important systems

705
00:28:30,900 --> 00:28:32,600
whereas the mercy completely

706
00:28:32,600 --> 00:28:34,830
of an automated Artificial Intelligence

707
00:28:34,830 --> 00:28:39,240
without some ability to
appeal to a human factor.

708
00:28:39,240 --> 00:28:43,020
And that appeal needs to be
available and non-bureaucratic.

709
00:28:43,020 --> 00:28:47,010
It has to be relatively easy to access.

710
00:28:47,010 --> 00:28:48,830
Without that, I think it
will be very difficult

711
00:28:48,830 --> 00:28:51,149
to get the buy-in, that
we all need if we want AI

712
00:28:51,150 --> 00:28:54,240
to progress the way that we hope it does.

713
00:28:54,240 --> 00:28:58,560
So, I think that covers it up, covers it.

714
00:28:58,560 --> 00:29:00,129
Thank you very much for your attention.

715
00:29:00,130 --> 00:29:01,943
Look forward to your questions.

