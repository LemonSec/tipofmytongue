1
00:00:01,690 --> 00:00:03,280
- Hello, I'm Bethan Cantrell.

2
00:00:03,280 --> 00:00:05,400
And I'll be talking today
about Privacy Governance,

3
00:00:05,400 --> 00:00:08,530
Threat Modeling, and
Representation and Design.

4
00:00:08,530 --> 00:00:11,670
I'm joining RSAC from Redmond, Washington

5
00:00:11,670 --> 00:00:14,360
the traditional land
of the Duwamish people.

6
00:00:14,360 --> 00:00:16,419
While I'm going through
the presentation today

7
00:00:16,420 --> 00:00:19,370
please feel free to jump
into the comments at any time

8
00:00:19,370 --> 00:00:20,760
and ask any questions

9
00:00:20,760 --> 00:00:23,060
or provide any observations
that you may have.

10
00:00:24,535 --> 00:00:26,310
And there will probably also be time

11
00:00:26,310 --> 00:00:28,740
for more discussion at the
end of the conference as well,

12
00:00:28,740 --> 00:00:33,739
but certainly feel free to
express yourself as we go along.

13
00:00:33,810 --> 00:00:36,540
This presentation is
broken into three sections

14
00:00:36,540 --> 00:00:39,040
covering agency and privacy governance,

15
00:00:39,040 --> 00:00:41,280
personal threat modeling
and decision-making

16
00:00:41,280 --> 00:00:43,703
and representation and
design-side privacy.

17
00:00:45,500 --> 00:00:46,710
The goal of the presentation

18
00:00:46,710 --> 00:00:48,240
is not to provide end-to-end

19
00:00:48,240 --> 00:00:49,720
how to guidance on ensuring

20
00:00:49,720 --> 00:00:52,250
inclusive representational
service development.

21
00:00:52,250 --> 00:00:54,520
This is intended as a
useful thought exercise

22
00:00:54,520 --> 00:00:56,260
to help you help the data subjects

23
00:00:56,260 --> 00:00:58,670
affected by the systems you work on.

24
00:00:58,670 --> 00:01:00,160
Each section has a check-in

25
00:01:00,160 --> 00:01:02,940
that provides a summary
reframe of it subject,

26
00:01:02,940 --> 00:01:04,519
and those are listed here.

27
00:01:04,519 --> 00:01:06,119
Privacy governance is important

28
00:01:06,120 --> 00:01:07,800
because it supports agency.

29
00:01:07,800 --> 00:01:10,280
Privacy by design on
its own is insufficient

30
00:01:10,280 --> 00:01:12,803
because it doesn't know
about many of the risks.

31
00:01:13,650 --> 00:01:15,250
Risks are often complex

32
00:01:15,250 --> 00:01:16,500
but intuitive threat modeling

33
00:01:16,500 --> 00:01:19,410
can translate into simple
adaptable procedure

34
00:01:19,410 --> 00:01:21,899
and design-side privacy is representative,

35
00:01:21,900 --> 00:01:23,380
representative listening systems

36
00:01:23,380 --> 00:01:25,603
may drive more representative design.

37
00:01:27,110 --> 00:01:30,160
First, let's talk about
agency and privacy.

38
00:01:30,160 --> 00:01:32,990
Agency is a person's
independent capability

39
00:01:32,990 --> 00:01:35,369
or ability to act on their own will.

40
00:01:35,370 --> 00:01:38,320
When we describe people or
a person is marginalized,

41
00:01:38,320 --> 00:01:40,589
we are saying that they have less agency

42
00:01:40,590 --> 00:01:43,040
compared to other individuals or groups.

43
00:01:43,040 --> 00:01:44,260
As we can see from history

44
00:01:44,260 --> 00:01:46,070
and current events around the world,

45
00:01:46,070 --> 00:01:48,710
folks who are marginalized
frequently stay marginalized

46
00:01:48,710 --> 00:01:51,679
or are pushed further down because systems

47
00:01:51,680 --> 00:01:53,030
whether they're a social service,

48
00:01:53,030 --> 00:01:55,170
a shopping app or something else,

49
00:01:55,170 --> 00:01:57,130
frequently don't acknowledge the values

50
00:01:57,130 --> 00:01:59,149
or risks of marginalized people

51
00:01:59,150 --> 00:02:00,560
and don't create environments

52
00:02:00,560 --> 00:02:03,670
where people can make decisions
to protect their agency

53
00:02:03,670 --> 00:02:06,030
or support their values and goals.

54
00:02:06,030 --> 00:02:08,009
This session will
provide some connectivity

55
00:02:08,009 --> 00:02:10,789
between identity, agency and privacy

56
00:02:10,789 --> 00:02:13,400
in a context that can
help folks create systems

57
00:02:13,400 --> 00:02:15,320
that are agency protecting

58
00:02:15,320 --> 00:02:18,030
using the example of
personal threat modeling.

59
00:02:18,030 --> 00:02:20,430
The attributes that make people unique

60
00:02:20,430 --> 00:02:22,810
are relevant in many digital environments

61
00:02:22,810 --> 00:02:26,270
from employment to participation
in online communities

62
00:02:26,270 --> 00:02:29,450
whether the digital environment
or online services for work,

63
00:02:29,450 --> 00:02:32,609
government services, a social
network or something else,

64
00:02:32,610 --> 00:02:35,340
it needs to be implemented
so there is benefit to people

65
00:02:35,340 --> 00:02:38,663
without opportunity for
individual or systematic crime.

66
00:02:41,140 --> 00:02:43,670
To protect the agency, we
need to protect privacy.

67
00:02:43,670 --> 00:02:45,630
Privacy governance programs are in place

68
00:02:45,630 --> 00:02:47,490
to ensure that a company's commitments

69
00:02:47,490 --> 00:02:50,483
are implemented in a repeatable
and verifiable manner.

70
00:02:52,210 --> 00:02:53,700
Many of us work at organizations

71
00:02:53,700 --> 00:02:56,630
that have implemented systems
to ensure privacy by design

72
00:02:56,630 --> 00:02:59,440
and to operationalize the
company's published commitments

73
00:02:59,440 --> 00:03:02,030
to certain privacy
practices such as these,

74
00:03:02,030 --> 00:03:04,360
transparency choice, data minimization

75
00:03:04,360 --> 00:03:06,270
and data subject rights.

76
00:03:06,270 --> 00:03:08,660
Many such programs don't
just look at compliance

77
00:03:08,660 --> 00:03:10,590
to existing laws or policies.

78
00:03:10,590 --> 00:03:13,400
They also look for potential
harms that may not be addressed

79
00:03:13,400 --> 00:03:15,163
with existing governance controls.

80
00:03:16,130 --> 00:03:18,269
Again, though a relatively small group

81
00:03:18,270 --> 00:03:20,590
of people identifying
those potential harms

82
00:03:20,590 --> 00:03:23,020
on behalf of thousands
or millions of users

83
00:03:23,020 --> 00:03:25,860
is unlikely to be consistently effective.

84
00:03:25,860 --> 00:03:28,160
And while harm reducing requirements

85
00:03:28,160 --> 00:03:30,560
usually do get developed eventually,

86
00:03:30,560 --> 00:03:32,320
that's not always the case

87
00:03:32,320 --> 00:03:35,000
and sometimes a lot of
people are harmed first.

88
00:03:35,000 --> 00:03:37,690
Driving for a more
representative design process

89
00:03:37,690 --> 00:03:39,200
that is employing creative

90
00:03:39,200 --> 00:03:41,359
respectful methods to reduce harms

91
00:03:41,360 --> 00:03:43,010
seems like a very good way to go.

92
00:03:44,370 --> 00:03:47,500
Here, we'll review a
collection of media excerpts

93
00:03:47,500 --> 00:03:49,610
that describe various privacy failures

94
00:03:49,610 --> 00:03:50,960
where people were marginalized

95
00:03:50,960 --> 00:03:54,680
for reasons related to
race, class, diverse gender

96
00:03:54,680 --> 00:03:56,810
and straight up power asymmetry.

97
00:03:56,810 --> 00:03:59,250
Harm may have been prevented
if the decision-making

98
00:03:59,250 --> 00:04:01,423
or design group was more representative.

99
00:04:03,010 --> 00:04:05,450
In the first example, Dr. Khiara Bridges

100
00:04:05,450 --> 00:04:06,487
illustrates in the book

101
00:04:06,487 --> 00:04:08,359
"The Poverty of Privacy Rights",

102
00:04:08,360 --> 00:04:10,500
that based on the way
poor mothers are treated

103
00:04:10,500 --> 00:04:14,120
in many social service organizations,

104
00:04:14,120 --> 00:04:16,540
they don't have privacy by default.

105
00:04:16,540 --> 00:04:18,529
Expectant mothers seeking care

106
00:04:18,529 --> 00:04:20,359
are often only provided with care

107
00:04:20,360 --> 00:04:22,590
if they give up an extensive
amount of information

108
00:04:22,590 --> 00:04:24,609
about themselves, their history

109
00:04:24,610 --> 00:04:26,480
and their family circumstances

110
00:04:26,480 --> 00:04:28,410
if they don't provide that information,

111
00:04:28,410 --> 00:04:29,960
they risk not receiving the care

112
00:04:29,960 --> 00:04:33,049
and also they risk losing
access to their child

113
00:04:33,050 --> 00:04:34,660
as social services could decide

114
00:04:34,660 --> 00:04:36,460
that failure to follow through with care

115
00:04:36,460 --> 00:04:38,143
indicates an unsafe environment.

116
00:04:39,650 --> 00:04:42,440
In the second example,
an elementary school

117
00:04:42,440 --> 00:04:44,140
and its district administrators

118
00:04:44,140 --> 00:04:48,120
repeatedly decided to use an
outdoor area called the cage

119
00:04:48,120 --> 00:04:50,580
to detain a young student of color

120
00:04:50,580 --> 00:04:53,103
in full view of that student's classmates.

121
00:04:54,670 --> 00:04:58,150
And the 3rd example, eight
LA County Deputy Sheriff

122
00:04:58,150 --> 00:05:00,830
used their personal cell
phones at a crash site

123
00:05:00,830 --> 00:05:04,150
to take photographs of
the deceased family.

124
00:05:04,150 --> 00:05:06,729
Their behavior was not legally prohibited.

125
00:05:06,730 --> 00:05:11,580
And as of this article, this
presentation being developed

126
00:05:11,580 --> 00:05:15,423
that policy still had not been passed.

127
00:05:16,890 --> 00:05:18,039
In the 4th example,

128
00:05:18,040 --> 00:05:21,000
a survey asks a gender identity question

129
00:05:21,980 --> 00:05:24,880
but then provides choices
that represent assigned sex

130
00:05:24,880 --> 00:05:28,980
answers effectively erasing
some people's gender identity.

131
00:05:28,980 --> 00:05:31,990
In the 5th example,
police officers arrest,

132
00:05:31,990 --> 00:05:34,330
transport and detain a Black man

133
00:05:34,330 --> 00:05:37,030
based solely on facial recognition

134
00:05:37,030 --> 00:05:38,780
which is notoriously bad

135
00:05:38,780 --> 00:05:41,409
at identifying people
the darker their skin is.

136
00:05:41,410 --> 00:05:42,960
The man and his family went through

137
00:05:42,960 --> 00:05:44,772
entirely avoidable trauma.

138
00:05:46,010 --> 00:05:49,060
In the 6th example, we
learn about dark patterns

139
00:05:49,060 --> 00:05:50,560
where design features are used

140
00:05:50,560 --> 00:05:53,250
to manipulate data subjects and users

141
00:05:53,250 --> 00:05:56,260
and to generating more
revenue for the service.

142
00:05:56,260 --> 00:05:58,760
These articles are all linked
from the resource slide

143
00:05:58,760 --> 00:05:59,933
at the end of the deck.

144
00:06:01,320 --> 00:06:02,599
In this first section,

145
00:06:02,600 --> 00:06:04,530
we learned that agency is our ability

146
00:06:04,530 --> 00:06:06,700
to take action based on our own will.

147
00:06:06,700 --> 00:06:09,630
Privacy is an increasingly
important governance area

148
00:06:09,630 --> 00:06:11,980
because privacy protects agency.

149
00:06:11,980 --> 00:06:13,580
Privacy governance doesn't always know

150
00:06:13,580 --> 00:06:15,219
what risks to watch for

151
00:06:15,220 --> 00:06:19,070
undermining agency and creating
risk in a variety of ways.

152
00:06:19,070 --> 00:06:21,560
And now let's learn about threat modeling.

153
00:06:21,560 --> 00:06:24,300
This slide in the next few
slides include diagrams

154
00:06:24,300 --> 00:06:27,340
that are representing threat
models with different examples.

155
00:06:27,340 --> 00:06:28,669
While we walk through them,

156
00:06:28,670 --> 00:06:31,600
roadrunner will be making some decisions.

157
00:06:31,600 --> 00:06:34,970
In those same scenarios you
make different decisions based

158
00:06:34,970 --> 00:06:38,580
because you have different
lived experiences and values.

159
00:06:38,580 --> 00:06:41,070
Threat models are not
literal design explanations

160
00:06:41,070 --> 00:06:42,800
like blueprints for a house.

161
00:06:42,800 --> 00:06:44,760
There's symbolic models with indicators

162
00:06:44,760 --> 00:06:46,340
for things that need to be protected

163
00:06:46,340 --> 00:06:49,030
in the context of things
that could cause risk.

164
00:06:49,030 --> 00:06:51,179
When we see that modeling,
we can make better

165
00:06:51,180 --> 00:06:53,770
we can better understand
what mitigations to make

166
00:06:53,770 --> 00:06:55,299
for those risks.

167
00:06:55,300 --> 00:06:58,070
Engineers use threat models to
make sure they've identified

168
00:06:58,070 --> 00:07:00,870
and managed security risks during design.

169
00:07:00,870 --> 00:07:02,820
Threat models work for technical privacy

170
00:07:02,820 --> 00:07:04,500
as well as other kinds of risks

171
00:07:04,500 --> 00:07:07,640
including online safety, physical safety

172
00:07:07,640 --> 00:07:09,293
or other things as well.

173
00:07:11,000 --> 00:07:13,580
This circle made them
an orange dashed line

174
00:07:13,580 --> 00:07:15,030
represents a trust boundary.

175
00:07:15,030 --> 00:07:16,710
It indicates our opportunity

176
00:07:16,710 --> 00:07:18,900
to make a decision based on our awareness.

177
00:07:18,900 --> 00:07:20,530
And we use it pretty intuitively.

178
00:07:20,530 --> 00:07:23,179
At the park the trust
boundary may be farther out

179
00:07:23,180 --> 00:07:25,323
and on a crowded sidewalk, it's closer in.

180
00:07:27,040 --> 00:07:29,140
We use this intuition for other people

181
00:07:29,140 --> 00:07:30,870
during suspenseful movies.

182
00:07:30,870 --> 00:07:33,570
If roadrunner needed to go
through this scary looking door

183
00:07:33,570 --> 00:07:35,349
the audience would be
shouting for roadrunner

184
00:07:35,350 --> 00:07:36,623
to pay attention to,

185
00:07:37,760 --> 00:07:39,650
for instance does the door latch work

186
00:07:39,650 --> 00:07:41,770
so that the roadrunner
can get back out again?

187
00:07:41,770 --> 00:07:44,902
Or is there an anvil
suspended over the doorway?

188
00:07:46,040 --> 00:07:47,410
Because we have some awareness,

189
00:07:47,410 --> 00:07:49,300
we have an opportunity to make decisions

190
00:07:49,300 --> 00:07:50,980
based on the risks we identify

191
00:07:50,980 --> 00:07:53,097
and how those risks
weigh against our goals

192
00:07:53,097 --> 00:07:54,733
and our priorities.

193
00:07:56,820 --> 00:07:59,064
There's this idea that IT is very complex

194
00:07:59,064 --> 00:08:01,030
and we shouldn't expect data subjects

195
00:08:01,030 --> 00:08:02,729
or users to understand it.

196
00:08:02,730 --> 00:08:04,560
But the reality is the data subjects

197
00:08:04,560 --> 00:08:06,790
don't need to understand any more with IT

198
00:08:06,790 --> 00:08:09,760
than the average home owner
needs to understand plumbing.

199
00:08:09,760 --> 00:08:12,890
Modeling as a tool that can
help improve that understanding.

200
00:08:12,890 --> 00:08:14,099
We can break down the process

201
00:08:14,100 --> 00:08:16,940
for personal threat modeling
using just a few steps.

202
00:08:16,940 --> 00:08:18,740
This is a very high level set of steps.

203
00:08:18,740 --> 00:08:21,090
We could go deep into the
weeds on risk assessment

204
00:08:21,090 --> 00:08:23,643
with threat modeling, but
this is just an intro.

205
00:08:26,550 --> 00:08:29,550
First enable timely
decisions with awareness.

206
00:08:29,550 --> 00:08:32,630
Whether you're home feeling
safe at the park and relaxed

207
00:08:32,630 --> 00:08:33,539
or looking at your phone

208
00:08:33,539 --> 00:08:35,849
and considering whether
to install a new app

209
00:08:35,850 --> 00:08:37,620
or on a crowded sidewalk,

210
00:08:37,620 --> 00:08:40,890
you have some awareness
of your environment.

211
00:08:40,890 --> 00:08:43,049
The trust boundary
represents your awareness

212
00:08:43,049 --> 00:08:45,209
and starts big and then get smaller

213
00:08:46,660 --> 00:08:48,569
when we see something
that needs attention.

214
00:08:48,570 --> 00:08:51,140
In this case that same scary door

215
00:08:51,140 --> 00:08:53,372
from the last slide is
what needs attention.

216
00:08:54,350 --> 00:08:56,200
When something needs your attention,

217
00:08:56,200 --> 00:08:59,030
identify the deciding
factors for decision-making

218
00:08:59,030 --> 00:09:00,563
like whether we see an anvil.

219
00:09:01,730 --> 00:09:04,620
The 3rd step is to decide
how the potential risks

220
00:09:04,620 --> 00:09:06,670
weigh against your values or goals.

221
00:09:06,670 --> 00:09:08,000
This is also an opportunity

222
00:09:08,000 --> 00:09:11,330
to determine if there's a
way to manage those risks.

223
00:09:11,330 --> 00:09:13,190
And finally take action

224
00:09:13,190 --> 00:09:15,890
based on what you decided
in steps two and three.

225
00:09:15,890 --> 00:09:17,960
We'll also look at a sample plan

226
00:09:18,970 --> 00:09:21,642
for how folks can decide to take action

227
00:09:21,642 --> 00:09:23,193
in the next few slides.

228
00:09:25,870 --> 00:09:27,983
Different people will
have different outcomes

229
00:09:27,983 --> 00:09:29,410
to the same issues

230
00:09:29,410 --> 00:09:31,170
because they have
different values and goals

231
00:09:31,170 --> 00:09:32,339
and that's okay.

232
00:09:32,340 --> 00:09:33,940
This is an opportunity for people

233
00:09:33,940 --> 00:09:36,853
to make their own decisions
about how they manage risks.

234
00:09:39,680 --> 00:09:42,209
In this example model, roadrunner
wants to plan their day

235
00:09:42,210 --> 00:09:43,910
so they look for a horse, go back.

236
00:09:45,020 --> 00:09:46,819
Here's the trust boundary circle again,

237
00:09:46,820 --> 00:09:48,210
a dashed orange line.

238
00:09:48,210 --> 00:09:50,160
Because roadrunner cares about privacy

239
00:09:50,160 --> 00:09:53,160
they are being aware and
paying attention to the app.

240
00:09:53,160 --> 00:09:54,770
When roadrunner looks at the app,

241
00:09:54,770 --> 00:09:57,500
they look at the information the app wants

242
00:09:57,500 --> 00:09:59,740
that's listed in the UX and then noticed

243
00:09:59,740 --> 00:10:02,900
and they compare that to
what the app is used for.

244
00:10:02,900 --> 00:10:04,829
In this intentionally silly example

245
00:10:04,830 --> 00:10:06,610
the app wants roadrunners DNA

246
00:10:06,610 --> 00:10:09,940
in order to provide an
extra accurate horoscope.

247
00:10:09,940 --> 00:10:11,900
roadrunner decides to use a different app.

248
00:10:11,900 --> 00:10:14,920
This example model is
evaluating a messaging service

249
00:10:14,920 --> 00:10:16,310
from an end-to-end perspective

250
00:10:16,310 --> 00:10:20,619
because roadrunner wants to
use peer-to-peer messaging

251
00:10:20,620 --> 00:10:23,410
and the confidence and
also wants the confidence

252
00:10:23,410 --> 00:10:24,959
that comes from a paid service.

253
00:10:26,720 --> 00:10:28,380
roadrunner expects that there will be

254
00:10:28,380 --> 00:10:30,610
more data moving around
because there's payments

255
00:10:30,610 --> 00:10:33,240
and also learns about code
of conduct requirements

256
00:10:33,240 --> 00:10:35,980
when they read the privacy notice.

257
00:10:35,980 --> 00:10:38,500
Data subjects can learn about
those additional data flows

258
00:10:38,500 --> 00:10:42,220
from looking at the notice
and the user experience

259
00:10:42,220 --> 00:10:44,170
and also reading the privacy statement.

260
00:10:45,740 --> 00:10:49,360
Having a more extensive
data flow means that

261
00:10:49,360 --> 00:10:52,040
roadrunner needs to trust
more of that service

262
00:10:52,040 --> 00:10:54,589
to keep their data safe.

263
00:10:54,590 --> 00:10:57,160
That means that they need
to have some expectation

264
00:10:57,160 --> 00:10:59,890
that all of the components
of the data flows

265
00:10:59,890 --> 00:11:01,640
have been appropriately protected

266
00:11:02,610 --> 00:11:04,713
and that the organization has the maturity

267
00:11:04,713 --> 00:11:07,219
and the integrity to have those kinds

268
00:11:07,220 --> 00:11:09,033
of systematic protections in place.

269
00:11:10,410 --> 00:11:12,160
This more extensive example model

270
00:11:12,160 --> 00:11:13,500
is looking at value goals

271
00:11:13,500 --> 00:11:17,030
that go along with a
complex social network

272
00:11:17,030 --> 00:11:19,790
rather than at technical privacy issues.

273
00:11:19,790 --> 00:11:22,750
Roadrunner wants to rejoin
a popular social network

274
00:11:22,750 --> 00:11:25,330
that processes a lot of
personal information.

275
00:11:25,330 --> 00:11:28,530
Roadrunner evaluates description
of social network, UX

276
00:11:28,530 --> 00:11:31,370
and lands on five
concern areas to plan for

277
00:11:31,370 --> 00:11:34,430
misinformation, location
privacy and safety,

278
00:11:34,430 --> 00:11:38,219
conformity, bigotry and group privacy.

279
00:11:38,220 --> 00:11:40,390
Again, the trust boundary
indicates awareness

280
00:11:40,390 --> 00:11:43,020
and the opportunity to
look for information

281
00:11:43,020 --> 00:11:45,490
before making a decision and acting on it.

282
00:11:45,490 --> 00:11:46,890
We'll look at roadrunner's plan

283
00:11:46,890 --> 00:11:49,263
to address these concerns
in the next slide.

284
00:11:51,420 --> 00:11:53,860
Roadrunner has come up with
a set of dos and don'ts

285
00:11:53,860 --> 00:11:56,460
for the risks identified
in the prior slide

286
00:11:56,460 --> 00:11:58,640
and we can see them organized here.

287
00:11:58,640 --> 00:12:01,040
Some of the items on these lists

288
00:12:01,040 --> 00:12:05,860
may have account or profile
controls to support them.

289
00:12:05,860 --> 00:12:08,610
Others rely on roadrunner
to proactively decide

290
00:12:08,610 --> 00:12:12,100
how to act on his values, such
as in the conformity list,

291
00:12:12,100 --> 00:12:15,570
which says do connect to
a diverse set of people

292
00:12:15,570 --> 00:12:17,950
and don't create a homogenous bubble.

293
00:12:17,950 --> 00:12:21,070
Or the location privacy
and safety list which says,

294
00:12:21,070 --> 00:12:24,420
don't post information about
travel away from home or town,

295
00:12:24,420 --> 00:12:25,853
at least until you return.

296
00:12:27,710 --> 00:12:29,060
Closing out the section,

297
00:12:29,060 --> 00:12:31,790
we learned the threat models
are a simple design tool

298
00:12:31,790 --> 00:12:34,099
for clarifying and managing risk.

299
00:12:34,100 --> 00:12:36,250
We can apply threat
models to more abstract

300
00:12:36,250 --> 00:12:39,900
or complicated scenarios
like IT systems or policy,

301
00:12:39,900 --> 00:12:42,060
we can threat model for
different types of risks

302
00:12:42,060 --> 00:12:43,189
in different environments

303
00:12:43,190 --> 00:12:46,173
especially complicated
environments like social networks.

304
00:12:47,180 --> 00:12:50,319
In the next section, we'll
discuss representation,

305
00:12:50,320 --> 00:12:52,400
privacy and design.

306
00:12:52,400 --> 00:12:55,500
Privacy by design continues
to be a necessary component

307
00:12:55,500 --> 00:12:57,230
of trustworthy data governance.

308
00:12:57,230 --> 00:12:58,940
However, earlier in this deck,

309
00:12:58,940 --> 00:13:02,910
we saw six broadly varied
examples of privacy failures

310
00:13:02,910 --> 00:13:04,010
that could have happened

311
00:13:04,010 --> 00:13:07,250
even when relevant privacy
controls are in place.

312
00:13:07,250 --> 00:13:09,720
That's where design side privacy comes in.

313
00:13:09,720 --> 00:13:12,700
Design-side privacy combines
diverse representation,

314
00:13:12,700 --> 00:13:14,580
knowledge of technical components

315
00:13:14,580 --> 00:13:16,760
and adherence to privacy principles.

316
00:13:16,760 --> 00:13:18,580
With the goal of ensuring data subjects

317
00:13:18,580 --> 00:13:21,430
don't experience harm, or loss of agency

318
00:13:21,430 --> 00:13:23,173
as a result of using the service.

319
00:13:24,850 --> 00:13:27,590
This chart shows the
relationship between risk

320
00:13:27,590 --> 00:13:29,130
introduced by a service

321
00:13:29,130 --> 00:13:32,370
and the critical considerations
for the design team.

322
00:13:32,370 --> 00:13:34,940
Can the data subjects
understand the notice?

323
00:13:34,940 --> 00:13:37,510
Is it clear how to manage choices?

324
00:13:37,510 --> 00:13:39,840
As the service types become higher risk,

325
00:13:39,840 --> 00:13:43,850
does the data subject know how
to protect their experience?

326
00:13:43,850 --> 00:13:45,640
Do they know how to manage their choices

327
00:13:45,640 --> 00:13:47,730
within the service environment?

328
00:13:47,730 --> 00:13:50,130
For services where the
data subjects agency

329
00:13:50,130 --> 00:13:53,110
may be directly effected
as with social services

330
00:13:53,110 --> 00:13:55,850
or other projects focused
on marginalized people,

331
00:13:55,850 --> 00:13:58,550
do the potential harms outweigh the value,

332
00:13:58,550 --> 00:14:01,199
have all the (indistinct)

333
00:14:03,210 --> 00:14:05,690
been fully mitigated, or
is there any responsibility

334
00:14:05,690 --> 00:14:09,030
on the data subject to manage some risk?

335
00:14:09,030 --> 00:14:10,930
Advocacy organizations do great work

336
00:14:10,930 --> 00:14:12,959
representing their members

337
00:14:12,960 --> 00:14:14,863
in various environments, including IT.

338
00:14:15,720 --> 00:14:18,750
Disability rights
advocates have a motto of,

339
00:14:18,750 --> 00:14:20,980
nothing about us without us

340
00:14:20,980 --> 00:14:22,610
that aligns closely with the principles

341
00:14:22,610 --> 00:14:24,580
of the design justice network.

342
00:14:24,580 --> 00:14:25,990
The design justice network

343
00:14:27,080 --> 00:14:29,440
continues to develop and
evangelize design principles

344
00:14:29,440 --> 00:14:31,530
that work to reduce
marginalization and harm

345
00:14:31,530 --> 00:14:33,170
across many different types

346
00:14:33,170 --> 00:14:35,033
of service and project environments.

347
00:14:37,070 --> 00:14:38,380
When a project may have tens

348
00:14:38,380 --> 00:14:40,530
of thousands or even millions of users

349
00:14:40,530 --> 00:14:42,170
the design team cannot be diverse enough

350
00:14:42,170 --> 00:14:45,060
to represent the needs of all
the users or data subjects.

351
00:14:45,060 --> 00:14:47,729
There are systems in place
to help scale these already,

352
00:14:47,730 --> 00:14:50,450
such as accessibility,
governance programs,

353
00:14:50,450 --> 00:14:53,080
privacy programs and others,

354
00:14:53,080 --> 00:14:55,170
but developing listening systems

355
00:14:55,170 --> 00:14:57,209
so that the service in no way

356
00:14:57,210 --> 00:15:00,510
undermines their values or
safety can only be a good thing

357
00:15:00,510 --> 00:15:03,530
especially for services
that are necessary for use

358
00:15:03,530 --> 00:15:08,173
like public schools, banking,
health, or social services.

359
00:15:09,740 --> 00:15:11,940
For a person to be
required to use a service

360
00:15:11,940 --> 00:15:13,523
and for that service to do harm

361
00:15:13,523 --> 00:15:16,680
is a harsh reality that
many people face today.

362
00:15:16,680 --> 00:15:19,140
For this reason if a high-stakes service

363
00:15:19,140 --> 00:15:20,699
is critical to your business,

364
00:15:20,700 --> 00:15:23,863
diverse representation is also
critical to your business.

365
00:15:25,100 --> 00:15:27,530
There's a lot we can
learn from data subjects.

366
00:15:27,530 --> 00:15:29,410
In this case, the data subject

367
00:15:29,410 --> 00:15:32,100
is relying on notice and
their own prior experience

368
00:15:32,100 --> 00:15:35,340
with the service to
develop a plan of action

369
00:15:35,340 --> 00:15:37,050
for how to use the social network

370
00:15:37,050 --> 00:15:39,699
while also supporting
their own values and goals.

371
00:15:39,700 --> 00:15:43,250
Roadrunners dos and don'ts
could be used to inform design

372
00:15:43,250 --> 00:15:45,290
for privacy and media controls

373
00:15:45,290 --> 00:15:47,030
and used to support the narrative

374
00:15:47,030 --> 00:15:50,020
of why those changes are
beneficial to a larger group

375
00:15:50,020 --> 00:15:52,060
rather than just to roadrunner.

376
00:15:52,060 --> 00:15:53,880
Going back to some of the examples

377
00:15:53,880 --> 00:15:55,930
we saw of privacy failures,

378
00:15:55,930 --> 00:15:57,640
how might data subjects have helped

379
00:15:57,640 --> 00:16:01,460
inform more respectful
services for expectant mothers,

380
00:16:01,460 --> 00:16:04,140
more care in the treatment
of kids in schools

381
00:16:04,140 --> 00:16:06,713
or in the use of gender
identity and surveys?

382
00:16:08,520 --> 00:16:10,800
As I noted earlier in the presentation

383
00:16:10,800 --> 00:16:12,829
this is not a how to, or a case study

384
00:16:12,830 --> 00:16:14,480
for setting up a listening system

385
00:16:14,480 --> 00:16:17,850
based on a representative
number of personal threat models

386
00:16:17,850 --> 00:16:19,630
for various service environments.

387
00:16:19,630 --> 00:16:21,620
The goal has been to frame a clear message

388
00:16:21,620 --> 00:16:23,960
that people from

389
00:16:23,960 --> 00:16:25,250
people benefit from knowing

390
00:16:25,250 --> 00:16:27,690
how to use modeling for decision-making

391
00:16:27,690 --> 00:16:30,840
and that it's beneficial for organizations

392
00:16:30,840 --> 00:16:33,030
to have this kind of insight

393
00:16:33,030 --> 00:16:35,160
from a representative
cross sample of people

394
00:16:35,160 --> 00:16:37,610
so they can build a service
that doesn't do harm.

395
00:16:39,730 --> 00:16:41,750
In this section, we learned that

396
00:16:41,750 --> 00:16:44,210
representative design
becomes more important

397
00:16:44,210 --> 00:16:46,820
when a service of any type
becomes more sensitive

398
00:16:46,820 --> 00:16:49,430
and or critical to safety and wellness.

399
00:16:49,430 --> 00:16:51,160
Information from the listening system

400
00:16:51,160 --> 00:16:53,209
must augment the existing expertise

401
00:16:53,210 --> 00:16:55,040
of our own diverse design,

402
00:16:55,040 --> 00:16:58,150
development, legal and governance teams.

403
00:16:58,150 --> 00:16:59,949
How a listening system is designed

404
00:16:59,950 --> 00:17:02,360
can vary based on the
implementing organization

405
00:17:02,360 --> 00:17:03,720
and the participants

406
00:17:03,720 --> 00:17:06,650
we just need to make sure it's respectful.

407
00:17:06,650 --> 00:17:09,550
How might a data subject
driven listening system

408
00:17:09,550 --> 00:17:10,883
look for your projects?

409
00:17:13,569 --> 00:17:16,659
Thank you for your time
during this discussion.

