1
00:00:01,360 --> 00:00:03,660
- Hello, and welcome to our talk,

2
00:00:03,660 --> 00:00:05,290
Securing Tesla and Mobileye

3
00:00:05,290 --> 00:00:07,820
from Split-Second Phantom Attacks.

4
00:00:07,820 --> 00:00:09,100
This is a joint talk

5
00:00:09,100 --> 00:00:11,680
and I will present the
first part of the talk.

6
00:00:11,680 --> 00:00:12,700
My name is Ben Nassi

7
00:00:12,700 --> 00:00:15,719
I'm a PhD student of Ben-Gurion
University of the Negev.

8
00:00:15,720 --> 00:00:17,670
And the second part of
the talk will be presented

9
00:00:17,670 --> 00:00:22,550
by Dr. Yisroel Mirsky, a
postdoctoral fellow from Georgia

10
00:00:22,550 --> 00:00:23,550
from Georgia Tech.

11
00:00:24,610 --> 00:00:27,850
Okay, now this talk is
based on three papers

12
00:00:27,850 --> 00:00:32,750
that were published at CCS
and at AutoSec21 Workshop

13
00:00:32,750 --> 00:00:34,483
and you can find them online.

14
00:00:37,340 --> 00:00:41,730
The first paper which is
named Phantom of the ADAS

15
00:00:41,730 --> 00:00:46,730
was presented at CCS and it
consists most of the information

16
00:00:46,760 --> 00:00:48,360
that is provided in this sector.

17
00:00:51,510 --> 00:00:52,830
So let's start to discuss about

18
00:00:52,830 --> 00:00:55,540
perception of computerized
driver intelligence.

19
00:00:55,540 --> 00:00:57,987
And virtual perception is the ability

20
00:00:57,987 --> 00:01:01,130
to interpret the surrounding
physical environment

21
00:01:01,130 --> 00:01:04,083
using algorithms that obtain
data from various sensors.

22
00:01:08,530 --> 00:01:11,750
And virtual perception of
semi or fully-autonomous cars

23
00:01:11,750 --> 00:01:14,800
usually consists of
algorithms and hardware

24
00:01:14,800 --> 00:01:17,679
structured in four layers.

25
00:01:17,680 --> 00:01:19,480
And as you can see on the bottom

26
00:01:19,480 --> 00:01:24,337
there is a picture taken
from the dashboard of Tesla

27
00:01:24,337 --> 00:01:27,180
with full self driving functionality.

28
00:01:27,180 --> 00:01:30,039
Now let's understand exactly
how virtual perception,

29
00:01:30,039 --> 00:01:32,780
the layers of virtual perception.

30
00:01:32,780 --> 00:01:36,880
Now the lowest layer is
the layer of the sensors.

31
00:01:36,880 --> 00:01:40,830
Sensors are RADAR, video cameras,

32
00:01:40,830 --> 00:01:45,590
ultrasonic sensors,
LiDARS and GPS sensors.

33
00:01:45,590 --> 00:01:50,040
On top of this layer, sorry

34
00:01:50,040 --> 00:01:53,130
Sensor fusion is usually the
most common used approach

35
00:01:53,130 --> 00:01:55,830
that is being used by the car industry,

36
00:01:55,830 --> 00:01:57,730
and the reasons are is that,

37
00:01:57,730 --> 00:02:00,800
sensor fusion is used
to increase accuracy,

38
00:02:00,800 --> 00:02:02,780
provide redundancy,

39
00:02:02,780 --> 00:02:04,960
and increase the overall robustness

40
00:02:04,960 --> 00:02:06,883
of the semi or fully-autonomous car.

41
00:02:08,940 --> 00:02:11,380
And the combination of sensors used

42
00:02:11,380 --> 00:02:14,380
varies among car manufacturers.

43
00:02:14,380 --> 00:02:18,380
For example, Tesla includes
front facing RADAR,

44
00:02:18,380 --> 00:02:21,180
a set of video cameras, and
a set of ultrasonic sensors.

45
00:02:22,320 --> 00:02:25,079
Yandex self-driving
cars on the other hand,

46
00:02:25,080 --> 00:02:30,080
include RADAR, LiDAR, and
a set of video cameras.

47
00:02:32,340 --> 00:02:34,733
Now on top of this layer,

48
00:02:34,733 --> 00:02:36,853
that is the sensor refinement layer,

49
00:02:39,718 --> 00:02:41,415
which consists of algorithms

50
00:02:41,415 --> 00:02:43,549
that are used for filtering, and alignment

51
00:02:43,550 --> 00:02:47,493
and something that signal
processing is being done.

52
00:02:48,600 --> 00:02:49,433
On the top of this layer,

53
00:02:49,433 --> 00:02:51,549
that's the object refinement layer.

54
00:02:51,550 --> 00:02:55,003
I will speak about object
detectors in two slides from now.

55
00:02:56,670 --> 00:03:00,920
And the last layer in perception
is the situation refinement

56
00:03:00,920 --> 00:03:04,420
which mainly consists
of assessment algorithms

57
00:03:04,420 --> 00:03:06,480
which intended to

58
00:03:07,620 --> 00:03:10,610
assess the intentions
of the nearby objects,

59
00:03:10,610 --> 00:03:15,209
for example the pedestrian,
or the nearby house.

60
00:03:15,209 --> 00:03:17,000
And virtual perception
as I mentioned before

61
00:03:17,000 --> 00:03:19,383
consists of these four layers.

62
00:03:20,240 --> 00:03:24,183
Hardware and algorithms
are combined together.

63
00:03:26,850 --> 00:03:30,739
Now the research around virtual perception

64
00:03:30,740 --> 00:03:33,223
started about 40 years ago,

65
00:03:35,210 --> 00:03:38,880
on the lefted side of
the timeline you can see

66
00:03:38,880 --> 00:03:41,190
the prototype, which was a Terregator,

67
00:03:41,190 --> 00:03:42,590
which is a road following robot

68
00:03:42,590 --> 00:03:44,393
that was developed at CMU.

69
00:03:46,120 --> 00:03:48,640
40 years later, basically today,

70
00:03:48,640 --> 00:03:51,190
are commercial semi-autonomous cars

71
00:03:51,190 --> 00:03:53,960
wherein they love commercial
semi-autonomous cars.

72
00:03:53,960 --> 00:03:56,710
Teslas are about to acquire
level three capabilities

73
00:03:56,710 --> 00:03:58,236
with the addition of the upcoming

74
00:03:58,236 --> 00:04:00,773
full self-driving functionality.

75
00:04:00,773 --> 00:04:01,620
And somewhere in the future,

76
00:04:01,620 --> 00:04:04,600
we are about to see fully-autonomous cars

77
00:04:04,600 --> 00:04:08,040
that drive without any manual involvement.

78
00:04:08,040 --> 00:04:09,066
Now, I think that you will agree with me

79
00:04:09,067 --> 00:04:12,620
that virtual perception
has improved significantly

80
00:04:12,620 --> 00:04:14,343
over the last 40 years.

81
00:04:17,420 --> 00:04:18,660
Now don't get me wrong,

82
00:04:18,660 --> 00:04:23,130
there are still open challenges
for virtual perception.

83
00:04:23,130 --> 00:04:27,909
One example is acquiring level
four and five capabilities.

84
00:04:27,910 --> 00:04:31,276
For example, driving in
unknown or unmapped areas

85
00:04:31,276 --> 00:04:35,520
is still considered a complex task.

86
00:04:35,520 --> 00:04:38,450
Also, the signs needs to find a way

87
00:04:38,450 --> 00:04:40,659
to overcome some perceptual challenges

88
00:04:40,660 --> 00:04:43,690
and some well-known fatal
accidents demonstrates

89
00:04:44,995 --> 00:04:47,903
the perceptual challenges
that ADAS still have.

90
00:04:49,100 --> 00:04:53,060
In these two well-known fatal accidents,

91
00:04:53,060 --> 00:04:56,450
that's a autopilot drive
directly and crashed

92
00:04:57,958 --> 00:05:01,920
to parked cars that park
on the side of the road

93
00:05:01,920 --> 00:05:06,393
and this picture is rather
taken from the accident.

94
00:05:08,300 --> 00:05:10,690
Now also the are the
adversarial AI attacks

95
00:05:10,690 --> 00:05:13,480
which also emphasize the difference

96
00:05:13,480 --> 00:05:17,650
in the perceptual challenges
that they are currently facing.

97
00:05:20,640 --> 00:05:24,719
And I want to discuss
about adversarial attacks.

98
00:05:24,720 --> 00:05:28,150
And adversarial attacks in our context

99
00:05:28,150 --> 00:05:32,640
are physical objects that are
perceived differently by AI

100
00:05:34,710 --> 00:05:36,363
than by humans.

101
00:05:37,645 --> 00:05:39,030
And previous studies showed

102
00:05:39,030 --> 00:05:44,030
that by adding a physical
artifact to an existing object

103
00:05:44,244 --> 00:05:46,615
it causes the object
detector to misclassify

104
00:05:46,615 --> 00:05:48,215
the detected object.

105
00:05:48,215 --> 00:05:50,479
And it was done by adding stickers,

106
00:05:50,480 --> 00:05:52,473
it was done by adding graffiti,

107
00:05:52,473 --> 00:05:56,910
it was done by printing road
signs with blurred color

108
00:05:56,910 --> 00:06:00,443
as can see in the picture
on the right side.

109
00:06:02,630 --> 00:06:06,757
However, we are hearing
about adversarial attacks

110
00:06:06,757 --> 00:06:07,930
for the last five years

111
00:06:07,930 --> 00:06:09,610
and immediate question that rises is

112
00:06:09,610 --> 00:06:14,300
why haven't adversarial attacks
been observed in the wild?

113
00:06:14,300 --> 00:06:17,160
And we tried to think
about several reasons

114
00:06:17,160 --> 00:06:19,947
about the disadvantage
of adversarial attacks,

115
00:06:19,947 --> 00:06:22,087
and one reason is that,

116
00:06:22,087 --> 00:06:23,875
is because that adversarial attacks

117
00:06:23,875 --> 00:06:26,720
require the attackers to
approach the attack scene,

118
00:06:26,720 --> 00:06:30,640
which is something that
attackers would like to avoid.

119
00:06:30,640 --> 00:06:31,539
Also some of them,

120
00:06:31,540 --> 00:06:34,683
several attacks required
skilled attackers,

121
00:06:35,630 --> 00:06:39,409
some of the attacks require understanding

122
00:06:39,410 --> 00:06:41,430
or deep understanding data science

123
00:06:42,670 --> 00:06:46,560
which mainly limits the amount,

124
00:06:46,560 --> 00:06:49,954
limits the people that
the type of the attack

125
00:06:49,954 --> 00:06:52,287
that can apply this attacks.

126
00:06:53,503 --> 00:06:57,909
Other attacks require are
basically white-box attacks,

127
00:06:57,910 --> 00:06:59,010
which means that they demand

128
00:06:59,010 --> 00:07:01,250
full knowledge of the attack model.

129
00:07:02,640 --> 00:07:05,190
Other attacks need forensic
evidence at the attack scenes,

130
00:07:05,190 --> 00:07:09,130
for example stickers may help the police

131
00:07:09,130 --> 00:07:11,043
to find the attacker.

132
00:07:12,860 --> 00:07:15,174
And also, some of the attacks

133
00:07:15,174 --> 00:07:17,270
require complicated preparation,

134
00:07:17,270 --> 00:07:21,854
and for those of you who
have trained a human network

135
00:07:21,855 --> 00:07:23,410
or an adversarial human network

136
00:07:23,410 --> 00:07:27,410
is somehow a very complicated preparation

137
00:07:27,410 --> 00:07:30,310
in order to train a model that can output

138
00:07:30,310 --> 00:07:32,313
an adversarial instance.

139
00:07:35,040 --> 00:07:37,350
Now, I want us to spend few slides

140
00:07:38,287 --> 00:07:39,830
in order to understand how computer vision

141
00:07:39,830 --> 00:07:43,260
identify objects in a picture.

142
00:07:43,260 --> 00:07:46,360
And object detectors are algorithms

143
00:07:46,360 --> 00:07:47,750
which are used to identify the location

144
00:07:47,750 --> 00:07:51,390
of instances of objects
of a certain class.

145
00:07:51,390 --> 00:07:54,219
This can be pedestrians,
it can be road signs,

146
00:07:54,220 --> 00:07:56,087
it can be, for example a car.

147
00:07:57,630 --> 00:08:01,800
And object detectors
receive frames as input

148
00:08:01,800 --> 00:08:04,270
and output a set of bounding boxes

149
00:08:04,270 --> 00:08:07,323
with classifications
of associated objects.

150
00:08:08,400 --> 00:08:13,162
And in this slide you
can see on the left side,

151
00:08:13,163 --> 00:08:15,054
a pedestrian object detector.

152
00:08:15,054 --> 00:08:18,256
In the middle you can see a car detector

153
00:08:18,256 --> 00:08:22,839
and on the right side you
can see a road sign detector.

154
00:08:26,005 --> 00:08:29,453
And object detected starts
with a stream of frames,

155
00:08:29,453 --> 00:08:32,620
and it was indicated before,

156
00:08:32,620 --> 00:08:35,970
the object detector
receives frame as an input

157
00:08:35,970 --> 00:08:38,929
and boundaries of objects are detected

158
00:08:38,929 --> 00:08:42,829
and suppressed to remove
duplications of the same object.

159
00:08:42,830 --> 00:08:44,687
Now, this component is mainly

160
00:08:44,687 --> 00:08:48,310
idealized on several architecture

161
00:08:48,310 --> 00:08:50,703
for example in YOLO's
that famous architecture,

162
00:08:51,970 --> 00:08:54,190
there are some others
interesting architecture

163
00:08:54,190 --> 00:08:59,190
that have been used for the,
to detect object inside a frame

164
00:09:00,600 --> 00:09:03,250
and there is an algorithm
which is used to suppress

165
00:09:04,350 --> 00:09:07,970
or to remove the duplications
of the same object

166
00:09:07,970 --> 00:09:11,160
which is followed by the object detector.

167
00:09:11,160 --> 00:09:13,829
However, these two components are very

168
00:09:13,830 --> 00:09:15,973
or well-known to most of the people.

169
00:09:18,090 --> 00:09:20,540
There is another component which is called

170
00:09:20,540 --> 00:09:22,030
the persistency function.

171
00:09:22,030 --> 00:09:25,709
This component is not exactly
known to most of the people.

172
00:09:25,710 --> 00:09:28,060
And a persistency function is a component

173
00:09:28,060 --> 00:09:31,439
which is used to decrease
false positive detections

174
00:09:31,440 --> 00:09:34,390
by verifying the existence
of a detected object

175
00:09:34,390 --> 00:09:36,600
in the few consecutive frames.

176
00:09:36,600 --> 00:09:39,330
And a persistency
function works as follows.

177
00:09:39,330 --> 00:09:41,450
Given a detected object,

178
00:09:41,450 --> 00:09:45,822
it verifies whether
its duration is greater

179
00:09:45,822 --> 00:09:48,262
than a pre-defined appearance duration.

180
00:09:48,262 --> 00:09:50,213
Then if its duration is greater

181
00:09:50,213 --> 00:09:52,620
than the pre-defined appearance duration,

182
00:09:52,620 --> 00:09:55,120
then the persistency function

183
00:09:55,120 --> 00:09:57,133
accepts the object is a real object,

184
00:10:00,252 --> 00:10:01,930
it classify as a real object,

185
00:10:01,930 --> 00:10:03,689
otherwise the object is rejected

186
00:10:03,690 --> 00:10:05,583
and considered as false positive.

187
00:10:06,950 --> 00:10:10,412
Now, as you can understand

188
00:10:10,412 --> 00:10:12,280
using a higher appearance threshold,

189
00:10:12,280 --> 00:10:14,800
will result in a lower false positive rate

190
00:10:14,800 --> 00:10:15,969
by the object detector

191
00:10:15,970 --> 00:10:19,822
and this is a commonly used practice

192
00:10:19,822 --> 00:10:21,513
in the area of time series.

193
00:10:22,600 --> 00:10:26,030
You can set the desired
false positive rate

194
00:10:26,030 --> 00:10:28,443
by increasing the appearance threshold.

195
00:10:30,250 --> 00:10:34,060
However, in our context,

196
00:10:34,060 --> 00:10:36,329
real-time driving requires that objects,

197
00:10:36,330 --> 00:10:38,030
for example pedestrians,

198
00:10:38,030 --> 00:10:40,329
be detected as quickly as possible,

199
00:10:40,330 --> 00:10:43,460
so that the car immediately react to them.

200
00:10:43,460 --> 00:10:45,290
So you have a tradeoff point,

201
00:10:45,290 --> 00:10:49,569
where the appearance threshold
is set by the manufacturer,

202
00:10:49,570 --> 00:10:50,710
based on its experience,

203
00:10:50,710 --> 00:10:53,550
by taking both constraints into account.

204
00:10:54,575 --> 00:10:55,750
The appearance threshold must decrease

205
00:10:55,750 --> 00:10:57,317
the false positive rate,

206
00:10:57,317 --> 00:11:00,580
but it should be short
enough to detect objects

207
00:11:00,580 --> 00:11:03,873
and in our case, it's
milliseconds for driving cars.

208
00:11:09,033 --> 00:11:12,630
Okay. Now only objects that
appear for a period of time

209
00:11:12,630 --> 00:11:15,220
greater than the appearance
threshold are considered

210
00:11:15,220 --> 00:11:16,800
by the persistency function,

211
00:11:16,800 --> 00:11:20,344
and they are object that
appear in the detected area,

212
00:11:20,345 --> 00:11:23,428
and see in here, it's the green area.

213
00:11:26,084 --> 00:11:28,603
And we ask this question,

214
00:11:28,604 --> 00:11:30,495
can we apply a split-second attack

215
00:11:30,495 --> 00:11:32,844
that appears in the detected area

216
00:11:32,844 --> 00:11:35,050
but it is imperceptible to the human eye?

217
00:11:35,050 --> 00:11:36,859
Or in other words,

218
00:11:36,860 --> 00:11:38,140
can we exploit the time domain

219
00:11:38,140 --> 00:11:40,573
by applying an attack with a duration

220
00:11:40,573 --> 00:11:43,670
that is slightly longer than
the appearance threshold?

221
00:11:43,670 --> 00:11:44,949
And this is where we will start

222
00:11:44,950 --> 00:11:47,613
to discuss about
split-second phantom attacks.

223
00:11:48,750 --> 00:11:51,100
Now split-second phantom attack

224
00:11:51,100 --> 00:11:53,356
is an attack where the attacker presents

225
00:11:53,356 --> 00:11:57,270
a phantom object to a
car for a split second,

226
00:11:57,270 --> 00:12:00,964
and this phantom triggers
an undesired reaction

227
00:12:00,964 --> 00:12:01,797
from the car.

228
00:12:01,797 --> 00:12:05,250
Now this can be done by
projecting the phantom object

229
00:12:05,250 --> 00:12:08,030
from a drone, or this can be done

230
00:12:08,030 --> 00:12:11,689
by hacking an internet
connected digital billboard

231
00:12:11,690 --> 00:12:15,463
and embedding a phantom
object into an advertisement

232
00:12:15,463 --> 00:12:17,382
into an existing advertisement.

233
00:12:19,300 --> 00:12:21,790
And the significance of
this attack with respect

234
00:12:21,790 --> 00:12:23,910
to the previously suggested

235
00:12:25,195 --> 00:12:28,076
or discussed adversarial attacks is that,

236
00:12:28,076 --> 00:12:30,727
split-second phantom attacks
can be applied remotely,

237
00:12:30,727 --> 00:12:32,270
and do not require the attacker

238
00:12:32,270 --> 00:12:33,720
to approach the attack scene.

239
00:12:34,860 --> 00:12:37,663
They also do not require
special expertise.

240
00:12:38,900 --> 00:12:41,372
They do not rely on white-box approach.

241
00:12:42,820 --> 00:12:47,090
They do not leave any
evidence at the attack scene.

242
00:12:47,090 --> 00:12:50,090
They do not require any
complex preparation.

243
00:12:50,090 --> 00:12:53,004
And finally, as opposed to
at the adversarial attacks

244
00:12:53,004 --> 00:12:55,410
that exploit the space domain,

245
00:12:55,410 --> 00:12:58,677
split-second phantom attacks
exploit the time domain.

246
00:13:01,960 --> 00:13:04,670
Now, why does a commercial ADAS,

247
00:13:04,670 --> 00:13:08,300
advanced driver-assistance
system detect phantom object?

248
00:13:08,300 --> 00:13:10,416
Now this is an experiment that we did,

249
00:13:11,306 --> 00:13:15,790
we projected a road sign on a tree,

250
00:13:15,790 --> 00:13:20,122
you can see in the
picture on the left side,

251
00:13:21,160 --> 00:13:23,620
and we placed a car
which was equipped with

252
00:13:23,620 --> 00:13:27,950
Mobileye 630 in front of
this projected road sign.

253
00:13:27,950 --> 00:13:30,200
Now, here is an interesting fact.

254
00:13:30,200 --> 00:13:34,270
Mobileye 630 PRO considers
the projected road sign

255
00:13:34,270 --> 00:13:36,297
a real speed limit sign.

256
00:13:37,740 --> 00:13:39,190
And some of you are
probably not that surprised

257
00:13:39,190 --> 00:13:43,123
because Mobileye 630 PRO
consists only of a video camera.

258
00:13:44,800 --> 00:13:47,890
Now we made another experiment.

259
00:13:47,890 --> 00:13:50,990
We took a picture of Mr Elon Musk

260
00:13:50,990 --> 00:13:52,440
and projected it on the road.

261
00:13:53,500 --> 00:13:57,740
And we placed Tesla Model X
in front of this projection.

262
00:13:57,740 --> 00:14:00,430
Now here is an interesting fact.

263
00:14:00,430 --> 00:14:03,142
Tesla Model X, which
was with hardware 2.5,

264
00:14:06,347 --> 00:14:08,780
considered the projected
person a real person.

265
00:14:08,780 --> 00:14:11,660
Now, despite the fact
that this car is equipped

266
00:14:12,930 --> 00:14:14,739
with RADAR and ultrasonic sensor

267
00:14:14,740 --> 00:14:19,320
that intended to detect
depth and it can understand

268
00:14:19,320 --> 00:14:22,794
that this picture is completely depthless

269
00:14:22,794 --> 00:14:25,260
as this is the result of a projection

270
00:14:25,260 --> 00:14:28,069
but it still detected by Tesla Model X.

271
00:14:31,714 --> 00:14:33,855
And we repeated the same experiment,

272
00:14:33,855 --> 00:14:36,365
however this time, with
a projection of a car,

273
00:14:36,365 --> 00:14:37,640
it was projected on the road.

274
00:14:37,640 --> 00:14:42,640
And again here's a fact, Tesla
Model X with hardware 2.5,

275
00:14:43,320 --> 00:14:46,640
considers the projected car a real car.

276
00:14:46,640 --> 00:14:51,640
This is despite the fact
that the car have greater

277
00:14:52,620 --> 00:14:54,740
radio signature than a pedestrian,

278
00:14:54,740 --> 00:14:57,913
so we would expect that
the RADAR to ignore or,

279
00:14:58,777 --> 00:15:03,166
to ignore this or consider
this classification

280
00:15:03,166 --> 00:15:04,646
as a false positive.

281
00:15:04,646 --> 00:15:07,242
However, as you can see on the left side,

282
00:15:09,417 --> 00:15:11,460
the picture that is taken
from the visual showed

283
00:15:11,460 --> 00:15:14,643
that it is considered
as a real car by Tesla.

284
00:15:16,760 --> 00:15:19,085
Now, I think that you will agree with me

285
00:15:19,085 --> 00:15:20,223
that there is a difference between

286
00:15:20,223 --> 00:15:24,365
what an ADAS thinks it sees
and what is actually there

287
00:15:24,365 --> 00:15:27,553
and the three following
picture demonstrate them.

288
00:15:31,900 --> 00:15:36,900
Now I want to mark that we
don't consider phantoms as bugs.

289
00:15:37,343 --> 00:15:39,490
They are not the result of
poor security implementation,

290
00:15:39,490 --> 00:15:42,393
they are not for example an SQL injection.

291
00:15:43,470 --> 00:15:46,517
Phantoms exploit a fundamental
inability of object detectors

292
00:15:46,517 --> 00:15:50,370
to distinguish between
real and fake objects.

293
00:15:50,370 --> 00:15:53,573
And we consider phantoms
as scientific gap.

294
00:15:55,850 --> 00:15:58,143
So why are phantoms a scientific gap?

295
00:15:59,150 --> 00:16:02,970
Object detectors are the output
of a long training process,

296
00:16:02,970 --> 00:16:06,020
and previous studies have already
discussed the implications

297
00:16:06,020 --> 00:16:08,300
of using training process to create AI.

298
00:16:08,300 --> 00:16:10,093
And they discussed about the high level

299
00:16:10,094 --> 00:16:11,620
of computing capabilities,

300
00:16:11,620 --> 00:16:14,800
and they discussed about
a high energy consumption

301
00:16:14,800 --> 00:16:17,339
and they also discussed
about the high cost,

302
00:16:17,340 --> 00:16:20,010
and the long training time.

303
00:16:20,010 --> 00:16:25,010
However, it seems that in the
context of autonomous driving,

304
00:16:25,210 --> 00:16:29,610
it seems that the training
process creates robust AI models

305
00:16:29,610 --> 00:16:32,710
that function better
than humans in real life.

306
00:16:32,710 --> 00:16:35,860
And you can see the articles in this slide

307
00:16:35,860 --> 00:16:39,680
where they are taken
from about two years ago,

308
00:16:39,680 --> 00:16:41,683
Tesla two years ago was considered,

309
00:16:41,683 --> 00:16:45,930
Tesla's autopilot was
considered nine times safer

310
00:16:45,930 --> 00:16:50,199
than the average driver,
it was about two years ago.

311
00:16:52,195 --> 00:16:56,366
However, we tried to ask a
different type of question,

312
00:16:56,366 --> 00:16:58,770
what object detectors don't learn?

313
00:16:58,770 --> 00:17:00,784
And we're trying to understand

314
00:17:00,784 --> 00:17:02,504
that the following three characteristics

315
00:17:02,504 --> 00:17:03,835
are not taken into account

316
00:17:03,835 --> 00:17:07,170
or aren't trained or aren't
learned by object detectors.

317
00:17:07,170 --> 00:17:08,920
Now the first one is context.

318
00:17:08,920 --> 00:17:12,040
You can see how unrealistic
objects are being considered

319
00:17:12,040 --> 00:17:14,173
by advanced driver-assistance systems.

320
00:17:17,010 --> 00:17:19,364
A road sign that is projected on a tree

321
00:17:19,364 --> 00:17:21,990
is of course unrealistic object,

322
00:17:21,990 --> 00:17:24,640
however, it is still being detected

323
00:17:24,640 --> 00:17:27,040
by a Mobileye 630 PRO.

324
00:17:27,040 --> 00:17:31,129
Also in some cases, colors
aren't taken into account.

325
00:17:31,130 --> 00:17:35,197
This road sign consists
only of green colors

326
00:17:36,770 --> 00:17:38,420
and it is still being detected

327
00:17:38,420 --> 00:17:41,823
by one of the state of
the art object detectors

328
00:17:42,890 --> 00:17:45,080
currently exists.

329
00:17:45,080 --> 00:17:48,653
And also texture, in this
case isn't taken into account,

330
00:17:48,653 --> 00:17:50,060
you can see how transparent

331
00:17:50,060 --> 00:17:52,750
and in some cases even skewed objects

332
00:17:52,750 --> 00:17:55,594
are still considered by advanced
driver-assistance systems

333
00:17:55,594 --> 00:17:59,690
as real object despite the texture.

334
00:18:01,690 --> 00:18:05,030
And object detectors, we concluded
that object detectors are

335
00:18:05,030 --> 00:18:06,480
essentially feature matchers.

336
00:18:07,490 --> 00:18:09,330
They lack the ability of humans

337
00:18:09,330 --> 00:18:11,773
to ignore fake or inauthentic object.

338
00:18:12,750 --> 00:18:14,820
Basically they consider an object

339
00:18:14,820 --> 00:18:18,983
similar to the objects
that appear in the dataset

340
00:18:18,983 --> 00:18:20,483
that they were trained on.

341
00:18:23,220 --> 00:18:25,724
Now let's try to understand
how attack system

342
00:18:25,724 --> 00:18:28,601
exploit this fact and apply the attack.

343
00:18:28,602 --> 00:18:31,350
And in the first experiment that we did

344
00:18:31,350 --> 00:18:34,000
is we tried to detect
the appearance threshold,

345
00:18:34,000 --> 00:18:37,323
and in order to do so,
we used two road signs,

346
00:18:40,003 --> 00:18:42,023
for Tesla we used a stop sign

347
00:18:42,023 --> 00:18:44,929
and for Mobileye we used a speed limit.

348
00:18:44,930 --> 00:18:49,810
And we flash this road
sign in various situations,

349
00:18:49,810 --> 00:18:50,970
in front of this,

350
00:18:52,235 --> 00:18:55,417
in front of Tesla and in
front of Mobileye 630.

351
00:18:55,417 --> 00:18:59,822
And we can see the
results on the right side.

352
00:19:00,800 --> 00:19:04,267
And we found that Mobileye
630 detects a phantom,

353
00:19:04,267 --> 00:19:06,974
a phantom road sign, in this case

354
00:19:06,974 --> 00:19:11,665
that appears for 125
millisecond, 100% of the time.

355
00:19:11,665 --> 00:19:14,134
And the case for Tesla
is somehow different.

356
00:19:14,134 --> 00:19:16,112
Tesla detects a phantom that appears

357
00:19:16,112 --> 00:19:21,020
for 416 millisecond 100% of the time.

358
00:19:21,020 --> 00:19:24,873
Tesla have greater appearance
threshold than Mobileye.

359
00:19:27,830 --> 00:19:30,639
Now at the beginning I
explained about the remotes,

360
00:19:30,640 --> 00:19:33,210
attack models that they used to attack

361
00:19:33,210 --> 00:19:34,970
advanced driver-assistance systems,

362
00:19:34,970 --> 00:19:39,190
and one was to mount a projector to drone.

363
00:19:39,190 --> 00:19:42,890
And in this experiment, we
mounted a projector to a drone

364
00:19:42,890 --> 00:19:47,890
and we used it to flash a
road sign for 125 millisecond

365
00:19:48,360 --> 00:19:51,659
on the building, on the
right side of the road.

366
00:19:51,660 --> 00:19:54,910
And the driving car was equipped

367
00:19:54,910 --> 00:19:58,080
with the Mobileye 630 PRO that detected

368
00:20:00,882 --> 00:20:02,620
this phantom road sign,

369
00:20:02,620 --> 00:20:07,092
it's real road sign and
notified about it to the driver.

370
00:20:11,680 --> 00:20:14,010
We also conduct another experiment

371
00:20:14,010 --> 00:20:15,797
for Tesla Model X,

372
00:20:15,797 --> 00:20:19,360
and in this experiment,
instead of using a road sign,

373
00:20:19,360 --> 00:20:22,919
we used a phantom of the pedestrian.

374
00:20:22,920 --> 00:20:26,880
We projected the pedestrian on the road

375
00:20:26,880 --> 00:20:30,080
from the side of the
road, by using a projector

376
00:20:30,080 --> 00:20:33,560
and we didn't use a
split-second projection,

377
00:20:33,560 --> 00:20:36,490
mainly because due to safety concerns.

378
00:20:36,490 --> 00:20:38,300
And you can see how the car,

379
00:20:39,608 --> 00:20:41,990
detects the pedestrian

380
00:20:41,990 --> 00:20:45,710
and immediately and
automatically reduces its speed

381
00:20:45,710 --> 00:20:49,240
from 14 miles per hour
to 18 miles per hour.

382
00:20:49,240 --> 00:20:50,860
And you can see it again,

383
00:20:50,860 --> 00:20:53,020
the car is driving at 18 miles per hour,

384
00:20:53,020 --> 00:20:55,420
and right now, it decreases its speed

385
00:20:56,532 --> 00:20:58,131
because it detects a pedestrian.

386
00:21:03,964 --> 00:21:06,850
Finally there's the second
way to apply the attack

387
00:21:06,850 --> 00:21:10,352
which is to apply it
via a digital billboard.

388
00:21:14,402 --> 00:21:15,860
Many digital billboard nowadays

389
00:21:15,860 --> 00:21:17,080
are connected to the internet,

390
00:21:17,080 --> 00:21:19,139
so attackers can hack
the digital billboard

391
00:21:19,140 --> 00:21:23,460
and to embed a phantom into
an existing advertisement.

392
00:21:23,460 --> 00:21:25,900
And we developed an
algorithm to embed a phantom

393
00:21:25,900 --> 00:21:29,573
into an existing
advertisement in a hidden way.

394
00:21:30,920 --> 00:21:35,390
We take any given advertisement,
or the given advertisement

395
00:21:35,390 --> 00:21:39,370
to compute a local score
for each and every block

396
00:21:39,370 --> 00:21:42,699
in every frame of the advertisement,

397
00:21:42,700 --> 00:21:46,180
and the local score for each
block is computed as follows.

398
00:21:46,180 --> 00:21:50,060
We extract key points
from each and every frame,

399
00:21:50,060 --> 00:21:54,090
and we compute a local
score for each block

400
00:21:54,090 --> 00:21:58,010
based on how much a
dead area the block is.

401
00:21:58,010 --> 00:22:03,010
It means basically with
respect to the key point

402
00:22:03,300 --> 00:22:08,300
that were extracted, basically
on how distant the block is

403
00:22:09,442 --> 00:22:11,942
from the extracted key points.

404
00:22:12,923 --> 00:22:17,150
And we also compute a global
score for each and every block.

405
00:22:17,150 --> 00:22:19,220
And the global score is computed

406
00:22:19,220 --> 00:22:23,560
with respect to the score of the block

407
00:22:23,560 --> 00:22:26,169
in the subsequent consecutive frames.

408
00:22:26,170 --> 00:22:30,880
This is done in order to take
the time domain into account

409
00:22:30,880 --> 00:22:35,500
because the some cases, a
phantom needs to be embedded

410
00:22:35,500 --> 00:22:40,500
to a few consecutive
frames in order to meet

411
00:22:40,756 --> 00:22:44,071
the appearance threshold that is defined

412
00:22:44,071 --> 00:22:48,889
in the advanced driver-assistance systems.

413
00:22:48,890 --> 00:22:53,890
And this is a demonstration
of the output of the algorithm

414
00:22:54,042 --> 00:22:57,919
an algorithm advertisement embedded

415
00:22:59,060 --> 00:23:00,730
a stop sign to the advertisement

416
00:23:00,730 --> 00:23:03,040
according to the output of the algorithm

417
00:23:03,040 --> 00:23:04,827
you can see the stop sign right now.

418
00:23:05,710 --> 00:23:09,146
It is being flashed for 500 milliseconds

419
00:23:09,146 --> 00:23:12,125
and we took this advertisement,

420
00:23:12,125 --> 00:23:15,590
and presented it on a digital billboard

421
00:23:15,590 --> 00:23:18,034
in an experiment we conducted.

422
00:23:18,035 --> 00:23:23,035
We conducted and we get to
test the Tesla's autopilot,

423
00:23:23,210 --> 00:23:28,210
and as you can see, Tesla
detects the phantom stop sign

424
00:23:28,660 --> 00:23:33,083
and immediately stops the car
in the middle of the road.

425
00:23:33,083 --> 00:23:35,687
Despite the fact that
it's not a real road sign.

426
00:23:35,687 --> 00:23:38,909
You can see that now, this is
where the phantom stop sign

427
00:23:38,910 --> 00:23:42,820
appear on the left, on the
left side of the advertisement

428
00:23:44,063 --> 00:23:45,190
and you can see the dashboard

429
00:23:45,190 --> 00:23:48,853
where Tesla detects that it's a stop sign,

430
00:23:49,867 --> 00:23:53,272
and immediately triggers the car to stop

431
00:23:53,272 --> 00:23:55,889
in the middle of the road.

432
00:23:55,890 --> 00:23:58,833
Now I will let Yisroel
to continue the talk.

433
00:23:59,920 --> 00:24:01,060
- Thank you Ben.

434
00:24:01,060 --> 00:24:02,760
Hi everybody, My name is Yisroel Mirsky,

435
00:24:02,760 --> 00:24:03,860
I'm a Postdoctoral fellow

436
00:24:03,860 --> 00:24:05,760
at the Georgia Institute of Technology,

437
00:24:05,760 --> 00:24:07,430
as well as the research project manager

438
00:24:07,430 --> 00:24:11,210
at the Ben-Gurion Cyber Security
Research Center in Israel.

439
00:24:11,210 --> 00:24:12,720
Now that we've seen the vulnerability

440
00:24:12,720 --> 00:24:14,530
and understand its implications,

441
00:24:14,530 --> 00:24:15,860
let's talk about what we can do

442
00:24:15,860 --> 00:24:19,270
to strengthen our AI systems
against such attacks.

443
00:24:19,270 --> 00:24:21,360
In other words, how can we close this gap

444
00:24:21,360 --> 00:24:23,362
between the AI and the human driver?

445
00:24:24,970 --> 00:24:27,630
Now, even though autonomous
vehicles use sensor fusion,

446
00:24:27,630 --> 00:24:29,680
for example, RADAR and LiDAR.

447
00:24:29,680 --> 00:24:31,260
The autonomous system will still react

448
00:24:31,260 --> 00:24:33,240
to objects detected in the images alone.

449
00:24:33,240 --> 00:24:35,660
However, this makes sense
if you think about it.

450
00:24:35,660 --> 00:24:37,990
If the AI detects a person
in the middle of the road,

451
00:24:37,990 --> 00:24:39,790
based on the camera alone,

452
00:24:39,790 --> 00:24:40,820
you're going to react

453
00:24:40,820 --> 00:24:43,040
because you don't wanna
cause a fatal mistake.

454
00:24:43,040 --> 00:24:45,860
Therefore, we need a solution
which can validate objects

455
00:24:45,860 --> 00:24:48,360
identified by the camera sensors AI.

456
00:24:48,360 --> 00:24:50,959
We also need the solution to
be independent and lightweight,

457
00:24:50,960 --> 00:24:54,600
so that we can easily integrate
it into existing systems.

458
00:24:54,600 --> 00:24:56,350
So how are we going to do this?

459
00:24:56,350 --> 00:24:57,570
Well we thought about this a lot

460
00:24:57,570 --> 00:24:58,899
and we tried to think of reasons

461
00:24:58,900 --> 00:25:00,660
why human can tell the difference

462
00:25:00,660 --> 00:25:03,550
between a flat projected
image and AI cannot.

463
00:25:03,550 --> 00:25:04,639
We then understood that,

464
00:25:04,640 --> 00:25:06,660
an object detection AI has been trained

465
00:25:06,660 --> 00:25:07,970
to do one thing only.

466
00:25:07,970 --> 00:25:10,610
Identify objects by matching
patterns in geometry.

467
00:25:10,610 --> 00:25:12,120
In contrast, a human's intuition

468
00:25:12,120 --> 00:25:14,270
will raise a red flag if certain aspects

469
00:25:14,270 --> 00:25:15,660
or contexts are right.

470
00:25:15,660 --> 00:25:16,940
For example, on the right,

471
00:25:16,940 --> 00:25:19,480
we can see that Google's Cloud AI

472
00:25:19,480 --> 00:25:22,740
is convinced that this hairy,
amorphous blob is a cat

473
00:25:22,740 --> 00:25:24,320
just because it has fur, eyes

474
00:25:24,320 --> 00:25:26,320
and some pointy ears somewhere at there.

475
00:25:27,180 --> 00:25:29,080
Of course we still love
deep learning models

476
00:25:29,080 --> 00:25:32,720
but we just want them to be a
little bit more intelligent.

477
00:25:32,720 --> 00:25:35,470
Until then, what we
will do to identify the,

478
00:25:35,470 --> 00:25:37,390
what will we do to identify
these false objects

479
00:25:37,390 --> 00:25:39,260
detected by the camera?

480
00:25:39,260 --> 00:25:41,780
Well, instead of throwing
another classifier to the problem

481
00:25:41,780 --> 00:25:44,000
which will fail for all the same reasons.

482
00:25:44,000 --> 00:25:46,410
We will first take a look
at the detected object

483
00:25:46,410 --> 00:25:49,440
and then analyze different
aspects of the object

484
00:25:49,440 --> 00:25:51,670
to determine if it's real or fake.

485
00:25:51,670 --> 00:25:53,400
We've identified seven aspects,

486
00:25:53,400 --> 00:25:55,160
which can be extracted from an image

487
00:25:55,160 --> 00:25:56,920
to better capture the truth.

488
00:25:56,920 --> 00:25:58,610
Let's look at the attacks in the area

489
00:25:58,610 --> 00:26:00,639
where a phantom road
sign has been projected

490
00:26:00,640 --> 00:26:01,490
in the other car.

491
00:26:03,030 --> 00:26:05,580
First, we can consider if the size,

492
00:26:05,580 --> 00:26:07,929
the sign is larger or
smaller than it should be.

493
00:26:07,930 --> 00:26:09,030
That might be problematic.

494
00:26:09,030 --> 00:26:11,980
For example, a traffic sign
which is not regulation size

495
00:26:11,980 --> 00:26:13,150
should be ignored.

496
00:26:13,150 --> 00:26:15,670
The size of the sign can
be determined on vehicles

497
00:26:15,670 --> 00:26:18,470
which use multiple cameras
through stereoscopic imaging.

498
00:26:19,400 --> 00:26:22,210
Second, we can look at
the angle of the sign.

499
00:26:22,210 --> 00:26:23,623
Cameras capture perspective,

500
00:26:25,447 --> 00:26:27,609
so if the shape of the
sign does not match,

501
00:26:27,609 --> 00:26:30,179
then we can consider it being fake

502
00:26:30,180 --> 00:26:32,140
because it is being projected on an angle

503
00:26:32,140 --> 00:26:33,870
facing away from the car.

504
00:26:33,870 --> 00:26:36,600
Third, we can look at
the focus of the sign.

505
00:26:36,600 --> 00:26:39,439
A projected sign may be
blurrier than its surroundings,

506
00:26:39,440 --> 00:26:41,990
if the projector is out of
focus or partially blurry

507
00:26:41,990 --> 00:26:44,343
if it is being projected
on an uneven surface.

508
00:26:45,490 --> 00:26:46,900
The focal range change of the camera

509
00:26:46,900 --> 00:26:49,110
can also be used to identify a range

510
00:26:49,110 --> 00:26:50,879
whether the sign is contextually,

511
00:26:50,880 --> 00:26:54,230
and understand whether the
design contextually makes sense.

512
00:26:54,230 --> 00:26:55,630
Speaking of context,

513
00:26:55,630 --> 00:26:57,450
if the placement of the sign is impossible

514
00:26:57,450 --> 00:26:58,990
or simply abnormal,

515
00:26:58,990 --> 00:27:00,440
it indicates that is a phantom.

516
00:27:00,440 --> 00:27:03,200
For example, a traffic sign
that does not have a post

517
00:27:03,200 --> 00:27:05,593
or some pedestrian
floating over the ground.

518
00:27:06,640 --> 00:27:08,540
We can also consider
the surface of the sign.

519
00:27:08,540 --> 00:27:10,820
If it's distorted, lumpy or has features

520
00:27:10,820 --> 00:27:14,159
which do not match the typical
features of a detected sign.

521
00:27:14,160 --> 00:27:16,660
For example, think of what
a stop sign may look like

522
00:27:16,660 --> 00:27:18,510
if it were projected on a brick wall.

523
00:27:19,420 --> 00:27:22,530
Another aspect is light,
being emitted from the sign.

524
00:27:22,530 --> 00:27:24,520
Since phantoms emit their own light

525
00:27:24,520 --> 00:27:26,330
via the projector or TV screen,

526
00:27:26,330 --> 00:27:27,639
they will be inherently different

527
00:27:27,640 --> 00:27:28,970
compared to the light reflected

528
00:27:28,970 --> 00:27:31,263
from the road, from a regular road sign.

529
00:27:32,180 --> 00:27:35,090
This can be determined
passively through image analysis

530
00:27:35,090 --> 00:27:38,480
or actively by shining a
light source onto the object.

531
00:27:38,480 --> 00:27:41,460
Finally, we can consider
the depth of the scene.

532
00:27:41,460 --> 00:27:43,410
We can detect Phantoms

533
00:27:43,410 --> 00:27:46,550
if the object has the wrong surface shape

534
00:27:46,550 --> 00:27:48,750
or its placement in a
3D scene is abnormal.

535
00:27:48,750 --> 00:27:51,770
For example, a traffic
sign projected onto a tree,

536
00:27:51,770 --> 00:27:55,810
or a 3D person or car projected
on the surface of the road.

537
00:27:55,810 --> 00:27:58,320
However, how can you
get the depth perception

538
00:27:58,320 --> 00:28:00,040
from a single camera?

539
00:28:00,040 --> 00:28:02,590
So we use a technique where we take a look

540
00:28:02,590 --> 00:28:05,560
at two subsequent frames
captured by the camera.

541
00:28:05,560 --> 00:28:07,460
If they are captured while the vehicle

542
00:28:07,460 --> 00:28:09,150
or projection is in motion,

543
00:28:09,150 --> 00:28:12,680
then we can compute the depth
implicitly using optical flow,

544
00:28:12,680 --> 00:28:14,593
much like stereoscopical viewing.

545
00:28:16,000 --> 00:28:18,300
To give you a better idea
of what I'm talking about,

546
00:28:18,300 --> 00:28:19,800
here's what it looks like.

547
00:28:19,800 --> 00:28:21,540
On the left is the first frame,

548
00:28:21,540 --> 00:28:22,920
then using the second frame,

549
00:28:22,920 --> 00:28:25,270
we can compute the image on the right.

550
00:28:25,270 --> 00:28:28,460
The directional shift between
the pixels is the color

551
00:28:28,460 --> 00:28:30,163
and the brightness is the speed.

552
00:28:31,020 --> 00:28:32,570
Now imagine what a person projected

553
00:28:32,570 --> 00:28:33,950
on the road would look like.

554
00:28:33,950 --> 00:28:35,410
It would be a solid, flat color,

555
00:28:35,410 --> 00:28:37,210
just like the rest of the road.

556
00:28:37,210 --> 00:28:39,540
This is a strong indication of a phantom,

557
00:28:39,540 --> 00:28:40,760
compared to a real person,

558
00:28:40,760 --> 00:28:42,610
which would produce a distinct shape.

559
00:28:43,530 --> 00:28:45,810
So how do you utilize
multiple perspectives

560
00:28:45,810 --> 00:28:48,800
without falling into the
same traps as before?

561
00:28:48,800 --> 00:28:50,680
So we suggest using a machine learning

562
00:28:50,680 --> 00:28:53,950
and sample approach called
a Committee of Experts.

563
00:28:53,950 --> 00:28:56,800
The idea is that you use
a diverse set of models

564
00:28:56,800 --> 00:28:57,919
on the same image,

565
00:28:57,920 --> 00:29:00,620
where each model has a different
perspective on the image.

566
00:29:00,620 --> 00:29:02,540
Then, you combine their predictions

567
00:29:02,540 --> 00:29:04,879
by considering each of their viewpoints.

568
00:29:04,880 --> 00:29:07,160
What is key in a committee of experts

569
00:29:07,160 --> 00:29:08,800
is that there would be disagreements.

570
00:29:08,800 --> 00:29:10,700
These disagreements help the ensemble

571
00:29:10,700 --> 00:29:12,553
to produce more robust predictions.

572
00:29:13,790 --> 00:29:14,889
To evaluate this approach,

573
00:29:14,890 --> 00:29:18,570
we developed this detection
model called Ghostbuster.

574
00:29:18,570 --> 00:29:21,659
Here we only consider
four of the seven aspects.

575
00:29:21,660 --> 00:29:25,570
Context, surface, light, and depth.

576
00:29:25,570 --> 00:29:27,820
The way the system works is as follows.

577
00:29:27,820 --> 00:29:30,580
When the onboard object
detector identifies an object,

578
00:29:30,580 --> 00:29:31,889
say a road sign,

579
00:29:31,890 --> 00:29:35,090
their cropped image is passed
to our model for verification.

580
00:29:35,090 --> 00:29:37,830
Then four different aspects
are extracted from the image

581
00:29:37,830 --> 00:29:40,889
and passed to their corresponding experts.

582
00:29:40,890 --> 00:29:42,810
The experts, reason of whether the object

583
00:29:42,810 --> 00:29:45,260
in their perspective makes sense or not.

584
00:29:45,260 --> 00:29:47,520
Finally, the internal representations

585
00:29:47,520 --> 00:29:49,936
also known as embeddings
from each of these networks,

586
00:29:49,936 --> 00:29:52,980
are concatenated and passed
through a single network

587
00:29:52,980 --> 00:29:55,683
which makes a decision based
on the expert's feedback.

588
00:29:56,600 --> 00:29:58,409
Although this model may seem intimidating,

589
00:29:58,410 --> 00:30:01,040
it's important to note that it
only has 1 million parameters

590
00:30:01,040 --> 00:30:03,379
which is nothing compared
to the hundreds of millions

591
00:30:03,380 --> 00:30:05,530
which object detection models use.

592
00:30:05,530 --> 00:30:08,300
Therefore, adding this model
to the system as a guard,

593
00:30:08,300 --> 00:30:11,868
has a negligible impact
on the car's resources.

594
00:30:11,868 --> 00:30:15,750
When evaluating seven state
of the art object detectors,

595
00:30:15,750 --> 00:30:19,300
we found that the models were
susceptible to phantom attacks

596
00:30:19,300 --> 00:30:22,720
with a 92 to 99% attack success rate.

597
00:30:22,720 --> 00:30:25,450
However, with the defense model in place,

598
00:30:25,450 --> 00:30:28,360
the attack success rate dropped below 1%

599
00:30:28,360 --> 00:30:31,830
even when tuning the model
to have zero false alarms.

600
00:30:31,830 --> 00:30:34,260
Below, we can see here
the committee at work.

601
00:30:34,260 --> 00:30:35,910
The table shows that every expert

602
00:30:35,910 --> 00:30:38,317
has a useful and unique contribution

603
00:30:38,317 --> 00:30:40,620
where the combination of all experts

604
00:30:40,620 --> 00:30:42,110
outperforms the baseline model

605
00:30:42,110 --> 00:30:44,362
which in this case is a single classifier.

606
00:30:45,770 --> 00:30:47,920
Here are some visual examples
of these disagreements

607
00:30:47,920 --> 00:30:50,500
which ultimately lead to
correct classifications.

608
00:30:50,500 --> 00:30:52,120
For example, here in the top left,

609
00:30:52,120 --> 00:30:54,000
the context expert found it strange

610
00:30:54,000 --> 00:30:56,650
for a speed limit sign
to be over the highway

611
00:30:56,650 --> 00:30:59,550
but the light expert identified
it as a digital road sign.

612
00:31:00,550 --> 00:31:04,930
Okay. So we know that the model
is robust to the environment

613
00:31:04,930 --> 00:31:07,370
but is it robust to the adversary?

614
00:31:07,370 --> 00:31:08,860
In other words, what if the attacker

615
00:31:08,860 --> 00:31:12,392
uses adversarial machine learning
to fool our defense model?

616
00:31:13,310 --> 00:31:14,169
We attacked our model

617
00:31:14,170 --> 00:31:16,810
with eight different adversarial
machine learning attacks.

618
00:31:16,810 --> 00:31:17,810
Through these experiments,

619
00:31:17,810 --> 00:31:19,649
we found that the committee as a whole,

620
00:31:19,650 --> 00:31:22,590
is much stronger than any single expert.

621
00:31:22,590 --> 00:31:26,220
This is because the combiner
model can identify an outlier

622
00:31:26,220 --> 00:31:29,150
when only one of these
aspects have been altered.

623
00:31:29,150 --> 00:31:31,920
This makes it significantly
more challenging for an attacker

624
00:31:31,920 --> 00:31:34,510
because some aspects can
not easily be altered

625
00:31:34,510 --> 00:31:36,573
like depth and lighting aspects.

626
00:31:38,100 --> 00:31:39,830
So in summary, there exists

627
00:31:39,830 --> 00:31:43,260
a perceptual gap between
AI and human drivers.

628
00:31:43,260 --> 00:31:45,690
And this gap can be
exploited by an attacker

629
00:31:45,690 --> 00:31:47,940
even with sensor fusion in place.

630
00:31:47,940 --> 00:31:50,170
However, if we can force our models

631
00:31:50,170 --> 00:31:53,810
to consider multiple aspects
or perspectives on the data,

632
00:31:53,810 --> 00:31:55,590
then we can mimic a human's intuition

633
00:31:55,590 --> 00:31:57,570
and close this gap further.

634
00:31:57,570 --> 00:32:01,010
So how can we apply what we've
learned in this presentation?

635
00:32:01,010 --> 00:32:03,590
For future systems we should
always consider redundancies

636
00:32:03,590 --> 00:32:06,310
when developing safety
critical AI systems,

637
00:32:06,310 --> 00:32:09,570
especially if the AI
subsystem has a direct control

638
00:32:09,570 --> 00:32:11,490
over the system's behavior.

639
00:32:11,490 --> 00:32:13,780
Some lessons we can learn from this are,

640
00:32:13,780 --> 00:32:18,780
one, we should analyze the
edge cases of an AI model

641
00:32:18,830 --> 00:32:21,220
and by trying to answer
the following questions.

642
00:32:21,220 --> 00:32:23,230
What did not appear in the training set

643
00:32:23,230 --> 00:32:25,800
that the AI model could
encounter in the wild?

644
00:32:25,800 --> 00:32:28,659
And what did the model really learn?

645
00:32:28,660 --> 00:32:32,540
Two, we should test the
robustness of our AI models

646
00:32:32,540 --> 00:32:34,570
by providing edge cases.

647
00:32:34,570 --> 00:32:37,939
And three, we should secure AI models

648
00:32:37,940 --> 00:32:40,080
by retraining model with edge cases,

649
00:32:40,080 --> 00:32:42,718
and we should make sure we
add features or perspectives

650
00:32:42,719 --> 00:32:46,130
that distinguish between
good and bad cases.

651
00:32:46,130 --> 00:32:46,963
That's all.

652
00:32:46,963 --> 00:32:48,290
Thank you for joining our session.

