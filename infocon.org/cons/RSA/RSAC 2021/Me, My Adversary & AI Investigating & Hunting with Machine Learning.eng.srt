1
00:00:01,500 --> 00:00:03,540
- Hello, I am Jess Garcia.

2
00:00:03,540 --> 00:00:05,930
I am the founder of One eSecurity,

3
00:00:05,930 --> 00:00:08,219
a global detection and response firm.

4
00:00:08,220 --> 00:00:10,830
And I'm also a SANS instructor,

5
00:00:10,830 --> 00:00:13,467
a Senior Instructor with
the SANS Institute, okay?

6
00:00:13,467 --> 00:00:16,010
I'm very happy today to be here talking

7
00:00:16,010 --> 00:00:19,610
about artificial
intelligence, applied to DFIR

8
00:00:19,610 --> 00:00:20,830
specifically to threat hunting

9
00:00:20,830 --> 00:00:23,119
and digital investigations, okay?

10
00:00:23,120 --> 00:00:27,263
I would start by talking
about the DFIR myth.

11
00:00:28,322 --> 00:00:31,900
In DFIR we have these Red button myth

12
00:00:31,900 --> 00:00:35,560
in which in a distant future,
we will have the capability

13
00:00:35,560 --> 00:00:38,150
to actually press a Red bottom

14
00:00:38,150 --> 00:00:41,890
and have our investigation
solved automatically.

15
00:00:41,890 --> 00:00:44,290
I'm sad to say that the
artificial intelligence

16
00:00:44,290 --> 00:00:45,123
is not there yet.

17
00:00:45,123 --> 00:00:47,058
I don't think it's gonna
be there anytime soon,

18
00:00:47,058 --> 00:00:50,400
but the point of this presentation
is actually showing you

19
00:00:50,400 --> 00:00:51,857
that even when we are not there

20
00:00:51,857 --> 00:00:55,510
and we will not be there
soon, artificial intelligence

21
00:00:55,510 --> 00:00:56,580
and machine learning, deep learning

22
00:00:56,580 --> 00:01:00,847
can be a very powerful
weapon in data forensics

23
00:01:00,847 --> 00:01:02,300
and it's response, okay?

24
00:01:02,300 --> 00:01:07,030
So this talk is about turning
AI into a powerful weapon.

25
00:01:07,030 --> 00:01:10,960
Now, our objective is
to help forensicators,

26
00:01:10,960 --> 00:01:13,800
these forensic experts to move from the,

27
00:01:13,800 --> 00:01:17,619
or to compliment their
already existing capabilities

28
00:01:17,620 --> 00:01:21,090
with their powerful commercial
or open source tools,

29
00:01:21,090 --> 00:01:24,460
source tools or platforms into including

30
00:01:24,460 --> 00:01:28,770
other newer technologies,
such as the data,

31
00:01:28,770 --> 00:01:33,360
most interestingly data science
and artificial intelligence.

32
00:01:33,360 --> 00:01:36,311
Last year I started the project

33
00:01:36,311 --> 00:01:38,950
called data science forensics.

34
00:01:38,950 --> 00:01:42,430
Okay, you can go to the data
science forensics.io website,

35
00:01:42,430 --> 00:01:44,990
and you will find all about it.

36
00:01:44,990 --> 00:01:47,339
It's a free project,
it's a community project,

37
00:01:47,340 --> 00:01:50,040
and we've been doing quite a few things

38
00:01:50,040 --> 00:01:51,220
during the last year.

39
00:01:51,220 --> 00:01:54,800
The road so far for the
supernatural (indistinct)

40
00:01:54,800 --> 00:01:57,149
has been several subprojects

41
00:01:57,150 --> 00:01:59,340
that we've been creating such as

42
00:01:59,340 --> 00:02:01,900
the data science forensics library,

43
00:02:01,900 --> 00:02:05,620
which is a project that
facilitates the ingestion

44
00:02:05,620 --> 00:02:09,130
and analysis of forensic data

45
00:02:09,130 --> 00:02:11,065
from the output of your tools,

46
00:02:11,065 --> 00:02:14,510
to your data science
environment, typically Jupyter,

47
00:02:14,510 --> 00:02:16,483
for those who are familiar with that.

48
00:02:17,430 --> 00:02:20,410
We've also been working
on a virtual machine

49
00:02:20,410 --> 00:02:22,130
that facilitates all this, okay.

50
00:02:22,130 --> 00:02:25,700
Setting up a data science
environment with Jupyter

51
00:02:25,700 --> 00:02:27,980
and all the plugins can
be a little cumbersome.

52
00:02:27,980 --> 00:02:31,072
So we are working on a
virtual machine, DAISY

53
00:02:32,230 --> 00:02:35,049
which is designed for data
science, artificial intelligence,

54
00:02:35,050 --> 00:02:37,890
applied to DFIR.

55
00:02:37,890 --> 00:02:41,070
We've also been working in
defining the model, HAM,

56
00:02:41,070 --> 00:02:44,549
the harmonized artifact
model, and some other formats,

57
00:02:44,550 --> 00:02:49,550
which facilitate the harmonization
of all the tool outputs

58
00:02:49,820 --> 00:02:53,510
into a format, which is
digestible by data science

59
00:02:53,510 --> 00:02:56,690
and artificial intelligence programs.

60
00:02:56,690 --> 00:03:00,030
And other very cool project
we've been working on is ADAM,

61
00:03:00,030 --> 00:03:03,820
the adversary emulator
specifically for data science,

62
00:03:03,820 --> 00:03:05,670
as I will introduce later.

63
00:03:05,670 --> 00:03:10,609
And to end up with the star today, D4ML

64
00:03:10,610 --> 00:03:15,300
which basically is
machine learning for DFIR.

65
00:03:15,300 --> 00:03:16,450
Okay.

66
00:03:16,450 --> 00:03:21,450
So last year at the SANS
DFIR Summit in July,

67
00:03:21,460 --> 00:03:24,740
I presented a little bit of an appetizer

68
00:03:24,740 --> 00:03:26,170
of this project, okay?

69
00:03:26,170 --> 00:03:30,829
The first steps and I was able to present

70
00:03:30,830 --> 00:03:35,050
how a specific threat, a
specific machine learning model

71
00:03:35,050 --> 00:03:38,840
called autoencoders are very
good at detecting anomalies

72
00:03:38,840 --> 00:03:43,130
in that specific case about
malicious logons, okay?

73
00:03:43,130 --> 00:03:45,090
If you want to see the whole presentation,

74
00:03:45,090 --> 00:03:47,340
you can go, you have
the link in the bottom.

75
00:03:47,340 --> 00:03:49,000
So that was the first step.

76
00:03:49,000 --> 00:03:51,640
But during this year from July till now,

77
00:03:51,640 --> 00:03:53,480
we've been digging deeper.

78
00:03:53,480 --> 00:03:54,313
Okay.

79
00:03:54,313 --> 00:03:59,313
So we know that machine learning
cannot catch evil activity

80
00:03:59,460 --> 00:04:03,860
because evil is something
blurry, in any case, right?

81
00:04:03,860 --> 00:04:07,290
But let's focus on what
machine learning is good at

82
00:04:07,290 --> 00:04:10,200
and what technologies
it can provide, okay?

83
00:04:10,200 --> 00:04:12,089
So we know that machine learning

84
00:04:12,090 --> 00:04:14,380
when you provide the specific data set

85
00:04:14,380 --> 00:04:17,839
is good at understanding the big picture

86
00:04:17,839 --> 00:04:21,630
in that dataset scope, is
good in complex scenarios

87
00:04:21,630 --> 00:04:25,510
with many variables and in
a very specific field, okay?

88
00:04:25,510 --> 00:04:30,510
So if you fulfill those
let's say restrictions,

89
00:04:30,653 --> 00:04:33,060
data science will be doing a good job.

90
00:04:33,060 --> 00:04:35,670
Sorry, machine learning will
be doing a good job, okay?

91
00:04:35,670 --> 00:04:38,080
And specifically, what type of things

92
00:04:38,080 --> 00:04:40,240
can technically machine learning do?

93
00:04:40,240 --> 00:04:42,760
Well, I'm not gonna be going
into the technical detail,

94
00:04:42,760 --> 00:04:45,800
but it can do classification,
clustering, prediction,

95
00:04:45,800 --> 00:04:49,310
noise filtering, and
more interestingly for us

96
00:04:49,310 --> 00:04:51,190
anomaly detection.

97
00:04:51,190 --> 00:04:55,730
In the end and malicious
event tends to be an anomaly

98
00:04:55,730 --> 00:04:57,170
in many cases, okay?

99
00:04:57,170 --> 00:04:58,750
From different points of view

100
00:04:58,750 --> 00:05:00,890
that I will be explaining a little later.

101
00:05:00,890 --> 00:05:04,750
So we were gonna be
using anomaly detection

102
00:05:04,750 --> 00:05:07,680
in different areas, threat
hunting, digital investigations,

103
00:05:07,680 --> 00:05:11,470
and possibly in the future
CTI and other DFIR areas.

104
00:05:11,470 --> 00:05:13,840
So what is our objective?

105
00:05:13,840 --> 00:05:16,869
Well, our methodology is gonna be first,

106
00:05:16,870 --> 00:05:21,160
the first part is gonna be
the traditional DFIR process

107
00:05:21,160 --> 00:05:22,670
where you have some data,

108
00:05:22,670 --> 00:05:24,450
what you collect from your computers,

109
00:05:24,450 --> 00:05:27,560
from your different
environments, you process it

110
00:05:27,560 --> 00:05:28,800
and you get some output.

111
00:05:28,800 --> 00:05:30,870
From there we will ingest that

112
00:05:30,870 --> 00:05:33,810
into our data science
environment, transform it

113
00:05:33,810 --> 00:05:37,470
to have a common harmonized format,

114
00:05:37,470 --> 00:05:41,690
that is let's say, friendly for
the data science environment

115
00:05:41,690 --> 00:05:42,730
and more interesting in this case

116
00:05:42,730 --> 00:05:44,970
for the machine learning environment.

117
00:05:44,970 --> 00:05:47,710
And once we have that, we will be able to,

118
00:05:47,710 --> 00:05:50,960
we will try to detect
anomalies in that data set.

119
00:05:50,960 --> 00:05:51,799
Okay?

120
00:05:51,800 --> 00:05:56,340
So in order to make this
much more interesting,

121
00:05:56,340 --> 00:05:59,409
what we will do, is ask
ourselves a big question.

122
00:05:59,410 --> 00:06:04,090
Could we be able to
detect an unknown attack

123
00:06:04,090 --> 00:06:06,739
without indicators of compromise?

124
00:06:06,740 --> 00:06:09,960
Just by doing multi-artifact
machine learning

125
00:06:09,960 --> 00:06:11,370
anomaly analysis.

126
00:06:11,370 --> 00:06:13,020
And to make it even more fun,

127
00:06:13,020 --> 00:06:15,370
we will apply to a very specific case,

128
00:06:15,370 --> 00:06:18,020
all of you know about which is SolarWinds.

129
00:06:18,020 --> 00:06:22,810
So would we have been able to
detect the SolarWinds attack

130
00:06:22,810 --> 00:06:26,310
with a methodology, like the
one we are gonna be presenting?

131
00:06:26,310 --> 00:06:30,066
If we were doing threat
hunting back in the time,

132
00:06:30,066 --> 00:06:33,539
the way we are gonna be discussing today.

133
00:06:33,540 --> 00:06:36,220
We will answer this question
throughout the presentation.

134
00:06:36,220 --> 00:06:37,430
Okay?

135
00:06:37,430 --> 00:06:40,000
So what we're gonna be
presenting as a methodology,

136
00:06:40,000 --> 00:06:41,610
which is divided in three layers,

137
00:06:41,610 --> 00:06:42,560
as you can see here.

138
00:06:42,560 --> 00:06:46,140
The first layer is the
traditional threat hunting,

139
00:06:46,140 --> 00:06:47,539
data investigation layer,

140
00:06:47,540 --> 00:06:50,230
where you're gonna be
hunting for some threats.

141
00:06:50,230 --> 00:06:52,850
And then you're gonna
be getting some data,

142
00:06:52,850 --> 00:06:54,670
some output from your forensic tools

143
00:06:54,670 --> 00:06:56,340
that you're gonna be analyzing.

144
00:06:56,340 --> 00:06:58,530
The second part, which
is the second layer,

145
00:06:58,530 --> 00:07:00,429
the Yellow layer is gonna
be machine learning.

146
00:07:00,430 --> 00:07:03,085
We're gonna be defining
a machine learning model,

147
00:07:03,085 --> 00:07:06,399
which is able to detect those
things we want to detect.

148
00:07:06,399 --> 00:07:08,804
And the third layer is taking
that to the real world.

149
00:07:08,804 --> 00:07:09,637
Okay.

150
00:07:09,637 --> 00:07:11,730
Now that we have a machine
learning model that works,

151
00:07:11,730 --> 00:07:14,660
can we actually apply
that to real-world data

152
00:07:14,660 --> 00:07:16,460
and make it work?

153
00:07:16,460 --> 00:07:18,390
And let's go for it, okay?

154
00:07:18,390 --> 00:07:20,599
So let's go, let's start,
we're gonna be starting

155
00:07:20,600 --> 00:07:24,970
with the DFIR layer.

156
00:07:24,970 --> 00:07:29,970
In the DFIR layer is the
tradition threat hunting model.

157
00:07:30,160 --> 00:07:31,520
We're gonna be defining
the threat hunting model

158
00:07:31,520 --> 00:07:34,140
in which we are gonna be using
the MITRE ATT&CK framework.

159
00:07:34,140 --> 00:07:37,130
And we're gonna be using, for example,

160
00:07:37,130 --> 00:07:40,330
let's say a policy where
we're gonna be hunting for

161
00:07:40,330 --> 00:07:44,270
the top five, most common
adversary techniques.

162
00:07:44,270 --> 00:07:47,210
Okay, we can use the Red
Canary Threat Detection Report.

163
00:07:47,210 --> 00:07:48,650
And if you take a look at that report,

164
00:07:48,650 --> 00:07:52,390
it says that the most
common adversary techniques

165
00:07:52,390 --> 00:07:56,060
as used by adversaries
are process injection,

166
00:07:56,060 --> 00:07:58,730
scheduled tasks, Windows admin shares,

167
00:07:58,730 --> 00:08:01,630
PowerShell and remote file copy, okay?

168
00:08:01,630 --> 00:08:04,159
So let's imagine that that's our policy

169
00:08:04,160 --> 00:08:06,210
and we're gonna be hunting for that.

170
00:08:06,210 --> 00:08:07,043
Guess what?

171
00:08:07,043 --> 00:08:11,440
SolarWinds actually uses
T 10 53 zero zero five,

172
00:08:12,420 --> 00:08:14,550
which is basically Red Canary top two,

173
00:08:14,550 --> 00:08:19,110
which is the creation of
malicious schedule tasks, okay?

174
00:08:19,110 --> 00:08:23,497
So this means that if we were
hunting for scheduled tasks,

175
00:08:23,497 --> 00:08:27,855
and the scheduled tasks were up in the,

176
00:08:27,855 --> 00:08:30,740
in our analysis of scale, okay.

177
00:08:30,740 --> 00:08:32,190
The Medusa's scheduled task,

178
00:08:32,190 --> 00:08:37,020
we could have been detecting
the SolarWinds attack.

179
00:08:37,020 --> 00:08:39,419
In order, first off, what
we're gonna be trying to do

180
00:08:39,419 --> 00:08:44,420
is get familiar with this T
10 53 zero five technique.

181
00:08:44,680 --> 00:08:45,512
Okay.

182
00:08:45,513 --> 00:08:48,180
If you want to know how, from
the offensive point of view,

183
00:08:48,180 --> 00:08:52,180
how it works, okay, how the
creation of a scheduled task,

184
00:08:52,180 --> 00:08:55,088
malicious scale, the
scheduled tasks works,

185
00:08:55,088 --> 00:08:58,750
the Mordor project by Roberto
Rodriguez and his brother

186
00:08:58,750 --> 00:09:01,160
basically is a great location to look.

187
00:09:01,160 --> 00:09:02,459
What they've done is an amazing work

188
00:09:02,460 --> 00:09:07,410
in terms of profiling, how
specific different types

189
00:09:07,410 --> 00:09:09,400
of techniques of MITRE ATT&CK techniques

190
00:09:09,400 --> 00:09:12,270
are actually implemented and
showing you how they work.

191
00:09:12,270 --> 00:09:13,140
Okay.

192
00:09:13,140 --> 00:09:15,460
A complimentary project
to the Mordor project

193
00:09:15,460 --> 00:09:18,400
is The Threat Hunter Playbook
also by Roberto Rodriguez,

194
00:09:18,400 --> 00:09:21,510
which actually finds how
to detect those attacks

195
00:09:21,510 --> 00:09:24,580
mostly by using detection on event logs.

196
00:09:24,580 --> 00:09:25,413
Cool.

197
00:09:25,413 --> 00:09:27,810
So if we look at this,
we would be able to know

198
00:09:27,810 --> 00:09:31,930
how to detect the creation
of scheduled tasks

199
00:09:31,930 --> 00:09:34,589
using event logs.

200
00:09:34,590 --> 00:09:35,907
But of course we are forensicators

201
00:09:35,907 --> 00:09:38,050
and we want to have a wider view.

202
00:09:38,050 --> 00:09:38,882
Okay.

203
00:09:38,883 --> 00:09:40,640
And this means that we are not gonna be

204
00:09:40,640 --> 00:09:43,490
restricting ourselves only to event logs,

205
00:09:43,490 --> 00:09:46,550
but we are gonna be also
covering forensic artifacts.

206
00:09:46,550 --> 00:09:49,740
For that, what we can do
is rely on the science,

207
00:09:49,740 --> 00:09:52,000
DFIR Hunt Evil Poster,

208
00:09:52,000 --> 00:09:54,020
that Hunt Evil Poster

209
00:09:54,020 --> 00:09:57,160
is a fantastic resource for identifying

210
00:09:57,160 --> 00:09:59,390
different types of techniques

211
00:09:59,390 --> 00:10:01,230
from the forensic point of view.

212
00:10:01,230 --> 00:10:03,950
And as you can see in the
slide for specifically

213
00:10:03,950 --> 00:10:06,410
for scheduled tasks in
the destination machine,

214
00:10:06,410 --> 00:10:08,920
you can see that there are
three types of artifacts

215
00:10:08,920 --> 00:10:10,449
that are typically created,

216
00:10:10,450 --> 00:10:13,820
event logs, registry and
file system artifacts.

217
00:10:13,820 --> 00:10:17,370
In the event logs side, you
have the security event logs,

218
00:10:17,370 --> 00:10:20,910
and more specifically the
scheduled tasks event logs

219
00:10:20,910 --> 00:10:22,920
in the registry, you
have different entries.

220
00:10:22,920 --> 00:10:25,279
And in the file system,
we're gonna be focusing

221
00:10:25,279 --> 00:10:28,450
on a very specific directory,

222
00:10:28,450 --> 00:10:32,290
which is the, Windows Systems32
tasks folder, which are,

223
00:10:32,290 --> 00:10:34,860
which is where if the files are created

224
00:10:34,860 --> 00:10:36,760
when you create the scheduled tasks.

225
00:10:36,760 --> 00:10:37,760
Okay?

226
00:10:37,760 --> 00:10:42,240
So in a nutshell, we have
defined what we want to detect

227
00:10:42,240 --> 00:10:44,330
and how we're gonna be detecting it.

228
00:10:44,330 --> 00:10:45,410
Okay.

229
00:10:45,410 --> 00:10:47,949
At this point we're gonna be needing to do

230
00:10:47,950 --> 00:10:49,800
is collect the data, right?

231
00:10:49,800 --> 00:10:51,790
And that's the traditional
forensic processes.

232
00:10:51,790 --> 00:10:56,790
I'm gonna be using my SIEM or
my EDR or a forensic agents

233
00:10:57,050 --> 00:10:59,920
to collect the artifacts,
then I will process them

234
00:10:59,920 --> 00:11:02,329
as needed with other tools potentially

235
00:11:02,330 --> 00:11:04,590
like KAPE or Eric Zimmerman's tools

236
00:11:04,590 --> 00:11:08,170
or at Plaso-Volatility or what have you.

237
00:11:08,170 --> 00:11:12,719
And it will eventually
generate an number of files

238
00:11:12,720 --> 00:11:15,950
in different formats,
typically JSON or CSV.

239
00:11:15,950 --> 00:11:16,783
Cool.

240
00:11:16,783 --> 00:11:19,870
So at this point we have
finished the DFIR process

241
00:11:19,870 --> 00:11:24,315
without analysis, but with the
output of our forensic tools.

242
00:11:24,315 --> 00:11:25,310
What next?

243
00:11:25,310 --> 00:11:27,349
We're gonna focusing now

244
00:11:27,350 --> 00:11:29,420
on the machine learning part of the scene,

245
00:11:29,420 --> 00:11:31,420
which is the most interesting, obviously.

246
00:11:31,420 --> 00:11:32,719
Okay.

247
00:11:32,720 --> 00:11:35,320
Once we have the output
of this tool, what we,

248
00:11:35,320 --> 00:11:37,610
you can see in the slide as raw output,

249
00:11:37,610 --> 00:11:42,340
what you have is the data, the
output of your forensic tools

250
00:11:42,340 --> 00:11:46,420
in typically a JSON or CSV format, okay?

251
00:11:46,420 --> 00:11:48,533
However, if you use different tools,

252
00:11:48,533 --> 00:11:49,690
different tools are gonna be having

253
00:11:49,690 --> 00:11:52,180
different naming conventions
for different columns.

254
00:11:52,180 --> 00:11:53,270
And that's gonna be a mess.

255
00:11:53,270 --> 00:11:55,829
So what we need to do
is harmonize that model

256
00:11:55,830 --> 00:11:57,637
in what we call the HAM model.

257
00:11:57,637 --> 00:11:59,780
The data science forensics library

258
00:11:59,780 --> 00:12:02,420
that I was talking about
helps in harmonizing

259
00:12:02,420 --> 00:12:05,329
that for different tools like Kansa, Kape,

260
00:12:05,330 --> 00:12:06,770
Volatility, and some others.

261
00:12:06,770 --> 00:12:09,600
From there, we will apply
the machine learning process

262
00:12:09,600 --> 00:12:11,710
of feature selection,
and feature engineering

263
00:12:11,710 --> 00:12:14,220
to get what we call HTML format,

264
00:12:14,220 --> 00:12:17,130
which could be the
features that are useful

265
00:12:17,130 --> 00:12:19,100
for that specific type of detection

266
00:12:20,100 --> 00:12:22,880
from a machine learning
model point of view.

267
00:12:22,880 --> 00:12:25,810
Let me go now to
explaining you what model,

268
00:12:25,810 --> 00:12:27,579
I have been talking already
about the autoencoder,

269
00:12:27,580 --> 00:12:28,907
but what is autoencoder?

270
00:12:28,907 --> 00:12:30,040
And how it works.

271
00:12:30,040 --> 00:12:31,360
How does it work?

272
00:12:31,360 --> 00:12:32,540
Well, the things that following

273
00:12:32,540 --> 00:12:34,800
what you're gonna be doing is presenting

274
00:12:34,800 --> 00:12:39,050
a number of data entries to this model,

275
00:12:39,050 --> 00:12:40,380
to this autoencoder.

276
00:12:40,380 --> 00:12:43,140
We're gonna be kind of going high level,

277
00:12:43,140 --> 00:12:44,360
and we're gonna be imagining

278
00:12:44,360 --> 00:12:48,020
we are trying to detect cats,
anomalies in cats, okay?

279
00:12:48,020 --> 00:12:49,610
So what we're gonna be doing is presenting

280
00:12:49,610 --> 00:12:51,880
many, many, many, many different cats.

281
00:12:51,880 --> 00:12:56,733
And the machine learning
model will understand

282
00:12:56,733 --> 00:12:59,160
will get the essence of what a cat is,

283
00:12:59,160 --> 00:13:02,030
what it's what you can see in the middle.

284
00:13:02,030 --> 00:13:04,930
This is by doing a process
of dimensionality reduction,

285
00:13:04,930 --> 00:13:07,050
but removes all the non-important stuff

286
00:13:07,050 --> 00:13:10,329
and retains the essence of
what you're looking for.

287
00:13:10,329 --> 00:13:13,800
And then what it will try
to do is with that essence,

288
00:13:13,800 --> 00:13:16,810
try to reconstruct the original cat.

289
00:13:16,810 --> 00:13:21,410
So if it's a very normal
cat, the essence of the cat

290
00:13:21,410 --> 00:13:22,834
will be very well-defined

291
00:13:22,835 --> 00:13:25,880
and you will be able
to reconstruct the cat.

292
00:13:25,880 --> 00:13:28,117
But if it's not a cat,
you will not be able

293
00:13:29,236 --> 00:13:30,790
to reconstruct the cat properly,

294
00:13:30,790 --> 00:13:33,189
and that will create a huge error.

295
00:13:33,190 --> 00:13:34,937
The measure of that error,

296
00:13:34,937 --> 00:13:36,760
that you will be seen
in a couple of slides,

297
00:13:36,760 --> 00:13:39,650
will tell you if we are good,

298
00:13:39,650 --> 00:13:42,840
I mean, if it's a cat or
if it's not a cat, okay?

299
00:13:42,840 --> 00:13:47,840
So, how does these map to
the discussion we're having?

300
00:13:48,740 --> 00:13:51,370
Here, what we have let's think

301
00:13:51,370 --> 00:13:56,370
about the schedule task events logs, okay?

302
00:13:56,847 --> 00:13:58,040
And the specific events,

303
00:13:58,040 --> 00:14:00,740
that's what we're gonna be
providing each of the columns,

304
00:14:00,740 --> 00:14:02,910
each of the fields of those event logs

305
00:14:02,910 --> 00:14:05,170
is what we're gonna be providing
to the machine learning,

306
00:14:05,170 --> 00:14:08,849
to the machine learning model,
okay, to the outline cover.

307
00:14:08,850 --> 00:14:10,630
And then as you can see in this slide,

308
00:14:10,630 --> 00:14:12,390
were gonna be providing the event ID,

309
00:14:12,390 --> 00:14:14,860
the computer name, et
cetera, et cetera, et cetera,

310
00:14:14,860 --> 00:14:17,080
for mainly computers, okay?

311
00:14:17,080 --> 00:14:20,143
So after seeing this,

312
00:14:21,010 --> 00:14:22,700
it will know what the essence

313
00:14:23,650 --> 00:14:27,430
of a scheduled task logs look like.

314
00:14:27,430 --> 00:14:29,469
I mean, we'll be able to identify

315
00:14:29,470 --> 00:14:32,820
what is not the same as the rest, okay?

316
00:14:32,820 --> 00:14:35,330
Note that this is unsupervised.

317
00:14:35,330 --> 00:14:39,010
At no point, there is a
human saying, this is a cat,

318
00:14:39,010 --> 00:14:40,560
or this is not a cat.

319
00:14:40,560 --> 00:14:42,689
What the model will do is like,

320
00:14:42,690 --> 00:14:46,570
this is very different from
what I know that a cat is,

321
00:14:46,570 --> 00:14:49,760
you need to evaluate if
this is a cat or not a cat,

322
00:14:49,760 --> 00:14:51,920
but it will give you the
measure of the anomaly.

323
00:14:51,921 --> 00:14:56,530
And this is where we go to the precisely,

324
00:14:56,530 --> 00:14:57,569
the measure of the anomaly,

325
00:14:57,570 --> 00:15:00,000
what we call the loss in
machine learning, okay?

326
00:15:00,000 --> 00:15:04,500
With a standard autoencoder,
if you present normal cats,

327
00:15:04,500 --> 00:15:07,153
you will see that the error is very low,

328
00:15:07,153 --> 00:15:09,040
as you see in the bottom
of the screen, okay?

329
00:15:09,040 --> 00:15:10,280
The normal cats.

330
00:15:10,280 --> 00:15:12,380
If you present cats, which are still cats,

331
00:15:12,380 --> 00:15:16,770
but weird cats, okay, it
will give you a higher error,

332
00:15:16,770 --> 00:15:19,199
which are the cats that you are seeing

333
00:15:19,199 --> 00:15:20,859
at the middle of the screen,

334
00:15:20,860 --> 00:15:25,300
and if you see, if you
provide to the neural network

335
00:15:25,300 --> 00:15:28,160
an elephant, it will say, "Oh my God,"

336
00:15:28,160 --> 00:15:31,270
the reconstruction error is huge

337
00:15:31,270 --> 00:15:34,350
because I cannot reconstruct an elephant

338
00:15:34,350 --> 00:15:36,910
with the essence of a cat, okay.

339
00:15:36,910 --> 00:15:41,469
So by knowing how big
is the loss, the error,

340
00:15:41,470 --> 00:15:46,180
we can know if that is
an anomaly or not, okay?

341
00:15:46,180 --> 00:15:48,402
There is a, we've been doing a research,

342
00:15:49,349 --> 00:15:51,461
further on autoencoders.

343
00:15:51,461 --> 00:15:52,740
There is the type of autoencoder,

344
00:15:52,740 --> 00:15:55,673
which is extremely
interesting, it's called LSTM.

345
00:15:55,673 --> 00:15:57,720
LSTM autoencoders are actually

346
00:15:57,720 --> 00:16:01,190
long, short-term memory autoencoder.

347
00:16:01,190 --> 00:16:05,650
Are autoencoders that have
memory, they remember the past.

348
00:16:05,650 --> 00:16:10,650
So it's not only now if
the cat is strange or not,

349
00:16:12,896 --> 00:16:17,890
it will also take into
account when the cat appears.

350
00:16:17,890 --> 00:16:21,680
So if I have a normal cat, which
is a completely normal cat,

351
00:16:21,680 --> 00:16:26,680
but that cat never appears at
two AM, and all of a sudden

352
00:16:26,940 --> 00:16:29,700
I see the cat appearing
at two AM in the morning,

353
00:16:29,700 --> 00:16:31,387
the LSTM autoencoder will say,

354
00:16:31,387 --> 00:16:33,290
"Hey, I have an anomaly here."

355
00:16:33,290 --> 00:16:36,380
This is an anomaly,
not because it's a cat,

356
00:16:36,380 --> 00:16:39,680
a strange cat, because it's
a cat that is appearing

357
00:16:39,680 --> 00:16:42,339
at the time that is not supposed to, okay.

358
00:16:42,340 --> 00:16:44,320
So basically can detect an anomaly,

359
00:16:44,320 --> 00:16:46,765
taking the time into account.

360
00:16:46,765 --> 00:16:47,689
Cool.

361
00:16:47,690 --> 00:16:50,770
So we have selected two models,
the standard autoencoder

362
00:16:50,770 --> 00:16:52,930
and the LSTM autoencoder.

363
00:16:52,930 --> 00:16:56,670
And the next thing we're
gonna be doing is injecting

364
00:16:56,670 --> 00:17:01,670
that malicious let's say
attack in our data, okay?

365
00:17:02,460 --> 00:17:06,160
Since we normally don't
have the SolarWinds attack

366
00:17:06,160 --> 00:17:09,030
in our networks, we
will have to emulate it

367
00:17:09,030 --> 00:17:11,020
and for that, we will use ADAM, okay.

368
00:17:11,020 --> 00:17:14,280
ADAM will emulate the attack by injecting

369
00:17:14,280 --> 00:17:17,910
the malicious attack following,

370
00:17:17,910 --> 00:17:21,950
well, the indications
you provide in the data

371
00:17:21,950 --> 00:17:24,940
that is gonna be ingested by
the machine learning model.

372
00:17:24,940 --> 00:17:27,542
And then we will see if
the machine learning model

373
00:17:27,542 --> 00:17:29,570
is able to catch the detection or not

374
00:17:29,570 --> 00:17:33,629
by evaluating the anomalies
that it has produced.

375
00:17:33,630 --> 00:17:34,463
Okay.

376
00:17:34,463 --> 00:17:35,389
So fantastic.

377
00:17:35,390 --> 00:17:39,320
Let's say we are happy, we have identified

378
00:17:39,320 --> 00:17:42,820
that these autoencoder
and autoencoder LSTM

379
00:17:42,820 --> 00:17:46,470
are good at detecting this
type of technique, okay?

380
00:17:46,470 --> 00:17:48,770
And we have fine tune, certain parameters

381
00:17:48,770 --> 00:17:50,020
to make it effective.

382
00:17:50,020 --> 00:17:53,040
So what would we do at that point?

383
00:17:53,040 --> 00:17:56,710
What we would do, could be to
go to the real world, okay.

384
00:17:56,710 --> 00:17:59,750
Now we have this function,
as you will see later,

385
00:17:59,750 --> 00:18:02,704
is find machine learning anomalies, okay.

386
00:18:02,704 --> 00:18:04,413
And which would provide the data,

387
00:18:05,740 --> 00:18:08,990
the real-world data
and you will figure out

388
00:18:08,990 --> 00:18:10,780
you will apply the machine learning models

389
00:18:10,780 --> 00:18:14,460
and figure out what anomalies are shown.

390
00:18:14,460 --> 00:18:16,460
If you have multiple artifacts,

391
00:18:16,460 --> 00:18:18,980
you can follow the traditional

392
00:18:18,980 --> 00:18:22,750
detailed investigation process
of analyzing one thing.

393
00:18:22,750 --> 00:18:25,880
And then from the
findings, you find a filter

394
00:18:25,880 --> 00:18:27,330
or the type of evidence

395
00:18:27,330 --> 00:18:29,149
and then analyze that and filter

396
00:18:29,150 --> 00:18:31,880
and pivot to another artifacts
and so on and so forth.

397
00:18:31,880 --> 00:18:34,370
You could use the CTI platform, okay.

398
00:18:34,370 --> 00:18:37,270
In case they are IOCs, but
remember in this game today,

399
00:18:37,270 --> 00:18:40,180
we're not gonna be counting with,

400
00:18:40,180 --> 00:18:42,450
we are gonna be assuming
there are no IOCs.

401
00:18:42,450 --> 00:18:45,460
So we will leave behind the CTI platform.

402
00:18:45,460 --> 00:18:47,760
We will just focus on the manual analysis.

403
00:18:47,760 --> 00:18:50,210
So this is in the end, what we could do.

404
00:18:50,210 --> 00:18:53,650
So we could actually
analyze the event logs

405
00:18:53,650 --> 00:18:54,980
for the scheduled tasks.

406
00:18:54,980 --> 00:18:59,980
Then select the top 25%
anomalies, for instance,

407
00:19:00,170 --> 00:19:03,120
you will decide that percentage
based on your analysis,

408
00:19:03,120 --> 00:19:05,350
on the machine learning phase.

409
00:19:05,350 --> 00:19:10,060
Then you extract those
and filter the file list,

410
00:19:10,060 --> 00:19:12,980
for instance, you can
generate a file listing

411
00:19:12,980 --> 00:19:16,340
with MAC time or other tools, okay.

412
00:19:16,340 --> 00:19:17,870
Analyzer McAfee et cetera.

413
00:19:17,870 --> 00:19:20,010
And then you can filter those files

414
00:19:20,010 --> 00:19:23,879
which have been presented as
anomalies in the event logs

415
00:19:23,880 --> 00:19:24,770
and so on and so forth.

416
00:19:24,770 --> 00:19:27,510
You could pivot to
other forensic artifacts

417
00:19:27,510 --> 00:19:29,510
as you do in the real world.

418
00:19:29,510 --> 00:19:30,343
Okay.

419
00:19:30,343 --> 00:19:32,640
And to end up with, you could analyze,

420
00:19:32,640 --> 00:19:36,610
your analysts could analyze what is found,

421
00:19:36,610 --> 00:19:37,840
what are the top anomalies

422
00:19:37,840 --> 00:19:39,959
and then dig deeper on those.

423
00:19:39,960 --> 00:19:41,583
So demo time guys.

424
00:19:41,583 --> 00:19:44,540
What we're gonna be doing now is making,

425
00:19:44,540 --> 00:19:47,593
identifying if this
works in real-world data.

426
00:19:47,593 --> 00:19:50,490
What we're gonna be presenting is,

427
00:19:50,490 --> 00:19:51,470
we're gonna be restricting

428
00:19:51,470 --> 00:19:54,540
our analysis to only two artifacts.

429
00:19:54,540 --> 00:19:57,840
I think it's enough to show how this works

430
00:19:57,840 --> 00:20:00,350
and it's to not gonna be
complicating things too much.

431
00:20:00,350 --> 00:20:01,183
Okay.

432
00:20:01,183 --> 00:20:03,370
Those two artifacts are
gonna be the event logs,

433
00:20:03,370 --> 00:20:05,679
specifically, the
schedule task event logs,

434
00:20:05,680 --> 00:20:08,560
and the file listing,

435
00:20:08,560 --> 00:20:10,500
file listing means not
the contents of the files,

436
00:20:10,500 --> 00:20:12,090
but the metadata the files

437
00:20:12,090 --> 00:20:16,590
for the files in the Windows
System32 tasks folder,

438
00:20:16,590 --> 00:20:21,159
which are where their
tasks files are created

439
00:20:21,160 --> 00:20:23,900
when you create under
scheduled task, okay?

440
00:20:23,900 --> 00:20:28,850
This is gonna be just, I want you to know

441
00:20:28,850 --> 00:20:31,100
that this is real world data.

442
00:20:31,100 --> 00:20:32,429
This is not fake data.

443
00:20:32,430 --> 00:20:35,469
We have been partnering
with a customer of ours

444
00:20:35,469 --> 00:20:38,930
in order to do these demos,
Fortune Global 500 company.

445
00:20:38,930 --> 00:20:41,780
They provided several amounts of data.

446
00:20:41,780 --> 00:20:44,240
We have selected 30 days of data, okay.

447
00:20:44,240 --> 00:20:49,240
From 10, sorry from a 100
production servers, okay?

448
00:20:49,738 --> 00:20:51,438
(indistinct) accounts for 200,000,

449
00:20:56,021 --> 00:20:58,750
220,000 events, more or less

450
00:20:58,750 --> 00:21:03,750
for the schedule tasks
event log exclusively

451
00:21:03,950 --> 00:21:08,950
and about 23, 24 million lines
in the file system timelines.

452
00:21:10,210 --> 00:21:11,820
Okay, so quite a lot of data.

453
00:21:11,820 --> 00:21:15,889
Again, totally real-world
data production servers.

454
00:21:15,890 --> 00:21:19,010
So the next thing we've
done is we've processed this

455
00:21:19,010 --> 00:21:21,510
forensically, as I've shown
you in previous slides

456
00:21:21,510 --> 00:21:24,810
and we have injected
the SolarWinds attack.

457
00:21:24,810 --> 00:21:27,491
What we've done is gone to the sources.

458
00:21:27,491 --> 00:21:29,129
The (indistinct) sources that explain you

459
00:21:29,130 --> 00:21:32,180
what the SolarWinds
attack actually looks like

460
00:21:32,180 --> 00:21:35,620
from the point of view of
the scheduled tasks creation.

461
00:21:35,620 --> 00:21:37,350
And then what we've injected

462
00:21:37,350 --> 00:21:39,490
is the scale tasking two location,

463
00:21:39,490 --> 00:21:44,334
in the event log data and in
the file listing data, okay?

464
00:21:44,334 --> 00:21:48,840
And what we are gonna be deciding

465
00:21:50,180 --> 00:21:53,070
or framing is the following.

466
00:21:53,070 --> 00:21:56,470
If we count with the possibility

467
00:21:56,470 --> 00:21:59,490
from the resource endpoint of
view in my threat hunting team

468
00:21:59,490 --> 00:22:02,940
to analyze the top 100 anomalies, okay?

469
00:22:02,940 --> 00:22:05,150
So my team will be able to go through

470
00:22:05,150 --> 00:22:09,350
all the 10, 100 top
anomalies and analyze deeper

471
00:22:09,350 --> 00:22:12,510
if they need to identify if
it's a false, positive or not.

472
00:22:12,510 --> 00:22:14,190
We're gonna be finding 100.

473
00:22:14,190 --> 00:22:15,670
It's gonna be depending on your resources,

474
00:22:15,670 --> 00:22:18,720
if you decide there's
1000 or only 50, okay?

475
00:22:18,720 --> 00:22:20,940
It doesn't matter, let's define 100.

476
00:22:20,940 --> 00:22:22,210
And the question would be,

477
00:22:22,210 --> 00:22:24,740
I will detect the SolarWinds attack,

478
00:22:24,740 --> 00:22:29,740
if the malicious task is
in the top 100 anomalies,

479
00:22:30,360 --> 00:22:33,193
because that's the
resources I could use, okay.

480
00:22:34,430 --> 00:22:37,230
Yeah, it's not the two, for
you to understand very well

481
00:22:37,230 --> 00:22:38,470
what I'm gonna be doing.

482
00:22:38,470 --> 00:22:41,740
I'm gonna be dividing the
demo in three phases, okay.

483
00:22:41,740 --> 00:22:43,970
The first phase, in the first phase,

484
00:22:43,970 --> 00:22:48,617
I'm gonna be introducing
you to the autoencoder.

485
00:22:49,850 --> 00:22:51,840
I want you to feel, to get a feeling

486
00:22:51,840 --> 00:22:55,310
of how an autoencoder
actually works, okay.

487
00:22:55,310 --> 00:22:56,929
So this has nothing to do,

488
00:22:56,930 --> 00:22:59,200
this first part has nothing to do

489
00:22:59,200 --> 00:23:00,930
with the SolarWinds attack, okay.

490
00:23:00,930 --> 00:23:02,470
It's just to get a feeling

491
00:23:02,470 --> 00:23:05,600
and I will explain some machine
learning concepts and such.

492
00:23:05,600 --> 00:23:09,419
The second part, we would
use the pivoting methodology.

493
00:23:09,420 --> 00:23:12,450
Okay, so in that I will explain later

494
00:23:12,450 --> 00:23:13,920
what exactly what we're gonna be doing,

495
00:23:13,920 --> 00:23:16,293
but we are gonna be a pivoting,

496
00:23:16,293 --> 00:23:18,429
we're gonna be analyzing data logs.

497
00:23:18,430 --> 00:23:21,160
We're gonna be analyzing the file listing.

498
00:23:21,160 --> 00:23:24,300
And then we're gonna be
pivoting from the event logs

499
00:23:24,300 --> 00:23:26,310
to the file listing, okay?

500
00:23:26,310 --> 00:23:28,020
So that's another technique.

501
00:23:28,020 --> 00:23:31,070
And the third phase of
the demo we'll cover

502
00:23:31,070 --> 00:23:34,720
is specifically the use
of the LSTM autoencoder

503
00:23:35,700 --> 00:23:40,700
that remember is able to
detect the time variance, okay.

504
00:23:46,210 --> 00:23:48,470
Or the anomalies in time,

505
00:23:48,470 --> 00:23:51,760
and we will try to define

506
00:23:51,760 --> 00:23:53,420
or what we'll try to identify

507
00:23:53,420 --> 00:23:58,250
if that's actually doing
the detection, okay.

508
00:23:58,250 --> 00:24:02,630
So let's start with getting a nice feeling

509
00:24:02,630 --> 00:24:05,830
about this autoencoder.

510
00:24:05,830 --> 00:24:07,560
How does it work?

511
00:24:07,560 --> 00:24:09,560
Let's learn about it.

512
00:24:09,560 --> 00:24:10,750
Let's start with the demo.

513
00:24:10,750 --> 00:24:15,070
Here we have a total of 6,000 entries,

514
00:24:15,070 --> 00:24:17,560
which would be the whole data
set, don't worry about that.

515
00:24:17,560 --> 00:24:22,560
And we have a total of
8,500 unique entries.

516
00:24:22,600 --> 00:24:25,332
What we're gonna be doing
is I'm gonna be running

517
00:24:25,333 --> 00:24:29,080
a machine learning function
called find anomalies ML,

518
00:24:29,080 --> 00:24:31,639
which I have coded, okay.

519
00:24:31,640 --> 00:24:33,630
This as I said, this is open source

520
00:24:33,630 --> 00:24:37,140
and I will be running this on the,

521
00:24:37,140 --> 00:24:38,770
with a simple autoencoder,

522
00:24:38,770 --> 00:24:40,440
as you're seeing now in the screen,

523
00:24:40,440 --> 00:24:42,870
in order to produce some anomalies, okay.

524
00:24:42,870 --> 00:24:44,649
In the end, it's gonna be telling me

525
00:24:44,650 --> 00:24:48,300
what are the most anomalies
and the less anomalies entries

526
00:24:48,300 --> 00:24:49,750
by looking at the error.

527
00:24:49,750 --> 00:24:54,750
I have injected a first event,
which is really strange.

528
00:24:56,220 --> 00:24:58,000
I made it up, so it's really strange.

529
00:24:58,000 --> 00:25:00,950
And I wouldn't run the machine
learning for three loops.

530
00:25:00,950 --> 00:25:02,080
Why three loops?

531
00:25:02,080 --> 00:25:03,909
Well, it doesn't need to be three.

532
00:25:03,910 --> 00:25:06,970
It can be several loops,
but it's important

533
00:25:06,970 --> 00:25:08,130
because the machine learning

534
00:25:08,130 --> 00:25:11,320
has a degree of randomness, okay.

535
00:25:11,320 --> 00:25:14,919
So you may run it one time
and not work properly.

536
00:25:14,920 --> 00:25:19,280
So and you can see there,
the machine learning model,

537
00:25:19,280 --> 00:25:21,090
which is based on three layers, okay.

538
00:25:21,090 --> 00:25:24,180
As I was showing before is a
simplification the autoencoder

539
00:25:24,181 --> 00:25:27,750
And if you run this, you
will detect the event

540
00:25:27,750 --> 00:25:29,900
on position number 20.

541
00:25:29,900 --> 00:25:34,250
20, top anomaly out of 8,500.

542
00:25:34,250 --> 00:25:36,930
Well, that's a pretty
big, good number, right?

543
00:25:36,930 --> 00:25:40,660
So it has detected that is very anomalous.

544
00:25:40,660 --> 00:25:42,370
I'm gonna be creating
now, I'm gonna be running

545
00:25:42,370 --> 00:25:47,370
the neural network with a
less strange event, okay?

546
00:25:48,113 --> 00:25:51,290
This event is more similar
to the rest of the events

547
00:25:51,290 --> 00:25:53,920
that are seen in those servers, okay.

548
00:25:53,920 --> 00:25:56,640
And how, in what terms?

549
00:25:56,640 --> 00:25:58,510
Well, in the different fields, okay.

550
00:25:58,510 --> 00:26:02,120
Username, action name,
et cetera, et cetera.

551
00:26:02,120 --> 00:26:05,649
And if I run the machine
learning model, again,

552
00:26:05,650 --> 00:26:10,650
I will basically detect,
this as anomaly number 271

553
00:26:12,560 --> 00:26:16,647
out of 8,500, which is
still not bad, okay.

554
00:26:16,647 --> 00:26:20,090
So if I provide something
very anomalous top 20,

555
00:26:20,090 --> 00:26:24,560
if it's not so anomalies top 200.

556
00:26:24,560 --> 00:26:26,899
Let's say, or top 300, let's do something

557
00:26:26,900 --> 00:26:28,200
which is very average.

558
00:26:28,200 --> 00:26:30,800
I'm gonna be injecting an event,
which is very, very similar

559
00:26:30,800 --> 00:26:32,740
to the rest of the events

560
00:26:32,740 --> 00:26:36,110
that are happening in
that in those servers.

561
00:26:36,110 --> 00:26:37,189
And guess what?

562
00:26:37,190 --> 00:26:42,190
My detection is 2000 in terms
of anomaly, 2000 out of 8,500.

563
00:26:44,560 --> 00:26:48,020
So obviously when I feed the
neural networks, something

564
00:26:48,020 --> 00:26:52,650
which is very similar to what you've seen,

565
00:26:52,650 --> 00:26:57,650
the thing is that you will not be able

566
00:26:57,952 --> 00:26:59,390
to detect this as this anomaly

567
00:26:59,390 --> 00:27:01,828
because this is very, in
reality is not an anomaly.

568
00:27:01,828 --> 00:27:05,639
It's very similar to
the other events, okay?

569
00:27:05,640 --> 00:27:06,473
Great.

570
00:27:06,473 --> 00:27:08,230
So now you are familiar

571
00:27:08,230 --> 00:27:10,457
on how an autoencoder works.

572
00:27:10,457 --> 00:27:15,457
The more a strange the input,
the bigger the anomaly rate

573
00:27:17,410 --> 00:27:21,437
or the top, the upper, it
appears in the list of anomalies,

574
00:27:21,437 --> 00:27:25,290
the more similar to the
rest of the data set,

575
00:27:25,290 --> 00:27:30,290
the lower, it appears in
anomaly scale, let's say, right?

576
00:27:31,250 --> 00:27:33,380
So let's go now to the interesting part,

577
00:27:33,380 --> 00:27:36,391
let's go to the SolarWinds case.

578
00:27:36,391 --> 00:27:38,960
The phase number two, as you may remember,

579
00:27:38,960 --> 00:27:40,370
is the pivoting phase.

580
00:27:40,370 --> 00:27:42,770
So what I'm gonna be
doing is, three things.

581
00:27:42,770 --> 00:27:46,420
First, I'm gonna be running
my machine learning model,

582
00:27:46,420 --> 00:27:48,573
the autoencoder, the standard autoencoder,

583
00:27:49,957 --> 00:27:52,303
on only the event log data.

584
00:27:53,840 --> 00:27:58,840
Then I will be running
the simple autoencoder

585
00:27:59,054 --> 00:28:04,054
on the file listing
data, standalone, okay?

586
00:28:04,356 --> 00:28:09,356
And in third place, I will
filter the file listing

587
00:28:09,950 --> 00:28:14,950
with the anomalies, with
the top 25% anomalies,

588
00:28:15,230 --> 00:28:18,730
of the event log data.

589
00:28:18,730 --> 00:28:20,870
So let's say I run the event log data,

590
00:28:20,870 --> 00:28:24,419
I get the top 25 most strange things.

591
00:28:24,420 --> 00:28:28,870
And then out of the task
files, I filter those

592
00:28:28,870 --> 00:28:31,540
that appear in the top 25%, okay.

593
00:28:31,540 --> 00:28:34,399
That is useful to eliminate noise,

594
00:28:34,400 --> 00:28:38,453
that could be created by other things

595
00:28:39,750 --> 00:28:44,750
in the file listing data set, okay.

596
00:28:45,190 --> 00:28:48,010
So that pivoting will just
seem like in the real world

597
00:28:48,010 --> 00:28:51,510
is gonna be filtering out noise, okay.

598
00:28:51,510 --> 00:28:54,146
The third, so this is gonna be

599
00:28:54,146 --> 00:28:56,520
what we're gonna be seeing now, okay.

600
00:28:56,520 --> 00:28:59,210
Let's go (indistinct),
remember, first part

601
00:28:59,210 --> 00:29:03,230
analyzing only the
anomalies out of the 8,500,

602
00:29:03,230 --> 00:29:05,727
I'm gonna be looking at
the SolarWinds attack.

603
00:29:05,727 --> 00:29:09,100
The SolarWinds attack, you're
seeing the four entries

604
00:29:09,100 --> 00:29:11,500
that are associated to event cost manager,

605
00:29:11,500 --> 00:29:14,623
which is the name of the schedule of tasks

606
00:29:14,623 --> 00:29:17,300
that is created by SolarWinds, okay.

607
00:29:17,300 --> 00:29:18,133
There are two events

608
00:29:18,133 --> 00:29:20,663
that are associated to
the creation of the task,

609
00:29:20,663 --> 00:29:24,800
and two events associated to
the execution of the task.

610
00:29:24,800 --> 00:29:28,169
And as you can see, the
results are not very good.

611
00:29:28,170 --> 00:29:30,350
The tasks are not so anomalous.

612
00:29:30,350 --> 00:29:35,350
I get a detection of 1,700 out of 8,500.

613
00:29:36,730 --> 00:29:38,060
That's not very good.

614
00:29:38,060 --> 00:29:39,919
So from my analysis point of view,

615
00:29:39,920 --> 00:29:44,007
I would have not detected it,
okay, it's not in my top 100.

616
00:29:44,007 --> 00:29:46,110
Let's go to file listing.

617
00:29:46,110 --> 00:29:47,677
Now, I'm gonna be analyzing,

618
00:29:47,677 --> 00:29:49,709
what are the top anomalies,

619
00:29:49,710 --> 00:29:54,710
and if this entry, this
malicious scale of task,

620
00:29:55,520 --> 00:29:57,610
as you can see, I have inserted it

621
00:29:57,610 --> 00:30:02,178
in the file listing data set.

622
00:30:02,178 --> 00:30:04,500
You can see the different timestamps.

623
00:30:04,500 --> 00:30:06,090
You can see the size of the file,

624
00:30:06,090 --> 00:30:07,689
the computer name, et cetera.

625
00:30:07,690 --> 00:30:10,170
I'm gonna be running
the simple autoencoder.

626
00:30:10,170 --> 00:30:15,047
And I'm gonna be seeing how
strange that SolarWinds file is.

627
00:30:16,289 --> 00:30:17,434
And guess what?

628
00:30:17,434 --> 00:30:18,267
1, 800.

629
00:30:19,759 --> 00:30:20,592
Not very good.

630
00:30:20,592 --> 00:30:21,871
So I'm not able to detect

631
00:30:21,871 --> 00:30:24,940
the, as a very big anomaly

632
00:30:24,940 --> 00:30:27,340
on the file system on top.

633
00:30:27,340 --> 00:30:29,169
So the third part, as you may remember,

634
00:30:29,170 --> 00:30:31,640
I will gonna be filtering the file listing

635
00:30:31,640 --> 00:30:36,480
with the top 25% anomalies
of the event logs.

636
00:30:36,480 --> 00:30:40,240
And that gives me a data set,
which is very reduced, 735.

637
00:30:40,240 --> 00:30:42,800
And in that reduced data set,

638
00:30:42,800 --> 00:30:47,800
I'm gonna be running my
machine learning process, okay.

639
00:30:49,750 --> 00:30:51,223
Let's see what we get.

640
00:30:52,080 --> 00:30:57,080
Since the detection is top
number 95, sorry, 91 anomaly

641
00:31:00,420 --> 00:31:03,420
according to the scenario
we had originally defined,

642
00:31:03,420 --> 00:31:08,420
I have detected that
malicious file on the top 100.

643
00:31:08,770 --> 00:31:12,889
So my analysts would have
investigated it in-depth

644
00:31:12,890 --> 00:31:17,890
and would have luckily
detected this attack, okay?

645
00:31:18,590 --> 00:31:22,470
So, well, it looks like a summary.

646
00:31:22,470 --> 00:31:27,347
If I do the analysis of the
scheduled task event only

647
00:31:28,530 --> 00:31:30,774
I don't detect anything notable.

648
00:31:30,774 --> 00:31:34,690
If I do the analysis,
only on the file listing,

649
00:31:34,691 --> 00:31:37,700
I don't detect anything notable either,

650
00:31:37,700 --> 00:31:40,850
but if I pivot, as I could do
in a real-world investigation

651
00:31:40,850 --> 00:31:45,850
on the top 25% anomalies,
I get a reduced data set,

652
00:31:46,090 --> 00:31:49,899
and my hit is now in the top 100,

653
00:31:49,900 --> 00:31:54,700
which means I could have
detected the SolarWinds attack

654
00:31:54,700 --> 00:31:56,570
in my threat hunting process.

655
00:31:56,570 --> 00:31:58,700
Fantastic, okay.

656
00:31:58,700 --> 00:31:59,533
That's great.

657
00:31:59,533 --> 00:32:01,139
So we have methodology.

658
00:32:01,140 --> 00:32:04,503
Let's change now to the
phase three of the demo.

659
00:32:04,503 --> 00:32:08,050
In this case, we're gonna be
using the LSTM autoencoder

660
00:32:08,050 --> 00:32:11,419
only for the event logs.

661
00:32:11,420 --> 00:32:12,780
Okay, so we are not gonna be pivoting,

662
00:32:12,780 --> 00:32:15,320
we are gonna be keeping it simple, okay.

663
00:32:15,320 --> 00:32:20,320
Let's see how good this new
LSTM autoencoder technology,

664
00:32:21,500 --> 00:32:26,390
okay, in detecting the SolarWinds

665
00:32:26,390 --> 00:32:29,530
scheduled tasks event logs, okay?

666
00:32:29,530 --> 00:32:32,820
So remember that we have injected

667
00:32:32,820 --> 00:32:35,540
four scheduled tasks event logs,

668
00:32:35,540 --> 00:32:38,290
two for the creation and
two for the execution,

669
00:32:38,290 --> 00:32:40,260
that's the normal thing, okay.

670
00:32:40,260 --> 00:32:43,260
And what we're gonna be doing is running

671
00:32:43,260 --> 00:32:46,020
the machine learning
process on 30 days of data.

672
00:32:46,020 --> 00:32:48,440
As you can see, there are 30 days of data

673
00:32:48,440 --> 00:32:51,240
that accounts for approximately
200,000, 220,000, events

674
00:32:54,040 --> 00:32:56,280
as you're seeing this screen now

675
00:32:56,280 --> 00:32:58,020
that's a lot of events, right?

676
00:32:58,020 --> 00:32:59,777
We're gonna be running
the machine learning,

677
00:32:59,777 --> 00:33:01,820
the LSTM machine learning model.

678
00:33:01,820 --> 00:33:03,179
As you can see, it's a little different

679
00:33:03,180 --> 00:33:04,966
than the previous one.

680
00:33:04,966 --> 00:33:06,669
It's an LSTM learning model,

681
00:33:06,670 --> 00:33:10,170
it has other other layers
that we didn't see before.

682
00:33:10,170 --> 00:33:12,790
And since this is a much larger data set,

683
00:33:12,790 --> 00:33:17,040
it will take about 15
minutes for the for the data

684
00:33:17,040 --> 00:33:20,230
to get processed to get the predictions,

685
00:33:20,230 --> 00:33:22,310
to identify the anomalies, okay.

686
00:33:22,310 --> 00:33:24,250
I have obviously edited the video,

687
00:33:24,250 --> 00:33:29,070
so you don't have to wait 20
minutes here, 15 or 20 minutes.

688
00:33:29,070 --> 00:33:29,923
Guess what?

689
00:33:30,887 --> 00:33:35,887
Now, what is the detection
of the creation of the tasks?

690
00:33:36,580 --> 00:33:39,800
The creation of the tasks,
which are event logs

691
00:33:39,800 --> 00:33:42,720
event IDs one, zero, eight,
seven, one, four, zero

692
00:33:42,720 --> 00:33:46,630
are detected in the first and second place

693
00:33:46,630 --> 00:33:50,310
as the top two anomalies
in these data set.

694
00:33:50,310 --> 00:33:55,310
So individually these
events were not special,

695
00:33:55,430 --> 00:33:59,380
but because they appear when they appear,

696
00:33:59,380 --> 00:34:03,150
when that event is,
does not normally appear

697
00:34:03,150 --> 00:34:08,150
in that moment in time in
the 100 server data set

698
00:34:09,380 --> 00:34:12,844
that is detected as extremely anomalous,

699
00:34:12,844 --> 00:34:15,540
as anomalous as the most anomalous event

700
00:34:15,540 --> 00:34:17,980
in that month, okay, of data.

701
00:34:17,980 --> 00:34:22,463
So as you can see, it's a
pretty, interesting results,

702
00:34:22,463 --> 00:34:27,463
(indistinct) being able to
detect the SolarWinds attack

703
00:34:27,719 --> 00:34:32,719
by a standard threat hunting
process using a schema, okay?

704
00:34:32,750 --> 00:34:37,020
So hope you are excited,
so excited as I am,

705
00:34:37,020 --> 00:34:39,540
in the end I want to summarize saying

706
00:34:39,540 --> 00:34:41,870
that everything is open source, okay.

707
00:34:41,870 --> 00:34:46,002
So you will find these on
the DS Forensics website,

708
00:34:47,550 --> 00:34:50,030
the DS Forensics library will
help you accomplish all this,

709
00:34:50,030 --> 00:34:52,159
and all the other programs
I've been mentioning,

710
00:34:52,159 --> 00:34:53,699
this is real-world data.

711
00:34:53,699 --> 00:34:55,529
We didn't make anything up, we run.

712
00:34:55,530 --> 00:34:57,640
The only thing we did
was do the injections

713
00:34:57,640 --> 00:34:58,930
in the real data.

714
00:34:58,930 --> 00:35:02,660
And well, we are gonna be
continuing investigating

715
00:35:02,660 --> 00:35:05,673
to make this more and
more actionable, okay?

716
00:35:06,960 --> 00:35:10,410
Closing remarks, artificial
intelligence is not magic,

717
00:35:10,410 --> 00:35:11,330
but as you've seen,

718
00:35:11,330 --> 00:35:14,130
it can be an extremely
powerful weapon for DFIR.

719
00:35:14,130 --> 00:35:18,150
And closer remarks, as you
know, how do you apply this?

720
00:35:18,150 --> 00:35:20,050
When you go back to the office.

721
00:35:20,050 --> 00:35:23,130
You can download the
DAISY Virtual Machine,

722
00:35:23,130 --> 00:35:25,330
which is gonna be ready for analysis.

723
00:35:25,330 --> 00:35:27,490
The more memory you can give it to it,

724
00:35:27,490 --> 00:35:30,310
the bigger data sets you
will be able to process.

725
00:35:30,310 --> 00:35:33,520
We've been playing with
memories of 256 gigs,

726
00:35:33,520 --> 00:35:34,980
one terabyte of memory.

727
00:35:34,980 --> 00:35:36,610
But if you have 16 gigs of memory,

728
00:35:36,610 --> 00:35:40,411
you will be still be able to
play with it or eight gigs

729
00:35:40,411 --> 00:35:41,890
only with smaller datasets.

730
00:35:41,890 --> 00:35:45,330
Go to the DS Forensics
website, to get familiar

731
00:35:45,330 --> 00:35:47,440
with how all these technologies

732
00:35:47,440 --> 00:35:50,030
and everything that I
have presented works.

733
00:35:50,030 --> 00:35:54,630
Define what MITRE ATT&CK
techniques you want to hunt for,

734
00:35:54,630 --> 00:35:58,070
identify the artifacts,
collect them, process them,

735
00:35:58,070 --> 00:36:00,380
feed them to the machine learning model

736
00:36:00,380 --> 00:36:02,500
and analyze the anomalies.

737
00:36:02,500 --> 00:36:06,330
Review, pivot, repeat, okay.

738
00:36:06,330 --> 00:36:11,330
So this is all I hope this
can be useful for you.

739
00:36:12,600 --> 00:36:16,060
And if I have created,
we have created a webpage

740
00:36:16,060 --> 00:36:20,779
under DS Forensics dot io slash rsac 21

741
00:36:20,780 --> 00:36:25,780
to well collect all the
information in this conference.

742
00:36:26,300 --> 00:36:27,330
In general, you can go to

743
00:36:27,330 --> 00:36:29,730
the data science forensics dot io website

744
00:36:29,730 --> 00:36:32,310
for all the information about these topics

745
00:36:32,310 --> 00:36:33,482
or to One eSecurity, if you need

746
00:36:33,482 --> 00:36:35,490
professional help on this area.

747
00:36:35,490 --> 00:36:36,323
Okay?

748
00:36:36,323 --> 00:36:41,323
Thank you very much and
as you know I'm gonna be,

749
00:36:41,930 --> 00:36:46,230
I'm around for questions,
this has been going on long,

750
00:36:46,230 --> 00:36:48,890
but and I hope you've been able to

751
00:36:48,890 --> 00:36:50,850
send all your questions already,

752
00:36:50,850 --> 00:36:53,110
but if you have more I'm here, okay.

753
00:36:53,110 --> 00:36:56,063
Thank you very much and see you around.

