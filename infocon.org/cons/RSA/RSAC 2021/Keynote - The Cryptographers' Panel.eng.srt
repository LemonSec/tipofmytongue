1
00:00:00,119 --> 00:00:05,119
(air whooshing)
(music)

2
00:00:21,753 --> 00:00:23,670
(air whooshing)

3
00:00:23,670 --> 00:00:25,430
- The last 12 months have been marked

4
00:00:25,430 --> 00:00:26,983
by digital experiences.

5
00:00:27,820 --> 00:00:29,400
Experiences that have been made possible

6
00:00:29,400 --> 00:00:30,703
by digital technologies.

7
00:00:31,620 --> 00:00:33,510
But that's only part of the story.

8
00:00:33,510 --> 00:00:36,269
These technologies have been
widely used and deployed

9
00:00:36,270 --> 00:00:38,620
because people perceive
them to be trustworthy.

10
00:00:39,560 --> 00:00:42,040
Today, I'm joined by
four amazing panelists

11
00:00:42,040 --> 00:00:45,710
whose work has paved the way
for designing and building

12
00:00:45,710 --> 00:00:48,563
trustworthy, secure,
and resilient systems.

13
00:00:49,410 --> 00:00:51,712
Ron Rivest, the R in RSA.

14
00:00:52,760 --> 00:00:55,513
Adi Shamir, the S in RSA.

15
00:00:56,720 --> 00:00:59,863
Carmela Troncoso and Ross Anderson.

16
00:01:01,780 --> 00:01:04,110
Now, as you all know,
this particular panel

17
00:01:04,110 --> 00:01:08,340
has been a mainstay of the
RSA Conference for many years.

18
00:01:08,340 --> 00:01:09,800
Now, unfortunately, in the last few years,

19
00:01:09,800 --> 00:01:12,149
we've seen the term
crypto get a bit usurped

20
00:01:12,150 --> 00:01:13,850
by a different community of people,

21
00:01:13,850 --> 00:01:15,827
who may expect us to talk
about things like blockchain

22
00:01:15,827 --> 00:01:18,899
and Bitcoin, and I hate to
disappoint those people,

23
00:01:18,900 --> 00:01:21,340
so I thought maybe the
first question I could ask

24
00:01:21,340 --> 00:01:24,290
is about a topic that's
come up very recently,

25
00:01:24,290 --> 00:01:26,503
the non-fungible token, or the NFT.

26
00:01:27,430 --> 00:01:29,090
Maybe starting with you, Ron.

27
00:01:29,090 --> 00:01:30,330
Do you have a perspective on NFTs?

28
00:01:30,330 --> 00:01:33,380
Is there any reality or any
substance behind the hype,

29
00:01:33,380 --> 00:01:34,679
and what are they exactly?

30
00:01:36,260 --> 00:01:37,230
- Zully, it's a great question,

31
00:01:37,230 --> 00:01:39,300
and I'm actually glad
you gave me a heads up

32
00:01:39,300 --> 00:01:40,289
that this would be one of the topics

33
00:01:40,290 --> 00:01:41,280
we might talk about today,

34
00:01:41,280 --> 00:01:44,450
because a year ago, I
had no idea what NFT was.

35
00:01:44,450 --> 00:01:47,907
But I had been paying
attention to the Bitcoin craze

36
00:01:47,907 --> 00:01:49,890
and things happening there,

37
00:01:49,890 --> 00:01:51,870
and this is an outgrowth of that.

38
00:01:51,870 --> 00:01:54,490
So the Bitcoin people, as we all know,

39
00:01:54,490 --> 00:01:57,250
have figured out how to
manage a public ledger,

40
00:01:57,250 --> 00:02:00,533
figured out how to have
tokens on the ledger

41
00:02:00,533 --> 00:02:03,180
and figure out who owns which token,

42
00:02:03,180 --> 00:02:05,983
and NFTs are an extension of that,

43
00:02:07,250 --> 00:02:09,870
putting a little bit of extra
information with the token.

44
00:02:09,870 --> 00:02:12,650
So thinking about it,

45
00:02:12,650 --> 00:02:14,540
I tend to go at these things
skeptically, as you'll see,

46
00:02:14,540 --> 00:02:16,790
but thinking about it, I came
up with the following analogy,

47
00:02:16,790 --> 00:02:18,519
and maybe we'll see if it works with you.

48
00:02:18,520 --> 00:02:21,440
So the analogy, remember
the tulip craze, right?

49
00:02:21,440 --> 00:02:22,400
Well, let's start with tulips.

50
00:02:22,400 --> 00:02:24,500
So tulips are a physical object,

51
00:02:24,500 --> 00:02:25,950
and you can own them,
you can possess them,

52
00:02:25,950 --> 00:02:27,810
you can plant them, you can enjoy them.

53
00:02:27,810 --> 00:02:29,940
And then you can have
a picture of a tulip,

54
00:02:29,940 --> 00:02:33,130
so that's level two, it's like
inception, with many layers.

55
00:02:33,130 --> 00:02:34,609
So then we can have a picture of a tulip,

56
00:02:34,610 --> 00:02:37,497
so this is the next layer,
and you're gonna enjoy that,

57
00:02:37,497 --> 00:02:38,640
but you can pass it around,

58
00:02:38,640 --> 00:02:43,106
but anybody can copy
it and enjoy the copy.

59
00:02:43,106 --> 00:02:46,019
But then we have the third
level, which is the NFT,

60
00:02:46,020 --> 00:02:49,960
which is sort of a token which
points at the picture, right?

61
00:02:49,960 --> 00:02:52,790
So we're two levels removed from reality.

62
00:02:52,790 --> 00:02:54,970
And you can say, what's left?

63
00:02:54,970 --> 00:02:56,500
And I like to think of it a bit like,

64
00:02:56,500 --> 00:02:58,210
and as you can see, I'm a skeptic,

65
00:02:58,210 --> 00:03:00,150
it's a bit like homeopathic medicine.

66
00:03:00,150 --> 00:03:02,110
You dilute it, you
dilute it, you dilute it.

67
00:03:02,110 --> 00:03:02,943
You say, what's left?

68
00:03:02,943 --> 00:03:04,070
We start off with a tulip,

69
00:03:04,070 --> 00:03:05,357
then we have the picture of the tulip,

70
00:03:05,357 --> 00:03:07,870
and then we have the NFT for
the picture of the tulip.

71
00:03:07,870 --> 00:03:08,850
So what's left?

72
00:03:08,850 --> 00:03:11,280
That's the, beauty is
in the eye of beholder,

73
00:03:11,280 --> 00:03:12,800
and I tend to be a skeptic,

74
00:03:12,800 --> 00:03:14,970
I'm probably not gonna buy any NFTs,

75
00:03:14,970 --> 00:03:17,150
but who knows, I might sell one.

76
00:03:17,150 --> 00:03:21,110
And there's lots of interesting
market opportunities

77
00:03:21,110 --> 00:03:22,490
here as well, so we'll see where it goes,

78
00:03:22,490 --> 00:03:27,490
but it's a very interesting
marketplace to see what happens.

79
00:03:31,160 --> 00:03:34,079
- I'm actually a much
more positive than Ron.

80
00:03:34,080 --> 00:03:37,210
I think it's a nice
way for digital artists

81
00:03:37,210 --> 00:03:39,473
to monetize their creations.

82
00:03:41,170 --> 00:03:43,440
I think that we should all look at it

83
00:03:43,440 --> 00:03:45,750
like a game of Monopoly.

84
00:03:45,750 --> 00:03:48,620
So a group of people decide to join forces

85
00:03:48,620 --> 00:03:49,830
and play the game.

86
00:03:49,830 --> 00:03:52,100
And in that game, some people claim

87
00:03:52,100 --> 00:03:54,320
that they own the White House.

88
00:03:54,320 --> 00:03:56,260
In the real world it doesn't give them

89
00:03:56,260 --> 00:03:59,720
the right to evict Donald
Trump or Joe Biden,

90
00:03:59,720 --> 00:04:04,720
but they can play the game as
if they own the White House.

91
00:04:05,740 --> 00:04:10,740
So I think that it makes
sense in certain situations.

92
00:04:12,010 --> 00:04:13,810
Certainly, it's not harmful.

93
00:04:13,810 --> 00:04:17,410
Some people collect coins,
some people collect stamps,

94
00:04:17,410 --> 00:04:19,290
some people will collect NFTs.

95
00:04:19,290 --> 00:04:22,930
If they want to pay money
for this, it's fine with me.

96
00:04:22,930 --> 00:04:27,930
So I thought about trying
to do a little experiment

97
00:04:28,940 --> 00:04:31,730
to see if there is any substance in this,

98
00:04:31,730 --> 00:04:34,813
so I tried to draw a multicolored cat,

99
00:04:34,813 --> 00:04:37,099
but I'm a very bad artist.

100
00:04:37,100 --> 00:04:40,540
So I first looked at other
kinds of art (indistinct),

101
00:04:42,939 --> 00:04:44,490
and actually there is something

102
00:04:44,490 --> 00:04:49,057
which is a very artistic object.

103
00:04:49,057 --> 00:04:51,820
For those of you who don't recognize this,

104
00:04:51,820 --> 00:04:53,960
this is an original copy

105
00:04:53,960 --> 00:04:58,900
of the MIT Technical Report from 1977,

106
00:04:58,900 --> 00:05:03,900
which even has our handwritten
signature on the first page.

107
00:05:04,820 --> 00:05:09,820
And I think that we may
actually try to do something

108
00:05:09,820 --> 00:05:14,820
with this, checking whether
NFTs are worth anything or not.

109
00:05:16,130 --> 00:05:18,460
- That is wonderful. Oh my
God, that's so exciting.

110
00:05:18,460 --> 00:05:21,359
So I think what we'll do is
we'll work with you, Adi,

111
00:05:21,360 --> 00:05:24,330
to develop an NFT of the first page

112
00:05:24,330 --> 00:05:25,990
of that MIT Technical Report

113
00:05:25,990 --> 00:05:27,080
with all of your signatures on it,

114
00:05:27,080 --> 00:05:28,870
including not just you and Ron,

115
00:05:28,870 --> 00:05:31,010
but also Len Adleman,
who's not here today.

116
00:05:31,010 --> 00:05:33,849
And we're gonna provide that to a charity

117
00:05:33,850 --> 00:05:36,730
and have the proceeds be
donated to a well-known charity.

118
00:05:36,730 --> 00:05:38,860
We're gonna work on that
and I'll plan to announce

119
00:05:38,860 --> 00:05:40,990
the details on the conference website

120
00:05:40,990 --> 00:05:42,570
or through the conference
communications channels

121
00:05:42,570 --> 00:05:43,830
when they become available.

122
00:05:43,830 --> 00:05:45,486
So super exciting, thank
you for doing that, Adi.

123
00:05:45,487 --> 00:05:48,170
And I think that we took
a popular news topic

124
00:05:48,170 --> 00:05:50,210
and I think showed some wonderful insights

125
00:05:50,210 --> 00:05:52,909
and showed a really interesting
artistic bend to it.

126
00:05:52,910 --> 00:05:54,630
So I wanna switch gears
to another news topic

127
00:05:54,630 --> 00:05:57,180
which has come up recently back in March,

128
00:05:57,180 --> 00:05:58,570
and we are just in April right now,

129
00:05:58,570 --> 00:06:00,733
but it feels like so long ago.

130
00:06:00,733 --> 00:06:03,520
Claus Schnorr, a well-known cryptographer

131
00:06:03,520 --> 00:06:06,560
proposed a new algorithm
for integer factorization

132
00:06:07,640 --> 00:06:09,210
and he proposed a new
efficient way to do it.

133
00:06:09,210 --> 00:06:12,349
And in his paper, he claims
that his new approach will,

134
00:06:12,350 --> 00:06:15,550
and I quote, "destroy
the RSA cryptosystem".

135
00:06:15,550 --> 00:06:17,380
Now, of course, we have two people here

136
00:06:17,380 --> 00:06:18,760
who I'm sure have a very vested interest

137
00:06:18,760 --> 00:06:21,130
in seeing the outcome of this research.

138
00:06:21,130 --> 00:06:22,180
So maybe starting with you, Ron,

139
00:06:22,180 --> 00:06:24,300
and any thoughts on this
new Claus Schnorr result,

140
00:06:24,300 --> 00:06:26,323
and what is this all about?

141
00:06:27,370 --> 00:06:29,980
- This is certainly a
result that caught my eye

142
00:06:29,980 --> 00:06:32,487
when it happened so a
number of people said,

143
00:06:32,487 --> 00:06:34,037
"Hey, Ron, did you see this?"

144
00:06:34,037 --> 00:06:35,799
And they pointed to the sentence you read,

145
00:06:35,800 --> 00:06:38,140
this destroys the RSA cryptosystem.

146
00:06:38,140 --> 00:06:40,510
So it caught my eye and I said,

147
00:06:40,510 --> 00:06:44,481
well, this is material
which looks pretty technical

148
00:06:44,481 --> 00:06:48,030
and I'm not sure I'm the
best person to evaluate it.

149
00:06:48,030 --> 00:06:49,840
So I started asking around,

150
00:06:49,840 --> 00:06:51,299
and of course, I asked the author,

151
00:06:51,300 --> 00:06:54,697
first of all, Claus Schnorr,
"Claus what's going on here,

152
00:06:54,697 --> 00:06:58,007
"do you really believe this
destroys the RSA cryptosystem?

153
00:06:58,007 --> 00:07:00,733
"Do you have any
demonstrated factorizations?

154
00:07:02,047 --> 00:07:03,870
"How are the criticisms going?"

155
00:07:03,870 --> 00:07:07,050
And I pointed to him at a
couple of criticisms on the web

156
00:07:07,050 --> 00:07:10,600
that he hadn't seen, and
so he looked at those

157
00:07:10,600 --> 00:07:12,420
and he had seen some others,

158
00:07:12,420 --> 00:07:15,020
and he posted as recently as four days ago

159
00:07:15,020 --> 00:07:17,030
an updated version of his paper.

160
00:07:17,030 --> 00:07:20,390
So I think the dust still
hasn't settled on this yet.

161
00:07:20,390 --> 00:07:21,990
As everything, I tend to be skeptical

162
00:07:21,990 --> 00:07:23,420
until the proof is in the pudding,

163
00:07:23,420 --> 00:07:26,050
and with factoring, I wanna
see numbers get factored.

164
00:07:26,050 --> 00:07:27,660
So we'll see how it goes,

165
00:07:27,660 --> 00:07:30,390
but I think there's some work to be done

166
00:07:30,390 --> 00:07:33,623
to evaluate these ideas and
see if they really hold water.

167
00:07:34,560 --> 00:07:37,210
I don't think the jury's
come up with a decision yet.

168
00:07:38,550 --> 00:07:41,443
- Maybe I can add my two cents.

169
00:07:42,460 --> 00:07:45,609
I, of course, looked
carefully at the paper,

170
00:07:45,610 --> 00:07:48,440
as did many other researchers,

171
00:07:48,440 --> 00:07:51,840
and some of them even
tried to implement it.

172
00:07:51,840 --> 00:07:55,530
And in particular, Leo Ducas from France

173
00:07:55,530 --> 00:08:00,530
had tried to implement
eight on the 400 beat,

174
00:08:02,060 --> 00:08:03,650
how you'd say, number,

175
00:08:03,650 --> 00:08:07,359
and he claims that the
number of factoring relations

176
00:08:07,360 --> 00:08:10,610
you get from the shortest
vector or problem

177
00:08:10,610 --> 00:08:13,980
is way lower than
anticipated in the paper.

178
00:08:13,980 --> 00:08:18,890
And these are not sufficient
in order to factorize,

179
00:08:18,890 --> 00:08:21,719
unless you go up to totally unreasonable

180
00:08:21,720 --> 00:08:23,200
number of dimensions.

181
00:08:23,200 --> 00:08:28,200
So at the moment, it looks
as if it doesn't factorize

182
00:08:29,700 --> 00:08:33,429
even numbers which are much
smaller than today's keys,

183
00:08:33,429 --> 00:08:35,972
but we should always keep our mind open.

184
00:08:37,809 --> 00:08:39,650
- Maybe I could ask a follow-on question,

185
00:08:39,650 --> 00:08:41,840
this interesting question of Claus

186
00:08:41,840 --> 00:08:44,030
sort of announcing this
potentially really big result.

187
00:08:44,030 --> 00:08:46,579
And we all know the implications
of integer factorization.

188
00:08:46,580 --> 00:08:48,410
And of course, a break
in the RSA algorithm

189
00:08:48,410 --> 00:08:51,160
are wide and far reaching.

190
00:08:51,160 --> 00:08:52,560
Do you think there's an appropriate way

191
00:08:52,560 --> 00:08:54,880
to deal with responsible disclosure

192
00:08:54,880 --> 00:08:56,040
of these types of results?

193
00:08:56,040 --> 00:08:57,240
And how would you think about that.

194
00:08:57,240 --> 00:08:58,560
And this is maybe a topic to you, Adi,

195
00:08:58,560 --> 00:09:00,560
but of course, open to the entire panel.

196
00:09:01,400 --> 00:09:04,600
- So there is a well-established approach

197
00:09:04,600 --> 00:09:09,470
of ethical hacking and
responsible disclosure,

198
00:09:09,470 --> 00:09:14,260
according to which you are
supposed to find the company

199
00:09:14,260 --> 00:09:17,319
or a standards organization

200
00:09:17,320 --> 00:09:20,270
in charge of a particular crypto system,

201
00:09:20,270 --> 00:09:23,040
let them know in advance
about the problem,

202
00:09:23,040 --> 00:09:26,060
give them enough time to fix it.

203
00:09:26,060 --> 00:09:29,170
I've done it in the past
many times, it usually works.

204
00:09:29,170 --> 00:09:31,209
Occasionally, you get a letter

205
00:09:31,210 --> 00:09:34,380
from a lawyer threatening a lawsuit.

206
00:09:34,380 --> 00:09:35,620
One thing I can promise

207
00:09:35,620 --> 00:09:39,260
is that if Claus Schnorr's
approach will succeed,

208
00:09:39,260 --> 00:09:41,090
I'll be the first to applaud

209
00:09:41,090 --> 00:09:46,090
and I'm going to not sue Claus
Schnorr in any way or form.

210
00:09:48,810 --> 00:09:51,953
- So I think one complicated
thing, if I may jump in here,

211
00:09:51,953 --> 00:09:55,134
is that when we break
something as big as RSA,

212
00:09:55,134 --> 00:09:56,990
that is not a one company deploying it,

213
00:09:56,990 --> 00:09:59,350
like everybody's deploying servers.

214
00:09:59,350 --> 00:10:02,520
So it is very hard to do
this responsible disclosure.

215
00:10:02,520 --> 00:10:05,050
And then comes the
question, what should we do?

216
00:10:05,050 --> 00:10:06,839
Make it public as soon as possible

217
00:10:06,840 --> 00:10:08,090
so that it gets fixed,

218
00:10:08,090 --> 00:10:11,330
or keep it secret so that
nobody can exploit it?

219
00:10:11,330 --> 00:10:13,980
And this is becoming a
very complicated thing,

220
00:10:13,980 --> 00:10:17,860
especially in corona times,
I have to say that we have

221
00:10:17,860 --> 00:10:21,500
found a couple of bugs on
other people corona apps,

222
00:10:21,500 --> 00:10:22,853
and then it was very hard to decide

223
00:10:22,853 --> 00:10:25,699
whether to go public in a product

224
00:10:25,700 --> 00:10:26,840
that is being used by millions

225
00:10:26,840 --> 00:10:29,480
and it is very important nowadays,

226
00:10:29,480 --> 00:10:32,120
and then we need to save
those people very fast,

227
00:10:32,120 --> 00:10:34,630
or try to talk with the company.

228
00:10:34,630 --> 00:10:38,030
Especially if sometimes,
it is actually not fixable

229
00:10:38,030 --> 00:10:43,030
and it is not sure that we
can unplug the plug very fast.

230
00:10:43,710 --> 00:10:45,863
I don't know if anyone
had thoughts on this.

231
00:10:47,060 --> 00:10:49,920
- Well, when we started
doing work on the economics

232
00:10:49,920 --> 00:10:53,360
of information security 20 years ago,

233
00:10:53,360 --> 00:10:55,510
one of the first big problems that came up

234
00:10:55,510 --> 00:10:57,360
was responsible disclosure.

235
00:10:57,360 --> 00:10:59,560
Back in those days, people were split

236
00:10:59,560 --> 00:11:00,890
between the bug track guys

237
00:11:00,890 --> 00:11:02,740
who want to disclose everything at once

238
00:11:02,740 --> 00:11:04,270
and the company lawyers

239
00:11:04,270 --> 00:11:06,600
who wanted everything kept quiet forever.

240
00:11:06,600 --> 00:11:08,850
And the current responsible
disclosure regime

241
00:11:10,061 --> 00:11:11,570
has come out of that.

242
00:11:11,570 --> 00:11:14,260
There are problems though
if you break something big,

243
00:11:14,260 --> 00:11:16,990
if you break Linux, if you
break a random number generator

244
00:11:16,990 --> 00:11:18,540
that everyone is using for keys,

245
00:11:18,540 --> 00:11:23,540
or if you break an add on to
TLS, then all of a sudden,

246
00:11:23,660 --> 00:11:26,930
lots of people are just gonna
have to scramble to fix stuff.

247
00:11:26,930 --> 00:11:29,270
And if we had a catastrophic failure

248
00:11:29,270 --> 00:11:31,140
of public key cryptography,

249
00:11:31,140 --> 00:11:33,010
well, it wouldn't be the end of the world,

250
00:11:33,010 --> 00:11:35,439
since a lot of the things that we rely on

251
00:11:35,440 --> 00:11:38,240
depends on share key
cryptography, such as EMV,

252
00:11:38,240 --> 00:11:40,683
obviously to some extent,

253
00:11:41,560 --> 00:11:43,449
but it would be an interesting year or two

254
00:11:43,450 --> 00:11:44,753
while everybody upgraded.

255
00:11:46,290 --> 00:11:47,699
- Absolutely, so it looks like the jury

256
00:11:47,700 --> 00:11:50,420
still maybe a bit out on
hopefully the Claus Schnorr result

257
00:11:50,420 --> 00:11:52,870
and hopefully will not have
the level of resilience

258
00:11:52,870 --> 00:11:55,220
given the implications
of a result like that.

259
00:11:55,220 --> 00:11:56,603
Although, I think I would agree with Adi,

260
00:11:56,603 --> 00:11:57,510
I would applaud the effort.

261
00:11:57,510 --> 00:11:59,850
I think it would be a phenomenal
mathematical breakthrough,

262
00:11:59,850 --> 00:12:02,010
but maybe not one that would be

263
00:12:02,010 --> 00:12:04,470
easy for us to deal with in the aftermath.

264
00:12:04,470 --> 00:12:06,328
Another avenue that I think has been--.

265
00:12:06,328 --> 00:12:09,333
- Perhaps, I could interject there, Zully?

266
00:12:09,333 --> 00:12:10,165
- Absolutely.

267
00:12:10,166 --> 00:12:11,580
- I think we haven't made the point yet

268
00:12:11,580 --> 00:12:14,140
that factoring has a unique, not unique,

269
00:12:14,140 --> 00:12:17,000
but a very important property
that you can demonstrate

270
00:12:17,000 --> 00:12:19,400
that you can factor without revealing how,

271
00:12:19,400 --> 00:12:21,760
so you can factor some
of the challenge numbers

272
00:12:21,760 --> 00:12:23,930
and give people notice of a year and say,

273
00:12:23,930 --> 00:12:25,660
or two years, or even in the case of RSA,

274
00:12:25,660 --> 00:12:27,530
or whatever it takes, and say,

275
00:12:27,530 --> 00:12:30,170
I can factor big numbers,
I'm not gonna tell you how,

276
00:12:30,170 --> 00:12:33,099
the method will be revealed
in a couple of years.

277
00:12:33,100 --> 00:12:35,743
So be prepared, change your systems now.

278
00:12:36,600 --> 00:12:40,240
That's maybe workable in the case of RSA.

279
00:12:40,240 --> 00:12:41,690
The bug in that reasoning

280
00:12:41,690 --> 00:12:43,530
is that a lot of the factoring algorithms

281
00:12:43,530 --> 00:12:46,140
are in fact distributed require
people all over the planet

282
00:12:46,140 --> 00:12:48,420
working with their computers to do this,

283
00:12:48,420 --> 00:12:50,823
so the method would be
widely known perhaps.

284
00:12:52,190 --> 00:12:55,440
- Yeah, it's almost like a proof of life.

285
00:12:55,440 --> 00:12:59,040
- Of course, somebody who could
work out elliptic curve logs

286
00:12:59,040 --> 00:13:00,560
could simply steal all Satoshi's Bitcoins,

287
00:13:00,560 --> 00:13:03,496
and that would be a different
way of announcing the break.

288
00:13:03,496 --> 00:13:05,872
(Ron laughs)

289
00:13:05,873 --> 00:13:07,253
- And actually, speaking of Bitcoin,

290
00:13:07,253 --> 00:13:08,810
I think the other way people

291
00:13:08,810 --> 00:13:11,030
have talked about potentially
compromising Bitcoin

292
00:13:11,030 --> 00:13:14,270
is if you can build, let's say,
a quantum computer at scale

293
00:13:14,270 --> 00:13:15,240
and use that, to me that would be

294
00:13:15,240 --> 00:13:17,310
the first killer application
of quantum computing

295
00:13:17,310 --> 00:13:19,219
in terms of actual cryptography.

296
00:13:19,220 --> 00:13:20,610
Now, this is where I think we've also seen

297
00:13:20,610 --> 00:13:22,790
some interesting research
that's been published

298
00:13:22,790 --> 00:13:25,430
and some of which may
also not have held up

299
00:13:25,430 --> 00:13:27,130
in terms of scientific scrutiny.

300
00:13:27,130 --> 00:13:28,920
I know, Adi, you've been
looking at a bit this area

301
00:13:28,920 --> 00:13:30,209
in some of the research, can you talk

302
00:13:30,210 --> 00:13:31,977
about some of the recent
results of this area,

303
00:13:31,977 --> 00:13:34,090
and some of the issues that have come up

304
00:13:34,090 --> 00:13:36,190
when they'd been looked at in more detail?

305
00:13:37,430 --> 00:13:41,569
- So this year, focus in quantum computing

306
00:13:41,570 --> 00:13:45,493
had been two steps ahead, one step back.

307
00:13:46,380 --> 00:13:51,363
Because for example,
just a short time ago,

308
00:13:52,680 --> 00:13:57,680
a paper which claimed the
discovery experimental evidence

309
00:13:57,780 --> 00:14:02,780
for the Marjorana Fermion which
is their fundamental object

310
00:14:05,040 --> 00:14:07,670
that Microsoft is trying to use

311
00:14:07,670 --> 00:14:12,439
as its topological qubit
had been retracted,

312
00:14:12,440 --> 00:14:14,520
the paper had been retracted.

313
00:14:14,520 --> 00:14:19,520
And at the moment, it's not
clear at all if Majoranas exist

314
00:14:19,620 --> 00:14:22,590
and whether Microsoft
will be able to proceed

315
00:14:22,590 --> 00:14:25,540
in the way that they've
pursued over the last 10 years.

316
00:14:25,540 --> 00:14:29,550
And I still remember
how I attended, in 2018,

317
00:14:31,250 --> 00:14:34,300
a talk in which a vice
president of Microsoft

318
00:14:34,300 --> 00:14:38,796
was quoted very confidentially
that by the end of 2018

319
00:14:39,890 --> 00:14:44,890
there is going to be a
Majorana Fermion discovered.

320
00:14:45,243 --> 00:14:47,943
We are several years later
and it still didn't happen.

321
00:14:49,540 --> 00:14:54,540
IBM, for example, had attacked
the previous claim by Google

322
00:14:59,060 --> 00:15:01,020
about quantum supremacy.

323
00:15:01,020 --> 00:15:05,793
They said that using their supercomputer,

324
00:15:07,623 --> 00:15:09,700
one of the strongest in the world,

325
00:15:09,700 --> 00:15:14,700
they were able to achieve
to produce same distribution

326
00:15:15,540 --> 00:15:19,010
as the one produced by
Google's quantum computer.

327
00:15:19,010 --> 00:15:22,640
But very recently, there
had been another claim

328
00:15:23,560 --> 00:15:28,560
saying that the same
distribution which Google claimed

329
00:15:29,140 --> 00:15:33,050
shows the supremacy of quantum
computers could be achieved,

330
00:15:33,050 --> 00:15:37,390
it could be simple using only 60 GPUs

331
00:15:37,390 --> 00:15:39,290
over a short period of time.

332
00:15:39,290 --> 00:15:42,737
So this is the going back
that I mentioned before.

333
00:15:42,737 --> 00:15:45,250
There are very strong claims being made

334
00:15:45,250 --> 00:15:50,240
and then they are backpedaled or diluted.

335
00:15:51,220 --> 00:15:53,780
So IBM, by the way, had announced

336
00:15:53,780 --> 00:15:56,439
a very, very ambitious roadmap,

337
00:15:56,440 --> 00:16:01,220
in which by the year 2023, in two years,

338
00:16:01,220 --> 00:16:03,310
they say that they will have

339
00:16:03,310 --> 00:16:08,000
more than 1,000 high
quality qubits available

340
00:16:08,000 --> 00:16:10,020
in one of their machines.

341
00:16:10,020 --> 00:16:11,420
Let's wait and see.

342
00:16:11,420 --> 00:16:13,439
- Yes, it's astonishing
to me how much energy

343
00:16:13,440 --> 00:16:15,440
is going into the commercialization

344
00:16:15,440 --> 00:16:18,070
of technology that doesn't yet exist.

345
00:16:18,070 --> 00:16:20,630
There are so many startups happening,

346
00:16:20,630 --> 00:16:23,370
the amount of money being
invested in this technology,

347
00:16:23,370 --> 00:16:27,140
this nascent technology is incredible.

348
00:16:27,140 --> 00:16:31,490
And one wonders if there's
really gonna be substance there.

349
00:16:31,490 --> 00:16:33,390
I think the two major questions are,

350
00:16:33,390 --> 00:16:36,240
can you build a quantum computer at scale

351
00:16:36,240 --> 00:16:39,140
that will last long enough
to do a useful computation?

352
00:16:39,140 --> 00:16:40,260
That's number one.

353
00:16:40,260 --> 00:16:42,730
And number two is, are
there useful applications

354
00:16:42,730 --> 00:16:45,240
for this technology, even
if you could build it?

355
00:16:45,240 --> 00:16:48,980
And I think the answer so
far are not clear and maybe.

356
00:16:48,980 --> 00:16:50,063
So we'll see.

357
00:16:51,300 --> 00:16:55,849
- Well, my own personal view on this

358
00:16:55,850 --> 00:16:58,090
as someone who works
with quantum mechanics,

359
00:16:58,090 --> 00:17:02,070
we use lasers and iron beams
and so on and clamp resistance,

360
00:17:02,070 --> 00:17:04,210
is that I've observed that the physicists

361
00:17:04,210 --> 00:17:07,040
are basically copying Ron and Adi,

362
00:17:07,040 --> 00:17:09,329
because they observed
how the RSA discovery

363
00:17:09,329 --> 00:17:11,960
all those years ago
enabled number of theorists

364
00:17:11,960 --> 00:17:14,670
to get their shovels
into the military budget.

365
00:17:14,670 --> 00:17:16,910
And they wanted to do the
same for quantum mechanics,

366
00:17:16,910 --> 00:17:19,119
and in particular, for quantum optics.

367
00:17:19,119 --> 00:17:22,550
Now, this has actually brought
a number of very interesting

368
00:17:22,550 --> 00:17:24,639
and useful results, but they've been

369
00:17:24,640 --> 00:17:28,300
nowhere near cryptography
or quantum computing,

370
00:17:28,300 --> 00:17:30,610
they've been in quantum
sensing and metrology.

371
00:17:30,610 --> 00:17:34,010
We now have got very much
better accelerometers and gyros,

372
00:17:34,010 --> 00:17:35,810
instruments for archeologists,

373
00:17:35,810 --> 00:17:38,440
means of doing gravimetric
navigation in submarines,

374
00:17:38,440 --> 00:17:39,800
and so on and so forth.

375
00:17:39,800 --> 00:17:42,710
That's the real payoff that I see here.

376
00:17:42,710 --> 00:17:46,680
As far as quantum
cryptography is concerned,

377
00:17:46,680 --> 00:17:49,390
I'm entirely unimpressed
because all you can do

378
00:17:49,390 --> 00:17:50,930
is re-key a line in crypto,

379
00:17:50,930 --> 00:17:53,580
and we've known how to
do that for 40 years.

380
00:17:53,580 --> 00:17:56,320
And the proof's based
on quantum entanglement.

381
00:17:56,320 --> 00:17:58,820
Don't convince me, because
that's interpretation

382
00:17:58,820 --> 00:18:03,060
only works in certain
interpretations of quantum mechanics.

383
00:18:03,060 --> 00:18:04,710
Now, I don't want to go into a debate

384
00:18:04,710 --> 00:18:06,537
on foundations of quantum computing,

385
00:18:06,537 --> 00:18:09,060
but I personally am a skeptic.

386
00:18:09,060 --> 00:18:11,149
I'm not surprised that nobody has seen

387
00:18:11,150 --> 00:18:13,270
any real quantum speed up yet

388
00:18:13,270 --> 00:18:16,129
and I would not be surprised
if that just doesn't happen.

389
00:18:16,130 --> 00:18:17,840
Of course, it would be great if it does

390
00:18:17,840 --> 00:18:19,480
and the jury is still out,

391
00:18:19,480 --> 00:18:22,850
but there are many more
complex issues around here

392
00:18:22,850 --> 00:18:25,332
than just whether you can
build a better line in crypto.

393
00:18:28,300 --> 00:18:33,300
- Some companies try to
combine the two biggest types,

394
00:18:34,830 --> 00:18:37,030
machine learning and quantum computing,

395
00:18:37,030 --> 00:18:39,700
and saying that the quantum computers

396
00:18:39,700 --> 00:18:42,600
we are going to have
just around the corner

397
00:18:42,600 --> 00:18:46,770
are going to solve the
old framing problems

398
00:18:48,660 --> 00:18:51,040
we are having with large datasets.

399
00:18:51,040 --> 00:18:52,373
Again, I don't believe it.

400
00:18:53,960 --> 00:18:58,120
- Well, quantum computers
as they currently exist

401
00:18:58,120 --> 00:19:00,010
have to have the data set up

402
00:19:00,010 --> 00:19:02,710
like the very first
computers in the 1940s,

403
00:19:02,710 --> 00:19:05,620
using thumbwheel switches
on the front panel

404
00:19:05,620 --> 00:19:08,050
or the equivalent of that.

405
00:19:08,050 --> 00:19:12,040
And you might just about
set up a 1000-bit RSA key

406
00:19:12,040 --> 00:19:14,350
for factoring using some
thumbwheel switches,

407
00:19:14,350 --> 00:19:15,551
but there's no way you can get

408
00:19:15,551 --> 00:19:18,913
a gigabit size neural network model.

409
00:19:22,500 --> 00:19:24,310
- I imagine that's still
one of the many challenges

410
00:19:24,310 --> 00:19:26,440
that comes up when we try to build

411
00:19:26,440 --> 00:19:27,620
any type of machine learning system,

412
00:19:27,620 --> 00:19:29,739
obviously doing it on a
quantum computers is one thing,

413
00:19:29,740 --> 00:19:31,260
but I think we still haven't
figured out the basics

414
00:19:31,260 --> 00:19:34,180
of how to make machine
learning work at scale.

415
00:19:34,180 --> 00:19:35,350
I think one of the biggest challenges

416
00:19:35,350 --> 00:19:36,899
is what happens with machine learning

417
00:19:36,900 --> 00:19:38,720
when you're dealing with
adversarial environments.

418
00:19:38,720 --> 00:19:39,890
How do you deal with situations

419
00:19:39,890 --> 00:19:42,230
where the world around you
may not be trustworthy?

420
00:19:42,230 --> 00:19:44,570
How do you build resilience
into these systems?

421
00:19:44,570 --> 00:19:46,389
Now, Carmela I know that
you've done some research

422
00:19:46,390 --> 00:19:47,580
on this area, you've probably spent

423
00:19:47,580 --> 00:19:49,040
a lot of time thinking about it.

424
00:19:49,040 --> 00:19:50,730
I'd love to get your perspective.

425
00:19:50,730 --> 00:19:54,733
Where are we on being able
to build secure systems

426
00:19:54,733 --> 00:19:57,683
that work under adversarial
conditions in machine learning?

427
00:19:58,780 --> 00:20:00,303
- So we hear a lot this word

428
00:20:00,303 --> 00:20:01,517
of transformative machine learning.

429
00:20:01,517 --> 00:20:05,610
And when people talk about it,
they actually seem to mean,

430
00:20:05,610 --> 00:20:08,520
it's machine learning we
can trust to not harm us.

431
00:20:08,520 --> 00:20:10,889
And on that, we typically
build four dimensions,

432
00:20:10,890 --> 00:20:13,510
so it has to be robust, as you said,

433
00:20:13,510 --> 00:20:16,210
can protect against adversarial settings,

434
00:20:16,210 --> 00:20:18,790
has to be fair so that it doesn't damage

435
00:20:18,790 --> 00:20:20,940
the part of the siding or the other one,

436
00:20:20,940 --> 00:20:22,490
it has to be explainable

437
00:20:22,490 --> 00:20:24,540
so that we can understand what's going on,

438
00:20:24,540 --> 00:20:27,050
and it also has to be privacy preserving.

439
00:20:27,050 --> 00:20:29,129
And I'd like to talk first the fact

440
00:20:29,130 --> 00:20:30,760
that there are more and more results

441
00:20:30,760 --> 00:20:32,530
that indicate that these four dimensions,

442
00:20:32,530 --> 00:20:34,430
this may not be compatible.

443
00:20:34,430 --> 00:20:36,090
So when you put more privacy,

444
00:20:36,090 --> 00:20:38,929
maybe you'll also have less robustness,

445
00:20:38,930 --> 00:20:40,340
or would you put more robustness,

446
00:20:40,340 --> 00:20:42,760
you end up losing privacy.

447
00:20:42,760 --> 00:20:43,720
But in particularly, when we think

448
00:20:43,720 --> 00:20:45,480
about the very meaning of these things

449
00:20:45,480 --> 00:20:48,270
is trustworthiness about
it's not gonna damage us.

450
00:20:48,270 --> 00:20:50,410
And we talk a lot about
the privacy problems

451
00:20:50,410 --> 00:20:52,220
of this collection of data

452
00:20:52,220 --> 00:20:54,640
and we see a lot of companies diving

453
00:20:54,640 --> 00:20:56,770
into this privacy
preserving machine learning,

454
00:20:56,770 --> 00:20:59,220
which is also a lot, a
ton of money creating

455
00:21:00,520 --> 00:21:02,139
huge federated learning models

456
00:21:02,140 --> 00:21:04,310
that are gonna be different (indistinct).

457
00:21:04,310 --> 00:21:05,700
But the question when we thought about

458
00:21:05,700 --> 00:21:08,153
not doing damage in society,
it's not really about

459
00:21:08,153 --> 00:21:09,940
whether we collect the data or not.

460
00:21:09,940 --> 00:21:12,120
What are these companies
gonna do with the data?

461
00:21:12,120 --> 00:21:14,679
And what happens when
maybe the business model

462
00:21:14,680 --> 00:21:19,420
is actually what is not aligned
with societal interests.

463
00:21:19,420 --> 00:21:20,320
And in that sense,

464
00:21:20,320 --> 00:21:22,480
maybe it's the question
that we should making

465
00:21:22,480 --> 00:21:24,490
is not can we make the machine trustable,

466
00:21:24,490 --> 00:21:27,430
but can actually make
the ones that are using

467
00:21:27,430 --> 00:21:29,970
this machine learning
something we want to trust

468
00:21:29,970 --> 00:21:32,670
with them deciding how
the work is gonna work.

469
00:21:36,990 --> 00:21:41,170
- Well, in addition to
all the privacy issues

470
00:21:41,170 --> 00:21:44,610
and explainability issues,
there's a whole set

471
00:21:44,610 --> 00:21:47,560
of robustness issues when you
start using neural networks

472
00:21:47,560 --> 00:21:50,700
to do real work in
safety critical systems,

473
00:21:50,700 --> 00:21:53,720
because neural networks
are very highly optimized.

474
00:21:53,720 --> 00:21:57,170
And if you're an adversary, you
can try and pessimise these,

475
00:21:57,170 --> 00:21:59,720
you can use the same
gradient descent methods

476
00:21:59,720 --> 00:22:02,670
to look for inputs, which
will cause the neural network

477
00:22:02,670 --> 00:22:04,040
to take as long as possible

478
00:22:04,040 --> 00:22:06,750
or to burn as much energy as possible.

479
00:22:06,750 --> 00:22:08,743
And we found that in particular,

480
00:22:09,840 --> 00:22:11,899
natural language processing systems,

481
00:22:11,900 --> 00:22:13,960
which are getting everywhere nowadays,

482
00:22:13,960 --> 00:22:17,080
are very, very fragile
to this kind of attack.

483
00:22:17,080 --> 00:22:19,659
And we found, for example,
using these methods,

484
00:22:19,660 --> 00:22:22,930
that if you start putting
foreign characters

485
00:22:22,930 --> 00:22:24,960
and symbols into the input,

486
00:22:24,960 --> 00:22:26,920
to natural language processing systems,

487
00:22:26,920 --> 00:22:28,780
it typically sends them haywire.

488
00:22:28,780 --> 00:22:32,360
And this happens basically
to all the big systems

489
00:22:32,360 --> 00:22:34,870
that the big tech companies
have deployed at scale.

490
00:22:34,870 --> 00:22:36,850
And when do the optimization,

491
00:22:36,850 --> 00:22:40,240
we find that our system
comes up with ideas

492
00:22:40,240 --> 00:22:41,870
such as putting Chinese characters

493
00:22:41,870 --> 00:22:43,610
into Russian text and then putting it

494
00:22:43,610 --> 00:22:47,010
into a Russian to English
machine translation system.

495
00:22:47,010 --> 00:22:49,780
And this is ongoing research

496
00:22:49,780 --> 00:22:52,543
and we've discovered many,
many new wrinkles on this.

497
00:22:53,580 --> 00:22:55,879
And when we start seeing
machine learning systems

498
00:22:55,880 --> 00:22:58,860
used in conflict, for example,

499
00:22:58,860 --> 00:23:01,189
if you have Chinese drone swarms

500
00:23:01,190 --> 00:23:06,190
and Indian drone swarms battling
it out over the Himalayas,

501
00:23:06,200 --> 00:23:08,690
then you can imagine that
there will be lots and lots

502
00:23:08,690 --> 00:23:10,460
and lots of this stuff going on,

503
00:23:10,460 --> 00:23:11,740
and people haven't really started

504
00:23:11,740 --> 00:23:13,865
scratching the surface yet.

505
00:23:13,865 --> 00:23:15,140
- Maybe I can get a
perspective on the panel.

506
00:23:15,140 --> 00:23:16,980
I mean, it seems like
one of the challenges

507
00:23:16,980 --> 00:23:20,630
with machine learning is
also that a lot of people

508
00:23:20,630 --> 00:23:22,930
don't understand how these
systems work at all, right,

509
00:23:22,930 --> 00:23:24,750
and you're using them in different places.

510
00:23:24,750 --> 00:23:27,130
Is there a concern,
maybe, Adi, I can ask you

511
00:23:27,130 --> 00:23:29,280
about what happens if
you take a technology

512
00:23:29,280 --> 00:23:30,340
that people don't understand

513
00:23:30,340 --> 00:23:32,300
and try to make it work at scale?

514
00:23:32,300 --> 00:23:33,133
Are there particular issues

515
00:23:33,133 --> 00:23:34,930
you're worried about in that regard?

516
00:23:36,370 --> 00:23:39,082
- Well, it's also mentioned before,

517
00:23:40,100 --> 00:23:45,100
the issue of adversarial
examples is happening everywhere.

518
00:23:46,410 --> 00:23:49,950
Machine learning, at the moment,

519
00:23:49,950 --> 00:23:51,303
they're totally untrustworthy.

520
00:23:52,260 --> 00:23:56,250
We don't have, at the
moment, a good understanding

521
00:23:56,250 --> 00:23:57,920
where the adversarial
examples are coming from

522
00:23:57,920 --> 00:24:00,730
and what do they represent, et cetera.

523
00:24:00,730 --> 00:24:03,300
Some focus is being
made along these lines,

524
00:24:03,300 --> 00:24:07,990
but I think that until we
solve the robustness issue,

525
00:24:07,990 --> 00:24:10,680
I'll be very worried about deploying

526
00:24:11,640 --> 00:24:16,640
any kind of big machine learning system

527
00:24:17,160 --> 00:24:20,290
that no one understands and no one knows

528
00:24:20,290 --> 00:24:22,210
in which ways it can fail.

529
00:24:22,210 --> 00:24:25,490
- If I can interject, at a high level,

530
00:24:25,490 --> 00:24:26,960
we have the maxim in security

531
00:24:26,960 --> 00:24:29,830
that complexity is the enemy of security.

532
00:24:29,830 --> 00:24:31,540
So the more complicated you make a system,

533
00:24:31,540 --> 00:24:32,879
the more vulnerable it can become

534
00:24:32,880 --> 00:24:35,410
to all kinds of faults and penetrations,

535
00:24:35,410 --> 00:24:37,910
and machine learning is
nothing but complicated.

536
00:24:37,910 --> 00:24:39,900
I mean, it's very, very complicated

537
00:24:39,900 --> 00:24:41,750
with millions of parameters.

538
00:24:41,750 --> 00:24:44,380
So it violates one of the
basic tenants of security

539
00:24:44,380 --> 00:24:46,780
to start with, and then
we get into the details.

540
00:24:47,690 --> 00:24:49,410
- So given the complexity and security

541
00:24:49,410 --> 00:24:52,100
are in many ways
antithetical to each other,

542
00:24:52,100 --> 00:24:53,649
when you take these complex systems,

543
00:24:53,650 --> 00:24:55,130
especially ones that use machine learning,

544
00:24:55,130 --> 00:24:57,610
which is largely an opaque technology,

545
00:24:57,610 --> 00:24:59,870
in the context of a much
bigger supply chain,

546
00:24:59,870 --> 00:25:01,909
that can create a whole set
of interesting implications.

547
00:25:01,910 --> 00:25:02,960
Recently, we did see

548
00:25:02,960 --> 00:25:05,410
a major supply chain
attacked with SolarWinds.

549
00:25:05,410 --> 00:25:06,890
In fact, the CEO of SolarWinds,

550
00:25:06,890 --> 00:25:09,380
Sudhakar Ramakrishna is gonna
be one of the keynote speakers

551
00:25:09,380 --> 00:25:11,490
at RSA Conference this year.

552
00:25:11,490 --> 00:25:12,540
Ross, maybe I can ask you,

553
00:25:12,540 --> 00:25:14,920
what is your perspective
on supply chain attacks

554
00:25:14,920 --> 00:25:16,220
and supply chain security?

555
00:25:17,810 --> 00:25:20,110
- Well, the SolarWinds attack

556
00:25:20,110 --> 00:25:22,306
is a very useful reminder to us all,

557
00:25:22,307 --> 00:25:23,840
for a couple of reasons.

558
00:25:23,840 --> 00:25:28,439
First, if you look at a
large corporate IT shop,

559
00:25:28,440 --> 00:25:30,960
there are dozens to hundreds of suppliers

560
00:25:30,960 --> 00:25:32,070
who have got their software

561
00:25:32,070 --> 00:25:34,100
on thousands and thousands of machines.

562
00:25:34,100 --> 00:25:35,840
SolarWinds was an example,

563
00:25:35,840 --> 00:25:39,470
people in big window shops use
it for optimizing databases.

564
00:25:39,470 --> 00:25:42,470
The second thing that
we take away from that

565
00:25:42,470 --> 00:25:45,540
is that SolarWinds was a mature company.

566
00:25:45,540 --> 00:25:47,670
Once upon a time, it was a keen startup

567
00:25:47,670 --> 00:25:49,810
with lots of lively engineers,

568
00:25:49,810 --> 00:25:52,240
but recently it had become a monopoly.

569
00:25:52,240 --> 00:25:55,270
And much of the technical expertise

570
00:25:55,270 --> 00:25:58,610
had been farmed out to
engineers in Eastern Europe.

571
00:25:58,610 --> 00:26:01,139
And so they weren't caring
as much about security

572
00:26:01,140 --> 00:26:02,060
as they used to.

573
00:26:02,060 --> 00:26:04,360
In essence, the company
was being run by bankers

574
00:26:04,360 --> 00:26:05,850
as a cash cow.

575
00:26:05,850 --> 00:26:08,179
And so one of the pieces of due diligence

576
00:26:08,180 --> 00:26:10,870
you now have to do if
you're running a big IT shop

577
00:26:10,870 --> 00:26:15,310
is to ask yourself about the
culture and the ownership

578
00:26:15,310 --> 00:26:18,020
and the competence of all those suppliers

579
00:26:18,020 --> 00:26:21,150
who have got stuff within
your security perimeter.

580
00:26:21,150 --> 00:26:23,140
And this is something that big IT shops

581
00:26:23,140 --> 00:26:25,200
are not used to doing.

582
00:26:25,200 --> 00:26:30,183
- Let me mention one recent
announcement that Intel made.

583
00:26:31,520 --> 00:26:36,093
Intel used to produce all the
chips in Intel owned fabs.

584
00:26:37,000 --> 00:26:39,200
Under the new CEO, they've declared

585
00:26:39,200 --> 00:26:42,040
that they are going to farm it out

586
00:26:42,040 --> 00:26:44,629
to all kinds of other fabs.

587
00:26:44,630 --> 00:26:49,630
And I see this as something
which is of strategic importance

588
00:26:50,410 --> 00:26:53,343
as far as the security of
supply chains are concerned.

589
00:26:54,340 --> 00:26:55,889
The U.S. is already producing

590
00:26:55,890 --> 00:26:59,750
only about 12% of the
semiconductors in the world,

591
00:26:59,750 --> 00:27:01,750
and it will go way down if Intel

592
00:27:01,750 --> 00:27:05,450
stops making those in the fabs.

593
00:27:05,450 --> 00:27:08,720
And I'm in particular worried about

594
00:27:10,260 --> 00:27:13,990
certain process-related aspects of chips,

595
00:27:13,990 --> 00:27:18,990
which might change in
an imperceptible way.

596
00:27:19,350 --> 00:27:21,360
For example, in each Intel chip,

597
00:27:21,360 --> 00:27:25,810
there is a random number generator

598
00:27:25,810 --> 00:27:29,493
and I'm afraid that by
tweaking a little bit

599
00:27:30,690 --> 00:27:33,337
the process parameters,
the quality of those

600
00:27:33,337 --> 00:27:36,210
random number generators
could deteriorate.

601
00:27:36,210 --> 00:27:39,810
I haven't seen the complete
design of the generators,

602
00:27:39,810 --> 00:27:42,560
but I wouldn't be surprised
if it could have an effect.

603
00:27:44,520 --> 00:27:45,353
- I think what's remarkable here

604
00:27:45,353 --> 00:27:46,590
is when you look at these systems

605
00:27:46,590 --> 00:27:48,110
and as they get more complex,

606
00:27:48,110 --> 00:27:51,209
all the interdependencies can
create all sorts of issues.

607
00:27:51,210 --> 00:27:53,900
Now, today, we're seeing
two examples of this,

608
00:27:53,900 --> 00:27:55,780
obviously within the context of COVID-19,

609
00:27:55,780 --> 00:27:57,620
there's digital contact racing,

610
00:27:57,620 --> 00:27:59,320
as well as the idea of immunity passports,

611
00:27:59,320 --> 00:28:00,153
both of which involve

612
00:28:00,153 --> 00:28:02,600
a lot of complex
components coming together.

613
00:28:02,600 --> 00:28:03,800
Carmela, I know you spent a lot of time

614
00:28:03,800 --> 00:28:05,720
thinking about some of these technologies.

615
00:28:05,720 --> 00:28:07,620
Can you shed some light
on your perspectives

616
00:28:07,620 --> 00:28:09,493
and where we are gonna go with them?

617
00:28:10,760 --> 00:28:11,713
- So I think that complexity

618
00:28:11,713 --> 00:28:13,750
and we said here the supply chain

619
00:28:13,750 --> 00:28:15,550
showed very well in the contact tracing

620
00:28:15,550 --> 00:28:17,290
and the digital contact tracing.

621
00:28:17,290 --> 00:28:18,710
When some of this discussion has started

622
00:28:18,710 --> 00:28:20,230
and we're talking about the protocols,

623
00:28:20,230 --> 00:28:21,180
at the end of the day the protocol

624
00:28:21,180 --> 00:28:23,640
is a very small part of the system.

625
00:28:23,640 --> 00:28:24,960
And one of the things is that protocol

626
00:28:24,960 --> 00:28:26,760
has to run on the mobile phone,

627
00:28:26,760 --> 00:28:30,560
and the mobile phone is not
owned by us as the people.

628
00:28:30,560 --> 00:28:32,080
The mobile phone of the product system

629
00:28:32,080 --> 00:28:35,860
is owned by two companies
that had to enter into this,

630
00:28:35,860 --> 00:28:38,540
and they actually
implemented the protocol.

631
00:28:38,540 --> 00:28:40,470
And with that uncertain interdependency,

632
00:28:40,470 --> 00:28:44,010
that actually meant several
choices for privacy engineering

633
00:28:44,010 --> 00:28:46,180
that I don't have the
time to talk about today,

634
00:28:46,180 --> 00:28:47,450
but they took some decisions

635
00:28:47,450 --> 00:28:50,210
that defined the privacy
for the whole world.

636
00:28:50,210 --> 00:28:51,520
And even though they did that

637
00:28:51,520 --> 00:28:53,700
and even though they got those things,

638
00:28:53,700 --> 00:28:55,640
I think that I found very interesting

639
00:28:55,640 --> 00:28:58,690
is that under the Data Protection
Regulation, for instance,

640
00:28:58,690 --> 00:29:00,530
they are still part of the supply chain,

641
00:29:00,530 --> 00:29:02,570
and as such, not subject to the law,

642
00:29:02,570 --> 00:29:06,090
so very free, which is
something very surprising.

643
00:29:06,090 --> 00:29:08,040
And other dependencies that we have,

644
00:29:08,040 --> 00:29:09,340
you said immunity passports,

645
00:29:09,340 --> 00:29:11,600
dependencies that we're
seeing how they start,

646
00:29:11,600 --> 00:29:14,939
we're seeing how the WHO
and the European Commission,

647
00:29:14,940 --> 00:29:16,190
they're creating big proposals

648
00:29:16,190 --> 00:29:19,080
based on public key infrastructure

649
00:29:19,080 --> 00:29:20,530
that will create that dependency

650
00:29:20,530 --> 00:29:23,270
on an infrastructure that
does not even exist yet,

651
00:29:23,270 --> 00:29:26,580
that actually will determine
the success of these platforms.

652
00:29:26,580 --> 00:29:28,179
And who's gonna run this

653
00:29:28,180 --> 00:29:29,750
and how much we are gonna depend on them

654
00:29:29,750 --> 00:29:32,100
is something that we have to think about

655
00:29:32,100 --> 00:29:35,574
when we're gonna think about
the resilience of the systems.

656
00:29:35,575 --> 00:29:36,408
- That makes a lot of sense.

657
00:29:36,408 --> 00:29:37,560
And what's interesting here

658
00:29:37,560 --> 00:29:40,070
is that these are just
a couple of examples

659
00:29:40,070 --> 00:29:44,220
that have come up over
the last just year or so.

660
00:29:44,220 --> 00:29:45,053
And we've spent a lot of time

661
00:29:45,053 --> 00:29:47,380
trying to engineer privacy
into these systems.

662
00:29:47,380 --> 00:29:48,970
Maybe I'll ask a bit of
a controversial question,

663
00:29:48,970 --> 00:29:50,630
to what extent do you think our attempt

664
00:29:50,630 --> 00:29:52,800
to engineer privacy has maybe hindered

665
00:29:52,800 --> 00:29:55,190
some of the attempts we've
made to address COVID?

666
00:29:55,190 --> 00:29:56,410
And I'll leave it to the panel,

667
00:29:56,410 --> 00:29:57,650
and anybody who wants to take that on.

668
00:29:57,650 --> 00:30:00,120
Maybe Adi, starting with
you, given you're in Israel,

669
00:30:00,120 --> 00:30:01,439
where there's been a really good job

670
00:30:01,440 --> 00:30:02,840
of getting to herd immunity.

671
00:30:03,730 --> 00:30:07,670
- I think that we have to admit the fact

672
00:30:07,670 --> 00:30:12,670
that privacy considerations
have reduced the effectiveness

673
00:30:13,070 --> 00:30:18,070
of many of the contact
tracing systems we are using.

674
00:30:19,580 --> 00:30:24,580
In some sense, Apple and
Google being a monopoly,

675
00:30:26,120 --> 00:30:30,219
controlling the Bluetooth API,

676
00:30:30,220 --> 00:30:35,070
have prevented many
countries from doing things

677
00:30:35,070 --> 00:30:40,070
which are not so, that do
not really violate privacy.

678
00:30:40,300 --> 00:30:45,300
I know that they've just
blocked an attempt by the NHS

679
00:30:46,860 --> 00:30:51,860
in the U.K. to update their British system

680
00:30:52,630 --> 00:30:57,160
and deal with situations in
which someone walks into a bar

681
00:30:57,160 --> 00:31:00,060
and wants to scan a QR code

682
00:31:00,060 --> 00:31:03,980
because this has location
information associated with it.

683
00:31:03,980 --> 00:31:06,610
Israel, for example,
was forced not to use,

684
00:31:06,610 --> 00:31:08,959
in its contact tracing application,

685
00:31:08,960 --> 00:31:12,750
not to use the Apple Google API,

686
00:31:12,750 --> 00:31:16,800
because Apple and Google refused to allow

687
00:31:16,800 --> 00:31:21,399
some minimal amount of location
information to be passed on.

688
00:31:21,400 --> 00:31:26,200
So we have to admit it, we
are getting less capable

689
00:31:26,200 --> 00:31:28,020
contact tracing programs,

690
00:31:28,020 --> 00:31:30,760
people will say that it's
a price worth paying,

691
00:31:30,760 --> 00:31:32,073
but we are paying a price.

692
00:31:33,370 --> 00:31:35,750
- I would have a question
for that, which is,

693
00:31:35,750 --> 00:31:39,470
you say that this bit of
location that Israel had,

694
00:31:39,470 --> 00:31:41,920
you think that really had
an effect on how Israel

695
00:31:41,920 --> 00:31:43,610
has come out better,

696
00:31:43,610 --> 00:31:45,330
or is it about the speed of vaccination?

697
00:31:45,330 --> 00:31:46,610
Is that bit of information

698
00:31:46,610 --> 00:31:48,949
that actually inserts a
lot of risk in the system,

699
00:31:48,950 --> 00:31:50,283
what made the difference?

700
00:31:51,290 --> 00:31:55,680
- The vaccination in Israel
had been a great success,

701
00:31:55,680 --> 00:32:00,623
but the contact tracing was
something which was limping,

702
00:32:02,408 --> 00:32:07,010
and not too many people had
downloaded the application.

703
00:32:07,010 --> 00:32:11,000
And actually, it was the security services

704
00:32:11,000 --> 00:32:16,000
which provided the contact
information between phones.

705
00:32:16,110 --> 00:32:18,159
And that's certainly
not privacy preserving

706
00:32:18,160 --> 00:32:19,610
in any way and form.

707
00:32:19,610 --> 00:32:21,939
So Israel is successful,

708
00:32:21,940 --> 00:32:25,533
but not because it tried to
be very privacy preserving.

709
00:32:26,790 --> 00:32:29,830
- I would say that the
experience of the U.K.

710
00:32:29,830 --> 00:32:31,210
and a number of other countries

711
00:32:31,210 --> 00:32:33,200
where I've been observing this

712
00:32:33,200 --> 00:32:36,520
has been that tech has got
in the way, not just privacy.

713
00:32:36,520 --> 00:32:38,680
The contact tracing that
has worked in Britain

714
00:32:38,680 --> 00:32:40,980
has been the old fashioned variety

715
00:32:40,980 --> 00:32:43,860
where nurses at a general
practice form people up,

716
00:32:43,860 --> 00:32:45,679
they can speak the local language,

717
00:32:45,680 --> 00:32:49,200
they can get compliance from
people by winning their trust.

718
00:32:49,200 --> 00:32:51,310
Where we tried to put
that into call centers,

719
00:32:51,310 --> 00:32:53,120
it worked remarkably less well,

720
00:32:53,120 --> 00:32:56,050
and as for the app, it
worked almost not at all.

721
00:32:56,050 --> 00:32:57,770
We're going to see the same thing again

722
00:32:57,770 --> 00:33:00,330
when it comes to vaccine passports

723
00:33:00,330 --> 00:33:02,133
and immunity certification.

724
00:33:03,470 --> 00:33:08,090
If you try to bring in a
vaccine passport in the U.K.

725
00:33:08,090 --> 00:33:11,000
where we will have
vaccinated everybody by July,

726
00:33:11,000 --> 00:33:12,770
then by the time you have
written some software

727
00:33:12,770 --> 00:33:14,780
and tested it, it's too late,

728
00:33:14,780 --> 00:33:16,520
where there will be a requirement

729
00:33:16,520 --> 00:33:18,150
is with international travel.

730
00:33:18,150 --> 00:33:20,830
If people from Britain
want to travel to countries

731
00:33:20,830 --> 00:33:22,270
that might take another year or two

732
00:33:22,270 --> 00:33:24,950
to vaccinate their people
and come back again.

733
00:33:24,950 --> 00:33:27,670
But then we have good old
fashioned paper mechanisms

734
00:33:27,670 --> 00:33:30,360
like we have for your
yellow fever vaccination.

735
00:33:30,360 --> 00:33:32,862
And so I've got my vaccine card,

736
00:33:34,170 --> 00:33:38,250
which I had written by the
nurse when I got my jab,

737
00:33:38,250 --> 00:33:40,750
and that's fine, I can
stick it in my passport.

738
00:33:40,750 --> 00:33:43,180
And that's good enough.

739
00:33:43,180 --> 00:33:47,110
Trying to build an all seeing,
all dancing worldwide system

740
00:33:47,110 --> 00:33:49,683
is the wrong thing to
do at a time like this.

741
00:33:50,740 --> 00:33:52,810
It's just rent seeking by tech companies

742
00:33:52,810 --> 00:33:54,230
who want to don governments

743
00:33:54,230 --> 00:33:55,990
for hundreds of millions of dollars.

744
00:33:55,990 --> 00:33:58,410
And in the process, they
will cause thousands

745
00:33:58,410 --> 00:34:01,130
of more lives to be unnecessarily lost.

746
00:34:01,130 --> 00:34:03,540
- I think the point
question you raised, Zully,

747
00:34:03,540 --> 00:34:06,100
was really one of privacy
versus effectiveness,

748
00:34:06,100 --> 00:34:08,520
and there's a couple of angles to that.

749
00:34:08,520 --> 00:34:12,370
One is just the GPS location
information that Carmela raised

750
00:34:12,370 --> 00:34:14,699
which is real experimented with,

751
00:34:14,699 --> 00:34:16,879
but there's also the issue of adoption.

752
00:34:16,880 --> 00:34:20,130
These apps are not effective
unless they're adopted widely.

753
00:34:20,130 --> 00:34:22,170
And one of the reasons
people don't adopt them

754
00:34:22,170 --> 00:34:23,179
is because they perceive them

755
00:34:23,179 --> 00:34:25,259
as being invasive of their privacy.

756
00:34:25,260 --> 00:34:30,219
So trying to win on a technical point

757
00:34:30,219 --> 00:34:32,370
by having a less private system

758
00:34:32,370 --> 00:34:34,920
may just get you a less
effective system overall

759
00:34:34,920 --> 00:34:37,060
because fewer people
will be using the system.

760
00:34:37,060 --> 00:34:38,440
- Which is a great capstone point

761
00:34:38,440 --> 00:34:39,920
because look, at the
beginning of the panel,

762
00:34:39,920 --> 00:34:42,409
we talked about the idea
that digital systems

763
00:34:42,409 --> 00:34:44,100
are only adopted by people,

764
00:34:44,100 --> 00:34:45,750
people who think that they're trustworthy

765
00:34:45,750 --> 00:34:47,840
and that they're resilient
and that they're secure.

766
00:34:47,840 --> 00:34:49,840
And maybe kind of in closing,
we have a few minutes left,

767
00:34:49,840 --> 00:34:50,969
I'd like to ask each panelist,

768
00:34:50,969 --> 00:34:52,679
provide your perspective on resilience,

769
00:34:52,679 --> 00:34:54,350
that's been the theme that's been common

770
00:34:54,350 --> 00:34:56,299
throughout this entire conversation.

771
00:34:56,300 --> 00:34:57,820
How should people be
thinking about resilience

772
00:34:57,820 --> 00:35:00,003
and what are your key
takeaways for the panel?

773
00:35:01,480 --> 00:35:03,460
And maybe starting with you, Ron?

774
00:35:03,460 --> 00:35:05,690
- Sure, I'd be happy to start with that.

775
00:35:05,690 --> 00:35:08,360
Yeah, resilience is a
interesting property.

776
00:35:08,360 --> 00:35:10,160
It's one we really should be striving for

777
00:35:10,160 --> 00:35:11,623
as we build secure systems.

778
00:35:12,480 --> 00:35:15,080
But as I think about it,
it's one that cryptographers

779
00:35:15,080 --> 00:35:18,840
are actually pretty terrible
at designing resilient systems.

780
00:35:18,840 --> 00:35:21,910
I mean, resilience means
you do well in the face

781
00:35:21,910 --> 00:35:23,127
of a break-in or something like that.

782
00:35:23,127 --> 00:35:25,890
And we have ideas for coping with that,

783
00:35:25,890 --> 00:35:29,350
we have ideas on forward
secrecy and so on too

784
00:35:29,350 --> 00:35:31,020
that come into play.

785
00:35:31,020 --> 00:35:33,759
But the idea of re-keying and
re-authenticating everybody

786
00:35:33,760 --> 00:35:35,510
is not one that we talk about much.

787
00:35:36,925 --> 00:35:37,890
There are things in that direction,

788
00:35:37,890 --> 00:35:40,710
now Adi will probably contradict
me on some of these things,

789
00:35:40,710 --> 00:35:43,710
but I think overall, I would
give us a grade of a C minus

790
00:35:43,710 --> 00:35:45,590
as cryptographers on resilience.

791
00:35:45,590 --> 00:35:48,650
I think the systems we tend
to design tend to be brittle

792
00:35:48,650 --> 00:35:52,033
and to break it if there's
a serious key compromise.

793
00:35:53,460 --> 00:35:58,060
- So I will actually give
system designers a D or an F,

794
00:35:58,060 --> 00:36:00,870
but I'll give cryptographers an A.

795
00:36:02,020 --> 00:36:05,300
I think that the cryptographic community

796
00:36:05,300 --> 00:36:09,770
had done great things in standardizing

797
00:36:11,130 --> 00:36:13,730
very good crypto systems.

798
00:36:13,730 --> 00:36:14,947
I think that the turning point

799
00:36:14,947 --> 00:36:17,890
was when AES was standardized.

800
00:36:17,890 --> 00:36:22,196
Since then, there have been
several carefully constructed,

801
00:36:24,870 --> 00:36:29,509
multi-year attempts to
design crypto systems

802
00:36:29,510 --> 00:36:31,770
which have been looked at.

803
00:36:31,770 --> 00:36:33,500
And over the last year,

804
00:36:33,500 --> 00:36:38,220
there is, of course, the
Post-Quantum Cryptography by NIST,

805
00:36:38,220 --> 00:36:41,129
there is a format preserving scheme

806
00:36:41,130 --> 00:36:43,310
which is now being standardized.

807
00:36:43,310 --> 00:36:47,660
NIST had just announced the third stage

808
00:36:49,040 --> 00:36:52,350
in evaluating lightweight cryptography.

809
00:36:52,350 --> 00:36:57,350
ISO is looking now at a
number of other standards

810
00:36:58,860 --> 00:37:01,040
including fully homomorphic encryption,

811
00:37:01,040 --> 00:37:03,570
it might be maybe premature,

812
00:37:03,570 --> 00:37:07,510
but they are trying to standardize
multi-party computations,

813
00:37:07,510 --> 00:37:09,240
secure multi-party computation

814
00:37:09,240 --> 00:37:10,910
and fully homomorphic encryption.

815
00:37:10,910 --> 00:37:15,220
I think that with all this
careful standardization,

816
00:37:15,220 --> 00:37:17,549
it is going to greatly enhance

817
00:37:17,550 --> 00:37:22,470
our ability to have robust
and secure crypto systems.

818
00:37:22,470 --> 00:37:24,633
Systems is something totally different.

819
00:37:27,150 --> 00:37:28,570
- Carmela.

820
00:37:28,570 --> 00:37:31,010
- I'm actually gonna
give cryptographers a B,

821
00:37:31,010 --> 00:37:34,720
because I think indeed we have
a lot of very good schemes

822
00:37:34,720 --> 00:37:35,959
that can be deployed,

823
00:37:35,960 --> 00:37:37,910
but we don't think that
much about deployment

824
00:37:37,910 --> 00:37:40,670
and things like which Ron
was saying about tweaking

825
00:37:40,670 --> 00:37:42,010
and how to monitor all these things

826
00:37:42,010 --> 00:37:43,310
are very hard in practice.

827
00:37:43,310 --> 00:37:46,049
And sometimes we don't
offer help to developers,

828
00:37:46,050 --> 00:37:49,250
especially when we talk
about things, like you say,

829
00:37:49,250 --> 00:37:51,790
fully homomorphic encryption
or multi-party computation

830
00:37:51,790 --> 00:37:53,460
that have a lot of tricks.

831
00:37:53,460 --> 00:37:54,293
But I think that the thing

832
00:37:54,293 --> 00:37:56,360
that we probably need to think about

833
00:37:56,360 --> 00:37:57,470
and my experience has been

834
00:37:57,470 --> 00:37:59,850
is that the more that we
move to this platform,

835
00:37:59,850 --> 00:38:02,430
like the mobile platform,
the cloud platform,

836
00:38:02,430 --> 00:38:03,990
we're actually removing resilience

837
00:38:03,990 --> 00:38:06,959
because we're putting all of
our eggs in the same basket.

838
00:38:06,960 --> 00:38:10,580
And an example happened last
year when Amazon went off,

839
00:38:10,580 --> 00:38:13,110
and half of the U.S.
couldn't enter the doors

840
00:38:13,110 --> 00:38:14,990
because they had Amazon Ring.

841
00:38:14,990 --> 00:38:17,629
So we need to really think
about, if we want to resilience,

842
00:38:17,630 --> 00:38:19,810
maybe we need more decentralization.

843
00:38:19,810 --> 00:38:20,643
- Ross, you are the final grader here,

844
00:38:20,643 --> 00:38:22,263
let's see how you turn out.

845
00:38:23,510 --> 00:38:26,280
- Well, I agree absolutely with Carmela,

846
00:38:26,280 --> 00:38:28,760
and I would put it more
strongly than that.

847
00:38:28,760 --> 00:38:32,580
We've already seen some systemic
problems with Heartbleed,

848
00:38:32,580 --> 00:38:34,040
with the hack of Comodo.

849
00:38:34,040 --> 00:38:35,930
Imagine what would happen if Verisign,

850
00:38:35,930 --> 00:38:38,290
for example, got hacked by the Chinese,

851
00:38:38,290 --> 00:38:39,800
but it's bigger than that.

852
00:38:39,800 --> 00:38:42,190
We tend to rely on products like Signal,

853
00:38:42,190 --> 00:38:44,210
for example, for secure messaging.

854
00:38:44,210 --> 00:38:45,960
But what happens if Signal breaks

855
00:38:45,960 --> 00:38:47,360
or is broken by governments

856
00:38:47,360 --> 00:38:50,810
as a result of it taking
cryptocurrency on board.

857
00:38:50,810 --> 00:38:52,950
And the big problem is what happens

858
00:38:52,950 --> 00:38:56,200
if Amazon goes down or Azure or Google?

859
00:38:56,200 --> 00:39:00,490
Now, people talk about having
a multi-cloud strategy,

860
00:39:00,490 --> 00:39:02,959
and sure, you can put
stuff in a container,

861
00:39:02,960 --> 00:39:04,760
but it's very, very hard indeed

862
00:39:04,760 --> 00:39:07,870
to actually migrate real
applications from Azure,

863
00:39:07,870 --> 00:39:10,047
if you're used to Azure, to AWS.

864
00:39:11,020 --> 00:39:13,240
Because all the stuff around
it changes, the crypto,

865
00:39:13,240 --> 00:39:16,069
how you talk to your hardware,
security modules, and so on.

866
00:39:16,070 --> 00:39:17,780
That's really where the
rubber hits the road

867
00:39:17,780 --> 00:39:20,430
when it comes to resilience
and applied cryptography.

868
00:39:22,346 --> 00:39:24,480
- Well, I can say watching
four professors give out grades

869
00:39:24,480 --> 00:39:26,230
gave me flashbacks to graduate school.

870
00:39:26,230 --> 00:39:29,440
And I'm super excited, but I
wanna give this panel an A plus

871
00:39:29,440 --> 00:39:32,510
for just a phenomenal,
just insightful discussion.

872
00:39:32,510 --> 00:39:35,100
And I think that the audience
will love to hear it.

873
00:39:35,100 --> 00:39:36,120
I think so many great insights.

874
00:39:36,120 --> 00:39:38,620
So thank you all very much
for today's discussion.

875
00:39:39,630 --> 00:39:41,767
- Thank you.
- Thank you, Zully.

876
00:39:43,080 --> 00:39:43,913
- Please stay with us

877
00:39:43,913 --> 00:39:46,940
for a special 30th
anniversary spotlight segment

878
00:39:46,940 --> 00:39:50,550
with one of the founders of
our industry, Whitfield Diffie.

879
00:39:50,550 --> 00:39:51,840
Let's get started.

880
00:39:51,840 --> 00:39:54,020
Whit, have you had any
interesting accolades

881
00:39:54,020 --> 00:39:54,853
over the past year?

882
00:39:54,853 --> 00:39:57,280
What is the most memorable
one you've ever gotten?

883
00:39:57,280 --> 00:39:58,750
- Two, three months ago,

884
00:39:58,750 --> 00:40:03,370
I was elected to the NSA
Cryptologic Hall of Honor.

885
00:40:03,370 --> 00:40:08,259
And as much as an NSA is
noted for loving its enemies,

886
00:40:08,260 --> 00:40:11,800
this struck me as the
most surprising honor

887
00:40:13,750 --> 00:40:15,740
I have ever ever received,

888
00:40:15,740 --> 00:40:17,993
and I'm actually very pleased with it.

889
00:40:19,000 --> 00:40:22,490
- Whit, quickly name the best
practice in cyber security

890
00:40:22,490 --> 00:40:24,033
over the past 30 years.

891
00:40:25,140 --> 00:40:26,272
- Best practice?

892
00:40:28,579 --> 00:40:31,360
Ah, the best practice.

893
00:40:31,360 --> 00:40:33,270
Well, I don't think people know

894
00:40:33,270 --> 00:40:35,490
how to practice cyber security,

895
00:40:35,490 --> 00:40:38,339
that's why they're
constantly caught off guard

896
00:40:38,340 --> 00:40:40,973
when they find themselves
doing the real thing.

897
00:40:42,010 --> 00:40:44,110
- Name the single most dangerous breach

898
00:40:44,110 --> 00:40:45,453
over the last 30 years.

899
00:40:46,570 --> 00:40:48,070
- Well, the one sticks in my mind

900
00:40:48,070 --> 00:40:50,163
is the Bureau of Personnel Management.

901
00:40:51,040 --> 00:40:52,860
Though the most interesting, I think,

902
00:40:52,860 --> 00:40:54,950
was something that just happened,

903
00:40:54,950 --> 00:40:59,950
and at first, it appeared that
FireEye had been broken into

904
00:41:02,940 --> 00:41:06,000
and then it turned out that
that was really to their credit,

905
00:41:06,000 --> 00:41:08,100
they were the only people alert enough

906
00:41:08,100 --> 00:41:10,283
to notice they had been broken into.

907
00:41:11,150 --> 00:41:14,420
- Whit, if you wrote a book
about the last 30 years

908
00:41:14,420 --> 00:41:16,470
in our industry, what would the title be?

909
00:41:17,527 --> 00:41:19,750
- "The Last 30 Years in Our Industry".

910
00:41:19,750 --> 00:41:20,583
- I love it.

911
00:41:20,583 --> 00:41:22,980
Whit, this year's conference
theme is resilience.

912
00:41:22,980 --> 00:41:24,087
How do you define that word?

913
00:41:24,087 --> 00:41:27,960
- Oh, resilience, resilience,
resilience. Springy, bouncy.

914
00:41:27,960 --> 00:41:29,350
Yeah, exactly the opposite

915
00:41:29,350 --> 00:41:31,470
of what we have in cyber security today,

916
00:41:31,470 --> 00:41:32,923
where things are fragile.

917
00:41:34,130 --> 00:41:34,963
- I love it.

918
00:41:34,963 --> 00:41:36,160
This is one answer I'm
looking forward to right now.

919
00:41:36,160 --> 00:41:39,100
Whit, if you could design a
piece of advice short enough

920
00:41:39,100 --> 00:41:42,630
to fit on a bumper sticker
regarding cybersecurity,

921
00:41:42,630 --> 00:41:46,400
what would that advice be for
your fellow security experts?

922
00:41:46,400 --> 00:41:49,290
- Short enough to fit on a bumper sticker.

923
00:41:49,290 --> 00:41:52,013
A big bumper sticker or
a small bumper sticker?

924
00:41:54,547 --> 00:41:56,410
"Unplug it, baby!"

925
00:41:56,410 --> 00:41:58,259
(Zulfikar laughs)

926
00:41:58,260 --> 00:41:59,410
- What is the biggest challenge

927
00:41:59,410 --> 00:42:01,609
that you see for our
industry going forward?

928
00:42:03,140 --> 00:42:06,750
- To get industry actually
interested in security

929
00:42:06,750 --> 00:42:09,500
rather than wanting to
have enough, more control

930
00:42:09,500 --> 00:42:12,793
over its customers than is
consistent with secure systems.

931
00:42:13,686 --> 00:42:14,870
- In just one word,

932
00:42:14,870 --> 00:42:17,163
what is the biggest challenge to security?

933
00:42:22,070 --> 00:42:23,123
- Companies.

934
00:42:24,090 --> 00:42:24,923
- All right, one final question,

935
00:42:24,923 --> 00:42:27,270
and here you can give me a longer answer.

936
00:42:27,270 --> 00:42:29,170
What does the future hold

937
00:42:29,170 --> 00:42:31,033
for cybersecurity in this industry?

938
00:42:32,970 --> 00:42:34,259
- What does the future hold?

939
00:42:34,260 --> 00:42:36,180
I mean, it's impossible to believe

940
00:42:36,180 --> 00:42:39,080
that somehow this problem can't be solved.

941
00:42:39,080 --> 00:42:40,710
On the other hand,

942
00:42:40,710 --> 00:42:44,710
can it be solved consistent
with a free society?

943
00:42:44,710 --> 00:42:48,380
I mean, there are so many
things in human society

944
00:42:48,380 --> 00:42:52,010
that seem to me to be
decreasing our freedom

945
00:42:52,010 --> 00:42:53,580
and pulling us together.

946
00:42:53,580 --> 00:42:56,600
So for a long time, I've been saying

947
00:42:56,600 --> 00:42:58,890
that I saw no way that human freedom

948
00:42:58,890 --> 00:43:03,020
could stand against
improving communications.

949
00:43:03,020 --> 00:43:06,690
And I doubt we're a decade
from a bunch of early adopters

950
00:43:06,690 --> 00:43:09,650
getting communicators put in their heads,

951
00:43:09,650 --> 00:43:11,570
and you won't have to
force people to get them

952
00:43:11,570 --> 00:43:14,930
because you won't be
competitive without it.

953
00:43:14,930 --> 00:43:19,930
So I'm guessing that the
whole situation will change

954
00:43:20,380 --> 00:43:23,910
and that the freedom we now enjoy

955
00:43:23,910 --> 00:43:26,370
will become very hard to come by,

956
00:43:26,370 --> 00:43:28,890
and we'll think, oh gosh,

957
00:43:28,890 --> 00:43:31,993
back in those days when
we actually had privacy.

958
00:43:33,740 --> 00:43:34,573
- I love it.

959
00:43:34,573 --> 00:43:35,406
So Whit, first of all,

960
00:43:35,406 --> 00:43:37,220
I wanna thank you for your time
today, and more importantly,

961
00:43:37,220 --> 00:43:39,390
thank you for your amazing contributions

962
00:43:39,390 --> 00:43:41,069
for the past few decades.

963
00:43:41,070 --> 00:43:43,753
- Well, thank you, thank
you for appreciating it.

