1
00:00:01,270 --> 00:00:03,120
- Welcome to today's discussion

2
00:00:03,120 --> 00:00:06,189
of misinformation hot
topics including elections,

3
00:00:06,190 --> 00:00:09,900
and COVID and other emerging issues.

4
00:00:09,900 --> 00:00:12,760
We're delighted to have
you with us here today.

5
00:00:12,760 --> 00:00:16,210
We are looking at a year
that is following 2020

6
00:00:16,210 --> 00:00:17,670
which was a very difficult

7
00:00:17,670 --> 00:00:20,120
and challenging year in misinformation.

8
00:00:20,120 --> 00:00:23,410
2021 is proving to be just as challenging.

9
00:00:23,410 --> 00:00:26,550
And today with us, we have three experts

10
00:00:26,550 --> 00:00:29,110
in the field of mis-and disInformation

11
00:00:29,110 --> 00:00:31,630
to help us unpack and understand some

12
00:00:31,630 --> 00:00:35,280
of what's going on with
mis-and disinformation,

13
00:00:35,280 --> 00:00:38,020
and to help us wrap our heads
around what's coming next,

14
00:00:38,020 --> 00:00:39,630
and where can we go

15
00:00:39,630 --> 00:00:43,610
in the battle against
mis-and disinformation.

16
00:00:43,610 --> 00:00:45,690
With me today, we have John Kelly,

17
00:00:45,690 --> 00:00:48,860
who is the founder and head of Graphika.

18
00:00:48,860 --> 00:00:50,200
We have Mark Schneider

19
00:00:50,200 --> 00:00:51,440
of the MITRE corporation

20
00:00:51,440 --> 00:00:53,985
where he is Co-Director
of Election Integrity.

21
00:00:53,985 --> 00:00:56,320
And Isabella Camargo

22
00:00:56,320 --> 00:00:59,250
from the Stanford Internet Observatory.

23
00:00:59,250 --> 00:01:01,252
Thank you all for joining me.

24
00:01:01,252 --> 00:01:04,519
And without further ado, why
don't we just jump right in

25
00:01:04,519 --> 00:01:08,200
and I'll start by asking
John, what did we learn,

26
00:01:08,200 --> 00:01:12,078
about misinformation during
the 2020 election cycle?

27
00:01:12,078 --> 00:01:14,035
- (chuckles) But I mean, I, in a nutshell

28
00:01:14,035 --> 00:01:18,509
I'd say it's kind of the corrollery of,

29
00:01:18,510 --> 00:01:20,330
you know with friends like
these who needs enemies.

30
00:01:20,330 --> 00:01:22,800
In other words there's
kind of domestic sources

31
00:01:22,800 --> 00:01:25,840
of misinformation like we saw, you know

32
00:01:25,840 --> 00:01:28,572
who needs the foreign adversaries that

33
00:01:28,572 --> 00:01:33,030
we were worried about so much
after the 2016 election cycle.

34
00:01:33,030 --> 00:01:35,370
And so that, that is
sort of like the headline

35
00:01:35,370 --> 00:01:37,090
but so much of what happened, turned out

36
00:01:37,090 --> 00:01:38,860
to be domestic when we
have been so worried

37
00:01:38,860 --> 00:01:39,693
about foreign stuff.

38
00:01:39,693 --> 00:01:42,649
Now that's not to say that
there were foreign attempts,

39
00:01:42,650 --> 00:01:45,840
I mean, graphic and with our
partners who we attracted

40
00:01:45,840 --> 00:01:49,679
at least 12 foreign information operations

41
00:01:49,679 --> 00:01:52,480
that at least lobbed a couple
of grenades in the direction

42
00:01:52,480 --> 00:01:55,152
of the 2020 election,
but overwhelmingly what

43
00:01:55,152 --> 00:01:58,403
what we saw was kind
of domestic in nature.

44
00:01:59,370 --> 00:02:00,360
- Thanks, John.

45
00:02:00,360 --> 00:02:01,929
Mark, what did you see what happened

46
00:02:01,930 --> 00:02:03,420
with this splint program and other ways

47
00:02:03,420 --> 00:02:06,820
that we were watching
misinformation there?

48
00:02:06,820 --> 00:02:09,750
- Sure. So I'm with squint

49
00:02:09,750 --> 00:02:13,190
which is the social, See
Something Say Something Program.

50
00:02:13,190 --> 00:02:16,050
We enabled election officials,

51
00:02:16,050 --> 00:02:16,883
and

52
00:02:16,883 --> 00:02:19,770
other groups,

53
00:02:19,770 --> 00:02:22,680
and interested parties,

54
00:02:22,680 --> 00:02:26,840
to be able to rapidly
report misinformation,

55
00:02:26,840 --> 00:02:27,750
and turn that around

56
00:02:27,750 --> 00:02:31,370
and get that useful
actionable information,

57
00:02:31,370 --> 00:02:34,210
into the hands of the election officials.

58
00:02:34,210 --> 00:02:38,190
And what, what we saw was a
variety of different things,

59
00:02:38,190 --> 00:02:40,890
you know, we saw, you know,

60
00:02:40,890 --> 00:02:43,070
claims getting distorted, you know

61
00:02:43,070 --> 00:02:45,640
where there would be
a small incident where

62
00:02:45,640 --> 00:02:50,343
there was a few ballots
that were misplaced,

63
00:02:51,390 --> 00:02:53,640
and that got conflated with examples

64
00:02:53,640 --> 00:02:57,230
of like a mail tray that was found.

65
00:02:57,230 --> 00:02:59,549
And all of a sudden there
were lots of you know,

66
00:02:59,550 --> 00:03:02,220
mail trays, and ballots
lost all over the place.

67
00:03:02,220 --> 00:03:05,770
So you had this combination of,

68
00:03:05,770 --> 00:03:09,420
not only the mis-information,

69
00:03:09,420 --> 00:03:11,869
but it getting mixed
together and amplified,

70
00:03:11,870 --> 00:03:14,610
and new stories coming out of that.

71
00:03:14,610 --> 00:03:18,380
And I think that was
something that, you know,

72
00:03:18,380 --> 00:03:22,113
both was occurring with domestic,

73
00:03:22,113 --> 00:03:26,090
people just innocently
spreading misinformation,

74
00:03:26,090 --> 00:03:27,710
as well as, you know,

75
00:03:27,710 --> 00:03:32,710
possibly foreign malign influence
where they were amplified.

76
00:03:32,710 --> 00:03:35,103
Those sorts of pieces of misinformation.

77
00:03:36,090 --> 00:03:37,188
- Thanks, and Isabella

78
00:03:37,188 --> 00:03:40,036
at the Election Integrity
Partnership out of Stanford.

79
00:03:40,036 --> 00:03:44,453
What were your learnings
out of this election cycle?

80
00:03:45,650 --> 00:03:48,150
- Absolutely. I would reiterate a lot

81
00:03:48,150 --> 00:03:50,670
of what John pointed out
in terms of the content,

82
00:03:50,670 --> 00:03:54,369
and Mark in terms of what was
really needed in this space.

83
00:03:54,370 --> 00:03:56,400
Some way to respond to the influx

84
00:03:56,400 --> 00:03:58,270
of this information that we were seeing,

85
00:03:58,270 --> 00:04:00,290
and really what double down on that point

86
00:04:00,290 --> 00:04:03,150
that there's really a
vacuum in this space,

87
00:04:03,150 --> 00:04:04,804
for the systematic documentation,

88
00:04:04,804 --> 00:04:08,650
and the ability to put
narratives into context,

89
00:04:08,650 --> 00:04:10,900
in a way that matches the pace

90
00:04:10,900 --> 00:04:13,460
at which these narratives are created.

91
00:04:13,460 --> 00:04:15,180
So that is something that
we really focused on,

92
00:04:15,180 --> 00:04:17,640
in the Election Integrity Partnership,

93
00:04:17,640 --> 00:04:20,079
and thinking about, you know,
what are the different facets

94
00:04:20,079 --> 00:04:23,260
to creating a way to get the research

95
00:04:23,260 --> 00:04:25,210
that so many people are
doing in this space,

96
00:04:25,210 --> 00:04:28,239
into a context where it can
have impact in real time.

97
00:04:28,240 --> 00:04:31,090
So that's something that
I was really interested in

98
00:04:31,090 --> 00:04:32,169
during the election.

99
00:04:32,170 --> 00:04:33,460
How do we, you know,

100
00:04:33,460 --> 00:04:35,169
track these narratives down quickly enough

101
00:04:35,170 --> 00:04:36,570
to actually respond to them?

102
00:04:37,477 --> 00:04:39,560
And one thing that made
that really difficult

103
00:04:39,560 --> 00:04:42,410
on the content side, is this
idea of repeat offenders,

104
00:04:42,410 --> 00:04:44,060
which we highlight in our report.

105
00:04:45,020 --> 00:04:47,469
There was a very small handful of users

106
00:04:47,470 --> 00:04:50,560
that ended up creating a lot
of the actual disinformation,

107
00:04:50,560 --> 00:04:52,900
and this infrastructure
that really exists to

108
00:04:52,900 --> 00:04:54,570
peddle these narratives.

109
00:04:54,570 --> 00:04:55,450
So those were, I think

110
00:04:55,450 --> 00:04:58,469
two of my biggest takeaways
from the election.

111
00:04:58,470 --> 00:04:59,800
- Thank you, Isabella.

112
00:04:59,800 --> 00:05:04,160
You know, last year it
seems to be walking with us

113
00:05:04,160 --> 00:05:05,510
into the next election cycle,

114
00:05:05,510 --> 00:05:07,409
we certainly need to be preparing,

115
00:05:07,410 --> 00:05:10,960
and taking these lessons
learned as we face 2022.

116
00:05:10,960 --> 00:05:14,739
But here we are in 2021,
and what I'm seeing

117
00:05:14,740 --> 00:05:17,660
and I wanna ask you all
about, is massive amounts

118
00:05:17,660 --> 00:05:21,713
of mis-and disinformation
on the COVID-19 situation.

119
00:05:22,700 --> 00:05:25,670
So we're seeing things
that are our narrative

120
00:05:25,670 --> 00:05:28,100
in nature that persists over time.

121
00:05:28,100 --> 00:05:31,520
And we're also seeing things
that pop up and then pop down.

122
00:05:31,520 --> 00:05:32,859
The latest thing that we're seeing in,

123
00:05:32,860 --> 00:05:36,090
in our program is this week,
it's all about miscarriages,

124
00:05:36,090 --> 00:05:39,130
miscarriages that are
resulting from the J&J vaccine.

125
00:05:39,130 --> 00:05:40,969
There's no data behind that,

126
00:05:40,970 --> 00:05:43,590
but that's the current
allegations that we're seeing,

127
00:05:43,590 --> 00:05:46,989
and you know, looking around
at all of you, I'm wondering,

128
00:05:46,990 --> 00:05:48,740
what do you think we can do

129
00:05:48,740 --> 00:05:51,640
about the Coronavirus
related misinformation?

130
00:05:51,640 --> 00:05:53,890
Is this a different
challenge from elections?

131
00:05:53,890 --> 00:05:57,310
If so, how, how are we doing there?

132
00:05:57,310 --> 00:05:58,763
John let's start with you.

133
00:06:00,520 --> 00:06:02,930
- I would offer just some kind
of high level observations

134
00:06:02,930 --> 00:06:05,610
about how we've seen the
space evolve around COVID.

135
00:06:05,610 --> 00:06:09,402
And then I,

136
00:06:09,402 --> 00:06:10,700
you know,

137
00:06:10,700 --> 00:06:12,453
I believe Isabella you probably have the,

138
00:06:12,453 --> 00:06:14,679
some of the best Intel on
kind of what's happening

139
00:06:14,680 --> 00:06:16,030
on the ground with a lot of this stuff

140
00:06:16,030 --> 00:06:17,559
with the virality project and some

141
00:06:17,560 --> 00:06:20,110
of the new work there.

142
00:06:20,110 --> 00:06:23,010
But I'd say in broad strokes,
well, what we've seen

143
00:06:23,010 --> 00:06:26,780
is that COVID provided a way for a lot

144
00:06:26,780 --> 00:06:30,679
of formerly separate,
conspiracy-oriented communities

145
00:06:30,680 --> 00:06:33,360
to begin swirling together
and kind of sharing some

146
00:06:33,360 --> 00:06:35,900
of the same meta-narratives
narratives framing,

147
00:06:35,900 --> 00:06:37,099
and talking points, and also some

148
00:06:37,100 --> 00:06:39,650
of the same kind of points of contact.

149
00:06:39,650 --> 00:06:41,479
So you might've had things, you know,

150
00:06:41,480 --> 00:06:43,920
communities that before were
very different, you know,

151
00:06:43,920 --> 00:06:45,930
all kind of around
conspiracies, but, you know,

152
00:06:45,930 --> 00:06:48,200
you've got some that
are around more health,

153
00:06:48,200 --> 00:06:49,900
and medical oriented conspiracies,

154
00:06:49,900 --> 00:06:52,409
the anti-vax community which has existed

155
00:06:52,410 --> 00:06:54,746
for a long, you know, in
the long standing way,

156
00:06:54,746 --> 00:06:59,560
you've got tech sort, of
technophobic communities

157
00:06:59,560 --> 00:07:00,960
that believe that, you know,

158
00:07:02,290 --> 00:07:03,947
electrical wires cause cancer,

159
00:07:03,947 --> 00:07:07,380
and that 5g is going to
kill humans and animals,

160
00:07:07,380 --> 00:07:08,770
and things like that.

161
00:07:08,770 --> 00:07:11,118
And then you've got other conspiracies

162
00:07:11,118 --> 00:07:13,120
that are more political in nature.

163
00:07:13,120 --> 00:07:16,340
So various stripes of
political conspiracy,

164
00:07:16,340 --> 00:07:18,049
and COVID provided a way for all of these

165
00:07:18,050 --> 00:07:22,420
to start coming together, and
really starting to gel as,

166
00:07:22,420 --> 00:07:25,880
as kind of a unified
conspiracy front, if you will.

167
00:07:25,880 --> 00:07:27,190
And that happened across countries.

168
00:07:27,190 --> 00:07:29,780
So even though kind of the early phases

169
00:07:29,780 --> 00:07:33,080
of COVID misinformation
were really led by,

170
00:07:33,080 --> 00:07:35,580
primarily kind of the European
and American right-wing

171
00:07:35,580 --> 00:07:37,830
far right, they've been
joined by all kinds

172
00:07:37,830 --> 00:07:40,580
of other folks that
really don't share, much

173
00:07:40,580 --> 00:07:42,599
of anything else with the far right.

174
00:07:42,600 --> 00:07:45,623
Including kind of health and
wellness oriented communities.

175
00:07:46,876 --> 00:07:49,609
- So Isabella, I'd be
interested in your thoughts on,

176
00:07:49,610 --> 00:07:52,478
are the narratives similar,
you know, around Coronavirus,

177
00:07:52,478 --> 00:07:55,940
are there similar here
in the United States to

178
00:07:55,940 --> 00:07:58,580
what we might see in Canada or Belarus or

179
00:07:58,580 --> 00:08:02,440
or are we really a different
kind of audience here?

180
00:08:02,440 --> 00:08:04,770
in the-
- Yeah, so

181
00:08:04,770 --> 00:08:07,630
I think this has been something
really really interesting,

182
00:08:07,630 --> 00:08:09,600
for, for a bit of context
the virality project

183
00:08:09,600 --> 00:08:12,250
that John mentioned is,
the second iteration

184
00:08:12,250 --> 00:08:14,740
of what we were doing in the
Election Integrity Partnership,

185
00:08:14,740 --> 00:08:18,620
now focused on, vaccine
disinformation response.

186
00:08:18,620 --> 00:08:20,600
Since a lot of the partners are the same,

187
00:08:20,600 --> 00:08:22,870
we did have this big open
question of rehashing

188
00:08:22,870 --> 00:08:24,170
what's gonna be in and out of scope

189
00:08:24,170 --> 00:08:25,570
for this new partnership.

190
00:08:25,570 --> 00:08:29,190
And whether we were going to
focus only on U.S. narratives

191
00:08:29,190 --> 00:08:33,000
as we are focused on, U.S.
partners in government,

192
00:08:33,000 --> 00:08:34,683
and providing briefings to them.

193
00:08:35,700 --> 00:08:37,433
Versus having a global
context that was something

194
00:08:37,433 --> 00:08:40,689
that we really went back and
forth on because, you know,

195
00:08:40,690 --> 00:08:43,840
you can only track so much
disinformation, but when it comes

196
00:08:43,840 --> 00:08:47,490
to the vaccine, this is
really a global question,

197
00:08:47,490 --> 00:08:50,660
because someone dying in the UK is a story

198
00:08:50,660 --> 00:08:53,925
that is picked up by domestic
anti-vaccine influencers,

199
00:08:53,925 --> 00:08:57,449
to promote vaccine hesitancy
in the U S population.

200
00:08:57,450 --> 00:09:01,340
So this is a far more difficult problem

201
00:09:01,340 --> 00:09:04,180
I'm finding at least,
than the election itself,

202
00:09:04,180 --> 00:09:07,000
because it is global in context.

203
00:09:07,000 --> 00:09:11,280
The actual stories are
changing so so quickly,

204
00:09:11,280 --> 00:09:15,470
with just this week the
Johnson & Johnson vaccine,

205
00:09:15,470 --> 00:09:19,270
coming out as being halted
in the United States.

206
00:09:19,270 --> 00:09:21,960
So, and I think the last
thing here, is especially

207
00:09:21,960 --> 00:09:24,930
as disinformation researchers,
and people really focused

208
00:09:24,930 --> 00:09:27,839
on that space, it is even more difficult

209
00:09:27,840 --> 00:09:30,870
for us to actually try
to parse out, you know

210
00:09:30,870 --> 00:09:32,280
what can we say with certainty here?

211
00:09:32,280 --> 00:09:34,380
You need even more knowledge

212
00:09:34,380 --> 00:09:36,840
on context on the actual situation.

213
00:09:36,840 --> 00:09:38,570
So we've really been kind
of swimming through this

214
00:09:38,570 --> 00:09:40,090
and swimming through, you know,

215
00:09:40,090 --> 00:09:42,440
how do you take a global context?

216
00:09:42,440 --> 00:09:45,410
How do you parse out
all of these narratives

217
00:09:45,410 --> 00:09:48,550
that are kind of swapping back
and forth very, very quickly?

218
00:09:48,550 --> 00:09:50,598
So it has been a, a big challenge

219
00:09:50,599 --> 00:09:51,840
and we're still kind of working

220
00:09:51,840 --> 00:09:53,925
through how do we adapt here.

221
00:09:53,925 --> 00:09:54,757
- Thank you.

222
00:09:54,758 --> 00:09:55,600
Thank you, Bella.

223
00:09:55,600 --> 00:09:57,920
Mark, what are your
thoughts on the similarities

224
00:09:57,920 --> 00:10:00,640
and differences in dealing
with misinformation

225
00:10:00,640 --> 00:10:01,860
around elections,

226
00:10:01,860 --> 00:10:05,113
and misinformation
around a global pandemic?

227
00:10:06,020 --> 00:10:07,943
- Yeah so that's a good question,

228
00:10:07,943 --> 00:10:12,930
because with the election misinformation

229
00:10:12,930 --> 00:10:17,930
you know, you, you could
easily sort of divide,

230
00:10:18,200 --> 00:10:20,993
the the misinformation into like,

231
00:10:22,350 --> 00:10:25,130
targeted audiences or types

232
00:10:25,130 --> 00:10:27,923
of things where they're
trying to divide communities,

233
00:10:29,020 --> 00:10:32,689
try to suppress a
turnout, things like that.

234
00:10:32,690 --> 00:10:34,160
With

235
00:10:34,160 --> 00:10:34,992
the

236
00:10:35,850 --> 00:10:36,683
COVID,

237
00:10:38,182 --> 00:10:39,820
what you don't have that,

238
00:10:39,820 --> 00:10:43,500
I guess central topic of the election.

239
00:10:43,500 --> 00:10:46,290
So it can go off in lots
of different directions,

240
00:10:46,290 --> 00:10:49,410
you know, we can go off
discussing blood clots

241
00:10:49,410 --> 00:10:52,430
or discussing miscarriages or whatnot,

242
00:10:52,430 --> 00:10:55,599
and it's, as John had mentioned, you know,

243
00:10:55,600 --> 00:11:00,403
these all these disparate
groups coming together,

244
00:11:02,260 --> 00:11:06,960
to, you know, combine their
mis-and disinformation.

245
00:11:06,960 --> 00:11:09,063
And that was something that,

246
00:11:09,960 --> 00:11:12,850
we didn't see in the same
way during the elections.

247
00:11:12,850 --> 00:11:16,160
It was during the elections
it was more about,

248
00:11:16,160 --> 00:11:18,600
you know, going off to try to target

249
00:11:19,540 --> 00:11:21,069
either a group,

250
00:11:21,070 --> 00:11:25,473
or a particular voting system
vendor or something like that.

251
00:11:27,180 --> 00:11:31,065
- Yeah, really helpful observations there.

252
00:11:31,066 --> 00:11:33,430
Let's take a step back
and start thinking about

253
00:11:33,430 --> 00:11:35,760
the bigger picture of misinformation,

254
00:11:35,760 --> 00:11:37,220
so far in our conversation

255
00:11:37,220 --> 00:11:39,790
we focused on last
year's big problem space,

256
00:11:39,790 --> 00:11:41,250
and that's the elections.

257
00:11:41,250 --> 00:11:42,850
This year is big problem space,

258
00:11:42,850 --> 00:11:45,110
we're all focused on Coronavirus.

259
00:11:45,110 --> 00:11:47,580
What about misinformation generally?

260
00:11:47,580 --> 00:11:50,030
John I'm wondering what you think about,

261
00:11:50,030 --> 00:11:51,620
how well are we doing,

262
00:11:51,620 --> 00:11:56,250
on growing our capability, as
a nation and even globally,

263
00:11:56,250 --> 00:11:58,030
in, in fighting misinformation,

264
00:11:58,030 --> 00:11:59,860
and where could we be stronger

265
00:11:59,860 --> 00:12:02,816
and what should we be
doing differently here?

266
00:12:02,816 --> 00:12:04,960
- Well I say, there's
good news and bad news.

267
00:12:04,960 --> 00:12:08,140
I think the good news is
that we're finding that,

268
00:12:08,140 --> 00:12:10,010
and there's good news a number of funds,

269
00:12:10,010 --> 00:12:10,960
particularly the kinds

270
00:12:10,960 --> 00:12:14,010
of collaborations between
different entities that we saw

271
00:12:14,010 --> 00:12:17,610
with the Election Integrity Partnership,

272
00:12:17,610 --> 00:12:18,897
that were very helpful with now around

273
00:12:18,897 --> 00:12:22,480
the virality project where you
have, different institutions

274
00:12:22,480 --> 00:12:24,690
and kind of different sets of overlapping

275
00:12:24,690 --> 00:12:26,020
but complimentary expertise

276
00:12:26,020 --> 00:12:28,170
that are learning how to come together,

277
00:12:28,170 --> 00:12:31,010
and as a group fight this
stuff, what are the procedures?

278
00:12:31,010 --> 00:12:33,340
What are the techniques?
What are the processes?

279
00:12:33,340 --> 00:12:34,930
How do you kind of institutionalize

280
00:12:34,930 --> 00:12:37,800
that in initially kind of lightweight

281
00:12:37,800 --> 00:12:39,740
innovative ways that we
might be able to figure

282
00:12:39,740 --> 00:12:42,430
out a way to better
institutionalized and support?

283
00:12:42,430 --> 00:12:43,810
I think that's all great news,

284
00:12:43,810 --> 00:12:47,090
I think the platforms have
invested a great deal since 2016,

285
00:12:47,091 --> 00:12:48,620
and the capabilities,

286
00:12:48,620 --> 00:12:50,730
or at least the platforms
that can afford it.

287
00:12:50,730 --> 00:12:52,853
And capabilities to detect,

288
00:12:53,740 --> 00:12:55,470
you know, particularly
foreign state-sponsored

289
00:12:55,470 --> 00:12:59,310
and other kind of coordinated
manipulation campaigns.

290
00:12:59,310 --> 00:13:01,660
We have one actor being
a foreign government,

291
00:13:01,660 --> 00:13:02,589
or another entity

292
00:13:02,590 --> 00:13:05,740
that is controlling
multiple online assets,

293
00:13:05,740 --> 00:13:08,486
in order to drive some kind
of an information operation.

294
00:13:08,486 --> 00:13:11,819
I think that those moles are
getting whacked a lot faster

295
00:13:11,820 --> 00:13:13,650
than they used to and that's great news.

296
00:13:13,650 --> 00:13:15,260
At the same time, however,

297
00:13:15,260 --> 00:13:18,587
as we've just been discussing
with the COVID infodemic,

298
00:13:18,587 --> 00:13:21,353
and some of the things that are happening,

299
00:13:21,353 --> 00:13:23,410
in kind of the aftermath of the election

300
00:13:23,410 --> 00:13:25,310
with what had been the
political disinformation

301
00:13:25,310 --> 00:13:26,329
around the election,

302
00:13:26,330 --> 00:13:28,320
is that these things take
on a life of their own,

303
00:13:28,320 --> 00:13:30,920
where the thing you're
fighting is not you know,

304
00:13:30,920 --> 00:13:33,089
the Russians running a
troll farm somewhere,

305
00:13:33,090 --> 00:13:37,680
it's actually make, the most of the energy

306
00:13:37,680 --> 00:13:42,277
of that misinformation is
legitimate, you know, organic,

307
00:13:42,278 --> 00:13:45,190
people that are wrapped
up in it and believe it.

308
00:13:45,190 --> 00:13:48,800
And you know, that is a real
problem that requires some kind

309
00:13:48,800 --> 00:13:51,663
of very intelligent, targeted
solutions to help mitigate.

310
00:13:53,570 --> 00:13:56,030
- Yeah Mark, if you had five minutes

311
00:13:56,030 --> 00:13:58,579
with some really influential policymakers.

312
00:13:58,580 --> 00:14:01,010
What kinds of things might
you be recommending to them?

313
00:14:01,010 --> 00:14:03,154
How can we do this better?

314
00:14:03,154 --> 00:14:08,154
- So, you know, I think something
John picked up on speed,

315
00:14:09,010 --> 00:14:12,950
and the ability also to take a step back,

316
00:14:12,950 --> 00:14:16,283
and look at the larger influence campaign.

317
00:14:17,180 --> 00:14:21,520
So MITRES developed a couple
of different technologies,

318
00:14:21,520 --> 00:14:23,360
one of them is called CLIO

319
00:14:23,360 --> 00:14:26,930
or the common language
for influence operations,

320
00:14:26,930 --> 00:14:28,209
another one is SPICE

321
00:14:28,210 --> 00:14:33,210
the Structured Process for
Influence Campaign Evaluation.

322
00:14:33,350 --> 00:14:37,150
And using these tools
along with some others,

323
00:14:37,150 --> 00:14:40,600
the ability to see how effective,

324
00:14:40,600 --> 00:14:43,300
different responses are, and to see how

325
00:14:43,300 --> 00:14:47,563
the campaigns unfold and
how they can be disrupted,

326
00:14:49,200 --> 00:14:51,463
you know, quickly and effectively.

327
00:14:53,540 --> 00:14:54,510
- Yeah, great points Mark.

328
00:14:54,510 --> 00:14:56,439
You and I have had a
number of conversations

329
00:14:56,440 --> 00:14:57,690
over the years about,

330
00:14:57,690 --> 00:15:01,380
How to get that common
language, that common framework,

331
00:15:01,380 --> 00:15:04,060
that everybody could
latch onto for combating

332
00:15:04,060 --> 00:15:05,640
the misinformation.

333
00:15:05,640 --> 00:15:08,653
I personally feel that's a
very, very important priority.

334
00:15:09,670 --> 00:15:12,040
Bella, what do you think
are some of the best things

335
00:15:12,040 --> 00:15:14,020
that we can do going forward,

336
00:15:14,020 --> 00:15:16,810
to make ourselves stronger in this fight?

337
00:15:16,810 --> 00:15:18,040
- Right, absolutely.

338
00:15:18,040 --> 00:15:22,620
I mean, I agree with John in the idea that

339
00:15:22,620 --> 00:15:24,160
I'm really excited by the collaborations

340
00:15:24,160 --> 00:15:25,650
that are emerging in this space.

341
00:15:25,650 --> 00:15:27,829
I think it's so important to engage,

342
00:15:27,830 --> 00:15:31,190
with researchers coming at
this from all different angles,

343
00:15:31,190 --> 00:15:32,650
especially for the virality project,

344
00:15:32,650 --> 00:15:34,260
it's been super helpful to have.

345
00:15:34,260 --> 00:15:35,850
Some experts who have been looking at

346
00:15:35,850 --> 00:15:38,755
the vaccine misinformation
space for a long, long time,

347
00:15:38,755 --> 00:15:40,420
because a lot of these narratives

348
00:15:40,420 --> 00:15:41,949
and tropes are being recycled

349
00:15:41,950 --> 00:15:46,030
from previous information
operations in that space.

350
00:15:46,030 --> 00:15:48,060
So, it's so important
to continue to engage,

351
00:15:48,060 --> 00:15:49,973
especially the academic front here.

352
00:15:51,140 --> 00:15:53,210
So I think that's something
that I'm really hopeful

353
00:15:53,210 --> 00:15:55,560
and hopeful that we're
getting better at those.

354
00:15:57,650 --> 00:15:59,770
The, the biggest thing
the virality project that,

355
00:15:59,770 --> 00:16:03,069
has been different than the election,

356
00:16:03,070 --> 00:16:04,620
for me has been the lack

357
00:16:04,620 --> 00:16:08,360
of a centralized response situation,

358
00:16:08,360 --> 00:16:10,140
for the election, there was the EI-ISAC

359
00:16:10,140 --> 00:16:11,790
which we were able to collaborate with,

360
00:16:11,790 --> 00:16:13,760
in order to engage with
election officials.

361
00:16:13,760 --> 00:16:15,430
However, for vaccine disinformation

362
00:16:15,430 --> 00:16:18,589
it's really unclear who we're
supposed to be engaging with,

363
00:16:18,590 --> 00:16:20,400
to actually deliver
the counter narratives.

364
00:16:20,400 --> 00:16:22,020
So we've assembled a coalition

365
00:16:22,020 --> 00:16:23,900
of stakeholders across public health,

366
00:16:23,900 --> 00:16:25,621
government, civil society,

367
00:16:25,621 --> 00:16:27,850
but that has been a really big pain point.

368
00:16:27,850 --> 00:16:31,300
And then when we actually get
these stakeholders engaged,

369
00:16:31,300 --> 00:16:33,550
the biggest question that we're getting,

370
00:16:33,550 --> 00:16:35,130
which is actually very different again

371
00:16:35,130 --> 00:16:36,810
from the election context is,

372
00:16:36,810 --> 00:16:38,609
what do we do? What do we say?

373
00:16:38,610 --> 00:16:40,810
And when do we actually say something?

374
00:16:40,810 --> 00:16:42,869
So both the question of thresholds,

375
00:16:42,870 --> 00:16:44,890
for actually issuing counter narratives,

376
00:16:44,890 --> 00:16:47,270
as well as how to best
construct counter narratives,

377
00:16:47,270 --> 00:16:49,410
is something that we've
seen, everyone from

378
00:16:49,410 --> 00:16:52,339
the CDC to coalitions of doctors,

379
00:16:52,340 --> 00:16:54,760
to public health officials
and everyone in between,

380
00:16:54,760 --> 00:16:57,646
asking us and basically saying, you know,

381
00:16:57,647 --> 00:17:00,930
"give us a green light and a
message and we'll put it out."

382
00:17:00,930 --> 00:17:03,260
But that actual process is something that,

383
00:17:03,260 --> 00:17:05,170
we weren't expecting,

384
00:17:05,170 --> 00:17:07,810
because as researchers of disinformation,

385
00:17:07,810 --> 00:17:10,230
I think we're good at
finding the disinformation,

386
00:17:10,230 --> 00:17:11,970
and saying how is it spreading.

387
00:17:11,970 --> 00:17:15,050
But when it comes to, okay, first of all,

388
00:17:15,050 --> 00:17:17,609
it's at the threshold that
we actually need to respond.

389
00:17:17,609 --> 00:17:18,740
We can kind of answer that,

390
00:17:18,740 --> 00:17:20,380
and we're getting better at answering it.

391
00:17:20,380 --> 00:17:21,530
But then what to say and how

392
00:17:21,530 --> 00:17:23,030
to actually engage with people,

393
00:17:23,030 --> 00:17:25,685
is, is something that we
see as kind of outside

394
00:17:25,685 --> 00:17:28,129
of our scope right now, and
that we're trying to understand

395
00:17:28,130 --> 00:17:30,900
how do we, who, who's best at doing that,

396
00:17:30,900 --> 00:17:32,450
and how do we engage with them?

397
00:17:34,215 --> 00:17:35,766
- Okay go a head then.

398
00:17:35,767 --> 00:17:37,560
- I mean I would just add to that,

399
00:17:37,560 --> 00:17:38,859
that I think it's that kind

400
00:17:38,859 --> 00:17:41,870
of strategic communications
response is a real,

401
00:17:41,870 --> 00:17:43,070
what's called it an opportunity

402
00:17:43,070 --> 00:17:45,633
that's waiting for some energy applied.

403
00:17:45,633 --> 00:17:50,470
You know, so for instance,
just as, you know

404
00:17:50,470 --> 00:17:52,390
as we detected the kind of confluence

405
00:17:52,390 --> 00:17:55,230
of these different varieties
of conspiracy theories,

406
00:17:55,230 --> 00:17:58,740
you know, you have to realize
that, information travels

407
00:17:58,740 --> 00:18:00,660
in really complex online networks,

408
00:18:00,660 --> 00:18:02,840
and communities sort of have ways

409
00:18:02,840 --> 00:18:04,996
of conditioning algorithms
and relationships,

410
00:18:04,997 --> 00:18:07,083
that sort of draw in the kind of content

411
00:18:07,083 --> 00:18:09,160
that's interesting to that community.

412
00:18:09,160 --> 00:18:12,283
And so there are lots of
different audiences out there,

413
00:18:12,283 --> 00:18:13,790
that you need to respond to,

414
00:18:13,790 --> 00:18:15,540
and each of them have their
own preferred channels,

415
00:18:15,540 --> 00:18:17,690
each of them have their
own master narratives

416
00:18:17,690 --> 00:18:20,517
that you need to sort of
show some appreciation for,

417
00:18:20,517 --> 00:18:22,260
and the construction of your messaging.

418
00:18:22,260 --> 00:18:23,420
Some of those communities

419
00:18:23,420 --> 00:18:25,970
are probably lost causes quite honestly

420
00:18:25,970 --> 00:18:27,800
and other ones are persuadable.

421
00:18:27,800 --> 00:18:29,830
And being able to kind of tell those apart

422
00:18:29,830 --> 00:18:33,040
and know how to kind of target
your response appropriately

423
00:18:33,040 --> 00:18:35,572
is, is a real challenge
that it has to be met.

424
00:18:36,790 --> 00:18:39,350
- At the risk of interjecting
a little too much content

425
00:18:39,350 --> 00:18:40,810
for a moderator, I'm gonna go ahead

426
00:18:40,810 --> 00:18:42,102
and share a little story.

427
00:18:43,170 --> 00:18:46,710
We've been involved right
now, we've turned squints lens

428
00:18:46,710 --> 00:18:49,250
for crowdsourcing onto the COVID problem.

429
00:18:49,250 --> 00:18:52,420
And that's, you know it's pulling
in all kinds of craziness.

430
00:18:52,420 --> 00:18:55,480
And, and one of the things
that's really plagued us,

431
00:18:55,480 --> 00:18:57,680
is that people in the crowd

432
00:18:57,680 --> 00:18:59,620
for plant it's a crowdsourced app,

433
00:18:59,620 --> 00:19:01,227
have been saying to us, "you know,

434
00:19:01,228 --> 00:19:03,740
I was getting all kinds of misinformation,

435
00:19:03,740 --> 00:19:07,480
on this Facebook page, or this
other page that I belong to."

436
00:19:07,480 --> 00:19:10,350
And all of a sudden that has disappeared,

437
00:19:10,350 --> 00:19:13,020
and it says," you know on
this page where I used to go,

438
00:19:13,020 --> 00:19:15,340
it says if you want to
join this conversation,

439
00:19:15,340 --> 00:19:16,699
come join me over at,"

440
00:19:16,700 --> 00:19:19,120
and then it names some
alternative platform,

441
00:19:19,120 --> 00:19:21,409
and, you know, an alternative link.

442
00:19:21,410 --> 00:19:24,930
And those are platforms where
people don't necessarily

443
00:19:24,930 --> 00:19:26,060
want to go to, right?

444
00:19:26,060 --> 00:19:29,360
It's not all platforms are created equal.

445
00:19:29,360 --> 00:19:31,740
But sometimes, what we're seeing is that

446
00:19:31,740 --> 00:19:34,510
the information you need to be aware of,

447
00:19:34,510 --> 00:19:37,314
is shifting locations quite a bit.

448
00:19:37,314 --> 00:19:40,660
So in addition to figuring
out how to communicate

449
00:19:40,660 --> 00:19:43,970
the misinformation, with
the right stakeholders,

450
00:19:43,970 --> 00:19:47,220
we actually have to figure out,
where is the misinformation,

451
00:19:47,220 --> 00:19:49,803
so that we actually can,
can take that pulse.

452
00:19:51,090 --> 00:19:53,189
I wanna, I wanna, yeah, go ahead, John.

453
00:19:53,190 --> 00:19:56,900
- I would say my, I completely
appreciate your observation,

454
00:19:56,900 --> 00:20:00,260
and I think that, you know,
again, it ties into to the point

455
00:20:00,260 --> 00:20:01,700
I was just trying to make, which is that

456
00:20:01,700 --> 00:20:05,020
there are different audiences
with different, you know,

457
00:20:05,020 --> 00:20:07,150
response objectives in different places.

458
00:20:07,150 --> 00:20:09,260
And, you know, so some of those people

459
00:20:09,260 --> 00:20:10,610
are migrated communities where

460
00:20:10,610 --> 00:20:12,810
they don't want to be
counter-influenced, you know,

461
00:20:12,810 --> 00:20:14,179
they're they're trying to go there

462
00:20:14,180 --> 00:20:16,670
to avoid the counter influence,

463
00:20:16,670 --> 00:20:18,910
and, and other folks are
kind of being reached

464
00:20:18,910 --> 00:20:21,840
on mainstream platforms
where there's, you know,

465
00:20:21,840 --> 00:20:24,060
a much better ability to kind of go

466
00:20:24,060 --> 00:20:26,690
and target them with better information.

467
00:20:26,690 --> 00:20:27,600
But what do you do?

468
00:20:27,600 --> 00:20:29,669
You know, when you have
a kind of, you know,

469
00:20:29,670 --> 00:20:33,350
as we go the kind of Internet
Commons gets divided,

470
00:20:33,350 --> 00:20:35,050
into more and more little sub networks,

471
00:20:35,050 --> 00:20:37,070
and micro platforms where,

472
00:20:37,070 --> 00:20:39,770
people kind of self-segregate
into them based on,

473
00:20:39,770 --> 00:20:41,770
their kind of sociopolitical
characteristics.

474
00:20:41,770 --> 00:20:44,740
And then do you, how
do you deal with that?

475
00:20:44,740 --> 00:20:45,573
And particularly some

476
00:20:45,573 --> 00:20:49,130
of these platforms don't
have, effective policies

477
00:20:49,130 --> 00:20:51,880
or even any policies that are
trying to prevent this stuff.

478
00:20:51,880 --> 00:20:55,700
I mean, there are active
identified Russian personas active

479
00:20:55,700 --> 00:20:58,670
on some of these platforms
and the platforms have said,

480
00:20:58,670 --> 00:21:00,080
they're not even gonna
try and shut them down.

481
00:21:00,080 --> 00:21:02,689
And they're out there
messaging right now. (chuckles)

482
00:21:02,690 --> 00:21:05,460
- Exactly, there's such a, such a range

483
00:21:05,460 --> 00:21:09,610
of policies and procedures,
across the platform space.

484
00:21:09,610 --> 00:21:11,550
Mark, I wanted to turn to you on,

485
00:21:11,550 --> 00:21:14,750
on a question that comes up frequently,

486
00:21:14,750 --> 00:21:17,990
and it's, what is the relationship between

487
00:21:17,990 --> 00:21:21,380
and the distinctions between,

488
00:21:21,380 --> 00:21:25,192
misinformation and influence,
and how, why does that matter

489
00:21:25,192 --> 00:21:28,830
and how should we be thinking about that?

490
00:21:28,830 --> 00:21:31,832
- Sure, so misinformation,

491
00:21:33,410 --> 00:21:35,693
is incorrect information.

492
00:21:36,730 --> 00:21:40,080
You know, you can, some
people conflate misinformation

493
00:21:40,080 --> 00:21:41,820
and disinformation where,

494
00:21:41,820 --> 00:21:46,060
one has an intent and
one may not have intent,

495
00:21:46,060 --> 00:21:50,020
but there's more than just telling lies,

496
00:21:50,020 --> 00:21:52,483
or stretching the truth.

497
00:21:53,920 --> 00:21:57,880
Influence involves all sorts
of different activities.

498
00:21:57,880 --> 00:22:02,800
It can be making sure a true
story gets much more attention.

499
00:22:02,800 --> 00:22:05,440
So for example, as we
were talking about earlier

500
00:22:05,440 --> 00:22:08,840
with COVID and blood clots,

501
00:22:08,840 --> 00:22:11,379
you know, playing up that story

502
00:22:11,380 --> 00:22:14,979
to create fear that you're gonna have,

503
00:22:14,979 --> 00:22:18,230
negative health consequences occur,

504
00:22:18,230 --> 00:22:22,330
because you got the vaccine, is influence.

505
00:22:22,330 --> 00:22:25,800
It's not disinformation or misinformation,

506
00:22:25,800 --> 00:22:28,830
because it's true that a
small number of people,

507
00:22:28,830 --> 00:22:30,639
have gotten blood clots.

508
00:22:30,640 --> 00:22:33,670
It's also true that odds are,

509
00:22:33,670 --> 00:22:36,020
you're gonna have a more likely

510
00:22:36,020 --> 00:22:39,710
to get a blood clot from COVID than not.

511
00:22:39,710 --> 00:22:44,710
So by isolating a truth,

512
00:22:45,400 --> 00:22:48,650
and sharing only that portion of it,

513
00:22:48,650 --> 00:22:51,660
you're creating influence
and, and, you know.

514
00:22:51,660 --> 00:22:56,660
Influence ultimately is about
changing cognitive behaviors,

515
00:22:57,610 --> 00:23:00,949
and eventually, you know,

516
00:23:00,950 --> 00:23:03,740
if it's an effective influence campaign,

517
00:23:03,740 --> 00:23:08,530
results in people, you
know, being persuaded

518
00:23:08,530 --> 00:23:11,197
and motivated to go out
and do stuff, you know,

519
00:23:11,198 --> 00:23:12,980
in the case of vaccines,

520
00:23:12,980 --> 00:23:16,850
it's not get a vaccine, or
in the case of elections,

521
00:23:16,850 --> 00:23:20,023
it may be to not vote,
or to vote differently.

522
00:23:21,170 --> 00:23:25,840
So, you know, understanding
that influence is much larger,

523
00:23:25,840 --> 00:23:28,387
than just missing
disinformation, is critical

524
00:23:28,387 --> 00:23:33,387
in order to be able to
stop malign influence.

525
00:23:35,700 --> 00:23:37,900
- Thoughts from others
about the difference

526
00:23:37,900 --> 00:23:39,790
in the role that influence plays

527
00:23:39,790 --> 00:23:42,240
as opposed to straight-up
this information.

528
00:23:43,850 --> 00:23:46,669
- Well, kind of my
thought on that, you know,

529
00:23:46,670 --> 00:23:49,760
is as Mark was taught, was talking is that

530
00:23:49,760 --> 00:23:52,070
you've got sort of two
different, two different things

531
00:23:52,070 --> 00:23:54,129
that are at stake and kind
of a flow of information,

532
00:23:54,130 --> 00:23:55,897
and 21st century online networks, right.

533
00:23:55,897 --> 00:23:57,732
One is this sort of,

534
00:23:57,732 --> 00:24:00,478
the traditional American
social scientific way

535
00:24:00,478 --> 00:24:02,860
of studying influence on, you know

536
00:24:02,860 --> 00:24:05,729
it's very kind of psychology
oriented approach.

537
00:24:05,730 --> 00:24:07,880
Which is this information
you expose somebody,

538
00:24:07,880 --> 00:24:10,020
what's the effect of that?
Is there a priming effect,

539
00:24:10,020 --> 00:24:12,540
is there one of a
multitude of other effects?

540
00:24:12,540 --> 00:24:14,144
Does it persuade somebody,
do they change their mind?

541
00:24:14,144 --> 00:24:16,810
You know, what are the
kind of cognitive effects

542
00:24:16,810 --> 00:24:19,190
of that communicative exposure.

543
00:24:19,190 --> 00:24:20,770
But what happens more and more

544
00:24:20,770 --> 00:24:22,970
in the 21st century than
it did kind of in the age

545
00:24:22,970 --> 00:24:23,990
of broadcast,

546
00:24:23,990 --> 00:24:28,790
is that these flows of information
actually form communities

547
00:24:28,790 --> 00:24:30,710
that become the repetitive,

548
00:24:30,710 --> 00:24:34,665
the basis of the repetitive
cycling of future information.

549
00:24:34,665 --> 00:24:37,780
And as Mark quite rightly
pointed out, you know,

550
00:24:37,780 --> 00:24:39,470
and as sometimes, you know,

551
00:24:39,470 --> 00:24:42,230
is kind of known by
propaganda researchers.

552
00:24:42,230 --> 00:24:44,290
I mean, the most effective propaganda

553
00:24:44,290 --> 00:24:46,070
is often just completely true stuff,

554
00:24:46,070 --> 00:24:47,760
presented in a certain way,

555
00:24:47,760 --> 00:24:50,070
in a certain framing at a certain time.

556
00:24:50,070 --> 00:24:52,899
And so the formation of online networks

557
00:24:52,900 --> 00:24:55,210
and community structures
where people come together,

558
00:24:55,210 --> 00:24:57,420
maybe you know, I was interested in yoga

559
00:24:57,420 --> 00:24:59,540
and you're interested in a
certain strand of politics,

560
00:24:59,540 --> 00:25:01,030
but now here we are together,

561
00:25:01,030 --> 00:25:04,060
adjacent to one another in network space,

562
00:25:04,060 --> 00:25:06,260
in the future receiving position

563
00:25:06,260 --> 00:25:08,910
of a lot of common
information in the future,

564
00:25:08,910 --> 00:25:09,970
it's sort of the, there's

565
00:25:09,970 --> 00:25:12,550
there's the impact of
any particular message,

566
00:25:12,550 --> 00:25:14,010
we're just kind of the micro level.

567
00:25:14,010 --> 00:25:16,623
And then there's the community formation,

568
00:25:17,750 --> 00:25:21,733
that is in a way, the longer
term, more challenging issue.

569
00:25:23,960 --> 00:25:26,654
- So Bella, when we think about influence,

570
00:25:26,654 --> 00:25:29,850
and the relationship of
influence, to narratives.

571
00:25:29,850 --> 00:25:31,370
What are your thoughts on that?

572
00:25:31,370 --> 00:25:33,100
Is it a way to detect influence

573
00:25:33,100 --> 00:25:36,100
when we're tracking narratives
or do we need something more?

574
00:25:36,950 --> 00:25:39,560
- Right, I mean, I would,

575
00:25:39,560 --> 00:25:42,960
I think I would, reiterate
what John just just said here

576
00:25:42,960 --> 00:25:47,030
I think the idea that we have vehicles,

577
00:25:47,030 --> 00:25:50,250
of influence already
constructed on these platforms,

578
00:25:50,250 --> 00:25:53,600
where very, very quickly information

579
00:25:53,600 --> 00:25:56,730
whether true or not true can be dispersed,

580
00:25:56,730 --> 00:25:59,131
and really with this veneer of authority.

581
00:25:59,131 --> 00:26:00,727
A lot of these users and communities,

582
00:26:00,727 --> 00:26:05,727
use some platform features,
such as being verified,

583
00:26:05,880 --> 00:26:07,950
having a lot of followers,
getting a lot of retweets,

584
00:26:07,950 --> 00:26:12,140
all of these things, which
can lead to this veneer of,

585
00:26:12,140 --> 00:26:13,850
okay, this information might be true,

586
00:26:13,850 --> 00:26:17,179
a lot of people agree here, et cetera.

587
00:26:17,180 --> 00:26:21,360
So thinking about these are
structures that already exist

588
00:26:21,360 --> 00:26:25,169
on the platforms, and so if
we just think about, you know,

589
00:26:25,170 --> 00:26:28,040
with a blind eye towards, who
is sharing this information?

590
00:26:28,040 --> 00:26:30,250
what kind of authority do
they have in the communities

591
00:26:30,250 --> 00:26:32,110
that they're a part of?

592
00:26:32,110 --> 00:26:34,570
And, and how do those communities
think about authenticity

593
00:26:34,570 --> 00:26:38,550
in the information space,
without taking that into account,

594
00:26:38,550 --> 00:26:40,889
when they're thinking about
the narratives themselves,

595
00:26:40,890 --> 00:26:42,788
we won't really be able to deal
with the narratives, right?

596
00:26:42,788 --> 00:26:45,000
And then going back again to the point

597
00:26:45,000 --> 00:26:47,410
of these communities have
preexisting conditions

598
00:26:47,410 --> 00:26:49,330
that we need to take into
account when we're thinking

599
00:26:49,330 --> 00:26:51,470
about actually debunking
the narratives, right?

600
00:26:51,470 --> 00:26:54,240
Like what are the, the, what
is the overall foundation

601
00:26:54,240 --> 00:26:55,860
where this infrastructure
was built on, right?

602
00:26:55,860 --> 00:26:57,966
What are the pre-existing
beliefs in this community?

603
00:26:57,967 --> 00:26:59,910
Who they trust and who they not trust?

604
00:26:59,910 --> 00:27:01,690
So when it comes to
this idea of influence,

605
00:27:01,690 --> 00:27:04,710
that's really also what
I think about is kind

606
00:27:04,710 --> 00:27:08,070
of the entire ecosystem where
this narrative is living,

607
00:27:08,070 --> 00:27:10,669
and how that affects how
the narrative is perceived.

608
00:27:12,010 --> 00:27:13,580
- Oh, you all three are pointing at,

609
00:27:13,580 --> 00:27:15,330
where we kind of need to go

610
00:27:15,330 --> 00:27:17,449
to grapple with some of these issues.

611
00:27:17,450 --> 00:27:19,080
And I we're running out of time,

612
00:27:19,080 --> 00:27:20,970
but I wanna ask each of you.

613
00:27:20,970 --> 00:27:22,720
What do you think are the goals,

614
00:27:22,720 --> 00:27:24,740
with the next level of technology?

615
00:27:24,740 --> 00:27:26,640
What's just at the horizon

616
00:27:26,640 --> 00:27:30,170
of the possible or the
impossible right now?

617
00:27:30,170 --> 00:27:31,003
Mark.

618
00:27:32,110 --> 00:27:32,943
- So

619
00:27:34,500 --> 00:27:35,720
I think,

620
00:27:35,720 --> 00:27:37,266
you know,

621
00:27:37,266 --> 00:27:38,099
we're

622
00:27:38,099 --> 00:27:42,110
for a long time the technology
has been focused on,

623
00:27:42,110 --> 00:27:46,803
trying to detect, you know,
missing disinformation,

624
00:27:48,080 --> 00:27:51,510
but really where we need to be focused,

625
00:27:51,510 --> 00:27:56,510
is trying to disrupt the flow
of missing disinformation,

626
00:27:58,470 --> 00:28:01,840
trying to break down the isolation

627
00:28:01,840 --> 00:28:06,110
of communities that form
these sounding chambers.

628
00:28:06,110 --> 00:28:09,939
And being able to evaluate

629
00:28:09,940 --> 00:28:13,833
how well various
different techniques work,

630
00:28:14,750 --> 00:28:19,420
is, is key, you know and
so having technologies

631
00:28:19,420 --> 00:28:23,430
that allow you to do that in
real time or near real time,

632
00:28:23,430 --> 00:28:26,120
is where we're on the cusp of.

633
00:28:26,120 --> 00:28:28,770
And where I think we need to go.

634
00:28:28,770 --> 00:28:29,603
- John what do you think

635
00:28:29,603 --> 00:28:32,673
about the next set of
technologies and goals?

636
00:28:33,860 --> 00:28:37,139
- Well, as well forgive me
I'm gonna mention something

637
00:28:37,140 --> 00:28:39,980
you and I were just talking
about the other day,

638
00:28:39,980 --> 00:28:41,450
which, you know I think that

639
00:28:41,450 --> 00:28:45,190
there's a lot of fruitful work to be done,

640
00:28:45,190 --> 00:28:46,860
at the intersection of what you guys

641
00:28:46,860 --> 00:28:49,459
at MITRE have been doing in
terms of kind of detailing

642
00:28:49,460 --> 00:28:50,640
figuring out how to kind of detail

643
00:28:50,640 --> 00:28:53,600
and specify some of
this stuff with the kind

644
00:28:53,600 --> 00:28:56,350
of stuff that Isabella has been learning,

645
00:28:56,350 --> 00:28:58,939
you know, and and sort
of helping to teach me

646
00:28:58,940 --> 00:29:01,910
about what is they're
seeing in the trenches

647
00:29:01,910 --> 00:29:03,100
in these kinds of partnerships

648
00:29:03,100 --> 00:29:04,889
that are looking at just
tons and tons of this stuff,

649
00:29:04,890 --> 00:29:06,640
which is around narratives and flow.

650
00:29:06,640 --> 00:29:09,482
And so, you know, at Graphica
we've got fantastic technology

651
00:29:09,482 --> 00:29:12,330
for kind of understanding the structure

652
00:29:12,330 --> 00:29:13,163
of these online networks

653
00:29:13,163 --> 00:29:14,960
and how the information
is flowing through it

654
00:29:14,960 --> 00:29:17,077
and kind of where the
foci of influence are

655
00:29:17,077 --> 00:29:19,400
for all these different communities.

656
00:29:19,400 --> 00:29:22,510
But there's so much data
pouring through that network,

657
00:29:22,510 --> 00:29:24,657
and to sort of find automated ways

658
00:29:24,657 --> 00:29:27,909
or algorithmic ways of helping to build,

659
00:29:27,910 --> 00:29:30,640
the, discover the
narratives and understand

660
00:29:30,640 --> 00:29:33,530
the relationships with them
by analyzing that content,

661
00:29:33,530 --> 00:29:35,580
I think is really really
powerful because right now,

662
00:29:35,580 --> 00:29:37,750
you know we can enable a team of humans

663
00:29:37,750 --> 00:29:40,040
to kind of sit there and look
at everything pouring through,

664
00:29:40,040 --> 00:29:42,270
you know, Q Anon cluster two,

665
00:29:42,270 --> 00:29:43,440
and describe it,

666
00:29:43,440 --> 00:29:45,300
but they still have to
read it all you know.

667
00:29:45,300 --> 00:29:46,760
- Right, right.

668
00:29:46,760 --> 00:29:47,970
So Bella, you know,

669
00:29:47,970 --> 00:29:50,111
at the risk of knowing that
John stole your thunder,

670
00:29:50,111 --> 00:29:52,000
I wonder what your thoughts are.

671
00:29:52,000 --> 00:29:54,030
If you could wave the magic one,

672
00:29:54,030 --> 00:29:55,242
where would we be on the next set

673
00:29:55,242 --> 00:29:59,600
of technological capabilities
in this space in two years?

674
00:29:59,600 --> 00:30:00,563
- Yeah, I mean, yeah, this is,

675
00:30:00,564 --> 00:30:05,564
I think the crux of the
crux of the issue here,

676
00:30:06,150 --> 00:30:08,530
and something that we
haven't touched on as much,

677
00:30:08,530 --> 00:30:10,407
is kind of this idea of
the cross platform nature,

678
00:30:10,407 --> 00:30:12,810
of these narratives, right?

679
00:30:12,810 --> 00:30:14,080
And so it's something

680
00:30:14,080 --> 00:30:16,610
that's really really
human expensive right now,

681
00:30:16,610 --> 00:30:18,238
both to understand how narratives

682
00:30:18,239 --> 00:30:20,700
are coming up on different platforms.

683
00:30:20,700 --> 00:30:22,290
And also even on the same platform

684
00:30:22,290 --> 00:30:24,540
understanding that two pieces of context,

685
00:30:24,540 --> 00:30:26,659
one which might be talking
about miscarriages,

686
00:30:26,660 --> 00:30:29,130
one about infertility from the vaccine,

687
00:30:29,130 --> 00:30:32,970
are actually embedded in
the same overall framing.

688
00:30:32,970 --> 00:30:35,849
And this is extremely
human expensive right now,

689
00:30:35,849 --> 00:30:37,320
to give a bit of context,

690
00:30:37,320 --> 00:30:38,750
for the Election Integrity Partnership,

691
00:30:38,750 --> 00:30:41,740
we had the ability to hire 30 students,

692
00:30:41,740 --> 00:30:45,970
which created the bedrock of
the actual partnership itself,

693
00:30:45,970 --> 00:30:48,590
and being from a computer
science background myself,

694
00:30:48,590 --> 00:30:51,780
I immediately said, let's hire
30 computer science students.

695
00:30:51,780 --> 00:30:54,050
They know how to code, they
know how to look at data,

696
00:30:54,050 --> 00:30:55,690
and it'll be great,

697
00:30:55,690 --> 00:30:58,318
ended up being completely
completely wrong there.

698
00:30:58,318 --> 00:31:02,600
We ended up hiring a mix of
about 20 technical backgrounds,

699
00:31:02,600 --> 00:31:04,889
and 10 non-technical backgrounds.

700
00:31:04,890 --> 00:31:08,210
And we found that the people
who had that research ability,

701
00:31:08,210 --> 00:31:10,010
maybe from a political science background,

702
00:31:10,010 --> 00:31:11,258
international relations, anything

703
00:31:11,258 --> 00:31:14,390
in the social sciences
did far, far better,

704
00:31:14,390 --> 00:31:17,030
especially at the beginning,
everyone really got trained up,

705
00:31:17,030 --> 00:31:18,379
but at the beginning they were able

706
00:31:18,380 --> 00:31:21,090
to pick up the skills a lot more quickly.

707
00:31:21,090 --> 00:31:22,980
Because it's just sometimes isn't about,

708
00:31:22,980 --> 00:31:23,995
what the numbers are saying
or what the data say.

709
00:31:23,995 --> 00:31:28,469
And, you know, we can,
we can get there, right.

710
00:31:28,470 --> 00:31:30,570
But just pulling the
data from CrowdTangle,

711
00:31:30,570 --> 00:31:33,669
isn't gonna get you to
really understanding,

712
00:31:33,670 --> 00:31:36,530
how are people looking
at the different posts.

713
00:31:36,530 --> 00:31:37,850
How does this meme interact?

714
00:31:37,850 --> 00:31:39,010
with this video on TikTok,

715
00:31:39,010 --> 00:31:40,500
interact with this tweet,

716
00:31:40,500 --> 00:31:42,983
and pulling all that together is really

717
00:31:42,983 --> 00:31:45,629
really expensive right now
for a team of analysts.

718
00:31:45,630 --> 00:31:48,300
There's ways that we can amplify
the work of these analysts,

719
00:31:48,300 --> 00:31:50,190
but it all comes down to,

720
00:31:50,190 --> 00:31:53,030
unifying what's happening
across different platforms.

721
00:31:53,030 --> 00:31:54,410
And also thinking about

722
00:31:54,410 --> 00:31:57,360
how do we really understand
narratives in a moral.

723
00:31:58,670 --> 00:32:00,960
- Yeah., and I would just add to that,

724
00:32:00,960 --> 00:32:03,510
that I think it's right, that,
that kind of sophistication

725
00:32:03,510 --> 00:32:06,010
of the called the non-technical analysts,

726
00:32:06,010 --> 00:32:08,940
the the person who can sort of read

727
00:32:08,940 --> 00:32:11,530
the material that a community
exposure is exposed to,

728
00:32:11,530 --> 00:32:13,740
with a kind of empathetic understanding

729
00:32:13,740 --> 00:32:16,510
of the way it's received in
that community is so critical.

730
00:32:16,510 --> 00:32:17,970
Particularly as we develop tools,

731
00:32:17,970 --> 00:32:19,960
I mean, at Graphica, you know,

732
00:32:19,960 --> 00:32:21,710
we've done some interesting work,

733
00:32:21,710 --> 00:32:24,360
on trying to look at how
different types of classifiers,

734
00:32:24,360 --> 00:32:26,187
for things like toxicity,
in conversations,

735
00:32:26,188 --> 00:32:30,750
are not equally valid across
different online communities.

736
00:32:30,750 --> 00:32:32,490
I mean, a sort of toxicity measure

737
00:32:32,490 --> 00:32:34,130
that might work really well,

738
00:32:34,130 --> 00:32:36,920
on one type of community
and identify sort of the,

739
00:32:36,920 --> 00:32:40,330
the more negative conversation will fail,

740
00:32:40,330 --> 00:32:41,909
completely with a
different group of people

741
00:32:41,910 --> 00:32:43,180
that it wasn't trained on.

742
00:32:43,180 --> 00:32:45,977
And to sort of believe in
the creation of these tools,

743
00:32:45,977 --> 00:32:47,840
that sort of one size fits all

744
00:32:47,840 --> 00:32:50,810
without any understanding
of the individual

745
00:32:50,810 --> 00:32:53,399
kind of perspectives and values

746
00:32:53,400 --> 00:32:55,270
of the individual communities.

747
00:32:55,270 --> 00:32:56,853
You have to include that part.

748
00:32:58,977 --> 00:33:03,050
- Well, it is now time for
us to start to wrap up,

749
00:33:03,050 --> 00:33:07,280
and I'd like to just go around
and ask each of you for some.

750
00:33:07,280 --> 00:33:10,040
What are your closing
thoughts and perspectives on,

751
00:33:10,040 --> 00:33:12,450
on the issues that we
face in misinformation

752
00:33:12,450 --> 00:33:15,470
and the best advice that you have for,

753
00:33:15,470 --> 00:33:17,120
for policymakers and big thinkers

754
00:33:17,120 --> 00:33:19,449
and technologists going forward?

755
00:33:19,450 --> 00:33:20,370
Mark, what are your,

756
00:33:20,370 --> 00:33:23,053
what are your final words
of wisdom here tonight?

757
00:33:24,280 --> 00:33:26,250
- You know, I think
this is, is gonna touch

758
00:33:26,250 --> 00:33:29,210
on something that both
Isabella and John is,

759
00:33:29,210 --> 00:33:31,350
is we need a common language.

760
00:33:31,350 --> 00:33:34,500
We need to be able to share data,

761
00:33:34,500 --> 00:33:36,380
independent of source,

762
00:33:36,380 --> 00:33:39,920
and use it in a variety
of different tools,

763
00:33:39,920 --> 00:33:41,390
to perform the analysis.

764
00:33:41,390 --> 00:33:43,560
Whether that's by computer scientists,

765
00:33:43,560 --> 00:33:46,260
social scientists, or anyone else

766
00:33:48,150 --> 00:33:48,983
- John.

767
00:33:51,019 --> 00:33:53,050
- Well, sort of talked about what I think

768
00:33:53,050 --> 00:33:55,960
is the most exciting area in
the technology side of it.

769
00:33:55,960 --> 00:33:58,200
I'd say that my kind
of point I always like

770
00:33:58,200 --> 00:34:01,650
to make is that we get
sort of really scared

771
00:34:01,650 --> 00:34:03,330
when we're trying to
grapple with this space ,

772
00:34:03,330 --> 00:34:04,360
of things that you know,

773
00:34:04,360 --> 00:34:06,820
we ought to be extremely respectful of,

774
00:34:06,820 --> 00:34:09,223
like the first amendment and privacy,

775
00:34:09,223 --> 00:34:13,360
but we we have to grapple with those,

776
00:34:13,360 --> 00:34:14,540
those real things in a way

777
00:34:14,540 --> 00:34:16,500
that still lets us get some
traction on the problem.

778
00:34:16,500 --> 00:34:18,650
And I think that one of
the key ways to do that,

779
00:34:18,650 --> 00:34:21,710
is to be able to tell the you know,

780
00:34:21,710 --> 00:34:24,530
make a clear distinction
between online speech

781
00:34:24,530 --> 00:34:26,260
as it's protected by the first amendment

782
00:34:26,260 --> 00:34:27,679
which as it ought to be,

783
00:34:27,679 --> 00:34:29,770
and those protections ought to be robust.

784
00:34:29,770 --> 00:34:33,630
And, you know, you know,
err on the side of generous,

785
00:34:33,630 --> 00:34:34,870
but at the same time, you know

786
00:34:34,870 --> 00:34:37,029
the first amendment doesn't
protect criminal conspiracy,

787
00:34:37,030 --> 00:34:38,800
it doesn't protect fighting words,

788
00:34:38,800 --> 00:34:40,302
it doesn't protect yelling fire
in a crowded movie theater.

789
00:34:40,302 --> 00:34:41,780
There are all kinds of things

790
00:34:41,780 --> 00:34:44,170
that the law is fairly well settled on,

791
00:34:44,170 --> 00:34:46,100
that it doesn't cover, and yet we have

792
00:34:46,100 --> 00:34:48,810
a lot of things like
manipulative troll farms

793
00:34:48,810 --> 00:34:50,875
that set up thousands of
fake accounts, you know,

794
00:34:50,876 --> 00:34:54,190
pretending to be real
people that they are not,

795
00:34:54,190 --> 00:34:55,053
being you know, that are out there

796
00:34:55,053 --> 00:34:58,153
to manipulate real audiences
which is a type of fraud.

797
00:34:58,153 --> 00:35:00,957
They're doing it for money,
they're getting paid to do it.

798
00:35:00,957 --> 00:35:03,650
The people that do this and
that's commercial fraud.

799
00:35:03,650 --> 00:35:05,480
That ought to be something
you go to jail for,

800
00:35:05,480 --> 00:35:08,060
not something you can actually,
you know, by a beach house,

801
00:35:08,060 --> 00:35:10,883
by running a political
consultancy that does.

802
00:35:11,810 --> 00:35:14,363
- Yeah, I totally agree.

803
00:35:14,363 --> 00:35:15,196
Bella.

804
00:35:16,527 --> 00:35:20,040
- For me, I think my biggest
hope kind of moving forward,

805
00:35:20,040 --> 00:35:22,968
is we're really realizing how important,

806
00:35:22,968 --> 00:35:24,696
online conversations are,

807
00:35:24,696 --> 00:35:27,790
to the safety of our
critical infrastructure.

808
00:35:27,790 --> 00:35:29,450
And it shouldn't be the case

809
00:35:29,450 --> 00:35:31,460
that we are starting coalitions,

810
00:35:31,460 --> 00:35:33,929
108 days out of an election.

811
00:35:33,929 --> 00:35:38,090
We are still kind of running
on, on fumes from that,

812
00:35:38,090 --> 00:35:41,430
and now doing the same thing
for vaccine disinformation.

813
00:35:41,430 --> 00:35:43,299
These are huge, huge problems,

814
00:35:43,300 --> 00:35:46,130
which lead to large protests
outside of polling sites,

815
00:35:46,130 --> 00:35:49,540
and danger for people who
run our actual elections,

816
00:35:49,540 --> 00:35:54,259
and distribution sites for
vaccines of becoming unsafe.

817
00:35:54,260 --> 00:35:56,700
So these online conversations really

818
00:35:56,700 --> 00:35:59,270
do have offline repercussions.

819
00:35:59,270 --> 00:36:01,791
And I think that I love,

820
00:36:01,791 --> 00:36:03,640
I love what we do.

821
00:36:03,640 --> 00:36:06,730
I love the collaboration
between academics,

822
00:36:06,730 --> 00:36:10,800
and think tanks and
companies like Graphika,

823
00:36:10,800 --> 00:36:12,900
but we shouldn't be running around kind

824
00:36:12,900 --> 00:36:14,720
of begging for money to fund this.

825
00:36:14,720 --> 00:36:17,870
And we should have a more sustainable way,

826
00:36:17,870 --> 00:36:21,150
especially for the people who protect us,

827
00:36:21,150 --> 00:36:23,030
in this kind of critical
infrastructure space,

828
00:36:23,030 --> 00:36:25,920
to understand these
narratives and you know,

829
00:36:25,920 --> 00:36:28,250
really keeping in keeping
in mind first protection,

830
00:36:28,250 --> 00:36:29,550
first amendment protections,

831
00:36:29,550 --> 00:36:31,234
but just to understand what's going on.

832
00:36:31,234 --> 00:36:33,563
So it's not such an uneven playing field.

833
00:36:34,640 --> 00:36:35,473
- Thank you, Bella.

834
00:36:35,473 --> 00:36:37,240
And thank you all, thank you, Mark,

835
00:36:37,240 --> 00:36:38,890
thank you, John, thank you Bella.

836
00:36:38,890 --> 00:36:40,210
For an excellent discussion,

837
00:36:40,210 --> 00:36:43,460
I know we just touched
the tip of the iceberg,

838
00:36:43,460 --> 00:36:47,303
but I now looking forward
to our Q&A session.

839
00:36:47,303 --> 00:36:48,582
Thank you.

840
00:36:48,582 --> 00:36:50,582
- Thank you.
- Thank you.

