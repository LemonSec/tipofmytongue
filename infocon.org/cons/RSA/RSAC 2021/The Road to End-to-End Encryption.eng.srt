1
00:00:01,050 --> 00:00:02,403
- My name is Max,

2
00:00:03,320 --> 00:00:06,340
and I'm head of security
engineering at Zoom.

3
00:00:06,340 --> 00:00:07,470
I'm here to talk to you today

4
00:00:07,470 --> 00:00:10,180
about how we built in
end-to-end encryption

5
00:00:10,180 --> 00:00:11,713
to Zoom's meeting product.

6
00:00:14,340 --> 00:00:17,560
In 2020, we published
a white paper on GitHub

7
00:00:17,560 --> 00:00:19,520
that describes our phased approach

8
00:00:19,520 --> 00:00:21,740
for introducing end-to-end encryption

9
00:00:21,740 --> 00:00:24,740
and identity into Zoom's meetings.

10
00:00:24,740 --> 00:00:26,645
We got a lot of great
feedback on that paper

11
00:00:26,645 --> 00:00:29,313
from the community, so thank
you very much for that.

12
00:00:30,200 --> 00:00:32,280
A few months later, we had a prototype

13
00:00:32,280 --> 00:00:34,320
and this is a picture of the first ever

14
00:00:34,320 --> 00:00:35,850
end-to-end encrypted Zoom meeting,

15
00:00:35,850 --> 00:00:38,470
which I was lucky enough to be a part of.

16
00:00:38,470 --> 00:00:41,010
In October, 2020, we released to the world

17
00:00:41,010 --> 00:00:42,530
the first version of the Zoom client

18
00:00:42,530 --> 00:00:44,160
that had end-to-end encryption,

19
00:00:44,160 --> 00:00:47,610
which is available to both
paid and free users alike

20
00:00:47,610 --> 00:00:49,623
so we recommend you try it out today.

21
00:00:52,270 --> 00:00:55,560
Today I wanna talk to you about
what it took to get there.

22
00:00:55,560 --> 00:00:58,130
So the first thing I wanted
to talk a little bit about

23
00:00:58,130 --> 00:00:59,860
is what's the landscape out there

24
00:00:59,860 --> 00:01:01,830
for video conferencing security?

25
00:01:01,830 --> 00:01:04,220
What are other people doing in the field?

26
00:01:04,220 --> 00:01:05,640
I also want to mention briefly

27
00:01:05,640 --> 00:01:08,580
what encryption looked like
at Zoom before end-to-end,

28
00:01:08,580 --> 00:01:11,210
how we overcame challenges
to offer end-to-end,

29
00:01:11,210 --> 00:01:14,023
and what features we intend
to offer in the future.

30
00:01:16,750 --> 00:01:18,100
So, what's out there?

31
00:01:18,100 --> 00:01:21,419
The most basic setup for video
conferencing is as follows,

32
00:01:21,420 --> 00:01:23,900
Alice and Bob connect to a server

33
00:01:23,900 --> 00:01:27,630
via something like a TLS
protected connection,

34
00:01:27,630 --> 00:01:30,390
and they're going to exchange
video conferencing data

35
00:01:30,390 --> 00:01:32,410
whether audio or visual screenshare

36
00:01:32,410 --> 00:01:33,577
through some common server,

37
00:01:33,577 --> 00:01:36,780
and this could be a cloud
hosted server in a lot of cases.

38
00:01:36,780 --> 00:01:39,510
And so I say, Alice and
Bob are using TLS here

39
00:01:39,510 --> 00:01:42,040
because it affords them
a lot of protections.

40
00:01:42,040 --> 00:01:43,830
It protects them against their ISPs

41
00:01:43,830 --> 00:01:46,480
or their local coffee shop

42
00:01:46,480 --> 00:01:48,560
from intervening on their connections.

43
00:01:48,560 --> 00:01:51,160
And so obviously they're
getting a lot of protection.

44
00:01:51,160 --> 00:01:53,110
And the server in this scenario

45
00:01:53,110 --> 00:01:55,720
gets to see the plain
text of the conversation.

46
00:01:55,720 --> 00:01:56,860
And this is quite useful

47
00:01:56,860 --> 00:01:58,300
if you consider someone like Carol

48
00:01:58,300 --> 00:02:00,060
coming into the conversation

49
00:02:00,060 --> 00:02:01,870
on a phone on a bad connection.

50
00:02:01,870 --> 00:02:03,940
So imagine Carol has a 3G phone

51
00:02:05,260 --> 00:02:08,044
and she can't handle the full throughput

52
00:02:08,044 --> 00:02:09,750
of the conversation.

53
00:02:09,750 --> 00:02:12,710
Well, the server can down sample
the video and audio for her

54
00:02:12,710 --> 00:02:14,430
so she can get a good experience

55
00:02:14,430 --> 00:02:17,233
while Alice and Bob both get
to enjoy full resolution.

56
00:02:18,240 --> 00:02:19,073
So this is the way

57
00:02:19,073 --> 00:02:21,460
a lot of video conferencing
software works today.

58
00:02:23,360 --> 00:02:24,830
What are the issues?

59
00:02:24,830 --> 00:02:25,960
Well, the problem here

60
00:02:25,960 --> 00:02:28,680
is that the server has
access to the plain text

61
00:02:28,680 --> 00:02:30,410
and, you know, let's
think a little bit about

62
00:02:30,410 --> 00:02:32,859
what can go wrong with
point-to-point encryption.

63
00:02:34,650 --> 00:02:36,260
The problem with this approach

64
00:02:36,260 --> 00:02:38,679
is that all the bad guy really needs to do

65
00:02:38,680 --> 00:02:41,330
is get access to the
server and just listen.

66
00:02:41,330 --> 00:02:43,550
They don't need to inject any new traffic,

67
00:02:43,550 --> 00:02:44,410
they don't need to perform

68
00:02:44,410 --> 00:02:46,240
any interesting social engineering,

69
00:02:46,240 --> 00:02:47,830
all they have to do is copy the data

70
00:02:47,830 --> 00:02:49,660
that's flowing through the server.

71
00:02:49,660 --> 00:02:51,180
And this is very hard to detect

72
00:02:51,180 --> 00:02:52,620
if an attacker is good at this,

73
00:02:52,620 --> 00:02:54,940
they shouldn't leave much of a trace.

74
00:02:54,940 --> 00:02:57,109
And when you think about who
might actually wanna do this

75
00:02:57,110 --> 00:02:59,293
there's a large list of
people who could do it.

76
00:02:59,293 --> 00:03:01,840
The first group that comes to mind

77
00:03:01,840 --> 00:03:04,230
is if this is some sort
of commercial product

78
00:03:04,230 --> 00:03:07,750
on employees who work at the
video conferencing company

79
00:03:07,750 --> 00:03:09,550
might be able to interpose on meetings.

80
00:03:09,550 --> 00:03:10,950
And you can think about disgruntled

81
00:03:10,950 --> 00:03:12,440
or compromised employees,

82
00:03:12,440 --> 00:03:13,890
there might be a lot to gain.

83
00:03:14,740 --> 00:03:17,530
A bad actor who has access
to the cloud infrastructure

84
00:03:17,530 --> 00:03:19,160
that's underlying this server

85
00:03:19,160 --> 00:03:20,740
might also get access to the meeting.

86
00:03:20,740 --> 00:03:21,830
And so if you think about it,

87
00:03:21,830 --> 00:03:23,980
that a lot of providers these days

88
00:03:23,980 --> 00:03:26,390
don't actually rack their
own physical software,

89
00:03:26,390 --> 00:03:29,029
obviously that introduces a new risk.

90
00:03:29,030 --> 00:03:30,400
There's a whole other party of people

91
00:03:30,400 --> 00:03:32,950
who could gain access
to the infrastructure.

92
00:03:32,950 --> 00:03:35,929
And finally, an attacker who gains access

93
00:03:35,930 --> 00:03:38,030
past the perimeter of this infrastructure

94
00:03:38,030 --> 00:03:39,965
might also get access to the meeting.

95
00:03:39,965 --> 00:03:44,310
If you think about the way
a lot of servers are set up,

96
00:03:44,310 --> 00:03:46,270
it's usually there's a secure perimeter

97
00:03:46,270 --> 00:03:48,690
but once you're inside of it,
you can do a lot of damage.

98
00:03:48,690 --> 00:03:51,530
And the same goes for
obviously video conferencing,

99
00:03:51,530 --> 00:03:53,090
that any attacker who gained access

100
00:03:53,090 --> 00:03:54,630
to the video conferencing server

101
00:03:54,630 --> 00:03:56,853
can exfiltrate the meeting data.

102
00:03:57,830 --> 00:04:00,050
Now I would make an obvious point here

103
00:04:00,050 --> 00:04:03,020
that meetings can be
extremely valuable targets.

104
00:04:03,020 --> 00:04:04,860
And especially in the age of COVID

105
00:04:04,860 --> 00:04:06,513
when a lot of things have gone virtual,

106
00:04:06,513 --> 00:04:09,650
some very high value meetings
are happening virtually.

107
00:04:09,650 --> 00:04:11,090
And you can think about board meetings

108
00:04:11,090 --> 00:04:12,900
of Fortune 500 companies,

109
00:04:12,900 --> 00:04:14,778
or meetings of government regulators.

110
00:04:14,778 --> 00:04:17,320
And these are very valuable targets.

111
00:04:17,320 --> 00:04:19,498
And the rewards, financial rewards

112
00:04:19,498 --> 00:04:22,433
for exfiltrating meeting
data might be quite large.

113
00:04:25,010 --> 00:04:28,520
A lot of times people come
up with another solution

114
00:04:28,520 --> 00:04:31,070
to the problems I just
mentioned in the previous slide,

115
00:04:31,070 --> 00:04:33,719
which is what I would
call an on-prem solution.

116
00:04:33,720 --> 00:04:36,570
So imagine a bank who wants
to do video conferencing,

117
00:04:36,570 --> 00:04:38,659
but maybe they didn't want to be subjected

118
00:04:38,660 --> 00:04:41,150
to the cloud risk that I
showed in the previous slide.

119
00:04:41,150 --> 00:04:42,820
Well, what if the video
conferencing provider

120
00:04:42,820 --> 00:04:44,560
shipped them some sort of software

121
00:04:44,560 --> 00:04:46,040
that they could run on-prem

122
00:04:46,040 --> 00:04:49,860
and then they can run
their meetings that way?

123
00:04:49,860 --> 00:04:53,910
Well, here's my list of risks
that I showed previously

124
00:04:53,910 --> 00:04:57,930
and they're almost all also
risks for an on-prem solution.

125
00:04:57,930 --> 00:05:00,280
The one I would say is
like there's no longer risk

126
00:05:00,280 --> 00:05:02,260
of a cloud infrastructure
being compromised,

127
00:05:02,260 --> 00:05:05,510
but attacks of an on-prem video server

128
00:05:05,510 --> 00:05:07,084
it could be just as hard to detect,

129
00:05:07,084 --> 00:05:09,890
and there are still bad
employees who are at play,

130
00:05:09,890 --> 00:05:11,407
and, you know, in this case,

131
00:05:11,407 --> 00:05:14,239
it's not the employees of the
video conferencing provider,

132
00:05:14,239 --> 00:05:15,609
it could be employees of the bank

133
00:05:15,610 --> 00:05:17,986
who are the ones who
might exfiltrate the data.

134
00:05:17,986 --> 00:05:20,551
And especially of executives,

135
00:05:20,552 --> 00:05:23,377
they might be incentivized to do that.

136
00:05:23,377 --> 00:05:26,060
Again, the on-prem solution

137
00:05:26,060 --> 00:05:28,452
would have a perimeter to protect

138
00:05:28,452 --> 00:05:33,390
and the value of the target
is not going to be decreased

139
00:05:33,390 --> 00:05:34,839
as a result of running a server on-prem.

140
00:05:34,839 --> 00:05:37,744
The one thing I will say
is this on-prem solution

141
00:05:37,744 --> 00:05:40,490
introduces a new risk here

142
00:05:40,490 --> 00:05:43,060
is that the software itself

143
00:05:43,060 --> 00:05:45,100
can now become a backdoor onto the prep.

144
00:05:45,100 --> 00:05:47,201
And so because video conferencing

145
00:05:47,201 --> 00:05:50,210
requires publicly visible ports,

146
00:05:50,210 --> 00:05:52,870
these ports might be the way
that the bad guy gets in.

147
00:05:52,870 --> 00:05:54,080
So there are risks involved

148
00:05:54,080 --> 00:05:55,763
with an on-prem solution as well.

149
00:05:59,210 --> 00:06:00,580
So this is the context

150
00:06:00,580 --> 00:06:04,599
in which we decided to
introduce end-to-end encryption

151
00:06:04,600 --> 00:06:05,610
into the Zoom product,

152
00:06:05,610 --> 00:06:07,670
and we think end-to-end
solves a lot of the risks

153
00:06:07,670 --> 00:06:09,360
that we saw in the previous slides

154
00:06:09,360 --> 00:06:11,620
about having some sort
of centralized server

155
00:06:11,620 --> 00:06:13,720
that's seeing the plain text of a meeting.

156
00:06:15,780 --> 00:06:18,260
Now before we get into
end-to-end encryption at Zoom,

157
00:06:18,260 --> 00:06:19,990
I want to describe a little bit

158
00:06:19,990 --> 00:06:24,990
about how encryption at Zoom
worked before we got here.

159
00:06:25,210 --> 00:06:26,960
And the one thing I will say at this point

160
00:06:26,960 --> 00:06:28,289
is that if there are any questions

161
00:06:28,290 --> 00:06:29,530
throughout the presentation,

162
00:06:29,530 --> 00:06:30,929
please enter them as we go,

163
00:06:30,930 --> 00:06:33,240
because I'll be taking questions

164
00:06:33,240 --> 00:06:35,940
and hopefully answering them
as I do the presentation.

165
00:06:36,960 --> 00:06:38,440
So in enhanced encryption

166
00:06:38,440 --> 00:06:39,920
which is what encryption
was called at Zoom

167
00:06:39,920 --> 00:06:43,460
before end-to-end, the
process is as follows,

168
00:06:43,460 --> 00:06:44,849
Alice, Bob, and Carol connect

169
00:06:44,850 --> 00:06:48,370
through some sort of cloud
hosted meeting server.

170
00:06:48,370 --> 00:06:50,260
And in order to protect their data

171
00:06:50,260 --> 00:06:53,450
as we saw in the first
point-to-point encryption examples,

172
00:06:53,450 --> 00:06:54,990
the server is going to cook up

173
00:06:54,990 --> 00:06:57,500
some 32 byte random meeting key

174
00:06:57,500 --> 00:06:58,680
that is then gonna be used

175
00:06:58,680 --> 00:07:01,460
to symmetrically encrypt meeting streams

176
00:07:01,460 --> 00:07:03,760
that are sent between the participants.

177
00:07:03,760 --> 00:07:04,610
And it's going to use

178
00:07:04,610 --> 00:07:08,740
some sort of standard encryption
algorithm like AES-GCM.

179
00:07:08,740 --> 00:07:10,330
Now the server knows the meeting key

180
00:07:10,330 --> 00:07:11,510
that's being used to encrypt,

181
00:07:11,510 --> 00:07:13,080
and it typically doesn't use it

182
00:07:13,080 --> 00:07:14,909
but it does in a very important case

183
00:07:14,910 --> 00:07:17,240
and that is, let's say,
Alice, Bob, and Carol

184
00:07:17,240 --> 00:07:20,380
wanna invite their
fourth friend, Doug Colin

185
00:07:20,380 --> 00:07:23,800
who's on an old fashioned PSTN phone.

186
00:07:23,800 --> 00:07:26,560
Now the Zoom server, because
it knows this meeting key,

187
00:07:26,560 --> 00:07:28,750
has the credentials with which to patch it

188
00:07:28,750 --> 00:07:31,050
in this old fashioned phone call

189
00:07:31,050 --> 00:07:34,305
and therefore I can send
decrypted data to the phone

190
00:07:34,305 --> 00:07:36,540
and get data from the phone and encrypt it

191
00:07:36,540 --> 00:07:39,460
and inject it into the meeting
with Alice, Bob, and Carol.

192
00:07:39,460 --> 00:07:41,080
So we will make the point

193
00:07:41,080 --> 00:07:45,340
that for a non-end-to-end
encrypted meeting,

194
00:07:45,340 --> 00:07:46,849
there is still an advantage of the server

195
00:07:46,850 --> 00:07:47,690
seeing the plain text,

196
00:07:47,690 --> 00:07:51,300
and that is to do something
like patch in a PSTN phone,

197
00:07:51,300 --> 00:07:52,950
or maybe to do a cloud recording.

198
00:07:55,500 --> 00:07:57,870
So here's how end-to-end encryption works,

199
00:07:57,870 --> 00:08:00,120
and we're gonna describe it
in a pretty simplified way

200
00:08:00,120 --> 00:08:02,165
but that gets the general point across.

201
00:08:02,165 --> 00:08:04,690
So the key idea here

202
00:08:04,690 --> 00:08:07,206
is that the server never
gets to see this meeting key,

203
00:08:07,206 --> 00:08:10,430
rather that the participants
in this case, Alice,

204
00:08:10,430 --> 00:08:13,160
is gonna be the one to
generate the meeting key,

205
00:08:13,160 --> 00:08:14,857
and she's gonna use
public key cryptography

206
00:08:14,857 --> 00:08:18,144
to distribute it to the other
participants of the meeting.

207
00:08:18,144 --> 00:08:20,200
And as we said before,

208
00:08:20,200 --> 00:08:22,810
there will not be the
possibility of a phone dial in

209
00:08:22,810 --> 00:08:25,610
because the server is not
able to bridge the connection.

210
00:08:28,450 --> 00:08:31,042
So here's the protocol
for what might happen

211
00:08:31,042 --> 00:08:33,120
when the meeting gets bootstrapped.

212
00:08:33,120 --> 00:08:34,440
So call Alice the leader,

213
00:08:34,440 --> 00:08:36,890
and she's the one who
generates this meeting key.

214
00:08:38,520 --> 00:08:42,939
Now, Alice, Bob, and Carol
all have identity signing keys

215
00:08:45,360 --> 00:08:47,720
that are associated with their devices.

216
00:08:47,720 --> 00:08:49,600
So the device keeps the secret key

217
00:08:49,600 --> 00:08:52,560
of this signing key in its storage,

218
00:08:52,560 --> 00:08:55,010
and that key never leaves the device.

219
00:08:55,010 --> 00:08:57,880
The public side of this
key is sent to the server

220
00:08:57,880 --> 00:09:00,853
which in turn will broadcast
it to the other participants.

221
00:09:01,820 --> 00:09:03,890
Next, Alice, Bob, and Carol

222
00:09:03,890 --> 00:09:06,920
will generate Diffie-Hellman
ephemeral keys

223
00:09:06,920 --> 00:09:08,959
just for the purposes of this meeting.

224
00:09:08,960 --> 00:09:10,840
So each of them will generate a secret key

225
00:09:10,840 --> 00:09:12,260
which doesn't leave the device,

226
00:09:12,260 --> 00:09:14,010
and a public key which does.

227
00:09:14,010 --> 00:09:16,210
Now what they're going to
do is sign that public key

228
00:09:16,210 --> 00:09:17,820
with their long-term identity,

229
00:09:17,820 --> 00:09:20,380
and send that signed
public key up to the server

230
00:09:20,380 --> 00:09:22,439
which in turn is gonna
broadcast it back out

231
00:09:22,440 --> 00:09:23,763
to the other participants.

232
00:09:24,860 --> 00:09:27,850
Now, Alice is ready to do a key exchange.

233
00:09:27,850 --> 00:09:30,230
So she's gonna perform a
Diffie-Hellman key exchange

234
00:09:30,230 --> 00:09:32,020
one with Bob and one with Carol,

235
00:09:32,020 --> 00:09:35,050
which is then going to
give her a shared secret

236
00:09:35,050 --> 00:09:36,800
with Bob and Carol respectively.

237
00:09:36,800 --> 00:09:38,380
She's then gonna use that shared secret

238
00:09:38,380 --> 00:09:40,870
to send the meeting key
over to Bob and Carol

239
00:09:40,870 --> 00:09:42,573
so that now Bob and
Carol get the meeting key

240
00:09:42,573 --> 00:09:44,593
but the server never gets to see it.

241
00:09:46,930 --> 00:09:48,160
And as before, that meeting key

242
00:09:48,160 --> 00:09:50,959
is gonna be what's used to
encrypt audio visual data

243
00:09:50,960 --> 00:09:52,510
as it flows through the server.

244
00:09:55,150 --> 00:09:59,360
Now, when Carol leaves, the
key ought to be rotated.

245
00:09:59,360 --> 00:10:01,480
So Alice performs the
whole operation again,

246
00:10:01,480 --> 00:10:02,850
she makes a totally new meeting key

247
00:10:02,850 --> 00:10:05,140
that was not related to the prior one,

248
00:10:05,140 --> 00:10:08,110
and then she uses the same
Diffie-Hellman key exchange

249
00:10:08,110 --> 00:10:11,040
to send an encrypted copy
of that meeting key to Bob.

250
00:10:11,040 --> 00:10:12,449
This provides the property

251
00:10:12,450 --> 00:10:15,310
that even if Alice were
to get the cipher texts

252
00:10:15,310 --> 00:10:16,869
of the meeting after she left,

253
00:10:16,870 --> 00:10:18,160
she wouldn't be able to decrypt it

254
00:10:18,160 --> 00:10:21,540
because it'll be decrypted
with the new key, mk sub two,

255
00:10:21,540 --> 00:10:23,089
that she didn't have access to.

256
00:10:30,040 --> 00:10:31,380
There are a lot of important ways

257
00:10:31,380 --> 00:10:33,684
that the end-to-end encryption
design that we have in Zoom

258
00:10:33,684 --> 00:10:36,230
works its way out into the user interface,

259
00:10:36,230 --> 00:10:39,950
and this is one of the
better examples of it.

260
00:10:39,950 --> 00:10:42,090
So the way it currently works

261
00:10:42,090 --> 00:10:43,350
that at least every 10 seconds,

262
00:10:43,350 --> 00:10:45,360
the leader, in this case Alice,

263
00:10:45,360 --> 00:10:47,860
sends a heartbeat message
to the other users

264
00:10:47,860 --> 00:10:50,290
in the meeting, Bob and Carol.

265
00:10:50,290 --> 00:10:53,170
And this includes a bunch of
some important information.

266
00:10:53,170 --> 00:10:54,640
First, the current key generation,

267
00:10:54,640 --> 00:10:56,973
so are you on mk1 or mk2,

268
00:10:58,230 --> 00:11:01,800
and also a list of meeting
participants who know the key.

269
00:11:01,800 --> 00:11:04,640
And so this way, Alice can
look at her user interface

270
00:11:04,640 --> 00:11:07,660
and know exactly who's gotten
keyed into the meeting.

271
00:11:07,660 --> 00:11:09,420
Moreover, the other people in this meeting

272
00:11:09,420 --> 00:11:11,300
can also see the same view.

273
00:11:11,300 --> 00:11:12,729
And so if Alice was in the meeting

274
00:11:12,730 --> 00:11:14,370
expecting only Bob and Charlie,

275
00:11:14,370 --> 00:11:16,050
and she sees Doug in the meeting,

276
00:11:16,050 --> 00:11:17,439
she knows that there is a problem

277
00:11:17,440 --> 00:11:19,860
and there's no way for the
server to inject a Doug

278
00:11:19,860 --> 00:11:22,510
without it being reflected
in the user interface.

279
00:11:22,510 --> 00:11:24,470
Now, the reason we send
this every 10 seconds

280
00:11:24,470 --> 00:11:25,840
is that we say a malicious server

281
00:11:25,840 --> 00:11:27,490
might withhold these updates

282
00:11:27,490 --> 00:11:30,473
to potentially hide the fact

283
00:11:30,473 --> 00:11:33,270
that an new member has
entered into the meeting.

284
00:11:33,270 --> 00:11:35,430
Well, in that case, the
client will disconnect

285
00:11:35,430 --> 00:11:37,430
if they miss enough of these heartbeats.

286
00:11:38,910 --> 00:11:41,770
We'll make the point that
at this phase of our design,

287
00:11:41,770 --> 00:11:43,310
these display names are arbitrary

288
00:11:43,310 --> 00:11:45,459
meaning people in the meeting
get to write the names,

289
00:11:45,460 --> 00:11:48,040
type the names themselves
into their Zoom clients.

290
00:11:48,040 --> 00:11:49,280
There's obviously social pressure

291
00:11:49,280 --> 00:11:50,760
to do something reasonable here,

292
00:11:50,760 --> 00:11:51,593
but in the future

293
00:11:51,593 --> 00:11:55,310
we hope to really make some
good improvements upon this

294
00:11:55,311 --> 00:11:57,633
by providing stronger notions of identity.

295
00:12:00,820 --> 00:12:03,630
As with a lot of other
end-to-end encrypted products,

296
00:12:03,630 --> 00:12:05,820
there is a risk of a
meddler in the middle,

297
00:12:05,820 --> 00:12:08,590
or what used to be known as
a man in the middle attack

298
00:12:08,590 --> 00:12:10,190
in this configuration.

299
00:12:10,190 --> 00:12:12,300
And it's the basic man in the middle

300
00:12:12,300 --> 00:12:13,900
that you've read about for years.

301
00:12:13,900 --> 00:12:15,400
It's that Alice and Bob

302
00:12:15,400 --> 00:12:16,699
think they're connecting to each other,

303
00:12:16,700 --> 00:12:19,720
but rather there is a
malicious Eve in the middle

304
00:12:19,720 --> 00:12:21,160
who's doing some fancy footwork

305
00:12:21,160 --> 00:12:23,449
with regards to swapping
out their public keys

306
00:12:23,450 --> 00:12:25,100
and injecting her public key.

307
00:12:25,100 --> 00:12:26,330
And you want them in a scenario

308
00:12:26,330 --> 00:12:28,760
where everyone's connected
to Eve and they don't know it

309
00:12:28,760 --> 00:12:31,939
now, Eve has to decrypt the
whole contents of the meeting.

310
00:12:31,940 --> 00:12:35,110
So in Zoom phase one
end-to-end encryption,

311
00:12:35,110 --> 00:12:36,260
we provide a counter measure

312
00:12:36,260 --> 00:12:38,700
in the form of a meeting
leader security code.

313
00:12:38,700 --> 00:12:39,990
And so what Alice ought to do

314
00:12:39,990 --> 00:12:42,420
is she should click a
button on her user interface

315
00:12:42,420 --> 00:12:45,750
that exposes a code, as
you see here on the slide.

316
00:12:45,750 --> 00:12:47,490
She should then go and read that code out

317
00:12:47,490 --> 00:12:49,010
to everyone else in the meeting

318
00:12:49,010 --> 00:12:52,110
who should check that their
client show the same code.

319
00:12:52,110 --> 00:12:52,943
And if that's the case,

320
00:12:52,943 --> 00:12:55,380
they know they're not being
meddler-in-the-middled

321
00:12:55,380 --> 00:12:56,439
Now, as you can imagine,

322
00:12:56,440 --> 00:12:58,610
this is not the cleanest user interface

323
00:12:58,610 --> 00:13:00,817
but it's the simplest way to provide

324
00:13:00,817 --> 00:13:04,140
meddler-in-the-middle protection

325
00:13:04,140 --> 00:13:06,840
into the framework that we've established.

326
00:13:06,840 --> 00:13:07,950
There's another point here too

327
00:13:07,950 --> 00:13:10,500
which is that if Alice
drops out of the meeting

328
00:13:10,500 --> 00:13:12,680
and maybe Bob now becomes the new host,

329
00:13:12,680 --> 00:13:14,550
the UI has to reflect that fact,

330
00:13:14,550 --> 00:13:15,569
and it ought to say

331
00:13:15,570 --> 00:13:19,280
that a new meeting code
exchange should happen.

332
00:13:19,280 --> 00:13:22,230
And this is something you would
see in Zoom's client today.

333
00:13:23,160 --> 00:13:25,880
And as I said, we're not
fully satisfied with this UX

334
00:13:25,880 --> 00:13:29,140
we worry that a lot of our
users might skip this step

335
00:13:29,140 --> 00:13:31,810
in which case, there's no
way for them to be guaranteed

336
00:13:31,810 --> 00:13:33,859
that there isn't a meddler in the middle.

337
00:13:35,790 --> 00:13:38,640
Another thing we can do to
prevent this type of attack

338
00:13:38,640 --> 00:13:40,949
is this nice feature
called Lock the Meeting.

339
00:13:40,950 --> 00:13:44,190
Once Alice has led her other
participants into the meeting

340
00:13:44,190 --> 00:13:46,030
in this case, Bob and Carol,

341
00:13:46,030 --> 00:13:48,170
she could push a button
that says Lock the Meeting.

342
00:13:48,170 --> 00:13:49,410
This prevents her client

343
00:13:49,410 --> 00:13:51,420
from letting anyone else into the meeting

344
00:13:51,421 --> 00:13:54,900
that she doesn't want there.

345
00:13:54,900 --> 00:13:57,441
And if you combine this feature
with the previous feature,

346
00:13:57,441 --> 00:13:59,366
the meeting security codes,

347
00:13:59,366 --> 00:14:00,980
you have a pretty good guarantee

348
00:14:00,980 --> 00:14:03,510
that if you do the meeting
code exchange once,

349
00:14:03,510 --> 00:14:04,819
you never have to bother with it again

350
00:14:04,820 --> 00:14:06,470
for the purposes of this meeting.

351
00:14:10,360 --> 00:14:11,870
So that was a brief description

352
00:14:11,870 --> 00:14:13,240
of end-to-end encryption phase one,

353
00:14:13,240 --> 00:14:14,123
and I laid out a case

354
00:14:14,123 --> 00:14:16,390
that there's a little bit
more work to be done here.

355
00:14:16,390 --> 00:14:19,430
And this is coming in
phases two and beyond.

356
00:14:19,430 --> 00:14:22,012
And we're gonna go through
what that looks like right now.

357
00:14:23,160 --> 00:14:25,459
And the bread and butter
of phase two and beyond

358
00:14:25,460 --> 00:14:27,530
is a notion of better identity.

359
00:14:27,530 --> 00:14:30,920
And there are two things
that we'd like to say here.

360
00:14:30,920 --> 00:14:33,579
The first thing that comes to mind

361
00:14:33,580 --> 00:14:36,650
is that Alice, if she's talking
a lot to Bob and Charlie

362
00:14:36,650 --> 00:14:41,650
or Bob and Carol, can
she book keep some states

363
00:14:41,780 --> 00:14:43,130
so that she doesn't always need to do

364
00:14:43,130 --> 00:14:45,540
the same meeting leader
security code exchange

365
00:14:45,540 --> 00:14:46,980
that we saw previously.

366
00:14:46,980 --> 00:14:48,430
And the answer is yes,

367
00:14:48,430 --> 00:14:49,819
she can do something that's called TOFU

368
00:14:49,820 --> 00:14:51,720
or trust on first use,

369
00:14:51,720 --> 00:14:54,080
which means that when she
first meets Bob and Carol,

370
00:14:54,080 --> 00:14:56,280
it could be something where
she has to do an exchange,

371
00:14:56,280 --> 00:14:58,117
but every subsequent meeting after that,

372
00:14:58,117 --> 00:14:59,855
she should be able to leverage the fact

373
00:14:59,855 --> 00:15:02,130
that she previously authenticated them.

374
00:15:02,130 --> 00:15:03,370
And what she really needs to do

375
00:15:03,370 --> 00:15:04,890
is write down their public keys

376
00:15:04,890 --> 00:15:06,910
so that when she sees
those public keys again,

377
00:15:06,910 --> 00:15:09,170
she doesn't have to expose a warning

378
00:15:09,170 --> 00:15:11,290
that this is a new person.

379
00:15:11,290 --> 00:15:13,750
And the other part of the
identity that we hope to provide

380
00:15:13,750 --> 00:15:16,630
is that we're thinking hard

381
00:15:16,630 --> 00:15:19,140
about how to move trust and
what your identity means

382
00:15:19,140 --> 00:15:21,220
away from Zoom, the service provider,

383
00:15:21,220 --> 00:15:25,530
to other third parties that
our users are trusting already.

384
00:15:25,530 --> 00:15:27,310
And we think the best candidate for this

385
00:15:27,310 --> 00:15:30,599
is what's called an
identity provider or an IDP.

386
00:15:30,600 --> 00:15:32,270
And these IDPs are what our customers

387
00:15:32,270 --> 00:15:36,220
are currently using today to
manage SSOs or single sign-ons.

388
00:15:36,220 --> 00:15:38,420
And so our thinking here
is that our customers

389
00:15:38,420 --> 00:15:41,507
are already trusting these services

390
00:15:41,508 --> 00:15:43,990
to authenticate their users

391
00:15:43,990 --> 00:15:47,610
and also to maintain a notion
of a roster for their company,

392
00:15:47,610 --> 00:15:48,810
can we leverage this fact

393
00:15:48,810 --> 00:15:51,213
to expose that identity into meetings?

394
00:15:52,160 --> 00:15:54,160
So let's go over the TOFU portion first

395
00:15:54,160 --> 00:15:57,219
and then we'll hit the
third-party IDP design

396
00:15:57,220 --> 00:15:58,170
a little bit later.

397
00:15:59,560 --> 00:16:02,280
So here's a basic screenshot
of what it might look like

398
00:16:02,280 --> 00:16:05,760
to do a TOFU style security upgrade.

399
00:16:05,760 --> 00:16:08,610
So imagine this is a four way meeting,

400
00:16:08,610 --> 00:16:11,314
and let's say for now,
I'm the person Alex,

401
00:16:11,314 --> 00:16:13,740
Alex might be talking
to Carol all the time

402
00:16:13,740 --> 00:16:16,190
in which case his client, you know,

403
00:16:16,190 --> 00:16:19,536
can reflect the fact that this is Carol

404
00:16:19,536 --> 00:16:21,540
who Alex talks to all the time.

405
00:16:21,540 --> 00:16:23,770
But if Bob and Alice show
up for the first time,

406
00:16:23,770 --> 00:16:26,569
or maybe Alice shows up on a new device,

407
00:16:26,570 --> 00:16:28,650
Alex can be in a position

408
00:16:28,650 --> 00:16:31,100
where his client can
alert him to that fact.

409
00:16:31,100 --> 00:16:34,140
And this is going a long way

410
00:16:34,140 --> 00:16:37,210
towards reusing the security code exchange

411
00:16:37,210 --> 00:16:40,323
so you really only need to do
it once or a handful of times.

412
00:16:42,460 --> 00:16:44,080
A problem comes up right away, though.

413
00:16:44,080 --> 00:16:47,640
And you could imagine if the
most simple implementation

414
00:16:47,640 --> 00:16:49,750
of remembering people's public keys

415
00:16:49,750 --> 00:16:52,795
is just writing down
other people's long-term

416
00:16:52,795 --> 00:16:56,089
device signing keys that
we saw on the key exchange.

417
00:16:56,090 --> 00:16:58,090
Well, look at the case
of where Alice and Bob

418
00:16:58,090 --> 00:16:59,990
have three devices each.

419
00:16:59,990 --> 00:17:02,040
So when Alice and Bob first communicate,

420
00:17:02,040 --> 00:17:02,872
they'll get a warning,

421
00:17:02,873 --> 00:17:05,869
but they'll also get a warning
whenever they communicate

422
00:17:05,869 --> 00:17:07,310
on a new pair of devices.

423
00:17:07,310 --> 00:17:08,960
And so there are nine total pairings,

424
00:17:08,960 --> 00:17:11,380
which means nine possible
warnings for Alice and Bob

425
00:17:11,380 --> 00:17:13,140
if each of them have three devices.

426
00:17:13,140 --> 00:17:14,951
And you can imagine,

427
00:17:14,951 --> 00:17:17,420
they will get so used to
seeing these warnings,

428
00:17:17,420 --> 00:17:18,630
so they'll just start to ignore them

429
00:17:18,630 --> 00:17:19,750
and they'll have warning fatigue,

430
00:17:19,750 --> 00:17:21,992
in which case the warning
doesn't do much of anything.

431
00:17:23,050 --> 00:17:24,720
And moreover, this
problem doesn't go away,

432
00:17:24,720 --> 00:17:28,339
it continues whenever either
one of them adds a new device

433
00:17:28,339 --> 00:17:30,209
when Bob, in this case,
that's a fourth device,

434
00:17:30,210 --> 00:17:31,490
there need to be three new warnings.

435
00:17:31,490 --> 00:17:32,493
So can we do better?

436
00:17:33,810 --> 00:17:35,530
And the answer is yes, in our proposal,

437
00:17:35,530 --> 00:17:39,250
we have a notion for how Alice and Bob

438
00:17:39,250 --> 00:17:40,860
can get away with just one warning

439
00:17:40,860 --> 00:17:42,600
when they talk to each
other for the first time

440
00:17:42,600 --> 00:17:44,500
on any of their devices.

441
00:17:44,500 --> 00:17:45,750
And really what this looks like

442
00:17:45,750 --> 00:17:47,530
is both Alice and Bob should have a notion

443
00:17:47,530 --> 00:17:49,280
about personal device cloud.

444
00:17:49,280 --> 00:17:51,050
They should have a handful of devices,

445
00:17:51,050 --> 00:17:53,980
any one of which is authorized
to speak on their behalf,

446
00:17:53,980 --> 00:17:55,693
but as long as any one of
those devices in the cloud

447
00:17:55,693 --> 00:17:59,129
speaks to any of the other
devices in Bob's cloud,

448
00:17:59,130 --> 00:18:00,480
then that's the only warning you need,

449
00:18:00,480 --> 00:18:01,640
that's the TOFU warning

450
00:18:01,640 --> 00:18:03,630
that has to be presented to the user

451
00:18:03,630 --> 00:18:06,580
and the rest can be handled
automatically without warnings.

452
00:18:09,020 --> 00:18:11,620
Moreover, if Bob adds a fourth device,

453
00:18:11,620 --> 00:18:13,520
there should be no additional warning

454
00:18:13,520 --> 00:18:16,889
so as long as that device
has been authorized by Bob.

455
00:18:16,890 --> 00:18:18,930
So this would be a big improvement.

456
00:18:18,930 --> 00:18:20,440
How do we build it?

457
00:18:20,440 --> 00:18:21,730
Well, let's start building up a notion

458
00:18:21,730 --> 00:18:23,560
of identity for Alice.

459
00:18:23,560 --> 00:18:25,030
The first thing we wanna do

460
00:18:25,030 --> 00:18:29,180
is commit her to something
like a long-term identity

461
00:18:29,180 --> 00:18:31,160
that she's gonna expose to other users.

462
00:18:31,160 --> 00:18:33,820
So maybe there will be a display name,

463
00:18:33,820 --> 00:18:36,450
like a human readable name,

464
00:18:36,450 --> 00:18:37,740
and maybe an email address

465
00:18:37,740 --> 00:18:41,230
in case there's any ambiguity
in her human readable name,

466
00:18:41,230 --> 00:18:44,510
and also a set of devices
that constitute her identity.

467
00:18:44,510 --> 00:18:48,430
So in this case, Alice has three
devices, three public keys.

468
00:18:48,430 --> 00:18:50,060
Each of those devices has a secret key,

469
00:18:50,060 --> 00:18:52,399
a corresponding secret
key to that public key

470
00:18:52,400 --> 00:18:54,550
that never leaves the device.

471
00:18:54,550 --> 00:18:56,000
And a typical flow for Alice

472
00:18:56,000 --> 00:18:58,440
might be that she first
signs up with a laptop,

473
00:18:58,440 --> 00:18:59,780
later adds a phone,

474
00:18:59,780 --> 00:19:02,430
but when she adds a phone
that has to be approved

475
00:19:02,430 --> 00:19:04,390
by one of her previous devices

476
00:19:04,390 --> 00:19:06,061
so in this case, her laptop.

477
00:19:06,061 --> 00:19:07,909
And then later she might add a tablet,

478
00:19:07,910 --> 00:19:09,120
and as long as that's approved

479
00:19:09,120 --> 00:19:10,939
by one of her previous devices,

480
00:19:10,940 --> 00:19:12,570
be it her laptop or her phone,

481
00:19:12,570 --> 00:19:15,173
that device now becomes
part of her identity.

482
00:19:18,680 --> 00:19:20,440
So let's drill down a little bit

483
00:19:20,440 --> 00:19:23,150
on what this approval
process actually looks like.

484
00:19:23,150 --> 00:19:24,520
There are two things that happens

485
00:19:24,520 --> 00:19:27,210
when an old device approves a new one.

486
00:19:27,210 --> 00:19:29,984
The first is that the old one
delegates signing authority

487
00:19:29,984 --> 00:19:32,760
over to the new one via a signature.

488
00:19:32,760 --> 00:19:34,410
And this means the old client

489
00:19:34,410 --> 00:19:36,240
used to be able to sign
on behalf of Alice,

490
00:19:36,240 --> 00:19:37,140
it's gonna sign a statement

491
00:19:37,140 --> 00:19:38,810
saying the new device is able to sign

492
00:19:38,810 --> 00:19:40,416
on behalf of Alice too.

493
00:19:40,416 --> 00:19:42,370
But there's a second
thing that happens here

494
00:19:42,370 --> 00:19:43,860
that's totally independent,

495
00:19:43,860 --> 00:19:45,409
and that is the old one might have

496
00:19:45,410 --> 00:19:49,950
some private keys used
for encryption and secrets

497
00:19:49,950 --> 00:19:52,170
that it can gossip over to the new device.

498
00:19:52,170 --> 00:19:54,470
So this way, the old
device and the new device

499
00:19:54,470 --> 00:19:56,570
can share secrets through the server

500
00:19:56,570 --> 00:19:58,540
in such a way where the
server can't see them.

501
00:19:58,540 --> 00:20:01,224
This is gonna be important
later in the talk,

502
00:20:01,224 --> 00:20:03,954
and it's a very powerful mechanism

503
00:20:03,954 --> 00:20:06,903
that unlocks a whole
bunch of new features.

504
00:20:09,430 --> 00:20:11,210
One last thing we wanna describe here

505
00:20:11,210 --> 00:20:13,870
is that when Alice revokes a device,

506
00:20:13,870 --> 00:20:16,500
she should sign a
statement to that effect.

507
00:20:16,500 --> 00:20:17,700
And so in this slide,

508
00:20:17,700 --> 00:20:20,360
Alice threw away her phone
or she left in a taxi cab,

509
00:20:20,360 --> 00:20:23,010
she should sign a statement
with one of her other devices

510
00:20:23,010 --> 00:20:25,023
saying to no longer trust that device.

511
00:20:27,640 --> 00:20:30,780
So now we can improve
upon the contact sync UI

512
00:20:30,780 --> 00:20:32,389
that I showed you earlier.

513
00:20:32,390 --> 00:20:35,450
So if you recall before we had a pop-up

514
00:20:35,450 --> 00:20:37,210
that said Alice is on a new device.

515
00:20:37,210 --> 00:20:38,850
In this case, we don't see that.

516
00:20:38,850 --> 00:20:40,480
So everyone else remembered the fact

517
00:20:40,480 --> 00:20:43,100
that they spoke to Alice
on her old devices,

518
00:20:43,100 --> 00:20:44,980
and when Alice delegates to the new device

519
00:20:44,980 --> 00:20:45,990
they're happy with that

520
00:20:45,990 --> 00:20:47,460
and they don't need to show a warning.

521
00:20:47,460 --> 00:20:49,330
Now, of course, when Bob
shows up for the first time

522
00:20:49,330 --> 00:20:50,460
you do need to show that warning,

523
00:20:50,460 --> 00:20:52,810
so we've gotten rid of
one of the two warnings.

524
00:20:55,840 --> 00:20:56,980
Now, if you consider the fact

525
00:20:56,980 --> 00:20:59,430
that Alice is gonna be speaking
to a lot of different people

526
00:20:59,430 --> 00:21:01,140
across a lot of different devices,

527
00:21:01,140 --> 00:21:03,510
there's a notion that she
needs to sync this record

528
00:21:03,510 --> 00:21:05,150
of who she spoken to through the server

529
00:21:05,150 --> 00:21:07,300
so that her other devices can see it.

530
00:21:07,300 --> 00:21:08,149
And we thought pretty hard

531
00:21:08,150 --> 00:21:10,440
about how Alice would wanna do this.

532
00:21:10,440 --> 00:21:13,189
The naive way would just
be to write in plain text

533
00:21:13,189 --> 00:21:15,480
signed statements from Alice on the form

534
00:21:15,480 --> 00:21:18,050
like I just spoke to Alex on my phone,

535
00:21:18,050 --> 00:21:20,399
or I just spoke to Alex,
you know, on his phone

536
00:21:20,400 --> 00:21:22,260
or Alex on his desktop,

537
00:21:22,260 --> 00:21:24,430
and then sync that across her devices.

538
00:21:24,430 --> 00:21:25,263
But we were quite worried

539
00:21:25,263 --> 00:21:28,560
about accumulating all of that
valuable data on the server,

540
00:21:28,560 --> 00:21:29,919
and if it were ever exfiltrated

541
00:21:29,920 --> 00:21:33,190
it would be extremely
damaging to a lot of people.

542
00:21:33,190 --> 00:21:35,910
So instead, what we're building

543
00:21:35,910 --> 00:21:39,010
is that Alice is syncing an encrypted copy

544
00:21:39,010 --> 00:21:40,550
of this contact history

545
00:21:40,550 --> 00:21:43,690
so that even in the worst case scenario,

546
00:21:43,690 --> 00:21:45,120
if this data is exfiltrated,

547
00:21:45,120 --> 00:21:47,800
the attacker can't see
who Alice has spoken to.

548
00:21:47,800 --> 00:21:49,830
Now you might be saying,
well, the server knows anyway

549
00:21:49,830 --> 00:21:51,610
who spoke to who, it probably has logs

550
00:21:51,610 --> 00:21:54,330
as to who joined which
meetings, that might be the case

551
00:21:54,330 --> 00:21:57,179
but those logs could be
thrown away all the time.

552
00:21:57,180 --> 00:21:59,110
And these contact histories

553
00:21:59,110 --> 00:22:00,709
might go back a long distance

554
00:22:00,710 --> 00:22:05,710
to prevent Alice from having
to reauthenticate other people

555
00:22:05,930 --> 00:22:06,930
in her contact book.

556
00:22:06,930 --> 00:22:10,300
So there's a real advantage
to encrypting these records

557
00:22:10,300 --> 00:22:12,149
as they're synced through the server.

558
00:22:14,170 --> 00:22:16,200
So that's the UX you're
trying to build up to.

559
00:22:16,200 --> 00:22:17,890
Well, what's the mechanism?

560
00:22:17,890 --> 00:22:19,980
And here we're introducing
an idea in our paper

561
00:22:19,980 --> 00:22:22,080
called signature chains.

562
00:22:22,080 --> 00:22:25,210
And so, as I said before,
Alice has a series of devices

563
00:22:25,210 --> 00:22:27,550
that she's introducing and approving

564
00:22:27,550 --> 00:22:29,040
and potentially revoking,

565
00:22:29,040 --> 00:22:30,438
and each of these operations

566
00:22:30,439 --> 00:22:33,210
should be serialized in such a way

567
00:22:33,210 --> 00:22:36,040
where replaying them in a
different order is impossible.

568
00:22:36,040 --> 00:22:39,020
So what that means is that
Alice makes a first device

569
00:22:39,020 --> 00:22:42,210
in this case, her phone,
on link number one

570
00:22:42,210 --> 00:22:44,674
and link number two she might
provision a second computer

571
00:22:44,674 --> 00:22:46,939
which in this case is her laptop.

572
00:22:46,939 --> 00:22:48,940
Now that might also be self-signed,

573
00:22:48,940 --> 00:22:50,800
but she's also signing the fact

574
00:22:50,800 --> 00:22:51,940
that this is link number two

575
00:22:51,940 --> 00:22:54,302
and she signs a hash of the previous link,

576
00:22:54,302 --> 00:22:55,199
that's link number one,

577
00:22:55,200 --> 00:22:58,010
saying that this can later
only be played in one order.

578
00:22:58,010 --> 00:23:00,720
Now she'll go back to her
phone and approve her laptop,

579
00:23:00,720 --> 00:23:01,970
and that's another signed statement.

580
00:23:01,970 --> 00:23:03,940
And finally, she might
revoke her phone later

581
00:23:03,940 --> 00:23:06,350
with the laptop, and that's
a fourth signed statement.

582
00:23:06,350 --> 00:23:08,270
And so the advantage of a signature chain

583
00:23:08,270 --> 00:23:11,030
is that Alice performs these
signatures in such a way

584
00:23:11,030 --> 00:23:13,830
so that they can only be
replayed in one possible order.

585
00:23:13,830 --> 00:23:15,560
And then a malicious
server can't reorder them

586
00:23:15,560 --> 00:23:17,312
in a way to get the wrong answer.

587
00:23:18,390 --> 00:23:19,920
So that's the first building block,

588
00:23:19,920 --> 00:23:22,710
but there are a lot of attacks
that are still possible

589
00:23:22,710 --> 00:23:25,160
if you're considering a
malicious server here.

590
00:23:25,160 --> 00:23:26,983
And the first, the most obvious one

591
00:23:26,983 --> 00:23:30,310
is that the server might with
hold the end of these chains.

592
00:23:30,310 --> 00:23:32,620
So the server might have shown Alice

593
00:23:32,620 --> 00:23:34,739
the extent of her four links in her chain,

594
00:23:34,740 --> 00:23:38,300
but then when Bob goes to
ask what's Alice's identity

595
00:23:38,300 --> 00:23:40,210
he would need to play back this chain

596
00:23:40,210 --> 00:23:41,840
but let's say the server withheld the fact

597
00:23:41,840 --> 00:23:43,750
that Alice revoked her device.

598
00:23:43,750 --> 00:23:45,360
This might be a very valuable attack

599
00:23:45,360 --> 00:23:47,560
if potentially the
server wants to show Bob

600
00:23:47,560 --> 00:23:48,393
that revoked device,

601
00:23:48,393 --> 00:23:50,600
so it can withhold the end of the chain.

602
00:23:50,600 --> 00:23:55,600
Another attack is that the
server might try to fork Alice.

603
00:23:55,750 --> 00:23:58,920
So Alice might be asking
for her own identity

604
00:23:58,920 --> 00:24:00,020
on a new device,

605
00:24:00,020 --> 00:24:01,610
and she might get a shortened version

606
00:24:01,610 --> 00:24:02,870
of that identity chain

607
00:24:02,870 --> 00:24:05,949
one that potentially
leaves off the last link.

608
00:24:05,950 --> 00:24:08,230
Now, when Alice goes and
adds a new signature to it

609
00:24:08,230 --> 00:24:09,240
she'll fork the chain

610
00:24:09,240 --> 00:24:11,780
and potentially remove that replication.

611
00:24:11,780 --> 00:24:13,060
So this is a way in which the server

612
00:24:13,060 --> 00:24:14,149
can play a game on Alice,

613
00:24:14,150 --> 00:24:17,040
so that Alice herself signs a truncated

614
00:24:17,040 --> 00:24:19,770
or fork version of her signature chain.

615
00:24:19,770 --> 00:24:22,080
And finally, the server can
do something pretty fancy here

616
00:24:22,080 --> 00:24:25,470
and just wholesale
substitute what Alice wrote

617
00:24:25,470 --> 00:24:27,130
with an evil attacker.

618
00:24:27,130 --> 00:24:29,914
So, you know, Alice might ask the server

619
00:24:29,914 --> 00:24:31,580
what's my signature chain,

620
00:24:31,580 --> 00:24:32,629
and she might get the right answer,

621
00:24:32,630 --> 00:24:34,770
but when Bob goes to ask the same thing,

622
00:24:34,770 --> 00:24:36,080
it might get a totally different chain

623
00:24:36,080 --> 00:24:38,040
and there's nothing to do
with what Alice signed.

624
00:24:38,040 --> 00:24:39,990
We can call this a substitution attack.

625
00:24:41,050 --> 00:24:42,780
And so in all of these cases,

626
00:24:42,780 --> 00:24:44,776
we are building up a mechanism

627
00:24:44,776 --> 00:24:47,580
to avoid this whole class of attacks.

628
00:24:47,580 --> 00:24:49,000
And the solution which you could read more

629
00:24:49,000 --> 00:24:51,920
about in the paper is
called a transparency tree.

630
00:24:51,920 --> 00:24:54,370
So the idea here is to
store all identities

631
00:24:54,370 --> 00:24:56,090
in an authenticated structure,

632
00:24:56,090 --> 00:24:58,600
similar to what you might
see in the Keybase product,

633
00:24:58,600 --> 00:25:01,399
the CONIKS paper, Key
Transparency, or SEEMless.

634
00:25:01,400 --> 00:25:03,980
And the idea here is
that this data structure

635
00:25:03,980 --> 00:25:08,880
is append only, privacy
preserving, and auditable.

636
00:25:08,880 --> 00:25:10,700
And it's working towards
building this guarantee

637
00:25:10,700 --> 00:25:13,497
that all users have a
consistent view of any identity,

638
00:25:13,497 --> 00:25:16,570
and that any impersonation
attacks can be detected

639
00:25:16,570 --> 00:25:18,350
either at the time of the request

640
00:25:18,350 --> 00:25:20,053
or after the fact in an audit.

641
00:25:21,610 --> 00:25:23,280
And so if you go back to the example

642
00:25:23,280 --> 00:25:24,600
where the server was really trying

643
00:25:24,600 --> 00:25:26,699
to play Bob off against Alice

644
00:25:26,700 --> 00:25:29,425
by showing them conflicting
versions of the tree,

645
00:25:29,425 --> 00:25:31,740
let's assume for a fact that Alice and Bob

646
00:25:31,740 --> 00:25:35,450
both got the same version
of the root of the tree,

647
00:25:35,450 --> 00:25:37,150
then they both worked their way down

648
00:25:37,150 --> 00:25:38,570
to the same leaf of the tree

649
00:25:38,570 --> 00:25:40,169
using cryptographic hash functions

650
00:25:40,170 --> 00:25:41,833
and finally would get to the same end

651
00:25:41,833 --> 00:25:44,240
of Alice's signature chain in both cases.

652
00:25:44,240 --> 00:25:46,780
So as long as Alice and
Bob get the same root,

653
00:25:46,780 --> 00:25:49,370
they're gonna get the same
version for Alice's identity,

654
00:25:49,370 --> 00:25:52,479
and therefore the same
signed sequence of statements

655
00:25:52,480 --> 00:25:55,623
that allow Bob to build up
Alice's identity on his sign.

656
00:25:58,080 --> 00:26:00,350
Now there's one thing we need
to be careful to support here

657
00:26:00,350 --> 00:26:03,429
and that is Alice probably
has a mutable identity,

658
00:26:03,430 --> 00:26:06,320
and that if she's known
as alice@company.co

659
00:26:06,320 --> 00:26:07,939
she might eventually change her mind

660
00:26:07,940 --> 00:26:09,710
and decide she wants to change her name,

661
00:26:09,710 --> 00:26:10,600
or she wants to switch

662
00:26:10,600 --> 00:26:13,120
potentially to a different
organization within the company,

663
00:26:13,120 --> 00:26:15,659
so there needs to be another
level of redirection.

664
00:26:15,660 --> 00:26:16,980
And the way we implement that

665
00:26:16,980 --> 00:26:19,690
is that we first store
in that transparency tree

666
00:26:19,690 --> 00:26:23,050
a mapping of Alice's
human readable identity

667
00:26:23,050 --> 00:26:25,220
and her email address to her user ID,

668
00:26:25,220 --> 00:26:28,920
and then a mapping of her
user ID to her last signature.

669
00:26:28,920 --> 00:26:29,940
And we're doing this again

670
00:26:29,940 --> 00:26:32,950
through some sort of
hash function called H.

671
00:26:32,950 --> 00:26:36,200
Now I've left out what this
hash function is actually doing

672
00:26:36,200 --> 00:26:39,240
you need to think a little
bit harder about that.

673
00:26:39,240 --> 00:26:42,020
Well, it can't be a standard
cryptographic hash function

674
00:26:42,020 --> 00:26:43,570
because that would leak too much data

675
00:26:43,570 --> 00:26:46,990
about how Alice's identity is changing

676
00:26:46,990 --> 00:26:49,000
to the other users in the tree.

677
00:26:49,000 --> 00:26:51,498
Instead, as first proposed
in the paper, CONIKS,

678
00:26:51,498 --> 00:26:55,530
we're gonna use a verifiable
random function or a VRF.

679
00:26:55,530 --> 00:26:58,940
And in this case, a VRF
functions in a lot of ways

680
00:26:58,940 --> 00:27:01,450
like a hash function, but has
other important properties

681
00:27:01,450 --> 00:27:02,780
so that users stored in the tree

682
00:27:02,780 --> 00:27:04,980
can learn information
about their neighbors.

683
00:27:07,400 --> 00:27:08,470
What is a VRF?

684
00:27:08,470 --> 00:27:10,320
Well, it's a one-way random function

685
00:27:10,320 --> 00:27:11,820
that only the server can compute,

686
00:27:11,820 --> 00:27:13,570
but all clients can verify,

687
00:27:13,570 --> 00:27:15,429
and you could think about
one way to implement it

688
00:27:15,430 --> 00:27:17,290
is with a non-randomized signature.

689
00:27:17,290 --> 00:27:18,960
And there are a lot of
evolving standards here

690
00:27:18,960 --> 00:27:20,620
and you can see some IETF drafts

691
00:27:20,620 --> 00:27:21,600
for people actually working

692
00:27:21,600 --> 00:27:24,230
on practical solutions
to building out VRFs.

693
00:27:28,260 --> 00:27:32,120
So that was our story for TOFU
or improved versions of TOFU

694
00:27:32,120 --> 00:27:35,489
about how Alice is gonna
sync a notion of her identity

695
00:27:35,490 --> 00:27:37,810
to other people in her meetings,

696
00:27:37,810 --> 00:27:41,030
and that identity might
encapsulate many devices

697
00:27:41,030 --> 00:27:42,470
not just one.

698
00:27:42,470 --> 00:27:44,780
But how do we know who
Alice is in the first place?

699
00:27:44,780 --> 00:27:46,770
Or we're just taking Zoom's word for it?

700
00:27:46,770 --> 00:27:49,980
And so in this case, we wanna
build up to a new feature

701
00:27:49,980 --> 00:27:53,130
which we're calling identity
provider attestations.

702
00:27:53,130 --> 00:27:58,030
And so imagine Alice is in an
organization that uses an IDP

703
00:27:58,030 --> 00:27:59,670
or an identity provider,

704
00:27:59,670 --> 00:28:01,290
and we wanna build up a system

705
00:28:01,290 --> 00:28:03,340
where Alice can have that provider

706
00:28:04,590 --> 00:28:06,100
sign off on what her identity is

707
00:28:06,100 --> 00:28:08,617
rather than having Zoom
be in charge of that,

708
00:28:08,617 --> 00:28:11,750
and this way we've moved
trust from the Zoom service

709
00:28:11,750 --> 00:28:14,590
into another service that
Alice is trusting anyway.

710
00:28:14,590 --> 00:28:18,040
And this process looks
a little bit like this.

711
00:28:18,040 --> 00:28:20,100
Alice could authenticate her IDP

712
00:28:20,100 --> 00:28:22,510
using something like the OAuth2 protocol.

713
00:28:22,510 --> 00:28:24,528
And when she does that,
she could post to it

714
00:28:24,528 --> 00:28:25,361
the tail of her sigchain,

715
00:28:26,344 --> 00:28:28,983
the last thing that she signed
that was in her sigchain.

716
00:28:30,750 --> 00:28:32,710
When Alice goes to join a meeting,

717
00:28:32,710 --> 00:28:34,790
she can request the
identity provider signature

718
00:28:34,790 --> 00:28:36,570
over her sigchain tail,

719
00:28:36,570 --> 00:28:39,520
what she then tends to show to
other people in the meeting.

720
00:28:40,649 --> 00:28:41,949
When Bob comes along,

721
00:28:41,950 --> 00:28:44,081
he first checks that the identity provider

722
00:28:44,081 --> 00:28:47,990
is authorized to speak on
behalf of Alice's company.

723
00:28:47,990 --> 00:28:50,550
So he's gonna make a TLS
connection to that company,

724
00:28:50,550 --> 00:28:54,629
in this case company.com is
associated with Alice's identity

725
00:28:54,630 --> 00:28:56,050
and get some sort of URL

726
00:28:56,050 --> 00:28:57,950
that points to Alice identity provider

727
00:28:57,950 --> 00:29:01,150
and also the public key
of that identity provider.

728
00:29:01,150 --> 00:29:03,288
And finally, Bob can check the fact

729
00:29:03,288 --> 00:29:05,980
that the statement of Alice's identity

730
00:29:05,980 --> 00:29:07,480
has been signed properly,

731
00:29:07,480 --> 00:29:08,960
that it's reflecting an identity

732
00:29:08,960 --> 00:29:10,410
that he could show on a screen,

733
00:29:10,410 --> 00:29:12,330
and finally, this will
encapsulate the identity

734
00:29:12,330 --> 00:29:15,080
that Alice was building
up in the previous slides.

735
00:29:15,080 --> 00:29:17,649
So in other words, Bob
can get Alice's chain tail

736
00:29:17,650 --> 00:29:19,310
as corroborated through a third party.

737
00:29:19,310 --> 00:29:21,010
So now he really knows this is Alice

738
00:29:21,010 --> 00:29:23,379
not somebody who compromised in servers

739
00:29:23,380 --> 00:29:24,703
pretending to be Alice.

740
00:29:27,740 --> 00:29:31,110
So just to recap, to improve
upon end-to-end encryption,

741
00:29:31,110 --> 00:29:33,899
we're building up a stronger
notion of identity at Zoom.

742
00:29:33,900 --> 00:29:34,962
Users can have identities

743
00:29:34,962 --> 00:29:37,780
composed of multiple independent devices.

744
00:29:37,780 --> 00:29:40,040
Each of these devices has a private key

745
00:29:40,040 --> 00:29:41,470
that never leaves the device,

746
00:29:41,470 --> 00:29:44,470
but a public key that it's
shared with all participants.

747
00:29:44,470 --> 00:29:46,670
Users can conveniently
remember each other's keys

748
00:29:46,670 --> 00:29:49,970
and not on a per device basis
but more on a per user basis.

749
00:29:49,970 --> 00:29:53,160
Therefore, any warnings
that are exposed in the UI

750
00:29:53,160 --> 00:29:56,290
are once per user and once per device.

751
00:29:56,290 --> 00:29:58,540
And finally, it's interesting
to think about ways

752
00:29:58,540 --> 00:30:01,240
in which third parties can
attest to these identities

753
00:30:01,240 --> 00:30:02,480
without having to trust Zoom,

754
00:30:02,480 --> 00:30:05,890
and we built up a mechanism
through the OAuth2 protocol

755
00:30:05,890 --> 00:30:07,700
where identity providers can do that

756
00:30:07,700 --> 00:30:08,900
rather than Zoom itself.

757
00:30:10,350 --> 00:30:13,120
And I wanted to provide
a little bit of a teaser

758
00:30:13,120 --> 00:30:14,070
for the future,

759
00:30:14,070 --> 00:30:15,730
you know, the applications
for this technology

760
00:30:15,730 --> 00:30:17,660
goes way beyond meetings,

761
00:30:17,660 --> 00:30:19,657
we can take about the same technology,

762
00:30:19,657 --> 00:30:22,110
basically, I'm planning any context

763
00:30:22,110 --> 00:30:24,110
where Alice and Bob need
end-to-end encryption.

764
00:30:24,110 --> 00:30:27,092
So you think about async
communications like messaging.

765
00:30:29,724 --> 00:30:32,220
So here's a recap of
what we've learned today

766
00:30:32,220 --> 00:30:34,700
and what you can think
about going forward.

767
00:30:34,700 --> 00:30:36,630
We think the most digestible nugget here

768
00:30:36,630 --> 00:30:37,860
is to think a little bit more

769
00:30:37,860 --> 00:30:41,320
about the risks surrounding
point-to-point encryption

770
00:30:41,320 --> 00:30:44,350
or potentially on-prem solutions.

771
00:30:44,350 --> 00:30:45,929
And we'd like you to think

772
00:30:45,930 --> 00:30:47,300
about how you can share your knowledge

773
00:30:47,300 --> 00:30:48,950
of the core design concepts

774
00:30:48,950 --> 00:30:52,060
that we presented behind
end-to-end encryption.

775
00:30:52,060 --> 00:30:53,950
Going a little bit further out, you know,

776
00:30:53,950 --> 00:30:56,550
maybe think about your own
product in your own organization

777
00:30:56,550 --> 00:30:59,480
and how can end-to-end
encryption benefit you,

778
00:30:59,480 --> 00:31:00,850
and maybe there's a class of threats

779
00:31:00,850 --> 00:31:01,840
that are quite important

780
00:31:01,840 --> 00:31:05,139
that you could use end-to-end
technology to guard against.

781
00:31:05,140 --> 00:31:06,300
And if you have a little bit more time

782
00:31:06,300 --> 00:31:07,930
you can check out our white paper

783
00:31:07,930 --> 00:31:09,464
as I described, a lot of these details

784
00:31:09,464 --> 00:31:11,960
are covered in way more
depth in that paper.

785
00:31:11,960 --> 00:31:13,970
And of course, if you have any comments

786
00:31:13,970 --> 00:31:15,799
about how we can improve
the design going forward,

787
00:31:15,799 --> 00:31:17,879
we'd love to see them.

788
00:31:17,880 --> 00:31:20,410
And finally, within six months,
you should be on the lookout

789
00:31:20,410 --> 00:31:22,910
for these new features landing in the app

790
00:31:22,910 --> 00:31:25,500
and more details to come on those,

791
00:31:25,500 --> 00:31:27,220
but we are really excited

792
00:31:27,220 --> 00:31:28,610
about the way that we can integrate

793
00:31:28,610 --> 00:31:31,490
a strong notion of identity
with end-to-end encryption

794
00:31:31,490 --> 00:31:33,963
to provide better value
for Zoom's customers.

795
00:31:36,940 --> 00:31:38,650
And that's all I had, thank you so much,

796
00:31:38,650 --> 00:31:40,187
thank you for your questions.

797
00:31:40,187 --> 00:31:44,639
And we look forward to talking more

798
00:31:44,640 --> 00:31:47,290
about all the stuff we're
working on here, thank you.

