1
00:00:01,660 --> 00:00:04,019
- Hi everybody, my name is Sherin Matthews

2
00:00:04,019 --> 00:00:06,209
and here's my colleague Amanda House.

3
00:00:06,210 --> 00:00:08,990
We both are data scientists from McAfee.

4
00:00:08,990 --> 00:00:11,830
We are here to present
our session on ethical AI.

5
00:00:11,830 --> 00:00:13,920
With that being said, we are
here throughout the session.

6
00:00:13,920 --> 00:00:16,680
So feel free to put out
your questions in the chat.

7
00:00:16,680 --> 00:00:18,720
Now, going over ethical AI,

8
00:00:18,720 --> 00:00:21,910
bias in AI has become a hot topic.

9
00:00:21,910 --> 00:00:25,690
Most research focuses on
bias applied to humans.

10
00:00:25,690 --> 00:00:28,290
Now, should I be wary of AI?

11
00:00:28,290 --> 00:00:29,122
Right?

12
00:00:29,123 --> 00:00:32,020
Of key concern is whether a
model could potentially harm

13
00:00:32,020 --> 00:00:33,830
an underprivileged group.

14
00:00:33,830 --> 00:00:36,760
Suppose a university decide
to take historical admissions

15
00:00:36,760 --> 00:00:40,879
data and build a model to
automate decision making processes

16
00:00:40,880 --> 00:00:42,670
for incoming freshmen.

17
00:00:42,670 --> 00:00:44,930
How do you think we could
improve that decision making

18
00:00:44,930 --> 00:00:47,010
and check if there are any human errors

19
00:00:47,010 --> 00:00:48,363
or bias introduced in it?

20
00:00:49,250 --> 00:00:51,350
Now, if you look closer at the data set

21
00:00:51,350 --> 00:00:52,730
over the last 10 years,

22
00:00:52,730 --> 00:00:55,080
the university has admitted

23
00:00:55,080 --> 00:00:57,100
twice more than men than women.

24
00:00:57,100 --> 00:00:58,990
Now, this data set has bias

25
00:00:58,990 --> 00:01:01,540
from previous humans making decisions.

26
00:01:01,540 --> 00:01:04,212
What happens when we train
a model on that bias?

27
00:01:05,060 --> 00:01:07,070
The model will essentially
learned that bias

28
00:01:07,070 --> 00:01:09,809
and this will be present
in its classification.

29
00:01:09,810 --> 00:01:12,140
So the algorithm will learn to accept

30
00:01:12,140 --> 00:01:14,550
more men applications over women.

31
00:01:14,550 --> 00:01:17,408
Now, this problem is wide reaching.

32
00:01:17,408 --> 00:01:19,610
Analyst report recently did a deep dive

33
00:01:19,610 --> 00:01:21,570
on facial recognition models

34
00:01:21,570 --> 00:01:23,729
and found that many exhibit bias

35
00:01:23,730 --> 00:01:27,170
towards the races of countries
that they were trained on.

36
00:01:27,170 --> 00:01:28,430
Facial recognition systems

37
00:01:28,430 --> 00:01:30,010
or artificial recognition technologies

38
00:01:30,010 --> 00:01:34,180
have been used by police forces
for more than two decades.

39
00:01:34,180 --> 00:01:36,360
Recent studies by MIT analyst

40
00:01:36,360 --> 00:01:39,380
have found that while the
technology works literally well

41
00:01:39,380 --> 00:01:41,009
on Caucasian men,

42
00:01:41,010 --> 00:01:43,970
the results are less accurate
in other demographics.

43
00:01:43,970 --> 00:01:47,080
In part where it lacks diversity.

44
00:01:47,080 --> 00:01:48,750
There is another work by Coded Bias,

45
00:01:48,750 --> 00:01:51,020
which is premiered on
Sundance Film Festival,

46
00:01:51,020 --> 00:01:53,050
and also which is
available now on Netflix.

47
00:01:53,050 --> 00:01:55,149
It follows the MIT's work,

48
00:01:55,150 --> 00:01:57,590
or researchers work in
her exploration of bias

49
00:01:57,590 --> 00:02:00,060
in facial recognition technologies.

50
00:02:00,060 --> 00:02:02,930
Now, she started
investigating this bias in AI

51
00:02:02,930 --> 00:02:05,790
after noticing that a
facial recognition program

52
00:02:05,790 --> 00:02:07,970
cannot identify her face.

53
00:02:07,970 --> 00:02:09,440
As a woman of color,

54
00:02:09,440 --> 00:02:11,690
the technology only registers the face

55
00:02:11,690 --> 00:02:13,390
when she puts a white mask.

56
00:02:13,390 --> 00:02:16,966
And after further investigation,
she detected a pattern

57
00:02:16,967 --> 00:02:19,500
and a faulty on facial
recognition technology

58
00:02:19,500 --> 00:02:21,960
and an AI technology, tends to be bias

59
00:02:21,960 --> 00:02:24,630
against historically marginalized people.

60
00:02:24,630 --> 00:02:28,240
Now let's move on to
ingrained algorithmic bias.

61
00:02:28,240 --> 00:02:29,260
What does it mean?

62
00:02:29,260 --> 00:02:32,590
Now, Duke University researchers
have created an algorithm.

63
00:02:32,590 --> 00:02:34,850
An AI algorithm called PULSE.

64
00:02:34,850 --> 00:02:37,239
That pixelates a picture of a face

65
00:02:37,240 --> 00:02:40,010
and then explores a
range of possibilities.

66
00:02:40,010 --> 00:02:42,820
Which means it is
computer generated images

67
00:02:42,820 --> 00:02:45,049
that can produce pixelated face.

68
00:02:45,050 --> 00:02:48,300
Now the input can be a low
resolution input image.

69
00:02:48,300 --> 00:02:52,370
The output is a StyleGAN
high resolution image.

70
00:02:52,370 --> 00:02:56,570
Which is perceptional,
realistically and downscale image.

71
00:02:56,570 --> 00:02:58,650
Now, one troop of Hollywood spy movies

72
00:02:58,650 --> 00:03:00,940
has taken a turn towards this, I feel.

73
00:03:00,940 --> 00:03:03,300
You can create a very high quality image

74
00:03:03,300 --> 00:03:06,060
from a blurry or a grainy pixelated image.

75
00:03:06,060 --> 00:03:09,470
Now AI has delivered
part of this from that.

76
00:03:09,470 --> 00:03:12,900
As you can notice, AI
generated photo-realistic image

77
00:03:12,900 --> 00:03:15,920
from low resolution image,
actually looks very similar

78
00:03:15,920 --> 00:03:17,440
to the person that is being given.

79
00:03:17,440 --> 00:03:20,810
So for example, the image on
the left of the Mona Lisa,

80
00:03:20,810 --> 00:03:22,830
there is actually no resemblance

81
00:03:22,830 --> 00:03:24,960
from the image on the right that you see.

82
00:03:24,960 --> 00:03:26,530
Like the image that is being created

83
00:03:26,530 --> 00:03:27,860
from the low resolution image,

84
00:03:27,860 --> 00:03:30,420
only contains so much information.

85
00:03:30,420 --> 00:03:33,510
However convincingly the AI
renders that imaginative face

86
00:03:33,510 --> 00:03:35,250
and the computer-generated face

87
00:03:35,250 --> 00:03:37,700
can be quite uncanny these days.

88
00:03:37,700 --> 00:03:40,750
But there is no dodging the
fact that the original image

89
00:03:40,750 --> 00:03:42,763
was actually very information sparse.

90
00:03:43,600 --> 00:03:44,900
Move on to the next slide.

91
00:03:48,520 --> 00:03:52,190
So this is an example of
how AI turned Barack Obama

92
00:03:52,190 --> 00:03:53,320
into a white man.

93
00:03:53,320 --> 00:03:56,530
The input is actually
a low resolution image

94
00:03:56,530 --> 00:03:59,800
of Barack Obama, which was
incorporated into an algorithm

95
00:03:59,800 --> 00:04:02,690
designed to generate pixelated faces.

96
00:04:02,690 --> 00:04:04,410
The result was being a white man

97
00:04:04,410 --> 00:04:07,700
and the newly published
and reproduce image

98
00:04:07,700 --> 00:04:11,320
clearly illustrates the
bias in AI research.

99
00:04:11,320 --> 00:04:15,739
Bias is not just about Obama
or any actress or anything,

100
00:04:15,740 --> 00:04:17,019
someone specific like that.

101
00:04:17,019 --> 00:04:19,919
The point is, what causes these outputs?

102
00:04:19,920 --> 00:04:22,540
And how does AI bias occur?

103
00:04:22,540 --> 00:04:24,700
For that, first we need to know a bit more

104
00:04:24,700 --> 00:04:27,219
about the technology
that is being used here.

105
00:04:27,220 --> 00:04:28,980
The software that generates these images,

106
00:04:28,980 --> 00:04:31,950
mixed with the technology
that I mentioned previously,

107
00:04:31,950 --> 00:04:33,440
which is called PULSE.

108
00:04:33,440 --> 00:04:36,090
Which mainly uses a technique
that is like upgrading

109
00:04:36,090 --> 00:04:38,469
the level of visual data processing.

110
00:04:38,470 --> 00:04:42,580
Upgrading is like zooming or
optimizing that we see on TV,

111
00:04:42,580 --> 00:04:44,430
but unlike Hollywood movies,

112
00:04:44,430 --> 00:04:47,410
new programs cannot
create data from nothing.

113
00:04:47,410 --> 00:04:51,300
These programs employ ML
models to fill in those blanks

114
00:04:51,300 --> 00:04:53,510
and convert a low resolution image

115
00:04:53,510 --> 00:04:55,170
to a high resolution image.

116
00:04:55,170 --> 00:04:56,360
Now to do this,

117
00:04:56,360 --> 00:04:58,390
the technology made use
of another algorithm,

118
00:04:58,390 --> 00:04:59,450
which is called StyleGAN,

119
00:04:59,450 --> 00:05:01,973
which is mainly responsible
for creating very realistic

120
00:05:01,973 --> 00:05:05,640
looking faces for the people
who are not even present.

121
00:05:05,640 --> 00:05:09,710
If you wish to see more examples
of StyleGAN created images,

122
00:05:09,710 --> 00:05:11,219
please feel free to refer this website.

123
00:05:11,220 --> 00:05:12,890
This person does not exist.

124
00:05:12,890 --> 00:05:15,030
Which creates very realistic looking faces

125
00:05:15,030 --> 00:05:18,059
that are often used to create
fake social media profiles.

126
00:05:18,060 --> 00:05:19,020
This is part of the technology

127
00:05:19,020 --> 00:05:20,919
which is used for creating deep fakes.

128
00:05:20,920 --> 00:05:23,040
Now, this technology creator said that,

129
00:05:23,040 --> 00:05:26,790
when using this algorithm to
extend the range of pixels,

130
00:05:26,790 --> 00:05:29,330
the algorithm actually generated faces

131
00:05:29,330 --> 00:05:31,020
with Caucasian features.

132
00:05:31,020 --> 00:05:34,099
It created less faces
with people of color.

133
00:05:34,100 --> 00:05:36,650
So this bias is actually inherited

134
00:05:36,650 --> 00:05:39,270
from the StyleGAN bias set of data.

135
00:05:39,270 --> 00:05:41,750
Which means there might be other factors

136
00:05:41,750 --> 00:05:44,160
that they don't realize,
but the training data,

137
00:05:44,160 --> 00:05:46,440
it had an in built bias in it.

138
00:05:46,440 --> 00:05:49,640
So in other words, given
the nature of the data

139
00:05:49,640 --> 00:05:51,349
that StyleGAN has been trained on,

140
00:05:51,350 --> 00:05:53,870
whenever it tries to create a face

141
00:05:53,870 --> 00:05:55,900
that looks like a data input image,

142
00:05:55,900 --> 00:05:58,880
it automatically orients
into white shapes.

143
00:05:58,880 --> 00:06:01,090
Which means that whenever you're deploying

144
00:06:01,090 --> 00:06:04,263
any kind of technology like
this, it has to be well tested.

145
00:06:05,100 --> 00:06:08,430
Now moving on to the part
as to why AI must be ethical

146
00:06:08,430 --> 00:06:10,790
and why we as, you know,
security professionals

147
00:06:10,790 --> 00:06:12,460
must be considered toward it.

148
00:06:12,460 --> 00:06:15,430
So first part I think is
on facial recognition.

149
00:06:15,430 --> 00:06:17,730
Now facial recognition
has improved over time.

150
00:06:17,730 --> 00:06:19,970
I think probably often a reliable tool

151
00:06:19,970 --> 00:06:21,830
and it does make mistakes.

152
00:06:21,830 --> 00:06:25,251
Now like humans, it can make
mistakes in interpreting

153
00:06:25,251 --> 00:06:29,467
the results produced by the
facial recognition tool.

154
00:06:29,467 --> 00:06:33,900
If you train the model poorly,
AI can develop strong bias.

155
00:06:33,900 --> 00:06:36,210
Now this can go for many
other variables, you know,

156
00:06:36,210 --> 00:06:39,710
beyond gender, ethnicity
or economic wealth.

157
00:06:39,710 --> 00:06:42,180
It can go to any other
variable imaginable.

158
00:06:42,180 --> 00:06:45,650
So since AI is mainly
trained on historical data,

159
00:06:45,650 --> 00:06:49,010
it will learn the decisions
based on those ideas.

160
00:06:49,010 --> 00:06:51,230
Additionally, there's
something called over-reliance.

161
00:06:51,230 --> 00:06:53,990
What it means is that,
AI can make mistakes

162
00:06:53,990 --> 00:06:56,020
and they're two critical mistakes.

163
00:06:56,020 --> 00:07:00,130
So one must note that a
well-trained AI will often be better

164
00:07:00,130 --> 00:07:03,560
in predicting data human,
but it can make mistakes.

165
00:07:03,560 --> 00:07:05,600
AI is an incredible technology

166
00:07:05,600 --> 00:07:07,760
which might provide fantastic benefits

167
00:07:07,760 --> 00:07:09,650
but it must be regulated.

168
00:07:09,650 --> 00:07:13,450
Additionally, I feel AI
must have an explainable log

169
00:07:13,450 --> 00:07:14,830
and it must be governed.

170
00:07:14,830 --> 00:07:19,120
Now, if you cannot explain your
model, you cannot trust it.

171
00:07:19,120 --> 00:07:21,070
AI as such, must be transparent

172
00:07:21,070 --> 00:07:22,710
and humans must be able to know

173
00:07:22,710 --> 00:07:25,823
as to why AI has made
that particular decision.

174
00:07:26,670 --> 00:07:30,100
Now, humans cannot
completely over rely on AI.

175
00:07:30,100 --> 00:07:32,790
Our future will undoubtedly
consist of human machine

176
00:07:32,790 --> 00:07:33,670
collaboration,

177
00:07:33,670 --> 00:07:36,730
but that doesn't mean that
people should over-rely

178
00:07:36,730 --> 00:07:38,860
on machines especially
when AI is dealing with

179
00:07:38,860 --> 00:07:40,640
human-making decisions.

180
00:07:40,640 --> 00:07:43,110
This is coming from a more generic part.

181
00:07:43,110 --> 00:07:45,640
Now you might be saying why as I,

182
00:07:45,640 --> 00:07:47,510
as a security professional,
should care about it.

183
00:07:47,510 --> 00:07:50,580
I don't train models to
determine whether someone

184
00:07:50,580 --> 00:07:52,950
gets a loan or someone goes to a college.

185
00:07:52,950 --> 00:07:55,670
The only data maybe I look
at is malware classification

186
00:07:55,670 --> 00:07:57,200
for file base protection.

187
00:07:57,200 --> 00:07:58,920
Now, while that might be true,

188
00:07:58,920 --> 00:08:01,010
bias is important for every profession,

189
00:08:01,010 --> 00:08:03,570
whether you are dealing
with humans or not.

190
00:08:03,570 --> 00:08:06,010
So as security professional as such,

191
00:08:06,010 --> 00:08:08,887
we approach the model
building with our own biases.

192
00:08:08,887 --> 00:08:11,474
There are a number of
static dynamic features

193
00:08:11,474 --> 00:08:13,090
that we collect say from file.

194
00:08:13,090 --> 00:08:15,460
It is also possible that
we might be bringing

195
00:08:15,460 --> 00:08:18,530
our own personal bias that
we introduced to the model.

196
00:08:18,530 --> 00:08:21,200
So for example, let's talk
about the source data.

197
00:08:21,200 --> 00:08:23,430
The data can create bias
when the source models are,

198
00:08:23,430 --> 00:08:26,790
source materials rather,
aren't fairly diagnosed.

199
00:08:26,790 --> 00:08:29,800
What it means is that the AI
that is fed to the bias data

200
00:08:29,800 --> 00:08:33,020
is going to understand only
a partial view of the word

201
00:08:33,020 --> 00:08:36,380
and make decisions based on
that narrow understanding.

202
00:08:36,380 --> 00:08:37,600
To give an example,

203
00:08:37,600 --> 00:08:40,820
say you have a spam
classifier and it was trained

204
00:08:40,820 --> 00:08:43,669
on some representative
set of benign classifies.

205
00:08:43,669 --> 00:08:44,502
It wasn't rather,

206
00:08:44,503 --> 00:08:47,010
trained on a representative
set of benign emails.

207
00:08:47,010 --> 00:08:49,300
Suppose you have emails
off different languages

208
00:08:49,300 --> 00:08:51,520
or something that makes use of slang.

209
00:08:51,520 --> 00:08:54,530
So inevitably, will provide falses,

210
00:08:54,530 --> 00:08:57,150
when you actually see that
in the real test data.

211
00:08:57,150 --> 00:09:00,959
Even commonly used,
misintentional use of grammar

212
00:09:00,960 --> 00:09:02,750
or spelling, several syntax,

213
00:09:02,750 --> 00:09:05,780
might actually cause your
spam classifier to block

214
00:09:05,780 --> 00:09:07,160
some benign text.

215
00:09:07,160 --> 00:09:08,300
So that being said,

216
00:09:08,300 --> 00:09:10,189
this is something that can be introduced.

217
00:09:10,190 --> 00:09:13,520
Additionally, not all
your files are equal.

218
00:09:13,520 --> 00:09:16,410
So for human data, you
have privileged class,

219
00:09:16,410 --> 00:09:17,980
you have unprivileged class.

220
00:09:17,980 --> 00:09:21,900
Now if you build a model that
supports or favors rather,

221
00:09:21,900 --> 00:09:23,829
you're privileged class unfairly.

222
00:09:23,830 --> 00:09:27,260
We further hurt the unprivileged minority.

223
00:09:27,260 --> 00:09:28,189
Now, for example,

224
00:09:28,190 --> 00:09:31,610
let me give a very similar
example in security domain.

225
00:09:31,610 --> 00:09:32,443
Say you have P files,

226
00:09:32,443 --> 00:09:35,180
which are the most commonly used malwares.

227
00:09:35,180 --> 00:09:38,939
So if they're P files in your
dataset than other samples,

228
00:09:38,940 --> 00:09:41,220
potentially we might be
able to create a model

229
00:09:41,220 --> 00:09:43,260
which favors P samples more.

230
00:09:43,260 --> 00:09:46,700
Now, does that mean that your
detected malicious samples

231
00:09:46,700 --> 00:09:49,980
of other class like BB.
Does that make it harder?

232
00:09:49,980 --> 00:09:52,890
Or does that make open your
model to more vulnerabilities?

233
00:09:52,890 --> 00:09:56,800
Now while our data might not
directly have human related

234
00:09:56,800 --> 00:10:00,002
features our models do impact humans.

235
00:10:01,030 --> 00:10:02,920
Now next we want to,

236
00:10:02,920 --> 00:10:06,790
how do we connect
explainability on to bias?

237
00:10:06,790 --> 00:10:10,880
And how explainability as a
concept can be used to improve

238
00:10:10,880 --> 00:10:13,120
or you know explain better.

239
00:10:13,120 --> 00:10:15,370
Now this slide is taken from a paper.

240
00:10:15,370 --> 00:10:17,020
The work is taken from a paper.

241
00:10:17,020 --> 00:10:19,210
But what this essentially tries to do is,

242
00:10:19,210 --> 00:10:20,570
it tries to draw connections

243
00:10:20,570 --> 00:10:23,340
between model insights and explanations.

244
00:10:23,340 --> 00:10:24,700
So what we are trying to do here is,

245
00:10:24,700 --> 00:10:27,890
we're trying to understand XAI and bias

246
00:10:27,890 --> 00:10:30,170
from a very cognitive perspective.

247
00:10:30,170 --> 00:10:32,319
It draws connections
between model insights,

248
00:10:32,320 --> 00:10:35,650
explanations that the AI
algorithm commonly produce,

249
00:10:35,650 --> 00:10:38,120
and in turn proposes
a term which is called

250
00:10:38,120 --> 00:10:40,190
a User-centric XAI framework.

251
00:10:40,190 --> 00:10:41,980
The image that you see on the right.

252
00:10:41,980 --> 00:10:45,070
Now within this framework,
using framework like this,

253
00:10:45,070 --> 00:10:47,630
an XAI researcher or even a designer

254
00:10:47,630 --> 00:10:49,340
can actually identify pathways

255
00:10:50,520 --> 00:10:53,050
around which say human cognitive patterns

256
00:10:53,050 --> 00:10:55,530
can drive for building XAI models

257
00:10:55,530 --> 00:10:59,730
and how XAI can be used to
mitigate any kind of bias.

258
00:10:59,730 --> 00:11:02,520
So for doing this, there
are four key aspects.

259
00:11:02,520 --> 00:11:04,170
The four key aspects that you see

260
00:11:05,019 --> 00:11:08,700
on the left on the bottom.

261
00:11:08,700 --> 00:11:11,343
One, is how do people usually reason?

262
00:11:12,628 --> 00:11:14,130
How should people reason?

263
00:11:14,130 --> 00:11:16,090
And how do people do it right now?

264
00:11:16,090 --> 00:11:18,820
And then we'll talk about,
take an example from each,

265
00:11:18,820 --> 00:11:21,970
as to how explanations are
currently being generated

266
00:11:21,970 --> 00:11:23,870
and then how XAI can support

267
00:11:23,870 --> 00:11:25,520
that reasoning to make it better.

268
00:11:26,370 --> 00:11:28,750
So first point, how do people,

269
00:11:28,750 --> 00:11:31,200
how should people rather
reason or explain?

270
00:11:31,200 --> 00:11:33,720
So if I can put it in a very cognitive way

271
00:11:33,720 --> 00:11:37,620
people reason using deduction,
induction or abduction.

272
00:11:37,620 --> 00:11:40,853
So deduction reasoning is more
like a top-down reasoning.

273
00:11:40,853 --> 00:11:44,770
It's a process of reasoning
from a premise to a conclusion.

274
00:11:44,770 --> 00:11:47,860
Like you follow some kind of
rule boundaries or something.

275
00:11:47,860 --> 00:11:48,700
Another form of inference is,

276
00:11:48,700 --> 00:11:51,610
inference with the best explanation.

277
00:11:51,610 --> 00:11:53,660
There you are making use
of some kind clustering.

278
00:11:53,660 --> 00:11:55,900
You are trying to see
whether your observation

279
00:11:55,900 --> 00:11:58,640
is more, you know, closer
to other observations

280
00:11:58,640 --> 00:12:00,117
and making use of clustering.

281
00:12:00,118 --> 00:12:02,820
As we approach I'll be
able to explain that.

282
00:12:02,820 --> 00:12:04,740
But how do people actually reason?

283
00:12:04,740 --> 00:12:06,440
What causes the error?

284
00:12:06,440 --> 00:12:10,210
So, usually people ended up
doing a very fast intuitive

285
00:12:10,210 --> 00:12:11,713
or a low effort approach.

286
00:12:11,713 --> 00:12:15,040
Wherein we might employ
heuristics to make decisions.

287
00:12:15,040 --> 00:12:15,872
So for example,

288
00:12:15,873 --> 00:12:18,397
I'm given a task to
quickly recognize an item.

289
00:12:18,397 --> 00:12:22,330
We might apply some kind of
representative heuristics

290
00:12:22,330 --> 00:12:25,210
to detect, say the reasoning to compare it

291
00:12:25,210 --> 00:12:26,493
with other elements.

292
00:12:26,493 --> 00:12:28,050
Have I seen that element before?

293
00:12:28,050 --> 00:12:30,089
Have I seen something similar before?

294
00:12:30,090 --> 00:12:33,600
So I'm trying to determine the
similarity of that test case

295
00:12:33,600 --> 00:12:35,550
to what previously observed item.

296
00:12:35,550 --> 00:12:38,910
So an experienced person who
has seen many examples of this

297
00:12:38,910 --> 00:12:40,380
and learn from generalizing it

298
00:12:40,380 --> 00:12:42,180
can make a decision pretty quickly.

299
00:12:42,180 --> 00:12:45,197
However, this can lead to
something called cognitive bias

300
00:12:45,197 --> 00:12:46,660
for inexperienced people.

301
00:12:46,660 --> 00:12:49,240
That is termed as representative bias.

302
00:12:49,240 --> 00:12:51,210
Now, as I said, I pick one example

303
00:12:51,210 --> 00:12:54,130
and I'll walk through how
each of these can help

304
00:12:54,130 --> 00:12:56,160
you mitigate that.

305
00:12:56,160 --> 00:12:58,579
So now we have moving on to how, you know,

306
00:12:58,580 --> 00:13:01,020
how XAI generates explanations.

307
00:13:01,020 --> 00:13:04,960
So for XAI as such,
explainability, as a concept,

308
00:13:04,960 --> 00:13:06,480
it's used to provide support

309
00:13:06,480 --> 00:13:09,140
and transparency where users can see

310
00:13:09,140 --> 00:13:11,140
some aspect of the interstate

311
00:13:11,140 --> 00:13:13,260
or the functioning of an AI system.

312
00:13:13,260 --> 00:13:16,580
Now, say example, if you are
using AI for detection aid,

313
00:13:16,580 --> 00:13:19,340
a user might want to see explanations

314
00:13:19,340 --> 00:13:21,040
to improve the decision-making.

315
00:13:21,040 --> 00:13:24,469
Say a system behave unexpectedly
or giving some errors.

316
00:13:24,470 --> 00:13:27,820
You might want to understand
explanations for scrutiny

317
00:13:27,820 --> 00:13:31,370
or debugging to be able to
identify the offending fault

318
00:13:31,370 --> 00:13:33,050
or take control to make corrections.

319
00:13:33,050 --> 00:13:36,693
So that's how usually XAI
takes, XAI generates prediction

320
00:13:36,693 --> 00:13:38,500
and it can use many algorithms.

321
00:13:38,500 --> 00:13:40,100
You can use clustering based approach.

322
00:13:40,100 --> 00:13:43,310
You can use algorithms
which are local or global

323
00:13:43,310 --> 00:13:45,560
depending on the use
case that you're working.

324
00:13:45,560 --> 00:13:47,869
Now with that being
said, how can you target,

325
00:13:47,870 --> 00:13:50,830
how can you make use of XAI
to support the reasoning

326
00:13:50,830 --> 00:13:52,420
and mitigate those bias?

327
00:13:52,420 --> 00:13:56,507
So as I said, let's pick up the
case of representative bias.

328
00:13:56,507 --> 00:13:58,770
Now representative bias, as we saw before,

329
00:13:58,770 --> 00:14:01,317
it mainly happens when a decision maker

330
00:14:01,317 --> 00:14:04,970
perceives a current situation
similar to say a wrong case

331
00:14:04,970 --> 00:14:06,710
of a wrong classification.

332
00:14:06,710 --> 00:14:08,910
This might be due to lack of experience

333
00:14:08,910 --> 00:14:12,439
of seeing those examples
before and even lack of focus.

334
00:14:12,440 --> 00:14:15,122
And this can happen also with
the machine learning model.

335
00:14:15,122 --> 00:14:16,610
Where the model actually picks up

336
00:14:16,610 --> 00:14:18,810
the wrong selling features, right?

337
00:14:18,810 --> 00:14:20,310
So to mitigate this kind of case,

338
00:14:20,310 --> 00:14:23,199
we can use something
called prototype instance.

339
00:14:23,200 --> 00:14:26,990
What it does is, it
represents different outcomes

340
00:14:26,990 --> 00:14:28,055
by rank ordering.

341
00:14:28,055 --> 00:14:29,382
You can create a rank ordering

342
00:14:29,382 --> 00:14:31,857
by the similarity of that test case

343
00:14:31,857 --> 00:14:36,170
by explicitly showing some
kind of similarity metric.

344
00:14:36,170 --> 00:14:39,260
So to allow those comparison
between case by case

345
00:14:39,260 --> 00:14:42,540
by inspecting features, the
difference between value,

346
00:14:42,540 --> 00:14:45,010
you can see contrast as to difference

347
00:14:45,010 --> 00:14:47,110
between that particular
test case to other cases.

348
00:14:47,110 --> 00:14:50,600
And that might help if you
can mitigate bias in some way.

349
00:14:50,600 --> 00:14:53,030
So Amanda will now talk
about where does bias exists

350
00:14:53,030 --> 00:14:56,453
in detail and how we would
measure it and mitigate bias.

351
00:14:58,570 --> 00:14:59,639
- Thanks Sherin.

352
00:14:59,639 --> 00:15:02,510
So now that Sherin has given
us a great overview of bias

353
00:15:02,510 --> 00:15:04,510
and why we should be concerned about bias.

354
00:15:04,510 --> 00:15:07,420
I'd like to dive into talking
about where bias exists

355
00:15:07,420 --> 00:15:08,810
and then on the following slides

356
00:15:08,810 --> 00:15:10,310
also how to measure that bias

357
00:15:10,310 --> 00:15:12,142
and how to mitigate bias as well.

358
00:15:13,290 --> 00:15:18,290
So the place bias primarily
exists would be in the data.

359
00:15:19,490 --> 00:15:21,760
So there's three places the bias can exist

360
00:15:21,760 --> 00:15:25,300
in the first place that we
were gonna look at is the data.

361
00:15:25,300 --> 00:15:28,870
An example of this relates
back to the documentary

362
00:15:28,870 --> 00:15:30,650
that Sherin had talked
about on the first slide

363
00:15:30,650 --> 00:15:32,470
called Coated Bias.

364
00:15:32,470 --> 00:15:35,420
And in this documentary an
algorithm bias researcher

365
00:15:35,420 --> 00:15:38,040
discovers that when she
tried out a smart mirror

366
00:15:38,040 --> 00:15:41,400
that use computer vision
software as a black woman,

367
00:15:41,400 --> 00:15:43,170
her face was not detected.

368
00:15:43,170 --> 00:15:45,310
However, when she held up a white mask

369
00:15:45,310 --> 00:15:47,800
the mirror finally detected her.

370
00:15:47,800 --> 00:15:49,740
And the most likely culprit for this era

371
00:15:49,740 --> 00:15:52,860
was of the data that was
used to train the mirror.

372
00:15:52,860 --> 00:15:55,030
Didn't have a diverse set of images.

373
00:15:55,030 --> 00:15:56,910
So NIS actually has a great report

374
00:15:56,910 --> 00:15:59,219
that discusses the facial
recognition algorithms

375
00:15:59,220 --> 00:16:02,053
can be biased towards the
regions they are trained in.

376
00:16:03,180 --> 00:16:05,719
This is because most algorithms
are trained on pictures

377
00:16:05,720 --> 00:16:07,950
that represent the people in that region.

378
00:16:07,950 --> 00:16:10,500
And it is highly likely the
mirror was trained on pictures

379
00:16:10,500 --> 00:16:12,400
only from a small set of races

380
00:16:12,400 --> 00:16:14,130
and did not contain enough pictures

381
00:16:14,130 --> 00:16:16,150
of African-American people.

382
00:16:16,150 --> 00:16:18,430
This shows that the data
used to train the model

383
00:16:18,430 --> 00:16:20,713
was potentially not diverse enough.

384
00:16:21,550 --> 00:16:25,349
The second place that we can
have bias exists is in people.

385
00:16:25,350 --> 00:16:27,180
So people are the ones who train models

386
00:16:27,180 --> 00:16:29,699
and curate the data used to train models.

387
00:16:29,700 --> 00:16:32,780
As people, we all bring our
own biases to model building.

388
00:16:32,780 --> 00:16:34,500
An example of this as an algorithm

389
00:16:34,500 --> 00:16:36,360
that the University of Texas at Austin

390
00:16:36,360 --> 00:16:38,110
used to grade applicants

391
00:16:38,110 --> 00:16:40,380
to the Computer Science PhD program.

392
00:16:40,380 --> 00:16:43,336
So in 2013 UT started using
a machine learning system

393
00:16:43,336 --> 00:16:45,780
called GRADE and GRADE stands for

394
00:16:45,780 --> 00:16:48,020
GRaduate ADmissions Evaluator.

395
00:16:48,020 --> 00:16:50,040
And it was created by a UT faculty member

396
00:16:50,040 --> 00:16:52,310
and graduate student in Computer Science.

397
00:16:52,310 --> 00:16:53,760
And it was originally created to help

398
00:16:53,760 --> 00:16:55,230
the graduate admissions committee

399
00:16:55,230 --> 00:16:57,460
in the department save time.

400
00:16:57,460 --> 00:17:00,090
So GRADE predicts how likely
the admissions committee is

401
00:17:00,090 --> 00:17:02,830
to approve an applicant and
it expresses that prediction

402
00:17:02,830 --> 00:17:05,140
as a numerical score out of five.

403
00:17:05,140 --> 00:17:08,440
The system also explains
what factors most impacted

404
00:17:08,440 --> 00:17:09,819
its decision.

405
00:17:09,819 --> 00:17:12,760
GRADES creators have said that
the system is only programmed

406
00:17:12,760 --> 00:17:15,010
to replicate what the admissions
committee was doing prior

407
00:17:15,010 --> 00:17:19,470
to 2013, not to make better
decisions than humans could.

408
00:17:19,470 --> 00:17:20,869
And the system isn't programmed

409
00:17:20,869 --> 00:17:23,760
to use race or gender
to make its predictions.

410
00:17:23,760 --> 00:17:26,190
And in fact, they said, when
it's given those features

411
00:17:26,190 --> 00:17:28,563
as options, it actually
weights them as zero.

412
00:17:29,549 --> 00:17:32,230
GRADES creators have said
that this is evidence

413
00:17:32,230 --> 00:17:35,390
that the committee's decisions
are gender and race neutral.

414
00:17:35,390 --> 00:17:37,420
However, something that
should be considered

415
00:17:37,420 --> 00:17:39,840
is it's possible that the
admissions committee prior

416
00:17:39,840 --> 00:17:44,260
to 2013 that the data source
form brought their own biases

417
00:17:44,260 --> 00:17:47,030
to the selection process and
those biases were then coded

418
00:17:47,030 --> 00:17:50,040
into the model by using that past data.

419
00:17:50,040 --> 00:17:51,639
This could harm minority classes

420
00:17:51,640 --> 00:17:55,163
if biases prior to 2013 existed
in the admissions committee.

421
00:17:57,900 --> 00:18:01,940
And finally, the last
place to look for bias

422
00:18:01,940 --> 00:18:03,340
is in the model itself.

423
00:18:03,340 --> 00:18:06,429
And so a great example of this is actually

424
00:18:06,430 --> 00:18:09,870
the Twitter bot Tay created by Microsoft.

425
00:18:09,870 --> 00:18:12,196
So prior to releasing Tay,

426
00:18:12,196 --> 00:18:14,530
Microsoft made sure that Tay was trained

427
00:18:14,530 --> 00:18:15,670
on a diverse set of data.

428
00:18:15,670 --> 00:18:17,930
And initially when she
started interacting with users

429
00:18:17,930 --> 00:18:20,870
on Twitter, her tweets
were mostly harmless.

430
00:18:20,870 --> 00:18:23,649
However, after some users
shared racist language with Tay

431
00:18:23,650 --> 00:18:25,650
she ended up picking up that information

432
00:18:25,650 --> 00:18:28,840
and started tweeting
racist content herself.

433
00:18:28,840 --> 00:18:31,669
She also had a feature where
you could essentially tell her

434
00:18:31,670 --> 00:18:34,330
to repeat the exact tweet
that you tweeted at her.

435
00:18:34,330 --> 00:18:36,550
And she would repeat anything you said.

436
00:18:36,550 --> 00:18:38,700
And some users of course manipulated this

437
00:18:38,700 --> 00:18:41,150
to have her tweet racist stuff.

438
00:18:41,150 --> 00:18:44,370
So these are some examples
of where bias exists.

439
00:18:44,370 --> 00:18:47,773
Now let's talk about
how we can measure bias.

440
00:18:49,120 --> 00:18:51,120
So there are numerous
statistics and metrics

441
00:18:51,120 --> 00:18:52,870
that can be used to measure bias.

442
00:18:52,870 --> 00:18:54,800
However, I have highlighted a few

443
00:18:54,800 --> 00:18:58,510
from an open source tool
called AI 360 Fairness tool.

444
00:18:58,510 --> 00:18:59,850
And while the tool has the ability

445
00:18:59,850 --> 00:19:02,080
to automatically calculate
these metrics for you,

446
00:19:02,080 --> 00:19:05,100
you can also hand calculate
all of these metrics.

447
00:19:05,100 --> 00:19:07,990
At their core the metrics are
based on the confusion matrix

448
00:19:07,990 --> 00:19:10,700
output, and knowing which
class is the favorable

449
00:19:10,700 --> 00:19:12,330
and unfavorable class.

450
00:19:12,330 --> 00:19:14,250
For example, if we were measuring bias

451
00:19:14,250 --> 00:19:17,050
in an algorithm that
determined college admittance

452
00:19:17,050 --> 00:19:19,840
and factored in race, our
favorable class might be white

453
00:19:19,840 --> 00:19:22,800
and are unfavorable class
might be African-Americans.

454
00:19:22,800 --> 00:19:25,240
Since in the past,
African-Americans may have received

455
00:19:25,240 --> 00:19:28,270
unfavorable admissions decisions.

456
00:19:28,270 --> 00:19:30,690
In security we have to be
a little bit more creative

457
00:19:30,690 --> 00:19:32,730
with this and think of our favorable class

458
00:19:32,730 --> 00:19:35,390
as something that maybe we
typically have more data

459
00:19:35,390 --> 00:19:38,110
or more experience with like PE files

460
00:19:38,110 --> 00:19:39,770
and are unfavorable class

461
00:19:39,770 --> 00:19:41,980
might be something that
we have less data for

462
00:19:41,980 --> 00:19:45,020
or that our model has
performed worse in the past

463
00:19:45,020 --> 00:19:46,940
such as .net files.

464
00:19:46,940 --> 00:19:48,580
All of these metrics have a range

465
00:19:48,580 --> 00:19:50,669
in which if the output falls,

466
00:19:50,670 --> 00:19:53,010
the model is determined to be fair.

467
00:19:53,010 --> 00:19:54,850
For example, we'll take the first metrics,

468
00:19:54,850 --> 00:19:56,629
statistical parity difference.

469
00:19:56,630 --> 00:20:00,990
It has a fair range of
negative 0.1 and 0.1.

470
00:20:00,990 --> 00:20:03,390
Where anything in that
range is considered fair.

471
00:20:04,240 --> 00:20:07,350
So if we get a statistical
parody difference of 0.2

472
00:20:08,490 --> 00:20:11,260
then we can conclude that
bias exists in our model

473
00:20:11,260 --> 00:20:14,629
because it resides
outside of the fair range.

474
00:20:14,630 --> 00:20:17,000
Each of these metrics
has an ideal use case

475
00:20:17,000 --> 00:20:18,810
where they would perform best.

476
00:20:18,810 --> 00:20:21,790
There are two opposing world views

477
00:20:21,790 --> 00:20:24,110
that we can use to kind
of group the applications

478
00:20:24,110 --> 00:20:25,409
of these metrics.

479
00:20:25,410 --> 00:20:27,930
The first one is we're all equal.

480
00:20:27,930 --> 00:20:30,460
And the second one is what
you see is what you get.

481
00:20:30,460 --> 00:20:32,340
The we're all equal worldview

482
00:20:32,340 --> 00:20:34,370
holds that all groups
have similar abilities

483
00:20:34,370 --> 00:20:36,120
with respect to the task

484
00:20:36,120 --> 00:20:38,649
even if we cannot observe this property.

485
00:20:38,650 --> 00:20:40,650
And the, what you see is
what you get worldview

486
00:20:40,650 --> 00:20:42,840
holds that the observations

487
00:20:42,840 --> 00:20:45,223
reflect ability with respect to the task.

488
00:20:46,410 --> 00:20:48,390
For example, if the application follows

489
00:20:48,390 --> 00:20:49,783
that we are all equal worldview

490
00:20:49,783 --> 00:20:52,310
then the demographic parity
metrics should be used

491
00:20:52,310 --> 00:20:55,470
like disparate impact and
statistical parity difference.

492
00:20:55,470 --> 00:20:57,040
If the application follows

493
00:20:57,040 --> 00:20:58,960
the what you see is what you get worldview

494
00:20:58,960 --> 00:21:00,974
than the equality of odds
metrics should be used

495
00:21:00,974 --> 00:21:04,389
such as average odds difference.

496
00:21:04,390 --> 00:21:08,130
Other group fairness metrics
lie between the two worldviews.

497
00:21:08,130 --> 00:21:10,780
In addition, there is also
the concept of group fairness

498
00:21:10,780 --> 00:21:12,760
versus individual fairness.

499
00:21:12,760 --> 00:21:14,920
Group fairness and it's broader sense

500
00:21:14,920 --> 00:21:16,740
partitions of population into groups.

501
00:21:16,740 --> 00:21:18,390
Defined by protected attributes

502
00:21:18,390 --> 00:21:20,730
and seeks for some statistical
measure to be equal

503
00:21:20,730 --> 00:21:22,690
across all the groups.

504
00:21:22,690 --> 00:21:24,210
Individual fairness on the other hand,

505
00:21:24,210 --> 00:21:26,580
and its broader sense seeks
for similar individuals

506
00:21:26,580 --> 00:21:28,122
to be treated similarity.

507
00:21:30,430 --> 00:21:33,210
If the application is
concerned with both individual

508
00:21:33,210 --> 00:21:35,800
and group fairness then
something like the field index

509
00:21:35,800 --> 00:21:36,842
should be used.

510
00:21:40,060 --> 00:21:41,899
So now that we know
how to measure our bias

511
00:21:41,900 --> 00:21:45,300
what do we do once we know that
bias exists within our data

512
00:21:45,300 --> 00:21:47,470
our model, our predictions?

513
00:21:47,470 --> 00:21:49,050
There's actually a few techniques

514
00:21:49,050 --> 00:21:50,750
that we can use to mitigate bias.

515
00:21:50,750 --> 00:21:52,400
And I have some listed on the slide

516
00:21:52,400 --> 00:21:54,080
but it's not an exhaustive list

517
00:21:54,080 --> 00:21:57,560
of all of the techniques that
can be used to mitigate bias.

518
00:21:57,560 --> 00:21:59,200
But these are some of the important ones

519
00:21:59,200 --> 00:22:00,550
that I wanted to highlight.

520
00:22:01,600 --> 00:22:03,250
The most important thing to remember

521
00:22:03,250 --> 00:22:05,630
is that mitigating bias
starts with your data.

522
00:22:05,630 --> 00:22:08,100
This is always the first
place you should look for bias

523
00:22:08,100 --> 00:22:11,399
and it's ground zero for
trying to mitigate bias.

524
00:22:11,400 --> 00:22:13,910
There are two techniques that
can be used to mitigate bias

525
00:22:13,910 --> 00:22:15,160
in your data.

526
00:22:15,160 --> 00:22:19,340
And these are re-weighting
and optimized pre-processing.

527
00:22:19,340 --> 00:22:22,250
Re-weighting generates weights
for the training examples

528
00:22:22,250 --> 00:22:26,230
in each group, differently
to ensure fairness

529
00:22:26,230 --> 00:22:27,563
before classification.

530
00:22:28,400 --> 00:22:29,760
Optimize pre-processing

531
00:22:29,760 --> 00:22:31,620
learns a probabilistic transformation

532
00:22:31,620 --> 00:22:34,070
that edits the features
and labels in the data

533
00:22:34,070 --> 00:22:36,270
with group fairness, individual distortion

534
00:22:36,270 --> 00:22:39,410
and data fidelity
constraints and objectives.

535
00:22:39,410 --> 00:22:41,810
In addition, you can look at
some more simplistic techniques

536
00:22:41,810 --> 00:22:43,050
to mitigate bias in your data

537
00:22:43,050 --> 00:22:45,580
such as under sampling and oversampling

538
00:22:45,580 --> 00:22:47,040
and also sourcing more data.

539
00:22:47,040 --> 00:22:48,629
And all of these can help you to ensure

540
00:22:48,630 --> 00:22:50,900
that you have a more balanced data set

541
00:22:50,900 --> 00:22:52,800
for particular features
that you were concerned

542
00:22:52,800 --> 00:22:54,013
about being biased.

543
00:22:55,530 --> 00:22:57,410
The next place that you
should look to mitigate bias,

544
00:22:57,410 --> 00:22:59,130
if the data approaches do not work

545
00:22:59,130 --> 00:23:02,473
or can't be implemented
is in your classifier.

546
00:23:03,330 --> 00:23:05,483
One, such example is
adversarial debiasing.

547
00:23:06,720 --> 00:23:09,180
And adversarial debiasing
learns a classifier

548
00:23:09,180 --> 00:23:11,170
to maximize prediction accuracy

549
00:23:11,170 --> 00:23:14,029
and simultaneously reduce
an adversary's ability

550
00:23:14,029 --> 00:23:17,800
to determine the protected
attribute from the predictions.

551
00:23:17,800 --> 00:23:19,950
This approach leads to a fair classifier.

552
00:23:19,950 --> 00:23:22,450
As a predictions can not
carry any group discrimination

553
00:23:22,450 --> 00:23:25,970
information that the adversary
can explore it in the future.

554
00:23:25,970 --> 00:23:28,570
Finally, the last place you
can look to mitigate bias

555
00:23:28,570 --> 00:23:31,040
is in the predictions themselves.

556
00:23:31,040 --> 00:23:34,290
An example of this is reject
option based classification

557
00:23:35,550 --> 00:23:37,470
and reject option based classification

558
00:23:37,470 --> 00:23:39,980
gives favorable outcomes
to unprivileged groups

559
00:23:39,980 --> 00:23:42,630
and unfavorable outcomes
to privileged groups

560
00:23:42,630 --> 00:23:45,150
and a confidence band around
the decision boundaries

561
00:23:45,150 --> 00:23:47,050
with the highest uncertainty.

562
00:23:47,050 --> 00:23:48,740
So again, these are just a few techniques

563
00:23:48,740 --> 00:23:50,240
that you can use to mitigate bias.

564
00:23:50,240 --> 00:23:51,650
It's not an exhaustive list

565
00:23:51,650 --> 00:23:53,750
and there are many more out there as well.

566
00:23:56,010 --> 00:23:58,379
So now that we know how to
measure and mitigate bias,

567
00:23:58,380 --> 00:24:01,270
let's apply this to a real world example

568
00:24:01,270 --> 00:24:04,170
to kinda see how you
would use them in action.

569
00:24:04,170 --> 00:24:06,080
So this example that I'm
going to be discussing

570
00:24:06,080 --> 00:24:08,340
is actually from a paper
that we have authored

571
00:24:08,340 --> 00:24:11,149
here at McAfee and is
currently under review

572
00:24:11,150 --> 00:24:13,620
and the paper details
algorithms that we created

573
00:24:13,620 --> 00:24:16,010
to detect deep fake images and videos.

574
00:24:16,010 --> 00:24:18,610
And it also introduces
a new deep fake data set

575
00:24:18,610 --> 00:24:20,760
with high quality images.

576
00:24:20,760 --> 00:24:22,610
And we wanted to make
sure to do an analysis

577
00:24:22,610 --> 00:24:25,020
on this new data set to
determine how diverse

578
00:24:25,020 --> 00:24:27,879
our images were to make sure
that our model wasn't biased

579
00:24:27,880 --> 00:24:30,770
towards certain ages, races, or genders.

580
00:24:30,770 --> 00:24:33,990
So what we did was we took
images created by styleGAN

581
00:24:33,990 --> 00:24:35,270
which were deep fake images

582
00:24:35,270 --> 00:24:38,100
because they're not images of real people.

583
00:24:38,100 --> 00:24:39,810
And then we took images of real people

584
00:24:39,810 --> 00:24:42,399
that we scraped from the
internet, and we had our data set.

585
00:24:42,400 --> 00:24:45,680
So we had the deep fake
images and the real images.

586
00:24:45,680 --> 00:24:47,170
We then took those images

587
00:24:47,170 --> 00:24:49,270
and passed them to an open source tool

588
00:24:49,270 --> 00:24:51,900
that you can find on
GitHub called DeepFace.

589
00:24:51,900 --> 00:24:54,620
And DeepFace is a lightweight
facial recognition

590
00:24:54,620 --> 00:24:58,580
and facial attribute analysis
framework for Python.

591
00:24:58,580 --> 00:25:01,189
And it can detect things
like age, gender, emotion

592
00:25:01,190 --> 00:25:04,600
and race, and it uses
state of the art models

593
00:25:04,600 --> 00:25:07,820
such as VGG-Face in order
to detect the faces.

594
00:25:07,820 --> 00:25:11,397
And the library is mainly
based off Kears and Tensorflow.

595
00:25:11,397 --> 00:25:13,179
And so what we did was
when we pass the images

596
00:25:13,180 --> 00:25:14,960
to this open source tool,

597
00:25:14,960 --> 00:25:18,270
it gave us a output of what it determined

598
00:25:18,270 --> 00:25:22,710
to be the race, age and gender
of the person in the photo.

599
00:25:22,710 --> 00:25:25,050
And this was easier than manually going

600
00:25:25,050 --> 00:25:27,700
and labeling all of our
data, all of our pictures

601
00:25:27,700 --> 00:25:29,350
because we had so many pictures.

602
00:25:29,350 --> 00:25:31,350
So this tool kind of automated the process

603
00:25:31,350 --> 00:25:33,649
and made it easier for us to get a feel

604
00:25:33,650 --> 00:25:35,473
of the diversity of our image sets.

605
00:25:37,140 --> 00:25:41,250
You can see an example of the
results that we had for race.

606
00:25:41,250 --> 00:25:43,810
So as you can see on the
slide, a lot of our images

607
00:25:43,810 --> 00:25:46,050
from the sub sample of
images in our dataset

608
00:25:46,050 --> 00:25:47,879
lean towards Caucasian

609
00:25:47,880 --> 00:25:49,410
and we didn't have as many images

610
00:25:49,410 --> 00:25:51,460
for African-Americans or Indians.

611
00:25:51,460 --> 00:25:52,700
And so we really focused

612
00:25:52,700 --> 00:25:56,030
on how can we mitigate
race in our algorithm

613
00:25:56,030 --> 00:25:58,963
and what can we do to kind
of mitigate this bias.

614
00:26:00,650 --> 00:26:03,280
So you can see on this slide

615
00:26:03,280 --> 00:26:05,911
the two tables detailing the results

616
00:26:05,911 --> 00:26:10,410
of the statistical measures
of bias that we calculated

617
00:26:10,410 --> 00:26:12,620
that I discussed on the previous slide.

618
00:26:12,620 --> 00:26:16,350
As you can see for age and
gender, all of these fall

619
00:26:16,350 --> 00:26:19,723
within the fair range for each
of these different metrics.

620
00:26:20,700 --> 00:26:23,230
And the only one that's
a real concern is race.

621
00:26:23,230 --> 00:26:26,970
So we have two instances
where race does not fall

622
00:26:26,970 --> 00:26:28,590
within that fair range

623
00:26:28,590 --> 00:26:31,780
and instead falls in outside into bias.

624
00:26:31,780 --> 00:26:33,899
And that would be in the
statistical parody difference

625
00:26:33,900 --> 00:26:35,393
and the disparate impact.

626
00:26:36,330 --> 00:26:37,649
And so we really wanted to focus

627
00:26:37,650 --> 00:26:39,760
on how could we mitigate these.

628
00:26:39,760 --> 00:26:42,910
And so what we chose to
use was a technique called

629
00:26:42,910 --> 00:26:44,240
adversarial debiasing.

630
00:26:44,240 --> 00:26:46,260
And the technique of adversarial debiasing

631
00:26:46,260 --> 00:26:48,310
is currently one of the
most popular techniques

632
00:26:48,310 --> 00:26:50,070
used to combat bias.

633
00:26:50,070 --> 00:26:52,620
It relies on adversarial
training to remove bias

634
00:26:52,620 --> 00:26:55,770
from the latent representations
learned by the model.

635
00:26:55,770 --> 00:26:59,480
So let Z in this diagram
you see on the screen

636
00:26:59,480 --> 00:27:00,970
be some sensitive attributes

637
00:27:00,970 --> 00:27:02,430
that we want to prevent our algorithm

638
00:27:02,430 --> 00:27:04,520
from discriminating on.

639
00:27:04,520 --> 00:27:07,770
For example age or race,
in our case it's race.

640
00:27:07,770 --> 00:27:10,420
And it's typically
insufficient to simply remove Z

641
00:27:10,420 --> 00:27:11,490
from our training data

642
00:27:11,490 --> 00:27:14,380
because it's often highly
correlated with other features.

643
00:27:14,380 --> 00:27:16,750
In our case, it's difficult to remove Z

644
00:27:16,750 --> 00:27:18,620
because we're dealing with images

645
00:27:18,620 --> 00:27:20,909
and we can't simply source more images

646
00:27:20,910 --> 00:27:24,170
because getting deep fake
images is hard to do.

647
00:27:24,170 --> 00:27:27,000
As well as scraping real
images from the internet.

648
00:27:27,000 --> 00:27:30,220
And so it wasn't simply easy
to either balance the races

649
00:27:30,220 --> 00:27:31,363
or anything like that.

650
00:27:32,780 --> 00:27:36,139
And so what we really want
to do is to prevent our model

651
00:27:36,140 --> 00:27:38,850
from learning a representation
of the input that relies on Z

652
00:27:38,850 --> 00:27:40,600
in any substantial way.

653
00:27:40,600 --> 00:27:43,500
And to this end, we trained
our model to simultaneously

654
00:27:43,500 --> 00:27:44,760
predictable the label Y

655
00:27:44,760 --> 00:27:48,270
and prevent a jointly trained
adversary from predicting Z.

656
00:27:48,270 --> 00:27:50,720
So this technique allowed
us to mitigate bias

657
00:27:50,720 --> 00:27:52,160
at the classifier level.

658
00:27:52,160 --> 00:27:55,210
Since we didn't have the option
to balance out the images

659
00:27:55,210 --> 00:27:57,693
and sourcing more images
was difficult for us.

660
00:28:00,760 --> 00:28:02,390
So you may be wondering,

661
00:28:02,390 --> 00:28:04,250
I work in security and
as Sherin mentioned,

662
00:28:04,250 --> 00:28:06,550
bias is also important to security,

663
00:28:06,550 --> 00:28:08,399
but you might be wondering
how can I use this a malware

664
00:28:08,400 --> 00:28:10,960
detection since I'm not
dealing with images of humans

665
00:28:10,960 --> 00:28:13,490
or data related to humans.

666
00:28:13,490 --> 00:28:14,860
So I'm gonna walk through the process

667
00:28:14,860 --> 00:28:16,428
of how you would go about applying this

668
00:28:16,428 --> 00:28:19,870
in the example of malware detection.

669
00:28:19,870 --> 00:28:21,909
So first let's consider which features

670
00:28:21,910 --> 00:28:24,920
you might have that exhibit
bias in your model or data set.

671
00:28:24,920 --> 00:28:27,213
So some examples of this
might include file types.

672
00:28:27,213 --> 00:28:29,170
As we've mentioned previously

673
00:28:29,170 --> 00:28:31,570
a lot of our data sets
consists of PE files

674
00:28:31,570 --> 00:28:33,520
and not so much .net files.

675
00:28:33,520 --> 00:28:35,440
So maybe we have a bias towards PE files

676
00:28:35,440 --> 00:28:37,590
because we have more data for them

677
00:28:37,590 --> 00:28:38,820
or malicious first benign.

678
00:28:38,820 --> 00:28:42,960
Sometimes it can be easier
to get benign samples

679
00:28:42,960 --> 00:28:43,900
and malicious samples.

680
00:28:43,900 --> 00:28:45,663
So maybe we have a lot more benign samples

681
00:28:45,663 --> 00:28:46,900
than malicious samples

682
00:28:46,900 --> 00:28:50,130
and we can be biased
towards benign samples.

683
00:28:50,130 --> 00:28:52,380
And then there's also malware families.

684
00:28:52,380 --> 00:28:54,960
So we try to have a diverse
set of malware families

685
00:28:54,960 --> 00:28:56,340
and the data sets that we look at

686
00:28:56,340 --> 00:28:58,659
but sometimes maybe we'll
have a lot more examples

687
00:28:58,660 --> 00:29:00,760
of ransomware then we do have a emotet.

688
00:29:00,760 --> 00:29:03,920
Which means we might miss emotet samples.

689
00:29:03,920 --> 00:29:05,490
So these are just some instances

690
00:29:05,490 --> 00:29:07,340
of where bias may exist in your data.

691
00:29:07,340 --> 00:29:08,730
And it's not an exclusive list

692
00:29:08,730 --> 00:29:11,270
but it's some things to think about.

693
00:29:11,270 --> 00:29:12,673
So now that you've identified the features

694
00:29:12,673 --> 00:29:16,310
that might exhibit bias,
you need to measure the bias

695
00:29:16,310 --> 00:29:18,770
in those features using the
bias metrics that I discussed

696
00:29:18,770 --> 00:29:21,280
such as statistical parity difference.

697
00:29:21,280 --> 00:29:22,888
So you wanna go through
and actually measure

698
00:29:22,888 --> 00:29:25,182
all of these features you identified,

699
00:29:25,182 --> 00:29:28,700
using the output from your
model and the confusion matrix.

700
00:29:28,700 --> 00:29:30,980
To determine if any of those metrics

701
00:29:30,980 --> 00:29:32,570
lie outside of the fair range

702
00:29:32,570 --> 00:29:36,003
and you do have bias in one
of those feature categories.

703
00:29:36,900 --> 00:29:39,360
Then once you identify that bias,

704
00:29:39,360 --> 00:29:41,300
you can go ahead with mitigating that bias

705
00:29:41,300 --> 00:29:43,607
and remember the first
place to mitigate bias

706
00:29:43,607 --> 00:29:45,040
and the most important place to look

707
00:29:45,040 --> 00:29:46,909
is always in your data set.

708
00:29:46,910 --> 00:29:49,770
And again, you can do this
simply by adding more,

709
00:29:49,770 --> 00:29:51,670
so say you have more ransomware samples

710
00:29:51,670 --> 00:29:53,180
than you do emotet samples.

711
00:29:53,180 --> 00:29:55,700
You can try sourcing more
emotet samples and adding them.

712
00:29:55,700 --> 00:29:57,470
If it's hard to source more emotet samples

713
00:29:57,470 --> 00:29:58,950
maybe you can balance out the family

714
00:29:58,950 --> 00:30:02,720
so that you down sample
ransomware to be closer to emotet.

715
00:30:02,720 --> 00:30:07,720
So maybe the model is not
as biased to ransomware.

716
00:30:07,920 --> 00:30:09,900
And then of course you can
use the other techniques

717
00:30:09,900 --> 00:30:12,850
that I discussed on the previous
side, such as re-weighting

718
00:30:14,735 --> 00:30:15,580
and optimized pre-processing.

719
00:30:17,473 --> 00:30:20,419
And finally, once you've
mitigated that bias

720
00:30:20,420 --> 00:30:23,270
you want to remeasure
the bias and the features

721
00:30:23,270 --> 00:30:24,990
using those bias metrics again.

722
00:30:24,990 --> 00:30:28,150
To ensure that your mitigation
step was successful.

723
00:30:28,150 --> 00:30:31,923
So this is using the
statistical parody metrics

724
00:30:31,923 --> 00:30:35,510
that disparate impact, again,
remeasuring those same metrics

725
00:30:35,510 --> 00:30:38,520
on the new model and the
output of the confusion matrix

726
00:30:38,520 --> 00:30:42,100
from that model to determine
if those bias metrics now lie

727
00:30:42,100 --> 00:30:43,659
within the fair range.

728
00:30:43,660 --> 00:30:46,560
And you wanna remember to look
at your false positive rate

729
00:30:48,089 --> 00:30:49,345
and your true positive rate.

730
00:30:49,345 --> 00:30:51,910
To ensure that they are
still in an acceptable range

731
00:30:51,910 --> 00:30:55,620
because there can be a
trade-off between bias,

732
00:30:55,620 --> 00:30:57,530
mitigating bias and false positive rate

733
00:30:57,530 --> 00:30:58,680
and true positive rate.

734
00:31:01,720 --> 00:31:04,410
So some key takeaways
from all of the stuff

735
00:31:04,410 --> 00:31:07,670
that we've discussed in
this presentation are,

736
00:31:07,670 --> 00:31:10,660
one understand the biases that
you bring to model building

737
00:31:10,660 --> 00:31:12,783
and the biases that exist in your data.

738
00:31:13,720 --> 00:31:16,050
Second, you can use an open source tool

739
00:31:16,050 --> 00:31:17,879
such as the AI 360 Fairness tool.

740
00:31:17,880 --> 00:31:20,430
To measure attributes your
model exhibits bias towards

741
00:31:20,430 --> 00:31:22,960
and again you can also hand
calculate all the metrics

742
00:31:22,960 --> 00:31:24,460
that we've mentioned,

743
00:31:24,460 --> 00:31:27,300
and then finally you
should go and reevaluate

744
00:31:27,300 --> 00:31:30,000
all the AI models within your organization

745
00:31:30,000 --> 00:31:34,517
and mitigate bias in any data
or models that you might find.

746
00:31:34,517 --> 00:31:36,510
And with that, we'd like
to thank you for listening

747
00:31:36,510 --> 00:31:38,210
to our presentation and Sherin and I

748
00:31:38,210 --> 00:31:39,810
can take any questions you have.

