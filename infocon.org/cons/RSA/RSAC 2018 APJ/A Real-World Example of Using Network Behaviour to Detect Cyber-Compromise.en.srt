1
00:00:11,120 --> 00:00:18,020
we've heard a lot about Big Data

2
00:00:14,860 --> 00:00:20,660
behavioral analytics machine learning

3
00:00:18,020 --> 00:00:23,509
and artificial intelligence in the last

4
00:00:20,660 --> 00:00:26,509
last couple of days what you'll hear

5
00:00:23,510 --> 00:00:29,840
from me today will not be an experts

6
00:00:26,510 --> 00:00:31,520
guide to the journey I won't be an

7
00:00:29,840 --> 00:00:35,450
experts guide to artificial intelligence

8
00:00:31,520 --> 00:00:38,870
and similar to many of my colleagues

9
00:00:35,450 --> 00:00:40,940
that last week or the last few days you

10
00:00:38,870 --> 00:00:42,379
won't hear me say that that that this is

11
00:00:40,940 --> 00:00:45,680
a silver board that will solve all of

12
00:00:42,380 --> 00:00:48,800
your cyber defense needs what you will

13
00:00:45,680 --> 00:00:51,170
get is a bit of a story about it our

14
00:00:48,800 --> 00:00:53,809
journey I'll share with you some of

15
00:00:51,170 --> 00:00:56,780
onsets experiences that will hopefully

16
00:00:53,810 --> 00:00:58,970
be helpful for your organization's as

17
00:00:56,780 --> 00:01:02,300
you go down a big data and machine

18
00:00:58,970 --> 00:01:03,729
learning journey the story will be three

19
00:01:02,300 --> 00:01:06,560
parts

20
00:01:03,729 --> 00:01:08,960
firstly I'll describe our initial

21
00:01:06,560 --> 00:01:12,700
implementation of a machine learning

22
00:01:08,960 --> 00:01:15,469
solution focused on network behavior

23
00:01:12,700 --> 00:01:19,789
I'll then describe how we expanded the

24
00:01:15,469 --> 00:01:23,990
platform to handle a much greater number

25
00:01:19,789 --> 00:01:28,130
of use cases and lastly I'll describe

26
00:01:23,990 --> 00:01:32,560
some examples of how our team began

27
00:01:28,130 --> 00:01:32,560
really embracing behavioral analytics

28
00:01:37,039 --> 00:01:40,490
please try again

29
00:01:43,880 --> 00:01:55,419
I have some help please cross my up sky

30
00:01:52,759 --> 00:01:55,420
the next frame please

31
00:01:59,439 --> 00:02:07,750
yeah thank you so firstly a quick

32
00:02:05,350 --> 00:02:11,980
overview of the landscape that we're

33
00:02:07,750 --> 00:02:15,280
dealing with we know that this threat

34
00:02:11,980 --> 00:02:16,629
landscape is rapidly evolving with

35
00:02:15,280 --> 00:02:21,280
threat actors become ever more

36
00:02:16,629 --> 00:02:26,769
sophisticated and agile cybercrime is

37
00:02:21,280 --> 00:02:28,780
well funded well organized again we all

38
00:02:26,769 --> 00:02:31,030
know that defense and and preventative

39
00:02:28,780 --> 00:02:36,000
measures have to be a critical part of

40
00:02:31,030 --> 00:02:39,519
our defenses but we also know that

41
00:02:36,000 --> 00:02:42,489
sophisticated well resourced advanced

42
00:02:39,519 --> 00:02:47,590
persistent threat actors impossible to

43
00:02:42,489 --> 00:02:50,530
stop from from intruding apt actors know

44
00:02:47,590 --> 00:02:53,349
the rules if an intrusion protection

45
00:02:50,530 --> 00:02:56,379
prevention system has default alert

46
00:02:53,349 --> 00:02:59,048
thresholds then the bad guys will go

47
00:02:56,379 --> 00:03:02,920
underneath that flaw they'll make

48
00:02:59,049 --> 00:03:05,230
thousands of tiny changes to the same

49
00:03:02,920 --> 00:03:07,030
malware to ensure that it hashes

50
00:03:05,230 --> 00:03:10,929
differently every time and avoid those

51
00:03:07,030 --> 00:03:15,130
types of controls they may use zero-day

52
00:03:10,930 --> 00:03:17,319
exploits that don't have patches that

53
00:03:15,130 --> 00:03:20,880
are not in antivirus databases and

54
00:03:17,319 --> 00:03:24,578
cannot be blocked using known signatures

55
00:03:20,880 --> 00:03:26,680
many victims of a PT's don't know

56
00:03:24,579 --> 00:03:31,840
they've been attacked until it's too

57
00:03:26,680 --> 00:03:35,799
late but there's also advances that help

58
00:03:31,840 --> 00:03:38,500
the defense side big data means that we

59
00:03:35,799 --> 00:03:41,849
now have the ability to capture and use

60
00:03:38,500 --> 00:03:44,919
far more data than we could before

61
00:03:41,849 --> 00:03:48,160
advanced algorithms allow us to use

62
00:03:44,919 --> 00:03:50,079
historic data to gain a much more

63
00:03:48,160 --> 00:03:56,440
in-depth understanding of what's

64
00:03:50,079 --> 00:03:58,569
happening in our network so a way that

65
00:03:56,440 --> 00:04:01,950
we can identify attacks is by

66
00:03:58,569 --> 00:04:05,980
understanding the kill chain methodology

67
00:04:01,950 --> 00:04:07,780
on the left-hand side it might start

68
00:04:05,980 --> 00:04:11,769
with an initial or will start with an

69
00:04:07,780 --> 00:04:13,540
initial attack it could be spearfishing

70
00:04:11,770 --> 00:04:16,329
it could be user navigating to a

71
00:04:13,540 --> 00:04:19,300
compromised website it could be a direct

72
00:04:16,329 --> 00:04:22,500
attack on a vulnerability the threat

73
00:04:19,300 --> 00:04:22,500
vector can take many forms

74
00:04:23,370 --> 00:04:28,300
once the attacker has compromised the

75
00:04:25,780 --> 00:04:33,580
network they'll attempt to gain a

76
00:04:28,300 --> 00:04:35,860
foothold and then scan the network to

77
00:04:33,580 --> 00:04:38,979
see what I think what other assets they

78
00:04:35,860 --> 00:04:42,220
can compromise they'll move laterally

79
00:04:38,980 --> 00:04:45,070
through the network as they take over

80
00:04:42,220 --> 00:04:48,700
further devices and attempt to gain more

81
00:04:45,070 --> 00:04:51,520
and more privileges the vast majority of

82
00:04:48,700 --> 00:04:54,820
attacks at least advanced attacks need

83
00:04:51,520 --> 00:04:58,890
lateral movement if they're looking to

84
00:04:54,820 --> 00:05:01,680
steal data they'll typically move data

85
00:04:58,890 --> 00:05:06,849
to a small number of staging points

86
00:05:01,680 --> 00:05:09,700
before exfiltrating NSAID has worked and

87
00:05:06,850 --> 00:05:12,090
is continuing to work with los alamos

88
00:05:09,700 --> 00:05:15,789
national National Laboratory or LAN all

89
00:05:12,090 --> 00:05:19,630
announced new to implement this solution

90
00:05:15,790 --> 00:05:23,610
known as path scan path scan uses

91
00:05:19,630 --> 00:05:26,110
NetFlow data as its main data source

92
00:05:23,610 --> 00:05:30,490
billions of training transactions every

93
00:05:26,110 --> 00:05:32,830
day describing the from address and to

94
00:05:30,490 --> 00:05:35,260
address of individual network

95
00:05:32,830 --> 00:05:39,510
transactions flow into a cloud cloud era

96
00:05:35,260 --> 00:05:42,940
data leak paths scan analyzes and scores

97
00:05:39,510 --> 00:05:45,640
individual transactions each of which on

98
00:05:42,940 --> 00:05:48,430
its own has a tiny probability of being

99
00:05:45,640 --> 00:05:51,940
malicious it's the combination that

100
00:05:48,430 --> 00:05:57,820
flows will lead to alerting over

101
00:05:51,940 --> 00:06:00,270
threshold you can see here some of the

102
00:05:57,820 --> 00:06:06,120
visualization that we see in path scan

103
00:06:00,270 --> 00:06:08,140
on the left hand side is reconnaissance

104
00:06:06,120 --> 00:06:12,760
reconnaissance is where an individual

105
00:06:08,140 --> 00:06:17,039
machine is reaching out to look at other

106
00:06:12,760 --> 00:06:19,690
machines in its vicinity so it could be

107
00:06:17,040 --> 00:06:23,350
it could be something quite benign and

108
00:06:19,690 --> 00:06:25,240
normal it may be it may be an email

109
00:06:23,350 --> 00:06:27,819
server broadcasting

110
00:06:25,240 --> 00:06:30,520
to its subscribers or sending messages

111
00:06:27,819 --> 00:06:34,319
to its subscribers or it could be

112
00:06:30,520 --> 00:06:37,840
malicious path scam we'll learn about

113
00:06:34,319 --> 00:06:40,240
what is a regular broadcast regular

114
00:06:37,840 --> 00:06:43,299
activity that that happens happens

115
00:06:40,240 --> 00:06:45,069
really often and it will sign those

116
00:06:43,300 --> 00:06:48,460
particular transactions a tiny

117
00:06:45,069 --> 00:06:50,610
probability of being malicious whereas

118
00:06:48,460 --> 00:06:53,409
when it sees that type of pattern a

119
00:06:50,610 --> 00:06:56,349
machine scanning the network for the

120
00:06:53,410 --> 00:06:58,300
first time that's much more likely to be

121
00:06:56,349 --> 00:07:03,580
assigned a higher probability of being

122
00:06:58,300 --> 00:07:07,360
malicious one example we saw was where a

123
00:07:03,580 --> 00:07:11,198
particular machine was was communicating

124
00:07:07,360 --> 00:07:14,110
with lots of printers in the network in

125
00:07:11,199 --> 00:07:16,449
fairly short succession there was first

126
00:07:14,110 --> 00:07:19,690
a message to a printer in Fiji then to

127
00:07:16,449 --> 00:07:22,300
one in Samoa then to one in PNG when

128
00:07:19,690 --> 00:07:25,419
that alert came through our analysts

129
00:07:22,300 --> 00:07:27,190
were wondering is there an attacker who

130
00:07:25,419 --> 00:07:28,990
knows there's a vulnerability in a

131
00:07:27,190 --> 00:07:31,690
particular type of printer and that's

132
00:07:28,990 --> 00:07:33,940
what they're looking for or perhaps they

133
00:07:31,690 --> 00:07:36,759
were looking for a foothold to establish

134
00:07:33,940 --> 00:07:40,630
persistence in the network as it turns

135
00:07:36,759 --> 00:07:42,250
out we had someone had a system going

136
00:07:40,630 --> 00:07:43,840
live the next week and they wanted to

137
00:07:42,250 --> 00:07:46,690
make sure that the printers in each of

138
00:07:43,840 --> 00:07:47,888
our regional offices were working so

139
00:07:46,690 --> 00:07:51,520
they were going out and actually doing a

140
00:07:47,889 --> 00:07:54,849
test print now although that ended up

141
00:07:51,520 --> 00:07:57,789
being a very benign explanation we don't

142
00:07:54,849 --> 00:07:59,229
treat that as a false positive this is

143
00:07:57,789 --> 00:08:01,509
the kind of information that we weren't

144
00:07:59,229 --> 00:08:03,610
previously able to we wouldn't have been

145
00:08:01,509 --> 00:08:04,930
alerted to that type of thing in the

146
00:08:03,610 --> 00:08:06,460
past and was something that was well

147
00:08:04,930 --> 00:08:08,409
worth investigating because it's quite

148
00:08:06,460 --> 00:08:14,169
anomalous even though it ended up being

149
00:08:08,409 --> 00:08:17,800
being benign in the middle you see

150
00:08:14,169 --> 00:08:20,169
lateral movement lateral movement is

151
00:08:17,800 --> 00:08:21,909
where an attacker moves from one machine

152
00:08:20,169 --> 00:08:26,380
to another machine to another machine to

153
00:08:21,909 --> 00:08:28,120
another machine and again it's much more

154
00:08:26,380 --> 00:08:31,030
likely to be a threat if it's a series

155
00:08:28,120 --> 00:08:34,390
of relatively unconnected unconnected

156
00:08:31,030 --> 00:08:36,578
events where it's unusual for a

157
00:08:34,390 --> 00:08:37,590
particular sequence of lateral movement

158
00:08:36,578 --> 00:08:40,650
to occur

159
00:08:37,590 --> 00:08:44,150
so for example a machine that belongs in

160
00:08:40,650 --> 00:08:48,060
HR talking to machine that is in finance

161
00:08:44,150 --> 00:08:51,569
to IT operations to an IT administrator

162
00:08:48,060 --> 00:08:53,790
or a payments operator I've deliberately

163
00:08:51,570 --> 00:08:56,279
chosen that type of a seek of a sequence

164
00:08:53,790 --> 00:08:58,020
because as each each hopping that

165
00:08:56,279 --> 00:08:59,970
sequence you're getting to someone who

166
00:08:58,020 --> 00:09:02,970
has more credentials who can do more

167
00:08:59,970 --> 00:09:05,130
dangerous things and it's not a normal

168
00:09:02,970 --> 00:09:07,560
that type of sequence is not a normal

169
00:09:05,130 --> 00:09:11,010
banking process but it's the type of

170
00:09:07,560 --> 00:09:16,020
both attack or type of behaviors and say

171
00:09:11,010 --> 00:09:19,200
that an attack vector may use and then

172
00:09:16,020 --> 00:09:21,210
on the right hand side you can see an

173
00:09:19,200 --> 00:09:24,900
example of what might be data staging

174
00:09:21,210 --> 00:09:26,430
where if an attacker has gained it's

175
00:09:24,900 --> 00:09:30,360
going to access to a decent number of

176
00:09:26,430 --> 00:09:34,170
machines or accounts on the network they

177
00:09:30,360 --> 00:09:37,170
may look to to bring all that data to

178
00:09:34,170 --> 00:09:39,810
one or a small number of staging

179
00:09:37,170 --> 00:09:44,459
machines before exfiltrating that and

180
00:09:39,810 --> 00:09:45,930
getting that out of the organization now

181
00:09:44,460 --> 00:09:48,980
won't it be clear that these models are

182
00:09:45,930 --> 00:09:52,079
very much still a maturing capability

183
00:09:48,980 --> 00:09:53,400
they've alerted our sock to lots of

184
00:09:52,080 --> 00:09:54,950
unusual behaviors that have been well

185
00:09:53,400 --> 00:09:58,829
worth investigating

186
00:09:54,950 --> 00:10:01,800
however it's an ongoing challenge to

187
00:09:58,830 --> 00:10:03,120
improve the signal-to-noise ratio and

188
00:10:01,800 --> 00:10:06,630
we're working really closely with our

189
00:10:03,120 --> 00:10:08,190
partners to improve the model as well as

190
00:10:06,630 --> 00:10:12,300
improving the quality of the source data

191
00:10:08,190 --> 00:10:13,770
on which the models rely so let's look

192
00:10:12,300 --> 00:10:17,120
firstly at some of the source data

193
00:10:13,770 --> 00:10:20,510
issues that we've had to deal with

194
00:10:17,120 --> 00:10:22,890
firstly our existing network telemetry

195
00:10:20,510 --> 00:10:26,610
was set up for operational purposes

196
00:10:22,890 --> 00:10:29,370
rather than for security purposes the

197
00:10:26,610 --> 00:10:32,070
network metadata was seen by our

198
00:10:29,370 --> 00:10:34,500
operational teams as not especially

199
00:10:32,070 --> 00:10:36,959
important particularly compared to real

200
00:10:34,500 --> 00:10:38,970
network traffic so whenever we had

201
00:10:36,959 --> 00:10:40,410
network constraints the first thing

202
00:10:38,970 --> 00:10:44,790
would happen is that the network

203
00:10:40,410 --> 00:10:47,310
metadata the NetFlow will be dropped the

204
00:10:44,790 --> 00:10:48,630
the network operations team was aware of

205
00:10:47,310 --> 00:10:50,550
this issue

206
00:10:48,630 --> 00:10:53,090
but had not been previously deemed a

207
00:10:50,550 --> 00:10:55,319
high enough priority for them to really

208
00:10:53,090 --> 00:10:58,140
do anything at least from an investment

209
00:10:55,320 --> 00:11:01,500
perspective about the introduction of

210
00:10:58,140 --> 00:11:04,199
parts can meant that investing in better

211
00:11:01,500 --> 00:11:06,060
network infrastructure became more

212
00:11:04,200 --> 00:11:08,010
important but that's something that we

213
00:11:06,060 --> 00:11:09,750
didn't know ahead of time I think the

214
00:11:08,010 --> 00:11:11,880
organization overall knew but the

215
00:11:09,750 --> 00:11:13,140
project team implementing this and then

216
00:11:11,880 --> 00:11:17,040
certainly the business case we put up

217
00:11:13,140 --> 00:11:19,610
didn't know about this service lack of

218
00:11:17,040 --> 00:11:22,910
data quality we defined in this area

219
00:11:19,610 --> 00:11:26,340
another issue that we worked through was

220
00:11:22,910 --> 00:11:29,969
net flow captures IP addresses rather

221
00:11:26,340 --> 00:11:33,210
than fully qualified domain names this

222
00:11:29,970 --> 00:11:35,910
meant to to get the best some kind of

223
00:11:33,210 --> 00:11:38,490
business context to the IP addresses we

224
00:11:35,910 --> 00:11:40,140
needed to get the domain names now

225
00:11:38,490 --> 00:11:42,150
probably think that's a fairly easy

226
00:11:40,140 --> 00:11:45,000
thing you just do a reverse DNS lookup

227
00:11:42,150 --> 00:11:48,030
and you've got the information once

228
00:11:45,000 --> 00:11:51,540
again though we hadn't had a really good

229
00:11:48,030 --> 00:11:53,490
reason to to implement a proper reverse

230
00:11:51,540 --> 00:11:55,650
DNS lookup we hadn't spent enough time

231
00:11:53,490 --> 00:11:58,530
making sure that that information was

232
00:11:55,650 --> 00:12:00,060
really well kept in our Sandy be I'm

233
00:11:58,530 --> 00:12:00,780
sure every single one of you your

234
00:12:00,060 --> 00:12:02,609
organization's

235
00:12:00,780 --> 00:12:05,520
the same DV isn't quite what you'd like

236
00:12:02,610 --> 00:12:08,760
to be but that was something that that

237
00:12:05,520 --> 00:12:11,550
took us quite a lot of manual effort to

238
00:12:08,760 --> 00:12:12,930
clean it up and make sure that we were

239
00:12:11,550 --> 00:12:14,760
able to provide some sort of business

240
00:12:12,930 --> 00:12:16,620
context so that our analysts weren't

241
00:12:14,760 --> 00:12:19,470
just forever looking at IP addresses and

242
00:12:16,620 --> 00:12:22,230
trying to work it was happening having

243
00:12:19,470 --> 00:12:25,920
said that for workstations and that had

244
00:12:22,230 --> 00:12:27,600
IP addresses assigned through DHCP we

245
00:12:25,920 --> 00:12:30,089
didn't bother to try and identify those

246
00:12:27,600 --> 00:12:32,480
machines through paths game I'll talk

247
00:12:30,090 --> 00:12:35,640
later about what we did about ingesting

248
00:12:32,480 --> 00:12:38,540
DHCP logs into other parts of the

249
00:12:35,640 --> 00:12:41,580
platform but we made the decision that

250
00:12:38,540 --> 00:12:43,110
at least for the purpose of this

251
00:12:41,580 --> 00:12:45,330
analysis the the machine learning and

252
00:12:43,110 --> 00:12:50,790
pass game they will just rely on the raw

253
00:12:45,330 --> 00:12:52,950
IP addresses for workstations we've also

254
00:12:50,790 --> 00:12:55,410
discovered some other anomalies in our

255
00:12:52,950 --> 00:12:59,060
network that we weren't previously aware

256
00:12:55,410 --> 00:13:00,870
of like every large organization and

257
00:12:59,060 --> 00:13:04,229
said has take

258
00:13:00,870 --> 00:13:06,839
over other companies and there's lots of

259
00:13:04,230 --> 00:13:09,270
legacy issues to deal with whether it

260
00:13:06,839 --> 00:13:12,120
inconsistencies between our core network

261
00:13:09,270 --> 00:13:14,730
and between some of the the networks for

262
00:13:12,120 --> 00:13:17,100
companies that we've taken over and once

263
00:13:14,730 --> 00:13:19,140
again operationally we've worked our way

264
00:13:17,100 --> 00:13:21,210
around dealing with those

265
00:13:19,140 --> 00:13:22,230
inconsistencies but when you come in

266
00:13:21,210 --> 00:13:25,380
when you're getting to actually

267
00:13:22,230 --> 00:13:27,420
capturing the data if it's not exactly

268
00:13:25,380 --> 00:13:33,750
the same and that causes lots of other

269
00:13:27,420 --> 00:13:36,839
issues you need to deal with now for a

270
00:13:33,750 --> 00:13:39,750
while it was really difficult to answer

271
00:13:36,839 --> 00:13:43,170
the question of how much of our network

272
00:13:39,750 --> 00:13:46,080
do we have covered through path scan and

273
00:13:43,170 --> 00:13:47,279
also what's the quality of data for the

274
00:13:46,080 --> 00:13:50,160
part of the network that we do have

275
00:13:47,279 --> 00:13:52,470
covered it was only actually after

276
00:13:50,160 --> 00:13:54,719
creating a model for describing both the

277
00:13:52,470 --> 00:13:56,850
coverage and quality they were rounded

278
00:13:54,720 --> 00:13:58,350
to really properly track our progress of

279
00:13:56,850 --> 00:14:02,700
exactly we were at or near the

280
00:13:58,350 --> 00:14:05,100
implementation another challenge another

281
00:14:02,700 --> 00:14:08,730
type of challenge about to talk about is

282
00:14:05,100 --> 00:14:15,060
filtering out some of the noise on the

283
00:14:08,730 --> 00:14:16,790
network IP telephony was was one example

284
00:14:15,060 --> 00:14:19,739
of where there was way too much noise

285
00:14:16,790 --> 00:14:21,630
it's not that unusual for someone to

286
00:14:19,740 --> 00:14:24,839
call someone that they've never spoken

287
00:14:21,630 --> 00:14:26,040
to you before and for that person to go

288
00:14:24,839 --> 00:14:27,810
and call someone that they haven't

289
00:14:26,040 --> 00:14:31,469
called before that's a very normal

290
00:14:27,810 --> 00:14:33,329
scenario but for a while we were finding

291
00:14:31,470 --> 00:14:37,380
that lots of our alerts were based

292
00:14:33,330 --> 00:14:39,990
around IP telephony so what we ended up

293
00:14:37,380 --> 00:14:42,720
doing with that challenge firstly we'll

294
00:14:39,990 --> 00:14:45,589
work with with lanolin eyr partners - to

295
00:14:42,720 --> 00:14:47,640
incorporate port scoring into the model

296
00:14:45,589 --> 00:14:50,220
so that we can at least use that as an

297
00:14:47,640 --> 00:14:53,699
extra data point but we've also done a

298
00:14:50,220 --> 00:14:56,490
lot of filtering and and and tuning of

299
00:14:53,700 --> 00:14:58,680
the the algorithm to try and have vo

300
00:14:56,490 --> 00:15:02,900
that particular use case caused cause

301
00:14:58,680 --> 00:15:06,449
less noise another one that causes some

302
00:15:02,900 --> 00:15:08,010
causeway to me alerts early on was an on

303
00:15:06,450 --> 00:15:10,830
burn on part of our

304
00:15:08,010 --> 00:15:14,700
our network nonprofits very nature

305
00:15:10,830 --> 00:15:17,160
doesn't really have as many normal you

306
00:15:14,700 --> 00:15:18,990
know normal usage of a network the idea

307
00:15:17,160 --> 00:15:22,439
of non prod is that you're continually

308
00:15:18,990 --> 00:15:24,930
trying new things and so we found early

309
00:15:22,440 --> 00:15:28,320
on again there was a large number of the

310
00:15:24,930 --> 00:15:30,660
alerts were from non fraud were benign

311
00:15:28,320 --> 00:15:32,910
so we had to tune things quite a bit to

312
00:15:30,660 --> 00:15:35,189
to try and I guess reduce the threshold

313
00:15:32,910 --> 00:15:42,150
of alerting also increased the threshold

314
00:15:35,190 --> 00:15:44,460
of alerting in that area testing is

315
00:15:42,150 --> 00:15:47,160
another area that you have to be really

316
00:15:44,460 --> 00:15:49,530
careful about in some ways testing this

317
00:15:47,160 --> 00:15:52,050
kind of model is a little a little bit

318
00:15:49,530 --> 00:15:54,810
like working with Schrodinger's cat the

319
00:15:52,050 --> 00:15:57,300
the mere fact of testing the model if

320
00:15:54,810 --> 00:15:59,099
you try and repeat that test the the

321
00:15:57,300 --> 00:16:01,349
model works and whether it has seen this

322
00:15:59,100 --> 00:16:03,960
type of behavior before so if you behave

323
00:16:01,350 --> 00:16:06,660
in a certain way and repeat that that

324
00:16:03,960 --> 00:16:08,430
behavior the model will think it's more

325
00:16:06,660 --> 00:16:11,670
benign than what it did the first time

326
00:16:08,430 --> 00:16:14,579
you tried it so whenever you do a system

327
00:16:11,670 --> 00:16:16,620
test or a Red Team exercise you need to

328
00:16:14,580 --> 00:16:18,330
be really careful to make it a

329
00:16:16,620 --> 00:16:20,940
completely new test you might be trying

330
00:16:18,330 --> 00:16:22,380
the same kind of thing but make sure you

331
00:16:20,940 --> 00:16:25,380
pick different machines different

332
00:16:22,380 --> 00:16:26,970
network paths so the you're not working

333
00:16:25,380 --> 00:16:30,450
with data that's that's polluted that

334
00:16:26,970 --> 00:16:31,440
that is a challenge that I don't think

335
00:16:30,450 --> 00:16:35,880
we've really thought about until we

336
00:16:31,440 --> 00:16:37,620
actually got got into that and I guess

337
00:16:35,880 --> 00:16:41,939
the last challenge is is probably a more

338
00:16:37,620 --> 00:16:44,250
fundamental one again NZD again like

339
00:16:41,940 --> 00:16:46,680
every organisation is being disrupted

340
00:16:44,250 --> 00:16:49,440
and is having to respond to change

341
00:16:46,680 --> 00:16:52,410
really rapidly this means constant

342
00:16:49,440 --> 00:16:53,970
business change and in an environment of

343
00:16:52,410 --> 00:16:56,219
where there's billions of events every

344
00:16:53,970 --> 00:16:59,840
day it's really hard to differentiate

345
00:16:56,220 --> 00:17:05,579
between legitimate business change and

346
00:16:59,840 --> 00:17:07,140
and potentially malicious activity while

347
00:17:05,579 --> 00:17:10,010
the algorithms we use to find

348
00:17:07,140 --> 00:17:12,780
maliciousness will continue to improve

349
00:17:10,010 --> 00:17:17,189
the bad guys are also getting better at

350
00:17:12,780 --> 00:17:20,099
blending into the network so in addition

351
00:17:17,189 --> 00:17:21,199
to trying to continually tune the model

352
00:17:20,099 --> 00:17:22,698
make it better

353
00:17:21,199 --> 00:17:24,949
both the work that we're doing and the

354
00:17:22,699 --> 00:17:28,760
work we do with our partners part

355
00:17:24,949 --> 00:17:31,309
solution is also to not only try and

356
00:17:28,760 --> 00:17:33,408
reduce the number of signals but also

357
00:17:31,309 --> 00:17:35,840
helping out on our analysts to more

358
00:17:33,409 --> 00:17:37,820
quickly triage those alerts that are

359
00:17:35,840 --> 00:17:39,470
actually actually are generated to

360
00:17:37,820 --> 00:17:46,070
determine if they're worth a full

361
00:17:39,470 --> 00:17:47,899
investigation so the last few minutes

362
00:17:46,070 --> 00:17:50,960
I've talked about the the relatively

363
00:17:47,899 --> 00:17:56,260
narrow use case of analyzing Network

364
00:17:50,960 --> 00:17:59,450
flows but once we started on the journey

365
00:17:56,260 --> 00:18:03,230
we realized we could actually get a lot

366
00:17:59,450 --> 00:18:06,529
more value by correlating events across

367
00:18:03,230 --> 00:18:09,919
a wide variety of data sources a weak

368
00:18:06,529 --> 00:18:11,809
anomaly signal from one data source is

369
00:18:09,919 --> 00:18:14,510
not really like their stand out from the

370
00:18:11,809 --> 00:18:17,990
noise whereas weak signals from multiple

371
00:18:14,510 --> 00:18:23,149
sources can be amplified if we correlate

372
00:18:17,990 --> 00:18:24,980
them as a result of this realization our

373
00:18:23,149 --> 00:18:28,840
focus has moved to ingestion of data

374
00:18:24,980 --> 00:18:36,860
from many sources such as proxy logs

375
00:18:28,840 --> 00:18:38,539
threat Intel endpoint and HR data well

376
00:18:36,860 --> 00:18:40,850
we've got a lot more sources to to

377
00:18:38,539 --> 00:18:42,559
ingest in the future with a least reach

378
00:18:40,850 --> 00:18:45,230
a point of reasonable maturity in

379
00:18:42,559 --> 00:18:50,899
bringing data into a data Lake which we

380
00:18:45,230 --> 00:18:52,940
call advanced cyber analytics did

381
00:18:50,899 --> 00:18:57,549
someone log on to a machine for the

382
00:18:52,940 --> 00:19:00,380
first time did they access a website

383
00:18:57,549 --> 00:19:03,710
that had never been accessed or that

384
00:19:00,380 --> 00:19:07,210
they had never visited did they log into

385
00:19:03,710 --> 00:19:07,210
their machine from an unusual location

386
00:19:07,750 --> 00:19:14,600
was there a registry change on their on

387
00:19:10,279 --> 00:19:18,559
their machine did they attempt to access

388
00:19:14,600 --> 00:19:22,908
a blocked website did they receive

389
00:19:18,559 --> 00:19:26,899
delegated HR access access to a new

390
00:19:22,909 --> 00:19:30,610
system or was their account being used

391
00:19:26,899 --> 00:19:30,610
in an unusual time during the day

392
00:19:31,010 --> 00:19:36,470
a lot of those examples I've just

393
00:19:32,960 --> 00:19:41,480
mentioned are not especially unusual

394
00:19:36,470 --> 00:19:43,310
when you look at them in isolation you

395
00:19:41,480 --> 00:19:46,280
know if we had to investigate every

396
00:19:43,310 --> 00:19:48,950
single time a user logged onto a machine

397
00:19:46,280 --> 00:19:51,320
for the first time the sock would be

398
00:19:48,950 --> 00:19:53,990
would be overwhelmed and it become it

399
00:19:51,320 --> 00:19:56,300
would be a waste of their time however

400
00:19:53,990 --> 00:19:58,730
if we see a lot of these types of things

401
00:19:56,300 --> 00:20:01,149
occurring at the same time that is worth

402
00:19:58,730 --> 00:20:04,090
investigating

403
00:20:01,150 --> 00:20:07,850
conversely there'll be other situations

404
00:20:04,090 --> 00:20:10,159
where correlation is expected and the

405
00:20:07,850 --> 00:20:14,540
absence of correlation should be

406
00:20:10,160 --> 00:20:17,590
investigated a proxy entry that's not

407
00:20:14,540 --> 00:20:21,590
correlated with complimentary endpoint

408
00:20:17,590 --> 00:20:23,270
geolocation and network activity could

409
00:20:21,590 --> 00:20:27,649
be evidence of spoofing or a

410
00:20:23,270 --> 00:20:29,800
man-in-the-middle attack to areas where

411
00:20:27,650 --> 00:20:32,840
as we're particularly interested in a

412
00:20:29,800 --> 00:20:38,030
credential analytics and privileged user

413
00:20:32,840 --> 00:20:40,070
misuse credential analytics is the

414
00:20:38,030 --> 00:20:43,129
second project that we're working on

415
00:20:40,070 --> 00:20:45,740
with a wire and LAN or it's a series of

416
00:20:43,130 --> 00:20:48,260
machine learning algorithms that involve

417
00:20:45,740 --> 00:20:52,310
building up a model of views of behavior

418
00:20:48,260 --> 00:20:56,690
and looking for anomalies the types of

419
00:20:52,310 --> 00:20:58,120
things we're looking at is when does the

420
00:20:56,690 --> 00:21:00,980
user login

421
00:20:58,120 --> 00:21:05,090
what machine do they log in from both

422
00:21:00,980 --> 00:21:07,940
directly or through remote access what

423
00:21:05,090 --> 00:21:10,879
location they looking from what are the

424
00:21:07,940 --> 00:21:14,270
windows processes could start when they

425
00:21:10,880 --> 00:21:18,280
do logon and how does the user's

426
00:21:14,270 --> 00:21:20,930
behavior differ from their peer group

427
00:21:18,280 --> 00:21:24,080
we're in the beta stage of this model

428
00:21:20,930 --> 00:21:27,710
and we think it has real potential at

429
00:21:24,080 --> 00:21:30,260
data sources windows con logs and Active

430
00:21:27,710 --> 00:21:32,360
Directory and if the beta testing is

431
00:21:30,260 --> 00:21:37,190
successful then we'll consider expanding

432
00:21:32,360 --> 00:21:40,129
it additional use cases that can involve

433
00:21:37,190 --> 00:21:43,820
privileged users and also correlations

434
00:21:40,130 --> 00:21:44,630
with asset criticality identity and

435
00:21:43,820 --> 00:21:47,629
access management

436
00:21:44,630 --> 00:21:50,000
is a major focus for us and many of the

437
00:21:47,630 --> 00:21:54,070
cybersecurity scenarios that we fear the

438
00:21:50,000 --> 00:21:54,070
most involve an escalation of privileges

439
00:22:00,430 --> 00:22:06,620
we're also watching developments with a

440
00:22:03,140 --> 00:22:09,020
patchy spot a patchy spot is an open

441
00:22:06,620 --> 00:22:11,889
source project with similar goals to

442
00:22:09,020 --> 00:22:15,220
what we have improving cybersecurity

443
00:22:11,890 --> 00:22:17,560
through the use of machine learning and

444
00:22:15,220 --> 00:22:22,520
consolidating enterprise security data

445
00:22:17,560 --> 00:22:25,210
into a into an IT telemetry hub when we

446
00:22:22,520 --> 00:22:27,530
first looked at it we were interested

447
00:22:25,210 --> 00:22:30,860
interested enough to align our data

448
00:22:27,530 --> 00:22:33,860
model with its data model but it was

449
00:22:30,860 --> 00:22:36,800
still immature if the project gains

450
00:22:33,860 --> 00:22:38,709
greater acceptance and maturity then

451
00:22:36,800 --> 00:22:41,990
we'll look to embrace that more fully

452
00:22:38,710 --> 00:22:45,290
especially because it we're hoping it'll

453
00:22:41,990 --> 00:22:47,210
allow us to to incorporate models that

454
00:22:45,290 --> 00:22:50,740
have been developed elsewhere into into

455
00:22:47,210 --> 00:22:50,740
our might in addition to our models you

456
00:22:52,990 --> 00:22:59,870
can see here a really high-level view of

457
00:22:56,290 --> 00:23:03,820
the of the the advanced cybernetics

458
00:22:59,870 --> 00:23:07,310
architecture we currently ingest from 31

459
00:23:03,820 --> 00:23:11,540
separate data sources using Apache flume

460
00:23:07,310 --> 00:23:13,790
and Kafka to bring the data in the data

461
00:23:11,540 --> 00:23:17,570
in raw form enter that into the Hadoop

462
00:23:13,790 --> 00:23:19,700
distributed file system within transform

463
00:23:17,570 --> 00:23:22,760
that data into data products that have

464
00:23:19,700 --> 00:23:24,260
been optimized for analytics many of the

465
00:23:22,760 --> 00:23:25,910
data products are based on the Apache

466
00:23:24,260 --> 00:23:29,510
spot open data and we'll I spoke about a

467
00:23:25,910 --> 00:23:32,390
couple of minutes ago now hfts is

468
00:23:29,510 --> 00:23:34,520
already pretty big this slide is

469
00:23:32,390 --> 00:23:37,610
slightly out of date we're now bringing

470
00:23:34,520 --> 00:23:41,290
in two billion messages per day and our

471
00:23:37,610 --> 00:23:45,169
file system is around 1.5 petabytes of

472
00:23:41,290 --> 00:23:46,490
completely of compress storage we

473
00:23:45,170 --> 00:23:50,680
eventually think we'll get up to around

474
00:23:46,490 --> 00:23:52,730
about 10 billion messages per day and

475
00:23:50,680 --> 00:23:54,020
we've committed to storing them for at

476
00:23:52,730 --> 00:23:55,070
least 12 months I think it'll be quite a

477
00:23:54,020 --> 00:23:56,720
lot longer

478
00:23:55,070 --> 00:23:59,210
than that and that actually addresses

479
00:23:56,720 --> 00:24:02,120
another shortcoming that we had prior to

480
00:23:59,210 --> 00:24:04,730
us she was putting this platform in now

481
00:24:02,120 --> 00:24:06,739
we previously had to purge logs a lot

482
00:24:04,730 --> 00:24:10,279
quicker than we wanted to because we had

483
00:24:06,740 --> 00:24:11,690
nowhere to store them we're now not only

484
00:24:10,279 --> 00:24:13,970
using advanced cybernetics for

485
00:24:11,690 --> 00:24:15,710
analytical purposes but also for

486
00:24:13,970 --> 00:24:19,850
forensic purposes and that's been really

487
00:24:15,710 --> 00:24:22,730
helpful and then some of the tools that

488
00:24:19,850 --> 00:24:24,759
we use to analyze the data include the

489
00:24:22,730 --> 00:24:26,169
cloud era data science workbench

490
00:24:24,759 --> 00:24:30,019
tensorflow

491
00:24:26,169 --> 00:24:32,450
hue and hunk we're not prescriptive

492
00:24:30,019 --> 00:24:34,519
about about what analytics tools should

493
00:24:32,450 --> 00:24:37,700
be used and we're still experimenting

494
00:24:34,519 --> 00:24:41,379
quite a lot to determine what tools are

495
00:24:37,700 --> 00:24:41,379
the best fit for what kinds of analytics

496
00:24:43,299 --> 00:24:50,440
we've aligned our teams into integrated

497
00:24:46,460 --> 00:24:54,950
sports some focus on data ingestion

498
00:24:50,440 --> 00:24:57,529
others on data science no leaks and we

499
00:24:54,950 --> 00:24:59,509
have other teams again focusing on

500
00:24:57,529 --> 00:25:01,840
operation operationalization and

501
00:24:59,509 --> 00:25:03,860
operationalize a you know what I mean

502
00:25:01,840 --> 00:25:06,019
making the preferred that platform

503
00:25:03,860 --> 00:25:10,370
operational particularly for the sock

504
00:25:06,019 --> 00:25:12,769
analysts and we've got others who play a

505
00:25:10,370 --> 00:25:15,979
coordinating function on managing both a

506
00:25:12,769 --> 00:25:20,299
backlog and our also a release train so

507
00:25:15,980 --> 00:25:23,360
typical flow is for soccer analysts or

508
00:25:20,299 --> 00:25:25,639
our threat intelligence team to identify

509
00:25:23,360 --> 00:25:28,279
a threat and have that raised on the

510
00:25:25,639 --> 00:25:30,139
backlog we run prioritized

511
00:25:28,279 --> 00:25:32,690
prioritization meetings at the end of

512
00:25:30,139 --> 00:25:34,219
every sprint and sometimes have ad hoc

513
00:25:32,690 --> 00:25:38,620
prioritization meetings if something's

514
00:25:34,220 --> 00:25:41,419
urgent we tend to work backwards to

515
00:25:38,620 --> 00:25:45,500
understand what is required to meet a

516
00:25:41,419 --> 00:25:47,500
particular use case so once the request

517
00:25:45,500 --> 00:25:49,940
has been raised our data scientists

518
00:25:47,500 --> 00:25:52,570
they'll look at whether whether the

519
00:25:49,940 --> 00:25:55,580
request can be met by a traditional

520
00:25:52,570 --> 00:25:57,350
specific query or perhaps whether a

521
00:25:55,580 --> 00:26:01,070
behavioral based approach is best or

522
00:25:57,350 --> 00:26:02,870
maybe a combination of the two data

523
00:26:01,070 --> 00:26:05,550
engineers will look at whether data is

524
00:26:02,870 --> 00:26:08,879
available already on the platform

525
00:26:05,550 --> 00:26:11,790
if it's not then they'll raise an

526
00:26:08,880 --> 00:26:13,410
additional backlog item for the data

527
00:26:11,790 --> 00:26:15,120
ingestion team to go out and actually

528
00:26:13,410 --> 00:26:18,240
get that data and bring it into the

529
00:26:15,120 --> 00:26:20,760
platform and then the data ingestion

530
00:26:18,240 --> 00:26:24,810
team's priorities are driven by a

531
00:26:20,760 --> 00:26:26,750
combination of the priority of the of

532
00:26:24,810 --> 00:26:30,300
the the queries that are being requested

533
00:26:26,750 --> 00:26:33,450
by the volume of use cases that require

534
00:26:30,300 --> 00:26:36,149
a particular data source and also by how

535
00:26:33,450 --> 00:26:40,970
easy it is to ingest the different data

536
00:26:36,150 --> 00:26:43,380
sources the roles that backlog manager

537
00:26:40,970 --> 00:26:44,940
at the front of the process and the

538
00:26:43,380 --> 00:26:47,640
release train engineer at the end of the

539
00:26:44,940 --> 00:26:49,410
process are critical they're the people

540
00:26:47,640 --> 00:26:52,890
who make sure that the teams remain

541
00:26:49,410 --> 00:26:57,000
aligned and the process is very

542
00:26:52,890 --> 00:26:58,860
iterative often the data scientists or

543
00:26:57,000 --> 00:27:02,130
the sock analysts need to actually see

544
00:26:58,860 --> 00:27:04,770
the data before knowing exactly what

545
00:27:02,130 --> 00:27:07,020
they want and sometimes by seeing the

546
00:27:04,770 --> 00:27:09,480
data that generates a whole series of

547
00:27:07,020 --> 00:27:11,940
further questions which in turn mean

548
00:27:09,480 --> 00:27:15,180
more queries more models and often

549
00:27:11,940 --> 00:27:18,390
augmenting the data because you don't

550
00:27:15,180 --> 00:27:21,590
you don't always know where a particular

551
00:27:18,390 --> 00:27:24,720
line of thinking is going to take you

552
00:27:21,590 --> 00:27:26,520
we've been we've been a little bit ad

553
00:27:24,720 --> 00:27:31,170
hoc in determining what our priorities

554
00:27:26,520 --> 00:27:32,970
are so far usually where we got it

555
00:27:31,170 --> 00:27:34,530
firstly by the collective views of the

556
00:27:32,970 --> 00:27:36,990
team as to what what they think is

557
00:27:34,530 --> 00:27:40,230
important with the decision-maker every

558
00:27:36,990 --> 00:27:41,580
two weeks being the product owner this

559
00:27:40,230 --> 00:27:43,320
hasn't really been really been a problem

560
00:27:41,580 --> 00:27:45,870
so far there's been a fairly decent

561
00:27:43,320 --> 00:27:48,659
consensus that the priority was to first

562
00:27:45,870 --> 00:27:51,239
get the data into the platform and the

563
00:27:48,660 --> 00:27:54,030
highest party query is also there was a

564
00:27:51,240 --> 00:27:57,060
decent consensus over that I suspect

565
00:27:54,030 --> 00:27:58,560
that fairly soon we're gonna have to

566
00:27:57,060 --> 00:28:00,840
become a little more structured about

567
00:27:58,560 --> 00:28:03,659
how we do this and as we're starting to

568
00:28:00,840 --> 00:28:06,030
pivot from from the focus being the data

569
00:28:03,660 --> 00:28:09,170
ingestion to focusing a lot more on the

570
00:28:06,030 --> 00:28:11,730
models and queries that we're building

571
00:28:09,170 --> 00:28:14,550
we've we've started using the martyr

572
00:28:11,730 --> 00:28:16,140
attack framework to identify where we

573
00:28:14,550 --> 00:28:17,980
have the largest exposures in our

574
00:28:16,140 --> 00:28:19,780
detection capability

575
00:28:17,980 --> 00:28:21,880
and this will not only drive our

576
00:28:19,780 --> 00:28:29,100
analytics priorities but will also drive

577
00:28:21,880 --> 00:28:31,090
our telemetry priorities as well and

578
00:28:29,100 --> 00:28:33,520
similar to what I described with net

579
00:28:31,090 --> 00:28:36,399
flow we do regularly find the quality of

580
00:28:33,520 --> 00:28:38,440
source data is not what we would have

581
00:28:36,400 --> 00:28:40,510
expected I think we've probably stopped

582
00:28:38,440 --> 00:28:44,260
expecting high quality data but it's

583
00:28:40,510 --> 00:28:47,920
never where you hope it's going to be so

584
00:28:44,260 --> 00:28:51,310
a recent example was window scum events

585
00:28:47,920 --> 00:28:53,500
we assumed that we hoped at least that

586
00:28:51,310 --> 00:28:54,909
that that most events were being logged

587
00:28:53,500 --> 00:28:57,270
and then realized that actually a lot of

588
00:28:54,910 --> 00:28:59,980
events were not actually being logged

589
00:28:57,270 --> 00:29:02,889
which caused us to adjust the scope of

590
00:28:59,980 --> 00:29:05,280
our latest sprint so the way that we

591
00:29:02,890 --> 00:29:08,050
deal with missing or poor data quality

592
00:29:05,280 --> 00:29:09,010
is consistent with the way we we deal

593
00:29:08,050 --> 00:29:11,950
with things in the rest of the platform

594
00:29:09,010 --> 00:29:13,420
we treat the factor of the data quality

595
00:29:11,950 --> 00:29:17,550
not being where we thought was going to

596
00:29:13,420 --> 00:29:22,270
be as a new piece of information and we

597
00:29:17,550 --> 00:29:23,950
we then adjust weari prioritize and and

598
00:29:22,270 --> 00:29:26,379
we and we move on with whatever the next

599
00:29:23,950 --> 00:29:28,240
set of priorities are so sometimes

600
00:29:26,380 --> 00:29:31,240
that'll mean breaking up a use case into

601
00:29:28,240 --> 00:29:34,180
multiple pieces one that we can deliver

602
00:29:31,240 --> 00:29:35,440
in the current sprint and one or more

603
00:29:34,180 --> 00:29:38,290
that go into the backlog for

604
00:29:35,440 --> 00:29:39,580
prioritization in the future often

605
00:29:38,290 --> 00:29:41,889
that'll be a little that'll involve

606
00:29:39,580 --> 00:29:43,570
round Geographic lines we might find

607
00:29:41,890 --> 00:29:46,690
that the the data quality is quite good

608
00:29:43,570 --> 00:29:48,700
in one country but in other countries

609
00:29:46,690 --> 00:29:50,920
it's it's not the same format or it's

610
00:29:48,700 --> 00:29:52,960
poorer and so that might be the example

611
00:29:50,920 --> 00:29:54,430
of where we split a use case into

612
00:29:52,960 --> 00:29:57,510
multiple pieces and not necessarily

613
00:29:54,430 --> 00:30:01,210
deliver them all at the same time

614
00:29:57,510 --> 00:30:01,660
now I cannot stress enough how important

615
00:30:01,210 --> 00:30:05,470
it is

616
00:30:01,660 --> 00:30:08,680
the teams are aligned and agile we're

617
00:30:05,470 --> 00:30:11,110
constantly learning and it's critical

618
00:30:08,680 --> 00:30:13,800
that feedback from one of our teams can

619
00:30:11,110 --> 00:30:16,000
be used to realign other teams a

620
00:30:13,800 --> 00:30:18,760
waterfall based approach for anything

621
00:30:16,000 --> 00:30:20,980
but standing up the initial platform is

622
00:30:18,760 --> 00:30:23,680
bound to fail and I'd say the same thing

623
00:30:20,980 --> 00:30:27,280
for any any project involves Big Data

624
00:30:23,680 --> 00:30:28,360
bed security or anything else so this

625
00:30:27,280 --> 00:30:28,940
type of approach can be quite

626
00:30:28,360 --> 00:30:32,320
challenging

627
00:30:28,940 --> 00:30:34,910
for many individuals and organizations

628
00:30:32,320 --> 00:30:37,429
aims its partway through a major agile

629
00:30:34,910 --> 00:30:39,740
transformation however when we started a

630
00:30:37,430 --> 00:30:41,600
journey eighteen months ago there was a

631
00:30:39,740 --> 00:30:44,710
lot of organizational resistance to this

632
00:30:41,600 --> 00:30:47,000
way of working even within the team

633
00:30:44,710 --> 00:30:49,160
there was some frustration from some of

634
00:30:47,000 --> 00:30:52,610
the project managers that what they saw

635
00:30:49,160 --> 00:30:54,200
as continually changing scope they

636
00:30:52,610 --> 00:30:57,229
bought into the process a lot more once

637
00:30:54,200 --> 00:30:59,090
they realized that while exactly what

638
00:30:57,230 --> 00:31:02,270
they were working on was changing quite

639
00:30:59,090 --> 00:31:06,470
a lot the overall mission was remaining

640
00:31:02,270 --> 00:31:09,230
constant it also helped when the entire

641
00:31:06,470 --> 00:31:11,990
team realized that they could influence

642
00:31:09,230 --> 00:31:13,880
the priorities however once a two-week

643
00:31:11,990 --> 00:31:16,040
sprint had been agreed that was the

644
00:31:13,880 --> 00:31:18,230
scope it was locked the next opportunity

645
00:31:16,040 --> 00:31:21,800
to change scope was never more than two

646
00:31:18,230 --> 00:31:24,680
weeks away in addition to the cyber

647
00:31:21,800 --> 00:31:26,570
defense use cases we said the platform

648
00:31:24,680 --> 00:31:29,840
can be used through a lot of other

649
00:31:26,570 --> 00:31:32,689
purposes in the future the telemetry

650
00:31:29,840 --> 00:31:36,740
we've captured could be really useful

651
00:31:32,690 --> 00:31:39,320
for IT operations and the models and

652
00:31:36,740 --> 00:31:46,850
analytical thinking can be really

653
00:31:39,320 --> 00:31:49,060
valuable for our fraud team we've spent

654
00:31:46,850 --> 00:31:51,169
the last 15 or the last 30 minutes or so

655
00:31:49,060 --> 00:31:53,480
really focusing on some of the

656
00:31:51,170 --> 00:31:55,670
implementation challenges we've had both

657
00:31:53,480 --> 00:31:58,640
with the machine learning models from

658
00:31:55,670 --> 00:32:00,080
from path scan and also from I guess

659
00:31:58,640 --> 00:32:03,020
some of the more structure queries and

660
00:32:00,080 --> 00:32:06,860
the overall advanced cyber Linux pro

661
00:32:03,020 --> 00:32:09,530
platform what I'd like to do now is talk

662
00:32:06,860 --> 00:32:12,199
a little bit about some of the changes

663
00:32:09,530 --> 00:32:15,800
that I guess our teams have served the

664
00:32:12,200 --> 00:32:17,180
way new ways of working from our sock in

665
00:32:15,800 --> 00:32:23,300
in particular way that they're starting

666
00:32:17,180 --> 00:32:25,880
to embrace behavioral analytics so when

667
00:32:23,300 --> 00:32:29,300
we first started this I think we had a

668
00:32:25,880 --> 00:32:30,590
reasonable idea of what it was that we

669
00:32:29,300 --> 00:32:32,990
were going to be focusing on where we

670
00:32:30,590 --> 00:32:34,340
thought the the main priorities were

671
00:32:32,990 --> 00:32:37,600
going to be I'm seeing people have

672
00:32:34,340 --> 00:32:37,600
laughs some people who clearly know Luke

673
00:32:39,070 --> 00:32:44,220
but what we found actually is that

674
00:32:41,520 --> 00:32:45,870
it's what we thought we started was not

675
00:32:44,220 --> 00:32:49,350
necessarily the same as what we were

676
00:32:45,870 --> 00:32:51,360
we're now going and something I've been

677
00:32:49,350 --> 00:32:52,949
really pleased about is to see some of

678
00:32:51,360 --> 00:32:55,709
our sake analysts start to really

679
00:32:52,950 --> 00:32:58,530
embrace this this idea of behavioral

680
00:32:55,710 --> 00:33:02,100
analytics so Luke Stella you can see

681
00:32:58,530 --> 00:33:05,129
here he's an analyst in in airsoft he is

682
00:33:02,100 --> 00:33:07,439
a security expert he knows his way

683
00:33:05,130 --> 00:33:11,160
around program having some however he's

684
00:33:07,440 --> 00:33:12,960
not a data scientist but what he started

685
00:33:11,160 --> 00:33:15,300
to do is start to look at rather than

686
00:33:12,960 --> 00:33:18,870
looking for hunting for specific things

687
00:33:15,300 --> 00:33:21,600
he started hunting for for behavioral

688
00:33:18,870 --> 00:33:23,879
based activities and the example that

689
00:33:21,600 --> 00:33:26,159
that he's looked at is analyzing the

690
00:33:23,880 --> 00:33:29,520
PowerShell Empire command and control

691
00:33:26,160 --> 00:33:32,040
framework looking at the behavioral he

692
00:33:29,520 --> 00:33:35,250
installed that Rena Rena through its its

693
00:33:32,040 --> 00:33:37,620
defaults and implantation and looked at

694
00:33:35,250 --> 00:33:41,130
the behavioral characteristics of a

695
00:33:37,620 --> 00:33:43,320
machine that beacons out to a c2 server

696
00:33:41,130 --> 00:33:47,100
and he compared that with the behavior

697
00:33:43,320 --> 00:33:51,419
of more typical web browsing so far

698
00:33:47,100 --> 00:33:53,040
we've only used proxy data with the next

699
00:33:51,420 --> 00:33:54,690
steps being will start correlating with

700
00:33:53,040 --> 00:34:01,649
other types of data such as endpoint

701
00:33:54,690 --> 00:34:05,550
data so one behavioral item we've looked

702
00:34:01,650 --> 00:34:07,470
at is how bursty the traffic is so let's

703
00:34:05,550 --> 00:34:11,639
imagine a scenario where a user

704
00:34:07,470 --> 00:34:15,120
navigates to google.com the proxy will

705
00:34:11,639 --> 00:34:17,520
record the request to Google as well as

706
00:34:15,120 --> 00:34:22,080
additional requests two related URLs to

707
00:34:17,520 --> 00:34:24,690
load images and text the user will

708
00:34:22,080 --> 00:34:27,659
typically spend some time reviewing the

709
00:34:24,690 --> 00:34:30,450
results of the search before clicking a

710
00:34:27,659 --> 00:34:32,750
link and sending another series of

711
00:34:30,449 --> 00:34:34,799
requests to a new site and some sites

712
00:34:32,750 --> 00:34:39,600
which will again be captured by the

713
00:34:34,800 --> 00:34:41,250
proxy and this process may repeat after

714
00:34:39,600 --> 00:34:42,540
another period of time of looking at the

715
00:34:41,250 --> 00:34:44,100
site that's been returned and working

716
00:34:42,540 --> 00:34:46,110
out where to go next you know its

717
00:34:44,100 --> 00:34:49,549
standard way that all of us browse the

718
00:34:46,110 --> 00:34:53,680
Internet and the amount of idle time

719
00:34:49,550 --> 00:34:56,150
between requests can vary greatly

720
00:34:53,679 --> 00:34:57,799
depending on how much time you spend on

721
00:34:56,150 --> 00:34:59,350
a particular page before navigating away

722
00:34:57,800 --> 00:35:02,780
from it

723
00:34:59,350 --> 00:35:06,460
now in contrast a machine that's been

724
00:35:02,780 --> 00:35:09,410
created through a remote access Trojan

725
00:35:06,460 --> 00:35:13,130
created through the default Empire

726
00:35:09,410 --> 00:35:16,279
framework has a much more regular series

727
00:35:13,130 --> 00:35:19,880
of requests to be more consistent time

728
00:35:16,280 --> 00:35:21,890
between requests so time T T plus 1 T

729
00:35:19,880 --> 00:35:25,580
plus 2 T plus 3 will be more consistent

730
00:35:21,890 --> 00:35:28,640
and it's likely to have a different

731
00:35:25,580 --> 00:35:34,700
pattern of content types or secondary

732
00:35:28,640 --> 00:35:38,930
requests so we can look here again an

733
00:35:34,700 --> 00:35:43,009
example of the browsing patterns broken

734
00:35:38,930 --> 00:35:45,620
up by hour of the day on the Left we can

735
00:35:43,010 --> 00:35:50,470
see that for a particular website there

736
00:35:45,620 --> 00:35:50,470
was a median of five requests per hour

737
00:35:50,500 --> 00:35:57,470
however between the hours of 2:00 p.m.

738
00:35:53,990 --> 00:35:59,359
and 3:00 p.m. there were 20 requests in

739
00:35:57,470 --> 00:36:02,000
in in that time so the variance between

740
00:35:59,360 --> 00:36:05,600
or the difference between the maximum

741
00:36:02,000 --> 00:36:08,810
requests and the median requests per

742
00:36:05,600 --> 00:36:11,630
hour there was a difference of 300%

743
00:36:08,810 --> 00:36:16,250
well one was yes mine was 300 sync rate

744
00:36:11,630 --> 00:36:20,540
in the other in contrast you can see

745
00:36:16,250 --> 00:36:22,490
that for the for the the c2 server or at

746
00:36:20,540 --> 00:36:25,250
least they the the workstation that was

747
00:36:22,490 --> 00:36:27,379
was bacon into a c2 server while there

748
00:36:25,250 --> 00:36:29,630
was some variance in the number of

749
00:36:27,380 --> 00:36:31,880
requests per hour it was far far less

750
00:36:29,630 --> 00:36:34,550
you know with only a 23% difference

751
00:36:31,880 --> 00:36:38,660
between the median and them and the

752
00:36:34,550 --> 00:36:41,390
other maximum now I do want to stress

753
00:36:38,660 --> 00:36:44,029
that that these numbers were mocked up

754
00:36:41,390 --> 00:36:46,460
using the framework for an attacker

755
00:36:44,030 --> 00:36:48,800
who's using the default framework this

756
00:36:46,460 --> 00:36:51,020
is the kind of thing you'd see a more

757
00:36:48,800 --> 00:36:53,780
sophisticated actor you would expect

758
00:36:51,020 --> 00:36:54,860
them to to you know this is the kind of

759
00:36:53,780 --> 00:36:57,620
thing they would probably look table

760
00:36:54,860 --> 00:36:59,480
obfuscated but I think it gives you a

761
00:36:57,620 --> 00:37:01,250
good good idea of the types of things

762
00:36:59,480 --> 00:37:04,930
though that

763
00:37:01,250 --> 00:37:04,930
we're playing within experimenting with

764
00:37:05,920 --> 00:37:11,720
so another technique can be used as

765
00:37:08,330 --> 00:37:14,630
analyzing referrals the majority of

766
00:37:11,720 --> 00:37:18,500
normal browsing is that you access a

767
00:37:14,630 --> 00:37:21,200
page via a link from another page the

768
00:37:18,500 --> 00:37:23,150
exceptions will be your favorites and

769
00:37:21,200 --> 00:37:25,250
maybe a very small number of sites we

770
00:37:23,150 --> 00:37:29,960
actually physically type in the address

771
00:37:25,250 --> 00:37:34,810
into into your browser so the example on

772
00:37:29,960 --> 00:37:36,560
the left and shows a call shows

773
00:37:34,810 --> 00:37:38,960
stackoverflow as being the example

774
00:37:36,560 --> 00:37:41,090
website that we used they were around

775
00:37:38,960 --> 00:37:44,869
about night there were 19 separate

776
00:37:41,090 --> 00:37:48,140
domains that called that website some of

777
00:37:44,869 --> 00:37:53,750
the examples are Google DuckDuckGo being

778
00:37:48,140 --> 00:37:57,529
and so on whereas for the malicious c2

779
00:37:53,750 --> 00:37:59,780
domain at least using the framework

780
00:37:57,530 --> 00:38:01,820
there was it wasn't caught from anything

781
00:37:59,780 --> 00:38:07,190
as it was just it was just navigated to

782
00:38:01,820 --> 00:38:10,250
directly we can we can apply similar

783
00:38:07,190 --> 00:38:14,890
analysis on whether a page is a referrer

784
00:38:10,250 --> 00:38:16,940
page again so from stackoverflow.com

785
00:38:14,890 --> 00:38:19,368
chances are some would navigate to

786
00:38:16,940 --> 00:38:20,960
another site so stack stack overflow is

787
00:38:19,369 --> 00:38:24,470
both being referred to and being

788
00:38:20,960 --> 00:38:27,560
referred from now a slightly more

789
00:38:24,470 --> 00:38:30,799
advanced attacker may spoof the request

790
00:38:27,560 --> 00:38:34,970
they can change the the HTTP HTTP header

791
00:38:30,800 --> 00:38:36,470
to make it look as though the c2 servers

792
00:38:34,970 --> 00:38:38,270
being called always been referred to

793
00:38:36,470 --> 00:38:41,230
from another page and that's where you

794
00:38:38,270 --> 00:38:43,849
might want to look at other other other

795
00:38:41,230 --> 00:38:45,800
pieces of data from the proxy logs or

796
00:38:43,849 --> 00:38:49,040
from the endpoint to correlate to say

797
00:38:45,800 --> 00:38:50,690
did the referrer that that was mentioned

798
00:38:49,040 --> 00:38:56,450
in the proxy log did it actually exist

799
00:38:50,690 --> 00:39:02,570
and you can see there's there's other

800
00:38:56,450 --> 00:39:05,750
types of data that we can analyze to to

801
00:39:02,570 --> 00:39:09,730
look at whether normal behavior or c2

802
00:39:05,750 --> 00:39:09,730
type behavior is what we're looking at

803
00:39:16,110 --> 00:39:21,450
so what should you do I find it hard to

804
00:39:19,350 --> 00:39:23,460
imagine that anyone who's at n of this

805
00:39:21,450 --> 00:39:25,830
conference this week hasn't at least

806
00:39:23,460 --> 00:39:27,720
started thinking a bit more about

807
00:39:25,830 --> 00:39:29,069
analytics about big data about machine

808
00:39:27,720 --> 00:39:31,350
learning it's it's um

809
00:39:29,070 --> 00:39:32,520
certainly a large part of what I've I've

810
00:39:31,350 --> 00:39:37,650
been attending I'm sure many of you that

811
00:39:32,520 --> 00:39:40,740
are the same if you do decide to get on

812
00:39:37,650 --> 00:39:42,930
this path I think it's it's really

813
00:39:40,740 --> 00:39:45,870
important that you start by getting your

814
00:39:42,930 --> 00:39:47,819
infrastructure right you get the big

815
00:39:45,870 --> 00:39:52,920
data platform in place make sure to set

816
00:39:47,820 --> 00:39:55,020
up ingest your first source of data but

817
00:39:52,920 --> 00:39:57,270
also think about things like getting the

818
00:39:55,020 --> 00:39:58,680
data lineage right up front really think

819
00:39:57,270 --> 00:40:00,930
about making sure that you really

820
00:39:58,680 --> 00:40:02,069
understand the data going to the

821
00:40:00,930 --> 00:40:04,230
platform assume that you're going to

822
00:40:02,070 --> 00:40:05,400
bring in large numbers of sources of

823
00:40:04,230 --> 00:40:08,760
data and you're going to have to manage

824
00:40:05,400 --> 00:40:11,180
that data also look at putting in

825
00:40:08,760 --> 00:40:13,350
structures to measure your data quality

826
00:40:11,180 --> 00:40:14,759
that's going to be something that helps

827
00:40:13,350 --> 00:40:17,640
you work out how far through you are

828
00:40:14,760 --> 00:40:19,440
it's also going to be helpful in going

829
00:40:17,640 --> 00:40:21,000
back and continue asking for the more

830
00:40:19,440 --> 00:40:23,070
funding to keep on going with this thing

831
00:40:21,000 --> 00:40:31,590
to be able to describe the the the

832
00:40:23,070 --> 00:40:33,570
overall journey that that you're on and

833
00:40:31,590 --> 00:40:35,700
it's also really important all this can

834
00:40:33,570 --> 00:40:37,620
be really helpful but don't describe it

835
00:40:35,700 --> 00:40:40,710
as a silver bullet to you for your

836
00:40:37,620 --> 00:40:41,850
organization we've heard that message a

837
00:40:40,710 --> 00:40:44,340
lot of times this week and it's

838
00:40:41,850 --> 00:40:48,270
absolutely true but having said that

839
00:40:44,340 --> 00:40:50,160
we've also heard that the attackers are

840
00:40:48,270 --> 00:40:51,720
all starting all the advanced attackers

841
00:40:50,160 --> 00:40:54,259
at least as are using machine learning

842
00:40:51,720 --> 00:40:56,939
they're using artificial intelligence

843
00:40:54,260 --> 00:41:00,450
not going down the same thing same path

844
00:40:56,940 --> 00:41:02,250
in defense potentially puts your

845
00:41:00,450 --> 00:41:04,669
organization further behind the

846
00:41:02,250 --> 00:41:04,670
attackers

847
00:41:08,120 --> 00:41:14,509
so some of the key lessons that we found

848
00:41:11,920 --> 00:41:17,600
firstly it really is a journey of

849
00:41:14,510 --> 00:41:20,750
learning starting the project the

850
00:41:17,600 --> 00:41:23,960
mindset that things will change will

851
00:41:20,750 --> 00:41:26,770
help you structure your team and work

852
00:41:23,960 --> 00:41:29,630
patterns to support this approach

853
00:41:26,770 --> 00:41:34,700
relationships the vendors are crucial

854
00:41:29,630 --> 00:41:35,810
and you need to I think also know that

855
00:41:34,700 --> 00:41:37,160
they're going to be on the learn Jean as

856
00:41:35,810 --> 00:41:41,509
well it needs to be a partnership

857
00:41:37,160 --> 00:41:43,490
approach that you take and usability is

858
00:41:41,510 --> 00:41:45,530
critical

859
00:41:43,490 --> 00:41:48,200
initially we focused a little bit too

860
00:41:45,530 --> 00:41:51,350
much on the algorithms and not really

861
00:41:48,200 --> 00:41:53,270
enough on how our analyst were actually

862
00:41:51,350 --> 00:41:54,799
going to use this data unfortunately

863
00:41:53,270 --> 00:41:57,470
when we first put the capability in

864
00:41:54,800 --> 00:42:00,440
place they end up with a whole heap of

865
00:41:57,470 --> 00:42:03,049
extra extra work we were giving them a

866
00:42:00,440 --> 00:42:05,120
lot of alerts some of those alerts were

867
00:42:03,050 --> 00:42:07,610
very Babel but it was still extra work

868
00:42:05,120 --> 00:42:09,799
for them to have to analyze what was

869
00:42:07,610 --> 00:42:12,050
going on investigate and we hadn't put

870
00:42:09,800 --> 00:42:14,330
enough thought into giving them that

871
00:42:12,050 --> 00:42:18,110
support to make sure that it wasn't just

872
00:42:14,330 --> 00:42:21,230
extra work for them so that's something

873
00:42:18,110 --> 00:42:23,000
that we've we did have to spend quite a

874
00:42:21,230 --> 00:42:24,560
bit extra time in in creating new play

875
00:42:23,000 --> 00:42:26,870
books for them to help help manage that

876
00:42:24,560 --> 00:42:28,250
that situation and and over the next 12

877
00:42:26,870 --> 00:42:31,700
months we'll be will be looking really

878
00:42:28,250 --> 00:42:33,710
seriously at security at orchestration

879
00:42:31,700 --> 00:42:35,660
and automation to try and again remove

880
00:42:33,710 --> 00:42:40,760
reduce their workload and not just keep

881
00:42:35,660 --> 00:42:42,259
pushing more alerts to them so in

882
00:42:40,760 --> 00:42:45,140
closing that this type of platform

883
00:42:42,260 --> 00:42:48,740
should be seen as a tool that evolves

884
00:42:45,140 --> 00:42:50,980
over time we've delivered a lot of what

885
00:42:48,740 --> 00:42:53,450
we intend to when we started the journey

886
00:42:50,980 --> 00:42:56,180
but we haven't we don't really see the

887
00:42:53,450 --> 00:42:58,370
finish line yet and we haven't even

888
00:42:56,180 --> 00:43:00,230
begun to explore the predictive

889
00:42:58,370 --> 00:43:01,730
capabilities I think this has been a

890
00:43:00,230 --> 00:43:02,920
good start for us but there's a long way

891
00:43:01,730 --> 00:43:04,670
to go

892
00:43:02,920 --> 00:43:06,080
though I think we need to keep

893
00:43:04,670 --> 00:43:08,230
remembering that the attackers are

894
00:43:06,080 --> 00:43:11,840
continually continually innovating and

895
00:43:08,230 --> 00:43:14,860
so must we are there any questions

896
00:43:11,840 --> 00:43:14,860
anyone has

897
00:43:35,440 --> 00:43:40,060
I think visualization is really

898
00:43:38,440 --> 00:43:41,380
important though you saw some of the

899
00:43:40,060 --> 00:43:44,049
pictures we saw before that's a way of

900
00:43:41,380 --> 00:43:48,100
visualization visualizing the you know

901
00:43:44,050 --> 00:43:50,040
that particular use case but for a lot

902
00:43:48,100 --> 00:43:53,380
of our other tools we haven't really

903
00:43:50,040 --> 00:43:55,330
spent a lot of time yet looking at the

904
00:43:53,380 --> 00:43:57,640
visualization it's been more focused on

905
00:43:55,330 --> 00:44:00,970
the alerts I think though that goes to

906
00:43:57,640 --> 00:44:03,100
my point a minute or so ago of where a

907
00:44:00,970 --> 00:44:05,770
lot of our focus has been on generating

908
00:44:03,100 --> 00:44:08,710
alerts and not necessarily enough but

909
00:44:05,770 --> 00:44:10,180
yet on making it really easy for those

910
00:44:08,710 --> 00:44:12,910
alerts to be consumed and I think that's

911
00:44:10,180 --> 00:44:15,819
where that's where visualization comes

912
00:44:12,910 --> 00:44:19,870
in you know we're using a range of

913
00:44:15,820 --> 00:44:21,160
different different tools to try and

914
00:44:19,870 --> 00:44:25,089
analyze the data but I don't think would

915
00:44:21,160 --> 00:44:26,470
really we probably won't reach a

916
00:44:25,090 --> 00:44:28,510
standard I think we will use different

917
00:44:26,470 --> 00:44:30,040
tools for different purposes but I'm

918
00:44:28,510 --> 00:44:31,450
probably not in the best position to

919
00:44:30,040 --> 00:44:32,830
talk about where we're going to end up

920
00:44:31,450 --> 00:44:34,960
going with that but I do think it is

921
00:44:32,830 --> 00:44:39,180
critical that that we provide that

922
00:44:34,960 --> 00:44:39,180
capability to to us to us off

923
00:44:46,110 --> 00:44:51,580
yes we're we're just going down the path

924
00:44:48,340 --> 00:44:52,720
at the moment of you know we'll be doing

925
00:44:51,580 --> 00:44:54,460
vendor selection all those sort of

926
00:44:52,720 --> 00:44:56,109
things over that over the coming coming

927
00:44:54,460 --> 00:44:57,670
months but we have part of that is in

928
00:44:56,110 --> 00:44:59,370
response to realizing that which is

929
00:44:57,670 --> 00:45:01,870
continually pushing more and more work

930
00:44:59,370 --> 00:45:03,009
onto the sock and we needed to help out

931
00:45:01,870 --> 00:45:05,290
on the other side otherwise we just need

932
00:45:03,010 --> 00:45:06,700
to keep continually growing which as we

933
00:45:05,290 --> 00:45:09,940
as we all know is it's pretty difficult

934
00:45:06,700 --> 00:45:11,109
to in this labor market so I'm that's

935
00:45:09,940 --> 00:45:13,840
been a lot of the drive for there as

936
00:45:11,110 --> 00:45:15,700
well as other things we've by by

937
00:45:13,840 --> 00:45:19,420
creating this platform we've now

938
00:45:15,700 --> 00:45:21,069
actually given ourselves a lot more

939
00:45:19,420 --> 00:45:23,260
opportunity it's doing more things so

940
00:45:21,070 --> 00:45:27,490
another example that's not directly

941
00:45:23,260 --> 00:45:29,230
related to to thread hunting is we've

942
00:45:27,490 --> 00:45:31,870
now bought brought all about threat

943
00:45:29,230 --> 00:45:33,250
intelligence into the platform so that

944
00:45:31,870 --> 00:45:36,310
then allows it because we've got our

945
00:45:33,250 --> 00:45:39,160
threat Intel in there we've got our

946
00:45:36,310 --> 00:45:41,470
telemetry and fairly soon we'll have our

947
00:45:39,160 --> 00:45:44,140
our security incident data in the

948
00:45:41,470 --> 00:45:47,049
platform we can start to do analytics

949
00:45:44,140 --> 00:45:48,520
for example on on how valuable or each

950
00:45:47,050 --> 00:45:51,880
of our threat intelligence feeds and

951
00:45:48,520 --> 00:45:53,230
start to correlate well piece of

952
00:45:51,880 --> 00:45:55,180
intelligence how does that work with

953
00:45:53,230 --> 00:45:56,740
with the changed firewall or with an

954
00:45:55,180 --> 00:45:58,390
incident and started to do a lot more of

955
00:45:56,740 --> 00:45:59,919
that kind of correlations well so I

956
00:45:58,390 --> 00:46:03,850
think there's a lot of opportunity there

957
00:45:59,920 --> 00:46:07,320
the the platform has bought not only in

958
00:46:03,850 --> 00:46:07,319
a slicer for hunting for threats

959
00:46:34,260 --> 00:46:38,680
yeah we had a lot of debate about that

960
00:46:36,700 --> 00:46:40,930
early on things you know there was a

961
00:46:38,680 --> 00:46:43,960
camp saying well let's do all the work

962
00:46:40,930 --> 00:46:46,270
upfront but what we decided to do was

963
00:46:43,960 --> 00:46:47,830
bring data in as raw or format as

964
00:46:46,270 --> 00:46:49,900
possible

965
00:46:47,830 --> 00:46:51,790
sometimes we actually both sometimes we

966
00:46:49,900 --> 00:46:56,410
bring in the raw format and the and the

967
00:46:51,790 --> 00:46:59,020
I guess that the the data that have been

968
00:46:56,410 --> 00:47:01,270
cleansed but we thought by having the

969
00:46:59,020 --> 00:47:03,520
format as raw as possible that gave us

970
00:47:01,270 --> 00:47:05,280
the best best chance of seeing what was

971
00:47:03,520 --> 00:47:07,030
really going on and not what had been

972
00:47:05,280 --> 00:47:09,150
what might have been transformed

973
00:47:07,030 --> 00:47:11,310
particularly where a lot of the training

974
00:47:09,150 --> 00:47:14,800
some of the choices of day that we had

975
00:47:11,310 --> 00:47:16,240
might have been for um it may have been

976
00:47:14,800 --> 00:47:18,220
transformed for a particular purpose

977
00:47:16,240 --> 00:47:19,209
that wasn't directly for security it

978
00:47:18,220 --> 00:47:22,029
might have been for an operational

979
00:47:19,210 --> 00:47:23,470
purpose that had been transformed that

980
00:47:22,030 --> 00:47:26,230
is one of the advantages of ever dupe

981
00:47:23,470 --> 00:47:27,910
you know I've worked on data warehouse

982
00:47:26,230 --> 00:47:29,500
projects we're getting the data model

983
00:47:27,910 --> 00:47:31,779
right is critical if you don't get it

984
00:47:29,500 --> 00:47:34,030
right up front there's a whole heap of

985
00:47:31,780 --> 00:47:36,100
things you can't do whereas with a dupe

986
00:47:34,030 --> 00:47:38,140
you do get a lot more flexibility to

987
00:47:36,100 --> 00:47:42,220
have multiple data models on the same

988
00:47:38,140 --> 00:47:45,520
piece of data so you know you know hfts

989
00:47:42,220 --> 00:47:46,930
the Hadoop file system we might have the

990
00:47:45,520 --> 00:47:48,850
same data in multiple places in

991
00:47:46,930 --> 00:47:50,890
different formats we call them data

992
00:47:48,850 --> 00:47:54,310
products but that's that's something

993
00:47:50,890 --> 00:47:55,930
that has been quite helpful I think is a

994
00:47:54,310 --> 00:48:01,529
you know it's a it's a power powerful

995
00:47:55,930 --> 00:48:01,529
thing about that platform yeah

996
00:48:12,410 --> 00:48:17,509
some of the items that we've brought in

997
00:48:14,150 --> 00:48:19,549
a proprietary button some of them were

998
00:48:17,509 --> 00:48:21,440
purchased for example the the Pascal

999
00:48:19,549 --> 00:48:24,109
model is is a is a proprietor we've

1000
00:48:21,440 --> 00:48:25,789
purchased that's something that we'll

1001
00:48:24,109 --> 00:48:27,529
look at I think in the future certainly

1002
00:48:25,789 --> 00:48:30,589
this presentation is part of part of

1003
00:48:27,529 --> 00:48:33,470
that sharing I think that's something

1004
00:48:30,589 --> 00:48:36,200
we'll have to look at also talk about a

1005
00:48:33,470 --> 00:48:38,479
patchy spot as being a potential place

1006
00:48:36,200 --> 00:48:39,680
that we might end up sharing certainly

1007
00:48:38,479 --> 00:48:42,319
where we're interested in potentially

1008
00:48:39,680 --> 00:48:44,930
taking some of that those types of

1009
00:48:42,319 --> 00:48:46,489
models and so that might might be the

1010
00:48:44,930 --> 00:48:47,509
kind of place that we potentially share

1011
00:48:46,489 --> 00:48:49,609
but that's something we haven't really

1012
00:48:47,509 --> 00:48:53,619
looked at yet we've been just trying to

1013
00:48:49,609 --> 00:48:53,619
get get our own capability up first you

1014
00:48:55,509 --> 00:49:05,440
know the questions thanks very much

1015
00:49:00,489 --> 00:49:05,440
Thank You Damien and thank everybody

