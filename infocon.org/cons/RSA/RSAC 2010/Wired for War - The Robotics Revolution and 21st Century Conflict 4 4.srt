1
00:00:00,089 --> 00:00:03,830
laws in particular in war are

2
00:00:03,830 --> 00:00:06,960
under siege from two sides that is

3
00:00:06,960 --> 00:00:10,840
the prevailing laws of war the Geneva
Conventions are so old that if they were

4
00:00:10,840 --> 00:00:11,590
people

5
00:00:11,590 --> 00:00:16,240
they would qualify for Medicare and
actually were trying to apply them to a

6
00:00:16,240 --> 00:00:17,990
21st century technology

7
00:00:17,990 --> 00:00:21,609
like a Reaper drone it's been utilized
again to 21st century

8
00:00:21,609 --> 00:00:26,070
have the Syrian war like an insurgent
who hides out in the hospital or school

9
00:00:26,070 --> 00:00:26,880
or home

10
00:00:26,880 --> 00:00:30,019
because they know that's what they're
not supposed to do

11
00:00:30,019 --> 00:00:34,829
the taking advantage of the Los I
remember being at Human Rights Watch

12
00:00:34,829 --> 00:00:39,050
and asking to move the officials there
what law would we turn to in these kinda

13
00:00:39,050 --> 00:00:39,719
been since

14
00:00:39,719 --> 00:00:43,690
and 1 I've they got an argument from me
and one of them said with a gene

15
00:00:43,690 --> 00:00:45,510
conventions like we've always turn to
you

16
00:00:45,510 --> 00:00:48,789
and the other one immediately yelled
back at him nah nah nah Geneva

17
00:00:48,789 --> 00:00:50,090
Convention won't work in this

18
00:00:50,090 --> 00:00:53,410
what we should turn to is the star Trak
prime directive

19
00:00:53,410 --> 00:00:56,520
it's kinda funny but

20
00:00:56,520 --> 00:01:00,309
the point is this are I love track i
cant

21
00:01:00,309 --> 00:01:03,379
call captain kirk as an expert witness
in a

22
00:01:03,379 --> 00:01:07,990
real court of law that is it may be
inspirational but we're talking about

23
00:01:07,990 --> 00:01:09,380
real technology right now

24
00:01:09,380 --> 00:01:12,630
and real legal dilemmas and this leads
to

25
00:01:12,630 --> 00:01:16,820
what i think is the last ripple effect
that's important ethics I dove of this

26
00:01:16,820 --> 00:01:21,310
now when you talk about robots and
ethics people say oh you mean awesome of

27
00:01:21,310 --> 00:01:22,110
laws right

28
00:01:22,110 --> 00:01:26,700
wrong there's three problems with
Asimov's Three loss

29
00:01:26,700 --> 00:01:31,240
the first is their fiction how would you

30
00:01:31,240 --> 00:01:34,520
program how would you turn Robuchon not
harm human

31
00:01:34,520 --> 00:01:37,540
into a series of 0's and 1's not that
easy

32
00:01:37,540 --> 00:01:42,479
the second problem is their fiction as a
mock created them

33
00:01:42,479 --> 00:01:47,130
as plot devices and all of the stories
they try and follow the laws but end up

34
00:01:47,130 --> 00:01:51,689
violating them as the poster for the
movie version put love I Robot put it

35
00:01:51,689 --> 00:01:55,430
rules were made to be broken and then
the third problem

36
00:01:55,430 --> 00:02:00,619
is were deliberately violating these
laws you don't give a robotic system

37
00:02:00,619 --> 00:02:04,590
a .50-caliber machine gun or a Hellfire
missile if not to harm humans

38
00:02:04,590 --> 00:02:08,369
that's kinda the point of it or I don't
want a system that takes

39
00:02:08,369 --> 00:02:11,830
orders from any human like they would
under are some of los I don't wanna

40
00:02:11,830 --> 00:02:14,040
robot to walk up to Osama bin Laden

41
00:02:14,040 --> 00:02:17,650
and he can reprogram it easily enough
most importantly

42
00:02:17,650 --> 00:02:21,599
is that that's a bit over misdirection
it's not the ethics

43
00:02:21,599 --> 00:02:26,230
on the robots that matter it's the
ethics the people behind the robots that

44
00:02:26,230 --> 00:02:26,690
matter

45
00:02:26,690 --> 00:02:31,769
think about these questions what kinda
robot should we build what kinda

46
00:02:31,769 --> 00:02:33,129
software should we design

47
00:02:33,129 --> 00:02:36,510
work on actual future they have what
should we not

48
00:02:36,510 --> 00:02:40,769
what organizations and individuals
should be allowed

49
00:02:40,769 --> 00:02:44,959
2009 UC system's which one should be
prevented

50
00:02:44,959 --> 00:02:48,620
what kind of training our licensing
should they be required to have

51
00:02:48,620 --> 00:02:52,050
who is it ethical to take money from in
research and design

52
00:02:52,050 --> 00:02:55,750
who was it not ethical to take money
from who should

53
00:02:55,750 --> 00:02:59,720
own or be able to access the information
the unmanned systems are

54
00:02:59,720 --> 00:03:03,340
always gathering about the world around
them who should be prevented from that

55
00:03:03,340 --> 00:03:08,260
all these are innocence ethical
questions but they're also

56
00:03:08,260 --> 00:03:11,519
legal questions because ethics without
accountability is empty

57
00:03:11,519 --> 00:03:16,560
the problem here is this if you were a
young software programmer if you were a

58
00:03:16,560 --> 00:03:17,769
young robot assist

59
00:03:17,769 --> 00:03:22,750
you wouldn't have a code about six to
turn to for guidance and assistance

60
00:03:22,750 --> 00:03:27,000
like you would for example what you were
doing medical research and the broader

61
00:03:27,000 --> 00:03:27,870
field

62
00:03:27,870 --> 00:03:32,690
has a challenge right now that it isn't
pushing is kinda ethical dialogues like

63
00:03:32,690 --> 00:03:35,159
other cutting-edge fields are for
example the field

64
00:03:35,159 --> 00:03:38,780
genetics is pushing discussions about it
impact on society

65
00:03:38,780 --> 00:03:42,540
and most disappointing to me

66
00:03:42,540 --> 00:03:46,230
wasn't email that I received after our
talk a lot like this

67
00:03:46,230 --> 00:03:50,849
and I give a talk at an engineering
school and a professor chastise me in

68
00:03:50,849 --> 00:03:52,159
the email for quote

69
00:03:52,159 --> 00:03:57,040
troubling his students by quote asking
them to think about the ethics of their

70
00:03:57,040 --> 00:03:57,599
work

71
00:03:57,599 --> 00:04:01,319
so in ending

72
00:04:01,319 --> 00:04:04,379
it sounds like I've been talking about
the future

73
00:04:04,379 --> 00:04:08,060
but notice how every single picture you
saw

74
00:04:08,060 --> 00:04:11,680
every single example I gave you was not
from the future technically

75
00:04:11,680 --> 00:04:15,150
it was from the past the war and so it's
up to challenge

76
00:04:15,150 --> 00:04:19,479
for all of us well before have to worry
about your room by sneaking up on you

77
00:04:19,479 --> 00:04:23,780
its the fact that what is unveiling its
of right now

78
00:04:23,780 --> 00:04:27,320
looks like science fiction feels like
science fiction

79
00:04:27,320 --> 00:04:30,940
but it is already part OVR reality

80
00:04:30,940 --> 00:04:34,840
and go back to those examples I gave you
have passed technologies

81
00:04:34,840 --> 00:04:39,470
many of which came out in the realm of
science fiction whether it was

82
00:04:39,470 --> 00:04:42,960
flying machines or the atomic bomb

83
00:04:42,960 --> 00:04:46,520
which HG wells was the first one to
conceal up even gave the atomic bomb its

84
00:04:46,520 --> 00:04:48,009
name

85
00:04:48,009 --> 00:04:52,639
we didn't wrestle with the implications
the science fiction like technologies

86
00:04:52,639 --> 00:04:57,290
until after the fact and so we kinda
laughed at the mistakes they made back

87
00:04:57,290 --> 00:04:57,800
then

88
00:04:57,800 --> 00:05:01,960
like having the person walk in front of
the automobile with a flag in firing of

89
00:05:01,960 --> 00:05:07,270
where our future generations going to
laugh at us the same way

90
00:05:07,270 --> 00:05:10,289
so unending what I'd like to do is
actually jumped into the realm of

91
00:05:10,289 --> 00:05:12,830
science fiction for a sec

92
00:05:12,830 --> 00:05:16,990
a couple years ago the AFI the American
Film Institute

93
00:05:16,990 --> 00:05:20,360
did a survey up all the characters that
it

94
00:05:20,360 --> 00:05:24,600
ever been in hollywood movie over the
last century have movies

95
00:05:24,600 --> 00:05:28,360
and how to follow those characters which
ones represented

96
00:05:28,360 --> 00:05:31,990
humanity at its best and which ones
represented

97
00:05:31,990 --> 00:05:36,310
humanity at its worst that is which were
the top 100 hollywood heroes

98
00:05:36,310 --> 00:05:39,940
and which were the top 100 Hollywood
villains

99
00:05:39,940 --> 00:05:43,960
I've every single movie ever made only
one character

100
00:05:43,960 --> 00:05:49,710
made it onto both less up top 100 heroes
in top 100 villains

101
00:05:49,710 --> 00:05:53,350
the Terminator a robot killing machine

102
00:05:53,350 --> 00:05:57,310
and to the point here is this it
illustrates the duality

103
00:05:57,310 --> 00:06:02,720
a bar technology it can be used for both
good and evil

104
00:06:02,720 --> 00:06:06,229
but I think it also do illustrates the
duality of us

105
00:06:06,229 --> 00:06:10,500
the people behind the technology its
human creativity

106
00:06:10,500 --> 00:06:13,750
that really is what distinguishes us
from all the other species

107
00:06:13,750 --> 00:06:16,930
it's our human creativity that invented
fire

108
00:06:16,930 --> 00:06:20,319
it's our human creativity that took our
species to the moon

109
00:06:20,319 --> 00:06:24,620
its creativity that don't work so are
literature and architecture

110
00:06:24,620 --> 00:06:28,860
to show our love and appreciation for
beauty and now we're using human

111
00:06:28,860 --> 00:06:29,740
creativity

112
00:06:29,740 --> 00:06:32,910
to build this incredible technology that

113
00:06:32,910 --> 00:06:37,150
if you believe both the scientists and
the science fiction writers or

114
00:06:37,150 --> 00:06:41,520
the Battlestar Galactica fans we may be
literally creating an entirely new

115
00:06:41,520 --> 00:06:43,900
species

116
00:06:43,900 --> 00:06:48,509
but if we're honest with ourselves
really doing it because we can't push

117
00:06:48,509 --> 00:06:49,340
past

118
00:06:49,340 --> 00:06:53,400
that age-old human need to fight one
another

119
00:06:53,400 --> 00:06:56,710
and so the ending question on review is
this

120
00:06:56,710 --> 00:07:00,330
is it the technology

121
00:07:00,330 --> 00:07:03,449
or is it us that's wired for war thank
you

