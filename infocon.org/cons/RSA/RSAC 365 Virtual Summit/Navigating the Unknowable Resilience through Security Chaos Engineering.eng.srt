1
00:00:00,220 --> 00:00:02,780
- Hello everybody, and
welcome to our session

2
00:00:02,780 --> 00:00:04,840
on Navigating the Unknowable:

3
00:00:04,840 --> 00:00:08,890
Creating Resilience with
Security Chaos Engineering.

4
00:00:08,890 --> 00:00:11,750
We are so happy to be here today.

5
00:00:11,750 --> 00:00:14,820
We wanna thank RSA for
highlighting our work

6
00:00:14,820 --> 00:00:17,690
within Resilience and
Security Chaos Engineering,

7
00:00:17,690 --> 00:00:22,690
and for having us be a part of
their RSA 365 Virtual Series.

8
00:00:22,780 --> 00:00:25,393
It is truly, truly an honor.

9
00:00:26,430 --> 00:00:28,410
My name is Jamie Dicken

10
00:00:28,410 --> 00:00:32,680
and I am the manager of Applied
Security at Cardinal Health.

11
00:00:32,680 --> 00:00:34,000
I lead a team that's focused on

12
00:00:34,000 --> 00:00:35,680
continuous security validation

13
00:00:35,680 --> 00:00:38,280
and security chaos engineering.

14
00:00:38,280 --> 00:00:41,890
And my co-speaker today is the
one and only Aaron Rinehart.

15
00:00:41,890 --> 00:00:44,100
He is the CTO of Verica.

16
00:00:44,100 --> 00:00:48,000
He is one of the pioneers of
Security Chaos Engineering.

17
00:00:48,000 --> 00:00:50,360
He was the leader of ChaosSlingr

18
00:00:50,360 --> 00:00:51,870
back at UnitedHealth Group.

19
00:00:51,870 --> 00:00:55,010
And he is actually the
co-author of an O'Reilly Report

20
00:00:55,010 --> 00:00:58,170
on security chaos engineering.

21
00:00:58,170 --> 00:01:00,520
Like I said, we are just
so excited to be here

22
00:01:00,520 --> 00:01:03,540
and we have an awesome talk
lined up for you today.

23
00:01:03,540 --> 00:01:06,120
We're going to share

24
00:01:06,120 --> 00:01:08,620
how we believe that
security chaos engineering

25
00:01:08,620 --> 00:01:10,750
is actually the solution

26
00:01:10,750 --> 00:01:13,800
to a lot of the engineering
challenges we've had

27
00:01:13,800 --> 00:01:16,030
over the past couple of decades.

28
00:01:16,030 --> 00:01:18,320
We will also give you
some real world examples

29
00:01:18,320 --> 00:01:21,270
of how we leverage
security chaos engineering

30
00:01:21,270 --> 00:01:22,160
at our companies,

31
00:01:22,160 --> 00:01:23,759
and tell you how you can go ahead

32
00:01:23,760 --> 00:01:25,803
and get started as well.

33
00:01:26,900 --> 00:01:29,890
So whatever your area of expertise,

34
00:01:29,890 --> 00:01:31,010
I think it's safe to say that

35
00:01:31,010 --> 00:01:34,370
we can all align on one basic truth,

36
00:01:34,370 --> 00:01:37,550
and that's that system
engineering is messy.

37
00:01:37,550 --> 00:01:39,330
So in the beginning,

38
00:01:39,330 --> 00:01:42,399
we start with these beautifully
simplistic representations

39
00:01:42,400 --> 00:01:44,620
of either what we want to build,

40
00:01:44,620 --> 00:01:46,560
or what we think that we did build.

41
00:01:46,560 --> 00:01:49,070
But it doesn't take long for us to sour on

42
00:01:49,070 --> 00:01:51,750
those initially perfect creations.

43
00:01:51,750 --> 00:01:54,700
And it's not always our fault,

44
00:01:54,700 --> 00:01:57,090
it's just that complexity has a way

45
00:01:57,090 --> 00:01:59,310
of sneaking into our system.

46
00:01:59,310 --> 00:02:01,670
And as time goes on,

47
00:02:01,670 --> 00:02:04,610
our problems seem to compound

48
00:02:04,610 --> 00:02:07,970
and we get to the point
where we recognize that

49
00:02:07,970 --> 00:02:11,330
what we're doing is fighting
battle after battle.

50
00:02:11,330 --> 00:02:14,090
And what we need is a new radical way

51
00:02:14,090 --> 00:02:17,100
to secure and stabilize our systems,

52
00:02:17,100 --> 00:02:19,423
if we ever wanna get ahead of this.

53
00:02:20,450 --> 00:02:22,109
The problem that we've been facing

54
00:02:22,110 --> 00:02:24,210
is that the old school approach

55
00:02:24,210 --> 00:02:27,180
to both security and site reliability

56
00:02:27,180 --> 00:02:29,770
was that it was design-oriented.

57
00:02:29,770 --> 00:02:33,830
And so in theory, if you
wanted to assess a system,

58
00:02:33,830 --> 00:02:36,290
you would go and you would
take a look at things

59
00:02:36,290 --> 00:02:38,230
like single points of failure.

60
00:02:38,230 --> 00:02:41,260
You'd pull up your infrastructure
diagrams and identify

61
00:02:41,260 --> 00:02:43,500
where could latency possibly occur?

62
00:02:43,500 --> 00:02:47,090
What different components
can interact with each other?

63
00:02:47,090 --> 00:02:51,520
But we all know that there are
inherent problems with that.

64
00:02:51,520 --> 00:02:53,670
So think about first of all,

65
00:02:53,670 --> 00:02:56,489
when is this documentation created?

66
00:02:56,490 --> 00:02:59,330
If it's created prior to a deployment,

67
00:02:59,330 --> 00:03:02,340
then our documentation
might not even reflect

68
00:03:02,340 --> 00:03:04,933
the system that went into production.

69
00:03:05,870 --> 00:03:08,930
If the documentation
was created afterwards,

70
00:03:08,930 --> 00:03:12,410
it's still dependent on
the lens of the person

71
00:03:12,410 --> 00:03:14,220
who created that documentation,

72
00:03:14,220 --> 00:03:15,913
and the memory that they have.

73
00:03:16,800 --> 00:03:19,240
Then you gotta figure, is it updated?

74
00:03:19,240 --> 00:03:20,780
And if it's updated,

75
00:03:20,780 --> 00:03:23,560
does it still show every
point of integration?

76
00:03:23,560 --> 00:03:26,073
Does it show all of
the downstream effects?

77
00:03:27,070 --> 00:03:28,640
The challenge that we have

78
00:03:28,640 --> 00:03:33,510
is that no matter how we approach this,

79
00:03:33,510 --> 00:03:36,359
our process fails

80
00:03:36,360 --> 00:03:39,240
because it assumes that the
documented representation

81
00:03:39,240 --> 00:03:41,720
of a system is actually correct.

82
00:03:41,720 --> 00:03:43,820
And I'm sure that a lot of us as engineers

83
00:03:43,820 --> 00:03:45,140
have seen plenty of times

84
00:03:45,140 --> 00:03:47,640
where that assumption doesn't hold.

85
00:03:47,640 --> 00:03:50,089
If you process of a system...

86
00:03:50,090 --> 00:03:52,930
If your process of
evaluating a system rather

87
00:03:52,930 --> 00:03:54,610
is dependent on

88
00:03:54,610 --> 00:03:55,810
just an outdated

89
00:03:55,810 --> 00:03:59,370
or a flat out incorrect
representation of the system,

90
00:03:59,370 --> 00:04:02,370
whether you misremember it,

91
00:04:02,370 --> 00:04:05,780
or you didn't know all of
the details from the start,

92
00:04:05,780 --> 00:04:09,360
your evaluation of that system
is ultimately going to fail.

93
00:04:09,360 --> 00:04:11,300
And there are going to
be plenty of problems

94
00:04:11,300 --> 00:04:12,793
that you don't anticipate.

95
00:04:13,970 --> 00:04:16,610
It was complexity
scientists, David Snowden,

96
00:04:16,610 --> 00:04:18,286
who told us that,

97
00:04:18,286 --> 00:04:20,126
"it's actually impossible

98
00:04:20,127 --> 00:04:22,627
"for a human to model, document,

99
00:04:22,627 --> 00:04:25,037
"or diagram a complex system.

100
00:04:25,037 --> 00:04:27,676
"The only way to understand it

101
00:04:27,677 --> 00:04:29,627
"is to actually interact with it."

102
00:04:30,640 --> 00:04:32,530
So then the answer really

103
00:04:32,530 --> 00:04:36,210
is not to rely on our own
recollection of our systems,

104
00:04:36,210 --> 00:04:38,489
but rather to learn from them

105
00:04:38,490 --> 00:04:40,710
and to use empirical data.

106
00:04:40,710 --> 00:04:44,180
And that's really what the
heart of chaos engineering is,

107
00:04:44,180 --> 00:04:47,270
implementing experiments
that clearly remind us

108
00:04:47,270 --> 00:04:50,060
what our real system landscapes look like,

109
00:04:50,060 --> 00:04:53,760
so that we can start to tease
out those false assumptions.

110
00:04:53,760 --> 00:04:55,510
So Aaron, I'm gonna turn it over to you

111
00:04:55,510 --> 00:04:57,942
to tell us how to do things differently.

112
00:04:57,942 --> 00:05:00,240
(keyboard key tapping)
- Thanks Jamie.

113
00:05:00,240 --> 00:05:02,150
So really it's about evolving

114
00:05:02,150 --> 00:05:04,522
towards a new approach of learning.

115
00:05:07,760 --> 00:05:09,980
You know, and the new approach to learning

116
00:05:11,260 --> 00:05:12,750
is one that moves us away

117
00:05:12,750 --> 00:05:16,310
from a process of continuously
fixing and responding.

118
00:05:16,310 --> 00:05:18,210
When you're constantly in a war room

119
00:05:18,210 --> 00:05:21,630
or are responding to an active incident,

120
00:05:21,630 --> 00:05:24,080
those are not good opportunities to learn.

121
00:05:24,080 --> 00:05:27,219
People are worried about
being blamed, named or shamed.

122
00:05:27,220 --> 00:05:30,370
It's not a good opportunity to learn.

123
00:05:30,370 --> 00:05:34,150
Both chaos engineering and
security chaos engineering

124
00:05:34,150 --> 00:05:36,219
are a proactive exercise

125
00:05:36,220 --> 00:05:38,350
where there is no incident
in a war room going on.

126
00:05:38,350 --> 00:05:40,453
We're able to learn with eyes wide open.

127
00:05:43,030 --> 00:05:45,159
So this begs the question that

128
00:05:45,160 --> 00:05:47,380
how does the system become stable?

129
00:05:47,380 --> 00:05:51,690
So earlier on in my journey
with security chaos engineering,

130
00:05:51,690 --> 00:05:52,523
I ran into

131
00:05:52,523 --> 00:05:54,900
one of the world's largest
payment processing companies.

132
00:05:54,900 --> 00:05:56,989
And this engineer was
describing a situation

133
00:05:56,990 --> 00:06:00,020
where their legacy flagship application...

134
00:06:01,230 --> 00:06:04,120
it was application that ran
all of the payment transactions

135
00:06:04,120 --> 00:06:06,850
for the large payment processing company.

136
00:06:06,850 --> 00:06:11,340
And they thought the engineers
were competent, confident,

137
00:06:11,340 --> 00:06:13,260
understood how the system operated well.

138
00:06:13,260 --> 00:06:15,310
It really had an incident and an outage

139
00:06:15,310 --> 00:06:17,470
and for scalability reasons,

140
00:06:17,470 --> 00:06:19,100
they needed to move it over to Kubernetes.

141
00:06:19,100 --> 00:06:22,180
And they were concerned
about this transition

142
00:06:22,180 --> 00:06:25,120
and the lack of stability of Kubernetes.

143
00:06:25,120 --> 00:06:26,510
Well, I started thinking to myself

144
00:06:26,510 --> 00:06:29,640
was that legacy system always stable?

145
00:06:29,640 --> 00:06:31,289
How did it become stable?

146
00:06:31,290 --> 00:06:33,060
Well, it became stable

147
00:06:33,060 --> 00:06:35,250
and how the engineers became
confident and competent

148
00:06:35,250 --> 00:06:37,720
through a series of unforeseen events.

149
00:06:37,720 --> 00:06:39,620
They learned about what
the differences was

150
00:06:39,620 --> 00:06:41,620
between how they believed
the system worked

151
00:06:41,620 --> 00:06:43,210
and how it actually worked in reality

152
00:06:43,210 --> 00:06:45,210
through a series of unforeseen events.

153
00:06:45,210 --> 00:06:48,260
Unfortunately learning
through that process,

154
00:06:48,260 --> 00:06:50,150
it counters customer pain.

155
00:06:50,150 --> 00:06:52,280
So with chaos engineering and
security chaos engineering

156
00:06:52,280 --> 00:06:54,049
we're trying to be proactive,

157
00:06:54,050 --> 00:06:57,100
surface the difference between

158
00:06:57,100 --> 00:06:58,670
what we believe our system is

159
00:06:58,670 --> 00:07:00,270
and how it functions in reality

160
00:07:00,270 --> 00:07:02,943
before it manifests in the customer pain.

161
00:07:06,560 --> 00:07:08,460
So it's also important
like you described before

162
00:07:08,460 --> 00:07:11,030
as people are in a
different cognitive space

163
00:07:11,030 --> 00:07:14,020
when an incident or outage occurs,

164
00:07:14,020 --> 00:07:16,650
especially security outage or incidents,

165
00:07:16,650 --> 00:07:18,789
is that people are worried

166
00:07:18,790 --> 00:07:21,920
about being blamed, named, and shamed.

167
00:07:21,920 --> 00:07:23,300
And people are worried
about losing their jobs

168
00:07:23,300 --> 00:07:25,513
'cause let's be honest, it's
not about what happened.

169
00:07:25,513 --> 00:07:27,409
It's about get that
thing back up and running

170
00:07:27,410 --> 00:07:28,600
we're losing money

171
00:07:28,600 --> 00:07:30,533
and that's not a good
learning environment.

172
00:07:33,330 --> 00:07:35,030
So chaos engineering,

173
00:07:35,030 --> 00:07:36,960
it work bits in the
world of instrumentation,

174
00:07:36,960 --> 00:07:39,150
somewhat of a loose definition

175
00:07:39,150 --> 00:07:40,450
but folks in the space

176
00:07:40,450 --> 00:07:43,000
like to break the instrumentation down

177
00:07:43,000 --> 00:07:45,870
as the testing and experimentation.

178
00:07:45,870 --> 00:07:48,180
So testing would be the
verification or validation

179
00:07:48,180 --> 00:07:50,622
of something we know to be true or false.

180
00:07:50,622 --> 00:07:51,830
In our world of security

181
00:07:51,830 --> 00:07:54,030
it's like a (indistinct),
an attack pattern,

182
00:07:55,270 --> 00:07:56,950
you know, something we
already kind of know

183
00:07:56,950 --> 00:07:58,513
and we can instrument for.

184
00:07:59,440 --> 00:08:01,110
Whereas experimentation

185
00:08:01,110 --> 00:08:03,060
we're trying to drive new information

186
00:08:04,590 --> 00:08:05,729
that we didn't have before.

187
00:08:05,730 --> 00:08:08,397
Meaning about the unknown
unknowns of the system.

188
00:08:08,397 --> 00:08:09,849
And we do that in the form of

189
00:08:09,850 --> 00:08:14,850
I believe if X occurs on my
system, Y is the prior result?

190
00:08:14,890 --> 00:08:17,250
We never do a case experiment
we know is going to fail.

191
00:08:17,250 --> 00:08:19,740
We only do what we think
is going to be true

192
00:08:19,740 --> 00:08:21,400
'cause if you know it's
gonna fail, just fix it.

193
00:08:21,400 --> 00:08:23,467
'Cause you're not gonna
learn anything new.

194
00:08:26,010 --> 00:08:27,120
Chaos Engineering.

195
00:08:27,120 --> 00:08:29,380
So my definition of chaos engineering

196
00:08:29,380 --> 00:08:31,440
is it's the idea or is the technique

197
00:08:31,440 --> 00:08:34,350
of proactively introducing
turbulent conditions

198
00:08:34,350 --> 00:08:35,890
into a distributed system

199
00:08:35,890 --> 00:08:37,240
to try to determine the conditions

200
00:08:37,240 --> 00:08:41,789
by which the system will fail
before it actually fails.

201
00:08:41,789 --> 00:08:43,462
It's a proactive exercise.

202
00:08:44,950 --> 00:08:46,870
Security Chaos Engineering newsflash

203
00:08:46,870 --> 00:08:48,330
is not a whole lot different.

204
00:08:48,330 --> 00:08:51,380
Really the security bits
are just really more in tune

205
00:08:51,380 --> 00:08:53,010
with the use cases and the value

206
00:08:53,010 --> 00:08:56,100
in terms of the engineering
aspects of security

207
00:08:56,100 --> 00:08:58,723
we're attempting to instrument.

208
00:09:02,050 --> 00:09:04,900
So one of the goals, the (indistinct)

209
00:09:04,900 --> 00:09:06,780
we're trying to achieve

210
00:09:06,780 --> 00:09:08,800
is we're trying to understand

211
00:09:08,800 --> 00:09:11,430
where are the gaps or the (indistinct)

212
00:09:11,430 --> 00:09:16,319
that allow malicious
activity to be successful

213
00:09:16,320 --> 00:09:17,163
to begin with,

214
00:09:18,150 --> 00:09:21,100
before an adversary has a chance
to take advantage of them.

215
00:09:23,470 --> 00:09:28,300
And the reason why this
is so big and so important

216
00:09:28,300 --> 00:09:31,000
is a lot of malicious
code or malicious activity

217
00:09:31,000 --> 00:09:32,710
often would never be successful,

218
00:09:32,710 --> 00:09:34,840
if there weren't the
accidents mistakes we make

219
00:09:34,840 --> 00:09:39,370
as a normal byproduct of building things.

220
00:09:39,370 --> 00:09:40,570
Accidents, mistakes happen,

221
00:09:40,570 --> 00:09:44,240
especially with the size,
scale, speed and complexity

222
00:09:44,240 --> 00:09:45,976
of modern microservice architecture

223
00:09:45,976 --> 00:09:47,360
(indistinct) a public cloud.

224
00:09:47,360 --> 00:09:49,960
It's just, we've never dealt
with the speed of complexity

225
00:09:49,960 --> 00:09:51,900
post-deployment that we have now

226
00:09:51,900 --> 00:09:54,689
and opportunity for
accidents, mistakes increases.

227
00:09:54,690 --> 00:09:56,260
What we're trying to do

228
00:09:56,260 --> 00:09:58,930
is proactively inject those
failures into the system,

229
00:09:58,930 --> 00:10:01,620
to determine whether
or not we are prepared

230
00:10:01,620 --> 00:10:02,610
and could detect those things

231
00:10:02,610 --> 00:10:05,457
before an adversary can
actually utilize them

232
00:10:05,457 --> 00:10:06,863
and take advantage of them.

233
00:10:09,610 --> 00:10:11,860
So through this process, really

234
00:10:11,860 --> 00:10:14,470
what the goals we're trying to achieve is

235
00:10:14,470 --> 00:10:16,760
we're trying to reduce
the amount of uncertainty

236
00:10:16,760 --> 00:10:19,510
in the assumptions we have
inherently in the system

237
00:10:19,510 --> 00:10:20,850
through building confidence;

238
00:10:20,850 --> 00:10:23,220
through good instrumentation and data.

239
00:10:23,220 --> 00:10:24,880
As engineers we don't
believe in two things;

240
00:10:24,880 --> 00:10:25,713
we don't believe in hope,

241
00:10:25,713 --> 00:10:27,250
or we don't believe in luck, right?

242
00:10:27,250 --> 00:10:28,600
Either it works or it doesn't.

243
00:10:28,600 --> 00:10:30,800
And we believe in
instrumentation and data.

244
00:10:30,800 --> 00:10:32,800
And we were trying to instrument

245
00:10:32,800 --> 00:10:35,339
and build empirical data

246
00:10:35,340 --> 00:10:37,183
that shows us whether or not

247
00:10:37,183 --> 00:10:39,187
a system works as it's supposed to.

248
00:10:39,187 --> 00:10:40,963
And that helps us build confidence.

249
00:10:43,340 --> 00:10:45,990
So here's some security
chaos, engineering use cases.

250
00:10:47,170 --> 00:10:49,439
There are more use cases documented

251
00:10:49,440 --> 00:10:52,270
in the "Security Chaos
Engineering" O'Reilly Report

252
00:10:52,270 --> 00:10:56,460
as well as in the "Chaos
Engineering" O'Reilly book.

253
00:10:56,460 --> 00:10:57,840
But a few use cases

254
00:10:57,840 --> 00:10:59,610
you can get started in using them

255
00:10:59,610 --> 00:11:04,140
chaos engineering for
security on incident response.

256
00:11:04,140 --> 00:11:06,610
Security control
validation is a great one.

257
00:11:06,610 --> 00:11:08,523
Also, I really like
security observability.

258
00:11:08,523 --> 00:11:10,950
It's a great way to understand

259
00:11:12,552 --> 00:11:14,550
how well you can understand

260
00:11:14,550 --> 00:11:16,199
what's happening inside of the system,

261
00:11:16,200 --> 00:11:17,740
especially the system security

262
00:11:17,740 --> 00:11:19,370
through the kind of
output you get from it,

263
00:11:19,370 --> 00:11:21,350
the log events, the alerts,

264
00:11:21,350 --> 00:11:24,210
and because we're proactive,
we're not worried about

265
00:11:24,210 --> 00:11:26,380
an incident or an outage,
and try to chase that down.

266
00:11:26,380 --> 00:11:27,570
We're able to kind of determine,

267
00:11:27,570 --> 00:11:29,720
wow this log data made no sense.

268
00:11:29,720 --> 00:11:31,450
This control is not saying
the right information

269
00:11:31,450 --> 00:11:34,810
for me to make a determinable
decision on what to do here.

270
00:11:34,810 --> 00:11:37,119
And lastly, compliance.

271
00:11:37,120 --> 00:11:38,900
Compliance is great

272
00:11:38,900 --> 00:11:41,949
every case engineering experiment

273
00:11:41,950 --> 00:11:44,920
whether it's security or
availability stability based

274
00:11:44,920 --> 00:11:46,360
has compliance value.

275
00:11:46,360 --> 00:11:47,230
Essentially you're proving

276
00:11:47,230 --> 00:11:50,510
whether the technology worked
the way you had it documented

277
00:11:50,510 --> 00:11:51,800
or the way you thought it did.

278
00:11:51,800 --> 00:11:55,099
And all that has auditable value.

279
00:11:55,100 --> 00:11:56,640
- There are a handful of companies

280
00:11:56,640 --> 00:11:59,390
that are implementing security
chaos engineering today.

281
00:11:59,390 --> 00:12:01,850
And we are already seeing
that number increase

282
00:12:01,850 --> 00:12:06,420
which is fantastic, and just
tremendously exciting to us.

283
00:12:06,420 --> 00:12:07,979
Cardinal Health, where I work

284
00:12:07,980 --> 00:12:08,860
was one of the first

285
00:12:08,860 --> 00:12:11,010
to really begin implementing
that discipline.

286
00:12:11,010 --> 00:12:14,030
And we started in the summer of last year.

287
00:12:14,030 --> 00:12:16,350
Of the use cases that
Aaron just described.

288
00:12:16,350 --> 00:12:17,930
The one that really drove our adoption

289
00:12:17,930 --> 00:12:20,213
was security control validation.

290
00:12:21,230 --> 00:12:22,630
At the end of the day,

291
00:12:22,630 --> 00:12:24,810
we like so many other companies

292
00:12:24,810 --> 00:12:27,560
put our faith in the idea that the tools,

293
00:12:27,560 --> 00:12:30,170
the technical designs
and technology standards

294
00:12:30,170 --> 00:12:33,150
that we have keep us secure.

295
00:12:33,150 --> 00:12:35,240
And it's not that we
don't trust our people

296
00:12:35,240 --> 00:12:37,420
but look at the data.

297
00:12:37,420 --> 00:12:38,516
So

298
00:12:38,516 --> 00:12:42,060
(indistinct) 2020 cost
of a data breach report.

299
00:12:42,060 --> 00:12:44,619
You know, a lot of people
regard that the key takeaway

300
00:12:44,620 --> 00:12:49,090
is that 52% of data breaches
are caused by malicious actors.

301
00:12:49,090 --> 00:12:51,530
And really these numbers have not changed

302
00:12:51,530 --> 00:12:52,713
in the last 10 years.

303
00:12:53,640 --> 00:12:56,590
But while that's what a lot
of people take away from this

304
00:12:56,590 --> 00:12:57,880
what I take away

305
00:12:57,880 --> 00:13:01,180
is that there's actually
still 48% of data breaches

306
00:13:01,180 --> 00:13:04,819
that are caused by mistakes and accidents.

307
00:13:04,820 --> 00:13:08,660
And to me, these are preventable failures

308
00:13:08,660 --> 00:13:10,467
and the ones that my team (indistinct).

309
00:13:11,510 --> 00:13:12,880
Our leadership at Cardinal health

310
00:13:12,880 --> 00:13:15,970
realized that we needed a
different approach to security.

311
00:13:15,970 --> 00:13:20,050
One that focused both on
building security controls

312
00:13:20,050 --> 00:13:22,420
but also validating that
the ones that we had

313
00:13:22,420 --> 00:13:25,550
stayed in place and
didn't degrade over time.

314
00:13:25,550 --> 00:13:29,069
And really we wanted a
way to get after that 48%

315
00:13:29,070 --> 00:13:31,003
which we believed was preventable.

316
00:13:32,190 --> 00:13:33,730
So last summer

317
00:13:33,730 --> 00:13:36,190
was when we really invested in a team

318
00:13:36,190 --> 00:13:37,910
to fix this.

319
00:13:37,910 --> 00:13:40,620
And to really start something new.

320
00:13:40,620 --> 00:13:41,810
And we first started...

321
00:13:41,810 --> 00:13:44,689
I came from a career in
software development,

322
00:13:44,690 --> 00:13:45,750
but we started to see

323
00:13:45,750 --> 00:13:48,010
what are some of the other key skill sets

324
00:13:48,010 --> 00:13:50,270
that we wanted to have on this team.

325
00:13:50,270 --> 00:13:51,230
And really the fact that

326
00:13:51,230 --> 00:13:53,710
we were building a multi-disciplinary team

327
00:13:53,710 --> 00:13:56,260
we viewed as key to our success.

328
00:13:56,260 --> 00:13:57,260
So on my team

329
00:13:57,260 --> 00:14:01,640
I have somebody who knows a
lot about networks security.

330
00:14:01,640 --> 00:14:04,439
I have somebody who knows texture,

331
00:14:04,440 --> 00:14:08,020
I have somebody who is a former
systems engineer themselves

332
00:14:08,020 --> 00:14:11,100
and then somebody from the
risk and the privacy space.

333
00:14:11,100 --> 00:14:13,440
But as I so delicately put it,

334
00:14:13,440 --> 00:14:14,900
one of the things that we wanna do

335
00:14:14,900 --> 00:14:17,130
is have people with good hypothesis

336
00:14:17,130 --> 00:14:19,810
on where some of the skeletons
in the closet are buried

337
00:14:19,810 --> 00:14:22,250
so that we can make good hypotheses

338
00:14:22,250 --> 00:14:23,500
on where to look first

339
00:14:23,500 --> 00:14:25,600
and how to address some of these concerns.

340
00:14:26,980 --> 00:14:27,870
Next,

341
00:14:27,870 --> 00:14:30,690
a lot of times we do build
a lot of our own validations

342
00:14:30,690 --> 00:14:32,390
using our own custom scripts

343
00:14:33,830 --> 00:14:36,290
and APIs from our platforms.

344
00:14:36,290 --> 00:14:37,939
But other times we do take a look

345
00:14:37,940 --> 00:14:39,686
and we see what else is out there,

346
00:14:39,686 --> 00:14:43,960
whether that's open source
solutions that already exist

347
00:14:43,960 --> 00:14:47,070
or commercial technologies
if that's more important.

348
00:14:47,070 --> 00:14:49,540
But ultimately at the end of the day,

349
00:14:49,540 --> 00:14:50,372
our goal

350
00:14:50,373 --> 00:14:54,230
is to identify these unknown
technical security gaps

351
00:14:54,230 --> 00:14:55,780
and partner with the organization

352
00:14:55,780 --> 00:14:57,310
to remediate them

353
00:14:57,310 --> 00:14:59,662
before a bad guy finds them for us.

354
00:15:00,700 --> 00:15:03,740
So how exactly do we do that?

355
00:15:03,740 --> 00:15:08,260
So as excited to just go nuts
and get access to every system

356
00:15:08,260 --> 00:15:11,290
and every project, and look
for technical security gaps

357
00:15:11,290 --> 00:15:13,680
we knew that we really
needed a disciplined

358
00:15:13,680 --> 00:15:16,800
and a repeatable process
for, to do with this.

359
00:15:16,800 --> 00:15:19,670
And we identified three key goals.

360
00:15:19,670 --> 00:15:21,439
So first was that

361
00:15:21,440 --> 00:15:23,360
our process had to identify

362
00:15:23,360 --> 00:15:25,960
indisputable critical security gaps

363
00:15:25,960 --> 00:15:29,810
that when the organization
considered the risk

364
00:15:29,810 --> 00:15:32,239
we agreed it was worth fixing.

365
00:15:32,240 --> 00:15:33,620
Otherwise you don't need somebody

366
00:15:33,620 --> 00:15:35,900
who's just planning out
a whole bunch of things

367
00:15:35,900 --> 00:15:39,069
but then ultimately doing
nothing with the data.

368
00:15:39,070 --> 00:15:40,470
So what we had to do is

369
00:15:40,470 --> 00:15:43,410
we knew we had to establish benchmarks,

370
00:15:43,410 --> 00:15:46,000
ones that were relevant to our company

371
00:15:46,000 --> 00:15:48,260
and relevant to our systems.

372
00:15:48,260 --> 00:15:51,080
They couldn't just be
theoretical best practices

373
00:15:51,080 --> 00:15:54,250
that people considered to
be too lofty to achieve.

374
00:15:54,250 --> 00:15:56,350
They need us to do things
that the organization

375
00:15:56,350 --> 00:15:57,733
agreed we had to fix.

376
00:15:58,940 --> 00:16:02,090
Second is that our process had
to be able to be big enough

377
00:16:02,090 --> 00:16:05,950
to see the big picture of
a technical security gap.

378
00:16:05,950 --> 00:16:09,010
Where as our previous
Frontline's engineers

379
00:16:09,010 --> 00:16:12,270
had either seen evidence
of gaps in the past

380
00:16:12,270 --> 00:16:14,030
or they had hypotheses.

381
00:16:14,030 --> 00:16:15,910
What they lacked was the detail

382
00:16:15,910 --> 00:16:19,040
to be able to describe where this was,

383
00:16:19,040 --> 00:16:20,980
what the risk to the organization was,

384
00:16:20,980 --> 00:16:23,130
and really drive it to remediation.

385
00:16:23,130 --> 00:16:26,670
And so we wanted to be the
answer to that challenge.

386
00:16:26,670 --> 00:16:28,560
And then finally we needed to make sure

387
00:16:28,560 --> 00:16:32,229
that any gap that we identified
and drove to completion

388
00:16:32,230 --> 00:16:35,283
wasn't just unknowingly
re-introduced in the future.

389
00:16:36,400 --> 00:16:38,020
So with these goals in mind

390
00:16:38,020 --> 00:16:39,410
we created a process called

391
00:16:39,410 --> 00:16:42,170
Continuous Verification and Validation.

392
00:16:42,170 --> 00:16:45,969
And simply put, we wanted
to, on a regular basis,

393
00:16:45,970 --> 00:16:48,050
continuously verify that our controls

394
00:16:48,050 --> 00:16:50,240
were where we believed that
they were supposed to be

395
00:16:50,240 --> 00:16:53,090
and validate that they
were implemented correctly.

396
00:16:53,090 --> 00:16:55,710
And this process to us
has five main steps.

397
00:16:55,710 --> 00:16:56,660
First is obviously

398
00:16:56,660 --> 00:17:00,260
you need to understand which
control you're validating

399
00:17:00,260 --> 00:17:01,760
and then you need to understand

400
00:17:01,760 --> 00:17:02,939
what are the benchmarks

401
00:17:02,940 --> 00:17:05,280
that we're going to assess this control by

402
00:17:06,140 --> 00:17:08,430
so largely to give authority

403
00:17:08,430 --> 00:17:10,619
to the benchmarks that we're using,

404
00:17:10,619 --> 00:17:12,949
we like to use the
patterns and the standards

405
00:17:12,950 --> 00:17:15,380
that are set forth by our
security architecture team

406
00:17:15,380 --> 00:17:17,440
and are approved by our CSO.

407
00:17:17,440 --> 00:17:18,980
But if those don't exist

408
00:17:18,980 --> 00:17:21,569
because it's a brand new
area that we're looking at

409
00:17:21,569 --> 00:17:22,819
that's where we will start

410
00:17:22,819 --> 00:17:24,780
to make some of our own recommendations,

411
00:17:24,780 --> 00:17:28,580
socialize those recommendations
with the relevant teams

412
00:17:28,580 --> 00:17:29,770
and get that buy-in.

413
00:17:29,770 --> 00:17:32,580
So that, again, like I was saying before

414
00:17:32,580 --> 00:17:35,490
we wanted to make sure that
if we identify something

415
00:17:35,490 --> 00:17:37,213
we agree that it has to be fixed.

416
00:17:38,270 --> 00:17:40,139
The next is where we
get to do the fun part.

417
00:17:40,140 --> 00:17:40,980
And this is really

418
00:17:40,980 --> 00:17:43,420
where security chaos engineering comes in.

419
00:17:43,420 --> 00:17:45,270
And that's where we build the automation

420
00:17:45,270 --> 00:17:47,530
to validate those standards.

421
00:17:47,530 --> 00:17:49,970
Again, we could be
writing our own scripts.

422
00:17:49,970 --> 00:17:51,590
We could be using technologies

423
00:17:51,590 --> 00:17:53,240
but we need to learn

424
00:17:53,240 --> 00:17:55,363
what our systems actually look like.

425
00:17:56,540 --> 00:17:59,129
Next we start to create dashboards

426
00:17:59,130 --> 00:18:03,470
to show the real picture of
that technical security gap.

427
00:18:03,470 --> 00:18:04,950
And this does two things.

428
00:18:04,950 --> 00:18:07,610
So one is, it gives us
that real time visibility

429
00:18:07,610 --> 00:18:11,240
into how we're doing with
our security posture.

430
00:18:11,240 --> 00:18:12,900
But it's also a really good tool

431
00:18:12,900 --> 00:18:14,320
when we talk to our leaders.

432
00:18:14,320 --> 00:18:16,090
And when we're talking to partner teams

433
00:18:16,090 --> 00:18:17,852
to help drive these remediations.

434
00:18:19,000 --> 00:18:20,750
And then finally,

435
00:18:20,750 --> 00:18:21,730
if we start to see

436
00:18:21,730 --> 00:18:24,330
that our adherence to
those benchmarks decreases

437
00:18:24,330 --> 00:18:26,800
that's where we create an
issue in our risk register.

438
00:18:26,800 --> 00:18:28,540
And we have a governance process

439
00:18:28,540 --> 00:18:30,463
to drive that through remediation.

440
00:18:31,530 --> 00:18:34,060
So that right there is
the Cardinal Health story.

441
00:18:34,060 --> 00:18:35,810
So Aaron, I'm gonna turn it over to you

442
00:18:35,810 --> 00:18:37,803
to talk about your work with ChaoSlingr.

443
00:18:39,789 --> 00:18:41,170
- Thank you Jamie.

444
00:18:41,170 --> 00:18:43,830
So about 4 1/2 years ago

445
00:18:43,830 --> 00:18:46,870
I started this journey
on UnitedHealth Group.

446
00:18:46,870 --> 00:18:49,300
I was the chief security
architects to the company.

447
00:18:49,300 --> 00:18:52,370
And so we ended up writing
this open source tool

448
00:18:52,370 --> 00:18:53,550
called ChaoSlingr.

449
00:18:53,550 --> 00:18:54,580
And I'm gonna talk about

450
00:18:54,580 --> 00:18:56,929
the primary example of ChaoSlingr.

451
00:18:56,930 --> 00:18:59,370
Primary so when we open sourced it

452
00:18:59,370 --> 00:19:02,270
so really we created this tool
to really be a methodology

453
00:19:02,270 --> 00:19:04,850
for verifying and validating

454
00:19:04,850 --> 00:19:08,290
that the security we're
building in the cloud,

455
00:19:08,290 --> 00:19:09,123
AWS at the time

456
00:19:09,123 --> 00:19:11,180
were undergoing the universe
cloud transformation.

457
00:19:11,180 --> 00:19:13,020
We're trying to determine
that, Hey, you know

458
00:19:13,020 --> 00:19:16,290
all these decisions we're
making were good decisions

459
00:19:16,290 --> 00:19:20,399
and they were functional and
effective decisions as well.

460
00:19:20,400 --> 00:19:22,550
And so in the process of open sourcing it,

461
00:19:22,550 --> 00:19:23,830
so we needed a good example,

462
00:19:23,830 --> 00:19:26,270
the rest of the world could understand

463
00:19:26,270 --> 00:19:27,910
from an experiment perspective.

464
00:19:27,910 --> 00:19:29,830
So the example that we opened sourced

465
00:19:29,830 --> 00:19:31,699
was something that we call ports layer,

466
00:19:31,700 --> 00:19:36,700
which was the injection of a misconfigured

467
00:19:36,960 --> 00:19:38,270
or unauthorized port chains.

468
00:19:38,270 --> 00:19:40,430
For some odd reason, it
still happens all the time

469
00:19:40,430 --> 00:19:42,850
in the cloud and the data
center will have you.

470
00:19:42,850 --> 00:19:44,010
So anyway, it was a good example,

471
00:19:44,010 --> 00:19:45,740
whether you're a software engineer

472
00:19:45,740 --> 00:19:48,630
whether you're a network
engineer, a firewall engineer,

473
00:19:48,630 --> 00:19:50,040
or it doesn't matter,

474
00:19:50,040 --> 00:19:51,070
everybody kind of understands

475
00:19:51,070 --> 00:19:52,460
what a firewall is supposed to do

476
00:19:52,460 --> 00:19:57,460
and sort of have a basic
understanding of network.

477
00:19:57,540 --> 00:19:59,590
Well, some of the reasons why

478
00:19:59,590 --> 00:20:02,030
an authorized or misconfigured
port change could happen

479
00:20:02,030 --> 00:20:03,710
is one obviously the port,

480
00:20:03,710 --> 00:20:05,590
the change could be applied incorrectly.

481
00:20:05,590 --> 00:20:08,820
Somebody could have made
the change out of band.

482
00:20:08,820 --> 00:20:11,669
Somebody could have filled
out a ticket incorrectly,

483
00:20:11,670 --> 00:20:13,410
lots of reasons why.

484
00:20:13,410 --> 00:20:16,440
Somebody could have been
misunderstood flow, for example.

485
00:20:16,440 --> 00:20:18,500
Lots of reasons why that happens.

486
00:20:18,500 --> 00:20:21,257
Anyway so our assumption was that

487
00:20:23,090 --> 00:20:25,010
if we've been solving
for this kind of problem

488
00:20:25,010 --> 00:20:27,280
for 20 years, right, 20 plus years.

489
00:20:27,280 --> 00:20:30,660
So our assumption was our firewalls

490
00:20:30,660 --> 00:20:33,160
needed to be able to
immediately detect and block

491
00:20:33,160 --> 00:20:34,100
this kind of issue.

492
00:20:34,100 --> 00:20:35,419
And it would be a non-issue.

493
00:20:35,420 --> 00:20:37,455
So we did was we wrote this ChaoSlingr

494
00:20:37,455 --> 00:20:38,530
we wrote this experiment

495
00:20:38,530 --> 00:20:41,910
and we started injecting that
in our AWS security groups.

496
00:20:41,910 --> 00:20:43,540
What we started finding out was

497
00:20:43,540 --> 00:20:45,420
that was not always the case.

498
00:20:45,420 --> 00:20:47,670
What we found out was
about 60% of the time

499
00:20:47,670 --> 00:20:49,640
our firewalls caught it and blocked it.

500
00:20:49,640 --> 00:20:51,880
But our expectation was a 100% of the time

501
00:20:51,880 --> 00:20:53,040
that would happen.

502
00:20:53,040 --> 00:20:53,920
The issue was,

503
00:20:53,920 --> 00:20:54,850
it was a drift issue

504
00:20:54,850 --> 00:20:57,840
between a commercial and a
non-commercial environments.

505
00:20:57,840 --> 00:20:59,290
So this was proactive

506
00:20:59,290 --> 00:21:00,530
remember we were able to fix that,

507
00:21:00,530 --> 00:21:01,980
it was a non-issue.

508
00:21:01,980 --> 00:21:03,430
So that was the first thing we learned.

509
00:21:03,430 --> 00:21:04,930
The second thing we learned was

510
00:21:04,930 --> 00:21:07,160
the cloud native
configuration management tool

511
00:21:07,160 --> 00:21:08,170
that we're using

512
00:21:08,170 --> 00:21:10,100
caught it and brought
the change every time.

513
00:21:10,100 --> 00:21:11,740
So something we were barely playing for,

514
00:21:11,740 --> 00:21:12,630
barely planning for.

515
00:21:12,630 --> 00:21:14,580
Kind of (indistinct) was catching it

516
00:21:14,580 --> 00:21:15,530
and blocking it every time.

517
00:21:15,530 --> 00:21:17,220
This is the second thing we learned.

518
00:21:17,220 --> 00:21:18,660
The third thing we learned was

519
00:21:18,660 --> 00:21:20,450
at the time we didn't really use a SIM.

520
00:21:20,450 --> 00:21:22,880
We used a sort of our own homegrown,

521
00:21:22,880 --> 00:21:26,250
sort of the security logging
and monitoring solution.

522
00:21:26,250 --> 00:21:27,880
And we were expecting...

523
00:21:28,850 --> 00:21:29,683
I had a little faith

524
00:21:29,683 --> 00:21:31,640
that the alert was
actually gonna be generated

525
00:21:31,640 --> 00:21:33,210
from the events.

526
00:21:33,210 --> 00:21:34,460
'Cause the log events were sent

527
00:21:34,460 --> 00:21:36,030
for the configuration management tool

528
00:21:36,030 --> 00:21:38,180
and the firewall tool that
you're correlated It alert.

529
00:21:38,180 --> 00:21:39,013
Right?

530
00:21:39,013 --> 00:21:40,040
Well, that actually
happened, so that was great.

531
00:21:40,040 --> 00:21:40,974
We built confidence

532
00:21:40,974 --> 00:21:42,570
that our homegrown solution

533
00:21:42,570 --> 00:21:44,129
was actually throwing the run alerts.

534
00:21:44,130 --> 00:21:45,760
So that was the third thing we learned.

535
00:21:45,760 --> 00:21:46,780
The fourth thing we learned

536
00:21:46,780 --> 00:21:49,530
is when the alert got to the
security operation center,

537
00:21:50,690 --> 00:21:53,552
the SOC analyst did not know what to do.

538
00:21:53,552 --> 00:21:54,840
They couldn't determine

539
00:21:54,840 --> 00:21:56,909
which AWS account structure it came from.

540
00:21:56,910 --> 00:21:59,440
Because we had a lot of account

541
00:21:59,440 --> 00:22:01,560
we had both non-commercial and commercial.

542
00:22:01,560 --> 00:22:04,389
And as an engineer, you can see

543
00:22:04,390 --> 00:22:05,610
I can map back that IP address

544
00:22:05,610 --> 00:22:07,189
to figure out where that came from.

545
00:22:07,190 --> 00:22:09,870
That's right, it could
take 15 minutes, 30 minutes

546
00:22:09,870 --> 00:22:12,687
up to three hours if
(indistinct) that is in place

547
00:22:12,687 --> 00:22:13,810
it could have take a lot longer

548
00:22:13,810 --> 00:22:16,639
because it hides IP address?

549
00:22:16,640 --> 00:22:19,640
Well, the point here is that

550
00:22:19,640 --> 00:22:21,850
had that actually been
an outage or an incident

551
00:22:21,850 --> 00:22:24,659
the UnitedHealth Group
loses millions of dollars

552
00:22:24,660 --> 00:22:25,560
when they're down.

553
00:22:27,725 --> 00:22:30,060
But there was no outage,
there was no incident.

554
00:22:30,060 --> 00:22:33,940
All we had to do was add
metadata to that event

555
00:22:33,940 --> 00:22:37,370
and we had fixed the problem.

556
00:22:37,370 --> 00:22:39,790
And that kind of illustrates

557
00:22:39,790 --> 00:22:40,623
sort of how we're able

558
00:22:40,623 --> 00:22:43,460
to test and instrument part of the chain.

559
00:22:43,460 --> 00:22:45,130
And then the idea is

560
00:22:45,130 --> 00:22:48,110
once you prove your case
experiments successful

561
00:22:48,110 --> 00:22:49,570
it becomes more of a regression test

562
00:22:49,570 --> 00:22:51,713
and you run it more
over time periodically.

563
00:22:54,204 --> 00:22:55,404
- Thanks Aaron for that.

564
00:22:56,290 --> 00:22:58,210
So if you're looking to get started

565
00:22:58,210 --> 00:22:59,910
with security chaos engineering,

566
00:22:59,910 --> 00:23:03,590
and you need more of a
foundation before you implement,

567
00:23:03,590 --> 00:23:04,530
the great news

568
00:23:04,530 --> 00:23:09,530
is that there's now an official
O'Reilly report on the topic

569
00:23:10,750 --> 00:23:11,990
and it's free.

570
00:23:11,990 --> 00:23:15,720
So it was written both by
Aaron and by Kelly Shortridge

571
00:23:15,720 --> 00:23:19,280
who is the VP of product
strategy at Capsule8.

572
00:23:19,280 --> 00:23:22,330
And it contains a whole bunch of stories

573
00:23:22,330 --> 00:23:24,600
for people who are doing
this in the real world.

574
00:23:24,600 --> 00:23:25,860
So for example

575
00:23:25,860 --> 00:23:28,370
if you've enjoyed the
conversation a little bit

576
00:23:28,370 --> 00:23:29,320
about Cardinal Health,

577
00:23:29,320 --> 00:23:30,700
and you wanna learn more,

578
00:23:30,700 --> 00:23:33,830
I go into much more detail on that report.

579
00:23:33,830 --> 00:23:36,220
And we have people from across the globe

580
00:23:36,220 --> 00:23:38,420
who have done this at
their companies as well,

581
00:23:38,420 --> 00:23:41,370
including Verica, where Aaron works,

582
00:23:41,370 --> 00:23:44,290
Google, Capital One and others.

583
00:23:44,290 --> 00:23:47,730
I really can't say enough
good things about this book

584
00:23:47,730 --> 00:23:52,200
because it does contain so
many just real world examples.

585
00:23:52,200 --> 00:23:55,023
So please, please,
please do check that out.

586
00:23:56,220 --> 00:23:59,490
Next, if you're thinking that
security chaos engineering

587
00:23:59,490 --> 00:24:02,760
is just too cutting edge
for your organization

588
00:24:02,760 --> 00:24:06,300
this is where I really encourage
you to adjust your mindset

589
00:24:06,300 --> 00:24:09,653
and think of this really
just like standard testing.

590
00:24:10,490 --> 00:24:11,530
As I said in the beginning

591
00:24:11,530 --> 00:24:13,930
my background is actually
in software development

592
00:24:13,930 --> 00:24:15,350
and what really attracted me

593
00:24:16,617 --> 00:24:18,740
to Security Chaos Engineering

594
00:24:18,740 --> 00:24:23,290
was that the parallels between
this and software testing

595
00:24:23,290 --> 00:24:25,870
were just too profound to ignore.

596
00:24:25,870 --> 00:24:27,959
It's just that instead of testing

597
00:24:27,960 --> 00:24:31,030
that your system meets the
functional requirements

598
00:24:31,030 --> 00:24:32,260
that were set forth

599
00:24:32,260 --> 00:24:35,320
you're testing that your system's meeting

600
00:24:35,320 --> 00:24:36,889
not only the security requirements

601
00:24:36,890 --> 00:24:39,800
but the resilience requirements as well.

602
00:24:39,800 --> 00:24:41,870
And you can do things

603
00:24:41,870 --> 00:24:46,080
like the use cases are
really super similar.

604
00:24:46,080 --> 00:24:49,520
So you can run this in
a non-prod environment.

605
00:24:49,520 --> 00:24:51,460
That's awaiting promotion to production

606
00:24:51,460 --> 00:24:55,450
to make sure that you haven't
done any unknown damage.

607
00:24:55,450 --> 00:24:58,180
You can even look at this

608
00:24:58,180 --> 00:25:00,350
like where my team wants to go

609
00:25:00,350 --> 00:25:01,929
which is taking a look at this

610
00:25:01,930 --> 00:25:04,010
like it's test driven development.

611
00:25:04,010 --> 00:25:05,940
So where we really wanna be

612
00:25:05,940 --> 00:25:07,530
is we wanna start partnering

613
00:25:07,530 --> 00:25:10,250
with our security
architecture team and others

614
00:25:10,250 --> 00:25:13,420
that as our company is
developing new patterns,

615
00:25:13,420 --> 00:25:14,710
and new standards

616
00:25:14,710 --> 00:25:16,680
we're writing our tests upfront.

617
00:25:16,680 --> 00:25:19,520
And as those new controls are built

618
00:25:19,520 --> 00:25:21,629
we see our tests start to pass

619
00:25:21,630 --> 00:25:25,210
to know that we are deploying
a new security control

620
00:25:25,210 --> 00:25:27,943
in a way that we think
is best for our company.

621
00:25:29,270 --> 00:25:31,879
So when you think about it that way,

622
00:25:31,880 --> 00:25:33,530
security chaos engineering

623
00:25:33,530 --> 00:25:37,180
really stops being just so esoteric

624
00:25:37,180 --> 00:25:39,870
and really becomes logical.

625
00:25:39,870 --> 00:25:42,580
As I see it, just as the
world has software engineering

626
00:25:42,580 --> 00:25:47,090
and systems engineering first
adopted testing methodologies,

627
00:25:47,090 --> 00:25:49,250
really the rest of the
systems engineering world

628
00:25:49,250 --> 00:25:50,693
is going to do the same.

629
00:25:51,550 --> 00:25:53,530
And the good news is that

630
00:25:53,530 --> 00:25:56,310
once you've gotten past
that mindset barrier

631
00:25:56,310 --> 00:25:58,070
and you're ready to experiment,

632
00:25:58,070 --> 00:26:01,110
it's possible to start very, very small.

633
00:26:01,110 --> 00:26:03,840
I think one of the myths of
security chaos engineering

634
00:26:03,840 --> 00:26:06,230
is that before you can do anything

635
00:26:06,230 --> 00:26:07,100
you have to have

636
00:26:07,100 --> 00:26:11,480
this massive systems, wide
experiment in production

637
00:26:11,480 --> 00:26:14,130
that takes your multiple weeks to build

638
00:26:14,130 --> 00:26:17,040
or requires VP approval and everything.

639
00:26:17,040 --> 00:26:19,830
But that doesn't have to be the case.

640
00:26:19,830 --> 00:26:21,429
So if you take a look at the example

641
00:26:21,430 --> 00:26:23,350
that Aaron had just shared,

642
00:26:23,350 --> 00:26:24,780
really what I see here

643
00:26:24,780 --> 00:26:28,629
it's the opportunity for multiple tests.

644
00:26:28,630 --> 00:26:31,630
So instead of doing all of this at once

645
00:26:31,630 --> 00:26:34,440
and recreating port Slinger
in your environment,

646
00:26:34,440 --> 00:26:35,730
you can start to do things

647
00:26:35,730 --> 00:26:38,820
like test maybe just the fact

648
00:26:38,820 --> 00:26:40,960
did you receive that log message

649
00:26:40,960 --> 00:26:43,880
when something was changed?

650
00:26:43,880 --> 00:26:45,679
You could even do that manually.

651
00:26:45,679 --> 00:26:48,900
You could do something in a
non-production environment

652
00:26:48,900 --> 00:26:51,100
or safely in a production one,

653
00:26:51,100 --> 00:26:54,959
and verify that you get a log message.

654
00:26:54,960 --> 00:26:57,580
Similarly, if a log message is recorded

655
00:26:57,580 --> 00:27:01,320
you could just test the
incident response portion.

656
00:27:01,320 --> 00:27:02,540
And what this allows you to do

657
00:27:02,540 --> 00:27:04,409
is you can start to really

658
00:27:04,410 --> 00:27:06,920
or you can start to get
a couple of your examples

659
00:27:06,920 --> 00:27:09,160
and then organically
grow the business case

660
00:27:09,160 --> 00:27:10,950
to do more concerted efforts,

661
00:27:10,950 --> 00:27:12,660
with security chaos engineering,

662
00:27:12,660 --> 00:27:14,493
and security control validation.

663
00:27:16,040 --> 00:27:19,030
So if you're wondering where to begin

664
00:27:19,030 --> 00:27:21,530
you may have a few high value,

665
00:27:21,530 --> 00:27:24,399
low effort targets in
mind, and that's awesome.

666
00:27:24,400 --> 00:27:26,010
And you can start there,

667
00:27:26,010 --> 00:27:28,590
like I said, I guarantee you'll be able

668
00:27:30,051 --> 00:27:32,560
to prove your value pretty quickly

669
00:27:32,560 --> 00:27:36,690
and organically grow that case
for a sustained investment.

670
00:27:36,690 --> 00:27:38,760
But if you're like me,

671
00:27:38,760 --> 00:27:39,710
maybe you have

672
00:27:39,710 --> 00:27:43,330
a lot of discrete high
value testing possibilities

673
00:27:43,330 --> 00:27:45,919
and it's hard to make sense of them all.

674
00:27:45,920 --> 00:27:47,020
In this case,

675
00:27:47,020 --> 00:27:49,550
to me it's okay for you to be what I call

676
00:27:49,550 --> 00:27:52,870
the right kind of lazy
and be a little selfish.

677
00:27:52,870 --> 00:27:56,199
So start to look at the
health of your team.

678
00:27:56,200 --> 00:27:58,210
Like, do you have engineers

679
00:27:58,210 --> 00:28:01,100
who log into your systems every day

680
00:28:01,100 --> 00:28:04,129
just to make sure that
services are up and running?

681
00:28:04,130 --> 00:28:08,053
Worse do they do this on the
weekends or in off hours?

682
00:28:09,330 --> 00:28:11,260
When you are that right, kind of lazy

683
00:28:11,260 --> 00:28:14,100
you start to put the
engineering effort upfront

684
00:28:14,100 --> 00:28:16,909
to be able to build
some simple validations

685
00:28:16,910 --> 00:28:18,540
and some simple tests

686
00:28:18,540 --> 00:28:22,450
that will push alerts to
you when something fails.

687
00:28:22,450 --> 00:28:24,110
And in this case,

688
00:28:24,110 --> 00:28:27,939
to me this is the beauty of
security chaos engineering.

689
00:28:27,940 --> 00:28:31,330
Because it allows you to
start building confidence

690
00:28:31,330 --> 00:28:33,210
that your security's working,

691
00:28:33,210 --> 00:28:34,853
even when you aren't.

692
00:28:36,630 --> 00:28:38,890
And so there you have it.

693
00:28:38,890 --> 00:28:41,570
As you can see security chaos engineering

694
00:28:41,570 --> 00:28:45,330
is a field that it seems
so esoteric and out there,

695
00:28:45,330 --> 00:28:46,949
but really at its core,

696
00:28:46,950 --> 00:28:49,540
it's just incredibly simple.

697
00:28:49,540 --> 00:28:53,090
Instead of allowing the
complexity of our systems

698
00:28:53,090 --> 00:28:54,280
to overwhelm us

699
00:28:54,280 --> 00:28:59,010
and cause so many opportunities
for mistakes and accidents,

700
00:28:59,010 --> 00:29:02,780
the solution is to really
flip that model on its head.

701
00:29:02,780 --> 00:29:05,800
Instead of relying on our
outdated documentation

702
00:29:05,800 --> 00:29:07,830
and our preconceived notions,

703
00:29:07,830 --> 00:29:12,370
and instead of relying on
our production incidents

704
00:29:12,370 --> 00:29:14,689
to teach us about our systems,

705
00:29:14,690 --> 00:29:17,660
we can proactively go in, eyes wide open,

706
00:29:17,660 --> 00:29:20,620
ready to experiment in a time of calm

707
00:29:20,620 --> 00:29:23,489
and actually learn about our systems.

708
00:29:23,490 --> 00:29:26,470
And when we do this, we
actually get the opportunity.

709
00:29:26,470 --> 00:29:28,980
And I do see it as an opportunity

710
00:29:28,980 --> 00:29:31,830
to re remember our
systems for what they are

711
00:29:31,830 --> 00:29:35,360
and not what we hope for them to be.

712
00:29:35,360 --> 00:29:38,330
And it's through testing
and experimentation

713
00:29:38,330 --> 00:29:41,929
that we get the opportunity
to identify ways

714
00:29:41,930 --> 00:29:45,540
for us to proactively secure
and stabilize our systems

715
00:29:45,540 --> 00:29:46,480
and get ourselves

716
00:29:46,480 --> 00:29:49,490
out of the reactionary
fire drill engineering

717
00:29:49,490 --> 00:29:50,873
practices of today.

718
00:29:52,410 --> 00:29:53,470
And there you have it.

719
00:29:53,470 --> 00:29:56,110
We hope that this session was valuable

720
00:29:56,110 --> 00:29:58,129
and that you learned something.

721
00:29:58,130 --> 00:30:01,770
Again, major, major thanks
to the organizers of RSA

722
00:30:01,770 --> 00:30:03,020
for putting this together,

723
00:30:03,020 --> 00:30:04,870
and for highlighting us and our work

724
00:30:04,870 --> 00:30:08,070
in both resilience and
security chaos engineering.

725
00:30:08,070 --> 00:30:09,860
We were incredibly excited

726
00:30:09,860 --> 00:30:14,033
to be a part of the 365 Virtual Series.

727
00:30:15,060 --> 00:30:17,649
And a few things to keep note;

728
00:30:17,650 --> 00:30:20,612
one, if you are looking to learn more

729
00:30:20,612 --> 00:30:23,050
there is a link at the
bottom of this slide,

730
00:30:23,050 --> 00:30:25,760
that's not only going
to get you a free copy

731
00:30:25,760 --> 00:30:29,060
of the "Security Chaos
Engineering" report,

732
00:30:29,060 --> 00:30:30,679
but it's also going to get you a copy

733
00:30:30,680 --> 00:30:35,460
of the official O'Reilly book
on chaos engineering as well.

734
00:30:35,460 --> 00:30:38,230
So both books are incredibly awesome.

735
00:30:38,230 --> 00:30:39,600
Make sure to check those out

736
00:30:39,600 --> 00:30:41,663
and again they are absolutely free.

737
00:30:42,520 --> 00:30:45,910
Next, we are going to be
continuing this conversation

738
00:30:45,910 --> 00:30:48,560
in a live Q&A session,

739
00:30:48,560 --> 00:30:50,850
if you're joining us here today.

740
00:30:50,850 --> 00:30:53,050
So we are so excited

741
00:30:53,050 --> 00:30:55,760
to be able to go a little
bit more behind the scenes,

742
00:30:55,760 --> 00:30:57,390
talk about our stories

743
00:30:57,390 --> 00:30:59,770
and answer whatever questions you have.

744
00:30:59,770 --> 00:31:02,450
And of course, we've provided
our contact information,

745
00:31:02,450 --> 00:31:05,340
if you'd like to continue
the conversation there.

746
00:31:05,340 --> 00:31:06,783
Thank you for everything.

