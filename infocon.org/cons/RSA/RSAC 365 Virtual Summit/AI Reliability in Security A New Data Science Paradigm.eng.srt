1
00:00:00,090 --> 00:00:02,989
- Hi, my name is Celeste Fralick.

2
00:00:02,990 --> 00:00:04,490
I'm the Chief Data Scientist

3
00:00:04,490 --> 00:00:08,093
and Senior Principal Engineer of McAfee.

4
00:00:09,780 --> 00:00:13,430
Today, as a summary of the
agenda you see on the slide,

5
00:00:13,430 --> 00:00:18,430
I hope the audience will walk
away with old and new ideas

6
00:00:18,510 --> 00:00:22,580
about AI reliability and
begin to put into place

7
00:00:22,580 --> 00:00:26,849
some key monitors in DevOps and ML Ops.

8
00:00:26,850 --> 00:00:30,160
These monitors are the baseline,

9
00:00:30,160 --> 00:00:32,450
the foundation of AI reliability,

10
00:00:32,450 --> 00:00:35,850
and I will give you some
proposed mathematical models

11
00:00:35,850 --> 00:00:40,555
for AI reliability that will
end up with the population

12
00:00:40,555 --> 00:00:43,920
of an important AI reliability term,

13
00:00:43,920 --> 00:00:48,563
I want to introduce today
called mean time to decay, MTTD,

14
00:00:49,750 --> 00:00:54,533
so let's walk you
through how to get there.

15
00:00:55,650 --> 00:00:58,390
First of all, to start at the
beginning, there are three

16
00:00:58,390 --> 00:01:03,390
words that have become very
prevalent in speaking about AI.

17
00:01:05,239 --> 00:01:08,320
The first one is robustness
and has got a whole bunch

18
00:01:08,320 --> 00:01:12,536
of cinnamon, synonyms, sorry (chuckles),

19
00:01:13,510 --> 00:01:15,580
I must be thinking about breakfast,

20
00:01:15,580 --> 00:01:17,990
hardiness, lustiness, strength, soundness,

21
00:01:17,990 --> 00:01:19,339
health, toughness.

22
00:01:19,340 --> 00:01:21,950
Typically, it has referred to the ability

23
00:01:21,950 --> 00:01:26,130
of a computer system to cope with errors

24
00:01:26,130 --> 00:01:29,110
during execution and erroneous input.

25
00:01:29,110 --> 00:01:32,220
We can extend that concept to AI,

26
00:01:32,220 --> 00:01:36,039
how effective is your
algorithm while being tested

27
00:01:36,040 --> 00:01:40,570
on a new independent,
but similar data set?

28
00:01:40,570 --> 00:01:44,970
This property is also known
as algorithmic stability,

29
00:01:44,970 --> 00:01:46,810
and in fact, in many cases,

30
00:01:46,810 --> 00:01:51,620
we minimize the error rates
such as mean squared error

31
00:01:51,620 --> 00:01:54,440
and maximize peak signal to noise ratio

32
00:01:54,440 --> 00:01:57,633
to ensure a stable algorithm,
so that's robustness.

33
00:01:59,310 --> 00:02:03,422
The second word is resilience.

34
00:02:03,422 --> 00:02:07,860
Resilience means in its
inputs, ebb and flow,

35
00:02:07,860 --> 00:02:11,020
does the model still perform the same way?

36
00:02:11,020 --> 00:02:14,200
It also implies toughness,

37
00:02:14,200 --> 00:02:16,649
I like to think I'm very resilient,

38
00:02:16,650 --> 00:02:19,880
and the ability to adapt to risks

39
00:02:19,880 --> 00:02:24,170
and continued operation
at core business functions

40
00:02:24,170 --> 00:02:26,503
despite challenges.

41
00:02:26,503 --> 00:02:28,630
The last term is trustworthiness,

42
00:02:28,630 --> 00:02:30,490
and trustworthiness has been used

43
00:02:30,490 --> 00:02:33,560
by the Boy Scouts for many years,

44
00:02:33,560 --> 00:02:37,550
and now it is raised by
NIST the National Institute

45
00:02:37,550 --> 00:02:40,280
of Standards and Technology
in the United States,

46
00:02:40,280 --> 00:02:43,183
and also in their new AI documents,

47
00:02:44,250 --> 00:02:47,080
but these three words
are critical to describe

48
00:02:48,180 --> 00:02:50,260
what we expect out of AI,

49
00:02:50,260 --> 00:02:54,940
but we can use it to form
the basis of AI reliability.

50
00:02:54,940 --> 00:02:59,750
AI reliability is the intended
performance over time.

51
00:02:59,750 --> 00:03:04,190
Performance can be accuracy,
it can be false positives,

52
00:03:04,190 --> 00:03:08,133
AUC or whatever your business need,

53
00:03:09,550 --> 00:03:14,340
states for what's important for you,

54
00:03:14,340 --> 00:03:16,590
for the business, for the customer.

55
00:03:16,590 --> 00:03:21,590
Our three common words combine
to support AI reliability

56
00:03:24,670 --> 00:03:28,730
and they ensure
reliability can be measured

57
00:03:28,730 --> 00:03:31,690
via the monitors each word represents,

58
00:03:31,690 --> 00:03:35,910
and AI reliability can answer the question

59
00:03:35,910 --> 00:03:40,910
how stable is your monitor or
how stable is your algorithm?

60
00:03:42,440 --> 00:03:47,440
Can your model, does it have
the ability to adapt to risks?

61
00:03:49,716 --> 00:03:52,210
And can you adopt AI without fear?

62
00:03:52,210 --> 00:03:57,210
And that's where bias and AML,

63
00:03:57,210 --> 00:04:01,600
adversarial machine learning,
comes into play and ethics.

64
00:04:01,600 --> 00:04:06,600
So these three words combine
to form AI reliability

65
00:04:12,120 --> 00:04:15,153
and it begs the question,
most importantly,

66
00:04:17,130 --> 00:04:20,332
how long will my model last in the field?

67
00:04:23,560 --> 00:04:25,890
Additional reasons for AI are numerous,

68
00:04:25,890 --> 00:04:29,260
that I'm listing here, just as your car

69
00:04:29,260 --> 00:04:32,039
or your airplane's
reliability provides you

70
00:04:32,040 --> 00:04:36,510
a sense of security and
certain expectations,

71
00:04:36,510 --> 00:04:40,500
reliability can be critical
to us as customers,

72
00:04:40,500 --> 00:04:44,320
but the company CEO is
concerned about growth,

73
00:04:44,320 --> 00:04:47,240
he's concerned about competition,

74
00:04:47,240 --> 00:04:51,520
and he's concerned about
customer satisfaction.

75
00:04:51,520 --> 00:04:56,520
Well, suffice it to say that
as a revitalized technology,

76
00:04:58,030 --> 00:05:02,940
from the discovery of AI
by John McCarthy in 1956,

77
00:05:02,940 --> 00:05:06,400
and one that currently manages most facets

78
00:05:06,400 --> 00:05:09,010
of our digital life, it's timely.

79
00:05:09,010 --> 00:05:14,010
No, I'd say it's probably
required right now to assess AI,

80
00:05:15,874 --> 00:05:17,343
AI's reliability.

81
00:05:18,610 --> 00:05:20,750
Now, AI reliability,

82
00:05:20,750 --> 00:05:23,680
mathematics safety's been
around for a long time,

83
00:05:23,680 --> 00:05:26,520
they're applied to
everything from our cars,

84
00:05:26,520 --> 00:05:29,539
our refrigerators, our computer chips

85
00:05:29,540 --> 00:05:31,863
and phones to the very clothes we wear.

86
00:05:33,100 --> 00:05:35,510
It is the manufacturer's
prediction of how long

87
00:05:35,510 --> 00:05:39,610
the product will perform
its intended function,

88
00:05:39,610 --> 00:05:43,540
it's the best by date of your milk,

89
00:05:43,540 --> 00:05:45,900
to the recommended cleaning frequency

90
00:05:45,900 --> 00:05:47,960
of your teeth by your dentist.

91
00:05:47,960 --> 00:05:52,960
It's graphically typically
shown as a bathtub curve.

92
00:05:53,490 --> 00:05:56,504
It starts with infant mortality,

93
00:05:56,504 --> 00:06:00,783
with a decreasing failure rate
and this is known as quality,

94
00:06:01,630 --> 00:06:06,230
this is the great car that you
bought and works just great.

95
00:06:06,230 --> 00:06:09,410
Then there's a happy place
known as a normal life or useful

96
00:06:09,410 --> 00:06:13,670
life where you have a low
or constant failure rate.

97
00:06:13,670 --> 00:06:15,633
And then, pretty soon,

98
00:06:17,050 --> 00:06:21,470
you have increasing calls
to the car repairman

99
00:06:21,470 --> 00:06:24,700
and you have an end of life wear-out

100
00:06:24,700 --> 00:06:27,700
with an increasing failure rate,

101
00:06:27,700 --> 00:06:31,680
and your milk starts to
sour and things like that.

102
00:06:31,680 --> 00:06:35,330
The general definition
of reliability embraces

103
00:06:35,330 --> 00:06:38,911
mathematical models that
build this bathtub curve

104
00:06:38,911 --> 00:06:41,373
over years and years.

105
00:06:43,670 --> 00:06:46,550
Let's take now, that's
normally for hardware,

106
00:06:46,550 --> 00:06:49,180
but let's take a look at software.

107
00:06:49,180 --> 00:06:54,180
Now, software reliability
has also evolved over years,

108
00:06:55,810 --> 00:07:00,810
and let me quote, because I
think it's an incredible quote,

109
00:07:01,590 --> 00:07:05,969
it says, "It's an estimate
of the level of business risk

110
00:07:05,970 --> 00:07:09,490
and the likelihood of
potential application failures

111
00:07:09,490 --> 00:07:12,505
and defects the
application will experience

112
00:07:12,505 --> 00:07:14,980
when placed in operation."

113
00:07:14,980 --> 00:07:18,513
I think that's a great definition
of software reliability.

114
00:07:19,650 --> 00:07:24,650
We're very familiar with the
fact that it can utilize errors

115
00:07:25,880 --> 00:07:29,210
detected at each stage of development,

116
00:07:29,210 --> 00:07:32,930
it's even using neural
networks and other AI tools

117
00:07:32,930 --> 00:07:35,720
to assess and predict failures.

118
00:07:35,720 --> 00:07:40,720
Software reliability still
addresses those core items

119
00:07:40,850 --> 00:07:44,670
such as functional and
structural quality, typically,

120
00:07:44,670 --> 00:07:49,410
in the SQA organization,
as well as the old standby

121
00:07:49,410 --> 00:07:52,790
that I mentioned that error
is detected at each stage.

122
00:07:52,790 --> 00:07:55,850
They also use combinatorial test

123
00:07:55,850 --> 00:07:58,850
designed to identify the
optimum number of tests

124
00:07:58,850 --> 00:08:01,360
needed for the coverage they desire.

125
00:08:01,360 --> 00:08:04,090
Well, Microsoft has done a great job

126
00:08:04,090 --> 00:08:09,080
at creating some reliability measurements,

127
00:08:09,080 --> 00:08:12,849
such as the Office Customer
Experience Improvement Program

128
00:08:12,850 --> 00:08:17,850
or CEIP and Microsoft Reliability
Analysis Service, M-R-A-S.

129
00:08:21,450 --> 00:08:24,060
You can find details of these acronyms

130
00:08:24,060 --> 00:08:29,060
and other Microsoft programs
within this paper here.

131
00:08:30,330 --> 00:08:32,780
Now, I'm sure you're all familiar

132
00:08:32,780 --> 00:08:37,500
with the CISQ's Software
Quality Model on Reliability,

133
00:08:37,500 --> 00:08:38,970
but what I wanna point out

134
00:08:38,970 --> 00:08:42,289
and what I've highlighted in yellow there,

135
00:08:42,289 --> 00:08:46,610
is the fact that they
also call out those three

136
00:08:46,610 --> 00:08:50,700
important words, the resiliency,
stability or robustness,

137
00:08:50,700 --> 00:08:53,060
and all important monitoring,

138
00:08:53,060 --> 00:08:56,859
and monitoring is so important

139
00:08:56,860 --> 00:09:01,860
to the critical concept of reliability.

140
00:09:01,900 --> 00:09:06,900
However, both software and
general reliability equations of

141
00:09:07,370 --> 00:09:12,370
the bathtub curve have yet to
address the reliability of AI.

142
00:09:15,284 --> 00:09:18,480
And as I don't think AI reliability

143
00:09:18,480 --> 00:09:21,810
fits into either one of
them, but we can certainly

144
00:09:21,810 --> 00:09:23,959
utilize some of their calculations,

145
00:09:23,960 --> 00:09:27,070
and so a new paradigm
needs to be considered

146
00:09:27,070 --> 00:09:30,713
so we can predict the life
of the model in the field.

147
00:09:32,690 --> 00:09:37,080
So consider then these
questions about reliability.

148
00:09:37,080 --> 00:09:40,173
How do you predict
reliability of an AI model?

149
00:09:41,110 --> 00:09:42,620
How do you utilize robustness

150
00:09:42,620 --> 00:09:44,583
resilience and trustworthiness?

151
00:09:45,480 --> 00:09:48,450
What are the factors or
features that drive reliability

152
00:09:48,450 --> 00:09:50,930
and how do they change?

153
00:09:50,930 --> 00:09:54,803
And how does AI reliability
change over time?

154
00:09:57,000 --> 00:09:59,930
To give some guidance to
answering these questions,

155
00:09:59,930 --> 00:10:02,969
we must look at the
entirety of AI development

156
00:10:02,970 --> 00:10:05,613
and deployment in the
field from soup to nuts.

157
00:10:06,680 --> 00:10:10,810
Most developers are anxious to create

158
00:10:12,360 --> 00:10:15,200
the latest exciting model, I know I am,

159
00:10:15,200 --> 00:10:20,200
and as developers, or DevOps,
or development operations,

160
00:10:20,230 --> 00:10:23,470
has been a mainstay of developing code,

161
00:10:23,470 --> 00:10:26,530
as well as instant models
for years and years,

162
00:10:26,530 --> 00:10:30,810
but DevOps is necessary,
but it's not sufficient.

163
00:10:30,810 --> 00:10:33,290
AI is more than just software code,

164
00:10:33,290 --> 00:10:38,280
it is based on curated data
that requires targeted actions

165
00:10:38,280 --> 00:10:43,189
built-in from the beginning of
and during model development,

166
00:10:43,190 --> 00:10:46,820
but once, also monitored,

167
00:10:46,820 --> 00:10:50,660
you also have to consider
the deployment of the model

168
00:10:50,660 --> 00:10:52,622
in the field with your customers.

169
00:10:56,010 --> 00:10:58,510
So implementing monitors
at the beginning of,

170
00:10:58,510 --> 00:11:00,860
during and while on the field constitute

171
00:11:00,860 --> 00:11:04,710
the typical organizational
structure of DevOps and ML Ops.

172
00:11:04,710 --> 00:11:08,860
And while the picture here on
the right is just an example,

173
00:11:08,860 --> 00:11:13,020
yours may be more or less
delineated or skewed,

174
00:11:13,020 --> 00:11:16,210
most likely DevOps, most are.

175
00:11:16,210 --> 00:11:20,940
But what we need to do is
monitor designed-in checks

176
00:11:20,940 --> 00:11:24,160
and balances as well as field performance.

177
00:11:24,160 --> 00:11:27,880
And now AI is moving
from development-centric

178
00:11:27,880 --> 00:11:31,013
to consumption-centric activities.

179
00:11:32,809 --> 00:11:36,790
And we'll find that just as in DevOps,

180
00:11:36,790 --> 00:11:41,790
that with ML Ops, there is a 3
to 15% profit margin increase

181
00:11:42,767 --> 00:11:46,900
with implementing ML Ops,
which includes model monitoring

182
00:11:46,900 --> 00:11:50,329
there in the lower right.

183
00:11:50,330 --> 00:11:55,330
In fact, ML Ops market,
they believe will be greater

184
00:11:56,100 --> 00:12:01,100
than $4 billion by 2025,
which is only four years away.

185
00:12:05,482 --> 00:12:09,797
So it's important to consider
that you look at both DevOps

186
00:12:11,630 --> 00:12:15,750
and ML Ops as well as field performance.

187
00:12:15,750 --> 00:12:18,390
Now, to review what we've discussed,

188
00:12:18,390 --> 00:12:22,280
consider that robustness,
resilience and trustworthiness

189
00:12:22,280 --> 00:12:26,209
provide input into monitors
that can be implemented

190
00:12:26,210 --> 00:12:29,810
in DevOps, ML Ops or in the field.

191
00:12:29,810 --> 00:12:32,699
We ensure that there is
always a feedback loop,

192
00:12:32,700 --> 00:12:34,483
as you see at the top there,

193
00:12:35,880 --> 00:12:40,880
and we wanna make sure that
that feedback loop goes from

194
00:12:41,080 --> 00:12:45,500
into DevOps and ML Ops to place alerts,

195
00:12:45,500 --> 00:12:49,760
revisions, thresholds
or root-cause analysis.

196
00:12:49,760 --> 00:12:53,110
I've referred to the term monitors

197
00:12:53,110 --> 00:12:55,560
throughout the talk thus far,

198
00:12:55,560 --> 00:12:58,512
so let's take a look at
some of those examples.

199
00:12:59,520 --> 00:13:04,520
Now I don't, let's see, I
wanna make sure you can...

200
00:13:08,360 --> 00:13:10,423
There we go, all right.

201
00:13:13,230 --> 00:13:17,520
These next two slides, I know
are maybe a lot to take in,

202
00:13:21,257 --> 00:13:25,489
but what I want to do is
just consider suggestions

203
00:13:26,750 --> 00:13:30,740
on monitors and what I've
done in these next few slides

204
00:13:30,740 --> 00:13:35,660
is just giving you an idea
of some of those monitors

205
00:13:35,660 --> 00:13:39,742
that can be placed in
ML Ops, both, or DevOps.

206
00:13:41,260 --> 00:13:42,110
Okay?

207
00:13:42,110 --> 00:13:46,470
And you should all be
familiar with some of these,

208
00:13:46,470 --> 00:13:50,310
but what I wanna do is make sure that

209
00:13:51,610 --> 00:13:54,863
you look at these from a
prioritized standpoint,

210
00:13:56,190 --> 00:13:59,580
things such as low-hanging fruit,

211
00:13:59,580 --> 00:14:03,350
like skewness, kurtosis,
variance for concept draft

212
00:14:03,350 --> 00:14:06,470
or data decay, volumes and types,

213
00:14:06,470 --> 00:14:11,310
looking at down-sampling
your storage, your caches,

214
00:14:11,310 --> 00:14:14,439
looking at your error rates or ROC curves

215
00:14:14,440 --> 00:14:16,413
over a set of of time.

216
00:14:20,396 --> 00:14:24,327
So these in itself, can
be a good place to start.

217
00:14:25,600 --> 00:14:29,210
The second page is additional monitors

218
00:14:29,210 --> 00:14:30,180
that you might consider,

219
00:14:30,180 --> 00:14:33,103
adversarial machine
learning or model hacking,

220
00:14:34,490 --> 00:14:37,720
where poisoning or evasion can drive up

221
00:14:37,720 --> 00:14:40,453
your false positives or false negatives.

222
00:14:41,300 --> 00:14:45,703
Your bias, certainly,
every one of us have it,

223
00:14:46,710 --> 00:14:51,620
we want to make sure that
we also consider within

224
00:14:51,620 --> 00:14:54,701
not just sampling measurement algorithmic,

225
00:14:54,701 --> 00:14:57,173
but also prejudicial.

226
00:14:58,500 --> 00:15:02,190
In prejudicial, remember,
we have societal changes

227
00:15:03,240 --> 00:15:07,590
such as racial unrest in the US,

228
00:15:07,590 --> 00:15:11,943
so your bias may become
much more sensitive

229
00:15:13,450 --> 00:15:14,820
over a period of time.

230
00:15:14,820 --> 00:15:17,220
How does your AI model react to that?

231
00:15:17,220 --> 00:15:20,020
So you have to keep in mind some

232
00:15:20,020 --> 00:15:23,173
of your societal concerns as well.

233
00:15:26,510 --> 00:15:29,560
And certainly, explainability,

234
00:15:29,560 --> 00:15:33,239
where you can explain
explainability-by-design

235
00:15:33,240 --> 00:15:38,193
with some of the LIME,
Grad-CAM and SHAP monitors.

236
00:15:41,735 --> 00:15:46,240
And interpretability,
which is something that

237
00:15:46,240 --> 00:15:50,410
is not very common yet, but
it implies that you have

238
00:15:50,410 --> 00:15:55,410
UI and UX principles first,
user interface principles first,

239
00:15:56,940 --> 00:15:59,840
and it enables people to
be part of the decision,

240
00:15:59,840 --> 00:16:04,490
making an AI explainability
helps that quite a bit as well.

241
00:16:04,490 --> 00:16:07,050
Certainly, anomalies, and for this crowd,

242
00:16:07,050 --> 00:16:10,349
cybersecurity obviously is very important,

243
00:16:10,350 --> 00:16:13,170
with the number and type of malware,

244
00:16:13,170 --> 00:16:17,770
including the exclusive detections

245
00:16:19,860 --> 00:16:24,860
as well as the falses
and the threat families,

246
00:16:24,990 --> 00:16:27,690
have the threat families
changed over time?

247
00:16:27,690 --> 00:16:32,690
So after you've completed
looking at these monitors

248
00:16:33,720 --> 00:16:36,010
and you selected them,
you've baseline them,

249
00:16:36,010 --> 00:16:37,373
and you've measured them,

250
00:16:38,320 --> 00:16:43,320
now you have a model that
has an expected lifetime,

251
00:16:46,200 --> 00:16:50,440
noted by the red dot there,
until it starts decay,

252
00:16:50,440 --> 00:16:52,773
noted here as the solid black line.

253
00:16:54,000 --> 00:16:56,430
When retraining occurs
because of the decay,

254
00:16:56,430 --> 00:17:01,430
the model is refreshed, as
noted from the red arrow,

255
00:17:01,890 --> 00:17:03,830
and measured by the business goal,

256
00:17:03,830 --> 00:17:07,339
and in this graph, I've selected accuracy,

257
00:17:07,339 --> 00:17:10,062
seen here on the Y axis,
yours may be different.

258
00:17:13,740 --> 00:17:18,400
And the decay causes are highly dependent,

259
00:17:18,400 --> 00:17:21,930
the decay causes there in the rectangles,

260
00:17:21,930 --> 00:17:24,233
blue, green, pink, and purplish,

261
00:17:25,627 --> 00:17:29,159
they're highly dependent
on what monitors you have

262
00:17:29,160 --> 00:17:33,140
put in place such as those
listed in the last two slides.

263
00:17:33,140 --> 00:17:38,140
So they may be single, they
may be cumulative causes,

264
00:17:38,250 --> 00:17:41,770
they may have different
uses of units of measure,

265
00:17:41,770 --> 00:17:44,933
so you need to consider each one of those.

266
00:17:44,933 --> 00:17:48,270
Now, I know you think
that that's probably it,

267
00:17:48,270 --> 00:17:50,690
you can go off and do your calculations,

268
00:17:50,690 --> 00:17:52,743
but of course, we have to have some math.

269
00:17:54,730 --> 00:17:58,123
First of all, we want to consider the

270
00:18:01,240 --> 00:18:04,900
calculate the instantaneous
decay rate of our model,

271
00:18:04,900 --> 00:18:07,290
we want to consider the
conditional probability

272
00:18:07,290 --> 00:18:12,290
that B will occur given A
is known to have occurred.

273
00:18:12,520 --> 00:18:15,410
Now that's just for two
causes, but of course,

274
00:18:15,410 --> 00:18:19,010
you may have, pardon
me, you may have perhaps

275
00:18:19,010 --> 00:18:21,379
a couple of dozen causes,
we don't know that,

276
00:18:21,380 --> 00:18:23,970
it's based on what you've monitored.

277
00:18:23,970 --> 00:18:27,320
Your probability calculations will

278
00:18:27,320 --> 00:18:29,342
then be converted to a rate,

279
00:18:31,030 --> 00:18:36,030
and after you place them into a rate,

280
00:18:39,640 --> 00:18:44,300
you can let the change
of time approach to zero

281
00:18:44,300 --> 00:18:49,300
and obtain the derivative of M of T,

282
00:18:49,770 --> 00:18:54,570
denoted by M prime of T divided
by R of T in equation three,

283
00:18:54,570 --> 00:18:57,800
and lastly, you'll get an
instantaneous failure rate

284
00:18:57,800 --> 00:18:59,530
of H sub T.

285
00:18:59,530 --> 00:19:04,530
Now, however, it may be more
useful for you to consider

286
00:19:04,920 --> 00:19:09,460
a single average number or
even a cumulative decay rate

287
00:19:09,460 --> 00:19:11,540
of the original model.

288
00:19:11,540 --> 00:19:16,389
So here I provided that

289
00:19:18,070 --> 00:19:23,070
by integrating the
instantaneous failure rate

290
00:19:23,740 --> 00:19:26,900
over the interval and dividing
by the time difference.

291
00:19:26,900 --> 00:19:29,090
And the time difference may be different,

292
00:19:29,090 --> 00:19:31,610
might be weekly, might be
daily, might be monthly,

293
00:19:31,610 --> 00:19:36,610
might be time zero, might
be when you go live,

294
00:19:36,850 --> 00:19:41,179
time 1 might be, you know, 60 days,

295
00:19:41,180 --> 00:19:44,710
time 2 might be, you know, 62 days,

296
00:19:44,710 --> 00:19:49,710
time 3 might be, you know, 80 days,

297
00:19:49,780 --> 00:19:51,973
it really, really depends.

298
00:19:52,870 --> 00:19:55,112
So at this point,

299
00:19:56,700 --> 00:20:01,700
when you're calculating
your average decay rate,

300
00:20:01,980 --> 00:20:05,350
this is all based on
the first four equations

301
00:20:07,110 --> 00:20:10,459
that I showed you, so by
integrating the instantaneous

302
00:20:10,460 --> 00:20:13,730
failure rate over the interval

303
00:20:13,730 --> 00:20:17,360
and dividing the time difference,
you can achieve an ADR,

304
00:20:17,360 --> 00:20:19,000
or average decay rate.

305
00:20:19,000 --> 00:20:21,843
Now in equation six,

306
00:20:27,466 --> 00:20:30,870
sorry, just checking my notes here,

307
00:20:30,870 --> 00:20:35,592
as the monitor changes are measured,

308
00:20:36,470 --> 00:20:40,010
you may find that cumulative
decay rate is required,

309
00:20:40,010 --> 00:20:42,440
and I like this quite a bit.

310
00:20:42,440 --> 00:20:46,480
It's a small fraction of a
change in a few monitors may be

311
00:20:46,480 --> 00:20:51,060
enough to alert that the
model has decayed perilously.

312
00:20:51,060 --> 00:20:55,389
This is well documented in
the 1986 Challenger explosion

313
00:20:55,390 --> 00:20:59,230
disaster, where an O-ring
caused a fatal error

314
00:21:00,074 --> 00:21:03,060
due to cumulative failures.

315
00:21:03,060 --> 00:21:06,320
Unfortunately, there were eight O-rings,

316
00:21:06,320 --> 00:21:10,770
and all had to function at 0.99 or 99%.

317
00:21:10,770 --> 00:21:12,629
The system reliability, therefore,

318
00:21:12,630 --> 00:21:15,203
was 0.99 to the eighth power,

319
00:21:16,080 --> 00:21:20,760
or 0.89, a potential failure rate of 0.11,

320
00:21:21,700 --> 00:21:24,540
or about one out of nine instances,

321
00:21:24,540 --> 00:21:29,360
and the explosion occurred
in the 12th shuttle launch,

322
00:21:29,360 --> 00:21:33,572
so that cumulative decay
rate is very important.

323
00:21:35,940 --> 00:21:38,850
These calculations are based

324
00:21:38,850 --> 00:21:41,899
on a general reliability theory.

325
00:21:41,900 --> 00:21:45,670
Typically, software checks and balances

326
00:21:45,670 --> 00:21:49,990
can be added to cumulative
probability equation.

327
00:21:49,990 --> 00:21:53,260
Now, also consider whether
the change in monitor

328
00:21:53,260 --> 00:21:57,913
is independent of others
or is dependent on others.

329
00:21:59,170 --> 00:22:03,190
What we wanna get to
is mean time to decay,

330
00:22:03,190 --> 00:22:06,080
so the probability distribution function

331
00:22:06,080 --> 00:22:11,080
is related to the CDF or
cumulative distribution function,

332
00:22:11,280 --> 00:22:14,200
being an integration
equation that I haven't shown

333
00:22:14,200 --> 00:22:17,700
but it's in most reliability books,

334
00:22:17,700 --> 00:22:22,700
and can be related to
the monitor failure rates

335
00:22:23,000 --> 00:22:25,824
with equation seven here.

336
00:22:25,824 --> 00:22:30,824
CDF helps to specify the
distribution of multivariate,

337
00:22:32,940 --> 00:22:35,740
random variables, and
therefore, it's great

338
00:22:35,740 --> 00:22:39,700
at reflecting the
monitors we have in place.

339
00:22:39,700 --> 00:22:43,810
Now, mean time to decay
is what we're looking for,

340
00:22:43,810 --> 00:22:48,810
lambda is failure rate and
since instantaneous failure rate

341
00:22:53,040 --> 00:22:58,040
H of T varies over time, we
define a single average number

342
00:22:58,390 --> 00:23:02,470
that reflects behavior
over a specific interval.

343
00:23:02,470 --> 00:23:07,320
Now, MTTD can be reported to analysts,

344
00:23:07,320 --> 00:23:10,710
it can be reported to
customers, competitors,

345
00:23:10,710 --> 00:23:15,080
and investors to
demonstrate the robustness,

346
00:23:15,080 --> 00:23:20,080
the resilience and trustworthiness
of any AI model you use.

347
00:23:22,640 --> 00:23:25,563
See how I linked that
back to those three words?

348
00:23:26,420 --> 00:23:31,420
All right, to summarize,
defining and implementing

349
00:23:32,930 --> 00:23:35,310
your monitors will be
the hardest challenge,

350
00:23:35,310 --> 00:23:39,260
that's definitely the
hardest, the math is simpler.

351
00:23:39,260 --> 00:23:42,560
To select the monitors, you
can utilize the two slides

352
00:23:42,560 --> 00:23:44,840
that I provided for you here

353
00:23:44,840 --> 00:23:47,330
or you can review the
NIST and ISO documents

354
00:23:47,330 --> 00:23:51,449
for added monitor insights in robustness,

355
00:23:51,450 --> 00:23:54,783
resilience and trustworthiness.

356
00:23:57,460 --> 00:24:02,270
Identify and improve current
monitors in your DevOps, ML Ops

357
00:24:02,270 --> 00:24:07,270
and in the field, make sure
that you have a feedback loop,

358
00:24:07,870 --> 00:24:12,280
and once monitors are
implemented, you can collect

359
00:24:13,861 --> 00:24:18,563
the metrics such as the
change in units per time,

360
00:24:19,470 --> 00:24:22,930
and utilize these monitor metrics

361
00:24:22,930 --> 00:24:25,063
within your reliability models.

362
00:24:26,090 --> 00:24:31,090
And you can calculate
instantaneous, cumulative,

363
00:24:31,250 --> 00:24:33,730
average failure rate,
and you can culminate

364
00:24:33,730 --> 00:24:38,730
in a reportable and very
public mean time to decay.

365
00:24:41,330 --> 00:24:44,840
Now, while we have enjoyed
developing cool new models

366
00:24:44,840 --> 00:24:49,840
since AI has resurfaced, it
is important for all of us now

367
00:24:50,720 --> 00:24:55,720
to consider long-term implications,
improve our processes,

368
00:24:55,980 --> 00:25:00,980
and set our sights on
calculating AI reliability.

369
00:25:01,270 --> 00:25:02,523
Thank you very much.

