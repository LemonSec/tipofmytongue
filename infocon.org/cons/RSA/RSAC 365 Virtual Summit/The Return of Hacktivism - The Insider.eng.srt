1
00:00:00,770 --> 00:00:02,780
- Well, hello everyone.

2
00:00:02,780 --> 00:00:05,260
Thank you for joining us today.

3
00:00:05,260 --> 00:00:08,309
We are gonna be talking about
The Return of Hacktivism,

4
00:00:08,310 --> 00:00:11,210
and more specifically, The Insider.

5
00:00:11,210 --> 00:00:12,760
So I am Jadee Hansen.

6
00:00:12,760 --> 00:00:15,520
I've been working in
security and technology

7
00:00:15,520 --> 00:00:17,630
for the past 17 years.

8
00:00:17,630 --> 00:00:21,939
I'm currently the CISO and CIO at Code42,

9
00:00:21,940 --> 00:00:23,740
where I'm honored to lead

10
00:00:23,740 --> 00:00:27,930
a really progressive and
transparent security team.

11
00:00:27,930 --> 00:00:30,310
We have a large focus on securing

12
00:00:30,310 --> 00:00:32,589
and driving the business forward.

13
00:00:32,590 --> 00:00:35,070
So Code42, is a software company.

14
00:00:35,070 --> 00:00:37,920
And what we do at Code42
is we develop software

15
00:00:37,920 --> 00:00:41,280
for security teams just like yours to use.

16
00:00:41,280 --> 00:00:44,300
I'm really excited to share with you today

17
00:00:44,300 --> 00:00:47,349
some of our learnings on
this really important topic,

18
00:00:47,350 --> 00:00:48,183
Insider Risk.

19
00:00:48,183 --> 00:00:50,080
So Masha, I'll turn it to you.

20
00:00:50,080 --> 00:00:51,640
- Thanks, Jadee, it's really a pleasure

21
00:00:51,640 --> 00:00:53,880
to be able to do this with you today.

22
00:00:53,880 --> 00:00:55,030
My name is Masha Sedova

23
00:00:55,030 --> 00:00:58,260
and I am the founder of Elevate Security.

24
00:00:58,260 --> 00:01:02,120
And my career has been focused
on looking at human risk

25
00:01:02,120 --> 00:01:05,870
in organizations and giving tools to CISOs

26
00:01:05,870 --> 00:01:08,723
and security teams to be able to measure

27
00:01:08,723 --> 00:01:11,810
the employee risk in their organization,

28
00:01:11,810 --> 00:01:14,820
and then have tools available to them

29
00:01:14,820 --> 00:01:18,039
to proactively secure and
contextually influence

30
00:01:18,040 --> 00:01:22,820
employees in their orgs to be
part of the security defense.

31
00:01:22,820 --> 00:01:25,419
A lot of what my career
has been focused on

32
00:01:25,420 --> 00:01:28,100
is knitting together the
concepts of behavioral science

33
00:01:28,100 --> 00:01:30,600
and psychology into security,

34
00:01:30,600 --> 00:01:32,679
which is a theme that I'm really excited

35
00:01:32,680 --> 00:01:35,720
to weave into some of
our conversation today.

36
00:01:35,720 --> 00:01:39,090
So with that, let's get started.

37
00:01:39,090 --> 00:01:43,040
So in the next 30 or so
minutes that we have together,

38
00:01:43,040 --> 00:01:45,270
what Jadee and I are going
to cover with you all

39
00:01:45,270 --> 00:01:48,259
is the question of what is Insider Risk?

40
00:01:48,260 --> 00:01:50,520
When we talk about this, what do we mean

41
00:01:50,520 --> 00:01:52,119
and ensure that we're all on the same page

42
00:01:52,120 --> 00:01:53,760
with a common definition?

43
00:01:53,760 --> 00:01:56,320
And then taking a look at 2020

44
00:01:56,320 --> 00:01:59,070
and understanding how all the changes

45
00:01:59,070 --> 00:02:01,460
that happened in that year,

46
00:02:01,460 --> 00:02:03,750
how those affected the
Insider Threat Landscape

47
00:02:03,750 --> 00:02:06,523
and how that looks
different from years before.

48
00:02:07,680 --> 00:02:10,530
Jadee and I will walk through
some recent cases in the news

49
00:02:10,530 --> 00:02:14,010
that are really valuable lessons
for us all to learn from.

50
00:02:14,010 --> 00:02:16,459
And then we'll walk through
a couple of examples

51
00:02:16,460 --> 00:02:18,640
of what you can do to
help address this risk

52
00:02:18,640 --> 00:02:20,410
in your organization.

53
00:02:20,410 --> 00:02:22,799
And then look around the
corner around what's next

54
00:02:22,800 --> 00:02:25,400
and how you can think
about solving this problem

55
00:02:25,400 --> 00:02:27,633
going backs in your org tomorrow.

56
00:02:29,140 --> 00:02:32,382
So what is Insider Risk?

57
00:02:33,320 --> 00:02:36,420
One of my favorite
definitions of Insider Risk

58
00:02:37,440 --> 00:02:39,079
is this spectrum.

59
00:02:39,080 --> 00:02:40,930
Because when we think about Insider Risk,

60
00:02:40,930 --> 00:02:43,290
there isn't just one type of risk

61
00:02:43,290 --> 00:02:45,200
that exists in our organization,

62
00:02:45,200 --> 00:02:47,810
there are primarily two
ends of the spectrum,

63
00:02:47,810 --> 00:02:52,160
one is malicious, and that is
when an employee or contractor

64
00:02:52,160 --> 00:02:54,250
or some worker in our organizations,

65
00:02:54,250 --> 00:02:58,400
decides to act with the
conscious decision to do harm.

66
00:02:58,400 --> 00:03:01,160
So this is a knowledgeable person

67
00:03:01,160 --> 00:03:04,630
who is acting against the
best interests of a company

68
00:03:04,630 --> 00:03:08,670
and knowingly trying to
steal data or information

69
00:03:08,670 --> 00:03:10,019
or cause harm.

70
00:03:10,020 --> 00:03:13,490
Now on the other end, you
have accidental Insider Risk.

71
00:03:13,490 --> 00:03:15,540
And quite honestly, this
is the one that happens

72
00:03:15,540 --> 00:03:18,350
significantly more often in the landscape.

73
00:03:18,350 --> 00:03:21,920
And accidental is when employees who maybe

74
00:03:21,920 --> 00:03:23,600
because they don't know better,

75
00:03:23,600 --> 00:03:25,790
maybe because they're distracted,

76
00:03:25,790 --> 00:03:27,540
there's a lot of reasons
that go into this.

77
00:03:27,540 --> 00:03:29,250
And we'll talk about a few of them.

78
00:03:29,250 --> 00:03:32,150
But they are not consciously
trying to cause harm,

79
00:03:32,150 --> 00:03:33,860
but still do so anyway.

80
00:03:33,860 --> 00:03:36,030
And so in this presentation,
we're actually gonna cover

81
00:03:36,030 --> 00:03:38,600
both types of risks and talk about

82
00:03:38,600 --> 00:03:42,030
which kind of Insider
Risk we are focusing on

83
00:03:42,030 --> 00:03:44,933
when we give a specific example
or remediation technique.

84
00:03:47,080 --> 00:03:48,960
- You are also gonna notice that

85
00:03:48,960 --> 00:03:52,790
we're using the term "Insider
Risk" in this presentation,

86
00:03:52,790 --> 00:03:54,940
and that's very, very intentional.

87
00:03:54,940 --> 00:03:57,170
So I wanna take a minute and just explain

88
00:03:57,170 --> 00:03:58,950
why we're not talking about DLP

89
00:03:58,950 --> 00:04:01,310
and we're not talking
about Insider Threat.

90
00:04:01,310 --> 00:04:04,720
So first, the DLP I think we can all agree

91
00:04:04,720 --> 00:04:08,240
this has been kind of a
failed technology for us.

92
00:04:08,240 --> 00:04:12,760
I can say that I am a
recovering DLP user myself.

93
00:04:12,760 --> 00:04:14,840
And so for the last six years,

94
00:04:14,840 --> 00:04:17,820
Gartner has been giving this
same advice to customers

95
00:04:17,820 --> 00:04:21,120
about DLP and the advice is don't do it.

96
00:04:21,120 --> 00:04:25,130
It's very cumbersome, it's
designed to create and address

97
00:04:25,130 --> 00:04:28,130
a very specific compliance problem.

98
00:04:28,130 --> 00:04:30,610
And we haven't really
made the necessary changes

99
00:04:30,610 --> 00:04:34,870
that we need to to meet
users where they are today.

100
00:04:34,870 --> 00:04:36,950
DLP hasn't made that change.

101
00:04:36,950 --> 00:04:39,750
And so Inter Insider Threat,

102
00:04:39,750 --> 00:04:42,900
so this term and the
technology in this category,

103
00:04:42,900 --> 00:04:43,799
we're getting better.

104
00:04:43,800 --> 00:04:46,320
But as Masha pointed out,

105
00:04:46,320 --> 00:04:49,469
this term is very focused
on that malicious user

106
00:04:49,470 --> 00:04:50,780
and we need something broader.

107
00:04:50,780 --> 00:04:54,700
We know that most of the data
issues that we have today

108
00:04:54,700 --> 00:04:58,150
and most of the events that
happen in our organization

109
00:04:58,150 --> 00:05:02,380
that cause issues are due
to non malicious actions

110
00:05:02,380 --> 00:05:03,830
by our employees,

111
00:05:03,830 --> 00:05:05,580
so we need something that can move beyond

112
00:05:05,580 --> 00:05:10,060
that 1% of employees, that
is the malicious person

113
00:05:10,060 --> 00:05:13,160
trying to exfil data and cover visibility

114
00:05:13,160 --> 00:05:15,100
to all of our end users.

115
00:05:15,100 --> 00:05:17,490
So inter Insider Risk.

116
00:05:17,490 --> 00:05:19,150
And so Insider Risk.

117
00:05:19,150 --> 00:05:21,070
This is a category that's really focused

118
00:05:21,070 --> 00:05:24,760
on protecting all of your
data, all of your users,

119
00:05:24,760 --> 00:05:28,190
and from everyday exfiltration
risk, no matter intense,

120
00:05:28,190 --> 00:05:31,120
so no matter malicious or not malicious.

121
00:05:31,120 --> 00:05:34,250
Insider Risk is that 100% visibility

122
00:05:34,250 --> 00:05:37,180
that security teams are looking for.

123
00:05:37,180 --> 00:05:40,340
And this distinction, though it's small,

124
00:05:40,340 --> 00:05:43,292
it's a really important
distinction that we wanna make.

125
00:05:45,360 --> 00:05:47,960
So let's move forward
and what I wanna do next

126
00:05:47,960 --> 00:05:51,690
is focus on how Insider Risk has changed,

127
00:05:51,690 --> 00:05:55,620
and what this really means
for your organizations.

128
00:05:55,620 --> 00:05:58,270
So this first screenshot here,

129
00:05:58,270 --> 00:06:01,479
this is from the 2019 data breach report.

130
00:06:01,480 --> 00:06:03,980
And you can see on the yellow line there

131
00:06:03,980 --> 00:06:07,730
that employee error has
consistently going up

132
00:06:07,730 --> 00:06:09,300
year after year.

133
00:06:09,300 --> 00:06:13,790
In 2019, employee error was
in the top three sources

134
00:06:13,790 --> 00:06:15,740
for data breaches.

135
00:06:15,740 --> 00:06:17,840
This is actually beating out malware.

136
00:06:17,840 --> 00:06:21,563
So a really important
trend to keep our eye on.

137
00:06:22,970 --> 00:06:27,150
And this next slide, this is
another really important stat

138
00:06:27,150 --> 00:06:29,250
from the data breach report.

139
00:06:29,250 --> 00:06:34,250
30% of breaches are happening
as a result of Insider Risk.

140
00:06:34,550 --> 00:06:37,400
A lot has changed in 2020

141
00:06:37,400 --> 00:06:40,179
and the increase of
breaches caused by insiders

142
00:06:40,180 --> 00:06:42,760
has really only gone up.

143
00:06:42,760 --> 00:06:46,590
And so we're gonna dig into
a few of the reasons why,

144
00:06:46,590 --> 00:06:48,090
starting with this next slide.

145
00:06:49,240 --> 00:06:52,580
So the very first reason macro reason

146
00:06:52,580 --> 00:06:55,890
why we think Insider
Risk is such an issue now

147
00:06:55,890 --> 00:06:57,500
is the fact that we're leveraging

148
00:06:57,500 --> 00:06:59,590
more and more collaboration technology

149
00:06:59,590 --> 00:07:01,429
within our organizations.

150
00:07:01,430 --> 00:07:04,237
There was a comment from the IDC that said

151
00:07:04,237 --> 00:07:06,980
"Adoption of collaboration technologies

152
00:07:06,980 --> 00:07:11,237
accelerated by almost five
years just in the year 2020,

153
00:07:11,237 --> 00:07:13,300
based on what we were faced with,

154
00:07:13,300 --> 00:07:16,520
and everybody kinda moving remote."

155
00:07:16,520 --> 00:07:19,469
So these collaboration
technology platforms,

156
00:07:19,470 --> 00:07:21,510
man, they're great.

157
00:07:21,510 --> 00:07:26,150
But they also pose a ton of
risks to a company's data.

158
00:07:26,150 --> 00:07:30,120
using cloud storage locations
make it a lot harder to track

159
00:07:30,120 --> 00:07:33,330
where your data is, how
your users are sharing it.

160
00:07:33,330 --> 00:07:36,890
One thing that I run into
on a very regular basis

161
00:07:36,890 --> 00:07:40,469
is my users just
accidentally making documents

162
00:07:40,470 --> 00:07:42,760
they intend to be private public links,

163
00:07:42,760 --> 00:07:45,810
because these platforms
make it so easy to share.

164
00:07:45,810 --> 00:07:48,160
So certainly putting our
data at a higher risk.

165
00:07:49,870 --> 00:07:53,620
The second reason why
we believe Insider Risk

166
00:07:53,620 --> 00:07:57,600
is becoming a bigger issue and on the rise

167
00:07:57,600 --> 00:07:59,130
is employee turnover.

168
00:07:59,130 --> 00:08:02,200
So turnover at companies has
really never been higher.

169
00:08:02,200 --> 00:08:05,060
Just since February, over 3 million people

170
00:08:05,060 --> 00:08:07,410
have left their place of employment

171
00:08:07,410 --> 00:08:09,230
and why is it such a big deal

172
00:08:09,230 --> 00:08:12,010
when people leave their
place of employment?

173
00:08:12,010 --> 00:08:14,150
It's a big deal, because we know

174
00:08:14,150 --> 00:08:16,419
that when people leave companies,

175
00:08:16,420 --> 00:08:18,320
they take all their data with them.

176
00:08:18,320 --> 00:08:21,060
Our latest data research report,

177
00:08:21,060 --> 00:08:23,550
people were asked if
they took data with them

178
00:08:23,550 --> 00:08:25,510
when they left their employer

179
00:08:25,510 --> 00:08:29,349
and 63% of people
responded, "Yes, they did".

180
00:08:29,350 --> 00:08:31,920
And these are just the
people that are admitting it.

181
00:08:31,920 --> 00:08:35,320
We know that most people when
they leave the organization

182
00:08:35,320 --> 00:08:36,763
are taking data with them.

183
00:08:38,720 --> 00:08:42,820
Not surprisingly, another
reason we believe Insider Risk

184
00:08:42,820 --> 00:08:46,130
is on the rise is remote work.

185
00:08:46,130 --> 00:08:48,620
So remote work is something
that has been growing

186
00:08:48,620 --> 00:08:51,430
in the past few years,
but overnight accelerated

187
00:08:51,430 --> 00:08:53,810
with the impacts of COVID-19.

188
00:08:53,810 --> 00:08:54,642
So many of you

189
00:08:54,643 --> 00:08:58,400
are probably listening to
this talk right now from home.

190
00:08:58,400 --> 00:09:00,970
And you're thinking,
"Okay, what does this mean?

191
00:09:00,970 --> 00:09:05,070
Why does this play a
role into the data risk?"

192
00:09:05,070 --> 00:09:06,503
So I'll explain it.

193
00:09:07,480 --> 00:09:12,470
When we're at home, our
data is at a higher degree

194
00:09:12,470 --> 00:09:14,860
of leaving the organization.

195
00:09:14,860 --> 00:09:17,900
In a recent poll, 61% of security leaders

196
00:09:17,900 --> 00:09:21,130
told us that remote work
was a contributing factor

197
00:09:21,130 --> 00:09:23,620
in their particular data breach,

198
00:09:23,620 --> 00:09:26,770
and it's really not
super hard to understand.

199
00:09:26,770 --> 00:09:28,223
So on this next slide,

200
00:09:31,264 --> 00:09:33,270
there are two important
things going on here

201
00:09:33,270 --> 00:09:36,620
that I wanna describe
related to remote work.

202
00:09:36,620 --> 00:09:40,410
First employees feel as
though there's less oversight

203
00:09:40,410 --> 00:09:42,150
when they're working from home,

204
00:09:42,150 --> 00:09:45,959
and they're more likely to
engage in a malicious action.

205
00:09:45,960 --> 00:09:47,610
The other thing that's going on

206
00:09:47,610 --> 00:09:50,390
is that we're incredibly
distracted at home,

207
00:09:50,390 --> 00:09:53,650
especially this year with
kids and family members

208
00:09:53,650 --> 00:09:55,600
also working at home with us.

209
00:09:55,600 --> 00:09:58,770
I have my girls, I have
my dog, my husband,

210
00:09:58,770 --> 00:09:59,603
we're all at home

211
00:09:59,603 --> 00:10:03,900
and there are endless,
endless distractions.

212
00:10:03,900 --> 00:10:06,810
I love some of these images
that you see on the screen

213
00:10:06,810 --> 00:10:09,959
that show what it's really
like working from home,

214
00:10:09,960 --> 00:10:13,140
and all the distractions
that come with it.

215
00:10:13,140 --> 00:10:15,340
So what does it mean
for your company's data?

216
00:10:15,340 --> 00:10:18,720
Distractions lead to more
and more of our employees

217
00:10:18,720 --> 00:10:19,810
just making mistakes,

218
00:10:19,810 --> 00:10:22,599
again, not malicious
mistakes, but mistakes.

219
00:10:22,600 --> 00:10:26,430
And these mistakes lead to
unintentional data exposures

220
00:10:26,430 --> 00:10:27,943
for our organizations.

221
00:10:29,950 --> 00:10:33,020
- Yeah, so when we also
take a look at the reasons

222
00:10:33,020 --> 00:10:35,250
why data leaves our networks

223
00:10:35,250 --> 00:10:39,470
and put on the perspective
of malicious insider threat,

224
00:10:39,470 --> 00:10:42,490
we can see that, this is
one of my favorite models

225
00:10:42,490 --> 00:10:45,780
that explains how
incidents like this happen.

226
00:10:45,780 --> 00:10:48,300
So if we take a person
who has a predisposition

227
00:10:48,300 --> 00:10:50,969
of circumventing authority

228
00:10:50,970 --> 00:10:55,660
and maybe being inclined to
stealing data to begin with,

229
00:10:55,660 --> 00:10:57,400
and then they have
stressors in their life,

230
00:10:57,400 --> 00:11:00,160
sometimes financial stress,
or sometimes emotional,

231
00:11:00,160 --> 00:11:03,439
sometimes mental or physical,

232
00:11:03,440 --> 00:11:04,670
there's a lot of different reasons

233
00:11:04,670 --> 00:11:07,010
why we think about stressors,

234
00:11:07,010 --> 00:11:11,390
we amp up a situation
in which an individual

235
00:11:11,390 --> 00:11:16,390
might be more apt to
conducting what is illegal

236
00:11:17,900 --> 00:11:19,510
or unethical actions.

237
00:11:19,510 --> 00:11:22,819
Now, in the moment where
they may want to do so

238
00:11:22,820 --> 00:11:25,430
there's a specific choice that happens.

239
00:11:25,430 --> 00:11:29,300
They might be faced with a
mitigating factor or a solution

240
00:11:29,300 --> 00:11:32,120
that gets them off of this
track and back to normal.

241
00:11:32,120 --> 00:11:36,200
And mitigating solution, while
that sounds maybe foreign,

242
00:11:36,200 --> 00:11:40,687
looks totally normal, it's our
everyday society and saying,

243
00:11:40,687 --> 00:11:43,190
"This is normal behavior that is not..."

244
00:11:43,190 --> 00:11:46,290
And there are ways that
we can get somebody

245
00:11:46,290 --> 00:11:48,510
to rethink their actions.

246
00:11:48,510 --> 00:11:52,700
Now, without those norms,
those type of stressors

247
00:11:52,700 --> 00:11:56,240
can amplify and ultimately
lead to concerning behavior.

248
00:11:56,240 --> 00:11:58,690
Now, if we take a look at the situation

249
00:11:58,690 --> 00:11:59,880
that Jadee just talked about,

250
00:11:59,880 --> 00:12:02,480
and we're all working from home,

251
00:12:02,480 --> 00:12:05,470
and the circumstances of
our normal work environment

252
00:12:05,470 --> 00:12:07,170
has totally changed,

253
00:12:07,170 --> 00:12:09,920
we no longer have those work norms,

254
00:12:09,920 --> 00:12:12,099
to help us with those mitigations.

255
00:12:12,100 --> 00:12:14,690
So not only do we have increased stress,

256
00:12:14,690 --> 00:12:15,880
I'm gonna mention in a minute,

257
00:12:15,880 --> 00:12:18,310
but the things that help
us get back on track

258
00:12:18,310 --> 00:12:19,349
aren't really there.

259
00:12:19,350 --> 00:12:22,769
Before, if we have some
suspicious activity

260
00:12:22,769 --> 00:12:24,630
happening around us,

261
00:12:24,630 --> 00:12:26,689
it's a lot harder to get
away with that, frankly.

262
00:12:26,690 --> 00:12:28,452
We're sharing cubicle space.

263
00:12:28,452 --> 00:12:32,820
And you can kinda see if
someone is plugging in USBs

264
00:12:32,820 --> 00:12:34,550
where they're not
supposed to occasionally,

265
00:12:34,550 --> 00:12:36,640
or putting in laptops into a bag

266
00:12:36,640 --> 00:12:38,860
that they haven't taken out before.

267
00:12:38,860 --> 00:12:41,120
There's more norms that we can observe

268
00:12:41,120 --> 00:12:43,720
and frankly, hold each
other accountable to.

269
00:12:43,720 --> 00:12:46,440
Work from home, that doesn't exist for us.

270
00:12:46,440 --> 00:12:50,790
And there's a lot less
people to even ask for help.

271
00:12:50,790 --> 00:12:53,689
Your IT department is
significantly far away.

272
00:12:53,690 --> 00:12:55,117
There's no cube mates to say,

273
00:12:55,117 --> 00:12:57,329
"Is that a real phishing attack or not?"

274
00:12:57,330 --> 00:13:01,232
So both unintentional and malicious,

275
00:13:01,232 --> 00:13:04,500
have increased chances of happening

276
00:13:04,500 --> 00:13:06,310
in this type of environment.

277
00:13:06,310 --> 00:13:08,380
And as we saw in the previous slide,

278
00:13:08,380 --> 00:13:11,080
not only is mitigating factors,
one of the components of it,

279
00:13:11,080 --> 00:13:12,180
but the stress

280
00:13:12,180 --> 00:13:14,670
and the stressors that
amplified the situation.

281
00:13:14,670 --> 00:13:17,780
And according to the most
recent Gallup report,

282
00:13:17,780 --> 00:13:21,250
Americans are at a nine year old time low

283
00:13:21,250 --> 00:13:23,540
of dissatisfaction and unhappiness.

284
00:13:23,540 --> 00:13:26,110
And for many people,
it's a different reason.

285
00:13:26,110 --> 00:13:27,760
But there's plenty to choose from.

286
00:13:27,760 --> 00:13:30,650
Politics, pandemics,
unemployment to social movements.

287
00:13:30,650 --> 00:13:33,640
There's a lot going on
in everybody's life,

288
00:13:33,640 --> 00:13:38,520
that truly increases the stress
factor for every individual

289
00:13:38,520 --> 00:13:41,280
and as a whole organizations,

290
00:13:41,280 --> 00:13:45,329
which then translates
to more of a tinderbox

291
00:13:45,330 --> 00:13:48,243
as it relates to insider
threat and this risk.

292
00:13:50,405 --> 00:13:51,620
- So what we wanted to do next

293
00:13:51,620 --> 00:13:55,470
was dig into a few of the recent cases.

294
00:13:55,470 --> 00:13:58,350
I'm gonna start with this first one

295
00:13:58,350 --> 00:14:01,120
that took place at Tesla.

296
00:14:01,120 --> 00:14:02,210
So Tesla obviously

297
00:14:02,210 --> 00:14:05,210
has some really important
intellectual property to protect,

298
00:14:05,210 --> 00:14:09,170
has some very public insider
threat cases in the news,

299
00:14:09,170 --> 00:14:11,229
in this case here outlined.

300
00:14:11,230 --> 00:14:14,700
An employee at Tesla
certainly was on that track

301
00:14:14,700 --> 00:14:19,700
where they were passed up
for getting a job promotion

302
00:14:19,970 --> 00:14:22,430
and so this was an increased stress moment

303
00:14:22,430 --> 00:14:24,130
in this employee's life.

304
00:14:24,130 --> 00:14:28,330
And they chose to exfiltrate
very sensitive data.

305
00:14:28,330 --> 00:14:30,230
So employee had to trusted access,

306
00:14:30,230 --> 00:14:32,020
sent a large amount of sensitive data

307
00:14:32,020 --> 00:14:34,040
to an unknown third party.

308
00:14:34,040 --> 00:14:36,900
Elan later called the action extensive

309
00:14:36,900 --> 00:14:38,709
and damaging sabotage.

310
00:14:38,710 --> 00:14:40,670
And the impact here for Tesla

311
00:14:40,670 --> 00:14:44,140
was a very expensive
lawsuit, reputation damage,

312
00:14:44,140 --> 00:14:46,260
and we have not seen yet

313
00:14:46,260 --> 00:14:48,800
but certainly impact to the stolen IP.

314
00:14:48,800 --> 00:14:53,793
And this impact will certainly
come to fruition a bit later.

315
00:14:55,100 --> 00:14:57,010
The next case here is

316
00:14:57,010 --> 00:14:59,770
something that happened
at Shopify this year.

317
00:14:59,770 --> 00:15:03,079
The media reports that there
were two rogue employees

318
00:15:03,080 --> 00:15:07,300
that legitimately had access
to customer transaction records

319
00:15:07,300 --> 00:15:10,882
and stole all of those
customer transaction records.

320
00:15:12,937 --> 00:15:15,130
And you think about like

321
00:15:15,130 --> 00:15:17,263
what's the harm with transaction records?

322
00:15:17,263 --> 00:15:20,850
Well, typically these records
include payment information,

323
00:15:20,850 --> 00:15:22,920
and Shopify didn't confirm

324
00:15:22,920 --> 00:15:25,030
that they lost credit card numbers or not.

325
00:15:25,030 --> 00:15:27,880
But multiple shoppers during that time,

326
00:15:27,880 --> 00:15:29,580
also claimed that they suffered

327
00:15:29,580 --> 00:15:33,480
fraudulent credit card charges
with the breach time window.

328
00:15:33,480 --> 00:15:37,720
So the impact for Shopify

329
00:15:37,720 --> 00:15:40,130
wasn't necessarily just
for their consumers,

330
00:15:40,130 --> 00:15:42,410
but Shopify last merchants,

331
00:15:42,410 --> 00:15:45,689
their stock dropped due
to this particular news.

332
00:15:45,690 --> 00:15:47,943
So certainly a big impact for the company.

333
00:15:49,340 --> 00:15:52,290
This last example that I wanted to share

334
00:15:52,290 --> 00:15:56,660
is one that actually took
place in my company at Code42.

335
00:15:56,660 --> 00:15:59,680
And I mentioned before
the Insider Risk programs,

336
00:15:59,680 --> 00:16:03,989
they need to cover both malicious
and non malicious actions.

337
00:16:03,990 --> 00:16:08,990
And this example of
our company's data here

338
00:16:09,610 --> 00:16:13,690
was put at risk from a
non malicious action.

339
00:16:13,690 --> 00:16:17,400
So as part of my internal
Insider Risk program,

340
00:16:17,400 --> 00:16:21,640
what we do is we go through
and we have visibility

341
00:16:21,640 --> 00:16:23,540
to all of our employees actions,

342
00:16:23,540 --> 00:16:26,390
and we identified a large amount of data

343
00:16:26,390 --> 00:16:31,010
moving from personal machines to personal

344
00:16:31,010 --> 00:16:35,150
I'm sorry, corporate machines
to personal iCloud drives,

345
00:16:35,150 --> 00:16:38,492
and this was for two of
our senior level employees.

346
00:16:39,490 --> 00:16:43,560
Because my company has decided
to use only Box and G drive

347
00:16:43,560 --> 00:16:45,930
as our corporate solutions.

348
00:16:45,930 --> 00:16:50,930
This iCloud data movement was
certainly a very big red flag.

349
00:16:51,460 --> 00:16:55,000
So we dug in and upon further review,

350
00:16:55,000 --> 00:16:59,270
this was not an intentional
data exfiltration action,

351
00:16:59,270 --> 00:17:02,880
the employees device
itself was misconfigured

352
00:17:02,880 --> 00:17:06,300
to allow this sync to take
place in the first place.

353
00:17:06,300 --> 00:17:07,810
But this is a really great example

354
00:17:07,810 --> 00:17:09,710
of the level of visibility,

355
00:17:09,710 --> 00:17:14,609
we think customers and
companies across need

356
00:17:14,609 --> 00:17:19,012
to address this risk to data exfiltration.

357
00:17:21,000 --> 00:17:24,869
- And I had two examples
that I wanted to bring up

358
00:17:26,099 --> 00:17:27,329
for this particular talk

359
00:17:28,890 --> 00:17:31,470
related to both related to hacktivism.

360
00:17:31,470 --> 00:17:33,170
Now hacktivism for those
of you who are listening

361
00:17:33,170 --> 00:17:35,030
who may not be familiar

362
00:17:35,030 --> 00:17:38,290
is when someone hacks for
social or political purpose.

363
00:17:38,290 --> 00:17:40,070
And when we say hack in this case,

364
00:17:40,070 --> 00:17:42,030
it also means insider threat risk,

365
00:17:42,030 --> 00:17:45,480
so something that causes malicious action

366
00:17:45,480 --> 00:17:46,740
and malicious harm.

367
00:17:46,740 --> 00:17:51,740
So in 2017, during Trump's presidency,

368
00:17:52,630 --> 00:17:56,060
an employee at Twitter without
the company's permission,

369
00:17:56,060 --> 00:17:58,240
deleted the President's account

370
00:17:58,240 --> 00:18:01,120
for a very visible 11 minutes

371
00:18:01,120 --> 00:18:04,209
and that obviously put
Twitter in the headlines

372
00:18:04,210 --> 00:18:09,210
and had a lot of cleanup to
do as a function of that.

373
00:18:09,300 --> 00:18:12,270
And this was totally in line

374
00:18:12,270 --> 00:18:15,440
with the employees access level.

375
00:18:15,440 --> 00:18:17,920
However, it did not have
the level of controls

376
00:18:17,920 --> 00:18:21,600
or authentication that
Twitter has since put in.

377
00:18:21,600 --> 00:18:24,370
One of the examples that taught Twitter

378
00:18:24,370 --> 00:18:26,409
how to think about insider threat

379
00:18:26,410 --> 00:18:29,010
in a little bit of a different capacity.

380
00:18:29,010 --> 00:18:33,810
And, most recently, at the
very beginning of January

381
00:18:33,810 --> 00:18:36,629
an angry staffer at the State Department

382
00:18:36,630 --> 00:18:39,560
decided to update the
State Department's website,

383
00:18:39,560 --> 00:18:43,020
reflecting their opinion of Trump's term

384
00:18:43,020 --> 00:18:47,190
and saying that it ended
several weeks earlier

385
00:18:47,190 --> 00:18:48,690
than it actually did.

386
00:18:48,690 --> 00:18:52,030
So again, this might seem funny

387
00:18:52,030 --> 00:18:55,822
or something that's worth
putting social media

388
00:18:58,110 --> 00:19:00,209
or news attention on.

389
00:19:00,210 --> 00:19:03,030
However, this is something

390
00:19:03,030 --> 00:19:05,480
that may be an incremental change.

391
00:19:05,480 --> 00:19:08,720
But in the back end, who knows
what a disgruntled employee

392
00:19:08,720 --> 00:19:11,630
with this type of access might
also be interested in doing

393
00:19:11,630 --> 00:19:13,200
and might be able to do?

394
00:19:13,200 --> 00:19:18,030
So all of these are relatively
easy changes to restore.

395
00:19:18,030 --> 00:19:20,200
However, both of them have put Twitter

396
00:19:20,200 --> 00:19:22,444
and State Department on headlines,

397
00:19:22,444 --> 00:19:27,340
and are maybe symptoms of
much larger insider threat,

398
00:19:27,340 --> 00:19:29,220
things happening in their organizations

399
00:19:29,220 --> 00:19:32,533
related to hacktivism and
politically motivated actions.

400
00:19:34,070 --> 00:19:37,040
So when we think about
the fact that there are

401
00:19:37,040 --> 00:19:40,050
these insider threats
in our organizations,

402
00:19:40,050 --> 00:19:41,899
the next question is, "Well,
what do I do about it?

403
00:19:41,900 --> 00:19:44,270
Yes, I understand that 2020

404
00:19:44,270 --> 00:19:46,810
is introducing an additional
risk to my organization,

405
00:19:46,810 --> 00:19:49,889
but what can I as a security practitioner,

406
00:19:49,890 --> 00:19:53,540
or as the CISO do, going
back to my organization

407
00:19:53,540 --> 00:19:55,350
and to address this threat?"

408
00:19:55,350 --> 00:19:56,969
Well, there's two components

409
00:19:56,970 --> 00:19:59,730
that we want to talk
through with the next.

410
00:19:59,730 --> 00:20:03,670
The first is employee risk
and the second is data risk,

411
00:20:03,670 --> 00:20:06,790
the two elements of Insider Risk.

412
00:20:06,790 --> 00:20:09,003
So let's start with employee risk first.

413
00:20:10,360 --> 00:20:11,840
So in all of those case studies

414
00:20:11,840 --> 00:20:13,639
that we just walked through with you,

415
00:20:13,640 --> 00:20:16,100
we heard that there were employees

416
00:20:16,100 --> 00:20:18,639
making risky decisions in an organization,

417
00:20:18,640 --> 00:20:21,460
sometimes knowingly,
sometimes unknowingly.

418
00:20:21,460 --> 00:20:25,770
But very few organizations
that I work with today

419
00:20:25,770 --> 00:20:27,848
have the ability to answer

420
00:20:27,848 --> 00:20:30,710
which employees are
making risky decisions,

421
00:20:30,710 --> 00:20:33,430
what decisions are they
making, and how often?

422
00:20:33,430 --> 00:20:35,510
And why that is incredibly important

423
00:20:35,510 --> 00:20:38,820
is if we are able to
answer this question today,

424
00:20:38,820 --> 00:20:41,409
then we can have a sense of control

425
00:20:41,410 --> 00:20:45,120
of where we need to be
investing our time, resources,

426
00:20:45,120 --> 00:20:49,149
monitoring, controls, education,

427
00:20:49,150 --> 00:20:52,017
any type of intervention to
start addressing this risk.

428
00:20:52,017 --> 00:20:54,390
And this is primarily for

429
00:20:54,390 --> 00:20:57,130
when we look at the
unintentional insider threat.

430
00:20:57,130 --> 00:20:58,890
Because when we look at malicious,

431
00:20:58,890 --> 00:21:00,890
it's often a lot more calculated,

432
00:21:00,890 --> 00:21:04,640
and doesn't always
necessarily have the pre flags

433
00:21:04,640 --> 00:21:08,360
that the accidental might have.

434
00:21:08,360 --> 00:21:11,250
But I have some great
news to share with you all

435
00:21:11,250 --> 00:21:13,570
around how you might even
start on understanding

436
00:21:13,570 --> 00:21:17,050
which employees are making
your risky decisions.

437
00:21:17,050 --> 00:21:19,780
And that is the fact
that the past decisions

438
00:21:19,780 --> 00:21:23,600
of our employees are one of
the best predictors we can use

439
00:21:23,600 --> 00:21:25,060
for future actions.

440
00:21:25,060 --> 00:21:28,889
There was a fantastic study
done in 2015 by Dr. Caputo

441
00:21:28,890 --> 00:21:31,270
around phishing susceptibility
and spear phishing

442
00:21:31,270 --> 00:21:33,120
as one example of risk.

443
00:21:33,120 --> 00:21:35,020
And her work found that

444
00:21:35,020 --> 00:21:38,500
if an employee clicked on one
spear phishing out of three,

445
00:21:38,500 --> 00:21:40,930
it was a very high indication
that they were going to click

446
00:21:40,930 --> 00:21:43,110
on the future second, and third.

447
00:21:43,110 --> 00:21:46,209
So how we deal with one
risk and one point of time,

448
00:21:46,210 --> 00:21:48,900
is an excellent map of our mindsets

449
00:21:48,900 --> 00:21:50,260
around how we think about security

450
00:21:50,260 --> 00:21:52,520
and the decisions we make in the future.

451
00:21:52,520 --> 00:21:56,090
So let's look at our employees risks

452
00:21:56,090 --> 00:21:58,649
as a function of all of
the behaviors and decisions

453
00:21:58,650 --> 00:22:00,410
they make on your network.

454
00:22:00,410 --> 00:22:01,243
We can take a look at

455
00:22:01,243 --> 00:22:03,950
how proactive someone is around security,

456
00:22:03,950 --> 00:22:07,090
do they store strong
passwords, do they use MFA?

457
00:22:07,090 --> 00:22:08,439
Or are they on the other end

458
00:22:08,440 --> 00:22:10,250
where they're navigating
to malicious sites

459
00:22:10,250 --> 00:22:14,310
downloading malware, as Jadee
was mentioning accidentally,

460
00:22:14,310 --> 00:22:16,409
but oversharing not being mindful

461
00:22:16,410 --> 00:22:18,550
about the permissions they're
setting in their accounts,

462
00:22:18,550 --> 00:22:22,120
but regularly making
small but risky trade offs

463
00:22:22,120 --> 00:22:23,659
that do not benefit security,

464
00:22:23,660 --> 00:22:26,970
that helps us understand that
this is a risky employee.

465
00:22:26,970 --> 00:22:28,970
Now, the good news is that this data

466
00:22:28,970 --> 00:22:31,570
exists in most of your
environments already.

467
00:22:31,570 --> 00:22:35,379
These are related to the logs
for your security solutions

468
00:22:35,380 --> 00:22:36,560
that you've likely invested in

469
00:22:36,560 --> 00:22:40,550
from endpoint, to email security,
to curb these solutions.

470
00:22:40,550 --> 00:22:43,970
And so instead of just thinking
of them as incident logs,

471
00:22:43,970 --> 00:22:47,060
think of those logs as security decisions

472
00:22:47,060 --> 00:22:49,560
that employees are making
that you can use to learn from

473
00:22:49,560 --> 00:22:52,300
around where your risks are growing

474
00:22:52,300 --> 00:22:55,139
or existing in your organization.

475
00:22:55,140 --> 00:23:00,140
And in doing so, you are able
to be able to map the risk

476
00:23:00,580 --> 00:23:03,220
for not just a department

477
00:23:03,220 --> 00:23:05,710
and not just to someone with
a particular access type,

478
00:23:05,710 --> 00:23:10,090
but to every individual based
on what kind of decisions

479
00:23:10,090 --> 00:23:14,610
you are seeing them do well or poorly.

480
00:23:14,610 --> 00:23:17,199
And why that is incredibly exciting

481
00:23:17,200 --> 00:23:20,070
and a really valuable way to think about

482
00:23:20,070 --> 00:23:21,950
managing distrust going forward

483
00:23:21,950 --> 00:23:23,420
is that it helps us think about

484
00:23:23,420 --> 00:23:27,040
how we architect our permissioning
going forward as well.

485
00:23:27,040 --> 00:23:30,670
And, as we all may be familiar with

486
00:23:30,670 --> 00:23:33,270
the prevalence of work
from home environments,

487
00:23:33,270 --> 00:23:36,360
has given rise to zero trust security,

488
00:23:36,360 --> 00:23:39,889
which is basically allowing,

489
00:23:39,890 --> 00:23:41,900
making sure that we're not granting access

490
00:23:41,900 --> 00:23:46,000
to a user to particular permit
to data or to applications

491
00:23:46,000 --> 00:23:48,610
until that identity has been verified.

492
00:23:48,610 --> 00:23:51,340
And now the things that
can go into that identity

493
00:23:51,340 --> 00:23:53,709
and to determine who that individual is,

494
00:23:53,710 --> 00:23:55,910
can be a variety of things,

495
00:23:55,910 --> 00:23:59,370
including the user's
location, the time of day,

496
00:23:59,370 --> 00:24:02,949
but their risk reputation
and what we just talked about

497
00:24:02,950 --> 00:24:06,260
is a valuable component that
can go into the passport

498
00:24:06,260 --> 00:24:08,070
of understanding of whether or not

499
00:24:08,070 --> 00:24:10,590
we should be allowing users access

500
00:24:10,590 --> 00:24:12,790
to specific types of
data and permissioning.

501
00:24:12,790 --> 00:24:16,980
And that is a very
effective way of actioning

502
00:24:16,980 --> 00:24:19,180
what we know about employee risk,

503
00:24:19,180 --> 00:24:23,620
and helping us mitigate and
minimize the blast radius

504
00:24:23,620 --> 00:24:25,219
of risky decisions

505
00:24:25,220 --> 00:24:27,933
that an employee might be
able to make in the future.

506
00:24:31,530 --> 00:24:35,040
- So I'm gonna take it the next mile.

507
00:24:35,040 --> 00:24:38,399
So we talked a lot about
knowing your employee risk.

508
00:24:38,400 --> 00:24:40,680
And I wanna just give you a couple ideas

509
00:24:40,680 --> 00:24:42,280
and thought provoking things

510
00:24:42,280 --> 00:24:44,280
that you can take back
to your organization

511
00:24:44,280 --> 00:24:47,040
around knowing your data risk.

512
00:24:47,040 --> 00:24:51,005
So as Masha set the stage
for knowing employee risk,

513
00:24:51,006 --> 00:24:55,470
we also have to do the
combination of employee risk

514
00:24:55,470 --> 00:24:57,600
and employee risk indicators

515
00:24:57,600 --> 00:25:01,929
tied to our data at our
organization and data access.

516
00:25:01,930 --> 00:25:04,410
And this is where it all
kinda comes together.

517
00:25:04,410 --> 00:25:07,740
So first off, you all have
to evaluate the impacts

518
00:25:07,740 --> 00:25:09,090
that we talked about earlier

519
00:25:09,090 --> 00:25:11,470
in the presentation to your company.

520
00:25:11,470 --> 00:25:14,603
Are we all using more and
more collaboration software?

521
00:25:14,603 --> 00:25:17,630
Do you have a high workforce turnover?

522
00:25:17,630 --> 00:25:19,640
For most of us, we're certainly in a spot

523
00:25:19,640 --> 00:25:22,060
where we have more and
more remote workers.

524
00:25:22,060 --> 00:25:26,800
And we are all part of
this crazy 2020 year

525
00:25:26,800 --> 00:25:30,680
and into 2021 year, and we
all have increased stress

526
00:25:30,680 --> 00:25:32,390
throughout our organization.

527
00:25:32,390 --> 00:25:35,230
So once we evaluate all
of these impact factors

528
00:25:35,230 --> 00:25:37,670
on our organization, I'd encourage you all

529
00:25:37,670 --> 00:25:41,780
to evaluate your data
exfiltration visibility

530
00:25:41,780 --> 00:25:44,110
across the organization.

531
00:25:44,110 --> 00:25:47,620
So it's really important
that each of us understand

532
00:25:47,620 --> 00:25:49,969
where exfiltration points are

533
00:25:49,970 --> 00:25:52,960
and where we need additional monitoring

534
00:25:52,960 --> 00:25:55,120
and additional visibility.

535
00:25:55,120 --> 00:25:56,510
In one of our latest surveys,

536
00:25:56,510 --> 00:26:01,240
our respondents indicated that
the top exfiltration is email

537
00:26:01,240 --> 00:26:03,870
followed by printing,
external hard drives,

538
00:26:03,870 --> 00:26:06,030
that's not gonna be the
exact same for all of you.

539
00:26:06,030 --> 00:26:07,777
But getting a sense of like,

540
00:26:07,777 --> 00:26:11,950
"Where is important data
leaving my organization?",

541
00:26:11,950 --> 00:26:14,000
is a really important question to answer.

542
00:26:15,750 --> 00:26:18,660
The other thing that I
would challenge you on

543
00:26:18,660 --> 00:26:22,740
is what you tie back to
what Masha talked about,

544
00:26:22,740 --> 00:26:25,530
like what are our users
doing with your data?

545
00:26:25,530 --> 00:26:28,357
Are users using the
corporate storage locations

546
00:26:28,357 --> 00:26:30,000
that you've provided them?

547
00:26:30,000 --> 00:26:32,530
Or are they using something
else because they like it more,

548
00:26:32,530 --> 00:26:34,379
they like the UI more?

549
00:26:34,380 --> 00:26:37,020
In another one of our studies,

550
00:26:37,020 --> 00:26:42,020
we also found that 37% of
our of employees responded

551
00:26:42,490 --> 00:26:47,200
that they are using unsanctioned
cloud sharing technology.

552
00:26:47,200 --> 00:26:48,550
So they're essentially

553
00:26:48,550 --> 00:26:50,669
not using the corporate
sharing technology,

554
00:26:50,670 --> 00:26:52,780
but they're flipping to a different one.

555
00:26:52,780 --> 00:26:55,740
And the thing that we
all have to ask ourselves

556
00:26:55,740 --> 00:26:59,860
as security professionals is,
do we have visibility to that?

557
00:26:59,860 --> 00:27:01,810
And do we have the right monitoring

558
00:27:01,810 --> 00:27:06,070
to all those different extra
exfiltration locations,

559
00:27:06,070 --> 00:27:08,562
so that we can assess risk for ourselves?

560
00:27:11,770 --> 00:27:16,643
So as we talk about what's
next here, we covered a ton.

561
00:27:17,930 --> 00:27:20,950
But in summary, we wanna
emphasize the importance

562
00:27:20,950 --> 00:27:22,950
of first knowing your employee risk,

563
00:27:22,950 --> 00:27:25,240
and then also knowing your data risk.

564
00:27:25,240 --> 00:27:27,640
And we thought we would wrap this up

565
00:27:27,640 --> 00:27:31,930
with two really simple questions
for you to think through.

566
00:27:31,930 --> 00:27:33,490
And the first one is like

567
00:27:33,490 --> 00:27:36,100
what types of mistakes are
on the rise internally?

568
00:27:36,100 --> 00:27:38,831
And Masha talked a lot about this.

569
00:27:38,831 --> 00:27:42,330
What are the employees in
my organization doing today?

570
00:27:42,330 --> 00:27:44,949
And what are those non malicious mistakes

571
00:27:44,950 --> 00:27:46,320
that are happening?

572
00:27:46,320 --> 00:27:49,550
Somebody syncing their device
to iCloud unintentionally

573
00:27:49,550 --> 00:27:51,610
is a really great example of that.

574
00:27:51,610 --> 00:27:55,399
And then secondly, what
is my risk to data?

575
00:27:55,400 --> 00:27:59,020
Where do I actually have those
data exfiltration points?

576
00:27:59,020 --> 00:28:01,460
And do I have the right visibility

577
00:28:01,460 --> 00:28:04,323
and the right monitoring
on those different points?

578
00:28:05,560 --> 00:28:10,560
And so as we close, we
also want to leave you

579
00:28:12,510 --> 00:28:15,650
with a few things to think about

580
00:28:15,650 --> 00:28:18,480
related to employee risk and data risk.

581
00:28:18,480 --> 00:28:21,090
And these are almost considerations

582
00:28:21,090 --> 00:28:23,260
as you ask those first two questions,

583
00:28:23,260 --> 00:28:25,870
and you start to solve this problem.

584
00:28:25,870 --> 00:28:28,449
First, make sure that
you plan your investments

585
00:28:28,450 --> 00:28:31,700
in technology and programs
that really embrace

586
00:28:31,700 --> 00:28:34,660
where your employees want to work.

587
00:28:34,660 --> 00:28:37,110
I think we can probably all agree

588
00:28:37,110 --> 00:28:40,959
that this thing called
remote work is here to stay.

589
00:28:40,960 --> 00:28:43,910
And we all need to consider
how do we get visibility

590
00:28:43,910 --> 00:28:45,790
without what we used to have,

591
00:28:45,790 --> 00:28:49,360
without the office without
the network perimeter?

592
00:28:49,360 --> 00:28:51,449
Secondly, make sure you factor in

593
00:28:51,450 --> 00:28:54,420
how your employees want to work.

594
00:28:54,420 --> 00:28:58,060
We know that collaboration
solutions do pose a risk to data.

595
00:28:58,060 --> 00:29:01,980
But we also know that there
are really needed part

596
00:29:01,980 --> 00:29:03,940
of our business today.

597
00:29:03,940 --> 00:29:07,420
Our teams, my teams love
collaboration solutions,

598
00:29:07,420 --> 00:29:10,020
they drive efficiencies, innovations,

599
00:29:10,020 --> 00:29:13,020
they essentially help
get work done faster.

600
00:29:13,020 --> 00:29:16,020
And these solutions are
really here to stay.

601
00:29:16,020 --> 00:29:18,470
So we need to make sure
that we plan investments,

602
00:29:18,470 --> 00:29:21,450
not to block these
collaboration technologies,

603
00:29:21,450 --> 00:29:23,610
but instead allow the security team

604
00:29:23,610 --> 00:29:26,530
to have visibility to these solutions.

605
00:29:26,530 --> 00:29:28,920
One staggering statistic that I'd share

606
00:29:28,920 --> 00:29:32,150
is that 51% of security leaders

607
00:29:32,150 --> 00:29:34,530
received daily or weekly complaints

608
00:29:34,530 --> 00:29:37,290
about blocking legitimate work.

609
00:29:37,290 --> 00:29:40,450
I don't want to be one
of those security teams

610
00:29:40,450 --> 00:29:43,040
like I definitely wanna enable my team

611
00:29:43,040 --> 00:29:44,960
to do the work that they need to do

612
00:29:44,960 --> 00:29:47,350
so that I don't have
employees go around me

613
00:29:47,350 --> 00:29:48,837
and I can still have the visibility

614
00:29:48,837 --> 00:29:50,493
and the monitoring that I need.

615
00:29:51,980 --> 00:29:54,600
- The last point that I
wanted to share with you all

616
00:29:54,600 --> 00:29:59,480
is three thinking Insider
Risk as this big headline

617
00:29:59,480 --> 00:30:01,760
that is always a lawsuit.

618
00:30:01,760 --> 00:30:04,470
Insider Risk can show
up in our organizations

619
00:30:04,470 --> 00:30:08,670
as 1000 small decisions that
our employees are making

620
00:30:08,670 --> 00:30:10,900
or data leaving our network.

621
00:30:10,900 --> 00:30:14,540
Each of those can be lessons that teach us

622
00:30:14,540 --> 00:30:18,210
how we defend ourselves
against a much larger event.

623
00:30:18,210 --> 00:30:21,910
And as we talked about with
the Code42 example, or Twitter,

624
00:30:21,910 --> 00:30:25,800
learning from access to
certain types of permissions,

625
00:30:25,800 --> 00:30:28,810
like deleting accounts
without a secondary control,

626
00:30:28,810 --> 00:30:32,860
each of those instead of
being seen as failures,

627
00:30:32,860 --> 00:30:36,929
use those for opportunities
to apply small changes

628
00:30:36,930 --> 00:30:40,160
and evolve your security
posture of your organization

629
00:30:40,160 --> 00:30:43,580
to be able to withstand
something much larger,

630
00:30:43,580 --> 00:30:48,280
and encourage every part
of your security team

631
00:30:48,280 --> 00:30:51,389
to take these as learning opportunities

632
00:30:51,390 --> 00:30:53,640
and evolve as a security team

633
00:30:53,640 --> 00:30:57,320
because that's how we get better.

634
00:30:57,320 --> 00:31:00,480
We don't plan for the the
future theoretical thing,

635
00:31:00,480 --> 00:31:02,960
we learn about the things
that are happening today

636
00:31:02,960 --> 00:31:05,920
and help us to get to where
we actually want to go

637
00:31:05,920 --> 00:31:06,840
in the future.

638
00:31:06,840 --> 00:31:09,520
And with that, we're looking
forward to seeing you all

639
00:31:09,520 --> 00:31:12,173
in our interactive session for questions.

