1
00:00:00,290 --> 00:00:01,980
- Good afternoon, my name is Brett Tucker

2
00:00:01,980 --> 00:00:05,430
and I am a technical manager
of cyber risk management

3
00:00:05,430 --> 00:00:07,050
at the Software Engineering Institute

4
00:00:07,050 --> 00:00:09,120
within Carnegie Mellon University.

5
00:00:09,120 --> 00:00:10,350
Today, I'm gonna talk to you a little bit

6
00:00:10,350 --> 00:00:13,930
about artificial intelligence
and risks related to it.

7
00:00:13,930 --> 00:00:16,120
Name of my presentation is
"Really a New Mouse Trap?

8
00:00:16,120 --> 00:00:19,330
Exploring Risks with
Artificial Intelligence."

9
00:00:19,330 --> 00:00:23,307
This title is not an error.

10
00:00:23,307 --> 00:00:26,930
I'd like to make an argument or a case

11
00:00:26,930 --> 00:00:28,840
now and toward the end of the presentation

12
00:00:28,840 --> 00:00:32,409
to bring up the idea
that AI is a technology,

13
00:00:32,409 --> 00:00:35,379
it's great, it is exciting,
it's been around for a while,

14
00:00:35,380 --> 00:00:38,760
and it's kinda coming
back again to harness

15
00:00:38,760 --> 00:00:41,530
as our technology and
our savvy gets better.

16
00:00:41,530 --> 00:00:43,360
But we should really be
treating it just as much

17
00:00:43,360 --> 00:00:46,540
as we treat any other technology risks,

18
00:00:46,540 --> 00:00:49,839
new technologies that we
bring into organizations.

19
00:00:49,840 --> 00:00:51,430
So let's kind of go through that

20
00:00:51,430 --> 00:00:55,240
and talk about what we think about risk

21
00:00:55,240 --> 00:00:57,740
and how we would apply it to this new,

22
00:00:57,740 --> 00:01:00,793
great technology as it's
coming more into its own.

23
00:01:01,640 --> 00:01:05,099
First, let's level set
the entire audience here

24
00:01:05,099 --> 00:01:06,509
as to what I mean by risk.

25
00:01:06,510 --> 00:01:08,250
And there's a lot to be discussed here,

26
00:01:08,250 --> 00:01:09,620
but let's just start with the idea

27
00:01:09,620 --> 00:01:13,110
that risk is nothing more
than an uncertainty, right?

28
00:01:13,110 --> 00:01:15,210
We live in times of uncertainty.

29
00:01:15,210 --> 00:01:17,910
And I gotta tell ya, I'm
gonna break your bubble here,

30
00:01:17,910 --> 00:01:20,860
that the risk environment
is really not contracting.

31
00:01:20,860 --> 00:01:24,160
If anything, we are seeing a
lot more of this uncertainty

32
00:01:24,160 --> 00:01:28,140
come about as we have these
grand shifts in technology

33
00:01:28,140 --> 00:01:32,020
and as we have things like
pandemics that take place, right?

34
00:01:32,020 --> 00:01:35,830
We recognize that we're
living in this environment

35
00:01:35,830 --> 00:01:39,240
that's ever shifting and we
really need to learn to adapt.

36
00:01:39,240 --> 00:01:41,960
We need to learn to
survive that uncertainty.

37
00:01:41,960 --> 00:01:45,130
And we do that through a process,

38
00:01:45,130 --> 00:01:47,429
and hopefully as I go through this

39
00:01:47,430 --> 00:01:49,720
you'll see a more standardized process

40
00:01:49,720 --> 00:01:52,220
so that way we can make better decisions.

41
00:01:52,220 --> 00:01:53,620
Ultimately, that's the goal.

42
00:01:53,620 --> 00:01:56,030
We wanna make risk-informed
decisions, right?

43
00:01:56,030 --> 00:01:58,440
And that awareness is gonna take us there.

44
00:01:58,440 --> 00:02:00,539
Now, I wanna say one
other thing about risk

45
00:02:00,540 --> 00:02:01,840
that I want you to keep in mind

46
00:02:01,840 --> 00:02:04,270
because AI is a great technology.

47
00:02:04,270 --> 00:02:06,210
It has lots of opportunity to it.

48
00:02:06,210 --> 00:02:09,669
So remember that uncertainty
can be linked to a threat

49
00:02:09,669 --> 00:02:10,979
just as much as an opportunity.

50
00:02:10,979 --> 00:02:13,399
Things that can impact
our enterprise negatively

51
00:02:13,400 --> 00:02:15,490
just as much as things that are uncertain

52
00:02:15,490 --> 00:02:17,120
that can affect us positively

53
00:02:17,120 --> 00:02:20,800
and give us some kind of
uptick for our organization.

54
00:02:20,800 --> 00:02:22,270
So at the end of the day,

55
00:02:22,270 --> 00:02:24,870
what we really can only account for here

56
00:02:24,870 --> 00:02:27,800
is the fact that uncertainty exists.

57
00:02:27,800 --> 00:02:30,140
But the good news is
that we can decompose it

58
00:02:30,140 --> 00:02:33,970
and we can think about risk
in a lot of different ways,

59
00:02:33,970 --> 00:02:35,940
the way we define it, right?

60
00:02:35,940 --> 00:02:39,109
But for today, let's just argue
that risk is an uncertainty

61
00:02:39,110 --> 00:02:41,650
that's gonna result in
some kind of consequence.

62
00:02:41,650 --> 00:02:43,130
It's gonna lead to an impact,

63
00:02:43,130 --> 00:02:46,950
a way that we feel pain,
if you will, right?

64
00:02:46,950 --> 00:02:50,470
So that risk or that,
excuse me, that consequence,

65
00:02:50,470 --> 00:02:52,540
that impact is gonna come about

66
00:02:52,540 --> 00:02:55,590
from some threat, some actor.

67
00:02:55,590 --> 00:02:56,860
Now, by the way it doesn't necessarily

68
00:02:56,860 --> 00:02:58,500
have to be a human being as an actor.

69
00:02:58,500 --> 00:02:59,670
We recognize there could be

70
00:02:59,670 --> 00:03:02,530
a force majeure kind of
event that could take place,

71
00:03:02,530 --> 00:03:05,250
like a tornado, a hurricane, a pandemic,

72
00:03:05,250 --> 00:03:07,380
something like that
that's gonna come along

73
00:03:07,380 --> 00:03:09,650
and it could be external
to our organization.

74
00:03:09,650 --> 00:03:11,340
Or it could be like an insider threat,

75
00:03:11,340 --> 00:03:14,960
somebody who's the organization
who is acting maliciously.

76
00:03:14,960 --> 00:03:17,440
Or it could be somebody
who is just maybe ignorant

77
00:03:17,440 --> 00:03:18,590
and makes a mistake.

78
00:03:18,590 --> 00:03:19,870
We all make mistakes.

79
00:03:19,870 --> 00:03:21,483
It could be the case, right?

80
00:03:22,340 --> 00:03:24,670
And these risks, they happen
within an environment.

81
00:03:24,670 --> 00:03:26,600
Some kind of condition has to exist.

82
00:03:26,600 --> 00:03:28,900
Now in cyberspace, we
all know this in terms

83
00:03:28,900 --> 00:03:30,020
of being a vulnerability,

84
00:03:30,020 --> 00:03:32,140
and those of us who are familiar

85
00:03:32,140 --> 00:03:34,160
with the recent SolarWinds
event and all that,

86
00:03:34,160 --> 00:03:36,320
we've found that there
are grand vulnerabilities

87
00:03:36,320 --> 00:03:38,540
to be found in our
systems that we're using.

88
00:03:38,540 --> 00:03:40,690
Our assets are plagued with them.

89
00:03:40,690 --> 00:03:44,740
So these conditions exist
and we must address them.

90
00:03:44,740 --> 00:03:46,080
We must identify them.

91
00:03:46,080 --> 00:03:48,830
And if we can take that wheel or that cog

92
00:03:48,830 --> 00:03:50,560
out of this mesh here,

93
00:03:50,560 --> 00:03:53,060
we can find that we could
really kind of limit

94
00:03:53,060 --> 00:03:55,790
or mitigate our exposure to that risk

95
00:03:55,790 --> 00:03:57,359
if we were to eliminate all threats,

96
00:03:57,360 --> 00:04:00,030
educate our people so
that there's no such thing

97
00:04:00,030 --> 00:04:02,370
as a mistake that can be
made in our enterprise.

98
00:04:02,370 --> 00:04:06,650
Or we could, as another
example, dial away all impact.

99
00:04:06,650 --> 00:04:09,290
Maybe we make it so that we
don't necessarily feel pain

100
00:04:09,290 --> 00:04:11,500
if a risk does come to fruition.

101
00:04:11,500 --> 00:04:14,617
Either way, we're gonna
treat these conditions

102
00:04:14,617 --> 00:04:17,050
and these elements of risk

103
00:04:17,050 --> 00:04:18,810
such that we can respond to them

104
00:04:18,810 --> 00:04:21,810
and eliminate or reduce, actually.

105
00:04:21,810 --> 00:04:23,610
We'll never completely
eliminate risk, right?

106
00:04:23,610 --> 00:04:27,140
There will always be residual,
but we reduce that exposure.

107
00:04:27,140 --> 00:04:29,060
And remember, I said that the good news

108
00:04:29,060 --> 00:04:32,530
is that we can address these
risks in a standardized manner.

109
00:04:32,530 --> 00:04:36,710
That said, this is a
very, very general process

110
00:04:36,710 --> 00:04:38,190
for addressing any risk.

111
00:04:38,190 --> 00:04:40,870
I'd like to point you,
I have a reference slide

112
00:04:40,870 --> 00:04:42,570
at the very end of this
for those interested,

113
00:04:42,570 --> 00:04:45,860
that I just wrote a publication,

114
00:04:45,860 --> 00:04:48,110
a technical note for OCTAVE FORTE,

115
00:04:48,110 --> 00:04:50,710
which is as standardized
risk management process

116
00:04:50,710 --> 00:04:53,919
for enterprise risk and
organization connecting CISOs

117
00:04:53,920 --> 00:04:56,840
to enterprise risk in a better sense.

118
00:04:56,840 --> 00:04:58,299
But there are other
frameworks that can be used.

119
00:04:58,300 --> 00:04:59,430
Risk management framework.

120
00:04:59,430 --> 00:05:01,050
FAIR has a great process.

121
00:05:01,050 --> 00:05:03,660
So whatever you decide
to use, you're gonna find

122
00:05:03,660 --> 00:05:06,160
that they all have roughly
these same elements, right?

123
00:05:06,160 --> 00:05:07,480
We're gonna plan for the plan,

124
00:05:07,480 --> 00:05:10,830
we're gonna identify our risks,
we're gonna analyze them.

125
00:05:10,830 --> 00:05:14,640
So we're gonna assume that
this process has been done

126
00:05:14,640 --> 00:05:16,240
with artificial intelligence today

127
00:05:16,240 --> 00:05:17,910
as we're gonna talk about it, right?

128
00:05:17,910 --> 00:05:20,420
But I can't say more,

129
00:05:20,420 --> 00:05:22,140
I can't provide any more greater emphasis

130
00:05:22,140 --> 00:05:25,210
that you wanna attack this
problem in a standardized manner

131
00:05:25,210 --> 00:05:27,690
much like you would any other
risk in your enterprise.

132
00:05:27,690 --> 00:05:31,130
Sure, AI may be special,
it's got great benefits,

133
00:05:31,130 --> 00:05:33,180
or maybe it's something
that your organization

134
00:05:33,180 --> 00:05:35,770
has never seen before,
but you must approach it

135
00:05:35,770 --> 00:05:38,370
as you would have all of your other risks.

136
00:05:38,370 --> 00:05:39,203
Okay.

137
00:05:40,650 --> 00:05:42,221
So let's talk a little bit more about

138
00:05:42,221 --> 00:05:44,130
the matter at hand, right?

139
00:05:44,130 --> 00:05:47,060
The technology piece, the
artificial intelligence side.

140
00:05:47,060 --> 00:05:49,220
Now, we have lots of great experts

141
00:05:49,220 --> 00:05:50,270
at the Software Engineer Institute

142
00:05:50,270 --> 00:05:52,010
that specialize in
artificial intelligence,

143
00:05:52,010 --> 00:05:54,890
and I am not necessarily one of them.

144
00:05:54,890 --> 00:05:58,330
So I don't lay claim that
I am risk expert, though.

145
00:05:58,330 --> 00:06:01,130
And I would argue that as
I've gone down this path,

146
00:06:01,130 --> 00:06:02,810
I've learned a little bit.

147
00:06:02,810 --> 00:06:04,120
The fact that it's a new technology

148
00:06:04,120 --> 00:06:06,120
obviously that's an obvious one, right?

149
00:06:06,120 --> 00:06:08,430
But I have learned for sure that AI,

150
00:06:08,430 --> 00:06:11,120
much like other technologies
that we've seen in the past,

151
00:06:11,120 --> 00:06:14,460
are truly here to be a
market disruptor, right?

152
00:06:14,460 --> 00:06:15,940
And maybe not even a market disruptor,

153
00:06:15,940 --> 00:06:17,830
it could be a geopolitical disruptor.

154
00:06:17,830 --> 00:06:21,800
I mean, think about militaries
using this kind of technology

155
00:06:21,800 --> 00:06:24,340
in application on the
battlefield for example,

156
00:06:24,340 --> 00:06:26,880
which could be a very significant concern.

157
00:06:26,880 --> 00:06:29,530
So we know that we have
to address these risks

158
00:06:29,530 --> 00:06:31,280
not only in terms of how it's implemented

159
00:06:31,280 --> 00:06:33,700
in the organization, how it's being used,

160
00:06:33,700 --> 00:06:35,610
but also how we're ingesting this

161
00:06:35,610 --> 00:06:38,580
as a new asset into the
organization, right?

162
00:06:38,580 --> 00:06:40,469
Once again, let's revisit this idea

163
00:06:40,470 --> 00:06:42,400
that if we can dial away the impact,

164
00:06:42,400 --> 00:06:44,719
if we can make it so that
we don't feel the pain,

165
00:06:44,720 --> 00:06:46,960
or maybe we eliminate the threat actor,

166
00:06:46,960 --> 00:06:48,890
or maybe at some point we can control

167
00:06:48,890 --> 00:06:50,539
the fact that we have no vulnerabilities,

168
00:06:50,540 --> 00:06:52,160
or we've controlled the condition,

169
00:06:52,160 --> 00:06:54,657
that we've overall reduced
that risk exposure.

170
00:06:54,657 --> 00:06:59,039
And that's what we're shooting
for in this regard, okay?

171
00:06:59,040 --> 00:07:02,810
So there are a couple ways
that I've sliced and diced

172
00:07:02,810 --> 00:07:06,630
artificial intelligence and
thought of some high-level risks

173
00:07:06,630 --> 00:07:09,330
that you may want to
address in your registers,

174
00:07:09,330 --> 00:07:11,520
your risk register, right?

175
00:07:11,520 --> 00:07:14,460
I'm gonna go through each
of these at greater detail,

176
00:07:14,460 --> 00:07:16,570
but I want you to know that these items

177
00:07:16,570 --> 00:07:19,469
that I have listed here, an
ill-defined problem statement,

178
00:07:20,410 --> 00:07:22,850
maybe there's a lack of
expertise in the organization,

179
00:07:22,850 --> 00:07:25,250
and then maybe there's
challenges with the model

180
00:07:25,250 --> 00:07:28,480
and the way that we have the the system

181
00:07:28,480 --> 00:07:31,690
managing the data within that model.

182
00:07:31,690 --> 00:07:34,700
Maybe we have unrealistic
expectations from stakeholders.

183
00:07:34,700 --> 00:07:37,610
And once again, there's
challenges with data,

184
00:07:37,610 --> 00:07:39,920
lack of the ability to verify the model.

185
00:07:39,920 --> 00:07:41,360
Whatever the case may be,

186
00:07:41,360 --> 00:07:44,600
these are all connected in a sense, right?

187
00:07:44,600 --> 00:07:46,890
You don't want to deny
yourself the ability

188
00:07:46,890 --> 00:07:49,250
to identify the interdependency

189
00:07:49,250 --> 00:07:51,170
between each of these elements.

190
00:07:51,170 --> 00:07:54,740
And understand that as
I'm addressing one risk,

191
00:07:54,740 --> 00:07:57,600
there may be benefit that I'm
addressing multiple others.

192
00:07:57,600 --> 00:07:59,230
So you may hear things repeated,

193
00:07:59,230 --> 00:08:01,100
but that's important that you're hearing

194
00:08:01,100 --> 00:08:03,490
solutions being repeated
from one risk to another

195
00:08:03,490 --> 00:08:05,932
because there's gonna be an efficiency

196
00:08:05,932 --> 00:08:08,873
or a savings on the economy of scale here.

197
00:08:09,900 --> 00:08:11,109
Okay.

198
00:08:11,110 --> 00:08:13,870
So let's go into the details here

199
00:08:13,870 --> 00:08:15,210
and talk about each of these.

200
00:08:15,210 --> 00:08:17,799
Let's talk about an
ill-defined problem statement.

201
00:08:17,800 --> 00:08:22,590
And I love to think about
this in terms of the pandemic

202
00:08:22,590 --> 00:08:24,239
that we're just recently suffering with,

203
00:08:24,240 --> 00:08:25,860
COVID for example, right?

204
00:08:25,860 --> 00:08:28,710
Nobody really would have thought in 2019

205
00:08:28,710 --> 00:08:31,770
that there would be a pandemic
that would be so profound

206
00:08:31,770 --> 00:08:33,949
as to bring our economy to a halt.

207
00:08:33,950 --> 00:08:38,523
And I would challenge any of
you that it would be really,

208
00:08:41,010 --> 00:08:44,010
I don't know, a shift of your paradigm

209
00:08:44,010 --> 00:08:47,970
or a disruptor for you
to start thinking about,

210
00:08:47,970 --> 00:08:50,780
well, how does that pandemic
play in my organization?

211
00:08:50,780 --> 00:08:52,560
I mean, have I thought
of all the elements?

212
00:08:52,560 --> 00:08:54,099
Have I thought about
how other organizations

213
00:08:54,100 --> 00:08:56,100
interface with me and
how they're impacted?

214
00:08:56,100 --> 00:08:58,810
So now you have a supply
chain risk management issue.

215
00:08:58,810 --> 00:09:00,069
All this goes to the idea

216
00:09:00,070 --> 00:09:02,170
of how you're defining
your problem statement.

217
00:09:02,170 --> 00:09:03,500
It's not just the pandemic.

218
00:09:03,500 --> 00:09:05,407
There are other little
pieces to it, right?

219
00:09:05,407 --> 00:09:09,570
And artificial intelligence
is no different, right?

220
00:09:09,570 --> 00:09:11,290
Once again, you have this matter

221
00:09:11,290 --> 00:09:12,689
where you're ingesting this asset,

222
00:09:12,690 --> 00:09:15,090
this technology asset
into your organization.

223
00:09:15,090 --> 00:09:16,930
And as you bring this technology in,

224
00:09:16,930 --> 00:09:20,359
you have a provider, some
kind of supply chain element,

225
00:09:20,360 --> 00:09:23,060
that is going to be helping
you with that implementation.

226
00:09:23,060 --> 00:09:25,449
Maybe you're buying off the shelf product.

227
00:09:25,450 --> 00:09:26,770
Maybe you have someone that's actually

228
00:09:26,770 --> 00:09:27,650
developing it for you,

229
00:09:27,650 --> 00:09:29,600
or maybe you're developing it in-house.

230
00:09:30,590 --> 00:09:33,660
That said, whoever it is
that's helping you do this,

231
00:09:33,660 --> 00:09:35,449
whether it's internal or external,

232
00:09:35,450 --> 00:09:37,460
they need to understand
what the problem is

233
00:09:37,460 --> 00:09:40,000
that you're trying to address
with this technology, right?

234
00:09:40,000 --> 00:09:41,500
They need to get into your head

235
00:09:41,500 --> 00:09:45,250
and do a deep, deep search of
requirements exploration here

236
00:09:45,250 --> 00:09:47,180
to understand where you're going with it

237
00:09:47,180 --> 00:09:48,699
and what you wanna get out of it, right?

238
00:09:48,700 --> 00:09:53,400
And this all comes down to
great requirements exploration.

239
00:09:53,400 --> 00:09:56,750
Let's face it too, if you
don't understand a problem

240
00:09:56,750 --> 00:09:59,270
in its entirety, it's always best

241
00:09:59,270 --> 00:10:02,000
to decompose it, chunk it up, right?

242
00:10:02,000 --> 00:10:03,700
If we put it into little parts

243
00:10:03,700 --> 00:10:05,120
and we address each little part,

244
00:10:05,120 --> 00:10:07,750
and maybe use AI as just one element

245
00:10:07,750 --> 00:10:09,840
of a solution for one part,

246
00:10:09,840 --> 00:10:12,300
we've effectively diluted that risk

247
00:10:12,300 --> 00:10:15,683
that may be rooted in
the whole process, right?

248
00:10:16,590 --> 00:10:18,620
In other words, if we don't use AI

249
00:10:18,620 --> 00:10:21,300
in other elements of this
problem that we've decomposed,

250
00:10:21,300 --> 00:10:23,150
if this one part goes wrong,

251
00:10:23,150 --> 00:10:25,790
maybe it won't end our whole reality

252
00:10:25,790 --> 00:10:28,150
or our whole existence of
our organization, right?

253
00:10:28,150 --> 00:10:29,970
It won't be as threatening.

254
00:10:29,970 --> 00:10:34,160
So we wanna bring it in
in a migratory sense.

255
00:10:34,160 --> 00:10:35,380
And that brings me to this idea

256
00:10:35,380 --> 00:10:37,350
that you may also wanna consider

257
00:10:37,350 --> 00:10:39,850
an agile implementation
of some sort, right?

258
00:10:39,850 --> 00:10:42,440
Let's take it a step
at a time, if you will.

259
00:10:42,440 --> 00:10:45,110
And let's bring in pieces and parts

260
00:10:45,110 --> 00:10:46,540
and understand how those pieces and parts

261
00:10:46,540 --> 00:10:48,819
are interacting in our organization.

262
00:10:48,820 --> 00:10:52,500
And that way we can really
kinda get on top of this idea

263
00:10:52,500 --> 00:10:55,210
that we're understanding the
problem as we get smarter

264
00:10:55,210 --> 00:10:57,440
about its implementation piece by piece

265
00:10:57,440 --> 00:10:59,603
with this agile implementation.

266
00:11:02,420 --> 00:11:04,120
Okay, and we all know this.

267
00:11:04,120 --> 00:11:06,090
You guys are all sitting in this audience,

268
00:11:06,090 --> 00:11:07,080
whether you're at home,

269
00:11:07,080 --> 00:11:09,140
or maybe you're with a group of folks,

270
00:11:09,140 --> 00:11:11,920
and you know that you've come

271
00:11:11,920 --> 00:11:13,930
from a very rigored background.

272
00:11:13,930 --> 00:11:16,939
All of us have in this in
this particular path, right?

273
00:11:16,940 --> 00:11:18,510
Whether you're homegrown

274
00:11:18,510 --> 00:11:20,530
and you learned to be a
hacker in your basement,

275
00:11:20,530 --> 00:11:21,770
you still put in the effort

276
00:11:21,770 --> 00:11:24,110
and it took a lot to
learn that technology.

277
00:11:24,110 --> 00:11:27,320
Or whether you've gone
through those advanced degrees

278
00:11:27,320 --> 00:11:31,200
and you have just a
profound body of expertise

279
00:11:31,200 --> 00:11:33,510
for elements of cybersecurity

280
00:11:33,510 --> 00:11:36,280
and maybe even artificial intelligence.

281
00:11:36,280 --> 00:11:38,420
That pedigree was not easy.

282
00:11:38,420 --> 00:11:40,770
It was not easy to achieve, right?

283
00:11:40,770 --> 00:11:43,630
So as you get that, you understand too

284
00:11:43,630 --> 00:11:45,290
that this is a team sport

285
00:11:45,290 --> 00:11:48,089
and you cannot be the only
person in the organization

286
00:11:48,090 --> 00:11:49,730
that's doing this work.

287
00:11:49,730 --> 00:11:52,640
If you are, by the way,
that's a risk in and of itself

288
00:11:52,640 --> 00:11:53,830
'cause you're linchpin, right?

289
00:11:53,830 --> 00:11:55,130
If something happens to you,

290
00:11:55,130 --> 00:11:57,510
the organization's gonna
come to a screeching halt.

291
00:11:57,510 --> 00:12:00,330
So we understand that we need more people

292
00:12:00,330 --> 00:12:01,790
with the skills and abilities

293
00:12:01,790 --> 00:12:04,420
that are related to this
artificial intelligence

294
00:12:04,420 --> 00:12:05,930
as a tool, right?

295
00:12:05,930 --> 00:12:08,760
And I mean, think about all
the pieces that go with that.

296
00:12:08,760 --> 00:12:13,230
I mean, we have people who have
expertise in data collection

297
00:12:13,230 --> 00:12:16,170
and system engineering
and model development

298
00:12:16,170 --> 00:12:17,400
and even just problem definition, right?

299
00:12:17,400 --> 00:12:19,100
You need great mathematicians.

300
00:12:19,100 --> 00:12:20,500
You need people who understand

301
00:12:20,500 --> 00:12:22,570
how to decompose these things.

302
00:12:22,570 --> 00:12:24,650
And they're not easy to find.

303
00:12:24,650 --> 00:12:27,209
Okay, so how do we address this, right?

304
00:12:27,210 --> 00:12:28,690
What are we gonna do?

305
00:12:28,690 --> 00:12:31,840
I would argue that all organizations

306
00:12:31,840 --> 00:12:36,840
should have a strong
human resource strategy

307
00:12:37,070 --> 00:12:39,650
in terms of how you do recruiting,

308
00:12:39,650 --> 00:12:42,030
where are you doing the recruiting from.

309
00:12:42,030 --> 00:12:42,863
And you're going to say,

310
00:12:42,863 --> 00:12:44,813
"Well, Brett, this is not
really artificial intelligence.

311
00:12:44,813 --> 00:12:46,350
This is all risks."

312
00:12:46,350 --> 00:12:49,560
Aha, once again, this
is one of those elements

313
00:12:49,560 --> 00:12:51,160
where it's broadly applicable

314
00:12:51,160 --> 00:12:53,230
across many different technology risks

315
00:12:53,230 --> 00:12:54,660
or risks in general, right?

316
00:12:54,660 --> 00:12:56,699
All organizations struggle with this.

317
00:12:56,700 --> 00:12:58,250
But it's very profound, obviously,

318
00:12:58,250 --> 00:12:59,360
with artificial intelligence

319
00:12:59,360 --> 00:13:01,660
because there are high demands here

320
00:13:01,660 --> 00:13:03,640
and you're gonna have to consider

321
00:13:03,640 --> 00:13:05,680
what kind of resources are
you gonna bring to the table

322
00:13:05,680 --> 00:13:07,949
to hire these people that bring

323
00:13:07,950 --> 00:13:10,660
the special expertise to the table.

324
00:13:10,660 --> 00:13:14,199
So that said, do not deny the fact

325
00:13:14,200 --> 00:13:16,030
that you have people
within your organization

326
00:13:16,030 --> 00:13:18,220
who may be thirsting to understand

327
00:13:18,220 --> 00:13:19,510
artificial intelligence more,

328
00:13:19,510 --> 00:13:21,950
who are looking for those
additional opportunities.

329
00:13:21,950 --> 00:13:23,610
And there's benefit in that, right?

330
00:13:23,610 --> 00:13:26,210
Rather than going to the
street and bringing somebody in

331
00:13:26,210 --> 00:13:28,340
and educating them on your organization

332
00:13:28,340 --> 00:13:29,390
and getting them oriented

333
00:13:29,390 --> 00:13:31,680
with your culture and your mission,

334
00:13:31,680 --> 00:13:34,400
try to find people in
house that you can educate

335
00:13:34,400 --> 00:13:38,240
and provide that little boost of help.

336
00:13:38,240 --> 00:13:40,020
Maybe you send them to an RSA conference,

337
00:13:40,020 --> 00:13:43,930
maybe you send them to school,
and have that intellect

338
00:13:43,930 --> 00:13:47,920
and have that skillset fostered in-house.

339
00:13:47,920 --> 00:13:50,010
But it takes time. This
is a long game, right?

340
00:13:50,010 --> 00:13:51,069
It's strategic.

341
00:13:51,070 --> 00:13:53,140
So you can't just go
tactically making decisions

342
00:13:53,140 --> 00:13:54,880
about educating the workforce.

343
00:13:54,880 --> 00:13:57,160
It takes thought. It
takes time and energy.

344
00:13:57,160 --> 00:13:59,709
This is a big one to be
putting time and energy to.

345
00:14:01,410 --> 00:14:04,110
Okay, so let's talk about customers now,

346
00:14:04,110 --> 00:14:07,020
'cause we've talked about
we think we had a problem

347
00:14:07,020 --> 00:14:08,510
and we know that we have people

348
00:14:08,510 --> 00:14:11,660
that can address that problem, we believe,

349
00:14:11,660 --> 00:14:14,110
and at the same time, we know
that there's gonna be somebody

350
00:14:14,110 --> 00:14:15,800
who's paying the bills, right?

351
00:14:15,800 --> 00:14:16,890
And let's face it,

352
00:14:16,890 --> 00:14:18,970
anyone who's worked on
any kind of project before

353
00:14:18,970 --> 00:14:20,850
knows that customer expectations

354
00:14:20,850 --> 00:14:23,053
can be challenging at times, right?

355
00:14:24,380 --> 00:14:28,610
There's a lot of talk about
documenting requirements

356
00:14:28,610 --> 00:14:30,940
and you put it in a nice, special tool.

357
00:14:30,940 --> 00:14:34,860
You actually even time off to
the function that you have.

358
00:14:34,860 --> 00:14:35,900
And this is going back

359
00:14:35,900 --> 00:14:38,550
to that agile development piece, right?

360
00:14:38,550 --> 00:14:42,500
We address each requirement
one element at a time,

361
00:14:42,500 --> 00:14:44,940
and yet you run into this challenge here

362
00:14:44,940 --> 00:14:47,340
where the customer's not happy.

363
00:14:47,340 --> 00:14:49,080
I mean, I like to think of this actually

364
00:14:49,080 --> 00:14:50,690
in terms of New Coca-Cola.

365
00:14:50,690 --> 00:14:52,323
Remember anybody from back in the eighties

366
00:14:52,323 --> 00:14:54,410
when we had New Coke, right?

367
00:14:54,410 --> 00:14:56,870
And Coca-Cola came out
with this great product,

368
00:14:56,870 --> 00:14:58,850
they were modifying their recipe,

369
00:14:58,850 --> 00:15:01,950
and it was a complete
and utter failure, right?

370
00:15:01,950 --> 00:15:05,350
And it was because the customers
had different expectations.

371
00:15:05,350 --> 00:15:07,050
They had a product in mind

372
00:15:07,050 --> 00:15:11,030
that they wanted to be
satisfied not only in taste,

373
00:15:11,030 --> 00:15:14,449
but it had to have that
certain je ne sais quoi to it,

374
00:15:14,450 --> 00:15:17,110
that certain element of this is Coca-Cola

375
00:15:17,110 --> 00:15:18,910
in my mind as I'm tasting it.

376
00:15:18,910 --> 00:15:20,350
And they didn't make that connection.

377
00:15:20,350 --> 00:15:21,230
And we could talk about that

378
00:15:21,230 --> 00:15:23,570
in not just the soft drink industry.

379
00:15:23,570 --> 00:15:25,270
I think of Crystal Pepsi

380
00:15:25,270 --> 00:15:28,130
and all those great products
that have gone to pass.

381
00:15:28,130 --> 00:15:29,160
But we can think about this

382
00:15:29,160 --> 00:15:31,360
in other industries as well, right?

383
00:15:31,360 --> 00:15:33,390
Where the customer set came in

384
00:15:33,390 --> 00:15:35,240
and they had these great ideas

385
00:15:35,240 --> 00:15:37,690
of things that they wanted
and just didn't get it.

386
00:15:37,690 --> 00:15:40,160
Artificial intelligence
is no different, right?

387
00:15:40,160 --> 00:15:42,339
So we need to educate our customers,

388
00:15:42,340 --> 00:15:43,690
we need them to understand

389
00:15:43,690 --> 00:15:47,530
that the things that we're
having done with this technology,

390
00:15:47,530 --> 00:15:49,839
and that is decision-making, right?

391
00:15:49,840 --> 00:15:52,140
Automated decision-making
that's taking place

392
00:15:52,140 --> 00:15:54,360
at just a faster rate
than what humans would.

393
00:15:54,360 --> 00:15:56,820
And maybe even a little more standardized,

394
00:15:56,820 --> 00:16:01,820
maybe it's got the ability to
ingest data at greater rates

395
00:16:01,870 --> 00:16:05,280
faster than a human
could and make decisions.

396
00:16:05,280 --> 00:16:08,420
That all said, the customer
needs to understand

397
00:16:08,420 --> 00:16:10,979
that that's being done with technology

398
00:16:10,980 --> 00:16:13,280
that could be flawed, right?

399
00:16:13,280 --> 00:16:15,760
There are decisions that
still could be made improperly

400
00:16:15,760 --> 00:16:18,319
because maybe your model is flawed

401
00:16:18,320 --> 00:16:20,280
or maybe your data is poisoned.

402
00:16:20,280 --> 00:16:21,959
And a customer has to understand that

403
00:16:21,960 --> 00:16:24,867
as they're getting this
artificial intelligence technology

404
00:16:24,867 --> 00:16:26,319
and they're using it,

405
00:16:26,320 --> 00:16:31,070
that their expectations may
not necessarily be made, okay?

406
00:16:31,070 --> 00:16:34,650
So once again as we're
educating these customers

407
00:16:34,650 --> 00:16:39,030
and we understand, or
they understand better

408
00:16:39,030 --> 00:16:41,319
what they're getting, they
have to understand, too,

409
00:16:41,320 --> 00:16:43,600
that their risk appetite may be changing.

410
00:16:43,600 --> 00:16:44,560
Now, risk appetite,

411
00:16:44,560 --> 00:16:46,459
let's go back and visit that for a second,

412
00:16:46,460 --> 00:16:48,140
is the amount of risk
that we're willing to take

413
00:16:48,140 --> 00:16:50,030
into an organization, right?

414
00:16:50,030 --> 00:16:51,666
So you have to tell your customer,

415
00:16:51,667 --> 00:16:54,240
"Hey, you're bringing
in this new technology

416
00:16:54,240 --> 00:16:57,980
and it may make a mistake and
it may cost you X dollars.

417
00:16:57,980 --> 00:17:00,780
Or it may be a detriment

418
00:17:00,780 --> 00:17:03,050
to the safety of employees
in the organization."

419
00:17:03,050 --> 00:17:04,700
And right there, the
customer's put on notice.

420
00:17:04,700 --> 00:17:05,780
They're like, "Whoa, wait a second.

421
00:17:05,780 --> 00:17:06,787
How is that the case?"

422
00:17:06,787 --> 00:17:08,589
And you start talking
about, well, let's face it.

423
00:17:08,589 --> 00:17:11,429
We're making decisions on how
equipment is gonna function,

424
00:17:11,430 --> 00:17:15,430
and if that equipment should
happen to harm somebody

425
00:17:16,410 --> 00:17:19,180
or do damage to your
organization or your operations,

426
00:17:19,180 --> 00:17:20,520
you're gonna run into some problems

427
00:17:20,520 --> 00:17:23,233
and you need to understand
how that's going to happen.

428
00:17:24,160 --> 00:17:25,801
That might right there stop the idea

429
00:17:25,801 --> 00:17:28,810
of an appetite space of an organization

430
00:17:28,810 --> 00:17:31,710
wanting to even adopt AI
until it's better understood

431
00:17:31,710 --> 00:17:34,583
or until it's proven itself, okay?

432
00:17:36,780 --> 00:17:39,129
Okay, so now you've
convinced the customer,

433
00:17:39,130 --> 00:17:41,060
yes, AI is the thing you need.

434
00:17:41,060 --> 00:17:43,340
You got that team and they're educated

435
00:17:43,340 --> 00:17:45,340
and they know what they're doing, right?

436
00:17:45,340 --> 00:17:48,360
That said, you gotta recognize, too,

437
00:17:48,360 --> 00:17:51,260
that your environment and your
problem is always shifting.

438
00:17:51,260 --> 00:17:53,300
You may have defined it on one day

439
00:17:53,300 --> 00:17:56,649
but the next day it's
completely fallen apart.

440
00:17:56,650 --> 00:17:59,110
And this is, I think of the Mike Tyson,

441
00:17:59,110 --> 00:18:00,763
I think it's Mike Tyson who said,

442
00:18:02,071 --> 00:18:05,389
"A fight is great, and you
think you have a great plan

443
00:18:05,390 --> 00:18:07,080
until you get punched in the face."

444
00:18:07,080 --> 00:18:10,250
Was it Tyson? I don't
know, I'm not a big boxer.

445
00:18:10,250 --> 00:18:12,350
But let's face it, that's a truism

446
00:18:12,350 --> 00:18:14,310
for anything that you're working with here

447
00:18:14,310 --> 00:18:18,020
with respect to building
models around problems, right?

448
00:18:18,020 --> 00:18:19,300
We're trying to figure out

449
00:18:19,300 --> 00:18:22,980
how we can use a model within a system

450
00:18:22,980 --> 00:18:25,370
that's going to use data,
it's gonna ingest it,

451
00:18:25,370 --> 00:18:28,889
and it's going to make
decisions with that data,

452
00:18:28,890 --> 00:18:30,353
and it's gonna take action.

453
00:18:31,300 --> 00:18:34,520
And we have to understand that
if elements of that picture

454
00:18:34,520 --> 00:18:36,050
have changed within that model,

455
00:18:36,050 --> 00:18:37,780
and I like to think about,

456
00:18:37,780 --> 00:18:42,129
maybe let's think about a
battlefield for example.

457
00:18:42,130 --> 00:18:44,520
We're making decisions real time

458
00:18:44,520 --> 00:18:46,970
on how to employ assets in a battlefield.

459
00:18:46,970 --> 00:18:50,090
Geography can change, weather can change.

460
00:18:50,090 --> 00:18:52,320
The enemy may use different tactics.

461
00:18:52,320 --> 00:18:55,639
All these things may go to impacting

462
00:18:55,640 --> 00:18:56,960
how your model was designed

463
00:18:56,960 --> 00:19:00,700
and the solution that it was seeking.

464
00:19:00,700 --> 00:19:04,210
So all that said, your
risk is your uncertainty

465
00:19:04,210 --> 00:19:06,082
and the execution of that model

466
00:19:06,082 --> 00:19:09,780
is being challenged time and again.

467
00:19:09,780 --> 00:19:12,260
So once again, what do we
wanna think about here?

468
00:19:12,260 --> 00:19:14,879
Well, clearly requirements exploration

469
00:19:14,880 --> 00:19:16,150
was a big thing right?

470
00:19:16,150 --> 00:19:18,160
And by the way, it's not just the idea

471
00:19:18,160 --> 00:19:19,900
of requirements exploration upfront

472
00:19:19,900 --> 00:19:22,100
as you're bringing the
asset in the organization,

473
00:19:22,100 --> 00:19:24,110
but you realize that
there's a lifecycle to it

474
00:19:24,110 --> 00:19:27,409
along with the AI solution
throughout its entire life.

475
00:19:27,410 --> 00:19:30,170
Things are always shifting,
the environment is changing,

476
00:19:30,170 --> 00:19:32,010
and you need to have that

477
00:19:32,010 --> 00:19:35,420
requirements exploration
process to never end.

478
00:19:35,420 --> 00:19:38,670
It needs to be iterative with the life

479
00:19:38,670 --> 00:19:41,330
of this technology that
you're bringing in.

480
00:19:41,330 --> 00:19:43,179
But that said, you gotta
be smart about it, right?

481
00:19:43,180 --> 00:19:45,610
So you have to have a
nimble software architecture

482
00:19:45,610 --> 00:19:47,760
and understand that as I'm bringing

483
00:19:47,760 --> 00:19:49,180
new elements into this model,

484
00:19:49,180 --> 00:19:52,030
that it's flexible enough
to take these changes on

485
00:19:52,030 --> 00:19:55,639
and to operate and behave
the way that I expect it.

486
00:19:55,640 --> 00:19:57,990
Which can also in and of
itself be challenging.

487
00:19:57,990 --> 00:20:00,680
So that way you may decide, okay,

488
00:20:00,680 --> 00:20:02,670
so if I'm gonna be doing this,

489
00:20:02,670 --> 00:20:05,200
the software architecture needs

490
00:20:05,200 --> 00:20:07,060
to be developed in an iterative manner.

491
00:20:07,060 --> 00:20:08,679
So once again, this brings us back

492
00:20:08,680 --> 00:20:10,680
to that agile development idea, right?

493
00:20:10,680 --> 00:20:13,990
Let's get pieces together and
understand how they work best

494
00:20:13,990 --> 00:20:16,433
and then we'll start
adding on incrementally.

495
00:20:19,500 --> 00:20:22,400
Okay, so now the exciting part, right?

496
00:20:22,400 --> 00:20:27,400
Now we talk about real AI
challenge in terms of data.

497
00:20:27,560 --> 00:20:29,070
And we all recognize that

498
00:20:29,070 --> 00:20:31,899
artificial intelligence thirsts for data.

499
00:20:31,900 --> 00:20:35,870
You hear about lakes and pools
and oceans of data, right?

500
00:20:35,870 --> 00:20:38,409
And there's a lot of different
things to talk about here.

501
00:20:38,410 --> 00:20:40,520
Let's level set with the idea first

502
00:20:40,520 --> 00:20:42,080
that we all know that there's this notion

503
00:20:42,080 --> 00:20:43,860
of relevance of the day, right?

504
00:20:43,860 --> 00:20:46,929
And what we're speaking
to there is how usable

505
00:20:46,930 --> 00:20:49,320
or how useful is that
data that we're collecting

506
00:20:49,320 --> 00:20:52,740
for answering the answering the problem

507
00:20:52,740 --> 00:20:54,370
that we're trying to solve, right?

508
00:20:54,370 --> 00:20:56,679
How best does it fit the
model that we're using?

509
00:20:56,680 --> 00:20:58,090
And then you have to worry about,

510
00:20:58,090 --> 00:21:00,733
well, okay, is this information accurate?

511
00:21:01,750 --> 00:21:03,900
So the data, how good is it?

512
00:21:03,900 --> 00:21:06,810
And that could go to
all types of questions.

513
00:21:06,810 --> 00:21:07,980
Where are you collecting it from?

514
00:21:07,980 --> 00:21:08,920
How are you collecting it?

515
00:21:08,920 --> 00:21:11,660
What kind of sensors are you using, right?

516
00:21:11,660 --> 00:21:15,980
Despite using the very correct
sensor that I know I need,

517
00:21:15,980 --> 00:21:17,980
and despite going to the right location

518
00:21:17,980 --> 00:21:22,230
and getting it from the right
spot, it could be victimized.

519
00:21:22,230 --> 00:21:23,920
I mean, the data could be poisoned.

520
00:21:23,920 --> 00:21:24,930
In other words, the threat actor

521
00:21:24,930 --> 00:21:26,860
will come in and they'll make changes,

522
00:21:26,860 --> 00:21:30,129
or they'll alter or do something to it

523
00:21:30,130 --> 00:21:32,750
to make it so that it is going to

524
00:21:32,750 --> 00:21:34,980
lead you down an improper path,

525
00:21:34,980 --> 00:21:36,536
or it's going to break your model

526
00:21:36,536 --> 00:21:38,836
and you're gonna get the
right answers, right?

527
00:21:39,930 --> 00:21:43,370
There could be bias, too, in
how your model was developed

528
00:21:43,370 --> 00:21:47,530
in terms of favoring one
data set versus another,

529
00:21:47,530 --> 00:21:51,180
or elements of data sets that
that may be more important

530
00:21:51,180 --> 00:21:53,583
and have greater priority, right?

531
00:21:54,500 --> 00:21:56,660
That said, maybe you have a broken sensor,

532
00:21:56,660 --> 00:21:58,840
or maybe your strategy for implementation

533
00:21:58,840 --> 00:22:00,209
of those sensors is off, right?

534
00:22:00,210 --> 00:22:04,030
So you could have faulty data
collection in that regard.

535
00:22:04,030 --> 00:22:05,889
And we always struggle with this idea

536
00:22:05,890 --> 00:22:07,430
of having enough data as well.

537
00:22:07,430 --> 00:22:10,810
So bear in mind, too,
that a low volume of data

538
00:22:10,810 --> 00:22:13,409
could limit our ability to implement

539
00:22:13,410 --> 00:22:15,370
this artificial intelligence

540
00:22:15,370 --> 00:22:17,652
in the way that we want and see fit.

541
00:22:19,420 --> 00:22:20,650
So what are we gonna do here?

542
00:22:20,650 --> 00:22:23,630
So there's a lot of
things to think about here

543
00:22:23,630 --> 00:22:25,080
that I'm going to talk about,

544
00:22:26,080 --> 00:22:30,470
but that said there's probably
a whole lot more, okay?

545
00:22:30,470 --> 00:22:32,800
So let's just start at
the real basic level

546
00:22:32,800 --> 00:22:35,500
of thinking about the
model that you've developed

547
00:22:35,500 --> 00:22:39,500
and how the data is being
used, how it's being collected,

548
00:22:39,500 --> 00:22:42,320
and how often do I have
to refresh that data.

549
00:22:42,320 --> 00:22:43,939
You know where I think
about this classically

550
00:22:43,940 --> 00:22:46,750
is with elections, and I'm not
here to get political on you.

551
00:22:46,750 --> 00:22:50,770
But what I am trying to
think about is you see polls

552
00:22:50,770 --> 00:22:53,389
that are happening
leading up to an election

553
00:22:53,390 --> 00:22:55,560
like every week or something like that.

554
00:22:55,560 --> 00:22:57,460
That refresh rate is really high.

555
00:22:57,460 --> 00:23:00,660
And by the way, it's costing
a whole lot of money, right?

556
00:23:00,660 --> 00:23:02,730
And the funny thing is,
is as you watch the polls

557
00:23:02,730 --> 00:23:06,160
there's really not a lot of shift

558
00:23:06,160 --> 00:23:07,483
in one week To another.

559
00:23:08,380 --> 00:23:11,330
That said, you'll see
too where they'll show,

560
00:23:11,330 --> 00:23:15,960
oh, well this poll was trending
a while back in January,

561
00:23:15,960 --> 00:23:18,210
and now in November, it's
changed or it's shifted.

562
00:23:18,210 --> 00:23:19,740
So not only do you wanna think about

563
00:23:19,740 --> 00:23:21,460
how often you're refreshing your data,

564
00:23:21,460 --> 00:23:22,427
but you want to think about,

565
00:23:22,427 --> 00:23:26,090
"Am I really gonna want to
expunge or get rid of old data?

566
00:23:26,090 --> 00:23:28,149
And if I do, how long ago?

567
00:23:28,150 --> 00:23:31,530
Is it temporal or am I
doing it based on quality?

568
00:23:31,530 --> 00:23:34,129
Am I doing it because
I've changed up my model?"

569
00:23:34,130 --> 00:23:35,750
So there's a lot of things
to think about there

570
00:23:35,750 --> 00:23:38,150
in terms of how much of
that are you gonna keep.

571
00:23:39,090 --> 00:23:41,618
You're also gonna think about
once you've collected it,

572
00:23:41,618 --> 00:23:44,030
how am I gonna clean it?

573
00:23:44,030 --> 00:23:46,450
Data can be noisy and
we all know this, right?

574
00:23:46,450 --> 00:23:49,636
We see where you have aberrant data

575
00:23:49,636 --> 00:23:50,693
that's gonna come in and it's gonna,

576
00:23:52,350 --> 00:23:54,290
maybe not fit the model
that you're thinking,

577
00:23:54,290 --> 00:23:56,610
or maybe it's just noise, right?

578
00:23:56,610 --> 00:23:58,899
I like to, once again,
thinking about those polls,

579
00:23:58,900 --> 00:24:02,410
you get people who have all
different types of opinions,

580
00:24:02,410 --> 00:24:05,190
and some of that may not necessarily

581
00:24:05,190 --> 00:24:07,310
fit the question that you're even asking.

582
00:24:07,310 --> 00:24:08,710
So you're gonna wanna think about

583
00:24:08,710 --> 00:24:10,100
how am I gonna clean that data?

584
00:24:10,100 --> 00:24:14,169
How am I gonna get to a point
where my system can use it

585
00:24:14,170 --> 00:24:18,133
to make and discern a proper
decision that I'm looking for?

586
00:24:19,350 --> 00:24:21,980
Now, that costs a lot
of money, it costs time.

587
00:24:21,980 --> 00:24:24,630
It takes special people who have training

588
00:24:24,630 --> 00:24:27,060
to sit down and look at it.

589
00:24:27,060 --> 00:24:29,080
Maybe you could automate elements of it,

590
00:24:29,080 --> 00:24:29,939
but at the end of the day

591
00:24:29,940 --> 00:24:31,830
you're gonna have to have
somebody checking it, right?

592
00:24:31,830 --> 00:24:34,449
Some subject matter experts
are gonna have to come in

593
00:24:34,450 --> 00:24:37,660
and recognize that there's
some elements of cleaning

594
00:24:37,660 --> 00:24:40,410
that's gonna need to be done, okay?

595
00:24:40,410 --> 00:24:42,140
Once again, we're hitting upon

596
00:24:42,140 --> 00:24:44,210
the interdependency idea here too.

597
00:24:44,210 --> 00:24:47,320
Think back to the slide
where I talked about

598
00:24:47,320 --> 00:24:50,149
the lack of expertise in an organization.

599
00:24:50,150 --> 00:24:51,800
You may hire analysts,

600
00:24:51,800 --> 00:24:53,490
but it's gonna take some
significant training

601
00:24:53,490 --> 00:24:55,510
for them to understand the
data that they're cleansing

602
00:24:55,510 --> 00:24:57,560
and what they're gonna do with it, right?

603
00:24:58,470 --> 00:25:01,743
So something to keep in mind.

604
00:25:03,330 --> 00:25:07,820
Now, (sighs) let's see, let's
assume we've gotten this far

605
00:25:07,820 --> 00:25:09,379
and we even have something stood up

606
00:25:09,380 --> 00:25:10,763
and we even have decisions being made.

607
00:25:10,763 --> 00:25:12,409
And the artificial intelligence

608
00:25:12,410 --> 00:25:14,588
is steaming right along
in your enterprise.

609
00:25:14,588 --> 00:25:18,300
And you've maybe automated decisions

610
00:25:18,300 --> 00:25:21,790
with respect to selecting
a configuration of controls

611
00:25:21,790 --> 00:25:24,670
in your security stack, which
would be phenomenal, right?

612
00:25:24,670 --> 00:25:25,910
It's something that we would all want.

613
00:25:25,910 --> 00:25:30,190
Real time controls solution
for your organization

614
00:25:30,190 --> 00:25:33,140
such that it senses
threats in the environment

615
00:25:33,140 --> 00:25:37,440
and it knows and understands
and identifies vulnerabilities,

616
00:25:37,440 --> 00:25:39,160
and it's making these risk-based decisions

617
00:25:39,160 --> 00:25:42,270
to literally go in and
change your control stack

618
00:25:42,270 --> 00:25:46,000
so that way the gates stay up at all times

619
00:25:46,000 --> 00:25:47,460
and the bad guys or bad girls

620
00:25:47,460 --> 00:25:49,791
are not getting into your organization.

621
00:25:49,791 --> 00:25:51,520
It'd be phenomenal, right?

622
00:25:51,520 --> 00:25:54,510
But I'm here to say that it's
gonna be really challenging

623
00:25:54,510 --> 00:25:56,840
to understand how you verify

624
00:25:56,840 --> 00:26:00,010
that the correct decisions are being made.

625
00:26:00,010 --> 00:26:01,390
Let me give you an example.

626
00:26:01,390 --> 00:26:02,370
Think about it this way.

627
00:26:02,370 --> 00:26:05,090
We all know that as we
modify our control stack,

628
00:26:05,090 --> 00:26:09,399
that there's a balance
that's struck there, right?

629
00:26:09,400 --> 00:26:11,170
As we limit the number of people

630
00:26:11,170 --> 00:26:12,900
that can come through our firewall

631
00:26:12,900 --> 00:26:17,620
and access the services
that we're providing,

632
00:26:17,620 --> 00:26:18,581
as those controls are shifting,

633
00:26:18,582 --> 00:26:22,330
we may be blocking out some customers.

634
00:26:22,330 --> 00:26:25,899
We may be, as I've talked about before,

635
00:26:25,900 --> 00:26:29,030
making decisions or the AI
may be making decisions,

636
00:26:29,030 --> 00:26:32,530
that are actually putting
some customers in jeopardy.

637
00:26:32,530 --> 00:26:34,240
Maybe if they rely on your services

638
00:26:34,240 --> 00:26:37,670
for lifesaving kind of services,

639
00:26:37,670 --> 00:26:39,713
this could be really important, right?

640
00:26:40,620 --> 00:26:42,570
So you wanna think about how is it

641
00:26:42,570 --> 00:26:45,760
that I'm prioritizing these
decisions that are being made?

642
00:26:45,760 --> 00:26:49,830
Am I educating the customer
such that they know

643
00:26:49,830 --> 00:26:52,800
that the decisions being made
are the ones that they need,

644
00:26:52,800 --> 00:26:54,649
are the ones that they want?

645
00:26:54,650 --> 00:26:57,400
Once again, go revisit
the expectation slide

646
00:26:57,400 --> 00:26:59,150
and let's tie it all together.

647
00:26:59,150 --> 00:27:00,380
And you can see that this register

648
00:27:00,380 --> 00:27:03,570
really has some silver
threads of commonality

649
00:27:03,570 --> 00:27:05,290
going through it, right?

650
00:27:05,290 --> 00:27:07,110
So what are some things
that we can do here?

651
00:27:07,110 --> 00:27:09,629
Well, first of all, we have
to have an understanding

652
00:27:09,630 --> 00:27:11,010
or know what you're expecting

653
00:27:11,010 --> 00:27:14,020
from the technology solution
that you've selected, okay?

654
00:27:14,020 --> 00:27:16,220
And this goes back to the
problem definition statement.

655
00:27:16,220 --> 00:27:18,110
What questions am I trying to answer?

656
00:27:18,110 --> 00:27:20,250
And how do I expect them to be answered?

657
00:27:20,250 --> 00:27:22,020
This selection of cyber controls

658
00:27:22,020 --> 00:27:23,900
is a real pertinent
example for you, right?

659
00:27:23,900 --> 00:27:26,057
We know that we want to keep out

660
00:27:26,057 --> 00:27:27,940
APTs or advanced persistent threats.

661
00:27:27,940 --> 00:27:31,900
We wanna keep out folks
from getting into our system

662
00:27:31,900 --> 00:27:34,130
or bots from getting into our system

663
00:27:34,130 --> 00:27:36,380
and doing any significant damage.

664
00:27:36,380 --> 00:27:38,840
To think about that confidentiality,

665
00:27:38,840 --> 00:27:41,199
integrity, and availability idea.

666
00:27:41,200 --> 00:27:43,230
Okay, fair enough, right?

667
00:27:43,230 --> 00:27:47,810
But that said, we know that
our appetite for those things

668
00:27:47,810 --> 00:27:49,060
may be changing over time.

669
00:27:49,060 --> 00:27:54,060
Maybe we have data in our
system that has maybe grown old

670
00:27:54,470 --> 00:27:56,270
and no longer has relevance to anybody,

671
00:27:56,270 --> 00:27:59,520
even if it is an APT
that gets into a system.

672
00:27:59,520 --> 00:28:01,379
So how do I verify that even though

673
00:28:01,380 --> 00:28:05,100
I've insulated that data,
that it was worth the resource

674
00:28:05,100 --> 00:28:07,322
that I've invested in protecting it?

675
00:28:08,550 --> 00:28:12,500
So once again, something that
you may start dismissing,

676
00:28:12,500 --> 00:28:17,500
or may be assuming that is
almost a foregone conclusion

677
00:28:19,060 --> 00:28:23,290
as a customer because you're
taking this AI for granted now.

678
00:28:23,290 --> 00:28:25,750
But that said, all the while
you could be losing sight

679
00:28:25,750 --> 00:28:28,250
of those critical priorities
in your organization.

680
00:28:29,440 --> 00:28:32,500
Okay, so now as we come to the end

681
00:28:32,500 --> 00:28:34,320
of the presentation in a sense,

682
00:28:34,320 --> 00:28:36,340
you're left saying, "Well, what do I do?

683
00:28:36,340 --> 00:28:39,300
Like, what do I consider?
Or where do I go next?"

684
00:28:39,300 --> 00:28:41,760
So there's some thoughts that
I wanna leave you with here.

685
00:28:41,760 --> 00:28:44,190
And this slide is probably
the biggest takeaway

686
00:28:44,190 --> 00:28:46,740
that you're gonna want from
this whole discussion, right?

687
00:28:46,740 --> 00:28:48,650
We've talked a lot about
artificial intelligence.

688
00:28:48,650 --> 00:28:50,070
We've talked a lot about risk.

689
00:28:50,070 --> 00:28:51,320
But what's it all mean, right?

690
00:28:51,320 --> 00:28:53,970
So I think as technologists

691
00:28:53,970 --> 00:28:57,050
and as technology managers
out there in the audience,

692
00:28:57,050 --> 00:28:59,129
we really wanna think about

693
00:28:59,130 --> 00:29:01,820
how we're going to manage this technology.

694
00:29:01,820 --> 00:29:03,050
What are we gonna do with it?

695
00:29:03,050 --> 00:29:06,860
And not only that, what kind
of policy do we have around it?

696
00:29:06,860 --> 00:29:08,659
It's not just a technology piece.

697
00:29:08,660 --> 00:29:10,530
It's what we have written on paper

698
00:29:10,530 --> 00:29:12,440
and what we expect of our employees,

699
00:29:12,440 --> 00:29:14,490
of our organization, of our customers,

700
00:29:14,490 --> 00:29:18,820
to behave, to feed this
data in the system.

701
00:29:18,820 --> 00:29:20,620
Or maybe there are people who are

702
00:29:20,620 --> 00:29:22,780
going to be using the results.

703
00:29:22,780 --> 00:29:23,613
We need to think about that

704
00:29:23,613 --> 00:29:25,697
and how we're gonna
manage that technology.

705
00:29:27,110 --> 00:29:32,100
I think again about the
current pandemic situation

706
00:29:32,100 --> 00:29:34,490
that we're in as an example here.

707
00:29:34,490 --> 00:29:37,690
This is the idea that if we had a policy

708
00:29:37,690 --> 00:29:40,130
to tell our people how to work remotely,

709
00:29:40,130 --> 00:29:41,610
we wouldn't have had a grand panic

710
00:29:41,610 --> 00:29:43,260
when everybody was forced to stay home

711
00:29:43,260 --> 00:29:46,060
in February, March timeframe.

712
00:29:46,060 --> 00:29:47,940
And we could have really thought through

713
00:29:47,940 --> 00:29:49,780
how to manage the technology

714
00:29:49,780 --> 00:29:51,420
that they were taking home with them

715
00:29:51,420 --> 00:29:53,610
and been a little bit more effective

716
00:29:53,610 --> 00:29:55,459
as some organizations go.

717
00:29:55,460 --> 00:29:58,470
AI is no different in this regard.

718
00:29:58,470 --> 00:30:01,945
Now, some thoughts here
on how you can manage that

719
00:30:01,945 --> 00:30:05,750
and get the connection
of policy and technology.

720
00:30:05,750 --> 00:30:07,290
I would recommend that you set up

721
00:30:07,290 --> 00:30:09,590
a technology council for your organization

722
00:30:09,590 --> 00:30:11,020
if you don't already have one.

723
00:30:11,020 --> 00:30:13,300
They need to be considering the ingestion

724
00:30:13,300 --> 00:30:14,669
of this artificial intelligence,

725
00:30:14,670 --> 00:30:17,450
and they should also
have a real strong tie

726
00:30:17,450 --> 00:30:19,750
to your risk management program

727
00:30:19,750 --> 00:30:21,310
that you have in your organization.

728
00:30:21,310 --> 00:30:24,409
Maybe you have a strong enterprise
risk management program.

729
00:30:24,410 --> 00:30:26,210
Maybe you have a strong
risk management program

730
00:30:26,210 --> 00:30:28,680
within your CISO organization.

731
00:30:28,680 --> 00:30:30,770
Either way, those ties need to be strong

732
00:30:30,770 --> 00:30:34,480
between this technology council
of subject matter experts

733
00:30:34,480 --> 00:30:37,000
and their discussion with
risk management experts

734
00:30:37,000 --> 00:30:38,760
to understand how that uncertainty

735
00:30:38,760 --> 00:30:42,557
ties to the technology
that you're bringing in

736
00:30:42,557 --> 00:30:44,080
and the decisions that you're making

737
00:30:44,080 --> 00:30:46,122
in your organization, okay?

738
00:30:47,490 --> 00:30:48,960
So you also wanna think about, well,

739
00:30:48,960 --> 00:30:51,490
where's the technology
gonna be implemented?

740
00:30:51,490 --> 00:30:52,520
Is it going to be within

741
00:30:52,520 --> 00:30:54,230
a certain function of my organization?

742
00:30:54,230 --> 00:30:55,740
What services will be impacted?

743
00:30:55,740 --> 00:30:57,710
What customers will be impacted?

744
00:30:57,710 --> 00:30:58,960
You need to think about the appetite

745
00:30:58,960 --> 00:31:01,950
that you have for serving those customers

746
00:31:01,950 --> 00:31:05,540
and/or possibly failing them
if you think about it, right?

747
00:31:05,540 --> 00:31:08,800
This technology is nothing more than

748
00:31:08,800 --> 00:31:11,450
doing human things and making decisions.

749
00:31:11,450 --> 00:31:13,750
It's just doing it a lot faster, right?

750
00:31:13,750 --> 00:31:15,800
Are you comfortable with that speed

751
00:31:15,800 --> 00:31:18,678
of the technology making those decisions?

752
00:31:18,678 --> 00:31:19,927
And in the time and place

753
00:31:19,927 --> 00:31:22,649
that these decisions are being made,

754
00:31:22,650 --> 00:31:26,810
are you comfortable with the
results that you're getting?

755
00:31:26,810 --> 00:31:28,879
Is your customer
comfortable with the results

756
00:31:28,880 --> 00:31:30,693
that they're getting in that regard, too?

757
00:31:31,780 --> 00:31:34,620
And finally, we don't wanna
walk away from the idea

758
00:31:34,620 --> 00:31:37,320
of continually revisiting the equation

759
00:31:37,320 --> 00:31:40,570
of that balance between
the risk that you're taking

760
00:31:40,570 --> 00:31:42,520
and the results that
you're getting, right?

761
00:31:42,520 --> 00:31:43,353
Am I getting a return

762
00:31:43,353 --> 00:31:46,400
on that risk investment in
artificial intelligence?

763
00:31:46,400 --> 00:31:48,630
Does the business case still make sense?

764
00:31:48,630 --> 00:31:50,540
You can get to a point
where we're talking about

765
00:31:50,540 --> 00:31:52,190
that data challenges part,

766
00:31:52,190 --> 00:31:55,390
where collection of that
data is just so expensive

767
00:31:55,390 --> 00:31:57,550
that it's prohibitive in terms

768
00:31:57,550 --> 00:32:00,040
of what you're getting out
of it in terms of benefit,

769
00:32:00,040 --> 00:32:02,350
in terms of automating
that decision being made

770
00:32:02,350 --> 00:32:04,159
versus having someone who just has

771
00:32:04,160 --> 00:32:05,520
some subject matter expertise

772
00:32:05,520 --> 00:32:07,150
and a nice, strong governance structure

773
00:32:07,150 --> 00:32:08,800
that they can consult with,

774
00:32:08,800 --> 00:32:12,800
make the decision, and you
can move along smartly.

775
00:32:12,800 --> 00:32:14,360
Now, I used a real powerful example

776
00:32:14,360 --> 00:32:15,600
with artificial intelligence

777
00:32:15,600 --> 00:32:20,340
and modifying and updating
your control stack.

778
00:32:20,340 --> 00:32:21,689
That could be very much a place

779
00:32:21,690 --> 00:32:23,980
where there is a strong
return on this investment,

780
00:32:23,980 --> 00:32:25,210
but if that's the case,

781
00:32:25,210 --> 00:32:27,840
someone needs to continually
revisit that model

782
00:32:27,840 --> 00:32:30,070
and make sure that the
decisions being made

783
00:32:30,070 --> 00:32:34,899
are not hurting you in
terms of your customer base,

784
00:32:34,900 --> 00:32:35,790
in terms of revenue,

785
00:32:35,790 --> 00:32:37,990
in terms of delivering
those critical services

786
00:32:37,990 --> 00:32:41,280
that makes your organization go, okay?

787
00:32:41,280 --> 00:32:44,155
So at the end of the day,
I wanna say that technology

788
00:32:44,155 --> 00:32:48,030
in terms of artificial
intelligence isn't magical, right?

789
00:32:48,030 --> 00:32:50,510
There are a lot of cases
where we've done this before.

790
00:32:50,510 --> 00:32:52,600
We've ingested other technologies.

791
00:32:52,600 --> 00:32:54,820
If you recall back in the
day when mobile devices

792
00:32:54,820 --> 00:32:58,679
were coming in vogue, and we
were trying to incorporate

793
00:32:58,680 --> 00:33:02,090
those more into our daily
tasks and organizations.

794
00:33:02,090 --> 00:33:04,459
I remember myself owning
a Blackberry at one point.

795
00:33:04,460 --> 00:33:05,610
That was exciting, right?

796
00:33:05,610 --> 00:33:06,820
But before that there was PalmPilots

797
00:33:06,820 --> 00:33:08,230
and there was trying to figure out,

798
00:33:08,230 --> 00:33:10,100
well, how does this technology
fit in the organization?

799
00:33:10,100 --> 00:33:11,419
What risks am I taking by having

800
00:33:11,420 --> 00:33:15,060
somebody take that home with them, right?

801
00:33:15,060 --> 00:33:16,379
We've kinda seen that before.

802
00:33:16,380 --> 00:33:18,950
How about another example
would be cloud implementation.

803
00:33:18,950 --> 00:33:20,900
We've seen that technology come along now,

804
00:33:20,900 --> 00:33:22,850
and we've seen organizations kinda wrestle

805
00:33:22,850 --> 00:33:24,820
with the challenges there.

806
00:33:24,820 --> 00:33:26,230
What I'm here to say is

807
00:33:26,230 --> 00:33:27,690
whether it's artificial intelligence,

808
00:33:27,690 --> 00:33:31,910
cloud implementation, or mobile devices,

809
00:33:31,910 --> 00:33:34,430
there are a lot of common challenges here

810
00:33:34,430 --> 00:33:36,700
where we trying to find the right people

811
00:33:36,700 --> 00:33:39,100
to answer the right question

812
00:33:39,100 --> 00:33:41,139
and understanding the
bounds of that question

813
00:33:41,140 --> 00:33:44,880
and what our solution is
trying to get to, okay?

814
00:33:44,880 --> 00:33:46,880
So I invite you to reach out to me.

815
00:33:46,880 --> 00:33:48,400
My name, once again, is Brett Tucker

816
00:33:48,400 --> 00:33:49,820
at the Software Engineering Institute.

817
00:33:49,820 --> 00:33:53,060
I'll leave you with my
contact information here.

818
00:33:53,060 --> 00:33:56,800
Please feel free to ask any
questions as you see fit.

819
00:33:56,800 --> 00:33:58,360
I'm happy to serve.

820
00:33:58,360 --> 00:33:59,409
Thanks for your time.

