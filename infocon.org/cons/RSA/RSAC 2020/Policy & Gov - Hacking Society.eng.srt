1
00:00:06,533 --> 00:00:09,033
>> SPEAKER: Please
welcome Bruce Schneier.

2
00:00:18,699 --> 00:00:19,733
>> BRUCE SCHNEIER:
Hey, good morning.

3
00:00:19,800 --> 00:00:21,166
Thanks for coming out.

4
00:00:22,800 --> 00:00:28,100
Let's talk about the tax system
as an IT security problem.

5
00:00:28,166 --> 00:00:31,066
It has code.

6
00:00:31,133 --> 00:00:33,533
It's just a series of
algorithms that take and

7
00:00:33,600 --> 00:00:36,899
inputs tax information for
the year and produces some

8
00:00:36,966 --> 00:00:39,733
outputs, the
amount of tax owed.

9
00:00:39,799 --> 00:00:41,500
It's incredibly
complex code.

10
00:00:41,566 --> 00:00:44,600
It consists of laws,
government laws, tax

11
00:00:44,666 --> 00:00:48,933
authority rulings, judicial
decisions, lawyer opinions.

12
00:00:49,000 --> 00:00:51,466
There are bugs
in the code.

13
00:00:51,533 --> 00:00:54,000
There are mistakes in how
the law is written, how

14
00:00:54,066 --> 00:00:56,200
it's interpreted.

15
00:00:56,266 --> 00:01:00,266
Some of those bugs are
vulnerabilities, and

16
00:01:00,333 --> 00:01:03,833
attackers look for
exploitable vulnerabilities.

17
00:01:03,899 --> 00:01:07,366
We call them
tax loopholes.

18
00:01:07,433 --> 00:01:08,099
Right?

19
00:01:08,166 --> 00:01:11,166
Attackers exploit
these vulnerabilities.

20
00:01:11,233 --> 00:01:14,266
We call it tax avoidance.

21
00:01:14,333 --> 00:01:16,765
And vulnerabilities,
loopholes are everywhere

22
00:01:16,833 --> 00:01:17,599
in the tax code.

23
00:01:17,666 --> 00:01:19,700
And, actually, there are
thousands of black hat

24
00:01:19,766 --> 00:01:22,966
security researchers that
examine every line of the

25
00:01:23,033 --> 00:01:25,700
tax code looking for
vulnerabilities.

26
00:01:25,766 --> 00:01:27,766
We call them
tax attorneys.

27
00:01:29,400 --> 00:01:31,700
Some of these
bugs are mistakes.

28
00:01:31,766 --> 00:01:36,233
There is -- in the 2017
tax law, there was an

29
00:01:36,299 --> 00:01:41,000
actual mistake, a typo,
that categorized military

30
00:01:41,066 --> 00:01:44,299
death benefits as earned
income, and as a result,

31
00:01:44,366 --> 00:01:47,533
surviving family members
got unexpected tax bills

32
00:01:47,599 --> 00:01:49,933
of $10,000 or more.

33
00:01:50,000 --> 00:01:52,900
Some of these are
emergent properties.

34
00:01:52,966 --> 00:01:55,366
There is the, I'm going to
read it, the double Irish

35
00:01:55,433 --> 00:01:57,299
with a Dutch sandwich.

36
00:01:57,366 --> 00:01:59,500
This is the trick
that lets U.S. companies

37
00:01:59,566 --> 00:02:03,400
like Google and
Apple avoid paying U.S.

38
00:02:03,466 --> 00:02:06,798
tax, and, actually,
Google is possibly being

39
00:02:06,866 --> 00:02:09,699
prosecuted for
that right now.

40
00:02:09,765 --> 00:02:12,066
Some of these
vulnerabilities are

41
00:02:12,133 --> 00:02:15,699
deliberately created in
the tax code by lobbyists

42
00:02:15,766 --> 00:02:18,799
trying to gain some
advantage to their industry.

43
00:02:18,866 --> 00:02:23,599
Sometimes a legislator knows
about it; sometimes they don't.

44
00:02:23,666 --> 00:02:27,366
I guess this is analogous
to a government sneaking a

45
00:02:27,433 --> 00:02:31,965
programmer into Microsoft to
drop a vulnerability in Windows.

46
00:02:32,033 --> 00:02:36,632
All right, so this
is my big idea.

47
00:02:36,699 --> 00:02:39,400
We here in our community
have developed some very

48
00:02:39,466 --> 00:02:44,466
effective techniques to deal
with code, to deal with tech.

49
00:02:47,566 --> 00:02:51,033
We started by examining
purely technical systems.

50
00:02:51,099 --> 00:02:55,099
Increasingly, we study
sociotechnical systems.

51
00:02:55,166 --> 00:03:00,633
Can our expertise in IT
security transfer to

52
00:03:00,699 --> 00:03:05,500
broader social systems
like the tax code, like

53
00:03:05,566 --> 00:03:07,900
the system we use to
choose our elected

54
00:03:07,966 --> 00:03:12,699
officials, like
the market economy?

55
00:03:12,766 --> 00:03:16,833
Is our way of thinking,
our analytical framework,

56
00:03:16,900 --> 00:03:21,000
our procedural mindset valuable
in this broader context?

57
00:03:21,066 --> 00:03:22,199
Can we hack society?

58
00:03:22,266 --> 00:03:25,833
And, actually, more
importantly, can we help

59
00:03:25,900 --> 00:03:28,699
secure the systems
that make up society?

60
00:03:30,699 --> 00:03:32,800
So back to the tax code.

61
00:03:32,866 --> 00:03:35,199
We know how to fix
this problem before the

62
00:03:35,266 --> 00:03:37,433
code is deployed.

63
00:03:37,500 --> 00:03:40,766
Secure development
processes, source code audits.

64
00:03:40,833 --> 00:03:43,900
How do we do that
for the tax code?

65
00:03:43,966 --> 00:03:44,666
Like, who does it?

66
00:03:44,733 --> 00:03:46,300
Who pays for it?

67
00:03:46,366 --> 00:03:48,199
And what about
those deliberate

68
00:03:48,266 --> 00:03:50,599
vulnerabilities?

69
00:03:50,666 --> 00:03:53,733
We know how to fix the
problem with running code.

70
00:03:53,800 --> 00:03:56,466
Vulnerability finding by
white hat researchers, bug

71
00:03:56,533 --> 00:03:57,933
bounties, patching.

72
00:03:58,000 --> 00:04:02,300
How do you patch
the tax code?

73
00:04:02,366 --> 00:04:05,333
How do you create laws and
policies to implement the

74
00:04:05,400 --> 00:04:06,599
notion of patching?

75
00:04:06,666 --> 00:04:10,698
I mean right now passing
tax legislation is a big

76
00:04:10,766 --> 00:04:13,000
deal politically.

77
00:04:13,066 --> 00:04:15,533
And here's the big
question: Can we design a

78
00:04:15,599 --> 00:04:18,300
security system to
deal with bugs and

79
00:04:18,366 --> 00:04:20,466
vulnerabilities in the
tax code and then build

80
00:04:20,533 --> 00:04:22,566
procedures to
implement it?

81
00:04:24,233 --> 00:04:26,533
So security technologists
have a certain way of

82
00:04:26,600 --> 00:04:28,266
looking at the world.

83
00:04:28,333 --> 00:04:31,766
It's systems thinking with
an adversarial mindset.

84
00:04:31,833 --> 00:04:34,000
I call it a
hacker mindset.

85
00:04:34,066 --> 00:04:37,633
We think about how systems
fail, how they can be made

86
00:04:37,699 --> 00:04:42,433
to fail, and we think
about everything in this way.

87
00:04:42,500 --> 00:04:44,699
And we've developed what
I think is a unique skill

88
00:04:44,766 --> 00:04:48,899
set: Understanding
technical systems with

89
00:04:48,966 --> 00:04:50,500
human dimensions,
understanding

90
00:04:50,566 --> 00:04:53,399
sociotechnical systems,
thinking about these

91
00:04:53,466 --> 00:04:57,466
systems in an adversarial
mindset, adaptive

92
00:04:57,533 --> 00:05:01,566
malicious adversaries, and
understanding the security

93
00:05:01,633 --> 00:05:05,299
of complex adaptive
systems, and understanding

94
00:05:05,366 --> 00:05:08,066
iterative
security solutions.

95
00:05:08,133 --> 00:05:10,866
This way of thinking
generalizes, and it's my

96
00:05:10,933 --> 00:05:13,699
contention that the worlds
of tech and policy are

97
00:05:13,766 --> 00:05:16,966
converging, that the tax
code is now becoming

98
00:05:17,033 --> 00:05:21,399
actual code, and that
where once purely systems

99
00:05:21,466 --> 00:05:24,600
are increase
sociotechnical systems.

100
00:05:24,666 --> 00:05:29,300
And as society's systems
become more complex, as

101
00:05:29,366 --> 00:05:32,300
the world looks more like
a computer, our security

102
00:05:32,366 --> 00:05:36,633
skills become more
broadly applicable.

103
00:05:36,699 --> 00:05:38,666
So that's
basically my talk.

104
00:05:38,733 --> 00:05:40,300
It's preliminary work.

105
00:05:40,366 --> 00:05:46,000
I have a lot of examples
and a lot of detail.

106
00:05:46,066 --> 00:05:48,832
I'm going to throw a
bunch of stuff at you.

107
00:05:48,899 --> 00:05:51,600
And I want to convince you
that we have this unique

108
00:05:51,666 --> 00:05:54,733
framework for solving
security problems, and

109
00:05:54,800 --> 00:05:59,233
there are new domains
we can apply them to.

110
00:05:59,300 --> 00:06:01,233
I guess I want to put a
caveat here in the beginning.

111
00:06:01,300 --> 00:06:05,866
I don't want to say that
tech can fix everything.

112
00:06:05,933 --> 00:06:08,233
This isn't
technological solutionism.

113
00:06:08,300 --> 00:06:10,166
This isn't Silicon
Valley saving the world.

114
00:06:10,233 --> 00:06:13,399
This is a way that I think
we can blend tech and

115
00:06:13,466 --> 00:06:14,800
policy in a new way.

116
00:06:14,866 --> 00:06:16,899
All right.

117
00:06:16,966 --> 00:06:20,933
So to do this, we need to
broaden some definitions.

118
00:06:21,000 --> 00:06:23,500
Let's talk about a hack.

119
00:06:23,566 --> 00:06:27,599
A hack is something a
system allows but is

120
00:06:27,666 --> 00:06:33,133
unwanted and unanticipated
by the system designers.

121
00:06:33,199 --> 00:06:37,699
More than that, it is an
exploitation of the system.

122
00:06:37,766 --> 00:06:41,666
Something desired by the
attacker at the expense of

123
00:06:41,733 --> 00:06:43,733
some other part
of the system.

124
00:06:45,133 --> 00:06:48,566
So in his memoirs, Edward
Snowden writes that the U.S.

125
00:06:48,633 --> 00:06:51,866
intelligence community
hacked the constitution in

126
00:06:51,933 --> 00:06:54,633
order to justify
mass surveillance.

127
00:06:54,699 --> 00:06:57,133
We can argue whether
that's true or not, but

128
00:06:57,199 --> 00:07:02,000
everyone here intuitively
knows what he means by that.

129
00:07:02,066 --> 00:07:06,832
Other examples of hacks:
So lack of standing is a

130
00:07:06,899 --> 00:07:09,433
hack the NSA used to
avoid litigating the

131
00:07:09,500 --> 00:07:12,100
constitutionality
of their actions.

132
00:07:12,166 --> 00:07:15,366
Vulnerability, of course,
is that there's a body of

133
00:07:15,433 --> 00:07:20,933
law out of reach of
conventional judicial review.

134
00:07:21,000 --> 00:07:24,933
Using the old writs act
against Apple as the FBI

135
00:07:25,000 --> 00:07:27,899
did in 2016 is a hack.

136
00:07:27,966 --> 00:07:29,333
Maybe you think
it's a good hack.

137
00:07:29,399 --> 00:07:30,833
All hacks aren't bad.

138
00:07:30,899 --> 00:07:34,066
But it is definitely
an unintended and

139
00:07:34,133 --> 00:07:39,633
unanticipated use
out of a 1789 law.

140
00:07:39,699 --> 00:07:42,133
So this all makes
sense to me in my head.

141
00:07:42,199 --> 00:07:44,866
And my guess is it
makes some sense to you,

142
00:07:44,933 --> 00:07:47,666
but is it useful?

143
00:07:47,733 --> 00:07:49,633
I think it is.

144
00:07:49,699 --> 00:07:52,233
I think this way of
looking at the world can

145
00:07:52,300 --> 00:07:56,100
usefully inform
policy decisions.

146
00:07:56,166 --> 00:07:59,866
Let's talk about hacking
the legislative process.

147
00:07:59,933 --> 00:08:02,533
Bills now are so
complicated that no one

148
00:08:02,600 --> 00:08:04,399
who votes on them truly
understands them.

149
00:08:04,466 --> 00:08:08,033
You just add one sentence
to a bill, it makes

150
00:08:08,100 --> 00:08:11,100
references to other laws,
and the combination

151
00:08:11,166 --> 00:08:13,533
results in some
specific outcome unknown

152
00:08:13,600 --> 00:08:15,666
to most everyone.

153
00:08:15,733 --> 00:08:17,699
And there's a whole
industry dedicated to

154
00:08:17,766 --> 00:08:21,333
engineering these
unanticipated consequences.

155
00:08:21,399 --> 00:08:23,399
It sounds like
spaghetti code.

156
00:08:25,600 --> 00:08:30,800
We can think of VC funding
as a hack of market economics.

157
00:08:30,866 --> 00:08:34,665
So markets are based on
knowledgeable buyers

158
00:08:34,732 --> 00:08:38,465
making decisions amongst
competing products.

159
00:08:38,533 --> 00:08:41,633
The pressure to sell to
those buyers depresses

160
00:08:41,700 --> 00:08:43,833
prices and incents
innovation.

161
00:08:43,899 --> 00:08:46,666
That's basically the
mechanic of the markets.

162
00:08:48,799 --> 00:08:50,933
VC funding hacks
that process.

163
00:08:51,000 --> 00:08:55,533
The external injection of
money means that companies

164
00:08:55,600 --> 00:08:59,433
don't have to compete in
the traditional manner.

165
00:08:59,500 --> 00:09:02,066
The best strategy for
a start-up is to take

166
00:09:02,133 --> 00:09:06,000
enormous risk to be
successful, because

167
00:09:06,066 --> 00:09:09,100
otherwise they're dead,
and they can destroy

168
00:09:09,166 --> 00:09:12,533
without providing viable
alternatives as long as

169
00:09:12,600 --> 00:09:17,399
they have that external
funding source to do it.

170
00:09:17,466 --> 00:09:18,799
And this is a
vulnerability in the

171
00:09:18,866 --> 00:09:23,733
market system, which
makes Uber a hack. Right?

172
00:09:23,799 --> 00:09:27,566
VC funding means they can
lose $0.41 on every dollar

173
00:09:27,633 --> 00:09:30,899
until they destroy
the taxi industry.

174
00:09:30,966 --> 00:09:32,600
WeWork is a hack.

175
00:09:32,666 --> 00:09:33,399
I guess was a hack.

176
00:09:33,466 --> 00:09:35,066
Are they still around?

177
00:09:35,133 --> 00:09:38,700
Their business model
loses $2.6 billion a year.

178
00:09:41,899 --> 00:09:45,566
We could look at money and
politics as a similar example.

179
00:09:45,633 --> 00:09:49,700
The injection of private cash
hacks the Democratic process.

180
00:09:52,566 --> 00:09:54,566
So think about markets
more generally.

181
00:09:54,633 --> 00:09:57,500
They're really based on
three things: Information,

182
00:09:57,566 --> 00:10:00,000
choice, and agency.

183
00:10:00,066 --> 00:10:02,799
And they are all
under attack.

184
00:10:02,866 --> 00:10:05,933
Complex product offerings
obscure information.

185
00:10:06,000 --> 00:10:08,500
Just try to compare
prices of cell phone programs

186
00:10:08,566 --> 00:10:10,666
or credit cards.

187
00:10:10,733 --> 00:10:14,632
Monopolies remove our
ability to choose.

188
00:10:14,700 --> 00:10:17,233
Products and services we
can't reasonably live

189
00:10:17,299 --> 00:10:19,533
without deprive
us of agency.

190
00:10:19,600 --> 00:10:22,700
There's probably an
entire talk on this.

191
00:10:25,333 --> 00:10:28,100
So metaphors matter here.

192
00:10:28,166 --> 00:10:32,366
Most people don't consider
our Democratic process or

193
00:10:32,433 --> 00:10:35,366
the market as
sociotechnical systems.

194
00:10:35,433 --> 00:10:39,033
And I think this is
similar to us only

195
00:10:39,100 --> 00:10:42,233
thinking in terms
of tech systems.

196
00:10:42,299 --> 00:10:45,165
Remember 15 years ago when
we thought our security

197
00:10:45,233 --> 00:10:48,000
domain ended at the
keyboard and chair?

198
00:10:48,066 --> 00:10:50,633
Today we know that all
computer systems are

199
00:10:50,700 --> 00:10:53,500
actually complex
sociotechnical systems,

200
00:10:53,566 --> 00:10:54,533
that they are embedded.

201
00:10:54,600 --> 00:10:58,700
In systems, people say nested
in broader social systems.

202
00:10:58,766 --> 00:11:01,699
And it turns out all
modern systems are like

203
00:11:01,766 --> 00:11:04,500
that, too, just as the
balance between socio and

204
00:11:04,566 --> 00:11:06,566
technical are different.

205
00:11:08,933 --> 00:11:10,833
There's a difference
between determinism and

206
00:11:10,899 --> 00:11:13,066
non-determinism that
I think matters here.

207
00:11:15,766 --> 00:11:19,199
A bug in software
is deterministic.

208
00:11:19,266 --> 00:11:23,565
Who gets elected, world
events, social trends,

209
00:11:23,633 --> 00:11:25,233
those are
non-deterministic.

210
00:11:25,299 --> 00:11:26,665
Users are
non-deterministic.

211
00:11:26,733 --> 00:11:29,199
Hackers are
non-deterministic.

212
00:11:29,266 --> 00:11:32,799
Determinism is a majority
condition of computer systems.

213
00:11:32,866 --> 00:11:37,799
We in security deal with
non-determinism all the

214
00:11:37,866 --> 00:11:43,266
time and it's a majority
condition in social systems.

215
00:11:43,333 --> 00:11:45,266
I think we need to
generalize non-determinism

216
00:11:45,333 --> 00:11:50,833
better, both in our
systems and in social systems.

217
00:11:50,899 --> 00:11:55,366
Also, what do we
actually mean by a hack?

218
00:11:55,433 --> 00:11:59,500
In our world in computer
security, we tend to work

219
00:11:59,566 --> 00:12:02,366
with conventional
systems created for some

220
00:12:02,433 --> 00:12:05,766
purpose by someone.

221
00:12:05,833 --> 00:12:08,500
Social systems aren't
really like that.

222
00:12:08,566 --> 00:12:09,966
They evolve.

223
00:12:10,033 --> 00:12:11,399
New purposes emerge.

224
00:12:11,466 --> 00:12:15,200
A hack can be an emergent
property; it's not clear

225
00:12:15,266 --> 00:12:17,233
whether they're
good or bad.

226
00:12:17,299 --> 00:12:20,799
There's a lot of
perspective that matters here.

227
00:12:20,866 --> 00:12:24,600
If VC funding is simply
a way for the wealthy to

228
00:12:24,666 --> 00:12:27,566
invest their money,
then it's the market

229
00:12:27,633 --> 00:12:28,666
working as intended.

230
00:12:28,733 --> 00:12:33,065
And it's not obvious to me how
to handle this generalization.

231
00:12:35,166 --> 00:12:39,200
Another concept that
generalizes: Changes in

232
00:12:39,266 --> 00:12:40,833
the threat model.

233
00:12:40,899 --> 00:12:42,733
So we know how this works.

234
00:12:42,799 --> 00:12:45,132
A system is created for
some particular threat

235
00:12:45,200 --> 00:12:48,066
model and then
things change.

236
00:12:48,133 --> 00:12:51,866
Maybe its uses changes,
technology changes,

237
00:12:51,933 --> 00:12:56,266
circumstance changes, or
just a change in scale

238
00:12:56,333 --> 00:12:58,733
that causes a
change in kind.

239
00:12:58,799 --> 00:13:00,966
So the old security
assumptions are no longer true.

240
00:13:01,033 --> 00:13:04,500
The threat model has
changed, but no one

241
00:13:04,566 --> 00:13:09,299
notices it, so the system
kind of slides into insecurity.

242
00:13:11,333 --> 00:13:14,100
I've heard political scientists
call this concept drift.

243
00:13:14,166 --> 00:13:18,566
So let's talk about a
change in the threat model.

244
00:13:18,633 --> 00:13:21,200
Too big to fail.

245
00:13:21,266 --> 00:13:24,699
So this is a concept that
some corporations are so

246
00:13:24,766 --> 00:13:27,399
big and so important to
the functioning of our

247
00:13:27,466 --> 00:13:30,566
society that they can't
be allowed to fail.

248
00:13:32,200 --> 00:13:36,866
In 2008, U.S. government
bailed out several major banks

249
00:13:36,933 --> 00:13:41,600
to the tune of $700 billion
because of their very bad

250
00:13:41,666 --> 00:13:45,166
business decisions because
they were too big to fail.

251
00:13:45,233 --> 00:13:48,032
The fear was if the
government didn't do that,

252
00:13:48,100 --> 00:13:50,899
the banks would collapse
and take the economy with it.

253
00:13:51,733 --> 00:13:56,565
The banks are literally
too big to be allowed to fail.

254
00:13:56,633 --> 00:13:57,666
Not the first time.

255
00:13:57,733 --> 00:14:00,665
In 1979, U.S. government
bailed out Chrysler.

256
00:14:00,733 --> 00:14:02,899
Back then, it was
national security.

257
00:14:02,966 --> 00:14:04,700
They were building
the M1 Abrams tank.

258
00:14:04,766 --> 00:14:08,833
It was jobs, saving
700,000 jobs, saving

259
00:14:08,899 --> 00:14:11,700
suppliers and the whole
ecosystem, and there was

260
00:14:11,766 --> 00:14:15,500
an auto trade war going on
with Japan at the time.

261
00:14:15,566 --> 00:14:18,466
So this is an emergent
vulnerability.

262
00:14:18,533 --> 00:14:20,733
When the mechanisms of
the market economy were

263
00:14:20,799 --> 00:14:23,399
invented, nothing could
ever be that big.

264
00:14:23,466 --> 00:14:25,766
No one could conceive of
anything being that big.

265
00:14:28,399 --> 00:14:32,033
Our economic system is
based on an open market

266
00:14:32,100 --> 00:14:36,100
and relies on the fact
that the cost of failing is

267
00:14:36,166 --> 00:14:42,866
paid by the entity failing
and that guides behavior.

268
00:14:42,933 --> 00:14:44,933
That doesn't work if
you're too big to fail.

269
00:14:46,933 --> 00:14:50,233
A company that's trading
off private gains and

270
00:14:50,299 --> 00:14:52,866
public losses is not
going to make the same

271
00:14:52,933 --> 00:14:56,500
decisions, and this
perturbs market economics.

272
00:14:56,566 --> 00:15:03,033
We can look at threat model
changes in our political system.

273
00:15:03,100 --> 00:15:04,200
Election security.

274
00:15:04,266 --> 00:15:09,266
The U.S. system of securing
elections is basically

275
00:15:09,333 --> 00:15:11,733
based on representatives
of the two opposing

276
00:15:11,799 --> 00:15:14,699
parties sitting together
and making sure none of

277
00:15:14,766 --> 00:15:16,933
them does anything bad.

278
00:15:17,000 --> 00:15:19,166
That made perfect sense
against the threats

279
00:15:19,233 --> 00:15:22,099
in the mid-1800s.

280
00:15:22,166 --> 00:15:27,799
It is useless against modern
threats against elections.

281
00:15:27,866 --> 00:15:29,899
The apportioning of
representatives.

282
00:15:29,966 --> 00:15:33,000
Gerrymandering is much
more effective with modern

283
00:15:33,066 --> 00:15:35,066
surveillance systems.

284
00:15:36,200 --> 00:15:38,866
Like markets, Democracy
is based on information,

285
00:15:38,933 --> 00:15:41,165
choice, and agency, and
all three are under attack.

286
00:15:44,233 --> 00:15:46,333
So another thing we need
to generalize is who the

287
00:15:46,399 --> 00:15:49,133
attackers and defenders are.

288
00:15:49,200 --> 00:15:51,933
So we know that the term
attacker and defender

289
00:15:52,000 --> 00:15:54,666
doesn't carry moral weight.

290
00:15:54,733 --> 00:15:57,299
All security systems
are embedded in some

291
00:15:57,366 --> 00:15:59,033
broader social concept.

292
00:15:59,100 --> 00:16:00,399
We could have
the police attacking

293
00:16:00,466 --> 00:16:01,399
and criminals defending.

294
00:16:01,466 --> 00:16:02,766
We could have
criminals attacking and

295
00:16:02,833 --> 00:16:03,899
the police defending.

296
00:16:03,966 --> 00:16:07,533
To us, it's basically
all the same tech.

297
00:16:07,600 --> 00:16:10,000
But normally our
attackers and defenders are

298
00:16:10,066 --> 00:16:12,500
in different groups.

299
00:16:12,566 --> 00:16:15,866
This isn't true
with the tax code

300
00:16:15,933 --> 00:16:17,966
or political gerrymandering.

301
00:16:18,033 --> 00:16:23,633
The attackers are members of
the same society that's defending.

302
00:16:23,700 --> 00:16:27,299
The defenders are society
as a whole and the

303
00:16:27,366 --> 00:16:30,500
attackers are some
subset of them.

304
00:16:30,566 --> 00:16:33,266
Or worse, it's two groups
trying to game the same

305
00:16:33,333 --> 00:16:36,466
system, so each trying to
immunize the system to

306
00:16:36,533 --> 00:16:39,166
attacks by the other group
by leading vulnerable

307
00:16:39,233 --> 00:16:41,532
attacks to their
own attacks.

308
00:16:41,600 --> 00:16:45,566
And you can see this in
voting rights where the

309
00:16:45,633 --> 00:16:47,466
different groups try
to attack and defend

310
00:16:47,533 --> 00:16:49,299
at the same time.

311
00:16:49,366 --> 00:16:52,000
It's more about abstract
principles, notions of

312
00:16:52,066 --> 00:16:56,100
equality, justice,
and fairness.

313
00:16:56,166 --> 00:16:58,866
And this gets back to our
definition of the word hack.

314
00:16:58,933 --> 00:17:02,533
When a lobbyist gets a law
passed, have they hacked

315
00:17:02,600 --> 00:17:05,799
the system, or are they
just using it as intended?

316
00:17:08,366 --> 00:17:09,965
All right. Some more examples.

317
00:17:10,032 --> 00:17:14,299
Let's talk about hacks
of cognitive systems.

318
00:17:14,366 --> 00:17:17,799
Remember the security
adage that script kitties

319
00:17:17,866 --> 00:17:22,900
hack computers while smart
attackers hack people?

320
00:17:22,965 --> 00:17:25,565
Lots of attackers
hack people.

321
00:17:25,633 --> 00:17:30,833
Advertising is a hack of
our cognitive system of choice.

322
00:17:30,900 --> 00:17:34,166
It's always been psychological;
now it's scientific.

323
00:17:34,233 --> 00:17:36,166
Now it's targeted.

324
00:17:36,233 --> 00:17:37,899
Lots of people have
written about modern

325
00:17:37,966 --> 00:17:40,366
behavioral advertising and
how it affects our ability

326
00:17:40,433 --> 00:17:43,066
to rationally choose.

327
00:17:43,133 --> 00:17:45,700
It feels like
a hack to me.

328
00:17:45,766 --> 00:17:48,500
And kind of all of my
market and democracy

329
00:17:48,566 --> 00:17:53,500
examples really bubble up
to persuasion as a hack.

330
00:17:53,566 --> 00:17:56,733
Social media hacks our
attention by manufacturing

331
00:17:56,799 --> 00:17:59,900
outrage, by
being addictive.

332
00:17:59,966 --> 00:18:01,933
And AI and robotics
are going to hack our

333
00:18:02,000 --> 00:18:05,133
cognitive systems because
we all have a lot of

334
00:18:05,200 --> 00:18:07,599
cognitive shortcuts.

335
00:18:07,666 --> 00:18:13,265
Two over one is a face;
a face is a creature;

336
00:18:13,333 --> 00:18:16,633
language indicates
intelligence, emotion,

337
00:18:16,700 --> 00:18:19,599
intention, and so on.

338
00:18:19,666 --> 00:18:21,933
These are all really
reasonable cognitive

339
00:18:22,000 --> 00:18:23,966
shortcuts for the
environment we are

340
00:18:24,033 --> 00:18:28,632
involved in and they will
all fail with artificial

341
00:18:28,700 --> 00:18:30,099
people-like systems.

342
00:18:32,900 --> 00:18:35,500
All right. So this is a
lot of examples, but I really

343
00:18:35,566 --> 00:18:37,400
want to give you a feel
for sort of how I'm

344
00:18:37,466 --> 00:18:38,832
thinking about this.

345
00:18:38,900 --> 00:18:42,599
Let me talk about one
thing in a little more detail.

346
00:18:42,666 --> 00:18:46,200
So, last fall, I started
using computer security

347
00:18:46,266 --> 00:18:48,700
techniques to study
propaganda and misinformation.

348
00:18:48,766 --> 00:18:52,599
So I did this work with
political scientist Henry

349
00:18:52,666 --> 00:18:56,033
Theral at GW University.

350
00:18:56,099 --> 00:19:00,599
Here's our thinking:
Democracy can be thought

351
00:19:00,666 --> 00:19:06,399
of as an information
system, and we're using

352
00:19:06,466 --> 00:19:08,899
that to understand
the current waves of

353
00:19:08,966 --> 00:19:11,299
information attacks,
specifically this question.

354
00:19:11,366 --> 00:19:14,933
How is it that the same
disinformation campaigns

355
00:19:15,000 --> 00:19:18,400
that act as a stabilizing
influence in a country

356
00:19:18,466 --> 00:19:21,632
like Russia can be
destabilizing in the

357
00:19:21,700 --> 00:19:23,799
United States?

358
00:19:23,866 --> 00:19:27,033
And our answer is
that autocracies and

359
00:19:27,099 --> 00:19:31,533
democracies work differently
as information systems.

360
00:19:31,599 --> 00:19:33,633
So let me explain.

361
00:19:33,700 --> 00:19:38,000
There are two types of
knowledge that society

362
00:19:38,066 --> 00:19:41,500
uses to solve
political problems.

363
00:19:41,566 --> 00:19:44,433
The first is what I call
common political knowledge.

364
00:19:44,500 --> 00:19:48,299
That's information that
society broadly agrees on.

365
00:19:48,366 --> 00:19:51,366
It's things like who the
rulers are, how they're

366
00:19:51,433 --> 00:19:54,400
chosen, how
government functions.

367
00:19:54,466 --> 00:19:56,832
That's common
political knowledge.

368
00:19:56,900 --> 00:19:59,900
Then there is contested
political knowledge, and

369
00:19:59,966 --> 00:20:03,733
that's the stuff
we disagree about.

370
00:20:03,799 --> 00:20:06,099
So it's things like how
much of a role should our

371
00:20:06,166 --> 00:20:08,533
government play
in our economy?

372
00:20:08,599 --> 00:20:11,265
What sorts of regulations
are beneficial and

373
00:20:11,333 --> 00:20:12,066
what are harmful?

374
00:20:12,133 --> 00:20:14,400
What should the
tax rates be?

375
00:20:14,466 --> 00:20:16,199
That's the stuff
we disagree about.

376
00:20:16,266 --> 00:20:19,400
That's contested
political knowledge.

377
00:20:19,466 --> 00:20:21,799
So democracies and
autocracies have different

378
00:20:21,866 --> 00:20:26,500
needs for common and
contested political knowledge.

379
00:20:26,566 --> 00:20:29,700
Democracies draw on
disagreements within their

380
00:20:29,766 --> 00:20:32,566
populations to
solve problems.

381
00:20:32,633 --> 00:20:35,900
That's how we work.

382
00:20:35,966 --> 00:20:38,565
But in order for it to
work, there needs to be

383
00:20:38,633 --> 00:20:41,233
common political knowledge
on how governments

384
00:20:41,299 --> 00:20:43,766
function and how political
leaders are chosen.

385
00:20:45,200 --> 00:20:47,433
All right? We have to know how
elections work so we can

386
00:20:47,500 --> 00:20:50,566
campaign for our side.

387
00:20:50,633 --> 00:20:55,966
And through that process,
we solve political problems.

388
00:20:56,033 --> 00:20:59,599
In an autocracy, you need
common political knowledge

389
00:20:59,666 --> 00:21:04,966
over who is in charge,
but they tend to suppress

390
00:21:05,033 --> 00:21:07,699
other common political
knowledge about how the

391
00:21:07,766 --> 00:21:10,700
government is actually
working, about other

392
00:21:10,766 --> 00:21:13,733
political movements
and their support.

393
00:21:13,799 --> 00:21:16,533
They benefit from those
things being contested.

394
00:21:16,599 --> 00:21:21,866
So that difference in
information usage leads to

395
00:21:21,933 --> 00:21:24,566
a difference in threat
models, which leads to a

396
00:21:24,633 --> 00:21:27,299
difference in
vulnerabilities.

397
00:21:27,366 --> 00:21:30,633
So authoritarian regimes
are vulnerable to

398
00:21:30,700 --> 00:21:34,333
information attacks that
challenge their monopoly on

399
00:21:34,400 --> 00:21:36,433
common political knowledge.

400
00:21:36,500 --> 00:21:39,733
That is why an open
internet is so dangerous

401
00:21:39,799 --> 00:21:42,066
to an autocracy.

402
00:21:42,133 --> 00:21:45,666
Democracies are vulnerable
to information attacks

403
00:21:45,733 --> 00:21:49,399
that turn common political
knowledge into contested

404
00:21:49,466 --> 00:21:52,966
political knowledge, which
is why you're seeing

405
00:21:53,033 --> 00:21:55,765
information attacks in the
United States and Europe

406
00:21:55,833 --> 00:22:00,700
that try to cast doubt on
the fairness of elections,

407
00:22:00,766 --> 00:22:03,500
the fairness of the police
and courts, the fairness

408
00:22:03,566 --> 00:22:05,566
of the Census.

409
00:22:06,966 --> 00:22:10,899
The same information
attack, but they increase

410
00:22:10,966 --> 00:22:13,966
the stability in one
regime and decrease the

411
00:22:14,033 --> 00:22:16,666
stability in another.

412
00:22:16,733 --> 00:22:18,933
Here's another way of
saying this: There is

413
00:22:19,000 --> 00:22:20,433
something in political
science called a

414
00:22:20,500 --> 00:22:23,599
dictator's dilemma and it
kind of goes like this.

415
00:22:23,666 --> 00:22:26,799
As a dictator, you need
accurate information about

416
00:22:26,866 --> 00:22:30,066
how your country is
running, but that accurate

417
00:22:30,133 --> 00:22:32,966
information is also
dangerous because it tells

418
00:22:33,033 --> 00:22:36,966
everybody how not well
your country is running.

419
00:22:37,033 --> 00:22:39,366
So you're always trying
to balance this need for

420
00:22:39,433 --> 00:22:43,433
information with this need
to suppress the information.

421
00:22:43,500 --> 00:22:47,533
There is a corresponding
democracies dilemma, and

422
00:22:47,599 --> 00:22:51,233
that's this: It's the same
open flows of information

423
00:22:51,299 --> 00:22:54,900
that are necessary for
democracy to function are

424
00:22:54,966 --> 00:22:56,966
also potential
attack vectors.

425
00:22:59,233 --> 00:23:01,366
This feels like a useful
way of thinking about

426
00:23:01,433 --> 00:23:03,700
propaganda and it's
something we are

427
00:23:03,766 --> 00:23:04,700
continuing to develop.

428
00:23:04,766 --> 00:23:08,200
So let's hack some other
cognitive systems.

429
00:23:08,266 --> 00:23:10,433
Fear.

430
00:23:10,500 --> 00:23:14,233
I've written years ago
that our sense of fear is

431
00:23:14,299 --> 00:23:17,433
optimized for living in
small family groups in the

432
00:23:17,500 --> 00:23:22,099
East African highlands in
100,000 BC and not well

433
00:23:22,166 --> 00:23:25,533
designed for 2020
San Francisco.

434
00:23:25,599 --> 00:23:29,332
Terrorism directly targets
our cognitive shortcuts

435
00:23:29,400 --> 00:23:30,400
about fear.

436
00:23:30,466 --> 00:23:35,666
It's terrifying, vivid,
spectacular, random.

437
00:23:35,733 --> 00:23:38,299
It's basically tailormade
for us to exaggerate the

438
00:23:38,366 --> 00:23:41,733
risk and overreact.

439
00:23:41,799 --> 00:23:43,866
Right? Trust.

440
00:23:43,933 --> 00:23:47,599
Our intuitions are based
on trusting individuals

441
00:23:47,666 --> 00:23:52,733
peer to peer, trusting
organizations, brands.

442
00:23:52,799 --> 00:23:54,900
It's not what
we're used to.

443
00:23:54,966 --> 00:23:59,399
And this can be misused by
others to manipulate us.

444
00:23:59,466 --> 00:24:02,166
We naturally
trust authority.

445
00:24:02,233 --> 00:24:03,933
Something in print
is an authority.

446
00:24:04,000 --> 00:24:06,133
The computer said
so is an authority.

447
00:24:06,200 --> 00:24:09,700
Lots of examples of those
trust heuristics being attacked.

448
00:24:09,766 --> 00:24:13,500
You can even think of
junk food as hacking our

449
00:24:13,566 --> 00:24:17,066
biological systems of food
desirability because our

450
00:24:17,133 --> 00:24:21,466
security is based on our
100,000-year-old diet, not

451
00:24:21,533 --> 00:24:24,899
on modern processed
food production.

452
00:24:24,966 --> 00:24:28,299
The change in the threat model
has led to a vulnerability.

453
00:24:28,366 --> 00:24:32,799
I think any industry
that has been upended by

454
00:24:32,866 --> 00:24:38,733
technology is worth
examining from this perspective.

455
00:24:38,799 --> 00:24:40,866
Our system for choosing
elected officials, not

456
00:24:40,933 --> 00:24:42,733
voting specifically,
but election process in

457
00:24:42,799 --> 00:24:46,299
general, the news
industry, distance

458
00:24:46,366 --> 00:24:48,599
learning and
higher education.

459
00:24:48,666 --> 00:24:51,099
Any social system that has
slipped into complexity is

460
00:24:51,166 --> 00:24:53,066
worthy of examination.

461
00:24:53,133 --> 00:24:55,000
The tech industry,
of course, the media

462
00:24:55,066 --> 00:24:57,200
industry,
financial markets.

463
00:24:59,733 --> 00:25:03,000
In all of these cases,
differences in degree lead

464
00:25:03,066 --> 00:25:06,700
to differences in kind, and
they have security ramifications.

465
00:25:06,766 --> 00:25:09,200
We know this is true
for mass surveillance.

466
00:25:09,266 --> 00:25:12,200
I think it's true for a
lot of other things as well.

467
00:25:14,933 --> 00:25:17,066
The ability of people to
coordinate on the internet

468
00:25:17,133 --> 00:25:20,000
has changed the
nature of attack.

469
00:25:20,066 --> 00:25:22,000
Remember the great -- I
don't know if it's great

470
00:25:22,066 --> 00:25:25,233
-- the story of
Microsoft's chat bot Tay?

471
00:25:25,299 --> 00:25:28,633
Turned into a racist,
misogynistic Nazi in less

472
00:25:28,700 --> 00:25:32,333
than 24 hours by a
coordinated attack by Fortran.

473
00:25:34,400 --> 00:25:36,233
More recently, the people
running the Democratic

474
00:25:36,299 --> 00:25:40,133
caucuses in Iowa didn't
realize that publicizing

475
00:25:40,200 --> 00:25:43,799
their help number would
leave them vulnerable to

476
00:25:43,866 --> 00:25:46,866
denial of service attack.

477
00:25:46,933 --> 00:25:50,433
We have moved in a lot of
places from good faith

478
00:25:50,500 --> 00:25:53,599
systems to ones where
people and institutions

479
00:25:53,666 --> 00:25:55,700
behave strategically.

480
00:25:55,766 --> 00:25:58,599
And security against that
stuff is what we're good at.

481
00:25:58,666 --> 00:26:03,299
I think power
matters here.

482
00:26:05,666 --> 00:26:08,566
All of these hacks are
about rearranging power,

483
00:26:08,633 --> 00:26:10,833
just as cryptography is
about rearranging power.

484
00:26:13,033 --> 00:26:15,765
In her great book Between
Truth and Power, Julie E.

485
00:26:15,833 --> 00:26:18,766
Cohen, law professor,
wrote that in the realm of

486
00:26:18,833 --> 00:26:21,566
government, power
interprets regulation as

487
00:26:21,633 --> 00:26:24,433
damage and
routes around it.

488
00:26:24,500 --> 00:26:27,866
Once the powerful
understood that they had

489
00:26:27,933 --> 00:26:31,233
to hack the regulatory
process, they developed

490
00:26:31,299 --> 00:26:37,433
competence to do just that,
and that impedes solutions.

491
00:26:37,500 --> 00:26:39,266
So elections are
a good example.

492
00:26:39,333 --> 00:26:40,599
I have already mentioned
money and politics are

493
00:26:40,666 --> 00:26:42,500
changing the threat model.

494
00:26:42,566 --> 00:26:46,433
So most U.S. election spending
takes place on television,

495
00:26:46,500 --> 00:26:49,299
secondarily on the internet.

496
00:26:49,366 --> 00:26:51,666
Now there are ways
to regulate this.

497
00:26:51,733 --> 00:26:53,633
Other countries restrict
advertising to some

498
00:26:53,700 --> 00:26:56,233
small-time window, and there
are other things they do.

499
00:26:56,299 --> 00:27:00,266
But the platforms on which
this debate would occur

500
00:27:00,333 --> 00:27:03,799
are the very ones
that profit most

501
00:27:03,866 --> 00:27:06,533
from political advertising.

502
00:27:06,599 --> 00:27:09,332
And power will fight
security if it's

503
00:27:09,400 --> 00:27:11,400
against their interests.

504
00:27:11,466 --> 00:27:14,399
Think about the FBI
versus strong encryption.

505
00:27:14,466 --> 00:27:18,433
Those in power will fight
to retain their power.

506
00:27:18,500 --> 00:27:20,566
So one last concept
I want to look at.

507
00:27:20,633 --> 00:27:22,166
The notion of
a class break.

508
00:27:22,233 --> 00:27:26,633
So, in general, and we
know the story, computers

509
00:27:26,700 --> 00:27:30,200
replace expertise and
skill with an ability.

510
00:27:30,266 --> 00:27:35,000
You used to have to train
to be a calligrapher.

511
00:27:35,066 --> 00:27:37,599
Now you can use
any font you want.

512
00:27:37,666 --> 00:27:39,332
Driving is
currently a skill.

513
00:27:39,400 --> 00:27:42,200
How long will that last?

514
00:27:42,266 --> 00:27:44,500
This is also true
for security.

515
00:27:44,566 --> 00:27:48,566
One expert finds a
Zero-day, publishes it,

516
00:27:48,633 --> 00:27:51,766
now anyone can use it,
especially if it's

517
00:27:51,833 --> 00:27:53,833
embedded in a
software program.

518
00:27:55,633 --> 00:27:58,400
So this generalizes when
you deal with complex

519
00:27:58,466 --> 00:28:00,099
sociotechnical systems.

520
00:28:00,166 --> 00:28:03,466
Someone invented the
double Irish with a Dutch

521
00:28:03,533 --> 00:28:07,033
sandwich, but now
it's a class break.

522
00:28:07,099 --> 00:28:10,066
Once the loophole was
found, any company can

523
00:28:10,133 --> 00:28:12,099
take advantage of it.

524
00:28:12,166 --> 00:28:14,566
Misinformation on social
networks is a class break.

525
00:28:16,733 --> 00:28:18,966
And Russia might have
invented the techniques;

526
00:28:19,033 --> 00:28:21,399
now everyone can do it.

527
00:28:21,466 --> 00:28:23,299
Different techniques of
psychological manipulation

528
00:28:23,366 --> 00:28:25,366
are class breaks.

529
00:28:26,833 --> 00:28:29,500
The notion of a class
break drastically changes

530
00:28:29,566 --> 00:28:30,933
how we need to
think about risk.

531
00:28:31,000 --> 00:28:34,533
And I don't think that's
something well understood

532
00:28:34,599 --> 00:28:37,233
outside of our world.

533
00:28:37,299 --> 00:28:42,633
So we also need to
generalize the solutions

534
00:28:42,700 --> 00:28:44,599
we routinely use.

535
00:28:44,666 --> 00:28:46,033
I'll hit on a few of them.

536
00:28:46,099 --> 00:28:49,500
Transparency is a big one.

537
00:28:49,566 --> 00:28:52,666
And we see that in the
greater world, open

538
00:28:52,733 --> 00:28:56,599
government laws,
mandatory public tax and

539
00:28:56,666 --> 00:29:02,700
informational filings,
ingredient labels on products.

540
00:29:02,766 --> 00:29:05,866
Truth in lending
statements on financial

541
00:29:05,933 --> 00:29:09,166
products reduce corporate
excesses, even if no

542
00:29:09,233 --> 00:29:11,265
one reads them.

543
00:29:11,333 --> 00:29:13,666
I think we can achieve a
lot through transparency.

544
00:29:15,700 --> 00:29:20,099
We have other solutions in
our tech toolkit, defense

545
00:29:20,166 --> 00:29:23,899
in-depth,
compartmentalization,

546
00:29:23,966 --> 00:29:28,765
isolation, segmenting,
sandboxing, audit,

547
00:29:28,833 --> 00:29:33,000
incident response, patching.

548
00:29:33,066 --> 00:29:35,299
Iteration matters here.

549
00:29:35,366 --> 00:29:37,400
We know we never actually
solve a security problem;

550
00:29:37,466 --> 00:29:39,966
we iterate.

551
00:29:40,033 --> 00:29:45,632
Is there some way to iterate
law, to have extensible law?

552
00:29:45,700 --> 00:29:49,400
Can we implement some
rapid feedback in our laws

553
00:29:49,466 --> 00:29:51,466
and regulations?

554
00:29:52,500 --> 00:29:56,099
Resilience is an
important concept.

555
00:29:56,166 --> 00:29:58,633
It's how we deal with
systems on a continuous

556
00:29:58,700 --> 00:30:02,966
attack, which is the normal
situation in social systems.

557
00:30:06,966 --> 00:30:10,699
So when I wrote Beyond
Fear back in 2003, I gave

558
00:30:10,766 --> 00:30:14,233
five steps to evaluate
a security system.

559
00:30:14,299 --> 00:30:15,533
What are you
trying to protect?

560
00:30:15,599 --> 00:30:18,332
What are the risks?

561
00:30:18,400 --> 00:30:21,833
How well does your
solution mitigate the risks?

562
00:30:21,900 --> 00:30:25,233
What other risks does
your solution cause?

563
00:30:25,299 --> 00:30:28,799
And what are the
non-security trade-offs?

564
00:30:28,866 --> 00:30:32,466
I think we can generalize
that framework.

565
00:30:32,533 --> 00:30:35,265
Systems that are
decentralized and multiply

566
00:30:35,333 --> 00:30:39,133
controlled, they're
a lot harder to fix.

567
00:30:39,200 --> 00:30:40,700
But we have
experience with that.

568
00:30:40,766 --> 00:30:42,766
We have a lot of
experience with that.

569
00:30:45,099 --> 00:30:49,700
So all of this leads
to some big questions.

570
00:30:49,766 --> 00:30:54,566
What should policy for the
information economy look like?

571
00:30:54,633 --> 00:30:58,166
What components will
rule of law 2.0 have?

572
00:31:00,299 --> 00:31:02,366
What should economic
institutions for the

573
00:31:02,433 --> 00:31:05,200
information
economy look like?

574
00:31:05,266 --> 00:31:07,700
Industrial area
capitalism is looking

575
00:31:07,766 --> 00:31:10,833
increasingly unlikely.

576
00:31:10,900 --> 00:31:14,400
How do we address the
problems that are baked

577
00:31:14,466 --> 00:31:17,199
into our technological
infrastructure without

578
00:31:17,266 --> 00:31:19,599
destroying what
it provides?

579
00:31:19,666 --> 00:31:22,765
And one problem I see
immediately is we don't

580
00:31:22,833 --> 00:31:25,933
have policy institutions
with footprints to match

581
00:31:26,000 --> 00:31:27,599
the technologies.

582
00:31:27,666 --> 00:31:31,700
And Facebook is global, yet
it's only regulated nationally.

583
00:31:31,766 --> 00:31:36,433
Those that have been
around for a while

584
00:31:36,500 --> 00:31:38,599
remember when tech
used to be a solution;

585
00:31:38,666 --> 00:31:40,500
now it's the problem.

586
00:31:40,566 --> 00:31:42,566
In reality, it's both.

587
00:31:44,433 --> 00:31:47,666
And our problem tends
to be social problems

588
00:31:47,733 --> 00:31:50,700
masquerading as tech
problems and tech

589
00:31:50,766 --> 00:31:52,966
solutions masquerading
as social solutions.

590
00:31:53,033 --> 00:31:56,565
And we need to better
integrate tech and policy.

591
00:31:58,566 --> 00:32:02,266
Computer security has long
integrated tech and people.

592
00:32:02,333 --> 00:32:04,566
I think we can do this for
a much broader set of systems.

593
00:32:04,633 --> 00:32:08,666
I think we need to upend
the idea that society is

594
00:32:08,733 --> 00:32:12,666
somehow solid, stable,
and naturally just there.

595
00:32:12,733 --> 00:32:14,666
We build society.

596
00:32:14,733 --> 00:32:18,200
Increasingly, we build
it with technology.

597
00:32:18,266 --> 00:32:22,266
And technology is not on
some inevitable trajectory.

598
00:32:22,333 --> 00:32:23,966
It interacts with the
country's political and

599
00:32:24,033 --> 00:32:25,533
social institutions.

600
00:32:25,599 --> 00:32:27,599
So it's not just one
effective technology.

601
00:32:27,666 --> 00:32:31,233
It depends on the
details of society.

602
00:32:31,299 --> 00:32:36,500
Computer security has already
had an impact on technology.

603
00:32:36,566 --> 00:32:38,700
And now we need to have
an impact on the broader

604
00:32:38,766 --> 00:32:41,033
public interest.

605
00:32:41,099 --> 00:32:43,866
So this is what I'm
working on right now.

606
00:32:43,933 --> 00:32:47,433
Currently, it
is this talk.

607
00:32:47,500 --> 00:32:50,400
It will probably become
some articles and essays.

608
00:32:50,466 --> 00:32:54,199
Maybe it'll be a book.

609
00:32:54,266 --> 00:32:57,299
I think this framework
has some value.

610
00:32:57,366 --> 00:33:00,133
It gives structure to
thinking about adversaries

611
00:33:00,200 --> 00:33:03,700
inside a social system,
how we delineate the rules

612
00:33:03,766 --> 00:33:06,500
of the game, how people
hack the meta game, how

613
00:33:06,566 --> 00:33:09,900
they hack the metagame, and
how we can secure all of that.

614
00:33:12,200 --> 00:33:14,599
I think it's easy to get
carried away with this

615
00:33:14,666 --> 00:33:17,000
kind of thinking.

616
00:33:17,066 --> 00:33:19,299
All models are wrong,
but some are useful is

617
00:33:19,366 --> 00:33:21,433
the great quote.

618
00:33:21,500 --> 00:33:24,166
Which systems are
analogous to network

619
00:33:24,233 --> 00:33:26,765
computers and
which are not?

620
00:33:26,833 --> 00:33:29,200
When are innovations
analogous to a hack with

621
00:33:29,266 --> 00:33:31,700
security implications and
when are they just novel

622
00:33:31,766 --> 00:33:35,766
uses or innovations
or social progress?

623
00:33:35,833 --> 00:33:37,233
There are bugs
in everything.

624
00:33:37,299 --> 00:33:39,200
When is a bug a
vulnerability?

625
00:33:39,266 --> 00:33:41,799
When is a vulnerability
deserving of attention?

626
00:33:41,866 --> 00:33:43,200
When is it catastrophic?

627
00:33:43,266 --> 00:33:48,099
There's probably a good
analogy to cancer here.

628
00:33:48,166 --> 00:33:50,866
Everybody has cancerous
cells in their body all

629
00:33:50,933 --> 00:33:54,533
the time, but most
cancers don't grow.

630
00:33:54,599 --> 00:33:57,433
It depends on the
environment and other

631
00:33:57,500 --> 00:33:58,533
external factors.

632
00:33:58,599 --> 00:34:02,000
I think it's the
same in our field.

633
00:34:02,066 --> 00:34:04,133
The difference, of course,
is that cancer cells are

634
00:34:04,200 --> 00:34:08,833
not intelligent malicious,
adaptive adversaries, and

635
00:34:08,900 --> 00:34:10,900
that's who we're dealing with.

636
00:34:13,366 --> 00:34:15,199
I also think it's
important to have humility

637
00:34:15,266 --> 00:34:17,133
in this endeavor.

638
00:34:17,199 --> 00:34:21,933
All the examples I used
are large policy issues

639
00:34:22,000 --> 00:34:25,599
with history and expertise
and a huge body of

640
00:34:25,666 --> 00:34:27,633
existing knowledge.

641
00:34:27,699 --> 00:34:30,533
And we just can't think
that we can barge in and

642
00:34:30,599 --> 00:34:33,966
solve the world's problems
just because we're good at

643
00:34:34,033 --> 00:34:38,132
the problems in
our own world.

644
00:34:38,199 --> 00:34:40,833
The literature is filled
with intellectuals who are

645
00:34:40,900 --> 00:34:44,266
experts in their field,
overgeneralized, and fell

646
00:34:44,333 --> 00:34:45,766
flat on their face.

647
00:34:45,833 --> 00:34:48,733
Kind of want
to avoid that.

648
00:34:48,800 --> 00:34:51,199
And the last thing we want
is another tech can fix

649
00:34:51,266 --> 00:34:55,633
everything solution,
especially coming from the

650
00:34:55,699 --> 00:34:58,699
monoculture of Silicon
Valley, at the expense and

651
00:34:58,766 --> 00:35:01,866
lives of, like,
everybody else.

652
00:35:01,933 --> 00:35:03,400
I think we need a lot
of people from a lot of

653
00:35:03,466 --> 00:35:05,732
disciplines working
together to solve any of

654
00:35:05,800 --> 00:35:09,000
this, but I like tech
to be involved in these

655
00:35:09,066 --> 00:35:11,066
broader conversations.

656
00:35:12,900 --> 00:35:17,433
So I once heard this quote
about mathematical literacy.

657
00:35:17,500 --> 00:35:19,766
It's not that math
can solve the world's

658
00:35:19,833 --> 00:35:23,000
problems; it's just that
the world's problems would

659
00:35:23,066 --> 00:35:25,866
be easier to solve if
everyone just knew a

660
00:35:25,933 --> 00:35:27,933
little more math.

661
00:35:28,766 --> 00:35:31,933
I think the same thing
holds true for security.

662
00:35:32,000 --> 00:35:34,599
It's not that the security
mindset or security

663
00:35:34,666 --> 00:35:38,199
thinking will solve the
world's problems; it's

664
00:35:38,266 --> 00:35:40,300
just that the world's
problems would be easier to

665
00:35:40,366 --> 00:35:43,199
solve if everyone
just understood a

666
00:35:43,266 --> 00:35:45,666
little more security.

667
00:35:45,733 --> 00:35:47,099
And this is important.

668
00:35:47,166 --> 00:35:49,533
So I have one final
example about a hack

669
00:35:49,599 --> 00:35:51,233
against the tax code.

670
00:35:51,300 --> 00:35:53,800
In January, the New York
Times reported about this

671
00:35:53,866 --> 00:35:55,833
new kind of tax fraud.

672
00:35:55,900 --> 00:35:58,766
It's called cum ex
trading, which is Latin

673
00:35:58,833 --> 00:35:59,800
for with/without.

674
00:35:59,866 --> 00:36:02,099
I'm going to read a
sentence from the article.

675
00:36:02,166 --> 00:36:05,300
Through careful timing
and the coordination of a

676
00:36:05,366 --> 00:36:07,933
dozen different
transactions, cum ex

677
00:36:08,000 --> 00:36:11,866
trades produce two refunds
for dividend tax paid on

678
00:36:11,933 --> 00:36:14,233
one basket of stocks.

679
00:36:14,300 --> 00:36:18,033
That's one refund obtained
legally and the second

680
00:36:18,099 --> 00:36:19,866
illegally received.

681
00:36:19,933 --> 00:36:20,833
It was a hack.

682
00:36:20,900 --> 00:36:25,433
This was something
the system permitted,

683
00:36:25,500 --> 00:36:29,533
unanticipated and unintended
by the system's creators.

684
00:36:29,599 --> 00:36:34,500
From 2006 to 2011, the
bankers, lawyers, and

685
00:36:34,566 --> 00:36:38,232
investors who used this
hack made off with $60

686
00:36:38,300 --> 00:36:40,300
billion from EU countries.

687
00:36:43,266 --> 00:36:46,533
Right now, there are
prosecutions, primarily in

688
00:36:46,599 --> 00:36:51,066
Germany, and it is unclear
whether the law was broken.

689
00:36:54,099 --> 00:36:57,099
The hack is
permitted by the system.

690
00:36:57,166 --> 00:36:59,133
They're debating whether
there is some metasystem

691
00:36:59,199 --> 00:37:02,733
of don't do anything this
blatantly horrible that

692
00:37:02,800 --> 00:37:07,133
they can convict the
person of, or we have a

693
00:37:07,199 --> 00:37:10,199
vulnerability in our laws
that we need to patch.

694
00:37:12,599 --> 00:37:15,866
So a year ago, I stood on
this same stage and talked

695
00:37:15,933 --> 00:37:18,566
about the need for public
interest technologists,

696
00:37:18,633 --> 00:37:21,098
for technologists to
understand the social

697
00:37:21,166 --> 00:37:23,599
ramifications of their
work, for technologists to

698
00:37:23,666 --> 00:37:26,400
get involved in public
policy, to bridge the gap

699
00:37:26,466 --> 00:37:28,566
between tech and policy.

700
00:37:28,633 --> 00:37:30,899
So this is a piece of it.

701
00:37:30,966 --> 00:37:34,199
Hacking society and
securing against those

702
00:37:34,266 --> 00:37:37,766
hacks is how we in the
computer security field

703
00:37:37,833 --> 00:37:41,566
can use our expertise for
broader social progress.

704
00:37:41,633 --> 00:37:44,832
And I think we
have to do that.

705
00:37:44,900 --> 00:37:45,566
So thank you.

706
00:37:55,933 --> 00:37:57,333
>> BRUCE SCHNEIER: So I
left a bunch of time for

707
00:37:57,400 --> 00:37:59,066
questions and comments
because I really want

708
00:37:59,133 --> 00:38:00,933
questions and comments.

709
00:38:01,000 --> 00:38:04,266
This is a work in progress
and something I'm thinking

710
00:38:04,333 --> 00:38:08,233
about, so I'm curious
what you all think.

711
00:38:08,300 --> 00:38:12,800
There are two microphones
that everyone is scared to

712
00:38:12,866 --> 00:38:14,166
get in front of.

713
00:38:14,233 --> 00:38:15,166
Here comes one person.

714
00:38:15,233 --> 00:38:17,366
And if you don't want
to get in front of a

715
00:38:17,433 --> 00:38:18,599
microphone, email me.

716
00:38:18,666 --> 00:38:23,300
If you have an idea, a
rebuttal, another example,

717
00:38:23,366 --> 00:38:24,566
send it to me.

718
00:38:24,633 --> 00:38:25,598
I'm really curious.

719
00:38:25,666 --> 00:38:26,900
I'll chase down
the details.

720
00:38:26,966 --> 00:38:31,799
But anything that is
sparked by this talk,

721
00:38:31,866 --> 00:38:33,866
please tell me. Yes?

722
00:38:33,933 --> 00:38:36,699
>> AUDIENCE: So I have
an idea of comparing the

723
00:38:36,766 --> 00:38:41,333
social systems to system
when we come to securing

724
00:38:41,400 --> 00:38:42,466
both of those.

725
00:38:42,533 --> 00:38:45,566
I would like to get your
opinion on what I call the

726
00:38:45,633 --> 00:38:50,732
permit hack, where in the
society through fear of

727
00:38:50,800 --> 00:38:54,866
all of these kinds of
things move into a

728
00:38:54,933 --> 00:38:58,966
situation where the actual
objectives of the system

729
00:38:59,033 --> 00:39:01,232
completely change, right?

730
00:39:01,300 --> 00:39:06,000
In IT, we only have one
variable where the system

731
00:39:06,066 --> 00:39:09,533
itself that we need to
defend is very clear.

732
00:39:09,599 --> 00:39:13,566
And society of the
objective of what we are

733
00:39:13,633 --> 00:39:16,232
trying to do keeps
changing, right?

734
00:39:16,300 --> 00:39:17,733
>> BRUCE SCHNEIER: Yeah.
I think it's less different

735
00:39:17,800 --> 00:39:18,800
than you want.

736
00:39:18,866 --> 00:39:20,966
We like to think that
our systems end at the

737
00:39:21,033 --> 00:39:24,500
keyboard and screen, but,
in fact, they don't.

738
00:39:24,566 --> 00:39:28,165
We are used to systems --
internet was invented with

739
00:39:28,233 --> 00:39:29,800
one particular threat
model and that completely

740
00:39:29,866 --> 00:39:33,000
changed, and we have an
internet designed for a

741
00:39:33,066 --> 00:39:35,232
very benign threat
environment being used for

742
00:39:35,300 --> 00:39:35,933
critical infrastructure.

743
00:39:36,000 --> 00:39:38,099
We're used to
system drift.

744
00:39:38,166 --> 00:39:41,666
I think we're used to
systems that expand.

745
00:39:41,733 --> 00:39:44,933
We often don't think in
that way, but those of us,

746
00:39:45,000 --> 00:39:47,199
I think, who do security
well are constantly

747
00:39:47,266 --> 00:39:48,699
thinking about that.

748
00:39:48,766 --> 00:39:51,400
But, yes, I think there is
a difference that social

749
00:39:51,466 --> 00:39:53,165
systems tend
to evolve more.

750
00:39:53,233 --> 00:39:55,066
They are not
deliberately created.

751
00:39:55,133 --> 00:39:58,000
Who created the
market economy?

752
00:39:58,066 --> 00:40:02,399
Well, it kind of showed up
when it was the right time.

753
00:40:02,466 --> 00:40:05,699
We know who created our
constitution and we can

754
00:40:05,766 --> 00:40:08,599
look at their debates and
really learn about the

755
00:40:08,666 --> 00:40:10,400
threat model they were
thinking about, the

756
00:40:10,466 --> 00:40:12,633
vulnerabilities they were
looking at, what they missed.

757
00:40:12,699 --> 00:40:13,599
But I think you're right.

758
00:40:13,666 --> 00:40:18,266
This is going to be a
tough expansion because of

759
00:40:18,333 --> 00:40:21,766
those fuzzy borders.

760
00:40:21,833 --> 00:40:23,533
I'm not sure of the
answer, but I think it's

761
00:40:23,599 --> 00:40:25,000
still worth poking at.

762
00:40:26,833 --> 00:40:29,366
>> DAN WOODS: Dan Woods
from Early Adopter Research.

763
00:40:29,433 --> 00:40:33,099
Do you know of any book
that explains the kind of

764
00:40:33,166 --> 00:40:37,666
basics of political theory
from utilitarianism, lock,

765
00:40:37,733 --> 00:40:41,433
so all of these things for
technologists so that this

766
00:40:41,500 --> 00:40:44,533
could be -- they could
then do a better job of

767
00:40:44,599 --> 00:40:48,300
mapping what we know to
what the political and

768
00:40:48,366 --> 00:40:49,033
social frames know?

769
00:40:49,099 --> 00:40:50,699
>> BRUCE SCHNEIER:
It's funny.

770
00:40:50,766 --> 00:40:53,966
I teach at the Harvard
Kennedy school and teach

771
00:40:54,033 --> 00:40:56,433
tech to policy kids.

772
00:40:56,500 --> 00:40:59,099
We're constantly writing
these papers like machine

773
00:40:59,166 --> 00:41:00,900
learning for policymakers.

774
00:41:00,966 --> 00:41:04,598
You want the other one,
like social systems for techies.

775
00:41:04,666 --> 00:41:05,500
I don't know.

776
00:41:05,566 --> 00:41:06,832
That's a great idea.

777
00:41:06,900 --> 00:41:09,699
I have been reading
political theory books.

778
00:41:09,766 --> 00:41:11,766
The way you do this is
you go online, look for

779
00:41:11,833 --> 00:41:13,733
political theory classes
at universities, and buy

780
00:41:13,800 --> 00:41:16,000
their textbooks.

781
00:41:16,066 --> 00:41:17,698
So I have been
reading a bunch.

782
00:41:17,766 --> 00:41:18,833
I don't remember names.

783
00:41:18,900 --> 00:41:21,133
But there are political
theory books that are used in

784
00:41:21,199 --> 00:41:23,833
these undergraduate classes
that go into all of that.

785
00:41:23,900 --> 00:41:25,133
They are not
written for techies.

786
00:41:25,199 --> 00:41:28,066
They're written for
humanities majors.

787
00:41:28,133 --> 00:41:29,732
But one for a techie?

788
00:41:29,800 --> 00:41:31,766
That's a great idea.

789
00:41:31,833 --> 00:41:34,199
If you need a project, I
would love to read it.

790
00:41:34,266 --> 00:41:35,099
>> DAN WOODS: Okay.

791
00:41:35,166 --> 00:41:35,866
>> BRUCE SCHNEIER:
It's done.

792
00:41:35,933 --> 00:41:37,199
>> DAN WOODS:
I'll get started.

793
00:41:37,266 --> 00:41:39,400
>> BRUCE SCHNEIER: Okay.
Let me know next year.

794
00:41:39,466 --> 00:41:42,400
>> ALEX ZERBRINA: Hi, Bruce.
Thank you for speaking today.

795
00:41:42,466 --> 00:41:43,900
My name is Alex Zerbrina.

796
00:41:43,966 --> 00:41:46,000
I am currently a student
at San Francisco State

797
00:41:46,066 --> 00:41:48,433
University, and I'm a
political science major

798
00:41:48,500 --> 00:41:50,599
who specializes
in terrorism.

799
00:41:50,666 --> 00:41:52,300
>> BRUCE SCHNEIER:All right.
Tell me why I'm all wrong.

800
00:41:52,366 --> 00:41:54,699
>> ALEX ZERBRINA: Oh, no,
no, no. I'm not saying that.

801
00:41:54,766 --> 00:41:56,233
>> BRUCE SCHNEIER: This
is my nightmare scenario,

802
00:41:56,300 --> 00:41:57,633
someone who
knows something.

803
00:41:57,699 --> 00:41:59,433
>> ALEX ZERBRINA: No, no,
I'm not going to tell you that.

804
00:41:59,500 --> 00:42:00,900
No, I'm not going
to tell you that.

805
00:42:00,966 --> 00:42:03,633
What I wanted to know
is that you spoke about

806
00:42:03,699 --> 00:42:07,000
terrorism and attacks that
seem to be random, but

807
00:42:07,066 --> 00:42:08,000
they are really not.

808
00:42:08,066 --> 00:42:10,066
How do you think we should
prevent those attacks,

809
00:42:10,133 --> 00:42:13,198
especially if they use
technology such as

810
00:42:13,266 --> 00:42:15,199
terrorists recruiting
on, say, Twitter?

811
00:42:15,266 --> 00:42:18,333
>> BRUCE SCHNEIER: This is
not really the topic of

812
00:42:18,400 --> 00:42:20,966
the talk, but the
answer is you can't.

813
00:42:23,000 --> 00:42:26,666
I mean, you know, random acts
of violence cannot be prevented.

814
00:42:26,733 --> 00:42:29,866
And that's, in a sense,
why it's so fearful.

815
00:42:29,933 --> 00:42:36,199
And, unfortunately, I
think the best we can do

816
00:42:36,266 --> 00:42:39,833
-- well, a lot of what we
do is we move it around.

817
00:42:39,900 --> 00:42:44,033
We block off certain
techniques and targets and

818
00:42:44,099 --> 00:42:45,400
force the terrorists
to choose other

819
00:42:45,466 --> 00:42:47,466
techniques and targets.

820
00:42:47,533 --> 00:42:49,799
That largely doesn't
work very well.

821
00:42:49,866 --> 00:42:52,099
We do a lot of stuff
against airplanes because

822
00:42:52,166 --> 00:42:56,566
airplanes are particularly
disastrous targets.

823
00:42:56,633 --> 00:42:59,698
Right? A bomb goes off in this
room, and some people die,

824
00:42:59,766 --> 00:43:01,933
some will get injured,
and everyone is okay.

825
00:43:02,000 --> 00:43:04,766
A bomb goes off on this
airplane and the airplane

826
00:43:04,833 --> 00:43:06,166
crashes and we all die.

827
00:43:06,233 --> 00:43:08,933
That has a particular
failure mode, which is why

828
00:43:09,000 --> 00:43:11,766
that is protected more
than other things.

829
00:43:11,833 --> 00:43:15,800
Once you get out of
airplanes, you're just

830
00:43:15,866 --> 00:43:18,099
moving around what the
terrorists are doing, and

831
00:43:18,166 --> 00:43:22,266
that sends you upstream
to geopolitical solutions

832
00:43:22,333 --> 00:43:26,199
very quickly, that the
rest of the money is just

833
00:43:26,266 --> 00:43:29,099
expended in forcing the
bad guys to change their

834
00:43:29,166 --> 00:43:31,233
tactics and target.

835
00:43:31,300 --> 00:43:34,000
I see you there.

836
00:43:34,866 --> 00:43:38,133
>> AUDIENCE: Hi. When we are
talking about security and

837
00:43:38,199 --> 00:43:41,766
securing organizations and
systems, we often raise

838
00:43:41,833 --> 00:43:44,166
up the security awareness.

839
00:43:44,233 --> 00:43:48,300
What about here if you try
to secure the society?

840
00:43:48,366 --> 00:43:51,533
What about the awareness
of the people and how to

841
00:43:51,599 --> 00:43:54,033
raise their level
of education?

842
00:43:54,099 --> 00:43:57,699
Because with low level
of education, they are

843
00:43:57,766 --> 00:44:01,266
certainly a target to a
different kind of hacks

844
00:44:01,333 --> 00:44:04,766
like disinformation
and stuff like that.

845
00:44:04,833 --> 00:44:06,099
>> BRUCE SCHNEIER: I
think that's interesting.

846
00:44:06,166 --> 00:44:08,033
I haven't done a lot of
thinking about awareness

847
00:44:08,099 --> 00:44:10,133
as a security measure.

848
00:44:10,199 --> 00:44:11,300
I should.

849
00:44:11,366 --> 00:44:15,033
Off the top of my head,
a lot of these attacks

850
00:44:15,099 --> 00:44:16,800
aren't attacks
against the user.

851
00:44:16,866 --> 00:44:19,166
They are more attacks
against the code.

852
00:44:19,233 --> 00:44:22,500
You need to think about
what are attacks against

853
00:44:22,566 --> 00:44:25,665
the users in these
social systems.

854
00:44:25,733 --> 00:44:29,566
If we have those,
how much is awareness

855
00:44:29,633 --> 00:44:30,433
going to be
a defense?

856
00:44:30,500 --> 00:44:35,333
So maybe an example might
be nutritional labels.

857
00:44:35,400 --> 00:44:37,766
If high fructose syrup
is a hack against our

858
00:44:37,833 --> 00:44:42,800
biological need for quick
energy and our nutritional

859
00:44:42,866 --> 00:44:47,066
labels are some kind of
literacy or education solution.

860
00:44:47,133 --> 00:44:48,198
That's where I'd look.

861
00:44:48,266 --> 00:44:49,599
My guess is
it's part of it.

862
00:44:49,666 --> 00:44:54,400
I tend not to be a big fan
in our field of education

863
00:44:54,466 --> 00:44:56,098
as a solution.

864
00:44:56,166 --> 00:45:00,066
I mean, I want our systems
to work even with an

865
00:45:00,133 --> 00:45:02,265
uneducated user.

866
00:45:02,333 --> 00:45:05,500
And I think this is just
sophistication of our field.

867
00:45:05,566 --> 00:45:08,899
The early automobiles
were sold with a toolkit

868
00:45:08,966 --> 00:45:09,866
and a repair manual.

869
00:45:09,933 --> 00:45:10,866
Now they're not.

870
00:45:10,933 --> 00:45:11,833
Now everybody can drive.

871
00:45:11,900 --> 00:45:13,099
You don't need to be
an expert in internal

872
00:45:13,166 --> 00:45:15,166
combustion to drive a car.

873
00:45:15,233 --> 00:45:17,233
And you shouldn't need to
be an expert in anything

874
00:45:17,300 --> 00:45:18,633
to use a computer.

875
00:45:18,699 --> 00:45:23,233
I want the fixes more
embedded in the system

876
00:45:23,300 --> 00:45:25,733
than to rely on the user.

877
00:45:25,800 --> 00:45:27,933
But that's worth thinking
about in these broader

878
00:45:28,000 --> 00:45:30,400
systems because they are
so much more user-focused

879
00:45:30,466 --> 00:45:31,900
than a tech system.

880
00:45:31,966 --> 00:45:32,665
Thank you for that.

881
00:45:32,733 --> 00:45:34,000
Something to think about.

882
00:45:34,066 --> 00:45:35,000
Yes.

883
00:45:35,066 --> 00:45:36,899
>> AUDIENCE: Some very
interesting ideas.

884
00:45:36,966 --> 00:45:39,799
Security, tech security
folks are very good at

885
00:45:39,866 --> 00:45:43,533
spotting problems, very good
at coming up with solutions.

886
00:45:43,599 --> 00:45:46,133
But what you haven't
talked about is how are we

887
00:45:46,199 --> 00:45:48,433
going to get that bell,
that terrific bell on that

888
00:45:48,500 --> 00:45:54,833
cat because we're not the
people, usually, who have

889
00:45:54,900 --> 00:45:58,000
the power or the
influence to implement.

890
00:45:58,066 --> 00:46:00,533
>> BRUCE SCHNEIER: What I
want is more techies

891
00:46:00,599 --> 00:46:01,599
in the room.

892
00:46:01,666 --> 00:46:04,066
I mean, this is really what I
push in public interest tech.

893
00:46:04,133 --> 00:46:05,098
Right now, we don't.

894
00:46:05,166 --> 00:46:07,199
We are not involved in the
conversations, and I think

895
00:46:07,266 --> 00:46:09,500
we can contribute to
these conversations.

896
00:46:09,566 --> 00:46:13,366
Last year, we had
a sitting U.S. Senator

897
00:46:13,433 --> 00:46:16,800
in a public hearing asking
Mark Zuckerberg this question:

898
00:46:16,866 --> 00:46:19,666
How does Facebook make money?

899
00:46:19,733 --> 00:46:23,866
Right? On the one hand, my
god, you don't know?

900
00:46:23,933 --> 00:46:26,300
And two, no one on your
staff told you that was

901
00:46:26,366 --> 00:46:28,599
a stupid question?

902
00:46:28,666 --> 00:46:32,400
The bar is
really low here.

903
00:46:32,466 --> 00:46:34,066
And we need to do better.

904
00:46:35,199 --> 00:46:37,800
How? I don't have
a good answer.

905
00:46:37,866 --> 00:46:39,699
I think we're trying
a lot of things.

906
00:46:39,766 --> 00:46:41,733
But, yes, I think that
is a big part of the

907
00:46:41,800 --> 00:46:45,833
solution, getting
technologists involved in

908
00:46:45,900 --> 00:46:48,766
public policies because
all of these problems have

909
00:46:48,833 --> 00:46:50,833
some tech component.

910
00:46:51,966 --> 00:46:52,766
That's not a great answer.

911
00:46:52,833 --> 00:46:54,533
It's what I got for you.

912
00:46:54,599 --> 00:46:55,633
Let me go to
the next person.

913
00:47:01,533 --> 00:47:03,000
>> LOGAN: Hi.
My name is Logan.

914
00:47:03,066 --> 00:47:05,598
I am a researcher in both
government and computer

915
00:47:05,666 --> 00:47:07,400
science at
Georgetown University.

916
00:47:07,466 --> 00:47:08,500
I really like
this paradigm.

917
00:47:08,566 --> 00:47:10,866
I think it's very
fascinating; however, just

918
00:47:10,933 --> 00:47:12,699
off your talk, it seems
like it's focused on

919
00:47:12,766 --> 00:47:15,766
ironing out the kinks and
the bugs in systems.

920
00:47:15,833 --> 00:47:17,400
But when you look at how
entrenched some of these

921
00:47:17,466 --> 00:47:19,766
broader sociopolitical
systems are, some of them

922
00:47:19,833 --> 00:47:22,000
may be flawed to the core.

923
00:47:22,066 --> 00:47:24,765
Are you worried that this
paradigm may focus more on

924
00:47:24,833 --> 00:47:27,066
just ironing out the bumps
when some of the systems

925
00:47:27,133 --> 00:47:29,598
may need to be
replaced entirely?

926
00:47:29,666 --> 00:47:31,033
>> BRUCE SCHNEIER: Yeah.
That's a good comment.

927
00:47:31,099 --> 00:47:32,366
And you're right.

928
00:47:32,433 --> 00:47:35,400
In computer security,
we tend to iron out the bumps.

929
00:47:35,466 --> 00:47:36,633
That's what we do.

930
00:47:36,699 --> 00:47:40,599
Rarely do we say that the
internet is fundamentally

931
00:47:40,666 --> 00:47:43,166
broken; make a new one.

932
00:47:43,233 --> 00:47:45,466
If we say that, people
look at you and say are

933
00:47:45,533 --> 00:47:46,433
you an idiot?

934
00:47:46,500 --> 00:47:47,633
That's never
going to happen.

935
00:47:47,699 --> 00:47:51,066
So I think this kind of
thinking is about the bumps.

936
00:47:51,133 --> 00:47:52,133
You're right.

937
00:47:52,199 --> 00:47:55,466
We're not going to fix the
broad structural issues

938
00:47:55,533 --> 00:47:57,966
with this kind
of thinking.

939
00:47:58,033 --> 00:48:00,766
That takes sort of another
level of abstraction.

940
00:48:00,833 --> 00:48:03,199
Am I worried that this
will obscure that?

941
00:48:03,266 --> 00:48:04,266
I am not.

942
00:48:04,333 --> 00:48:07,833
I think both are a thing
and we have to deal with them.

943
00:48:07,900 --> 00:48:10,000
Society is terrible at making
broad structural changes.

944
00:48:10,066 --> 00:48:12,265
I don't think
I can fix that.

945
00:48:12,333 --> 00:48:15,533
But in the absence of
that, I think starting to

946
00:48:15,599 --> 00:48:21,366
think about what power is
doing as hacking and how

947
00:48:21,433 --> 00:48:25,233
they're exploiting as
vulnerabilities, I think

948
00:48:25,300 --> 00:48:27,400
that would go some way to
changing the way we think

949
00:48:27,466 --> 00:48:28,066
of the dynamic.

950
00:48:28,133 --> 00:48:29,698
Hopefully that will help.

951
00:48:29,766 --> 00:48:31,466
But you make a
very good point.

952
00:48:31,533 --> 00:48:33,098
>> LOGAN: Thank you.

953
00:48:33,166 --> 00:48:35,066
>> BRUCE SCHNEIER: All right.
You're my last question.

954
00:48:35,133 --> 00:48:38,366
>> TOM SEGO: I'm Tom
Sego, CEO of BlastWave.

955
00:48:38,433 --> 00:48:41,733
My question is really
around incentives and purpose.

956
00:48:41,800 --> 00:48:45,133
I kind of see
two broad groups.

957
00:48:45,199 --> 00:48:48,633
One in which one group
is trying to gain and

958
00:48:48,699 --> 00:48:51,900
leverage power and
maximize what they can do

959
00:48:51,966 --> 00:48:53,933
with that power, and then
the other group is trying

960
00:48:54,000 --> 00:48:57,900
to immunize the system
from these hacks.

961
00:48:57,966 --> 00:49:00,133
They're trying to
make it invulnerable.

962
00:49:00,199 --> 00:49:02,300
And I'm curious, like, how
do you deal with those

963
00:49:02,366 --> 00:49:04,699
different types of
opposing purposes?

964
00:49:04,766 --> 00:49:07,566
There's no single
requirements document that

965
00:49:07,633 --> 00:49:08,500
we've all agreed upon.

966
00:49:08,566 --> 00:49:09,366
>> BRUCE SCHNEIER: Right.

967
00:49:09,433 --> 00:49:11,533
And I think that's what
I talk about in that the

968
00:49:11,599 --> 00:49:12,833
systems evolve.

969
00:49:12,900 --> 00:49:16,133
It's not like we have
a spec we can look at.

970
00:49:16,199 --> 00:49:17,766
Although there are
vulnerabilities in specs, too.

971
00:49:17,833 --> 00:49:22,833
I don't know if I
have a good answer.

972
00:49:22,900 --> 00:49:23,666
That's a good question.

973
00:49:23,733 --> 00:49:27,400
And I think this might
speak to the edges of

974
00:49:27,466 --> 00:49:30,165
where my generalization
starts failing.

975
00:49:30,233 --> 00:49:33,266
What do we do when there
isn't a consensus on what

976
00:49:33,333 --> 00:49:35,500
the system is
supposed to do?

977
00:49:35,566 --> 00:49:38,566
I think I got to that when
I talked about VC funding.

978
00:49:38,633 --> 00:49:41,633
Is that a hack or a not?

979
00:49:41,699 --> 00:49:43,433
From this
perspective, it is.

980
00:49:43,500 --> 00:49:45,466
From that perspective,
it's just that's the way

981
00:49:45,533 --> 00:49:46,500
the system works.

982
00:49:46,566 --> 00:49:49,000
Are lobbyists a hack?

983
00:49:49,066 --> 00:49:51,066
Yeah, kind of.

984
00:49:51,133 --> 00:49:53,198
But, no, that's how
we get laws passed.

985
00:49:53,266 --> 00:49:56,400
I don't know.

986
00:49:56,466 --> 00:49:59,199
As I flesh this out, I'm
going to have to be a

987
00:49:59,266 --> 00:50:02,300
little more rigid, but I
think there's value in

988
00:50:02,366 --> 00:50:04,366
having a squishy
definition.

989
00:50:06,866 --> 00:50:11,233
You can claim legitimately
that gay marriage is a hack.

990
00:50:11,300 --> 00:50:14,400
It's taking this
particular system and

991
00:50:14,466 --> 00:50:16,199
we're going to use
it in this new way.

992
00:50:16,266 --> 00:50:19,666
A lot of us think
that's a great idea.

993
00:50:19,733 --> 00:50:22,366
But, you know,
it was a hack.

994
00:50:24,433 --> 00:50:26,333
So is that good or bad?

995
00:50:26,400 --> 00:50:28,400
Well, you know,
there are good hacks.

996
00:50:30,266 --> 00:50:32,199
The question is now, what
is it supposed to do?

997
00:50:32,266 --> 00:50:33,400
What are its goals?

998
00:50:33,466 --> 00:50:34,466
Whose society?

999
00:50:34,533 --> 00:50:36,400
That's where that
gets embedded.

1000
00:50:36,466 --> 00:50:38,400
So I don't have
a good answer.

1001
00:50:38,466 --> 00:50:40,000
But that's a
good question.

1002
00:50:40,066 --> 00:50:41,033
>> TOM SEGO:
Okay, thank you.

1003
00:50:41,099 --> 00:50:42,400
>> BRUCE SCHNEIER: So I
don't -- I wasn't taking notes.

1004
00:50:42,466 --> 00:50:44,266
Can you email me
that question again?

1005
00:50:44,333 --> 00:50:45,800
Just end me an email.

1006
00:50:45,866 --> 00:50:46,733
Thank you.

1007
00:50:46,800 --> 00:50:47,533
All right.

1008
00:50:47,599 --> 00:50:48,500
I have to leave the stage.

1009
00:50:48,566 --> 00:50:49,598
Thank you all.

1010
00:50:49,666 --> 00:50:52,933
Any other questions,
comments, suggestions for

1011
00:50:53,000 --> 00:50:55,466
examples, things to look
at, places where I'm

1012
00:50:55,533 --> 00:50:58,066
completely wrong, please
email me because I want to

1013
00:50:58,133 --> 00:50:59,033
keep thinking about this.

1014
00:50:59,099 --> 00:50:59,833
Thanks, all.

1015
00:50:59,900 --> 00:51:00,766
Have a great conference.

1016
00:51:00,833 --> 00:51:02,033
I'll see you next year.

