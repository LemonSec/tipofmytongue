1
00:00:06,566 --> 00:00:08,500
>> ANNOUNCER: Please
welcome Co-Founder and

2
00:00:08,566 --> 00:00:12,133
Editor-At-Large,
Recode, Kara Swisher.

3
00:00:14,333 --> 00:00:16,500
>> KARA SWISHER:
Hi, everybody.

4
00:00:16,566 --> 00:00:18,666
Wow.

5
00:00:18,733 --> 00:00:21,533
How you doing?

6
00:00:21,600 --> 00:00:26,366
I'm so glad that speech
was before me because I am

7
00:00:26,433 --> 00:00:28,833
not a helper.

8
00:00:28,899 --> 00:00:30,100
I'm the opposite
of a helper.

9
00:00:30,166 --> 00:00:31,699
I'm also not an optimist.

10
00:00:31,766 --> 00:00:33,866
So, I'm the pessimist here
to talk about some things

11
00:00:33,933 --> 00:00:35,799
about where tech is going.

12
00:00:35,866 --> 00:00:37,399
I want to try to
explain myself.

13
00:00:37,466 --> 00:00:38,266
I'm not a techie.

14
00:00:38,333 --> 00:00:39,966
I write about tech and
have written about tech

15
00:00:40,033 --> 00:00:42,766
for about thirty -
twenty-five to thirty years.

16
00:00:42,833 --> 00:00:45,366
I'm kind of the person - I
was trying to figure out a

17
00:00:45,433 --> 00:00:47,333
way to describe what I
do in the tech industry

18
00:00:47,399 --> 00:00:48,866
besides being a reporter.

19
00:00:48,933 --> 00:00:50,500
Now I have a column
in the New York Times.

20
00:00:50,566 --> 00:00:51,799
I have a podcast.

21
00:00:51,866 --> 00:00:55,000
I talk about a
lot of issues.

22
00:00:55,066 --> 00:00:58,399
I'm sort of like a text
version of Elizabeth Warren.

23
00:00:58,466 --> 00:01:01,166
I kind of attack
billionaires beautifully

24
00:01:01,233 --> 00:01:03,632
and put them
in their place.

25
00:01:03,700 --> 00:01:06,400
She actually learned a lot
from me and she's doing

26
00:01:06,466 --> 00:01:07,433
very well.

27
00:01:07,500 --> 00:01:09,033
I think she's doing
a nice job up there.

28
00:01:09,099 --> 00:01:11,466
Anyway, I want to talk
about - I'm talking about

29
00:01:11,533 --> 00:01:13,466
social media which I write
a lot about and about

30
00:01:13,533 --> 00:01:17,200
issues that we face with
the tech industry in terms

31
00:01:17,266 --> 00:01:19,799
of how we're moving into
the next phase, which is a

32
00:01:19,866 --> 00:01:21,700
different phase than the
one that has come before

33
00:01:21,766 --> 00:01:24,866
that, this one, that
we have been through.

34
00:01:24,933 --> 00:01:27,233
We were sort of in a sort
of euphoric phase in the

35
00:01:27,299 --> 00:01:28,933
beginning of tech,
especially in the

36
00:01:29,000 --> 00:01:30,299
beginning of the Internet
when I started covering it

37
00:01:30,366 --> 00:01:31,500
in the 1990s.

38
00:01:31,566 --> 00:01:33,299
Now we're moving into a
very different phase which

39
00:01:33,366 --> 00:01:36,233
is about reality, about
facing facts, about

40
00:01:36,299 --> 00:01:39,000
understanding the impact
of our inventions.

41
00:01:39,066 --> 00:01:41,299
So, I'm going to talk a
little bit about that and

42
00:01:41,366 --> 00:01:43,533
go through some slides and
trends that I think are

43
00:01:43,599 --> 00:01:45,000
really important.

44
00:01:45,066 --> 00:01:47,700
Cybersecurity obviously
fits within a lot of these

45
00:01:47,766 --> 00:01:50,700
things because a lot of
what's happening is about

46
00:01:50,766 --> 00:01:53,700
people's safety and the
safety of consumers, the

47
00:01:53,766 --> 00:01:56,066
safety of businesses,
the safety of countries.

48
00:01:56,133 --> 00:01:58,699
And a lot of what's been
invented over the past

49
00:01:58,766 --> 00:02:02,799
twenty years and been
widely deployed, it has

50
00:02:02,866 --> 00:02:05,766
created as much as
opportunities as has

51
00:02:05,833 --> 00:02:07,433
created a lot of problems
that we have to deal with

52
00:02:07,500 --> 00:02:09,933
and figure out and learn
how to live with and how

53
00:02:10,000 --> 00:02:11,166
to regulate.

54
00:02:11,233 --> 00:02:13,666
So, I am going to go
through some trends that I

55
00:02:13,733 --> 00:02:15,599
think are important and
that I have seen over the

56
00:02:15,666 --> 00:02:17,000
past couple of years and
things that I'm thinking

57
00:02:17,066 --> 00:02:19,833
about as a write about
different things.

58
00:02:19,900 --> 00:02:24,733
Obviously, as you look
through tech on the whole,

59
00:02:24,800 --> 00:02:26,833
it has been a real
positive for humanity.

60
00:02:26,900 --> 00:02:29,333
A lot of the inventions,
especially recently, with

61
00:02:29,400 --> 00:02:32,500
the worldwide network of
information, with all

62
00:02:32,566 --> 00:02:34,733
kinds of devices, with
connectivity, but it has

63
00:02:34,800 --> 00:02:38,199
also again, presented a
real problem in terms of

64
00:02:38,266 --> 00:02:41,166
disinformation, in terms
of fake news, in terms of

65
00:02:41,233 --> 00:02:44,166
polarization and
lack of security.

66
00:02:44,233 --> 00:02:46,433
And so, we'll talk about
those and what I think the

67
00:02:46,500 --> 00:02:47,733
next trends will be.

68
00:02:47,800 --> 00:02:51,333
I do a long running list
of these trends that I

69
00:02:51,400 --> 00:02:54,500
change as I see different
things, as I see new

70
00:02:54,566 --> 00:02:55,500
issues emerge.

71
00:02:55,566 --> 00:02:58,433
This week, I'm writing
in the Times about the

72
00:02:58,500 --> 00:03:00,599
coronavirus and the
implications of

73
00:03:00,666 --> 00:03:02,400
teleworking and
telecommunicating and

74
00:03:02,466 --> 00:03:05,633
things like that and I
will talk about those

75
00:03:05,699 --> 00:03:07,300
implications at
the very end.

76
00:03:07,366 --> 00:03:11,633
But what I wanted - what
this title page is from,

77
00:03:11,699 --> 00:03:13,000
is I was passing a church.

78
00:03:13,066 --> 00:03:15,533
I live part-time in
Washington, D.C. and I was

79
00:03:15,599 --> 00:03:18,465
passing the 12th Street
Baptist Church and this

80
00:03:18,533 --> 00:03:19,965
was on the front of it.

81
00:03:20,033 --> 00:03:22,866
You know, they were giving
sort of messages to people

82
00:03:22,933 --> 00:03:25,233
who were coming
in to worship.

83
00:03:25,300 --> 00:03:26,599
And I was really
struck by it.

84
00:03:26,666 --> 00:03:29,599
It is believing what you see and not seeing what you believe.

85
00:03:29,666 --> 00:03:33,266
I think we're in an era
where people are seeing

86
00:03:33,333 --> 00:03:36,066
what they believe and not
believing what they see,

87
00:03:36,133 --> 00:03:40,033
whether it's news,
whether it's information.

88
00:03:40,099 --> 00:03:42,698
Because of what's happened
with the internet is there

89
00:03:42,766 --> 00:03:45,366
is so much noise and a lot
of ugliness, we don't -

90
00:03:45,433 --> 00:03:48,132
we're not really
believing what we see.

91
00:03:48,199 --> 00:03:50,000
I think it is really
important to return to

92
00:03:50,066 --> 00:03:54,033
that period where facts
matter, where we can all

93
00:03:54,099 --> 00:03:56,299
agree on a common set of
facts and everything is

94
00:03:56,366 --> 00:03:59,666
not colored by whoever is
the noisiest or whoever

95
00:03:59,733 --> 00:04:02,900
can make an argument on
another side of something.

96
00:04:02,966 --> 00:04:04,566
I'm not just talking
politically, but it

97
00:04:04,633 --> 00:04:07,666
reaches everywhere,
whether it is about

98
00:04:07,733 --> 00:04:10,166
vaccinations, whether it
is about political life,

99
00:04:10,233 --> 00:04:11,733
whether it is about what
you think of the movie

100
00:04:11,800 --> 00:04:13,366
Parasite - it was
great, by the way.

101
00:04:13,433 --> 00:04:14,266
You should all see it.

102
00:04:14,333 --> 00:04:16,500
It deserved the Oscar.

103
00:04:16,565 --> 00:04:19,099
In any case, so I want
people to think about this

104
00:04:19,166 --> 00:04:22,866
overall as I go through
these really quickly.

105
00:04:22,933 --> 00:04:26,666
First slide, I put this
up here only because Mark

106
00:04:26,733 --> 00:04:29,333
Zuckerberg has a new book
out called 'The Inside Story.'

107
00:04:29,399 --> 00:04:30,966
It's about Facebook.

108
00:04:31,033 --> 00:04:33,233
In it is something I knew
but not a lot of people

109
00:04:33,300 --> 00:04:36,333
knew, but Mark
Zuckerberg's person that

110
00:04:36,399 --> 00:04:39,466
he looked up to in history
is Augustus Caesar.

111
00:04:39,533 --> 00:04:42,000
I don't know if you knew
that or anything else or

112
00:04:42,066 --> 00:04:43,698
you know a lot about
Augustus Caesar, but he

113
00:04:43,766 --> 00:04:47,733
was the emperor after
Julius Caesar who really

114
00:04:47,800 --> 00:04:51,100
was a very successful
emperor, brought a lot of

115
00:04:51,166 --> 00:04:55,366
good things to Rome, was
sort of brought in from

116
00:04:55,433 --> 00:04:59,099
sort of a dying republic
to a very powerful empire.

117
00:04:59,166 --> 00:05:01,399
At the same time, he was
an emperor which is not

118
00:05:01,466 --> 00:05:04,566
really the kind of
government most people, at

119
00:05:04,633 --> 00:05:06,799
least people who live in
democracies, want to live in.

120
00:05:06,866 --> 00:05:09,199
And so I thought it was a
really interesting fact

121
00:05:09,266 --> 00:05:12,833
that this is a person that
Mark looks up to because I

122
00:05:12,899 --> 00:05:17,766
think about where we are
right now, in a state of

123
00:05:17,833 --> 00:05:20,133
technology companies that
are run by essentially

124
00:05:20,199 --> 00:05:23,233
emperors who, not just
Mark but across the

125
00:05:23,300 --> 00:05:25,899
spectrum, companies that
are founders, founders

126
00:05:25,966 --> 00:05:28,100
that have an enormous
amounts of control.

127
00:05:28,166 --> 00:05:30,800
And so, a lot of our
decision-making are in the

128
00:05:30,866 --> 00:05:32,800
hands of a single person.

129
00:05:32,866 --> 00:05:36,366
In this case, Mark at
Facebook who cannot be

130
00:05:36,433 --> 00:05:39,000
fired, he cannot lose
his job, he controls the

131
00:05:39,066 --> 00:05:42,000
board, he has complete
control and he's making

132
00:05:42,066 --> 00:05:44,732
decisions for billions of
people at a company that

133
00:05:44,800 --> 00:05:46,033
has enormous impact.

134
00:05:46,100 --> 00:05:47,333
I'm just using
Facebook as an example.

135
00:05:47,399 --> 00:05:49,066
You can go through a
number of companies.

136
00:05:49,133 --> 00:05:51,066
But we are in an era
where a lot of major

137
00:05:51,133 --> 00:05:54,599
decision-making is being
made by single people

138
00:05:54,666 --> 00:05:57,466
which I think is
problematic, whether it is

139
00:05:57,533 --> 00:06:00,033
at Amazon or any other
companies you think of,

140
00:06:00,100 --> 00:06:02,100
and we have to start
thinking about, it is not

141
00:06:02,166 --> 00:06:03,933
that we want a million
people making decisions,

142
00:06:04,000 --> 00:06:06,033
but these are people that
have no accountability and

143
00:06:06,100 --> 00:06:09,466
no regulations to
stop them from doing

144
00:06:09,533 --> 00:06:10,766
anything they want

145
00:06:10,833 --> 00:06:14,966
And their attributes
are very dominating.

146
00:06:15,033 --> 00:06:18,733
In fact, in his book by
Steven Levy - I just did a

147
00:06:18,800 --> 00:06:21,433
podcast on it - one of the
things that Mark used to

148
00:06:21,500 --> 00:06:24,033
do before he got
understanding that this

149
00:06:24,100 --> 00:06:26,266
might be a problem, say,
if there was an antitrust

150
00:06:26,333 --> 00:06:29,699
case, he used to start
meetings at Facebook in

151
00:06:29,766 --> 00:06:32,133
the early days by
screaming "dominate,"

152
00:06:32,199 --> 00:06:34,800
which is probably not the
best thing to do today,

153
00:06:34,866 --> 00:06:37,699
given the current
environment.

154
00:06:37,766 --> 00:06:40,800
And dominate he did, and
dominate Facebook does in

155
00:06:40,866 --> 00:06:42,733
that particular area
of social media.

156
00:06:42,800 --> 00:06:45,466
But I wanted to give you
the sense that we're in an

157
00:06:45,533 --> 00:06:49,399
era run by, as it was
back with Edison and the

158
00:06:49,466 --> 00:06:53,133
Rockefellers and Carnegie,
the very strong leaders of

159
00:06:53,199 --> 00:06:54,699
the companies that are
making decisions that

160
00:06:54,766 --> 00:06:56,399
affect millions and
billions of people

161
00:06:56,466 --> 00:06:58,666
around the world.

162
00:06:58,733 --> 00:06:59,366
So, the first one.

163
00:06:59,433 --> 00:07:01,133
These are kind of
the key trends.

164
00:07:01,199 --> 00:07:03,500
I keep changing and adding
to them as I go, as I go

165
00:07:03,566 --> 00:07:04,965
through things.

166
00:07:05,033 --> 00:07:07,866
I take things off and I
put them on, but I think

167
00:07:07,933 --> 00:07:11,199
we're in an era where even
though there has been

168
00:07:11,266 --> 00:07:13,433
enormous invention over
the past twenty years and

169
00:07:13,500 --> 00:07:15,800
it is really astonishing
when you think that.

170
00:07:15,866 --> 00:07:18,265
I think the iPhone just
came in around - I forget.

171
00:07:18,333 --> 00:07:19,766
It was 2004.

172
00:07:19,833 --> 00:07:23,133
I was writing a piece
about what the last

173
00:07:23,199 --> 00:07:27,033
outbreak of a disease was,
SARS, when we didn't have

174
00:07:27,100 --> 00:07:30,266
many collaborative and
communications systems

175
00:07:30,333 --> 00:07:33,166
that would allow us
to telecommute and do

176
00:07:33,233 --> 00:07:35,166
telework or
telecommunication.

177
00:07:35,233 --> 00:07:38,133
It was an era, even just
from 2003, where there was

178
00:07:38,199 --> 00:07:43,399
no Facebook, no iPhone, no
Uber, no - you can list

179
00:07:43,466 --> 00:07:44,933
you know, a dozen
different things that

180
00:07:45,000 --> 00:07:46,633
we use all day.

181
00:07:46,699 --> 00:07:48,699
But this - so, there has
been enormous change in

182
00:07:48,766 --> 00:07:51,133
the past twenty years, but
actually what is coming I

183
00:07:51,199 --> 00:07:53,533
think is really, it's sort
of you ain't seen nothing

184
00:07:53,600 --> 00:07:54,933
yet kind of thing.

185
00:07:55,000 --> 00:07:58,266
The key trends are really
massive and in a way that

186
00:07:58,333 --> 00:08:01,366
they haven't been yet even
though the proliferation

187
00:08:01,433 --> 00:08:05,666
of cell phones and the
World Wide Web and instant

188
00:08:05,733 --> 00:08:07,699
communication are
amazing things.

189
00:08:07,766 --> 00:08:09,633
What is coming next is
much, much bigger and much

190
00:08:09,699 --> 00:08:12,266
more important to think
about, especially because

191
00:08:12,333 --> 00:08:14,633
it is when the analog does
really mash up with the

192
00:08:14,699 --> 00:08:18,133
digital in ways that are
- we're going to have to

193
00:08:18,199 --> 00:08:19,300
think really hard about.

194
00:08:19,366 --> 00:08:21,199
Each of these things
- each of these areas

195
00:08:21,266 --> 00:08:23,266
presents a real
cybersecurity and safety

196
00:08:23,333 --> 00:08:25,933
challenge for humanity.

197
00:08:26,000 --> 00:08:27,800
The first one is obviously
artificial intelligence,

198
00:08:27,866 --> 00:08:29,166
she was just
talking about that.

199
00:08:29,233 --> 00:08:31,199
It is a critically
important technology.

200
00:08:31,266 --> 00:08:35,500
Quantum computing and
related machine learning,

201
00:08:35,566 --> 00:08:37,166
robotics and
automatization, which is

202
00:08:37,232 --> 00:08:40,265
going to change our job
scenarios going forward.

203
00:08:40,332 --> 00:08:41,966
It is not very clear what
that is going to be.

204
00:08:42,033 --> 00:08:43,899
There are some people who
are very worried; there

205
00:08:43,966 --> 00:08:47,333
are others who think we're
overestimating the impact.

206
00:08:47,399 --> 00:08:49,799
But I do think robotics
and automatization are

207
00:08:49,866 --> 00:08:52,899
going to be massive issues
- massive trends going

208
00:08:52,966 --> 00:08:54,799
forward that we have
to be thinking about.

209
00:08:54,866 --> 00:08:57,199
You only have to visit an
Amazon warehouse for two

210
00:08:57,266 --> 00:08:59,899
seconds to understand the
implications of what is

211
00:08:59,966 --> 00:09:00,933
going to be happening.

212
00:09:01,000 --> 00:09:02,833
Obviously, a
transportation explosion.

213
00:09:02,899 --> 00:09:04,766
You're going to hear from
Mary Barra later who is

214
00:09:04,833 --> 00:09:09,733
the head of GM, whether it
is autonomous vehicles.

215
00:09:09,799 --> 00:09:14,766
I'm going to be riding in
one - a new one down in

216
00:09:14,833 --> 00:09:17,165
Silicon Valley tomorrow
- whether it is urban

217
00:09:17,233 --> 00:09:20,599
mobility of all kinds like
scooters or other urban

218
00:09:20,666 --> 00:09:23,833
mobility devices and there
are dozens being tested,

219
00:09:23,899 --> 00:09:26,600
including a hover craft
which is being funded by

220
00:09:26,666 --> 00:09:28,200
Larry Page from Google.

221
00:09:28,266 --> 00:09:31,533
And drones obviously for
delivery and other things

222
00:09:31,600 --> 00:09:33,833
they are going
to be doing.

223
00:09:33,899 --> 00:09:35,766
It's a very - the
transportation explosion

224
00:09:35,833 --> 00:09:37,199
is just beginning.

225
00:09:37,266 --> 00:09:38,799
We're going to see very
big changes and it will

226
00:09:38,866 --> 00:09:40,899
have massive impacts on
cities, on people, on

227
00:09:40,966 --> 00:09:43,700
jobs, on businesses, of
everything from insurance

228
00:09:43,766 --> 00:09:46,632
to parking lots to
mechanics to everything.

229
00:09:46,700 --> 00:09:50,066
We are so - the bigger - a
big trend also is endless

230
00:09:50,133 --> 00:09:52,233
choice, the ability for
people to get anything

231
00:09:52,299 --> 00:09:53,833
delivered from
anywhere, at any time, and

232
00:09:53,899 --> 00:09:55,033
anything they want.

233
00:09:55,100 --> 00:09:58,233
Obviously, it is fueled by
algorithms and things that

234
00:09:58,299 --> 00:09:59,866
will suggest things to
you, but the idea of

235
00:09:59,933 --> 00:10:03,266
endless choice is really
a powerful one, much more

236
00:10:03,333 --> 00:10:04,966
than I think
people realize.

237
00:10:05,033 --> 00:10:07,600
Obviously, privacy is
under assault when data is

238
00:10:07,666 --> 00:10:08,966
the new gold.

239
00:10:09,033 --> 00:10:11,266
We're just starting to
face some privacy issues

240
00:10:11,333 --> 00:10:14,466
and with privacy, it is
the same thing in terms of

241
00:10:14,533 --> 00:10:16,866
people not just stealing
your information but you

242
00:10:16,933 --> 00:10:18,633
handing over information.

243
00:10:18,700 --> 00:10:21,633
I know you all talk about
attacks and I have heard a

244
00:10:21,700 --> 00:10:24,333
couple of the speeches
here about attacks and

245
00:10:24,399 --> 00:10:25,833
things like that, but one
of the things that is

246
00:10:25,899 --> 00:10:28,200
critical to think about is
a lot of this information

247
00:10:28,266 --> 00:10:30,565
is not stolen, it
is handed over.

248
00:10:30,633 --> 00:10:33,833
It is given over by
people, either through

249
00:10:33,899 --> 00:10:38,833
agreements they make with
the companies they get or -

250
00:10:38,899 --> 00:10:40,933
or just as a
matter of course.

251
00:10:41,000 --> 00:10:42,500
If you're standing with
a cell phone, they're

252
00:10:42,566 --> 00:10:45,299
pinging information
almost constantly without

253
00:10:45,366 --> 00:10:46,799
a lot of consent.

254
00:10:46,866 --> 00:10:50,633
It has all kinds of
implications in lots of

255
00:10:50,700 --> 00:10:52,766
ways and it is an
important area for

256
00:10:52,833 --> 00:10:54,799
regulation, actually.

257
00:10:54,866 --> 00:10:56,799
We are in a state of
continuous partial attack

258
00:10:56,866 --> 00:10:59,799
hacking where we are
always being hacked at

259
00:10:59,866 --> 00:11:01,799
some point or there is
always an ability and a

260
00:11:01,866 --> 00:11:04,100
vulnerability in almost
everything we use.

261
00:11:04,166 --> 00:11:06,533
As we add more and more
devices to our world,

262
00:11:06,600 --> 00:11:10,100
including stuff in our
home, all kinds of things,

263
00:11:10,166 --> 00:11:12,666
at the office, when we're
doing telecommuting, it

264
00:11:12,733 --> 00:11:15,132
opens up a lot of
vulnerabilities.

265
00:11:15,200 --> 00:11:17,833
We're also in a state of continuous partial attention.

266
00:11:17,899 --> 00:11:19,633
I think you all know what
I mean, of those of you on

267
00:11:19,700 --> 00:11:22,466
your cell phones right
now, because you really

268
00:11:22,533 --> 00:11:25,766
have to watch the JLo
TikTok challenge right now -

269
00:11:25,833 --> 00:11:29,299
and I did that for a while today, it was highly enjoyable.

270
00:11:29,366 --> 00:11:31,066
But we are - we participate.

271
00:11:31,133 --> 00:11:33,233
We have a human race
that is in a state of

272
00:11:33,299 --> 00:11:34,833
continuous partial
attention because they're

273
00:11:34,899 --> 00:11:36,766
always hacked
into the system.

274
00:11:36,833 --> 00:11:39,333
We're also in a time of
very serious political and

275
00:11:39,399 --> 00:11:41,399
social unrest, fueled
somewhat by the social

276
00:11:41,466 --> 00:11:43,433
media although they're
part of trends that have

277
00:11:43,500 --> 00:11:47,000
long existed, but the
polarization has been

278
00:11:47,066 --> 00:11:50,600
accelerated by social
media and other things.

279
00:11:50,666 --> 00:11:52,833
It is an accelerant,
not a driver yet, but

280
00:11:52,899 --> 00:11:55,200
it is certainly
becoming a driver.

281
00:11:55,266 --> 00:11:57,500
We have a Silicon Valley
that is drunk on capital

282
00:11:57,566 --> 00:11:59,966
and disruptive, sort of
throwing money at everything.

283
00:12:00,033 --> 00:12:02,500
I think SoftBank and some
other companies that have

284
00:12:02,566 --> 00:12:05,333
been throwing billions of
dollars at problematic

285
00:12:05,399 --> 00:12:09,700
companies is something that creates a lot of confusion.

286
00:12:09,766 --> 00:12:11,766
And it is not aimed at
innovation; it is just

287
00:12:11,833 --> 00:12:13,833
aimed at deploying capital
for no good reason.

288
00:12:13,899 --> 00:12:15,666
There is a lot of
questions about when

289
00:12:15,733 --> 00:12:16,665
regulation is coming.

290
00:12:16,733 --> 00:12:18,000
There is a lot of
regulation in Europe and

291
00:12:18,066 --> 00:12:20,166
everywhere else, but in
the United States, they

292
00:12:20,233 --> 00:12:24,599
can't agree on lunch in the
US Senate and whole Congress.

293
00:12:24,666 --> 00:12:26,633
So, it is hard to imagine
there is going to be any

294
00:12:26,700 --> 00:12:29,200
serious regulation,
including privacy regulation.

295
00:12:29,266 --> 00:12:30,766
There is obviously a new
privacy bill here in

296
00:12:30,833 --> 00:12:32,632
California which is
essentially the law of the

297
00:12:32,700 --> 00:12:36,133
land, but we have to be
thinking hard about that.

298
00:12:36,200 --> 00:12:38,166
And then obviously,
just recently, the

299
00:12:38,233 --> 00:12:39,233
climate change issues.

300
00:12:39,299 --> 00:12:43,233
And the
coronavirus conundrum.

301
00:12:43,299 --> 00:12:44,466
These are analog attacks.

302
00:12:44,533 --> 00:12:47,399
These are analog threats
to our world, both of

303
00:12:47,466 --> 00:12:50,700
which will sort of make
the rest of this pointless.

304
00:12:50,766 --> 00:12:54,833
You know, coronavirus or a
major climate event will

305
00:12:54,899 --> 00:12:57,866
certainly make
all of this moot.

306
00:12:57,933 --> 00:12:59,766
And so, these are issues
we have to deal with, with

307
00:12:59,833 --> 00:13:03,266
both technological and
human solutions but it

308
00:13:03,333 --> 00:13:06,799
certainly - it does
present the biggest problem.

309
00:13:06,866 --> 00:13:10,433
I recently wrote a column
in the Times about the

310
00:13:10,500 --> 00:13:12,633
fact that I think the
next - the world's first

311
00:13:12,700 --> 00:13:14,633
trillionaire is going
to be a climate change

312
00:13:14,700 --> 00:13:17,866
technologist who is going
to figure out different ways.

313
00:13:17,933 --> 00:13:21,165
I actually just made that
up which I like to do

314
00:13:21,233 --> 00:13:22,965
sometimes, but I wanted to
get people thinking about

315
00:13:23,033 --> 00:13:24,700
where they can
make investments.

316
00:13:24,766 --> 00:13:27,132
Of course, the week
after I said that, and I

317
00:13:27,200 --> 00:13:29,133
literally just made it
up, Jeff Bezos gave $10

318
00:13:29,200 --> 00:13:31,799
billion to climate change
technologies and solutions

319
00:13:31,866 --> 00:13:33,733
and I feel like
a genius now.

320
00:13:33,799 --> 00:13:36,699
He listens to
everything I say.

321
00:13:36,766 --> 00:13:39,665
But what's important - he
does not at all because he

322
00:13:39,733 --> 00:13:41,965
wouldn't work out that
much if I were running the

323
00:13:42,033 --> 00:13:43,433
show over there.

324
00:13:43,500 --> 00:13:44,799
So - he looks good though.

325
00:13:44,866 --> 00:13:45,899
That's fine.

326
00:13:45,966 --> 00:13:48,399
I don't like to talk about
people - anyway, he looks fine.

327
00:13:48,466 --> 00:13:49,200
Looks good.

328
00:13:49,266 --> 00:13:50,799
I'm glad for him.

329
00:13:50,866 --> 00:13:51,933
Good for him.

330
00:13:52,000 --> 00:13:53,933
Good for the richest man
in the world to look good.

331
00:13:54,000 --> 00:13:56,433
So, so I want to talk
about some of the issues

332
00:13:56,500 --> 00:13:58,100
that you have to be
thinking about that we

333
00:13:58,166 --> 00:13:59,299
have to think really hard.

334
00:13:59,366 --> 00:14:03,165
This is a quote from
Tristan Harris who at is

335
00:14:03,233 --> 00:14:06,899
the Center for Human Innovation, something like that.

336
00:14:06,966 --> 00:14:09,100
He is talking about AI.

337
00:14:09,166 --> 00:14:10,766
I will just read this
because I think it was a

338
00:14:10,833 --> 00:14:13,699
really powerful thing and
he uses the term human

339
00:14:13,766 --> 00:14:16,399
downgrading, because it
is inhuman, a lot of the

340
00:14:16,466 --> 00:14:17,233
stuff that's happening.

341
00:14:17,299 --> 00:14:18,533
I don't mean it is cruel.

342
00:14:18,600 --> 00:14:21,166
I mean it is not human
is probably the word I

343
00:14:21,233 --> 00:14:22,632
probably want to use here.

344
00:14:22,700 --> 00:14:24,766
Increasingly powerful AI
is pointed at your brain

345
00:14:24,833 --> 00:14:26,766
to reverse engineer, what
can I throw in front of

346
00:14:26,833 --> 00:14:29,065
your nervous system to
crawl down your brainstem

347
00:14:29,133 --> 00:14:30,033
and get something
out of you?

348
00:14:30,100 --> 00:14:32,333
Whether it is an ad-click
or addiction or political

349
00:14:32,399 --> 00:14:34,700
convergence or whatever,
this is all part of one

350
00:14:34,766 --> 00:14:35,699
connected system.

351
00:14:35,766 --> 00:14:37,766
We call it human
downgrading which is the

352
00:14:37,833 --> 00:14:39,833
social climate
change of culture.

353
00:14:39,899 --> 00:14:41,700
The problem with human
downgrading which is while

354
00:14:41,766 --> 00:14:45,065
we have been upgrading
the machines, we've been

355
00:14:45,133 --> 00:14:47,299
downgrading humans,
downgrading our attention

356
00:14:47,366 --> 00:14:49,665
spans, downgrading our
civility, our decency,

357
00:14:49,733 --> 00:14:52,000
downgrading our democracy,
downgrading mental health.

358
00:14:52,066 --> 00:14:54,266
I think this is really
important to think about

359
00:14:54,333 --> 00:14:56,933
the systemic ways these
all fit together.

360
00:14:57,000 --> 00:14:58,899
We tend to cover them
in different pieces.

361
00:14:58,966 --> 00:15:00,833
But what's happening is
- and the reason I put

362
00:15:00,899 --> 00:15:03,399
Augustus Caesar is these
decisions are being made

363
00:15:03,466 --> 00:15:07,100
by single people at
companies and not by us as

364
00:15:07,166 --> 00:15:10,833
a whole, as a community,
as a group of citizens.

365
00:15:10,899 --> 00:15:13,133
A lot of the decisions
that are leading to some

366
00:15:13,200 --> 00:15:16,200
of this stuff are not
being made thinking hard

367
00:15:16,266 --> 00:15:20,000
about the systemic issues
because each company is

368
00:15:20,066 --> 00:15:22,700
part of a problem and
doesn't seem - it doesn't work

369
00:15:22,766 --> 00:15:26,366
together to understand this downgrading that is happening.

370
00:15:26,433 --> 00:15:29,733
A lot of their business
plans depend on taking

371
00:15:29,799 --> 00:15:33,433
little bits of humanity
and trading it for cash,

372
00:15:33,500 --> 00:15:35,866
essentially, for themselves.

373
00:15:35,933 --> 00:15:37,466
So, it is unhuman.

374
00:15:37,533 --> 00:15:38,799
You have to think
of it that way.

375
00:15:38,866 --> 00:15:40,165
It is not thinking of
humanity, it is thinking

376
00:15:40,233 --> 00:15:40,833
of growing.

377
00:15:40,899 --> 00:15:42,166
It is thinking of
everything else.

378
00:15:42,233 --> 00:15:43,500
It also links, like
I just talked about.

379
00:15:43,566 --> 00:15:44,666
It's systemic.

380
00:15:44,733 --> 00:15:46,899
We need to move from
this disconnected set of

381
00:15:46,966 --> 00:15:48,799
grievances and scandals
that these problems are

382
00:15:48,866 --> 00:15:49,966
seemingly separate.

383
00:15:50,033 --> 00:15:50,833
They are not.

384
00:15:50,899 --> 00:15:52,466
Tech addiction,
polarization,

385
00:15:52,533 --> 00:15:54,500
outrage-ification of
culture, the rise of

386
00:15:54,566 --> 00:15:57,133
vanities, microcelebrity
cultures, everyone has

387
00:15:57,200 --> 00:15:58,100
to be famous.

388
00:15:58,166 --> 00:15:59,466
These are not
separate problems.

389
00:15:59,533 --> 00:16:01,399
They're actually coming
from one thing which is

390
00:16:01,466 --> 00:16:05,100
the race to capture human
attention by tech giants.

391
00:16:05,166 --> 00:16:07,700
The race to capture
human attention.

392
00:16:07,766 --> 00:16:09,733
It is A critically
important part of the

393
00:16:09,799 --> 00:16:13,165
business plan that these
companies have and they

394
00:16:13,233 --> 00:16:15,933
have very little thought
to the impact of these, to

395
00:16:16,000 --> 00:16:18,466
the security of these,
to the safety of the

396
00:16:18,533 --> 00:16:19,933
things they create.

397
00:16:20,000 --> 00:16:22,200
Someone - one person
recently asked me why I

398
00:16:22,266 --> 00:16:25,565
thought it took so long for Twitter and other companies

399
00:16:25,633 --> 00:16:27,933
to start remove hate
speech from these platforms.

400
00:16:28,000 --> 00:16:30,866
One of the things I said,
again, it is run by single

401
00:16:30,933 --> 00:16:33,133
people who have nobody -
who are accountable to

402
00:16:33,200 --> 00:16:36,466
nobody and don't seem
to understand the

403
00:16:36,533 --> 00:16:39,000
implications of things
or understand the

404
00:16:39,066 --> 00:16:42,766
consequences of things
or not self-reflective.

405
00:16:42,833 --> 00:16:44,733
A joke I always make is
that everyone in Silicon

406
00:16:44,799 --> 00:16:46,299
Valley can't see in the
mirror because they have

407
00:16:46,366 --> 00:16:47,766
no reflection whatsoever.

408
00:16:47,833 --> 00:16:50,266
They're unable to
reflect on anything.

409
00:16:50,333 --> 00:16:51,699
But the real fact of the
matter is their business

410
00:16:51,766 --> 00:16:53,933
plans depend on this.

411
00:16:54,000 --> 00:16:56,666
They don't have an ability
to see what's coming

412
00:16:56,733 --> 00:16:58,766
around the corner because
they're not looking at it.

413
00:16:58,833 --> 00:17:01,100
One of the things I always
say to people is the

414
00:17:01,166 --> 00:17:03,933
reason why so much is so
unsafe, not just from

415
00:17:04,000 --> 00:17:07,066
hacking but in general is
so unsafe is because the

416
00:17:07,133 --> 00:17:08,966
people who are creating
these things have never

417
00:17:09,032 --> 00:17:11,832
felt unsafe a day in their
lives and they do not

418
00:17:11,900 --> 00:17:14,099
understand what it
means to be unsafe.

419
00:17:14,165 --> 00:17:16,000
And so, they
don't anticipate.

420
00:17:16,066 --> 00:17:18,366
They're protected in all
kinds of ways and don't

421
00:17:18,433 --> 00:17:20,333
understand the
vulnerabilities that

422
00:17:20,400 --> 00:17:23,400
people have, whether it be
from cyberattacks, whether

423
00:17:23,465 --> 00:17:26,866
it be from hacking,
whenever it be from

424
00:17:26,933 --> 00:17:29,566
personal attacks or hate
speech or fake news or

425
00:17:29,633 --> 00:17:30,566
anything else.

426
00:17:30,633 --> 00:17:32,900
They operate in a
different system, but they

427
00:17:32,966 --> 00:17:34,399
have to begin to
understand what the

428
00:17:34,466 --> 00:17:37,933
experience is for the
rest of the human race.

429
00:17:38,000 --> 00:17:40,500
It is irresponsible.

430
00:17:40,566 --> 00:17:42,900
If your platform doesn't
hold people accountable to

431
00:17:42,966 --> 00:17:45,299
rules, then your rules
don't have much meaning.

432
00:17:45,366 --> 00:17:47,533
You don't see it just on
Reddit, but on Twitter

433
00:17:47,599 --> 00:17:49,533
where people are allowed
to break rules because of

434
00:17:49,599 --> 00:17:51,133
newsworthiness - in this
case, they're talking about

435
00:17:51,200 --> 00:17:54,299
Donald Trump, obviously -
which are very vague.

436
00:17:54,366 --> 00:17:56,700
People don't know what it
actually means and you end

437
00:17:56,766 --> 00:17:58,833
up with more and
more vitriol.

438
00:17:58,900 --> 00:18:00,566
When you think about
how these platforms are

439
00:18:00,633 --> 00:18:02,766
designed and the
architecture of them, the

440
00:18:02,833 --> 00:18:06,266
idea is around growth and
engagement, and engagement

441
00:18:06,333 --> 00:18:09,299
in these terms leads to
enragement and leads to

442
00:18:09,366 --> 00:18:12,900
people who are being - who
behave in ever more sloppy

443
00:18:12,966 --> 00:18:15,666
ways and every more
uncareful ways and ever

444
00:18:15,733 --> 00:18:16,966
more irresponsible ways.

445
00:18:17,033 --> 00:18:19,033
It encourages
irresponsibility and it

446
00:18:19,099 --> 00:18:22,599
encourages, in linking it
with addiction, it allows

447
00:18:22,666 --> 00:18:24,832
people to start to really
do things that they

448
00:18:24,900 --> 00:18:26,366
wouldn't necessarily do.

449
00:18:26,433 --> 00:18:30,166
It is very similar in
many ways to sugar or

450
00:18:30,233 --> 00:18:32,299
cigarettes or things like
that and when some people

451
00:18:32,366 --> 00:18:34,166
say I can just turn it
off, the fact of the

452
00:18:34,233 --> 00:18:37,299
matter is in many ways you can't, for lots of reasons.

453
00:18:37,366 --> 00:18:39,200
So, you are linked to
these devices and linked

454
00:18:39,266 --> 00:18:41,666
to these technologies in
ways that it can help you

455
00:18:41,733 --> 00:18:43,200
which is a great thing,
but it doesn't have the

456
00:18:43,266 --> 00:18:45,533
other side which is the
safety part of it that is

457
00:18:45,599 --> 00:18:47,599
needed to put in it
in all kinds of ways.

458
00:18:50,799 --> 00:18:51,966
It is ugly in many places.

459
00:18:52,033 --> 00:18:55,099
As everybody knows, if you
really examine the full

460
00:18:55,166 --> 00:18:57,099
surface of these
harms, polarization,

461
00:18:57,166 --> 00:19:00,066
radicalization,
outrage-ification, groups

462
00:19:00,133 --> 00:19:03,066
being marginalized, people
feeling threatened, and

463
00:19:03,133 --> 00:19:04,933
trolled, and also hacking
- I didn't include that in

464
00:19:05,000 --> 00:19:06,666
here - are these are the
direct consequences of

465
00:19:06,733 --> 00:19:09,899
this race to get attention
because that is the stuff

466
00:19:09,966 --> 00:19:12,433
that is best at getting
attention, it turns out,

467
00:19:12,500 --> 00:19:13,799
with outrage.

468
00:19:13,866 --> 00:19:16,166
For every word of moral
outrage you add to a

469
00:19:16,233 --> 00:19:18,233
tweet, for example, it
increases the retweet

470
00:19:18,299 --> 00:19:20,099
rate by 17%.

471
00:19:20,166 --> 00:19:22,666
So, if you are trying to
get people's attention,

472
00:19:22,733 --> 00:19:25,233
you see it politically
very easily, the stuff

473
00:19:25,299 --> 00:19:28,133
that is angry and mean
tends to overwhelm

474
00:19:28,200 --> 00:19:29,200
everything else.

475
00:19:29,266 --> 00:19:31,566
Obviously, we all just
experienced the second

476
00:19:31,633 --> 00:19:35,200
democratic debate which I
am still recovering from.

477
00:19:35,266 --> 00:19:37,500
It creates more if you can
do sound bites that are

478
00:19:37,566 --> 00:19:40,666
quickly retweeted or you
start attacking people in ways.

479
00:19:40,733 --> 00:19:43,500
It allows - it takes away
from the ability to do

480
00:19:43,566 --> 00:19:46,400
substantive discussions
and it actually works better.

481
00:19:46,466 --> 00:19:50,000
Unfortunately, again,
these businesses are

482
00:19:50,066 --> 00:19:54,733
architected that way
because it is - it works

483
00:19:54,799 --> 00:19:56,599
better for their
business plans.

484
00:19:56,666 --> 00:19:59,299
When - in the early days
when Google, for example,

485
00:19:59,366 --> 00:20:03,799
was first founded, it was
architected for speed,

486
00:20:03,866 --> 00:20:06,500
accuracy, and context when
you searched for something.

487
00:20:06,566 --> 00:20:08,666
When you searched for Ford
Motor Company, you tended

488
00:20:08,733 --> 00:20:10,866
to get Ford Motor Company.

489
00:20:10,933 --> 00:20:13,733
In the new paradigm with
a lot of companies, it is

490
00:20:13,799 --> 00:20:19,666
architected for speed,
still, engagement, and -

491
00:20:19,733 --> 00:20:25,899
speed, engagement - speed,
engagement, and growth.

492
00:20:25,966 --> 00:20:29,233
Wanting you to click
more or move further.

493
00:20:29,299 --> 00:20:32,066
So, those are very
different architectural

494
00:20:32,133 --> 00:20:36,266
ways to design a system
that creates more moral

495
00:20:36,333 --> 00:20:38,566
outrage or more anger
which then leads to more

496
00:20:38,633 --> 00:20:40,533
anger and more anger
behind that and it keeps

497
00:20:40,599 --> 00:20:42,299
people on these platforms.

498
00:20:42,366 --> 00:20:44,066
I know I am painting a
really dire picture.

499
00:20:44,133 --> 00:20:46,033
The other lady was
much nicer than me.

500
00:20:46,099 --> 00:20:48,233
I'm sorry.

501
00:20:48,299 --> 00:20:50,433
I didn't see the Mr.
Rogers movie at all.

502
00:20:50,500 --> 00:20:52,500
So, he's very nice.

503
00:20:52,566 --> 00:20:53,466
Very sweet.

504
00:20:53,533 --> 00:20:55,565
He's wonderful,
he's great.

505
00:20:55,633 --> 00:20:56,633
I'm sure it is lovely.

506
00:20:56,700 --> 00:20:58,633
Tom Hanks, who
doesn't love him?

507
00:20:58,700 --> 00:21:00,433
Anyway.

508
00:21:00,500 --> 00:21:02,166
It is loud.

509
00:21:02,233 --> 00:21:03,099
This is the
other part of it.

510
00:21:03,166 --> 00:21:04,033
It is super loud.

511
00:21:04,099 --> 00:21:05,166
It is designed to be loud.

512
00:21:05,233 --> 00:21:07,433
Loud creates a deafening
noise so that you are

513
00:21:07,500 --> 00:21:09,599
completely confused.

514
00:21:09,666 --> 00:21:12,299
When I was a young
reporter, I covered retail

515
00:21:12,366 --> 00:21:14,866
and I covered a company
called Circuit City which

516
00:21:14,933 --> 00:21:16,833
you may not remember but
it doesn't exist anymore.

517
00:21:16,900 --> 00:21:18,766
But it was you would walk
into Circuit City and

518
00:21:18,833 --> 00:21:21,700
there would be a wall of
televisions and you sort

519
00:21:21,766 --> 00:21:23,266
of look at it and you
stand in front of it.

520
00:21:23,333 --> 00:21:25,966
It was kind of beautiful
in a weird technology way,

521
00:21:26,033 --> 00:21:28,033
but it was big televisions
with big backs.

522
00:21:28,099 --> 00:21:31,066
Now they're all skinny but these were big, giant televisions.

523
00:21:31,133 --> 00:21:33,766
And I was there with the
CEO and we walked in and I

524
00:21:33,833 --> 00:21:36,233
was sort of like, what
are you doing here?

525
00:21:36,299 --> 00:21:39,000
He goes ah, the
wall of confusion.

526
00:21:39,066 --> 00:21:39,900
I was like what?

527
00:21:39,966 --> 00:21:42,065
He goes, we want to
confuse and upset people

528
00:21:42,133 --> 00:21:44,133
so we can trap them
down over there.

529
00:21:44,200 --> 00:21:46,866
It was sort of like - he
had a moment of honesty

530
00:21:46,933 --> 00:21:49,733
which was sort of great
for a reporter to hear.

531
00:21:49,799 --> 00:21:51,666
But the idea was that you
confuse and upset people

532
00:21:51,733 --> 00:21:53,533
like you would an animal
and then you lead them to

533
00:21:53,599 --> 00:21:56,765
the kill zone which was to
buy the TV they wanted to

534
00:21:56,833 --> 00:21:59,366
sell you at the time
because you were so confused.

535
00:21:59,433 --> 00:22:01,400
What they would do, they
would actually make some

536
00:22:01,466 --> 00:22:05,666
of the televisions - they
would purposely make them

537
00:22:05,733 --> 00:22:08,332
fuzzy so you wouldn't like
them and then put them

538
00:22:08,400 --> 00:22:09,266
next to another one.

539
00:22:09,333 --> 00:22:11,766
It was really brilliant in
an evil way, in an evil

540
00:22:11,833 --> 00:22:13,599
television way.

541
00:22:13,666 --> 00:22:14,700
If that is evil, really.

542
00:22:14,766 --> 00:22:17,133
But I was also in
admiration of it.

543
00:22:17,200 --> 00:22:19,366
And what it was,
it was loud.

544
00:22:19,433 --> 00:22:21,799
It was meant to make you
do something by confusing

545
00:22:21,866 --> 00:22:26,799
and upsetting you and making
you sort of move quickly.

546
00:22:26,866 --> 00:22:29,466
So, it sort of -
this is a quote.

547
00:22:29,533 --> 00:22:31,299
It is sort of a
civilizational moment when

548
00:22:31,366 --> 00:22:33,599
intelligent species, us,
we produce technology

549
00:22:33,666 --> 00:22:35,733
where the technology can
simulate the weaknesses

550
00:22:35,799 --> 00:22:36,900
of the creator.

551
00:22:36,966 --> 00:22:39,000
It is almost like a puppet
that we've created that we

552
00:22:39,066 --> 00:22:41,099
can actually simulate a
version of its creator and

553
00:22:41,166 --> 00:22:43,099
know exactly what puppet
strings to pull on the

554
00:22:43,166 --> 00:22:45,700
creator so we're
all outraged.

555
00:22:45,766 --> 00:22:48,566
We're building technology
that is taking advantage

556
00:22:48,633 --> 00:22:51,366
of us based on the
weaknesses that we have as

557
00:22:51,433 --> 00:22:52,766
a civilization.

558
00:22:52,833 --> 00:22:55,166
It is working quite
beautifully right now.

559
00:22:55,233 --> 00:22:57,866
It is really something
we have to, again, think

560
00:22:57,933 --> 00:23:01,200
about is how do we want to
design these things and

561
00:23:01,266 --> 00:23:04,000
create them so that we're
not essentially slaves to

562
00:23:04,066 --> 00:23:04,866
these technologies?

563
00:23:04,933 --> 00:23:07,333
And when they do give us -
when we're not scared of

564
00:23:07,400 --> 00:23:09,433
them, we're not scared
they're going to be hacked

565
00:23:09,500 --> 00:23:11,766
or they're going to take
advantage of us or cheat

566
00:23:11,833 --> 00:23:13,733
us or something like
that, where it is more

567
00:23:13,799 --> 00:23:16,633
technology that is helpful
to us, which the previous

568
00:23:16,700 --> 00:23:20,500
speaker was talking about
being helpful which I

569
00:23:20,566 --> 00:23:22,866
think is what the point of
them in the first place is.

570
00:23:26,200 --> 00:23:26,933
It is hacked.

571
00:23:27,000 --> 00:23:27,766
You all know about that.

572
00:23:27,833 --> 00:23:30,033
I'm not as expert as
you are on this issue.

573
00:23:30,099 --> 00:23:33,433
But you can just look at a couple of different things.

574
00:23:33,500 --> 00:23:35,000
It is constantly
being hacked.

575
00:23:35,066 --> 00:23:37,166
At the same time, it is
being used as a system as

576
00:23:37,233 --> 00:23:38,066
it was built.

577
00:23:38,133 --> 00:23:40,566
I always say, everyone's
sort of - the issue of

578
00:23:40,633 --> 00:23:41,766
whether the
Russians hacked.

579
00:23:41,833 --> 00:23:42,433
They didn't.

580
00:23:42,500 --> 00:23:43,400
They didn't hack
us actually.

581
00:23:43,466 --> 00:23:44,966
They used the system
as it was built.

582
00:23:45,033 --> 00:23:47,099
They were customers
of Facebook.

583
00:23:47,166 --> 00:23:49,533
When people always talk
about hacking or whatever

584
00:23:49,599 --> 00:23:51,133
political side is
discussing it, I'm always

585
00:23:51,200 --> 00:23:52,900
like, if they just
were using the tools as

586
00:23:52,966 --> 00:23:53,933
they were built.

587
00:23:54,000 --> 00:23:56,433
But the system remains
porous and it becomes more

588
00:23:56,500 --> 00:23:59,833
and more porous every day
as we add new devices to

589
00:23:59,900 --> 00:24:02,900
it, as we need new
apps, everything we do.

590
00:24:02,966 --> 00:24:05,265
It wasn't designed
with safety in mind in

591
00:24:05,333 --> 00:24:06,933
any way whatsoever.

592
00:24:07,000 --> 00:24:08,833
Even though there are all
kinds of safety areas

593
00:24:08,900 --> 00:24:12,133
around it, they're not
designed in ways to make

594
00:24:12,200 --> 00:24:14,400
it easy for people to
feel safe or be safe.

595
00:24:16,700 --> 00:24:19,200
And the worst part is the
companies, all of these

596
00:24:19,266 --> 00:24:22,433
companies, whether they be
tech companies or banks or

597
00:24:22,500 --> 00:24:25,466
pharmaceutical companies,
are battling nation states

598
00:24:25,533 --> 00:24:26,765
in this fight.

599
00:24:26,833 --> 00:24:29,466
They have to - you are
actually battling, whether

600
00:24:29,533 --> 00:24:31,166
you are Facebook or some
other companies, you are

601
00:24:31,233 --> 00:24:34,000
battling China, you are
battling Russia, you're

602
00:24:34,066 --> 00:24:35,466
battling Iran.

603
00:24:35,533 --> 00:24:36,966
That is not the
way it should be.

604
00:24:37,033 --> 00:24:38,866
Our government has
largely abrogated its

605
00:24:38,933 --> 00:24:42,799
responsibility in many
ways to really take this on,

606
00:24:42,866 --> 00:24:45,200
to take on these issues
and it leaves these companies

607
00:24:45,266 --> 00:24:50,066
vulnerable to constant
attacks and constant hacking.

608
00:24:50,133 --> 00:24:52,366
Political will is weakened
by this Trump influence

609
00:24:52,433 --> 00:24:54,833
saying it didn't happen,
saying it's a hoax, saying

610
00:24:54,900 --> 00:24:58,766
that somehow Don Lemon from
CNN did it, which he didn't.

611
00:24:58,833 --> 00:25:00,766
I don't think he knows how
to turn on his computer.

612
00:25:00,833 --> 00:25:03,799
I'm pretty sure he wasn't
responsible for the

613
00:25:03,866 --> 00:25:06,133
hacking the Russians did.

614
00:25:06,200 --> 00:25:07,866
But the political will is
weakened when you call

615
00:25:07,933 --> 00:25:10,200
into question that it is
happening and everyone who

616
00:25:10,266 --> 00:25:12,400
is in cybersecurity
does understand.

617
00:25:12,466 --> 00:25:15,299
You're in a constant state
of hacking and a constant

618
00:25:15,366 --> 00:25:18,566
state of the white hats
versus the black hats.

619
00:25:18,633 --> 00:25:20,633
In the case of the black
hats, often these nation

620
00:25:20,700 --> 00:25:22,433
states that
are doing this.

621
00:25:22,500 --> 00:25:24,966
But political will to do anything about it is weakened.

622
00:25:25,033 --> 00:25:28,533
The election - the fact
that election safety stuff

623
00:25:28,599 --> 00:25:31,599
did not get through the
Senate for political

624
00:25:31,666 --> 00:25:34,899
reasons is probably one of
the most depressing parts

625
00:25:34,966 --> 00:25:38,233
of what's been going on
because fair elections

626
00:25:38,299 --> 00:25:40,066
should be - fair elections
where people can - where

627
00:25:40,133 --> 00:25:42,066
their vote can be counted
is critically important

628
00:25:42,133 --> 00:25:44,833
and there are plenty of
great solutions out there.

629
00:25:44,900 --> 00:25:47,266
Microsoft has a bunch that
are really interesting;

630
00:25:47,333 --> 00:25:50,066
all kinds of companies are
very innovative in making

631
00:25:50,133 --> 00:25:52,433
sure our votes
are protected.

632
00:25:52,500 --> 00:25:55,500
But we spend a lot of time
arguing over whether we

633
00:25:55,566 --> 00:25:58,166
should have this in
place and because of the

634
00:25:58,233 --> 00:25:59,899
political will and
because it gets dragged -

635
00:25:59,966 --> 00:26:01,832
everything gets dragged
into politics these days,

636
00:26:01,900 --> 00:26:03,533
we can't do things that
are critically important

637
00:26:03,599 --> 00:26:05,200
to maintaining
a democracy.

638
00:26:05,266 --> 00:26:07,299
What is most important,
what has happened with the

639
00:26:07,366 --> 00:26:10,533
Russians, I was a
communist studies major in

640
00:26:10,599 --> 00:26:14,000
college, of all things,
but we'll never find out

641
00:26:14,066 --> 00:26:14,700
the true impact.

642
00:26:14,766 --> 00:26:17,166
The whole point is
to create discord.

643
00:26:17,233 --> 00:26:18,966
Right now, there is a lot
of arguments of whose side

644
00:26:19,033 --> 00:26:19,632
Russia is on.

645
00:26:19,700 --> 00:26:21,200
They're on their side.

646
00:26:21,266 --> 00:26:24,033
They're on their side to
create discord and anger

647
00:26:24,099 --> 00:26:26,399
within our country
to weaken us.

648
00:26:26,466 --> 00:26:28,299
They may have a
preference, they may not

649
00:26:28,366 --> 00:26:29,366
have a preference.

650
00:26:29,433 --> 00:26:31,200
We'll never know the true
impact of what they're

651
00:26:31,266 --> 00:26:33,299
doing but what they are
absolutely doing is

652
00:26:33,366 --> 00:26:34,299
creating discord.

653
00:26:34,366 --> 00:26:35,866
I will give one
quick example.

654
00:26:35,933 --> 00:26:38,366
If you remember a couple
years ago, Roseanne Barr

655
00:26:38,433 --> 00:26:40,799
said something stupid
which just means it's an

656
00:26:40,866 --> 00:26:42,500
average Wednesday for her.

657
00:26:42,566 --> 00:26:44,833
She said something dumb
about Valerie Jarrett.

658
00:26:44,900 --> 00:26:46,099
She shouldn't
have said it.

659
00:26:46,166 --> 00:26:47,666
And everyone piled on her.

660
00:26:47,733 --> 00:26:50,566
And then a couple of days
later, Sam B, who is on

661
00:26:50,633 --> 00:26:52,599
the other side of the
spectrum from her, said

662
00:26:52,666 --> 00:26:53,966
something stupid
about Ivanka Trump.

663
00:26:54,033 --> 00:26:56,466
Shouldn't have said it, it
was rude, it was stupid.

664
00:26:56,533 --> 00:26:57,933
Everyone piled on her.

665
00:26:58,000 --> 00:27:01,833
I had a company that was
looking at tweets, look at

666
00:27:01,900 --> 00:27:04,733
the attacks on these and
how rage was created.

667
00:27:04,799 --> 00:27:08,366
It turned out the bots
that were attacking

668
00:27:08,433 --> 00:27:11,533
Roseanne Barr were also
attacking - they were the

669
00:27:11,599 --> 00:27:15,299
exact same bots and they
were also on the other side.

670
00:27:15,366 --> 00:27:18,000
So, it was the same bots
attacking each other to

671
00:27:18,066 --> 00:27:20,900
generate outrage which
then dragged in humanity.

672
00:27:20,966 --> 00:27:21,899
People then started.

673
00:27:21,966 --> 00:27:25,500
It certainly wasn't
started by regular people.

674
00:27:25,566 --> 00:27:27,200
Most of the people went
oh, Roseanne Barr said

675
00:27:27,266 --> 00:27:28,766
something stupid and
didn't get that upset by

676
00:27:28,833 --> 00:27:32,333
it, but then became as it
was cycled through not

677
00:27:32,400 --> 00:27:34,799
just Twitter but cable
news and everything else

678
00:27:34,866 --> 00:27:36,400
and the political atmosphere.

679
00:27:36,466 --> 00:27:39,332
And so, we'll never find
out its true impact or

680
00:27:39,400 --> 00:27:41,466
whether it changed votes,
but it certainly was

681
00:27:41,533 --> 00:27:43,832
designed to create
confusion, just like I was

682
00:27:43,900 --> 00:27:46,400
talking about in Circuit
City, and as more devices

683
00:27:46,466 --> 00:27:49,132
are connected, it is like
pushing back the ocean.

684
00:27:49,200 --> 00:27:50,533
My son goes
around our house.

685
00:27:50,599 --> 00:27:52,899
I have them in our house
because I test out a lot

686
00:27:52,966 --> 00:27:56,000
of these different
home products, Alexa or

687
00:27:56,066 --> 00:27:57,000
things like that.

688
00:27:57,066 --> 00:27:59,933
My son actually goes
around and unplugs them

689
00:28:00,000 --> 00:28:02,700
and he goes, electricity
will beat them all or

690
00:28:02,766 --> 00:28:03,933
lack of it.

691
00:28:04,000 --> 00:28:04,866
I like my son a lot.

692
00:28:04,933 --> 00:28:06,000
I think he's
smarter than I am.

693
00:28:06,066 --> 00:28:09,866
He doesn't want any of
those things in the home.

694
00:28:09,933 --> 00:28:11,533
But as they're more
connected and people are

695
00:28:11,599 --> 00:28:14,166
connecting and become at
ease with them, it is

696
00:28:14,233 --> 00:28:16,399
going to be - it is
going to present a real

697
00:28:16,466 --> 00:28:20,132
challenge for
cybersecurity, people who

698
00:28:20,200 --> 00:28:22,333
are interested in
protecting people, and

699
00:28:22,400 --> 00:28:25,233
it's going to be a real
opportunity for people who

700
00:28:25,299 --> 00:28:27,366
want to hack into
people's homes.

701
00:28:27,433 --> 00:28:29,099
Obviously, you have seen
all the stories with Ring

702
00:28:29,166 --> 00:28:31,299
and people talking to
people in their homes and

703
00:28:31,366 --> 00:28:32,299
things like that.

704
00:28:32,366 --> 00:28:33,799
You are just going to see
more and more of that as

705
00:28:33,866 --> 00:28:35,599
people become more
comfortable with these

706
00:28:35,666 --> 00:28:38,966
technologies and at the
same time, get more taken

707
00:28:39,033 --> 00:28:41,332
advantage of them by them.

708
00:28:41,400 --> 00:28:44,666
So very quickly, I am not
- you don't have to get a

709
00:28:44,733 --> 00:28:46,566
lecture from me about
climate change.

710
00:28:46,633 --> 00:28:47,099
It is here.

711
00:28:47,166 --> 00:28:47,866
It is happening.

712
00:28:47,933 --> 00:28:49,400
It is going to be
something that is going to

713
00:28:49,466 --> 00:28:52,199
be devastating
to our planet.

714
00:28:52,266 --> 00:28:54,633
There is more need for - but
it is also an opportunity.

715
00:28:54,700 --> 00:28:56,200
There is more need for
businesses that operate

716
00:28:56,266 --> 00:28:58,099
and react quickly to
the quick changing

717
00:28:58,166 --> 00:29:00,700
environment, more reliance
on AI, quantum computing,

718
00:29:00,766 --> 00:29:02,433
and more predictive
analysis and risk

719
00:29:02,500 --> 00:29:04,533
management, and more
opportunities for

720
00:29:04,599 --> 00:29:06,265
businesses to help
their customers.

721
00:29:06,333 --> 00:29:07,900
When I did say that the
first trillionaire is

722
00:29:07,966 --> 00:29:11,000
going to be a climate tech
person, what I meant to

723
00:29:11,066 --> 00:29:13,599
say is instead of - there
is no real challenge in

724
00:29:13,666 --> 00:29:15,599
our country that hasn't
been met by a combination

725
00:29:15,666 --> 00:29:19,433
of marketplace solutions,
government help and the

726
00:29:19,500 --> 00:29:20,933
citizenry that is
interested in it.

727
00:29:21,000 --> 00:29:22,466
That is what has
got to happen here.

728
00:29:22,533 --> 00:29:24,832
You have seen it happen
many times before the AIDS

729
00:29:24,900 --> 00:29:27,799
crisis, the fact that we
have an AIDS - almost

730
00:29:27,866 --> 00:29:29,633
curing AIDS in a
very short time.

731
00:29:29,700 --> 00:29:31,733
It was a combination of a
lot of things and climate

732
00:29:31,799 --> 00:29:33,233
change is going to have
to have that happen.

733
00:29:33,299 --> 00:29:34,766
It is going to require
a massive government

734
00:29:34,833 --> 00:29:37,333
presence but also a
lot of innovation.

735
00:29:37,400 --> 00:29:39,599
And obviously climate
change presents probably

736
00:29:39,666 --> 00:29:43,166
the most existential
security threat to our

737
00:29:43,233 --> 00:29:47,500
planet which is existence,
which is why it is existential.

738
00:29:47,566 --> 00:29:50,333
And lastly, it's pandemic.

739
00:29:50,400 --> 00:29:54,099
What are the rising risks of epidemics like coronavirus?

740
00:29:54,166 --> 00:29:56,233
It is called
novel coronavirus.

741
00:29:56,299 --> 00:29:57,933
In a hopelessly
interconnected world, how

742
00:29:58,000 --> 00:30:00,166
does the rise of
tele-education, telecommuting,

743
00:30:00,233 --> 00:30:01,832
and tele-business
have on security?

744
00:30:01,900 --> 00:30:03,666
It's issues we have to be
thinking about because

745
00:30:03,733 --> 00:30:05,700
we're going to see
more of these epidemics

746
00:30:05,766 --> 00:30:06,933
as we move forward.

747
00:30:07,000 --> 00:30:08,733
And how do you build
secure systems that are

748
00:30:08,799 --> 00:30:11,966
both robust and safe and
allow people to operate in

749
00:30:12,033 --> 00:30:14,599
a world where, in cases
like what is coming, which

750
00:30:14,666 --> 00:30:21,666
and CDC is warning of,
it could be very bad.

751
00:30:21,733 --> 00:30:22,799
One of the last things I
want to say is that
it is fixable.

752
00:30:22,866 --> 00:30:25,133
This is not I am going to
go out on a positive note

753
00:30:25,200 --> 00:30:26,933
because it is fixable
because so many of these

754
00:30:27,000 --> 00:30:29,599
technologies are so life
changing, so amazing.

755
00:30:29,666 --> 00:30:32,466
The beginning of the internet was meant to be amazing.

756
00:30:32,533 --> 00:30:35,065
If you were there at the
beginning, it really did

757
00:30:35,133 --> 00:30:37,400
have a feeling of
hopefulness, of

758
00:30:37,466 --> 00:30:41,332
connectivity, of bringing
our humanity into one thought.

759
00:30:41,400 --> 00:30:43,266
It was sort of the
Star Trek future.

760
00:30:43,333 --> 00:30:45,299
I always think the world
is either you're either a

761
00:30:45,366 --> 00:30:47,966
Star Trek person which is
hopeful or you're a Star

762
00:30:48,033 --> 00:30:51,199
Wars person where you are
not because the empire is

763
00:30:51,266 --> 00:30:53,533
always freaking striking
back and it often - I mean,

764
00:30:53,599 --> 00:30:55,700
I know the last one but that wasn't a good movie anyway.

765
00:30:55,766 --> 00:30:57,000
Sorry, Disney.

766
00:30:57,066 --> 00:30:58,766
That sucked.

767
00:30:58,833 --> 00:31:01,566
But - sorry Bob Iger.

768
00:31:01,633 --> 00:31:02,500
Have a nice life.

769
00:31:02,566 --> 00:31:03,733
You can quit now.

770
00:31:03,799 --> 00:31:07,266
So - he's actually a
really clever man.

771
00:31:07,333 --> 00:31:09,666
In any case, it is fixable
and like climate change,

772
00:31:09,733 --> 00:31:12,133
it can be catastrophic if
it is not fixed, if we

773
00:31:12,200 --> 00:31:13,133
don't fix it.

774
00:31:13,200 --> 00:31:15,133
Unlike climate change,
there is only really about

775
00:31:15,200 --> 00:31:17,799
a thousand people among
five or six or seven

776
00:31:17,866 --> 00:31:21,066
companies that really need
to change what they're doing.

777
00:31:21,133 --> 00:31:24,500
We can have an impact
on them as citizens, as

778
00:31:24,566 --> 00:31:27,966
journalists, as employees
and everything else.

779
00:31:28,033 --> 00:31:30,299
So, I think it is really
important to know that the

780
00:31:30,366 --> 00:31:32,633
system itself is not
what's wrong, it is how it

781
00:31:32,700 --> 00:31:34,500
is being deployed.

782
00:31:34,566 --> 00:31:37,033
There is a great book that
I leave you with called

783
00:31:37,099 --> 00:31:40,765
'Tools and Weapons' by
Brad Smith from Microsoft.

784
00:31:40,833 --> 00:31:41,799
It is a really great book.

785
00:31:41,866 --> 00:31:43,633
I did a great
podcast with him.

786
00:31:43,700 --> 00:31:46,099
Right now, the technology
we have and technology we

787
00:31:46,166 --> 00:31:48,566
are going to be creating
going forward, whether it

788
00:31:48,633 --> 00:31:52,133
be space travel, whether
it be VR, whether it be

789
00:31:52,200 --> 00:31:54,366
heliography - I'm waiting
for the time machine to be

790
00:31:54,433 --> 00:31:57,333
created by someone,
please, very soon.

791
00:31:57,400 --> 00:32:00,266
It can be either a tool or
a weapon and we have to be

792
00:32:00,333 --> 00:32:02,666
thinking of it, how we can
make it a tool and not a

793
00:32:02,733 --> 00:32:04,599
weapon that can be
used against us.

794
00:32:04,666 --> 00:32:06,966
I think that is really
something that is critical

795
00:32:07,033 --> 00:32:10,033
in jobs like yours is to
think about how we can use

796
00:32:10,099 --> 00:32:12,832
your technologies, use
your knowledge, use your

797
00:32:12,900 --> 00:32:15,766
abilities to make these
technologies and make them

798
00:32:15,833 --> 00:32:18,666
the greatest thing
they can be rather than

799
00:32:18,733 --> 00:32:19,966
our worst nightmare.

800
00:32:20,033 --> 00:32:21,433
Anyway, thank
you very much.

801
00:32:21,500 --> 00:32:22,799
Have a great rest
of the conference.

802
00:32:22,866 --> 00:32:24,333
Thank you.

