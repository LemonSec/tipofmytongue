1
00:00:06,599 --> 00:00:08,666
>> SPEAKER: To lead
today's discussion, please

2
00:00:08,733 --> 00:00:11,800
welcome moderator
Craig Spiezle.

3
00:00:15,466 --> 00:00:16,532
>> CRAIG SPIEZLE:
Thank you.

4
00:00:16,600 --> 00:00:17,600
Thank you, everyone.

5
00:00:17,666 --> 00:00:19,533
Hey, I really appreciate
everyone joining today.

6
00:00:19,600 --> 00:00:21,533
I think this session is
going to - promises to be

7
00:00:21,600 --> 00:00:25,233
one of the most
interesting, colorful, and

8
00:00:25,300 --> 00:00:29,233
perhaps debateful topic
with lots of different

9
00:00:29,300 --> 00:00:30,566
perspectives we have.

10
00:00:30,633 --> 00:00:32,732
I'm really fortunate to
have with us today a

11
00:00:32,799 --> 00:00:35,733
really distinguished
panel of experts, Katie

12
00:00:35,799 --> 00:00:38,766
Arrington who is a CISO
for Acquisitions for

13
00:00:38,833 --> 00:00:42,299
Department of Defense,
Andy Purdy, CISO of Huawei

14
00:00:42,366 --> 00:00:46,133
USA, Bruce Schneier,
Security Technologist at

15
00:00:46,200 --> 00:00:50,100
Harvard University and
others, and last but not

16
00:00:50,166 --> 00:00:52,166
least, Kathryn Waldron
who is a fellow at

17
00:00:52,233 --> 00:00:54,000
R Street Institute.

18
00:00:54,066 --> 00:00:55,533
I also want to, by the
way, if you're wondering

19
00:00:55,600 --> 00:00:57,233
what's going on to the
side, a good friend of

20
00:00:57,299 --> 00:01:01,366
mine, Summer Cordy, is a
graphic recorder and she's

21
00:01:01,433 --> 00:01:03,833
going to be capturing some
of the key points and

22
00:01:03,899 --> 00:01:06,633
issues throughout the
afternoon, and hopefully

23
00:01:06,700 --> 00:01:07,966
at the end we'll
have a good graphic

24
00:01:08,033 --> 00:01:14,433
representation of
the takeaways here.

25
00:01:14,500 --> 00:01:16,500
This session has been
kind of crowdsourced.

26
00:01:16,566 --> 00:01:19,700
I've received over 100
submissions or I should

27
00:01:19,766 --> 00:01:23,500
say suggestions of points
of what we should talk about.

28
00:01:23,566 --> 00:01:25,933
And, again, we're focused
on supply chain, but we're

29
00:01:26,000 --> 00:01:29,000
going to try in the next
50 or 60 minutes to go

30
00:01:29,066 --> 00:01:30,400
over these five points here.

31
00:01:30,466 --> 00:01:32,798
We'll talk about some of
the complexity of the

32
00:01:32,866 --> 00:01:34,299
supply chain.

33
00:01:34,366 --> 00:01:37,266
How do you really keep
it secure and resilient?

34
00:01:37,333 --> 00:01:39,933
Backdoors are a really big
issue a lot of times, so

35
00:01:40,000 --> 00:01:42,766
we'll talk about some of
the myths and realities,

36
00:01:42,833 --> 00:01:44,166
I think.

37
00:01:44,233 --> 00:01:46,099
And then we're going to
really drill into some of

38
00:01:46,166 --> 00:01:48,866
the issues that are top of
mind in the news, and that

39
00:01:48,933 --> 00:01:54,066
is the issue of is this a
trade war and/or is it an

40
00:01:54,133 --> 00:01:55,933
issue of critical
infrastructure.

41
00:01:56,000 --> 00:01:57,866
And then I think we want
to really drill down in

42
00:01:57,933 --> 00:02:00,833
the role of testing and
some things road ahead.

43
00:02:00,900 --> 00:02:02,933
I doubt we're going to
have time for Q&A, but I

44
00:02:03,000 --> 00:02:05,166
do have several things
that have been already

45
00:02:05,233 --> 00:02:06,500
submitted to us and
we're going to try to

46
00:02:06,566 --> 00:02:07,633
go through those.

47
00:02:09,566 --> 00:02:10,733
These are some
of the headlines.

48
00:02:10,800 --> 00:02:12,466
If you look at these
headlines or any of the

49
00:02:12,533 --> 00:02:15,232
news, it has
been a whirlwind.

50
00:02:15,300 --> 00:02:17,599
Different governments'
point of view, confusing

51
00:02:17,666 --> 00:02:22,533
messages, conflicting
maybe even conflated here.

52
00:02:22,599 --> 00:02:24,632
These are some things
that make this afternoon

53
00:02:24,699 --> 00:02:26,099
exciting, I think.

54
00:02:26,166 --> 00:02:28,400
So, with that, to kick
this off, I've asked each

55
00:02:28,466 --> 00:02:30,533
of our panelists to give
a brief opening comment.

56
00:02:30,599 --> 00:02:33,033
And starting with Katie.

57
00:02:33,099 --> 00:02:34,833
>> KATIE ARRINGTON: So,
good afternoon, everyone.

58
00:02:34,900 --> 00:02:37,800
I do apologize, I have a
little bit of laryngitis,

59
00:02:37,866 --> 00:02:41,000
but I figured you'd all
lump through it with us.

60
00:02:41,066 --> 00:02:42,066
I'm Katie Arrington.

61
00:02:42,133 --> 00:02:43,800
I'm the CISO for
Acquisition and

62
00:02:43,866 --> 00:02:45,599
Sustainment over at DoD.

63
00:02:45,666 --> 00:02:50,266
My job is to, partially, I
oversee the supply chain

64
00:02:50,333 --> 00:02:53,000
risk management for the
Department of Defense, and

65
00:02:53,066 --> 00:02:54,366
that includes the DIB.

66
00:02:54,433 --> 00:02:56,166
So the supply chain
is something that

67
00:02:56,233 --> 00:02:57,900
resonates with me.

68
00:02:57,966 --> 00:03:01,199
I thank you for inviting
me on this lovely panel

69
00:03:01,266 --> 00:03:02,632
today, and I'm
sure we'll have a

70
00:03:02,699 --> 00:03:05,633
very robust discussion.

71
00:03:05,699 --> 00:03:06,733
>> CRAIG SPIEZLE: Great. Andy?

72
00:03:06,800 --> 00:03:08,099
>> ANDY PURDY: It's a
pleasure to be here.

73
00:03:08,166 --> 00:03:09,599
I'm particularly pleased
that all four of the

74
00:03:09,666 --> 00:03:12,933
members of the panel
are here today.

75
00:03:13,000 --> 00:03:17,300
As Art Coviello suggested
yesterday, this is your time.

76
00:03:17,366 --> 00:03:19,366
It's critically important
for the cybersecurity

77
00:03:19,433 --> 00:03:23,266
community to help America
move forward as we become

78
00:03:23,333 --> 00:03:25,833
increasingly
dependent on ICT.

79
00:03:25,900 --> 00:03:28,633
It's more and more
important, finally, that

80
00:03:28,699 --> 00:03:31,833
the U.S. walk the talk
for cybersecurity.

81
00:03:31,900 --> 00:03:33,500
I ask you some questions.

82
00:03:33,566 --> 00:03:36,699
What does history tell us
about the importance of

83
00:03:36,766 --> 00:03:39,399
America's national
security to the free

84
00:03:39,466 --> 00:03:41,400
countries of the world?

85
00:03:41,466 --> 00:03:45,500
Should the U.S. private
sector lead a U.S. technology

86
00:03:45,566 --> 00:03:49,866
industrial policy, because do
we have a path forward for the U.S.

87
00:03:49,933 --> 00:03:52,733
to continue to lead in the
competition for technology

88
00:03:52,800 --> 00:03:56,166
and innovation with China,
about the campaign against

89
00:03:56,233 --> 00:03:59,266
Huawei, against Huawei's
ability to purchase from

90
00:03:59,333 --> 00:04:00,599
American companies?

91
00:04:00,666 --> 00:04:03,400
Who is more likely to
be heard, Huawei or the

92
00:04:03,466 --> 00:04:05,300
American companies
that support the U.S.

93
00:04:05,366 --> 00:04:07,366
defense industrial base?

94
00:04:07,433 --> 00:04:09,632
And before Huawei's gear
is ripped and replaced

95
00:04:09,699 --> 00:04:12,033
from the companies serving
rural America, should

96
00:04:12,099 --> 00:04:14,566
experts consider
the consequences?

97
00:04:14,633 --> 00:04:17,500
Should effective risk
mitigation measures

98
00:04:17,565 --> 00:04:19,132
be considered?

99
00:04:19,199 --> 00:04:23,133
Will rip and replace take
more time and more money

100
00:04:23,199 --> 00:04:24,933
than anticipated?

101
00:04:25,000 --> 00:04:27,233
What should the United
States focus on that's

102
00:04:27,300 --> 00:04:29,300
most important
for America?

103
00:04:29,366 --> 00:04:31,800
Are you willing to help
America have a clear-eyed

104
00:04:31,866 --> 00:04:35,199
focus on national security
and maintaining American

105
00:04:35,266 --> 00:04:37,933
competitiveness
in the world?

106
00:04:38,000 --> 00:04:40,199
>> CRAIG SPIEZLE:
Great, Andy. Bruce.

107
00:04:40,266 --> 00:04:42,233
>> BRUCE SCHNEIER: Hi.
Happy to be here.

108
00:04:42,300 --> 00:04:46,366
I want to make three quick
points about the general topic.

109
00:04:46,433 --> 00:04:48,566
First, supply chain
security is an

110
00:04:48,633 --> 00:04:51,666
insurmountably hard problem.

111
00:04:51,733 --> 00:04:54,966
This is not - we can put
backdoors in systems that

112
00:04:55,033 --> 00:04:56,233
cannot be found.

113
00:04:56,300 --> 00:04:58,133
We know that, especially
if you control the hardware.

114
00:04:58,199 --> 00:05:03,366
And it's not just do you
trust the country in which

115
00:05:03,433 --> 00:05:04,933
the company whose
equipment you have to

116
00:05:05,000 --> 00:05:06,600
trust is located.

117
00:05:06,666 --> 00:05:08,600
This might be an American
product, but this is not

118
00:05:08,666 --> 00:05:10,600
made in the U.S.

119
00:05:10,666 --> 00:05:12,066
Its parts don't
come from the U.S.

120
00:05:12,133 --> 00:05:14,698
Its programmers carry 100
different passports, any

121
00:05:14,766 --> 00:05:16,866
one of which could
subvert this system.

122
00:05:16,933 --> 00:05:19,633
When you think of supply
chain, do not just think

123
00:05:19,699 --> 00:05:21,366
of the company.

124
00:05:21,433 --> 00:05:22,599
Worry about
Apple equipment.

125
00:05:22,666 --> 00:05:23,300
Worry about Cisco.

126
00:05:23,366 --> 00:05:25,265
Worry about everything.

127
00:05:25,333 --> 00:05:28,233
And there is no way I as
a security engineer can

128
00:05:28,300 --> 00:05:31,233
secure against a backdoor
when someone controls

129
00:05:31,300 --> 00:05:34,266
hardware, software,
and assembly.

130
00:05:34,333 --> 00:05:37,866
Second, there is a lot
more to 5G security than

131
00:05:37,933 --> 00:05:38,733
supply chain.

132
00:05:38,800 --> 00:05:43,300
5G is insecure primarily
because the protocols are

133
00:05:43,366 --> 00:05:45,832
insecure, because the
governments, like the

134
00:05:45,899 --> 00:05:49,899
United States, like to
use the systems to spy.

135
00:05:49,966 --> 00:05:51,899
And we do not have
secure protocols.

136
00:05:51,966 --> 00:05:54,866
Some of them we inherited
from 4G and 3G that are

137
00:05:54,933 --> 00:05:56,933
insecure, moving into 5G.

138
00:05:57,000 --> 00:05:59,966
We want to secure 5G, we
have to do a lot more than

139
00:06:00,033 --> 00:06:00,866
worry about the supply chain.

140
00:06:00,933 --> 00:06:06,000
And third, tying national
security to trade policy

141
00:06:06,066 --> 00:06:08,933
makes for impossible
security trade-offs.

142
00:06:09,000 --> 00:06:12,033
Either this is a national
security issue, in which

143
00:06:12,100 --> 00:06:15,566
case there are things we
do and we don't do, or

144
00:06:15,633 --> 00:06:18,933
this is a trade issue, in
which case we negotiate on

145
00:06:19,000 --> 00:06:19,833
a variety of things.

146
00:06:19,899 --> 00:06:20,966
It cannot be both.

147
00:06:21,033 --> 00:06:23,033
It just doesn't work.

148
00:06:23,100 --> 00:06:24,333
So that's what I'll start with.

149
00:06:28,399 --> 00:06:29,899
>> CRAIG SPIEZLE: Great.

150
00:06:29,966 --> 00:06:30,766
>> KATHRYN WALDRON: Oh, man.

151
00:06:30,833 --> 00:06:31,899
I really want to get into
that last statement you

152
00:06:31,966 --> 00:06:33,233
just made, but I'll save that
for the rest of the panel.

153
00:06:33,300 --> 00:06:34,600
>> KATIE ARRINGTON:
I'm biting my tongue.

154
00:06:34,666 --> 00:06:37,166
>> KATHRYN WALDRON: I'm Kathryn
Waldron. I'm a fellow at the

155
00:06:37,233 --> 00:06:38,533
R Street Institute in DC.

156
00:06:38,600 --> 00:06:40,733
We're a think tank.

157
00:06:40,800 --> 00:06:43,733
Man, I just want to address
that, but I'll wait.

158
00:06:43,800 --> 00:06:46,466
Just as an opening
statement, I'll just say I

159
00:06:46,533 --> 00:06:49,300
think all of us on stage
can agree that supply

160
00:06:49,366 --> 00:06:52,433
chain security is a vital
component of American

161
00:06:52,500 --> 00:06:54,233
national security, which
means we get to then

162
00:06:54,300 --> 00:06:56,733
address the much more fun
question of determining

163
00:06:56,800 --> 00:06:59,199
whether or not a company
is a weak link in the

164
00:06:59,266 --> 00:07:02,199
supply chain and then what
do you do when a company

165
00:07:02,266 --> 00:07:05,033
is a weak link and what
justifies taking that link

166
00:07:05,100 --> 00:07:07,566
out of the supply
chain entirely.

167
00:07:07,633 --> 00:07:10,265
I think we're going to
hear a wide variety of

168
00:07:10,333 --> 00:07:13,866
debate on that issue
because supply chain is so

169
00:07:13,933 --> 00:07:15,233
context specific.

170
00:07:15,300 --> 00:07:17,033
Not only do you have to
take into consideration

171
00:07:17,100 --> 00:07:18,966
technical solutions.

172
00:07:19,033 --> 00:07:22,333
You have to take into
consideration the

173
00:07:22,399 --> 00:07:24,233
pervasiveness of the
infrastructure in

174
00:07:24,300 --> 00:07:26,699
question, you have to take
into account the history

175
00:07:26,766 --> 00:07:29,733
and structure of the
company in question, and

176
00:07:29,800 --> 00:07:32,033
the history and legal
structure of the company

177
00:07:32,100 --> 00:07:34,466
of origin, among
many other factors.

178
00:07:34,533 --> 00:07:37,533
I think the United States
has quite rightly raised

179
00:07:37,600 --> 00:07:39,600
concerns on all these
issues, these factors

180
00:07:39,666 --> 00:07:40,500
about Huawei.

181
00:07:40,566 --> 00:07:44,633
But as we've seen the
U.S.'s limited success, or

182
00:07:44,699 --> 00:07:47,699
perhaps not success
depending on how you want

183
00:07:47,766 --> 00:07:49,433
to quantify it, in regards
to their strategy of

184
00:07:49,500 --> 00:07:51,633
essentially kicking Huawei
out of the global system,

185
00:07:51,699 --> 00:07:54,266
I think raises some
questions in terms of is

186
00:07:54,333 --> 00:07:57,366
that a good model for
dealing with national

187
00:07:57,433 --> 00:08:00,300
security and supply chain
security going forward.

188
00:08:00,366 --> 00:08:02,233
I'm going to argue
that, no, it's not.

189
00:08:02,300 --> 00:08:05,500
We need to have a much
more holistic structure

190
00:08:05,566 --> 00:08:07,832
approach that not only
looks at the risk of the

191
00:08:07,899 --> 00:08:11,800
moment but then also what
sort of policies can we

192
00:08:11,866 --> 00:08:14,532
put in place that will
have positive economic

193
00:08:14,600 --> 00:08:19,266
growth and will provide
market competitors.

194
00:08:19,333 --> 00:08:20,500
>> CRAIG SPIEZLE: Great.

195
00:08:20,566 --> 00:08:22,099
Well, actually, I've just
advanced the slide and

196
00:08:22,166 --> 00:08:24,832
focusing on the 5G - I'm
not sure if it's up there,

197
00:08:24,899 --> 00:08:27,533
it is up there, great -
of the supply chain.

198
00:08:27,600 --> 00:08:29,933
I think it talks about
everything here.

199
00:08:30,000 --> 00:08:31,366
Bruce, I'm glad you
mentioned standards

200
00:08:31,433 --> 00:08:32,600
because that's one
of the challenges.

201
00:08:32,666 --> 00:08:34,299
You look at it,
it's very complex.

202
00:08:34,366 --> 00:08:36,000
There are lots of
components, lots of

203
00:08:36,066 --> 00:08:39,000
players, and each one of
them has subcomponents or

204
00:08:39,066 --> 00:08:40,200
ingredients within them.

205
00:08:40,265 --> 00:08:44,533
Actually, Andy, I'm going
to start with you, give

206
00:08:44,600 --> 00:08:45,700
you the first chance.

207
00:08:45,766 --> 00:08:47,333
Again, we want to try to
keep comments to roughly

208
00:08:47,399 --> 00:08:48,399
a minute here.

209
00:08:48,466 --> 00:08:52,299
But, again, Huawei is
just one component of it.

210
00:08:52,366 --> 00:08:55,333
You spend a lot of time
working with carriers.

211
00:08:55,399 --> 00:08:57,566
What are some of the other
risks from operators and

212
00:08:57,633 --> 00:08:59,533
such and other equipment
providers as you look at

213
00:08:59,600 --> 00:09:01,066
the supply chain?

214
00:09:01,133 --> 00:09:02,399
>> ANDY PURDY: Well, I
think we need to look at

215
00:09:02,466 --> 00:09:05,166
it kind of globally and
from the U.S. perspective.

216
00:09:05,233 --> 00:09:07,699
What is most important to
U.S. national security?

217
00:09:07,766 --> 00:09:10,833
We saw in the recent issue
of the DoD publication

218
00:09:10,899 --> 00:09:12,466
Prism an article by Tom Donahue.

219
00:09:12,533 --> 00:09:16,399
I suggest that you all read
about the worst possible day.

220
00:09:16,466 --> 00:09:18,366
We need to make sure that
experts come together to

221
00:09:18,433 --> 00:09:21,399
look at that scenario
because we believe there

222
00:09:21,466 --> 00:09:24,533
are other attack vectors
that could be used, and we

223
00:09:24,600 --> 00:09:27,033
need to make sure the
experts come together to

224
00:09:27,100 --> 00:09:28,333
consider those attack vectors.

225
00:09:28,399 --> 00:09:30,933
And we need to think
about, well, are we going -

226
00:09:31,000 --> 00:09:34,366
to one of Bruce's points -
are we going to consider

227
00:09:34,433 --> 00:09:37,566
a vendor trusted
because they're not

228
00:09:37,633 --> 00:09:38,500
headquartered in China?

229
00:09:38,566 --> 00:09:39,833
I don't think so.

230
00:09:39,899 --> 00:09:42,633
I think, as I'm starting
to learn from you all,

231
00:09:42,700 --> 00:09:44,100
that we can't trust anybody.

232
00:09:44,166 --> 00:09:46,833
That's the approach I
think we need to take in

233
00:09:46,899 --> 00:09:47,666
addressing the risk.

234
00:09:47,733 --> 00:09:49,366
>> CRAIG SPIEZLE:
Actually, I want to jump in.

235
00:09:49,433 --> 00:09:50,833
Bruce, you wrote an
article, I think it was in

236
00:09:50,899 --> 00:09:54,233
September, and I think
there was a challenge

237
00:09:54,299 --> 00:09:56,399
building a
trustworthy ecosystem from

238
00:09:56,466 --> 00:09:57,799
untrustworthy components.

239
00:09:57,866 --> 00:09:58,533
>> BRUCE SCHNEIER: Yeah.

240
00:09:58,600 --> 00:09:59,799
Actually, this is
something - I think this

241
00:09:59,866 --> 00:10:01,533
is a research question
worthy of our industry.

242
00:10:01,600 --> 00:10:03,799
I was going to close with
that, but we'll do it now.

243
00:10:03,866 --> 00:10:06,366
The internet was
invented to answer a

244
00:10:06,433 --> 00:10:07,399
very specific question.

245
00:10:07,466 --> 00:10:11,600
Can you build a reliable
network out of unreliable parts?

246
00:10:11,666 --> 00:10:13,000
The answer turns out to be yes.

247
00:10:13,066 --> 00:10:15,833
There is a similar
question that would be

248
00:10:15,899 --> 00:10:17,600
great if we could answer.

249
00:10:17,666 --> 00:10:20,100
Can we build a trustworthy
network out of

250
00:10:20,166 --> 00:10:21,299
untrustworthy parts?

251
00:10:21,366 --> 00:10:24,299
And I don't know if the
answer is yes or not

252
00:10:24,366 --> 00:10:25,899
because we are going to
be living in a world of

253
00:10:25,966 --> 00:10:29,833
untrustworthy parts, and
they might come from

254
00:10:29,899 --> 00:10:32,133
China, they might come
from another country, they

255
00:10:32,200 --> 00:10:36,133
might come from - right,
they might be Swiss crypto

256
00:10:36,200 --> 00:10:41,266
equipment, and yet we
have to somehow get

257
00:10:41,333 --> 00:10:44,533
trustworthiness out
of that mishmash.

258
00:10:44,600 --> 00:10:48,299
I think that's a research
question that is Maja

259
00:10:48,366 --> 00:10:51,000
DARPA funding level
and something we

260
00:10:51,066 --> 00:10:52,600
should think about.

261
00:10:52,666 --> 00:10:53,566
>> CRAIG SPIEZLE: Great.

262
00:10:53,633 --> 00:10:54,666
Well, Katie, you know,
this is kind of in your

263
00:10:54,733 --> 00:10:56,632
wheelhouse a little bit.

264
00:10:56,700 --> 00:10:59,033
You're Acquisitions for
all of Department of Defense.

265
00:10:59,100 --> 00:11:00,966
How do you secure everything?

266
00:11:01,033 --> 00:11:02,466
>> KATIE ARRINGTON: Well,
you buy down the risk and

267
00:11:02,533 --> 00:11:04,233
you buy up the
uncertainty. Right?

268
00:11:04,299 --> 00:11:05,399
You can't secure everything.

269
00:11:05,466 --> 00:11:09,266
But what you can do is
when you have highly

270
00:11:09,333 --> 00:11:12,565
suspect, we have our own
data, the recommendation

271
00:11:12,633 --> 00:11:15,700
was made to take Huawei out
for a very specific reason.

272
00:11:15,766 --> 00:11:17,366
The law is the law.

273
00:11:17,433 --> 00:11:19,533
We can sit here and
juxtapose about all the

274
00:11:19,600 --> 00:11:23,166
things, right, but
the law is the law.

275
00:11:23,233 --> 00:11:25,899
And in my job, I work at
the Department of Defense,

276
00:11:25,966 --> 00:11:27,833
I'm going to enforce the law.

277
00:11:27,899 --> 00:11:31,766
When we have a known
vulnerability and that the

278
00:11:31,833 --> 00:11:34,632
compiled risk of everyone
in this room, if you're a

279
00:11:34,700 --> 00:11:38,233
U.S. citizen, there is a
compiled risk, and our job

280
00:11:38,299 --> 00:11:41,299
in the DoD is to make sure
that you're safe, that

281
00:11:41,366 --> 00:11:43,799
we're doing our best
to buy down the risk.

282
00:11:43,866 --> 00:11:48,233
When we have our data,
which we do have, which

283
00:11:48,299 --> 00:11:49,866
are reasons we are doing
the things that we are

284
00:11:49,933 --> 00:11:54,333
doing, that is our way
to buy down the risk.

285
00:11:54,399 --> 00:11:56,200
I harken the room.

286
00:11:56,266 --> 00:12:00,466
We talk about 5G, right,
and how compelling it is.

287
00:12:00,533 --> 00:12:04,433
I'm of the mindset it's
not going to be that

288
00:12:04,500 --> 00:12:06,600
impactful to the
Department of Defense as

289
00:12:06,666 --> 00:12:09,366
far as what primarily
matters to me, which are

290
00:12:09,433 --> 00:12:11,733
my weapons systems,
which is my critical

291
00:12:11,799 --> 00:12:15,066
infrastructure that
I have control over.

292
00:12:15,133 --> 00:12:17,833
We have a different
mindset, obviously.

293
00:12:17,899 --> 00:12:21,466
But the challenge point
with all the panelists in

294
00:12:21,533 --> 00:12:23,299
the room is we're the
people that actually have

295
00:12:23,366 --> 00:12:27,600
to do it, and I couldn't
in good conscious do it

296
00:12:27,666 --> 00:12:30,533
unless we took Huawei out.

297
00:12:30,600 --> 00:12:32,233
It's too much of a risk.

298
00:12:32,299 --> 00:12:35,233
We have much bigger things
that we can focus on to

299
00:12:35,299 --> 00:12:36,966
buy down risk.

300
00:12:37,033 --> 00:12:40,399
Having this out of our
most trusted environments

301
00:12:40,466 --> 00:12:45,100
isn't something that we in
the DoD even are concerned

302
00:12:45,166 --> 00:12:46,366
it's an impediment.

303
00:12:46,433 --> 00:12:49,933
It's a have to do because
the risk is so high.

304
00:12:50,000 --> 00:12:51,266
>> CRAIG SPIEZLE: As
expected, we have some

305
00:12:51,333 --> 00:12:53,100
different points of view
here, and I'm sure Andy

306
00:12:53,166 --> 00:12:54,299
has some response to that.

307
00:12:54,366 --> 00:12:57,199
But we actually want to
jump to this next slide

308
00:12:57,266 --> 00:12:59,833
and we're really talking
about one of the key

309
00:12:59,899 --> 00:13:01,233
questions here.

310
00:13:01,299 --> 00:13:03,833
I'm going to go to the
other end of the panel.

311
00:13:03,899 --> 00:13:06,000
Kathryn wrote, you wrote
also an article, as I

312
00:13:06,066 --> 00:13:10,100
recall, and it talked
about is supply chain risk

313
00:13:10,166 --> 00:13:13,500
is by judging a
country of origin.

314
00:13:13,566 --> 00:13:16,799
Is that an
appropriate metric?

315
00:13:16,866 --> 00:13:18,433
That was about three
months ago, I think,

316
00:13:18,500 --> 00:13:19,266
you wrote that.

317
00:13:19,333 --> 00:13:19,966
Where are you?

318
00:13:20,033 --> 00:13:21,033
Have you changed on that?

319
00:13:21,100 --> 00:13:22,733
Is it any different?

320
00:13:22,799 --> 00:13:24,132
>> KATHRYN WALDRON: The
piece I wrote, which was

321
00:13:24,200 --> 00:13:28,000
in response to our tech
talk, pretty much argued

322
00:13:28,066 --> 00:13:31,200
that the U.S. government
should beware taking

323
00:13:31,266 --> 00:13:35,000
drastic action towards Chinese
companies, which isn't to

324
00:13:35,066 --> 00:13:36,399
say that there aren't
significant national

325
00:13:36,466 --> 00:13:38,600
security concerns when
it comes to Chinese companies.

326
00:13:38,666 --> 00:13:40,033
I very much
believe there are.

327
00:13:40,100 --> 00:13:44,299
However, the point I
raised in the article was

328
00:13:44,366 --> 00:13:48,665
that all the concern is
frequently what are they

329
00:13:48,733 --> 00:13:49,565
going to do with my data.

330
00:13:49,633 --> 00:13:50,666
China is going to take my data.

331
00:13:50,733 --> 00:13:52,799
They're building a database.

332
00:13:52,866 --> 00:13:54,233
We don't know what they're
going to do with it.

333
00:13:54,299 --> 00:13:57,299
And we haven't really
talked - we haven't had an

334
00:13:57,366 --> 00:13:59,366
answer in the national
conversation about what

335
00:13:59,433 --> 00:14:01,833
data constitutes the
national security risk.

336
00:14:01,899 --> 00:14:04,399
Part of the reason we
don't know is because we

337
00:14:04,466 --> 00:14:05,733
don't know how data
is going to be used

338
00:14:05,799 --> 00:14:06,600
in the future.

339
00:14:06,666 --> 00:14:09,433
But without knowing the
answer to that, it's also

340
00:14:09,500 --> 00:14:13,166
hard to judge the
proportionate response to

341
00:14:13,233 --> 00:14:16,165
any particular company.

342
00:14:16,233 --> 00:14:18,632
So while I definitely
agree with Bruce's point

343
00:14:18,700 --> 00:14:22,500
that we cannot just look
at country of origin, like

344
00:14:22,566 --> 00:14:25,533
you need to be equally
suspicious of American

345
00:14:25,600 --> 00:14:28,333
companies, European
companies, at the same

346
00:14:28,399 --> 00:14:31,200
time I would also agree
with Katie that country of

347
00:14:31,266 --> 00:14:35,033
origin, for some companies
and some infrastructure,

348
00:14:35,100 --> 00:14:37,200
there's just too high of a
risk for us to be really

349
00:14:37,266 --> 00:14:40,000
comfortable with including
them in our supply chain.

350
00:14:40,066 --> 00:14:42,000
>> KATIE ARRINGTON: And
we do all kinds of risk

351
00:14:42,066 --> 00:14:44,433
mitigation, to
your point on that.

352
00:14:44,500 --> 00:14:48,466
Just because it comes from
China, North Korea, Iran,

353
00:14:48,533 --> 00:14:52,500
Russia, we always are
looking at the source code.

354
00:14:52,566 --> 00:14:55,333
Like first things first,
let's all go back to where

355
00:14:55,399 --> 00:14:59,633
the source code was
created, who created it,

356
00:14:59,700 --> 00:15:01,700
and how has it been
transmitted since then.

357
00:15:01,766 --> 00:15:05,199
I work on a great deal of that.

358
00:15:05,266 --> 00:15:06,165
Part of my job.

359
00:15:06,233 --> 00:15:08,733
I can tell you companies
think that they're doing

360
00:15:08,799 --> 00:15:11,933
risk mitigation practices,
especially in software.

361
00:15:12,000 --> 00:15:15,200
They're woefully
underestimating because I

362
00:15:15,266 --> 00:15:17,366
use - you know, when I
talk about the CMMC and I

363
00:15:17,433 --> 00:15:20,566
go out and talk to people,
I say everybody thinks

364
00:15:20,633 --> 00:15:23,299
when they walk out of the
room in the morning, when

365
00:15:23,366 --> 00:15:25,833
they walk away from the
mirror, they look great.

366
00:15:25,899 --> 00:15:28,700
Everyone assumes that the
risk mitigation and the

367
00:15:28,766 --> 00:15:31,899
testing that I'm doing is
good because I say I'm good.

368
00:15:31,966 --> 00:15:35,433
It's that when you put the
mirror up and you say,

369
00:15:35,500 --> 00:15:37,866
yeah, no, you didn't draw
your eyebrows on right

370
00:15:37,933 --> 00:15:39,766
today, it's a different game.

371
00:15:39,833 --> 00:15:41,000
That's part of our problem.

372
00:15:41,066 --> 00:15:42,399
We have to look at these things.

373
00:15:42,466 --> 00:15:45,666
We have to look at them
and not be country centric.

374
00:15:45,733 --> 00:15:48,632
We need to understand what
it is we want to be able

375
00:15:48,700 --> 00:15:51,966
to get out of it, what
testing we say needs to be

376
00:15:52,033 --> 00:15:55,633
done to be able to use it
so we can determine the risk.

377
00:15:55,700 --> 00:15:56,666
>> CRAIG SPIEZLE:
Actually, speaking about

378
00:15:56,733 --> 00:15:59,099
eyebrows and fixing them.

379
00:15:59,166 --> 00:16:00,666
Bruce, I know you
work on it real hard.

380
00:16:00,733 --> 00:16:02,799
As, really, the security
technologist, I'm sure you

381
00:16:02,866 --> 00:16:04,165
have some views on this.

382
00:16:04,233 --> 00:16:05,433
>> BRUCE SCHNEIER:
Actually, I want to point

383
00:16:05,500 --> 00:16:06,533
out to make sure you
understand what the

384
00:16:06,600 --> 00:16:08,166
risk is here.

385
00:16:08,233 --> 00:16:10,532
The risk actually isn't spying.

386
00:16:10,600 --> 00:16:12,233
Do you remember the story
of the African Union?

387
00:16:12,299 --> 00:16:14,000
The African Union got a
new headquarters, I think

388
00:16:14,066 --> 00:16:18,000
it was in Nairobi, and China
built the infrastructure.

389
00:16:18,066 --> 00:16:20,166
A couple of years ago they
discovered that every

390
00:16:20,233 --> 00:16:22,433
night all the data was
being shipped back to Shanghai.

391
00:16:22,500 --> 00:16:23,566
Surprise.

392
00:16:23,633 --> 00:16:26,333
That actually isn't the
worry because you'll

393
00:16:26,399 --> 00:16:27,733
discover that.

394
00:16:27,799 --> 00:16:30,699
If 5G routers are shipping
a copy of all the data

395
00:16:30,766 --> 00:16:33,165
back to China, we
will find that out.

396
00:16:33,233 --> 00:16:35,699
That's not what is
going to happen.

397
00:16:35,766 --> 00:16:37,799
It's not going to be some
kind of traffic shaping

398
00:16:37,866 --> 00:16:40,966
BGR attack because only
the United States has the

399
00:16:41,033 --> 00:16:43,966
infrastructure to catch,
to perform that and to

400
00:16:44,033 --> 00:16:45,100
get the data.

401
00:16:45,166 --> 00:16:48,466
It's going to be some kind
of sleeper command that

402
00:16:48,533 --> 00:16:52,933
has equipment turn off or
degrade when it's turned on.

403
00:16:53,000 --> 00:16:54,933
That's the worry here.

404
00:16:55,000 --> 00:16:57,633
That equipment - the U.S.

405
00:16:57,700 --> 00:16:59,433
is going to do this, too,
duh, because it's such

406
00:16:59,500 --> 00:17:00,733
a good idea.

407
00:17:00,799 --> 00:17:03,433
The equipment gets out
into the world and then we

408
00:17:03,500 --> 00:17:05,465
have the ability
to control it

409
00:17:05,532 --> 00:17:06,433
when something happens.

410
00:17:06,500 --> 00:17:07,733
That's the risk.

411
00:17:07,799 --> 00:17:10,165
That's what we're
concerned about.

412
00:17:10,233 --> 00:17:13,265
That is extraordinarily
hard to detect.

413
00:17:13,333 --> 00:17:16,066
Lots of competent
engineers at this show can

414
00:17:16,133 --> 00:17:18,500
put in that backdoor and
it'll never be discovered.

415
00:17:18,566 --> 00:17:21,966
This is what we
have to worry about.

416
00:17:22,032 --> 00:17:23,799
>> CRAIG SPIEZLE: Let's
jump to backdoors and then

417
00:17:23,866 --> 00:17:26,566
I'm going to let Andy kind
of maybe take the first.

418
00:17:26,633 --> 00:17:29,066
There's a lot of
accusations, a lot of

419
00:17:29,133 --> 00:17:30,933
statements about backdoors.

420
00:17:31,000 --> 00:17:33,500
Sometimes Bruce and I, we
talked about it the other

421
00:17:33,566 --> 00:17:36,900
day, is a vulnerability a
backdoor if someone finds it?

422
00:17:36,966 --> 00:17:39,765
People are making a lot of
jumping to conclusions in

423
00:17:39,833 --> 00:17:40,900
a lot of this area.

424
00:17:40,966 --> 00:17:43,265
Andy, perhaps you can
provide some insights on this.

425
00:17:43,333 --> 00:17:44,933
>> ANDY PURDY: Let me use
that as a basis to follow

426
00:17:45,000 --> 00:17:48,033
up on two points and I'll
frame them as questions.

427
00:17:48,099 --> 00:17:51,166
Is America ready for
the worst possible day?

428
00:17:51,233 --> 00:17:53,566
Back in Hurricane Katrina
when everything was down,

429
00:17:53,633 --> 00:17:54,900
the question was do
you have satellite

430
00:17:54,966 --> 00:17:57,166
communications, some kind
of satellite phone so you

431
00:17:57,233 --> 00:17:58,666
can communicate?

432
00:17:58,733 --> 00:18:03,066
Is America ready when our
ability to communicate

433
00:18:03,133 --> 00:18:07,133
with our forces, both in the
U.S. and overseas, are degraded?

434
00:18:07,200 --> 00:18:08,000
Are we ready?

435
00:18:08,066 --> 00:18:11,900
And isn't it true that at
least five nations of the

436
00:18:11,966 --> 00:18:14,000
world can virtually
implant hidden

437
00:18:14,066 --> 00:18:17,866
functionality and backdoors
in hardware and software?

438
00:18:17,933 --> 00:18:21,666
If those two things are
true, please, help America

439
00:18:21,733 --> 00:18:24,366
be safer, help figure out
how we can come up with

440
00:18:24,433 --> 00:18:26,900
uniform standards and
conformance programs,

441
00:18:26,966 --> 00:18:30,533
testing, ongoing testing
and continuous monitoring

442
00:18:30,599 --> 00:18:32,700
to help make
sure we're safe.

443
00:18:32,766 --> 00:18:35,866
>> KATIE ARRINGTON: I
don't disagree in the

444
00:18:35,933 --> 00:18:39,166
philosophy, but I can tell
you, like the continuous

445
00:18:39,233 --> 00:18:41,166
monitoring, DoD, we're
going to be doing it.

446
00:18:41,233 --> 00:18:44,099
We're doing it, right, how
we communicate with our

447
00:18:44,166 --> 00:18:44,933
global partners.

448
00:18:45,000 --> 00:18:48,000
But when there is an
exacerbated big risk, it's

449
00:18:48,066 --> 00:18:52,099
much easier to say put
this one off to the side

450
00:18:52,166 --> 00:18:54,099
because we do have a
multitude of others

451
00:18:54,166 --> 00:18:55,133
to deal with.

452
00:18:55,200 --> 00:18:58,500
And this is the problem is
that we have so many risks

453
00:18:58,566 --> 00:19:02,766
globally, internally, that
we shouldn't be stuck on

454
00:19:02,833 --> 00:19:07,200
we know this is a known
big risk and why.

455
00:19:07,266 --> 00:19:09,400
It has been we have our data.

456
00:19:09,466 --> 00:19:11,599
We have our research.

457
00:19:11,666 --> 00:19:13,866
I don't know that
anybody on the panel has

458
00:19:13,933 --> 00:19:16,733
classified - that can see
classified information.

459
00:19:16,799 --> 00:19:18,466
I can tell you where we sit.

460
00:19:18,533 --> 00:19:21,799
There is a reason why
we did what we did.

461
00:19:21,866 --> 00:19:25,500
Backdoors being what they are,
that isn't the problem.

462
00:19:25,566 --> 00:19:29,633
It's when you are willing
to convey control to

463
00:19:29,700 --> 00:19:33,533
another country is a
problem in the United

464
00:19:33,599 --> 00:19:35,799
States, end of
story, period.

465
00:19:35,866 --> 00:19:39,500
If the origins of the
particular product were

466
00:19:39,566 --> 00:19:42,099
wonderful and they were
fraught with amazing

467
00:19:42,166 --> 00:19:45,166
pieces of democracy, I
would say wonderful.

468
00:19:45,233 --> 00:19:47,366
But they're not.

469
00:19:47,433 --> 00:19:48,733
I'm not willing to risk it.

470
00:19:48,799 --> 00:19:52,799
It's obviously by law, by
legislation, by executive

471
00:19:52,866 --> 00:19:57,866
order, there are much higher
people that agree with this.

472
00:19:57,933 --> 00:20:00,733
>> CRAIG SPIEZLE: Andy?

473
00:20:00,799 --> 00:20:02,633
>> ANDY PURDY: I encourage
the experts there because

474
00:20:02,700 --> 00:20:04,400
I do not believe America
is ready for the

475
00:20:04,466 --> 00:20:05,866
worst possible day.

476
00:20:05,933 --> 00:20:08,733
I hope we will concentrate
on trying to address those

477
00:20:08,799 --> 00:20:11,266
risks to make sure that
America is ready to

478
00:20:11,333 --> 00:20:13,166
respond when we have to.

479
00:20:13,233 --> 00:20:15,200
I do not believe that
right now we are ready.

480
00:20:15,266 --> 00:20:17,500
I hope we will focus some
of the experts in this

481
00:20:17,566 --> 00:20:20,033
community to help make
sure America is ready.

482
00:20:20,099 --> 00:20:22,299
>> BRUCE SCHNEIER: Our
only hope now is like our

483
00:20:22,366 --> 00:20:27,166
stuff won't work, but their
stuff won't work either. Right?

484
00:20:27,233 --> 00:20:29,099
The hope is that we are
just so embedded in their

485
00:20:29,166 --> 00:20:31,433
supply chain that like
nothing is going to work.

486
00:20:31,500 --> 00:20:33,466
I think that's probably
our best hope for the

487
00:20:33,533 --> 00:20:35,233
worst possible day.

488
00:20:35,299 --> 00:20:37,099
>> CRAIG SPIEZLE: I guess
on that note, we're kind

489
00:20:37,166 --> 00:20:39,299
of advancing to the
next slide here.

490
00:20:39,366 --> 00:20:40,966
I think that's already
happened a little bit in

491
00:20:41,033 --> 00:20:42,233
some other areas.

492
00:20:42,299 --> 00:20:45,799
I'm going to go to
Kathryn on this one.

493
00:20:45,866 --> 00:20:48,433
In a sense, the
headlines about the U.S.

494
00:20:48,500 --> 00:20:51,233
infiltrating, compromising
products, Cisco routers

495
00:20:51,299 --> 00:20:53,733
that were being
shipped to Syria.

496
00:20:53,799 --> 00:20:56,833
NSA with RSA here, many of
you may remember I think

497
00:20:56,900 --> 00:20:58,333
it was 2003 -

498
00:20:58,400 --> 00:20:59,366
>> BRUCE SCHNEIER: You
should have put up that

499
00:20:59,433 --> 00:21:00,833
picture that we could all
see classified information.

500
00:21:00,900 --> 00:21:02,166
>> CRAIG SPIEZLE: Yeah.

501
00:21:02,233 --> 00:21:04,200
Is this the pot calling
the kettle black?

502
00:21:04,266 --> 00:21:05,966
>> KATHRYN WALDRON:
Yeah. I mean, yes.

503
00:21:06,033 --> 00:21:08,399
All companies - all
countries are engaged in

504
00:21:08,466 --> 00:21:10,500
spying through various
different means.

505
00:21:10,566 --> 00:21:13,200
I don't think that's
a surprise to anyone.

506
00:21:13,266 --> 00:21:14,966
But to get back on the
issue of backdoors, which

507
00:21:15,033 --> 00:21:16,533
I thought was interesting
and which I did not

508
00:21:16,599 --> 00:21:17,599
get asked on.

509
00:21:17,666 --> 00:21:18,533
>> CRAIG SPIEZLE: We
can go back to that.

510
00:21:18,599 --> 00:21:19,599
>> KATHRYN WALDRON: Go back.

511
00:21:19,666 --> 00:21:20,500
>> KATIE ARRINGTON: I want to
go back to what you just said.

512
00:21:20,566 --> 00:21:21,866
>> KATHRYN WALDRON:
Yeah, okay.

513
00:21:21,933 --> 00:21:23,466
But I think one thing to
keep in mind which isn't

514
00:21:23,533 --> 00:21:25,399
typically mentioned in the
context of supply chain in

515
00:21:25,466 --> 00:21:28,632
regard to backdoors is how
other conversations going

516
00:21:28,700 --> 00:21:32,000
on in regard to backdoors
in the policy world could

517
00:21:32,066 --> 00:21:34,333
have interesting impacts
on the supply chain

518
00:21:34,400 --> 00:21:35,000
side of it.

519
00:21:35,066 --> 00:21:36,599
There has been a
resurgence lately, I

520
00:21:36,666 --> 00:21:39,000
think, in the going dark
debate in regard to

521
00:21:39,066 --> 00:21:41,599
backdoors and whether
or not the U.S. government

522
00:21:41,666 --> 00:21:44,666
should be -
should essentially require

523
00:21:44,733 --> 00:21:46,799
companies to build in
backdoors for the sake of

524
00:21:46,866 --> 00:21:50,099
law enforcement and child
exploitation online, which

525
00:21:50,166 --> 00:21:50,933
is a very noble cause.

526
00:21:51,000 --> 00:21:54,666
But I think we need to be
aware of how encouraging

527
00:21:54,733 --> 00:21:58,000
backdoors in that policy
debate could therefore

528
00:21:58,066 --> 00:22:00,366
lead to terrible
precedence in regard to

529
00:22:00,433 --> 00:22:02,333
building backdoors that
could be vulnerabilities

530
00:22:02,400 --> 00:22:03,500
in the supply
chain debate.

531
00:22:03,566 --> 00:22:05,533
And so just because you
think you're having two

532
00:22:05,599 --> 00:22:07,099
different policy
discussions doesn't mean

533
00:22:07,166 --> 00:22:08,433
you're not going to
accidentally end up

534
00:22:08,500 --> 00:22:11,366
somewhere where you've
compromised both of them.

535
00:22:11,433 --> 00:22:13,200
>> KATIE ARRINGTON: I want
to fundamentally go back

536
00:22:13,266 --> 00:22:14,599
to a point you just made.

537
00:22:14,666 --> 00:22:15,733
>> CRAIG SPIEZLE: I was
going to get Bruce.

538
00:22:15,799 --> 00:22:16,700
>> KATIE ARRINGTON: Okay.

539
00:22:16,766 --> 00:22:17,900
>> CRAIG SPIEZLE: And
then we'll let you.

540
00:22:17,966 --> 00:22:19,199
>> BRUCE SCHNEIER: So I
think this is important.

541
00:22:19,266 --> 00:22:20,466
This is the way you
have to think about it.

542
00:22:20,533 --> 00:22:23,065
One internet, one
world, one answer.

543
00:22:23,133 --> 00:22:26,666
If we are going to be
as a country saying that

544
00:22:26,733 --> 00:22:30,265
security is paramount,
then we need to build

545
00:22:30,333 --> 00:22:33,666
systems that are secure
and no one gets to spy.

546
00:22:33,733 --> 00:22:36,633
Either no one gets to spy
or everyone gets to spy.

547
00:22:36,700 --> 00:22:38,833
What the U.S. likes
is this we get to

548
00:22:38,900 --> 00:22:40,866
spy and nobody else does.

549
00:22:40,933 --> 00:22:43,900
That's a great position
and it was fun to have for

550
00:22:43,966 --> 00:22:47,033
a while and we had the
tech chops to do it, but

551
00:22:47,099 --> 00:22:49,033
those days are over.

552
00:22:49,099 --> 00:22:52,633
If we're going to champion
a secure 5G system - I

553
00:22:52,700 --> 00:22:55,566
guess 5G is too late - a
secure 6G system, we're

554
00:22:55,633 --> 00:22:58,700
going to recognize that
security will come at the

555
00:22:58,766 --> 00:23:00,599
expense of surveillance,
and that will be

556
00:23:00,666 --> 00:23:02,533
surveillance for China
and surveillance

557
00:23:02,599 --> 00:23:03,500
for the United States.

558
00:23:03,566 --> 00:23:06,266
If we like the fact that
we can use the cellular

559
00:23:06,333 --> 00:23:09,266
network to spy on our
adversaries, then they get

560
00:23:09,333 --> 00:23:10,666
to spy on us.

561
00:23:10,733 --> 00:23:11,733
Pick one.

562
00:23:11,799 --> 00:23:13,299
You can't have both.

563
00:23:13,366 --> 00:23:14,700
Just pick one.

564
00:23:17,166 --> 00:23:20,399
>> KATIE ARRINGTON: With
that, the point that you

565
00:23:20,466 --> 00:23:23,933
made, I don't know that,
at this community, they

566
00:23:24,000 --> 00:23:28,599
quite get the cultural shift
in the Department of Defense.

567
00:23:28,666 --> 00:23:29,566
>> BRUCE SCHNEIER:
Probably not.

568
00:23:29,633 --> 00:23:31,233
>> KATIE ARRINGTON:
Okay. There's a big -

569
00:23:31,299 --> 00:23:32,799
>> BRUCE SCHNEIER:
We're not DoD people.

570
00:23:32,866 --> 00:23:34,366
I'm just saying.

571
00:23:34,433 --> 00:23:35,900
>> KATIE ARRINGTON: We
have changed culture in

572
00:23:35,966 --> 00:23:39,733
the past 18 to 24
months where security

573
00:23:39,799 --> 00:23:41,633
has become foundational.

574
00:23:41,700 --> 00:23:42,633
Right?

575
00:23:42,700 --> 00:23:45,933
You either build it on a
secure environment because

576
00:23:46,000 --> 00:23:49,099
- as secure as you can at
the time - because cost,

577
00:23:49,166 --> 00:23:51,265
schedule, performance
do not matter in

578
00:23:51,333 --> 00:23:52,766
an unsecured environment.

579
00:23:52,833 --> 00:23:54,566
It doesn't matter if you
deliver something in the

580
00:23:54,633 --> 00:23:57,500
supply chain at the
approved cost or at the

581
00:23:57,566 --> 00:23:59,533
performance that you
agreed upon or the

582
00:23:59,599 --> 00:24:02,200
schedule if you get
it and it don't work. Right?

583
00:24:02,266 --> 00:24:03,233
Your adversary has it.

584
00:24:03,299 --> 00:24:04,866
Your competitor has it.

585
00:24:04,933 --> 00:24:08,866
The question - the point
that I was going to hit on

586
00:24:08,933 --> 00:24:14,766
with what Kathryn said was
it's really interesting

587
00:24:14,833 --> 00:24:18,433
when you bring up we
do it to each other. Right?

588
00:24:18,500 --> 00:24:21,933
But capitalism is
a beautiful thing. Right?

589
00:24:22,000 --> 00:24:25,033
What one product vendor
in the United States or a

590
00:24:25,099 --> 00:24:27,666
product vendor in the UK
or a product vendor in

591
00:24:27,733 --> 00:24:32,566
Australia, they have
a disparate mission.

592
00:24:32,633 --> 00:24:34,700
Theirs is not unified.

593
00:24:34,766 --> 00:24:38,766
It is not with a five-year
global plan to take

594
00:24:38,833 --> 00:24:41,266
economic domination over.

595
00:24:41,333 --> 00:24:44,733
It's not to take
away human rights.

596
00:24:44,799 --> 00:24:48,766
It's not to prevail
communism for the most part.

597
00:24:48,833 --> 00:24:52,766
So, yes, you do have to
assume some risk in

598
00:24:52,833 --> 00:24:53,966
your supply chain.

599
00:24:54,033 --> 00:24:54,832
You always will.

600
00:24:54,900 --> 00:24:57,266
Nothing will ever
be 100% secure.

601
00:24:57,333 --> 00:24:58,566
We know that.

602
00:24:58,633 --> 00:25:02,566
But when you have a
product that could take

603
00:25:02,633 --> 00:25:07,366
over, run, manipulate the
most critical things in

604
00:25:07,433 --> 00:25:12,000
our country, why would you
not want to be sure that

605
00:25:12,066 --> 00:25:16,200
company has all the right
philosophical endeavors to

606
00:25:16,266 --> 00:25:18,866
which they don't, period.

607
00:25:18,933 --> 00:25:20,033
You can talk all you want.

608
00:25:20,099 --> 00:25:20,899
Right?

609
00:25:20,966 --> 00:25:22,366
You can talk about will
you make this trade-off;

610
00:25:22,433 --> 00:25:23,700
will you make
that trade-off.

611
00:25:23,766 --> 00:25:25,500
I don't want to be in a
world - to your point,

612
00:25:25,566 --> 00:25:28,333
Andy - where I wake up
one morning and the banks

613
00:25:28,400 --> 00:25:33,400
don't work and the traffic
lights don't work and

614
00:25:33,466 --> 00:25:35,132
break, break, break down.

615
00:25:35,200 --> 00:25:39,133
I want to make sure that
that control remains here

616
00:25:39,200 --> 00:25:40,533
where I can touch you.

617
00:25:40,599 --> 00:25:42,399
>> ANDY PURDY: But, Katie,
is it true or is it not

618
00:25:42,466 --> 00:25:44,899
true that at least five
nations of the world have

619
00:25:44,966 --> 00:25:47,399
the power to virtually
implant hidden

620
00:25:47,466 --> 00:25:49,966
functionality in hardware
and software and launch

621
00:25:50,033 --> 00:25:52,033
later a virtual tech.

622
00:25:52,099 --> 00:25:53,033
>> KATIE ARRINGTON:
That is not -

623
00:25:53,099 --> 00:25:54,332
>> ANDY PURDY:
Is that true?

624
00:25:54,400 --> 00:25:55,400
>> BRUCE SCHNEIER: Of
course, it's true. Which five?

625
00:25:55,466 --> 00:25:57,000
Probably 30.
Probably 50. Which five?

626
00:25:57,066 --> 00:25:57,933
>> KATIE ARRINGTON:
That's ridiculous.

627
00:25:58,000 --> 00:25:58,966
>> CRAIG SPIEZLE: Okay.

628
00:25:59,033 --> 00:25:59,966
>> ANDY PURDY: I'm just
asking if five are true.

629
00:26:00,033 --> 00:26:00,966
I told it's maybe six.

630
00:26:01,033 --> 00:26:02,166
>> BRUCE SCHNEIER:
Which six?

631
00:26:02,233 --> 00:26:06,066
U.S., UK, Russia, China,
Iran, who do you got?

632
00:26:06,133 --> 00:26:06,900
>> KATIE ARRINGTON: The
bottom line is is

633
00:26:06,966 --> 00:26:09,233
we're a democracy.
We're different.

634
00:26:09,299 --> 00:26:10,166
>> CRAIG SPIEZLE: I'm
going to ask a little

635
00:26:10,233 --> 00:26:11,332
different question there.

636
00:26:11,400 --> 00:26:12,333
And we're not
picking on you.

637
00:26:12,400 --> 00:26:13,533
>> KATIE ARRINGTON:
It's all right.

638
00:26:13,599 --> 00:26:16,299
>> CRAIG SPIEZLE: But
France, the UK, the EU,

639
00:26:16,366 --> 00:26:17,799
and I'm not sure of the
status of Germany, they've

640
00:26:17,866 --> 00:26:20,233
said, hey, we're willing
to manage the risk and

641
00:26:20,299 --> 00:26:21,733
accept Huawei in
certain parts of our

642
00:26:21,799 --> 00:26:24,033
infrastructure, and
supposedly they even

643
00:26:24,099 --> 00:26:26,166
shared the same
intelligence data that

644
00:26:26,233 --> 00:26:27,000
you've been sharing.

645
00:26:27,066 --> 00:26:30,433
This gets into the
confusion headlines is how

646
00:26:30,500 --> 00:26:32,400
much of this is being
influenced by a trade

647
00:26:32,466 --> 00:26:34,933
issue or as a bona
fide security.

648
00:26:37,066 --> 00:26:38,333
>> KATIE ARRINGTON: Put
me in a - I work in the

649
00:26:38,400 --> 00:26:39,500
Department of Defense.

650
00:26:39,566 --> 00:26:41,500
There is only so much
I can - I am actually

651
00:26:41,566 --> 00:26:43,133
responsible to talk about.

652
00:26:43,200 --> 00:26:45,099
Katie Arrington, on the
other side, I could

653
00:26:45,166 --> 00:26:45,966
answer that question.

654
00:26:46,033 --> 00:26:49,466
But as a DoD employee, I
can give you this answer.

655
00:26:49,533 --> 00:26:52,000
Period, end of
story, it's law.

656
00:26:52,066 --> 00:26:54,766
We uphold the law in the
Department of Defense.

657
00:26:54,833 --> 00:26:56,599
Your Senator, your
Congressman, and your

658
00:26:56,666 --> 00:26:59,500
President all had a reason
to do what they did.

659
00:26:59,566 --> 00:27:01,000
>> BRUCE SCHNEIER:
Unfortunately, when you

660
00:27:01,066 --> 00:27:03,299
look at the news, the
Administration is trying

661
00:27:03,366 --> 00:27:05,166
to play it both ways.

662
00:27:05,233 --> 00:27:06,899
I think this hurts us.

663
00:27:06,966 --> 00:27:10,199
It's probably a national
security issue, but we

664
00:27:10,266 --> 00:27:12,900
talk about it like
it's a trade issue.

665
00:27:12,966 --> 00:27:17,466
I think we really hurt
ourselves with - ourselves

666
00:27:17,533 --> 00:27:20,765
with our allies when we
try to play both ends of that.

667
00:27:20,833 --> 00:27:23,966
Maybe on stage, for you
it's not a security issue.

668
00:27:24,033 --> 00:27:25,599
But coming out of the
White House, there's a lot

669
00:27:25,666 --> 00:27:27,099
of talk of it
being a trade issue.

670
00:27:27,166 --> 00:27:28,933
I wish they would
stop doing that.

671
00:27:29,000 --> 00:27:31,066
Or at least if they are
doing that, stop talking

672
00:27:31,133 --> 00:27:32,533
about it as a
national security issue.

673
00:27:32,599 --> 00:27:33,733
Pick one.

674
00:27:33,799 --> 00:27:34,833
>> KATIE ARRINGTON: But if
you want to go down that

675
00:27:34,900 --> 00:27:37,066
road, right, China has
been blatantly ignoring

676
00:27:37,133 --> 00:27:38,466
IP law.

677
00:27:38,533 --> 00:27:42,132
They have blatantly been
taking whatever they want.

678
00:27:42,200 --> 00:27:45,433
You can't have
it both ways. Right?

679
00:27:45,500 --> 00:27:49,233
You can't have your IP and
your data rights be yours

680
00:27:49,299 --> 00:27:51,566
and then give it to a
country who takes it,

681
00:27:51,633 --> 00:27:54,333
mimics, copies,
undersells, underbids

682
00:27:54,400 --> 00:27:55,633
you continually.

683
00:27:55,700 --> 00:27:57,733
Yeah, you can't
have it both ways. Right?

684
00:27:57,799 --> 00:27:59,233
>> BRUCE SCHNEIER: But now
you're talking trade war.

685
00:27:59,299 --> 00:28:00,299
>> KATIE ARRINGTON:
It's not a trade war.

686
00:28:00,366 --> 00:28:01,299
>> BRUCE SCHNEIER: You
just said that's -

687
00:28:01,366 --> 00:28:03,700
undersells, underbids,
that's all trade war talk.

688
00:28:03,766 --> 00:28:04,833
That's not national
security talk.

689
00:28:04,900 --> 00:28:05,633
>> KATIE ARRINGTON:
It's national security.

690
00:28:05,700 --> 00:28:08,299
So when I develop a
capability and the IP is

691
00:28:08,366 --> 00:28:12,000
in the Lockheed Martins
of the world, I disagree.

692
00:28:12,066 --> 00:28:13,266
>> CRAIG SPIEZLE: I'm glad
we are at least expressing

693
00:28:13,333 --> 00:28:15,000
how we feel here.

694
00:28:15,066 --> 00:28:18,333
Like I said, I can
guarantee this is going to

695
00:28:18,400 --> 00:28:19,466
be a call for discussion.

696
00:28:19,533 --> 00:28:21,599
But we're just
warming up, by the way.

697
00:28:23,766 --> 00:28:25,666
If you thought the
Presidential debate the

698
00:28:25,733 --> 00:28:27,399
other night was wild,
I think we're headed to

699
00:28:27,466 --> 00:28:28,866
that here.

700
00:28:28,933 --> 00:28:30,666
>> ANDY PURDY: I'm
going to make a point.

701
00:28:30,733 --> 00:28:32,700
If the point is true, and
maybe it's 30 countries,

702
00:28:32,766 --> 00:28:34,700
but if it's possible to
virtually implant hidden

703
00:28:34,766 --> 00:28:36,700
functionality, then the
issue of saying, okay,

704
00:28:36,766 --> 00:28:38,333
we're blocking Huawei
equipment, that solves

705
00:28:38,400 --> 00:28:39,099
the problem.

706
00:28:39,166 --> 00:28:40,366
That doesn't
solve the problem.

707
00:28:40,433 --> 00:28:42,400
We need to make sure we
can find the bad stuff in

708
00:28:42,466 --> 00:28:43,332
all the products.

709
00:28:43,400 --> 00:28:45,733
We hope this community can
help give us that level -

710
00:28:45,799 --> 00:28:47,599
>> KATIE ARRINGTON: We
are 100% supportive.

711
00:28:47,666 --> 00:28:51,765
I am all in on everything,
but there is just a major

712
00:28:51,833 --> 00:28:54,000
risk with this one
particular thing.

713
00:28:54,066 --> 00:28:56,333
We should have the
discussion about what does

714
00:28:56,400 --> 00:28:59,566
supply chain risk look
like, what does risk

715
00:28:59,633 --> 00:29:02,533
mitigation look like,
understanding that risk

716
00:29:02,599 --> 00:29:05,033
does not commute what my
risk is, it's not what

717
00:29:05,099 --> 00:29:05,866
your risk is.

718
00:29:05,933 --> 00:29:09,233
It's a different paradigm
that we can have - 100%

719
00:29:09,299 --> 00:29:12,533
have that 100%
conversation because it

720
00:29:12,599 --> 00:29:14,332
absolutely needs to be.

721
00:29:14,400 --> 00:29:17,466
But the Court of
Appeals held it up.

722
00:29:17,533 --> 00:29:18,666
They didn't change it.

723
00:29:18,733 --> 00:29:20,133
So let's move forward.

724
00:29:20,200 --> 00:29:21,466
>> BRUCE SCHNEIER: My
point is basically, yes,

725
00:29:21,533 --> 00:29:24,466
this won't solve the
problem, but it solves

726
00:29:24,533 --> 00:29:25,799
like an easy piece
of the problem.

727
00:29:25,866 --> 00:29:29,299
I mean, that's a
plausible argument.

728
00:29:29,366 --> 00:29:30,133
I would buy that.

729
00:29:30,200 --> 00:29:31,299
I think we have a bigger
problem in that we don't

730
00:29:31,366 --> 00:29:32,299
have many alternatives.

731
00:29:32,366 --> 00:29:36,666
I'm actually curious what
you're buying instead

732
00:29:36,733 --> 00:29:39,666
because, I mean, I kind of
think 5G is lost and our

733
00:29:39,733 --> 00:29:42,599
only hope now is to
try to secure 6G.

734
00:29:42,666 --> 00:29:45,033
Good luck and I
hope you can do it.

735
00:29:45,099 --> 00:29:47,733
I'm rooting for you,
but I'm not optimistic.

736
00:29:47,799 --> 00:29:48,933
>> CRAIG SPIEZLE: I think
this kind of gets -

737
00:29:49,000 --> 00:29:50,433
>> KATIE ARRINGTON: Who
are you rooting for?

738
00:29:50,500 --> 00:29:52,233
>> BRUCE SCHNEIER: I'm
rooting for that you can

739
00:29:52,299 --> 00:29:54,500
build a Huawei-free
5G network.

740
00:29:54,566 --> 00:29:55,333
>> KATIE ARRINGTON: Why
would I have to build a

741
00:29:55,400 --> 00:29:56,366
5G network?

742
00:29:56,433 --> 00:29:58,466
When did the Department
of Defense ever build

743
00:29:58,533 --> 00:29:59,866
a network?

744
00:29:59,933 --> 00:30:02,066
>> BRUCE SCHNEIER:
All right. Then I'll -

745
00:30:02,133 --> 00:30:03,166
>> KATIE ARRINGTON:
So when you say -

746
00:30:03,233 --> 00:30:04,466
>> BRUCE SCHNEIER: I'll
root for some magical

747
00:30:04,533 --> 00:30:06,199
thing happening
where you get it.

748
00:30:06,266 --> 00:30:07,000
>> KATIE ARRINGTON:
How did you get it to

749
00:30:07,066 --> 00:30:07,766
begin with?

750
00:30:07,833 --> 00:30:09,033
>> BRUCE SCHNEIER:
I don't know.

751
00:30:09,099 --> 00:30:13,765
Look, I'm not DoD saying
no Huawei in my network.

752
00:30:13,833 --> 00:30:16,299
But it's not going to be
there will be an empty box

753
00:30:16,366 --> 00:30:18,066
where the equipment used
to be and there will be

754
00:30:18,133 --> 00:30:19,133
air there.

755
00:30:19,200 --> 00:30:20,733
There will be something
there to accept it.

756
00:30:20,799 --> 00:30:21,866
>> KATIE ARRINGTON: We can
have routers and switches

757
00:30:21,933 --> 00:30:23,400
for other countries
- other - yeah?

758
00:30:23,466 --> 00:30:24,500
>> BRUCE SCHNEIER: Yes.

759
00:30:24,566 --> 00:30:26,133
>> KATIE ARRINGTON: Okay. And
how much dependency do you think

760
00:30:26,200 --> 00:30:30,099
I really have on the 4G
networks or the 5G networks?

761
00:30:30,166 --> 00:30:31,233
>> BRUCE SCHNEIER:
Are you on 3G still?

762
00:30:31,299 --> 00:30:33,266
I don't want to know.
Don't tell me.

763
00:30:33,333 --> 00:30:34,599
>> KATIE ARRINGTON:
Don't assume. Don't assume.

764
00:30:34,666 --> 00:30:35,866
>> BRUCE SCHNEIER: Don't
tell me how bad it is.

765
00:30:35,933 --> 00:30:37,099
>> CRAIG SPIEZLE: We're
going to go to the next

766
00:30:37,166 --> 00:30:38,200
topic here.

767
00:30:38,266 --> 00:30:39,900
I think this is one of
the things that I think

768
00:30:39,966 --> 00:30:41,799
everyone can maybe take
something away with them,

769
00:30:41,866 --> 00:30:43,066
I'm hopeful.

770
00:30:43,133 --> 00:30:45,200
It's about objective
criteria and testing.

771
00:30:45,266 --> 00:30:46,266
>> KATIE ARRINGTON: Yes.

772
00:30:46,333 --> 00:30:47,333
>> CRAIG SPIEZLE: So
whether it's Huawei,

773
00:30:47,400 --> 00:30:48,933
whether it's Eriksson,
whether it's Cisco,

774
00:30:49,000 --> 00:30:50,500
whether it's Nokia.

775
00:30:50,566 --> 00:30:54,166
I believe that we should
have objective criteria.

776
00:30:54,233 --> 00:30:56,633
If you look at the NIST
framework and different

777
00:30:56,700 --> 00:30:59,033
things, what can companies
measure against, what

778
00:30:59,099 --> 00:31:00,332
could countries
measure against?

779
00:31:00,400 --> 00:31:05,666
I think one of the
challenges that at least I

780
00:31:05,733 --> 00:31:08,166
see is we don't know what
those criteria that people

781
00:31:08,233 --> 00:31:09,566
are making conclusions on.

782
00:31:09,633 --> 00:31:12,166
This is one of the areas
that we need some help on,

783
00:31:12,233 --> 00:31:13,832
I think, and how do we
make this judgment and

784
00:31:13,900 --> 00:31:15,566
based on your
risk appetite for

785
00:31:15,633 --> 00:31:16,633
your application.

786
00:31:16,700 --> 00:31:18,200
Obviously, for critical
infrastructure, it might

787
00:31:18,266 --> 00:31:19,166
be one thing.

788
00:31:21,966 --> 00:31:24,099
In a home Io2 device, it
may be a different set

789
00:31:24,166 --> 00:31:25,566
of criteria.

790
00:31:25,633 --> 00:31:28,233
I know, Katie,
you've been working on a

791
00:31:28,299 --> 00:31:29,633
certification framework.

792
00:31:29,700 --> 00:31:34,333
We're running on time, but
a short update on that.

793
00:31:34,400 --> 00:31:36,866
What can we leverage and
what can we share that

794
00:31:36,933 --> 00:31:38,000
other - we can test?

795
00:31:38,066 --> 00:31:40,333
We're going to plug in -
like Bruce said, if we're

796
00:31:40,400 --> 00:31:43,500
going to plug in Cisco or
Eriksson, how do we make

797
00:31:43,566 --> 00:31:45,466
sure they're at least
based on the level playing

798
00:31:45,533 --> 00:31:47,065
field the same criteria?

799
00:31:47,133 --> 00:31:50,666
>> KATIE ARRINGTON: First,
CMMC is rolling out as

800
00:31:50,733 --> 00:31:53,066
scheduled, meeting
all the marks.

801
00:31:53,133 --> 00:31:54,566
It'll start showing
up in the Depart -

802
00:31:54,633 --> 00:31:55,533
>> CRAIG SPIEZLE:
And CMMC is?

803
00:31:55,599 --> 00:31:56,966
>> KATIE ARRINGTON:
Oh, sorry. I forgot.

804
00:31:57,033 --> 00:32:00,599
CMMC is the
Cybersecurity Maturity Model

805
00:32:00,666 --> 00:32:03,633
Certification that the
DoD is running out.

806
00:32:03,700 --> 00:32:05,299
It is based on the NIST.

807
00:32:05,366 --> 00:32:08,200
It is based on ISO 27001.

808
00:32:08,266 --> 00:32:13,166
It is based on controls
the UK has implemented,

809
00:32:13,233 --> 00:32:15,233
that the
Australians have had.

810
00:32:15,299 --> 00:32:16,433
We took the best of breed.

811
00:32:16,500 --> 00:32:17,900
We brought them together.

812
00:32:17,966 --> 00:32:21,065
We worked with industry,
academia to figure out

813
00:32:21,133 --> 00:32:24,299
what were the controls,
the capabilities that we

814
00:32:24,366 --> 00:32:27,433
wanted, the outcome, and
put it in a model that we

815
00:32:27,500 --> 00:32:29,166
could trust but
then verify. Right?

816
00:32:29,233 --> 00:32:33,099
We have created the model
to go out and then have

817
00:32:33,166 --> 00:32:36,033
the companies that work
within the DoD supply

818
00:32:36,099 --> 00:32:39,866
chain get certified that
they are ready, willing,

819
00:32:39,933 --> 00:32:43,366
and capable to do the work
that we need them to do.

820
00:32:43,433 --> 00:32:44,833
>> CRAIG SPIEZLE: Would
you let Huawei apply their

821
00:32:44,900 --> 00:32:47,400
products to that,
the same criteria?

822
00:32:47,466 --> 00:32:48,733
>> KATIE ARRINGTON:
It's against the law.

823
00:32:48,799 --> 00:32:52,766
Why are you asking
a silly question?

824
00:32:52,833 --> 00:32:53,866
I don't understand.

825
00:32:53,933 --> 00:32:56,633
Listen, if we really want
to have a discussion about

826
00:32:56,700 --> 00:32:59,599
how we can get good on our
supply chain because it

827
00:32:59,666 --> 00:33:01,466
matters, let's have
a conversation.

828
00:33:01,533 --> 00:33:03,533
But this is a moot point.

829
00:33:03,599 --> 00:33:04,599
The law is done.

830
00:33:04,666 --> 00:33:05,700
>> BRUCE SCHNEIER:
But it's interesting

831
00:33:05,766 --> 00:33:07,000
hypothetically saying.

832
00:33:07,066 --> 00:33:08,466
That's the point.

833
00:33:08,533 --> 00:33:09,866
It's an interesting
hypothetical -

834
00:33:09,933 --> 00:33:10,933
I understand it's illegal.

835
00:33:11,000 --> 00:33:14,799
If it were legal, would
that be something reasonable?

836
00:33:14,866 --> 00:33:17,033
It's a good question.

837
00:33:17,099 --> 00:33:19,666
>> KATIE ARRINGTON: The
testing and the standards,

838
00:33:19,733 --> 00:33:23,599
A - so even Huawei can
admit their programmers

839
00:33:23,666 --> 00:33:26,299
are where Microsoft
was 25 years ago. Right?

840
00:33:26,366 --> 00:33:27,966
They're not incredibly robust.

841
00:33:28,033 --> 00:33:30,899
They don't have -
you can - listen.

842
00:33:30,966 --> 00:33:32,500
>> BRUCE SCHNEIER: There's
a robust programmer?

843
00:33:32,566 --> 00:33:33,833
>> KATIE ARRINGTON:
Andy, it's your job.

844
00:33:33,900 --> 00:33:34,866
I get it. Right?

845
00:33:34,933 --> 00:33:36,066
That's what you work for?

846
00:33:36,133 --> 00:33:37,233
And I get that.

847
00:33:37,299 --> 00:33:40,433
But I go by what - I read
the analysis and I can

848
00:33:40,500 --> 00:33:42,433
say peace.

849
00:33:42,500 --> 00:33:45,966
What we have to do is
determine what are the

850
00:33:46,033 --> 00:33:49,065
standards and requirements
for testing a product. Right?

851
00:33:49,133 --> 00:33:51,433
This is where we really
have to get good.

852
00:33:51,500 --> 00:33:54,566
We don't have a really -
we're doing the CMMC for

853
00:33:54,633 --> 00:33:58,700
companies to get critical
thinking around security

854
00:33:58,766 --> 00:34:00,933
and why they're doing it.

855
00:34:01,000 --> 00:34:03,033
We don't ask ourselves
often why do we do

856
00:34:03,099 --> 00:34:04,399
two-factor authentication.

857
00:34:04,466 --> 00:34:06,332
Why is that needed?

858
00:34:06,400 --> 00:34:07,666
>> BRUCE SCHNEIER: We ask
ourselves that all the time.

859
00:34:07,733 --> 00:34:08,666
>> KATIE ARRINGTON: Why?

860
00:34:08,733 --> 00:34:10,833
Because my husband knows
my first password.

861
00:34:10,900 --> 00:34:12,699
Two-factor means that
there is a redundancy

862
00:34:12,766 --> 00:34:15,199
built into the system,
that you have to have more.

863
00:34:15,266 --> 00:34:17,433
But think why we do it.

864
00:34:17,500 --> 00:34:20,565
When it comes to product,
we have to get to that

865
00:34:20,632 --> 00:34:23,232
same level of
rigor of testing.

866
00:34:23,300 --> 00:34:26,133
The second part to this is
once you go through the

867
00:34:26,199 --> 00:34:29,199
rigors of testing, you
continually have to do

868
00:34:29,266 --> 00:34:32,166
penetration testing, you
continually have to test

869
00:34:32,233 --> 00:34:35,333
that product, because no
enterprise network, no

870
00:34:35,400 --> 00:34:37,033
environment stays static.

871
00:34:37,099 --> 00:34:38,466
It will
continually change.

872
00:34:38,533 --> 00:34:40,232
It's changing at
the speed of light.

873
00:34:40,300 --> 00:34:43,633
We have to have the tools
in place to buy down the

874
00:34:43,699 --> 00:34:46,800
risk as it continues to
evolve but understand

875
00:34:46,866 --> 00:34:49,000
baseline,
underwriter laboratories.

876
00:34:49,065 --> 00:34:51,598
We need standards, we need
to impart them, we're

877
00:34:51,666 --> 00:34:54,500
using them, but we
need to do more beyond that

878
00:34:54,565 --> 00:34:55,565
static test.

879
00:34:55,632 --> 00:34:56,899
>> CRAIG SPIEZLE: So,
Bruce, we've spent a lot

880
00:34:56,966 --> 00:34:58,533
of time in the past
talking about IoT devices

881
00:34:58,599 --> 00:34:59,533
and how do you test them.

882
00:34:59,599 --> 00:35:02,199
And so if you think about
today, it's like, okay, so

883
00:35:02,266 --> 00:35:03,833
Department of Defense
have their criteria, but

884
00:35:03,900 --> 00:35:05,599
there's, to the best of
my knowledge, there's no

885
00:35:05,666 --> 00:35:07,833
single federal agency.

886
00:35:07,900 --> 00:35:10,333
Should there be an
independent testing of

887
00:35:10,400 --> 00:35:11,533
some of this?

888
00:35:11,599 --> 00:35:12,533
>> BRUCE SCHNEIER:
I think in the past, testing

889
00:35:12,599 --> 00:35:13,766
makes sense.

890
00:35:13,833 --> 00:35:17,633
There is testing against
errors and bad practices

891
00:35:17,699 --> 00:35:20,500
and there's testing
against malicious activity.

892
00:35:20,566 --> 00:35:23,566
If your testing is robust,
then the devil himself

893
00:35:23,633 --> 00:35:25,966
should be able to send
a product through it.

894
00:35:26,033 --> 00:35:28,500
If your testing is not
robust, then it's not

895
00:35:28,566 --> 00:35:32,533
robust and we
have a problem.

896
00:35:32,599 --> 00:35:35,233
I worry a lot about -
we're talking about

897
00:35:35,300 --> 00:35:38,033
intelligent, adaptive,
malicious adversaries.

898
00:35:38,099 --> 00:35:41,900
We're not testing whether
this table can take so

899
00:35:41,966 --> 00:35:43,165
much weight.

900
00:35:43,233 --> 00:35:48,166
We know about how hard it
is to find a backdoor, how

901
00:35:48,233 --> 00:35:49,500
they can be put in.

902
00:35:49,566 --> 00:35:51,598
This is not something
we're going to find

903
00:35:51,666 --> 00:35:54,033
through underwriter's
lab-like testing.

904
00:35:54,099 --> 00:35:56,733
We'll find things like
default passwords and bad

905
00:35:56,800 --> 00:36:00,366
coding practices and
stuff that will imply

906
00:36:00,433 --> 00:36:03,066
vulnerabilities that can
be found and exploited,

907
00:36:03,133 --> 00:36:05,000
but not deliberately
inserted things.

908
00:36:05,066 --> 00:36:08,133
Those are very
different kinds of animals.

909
00:36:08,199 --> 00:36:12,066
I really worry about being
able to test for the former.

910
00:36:12,133 --> 00:36:13,866
I don't think you
can do it well.

911
00:36:13,933 --> 00:36:16,233
>> CRAIG SPIEZLE: I think
coming back to the RSA

912
00:36:16,300 --> 00:36:19,199
theme of human technology,
is the risk going to be

913
00:36:19,266 --> 00:36:21,433
more of the human factor
or is it going to be more

914
00:36:21,500 --> 00:36:24,033
of the rogue employee,
the operator that has

915
00:36:24,099 --> 00:36:24,699
the ability?

916
00:36:24,766 --> 00:36:26,366
It's not the
device itself.

917
00:36:26,433 --> 00:36:28,133
It's those who
are operating it.

918
00:36:28,199 --> 00:36:29,066
Andy, do you
have anything?

919
00:36:29,133 --> 00:36:30,332
>> ANDY PURDY: I think
Kathryn hasn't had a chance.

920
00:36:30,400 --> 00:36:31,300
Did you want to weigh in?

921
00:36:31,366 --> 00:36:34,599
>> KATHRYN WALDRON: Oh,
I'll weigh in on this. Sure.

922
00:36:34,666 --> 00:36:36,333
>> ANDY PURDY: Okay. I really
applaud after the work -

923
00:36:36,400 --> 00:36:37,733
>> KATHRYN WALDRON: I
won't weigh in on this.

924
00:36:37,800 --> 00:36:38,633
>> ANDY PURDY: I'm sorry.

925
00:36:38,699 --> 00:36:41,599
I misunderstood you.
I misunderstood you.

926
00:36:41,666 --> 00:36:44,500
>> KATHRYN WALDRON:
No worries. Absolutely.

927
00:36:44,566 --> 00:36:46,866
I think you're right, the
independent testing would

928
00:36:46,933 --> 00:36:48,000
certainly be beneficial.

929
00:36:48,066 --> 00:36:49,799
Obviously, in the case of
Huawei, the UK has tried

930
00:36:49,866 --> 00:36:53,066
to take test steps in this
direction, not always to

931
00:36:53,133 --> 00:36:54,866
the benefit of Huawei
since they've discovered

932
00:36:54,933 --> 00:36:58,566
plenty of bugs
in the system.

933
00:36:58,633 --> 00:37:01,066
But in regard to the
appetite for that in

934
00:37:01,133 --> 00:37:03,732
Washington, certainly for
Huawei, it's illegal in

935
00:37:03,800 --> 00:37:05,699
regard to like getting
that reversed or modified

936
00:37:05,766 --> 00:37:07,000
in any way.

937
00:37:07,066 --> 00:37:09,033
I don't think it matters,
because at this point,

938
00:37:09,099 --> 00:37:12,500
because of Huawei in the
minds of policymakers in

939
00:37:12,566 --> 00:37:16,698
Washington, China is now -
tech companies from China

940
00:37:16,766 --> 00:37:18,500
is now synonymous
with the name of the

941
00:37:18,566 --> 00:37:19,799
Chinese government.

942
00:37:19,866 --> 00:37:21,933
I don't think you're
going to find that able -

943
00:37:22,000 --> 00:37:24,900
they're not going to undo
that regardless of how

944
00:37:24,966 --> 00:37:28,000
much testing they go
through, I think.

945
00:37:28,066 --> 00:37:31,198
>> ANDY PURDY: Let me
suggest a statement in question.

946
00:37:31,266 --> 00:37:33,133
I asked from the floor
of a panel yesterday to

947
00:37:33,199 --> 00:37:35,566
Admiral Blair who was
talking about the Huawei

948
00:37:35,633 --> 00:37:39,133
CSEC UK program, and I
suggested that it's really

949
00:37:39,199 --> 00:37:41,533
got - that's really
focusing on just the

950
00:37:41,599 --> 00:37:43,566
product and that's important.

951
00:37:43,633 --> 00:37:46,633
But what our customers
have recommended to the

952
00:37:46,699 --> 00:37:48,733
FCC is a
three-legged stool.

953
00:37:48,800 --> 00:37:50,199
It's not just
the products.

954
00:37:50,266 --> 00:37:52,433
It's also got to be the
customers and carriers and

955
00:37:52,500 --> 00:37:54,633
what they do for
cybersecurity, how they

956
00:37:54,699 --> 00:37:57,233
promote resilience, how
they manage the networks

957
00:37:57,300 --> 00:37:59,300
in terms of anomalous
activity, hopefully as

958
00:37:59,366 --> 00:38:00,866
part of legal
information sharing with

959
00:38:00,933 --> 00:38:01,633
the government.

960
00:38:01,699 --> 00:38:03,466
The government can
then connect the dots.

961
00:38:03,533 --> 00:38:06,598
The third leg of the stool
is whether and how and

962
00:38:06,666 --> 00:38:09,733
when the equipment
supplier access the

963
00:38:09,800 --> 00:38:12,000
customer networks
and the customer data and

964
00:38:12,066 --> 00:38:12,933
does updates.

965
00:38:13,000 --> 00:38:16,800
There you need what we do,
and we're open to doing

966
00:38:16,866 --> 00:38:17,533
it better.

967
00:38:17,599 --> 00:38:19,966
We have specially
configured laptops, so

968
00:38:20,033 --> 00:38:23,299
every time we touch the
customer network or data,

969
00:38:23,366 --> 00:38:26,633
after written permission,
every keystroke is logged.

970
00:38:26,699 --> 00:38:28,266
It's fully reconstructed.

971
00:38:28,333 --> 00:38:30,133
The information is
owned by the customer.

972
00:38:30,199 --> 00:38:32,166
We're not saying it's
perfect, but hopefully all

973
00:38:32,233 --> 00:38:33,966
the telecom operators
are doing that.

974
00:38:34,033 --> 00:38:35,699
That's the kind of thing
in these three legs, if we

975
00:38:35,766 --> 00:38:37,800
can get input from the
community, and I've got

976
00:38:37,866 --> 00:38:39,800
some suggestions on
transparency, that can

977
00:38:39,866 --> 00:38:42,766
help us move forward
to address risk and

978
00:38:42,833 --> 00:38:44,599
promote resilience.

979
00:38:44,666 --> 00:38:45,966
>> CRAIG SPIEZLE: Kathryn,
do you want to follow up

980
00:38:46,033 --> 00:38:46,966
on that?

981
00:38:47,033 --> 00:38:48,232
>> KATHRYN WALDRON: I
mean, I think that's great.

982
00:38:48,300 --> 00:38:50,199
That's not going
to help you guys.

983
00:38:50,266 --> 00:38:51,199
>> CRAIG SPIEZLE:
Actually, I just got a

984
00:38:51,266 --> 00:38:53,433
reminder that we should
take a quick look and see

985
00:38:53,500 --> 00:38:55,633
how Summer is doing on
the side there, if she's

986
00:38:55,699 --> 00:38:56,800
capturing everything there.

987
00:38:56,866 --> 00:38:59,300
I think they want to
catch that on camera.

988
00:38:59,366 --> 00:39:01,133
Are they going to do that?

989
00:39:01,199 --> 00:39:03,666
>> BRUCE SCHNEIER: It's
less ranty than we were.

990
00:39:03,733 --> 00:39:05,000
>> CRAIG SPIEZLE: At
least she hasn't done the

991
00:39:05,066 --> 00:39:06,899
caricatures of you and
I this year, Bruce.

992
00:39:06,966 --> 00:39:09,866
We're in better
shape so far there.

993
00:39:09,933 --> 00:39:11,199
Summer is really
doing a good job.

994
00:39:11,266 --> 00:39:13,166
It's interesting to see
the key points here we're

995
00:39:13,233 --> 00:39:15,233
collecting here.

996
00:39:16,400 --> 00:39:18,699
I guess do we risk
this false narrative.

997
00:39:18,766 --> 00:39:22,466
Okay, we've blacklisted
Huawei, but are we going

998
00:39:22,533 --> 00:39:25,333
to do the same robust
through every other

999
00:39:25,400 --> 00:39:26,199
component in there?

1000
00:39:26,266 --> 00:39:28,599
And if not, are we just
going to have a false

1001
00:39:28,666 --> 00:39:31,866
sense of security?

1002
00:39:31,933 --> 00:39:33,266
Bruce, I'm going to
let you take that one.

1003
00:39:33,333 --> 00:39:34,333
>> BRUCE SCHNEIER:
Remember the first

1004
00:39:34,400 --> 00:39:36,400
sentence I said when I
got up here, supply chain

1005
00:39:36,466 --> 00:39:41,199
security is an
insurmountably hard problem.

1006
00:39:41,266 --> 00:39:42,900
It is not just the country.

1007
00:39:42,966 --> 00:39:43,766
It is the equipment.

1008
00:39:43,833 --> 00:39:44,566
It is the program.

1009
00:39:44,633 --> 00:39:46,033
It is the assembly.

1010
00:39:46,099 --> 00:39:46,866
It is the shipping.

1011
00:39:46,933 --> 00:39:48,966
It is the update.

1012
00:39:49,033 --> 00:39:51,066
We have fake apps in
the Google Play store.

1013
00:39:51,133 --> 00:39:54,198
We had a Cisco box sent
for the Syrian telephone

1014
00:39:54,266 --> 00:39:56,366
company the NSA intercepted.

1015
00:39:56,433 --> 00:39:58,633
NotPetya is being
distributed through a fake

1016
00:39:58,699 --> 00:40:01,000
update to a popular
Ukrainian accounting package.

1017
00:40:01,066 --> 00:40:04,000
We can go on and on and on.

1018
00:40:04,066 --> 00:40:05,899
You can build a
U.S.-only iPhone.

1019
00:40:05,966 --> 00:40:07,165
It'll cost ten times as much.

1020
00:40:07,233 --> 00:40:09,566
Nobody will buy it.

1021
00:40:09,633 --> 00:40:15,098
Our industry is deeply and
irrevocably international

1022
00:40:15,166 --> 00:40:20,066
in ways that make this
impossible to solve.

1023
00:40:20,133 --> 00:40:24,066
Now that's not fun, but
that's the world we're in.

1024
00:40:24,133 --> 00:40:27,933
Now what can we
do given that?

1025
00:40:28,000 --> 00:40:31,199
I said that my only hope
of hostilities erupting in

1026
00:40:31,266 --> 00:40:34,533
the U.S. and China is that
our stuff will fail, obviously,

1027
00:40:34,599 --> 00:40:36,500
but hopefully
their stuff will fail, too.

1028
00:40:36,566 --> 00:40:39,366
Nobody goes anywhere
and everything is down.

1029
00:40:39,433 --> 00:40:42,933
That's my best scenario
because offense is so much

1030
00:40:43,000 --> 00:40:44,433
easier than defense here.

1031
00:40:44,500 --> 00:40:45,366
I worry about it.

1032
00:40:45,433 --> 00:40:47,266
>> KATIE ARRINGTON: In the
supply - so the supply

1033
00:40:47,333 --> 00:40:48,400
chain, you're
absolutely right.

1034
00:40:48,466 --> 00:40:50,266
And it's like security.

1035
00:40:50,333 --> 00:40:52,666
You'll never be 100%
secure because it will

1036
00:40:52,733 --> 00:40:54,466
constantly change. Right?

1037
00:40:54,533 --> 00:40:56,033
But you buy down your risk.

1038
00:40:56,099 --> 00:40:58,300
One of the things within
our supply chain, I think,

1039
00:40:58,366 --> 00:41:01,699
that we, and the
coronavirus has, I think,

1040
00:41:01,766 --> 00:41:04,533
illuminated it, and we
talked briefly about this

1041
00:41:04,599 --> 00:41:07,599
beforehand, is how
impactful it is to our -

1042
00:41:07,666 --> 00:41:11,866
how impactful that is to
everyone's supply chain,

1043
00:41:11,933 --> 00:41:15,599
not just DoD, like
everybody's supply chain.

1044
00:41:15,666 --> 00:41:19,599
We have to start thinking
about what the risk is in

1045
00:41:19,666 --> 00:41:20,866
the supply chain.

1046
00:41:20,933 --> 00:41:22,400
You can't solve
all the problems.

1047
00:41:22,466 --> 00:41:25,266
You have to prioritize
what are your biggest

1048
00:41:25,333 --> 00:41:28,199
challenge points in it to
work to secure those as

1049
00:41:28,266 --> 00:41:31,766
best as you can and move
down that path because

1050
00:41:31,833 --> 00:41:33,333
you're never going to be
able to boil the ocean.

1051
00:41:33,400 --> 00:41:34,866
I mean, you're 100% right.

1052
00:41:34,933 --> 00:41:36,199
It's a big challenge.

1053
00:41:36,266 --> 00:41:37,566
>> CRAIG SPIEZLE:
Let's pivot.

1054
00:41:37,633 --> 00:41:40,399
The U.S. government has
multiple efforts on supply

1055
00:41:40,466 --> 00:41:42,566
chain security.

1056
00:41:42,633 --> 00:41:45,799
One, Department of
Commerce on our NTIAs, the

1057
00:41:45,866 --> 00:41:48,366
SBOM, software
bill of materials.

1058
00:41:48,433 --> 00:41:50,500
And the whole concept
between that is think of

1059
00:41:50,566 --> 00:41:51,466
the nutrition label.

1060
00:41:51,533 --> 00:41:53,900
What goes into all
of your products?

1061
00:41:53,966 --> 00:41:57,400
The idea is it may not -
it may be safe today, but

1062
00:41:57,466 --> 00:42:00,333
as you're building that
into your infrastructure,

1063
00:42:00,400 --> 00:42:01,766
does that
software library have a

1064
00:42:01,833 --> 00:42:03,433
vulnerability tomorrow.

1065
00:42:03,500 --> 00:42:04,933
I think that's
a good concept.

1066
00:42:05,000 --> 00:42:08,333
I know NIST is also doing
a supply chain initiative.

1067
00:42:08,400 --> 00:42:11,099
There are multiple
government agencies.

1068
00:42:11,166 --> 00:42:13,166
But I think
that's another thing.

1069
00:42:13,233 --> 00:42:14,099
It's not just the device.

1070
00:42:14,166 --> 00:42:17,000
We need to look at there's
multiple layers of that.

1071
00:42:17,066 --> 00:42:19,165
Does anyone have any response?

1072
00:42:19,233 --> 00:42:21,199
>> ANDY PURDY: Let
me make a suggestion.

1073
00:42:21,266 --> 00:42:22,566
First of all, I didn't
finish making the point

1074
00:42:22,633 --> 00:42:23,500
commending DoD.

1075
00:42:23,566 --> 00:42:26,332
We were working hard
when I was at DHS.

1076
00:42:26,400 --> 00:42:28,333
We launched the Software
and Supply Chain Assurance

1077
00:42:28,400 --> 00:42:32,000
Forum years ago, co-led by
DoD, DHS, NIST, and GSA,

1078
00:42:32,066 --> 00:42:34,566
the effort to try to
create risk-informed

1079
00:42:34,633 --> 00:42:37,033
procurement requirements
that would stick.

1080
00:42:37,099 --> 00:42:40,900
Now in the recent time, it
looks like DoD has a new

1081
00:42:40,966 --> 00:42:44,033
effort to really not just
make sure this stuff gets

1082
00:42:44,099 --> 00:42:45,966
funded, but to
actually make sure it's

1083
00:42:46,033 --> 00:42:46,633
with assurance.

1084
00:42:46,699 --> 00:42:48,000
I think that's a
very good effort.

1085
00:42:48,066 --> 00:42:49,598
When we look at the
three-legged stool, the

1086
00:42:49,666 --> 00:42:51,333
community
hopefully can help.

1087
00:42:51,400 --> 00:42:53,500
DHS and NIST were
trying to get greater

1088
00:42:53,566 --> 00:42:57,133
capabilities to monitor
the carrier networks to

1089
00:42:57,199 --> 00:42:58,833
find anomalous conduct.

1090
00:42:58,900 --> 00:43:00,866
We need to do a
transparency initiative.

1091
00:43:00,933 --> 00:43:05,566
Have DoD, have the NSA
Cyber-collaboration Group,

1092
00:43:05,633 --> 00:43:07,533
have some of the
telecoms and U.S.

1093
00:43:07,599 --> 00:43:10,966
telecom call on the
equipment suppliers to

1094
00:43:11,033 --> 00:43:14,699
come in, open up the
kimono, and tell you what

1095
00:43:14,766 --> 00:43:18,500
is it we're doing on
software, supply chain, on

1096
00:43:18,566 --> 00:43:21,433
internal testing, on
product development, and

1097
00:43:21,500 --> 00:43:22,400
on software engineering.

1098
00:43:22,466 --> 00:43:24,098
Have those discussions.

1099
00:43:24,166 --> 00:43:25,266
Nothing asked in return.

1100
00:43:25,333 --> 00:43:26,133
That's one.

1101
00:43:26,199 --> 00:43:30,733
Two, have - call on the
equipment suppliers to

1102
00:43:30,800 --> 00:43:32,699
create industry minimum
best practices for

1103
00:43:32,766 --> 00:43:33,833
assurance and
transparency.

1104
00:43:33,900 --> 00:43:36,400
And third, and I know we
might not agree to this,

1105
00:43:36,466 --> 00:43:39,133
but why don't folks say
what is necessary for the

1106
00:43:39,199 --> 00:43:41,300
community to go into
the factories and the

1107
00:43:41,366 --> 00:43:43,433
development of companies
like Huawei and Nokia

1108
00:43:43,500 --> 00:43:44,233
and Eriksson?

1109
00:43:44,300 --> 00:43:46,199
What's necessary to
go in so you can build

1110
00:43:46,266 --> 00:43:47,766
transparency, you
can build assurance?

1111
00:43:47,833 --> 00:43:50,233
We can have measures
that you have a basis

1112
00:43:50,300 --> 00:43:52,366
for confidence.

1113
00:43:52,433 --> 00:43:53,699
>> CRAIG SPIEZLE: I think
one of the things as a

1114
00:43:53,766 --> 00:43:55,566
takeaway as we think about
this, it's not just for

1115
00:43:55,633 --> 00:43:58,698
5G, but what tools are
there to help companies,

1116
00:43:58,766 --> 00:44:00,533
enterprises, better
understand their risk?

1117
00:44:00,599 --> 00:44:02,699
And so the SBOM was
one of those areas.

1118
00:44:02,766 --> 00:44:04,933
I think as we look at
that, there are multiple

1119
00:44:05,000 --> 00:44:05,800
government efforts.

1120
00:44:05,866 --> 00:44:09,400
And so I think as a
takeaway, I think industry

1121
00:44:09,466 --> 00:44:12,000
is looking for those best
practices so they can do

1122
00:44:12,066 --> 00:44:13,832
their own risk assessment,
not that they may not

1123
00:44:13,900 --> 00:44:15,733
agree with yours, but I
want to make it based on

1124
00:44:15,800 --> 00:44:17,199
my risk appetite.

1125
00:44:17,266 --> 00:44:19,800
I think that's one of the
challenges is carriers -

1126
00:44:19,866 --> 00:44:22,466
and I've spoken to them
- they don't have that criteria.

1127
00:44:22,533 --> 00:44:25,433
They want to be able to do
it as robust or even more

1128
00:44:25,500 --> 00:44:28,166
robust than the Department
of Defense, but they don't

1129
00:44:28,233 --> 00:44:30,433
have a playbook to work from.

1130
00:44:32,800 --> 00:44:34,433
On that comment?

1131
00:44:34,500 --> 00:44:36,333
>> KATHRYN WALDRON: I'm
nodding because I was in a

1132
00:44:36,400 --> 00:44:38,733
meeting once with people
from a major telecom

1133
00:44:38,800 --> 00:44:41,900
carrier and I asked them
about supply chain.

1134
00:44:41,966 --> 00:44:45,400
The answer I got back was,
yeah, we just kind of like

1135
00:44:45,466 --> 00:44:46,566
feel it out.

1136
00:44:46,633 --> 00:44:49,698
I found that answer to
be incredibly alarming,

1137
00:44:49,766 --> 00:44:52,466
obviously, from a national
security perspective.

1138
00:44:52,533 --> 00:44:55,133
But there are
resources out there.

1139
00:44:55,199 --> 00:44:57,666
Certainly, there are things
like the NIST framework.

1140
00:44:57,733 --> 00:45:01,866
NIST is working on
frameworks specifically in

1141
00:45:01,933 --> 00:45:02,833
regard to supply chain.

1142
00:45:02,900 --> 00:45:06,033
But I do think we need to
be concerned about how

1143
00:45:06,099 --> 00:45:09,599
often carriers and other
companies are auditing

1144
00:45:09,666 --> 00:45:11,133
themselves or
being audited.

1145
00:45:11,199 --> 00:45:14,166
The issue that we see
with frameworks and other

1146
00:45:14,233 --> 00:45:19,500
checklist type programs is
suppose you check all your

1147
00:45:19,566 --> 00:45:21,899
boxes, you think you're
secure, well, great, like

1148
00:45:21,966 --> 00:45:25,966
you said, we're secure for
today, but it can lull you

1149
00:45:26,033 --> 00:45:27,665
into a false
sense of compliance.

1150
00:45:27,733 --> 00:45:29,566
That is something that we
need to essentially watch

1151
00:45:29,633 --> 00:45:32,732
out for when encouraging
the adoption of other

1152
00:45:32,800 --> 00:45:38,466
frameworks and checklists
and supply chain auditing.

1153
00:45:38,533 --> 00:45:39,799
>> KATIE ARRINGTON: I
agree with Andy on the

1154
00:45:39,866 --> 00:45:43,866
point, though, that we
need to do more about

1155
00:45:43,933 --> 00:45:46,433
baking security protocols
and being very clear and

1156
00:45:46,500 --> 00:45:50,466
concise on the very get-go
what it is we think is

1157
00:45:50,533 --> 00:45:51,400
a requirement.

1158
00:45:51,466 --> 00:45:53,199
You cannot deny that.

1159
00:45:53,266 --> 00:45:55,566
I think the DevSecops, I
mean, as we move through

1160
00:45:55,633 --> 00:45:59,133
that, I think it's hugely
beneficial to see that,

1161
00:45:59,199 --> 00:46:00,099
where we're going.

1162
00:46:00,166 --> 00:46:01,366
We're baking security in.

1163
00:46:01,433 --> 00:46:06,333
The control of the
repositories is - I think

1164
00:46:06,400 --> 00:46:08,533
we have to also look at as
well as we develop in this

1165
00:46:08,599 --> 00:46:11,666
environment that whatever
we house them in the cloud

1166
00:46:11,733 --> 00:46:14,133
instantiations, who is
watching over that, who is

1167
00:46:14,199 --> 00:46:16,733
protecting those crown
jewels for companies, for

1168
00:46:16,800 --> 00:46:18,099
NSA, for anybody.

1169
00:46:18,166 --> 00:46:19,500
I mean, that's part of it.

1170
00:46:19,566 --> 00:46:22,899
But rolling security in
at the base level at the

1171
00:46:22,966 --> 00:46:26,232
start, building on top
of it, being clear and

1172
00:46:26,300 --> 00:46:29,233
concise about the
requirement that you need

1173
00:46:29,300 --> 00:46:30,633
for your particular company.

1174
00:46:30,699 --> 00:46:33,800
I mean, the DoD is just
one sliver of this.

1175
00:46:33,866 --> 00:46:36,300
Granted, we're the largest
buyer in the country.

1176
00:46:36,366 --> 00:46:41,033
But on the other side,
we do need better requirements.

1177
00:46:41,099 --> 00:46:42,533
We are working on that.

1178
00:46:42,599 --> 00:46:45,766
That's part of, you know,
20 years ago when a lot of

1179
00:46:45,833 --> 00:46:48,066
weapons systems were
developed, things - I

1180
00:46:48,133 --> 00:46:48,732
mean, you were there.

1181
00:46:48,800 --> 00:46:53,099
Was the world
the same at all?

1182
00:46:53,166 --> 00:46:54,599
>> CRAIG SPIEZLE: We were
joking before about this void.

1183
00:46:54,666 --> 00:46:56,533
If you rip out Huawei,
what do you have?

1184
00:46:56,599 --> 00:46:58,933
One of the challenges has
been China has been out

1185
00:46:59,000 --> 00:47:01,233
innovating the
U.S. in many areas.

1186
00:47:01,300 --> 00:47:02,099
This is not new.

1187
00:47:02,166 --> 00:47:02,933
It's happened.

1188
00:47:03,000 --> 00:47:05,333
Huawei, right, wrong, or
indifferent, they're able

1189
00:47:05,400 --> 00:47:07,566
to come to the market 6 to
12 months before anyone

1190
00:47:07,633 --> 00:47:09,598
else with more
innovative products.

1191
00:47:09,666 --> 00:47:13,400
The question, and maybe,
Bruce, because you had

1192
00:47:13,466 --> 00:47:14,732
actually posed the
question to Katie, so I'm

1193
00:47:14,800 --> 00:47:16,133
going to point this to you.

1194
00:47:16,199 --> 00:47:20,933
Should the U.S. be investing
more in to spur innovation

1195
00:47:21,000 --> 00:47:22,099
and incentives?

1196
00:47:22,166 --> 00:47:25,800
That said in context,
according to Wall Street

1197
00:47:25,866 --> 00:47:28,633
Journal, they already
invested $50 billion in

1198
00:47:28,699 --> 00:47:30,800
tax incentives to Cisco.

1199
00:47:30,866 --> 00:47:32,866
>> BRUCE SCHNEIER: I don't
think it's possible.

1200
00:47:32,933 --> 00:47:35,833
I think we are too
international for that.

1201
00:47:35,900 --> 00:47:38,166
And that even if the tax
incentives go to Cisco,

1202
00:47:38,233 --> 00:47:40,933
the boxes are not built in the
U.S. and they're not going

1203
00:47:41,000 --> 00:47:41,800
to be.

1204
00:47:41,866 --> 00:47:43,000
That's going to cost too much.

1205
00:47:43,066 --> 00:47:45,299
We are not willing - we as
a country are not willing

1206
00:47:45,366 --> 00:47:49,900
to pay the price for the
security that we think we need.

1207
00:47:49,966 --> 00:47:53,500
You talked about software
bill of materials.

1208
00:47:53,566 --> 00:47:55,533
I think those are great,
but those are not going to

1209
00:47:55,599 --> 00:47:57,033
solve these threats.

1210
00:47:57,099 --> 00:48:00,000
Software bill of materials
solves Heartbleed shows up

1211
00:48:00,066 --> 00:48:03,433
and, uh-oh, what of my
equipment is dependent on

1212
00:48:03,500 --> 00:48:04,400
that protocol.

1213
00:48:04,466 --> 00:48:05,533
Am I vulnerable?

1214
00:48:05,599 --> 00:48:06,400
That's what that solves.

1215
00:48:06,466 --> 00:48:08,833
That's really needed,
especially with IoT

1216
00:48:08,900 --> 00:48:10,599
because stuff is coming
from libraries into

1217
00:48:10,666 --> 00:48:12,599
libraries into libraries
and you don't know what

1218
00:48:12,666 --> 00:48:14,599
you have in
your equipment.

1219
00:48:14,666 --> 00:48:17,800
But that's not against
the nation state threat.

1220
00:48:17,866 --> 00:48:20,000
Nation state threats are
largely above the pay

1221
00:48:20,066 --> 00:48:21,500
grade of companies.

1222
00:48:21,566 --> 00:48:24,765
Just ask Sony, ask Maersk,
ask the Brazilian oil

1223
00:48:24,833 --> 00:48:27,166
company NSA hacked.

1224
00:48:27,233 --> 00:48:30,199
These are not going to
be the threats these

1225
00:48:30,266 --> 00:48:31,733
companies can deal with.

1226
00:48:31,800 --> 00:48:34,866
As a nation, as DoD, we
kind of expect you to deal

1227
00:48:34,933 --> 00:48:36,266
with those threats. All right?

1228
00:48:36,333 --> 00:48:39,366
We kind of outsource that
to you and that's probably

1229
00:48:39,433 --> 00:48:41,133
the right way to do it.

1230
00:48:41,199 --> 00:48:43,766
But we are not going
to build a U.S.-only

1231
00:48:43,833 --> 00:48:44,599
capability here.

1232
00:48:44,666 --> 00:48:46,766
The only country that
might be able to go it

1233
00:48:46,833 --> 00:48:49,566
alone is China because
they're big enough.

1234
00:48:49,633 --> 00:48:51,933
Russia is trying and they
can't, India can't, Europe

1235
00:48:52,000 --> 00:48:53,266
can't, we can't.

1236
00:48:53,333 --> 00:48:58,733
We are all, oddly
enough, too small.

1237
00:48:58,800 --> 00:49:00,099
>> CRAIG SPIEZLE: Anyone
want to follow up on that

1238
00:49:00,166 --> 00:49:02,766
comment at all about
innovation investment?

1239
00:49:02,833 --> 00:49:05,099
>> KATIE ARRINGTON: We are
investing in innovation

1240
00:49:05,166 --> 00:49:06,400
without a doubt. Right?

1241
00:49:06,466 --> 00:49:08,165
But we do look to industry.

1242
00:49:08,233 --> 00:49:09,500
The Department of Defense
has always looked at

1243
00:49:09,566 --> 00:49:12,265
industry to come up with
innovation because we are

1244
00:49:12,333 --> 00:49:13,433
the Department of Defense.

1245
00:49:13,500 --> 00:49:14,966
Our job is to defend.

1246
00:49:15,033 --> 00:49:19,732
Innovation, to put it, we
need to make sure that as

1247
00:49:19,800 --> 00:49:21,766
we go forward that
we're putting the right

1248
00:49:21,833 --> 00:49:24,666
resources after the right
technology, not trying to

1249
00:49:24,733 --> 00:49:28,000
go after things that have
already been breached.

1250
00:49:28,066 --> 00:49:31,566
The comment that you made
is that China can bring it

1251
00:49:31,633 --> 00:49:33,698
to the market.

1252
00:49:33,766 --> 00:49:35,266
>> CRAIG SPIEZLE: Huawei.

1253
00:49:35,333 --> 00:49:36,599
>> KATIE ARRINGTON:
Well, built it better.

1254
00:49:36,666 --> 00:49:39,133
Because you based it on
my work, like a lot of my

1255
00:49:39,199 --> 00:49:39,800
work. Right?

1256
00:49:39,866 --> 00:49:40,699
Let's be really honest.

1257
00:49:40,766 --> 00:49:42,633
It's a lot of my tax
dollars that have gone

1258
00:49:42,699 --> 00:49:45,300
into this that our
adversaries have kind of

1259
00:49:45,366 --> 00:49:47,500
stolen and then built upon it.

1260
00:49:47,566 --> 00:49:49,866
So, yeah, they
can do it cheaper. Right?

1261
00:49:49,933 --> 00:49:51,500
They don't have to build
and know how to build a

1262
00:49:51,566 --> 00:49:52,832
Navy war ship.

1263
00:49:52,900 --> 00:49:54,033
>> BRUCE SCHNEIER: That
shouldn't be relevant for

1264
00:49:54,099 --> 00:49:55,066
this conversation.

1265
00:49:55,133 --> 00:49:56,133
That's the trade war part.

1266
00:49:56,199 --> 00:49:57,233
>> KATIE ARRINGTON:
It isn't, but it is.

1267
00:49:57,300 --> 00:49:58,633
But what are you
willing to sacrifice?

1268
00:49:58,699 --> 00:50:01,966
We lost 70 years worth
of naval intelligence

1269
00:50:02,033 --> 00:50:03,066
and design.

1270
00:50:03,133 --> 00:50:04,299
>> BRUCE SCHNEIER: I know.

1271
00:50:04,366 --> 00:50:05,633
But lots of things are
true but aren't relevant

1272
00:50:05,699 --> 00:50:08,733
to this 5G supply
chain discussion.

1273
00:50:08,800 --> 00:50:12,733
I get that we're annoyed
and getting even is good.

1274
00:50:12,800 --> 00:50:14,300
>> KATIE ARRINGTON: But
you keep looking for the

1275
00:50:14,366 --> 00:50:15,500
DoD to solve a problem.

1276
00:50:15,566 --> 00:50:18,533
>> BRUCE SCHNEIER: It's a
defense problem. It's your name.

1277
00:50:18,599 --> 00:50:19,699
>> KATIE ARRINGTON:
Because -

1278
00:50:19,766 --> 00:50:21,500
>> CRAIG SPIEZLE: Maybe we need
to do an audience poll here.

1279
00:50:21,566 --> 00:50:24,165
>> KATIE ARRINGTON: Who
owns the - who owns 4G?

1280
00:50:24,233 --> 00:50:27,500
Does the Department of
Defense in the United States

1281
00:50:27,566 --> 00:50:30,033
own it? No.

1282
00:50:30,099 --> 00:50:32,233
>> BRUCE SCHNEIER: Then
why did you rely on it?

1283
00:50:32,300 --> 00:50:33,333
>> KATIE ARRINGTON: What I
said, I'm not going to put

1284
00:50:33,400 --> 00:50:34,500
it in my networks.

1285
00:50:34,566 --> 00:50:36,165
I'm not going to put it
in the stuff that I'm

1286
00:50:36,233 --> 00:50:39,900
responsible for. Period.

1287
00:50:39,966 --> 00:50:41,799
>> BRUCE SCHNEIER: Why am
I defending China again?

1288
00:50:41,866 --> 00:50:44,533
>> ANDY PURDY: I would say
block Huawei if you must,

1289
00:50:44,599 --> 00:50:46,433
but we need to do a whole
lot more to make America

1290
00:50:46,500 --> 00:50:48,233
safer and make America
more competitive in

1291
00:50:48,300 --> 00:50:49,366
the world.

1292
00:50:49,433 --> 00:50:51,566
I think it's great that the U.S.
government is starting to focus

1293
00:50:51,633 --> 00:50:52,698
on ICT.

1294
00:50:52,766 --> 00:50:53,766
That's great.

1295
00:50:53,833 --> 00:50:55,733
And to the extent that the U.S.
government can help create

1296
00:50:55,800 --> 00:50:59,366
greater competition among
equipment providers, the

1297
00:50:59,433 --> 00:51:00,833
world will be better off.

1298
00:51:00,900 --> 00:51:03,000
But we've got to have
experts who can come up

1299
00:51:03,066 --> 00:51:06,598
with some ideas, such as
if - and that's an if - if

1300
00:51:06,666 --> 00:51:10,633
the U.S. needs its own telecom
equipment provider, why in

1301
00:51:10,699 --> 00:51:14,800
the world wouldn't the U.S. call
Huawei's bluff for the offer to

1302
00:51:14,866 --> 00:51:16,333
license our 5G technology?

1303
00:51:16,400 --> 00:51:19,233
Why wouldn't you at least
have those conversations?

1304
00:51:19,300 --> 00:51:21,433
And why don't you think
about ways that the

1305
00:51:21,500 --> 00:51:25,366
experts could say you guys
want to come into Huawei's

1306
00:51:25,433 --> 00:51:28,000
facilities and
Eriksson to deep dive?

1307
00:51:28,066 --> 00:51:29,098
What can be done?

1308
00:51:29,166 --> 00:51:31,266
How can third parties
monitor what's happening

1309
00:51:31,333 --> 00:51:34,199
in the actual creation and
manufacture to make sure

1310
00:51:34,266 --> 00:51:35,633
that we know what we're
doing and there's

1311
00:51:35,699 --> 00:51:37,033
independent transparency?

1312
00:51:37,099 --> 00:51:39,400
Make requests to the
companies of what can be

1313
00:51:39,466 --> 00:51:41,699
done to add the kind of
level of trust that's not

1314
00:51:41,766 --> 00:51:42,599
there now.

1315
00:51:42,666 --> 00:51:45,233
But to simply stand silent
and not make demands and

1316
00:51:45,300 --> 00:51:47,533
requests on the equipment
providers doesn't help

1317
00:51:47,599 --> 00:51:48,933
the situation.

1318
00:51:49,000 --> 00:51:50,400
>> CRAIG SPIEZLE:
We're going to go to -

1319
00:51:50,466 --> 00:51:51,732
>> BRUCE SCHNEIER:
I'll agree with that.

1320
00:51:51,800 --> 00:51:53,400
>> CRAIG SPIEZLE: It's a
good point for maybe a

1321
00:51:53,466 --> 00:51:56,433
wrap-up lightning round
of comments there.

1322
00:51:56,500 --> 00:51:59,233
Katie, again, it looks
like they're beating up on

1323
00:51:59,300 --> 00:52:00,500
you here, but maybe.

1324
00:52:00,566 --> 00:52:01,598
>> BRUCE SCHNEIER: We
don't mean to, though.

1325
00:52:01,666 --> 00:52:03,099
You're like on the good side.

1326
00:52:03,166 --> 00:52:05,566
>> KATIE ARRINGTON: I
am on the good side. Right?

1327
00:52:05,633 --> 00:52:14,533
Listen, I came here today
because sometimes you've

1328
00:52:14,599 --> 00:52:16,533
just got to say the truth
and you've just got to

1329
00:52:16,599 --> 00:52:17,900
hold the line.

1330
00:52:17,966 --> 00:52:22,232
And then when you do
that and then the right

1331
00:52:22,300 --> 00:52:27,533
behaviors become normal,
what you desire, Mr. Purdy,

1332
00:52:27,599 --> 00:52:28,566
becomes a reality.

1333
00:52:28,633 --> 00:52:32,133
But unless you're willing
to draw the line and say I

1334
00:52:32,199 --> 00:52:33,400
can't move off of this.

1335
00:52:33,466 --> 00:52:35,266
You're going to have
to do something better.

1336
00:52:35,333 --> 00:52:40,833
We all have to do better,
1,000% everybody.

1337
00:52:40,900 --> 00:52:44,933
And I think that this
event has brought to light

1338
00:52:45,000 --> 00:52:49,066
what we want, where do we
want to go, what is the

1339
00:52:49,133 --> 00:52:51,732
expectation for supply
chain, what is the

1340
00:52:51,800 --> 00:52:55,266
expectation for security,
because it came up on all

1341
00:52:55,333 --> 00:52:56,166
of us.

1342
00:52:56,233 --> 00:53:00,166
Everyone in this room,
technology snuck up on us.

1343
00:53:00,233 --> 00:53:01,066
It came.

1344
00:53:01,133 --> 00:53:04,765
We didn't put the tools
in 20 years ago, did we?

1345
00:53:04,833 --> 00:53:05,433
Yes or no?

1346
00:53:05,500 --> 00:53:06,099
>> ANDY PURDY: No.

1347
00:53:06,166 --> 00:53:06,766
>> KATIE ARRINGTON: No.

1348
00:53:06,833 --> 00:53:08,533
So it came - we got it.

1349
00:53:08,599 --> 00:53:10,000
We've got to get better.

1350
00:53:10,066 --> 00:53:14,698
But this is the event that
made us kind of wake up, okay.

1351
00:53:14,766 --> 00:53:17,633
To have that kind of
dialogue going future, now

1352
00:53:17,699 --> 00:53:20,900
we see it as a must,
absolutely must.

1353
00:53:20,966 --> 00:53:23,633
But where I am today and
where I'll be in ten years

1354
00:53:23,699 --> 00:53:25,133
is a different story. Right?

1355
00:53:25,199 --> 00:53:27,533
Today I have to buy down
the risk for what I know

1356
00:53:27,599 --> 00:53:28,533
is a problem.

1357
00:53:28,599 --> 00:53:30,900
I have to protect
you as a nation.

1358
00:53:30,966 --> 00:53:32,400
That's our job.

1359
00:53:32,466 --> 00:53:34,732
The men and women that
sign up to volunteer to

1360
00:53:34,800 --> 00:53:37,533
protect your lives,
protect your freedom.

1361
00:53:37,599 --> 00:53:42,966
My job, the SecDef, our
job is to do the best

1362
00:53:43,033 --> 00:53:44,866
thing that we can do at
the right moment, at the

1363
00:53:44,933 --> 00:53:46,800
right time, to protect
the war fighter and the

1364
00:53:46,866 --> 00:53:48,500
national defense.

1365
00:53:48,566 --> 00:53:49,500
>> CRAIG SPIEZLE: Great.

1366
00:53:49,566 --> 00:53:50,399
>> KATIE ARRINGTON:
Can we do better? Yes.

1367
00:53:50,466 --> 00:53:52,400
Will we? Yes.

1368
00:53:52,466 --> 00:53:54,133
>> CRAIG SPIEZLE: Bruce,
a closing comment?

1369
00:53:57,199 --> 00:53:58,099
>> BRUCE SCHNEIER: We
didn't do the right thing

1370
00:53:58,166 --> 00:53:59,933
20 years ago and I
actually will lay the

1371
00:54:00,000 --> 00:54:02,233
blame a lot on the NSA for
that because that was back

1372
00:54:02,300 --> 00:54:04,566
when we had an advantage
in spying on telecom

1373
00:54:04,633 --> 00:54:06,098
networks and we
wanted to keep it.

1374
00:54:06,166 --> 00:54:09,433
And, yeah, we did a lot of
things bad to make these

1375
00:54:09,500 --> 00:54:12,400
protocols insecure.

1376
00:54:12,466 --> 00:54:15,799
We can help -
we can do things here.

1377
00:54:15,866 --> 00:54:17,400
They are going
to be expensive.

1378
00:54:17,466 --> 00:54:20,900
I do not believe that we
have the appetite, whether

1379
00:54:20,966 --> 00:54:23,133
it's the military, the
government, industry,

1380
00:54:23,199 --> 00:54:27,800
anybody, to spend the
money on secure devices,

1381
00:54:27,866 --> 00:54:29,900
on devices that have the
kind of supply chain

1382
00:54:29,966 --> 00:54:32,098
security that would
make a difference.

1383
00:54:32,166 --> 00:54:33,033
We're not.

1384
00:54:33,099 --> 00:54:34,866
And until we are, we're
not going to get it.

1385
00:54:34,933 --> 00:54:35,699
We could complain.

1386
00:54:35,766 --> 00:54:38,500
We can wish for things.

1387
00:54:38,566 --> 00:54:41,466
But it's not going to
happen without real funding.

1388
00:54:41,533 --> 00:54:43,199
It's not funding the
market is going to provide.

1389
00:54:43,266 --> 00:54:44,300
The market doesn't care.

1390
00:54:44,366 --> 00:54:46,066
This has to come from
the top because it's

1391
00:54:46,133 --> 00:54:48,133
fundamentally a national
security problem.

1392
00:54:48,199 --> 00:54:51,500
This is how we solve these
collective action problems.

1393
00:54:51,566 --> 00:54:54,732
Until that happens, this
is all going to be talk

1394
00:54:54,800 --> 00:54:55,699
on panels.

1395
00:54:55,766 --> 00:54:57,333
There's not going
to be security. That's sad.

1396
00:54:57,400 --> 00:54:58,766
>> CRAIG SPIEZLE: Kathryn?

1397
00:54:58,833 --> 00:55:00,233
>> KATHRYN WALDRON: I mean,
it is a national security issue.

1398
00:55:00,300 --> 00:55:02,533
It's, to many people,
an economic issue.

1399
00:55:02,599 --> 00:55:05,000
I think, you know, we've
debated the whole time is

1400
00:55:05,066 --> 00:55:06,665
this trade or is this
national security.

1401
00:55:06,733 --> 00:55:08,900
The unfortunate
answer is it's both.

1402
00:55:08,966 --> 00:55:10,799
It depends on who you're
talking to as to which one

1403
00:55:10,866 --> 00:55:12,133
they're going
to prioritize.

1404
00:55:12,199 --> 00:55:14,400
Part of the reason that we
are in this situation with

1405
00:55:14,466 --> 00:55:18,033
China and 5G is that for
so many companies over the

1406
00:55:18,099 --> 00:55:21,300
past 20 years, their
answer has been, oh, I'm

1407
00:55:21,366 --> 00:55:23,599
going to prioritize the
economic aspect and not

1408
00:55:23,666 --> 00:55:24,733
the security aspect.

1409
00:55:24,800 --> 00:55:27,333
And so if we really are
screwed when it comes to

1410
00:55:27,400 --> 00:55:31,400
5G and we want to think
about securing 6G, then we

1411
00:55:31,466 --> 00:55:33,400
need to spend an awful lot
of time about thinking how

1412
00:55:33,466 --> 00:55:36,366
did we end up in the
spot where we are now.

1413
00:55:36,433 --> 00:55:38,033
>> CRAIG SPIEZLE: Great. I
just want to wrap it up here.

1414
00:55:38,099 --> 00:55:40,800
I think looking ahead -
I'm not sure if that slide

1415
00:55:40,866 --> 00:55:43,333
is up there - I think
hopefully we can learn a

1416
00:55:43,400 --> 00:55:44,199
lot by this.

1417
00:55:44,266 --> 00:55:47,699
Hopefully, we can apply
the same rigor into other

1418
00:55:47,766 --> 00:55:49,333
areas and other
components, other

1419
00:55:49,400 --> 00:55:52,199
operators, and look at how
do we improve resiliency.

1420
00:55:52,266 --> 00:55:53,766
It's not guaranteed there.

1421
00:55:53,833 --> 00:55:57,099
On a note, I understand
the Administration is

1422
00:55:57,166 --> 00:55:58,800
pulling together thought
leaders at the end of

1423
00:55:58,866 --> 00:56:02,133
April or the beginning of
April on 5G, and so maybe

1424
00:56:02,199 --> 00:56:04,900
some of us can be part
of that discussion here.

1425
00:56:04,966 --> 00:56:06,533
And, again, I want to
thank the panel for

1426
00:56:06,599 --> 00:56:07,400
their insights.

1427
00:56:07,466 --> 00:56:09,833
As I promised, this would
be colorful, and I think

1428
00:56:09,900 --> 00:56:10,833
different points of view.

1429
00:56:10,900 --> 00:56:12,466
Hopefully, we
delivered on that.

1430
00:56:12,533 --> 00:56:13,400
And so thank you again.

1431
00:56:13,466 --> 00:56:15,732
Thank you, everyone, for
your insights today.

1432
00:56:15,800 --> 00:56:16,533
>> BRUCE SCHNEIER:
Thanks for coming.

1433
00:56:16,599 --> 00:56:17,566
>> KATHRYN
WALDRON: Thank you.

