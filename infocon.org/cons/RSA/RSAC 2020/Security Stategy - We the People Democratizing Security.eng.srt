1
00:00:06,966 --> 00:00:08,766
>> ANNOUNCER: Please
welcome Head of Advisory

2
00:00:08,833 --> 00:00:11,733
CISOs, Cisco,
Wendy Nather.

3
00:00:12,966 --> 00:00:15,599
(Applause)

4
00:00:20,500 --> 00:00:21,833
>> WENDY NATHER:
Thank you.

5
00:00:26,733 --> 00:00:28,866
What were we thinking?

6
00:00:30,533 --> 00:00:33,899
No, seriously, what
were we thinking?

7
00:00:33,966 --> 00:00:35,733
I know what we
were thinking.

8
00:00:35,799 --> 00:00:38,299
We were thinking that IT
professionals would be the

9
00:00:38,366 --> 00:00:41,033
only ones to be using
technology and that

10
00:00:41,100 --> 00:00:44,000
companies would control
how it was used.

11
00:00:44,066 --> 00:00:46,933
We even thought you had
to know how to program in

12
00:00:47,000 --> 00:00:49,000
order to be able
to use technology.

13
00:00:50,899 --> 00:00:54,799
It used to be, once upon
a time, that we were the

14
00:00:54,866 --> 00:00:56,966
masters of the universe.

15
00:00:57,033 --> 00:00:59,633
We had all the knowledge,
so we made all the rules

16
00:00:59,700 --> 00:01:02,866
and everybody else was
supposed to follow them.

17
00:01:02,933 --> 00:01:05,166
We issued and
managed the devices.

18
00:01:05,233 --> 00:01:07,133
We wrote the software.

19
00:01:07,200 --> 00:01:08,266
We ran the servers.

20
00:01:08,333 --> 00:01:10,066
We pulled the cables.

21
00:01:10,133 --> 00:01:11,833
But none of that is
working out as well as it

22
00:01:11,900 --> 00:01:13,766
used to, is it?

23
00:01:13,833 --> 00:01:17,433
Because now laptops have
left the building, so have

24
00:01:17,500 --> 00:01:20,900
the servers and the
software, and every one of

25
00:01:20,966 --> 00:01:23,933
us is a consumer of
technology and we do what

26
00:01:24,000 --> 00:01:26,900
we want with
our own devices.

27
00:01:26,966 --> 00:01:32,565
We are trying to secure
with an unsustainable

28
00:01:32,633 --> 00:01:35,533
security model and it's
time to break it and put

29
00:01:35,599 --> 00:01:37,599
it back together.

30
00:01:37,666 --> 00:01:40,333
Now how do we put
this back together?

31
00:01:40,400 --> 00:01:42,700
In some industries they've
reinvented themselves

32
00:01:42,766 --> 00:01:45,933
based on the demands
of their consumers.

33
00:01:46,000 --> 00:01:48,366
Now, if every one of us
is a consumer then tech

34
00:01:48,433 --> 00:01:50,900
belongs to all of us.

35
00:01:50,966 --> 00:01:53,298
Tech is being
democratized.

36
00:01:53,366 --> 00:01:56,700
And so, it is time to
democratize security.

37
00:01:59,266 --> 00:02:01,400
Now what does
that even mean?

38
00:02:01,466 --> 00:02:04,265
You're going to hear this
phrase, "democratizing

39
00:02:04,333 --> 00:02:08,500
security," this week at RSA
from a bunch of other people.

40
00:02:08,566 --> 00:02:10,800
And it can mean
different things.

41
00:02:10,866 --> 00:02:13,933
For example, it can mean
sharing security tools and

42
00:02:14,000 --> 00:02:17,099
software with people
outside of the industry.

43
00:02:17,166 --> 00:02:21,466
It can mean what 451
Research describes as the

44
00:02:21,533 --> 00:02:24,433
rise of the citizen
contributor, people

45
00:02:24,500 --> 00:02:26,699
outside the industry who
are bringing knowledge and

46
00:02:26,766 --> 00:02:30,000
research and vulnerabilities
and so on, inside.

47
00:02:30,066 --> 00:02:32,566
I'm going to talk about
what it means to me and to

48
00:02:32,633 --> 00:02:37,333
people I've asked about
it all around the world.

49
00:02:37,400 --> 00:02:39,833
And I'm also going to
talk about three ways that

50
00:02:39,900 --> 00:02:41,599
we can do this.

51
00:02:41,666 --> 00:02:45,133
We can start by changing
our model of security from

52
00:02:45,199 --> 00:02:48,633
a control model
to collaboration.

53
00:02:48,699 --> 00:02:51,166
We can simplify and
redesign the security

54
00:02:51,233 --> 00:02:55,300
controls that we use, and
we need to open up our

55
00:02:55,366 --> 00:02:58,199
security culture
to everybody.

56
00:02:58,266 --> 00:03:00,266
Let's start
with number one.

57
00:03:01,866 --> 00:03:03,099
I love this quote.

58
00:03:03,166 --> 00:03:06,266
"When you can't control
people's behavior you have

59
00:03:06,333 --> 00:03:10,233
to deputize the majority
of users to be CIOs.

60
00:03:10,300 --> 00:03:13,133
Security should be
designed to be adopted

61
00:03:13,199 --> 00:03:16,433
rather than engineered
to be enforced."

62
00:03:16,500 --> 00:03:20,266
And this is of course from
my great grand boss, Doug

63
00:03:20,333 --> 00:03:23,099
Song, the co-founder, with
John Oberheide, of Duo

64
00:03:23,166 --> 00:03:25,599
Security and
now GM at Cisco.

65
00:03:25,666 --> 00:03:27,733
And he was the first
person that I heard

66
00:03:27,800 --> 00:03:29,800
democratizing
security from.

67
00:03:32,733 --> 00:03:35,199
It used to be, once upon
a time, that we had work

68
00:03:35,266 --> 00:03:37,899
stuff and we
had life stuff.

69
00:03:37,966 --> 00:03:41,733
You used very different
systems and software at

70
00:03:41,800 --> 00:03:45,133
work than you did when you
went home, because nobody

71
00:03:45,199 --> 00:03:49,366
goes home and uses ERP
software for fun, right?

72
00:03:49,433 --> 00:03:50,366
Right.

73
00:03:50,433 --> 00:03:52,698
Because if anybody does, I
want to stage an intervention.

74
00:03:55,066 --> 00:03:59,033
But now we are using the
same software and systems,

75
00:03:59,099 --> 00:04:02,899
the same SAAS for work
stuff and life stuff, the

76
00:04:02,966 --> 00:04:04,900
same things that our
employers are using like

77
00:04:04,966 --> 00:04:07,800
Gmail where the only
difference is which

78
00:04:07,866 --> 00:04:10,400
username you
are typing in.

79
00:04:10,466 --> 00:04:13,333
So, as work and life
are getting mashed up

80
00:04:13,400 --> 00:04:15,199
together, people are
getting nervous about

81
00:04:15,266 --> 00:04:17,366
whether their employers
are going to try to

82
00:04:17,433 --> 00:04:20,233
control their lives.

83
00:04:20,300 --> 00:04:23,233
They say, don't you dare
wipe my personal data off

84
00:04:23,300 --> 00:04:25,133
of my device.

85
00:04:25,199 --> 00:04:27,100
I'm going to do
what I want with it.

86
00:04:28,866 --> 00:04:33,265
When people worry about control, they start pushing back.

87
00:04:33,333 --> 00:04:36,033
Now, when I was working at
a Swiss bank, I found out

88
00:04:36,100 --> 00:04:39,066
later from my team that
whenever I would walk

89
00:04:39,133 --> 00:04:40,966
through the trading floor
in my little designer

90
00:04:41,033 --> 00:04:45,666
suit, the traders would
literally see me coming,

91
00:04:45,733 --> 00:04:47,100
they would unplug the
modems from their

92
00:04:47,166 --> 00:04:49,899
workstations, hide them
in the desk drawer, wait

93
00:04:49,966 --> 00:04:52,866
until I went by, and then
they would pull them back

94
00:04:52,933 --> 00:04:55,466
out and plug them back in.

95
00:04:55,533 --> 00:04:57,933
Now, was I the threat they
should have been focused on?

96
00:05:00,066 --> 00:05:03,633
But I was the most
visible threat.

97
00:05:03,699 --> 00:05:07,233
This is what we run into
when we run into control issues.

98
00:05:09,399 --> 00:05:13,133
We have to change security
from an authoritarian

99
00:05:13,199 --> 00:05:17,466
control model to a
collaboration model.

100
00:05:17,533 --> 00:05:19,433
Now, let me ask you, do
you think that we are the

101
00:05:19,500 --> 00:05:22,000
ultimate authorities
on security?

102
00:05:22,066 --> 00:05:24,666
Should we be making all
the security decisions?

103
00:05:26,633 --> 00:05:27,633
No.

104
00:05:28,933 --> 00:05:32,433
I'm going to put it to you
that sometimes, sometimes

105
00:05:32,500 --> 00:05:35,866
users can make better
security decisions than we can.

106
00:05:35,933 --> 00:05:37,533
I know; it's time to
clutch your pearls.

107
00:05:37,600 --> 00:05:42,666
It is a scary thing to
contemplate but really, we

108
00:05:42,733 --> 00:05:45,300
need to face this reality.

109
00:05:45,366 --> 00:05:47,566
I have a friend who is
a CISO at a major tech

110
00:05:47,633 --> 00:05:50,198
company, you are going
to see him on this stage

111
00:05:50,266 --> 00:05:53,166
later in the week, and
he tells me how when the

112
00:05:53,233 --> 00:05:55,833
business comes to him with
an initiative and they

113
00:05:55,899 --> 00:06:00,666
say, is this safe to do,
he says, I don't know, is it?

114
00:06:00,733 --> 00:06:02,733
You tell me.

115
00:06:02,800 --> 00:06:05,300
Now, that may sound kind
of snarky but what he is

116
00:06:05,366 --> 00:06:07,599
really doing is he is
empowering them to make

117
00:06:07,666 --> 00:06:11,332
the security decisions
because he is - he knows

118
00:06:11,399 --> 00:06:13,399
that they are better at
weighing the business

119
00:06:13,466 --> 00:06:18,000
opportunity against the
business risk than he is.

120
00:06:18,066 --> 00:06:21,265
This is what we have to
make room for when we are

121
00:06:21,333 --> 00:06:23,533
changing our security
control model.

122
00:06:26,500 --> 00:06:28,399
Now, the word
collaboration is actually

123
00:06:28,466 --> 00:06:29,766
not new.

124
00:06:29,833 --> 00:06:32,833
It's been twenty years now
since the Jericho Forum

125
00:06:32,899 --> 00:06:36,566
started talking about
de-parameterization and

126
00:06:36,633 --> 00:06:39,066
they proposed a
collaboration-oriented

127
00:06:39,133 --> 00:06:40,099
architecture.

128
00:06:40,166 --> 00:06:42,300
There's that word,
collaboration.

129
00:06:42,366 --> 00:06:44,532
Because when you can't
control something that you

130
00:06:44,600 --> 00:06:48,266
need to secure, you are
stuck with collaboration.

131
00:06:48,333 --> 00:06:50,866
What that's going to look
like, of course now we are

132
00:06:50,933 --> 00:06:54,699
calling it Zero Trust, but
I don't like the term Zero

133
00:06:54,766 --> 00:06:56,600
Trust as much as I
like collaboration.

134
00:06:56,666 --> 00:06:57,966
Sorry, John Kindervag.

135
00:06:58,033 --> 00:07:00,933
But it is the same thing.

136
00:07:01,000 --> 00:07:03,033
We are ending up saying
look, do what you want on

137
00:07:03,100 --> 00:07:05,600
your own time with your
own devices, but if you

138
00:07:05,666 --> 00:07:08,332
want to access our
resources, you have to

139
00:07:08,399 --> 00:07:10,633
meet our security
requirements.

140
00:07:10,699 --> 00:07:12,466
Now, they might be
different requirements

141
00:07:12,533 --> 00:07:14,100
based on what
we're protecting.

142
00:07:14,166 --> 00:07:16,199
We might care what
you are using.

143
00:07:16,266 --> 00:07:17,699
We might not care.

144
00:07:17,766 --> 00:07:20,100
But either way, you figure
out how you want to meet

145
00:07:20,166 --> 00:07:23,133
our requirements
and then we'll talk.

146
00:07:23,199 --> 00:07:25,533
Collaboration.

147
00:07:25,600 --> 00:07:28,033
What will this look
like when we get to an

148
00:07:28,100 --> 00:07:29,533
architecture?

149
00:07:29,600 --> 00:07:33,333
I've got a very, very
high-level diagram here.

150
00:07:33,399 --> 00:07:35,833
If you imagine that in
the green box is what we

151
00:07:35,899 --> 00:07:38,533
normally keep in the data
center and that's what we

152
00:07:38,600 --> 00:07:40,100
assume that we control.

153
00:07:40,166 --> 00:07:41,666
You've got your web
servers, you've got

154
00:07:41,733 --> 00:07:43,833
database, you've got
back office servers and

155
00:07:43,899 --> 00:07:45,433
everything.

156
00:07:45,500 --> 00:07:47,399
But there's a whole bunch
of stuff that we are using

157
00:07:47,466 --> 00:07:49,833
externally like
infrastructure as a

158
00:07:49,899 --> 00:07:51,833
service, you are running
stuff in the cloud.

159
00:07:51,899 --> 00:07:53,699
You've got software
as a service.

160
00:07:53,766 --> 00:07:55,699
You are using
third party APIs.

161
00:07:55,766 --> 00:07:59,866
And, of course, you don't
control the internet.

162
00:07:59,933 --> 00:08:01,500
And you don't
control the user.

163
00:08:01,566 --> 00:08:03,566
You might think you
do but you don't.

164
00:08:05,199 --> 00:08:07,466
And, of course, when we
look below this, there are

165
00:08:07,533 --> 00:08:09,933
a whole bunch of layers
that we might or might not

166
00:08:10,000 --> 00:08:12,500
have access to, that we
might or might not control

167
00:08:12,566 --> 00:08:14,099
even in our own
data center.

168
00:08:14,166 --> 00:08:15,899
Another department
might control some

169
00:08:15,966 --> 00:08:17,600
of the layers underneath.

170
00:08:17,666 --> 00:08:21,166
If you are using - if
you are running VMs, for

171
00:08:21,233 --> 00:08:23,966
example, you are only
going to be able to

172
00:08:24,033 --> 00:08:27,533
control that top layer,
the application layer; you

173
00:08:27,600 --> 00:08:29,833
are not going to be able
to control the hyper

174
00:08:29,899 --> 00:08:33,100
visor, the operating system,
the hardware, the network.

175
00:08:33,166 --> 00:08:35,266
With SAAS, you are only
controlling the little

176
00:08:35,332 --> 00:08:39,366
wedge of what you are - of
what your account is with

177
00:08:39,433 --> 00:08:40,732
that provider.

178
00:08:40,799 --> 00:08:42,866
If you are using a
third-party API, the only

179
00:08:42,933 --> 00:08:46,399
part you control
is your use of it.

180
00:08:46,466 --> 00:08:49,299
And, as I mentioned, we
don't control the user.

181
00:08:49,366 --> 00:08:51,199
The users are
pushing back.

182
00:08:51,266 --> 00:08:54,433
They are taking back
control and we can either

183
00:08:54,500 --> 00:08:58,600
fight it or we can work
with them to make it

184
00:08:58,666 --> 00:09:00,666
work for them.

185
00:09:00,733 --> 00:09:03,366
But imagine how our
architectures will look in

186
00:09:03,433 --> 00:09:05,833
the future when we start
taking this into account,

187
00:09:05,899 --> 00:09:08,700
these boundaries of
negotiation, these

188
00:09:08,766 --> 00:09:10,500
boundaries of control.

189
00:09:10,566 --> 00:09:12,799
We already have this
with third parties.

190
00:09:12,866 --> 00:09:15,466
We're just not explicit
enough about it.

191
00:09:15,533 --> 00:09:17,333
But think about how
this would change your

192
00:09:17,399 --> 00:09:20,233
architecture if you are
thinking about what you

193
00:09:20,299 --> 00:09:23,266
control and
what you don't.

194
00:09:23,333 --> 00:09:27,665
And, in fact, you might
want to give away control.

195
00:09:27,733 --> 00:09:29,433
What would this look like?

196
00:09:29,500 --> 00:09:32,333
It might be setting up a
self-service portal for

197
00:09:32,399 --> 00:09:34,966
your users, for
researchers to look at

198
00:09:35,033 --> 00:09:37,833
what data they want to
use and looking up what

199
00:09:37,899 --> 00:09:40,200
regulations and security
controls they need to

200
00:09:40,266 --> 00:09:41,199
comply with.

201
00:09:41,266 --> 00:09:44,399
I've seen this already
at universities.

202
00:09:44,466 --> 00:09:47,799
It might be externalizing
the business rules into a

203
00:09:47,866 --> 00:09:50,533
separate engine from the
application so that the

204
00:09:50,600 --> 00:09:52,733
business can change those
rules whenever they need

205
00:09:52,799 --> 00:09:56,233
to without you rewriting
the whole application.

206
00:09:56,299 --> 00:09:58,000
There are lots of
ways to do this.

207
00:09:58,066 --> 00:10:00,233
And I know it makes people
nervous, especially

208
00:10:00,299 --> 00:10:02,799
security people, to
think about the idea of

209
00:10:02,866 --> 00:10:04,699
giving away control.

210
00:10:04,766 --> 00:10:06,899
So, let me talk about
it in a different way.

211
00:10:06,966 --> 00:10:10,399
Let me reframe it and see
if it makes you feel better.

212
00:10:10,466 --> 00:10:15,866
If you think about control
equaling cost, everything

213
00:10:15,933 --> 00:10:17,733
that you still need
to control is going to

214
00:10:17,799 --> 00:10:19,366
cost you more.

215
00:10:19,433 --> 00:10:20,533
You have to manage it.

216
00:10:20,600 --> 00:10:21,766
You have to maintain it.

217
00:10:21,833 --> 00:10:25,266
You've got to enforce
policies associated with it.

218
00:10:25,333 --> 00:10:29,699
All of that costs time,
money and people.

219
00:10:29,766 --> 00:10:32,199
So, if you want to
optimize what you really

220
00:10:32,266 --> 00:10:35,632
need to control and
centralize to manage

221
00:10:35,700 --> 00:10:38,466
security risk and what you
can give away - again,

222
00:10:38,533 --> 00:10:42,100
we're already doing this
with Cloud, we are just

223
00:10:42,166 --> 00:10:45,366
not being very explicit
about it but really is

224
00:10:45,433 --> 00:10:50,066
about optimizing cost
- then if we do this

225
00:10:50,133 --> 00:10:53,933
together, we can let the business make the decisions.

226
00:10:54,000 --> 00:10:56,633
They can be as agile
as they want to be.

227
00:10:56,700 --> 00:11:00,500
We can go zooming off into
the future at the speed of

228
00:11:00,566 --> 00:11:03,700
the business, which is
what we all want to do, right.

229
00:11:03,766 --> 00:11:07,165
We want to go hyper
drive with the business.

230
00:11:07,233 --> 00:11:09,233
This is what collaboration
will bring us.

231
00:11:11,433 --> 00:11:14,165
Next, we're going
to simplify design.

232
00:11:14,233 --> 00:11:16,532
Here's another
great quote.

233
00:11:16,600 --> 00:11:19,366
"Design measures that take
away the complexity in

234
00:11:19,433 --> 00:11:22,033
security and make
it understandable.

235
00:11:22,100 --> 00:11:25,000
Factor people in
during the design and

236
00:11:25,066 --> 00:11:28,033
implementation of
security measures."

237
00:11:28,100 --> 00:11:30,433
This is a quote from my
friend Elizabeth Kolade

238
00:11:30,500 --> 00:11:33,333
who is with the
Cybersecurity Experts

239
00:11:33,399 --> 00:11:37,200
Association of Nigeria and
this is what democratizing

240
00:11:37,266 --> 00:11:40,132
security means to her.

241
00:11:40,200 --> 00:11:42,633
So, let's talk
about design.

242
00:11:42,700 --> 00:11:44,733
Have you noticed that
we keep asking the same

243
00:11:44,799 --> 00:11:48,132
questions year after year
in RSA, like why doesn't

244
00:11:48,200 --> 00:11:51,000
management listen to
our risk analysis?

245
00:11:51,066 --> 00:11:53,200
Or why are security
professionals getting

246
00:11:53,266 --> 00:11:55,565
burned out?
Because we are.

247
00:11:55,633 --> 00:11:57,799
It's a real problem and
we're talking about it

248
00:11:57,866 --> 00:11:59,899
more and more.

249
00:11:59,966 --> 00:12:02,733
And my favorite, why do
users keep clicking things

250
00:12:02,799 --> 00:12:05,399
we tell them not to?

251
00:12:05,466 --> 00:12:06,799
Well, I'll tell you why.

252
00:12:06,866 --> 00:12:09,533
It's because users have
different priorities that

253
00:12:09,600 --> 00:12:14,233
are not security but also,
hello, the whole internet

254
00:12:14,299 --> 00:12:16,466
is built to click on.

255
00:12:16,533 --> 00:12:19,000
It's all about clicking.

256
00:12:19,066 --> 00:12:22,233
And we do not help as
security vendors when we

257
00:12:22,299 --> 00:12:25,066
say here, click on this to
download our 18-page white

258
00:12:25,133 --> 00:12:27,133
paper on how not
to click on things.

259
00:12:29,966 --> 00:12:31,466
This is getting ridiculous.

260
00:12:31,533 --> 00:12:33,799
We have to think about
whether we are yelling at

261
00:12:33,866 --> 00:12:36,966
them for the wrong things.

262
00:12:37,033 --> 00:12:38,700
Whether we're yelling at
them for doing what's

263
00:12:38,766 --> 00:12:41,533
natural, what they're
expected to do.

264
00:12:41,600 --> 00:12:43,700
When I was working for
a state agency, I had a

265
00:12:43,766 --> 00:12:46,533
general counsel who wanted
me to put in a mail filter

266
00:12:46,600 --> 00:12:49,600
to stop SSNs from
leaving the agency.

267
00:12:49,666 --> 00:12:50,266
Great.

268
00:12:50,333 --> 00:12:52,500
So, I put in
the mail filter.

269
00:12:52,566 --> 00:12:55,333
Two months later, guess
who was caught mailing

270
00:12:55,399 --> 00:12:58,399
himself social security
numbers in performance

271
00:12:58,466 --> 00:13:00,466
evaluations to
his home address?

272
00:13:03,200 --> 00:13:06,466
The same guy who wanted me
to stop this was doing it.

273
00:13:08,466 --> 00:13:10,500
But this is natural.

274
00:13:10,566 --> 00:13:12,299
This is normal.

275
00:13:12,366 --> 00:13:14,366
We have to stop
fighting this.

276
00:13:17,533 --> 00:13:19,366
Now, some of you know
that I'm new to Cisco.

277
00:13:19,433 --> 00:13:21,833
Duo was acquired by Cisco
about maybe a year and a

278
00:13:21,899 --> 00:13:24,899
half ago, and a little
while in I got an email

279
00:13:24,966 --> 00:13:29,399
that said your paid time
off is about to expire.

280
00:13:29,466 --> 00:13:30,366
And I thought, what?

281
00:13:30,433 --> 00:13:31,799
Is that even a thing?

282
00:13:31,866 --> 00:13:32,665
I mean, how would I know?

283
00:13:32,733 --> 00:13:34,099
I was new to Cisco.

284
00:13:34,166 --> 00:13:35,033
Maybe it was a thing.

285
00:13:35,100 --> 00:13:38,966
I almost clicked
on that link.

286
00:13:39,033 --> 00:13:40,700
Turns out it was a
phishing exercise.

287
00:13:40,766 --> 00:13:41,933
Thank goodness.

288
00:13:42,000 --> 00:13:45,066
But, again, why are
we yelling at users?

289
00:13:45,133 --> 00:13:47,799
Wouldn't it be easier and
better if we just secured

290
00:13:47,866 --> 00:13:50,665
things so that it wouldn't
matter if they clicked or

291
00:13:50,733 --> 00:13:52,833
not instead of saying
click that, no, don't

292
00:13:52,899 --> 00:13:55,133
click that, click that,
don't click that.

293
00:13:55,200 --> 00:13:57,266
This is a losing
proposition for all of us.

294
00:14:01,266 --> 00:14:04,433
What we are doing instead
year after year is we

295
00:14:04,500 --> 00:14:07,100
think that if we train
them louder and explain

296
00:14:07,166 --> 00:14:11,500
harder, that they will
stop clicking on things.

297
00:14:11,566 --> 00:14:14,200
Year after year after
year, oh we have to

298
00:14:14,266 --> 00:14:16,665
educate them more, we have
to educate them more.

299
00:14:16,733 --> 00:14:17,965
No.

300
00:14:18,033 --> 00:14:21,133
We are turning
ourselves into a meme.

301
00:14:21,200 --> 00:14:22,899
We cannot keep doing this.

302
00:14:22,966 --> 00:14:24,966
They're just trying
to do their job.

303
00:14:27,100 --> 00:14:29,700
Now, when I started in
this field about 35 years

304
00:14:29,766 --> 00:14:34,399
ago, it was a very small
community and we could

305
00:14:34,466 --> 00:14:37,200
describe everything as
intuitively obvious

306
00:14:37,266 --> 00:14:39,599
because we all had the
same level of knowledge,

307
00:14:39,666 --> 00:14:41,333
the same background.

308
00:14:41,399 --> 00:14:43,433
That's not the
case anymore.

309
00:14:43,500 --> 00:14:46,399
Today we write
for the world.

310
00:14:46,466 --> 00:14:49,333
We write for our
friends, our neighbors.

311
00:14:49,399 --> 00:14:52,366
We write for people in
countries we have

312
00:14:52,433 --> 00:14:54,500
never been to.

313
00:14:54,566 --> 00:14:56,466
They don't know what
we know and we cannot

314
00:14:56,533 --> 00:14:57,566
expect them to.

315
00:14:57,633 --> 00:15:00,600
In fact, they shouldn't
have to know what we know.

316
00:15:00,666 --> 00:15:03,899
They should not have to.

317
00:15:03,966 --> 00:15:05,666
Now, think about a spoon.

318
00:15:05,733 --> 00:15:07,333
Just go with me here.

319
00:15:07,399 --> 00:15:10,000
Think about a spoon.

320
00:15:10,066 --> 00:15:13,299
A spoon is really, really
hard to get wrong.

321
00:15:13,366 --> 00:15:15,000
It's really hard
to use it wrong.

322
00:15:15,066 --> 00:15:18,033
It is such a beautiful
design that you learn how

323
00:15:18,100 --> 00:15:20,833
to use it as a toddler and
you can go anywhere in the

324
00:15:20,899 --> 00:15:23,799
world and you see a spoon,
you can pick it up and you

325
00:15:23,866 --> 00:15:25,766
know how to use it.

326
00:15:25,833 --> 00:15:28,600
You don't have to have annual spoon awareness training.

327
00:15:32,533 --> 00:15:35,500
So, wouldn't it be great
if we could design

328
00:15:35,566 --> 00:15:38,766
security to be as easy to
use and as difficult to

329
00:15:38,833 --> 00:15:42,333
get wrong as a spoon.

330
00:15:42,399 --> 00:15:44,799
Here's a concrete example.

331
00:15:44,866 --> 00:15:48,000
Remember when Apple came
out with their screen lock

332
00:15:48,066 --> 00:15:50,100
on the iPhone and you had
to punch in four numbers

333
00:15:50,166 --> 00:15:51,500
to unlock it.

334
00:15:51,566 --> 00:15:55,000
According to their own
data, only 35% of their

335
00:15:55,066 --> 00:15:56,166
workforce enabled it.

336
00:15:56,233 --> 00:15:59,065
Even Steve Jobs refused to
use it because it was so

337
00:15:59,133 --> 00:16:02,100
annoying to type in
those four numbers.

338
00:16:02,166 --> 00:16:04,566
When they introduced touch
ID, it was a completely

339
00:16:04,633 --> 00:16:07,333
different story and the
usage, the adoption shot

340
00:16:07,399 --> 00:16:09,399
up to 80%.

341
00:16:10,433 --> 00:16:14,299
Now, biometrics were already
a thing; that wasn't new.

342
00:16:14,366 --> 00:16:18,533
It was not as easy to use
but here's the brilliant

343
00:16:18,600 --> 00:16:21,133
thing about the
design of the iPhone.

344
00:16:21,200 --> 00:16:24,033
They put the biometric,
the touch ID, right on the

345
00:16:24,100 --> 00:16:28,799
home button where users
were going to click anyway.

346
00:16:28,866 --> 00:16:31,165
Now that is
seamless design.

347
00:16:31,233 --> 00:16:32,833
It's where they wanted to
click, where they were

348
00:16:32,899 --> 00:16:34,533
going to click anyway.

349
00:16:34,600 --> 00:16:36,600
Security was right
there for them.

350
00:16:36,666 --> 00:16:38,799
This is what we should
be working towards.

351
00:16:38,866 --> 00:16:42,333
Security is what - should
be what the users would

352
00:16:42,399 --> 00:16:45,500
rather do anyway.

353
00:16:45,566 --> 00:16:49,000
We also have to design for
a wider range of personas.

354
00:16:49,066 --> 00:16:52,666
Not everybody is an admin
or tech professional,

355
00:16:52,733 --> 00:16:57,000
especially in small
and medium businesses.

356
00:16:57,066 --> 00:16:59,333
We have to be able to
design things for the way

357
00:16:59,399 --> 00:17:01,966
that they want to use
them, not the way we think

358
00:17:02,033 --> 00:17:04,266
they should use them.

359
00:17:04,333 --> 00:17:07,599
And we also have to
radically simplify what we

360
00:17:07,665 --> 00:17:09,265
are providing to them.

361
00:17:09,333 --> 00:17:11,433
As tech professionals,
we're comfortable with a

362
00:17:11,500 --> 00:17:13,200
certain amount of
complexity but our

363
00:17:13,266 --> 00:17:14,666
users are not.

364
00:17:14,733 --> 00:17:17,133
They do not want to sit
and admire our dashboards

365
00:17:17,200 --> 00:17:19,866
for hours on end,
examining all the little

366
00:17:19,933 --> 00:17:20,965
features and everything.

367
00:17:21,032 --> 00:17:21,699
No.

368
00:17:21,766 --> 00:17:22,766
They have a job to do.

369
00:17:22,833 --> 00:17:24,599
They want to get in and do
it and they want to get

370
00:17:24,665 --> 00:17:26,265
back out again.

371
00:17:26,333 --> 00:17:31,333
So, we have to simplify
functions, data,

372
00:17:31,400 --> 00:17:35,233
operations, all of those
things to make it easier

373
00:17:35,299 --> 00:17:38,666
no matter who is
going to use it.

374
00:17:38,733 --> 00:17:41,399
And then finally, we need
to build in a consumer

375
00:17:41,466 --> 00:17:43,065
grade experience.

376
00:17:43,133 --> 00:17:45,366
We are all used to the
consumer grade experience

377
00:17:45,433 --> 00:17:48,233
when we're at home and
users are pushing back now.

378
00:17:48,299 --> 00:17:50,400
When they come in to work,
they don't want this other

379
00:17:50,466 --> 00:17:53,166
kind of engineering
grade experience.

380
00:17:53,233 --> 00:17:56,799
They want the same slick
experience in both areas

381
00:17:56,866 --> 00:17:59,533
so we have to be
able to put that in.

382
00:17:59,599 --> 00:18:03,566
When we democratize tech
and security, people are

383
00:18:03,633 --> 00:18:04,900
going to vote.

384
00:18:04,966 --> 00:18:07,132
They're going to vote for
what they want with their

385
00:18:07,200 --> 00:18:09,066
budgets, with their
compliance or their

386
00:18:09,133 --> 00:18:12,266
noncompliance, and we have
to be ready to compete in

387
00:18:12,333 --> 00:18:14,333
that marketplace.

388
00:18:15,700 --> 00:18:18,900
We have to simplify the
experience for users and

389
00:18:18,966 --> 00:18:22,866
get better results
for everyone.

390
00:18:22,933 --> 00:18:26,733
Finally, we have to
open up the culture.

391
00:18:26,799 --> 00:18:29,866
In a digital society,
security has to be basic

392
00:18:29,933 --> 00:18:33,566
knowledge freely available
and taught to all ages.

393
00:18:33,633 --> 00:18:36,133
Both the laws and the
morals of this society

394
00:18:36,200 --> 00:18:39,533
must adapt and
behave accordingly.

395
00:18:39,599 --> 00:18:41,533
This is from my friend
Sarah Kriesche who is a

396
00:18:41,599 --> 00:18:43,599
freelance journalist
in Austria.

397
00:18:43,666 --> 00:18:47,966
This is what democratizing
security means to her.

398
00:18:48,033 --> 00:18:50,265
The morals, how do we
change the morals?

399
00:18:50,333 --> 00:18:53,233
First of all, we cannot
keep shoehorning people

400
00:18:53,299 --> 00:18:56,166
into our narrow
security culture.

401
00:18:56,233 --> 00:18:58,899
We have to stop thinking
of ourselves as wizards

402
00:18:58,966 --> 00:19:00,632
and them as muggles.

403
00:19:00,700 --> 00:19:05,733
We have to bring security
to them in ways that fit

404
00:19:05,799 --> 00:19:07,599
no matter what
age they are.

405
00:19:07,666 --> 00:19:10,265
For example, when people
- when kids are in

406
00:19:10,333 --> 00:19:12,900
kindergarten, we can't
expect them to type in

407
00:19:12,966 --> 00:19:15,866
passwords when they don't
know their letters and numbers.

408
00:19:15,933 --> 00:19:17,599
We're working on things
like that but I'm going to

409
00:19:17,666 --> 00:19:19,700
take it a step further
and I'm going to propose

410
00:19:19,766 --> 00:19:21,633
something very shocking.

411
00:19:21,700 --> 00:19:24,599
I think we should get rid
of parental controls on

412
00:19:24,666 --> 00:19:28,000
kids' devices - that's the
second pearl clutching

413
00:19:28,066 --> 00:19:32,799
moment - because we can no
longer raise children to

414
00:19:32,866 --> 00:19:35,799
be compliant with somebody
else's security model.

415
00:19:35,866 --> 00:19:38,400
We have to teach them
to make good security

416
00:19:38,466 --> 00:19:42,366
decisions for themselves
in their own interests.

417
00:19:42,433 --> 00:19:47,400
So, I don't use parental
controls at home, but my

418
00:19:47,466 --> 00:19:49,533
teenage daughter came to
me a couple months ago and

419
00:19:49,599 --> 00:19:51,733
said mom, can you help me
turn on parental controls?

420
00:19:51,799 --> 00:19:53,466
I was like, what?

421
00:19:53,533 --> 00:19:58,866
It's because she wanted
help enforcing her study time.

422
00:19:58,933 --> 00:20:00,666
So, she set up the
controls the way she

423
00:20:00,733 --> 00:20:04,133
wanted them and I put on
the password and whenever

424
00:20:04,200 --> 00:20:06,966
she wants to change them,
she comes back to me and I

425
00:20:07,033 --> 00:20:10,533
unlock it and she changes
it the way she wants to.

426
00:20:10,599 --> 00:20:13,433
We have to get them to
make their own decisions.

427
00:20:14,299 --> 00:20:15,633
(Applause)

428
00:20:15,833 --> 00:20:16,599
Thank you.

429
00:20:16,666 --> 00:20:17,533
All the teenagers.

430
00:20:17,599 --> 00:20:19,265
Yay.

431
00:20:21,133 --> 00:20:22,966
Because the other thing,
have you noticed that we

432
00:20:23,033 --> 00:20:27,832
keep making the same security mistakes year after year?

433
00:20:27,900 --> 00:20:29,299
We did it with
web servers.

434
00:20:29,366 --> 00:20:34,866
We had the same security
mistakes with mobile, with IoT.

435
00:20:34,933 --> 00:20:37,533
It's not because people
are not listening.

436
00:20:37,599 --> 00:20:39,332
It's because it is a
different demographic.

437
00:20:39,400 --> 00:20:42,033
It's a different
population writing that

438
00:20:42,099 --> 00:20:44,399
software with each
new technology.

439
00:20:44,466 --> 00:20:46,233
That means we have
to teach everybody.

440
00:20:46,299 --> 00:20:49,433
So, it doesn't matter
who comes in with new

441
00:20:49,500 --> 00:20:53,933
technology; they know how
to apply the security controls.

442
00:20:54,000 --> 00:20:57,933
Everybody can secure it
because they get it.

443
00:20:58,000 --> 00:21:02,166
It can no longer be the
province of professionals,

444
00:21:02,233 --> 00:21:05,466
analysts, vendors,
governments.

445
00:21:05,533 --> 00:21:08,832
It's got to be security
of, by and for the people.

446
00:21:08,900 --> 00:21:10,466
Even though we're the ones
who have been working on

447
00:21:10,533 --> 00:21:11,933
this for decades.

448
00:21:12,000 --> 00:21:15,233
I mean, I remember when
Spaf was still at Georgia Tech.

449
00:21:15,299 --> 00:21:18,033
This was before Cult of
the Dead Cow and this was

450
00:21:18,099 --> 00:21:19,366
before the L0pht.

451
00:21:19,433 --> 00:21:21,799
I mean, security did not
start in the 90s, people.

452
00:21:21,866 --> 00:21:23,200
I just want to
tell you that.

453
00:21:23,266 --> 00:21:26,233
And you millennials with
your bit chain and your block -

454
00:21:26,799 --> 00:21:28,299
(Laughter)

455
00:21:30,099 --> 00:21:32,133
Did somebody just
yell okay boomer?

456
00:21:32,200 --> 00:21:33,366
(Laughter)

457
00:21:33,433 --> 00:21:35,700
Pete, was that you?

458
00:21:35,766 --> 00:21:38,599
Okay, that's fair.
I am a boomer.

459
00:21:38,666 --> 00:21:43,000
And maybe I am not the
right person to take this

460
00:21:43,066 --> 00:21:45,166
on by myself because I've
been thinking the old way

461
00:21:45,233 --> 00:21:46,666
for so long.

462
00:21:46,733 --> 00:21:49,033
Corporate, authoritarian.

463
00:21:49,099 --> 00:21:51,566
This is why we need to
bring in all stakeholders

464
00:21:51,633 --> 00:21:53,466
of all ages.

465
00:21:53,533 --> 00:21:56,366
Everybody, everybody.

466
00:21:56,433 --> 00:21:58,933
So that we can
do this together.

467
00:21:59,000 --> 00:22:01,500
Now, I know this
is a lot to ask.

468
00:22:01,566 --> 00:22:04,000
It's a big, big
undertaking.

469
00:22:04,066 --> 00:22:06,066
But I believe
we can do this.

470
00:22:07,900 --> 00:22:13,500
We can start by going from
control to collaboration.

471
00:22:13,566 --> 00:22:15,900
We can simplify and
redesign the security

472
00:22:15,966 --> 00:22:18,399
controls that we are
providing to people.

473
00:22:18,466 --> 00:22:20,466
And we have to
open the culture.

474
00:22:22,766 --> 00:22:24,433
I believe that
we can do this.

475
00:22:26,500 --> 00:22:27,866
We can do this.

476
00:22:29,733 --> 00:22:31,033
Let's do this.

477
00:22:32,733 --> 00:22:34,433
(Applause)

