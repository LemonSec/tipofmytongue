1
00:00:13,300 --> 00:00:16,266
>> SPEAKER: Please welcome
Program Committee Chair RSA

2
00:00:16,332 --> 00:00:18,732
Conference Hugh Thompson.

3
00:00:20,699 --> 00:00:26,899
>> HUGH THOMPSON: Hey, everybody.
Hello. Welcome.

4
00:00:26,966 --> 00:00:31,800
Welcome to the last session
of RSA Conference 2020.

5
00:00:31,866 --> 00:00:32,933
How is everybody feeling?

6
00:00:34,466 --> 00:00:36,233
All right. Okay. Good.

7
00:00:36,299 --> 00:00:38,100
Enthusiastic response.

8
00:00:38,166 --> 00:00:40,366
Well, thanks so
much for being here.

9
00:00:40,433 --> 00:00:47,033
I know some of you have changed
flights to be here on Friday to

10
00:00:47,100 --> 00:00:51,833
watch the magic and mystery of
Penn and Teller who are here in

11
00:00:51,899 --> 00:00:54,666
the house, by the way,
just to let you know.

12
00:00:54,733 --> 00:00:58,065
And I think you
have chosen wisely.

13
00:00:58,133 --> 00:01:00,766
This is going to be an
amazing, amazing show.

14
00:01:00,833 --> 00:01:06,233
And there are some concerning
items behind us and black cloth.

15
00:01:06,299 --> 00:01:08,266
I'm not quite sure
what's going to happen.

16
00:01:08,333 --> 00:01:11,533
Our insurance adjuster is a
little worried about it, too.

17
00:01:11,599 --> 00:01:13,400
But it will be great.

18
00:01:13,466 --> 00:01:15,000
It will be great.
It will be great.

19
00:01:15,066 --> 00:01:15,866
No fire.

20
00:01:15,933 --> 00:01:17,099
I was promised.

21
00:01:17,166 --> 00:01:21,298
But, you know, if you look at
RSA Conference 2020, we've

22
00:01:21,366 --> 00:01:27,466
really tried to focus it around
this idea of the human element.

23
00:01:27,533 --> 00:01:28,933
And you've seen it everywhere.

24
00:01:29,000 --> 00:01:30,133
You've seen the posters.

25
00:01:30,200 --> 00:01:33,700
You've seen it as a theme
in a lot of the talks.

26
00:01:33,766 --> 00:01:38,133
And it's one of the greatest
unsolved mysteries, I would say,

27
00:01:38,200 --> 00:01:41,966
of the security space in
a bunch of different ways.

28
00:01:42,033 --> 00:01:44,733
One is we need humanity.

29
00:01:44,799 --> 00:01:49,200
We need diversity of thought and
views and perspectives to be

30
00:01:49,266 --> 00:01:50,799
able to deal with
the security problem.

31
00:01:50,866 --> 00:01:51,933
That's one.

32
00:01:52,000 --> 00:01:55,566
We've had a lot of sessions
and workshops focused on that.

33
00:01:55,633 --> 00:02:02,500
We also need to understand that
our users are very diverse and

34
00:02:02,566 --> 00:02:06,233
have certain ways of thinking
and we need to know how they're

35
00:02:06,299 --> 00:02:10,666
going to assess risk and look at
a problem, but we also need to

36
00:02:10,733 --> 00:02:14,933
understand what our
limitations are as people.

37
00:02:15,900 --> 00:02:18,366
I'll give you an example.

38
00:02:18,433 --> 00:02:22,799
Somebody a long time ago handed
me what was the best selling,

39
00:02:22,866 --> 00:02:25,433
I think, self-help
book of all time.

40
00:02:25,500 --> 00:02:26,733
Some of you may have read it.

41
00:02:26,800 --> 00:02:28,933
How to Win Friends
and Influence People.

42
00:02:29,000 --> 00:02:31,466
I don't know if you've
ever seen this book.

43
00:02:31,533 --> 00:02:32,132
You've seen it.

44
00:02:32,199 --> 00:02:34,566
Some of you have seen it.

45
00:02:34,633 --> 00:02:37,166
I'm reading this thing
and I'm like, okay, that's

46
00:02:37,233 --> 00:02:38,166
really interesting.

47
00:02:38,233 --> 00:02:40,699
It was written like 80 years
ago, or 80+ years ago.

48
00:02:40,766 --> 00:02:43,533
I'm like, well, a little dated
examples, interesting, okay,

49
00:02:43,599 --> 00:02:46,599
that's interesting, and it says
things like always call people

50
00:02:46,666 --> 00:02:49,966
by their first name and
give sincere compliments.

51
00:02:50,033 --> 00:02:53,766
I look at this and I'm like
this is a social engineering

52
00:02:53,833 --> 00:02:56,099
handbook. Right?

53
00:02:56,166 --> 00:02:59,698
This is exactly what you
should do in a phishing email,

54
00:02:59,766 --> 00:03:00,833
for example.

55
00:03:00,900 --> 00:03:06,933
And you look at this stuff, and
what's amazing about it is that

56
00:03:07,000 --> 00:03:12,000
book is still as relevant today
as it was 80 years ago, and it

57
00:03:12,066 --> 00:03:15,033
will probably still be as
relevant 80 years from now

58
00:03:15,099 --> 00:03:22,465
because there are a set of call
them zero days in people that,

59
00:03:22,533 --> 00:03:26,266
on the one hand, we love those
zero days, on the other hand,

60
00:03:26,333 --> 00:03:28,666
they're really, really
difficult to fix.

61
00:03:28,733 --> 00:03:32,500
And we've all got those, but
we've also got some unique ones

62
00:03:32,566 --> 00:03:34,233
that are specific to us.

63
00:03:34,300 --> 00:03:39,766
To start off, I'd love to share
a personal security story with

64
00:03:39,833 --> 00:03:42,766
you related to one of my own.

65
00:03:42,833 --> 00:03:47,566
I'd say one of my biggest
weaknesses is puzzles.

66
00:03:47,633 --> 00:03:52,766
If somebody presents or hands me
the most complicated messed up

67
00:03:52,833 --> 00:03:57,233
puzzle I've ever seen, I just
can't help but get into it,

68
00:03:57,300 --> 00:04:03,066
right, 72 hours, no sleep,
I just have to get it done.

69
00:04:04,199 --> 00:04:09,800
I made the tragic mistake of
having a conversation 10 years

70
00:04:09,866 --> 00:04:14,666
ago with a colleague of mine, I
was a professor at the time, and

71
00:04:14,733 --> 00:04:17,733
he mentioned to me just really,
really casually, like over

72
00:04:17,800 --> 00:04:24,066
lunch, hey, have you heard about
this extreme couponing thing?

73
00:04:25,133 --> 00:04:26,866
I said, no, no, no.

74
00:04:26,933 --> 00:04:28,866
I haven't.

75
00:04:28,933 --> 00:04:32,433
And this is where I went wrong
and said can you tell me more

76
00:04:32,500 --> 00:04:35,100
about that. All right?

77
00:04:35,166 --> 00:04:38,433
How many people here have
experienced, heard of, or

78
00:04:38,500 --> 00:04:40,866
participated in extreme
couponing, just out

79
00:04:40,933 --> 00:04:41,899
of curiosity?

80
00:04:41,966 --> 00:04:43,133
Okay. Oh, wow.

81
00:04:43,199 --> 00:04:47,399
Even an actual
yell of endorsement.

82
00:04:47,466 --> 00:04:53,866
For the uninitiated, you may
be familiar with couponing, in

83
00:04:53,933 --> 00:04:55,166
general. Right?

84
00:04:55,233 --> 00:04:59,833
You know those coupons that you
see in the newspaper, $3 off of

85
00:04:59,899 --> 00:05:02,500
a tube of toothpaste type of
thing if you bring it into

86
00:05:02,566 --> 00:05:04,032
the store.

87
00:05:04,100 --> 00:05:08,966
There is a whole cult around
this thing called extreme

88
00:05:09,033 --> 00:05:15,300
couponing, which is taking that
coupon, but then hunting for

89
00:05:15,366 --> 00:05:20,899
other places that have other
coupons that intersect with that

90
00:05:20,966 --> 00:05:25,199
coupon that have
unexpected consequences.

91
00:05:25,266 --> 00:05:28,500
Like, for example, imagine if
you have a coupon that's like,

92
00:05:28,566 --> 00:05:33,599
okay, I'm going to give you
20% of a Listerine bottle.

93
00:05:33,666 --> 00:05:34,466
Right?

94
00:05:34,533 --> 00:05:37,366
Good coupon to have,
especially in these times.

95
00:05:37,433 --> 00:05:43,199
And then you get another
coupon that says $3 off a

96
00:05:43,266 --> 00:05:45,199
Listerine bottle.

97
00:05:45,266 --> 00:05:50,899
Now if you combine those two
coupons with a special that is

98
00:05:50,966 --> 00:05:56,066
currently going on at your
favorite drugstore or maybe a

99
00:05:56,133 --> 00:06:05,265
food store, you can get the
cashier to give you say $1 for

100
00:06:05,333 --> 00:06:08,233
taking that Listerine. Right?

101
00:06:08,300 --> 00:06:10,500
It's unbelievable. Right?

102
00:06:10,566 --> 00:06:13,433
And the concept of
it was so intriguing.

103
00:06:13,500 --> 00:06:20,733
I started to build these
Excel models and tabs.

104
00:06:20,800 --> 00:06:23,000
And my PhD is in statistics.

105
00:06:23,066 --> 00:06:27,732
I started to run these Monte
Carlo simulations on if in this

106
00:06:27,800 --> 00:06:33,866
week's circular there is a Crest
coupon, I am set for life.

107
00:06:33,933 --> 00:06:35,800
The kids are, you know.

108
00:06:35,866 --> 00:06:42,032
And so I prepared for my first
what's called the shopping trip

109
00:06:42,100 --> 00:06:46,500
in the nomenclature of extreme
couponing, and it's where you've

110
00:06:46,566 --> 00:06:49,666
done all your prep, you've
gathered all your coupons,

111
00:06:49,733 --> 00:06:52,533
you've checked the date to make
sure it intersects with all

112
00:06:52,600 --> 00:06:53,933
the coupons.

113
00:06:54,000 --> 00:06:58,433
You show up at the store and
you buy the most concerning

114
00:06:58,500 --> 00:07:03,766
collection of things you've
ever seen in your entire life.

115
00:07:03,833 --> 00:07:10,199
The stuff that you buy on
one trip is called the haul.

116
00:07:10,266 --> 00:07:11,066
Right?

117
00:07:11,133 --> 00:07:12,832
I just want to show
you a quick example.

118
00:07:12,899 --> 00:07:16,300
I think we have a picture
of a haul, a typical haul.

119
00:07:16,366 --> 00:07:17,466
Right?

120
00:07:17,533 --> 00:07:23,166
And you'll notice some eclectic
items in here, probably more

121
00:07:23,233 --> 00:07:25,833
vitamins than you ever needed.

122
00:07:25,899 --> 00:07:29,800
There are some like a bag of
Ritz, some really concerning

123
00:07:29,866 --> 00:07:31,832
stuff up at the top.

124
00:07:31,899 --> 00:07:36,266
And so when I went in and did my
first shopping trip, it was in

125
00:07:36,333 --> 00:07:39,600
New York and it was very cold,
so I went to the Rite Aid that

126
00:07:39,666 --> 00:07:43,933
was maybe two minutes away from
where we were and I bought - let

127
00:07:44,000 --> 00:07:50,466
me see if I get this right - 25
bottles of women's facial hair

128
00:07:50,533 --> 00:07:51,666
remover. Right?

129
00:07:51,733 --> 00:07:55,000
That was amazing
coupons on that thing.

130
00:07:55,066 --> 00:07:56,666
Unbelievable.

131
00:07:56,733 --> 00:08:05,600
Seventy-five toothbrushes, 150
tubes of toothpaste because it

132
00:08:05,666 --> 00:08:08,500
was a combo coupon,
which is very rare.

133
00:08:08,566 --> 00:08:11,066
If you run across that,
treasure that coupon.

134
00:08:11,133 --> 00:08:12,666
And so I end up with all this.

135
00:08:12,733 --> 00:08:16,033
I had a charcoal lighter
and crazy things.

136
00:08:16,100 --> 00:08:20,899
And so I get up to the counter,
I pull out this massive stash

137
00:08:20,966 --> 00:08:21,633
of coupons.

138
00:08:21,699 --> 00:08:22,866
It's color-coded.

139
00:08:22,933 --> 00:08:29,566
The expression that the lady at
the checkout gave me, I will

140
00:08:29,633 --> 00:08:31,233
never forget it.

141
00:08:31,300 --> 00:08:32,765
I will really never forget it.

142
00:08:32,832 --> 00:08:35,799
So, anyway, an hour and a half
later, as we kind of like are

143
00:08:35,866 --> 00:08:40,899
checked out of this thing, I
take all this stuff, I take the

144
00:08:40,966 --> 00:08:44,899
shopping cart, and they let me
borrow it, and said as long as

145
00:08:44,966 --> 00:08:49,399
you promise never to come back,
I brought this stuff back home,

146
00:08:49,466 --> 00:08:53,200
and I was so excited to show
this stuff to my wife, right,

147
00:08:53,266 --> 00:08:56,799
because I'm the hunter-gatherer
and I've, you know, gone to the

148
00:08:56,866 --> 00:09:00,733
dangerous Rite Aid and come
back with all this stuff.

149
00:09:00,799 --> 00:09:02,199
And I'm laying it all out.

150
00:09:02,266 --> 00:09:08,565
We're in a New York apartment,
not known for its spacious

151
00:09:08,633 --> 00:09:10,700
places to put stuff.

152
00:09:10,766 --> 00:09:15,799
And she looks at this stuff and
she's like you can never touch a

153
00:09:15,866 --> 00:09:18,165
coupon again.

154
00:09:18,233 --> 00:09:20,833
That was the end of it.

155
00:09:20,899 --> 00:09:24,166
We tried to give those things
away like instead of wine when

156
00:09:24,233 --> 00:09:27,132
we went to people's houses.

157
00:09:27,200 --> 00:09:28,799
Alienated a lot of
people on the way.

158
00:09:28,866 --> 00:09:35,033
But the point of it is if
somebody knew that about me,

159
00:09:35,100 --> 00:09:40,233
they would use the fact that I'm
so compelled to solve a puzzle,

160
00:09:40,299 --> 00:09:43,833
they would use that
as a way to get me.

161
00:09:43,899 --> 00:09:46,700
They would use that as a way to
get me to click on something.

162
00:09:46,766 --> 00:09:51,033
They would use me to get that
- to get me to show up at

163
00:09:51,100 --> 00:09:52,466
an event.

164
00:09:52,533 --> 00:09:57,233
And it's this uniqueness about
us that, on the one hand, are so

165
00:09:57,299 --> 00:10:03,366
great, on the other hand, create
opportunities for attackers

166
00:10:03,433 --> 00:10:04,833
to leverage.

167
00:10:04,899 --> 00:10:07,266
That's what we're going to
explore during this final

168
00:10:07,333 --> 00:10:11,500
session in a very unique way.

169
00:10:11,566 --> 00:10:14,399
We've got Penn and Teller
here, as I've mentioned. Right?

170
00:10:14,466 --> 00:10:19,000
Magic, I think, is one of the
ultimate manifestations of

171
00:10:19,066 --> 00:10:21,600
looking at these
kinds of vulnerabilities and

172
00:10:21,666 --> 00:10:23,933
leveraging them.

173
00:10:24,000 --> 00:10:27,733
But we've also had some really,
really bright people that have

174
00:10:27,799 --> 00:10:31,100
been studying this
problem for a long time.

175
00:10:31,166 --> 00:10:36,799
My first guest is one of the
world's leading experts on the

176
00:10:36,866 --> 00:10:42,299
intersection of security and
privacy and the human element.

177
00:10:42,366 --> 00:10:46,366
Please join me in welcoming
Dr. Lorrie Cranor.

178
00:10:51,766 --> 00:10:54,665
Hey, Lori.
Thanks so much for coming.

179
00:10:54,733 --> 00:10:57,599
Have a seat.

180
00:10:57,666 --> 00:10:58,866
Thanks for being here.

181
00:10:58,933 --> 00:11:00,600
>> DR. LORRIE CRANOR:
Thank you. This is fun.

182
00:11:00,666 --> 00:11:04,100
>> HUGH THOMPSON: And I should
have asked you this backstage.

183
00:11:04,166 --> 00:11:05,833
Don't read anything into it.

184
00:11:05,899 --> 00:11:12,766
But I was asked to ask you if
you would be open or have ever

185
00:11:12,833 --> 00:11:22,966
considered lying in a long
box and being sawed in half.

186
00:11:24,433 --> 00:11:28,799
It could be completely unrelated
to the show, but I promised

187
00:11:28,866 --> 00:11:30,199
I would.

188
00:11:30,266 --> 00:11:31,733
>> DR. LORRIE CRANOR: Yeah, that
would be a new thing for me.

189
00:11:31,799 --> 00:11:32,933
>> HUGH THOMPSON:
Okay. All right. Okay.

190
00:11:33,000 --> 00:11:35,200
I'll take that as a hard maybe.

191
00:11:35,266 --> 00:11:36,333
>> DR. LORRIE CRANOR: Maybe.

192
00:11:36,399 --> 00:11:38,433
>> HUGH THOMPSON: Thanks
so much for being here.

193
00:11:38,500 --> 00:11:40,566
I am such a big fan of yours.

194
00:11:40,633 --> 00:11:43,799
I've been reading your papers
for a really, really long time.

195
00:11:43,866 --> 00:11:45,533
You and I have been
friends for a long time.

196
00:11:45,600 --> 00:11:47,133
>> DR. LORRIE CRANOR:
Thank you. Thank you. Yeah.

197
00:11:47,200 --> 00:11:49,799
>> HUGH THOMPSON: I would argue,
some of the work that you have

198
00:11:49,866 --> 00:11:53,633
done is going to set the
foundation for the biggest

199
00:11:53,700 --> 00:11:58,200
problems that we face now
in information security.

200
00:11:58,266 --> 00:12:01,833
I wanted to ask you about some
of the research, especially

201
00:12:01,899 --> 00:12:03,433
around passwords.

202
00:12:03,500 --> 00:12:04,700
>> DR. LORRIE CRANOR: Okay.

203
00:12:04,766 --> 00:12:07,065
>> HUGH THOMPSON: There has been
a lot of discussion every year,

204
00:12:07,133 --> 00:12:10,000
I guess, since the beginning of
RSA conference about passwords

205
00:12:10,066 --> 00:12:11,966
and why do we still have them.

206
00:12:12,033 --> 00:12:14,766
There are probably 3,000 vendors
on the show floor that we'll get

207
00:12:14,833 --> 00:12:17,433
rid of them for you potentially.

208
00:12:17,500 --> 00:12:23,133
But you've done a lot of work on
how people choose passwords and

209
00:12:23,200 --> 00:12:24,966
what is a good or bad password.

210
00:12:25,033 --> 00:12:28,666
I wonder if you could share some
of that with us and maybe some

211
00:12:28,733 --> 00:12:29,899
things that surprised you.

212
00:12:29,966 --> 00:12:31,266
>> DR. LORRIE CRANOR:
Sure. Yeah.

213
00:12:31,333 --> 00:12:33,833
I've been working with students
and my colleagues at Carnegie

214
00:12:33,899 --> 00:12:37,533
Mellon on passwords for
about ten years now.

215
00:12:37,600 --> 00:12:41,566
We've been very interested in
how people choose passwords.

216
00:12:41,633 --> 00:12:45,166
We do studies with users
to see what they do.

217
00:12:45,233 --> 00:12:48,833
One of the things that we do
is we give people pairs of

218
00:12:48,899 --> 00:12:53,333
passwords and we ask them to
tell us which one is better or

219
00:12:53,399 --> 00:12:55,233
are they basically the same.

220
00:12:55,299 --> 00:12:59,533
We've had some really
interesting results here.

221
00:12:59,600 --> 00:13:02,033
Maybe I can try
with the audience?

222
00:13:02,100 --> 00:13:03,799
>> HUGH THOMPSON:
Yeah, go for it. Go for it.

223
00:13:03,866 --> 00:13:05,566
Yeah. Yeah. This is good.

224
00:13:05,633 --> 00:13:06,566
>> DR. LORRIE CRANOR: All right.

225
00:13:06,633 --> 00:13:08,899
Well, you all are pretty smart.

226
00:13:08,966 --> 00:13:11,666
Here's a pair of passwords.

227
00:13:11,733 --> 00:13:17,465
One password is I love you 88,
and the other password is I eat

228
00:13:17,533 --> 00:13:19,266
kale 88.

229
00:13:19,333 --> 00:13:22,733
They both have the same number
of lowercase letters and an 88

230
00:13:22,799 --> 00:13:24,233
at the end.

231
00:13:24,299 --> 00:13:27,066
We have to decide which one
is better or are they equal.

232
00:13:27,133 --> 00:13:31,066
Raise your hands for I
love you 88 is better.

233
00:13:31,133 --> 00:13:32,200
Okay.
Nobody is raising their hand.

234
00:13:32,266 --> 00:13:33,000
Oh, no.

235
00:13:33,066 --> 00:13:33,666
No, wait.

236
00:13:33,733 --> 00:13:34,465
Oh, okay.

237
00:13:34,533 --> 00:13:35,666
I got a few. I got a few.

238
00:13:35,733 --> 00:13:36,632
All right.

239
00:13:36,700 --> 00:13:39,633
I eat kale 88 is better. Okay.

240
00:13:39,700 --> 00:13:42,233
And they're equal.

241
00:13:42,299 --> 00:13:43,566
There are a lot of equals.

242
00:13:43,633 --> 00:13:44,600
Okay.

243
00:13:44,666 --> 00:13:47,433
When we do our user study, most
people think they're equal

244
00:13:47,500 --> 00:13:51,733
because same number of lowercase
letters, same 88 at the end.

245
00:13:51,799 --> 00:13:55,799
It turns out I eat kale 88 is
way, way, way better, like four

246
00:13:55,866 --> 00:13:57,500
trillion times better.

247
00:13:57,566 --> 00:14:02,333
And the reason for that is that
I love you is one of the most

248
00:14:02,399 --> 00:14:07,433
common passwords that people
use, and I eat kale, nobody

249
00:14:07,500 --> 00:14:08,100
eats kale.

250
00:14:08,166 --> 00:14:10,299
>> HUGH THOMPSON:
Wow. Wow. Geez.

251
00:14:10,366 --> 00:14:12,633
>> DR. LORRIE CRANOR: And my
kids - I eat kale, but my kids

252
00:14:12,700 --> 00:14:13,633
do not.

253
00:14:13,700 --> 00:14:14,966
They're like, Mom,
no one eats kale.

254
00:14:15,033 --> 00:14:18,399
So, yeah. It surprises people.

255
00:14:18,466 --> 00:14:20,733
>> HUGH THOMPSON: There's a lot
of lessons you could take away

256
00:14:20,799 --> 00:14:22,966
from this. Right?

257
00:14:23,033 --> 00:14:27,100
On the one hand, it's a good
sign for humanity that many

258
00:14:27,166 --> 00:14:31,500
people are choosing the I love
you passwords, but it also

259
00:14:31,566 --> 00:14:34,899
reinforces, well, my mom has
always said about eat your

260
00:14:34,966 --> 00:14:37,033
vegetables and
they're good for you.

261
00:14:37,100 --> 00:14:40,366
I never heard it
expressed in passwords.

262
00:14:40,433 --> 00:14:43,399
But this - yeah, that's
interesting because it's not

263
00:14:43,466 --> 00:14:47,266
intuitive that one would be
better than the other just

264
00:14:47,333 --> 00:14:51,799
because - especially the way
that current systems judge

265
00:14:51,866 --> 00:14:53,899
password strength. Right?

266
00:14:53,966 --> 00:14:57,866
It tends to be this checklist of
do you have upper and lowercase,

267
00:14:57,933 --> 00:14:59,633
is there a special symbol.

268
00:14:59,700 --> 00:15:02,700
And so I'm curious. Okay.

269
00:15:02,766 --> 00:15:05,165
What other ones have you tried?

270
00:15:05,233 --> 00:15:06,532
Give us another chance.
Give us another chance.

271
00:15:06,600 --> 00:15:07,533
>> DR. LORRIE CRANOR:
Okay. Okay. I'll give you

272
00:15:07,600 --> 00:15:08,466
another chance.

273
00:15:08,533 --> 00:15:09,399
>> HUGH THOMPSON:
What have you got?

274
00:15:09,466 --> 00:15:15,033
So we've got SpongeBob01
and Sponge01Bob.

275
00:15:15,100 --> 00:15:15,933
>> HUGH THOMPSON: Okay.

276
00:15:16,000 --> 00:15:16,733
>> DR. LORRIE CRANOR: Okay?

277
00:15:16,799 --> 00:15:17,733
>> HUGH THOMPSON: Both
beloved characters.

278
00:15:17,799 --> 00:15:19,066
>> DR. LORRIE CRANOR:
Yeah. Yeah.

279
00:15:19,133 --> 00:15:20,533
And same letters and numbers.

280
00:15:20,600 --> 00:15:24,766
Okay. So raise your hand for
SpongeBob01 is better.

281
00:15:24,833 --> 00:15:29,366
Okay. Raise your hand for
Sponge01Bob is better.

282
00:15:29,433 --> 00:15:32,266
And raise your hand
for they are the same.

283
00:15:32,333 --> 00:15:33,565
All right.

284
00:15:33,633 --> 00:15:35,933
You all actually did
pretty well on this one.

285
00:15:36,000 --> 00:15:40,500
Sponge01Bob is by far
better than SpongeBob01.

286
00:15:40,566 --> 00:15:43,866
And this is something that's not
obvious to people who are not in

287
00:15:43,933 --> 00:15:45,066
the security field.

288
00:15:45,133 --> 00:15:48,633
The reason it's better is that
most people put their numbers at

289
00:15:48,700 --> 00:15:50,299
the end of the password.

290
00:15:50,366 --> 00:15:54,466
Just by moving your numbers into
the middle, you have a much

291
00:15:54,533 --> 00:15:55,733
stronger password.

292
00:15:55,799 --> 00:15:58,733
And the same thing goes for
punctuation, capital letters,

293
00:15:58,799 --> 00:15:59,833
whatever.

294
00:15:59,899 --> 00:16:01,299
>> HUGH THOMPSON: I can't
believe you get to do stuff like

295
00:16:01,366 --> 00:16:02,500
this all day.

296
00:16:02,566 --> 00:16:05,066
Like, guys, which one is
better, this one or that one?

297
00:16:05,133 --> 00:16:08,533
Well, let me ask you, you know,
I know you've also done a lot of

298
00:16:08,600 --> 00:16:13,399
work on how people make good
choices around security,

299
00:16:13,466 --> 00:16:16,733
specifically on IoT devices.

300
00:16:16,799 --> 00:16:21,033
You've been working on this
IoT labeling for a long time.

301
00:16:21,100 --> 00:16:23,133
What have you learned from that?

302
00:16:23,200 --> 00:16:28,399
Is it possible to communicate
to the average person through

303
00:16:28,466 --> 00:16:34,000
something like a food nutrition
label of how safe is a device,

304
00:16:34,066 --> 00:16:35,500
and do they even care?

305
00:16:35,566 --> 00:16:36,666
>> DR. LORRIE CRANOR:
Yeah. Yeah.

306
00:16:36,733 --> 00:16:40,333
We've been designing this IoT
security and privacy label and

307
00:16:40,399 --> 00:16:44,600
it looks like a nutrition label,
and we've been testing it with

308
00:16:44,666 --> 00:16:48,166
people to find out whether it
actually makes sense to them.

309
00:16:48,233 --> 00:16:50,433
You can check this
out on our website.

310
00:16:50,500 --> 00:16:54,066
It's IoTSecurityPrivacy.org.

311
00:16:54,133 --> 00:16:57,966
When we've done studies, we've
shown people some of the

312
00:16:58,033 --> 00:17:01,200
ingredients on the label and
we've asked them whether they

313
00:17:01,266 --> 00:17:05,066
think that this particular item
on the label makes the product

314
00:17:05,133 --> 00:17:09,066
more risky or less risky and
whether they'd be more likely to

315
00:17:09,133 --> 00:17:11,833
buy it or less likely to buy it.

316
00:17:11,900 --> 00:17:15,733
And what we've found is that for
most of our ingredients, most

317
00:17:15,799 --> 00:17:19,098
people, when we ask them about
risk, they actually are going in

318
00:17:19,165 --> 00:17:20,265
the right direction.

319
00:17:20,333 --> 00:17:24,366
They understand which things
increase risk, which things

320
00:17:24,433 --> 00:17:25,133
decrease risk.

321
00:17:25,200 --> 00:17:28,200
This is good.

322
00:17:28,266 --> 00:17:30,933
But when we asked them whether
it would increase their

323
00:17:31,000 --> 00:17:35,833
likelihood of purchasing, we see
that for some things when risk

324
00:17:35,900 --> 00:17:39,200
is decreased, they are more
likely to purchase, which is

325
00:17:39,266 --> 00:17:42,200
good, but sometimes
we don't see that.

326
00:17:42,266 --> 00:17:45,766
One of the reasons we don't see
that is people will say, well, I

327
00:17:45,833 --> 00:17:49,700
know it decreases risk, but it
seems awfully inconvenient.

328
00:17:49,766 --> 00:17:54,166
So, for example, if we say this
smart speaker has multi-factor

329
00:17:54,233 --> 00:17:57,700
authentication, they say, great,
it decreases risk, but that's

330
00:17:57,766 --> 00:17:59,966
inconvenient, I won't buy it.

331
00:18:00,033 --> 00:18:01,265
>> HUGH THOMPSON:
Wow. Interesting.

332
00:18:01,333 --> 00:18:02,533
>> DR. LORRIE CRANOR: Yeah.

333
00:18:02,599 --> 00:18:06,832
>> HUGH THOMPSON: And does it
vary based on the type of thing?

334
00:18:06,900 --> 00:18:10,799
I can imagine a smart speaker,
although I would say that that's

335
00:18:10,866 --> 00:18:12,266
not a good choice.

336
00:18:12,333 --> 00:18:18,099
But is it different if it's a
chainsaw versus - like it's the

337
00:18:18,166 --> 00:18:21,700
internet-enabled chainsaw
versus it's some innocuous thing

338
00:18:21,766 --> 00:18:24,133
like that?

339
00:18:24,200 --> 00:18:25,466
>> DR. LORRIE CRANOR: Yeah.
It does make a difference.

340
00:18:25,533 --> 00:18:27,166
We have not
touched the chainsaws.

341
00:18:27,233 --> 00:18:28,500
>> HUGH THOMPSON: Well,
there is a market.

342
00:18:28,566 --> 00:18:29,366
There is a market.

343
00:18:29,433 --> 00:18:30,799
>> DR. LORRIE CRANOR:
Maybe we should. Yeah.

344
00:18:30,866 --> 00:18:34,433
We did try smart - smart
toothbrushes, smart light bulbs.

345
00:18:34,500 --> 00:18:37,366
Definitely, like smart light
bulbs raise fewer concerns for

346
00:18:37,433 --> 00:18:41,099
people than smart
speakers, for example.

347
00:18:41,166 --> 00:18:43,399
>> HUGH THOMPSON: Again, just
out of curiosity, what would be

348
00:18:43,466 --> 00:18:47,599
the key things people are
worried about on a smart

349
00:18:47,666 --> 00:18:50,000
toothbrush, only just
because you brought up a smart

350
00:18:50,066 --> 00:18:51,666
toothbrush and I can't.

351
00:18:51,733 --> 00:18:53,466
>> DR. LORRIE CRANOR: Well, we
actually found that most people

352
00:18:53,533 --> 00:18:55,799
don't believe
they actually exist.

353
00:18:55,866 --> 00:18:56,833
They do exist.

354
00:18:56,900 --> 00:18:57,866
>> HUGH THOMPSON: They do exist.

355
00:18:57,933 --> 00:18:59,133
>> DR. LORRIE CRANOR: But
people don't believe it.

356
00:18:59,200 --> 00:19:01,433
And people don't understand what
they should be worried about.

357
00:19:01,500 --> 00:19:05,733
Whereas I've actually seen these
things and they collect data

358
00:19:05,799 --> 00:19:08,500
about your brushing habits and
your gum health and they can

359
00:19:08,566 --> 00:19:11,733
actually wire it to your dental
insurance company, which sounds

360
00:19:11,799 --> 00:19:13,166
kind of scary to me.

361
00:19:13,233 --> 00:19:18,533
>> HUGH THOMPSON: If there is a
breach of toothbrush data, is

362
00:19:18,599 --> 00:19:21,599
that considered PII?

363
00:19:21,666 --> 00:19:22,733
>> DR. LORRIE CRANOR:
Yeah, I think so.

364
00:19:22,799 --> 00:19:25,966
>> HUGH THOMPSON: Sorry. Okay,
this is just blowing my mind.

365
00:19:26,033 --> 00:19:27,000
>> DR. LORRIE CRANOR:
Yeah. Yeah.

366
00:19:27,066 --> 00:19:30,033
>> HUGH THOMPSON: Well,
let me ask you this.

367
00:19:30,099 --> 00:19:35,799
It's hard enough for people to
make good security and usability

368
00:19:35,866 --> 00:19:38,233
trade-offs with risk as it is.

369
00:19:38,299 --> 00:19:41,166
And something that you pointed
out many times in your papers is

370
00:19:41,233 --> 00:19:44,733
that often it's people that
don't want to do something

371
00:19:44,799 --> 00:19:48,700
malicious; it's just they want
to get their work done and maybe

372
00:19:48,766 --> 00:19:51,166
it's easier to get their work
done through this thing that

373
00:19:51,233 --> 00:19:54,599
doesn't have a security
control getting in the way.

374
00:19:54,666 --> 00:19:58,500
But I've got to ask you, you
know, we're in a time where it

375
00:19:58,566 --> 00:20:06,200
is so easy for people to create
artifacts that look so credible

376
00:20:06,266 --> 00:20:07,500
and so real.

377
00:20:07,566 --> 00:20:11,066
If you look at some of the deep
fake video, deep fake audio that

378
00:20:11,133 --> 00:20:16,166
we've seen recently, do you
think that it is possible for

379
00:20:16,233 --> 00:20:21,666
people to develop an immunity
to that kind of stuff?

380
00:20:21,733 --> 00:20:29,099
How much at risk are we because
these tools are now free and

381
00:20:29,166 --> 00:20:33,799
they're available and you can
run a GAN on a MacBook Pro for

382
00:20:33,866 --> 00:20:37,066
ten hours and you get a
pretty good fake video.

383
00:20:37,133 --> 00:20:39,533
>> DR. LORRIE CRANOR: Yeah.
I think we definitely are at risk.

384
00:20:39,599 --> 00:20:44,765
And it is hard for people to
kind of take a step back and

385
00:20:44,833 --> 00:20:47,866
say, well, wait a minute,
is this really real or not.

386
00:20:47,933 --> 00:20:51,700
You don't even have to do
anything as fancy as fake

387
00:20:51,766 --> 00:20:53,000
a video.

388
00:20:53,066 --> 00:20:56,799
We're faking emails in just
plain text and people are

389
00:20:56,866 --> 00:20:57,533
falling for it.

390
00:20:57,599 --> 00:21:01,533
>> HUGH THOMPSON: XSTP. XSTP.

391
00:21:01,599 --> 00:21:03,633
>> DR. LORRIE CRANOR: Yeah.
So I think this is a

392
00:21:03,700 --> 00:21:05,200
difficult problem.

393
00:21:05,266 --> 00:21:08,233
>> HUGH THOMPSON: And have you
seen any of these indicators

394
00:21:08,299 --> 00:21:12,833
where just over time there
really has been improvement in

395
00:21:12,900 --> 00:21:14,000
the populous?

396
00:21:14,066 --> 00:21:19,366
Is there one type of scam that
was horrible, a plague on IT

397
00:21:19,433 --> 00:21:23,700
that we got immune
to as a collective?

398
00:21:23,766 --> 00:21:25,233
Give me some hope.

399
00:21:25,299 --> 00:21:26,433
I'm looking for hope.

400
00:21:26,500 --> 00:21:27,566
>> DR. LORRIE CRANOR:
Yeah. Yeah.

401
00:21:27,633 --> 00:21:30,466
Well, I think we keep seeing the
same scams over and over again.

402
00:21:30,533 --> 00:21:33,599
But we are getting a bit
better at looking out for just

403
00:21:33,666 --> 00:21:35,233
run-of-the-mill
phishing attacks.

404
00:21:35,299 --> 00:21:38,099
I think people are getting
better at detecting those.

405
00:21:38,166 --> 00:21:40,899
>> HUGH THOMPSON: Okay.
Moderate hope. Moderate hope.

406
00:21:40,966 --> 00:21:42,366
Lorrie, thank you so much.

407
00:21:42,433 --> 00:21:43,466
>> DR. LORRIE CRANOR:
All right. Thank you.

408
00:21:43,533 --> 00:21:44,832
>> HUGH THOMPSON:
Thanks for being here.

