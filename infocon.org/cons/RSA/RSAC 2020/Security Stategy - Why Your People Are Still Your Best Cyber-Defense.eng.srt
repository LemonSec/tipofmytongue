1
00:00:07,966 --> 00:00:10,266
Hydro
is a global aluminum producer.

2
00:00:10,266 --> 00:00:12,266
We are 36,000 employees.

3
00:00:12,333 --> 00:00:15,833
We do mining, melting,
forming, rolling,

4
00:00:15,900 --> 00:00:18,033
all over the world.

5
00:00:18,100 --> 00:00:20,733
It was very early in the
morning and I got a phone

6
00:00:20,800 --> 00:00:23,433
call that something
seriously was going wrong.

7
00:00:23,500 --> 00:00:25,666
We couldn't open
files anymore.

8
00:00:25,733 --> 00:00:28,733
And instead of opening
documents, we would see a

9
00:00:28,800 --> 00:00:32,466
ransom note saying that
the systems were encrypted.

10
00:00:32,533 --> 00:00:37,166
The first stage is to
see us as a victim.

11
00:00:37,233 --> 00:00:41,965
People started to come to
work and be motivated to

12
00:00:42,033 --> 00:00:44,033
really deal with
the challenge.

13
00:00:45,533 --> 00:00:47,100
>> CONSTANCE HOURIE:
Nunavut is a territory of

14
00:00:47,166 --> 00:00:48,933
25 communities.

15
00:00:49,000 --> 00:00:50,933
It's very large,
very remote.

16
00:00:51,000 --> 00:00:53,133
>> DEAN WELLS: The
government services are in

17
00:00:53,200 --> 00:00:55,899
all communities, and we
do educational services,

18
00:00:55,966 --> 00:00:59,266
health services, financial
across the territory.

19
00:00:59,333 --> 00:01:01,600
So at about 20 after eight
in the morning, I got a call.

20
00:01:01,666 --> 00:01:03,632
We've lost our
financial services.

21
00:01:03,700 --> 00:01:05,666
We've lost our health
services, the medivacs.

22
00:01:05,733 --> 00:01:07,000
These are communities
that are remote.

23
00:01:07,066 --> 00:01:08,599
There's no doctors
in a lot of them.

24
00:01:08,666 --> 00:01:10,899
And in a matter of a few
hours, it took out all of

25
00:01:10,966 --> 00:01:13,166
our workstations, all of
our servers, our phone

26
00:01:13,233 --> 00:01:14,666
systems, right across
the territory and

27
00:01:14,733 --> 00:01:16,533
all 25 communities.

28
00:01:16,599 --> 00:01:18,166
>> CONSTANCE HOURIE: It
shut down government.

29
00:01:18,233 --> 00:01:19,333
We had no phones.

30
00:01:19,400 --> 00:01:21,299
We had no computers.

31
00:01:21,366 --> 00:01:22,533
Everybody wanted to help.

32
00:01:22,599 --> 00:01:25,033
But we started off, I
think, with 12 people, and

33
00:01:25,099 --> 00:01:27,099
it got to 100 people, and
then pretty soon we had

34
00:01:27,166 --> 00:01:28,933
120 people in the room.

35
00:01:29,000 --> 00:01:30,599
>> DEAN WELLS: Hundreds of
people, they just wanted

36
00:01:30,666 --> 00:01:33,033
to help, right
across the territory.

37
00:01:33,099 --> 00:01:34,533
>> CONSTANCE HOURIE: In an
emergency like that, my

38
00:01:34,599 --> 00:01:38,766
most impactful memory was
the people, the teams.

39
00:01:38,833 --> 00:01:40,900
>> JO DE VLIEGHER: To make
sure that people were

40
00:01:40,966 --> 00:01:43,265
motivated, we had
to be transparent.

41
00:01:43,333 --> 00:01:45,000
>> OLAV SCHULSTA: A lot
of people took it very

42
00:01:45,066 --> 00:01:47,299
personally and they
want to fight this.

43
00:01:47,366 --> 00:01:51,500
We didn't have any, I
think, impact on delivery

44
00:01:51,566 --> 00:01:54,033
to customers due to
the cyber attack.

45
00:01:54,099 --> 00:01:56,700
And I think the transparency
was a big part of that.

46
00:01:56,766 --> 00:01:59,466
>> JO DE VLIEGHER: Prevention alone will not save you.

47
00:01:59,533 --> 00:02:03,233
Value your people, because
when systems are down,

48
00:02:03,299 --> 00:02:06,400
whether a company survives
or succeeds is all about

49
00:02:06,466 --> 00:02:08,233
the people that are there.

50
00:02:08,300 --> 00:02:09,566
>> DEAN WELLS: And the
only way you're going to

51
00:02:09,633 --> 00:02:11,400
get through that is
through knowledge sharing,

52
00:02:11,466 --> 00:02:14,633
information sharing, being
collaborative, and working

53
00:02:14,699 --> 00:02:15,699
with one another.

54
00:02:15,766 --> 00:02:17,033
>> CONSTANCE HOURIE: It
gave us a really good

55
00:02:17,099 --> 00:02:19,500
opportunity to build
a new foundation.

56
00:02:19,566 --> 00:02:21,433
And there's a lot more
things that we're able to

57
00:02:21,500 --> 00:02:24,933
do now that would have
taken us years to get to

58
00:02:25,000 --> 00:02:26,900
and we did it
in three weeks.

59
00:02:26,966 --> 00:02:29,966
So it's a whole new
IT world for us.

60
00:02:33,599 --> 00:02:34,866
>> OLAV SCHULSTA: We
usually say that our

61
00:02:34,933 --> 00:02:40,866
production is dependent 80%
of people and 20% of machinery.

62
00:02:40,933 --> 00:02:44,233
We see similarities in the
defense of a cyber attack.

63
00:02:44,300 --> 00:02:46,566
The people are the
most important thing.

64
00:02:46,633 --> 00:02:47,866
>> JO DE VLIEGHER: Hydro
has been recognized

65
00:02:47,933 --> 00:02:49,733
in cyber response.

66
00:02:49,800 --> 00:02:53,366
That had a lot to do with
how the 36,000 people have

67
00:02:53,433 --> 00:02:56,000
stood up for the challenge
and be creative and

68
00:02:56,066 --> 00:02:59,066
deliver in the most
adverse circumstances

69
00:02:59,133 --> 00:03:01,766
possible, and then
ultimately came out

70
00:03:01,833 --> 00:03:03,833
stronger than
how we started.

71
00:03:09,199 --> 00:03:11,533
>> SPEAKER: Please welcome
Corporate Vice President

72
00:03:11,599 --> 00:03:16,000
Cybersecurity Solutions
Group Microsoft, Ann Johnson.

73
00:03:23,933 --> 00:03:26,333
>> ANN JOHNSON:
Good afternoon.

74
00:03:26,400 --> 00:03:29,599
You know, it used to be
the case that if you saw a

75
00:03:29,666 --> 00:03:33,198
major attack on a global
industrial manufacturing

76
00:03:33,266 --> 00:03:36,599
company or an attack on
a government in a vast

77
00:03:36,666 --> 00:03:41,500
Arctic territory, you were
actually watching a Bond movie.

78
00:03:41,566 --> 00:03:44,166
But what we've learned,
and you saw in this video,

79
00:03:44,233 --> 00:03:49,133
is that life often
imitates art, and our industry

80
00:03:49,199 --> 00:03:55,066
deals with these kinds of threats every single day.

81
00:03:55,133 --> 00:03:56,866
These attacks can
be devastating.

82
00:03:56,933 --> 00:03:59,833
They're devastating far beyond the company's bottom line.

83
00:03:59,900 --> 00:04:00,500
Right?

84
00:04:00,566 --> 00:04:01,900
They think about that.

85
00:04:01,966 --> 00:04:04,333
But it's disruptions
in regular workflow.

86
00:04:04,400 --> 00:04:07,033
It could be disastrous
and upend the financial

87
00:04:07,099 --> 00:04:09,000
security of even the
families that work for

88
00:04:09,066 --> 00:04:10,800
these organizations.

89
00:04:10,866 --> 00:04:13,900
Compromised personal
data can lead to a

90
00:04:13,966 --> 00:04:15,966
lifetime of uncertainty.

91
00:04:16,033 --> 00:04:19,033
We fear that we're going
to have future attacks.

92
00:04:19,100 --> 00:04:22,533
And without a robust cyber
response, threats can drag

93
00:04:22,600 --> 00:04:26,133
on endlessly and they wear
down folks, folks that

94
00:04:26,199 --> 00:04:29,933
actually did nothing
wrong in the first place.

95
00:04:30,000 --> 00:04:32,333
You know, as an industry,
we talk a lot about how we

96
00:04:32,399 --> 00:04:34,266
use artificial
intelligence and how we

97
00:04:34,333 --> 00:04:38,100
use machine learning to
analyze massive amounts of

98
00:04:38,166 --> 00:04:40,233
security and threat
intelligence data.

99
00:04:40,300 --> 00:04:43,366
We talk at Microsoft about
how we see eight trillion

100
00:04:43,433 --> 00:04:46,699
signals each day for
detection and for prevention.

101
00:04:46,766 --> 00:04:49,266
And, of course, on the
machine side, we can

102
00:04:49,333 --> 00:04:55,266
rapidly separate threats,
real ones from false ones.

103
00:04:55,333 --> 00:04:59,266
But it's really on the
human side that we focus

104
00:04:59,333 --> 00:05:03,500
on the highest priority,
on the actionable tasks.

105
00:05:03,566 --> 00:05:06,732
And when we go to work
every day, we are

106
00:05:06,800 --> 00:05:09,600
defending against and we
are fixing problems that

107
00:05:09,666 --> 00:05:11,666
are potentially
devastating not just for

108
00:05:11,733 --> 00:05:14,366
businesses but for the
employees of those

109
00:05:14,433 --> 00:05:16,533
businesses around
the planet.

110
00:05:19,033 --> 00:05:24,466
In his 2018 book, AI
Superpowers, Kai-Fu Lee

111
00:05:24,533 --> 00:05:26,800
wrote about the need
to balance artificial

112
00:05:26,866 --> 00:05:29,566
intelligence with
human empathy.

113
00:05:29,633 --> 00:05:32,966
He talked about how our
future depends on AI's

114
00:05:33,033 --> 00:05:36,399
ability to think
coupled with the human's

115
00:05:36,466 --> 00:05:38,699
ability to love.

116
00:05:38,766 --> 00:05:42,933
And I can't agree more
with that sentiment.

117
00:05:43,000 --> 00:05:47,033
We work on some of the
most pervasive issues in tech.

118
00:05:47,100 --> 00:05:48,733
And by the way, I was
joking with someone.

119
00:05:48,800 --> 00:05:50,233
I said that's a pretty
bold statement because

120
00:05:50,300 --> 00:05:52,300
there are an awful
lot of issues in tech.

121
00:05:53,899 --> 00:05:58,366
However, as we think about
the issues we deal with

122
00:05:58,433 --> 00:06:02,466
like digital privacy,
ransomware, hybrid cloud

123
00:06:02,533 --> 00:06:05,566
security, and we are
constantly in an evolving

124
00:06:05,633 --> 00:06:09,599
and escalating cyber arms
race, we are also facing

125
00:06:09,666 --> 00:06:10,666
new challenges.

126
00:06:10,733 --> 00:06:12,800
We think about
election security.

127
00:06:12,866 --> 00:06:14,133
We're thinking about
the increase of

128
00:06:14,199 --> 00:06:15,633
nation state attacks.

129
00:06:15,699 --> 00:06:17,600
We're thinking about
political misinformation

130
00:06:17,666 --> 00:06:20,000
and things like deep
fakes, all types of new

131
00:06:20,066 --> 00:06:21,799
threats to us.

132
00:06:21,866 --> 00:06:25,300
So while I'm standing here
on stage, I do want to say

133
00:06:25,366 --> 00:06:27,866
that I'm ready whenever
they want to hire me as a

134
00:06:27,933 --> 00:06:30,599
consultant for the
next Bond movie.

135
00:06:30,666 --> 00:06:31,699
Probably won't happen.

136
00:06:31,766 --> 00:06:34,233
But while I wait for
Hollywood to invest in my

137
00:06:34,300 --> 00:06:36,100
predictions about
the future of AI and

138
00:06:36,166 --> 00:06:38,666
technology, I actually
want to start by

139
00:06:38,733 --> 00:06:41,233
revisiting the past.

140
00:06:41,300 --> 00:06:44,066
Does anyone here know
where this text came from?

141
00:06:44,133 --> 00:06:46,198
I would encourage you to
shout it out if you do.

142
00:06:46,266 --> 00:06:46,866
Join me.

143
00:06:46,933 --> 00:06:47,533
Come on.

144
00:06:47,600 --> 00:06:48,199
It's the afternoon.

145
00:06:48,266 --> 00:06:50,266
Have some fun.

146
00:06:51,133 --> 00:06:53,765
The program with a
personality, it will get

147
00:06:53,833 --> 00:06:55,633
on all your disks.

148
00:06:55,699 --> 00:06:57,699
It will infiltrate
your chips.

149
00:06:57,766 --> 00:07:00,433
Yes, it's Cloner.

150
00:07:00,500 --> 00:07:02,566
It will stick to
you like glue.

151
00:07:02,633 --> 00:07:05,032
It will modify
your RAM, too.

152
00:07:05,100 --> 00:07:07,100
Send in the Cloner.

153
00:07:08,600 --> 00:07:10,833
This is the text that
displayed on Apple 2

154
00:07:10,899 --> 00:07:13,300
computers after being
infected with the Elk

155
00:07:13,366 --> 00:07:16,599
Cloner virus in 1982.

156
00:07:16,666 --> 00:07:19,100
It was written as a
prank by a precocious

157
00:07:19,166 --> 00:07:21,899
15-year-old named Rick
Skrenta, and it is the

158
00:07:21,966 --> 00:07:25,633
first real PC computer
virus that actually spread

159
00:07:25,699 --> 00:07:27,333
in the wild.

160
00:07:27,399 --> 00:07:30,300
Rick discovered a machine
vulnerability and he

161
00:07:30,366 --> 00:07:33,500
decided to have a little
bit of fun exploiting it.

162
00:07:33,566 --> 00:07:37,633
His friends would trade
games on floppy disks.

163
00:07:37,699 --> 00:07:39,166
I don't even want to know
how many people in the

164
00:07:39,233 --> 00:07:43,033
audience don't know
what a floppy disk is.

165
00:07:43,100 --> 00:07:46,600
But they traded games, so
he wrote a boot sector

166
00:07:46,666 --> 00:07:51,666
virus to trigger after a
game loaded for the 50th time.

167
00:07:51,733 --> 00:07:54,300
He put it on a floppy disk
and he sent it out to his

168
00:07:54,366 --> 00:07:56,933
friends and he
set it loose.

169
00:07:57,000 --> 00:08:00,133
Now the worst crime
that Rick committed was

170
00:08:00,199 --> 00:08:04,133
mediocre poetry, but there
is a lesson here about the

171
00:08:04,199 --> 00:08:07,266
Elk Cloner virus episode
and it's still relevant to

172
00:08:07,333 --> 00:08:10,133
what we do today
in cybersecurity.

173
00:08:10,199 --> 00:08:12,800
What that teenager
exploited all the way back

174
00:08:12,866 --> 00:08:17,133
in 1982 wasn't just the
machine's vulnerability;

175
00:08:17,199 --> 00:08:19,600
it was also the
human vulnerability

176
00:08:19,666 --> 00:08:21,800
of ordinary people.

177
00:08:21,866 --> 00:08:24,500
That is some of the same
problems that plague us as

178
00:08:24,566 --> 00:08:26,765
an industry today.

179
00:08:26,833 --> 00:08:29,399
In the video you just
watched, you heard a

180
00:08:29,466 --> 00:08:32,133
little about what happened
to Hydro, the global

181
00:08:32,200 --> 00:08:35,066
aluminum manufacturer
headquartered in Oslo, and

182
00:08:35,133 --> 00:08:37,266
you also heard about what
happened in the Nunavut

183
00:08:37,332 --> 00:08:38,732
government in Canada.

184
00:08:38,799 --> 00:08:41,433
They're in the northern
territory in Canada.

185
00:08:41,500 --> 00:08:46,166
The Hydro attack started
in March of last year.

186
00:08:46,233 --> 00:08:48,766
It started as a ransomware
attack that spread across

187
00:08:48,833 --> 00:08:52,833
40 countries, impacting
35,000 employees.

188
00:08:52,899 --> 00:08:55,899
And the domino started
to fall after one single

189
00:08:55,966 --> 00:08:58,899
employee opened an
infected email from a

190
00:08:58,966 --> 00:08:59,966
trusted source.

191
00:09:00,033 --> 00:09:03,466
This opened the door just
enough to let the hackers in.

192
00:09:03,533 --> 00:09:06,533
There is a reason we actually call these things trojans.

193
00:09:06,600 --> 00:09:08,899
It's for a good name.

194
00:09:08,966 --> 00:09:11,766
Much like history's first
virus, the users of the

195
00:09:11,833 --> 00:09:14,399
infected machines were
treated to an unwanted

196
00:09:14,466 --> 00:09:19,200
message; however, instead
of a teenage troll's poem,

197
00:09:19,266 --> 00:09:21,733
they actually got a
ransom note demanding a

198
00:09:21,799 --> 00:09:24,233
payout in Bitcoin.

199
00:09:24,299 --> 00:09:26,399
The hackers captured
administrative

200
00:09:26,466 --> 00:09:29,799
credentials, they
encrypted files, and they

201
00:09:29,866 --> 00:09:31,766
actually paralyzed
the organization.

202
00:09:31,833 --> 00:09:34,733
Production was shut down
at the company's largest

203
00:09:34,799 --> 00:09:37,733
plant, and the C-suite had
to make a lot of really

204
00:09:37,799 --> 00:09:40,533
tough decisions at
this point in time.

205
00:09:40,600 --> 00:09:42,566
Should they negotiate?

206
00:09:42,633 --> 00:09:44,000
Should they pay?

207
00:09:44,066 --> 00:09:46,600
Should they try to
contain the story, keep it

208
00:09:46,666 --> 00:09:47,766
from the public?

209
00:09:47,833 --> 00:09:50,899
Or should they
actually go public?

210
00:09:50,966 --> 00:09:53,799
In the end, they made some
really smart decisions.

211
00:09:53,866 --> 00:09:56,066
They decided to be fully
transparent with their

212
00:09:56,133 --> 00:09:58,600
employees, they decided to
be fully transparent with

213
00:09:58,666 --> 00:10:00,933
the public, and they
called in the Microsoft

214
00:10:01,000 --> 00:10:04,299
DART team to help them
with their response.

215
00:10:04,366 --> 00:10:07,199
It is always our advice
that if you get hit with a

216
00:10:07,266 --> 00:10:09,933
ransomware attack, you
should never pay the ransom.

217
00:10:10,000 --> 00:10:12,899
As painful and as
destructive as that can

218
00:10:12,966 --> 00:10:15,666
be, the consequences
can be far worse if you

219
00:10:15,733 --> 00:10:18,132
actually pay the ransom.

220
00:10:18,200 --> 00:10:19,799
You have probably - many
of you have heard the

221
00:10:19,866 --> 00:10:22,600
expression don't
feed the birds.

222
00:10:22,666 --> 00:10:24,833
Well, that's because the
birds will come back to

223
00:10:24,899 --> 00:10:28,000
the places where they are
consistently being fed.

224
00:10:28,066 --> 00:10:31,299
The same thing is true
for cyber criminals.

225
00:10:31,366 --> 00:10:33,866
They are looking for
weakness in cyber

226
00:10:33,933 --> 00:10:35,966
defenses, and when they
find them, they will

227
00:10:36,033 --> 00:10:40,966
continually exploit them
over and over again.

228
00:10:41,033 --> 00:10:43,766
In the case of the
Hydro attack, high-tech

229
00:10:43,833 --> 00:10:46,565
forensics, crisis
management, machine

230
00:10:46,633 --> 00:10:48,766
learning all played a
critical role in the

231
00:10:48,833 --> 00:10:51,866
response, but it's really
worth mentioning that one

232
00:10:51,933 --> 00:10:54,200
of the most effective
innovations we use to

233
00:10:54,266 --> 00:10:56,665
mitigate the damage
was kind of retro.

234
00:10:56,733 --> 00:10:59,699
It was actually
human handwriting.

235
00:10:59,766 --> 00:11:03,665
The executives at Hydro
hand-wrote messages and

236
00:11:03,733 --> 00:11:05,965
then they photographed
those messages with their

237
00:11:06,033 --> 00:11:07,399
mobile phones.

238
00:11:07,466 --> 00:11:10,600
They then texted the
messages to their managers.

239
00:11:10,666 --> 00:11:13,399
The managers went and had
those messages printed and

240
00:11:13,466 --> 00:11:16,466
photocopied at a photocopy
shop in their local town

241
00:11:16,533 --> 00:11:19,366
and they hung them around
the building to inform the

242
00:11:19,433 --> 00:11:22,200
employees of the event
and also to provide them

243
00:11:22,266 --> 00:11:23,632
further instruction.

244
00:11:23,700 --> 00:11:25,666
This allowed them to
keep the event from

245
00:11:25,733 --> 00:11:28,165
spreading any further.

246
00:11:28,233 --> 00:11:30,199
In the attack on the
Nunavut government which

247
00:11:30,266 --> 00:11:33,132
happened last year also in
November, we're looking at

248
00:11:33,200 --> 00:11:35,566
a very similar type
of attack but a very

249
00:11:35,633 --> 00:11:37,700
different victim.

250
00:11:37,766 --> 00:11:41,266
The Nunavut CIO mentioned
in the video it affected

251
00:11:41,333 --> 00:11:43,533
people in every community.

252
00:11:43,600 --> 00:11:46,766
They lost health care
services, they lost

253
00:11:46,833 --> 00:11:50,632
financial services, their
first responders could not

254
00:11:50,700 --> 00:11:53,733
provide emergency
medical services, and

255
00:11:53,799 --> 00:11:55,899
they had to rebuild.

256
00:11:55,966 --> 00:11:59,633
But like Hydro, they
absolutely refused to

257
00:11:59,700 --> 00:12:01,066
pay the ransom.

258
00:12:01,133 --> 00:12:04,233
They reverted to old
low-tech solutions like

259
00:12:04,299 --> 00:12:07,533
paper forms and fax
machines as a temporary

260
00:12:07,600 --> 00:12:10,733
stopgap to keep their
systems running, and they

261
00:12:10,799 --> 00:12:13,433
also worked with our DART
team to help them rebuild

262
00:12:13,500 --> 00:12:15,533
their environment.

263
00:12:15,600 --> 00:12:18,333
When all was said and
done, we were able to

264
00:12:18,399 --> 00:12:21,633
mount successful defenses
with these customers in

265
00:12:21,700 --> 00:12:25,600
both attacks through a
combination of a few things.

266
00:12:25,666 --> 00:12:27,966
We leveraged
technology, of course.

267
00:12:28,033 --> 00:12:30,700
We leveraged new things
like machine learning.

268
00:12:30,766 --> 00:12:32,799
We prescribed
folks to use MFA.

269
00:12:32,866 --> 00:12:36,066
We talked about their
backup strategy.

270
00:12:36,133 --> 00:12:38,899
We talked about smart
processes, things that

271
00:12:38,966 --> 00:12:40,299
could be changed.

272
00:12:40,366 --> 00:12:44,399
But most importantly, we
leveraged the really good

273
00:12:44,466 --> 00:12:47,166
people at Hydro and the
really good people at

274
00:12:47,233 --> 00:12:50,399
Nunavut that came together
and really collaborated to

275
00:12:50,466 --> 00:12:53,466
get those systems
back online.

276
00:12:53,533 --> 00:12:55,533
The attackers
aren't stupid.

277
00:12:55,600 --> 00:12:57,100
None of them are.

278
00:12:57,166 --> 00:12:59,299
But they failed to
anticipate just how

279
00:12:59,366 --> 00:13:03,266
innovative, how ambitious,
and how aggressive the

280
00:13:03,333 --> 00:13:06,433
problem-solvers, the
humans would be.

281
00:13:06,500 --> 00:13:09,266
They underestimated
the target and they

282
00:13:09,333 --> 00:13:11,799
underestimated
those teams.

283
00:13:11,866 --> 00:13:15,000
They underestimated the
creativity and they

284
00:13:15,066 --> 00:13:17,500
underestimated
the compassion.

285
00:13:17,566 --> 00:13:20,466
And they failed to
anticipate the one thing

286
00:13:20,533 --> 00:13:22,700
that both Hydro and
Nunavut could put into

287
00:13:22,766 --> 00:13:26,966
action, the actual
human spirit.

288
00:13:27,033 --> 00:13:29,299
If you take nothing else
away from what I'm saying

289
00:13:29,366 --> 00:13:31,966
today, take it from me
that in cybersecurity,

290
00:13:32,033 --> 00:13:34,833
investing in the human
element will always give

291
00:13:34,899 --> 00:13:37,000
the good guys an edge.

292
00:13:37,066 --> 00:13:39,000
And we absolutely
can't wait.

293
00:13:39,066 --> 00:13:42,200
I'm not an alarmist, but
the stakes are getting higher.

294
00:13:42,266 --> 00:13:44,466
Attackers are targeting
everything, intellectual

295
00:13:44,533 --> 00:13:48,633
property, critical
infrastructure, and elections.

296
00:13:48,700 --> 00:13:51,533
If you think about a
conversation I recently

297
00:13:51,600 --> 00:13:54,833
had with Chris Wysopal
who is now the CTO of

298
00:13:54,899 --> 00:13:59,000
Veracode, we were talking
about how in 1998 he was

299
00:13:59,066 --> 00:14:02,433
called to testify about
cybersecurity issues

300
00:14:02,500 --> 00:14:05,100
before the Senate
Governmental Affairs

301
00:14:05,166 --> 00:14:06,133
Committee of the U.S.

302
00:14:06,200 --> 00:14:08,600
and he went with some
of his fellow hackers.

303
00:14:08,666 --> 00:14:10,966
You should watch the testimony if you have a moment.

304
00:14:11,033 --> 00:14:12,000
It's worth watching.

305
00:14:12,066 --> 00:14:13,333
It's fascinating.

306
00:14:13,399 --> 00:14:16,299
It's a little bit
surreal, actually.

307
00:14:16,366 --> 00:14:18,665
And it opens with the
chairman, Senator Fred

308
00:14:18,733 --> 00:14:20,199
Thompson of Tennessee.

309
00:14:20,266 --> 00:14:23,533
He's sitting fairly
imperiously over these hackers.

310
00:14:23,600 --> 00:14:26,200
As a side note, this was
just a few years after

311
00:14:26,266 --> 00:14:28,165
Thompson had actually
starred in the Hunt for

312
00:14:28,233 --> 00:14:30,500
Red October and
in Die Hard 2.

313
00:14:30,566 --> 00:14:33,399
A big presence for
these young hackers.

314
00:14:33,466 --> 00:14:35,666
And he looks down at the
group and he opens his

315
00:14:35,733 --> 00:14:39,032
remarks by saying due to
the sensitivity of their

316
00:14:39,100 --> 00:14:41,833
work, they will be using
their hacker names.

317
00:14:41,899 --> 00:14:44,766
They will be using Mudge
and Weld and Brian

318
00:14:44,833 --> 00:14:48,266
Oblivion and Kingpin
and Space Rogue.

319
00:14:48,333 --> 00:14:50,899
They were using not their
real names, but they were

320
00:14:50,966 --> 00:14:54,633
actually using
their hacker names.

321
00:14:54,700 --> 00:14:57,866
He went on - well, before
I go there, on a side

322
00:14:57,933 --> 00:15:00,700
note, I actually think it
would be worth the price

323
00:15:00,766 --> 00:15:03,699
of a plane ticket to DC if
you could get a sitting U.S.

324
00:15:03,766 --> 00:15:08,599
senator to refer to you as
either Kingpin or Space Rogue.

325
00:15:08,666 --> 00:15:10,933
And as a matter of fact,
if I ever get asked to

326
00:15:11,000 --> 00:15:14,700
testify before Congress,
my hacker name is Cyber

327
00:15:14,766 --> 00:15:18,033
Dino, which is a name I
share with the ceramic

328
00:15:18,100 --> 00:15:20,633
dinosaur that accompanies
all around the world to

329
00:15:20,700 --> 00:15:23,233
talk about cybersecurity.

330
00:15:23,299 --> 00:15:25,600
But back to the story.

331
00:15:25,666 --> 00:15:29,033
Chris was recounting the
fact that what would

332
00:15:29,100 --> 00:15:32,733
happen if one of America's
adversaries actually had

333
00:15:32,799 --> 00:15:34,866
the skills that that young
group of hackers would

334
00:15:34,933 --> 00:15:37,399
have, what would happen?

335
00:15:37,466 --> 00:15:40,866
He talked about how
unprepared and unsecured

336
00:15:40,933 --> 00:15:43,433
both corporations and
government entities were

337
00:15:43,500 --> 00:15:44,700
for malicious actors.

338
00:15:44,766 --> 00:15:47,466
His friends talked about
how easy it would be to

339
00:15:47,533 --> 00:15:50,700
actually turn
off the internet.

340
00:15:50,766 --> 00:15:53,500
And as I talked to Chris
later, he reflected on the

341
00:15:53,566 --> 00:15:56,266
fact that they were the
only ones that understood

342
00:15:56,333 --> 00:15:58,399
the urgency of the problem
who were sitting in the

343
00:15:58,466 --> 00:16:00,533
room at that time.

344
00:16:00,600 --> 00:16:02,700
He also reflected on
the fact that they were

345
00:16:02,766 --> 00:16:05,466
nowhere near where we are
today in terms of the

346
00:16:05,533 --> 00:16:09,399
scope and the scale
of cyber attacks.

347
00:16:09,466 --> 00:16:12,000
We didn't live in the
world of nation state

348
00:16:12,066 --> 00:16:14,533
cyber war like
we do today.

349
00:16:14,600 --> 00:16:18,933
We are prepared, but we
simply must do better and

350
00:16:19,000 --> 00:16:22,033
be better prepared
moving forward.

351
00:16:22,100 --> 00:16:24,966
We are still operating
under laws that were

352
00:16:25,033 --> 00:16:28,533
generated and produced
during the typewriter era.

353
00:16:28,600 --> 00:16:31,399
We need to rewrite some of
those laws and build up

354
00:16:31,466 --> 00:16:34,366
some of those capabilities
to keep an eye on the

355
00:16:34,433 --> 00:16:37,033
technology and the
attacks of today.

356
00:16:37,100 --> 00:16:40,566
The consequence of the
inaction among our leaders

357
00:16:40,633 --> 00:16:43,600
are enormous, but
they're not entirely

358
00:16:43,666 --> 00:16:44,399
unpredictable.

359
00:16:44,466 --> 00:16:47,233
We just have to listen
to the right people.

360
00:16:47,299 --> 00:16:49,500
And I fundamentally
believe there are more

361
00:16:49,566 --> 00:16:52,966
benevolent white hats and
wise cyber warriors out

362
00:16:53,033 --> 00:16:55,166
there than there
are malicious ones.

363
00:16:55,233 --> 00:16:57,366
We need to keep an eye out
for those folks and we

364
00:16:57,433 --> 00:16:59,733
actually need to
listen to those folks.

365
00:17:01,899 --> 00:17:07,965
In 2018, an 11-year-old
hacked a Florida voting

366
00:17:08,032 --> 00:17:12,199
machine at DEF CON in
under two minutes.

367
00:17:12,266 --> 00:17:15,400
Rachel Tobac, who is
the CEO of SocialProof

368
00:17:15,465 --> 00:17:18,433
Security, has right now
pinned on her Twitter feed

369
00:17:18,500 --> 00:17:22,598
a tweet that shows that
she could hack a similar

370
00:17:22,665 --> 00:17:24,665
system in two minutes.

371
00:17:26,665 --> 00:17:29,666
It's scary if
you're cynical.

372
00:17:29,733 --> 00:17:30,666
I'm a lot of things.

373
00:17:30,733 --> 00:17:34,366
The one thing I'm
not is cynical.

374
00:17:34,433 --> 00:17:39,000
I'm kind of a stack
half full kind of geek.

375
00:17:39,066 --> 00:17:42,599
I'm excited for our future
because I believe in the

376
00:17:42,666 --> 00:17:45,233
fundamental goodness
of most people.

377
00:17:45,299 --> 00:17:47,700
The young people digging
for vulnerabilities in our

378
00:17:47,766 --> 00:17:51,000
systems today are exactly
the people I want to hire

379
00:17:51,066 --> 00:17:54,366
to guarantee our
security of the future.

380
00:17:54,433 --> 00:17:58,566
Let the cynics laugh, but
there is wisdom in that.

381
00:17:58,633 --> 00:18:02,033
Mr. Rogers said in the
wake of the 9/11 attacks

382
00:18:02,099 --> 00:18:06,066
in the U.S., when he was
a boy and he saw scary

383
00:18:06,133 --> 00:18:09,233
things in the news, his
mother would say to look

384
00:18:09,299 --> 00:18:11,700
for the helpers.

385
00:18:11,766 --> 00:18:15,200
You will always find
people who are helping.

386
00:18:15,266 --> 00:18:18,700
Now maybe you didn't grow
up in the Mr. Rogers era.

387
00:18:18,766 --> 00:18:21,299
But for my generation,
he was more than

388
00:18:21,366 --> 00:18:23,033
a children's entertainer.

389
00:18:23,099 --> 00:18:25,666
He taught kids that they
had value, that they were

390
00:18:25,733 --> 00:18:29,399
worthy of love, but he
also taught them that they

391
00:18:29,466 --> 00:18:31,933
could deal with life's
challenges if they did

392
00:18:32,000 --> 00:18:35,433
their best to work
together to solve hard

393
00:18:35,500 --> 00:18:39,599
problems, and that is what
we see in the human spirit

394
00:18:39,666 --> 00:18:41,233
of cybersecurity.

395
00:18:41,299 --> 00:18:44,733
We see examples of these
helpers every day, people

396
00:18:44,799 --> 00:18:48,099
coming together time and
time again, doing what

397
00:18:48,166 --> 00:18:51,599
they do best to solve
complex problems.

398
00:18:51,666 --> 00:18:53,566
The helpers are there.

399
00:18:53,633 --> 00:18:56,299
They're working on
tomorrow's problems and

400
00:18:56,366 --> 00:18:59,166
they have this
unstoppable human spirit.

401
00:18:59,233 --> 00:19:02,966
There is so much positive
energy in cybersecurity,

402
00:19:03,033 --> 00:19:05,099
whether it's in the wake
of a cyber attack like the

403
00:19:05,166 --> 00:19:07,799
ones you saw at Hydro and
Nunavut or folks that are

404
00:19:07,866 --> 00:19:10,466
working to proactively
defend against those types

405
00:19:10,533 --> 00:19:11,366
of attacks.

406
00:19:11,433 --> 00:19:15,333
In our industry, these
helpers are cyber defenders.

407
00:19:15,400 --> 00:19:17,866
These are the people who
are leveraging technology

408
00:19:17,933 --> 00:19:20,766
to make notable progress
for our cyber defenses.

409
00:19:23,099 --> 00:19:24,765
And it's not
just the responsibility

410
00:19:24,833 --> 00:19:25,799
of global governments.

411
00:19:25,866 --> 00:19:27,666
We talk a lot
about government.

412
00:19:27,733 --> 00:19:30,133
But it's going to take an
ecosystem of all of us,

413
00:19:30,200 --> 00:19:33,233
public and private sector,
academic institutions,

414
00:19:33,299 --> 00:19:37,200
researchers, and,
yes, tech companies.

415
00:19:37,266 --> 00:19:39,799
I started today talking
about how human

416
00:19:39,866 --> 00:19:42,666
vulnerabilities are a
given in the tech space.

417
00:19:42,733 --> 00:19:46,733
It goes back to the bad
poetry of 1982, the early

418
00:19:46,799 --> 00:19:47,966
education of the U.S.

419
00:19:48,033 --> 00:19:52,500
Congress in 1998, election
interference in 2016.

420
00:19:52,566 --> 00:19:54,566
It's pretty much
baked into the pie.

421
00:19:54,633 --> 00:19:57,633
And there is no reason to
believe, by the way, that

422
00:19:57,700 --> 00:20:00,833
cyber crime is
going anywhere.

423
00:20:00,900 --> 00:20:04,000
However, there has also
always existed this

424
00:20:04,066 --> 00:20:07,733
temptation to believe that
a technological utopia, a

425
00:20:07,799 --> 00:20:09,799
perfect solution is
around the corner.

426
00:20:09,866 --> 00:20:12,733
That certainly does
drive innovation.

427
00:20:12,799 --> 00:20:15,433
And we will always keep
searching for that silver

428
00:20:15,500 --> 00:20:18,599
bullet to cyber crime.

429
00:20:18,666 --> 00:20:21,533
But I do not believe that
artificial intelligence,

430
00:20:21,599 --> 00:20:24,099
however advanced it
becomes, is going to be

431
00:20:24,166 --> 00:20:27,666
the perfect solution
in our lifetime.

432
00:20:27,733 --> 00:20:29,899
I think that things like
artificial intelligence

433
00:20:29,966 --> 00:20:33,466
and other technologies
are as useful as the rare

434
00:20:33,533 --> 00:20:36,632
pockets that we have
on women's dresses.

435
00:20:36,700 --> 00:20:39,233
If you don't actually know
how to use them and you

436
00:20:39,299 --> 00:20:43,866
don't know they're there,
they're not very useful.

437
00:20:43,933 --> 00:20:45,933
All the women in the
audience understand that joke.

438
00:20:48,066 --> 00:20:49,366
And you may disagree.

439
00:20:49,433 --> 00:20:51,566
But I also think that
super intelligence

440
00:20:51,633 --> 00:20:54,666
singularity is still a
really long way off.

441
00:20:57,366 --> 00:21:01,099
I shared this quotation at
the start of my talk, and

442
00:21:01,166 --> 00:21:03,233
I want to refer
to it again.

443
00:21:03,299 --> 00:21:06,233
AI alone is not enough.

444
00:21:06,299 --> 00:21:10,866
We need to combine AI with
that human empathy because

445
00:21:10,933 --> 00:21:15,700
we need both humans and technology working together.

446
00:21:15,766 --> 00:21:19,266
If we can create this kind
of synergy, we actually

447
00:21:19,333 --> 00:21:23,200
can harness the power
that we have and we can

448
00:21:23,266 --> 00:21:25,833
generate prosperity and
we can solve a lot of

449
00:21:25,900 --> 00:21:29,099
problems collectively.

450
00:21:29,166 --> 00:21:31,633
We need to embrace the
human spirit that we have

451
00:21:31,700 --> 00:21:33,266
in cybersecurity.

452
00:21:33,333 --> 00:21:35,900
And trust me, I am, as I
mentioned, a half stack

453
00:21:35,966 --> 00:21:36,733
full kind of geek.

454
00:21:36,799 --> 00:21:39,666
I love dreaming about
technology, but I am

455
00:21:39,733 --> 00:21:42,666
actually convinced every
day that the most important

456
00:21:42,733 --> 00:21:45,933
aspect of our work
remains fundamentally human.

457
00:21:48,366 --> 00:21:51,166
Let the tech romantics
yearn for utopia and let

458
00:21:51,233 --> 00:21:55,033
the pessimists assume
our imminent extinction.

459
00:21:55,099 --> 00:21:58,233
I am a realist and I'm
interested in finding the

460
00:21:58,299 --> 00:22:01,500
best that humanity has
to offer to help shape

461
00:22:01,566 --> 00:22:04,400
our shared future.

462
00:22:04,466 --> 00:22:07,599
As I close, I want to
tell you one more story.

463
00:22:07,666 --> 00:22:11,733
At age 11, a girl named
Aditi Shaw in Indian was

464
00:22:11,799 --> 00:22:14,833
diagnosed with a genetic
disorder that gradually

465
00:22:14,900 --> 00:22:17,766
decreases a
person's vision.

466
00:22:17,833 --> 00:22:21,866
At age 15, she was almost
completely blind, only

467
00:22:21,933 --> 00:22:25,000
able to see
light and dark.

468
00:22:25,066 --> 00:22:27,633
She had to relearn
how to learn.

469
00:22:27,700 --> 00:22:30,099
But through hard work, she
made it all the way to

470
00:22:30,166 --> 00:22:32,433
Georgia Tech where she
completed her master's

471
00:22:32,500 --> 00:22:34,533
degree in cybersecurity.

472
00:22:34,599 --> 00:22:37,466
She joined Microsoft
last December as a

473
00:22:37,533 --> 00:22:40,399
cybersecurity software
intern and I'm really

474
00:22:40,466 --> 00:22:42,565
looking forward to the
great things that she is

475
00:22:42,633 --> 00:22:45,000
going to accomplish.

476
00:22:45,066 --> 00:22:49,799
I also have a really
special soft spot for Aditi.

477
00:22:49,866 --> 00:22:52,700
I was also born with a
genetic vision defect.

478
00:22:52,766 --> 00:22:55,099
I have no vision
in my right eye.

479
00:22:55,166 --> 00:22:57,500
For years, I hid that
because I was afraid kids

480
00:22:57,566 --> 00:22:59,500
in school would
make fun of me.

481
00:22:59,566 --> 00:23:01,700
As an adult, I hid it
because I was afraid

482
00:23:01,766 --> 00:23:03,966
employers would
deny me opportunity.

483
00:23:04,033 --> 00:23:06,065
They would see me as
somehow defective.

484
00:23:08,133 --> 00:23:12,299
And I had dreams once
of being a Navy fighter

485
00:23:12,366 --> 00:23:15,966
pilot; however, it turns
out that the military

486
00:23:16,033 --> 00:23:18,466
isn't all that keen on
handing over the keys to a

487
00:23:18,533 --> 00:23:21,233
$15 million jet to someone
who can't even check

488
00:23:21,299 --> 00:23:23,099
traffic over their
right shoulder.

489
00:23:23,166 --> 00:23:26,599
Not quite sure why.

490
00:23:26,666 --> 00:23:29,533
So I adjusted my
plans for life.

491
00:23:29,599 --> 00:23:33,399
And today I am proud of
the fact that I am one of

492
00:23:33,466 --> 00:23:36,433
the helpers in such an
important industry,

493
00:23:36,500 --> 00:23:39,733
working with a team that
solves problems and makes

494
00:23:39,799 --> 00:23:43,333
changes for the
betterment of society.

495
00:23:43,400 --> 00:23:45,700
I am also proud that I
am allowed to work for a

496
00:23:45,766 --> 00:23:49,200
company that would hire
someone like me and

497
00:23:49,266 --> 00:23:51,200
someone like Aditi.

498
00:23:51,266 --> 00:23:53,799
So if you're responsible
for hiring people, you

499
00:23:53,866 --> 00:23:57,466
need to consider the
challenges folks overcome

500
00:23:57,533 --> 00:23:59,933
and you need to do the
extra work to find people

501
00:24:00,000 --> 00:24:03,233
like Aditi because the
ideas they will bring to

502
00:24:03,299 --> 00:24:07,500
your organization
are invaluable.

503
00:24:07,566 --> 00:24:10,866
And if you are responding
to a cyber attack, be

504
00:24:10,933 --> 00:24:14,000
forgiving of the human
weakness that we all share

505
00:24:14,066 --> 00:24:16,933
that may have led you
there and celebrate the

506
00:24:17,000 --> 00:24:20,533
strengths that make us
resilient in defending

507
00:24:20,599 --> 00:24:23,566
against those attacks.

508
00:24:23,633 --> 00:24:27,066
I want to thank RSA, again,
for allowing me to speak.

509
00:24:27,133 --> 00:24:29,700
And because I'm not
ashamed to pander, I want

510
00:24:29,766 --> 00:24:32,599
to show the things that
actually help my human

511
00:24:32,666 --> 00:24:36,133
spirit every single
day, my four-legged and

512
00:24:36,200 --> 00:24:37,633
two-legged children.

513
00:24:37,700 --> 00:24:38,466
Thank you so much.

514
00:24:38,533 --> 00:24:39,699
Have a great day.

