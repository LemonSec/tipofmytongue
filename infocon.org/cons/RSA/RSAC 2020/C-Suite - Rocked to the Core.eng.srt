1
00:00:06,166 --> 00:00:08,533
>> SPEAKER: To lead
today's discussion, please

2
00:00:08,599 --> 00:00:11,733
welcome moderator
Donna Dodson.

3
00:00:15,866 --> 00:00:17,466
>> DONNA DODSON: Good
afternoon, everyone.

4
00:00:17,533 --> 00:00:21,933
Thank you all for being here, particularly the contingency.

5
00:00:22,000 --> 00:00:24,333
I appreciate that.

6
00:00:24,399 --> 00:00:29,266
Today we're here to talk
about Rocked to the Core

7
00:00:29,333 --> 00:00:31,599
and looking at
vulnerabilities.

8
00:00:31,666 --> 00:00:35,233
I really have a wonderful
panel here for the

9
00:00:35,299 --> 00:00:41,766
discussion, Paul Kocher,
Marene Allison, Phil Venables.

10
00:00:41,833 --> 00:00:45,666
We really have the
A-team up on stage.

11
00:00:45,733 --> 00:00:50,032
I'm looking forward
to the conversation.

12
00:00:50,100 --> 00:00:52,766
But this is not
just a conversation.

13
00:00:52,833 --> 00:00:57,000
This is really actually
critical as we think about

14
00:00:57,066 --> 00:01:00,899
the world that we are in
today and the world of the

15
00:01:00,966 --> 00:01:04,299
future and digital
technologies, the

16
00:01:04,366 --> 00:01:06,366
directions that
we're going.

17
00:01:06,433 --> 00:01:09,833
Over the past few years,
we have had significant

18
00:01:09,900 --> 00:01:13,166
vulnerabilities in
basic building blocks

19
00:01:13,233 --> 00:01:17,099
of core technologies.

20
00:01:17,166 --> 00:01:20,899
We have had Heartbleed
and Spectre and Meltdown,

21
00:01:20,966 --> 00:01:23,232
Windows 10.

22
00:01:23,299 --> 00:01:26,599
As each of these have been
discovered, the downstream

23
00:01:26,666 --> 00:01:30,500
ripples will be
felt for years.

24
00:01:30,566 --> 00:01:34,900
We absolutely need to
create a stronger, more

25
00:01:34,966 --> 00:01:37,565
secure environment.

26
00:01:37,633 --> 00:01:39,699
We need stronger products.

27
00:01:39,766 --> 00:01:43,866
We need to reduce
our cyber exposure.

28
00:01:43,933 --> 00:01:45,066
How do we do that?

29
00:01:45,133 --> 00:01:49,466
How do we get out of the
game of whack-a-mole?

30
00:01:49,533 --> 00:01:52,033
What are some of the
challenges in the

31
00:01:52,099 --> 00:01:55,666
development lifecycle and
things that we can do to

32
00:01:55,733 --> 00:01:58,400
address these
significant failures?

33
00:01:58,466 --> 00:02:00,933
With that, I'm going to
start with the panel.

34
00:02:01,000 --> 00:02:02,166
I'll start with you, Paul.

35
00:02:02,233 --> 00:02:04,066
If you can tell us, what
do you see happening out

36
00:02:04,133 --> 00:02:08,799
there today?

37
00:02:08,866 --> 00:02:09,766
>> PAUL KOCHER: Actually,
I was going to start by

38
00:02:09,833 --> 00:02:11,133
saying I think it is
hilarious that you put

39
00:02:11,199 --> 00:02:14,933
Windows 10 as the bug as
opposed to the CryptoAPI bug.

40
00:02:15,000 --> 00:02:17,966
I probably wouldn't dig
into that too much.

41
00:02:18,033 --> 00:02:22,000
If you look at all of
those vulnerabilities you

42
00:02:22,066 --> 00:02:23,500
mentioned, there is kind
of a common thread that

43
00:02:23,566 --> 00:02:25,300
connects all of them the
way I see it, which is

44
00:02:25,366 --> 00:02:28,033
they all result from
unsafe optimizations.

45
00:02:28,099 --> 00:02:32,198
Heartbleed resulted from
Open SSL using their own

46
00:02:32,266 --> 00:02:35,465
sort of internally built
memory allocator and a

47
00:02:35,533 --> 00:02:37,132
bunch of other things that
made it really hard to

48
00:02:37,199 --> 00:02:38,266
analyze their code.

49
00:02:38,333 --> 00:02:41,699
Spectre and Meltdown are
obviously results from use

50
00:02:41,766 --> 00:02:44,266
of speculative execution
and processers, and the

51
00:02:44,333 --> 00:02:47,733
Windows 10 CryptoAPI bug
results from mismatches

52
00:02:47,800 --> 00:02:49,933
between the cached
versions of certificates

53
00:02:50,000 --> 00:02:51,266
that have been
validated and the new

54
00:02:51,333 --> 00:02:53,533
certificates coming in.

55
00:02:53,599 --> 00:02:56,000
At the heart of a lot of
this is, the way I see it,

56
00:02:56,066 --> 00:02:59,199
is we have optimized
away security over a very

57
00:02:59,266 --> 00:03:00,698
long period here.

58
00:03:00,766 --> 00:03:02,866
We have made choices to
get products to market as

59
00:03:02,933 --> 00:03:05,433
fast as possible, at the
lowest cost possible, with

60
00:03:05,500 --> 00:03:08,466
the least testing
possible, with absolutely

61
00:03:08,533 --> 00:03:10,366
no sacrifices
for performance.

62
00:03:10,433 --> 00:03:12,900
When you start beginning
with your set of

63
00:03:12,966 --> 00:03:15,599
constraints as saying I
can sacrifice none of

64
00:03:15,666 --> 00:03:17,000
these other things
and security is the

65
00:03:17,066 --> 00:03:18,900
afterthought, it is very
predictable that we're

66
00:03:18,966 --> 00:03:21,066
going to see things
like these coming.

67
00:03:21,133 --> 00:03:24,500
The very first step, as
I see it, whether you're

68
00:03:24,566 --> 00:03:26,833
building a product or
running a network or even

69
00:03:26,900 --> 00:03:29,133
managing the team that's
responsible for it, is to

70
00:03:29,199 --> 00:03:31,800
embrace some inefficiency
and say we are not going

71
00:03:31,866 --> 00:03:35,000
to build the car that runs
at 100% max speed, we're

72
00:03:35,066 --> 00:03:37,400
not going to build the
building with the absolute

73
00:03:37,466 --> 00:03:39,966
least possible amount of
steel in it, we're not

74
00:03:40,033 --> 00:03:42,366
going to go build the
airplane with one engine

75
00:03:42,433 --> 00:03:45,799
and one pilot and a giant
rocket motor on the back

76
00:03:45,866 --> 00:03:47,333
that will get there
in half the time.

77
00:03:47,400 --> 00:03:50,066
We need something which
looks more like a Volvo

78
00:03:50,133 --> 00:03:52,000
than a sports car in the
case of a processor.

79
00:03:52,066 --> 00:03:54,900
We need software with
very different kinds of

80
00:03:54,966 --> 00:03:56,966
architectures than the
ones that we have today.

81
00:03:59,166 --> 00:04:01,266
>> DONNA DODSON: Marene,
you are kind of the boots

82
00:04:01,333 --> 00:04:05,133
on the ground person
here on this panel.

83
00:04:05,199 --> 00:04:11,466
What do you see from these
kinds of systemic issues?

84
00:04:11,533 --> 00:04:15,899
What does that mean for you
and for your organization?

85
00:04:15,966 --> 00:04:17,333
>> MARENE ALLISON: Well,
in the past where we would

86
00:04:17,399 --> 00:04:20,433
be a consumer of the
product, we now have to

87
00:04:20,500 --> 00:04:25,000
fully understand how
the products work.

88
00:04:25,066 --> 00:04:27,732
We have always relied in
most of it in corporate

89
00:04:27,800 --> 00:04:29,500
America is around
the network.

90
00:04:29,566 --> 00:04:33,832
If you make lousy IT and
it is in my network, if I

91
00:04:33,899 --> 00:04:36,333
configure my network
right, I can reduce my

92
00:04:36,399 --> 00:04:39,266
vulnerability because I
don't allow the outside to

93
00:04:39,333 --> 00:04:41,199
come after it.

94
00:04:41,266 --> 00:04:43,066
That worked
fine for years.

95
00:04:43,133 --> 00:04:49,633
As we move into 5G, it
makes the concern of will

96
00:04:49,699 --> 00:04:53,666
we have the right products
in the 5G network that are

97
00:04:53,733 --> 00:04:55,699
now going to be
communicating and what

98
00:04:55,766 --> 00:04:57,333
will happen?

99
00:04:57,399 --> 00:05:00,000
These aren't just -
especially in my area

100
00:05:00,066 --> 00:05:03,332
which is healthcare, we're
not looking at an IoT

101
00:05:03,399 --> 00:05:07,399
sensor to tell you that your Ring device just went off.

102
00:05:07,466 --> 00:05:08,333
No.

103
00:05:08,399 --> 00:05:10,266
We're talking about
healthcare devices.

104
00:05:10,333 --> 00:05:15,600
Now looking at the full
supply chain and ensuring

105
00:05:15,666 --> 00:05:19,800
everything what our bill
of materials are going in

106
00:05:19,866 --> 00:05:22,765
is going to have to be
absolutely at the top

107
00:05:22,833 --> 00:05:23,699
of our issue.

108
00:05:23,766 --> 00:05:28,166
Wherein the past, we were
the victims of whatever

109
00:05:28,233 --> 00:05:30,666
new patch was going to
come, but today we're

110
00:05:30,733 --> 00:05:33,500
going to have to up the
game and actually require

111
00:05:33,566 --> 00:05:39,133
vendors to provide us more
robust resilient products.

112
00:05:39,199 --> 00:05:43,333
>> DONNA DODSON: Phil,
from your perspective, you

113
00:05:43,399 --> 00:05:48,300
have seen some of these
systemic issues and the

114
00:05:48,366 --> 00:05:53,566
challenges over the past
20 years in addressing them.

115
00:05:53,633 --> 00:05:56,399
What's the big fuss today?

116
00:05:56,466 --> 00:05:58,000
>> PHIL VENABLES: I think
you will probably all

117
00:05:58,066 --> 00:06:00,832
appreciate one of the
things is, to Paul's

118
00:06:00,899 --> 00:06:04,300
point, not just these kind
of optimizations that have

119
00:06:04,366 --> 00:06:06,166
been made that have
impacted security, but

120
00:06:06,233 --> 00:06:09,600
against that, there is a
more significant backdrop

121
00:06:09,666 --> 00:06:12,300
as well in that we have a
lot more - for want of a

122
00:06:12,366 --> 00:06:14,633
better phrase -
homogeneity risk that many

123
00:06:14,699 --> 00:06:18,133
of these products that
have been optimized are

124
00:06:18,199 --> 00:06:20,199
more pervasive
and consistent.

125
00:06:20,266 --> 00:06:23,233
When these issues occur,
we feel them more deeply

126
00:06:23,300 --> 00:06:24,633
than what we used to do.

127
00:06:24,699 --> 00:06:28,100
I think similarly, as
well, when you look across

128
00:06:28,166 --> 00:06:31,933
the environment, we
definitely - only in a few

129
00:06:32,000 --> 00:06:33,533
segments of a few
industries are we

130
00:06:33,600 --> 00:06:36,566
deliberately building in
alternate systems and

131
00:06:36,633 --> 00:06:37,866
alternate resilience.

132
00:06:37,933 --> 00:06:40,332
There was a great paper
a few years ago, Richard

133
00:06:40,399 --> 00:06:42,266
Danzig, the former
Secretary of the Navy, led

134
00:06:42,333 --> 00:06:44,933
a group that wrote this
paper called Surviving On

135
00:06:45,000 --> 00:06:47,266
a Diet of Poisoned Fruit.

136
00:06:47,333 --> 00:06:49,333
If you Google it, it
is up there on the web.

137
00:06:49,399 --> 00:06:52,066
It is a really interesting
kind of document.

138
00:06:52,133 --> 00:06:54,532
Essentially, its thesis is
we have to deliberately

139
00:06:54,600 --> 00:06:57,666
plan for more innate
resilience because we're

140
00:06:57,733 --> 00:06:59,966
always going to be
fundamentally exposed to

141
00:07:00,033 --> 00:07:01,699
these types of risks.

142
00:07:01,766 --> 00:07:03,699
And then similarly,
against that backdrop of

143
00:07:03,766 --> 00:07:06,199
homogeneity in many
products, operating

144
00:07:06,266 --> 00:07:09,100
systems and hardware, we
also see the fact that

145
00:07:09,166 --> 00:07:10,800
we're running in
environments that are more

146
00:07:10,866 --> 00:07:14,866
exposed to untrusted code
that can then lead to the

147
00:07:14,933 --> 00:07:16,866
manifestation of some of
these vulnerabilities.

148
00:07:16,933 --> 00:07:20,033
I think that digitization
of society, of the

149
00:07:20,100 --> 00:07:22,333
inevitable network effect
that creates homogeneity,

150
00:07:22,399 --> 00:07:25,066
and the fact that we are
having the ability to run

151
00:07:25,133 --> 00:07:28,233
untrusted code in more
locations just compounds this.

152
00:07:28,300 --> 00:07:31,199
I think I would agree, we
have to keep fundamentally

153
00:07:31,266 --> 00:07:33,233
thinking of different ways
to build more inherent

154
00:07:33,300 --> 00:07:35,033
resilience and security
into these things.

155
00:07:35,100 --> 00:07:37,800
I think the Spectre and
Meltdown vulnerabilities

156
00:07:37,866 --> 00:07:40,233
and Google's paper last
year about the universal

157
00:07:40,300 --> 00:07:43,500
read gadget was
definitely, I think, puts

158
00:07:43,566 --> 00:07:46,198
paid to any approach that
we're going to do anything

159
00:07:46,266 --> 00:07:48,966
other than hardware
changes as opposed to

160
00:07:49,033 --> 00:07:51,000
perpetual software
optimizations.

161
00:07:51,066 --> 00:07:53,899
The final thing I would
say on the Windows Crypt

162
00:07:53,966 --> 00:07:56,933
bug was kudos to our
friends in the National

163
00:07:57,000 --> 00:08:01,066
Security Agency for making
the very laudable decision

164
00:08:01,133 --> 00:08:03,799
to actually disclose and
patch that rather than do

165
00:08:03,866 --> 00:08:04,765
anything else with it.

166
00:08:04,833 --> 00:08:07,199
That was something that I
think we should give them

167
00:08:07,266 --> 00:08:09,233
some credit for in that.

168
00:08:09,300 --> 00:08:10,733
>> PAUL KOCHER: There is
a little bit of the blame

169
00:08:10,800 --> 00:08:13,500
they get for WannaCry
and so forth.

170
00:08:13,566 --> 00:08:17,500
>> PHIL VENABLES: It is
all about the forward.

171
00:08:17,566 --> 00:08:20,366
>> DONNA DODSON: As we're
looking at these systems

172
00:08:20,433 --> 00:08:28,166
and we received the kinds
of guidance from NSA or

173
00:08:28,233 --> 00:08:33,033
from other organizations
looking at what we should

174
00:08:33,100 --> 00:08:36,899
do, we still have several
unpatched systems even

175
00:08:36,966 --> 00:08:39,633
from Spectre and Meltdown.

176
00:08:39,700 --> 00:08:40,566
Why is that?

177
00:08:40,633 --> 00:08:43,899
What kinds of things do
people need to know to

178
00:08:43,966 --> 00:08:48,866
make better decisions so
that these risks that we

179
00:08:48,933 --> 00:08:52,399
understand that are
systemic are not out there

180
00:08:52,466 --> 00:08:56,000
three and five
years later?

181
00:08:56,066 --> 00:08:57,899
>> PAUL KOCHER: I think
there are different kinds

182
00:08:57,966 --> 00:08:59,299
of issues with some of
the different products.

183
00:08:59,366 --> 00:09:00,966
The Windows CryptoAPI
bug was kind of

184
00:09:01,033 --> 00:09:02,833
the best-case scenario.

185
00:09:02,899 --> 00:09:05,533
It was a security savvy
company, basically a bug

186
00:09:05,600 --> 00:09:08,433
and just a couple lines
of code and a really

187
00:09:08,500 --> 00:09:10,466
well-oiled update
mechanism to fix it.

188
00:09:10,533 --> 00:09:14,200
If you contrast with
Spectre variant one, we

189
00:09:14,266 --> 00:09:16,199
actually don't even really
know what a properly

190
00:09:16,266 --> 00:09:17,966
protected processer
looks like.

191
00:09:18,033 --> 00:09:21,700
You can't go buy one
because it is a problem

192
00:09:21,766 --> 00:09:24,599
with just a fundamental
performance optimization

193
00:09:24,666 --> 00:09:26,533
that you need to use
if you're a processor

194
00:09:26,600 --> 00:09:29,266
manufacturer and you
want to be competitive.

195
00:09:29,333 --> 00:09:32,866
At one extreme, we have so
long as you have Windows

196
00:09:32,933 --> 00:09:33,799
update on, you're fine.

197
00:09:33,866 --> 00:09:35,933
At the other, we have one
where we may never have a

198
00:09:36,000 --> 00:09:39,133
real fix to it or the
fixes will be this very

199
00:09:39,200 --> 00:09:42,333
messy, convoluted
evolution.

200
00:09:42,399 --> 00:09:45,100
For the latter kind of
problem, that sort of

201
00:09:45,166 --> 00:09:47,399
exposes something that we
have been doing wrong as

202
00:09:47,466 --> 00:09:50,433
an industry, which is that
we're not going back and

203
00:09:50,500 --> 00:09:52,799
fixing some of the
foundational problems that

204
00:09:52,866 --> 00:09:53,665
have kind of accumulated.

205
00:09:53,733 --> 00:09:56,099
We still have buffer
overflow problems popping

206
00:09:56,166 --> 00:09:58,366
up on a near daily basis
even though we have

207
00:09:58,433 --> 00:10:02,600
understood this issue now
for, what, 30-40 years?

208
00:10:02,666 --> 00:10:05,366
We still have a bunch
of these things that we

209
00:10:05,433 --> 00:10:08,633
haven't fixed and we have
to, at some point, go back

210
00:10:08,700 --> 00:10:10,566
and fix those or we're
just going to keep

211
00:10:10,633 --> 00:10:12,966
building these deeper and
deeper technology stacks

212
00:10:13,033 --> 00:10:16,299
with every single layer
being very, very likely to

213
00:10:16,366 --> 00:10:19,033
fail as opposed to having
relatively low failure

214
00:10:19,100 --> 00:10:19,966
probabilities.

215
00:10:20,033 --> 00:10:23,600
As I look forward, a lot
of the risk reduction

216
00:10:23,666 --> 00:10:27,200
efforts that I think we
have to do revolve around

217
00:10:27,266 --> 00:10:30,733
fixing some of these
really less exciting but

218
00:10:30,799 --> 00:10:34,033
really critical
low-level technologies.

219
00:10:34,100 --> 00:10:36,666
>> DONNA DODSON: So we
have two issues, right?

220
00:10:36,733 --> 00:10:41,733
We have the holistic
approach that we need to

221
00:10:41,799 --> 00:10:45,899
think about for products
and services that we're

222
00:10:45,966 --> 00:10:48,833
using both in
hardware and software.

223
00:10:48,899 --> 00:10:51,466
I want to get to
that in a minute.

224
00:10:51,533 --> 00:10:55,000
But once something has
been identified, what

225
00:10:55,066 --> 00:10:57,799
kinds of capabilities,
what kinds of things,

226
00:10:57,866 --> 00:11:02,133
Marene, are you looking
for in order to be able to

227
00:11:02,200 --> 00:11:05,433
patch and move ahead?

228
00:11:05,500 --> 00:11:08,666
Why are these systems
still unpatched after

229
00:11:08,733 --> 00:11:09,500
all this time?

230
00:11:09,566 --> 00:11:11,600
>> MARENE ALLISON: So
every time a patch comes

231
00:11:11,666 --> 00:11:14,566
out, we have to take a
risk-based approach on

232
00:11:14,633 --> 00:11:17,266
what we're going to
patch and where it sits

233
00:11:17,333 --> 00:11:19,132
in our environment.

234
00:11:19,200 --> 00:11:22,033
We actually could be
in the middle of the

235
00:11:22,100 --> 00:11:25,266
development of a drug,
and there is machinery in

236
00:11:25,333 --> 00:11:27,199
there that you
can't patch.

237
00:11:27,266 --> 00:11:28,933
You cannot patch
at had that time.

238
00:11:29,000 --> 00:11:32,399
That's where I talk about
the network isolation,

239
00:11:32,466 --> 00:11:37,533
because if it cannot talk
to the internet or it is not

240
00:11:37,600 --> 00:11:40,766
on the network and it is
in a PLC, I'm going to be

241
00:11:40,833 --> 00:11:44,033
far less worried about
getting a patch to it

242
00:11:44,100 --> 00:11:48,899
versus the machines, the
PCs that people use.

243
00:11:48,966 --> 00:11:53,799
In anyone's network, you
know, 99.99% of your

244
00:11:53,866 --> 00:11:58,299
network likely can be patched within hours, if necessary.

245
00:11:58,366 --> 00:12:03,366
It is going to be the long
tail in manufacturing and

246
00:12:03,433 --> 00:12:06,000
where the technology
is used where that

247
00:12:06,066 --> 00:12:07,600
vulnerability is.

248
00:12:07,666 --> 00:12:11,633
In some of the devices
that we have seen in

249
00:12:11,700 --> 00:12:16,100
healthcare, it depends
exactly where it is, and

250
00:12:16,166 --> 00:12:22,066
sometimes the risk of
patching is worse than patching.

251
00:12:22,133 --> 00:12:25,500
You have to weigh that
and take a risk-based approach.

252
00:12:25,566 --> 00:12:30,600
That, you know, it is very
easy for just patch everything.

253
00:12:30,666 --> 00:12:34,799
If you have a pacemaker
that has the vulnerability

254
00:12:34,866 --> 00:12:37,866
and it is inside a human
and you don't know what's

255
00:12:37,933 --> 00:12:40,600
going to happen to it
or you actually have to

256
00:12:40,666 --> 00:12:42,866
extract it out of the
human, then take a

257
00:12:42,933 --> 00:12:47,165
completely different view
and have a conversation of

258
00:12:47,233 --> 00:12:50,733
is it a health safety
issue because safety is

259
00:12:50,799 --> 00:12:53,466
number one, and then
it is going to be

260
00:12:53,533 --> 00:12:56,399
reputation and finance.

261
00:12:56,466 --> 00:12:59,799
Patient safety, for us,
ends up being our premier

262
00:12:59,866 --> 00:13:01,165
thing that we are.

263
00:13:01,233 --> 00:13:03,366
And every time one of
these come out, I have to

264
00:13:03,433 --> 00:13:05,966
go through the entire bill
of materials of all our

265
00:13:06,033 --> 00:13:09,233
medical devices and look
at where it is, how it is

266
00:13:09,299 --> 00:13:13,199
utilized, look at our
factories across the world

267
00:13:13,266 --> 00:13:17,199
and determine and decide
where we're going to patch.

268
00:13:17,266 --> 00:13:19,065
We have seen some of the
patches that have come out

269
00:13:19,133 --> 00:13:25,899
that have not patching was
less worse than the patching.

270
00:13:25,966 --> 00:13:30,633
And then also even in
instances as we see now in

271
00:13:30,700 --> 00:13:33,399
China with everybody
working from home, and

272
00:13:33,466 --> 00:13:37,266
we'll see this in North
Korea - in Korea and Japan

273
00:13:37,333 --> 00:13:41,199
and other countries also,
do the countries have the

274
00:13:41,266 --> 00:13:45,233
bandwidth to be
able to patch?

275
00:13:45,299 --> 00:13:47,165
What if you can't
get it there?

276
00:13:47,233 --> 00:13:50,565
Is it better to have your
people not getting a bad

277
00:13:50,633 --> 00:13:54,166
patch and being bricked,
or is it better to take

278
00:13:54,233 --> 00:13:58,132
the risk and having the
ability to tell if you

279
00:13:58,200 --> 00:14:00,233
have a problem
starting to develop?

280
00:14:00,299 --> 00:14:02,299
>> DONNA DODSON: Do you
get the right kinds of

281
00:14:02,366 --> 00:14:07,899
information that you need to make that kind of risk decision?

282
00:14:07,966 --> 00:14:09,266
>> MARENE ALLISON: Yes,
for the most part.

283
00:14:09,333 --> 00:14:10,333
There are a few.

284
00:14:10,399 --> 00:14:12,966
I think one of the
patches, it was WannaCry

285
00:14:13,033 --> 00:14:16,233
or Heartbleed, that
literally the patch came

286
00:14:16,299 --> 00:14:18,966
out and it was more
dangerous to apply it

287
00:14:19,033 --> 00:14:21,266
right away than not.

288
00:14:21,333 --> 00:14:25,899
Our process, mainly
because we have the

289
00:14:25,966 --> 00:14:28,899
healthcare manufacturing
and we have to go through

290
00:14:28,966 --> 00:14:32,166
such a process, it became
known that the patch was

291
00:14:32,233 --> 00:14:35,165
worse than the not
patching, and so we were

292
00:14:35,233 --> 00:14:37,532
able to pull back
and make a decision.

293
00:14:37,600 --> 00:14:40,766
We take a risk-based
approach on it and talk

294
00:14:40,833 --> 00:14:44,165
about it long and hard and
then where do we need to go.

295
00:14:44,233 --> 00:14:47,032
There will be some things,
there will be components

296
00:14:47,100 --> 00:14:50,266
of PLCs out in our
factories that are very

297
00:14:50,333 --> 00:14:52,966
old technology.

298
00:14:53,033 --> 00:14:55,033
As long as it doesn't
have a connection to the

299
00:14:55,100 --> 00:14:56,966
intent, let it run.

300
00:14:57,033 --> 00:14:57,700
Let it run, baby.

301
00:14:57,766 --> 00:14:58,966
Keep her going.

302
00:14:59,033 --> 00:15:04,733
Because if it doesn't have
the vulnerability, I don't

303
00:15:04,799 --> 00:15:05,933
need to patch it.

304
00:15:07,366 --> 00:15:08,699
>> PHIL VENABLES:
It's interesting.

305
00:15:08,766 --> 00:15:13,000
I think there's not a lot
of consideration given to

306
00:15:13,066 --> 00:15:15,866
that operational
technology issue.

307
00:15:15,933 --> 00:15:17,333
That sentiment
I hear a lot.

308
00:15:17,399 --> 00:15:19,566
We have a lot of companies
that we're invested in in

309
00:15:19,633 --> 00:15:21,833
the energy and health and
industrial manufacturing

310
00:15:21,899 --> 00:15:24,266
sectors, and this whole
push of like why can't

311
00:15:24,333 --> 00:15:25,366
they just patch.

312
00:15:25,433 --> 00:15:28,333
Also, not only those
operational risks of doing

313
00:15:28,399 --> 00:15:30,733
it, but also in many
sectors, there are some

314
00:15:30,799 --> 00:15:34,000
pretty strict regulations
about making sure that

315
00:15:34,066 --> 00:15:36,833
there is full system
testing of software that's

316
00:15:36,899 --> 00:15:39,833
upgraded to guarantee the
operational performance of

317
00:15:39,899 --> 00:15:41,200
those devices,
and that includes,

318
00:15:41,266 --> 00:15:42,599
in many cases, patches.

319
00:15:42,666 --> 00:15:46,766
There are many situations
where we're - industry

320
00:15:46,833 --> 00:15:48,833
regulations, in some
cases, are conspiring

321
00:15:48,899 --> 00:15:51,299
against the ability of
manufacturing companies to

322
00:15:51,366 --> 00:15:54,333
upgrade safety critical
equipment in ways.

323
00:15:54,399 --> 00:15:56,233
This kind of points to
this broader thing that I

324
00:15:56,299 --> 00:15:58,366
think, again, you will all
appreciate because you're

325
00:15:58,433 --> 00:16:01,633
at a conference like this
is oftentimes companies,

326
00:16:01,700 --> 00:16:04,333
apart from those issues,
are not deciding not to

327
00:16:04,399 --> 00:16:08,299
patch; they're making
those decisions and trade-offs.

328
00:16:08,366 --> 00:16:10,866
And a lot of companies now
over many years now, I

329
00:16:10,933 --> 00:16:12,566
think a lot of you all
have done, is invested

330
00:16:12,633 --> 00:16:15,766
quite heavily in the
software update pipelines

331
00:16:15,833 --> 00:16:17,866
and the change management
processes so that they're

332
00:16:17,933 --> 00:16:20,433
getting better at updating
and releasing software

333
00:16:20,500 --> 00:16:21,399
for all reasons.

334
00:16:21,466 --> 00:16:24,133
Once you've done that, you
can take more risk with

335
00:16:24,200 --> 00:16:26,366
introducing patches
because you can then test

336
00:16:26,433 --> 00:16:28,433
it quicker and roll it
back if there's a problem.

337
00:16:28,500 --> 00:16:31,633
Engineering our
environment to have the

338
00:16:31,700 --> 00:16:36,766
more inherent ability
to patch and update is

339
00:16:36,833 --> 00:16:39,799
probably a more important
thing over time than just

340
00:16:39,866 --> 00:16:41,399
worrying about
individual patches.

341
00:16:41,466 --> 00:16:43,866
I think figuring out a
way to get patches in the

342
00:16:43,933 --> 00:16:46,100
environment as opposed
to so you can put more

343
00:16:46,166 --> 00:16:48,333
patches in the environment
and more software updates

344
00:16:48,399 --> 00:16:50,133
is the critical problem.

345
00:16:50,200 --> 00:16:51,799
>> PAUL KOCHER: As you
look at the long tail of

346
00:16:51,866 --> 00:16:55,100
devices, the patching
approach also doesn't

347
00:16:55,166 --> 00:16:55,899
work very well.

348
00:16:55,966 --> 00:16:57,633
For the really big
companies, the Microsofts,

349
00:16:57,700 --> 00:17:01,033
the Apples, they can get a
high-quality, well-tested

350
00:17:01,100 --> 00:17:02,333
patch out pretty quickly.

351
00:17:02,399 --> 00:17:07,133
When you look at what the
patch production process

352
00:17:07,200 --> 00:17:10,098
looks like for a small
company, you have a whole

353
00:17:10,165 --> 00:17:11,532
bunch of things that
are very dangerous.

354
00:17:11,598 --> 00:17:13,733
You have developer
laptops or workstations

355
00:17:13,799 --> 00:17:14,933
with poor hygiene.

356
00:17:15,000 --> 00:17:17,433
You may have distribution
channels that are insecure.

357
00:17:17,500 --> 00:17:21,733
The question of what are
your risks that you're

358
00:17:21,799 --> 00:17:23,965
incurring through just
getting that patch are

359
00:17:24,032 --> 00:17:27,532
also non-trivial, and
it's not just functional

360
00:17:27,598 --> 00:17:29,433
problems but security
issues as well.

361
00:17:29,500 --> 00:17:32,433
>> DONNA DODSON: So we
have talked, you know,

362
00:17:32,500 --> 00:17:37,299
kind of at the end of
the lifecycle here with

363
00:17:37,366 --> 00:17:40,200
vulnerabilities and the
answer is a risk-based

364
00:17:40,266 --> 00:17:42,533
approach to patching.

365
00:17:42,599 --> 00:17:47,566
Certainly, there are many
things that we should be

366
00:17:47,633 --> 00:17:51,833
doing kind of upstream
from that, right?

367
00:17:51,900 --> 00:17:54,433
When we're looking at
products and services,

368
00:17:54,500 --> 00:17:57,933
when we're working with
our suppliers, when we're

369
00:17:58,000 --> 00:18:03,099
buying products, how can
we be better consumers?

370
00:18:03,166 --> 00:18:07,533
What are some things that
users and consumers can do

371
00:18:07,599 --> 00:18:11,899
to understand and minimize
and then, if needed,

372
00:18:11,966 --> 00:18:15,299
mitigate the risks
from vulnerabilities?

373
00:18:15,366 --> 00:18:18,433
How do we get
ahead of this game?

374
00:18:18,500 --> 00:18:19,933
>> MARENE ALLISON: Part of
it is understanding the

375
00:18:20,000 --> 00:18:22,200
bill of materials that's
going in, and do you

376
00:18:22,266 --> 00:18:23,866
need the component.

377
00:18:23,933 --> 00:18:26,900
The one that - I don't
know why every product

378
00:18:26,966 --> 00:18:30,366
from Microsoft has to have
IE in it, even if I don't

379
00:18:30,433 --> 00:18:32,366
want it to go
on the internet.

380
00:18:32,433 --> 00:18:37,700
I find all the time that
manufacturers think that's

381
00:18:37,766 --> 00:18:40,366
just a great
feature to have.

382
00:18:40,433 --> 00:18:44,533
We started going through
with our operational

383
00:18:44,599 --> 00:18:48,133
technology organization
and looking at are there

384
00:18:48,200 --> 00:18:51,633
pieces we don't need and
really looking at what are

385
00:18:51,700 --> 00:18:54,633
the vulnerable pieces of
the software and hardware

386
00:18:54,700 --> 00:18:58,366
we're now getting and
why are they there.

387
00:18:58,433 --> 00:19:03,166
We're also looking at for
us as we build products is

388
00:19:03,233 --> 00:19:08,500
what are those pieces we
need and when do we need them.

389
00:19:08,566 --> 00:19:12,633
You have to remember some
devices that you build and

390
00:19:12,700 --> 00:19:16,700
create actually go
into others' networks.

391
00:19:16,766 --> 00:19:19,900
And then it's how to look
at how are they secure and

392
00:19:19,966 --> 00:19:23,466
how you will operationally
deliver patches when necessary.

393
00:19:23,533 --> 00:19:27,866
There are a lot
of layers to it.

394
00:19:27,933 --> 00:19:30,166
Probably the easiest
one would be build it

395
00:19:30,233 --> 00:19:32,200
secure up front.

396
00:19:32,266 --> 00:19:34,266
>> DONNA DODSON:
How do you do that?

397
00:19:34,333 --> 00:19:35,433
Phil, how do you do that?

398
00:19:35,500 --> 00:19:36,766
>> PHIL VENABLES: It's
interesting because I

399
00:19:36,833 --> 00:19:38,866
think, more broadly, the
way you were going with

400
00:19:38,933 --> 00:19:43,433
this was I think a lot of
organizations underutilize

401
00:19:43,500 --> 00:19:47,133
architectural choices to
mitigate vulnerabilities.

402
00:19:47,200 --> 00:19:49,500
We're always going to have
- we all know software is

403
00:19:49,566 --> 00:19:50,733
always going to have bugs.

404
00:19:50,799 --> 00:19:52,566
Some of those bugs are
going to be vulnerabilities.

405
00:19:52,633 --> 00:19:53,900
Some of those
vulnerabilities are going

406
00:19:53,966 --> 00:19:55,899
to be on exploit path.

407
00:19:55,966 --> 00:19:57,933
Therefore, you've just
got to live with that and

408
00:19:58,000 --> 00:20:00,200
you've got to update
the environment.

409
00:20:00,266 --> 00:20:03,200
But we should be less
susceptible to one

410
00:20:03,266 --> 00:20:06,333
software flaw being
somehow creating a high

411
00:20:06,400 --> 00:20:09,566
risk in our operational
environments or supply

412
00:20:09,633 --> 00:20:11,599
chains or our customer
facing business or

413
00:20:11,666 --> 00:20:12,933
anywhere else.

414
00:20:13,000 --> 00:20:14,599
I think there are a lot
of things we can do to

415
00:20:14,666 --> 00:20:18,066
deploy, to place different
components of different

416
00:20:18,133 --> 00:20:20,766
systems in ways that it's
a bit like what they call,

417
00:20:20,833 --> 00:20:22,766
in risk, they call it the
Swiss cheese model, which

418
00:20:22,833 --> 00:20:23,799
sounds like a bad thing.

419
00:20:23,866 --> 00:20:25,900
Essentially, you line
up all the holes of

420
00:20:25,966 --> 00:20:27,399
something, you should
never be able to get one

421
00:20:27,466 --> 00:20:29,565
thing straight through all
the holes in the system.

422
00:20:29,633 --> 00:20:31,966
It should line up in a way
that there is at least one

423
00:20:32,033 --> 00:20:34,500
line of defense because
of architectural choices.

424
00:20:34,566 --> 00:20:37,400
I think that's an
under-researched,

425
00:20:37,466 --> 00:20:40,433
understudied problem.

426
00:20:40,500 --> 00:20:42,400
I know a lot of
organizations are having a

427
00:20:42,466 --> 00:20:44,666
lot of success with those
architectural choices that

428
00:20:44,733 --> 00:20:47,399
give them the flexibility
to change controls and not

429
00:20:47,466 --> 00:20:49,199
be susceptible to
any one failure.

430
00:20:49,266 --> 00:20:51,599
I think it is easy to say
everybody should just

431
00:20:51,666 --> 00:20:53,466
produce more secure
software and a lot of

432
00:20:53,533 --> 00:20:55,265
organizations are spending
a lot of time on that.

433
00:20:55,333 --> 00:20:58,066
It is generally hard,
but if they do that in

434
00:20:58,133 --> 00:21:00,799
conjunction with thinking
about architectural choices.

435
00:21:00,866 --> 00:21:03,000
One of the things is when
you look at some of the

436
00:21:03,066 --> 00:21:06,400
larger data breaches that
have occurred over the

437
00:21:06,466 --> 00:21:09,132
past decade, sometimes
you look at that and the

438
00:21:09,200 --> 00:21:13,366
companies that were
vulnerable to that, they

439
00:21:13,433 --> 00:21:15,599
essentially had a
compromise because of one

440
00:21:15,666 --> 00:21:16,799
control problem, one flaw,
one unpatched system.

441
00:21:16,866 --> 00:21:23,400
You think how can that one
thing result in that huge

442
00:21:23,466 --> 00:21:26,099
event as opposed to
there being some kind of

443
00:21:26,166 --> 00:21:29,500
segmentation, separation,
data minimization, other

444
00:21:29,566 --> 00:21:31,966
choices that they have
made to mitigate the

445
00:21:32,033 --> 00:21:33,299
impact of the one event.

446
00:21:33,366 --> 00:21:35,666
That whole kind of
reducing the blast radius

447
00:21:35,733 --> 00:21:38,033
of events, I think, is
something that we need to

448
00:21:38,099 --> 00:21:40,599
do more of from the
hardware up through the

449
00:21:40,666 --> 00:21:43,200
overall end-to-end systems
architecture, I think, is

450
00:21:43,266 --> 00:21:44,566
going to be something
that we'll see a

451
00:21:44,633 --> 00:21:46,933
lot more companies doing.

452
00:21:47,000 --> 00:21:49,000
>> PAUL KOCHER: I look to
how other industries solve

453
00:21:49,066 --> 00:21:51,599
problems when I come up
with one, looking at how

454
00:21:51,666 --> 00:21:54,399
aviation, for example,
gets pretty good safety

455
00:21:54,466 --> 00:21:57,366
overall or how structural
engineers work.

456
00:21:57,433 --> 00:21:59,133
All of the other
industries that try to get

457
00:21:59,200 --> 00:22:02,099
something to be reliable
and safe rely on redundancy.

458
00:22:02,166 --> 00:22:03,466
And what you're talking
about with getting

459
00:22:03,533 --> 00:22:05,866
multiple layers there,
it is fundamentally an

460
00:22:05,933 --> 00:22:09,233
approach based
on redundancy.

461
00:22:09,299 --> 00:22:11,466
If you look in contrast
at your typical piece of

462
00:22:11,533 --> 00:22:13,899
software or piece of
hardware, there are

463
00:22:13,966 --> 00:22:16,899
hundreds of millions of
lines of code, billions of

464
00:22:16,966 --> 00:22:19,500
transistors, every single
one of which is a single

465
00:22:19,566 --> 00:22:20,566
point of failure.

466
00:22:20,633 --> 00:22:23,766
We have not figured out
correctly how to use redundancy.

467
00:22:23,833 --> 00:22:25,833
There is some
preconditions.

468
00:22:25,900 --> 00:22:27,466
Certainly, you need memory
safety, because if you

469
00:22:27,533 --> 00:22:29,966
have unsafe programming
languages where any line

470
00:22:30,033 --> 00:22:32,466
of code can destroy the
security of anything on

471
00:22:32,533 --> 00:22:34,966
the entire process, you're
not going to be able to

472
00:22:35,033 --> 00:22:39,332
get any sort of redundancy
even after you have memory

473
00:22:39,400 --> 00:22:40,466
safe languages.

474
00:22:40,533 --> 00:22:41,533
It's just not used.

475
00:22:41,599 --> 00:22:46,299
If you had had redundant
verification of

476
00:22:46,366 --> 00:22:48,700
certificates, for example,
that would have addressed

477
00:22:48,766 --> 00:22:51,500
both Microsoft CrypotAPI
issue as well as like the

478
00:22:51,566 --> 00:22:54,133
Apple's Go To Fail and
a number of other bugs.

479
00:22:54,200 --> 00:22:55,366
And so when you have
something that's really

480
00:22:55,433 --> 00:22:58,566
critical like verifying a
cert that came in from the

481
00:22:58,633 --> 00:23:01,066
wilds of the internet,
having two pieces of

482
00:23:01,133 --> 00:23:04,033
software check that has
very little cost compared

483
00:23:04,099 --> 00:23:06,399
to the cost of having
extra structural supports

484
00:23:06,466 --> 00:23:08,966
in this building, yet
we're not doing it and we

485
00:23:09,033 --> 00:23:11,265
need to start doing
things like that.

486
00:23:11,333 --> 00:23:14,333
>> DONNA DODSON: So you
talked earlier about

487
00:23:14,400 --> 00:23:22,833
things like Spectre and
Meltdown and perhaps we

488
00:23:22,900 --> 00:23:27,766
really need to rethink
our security models.

489
00:23:27,833 --> 00:23:30,633
Redundancy is one of them.

490
00:23:30,700 --> 00:23:32,566
But what else do
we need to do?

491
00:23:32,633 --> 00:23:34,599
>> PAUL KOCHER: One of the
themes of this conference

492
00:23:34,666 --> 00:23:36,933
is the human problem.

493
00:23:37,000 --> 00:23:39,900
All of us people who are
older, and I'm starting to

494
00:23:39,966 --> 00:23:43,765
reach that age here,
grew up in a world where

495
00:23:43,833 --> 00:23:46,733
security didn't really
matter compared to the

496
00:23:46,799 --> 00:23:49,299
economic benefits of making things faster and cheaper.

497
00:23:49,366 --> 00:23:51,866
Those were where
you made your money.

498
00:23:51,933 --> 00:23:53,333
That's where you
were successful.

499
00:23:53,400 --> 00:23:54,666
That was where you
had the big wins.

500
00:23:54,733 --> 00:23:57,200
Security problems were
an interesting academic

501
00:23:57,266 --> 00:23:58,666
question, maybe they would
show up in the<i> </i>New York

502
00:23:58,733 --> 00:24:00,866
Times as a thing people
were complaining about,

503
00:24:00,933 --> 00:24:04,900
but it wasn't really
the big issue.

504
00:24:04,966 --> 00:24:06,565
All of the thought
patterns that were

505
00:24:06,633 --> 00:24:08,900
developed during this
period where security was

506
00:24:08,966 --> 00:24:11,466
insignificant
have stuck around.

507
00:24:11,533 --> 00:24:14,000
I met with somebody
working on one of the

508
00:24:14,066 --> 00:24:16,200
major browsers and was
talking about a security

509
00:24:16,266 --> 00:24:18,366
mechanism that would add
around a millisecond per

510
00:24:18,433 --> 00:24:22,033
page load, which was
utterly unacceptable.

511
00:24:22,099 --> 00:24:24,399
I mean, that might cost
you a whole second per

512
00:24:24,466 --> 00:24:27,565
thousand page loads
you do over your day.

513
00:24:27,633 --> 00:24:29,000
I actually suspect there
are people in this

514
00:24:29,066 --> 00:24:32,933
audience who would be very
happy if their employees

515
00:24:33,000 --> 00:24:35,500
had another millisecond
on each page load and had

516
00:24:35,566 --> 00:24:37,299
significantly better
security as a result.

517
00:24:37,366 --> 00:24:40,000
But there is this mantra
that page load times

518
00:24:40,066 --> 00:24:42,000
matter more than
security does.

519
00:24:42,066 --> 00:24:45,000
We have to change those
beliefs that were formed

520
00:24:45,066 --> 00:24:49,966
in the prior world where
performance gains were

521
00:24:50,033 --> 00:24:53,065
huge and security was
costing millions, not

522
00:24:53,133 --> 00:24:56,000
hundreds of billions to
trillions of dollars, in

523
00:24:56,066 --> 00:24:58,799
order to be able to make
the investments needed to

524
00:24:58,866 --> 00:25:00,766
go and actually address
a lot of these problems.

525
00:25:00,833 --> 00:25:02,900
>> PHIL VENABLES: That's a
great example because it

526
00:25:02,966 --> 00:25:08,199
also points to the lack
of an overall mature risk

527
00:25:08,266 --> 00:25:10,666
discipline to think
about those decisions.

528
00:25:10,733 --> 00:25:13,265
That browser example
is a perfect example.

529
00:25:13,333 --> 00:25:16,633
Because nobody knows how
that one millisecond adds

530
00:25:16,700 --> 00:25:21,400
up over a potential set of
other activities in and

531
00:25:21,466 --> 00:25:23,399
around the browser because
of how people may have

532
00:25:23,466 --> 00:25:26,733
built software on it,
everybody's only choice is

533
00:25:26,799 --> 00:25:29,633
to default to the
not give anything up.

534
00:25:29,700 --> 00:25:31,799
But if there was a broader
environment to look at

535
00:25:31,866 --> 00:25:33,566
what the consequences
of that would be, the

536
00:25:33,633 --> 00:25:36,299
developers themselves may
realize it is not actually

537
00:25:36,366 --> 00:25:38,400
consequential and therefore
they trade for safety.

538
00:25:38,466 --> 00:25:39,733
>> PAUL KOCHER: There's
actually a test that I'm

539
00:25:39,799 --> 00:25:40,533
trying to get
somebody to do.

540
00:25:40,599 --> 00:25:42,700
Should we offer two
versions of a software, a

541
00:25:42,766 --> 00:25:45,599
faster version and a safer
version, and just see what

542
00:25:45,666 --> 00:25:46,899
the population does.

543
00:25:46,966 --> 00:25:48,533
I don't know if I can ask
for hands in the room.

544
00:25:48,599 --> 00:25:50,265
How many of you would
download the safer version

545
00:25:50,333 --> 00:25:51,900
of a browser?

546
00:25:51,966 --> 00:25:53,933
How many would go for
the faster version?

547
00:25:55,933 --> 00:25:56,666
>> MARENE ALLISON: Not fair.

548
00:25:56,733 --> 00:25:58,200
You're at a security
conference.

549
00:26:02,666 --> 00:26:06,033
I like your
statistics, though.

550
00:26:06,099 --> 00:26:08,866
But, you know, there is
another side of this.

551
00:26:08,933 --> 00:26:12,700
Many companies, when we
looked at it, and if you

552
00:26:12,766 --> 00:26:17,900
look back 10 or 15 years,
it was how to make ITB cheaper.

553
00:26:17,966 --> 00:26:20,966
So part of that was to
consolidate, make flat

554
00:26:21,033 --> 00:26:25,533
networks, put less
friction into the system.

555
00:26:25,599 --> 00:26:28,633
Companies spent a lot of
time and money to save money.

556
00:26:28,700 --> 00:26:30,500
Right?

557
00:26:30,566 --> 00:26:32,833
They ultimately did it.

558
00:26:32,900 --> 00:26:37,833
But it wasn't until, and
we had - all the CISOs I

559
00:26:37,900 --> 00:26:40,533
know detect and protect,
and there we were out

560
00:26:40,599 --> 00:26:44,866
there putting more
technology in to look to

561
00:26:44,933 --> 00:26:47,866
detect and protect and
defend our organization,

562
00:26:47,933 --> 00:26:53,933
but somehow it wasn't
until NotPetya when I know

563
00:26:54,000 --> 00:26:57,466
myself and my other peers
that I have talked to

564
00:26:57,533 --> 00:27:01,765
went, holy crap, it is
about cyber resiliency.

565
00:27:01,833 --> 00:27:05,066
Do we have cyber
resiliency?

566
00:27:05,133 --> 00:27:08,533
There's a lot of supply
chains out there and a lot

567
00:27:08,599 --> 00:27:11,799
of companies and the OT
elements of what was being

568
00:27:11,866 --> 00:27:15,866
done there was not seen
as, in some companies, not

569
00:27:15,933 --> 00:27:16,966
even related to IT.

570
00:27:17,033 --> 00:27:19,866
They were separate.

571
00:27:19,933 --> 00:27:23,299
Well, what I have seen
so far in doing some

572
00:27:23,366 --> 00:27:27,000
inventories is they
have 90% more internet

573
00:27:27,066 --> 00:27:32,533
connected equipment
than in the IT side.

574
00:27:32,599 --> 00:27:35,500
So when you start looking
at that and the numbers,

575
00:27:35,566 --> 00:27:37,433
where is the risk?

576
00:27:37,500 --> 00:27:40,933
Because it wasn't under
the purview, and some of

577
00:27:41,000 --> 00:27:43,400
it actually gets back to
organizational structure,

578
00:27:43,466 --> 00:27:48,533
where the CISO reports to
and the like, but looking

579
00:27:48,599 --> 00:27:52,832
at company resilience
overall, certainly I'm

580
00:27:52,900 --> 00:27:56,133
sure the Sony breach and
Home Depot breach and on

581
00:27:56,200 --> 00:28:02,266
and on and on, but not to
NotPetya did everybody

582
00:28:02,333 --> 00:28:07,533
have a wake-up call and
say, oh, wow, how I

583
00:28:07,599 --> 00:28:12,233
operate can create
the vulnerability.

584
00:28:12,299 --> 00:28:14,799
Once that occurred and
companies started looking

585
00:28:14,866 --> 00:28:17,866
at their supply chains, it
was like a rush to supply chain.

586
00:28:17,933 --> 00:28:20,500
I was at the Executive
Security Action Forum

587
00:28:20,566 --> 00:28:24,900
yesterday with 150 CISOs
and the amount of people

588
00:28:24,966 --> 00:28:28,866
who were doing OT security
exponentially has

589
00:28:28,933 --> 00:28:30,633
increased in the
last two years.

590
00:28:30,700 --> 00:28:32,166
Exponentially.

591
00:28:32,233 --> 00:28:35,666
People were literally
looking at the stats from

592
00:28:35,733 --> 00:28:39,966
a few years ago, we're
barely scratching the surface.

593
00:28:40,033 --> 00:28:43,632
Today, almost every CISO has
it on their plate to look at.

594
00:28:43,700 --> 00:28:47,166
I think this is some of
that area of actually

595
00:28:47,233 --> 00:28:49,633
understanding that
redundancy, what are you

596
00:28:49,700 --> 00:28:51,966
going to need to do.

597
00:28:52,033 --> 00:28:56,065
And certainly companies
cannot afford to have 100%

598
00:28:56,133 --> 00:28:59,766
perfect security in 100%
of locations with full

599
00:28:59,833 --> 00:29:03,333
redundancy, but what you
need to do now is back

600
00:29:03,400 --> 00:29:07,233
into some of the business
continuity planning, your

601
00:29:07,299 --> 00:29:10,666
business impact analysis,
what is important to your

602
00:29:10,733 --> 00:29:13,799
company, what are
your crown jewels?

603
00:29:13,866 --> 00:29:16,799
And if something had to go
down, what are you willing

604
00:29:16,866 --> 00:29:18,833
to take a risk on?

605
00:29:18,900 --> 00:29:22,133
I hear a lot is nothing.

606
00:29:22,200 --> 00:29:24,099
It is all going to be up.

607
00:29:24,166 --> 00:29:27,599
Well, if it is, then start
taking out the checkbook

608
00:29:27,666 --> 00:29:30,133
because that
will cost a lot.

609
00:29:30,200 --> 00:29:33,700
I think between the, okay,
I'm not going to be able

610
00:29:33,766 --> 00:29:37,833
to patch everything in two
days, I'm not going to be

611
00:29:37,900 --> 00:29:41,900
able to have full
redundancy on everything

612
00:29:41,966 --> 00:29:45,265
in my entire corporation,
it starts to be more of a

613
00:29:45,333 --> 00:29:48,500
risk approach that people
have to understand in

614
00:29:48,566 --> 00:29:50,133
their organizations.

615
00:29:50,200 --> 00:29:52,933
>> PAUL KOCHER: Another
sort of organizational

616
00:29:53,000 --> 00:29:56,933
structure issue that I
have come into a number of

617
00:29:57,000 --> 00:29:58,799
different directions is
that you often will find

618
00:29:58,866 --> 00:30:01,900
companies that have a
lot of complex, often

619
00:30:01,966 --> 00:30:03,366
technical work going on
but with no security

620
00:30:03,433 --> 00:30:05,766
knowledge within that
particular function.

621
00:30:05,833 --> 00:30:09,299
This was certainly part
of why Spectre didn't get

622
00:30:09,366 --> 00:30:11,400
caught earlier is the
people working on things

623
00:30:11,466 --> 00:30:13,599
like branch predictors
weren't asking themselves

624
00:30:13,666 --> 00:30:15,332
what are the security
implications of this.

625
00:30:15,400 --> 00:30:17,133
They were only asking
questions about the

626
00:30:17,200 --> 00:30:19,500
performance implications
of what they were doing.

627
00:30:19,566 --> 00:30:22,900
As soon as somebody comes
along from the outside or

628
00:30:22,966 --> 00:30:24,899
from somewhere and says,
hey, let's think about the

629
00:30:24,966 --> 00:30:27,433
security implications of
what we're doing in these

630
00:30:27,500 --> 00:30:28,599
different components.

631
00:30:28,666 --> 00:30:30,765
Suddenly, a whole bunch of
questions get asked and

632
00:30:30,833 --> 00:30:33,866
things get discovered that
were being neglected before.

633
00:30:33,933 --> 00:30:36,500
And so making sure that
there is enough security

634
00:30:36,566 --> 00:30:39,900
knowledge in the
various groups in the

635
00:30:39,966 --> 00:30:41,733
organization, so if
something is going really

636
00:30:41,799 --> 00:30:43,866
wrong, that the
information gets fed back

637
00:30:43,933 --> 00:30:49,133
up, or if the assumptions
about what security

638
00:30:49,200 --> 00:30:52,233
properties a given
component or job function

639
00:30:52,299 --> 00:30:54,433
needs to provide have
changed, that there is

640
00:30:54,500 --> 00:30:55,966
some communication
about that.

641
00:30:56,033 --> 00:30:57,199
>> MARENE ALLISON: But,
you know, it's much easier

642
00:30:57,266 --> 00:30:58,700
to be a security engineer.

643
00:30:58,766 --> 00:31:01,099
I love being a security
engineer and doing that

644
00:31:01,166 --> 00:31:04,099
work, and I typically do
it with other engineers

645
00:31:04,166 --> 00:31:05,099
and IT people.

646
00:31:05,166 --> 00:31:07,832
They listen and we have
good conversations.

647
00:31:07,900 --> 00:31:10,733
It is harder to be a
security consultant to the

648
00:31:10,799 --> 00:31:13,933
business, especially in
the digital area where you

649
00:31:14,000 --> 00:31:17,233
have a lot of people
looking at big data and AI

650
00:31:17,299 --> 00:31:21,766
and the next best thing
that they're creating, and

651
00:31:21,833 --> 00:31:24,400
you may have to cause a
little bit of friction to

652
00:31:24,466 --> 00:31:27,500
have the conversation.

653
00:31:27,566 --> 00:31:30,233
As an engineer, am I
equipped to have that

654
00:31:30,299 --> 00:31:34,099
business discussion around
making sure things are

655
00:31:34,166 --> 00:31:36,200
secure and what that
would look like to my

656
00:31:36,266 --> 00:31:36,966
corporation?

657
00:31:37,033 --> 00:31:39,799
That is a big part of it.

658
00:31:39,866 --> 00:31:44,900
Hey, you know, again, the ESAF, there was a question.

659
00:31:44,966 --> 00:31:49,366
Over 50% of the CISOs
spend 80% of the time

660
00:31:49,433 --> 00:31:52,866
talking to their team and
20% of the time talking to

661
00:31:52,933 --> 00:31:54,799
their companies.

662
00:31:54,866 --> 00:31:58,700
The reality is a CISO
needs to spend 80% of the

663
00:31:58,766 --> 00:32:01,866
time talking to the
business and 20% to their

664
00:32:01,933 --> 00:32:04,900
team because our
job is consulting.

665
00:32:04,966 --> 00:32:08,832
Hey, the engineering
pot is real fun.

666
00:32:08,900 --> 00:32:09,733
I love that.

667
00:32:09,799 --> 00:32:13,266
But the most important job
we do is consulting with

668
00:32:13,333 --> 00:32:17,033
our businesses so that we
can do the right product.

669
00:32:17,099 --> 00:32:18,899
>> PHIL VENABLES: I think,
as well, I think to both

670
00:32:18,966 --> 00:32:22,332
of your points, it is
important to equip teams

671
00:32:22,400 --> 00:32:24,799
with the ability to not
just think about security

672
00:32:24,866 --> 00:32:26,933
risk or not just think
about what their

673
00:32:27,000 --> 00:32:31,633
productivity goal is for
the particular product

674
00:32:31,700 --> 00:32:33,266
they're designing, but
actually to think about

675
00:32:33,333 --> 00:32:34,833
all of the
spectrum of risk.

676
00:32:34,900 --> 00:32:38,133
There is software
reliability risk, capacity

677
00:32:38,200 --> 00:32:40,433
risk, resiliency
risk, security.

678
00:32:40,500 --> 00:32:43,500
In different industries,
there are different risks

679
00:32:43,566 --> 00:32:46,566
of whether it is
finance, health, energy.

680
00:32:46,633 --> 00:32:48,099
There are all sorts
of different things.

681
00:32:48,166 --> 00:32:49,832
Figuring out what the
right engineering

682
00:32:49,900 --> 00:32:52,333
production and quality
process is to make sure

683
00:32:52,400 --> 00:32:54,133
that all of those risks
are being considered

684
00:32:54,200 --> 00:32:56,099
because there has been
plenty of examples in many

685
00:32:56,166 --> 00:32:58,265
companies where companies
have made the right

686
00:32:58,333 --> 00:33:01,200
decision on security and
they've push security down

687
00:33:01,266 --> 00:33:03,700
and it has created
unintended brittleness

688
00:33:03,766 --> 00:33:07,000
that spiked the resiliency
risk that, in hindsight,

689
00:33:07,066 --> 00:33:09,366
because of some resiliency
issue, they wished they'd

690
00:33:09,433 --> 00:33:11,233
have made a slightly
different trade-off and

691
00:33:11,299 --> 00:33:13,700
maybe accepted some
insecurity in the name of

692
00:33:13,766 --> 00:33:16,299
resiliency because, for
that particular product,

693
00:33:16,366 --> 00:33:19,700
the most important thing was
a certain resiliency approach.

694
00:33:19,766 --> 00:33:23,266
And so thinking about how
organizations manage their

695
00:33:23,333 --> 00:33:24,633
- for want of a better
phrase - their risk

696
00:33:24,700 --> 00:33:27,066
appetite, what's the risk
envelope for a set of

697
00:33:27,133 --> 00:33:29,400
risks for a particular
product in a particular

698
00:33:29,466 --> 00:33:32,500
business context.

699
00:33:32,566 --> 00:33:35,000
Some organizations are
quite mature on that and

700
00:33:35,066 --> 00:33:35,933
certain industries.

701
00:33:36,000 --> 00:33:37,900
But, generally speaking,
how you manage that

702
00:33:37,966 --> 00:33:41,866
interplay between risks
is quite underutilized in

703
00:33:41,933 --> 00:33:43,799
many organizations.

704
00:33:43,866 --> 00:33:47,799
>> DONNA DODSON: Phil,
when you were a little bit

705
00:33:47,866 --> 00:33:51,233
more boots on the ground
than you are today, how

706
00:33:51,299 --> 00:33:56,066
did you manage resiliency
risk, enterprise risk,

707
00:33:56,133 --> 00:33:59,733
customer risk, and
cybersecurity risk?

708
00:33:59,799 --> 00:34:00,400
>> PHIL VENABLES: Yeah.

709
00:34:00,466 --> 00:34:02,832
So a bit of background.

710
00:34:02,900 --> 00:34:05,533
I was CISO at Goldman
Sachs for 17 years, and

711
00:34:05,599 --> 00:34:08,732
then from that I moved to
be chief risk officer for

712
00:34:08,800 --> 00:34:11,666
all of our enterprise
risks, and now I do things

713
00:34:11,733 --> 00:34:13,933
for our private equity and
our corporate clients as

714
00:34:14,000 --> 00:34:15,933
well as sitting on the
board of our bank.

715
00:34:16,000 --> 00:34:18,000
I have kind of been across
each of the layers of this.

716
00:34:18,065 --> 00:34:21,000
I get the privilege of
asking difficult questions

717
00:34:21,065 --> 00:34:24,000
to my former team that I
used to have difficulty

718
00:34:24,065 --> 00:34:26,966
answering when I was on
the other side of that table.

719
00:34:27,033 --> 00:34:29,000
Generally speaking,
what we did in the

720
00:34:29,065 --> 00:34:31,466
technology organization,
we created an integrated

721
00:34:31,533 --> 00:34:32,799
technology risk team.

722
00:34:32,866 --> 00:34:36,466
We had our CISO, now a guy
some of you may know, Andy

723
00:34:36,533 --> 00:34:38,433
Osmond that we hired from
Homeland Security a few

724
00:34:38,500 --> 00:34:42,599
years ago, so he's our
CISO and head of tech risk.

725
00:34:42,666 --> 00:34:44,233
We think about managing
the security risk.

726
00:34:44,300 --> 00:34:47,033
But in the technology risk
portfolio, we think about

727
00:34:47,099 --> 00:34:49,199
change risk, production
access risk, software

728
00:34:49,266 --> 00:34:51,566
reliability, capacity,
resiliency, and so

729
00:34:51,632 --> 00:34:53,799
bringing it altogether
into one risk framework

730
00:34:53,866 --> 00:34:55,699
has enabled us to start
thinking about those

731
00:34:55,766 --> 00:34:57,166
trade-off decisions.

732
00:34:57,233 --> 00:34:59,833
And then separate from
that in our independent

733
00:34:59,900 --> 00:35:02,733
risk management function,
which reports directly up

734
00:35:02,800 --> 00:35:05,866
to our board, we have a
whole set of teams that

735
00:35:05,933 --> 00:35:08,733
not only manage our
financial risks but manage

736
00:35:08,800 --> 00:35:11,400
all of our enterprise
operational risks that act

737
00:35:11,466 --> 00:35:13,333
as a check and balance
on those individual

738
00:35:13,400 --> 00:35:16,033
engineering decisions and
that's where we apply the

739
00:35:16,099 --> 00:35:19,533
business risk context to
set a board approved risk

740
00:35:19,599 --> 00:35:22,500
appetite for how those
decisions are made.

741
00:35:22,566 --> 00:35:25,366
And then ultimately at
a board level, both our

742
00:35:25,433 --> 00:35:27,699
holding company and our
bank boards oversee

743
00:35:27,766 --> 00:35:29,433
whether that whole
framework is being

744
00:35:29,500 --> 00:35:31,866
executed effectively.

745
00:35:31,933 --> 00:35:33,400
Really, it is about
bringing all of the

746
00:35:33,466 --> 00:35:34,900
different risk
disciplines together.

747
00:35:34,966 --> 00:35:38,232
And moreover than that, in
our scenario of planning

748
00:35:38,300 --> 00:35:40,800
approaches and kind
of incident learning

749
00:35:40,866 --> 00:35:43,333
exercises and drills
and exercises, how we

750
00:35:43,400 --> 00:35:45,599
constantly prepare for
these things, it is not

751
00:35:45,666 --> 00:35:49,300
picking one particular
scenario like a data breach.

752
00:35:49,366 --> 00:35:52,166
It is usually picking a
combination of scenarios

753
00:35:52,233 --> 00:35:54,266
that you really wouldn't
like to have happen at the

754
00:35:54,333 --> 00:35:56,633
same time, and then say
those are going to happen

755
00:35:56,699 --> 00:35:58,466
at the same time,
let's see how we do.

756
00:35:58,533 --> 00:36:00,866
That kind of tests
the edges of things.

757
00:36:00,933 --> 00:36:02,866
Bringing all of the risk
functions together for us

758
00:36:02,933 --> 00:36:05,000
has been - I think
most large financial

759
00:36:05,066 --> 00:36:07,566
institutions do this
whether they choose to or

760
00:36:07,633 --> 00:36:09,732
whether regulators ask
them to do it is a fairly

761
00:36:09,800 --> 00:36:11,366
useful practice.

762
00:36:11,433 --> 00:36:13,300
I think the challenges are
there is still a lot of

763
00:36:13,366 --> 00:36:16,666
work going on, things we
talked about before in

764
00:36:16,733 --> 00:36:20,066
terms of studying, what is
the modeling techniques,

765
00:36:20,133 --> 00:36:21,500
whether it is threat
modeling or risk

766
00:36:21,566 --> 00:36:23,765
quantification or other
types of modeling, that

767
00:36:23,833 --> 00:36:26,900
can yield useful insight
into those teams to make

768
00:36:26,966 --> 00:36:29,500
those decisions as opposed
to it being solely based

769
00:36:29,566 --> 00:36:30,899
on experience
and judgment.

770
00:36:30,966 --> 00:36:32,933
There is a lot of value
for experience and judgment.

771
00:36:33,000 --> 00:36:35,033
Results are nice to
augment that with actual

772
00:36:35,099 --> 00:36:39,433
models that deliver useful
predictive results.

773
00:36:39,500 --> 00:36:40,833
>> DONNA DODSON: We have
talked, you know, we kind

774
00:36:40,900 --> 00:36:43,633
of started out with
vulnerability management.

775
00:36:43,699 --> 00:36:47,733
Now we have worked our way
into resiliency and strong

776
00:36:47,800 --> 00:36:50,066
risk management practices.

777
00:36:50,133 --> 00:36:53,066
Given that we have - where
I live, it is one foot

778
00:36:53,133 --> 00:36:58,066
inside the beltway, one
foot outside the beltway,

779
00:36:58,133 --> 00:37:01,698
we hear a lot about
certifications and

780
00:37:01,766 --> 00:37:05,733
certification programs to
deal with vulnerabilities.

781
00:37:05,800 --> 00:37:07,599
Will they help?

782
00:37:07,666 --> 00:37:09,199
Will they hurt?

783
00:37:09,266 --> 00:37:15,166
What would the effect -
what are your thoughts on

784
00:37:15,233 --> 00:37:17,333
certification programs?

785
00:37:19,166 --> 00:37:22,833
>> MARENE ALLISON: I have
been around for SOX,

786
00:37:22,900 --> 00:37:25,900
HIPAA, and PCI.

787
00:37:25,966 --> 00:37:29,098
From the healthcare
perspective, HIPAA and

788
00:37:29,166 --> 00:37:31,533
protected health
information, the HIPAA

789
00:37:31,599 --> 00:37:35,199
law, the privacy law first
and then the security rule

790
00:37:35,266 --> 00:37:39,599
were absolutely imperative
to helping to change the

791
00:37:39,666 --> 00:37:42,333
paradigm of what
we could do.

792
00:37:42,400 --> 00:37:43,733
It was now required.

793
00:37:43,800 --> 00:37:48,833
Now I do believe that the
HIPAA rule is too narrow.

794
00:37:48,900 --> 00:37:52,133
It's unprotected health
information versus all

795
00:37:52,199 --> 00:37:54,000
health information.

796
00:37:54,066 --> 00:37:58,466
The ability to have a
rule that is more broadly

797
00:37:58,533 --> 00:38:02,900
applied, I believe, will
build a better game.

798
00:38:02,966 --> 00:38:04,466
PCI came out.

799
00:38:04,533 --> 00:38:07,199
It had some holes in it
when it first came out.

800
00:38:07,266 --> 00:38:08,199
Thank you, Banks.

801
00:38:08,266 --> 00:38:09,900
Thank you, right?

802
00:38:09,966 --> 00:38:11,966
Because it required me to
do that if I was going to

803
00:38:12,033 --> 00:38:12,732
take credit cards.

804
00:38:12,800 --> 00:38:14,333
I took a lot of them.

805
00:38:14,400 --> 00:38:16,133
It helped my program.

806
00:38:16,199 --> 00:38:20,500
And I think for the larger
corporations, the larger

807
00:38:20,566 --> 00:38:25,265
entities, it sometimes
feels like what are you doing.

808
00:38:25,333 --> 00:38:26,500
My cheese is good.

809
00:38:26,566 --> 00:38:28,698
Don't bother cutting
it around the edges.

810
00:38:28,766 --> 00:38:31,199
But I think for
the smaller companies,

811
00:38:31,266 --> 00:38:33,066
it raises the bar.

812
00:38:33,133 --> 00:38:37,866
And so I'm pretty much an
advocate having gone 2005

813
00:38:37,933 --> 00:38:41,599
with SOX and putting the
IT controls in at least

814
00:38:41,666 --> 00:38:43,233
two different companies.

815
00:38:43,300 --> 00:38:45,333
I'm managing them still
today at Johnson and

816
00:38:45,400 --> 00:38:52,066
Johnson, as well as
watching HIPAA and PCI evolve.

817
00:38:52,133 --> 00:38:56,533
Having those standards
around data, having 50

818
00:38:56,599 --> 00:38:59,633
different privacy laws in
50 different states, that

819
00:38:59,699 --> 00:39:00,733
is not helpful.

820
00:39:00,800 --> 00:39:02,633
That really is
not helpful.

821
00:39:02,699 --> 00:39:05,566
And if we could get a
standard around people

822
00:39:05,633 --> 00:39:09,399
data or healthcare data,
from my perspective,

823
00:39:09,466 --> 00:39:11,165
I'm all in.

824
00:39:11,233 --> 00:39:12,500
>> PAUL KOCHER: I'm
going to give maybe a

825
00:39:12,566 --> 00:39:13,165
counterpoint to that.

826
00:39:13,233 --> 00:39:13,833
>> MARENE ALLISON:
That's okay.

827
00:39:13,900 --> 00:39:15,199
>> PAUL KOCHER: My
experience with

828
00:39:15,266 --> 00:39:16,766
certifications often is
around the things where

829
00:39:16,833 --> 00:39:19,566
you try to come up with
some kind of a gold star

830
00:39:19,633 --> 00:39:22,799
that somebody will put on
the product to say that

831
00:39:22,866 --> 00:39:23,966
it's secure.

832
00:39:24,033 --> 00:39:26,266
The economics of the
systems end up generally

833
00:39:26,333 --> 00:39:27,300
being completely
backwards.

834
00:39:27,366 --> 00:39:30,000
The vender pays for the
certification, which means

835
00:39:30,066 --> 00:39:31,765
that they're incentivized
to pick the weakest

836
00:39:31,833 --> 00:39:34,000
certification process that
will get them the gold

837
00:39:34,066 --> 00:39:36,000
star which will help
them sell products.

838
00:39:36,066 --> 00:39:39,500
This is really a
sales exercise.

839
00:39:39,566 --> 00:39:43,533
In many cases, the sort of
overall process, no code

840
00:39:43,599 --> 00:39:46,000
gets harmed during
the certification.

841
00:39:46,066 --> 00:39:47,265
Nothing changes.

842
00:39:47,333 --> 00:39:50,066
It's entirely a paperwork
exercise that gets done at

843
00:39:50,133 --> 00:39:53,198
the end of the engineering
process, at which point

844
00:39:53,266 --> 00:39:56,199
the product often becomes
very brittle because you

845
00:39:56,266 --> 00:39:58,066
can't update it.

846
00:39:58,133 --> 00:39:59,832
The testing lab
perspective is, in some

847
00:39:59,900 --> 00:40:00,900
cases, even worse.

848
00:40:00,966 --> 00:40:04,098
There was an antipiracy
standard that came out and

849
00:40:04,166 --> 00:40:07,033
somebody really wanted my
team to do evaluations there.

850
00:40:07,099 --> 00:40:07,966
I did one.

851
00:40:08,033 --> 00:40:09,299
It was fine.

852
00:40:09,366 --> 00:40:11,500
Then the next two or three
requests that came in, the

853
00:40:11,566 --> 00:40:14,198
venders were saying am
I worse than the worst

854
00:40:14,266 --> 00:40:15,133
product on the market?

855
00:40:15,199 --> 00:40:16,866
Can you tell me that
I'm not so that I can go

856
00:40:16,933 --> 00:40:18,166
sell this thing?

857
00:40:18,233 --> 00:40:21,533
It was very clear that
this was a race to the bottom.

858
00:40:21,599 --> 00:40:24,033
Other companies that were
doing evaluations were all

859
00:40:24,099 --> 00:40:25,900
trying to make things as
cheap as possible, which

860
00:40:25,966 --> 00:40:28,199
meant you couldn't
actually pay good people

861
00:40:28,266 --> 00:40:30,033
to do the reviews because
you were trying to be

862
00:40:30,099 --> 00:40:31,933
absolutely as
cheap as possible.

863
00:40:32,000 --> 00:40:36,300
All of the incentives
basically drive you to

864
00:40:36,366 --> 00:40:38,400
something or it becomes a
marketing exercise that

865
00:40:38,466 --> 00:40:42,766
consumes paper but doesn't
make the product better.

866
00:40:42,833 --> 00:40:46,233
Now, there is a certain
idea that maybe if you

867
00:40:46,300 --> 00:40:48,500
force somebody to go
through having a money

868
00:40:48,566 --> 00:40:51,698
bonfire or doing some sort
of silly work, you kind of

869
00:40:51,766 --> 00:40:53,166
weed out the people who
don't really care at

870
00:40:53,233 --> 00:40:53,966
all about security.

871
00:40:54,033 --> 00:40:56,133
There is some
argument to that.

872
00:40:56,199 --> 00:40:58,733
In most cases from what
I've seen at least working

873
00:40:58,800 --> 00:41:03,166
from the technical side,
the resources spent doing

874
00:41:03,233 --> 00:41:05,433
certification at the end
of a product's design

875
00:41:05,500 --> 00:41:08,300
cycle would have been much
better spent if they had

876
00:41:08,366 --> 00:41:12,666
been invested in doing
improvements in the

877
00:41:12,733 --> 00:41:14,199
engineering process.

878
00:41:14,266 --> 00:41:17,033
And at the end of a
product's life development

879
00:41:17,099 --> 00:41:19,199
process, even a very
simple question of the

880
00:41:19,266 --> 00:41:21,533
vender, what is the
probability that you have

881
00:41:21,599 --> 00:41:23,466
a security bug and why?

882
00:41:23,533 --> 00:41:25,199
Write me a one-page
explanation of what you

883
00:41:25,266 --> 00:41:27,766
have done and why often
can give a lot more

884
00:41:27,833 --> 00:41:31,333
insight than a gold star
or the lack of a gold star

885
00:41:31,400 --> 00:41:33,766
from a third-party testing
lab that was handed

886
00:41:33,833 --> 00:41:35,699
megabytes of source code
and had two weeks to

887
00:41:35,766 --> 00:41:37,033
look through it.

888
00:41:37,099 --> 00:41:40,166
That's the cynical
counterpoint perhaps.

889
00:41:40,166 --> 00:41:41,866
>> MARENE ALLISON:
I'll counterpoint

890
00:41:41,933 --> 00:41:43,300
back your counterpoint.

891
00:41:43,366 --> 00:41:44,400
>> PAUL KOCHER: Excellent.

892
00:41:44,466 --> 00:41:46,333
>> MARENE ALLISON: Because
having worked with the FDA

893
00:41:46,400 --> 00:41:48,500
on the cybersecurity
protocols for medical

894
00:41:48,566 --> 00:41:51,933
devices, I think it is
absolutely the way to go

895
00:41:52,000 --> 00:41:55,166
because of the ability
then you're bringing

896
00:41:55,233 --> 00:41:57,633
evidence of what you have
talked about, there is

897
00:41:57,699 --> 00:42:00,599
testing, there's been
testing, you have to have

898
00:42:00,666 --> 00:42:03,800
a vulnerability management
program, you have to have

899
00:42:03,866 --> 00:42:05,066
an ability to update.

900
00:42:05,133 --> 00:42:07,299
And so it is a lot of
operational items.

901
00:42:07,366 --> 00:42:10,733
I agree with the UL type
testing on an IoT device.

902
00:42:10,800 --> 00:42:11,766
That may be one thing.

903
00:42:11,833 --> 00:42:15,033
But when you're talking
about something that's in

904
00:42:15,099 --> 00:42:18,866
human healthcare, having
it absolutely certified

905
00:42:18,933 --> 00:42:22,866
and having it reviewed
is imperative to health

906
00:42:22,933 --> 00:42:24,199
safety in the
United States.

907
00:42:24,266 --> 00:42:26,400
>> PAUL KOCHER: Actually, I think we kind of agreed there.

908
00:42:26,466 --> 00:42:27,699
>> MARENE ALLISON: Yes.

909
00:42:27,766 --> 00:42:28,966
>> PAUL KOCHER: You talk
about resources being

910
00:42:29,033 --> 00:42:30,333
spent in the
development process.

911
00:42:30,400 --> 00:42:31,500
>> MARENE ALLISON: You
notice we both wore

912
00:42:31,566 --> 00:42:34,098
gray suits, too.

913
00:42:34,166 --> 00:42:36,099
>> DONNA DODSON: Phil?

914
00:42:36,166 --> 00:42:37,466
>> PHIL VENABLES:
It's interesting.

915
00:42:37,533 --> 00:42:39,500
I think I'll kind of
generalize this up

916
00:42:39,566 --> 00:42:40,799
a little bit.

917
00:42:40,866 --> 00:42:43,166
So certifications,
regulation, everything,

918
00:42:43,233 --> 00:42:45,500
I think they're necessary
but not sufficient.

919
00:42:45,566 --> 00:42:48,966
I think they do run the
risk of getting gamed.

920
00:42:49,033 --> 00:42:52,500
And if you only rely on
that, then that's probably

921
00:42:52,566 --> 00:42:54,866
a path to danger because
you have got to do other things.

922
00:42:54,933 --> 00:42:57,066
I would say that with
metrics, it is interesting.

923
00:42:57,133 --> 00:42:59,799
We have spent a number of
years kind of quantifying

924
00:42:59,866 --> 00:43:01,166
certain things
with metrics.

925
00:43:01,233 --> 00:43:03,866
And again, this is not
- this is a lesson many

926
00:43:03,933 --> 00:43:05,233
organizations have learned.

927
00:43:05,300 --> 00:43:06,966
Any time you have a
metric, there is going to

928
00:43:07,033 --> 00:43:09,566
be an unintended
consequence of that metric.

929
00:43:09,633 --> 00:43:12,299
So it is important to
always have a second

930
00:43:12,366 --> 00:43:14,233
metric for every metric
that looks for the

931
00:43:14,300 --> 00:43:15,733
unintended consequence.

932
00:43:15,800 --> 00:43:18,000
There is plenty of
stories, not necessarily

933
00:43:18,066 --> 00:43:20,500
in security, but in other
walks of life where there

934
00:43:20,566 --> 00:43:25,533
was a thing many, many
decades ago in India,

935
00:43:25,599 --> 00:43:30,266
there were rewards by the
government for killing and

936
00:43:30,333 --> 00:43:32,400
handing in
poisonous snakes.

937
00:43:32,466 --> 00:43:34,000
What do you
think happened?

938
00:43:34,066 --> 00:43:36,066
Everybody started breeding
snakes to kill them and

939
00:43:36,133 --> 00:43:37,332
hand them in
to make money.

940
00:43:37,400 --> 00:43:38,966
The net result, there
were more snakes.

941
00:43:39,033 --> 00:43:40,966
There are loads of
other examples of that.

942
00:43:41,033 --> 00:43:42,866
Every metric you have, you
have got to have another

943
00:43:42,933 --> 00:43:44,833
metric that looks for the
unintended consequence of

944
00:43:44,900 --> 00:43:47,466
the primary metric
because there always is.

945
00:43:47,533 --> 00:43:50,633
>> DONNA DODSON: Can we
do that in this space?

946
00:43:50,699 --> 00:43:55,433
Can we have meaningful
programs to look at

947
00:43:55,500 --> 00:43:57,933
potential for
vulnerabilities?

948
00:43:58,000 --> 00:44:00,166
>> PHIL VENABLES: I think
some of the challenges we

949
00:44:00,233 --> 00:44:04,333
have, the other risk
disciplines in other

950
00:44:04,400 --> 00:44:06,533
segments, whether it's
engineering or safety or

951
00:44:06,599 --> 00:44:08,500
other things,
tend not to have.

952
00:44:08,566 --> 00:44:12,232
I think in many areas in
security, we are striving

953
00:44:12,300 --> 00:44:15,300
for the perfect measures
or the perfect models as

954
00:44:15,366 --> 00:44:17,966
opposed to the models that
are just useful enough.

955
00:44:18,033 --> 00:44:18,866
I like the notion.

956
00:44:18,933 --> 00:44:20,699
We do something similar
to what Paul was saying,

957
00:44:20,766 --> 00:44:24,133
which is just asking
people to rationalize

958
00:44:24,199 --> 00:44:28,466
their choices whether or
not it yields a useful and

959
00:44:28,533 --> 00:44:30,799
consistent mathematical
model but using

960
00:44:30,866 --> 00:44:34,033
quantitative analysis to
analyze their decision's

961
00:44:34,099 --> 00:44:36,099
yields, their thinking
process, and the process

962
00:44:36,166 --> 00:44:37,566
discipline that they
wouldn't have done if you

963
00:44:37,633 --> 00:44:38,899
hadn't otherwise
asked for that.

964
00:44:38,966 --> 00:44:41,833
I would also say just
never underestimate the

965
00:44:41,900 --> 00:44:44,733
basic ability of basic
counting metrics and basic

966
00:44:44,800 --> 00:44:47,333
metrics as of how many of
these things have we done

967
00:44:47,400 --> 00:44:49,666
in these places and does
that conform to the right

968
00:44:49,733 --> 00:44:51,566
control envelope
that we expect?

969
00:44:51,633 --> 00:44:53,633
But you end up using
a variety of things.

970
00:44:53,699 --> 00:44:55,833
We have done a lot with
scenario analysis and

971
00:44:55,900 --> 00:44:59,433
Monte Carlo simulations,
which, for certain cases,

972
00:44:59,500 --> 00:45:03,000
there is not enough
actuarial quality data or

973
00:45:03,066 --> 00:45:05,299
not enough consistency
in the models to

974
00:45:05,366 --> 00:45:06,766
drive useful outcome.

975
00:45:06,833 --> 00:45:08,400
But in many cases,
there absolutely is.

976
00:45:08,466 --> 00:45:09,732
We have done a
bunch of stuff on

977
00:45:09,800 --> 00:45:10,800
Bayesian network modelling.

978
00:45:10,866 --> 00:45:13,766
Again, for some domains,
it is not a useful tool

979
00:45:13,833 --> 00:45:15,633
and there is not
sufficient data to

980
00:45:15,699 --> 00:45:16,500
drive the models.

981
00:45:16,566 --> 00:45:18,265
In other domains,
there absolutely is.

982
00:45:18,333 --> 00:45:22,266
We've got less hung up on
modeling is bad unless we

983
00:45:22,333 --> 00:45:25,266
can use that model for all
things and really focused

984
00:45:25,333 --> 00:45:28,166
in on what's the best
model, best quantification

985
00:45:28,233 --> 00:45:31,633
technique for a particular
case in point, and then

986
00:45:31,699 --> 00:45:34,233
constantly evolving that
because sometimes the

987
00:45:34,300 --> 00:45:36,633
situation shears away from
the modeling and you've

988
00:45:36,699 --> 00:45:37,699
got to reset the modeling.

989
00:45:37,766 --> 00:45:41,099
I think we've got a lot
more comfortable with not

990
00:45:41,166 --> 00:45:44,366
necessarily always being
right in every case, that

991
00:45:44,433 --> 00:45:46,266
there's a lot more
ongoing experimentation.

992
00:45:46,333 --> 00:45:49,199
That's exactly the
same that every other

993
00:45:49,266 --> 00:45:51,699
quantitative risk
discipline in other fields do.

994
00:45:51,766 --> 00:45:54,166
There's no kind of
absolute, kind of

995
00:45:54,233 --> 00:45:57,300
universal laws of how to
do this in every industry.

996
00:45:57,366 --> 00:45:59,133
It's just different.

997
00:45:59,199 --> 00:46:00,933
>> PAUL KOCHER: When I've
looked at models like

998
00:46:01,000 --> 00:46:03,166
that, the one thing that
always seems to jump out

999
00:46:03,233 --> 00:46:05,633
is that the biggest
improvements that you get

1000
00:46:05,699 --> 00:46:07,966
come through reducing
complexity interactions.

1001
00:46:08,033 --> 00:46:09,033
>> PHIL VENABLES: Yeah.

1002
00:46:09,099 --> 00:46:10,033
>> PAUL KOCHER: In some
ways, I kind of think of

1003
00:46:10,099 --> 00:46:12,266
complexity as the
toxic waste of the

1004
00:46:12,333 --> 00:46:14,333
technological development.

1005
00:46:14,400 --> 00:46:16,866
And whether you're
stripping out unnecessary

1006
00:46:16,933 --> 00:46:19,800
copies of IE or reducing
the number of dependencies

1007
00:46:19,866 --> 00:46:22,733
that you have on a piece
of code, all of these

1008
00:46:22,800 --> 00:46:26,766
shrink your problem down
and reduce the complexity

1009
00:46:26,833 --> 00:46:29,000
of what you are dealing
with and let you focus

1010
00:46:29,066 --> 00:46:30,832
more resources
on what's left.

1011
00:46:30,900 --> 00:46:35,633
Everything good happens from stripping that stuff away.

1012
00:46:35,699 --> 00:46:38,066
>> PHIL VENABLES: Particularly, the dependency isolation piece.

1013
00:46:38,133 --> 00:46:41,832
We have yielded so much
benefits by isolating

1014
00:46:41,900 --> 00:46:43,900
dependencies between
systems so individual

1015
00:46:43,966 --> 00:46:46,299
system components can be
upgraded without worrying

1016
00:46:46,366 --> 00:46:49,033
about the others has
yielded a lot more agility.

1017
00:46:49,099 --> 00:46:51,633
That complexity reduction,
but really dependency

1018
00:46:51,699 --> 00:46:53,500
isolation has been key.

1019
00:46:53,566 --> 00:46:54,832
>> PAUL KOCHER: It's
interesting reading of

1020
00:46:54,900 --> 00:46:57,233
Boeing discovering that
they have debris in some

1021
00:46:57,300 --> 00:46:58,733
fraction of the
fuel tanks that

1022
00:46:58,800 --> 00:46:59,800
they manufactured recently.

1023
00:46:59,866 --> 00:47:01,666
If you kind of think of
the software equivalent of

1024
00:47:01,733 --> 00:47:05,366
that is how much debris or
unnecessary software and

1025
00:47:05,433 --> 00:47:08,266
code do you have sitting
around in your critical systems.

1026
00:47:08,333 --> 00:47:10,300
I bet there's more than
just one little piece in

1027
00:47:10,366 --> 00:47:12,000
some fraction of
your fuel tanks.

1028
00:47:12,066 --> 00:47:14,899
I bet it's a much, much
bigger problem for all

1029
00:47:14,966 --> 00:47:17,066
of us here.

1030
00:47:17,133 --> 00:47:20,799
>> DONNA DODSON: We have
heard from each of you

1031
00:47:20,866 --> 00:47:25,333
some notions about what we
can do to get a handle on

1032
00:47:25,400 --> 00:47:30,199
vulnerabilities, whether
it is redundancy or taking

1033
00:47:30,266 --> 00:47:33,966
a look across your system
and seeing where you have

1034
00:47:34,033 --> 00:47:39,533
dependencies that lead you
to very bad outcomes, et

1035
00:47:39,599 --> 00:47:42,500
cetera, et cetera.

1036
00:47:42,566 --> 00:47:45,165
For the folks who are
sitting in the audience

1037
00:47:45,233 --> 00:47:48,066
who have to deal with
these vulnerabilities, who

1038
00:47:48,133 --> 00:47:50,832
have concerns about
Spectre and Meltdown where

1039
00:47:50,900 --> 00:47:53,833
we said maybe not a
lot we can do there.

1040
00:47:53,900 --> 00:47:55,566
With the CryptoAPIs.

1041
00:47:55,633 --> 00:47:58,966
We all know
cryptography is hard.

1042
00:47:59,033 --> 00:48:03,633
In the couple of minutes
that we have left, what is

1043
00:48:03,699 --> 00:48:07,133
some really concrete
advice that the folks in

1044
00:48:07,199 --> 00:48:10,533
this room can walk away
with from you all on

1045
00:48:10,599 --> 00:48:14,400
dealing with
vulnerabilities today and

1046
00:48:14,466 --> 00:48:16,766
in the future?

1047
00:48:16,833 --> 00:48:18,300
>> MARENE ALLISON:
Probably the most

1048
00:48:18,366 --> 00:48:20,800
important thing is knowing
your bill of materials.

1049
00:48:20,866 --> 00:48:24,933
What is making
up your systems?

1050
00:48:25,000 --> 00:48:29,266
Literally, that means data
centers, as well as what

1051
00:48:29,333 --> 00:48:31,066
do you use for compute.

1052
00:48:31,133 --> 00:48:34,000
Configuration, how things
are set up, they're

1053
00:48:34,066 --> 00:48:36,598
extremely important.

1054
00:48:36,666 --> 00:48:41,366
This is one of those it
sounds so easy, but when

1055
00:48:41,433 --> 00:48:45,566
you rinse and repeat over
a million lines of code or

1056
00:48:45,633 --> 00:48:48,698
a million pieces of
equipment out there

1057
00:48:48,766 --> 00:48:53,099
becomes hard, very hard,
especially when with the

1058
00:48:53,166 --> 00:48:55,833
human element you have people who can configure things.

1059
00:48:55,900 --> 00:49:02,766
That comes back to the
lifecycle of the product.

1060
00:49:02,833 --> 00:49:06,000
Do you know what a
product is connected to?

1061
00:49:06,066 --> 00:49:07,066
Do you know what
it's made of?

1062
00:49:07,133 --> 00:49:08,633
Do you know what
it's connected to?

1063
00:49:08,699 --> 00:49:12,000
Do you recertify it, in
certain instances, if it

1064
00:49:12,066 --> 00:49:14,066
is in critical areas of
your network or in your

1065
00:49:14,133 --> 00:49:15,566
compute area or product?

1066
00:49:15,633 --> 00:49:20,698
And are you able to manage
any risk that comes out

1067
00:49:20,766 --> 00:49:22,599
with it and how are you
going to respond to it

1068
00:49:22,666 --> 00:49:24,366
from a resiliency?

1069
00:49:24,433 --> 00:49:26,333
>> PHIL VENABLES: I think
the other one just very

1070
00:49:26,400 --> 00:49:28,500
quickly would be to - and
we kind of touched on this

1071
00:49:28,566 --> 00:49:31,332
in the discussion - is we
have got to reduce the

1072
00:49:31,400 --> 00:49:33,566
dependency on the central
security teams for the

1073
00:49:33,633 --> 00:49:35,366
security of the
organization and just

1074
00:49:35,433 --> 00:49:37,766
basically for every
software development or

1075
00:49:37,833 --> 00:49:40,000
product team in your
organization, making sure

1076
00:49:40,066 --> 00:49:42,598
they have got the
training, the tooling, and

1077
00:49:42,666 --> 00:49:45,466
the oversight to make
sure that people are

1078
00:49:45,533 --> 00:49:48,133
consciously taking the
right risk decisions, and

1079
00:49:48,199 --> 00:49:50,500
then the security team is
about overseeing how all

1080
00:49:50,566 --> 00:49:54,165
of that process works, not
validating the output of a

1081
00:49:54,233 --> 00:49:56,699
team that is not
able or equipped to

1082
00:49:56,766 --> 00:49:59,500
produce secure software.

1083
00:49:59,566 --> 00:50:00,832
>> PAUL KOCHER: From a
technical perspective, I

1084
00:50:00,900 --> 00:50:04,366
would add to that also try
to build components that

1085
00:50:04,433 --> 00:50:06,099
are as isolated as
possible where you

1086
00:50:06,166 --> 00:50:08,766
understand all of the
interactions and ideally

1087
00:50:08,833 --> 00:50:11,366
even have monitoring and
analysis capabilities on

1088
00:50:11,433 --> 00:50:13,166
each of those
interactions, know what

1089
00:50:13,233 --> 00:50:15,400
state that each component
maintains, know what it's

1090
00:50:15,466 --> 00:50:16,833
characteristics are
supposed to be.

1091
00:50:16,900 --> 00:50:19,633
Really trying to take what
is often this sort of

1092
00:50:19,699 --> 00:50:22,466
evolved giant ball of
string and trying to

1093
00:50:22,533 --> 00:50:24,633
divide it into a set
of small individual

1094
00:50:24,699 --> 00:50:28,866
components to talk to each
other in an understood way.

1095
00:50:28,933 --> 00:50:32,033
That may mean turning off
things like hyperthreading

1096
00:50:32,099 --> 00:50:35,133
or APIs that are
enormously complicated

1097
00:50:35,199 --> 00:50:37,133
where you have a lot
of interaction between

1098
00:50:37,199 --> 00:50:41,500
components that are hard
to characterize and

1099
00:50:41,566 --> 00:50:43,698
recognizing that there
will be some cost to this.

1100
00:50:43,766 --> 00:50:46,533
You will not have as fast
a system when you're done.

1101
00:50:46,599 --> 00:50:52,500
You may have a system that
has worse performance,

1102
00:50:52,566 --> 00:50:54,500
higher cost to operate,
but at the end of the day

1103
00:50:54,566 --> 00:50:56,098
it will be more resilient
because you'll be able to

1104
00:50:56,166 --> 00:50:58,233
focus resources in just
the components that are

1105
00:50:58,300 --> 00:51:00,699
failing rather than the
entire system failing in

1106
00:51:00,766 --> 00:51:04,666
some complicated and
unanalyzable way.

1107
00:51:04,733 --> 00:51:06,599
>> DONNA DODSON: People,
process, technology,

1108
00:51:06,666 --> 00:51:10,800
looking at it together,
looking at risk management

1109
00:51:10,866 --> 00:51:14,366
and resiliency, and maybe
stepping back a little bit

1110
00:51:14,433 --> 00:51:16,599
from perfection
in cybersecurity.

1111
00:51:16,666 --> 00:51:20,866
Hopefully this will be
good takeaways for folks

1112
00:51:20,933 --> 00:51:21,766
in the audience.

1113
00:51:21,833 --> 00:51:23,533
Thank you all
for being here.

1114
00:51:23,599 --> 00:51:27,166
Thank you all to the panelists very much for your time.

1115
00:51:27,233 --> 00:51:28,033
>> MARENE ALLISON:
Thank you, Donna.

1116
00:51:28,566 --> 00:51:29,500
>> PAUL KOCHER:
Thank you, Donna.

