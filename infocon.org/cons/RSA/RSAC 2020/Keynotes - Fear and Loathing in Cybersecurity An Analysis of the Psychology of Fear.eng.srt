1
00:00:06,099 --> 00:00:09,166
>> ANNOUNCER: Please
welcome Dr. Jessica Barker.

2
00:00:22,000 --> 00:00:23,633
>> DR. JESSICA BARKER:
Good morning everybody.

3
00:00:23,699 --> 00:00:27,000
Thank you all so much for coming
to see my keynote this morning,

4
00:00:27,066 --> 00:00:31,266
where I'm talking primarily
about the psychology of fear.

5
00:00:31,866 --> 00:00:36,600
I'm really delighted to be here
at RSA, keynoting, particularly

6
00:00:36,666 --> 00:00:40,965
in a year when we're focusing on
the human element because I'm

7
00:00:41,033 --> 00:00:43,033
sure like a lot of
you, that's my passion.

8
00:00:43,100 --> 00:00:46,633
So, it feels amazing to be able
to speak on this stage about

9
00:00:46,700 --> 00:00:47,766
that today.

10
00:00:48,799 --> 00:00:54,700
So, what I am speaking about
today, I - I've always

11
00:00:54,766 --> 00:00:57,632
specialized in awareness
behavior and culture when it

12
00:00:57,700 --> 00:00:59,500
comes to cybersecurity.

13
00:00:59,566 --> 00:01:03,933
And so, what I'm talking about
today is some lessons from my

14
00:01:04,000 --> 00:01:08,266
experience and a lot from
scientific research that helps

15
00:01:08,333 --> 00:01:12,400
us understand how we can
communicate cybersecurity

16
00:01:12,466 --> 00:01:15,065
messages in the
most effective way.

17
00:01:16,200 --> 00:01:17,966
And when I say
communicate, who am I talking

18
00:01:18,033 --> 00:01:20,400
about communicating to?

19
00:01:20,466 --> 00:01:23,232
Primarily, I'm talking about
communicating with the end

20
00:01:23,299 --> 00:01:27,266
users, people who don't work
in security, maybe not in

21
00:01:27,333 --> 00:01:31,233
technology, and people who we're
desperately trying to reach with

22
00:01:31,299 --> 00:01:35,099
our awareness raising, trying to
promote more positive behaviors

23
00:01:35,166 --> 00:01:36,566
when it comes to security.

24
00:01:37,599 --> 00:01:41,166
So, I'm going to be speaking
about what fear has to do with

25
00:01:41,233 --> 00:01:45,200
this and why we need a good
understanding of the psychology

26
00:01:45,266 --> 00:01:49,733
of fear when we're communicating
cybersecurity messages.

27
00:01:49,799 --> 00:01:52,200
I'm going to be talking about
something called fear appeals,

28
00:01:52,266 --> 00:01:55,066
what they mean and particularly
what they mean when it comes to

29
00:01:55,133 --> 00:01:56,966
cybersecurity communications.

30
00:01:56,966 --> 00:02:02,598
I'm going to be talking about
how we can communicate a scary

31
00:02:02,666 --> 00:02:05,466
subject in the
most constructive and

32
00:02:05,533 --> 00:02:07,533
responsible way.

33
00:02:07,599 --> 00:02:11,333
When I first started exploring
the psychology of fear in

34
00:02:11,400 --> 00:02:16,233
relation to cybersecurity about
five years ago, I did it because

35
00:02:16,300 --> 00:02:18,233
I was doing
a lot of awareness raising.

36
00:02:18,300 --> 00:02:19,699
I still do.

37
00:02:19,766 --> 00:02:23,500
But at the time I wondered, can
I talk about this subject and

38
00:02:23,566 --> 00:02:24,666
not scare people?

39
00:02:24,733 --> 00:02:29,066
And I quickly realized, you
can't talk about cybersecurity

40
00:02:29,133 --> 00:02:32,433
and not elicit some sense of
fear among the people you are

41
00:02:32,500 --> 00:02:33,533
talking to.

42
00:02:33,599 --> 00:02:36,933
Inherently, we are always going
to be speaking to some extent

43
00:02:37,000 --> 00:02:38,000
about threats.

44
00:02:38,733 --> 00:02:41,400
So, what I then became
interested in is how can we do

45
00:02:41,466 --> 00:02:45,233
that in a way that is the most
constructive, is going to have

46
00:02:45,300 --> 00:02:48,933
the most positive impact, and
is the most responsible because

47
00:02:49,000 --> 00:02:51,466
we're dealing with an
emotion when it comes to fear.

48
00:02:52,533 --> 00:02:55,500
And then for the end of my
presentation, I'm going to be

49
00:02:55,566 --> 00:02:58,099
looking at how we
can move beyond fear.

50
00:02:58,166 --> 00:03:01,366
So firstly, how do we
use fear and how do we use

51
00:03:01,433 --> 00:03:02,433
fear responsibly?

52
00:03:02,500 --> 00:03:04,766
But then what else can we use?

53
00:03:04,833 --> 00:03:07,733
What other emotional
appeals can we draw on?

54
00:03:08,533 --> 00:03:10,632
So, throughout this
presentation, I will be drawing

55
00:03:10,699 --> 00:03:14,333
on psychology, sociology,
neuroscience, behavioral

56
00:03:14,400 --> 00:03:18,566
economics, all of these fields
that really help us understand

57
00:03:18,633 --> 00:03:21,400
people much better so that we
can really put people at the

58
00:03:21,466 --> 00:03:24,066
heart of our
security communications.

59
00:03:26,866 --> 00:03:31,699
So, why speak about
fear in cybersecurity?

60
00:03:31,766 --> 00:03:35,333
We know from behavioral
economics that people are more

61
00:03:35,400 --> 00:03:38,633
scared of things if they feel
like they don't understand them

62
00:03:38,699 --> 00:03:42,500
or if they feel like that
subject, that topic, that issue

63
00:03:42,566 --> 00:03:43,766
is out of control.

64
00:03:43,833 --> 00:03:48,633
And this is why, for example,
people are more likely to be

65
00:03:48,699 --> 00:03:52,866
afraid of air travel than
driving in their car, because it

66
00:03:52,933 --> 00:03:55,900
feels out of control and it
involves a lot of technology

67
00:03:55,966 --> 00:03:57,599
that they're not familiar with.

68
00:03:57,666 --> 00:04:00,366
They're not the one,
usually, piloting the plane.

69
00:04:00,433 --> 00:04:01,799
So, it feels unfamiliar.

70
00:04:01,866 --> 00:04:03,266
It feels out of control.

71
00:04:03,333 --> 00:04:07,033
So, even though there are way
more car accidents than there

72
00:04:07,099 --> 00:04:11,266
are plane accidents, people are
disproportionately afraid or

73
00:04:11,333 --> 00:04:14,033
more likely to be afraid
of flying in a plane.

74
00:04:15,500 --> 00:04:17,666
What does this got to
do with cybersecurity?

75
00:04:17,733 --> 00:04:22,399
Well, of course, what we see in
cybersecurity has been a huge

76
00:04:22,466 --> 00:04:26,233
rise in number of
public attacks.

77
00:04:26,300 --> 00:04:29,766
We see cybersecurity issues
hitting the headlines pretty

78
00:04:29,833 --> 00:04:31,899
much on a daily basis.

79
00:04:31,966 --> 00:04:36,033
And on one level, this is
good for awareness, isn't it?

80
00:04:36,100 --> 00:04:39,300
It means our board members are
more likely to be thinking and

81
00:04:39,366 --> 00:04:41,532
asking about cybersecurity.

82
00:04:41,600 --> 00:04:45,633
It maybe warms people up in our
organizations, our colleagues to

83
00:04:45,699 --> 00:04:47,633
cybersecurity awareness.

84
00:04:47,699 --> 00:04:50,366
It brings the topic to the
front of people's minds.

85
00:04:51,100 --> 00:04:56,033
So, that's great except that's
a double-sided coin, and it can

86
00:04:56,100 --> 00:04:59,899
mean that people feel like
cybersecurity is out of control.

87
00:04:59,966 --> 00:05:04,133
There are so many breaches and
hacks happening all the time,

88
00:05:04,199 --> 00:05:07,966
even to tech savvy companies
like Facebook and Twitter

89
00:05:08,033 --> 00:05:09,066
and Google.

90
00:05:09,133 --> 00:05:12,265
So, if it's happening to them,
well it must be huge, it must be

91
00:05:12,333 --> 00:05:14,366
out of control;
what chance do we have?

92
00:05:15,600 --> 00:05:18,300
And security can feel very
unfamiliar for a lot of people

93
00:05:18,366 --> 00:05:19,032
as well.

94
00:05:19,100 --> 00:05:21,466
We often talk in a
technical language.

95
00:05:21,533 --> 00:05:24,433
We often use terms that might
not make sense to people outside

96
00:05:24,500 --> 00:05:25,833
of the industry.

97
00:05:25,899 --> 00:05:28,866
Even in mainstream news
coverage, sometimes it's not

98
00:05:28,933 --> 00:05:32,832
that clear to the average person
what has happened and what is

99
00:05:32,899 --> 00:05:34,966
being spoken about and
what it means to them.

100
00:05:35,899 --> 00:05:39,699
So, unfamiliar and out of
control which means that when we

101
00:05:39,766 --> 00:05:43,233
go in to do our awareness
raising, we have to understand

102
00:05:43,300 --> 00:05:46,333
that cybersecurity is one
of these issues that people

103
00:05:46,399 --> 00:05:49,366
potentially have a
disproportionate fear -

104
00:05:49,433 --> 00:05:50,366
level of fear about.

105
00:05:50,366 --> 00:05:55,699
So, I've talked about
fear a little bit already.

106
00:05:55,766 --> 00:05:58,433
What do I mean by fear?

107
00:05:58,500 --> 00:06:01,399
Well, fear
of course is an emotion.

108
00:06:01,466 --> 00:06:03,133
Fear is not the danger.

109
00:06:03,199 --> 00:06:04,000
It's not the issue.

110
00:06:04,066 --> 00:06:05,399
It's not the threat.

111
00:06:05,466 --> 00:06:08,733
It's our
emotional response to it.

112
00:06:08,800 --> 00:06:11,766
There's been a great deal of
research in psychology and

113
00:06:11,833 --> 00:06:15,866
sociology and other human
centered disciplines as to what

114
00:06:15,933 --> 00:06:19,133
fear actually means
and how we respond to it.

115
00:06:19,933 --> 00:06:23,933
On the surface level, we can
think fear is just a thing.

116
00:06:24,000 --> 00:06:24,933
It just exists.

117
00:06:25,000 --> 00:06:27,833
It's the same wherever
you go, whoever you are.

118
00:06:27,899 --> 00:06:30,233
But of course, that's
absolutely not the case.

119
00:06:32,399 --> 00:06:36,133
A sociologist called Tudor has
done a great deal of work trying

120
00:06:36,199 --> 00:06:40,399
to understand how fear manifests
itself in society and what

121
00:06:40,466 --> 00:06:42,266
this means.

122
00:06:42,333 --> 00:06:45,766
And so, we can look at an
example that Tudor uses about

123
00:06:45,833 --> 00:06:46,933
a tiger.

124
00:06:47,000 --> 00:06:48,433
Tigers are scary, right?

125
00:06:48,500 --> 00:06:50,533
We're all going to
be scared of a tiger.

126
00:06:51,166 --> 00:06:53,866
Well, if we're alone in the
jungle and we come across a

127
00:06:53,933 --> 00:06:57,000
tiger, then yeah, we're going to
be pretty scared, I would have

128
00:06:57,066 --> 00:06:57,899
thought - most of us.

129
00:06:57,966 --> 00:06:59,000
I know I would be.

130
00:06:59,066 --> 00:07:01,198
I'm not trained
in tiger handling.

131
00:07:01,266 --> 00:07:03,366
So, that's going to
be quite scary to me.

132
00:07:05,366 --> 00:07:10,233
If I am in a group, a safari
group, and there is a safari

133
00:07:10,300 --> 00:07:14,866
leader there who is trained in
tracking tigers, has the right

134
00:07:14,933 --> 00:07:18,366
equipment in case something goes
wrong, knows what to do if the

135
00:07:18,433 --> 00:07:21,599
tiger is showing signs of
attacking, knows what signs of

136
00:07:21,666 --> 00:07:24,266
attack look like, well then
I'm going to feel much more

137
00:07:24,333 --> 00:07:27,800
confident and much less afraid
than if I'm just on my own.

138
00:07:28,800 --> 00:07:32,066
If I am that trained safari
leader, well then I'm going to

139
00:07:32,133 --> 00:07:33,765
feel even more confident again.

140
00:07:33,833 --> 00:07:36,800
I'm going to have to recognize
that I'm responsible for the

141
00:07:36,866 --> 00:07:39,832
group that's with me, that they
have put their trust in me, and

142
00:07:39,899 --> 00:07:44,233
that I'm playing this role, this
intermediary between the tiger

143
00:07:44,300 --> 00:07:45,833
and the people
that I'm shepherding.

144
00:07:47,166 --> 00:07:50,933
If I'm a hunter and I see
a tiger, I'm going to feel

145
00:07:51,000 --> 00:07:53,066
entirely differently again.

146
00:07:53,133 --> 00:07:54,532
I'm unlikely to feel afraid.

147
00:07:54,600 --> 00:07:58,399
I'm going to feel excited
because, you know, there's

148
00:07:58,466 --> 00:07:59,399
my fortune.

149
00:07:59,466 --> 00:08:00,899
That's what I have been
searching and that's what I'm

150
00:08:00,966 --> 00:08:02,033
here to do.

151
00:08:02,100 --> 00:08:05,500
If I see a tiger on the TV, I'm
at home with a cup of tea and

152
00:08:05,566 --> 00:08:07,732
I'm watching a documentary,
well, I'm not going to feel

153
00:08:07,800 --> 00:08:09,399
afraid at all.

154
00:08:09,466 --> 00:08:12,800
So, depending on who you are in
relation to the tiger, you're

155
00:08:12,866 --> 00:08:14,666
going to feel very differently.

156
00:08:14,733 --> 00:08:17,233
And of course, this is
the same with technology and

157
00:08:17,300 --> 00:08:18,699
cyber insecurity.

158
00:08:19,166 --> 00:08:23,600
In the example that I gave, I
see us as the safari leaders.

159
00:08:23,666 --> 00:08:25,766
We are the ones who are trained.

160
00:08:25,833 --> 00:08:28,666
We're the ones that know what
attacks look like, what signs of

161
00:08:28,733 --> 00:08:32,966
attack look like, and we are
responsible for the people that

162
00:08:33,033 --> 00:08:34,600
we are shepherding.

163
00:08:34,666 --> 00:08:36,633
But we have to understand
that they are going to have a

164
00:08:36,700 --> 00:08:40,833
different emotional response to
the tiger, the technology, the

165
00:08:40,899 --> 00:08:43,200
cyber insecurity than we are.

166
00:08:45,933 --> 00:08:49,733
So, in my introduction,
I mentioned fear appeals.

167
00:08:49,799 --> 00:08:53,233
Fear appeals, if you're not
familiar with the term, it's a

168
00:08:53,299 --> 00:08:57,299
message that tries
to change behavior by using a

169
00:08:57,366 --> 00:08:59,100
fear-based message.

170
00:08:59,166 --> 00:09:02,566
So, I'm using something scary to
try and influence you to change

171
00:09:02,633 --> 00:09:03,433
your behavior.

172
00:09:04,500 --> 00:09:07,666
And so, for example, we
see fear appeals on the TV.

173
00:09:07,733 --> 00:09:09,632
You know, don't drink and drive.

174
00:09:09,700 --> 00:09:13,833
We see fear appeals on cigarette
packets warning us of the health

175
00:09:13,899 --> 00:09:15,466
dangers of smoking.

176
00:09:15,533 --> 00:09:19,033
We see fear appeals
every day, all around us.

177
00:09:19,100 --> 00:09:23,166
And we've been using them in
society for many decades to try

178
00:09:23,233 --> 00:09:26,799
to encourage more positive
behaviors in all sorts of areas

179
00:09:26,866 --> 00:09:27,600
of life.

180
00:09:29,266 --> 00:09:33,199
Now psychologists have taken
these decades and decades of

181
00:09:33,266 --> 00:09:37,233
work around fear appeals to
analyze how come some of these

182
00:09:37,299 --> 00:09:39,933
work and how come
some of them don't?

183
00:09:40,000 --> 00:09:43,700
Because some are very successful
in changing behaviors and some -

184
00:09:43,766 --> 00:09:47,565
not even don't work, they can
actually make the problem worse.

185
00:09:47,633 --> 00:09:50,766
They can drive more
negative behaviors.

186
00:09:50,799 --> 00:09:54,632
So, psychologists have been
looking at why some work, why

187
00:09:54,700 --> 00:09:57,633
some don't, and what key
messages we can take from those.

188
00:09:59,866 --> 00:10:03,600
Now, when I talk about the
psychology of fear, I often get

189
00:10:03,666 --> 00:10:04,600
this question.

190
00:10:04,666 --> 00:10:08,000
And it might be something
you're all thinking at RSA.

191
00:10:08,066 --> 00:10:10,366
Surely it's very
straightforward.

192
00:10:10,433 --> 00:10:14,733
Social engineers use fear all
the time in their phishing,

193
00:10:14,799 --> 00:10:18,699
smishing, vishing messages
and they tend to work.

194
00:10:18,766 --> 00:10:20,065
So, isn't it straightforward?

195
00:10:20,133 --> 00:10:20,933
Isn't it easy?

196
00:10:21,000 --> 00:10:23,633
If they're doing it, why
can't we do it as well?

197
00:10:23,700 --> 00:10:25,100
And it's a very good question.

198
00:10:25,566 --> 00:10:30,000
The research shows that if you
are trying to promote, trying to

199
00:10:30,066 --> 00:10:35,100
encourage, trying to elicit a
kneejerk, immediate reaction,

200
00:10:35,166 --> 00:10:37,100
then fear works.

201
00:10:37,166 --> 00:10:39,000
So, perfect for
a phishing email.

202
00:10:39,066 --> 00:10:43,333
You can scare someone into what
behavioral economics call a hot

203
00:10:43,399 --> 00:10:47,733
state, basically, the thinking
fast part of the brain.

204
00:10:47,799 --> 00:10:51,799
You can encourage someone to get
into that clouded, emotional

205
00:10:51,866 --> 00:10:54,899
state where they will react
really quickly without thinking

206
00:10:54,966 --> 00:10:55,633
it through.

207
00:10:56,433 --> 00:11:00,466
So, for an immediate
reaction, fear is good.

208
00:11:00,533 --> 00:11:04,266
If you're trying to encourage
long-term behavioral change, it

209
00:11:04,333 --> 00:11:06,665
becomes much more complicated.

210
00:11:06,733 --> 00:11:10,433
And this is why social
engineers, cybercriminals can

211
00:11:10,500 --> 00:11:14,666
use fear really very effectively
in their attacks, but we need to

212
00:11:14,733 --> 00:11:18,465
be much more intelligent and
considered when we're using

213
00:11:18,533 --> 00:11:20,799
fear-based messaging ourselves.

214
00:11:22,933 --> 00:11:29,733
So, one of the most fundamental
pieces of research around fear

215
00:11:29,799 --> 00:11:32,899
and fear appeals was
conducted by a psychologist

216
00:11:32,966 --> 00:11:33,899
called Leventhal.

217
00:11:33,966 --> 00:11:39,633
And Leventhal had been
charged with looking at trying

218
00:11:39,700 --> 00:11:44,200
to increase the rate
of tetanus vaccination.

219
00:11:44,266 --> 00:11:46,266
So, Leventhal was
given this challenge.

220
00:11:46,333 --> 00:11:50,565
People, particularly in the
US some decades ago, weren't

221
00:11:50,633 --> 00:11:54,033
getting the tetanus vaccine even
though it was a pretty simple

222
00:11:54,100 --> 00:11:56,833
thing to do and it was
proven to be very effective.

223
00:11:57,733 --> 00:12:01,299
So, Leventhal was asked to look
into this and to try to drive

224
00:12:01,366 --> 00:12:05,266
the rate of people having
the tetanus vaccine.

225
00:12:05,333 --> 00:12:09,866
So, Leventhal and his teams took
groups of students and they

226
00:12:09,933 --> 00:12:12,665
tried to scare them into
getting the vaccine.

227
00:12:12,733 --> 00:12:16,532
They showed them horrible
images, described the symptoms

228
00:12:16,600 --> 00:12:20,500
of tetanus in great detail, and
really tried to encourage people

229
00:12:20,566 --> 00:12:22,366
to go and get the vaccine.

230
00:12:22,433 --> 00:12:25,665
They found that scaring
people really didn't work.

231
00:12:25,733 --> 00:12:29,299
People left the session
saying, I'm really scared.

232
00:12:29,366 --> 00:12:30,633
I'm going to do
something about it.

233
00:12:30,700 --> 00:12:32,266
I'm going to go
and get the vaccine.

234
00:12:32,333 --> 00:12:34,766
Almost 100% of people said that.

235
00:12:34,833 --> 00:12:38,866
Less than 1% actually
went and got the vaccine.

236
00:12:38,933 --> 00:12:40,933
So, simply scaring
people wasn't working.

237
00:12:41,000 --> 00:12:44,733
They took other groups of
students and used the same

238
00:12:44,799 --> 00:12:49,566
fear-based messaging, but then
they gave them lots of tools to

239
00:12:49,633 --> 00:12:52,166
go and get the vaccine.

240
00:12:52,233 --> 00:12:54,632
So, a researcher sat
down with these students.

241
00:12:54,700 --> 00:12:56,733
They gave them a
map of the campus.

242
00:12:56,799 --> 00:13:00,500
They pointed out where they
were, where the experiments were

243
00:13:00,566 --> 00:13:01,666
taking place.

244
00:13:01,733 --> 00:13:03,933
They showed them where
the health center was.

245
00:13:04,000 --> 00:13:06,933
They gave them the opening
times of the health center.

246
00:13:07,000 --> 00:13:10,000
They asked where they lived and
helped them map a route, you

247
00:13:10,066 --> 00:13:12,133
know, how are you going to get
to the health center to get

248
00:13:12,200 --> 00:13:13,100
the vaccine?

249
00:13:13,166 --> 00:13:14,466
When are you going to go?

250
00:13:14,533 --> 00:13:16,466
When are you going to
make an appointment?

251
00:13:16,533 --> 00:13:20,166
They really gave the
individuals the tools to go

252
00:13:20,233 --> 00:13:21,465
and get the vaccine.

253
00:13:22,466 --> 00:13:25,966
They checked back with these
groups some weeks later, and of

254
00:13:26,033 --> 00:13:29,399
course, compliance wasn't 100%,
but out of everyone that said

255
00:13:29,466 --> 00:13:33,333
they would have the vaccine,
which was about 100%, 30% of

256
00:13:33,399 --> 00:13:34,600
people did.

257
00:13:34,666 --> 00:13:38,700
So, compared to that 1%, much
more effective when people

258
00:13:38,766 --> 00:13:42,599
actually were given the tools
and the road map of how they

259
00:13:42,666 --> 00:13:44,733
could go and get the vaccine.

260
00:13:45,566 --> 00:13:48,533
So, based on Leventhal's work,
there was a lot more research

261
00:13:48,600 --> 00:13:53,433
done into fear appeals, when
they work and when they don't.

262
00:13:53,500 --> 00:13:57,033
And it's led to this, the
Extended Parallel Process Model.

263
00:13:57,966 --> 00:14:02,200
Psychologists have produced this
to help explain how to create a

264
00:14:02,266 --> 00:14:06,233
fear appeal that is more
likely to be effective.

265
00:14:06,299 --> 00:14:10,132
And this really illustrates how
we react when we're confronted

266
00:14:10,200 --> 00:14:12,066
with a fear-based message.

267
00:14:12,133 --> 00:14:15,366
We won't even be aware that
we're reacting like this, but

268
00:14:15,433 --> 00:14:18,165
this is how we process
a fear-based message.

269
00:14:18,466 --> 00:14:20,866
So, if someone comes to me
and they tell me - tells me

270
00:14:20,933 --> 00:14:23,899
something scary to try and
change my behavior, the first

271
00:14:23,966 --> 00:14:26,200
thing that I will do
is appraise the threat.

272
00:14:26,266 --> 00:14:30,299
I will decide, is
that threat real?

273
00:14:30,366 --> 00:14:33,533
Are there cyber criminals
out there hacking people and

274
00:14:33,600 --> 00:14:34,633
stealing data?

275
00:14:34,700 --> 00:14:38,399
I think we're at a point where
most people would agree, yes, so

276
00:14:38,466 --> 00:14:41,200
people appraise that threat and
they would probably be convinced

277
00:14:41,266 --> 00:14:45,165
that yes, the threat of
cyber insecurity is real.

278
00:14:46,299 --> 00:14:51,466
What we then do is we
consider our susceptibility.

279
00:14:51,533 --> 00:14:55,233
So, we have to be convinced not
only that the threat is real,

280
00:14:55,299 --> 00:14:57,533
but that it applies us.

281
00:14:57,600 --> 00:15:03,133
And if people feel like, yeah,
you know, there's criminal

282
00:15:03,200 --> 00:15:06,766
hackers out there stealing data,
but why would they want my data?

283
00:15:06,833 --> 00:15:10,065
If they feel like that, if they
feel like they would not be

284
00:15:10,133 --> 00:15:14,200
targeted or caught up in
cybercrime, we've lost them.

285
00:15:14,799 --> 00:15:17,533
So according to this model, we
have to convince people the

286
00:15:17,600 --> 00:15:21,700
threat is real, and crucially,
that it applies to them.

287
00:15:21,766 --> 00:15:25,033
They have to feel that they
are susceptible to the threat.

288
00:15:25,266 --> 00:15:29,333
Then the most important message
that has to come through is

289
00:15:29,399 --> 00:15:30,933
the efficacy.

290
00:15:31,000 --> 00:15:33,533
People will go on from
thinking, is the threat real?

291
00:15:33,600 --> 00:15:35,233
Does it apply to me?

292
00:15:35,299 --> 00:15:39,165
Is there something that I can
do about it and will that work?

293
00:15:39,233 --> 00:15:43,965
So, people will appraise the
efficacy of the message if it's

294
00:15:44,033 --> 00:15:46,066
having better passwords.

295
00:15:46,133 --> 00:15:48,333
Is it going to
improve my security to have

296
00:15:48,399 --> 00:15:49,866
better passwords?

297
00:15:49,933 --> 00:15:51,299
Is that going to work?

298
00:15:51,366 --> 00:15:54,500
Is that something I have
the time, the ability, the

299
00:15:54,566 --> 00:15:56,566
knowledge, and the tools to do?

300
00:15:56,633 --> 00:16:00,866
And people have to feel that
they can do it and that it will

301
00:16:00,933 --> 00:16:04,333
be successful for them
to actually engage in our

302
00:16:04,399 --> 00:16:06,200
fear-based message.

303
00:16:06,266 --> 00:16:09,733
Otherwise, they will just
engage with the emotion.

304
00:16:10,100 --> 00:16:13,166
So, if we convince them the
threat is real, it applies to

305
00:16:13,233 --> 00:16:16,866
them, there is something they
can do about it, it will work,

306
00:16:16,933 --> 00:16:21,066
and they are capable of it, then
they will engage in controlling

307
00:16:21,133 --> 00:16:22,466
the actual danger.

308
00:16:22,533 --> 00:16:25,299
So, then they will engage with
the behaviors we're encouraging.

309
00:16:26,166 --> 00:16:29,600
If we lose them at any point in
this model, at any point on that

310
00:16:29,666 --> 00:16:34,200
journey, they will just engage
with the fear, the emotion.

311
00:16:34,266 --> 00:16:38,433
And that means they'll
find a way of avoiding what

312
00:16:38,500 --> 00:16:40,066
we're recommending.

313
00:16:40,133 --> 00:16:43,100
They will either switch
off from listening to us.

314
00:16:43,166 --> 00:16:44,533
They will become tired.

315
00:16:44,600 --> 00:16:45,899
Their eyes will glaze over.

316
00:16:45,966 --> 00:16:48,033
They will think, well,
it doesn't apply to me.

317
00:16:48,100 --> 00:16:50,266
Or, you know, what's the point?

318
00:16:50,333 --> 00:16:51,699
It's so bad anyway.

319
00:16:51,766 --> 00:16:54,899
There's no way that I can - if
Google is being hacked, then

320
00:16:54,966 --> 00:16:56,966
what chance do I have?

321
00:16:57,033 --> 00:17:00,399
Or they'll think this person is
trying to sell me something.

322
00:17:00,466 --> 00:17:01,466
They've got an agenda.

323
00:17:01,533 --> 00:17:03,533
I don't need to listen to this.

324
00:17:03,533 --> 00:17:06,833
Or at the other end of the
scale, which I have seen happen,

325
00:17:06,900 --> 00:17:10,966
they will become so terrified
that they won't want to go on

326
00:17:11,032 --> 00:17:11,832
the internet.

327
00:17:11,900 --> 00:17:13,700
They won't want to open
any of their emails.

328
00:17:13,766 --> 00:17:15,700
They won't want to
click on any links.

329
00:17:16,200 --> 00:17:18,866
So, one way or another, they
are engaging with the emotional

330
00:17:18,933 --> 00:17:22,766
response, not the true danger.

331
00:17:22,833 --> 00:17:25,400
So, usually they
will engage with avoidance,

332
00:17:25,465 --> 00:17:27,699
reactance, doubt.

333
00:17:27,766 --> 00:17:30,566
One way or another, they will
convince themselves emotionally

334
00:17:30,633 --> 00:17:33,133
that what we're saying
doesn't apply to them.

335
00:17:34,599 --> 00:17:37,765
So, what we really need to do
with our fear-based messaging is

336
00:17:37,833 --> 00:17:41,000
make it relevant to the people,
to the individual people that

337
00:17:41,066 --> 00:17:44,633
we're speaking to, and we
really need to empower them.

338
00:17:44,700 --> 00:17:48,000
We really need them to feel
that they can engage with the

339
00:17:48,066 --> 00:17:49,533
responses we're recommending.

340
00:17:49,599 --> 00:17:53,599
And I'm going to encourage a few
ways that we can do that in my

341
00:17:53,666 --> 00:17:54,866
next few slides.

342
00:17:56,500 --> 00:18:00,433
One of those ways is to think
about all of the messages that

343
00:18:00,500 --> 00:18:01,533
we share with people.

344
00:18:01,599 --> 00:18:05,700
It becomes really difficult for
an end user to engage in our

345
00:18:05,766 --> 00:18:10,633
recommendations if
they're a really long list.

346
00:18:10,700 --> 00:18:13,633
If we're telling people ten,
twenty, a hundred things that

347
00:18:13,700 --> 00:18:16,666
they need to do,
that is so overwhelming.

348
00:18:16,733 --> 00:18:19,733
There's no way they're going to
engage with what we recommend.

349
00:18:19,799 --> 00:18:23,833
So, we really need to think
about how we can reduce the

350
00:18:23,900 --> 00:18:26,766
noise and focus on
the signal for people.

351
00:18:27,266 --> 00:18:32,599
If you have three, four, five
maximum key messages that you

352
00:18:32,666 --> 00:18:38,500
can focus on with your end users
or your kids or a media message,

353
00:18:38,566 --> 00:18:40,833
what are your top priorities?

354
00:18:40,900 --> 00:18:43,233
And the best way of doing this
is really to think about your

355
00:18:43,299 --> 00:18:44,700
risk assessments.

356
00:18:44,766 --> 00:18:49,099
What is it in your organization
that is most risky concerning

357
00:18:49,166 --> 00:18:50,332
your end users?

358
00:18:50,400 --> 00:18:52,599
Those are the messages
you want to focus on.

359
00:18:53,033 --> 00:18:55,265
You can't overwhelm them
with everything at once.

360
00:18:55,333 --> 00:18:58,033
You have to pick
your priorities.

361
00:18:58,099 --> 00:19:01,700
Then you change one or two
behaviors at a time and you keep

362
00:19:01,766 --> 00:19:03,000
building upon that.

363
00:19:03,000 --> 00:19:07,299
Another really important issue
when it comes to efficacy is

364
00:19:07,366 --> 00:19:11,766
making sure that people
have the tools for the job.

365
00:19:11,833 --> 00:19:15,933
And this is a really big pet
hate of mine, when we do

366
00:19:16,000 --> 00:19:19,500
awareness raising but we
don't give anyone a solution to

367
00:19:19,566 --> 00:19:20,833
engage with.

368
00:19:20,833 --> 00:19:24,799
So, for example, classic
one of course is passwords.

369
00:19:24,866 --> 00:19:28,666
We will tell people they need to
have a strong, unique password

370
00:19:28,733 --> 00:19:30,500
for all of their accounts.

371
00:19:30,566 --> 00:19:31,700
They've got how many accounts?

372
00:19:31,766 --> 00:19:34,400
According to some research,
thirty, according to other

373
00:19:34,466 --> 00:19:35,733
research, a hundred.

374
00:19:35,799 --> 00:19:37,966
I have lost track of
how many accounts I've got.

375
00:19:38,033 --> 00:19:39,132
I have no idea.

376
00:19:39,133 --> 00:19:42,200
But if we tell them they need to
have a strong, unique password

377
00:19:42,266 --> 00:19:44,833
for all of their accounts, we
have to tell them how they're

378
00:19:44,900 --> 00:19:46,000
going to manage that.

379
00:19:46,066 --> 00:19:49,866
We can't expect them to just
remember those because the

380
00:19:49,933 --> 00:19:52,433
cognitive load is way too high.

381
00:19:52,500 --> 00:19:54,566
Nobody can do that.

382
00:19:54,633 --> 00:19:57,666
And if we as security
professionals are recommending

383
00:19:57,733 --> 00:20:00,700
to people that they do it, we
have to ask ourselves, are we

384
00:20:00,766 --> 00:20:01,566
really doing that?

385
00:20:02,266 --> 00:20:05,866
We are either using a
password manager, great.

386
00:20:05,933 --> 00:20:09,500
Writing them down, that's
okay in some scenarios.

387
00:20:09,566 --> 00:20:12,366
Or we're reusing
weaker passwords.

388
00:20:12,433 --> 00:20:15,000
If we can't do it,
we can't ask them to.

389
00:20:15,066 --> 00:20:18,633
So, if you're in an organization
that is doing awareness raising

390
00:20:18,700 --> 00:20:22,633
about passwords but doesn't have
an enterprise level password

391
00:20:22,700 --> 00:20:25,833
management tool and isn't
rolling that out with good

392
00:20:25,900 --> 00:20:29,666
workshops, you're asking people
to do the impossible and they're

393
00:20:29,733 --> 00:20:32,566
going to switch off, not
just from that message, but

394
00:20:32,633 --> 00:20:35,766
potentially from other
security messages as well.

395
00:20:37,900 --> 00:20:41,033
And then what we really need
to do is make sure people feel

396
00:20:41,099 --> 00:20:44,000
empowered, that
they feel confident.

397
00:20:44,066 --> 00:20:46,866
This is one of the big issues I
see when I do awareness raising

398
00:20:46,933 --> 00:20:51,400
in clients, is people feel
intimidated by technology in

399
00:20:51,466 --> 00:20:55,332
general, often, but
also really by security.

400
00:20:55,400 --> 00:20:58,900
They feel like they don't want
to ask the stupid question.

401
00:20:58,966 --> 00:21:01,933
They feel like if they get
something wrong, the finger is

402
00:21:02,000 --> 00:21:03,200
going to be pointed at them.

403
00:21:03,266 --> 00:21:04,966
They're going to be blamed,
they're maybe going to be

404
00:21:05,033 --> 00:21:05,899
laughed at.

405
00:21:05,966 --> 00:21:09,933
So, people do not feel
psychologically safe when it

406
00:21:10,000 --> 00:21:12,599
comes to security in
their organization.

407
00:21:13,833 --> 00:21:18,233
So, we need to encourage people
to feel confident and also to

408
00:21:18,299 --> 00:21:22,500
feel intrinsically motivated
to engage in security because

409
00:21:22,566 --> 00:21:25,866
that's really the only way we
can get them to long-term change

410
00:21:25,933 --> 00:21:27,466
their behaviors.

411
00:21:27,533 --> 00:21:28,366
How do we do that?

412
00:21:30,033 --> 00:21:33,699
One of the key things, from
my point of view, is to shift

413
00:21:33,766 --> 00:21:35,000
our messaging.

414
00:21:35,066 --> 00:21:38,333
We generally, in security,
understandably, we're

415
00:21:38,400 --> 00:21:39,866
quite negative.

416
00:21:39,933 --> 00:21:43,299
We generally are quite
pessimistic with our messaging.

417
00:21:43,366 --> 00:21:46,299
And of course, we are because
we're usually dealing with

418
00:21:46,366 --> 00:21:47,200
a negative.

419
00:21:47,266 --> 00:21:49,333
We're dealing with threats.

420
00:21:49,400 --> 00:21:53,099
But the problem with that is
that people are far more likely

421
00:21:53,166 --> 00:21:56,399
to engage with a positive
message than a negative one.

422
00:21:57,133 --> 00:22:00,099
So, if we keep hammering
the negative messages and

423
00:22:00,166 --> 00:22:03,733
questioning why people aren't
engaging with them, we need to

424
00:22:03,799 --> 00:22:07,400
think about how people respond
to messages and whether we can

425
00:22:07,466 --> 00:22:10,599
actually change our
messaging to be more effective.

426
00:22:12,000 --> 00:22:15,133
People are far more likely to
engage with a positive message

427
00:22:15,200 --> 00:22:17,200
and we see this all the
time on social media.

428
00:22:17,266 --> 00:22:20,666
This is why we love the likes
and the retweets and all of the

429
00:22:20,733 --> 00:22:23,899
positivity that we get when
someone engages with a post

430
00:22:23,966 --> 00:22:27,065
because it triggers that
dopamine hit in our brain that

431
00:22:27,133 --> 00:22:29,466
we don't get from
a negative message.

432
00:22:30,166 --> 00:22:31,899
So, how do we do
this in security?

433
00:22:31,966 --> 00:22:37,000
Well, partly it's about not just
punishing bad behaviors, but

434
00:22:37,066 --> 00:22:40,133
more importantly looking at how
we can positively reinforce

435
00:22:40,200 --> 00:22:41,233
good behaviors.

436
00:22:41,299 --> 00:22:46,099
So, how can we reward people
when they engage in security in

437
00:22:46,166 --> 00:22:49,599
a positive way rather than just
waiting for people to make

438
00:22:49,666 --> 00:22:51,599
mistakes and punishing them?

439
00:22:52,933 --> 00:22:56,833
And the importance of being
positive ties in with the

440
00:22:56,900 --> 00:22:58,000
optimism bias.

441
00:22:58,066 --> 00:23:03,066
So, there has been research
conducted by neuroscientists,

442
00:23:03,133 --> 00:23:07,599
led by Dr. Tali Sharot, and this
research has been conducted

443
00:23:07,666 --> 00:23:12,799
around the world to look into
whether we approach our futures

444
00:23:12,866 --> 00:23:14,666
in a positive or a negative way.

445
00:23:15,299 --> 00:23:18,733
And what the researchers have
found over the last ten years is

446
00:23:18,799 --> 00:23:22,833
that the vast majority of
people, about 80% of people are

447
00:23:22,900 --> 00:23:26,566
wired towards optimism in
our own personal lives.

448
00:23:27,666 --> 00:23:31,166
So, we might think the world
around us is getting worse,

449
00:23:31,233 --> 00:23:32,332
but me?

450
00:23:32,400 --> 00:23:34,799
I am going to be just fine.

451
00:23:34,866 --> 00:23:39,799
And this optimism bias persists
regardless of geography, of

452
00:23:39,866 --> 00:23:43,833
gender, of ethnicity,
of socioeconomic status.

453
00:23:43,900 --> 00:23:48,200
Regardless of any demographic
factor, most people are wired

454
00:23:48,266 --> 00:23:51,000
towards optimism
about their own future.

455
00:23:52,066 --> 00:23:56,733
This is why people prefer, in
general, Fridays to Sundays.

456
00:23:56,799 --> 00:24:01,066
We love that feeling when we're
at work on a Friday, thinking

457
00:24:01,133 --> 00:24:05,299
about our day off on Sunday more
than we actually enjoy that day

458
00:24:05,366 --> 00:24:07,033
off on Sunday.

459
00:24:07,099 --> 00:24:08,332
Because we're optimistic.

460
00:24:10,933 --> 00:24:13,266
What does this got to
do with cybersecurity?

461
00:24:13,333 --> 00:24:19,200
Well, the research shows that
optimism persists in the face

462
00:24:19,266 --> 00:24:20,733
of statistics.

463
00:24:20,799 --> 00:24:23,966
So, you can talk to people
about negative facts.

464
00:24:24,033 --> 00:24:28,199
You can tell people how many
hacks are taking place, how much

465
00:24:28,266 --> 00:24:34,066
money it's costing, how it can
target or damage them and people

466
00:24:34,133 --> 00:24:37,900
will think yeah, that might be
happening to everyone around me,

467
00:24:37,966 --> 00:24:38,599
but me?

468
00:24:38,666 --> 00:24:39,866
I'm going to be just fine.

469
00:24:40,500 --> 00:24:44,333
It's like if you ask someone on
their wedding day what the rate

470
00:24:44,400 --> 00:24:45,966
of divorce is.

471
00:24:46,033 --> 00:24:49,399
People know that a good
proportion of marriages end

472
00:24:49,466 --> 00:24:50,733
in divorce.

473
00:24:50,799 --> 00:24:53,200
Do you think your marriage
is going to end in divorce?

474
00:24:53,266 --> 00:24:55,666
How many people on their wedding
day would think it would?

475
00:24:55,733 --> 00:24:58,265
Nobody does, because
it doesn't happen to me.

476
00:24:58,333 --> 00:25:00,266
Why would hackers want my data?

477
00:25:00,266 --> 00:25:05,700
And so, for example, from this
research by Sharot and her team,

478
00:25:05,766 --> 00:25:08,966
they asked people how likely
they thought they were of

479
00:25:09,033 --> 00:25:12,233
getting a terrible
disease like cancer.

480
00:25:12,299 --> 00:25:16,400
And the average response was,
you know, about 10% maybe.

481
00:25:16,466 --> 00:25:20,799
So, the researchers said to
them, unfortunately, we all have

482
00:25:20,866 --> 00:25:25,833
a statistical likelihood of
30% of developing cancer in

483
00:25:25,900 --> 00:25:27,299
our lifetime.

484
00:25:27,366 --> 00:25:28,900
Now you have that fact.

485
00:25:28,966 --> 00:25:33,433
How likely do you think you
are of developing cancer?

486
00:25:33,500 --> 00:25:36,599
Maybe 11%?

487
00:25:36,666 --> 00:25:38,500
People don't think it's
going to happen to them.

488
00:25:38,933 --> 00:25:42,833
So, if we keep trying to use
facts, negative facts to drive

489
00:25:42,900 --> 00:25:45,966
our awareness raising,
unfortunately it's going to keep

490
00:25:46,033 --> 00:25:49,533
falling flat because it doesn't
convince the individuals that it

491
00:25:49,599 --> 00:25:50,633
relates to them.

492
00:25:52,200 --> 00:25:55,833
So, that sounds like quite a
pessimistic message, ironically,

493
00:25:55,900 --> 00:26:00,700
but the good news, and it's
so obvious, but the research

494
00:26:00,766 --> 00:26:01,666
supports it.

495
00:26:01,733 --> 00:26:05,899
The good news is that optimism
makes people try harder.

496
00:26:05,966 --> 00:26:10,065
If you give people an optimistic
message, of course they are far

497
00:26:10,133 --> 00:26:13,299
more likely to engage in
it than a negative message.

498
00:26:13,966 --> 00:26:17,699
If you can tell someone, you
know, the sky is falling in,

499
00:26:17,766 --> 00:26:21,233
cybersecurity is terrible,
everyone's getting hacked, oh,

500
00:26:21,299 --> 00:26:23,933
but make sure you
change your password.

501
00:26:24,000 --> 00:26:25,833
I mean, why would
somebody bother?

502
00:26:26,299 --> 00:26:30,366
If you tell them cybersecurity
is real and it relates to you,

503
00:26:30,433 --> 00:26:35,933
but you can engage in these few
quite simple behaviors to make

504
00:26:36,000 --> 00:26:41,400
your resistance to the threat
95%, 98%, avoid that threat to

505
00:26:41,466 --> 00:26:42,733
such a high level?

506
00:26:42,799 --> 00:26:46,066
That's an optimistic message
that people are far more likely

507
00:26:46,133 --> 00:26:47,000
to engage in.

508
00:26:48,233 --> 00:26:52,066
So, this goes back, really, to
the fear appeal research; giving

509
00:26:52,133 --> 00:26:56,066
a strong efficacy message is far
more likely to be successful.

510
00:26:58,000 --> 00:27:03,599
Another tool that I find is
really engaging when it comes to

511
00:27:03,666 --> 00:27:06,599
scaling up awareness raising
and when it comes to changing

512
00:27:06,666 --> 00:27:10,765
behaviors in an
organization is social proof.

513
00:27:10,833 --> 00:27:14,266
Social proof can really help
to amplify our messages.

514
00:27:15,400 --> 00:27:19,299
So again, psychology shows us
that if we as humans don't know

515
00:27:19,366 --> 00:27:23,299
what to do, we will mimic the
behaviors of other people.

516
00:27:23,366 --> 00:27:27,966
This is what Trip Advisor,
Airbnb, Google reviews, all of

517
00:27:28,033 --> 00:27:30,765
these things are
based on, social proof.

518
00:27:30,833 --> 00:27:33,200
And researchers have even
put a statistic to it.

519
00:27:33,266 --> 00:27:35,333
So, I think it was the
University of Pennsylvania

520
00:27:35,400 --> 00:27:39,299
recently did some studies and
they found that if 25% of a

521
00:27:39,366 --> 00:27:43,333
group acts in a certain way, the
rest of the group will follow;

522
00:27:43,400 --> 00:27:45,966
25% is about that tipping point.

523
00:27:45,966 --> 00:27:50,099
So, if 25% of you get up and
leave now, I'm going to be

524
00:27:50,166 --> 00:27:51,599
talking to myself.

525
00:27:51,666 --> 00:27:55,765
Or, more likely, I'm going to
follow because we think other

526
00:27:55,833 --> 00:28:00,166
people know better, so we think
oh, that 25%, they must have

527
00:28:00,233 --> 00:28:01,399
heard a fire alarm.

528
00:28:01,466 --> 00:28:04,166
Or they must have seen the
schedule and know that she's

529
00:28:04,233 --> 00:28:05,166
talking too long.

530
00:28:05,233 --> 00:28:08,633
Or they must have decided
this is rubbish, so it must be

531
00:28:08,700 --> 00:28:10,766
rubbish, so I'm
going to leave as well.

532
00:28:10,833 --> 00:28:14,766
We put our trust in a good
number of other people behaving

533
00:28:14,833 --> 00:28:18,366
in a certain way, thinking good
enough for them, good enough

534
00:28:18,433 --> 00:28:19,099
for me.

535
00:28:19,966 --> 00:28:24,433
Think for a minute about social
proof and cybersecurity because

536
00:28:24,500 --> 00:28:27,299
when I look at a lot of
cybersecurity messages, what I

537
00:28:27,366 --> 00:28:30,833
see is us shooting
ourselves in the foot.

538
00:28:30,900 --> 00:28:34,099
For example, we try and do
awareness raising where we tell

539
00:28:34,166 --> 00:28:37,799
everyone how many bad
passwords there are out there

540
00:28:37,866 --> 00:28:39,866
on the internet.

541
00:28:39,933 --> 00:28:42,933
And we think we're scaring
people into looking and

542
00:28:43,000 --> 00:28:44,366
saying what?

543
00:28:44,433 --> 00:28:47,400
There's that many people using
password1 as a password?

544
00:28:47,466 --> 00:28:48,533
That's ridiculous.

545
00:28:48,599 --> 00:28:51,899
Instead, according to social
proof, what we're doing is we're

546
00:28:51,966 --> 00:28:56,065
saying everyone else is using a
terrible password, so you don't

547
00:28:56,133 --> 00:28:57,766
need to worry about it.

548
00:28:57,833 --> 00:28:59,299
So, people hear this message.

549
00:28:59,366 --> 00:29:02,033
They hear the fact that
there's loads of people using

550
00:29:02,099 --> 00:29:05,399
weak passwords out
there and they think sweet.

551
00:29:05,466 --> 00:29:06,399
I'm fine.

552
00:29:06,466 --> 00:29:08,733
Everyone else is
using a bad password.

553
00:29:08,799 --> 00:29:10,833
There's no need for
me to change either.

554
00:29:11,866 --> 00:29:15,766
So, how can we use social
proof to our advantage?

555
00:29:15,833 --> 00:29:19,500
If you're in an organization
that does phishing simulations,

556
00:29:19,566 --> 00:29:23,000
perhaps you communicate the
rate of people that click on

557
00:29:23,066 --> 00:29:24,299
the link.

558
00:29:24,366 --> 00:29:28,466
So, most communications go along
the lines of we did a phishing

559
00:29:28,533 --> 00:29:32,433
simulation and 20% of
people clicked the link.

560
00:29:32,500 --> 00:29:33,866
That's really bad.

561
00:29:33,933 --> 00:29:35,700
Do better next time.

562
00:29:35,766 --> 00:29:39,266
According to social proof, we
would be way more effective if

563
00:29:39,333 --> 00:29:42,433
we said we did a
phishing simulation.

564
00:29:42,500 --> 00:29:45,466
80% of people did
not click the link.

565
00:29:45,533 --> 00:29:47,099
That is fantastic.

566
00:29:47,166 --> 00:29:50,899
Next time, if you're in that
20%, join the majority of your

567
00:29:50,966 --> 00:29:54,433
colleagues and keep
our organization safe.

568
00:29:54,500 --> 00:29:56,099
That's an empowering message.

569
00:29:56,166 --> 00:29:59,733
That's an engaging message, and
that's a message that builds on

570
00:29:59,799 --> 00:30:00,700
social proof.

571
00:30:03,066 --> 00:30:07,200
I always think this is a very
funny story when it comes to

572
00:30:07,266 --> 00:30:08,200
social proof.

573
00:30:08,266 --> 00:30:11,166
Has anybody heard about the
Shed in Dulwich in London?

574
00:30:11,566 --> 00:30:15,733
So, this was a journalist who
wanted to see whether he could

575
00:30:15,799 --> 00:30:19,066
turn his garden shed - does that
translate to a US audience?

576
00:30:19,133 --> 00:30:19,733
A shed?

577
00:30:19,799 --> 00:30:20,700
Yes, thank you.

578
00:30:20,766 --> 00:30:25,166
Wanted to turn his garden
shed on TripAdvisor into

579
00:30:25,233 --> 00:30:26,332
a restaurant.

580
00:30:26,333 --> 00:30:28,500
His shed was not a restaurant.

581
00:30:28,566 --> 00:30:31,700
It was full of the usual
things that sheds are full of.

582
00:30:31,766 --> 00:30:35,099
But he put it up on TripAdvisor
and he got a few people to do

583
00:30:35,166 --> 00:30:36,265
some reviews.

584
00:30:36,333 --> 00:30:41,099
And he basically used social
proof on TripAdvisor to end up

585
00:30:41,166 --> 00:30:45,099
getting his shed as the
top-rated restaurant in his area

586
00:30:45,166 --> 00:30:49,332
despite never
having served any food.

587
00:30:49,333 --> 00:30:51,500
So, that's funny enough.

588
00:30:51,566 --> 00:30:56,066
But what he then decided to do
was actually open the shed one

589
00:30:56,133 --> 00:30:57,533
night for dinner.

590
00:30:57,599 --> 00:31:01,066
He was getting calls of people
trying to book into his shed,

591
00:31:01,133 --> 00:31:02,799
and he's saying I'm really
sorry, we're booked up for the

592
00:31:02,866 --> 00:31:04,133
next six months.

593
00:31:04,200 --> 00:31:04,799
That's fine.

594
00:31:04,866 --> 00:31:06,900
I'll book in in
six months' time.

595
00:31:06,900 --> 00:31:09,766
So, they opened up the shed.

596
00:31:09,833 --> 00:31:15,700
And he got some microwave meals
and people booked tables and

597
00:31:15,766 --> 00:31:19,066
they went and they were
served these microwave meals.

598
00:31:19,133 --> 00:31:22,900
And because of social proof,
they went away saying, "That was

599
00:31:22,966 --> 00:31:25,199
the best meal I've ever had.

600
00:31:25,266 --> 00:31:29,566
Can we come back next
year for my big birthday?"

601
00:31:29,633 --> 00:31:34,099
And gave amazing reviews on
TripAdvisor, all because they

602
00:31:34,166 --> 00:31:38,433
had been primed to think that
the Shed in Dulwich was the best

603
00:31:38,500 --> 00:31:39,466
place they could eat.

604
00:31:40,766 --> 00:31:45,633
So, I love that example about
social proof because it shows

605
00:31:45,700 --> 00:31:47,966
the power of social proof,
but also because it's funny.

606
00:31:48,033 --> 00:31:52,599
I love whenever you can get a
chance to be funny, particularly

607
00:31:52,666 --> 00:31:54,299
when talking
about cybersecurity.

608
00:31:54,366 --> 00:31:56,599
People don't really
expect us to be funny.

609
00:31:56,666 --> 00:31:59,733
People come into awareness
raising thinking it's going to

610
00:31:59,799 --> 00:32:02,200
be dry and it's
going to be boring.

611
00:32:02,266 --> 00:32:05,933
And there's so many funny
things we can talk about.

612
00:32:06,500 --> 00:32:09,833
And it's good to be funny
because research shows that a

613
00:32:09,900 --> 00:32:13,400
funny or a weird
message is more memorable.

614
00:32:13,466 --> 00:32:17,565
People are more likely to
go away and remember that.

615
00:32:17,566 --> 00:32:21,900
However, just like fear, using
humor comes with a warning.

616
00:32:21,966 --> 00:32:25,433
We have to understand that there
are different ways that you can

617
00:32:25,500 --> 00:32:29,033
use these emotions to be
effective, but that if you don't

618
00:32:29,099 --> 00:32:32,099
know what you're doing, they
can actually backfire on you.

619
00:32:33,033 --> 00:32:36,366
So, speaking of humor,
the CDC campaign about the

620
00:32:36,433 --> 00:32:39,066
zombie apocalypse.

621
00:32:39,133 --> 00:32:45,333
CDC wanted to do a big campaign
about pandemics and they chose

622
00:32:45,400 --> 00:32:49,333
the zombie apocalypse kind of
analogy to encourage people to

623
00:32:49,400 --> 00:32:53,466
prep like, a go-bag of all of
the stuff that they would need.

624
00:32:53,533 --> 00:32:56,966
And they were delighted, as any
awareness professional would be,

625
00:32:57,033 --> 00:32:59,000
to see this campaign go viral.

626
00:32:59,066 --> 00:33:00,466
People loved it.

627
00:33:00,533 --> 00:33:02,366
They thought it was hilarious.

628
00:33:02,433 --> 00:33:04,299
They were sharing
it on social media.

629
00:33:04,366 --> 00:33:05,233
They were watching it.

630
00:33:05,299 --> 00:33:06,566
They were talking about it.

631
00:33:06,900 --> 00:33:08,500
So, the CDC built on this.

632
00:33:08,566 --> 00:33:10,400
They did more and more
communications around

633
00:33:10,466 --> 00:33:11,065
this theme.

634
00:33:11,133 --> 00:33:14,166
Absolutely brilliant.

635
00:33:14,166 --> 00:33:17,033
What they didn't do, though, is
check whether it was having the

636
00:33:17,099 --> 00:33:20,033
intended result in
terms of behavior.

637
00:33:20,099 --> 00:33:22,399
So, some researchers
did then look into it.

638
00:33:22,466 --> 00:33:25,699
They looked into whether this
humorous campaign was actually

639
00:33:25,766 --> 00:33:28,133
driving positive behavior.

640
00:33:28,200 --> 00:33:32,333
And they found that when they
compared people who had seen the

641
00:33:32,400 --> 00:33:36,966
CDC zombie campaign with people
who hadn't, that actually the

642
00:33:37,033 --> 00:33:41,065
people who hadn't seen it were
more likely to be prepared for a

643
00:33:41,133 --> 00:33:43,833
zombie apocalypse
than the people who had.

644
00:33:44,866 --> 00:33:48,033
The researchers concluded that
basically the humor of the

645
00:33:48,099 --> 00:33:52,033
message, unfortunately,
undermined the efficacy.

646
00:33:52,099 --> 00:33:55,666
So, people thought it was so
funny that it couldn't be real,

647
00:33:55,733 --> 00:33:58,466
so they didn't take it seriously
and they didn't engage in the

648
00:33:58,533 --> 00:34:01,899
behaviors that the CDC
actually wanted to drive.

649
00:34:02,900 --> 00:34:06,166
So, it's a really good example
of the fact that awareness does

650
00:34:06,233 --> 00:34:11,065
not always lead to behaviors
or the behaviors that we want.

651
00:34:12,666 --> 00:34:16,033
And another example of that
would be the Got Milk campaign.

652
00:34:16,099 --> 00:34:18,666
I mean, everybody knows
this slogan, right?

653
00:34:18,733 --> 00:34:21,066
Surely that's a successful
awareness campaign.

654
00:34:21,132 --> 00:34:23,966
It was extremely popular.

655
00:34:24,033 --> 00:34:27,665
But throughout the use of the
campaign, milk consumption

656
00:34:27,733 --> 00:34:29,366
kept declining.

657
00:34:29,433 --> 00:34:32,933
People loved the campaign, but
they weren't drinking more milk;

658
00:34:33,000 --> 00:34:34,199
they were drinking less milk.

659
00:34:34,599 --> 00:34:37,933
So, the milk industry realized
that they had to do something

660
00:34:38,000 --> 00:34:39,000
about it.

661
00:34:39,065 --> 00:34:42,832
They did a lot of research to
look into this and they found

662
00:34:42,900 --> 00:34:45,833
that it just wasn't engaging
people to actually change their

663
00:34:45,900 --> 00:34:48,500
behaviors and how they
felt about drinking milk.

664
00:34:48,565 --> 00:34:51,098
And they discovered a few
things that would change those

665
00:34:51,166 --> 00:34:52,599
behaviors and that did.

666
00:34:52,833 --> 00:34:56,300
One was to focus on the protein
level of milk because people

667
00:34:56,366 --> 00:34:58,833
generally want to
consume more protein now.

668
00:34:58,900 --> 00:35:02,266
One was to have a partnership
with McDonald's where you could

669
00:35:02,333 --> 00:35:03,800
get milk as part
of the Happy Meal.

670
00:35:03,866 --> 00:35:08,533
And the other was to change the
shape of milk containers which

671
00:35:08,599 --> 00:35:12,466
were always like kind of square
ones and to make some round so

672
00:35:12,533 --> 00:35:14,633
they can go in
your car cup container.

673
00:35:15,833 --> 00:35:17,666
So, we can't just
think about awareness.

674
00:35:17,733 --> 00:35:22,099
We have to test our messages to
see if they land right, if they

675
00:35:22,166 --> 00:35:27,099
hit the points we want to in
terms of changing behavior

676
00:35:27,166 --> 00:35:31,033
because as that Got Milk
campaign clearly shows, it is

677
00:35:31,099 --> 00:35:32,599
not just about awareness.

678
00:35:32,666 --> 00:35:35,433
We can't just focus
on raising awareness.

679
00:35:35,500 --> 00:35:38,800
We have to think about whether
we're making our security

680
00:35:38,866 --> 00:35:42,599
usable, whether people are going
away from our campaigns and

681
00:35:42,666 --> 00:35:45,033
actually engaging in the
behaviors we recommend.

682
00:35:45,900 --> 00:35:49,500
And again, it comes back to that
fear appeal point of efficacy.

683
00:35:49,566 --> 00:35:52,665
People have to have
the tools to engage in.

684
00:35:52,733 --> 00:35:55,566
We have to build the
security around them.

685
00:35:57,066 --> 00:36:00,933
And the aviation industry gives
a fantastic example of this.

686
00:36:01,000 --> 00:36:05,533
Since the 1950s, accidents in
the aviation industry have

687
00:36:05,599 --> 00:36:10,866
rapidly declined, so they have a
really low level of incidence.

688
00:36:10,933 --> 00:36:13,566
This has partly been
technological because of the

689
00:36:13,633 --> 00:36:17,133
introduction of the jet
engine, but it's also very much

690
00:36:17,199 --> 00:36:18,300
human based.

691
00:36:18,300 --> 00:36:22,000
The 1950s is when the aviation
industry started to put people

692
00:36:22,066 --> 00:36:23,066
at the center.

693
00:36:23,133 --> 00:36:26,299
So, they started to look at how
the pilots and how the crew

694
00:36:26,366 --> 00:36:30,733
engaged in technology, and then
they created that technology

695
00:36:30,800 --> 00:36:31,633
around them.

696
00:36:32,833 --> 00:36:37,233
They also introduced a just
culture, so a culture that moved

697
00:36:37,300 --> 00:36:40,699
away from blaming
individuals for incidents.

698
00:36:40,766 --> 00:36:43,733
When an incident happens in the
aviation industry, it's not

699
00:36:43,800 --> 00:36:47,199
about pointing the finger; it's
about looking at that incident

700
00:36:47,266 --> 00:36:52,733
very openly so that you can see
what's happened, how can we stop

701
00:36:52,800 --> 00:36:53,900
it happening again?

702
00:36:55,066 --> 00:36:58,966
The aviation industry understood
that a fear-based culture, a

703
00:36:59,033 --> 00:37:03,033
culture that blames people for
incidents does not reduce the

704
00:37:03,099 --> 00:37:04,366
number of incidents.

705
00:37:04,433 --> 00:37:09,400
It just reduces the number of
reports of incidents which is

706
00:37:09,466 --> 00:37:11,732
the last thing that
we want in security.

707
00:37:12,166 --> 00:37:16,733
If we have a blame culture, a
fear culture, then people don't

708
00:37:16,800 --> 00:37:20,066
make less mistakes when it comes
to information and technology.

709
00:37:20,133 --> 00:37:22,500
They just don't
tell us about them.

710
00:37:22,566 --> 00:37:25,533
And we in this room all know
that that's the last thing that

711
00:37:25,599 --> 00:37:26,599
we want.

712
00:37:26,666 --> 00:37:29,466
If someone is at the center of
an incident and they keep it

713
00:37:29,533 --> 00:37:32,966
quiet, it just means we have
less time to act on it, to

714
00:37:33,033 --> 00:37:34,833
investigate, to remediate.

715
00:37:36,133 --> 00:37:38,966
So, we really need to be
thinking not just about

716
00:37:39,033 --> 00:37:43,066
awareness, not just about
behavior, but about culture.

717
00:37:43,133 --> 00:37:45,698
What kind of culture
do we want in security?

718
00:37:45,766 --> 00:37:50,433
What kind of security culture do
you have in your organization?

719
00:37:50,500 --> 00:37:52,566
Do you measure that culture?

720
00:37:52,633 --> 00:37:55,598
Do you have metrics in place to
tell you whether that culture is

721
00:37:55,666 --> 00:37:58,633
moving in the
direction you want it to?

722
00:37:58,699 --> 00:38:01,666
Do you know what behaviors would
make up the kind of culture you

723
00:38:01,733 --> 00:38:05,133
want and how do you raise
awareness that is targeted at

724
00:38:05,199 --> 00:38:06,366
those behaviors?

725
00:38:07,400 --> 00:38:10,599
We really need to start thinking
about our culture both in our

726
00:38:10,666 --> 00:38:14,900
organizations and the kind of
security culture that we have

727
00:38:14,966 --> 00:38:15,633
in general.

728
00:38:15,699 --> 00:38:18,233
What is the culture
of our profession?

729
00:38:18,300 --> 00:38:21,400
Because these are the things
that really influence us as

730
00:38:21,466 --> 00:38:26,000
individuals, as teams, as
organizations, but also really

731
00:38:26,066 --> 00:38:29,500
influences the people that
we're trying to engage with.

732
00:38:32,000 --> 00:38:35,099
So, I'm going to wrap up my
presentation now with some time

733
00:38:35,166 --> 00:38:36,133
for questions.

734
00:38:36,199 --> 00:38:38,800
But I just wanted to give you
the takeaways before I do.

735
00:38:40,166 --> 00:38:43,733
One of the key things that I
have hoped to convey today is

736
00:38:43,800 --> 00:38:46,933
that when you're talking about
something scary, which we often

737
00:38:47,000 --> 00:38:50,900
are when it comes to security,
we really need to focus on the

738
00:38:50,966 --> 00:38:53,433
efficacy of our message.

739
00:38:53,500 --> 00:38:57,233
Too much in the past we've gone
very heavy on the threat and we

740
00:38:57,300 --> 00:39:00,533
thought if we just talk enough
about the threat, we can scare

741
00:39:00,599 --> 00:39:02,533
people into behaving as we want.

742
00:39:03,633 --> 00:39:07,232
We need to move away from that
approach and focus much more on

743
00:39:07,300 --> 00:39:09,066
empowering people.

744
00:39:09,133 --> 00:39:13,000
How we do that is reducing the
noise so that people can really

745
00:39:13,066 --> 00:39:15,566
engage with our most
important messages.

746
00:39:16,666 --> 00:39:21,433
We can harness the optimism bias
and social proof to drive more

747
00:39:21,500 --> 00:39:24,433
positive behaviors and drive
a more positive culture.

748
00:39:25,733 --> 00:39:28,766
We absolutely have to provide
people with the tools.

749
00:39:28,833 --> 00:39:32,300
If you're doing an awareness
message but you don't have a

750
00:39:32,366 --> 00:39:35,233
tool that people can engage
with, you don't have a way that

751
00:39:35,300 --> 00:39:38,800
people can go and implement your
message, then I'd encourage you

752
00:39:38,866 --> 00:39:41,699
to take a step back and ask
whether what you're asking of

753
00:39:41,766 --> 00:39:43,866
people is really realistic.

754
00:39:43,866 --> 00:39:48,133
And really underpinning all
of this is harnessing a more

755
00:39:48,199 --> 00:39:51,599
positive culture in
cybersecurity so that people in

756
00:39:51,666 --> 00:39:56,333
our organizations feel they can
come to us with an incident,

757
00:39:56,400 --> 00:40:00,766
with a question, with a concern,
and so that we can have the kind

758
00:40:00,833 --> 00:40:04,300
of culture that actually looks
at incidents without blaming

759
00:40:04,366 --> 00:40:07,533
people and means we're
far more likely to increase

760
00:40:07,599 --> 00:40:08,333
the reporting.

761
00:40:09,433 --> 00:40:11,833
So, thank you so much for your
time, for your attention.

762
00:40:11,900 --> 00:40:14,633
It's been an absolute pleasure
to speak with you all today.

763
00:40:14,699 --> 00:40:17,733
I'm at RSA for the rest of the
week, so I would be delighted to

764
00:40:17,800 --> 00:40:20,766
chat with anyone
over the coming days.

765
00:40:20,833 --> 00:40:22,166
You can find me on Twitter.

766
00:40:22,233 --> 00:40:25,433
You can go and check out our
website and our blog where

767
00:40:25,500 --> 00:40:28,533
there's lots of my work and
the work of my colleagues.

768
00:40:28,599 --> 00:40:31,266
In the meantime, if anyone has
a question now, I would be very

769
00:40:31,333 --> 00:40:32,566
happy to take it.

770
00:40:38,666 --> 00:40:39,733
We have a question here.

771
00:40:39,800 --> 00:40:44,599
I think there's a mic coming
if we can just - ah, sorry.

772
00:40:44,666 --> 00:40:46,133
If you can come to the mic.

773
00:40:46,199 --> 00:40:47,300
Thank you.

774
00:40:48,566 --> 00:40:50,232
I'm going to grab
some water while you do.

775
00:40:54,633 --> 00:40:57,299
>> AUDIENCE: Hi. My name is
Zach with Clements Worldwide.

776
00:40:57,366 --> 00:40:58,400
>> DR. JESSICA BARKER: Hi.

777
00:40:58,466 --> 00:41:01,366
>> AUDIENCE: And my question is
how can an organization leverage

778
00:41:01,433 --> 00:41:04,566
social proof when mitigating
reputational harm after a

779
00:41:04,633 --> 00:41:05,966
data breach?

780
00:41:06,033 --> 00:41:07,766
>> DR. JESSICA BARKER: Yeah,
s social proof, particularly

781
00:41:07,833 --> 00:41:10,066
mitigating after a data breach.

782
00:41:10,133 --> 00:41:14,566
So, any positive lessons you can
take from a data breach, you

783
00:41:14,633 --> 00:41:17,466
know, that this incident
was reported to us.

784
00:41:17,533 --> 00:41:20,098
This is how the team
responded to it.

785
00:41:20,166 --> 00:41:22,133
This is what technology
we've put in place.

786
00:41:22,199 --> 00:41:24,000
This is what we're
doing to build on it.

787
00:41:24,066 --> 00:41:27,966
So, any positive messages you
can draw particularly around how

788
00:41:28,033 --> 00:41:31,633
people behaved that was key
to help you responding to the

789
00:41:31,699 --> 00:41:33,533
incident, I would
really encourage.

790
00:41:33,599 --> 00:41:37,199
When it comes to social proof in
general, I find one of the most

791
00:41:37,266 --> 00:41:40,966
important or one of the most
successful mechanisms is a

792
00:41:41,033 --> 00:41:42,466
champions program.

793
00:41:42,533 --> 00:41:46,533
Sometimes called champions,
ambassadors, whatever it might

794
00:41:46,599 --> 00:41:49,433
be, but people in an
organization who aren't security

795
00:41:49,500 --> 00:41:52,699
professionals, but they take on
the role a bit like health and

796
00:41:52,766 --> 00:41:57,300
safety where they become like,
a security rep in their area of

797
00:41:57,366 --> 00:41:58,433
the business.

798
00:41:58,500 --> 00:42:00,366
They are not expected to become
an expert, but they maybe get a

799
00:42:00,433 --> 00:42:04,433
little bit of training and they
have a route into security.

800
00:42:05,199 --> 00:42:07,933
And having a champion's network
can be great for getting your

801
00:42:08,000 --> 00:42:11,766
messaging out, but also
providing that kind of social

802
00:42:11,833 --> 00:42:14,633
proof of having people dotted
throughout the business.

803
00:42:14,699 --> 00:42:17,866
And that can be really helpful,
both in responding to an

804
00:42:17,933 --> 00:42:22,333
incident, because they may
feedback issues quicker than

805
00:42:22,400 --> 00:42:23,400
would otherwise happen.

806
00:42:23,466 --> 00:42:27,000
But also, in feeding out what's
happened after an incident.

807
00:42:27,066 --> 00:42:30,066
Your champions can go to their
teams and say this is what's

808
00:42:30,133 --> 00:42:33,399
happened, these are the good
things we can take form it, and

809
00:42:33,466 --> 00:42:36,299
this is what we're implementing
so that it doesn't happen again

810
00:42:36,366 --> 00:42:38,400
or so that we're
better protected.

811
00:42:39,266 --> 00:42:40,400
Thank you for your question.

812
00:42:42,199 --> 00:42:43,233
Hi.

813
00:42:43,300 --> 00:42:44,566
>> AUDIENCE: Hi. You
talked a lot about empowerment.

814
00:42:44,633 --> 00:42:45,799
I definitely agree.

815
00:42:45,866 --> 00:42:49,233
What do you think about repeat
offenders, people that are doing

816
00:42:49,300 --> 00:42:52,500
things that are clearly it's
not just an accident or it's an

817
00:42:52,566 --> 00:42:55,066
accident but it's the third
and fourth and fifth time?

818
00:42:55,133 --> 00:42:58,265
Where do you think
accountability comes within

819
00:42:58,333 --> 00:42:59,433
an organization?

820
00:42:59,433 --> 00:43:02,500
>> DR. JESSICA BARKER: Yeah,
repeat offenders are a problem.

821
00:43:02,566 --> 00:43:05,198
When it comes to repeat
offenders, I usually find there

822
00:43:05,266 --> 00:43:07,699
is something beneath that.

823
00:43:07,766 --> 00:43:10,699
And it can be that they just
don't understand what they are

824
00:43:10,766 --> 00:43:11,900
supposed to do.

825
00:43:11,966 --> 00:43:14,232
We may feel like, well, we
have delivered the training.

826
00:43:14,300 --> 00:43:15,699
Everyone else understands it.

827
00:43:15,766 --> 00:43:16,766
Why don't they?

828
00:43:16,833 --> 00:43:19,433
But as an individual, worth
exploring whether there is a

829
00:43:19,500 --> 00:43:21,966
blocker like that that they just
don't understand or they can't

830
00:43:22,033 --> 00:43:23,533
use the technology.

831
00:43:23,599 --> 00:43:28,666
Another issue can go beyond
security and it can be that

832
00:43:28,733 --> 00:43:31,233
they're not happy in their job,
that there's something wrong

833
00:43:31,300 --> 00:43:32,166
at home.

834
00:43:32,233 --> 00:43:33,566
It can be all of these things.

835
00:43:33,633 --> 00:43:36,098
Unfortunately, we know
insecurity can have such an

836
00:43:36,166 --> 00:43:42,366
impact on our field but we don't
necessarily have control over.

837
00:43:42,900 --> 00:43:46,666
So, then I would encourage
conversations with HR, with

838
00:43:46,733 --> 00:43:47,966
their line management.

839
00:43:48,033 --> 00:43:52,333
I would always encourage having
a positive and empowering

840
00:43:52,400 --> 00:43:56,199
approach for as long as possible
until it becomes to the point

841
00:43:56,266 --> 00:44:00,500
where someone is obviously being
willfully negligent or, you

842
00:44:00,566 --> 00:44:02,399
know, is actually
being malicious.

843
00:44:02,466 --> 00:44:07,366
Of course, there has to be some
repercussion on people when it

844
00:44:07,433 --> 00:44:10,033
gets to that stage, but I do
find the majority of people

845
00:44:10,099 --> 00:44:13,000
don't fall into that
particular category.

846
00:44:13,066 --> 00:44:14,433
Thank you.

847
00:44:16,066 --> 00:44:17,933
>> AUDIENCE: So, I think these
concepts are amazing for

848
00:44:18,000 --> 00:44:21,166
creating awareness campaigns
within a company, but I'm

849
00:44:21,233 --> 00:44:24,199
wondering from an industry
perspective or industry level,

850
00:44:24,266 --> 00:44:28,466
specifically in healthcare where
I'm from, we have 58% of data

851
00:44:28,533 --> 00:44:30,299
breaches come from insiders.

852
00:44:30,366 --> 00:44:32,466
We're the only critical
infrastructure where you're more

853
00:44:32,533 --> 00:44:35,098
likely to be breached by
your - compromised by your

854
00:44:35,166 --> 00:44:37,099
internal people.

855
00:44:37,166 --> 00:44:40,633
So, how do we use these
concepts to create awareness in

856
00:44:40,699 --> 00:44:41,966
an entire industry?

857
00:44:42,033 --> 00:44:44,165
Do you do it at the individual
company level and hope it

858
00:44:44,233 --> 00:44:47,900
percolates up or is there some
larger awareness campaign?

859
00:44:47,966 --> 00:44:49,232
>> DR. JESSICA BARKER:
That's really good question.

860
00:44:49,300 --> 00:44:51,900
Again, I think that obviously,
that insider problem is

861
00:44:51,966 --> 00:44:55,333
prevalent but of course there
are industries where it is more

862
00:44:55,400 --> 00:44:56,000
of an issue.

863
00:44:56,066 --> 00:44:58,466
And I would say
that is a mixture.

864
00:44:58,533 --> 00:45:02,333
If you can work on an
individual organizational basis,

865
00:45:02,400 --> 00:45:04,566
then fantastic.

866
00:45:04,633 --> 00:45:09,165
But also trying to work on the
culture of the whole industry.

867
00:45:09,233 --> 00:45:12,300
So, whether that's at industry
events, you know, whether that

868
00:45:12,366 --> 00:45:16,066
is trying to have some kind of
consortium across the industry.

869
00:45:16,133 --> 00:45:19,665
Anything that you can do to
try and impact the culture in

870
00:45:19,733 --> 00:45:23,866
general and then trying to build
it up in organizations as well.

871
00:45:26,333 --> 00:45:27,166
>> AUDIENCE: Hello.

872
00:45:27,233 --> 00:45:28,300
>> DR. JESSICA BARKER: Hi.

873
00:45:28,366 --> 00:45:30,433
>> AUDIENCE: So, you talked a
lot about how to engage users.

874
00:45:30,500 --> 00:45:33,866
My sort of bigger issue is how
to engage my management and

875
00:45:33,933 --> 00:45:34,900
the executives.

876
00:45:34,966 --> 00:45:39,533
And I wonder how you might
tailor your message to be able

877
00:45:39,599 --> 00:45:43,533
to deliver it to people who are
in a decision-making position.

878
00:45:43,599 --> 00:45:47,699
>> DR. JESSICA BARKER: Sure. I get that question a lot, and I

879
00:45:47,766 --> 00:45:49,900
also do a lot of
work with executives.

880
00:45:49,966 --> 00:45:56,266
I do find that social proof is
really impactful for executives.

881
00:45:56,333 --> 00:45:59,633
So, one of the most common
questions that I get from boards

882
00:45:59,699 --> 00:46:02,966
is how do we compare
to other organizations?

883
00:46:02,966 --> 00:46:07,933
So, if you can make it relevant
to them by showing what other

884
00:46:08,000 --> 00:46:11,000
organizations are doing, how
other organizations have been

885
00:46:11,066 --> 00:46:13,933
impacted by the threat, you
know, any organization that they

886
00:46:14,000 --> 00:46:17,900
relate to that's in the same
sector that they compete with,

887
00:46:17,966 --> 00:46:19,633
then fantastic.

888
00:46:20,300 --> 00:46:25,300
I also find that efficacy with
executives is really important.

889
00:46:25,366 --> 00:46:27,866
They don't want to
hear just about problems.

890
00:46:27,933 --> 00:46:32,199
They want to know very quickly
what can we do about it.

891
00:46:32,266 --> 00:46:36,300
So, always kind of going in with
a plan of what you want from

892
00:46:36,366 --> 00:46:37,633
them, what they can do.

893
00:46:38,233 --> 00:46:40,900
And the other thing that I
have found very effective with

894
00:46:40,966 --> 00:46:45,333
executives is, again, kind of
goes back to social proof.

895
00:46:45,400 --> 00:46:47,933
Sometimes for example, if you
have got to present to the board

896
00:46:48,000 --> 00:46:51,266
and you have got fifteen
minutes, half an hour maybe,

897
00:46:51,333 --> 00:46:54,800
it's very hard to
persuade them in that time.

898
00:46:54,866 --> 00:46:58,566
So, you need to do like a
pincer movement in the run up to

899
00:46:58,633 --> 00:46:59,665
that meeting.

900
00:46:59,733 --> 00:47:02,800
And you basically want to get
people who are around that table

901
00:47:02,866 --> 00:47:04,500
on your side before you go in.

902
00:47:05,099 --> 00:47:09,233
So, whether that's trying to
directly grab one of them for

903
00:47:09,300 --> 00:47:12,333
coffee in the weeks beforehand,
someone who has been a bit

904
00:47:12,400 --> 00:47:15,566
sympathetic to the cause before,
trying to get them to be a

905
00:47:15,633 --> 00:47:18,232
champion for you in the
room is going to be the most

906
00:47:18,300 --> 00:47:19,333
effective way.

907
00:47:19,400 --> 00:47:21,966
Or maybe you have to - you know,
it depends on the organization

908
00:47:22,033 --> 00:47:25,966
and the hierarchy, but maybe you
have to go up through the levels

909
00:47:26,033 --> 00:47:29,732
to get to that individual or
those couple of individuals who,

910
00:47:29,800 --> 00:47:34,400
when you are voicing your
concerns and what you want,

911
00:47:34,466 --> 00:47:37,232
they're going to be sat around
the table influencing their

912
00:47:37,300 --> 00:47:39,400
peers, saying you know what?

913
00:47:39,466 --> 00:47:40,098
He's right.

914
00:47:40,166 --> 00:47:41,066
We need to do this.

915
00:47:41,133 --> 00:47:42,533
I'm fully on board with it.

916
00:47:42,599 --> 00:47:44,599
And then it provides that
social proof for the other

917
00:47:44,666 --> 00:47:46,466
board members.

918
00:47:46,533 --> 00:47:47,232
Hope that helps.

919
00:47:49,033 --> 00:47:50,098
>> AUDIENCE: Yes. Hello.

920
00:47:50,166 --> 00:47:53,099
There's a point where I look at
cyber at the beginning was the

921
00:47:53,166 --> 00:47:56,099
objective was to create the fear
to get the people to understand

922
00:47:56,166 --> 00:47:57,433
there was a problem.

923
00:47:57,500 --> 00:47:58,766
There's a crossover point.

924
00:47:58,766 --> 00:48:01,633
We know that fear is false
evidence appearing real, so we

925
00:48:01,699 --> 00:48:06,133
have a situation where we're now
past the point of let's say we

926
00:48:06,199 --> 00:48:09,866
understand that the ones who
make big decisions understand

927
00:48:09,933 --> 00:48:11,366
this is a problem.

928
00:48:11,433 --> 00:48:13,866
You're saying
we're at that point.

929
00:48:13,933 --> 00:48:16,599
These are the techniques
you use beyond that.

930
00:48:16,666 --> 00:48:19,366
But when do you - what
would you do before that?

931
00:48:19,433 --> 00:48:23,000
Let's say, look at the
climate change symbol at that

932
00:48:23,066 --> 00:48:23,933
same point.

933
00:48:24,000 --> 00:48:26,000
So, I'm just trying to
understand a little bit how

934
00:48:26,066 --> 00:48:28,066
would you get before that
because there's still pockets

935
00:48:28,133 --> 00:48:29,966
where people
don't understand that.

936
00:48:30,033 --> 00:48:33,000
We had other discussions this
morning where millennials are

937
00:48:33,066 --> 00:48:37,399
actually more susceptible to
being attacked than, let's say,

938
00:48:37,466 --> 00:48:39,199
other folks who have
been at it a lot longer.

939
00:48:39,266 --> 00:48:40,566
So, I'm curious
to hear your point.

940
00:48:40,633 --> 00:48:42,566
>> DR. JESSICA BARKER: Sure. So, what you're saying is are there

941
00:48:42,633 --> 00:48:45,133
some groups of people where
this approach doesn't work and

942
00:48:45,199 --> 00:48:47,199
we need to kind of
take it back a level.

943
00:48:47,266 --> 00:48:49,233
I don't really think so.

944
00:48:49,300 --> 00:48:53,166
I think the research shows,
and I have found, it's never

945
00:48:53,233 --> 00:48:55,366
successful to go
heavy on the fear.

946
00:48:55,433 --> 00:48:57,599
If you think back to what we
have been doing over the last

947
00:48:57,666 --> 00:49:00,333
few decades, I think the
proof is in the pudding.

948
00:49:00,400 --> 00:49:04,599
We haven't been successful
in just hammering the fear.

949
00:49:05,066 --> 00:49:08,433
So, I would say however,
whatever level of awareness a

950
00:49:08,500 --> 00:49:11,766
group has, of course you need
to get them to understand the

951
00:49:11,833 --> 00:49:14,900
threat and that it's susceptible
to them, but then you need them

952
00:49:14,966 --> 00:49:15,900
to feel empowered.

953
00:49:15,966 --> 00:49:19,266
Otherwise, they just won't
switch on to the message.

954
00:49:22,366 --> 00:49:24,533
>> AUDIENCE: So, you talked
about moving away from a culture

955
00:49:24,599 --> 00:49:28,233
of blaming the end user, having
more kind of an open culture.

956
00:49:28,300 --> 00:49:31,866
What I'm struggling with is
where do you draw the line

957
00:49:31,933 --> 00:49:36,599
between consequence management
and having that open culture to

958
00:49:36,666 --> 00:49:40,099
make sure you have some
governance within place.

959
00:49:40,166 --> 00:49:42,933
So, you distinguish between
someone who is intentionally

960
00:49:43,000 --> 00:49:46,699
doing something or someone who
makes a mistake or something of

961
00:49:46,766 --> 00:49:47,766
that nature.

962
00:49:47,833 --> 00:49:49,366
>> DR. JESSICA BARKER: Of
course, it has to be balanced.

963
00:49:49,433 --> 00:49:53,099
One thing I do find though is an
empowering and positive security

964
00:49:53,166 --> 00:49:56,766
culture tends to be more
beneficial in that kind of a

965
00:49:56,833 --> 00:50:01,733
malicious insider threat
as well, partly because if

966
00:50:01,800 --> 00:50:05,500
everybody is feeling engaged in
security, and if everyone is

967
00:50:05,566 --> 00:50:08,765
practicing more positive
behaviors, those that willfully

968
00:50:08,833 --> 00:50:11,266
are not stand out so much more.

969
00:50:11,699 --> 00:50:14,633
So, it actually then - and
people then care about that and

970
00:50:14,699 --> 00:50:19,199
they're more likely to say,
you know, hey, Jess is being a

971
00:50:19,266 --> 00:50:21,333
little bit strange with
how much she's downloading.

972
00:50:21,400 --> 00:50:23,900
Like, I don't know, maybe
you want to look at that.

973
00:50:23,900 --> 00:50:26,400
So, the more positive culture
I think actually has a more

974
00:50:26,466 --> 00:50:28,366
positive impact on
malicious insiders.

975
00:50:28,433 --> 00:50:32,500
As I said earlier, of course,
a positive approach has to be

976
00:50:32,566 --> 00:50:36,299
balanced with some consequences
for people who are willfully

977
00:50:36,366 --> 00:50:38,766
negligent or malicious.

978
00:50:38,833 --> 00:50:44,333
But we can't keep treating
everyone who makes a mistake as

979
00:50:44,400 --> 00:50:45,533
if they should be punished.

980
00:50:46,933 --> 00:50:47,699
Thank you.

981
00:50:47,766 --> 00:50:48,800
And that is time.

982
00:50:48,866 --> 00:50:50,433
So, thank you all so
much for your attention.

983
00:50:50,500 --> 00:50:51,633
Thank you for your questions.

984
00:50:51,699 --> 00:50:53,099
I really appreciate it.

985
00:50:53,166 --> 00:50:53,500
Thank you.

