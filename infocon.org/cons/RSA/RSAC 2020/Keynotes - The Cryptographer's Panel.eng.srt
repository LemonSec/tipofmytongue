1
00:00:06,966 --> 00:00:10,266
>> ANNOUNCER: Please welcome
Chief Technology Officer RSA,

2
00:00:10,333 --> 00:00:11,500
Zulkifar Ramzan.

3
00:00:21,333 --> 00:00:22,933
>> ZULKIFAR RAMZAN: As reflected
in this morning's opening

4
00:00:23,000 --> 00:00:27,899
keynote, the central theme
of RSA Conference 2020 is the

5
00:00:27,966 --> 00:00:30,199
Human Element.

6
00:00:30,266 --> 00:00:33,233
Technologies become such an
inextricable part of our lives

7
00:00:33,299 --> 00:00:37,733
that we rarely pause for a
moment and consider that every

8
00:00:37,799 --> 00:00:42,866
technology, every innovation
began its life as an idea in the

9
00:00:42,933 --> 00:00:43,966
mind of an individual.

10
00:00:45,600 --> 00:00:49,266
I am therefore humbled and
honored to welcome to the stage

11
00:00:49,333 --> 00:00:54,033
five individuals whose
contributions to our industry

12
00:00:54,100 --> 00:00:55,799
have been unparalleled.

13
00:00:56,766 --> 00:01:01,199
Please join me in welcoming
Ron Rivest, the R in RSA.

14
00:01:03,166 --> 00:01:06,265
Adi Shamir, the S in RSA.

15
00:01:07,799 --> 00:01:14,500
Whit Diffie, Tal Rabin,
and Arvind Narayanan.

16
00:01:25,266 --> 00:01:28,099
So, Adi, I'd like to begin with
you since it's a big long since

17
00:01:28,166 --> 00:01:29,633
we've last heard
from you on this panel.

18
00:01:29,633 --> 00:01:33,166
The one theme from this
conference is the Human Element.

19
00:01:33,233 --> 00:01:36,366
As we know, technologists today
are trying to think about

20
00:01:36,433 --> 00:01:39,299
replacing the human element in
many systems with AI machine

21
00:01:39,366 --> 00:01:41,066
learning capabilities.

22
00:01:41,133 --> 00:01:43,833
I want to get your perspective
to start with on AI machine

23
00:01:43,900 --> 00:01:46,133
learning and the specifically
the security implications of

24
00:01:46,200 --> 00:01:46,966
these technologies.

25
00:01:48,233 --> 00:01:49,366
>> ADI SHAMIR: Thanks.

26
00:01:49,433 --> 00:01:52,400
I'd like to start with
a short personal note.

27
00:01:52,466 --> 00:01:57,532
As most of you know, it took the
administration six months last

28
00:01:57,599 --> 00:02:01,700
year to decide whether I should
be allowed into the US or not.

29
00:02:01,766 --> 00:02:02,833
Finally, I got a visa.

30
00:02:02,900 --> 00:02:07,933
And I really believe that since
every other country in the world

31
00:02:08,000 --> 00:02:12,699
makes such decisions within
three to five days, I think that

32
00:02:12,766 --> 00:02:16,766
whoever is in charge of the visa
application processing should be

33
00:02:16,833 --> 00:02:18,300
replaced by a
different neural network.

34
00:02:21,933 --> 00:02:25,198
Coming to think about it, I
think that you need only a

35
00:02:25,266 --> 00:02:28,198
shallow network to
distinguish between me and an

36
00:02:28,266 --> 00:02:29,166
Al-Qaeda terrorist.

37
00:02:31,933 --> 00:02:36,632
Anyway, what are the two
major problems in artificial

38
00:02:36,699 --> 00:02:39,300
intelligence in my opinion?

39
00:02:39,366 --> 00:02:42,500
One is that we don't understand
why they're working so well.

40
00:02:42,566 --> 00:02:46,699
And the second is we don't
understand why they are working

41
00:02:46,766 --> 00:02:47,533
so terribly.

42
00:02:50,900 --> 00:02:57,000
The main question which is
related to security is the issue

43
00:02:57,066 --> 00:02:59,500
of adversarial examples.

44
00:02:59,566 --> 00:03:02,400
As all of you have probably
seen, it is enough to take an

45
00:03:02,466 --> 00:03:08,166
image and change a few pixels
and then the neural network is

46
00:03:08,233 --> 00:03:11,366
making all kinds of
very strange decisions.

47
00:03:11,433 --> 00:03:15,033
I think that we are now starting
to understand what is going on,

48
00:03:15,099 --> 00:03:19,965
but until we will solve this
problem, I think that it will be

49
00:03:20,033 --> 00:03:25,599
very dangerous to use the neural
networks in autonomous vehicles,

50
00:03:25,666 --> 00:03:30,366
for example, in making life and
death decisions in medicine,

51
00:03:30,433 --> 00:03:31,632
et cetera.

52
00:03:31,699 --> 00:03:34,533
So, I believe that we
are making good progress.

53
00:03:34,599 --> 00:03:39,198
Machine learning has made
tremendous advances over the

54
00:03:39,266 --> 00:03:41,632
last ten years, but
we are not there yet.

55
00:03:41,699 --> 00:03:43,199
There are some
sticking problems.

56
00:03:44,900 --> 00:03:46,333
>> ZULKIFAR RAMZAN: And we've
seen recently the facial

57
00:03:46,400 --> 00:03:48,500
recognition ban I think is one
interesting example of the

58
00:03:48,566 --> 00:03:49,433
ethical implications of AI.

59
00:03:49,500 --> 00:03:51,566
Does the panel have any
comments on that area?

60
00:03:51,566 --> 00:03:53,033
>> RON RIVEST: Maybe
I can comment on that.

61
00:03:53,099 --> 00:03:56,033
Yeah, facial recognition is one
of the places where rubber meets

62
00:03:56,099 --> 00:03:59,099
the road, where we start seeing
applications of AI to our

63
00:03:59,166 --> 00:04:00,133
everyday experience.

64
00:04:00,199 --> 00:04:03,666
It's happening so fast and so
pervasively that we really

65
00:04:03,733 --> 00:04:06,900
should be concerned about what
are the rules of the road for

66
00:04:06,966 --> 00:04:07,733
this technology.

67
00:04:08,599 --> 00:04:11,766
It seems there's three -
or five properties that are

68
00:04:11,833 --> 00:04:12,266
relevant here.

69
00:04:12,333 --> 00:04:13,366
One is asymmetry.

70
00:04:13,433 --> 00:04:15,599
It used to be when somebody was
looking at you, you knew they

71
00:04:15,666 --> 00:04:17,666
were looking at you and
you could look at them.

72
00:04:17,733 --> 00:04:19,466
We don't have that anymore
with cameras everywhere in

73
00:04:19,533 --> 00:04:20,199
hidden forms.

74
00:04:21,366 --> 00:04:22,032
The other is permanence.

75
00:04:22,100 --> 00:04:23,899
What's being seen
is being recorded.

76
00:04:23,966 --> 00:04:26,766
That's a huge change
in the situation.

77
00:04:27,266 --> 00:04:28,733
There's three properties
that Bruce Schneider - and a

78
00:04:28,800 --> 00:04:31,033
wonderful op-ed he had recently
in the New York Times mentioned

79
00:04:31,100 --> 00:04:33,933
as well, which is really
where the AI part comes in.

80
00:04:33,933 --> 00:04:35,966
So, one of these
is identification.

81
00:04:36,033 --> 00:04:38,633
So, the systems are being
used to identify you as

82
00:04:38,699 --> 00:04:40,733
an individual.

83
00:04:40,800 --> 00:04:43,100
The Chinese are masters
of this at the moment.

84
00:04:43,166 --> 00:04:45,899
Of their billions of people,
they can identify who you

85
00:04:45,966 --> 00:04:47,199
are quickly.

86
00:04:47,266 --> 00:04:48,100
So, identification.

87
00:04:48,166 --> 00:04:49,166
Correlation.

88
00:04:49,233 --> 00:04:51,733
Correlating what is seen today
with what was seen yesterday

89
00:04:51,800 --> 00:04:52,699
with your other activities.

90
00:04:52,966 --> 00:04:54,699
And finally, discrimination.

91
00:04:54,766 --> 00:04:57,066
Using this information to
discriminate what ads you see,

92
00:04:57,133 --> 00:04:59,299
what services you
can get and so on too.

93
00:04:59,666 --> 00:05:02,733
So, identification, correlation
and discrimination are all

94
00:05:02,800 --> 00:05:05,466
implemented by AI and need,
in my opinion, to be

95
00:05:05,533 --> 00:05:06,966
carefully regulated.

96
00:05:07,033 --> 00:05:09,800
So, I think the bans today
of facial recognition are

97
00:05:09,866 --> 00:05:11,832
appropriate until we
understand better how to

98
00:05:11,899 --> 00:05:12,933
regulate this technology.

99
00:05:13,833 --> 00:05:16,133
>> ADI SHAMIR: I just want to
mention that in China they have

100
00:05:16,199 --> 00:05:18,833
a major failure now of the
facial recognition system

101
00:05:18,899 --> 00:05:21,233
because everyone is
walking with face masks.

102
00:05:21,300 --> 00:05:27,033
So, they started using face
recognition in order to pay.

103
00:05:27,100 --> 00:05:30,399
When you stay at the checkout
counter with the long line

104
00:05:30,466 --> 00:05:34,033
behind you, you don't want to
take off your face mask in order

105
00:05:34,100 --> 00:05:35,266
to actually pay.

106
00:05:35,933 --> 00:05:38,633
>> TAL RABIN: And they have
also had failures before that.

107
00:05:38,699 --> 00:05:43,433
There was a famous woman whose
picture was on the side of a bus

108
00:05:43,500 --> 00:05:47,233
and they used this technology
to try and stop people from

109
00:05:47,300 --> 00:05:50,899
jaywalking and they shamed them
by putting their face, then, on

110
00:05:50,966 --> 00:05:53,966
a bulletin board of
terrible jaywalkers.

111
00:05:54,033 --> 00:05:58,399
And this woman whose face passed
on the side - on the crosswalk

112
00:05:58,466 --> 00:06:02,933
on the bus at the time when the
red was light - the light was

113
00:06:03,000 --> 00:06:06,699
red was posted up on
that board of shame.

114
00:06:06,766 --> 00:06:08,966
So, there are also
failures in that respect.

115
00:06:09,566 --> 00:06:13,899
But I want to say one
interesting research that has

116
00:06:13,966 --> 00:06:18,633
come out also shows some of
these issues that relate to the

117
00:06:18,699 --> 00:06:22,266
learning algorithms and how we
still have a ways ahead of us

118
00:06:22,333 --> 00:06:24,733
when it comes to
security issues.

119
00:06:25,233 --> 00:06:29,766
There's some researchers that
showed that if you train a

120
00:06:29,833 --> 00:06:33,899
system to learn something, for
example, they were trying to

121
00:06:33,966 --> 00:06:38,800
teach the system to learn facial
expressions, but somehow the

122
00:06:38,866 --> 00:06:42,599
system also learned, and they
called it over learning, it

123
00:06:42,666 --> 00:06:47,066
learned about the race of the
individuals, even races that

124
00:06:47,133 --> 00:06:51,299
were not included in the
database which they trained on.

125
00:06:51,600 --> 00:06:54,800
So clearly, we have to
understand when we run such a

126
00:06:54,866 --> 00:06:59,966
system, what are we learning
about the data beyond what we

127
00:07:00,033 --> 00:07:03,133
really want to do and all the
privacy implications that are

128
00:07:03,199 --> 00:07:04,466
involved with that.

129
00:07:04,533 --> 00:07:07,133
So, I think that there's
a good side to that.

130
00:07:07,199 --> 00:07:10,600
It means that maybe not all
problems have been solved and

131
00:07:10,666 --> 00:07:14,000
that we as the security and
privacy community still have a

132
00:07:14,066 --> 00:07:19,032
lot to contribute to the
machine learning community.

133
00:07:19,066 --> 00:07:20,899
>> WHITFIELD DIFFIE: Talking
about putting up faces to

134
00:07:20,966 --> 00:07:22,100
shame people.

135
00:07:22,166 --> 00:07:26,199
Our founder of this conference,
Jim Bidzos, had proposed that

136
00:07:26,266 --> 00:07:31,000
the pictures of deadbeat dads
ought to appear on beer bottles

137
00:07:31,066 --> 00:07:32,732
so they have trouble
going out drinking.

138
00:07:35,300 --> 00:07:38,000
>> ZULKIFAR RAMZAN: As you think
about this sort of large-scale

139
00:07:38,066 --> 00:07:40,433
mass collection of data, the
topics of fairness and privacy

140
00:07:40,500 --> 00:07:42,033
certainly come up frequently.

141
00:07:42,100 --> 00:07:43,866
I know Arvind, you spent some
time thinking about this area.

142
00:07:43,933 --> 00:07:45,233
I'd love to get your
perspective and then the

143
00:07:45,300 --> 00:07:46,699
panelists'
perspective afterwards.

144
00:07:46,766 --> 00:07:47,733
>> ARVIND NARYANAN: Sure.

145
00:07:47,800 --> 00:07:50,199
Privacy is a
very overloaded term.

146
00:07:50,266 --> 00:07:52,300
In the technical community,
we usually mean something

147
00:07:52,366 --> 00:07:53,633
like confidentiality.

148
00:07:53,699 --> 00:07:56,600
But in general, when people say
their privacy is being violated,

149
00:07:56,666 --> 00:07:59,266
depending on context, they mean
a number of different things

150
00:07:59,333 --> 00:08:00,066
including fairness.

151
00:08:00,133 --> 00:08:01,966
We've been talking
about facial recognition.

152
00:08:02,033 --> 00:08:03,666
That is a good example.

153
00:08:03,733 --> 00:08:07,300
Why are privacy advocates up in
arms about facial recognition in

154
00:08:07,366 --> 00:08:08,532
public spaces?

155
00:08:08,600 --> 00:08:11,133
I mean, my face is the
most public thing about me.

156
00:08:11,199 --> 00:08:14,133
And when I'm in public, I don't
have an expectation of privacy.

157
00:08:14,199 --> 00:08:15,399
So, what are we worried about?

158
00:08:15,566 --> 00:08:18,366
Well, it's exactly the kind of
things we've been talking about.

159
00:08:18,433 --> 00:08:21,533
Because of inevitable
mistakes in facial recognition

160
00:08:21,600 --> 00:08:24,433
algorithms, the wrong person
being arrested and things

161
00:08:24,500 --> 00:08:25,300
like that.

162
00:08:25,366 --> 00:08:27,733
So those are issues of
discrimination, fairness,

163
00:08:27,800 --> 00:08:29,399
justice and so on. Right?

164
00:08:29,399 --> 00:08:30,833
So, this is more common
than you might think.

165
00:08:30,899 --> 00:08:32,732
It is the same thing
with genetic privacy.

166
00:08:32,799 --> 00:08:34,199
What are people worried about?

167
00:08:34,265 --> 00:08:36,765
People are worried that a
potential employer might use my

168
00:08:36,832 --> 00:08:40,732
genetic information to unjustly
deny me a job or my insurance

169
00:08:40,799 --> 00:08:43,566
company might set premiums
in a discriminatory way.

170
00:08:43,633 --> 00:08:45,700
Again, those are issues of
fairness and discrimination and,

171
00:08:45,766 --> 00:08:49,399
of course, law enforcement could
misuse my genetic information.

172
00:08:49,700 --> 00:08:52,633
I think the reason this is very
important for technologists to

173
00:08:52,700 --> 00:08:56,399
understand this distinction is
that there are a lot of very

174
00:08:56,466 --> 00:08:59,200
technically interesting
solutions that are things like

175
00:08:59,266 --> 00:09:03,366
oh, here is a way in which you
can use powerful cryptography in

176
00:09:03,433 --> 00:09:06,500
such a way that an insurance
company, for example, can query

177
00:09:06,566 --> 00:09:09,966
your doctor's database, not
learn any of the plain text

178
00:09:10,033 --> 00:09:14,466
information about your DNA but,
nonetheless, be able to compute

179
00:09:14,533 --> 00:09:15,799
what it is they want to compute.

180
00:09:15,866 --> 00:09:18,899
For example, it might be things
like how to set your insurance

181
00:09:18,966 --> 00:09:21,399
pricing and in a potentially
discriminatory way.

182
00:09:22,299 --> 00:09:23,366
Now, what have we done here?

183
00:09:23,433 --> 00:09:26,466
This is a very cool technology,
but it doesn't actually address

184
00:09:26,533 --> 00:09:27,733
the concern that people have.

185
00:09:27,799 --> 00:09:30,500
It actually enables the very
thing that they're worried about

186
00:09:30,566 --> 00:09:34,700
which is discriminatory pricing
using your genetic information.

187
00:09:34,700 --> 00:09:38,500
So, I wanted to just give that
note of caution when people say

188
00:09:38,566 --> 00:09:41,466
privacy, they often mean
fairness and so we should take

189
00:09:41,533 --> 00:09:44,299
that into account, and
cryptography is very powerful

190
00:09:44,366 --> 00:09:45,866
but only if we use
it in the right way.

191
00:09:47,666 --> 00:09:48,933
>> ZULKIFAR RAMZAN:
Tal, do you think fairness can

192
00:09:49,000 --> 00:09:50,133
be actually achieved?

193
00:09:51,233 --> 00:09:54,233
>> TAL RABIN: So, it's
an interesting question.

194
00:09:54,299 --> 00:09:56,933
For example, look at
the following thing.

195
00:09:57,000 --> 00:10:02,000
Let's say we want to find out
if a person would default on

196
00:10:02,066 --> 00:10:03,000
a loan.

197
00:10:03,066 --> 00:10:04,399
So, what would we say?

198
00:10:04,466 --> 00:10:07,766
Let's not use people
but let's say New Yorkers

199
00:10:07,833 --> 00:10:08,766
and Californians.

200
00:10:08,833 --> 00:10:13,899
And it turns out that New
Yorkers default on loans more

201
00:10:13,966 --> 00:10:16,000
often than Californians.

202
00:10:16,066 --> 00:10:20,799
So, should the number of a New
Yorker, any New Yorker, be lower

203
00:10:20,866 --> 00:10:22,733
because they default more?

204
00:10:22,799 --> 00:10:26,033
So, you would think that a
bank would say yes, I need that

205
00:10:26,100 --> 00:10:27,399
number to be lower.

206
00:10:27,466 --> 00:10:31,166
So, that in some way is
discriminatory because maybe I,

207
00:10:31,233 --> 00:10:36,233
who am a New Yorker, I am a very
upstanding loan payer and then I

208
00:10:36,299 --> 00:10:39,132
should have a high score
despite being a New Yorker.

209
00:10:39,133 --> 00:10:43,566
So, let's say that we equalize
it among people who do actually

210
00:10:43,633 --> 00:10:45,966
default and then
the number is okay.

211
00:10:46,033 --> 00:10:49,399
But now we look at the bank
person who wants to come and

212
00:10:49,466 --> 00:10:51,533
give a person a loan.

213
00:10:51,600 --> 00:10:55,166
And they look and they say,
okay, this number was equal for

214
00:10:55,233 --> 00:11:00,733
me and for Arvind, but in fact
she's from New York, so I know

215
00:11:00,799 --> 00:11:03,833
she's going to default more even
though their numbers were sort

216
00:11:03,899 --> 00:11:05,533
of equal.

217
00:11:05,600 --> 00:11:10,299
So, we have a tension here
of where we sort of equalize

218
00:11:10,366 --> 00:11:11,199
between things.

219
00:11:11,266 --> 00:11:14,165
Do we equalize among
fault/defaulters or do we

220
00:11:14,233 --> 00:11:17,333
equalize about New
Yorkers and Californians?

221
00:11:17,399 --> 00:11:18,899
So, there's an issue.

222
00:11:18,966 --> 00:11:23,066
And I think that when we
design these fairness protocols,

223
00:11:23,133 --> 00:11:25,333
we really have to think
what we mean by that.

224
00:11:25,666 --> 00:11:31,433
So, hopefully we can achieve
some things because sometimes

225
00:11:31,500 --> 00:11:34,200
it's great if
tools can aid people.

226
00:11:34,266 --> 00:11:38,299
For example, the judges use
tools to determine whether

227
00:11:38,366 --> 00:11:42,100
people would default
on their bail or not.

228
00:11:42,166 --> 00:11:43,333
And it helps.

229
00:11:43,399 --> 00:11:46,766
So, maybe we need to figure out
hybrid systems, but we also have

230
00:11:46,833 --> 00:11:51,100
to understand what property
of fairness the tools that we

231
00:11:51,166 --> 00:11:53,766
design are actually offering.

232
00:11:54,966 --> 00:11:56,399
>> ZULKIFAR RAMZAN: Fantastic.

233
00:11:56,399 --> 00:11:57,700
>> ARVIND NARYANAN: Sounds a
lot like security, actually.

234
00:11:57,766 --> 00:12:00,000
It's never going to be perfect,
but we still have to keep trying

235
00:12:00,066 --> 00:12:02,433
and do the best we can.

236
00:12:02,466 --> 00:12:03,733
>> ZULKIFAR RAMZAN: So, from
what I'm hearing, there's a lot

237
00:12:03,799 --> 00:12:05,665
of obviously technical
challenges for achieving both

238
00:12:05,733 --> 00:12:08,099
fairness and privacy as well.

239
00:12:08,100 --> 00:12:10,033
We've seen a lot of recent
work on the policy side.

240
00:12:10,100 --> 00:12:15,700
So, policies like GDPR and CCPA
and LGBD and others that are

241
00:12:15,766 --> 00:12:17,500
coming into the vogue.

242
00:12:17,566 --> 00:12:20,899
Do you think any of these
policies pose interesting

243
00:12:20,966 --> 00:12:21,899
technical challenges?

244
00:12:21,966 --> 00:12:23,733
Maybe I'll start with you,
Tal, and get the rest of the

245
00:12:23,799 --> 00:12:24,533
panel's perspective.

246
00:12:25,366 --> 00:12:27,165
>> TAL RABIN: So, I'll
talk about two things.

247
00:12:27,233 --> 00:12:31,799
First of all, it's about privacy
policies that are out there.

248
00:12:31,866 --> 00:12:35,866
I think that they're offering
now, our community a huge

249
00:12:35,933 --> 00:12:39,500
opportunity because clearly many
of the protocols that have been

250
00:12:39,566 --> 00:12:45,500
designed do not offer
appropriate privacy satisfactory

251
00:12:45,566 --> 00:12:48,033
conditions according
to these laws.

252
00:12:48,100 --> 00:12:53,266
So, this is 2020 so we can look
back twenty plus twenty to

253
00:12:53,333 --> 00:12:56,866
technologies that we started
developing in theory forty years

254
00:12:56,933 --> 00:12:59,799
ago of multi-party computations.

255
00:12:59,866 --> 00:13:02,799
And I think that these
technologies now can really have

256
00:13:02,866 --> 00:13:09,033
a huge impact on what we can
deliver and really enable us to

257
00:13:09,100 --> 00:13:12,133
do things that maybe otherwise
we would need to give up.

258
00:13:12,566 --> 00:13:17,100
Arvind gave it maybe as a
negative example of the doctor

259
00:13:17,166 --> 00:13:20,799
communicating with a hospital
and it's still exposing some

260
00:13:20,866 --> 00:13:24,500
information as it relates to
fairness, but it still offers

261
00:13:24,566 --> 00:13:28,000
privacy guarantees that we
cannot achieve otherwise.

262
00:13:28,066 --> 00:13:31,600
So, we need to use these
tools going forward.

263
00:13:31,600 --> 00:13:36,033
And specifically, about the
right to be forgotten which is

264
00:13:36,100 --> 00:13:41,700
one of these privacy things that
people are interested in, I want

265
00:13:41,766 --> 00:13:45,766
to say that here we
have a lot to examine.

266
00:13:45,833 --> 00:13:49,699
When we say the right to be
forgotten, what do we even mean?

267
00:13:49,766 --> 00:13:53,866
It was written in the law but
the definition of what it is, is

268
00:13:53,933 --> 00:13:55,466
not 100% clear.

269
00:13:55,466 --> 00:13:59,166
In fact, there's going to be a
paper in Eurocrypt which is one

270
00:13:59,233 --> 00:14:02,433
of our flagship conferences
where there's going to be a

271
00:14:02,500 --> 00:14:06,200
start of a discussion of what it
means, what do people actually

272
00:14:06,266 --> 00:14:10,132
want when they talk about the
right to be forgotten, and for

273
00:14:10,200 --> 00:14:14,033
us to try and start figuring out
the boundaries of what we can

274
00:14:14,100 --> 00:14:16,333
deliver and what
we cannot deliver.

275
00:14:16,399 --> 00:14:19,566
And here I'll just show you a
simple example of why there

276
00:14:19,633 --> 00:14:21,200
would be a complexity.

277
00:14:21,266 --> 00:14:27,299
So, let's say that Diffie posts
all kinds of information online.

278
00:14:27,366 --> 00:14:32,299
And then I go and I query that
information and I download some

279
00:14:32,366 --> 00:14:36,133
of Diffie's information from the
stuff that he has posted and I

280
00:14:36,200 --> 00:14:37,833
have it on my computer.

281
00:14:37,899 --> 00:14:41,466
Then I ask to be forgotten.

282
00:14:41,533 --> 00:14:44,833
I want the fact that I had
queried something about Diffie

283
00:14:44,899 --> 00:14:46,299
to be forgotten.

284
00:14:47,266 --> 00:14:53,299
Now Whit comes and asks for his
information to be forgotten.

285
00:14:53,366 --> 00:14:57,665
Now how does the search engine
that gave me the results of the

286
00:14:57,733 --> 00:15:02,399
search, how can they access if
they forgot my query, how can

287
00:15:02,466 --> 00:15:06,299
they satisfy Whit's
requirement to be forgotten?

288
00:15:06,366 --> 00:15:09,566
So, we see that there are
challenges here that we would

289
00:15:09,633 --> 00:15:10,633
need to address.

290
00:15:10,700 --> 00:15:14,133
We would need to figure out
what it means, how it will be

291
00:15:14,200 --> 00:15:17,700
satisfied, and so on and so
forth and whether there are

292
00:15:17,766 --> 00:15:21,033
conflicting requirements
which can never be satisfied.

293
00:15:21,733 --> 00:15:24,866
>> ADI SHAMIR: So, besides the
technical issues, I think the

294
00:15:24,933 --> 00:15:27,899
right to be forgotten
simply wouldn't work.

295
00:15:27,966 --> 00:15:31,166
The main thing it achieves is to
attract attention to the fact

296
00:15:31,233 --> 00:15:33,665
that someone had
asked to be forgotten.

297
00:15:33,733 --> 00:15:38,899
Now, several well-known cases,
one in Spain, a guy wanted to

298
00:15:38,966 --> 00:15:44,366
have his criminal record erased
and so many newspaper articles

299
00:15:44,433 --> 00:15:47,500
were written about that
particular guy who was making

300
00:15:47,566 --> 00:15:51,333
this particular request that
I better not start it in the

301
00:15:51,399 --> 00:15:53,333
first place.

302
00:15:53,399 --> 00:15:56,166
On top of it, there are all
kinds of services such as the

303
00:15:56,233 --> 00:15:59,532
Internet Archive which
enable to you roll back to a

304
00:15:59,600 --> 00:16:01,733
previous state.

305
00:16:01,799 --> 00:16:03,199
What are they
going to do with it?

306
00:16:03,266 --> 00:16:07,233
Are you going to eliminate it
also from such services whose

307
00:16:07,299 --> 00:16:12,366
main purpose is to show you what
was the state of information at

308
00:16:12,433 --> 00:16:14,100
some previous point in time?

309
00:16:14,100 --> 00:16:19,266
And if you look for, as a simple
example, what happens when a

310
00:16:19,333 --> 00:16:23,500
world document leaks out about
some government action or

311
00:16:23,566 --> 00:16:28,299
whatever, the first thing the
journalist usually do is to try

312
00:16:28,366 --> 00:16:33,933
to look at all the changes
if the changes have not been

313
00:16:34,000 --> 00:16:37,500
eliminated from the document to
see who wanted to make which

314
00:16:37,566 --> 00:16:39,233
changes to the
policy of the document.

315
00:16:39,299 --> 00:16:43,866
So, I personally don't believe
that the right to be forgotten

316
00:16:43,933 --> 00:16:45,066
will be successful.

317
00:16:46,766 --> 00:16:48,366
>> ARVIND NARYANAN: So, Adi, I
completely agree with you that

318
00:16:48,433 --> 00:16:50,633
the right to be forgotten
doesn't actually give anyone a

319
00:16:50,700 --> 00:16:51,433
right to be forgotten.

320
00:16:51,500 --> 00:16:52,666
It's kind of a silly name.

321
00:16:53,533 --> 00:16:57,033
But you know, what does
the law actually try to do?

322
00:16:57,100 --> 00:16:59,100
So, let's think about it from
the perspective of somebody that

323
00:16:59,166 --> 00:17:01,266
the law is trying to protect.

324
00:17:01,333 --> 00:17:03,266
Suppose you have
a criminal record.

325
00:17:03,333 --> 00:17:07,399
You've served your time and now
you are applying for a job.

326
00:17:07,465 --> 00:17:10,199
The first thing the employer
is going to do, we know, is to

327
00:17:10,266 --> 00:17:10,833
Google you.

328
00:17:11,598 --> 00:17:14,799
So, do you want the first
impression of you to the

329
00:17:14,866 --> 00:17:18,700
employer to be your criminal
record or would you want the

330
00:17:18,766 --> 00:17:22,233
opportunity to bring it up,
perhaps in an interview on your

331
00:17:22,299 --> 00:17:24,733
own terms with a proper
explanation and context.

332
00:17:25,165 --> 00:17:28,233
So, that's the right that the
right to be forgotten is trying

333
00:17:28,299 --> 00:17:29,000
to give people.

334
00:17:29,066 --> 00:17:32,000
Some people call it
the right to delist.

335
00:17:32,066 --> 00:17:33,133
I think that's a better term.

336
00:17:33,200 --> 00:17:37,566
It is a very specific right to
have certain search results

337
00:17:37,633 --> 00:17:39,700
removed from specific
search queries.

338
00:17:39,766 --> 00:17:41,433
It doesn't take down
the web page itself.

339
00:17:41,500 --> 00:17:43,166
And you are absolutely
right about that.

340
00:17:43,466 --> 00:17:46,666
So, going back to your example,
right, this guy in Spain who had

341
00:17:46,733 --> 00:17:49,200
the right to be forgotten
backfire against him.

342
00:17:49,266 --> 00:17:50,166
That was one case.

343
00:17:50,233 --> 00:17:52,366
But let's look at the
aggregate total of right to be

344
00:17:52,433 --> 00:17:53,099
forgotten cases.

345
00:17:53,599 --> 00:17:56,966
Until recently, we didn't even
quite know how many there were.

346
00:17:57,033 --> 00:18:01,065
Except a few months ago, Google
published a paper at the ACM CCS

347
00:18:01,133 --> 00:18:04,033
conference taking a look at the
right to be forgotten requests

348
00:18:04,099 --> 00:18:05,265
they've gotten over the years.

349
00:18:05,333 --> 00:18:10,200
It turns out they get 50,000 per
month, 50,000 per month from all

350
00:18:10,266 --> 00:18:12,900
countries combined
where they have this right.

351
00:18:12,966 --> 00:18:15,599
And that's a substantial number.

352
00:18:15,666 --> 00:18:18,500
It's not like Google is getting
overwhelmed with bogus requests.

353
00:18:18,566 --> 00:18:21,433
A substantial number of these
requests are actually granted

354
00:18:21,500 --> 00:18:23,766
according to the
provisions of the law.

355
00:18:24,066 --> 00:18:26,099
And really, the paper
goes into a lot of detail.

356
00:18:26,166 --> 00:18:28,666
What it shows is that the right
to be forgotten in aggregate,

357
00:18:28,733 --> 00:18:33,366
you know, ignoring some special
cases, is actually working quite

358
00:18:33,433 --> 00:18:37,900
well in enabling this rights to
delist certain search results

359
00:18:37,966 --> 00:18:42,099
for some people who want to have
rights, for example, connected

360
00:18:42,166 --> 00:18:44,500
to their criminal record
once they've already served

361
00:18:44,566 --> 00:18:45,233
their time.

362
00:18:45,533 --> 00:18:47,332
So, maybe a different
interpretation of the example

363
00:18:47,400 --> 00:18:51,066
that you gave is that this
person kind of took one for the

364
00:18:51,133 --> 00:18:55,400
team and his own right to be
forgotten case was splashed all

365
00:18:55,466 --> 00:18:58,899
over the papers, but because
of that he established a legal

366
00:18:58,966 --> 00:19:01,832
precedent which actually
helped those 50,000 other people

367
00:19:01,900 --> 00:19:02,799
per month.

368
00:19:02,866 --> 00:19:06,766
So that's maybe a more
optimistic way of looking at it.

369
00:19:06,799 --> 00:19:09,733
>> WHITFIELD DIFFIE: I'd like t
understand how the right to be

370
00:19:09,799 --> 00:19:12,633
forgotten can be anything other
than something that kind of

371
00:19:12,700 --> 00:19:14,666
keeps the little people in line.

372
00:19:14,733 --> 00:19:17,265
It's not a right to be
forgotten by the secret police.

373
00:19:17,333 --> 00:19:20,000
It's not going to be effective
towards anybody who can keep

374
00:19:20,066 --> 00:19:21,299
their own records.

375
00:19:21,366 --> 00:19:26,733
It just, you know, affects small
researchers, nosey busy bodies,

376
00:19:26,799 --> 00:19:31,633
employers, and we already have
laws against discrimination on

377
00:19:31,700 --> 00:19:35,733
the basis of things like race
and age that you can see clearly

378
00:19:35,799 --> 00:19:38,799
that affect the employers
despite the fact the employers

379
00:19:38,866 --> 00:19:39,833
know them.

380
00:19:40,000 --> 00:19:42,866
So, why don't we just say, you
know, that if you've served your

381
00:19:42,933 --> 00:19:46,700
time, then an employer is not
allowed to consider that in

382
00:19:46,766 --> 00:19:47,433
hiring you.

383
00:19:47,833 --> 00:19:49,433
>> ARVIND NARYANAN: Can I just
say one quick thing to that?

384
00:19:49,500 --> 00:19:50,766
So, we already have that, right?

385
00:19:50,833 --> 00:19:54,833
I mean, in the United States
there are millions of formerly

386
00:19:54,900 --> 00:19:57,666
incarcerated persons and
they can't find jobs.

387
00:19:57,733 --> 00:20:00,033
And it is a big
contributor to recidivism.

388
00:20:00,099 --> 00:20:02,099
So, the way I look at it, that
we can actually use something

389
00:20:02,166 --> 00:20:03,233
like this in the US.

390
00:20:03,666 --> 00:20:05,466
>> ADI SHAMIR: Let's
do a small experiment.

391
00:20:05,533 --> 00:20:08,366
Earlier I said some nasty things
about the people in charge of

392
00:20:08,433 --> 00:20:10,299
the visa application process.

393
00:20:10,366 --> 00:20:13,000
I ask for this to be forgotten.

394
00:20:13,066 --> 00:20:14,966
(Laughter)

395
00:20:16,666 --> 00:20:18,366
>> RON RIVEST: What
was that you said?

396
00:20:18,433 --> 00:20:19,466
>> TAL RABIN: I
have forgotten, Adi.

397
00:20:19,533 --> 00:20:20,899
I've done you the favor.

398
00:20:20,966 --> 00:20:22,366
But I want to say one thing.

399
00:20:22,433 --> 00:20:25,400
I think that maybe in the
context of the right to be

400
00:20:25,466 --> 00:20:28,866
forgotten, we can discuss
about it in various ways.

401
00:20:28,933 --> 00:20:32,933
But I think we do need
technologies to eliminate data

402
00:20:33,000 --> 00:20:36,099
from the internet because of
course there are things that we

403
00:20:36,166 --> 00:20:40,433
want as a society, not as the
individual, to have removed.

404
00:20:40,500 --> 00:20:45,133
For example, child pornography,
that would be something that

405
00:20:45,200 --> 00:20:46,700
would be very advantageous.

406
00:20:46,766 --> 00:20:49,400
Can you think about a child
whose videos have been put

407
00:20:49,466 --> 00:20:50,099
up there?

408
00:20:50,166 --> 00:20:51,700
Of course, we need to do it.

409
00:20:51,766 --> 00:20:54,599
And I think that these
technologies will never be

410
00:20:54,666 --> 00:20:58,099
perfect, but we do need
them for other purposes.

411
00:20:58,166 --> 00:21:01,765
So, I wouldn't just say because
we cannot satisfy the right to

412
00:21:01,833 --> 00:21:04,599
be forgotten or maybe because
we don't think it should

413
00:21:04,666 --> 00:21:05,666
be forgotten.

414
00:21:05,733 --> 00:21:08,799
I think we should work on these
types of technologies that

415
00:21:08,866 --> 00:21:12,599
enable deletion of
information in a mass way.

416
00:21:13,099 --> 00:21:14,366
>> ZULKIFAR RAMZAN: So, what's
interesting, we're seeing

417
00:21:14,433 --> 00:21:15,700
obviously the advent of
technologies that are doing

418
00:21:15,766 --> 00:21:16,366
the opposite.

419
00:21:16,433 --> 00:21:17,799
For example, blockchain.

420
00:21:17,866 --> 00:21:21,900
Adi, do you think blockchain
provides a major impediment to

421
00:21:21,966 --> 00:21:22,733
right to be forgotten?

422
00:21:23,966 --> 00:21:26,799
>> ADI SHAMIR: So, connecting
this to the previous issue,

423
00:21:26,866 --> 00:21:33,000
clearly blockchain is all about
making the past immutable.

424
00:21:33,066 --> 00:21:37,799
So, any legislation that will
require that people will be able

425
00:21:37,866 --> 00:21:43,700
to undo past actions is contrary
to the idea of a blockchain

426
00:21:43,766 --> 00:21:47,599
where after a certain number of
blocks have been accumulated in

427
00:21:47,666 --> 00:21:49,832
the blockchain, there is
no way to touch the past.

428
00:21:50,266 --> 00:21:54,299
So, all blockchains from now
on are being banned, becoming

429
00:21:54,366 --> 00:21:55,233
illegal, right.

430
00:21:55,299 --> 00:22:01,700
So, on top of it, I have
major reservations about the

431
00:22:01,766 --> 00:22:05,333
blockchain technology, not
because it doesn't work but, in

432
00:22:05,400 --> 00:22:10,799
most cases, it is over hyped and
there are much simpler ways how

433
00:22:10,866 --> 00:22:12,066
to achieve the same goal.

434
00:22:13,133 --> 00:22:18,666
There are few cases which I am
aware of where it makes sense

435
00:22:18,733 --> 00:22:24,299
and it does give considerable
benefit, but most of the use

436
00:22:24,366 --> 00:22:28,233
cases that have been proposed
are just nonsense in my opinion.

437
00:22:29,533 --> 00:22:30,500
>> ZULKIFAR RAMZAN: Now,
actually one use case we've

438
00:22:30,566 --> 00:22:33,066
heard about recently for
blockchain is the application of

439
00:22:33,133 --> 00:22:34,366
blockchain to voting.

440
00:22:34,433 --> 00:22:36,566
Ron, I know it is a topic
you think quite a bit about.

441
00:22:36,633 --> 00:22:37,633
What are your thoughts?

442
00:22:37,700 --> 00:22:38,866
>> RON RIVEST: I think
a lot about voting. Yes.

443
00:22:38,933 --> 00:22:40,066
Thank you.

444
00:22:40,133 --> 00:22:44,566
Blockchain is the wrong
security technology for voting.

445
00:22:44,633 --> 00:22:47,299
I like to think of it as
bringing a combination lock to a

446
00:22:47,366 --> 00:22:49,466
kitchen fire or something
like that, you know.

447
00:22:51,000 --> 00:22:53,799
It's good on its own for
certain things but it's not good

448
00:22:53,866 --> 00:22:54,433
for voting.

449
00:22:54,500 --> 00:22:56,533
Voting is an
interesting problem.

450
00:22:56,599 --> 00:23:00,500
It has requirements that are in
many ways stronger than we see

451
00:23:00,566 --> 00:23:02,133
in many other
security applications.

452
00:23:02,200 --> 00:23:06,266
The need for anonymous ballots
and secret ballots, you know,

453
00:23:06,333 --> 00:23:09,066
sort of pervades the space and
makes it really tough to do

454
00:23:09,133 --> 00:23:10,200
audits and other things.

455
00:23:11,633 --> 00:23:15,200
So, blockchain technology
doesn't really fit and it

456
00:23:15,266 --> 00:23:16,533
doesn't fit for a
couple of reasons.

457
00:23:16,599 --> 00:23:19,666
One is that we've learned that
we need software independence.

458
00:23:19,733 --> 00:23:22,399
Many things we do in society,
like flying an airplane or

459
00:23:22,466 --> 00:23:23,832
something, you need high tech.

460
00:23:23,900 --> 00:23:26,933
Voting is a place where you
actually don't need high tech to

461
00:23:27,000 --> 00:23:28,000
make it work.

462
00:23:28,066 --> 00:23:32,033
You can get by just fine with
paper ballots and if you can

463
00:23:32,099 --> 00:23:35,000
keep that as your foundation and
if you do use some technology,

464
00:23:35,066 --> 00:23:37,766
use the paper ballots to check
on it, you can do very well.

465
00:23:38,099 --> 00:23:40,233
So, we call this software
independence so you don't need

466
00:23:40,299 --> 00:23:43,166
to trust the results because
you trust some software.

467
00:23:43,233 --> 00:23:45,466
That's a dangerous path to go
down if you don't need to go

468
00:23:45,533 --> 00:23:46,433
down that path.

469
00:23:46,500 --> 00:23:47,966
And with voting, we
actually don't need to.

470
00:23:48,866 --> 00:23:50,833
So, blockchain
provides certain things.

471
00:23:50,900 --> 00:23:53,833
It's sort of I like to
think of as garbage in, garbage

472
00:23:53,900 --> 00:23:56,166
stored forever.

473
00:23:56,233 --> 00:23:57,866
Once they've had a chance to
manipulate your vote, it goes on

474
00:23:57,933 --> 00:24:00,266
the blockchain and never
gets changed again.

475
00:24:00,333 --> 00:24:02,233
It's just not the
right tech for this.

476
00:24:02,299 --> 00:24:05,099
And so, I think blockchains
are a mismatch for voting.

477
00:24:05,166 --> 00:24:07,700
Blockchains may have other
wonderful places to apply and

478
00:24:07,766 --> 00:24:11,266
I'm sure Tal can speak to some
of the places where it fits well

479
00:24:11,333 --> 00:24:13,700
and Adi has looked at it too,
but voting is not one of those.

480
00:24:14,733 --> 00:24:16,166
>> ZULKIFAR RAMZAN:
Obviously, we are in 2020.

481
00:24:16,233 --> 00:24:18,832
It's a major
election year in the US.

482
00:24:18,900 --> 00:24:22,200
Any thoughts on the security of
the election process coming in

483
00:24:22,266 --> 00:24:24,299
this year given this
stakes that are involved?

484
00:24:25,900 --> 00:24:27,033
>> RON RIVEST: So, maybe
I can speak to that again.

485
00:24:27,099 --> 00:24:30,733
I think we've learned some
lessons as we've come down the

486
00:24:30,799 --> 00:24:33,700
path to where we are today,
particularly in the 2000s.

487
00:24:33,766 --> 00:24:37,266
One of the things we have
learned is what I was talking

488
00:24:37,333 --> 00:24:39,633
about before, was the
importance of paper ballots.

489
00:24:40,466 --> 00:24:43,966
We see that putting a foundation
of trust on electronic

490
00:24:44,033 --> 00:24:47,065
components that are hackable is
just not the right way to go.

491
00:24:47,133 --> 00:24:49,900
And having a paper ballot for
every voter - a voter verified

492
00:24:49,966 --> 00:24:51,933
paper ballot is the way to go.

493
00:24:52,000 --> 00:24:52,833
So, we see that.

494
00:24:52,900 --> 00:24:55,733
So, that's one of my two
top recommendations, to use

495
00:24:55,799 --> 00:24:56,700
paper ballots.

496
00:24:56,766 --> 00:24:59,099
The other is to
check the paper ballots.

497
00:24:59,166 --> 00:25:02,200
And we have technology for doing
that now with risk limiting

498
00:25:02,266 --> 00:25:04,833
audits developed by Philip Stark
at Berkeley and many others.

499
00:25:05,500 --> 00:25:08,566
So, you take a random sample of
the ballots and you can check to

500
00:25:08,633 --> 00:25:10,766
see if it's consistent
with the reported outcomes.

501
00:25:10,833 --> 00:25:16,200
So, these are two of my
top things for security

502
00:25:16,266 --> 00:25:17,200
of elections.

503
00:25:17,266 --> 00:25:19,200
And we're starting to see
those roll out nicely for this

504
00:25:19,266 --> 00:25:20,000
election coming up.

505
00:25:20,366 --> 00:25:24,566
I think for the 2020 election,
about 80% of voters will be

506
00:25:24,633 --> 00:25:26,166
voting on paper
ballots which is great.

507
00:25:26,233 --> 00:25:28,799
It is a big improvement
since prior years.

508
00:25:28,866 --> 00:25:32,733
And, in particular, the
swing states will all be using

509
00:25:32,799 --> 00:25:33,400
paper ballots.

510
00:25:33,466 --> 00:25:35,533
So, that's a good thing too.

511
00:25:35,533 --> 00:25:36,799
But we do have adversaries.

512
00:25:36,866 --> 00:25:40,700
Security is about adversaries
and we have to be cognizant of

513
00:25:40,766 --> 00:25:43,033
the fact that we need to design
our systems to have procedures

514
00:25:43,099 --> 00:25:45,765
in place, that people checking
other people, people checking

515
00:25:45,833 --> 00:25:48,400
machines to make sure that
the vote tallies are right.

516
00:25:48,466 --> 00:25:51,799
So, we have a lot of work to do
and I hope most importantly that

517
00:25:51,866 --> 00:25:53,799
everybody goes and votes.

518
00:25:53,866 --> 00:25:55,066
>> ZULKIFAR RAMZAN: Absolutely.

519
00:26:00,266 --> 00:26:02,566
So, one exciting thing is
that certainly these election

520
00:26:02,633 --> 00:26:05,366
security issues have brought
security concepts to the main

521
00:26:05,433 --> 00:26:07,599
front in terms of the mainstream
media covering them for the

522
00:26:07,666 --> 00:26:08,666
first time.

523
00:26:08,733 --> 00:26:11,166
There's also another recent
security story that came up in

524
00:26:11,233 --> 00:26:12,466
the mainstream media.

525
00:26:12,533 --> 00:26:15,366
That was a story released by the
Washington Post about a company

526
00:26:15,433 --> 00:26:18,766
called Crypto AG which
allegedly, even though it is a

527
00:26:18,833 --> 00:26:21,700
Swiss company, was actually
co-owned by the United States

528
00:26:21,766 --> 00:26:23,000
Central Intelligence Agency.

529
00:26:23,466 --> 00:26:25,466
Whit, I'm sure you have an
interesting perspective on

530
00:26:25,533 --> 00:26:26,866
Crypto AG and
I'd love to hear it.

531
00:26:28,866 --> 00:26:31,900
>> WHITFIELD DIFFIE: So, yeah,
it came out last week that a

532
00:26:31,966 --> 00:26:36,466
company that was the most famous
crypto company in the world and

533
00:26:36,533 --> 00:26:40,866
was in Switzerland, a neutral
country with very strong laws

534
00:26:40,933 --> 00:26:43,633
about its neutrality, and it
turned out to have been owned

535
00:26:43,700 --> 00:26:47,566
for decades jointly by the
Central Intelligence Agency and

536
00:26:47,633 --> 00:26:52,166
the Bundesnachrichtendienst and
was selling crypto equipment

537
00:26:52,233 --> 00:26:54,366
that they knew how to
break all over the world.

538
00:26:55,333 --> 00:26:58,766
So, I think I should start by
saying where I'm coming from

539
00:26:58,833 --> 00:27:02,433
which is I am very enthusiastic
about intelligence.

540
00:27:02,500 --> 00:27:05,900
It's best stated in Cold War
terms that we could have

541
00:27:05,966 --> 00:27:10,166
imagined nothing worse than two
major nuclear powers who didn't

542
00:27:10,233 --> 00:27:11,666
know anything about each other.

543
00:27:12,799 --> 00:27:16,500
So, I basically celebrate
intelligence successes.

544
00:27:16,566 --> 00:27:19,933
I have to admit I am a Yankee; I
celebrate ours a bit more than I

545
00:27:20,000 --> 00:27:23,866
celebrate theirs, but in
general, I think intelligence is

546
00:27:23,933 --> 00:27:28,066
a very valuable contribution
to stability in the world.

547
00:27:28,066 --> 00:27:30,400
Now, what we learned from this,
I think the first thing we

548
00:27:30,466 --> 00:27:34,733
learned is it's easy to get the
illusion, working in academic

549
00:27:34,799 --> 00:27:39,400
industrial cryptography, that
they're playing fair and

550
00:27:39,466 --> 00:27:41,299
intelligence is not
about playing fair.

551
00:27:41,366 --> 00:27:43,200
It's about succeeding.

552
00:27:43,266 --> 00:27:46,466
And there's no reason to be
sitting, waiting for them to

553
00:27:46,533 --> 00:27:49,599
make up cryptographic algorithms
that maybe you can break and

554
00:27:49,666 --> 00:27:53,399
maybe you can't if instead you
can push one on them that you

555
00:27:53,466 --> 00:27:54,132
can break.

556
00:27:55,266 --> 00:28:00,166
And that is what this did with
amazing success for twenty,

557
00:28:00,233 --> 00:28:02,500
thirty, forty years.

558
00:28:02,566 --> 00:28:08,633
Now I think we look carefully
at this, we see two things that

559
00:28:08,700 --> 00:28:11,333
relate to the doctrine of
this community, the public

560
00:28:11,400 --> 00:28:12,666
cryptographic community.

561
00:28:13,466 --> 00:28:18,065
One is that we've always
preached that you should make

562
00:28:18,133 --> 00:28:19,433
cryptographic systems public.

563
00:28:19,500 --> 00:28:21,366
You shouldn't keep
them secret, right.

564
00:28:21,433 --> 00:28:23,733
And I think it's clear this
shows very clearly, we were

565
00:28:23,799 --> 00:28:24,833
right about that.

566
00:28:24,900 --> 00:28:28,533
They wouldn't have gotten away
with this if all of these people

567
00:28:28,599 --> 00:28:33,066
had been disclosing what
cryptography they were using.

568
00:28:33,066 --> 00:28:35,900
But the other thing we've been
preaching for a long time is

569
00:28:35,966 --> 00:28:39,832
well, designing cryptographic
systems is difficult.

570
00:28:39,900 --> 00:28:40,933
You shouldn't do it yourself.

571
00:28:41,000 --> 00:28:44,533
You should leave it to people
like Vincent who are good at it.

572
00:28:44,599 --> 00:28:49,133
And I think it's now clear that
if all these little countries

573
00:28:49,200 --> 00:28:53,066
had designed their own, maybe
they wouldn't have been terribly

574
00:28:53,133 --> 00:28:55,066
good but it would have meant
there had been a dozen or two

575
00:28:55,133 --> 00:28:58,333
dozen or three dozen crypto
analytic projects that would

576
00:28:58,400 --> 00:29:02,366
have had to have been fielded to
read their traffic rather than

577
00:29:02,433 --> 00:29:06,266
had they bought the equipment
from the same expert people who

578
00:29:06,333 --> 00:29:08,266
happen to be making
compromised equipment.

579
00:29:09,333 --> 00:29:12,400
So, these lessons are
very relevant today.

580
00:29:12,466 --> 00:29:18,666
We're accusing Kaspersky in
Russia or Huawei in China of

581
00:29:18,733 --> 00:29:22,933
building compromises into their
equipment and are hesitant to

582
00:29:23,000 --> 00:29:27,299
buy them for that reason and I
think perhaps we should be and

583
00:29:27,366 --> 00:29:28,266
perhaps they should be.

584
00:29:29,566 --> 00:29:32,366
>> ADI SHAMIR: The only thing
I don't understand is why they

585
00:29:32,433 --> 00:29:33,633
call it news.

586
00:29:33,700 --> 00:29:39,200
I heard this story twenty years
ago and it was Hans Buehler who

587
00:29:39,266 --> 00:29:39,799
wrote a book about it.

588
00:29:40,866 --> 00:29:42,366
>> WHITFIELD DIFFIE: The
story is not new, but the

589
00:29:42,433 --> 00:29:44,633
documentation of
the story is new.

590
00:29:44,700 --> 00:29:49,133
They got histories written by
both the yanks and the Germans.

591
00:29:49,566 --> 00:29:53,000
>> ADI SHAMIR: But was there
any fact which you didn't know?

592
00:29:53,000 --> 00:29:55,033
>> WHITFIELD DIFFIE: There's
a lot of sort of detail how

593
00:29:55,099 --> 00:29:59,366
complex - it's easy to say
oh, we bought this company in

594
00:29:59,433 --> 00:30:00,633
Switzerland, right.

595
00:30:00,700 --> 00:30:04,033
Doing something like that
covertly is quite tricky.

596
00:30:04,099 --> 00:30:08,666
And I think that's - get enough
detail of this, it becomes much

597
00:30:08,733 --> 00:30:09,733
more believable.

598
00:30:09,733 --> 00:30:12,000
>> ADI SHAMIR: I read the story
only in order to find out

599
00:30:12,066 --> 00:30:16,599
whether there's any hint how
they are, the CIA, manipulated

600
00:30:16,666 --> 00:30:18,733
the crypto in order
to make it breakable.

601
00:30:18,799 --> 00:30:21,200
Unfortunately, there
was no information.

602
00:30:23,366 --> 00:30:24,633
>> ZULKIFAR RAMZAN: There's
been a lot of effort recently,

603
00:30:24,700 --> 00:30:26,400
obviously in thinking about, you
know, who we're going to elect

604
00:30:26,466 --> 00:30:29,065
in the next election and the
implications it could have on

605
00:30:29,133 --> 00:30:30,833
cybersecurity policy.

606
00:30:30,900 --> 00:30:33,833
Specifically, another recent
news item is the policy around

607
00:30:33,900 --> 00:30:36,766
encryption in general and more
recently, some of the debate

608
00:30:36,833 --> 00:30:38,133
around end to end encryption.

609
00:30:38,599 --> 00:30:41,666
So, Whit, kind of following up
on your previous answer, what's

610
00:30:41,733 --> 00:30:44,466
your take on the current policy
debate on end to end encryption

611
00:30:44,533 --> 00:30:46,065
and where do you think
it's going to lead?

612
00:30:46,066 --> 00:30:47,933
>> WHITFIELD DIFFIE: Well, when
we pseudo won the argument

613
00:30:48,000 --> 00:30:51,266
around 2000, I said oh, this
isn't going to go away.

614
00:30:51,333 --> 00:30:54,866
Everybody basically is fighting
for what relative positions are

615
00:30:54,933 --> 00:30:59,466
going to be in the
information world and I haven't

616
00:30:59,533 --> 00:31:01,699
changed sides.

617
00:31:01,766 --> 00:31:07,033
And I think that the people who
are - who put themselves forward

618
00:31:07,099 --> 00:31:09,933
as being concerned with quote,
"law enforcement and national

619
00:31:10,000 --> 00:31:13,766
security," have a
very narrow view.

620
00:31:14,533 --> 00:31:18,332
And, yeah, cryptography will
often make their work harder.

621
00:31:18,400 --> 00:31:22,366
But the critical thing I believe
at this moment, you know, even

622
00:31:22,433 --> 00:31:25,166
though we haven't exactly seen
it yet, I kind of believe in

623
00:31:25,233 --> 00:31:27,832
that phrase, a
cyber Pearl Harbor.

624
00:31:27,900 --> 00:31:32,266
And we need to do everything we
can to make our systems secure.

625
00:31:32,933 --> 00:31:37,233
And if you build some kind of
backdoor mechanism into them, it

626
00:31:37,299 --> 00:31:40,466
makes them just
immensely more complex.

627
00:31:40,533 --> 00:31:46,466
And that - I think at present we
know how to be confident of the

628
00:31:46,533 --> 00:31:49,099
security of very simple objects.

629
00:31:49,166 --> 00:31:52,299
We haven't done badly with
crypto systems themselves, but

630
00:31:52,366 --> 00:31:55,799
anything the size of an
operating system, apps and so

631
00:31:55,866 --> 00:31:57,733
forth, we're very poor at it.

632
00:31:57,799 --> 00:32:01,133
So, I think we need - the
critical thing we need to do is

633
00:32:01,200 --> 00:32:03,366
everything to make
our systems secure.

634
00:32:03,433 --> 00:32:08,666
And I don't believe building
side doors into them is going

635
00:32:08,733 --> 00:32:09,599
to help.

636
00:32:09,666 --> 00:32:10,599
>> ZULKIFAR RAMZAN: Absolutely.

637
00:32:12,033 --> 00:32:15,533
>> RON RIVEST: This is a
complicated topic and as Whit

638
00:32:15,599 --> 00:32:17,866
says, I think it's going to
be around for a long time.

639
00:32:17,933 --> 00:32:21,033
There was a report which I can
recommend to you all to read

640
00:32:21,099 --> 00:32:23,899
which is called Moving the
Encryption Conversation Forward

641
00:32:23,966 --> 00:32:27,299
put out by the Encryption
Working Group sponsored by the

642
00:32:27,366 --> 00:32:29,266
Carnegie Endowment for
International Peace.

643
00:32:30,233 --> 00:32:35,166
It talks about the stakes, the
various players, what their

644
00:32:35,233 --> 00:32:37,366
positions are and how
intractable the problem

645
00:32:37,433 --> 00:32:38,466
really is.

646
00:32:38,533 --> 00:32:43,065
It's difficult to see how to
move forward, in any extent,

647
00:32:43,133 --> 00:32:44,333
whatever forward means.

648
00:32:44,400 --> 00:32:48,166
I mean, maybe progress in
some sense isn't needed.

649
00:32:48,233 --> 00:32:50,799
I was reminded by the
conversation earlier about the

650
00:32:50,866 --> 00:32:52,299
right to be forgotten.

651
00:32:52,366 --> 00:32:55,433
I think to coin a phrase, we all
sort of sense we should have a

652
00:32:55,500 --> 00:32:57,266
right to be private as well.

653
00:32:57,333 --> 00:33:00,599
So, RTBP or something like that.

654
00:33:02,033 --> 00:33:04,599
I think that's a sense that we
all have in a democracy, that

655
00:33:04,666 --> 00:33:07,366
citizens have the right to
communicate privately with each

656
00:33:07,433 --> 00:33:10,299
other using technology or
however they wish to do it, out

657
00:33:10,366 --> 00:33:11,733
on a rowboat on a lake.

658
00:33:11,799 --> 00:33:14,200
So, it is a difficult problem.

659
00:33:14,266 --> 00:33:16,799
This report that I mentioned
said that, you know, there are

660
00:33:16,866 --> 00:33:19,500
no technical solutions that are
going to keep everybody happy.

661
00:33:19,566 --> 00:33:24,700
It is a problem with
many, many special cases.

662
00:33:24,766 --> 00:33:27,966
If you want to try to think
about one case where perhaps

663
00:33:28,033 --> 00:33:33,799
progress might be possible, it's
the case of a device that's been

664
00:33:33,866 --> 00:33:36,033
captured by law enforcement;
it's in the hands law

665
00:33:36,099 --> 00:33:38,200
enforcement and they want to
know what's on that device.

666
00:33:38,266 --> 00:33:40,433
So, it's device storage,
it's data storage.

667
00:33:40,500 --> 00:33:45,633
It's not data communications on
the wire, but it's storage on

668
00:33:45,700 --> 00:33:46,366
the device.

669
00:33:47,266 --> 00:33:50,366
That's a use case where you
might think about hard if you

670
00:33:50,433 --> 00:33:52,466
want to try to see if there's
any technical approaches that

671
00:33:52,533 --> 00:33:53,565
would look interesting.

672
00:33:53,633 --> 00:33:56,333
But at the moment we don't have
any technical solutions that

673
00:33:56,400 --> 00:33:58,066
look acceptable to everybody.

674
00:34:00,133 --> 00:34:01,400
>> ZULKIFAR RAMZAN: So, as far
as we've seen, I think policies

675
00:34:01,466 --> 00:34:05,099
are certainly one attempt to get
a technical impediment or any

676
00:34:05,166 --> 00:34:07,599
impediment of any sort
into some of these areas.

677
00:34:07,666 --> 00:34:09,966
We've also heard very recently
about a technical impediment

678
00:34:10,033 --> 00:34:12,433
that is coming into vogue,
namely quantum computing.

679
00:34:12,900 --> 00:34:14,699
Now, as many of you know, if
quantum computers are built to

680
00:34:14,766 --> 00:34:16,833
scale, then in theory they
could be used to break the RSA

681
00:34:16,900 --> 00:34:18,866
cryptosystem and the
Diffie-Hellman key

682
00:34:18,933 --> 00:34:19,900
exchange protocol.

683
00:34:19,966 --> 00:34:21,766
So, at least three people on
this stage, I'm sure, have a

684
00:34:21,833 --> 00:34:24,966
vested interest in the outcome
of what's going to happen with

685
00:34:25,033 --> 00:34:25,799
quantum computing.

686
00:34:26,366 --> 00:34:28,933
Maybe, Ron, starting with you,
what's your take on quantum

687
00:34:29,000 --> 00:34:30,599
computing and where do you
think it's going to go?

688
00:34:30,599 --> 00:34:33,466
>> RON RIVEST: My position
hasn't really changed since last

689
00:34:33,533 --> 00:34:35,000
year I think we talked
a little bit about this.

690
00:34:35,065 --> 00:34:38,466
I hope people building
quantum computers fail.

691
00:34:38,533 --> 00:34:40,199
(Laughter)

692
00:34:42,233 --> 00:34:45,866
It would mean the demise of a
lovely algorithm which we all

693
00:34:45,933 --> 00:34:47,099
use and appreciate.

694
00:34:47,966 --> 00:34:50,400
On the other hand, one has to
accept the truth and maybe

695
00:34:50,466 --> 00:34:52,900
quantum computers will happen.

696
00:34:52,966 --> 00:34:56,766
You know, there are
applications of them maybe.

697
00:34:56,833 --> 00:35:00,500
They seem to be mostly designed
to break RSA and not much else,

698
00:35:00,566 --> 00:35:03,698
unlike maybe blockchain
which has a wider span

699
00:35:03,766 --> 00:35:05,199
of applications.

700
00:35:05,266 --> 00:35:09,366
So, I think it's good that we're
preparing now for the possible

701
00:35:09,433 --> 00:35:11,400
prevalence or use of
quantum computers.

702
00:35:11,466 --> 00:35:14,633
People are looking at post
quantum cryptography.

703
00:35:14,699 --> 00:35:17,500
But I actually am somewhat
skeptical about the question as

704
00:35:17,566 --> 00:35:19,332
to whether they'll
be built, in fact.

705
00:35:19,400 --> 00:35:22,800
I sort of give fusion power a
higher chance of succeeding than

706
00:35:22,866 --> 00:35:24,266
quantum computing.

707
00:35:25,400 --> 00:35:26,266
We'll see.

708
00:35:26,333 --> 00:35:28,633
>> ADI SHAMIR: Ron, I don't
understand your position.

709
00:35:28,699 --> 00:35:31,699
Quantum computing has
reached a supremacy stage.

710
00:35:31,766 --> 00:35:36,566
It's now reached quantum
supremacy, so you should have

711
00:35:36,633 --> 00:35:39,165
changed your mind
about the situation.

712
00:35:39,233 --> 00:35:40,833
>> RON RIVEST: So, the
particular demonstration that

713
00:35:40,900 --> 00:35:46,066
was done by Google did allegedly
show that a quantum computer can

714
00:35:46,133 --> 00:35:49,098
do something that a classical
computer cannot do efficiently.

715
00:35:49,166 --> 00:35:52,233
In fact, IBM said it could be
done still but with a few more

716
00:35:52,300 --> 00:35:53,466
bits, maybe not.

717
00:35:53,533 --> 00:35:55,598
But it wasn't breaking crypto
and it wasn't scaling up.

718
00:35:55,666 --> 00:35:58,066
There's a lot of scaling that
has to be done before you can

719
00:35:58,133 --> 00:36:01,198
make these quantum computers
usable for breaking crypto.

720
00:36:01,266 --> 00:36:04,166
And that may never happen.

721
00:36:04,233 --> 00:36:05,099
It's really hard.

722
00:36:06,233 --> 00:36:08,133
>> ADI SHAMIR:
I agree. I was just joking.

723
00:36:10,400 --> 00:36:12,333
>> ZULKIFAR RAMZAN: You know,
few years ago, a very wise

724
00:36:12,400 --> 00:36:15,233
person, that wise person being
Adi Shamir, said on this stage

725
00:36:15,300 --> 00:36:18,333
that people rarely break
crypto; they find ways to

726
00:36:18,400 --> 00:36:19,533
bypass cryptography.

727
00:36:20,300 --> 00:36:22,000
Quite recently we've seen a
lot of interesting attacks

728
00:36:22,066 --> 00:36:25,533
specifically in the mobile phone
space, things like SIM swapping.

729
00:36:25,599 --> 00:36:27,500
I know Arvind, you spent quite
a lot of time thinking about

730
00:36:27,566 --> 00:36:28,665
SIM swapping.

731
00:36:28,733 --> 00:36:30,633
What's the current state of
affairs and can you share some

732
00:36:30,699 --> 00:36:32,166
of your recent work in the area?

733
00:36:32,233 --> 00:36:33,199
>> ARVIND NARYANAN: Sure.

734
00:36:33,266 --> 00:36:36,500
So, we were looking at some
of the rhetoric around crypto

735
00:36:36,566 --> 00:36:39,265
currencies, that's how we
started looking into this, the

736
00:36:39,333 --> 00:36:43,000
rhetoric being that it's
ultra-secure because it relies

737
00:36:43,066 --> 00:36:44,899
only on math and cryptography.

738
00:36:44,966 --> 00:36:47,900
And while that part is true,
what is also happening is that

739
00:36:47,966 --> 00:36:51,366
a lot of people are losing
their crypto currency in very

740
00:36:51,433 --> 00:36:53,033
low-tech, old fashioned ways.

741
00:36:53,099 --> 00:36:55,666
And that brings us right
back to the human element.

742
00:36:55,666 --> 00:36:58,933
And, in particular, the
majority of people who have

743
00:36:59,000 --> 00:37:01,333
crypto currency store
them in online wallets.

744
00:37:01,400 --> 00:37:04,033
Which is fine if you protect
your account properly.

745
00:37:04,099 --> 00:37:07,099
But as we know, passwords are
very easy to compromise so these

746
00:37:07,166 --> 00:37:09,566
online wallet services will
make you get two-factor

747
00:37:09,633 --> 00:37:13,299
authentication, often using
SMS as a second factor.

748
00:37:13,733 --> 00:37:16,000
Now, if there's one thing that's
easier to compromise than

749
00:37:16,066 --> 00:37:18,732
passwords, it turns out
to be taking control of your

750
00:37:18,800 --> 00:37:19,633
mobile accounts.

751
00:37:19,633 --> 00:37:22,698
So, that's what we tried
to rigorously look at.

752
00:37:22,766 --> 00:37:26,300
Four of us at Princeton, Ben
Kaiser, Kevin Lee, Jonathan

753
00:37:26,366 --> 00:37:27,966
Mayer and myself.

754
00:37:28,033 --> 00:37:32,400
What we did is we wanted
to see how easy so-called SIM

755
00:37:32,466 --> 00:37:33,433
swaps are.

756
00:37:33,500 --> 00:37:37,300
What happens in a SIM swap is
that an attacker calls your

757
00:37:37,366 --> 00:37:40,633
mobile carrier, pretends to
be you, and convinces them to

758
00:37:40,699 --> 00:37:44,300
transfer your mobile service to
a SIM card that the attacker

759
00:37:44,366 --> 00:37:45,466
controls. Right?

760
00:37:45,466 --> 00:37:48,033
So, now they control your mobile
phone number and then they can

761
00:37:48,099 --> 00:37:51,533
use that to easily break the
two-factor authentication that

762
00:37:51,599 --> 00:37:53,099
you might have on
your online services.

763
00:37:53,900 --> 00:37:57,166
We tried five different mobile
carriers and, in each case, we

764
00:37:57,233 --> 00:37:59,866
created ten different prepaid
accounts and tried to SIM

765
00:37:59,933 --> 00:38:01,199
swap ourselves.

766
00:38:01,266 --> 00:38:03,599
We were successful
with all five carriers.

767
00:38:03,666 --> 00:38:06,800
All five were using
authentication methods that are

768
00:38:06,866 --> 00:38:08,333
known to be vulnerable.

769
00:38:08,400 --> 00:38:12,133
One interesting example is that
some carriers, if you call them

770
00:38:12,199 --> 00:38:16,466
and you are able to tell them
one or two of the numbers that

771
00:38:16,533 --> 00:38:19,900
most recently called you, then
they're convinced that you must

772
00:38:19,966 --> 00:38:21,866
be the right person.

773
00:38:21,933 --> 00:38:22,866
But how can this go wrong?

774
00:38:23,133 --> 00:38:26,366
The attacker can just call the
victim and insert a number into

775
00:38:26,433 --> 00:38:27,566
their call log, right?

776
00:38:27,633 --> 00:38:29,198
So, they hadn't
thought about this.

777
00:38:29,266 --> 00:38:33,000
So, we found many
vulnerabilities of this type and

778
00:38:33,066 --> 00:38:35,232
we published a research
paper on that recently.

779
00:38:35,233 --> 00:38:38,533
So, the one thing I would say is
if you have a few minutes and I

780
00:38:38,599 --> 00:38:40,933
think this is really worth a few
minutes of your time, go check

781
00:38:41,000 --> 00:38:42,400
all of your online accounts.

782
00:38:42,466 --> 00:38:44,165
Make sure two-factor
authentication is enabled.

783
00:38:44,233 --> 00:38:46,500
I'm not saying don't enable
two-factor authentication.

784
00:38:46,566 --> 00:38:49,098
But make sure it's a secure
second factor such as an

785
00:38:49,166 --> 00:38:52,366
authenticator app rather than
SMS which continues to be

786
00:38:52,433 --> 00:38:53,199
very vulnerable.

787
00:38:54,400 --> 00:38:56,033
>> ZULKIFAR RAMZAN: So, I think
the summary is security is

788
00:38:56,099 --> 00:38:57,066
really hard?

789
00:38:57,133 --> 00:38:59,466
>> ARVIND NARYANAN: Yes. But
there are things you can do.

790
00:39:00,033 --> 00:39:01,098
>> ZULKIFAR RAMZAN: Absolutely.

791
00:39:01,166 --> 00:39:02,933
So, you know, we are kind of on
the last question I want to ask.

792
00:39:03,000 --> 00:39:05,400
It is the year 2020.

793
00:39:05,466 --> 00:39:07,833
So, I'm curious from the panel,
and this is for each of you,

794
00:39:07,900 --> 00:39:10,966
what have been some of the
most noteworthy or maybe even

795
00:39:11,033 --> 00:39:14,033
surprising or unexpected trends
from the past twenty years and

796
00:39:14,099 --> 00:39:16,666
what do you think will be
important for the next twenty?

797
00:39:16,733 --> 00:39:18,566
Maybe starting with Ron
and working our way down?

798
00:39:18,633 --> 00:39:20,332
>> RON RIVEST: Sure.

799
00:39:20,400 --> 00:39:23,133
I guess the older I get, the
more I see things keep changing.

800
00:39:23,199 --> 00:39:26,599
I guess being prepared for
change is the main message that

801
00:39:26,666 --> 00:39:28,266
we've learned here.

802
00:39:28,333 --> 00:39:30,233
So, there's new technology.

803
00:39:30,300 --> 00:39:32,633
Quantum computing is
coming online maybe.

804
00:39:32,699 --> 00:39:33,733
So, we have new technology.

805
00:39:33,800 --> 00:39:34,833
We have new math.

806
00:39:34,900 --> 00:39:37,099
Fully homomorphic
encryption, perhaps.

807
00:39:37,166 --> 00:39:38,666
We have new environments.

808
00:39:38,733 --> 00:39:40,166
Smart phones have
really changed the game.

809
00:39:40,233 --> 00:39:42,633
Arvind was just talking about
some of the technologies there.

810
00:39:43,366 --> 00:39:45,366
We have new adversaries
in the upcoming elections.

811
00:39:45,433 --> 00:39:47,699
It may not be the Russians that
are the biggest threat, but it

812
00:39:47,766 --> 00:39:48,766
may be the Coronavirus.

813
00:39:48,833 --> 00:39:51,099
You know, so, we'll see.

814
00:39:51,166 --> 00:39:53,133
There are things
that need changed.

815
00:39:53,199 --> 00:39:54,366
We have to be
prepared for change.

816
00:39:54,433 --> 00:39:56,699
And it takes time to
prepare for change.

817
00:39:56,766 --> 00:39:58,966
It takes twenty years, I think,
to get technology from a white

818
00:39:59,033 --> 00:40:00,165
board to a product.

819
00:40:00,233 --> 00:40:03,566
Three more years in the
patent which I think is maybe

820
00:40:03,633 --> 00:40:04,765
related somehow.

821
00:40:05,800 --> 00:40:09,599
So, we need to keep preparing
for change, building up our

822
00:40:09,666 --> 00:40:12,800
expertise, getting ready for
things that could be wildly

823
00:40:12,866 --> 00:40:14,000
different than we expect.

824
00:40:14,066 --> 00:40:17,299
So, the technology, the
adversaries, the math, all of

825
00:40:17,366 --> 00:40:18,400
that keeps changing.

826
00:40:20,633 --> 00:40:24,966
>> ADI SHAMIR: I think one of
the important successes of the

827
00:40:25,033 --> 00:40:29,633
last twenty years has been the
successful introduction and

828
00:40:29,699 --> 00:40:31,533
deployment of AES.

829
00:40:31,599 --> 00:40:35,633
It is related to the prize
that has just been given.

830
00:40:36,466 --> 00:40:43,533
I think that it did a wonderful
job in securing our systems.

831
00:40:44,466 --> 00:40:51,199
However, I still have a bit of
uneasy feeling about the number

832
00:40:51,266 --> 00:40:52,966
of rounds in AES.

833
00:40:53,599 --> 00:40:57,466
The way we usually design
cryptosystems is to design

834
00:40:57,533 --> 00:41:02,933
something with a number of
rounds up to the brink at which

835
00:41:03,000 --> 00:41:07,666
we can no longer break it then
add a margin of safety by say,

836
00:41:07,733 --> 00:41:09,266
doubling the number of rounds.

837
00:41:09,966 --> 00:41:13,533
So, if I look at AES,
let's do it in reverse.

838
00:41:13,599 --> 00:41:18,766
Let's take the ten-round AES
and have it and ask how much

839
00:41:18,833 --> 00:41:22,066
security is there
in half of AES.

840
00:41:22,133 --> 00:41:23,633
Five round AES.

841
00:41:23,699 --> 00:41:29,099
And I've been banging my head
against this question over the

842
00:41:29,166 --> 00:41:33,000
last few years and the security
of half of AES has been going

843
00:41:33,066 --> 00:41:34,566
down and down.

844
00:41:34,633 --> 00:41:37,933
And this year in a few months,
I'll be presenting a paper at

845
00:41:38,000 --> 00:41:44,566
Eurocrypt in which we show that
half of AES can be broken with

846
00:41:44,633 --> 00:41:50,198
two to the 16th time in the data
which is absolutely nothing.

847
00:41:50,266 --> 00:41:54,333
It's something that can be done
in a fraction of a second or a

848
00:41:54,400 --> 00:41:55,133
few seconds.

849
00:41:57,000 --> 00:42:05,233
So, I believe that we violated
the major design criteria that

850
00:42:05,300 --> 00:42:08,766
we need reasonable margins of
safety against developments in

851
00:42:08,833 --> 00:42:09,933
the future.

852
00:42:10,000 --> 00:42:17,533
And in the past, such over
confidence led to our failures.

853
00:42:17,599 --> 00:42:22,300
For example, in the design
of AES, it was marginal to

854
00:42:22,366 --> 00:42:23,266
begin with.

855
00:42:23,333 --> 00:42:28,433
The NSA knew about the
differential crypto analysis and

856
00:42:28,500 --> 00:42:31,233
therefore knew how to break
fifteen out of the sixteen

857
00:42:31,300 --> 00:42:36,266
rounds of AES and they assumed
that sixteen rounds will give

858
00:42:36,333 --> 00:42:40,466
them security while later
developments reduced the

859
00:42:40,533 --> 00:42:45,098
complexity of breaking the AES,
full AES, to two to the 43.

860
00:42:46,233 --> 00:42:54,099
And I think that if we could
roll back to the year 2000, I

861
00:42:54,166 --> 00:42:57,233
remember at that time I gave the
advice that the number of rounds

862
00:42:57,300 --> 00:42:59,733
should be increased
to at least sixteen.

863
00:42:59,800 --> 00:43:02,233
I still stand behind this
twenty-year-old suggestion.

864
00:43:06,000 --> 00:43:08,166
>> WHITFIELD DIFFIE: Well, I
think what impresses me about

865
00:43:08,233 --> 00:43:10,866
cryptography is how
it goes on and on.

866
00:43:10,933 --> 00:43:13,599
I mean I've been saying for
years, you know, my late wife

867
00:43:13,666 --> 00:43:17,866
used to say a couple hundred
years ago, an Irish wolfhound

868
00:43:17,933 --> 00:43:20,166
did the worst thing any
Irish wolfhound could do.

869
00:43:20,233 --> 00:43:23,166
It killed the last
wolf in Ireland.

870
00:43:23,233 --> 00:43:26,400
And since then, the breed
has been more decorative.

871
00:43:27,133 --> 00:43:31,299
But I thought circa 2000
that so to speak, we'd solved

872
00:43:31,366 --> 00:43:32,400
the problem.

873
00:43:32,466 --> 00:43:35,400
To put it in American terms, you
know, the sweet bee algorithms

874
00:43:35,466 --> 00:43:37,232
seemed to me to be satisfactory.

875
00:43:37,300 --> 00:43:40,566
And I still think that's far
and away the solidest part of

876
00:43:40,633 --> 00:43:41,866
information security.

877
00:43:42,466 --> 00:43:44,966
And I thought cryptography would
sort of, you know, wither away

878
00:43:45,033 --> 00:43:46,299
as a research field.

879
00:43:46,366 --> 00:43:49,133
But every year it surges up.

880
00:43:49,199 --> 00:43:52,333
And I think, you know, the thing
that impressed me most was the

881
00:43:52,400 --> 00:43:57,000
rise of blockchain and Bitcoin
and thousands of people going to

882
00:43:57,066 --> 00:44:00,698
conferences that only
existed for a year or two.

883
00:44:00,766 --> 00:44:05,766
So, I'm just amazed
with the durability.

884
00:44:06,300 --> 00:44:08,533
>> TAL RABIN: I agree with
everything that Whit said.

885
00:44:08,599 --> 00:44:12,433
In fact, I want to enhance
something about the blockchain.

886
00:44:12,500 --> 00:44:15,500
What amazes me about that thing
which is definitely one of the

887
00:44:15,566 --> 00:44:18,799
things that happened in these
twenty years, is that it went

888
00:44:18,866 --> 00:44:23,433
back and it took things that
we have known since the 80s.

889
00:44:23,500 --> 00:44:29,166
Byzantine agreement, hash
functions and proof of work and

890
00:44:29,233 --> 00:44:31,000
combined all these things.

891
00:44:31,000 --> 00:44:34,800
So, this is another thing that I
love about this field, that it

892
00:44:34,866 --> 00:44:39,300
can go and go into the past,
bring things from the past and

893
00:44:39,366 --> 00:44:42,766
make them into magnificent and
wonderful things of the future.

894
00:44:43,166 --> 00:44:46,300
Maybe blockchain, we're
still lacking the killer app.

895
00:44:46,366 --> 00:44:48,966
Maybe everything can
be done better using some

896
00:44:49,033 --> 00:44:50,433
other technology.

897
00:44:50,500 --> 00:44:54,400
But even if it was just for the
introduction, for the fact that

898
00:44:54,466 --> 00:44:58,533
almost every person in the world
knows the word crypto, which it

899
00:44:58,599 --> 00:45:01,966
did not know ten years ago.

900
00:45:02,033 --> 00:45:03,500
>> WHITFIELD DIFFIE: Even if
I changed meaning a little.

901
00:45:03,566 --> 00:45:04,765
>> TAL RABIN: Completely.

902
00:45:04,833 --> 00:45:09,266
I have to tell you at Crypto
2018, I was the general chair

903
00:45:09,333 --> 00:45:12,666
and one of these kids who is
twenty comes up to me and he

904
00:45:12,733 --> 00:45:16,033
says, this conference
is not about crypto.

905
00:45:16,099 --> 00:45:17,933
How dare you use
that name, you know?

906
00:45:18,000 --> 00:45:22,800
To tell him that this conference
has been around for long before

907
00:45:22,866 --> 00:45:24,133
he was born.

908
00:45:24,199 --> 00:45:26,533
So, definitely with
a changed meaning.

909
00:45:26,533 --> 00:45:29,966
But I think that this is
the power of this field.

910
00:45:30,033 --> 00:45:34,299
And it really brings beauty to
all the things you can go, and

911
00:45:34,366 --> 00:45:37,433
you can keep on designing,
and as Ron said but also new

912
00:45:37,500 --> 00:45:40,533
technologies like fully
homomorphic encryption,

913
00:45:40,599 --> 00:45:45,033
obfuscation, things that maybe
today we don't even know 100%

914
00:45:45,099 --> 00:45:46,433
what to do with them.

915
00:45:46,500 --> 00:45:50,400
But maybe in twenty, forty more
years we will know much more.

916
00:45:50,466 --> 00:45:54,466
And I think that I don't see
it dying in the near future.

917
00:45:54,533 --> 00:45:58,766
So, I see a future for
everybody here for a long time.

918
00:45:59,933 --> 00:46:01,866
>> ARVIND NARYANAN: I want to
give a shoutout to differential

919
00:46:01,933 --> 00:46:06,633
privacy which, as you may know,
is a technology to release a

920
00:46:06,699 --> 00:46:09,733
data set while protecting
individual privacy and still

921
00:46:09,800 --> 00:46:12,366
allowing aggregate
analyses to be possible.

922
00:46:12,366 --> 00:46:14,033
It's about fifteen years old.

923
00:46:14,099 --> 00:46:16,699
For most of this duration,
it's been a relatively obscure

924
00:46:16,766 --> 00:46:20,933
technology but now it's facing
its first major public test,

925
00:46:21,000 --> 00:46:23,666
let's say, because it's being
used by the 2020 US Census.

926
00:46:24,300 --> 00:46:26,633
So, in this process, you know,
going from something obscure to

927
00:46:26,699 --> 00:46:29,333
something that we're all
going to be relying on, it's

928
00:46:29,400 --> 00:46:30,333
been interesting.

929
00:46:30,400 --> 00:46:33,433
There's been a lot of
misinformation about around it.

930
00:46:33,500 --> 00:46:36,866
The craziest one I've heard is
that differential privacy is

931
00:46:36,933 --> 00:46:41,099
actually a conspiracy by the
Trump Administration because

932
00:46:41,166 --> 00:46:43,933
they've secretly figured out a
way to break it and they are

933
00:46:44,000 --> 00:46:48,066
selling us a false promise
of confidentiality in our

934
00:46:48,133 --> 00:46:49,466
census responses.

935
00:46:49,533 --> 00:46:50,799
Well, that's not true.

936
00:46:50,866 --> 00:46:52,233
Differential
privacy long predates the

937
00:46:52,300 --> 00:46:53,300
Trump Administration.

938
00:46:53,366 --> 00:46:54,866
It also comes with a
mathematical proof.

939
00:46:54,933 --> 00:46:58,400
But the bigger issue here, I
think, is that what needs to

940
00:46:58,466 --> 00:47:02,133
happen is for every technologist
inventing something really cool

941
00:47:02,199 --> 00:47:04,566
like differential privacy or
quantum computing or whatever it

942
00:47:04,633 --> 00:47:07,899
is, we probably need something
like ten people whose role it is

943
00:47:07,966 --> 00:47:11,533
to explain it to the public
and to policy makers and more

944
00:47:11,599 --> 00:47:14,666
importantly, figure out how our
public institutions need to

945
00:47:14,733 --> 00:47:17,933
change to best adapt to these
technologies and minimize the

946
00:47:18,000 --> 00:47:19,133
harmful effects.

947
00:47:19,199 --> 00:47:20,566
You know, not lobbyists.

948
00:47:20,633 --> 00:47:22,433
We need neutral people
to be doing that.

949
00:47:22,500 --> 00:47:25,300
That's something that I
think we're lacking today and

950
00:47:25,366 --> 00:47:27,766
something which I hope we can
figure out in the next twenty

951
00:47:27,833 --> 00:47:28,966
years or so.

952
00:47:29,033 --> 00:47:30,133
>> ZULKIFAR RAMZAN: So,
I guess it all comes down to the

953
00:47:30,199 --> 00:47:31,300
human element.

954
00:47:31,366 --> 00:47:33,766
Please join me once again in
thanking our amazing panel for

955
00:47:33,833 --> 00:47:35,000
an insightful discussion.

956
00:47:35,066 --> 00:47:36,466
(Applause)

