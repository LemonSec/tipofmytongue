1
00:00:06,233 --> 00:00:09,565
>> ANNOUNCER: Please
welcome Jules Polonetsky!

2
00:00:23,000 --> 00:00:24,233
>> JULES POLONETSKY:
Hey, everybody.

3
00:00:24,300 --> 00:00:26,666
Thank you for
making it here today.

4
00:00:26,733 --> 00:00:28,833
This is not a group
that fears viruses.

5
00:00:28,899 --> 00:00:30,100
You fight them in every way.

6
00:00:30,166 --> 00:00:32,665
I asked whether anybody had
been making virus jokes at the

7
00:00:32,732 --> 00:00:35,132
security community and
they said nobody has.

8
00:00:35,200 --> 00:00:39,399
Apologies if it's been an
obvious tie-in for all of you.

9
00:00:39,466 --> 00:00:43,533
Kudos to all of you who are here
and who are actually paying

10
00:00:43,600 --> 00:00:45,500
attention to privacy
today in the midst of a

11
00:00:45,566 --> 00:00:47,033
security conference.

12
00:00:47,100 --> 00:00:50,766
A couple of years ago, there
were no privacy panels at an RSA

13
00:00:50,833 --> 00:00:52,866
or many of the other
security conferences.

14
00:00:52,933 --> 00:00:55,299
It's really just been in the
last couple of years where I

15
00:00:55,366 --> 00:00:58,600
think these two communities have
realized that they really are

16
00:00:58,666 --> 00:01:02,532
part and parcel, right, in more
than some of the obvious ways.

17
00:01:02,600 --> 00:01:06,066
Anybody who works in privacy,
who introduces themselves, right

18
00:01:06,133 --> 00:01:08,699
away people say to you, oh,
you're keeping the data safe.

19
00:01:08,766 --> 00:01:09,666
You're like, no, no, no, no.

20
00:01:09,733 --> 00:01:11,133
That's not exactly what I do.

21
00:01:11,200 --> 00:01:13,033
They want to know what
the difference is. Right?

22
00:01:13,099 --> 00:01:15,866
In some ways, the difference is
that those of you who work in

23
00:01:15,933 --> 00:01:19,966
security are worried about the
bad folks getting the data.

24
00:01:20,033 --> 00:01:22,833
And those of us who work in
privacy are worried about what

25
00:01:22,900 --> 00:01:25,700
the good folks, or at least the
people who have the data and

26
00:01:25,766 --> 00:01:30,299
have permissions to use it,
are actually doing with it.

27
00:01:30,366 --> 00:01:32,866
Are they actually doing
things that are legal?

28
00:01:32,933 --> 00:01:34,966
Are they doing
things that are moral?

29
00:01:35,033 --> 00:01:36,533
Are they doing things
that are trustworthy?

30
00:01:36,599 --> 00:01:38,933
Are they doing
things that are ethical?

31
00:01:39,000 --> 00:01:41,099
Those are very different things.

32
00:01:41,166 --> 00:01:44,733
For many years, the privacy and
security folks were maybe more

33
00:01:44,799 --> 00:01:46,733
adversaries than friends.

34
00:01:46,799 --> 00:01:49,966
I remember when I was the chief
privacy officer many years ago

35
00:01:50,033 --> 00:01:53,966
at DoubleClick, I was
scared of the security folks.

36
00:01:54,033 --> 00:01:56,633
I was sort of a lawyer and a
policy person and they were kind

37
00:01:56,700 --> 00:01:58,833
of hardcore tech folks.

38
00:01:58,900 --> 00:02:01,733
I wanted us to not have data
because I was going to have to

39
00:02:01,799 --> 00:02:05,066
explain, you know, why we had
it, what we were doing with it,

40
00:02:05,133 --> 00:02:08,598
were we going to lose it,
and they wanted to keep logs.

41
00:02:08,666 --> 00:02:12,133
They needed information for
forensic purposes, and they were

42
00:02:12,199 --> 00:02:15,933
worried about monitoring our
employee networks for fear that

43
00:02:16,000 --> 00:02:19,966
data would be going out and I
was worried about violating laws

44
00:02:20,033 --> 00:02:23,933
in Germany or having our
employees be upset because we

45
00:02:24,000 --> 00:02:26,133
were monitoring their behavior.

46
00:02:26,199 --> 00:02:30,366
More often than not, the
interactions we had were not

47
00:02:30,433 --> 00:02:33,299
super friendly and
not super positive.

48
00:02:33,366 --> 00:02:35,766
But I think, frankly, what we
have all learned over the last

49
00:02:35,833 --> 00:02:39,300
couple of years, especially
as privacy folks have started

50
00:02:39,366 --> 00:02:42,966
working on programs to support
access to data, to support

51
00:02:43,033 --> 00:02:47,000
deletion of data, the
intersection is clear, it is

52
00:02:47,066 --> 00:02:49,400
obvious, and I think now
we're certainly talking

53
00:02:49,466 --> 00:02:50,566
about partners.

54
00:02:50,633 --> 00:02:51,766
All right.

55
00:02:51,833 --> 00:02:54,599
I promised that this talk
would be about privacy as a

56
00:02:54,666 --> 00:02:55,833
human right.

57
00:02:55,900 --> 00:02:58,366
When I mean this, I don't mean
this just in sort of a moral

58
00:02:58,433 --> 00:02:59,266
way, right?

59
00:02:59,333 --> 00:03:00,066
Oh, it is important.

60
00:03:00,133 --> 00:03:01,099
It is a human right.

61
00:03:01,166 --> 00:03:04,166
What does it mean for people
actually working with data,

62
00:03:04,233 --> 00:03:08,733
people monitoring data,
people analyzing data, people

63
00:03:08,800 --> 00:03:11,000
selling data?

64
00:03:11,066 --> 00:03:15,766
What does it mean to actually
be thinking about privacy as a

65
00:03:15,833 --> 00:03:16,833
human right?

66
00:03:16,900 --> 00:03:20,933
Tim Cook, privacy is a human
right, Adela, probably a dozen

67
00:03:21,000 --> 00:03:24,900
other CEOs have prominently
declared that privacy is a

68
00:03:24,966 --> 00:03:25,766
human right.

69
00:03:25,833 --> 00:03:27,066
Let's jump into it a little bit.

70
00:03:27,133 --> 00:03:28,933
I'm going to try to walk a
little bit through some of my

71
00:03:29,000 --> 00:03:32,733
early days because I think some
of these issues as I've grown up

72
00:03:32,800 --> 00:03:37,533
in working in privacy for 20
years have developed and I've

73
00:03:37,599 --> 00:03:40,366
had either the good or
misfortune to be connected and

74
00:03:40,433 --> 00:03:41,698
working on some of those.

75
00:03:41,766 --> 00:03:46,399
I had been a congressional
staffer, and now Senator Chuck

76
00:03:46,466 --> 00:03:52,466
Schumer and a colleague of mine
who worked with me, he moved on,

77
00:03:52,533 --> 00:03:55,299
I went and got elected to the
state legislature, and he went

78
00:03:55,366 --> 00:03:58,133
and he moved on to some
company called DoubleClick.

79
00:03:58,199 --> 00:03:59,900
I didn't know much about that.

80
00:03:59,966 --> 00:04:04,633
I knew there was this big
sign in Manhattan, and it said

81
00:04:04,699 --> 00:04:07,333
DoubleClick welcomes
you to Silicon Alley.

82
00:04:07,400 --> 00:04:09,533
This was a brilliant marketing
thing because, obviously, there

83
00:04:09,599 --> 00:04:13,698
is no alley that is Silicon
Alley and there is no one area

84
00:04:13,766 --> 00:04:16,599
that is even the Silicon Alley
community, and there certainly

85
00:04:16,666 --> 00:04:18,000
wasn't back then.

86
00:04:18,065 --> 00:04:21,500
But the marketers at DoubleClick
realized that media needed

87
00:04:21,565 --> 00:04:25,166
somewhere to photograph, and so
they put up this great big sign

88
00:04:25,233 --> 00:04:26,933
sponsored by DoubleClick.

89
00:04:27,000 --> 00:04:30,733
And so I knew, oh, DoubleClick,
something to do with dot com of

90
00:04:30,800 --> 00:04:31,800
some sort.

91
00:04:31,866 --> 00:04:34,300
I ended up becoming the Consumer
Affairs Commissioner for New

92
00:04:34,366 --> 00:04:38,433
York City working on enforcing
the consumer protection laws.

93
00:04:38,500 --> 00:04:42,866
As, I think, many of you know,
we don't have real significant,

94
00:04:42,933 --> 00:04:45,533
comprehensive privacy
law in this country. Right?

95
00:04:45,600 --> 00:04:46,899
We have consumer protection law.

96
00:04:46,966 --> 00:04:49,333
The law says - and we have
got this at a federal level.

97
00:04:49,399 --> 00:04:52,133
When I was Consumer Affairs
Commissioner, I got to enforce

98
00:04:52,199 --> 00:04:53,133
it at a city level.

99
00:04:53,199 --> 00:04:54,399
Many states have it.

100
00:04:54,466 --> 00:04:57,100
The law basically says - and
these are all very similar - you

101
00:04:57,166 --> 00:05:00,600
can't deceive people and you
can't do things that are unfair.

102
00:05:00,666 --> 00:05:04,066
If I don't tell you about what
I'm doing with your data and I

103
00:05:04,133 --> 00:05:07,698
do something that seems
unpleasant, the Attorney

104
00:05:07,766 --> 00:05:11,366
General, the Federal Trade
Commission can bring an action

105
00:05:11,433 --> 00:05:13,866
against you under this
consumer protection law.

106
00:05:13,933 --> 00:05:19,366
We've kind of levered privacy
into consumer protection law,

107
00:05:19,433 --> 00:05:21,666
but it's not a perfect fit,
because as long as I'm not

108
00:05:21,733 --> 00:05:25,933
really deceiving you, if I say
something about it and you don't

109
00:05:26,000 --> 00:05:30,066
pay attention, and it's not
really terrible, I just might

110
00:05:30,133 --> 00:05:33,133
maybe do something creepy or
market to you in some way that's

111
00:05:33,199 --> 00:05:36,466
unpleasant, you don't have a
clear right to say you don't

112
00:05:36,533 --> 00:05:39,666
have my data, you don't have the
right to have my data, I have

113
00:05:39,733 --> 00:05:41,533
the rights to take
it or delete it.

114
00:05:41,600 --> 00:05:43,466
We've got consumer
protection law.

115
00:05:43,533 --> 00:05:49,166
When I became the Chief Privacy
Officer at DoubleClick, here is

116
00:05:49,233 --> 00:05:50,100
how it happened.

117
00:05:50,166 --> 00:05:53,233
I opened up the newspaper
one day and here they are

118
00:05:53,300 --> 00:05:54,300
in trouble.

119
00:05:54,366 --> 00:05:55,699
Evidently, they're doing
something with cookies.

120
00:05:55,766 --> 00:05:57,600
They're tracking
people's web surfing.

121
00:05:57,666 --> 00:06:00,000
That's not what the
big story was about.

122
00:06:00,066 --> 00:06:02,799
The big story was that they had
bought a data company and they

123
00:06:02,866 --> 00:06:06,233
were going to link this offline
data that they had about people

124
00:06:06,300 --> 00:06:10,166
to your online web browsing so
DoubleClick would know who

125
00:06:10,233 --> 00:06:11,233
you are.

126
00:06:11,300 --> 00:06:14,199
People were shocked because
people assumed you went online,

127
00:06:14,266 --> 00:06:16,699
you didn't give anybody your
name, whatever you searched,

128
00:06:16,766 --> 00:06:20,266
whatever sites you visited, you
were anonymous, and every site

129
00:06:20,333 --> 00:06:23,233
promised you we don't collect
personal information, you are

130
00:06:23,300 --> 00:06:26,166
anonymous, and so we all
felt quite comfortable.

131
00:06:26,233 --> 00:06:29,500
All of a sudden, here is this
company saying that, no, it's

132
00:06:29,566 --> 00:06:34,799
going to actually know by name
who we are as we move around on

133
00:06:34,866 --> 00:06:37,233
the web, and not only that,
they're going to link other

134
00:06:37,300 --> 00:06:43,766
offline data about me that this
co-op of catalogue data and

135
00:06:43,833 --> 00:06:46,766
companies will be able to
send you ads based on that

136
00:06:46,833 --> 00:06:48,800
offline data.

137
00:06:48,866 --> 00:06:51,166
This was shocking
and surprising.

138
00:06:51,233 --> 00:06:54,366
The Federal Trade Commission
opened up an investigation.

139
00:06:54,433 --> 00:06:59,233
Seventeen attorneys general
opened up investigations.

140
00:06:59,300 --> 00:07:01,699
Class action
lawyers filed lawsuits.

141
00:07:01,766 --> 00:07:03,833
Again, all under this
consumer protection law.

142
00:07:03,899 --> 00:07:07,466
And so I called up my pal,
former Schumer staffer colleague

143
00:07:07,533 --> 00:07:10,366
of mine who was working now with
DoubleClick, and I said, you

144
00:07:10,433 --> 00:07:13,533
know, you got out of Schumer's
office, which is a tough place

145
00:07:13,600 --> 00:07:16,766
to work, he's very hard-driven,
the staff really work very hard,

146
00:07:16,833 --> 00:07:19,433
and you've really got to be
on all the time, it's a great

147
00:07:19,500 --> 00:07:21,933
learning experience, but you're
working and you're learning, and

148
00:07:22,000 --> 00:07:25,733
here you are on the front page
of the New York Times defending

149
00:07:25,800 --> 00:07:26,766
all this tracking.

150
00:07:26,833 --> 00:07:28,133
What are you all
going to do about it?

151
00:07:28,199 --> 00:07:30,733
Well, we're going to hire
a chief privacy officer.

152
00:07:30,800 --> 00:07:33,199
Oh, that sounds
kind of interesting.

153
00:07:33,266 --> 00:07:35,033
What is that all about?

154
00:07:35,100 --> 00:07:38,766
When I became the first - one
of the earlier chief privacy

155
00:07:38,833 --> 00:07:42,500
officers, I share a few here
some of the nice headlines

156
00:07:42,566 --> 00:07:46,166
about, oh, here's this new role
and it's going to help people

157
00:07:46,233 --> 00:07:48,766
online and protect them.

158
00:07:48,833 --> 00:07:53,233
I didn't share some of the other
headlines that said things like

159
00:07:53,300 --> 00:07:56,199
what a weird dot
com kind of thing.

160
00:07:56,266 --> 00:07:58,766
They're calling some
employees ninjas.

161
00:07:58,833 --> 00:08:03,833
They're calling others
yahoos, privacy officers.

162
00:08:03,899 --> 00:08:07,333
This is really going to go
the way the rest of the dot com

163
00:08:07,399 --> 00:08:09,033
bust goes.

164
00:08:09,100 --> 00:08:12,866
But, of course, today we've got
hundreds of thousands of folks

165
00:08:12,933 --> 00:08:14,633
in this role.

166
00:08:14,699 --> 00:08:17,066
I show up the first day and
I don't know all that much.

167
00:08:17,133 --> 00:08:21,433
I have some clue that cookies
have something to do with data

168
00:08:21,500 --> 00:08:24,566
and it might be on people's
computers, but I hadn't yet sat

169
00:08:24,633 --> 00:08:26,066
down and gotten any briefing.

170
00:08:26,133 --> 00:08:27,099
I was a lawyer.

171
00:08:27,166 --> 00:08:28,832
We had AOL accounts.

172
00:08:28,899 --> 00:08:33,566
This wasn't the day of people
having email ubiquitously yet in

173
00:08:33,633 --> 00:08:34,566
their offices.

174
00:08:36,200 --> 00:08:42,133
I get a call from the Washington
Post and they say big problem.

175
00:08:42,200 --> 00:08:46,700
A technical expert told us that
you've hidden web bugs on the

176
00:08:46,766 --> 00:08:50,899
White House website and you're
tracking kids who are coming to

177
00:08:50,966 --> 00:08:55,033
learn about how to stay away
from drugs or more information

178
00:08:55,100 --> 00:08:56,266
about drugs.

179
00:08:56,333 --> 00:09:01,366
You're tracking kids
with drug abuse problems.

180
00:09:01,433 --> 00:09:04,000
What do you have
to say about this?

181
00:09:04,066 --> 00:09:07,066
I'm like that sounds
kind of weird and strange.

182
00:09:07,133 --> 00:09:08,066
How could that be?

183
00:09:08,133 --> 00:09:09,333
I've signed up to
work at this company.

184
00:09:09,399 --> 00:09:10,433
I think they do advertising.

185
00:09:10,500 --> 00:09:15,433
I don't know why we would be
putting secret code on the White

186
00:09:15,500 --> 00:09:17,766
House site and
tracking kids and drugs.

187
00:09:17,833 --> 00:09:22,065
Of course, what actually
happened was the White House

188
00:09:22,133 --> 00:09:25,700
decided, there is a drug czar
typically in Washington, the

189
00:09:25,766 --> 00:09:29,366
Office of National Drug Control
Policy, the drug czar, and they

190
00:09:29,433 --> 00:09:31,033
decided they would do
this cool new thing.

191
00:09:31,100 --> 00:09:33,033
They'd advertise on the internet
because that's where the kids

192
00:09:33,100 --> 00:09:38,066
were evidently, and they'd build
a website to teach kids to stay

193
00:09:38,133 --> 00:09:39,366
away from drugs.

194
00:09:39,433 --> 00:09:41,899
I don't know who thought this
would work, but they built this

195
00:09:41,966 --> 00:09:45,733
website and they built it on a
White House domain because it

196
00:09:45,799 --> 00:09:47,500
was an official
White House property.

197
00:09:47,566 --> 00:09:49,000
But the ad agency built it.

198
00:09:49,066 --> 00:09:51,399
And then, of course, they went
off and they bought ads on

199
00:09:51,466 --> 00:09:58,233
AltaVista and Lycos and AOL, all
the big cites of yesteryear, and

200
00:09:58,299 --> 00:10:02,733
the agency folks said, listen,
here's what our best clients do.

201
00:10:02,799 --> 00:10:03,699
You're the government.

202
00:10:03,766 --> 00:10:06,533
You want to spend taxpayer
dollars wisely, don't you?

203
00:10:06,600 --> 00:10:10,633
You want to know whether the
AOL ads are driving responses.

204
00:10:10,700 --> 00:10:13,333
Or is it the Lycos ads that
are getting the response?

205
00:10:13,399 --> 00:10:16,600
Where are you paying and
getting the best ROI?

206
00:10:16,666 --> 00:10:19,600
We'll put some code that will
help us give you reports.

207
00:10:19,666 --> 00:10:20,766
Okay, White House?

208
00:10:20,833 --> 00:10:22,165
Oh, sounds pretty good.

209
00:10:22,233 --> 00:10:26,299
Of course, a one by one tracking
pixel is put on this marketing

210
00:10:26,366 --> 00:10:30,133
site that Ogilvy won, the ad
agency, built just like it did

211
00:10:30,200 --> 00:10:32,700
for everybody else, but
it's a White House site.

212
00:10:32,766 --> 00:10:37,266
And some technical expert spots
the tracking pixel and there I

213
00:10:37,333 --> 00:10:38,833
get the call.

214
00:10:38,899 --> 00:10:43,566
Before I had time to respond,
the report said we just got a

215
00:10:43,633 --> 00:10:45,066
call from the White House
and the White House says

216
00:10:45,133 --> 00:10:46,100
that's terrible.

217
00:10:46,166 --> 00:10:48,733
They didn't know about it and
they're banning cookies on

218
00:10:48,799 --> 00:10:50,500
government websites.

219
00:10:50,566 --> 00:10:53,466
I said, okay, I think that's
probably going to create some

220
00:10:53,533 --> 00:10:55,466
technical problems.

221
00:10:55,533 --> 00:10:58,266
Half an hour, they call back - I
hadn't yet figured out what we

222
00:10:58,333 --> 00:11:00,500
were going to say - and they
said they just updated it.

223
00:11:00,566 --> 00:11:02,433
They haven't heard from
some of the webmasters.

224
00:11:02,500 --> 00:11:05,866
They're actually going to only
block - they're going to have

225
00:11:05,933 --> 00:11:08,866
policy no third-party cookies.

226
00:11:08,933 --> 00:11:12,366
And then literally about a half
an hour goes by and we've got to

227
00:11:12,433 --> 00:11:14,600
update that because it turns out
they even need some of these

228
00:11:14,666 --> 00:11:17,200
third-party cookies, but you're
going to have to get permission

229
00:11:17,266 --> 00:11:19,466
of the agency head.

230
00:11:19,533 --> 00:11:22,100
I thought, oh, what a
good policy process.

231
00:11:22,166 --> 00:11:23,766
It happened very quickly.

232
00:11:23,833 --> 00:11:26,399
They'll still get to do it if
they need to, but it's going to

233
00:11:26,466 --> 00:11:28,666
actually have to go
through some level of review.

234
00:11:28,733 --> 00:11:30,699
But here is the mistake I made.

235
00:11:30,766 --> 00:11:33,766
I had worked in Congress
and I had worked as a

236
00:11:33,833 --> 00:11:34,866
state legislator.

237
00:11:34,933 --> 00:11:37,733
I had never worked in the
major federal bureaucracies.

238
00:11:37,799 --> 00:11:40,366
And so here is how this
played out for many years.

239
00:11:40,433 --> 00:11:45,533
Some webmaster is working on
some new update to the web

240
00:11:45,600 --> 00:11:47,533
properties and decides
he wants some analytics.

241
00:11:47,600 --> 00:11:51,733
He somehow realizes - she
somehow realizes, oh, there's

242
00:11:51,799 --> 00:11:52,799
a policy here.

243
00:11:52,866 --> 00:11:56,133
We've got to get approval of
the secretary of the army, the

244
00:11:56,200 --> 00:11:58,833
secretary of energy to do this.

245
00:11:58,899 --> 00:12:02,533
She goes to her boss and says
when is our next meeting.

246
00:12:02,600 --> 00:12:05,566
He says, well, in three months,
I meet with my boss who will

247
00:12:05,633 --> 00:12:08,433
meet with the deputy and we're
going to try to get our budget

248
00:12:08,500 --> 00:12:10,600
and two major
priorities approved.

249
00:12:10,666 --> 00:12:14,100
Could you actually get on that
agenda something about cookies

250
00:12:14,166 --> 00:12:15,700
and tracking?

251
00:12:15,766 --> 00:12:19,033
No. That's not going to happen
because that's what we'll be

252
00:12:19,100 --> 00:12:21,399
talking about and we won't
get our budget approved.

253
00:12:21,466 --> 00:12:23,166
Don't do it.

254
00:12:23,233 --> 00:12:24,933
Or do what you want
and don't tell me.

255
00:12:25,000 --> 00:12:28,000
For years, this was the policy,
until it was updated during the

256
00:12:28,066 --> 00:12:29,100
Obama Administration.

257
00:12:29,166 --> 00:12:32,466
You technically needed the head
of the agency or their senior

258
00:12:32,533 --> 00:12:33,866
report to approve it.

259
00:12:33,933 --> 00:12:37,165
Nobody did and there would be
investigations by the GAO, the

260
00:12:37,233 --> 00:12:39,933
different auditors,
criticizing the agencies.

261
00:12:40,000 --> 00:12:41,500
I'm putting this out there
because I want you to start

262
00:12:41,566 --> 00:12:44,433
thinking about the way the
government and the regulator

263
00:12:44,500 --> 00:12:48,433
policy process works and sets
rules that govern how we use

264
00:12:48,500 --> 00:12:52,766
technology and some of the
rockiness and the nuances and

265
00:12:52,833 --> 00:12:55,366
how we're going to have to do it
better if we actually want to

266
00:12:55,433 --> 00:12:56,966
succeed in the world forward.

267
00:12:57,033 --> 00:12:59,833
Another couple of weeks go by
and this time it's the Wall

268
00:12:59,899 --> 00:13:04,200
Street Journal that calls and
they say we just discovered that

269
00:13:04,266 --> 00:13:09,333
DoubleClick is running a Jack
Daniel's ad on Snoopy.com.

270
00:13:11,333 --> 00:13:12,500
I said okay.

271
00:13:12,566 --> 00:13:15,766
They're like kids, Snoopy.com,
you're running an alcohol ad on

272
00:13:15,833 --> 00:13:16,500
a kids' site.

273
00:13:16,566 --> 00:13:17,333
That's pretty bad.

274
00:13:17,399 --> 00:13:18,399
You're in big trouble.

275
00:13:18,466 --> 00:13:19,233
What do you have to say?

276
00:13:19,299 --> 00:13:21,199
Can I get back to you, please?

277
00:13:21,266 --> 00:13:23,699
I call up the team that is
handling that property for us

278
00:13:23,766 --> 00:13:25,766
and I'm like, guys,
how is it possible?

279
00:13:25,833 --> 00:13:26,866
We have a policy against this.

280
00:13:26,933 --> 00:13:28,533
No alcohol on kids' sites.

281
00:13:28,600 --> 00:13:29,366
Come on.

282
00:13:29,433 --> 00:13:31,500
They said what kids' site?

283
00:13:31,566 --> 00:13:32,266
It's Snoopy.

284
00:13:32,333 --> 00:13:33,333
I'm like, yeah.

285
00:13:33,399 --> 00:13:37,333
They're like 89 - we have the
metrics - 89%, 90% of the people

286
00:13:37,399 --> 00:13:38,333
go to Snoopy.

287
00:13:38,399 --> 00:13:39,033
Kids don't go to Snoopy.

288
00:13:39,100 --> 00:13:40,333
It's like an
old school thing. Right?

289
00:13:40,399 --> 00:13:42,166
This is people who grew
up reading the comics.

290
00:13:42,233 --> 00:13:43,233
Kids are not.

291
00:13:43,299 --> 00:13:46,100
I'm like, okay, so I'm going
to call this reporter back and

292
00:13:46,166 --> 00:13:48,600
somehow say it's all good.

293
00:13:48,666 --> 00:13:52,066
Alcohol on Snoopy, it's okay
because it's not really kids.

294
00:13:52,133 --> 00:13:53,600
I actually have the numbers.

295
00:13:53,666 --> 00:13:55,466
No, they're going to kill me.

296
00:13:55,533 --> 00:13:56,366
We're going to look terrible.

297
00:13:56,433 --> 00:13:59,665
I know it's okay because
the numbers say it's okay.

298
00:13:59,733 --> 00:14:01,032
But you know what?

299
00:14:01,100 --> 00:14:05,133
The best thing I can do is
surrender quick and this will be

300
00:14:05,200 --> 00:14:06,100
a one-day story.

301
00:14:06,166 --> 00:14:08,233
I say, whoa, that was terrible.

302
00:14:08,299 --> 00:14:11,132
It was a mistake and we've put
new measures in place, and we

303
00:14:11,200 --> 00:14:13,566
will not allow that sort
of thing to happen again.

304
00:14:13,633 --> 00:14:17,100
And so here you have the
beginnings of the development of

305
00:14:17,166 --> 00:14:20,633
the sophisticated
policy processes that govern

306
00:14:20,700 --> 00:14:21,700
what happens.

307
00:14:21,766 --> 00:14:23,632
Okay.

308
00:14:23,700 --> 00:14:27,633
Today there are, because of
the GDPR, the European Data

309
00:14:27,700 --> 00:14:31,700
Protection Regulation, which
requires that many, many

310
00:14:31,766 --> 00:14:35,899
companies by law have a
data protection officer, an

311
00:14:35,966 --> 00:14:39,866
independent DPO who has to
report in at the highest level

312
00:14:39,933 --> 00:14:42,133
of management and
cannot easily be fired.

313
00:14:42,200 --> 00:14:45,200
Those of you who want real
serious guaranteed legislative

314
00:14:45,266 --> 00:14:49,000
job protection, there are about
500,000 of these individuals

315
00:14:49,066 --> 00:14:50,066
that have been registered.

316
00:14:50,133 --> 00:14:51,200
Some of them can do the role.

317
00:14:51,266 --> 00:14:53,433
You can have outside
consultants do it.

318
00:14:53,500 --> 00:14:56,333
Some of them may be doing
it for multiple companies.

319
00:14:56,399 --> 00:14:58,933
But 500,000 registered
with the EU regulators.

320
00:14:59,000 --> 00:15:02,000
The IEPP, the organization that
many of us that work in privacy

321
00:15:02,066 --> 00:15:05,899
belong to, has 50,000
and is rapidly growing.

322
00:15:05,966 --> 00:15:08,366
And many of the leading
legislative proposals at the

323
00:15:08,433 --> 00:15:12,500
state level, at the federal
level are saying, oh, if that's

324
00:15:12,566 --> 00:15:15,666
something useful, maybe
we should regulate it.

325
00:15:15,733 --> 00:15:20,899
And so you're starting to see
mandatory privacy and/or even

326
00:15:20,966 --> 00:15:25,233
security designees
showing up in the law.

327
00:15:25,299 --> 00:15:30,899
Nothing like a career that is
guaranteed by legislation.

328
00:15:30,966 --> 00:15:35,500
As we see these new privacy laws
starting to pop-up all across

329
00:15:35,566 --> 00:15:39,600
the U.S., many of them, almost
as an afterthought, say, oh, we

330
00:15:39,666 --> 00:15:41,733
ought to add something about
security in there, and so then

331
00:15:41,799 --> 00:15:46,132
there are two or three lines
adding promises to have

332
00:15:46,200 --> 00:15:49,733
reasonable security or
creating liability if there are

333
00:15:49,799 --> 00:15:51,399
data breaches.

334
00:15:51,466 --> 00:15:55,833
We're in a really interesting
transitional period today.

335
00:15:57,866 --> 00:16:01,033
Cookies, for me, kicked
off my sort of awareness.

336
00:16:01,100 --> 00:16:03,766
Let me just walk through a
little bit more before we move

337
00:16:03,833 --> 00:16:07,233
on to some of the other topics,
how that has played out given

338
00:16:07,299 --> 00:16:10,665
the way that we all have
handled the responses.

339
00:16:10,733 --> 00:16:13,965
So you see that triangle eye?

340
00:16:14,033 --> 00:16:16,799
This is a little more
sophisticated of an audience

341
00:16:16,866 --> 00:16:19,199
than perhaps the
general one or others.

342
00:16:19,266 --> 00:16:21,632
Does anyone know
- raise your hand.

343
00:16:21,700 --> 00:16:25,733
Do you know what that triangle
eye means when you see it?

344
00:16:25,799 --> 00:16:26,566
Not that many.

345
00:16:26,633 --> 00:16:27,566
Hardly any.

346
00:16:27,633 --> 00:16:28,633
Has anyone seen it?

347
00:16:28,700 --> 00:16:32,266
You have all seen this every
single day that you have been on

348
00:16:32,333 --> 00:16:36,533
the internet because this symbol
is on almost every banner ad

349
00:16:36,600 --> 00:16:39,633
that you see because it is the
symbol that the industry years

350
00:16:39,700 --> 00:16:43,100
ago decided let's tell people
about all of these cookies and

351
00:16:43,166 --> 00:16:46,766
that the ads are tracking them
and retargeting them because

352
00:16:46,833 --> 00:16:49,165
people are saying it is
creepy or it's invisible or it's

353
00:16:49,233 --> 00:16:50,233
not fair.

354
00:16:50,299 --> 00:16:53,199
And we have been talking about
it in privacy policies, but

355
00:16:53,266 --> 00:16:56,266
since nobody is reading those,
let's actually put it right out

356
00:16:56,333 --> 00:16:57,533
there on the page.

357
00:16:57,600 --> 00:17:01,333
It is actually on the
corner of, well, just about

358
00:17:01,399 --> 00:17:02,533
every banner ad.

359
00:17:02,600 --> 00:17:05,833
Now, I didn't realize you could
click only on the corner of

360
00:17:05,900 --> 00:17:06,733
an ad.

361
00:17:06,799 --> 00:17:07,833
I assumed I click
on an ad, I'm going to go to

362
00:17:07,900 --> 00:17:08,933
the advertiser's site.

363
00:17:09,000 --> 00:17:11,833
But if you somehow manage to
click on the corner where you

364
00:17:11,900 --> 00:17:15,299
see that Ad Choices symbol, you
will end up going to a page that

365
00:17:15,366 --> 00:17:18,366
tells you all about tracking
and behavioral advertising.

366
00:17:18,433 --> 00:17:20,700
It will tell you that you can
take an opt-out cookie if you

367
00:17:20,766 --> 00:17:21,799
don't want this.

368
00:17:21,866 --> 00:17:23,833
Of course, the more technical
people in this room are saying,

369
00:17:23,900 --> 00:17:26,133
yeah, but I clear my cookies all
the time, so what's that opt-out

370
00:17:26,200 --> 00:17:27,700
cookie going to do for me.

371
00:17:27,766 --> 00:17:29,733
Maybe that doesn't work for you.

372
00:17:29,799 --> 00:17:32,666
Clearly, we didn't
change the narrative.

373
00:17:32,733 --> 00:17:36,765
And as a result, in two years
from a couple of weeks ago,

374
00:17:36,833 --> 00:17:39,766
Chrome is going to be blocking
all third-party cookies, all the

375
00:17:39,833 --> 00:17:43,400
other major browsers do, because
the consensus has been, oh my

376
00:17:43,466 --> 00:17:46,199
God, whatever is happening
there behind the scenes is too

377
00:17:46,266 --> 00:17:49,700
aggressive, is too risky, is
too privacy intrusive, all

378
00:17:49,766 --> 00:17:51,233
third-party Cookies.

379
00:17:51,299 --> 00:17:54,233
You want the analytics, you want
your ad attribution, you want

380
00:17:54,299 --> 00:17:57,433
the things that are perhaps less
privacy invasive than trading

381
00:17:57,500 --> 00:17:58,966
and selling your information.

382
00:17:59,033 --> 00:18:02,565
Nobody made the case to
consumers and the browsers

383
00:18:02,633 --> 00:18:05,166
responding and
reacting end up blocking.

384
00:18:05,233 --> 00:18:06,299
But look, I'm guilty, too.

385
00:18:06,366 --> 00:18:07,333
I tried.

386
00:18:07,400 --> 00:18:09,566
When I was the chief privacy
officer at AOL, I said I want to

387
00:18:09,633 --> 00:18:10,566
tell people about this.

388
00:18:10,633 --> 00:18:11,900
We're being criticized
that we're doing some

389
00:18:11,966 --> 00:18:12,933
targeted advertising.

390
00:18:13,000 --> 00:18:16,433
I want to tell people so they
understand that we're just

391
00:18:16,500 --> 00:18:18,766
trying to give them a
better online experience.

392
00:18:18,833 --> 00:18:21,933
We created a penguin - I can't
recall why we thought a penguin

393
00:18:22,000 --> 00:18:27,266
was a good privacy icon - and
we created some videos and some

394
00:18:27,333 --> 00:18:28,900
graphic comics.

395
00:18:28,966 --> 00:18:31,332
The penguin, he was worried
about climate change even

396
00:18:31,400 --> 00:18:32,166
back then.

397
00:18:32,233 --> 00:18:33,700
He was reading
about climate change.

398
00:18:33,766 --> 00:18:36,966
And then as he'd go from site
to site, he'd start getting ads

399
00:18:37,033 --> 00:18:40,265
about anchovies and other things
that we presume penguins like.

400
00:18:40,333 --> 00:18:44,400
You could see his ad profile
being updated linked to

401
00:18:44,466 --> 00:18:45,433
his cookie.

402
00:18:45,500 --> 00:18:49,599
But we clearly didn't succeed
in changing the narrative.

403
00:18:50,333 --> 00:18:51,900
And then do not
track came along.

404
00:18:51,966 --> 00:18:58,033
We had an opportunity to
negotiate with advocates and

405
00:18:58,099 --> 00:19:01,832
academics and civil society
and come up with some sort of

406
00:19:01,900 --> 00:19:05,066
consensus where people simply
with their browser could

407
00:19:05,133 --> 00:19:06,966
indicate I don't
want to be tracked.

408
00:19:07,033 --> 00:19:09,199
Nobody could agree
what that really meant.

409
00:19:09,266 --> 00:19:11,700
Did that mean I could still do a
little tracking so I can still

410
00:19:11,766 --> 00:19:15,666
have that analytics and that ad
reporting, or did it mean, no,

411
00:19:15,733 --> 00:19:18,332
you have nothing, or does it
mean I give you advertising,

412
00:19:18,400 --> 00:19:20,266
it's just not very targeted.

413
00:19:20,333 --> 00:19:23,666
And so, unfortunately,
the effort failed.

414
00:19:23,733 --> 00:19:26,866
The browsers, many of them,
started pulling the do not track

415
00:19:26,933 --> 00:19:29,433
signal out, because if people
are using it and it does

416
00:19:29,500 --> 00:19:31,566
nothing, that's not a good deal.

417
00:19:31,633 --> 00:19:36,733
Here we are at this place where
we have legislators trying to

418
00:19:36,799 --> 00:19:40,700
fix this problem of too much
advertising and data going

419
00:19:40,766 --> 00:19:41,866
too far.

420
00:19:41,933 --> 00:19:45,433
The browser is using sort of
blunt technical efforts to try

421
00:19:45,500 --> 00:19:48,900
to control it and everybody else
sort of swirling their arms.

422
00:19:49,766 --> 00:19:52,500
Can we do better?
Can we do better?

423
00:19:52,566 --> 00:19:56,366
Because whether there is cookies
or ads, advertisers will spend

424
00:19:56,433 --> 00:20:00,099
money online, life will move on,
maybe we'll pay for some more

425
00:20:00,166 --> 00:20:01,966
things, that's not terrible
if some of their newspapers

426
00:20:02,033 --> 00:20:04,533
actually get a couple of bucks
from us so they can hire and

427
00:20:04,599 --> 00:20:05,466
pay reporters.

428
00:20:05,533 --> 00:20:07,399
I think it will work
out one way or another.

429
00:20:07,466 --> 00:20:11,265
I'm not quite sure that in all
these other areas, microphones

430
00:20:11,333 --> 00:20:14,799
in our home where every day we
read stories about is my smart

431
00:20:14,866 --> 00:20:18,166
TV spying on me, is my smart
home speaker spying on me, is

432
00:20:18,233 --> 00:20:22,099
Siri spying on me, drones,
facial recognition, where we're

433
00:20:22,166 --> 00:20:25,966
seeing localities banned because
of concerns that don't have the

434
00:20:26,033 --> 00:20:28,966
right rules in place that can
make sure that the technology is

435
00:20:29,033 --> 00:20:30,500
used in a reasonable way.

436
00:20:30,566 --> 00:20:35,133
Smart cities, cars, location,
genetics, brain computer, almost

437
00:20:35,200 --> 00:20:38,000
every area of our life,
we're going to be having these

438
00:20:38,066 --> 00:20:39,166
same debates.

439
00:20:39,233 --> 00:20:43,666
If we mess up the way we messed
up the debate and conversation

440
00:20:43,733 --> 00:20:46,133
around cookies,
it's going to be worse.

441
00:20:46,833 --> 00:20:51,033
We recently put out a paper
where we tried to identify ten

442
00:20:51,099 --> 00:20:53,733
of the technologies that we
thought were really going to

443
00:20:53,799 --> 00:20:57,933
create the privacy challenges
and stresses of next year.

444
00:20:58,000 --> 00:21:02,000
Many of you probably only dealt
with biometrics at a company if

445
00:21:02,066 --> 00:21:06,133
you really worked for a company
that was doing something unique

446
00:21:06,200 --> 00:21:07,066
or different.

447
00:21:07,133 --> 00:21:10,099
But today, almost every
organization in some way,

448
00:21:10,166 --> 00:21:13,633
whether it's scanning employees'
fingerprints for locking in and

449
00:21:13,700 --> 00:21:18,766
out, you name it, there is
biometrics of some sort.

450
00:21:18,833 --> 00:21:20,700
I'll tell you the truth.

451
00:21:20,766 --> 00:21:25,366
Who said that the only way we
interact with the internet is

452
00:21:25,433 --> 00:21:27,733
typing on a keyboard?

453
00:21:27,799 --> 00:21:28,933
Not everybody can
type on a keyboard.

454
00:21:29,000 --> 00:21:30,966
Not every device has a keyboard.

455
00:21:31,033 --> 00:21:33,765
And who said it's poking
on sort of a little screen with

456
00:21:33,833 --> 00:21:34,833
our finger?

457
00:21:34,900 --> 00:21:45,366
Voice, gait, gesture, sound,
brain-computer interfaces are

458
00:21:45,433 --> 00:21:49,599
starting to enter the market,
starting to enter schools.

459
00:21:49,666 --> 00:21:52,466
Schools have been trying to
figure out how do we teach kids.

460
00:21:52,533 --> 00:21:57,399
One leading education focus
says kids have to come ready

461
00:21:57,466 --> 00:21:58,500
to learn.

462
00:21:58,566 --> 00:22:00,566
It doesn't matter how good the
teacher is if a kid comes in and

463
00:22:00,633 --> 00:22:05,333
she's aggravated or not focused,
so kids have to be in a learning

464
00:22:05,400 --> 00:22:07,299
mindset, in a growth mindset.

465
00:22:07,366 --> 00:22:11,000
Oh, so can we give them a little
bit of a band to wear to test to

466
00:22:11,066 --> 00:22:13,433
see whether a little
bit of meditation helps.

467
00:22:13,500 --> 00:22:17,033
There are apps in the App Store
that come with $100 banner that

468
00:22:17,099 --> 00:22:20,933
can already help you control
things with your brain waves.

469
00:22:21,000 --> 00:22:22,066
Some of that is super exciting.

470
00:22:22,133 --> 00:22:23,166
Right?

471
00:22:23,233 --> 00:22:24,799
If I've got a disability, I can
control my wheelchair, I can

472
00:22:24,866 --> 00:22:26,066
control a cursor.

473
00:22:26,133 --> 00:22:29,900
Gaming, but this is starting to
enter into every other part of

474
00:22:29,966 --> 00:22:30,832
our life.

475
00:22:30,900 --> 00:22:32,833
Facebook already owns a
company that is a leader in

476
00:22:32,900 --> 00:22:34,966
brain-computer interfaces.

477
00:22:35,033 --> 00:22:39,033
The challenge that we're going
to have when these new user

478
00:22:39,099 --> 00:22:42,200
interfaces, which are important
and valuable, but each a

479
00:22:42,266 --> 00:22:45,066
sensitive biometric, is going to
make some of these debates that

480
00:22:45,133 --> 00:22:47,333
we've been having
until now silly.

481
00:22:47,933 --> 00:22:49,200
Real-world evidence.

482
00:22:49,266 --> 00:22:51,933
Today we go to the doctor, the
doctor knows what the doctor

483
00:22:52,000 --> 00:22:55,466
knows, the doctor says, are you
following the diet I told you?

484
00:22:55,533 --> 00:22:58,699
There is information out there
about how much you really are

485
00:22:58,766 --> 00:23:00,766
exercising and
what you're eating.

486
00:23:00,833 --> 00:23:05,700
And today, if I'm a pharma
company and I get a report that

487
00:23:05,766 --> 00:23:09,066
this drug caused an adverse
event because I go to my doctor

488
00:23:09,133 --> 00:23:12,099
and it gets reported up the
chain, then we learn that there

489
00:23:12,166 --> 00:23:13,399
are some bad side effects.

490
00:23:13,466 --> 00:23:15,632
But I might be out there yelling
about it on Facebook, saying

491
00:23:15,700 --> 00:23:18,400
every time I take this, I throw
up, or my cousin really got sick

492
00:23:18,466 --> 00:23:19,299
when this happened.

493
00:23:19,366 --> 00:23:20,833
There is a huge amount
of information out there.

494
00:23:20,900 --> 00:23:23,733
And so the people who want
real-world evidence, as they

495
00:23:23,799 --> 00:23:27,633
call it, to actually understand
better how we improve patient

496
00:23:27,700 --> 00:23:31,500
outcomes or how we do research,
but this means a lot of the data

497
00:23:31,566 --> 00:23:34,333
that is not covered by the
traditional health privacy laws.

498
00:23:34,400 --> 00:23:37,566
The HIPAA health privacy law
covers data that's collected

499
00:23:37,633 --> 00:23:40,566
when you are intersecting with
your medical provider, with

500
00:23:40,633 --> 00:23:42,400
medical care that's being
provided, when insurance is

501
00:23:42,466 --> 00:23:43,500
being paid.

502
00:23:43,566 --> 00:23:47,366
But my social media data pulled
in and studied and analyzed,

503
00:23:47,433 --> 00:23:52,500
social reputations, social
credit, social scoring.

504
00:23:52,566 --> 00:23:57,233
Our bodies themselves are
becoming part of the internet.

505
00:23:57,299 --> 00:23:58,066
Right?

506
00:23:58,133 --> 00:23:59,366
We're wearing smart watches.

507
00:23:59,433 --> 00:24:04,200
We're wearing devices that
have IP addresses, that have

508
00:24:04,266 --> 00:24:06,033
Mac addresses.

509
00:24:06,099 --> 00:24:10,000
It's a challenging, challenging
time and I don't think we've

510
00:24:10,066 --> 00:24:12,000
seen anything yet.

511
00:24:12,066 --> 00:24:15,133
We're going to have to
figure out how to manage these

512
00:24:15,200 --> 00:24:16,466
social norms.

513
00:24:16,533 --> 00:24:18,533
At the end of the day,
it is a social norm.

514
00:24:18,599 --> 00:24:22,533
Most of us figure out how, when
something new comes on, how

515
00:24:22,599 --> 00:24:23,566
to behave.

516
00:24:23,633 --> 00:24:25,633
You learn that you
don't type in all caps.

517
00:24:25,700 --> 00:24:26,866
That means you're
yelling, right?

518
00:24:26,933 --> 00:24:29,566
There is always somebody in the
family who just doesn't seem to

519
00:24:29,633 --> 00:24:35,866
get it, right, doesn't seem to
understand the norms, and we can

520
00:24:35,933 --> 00:24:36,966
make fun of some of them.

521
00:24:37,033 --> 00:24:40,733
My daughter thinks I'm
a goofus on Instagram.

522
00:24:40,799 --> 00:24:44,099
Even though I tell her I'm a
huge influencer on LinkedIn, she

523
00:24:44,166 --> 00:24:45,166
says that doesn't matter.

524
00:24:45,233 --> 00:24:46,566
You don't know how
to use Instagram.

525
00:24:46,633 --> 00:24:48,266
You're posting like
all these wordy things.

526
00:24:48,333 --> 00:24:51,466
I'm embarrassed of you.

527
00:24:51,533 --> 00:24:54,399
You don't get the norm here.

528
00:24:54,466 --> 00:24:58,632
Every time there is a new wave
of technology, we get disrupted.

529
00:24:58,700 --> 00:25:01,966
When cameras, when portable
cameras, you used to have to go

530
00:25:02,033 --> 00:25:03,466
and sit and have a portrait.

531
00:25:03,533 --> 00:25:05,599
So cameras were
not a risky thing.

532
00:25:05,666 --> 00:25:07,966
You went, you sat, it
was a very formal thing.

533
00:25:08,033 --> 00:25:09,033
You put on your best.

534
00:25:09,099 --> 00:25:12,599
And then when the portable
camera, when the Brownies became

535
00:25:12,666 --> 00:25:15,399
available, and people could just
take a picture of you in the

536
00:25:15,466 --> 00:25:17,233
street, it was shocking.

537
00:25:17,299 --> 00:25:20,833
There were efforts to ban these
terribly intrusive things.

538
00:25:20,900 --> 00:25:23,466
And now you're in the locker
room at your gym and there's

539
00:25:23,533 --> 00:25:26,099
like people in the hot tub there
with this HD thing that can

540
00:25:26,166 --> 00:25:29,733
stream every naked thing
that's there onto the internet.

541
00:25:29,799 --> 00:25:30,466
Right?

542
00:25:30,533 --> 00:25:31,399
We've kind of adjusted.

543
00:25:31,466 --> 00:25:33,132
We're not like,
oh my god. Right?

544
00:25:33,200 --> 00:25:35,566
Like, okay, they've got the
phone, but they're not going to

545
00:25:35,633 --> 00:25:38,799
use it even though it's
got that camera there.

546
00:25:40,099 --> 00:25:42,700
You remember when the Google
glasses came out and everybody

547
00:25:42,766 --> 00:25:45,133
was mocking and critiquing.

548
00:25:45,200 --> 00:25:46,766
I don't know, in San Francisco,
maybe it was a little bit more

549
00:25:46,833 --> 00:25:47,833
accepted around here.

550
00:25:47,900 --> 00:25:49,733
But the rest of us around
the world were like who would

551
00:25:49,799 --> 00:25:50,533
wear that?

552
00:25:50,599 --> 00:25:52,265
What is that? Right?

553
00:25:52,333 --> 00:25:57,466
What seemed to be most annoying
to people or most intrusive was

554
00:25:57,533 --> 00:26:00,099
the fact that they had this
camera and maybe somebody would

555
00:26:00,166 --> 00:26:03,966
take a picture of you or stream
a video and you didn't know

556
00:26:04,033 --> 00:26:04,966
about it. Right?

557
00:26:05,033 --> 00:26:06,265
They were not all that upset
that maybe you were going to

558
00:26:06,333 --> 00:26:07,966
like read your email while
you were sitting in a café.

559
00:26:08,033 --> 00:26:09,632
It was like you might
take pictures of them.

560
00:26:09,700 --> 00:26:12,900
Of course, it gets sort of
laughed out of the market.

561
00:26:12,966 --> 00:26:17,799
But then Snapchat comes along
with a teen audience and they

562
00:26:17,866 --> 00:26:20,366
take what is the primary thing
that everybody was freaking out

563
00:26:20,433 --> 00:26:23,500
about, this thing shoots video
and it immediately uploads it.

564
00:26:23,566 --> 00:26:24,433
Right?

565
00:26:24,500 --> 00:26:27,700
That's its only purpose, for
teens of all things, and

566
00:26:27,766 --> 00:26:29,333
everybody is like
where do I get that.

567
00:26:29,400 --> 00:26:30,866
They were putting
them in Times Square.

568
00:26:30,933 --> 00:26:33,099
They were putting them in
vending machines in the Grand

569
00:26:33,166 --> 00:26:36,066
Canyon and people were racing to
try to get the first ones and

570
00:26:36,133 --> 00:26:38,099
paying $200 for them.

571
00:26:38,166 --> 00:26:41,799
And then it turns out that
nobody even wants them, that it

572
00:26:41,866 --> 00:26:44,866
is dull and it's boring and
reportedly they have to destroy

573
00:26:44,933 --> 00:26:47,433
tens of thousands of them.

574
00:26:48,233 --> 00:26:51,466
A number of years, a colleague
of mine and I wrote a paper

575
00:26:51,533 --> 00:26:54,666
called A Theory of Creepy:
Technology, Privacy, and

576
00:26:54,733 --> 00:26:58,066
Shifting Social Norms where
we tried to sort of put an

577
00:26:58,133 --> 00:27:00,366
intellectual and legal
framework around this.

578
00:27:00,433 --> 00:27:03,066
It is always cool to be
published in a nice journal like

579
00:27:03,133 --> 00:27:04,700
the Yale Journal of
Law and Technology.

580
00:27:04,766 --> 00:27:08,799
The downside is that Polonetsky
and Creepy came up very high in

581
00:27:08,866 --> 00:27:11,833
Google for many, many years.

582
00:27:11,900 --> 00:27:15,833
Being an authority on Creepy
has its pros and cons.

583
00:27:15,900 --> 00:27:19,233
Look, we're being challenged
by some of these incredible

584
00:27:19,299 --> 00:27:21,566
opportunities. Right?

585
00:27:21,633 --> 00:27:25,833
In almost every area of
enterprise, of research, of

586
00:27:25,900 --> 00:27:31,266
life, we're seeing people truly
enthusiastic about using data,

587
00:27:31,333 --> 00:27:34,200
not just to make advertising
better, not just to make the

588
00:27:34,266 --> 00:27:38,666
latest delivery gimmick, and the
latest sort of isn't that cool,

589
00:27:38,733 --> 00:27:43,700
real breakthroughs that matter
to safety, to transportation, to

590
00:27:43,766 --> 00:27:47,866
medical care, to being able
to provide better service at

591
00:27:47,933 --> 00:27:48,900
our cities.

592
00:27:48,966 --> 00:27:52,466
Getting this right, the
stakes are really high.

593
00:27:52,533 --> 00:27:58,265
Every policy issue, every stress
in society seems to be playing

594
00:27:58,333 --> 00:28:01,266
out on digital
platforms right now.

595
00:28:01,333 --> 00:28:06,633
Every privacy or civil rights
leader or person worried about

596
00:28:06,700 --> 00:28:11,233
policy in the world, when you
say that proposal you have is

597
00:28:11,299 --> 00:28:14,366
going to limit innovation, you
get laughed out of the room,

598
00:28:14,433 --> 00:28:18,766
because to them, innovation
means don't regulate me because

599
00:28:18,833 --> 00:28:20,400
I'm going to make
money with this.

600
00:28:20,466 --> 00:28:23,299
I don't even know what the
downsides are, but it's a new

601
00:28:23,366 --> 00:28:26,500
way to make money, and
so it ought to be okay. Right?

602
00:28:26,566 --> 00:28:27,966
They're like, no, no, no.

603
00:28:28,033 --> 00:28:30,599
What are you doing for society
and what downsides are you

604
00:28:30,666 --> 00:28:32,366
bringing along with it?

605
00:28:32,433 --> 00:28:34,133
That's our framework.

606
00:28:34,200 --> 00:28:37,133
These are civil rights issues
and that changes the dynamic in

607
00:28:37,200 --> 00:28:38,366
a really significant way.

608
00:28:38,433 --> 00:28:39,400
How are we fixing it?

609
00:28:39,466 --> 00:28:43,466
What's happening in terms of
legislation in the U.S.?

610
00:28:43,533 --> 00:28:49,299
Well, we talked about how it's
really a civil rights issue in

611
00:28:49,366 --> 00:28:50,266
many ways.

612
00:28:50,333 --> 00:28:52,700
How am I treating people
differently based on data that

613
00:28:52,766 --> 00:28:54,200
reflects who they are?

614
00:28:54,266 --> 00:28:56,666
We talked about the
stress of social norms.

615
00:28:56,733 --> 00:28:59,133
Can government
access and get this data?

616
00:28:59,200 --> 00:29:02,933
Are corporations having too much
power over individuals because

617
00:29:03,000 --> 00:29:04,366
of how much data they have?

618
00:29:04,433 --> 00:29:08,400
Are foreign countries
interfering in our elections?

619
00:29:08,466 --> 00:29:11,966
Are automated decisions being
made where I'll be turned down

620
00:29:12,033 --> 00:29:16,832
for healthcare, I'll be turned
down for insurance, my probation

621
00:29:16,900 --> 00:29:18,500
will be extended?

622
00:29:18,566 --> 00:29:21,866
These are not
privacy issues. Right?

623
00:29:21,933 --> 00:29:23,833
These are issues of power.

624
00:29:23,900 --> 00:29:29,333
These are issues of human
rights at the end of the day.

625
00:29:29,400 --> 00:29:31,866
So how should we
regulate human rights? Right?

626
00:29:31,933 --> 00:29:34,566
It obviously shouldn't be
just like icons on ads.

627
00:29:34,633 --> 00:29:35,966
What are we doing?

628
00:29:36,033 --> 00:29:39,733
We're trying to move away from
this consumer protection law.

629
00:29:39,799 --> 00:29:42,866
I can do anything I want as long
as I don't deceive you or really

630
00:29:42,933 --> 00:29:46,200
harm you, otherwise it goes.

631
00:29:46,266 --> 00:29:47,566
Okay. We're moving away.

632
00:29:47,633 --> 00:29:50,599
Here in California, we have one
of the leading new consumer

633
00:29:50,666 --> 00:29:55,099
privacy laws, and it primarily
gives you the right to see,

634
00:29:55,166 --> 00:29:59,866
download, access, and tell
companies to delete it and not

635
00:29:59,933 --> 00:30:01,166
to sell it.

636
00:30:01,233 --> 00:30:05,666
So if I go to the thousands and
thousands and thousands of sites

637
00:30:05,733 --> 00:30:09,000
that I have been to and the
thousands and thousands of

638
00:30:09,066 --> 00:30:11,799
businesses that I have dealt
with, and then all the others

639
00:30:11,866 --> 00:30:13,566
that I don't even know I dealt
with because they're there

640
00:30:13,633 --> 00:30:16,299
behind the scenes, they're
third-party data companies, if I

641
00:30:16,366 --> 00:30:20,066
somehow find them, I can spend
an awful lot of time telling

642
00:30:20,133 --> 00:30:23,966
them all to not sell my data
and telling them to delete it.

643
00:30:24,033 --> 00:30:27,533
So Professor Daniel Solove,
one of the leading privacy

644
00:30:27,599 --> 00:30:31,133
academics, one of the most
published and respected authors,

645
00:30:31,200 --> 00:30:34,166
in an article he put out just a
couple weeks ago called The Myth

646
00:30:34,233 --> 00:30:38,899
of the Privacy Paradox points to
all these new consumer privacy

647
00:30:38,966 --> 00:30:41,233
laws, CCPAs.

648
00:30:41,299 --> 00:30:46,599
State after state is replicating
this California Act as sort of

649
00:30:46,666 --> 00:30:48,633
the new model.

650
00:30:48,700 --> 00:30:52,299
Can we do better?

651
00:30:52,366 --> 00:30:54,366
Let's look to Europe.

652
00:30:54,433 --> 00:30:55,933
The Europeans are
serious about privacy.

653
00:30:56,000 --> 00:30:59,166
They passed more than two years
ago what is considered the gold

654
00:30:59,233 --> 00:31:02,433
standard of privacy laws,
the General Data Protection

655
00:31:02,500 --> 00:31:04,799
Regulation, the GDPR.

656
00:31:04,866 --> 00:31:09,833
Let me point out that privacy
doesn't appear in the text of

657
00:31:09,900 --> 00:31:13,466
that statute, and it's
considered the most important

658
00:31:13,533 --> 00:31:14,733
privacy law.

659
00:31:14,799 --> 00:31:17,666
It's a data protection law
because it actually seeks to

660
00:31:17,733 --> 00:31:22,033
protect all of the rights and
freedoms that are guaranteed at

661
00:31:22,099 --> 00:31:25,700
the European Constitutional
level, their Charter of

662
00:31:25,766 --> 00:31:26,766
Human Rights.

663
00:31:26,833 --> 00:31:29,466
So, of course, privacy is in
there, but so is freedom of

664
00:31:29,533 --> 00:31:32,699
speech and freedom of
movement and all of the other

665
00:31:32,766 --> 00:31:35,799
constitutional level rights.

666
00:31:35,866 --> 00:31:39,066
This is what is influencing
companies around the world, and

667
00:31:39,133 --> 00:31:42,633
this is why it's considered a
human rights law because it's

668
00:31:42,700 --> 00:31:45,766
actually not solely about
protecting the data, it's about

669
00:31:45,833 --> 00:31:50,700
protecting all of the values
that intersect when data is the

670
00:31:50,766 --> 00:31:54,633
crux, when data prevents it,
when data creates it, when data

671
00:31:54,700 --> 00:31:55,866
makes it worse.

672
00:31:55,933 --> 00:31:56,933
What is our model?

673
00:31:57,000 --> 00:31:59,133
We have these opt-out laws
sort of popping up around

674
00:31:59,200 --> 00:32:00,166
the country.

675
00:32:00,233 --> 00:32:04,799
We have a Washington State
privacy proposal that tries to

676
00:32:04,866 --> 00:32:08,099
go a little bit further than
this opt-out model, including

677
00:32:08,166 --> 00:32:09,799
some of the GDPR concepts.

678
00:32:09,866 --> 00:32:12,566
The federal government is trying
to figure out how to handle

679
00:32:12,633 --> 00:32:13,666
it slowly.

680
00:32:13,733 --> 00:32:17,233
Maybe technology can play a bit
of a role if technology has been

681
00:32:17,299 --> 00:32:19,000
part of the problem.

682
00:32:19,066 --> 00:32:23,733
I'm optimistic that with more
sophisticated de-identification,

683
00:32:23,799 --> 00:32:26,566
we're seeing real advances
in differential privacy, in

684
00:32:26,633 --> 00:32:29,933
homomorphic encryption,
different models of federated

685
00:32:30,000 --> 00:32:33,599
learning where the data
stays on your device, where

686
00:32:33,666 --> 00:32:36,233
mathematically we've got
guarantees that are much

687
00:32:36,299 --> 00:32:39,333
stronger that might help us get
the utility that we want of the

688
00:32:39,400 --> 00:32:42,700
data, the analysis and the
research, but without actually

689
00:32:42,766 --> 00:32:44,799
having data
about any individual.

690
00:32:44,866 --> 00:32:46,533
That's an exciting path forward.

691
00:32:46,599 --> 00:32:50,765
We're seeing more data being
handled locally, keep it on your

692
00:32:50,833 --> 00:32:54,133
phone instead of sending it to
the cloud, and yet we can give

693
00:32:54,200 --> 00:32:59,700
you sophisticated tools, we can
give you sophisticated analysis.

694
00:32:59,766 --> 00:33:02,200
We're seeing synthetic data.

695
00:33:02,266 --> 00:33:05,900
We're seeing small data instead
of big data being needed to

696
00:33:05,966 --> 00:33:07,366
power machine learning.

697
00:33:07,433 --> 00:33:12,466
I'm optimistic that we can - we
need law, but that we can, in

698
00:33:12,533 --> 00:33:17,366
large part, if we are careful
about supporting and following

699
00:33:17,433 --> 00:33:20,466
some of these tech trends and
lean into them, solve some of

700
00:33:20,533 --> 00:33:21,433
our challenges.

701
00:33:21,500 --> 00:33:26,833
How do we make sure that
legislators understand this when

702
00:33:26,900 --> 00:33:29,733
they're still trying to figure
out how to deal with cookies on

703
00:33:29,799 --> 00:33:31,433
the government websites?

704
00:33:31,500 --> 00:33:35,333
We need to help legislators
understand the technical nuts

705
00:33:35,400 --> 00:33:39,799
and bolts, not to code,
not to write scripts.

706
00:33:39,866 --> 00:33:43,433
We need them understanding
enough because they are going to

707
00:33:43,500 --> 00:33:45,633
be regulating machine learning
and they're not going to become

708
00:33:45,700 --> 00:33:47,433
machine learning experts.

709
00:33:47,500 --> 00:33:48,700
They need the staffers.

710
00:33:48,766 --> 00:33:51,666
We need to support efforts to
give them the staff who have the

711
00:33:51,733 --> 00:33:52,765
tech chops.

712
00:33:52,833 --> 00:33:55,266
But all of us who work in these
areas need to spend the time

713
00:33:55,333 --> 00:34:00,233
being the translator of here
is what might matter to you if

714
00:34:00,299 --> 00:34:01,633
you're looking to
solve these problems.

715
00:34:01,700 --> 00:34:04,700
Let me simply tell you
how it works so you can make

716
00:34:04,766 --> 00:34:05,533
better decisions.

717
00:34:06,099 --> 00:34:09,966
Another area that we need to
figure out how to deal with.

718
00:34:10,033 --> 00:34:14,333
If I have data in my home,
right, if I have a letter, if I

719
00:34:14,400 --> 00:34:18,266
have information sitting on my
computer, it's protected under

720
00:34:18,333 --> 00:34:19,466
the Constitution.

721
00:34:19,533 --> 00:34:23,165
Things that are on my body,
things that are in my home, my

722
00:34:23,233 --> 00:34:27,133
letters, the government wants
it, it needs a warrant.

723
00:34:27,199 --> 00:34:32,033
Lots of that data, however, that
used to only be in our home, in

724
00:34:32,099 --> 00:34:34,966
a letter, or an email that I
downloaded and wasn't sitting on

725
00:34:35,033 --> 00:34:40,866
some server elsewhere, is now
sitting on cloud, is sitting in

726
00:34:40,933 --> 00:34:44,033
other companies' computers, is
sitting everywhere and anywhere

727
00:34:44,099 --> 00:34:47,433
around the world, and very
often, therefore, not subject to

728
00:34:47,500 --> 00:34:49,033
that same protection.

729
00:34:49,099 --> 00:34:51,766
A lawyer might be able to get it
simply with a subpoena because

730
00:34:51,833 --> 00:34:54,533
they are suing
somebody or suing you.

731
00:34:55,300 --> 00:34:57,800
The Supreme Court might be
helping us a little bit.

732
00:34:57,866 --> 00:35:02,466
A recent case called Carpenter
where we saw the justices

733
00:35:02,533 --> 00:35:08,232
appreciating that data that was
outside, cell tower location

734
00:35:08,300 --> 00:35:12,666
data was so intimate that it
actually needed a higher level

735
00:35:12,733 --> 00:35:13,800
of protection.

736
00:35:13,866 --> 00:35:14,966
I'm optimistic.

737
00:35:15,033 --> 00:35:17,033
We'll see how the court goes.

738
00:35:17,099 --> 00:35:20,833
But there is at least some hope
that justices that use cell

739
00:35:20,900 --> 00:35:25,300
phones get the intimacy and the
idea that the protection ought

740
00:35:25,366 --> 00:35:30,166
not to be dependent simply on
whether this is in my home or

741
00:35:30,233 --> 00:35:32,099
sitting on someone
else's computer.

742
00:35:32,800 --> 00:35:34,099
What else can we do?

743
00:35:34,166 --> 00:35:38,066
Well, if we want to actually
understand what's going on at

744
00:35:38,133 --> 00:35:41,033
some of these companies, do
we ask them for good reports?

745
00:35:41,099 --> 00:35:45,300
Hey, are you helping with
traffic, ride sharing company?

746
00:35:45,366 --> 00:35:46,366
Give us a report.

747
00:35:46,433 --> 00:35:54,366
Hey, healthcare data, can
you just give me a report?

748
00:35:54,433 --> 00:35:56,466
We need independent academics.

749
00:35:56,533 --> 00:36:01,433
Did foreign countries interfere
and use social media in ways?

750
00:36:01,500 --> 00:36:02,933
Hey, company, give us a report.

751
00:36:03,000 --> 00:36:03,966
No.

752
00:36:04,033 --> 00:36:06,833
We want independent academics
having access to be able to

753
00:36:06,900 --> 00:36:10,866
study and analyze and research
and use that data for the

754
00:36:10,933 --> 00:36:12,033
benefit of society.

755
00:36:12,099 --> 00:36:15,699
We need models where they can
share it in a responsible way

756
00:36:15,766 --> 00:36:19,000
that protects privacy, that has
the adequate de-identification,

757
00:36:19,066 --> 00:36:20,966
that goes through
an ethics review.

758
00:36:21,033 --> 00:36:24,533
Today, if I'm an academic
and I do a study and I'm at a

759
00:36:24,599 --> 00:36:27,800
university, I'm doing something
that's federally funded, I have

760
00:36:27,866 --> 00:36:31,033
got to go through an independent
review board that says, well,

761
00:36:31,099 --> 00:36:32,400
did you get the
proper permission?

762
00:36:32,466 --> 00:36:33,966
Is there some risk here?

763
00:36:34,033 --> 00:36:35,933
Is what you're doing
likely to be helpful?

764
00:36:36,000 --> 00:36:38,400
Is it going to harm the people
who are part of the experiment?

765
00:36:38,466 --> 00:36:41,232
Well, what if I'm a company and
I'm doing that same research, or

766
00:36:41,300 --> 00:36:44,733
I'm an academic and I'm working
with corporate data and it is

767
00:36:44,800 --> 00:36:46,199
not a federally funded project?

768
00:36:46,266 --> 00:36:49,433
I don't have that
same review process.

769
00:36:49,500 --> 00:36:53,933
We need those done in a credible
way so that we can rely that

770
00:36:54,000 --> 00:36:55,300
these sorts of
things get vetted.

771
00:36:55,900 --> 00:37:00,099
Then can we actually put the
lawyers aside for a second, and

772
00:37:00,166 --> 00:37:02,633
even the tech and security
people aside, and bring in some

773
00:37:02,699 --> 00:37:07,033
of the UI and the human-computer
interface and the design people?

774
00:37:07,099 --> 00:37:10,466
An academic recently did a
wonderful paper where she said,

775
00:37:10,533 --> 00:37:15,366
hey, let's take the nutrition
label and let's do the way a

776
00:37:15,433 --> 00:37:18,833
privacy notice would be done if
we let the privacy people write

777
00:37:18,900 --> 00:37:19,766
this. Right?

778
00:37:19,833 --> 00:37:21,000
I'll just point out one or two.

779
00:37:21,066 --> 00:37:25,633
And if you have read privacy
policies, they'll remind you how

780
00:37:25,699 --> 00:37:26,833
useless this is.

781
00:37:26,900 --> 00:37:29,900
Like most snack suppliers, we
may include sweetness to enhance

782
00:37:29,966 --> 00:37:33,299
your experience with the
help of our trusted partners.

783
00:37:33,366 --> 00:37:35,699
We only include as much sodium
as is necessary to keep

784
00:37:35,766 --> 00:37:37,500
providing our product to you.

785
00:37:37,566 --> 00:37:39,000
What am I going to do with that?

786
00:37:39,066 --> 00:37:41,866
That's what we're doing when
it comes to privacy policies.

787
00:37:41,933 --> 00:37:44,300
The nutrition label obviously
did a little bit better.

788
00:37:44,366 --> 00:37:49,666
Can we have the people who
actually work hard at working to

789
00:37:49,733 --> 00:37:53,533
make sure humans understand what
we're communicating in the room

790
00:37:53,599 --> 00:37:56,633
so that we
actually can do better?

791
00:37:58,566 --> 00:38:02,332
Educate policymakers,
contribute to these codes and

792
00:38:02,400 --> 00:38:03,533
best practices.

793
00:38:03,599 --> 00:38:05,933
That's setting norms. Right?

794
00:38:06,000 --> 00:38:08,199
Let's put some of
those rules in place.

795
00:38:08,266 --> 00:38:10,099
I was doing some
research recently.

796
00:38:10,166 --> 00:38:13,900
My father had colon cancer,
so I'm at high-risk,

797
00:38:13,966 --> 00:38:15,699
family history.

798
00:38:15,766 --> 00:38:18,566
I was reading about a new test
that you can take and you don't

799
00:38:18,633 --> 00:38:20,966
have to go in and do the
whole colonoscopy, but is it

800
00:38:21,033 --> 00:38:21,966
accurate enough?

801
00:38:22,033 --> 00:38:23,366
What are the false positives?

802
00:38:23,433 --> 00:38:27,166
I'm reading about it on the site
of the company that makes this.

803
00:38:27,233 --> 00:38:28,633
And then you know what I see?

804
00:38:28,699 --> 00:38:32,199
I see that triangle eye
and I know what that means.

805
00:38:32,266 --> 00:38:34,433
That means they're
going to be retargeting me on

806
00:38:34,500 --> 00:38:35,333
other websites.

807
00:38:35,400 --> 00:38:39,466
They're labeling me as
interested in colon cancer,

808
00:38:39,533 --> 00:38:43,732
maybe somebody who is at risk,
I don't know, and that will be

809
00:38:43,800 --> 00:38:47,966
used, there will be a profile
of me, and anywhere I go, the

810
00:38:48,033 --> 00:38:50,466
company that wants to target me
is going to be sharing that with

811
00:38:50,533 --> 00:38:52,533
other third parties, right,
because they're not in the ad

812
00:38:52,599 --> 00:38:55,800
business, they're in the cancer
test business, so they're going

813
00:38:55,866 --> 00:38:58,433
to be sharing it with their ad
tech company who is going to be

814
00:38:58,500 --> 00:39:01,666
selling it in a data place, so
also that someone can find me

815
00:39:01,733 --> 00:39:06,199
and remind me to maybe come
back to the site and transact.

816
00:39:06,266 --> 00:39:07,300
Right?

817
00:39:07,366 --> 00:39:11,733
Draw some boundaries that say
some of these things go too far

818
00:39:11,800 --> 00:39:16,400
so that we set some social norms
so that companies understand the

819
00:39:16,466 --> 00:39:18,165
boundaries in
addition to obviously having

820
00:39:18,233 --> 00:39:19,433
serious legislation.

821
00:39:19,966 --> 00:39:21,766
The issues are
more than privacy.

822
00:39:21,833 --> 00:39:23,666
Don't talk just to
us privacy people.

823
00:39:23,733 --> 00:39:25,866
Talk to people who worry
about low income issues.

824
00:39:25,933 --> 00:39:27,866
Talk to people who
represent immigrants.

825
00:39:27,933 --> 00:39:30,666
Talk to people who come from
different socioeconomic

826
00:39:30,733 --> 00:39:33,033
backgrounds, of different
ethnicities, because they're

827
00:39:33,099 --> 00:39:37,266
going to see issues and concerns
that maybe you don't and it is

828
00:39:37,333 --> 00:39:41,133
those issues that are going to
bring down any of the progress

829
00:39:41,199 --> 00:39:43,300
if you don't think it
through and actually solve

830
00:39:43,366 --> 00:39:44,300
those problems.

831
00:39:44,366 --> 00:39:46,833
And then this is something
- this last one is probably

832
00:39:46,900 --> 00:39:49,666
something that I think security
people get because you get that

833
00:39:49,733 --> 00:39:53,733
there is lots of bad people out
there, and anything they see,

834
00:39:53,800 --> 00:39:56,333
they will use to try
to get in the door.

835
00:39:56,400 --> 00:39:58,166
That's part of life.

836
00:39:58,233 --> 00:40:00,699
Us privacy people, we're
a little bit more naïve.

837
00:40:00,766 --> 00:40:02,633
If somebody comes and says
here's what this product does,

838
00:40:02,699 --> 00:40:05,233
it's going to help you do this,
it's going to help you get

839
00:40:05,300 --> 00:40:08,300
delivery, it's going to help do
this and that, we'd be like, oh,

840
00:40:08,366 --> 00:40:10,300
okay, and then we analyze it.

841
00:40:10,366 --> 00:40:14,000
We don't say, yeah, but how is
someone going to mess with this

842
00:40:14,066 --> 00:40:17,533
to do something discriminatory?

843
00:40:17,599 --> 00:40:20,800
How is this algorithm somehow
going to be optimized so that

844
00:40:20,866 --> 00:40:24,266
women don't get to
see X or Y or Z?

845
00:40:24,333 --> 00:40:30,333
Are we ethically hacking those
same consumer-facing tools,

846
00:40:30,400 --> 00:40:33,833
because someone will ethically
hack it and show that here's

847
00:40:33,900 --> 00:40:36,866
what you're doing in a way that
isn't fair and isn't equitable?

848
00:40:36,933 --> 00:40:42,099
Do that red team, that white
hat, ethical privacy hacking.

849
00:40:42,166 --> 00:40:44,933
Let's learn a bit from
the security community.

850
00:40:45,466 --> 00:40:46,699
I grew up in Coney Island.

851
00:40:46,766 --> 00:40:48,466
I was the state
legislator for that area.

852
00:40:48,533 --> 00:40:49,400
This is the Cyclone.

853
00:40:49,466 --> 00:40:52,333
It's the classic roller
coaster from those days.

854
00:40:52,400 --> 00:40:53,566
It's like a
wooden roller coaster.

855
00:40:53,633 --> 00:40:54,933
It's very scary.

856
00:40:55,000 --> 00:40:59,833
It's going to be a wild ride
because these issues are messy.

857
00:40:59,900 --> 00:41:00,900
And guess what?

858
00:41:00,966 --> 00:41:03,333
We haven't figured them
out in the rest of life.

859
00:41:03,400 --> 00:41:05,966
There is no way we're going
to solve them easily online.

860
00:41:06,033 --> 00:41:08,165
We haven't figured out the
balance of free speech.

861
00:41:08,233 --> 00:41:12,466
We haven't figured out what
is and what isn't the right

862
00:41:12,533 --> 00:41:13,665
discrimination.

863
00:41:13,733 --> 00:41:15,666
When am I discriminating
in a positive way?

864
00:41:15,733 --> 00:41:16,699
When is it a negative way?

865
00:41:16,766 --> 00:41:18,699
Who gets protected?

866
00:41:18,766 --> 00:41:21,400
When is it relevant to
treat people differently?

867
00:41:21,466 --> 00:41:22,566
When is it economic?

868
00:41:22,633 --> 00:41:24,466
We haven't solved these
problems elsewhere.

869
00:41:24,533 --> 00:41:25,732
It is messy.

870
00:41:25,800 --> 00:41:26,866
It is different
in every country.

871
00:41:26,933 --> 00:41:31,800
It is going to be messy and
uncomfortable and hard, but we

872
00:41:31,866 --> 00:41:32,900
have no choice.

873
00:41:32,966 --> 00:41:35,165
And if you're an optimist like I
am about what can be done with

874
00:41:35,233 --> 00:41:37,500
tech and data,
hang on for the ride.

875
00:41:38,266 --> 00:41:39,866
Love to take some questions.

876
00:41:39,933 --> 00:41:42,400
At the Future Privacy Forum,
we work with the chief privacy

877
00:41:42,466 --> 00:41:46,232
officers of about 200 companies
as well as with academics and

878
00:41:46,300 --> 00:41:48,900
civil society and foundations.

879
00:41:48,966 --> 00:41:51,500
We're optimistic about what can
be done with tech and data, but

880
00:41:51,566 --> 00:41:56,332
we think we get to those good
things if we have the rules, the

881
00:41:56,400 --> 00:41:58,000
norms, the protections in place.

882
00:41:58,066 --> 00:42:01,765
Otherwise, we end up swirling,
we'll ban things, we'll regulate

883
00:42:01,833 --> 00:42:03,300
things, we'll limit things.

884
00:42:03,366 --> 00:42:06,166
Some of them should be banned
and regulated and limited, but

885
00:42:06,233 --> 00:42:11,500
we'll go slower in areas where
we can go fast, and we'll miss

886
00:42:11,566 --> 00:42:13,399
out on the kinds of
opportunities that I think we

887
00:42:13,466 --> 00:42:14,299
all want to see.

888
00:42:14,933 --> 00:42:15,866
With a question.

889
00:42:15,933 --> 00:42:17,266
Anybody can come up
to the microphone.

890
00:42:17,333 --> 00:42:22,133
I'm happy to talk about any of
the privacy issues, data issues,

891
00:42:22,199 --> 00:42:26,633
legislative issues,
anything that is on your mind.

892
00:42:26,699 --> 00:42:28,099
Yes, please.

893
00:42:29,900 --> 00:42:35,733
>> AUDIENCE: You talked
about the DPO role for GR -

894
00:42:37,800 --> 00:42:40,099
thank you - GDPR.

895
00:42:40,166 --> 00:42:41,900
Microphone nerves here.

896
00:42:41,966 --> 00:42:45,400
Is there some level of
certification or something that

897
00:42:45,466 --> 00:42:49,266
one needs to achieve or look
for if one is hiring from the

898
00:42:49,333 --> 00:42:50,566
outside for that role?

899
00:42:51,666 --> 00:42:54,733
>> JULES POLONETSKY: This is
a fairly novel formal role.

900
00:42:54,800 --> 00:42:57,633
In Germany and some other
places in the world, this isn't

901
00:42:57,699 --> 00:42:58,500
that new.

902
00:42:58,566 --> 00:43:01,566
There have been
obligations to have a DPO.

903
00:43:01,633 --> 00:43:03,066
Not every company needs one.

904
00:43:03,133 --> 00:43:05,265
You have got
to do sort of large-scale,

905
00:43:05,333 --> 00:43:06,533
high-risk processing.

906
00:43:06,599 --> 00:43:09,566
If you are maybe a small
business, you may not need to.

907
00:43:09,633 --> 00:43:13,299
Many, many, many kinds of
activities fall under, and a lot

908
00:43:13,366 --> 00:43:15,733
of other companies have said,
you know what, maybe if I can

909
00:43:15,800 --> 00:43:18,633
afford to have this role, I
ought to, even if technically

910
00:43:18,699 --> 00:43:21,300
what I do isn't
high-risk processing.

911
00:43:21,366 --> 00:43:21,966
Number one.

912
00:43:22,766 --> 00:43:25,866
You don't today have to
have any certification.

913
00:43:25,933 --> 00:43:27,966
You can hire
anybody and anybody.

914
00:43:28,033 --> 00:43:30,665
No, that's not a very good idea
to hire anybody and anybody.

915
00:43:30,733 --> 00:43:33,533
There are efforts in Europe
which takes certification very

916
00:43:33,599 --> 00:43:38,300
seriously to create formal
certification programs.

917
00:43:38,366 --> 00:43:41,800
There are some informal ones
now, there are a few places.

918
00:43:41,866 --> 00:43:45,466
The IAPP, which isn't certifying
DPOs but certifies privacy

919
00:43:45,533 --> 00:43:49,000
professionals, has a host of
programs and training and tests

920
00:43:49,066 --> 00:43:50,066
you can take.

921
00:43:50,133 --> 00:43:53,866
I don't think we'll create
the same role in the U.S.

922
00:43:53,933 --> 00:43:57,233
In Europe, this is an
independent person.

923
00:43:57,300 --> 00:44:01,300
Again, great that you can't be
fired, but you are also not the

924
00:44:01,366 --> 00:44:04,800
person actually doing the work,
counseling on the product.

925
00:44:04,866 --> 00:44:06,866
You're almost sort of like an
independent auditor who is

926
00:44:06,933 --> 00:44:09,933
there, but you have got
to keep some distance.

927
00:44:10,000 --> 00:44:13,566
The chief privacy officers, the
legal privacy leads, I think

928
00:44:13,633 --> 00:44:17,033
what we expect in the U.S. is
I want to put somebody who is

929
00:44:17,099 --> 00:44:19,900
going to be hands-on,
who actually is handling the

930
00:44:19,966 --> 00:44:21,000
access requests.

931
00:44:21,066 --> 00:44:26,165
And if I say I cannot give you
access because I don't have a

932
00:44:26,233 --> 00:44:28,533
good way to identify you, I
have only your cookies, so I

933
00:44:28,599 --> 00:44:31,533
shouldn't give you all of the
sensitive information I have

934
00:44:31,599 --> 00:44:36,866
about you, I maybe want that
counselor playing that role.

935
00:44:36,933 --> 00:44:41,866
So I think we'll more
likely have certified privacy

936
00:44:41,933 --> 00:44:44,900
professionals who
have qualified training.

937
00:44:45,733 --> 00:44:51,333
Europe cares deeply about what
I'll call individual redress.

938
00:44:51,400 --> 00:44:54,233
One of the biggest issues
in the U.S. in this debate about

939
00:44:54,300 --> 00:44:57,166
privacy law is are you
going to get to sue companies?

940
00:44:57,233 --> 00:45:00,833
Is it only the FTC, is it only
these consumer regulators who

941
00:45:00,900 --> 00:45:03,300
get to stick up for you?

942
00:45:03,366 --> 00:45:05,966
If you write to the FTC today
and you say I got ripped off,

943
00:45:06,033 --> 00:45:07,098
they took my data.

944
00:45:07,166 --> 00:45:09,833
They stole my money, blah, blah,
blah, the FTC has no obligation

945
00:45:09,900 --> 00:45:12,466
to call you back and say, wait,
what, can we investigate.

946
00:45:12,533 --> 00:45:15,366
If they hear a lot of
complaints, they might take

947
00:45:15,433 --> 00:45:16,833
action against this company.

948
00:45:16,900 --> 00:45:22,199
In Europe, people need, under
the European legal system, the

949
00:45:22,266 --> 00:45:27,166
right to go to the DPA, to go
to the DPO, the data protection

950
00:45:27,233 --> 00:45:30,699
authority, and say my rights
were violated, what are you

951
00:45:30,766 --> 00:45:33,066
going to do about it, and
someone has to see, review.

952
00:45:33,133 --> 00:45:35,433
They can turn it down, but
they have got to review it.

953
00:45:35,500 --> 00:45:38,633
So we're probably going to
evolve in a different model.

954
00:45:38,699 --> 00:45:41,666
Hopefully that gives you a
little bit of information.

955
00:45:43,166 --> 00:45:48,233
>> AUDIENCE: An earlier keynote
speaker said that the BTK serial

956
00:45:48,300 --> 00:45:52,300
killer case was solved, this
was probably a decade ago, from

957
00:45:52,366 --> 00:45:56,599
using the results of a test
performed on the guy's daughter.

958
00:45:56,666 --> 00:45:59,033
Does HIPAA - they had a warrant.

959
00:45:59,099 --> 00:46:00,400
Does HIPAA allow that?

960
00:46:00,466 --> 00:46:03,766
Can the test results of your
relatives be used against you?

961
00:46:04,733 --> 00:46:10,199
>> JULES POLONETSKY: HIPAA only
helps protect the data that you

962
00:46:10,266 --> 00:46:15,433
give up when you go to the
doctor, when you go and submit a

963
00:46:15,500 --> 00:46:17,900
lab test that was ordered
by the doctor, when you're in

964
00:46:17,966 --> 00:46:19,133
the hospital.

965
00:46:19,199 --> 00:46:20,466
It is very limited.

966
00:46:20,533 --> 00:46:23,232
The goal was to make sure you
aren't nervous about giving your

967
00:46:23,300 --> 00:46:26,633
doctor information, and that the
medical system could easily zoom

968
00:46:26,699 --> 00:46:28,599
your data all throughout
the medical system.

969
00:46:28,666 --> 00:46:30,433
Boom, it goes to insurance.

970
00:46:30,500 --> 00:46:32,533
You go to another specialist,
you know, it's a pain in the

971
00:46:32,599 --> 00:46:34,800
neck, but they can
easily transfer it.

972
00:46:34,866 --> 00:46:37,366
That's all that HIPAA does.

973
00:46:37,433 --> 00:46:41,766
We don't have currently
privacy protection for all that

974
00:46:41,833 --> 00:46:42,800
other data.

975
00:46:42,866 --> 00:46:45,500
We have bank laws that protect
your banking data, we have

976
00:46:45,566 --> 00:46:47,566
credit laws that
protect your credit data.

977
00:46:47,633 --> 00:46:51,533
Your genetic data is
protected in some limited way.

978
00:46:51,599 --> 00:46:55,666
Your employer cannot say to you,
you want a job, let me have a

979
00:46:55,733 --> 00:46:59,233
genetic test, let's see how
smart your genes say you are, or

980
00:46:59,300 --> 00:47:01,333
let's see if you have got
any symptoms of significant

981
00:47:01,400 --> 00:47:04,666
illnesses that might make
it very expensive, or blah,

982
00:47:04,733 --> 00:47:05,433
blah, blah.

983
00:47:05,500 --> 00:47:06,466
I can't fire you.

984
00:47:06,533 --> 00:47:09,799
I can't turn you down
for health insurance.

985
00:47:10,833 --> 00:47:13,599
The genetics issue ends up
being really interesting.

986
00:47:13,666 --> 00:47:15,400
Here is what happens.

987
00:47:15,466 --> 00:47:18,000
There are a number of open
genetics databases, right, the

988
00:47:18,066 --> 00:47:20,198
genealogists of the world, the
people who are researching the

989
00:47:20,266 --> 00:47:23,300
answer string, have been loading
their data up to these open

990
00:47:23,366 --> 00:47:25,666
databases and they're finding
long-lost relatives and it is

991
00:47:25,733 --> 00:47:27,000
all very exciting.

992
00:47:27,066 --> 00:47:29,732
Then the cops are like,
well, that's kind of cool.

993
00:47:29,800 --> 00:47:33,233
We can go there just like any
other researcher, and if we have

994
00:47:33,300 --> 00:47:37,366
a little bit of evidence, we can
use this to find maybe somebody

995
00:47:37,433 --> 00:47:42,099
whose 2-3 relatives away, and
all of a sudden we have a lot of

996
00:47:42,166 --> 00:47:44,900
information and we can start
visiting people's cousins and

997
00:47:44,966 --> 00:47:47,633
relatives and uncles, and a
number of cases have been

998
00:47:47,699 --> 00:47:51,199
cracked because of these open
databases, but a lot of other

999
00:47:51,266 --> 00:47:56,233
people are like, wait a second,
I'm in this possible police

1000
00:47:56,300 --> 00:47:59,933
dragnet because my second cousin
three times removed loaded their

1001
00:48:00,000 --> 00:48:03,466
data and there is some
relationship to my data.

1002
00:48:03,533 --> 00:48:07,799
The one big open database has
pulled back some of their

1003
00:48:07,866 --> 00:48:10,533
information, the justice
department has put some rules in

1004
00:48:10,599 --> 00:48:15,733
place where they won't
pretend to be somebody, but we

1005
00:48:15,800 --> 00:48:19,866
absolutely need a
comprehensive privacy law that

1006
00:48:19,933 --> 00:48:21,666
gives us guarantees.

1007
00:48:21,733 --> 00:48:22,699
Here is the challenge.

1008
00:48:22,766 --> 00:48:25,233
Most of these privacy laws
aren't telling the government

1009
00:48:25,300 --> 00:48:25,966
what to do.

1010
00:48:26,033 --> 00:48:27,766
They're telling
companies what to do.

1011
00:48:27,833 --> 00:48:31,866
The government isn't super
eager to show up the cops, the

1012
00:48:31,933 --> 00:48:36,199
sheriffs, law enforcement, and
see their opportunity to get

1013
00:48:36,266 --> 00:48:37,266
data limited.

1014
00:48:37,333 --> 00:48:38,900
That's going to be a
really hard challenge.

1015
00:48:39,633 --> 00:48:40,466
Angelique.

1016
00:48:41,533 --> 00:48:43,732
>> ANGELIQUE: You mentioned
earlier that Europe's law, the

1017
00:48:43,800 --> 00:48:45,833
GDPR, is really
the gold standard.

1018
00:48:45,900 --> 00:48:47,800
Now that it's been
operationalized for almost a

1019
00:48:47,866 --> 00:48:51,033
couple of years, did Europe get
it right or are there pitfalls

1020
00:48:51,099 --> 00:48:53,766
we should be looking for as we
try and craft a federal law here

1021
00:48:53,833 --> 00:48:54,699
in the U.S.?

1022
00:48:55,500 --> 00:48:56,966
>> JULES POLONETSKY: I think
there are things we can learn

1023
00:48:57,033 --> 00:48:58,933
from what the Europeans did.

1024
00:48:59,000 --> 00:49:03,566
The GDPR has a lot of
sophistication and nuance in it.

1025
00:49:03,633 --> 00:49:07,566
The core point is you can't
collect any data unless you have

1026
00:49:07,633 --> 00:49:10,765
one of six legal
bases to collect data.

1027
00:49:10,833 --> 00:49:12,400
If not, it is illegal.

1028
00:49:12,466 --> 00:49:13,433
All right?

1029
00:49:13,500 --> 00:49:14,866
Here, we're kind of like, of
course, you can do it, just

1030
00:49:14,933 --> 00:49:18,133
don't do anything wrong, or what
rules do I need to follow, or

1031
00:49:18,199 --> 00:49:21,300
maybe I ought to ask permission
before I send you email.

1032
00:49:21,366 --> 00:49:25,699
The GDPR says, sorry, you don't
get to collect data unless you

1033
00:49:25,766 --> 00:49:27,966
have a legal basis to do so.

1034
00:49:28,033 --> 00:49:31,699
Each legal basis comes with a
different nuanced set of rules

1035
00:49:31,766 --> 00:49:35,599
if it is consent, which is only
allowed when it really is true

1036
00:49:35,666 --> 00:49:36,833
and fair consent.

1037
00:49:37,566 --> 00:49:41,466
So let me share with you an
inside data protection privacy

1038
00:49:41,533 --> 00:49:42,799
geek joke. Okay?

1039
00:49:42,866 --> 00:49:44,333
This is a true story.

1040
00:49:44,400 --> 00:49:48,766
A leading consulting firm asked
its employees - it wanted to pay

1041
00:49:48,833 --> 00:49:51,833
them and process benefits,
nothing special - it asked them

1042
00:49:51,900 --> 00:49:53,833
for consent to use their data.

1043
00:49:53,900 --> 00:49:57,533
So, in Europe, this room would
be howling because it is

1044
00:49:57,599 --> 00:49:59,866
well-known that you can't ask
employees for permission.

1045
00:49:59,933 --> 00:50:00,766
Of course, they're going to.

1046
00:50:00,833 --> 00:50:02,099
What, and not get paid?

1047
00:50:02,166 --> 00:50:05,099
Now it doesn't mean you can't
use the data, but the set of

1048
00:50:05,166 --> 00:50:08,666
legal bases that you use is a
very different one that comes

1049
00:50:08,733 --> 00:50:09,833
with different rights.

1050
00:50:09,900 --> 00:50:14,633
Each one, consent, contract,
legitimate interest, comes with

1051
00:50:14,699 --> 00:50:18,300
certain I can delete it, in
some places, I can revoke it.

1052
00:50:18,366 --> 00:50:20,466
In the U.S., we're rushing.

1053
00:50:20,533 --> 00:50:25,400
It took them seven years to
nuance the GDPR and to balance

1054
00:50:25,466 --> 00:50:30,133
these rights, and we're rushing
quickly to get it in place and

1055
00:50:30,199 --> 00:50:32,900
we're doing things that
are sometimes very blunt.

1056
00:50:32,966 --> 00:50:34,900
So, for instance, we want to
get people access to data.

1057
00:50:34,966 --> 00:50:35,866
Right?

1058
00:50:35,933 --> 00:50:36,766
It's their data.

1059
00:50:36,833 --> 00:50:38,099
They should have access to it.

1060
00:50:38,166 --> 00:50:41,033
But, of course, I need to
authenticate you because,

1061
00:50:41,099 --> 00:50:42,800
otherwise, you're creating a
security breach that's going to

1062
00:50:42,866 --> 00:50:43,933
get you in trouble.

1063
00:50:44,000 --> 00:50:45,566
Well, here's the messy thing.

1064
00:50:45,633 --> 00:50:47,732
There is lots of data I have
about people that is kind of

1065
00:50:47,800 --> 00:50:50,866
messy, it's not great, I
probably can't authenticate you,

1066
00:50:50,933 --> 00:50:53,833
maybe it's about your household
because I'm mailing things to

1067
00:50:53,900 --> 00:50:55,099
your house.

1068
00:50:55,166 --> 00:50:56,166
It's not perfect.

1069
00:50:56,233 --> 00:50:57,166
But guess what?

1070
00:50:57,233 --> 00:51:01,099
For a marketer, it's better
than random, so they do it.

1071
00:51:01,166 --> 00:51:02,033
Right?

1072
00:51:02,099 --> 00:51:04,400
There is data tied to
your cookies or the ad IDs of

1073
00:51:04,466 --> 00:51:05,299
your phone.

1074
00:51:05,366 --> 00:51:07,933
How do I properly balance?

1075
00:51:08,000 --> 00:51:10,699
Well, they've been dealing with
this in Europe for many years

1076
00:51:10,766 --> 00:51:13,766
and they understand that you
have rights, but at the same

1077
00:51:13,833 --> 00:51:16,966
time if it's going to impinge
the privacy of others, if it's

1078
00:51:17,033 --> 00:51:19,232
going to create a security risk,
then you have the right to turn

1079
00:51:19,300 --> 00:51:20,199
that down.

1080
00:51:20,266 --> 00:51:22,966
We're muddling our way
and I think there is a lot we

1081
00:51:23,033 --> 00:51:24,033
can learn.

1082
00:51:24,099 --> 00:51:29,733
One area where I hope Europe
will nuance, so the Europeans

1083
00:51:29,800 --> 00:51:34,166
last week, the president of the
European Commission said we lost

1084
00:51:34,233 --> 00:51:37,166
to the big global companies, the
Chinese companies, the American

1085
00:51:37,233 --> 00:51:39,400
companies, when it comes to
the direct to consumer data.

1086
00:51:39,466 --> 00:51:41,098
We can't lose.

1087
00:51:41,166 --> 00:51:43,300
We have got to have our own
digital champions for the next

1088
00:51:43,366 --> 00:51:45,966
generation, which is
machine learning and AI.

1089
00:51:46,033 --> 00:51:49,066
We're going to come up with a
regulated model because everyone

1090
00:51:49,133 --> 00:51:52,433
knows that if AI is not trusted,
it's terrible, it's scary, we'll

1091
00:51:52,500 --> 00:51:54,666
have to kill robots, we'll
have discriminatory things.

1092
00:51:54,733 --> 00:51:57,699
We are going to lead the world
by setting a regulated system

1093
00:51:57,766 --> 00:52:00,966
for trusted AI and
that's how Europe will lead.

1094
00:52:01,033 --> 00:52:03,433
And we're going to have
high-risk and we're going to

1095
00:52:03,500 --> 00:52:04,400
have low risk.

1096
00:52:04,466 --> 00:52:06,333
If you're in the high-risk
category, and it's not clear

1097
00:52:06,400 --> 00:52:09,900
fully exactly what ends up in
there, you may need to come and

1098
00:52:09,966 --> 00:52:12,133
pre-approve your
technologies before they can

1099
00:52:12,199 --> 00:52:13,166
get to the market.

1100
00:52:13,233 --> 00:52:15,366
That's a challenge. Right?

1101
00:52:15,433 --> 00:52:18,400
You certainly want laws that say
you can't do stuff wrong, but

1102
00:52:18,466 --> 00:52:21,333
the idea of maybe submitting
your technology for government

1103
00:52:21,400 --> 00:52:25,533
review is probably a slower
process and more restrictive

1104
00:52:25,599 --> 00:52:27,900
than we're likely to
want to see in the U.S.

1105
00:52:27,966 --> 00:52:31,966
I think there is a lot we can
learn and some good lessons, but

1106
00:52:32,033 --> 00:52:35,900
I think we would like to build
on what they did and keep a

1107
00:52:35,966 --> 00:52:38,500
little bit of the speed, not the
move fast and break stuff, but

1108
00:52:38,566 --> 00:52:43,366
move fast, think hard, get
things done so we get products

1109
00:52:43,433 --> 00:52:44,133
in the market.

1110
00:52:46,366 --> 00:52:48,833
>> AUDIENCE: Hey, so one of the
things I have been thinking

1111
00:52:48,900 --> 00:52:52,699
about is every time I interact
with anything, like let's say I

1112
00:52:52,766 --> 00:52:55,833
go get a haircut, they now
know that I got a haircut.

1113
00:52:56,866 --> 00:52:57,766
>> JULES POLONETSKY:
Who is the they?

1114
00:52:57,833 --> 00:53:00,166
>> AUDIENCE: Like the
barber, for example.

1115
00:53:00,233 --> 00:53:04,300
Everyone I interact with,
everyone I transact with, even

1116
00:53:04,366 --> 00:53:07,800
friends I would take a photo and
they have that photo, what does

1117
00:53:07,866 --> 00:53:12,000
the privacy community think
about the fact that I might not

1118
00:53:12,066 --> 00:53:16,732
be the one giving away my data,
but every time I transact, I

1119
00:53:16,800 --> 00:53:20,800
have to at the very least
tell the other party that I'm

1120
00:53:20,866 --> 00:53:23,766
transacting with them
so they could give that.

1121
00:53:23,833 --> 00:53:25,199
>> JULES POLONETSKY: Here
is where the Europeans got

1122
00:53:25,266 --> 00:53:26,533
it right.

1123
00:53:26,599 --> 00:53:31,000
They don't say it is about is
the data public or did someone

1124
00:53:31,066 --> 00:53:31,866
else have it.

1125
00:53:31,933 --> 00:53:33,166
It doesn't matter.

1126
00:53:33,233 --> 00:53:36,400
Before someone can use data,
whoever they are, your barber,

1127
00:53:36,466 --> 00:53:40,866
your friend, your neighbor, they
need a legitimate basis to do

1128
00:53:40,933 --> 00:53:42,333
what they want to do with it.

1129
00:53:42,400 --> 00:53:45,333
Your barber maybe needs to send
you an email saying a month went

1130
00:53:45,400 --> 00:53:49,533
by or, hey, you didn't pay me,
here is the bill, or whatever.

1131
00:53:49,599 --> 00:53:50,766
That's a pretty
legitimate thing.

1132
00:53:50,833 --> 00:53:52,099
He's got a right to do it.

1133
00:53:52,166 --> 00:53:56,633
Can your barber take a database
of, let's see, my people, here

1134
00:53:56,699 --> 00:53:58,699
is their ethnicity, here is
their height, oh, this one looks

1135
00:53:58,766 --> 00:54:01,433
wealthy, this one pays with this
kind of credit card, and then

1136
00:54:01,500 --> 00:54:03,066
sell it to a data broker?

1137
00:54:03,133 --> 00:54:06,665
Wait a second, what exactly is
their basis for doing that?

1138
00:54:06,733 --> 00:54:09,333
They probably don't have a
good basis for doing that.

1139
00:54:10,033 --> 00:54:12,098
The Europeans actually
have a thoughtful model.

1140
00:54:12,166 --> 00:54:15,766
We in the U.S. are sort of
saying it is all opt-in or

1141
00:54:15,833 --> 00:54:16,800
all opt-out.

1142
00:54:16,866 --> 00:54:19,866
Obviously, there is some data
that needs to be shared for

1143
00:54:19,933 --> 00:54:22,766
certain legitimate
purposes to move transactions.

1144
00:54:22,833 --> 00:54:26,433
So we're trying to think of
every purpose and creating lists

1145
00:54:26,500 --> 00:54:30,266
and saying these, you always
need permission, or you always

1146
00:54:30,333 --> 00:54:33,133
get an opt-out,
but not security.

1147
00:54:33,199 --> 00:54:34,033
Okay, but not research.

1148
00:54:34,099 --> 00:54:35,433
Oh, but not marketing.

1149
00:54:35,500 --> 00:54:37,099
Okay, but some marketing. Right?

1150
00:54:37,166 --> 00:54:38,900
That's not a way to legislate,
right, because we're going to

1151
00:54:38,966 --> 00:54:39,966
miss something.

1152
00:54:40,033 --> 00:54:42,699
We're going to miss including
something that's a problem or

1153
00:54:42,766 --> 00:54:46,133
we're going to open up a hole
that's going to let stuff that

1154
00:54:46,199 --> 00:54:48,566
we don't want happen happen.

1155
00:54:48,633 --> 00:54:52,366
So hopefully that's an answer.

1156
00:54:52,433 --> 00:54:54,166
Anyway, how am I on time?

1157
00:54:54,233 --> 00:54:55,800
I am at time.

1158
00:54:55,866 --> 00:54:57,500
Thank you so much for
being a great audience.

1159
00:54:57,566 --> 00:54:59,533
Thank you for your attention.

