1
00:00:07,560 --> 00:00:09,129
- [Ad Announcer] No matter
what business you're in,

2
00:00:09,130 --> 00:00:11,360
digital transformation never stops.

3
00:00:11,360 --> 00:00:14,549
At Verizon, we keep companies
ready for what's next.

4
00:00:14,550 --> 00:00:16,640
We weave security into their business,

5
00:00:16,640 --> 00:00:18,320
virtualize their operations,

6
00:00:18,320 --> 00:00:20,570
and build AI customer experiences.

7
00:00:20,570 --> 00:00:21,740
We also keep them ready

8
00:00:21,740 --> 00:00:24,313
for the next big opportunity, like 5G.

9
00:00:25,280 --> 00:00:28,073
Almost all of the Fortune
500 partner with us.

10
00:00:29,060 --> 00:00:31,209
When it comes to digital transformation,

11
00:00:31,210 --> 00:00:33,153
we're here. And we're ready.

12
00:00:40,090 --> 00:00:41,980
- Hello, and welcome to

13
00:00:41,980 --> 00:00:45,120
Does Artificial Intelligence
Need a General Counsel?

14
00:00:45,120 --> 00:00:46,129
If you are joining us live,

15
00:00:46,130 --> 00:00:48,050
our speakers are in the cyto-discussion

16
00:00:48,050 --> 00:00:50,339
answering your questions right now.

17
00:00:50,340 --> 00:00:51,780
For audio and video issues,

18
00:00:51,780 --> 00:00:55,220
please click the technical
support button below.

19
00:00:55,220 --> 00:00:58,300
I'd now like to turn it over
to Alan Brill and Paul Jackson,

20
00:00:58,300 --> 00:00:59,403
for the presentation.

21
00:01:02,740 --> 00:01:04,540
- Hi, my name is Alan Brill,

22
00:01:04,540 --> 00:01:06,240
and I'm a Senior Managing Director

23
00:01:06,240 --> 00:01:08,999
in the Cyber Risk Practice at Kroll,

24
00:01:08,999 --> 00:01:11,366
which is a division of Duff & Phelps.

25
00:01:11,367 --> 00:01:13,920
For this presentation,
I'm pleased to be joined

26
00:01:13,920 --> 00:01:16,600
by my friend and colleague, Paul Jackson,

27
00:01:16,600 --> 00:01:18,949
who leads our Cyber Risk Practice

28
00:01:18,950 --> 00:01:21,843
in the Asia Pacific region. Paul?

29
00:01:25,100 --> 00:01:27,070
- Thank you, Alan. In this presentation,

30
00:01:27,070 --> 00:01:29,029
we will be covering five areas.

31
00:01:29,030 --> 00:01:31,100
Firstly, where artificial intelligence

32
00:01:31,100 --> 00:01:33,250
is being used in business.

33
00:01:33,250 --> 00:01:35,600
Secondly, AI's popularity.

34
00:01:35,600 --> 00:01:38,399
Thirdly, what could and has gone wrong.

35
00:01:38,400 --> 00:01:40,640
Fourthly, how to course correct.

36
00:01:40,640 --> 00:01:43,383
And lastly, we'll wrap up
with some key takeaways.

37
00:01:44,810 --> 00:01:47,970
So, AI is being used
in business, of course,

38
00:01:47,970 --> 00:01:50,420
and increasingly in all types of industry

39
00:01:50,420 --> 00:01:52,560
and for a myriad of applications.

40
00:01:52,560 --> 00:01:55,670
And logically in areas such
as customer experience,

41
00:01:55,670 --> 00:01:59,240
where we have non-human
interactions with customers.

42
00:01:59,240 --> 00:02:04,240
R&D, we're enhancing and
speeding up research capability.

43
00:02:04,640 --> 00:02:07,430
Finance, where we're
helping to make faster

44
00:02:07,430 --> 00:02:10,509
and more accurate trading decisions, etc.

45
00:02:10,509 --> 00:02:12,149
But perhaps of most concern,

46
00:02:12,150 --> 00:02:14,330
as we will touch on in this presentation,

47
00:02:14,330 --> 00:02:17,000
is where AI predicts human behavior.

48
00:02:17,000 --> 00:02:21,640
Risks, and legal concerns,
begin to present themselves.

49
00:02:21,640 --> 00:02:23,260
So, by way of introduction,

50
00:02:23,260 --> 00:02:26,130
AI and its usage is only
really getting bigger,

51
00:02:26,130 --> 00:02:29,359
and as McKinsey says, by 2030,

52
00:02:29,360 --> 00:02:32,220
the average simulation shows that some 70%

53
00:02:32,220 --> 00:02:34,010
of companies might have adopted

54
00:02:34,010 --> 00:02:36,420
at least one type of AI technology.

55
00:02:36,420 --> 00:02:40,309
And marketsandmarkets.com
predict that the AI market

56
00:02:40,310 --> 00:02:45,310
will grow to 190 billion
in dollar industry by 2025,

57
00:02:45,690 --> 00:02:47,270
which is pretty staggering.

58
00:02:47,270 --> 00:02:48,890
And this is driven by the fact

59
00:02:48,890 --> 00:02:52,029
that 83% of businesses say that AI

60
00:02:52,030 --> 00:02:55,790
is a strategic priority
for their businesses today.

61
00:02:55,790 --> 00:02:57,912
So, as a starting point,

62
00:02:58,860 --> 00:03:00,870
Why built AI?

63
00:03:00,870 --> 00:03:02,070
Well, obviously if we think

64
00:03:02,070 --> 00:03:03,930
about traditional computer programming,

65
00:03:03,930 --> 00:03:05,860
it follows clearly-defined rules,

66
00:03:05,860 --> 00:03:08,560
so in other words, it does
what the human tells it to.

67
00:03:10,330 --> 00:03:12,340
It-- Sorry, let's go back to that.

68
00:03:12,340 --> 00:03:14,650
It's the basic tenet,

69
00:03:14,650 --> 00:03:17,360
if corporate counsels
obviously understand this,

70
00:03:17,360 --> 00:03:18,960
you know, it's based on rules,

71
00:03:18,960 --> 00:03:21,180
but they, you know,

72
00:03:21,180 --> 00:03:24,010
they are-- they believe
they're tech savvy,

73
00:03:24,010 --> 00:03:25,489
and they acknowledge their comfort level

74
00:03:25,490 --> 00:03:27,230
and confidence with technology.

75
00:03:27,230 --> 00:03:29,980
But, they have limitations
when artificial intelligence

76
00:03:29,980 --> 00:03:31,640
is making decisions for them,

77
00:03:31,640 --> 00:03:33,070
and making decisions on behalf

78
00:03:33,070 --> 00:03:35,033
of their companies and organizations.

79
00:03:36,126 --> 00:03:37,517
A quote,

80
00:03:37,517 --> 00:03:40,140
"Everything we love about
civilization is the product

81
00:03:40,140 --> 00:03:43,577
of intelligence, so amplifying
our human intelligence

82
00:03:43,578 --> 00:03:47,160
with artificial intelligence
has the potential of helping

83
00:03:47,160 --> 00:03:49,730
civilization flourish
like never before..."

84
00:03:49,730 --> 00:03:51,446
But, this has the proviso

85
00:03:51,446 --> 00:03:55,240
that we need to keep
technology beneficial,

86
00:03:55,240 --> 00:03:57,710
and we need to understand
the risks and concerns.

87
00:03:57,710 --> 00:03:59,440
Now, from my perspective

88
00:03:59,440 --> 00:04:01,630
as the APAC head of Cyber Risk Control,

89
00:04:01,630 --> 00:04:03,423
I have particular focus on where AI

90
00:04:03,423 --> 00:04:06,083
is being used in cybersecurity.

91
00:04:07,570 --> 00:04:10,769
You know, as a logical use case for this,

92
00:04:10,770 --> 00:04:14,938
detection and behavioral
monitoring of cybersecurity threats

93
00:04:14,938 --> 00:04:17,190
is a logical use case.

94
00:04:17,190 --> 00:04:19,380
At its strongest tiers,

95
00:04:19,380 --> 00:04:22,130
AI can help to predict malicious behavior

96
00:04:22,130 --> 00:04:24,300
and how a human might try to adapt

97
00:04:24,300 --> 00:04:27,230
to defeat security controls
that we have in place.

98
00:04:27,230 --> 00:04:29,320
It still needs to overcome the issues

99
00:04:29,320 --> 00:04:32,550
of falsely learning good behavior,

100
00:04:32,550 --> 00:04:35,260
and identifying that good
behavior as malicious

101
00:04:35,260 --> 00:04:37,570
or flagging that good
behavior as malicious,

102
00:04:37,570 --> 00:04:40,130
and we see in prototypes at the moment,

103
00:04:40,130 --> 00:04:42,450
that this is indeed the case.

104
00:04:42,450 --> 00:04:44,770
Where AI still has a
long way to go, though,

105
00:04:44,770 --> 00:04:47,560
in my line of work in particular,

106
00:04:47,560 --> 00:04:49,700
is in human behavior.

107
00:04:49,700 --> 00:04:53,000
So, employee investigations for example,

108
00:04:53,000 --> 00:04:56,476
defining when someone is
guilty or showing deceit,

109
00:04:56,476 --> 00:05:00,210
identifying or predicting
which humans might

110
00:05:00,210 --> 00:05:04,419
because a cause of risk
to an organization,

111
00:05:04,420 --> 00:05:07,270
or even to society in general.

112
00:05:07,270 --> 00:05:08,490
Let's look at inside a threat,

113
00:05:08,490 --> 00:05:11,220
because in an age of crippling penalties,

114
00:05:11,220 --> 00:05:14,200
for data loss incidents,
coupled with the high cost

115
00:05:14,200 --> 00:05:17,210
of reactive investigations,
after the fact,

116
00:05:17,210 --> 00:05:19,849
it's little wonder that
organizations are looking

117
00:05:19,850 --> 00:05:22,900
for more effective programs to proactively

118
00:05:22,900 --> 00:05:24,739
detect insider threats.

119
00:05:24,740 --> 00:05:27,470
Many such programs simply
seek to block capability

120
00:05:27,470 --> 00:05:29,440
to leak sensitive information,

121
00:05:29,440 --> 00:05:31,330
but to a percentage of organizations,

122
00:05:31,330 --> 00:05:35,400
they're now looking towards
AI for something more robust.

123
00:05:35,400 --> 00:05:37,549
Tools that can predict employee risk

124
00:05:37,550 --> 00:05:39,653
before it actually occurs,

125
00:05:40,850 --> 00:05:42,620
such tools could be very intrusive,

126
00:05:42,620 --> 00:05:44,400
you might think of them as big brother,

127
00:05:44,400 --> 00:05:47,960
minority report style tools,

128
00:05:47,960 --> 00:05:50,260
such as psycholinguistics,

129
00:05:50,260 --> 00:05:53,150
which is a prime example
where profiling users

130
00:05:53,150 --> 00:05:55,320
or employees based on how they communicate

131
00:05:55,320 --> 00:06:00,190
and how they write is
becoming an interesting field

132
00:06:00,190 --> 00:06:02,219
of predicting human behavior.

133
00:06:02,220 --> 00:06:04,520
On a broader scale, we
see on the societal level

134
00:06:04,520 --> 00:06:08,270
increased use of AI to detect
risks to public security.

135
00:06:08,270 --> 00:06:10,159
Perhaps most high-profile

136
00:06:10,160 --> 00:06:13,433
is the Chinese social scoring policy.

137
00:06:14,280 --> 00:06:15,419
We've read a lot about this,

138
00:06:15,420 --> 00:06:17,340
and I'm sure you've seen this in news,

139
00:06:17,340 --> 00:06:20,539
this combines factors such as
physical public activities,

140
00:06:20,540 --> 00:06:24,320
to activities online,
such as user social media,

141
00:06:24,320 --> 00:06:26,560
to actually define which individuals

142
00:06:26,560 --> 00:06:28,580
may be a risk to society.

143
00:06:28,580 --> 00:06:31,680
So, what are the risks
of getting this wrong?

144
00:06:31,680 --> 00:06:34,230
How might inaccuracies and biases

145
00:06:34,230 --> 00:06:37,840
affect critical decision-making
within our organizations?

146
00:06:37,840 --> 00:06:39,130
I'm going to turn it over to Alan now,

147
00:06:39,130 --> 00:06:40,582
to explain more about this.

148
00:06:43,230 --> 00:06:44,720
- Thank you.

149
00:06:44,720 --> 00:06:48,770
Well, Paul has talked
about all of the benefits

150
00:06:48,770 --> 00:06:51,539
that we've seen in
artificial intelligence.

151
00:06:51,540 --> 00:06:55,277
But, the real question
is "What could go wrong?"

152
00:06:56,670 --> 00:07:01,360
As Paul said, the systems
that are being built,

153
00:07:01,360 --> 00:07:05,760
are being built with
the best of intentions,

154
00:07:05,760 --> 00:07:06,593
but as we all know,

155
00:07:06,593 --> 00:07:11,593
sometimes good intentions lead
to less than good results.

156
00:07:12,180 --> 00:07:13,430
I'll give you an example.

157
00:07:14,460 --> 00:07:19,460
This is a photograph of a
weapons system used by the Navy,

158
00:07:19,720 --> 00:07:23,010
it's called a Phalanx, or sea wiz,

159
00:07:23,010 --> 00:07:25,340
close-in weapon system.

160
00:07:25,340 --> 00:07:28,869
It's a Gatling gun-style cannon,

161
00:07:28,870 --> 00:07:32,030
which can fire over 1000 rounds a minute

162
00:07:33,050 --> 00:07:36,310
of depleted uranium slugs.

163
00:07:36,310 --> 00:07:40,550
It is designed to knock
out in-bound missiles.

164
00:07:40,550 --> 00:07:44,470
So, somebody shoots a
missile at your ship,

165
00:07:44,470 --> 00:07:48,580
the artificial intelligence
in this weapon system,

166
00:07:48,580 --> 00:07:52,630
sees that on radar, fires the gun,

167
00:07:52,630 --> 00:07:57,630
and literally merges the
outgoing rounds on the radar

168
00:07:58,840 --> 00:08:01,803
to the incoming missile and takes it out.

169
00:08:03,100 --> 00:08:04,490
And it's very effective.

170
00:08:04,490 --> 00:08:09,140
Unfortunately, a few years
ago in one live-fire exercise,

171
00:08:09,140 --> 00:08:12,130
once the system was set on automatic,

172
00:08:12,130 --> 00:08:17,130
it saw an incoming missile,
which was fired as a test,

173
00:08:17,130 --> 00:08:18,719
and it hit it.

174
00:08:18,720 --> 00:08:20,990
But the 99% percent of shells

175
00:08:20,990 --> 00:08:24,063
that missed the missile, flew on.

176
00:08:25,180 --> 00:08:29,060
And they shredded the bridge
of another naval vessel

177
00:08:29,060 --> 00:08:31,960
unfortunately killing at least one

178
00:08:31,960 --> 00:08:35,539
of the naval officers on
the bridge at the time.

179
00:08:35,539 --> 00:08:38,030
Now clearly that was never intended,

180
00:08:38,030 --> 00:08:39,579
but apparently what had happened,

181
00:08:39,580 --> 00:08:42,100
was that in the design of the system,

182
00:08:42,100 --> 00:08:46,270
there wasn't a lot of thought
given to rounds that missed.

183
00:08:46,270 --> 00:08:49,160
Only to "how do we get it to hit?"

184
00:08:49,160 --> 00:08:53,719
So, when the rounds were
missing, they flew on.

185
00:08:53,720 --> 00:08:55,920
And the system wasn't really looking

186
00:08:55,920 --> 00:08:58,358
out beyond the target to see

187
00:08:58,358 --> 00:09:03,349
what might be hit, when
the bullets missed,

188
00:09:03,350 --> 00:09:04,543
as most of them did.

189
00:09:06,670 --> 00:09:09,319
Now, I'd like to tell
you about another system

190
00:09:09,320 --> 00:09:12,160
used by a financial services company,

191
00:09:12,160 --> 00:09:16,850
and this system was designed
to make decisions about loans.

192
00:09:16,850 --> 00:09:20,290
It would be able to
read a loan application,

193
00:09:20,290 --> 00:09:24,370
and determine whether or not
that loan was a good risk.

194
00:09:24,370 --> 00:09:27,363
Defining a good risk, is
it was going to be repaid.

195
00:09:28,250 --> 00:09:33,250
They gave the system 250,000
records of prior loans,

196
00:09:33,890 --> 00:09:37,990
including both the loan
application and the results,

197
00:09:37,990 --> 00:09:42,490
that is, was the loan paid when
it was supposed to be paid.

198
00:09:42,490 --> 00:09:47,490
And the software had to
take those 250,000 records

199
00:09:47,500 --> 00:09:50,952
and analyze them to
determine which factors

200
00:09:50,952 --> 00:09:55,952
were most related to the
likelihood of successful repayment.

201
00:09:56,750 --> 00:09:59,940
Sounds like a perfectly
reasonable thing to do.

202
00:09:59,940 --> 00:10:02,460
Well, the system worked on it,

203
00:10:02,460 --> 00:10:07,200
and one of the things that
it figured out by itself,

204
00:10:07,200 --> 00:10:11,300
was that one of the most important factors

205
00:10:11,300 --> 00:10:14,300
for whether or not a loan would be repaid,

206
00:10:14,300 --> 00:10:16,315
was the postal code

207
00:10:16,316 --> 00:10:20,383
indicating where the loan applicant lived.

208
00:10:21,290 --> 00:10:25,860
And, as a result, the AI
then placed greater weight

209
00:10:25,860 --> 00:10:30,550
in loan approval or denial
based upon nothing more

210
00:10:30,550 --> 00:10:34,512
than the address in the loan application.

211
00:10:35,400 --> 00:10:37,470
And, as I said, that
sounds like, you know,

212
00:10:37,470 --> 00:10:39,760
it's a reasonable thing to do, you said,

213
00:10:39,760 --> 00:10:41,620
you want to have the loans repaid,

214
00:10:41,620 --> 00:10:44,810
this system said, well people in this area

215
00:10:44,810 --> 00:10:46,334
don't repay their loans,

216
00:10:46,334 --> 00:10:49,920
it weighted heavier, you give fewer loans.

217
00:10:49,920 --> 00:10:53,949
Unfortunately, that's not
a very good thing to do.

218
00:10:53,950 --> 00:10:55,130
In the United States,

219
00:10:55,130 --> 00:11:00,010
there's a law called The Community
Reinvestment Act of 1977,

220
00:11:00,010 --> 00:11:02,189
so it's been around a long time,

221
00:11:02,190 --> 00:11:05,010
and that makes it illegal
to base lending decisions

222
00:11:05,010 --> 00:11:08,470
on the neighborhood where a person lives.

223
00:11:08,470 --> 00:11:11,630
Without it, what would
have been happening,

224
00:11:11,630 --> 00:11:14,100
was that financial institutions

225
00:11:14,100 --> 00:11:16,489
would put up a map on the wall,

226
00:11:16,489 --> 00:11:19,992
and they would outline in red pen

227
00:11:19,992 --> 00:11:24,180
those areas of minority communities,

228
00:11:24,180 --> 00:11:28,272
and they denied them
access to loan services.

229
00:11:29,190 --> 00:11:33,650
That turned out to be
what was called Redlining,

230
00:11:33,650 --> 00:11:38,252
and this system, because
it was not told about that,

231
00:11:39,680 --> 00:11:43,719
immediately learned how to do E-redlining.

232
00:11:43,720 --> 00:11:45,373
And that was disastrous.

233
00:11:49,390 --> 00:11:51,250
Why does this happen?

234
00:11:51,250 --> 00:11:55,190
Well, I think if you step back,

235
00:11:55,190 --> 00:11:59,700
you'll see that organizations
often bring in AI experts

236
00:11:59,700 --> 00:12:00,900
to built their systems,

237
00:12:00,900 --> 00:12:03,569
and they either bring
them in as employees,

238
00:12:03,570 --> 00:12:07,300
or they bring them in as contractors.

239
00:12:07,300 --> 00:12:10,410
But those experts are experts on the world

240
00:12:10,410 --> 00:12:13,290
of artificial intelligence
and deep learning,

241
00:12:13,290 --> 00:12:16,329
they're not necessarily going
to be subject matter experts,

242
00:12:16,330 --> 00:12:19,460
they're not necessarily going
to know what the laws are,

243
00:12:19,460 --> 00:12:23,100
and the rules and the
regulations relating to loans.

244
00:12:23,100 --> 00:12:26,240
They're focused on building a system

245
00:12:27,294 --> 00:12:30,833
that does what they're
told it needs to do.

246
00:12:31,870 --> 00:12:34,990
Historically, they do not focus,

247
00:12:34,990 --> 00:12:37,640
or even necessarily think,

248
00:12:37,640 --> 00:12:41,083
about the legal or regulatory issues,

249
00:12:42,100 --> 00:12:44,700
or the contractual agreements

250
00:12:44,700 --> 00:12:48,933
that a financial services
institution may be subject to.

251
00:12:50,620 --> 00:12:55,300
If you have a system that is
not going to follow the law,

252
00:12:55,300 --> 00:12:57,469
not going to follow a regulation,

253
00:12:57,470 --> 00:13:02,340
I would argue that however
good that system seems to be,

254
00:13:02,340 --> 00:13:04,050
in a conceptual sense,

255
00:13:04,050 --> 00:13:07,573
it's going to be a failure in operations.

256
00:13:08,460 --> 00:13:13,010
So, the question is, when
you're developing an AI system,

257
00:13:13,010 --> 00:13:18,010
or as an AI developer,
can you really afford

258
00:13:18,700 --> 00:13:22,840
to hope that your development
team figures this out,

259
00:13:22,840 --> 00:13:25,193
or somehow research it themselves.

260
00:13:26,250 --> 00:13:29,220
And I think the answer
has got to come down

261
00:13:29,220 --> 00:13:31,763
on the side of not a good idea.

262
00:13:34,290 --> 00:13:37,543
Now, that's a danger, but
here's another danger.

263
00:13:39,960 --> 00:13:42,540
We've seen, over and over again,

264
00:13:42,540 --> 00:13:47,540
that bias can creep into the
system during its development,

265
00:13:49,770 --> 00:13:52,163
particularly for deep learning systems,

266
00:13:53,050 --> 00:13:56,370
and that the standard
practices in computer science,

267
00:13:56,370 --> 00:13:59,200
that we use in developing the system,

268
00:13:59,200 --> 00:14:02,766
aren't really designed to even notice

269
00:14:02,766 --> 00:14:05,030
that this is happening.

270
00:14:05,030 --> 00:14:06,890
Let me give you an example.

271
00:14:06,890 --> 00:14:08,773
A facial recognition system,

272
00:14:08,773 --> 00:14:12,620
that was built for the use of the police,

273
00:14:12,620 --> 00:14:15,620
was 99% accurate.

274
00:14:15,620 --> 00:14:19,180
But it turned out that
it was only 99% accurate

275
00:14:19,180 --> 00:14:21,479
when the image that it was searching for

276
00:14:21,480 --> 00:14:23,430
was that of a white male.

277
00:14:23,430 --> 00:14:27,520
It was substantially less
accurate in identifying women,

278
00:14:27,520 --> 00:14:28,733
or people of color.

279
00:14:29,590 --> 00:14:32,970
This turned out to be because
the enormous training set

280
00:14:32,970 --> 00:14:36,300
used for the system, consisted mostly

281
00:14:36,300 --> 00:14:38,943
of photographs of white males.

282
00:14:42,000 --> 00:14:43,210
In the last few days,

283
00:14:43,210 --> 00:14:45,910
we've actually seen something
very interesting happen.

284
00:14:47,660 --> 00:14:50,560
We've seen that Amazon has made a decision

285
00:14:50,560 --> 00:14:52,880
that its facial recognition software

286
00:14:53,800 --> 00:14:57,750
will not be used by police
departments for the next year,

287
00:14:57,750 --> 00:15:01,140
because they want to have a better handle

288
00:15:01,140 --> 00:15:04,390
on how to make sure that
the system is accurate,

289
00:15:04,390 --> 00:15:08,699
and that it's not doing
too many false positives

290
00:15:08,700 --> 00:15:11,283
or false negatives in its operation.

291
00:15:12,400 --> 00:15:16,140
Now, another example involved Amazon,

292
00:15:16,140 --> 00:15:18,980
which had developed a new AI system

293
00:15:18,980 --> 00:15:21,570
to review the resumes of people

294
00:15:21,570 --> 00:15:23,940
that wanted to work for the company.

295
00:15:23,940 --> 00:15:28,110
Now, looking obviously to
select the top-level talent,

296
00:15:28,110 --> 00:15:29,100
and there were a lot of people

297
00:15:29,100 --> 00:15:31,060
that wanted to work for Amazon.

298
00:15:31,060 --> 00:15:34,839
So, they spent a lot of
money and a lot of time

299
00:15:34,840 --> 00:15:36,610
building a system.

300
00:15:36,610 --> 00:15:39,950
And they trained it, given the resumes

301
00:15:39,950 --> 00:15:44,130
that had been submitted to
them over the past 10 years.

302
00:15:44,130 --> 00:15:47,600
And most of those, we
understand, were from men.

303
00:15:47,600 --> 00:15:51,000
So, the AI system, in its training mode,

304
00:15:51,000 --> 00:15:53,143
as it was running through these,

305
00:15:54,160 --> 00:15:58,270
penalized resumes that
including the word "women's".

306
00:15:58,270 --> 00:16:03,270
It also downgraded graduates
of two all-women's colleges,

307
00:16:04,560 --> 00:16:07,453
simply because of where
they went to school.

308
00:16:09,240 --> 00:16:10,990
That was unacceptable.

309
00:16:10,990 --> 00:16:15,330
The system was apparently
never used in live operation,

310
00:16:15,330 --> 00:16:18,250
and ultimately it was abandoned

311
00:16:18,250 --> 00:16:21,530
after spending all that
money, all that time,

312
00:16:21,530 --> 00:16:23,063
and all that effort.

313
00:16:24,240 --> 00:16:26,050
So, what can we do?

314
00:16:26,050 --> 00:16:29,589
How can we course correct our AI systems?

315
00:16:29,590 --> 00:16:31,030
Well first,

316
00:16:31,030 --> 00:16:33,000
let's get some additional experts

317
00:16:33,000 --> 00:16:36,858
involved in defining
and building AI systems.

318
00:16:36,858 --> 00:16:40,170
To include experts on the law,

319
00:16:40,170 --> 00:16:44,979
what is it that a system is
lawfully able to do and not do?

320
00:16:44,980 --> 00:16:48,720
Let's look at compliance
and regulatory specialists,

321
00:16:48,720 --> 00:16:51,270
like compliance officers,

322
00:16:51,270 --> 00:16:54,257
they're usually focused on things like:

323
00:16:54,257 --> 00:16:57,959
"What are the regulations that
the system has to follow?"

324
00:16:57,960 --> 00:17:00,310
and "What are the corporate rules

325
00:17:00,310 --> 00:17:03,260
that govern any system that's built?

326
00:17:03,260 --> 00:17:07,160
Things that the company
wants and doesn't want.

327
00:17:07,160 --> 00:17:08,569
Well, let's get them involved.

328
00:17:08,569 --> 00:17:13,569
And, then let's look at
how we build data sets

329
00:17:13,849 --> 00:17:16,899
for AI training and later for testing.

330
00:17:16,900 --> 00:17:18,490
Can we make sure that they're large

331
00:17:18,490 --> 00:17:20,050
but even more than large,

332
00:17:20,050 --> 00:17:22,609
can we make sure that they're diverse?

333
00:17:22,609 --> 00:17:24,520
The rule that I think you need to follow,

334
00:17:24,520 --> 00:17:28,850
is that your training
and testing data sets

335
00:17:28,850 --> 00:17:33,760
have to be as diverse as
the ultimate population

336
00:17:33,760 --> 00:17:35,890
that you're going to be looking at.

337
00:17:35,890 --> 00:17:38,880
Looking at a subset doesn't help.

338
00:17:38,880 --> 00:17:42,090
And in fact, we've seen a number of cases

339
00:17:42,090 --> 00:17:44,939
in which a very large database

340
00:17:44,940 --> 00:17:47,460
was literally split in half,

341
00:17:47,460 --> 00:17:50,031
with half being used for testing,

342
00:17:50,031 --> 00:17:52,540
half being used for training.

343
00:17:52,540 --> 00:17:53,850
Well, given that the two parts

344
00:17:53,850 --> 00:17:56,480
were from the original same database,

345
00:17:56,480 --> 00:17:58,140
they had the same bias.

346
00:17:58,140 --> 00:18:01,805
So, the testing didn't notice the bias.

347
00:18:01,805 --> 00:18:04,200
So, what we want to do,

348
00:18:04,200 --> 00:18:08,010
is get the legal, the compliance,
and the regulatory experts

349
00:18:08,010 --> 00:18:11,120
involved in defining
and building a system.

350
00:18:11,120 --> 00:18:12,939
If we look at this circle,

351
00:18:12,940 --> 00:18:17,070
it represents the technical
capabilities of a system,

352
00:18:17,070 --> 00:18:22,035
that is, what a system could
do if it was allowed to do it.

353
00:18:22,036 --> 00:18:24,470
Well, if we get the compliance

354
00:18:24,470 --> 00:18:26,670
and regulatory people involved,

355
00:18:26,670 --> 00:18:29,865
they're going to be able to
define things that a system

356
00:18:29,865 --> 00:18:34,865
could do, like limit access by
geographic areas, redlining,

357
00:18:38,450 --> 00:18:43,380
and say no, those are
not authorized actions.

358
00:18:43,380 --> 00:18:48,380
Their job, in part, is
to define what it is

359
00:18:49,740 --> 00:18:53,750
that limits the action of the AI system.

360
00:18:53,750 --> 00:18:58,750
And to a large extent, also,
say if something bad happened,

361
00:18:59,050 --> 00:19:02,060
what evidence would I want to show

362
00:19:02,060 --> 00:19:04,393
that the system had worked correctly?

363
00:19:06,760 --> 00:19:11,070
So, think of it as legal
and compliance building

364
00:19:11,070 --> 00:19:15,230
a fence within the
capability of the AI system,

365
00:19:15,230 --> 00:19:20,110
that limits the freedom of
action of the AI system.

366
00:19:20,110 --> 00:19:23,770
You can think of it as
having areas that are safe,

367
00:19:23,770 --> 00:19:26,370
areas that are unsafe, and some areas

368
00:19:26,370 --> 00:19:28,551
that are questionable in which the system

369
00:19:28,551 --> 00:19:33,551
may seek guidance from
its human overseers.

370
00:19:36,480 --> 00:19:40,500
So, we want to collect the right data,

371
00:19:40,500 --> 00:19:42,340
we want to make sure

372
00:19:42,340 --> 00:19:45,470
that we're framing the problem correctly,

373
00:19:45,470 --> 00:19:50,470
not making it too general,
too generic, too large.

374
00:19:50,980 --> 00:19:55,980
We want to be able to answer
the important questions of,

375
00:19:55,987 --> 00:19:58,300
"Is the system operating within

376
00:19:58,300 --> 00:20:00,846
the set of laws and regulations

377
00:20:00,846 --> 00:20:05,090
and contractual and corporate provisions?"

378
00:20:05,090 --> 00:20:08,087
or "Is it off in the wild?"

379
00:20:08,087 --> 00:20:11,430
"Will our training set support this,

380
00:20:11,430 --> 00:20:14,620
and will our testing data set let us know,

381
00:20:14,620 --> 00:20:17,286
if this isn't, in fact, happening?"

382
00:20:19,400 --> 00:20:23,233
Dealing with implicit bias
can be very very difficult.

383
00:20:24,130 --> 00:20:28,380
Because it's something that
we may have grown up with,

384
00:20:28,380 --> 00:20:31,552
it's something that we may
not even realize that we have,

385
00:20:31,552 --> 00:20:34,500
and you need to be able to step back,

386
00:20:34,500 --> 00:20:37,700
and ask the question "Is there a problem,

387
00:20:37,700 --> 00:20:41,130
is there something that
I should be noticing

388
00:20:41,130 --> 00:20:43,210
when the system is being tested,

389
00:20:43,210 --> 00:20:46,017
when the system is in operation?"

390
00:20:47,850 --> 00:20:50,679
For example, an algorithm that's designed

391
00:20:50,680 --> 00:20:54,090
for one purpose, might
have been repurposed

392
00:20:54,090 --> 00:20:56,850
to speed up development of a system.

393
00:20:56,850 --> 00:20:59,300
And that's something that
can often be a problem.

394
00:21:00,980 --> 00:21:05,250
Ultimately, even though the
people developing the system

395
00:21:05,250 --> 00:21:09,470
are really technicians, with a
tremendous depth of knowledge

396
00:21:09,470 --> 00:21:13,700
in the technology, they
need to be able to also stop

397
00:21:13,700 --> 00:21:16,360
and think about things like fairness.

398
00:21:16,360 --> 00:21:20,439
Is what the system doing, moral?

399
00:21:20,440 --> 00:21:22,460
Is it optimal?

400
00:21:22,460 --> 00:21:24,570
Is it biased?

401
00:21:24,570 --> 00:21:26,919
And, how can we avoid that?

402
00:21:26,920 --> 00:21:28,730
How can we look look for it,

403
00:21:28,730 --> 00:21:30,223
how can we notice it?

404
00:21:31,310 --> 00:21:34,919
So, we've talked about a number of issues,

405
00:21:34,920 --> 00:21:36,380
but I think it's important

406
00:21:36,380 --> 00:21:39,570
that we give you some takeaways

407
00:21:39,570 --> 00:21:41,470
that we think will be helpful to you

408
00:21:41,470 --> 00:21:43,830
when you're involved in the development

409
00:21:43,830 --> 00:21:45,580
of an AI system.

410
00:21:45,580 --> 00:21:46,850
Paul?

411
00:21:46,850 --> 00:21:47,683
- Thank you, Alan.

412
00:21:47,683 --> 00:21:51,230
Yes, to wrap up this presentation,

413
00:21:51,230 --> 00:21:53,020
a few key takeaways.

414
00:21:53,020 --> 00:21:54,091
So firstly,

415
00:21:54,092 --> 00:21:57,540
AI system are being built in public

416
00:21:57,540 --> 00:21:59,300
and private sector systems.

417
00:21:59,300 --> 00:22:02,480
Yeah, we must accept that AI is growing,

418
00:22:02,480 --> 00:22:04,200
and it's not a subject that any of us

419
00:22:04,200 --> 00:22:05,560
can really afford to ignore,

420
00:22:05,560 --> 00:22:06,629
now or in the future,

421
00:22:06,630 --> 00:22:08,290
and indeed, we should be looking,

422
00:22:08,290 --> 00:22:10,379
as Alan said earlier in the presentation,

423
00:22:10,380 --> 00:22:12,630
to embrace this kind of technology

424
00:22:12,630 --> 00:22:14,830
and the myriad of benefits

425
00:22:14,830 --> 00:22:18,710
that it can bring to us in
our business and daily lives.

426
00:22:18,710 --> 00:22:20,650
Now, secondly,

427
00:22:20,650 --> 00:22:24,708
AI, being built by computer
data science specialists,

428
00:22:24,708 --> 00:22:27,811
generally have no, have the
potential for no knowledge

429
00:22:27,811 --> 00:22:30,550
of the legal and social science areas.

430
00:22:30,550 --> 00:22:32,889
The need to understand legal compliance

431
00:22:32,890 --> 00:22:36,210
and evidence considerations
really has parallels

432
00:22:36,210 --> 00:22:39,715
with how we operate in the
cybersecurity world in general,

433
00:22:39,715 --> 00:22:41,530
you know, in our line of business,

434
00:22:41,530 --> 00:22:45,410
we do encourage the
involvement of business,

435
00:22:45,410 --> 00:22:48,160
of legal compliance, of risk managers

436
00:22:48,160 --> 00:22:50,280
at the very early stages of development

437
00:22:50,280 --> 00:22:53,240
of cybersecurity systems
in order to ensure

438
00:22:53,240 --> 00:22:57,370
that they are both adding
security to an organization,

439
00:22:57,370 --> 00:22:59,449
but also enhancing the business.

440
00:22:59,450 --> 00:23:00,840
This is really important,

441
00:23:00,840 --> 00:23:02,240
and in the same way,

442
00:23:02,240 --> 00:23:05,240
AI should have the involvement
right from the get-go,

443
00:23:05,240 --> 00:23:06,640
of the different lines of business,

444
00:23:06,640 --> 00:23:08,900
and make sure it fits in
with the business model,

445
00:23:08,900 --> 00:23:10,610
and make sure it also matches

446
00:23:10,610 --> 00:23:13,053
the risk tolerance of an organization.

447
00:23:17,060 --> 00:23:21,360
Thirdly, we need to identify problems

448
00:23:21,360 --> 00:23:22,879
during design and build,

449
00:23:22,880 --> 00:23:27,086
it's far better to
anticipate potential risk,

450
00:23:27,086 --> 00:23:32,086
ethical, and legal issues during
the design and build phase,

451
00:23:32,800 --> 00:23:36,312
in order to avoid predictable problems.

452
00:23:37,660 --> 00:23:40,920
And lastly, as Alan has explained,

453
00:23:40,920 --> 00:23:43,720
AI can include bias.

454
00:23:43,720 --> 00:23:47,350
We need to be careful with
training and testing data,

455
00:23:47,350 --> 00:23:51,070
but we have to emphasize that, done well,

456
00:23:51,070 --> 00:23:53,072
the benefits of AI are compelling,

457
00:23:53,073 --> 00:23:56,990
however done wrongly,
they can introduce legal,

458
00:23:56,990 --> 00:23:59,912
and regulatory challenges
that may end up costing

459
00:23:59,912 --> 00:24:03,170
the organization far
more than it cost them

460
00:24:03,170 --> 00:24:06,128
to implement the AI in the first place.

461
00:24:06,128 --> 00:24:08,210
I think, you know, it's fair to say

462
00:24:08,210 --> 00:24:10,270
that we'd really all like to imagine

463
00:24:10,270 --> 00:24:14,020
a world where we're able
to predict and eliminate

464
00:24:14,020 --> 00:24:16,400
human error or maliciousness

465
00:24:16,400 --> 00:24:20,606
in preventing insider threat
and potential data loss.

466
00:24:20,606 --> 00:24:23,680
This is obviously something
that's close to my heart,

467
00:24:23,680 --> 00:24:26,850
and to Alan's heart, as
we, in our day-to-day work,

468
00:24:26,850 --> 00:24:29,040
we look to address cyber risk,

469
00:24:29,040 --> 00:24:32,190
we look to help organizations to mature,

470
00:24:32,190 --> 00:24:34,420
and to protect their employees,

471
00:24:34,420 --> 00:24:35,940
to protect their clients, customers,

472
00:24:35,940 --> 00:24:38,283
and protect their critical information.

473
00:24:39,360 --> 00:24:40,979
It is probably one of
the most common things

474
00:24:40,980 --> 00:24:43,590
that we as a firm
investigate, data breaches,

475
00:24:43,590 --> 00:24:46,830
and you know, anything that can be used,

476
00:24:46,830 --> 00:24:48,610
any technologies which will help us

477
00:24:48,610 --> 00:24:51,897
along the path of enhancing
security is a good thing.

478
00:24:51,897 --> 00:24:54,044
An outcome, that really in our world

479
00:24:54,045 --> 00:24:56,900
of addressing cybersecurity risk,

480
00:24:56,900 --> 00:25:00,942
is certainly something that
we all want to strive towards.

481
00:25:00,942 --> 00:25:03,650
I'd like to thank you
all for joining us today,

482
00:25:03,650 --> 00:25:07,021
and we look forward to
answering any questions

483
00:25:07,021 --> 00:25:10,320
that you may have,
following this presentation.

484
00:25:10,320 --> 00:25:11,320
Thank you very much.

