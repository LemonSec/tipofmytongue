1
00:00:09,260 --> 00:00:13,050
- Hello, and welcome to
Social Cybersecurity:

2
00:00:13,050 --> 00:00:15,080
Social Influence and Design

3
00:00:15,080 --> 00:00:17,470
In End-User Cybersecurity.

4
00:00:17,470 --> 00:00:18,869
If you're joining us live,

5
00:00:18,870 --> 00:00:21,300
our speaker is in the
Slido chat discussion

6
00:00:21,300 --> 00:00:24,230
answering your questions right now.

7
00:00:24,230 --> 00:00:26,170
For audio video issues,

8
00:00:26,170 --> 00:00:29,180
click the technical support button below.

9
00:00:29,180 --> 00:00:31,580
I'd now like to turn it over to Sauvik Das

10
00:00:31,580 --> 00:00:32,683
for the presentation.

11
00:00:36,370 --> 00:00:39,059
- [Sauvik] Thank you for
that introduction Casey,

12
00:00:39,060 --> 00:00:42,200
as she mentioned I'm
Sauvik, I am faculty at

13
00:00:42,200 --> 00:00:44,670
the School of Interactive
Computing at Georgia Tech,

14
00:00:44,670 --> 00:00:46,370
and I'm excited to speak to y'all

15
00:00:46,370 --> 00:00:48,793
about my research on
social cybersecurity today.

16
00:00:49,950 --> 00:00:52,170
So, before we get into the details,

17
00:00:52,170 --> 00:00:53,450
just a little bit about myself,

18
00:00:53,450 --> 00:00:55,820
I joined faculty at Georgia Tech in,

19
00:00:55,820 --> 00:00:58,500
about two and a half years ago now, wow.

20
00:00:58,500 --> 00:01:01,510
I got my P.h.D. from
Carnegie Mellon in 2017,

21
00:01:01,510 --> 00:01:03,800
and at Georgia Tech I direct the SPUD lab,

22
00:01:03,800 --> 00:01:07,210
which stands for Security,
Privacy, Usability, and Design,

23
00:01:07,210 --> 00:01:09,259
and the work we do is sort
of at this intersection

24
00:01:09,260 --> 00:01:12,130
of HCI, cybersecurity, and data science,

25
00:01:12,130 --> 00:01:14,070
which I hope will help contextualize

26
00:01:14,070 --> 00:01:16,820
the sort of things that
you're gonna hear about today.

27
00:01:16,820 --> 00:01:18,729
So, and again, I'd like
to remind everybody

28
00:01:18,730 --> 00:01:20,530
that I am currently on the Slido chat,

29
00:01:20,530 --> 00:01:24,400
so please do ask me
questions if you have them.

30
00:01:24,400 --> 00:01:28,100
So I'd like to start my talk
today with this question,

31
00:01:28,100 --> 00:01:29,970
how can we design systems that encourage

32
00:01:29,970 --> 00:01:31,970
better cybersecurity behaviors?

33
00:01:31,970 --> 00:01:32,823
And I think this is an important question

34
00:01:32,823 --> 00:01:36,070
because of how foundational
security is to computing.

35
00:01:36,070 --> 00:01:38,229
Our email, our money, our creative work,

36
00:01:38,230 --> 00:01:40,240
all the photos we take with
our friends and loved ones

37
00:01:40,240 --> 00:01:42,399
are all ultimately protected or not

38
00:01:42,400 --> 00:01:45,300
by the cybersecurity
decisions we decide to make

39
00:01:45,300 --> 00:01:47,283
on a day to day basis.

40
00:01:48,140 --> 00:01:51,870
And so it's no surprise
that the cybercrime industry

41
00:01:51,870 --> 00:01:53,800
has sort of evolved to exploit

42
00:01:53,800 --> 00:01:56,483
end users' cybersecurity
behaviors in that way.

43
00:01:57,430 --> 00:02:01,280
One McAfee estimate puts
the global annual damages

44
00:02:01,280 --> 00:02:04,310
at approximately 600 billion
U.S. dollars every year,

45
00:02:04,310 --> 00:02:06,850
with as many as two thirds
of all internet users

46
00:02:06,850 --> 00:02:09,829
having personal data
stolen in some capacity.

47
00:02:09,830 --> 00:02:11,980
And the Kaspersky estimates suggest that

48
00:02:11,980 --> 00:02:14,869
as many as 323 thousand new malware files

49
00:02:14,870 --> 00:02:17,670
are being produced every single day.

50
00:02:17,670 --> 00:02:19,220
But the kicker is that

51
00:02:19,220 --> 00:02:22,760
pretty much all of this
industry would be hamstrung

52
00:02:22,760 --> 00:02:25,790
if end users ended up using sort of

53
00:02:25,790 --> 00:02:28,109
expert recommended security
and privacy advice,

54
00:02:28,110 --> 00:02:30,100
like keeping their software up to date,

55
00:02:30,100 --> 00:02:32,170
using two-factor on important accounts,

56
00:02:32,170 --> 00:02:33,510
or using a password manager,

57
00:02:33,510 --> 00:02:36,470
and regularly updating
their passwords, right?

58
00:02:36,470 --> 00:02:37,967
But when we think about end user security

59
00:02:37,967 --> 00:02:39,250
and privacy behaviors

60
00:02:39,250 --> 00:02:42,800
we're confronted with more
sobering statistics, right?

61
00:02:42,800 --> 00:02:47,250
Like, one estimate from a
google software engineer

62
00:02:47,250 --> 00:02:50,530
in his USENIX ENIGMA talk
in 2018 suggested that

63
00:02:50,530 --> 00:02:52,330
fewer than 10 % of google consumers

64
00:02:52,330 --> 00:02:54,320
enabled two-factor authentication.

65
00:02:54,320 --> 00:02:56,890
Similarly the Pew Research
Center suggests that

66
00:02:56,890 --> 00:02:59,200
only about 12 % of U.S. internet users

67
00:02:59,200 --> 00:03:00,480
use password managers,

68
00:03:00,480 --> 00:03:04,019
and approximately only
22 % of smartphone users

69
00:03:04,020 --> 00:03:05,690
lock their screens and keep their phones

70
00:03:05,690 --> 00:03:07,120
completely up to date.

71
00:03:07,120 --> 00:03:10,360
And Android in their own estimation,

72
00:03:10,360 --> 00:03:12,540
suggests that as of
May 2019, suggests that

73
00:03:12,540 --> 00:03:16,242
only 2.3 % of their users use the latest

74
00:03:16,242 --> 00:03:20,192
minor version of the
Android operating system.

75
00:03:21,090 --> 00:03:24,520
So, we don't really think
in the end user context

76
00:03:24,520 --> 00:03:26,630
of those expert recommended
security behaviors

77
00:03:26,630 --> 00:03:29,030
as much as we think
about people, you know,

78
00:03:29,030 --> 00:03:31,230
sharing passwords with their
friends and loved ones,

79
00:03:31,230 --> 00:03:34,340
or ignoring software updates
because they're inconvenient,

80
00:03:34,340 --> 00:03:37,010
or falling for scams that seem
like they should be obvious,

81
00:03:37,010 --> 00:03:40,670
or propping electronically
locked doors with garbage cans.

82
00:03:40,670 --> 00:03:44,910
And I say this not to cast
blame or shame on these users,

83
00:03:44,910 --> 00:03:47,720
in fact I've actually probably
done each one of these things

84
00:03:47,720 --> 00:03:49,260
at some point in my life.

85
00:03:49,260 --> 00:03:51,649
Rather, I think all of these end users'

86
00:03:51,650 --> 00:03:53,757
behaviors and decisions
sort of makes sense

87
00:03:53,757 --> 00:03:55,860
in the context in which they sit,

88
00:03:55,860 --> 00:03:57,520
but it does speak to this disconnect

89
00:03:57,520 --> 00:04:00,840
between how experts want
people to use secure systems,

90
00:04:00,840 --> 00:04:03,690
and how end users actually
use those systems.

91
00:04:03,690 --> 00:04:06,120
And so we need to do
better as a community.

92
00:04:06,120 --> 00:04:09,450
So in 2014, I wanted to figure out

93
00:04:09,450 --> 00:04:10,679
a little bit more about this, right?

94
00:04:10,680 --> 00:04:12,730
I interviewed people and I asked people,

95
00:04:12,730 --> 00:04:14,190
what makes them use PIN on their phone?

96
00:04:14,190 --> 00:04:15,762
Or enable two-factor authentication?

97
00:04:15,762 --> 00:04:18,279
Or keep their software up to date?

98
00:04:18,279 --> 00:04:20,813
So I'm gonna be reading a few
quotes from that interview.

99
00:04:22,727 --> 00:04:24,060
"I started using a PIN

100
00:04:24,060 --> 00:04:25,750
because everyone around me had a PIN,

101
00:04:25,750 --> 00:04:27,180
so I kind of felt a group pressure

102
00:04:27,180 --> 00:04:28,427
to also use a PIN."

103
00:04:29,537 --> 00:04:31,409
"One of my boys wanted to use my phone,

104
00:04:31,410 --> 00:04:32,960
so I gave them my passcode.

105
00:04:32,960 --> 00:04:35,359
And not that I have
anything that I don't care

106
00:04:35,360 --> 00:04:36,510
for them to see or anything,

107
00:04:36,510 --> 00:04:39,087
but after they did that
then I changed it."

108
00:04:39,087 --> 00:04:40,620
"My friends have a lot
of different accounts,

109
00:04:40,620 --> 00:04:41,453
the same as me.

110
00:04:41,453 --> 00:04:42,659
But they didn't get into any trouble.

111
00:04:42,660 --> 00:04:44,420
So I think maybe it will not be dangerous

112
00:04:44,420 --> 00:04:45,820
to reuse passwords."

113
00:04:45,820 --> 00:04:48,610
Now did anybody notice a trend? Right.

114
00:04:48,610 --> 00:04:49,817
It's all social.

115
00:04:53,510 --> 00:04:55,170
And it turns out that security behaviors,

116
00:04:55,170 --> 00:04:56,490
like any human behaviors,

117
00:04:56,490 --> 00:04:59,233
is primarily driven by social influence.

118
00:05:00,370 --> 00:05:04,410
So, more broadly, in the
interview study that we conducted,

119
00:05:04,410 --> 00:05:07,660
approximately 50 % of all the end users'

120
00:05:07,660 --> 00:05:11,330
security and privacy behaviors
that were reported to us

121
00:05:11,330 --> 00:05:13,440
seemed to be driven to us by
some sort of social trigger,

122
00:05:13,440 --> 00:05:14,940
for example observing somebody else

123
00:05:14,940 --> 00:05:17,240
or having a conversation with a friend.

124
00:05:17,240 --> 00:05:18,780
And that might seem like a lot,

125
00:05:18,780 --> 00:05:20,409
but it shouldn't be that surprising.

126
00:05:20,410 --> 00:05:22,150
Social psychologists have for decades,

127
00:05:22,150 --> 00:05:23,510
provided empirical evidence

128
00:05:23,510 --> 00:05:25,890
that human beings are
largely social creatures,

129
00:05:25,890 --> 00:05:27,984
and that much of our
behavior can be explained

130
00:05:27,984 --> 00:05:31,020
by the social influences
that we face everyday.

131
00:05:31,020 --> 00:05:33,250
And yet we rarely consider
this social dimension

132
00:05:33,250 --> 00:05:35,173
in the design of our security systems.

133
00:05:36,560 --> 00:05:38,610
And I'm here to argue
that absent knowledge

134
00:05:38,610 --> 00:05:41,020
of how social influences
affect security behaviors

135
00:05:41,020 --> 00:05:43,873
and vice versa, we have little
hope of doing much better.

136
00:05:45,030 --> 00:05:47,320
So today, I want to share
with you two projects

137
00:05:47,320 --> 00:05:49,370
that I did in collaboration with Facebook

138
00:05:49,370 --> 00:05:50,790
to shed some light on

139
00:05:50,790 --> 00:05:53,680
how social influences affect
cybersecurity behaviors.

140
00:05:53,680 --> 00:05:54,580
And by the end of it all,

141
00:05:54,580 --> 00:05:56,680
I hope to have convinced
you of the following.

142
00:05:56,680 --> 00:05:59,320
Social influences drive
cybersecurity behaviors,

143
00:05:59,320 --> 00:06:01,740
and we can encourage better
cybersecurity behaviors

144
00:06:01,740 --> 00:06:03,693
by making cybersecurity more social.

145
00:06:06,390 --> 00:06:08,750
So let's start with that first
project that I mentioned,

146
00:06:08,750 --> 00:06:11,083
measuring social influence
in cybersecurity.

147
00:06:12,070 --> 00:06:13,270
So, of course the first step

148
00:06:13,270 --> 00:06:14,830
in making cybersecurity more social

149
00:06:14,830 --> 00:06:17,010
is to understand how
social influences affect

150
00:06:17,010 --> 00:06:19,280
cybersecurity behaviors
in the first place.

151
00:06:19,280 --> 00:06:21,530
And I was fortunate enough
to partner with Facebook

152
00:06:21,530 --> 00:06:22,969
in analyzing this.

153
00:06:22,970 --> 00:06:25,400
Specifically we analyzed
how the use and non-use

154
00:06:25,400 --> 00:06:27,140
of three optional security tools

155
00:06:27,140 --> 00:06:29,520
was affected by friends'
use of the same tools

156
00:06:29,520 --> 00:06:31,870
for about 1.5 million social

157
00:06:31,870 --> 00:06:34,260
Facebook user's social networks.

158
00:06:34,260 --> 00:06:37,490
So the three tools that we
studied were login notifications,

159
00:06:37,490 --> 00:06:39,210
which just sends users notifications

160
00:06:39,210 --> 00:06:40,940
about suspicious logins,

161
00:06:40,940 --> 00:06:42,860
login approvals, which
is Facebook's version

162
00:06:42,860 --> 00:06:44,550
of two-factor authentication,

163
00:06:44,550 --> 00:06:48,310
and if those first two security
tools seem a little bit more

164
00:06:48,310 --> 00:06:50,420
sort of, the standard
offering that you would expect

165
00:06:50,420 --> 00:06:52,360
a company like Facebook to provide,

166
00:06:52,360 --> 00:06:54,300
the second one, or,

167
00:06:54,300 --> 00:06:55,800
the third one, trusted contacts,

168
00:06:55,800 --> 00:06:57,580
is a little bit more social, right?

169
00:06:57,580 --> 00:06:59,000
So with trusted contacts,

170
00:06:59,000 --> 00:07:01,158
you specify three to 5 of your friends

171
00:07:01,158 --> 00:07:02,650
who can vouch for your identity

172
00:07:02,650 --> 00:07:04,820
if you ever lose access to your account.

173
00:07:05,660 --> 00:07:10,280
So we collected 750,000
data from 750,000 users

174
00:07:10,280 --> 00:07:12,700
who newly adopted one
of these security tools,

175
00:07:12,700 --> 00:07:15,621
as well as data from
750,000 counterbalancing

176
00:07:15,621 --> 00:07:17,609
so called use-nots,

177
00:07:17,610 --> 00:07:20,120
who had never adopted one
of these security tools.

178
00:07:20,120 --> 00:07:21,230
And then my research question

179
00:07:21,230 --> 00:07:24,210
really boiled down to this
idea, can we distinguish between

180
00:07:24,210 --> 00:07:27,229
which of our users was a security user,

181
00:07:27,230 --> 00:07:29,130
and which was a security use-not,

182
00:07:29,130 --> 00:07:32,409
based on the presence of social
influence in their network.

183
00:07:32,410 --> 00:07:33,970
And to do this, I used a technique

184
00:07:33,970 --> 00:07:36,290
called matched propensity
sampling analysis.

185
00:07:36,290 --> 00:07:38,990
I'm not gonna get into all
of the details of this today,

186
00:07:38,990 --> 00:07:41,690
but at a high level,
there are these sort of

187
00:07:41,690 --> 00:07:44,040
two steps you want to keep in mind.

188
00:07:44,040 --> 00:07:45,023
The first step,

189
00:07:47,150 --> 00:07:49,440
is that for each security tool,

190
00:07:49,440 --> 00:07:52,090
you want to empirically
select exposure levels

191
00:07:52,090 --> 00:07:54,539
to friends who use that
particular security tool.

192
00:07:54,540 --> 00:07:56,680
For example, for login notifications,

193
00:07:56,680 --> 00:07:59,140
you might pick 1 %, 5 %, or 10 %,

194
00:07:59,140 --> 00:08:00,960
and what these exposure levels let us do

195
00:08:00,960 --> 00:08:04,483
is they let us split our
user base into two sets.

196
00:08:05,770 --> 00:08:08,250
One set of users who are
exposed to that level,

197
00:08:08,250 --> 00:08:09,790
and one set of users who are not,

198
00:08:09,790 --> 00:08:11,940
so the exposed and the unexposed.

199
00:08:11,940 --> 00:08:14,080
So exposed users have at least

200
00:08:14,080 --> 00:08:17,080
5 % of their friends who
use login notifications,

201
00:08:17,080 --> 00:08:20,099
and unexposed users do
not have at least 5 %

202
00:08:20,100 --> 00:08:22,760
of their friends who
use login notifications.

203
00:08:22,760 --> 00:08:25,010
Then, for each of these exposure levels,

204
00:08:25,010 --> 00:08:27,560
you can compare the adoption
rate of those two groups.

205
00:08:27,560 --> 00:08:29,250
The exposed and the unexposed.

206
00:08:29,250 --> 00:08:32,820
So, are people who are
exposed at the 5 % level

207
00:08:32,820 --> 00:08:34,760
more likely or less likely

208
00:08:34,760 --> 00:08:37,010
to adopt login notifications themselves?

209
00:08:37,010 --> 00:08:38,850
And of course there's a
bunch of other hidden tricks

210
00:08:38,850 --> 00:08:41,169
going on in the background
using propensity sampling

211
00:08:41,169 --> 00:08:44,589
in order to make sure that
the comparison is more fair.

212
00:08:44,590 --> 00:08:45,970
I'm not gonna get into the details of that

213
00:08:45,970 --> 00:08:46,940
in the presentation,

214
00:08:46,940 --> 00:08:48,501
but I am, again, in the Slido,

215
00:08:48,501 --> 00:08:51,180
so feel free to ask questions there

216
00:08:51,180 --> 00:08:54,000
and hopefully I can answer your questions.

217
00:08:54,000 --> 00:08:55,130
But the broad idea here is

218
00:08:55,130 --> 00:08:56,920
that difference between the adoption rate

219
00:08:56,920 --> 00:09:00,939
of the exposed versus the
unexposed group of people,

220
00:09:00,940 --> 00:09:04,350
is sort of our proxy effect
for social influence.

221
00:09:04,350 --> 00:09:06,570
And the way you can think
about that working is that if

222
00:09:06,570 --> 00:09:10,383
we have this chart which is,

223
00:09:12,910 --> 00:09:16,120
hold on, okay, I think you can see it now,

224
00:09:16,120 --> 00:09:18,290
so if we have this chart where

225
00:09:18,290 --> 00:09:22,480
the x axis is the
aforementioned exposure levels

226
00:09:22,480 --> 00:09:23,717
that I mentioned before, you know,

227
00:09:23,717 --> 00:09:25,646
1 %, 5 %, or what have you,

228
00:09:25,647 --> 00:09:28,570
and the y axis is the
aforementioned proxy measure

229
00:09:28,570 --> 00:09:30,957
for social influence that
we were talking about,

230
00:09:30,957 --> 00:09:33,340
the exposed minus the
unexposed adoption rate,

231
00:09:33,340 --> 00:09:35,390
the social influence
had absolutely no effect

232
00:09:35,390 --> 00:09:36,860
due to this flat line at zero,

233
00:09:36,860 --> 00:09:38,490
because that suggests
that there's no difference

234
00:09:38,490 --> 00:09:39,550
in the adoption rate between

235
00:09:39,550 --> 00:09:40,916
those who do have a
certain amount of friends

236
00:09:40,917 --> 00:09:43,700
who use that tool, and those who do not.

237
00:09:43,700 --> 00:09:46,620
More likely however, you're likely to see

238
00:09:47,490 --> 00:09:50,370
sort of this line that
goes up and to the right,

239
00:09:50,370 --> 00:09:52,400
and this is what we call
the expected effect,

240
00:09:52,400 --> 00:09:55,670
the hypothesized effect,
because there's a ton of work

241
00:09:55,670 --> 00:09:57,829
in the social psychology
literature that suggests

242
00:09:57,830 --> 00:09:59,240
that the more likely your friend,

243
00:09:59,240 --> 00:10:01,330
the more of your friends who do something,

244
00:10:01,330 --> 00:10:03,650
the more likely you are
to do that same thing.

245
00:10:03,650 --> 00:10:05,579
And so we might expect to see something

246
00:10:05,580 --> 00:10:07,880
like this up and to the
right curve up here,

247
00:10:07,880 --> 00:10:09,113
but do we actually see?

248
00:10:09,960 --> 00:10:12,100
So if we plot the results
for trusted contacts,

249
00:10:12,100 --> 00:10:15,550
we see something that looks a
lot like the expected effect,

250
00:10:15,550 --> 00:10:17,370
social influence has a positive effect

251
00:10:17,370 --> 00:10:19,240
even at low levels of exposure,

252
00:10:19,240 --> 00:10:21,400
and that effect becomes
increasingly positive

253
00:10:21,400 --> 00:10:22,953
as you're exposed to more
and more of your friends

254
00:10:22,953 --> 00:10:24,209
who use trusted contacts.

255
00:10:24,210 --> 00:10:25,043
And that's great,

256
00:10:25,043 --> 00:10:27,430
that's some of the first
empirical evidence that we have

257
00:10:27,430 --> 00:10:28,900
that social influence effects

258
00:10:28,900 --> 00:10:31,130
cybersecurity and privacy behaviors.

259
00:10:31,130 --> 00:10:34,420
But when we start to plot the results

260
00:10:34,420 --> 00:10:37,069
for login approvals and
login notifications,

261
00:10:37,070 --> 00:10:40,000
we see something a bit more nuanced.

262
00:10:40,000 --> 00:10:42,450
Specifically what we see is that

263
00:10:42,450 --> 00:10:45,850
in the bottom half of
this graph over here,

264
00:10:45,850 --> 00:10:47,210
hold on,

265
00:10:47,210 --> 00:10:51,480
where we see this green
area that I'm highlighting,

266
00:10:51,480 --> 00:10:54,520
that's where social influence
has a negative effect

267
00:10:54,520 --> 00:10:56,750
on cybersecurity and privacy behaviors.

268
00:10:56,750 --> 00:10:59,520
So what's going on over there? Right?

269
00:10:59,520 --> 00:11:02,010
That seems incredibly counterintuitive.

270
00:11:02,010 --> 00:11:03,345
Why would social influence,

271
00:11:03,345 --> 00:11:05,560
why would it be that
the more of your friends

272
00:11:05,560 --> 00:11:06,880
use a particular tool,

273
00:11:06,880 --> 00:11:09,720
the less likely you are
to use that tool yourself.

274
00:11:09,720 --> 00:11:12,410
So in talking to my friends in marketing,

275
00:11:12,410 --> 00:11:15,660
and in social psychology,

276
00:11:15,660 --> 00:11:18,959
it turns out there might be
an effect that explains this,

277
00:11:18,960 --> 00:11:20,961
and it's called disaffiliation.

278
00:11:20,961 --> 00:11:23,679
And disaffiliation occurs when

279
00:11:23,679 --> 00:11:27,209
an uncool user group
starts using your product.

280
00:11:27,210 --> 00:11:32,060
So for example, when teenagers
stopped using Facebook

281
00:11:32,060 --> 00:11:33,939
when parents started
using Facebook, right,

282
00:11:33,940 --> 00:11:37,030
because all of a sudden it
stopped being a cool platform.

283
00:11:37,030 --> 00:11:40,540
Now, the early adopters of
security and privacy tools,

284
00:11:40,540 --> 00:11:42,860
tend to be perceived by non experts,

285
00:11:42,860 --> 00:11:44,710
so people who might be a
little bit overzealous,

286
00:11:44,710 --> 00:11:46,814
a little bit paranoid,
or a little bit nutty,

287
00:11:46,814 --> 00:11:49,900
and in turn this paranoia and nuttiness

288
00:11:49,900 --> 00:11:52,310
might stigmatize the use
of those security tools.

289
00:11:52,310 --> 00:11:53,699
'Cause if the only people you see

290
00:11:53,700 --> 00:11:55,140
using two-factor authentication

291
00:11:55,140 --> 00:11:57,480
are people who you feel
are a little bit paranoid,

292
00:11:57,480 --> 00:11:59,770
you're unlikely to use two-factor
authentication yourself,

293
00:11:59,770 --> 00:12:01,853
because maybe it's only
for paranoid people,

294
00:12:01,853 --> 00:12:03,729
and you're not a paranoid person.

295
00:12:03,730 --> 00:12:06,160
However, there are two
pieces of good news here.

296
00:12:06,160 --> 00:12:08,079
The first, is that all of these effects

297
00:12:08,080 --> 00:12:09,680
go up and to the right.

298
00:12:09,680 --> 00:12:12,609
And that means that more exposure is good.

299
00:12:12,610 --> 00:12:15,140
The more of your friends who
use a particular security tool,

300
00:12:15,140 --> 00:12:17,770
the more positive the
effect of social influence.

301
00:12:17,770 --> 00:12:19,670
It's just that for standard security tools

302
00:12:19,670 --> 00:12:22,800
like two-factor authentication
and login notifications,

303
00:12:22,800 --> 00:12:24,439
the effect remains negative

304
00:12:24,440 --> 00:12:27,230
until very high levels of exposure.

305
00:12:27,230 --> 00:12:28,790
But the second piece of good news,

306
00:12:28,790 --> 00:12:30,290
is that the design of a security tool

307
00:12:30,290 --> 00:12:33,089
appears to affect its
potential for social spread.

308
00:12:33,090 --> 00:12:35,730
The trusted contacts are
the more social tool,

309
00:12:35,730 --> 00:12:37,967
and it never experienced
of the stigmatic effect

310
00:12:37,967 --> 00:12:41,120
that I observed with the other two

311
00:12:41,120 --> 00:12:43,170
cybersecurity and privacy tools.

312
00:12:43,170 --> 00:12:44,870
And when speaking with the people

313
00:12:44,870 --> 00:12:46,320
who designed trusted contacts,

314
00:12:46,320 --> 00:12:48,810
as well as the people
who use trusted contacts,

315
00:12:48,810 --> 00:12:50,550
it truly came down to
these three dimensions

316
00:12:50,550 --> 00:12:53,623
of observability,
cooperation, stewardship.

317
00:12:53,624 --> 00:12:55,710
Observability captures this idea that

318
00:12:55,710 --> 00:12:57,450
people can see when you use it.

319
00:12:57,450 --> 00:12:58,860
When you specify your three friends

320
00:12:58,860 --> 00:12:59,980
who are your trusted contacts,

321
00:12:59,980 --> 00:13:02,070
they get a little notification
that makes them feel good,

322
00:13:02,070 --> 00:13:03,140
and alerts them to the fact

323
00:13:03,140 --> 00:13:05,360
that this is a tool that they can use too.

324
00:13:05,360 --> 00:13:06,870
Cooperation captures this idea that

325
00:13:06,870 --> 00:13:09,640
people like to work together
towards something, right?

326
00:13:09,640 --> 00:13:12,199
So with trusted contacts, your
friends are working together

327
00:13:12,200 --> 00:13:15,370
to provide greater security for you.

328
00:13:15,370 --> 00:13:18,270
Whereas with the traditional
cybersecurity tools,

329
00:13:18,270 --> 00:13:19,360
everybody's sort of cloistered

330
00:13:19,360 --> 00:13:20,640
in their own little digital bubble

331
00:13:20,640 --> 00:13:22,010
and doing what they can for themselves

332
00:13:22,010 --> 00:13:23,450
and only for themselves.

333
00:13:23,450 --> 00:13:25,240
And then finally stewardship
captures this idea

334
00:13:25,240 --> 00:13:29,130
that people like to act on
their concern for other people.

335
00:13:29,130 --> 00:13:31,160
So in this case, with trusted contacts,

336
00:13:31,160 --> 00:13:33,430
your friends are acting in your benefit,

337
00:13:33,430 --> 00:13:35,520
whereas with traditional
cybersecurity and privacy tools,

338
00:13:35,520 --> 00:13:37,470
there's no such concept of acting

339
00:13:37,470 --> 00:13:39,610
on behalf of somebody else.

340
00:13:39,610 --> 00:13:42,590
So, social influence
drives security behaviors,

341
00:13:42,590 --> 00:13:45,660
and design affects the
potential for social spread.

342
00:13:45,660 --> 00:13:47,459
But what can we do with this knowledge

343
00:13:47,460 --> 00:13:49,470
to actually improve cybersecurity

344
00:13:49,470 --> 00:13:51,740
and privacy behaviors more radically?

345
00:13:51,740 --> 00:13:54,950
So, it's really hard to make, to retrofit

346
00:13:54,950 --> 00:13:56,770
cooperation and stewardship

347
00:13:56,770 --> 00:13:59,319
into existing cybersecurity
and privacy tools,

348
00:13:59,320 --> 00:14:01,460
but you can definitely
make them more observable,

349
00:14:01,460 --> 00:14:03,900
and that's the hypothesis
that I wanted to test

350
00:14:03,900 --> 00:14:06,610
with my next study.

351
00:14:06,610 --> 00:14:10,450
So essentially what I did
was I ran a randomized

352
00:14:10,450 --> 00:14:14,450
between subjects experiment
with 50,000 Facebook users.

353
00:14:14,450 --> 00:14:17,000
Now the experiment was rather simple,

354
00:14:17,000 --> 00:14:18,760
I essentially piggy backed on

355
00:14:18,760 --> 00:14:21,100
Facebook's annual security
awareness campaign,

356
00:14:21,100 --> 00:14:23,030
which shows people on
the top of their newsfeed

357
00:14:23,030 --> 00:14:25,480
a little notification that
alerts them to the fact

358
00:14:25,480 --> 00:14:27,650
that extra security settings exist.

359
00:14:27,650 --> 00:14:29,880
Now before I came in and joined the team,

360
00:14:29,880 --> 00:14:32,090
Facebook was gonna use this
very vanilla notification,

361
00:14:32,090 --> 00:14:33,200
that just alerted people

362
00:14:33,200 --> 00:14:36,060
to the presence of extra
cybersecurity tools,

363
00:14:36,060 --> 00:14:38,010
there was nothing special about it,

364
00:14:38,010 --> 00:14:40,270
just a simple notification.

365
00:14:40,270 --> 00:14:41,984
When I joined the team, I decided,

366
00:14:41,984 --> 00:14:44,630
can we test social proof cues?

367
00:14:44,630 --> 00:14:47,010
Now I tested a variety
of social proof cues,

368
00:14:47,010 --> 00:14:48,770
I'm not gonna go into each
and every one of them,

369
00:14:48,770 --> 00:14:52,370
but you can see sort of the
spread on this slide over here.

370
00:14:52,370 --> 00:14:54,130
The raw number condition
just showed people

371
00:14:54,130 --> 00:14:55,197
the exact number of their friends

372
00:14:55,197 --> 00:14:57,233
who use extra security tools, and,

373
00:14:58,239 --> 00:14:59,680
of the way then
incentivized them to click.

374
00:14:59,680 --> 00:15:02,870
And then all the way down
to the some of your friends,

375
00:15:02,870 --> 00:15:05,147
which was very vague and
just informed people that

376
00:15:05,147 --> 00:15:08,030
you know, there were other
people in their friend group

377
00:15:08,030 --> 00:15:11,270
who also used these tools.

378
00:15:11,270 --> 00:15:14,060
And that they should check
out the tools as well.

379
00:15:14,060 --> 00:15:15,760
So essentially we had eight conditions,

380
00:15:15,760 --> 00:15:17,700
seven social variations

381
00:15:17,700 --> 00:15:20,040
that I very briefly spoke
about in the previous slide,

382
00:15:20,040 --> 00:15:22,170
as well as one non-social control.

383
00:15:22,170 --> 00:15:27,110
And we assigned randomly,
6,250 participants

384
00:15:27,110 --> 00:15:29,060
to each of these conditions,

385
00:15:29,060 --> 00:15:31,579
so the 50,000 participants
I talked about before,

386
00:15:31,580 --> 00:15:33,820
and we ran the experiment
for about three days,

387
00:15:33,820 --> 00:15:37,571
such that any of our participant users

388
00:15:37,571 --> 00:15:39,980
who logged into Facebook
in those three days

389
00:15:39,980 --> 00:15:44,980
saw the notification that
they were assigned to

390
00:15:45,610 --> 00:15:47,720
at the top of their newsfeed.

391
00:15:47,720 --> 00:15:51,120
So to measure the effectiveness of these

392
00:15:52,210 --> 00:15:54,663
you know, security and
privacy announcements,

393
00:15:54,663 --> 00:15:57,912
we calculated these
three different measures.

394
00:15:57,912 --> 00:15:59,597
The first was click through rate,

395
00:15:59,597 --> 00:16:02,189
which essentially captures
how many people clicked

396
00:16:02,190 --> 00:16:03,190
on that button, you know,

397
00:16:03,190 --> 00:16:05,120
the improve your account security button.

398
00:16:05,120 --> 00:16:06,958
The second is the seven day adoptions,

399
00:16:06,958 --> 00:16:09,120
and the third was the
five month adoptions,

400
00:16:09,120 --> 00:16:11,347
which is how many people actually adopted

401
00:16:11,347 --> 00:16:13,180
one of the promoted security tools

402
00:16:13,180 --> 00:16:15,810
which was the same period I
did in the previous study,

403
00:16:15,810 --> 00:16:17,069
seven days after exposure,

404
00:16:17,070 --> 00:16:19,500
and then 5 months after exposure.

405
00:16:19,500 --> 00:16:23,420
So this is what those numbers
look like at a broad level,

406
00:16:23,420 --> 00:16:24,910
aggregated across all conditions.

407
00:16:24,910 --> 00:16:26,600
So 93 % of participants

408
00:16:26,600 --> 00:16:28,660
logged in and saw the announcement,

409
00:16:28,660 --> 00:16:30,969
13 % clicked on announcement,

410
00:16:30,970 --> 00:16:32,490
4 % adopted one of the

411
00:16:32,490 --> 00:16:34,460
promoted tools within seven days,

412
00:16:34,460 --> 00:16:36,390
and about 10 % adopted
one of the promoted tools

413
00:16:36,390 --> 00:16:38,100
over 5 months.

414
00:16:38,100 --> 00:16:39,600
And here's what those numbers look like

415
00:16:39,600 --> 00:16:42,549
aggregated, split across
different conditions.

416
00:16:42,549 --> 00:16:43,600
And there are two key takeaways

417
00:16:43,600 --> 00:16:44,520
I want you to take from this,

418
00:16:44,520 --> 00:16:47,329
the first is that every
single social condition

419
00:16:47,330 --> 00:16:49,390
out performed the control condition

420
00:16:49,390 --> 00:16:51,660
in terms of soliciting more clicks.

421
00:16:51,660 --> 00:16:54,796
So in fact, we had a 36 %,

422
00:16:56,140 --> 00:16:59,630
we had a 36 % improvement
in click through rate

423
00:16:59,630 --> 00:17:02,090
if you compare the best
performing social condition

424
00:17:02,090 --> 00:17:04,569
which was raw number versus control,

425
00:17:04,569 --> 00:17:06,569
and the second thing I
want you to take away

426
00:17:06,569 --> 00:17:08,399
is that the best two
performing conditions,

427
00:17:08,400 --> 00:17:10,130
the raw number and the some conditions

428
00:17:10,130 --> 00:17:11,450
actually solicited 10 %

429
00:17:11,450 --> 00:17:13,440
more five month adoptions as well

430
00:17:13,440 --> 00:17:14,778
which is a pretty remarkable result

431
00:17:14,778 --> 00:17:15,611
when you consider the fact

432
00:17:15,611 --> 00:17:17,369
that I only changed a couple of words

433
00:17:17,369 --> 00:17:19,479
at the beginning of a
small little notification

434
00:17:19,480 --> 00:17:21,460
in an otherwise busy interface.

435
00:17:21,460 --> 00:17:23,089
If you've ever done AV tests before,

436
00:17:23,089 --> 00:17:25,260
you might have some experience with

437
00:17:25,260 --> 00:17:28,900
how much the effect size of
a change in copied, right,

438
00:17:28,900 --> 00:17:30,900
and it's typically not anywhere near this,

439
00:17:30,900 --> 00:17:34,130
maybe it might be 2 %
or 3 % if you're lucky.

440
00:17:34,130 --> 00:17:37,240
So, we can encourage better
cybersecurity behaviors

441
00:17:37,240 --> 00:17:39,393
by making security more social.

442
00:17:40,430 --> 00:17:43,900
Now, remember that I started
this talk with this question,

443
00:17:43,900 --> 00:17:45,140
how can we design systems

444
00:17:45,140 --> 00:17:47,740
that encourage better
cybersecurity behaviors?

445
00:17:47,740 --> 00:17:50,230
My research suggests
that there is a fruitful

446
00:17:50,230 --> 00:17:53,400
but untapped opportunity to

447
00:17:54,450 --> 00:17:55,900
improve cybersecurity behaviors

448
00:17:55,900 --> 00:17:57,650
by making social systems that are more

449
00:17:57,650 --> 00:17:59,913
observable, cooperative, and stewarded.

450
00:18:00,830 --> 00:18:03,520
By observable, I mean
answering this question,

451
00:18:03,520 --> 00:18:05,100
how can we make it easier for people

452
00:18:05,100 --> 00:18:08,540
to observe and emulate good
cybersecurity behaviors?

453
00:18:08,540 --> 00:18:11,070
Now to use a crude physical world analog,

454
00:18:11,070 --> 00:18:13,830
if a samurai or a ninja came
bursting into your window

455
00:18:13,830 --> 00:18:16,120
and there were a bunch of
other people around you

456
00:18:16,120 --> 00:18:18,110
you would be able to
immediately see the threat,

457
00:18:18,110 --> 00:18:19,750
and you'd be able to see
what everyone else does

458
00:18:19,750 --> 00:18:22,020
to protect themselves against that threat.

459
00:18:22,020 --> 00:18:23,410
So some people might run away,

460
00:18:23,410 --> 00:18:24,370
some people might hide,

461
00:18:24,370 --> 00:18:28,840
other people might be foolhardy
and run into their doom.

462
00:18:28,840 --> 00:18:31,429
And you can sort of observe and react

463
00:18:31,430 --> 00:18:32,580
as you find appropriate,

464
00:18:32,580 --> 00:18:34,750
maybe you decide, okay, I should also run,

465
00:18:34,750 --> 00:18:38,940
because that seems to be the
strategy to keep my life.

466
00:18:38,940 --> 00:18:41,127
Now, contrast this with
cybersecurity, right?

467
00:18:41,127 --> 00:18:43,149
We're kind of in a fogged up world,

468
00:18:43,150 --> 00:18:45,370
we can't really see the threats
that are coming towards us,

469
00:18:45,370 --> 00:18:46,985
we can't see what anybody else is doing

470
00:18:46,985 --> 00:18:49,409
to protect themselves
against those threats.

471
00:18:49,410 --> 00:18:51,590
So it's really unsurprising then,

472
00:18:51,590 --> 00:18:54,540
that end users often get blindsided

473
00:18:54,540 --> 00:18:55,810
by cybersecurity threats

474
00:18:55,810 --> 00:18:58,820
and often don't know
what to do about them.

475
00:18:58,820 --> 00:19:00,840
When I talk about cooperation,

476
00:19:00,840 --> 00:19:02,389
I really mean answering this question of

477
00:19:02,390 --> 00:19:04,110
how can we design cooperative systems

478
00:19:04,110 --> 00:19:06,340
that make group security a joint effort?

479
00:19:06,340 --> 00:19:08,154
A sum function or a max function

480
00:19:08,154 --> 00:19:09,980
instead of a min function.

481
00:19:09,980 --> 00:19:11,530
Now to use another crude world,

482
00:19:11,530 --> 00:19:13,430
physical world analog here.

483
00:19:13,430 --> 00:19:15,970
Consider the contrasting situations

484
00:19:15,970 --> 00:19:17,700
of walking alone with a friend at night

485
00:19:17,700 --> 00:19:21,590
versus walking alone down
a dark alleyway at night

486
00:19:21,590 --> 00:19:24,260
versus walking with a friend
down a dark alleyway at night.

487
00:19:24,260 --> 00:19:25,990
You feel more secure when
you're with a friend,

488
00:19:25,990 --> 00:19:27,760
and that's because you know implicitly

489
00:19:27,760 --> 00:19:30,660
that an attacker would have
to overcome the two of you

490
00:19:30,660 --> 00:19:33,970
to get to anything that
they might want, right?

491
00:19:33,970 --> 00:19:37,050
Now, in cybersecurity,
consider the situation where

492
00:19:37,050 --> 00:19:39,830
I as somebody with good
cybersecurity behaviors,

493
00:19:39,830 --> 00:19:41,960
am sharing sensitive
material with a friend

494
00:19:41,960 --> 00:19:44,060
who has weaker cybersecurity behaviors,

495
00:19:44,060 --> 00:19:46,470
I've effectively
declassified my information,

496
00:19:46,470 --> 00:19:48,730
because the easiest way to
get to that information now

497
00:19:48,730 --> 00:19:50,760
is by compromising my friend.

498
00:19:50,760 --> 00:19:53,959
And this creates, you know,
a perverse disincentive,

499
00:19:53,959 --> 00:19:57,550
so that experts are not
really incentivized to act

500
00:19:57,550 --> 00:20:00,470
with their non expert friends,

501
00:20:00,470 --> 00:20:03,950
and non experts have fewer
opportunities to learn

502
00:20:03,950 --> 00:20:05,780
from their expert friends
and fewer opportunities

503
00:20:05,780 --> 00:20:08,910
to benefit from being
with an expert friend

504
00:20:08,910 --> 00:20:13,203
and sort of joining in sort
of their security behaviors.

505
00:20:14,240 --> 00:20:15,920
And then finally, by stewardship I mean

506
00:20:15,920 --> 00:20:18,260
answering this question of,
how can we design systems

507
00:20:18,260 --> 00:20:20,530
that allow people to act on their concern

508
00:20:20,530 --> 00:20:23,030
for the security and
privacy of their loved ones?

509
00:20:23,030 --> 00:20:25,600
Preliminary research
suggests that a lot of people

510
00:20:26,740 --> 00:20:27,790
are more concerned about

511
00:20:27,790 --> 00:20:29,460
the security and privacy of
their friends and loved ones

512
00:20:29,460 --> 00:20:31,780
than they are about themselves, right?

513
00:20:31,780 --> 00:20:32,970
So, of course, you know,

514
00:20:32,970 --> 00:20:35,380
my dad needs to use a password manager,

515
00:20:35,380 --> 00:20:37,270
oh but I don't need to,
because I can memorize

516
00:20:37,270 --> 00:20:39,303
really strong passwords, you know, like,

517
00:20:40,180 --> 00:20:41,950
but he definitely should.

518
00:20:41,950 --> 00:20:43,960
But existing security and privacy tools

519
00:20:43,960 --> 00:20:47,080
really give us no outlet to
act on our concern for others.

520
00:20:47,080 --> 00:20:47,913
Again, we're sort of cloistered

521
00:20:47,913 --> 00:20:50,310
into our own digital bubbles,

522
00:20:50,310 --> 00:20:52,889
and anything we do is really
only for our own benefit.

523
00:20:52,890 --> 00:20:54,630
So how can we change that?

524
00:20:54,630 --> 00:20:56,320
Okay, so,

525
00:20:56,320 --> 00:20:58,070
of course, as a part of this presentation,

526
00:20:58,070 --> 00:20:59,450
as a part of this conference,

527
00:20:59,450 --> 00:21:01,610
we're all encouraged to
create an "apply" slide

528
00:21:01,610 --> 00:21:03,860
to give you some examples

529
00:21:03,860 --> 00:21:06,250
of what you can do with
this information, right?

530
00:21:06,250 --> 00:21:07,659
And my "apply" slide here is

531
00:21:07,659 --> 00:21:11,950
to take small steps towards
making security more social.

532
00:21:11,950 --> 00:21:13,160
The first thing I recommend you do

533
00:21:13,160 --> 00:21:14,710
is to share your knowledge.

534
00:21:14,710 --> 00:21:16,710
Have a regular public conversation

535
00:21:16,710 --> 00:21:18,380
about your pro-cybersecurity behaviors

536
00:21:18,380 --> 00:21:20,010
and why they are important to you.

537
00:21:20,010 --> 00:21:21,930
You're not gonna get a
lot of clout from this,

538
00:21:21,930 --> 00:21:23,474
but having that signal there,

539
00:21:23,474 --> 00:21:25,700
might just implicitly inform

540
00:21:25,700 --> 00:21:27,510
the people that you're connected with

541
00:21:27,510 --> 00:21:29,060
that cybersecurity is an important thing

542
00:21:29,060 --> 00:21:31,570
that they may need to consider
every once in a while.

543
00:21:31,570 --> 00:21:33,240
Be a safe ambassador.

544
00:21:33,240 --> 00:21:34,910
Make a dedicated safe space or time

545
00:21:34,910 --> 00:21:36,410
for friends, family and colleagues

546
00:21:36,410 --> 00:21:38,175
to ask for help and advice.

547
00:21:38,175 --> 00:21:41,070
Often times, experts do
not really want to burden

548
00:21:41,070 --> 00:21:43,370
their non expert friends
with cybersecurity advice

549
00:21:43,370 --> 00:21:44,203
and behaviors because they think

550
00:21:44,203 --> 00:21:45,149
that they don't want to hear it.

551
00:21:45,150 --> 00:21:46,610
And that might be true.

552
00:21:46,610 --> 00:21:49,260
But if you make a dedicated
safe space and time,

553
00:21:49,260 --> 00:21:50,590
it might be less burdensome

554
00:21:50,590 --> 00:21:51,949
for your friends and family members

555
00:21:51,950 --> 00:21:55,470
to come and ask you for your help.

556
00:21:55,470 --> 00:21:57,668
And then finally, advocate
for pro-social design

557
00:21:57,668 --> 00:22:00,199
in your institution, right?

558
00:22:00,200 --> 00:22:01,720
So consider how you could integrate

559
00:22:01,720 --> 00:22:03,390
things like "just-in-time" social proofs

560
00:22:03,390 --> 00:22:05,823
in observability and
cooperation and stewardship

561
00:22:05,823 --> 00:22:07,970
to encourage pro-security behaviors

562
00:22:07,970 --> 00:22:10,240
in your product and organization.

563
00:22:10,240 --> 00:22:14,600
And with that, I would
like to conclude my talk,

564
00:22:14,600 --> 00:22:16,240
and I would like to just remind everybody

565
00:22:16,240 --> 00:22:17,760
that even if you don't agree with any

566
00:22:17,760 --> 00:22:20,722
of the specific recommendations
that I laid out,

567
00:22:20,722 --> 00:22:23,915
I hope to have at least
convinced you of the following.

568
00:22:23,915 --> 00:22:27,240
Social influences strongly
affect cybersecurity behaviors,

569
00:22:27,240 --> 00:22:29,940
and we can encourage better
cybersecurity behaviors

570
00:22:29,940 --> 00:22:32,810
by designing more social
cybersecurity systems.

571
00:22:32,810 --> 00:22:35,668
If as a community we can keep
that simple point in mind,

572
00:22:35,669 --> 00:22:37,640
I think we stand a fighting chance

573
00:22:37,640 --> 00:22:39,100
of designing systems that encourage

574
00:22:39,100 --> 00:22:41,020
better cybersecurity behaviors.

575
00:22:41,020 --> 00:22:43,400
With that said, remember,
I am on the Slido,

576
00:22:43,400 --> 00:22:46,350
so I am happy to answer any
other questions you might have.

