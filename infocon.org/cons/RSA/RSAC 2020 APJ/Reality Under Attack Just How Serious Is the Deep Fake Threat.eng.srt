1
00:00:06,760 --> 00:00:10,719
- Hello, and welcome to
Reality Under Attack.

2
00:00:10,720 --> 00:00:13,383
Just how serious is the Deep Fake Threat?

3
00:00:14,840 --> 00:00:15,673
If you're joining us live,

4
00:00:15,673 --> 00:00:17,110
our speaker is in this live discussion,

5
00:00:17,110 --> 00:00:18,960
add your questions right now.

6
00:00:18,960 --> 00:00:20,550
For audio and video issues,

7
00:00:20,550 --> 00:00:23,410
click the technical support button below.

8
00:00:23,410 --> 00:00:25,560
I'd now like to turn it
over to Alyssa Miller

9
00:00:25,560 --> 00:00:26,610
for the presentation.

10
00:00:28,500 --> 00:00:29,620
- Hello, everybody.

11
00:00:29,620 --> 00:00:30,840
I am Alyssa Miller,

12
00:00:30,840 --> 00:00:33,770
a hacker and application
security advocate.

13
00:00:33,770 --> 00:00:35,480
And as you mentioned,

14
00:00:35,480 --> 00:00:38,669
we're talking all about deepfakes today.

15
00:00:38,670 --> 00:00:40,360
So to get things started,

16
00:00:40,360 --> 00:00:44,000
what I wanna do is I wanna
take you on a journey,

17
00:00:44,000 --> 00:00:46,363
and it's a journey of
both sight and sound.

18
00:00:47,400 --> 00:00:50,879
Everything that we perceive
in the world around us

19
00:00:50,880 --> 00:00:54,913
is we measure it using our five senses.

20
00:00:56,010 --> 00:00:59,839
And I think back to days of my youth

21
00:00:59,840 --> 00:01:02,230
when my parents would
take us to this place

22
00:01:02,230 --> 00:01:03,870
they call the Fun House.

23
00:01:03,870 --> 00:01:05,960
And in this Fun House,

24
00:01:05,960 --> 00:01:08,759
you would walk in, and
it was like a giant maze.

25
00:01:08,760 --> 00:01:10,850
But worse than a maze,

26
00:01:10,850 --> 00:01:13,009
they had all sorts of optical illusions

27
00:01:13,010 --> 00:01:16,320
that were designed to fool
your senses into thinking

28
00:01:16,320 --> 00:01:18,880
that different directions were the exit

29
00:01:18,880 --> 00:01:20,839
or that walls were in different places,

30
00:01:20,840 --> 00:01:23,700
or that floors were slanted

31
00:01:23,700 --> 00:01:25,340
when they weren't all these things.

32
00:01:25,340 --> 00:01:29,040
And as I scrambled, rushing
from one point to the other,

33
00:01:29,040 --> 00:01:32,090
trying to get out and
finding all these dead ends,

34
00:01:32,090 --> 00:01:33,600
what I realized was,

35
00:01:33,600 --> 00:01:37,649
I couldn't trust my senses to
assess the world around me.

36
00:01:37,650 --> 00:01:41,440
I couldn't tell what was real or fake.

37
00:01:41,440 --> 00:01:43,230
When it comes to deepfakes,

38
00:01:43,230 --> 00:01:45,070
this is the attack vector

39
00:01:45,070 --> 00:01:48,559
that attackers are
using against all of us,

40
00:01:48,560 --> 00:01:50,580
rather than attacking systems,

41
00:01:50,580 --> 00:01:53,210
rather than attacking our decision making.

42
00:01:53,210 --> 00:01:55,059
They're attacking our very senses

43
00:01:55,060 --> 00:01:58,400
and bringing us to a point
where we cannot trust the things

44
00:01:58,400 --> 00:01:59,420
that we see.

45
00:01:59,420 --> 00:02:02,140
We cannot trust the things that we hear.

46
00:02:02,140 --> 00:02:06,460
It affects our very ability to
perceive the world around us.

47
00:02:06,460 --> 00:02:08,229
So what are deepfakes?

48
00:02:08,229 --> 00:02:10,759
When I say deepfakes,
what I'm talking about is

49
00:02:10,759 --> 00:02:13,230
any form of artificial media

50
00:02:13,230 --> 00:02:16,230
created by deep learning neural networks.

51
00:02:16,230 --> 00:02:19,030
This could come in the
form of still images.

52
00:02:19,030 --> 00:02:22,140
It also can come in the form of audio.

53
00:02:22,140 --> 00:02:23,070
And most commonly,

54
00:02:23,070 --> 00:02:25,840
the one that a lot of us associate with

55
00:02:25,840 --> 00:02:29,110
when we hear the term
deepfake is we think of video.

56
00:02:29,110 --> 00:02:32,530
And this video here that
was created a while back

57
00:02:32,530 --> 00:02:34,710
as Steve Buscemi is a pretty common one,

58
00:02:34,710 --> 00:02:37,930
one that many of you may have seen before.

59
00:02:37,930 --> 00:02:40,701
So where do deepfakes begin?

60
00:02:40,701 --> 00:02:44,450
Deepfakes began back in 2017.

61
00:02:44,450 --> 00:02:46,820
And actually, they begin
a little sooner than that

62
00:02:46,820 --> 00:02:48,040
as part of a research project,

63
00:02:48,040 --> 00:02:50,410
but that research was released in 2017.

64
00:02:50,410 --> 00:02:52,120
And shortly thereafter,

65
00:02:52,120 --> 00:02:54,660
activity started popping up on Reddit,

66
00:02:54,660 --> 00:02:59,660
and people were using deepfake
technology to create porn.

67
00:03:00,430 --> 00:03:03,200
So what they were doing
was they were taking

68
00:03:03,200 --> 00:03:07,850
existing porn videos,
existing porn images,

69
00:03:07,850 --> 00:03:12,850
and placing the faces of
well known celebrities

70
00:03:13,010 --> 00:03:16,299
like Daisy Ridley, Natalie Portman,

71
00:03:16,300 --> 00:03:19,200
Gal Gadot, Natalie Dormer, Emma Watson,

72
00:03:19,200 --> 00:03:21,810
and you're placing them on
the bodies of the actresses

73
00:03:21,810 --> 00:03:23,377
in this porn video.

74
00:03:23,377 --> 00:03:26,370
Well, it didn't take long from there

75
00:03:26,370 --> 00:03:30,570
for deepfakes to progress
into the political arena.

76
00:03:30,570 --> 00:03:32,600
A lot of you may be
familiar with this video

77
00:03:32,600 --> 00:03:35,440
that was created by Jordan Peele in 2018,

78
00:03:35,440 --> 00:03:38,310
where he created a deepfake
video of Barack Obama.

79
00:03:38,310 --> 00:03:40,150
And it featured Barrack Obama

80
00:03:40,150 --> 00:03:42,480
saying all sorts of really
kind of nasty things

81
00:03:42,480 --> 00:03:45,030
about some of his political opponents.

82
00:03:45,030 --> 00:03:48,860
And a lot of people really
believe that this was real

83
00:03:48,860 --> 00:03:51,313
and indeed it gets shared
all over the internet.

84
00:03:52,230 --> 00:03:54,590
People who had cut out the portion,

85
00:03:54,590 --> 00:03:56,880
a few minutes in when
Jordan Peele shows up

86
00:03:56,880 --> 00:03:59,760
and says, "Hey, this is only fake."

87
00:03:59,760 --> 00:04:02,799
Since that time we've seen
other political leaders,

88
00:04:02,800 --> 00:04:04,840
also the target of deepfakes.

89
00:04:04,840 --> 00:04:08,260
And indeed, this is information

90
00:04:08,260 --> 00:04:11,609
that flies through the
internet quite quickly

91
00:04:11,610 --> 00:04:14,124
and it's very believable.

92
00:04:14,124 --> 00:04:17,920
But we've also seen it extend
beyond the political arena.

93
00:04:17,920 --> 00:04:21,670
We've also seen deepfakes used
in terms of business leaders,

94
00:04:21,670 --> 00:04:23,320
sometimes very innocuously.

95
00:04:23,320 --> 00:04:27,130
Here's Elon Musk's face
being placed on a baby.

96
00:04:27,130 --> 00:04:28,890
You may have seen this video of this

97
00:04:28,890 --> 00:04:30,380
cute, little baby crawling around

98
00:04:30,380 --> 00:04:31,290
only it's not so cute

99
00:04:31,290 --> 00:04:33,373
'cause it's got Elon Musk's face on it.

100
00:04:35,100 --> 00:04:37,880
We also a little more damaging,

101
00:04:37,880 --> 00:04:42,330
however, was this video that
surfaced of Mark Zuckerberg,

102
00:04:42,330 --> 00:04:45,490
where he was reportedly saying things

103
00:04:45,490 --> 00:04:48,710
about Facebook's privacy
policy and so forth.

104
00:04:48,710 --> 00:04:51,359
And these were things that
inflamed a lot of people,

105
00:04:51,360 --> 00:04:53,960
made a lot of people very angry.

106
00:04:53,960 --> 00:04:57,549
And this is where we start
to see some of the threats

107
00:04:57,550 --> 00:04:59,280
to the business world.

108
00:04:59,280 --> 00:05:02,020
So where a lot of attention
thus far has been paid

109
00:05:02,020 --> 00:05:04,200
to the political environment.

110
00:05:04,200 --> 00:05:07,330
I wanna examine the
threats little bit more

111
00:05:07,330 --> 00:05:10,590
as they exist in the business sense.

112
00:05:10,590 --> 00:05:12,169
So this is one of those things

113
00:05:12,170 --> 00:05:14,680
that's going largely untold so far

114
00:05:14,680 --> 00:05:18,110
as a story within deepfakes.

115
00:05:18,110 --> 00:05:20,730
So let's take a look at some of the things

116
00:05:20,730 --> 00:05:23,540
that we're starting to
see as potential threats

117
00:05:23,540 --> 00:05:26,250
when it comes to the business environment.

118
00:05:26,250 --> 00:05:30,823
So I mentioned before the
idea of deepfake audio,

119
00:05:31,730 --> 00:05:34,030
so we all think about video
when we think about deepfakes.

120
00:05:34,030 --> 00:05:37,849
But deepfake audio is
becoming increasingly

121
00:05:37,850 --> 00:05:40,700
a common place now as well,
there's a number of vendors

122
00:05:40,700 --> 00:05:42,870
who offer this as a service.

123
00:05:42,870 --> 00:05:44,480
And indeed, in 2019,

124
00:05:44,480 --> 00:05:47,740
there was a reported
social engineering attack

125
00:05:47,740 --> 00:05:50,440
against a UK-based energy company,

126
00:05:50,440 --> 00:05:55,440
where the CEO wired €220
million to attackers,

127
00:05:55,770 --> 00:05:59,740
because the attackers
reportedly used deepfake audio

128
00:05:59,740 --> 00:06:02,640
to come convince him
that it was the president

129
00:06:02,640 --> 00:06:05,610
of the parent organization contacting him

130
00:06:05,610 --> 00:06:08,020
with instructions on a wire transfer

131
00:06:08,020 --> 00:06:10,510
that needed to be made immediately.

132
00:06:10,510 --> 00:06:12,740
Now, some people disputed whether

133
00:06:12,740 --> 00:06:13,860
that's really the case.

134
00:06:13,860 --> 00:06:15,310
It's impossible for us to know

135
00:06:15,310 --> 00:06:17,540
for sure if this was deepfake audio,

136
00:06:17,540 --> 00:06:19,550
but that's what's suspected.

137
00:06:19,550 --> 00:06:21,533
And quite honestly,

138
00:06:21,533 --> 00:06:26,200
the chances of this actually
occurring are pretty strong.

139
00:06:26,200 --> 00:06:27,860
When we think about what social engineers

140
00:06:27,860 --> 00:06:30,060
are able to do today in
terms of social engineering

141
00:06:30,060 --> 00:06:32,640
without faking the voices of a person

142
00:06:32,640 --> 00:06:34,870
that's familiar to their target.

143
00:06:34,870 --> 00:06:37,170
Imagine how much more
effective that would be.

144
00:06:38,640 --> 00:06:39,919
But there was other threats as well.

145
00:06:39,920 --> 00:06:43,060
When we think about still
images and we think about video,

146
00:06:43,060 --> 00:06:46,500
think about the possibility
of using those images,

147
00:06:46,500 --> 00:06:49,060
those media for extortion.

148
00:06:49,060 --> 00:06:51,750
Now I bring up here the case of just Bezos

149
00:06:51,750 --> 00:06:53,160
and his extortion case,

150
00:06:53,160 --> 00:06:56,760
which, yes, in reality, I
have not heard any suggestion

151
00:06:56,760 --> 00:06:59,860
that there was anything other
than those were real images

152
00:06:59,860 --> 00:07:01,780
that were used to extort him.

153
00:07:01,780 --> 00:07:06,010
But it's not far of a leap
to see where a deepfake image

154
00:07:06,010 --> 00:07:08,580
or a deep fake video could
potentially be leveraged

155
00:07:08,580 --> 00:07:10,169
in this case.

156
00:07:10,170 --> 00:07:13,340
It doesn't take a whole lot of time

157
00:07:13,340 --> 00:07:15,599
for that media to be out there

158
00:07:15,600 --> 00:07:18,100
for it to have a very damaging
effect on the business

159
00:07:18,100 --> 00:07:23,100
that that person is leaving,
or even on their very marriage.

160
00:07:23,230 --> 00:07:26,000
And so it's not about,
yeah, maybe we can prove

161
00:07:26,000 --> 00:07:28,930
that it was a deepfake later
after it's been released.

162
00:07:28,930 --> 00:07:32,780
But that threat of ruining their image

163
00:07:32,780 --> 00:07:35,599
and not being able to repair their image,

164
00:07:35,600 --> 00:07:39,130
that's real and it could
still be used to leverage them

165
00:07:39,130 --> 00:07:42,020
into making bad decisions or doing things

166
00:07:42,020 --> 00:07:43,733
on behalf of the attackers.

167
00:07:44,900 --> 00:07:46,630
But when I talk threats to business,

168
00:07:46,630 --> 00:07:48,990
there's another perspective
I like to talk about,

169
00:07:48,990 --> 00:07:51,900
and it's what I call 'Outsider trading'

170
00:07:51,900 --> 00:07:55,099
Now, we all understand hopefully
what insider trading is

171
00:07:55,100 --> 00:07:57,590
where someone inside an organization

172
00:07:57,590 --> 00:08:01,330
uses their knowledge of events
within that organization

173
00:08:01,330 --> 00:08:03,810
and how those events
will affect stock price

174
00:08:03,810 --> 00:08:06,740
to make stock purchases or stock sales

175
00:08:06,740 --> 00:08:09,540
that will benefit them in the long run.

176
00:08:09,540 --> 00:08:11,610
But now imagine this scenario,

177
00:08:11,610 --> 00:08:14,160
imagine a case where
Tesla is getting ready

178
00:08:14,160 --> 00:08:17,060
to launch their newest model.

179
00:08:17,060 --> 00:08:19,040
And the night before that launch,

180
00:08:19,040 --> 00:08:23,480
a video surfaces of Elon
Musk talking to his investors

181
00:08:23,480 --> 00:08:26,850
about how there's problems with the line

182
00:08:26,850 --> 00:08:29,080
and the vehicle's gonna have issues

183
00:08:29,080 --> 00:08:30,900
or it's going to be delayed or whatever,

184
00:08:30,900 --> 00:08:33,799
something very negative about the launch.

185
00:08:33,799 --> 00:08:35,380
Well, of course, what's gonna happen

186
00:08:35,380 --> 00:08:37,100
when the markets open the next morning

187
00:08:37,100 --> 00:08:40,340
is their stock is going
to drop significantly.

188
00:08:40,340 --> 00:08:43,559
And now that attacker
can buy up that stock.

189
00:08:43,559 --> 00:08:45,180
And then when the news comes out

190
00:08:45,180 --> 00:08:46,800
that this was just a deepfake,

191
00:08:46,800 --> 00:08:49,650
the launch goes as planned,
Elon Musk stands on stage

192
00:08:49,650 --> 00:08:51,410
with his sledgehammer smashing windows

193
00:08:51,410 --> 00:08:53,350
or whatever he's going to do.

194
00:08:53,350 --> 00:08:54,650
The stock price recovers

195
00:08:54,650 --> 00:08:57,350
and that attacker is able
to sell off that stock

196
00:08:57,350 --> 00:09:00,090
and now they've made a tidy little profit.

197
00:09:00,090 --> 00:09:04,164
Without having to have any
insider information at all.

198
00:09:04,164 --> 00:09:06,720
But that threat gets bigger.

199
00:09:06,720 --> 00:09:10,540
If I can use that type of technology

200
00:09:10,540 --> 00:09:13,703
to impact an organization,
the size of Tesla,

201
00:09:15,230 --> 00:09:18,990
what can I do to complete
financial market?

202
00:09:18,990 --> 00:09:22,990
Could I manipulate the
entire automotive marketplace

203
00:09:22,990 --> 00:09:25,630
with a well placed deepfake video?

204
00:09:25,630 --> 00:09:28,430
Could I even bring this to the level

205
00:09:28,430 --> 00:09:31,729
where I'm able to impact the entire market

206
00:09:31,730 --> 00:09:34,823
and the entire economy of a small country?

207
00:09:35,830 --> 00:09:40,590
The fact is deepfake
video being so believable,

208
00:09:40,590 --> 00:09:42,480
and the way that it spreads,

209
00:09:42,480 --> 00:09:45,310
it becomes a real issue.

210
00:09:45,310 --> 00:09:48,609
And it becomes very effective in changing

211
00:09:48,610 --> 00:09:50,550
the minds of people.

212
00:09:50,550 --> 00:09:51,479
And so we will talk

213
00:09:51,480 --> 00:09:52,920
a little bit more about
that in a little bit.

214
00:09:52,920 --> 00:09:55,449
But I want to take a step back.

215
00:09:55,450 --> 00:09:57,333
How serious is this really?

216
00:09:58,390 --> 00:10:00,370
Is this something that you know,

217
00:10:00,370 --> 00:10:03,780
is this really as bad as
it's been made out to be?

218
00:10:03,780 --> 00:10:06,069
There's been, as I
said, a lot of activity,

219
00:10:06,070 --> 00:10:07,990
especially in the political arena.

220
00:10:07,990 --> 00:10:10,510
People have talked about
how horrible this could be

221
00:10:10,510 --> 00:10:12,850
in terms of voter manipulation.

222
00:10:12,850 --> 00:10:17,120
They have talked about how
this could be leveraged

223
00:10:17,120 --> 00:10:21,400
to manipulate and
potentially even start wars.

224
00:10:21,400 --> 00:10:24,319
But really, is it so real?

225
00:10:24,320 --> 00:10:27,580
Is that threat so imminent?

226
00:10:27,580 --> 00:10:31,830
Let's start by taking a look
at how deepfakes are created.

227
00:10:31,830 --> 00:10:35,140
And to do that, we need
to understand this concept

228
00:10:35,140 --> 00:10:39,930
of a Generative
Adversarial Network or GAN.

229
00:10:39,930 --> 00:10:42,680
GANs are what have been traditionally used

230
00:10:42,680 --> 00:10:43,693
to create deepfakes.

231
00:10:44,950 --> 00:10:46,720
Now pay attention to that name,

232
00:10:46,720 --> 00:10:49,820
because that name kind of
describes exactly what a GAN is.

233
00:10:49,820 --> 00:10:52,500
It's generative, it's
going to create something

234
00:10:52,500 --> 00:10:53,591
that doesn't exist.

235
00:10:53,591 --> 00:10:54,424
(air squealing)

236
00:10:54,424 --> 00:10:58,829
It's adversarial in that
you have two neural networks

237
00:10:58,830 --> 00:11:01,540
that are competing against each other,

238
00:11:01,540 --> 00:11:04,480
to ultimately produce this media.

239
00:11:04,480 --> 00:11:08,170
They are working in an
adversarial fashion.

240
00:11:08,170 --> 00:11:09,660
And then of course they're not

241
00:11:09,660 --> 00:11:11,390
it's their network together,

242
00:11:11,390 --> 00:11:13,590
these two neural networks
are working together

243
00:11:13,590 --> 00:11:15,430
to create this media.

244
00:11:15,430 --> 00:11:18,370
So how this works is you have
this first neural network,

245
00:11:18,370 --> 00:11:19,730
which is called the generator.

246
00:11:19,730 --> 00:11:23,070
And the generators job is
just to create the media.

247
00:11:23,070 --> 00:11:25,990
On the flip side, you
have the discriminator.

248
00:11:25,990 --> 00:11:28,270
The discriminator's job
is to look at the media

249
00:11:28,270 --> 00:11:29,840
that the generator creates,

250
00:11:29,840 --> 00:11:33,680
and determine how
realistic it appears to be.

251
00:11:33,680 --> 00:11:38,250
Now what we do is we're
going to take training sets.

252
00:11:38,250 --> 00:11:40,280
And in reality I show one
here with there's actually

253
00:11:40,280 --> 00:11:42,180
two training sets that we leverage.

254
00:11:42,180 --> 00:11:46,010
We leverage a training
set that is the subject

255
00:11:46,010 --> 00:11:50,590
or the person that we are trying to create

256
00:11:50,590 --> 00:11:51,423
in this fake video.

257
00:11:51,423 --> 00:11:53,170
And we also do the target,

258
00:11:53,170 --> 00:11:55,530
the person who we're trying to replace.

259
00:11:55,530 --> 00:11:57,060
So where I have a video, for instance,

260
00:11:57,060 --> 00:11:59,670
and I need to replace an
actor with Barrack Obama,

261
00:11:59,670 --> 00:12:03,490
I will train both on Barack
Obama's face and the actor.

262
00:12:03,490 --> 00:12:05,630
Then I feed that target media.

263
00:12:05,630 --> 00:12:07,580
So that media created with an actor,

264
00:12:07,580 --> 00:12:09,170
I feed it into the generator.

265
00:12:09,170 --> 00:12:12,160
And frame by frame, the
generator goes through

266
00:12:12,160 --> 00:12:15,339
and it attempts to replace
the face of that actor

267
00:12:15,340 --> 00:12:16,680
with Barrack Obama.

268
00:12:16,680 --> 00:12:19,790
Now the discriminator
looks at that and says,

269
00:12:19,790 --> 00:12:23,530
you know, it ultimately produces
a score that we call loss.

270
00:12:23,530 --> 00:12:26,170
And that score tells us
basically how realistic

271
00:12:26,170 --> 00:12:27,719
or fake it is.

272
00:12:27,720 --> 00:12:30,520
And if the results are acceptable,

273
00:12:30,520 --> 00:12:33,020
it passes that frame into the video,

274
00:12:33,020 --> 00:12:34,453
and that becomes our video.

275
00:12:35,750 --> 00:12:36,650
If it's not,

276
00:12:36,650 --> 00:12:39,290
it feeds that in formation
back to the generator,

277
00:12:39,290 --> 00:12:40,689
it updates the model

278
00:12:40,690 --> 00:12:44,230
that the generator is using
to create these images,

279
00:12:44,230 --> 00:12:47,080
such that the generator continues to learn

280
00:12:47,080 --> 00:12:49,180
from the discriminator
and learns how to get

281
00:12:49,180 --> 00:12:50,630
better and better.

282
00:12:50,630 --> 00:12:52,573
And you have this endless cycle

283
00:12:52,573 --> 00:12:55,439
where these two networks
working against each other

284
00:12:55,440 --> 00:12:57,680
are also kind of working together.

285
00:12:57,680 --> 00:12:59,069
Now I've oversimplified that

286
00:12:59,070 --> 00:13:01,420
it's a far more complex process than that.

287
00:13:01,420 --> 00:13:03,550
But that makes it pretty easy understand,

288
00:13:03,550 --> 00:13:05,563
just for purposes of our talk today.

289
00:13:06,820 --> 00:13:08,880
Now, how do we produce these,

290
00:13:08,880 --> 00:13:11,450
what's available in the consumer market?

291
00:13:11,450 --> 00:13:13,690
Well, there's a couple of
different solutions out there

292
00:13:13,690 --> 00:13:15,750
that consumers can access today.

293
00:13:15,750 --> 00:13:18,330
The first was this app called Fake App.

294
00:13:18,330 --> 00:13:20,830
Now, it's still available maybe.

295
00:13:20,830 --> 00:13:22,500
It's out there on the open source market.

296
00:13:22,500 --> 00:13:27,210
But unfortunately, a lot
of the open source repos

297
00:13:27,210 --> 00:13:29,810
that you're gonna see
for it contain malware

298
00:13:29,810 --> 00:13:30,642
and other things.

299
00:13:30,643 --> 00:13:33,490
They were you know, they
were modified by people.

300
00:13:33,490 --> 00:13:35,200
And so I wouldn't recommend going

301
00:13:35,200 --> 00:13:36,800
and trying to download Fake App.

302
00:13:38,110 --> 00:13:42,040
Faceswap is one that's
more commonly used today.

303
00:13:42,040 --> 00:13:43,400
It's written in Python.

304
00:13:43,400 --> 00:13:46,910
It's widely maintained,
it's continually updated.

305
00:13:46,910 --> 00:13:49,469
It is one of the most popular ways

306
00:13:49,470 --> 00:13:53,910
of creating deepfake video using a GAN.

307
00:13:53,910 --> 00:13:56,850
I've done some experimentation
with it myself.

308
00:13:56,850 --> 00:13:59,150
It's very easy to install on a PC

309
00:13:59,150 --> 00:14:01,840
with a decently powerful GPU,

310
00:14:01,840 --> 00:14:04,593
and it can produce these videos.

311
00:14:06,440 --> 00:14:11,440
There's also a service for
hire called Deepfakes web.

312
00:14:12,130 --> 00:14:14,100
So yes, this is a
website where you can go,

313
00:14:14,100 --> 00:14:16,660
you can log into the website
and for a fee you're paying

314
00:14:16,660 --> 00:14:20,660
for their time of in cloud compute.

315
00:14:20,660 --> 00:14:24,680
You've to hand them a video
and the training images

316
00:14:24,680 --> 00:14:28,160
that you want to use to
create your deep fake video,

317
00:14:28,160 --> 00:14:30,882
and they create the
deepfake video for you.

318
00:14:32,470 --> 00:14:34,710
And then finally, there was
this app called Deep Nude.

319
00:14:34,710 --> 00:14:36,510
Now, this was released,

320
00:14:36,510 --> 00:14:37,640
it was a mobile app,

321
00:14:37,640 --> 00:14:39,920
and it purported to give you x ray glasses

322
00:14:39,920 --> 00:14:43,939
where you could see how any
woman looked when she was naked.

323
00:14:43,940 --> 00:14:46,110
Basically, all it was doing
was creating deepfake images,

324
00:14:46,110 --> 00:14:49,350
taking a picture of a woman
that you took a picture of,

325
00:14:49,350 --> 00:14:51,890
and imposing her face on someone else,

326
00:14:51,890 --> 00:14:54,260
some other woman's nude body.

327
00:14:54,260 --> 00:14:56,700
Thankfully, that app
went away very quickly.

328
00:14:56,700 --> 00:14:58,210
There was a lot of backlash about it.

329
00:14:58,210 --> 00:14:59,700
It no longer exists.

330
00:14:59,700 --> 00:15:04,160
Some people supposedly
claim to have the APK

331
00:15:04,160 --> 00:15:07,083
for it again, I wouldn't
recommend downloading that.

332
00:15:07,980 --> 00:15:09,480
So those two have gone away.

333
00:15:09,480 --> 00:15:13,558
Faceswap though, and deepfakes
are still very active.

334
00:15:13,558 --> 00:15:15,319
Now, the reality is

335
00:15:15,320 --> 00:15:20,130
however, using these tools isn't
as easy as you would think.

336
00:15:20,130 --> 00:15:23,920
I have a PC that's pretty
well built for this,

337
00:15:23,920 --> 00:15:26,689
I bought a dedicated GPU card.

338
00:15:26,690 --> 00:15:28,610
It's a very powerful Nvidia.

339
00:15:28,610 --> 00:15:30,900
I went through hours and
hours and hours of training,

340
00:15:30,900 --> 00:15:33,730
I spent 72 hours training
my neural network

341
00:15:33,730 --> 00:15:36,940
to try to create a video
deep faking Alyssa Milano

342
00:15:36,940 --> 00:15:40,330
who you see on the right with over a video

343
00:15:40,330 --> 00:15:42,190
that I created of myself.

344
00:15:42,190 --> 00:15:44,340
In the middle you see the results.

345
00:15:44,340 --> 00:15:46,620
And the fact of the matter is,

346
00:15:46,620 --> 00:15:51,090
yeah, it's fun to watch and it looks good.

347
00:15:51,090 --> 00:15:53,300
But as you see this still image here,

348
00:15:53,300 --> 00:15:56,849
you can see it's pretty easy
to tell that it's not real.

349
00:15:56,850 --> 00:16:00,010
There's issues with resolution
and other things here.

350
00:16:00,010 --> 00:16:02,800
And indeed, this is what
researchers are looking at

351
00:16:02,800 --> 00:16:05,050
when it comes to detecting deepfakes.

352
00:16:05,050 --> 00:16:06,079
There are a number of ways

353
00:16:06,080 --> 00:16:08,860
that researchers have
found to detect deepfakes.

354
00:16:08,860 --> 00:16:12,770
The first is just
general warping of faces.

355
00:16:12,770 --> 00:16:15,640
So essentially, what's happening
in these neural networks

356
00:16:15,640 --> 00:16:20,020
is they're trying to apply
this training set of images

357
00:16:20,020 --> 00:16:23,370
to this other training set of images.

358
00:16:23,370 --> 00:16:27,200
And when you're trying to
apply an image of one person

359
00:16:27,200 --> 00:16:30,100
with one shape and size face

360
00:16:30,100 --> 00:16:32,470
to a person with a very
different shape and size

361
00:16:32,470 --> 00:16:33,780
in their face,

362
00:16:33,780 --> 00:16:37,370
there's just some warping and
some distortion that occurs.

363
00:16:37,370 --> 00:16:41,318
Now, there was a research
paper written on this in 2018.

364
00:16:41,318 --> 00:16:44,220
But very quickly, GANs have
gotten better and better

365
00:16:44,220 --> 00:16:46,290
and this is not as much
of an issue anymore.

366
00:16:46,290 --> 00:16:49,130
It's certainly not visually
detectable anymore.

367
00:16:49,130 --> 00:16:52,439
So then, researchers started
looking at eye blinking,

368
00:16:52,440 --> 00:16:56,100
and they realized that in deepfake videos,

369
00:16:56,100 --> 00:16:59,010
the GANs weren't reproducing
eye blinking very well

370
00:16:59,010 --> 00:17:01,410
and so they were able to start identifying

371
00:17:01,410 --> 00:17:03,660
when there wasn't, they
weren't frequent enough

372
00:17:03,660 --> 00:17:07,119
or the velocity of the
eyelids was not correct.

373
00:17:07,119 --> 00:17:09,790
However, as soon as that
research was released

374
00:17:09,790 --> 00:17:11,899
within months, we saw
GANS getting much better

375
00:17:11,900 --> 00:17:15,405
at replacing or duplicating eye blinking.

376
00:17:15,405 --> 00:17:18,369
Some of the more promising
tech more recently

377
00:17:18,369 --> 00:17:20,319
that we're looking at
in terms of research,

378
00:17:20,319 --> 00:17:22,780
is this idea of behavioral analytics.

379
00:17:22,780 --> 00:17:27,010
How do people behave when they're
delivering certain content

380
00:17:27,010 --> 00:17:29,810
and being able to detect whether
or not facial expressions

381
00:17:29,810 --> 00:17:32,470
and facial tics match up to the content

382
00:17:32,470 --> 00:17:34,510
that's being delivered in that video?

383
00:17:34,510 --> 00:17:35,790
Right now, researchers are saying

384
00:17:35,790 --> 00:17:37,908
they've got about 98% accuracy

385
00:17:37,909 --> 00:17:41,903
in using this behavioral
detection technique.

386
00:17:44,450 --> 00:17:46,580
But things are changing.

387
00:17:46,580 --> 00:17:50,153
So where deepfakes
traditionally have taken

388
00:17:50,153 --> 00:17:55,153
a great deal of time to
train on both the subject

389
00:17:55,310 --> 00:17:56,260
and the target.

390
00:17:56,260 --> 00:17:58,970
What we're seeing now is
deepfakes are starting to focus

391
00:17:58,970 --> 00:18:02,270
on how can I train wants on the ability to

392
00:18:02,270 --> 00:18:04,660
just map out facial landmarks?

393
00:18:04,660 --> 00:18:08,260
And then take a single
image and apply that

394
00:18:08,260 --> 00:18:12,340
and more fit based on
those facial landmarks.

395
00:18:12,340 --> 00:18:14,699
And so that's what you see happening here

396
00:18:14,700 --> 00:18:16,310
in these couple images,

397
00:18:16,310 --> 00:18:19,639
where you see one image being applied

398
00:18:19,640 --> 00:18:21,350
to this facial mapping.

399
00:18:21,350 --> 00:18:23,750
That's just normal facial recognition,

400
00:18:23,750 --> 00:18:28,130
and then being morphed
to create that face in

401
00:18:28,130 --> 00:18:29,773
that alternate environment.

402
00:18:31,730 --> 00:18:35,370
This has given us almost
real time capability

403
00:18:35,370 --> 00:18:36,699
to create deep fakes.

404
00:18:36,700 --> 00:18:40,410
In fact, some of you may have
heard of a plugin for Zoom

405
00:18:40,410 --> 00:18:44,390
and other video conferencing
software called Avatarify.

406
00:18:44,390 --> 00:18:47,700
And what Avatarify does
is it uses this approach

407
00:18:47,700 --> 00:18:52,470
to in basically real time,
create these deepfake images.

408
00:18:52,470 --> 00:18:55,100
So you can literally have Elon Musk,

409
00:18:55,100 --> 00:18:58,419
join your Zoom conference and talk to you.

410
00:18:58,420 --> 00:19:00,460
Now there's issues with this still.

411
00:19:00,460 --> 00:19:02,610
As I said, it's taking
a single still image

412
00:19:02,610 --> 00:19:05,780
and trying to morph to the
various facial mappings

413
00:19:05,780 --> 00:19:07,570
that happen frame by frame.

414
00:19:07,570 --> 00:19:10,000
Now, I don't have the ability
to show you video today,

415
00:19:10,000 --> 00:19:13,800
but I do, I can't show you
what this particular video

416
00:19:13,800 --> 00:19:16,159
looked like with the Mona Lisa,

417
00:19:16,160 --> 00:19:18,980
when the head changed position

418
00:19:18,980 --> 00:19:22,570
and if you watch this, you
see the background shift.

419
00:19:22,570 --> 00:19:24,800
You see how things kind of morph.

420
00:19:24,800 --> 00:19:26,450
And the reality is,

421
00:19:26,450 --> 00:19:29,360
in real time when you're
watching this on video,

422
00:19:29,360 --> 00:19:31,290
that's very evident.

423
00:19:31,290 --> 00:19:34,763
And so it's still quite discernible.

424
00:19:36,130 --> 00:19:39,380
So researchers are looking
at other things as well.

425
00:19:39,380 --> 00:19:41,460
Not only how do we detect deepfakes,

426
00:19:41,460 --> 00:19:45,090
but can we even prevent them
from being created at all?

427
00:19:45,090 --> 00:19:49,669
And so in 2019, a research
paper was released talking

428
00:19:49,670 --> 00:19:53,600
about this idea of
adversarial perturbations.

429
00:19:53,600 --> 00:19:55,480
Basically what this is,

430
00:19:55,480 --> 00:19:59,710
is it's visually undetectable
noise that is inserted

431
00:19:59,710 --> 00:20:02,310
into images of people

432
00:20:02,310 --> 00:20:07,310
that ultimately it breaks the
facial recognition algorithms.

433
00:20:09,160 --> 00:20:10,450
And so you can see that here,

434
00:20:10,450 --> 00:20:15,450
the first row of images are
just faces or facial images

435
00:20:15,830 --> 00:20:17,300
that haven't been treated.

436
00:20:17,300 --> 00:20:19,000
And from the green boxes, you can see

437
00:20:19,000 --> 00:20:22,120
that the facial recognition
software very easily

438
00:20:22,120 --> 00:20:23,763
detects where the face is.

439
00:20:24,608 --> 00:20:27,250
The second row is what
happens after those images

440
00:20:27,250 --> 00:20:30,350
have been treated with the perturbations,

441
00:20:30,350 --> 00:20:32,010
that digital noise is inserted

442
00:20:32,010 --> 00:20:35,740
and suddenly you see all
sorts of faces being detected,

443
00:20:35,740 --> 00:20:38,470
none of which are the correct face.

444
00:20:38,470 --> 00:20:40,400
And then that last row that you see there,

445
00:20:40,400 --> 00:20:43,090
that is the actual digital noise

446
00:20:43,090 --> 00:20:48,090
that was introduced into
those images to help forge

447
00:20:48,530 --> 00:20:51,113
that facial recognition library.

448
00:20:52,150 --> 00:20:54,540
So as you can see, there's a lot of

449
00:20:55,710 --> 00:20:57,700
there's a lot of promising technology here

450
00:20:57,700 --> 00:21:01,520
both in terms of deepfakes and
in terms of detecting them.

451
00:21:01,520 --> 00:21:05,200
The reality is right
now deepfakes are still

452
00:21:05,200 --> 00:21:08,100
fairly difficult to produce at a level

453
00:21:08,100 --> 00:21:11,429
where they are ultra convincing.

454
00:21:11,430 --> 00:21:13,610
Most of the videos that you see

455
00:21:13,610 --> 00:21:15,139
that have been created,

456
00:21:15,140 --> 00:21:18,260
have taken a significant
amount of time to be created.

457
00:21:18,260 --> 00:21:20,640
You saw the results of what I created.

458
00:21:20,640 --> 00:21:24,040
It takes that took a week
to create that video,

459
00:21:24,040 --> 00:21:25,730
just me with my computer.

460
00:21:25,730 --> 00:21:28,980
Now, obviously, with cloud
compute and other technologies,

461
00:21:28,980 --> 00:21:31,890
that helps, but there's
still a lot of manual effort

462
00:21:31,890 --> 00:21:33,640
that has to go into creating these.

463
00:21:33,640 --> 00:21:35,670
We also saw the Avatarify,

464
00:21:35,670 --> 00:21:38,300
something that's very scary,
but it has its own limitations

465
00:21:38,300 --> 00:21:40,830
in that it's only working from one image.

466
00:21:40,830 --> 00:21:42,870
It has to morph that image.

467
00:21:42,870 --> 00:21:44,489
Yes, it's doing it in real time.

468
00:21:44,490 --> 00:21:47,440
But there are easy ways visually right now

469
00:21:47,440 --> 00:21:48,630
to detect that.

470
00:21:48,630 --> 00:21:50,760
And certainly, technologically
there will probably

471
00:21:50,760 --> 00:21:53,753
be ways as research continues.

472
00:21:55,270 --> 00:21:56,910
But it is improving.

473
00:21:56,910 --> 00:21:58,510
This technology is
getting better and better.

474
00:21:58,510 --> 00:22:01,590
So how do we combat deepfakes?

475
00:22:01,590 --> 00:22:05,439
Well, the reality is when we
talk about combating deepfakes,

476
00:22:05,440 --> 00:22:06,910
deepfakes are misinformation.

477
00:22:06,910 --> 00:22:10,270
And misinformation, despite all
these technological advances

478
00:22:10,270 --> 00:22:12,290
we're making in detection,

479
00:22:12,290 --> 00:22:14,110
misinformation is a human problem

480
00:22:14,110 --> 00:22:16,990
that needs a human solution.

481
00:22:16,990 --> 00:22:19,080
When we talk about disinformation,

482
00:22:19,080 --> 00:22:23,020
disinformation is incredibly
difficult to manage.

483
00:22:23,020 --> 00:22:24,850
It's incredibly difficult to dislodge it

484
00:22:24,850 --> 00:22:26,919
from someone's brain once it's there.

485
00:22:26,920 --> 00:22:29,290
It plays on our existing emotions,

486
00:22:29,290 --> 00:22:32,520
belief systems and prejudices.

487
00:22:32,520 --> 00:22:34,453
And when we perceive misinformation,

488
00:22:36,195 --> 00:22:38,520
it's information that we
want to believe anyway.

489
00:22:38,520 --> 00:22:41,620
And so when we see it, we
insert it into this logic team

490
00:22:41,620 --> 00:22:42,659
in our brain,

491
00:22:42,660 --> 00:22:45,270
it just becomes another
step that fills in the gaps

492
00:22:45,270 --> 00:22:47,910
and things that we
already wanted to believe.

493
00:22:47,910 --> 00:22:48,743
So as a result,

494
00:22:48,743 --> 00:22:50,290
it's very difficult once it's in

495
00:22:50,290 --> 00:22:53,909
that logic chain to get it
out of that logic chain.

496
00:22:53,910 --> 00:22:58,190
So hitting the undo button
misinformation in someone's mind

497
00:22:58,190 --> 00:23:00,820
is very difficult.

498
00:23:00,820 --> 00:23:05,230
So how do we go about
combating disinformation?

499
00:23:05,230 --> 00:23:08,570
And this is the key when
it comes to what do we do

500
00:23:08,570 --> 00:23:10,460
about the deepfake threat.

501
00:23:10,460 --> 00:23:13,870
So when we're gonna combat
misinformation or disinformation,

502
00:23:13,870 --> 00:23:15,129
the very first thing we have to do is

503
00:23:15,130 --> 00:23:17,120
we have to put that warning out there

504
00:23:17,120 --> 00:23:20,449
that this type of technology exists.

505
00:23:20,450 --> 00:23:22,630
We're seeing media campaigns
already doing this.

506
00:23:22,630 --> 00:23:25,110
A lot of people are
talking about deepfakes

507
00:23:25,110 --> 00:23:26,939
within the political arenas,

508
00:23:26,940 --> 00:23:30,000
we're starting to see
politicians worldwide

509
00:23:30,000 --> 00:23:34,060
start to introduce legislation
around this as it pertains

510
00:23:34,060 --> 00:23:37,179
to political discourse in particular.

511
00:23:37,180 --> 00:23:38,060
And that's what we need.

512
00:23:38,060 --> 00:23:40,520
We need people to be
aware of it in advance,

513
00:23:40,520 --> 00:23:43,823
and that starts to fuel
their critical thinking.

514
00:23:46,060 --> 00:23:47,580
But when deep fakes show up,

515
00:23:47,580 --> 00:23:50,240
or any form of disinformation shows up,

516
00:23:50,240 --> 00:23:53,010
we have to rebuke that misinformation.

517
00:23:53,010 --> 00:23:57,320
We need to repeatedly and reinforce

518
00:23:57,320 --> 00:23:59,560
that this information,

519
00:23:59,560 --> 00:24:03,010
this media that you're seeing is not real.

520
00:24:03,010 --> 00:24:04,790
This is difficult.

521
00:24:04,790 --> 00:24:07,430
For instance, in the United States,

522
00:24:07,430 --> 00:24:10,660
last year, we saw a video
surface of Nancy Pelosi

523
00:24:10,660 --> 00:24:12,900
who is a high ranking official

524
00:24:12,900 --> 00:24:15,160
in the United States government.

525
00:24:15,160 --> 00:24:18,680
It was legitimate video
that wasn't even deepfaked.

526
00:24:18,680 --> 00:24:22,020
The people have just
slowed it down to the point

527
00:24:22,020 --> 00:24:24,920
that it made it look
like she was inebriated

528
00:24:24,920 --> 00:24:28,090
at a state of the union address

529
00:24:28,090 --> 00:24:30,352
that was being given by Donald Trump.

530
00:24:32,780 --> 00:24:35,760
It was reported very quickly
that this was a fake video

531
00:24:35,760 --> 00:24:36,990
that this was what happened.

532
00:24:36,990 --> 00:24:39,840
It was just you know, video
that had been slowed down.

533
00:24:39,840 --> 00:24:42,000
And yet for weeks and weeks

534
00:24:42,000 --> 00:24:45,890
after that video was being
shared on conservative websites,

535
00:24:45,890 --> 00:24:48,520
where it was being touted as real video,

536
00:24:48,520 --> 00:24:50,629
and people were getting very upset

537
00:24:50,630 --> 00:24:52,968
and raging about the fact

538
00:24:52,968 --> 00:24:56,780
that she was drunk at this address.

539
00:24:56,780 --> 00:25:00,750
So we have to keep reinforcing the reality

540
00:25:00,750 --> 00:25:02,810
that this is fake media.

541
00:25:02,810 --> 00:25:05,550
But even rein forcing it
as good as we are at that,

542
00:25:05,550 --> 00:25:07,010
is not enough.

543
00:25:07,010 --> 00:25:09,300
The step that we miss,

544
00:25:09,300 --> 00:25:14,300
is we need to figure out
how to replace that myth

545
00:25:14,460 --> 00:25:18,675
that fake information with our real facts

546
00:25:18,675 --> 00:25:20,940
in that logic chain in their brain

547
00:25:20,940 --> 00:25:22,950
that I talked about before.

548
00:25:22,950 --> 00:25:27,890
And to do that we have to play
into those same prejudices

549
00:25:27,890 --> 00:25:29,490
and values and so forth,

550
00:25:29,490 --> 00:25:31,940
that was used to make that propaganda

551
00:25:31,940 --> 00:25:34,300
stick in their minds in the first place.

552
00:25:34,300 --> 00:25:36,220
And that's the difficulty.

553
00:25:36,220 --> 00:25:39,020
It's how do I craft this message in a way

554
00:25:39,020 --> 00:25:41,129
that I'm playing on those same motivations

555
00:25:41,130 --> 00:25:44,730
and that ultimately,
the truth will fit into

556
00:25:44,730 --> 00:25:49,480
that logic chain the same
way that misinformation did.

557
00:25:49,480 --> 00:25:51,920
This is the step that
we have to get better at

558
00:25:51,920 --> 00:25:56,750
in terms of the media, in
terms of business and so forth.

559
00:25:56,750 --> 00:26:01,200
So when it comes to deepfakes,

560
00:26:01,200 --> 00:26:02,820
I'm going to leave you
with a couple thoughts.

561
00:26:02,820 --> 00:26:05,214
First of all, here's some references to

562
00:26:05,214 --> 00:26:08,290
many of the materials that I used today,

563
00:26:08,290 --> 00:26:10,570
a lot of those different studies,

564
00:26:10,570 --> 00:26:14,943
a lot of resources that are
out there available to you.

565
00:26:16,330 --> 00:26:18,379
But what should businesses do about this?

566
00:26:19,250 --> 00:26:22,230
So, immediately, what you can do

567
00:26:22,230 --> 00:26:23,600
from a business perspective,

568
00:26:23,600 --> 00:26:27,830
is start to minimize your
channels of company communication.

569
00:26:27,830 --> 00:26:29,149
Now, a lot of companies have

570
00:26:29,150 --> 00:26:31,710
their official YouTube channels,

571
00:26:31,710 --> 00:26:33,986
Twitter channels, everything else.

572
00:26:33,986 --> 00:26:35,250
Keep those.

573
00:26:35,250 --> 00:26:37,450
Use those as you always would,

574
00:26:37,450 --> 00:26:40,030
but ensure that those
are the sources of record

575
00:26:40,030 --> 00:26:42,450
for any information
coming from your company.

576
00:26:42,450 --> 00:26:46,270
So that when information is
originating from other sites,

577
00:26:46,270 --> 00:26:49,180
people know immediately that
that's not the right channel.

578
00:26:49,180 --> 00:26:53,140
Drive that consistent
distribution of information

579
00:26:53,140 --> 00:26:55,390
and ensure that anything that's important

580
00:26:55,390 --> 00:26:57,180
that comes out about your organization

581
00:26:57,180 --> 00:26:59,433
comes out through those same channels.

582
00:27:01,250 --> 00:27:04,080
Second is preparing for the future.

583
00:27:04,080 --> 00:27:06,909
The first thing we need to
do as we start to prepare

584
00:27:06,910 --> 00:27:09,630
for the future is starting to develop

585
00:27:09,630 --> 00:27:12,220
a disinformation response plan.

586
00:27:12,220 --> 00:27:16,420
This should be part of your
Incident Response Plan.

587
00:27:16,420 --> 00:27:20,000
This information be should
be treated as an incident.

588
00:27:20,000 --> 00:27:22,360
So if something occurs,
there should be a plan

589
00:27:22,360 --> 00:27:25,139
for how your organization
is going to react,

590
00:27:25,140 --> 00:27:27,010
whether something surfaces,

591
00:27:27,010 --> 00:27:31,943
that is purportedly, a video of a CEO,

592
00:27:32,800 --> 00:27:36,990
sharing information that's
not particularly complimentary

593
00:27:36,990 --> 00:27:38,740
of the organization or whatever.

594
00:27:38,740 --> 00:27:43,160
having that ability to respond quickly.

595
00:27:43,160 --> 00:27:46,130
knowing who you can work with
from a media perspective,

596
00:27:46,130 --> 00:27:48,640
who will be cooperative in
getting the truthful message

597
00:27:48,640 --> 00:27:50,640
out there, over and over again,

598
00:27:50,640 --> 00:27:54,248
helping you to ultimately
reinforce that message.

599
00:27:54,248 --> 00:27:56,230
And then extending beyond

600
00:27:56,230 --> 00:27:59,950
and really having an
organized central monitoring

601
00:27:59,950 --> 00:28:01,980
and reporting function.

602
00:28:01,980 --> 00:28:04,417
How do you become aware of deep fake media

603
00:28:04,417 --> 00:28:07,720
regarding your organization
that is out there

604
00:28:07,720 --> 00:28:08,553
on the internet,

605
00:28:08,553 --> 00:28:10,900
that is out there in public spaces,

606
00:28:10,900 --> 00:28:13,240
making sure that you can react.

607
00:28:13,240 --> 00:28:15,250
And then finally, for the long term,

608
00:28:15,250 --> 00:28:18,270
encourage responsible legislation.

609
00:28:18,270 --> 00:28:20,240
The fact of the matter is,

610
00:28:20,240 --> 00:28:23,350
this deepfakes fall into a very gray area

611
00:28:23,350 --> 00:28:25,560
when it comes to copyrights and so forth

612
00:28:25,560 --> 00:28:30,560
and legislators worldwide
do not have a good handle

613
00:28:31,180 --> 00:28:33,130
on how to deal with this.

614
00:28:33,130 --> 00:28:37,110
Additionally, look at private
sector fact verification,

615
00:28:37,110 --> 00:28:39,750
Facebook, and a few other
organizations right now

616
00:28:39,750 --> 00:28:41,610
are working on this.

617
00:28:41,610 --> 00:28:45,770
They've put up a million
dollar award for people

618
00:28:45,770 --> 00:28:48,639
that can come up with good technology

619
00:28:48,640 --> 00:28:51,090
for detecting deepfakes

620
00:28:51,090 --> 00:28:54,040
so they can add those to
their social platforms.

621
00:28:54,040 --> 00:28:58,409
And then finally, monitor
the development and detection

622
00:28:58,410 --> 00:28:59,860
and prevention countermeasures

623
00:28:59,860 --> 00:29:01,439
that are out there today.

624
00:29:01,440 --> 00:29:03,710
Be careful about buying into the hype.

625
00:29:03,710 --> 00:29:05,260
There are a number of organizations

626
00:29:05,260 --> 00:29:09,040
that are out there with the
doomsday scenarios already

627
00:29:09,040 --> 00:29:11,730
talking about how they have software

628
00:29:11,730 --> 00:29:13,840
and other solutions to help you with this.

629
00:29:13,840 --> 00:29:16,866
Be careful about diving into quickly.

630
00:29:16,866 --> 00:29:19,110
Don't buy into the hype,

631
00:29:19,110 --> 00:29:21,020
but do pay attention.

632
00:29:21,020 --> 00:29:23,129
Pay attention to the
research that's out there.

633
00:29:23,130 --> 00:29:27,150
As you saw, when our
senses are under attack,

634
00:29:27,150 --> 00:29:29,423
the world can be a very scary place.

635
00:29:30,330 --> 00:29:32,610
Researchers are continuing
to help us make sure

636
00:29:32,610 --> 00:29:34,840
that we can still trust our senses.

637
00:29:34,840 --> 00:29:36,600
But it's going to take a critical mind

638
00:29:36,600 --> 00:29:40,270
and critical approach from
all of us to make it work.

639
00:29:40,270 --> 00:29:42,490
So with that,

640
00:29:42,490 --> 00:29:45,130
I wanna say, thank you for
joining our session today.

641
00:29:45,130 --> 00:29:47,410
It's been lovely speaking to all of you.

642
00:29:47,410 --> 00:29:48,880
I will be available

643
00:29:49,880 --> 00:29:51,950
to continue answering
your questions throughout.

644
00:29:51,950 --> 00:29:55,710
So have a wonderful day
and a wonderful week.

645
00:29:55,710 --> 00:29:58,930
And certainly feel free to reach out to me

646
00:29:58,930 --> 00:30:01,100
at any of my social media outlets.

647
00:30:01,100 --> 00:30:03,659
And if you want to continue
this discussion further

648
00:30:03,660 --> 00:30:06,680
outside of the context of today's session,

649
00:30:06,680 --> 00:30:09,050
I'm happy to have those conversations.

650
00:30:09,050 --> 00:30:10,253
Thank you very much.

