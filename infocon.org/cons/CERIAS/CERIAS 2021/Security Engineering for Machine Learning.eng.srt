1
00:00:01,520 --> 00:00:04,000
and good afternoon or good morning

2
00:00:04,000 --> 00:00:06,000
depending on where you are in the world

3
00:00:06,000 --> 00:00:07,279
or suspected

4
00:00:07,279 --> 00:00:09,599
could be some good evenings as well

5
00:00:09,599 --> 00:00:11,679
we're very excited to kick off the

6
00:00:11,679 --> 00:00:12,480
serious

7
00:00:12,480 --> 00:00:14,719
summer security series with long

8
00:00:14,719 --> 00:00:16,800
long-time friend of sirius dr gary

9
00:00:16,800 --> 00:00:17,840
mcgraw

10
00:00:17,840 --> 00:00:19,680
gary is co-founder of the berryville

11
00:00:19,680 --> 00:00:21,199
institute of machine learning

12
00:00:21,199 --> 00:00:23,279
and recognized globally as a software

13
00:00:23,279 --> 00:00:24,560
security authority

14
00:00:24,560 --> 00:00:26,400
and has authored eight best-selling

15
00:00:26,400 --> 00:00:28,240
books on this topic

16
00:00:28,240 --> 00:00:30,400
gary produced the monthly silver bullet

17
00:00:30,400 --> 00:00:32,719
security podcast for ieee security and

18
00:00:32,719 --> 00:00:34,719
privacy magazine for 13 years

19
00:00:34,719 --> 00:00:37,120
where he had the opportunity to host

20
00:00:37,120 --> 00:00:39,520
serious founder gene spaff spafford

21
00:00:39,520 --> 00:00:41,840
so we're happy for him to have to return

22
00:00:41,840 --> 00:00:43,680
the favor gary welcome back to purdue

23
00:00:43,680 --> 00:00:44,640
we're excited about

24
00:00:44,640 --> 00:00:46,800
excited to hear you excited to have you

25
00:00:46,800 --> 00:00:49,200
here and excited to hear your talk

26
00:00:49,200 --> 00:00:51,600
great to be here joel so i understand

27
00:00:51,600 --> 00:00:53,680
that i apparently do this every 10 years

28
00:00:53,680 --> 00:00:54,320
or so

29
00:00:54,320 --> 00:00:57,520
first talk was what 2001 and then

30
00:00:57,520 --> 00:00:59,600
we'll pretend the other one was 2011 was

31
00:00:59,600 --> 00:01:01,440
really 2009.

32
00:01:01,440 --> 00:01:06,000
and now it's 2021 how did that happen

33
00:01:06,080 --> 00:01:08,320
well we'll get you on the book for 2031

34
00:01:08,320 --> 00:01:09,119
right now

35
00:01:09,119 --> 00:01:11,760
that sounds great i'm all for it um so

36
00:01:11,760 --> 00:01:12,880
do you want to tell us what the topic

37
00:01:12,880 --> 00:01:14,159
will be now or do you want to

38
00:01:14,159 --> 00:01:17,040
let us know later

39
00:01:17,439 --> 00:01:20,479
no yeah no you know hand wave

40
00:01:20,479 --> 00:01:23,680
equivocation how about that um so

41
00:01:23,680 --> 00:01:26,799
shall i go ahead and start it's all

42
00:01:26,799 --> 00:01:27,680
yours thank you

43
00:01:27,680 --> 00:01:31,840
let me let me share my screen here

44
00:01:32,799 --> 00:01:35,600
so what i'm going to be talking about is

45
00:01:35,600 --> 00:01:36,560
um

46
00:01:36,560 --> 00:01:38,960
security engineering for machine

47
00:01:38,960 --> 00:01:40,560
learning and

48
00:01:40,560 --> 00:01:43,520
there's kind of an important thing here

49
00:01:43,520 --> 00:01:45,040
which is a lot of people are using

50
00:01:45,040 --> 00:01:46,079
machine learning

51
00:01:46,079 --> 00:01:48,240
to do security so you have machine

52
00:01:48,240 --> 00:01:49,920
learning as say

53
00:01:49,920 --> 00:01:53,200
an enhancement of a security feature

54
00:01:53,200 --> 00:01:56,079
and fewer people are doing what i'm

55
00:01:56,079 --> 00:01:57,119
doing which is

56
00:01:57,119 --> 00:02:00,640
the security of machine learning so

57
00:02:00,640 --> 00:02:02,719
my view is that if you can't secure your

58
00:02:02,719 --> 00:02:03,920
machine learning stuff

59
00:02:03,920 --> 00:02:06,159
then why would you possibly use machine

60
00:02:06,159 --> 00:02:07,520
learning as a feature

61
00:02:07,520 --> 00:02:11,520
to enhance security but all people don't

62
00:02:11,520 --> 00:02:14,319
you know don't approach it the same way

63
00:02:14,319 --> 00:02:15,120
and then there's

64
00:02:15,120 --> 00:02:16,400
there's further complication in the

65
00:02:16,400 --> 00:02:17,599
field because there's some people that

66
00:02:17,599 --> 00:02:18,319
use

67
00:02:18,319 --> 00:02:21,200
security their machine learning for

68
00:02:21,200 --> 00:02:22,560
security but they're really worried

69
00:02:22,560 --> 00:02:23,920
about the security of the thing that

70
00:02:23,920 --> 00:02:25,440
they're building so that so they

71
00:02:25,440 --> 00:02:27,200
also end up doing security engineering

72
00:02:27,200 --> 00:02:29,680
for machine learning too

73
00:02:29,680 --> 00:02:31,760
um but what i'm talking about i call

74
00:02:31,760 --> 00:02:33,680
mlsec some people

75
00:02:33,680 --> 00:02:35,440
call this adversarial machine learning

76
00:02:35,440 --> 00:02:37,760
i'm not a big fan of that terminology

77
00:02:37,760 --> 00:02:40,400
but they call it that anyway and i'll

78
00:02:40,400 --> 00:02:41,280
give you a little bit of a

79
00:02:41,280 --> 00:02:44,560
background so joel mentioned a little

80
00:02:44,560 --> 00:02:45,120
bit

81
00:02:45,120 --> 00:02:47,760
i've been doing computer security stuff

82
00:02:47,760 --> 00:02:49,360
since 1995

83
00:02:49,360 --> 00:02:50,720
and i wrote one of the first books in

84
00:02:50,720 --> 00:02:53,599
the world on software security in 2001.

85
00:02:53,599 --> 00:02:56,160
i had a company called sigil that i ran

86
00:02:56,160 --> 00:02:57,200
up to about

87
00:02:57,200 --> 00:02:59,599
500 people with my colleagues and we

88
00:02:59,599 --> 00:03:02,159
sold that to synopsis in 2016.

89
00:03:02,159 --> 00:03:04,400
i worked for a public company for three

90
00:03:04,400 --> 00:03:06,640
years or two years something like that

91
00:03:06,640 --> 00:03:08,959
oh yeah two years and then decided that

92
00:03:08,959 --> 00:03:11,040
having a boss is a bad idea

93
00:03:11,040 --> 00:03:13,920
and i retired and i'm super bad at

94
00:03:13,920 --> 00:03:15,519
retirement i'm better at playing violin

95
00:03:15,519 --> 00:03:17,840
or building big giant bonfires

96
00:03:17,840 --> 00:03:21,120
and what i decided to do intellectually

97
00:03:21,120 --> 00:03:23,040
on retirement was take a look at machine

98
00:03:23,040 --> 00:03:23,519
learning

99
00:03:23,519 --> 00:03:25,760
because believe it or not back in the

100
00:03:25,760 --> 00:03:27,200
old days when i was a grad

101
00:03:27,200 --> 00:03:29,200
student i was very interested in machine

102
00:03:29,200 --> 00:03:31,840
learning and ai i actually have a phd in

103
00:03:31,840 --> 00:03:33,920
cognitive science and computer science

104
00:03:33,920 --> 00:03:34,560
from

105
00:03:34,560 --> 00:03:38,000
indiana down in bloomington um and i

106
00:03:38,000 --> 00:03:40,000
studied with doug hofstetter so part of

107
00:03:40,000 --> 00:03:41,200
what i did

108
00:03:41,200 --> 00:03:43,280
in grad school was build machine

109
00:03:43,280 --> 00:03:44,560
learning models

110
00:03:44,560 --> 00:03:46,720
using connectionism and genetic

111
00:03:46,720 --> 00:03:48,000
algorithms and

112
00:03:48,000 --> 00:03:50,480
and other things and wrote a series of

113
00:03:50,480 --> 00:03:51,920
publications about that

114
00:03:51,920 --> 00:03:53,360
but i really hadn't thought about

115
00:03:53,360 --> 00:03:55,360
machine learning for 25 years and i was

116
00:03:55,360 --> 00:03:56,640
reading all the stuff

117
00:03:56,640 --> 00:04:00,000
in the press about how much kind of

118
00:04:00,000 --> 00:04:02,640
progress we've made in the field and you

119
00:04:02,640 --> 00:04:04,239
know all the popular press you never can

120
00:04:04,239 --> 00:04:05,920
tell if what you read is true

121
00:04:05,920 --> 00:04:09,360
and uh and so i formed a reading group

122
00:04:09,360 --> 00:04:11,040
just to read through some of those

123
00:04:11,040 --> 00:04:13,280
papers and go look at the science and

124
00:04:13,280 --> 00:04:14,000
see what's

125
00:04:14,000 --> 00:04:16,720
actually happened out there and what we

126
00:04:16,720 --> 00:04:18,798
very quickly determined about two years

127
00:04:18,798 --> 00:04:19,279
ago

128
00:04:19,279 --> 00:04:21,839
was that a ton of progress had been made

129
00:04:21,839 --> 00:04:23,680
in machine learning but the progress was

130
00:04:23,680 --> 00:04:24,720
really

131
00:04:24,720 --> 00:04:28,000
due to unbelievable computer cycles that

132
00:04:28,000 --> 00:04:30,080
were available for super cheap

133
00:04:30,080 --> 00:04:32,639
and big giant reams of data that we

134
00:04:32,639 --> 00:04:33,680
didn't have before

135
00:04:33,680 --> 00:04:35,360
and if you put those two things together

136
00:04:35,360 --> 00:04:37,759
with the algorithms we mostly knew about

137
00:04:37,759 --> 00:04:40,960
um even in the late 90s late 80s

138
00:04:40,960 --> 00:04:43,280
and early 90s when roma heart mcclellan

139
00:04:43,280 --> 00:04:46,800
were working on this stuff in pdp series

140
00:04:46,800 --> 00:04:48,400
most of the progress you know is

141
00:04:48,400 --> 00:04:50,320
accounted for by those

142
00:04:50,320 --> 00:04:52,880
those big changes in hardware and data

143
00:04:52,880 --> 00:04:54,240
sets

144
00:04:54,240 --> 00:04:56,960
and not much attention some but not much

145
00:04:56,960 --> 00:04:58,639
had been paid to

146
00:04:58,639 --> 00:05:00,880
the security issues surrounding machine

147
00:05:00,880 --> 00:05:02,400
learning itself

148
00:05:02,400 --> 00:05:05,520
um so we started reading um

149
00:05:05,520 --> 00:05:08,240
papers and thinking about that and and

150
00:05:08,240 --> 00:05:09,120
then we

151
00:05:09,120 --> 00:05:10,800
very quickly realized that some more

152
00:05:10,800 --> 00:05:12,080
work needed to be done in machine

153
00:05:12,080 --> 00:05:12,880
learning so we

154
00:05:12,880 --> 00:05:15,280
we founded an institute which is called

155
00:05:15,280 --> 00:05:16,880
the berryville institute of machine

156
00:05:16,880 --> 00:05:17,360
learning

157
00:05:17,360 --> 00:05:19,280
now those of you in west lafayette or

158
00:05:19,280 --> 00:05:21,199
are familiar with west lafayette you may

159
00:05:21,199 --> 00:05:22,960
think you guys are off the beaten path

160
00:05:22,960 --> 00:05:25,360
but i live in a tiny little speck of a

161
00:05:25,360 --> 00:05:26,080
county

162
00:05:26,080 --> 00:05:29,120
and i my office is in uh 10 miles away

163
00:05:29,120 --> 00:05:30,320
from the county seat

164
00:05:30,320 --> 00:05:31,919
and there are probably more cows than

165
00:05:31,919 --> 00:05:33,440
people in my county

166
00:05:33,440 --> 00:05:35,280
so this is one of our logos and in the

167
00:05:35,280 --> 00:05:37,520
middle there is a shape that's actually

168
00:05:37,520 --> 00:05:39,039
the shape of my county clark county

169
00:05:39,039 --> 00:05:40,800
virginia where the little west virginia

170
00:05:40,800 --> 00:05:42,000
booger

171
00:05:42,000 --> 00:05:43,680
goes over the top of virginia at the

172
00:05:43,680 --> 00:05:45,360
very very top

173
00:05:45,360 --> 00:05:47,680
and berryville is is somewhere in the

174
00:05:47,680 --> 00:05:50,080
middle of that

175
00:05:50,080 --> 00:05:52,639
and so the berryville institute of

176
00:05:52,639 --> 00:05:53,919
machine learning is what we

177
00:05:53,919 --> 00:05:55,759
we've been calling our little

178
00:05:55,759 --> 00:05:57,199
organization

179
00:05:57,199 --> 00:06:00,000
um this is me and three other people and

180
00:06:00,000 --> 00:06:01,919
the three other co-founders and i

181
00:06:01,919 --> 00:06:04,400
did a bunch of work and we wrote a risk

182
00:06:04,400 --> 00:06:05,280
analysis

183
00:06:05,280 --> 00:06:08,560
of machine learning systems generically

184
00:06:08,560 --> 00:06:09,520
speaking

185
00:06:09,520 --> 00:06:11,759
which i'm going to talk about today we

186
00:06:11,759 --> 00:06:14,240
identified 78 risks and once we'd done

187
00:06:14,240 --> 00:06:15,120
that

188
00:06:15,120 --> 00:06:16,800
the open philanthropy people got in

189
00:06:16,800 --> 00:06:18,720
touch with us and they said hey you know

190
00:06:18,720 --> 00:06:20,960
this is great work we would like to fund

191
00:06:20,960 --> 00:06:21,759
you guys

192
00:06:21,759 --> 00:06:23,360
would you write something up and we'll

193
00:06:23,360 --> 00:06:24,560
give you some money so we wrote six

194
00:06:24,560 --> 00:06:26,000
sentences and they gave us some money

195
00:06:26,000 --> 00:06:26,639
and

196
00:06:26,639 --> 00:06:28,800
and now i had to form it into a

197
00:06:28,800 --> 00:06:30,479
corporation so bill's

198
00:06:30,479 --> 00:06:33,520
actually a virginia c corp and i'm an

199
00:06:33,520 --> 00:06:35,199
employee of myself

200
00:06:35,199 --> 00:06:36,639
needless to say another way of saying

201
00:06:36,639 --> 00:06:38,160
all this is that i really suck at

202
00:06:38,160 --> 00:06:39,440
retirement

203
00:06:39,440 --> 00:06:41,520
but i am very interested in this space

204
00:06:41,520 --> 00:06:43,440
something that we were doing for fun

205
00:06:43,440 --> 00:06:45,039
and what i want to share with you today

206
00:06:45,039 --> 00:06:46,800
is the results of

207
00:06:46,800 --> 00:06:49,120
a couple of years worth of work first

208
00:06:49,120 --> 00:06:50,639
let's start with an introduction to

209
00:06:50,639 --> 00:06:52,080
machine learning to catch everybody up

210
00:06:52,080 --> 00:06:53,280
to speed

211
00:06:53,280 --> 00:06:56,000
because the terms ai and machine

212
00:06:56,000 --> 00:06:57,840
learning and deep learning and all this

213
00:06:57,840 --> 00:06:58,319
stuff

214
00:06:58,319 --> 00:07:00,240
get thrown around kind of with impunity

215
00:07:00,240 --> 00:07:02,400
by the press as if they're equivalent

216
00:07:02,400 --> 00:07:03,599
and they're really not

217
00:07:03,599 --> 00:07:05,599
um we're never going to stop the press

218
00:07:05,599 --> 00:07:06,800
from doing what they do

219
00:07:06,800 --> 00:07:09,199
but kind of here's here's here's a view

220
00:07:09,199 --> 00:07:10,880
that i stole from melanie mitchell who

221
00:07:10,880 --> 00:07:12,800
wrote a fantastic book

222
00:07:12,800 --> 00:07:14,800
called artificial intelligence a guide

223
00:07:14,800 --> 00:07:16,240
for thinking humans you can see the

224
00:07:16,240 --> 00:07:16,960
cover

225
00:07:16,960 --> 00:07:19,599
right there and melanie allowed me to

226
00:07:19,599 --> 00:07:20,720
steal her slides

227
00:07:20,720 --> 00:07:24,080
so back in the 50s

228
00:07:24,080 --> 00:07:26,639
when mccarthy and those guys were were

229
00:07:26,639 --> 00:07:27,840
thinking about

230
00:07:27,840 --> 00:07:31,199
a.i there was a very famous um

231
00:07:31,199 --> 00:07:34,560
meeting at dartmouth and a few

232
00:07:34,560 --> 00:07:36,240
white men got together and they decided

233
00:07:36,240 --> 00:07:37,840
they could solve a.i

234
00:07:37,840 --> 00:07:39,599
and i think it was a week or maybe two

235
00:07:39,599 --> 00:07:41,360
weeks and it turned out they were way

236
00:07:41,360 --> 00:07:42,479
wrong

237
00:07:42,479 --> 00:07:44,560
but artificial intelligence really got

238
00:07:44,560 --> 00:07:46,240
its start in the 50s

239
00:07:46,240 --> 00:07:48,800
um and you know machine learning was a

240
00:07:48,800 --> 00:07:49,840
little bit a

241
00:07:49,840 --> 00:07:51,280
piece of that there was something called

242
00:07:51,280 --> 00:07:53,199
perceptrons that was

243
00:07:53,199 --> 00:07:55,039
uh that came about by a guy named

244
00:07:55,039 --> 00:07:57,360
rosenblatt early in the early days

245
00:07:57,360 --> 00:07:58,960
and that's kind of you know you can

246
00:07:58,960 --> 00:08:00,800
think about that green squares

247
00:08:00,800 --> 00:08:02,720
perceptrons and maybe

248
00:08:02,720 --> 00:08:05,520
some aspects of bayesian learning and

249
00:08:05,520 --> 00:08:06,000
then

250
00:08:06,000 --> 00:08:07,840
connectionism or deep learning was a

251
00:08:07,840 --> 00:08:09,840
little tiny subset of that all the way

252
00:08:09,840 --> 00:08:11,280
up through the 80s

253
00:08:11,280 --> 00:08:13,919
really when uh the pdp books were

254
00:08:13,919 --> 00:08:14,800
published

255
00:08:14,800 --> 00:08:17,680
so ai is this big thing there's lots of

256
00:08:17,680 --> 00:08:18,960
stuff that happened in ao there's lots

257
00:08:18,960 --> 00:08:20,800
of stuff still happening in symbolic ai

258
00:08:20,800 --> 00:08:21,840
and everywhere else

259
00:08:21,840 --> 00:08:24,240
that's not really machine learning but

260
00:08:24,240 --> 00:08:25,680
this is what it looked like

261
00:08:25,680 --> 00:08:27,840
up until i went to grad school then

262
00:08:27,840 --> 00:08:30,560
around the early 1990s

263
00:08:30,560 --> 00:08:32,479
it sort of turned into this because of

264
00:08:32,479 --> 00:08:34,640
pdp and the resurgence of connectionism

265
00:08:34,640 --> 00:08:35,519
so we had

266
00:08:35,519 --> 00:08:37,839
a.i and then machine learning was big

267
00:08:37,839 --> 00:08:39,440
but still deep learning and you know

268
00:08:39,440 --> 00:08:40,640
multi-layer

269
00:08:40,640 --> 00:08:42,159
connectionist networks were pretty

270
00:08:42,159 --> 00:08:44,240
little nobody invented convolution

271
00:08:44,240 --> 00:08:45,760
networks yet

272
00:08:45,760 --> 00:08:49,360
and then in the 2010s this happened

273
00:08:49,360 --> 00:08:52,399
now um when people talk about ai

274
00:08:52,399 --> 00:08:54,720
in the popular press they really mean

275
00:08:54,720 --> 00:08:55,839
deep learning

276
00:08:55,839 --> 00:08:58,480
so you get the terms ai and machine

277
00:08:58,480 --> 00:08:59,839
learning and deep learning

278
00:08:59,839 --> 00:09:02,480
all thrown around the same but basically

279
00:09:02,480 --> 00:09:04,399
what i'm talking about today when i say

280
00:09:04,399 --> 00:09:06,160
machine learning security i want to

281
00:09:06,160 --> 00:09:09,200
address the problem of um of uh

282
00:09:09,200 --> 00:09:12,160
multi-layer connections networks so deep

283
00:09:12,160 --> 00:09:12,959
learning

284
00:09:12,959 --> 00:09:15,920
uh uh kinds of things for machine

285
00:09:15,920 --> 00:09:16,800
learning

286
00:09:16,800 --> 00:09:18,480
so that's kind of you know setting the

287
00:09:18,480 --> 00:09:19,839
stage for

288
00:09:19,839 --> 00:09:22,320
uh what i'm gonna be talking about of

289
00:09:22,320 --> 00:09:23,279
course

290
00:09:23,279 --> 00:09:26,000
the the whole idea behind these deep

291
00:09:26,000 --> 00:09:26,959
neural networks

292
00:09:26,959 --> 00:09:30,320
is to use the brain as a metaphor for

293
00:09:30,320 --> 00:09:32,959
how computation may occur and the idea

294
00:09:32,959 --> 00:09:34,399
behind deep learning is well let's just

295
00:09:34,399 --> 00:09:36,080
look at something like the visual system

296
00:09:36,080 --> 00:09:37,360
in the human brain

297
00:09:37,360 --> 00:09:39,360
and let's figure out how that kind of

298
00:09:39,360 --> 00:09:40,640
sort of works and then we'll build a

299
00:09:40,640 --> 00:09:42,959
model of that with artificial neurons

300
00:09:42,959 --> 00:09:43,440
and

301
00:09:43,440 --> 00:09:45,760
and connections that are the same now if

302
00:09:45,760 --> 00:09:47,279
you look at a real neuron

303
00:09:47,279 --> 00:09:49,839
and you look at say the synaptic calcium

304
00:09:49,839 --> 00:09:51,680
channel stuff and all of the

305
00:09:51,680 --> 00:09:54,000
chemical soup that that stuff floats in

306
00:09:54,000 --> 00:09:55,120
and the way

307
00:09:55,120 --> 00:09:58,000
um you know axons and neurons really

308
00:09:58,000 --> 00:09:59,279
work

309
00:09:59,279 --> 00:10:01,440
what we're using today to count as a

310
00:10:01,440 --> 00:10:03,440
neuron is a very silly little model

311
00:10:03,440 --> 00:10:04,720
indeed

312
00:10:04,720 --> 00:10:06,480
but the important part according to the

313
00:10:06,480 --> 00:10:07,920
deep learning people

314
00:10:07,920 --> 00:10:09,839
is the number of connections between

315
00:10:09,839 --> 00:10:11,519
these neurons and the way thresholding

316
00:10:11,519 --> 00:10:12,079
works

317
00:10:12,079 --> 00:10:14,640
and the fact that we can feed back

318
00:10:14,640 --> 00:10:16,240
through a network and adjust

319
00:10:16,240 --> 00:10:19,040
weights over millions of cycles and in

320
00:10:19,040 --> 00:10:20,079
that way

321
00:10:20,079 --> 00:10:23,040
um change the statistical model that

322
00:10:23,040 --> 00:10:24,480
we're working on

323
00:10:24,480 --> 00:10:26,720
so here we have the visual system of the

324
00:10:26,720 --> 00:10:28,800
brain you have a picture of a dog and

325
00:10:28,800 --> 00:10:31,920
you know it goes into your retina and

326
00:10:31,920 --> 00:10:32,399
you got

327
00:10:32,399 --> 00:10:34,320
edge detection in your retina some

328
00:10:34,320 --> 00:10:35,839
simple shapes begin to get

329
00:10:35,839 --> 00:10:38,160
bit built up then you get into v1

330
00:10:38,160 --> 00:10:39,360
through v4

331
00:10:39,360 --> 00:10:41,279
and some more complex shapes get built

332
00:10:41,279 --> 00:10:42,959
up and it's sort of think about it as

333
00:10:42,959 --> 00:10:45,839
layers and ultimately you have something

334
00:10:45,839 --> 00:10:47,040
that recognizes

335
00:10:47,040 --> 00:10:50,079
faces and objects in your brain

336
00:10:50,079 --> 00:10:52,800
and so the connectionists or the deep

337
00:10:52,800 --> 00:10:53,839
learning people said

338
00:10:53,839 --> 00:10:56,160
well let's just take inspiration from

339
00:10:56,160 --> 00:10:57,279
that and let's build

340
00:10:57,279 --> 00:10:59,040
convolutional neural networks that do

341
00:10:59,040 --> 00:11:01,360
the same thing loosely speaking so we've

342
00:11:01,360 --> 00:11:02,720
got a bunch of layers we have an

343
00:11:02,720 --> 00:11:05,519
input layer where the raw pixels go or

344
00:11:05,519 --> 00:11:07,200
the process pixels go

345
00:11:07,200 --> 00:11:09,440
then we have a bunch of different layers

346
00:11:09,440 --> 00:11:10,240
and we feed

347
00:11:10,240 --> 00:11:13,680
up through those layers to the output

348
00:11:13,680 --> 00:11:15,440
and now the output we have a

349
00:11:15,440 --> 00:11:17,360
classification module

350
00:11:17,360 --> 00:11:19,839
um that takes all of that you know

351
00:11:19,839 --> 00:11:21,360
activation that's come up through the

352
00:11:21,360 --> 00:11:22,000
network

353
00:11:22,000 --> 00:11:26,160
and it said oh that's 85 dog and 15

354
00:11:26,160 --> 00:11:28,880
cat now obviously that's not really the

355
00:11:28,880 --> 00:11:30,240
way people do it you don't say oh that's

356
00:11:30,240 --> 00:11:31,360
eight

357
00:11:31,360 --> 00:11:33,920
percent dog you got there very cute 85

358
00:11:33,920 --> 00:11:34,560
dog

359
00:11:34,560 --> 00:11:37,680
you just say dog and so it's important

360
00:11:37,680 --> 00:11:38,640
to realize

361
00:11:38,640 --> 00:11:41,839
that when people say understand or they

362
00:11:41,839 --> 00:11:42,399
say

363
00:11:42,399 --> 00:11:45,600
natural language processing or they say

364
00:11:45,600 --> 00:11:48,720
any number of other things that this is

365
00:11:48,720 --> 00:11:51,279
loosely inspired by the brain but it's

366
00:11:51,279 --> 00:11:53,440
not really very brain-like

367
00:11:53,440 --> 00:11:56,240
in fact what we have is this massive

368
00:11:56,240 --> 00:11:58,560
statistical associative engine that can

369
00:11:58,560 --> 00:12:00,560
be generative and predictive

370
00:12:00,560 --> 00:12:04,320
um and it has in many cases hundreds of

371
00:12:04,320 --> 00:12:06,000
of layers that we're feeding that

372
00:12:06,000 --> 00:12:07,519
activation through

373
00:12:07,519 --> 00:12:10,959
sometimes recurrent layers sometimes

374
00:12:10,959 --> 00:12:12,880
feedback swooshes around through the

375
00:12:12,880 --> 00:12:13,839
system

376
00:12:13,839 --> 00:12:16,240
time is an issue we'll talk about that

377
00:12:16,240 --> 00:12:17,360
in a moment

378
00:12:17,360 --> 00:12:20,720
but it's critical to read

379
00:12:20,720 --> 00:12:23,040
the popular press coverage with an eye

380
00:12:23,040 --> 00:12:24,639
towards what's really happening

381
00:12:24,639 --> 00:12:26,399
and so one of the questions we wanted to

382
00:12:26,399 --> 00:12:28,560
ask is when you build one of these

383
00:12:28,560 --> 00:12:29,519
things

384
00:12:29,519 --> 00:12:31,839
how could an attacker really screw you

385
00:12:31,839 --> 00:12:33,920
up like what are the risks associated

386
00:12:33,920 --> 00:12:35,519
with machine learning

387
00:12:35,519 --> 00:12:37,920
and of course there are many gajillions

388
00:12:37,920 --> 00:12:38,639
of different

389
00:12:38,639 --> 00:12:40,720
sorts of machine learning architectures

390
00:12:40,720 --> 00:12:42,320
when it comes right down to it

391
00:12:42,320 --> 00:12:45,360
so we decided what we would do is get up

392
00:12:45,360 --> 00:12:48,320
over the whole problem and talk about a

393
00:12:48,320 --> 00:12:50,399
generic machine learning model

394
00:12:50,399 --> 00:12:52,320
as kind of our first order of business

395
00:12:52,320 --> 00:12:54,240
and so we came up with this

396
00:12:54,240 --> 00:12:56,079
generic machine learning model that has

397
00:12:56,079 --> 00:12:57,279
nine

398
00:12:57,279 --> 00:12:59,680
components and the components here are

399
00:12:59,680 --> 00:13:01,680
shown in this purple diagram

400
00:13:01,680 --> 00:13:04,240
processes are ovals like data set

401
00:13:04,240 --> 00:13:05,120
assembly

402
00:13:05,120 --> 00:13:08,160
and collections of stuff like number

403
00:13:08,160 --> 00:13:09,839
three data sets training

404
00:13:09,839 --> 00:13:12,720
validation and test sets are rectangles

405
00:13:12,720 --> 00:13:14,560
and the information

406
00:13:14,560 --> 00:13:17,120
flow is represented by arrows so we have

407
00:13:17,120 --> 00:13:18,800
raw data in the world that goes to data

408
00:13:18,800 --> 00:13:20,480
set assembly that goes to these data

409
00:13:20,480 --> 00:13:21,519
sets and then that goes through the

410
00:13:21,519 --> 00:13:22,800
learning algorithm and you have the

411
00:13:22,800 --> 00:13:23,519
model

412
00:13:23,519 --> 00:13:25,680
so that's kind of what we thought we

413
00:13:25,680 --> 00:13:27,920
would do to organize our thinking about

414
00:13:27,920 --> 00:13:28,480
risks

415
00:13:28,480 --> 00:13:31,519
we use this model to think about risks

416
00:13:31,519 --> 00:13:32,720
in every component

417
00:13:32,720 --> 00:13:35,440
and what i want to share with you today

418
00:13:35,440 --> 00:13:36,079
is

419
00:13:36,079 --> 00:13:39,040
our results i'm going to show you one or

420
00:13:39,040 --> 00:13:41,279
two risks associated with each one of

421
00:13:41,279 --> 00:13:43,440
these components so that's 18.

422
00:13:43,440 --> 00:13:45,920
two for the whole system so i'm going to

423
00:13:45,920 --> 00:13:47,279
show you 20 risks

424
00:13:47,279 --> 00:13:49,600
um over the course of this talk and you

425
00:13:49,600 --> 00:13:50,639
should realize

426
00:13:50,639 --> 00:13:52,800
that the 20 risk i'm going to cover are

427
00:13:52,800 --> 00:13:56,959
a subset of the 78 risks we identified

428
00:13:56,959 --> 00:13:58,720
in this work you can hear my puppy back

429
00:13:58,720 --> 00:14:00,160
there whining he thinks he wants out

430
00:14:00,160 --> 00:14:02,000
because it's getting hot up here

431
00:14:02,000 --> 00:14:03,760
but i'm not going to let him out so he's

432
00:14:03,760 --> 00:14:05,120
just going to have to sit down and go to

433
00:14:05,120 --> 00:14:06,480
sleep

434
00:14:06,480 --> 00:14:08,480
all right so um here's our machine

435
00:14:08,480 --> 00:14:10,240
learning risk analysis and we'll start

436
00:14:10,240 --> 00:14:11,600
with component one

437
00:14:11,600 --> 00:14:14,480
which is raw data in the real world now

438
00:14:14,480 --> 00:14:15,279
data

439
00:14:15,279 --> 00:14:17,760
play a critical role in machine learning

440
00:14:17,760 --> 00:14:19,120
systems in fact

441
00:14:19,120 --> 00:14:22,240
data are a really essential part of any

442
00:14:22,240 --> 00:14:23,519
machine learning system

443
00:14:23,519 --> 00:14:25,279
you think about it you have this sort of

444
00:14:25,279 --> 00:14:26,880
multi-layer thing and

445
00:14:26,880 --> 00:14:29,680
the adjustments are made when you put in

446
00:14:29,680 --> 00:14:30,800
some input

447
00:14:30,800 --> 00:14:32,399
and you get some output and you go oh

448
00:14:32,399 --> 00:14:34,160
that's wrong we need to adjust all those

449
00:14:34,160 --> 00:14:35,760
weights to make it righter

450
00:14:35,760 --> 00:14:38,320
and you do that millions of times you do

451
00:14:38,320 --> 00:14:40,560
all those adjustments

452
00:14:40,560 --> 00:14:44,160
and you can see how the ultimate model

453
00:14:44,160 --> 00:14:47,760
is is really very deeply impacted by the

454
00:14:47,760 --> 00:14:49,440
data set that it's trained on

455
00:14:49,440 --> 00:14:52,320
and in fact data is already a problem in

456
00:14:52,320 --> 00:14:54,160
security as we all know data security is

457
00:14:54,160 --> 00:14:56,079
hard but it gets even harder

458
00:14:56,079 --> 00:14:58,959
when we think about data being built

459
00:14:58,959 --> 00:15:00,079
into the machine

460
00:15:00,079 --> 00:15:03,040
itself through machine learning in fact

461
00:15:03,040 --> 00:15:04,639
i think about 60

462
00:15:04,639 --> 00:15:06,399
of the risk in from a security

463
00:15:06,399 --> 00:15:08,399
perspective in machine learning

464
00:15:08,399 --> 00:15:11,600
comes about because of data so

465
00:15:11,600 --> 00:15:13,519
when you go out to find a data set

466
00:15:13,519 --> 00:15:15,600
there's lots of raw data out there to be

467
00:15:15,600 --> 00:15:16,720
manipulated

468
00:15:16,720 --> 00:15:20,320
and um there are ways that attackers can

469
00:15:20,320 --> 00:15:22,480
attack this stuff but you know let me

470
00:15:22,480 --> 00:15:25,199
give you two examples of the 13 risks

471
00:15:25,199 --> 00:15:26,399
that we identified

472
00:15:26,399 --> 00:15:28,959
with raw data in the real world the

473
00:15:28,959 --> 00:15:31,199
first is data confidentiality

474
00:15:31,199 --> 00:15:32,720
this should be pretty obvious but if you

475
00:15:32,720 --> 00:15:34,800
train up a machine learning system

476
00:15:34,800 --> 00:15:37,839
on data that are confidential or they

477
00:15:37,839 --> 00:15:39,360
have secrets in them

478
00:15:39,360 --> 00:15:42,320
um those secrets get embedded directly

479
00:15:42,320 --> 00:15:42,880
into

480
00:15:42,880 --> 00:15:45,360
into the machine learning model and so

481
00:15:45,360 --> 00:15:46,399
if you have a

482
00:15:46,399 --> 00:15:48,480
ml system that's trained up on

483
00:15:48,480 --> 00:15:50,560
confidential or sensitive data those

484
00:15:50,560 --> 00:15:52,160
data are going to be built in

485
00:15:52,160 --> 00:15:54,560
and then attacks to extract that

486
00:15:54,560 --> 00:15:56,639
sensitive or confidential information

487
00:15:56,639 --> 00:15:59,120
from the ml system like indirectly

488
00:15:59,120 --> 00:16:00,399
through normal use

489
00:16:00,399 --> 00:16:02,560
are pretty well known um another way of

490
00:16:02,560 --> 00:16:04,160
putting that is if you put the data in

491
00:16:04,160 --> 00:16:05,680
there they're in there somewhere and

492
00:16:05,680 --> 00:16:07,040
somebody can probably get them back

493
00:16:07,040 --> 00:16:09,680
out that means if you use machine

494
00:16:09,680 --> 00:16:11,759
learning systems for places

495
00:16:11,759 --> 00:16:14,480
for sub domains where you know secrecy

496
00:16:14,480 --> 00:16:16,079
is super important like say

497
00:16:16,079 --> 00:16:18,480
medical devices or something like that

498
00:16:18,480 --> 00:16:20,320
you have to think two or three times

499
00:16:20,320 --> 00:16:22,720
about the kind of data that you're

500
00:16:22,720 --> 00:16:25,440
exposing and and what to do about that

501
00:16:25,440 --> 00:16:27,680
there are some ways to fuzz data and

502
00:16:27,680 --> 00:16:29,519
move it around and cleanse data but

503
00:16:29,519 --> 00:16:30,399
really

504
00:16:30,399 --> 00:16:31,839
they're not as good as we would like

505
00:16:31,839 --> 00:16:34,320
them to be so that's an example of one

506
00:16:34,320 --> 00:16:37,199
of the 78 risks that we've identified

507
00:16:37,199 --> 00:16:39,440
another example uh risk is

508
00:16:39,440 --> 00:16:40,959
trustworthiness data

509
00:16:40,959 --> 00:16:42,880
sources aren't always trustworthy or

510
00:16:42,880 --> 00:16:44,560
suitable or reliable

511
00:16:44,560 --> 00:16:47,920
in fact gpt3 a very important natural

512
00:16:47,920 --> 00:16:50,000
language processing model put out by

513
00:16:50,000 --> 00:16:54,000
openai uses a whole bunch of crap from

514
00:16:54,000 --> 00:16:55,360
reddit to be trained on

515
00:16:55,360 --> 00:16:58,320
so you know does that make it suitable i

516
00:16:58,320 --> 00:16:59,920
don't know have you ever been on reddit

517
00:16:59,920 --> 00:17:02,240
reddit's got some stuff that you don't

518
00:17:02,240 --> 00:17:04,079
really want a machine to think about but

519
00:17:04,079 --> 00:17:04,799
the machine is

520
00:17:04,799 --> 00:17:06,799
being taught that that's how people

521
00:17:06,799 --> 00:17:08,400
think and so it should think that way

522
00:17:08,400 --> 00:17:09,679
too

523
00:17:09,679 --> 00:17:13,119
so one question to ask if you get over

524
00:17:13,119 --> 00:17:13,919
that is

525
00:17:13,919 --> 00:17:16,400
how might an attacker tamper with or

526
00:17:16,400 --> 00:17:17,039
otherwise

527
00:17:17,039 --> 00:17:20,319
poisoned raw input data what happens if

528
00:17:20,319 --> 00:17:23,280
input drifts or changes or disappears

529
00:17:23,280 --> 00:17:24,559
while we're training

530
00:17:24,559 --> 00:17:25,919
you know and it's out there in the world

531
00:17:25,919 --> 00:17:27,599
or it gets changed somebody screws

532
00:17:27,599 --> 00:17:28,960
around with it

533
00:17:28,960 --> 00:17:31,600
so those are two examples of risks

534
00:17:31,600 --> 00:17:33,520
associated with the raw data

535
00:17:33,520 --> 00:17:35,840
that we're going to use it's not even

536
00:17:35,840 --> 00:17:37,679
processed yet it's just out there in the

537
00:17:37,679 --> 00:17:38,400
world

538
00:17:38,400 --> 00:17:41,280
you can't trust everything you find and

539
00:17:41,280 --> 00:17:42,160
these large

540
00:17:42,160 --> 00:17:45,039
models don't necessarily throw out the

541
00:17:45,039 --> 00:17:46,000
stuff they should throw

542
00:17:46,000 --> 00:17:49,440
out so one of the

543
00:17:49,440 --> 00:17:52,960
next components is assembling that data

544
00:17:52,960 --> 00:17:55,200
into something useful raw data have to

545
00:17:55,200 --> 00:17:56,880
be transformed into

546
00:17:56,880 --> 00:17:58,720
a format that the machine learning

547
00:17:58,720 --> 00:18:00,240
system can use

548
00:18:00,240 --> 00:18:01,679
and that involves usually a lot of

549
00:18:01,679 --> 00:18:04,000
pre-processing and of course

550
00:18:04,000 --> 00:18:06,799
if you pre-process incorrectly or you

551
00:18:06,799 --> 00:18:08,640
screw things up you can really bias your

552
00:18:08,640 --> 00:18:09,120
data

553
00:18:09,120 --> 00:18:10,320
you can screw up your whole machine

554
00:18:10,320 --> 00:18:12,160
learning system that way so the

555
00:18:12,160 --> 00:18:14,880
pre-processing step is open to attack

556
00:18:14,880 --> 00:18:17,120
then there's kind of an issue here that

557
00:18:17,120 --> 00:18:19,120
is going to come up over and over again

558
00:18:19,120 --> 00:18:21,440
this that is online versus offline

559
00:18:21,440 --> 00:18:23,520
models in an offline model

560
00:18:23,520 --> 00:18:25,120
you train up your machine learning

561
00:18:25,120 --> 00:18:27,440
system on all the data

562
00:18:27,440 --> 00:18:29,919
and you stop training it so you decide

563
00:18:29,919 --> 00:18:30,880
you're done

564
00:18:30,880 --> 00:18:32,640
you sort of freeze the weights and then

565
00:18:32,640 --> 00:18:34,400
you use that model

566
00:18:34,400 --> 00:18:37,120
to process new information but you don't

567
00:18:37,120 --> 00:18:38,640
change the weights you don't do

568
00:18:38,640 --> 00:18:42,480
any more learning in an online situation

569
00:18:42,480 --> 00:18:44,640
you keep on learning even when the model

570
00:18:44,640 --> 00:18:46,240
is fielded out there in the world so

571
00:18:46,240 --> 00:18:47,600
it's being used

572
00:18:47,600 --> 00:18:50,559
but it's online and it can be changed um

573
00:18:50,559 --> 00:18:52,000
it's still learning

574
00:18:52,000 --> 00:18:57,120
so uh at the data set assembly

575
00:18:57,120 --> 00:18:59,520
level we have to think about online

576
00:18:59,520 --> 00:19:01,120
versus offline models

577
00:19:01,120 --> 00:19:02,799
so let me give you two examples of the

578
00:19:02,799 --> 00:19:04,720
eight risks we found at data

579
00:19:04,720 --> 00:19:09,120
assembly the first is that the encoding

580
00:19:09,120 --> 00:19:12,000
that you do integrity is really really

581
00:19:12,000 --> 00:19:13,440
important so you can

582
00:19:13,440 --> 00:19:15,919
introduce and exacerbate problems in

583
00:19:15,919 --> 00:19:17,760
encoding during pre-processing

584
00:19:17,760 --> 00:19:20,160
does the pre-processing stuff step

585
00:19:20,160 --> 00:19:22,480
itself introduce security problems

586
00:19:22,480 --> 00:19:24,000
and one of the things you have to think

587
00:19:24,000 --> 00:19:26,000
about is bias

588
00:19:26,000 --> 00:19:28,720
so if we have a bunch of raw data and we

589
00:19:28,720 --> 00:19:30,400
decide we're just going to divide it up

590
00:19:30,400 --> 00:19:32,400
into males and females as if there are

591
00:19:32,400 --> 00:19:33,200
no intersex

592
00:19:33,200 --> 00:19:35,679
people or you know white people and

593
00:19:35,679 --> 00:19:37,440
black people and those are just

594
00:19:37,440 --> 00:19:39,520
absolutes you know which is absolutely

595
00:19:39,520 --> 00:19:41,039
ridiculous that's not the way

596
00:19:41,039 --> 00:19:43,520
the real world is then you introduce

597
00:19:43,520 --> 00:19:44,480
bias by

598
00:19:44,480 --> 00:19:46,160
kind of doing that separation and

599
00:19:46,160 --> 00:19:48,160
categorization and if you've done that

600
00:19:48,160 --> 00:19:49,360
yourself

601
00:19:49,360 --> 00:19:51,120
your machine learning system which is

602
00:19:51,120 --> 00:19:53,200
just a bunch of statistics is going to

603
00:19:53,200 --> 00:19:53,679
learn

604
00:19:53,679 --> 00:19:55,840
all that stuff there are really

605
00:19:55,840 --> 00:19:57,679
important moral and ethical

606
00:19:57,679 --> 00:20:00,400
implications here and there's the whole

607
00:20:00,400 --> 00:20:01,360
subset of

608
00:20:01,360 --> 00:20:04,080
machine learning security stuff devoted

609
00:20:04,080 --> 00:20:05,440
to bias

610
00:20:05,440 --> 00:20:07,360
and it's even caused people to get fired

611
00:20:07,360 --> 00:20:09,520
from google and all sorts of controversy

612
00:20:09,520 --> 00:20:11,120
and the usual stuff

613
00:20:11,120 --> 00:20:14,159
another example of a risk here is the

614
00:20:14,159 --> 00:20:14,880
way that

615
00:20:14,880 --> 00:20:17,919
data are tagged and bagged or annotated

616
00:20:17,919 --> 00:20:21,919
into features can be directly attacked

617
00:20:21,919 --> 00:20:24,240
including you know introducing attacker

618
00:20:24,240 --> 00:20:25,840
bias into

619
00:20:25,840 --> 00:20:28,960
a system so a machine learning system

620
00:20:28,960 --> 00:20:30,559
that's trained up on examples that are

621
00:20:30,559 --> 00:20:32,080
way too specific won't be able to

622
00:20:32,080 --> 00:20:33,200
generalize

623
00:20:33,200 --> 00:20:36,480
for example or and you know it's often

624
00:20:36,480 --> 00:20:38,640
hard to make machine learning systems

625
00:20:38,640 --> 00:20:40,559
do what you want them to do they're very

626
00:20:40,559 --> 00:20:42,159
good at slipping out from your

627
00:20:42,159 --> 00:20:44,000
constraints

628
00:20:44,000 --> 00:20:47,280
i'll give you an example there was a

629
00:20:47,280 --> 00:20:48,000
well-known model

630
00:20:48,000 --> 00:20:49,440
that was supposed to be trained up to

631
00:20:49,440 --> 00:20:51,840
discriminate between wolves and dogs so

632
00:20:51,840 --> 00:20:53,200
it got trained up on a bunch of wolf

633
00:20:53,200 --> 00:20:55,039
things and a bunch of dog things mostly

634
00:20:55,039 --> 00:20:56,080
pictures

635
00:20:56,080 --> 00:20:58,080
and you give it a new picture and it

636
00:20:58,080 --> 00:20:59,760
would say whether it was a wolf or a dog

637
00:20:59,760 --> 00:21:00,960
in the new picture so it's supposed to

638
00:21:00,960 --> 00:21:02,080
be a a wolf

639
00:21:02,080 --> 00:21:04,720
dog discriminator and it worked really

640
00:21:04,720 --> 00:21:06,240
well in his training set it worked well

641
00:21:06,240 --> 00:21:06,640
on its

642
00:21:06,640 --> 00:21:09,360
on its test set but when they put it out

643
00:21:09,360 --> 00:21:09,840
there

644
00:21:09,840 --> 00:21:11,679
in the world it started failing

645
00:21:11,679 --> 00:21:13,520
spectacularly because it turned out

646
00:21:13,520 --> 00:21:15,919
that what they had built wasn't really a

647
00:21:15,919 --> 00:21:17,280
wolf detector

648
00:21:17,280 --> 00:21:19,520
as much as it was a snow detector

649
00:21:19,520 --> 00:21:21,679
because it turned out that every single

650
00:21:21,679 --> 00:21:23,360
picture of a wolf had a little bit of

651
00:21:23,360 --> 00:21:26,559
snow in it and very quickly the machine

652
00:21:26,559 --> 00:21:27,679
learning model

653
00:21:27,679 --> 00:21:29,600
learned to look for snows like if

654
00:21:29,600 --> 00:21:32,000
there's snow there it's a wolf

655
00:21:32,000 --> 00:21:33,760
and of course if you put dog on snow it

656
00:21:33,760 --> 00:21:35,200
would say it's the wolf and it would be

657
00:21:35,200 --> 00:21:35,679
really

658
00:21:35,679 --> 00:21:38,080
confident about that which goes to show

659
00:21:38,080 --> 00:21:39,520
you that you know

660
00:21:39,520 --> 00:21:41,280
these things are going to learn stuff

661
00:21:41,280 --> 00:21:43,280
you don't really necessarily want them

662
00:21:43,280 --> 00:21:44,880
to learn

663
00:21:44,880 --> 00:21:47,600
and a lot of the human engineering that

664
00:21:47,600 --> 00:21:48,400
goes into

665
00:21:48,400 --> 00:21:51,360
machine learning is spent cleaning and

666
00:21:51,360 --> 00:21:54,240
deleting and aggregating and organizing

667
00:21:54,240 --> 00:21:54,960
and throw out

668
00:21:54,960 --> 00:21:57,760
just all out manipulating the data so it

669
00:21:57,760 --> 00:22:00,400
can be consumed by an ml algorithm

670
00:22:00,400 --> 00:22:01,840
and that's kind of clue g but that's

671
00:22:01,840 --> 00:22:03,360
what really happens out there in the

672
00:22:03,360 --> 00:22:04,159
world

673
00:22:04,159 --> 00:22:06,320
so that's two examples of risks

674
00:22:06,320 --> 00:22:09,440
associated at this component level

675
00:22:09,440 --> 00:22:12,080
the next component we'll talk about is

676
00:22:12,080 --> 00:22:14,559
data sets it turns out that

677
00:22:14,559 --> 00:22:16,240
data are grouped into training and

678
00:22:16,240 --> 00:22:18,480
validation and test sets

679
00:22:18,480 --> 00:22:21,440
and figuring out how to partition your

680
00:22:21,440 --> 00:22:22,400
data

681
00:22:22,400 --> 00:22:25,520
into these sets is really important and

682
00:22:25,520 --> 00:22:26,080
it

683
00:22:26,080 --> 00:22:29,120
deeply impacts the future behavior of

684
00:22:29,120 --> 00:22:30,320
the machine learning system you're

685
00:22:30,320 --> 00:22:31,120
building

686
00:22:31,120 --> 00:22:33,120
so if we have a training set we got to

687
00:22:33,120 --> 00:22:34,799
make sure that the training set

688
00:22:34,799 --> 00:22:36,880
and the validation set have the same

689
00:22:36,880 --> 00:22:39,039
kind of categories and coverage in the

690
00:22:39,039 --> 00:22:39,760
world

691
00:22:39,760 --> 00:22:41,200
and we got to make sure that the test

692
00:22:41,200 --> 00:22:43,280
set ultimately before we decide to

693
00:22:43,280 --> 00:22:44,799
release something it has to pass its

694
00:22:44,799 --> 00:22:46,080
little test set

695
00:22:46,080 --> 00:22:48,320
actually lines up with the training set

696
00:22:48,320 --> 00:22:49,600
and the validation set

697
00:22:49,600 --> 00:22:51,679
and so we're testing the whole range of

698
00:22:51,679 --> 00:22:53,280
possibility space

699
00:22:53,280 --> 00:22:55,679
and all that stuff is hard figuring out

700
00:22:55,679 --> 00:22:58,480
how to build those sets

701
00:22:58,480 --> 00:23:00,480
it's pretty pretty much like well we

702
00:23:00,480 --> 00:23:02,320
sort of do it this way but there aren't

703
00:23:02,320 --> 00:23:03,840
any really

704
00:23:03,840 --> 00:23:06,960
formal ways to decide that partitioning

705
00:23:06,960 --> 00:23:09,120
so here are some examples of risks in

706
00:23:09,120 --> 00:23:11,120
this data set assembly number one

707
00:23:11,120 --> 00:23:14,000
is poisoning of all the first three

708
00:23:14,000 --> 00:23:15,600
components in our generic model that is

709
00:23:15,600 --> 00:23:17,280
raw data in the world

710
00:23:17,280 --> 00:23:19,440
and data set assembly and the data sets

711
00:23:19,440 --> 00:23:21,360
themselves they're all subject to

712
00:23:21,360 --> 00:23:23,120
poisoning attacks whereby

713
00:23:23,120 --> 00:23:25,919
an attacker intentionally manipulates

714
00:23:25,919 --> 00:23:26,720
data

715
00:23:26,720 --> 00:23:29,679
in any of the first three components

716
00:23:29,679 --> 00:23:31,679
possibly in a coordinated fashion

717
00:23:31,679 --> 00:23:34,000
in order to cause the ml training to go

718
00:23:34,000 --> 00:23:35,600
off the rails

719
00:23:35,600 --> 00:23:37,600
i'll give you an example of this there

720
00:23:37,600 --> 00:23:39,520
was a famous ml system called

721
00:23:39,520 --> 00:23:42,159
tay that microsoft put out it was

722
00:23:42,159 --> 00:23:43,279
supposed to be

723
00:23:43,279 --> 00:23:45,919
a little twitter bot so tay was supposed

724
00:23:45,919 --> 00:23:48,000
to go out in the world and interact on

725
00:23:48,000 --> 00:23:49,039
twitter and be all

726
00:23:49,039 --> 00:23:52,320
cute oh look the little ml is on twitter

727
00:23:52,320 --> 00:23:53,520
tweeting away

728
00:23:53,520 --> 00:23:56,320
and of course as soon as they threw tay

729
00:23:56,320 --> 00:23:57,760
into twitter it became

730
00:23:57,760 --> 00:24:01,240
a racist xenophobic uh

731
00:24:01,240 --> 00:24:05,440
misanthropic troll

732
00:24:05,440 --> 00:24:07,600
kind of a little machine learning

733
00:24:07,600 --> 00:24:08,960
 if you will

734
00:24:08,960 --> 00:24:11,840
and it was such a bad actor that

735
00:24:11,840 --> 00:24:13,919
microsoft had to just turn it off

736
00:24:13,919 --> 00:24:15,919
and the reason it became a bad actor is

737
00:24:15,919 --> 00:24:17,679
because it was getting all this data

738
00:24:17,679 --> 00:24:18,080
about

739
00:24:18,080 --> 00:24:20,559
behavior and interacting with users on

740
00:24:20,559 --> 00:24:21,760
twitter that were

741
00:24:21,760 --> 00:24:24,640
betrothing it and it learned to just be

742
00:24:24,640 --> 00:24:26,000
a troll

743
00:24:26,000 --> 00:24:27,520
and so that will tell you what can

744
00:24:27,520 --> 00:24:29,200
happen its data were poisoned

745
00:24:29,200 --> 00:24:30,720
intentionally by the people that were

746
00:24:30,720 --> 00:24:32,000
interacting with it

747
00:24:32,000 --> 00:24:34,320
and it was so bad it was so public that

748
00:24:34,320 --> 00:24:36,720
they had to just turn the thing off

749
00:24:36,720 --> 00:24:38,960
that's one example another example has

750
00:24:38,960 --> 00:24:40,640
to do with kind of uh

751
00:24:40,640 --> 00:24:44,400
data transfer so in many ml systems are

752
00:24:44,400 --> 00:24:45,840
constructed by

753
00:24:45,840 --> 00:24:47,919
tuning in already trained best base

754
00:24:47,919 --> 00:24:49,440
model so that

755
00:24:49,440 --> 00:24:51,360
you know you have some generic

756
00:24:51,360 --> 00:24:52,720
capability like say

757
00:24:52,720 --> 00:24:55,279
a circle detector and then you fine-tune

758
00:24:55,279 --> 00:24:57,279
that into an oval detector

759
00:24:57,279 --> 00:24:59,679
with a round of specialized training the

760
00:24:59,679 --> 00:25:00,960
reason you do this is because

761
00:25:00,960 --> 00:25:04,559
it's expensive to economically speak

762
00:25:04,559 --> 00:25:07,679
speaking to train up a model especially

763
00:25:07,679 --> 00:25:08,960
a big one

764
00:25:08,960 --> 00:25:13,039
for example the the gtp 3 model from

765
00:25:13,039 --> 00:25:14,000
open ai costs

766
00:25:14,000 --> 00:25:16,000
12 and a half million dollars to train

767
00:25:16,000 --> 00:25:17,919
um and and people don't just have 12 and

768
00:25:17,919 --> 00:25:19,520
a half million bucks laying around to

769
00:25:19,520 --> 00:25:21,919
to train a language model up um so

770
00:25:21,919 --> 00:25:23,679
they'll start with gpd3 and then teach

771
00:25:23,679 --> 00:25:25,279
it some subtasks and

772
00:25:25,279 --> 00:25:27,279
and hopefully get to specialize a little

773
00:25:27,279 --> 00:25:29,279
bit and at least that

774
00:25:29,279 --> 00:25:31,840
now when you do a transfer like that of

775
00:25:31,840 --> 00:25:32,400
course

776
00:25:32,400 --> 00:25:34,960
all of the risks that are in the first

777
00:25:34,960 --> 00:25:35,520
model

778
00:25:35,520 --> 00:25:37,279
are going to end up in the second model

779
00:25:37,279 --> 00:25:41,039
too so pre-trained model risk carryover

780
00:25:41,039 --> 00:25:42,559
not only that you can intentionally

781
00:25:42,559 --> 00:25:44,080
build a trojan

782
00:25:44,080 --> 00:25:46,880
into a machine learning model release it

783
00:25:46,880 --> 00:25:48,400
to the world and then people will use

784
00:25:48,400 --> 00:25:49,840
that to do their thing

785
00:25:49,840 --> 00:25:51,520
and then you can you know tickle the

786
00:25:51,520 --> 00:25:53,679
trojan and some people call that

787
00:25:53,679 --> 00:25:54,640
backdooring

788
00:25:54,640 --> 00:25:56,240
in the literature unfortunately it's not

789
00:25:56,240 --> 00:25:57,880
really backdooring it's really trojan

790
00:25:57,880 --> 00:25:59,760
functionality

791
00:25:59,760 --> 00:26:02,240
and so that's two examples of the seven

792
00:26:02,240 --> 00:26:03,440
risks we found

793
00:26:03,440 --> 00:26:06,799
at this dataset level

794
00:26:06,799 --> 00:26:08,400
number four is the learning algorithm

795
00:26:08,400 --> 00:26:10,080
itself of course

796
00:26:10,080 --> 00:26:11,919
this is the technical heart of machine

797
00:26:11,919 --> 00:26:14,320
learning but there's less security risk

798
00:26:14,320 --> 00:26:15,279
here than there is

799
00:26:15,279 --> 00:26:17,840
in the data that we've already described

800
00:26:17,840 --> 00:26:19,760
um once again though we have that online

801
00:26:19,760 --> 00:26:21,360
versus offline thing

802
00:26:21,360 --> 00:26:23,279
and it should be pretty obvious primo

803
00:26:23,279 --> 00:26:25,760
fascia but offline stuff is way easier

804
00:26:25,760 --> 00:26:26,640
to secure than

805
00:26:26,640 --> 00:26:29,039
online stuff because when it's online

806
00:26:29,039 --> 00:26:30,720
you get something like tay

807
00:26:30,720 --> 00:26:32,320
where people screw around with it and

808
00:26:32,320 --> 00:26:34,720
nudge it to to do something bad so an

809
00:26:34,720 --> 00:26:36,000
online system

810
00:26:36,000 --> 00:26:38,960
that continues to adjust its learning

811
00:26:38,960 --> 00:26:40,320
during operations may

812
00:26:40,320 --> 00:26:42,640
drift from its intended operational use

813
00:26:42,640 --> 00:26:44,080
case to something else

814
00:26:44,080 --> 00:26:46,960
and clever attackers can nudge the thing

815
00:26:46,960 --> 00:26:47,440
um

816
00:26:47,440 --> 00:26:50,000
in an online situation in the wrong

817
00:26:50,000 --> 00:26:51,279
direction on purpose

818
00:26:51,279 --> 00:26:54,559
and so that's a big risk right there uh

819
00:26:54,559 --> 00:26:57,440
example from the real world is um google

820
00:26:57,440 --> 00:26:59,440
translate which had this feedback loop

821
00:26:59,440 --> 00:27:01,120
where it was like feeding itself but it

822
00:27:01,120 --> 00:27:02,799
was using its own translations like oh

823
00:27:02,799 --> 00:27:04,400
that's a good example of a translation

824
00:27:04,400 --> 00:27:05,679
then it would

825
00:27:05,679 --> 00:27:07,279
use that to train itself and it really

826
00:27:07,279 --> 00:27:09,360
thought it was great

827
00:27:09,360 --> 00:27:12,400
uh as it went off the rails um that was

828
00:27:12,400 --> 00:27:13,919
unintentional of course but there's a

829
00:27:13,919 --> 00:27:15,600
reason now that that google won't

830
00:27:15,600 --> 00:27:18,080
translate its own translations

831
00:27:18,080 --> 00:27:21,120
number two risk is reproducibility and

832
00:27:21,120 --> 00:27:22,080
this is kind of like

833
00:27:22,080 --> 00:27:24,799
a meta-level risk but it's important

834
00:27:24,799 --> 00:27:26,960
machine learning work has a tendency to

835
00:27:26,960 --> 00:27:29,039
be pretty sloppily reported out there

836
00:27:29,039 --> 00:27:31,840
and results that can't be reproduced may

837
00:27:31,840 --> 00:27:33,600
lead to over confidence or under

838
00:27:33,600 --> 00:27:34,799
confidence in a

839
00:27:34,799 --> 00:27:37,120
in a particular ml system to perform as

840
00:27:37,120 --> 00:27:38,880
it's supposed to perform

841
00:27:38,880 --> 00:27:40,960
and if you can't reproduce results in

842
00:27:40,960 --> 00:27:42,320
science land

843
00:27:42,320 --> 00:27:44,480
then you got all sorts of trouble and

844
00:27:44,480 --> 00:27:45,520
you know if you read

845
00:27:45,520 --> 00:27:47,840
some of these papers that come out of

846
00:27:47,840 --> 00:27:48,960
machine learning you

847
00:27:48,960 --> 00:27:50,640
you find people saying something like

848
00:27:50,640 --> 00:27:52,720
well we set the learning threshold

849
00:27:52,720 --> 00:27:54,240
according to

850
00:27:54,240 --> 00:27:57,120
empirical testing and we set it to four

851
00:27:57,120 --> 00:27:58,159
you know and you're just like

852
00:27:58,159 --> 00:28:02,000
okay four why four nobody really knows

853
00:28:02,000 --> 00:28:04,399
why the the parameters get set the way

854
00:28:04,399 --> 00:28:06,159
they do they just sort of did

855
00:28:06,159 --> 00:28:08,559
and they just sort of kind of worked um

856
00:28:08,559 --> 00:28:09,200
and there's

857
00:28:09,200 --> 00:28:11,520
a fair amount of work on what's called

858
00:28:11,520 --> 00:28:13,200
explainable ai that joel and i were

859
00:28:13,200 --> 00:28:15,440
talking about before this talk started

860
00:28:15,440 --> 00:28:17,120
that's an important aspect of really

861
00:28:17,120 --> 00:28:18,960
understanding what you built

862
00:28:18,960 --> 00:28:21,039
and reproducibility plays a role there

863
00:28:21,039 --> 00:28:22,240
so this is two

864
00:28:22,240 --> 00:28:25,440
of the 11 risks we we kind of identified

865
00:28:25,440 --> 00:28:26,880
at the learning algorithm

866
00:28:26,880 --> 00:28:30,080
component the next component is um

867
00:28:30,080 --> 00:28:33,440
called uh evaluation and the question to

868
00:28:33,440 --> 00:28:34,320
ask yourself

869
00:28:34,320 --> 00:28:36,640
as a as a researcher in ml is well when

870
00:28:36,640 --> 00:28:38,320
am i done with the training

871
00:28:38,320 --> 00:28:40,320
how good is this trained model and

872
00:28:40,320 --> 00:28:41,919
that's what those data sets

873
00:28:41,919 --> 00:28:45,120
validation and test sets are for um so

874
00:28:45,120 --> 00:28:46,559
you know if you get your validation and

875
00:28:46,559 --> 00:28:48,720
test sets wrong you're not going to know

876
00:28:48,720 --> 00:28:49,840
when you're done or you might think

877
00:28:49,840 --> 00:28:51,600
you're done when you're not

878
00:28:51,600 --> 00:28:53,679
or you might think your model is better

879
00:28:53,679 --> 00:28:54,720
than it really is

880
00:28:54,720 --> 00:28:57,760
so like i told you before that idea of

881
00:28:57,760 --> 00:28:59,760
the validation and test set stuff is

882
00:28:59,760 --> 00:29:01,360
pretty darn critical

883
00:29:01,360 --> 00:29:03,200
um so here are two examples of the seven

884
00:29:03,200 --> 00:29:05,360
risks we've sort of tagged at the

885
00:29:05,360 --> 00:29:06,480
evaluation level

886
00:29:06,480 --> 00:29:08,559
the number one risk has been around

887
00:29:08,559 --> 00:29:09,679
forever and it's called

888
00:29:09,679 --> 00:29:13,520
overfitting so obviously a sufficiently

889
00:29:13,520 --> 00:29:14,399
powerful machine

890
00:29:14,399 --> 00:29:16,080
is capable of learning its training data

891
00:29:16,080 --> 00:29:17,679
set so well

892
00:29:17,679 --> 00:29:19,200
that it just essentially builds a big

893
00:29:19,200 --> 00:29:21,039
giant lookup table and so

894
00:29:21,039 --> 00:29:23,600
this can be likened to memorizing the

895
00:29:23,600 --> 00:29:24,960
training data

896
00:29:24,960 --> 00:29:26,720
and of course if you do that you're

897
00:29:26,720 --> 00:29:28,640
really good at the training data but you

898
00:29:28,640 --> 00:29:30,159
don't really generalize

899
00:29:30,159 --> 00:29:33,360
so if you don't generalize and you're

900
00:29:33,360 --> 00:29:35,440
really good at just parroting out the

901
00:29:35,440 --> 00:29:37,120
stuff you got trained on

902
00:29:37,120 --> 00:29:38,960
that's called overfitting and there's

903
00:29:38,960 --> 00:29:40,640
always been this kind of

904
00:29:40,640 --> 00:29:43,039
you know how much fitting is enough

905
00:29:43,039 --> 00:29:45,120
fitting like how much generalization do

906
00:29:45,120 --> 00:29:47,039
we want oh that's too sloppy and you

907
00:29:47,039 --> 00:29:48,960
want to adjust it to just be exactly

908
00:29:48,960 --> 00:29:49,600
right

909
00:29:49,600 --> 00:29:51,279
and nobody can tell you exactly how to

910
00:29:51,279 --> 00:29:52,840
do that adjustment you just have to do

911
00:29:52,840 --> 00:29:55,760
it so that's one example

912
00:29:55,760 --> 00:29:58,240
and another example is just plain old

913
00:29:58,240 --> 00:30:00,159
bad evaluation data

914
00:30:00,159 --> 00:30:02,640
a bad evaluation data set that doesn't

915
00:30:02,640 --> 00:30:03,679
reflect

916
00:30:03,679 --> 00:30:05,760
the data that the ml system will

917
00:30:05,760 --> 00:30:07,200
ultimately see

918
00:30:07,200 --> 00:30:09,919
in production can mislead a researcher

919
00:30:09,919 --> 00:30:10,559
into thinking

920
00:30:10,559 --> 00:30:12,159
everything's working and everything's

921
00:30:12,159 --> 00:30:13,840
fine even when it's not

922
00:30:13,840 --> 00:30:16,960
so evaluation sets can be too small or

923
00:30:16,960 --> 00:30:18,960
too similar to the training set

924
00:30:18,960 --> 00:30:22,240
or whatever to be useful and give the

925
00:30:22,240 --> 00:30:24,399
person training up the ml model the

926
00:30:24,399 --> 00:30:26,000
impression that their ml system is

927
00:30:26,000 --> 00:30:27,200
better than it really is and then they

928
00:30:27,200 --> 00:30:28,000
put it in

929
00:30:28,000 --> 00:30:29,520
place and then it doesn't do what it's

930
00:30:29,520 --> 00:30:31,120
supposed to do so

931
00:30:31,120 --> 00:30:33,520
that's two of the seven risks that we

932
00:30:33,520 --> 00:30:34,640
identified

933
00:30:34,640 --> 00:30:38,559
at evaluation mode moving on

934
00:30:38,559 --> 00:30:42,080
the sixth component is inputs themselves

935
00:30:42,080 --> 00:30:44,000
of course this is very similar to raw

936
00:30:44,000 --> 00:30:45,679
data in the world you can see there's a

937
00:30:45,679 --> 00:30:47,279
there's a direct line there

938
00:30:47,279 --> 00:30:49,360
so we use the raw data in the row we get

939
00:30:49,360 --> 00:30:50,880
it all set up and then we do the

940
00:30:50,880 --> 00:30:52,720
learning and now we have inputs that

941
00:30:52,720 --> 00:30:54,080
we're putting into the system

942
00:30:54,080 --> 00:30:55,760
they probably come from the same place

943
00:30:55,760 --> 00:30:57,440
out there in the world so

944
00:30:57,440 --> 00:31:00,159
what input is fed into the trade model

945
00:31:00,159 --> 00:31:01,679
during production where does that come

946
00:31:01,679 --> 00:31:03,600
from who can impact that

947
00:31:03,600 --> 00:31:05,679
very similar in nature to the data set

948
00:31:05,679 --> 00:31:07,279
assembly risks and raw data

949
00:31:07,279 --> 00:31:09,840
risks that i've already described for

950
00:31:09,840 --> 00:31:11,600
risks from

951
00:31:11,600 --> 00:31:14,159
but this is where we have our first

952
00:31:14,159 --> 00:31:15,600
piece of terminology that

953
00:31:15,600 --> 00:31:17,760
gets thrown around with impunity in

954
00:31:17,760 --> 00:31:18,640
machine learning

955
00:31:18,640 --> 00:31:20,960
security and that is adversarial

956
00:31:20,960 --> 00:31:22,000
examples

957
00:31:22,000 --> 00:31:24,159
so let's just think about this from a

958
00:31:24,159 --> 00:31:25,840
computer security perspective one of the

959
00:31:25,840 --> 00:31:28,000
most important categories of computer

960
00:31:28,000 --> 00:31:29,039
security risk

961
00:31:29,039 --> 00:31:32,080
is malicious input and

962
00:31:32,080 --> 00:31:33,919
the machine learning version of

963
00:31:33,919 --> 00:31:36,240
malicious input has come to be known

964
00:31:36,240 --> 00:31:38,320
as adversarial examples i think the

965
00:31:38,320 --> 00:31:40,799
terminology absolutely sucks by the way

966
00:31:40,799 --> 00:31:43,039
i would rather call it malicious input

967
00:31:43,039 --> 00:31:44,159
but uh you know

968
00:31:44,159 --> 00:31:46,240
as in all new fields everybody reinvents

969
00:31:46,240 --> 00:31:47,440
the wheel and they call it something

970
00:31:47,440 --> 00:31:48,960
else imagine if you called

971
00:31:48,960 --> 00:31:51,360
wheels i don't know flick bloggers and

972
00:31:51,360 --> 00:31:53,440
we'd say but but we've had wheels for

973
00:31:53,440 --> 00:31:54,640
years and yeah but we're calling them

974
00:31:54,640 --> 00:31:56,080
flight bloggers and that's just kind of

975
00:31:56,080 --> 00:31:57,679
the way it goes in security

976
00:31:57,679 --> 00:32:00,240
uh which is sad but you know we have to

977
00:32:00,240 --> 00:32:01,279
roll with the punches

978
00:32:01,279 --> 00:32:04,000
so adversarial inputs are really

979
00:32:04,000 --> 00:32:04,720
important

980
00:32:04,720 --> 00:32:07,200
um you can train up a machine learning

981
00:32:07,200 --> 00:32:09,679
model say a visual system on some stuff

982
00:32:09,679 --> 00:32:11,919
and i'll give you an example at

983
00:32:11,919 --> 00:32:13,120
university of michigan

984
00:32:13,120 --> 00:32:15,600
these people trained up a stop sign

985
00:32:15,600 --> 00:32:16,399
identifier

986
00:32:16,399 --> 00:32:18,159
supposed to look at a picture and see

987
00:32:18,159 --> 00:32:19,679
the stop sign and say oh yeah there's a

988
00:32:19,679 --> 00:32:20,640
stop sign

989
00:32:20,640 --> 00:32:22,240
and with the placement of just a little

990
00:32:22,240 --> 00:32:24,159
bit of tape on an actual stop sign you

991
00:32:24,159 --> 00:32:24,960
could get

992
00:32:24,960 --> 00:32:28,080
this sign recognition email to go oh

993
00:32:28,080 --> 00:32:29,679
that's not a stop sign that's a speed

994
00:32:29,679 --> 00:32:30,960
limit 45 sign

995
00:32:30,960 --> 00:32:33,919
which is pretty different from stop and

996
00:32:33,919 --> 00:32:35,840
so you can see with just a little bit of

997
00:32:35,840 --> 00:32:38,320
tweak to the input

998
00:32:38,320 --> 00:32:40,320
because the system's not working the

999
00:32:40,320 --> 00:32:42,320
same way our brain works

1000
00:32:42,320 --> 00:32:44,000
uh it's doing something completely

1001
00:32:44,000 --> 00:32:46,320
different but it's statistically based

1002
00:32:46,320 --> 00:32:48,480
uh you get these weird surprising things

1003
00:32:48,480 --> 00:32:49,360
where the little

1004
00:32:49,360 --> 00:32:51,360
littlest tiniest tweak of the input can

1005
00:32:51,360 --> 00:32:52,559
sometimes cause

1006
00:32:52,559 --> 00:32:55,279
catastrophic problems i'll give you some

1007
00:32:55,279 --> 00:32:56,720
real examples i'll show you some

1008
00:32:56,720 --> 00:32:59,039
pictures of adversarial examples later

1009
00:32:59,039 --> 00:33:00,799
one of the issues is of course that

1010
00:33:00,799 --> 00:33:02,559
adversarial examples because they're

1011
00:33:02,559 --> 00:33:03,919
they got high sex appeal

1012
00:33:03,919 --> 00:33:06,000
have have been around for a long time

1013
00:33:06,000 --> 00:33:08,559
and they sort of appeal to everybody

1014
00:33:08,559 --> 00:33:10,880
um to so there's a lot of talk about

1015
00:33:10,880 --> 00:33:12,559
that it's an important

1016
00:33:12,559 --> 00:33:14,559
kind of kind of risk but it's not the

1017
00:33:14,559 --> 00:33:15,760
only one

1018
00:33:15,760 --> 00:33:18,559
uh another risk here is that uh what we

1019
00:33:18,559 --> 00:33:20,720
call controlled input stream

1020
00:33:20,720 --> 00:33:23,919
a trained ml system that takes its input

1021
00:33:23,919 --> 00:33:24,720
directly from

1022
00:33:24,720 --> 00:33:27,120
outside may be purposefully manipulated

1023
00:33:27,120 --> 00:33:29,279
by an attacker

1024
00:33:29,279 --> 00:33:31,200
for example through sensor blinding you

1025
00:33:31,200 --> 00:33:32,880
know imagine you have a

1026
00:33:32,880 --> 00:33:35,279
self-driving car with lidar and so it's

1027
00:33:35,279 --> 00:33:36,799
using a slide artist c

1028
00:33:36,799 --> 00:33:38,880
and you just use a laser to blind the

1029
00:33:38,880 --> 00:33:41,600
lidar and also and it can't see

1030
00:33:41,600 --> 00:33:43,919
and that's controlling the input stream

1031
00:33:43,919 --> 00:33:44,799
so um

1032
00:33:44,799 --> 00:33:48,000
that's two of five risks we identify

1033
00:33:48,000 --> 00:33:51,120
at the input level then we get to the

1034
00:33:51,120 --> 00:33:53,039
model itself so these are risks that are

1035
00:33:53,039 --> 00:33:55,039
associated with a fielded model

1036
00:33:55,039 --> 00:33:57,200
very similar to evaluation risks in many

1037
00:33:57,200 --> 00:33:58,080
respects

1038
00:33:58,080 --> 00:34:00,159
um but i'll give you two examples of the

1039
00:34:00,159 --> 00:34:01,679
five risks we found here

1040
00:34:01,679 --> 00:34:04,720
the first is improper reuse so remember

1041
00:34:04,720 --> 00:34:05,279
when i said

1042
00:34:05,279 --> 00:34:07,120
transfer is when you train something up

1043
00:34:07,120 --> 00:34:08,639
in advance and then you

1044
00:34:08,639 --> 00:34:10,800
do some specific more specific

1045
00:34:10,800 --> 00:34:12,719
retraining to get the thing to do

1046
00:34:12,719 --> 00:34:15,678
something particular

1047
00:34:15,839 --> 00:34:20,159
if you reuse a system

1048
00:34:20,159 --> 00:34:23,599
and it has a bunch of risks in it then

1049
00:34:23,599 --> 00:34:25,839
you know the outside of the intended use

1050
00:34:25,839 --> 00:34:27,199
then those

1051
00:34:27,199 --> 00:34:29,199
it's going to not necessarily do what

1052
00:34:29,199 --> 00:34:31,040
you think that it does

1053
00:34:31,040 --> 00:34:32,399
we give you an example from the real

1054
00:34:32,399 --> 00:34:34,000
world probably everybody's used their

1055
00:34:34,000 --> 00:34:35,280
shoe as a hammer

1056
00:34:35,280 --> 00:34:37,040
shoes make really crappy hammers as you

1057
00:34:37,040 --> 00:34:38,159
probably found out when you tried to

1058
00:34:38,159 --> 00:34:40,719
like hit that nail it didn't really work

1059
00:34:40,719 --> 00:34:42,239
and shoes were not designed to be

1060
00:34:42,239 --> 00:34:44,159
hammers

1061
00:34:44,159 --> 00:34:46,320
in particular stilettos make very bad

1062
00:34:46,320 --> 00:34:47,359
hammers so

1063
00:34:47,359 --> 00:34:50,960
um you know the the the whole idea of

1064
00:34:50,960 --> 00:34:52,800
i'm going to use this model to do that

1065
00:34:52,800 --> 00:34:54,000
thing you got to ask

1066
00:34:54,000 --> 00:34:55,918
yourself well did they expect me to do

1067
00:34:55,918 --> 00:34:57,359
that thing what's going to happen when i

1068
00:34:57,359 --> 00:34:59,040
try to do that thing with this thing

1069
00:34:59,040 --> 00:35:01,520
and you know that's uh that's that's a

1070
00:35:01,520 --> 00:35:02,079
risk

1071
00:35:02,079 --> 00:35:04,800
so improper reuse another risk is i've

1072
00:35:04,800 --> 00:35:06,000
already alluded to and that's this

1073
00:35:06,000 --> 00:35:07,599
trojan functionality

1074
00:35:07,599 --> 00:35:09,839
model transfer leads to the possibility

1075
00:35:09,839 --> 00:35:12,079
that what's being reused might be trojan

1076
00:35:12,079 --> 00:35:14,880
or otherwise damaged um and so it might

1077
00:35:14,880 --> 00:35:16,960
be the wrong thing or might have some

1078
00:35:16,960 --> 00:35:19,680
some uh functionality in it that you

1079
00:35:19,680 --> 00:35:20,240
don't really

1080
00:35:20,240 --> 00:35:24,640
expect um so every time it sees a tank

1081
00:35:24,640 --> 00:35:27,119
it says cat and you go everything's fine

1082
00:35:27,119 --> 00:35:28,640
here just a bunch of cats

1083
00:35:28,640 --> 00:35:30,880
and that's not really what's happening

1084
00:35:30,880 --> 00:35:32,320
so that's a

1085
00:35:32,320 --> 00:35:35,280
those are two of the five risks that we

1086
00:35:35,280 --> 00:35:37,440
identified at this level

1087
00:35:37,440 --> 00:35:39,680
next comes in the inference algorithm

1088
00:35:39,680 --> 00:35:41,119
you know there are more risks associated

1089
00:35:41,119 --> 00:35:42,079
with a fielded

1090
00:35:42,079 --> 00:35:43,680
model so we've trained the thing up

1091
00:35:43,680 --> 00:35:45,760
we've tested it we've thought about the

1092
00:35:45,760 --> 00:35:47,760
inputs we're gonna it's gonna get and

1093
00:35:47,760 --> 00:35:50,000
we know the model we've frozen it and we

1094
00:35:50,000 --> 00:35:50,880
have the whole

1095
00:35:50,880 --> 00:35:53,839
algorithm and we field that thing and

1096
00:35:53,839 --> 00:35:55,520
obviously output risks

1097
00:35:55,520 --> 00:35:59,680
arise here so um a fielded model

1098
00:35:59,680 --> 00:36:02,000
that's operating in an online system as

1099
00:36:02,000 --> 00:36:03,359
we've described before that's still

1100
00:36:03,359 --> 00:36:04,480
learning can be pushed

1101
00:36:04,480 --> 00:36:06,160
past its boundaries as we've already

1102
00:36:06,160 --> 00:36:07,680
described

1103
00:36:07,680 --> 00:36:09,680
and then once again we've sort of talked

1104
00:36:09,680 --> 00:36:10,960
about touched on this but

1105
00:36:10,960 --> 00:36:13,280
institutability is an issue and way too

1106
00:36:13,280 --> 00:36:16,320
many systems an ml system is

1107
00:36:16,320 --> 00:36:18,320
fielded without any real understanding

1108
00:36:18,320 --> 00:36:20,079
of how it works or why it does what it

1109
00:36:20,079 --> 00:36:21,520
does it just sort of does it

1110
00:36:21,520 --> 00:36:23,359
it seems to work most the time and

1111
00:36:23,359 --> 00:36:25,040
people begin to count on it

1112
00:36:25,040 --> 00:36:27,359
without really knowing it's just magic

1113
00:36:27,359 --> 00:36:28,720
how it does what it does

1114
00:36:28,720 --> 00:36:31,040
and if you integrate an ml system that

1115
00:36:31,040 --> 00:36:33,359
just works into a larger system

1116
00:36:33,359 --> 00:36:36,160
and then you rely on it if the ml system

1117
00:36:36,160 --> 00:36:36,720
fails

1118
00:36:36,720 --> 00:36:39,280
the bigger system's going to fail and

1119
00:36:39,280 --> 00:36:40,480
it's not going to be very good at its

1120
00:36:40,480 --> 00:36:42,720
task or it might be very biased at its

1121
00:36:42,720 --> 00:36:43,440
task

1122
00:36:43,440 --> 00:36:47,200
for example you might train up

1123
00:36:47,200 --> 00:36:49,599
a machine learning system this is a real

1124
00:36:49,599 --> 00:36:50,400
problem

1125
00:36:50,400 --> 00:36:52,640
on uh whether or not prisoners are going

1126
00:36:52,640 --> 00:36:54,320
to be recidivists and

1127
00:36:54,320 --> 00:36:57,280
you know go out there and do more crime

1128
00:36:57,280 --> 00:36:57,599
uh

1129
00:36:57,599 --> 00:37:00,079
and it turns out that all you created

1130
00:37:00,079 --> 00:37:02,079
with your machine learning system was

1131
00:37:02,079 --> 00:37:04,400
a black guy detector and it says yeah

1132
00:37:04,400 --> 00:37:05,680
all the black people they're going to go

1133
00:37:05,680 --> 00:37:06,720
out and do more crime

1134
00:37:06,720 --> 00:37:09,280
then obviously you have a big problem um

1135
00:37:09,280 --> 00:37:10,720
and if you don't know how your model is

1136
00:37:10,720 --> 00:37:12,320
doing what it's doing you didn't

1137
00:37:12,320 --> 00:37:14,240
realize that it was just a black person

1138
00:37:14,240 --> 00:37:15,520
detector that you built

1139
00:37:15,520 --> 00:37:18,480
um then you're gonna end up causing some

1140
00:37:18,480 --> 00:37:20,079
terrible things to happen

1141
00:37:20,079 --> 00:37:22,079
out there in the world so those are two

1142
00:37:22,079 --> 00:37:24,000
cases uh two risks of

1143
00:37:24,000 --> 00:37:26,560
five that we identified at that level

1144
00:37:26,560 --> 00:37:28,000
then finally we get to

1145
00:37:28,000 --> 00:37:31,680
our uh last component

1146
00:37:31,680 --> 00:37:33,920
and that is where the system output is

1147
00:37:33,920 --> 00:37:35,440
you know

1148
00:37:35,440 --> 00:37:38,240
happening and often obviously you want

1149
00:37:38,240 --> 00:37:40,480
some output that's the whole point

1150
00:37:40,480 --> 00:37:42,320
um and there are direct attacks on

1151
00:37:42,320 --> 00:37:43,760
output that are pretty

1152
00:37:43,760 --> 00:37:46,880
pretty obvious like you just do

1153
00:37:46,880 --> 00:37:49,359
an attacker in the middle so an attacker

1154
00:37:49,359 --> 00:37:51,280
tweaks the output stream directly

1155
00:37:51,280 --> 00:37:52,880
and this impacts the larger system that

1156
00:37:52,880 --> 00:37:54,400
the ml is embedded in

1157
00:37:54,400 --> 00:37:55,839
there are lots of ways to do this but

1158
00:37:55,839 --> 00:37:57,920
the most thing most common would just be

1159
00:37:57,920 --> 00:37:59,200
to interpose

1160
00:37:59,200 --> 00:38:00,720
between the output stream and the

1161
00:38:00,720 --> 00:38:02,240
receiver and

1162
00:38:02,240 --> 00:38:04,160
what makes this worse than usual than

1163
00:38:04,160 --> 00:38:05,920
your usual say attacker in the middle

1164
00:38:05,920 --> 00:38:06,640
attack

1165
00:38:06,640 --> 00:38:08,720
is that nobody really knows how the ml

1166
00:38:08,720 --> 00:38:10,079
thing works so

1167
00:38:10,079 --> 00:38:12,240
if it starts doing weird stuff you just

1168
00:38:12,240 --> 00:38:13,920
go oh well it's just ml

1169
00:38:13,920 --> 00:38:16,400
it's just doing slightly weird stuff or

1170
00:38:16,400 --> 00:38:18,160
it's probably right even though it seems

1171
00:38:18,160 --> 00:38:19,680
wrong or whatever you give it sort of

1172
00:38:19,680 --> 00:38:21,359
the benefit of the doubt much more than

1173
00:38:21,359 --> 00:38:22,480
you might

1174
00:38:22,480 --> 00:38:24,240
otherwise makes it easier to fly under

1175
00:38:24,240 --> 00:38:26,480
the radar as an attacker

1176
00:38:26,480 --> 00:38:28,240
and then number two is you know

1177
00:38:28,240 --> 00:38:30,079
providence ml systems

1178
00:38:30,079 --> 00:38:32,079
have to be trustworthy to be put into

1179
00:38:32,079 --> 00:38:33,200
place um

1180
00:38:33,200 --> 00:38:36,160
and even a temporary or partial attack

1181
00:38:36,160 --> 00:38:37,280
against output

1182
00:38:37,280 --> 00:38:39,280
can cause trustworthiness to plummet and

1183
00:38:39,280 --> 00:38:40,720
people will just say i'm never

1184
00:38:40,720 --> 00:38:43,119
using ml again it did all the wrong

1185
00:38:43,119 --> 00:38:44,400
things and i hate it and i throw the

1186
00:38:44,400 --> 00:38:45,760
whole thing away

1187
00:38:45,760 --> 00:38:47,839
and that's a that's yet another risk so

1188
00:38:47,839 --> 00:38:49,040
there are five more of those in the

1189
00:38:49,040 --> 00:38:50,079
paper

1190
00:38:50,079 --> 00:38:53,599
so what i've covered so far is 18

1191
00:38:53,599 --> 00:38:56,400
of the 78 risks that we've identified

1192
00:38:56,400 --> 00:38:58,000
but there are some more risks that

1193
00:38:58,000 --> 00:39:00,240
don't really apply to the components we

1194
00:39:00,240 --> 00:39:02,079
just did that as an intellectual

1195
00:39:02,079 --> 00:39:03,680
exercise so that we could think through

1196
00:39:03,680 --> 00:39:04,800
each component

1197
00:39:04,800 --> 00:39:07,200
but we realize that there are some risks

1198
00:39:07,200 --> 00:39:07,839
that

1199
00:39:07,839 --> 00:39:10,480
are over the component view maybe

1200
00:39:10,480 --> 00:39:10,960
between

1201
00:39:10,960 --> 00:39:14,640
multiple components or across components

1202
00:39:14,640 --> 00:39:17,520
and so we identified 10 risks at this

1203
00:39:17,520 --> 00:39:18,240
level

1204
00:39:18,240 --> 00:39:20,640
and i'll give you two examples the first

1205
00:39:20,640 --> 00:39:22,960
is black box discrimination

1206
00:39:22,960 --> 00:39:25,359
as i've described many data related

1207
00:39:25,359 --> 00:39:26,640
component risks

1208
00:39:26,640 --> 00:39:29,839
lead to bias in the behavior of an ml

1209
00:39:29,839 --> 00:39:30,480
system

1210
00:39:30,480 --> 00:39:33,520
so ml systems that operate on personal

1211
00:39:33,520 --> 00:39:34,320
data

1212
00:39:34,320 --> 00:39:36,240
or feed into high impact decision

1213
00:39:36,240 --> 00:39:38,480
processes like credit scoring or

1214
00:39:38,480 --> 00:39:39,440
employment

1215
00:39:39,440 --> 00:39:42,640
or medical diagnosis or recidivism

1216
00:39:42,640 --> 00:39:45,920
pose a great deal of real risk so when

1217
00:39:45,920 --> 00:39:47,599
biases are aligned

1218
00:39:47,599 --> 00:39:51,440
with gender or race or age attributes

1219
00:39:51,440 --> 00:39:53,119
operating the system with the ml

1220
00:39:53,119 --> 00:39:54,880
embedded in it may result in

1221
00:39:54,880 --> 00:39:56,240
discrimination with

1222
00:39:56,240 --> 00:39:59,119
respect to these you know of protective

1223
00:39:59,119 --> 00:40:00,079
glasses

1224
00:40:00,079 --> 00:40:01,280
i don't know where that's coming from

1225
00:40:01,280 --> 00:40:02,620
somebody's

1226
00:40:02,620 --> 00:40:05,770
[Music]

1227
00:40:06,640 --> 00:40:08,079
i don't know where that's coming from

1228
00:40:08,079 --> 00:40:10,480
it's not coming from mike machine

1229
00:40:10,480 --> 00:40:13,440
so uh so this this is a bad thing we got

1230
00:40:13,440 --> 00:40:14,400
to think about um

1231
00:40:14,400 --> 00:40:16,000
that sort of discrimination and there

1232
00:40:16,000 --> 00:40:17,520
are a lot of people that are

1233
00:40:17,520 --> 00:40:21,119
worried about bias um in a.i and

1234
00:40:21,119 --> 00:40:22,640
rightfully so

1235
00:40:22,640 --> 00:40:24,480
and we wrote we really don't know how to

1236
00:40:24,480 --> 00:40:26,079
deal with that right now i mean one of

1237
00:40:26,079 --> 00:40:27,359
the issues is that

1238
00:40:27,359 --> 00:40:30,079
we often first turn to historical data

1239
00:40:30,079 --> 00:40:31,760
to train up a machine learning system

1240
00:40:31,760 --> 00:40:34,960
you say well i've had a bank since 1850

1241
00:40:34,960 --> 00:40:36,720
and you go back and you say well you

1242
00:40:36,720 --> 00:40:38,480
know women didn't even have rights back

1243
00:40:38,480 --> 00:40:39,359
then so

1244
00:40:39,359 --> 00:40:40,560
we're going to train up a machine

1245
00:40:40,560 --> 00:40:42,319
learning system to do the same stuff we

1246
00:40:42,319 --> 00:40:42,960
did

1247
00:40:42,960 --> 00:40:45,520
when women didn't have rights like what

1248
00:40:45,520 --> 00:40:46,400
you know so

1249
00:40:46,400 --> 00:40:48,720
it's it's uh it's important that you

1250
00:40:48,720 --> 00:40:49,520
think about

1251
00:40:49,520 --> 00:40:51,280
this sort of discrimination and bias

1252
00:40:51,280 --> 00:40:53,440
that we're building into these systems

1253
00:40:53,440 --> 00:40:56,800
another example is overconfidence so the

1254
00:40:56,800 --> 00:40:59,200
system the particular error behavior

1255
00:40:59,200 --> 00:41:00,720
because they all have that

1256
00:41:00,720 --> 00:41:03,040
is integrated into a larger system but

1257
00:41:03,040 --> 00:41:05,040
its output is treated as always high

1258
00:41:05,040 --> 00:41:06,800
confidence so sometimes the thing goes

1259
00:41:06,800 --> 00:41:10,720
uh dog maybe and we go oh it said dog so

1260
00:41:10,720 --> 00:41:12,400
it's definitely a dog

1261
00:41:12,400 --> 00:41:14,560
users of the system might become over

1262
00:41:14,560 --> 00:41:15,440
confident

1263
00:41:15,440 --> 00:41:17,760
in operating the whole system for its

1264
00:41:17,760 --> 00:41:19,200
intended purpose so

1265
00:41:19,200 --> 00:41:21,680
if you develop overconfidence in an ml

1266
00:41:21,680 --> 00:41:22,400
system

1267
00:41:22,400 --> 00:41:25,200
um that's bad and that's easy to do

1268
00:41:25,200 --> 00:41:26,400
because most people don't really

1269
00:41:26,400 --> 00:41:27,520
understand how they work and they're

1270
00:41:27,520 --> 00:41:28,880
only vaguely described and there's a

1271
00:41:28,880 --> 00:41:30,800
bunch of parameters and blah blah

1272
00:41:30,800 --> 00:41:33,920
so that's that's a risk and there are

1273
00:41:33,920 --> 00:41:35,359
eight more of those

1274
00:41:35,359 --> 00:41:38,560
um at the in the in the system uh in the

1275
00:41:38,560 --> 00:41:39,440
paper that we

1276
00:41:39,440 --> 00:41:42,079
that we uh described 78 risks just like

1277
00:41:42,079 --> 00:41:42,560
this

1278
00:41:42,560 --> 00:41:45,520
so i've told you about 20. um i've sort

1279
00:41:45,520 --> 00:41:47,200
of gone really quickly

1280
00:41:47,200 --> 00:41:49,040
through a whole number of risks and what

1281
00:41:49,040 --> 00:41:50,400
i wanted to do now

1282
00:41:50,400 --> 00:41:53,440
is just step back from those and say

1283
00:41:53,440 --> 00:41:56,560
look let's just talk about the top five

1284
00:41:56,560 --> 00:41:58,640
sure 78 that's great but let's talk

1285
00:41:58,640 --> 00:41:59,920
about the top five

1286
00:41:59,920 --> 00:42:02,240
so you have to take home message here um

1287
00:42:02,240 --> 00:42:03,920
and you know makes it easier to think

1288
00:42:03,920 --> 00:42:04,480
about

1289
00:42:04,480 --> 00:42:06,640
well there's obviously a danger of just

1290
00:42:06,640 --> 00:42:08,800
focusing on the top five and you know

1291
00:42:08,800 --> 00:42:10,560
the first one i'll give you a perfect

1292
00:42:10,560 --> 00:42:12,400
example of that because adversarial

1293
00:42:12,400 --> 00:42:14,079
examples are what

1294
00:42:14,079 --> 00:42:16,160
everybody talks about when they talk

1295
00:42:16,160 --> 00:42:18,000
about machine learning sys

1296
00:42:18,000 --> 00:42:20,240
systems and and security and this is

1297
00:42:20,240 --> 00:42:22,400
only one of 78 risks

1298
00:42:22,400 --> 00:42:24,400
so this is probably the most commonly

1299
00:42:24,400 --> 00:42:25,680
attacked one so

1300
00:42:25,680 --> 00:42:27,839
this is where we fool the machine

1301
00:42:27,839 --> 00:42:29,839
learning system by providing malicious

1302
00:42:29,839 --> 00:42:30,640
input

1303
00:42:30,640 --> 00:42:32,480
often involving pretty small

1304
00:42:32,480 --> 00:42:34,240
perturbations that cause the system to

1305
00:42:34,240 --> 00:42:35,680
screw up

1306
00:42:35,680 --> 00:42:38,400
and so here's some examples so on the

1307
00:42:38,400 --> 00:42:40,079
left we see a school bus

1308
00:42:40,079 --> 00:42:42,800
and a grouse and a mayan temple and on

1309
00:42:42,800 --> 00:42:44,640
the right we see a school bus and a

1310
00:42:44,640 --> 00:42:46,000
grouse at a mayan temple

1311
00:42:46,000 --> 00:42:47,680
and in the middle we see this little

1312
00:42:47,680 --> 00:42:49,280
overlay that we use

1313
00:42:49,280 --> 00:42:52,400
to change the pixels of the image

1314
00:42:52,400 --> 00:42:55,359
um some so the the image on the left is

1315
00:42:55,359 --> 00:42:57,040
the original picture in the image on the

1316
00:42:57,040 --> 00:42:57,440
right

1317
00:42:57,440 --> 00:43:00,240
is the pixel with the mask overlaid on

1318
00:43:00,240 --> 00:43:01,040
it

1319
00:43:01,040 --> 00:43:03,599
as a human you just go still bust still

1320
00:43:03,599 --> 00:43:05,680
a grouse still a mayan temple

1321
00:43:05,680 --> 00:43:07,839
but a machine learning system says

1322
00:43:07,839 --> 00:43:10,079
leopard leopard leopard

1323
00:43:10,079 --> 00:43:11,760
so this goes to show you that those

1324
00:43:11,760 --> 00:43:14,000
systems don't process

1325
00:43:14,000 --> 00:43:17,200
visual imagery the way that we do um

1326
00:43:17,200 --> 00:43:20,640
and that means they're sub subject to

1327
00:43:20,640 --> 00:43:24,319
some bizarre statistical uh attacks like

1328
00:43:24,319 --> 00:43:26,319
adversarial examples

1329
00:43:26,319 --> 00:43:28,560
so there's lots of coverage on this

1330
00:43:28,560 --> 00:43:30,640
resulting in lots of attention and it's

1331
00:43:30,640 --> 00:43:33,200
probably disproportionately large

1332
00:43:33,200 --> 00:43:34,720
swamping out all sorts of other risks

1333
00:43:34,720 --> 00:43:36,880
but they're very much real

1334
00:43:36,880 --> 00:43:38,319
and that's something to understand so

1335
00:43:38,319 --> 00:43:40,079
that's risk number one for us

1336
00:43:40,079 --> 00:43:42,800
of the top five number two is data

1337
00:43:42,800 --> 00:43:43,680
poisoning

1338
00:43:43,680 --> 00:43:46,000
i already told you that data play an

1339
00:43:46,000 --> 00:43:48,079
absolutely outsized role in securing an

1340
00:43:48,079 --> 00:43:49,599
ml system so if you can screw around the

1341
00:43:49,599 --> 00:43:50,400
data

1342
00:43:50,400 --> 00:43:52,240
intentionally you can really screw up an

1343
00:43:52,240 --> 00:43:54,560
ml system in a coordinated way and the

1344
00:43:54,560 --> 00:43:54,880
whole

1345
00:43:54,880 --> 00:43:56,640
the whole system that the ml is built

1346
00:43:56,640 --> 00:43:58,640
into can be compromised

1347
00:43:58,640 --> 00:44:01,520
so data poisoning tax require very

1348
00:44:01,520 --> 00:44:03,359
special attention you have to ask

1349
00:44:03,359 --> 00:44:05,119
what fraction of the training data could

1350
00:44:05,119 --> 00:44:07,520
an attacker control and to what extent

1351
00:44:07,520 --> 00:44:09,839
um we're actually working on some

1352
00:44:09,839 --> 00:44:11,440
training right now

1353
00:44:11,440 --> 00:44:13,839
that makes this clear with real code so

1354
00:44:13,839 --> 00:44:15,040
you can have a little system

1355
00:44:15,040 --> 00:44:16,400
and you can train it up and then you can

1356
00:44:16,400 --> 00:44:18,560
poison the thing you can see the poison

1357
00:44:18,560 --> 00:44:20,560
train it up again and you see that it

1358
00:44:20,560 --> 00:44:22,160
does the wrong thing

1359
00:44:22,160 --> 00:44:23,839
uh and then we do one that's larger

1360
00:44:23,839 --> 00:44:25,359
where you can't look at the data set and

1361
00:44:25,359 --> 00:44:26,400
see the poison

1362
00:44:26,400 --> 00:44:28,240
and that's really where we are so you

1363
00:44:28,240 --> 00:44:30,880
know i want to build a toy example

1364
00:44:30,880 --> 00:44:32,800
people can look at they're going to go i

1365
00:44:32,800 --> 00:44:34,079
see how this works

1366
00:44:34,079 --> 00:44:35,680
oh man how are we supposed to find the

1367
00:44:35,680 --> 00:44:37,920
poison in this big giant data set that

1368
00:44:37,920 --> 00:44:39,680
we just scraped off of reddit

1369
00:44:39,680 --> 00:44:41,920
or whatever um so data poisoning is a

1370
00:44:41,920 --> 00:44:43,119
real thing

1371
00:44:43,119 --> 00:44:44,960
number three online system manipulation

1372
00:44:44,960 --> 00:44:46,560
i've explained this twice so far so

1373
00:44:46,560 --> 00:44:47,920
third time's a charm

1374
00:44:47,920 --> 00:44:50,400
um the online system continues to learn

1375
00:44:50,400 --> 00:44:51,839
during operational use

1376
00:44:51,839 --> 00:44:53,920
and so a clever attacker can nudge it in

1377
00:44:53,920 --> 00:44:55,280
the wrong direction on purpose and

1378
00:44:55,280 --> 00:44:57,440
retrain the thing to do the wrong thing

1379
00:44:57,440 --> 00:45:00,480
just like microsoft tray pay now

1380
00:45:00,480 --> 00:45:02,240
the thing that makes this particularly

1381
00:45:02,240 --> 00:45:03,760
complicated and interesting

1382
00:45:03,760 --> 00:45:07,040
is that ml engineers have to

1383
00:45:07,040 --> 00:45:10,000
consider a bunch stuff at once where did

1384
00:45:10,000 --> 00:45:11,040
the data come from

1385
00:45:11,040 --> 00:45:14,000
what algorithm did i use and um what do

1386
00:45:14,000 --> 00:45:15,200
i have to do to keep an eye on this

1387
00:45:15,200 --> 00:45:16,800
thing while it's running and make sure

1388
00:45:16,800 --> 00:45:18,079
it doesn't drift

1389
00:45:18,079 --> 00:45:21,119
so we have three roles that are kind of

1390
00:45:21,119 --> 00:45:23,040
pretty different in usual security

1391
00:45:23,040 --> 00:45:23,760
between

1392
00:45:23,760 --> 00:45:26,960
say security risk analysis and security

1393
00:45:26,960 --> 00:45:28,800
operations they all have to be

1394
00:45:28,800 --> 00:45:29,760
considered

1395
00:45:29,760 --> 00:45:32,640
to get online manipulation system

1396
00:45:32,640 --> 00:45:33,680
manipulation control

1397
00:45:33,680 --> 00:45:37,200
under control the fourth risk in our top

1398
00:45:37,200 --> 00:45:39,200
five is transfer learning

1399
00:45:39,200 --> 00:45:41,359
i've explained that for economic reasons

1400
00:45:41,359 --> 00:45:44,000
many ml systems are built by taking an

1401
00:45:44,000 --> 00:45:45,280
old ml system and

1402
00:45:45,280 --> 00:45:47,040
you know fine-tuning it to carry out

1403
00:45:47,040 --> 00:45:48,800
something more specific

1404
00:45:48,800 --> 00:45:50,400
um this is especially true with the

1405
00:45:50,400 --> 00:45:54,800
gigantic language models like gtp3

1406
00:45:54,800 --> 00:45:57,520
and a data transfer attack takes place

1407
00:45:57,520 --> 00:45:59,200
when the base system is compromised or

1408
00:45:59,200 --> 00:46:00,960
otherwise unsuitable

1409
00:46:00,960 --> 00:46:03,680
and then you use it and you know it

1410
00:46:03,680 --> 00:46:04,079
makes

1411
00:46:04,079 --> 00:46:06,640
unanticipated behavior happen and if

1412
00:46:06,640 --> 00:46:08,319
you're a clever attacker you can

1413
00:46:08,319 --> 00:46:11,119
hide that behavior in the original

1414
00:46:11,119 --> 00:46:12,480
system and then it gets transferred

1415
00:46:12,480 --> 00:46:13,280
right over

1416
00:46:13,280 --> 00:46:14,560
now you guys are going to shake your

1417
00:46:14,560 --> 00:46:16,319
head as security people but

1418
00:46:16,319 --> 00:46:19,359
believe it or not most of these machine

1419
00:46:19,359 --> 00:46:21,280
learning models that people use

1420
00:46:21,280 --> 00:46:24,079
are just out there in the world they're

1421
00:46:24,079 --> 00:46:24,960
available

1422
00:46:24,960 --> 00:46:27,119
in open source and they're already

1423
00:46:27,119 --> 00:46:28,960
trained versions for free and you can

1424
00:46:28,960 --> 00:46:30,000
just get them

1425
00:46:30,000 --> 00:46:33,440
and use them and it's kind of like

1426
00:46:33,440 --> 00:46:35,359
picking up gum off the floor in the

1427
00:46:35,359 --> 00:46:37,599
men's bathroom don't do that that's bad

1428
00:46:37,599 --> 00:46:40,480
don't do that oh and you know they they

1429
00:46:40,480 --> 00:46:41,119
haven't even

1430
00:46:41,119 --> 00:46:43,280
hit on checksums yet so that's

1431
00:46:43,280 --> 00:46:44,960
operationally out there

1432
00:46:44,960 --> 00:46:46,720
there are a lot of people in machine

1433
00:46:46,720 --> 00:46:48,319
learning that don't think like

1434
00:46:48,319 --> 00:46:50,560
security people at all they just want to

1435
00:46:50,560 --> 00:46:52,400
get their stuff done

1436
00:46:52,400 --> 00:46:55,200
and have some fun number five is data

1437
00:46:55,200 --> 00:46:57,040
confidentiality we sort of started with

1438
00:46:57,040 --> 00:46:57,440
this

1439
00:46:57,440 --> 00:46:59,599
and i will emphasize this once again but

1440
00:46:59,599 --> 00:47:01,119
data protection

1441
00:47:01,119 --> 00:47:03,599
is really hard so imagine you have to

1442
00:47:03,599 --> 00:47:05,520
protect the data when it's sitting still

1443
00:47:05,520 --> 00:47:07,599
when it's moving around

1444
00:47:07,599 --> 00:47:11,040
you know at ri at rest in in motion when

1445
00:47:11,040 --> 00:47:12,800
it's being computed

1446
00:47:12,800 --> 00:47:14,079
we already think about that from a

1447
00:47:14,079 --> 00:47:16,000
security perspective and now we throw in

1448
00:47:16,000 --> 00:47:17,200
machine learning

1449
00:47:17,200 --> 00:47:19,040
so one unique challenge in ml is

1450
00:47:19,040 --> 00:47:20,800
protecting sensitive or confidential

1451
00:47:20,800 --> 00:47:21,599
data

1452
00:47:21,599 --> 00:47:23,839
that get crammed into the model through

1453
00:47:23,839 --> 00:47:25,200
training they're

1454
00:47:25,200 --> 00:47:28,160
put right in uh when you think about

1455
00:47:28,160 --> 00:47:29,920
things like gdpr

1456
00:47:29,920 --> 00:47:31,520
here's a question for the european

1457
00:47:31,520 --> 00:47:33,280
regulators when i train up something on

1458
00:47:33,280 --> 00:47:35,440
some gdpr protected data

1459
00:47:35,440 --> 00:47:37,599
is that thing protected now like is all

1460
00:47:37,599 --> 00:47:38,960
the ml stuff

1461
00:47:38,960 --> 00:47:42,240
like gdpr related i don't know

1462
00:47:42,240 --> 00:47:44,880
that's it's something for regulators to

1463
00:47:44,880 --> 00:47:45,680
fret about

1464
00:47:45,680 --> 00:47:49,839
for the next 25 years there are subtle

1465
00:47:49,839 --> 00:47:52,400
but very effective extraction attacks

1466
00:47:52,400 --> 00:47:54,720
against machine learning systems data

1467
00:47:54,720 --> 00:47:56,720
and that's a really important category

1468
00:47:56,720 --> 00:47:58,079
of risk

1469
00:47:58,079 --> 00:47:59,920
and it's a set of attacks that haven't

1470
00:47:59,920 --> 00:48:01,440
been fully explored yet

1471
00:48:01,440 --> 00:48:04,960
so those are the top five of the 78

1472
00:48:04,960 --> 00:48:06,960
risks that we identified

1473
00:48:06,960 --> 00:48:09,440
in our work at bimmel and i'm going to

1474
00:48:09,440 --> 00:48:10,880
give you a pointer to where to learn

1475
00:48:10,880 --> 00:48:11,839
more and then we'll have

1476
00:48:11,839 --> 00:48:15,040
a few questions so here is where to

1477
00:48:15,040 --> 00:48:16,960
learn some more stuff that's my website

1478
00:48:16,960 --> 00:48:18,079
right there

1479
00:48:18,079 --> 00:48:20,079
and you can get um all the stuff i

1480
00:48:20,079 --> 00:48:21,520
talked about today

1481
00:48:21,520 --> 00:48:23,640
from the bimmel website

1482
00:48:23,640 --> 00:48:25,200
berryvilleiml.com

1483
00:48:25,200 --> 00:48:27,680
and there's my email address and there's

1484
00:48:27,680 --> 00:48:29,359
my little twitter handle

1485
00:48:29,359 --> 00:48:32,319
in case you want to tweet at me and i

1486
00:48:32,319 --> 00:48:32,640
think

1487
00:48:32,640 --> 00:48:34,880
sirius said they'd buy everybody who's

1488
00:48:34,880 --> 00:48:36,800
at this thing a copy of each of my books

1489
00:48:36,800 --> 00:48:37,440
so just

1490
00:48:37,440 --> 00:48:40,720
ask uh joel and he'll give you that book

1491
00:48:40,720 --> 00:48:43,040
um for free right joel i think that was

1492
00:48:43,040 --> 00:48:44,400
what our deal was

1493
00:48:44,400 --> 00:48:46,800
so let's take some questions this is

1494
00:48:46,800 --> 00:48:48,240
this has been fun but it's been

1495
00:48:48,240 --> 00:48:49,920
an awful lot of talking to my computer

1496
00:48:49,920 --> 00:48:51,280
so i'm feeling a little strange about

1497
00:48:51,280 --> 00:48:53,200
that

1498
00:48:53,200 --> 00:48:56,559
so gary i'll start us off where are you

1499
00:48:56,559 --> 00:48:57,520
seeing

1500
00:48:57,520 --> 00:49:01,040
um whether in dot gov dot com

1501
00:49:01,040 --> 00:49:04,880
wherever people are getting it right

1502
00:49:04,880 --> 00:49:07,359
yeah the people that are thinking about

1503
00:49:07,359 --> 00:49:08,720
this the most

1504
00:49:08,720 --> 00:49:12,240
let's just say it that way

1505
00:49:12,240 --> 00:49:15,280
are at the big companies so the usual

1506
00:49:15,280 --> 00:49:16,000
suspects

1507
00:49:16,000 --> 00:49:20,000
google microsoft facebook

1508
00:49:20,000 --> 00:49:22,319
though they all have very active machine

1509
00:49:22,319 --> 00:49:24,160
learning security groups and we talked

1510
00:49:24,160 --> 00:49:24,640
to them

1511
00:49:24,640 --> 00:49:26,319
at bimmel a fair amount in fact we're

1512
00:49:26,319 --> 00:49:28,160
having a little closed session

1513
00:49:28,160 --> 00:49:30,880
at the end of the week this week about

1514
00:49:30,880 --> 00:49:32,240
that stuff and i'm going to give a talk

1515
00:49:32,240 --> 00:49:32,880
about our

1516
00:49:32,880 --> 00:49:36,720
results for those guys um and open ai

1517
00:49:36,720 --> 00:49:41,040
and amazon and you know

1518
00:49:41,040 --> 00:49:43,599
those places are the places that you see

1519
00:49:43,599 --> 00:49:45,040
people making the most

1520
00:49:45,040 --> 00:49:47,359
progress and thinking about this now

1521
00:49:47,359 --> 00:49:48,720
look there's a whole bunch of machine

1522
00:49:48,720 --> 00:49:50,319
learning going on at google

1523
00:49:50,319 --> 00:49:52,480
and the machine learning group is finite

1524
00:49:52,480 --> 00:49:54,960
so once again they're all like wake up

1525
00:49:54,960 --> 00:49:56,160
wow wait a minute

1526
00:49:56,160 --> 00:49:58,319
don't do that whatever so they're always

1527
00:49:58,319 --> 00:49:59,839
kind of playing catch up just like any

1528
00:49:59,839 --> 00:50:01,760
good security engineering team does in

1529
00:50:01,760 --> 00:50:03,839
any gigantic corporation

1530
00:50:03,839 --> 00:50:06,160
um so you know part of what i think

1531
00:50:06,160 --> 00:50:07,520
bimmel's role is

1532
00:50:07,520 --> 00:50:09,839
is to make sure that those kind of

1533
00:50:09,839 --> 00:50:12,240
security groups that are great um

1534
00:50:12,240 --> 00:50:15,599
don't just get used as ass covering

1535
00:50:15,599 --> 00:50:18,160
you know maneuvers by the giant

1536
00:50:18,160 --> 00:50:19,760
corporations and so that's kind of what

1537
00:50:19,760 --> 00:50:20,319
uh

1538
00:50:20,319 --> 00:50:23,440
i think our role at bible is about

1539
00:50:23,440 --> 00:50:25,119
that answer for the attendees if you've

1540
00:50:25,119 --> 00:50:26,640
got questions please

1541
00:50:26,640 --> 00:50:29,920
type them into the q a section

1542
00:50:29,920 --> 00:50:32,960
and we'll get them addressed

1543
00:50:32,960 --> 00:50:35,119
yeah let me stop sharing my screen real

1544
00:50:35,119 --> 00:50:36,079
quick

1545
00:50:36,079 --> 00:50:39,119
like i can see you yeah

1546
00:50:39,119 --> 00:50:42,480
feel free to ask and i i put in the chat

1547
00:50:42,480 --> 00:50:45,680
uh a copy of the url

1548
00:50:45,680 --> 00:50:48,960
to the paper that we put out um

1549
00:50:48,960 --> 00:50:55,119
so you can snag a copy if you want uh

1550
00:50:55,119 --> 00:50:56,800
ask you to register there's sort of a

1551
00:50:56,800 --> 00:50:58,480
registration wall but you can just lie

1552
00:50:58,480 --> 00:50:59,920
to the registration thing just tell it

1553
00:50:59,920 --> 00:51:00,800
your bill gates

1554
00:51:00,800 --> 00:51:03,839
like everybody else does

1555
00:51:05,680 --> 00:51:09,119
i'm not asking myself a question

1556
00:51:09,119 --> 00:51:11,920
don't be shy

1557
00:51:15,599 --> 00:51:18,079
ah hey i see tim grant's asked what

1558
00:51:18,079 --> 00:51:19,280
would you like nist

1559
00:51:19,280 --> 00:51:22,640
to do i think that nist

1560
00:51:22,640 --> 00:51:25,839
is been gathering a lot of information

1561
00:51:25,839 --> 00:51:29,680
about attack building the taxonomy

1562
00:51:29,680 --> 00:51:32,160
it seemed to me more like a big giant

1563
00:51:32,160 --> 00:51:33,359
collection

1564
00:51:33,359 --> 00:51:35,839
um and so i think what nist can do that

1565
00:51:35,839 --> 00:51:37,119
would be very helpful

1566
00:51:37,119 --> 00:51:39,040
is to make sure that we don't just fall

1567
00:51:39,040 --> 00:51:41,680
down into the attack of the day hole

1568
00:51:41,680 --> 00:51:44,160
and worry about only adversarial input

1569
00:51:44,160 --> 00:51:45,440
for example

1570
00:51:45,440 --> 00:51:47,680
but we get past attacks and we start

1571
00:51:47,680 --> 00:51:50,240
thinking about systematic risks

1572
00:51:50,240 --> 00:51:52,000
of the sorts that we're identifying at

1573
00:51:52,000 --> 00:51:53,920
at uh at bimol

1574
00:51:53,920 --> 00:51:57,119
um and that's a that's a hard thing um

1575
00:51:57,119 --> 00:51:57,599
to

1576
00:51:57,599 --> 00:51:59,359
to do like you know figuring out how to

1577
00:51:59,359 --> 00:52:00,720
do that i don't know how to

1578
00:52:00,720 --> 00:52:03,359
do that um so i'm glad you guys at mr

1579
00:52:03,359 --> 00:52:04,240
are

1580
00:52:04,240 --> 00:52:06,559
paying attention to this problem i don't

1581
00:52:06,559 --> 00:52:07,839
think we have the answer

1582
00:52:07,839 --> 00:52:10,240
where we all want to be directed yet but

1583
00:52:10,240 --> 00:52:11,200
i do think that

1584
00:52:11,200 --> 00:52:13,200
making sure that the big companies don't

1585
00:52:13,200 --> 00:52:15,280
just go oh well if we all just do this

1586
00:52:15,280 --> 00:52:16,880
really easy task for

1587
00:52:16,880 --> 00:52:18,319
then everything will be taken care of

1588
00:52:18,319 --> 00:52:20,480
and don't fall for that

1589
00:52:20,480 --> 00:52:22,319
i'm pretty sure you won't but watch out

1590
00:52:22,319 --> 00:52:23,599
for that

1591
00:52:23,599 --> 00:52:26,720
thanks tim glad you're here

1592
00:52:26,720 --> 00:52:28,960
who else is well nice to see your uh

1593
00:52:28,960 --> 00:52:30,559
your name come up in there tim

1594
00:52:30,559 --> 00:52:33,839
so gary one of the uh the the i read an

1595
00:52:33,839 --> 00:52:34,640
article

1596
00:52:34,640 --> 00:52:36,400
it was probably six months ago but it

1597
00:52:36,400 --> 00:52:38,400
was industry specific

1598
00:52:38,400 --> 00:52:40,640
uh i quite honestly i thought a bit of

1599
00:52:40,640 --> 00:52:41,839
it was naive but

1600
00:52:41,839 --> 00:52:44,559
but it did was a nice uh conversation

1601
00:52:44,559 --> 00:52:45,599
starter here

1602
00:52:45,599 --> 00:52:48,880
so uh ml applied to the automotive

1603
00:52:48,880 --> 00:52:49,520
industry

1604
00:52:49,520 --> 00:52:51,760
especially like some of these autonomous

1605
00:52:51,760 --> 00:52:53,200
vehicles i guess that'd be more than

1606
00:52:53,200 --> 00:52:54,240
just automotive but

1607
00:52:54,240 --> 00:52:56,240
any of these autonomous vehicles any

1608
00:52:56,240 --> 00:52:57,599
just general thoughts of where they're

1609
00:52:57,599 --> 00:52:59,200
going right or wrong

1610
00:52:59,200 --> 00:53:02,000
well i mean look these systems work

1611
00:53:02,000 --> 00:53:03,119
really pretty well

1612
00:53:03,119 --> 00:53:05,040
until they don't and then they fail and

1613
00:53:05,040 --> 00:53:06,960
they fail in surprising ways

1614
00:53:06,960 --> 00:53:09,200
so if the surprising ways involve

1615
00:53:09,200 --> 00:53:10,930
killing humans that's bad

1616
00:53:10,930 --> 00:53:12,160
[Music]

1617
00:53:12,160 --> 00:53:14,000
and if you look at what's happened

1618
00:53:14,000 --> 00:53:16,319
recently with the autopilot systems and

1619
00:53:16,319 --> 00:53:18,400
the self-driving cars and the crashes

1620
00:53:18,400 --> 00:53:20,800
and you know there's a whole slurry of

1621
00:53:20,800 --> 00:53:22,400
them just from last week

1622
00:53:22,400 --> 00:53:25,440
um that's bad so

1623
00:53:25,440 --> 00:53:27,520
uh i guess you gotta count on the humans

1624
00:53:27,520 --> 00:53:28,720
to do the wrong thing instead of the

1625
00:53:28,720 --> 00:53:29,599
right thing

1626
00:53:29,599 --> 00:53:32,160
so part of the design should be the

1627
00:53:32,160 --> 00:53:34,000
email system works great when the humans

1628
00:53:34,000 --> 00:53:36,079
are working great and paying attention

1629
00:53:36,079 --> 00:53:38,640
the ml system also needs to do something

1630
00:53:38,640 --> 00:53:39,280
smart

1631
00:53:39,280 --> 00:53:41,839
when the humans do the dumb thing like

1632
00:53:41,839 --> 00:53:44,319
if human tries to do a really dumb thing

1633
00:53:44,319 --> 00:53:46,960
uh with autopilot the autopilot just

1634
00:53:46,960 --> 00:53:48,319
could say no i'm not playing along with

1635
00:53:48,319 --> 00:53:49,440
that forget it i'm not

1636
00:53:49,440 --> 00:53:52,240
gonna drive this is a dumb time to drive

1637
00:53:52,240 --> 00:53:53,920
go park

1638
00:53:53,920 --> 00:53:58,079
or something like i'm the father of a uh

1639
00:53:58,079 --> 00:54:00,319
engineer who works at one of the the big

1640
00:54:00,319 --> 00:54:02,160
three automotive industry

1641
00:54:02,160 --> 00:54:03,920
uh automotive manufacturers and i know

1642
00:54:03,920 --> 00:54:06,000
that uh he's been somewhat involved with

1643
00:54:06,000 --> 00:54:06,880
that and

1644
00:54:06,880 --> 00:54:08,720
and and i can tell you that that even

1645
00:54:08,720 --> 00:54:10,000
from his perspective

1646
00:54:10,000 --> 00:54:11,520
they recognize that and they're not

1647
00:54:11,520 --> 00:54:13,440
taking it lightly so it's it's

1648
00:54:13,440 --> 00:54:15,040
oh i know certainly coming it's

1649
00:54:15,040 --> 00:54:16,559
certainly coming i think oh i know and

1650
00:54:16,559 --> 00:54:18,800
it's not like self-driving stuff is

1651
00:54:18,800 --> 00:54:20,800
is exactly new i mean i was in a

1652
00:54:20,800 --> 00:54:21,839
self-driving hummer

1653
00:54:21,839 --> 00:54:25,359
at cmu in 1993. really and it had to be

1654
00:54:25,359 --> 00:54:26,960
a hummer because it had to carry around

1655
00:54:26,960 --> 00:54:28,720
his generator to run all the spark

1656
00:54:28,720 --> 00:54:29,599
stations

1657
00:54:29,599 --> 00:54:32,240
to do the analysis but it was using deep

1658
00:54:32,240 --> 00:54:32,960
learning

1659
00:54:32,960 --> 00:54:34,480
to drive itself or right now then it did

1660
00:54:34,480 --> 00:54:36,880
a pretty good job so tim's asking

1661
00:54:36,880 --> 00:54:38,319
another question and since tim is the

1662
00:54:38,319 --> 00:54:40,240
only guy asking questions we'll answer

1663
00:54:40,240 --> 00:54:40,880
them

1664
00:54:40,880 --> 00:54:43,760
he says uh is a trustworthy ai framework

1665
00:54:43,760 --> 00:54:44,640
feasible

1666
00:54:44,640 --> 00:54:46,640
that can reconcile fairness and

1667
00:54:46,640 --> 00:54:49,440
explainability and trustworthyness

1668
00:54:49,440 --> 00:54:52,960
altogether um it's probably exactly the

1669
00:54:52,960 --> 00:54:54,799
same amount feasible as it is for

1670
00:54:54,799 --> 00:54:58,000
all software so you know the the answer

1671
00:54:58,000 --> 00:55:00,079
is your mileage may vary

1672
00:55:00,079 --> 00:55:02,240
but even having a framework at all where

1673
00:55:02,240 --> 00:55:03,200
you say look

1674
00:55:03,200 --> 00:55:05,119
the this sort of stuff is important and

1675
00:55:05,119 --> 00:55:06,559
if you don't think about this at all if

1676
00:55:06,559 --> 00:55:07,839
we come to you and we go

1677
00:55:07,839 --> 00:55:10,640
is your ml system bias and you go what's

1678
00:55:10,640 --> 00:55:11,599
that mean

1679
00:55:11,599 --> 00:55:13,440
we know that you're that you haven't

1680
00:55:13,440 --> 00:55:15,280
thought about this properly

1681
00:55:15,280 --> 00:55:18,480
that's even helpful so right now

1682
00:55:18,480 --> 00:55:21,200
in the kind of wild west of everybody's

1683
00:55:21,200 --> 00:55:23,680
adopting ml because it's the hot new

1684
00:55:23,680 --> 00:55:26,559
um we can really benefit from a

1685
00:55:26,559 --> 00:55:28,319
framework because of that

1686
00:55:28,319 --> 00:55:29,599
now we're getting a little a bunch of

1687
00:55:29,599 --> 00:55:32,160
stuff so shall i just do these in order

1688
00:55:32,160 --> 00:55:34,480
joel

1689
00:55:35,760 --> 00:55:37,839
most companies implement vendor products

1690
00:55:37,839 --> 00:55:38,799
that they claim

1691
00:55:38,799 --> 00:55:41,440
use magic ml fairy dust i love it that

1692
00:55:41,440 --> 00:55:43,119
sounds like the magic crypto fairy dust

1693
00:55:43,119 --> 00:55:44,960
that i've talked about for 20 years

1694
00:55:44,960 --> 00:55:47,119
how can these companies determine how

1695
00:55:47,119 --> 00:55:48,000
vulnerable these

1696
00:55:48,000 --> 00:55:49,839
products are that's a really good

1697
00:55:49,839 --> 00:55:51,599
question i don't think many people are

1698
00:55:51,599 --> 00:55:53,680
providing services around that stuff

1699
00:55:53,680 --> 00:55:55,680
i know of a few tiny little startups

1700
00:55:55,680 --> 00:55:56,880
that are doing kind of

1701
00:55:56,880 --> 00:55:59,040
machine learning security and risk

1702
00:55:59,040 --> 00:56:00,400
assessment type work

1703
00:56:00,400 --> 00:56:02,079
but they're all very tiny they're all

1704
00:56:02,079 --> 00:56:04,960
like three guys in the dog

1705
00:56:04,960 --> 00:56:07,599
so i'm not sure who i would turn to

1706
00:56:07,599 --> 00:56:08,720
commercially

1707
00:56:08,720 --> 00:56:11,839
um to determine how how full of baloney

1708
00:56:11,839 --> 00:56:12,240
those

1709
00:56:12,240 --> 00:56:15,040
those products are but one good general

1710
00:56:15,040 --> 00:56:16,640
rule of thumb is if it's a startup and

1711
00:56:16,640 --> 00:56:18,559
they're saying we use ml to do the magic

1712
00:56:18,559 --> 00:56:19,280
thing

1713
00:56:19,280 --> 00:56:21,040
you can just put on your

1714
00:56:21,040 --> 00:56:22,799
detector and go up must be

1715
00:56:22,799 --> 00:56:26,079
 so 90

1716
00:56:26,079 --> 00:56:29,520
probably not um an anonymous attendee i

1717
00:56:29,520 --> 00:56:30,000
like that

1718
00:56:30,000 --> 00:56:31,760
are you also looking into the security

1719
00:56:31,760 --> 00:56:33,760
of systems running the models

1720
00:56:33,760 --> 00:56:36,160
eg how to lock down the api interfaces

1721
00:56:36,160 --> 00:56:38,480
sanitize inputs rate limit and so on

1722
00:56:38,480 --> 00:56:40,480
that's excellent question the answer is

1723
00:56:40,480 --> 00:56:42,079
yes and one of the things we're trying

1724
00:56:42,079 --> 00:56:43,280
to do now

1725
00:56:43,280 --> 00:56:44,880
at bimmel is we're taking some

1726
00:56:44,880 --> 00:56:47,040
technology from run safe security

1727
00:56:47,040 --> 00:56:50,000
and we're using that to rewrite some

1728
00:56:50,000 --> 00:56:52,000
open source stuff from pytorch so that

1729
00:56:52,000 --> 00:56:53,520
we have a protected version of that

1730
00:56:53,520 --> 00:56:55,599
where memory attacks are way harder

1731
00:56:55,599 --> 00:56:59,359
to do and so there are a lot of things

1732
00:56:59,359 --> 00:57:01,599
that you can do like that at run time

1733
00:57:01,599 --> 00:57:03,760
um guidance on system deployment is

1734
00:57:03,760 --> 00:57:05,280
gonna you know matter

1735
00:57:05,280 --> 00:57:06,960
it sort of matters what it is you're

1736
00:57:06,960 --> 00:57:08,880
doing with ml but you shouldn't just

1737
00:57:08,880 --> 00:57:10,319
assume that the ml

1738
00:57:10,319 --> 00:57:12,720
is always gonna do the right thing and

1739
00:57:12,720 --> 00:57:13,839
so i think the

1740
00:57:13,839 --> 00:57:16,559
the main piece of advice is assume it's

1741
00:57:16,559 --> 00:57:18,240
going to screw up

1742
00:57:18,240 --> 00:57:20,160
and it'll screw up in an unanticipated

1743
00:57:20,160 --> 00:57:22,000
way so maybe give it some

1744
00:57:22,000 --> 00:57:23,839
some guide rails and when it gets

1745
00:57:23,839 --> 00:57:25,839
outside the guide rails have a plan what

1746
00:57:25,839 --> 00:57:26,720
are you going to do

1747
00:57:26,720 --> 00:57:29,200
are you just going to like crash the car

1748
00:57:29,200 --> 00:57:30,799
are you just going to like

1749
00:57:30,799 --> 00:57:32,960
you know not give anybody loans you're

1750
00:57:32,960 --> 00:57:34,319
going to give everybody loans how are

1751
00:57:34,319 --> 00:57:35,280
you going to fail

1752
00:57:35,280 --> 00:57:38,640
so so uh knowing that is kind of

1753
00:57:38,640 --> 00:57:40,319
what you have to do so that's the answer

1754
00:57:40,319 --> 00:57:42,400
to that one tim schmiel asks

1755
00:57:42,400 --> 00:57:44,160
given that machine learning is being

1756
00:57:44,160 --> 00:57:46,799
applied to analysis of network traffic

1757
00:57:46,799 --> 00:57:47,520
data

1758
00:57:47,520 --> 00:57:50,000
what big risk should we be looking for

1759
00:57:50,000 --> 00:57:52,000
and since i'm not a network

1760
00:57:52,000 --> 00:57:54,720
uh traffic data guy the answer is beats

1761
00:57:54,720 --> 00:57:55,280
me man

1762
00:57:55,280 --> 00:57:57,200
i don't know uh maybe we should all look

1763
00:57:57,200 --> 00:57:59,119
for the evil bit we can ask uh steve

1764
00:57:59,119 --> 00:57:59,920
belvin

1765
00:57:59,920 --> 00:58:02,240
if we can get the the fact on the evil

1766
00:58:02,240 --> 00:58:03,440
bit and then we can just

1767
00:58:03,440 --> 00:58:05,839
set up ml to look for the evil bit i

1768
00:58:05,839 --> 00:58:06,720
don't know

1769
00:58:06,720 --> 00:58:09,760
good question not a network guy uh

1770
00:58:09,760 --> 00:58:11,839
donnie witt asks what's the name of the

1771
00:58:11,839 --> 00:58:12,880
dog

1772
00:58:12,880 --> 00:58:15,040
the name of the dog who's over there

1773
00:58:15,040 --> 00:58:16,720
right now sleeping on the couch because

1774
00:58:16,720 --> 00:58:18,160
this is his like

1775
00:58:18,160 --> 00:58:20,720
maybe tenth version of this talk he's

1776
00:58:20,720 --> 00:58:22,400
had to suffer food through and it's kind

1777
00:58:22,400 --> 00:58:24,480
of hot because i have the ac off up here

1778
00:58:24,480 --> 00:58:26,559
so you guys can hear me um the name of

1779
00:58:26,559 --> 00:58:28,319
the dog is moonshine

1780
00:58:28,319 --> 00:58:30,960
and moonshine is only four months old

1781
00:58:30,960 --> 00:58:32,960
and you know so he's not even halfway

1782
00:58:32,960 --> 00:58:34,880
grown he's going to be a gigantic dog

1783
00:58:34,880 --> 00:58:36,240
but boy is he good

1784
00:58:36,240 --> 00:58:37,920
i raised about 10 puppies and he's the

1785
00:58:37,920 --> 00:58:40,160
best one so far

1786
00:58:40,160 --> 00:58:42,319
finally tim grants gets to ask the very

1787
00:58:42,319 --> 00:58:43,520
last question i think

1788
00:58:43,520 --> 00:58:46,559
because we're done with our time

1789
00:58:46,559 --> 00:58:48,240
and tim should get to ask three

1790
00:58:48,240 --> 00:58:49,520
questions because he's being very

1791
00:58:49,520 --> 00:58:50,000
proactive

1792
00:58:50,000 --> 00:58:53,280
so he says do we over emphasize models

1793
00:58:53,280 --> 00:58:56,400
and endless tweaks over good data

1794
00:58:56,400 --> 00:58:59,520
um if yes your dog is bored

1795
00:58:59,520 --> 00:59:02,000
i know the dog is bored you really would

1796
00:59:02,000 --> 00:59:03,440
rather have a treat

1797
00:59:03,440 --> 00:59:06,480
um yes i think we do over emphasize

1798
00:59:06,480 --> 00:59:09,200
models and endless streaks on models

1799
00:59:09,200 --> 00:59:10,400
over good data

1800
00:59:10,400 --> 00:59:12,319
but the problem is that if you look at

1801
00:59:12,319 --> 00:59:14,559
the data work say the stuff that was

1802
00:59:14,559 --> 00:59:16,319
it's produced by ibm they have this

1803
00:59:16,319 --> 00:59:17,599
whole

1804
00:59:17,599 --> 00:59:20,480
ml data cleansing set look for bias or

1805
00:59:20,480 --> 00:59:21,440
whatever

1806
00:59:21,440 --> 00:59:24,480
they provide like 150 little ways

1807
00:59:24,480 --> 00:59:26,480
to look for stuff in your data but they

1808
00:59:26,480 --> 00:59:28,640
don't give you any guidance about

1809
00:59:28,640 --> 00:59:31,760
how to apply those tools so

1810
00:59:31,760 --> 00:59:34,319
there's no magic solution to if you just

1811
00:59:34,319 --> 00:59:36,559
run your data through that sieve

1812
00:59:36,559 --> 00:59:38,160
everything will come out fine on the

1813
00:59:38,160 --> 00:59:40,960
other end we don't really know

1814
00:59:40,960 --> 00:59:42,240
there are some people that are working

1815
00:59:42,240 --> 00:59:44,319
hard on that problem

1816
00:59:44,319 --> 00:59:46,640
but we've not solved that one yet so

1817
00:59:46,640 --> 00:59:48,480
another good question from tim

1818
00:59:48,480 --> 00:59:51,440
tim it is super great to see nist

1819
00:59:51,440 --> 00:59:52,319
psyched about

1820
00:59:52,319 --> 00:59:55,359
ml so thank you so much for asking good

1821
00:59:55,359 --> 00:59:57,440
questions and paying attention here

1822
00:59:57,440 --> 00:59:58,559
now i'm going to turn it back over to

1823
00:59:58,559 --> 01:00:00,880
joel thanks to you all for having me

1824
01:00:00,880 --> 01:00:02,400
here i really appreciated the

1825
01:00:02,400 --> 01:00:03,680
opportunity to tell you what we're doing

1826
01:00:03,680 --> 01:00:04,400
at mimo

1827
01:00:04,400 --> 01:00:07,119
that was fun gary thank you thank you

1828
01:00:07,119 --> 01:00:08,960
very very much can't wait for your talk

1829
01:00:08,960 --> 01:00:11,119
from 2031.

1830
01:00:11,119 --> 01:00:14,079
um i'm not sure we have that topic yet

1831
01:00:14,079 --> 01:00:14,400
but

1832
01:00:14,400 --> 01:00:15,680
let me know when you're ready to

1833
01:00:15,680 --> 01:00:17,760
identify that topic thanks to all the

1834
01:00:17,760 --> 01:00:20,000
attendees who tuned in to the very first

1835
01:00:20,000 --> 01:00:22,559
serious security seminar of the 2021

1836
01:00:22,559 --> 01:00:23,359
summer

1837
01:00:23,359 --> 01:00:26,240
uh we will be back next week at 1 30

1838
01:00:26,240 --> 01:00:27,520
eastern

1839
01:00:27,520 --> 01:00:29,119
note there are a couple people asked

1840
01:00:29,119 --> 01:00:30,960
questions of where they could find a cop

1841
01:00:30,960 --> 01:00:33,920
a recording of this uh of this talk in a

1842
01:00:33,920 --> 01:00:35,599
few days time

1843
01:00:35,599 --> 01:00:38,240
that will appear at the sirius website

1844
01:00:38,240 --> 01:00:40,160
i've got an address in the chat box but

1845
01:00:40,160 --> 01:00:41,040
you can

1846
01:00:41,040 --> 01:00:43,359
also just do a uh internet search for

1847
01:00:43,359 --> 01:00:45,359
the serious website you'll find it there

1848
01:00:45,359 --> 01:00:47,440
and everything is also posted for the

1849
01:00:47,440 --> 01:00:48,400
last uh

1850
01:00:48,400 --> 01:00:50,640
few couple years few years at our

1851
01:00:50,640 --> 01:00:52,240
youtube channel by the way

1852
01:00:52,240 --> 01:00:56,079
right is my 2001 talk your 2001

1853
01:00:56,079 --> 01:00:59,040
is on the serious website uh no actually

1854
01:00:59,040 --> 01:01:00,079
that one is not

1855
01:01:00,079 --> 01:01:03,520
because it uh you know i'm checking to

1856
01:01:03,520 --> 01:01:04,000
see if we

1857
01:01:04,000 --> 01:01:05,760
have it on videotape and to get it

1858
01:01:05,760 --> 01:01:08,240
digitized so get it digitized i'd love

1859
01:01:08,240 --> 01:01:10,959
to see what it

1860
01:01:11,119 --> 01:01:13,280
but i would say in 10 years i hope that

1861
01:01:13,280 --> 01:01:14,640
your talk will be about

1862
01:01:14,640 --> 01:01:17,599
uh how to deal with our computer robot

1863
01:01:17,599 --> 01:01:18,720
overlords

1864
01:01:18,720 --> 01:01:20,879
so

1865
01:01:22,720 --> 01:01:25,280
yeah well the puppet meat puppet me will

1866
01:01:25,280 --> 01:01:28,000
be speaking

1867
01:01:28,720 --> 01:01:31,760
thanks tune in next week for our next

1868
01:01:31,760 --> 01:01:33,520
talk and throughout the summer

1869
01:01:33,520 --> 01:01:36,720
again 1 30 eastern time

1870
01:01:36,720 --> 01:01:38,799
and take a look online at some of our

1871
01:01:38,799 --> 01:01:40,079
past talks

1872
01:01:40,079 --> 01:01:42,880
uh as mike said gary's might actually

1873
01:01:42,880 --> 01:01:43,359
show

1874
01:01:43,359 --> 01:01:47,680
up there but um

1875
01:01:48,319 --> 01:01:50,079
but it reads like a who's who in the

1876
01:01:50,079 --> 01:01:51,680
cyber security industry and you take a

1877
01:01:51,680 --> 01:01:52,480
look at

1878
01:01:52,480 --> 01:01:54,640
20 plus years worth of talk so with that

1879
01:01:54,640 --> 01:01:55,760
we're done for the week

1880
01:01:55,760 --> 01:01:57,520
gary again thanks very much always great

1881
01:01:57,520 --> 01:01:58,960
to have you on campus

1882
01:01:58,960 --> 01:02:00,720
whether it's virtual or in person and

1883
01:02:00,720 --> 01:02:02,240
with yeah we need to do it in person

1884
01:02:02,240 --> 01:02:04,559
guys everybody get vaccinated just

1885
01:02:04,559 --> 01:02:07,440
do it and with that we are done for the

1886
01:02:07,440 --> 01:02:09,039
day thanks everybody

1887
01:02:09,039 --> 01:02:21,839
thanks gary take care

1888
01:02:27,440 --> 01:02:29,520
you

