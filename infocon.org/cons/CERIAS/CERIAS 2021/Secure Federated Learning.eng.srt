1
00:00:01,040 --> 00:00:02,800
good afternoon everybody thanks for

2
00:00:02,800 --> 00:00:04,080
joining us

3
00:00:04,080 --> 00:00:05,759
i just want to throw out a couple of

4
00:00:05,759 --> 00:00:07,440
housekeeping tips

5
00:00:07,440 --> 00:00:09,599
we usually reserve the final 15 minutes

6
00:00:09,599 --> 00:00:11,360
for q a

7
00:00:11,360 --> 00:00:13,440
you can use the q a button to submit

8
00:00:13,440 --> 00:00:15,360
your questions

9
00:00:15,360 --> 00:00:16,800
it's usually found at the bottom of your

10
00:00:16,800 --> 00:00:19,039
screen

11
00:00:19,039 --> 00:00:22,080
we also um if you'd like us to open your

12
00:00:22,080 --> 00:00:22,800
microphone

13
00:00:22,800 --> 00:00:25,680
just use the raise your hand button and

14
00:00:25,680 --> 00:00:26,160
i will

15
00:00:26,160 --> 00:00:29,599
unmute you thanks okay uh we have

16
00:00:29,599 --> 00:00:30,000
another

17
00:00:30,000 --> 00:00:32,320
great talk in store today's security

18
00:00:32,320 --> 00:00:34,719
seminar will be presented by

19
00:00:34,719 --> 00:00:37,440
professor neil gong neil is an assistant

20
00:00:37,440 --> 00:00:38,079
professor

21
00:00:38,079 --> 00:00:39,680
in the department of electrical and

22
00:00:39,680 --> 00:00:41,680
computer engineering and the department

23
00:00:41,680 --> 00:00:44,320
of computer science at duke

24
00:00:44,320 --> 00:00:46,719
the focus of his cyber security research

25
00:00:46,719 --> 00:00:49,039
is in privacy and machine learning

26
00:00:49,039 --> 00:00:51,440
his talk is titled secure federated

27
00:00:51,440 --> 00:00:52,640
learning

28
00:00:52,640 --> 00:00:55,840
neil welcome to serious summer security

29
00:00:55,840 --> 00:00:56,879
seminar

30
00:00:56,879 --> 00:01:00,320
you now have the zoom floor yes

31
00:01:00,320 --> 00:01:03,039
uh thanks for the introduction mike uh

32
00:01:03,039 --> 00:01:04,640
hello everyone

33
00:01:04,640 --> 00:01:06,799
uh i am leo gong i'm assistant perfect

34
00:01:06,799 --> 00:01:08,080
at duke

35
00:01:08,080 --> 00:01:10,080
uh today i will talk about our work on

36
00:01:10,080 --> 00:01:12,080
secure federation

37
00:01:12,080 --> 00:01:14,000
before getting started i'd like to thank

38
00:01:14,000 --> 00:01:15,920
the service center

39
00:01:15,920 --> 00:01:18,640
for giving me the opportunity to present

40
00:01:18,640 --> 00:01:21,200
this talk

41
00:01:21,759 --> 00:01:24,840
as you may know centralized learning is

42
00:01:24,840 --> 00:01:27,680
a popular conventional machine learning

43
00:01:27,680 --> 00:01:29,280
paradigm

44
00:01:29,280 --> 00:01:31,600
suppose we are giving some clients these

45
00:01:31,600 --> 00:01:33,840
clients could be smartphone

46
00:01:33,840 --> 00:01:38,000
iot devices or self-driving cars

47
00:01:38,000 --> 00:01:41,119
each client has some data locally

48
00:01:41,119 --> 00:01:44,560
in centralized learning these clients

49
00:01:44,560 --> 00:01:48,240
send their raw data to a cloud server

50
00:01:48,240 --> 00:01:51,040
which is managed by a service provider

51
00:01:51,040 --> 00:01:52,000
such as google

52
00:01:52,000 --> 00:01:56,000
apple and facebook the service provider

53
00:01:56,000 --> 00:01:56,479
then

54
00:01:56,479 --> 00:02:00,000
uses the client law data to train a

55
00:02:00,000 --> 00:02:01,520
machine learning model

56
00:02:01,520 --> 00:02:04,159
and the model is used to provide

57
00:02:04,159 --> 00:02:07,360
services for the client

58
00:02:07,360 --> 00:02:09,758
however search centralized learning

59
00:02:09,758 --> 00:02:12,160
faces multiple challenges

60
00:02:12,160 --> 00:02:15,760
for example the first challenge is that

61
00:02:15,760 --> 00:02:18,319
the server may be variable to data

62
00:02:18,319 --> 00:02:19,360
bridges

63
00:02:19,360 --> 00:02:22,400
due to various network and system

64
00:02:22,400 --> 00:02:24,720
variabilities

65
00:02:24,720 --> 00:02:28,000
it was reported that over the past 10

66
00:02:28,000 --> 00:02:28,640
years

67
00:02:28,640 --> 00:02:31,920
there have been 300 data breaches

68
00:02:31,920 --> 00:02:34,840
involving the save of 100 000 or more

69
00:02:34,840 --> 00:02:36,800
records

70
00:02:36,800 --> 00:02:39,840
these data breaches put clients data

71
00:02:39,840 --> 00:02:43,280
security and privacy at high risk

72
00:02:43,280 --> 00:02:45,920
the second challenge is that centralized

73
00:02:45,920 --> 00:02:46,720
learning

74
00:02:46,720 --> 00:02:50,000
incurs high communications costs which

75
00:02:50,000 --> 00:02:53,040
may be intolerable for resource consumed

76
00:02:53,040 --> 00:02:55,360
clients such as smartphone and iot

77
00:02:55,360 --> 00:02:57,840
devices

78
00:02:58,000 --> 00:03:00,959
federal healing is an emerging machine

79
00:03:00,959 --> 00:03:02,879
learning pilot line to address the

80
00:03:02,879 --> 00:03:05,519
challenges of centralism

81
00:03:05,519 --> 00:03:08,159
specifically in federal learning clients

82
00:03:08,159 --> 00:03:11,519
data stay locally on themselves

83
00:03:11,519 --> 00:03:14,239
each client trains a machine learning

84
00:03:14,239 --> 00:03:15,680
locally

85
00:03:15,680 --> 00:03:18,959
and the client sends their models

86
00:03:18,959 --> 00:03:22,640
or model updates instead of their load

87
00:03:22,640 --> 00:03:26,080
load data to the server fedor telling

88
00:03:26,080 --> 00:03:28,159
has been widely deployed by intellect

89
00:03:28,159 --> 00:03:30,879
giant for example

90
00:03:30,879 --> 00:03:33,840
google uses fedor tilani for the last

91
00:03:33,840 --> 00:03:35,440
word prediction

92
00:03:35,440 --> 00:03:37,760
on a virtual keyboard app called

93
00:03:37,760 --> 00:03:39,760
keyboard

94
00:03:39,760 --> 00:03:42,159
apple uses fedorky learning to

95
00:03:42,159 --> 00:03:44,799
personalize siri

96
00:03:44,799 --> 00:03:46,959
while there are already many studies on

97
00:03:46,959 --> 00:03:49,200
improving the accuracy

98
00:03:49,200 --> 00:03:51,760
and reducing the communication cost of

99
00:03:51,760 --> 00:03:53,360
federality learning

100
00:03:53,360 --> 00:03:55,599
its security aspect is much less

101
00:03:55,599 --> 00:03:57,599
explored

102
00:03:57,599 --> 00:03:59,439
so in this talk we will discuss the

103
00:03:59,439 --> 00:04:02,480
security aspects of federation learning

104
00:04:02,480 --> 00:04:03,920
in particular we will discuss the

105
00:04:03,920 --> 00:04:06,319
following two questions

106
00:04:06,319 --> 00:04:08,640
the first question is what are the

107
00:04:08,640 --> 00:04:10,080
security issues

108
00:04:10,080 --> 00:04:12,840
of this emerging federated learning

109
00:04:12,840 --> 00:04:14,159
parable

110
00:04:14,159 --> 00:04:16,000
after understanding these security

111
00:04:16,000 --> 00:04:18,320
issues we will then discuss a second

112
00:04:18,320 --> 00:04:20,000
question

113
00:04:20,000 --> 00:04:23,040
that is how to build secure federation

114
00:04:23,040 --> 00:04:26,560
to address the security issues

115
00:04:26,560 --> 00:04:29,360
this is a load map of this talk in part

116
00:04:29,360 --> 00:04:30,479
one we will discuss

117
00:04:30,479 --> 00:04:32,560
our local model poisoning attacks to

118
00:04:32,560 --> 00:04:34,160
federate learning

119
00:04:34,160 --> 00:04:37,040
in part two we will discuss how to build

120
00:04:37,040 --> 00:04:38,880
secure federal key logging

121
00:04:38,880 --> 00:04:42,000
via bootstrapping trust on the server

122
00:04:42,000 --> 00:04:44,000
and in part three we will discuss our

123
00:04:44,000 --> 00:04:45,120
work on

124
00:04:45,120 --> 00:04:48,960
building properly secure feather healer

125
00:04:48,960 --> 00:04:52,320
so first let's look at part one before

126
00:04:52,320 --> 00:04:54,240
introducing our text i'd like to

127
00:04:54,240 --> 00:04:56,080
describe a little bit more

128
00:04:56,080 --> 00:04:59,680
technical background about federation

129
00:04:59,680 --> 00:05:02,880
we are given some clients each client

130
00:05:02,880 --> 00:05:05,520
has some data locally

131
00:05:05,520 --> 00:05:09,120
we also have a cloud server the server

132
00:05:09,120 --> 00:05:11,680
maintains a machine only model called

133
00:05:11,680 --> 00:05:13,840
global model

134
00:05:13,840 --> 00:05:17,039
each client also maintains a machine

135
00:05:17,039 --> 00:05:18,479
only model

136
00:05:18,479 --> 00:05:21,600
called local model the in federal key

137
00:05:21,600 --> 00:05:23,360
learning is an iterative process

138
00:05:23,360 --> 00:05:26,160
and in each iteration feather healing

139
00:05:26,160 --> 00:05:27,440
performs the following

140
00:05:27,440 --> 00:05:31,199
three steps in step one

141
00:05:31,199 --> 00:05:33,520
the server sends the current global

142
00:05:33,520 --> 00:05:34,240
model

143
00:05:34,240 --> 00:05:38,479
to the client or a subset of them

144
00:05:38,479 --> 00:05:41,600
in step 2 the clients train

145
00:05:41,600 --> 00:05:45,360
their local models based on the current

146
00:05:45,360 --> 00:05:49,520
global model and their local data

147
00:05:49,520 --> 00:05:52,479
for example client i may initialize its

148
00:05:52,479 --> 00:05:53,840
local model wi

149
00:05:53,840 --> 00:05:56,960
as the current global model w

150
00:05:56,960 --> 00:05:59,919
and then client i uses stochastic

151
00:05:59,919 --> 00:06:01,199
gradient descent

152
00:06:01,199 --> 00:06:04,880
to update its local model wi here alpha

153
00:06:04,880 --> 00:06:06,639
is the learning rate

154
00:06:06,639 --> 00:06:10,000
gi is a gradient of client i's

155
00:06:10,000 --> 00:06:13,280
local loss with respect to its local

156
00:06:13,280 --> 00:06:15,759
model wr

157
00:06:15,759 --> 00:06:18,720
each client can repeat this stochastic

158
00:06:18,720 --> 00:06:19,759
gradient descent

159
00:06:19,759 --> 00:06:23,280
for multiple steps locally

160
00:06:23,280 --> 00:06:25,600
then the clients send their local models

161
00:06:25,600 --> 00:06:28,240
to the server

162
00:06:28,240 --> 00:06:31,360
in step 3 the server aggregates the

163
00:06:31,360 --> 00:06:32,960
client's local models

164
00:06:32,960 --> 00:06:36,639
as a new global model for example

165
00:06:36,639 --> 00:06:39,039
in a federal learning method called fed

166
00:06:39,039 --> 00:06:41,360
average developed by google

167
00:06:41,360 --> 00:06:45,120
the the global model w is basically the

168
00:06:45,120 --> 00:06:45,759
average

169
00:06:45,759 --> 00:06:49,360
of the client local models so federally

170
00:06:49,360 --> 00:06:50,080
learning

171
00:06:50,080 --> 00:06:53,680
basically repeats these three steps for

172
00:06:53,680 --> 00:06:56,479
many iterations and the filo global

173
00:06:56,479 --> 00:06:57,199
model

174
00:06:57,199 --> 00:07:00,840
is used to provide services for the

175
00:07:00,840 --> 00:07:02,240
client

176
00:07:02,240 --> 00:07:05,520
we note that it is equivalent to send

177
00:07:05,520 --> 00:07:08,880
locomotor updates wi manage w

178
00:07:08,880 --> 00:07:13,120
instead of the local models wi in step 2

179
00:07:13,120 --> 00:07:16,240
to the server which we will use in part

180
00:07:16,240 --> 00:07:19,039
two of this talk

181
00:07:19,360 --> 00:07:21,759
federal heleni is basically a

182
00:07:21,759 --> 00:07:23,520
distributed system

183
00:07:23,520 --> 00:07:26,240
and due to its distributed lecture it is

184
00:07:26,240 --> 00:07:28,160
fundamentally variable to polynomial

185
00:07:28,160 --> 00:07:29,840
text

186
00:07:29,840 --> 00:07:33,520
specifically an attacker may have access

187
00:07:33,520 --> 00:07:35,759
to some religious clouds

188
00:07:35,759 --> 00:07:38,160
these malicious clients could be fake

189
00:07:38,160 --> 00:07:39,199
clients

190
00:07:39,199 --> 00:07:41,919
injected by an attacker into the

191
00:07:41,919 --> 00:07:44,240
federality learning system

192
00:07:44,240 --> 00:07:47,199
all genuine clients compromised bio

193
00:07:47,199 --> 00:07:49,280
attackers

194
00:07:49,280 --> 00:07:52,479
a malicious client can poison its local

195
00:07:52,479 --> 00:07:53,440
data

196
00:07:53,440 --> 00:07:57,440
which is known as data poisoning attack

197
00:07:57,440 --> 00:08:00,080
a malicious client can also directly

198
00:08:00,080 --> 00:08:02,000
poison its local model

199
00:08:02,000 --> 00:08:06,319
which we call locomotor coding attack

200
00:08:06,319 --> 00:08:09,039
eventually the global model is poisoned

201
00:08:09,039 --> 00:08:09,360
and

202
00:08:09,360 --> 00:08:11,840
the poisoned global model makes

203
00:08:11,840 --> 00:08:13,680
incorrect predictions

204
00:08:13,680 --> 00:08:16,800
as the attacker desires

205
00:08:16,800 --> 00:08:20,400
for example a single malicious client

206
00:08:20,400 --> 00:08:24,639
can arbitrarily change the global model

207
00:08:24,639 --> 00:08:26,800
learned by fed average which we just

208
00:08:26,800 --> 00:08:30,319
described in the previous slide

209
00:08:30,800 --> 00:08:33,519
to defend against these positive attacks

210
00:08:33,519 --> 00:08:34,958
the machine learning community has

211
00:08:34,958 --> 00:08:37,039
actually developed a multiple

212
00:08:37,039 --> 00:08:39,039
byzantine robust federated learning

213
00:08:39,039 --> 00:08:41,039
methods

214
00:08:41,039 --> 00:08:43,599
recall that federal learning has three

215
00:08:43,599 --> 00:08:46,080
steps in each iteration

216
00:08:46,080 --> 00:08:48,640
these byzantine robust methods

217
00:08:48,640 --> 00:08:49,519
essentially

218
00:08:49,519 --> 00:08:53,200
use violent and robust aggregation use

219
00:08:53,200 --> 00:08:56,560
in step 3 instead of the average

220
00:08:56,560 --> 00:08:57,760
aggregation rule

221
00:08:57,760 --> 00:09:01,120
used by fed average

222
00:09:01,200 --> 00:09:04,000
roughly speaking the key idea of this

223
00:09:04,000 --> 00:09:05,600
byzantine robust

224
00:09:05,600 --> 00:09:10,000
method is to remove outlier local models

225
00:09:10,000 --> 00:09:12,720
before aggregating them as a global

226
00:09:12,720 --> 00:09:14,640
model

227
00:09:14,640 --> 00:09:18,800
under some assumptions for example

228
00:09:18,800 --> 00:09:21,440
the client's local data iid and the loss

229
00:09:21,440 --> 00:09:23,440
function is smooth and so on

230
00:09:23,440 --> 00:09:26,480
these byzantine robust methods can

231
00:09:26,480 --> 00:09:28,800
even bound to the change of the global

232
00:09:28,800 --> 00:09:30,399
model parameters

233
00:09:30,399 --> 00:09:33,920
caused by medicines clients

234
00:09:34,080 --> 00:09:37,519
next i will use a medium as an example

235
00:09:37,519 --> 00:09:39,839
to illustrate how byzantine robust

236
00:09:39,839 --> 00:09:42,560
aggregation rule works

237
00:09:42,560 --> 00:09:45,760
suppose each local or global model has n

238
00:09:45,760 --> 00:09:47,519
parameters

239
00:09:47,519 --> 00:09:50,240
and these are the n parameters of client

240
00:09:50,240 --> 00:09:52,160
ones local model

241
00:09:52,160 --> 00:09:55,279
in some iteration and these are the m

242
00:09:55,279 --> 00:09:56,399
parameters

243
00:09:56,399 --> 00:09:58,880
of client two local model in some

244
00:09:58,880 --> 00:10:00,399
iteration

245
00:10:00,399 --> 00:10:02,959
and these are the m parameters of client

246
00:10:02,959 --> 00:10:05,680
and local model

247
00:10:05,680 --> 00:10:07,680
and these n-local models are sent to the

248
00:10:07,680 --> 00:10:10,560
server and the server aggregates them

249
00:10:10,560 --> 00:10:13,920
to obtain a global model

250
00:10:13,920 --> 00:10:17,360
specifically the server calculates the

251
00:10:17,360 --> 00:10:19,120
median

252
00:10:19,120 --> 00:10:22,160
of the n first

253
00:10:22,160 --> 00:10:26,320
parameters of the n local models

254
00:10:26,320 --> 00:10:28,399
as the first parameter of the global

255
00:10:28,399 --> 00:10:30,240
model

256
00:10:30,240 --> 00:10:33,200
the server calculates the median of the

257
00:10:33,200 --> 00:10:34,800
second parameters

258
00:10:34,800 --> 00:10:38,079
of the n local models as the second

259
00:10:38,079 --> 00:10:40,560
parameter of the global model

260
00:10:40,560 --> 00:10:43,360
and the server repeats this operation to

261
00:10:43,360 --> 00:10:45,680
calculate the m parameters of the global

262
00:10:45,680 --> 00:10:47,839
model

263
00:10:48,399 --> 00:10:51,200
our work basically shows that these

264
00:10:51,200 --> 00:10:52,640
byzantine robust

265
00:10:52,640 --> 00:10:54,959
federation learning methods are still

266
00:10:54,959 --> 00:10:55,839
variable

267
00:10:55,839 --> 00:10:58,959
to local model poisoning attacks in

268
00:10:58,959 --> 00:11:02,000
particular an attacker can still

269
00:11:02,000 --> 00:11:04,240
substantially increase the testing error

270
00:11:04,240 --> 00:11:05,360
rate of

271
00:11:05,360 --> 00:11:08,000
the global model learned by a byzantine

272
00:11:08,000 --> 00:11:10,480
robust method

273
00:11:10,480 --> 00:11:13,200
recall that this byzantine robust method

274
00:11:13,200 --> 00:11:14,160
can have

275
00:11:14,160 --> 00:11:17,360
nice theoretical guarantees based on

276
00:11:17,360 --> 00:11:18,480
some assumptions

277
00:11:18,480 --> 00:11:20,720
however they are still vulnerable to our

278
00:11:20,720 --> 00:11:22,640
attacks because

279
00:11:22,640 --> 00:11:26,000
those assumptions often do not hold

280
00:11:26,000 --> 00:11:29,120
and even if those assumptions do hold

281
00:11:29,120 --> 00:11:32,160
they cannot bound to the change of the

282
00:11:32,160 --> 00:11:33,279
testing error rate

283
00:11:33,279 --> 00:11:35,760
caused by our attacks because they can

284
00:11:35,760 --> 00:11:37,760
only bound to the change of the global

285
00:11:37,760 --> 00:11:40,640
model parameters

286
00:11:40,800 --> 00:11:42,959
we consider the following threat model

287
00:11:42,959 --> 00:11:45,680
in our attacks

288
00:11:45,760 --> 00:11:48,079
an attacker's goal is to increase the

289
00:11:48,079 --> 00:11:49,200
testing error rate

290
00:11:49,200 --> 00:11:52,320
of the global model an attacker has

291
00:11:52,320 --> 00:11:55,040
access to some religious clients

292
00:11:55,040 --> 00:11:57,760
which could be fixed client injected by

293
00:11:57,760 --> 00:11:58,959
an attacker

294
00:11:58,959 --> 00:12:01,440
or genuine client compromised by the

295
00:12:01,440 --> 00:12:03,279
attacker

296
00:12:03,279 --> 00:12:05,279
these medicines clients can send

297
00:12:05,279 --> 00:12:08,720
arbitrary local models to the server

298
00:12:08,720 --> 00:12:10,800
in terms of the attacker's background

299
00:12:10,800 --> 00:12:12,800
knowledge we consider both

300
00:12:12,800 --> 00:12:16,399
full and partial knowledge scenarios

301
00:12:16,399 --> 00:12:19,680
in the full launch scenario an attacker

302
00:12:19,680 --> 00:12:21,279
has access to the

303
00:12:21,279 --> 00:12:24,560
local data and local models on all

304
00:12:24,560 --> 00:12:26,240
clients

305
00:12:26,240 --> 00:12:28,959
in a partial logic scenario the attacker

306
00:12:28,959 --> 00:12:31,600
only has access to the local data

307
00:12:31,600 --> 00:12:33,760
and the local models on the malicious

308
00:12:33,760 --> 00:12:35,839
client

309
00:12:35,839 --> 00:12:37,760
as you can imagine the full launch

310
00:12:37,760 --> 00:12:38,959
scenario may be

311
00:12:38,959 --> 00:12:42,480
unrealistic in practice so we use

312
00:12:42,480 --> 00:12:45,839
this full knowledge scenario to estimate

313
00:12:45,839 --> 00:12:50,639
the upper bound of our attacks threat

314
00:12:50,720 --> 00:12:52,560
the attacker may or may not know the

315
00:12:52,560 --> 00:12:56,638
aggregation rule used by the server

316
00:12:56,800 --> 00:12:59,839
next i will introduce the key idea of

317
00:12:59,839 --> 00:13:02,240
our attack

318
00:13:02,240 --> 00:13:06,000
suppose w previous is a global model

319
00:13:06,000 --> 00:13:09,360
in the previous iteration and for

320
00:13:09,360 --> 00:13:11,519
simplicity let's assume we only have two

321
00:13:11,519 --> 00:13:16,240
clients w1 is a local model

322
00:13:16,240 --> 00:13:20,639
on client one in the current iteration

323
00:13:20,639 --> 00:13:24,399
w2 is a local model on client 2

324
00:13:24,399 --> 00:13:27,680
in the current iteration these two

325
00:13:27,680 --> 00:13:30,079
local models are sent to the server and

326
00:13:30,079 --> 00:13:30,880
the server

327
00:13:30,880 --> 00:13:33,680
aggregates them to obtain a new global

328
00:13:33,680 --> 00:13:34,000
model

329
00:13:34,000 --> 00:13:37,760
w in the current iteration

330
00:13:37,760 --> 00:13:40,959
so when there are no attacks the global

331
00:13:40,959 --> 00:13:41,760
model

332
00:13:41,760 --> 00:13:46,000
changes along some direction like this

333
00:13:46,000 --> 00:13:50,079
in two consecutive iterations

334
00:13:50,079 --> 00:13:53,519
our attack idea is to deviate the global

335
00:13:53,519 --> 00:13:54,560
model

336
00:13:54,560 --> 00:13:57,920
the most along the inverse

337
00:13:57,920 --> 00:14:01,360
of this direction for instance in this

338
00:14:01,360 --> 00:14:02,320
example

339
00:14:02,320 --> 00:14:05,279
we aim to deviate the global model w to

340
00:14:05,279 --> 00:14:05,920
be w

341
00:14:05,920 --> 00:14:09,519
prime let's assume

342
00:14:09,519 --> 00:14:12,720
client 1 is malicious and

343
00:14:12,720 --> 00:14:15,040
in order to deviate the global model

344
00:14:15,040 --> 00:14:16,959
should be w prime

345
00:14:16,959 --> 00:14:20,399
client one which is malicious

346
00:14:20,399 --> 00:14:24,240
sends a poisoned local model w1 prime

347
00:14:24,240 --> 00:14:27,440
to the server such that

348
00:14:27,440 --> 00:14:30,800
the aggregation of w1 prime

349
00:14:30,800 --> 00:14:35,839
and the w2 becomes w prime

350
00:14:36,079 --> 00:14:38,720
technically we formulate our local model

351
00:14:38,720 --> 00:14:39,600
position attack

352
00:14:39,600 --> 00:14:42,639
as this optimization problem the

353
00:14:42,639 --> 00:14:43,839
objective function

354
00:14:43,839 --> 00:14:46,079
is to maximize the deviation of the

355
00:14:46,079 --> 00:14:47,279
global model

356
00:14:47,279 --> 00:14:50,959
in some iteration w is a global model

357
00:14:50,959 --> 00:14:52,320
before attack

358
00:14:52,320 --> 00:14:56,320
w prime is a global model after attack

359
00:14:56,320 --> 00:14:59,440
s is the updated direction

360
00:14:59,440 --> 00:15:02,959
of the global model before attack

361
00:15:02,959 --> 00:15:05,519
without loss of generality let's assume

362
00:15:05,519 --> 00:15:06,320
the first

363
00:15:06,320 --> 00:15:11,199
c client on producers and the w1 prime

364
00:15:11,199 --> 00:15:14,000
two prime into wc prime are the poison

365
00:15:14,000 --> 00:15:15,120
local models

366
00:15:15,120 --> 00:15:18,480
on the resource client so

367
00:15:18,480 --> 00:15:21,519
our optimization problem aims to find

368
00:15:21,519 --> 00:15:25,360
the optimal point local models

369
00:15:25,360 --> 00:15:28,480
and the malicious client to maximize the

370
00:15:28,480 --> 00:15:31,759
deviation of the global model

371
00:15:31,759 --> 00:15:33,920
this constraint means that the global

372
00:15:33,920 --> 00:15:35,279
model w

373
00:15:35,279 --> 00:15:39,199
before attack is an aggregation

374
00:15:39,199 --> 00:15:42,480
of the of the local models on the b9

375
00:15:42,480 --> 00:15:44,240
client

376
00:15:44,240 --> 00:15:49,279
where a this a is an aggregation rule

377
00:15:49,279 --> 00:15:51,279
this constraint means that the global

378
00:15:51,279 --> 00:15:52,800
model w prime

379
00:15:52,800 --> 00:15:55,920
after attack is an aggregation

380
00:15:55,920 --> 00:15:59,279
of the poisoned local models on the

381
00:15:59,279 --> 00:16:00,880
malicious client

382
00:16:00,880 --> 00:16:04,000
and the benign local models

383
00:16:04,000 --> 00:16:07,120
on the benign client so

384
00:16:07,120 --> 00:16:09,759
in our attacks an attacker basically

385
00:16:09,759 --> 00:16:10,560
solves

386
00:16:10,560 --> 00:16:14,160
this optimization problem to find

387
00:16:14,160 --> 00:16:18,000
the optimal poison local models

388
00:16:18,000 --> 00:16:20,839
in all or multiple iterations of

389
00:16:20,839 --> 00:16:23,600
federation

390
00:16:23,600 --> 00:16:25,199
we looked at our formulation is

391
00:16:25,199 --> 00:16:28,000
applicable to any aggregation

392
00:16:28,000 --> 00:16:31,120
so in the future if a new aggregation

393
00:16:31,120 --> 00:16:32,720
rule is proposed

394
00:16:32,720 --> 00:16:36,480
our framework can be adapted to evaluate

395
00:16:36,480 --> 00:16:39,680
its security so next

396
00:16:39,680 --> 00:16:41,839
i will briefly discuss how to solve the

397
00:16:41,839 --> 00:16:43,040
optimization problem

398
00:16:43,040 --> 00:16:44,639
and the details can be found in our

399
00:16:44,639 --> 00:16:46,320
paper

400
00:16:46,320 --> 00:16:48,639
in a full laundry scenario the attacker

401
00:16:48,639 --> 00:16:50,240
has access to

402
00:16:50,240 --> 00:16:52,959
the local models on all clients and we

403
00:16:52,959 --> 00:16:53,600
can just

404
00:16:53,600 --> 00:16:55,360
solve the optimization problem using

405
00:16:55,360 --> 00:16:57,040
them

406
00:16:57,040 --> 00:16:58,800
in the partial launch scenario the

407
00:16:58,800 --> 00:17:00,959
attacker only has access to the local

408
00:17:00,959 --> 00:17:01,680
models

409
00:17:01,680 --> 00:17:04,959
on the mercutio's client we use them

410
00:17:04,959 --> 00:17:08,319
to estimate a global model w

411
00:17:08,319 --> 00:17:10,880
and then solve the optimization problem

412
00:17:10,880 --> 00:17:15,360
based on the estimated global model w

413
00:17:16,160 --> 00:17:18,559
when an attacker does not know the

414
00:17:18,559 --> 00:17:19,760
aggregation rule

415
00:17:19,760 --> 00:17:22,400
used by the server the attacker can

416
00:17:22,400 --> 00:17:23,119
assume

417
00:17:23,119 --> 00:17:26,160
one aggregation rule and then solves the

418
00:17:26,160 --> 00:17:27,679
optimization problem

419
00:17:27,679 --> 00:17:31,120
based on the assumed aggregation

420
00:17:31,120 --> 00:17:33,840
as we will see soon our attack

421
00:17:33,840 --> 00:17:36,160
constructed based on one aggregation

422
00:17:36,160 --> 00:17:37,039
rule

423
00:17:37,039 --> 00:17:39,760
will also be effective against other

424
00:17:39,760 --> 00:17:42,640
aggregation rules

425
00:17:43,280 --> 00:17:45,600
in our experiment we assume there are

426
00:17:45,600 --> 00:17:47,280
100 clients in total

427
00:17:47,280 --> 00:17:50,320
and 20 of them are leaders

428
00:17:50,320 --> 00:17:52,559
we evaluated our attacks on multiple

429
00:17:52,559 --> 00:17:55,120
data states

430
00:17:55,120 --> 00:17:57,919
the unique characteristics of federal

431
00:17:57,919 --> 00:17:59,120
learning is that

432
00:17:59,120 --> 00:18:02,080
the client's local data are often long

433
00:18:02,080 --> 00:18:03,760
id

434
00:18:03,760 --> 00:18:06,880
therefore we distribute a data set

435
00:18:06,880 --> 00:18:09,600
to the client such that the client's

436
00:18:09,600 --> 00:18:12,880
local data are long id

437
00:18:12,880 --> 00:18:14,799
next i will show you some experimental

438
00:18:14,799 --> 00:18:15,919
results on

439
00:18:15,919 --> 00:18:19,039
mnist which is a popular benchmark data

440
00:18:19,039 --> 00:18:20,799
center in machine learning

441
00:18:20,799 --> 00:18:22,640
the results on the other data sets can

442
00:18:22,640 --> 00:18:25,679
be found in our paper

443
00:18:25,760 --> 00:18:28,160
this table shows the results of our

444
00:18:28,160 --> 00:18:28,880
attacks

445
00:18:28,880 --> 00:18:32,240
and the multiple baseline attacks each

446
00:18:32,240 --> 00:18:33,200
though

447
00:18:33,200 --> 00:18:35,440
corresponds to a byzantine robust

448
00:18:35,440 --> 00:18:38,160
federation learning method

449
00:18:38,160 --> 00:18:40,720
this cardinal means low attack this

450
00:18:40,720 --> 00:18:41,760
canon means

451
00:18:41,760 --> 00:18:44,960
adding random gaussian noise to the

452
00:18:44,960 --> 00:18:46,080
local models

453
00:18:46,080 --> 00:18:49,440
on the medicines client this kind of

454
00:18:49,440 --> 00:18:50,320
means

455
00:18:50,320 --> 00:18:52,880
flipping the labels of the local

456
00:18:52,880 --> 00:18:54,320
training examples

457
00:18:54,320 --> 00:18:57,840
on the medicines client this canon means

458
00:18:57,840 --> 00:18:58,880
our attack

459
00:18:58,880 --> 00:19:01,200
in the partial learning scenario and

460
00:19:01,200 --> 00:19:02,960
these kind of means are attacked

461
00:19:02,960 --> 00:19:06,559
in the full loading scenario the numbers

462
00:19:06,559 --> 00:19:07,760
in this table

463
00:19:07,760 --> 00:19:10,880
are testing error rate of the global

464
00:19:10,880 --> 00:19:11,919
models

465
00:19:11,919 --> 00:19:13,840
learned by different methods under

466
00:19:13,840 --> 00:19:15,520
different attacks

467
00:19:15,520 --> 00:19:20,640
for example this number 0.75

468
00:19:20,640 --> 00:19:23,679
means that the global model

469
00:19:23,679 --> 00:19:27,440
learned by chrome under

470
00:19:27,440 --> 00:19:29,280
our attack in the partial launch

471
00:19:29,280 --> 00:19:30,720
scenario has

472
00:19:30,720 --> 00:19:34,000
a testing error rate of 0

473
00:19:34,000 --> 00:19:38,080
0.75 so as we can see from this table

474
00:19:38,080 --> 00:19:41,440
our attacks can significantly increase

475
00:19:41,440 --> 00:19:43,200
the testing error rates

476
00:19:43,200 --> 00:19:45,360
of the global models learned by

477
00:19:45,360 --> 00:19:46,799
byzantine robust

478
00:19:46,799 --> 00:19:50,799
methods this figure shows the

479
00:19:50,799 --> 00:19:52,960
impact of the fraction of malicious

480
00:19:52,960 --> 00:19:54,080
clients on

481
00:19:54,080 --> 00:19:57,280
the attack effectively the xx

482
00:19:57,280 --> 00:20:00,000
is a fraction of malicious clients the

483
00:20:00,000 --> 00:20:00,799
y-axis

484
00:20:00,799 --> 00:20:02,960
is a testing error rate of the global

485
00:20:02,960 --> 00:20:05,280
models learned by crew

486
00:20:05,280 --> 00:20:07,520
different curves correspond to different

487
00:20:07,520 --> 00:20:09,200
attacks

488
00:20:09,200 --> 00:20:12,480
as we can see our attacks are more

489
00:20:12,480 --> 00:20:13,440
effective

490
00:20:13,440 --> 00:20:17,360
when more clients become religious

491
00:20:17,360 --> 00:20:19,360
this figure shows the impact of the

492
00:20:19,360 --> 00:20:21,280
degree of lung id

493
00:20:21,280 --> 00:20:24,559
of the client's local data on the attack

494
00:20:24,559 --> 00:20:25,840
effectiveness

495
00:20:25,840 --> 00:20:28,960
the xx is the degree of lung id of the

496
00:20:28,960 --> 00:20:30,720
client's local data

497
00:20:30,720 --> 00:20:33,360
the y-ax is a testing error rate of the

498
00:20:33,360 --> 00:20:34,640
global model

499
00:20:34,640 --> 00:20:36,480
and the different curves correspond to

500
00:20:36,480 --> 00:20:39,120
different attacks

501
00:20:39,520 --> 00:20:41,440
as we can see our attacks are more

502
00:20:41,440 --> 00:20:44,159
effective when the client's

503
00:20:44,159 --> 00:20:47,360
local data are more like by the way

504
00:20:47,360 --> 00:20:50,640
the the details about how we

505
00:20:50,640 --> 00:20:53,760
simulate different degrees of lung id

506
00:20:53,760 --> 00:20:57,039
can be found in our paper

507
00:20:57,120 --> 00:20:59,600
this table shows the results when the

508
00:20:59,600 --> 00:21:00,480
attacker

509
00:21:00,480 --> 00:21:02,960
does not know the aggregation rule used

510
00:21:02,960 --> 00:21:05,200
by the server

511
00:21:05,200 --> 00:21:08,400
specifically this column means that

512
00:21:08,400 --> 00:21:11,440
the server uses chrome as the

513
00:21:11,440 --> 00:21:12,960
aggregation group

514
00:21:12,960 --> 00:21:15,840
this kernel means that the server uses

515
00:21:15,840 --> 00:21:17,039
trim the mean

516
00:21:17,039 --> 00:21:19,440
and this carnium means that the server

517
00:21:19,440 --> 00:21:21,679
uses medium

518
00:21:21,679 --> 00:21:24,960
this though means lower tank this though

519
00:21:24,960 --> 00:21:26,240
means that

520
00:21:26,240 --> 00:21:29,760
the attacker assumes the server uses

521
00:21:29,760 --> 00:21:30,400
chrome

522
00:21:30,400 --> 00:21:33,520
and performs attack based on crew and

523
00:21:33,520 --> 00:21:35,440
this though means that

524
00:21:35,440 --> 00:21:38,240
the attacker assumes the server uses

525
00:21:38,240 --> 00:21:39,360
streamed me

526
00:21:39,360 --> 00:21:43,039
and performs attack based on tremendous

527
00:21:43,039 --> 00:21:45,600
again the numbers in the table are the

528
00:21:45,600 --> 00:21:46,880
testing error rates

529
00:21:46,880 --> 00:21:50,159
of the global models as we can see from

530
00:21:50,159 --> 00:21:51,120
this table

531
00:21:51,120 --> 00:21:54,480
our attack constructed based on one

532
00:21:54,480 --> 00:21:56,240
aggregation view

533
00:21:56,240 --> 00:21:58,320
is also effective against other

534
00:21:58,320 --> 00:21:59,679
aggregation groups

535
00:21:59,679 --> 00:22:04,400
in most cases this table compares our

536
00:22:04,400 --> 00:22:06,559
local model poison attacks with data

537
00:22:06,559 --> 00:22:08,080
point attacks

538
00:22:08,080 --> 00:22:11,600
recall that data poisoning attacks

539
00:22:11,600 --> 00:22:14,080
poising the local data on the malicious

540
00:22:14,080 --> 00:22:15,039
clients

541
00:22:15,039 --> 00:22:18,000
while our local motor poisoning attacks

542
00:22:18,000 --> 00:22:19,919
poison the local models

543
00:22:19,919 --> 00:22:23,039
on the medial current

544
00:22:23,039 --> 00:22:26,240
this kind of means no attack this canon

545
00:22:26,240 --> 00:22:26,720
means

546
00:22:26,720 --> 00:22:29,039
the state-of-the-art theta polynomial

547
00:22:29,039 --> 00:22:30,320
tank

548
00:22:30,320 --> 00:22:33,200
and the last two canons correspond to

549
00:22:33,200 --> 00:22:34,240
our attacks

550
00:22:34,240 --> 00:22:36,720
in the partial and full launch scenarios

551
00:22:36,720 --> 00:22:38,559
respectively

552
00:22:38,559 --> 00:22:40,559
each zoe corresponds to a byzantine

553
00:22:40,559 --> 00:22:42,159
robust method

554
00:22:42,159 --> 00:22:44,480
again the numbers in the table are

555
00:22:44,480 --> 00:22:46,799
testing error rates of the global models

556
00:22:46,799 --> 00:22:49,840
under different attacks

557
00:22:49,840 --> 00:22:53,360
first data point attacks are ineffective

558
00:22:53,360 --> 00:22:56,320
for byzantine robust methods because

559
00:22:56,320 --> 00:22:57,520
this carbon

560
00:22:57,520 --> 00:23:01,520
is almost the same as this kernel

561
00:23:01,520 --> 00:23:05,120
second our attacks are effective

562
00:23:05,120 --> 00:23:08,159
for byzantine robust methods because

563
00:23:08,159 --> 00:23:11,280
these two canons have large

564
00:23:11,280 --> 00:23:14,720
testing average to summarize a little

565
00:23:14,720 --> 00:23:15,360
bit

566
00:23:15,360 --> 00:23:17,919
we proposed a general framework to

567
00:23:17,919 --> 00:23:19,840
attack federation learning

568
00:23:19,840 --> 00:23:22,960
our results show that existing byzantine

569
00:23:22,960 --> 00:23:23,679
robust

570
00:23:23,679 --> 00:23:26,880
federal learning methods are variable to

571
00:23:26,880 --> 00:23:30,080
locomotor coding attacks so

572
00:23:30,080 --> 00:23:33,520
that is part one of this talk

573
00:23:33,520 --> 00:23:36,559
and i think i saw

574
00:23:36,559 --> 00:23:39,840
some okay a link to the paper okay

575
00:23:39,840 --> 00:23:40,960
thanks mike for the

576
00:23:40,960 --> 00:23:44,159
for the link to the paper so

577
00:23:44,159 --> 00:23:47,120
maybe i can pause to see whether we have

578
00:23:47,120 --> 00:23:48,320
any questions about

579
00:23:48,320 --> 00:23:51,840
part one of this talk

580
00:23:54,559 --> 00:23:56,880
yeah do you have any any questions or

581
00:23:56,880 --> 00:23:59,760
comments about part one

582
00:23:59,760 --> 00:24:01,840
yeah neil hold just a second i have some

583
00:24:01,840 --> 00:24:03,360
hands being raised here so

584
00:24:03,360 --> 00:24:07,679
i'm going to start order uh saurabh

585
00:24:07,679 --> 00:24:09,520
nice to hear from you go ahead you have

586
00:24:09,520 --> 00:24:11,120
the mike

587
00:24:11,120 --> 00:24:14,480
uh hi nin can you hear me yes yes

588
00:24:14,480 --> 00:24:17,760
i can hear you so the the last part of

589
00:24:17,760 --> 00:24:19,520
what you presented with the data

590
00:24:19,520 --> 00:24:21,520
poisoning versus your model poisoning

591
00:24:21,520 --> 00:24:22,400
attack

592
00:24:22,400 --> 00:24:24,400
can be generalized from that and say

593
00:24:24,400 --> 00:24:26,720
that data poisoning is strictly a weaker

594
00:24:26,720 --> 00:24:27,200
attack

595
00:24:27,200 --> 00:24:30,840
form for federated learning than model

596
00:24:30,840 --> 00:24:32,559
poisoning

597
00:24:32,559 --> 00:24:36,799
can can we are you seeing like uh

598
00:24:36,799 --> 00:24:39,679
some fundamentally or theoretically we

599
00:24:39,679 --> 00:24:40,000
can

600
00:24:40,000 --> 00:24:42,960
yes yes for federated learning can be

601
00:24:42,960 --> 00:24:44,400
once and for all safe

602
00:24:44,400 --> 00:24:47,279
if our scheme is resilient against model

603
00:24:47,279 --> 00:24:48,960
poisoning then by definition it is

604
00:24:48,960 --> 00:24:52,400
resilient against data poisoning

605
00:24:52,400 --> 00:24:56,159
um i

606
00:24:56,159 --> 00:24:59,600
i'm not sure i think so first

607
00:24:59,600 --> 00:25:01,840
i think for uh so i can i can have the

608
00:25:01,840 --> 00:25:02,799
following comments

609
00:25:02,799 --> 00:25:06,000
so for any data pointing i think it's

610
00:25:06,000 --> 00:25:08,159
it's very easy to transform it into a

611
00:25:08,159 --> 00:25:09,440
local model problem

612
00:25:09,440 --> 00:25:12,000
so basically for any data protein attack

613
00:25:12,000 --> 00:25:12,640
we can

614
00:25:12,640 --> 00:25:14,720
we can easily design a locomotor polymer

615
00:25:14,720 --> 00:25:16,000
tag which has

616
00:25:16,000 --> 00:25:19,760
exactly the same uh effect as a data

617
00:25:19,760 --> 00:25:22,000
planning tag but not a vice versa

618
00:25:22,000 --> 00:25:25,279
for it because you see if we want to

619
00:25:25,279 --> 00:25:29,279
if we want to have a specific look model

620
00:25:29,279 --> 00:25:32,240
we have to reverse that local model back

621
00:25:32,240 --> 00:25:34,559
to how are we going to poison the local

622
00:25:34,559 --> 00:25:35,279
data

623
00:25:35,279 --> 00:25:39,279
to have that specific local model

624
00:25:39,279 --> 00:25:41,440
so to summarize a little bit we can we

625
00:25:41,440 --> 00:25:42,880
can easily

626
00:25:42,880 --> 00:25:46,159
use uh we can easily transform any

627
00:25:46,159 --> 00:25:48,559
data pattern attack to be a locomotor

628
00:25:48,559 --> 00:25:50,480
product attack but

629
00:25:50,480 --> 00:25:52,799
probably not that easy to go in the

630
00:25:52,799 --> 00:25:54,000
other direction

631
00:25:54,000 --> 00:25:57,039
and for your question if we can prevent

632
00:25:57,039 --> 00:25:58,880
the motor protein attacks

633
00:25:58,880 --> 00:26:01,919
does that mean we can

634
00:26:01,919 --> 00:26:04,240
we can also prevent all data poisoning

635
00:26:04,240 --> 00:26:06,559
attacks

636
00:26:06,559 --> 00:26:10,000
i think it depends on your defense

637
00:26:10,000 --> 00:26:11,600
it depends on your defense if your

638
00:26:11,600 --> 00:26:13,840
defense for example if

639
00:26:13,840 --> 00:26:16,960
if a defense does not have portable

640
00:26:16,960 --> 00:26:18,640
guarantee does not have theoretical

641
00:26:18,640 --> 00:26:19,840
guarantee

642
00:26:19,840 --> 00:26:22,880
then it may not be the case

643
00:26:22,880 --> 00:26:25,360
because if you say if if we come up with

644
00:26:25,360 --> 00:26:26,159
a defense

645
00:26:26,159 --> 00:26:29,120
this defense can empirically defend

646
00:26:29,120 --> 00:26:29,760
against

647
00:26:29,760 --> 00:26:33,039
some locomotor positive attacks

648
00:26:33,039 --> 00:26:35,360
okay some specific local motor planning

649
00:26:35,360 --> 00:26:36,799
attacks

650
00:26:36,799 --> 00:26:39,679
then that defense may not be able to

651
00:26:39,679 --> 00:26:40,960
defend against

652
00:26:40,960 --> 00:26:43,919
some very advanced the data permanent

653
00:26:43,919 --> 00:26:44,559
tax

654
00:26:44,559 --> 00:26:47,840
but if your if the defense

655
00:26:47,840 --> 00:26:50,880
has theoretical guarantee which means

656
00:26:50,880 --> 00:26:53,200
that if the defense can defend against

657
00:26:53,200 --> 00:26:55,520
all possible locomotor personal attacks

658
00:26:55,520 --> 00:26:57,200
like

659
00:26:57,200 --> 00:27:00,400
whatever however you change the local

660
00:27:00,400 --> 00:27:00,960
models

661
00:27:00,960 --> 00:27:02,559
you you construct the poison local

662
00:27:02,559 --> 00:27:04,640
models your defense is secure

663
00:27:04,640 --> 00:27:06,720
then i think that defense is also

664
00:27:06,720 --> 00:27:08,320
secured against the data protection

665
00:27:08,320 --> 00:27:09,840
attacks

666
00:27:09,840 --> 00:27:12,240
so quick follow-up so such a defense

667
00:27:12,240 --> 00:27:14,080
does not exist today right something

668
00:27:14,080 --> 00:27:15,520
that is resilience again

669
00:27:15,520 --> 00:27:18,080
all local model poisoning attacks

670
00:27:18,080 --> 00:27:20,240
actually in part three i'm going to

671
00:27:20,240 --> 00:27:23,039
uh discuss that a little bit yeah i see

672
00:27:23,039 --> 00:27:23,679
okay great

673
00:27:23,679 --> 00:27:27,760
thank you yeah thank you uh let's see

674
00:27:27,760 --> 00:27:30,000
i think we have uh yeah i have another

675
00:27:30,000 --> 00:27:30,960
uh

676
00:27:30,960 --> 00:27:34,159
hand raised here um richard

677
00:27:34,159 --> 00:27:36,880
if you'd like to ask yes sure and i've

678
00:27:36,880 --> 00:27:39,520
typed my question in the q a as well

679
00:27:39,520 --> 00:27:41,919
okay thank you very much for your talk

680
00:27:41,919 --> 00:27:43,039
it's been great

681
00:27:43,039 --> 00:27:45,760
what percentage of compromised clients

682
00:27:45,760 --> 00:27:48,080
are needed to achieve greater than

683
00:27:48,080 --> 00:27:52,960
0.5 testing error rate

684
00:27:52,960 --> 00:27:56,240
um i think this question depends on the

685
00:27:56,240 --> 00:27:57,919
data set and the specific

686
00:27:57,919 --> 00:28:01,120
federal learning setting yeah

687
00:28:01,120 --> 00:28:04,399
like and probably we can go back to some

688
00:28:04,399 --> 00:28:09,679
uh some slides

689
00:28:09,679 --> 00:28:13,440
here so here okay there we go yeah the x

690
00:28:13,440 --> 00:28:15,520
axis is a fraction of uh producer's

691
00:28:15,520 --> 00:28:17,840
clients

692
00:28:17,840 --> 00:28:22,159
so anything over five five percent

693
00:28:22,159 --> 00:28:24,799
right this is but this is for full audit

694
00:28:24,799 --> 00:28:26,799
attack if we look at the partial launch

695
00:28:26,799 --> 00:28:27,600
attack

696
00:28:27,600 --> 00:28:31,200
it's around like 10 percent yes okay

697
00:28:31,200 --> 00:28:33,520
but this is also for this specific

698
00:28:33,520 --> 00:28:35,039
method called chrome

699
00:28:35,039 --> 00:28:37,039
yeah and these curves may be different

700
00:28:37,039 --> 00:28:38,320
for different uh

701
00:28:38,320 --> 00:28:43,919
methods it's not that's not very many

702
00:28:43,919 --> 00:28:44,640
yeah

703
00:28:44,640 --> 00:28:48,320
yeah and also these these clients may

704
00:28:48,320 --> 00:28:49,440
not be

705
00:28:49,440 --> 00:28:51,520
only compromised they can also be fake

706
00:28:51,520 --> 00:28:53,039
clients

707
00:28:53,039 --> 00:28:54,640
like see i have a computer i have a

708
00:28:54,640 --> 00:28:56,080
powerful computer i can just use my

709
00:28:56,080 --> 00:28:57,200
computer

710
00:28:57,200 --> 00:29:00,320
to simulate mainly like uh smartphones

711
00:29:00,320 --> 00:29:04,000
yep yeah okay yeah thanks for the

712
00:29:04,000 --> 00:29:04,960
passing

713
00:29:04,960 --> 00:29:07,360
thank you

714
00:29:11,679 --> 00:29:14,320
yeah let's see do you have any more

715
00:29:14,320 --> 00:29:16,159
questions

716
00:29:16,159 --> 00:29:18,480
okay

717
00:29:22,480 --> 00:29:24,480
let me take our attack quality so you

718
00:29:24,480 --> 00:29:27,120
can defend to pinpaper

719
00:29:27,120 --> 00:29:29,520
yeah so let me read there are some more

720
00:29:29,520 --> 00:29:30,320
questions in

721
00:29:30,320 --> 00:29:32,880
queue uh in chat so what do you think

722
00:29:32,880 --> 00:29:34,720
are the limitations of your attack

723
00:29:34,720 --> 00:29:37,039
color cases where we can defend to

724
00:29:37,039 --> 00:29:40,240
enable server

725
00:29:40,720 --> 00:29:45,039
okay so i think um

726
00:29:45,039 --> 00:29:48,159
the limitation of our attack

727
00:29:48,159 --> 00:29:51,200
basically i guess i guess that's what we

728
00:29:51,200 --> 00:29:53,679
can discuss in the in the next two parts

729
00:29:53,679 --> 00:29:54,559
in part two and

730
00:29:54,559 --> 00:29:56,720
part three basically we can we can

731
00:29:56,720 --> 00:29:59,120
defend against these attacks and that's

732
00:29:59,120 --> 00:30:01,520
kind of uh leveraging the limitation of

733
00:30:01,520 --> 00:30:02,559
our attack

734
00:30:02,559 --> 00:30:06,480
so probably we can we can continue

735
00:30:06,880 --> 00:30:09,600
okay so let's uh let's continue to part

736
00:30:09,600 --> 00:30:10,799
two

737
00:30:10,799 --> 00:30:14,000
about uh building secure feather tiling

738
00:30:14,000 --> 00:30:15,039
via

739
00:30:15,039 --> 00:30:18,159
bootstrapping trust on the server

740
00:30:18,159 --> 00:30:20,480
so first the root cause of the

741
00:30:20,480 --> 00:30:22,000
insecurity of federal

742
00:30:22,000 --> 00:30:25,200
learning is that there is no

743
00:30:25,200 --> 00:30:28,720
lose trust in the system specifically

744
00:30:28,720 --> 00:30:30,960
from the server's perspective

745
00:30:30,960 --> 00:30:34,559
every client could be madison

746
00:30:34,559 --> 00:30:37,760
based on this observation we propose to

747
00:30:37,760 --> 00:30:40,159
build a secure federation learning via

748
00:30:40,159 --> 00:30:41,440
bootstrapping tasks

749
00:30:41,440 --> 00:30:44,480
on the server specifically

750
00:30:44,480 --> 00:30:47,360
the server collects a small clean

751
00:30:47,360 --> 00:30:48,840
training data set

752
00:30:48,840 --> 00:30:51,600
itself

753
00:30:51,600 --> 00:30:55,120
the server maintains a local model

754
00:30:55,120 --> 00:30:58,559
based on its collected training data set

755
00:30:58,559 --> 00:31:01,600
just like how a client maintains a local

756
00:31:01,600 --> 00:31:03,120
model

757
00:31:03,120 --> 00:31:05,200
we call the servers locomotor server

758
00:31:05,200 --> 00:31:06,720
mode

759
00:31:06,720 --> 00:31:09,279
then the server uses the server model to

760
00:31:09,279 --> 00:31:11,039
bootstrap trust

761
00:31:11,039 --> 00:31:14,799
specifically in each iteration

762
00:31:14,799 --> 00:31:17,760
the server assigns trust scores to the

763
00:31:17,760 --> 00:31:18,799
client

764
00:31:18,799 --> 00:31:21,679
based on the similarities between the

765
00:31:21,679 --> 00:31:23,200
client local models

766
00:31:23,200 --> 00:31:27,039
and the server model and then the server

767
00:31:27,039 --> 00:31:29,200
consenters these trust scores

768
00:31:29,200 --> 00:31:31,760
when aggregating the client local models

769
00:31:31,760 --> 00:31:34,720
to a global model

770
00:31:35,120 --> 00:31:37,760
recall that federal hiloni has three

771
00:31:37,760 --> 00:31:38,559
steps

772
00:31:38,559 --> 00:31:41,120
in step one the server sends its current

773
00:31:41,120 --> 00:31:42,799
global model to the client

774
00:31:42,799 --> 00:31:45,679
or a subset of them in step two the

775
00:31:45,679 --> 00:31:48,480
clients train their local models

776
00:31:48,480 --> 00:31:52,080
however unlike what we did in part one

777
00:31:52,080 --> 00:31:55,360
now we assume the uh we assume the

778
00:31:55,360 --> 00:31:56,000
client

779
00:31:56,000 --> 00:31:59,279
sends their local model update instead

780
00:31:59,279 --> 00:32:02,000
of their local models to the server

781
00:32:02,000 --> 00:32:04,960
in step 3 the server aggregates the

782
00:32:04,960 --> 00:32:05,679
client

783
00:32:05,679 --> 00:32:08,320
locomotor update as a global model

784
00:32:08,320 --> 00:32:09,279
update

785
00:32:09,279 --> 00:32:12,399
and uses the global mod update to update

786
00:32:12,399 --> 00:32:14,799
the global model

787
00:32:14,799 --> 00:32:18,320
so in our new federation learning method

788
00:32:18,320 --> 00:32:21,120
we basically design a new aggregation

789
00:32:21,120 --> 00:32:22,799
rule in step three

790
00:32:22,799 --> 00:32:25,840
and our new aggregation rule considers

791
00:32:25,840 --> 00:32:29,200
both the client local mod update and the

792
00:32:29,200 --> 00:32:30,640
server mode update

793
00:32:30,640 --> 00:32:34,000
to update the global model

794
00:32:34,000 --> 00:32:36,320
so next i will use an example to

795
00:32:36,320 --> 00:32:39,440
illustrate our new aggregation

796
00:32:39,440 --> 00:32:41,440
for simplicity let's assume we only have

797
00:32:41,440 --> 00:32:42,960
two clients

798
00:32:42,960 --> 00:32:45,279
these two clients have local model

799
00:32:45,279 --> 00:32:46,480
updates g1

800
00:32:46,480 --> 00:32:49,679
and g2 in some iteration

801
00:32:49,679 --> 00:32:53,360
the server model update is g0

802
00:32:53,360 --> 00:32:56,240
this graph illustrates these mod updates

803
00:32:56,240 --> 00:32:56,559
as

804
00:32:56,559 --> 00:32:59,679
vectors in the vector space our

805
00:32:59,679 --> 00:33:02,080
intuition is that

806
00:33:02,080 --> 00:33:05,880
if a locomotor update deviates

807
00:33:05,880 --> 00:33:08,320
substantially from the server mode

808
00:33:08,320 --> 00:33:09,440
update

809
00:33:09,440 --> 00:33:12,559
then the locomotive update may be

810
00:33:12,559 --> 00:33:16,080
from a malicious client for instance in

811
00:33:16,080 --> 00:33:17,120
our example

812
00:33:17,120 --> 00:33:20,320
the locomotor update g2

813
00:33:20,320 --> 00:33:22,559
debates substantially from the server

814
00:33:22,559 --> 00:33:24,399
mod update g0

815
00:33:24,399 --> 00:33:27,519
so the local model update g2 may be

816
00:33:27,519 --> 00:33:30,640
from a merger's client

817
00:33:30,640 --> 00:33:34,399
based on this intuition for each client

818
00:33:34,399 --> 00:33:38,399
we compute a cosign similarity between

819
00:33:38,399 --> 00:33:39,279
the client's

820
00:33:39,279 --> 00:33:43,519
local model and the local model update

821
00:33:43,519 --> 00:33:46,240
and the server mode update and we use

822
00:33:46,240 --> 00:33:48,000
the reroute to keep the cosine

823
00:33:48,000 --> 00:33:48,960
similarity

824
00:33:48,960 --> 00:33:52,000
as a client task score

825
00:33:52,000 --> 00:33:55,519
here we rule basically reset the cosine

826
00:33:55,519 --> 00:33:56,640
similarity

827
00:33:56,640 --> 00:34:00,080
as zero if it is negative

828
00:34:00,080 --> 00:34:02,559
and reroute keeps the cosine similarity

829
00:34:02,559 --> 00:34:06,240
as a task score if it's long negative

830
00:34:06,240 --> 00:34:09,679
in our example for client one we first

831
00:34:09,679 --> 00:34:11,520
compute the cosine similarity

832
00:34:11,520 --> 00:34:14,239
between local model objects g1 and

833
00:34:14,239 --> 00:34:17,040
server mode updates g0

834
00:34:17,040 --> 00:34:20,960
then we use ray rule to clip c1

835
00:34:20,960 --> 00:34:25,280
as a client one's trust for ts1

836
00:34:25,280 --> 00:34:27,599
since the cosine similarity c1 is

837
00:34:27,599 --> 00:34:28,560
positive

838
00:34:28,560 --> 00:34:33,119
the trust score ts1 is basically z1

839
00:34:33,119 --> 00:34:35,520
for client two we first compute the

840
00:34:35,520 --> 00:34:37,599
cosine similarity between g2 and

841
00:34:37,599 --> 00:34:40,719
g0 then we use red rule to clip

842
00:34:40,719 --> 00:34:43,839
g2 to be the client choose

843
00:34:43,839 --> 00:34:47,199
trust score ts2 since

844
00:34:47,199 --> 00:34:50,239
the cosine similarity c2 is negative the

845
00:34:50,239 --> 00:34:51,839
trust score tsq

846
00:34:51,839 --> 00:34:54,639
becomes zero

847
00:34:55,359 --> 00:34:59,119
in in local modal protein attacks the

848
00:34:59,119 --> 00:35:02,079
local models the poison local models

849
00:35:02,079 --> 00:35:03,839
from the malicious clients

850
00:35:03,839 --> 00:35:07,440
often have large magnitudes in order to

851
00:35:07,440 --> 00:35:11,520
dominate the impact of the local models

852
00:35:11,520 --> 00:35:15,359
from the benign client so we also

853
00:35:15,359 --> 00:35:18,320
propose to normalize the magnitude of

854
00:35:18,320 --> 00:35:20,560
the locomotor object

855
00:35:20,560 --> 00:35:23,839
specifically we normalize the magnitude

856
00:35:23,839 --> 00:35:27,440
of the local model update such that

857
00:35:27,440 --> 00:35:30,480
they have the same magnitude as the

858
00:35:30,480 --> 00:35:32,640
server mode update

859
00:35:32,640 --> 00:35:36,480
for instance g1 bar is a normalized

860
00:35:36,480 --> 00:35:39,760
local mode update for client one g2 bar

861
00:35:39,760 --> 00:35:42,480
is a normalized local mode update for

862
00:35:42,480 --> 00:35:44,720
client two

863
00:35:44,720 --> 00:35:47,599
finally the server computes the global

864
00:35:47,599 --> 00:35:49,040
model update g

865
00:35:49,040 --> 00:35:52,480
as the average of the client

866
00:35:52,480 --> 00:35:56,160
normalized locomotor update weighted by

867
00:35:56,160 --> 00:35:58,160
their trust scores

868
00:35:58,160 --> 00:36:01,040
and the server uses g to update the

869
00:36:01,040 --> 00:36:02,560
global model w

870
00:36:02,560 --> 00:36:06,880
here beta is a learning rate

871
00:36:07,599 --> 00:36:10,839
theoretically we show that under some

872
00:36:10,839 --> 00:36:12,000
assumptions

873
00:36:12,000 --> 00:36:14,160
for an arbitrary number of malicious

874
00:36:14,160 --> 00:36:16,000
clients the difference

875
00:36:16,000 --> 00:36:18,720
between the global model learned by fl

876
00:36:18,720 --> 00:36:19,680
trust

877
00:36:19,680 --> 00:36:22,400
and the optimal global model under no

878
00:36:22,400 --> 00:36:23,119
attacks

879
00:36:23,119 --> 00:36:26,880
can be bounded empirically we evaluated

880
00:36:26,880 --> 00:36:30,720
our our message on multiple data sets

881
00:36:30,720 --> 00:36:32,320
and in the following i will show you

882
00:36:32,320 --> 00:36:34,640
just the one experiment result

883
00:36:34,640 --> 00:36:37,119
on mnist and the results on the other

884
00:36:37,119 --> 00:36:38,000
data set

885
00:36:38,000 --> 00:36:41,680
can be found in our paper we assume

886
00:36:41,680 --> 00:36:43,839
there are 100 clients and 20 of them are

887
00:36:43,839 --> 00:36:45,440
malicious

888
00:36:45,440 --> 00:36:48,560
the server's training data set includes

889
00:36:48,560 --> 00:36:51,760
100 training examples sampled

890
00:36:51,760 --> 00:36:55,280
from amnesty uniformly at random

891
00:36:55,280 --> 00:36:59,040
this table shows the results each zone

892
00:36:59,040 --> 00:37:02,880
corresponds to no attack or an existing

893
00:37:02,880 --> 00:37:06,000
poisoning attack each carbon

894
00:37:06,000 --> 00:37:09,200
corresponds to a federal learning method

895
00:37:09,200 --> 00:37:12,240
here fl task is our method

896
00:37:12,240 --> 00:37:13,960
and the fed average is the

897
00:37:13,960 --> 00:37:15,920
state-of-the-art method

898
00:37:15,920 --> 00:37:18,960
in long adversarial settings

899
00:37:18,960 --> 00:37:21,040
again the numbers in the table are

900
00:37:21,040 --> 00:37:22,800
testing error rates

901
00:37:22,800 --> 00:37:24,880
of the global models learned by

902
00:37:24,880 --> 00:37:26,160
different methods

903
00:37:26,160 --> 00:37:29,200
under different attacks for example this

904
00:37:29,200 --> 00:37:31,240
number

905
00:37:31,240 --> 00:37:33,040
0.04 is

906
00:37:33,040 --> 00:37:36,640
a testing error rate of the global model

907
00:37:36,640 --> 00:37:41,040
learned by fed average under no attack

908
00:37:41,040 --> 00:37:43,839
as we can see from this table our fl

909
00:37:43,839 --> 00:37:44,320
task

910
00:37:44,320 --> 00:37:47,440
is robust against various existing

911
00:37:47,440 --> 00:37:49,200
poisoning attacks

912
00:37:49,200 --> 00:37:52,320
in particular the

913
00:37:52,320 --> 00:37:55,119
testing errors of fel trust under

914
00:37:55,119 --> 00:37:56,800
different attacks

915
00:37:56,800 --> 00:37:59,839
are similar to the testing error rate of

916
00:37:59,839 --> 00:38:00,240
fed

917
00:38:00,240 --> 00:38:05,119
average under no attack

918
00:38:05,119 --> 00:38:08,400
so if we deploy fl task and an attacker

919
00:38:08,400 --> 00:38:08,960
knows that

920
00:38:08,960 --> 00:38:12,480
fl task is deployed an attacker can

921
00:38:12,480 --> 00:38:16,320
design an adaptive attack to fl task

922
00:38:16,320 --> 00:38:20,079
we call that we formulated our locomotor

923
00:38:20,079 --> 00:38:20,960
protein attack

924
00:38:20,960 --> 00:38:23,839
as this optimization problem which is

925
00:38:23,839 --> 00:38:24,800
applicable

926
00:38:24,800 --> 00:38:28,720
to any aggregation therefore

927
00:38:28,720 --> 00:38:31,920
an attacker can construct an adaptive

928
00:38:31,920 --> 00:38:32,800
attack

929
00:38:32,800 --> 00:38:37,119
to fl task by replacing the aggregation

930
00:38:37,119 --> 00:38:37,760
rule

931
00:38:37,760 --> 00:38:40,960
a in this optimization problem

932
00:38:40,960 --> 00:38:45,280
as the aggregation rule of fel trust

933
00:38:45,280 --> 00:38:47,760
this is how we construct an adaptive

934
00:38:47,760 --> 00:38:50,400
attack to feel task

935
00:38:50,400 --> 00:38:53,440
this figure shows that our fl task is

936
00:38:53,440 --> 00:38:54,560
also robust

937
00:38:54,560 --> 00:38:58,640
against search adaptive attack the xx is

938
00:38:58,640 --> 00:39:00,960
a fraction of malicious clients

939
00:39:00,960 --> 00:39:04,000
the y-axis is the testing error rate

940
00:39:04,000 --> 00:39:06,320
of the global models this curve

941
00:39:06,320 --> 00:39:07,440
corresponds to

942
00:39:07,440 --> 00:39:10,560
fed average under no attack

943
00:39:10,560 --> 00:39:13,680
and this curve corresponds to fl trust

944
00:39:13,680 --> 00:39:17,119
under search adaptive attack as we can

945
00:39:17,119 --> 00:39:19,119
see from this graph

946
00:39:19,119 --> 00:39:22,800
fl trust under the adaptive attack

947
00:39:22,800 --> 00:39:25,839
and fed average under no attack

948
00:39:25,839 --> 00:39:28,880
can achieve similar testing error rates

949
00:39:28,880 --> 00:39:31,440
when up to 90 percent of clients are

950
00:39:31,440 --> 00:39:34,000
malicious

951
00:39:34,000 --> 00:39:37,040
to summarize the server can

952
00:39:37,040 --> 00:39:40,480
enhance security of federal learning via

953
00:39:40,480 --> 00:39:43,119
collecting a small training data set to

954
00:39:43,119 --> 00:39:43,839
bootstrap

955
00:39:43,839 --> 00:39:46,880
trust so that is

956
00:39:46,880 --> 00:39:49,760
part two of this talk and let's say do

957
00:39:49,760 --> 00:39:50,320
you have

958
00:39:50,320 --> 00:39:57,839
any questions about part two

959
00:40:04,160 --> 00:40:08,240
okay let me see q and e

960
00:40:09,200 --> 00:40:12,240
how i uh the first question is how iid

961
00:40:12,240 --> 00:40:16,240
is a data so we tried uh we tried in our

962
00:40:16,240 --> 00:40:18,079
uh in our paper we did a lot of

963
00:40:18,079 --> 00:40:20,400
experiments so we tried the different

964
00:40:20,400 --> 00:40:24,560
degrees of long id okay

965
00:40:24,560 --> 00:40:27,599
so we tried the different we tried

966
00:40:27,599 --> 00:40:29,200
different degrees of long id

967
00:40:29,200 --> 00:40:32,640
yeah and some are highly like id

968
00:40:32,640 --> 00:40:35,839
and some are not that high in id

969
00:40:35,839 --> 00:40:38,480
yeah so for more details please check

970
00:40:38,480 --> 00:40:41,040
our paper

971
00:40:44,720 --> 00:40:46,640
let me see the next question hello when

972
00:40:46,640 --> 00:40:48,400
ray ruiz applied to compute

973
00:40:48,400 --> 00:40:51,200
the charge score what fraction of

974
00:40:51,200 --> 00:40:52,000
clients are

975
00:40:52,000 --> 00:40:55,599
insured to have a long zero charge score

976
00:40:55,599 --> 00:40:57,520
if a large number of clients do not have

977
00:40:57,520 --> 00:40:59,280
a high closing same narrative with a

978
00:40:59,280 --> 00:41:00,800
rooted asset update

979
00:41:00,800 --> 00:41:04,240
can there be information loss

980
00:41:04,240 --> 00:41:07,440
so the question is uh

981
00:41:07,440 --> 00:41:11,119
how many uh what fraction of clients

982
00:41:11,119 --> 00:41:14,480
uh have long zero trust score

983
00:41:14,480 --> 00:41:17,280
uh we don't have any theoretical

984
00:41:17,280 --> 00:41:18,000
guarantee about

985
00:41:18,000 --> 00:41:21,440
this but in our ex and if

986
00:41:21,440 --> 00:41:24,480
if in some iteration

987
00:41:24,480 --> 00:41:28,079
okay if in some iteration

988
00:41:28,079 --> 00:41:32,839
no clients have non-zero charge score

989
00:41:32,839 --> 00:41:35,520
then we basically don't update the

990
00:41:35,520 --> 00:41:37,200
global model in that

991
00:41:37,200 --> 00:41:41,440
particular iteration and if

992
00:41:41,440 --> 00:41:45,119
and i agree if if

993
00:41:45,119 --> 00:41:48,640
this scenario happens a lot we may have

994
00:41:48,640 --> 00:41:49,359
some

995
00:41:49,359 --> 00:41:52,799
information loss yeah

996
00:41:54,240 --> 00:41:57,440
okay so i hope i hope that answers your

997
00:41:57,440 --> 00:41:58,400
question

998
00:41:58,400 --> 00:42:00,839
so let me see the next question does

999
00:42:00,839 --> 00:42:04,240
bootstrapping handle very high long idc

1000
00:42:04,240 --> 00:42:05,520
higher

1001
00:42:05,520 --> 00:42:10,079
than 0.6

1002
00:42:10,079 --> 00:42:12,560
yeah i think this is a very good

1003
00:42:12,560 --> 00:42:13,119
question

1004
00:42:13,119 --> 00:42:16,960
i think so the question is

1005
00:42:16,960 --> 00:42:19,280
again is about the long id setting right

1006
00:42:19,280 --> 00:42:20,319
so i think

1007
00:42:20,319 --> 00:42:23,359
in fao tasks there are two possible

1008
00:42:23,359 --> 00:42:25,440
places where long id may happen

1009
00:42:25,440 --> 00:42:29,040
one is the client local data align id

1010
00:42:29,040 --> 00:42:32,079
okay the second is the server's

1011
00:42:32,079 --> 00:42:35,280
training data set may have a different

1012
00:42:35,280 --> 00:42:36,720
distribution

1013
00:42:36,720 --> 00:42:39,680
right with uh with the client's local

1014
00:42:39,680 --> 00:42:40,640
data

1015
00:42:40,640 --> 00:42:43,760
so these two places could have low id

1016
00:42:43,760 --> 00:42:44,079
and

1017
00:42:44,079 --> 00:42:46,880
they may influence the effectiveness of

1018
00:42:46,880 --> 00:42:48,800
fuel tasks

1019
00:42:48,800 --> 00:42:51,119
first of all we don't have any

1020
00:42:51,119 --> 00:42:52,640
theoretical guarantee

1021
00:42:52,640 --> 00:42:55,520
about the performance of fo tasks under

1022
00:42:55,520 --> 00:42:56,880
such low id setting

1023
00:42:56,880 --> 00:43:00,240
but we did a lot of empirical

1024
00:43:00,240 --> 00:43:01,440
experiments

1025
00:43:01,440 --> 00:43:03,599
and the details can be found in our

1026
00:43:03,599 --> 00:43:06,240
paper so so we basically explored

1027
00:43:06,240 --> 00:43:09,440
one one important message is that

1028
00:43:09,440 --> 00:43:13,119
when the once the server's

1029
00:43:13,119 --> 00:43:16,480
training data set is not too

1030
00:43:16,480 --> 00:43:21,599
far away from the overall

1031
00:43:21,599 --> 00:43:24,160
training data distribution of the client

1032
00:43:24,160 --> 00:43:25,839
the nfl tasks

1033
00:43:25,839 --> 00:43:28,400
works well okay but if the server's

1034
00:43:28,400 --> 00:43:29,760
training data set

1035
00:43:29,760 --> 00:43:32,800
is very different from the

1036
00:43:32,800 --> 00:43:35,839
uh from the

1037
00:43:35,839 --> 00:43:39,200
distribution of the client data then for

1038
00:43:39,200 --> 00:43:40,400
task

1039
00:43:40,400 --> 00:43:43,760
will not work with well okay so i hope

1040
00:43:43,760 --> 00:43:44,400
that

1041
00:43:44,400 --> 00:43:47,440
answers your question

1042
00:43:47,440 --> 00:43:49,839
okay one more question how often do you

1043
00:43:49,839 --> 00:43:51,839
using updates the

1044
00:43:51,839 --> 00:43:54,880
bootstrap data set okay

1045
00:43:54,880 --> 00:43:57,680
so the question is how often do we

1046
00:43:57,680 --> 00:44:00,079
update the bootstrap data set

1047
00:44:00,079 --> 00:44:03,680
that's a good question we

1048
00:44:03,680 --> 00:44:07,119
we we didn't explore this

1049
00:44:07,119 --> 00:44:10,800
we didn't explore this uh this aspect

1050
00:44:10,800 --> 00:44:14,079
in our work

1051
00:44:14,079 --> 00:44:15,920
but if this is definitely a good

1052
00:44:15,920 --> 00:44:18,800
question especially if the data

1053
00:44:18,800 --> 00:44:20,880
is more like a time series for example

1054
00:44:20,880 --> 00:44:23,440
if data the data distribution the the

1055
00:44:23,440 --> 00:44:25,119
client's data distribution shifts

1056
00:44:25,119 --> 00:44:27,839
for example in real world okay the

1057
00:44:27,839 --> 00:44:29,839
client's data shifts over time

1058
00:44:29,839 --> 00:44:33,280
and then we may also need to

1059
00:44:33,280 --> 00:44:35,839
update the servers training data set

1060
00:44:35,839 --> 00:44:37,280
accordingly

1061
00:44:37,280 --> 00:44:40,880
and how often should we update

1062
00:44:40,880 --> 00:44:44,319
i think it may be application dependent

1063
00:44:44,319 --> 00:44:48,400
yeah maybe there is maybe there is some

1064
00:44:48,400 --> 00:44:51,119
way the server

1065
00:44:51,119 --> 00:44:53,359
maybe there is some way for the server

1066
00:44:53,359 --> 00:44:55,200
to decide

1067
00:44:55,200 --> 00:44:58,240
when it should update its

1068
00:44:58,240 --> 00:45:00,800
training data set based on some

1069
00:45:00,800 --> 00:45:02,160
statistics

1070
00:45:02,160 --> 00:45:03,599
yeah i think this is a very good

1071
00:45:03,599 --> 00:45:05,920
question i i don't have a very good

1072
00:45:05,920 --> 00:45:06,720
answer

1073
00:45:06,720 --> 00:45:08,319
yet but i think it's application

1074
00:45:08,319 --> 00:45:10,960
dependent and it's very interesting to

1075
00:45:10,960 --> 00:45:12,000
explore

1076
00:45:12,000 --> 00:45:14,640
like how can the server know when it

1077
00:45:14,640 --> 00:45:15,520
should update

1078
00:45:15,520 --> 00:45:18,560
its uh its um

1079
00:45:18,560 --> 00:45:20,720
it's training data set and how yeah i

1080
00:45:20,720 --> 00:45:23,040
think that's a very good question

1081
00:45:23,040 --> 00:45:26,960
it may be some future work to explore

1082
00:45:27,200 --> 00:45:30,400
okay one more question when you perform

1083
00:45:30,400 --> 00:45:32,240
the normalization and railroad

1084
00:45:32,240 --> 00:45:35,599
of the local weight where that

1085
00:45:35,599 --> 00:45:38,400
impacts the model performance at the

1086
00:45:38,400 --> 00:45:40,800
server

1087
00:45:40,800 --> 00:45:43,760
especially when there is no attack okay

1088
00:45:43,760 --> 00:45:44,319
yeah this

1089
00:45:44,319 --> 00:45:47,040
is a good question and actually our

1090
00:45:47,040 --> 00:45:49,119
empirical again it's empirical

1091
00:45:49,119 --> 00:45:51,920
our empirical results show that it's

1092
00:45:51,920 --> 00:45:52,720
actually

1093
00:45:52,720 --> 00:45:56,640
not influencing the model performance

1094
00:45:56,640 --> 00:46:00,079
when there are no attacks our our

1095
00:46:00,079 --> 00:46:02,800
one of our design goals is to make sure

1096
00:46:02,800 --> 00:46:04,640
that

1097
00:46:04,640 --> 00:46:08,880
our method works as well as fed average

1098
00:46:08,880 --> 00:46:12,240
when there are no attacks but again

1099
00:46:12,240 --> 00:46:14,720
this is based on the assumption that the

1100
00:46:14,720 --> 00:46:17,119
server's training data set

1101
00:46:17,119 --> 00:46:21,839
is not too different from the overall

1102
00:46:21,839 --> 00:46:25,280
training data of the of the client but

1103
00:46:25,280 --> 00:46:27,520
if the server's training data is too

1104
00:46:27,520 --> 00:46:28,880
different

1105
00:46:28,880 --> 00:46:32,160
from the overall training training data

1106
00:46:32,160 --> 00:46:33,839
distribution then

1107
00:46:33,839 --> 00:46:36,960
we will have performance loss

1108
00:46:36,960 --> 00:46:40,000
now okay

1109
00:46:40,800 --> 00:46:43,760
okay yeah thanks thanks for all the

1110
00:46:43,760 --> 00:46:45,280
questions

1111
00:46:45,280 --> 00:46:48,079
so now let's continue to uh part three

1112
00:46:48,079 --> 00:46:49,119
of this talk

1113
00:46:49,119 --> 00:46:52,720
about how to build a proverbially

1114
00:46:52,720 --> 00:46:56,400
secure uh federal learning method

1115
00:46:56,400 --> 00:46:59,760
so existing byzantine robust feathered

1116
00:46:59,760 --> 00:47:01,119
learning methods

1117
00:47:01,119 --> 00:47:04,400
including rfl tasks

1118
00:47:04,400 --> 00:47:07,680
can bond the change in global model

1119
00:47:07,680 --> 00:47:08,720
parameters

1120
00:47:08,720 --> 00:47:12,480
caused by medicinal clients based on

1121
00:47:12,480 --> 00:47:15,520
some assumptions however these

1122
00:47:15,520 --> 00:47:17,440
assumptions often do not hold in

1123
00:47:17,440 --> 00:47:19,040
practice

1124
00:47:19,040 --> 00:47:22,400
moreover they cannot

1125
00:47:22,400 --> 00:47:25,599
bond the change of testing error rate or

1126
00:47:25,599 --> 00:47:27,280
accuracy

1127
00:47:27,280 --> 00:47:30,480
caused by malicious clients

1128
00:47:30,480 --> 00:47:33,920
even if these assumptions do hold

1129
00:47:33,920 --> 00:47:36,640
to address these limitations we propose

1130
00:47:36,640 --> 00:47:37,359
to build

1131
00:47:37,359 --> 00:47:40,559
properly secure federation

1132
00:47:40,559 --> 00:47:43,520
for a given testing data set our

1133
00:47:43,520 --> 00:47:45,440
probably secure federated learning

1134
00:47:45,440 --> 00:47:48,240
can guarantee a lower bound of testing

1135
00:47:48,240 --> 00:47:49,359
accuracy

1136
00:47:49,359 --> 00:47:52,160
no matter what attacks the malicious

1137
00:47:52,160 --> 00:47:53,680
clients use

1138
00:47:53,680 --> 00:47:56,319
and the only assumption our method

1139
00:47:56,319 --> 00:47:57,839
requires is that

1140
00:47:57,839 --> 00:47:59,680
the number of malicious clients is

1141
00:47:59,680 --> 00:48:02,160
bounded

1142
00:48:03,440 --> 00:48:06,240
let's first formally define what we mean

1143
00:48:06,240 --> 00:48:08,559
by probable security

1144
00:48:08,559 --> 00:48:10,640
suppose we are given a set of benign

1145
00:48:10,640 --> 00:48:12,240
clients c

1146
00:48:12,240 --> 00:48:15,280
and a testing input x

1147
00:48:15,280 --> 00:48:17,280
we also have a federated learning

1148
00:48:17,280 --> 00:48:19,440
algorithm h

1149
00:48:19,440 --> 00:48:22,960
h c x is a label

1150
00:48:22,960 --> 00:48:27,040
predicted for x when the global model is

1151
00:48:27,040 --> 00:48:29,839
trained on c using federated learning

1152
00:48:29,839 --> 00:48:32,319
algorithm h

1153
00:48:32,319 --> 00:48:35,359
now assuming some clients in c

1154
00:48:35,359 --> 00:48:39,280
become religious and we denote the set

1155
00:48:39,280 --> 00:48:40,160
of clients

1156
00:48:40,160 --> 00:48:44,879
including the malicious ones as c prime

1157
00:48:45,040 --> 00:48:48,720
h c prime x is a label

1158
00:48:48,720 --> 00:48:52,079
predicted for x when the global model

1159
00:48:52,079 --> 00:48:54,800
is trained on c prime using fertility

1160
00:48:54,800 --> 00:48:57,359
learning aggregate h

1161
00:48:57,359 --> 00:48:59,440
we see the feather can only accurate

1162
00:48:59,440 --> 00:49:00,400
edge is

1163
00:49:00,400 --> 00:49:03,680
probably secure if h

1164
00:49:03,680 --> 00:49:07,280
c prime x equals h c x

1165
00:49:07,280 --> 00:49:10,319
for any c prime such that

1166
00:49:10,319 --> 00:49:12,880
the number of malicious clients is at

1167
00:49:12,880 --> 00:49:13,520
most

1168
00:49:13,520 --> 00:49:17,200
m star in other words a federated

1169
00:49:17,200 --> 00:49:19,440
learning algorithm is probably secure

1170
00:49:19,440 --> 00:49:22,640
if it's predicted label for a test

1171
00:49:22,640 --> 00:49:25,839
input is probably not affected

1172
00:49:25,839 --> 00:49:29,680
by a bounded number of malicious clients

1173
00:49:29,680 --> 00:49:33,359
here a we define m star as a certified

1174
00:49:33,359 --> 00:49:34,720
security level for

1175
00:49:34,720 --> 00:49:38,000
task input x note that different

1176
00:49:38,000 --> 00:49:40,400
testing inputs may have different

1177
00:49:40,400 --> 00:49:44,160
certified security levels

1178
00:49:44,160 --> 00:49:46,319
we propose an unstable feather genuine

1179
00:49:46,319 --> 00:49:47,920
method which is the first

1180
00:49:47,920 --> 00:49:51,760
method to achieve such probable security

1181
00:49:51,760 --> 00:49:56,000
suppose we are given inclined we

1182
00:49:56,000 --> 00:49:59,359
pick key clients uniformly at render

1183
00:49:59,359 --> 00:50:02,400
and train a global model on them

1184
00:50:02,400 --> 00:50:05,200
we can change the global model using any

1185
00:50:05,200 --> 00:50:07,119
federated learning method such as

1186
00:50:07,119 --> 00:50:10,960
fed average we repeat this operation to

1187
00:50:10,960 --> 00:50:11,359
train

1188
00:50:11,359 --> 00:50:15,359
n global models given a test

1189
00:50:15,359 --> 00:50:18,240
input x we use each of these n global

1190
00:50:18,240 --> 00:50:18,800
models

1191
00:50:18,800 --> 00:50:21,280
to predict its label so we have

1192
00:50:21,280 --> 00:50:24,319
unpredicted labels in total

1193
00:50:24,319 --> 00:50:27,200
then we take a majority vote among the

1194
00:50:27,200 --> 00:50:29,119
unpredicted labels

1195
00:50:29,119 --> 00:50:34,079
as the final predicted label for x

1196
00:50:35,119 --> 00:50:39,359
next i will use an example to illustrate

1197
00:50:39,359 --> 00:50:41,920
why our ensemble federal dynamic is

1198
00:50:41,920 --> 00:50:43,440
secure

1199
00:50:43,440 --> 00:50:47,200
suppose we have five clients and c5

1200
00:50:47,200 --> 00:50:50,240
is producers we train

1201
00:50:50,240 --> 00:50:54,800
four global models w1 w2 w3 and w4

1202
00:50:54,800 --> 00:50:58,319
and each global model is trained using

1203
00:50:58,319 --> 00:51:02,240
three randomly picked clients

1204
00:51:02,240 --> 00:51:05,760
and the global models w1 w2 and w3

1205
00:51:05,760 --> 00:51:09,680
are trained using only benign client

1206
00:51:09,680 --> 00:51:13,440
however the global model w4 is trained

1207
00:51:13,440 --> 00:51:16,160
using three clients including the

1208
00:51:16,160 --> 00:51:16,880
malicious

1209
00:51:16,880 --> 00:51:20,800
client c5 so w4 is pointed and

1210
00:51:20,800 --> 00:51:24,240
may make incorrect predictions given a

1211
00:51:24,240 --> 00:51:25,920
testing input x

1212
00:51:25,920 --> 00:51:28,880
the poison the global model w4 makes an

1213
00:51:28,880 --> 00:51:30,480
incorrect prediction

1214
00:51:30,480 --> 00:51:35,280
however the the global models w1 w2 and

1215
00:51:35,280 --> 00:51:36,000
w3

1216
00:51:36,000 --> 00:51:39,280
still make correct predictions

1217
00:51:39,280 --> 00:51:42,640
therefore the majority vote among the

1218
00:51:42,640 --> 00:51:43,920
four global models

1219
00:51:43,920 --> 00:51:48,400
is still correct and is not affected

1220
00:51:48,400 --> 00:51:51,200
by the meldicious client c5 no matter

1221
00:51:51,200 --> 00:51:52,240
what attacks

1222
00:51:52,240 --> 00:51:56,160
the malicious client c5 uses

1223
00:51:56,160 --> 00:51:59,440
theoretically we show that this ensemble

1224
00:51:59,440 --> 00:52:01,200
federated learning achieve the approval

1225
00:52:01,200 --> 00:52:02,319
security

1226
00:52:02,319 --> 00:52:05,280
specifically given a set of client c and

1227
00:52:05,280 --> 00:52:05,839
a test

1228
00:52:05,839 --> 00:52:08,400
input x we can derive the certified

1229
00:52:08,400 --> 00:52:11,359
security level m star for x

1230
00:52:11,359 --> 00:52:14,000
moreover we show that our derived

1231
00:52:14,000 --> 00:52:16,640
certified security level is tight

1232
00:52:16,640 --> 00:52:19,599
which means that without making more

1233
00:52:19,599 --> 00:52:21,040
assumptions

1234
00:52:21,040 --> 00:52:23,680
it is theoretically impossible to derive

1235
00:52:23,680 --> 00:52:25,599
a certified security level

1236
00:52:25,599 --> 00:52:29,040
that is larger than ours

1237
00:52:29,040 --> 00:52:31,280
in our evaluation we use certified

1238
00:52:31,280 --> 00:52:35,200
accuracy at aim as an evaluation metric

1239
00:52:35,200 --> 00:52:38,000
given a testing data set certified

1240
00:52:38,000 --> 00:52:39,520
accuracy at aim

1241
00:52:39,520 --> 00:52:43,119
is a fraction of testing input whose

1242
00:52:43,119 --> 00:52:46,160
labels are correctly predicted and

1243
00:52:46,160 --> 00:52:48,640
whose certified security levels are at

1244
00:52:48,640 --> 00:52:50,800
least in

1245
00:52:50,800 --> 00:52:53,520
certified accuracy at the end is a lower

1246
00:52:53,520 --> 00:52:56,000
bound of testing accuracy

1247
00:52:56,000 --> 00:52:58,640
a method can achieve when the number of

1248
00:52:58,640 --> 00:52:59,680
malicious clients

1249
00:52:59,680 --> 00:53:02,800
is at most n no matter what

1250
00:53:02,800 --> 00:53:04,960
poisoning attacks these medicinal

1251
00:53:04,960 --> 00:53:07,680
clients use

1252
00:53:08,160 --> 00:53:10,880
in this figure the xx is the number of

1253
00:53:10,880 --> 00:53:12,720
malicious clients in

1254
00:53:12,720 --> 00:53:15,280
the y-axis is the third fed accuracy at

1255
00:53:15,280 --> 00:53:16,480
n

1256
00:53:16,480 --> 00:53:19,200
this curve corresponds to fed average

1257
00:53:19,200 --> 00:53:21,280
this curve corresponds to our ensemble

1258
00:53:21,280 --> 00:53:22,720
fed average

1259
00:53:22,720 --> 00:53:25,760
in example that average we basically use

1260
00:53:25,760 --> 00:53:30,559
fed average to train the n global models

1261
00:53:30,559 --> 00:53:32,640
the results are obtained from amnesty

1262
00:53:32,640 --> 00:53:34,480
and we assume one thousand clients in

1263
00:53:34,480 --> 00:53:35,920
total

1264
00:53:35,920 --> 00:53:38,960
so as we can see the certified accuracy

1265
00:53:38,960 --> 00:53:40,079
of fed average

1266
00:53:40,079 --> 00:53:43,839
reduces to zero when just the one client

1267
00:53:43,839 --> 00:53:45,920
becomes malicious

1268
00:53:45,920 --> 00:53:49,040
however the certified accuracy of

1269
00:53:49,040 --> 00:53:51,920
ensemble fed average is still larger

1270
00:53:51,920 --> 00:53:52,520
than

1271
00:53:52,520 --> 00:53:58,480
0.85 when 20 clients become religious

1272
00:53:58,800 --> 00:54:00,960
this figure shows the impact of the

1273
00:54:00,960 --> 00:54:03,760
number of global models

1274
00:54:03,760 --> 00:54:06,880
on our method the xx is the number of

1275
00:54:06,880 --> 00:54:08,079
initial clients aim

1276
00:54:08,079 --> 00:54:10,960
the y-axis is a certified accuracy thing

1277
00:54:10,960 --> 00:54:11,520
different

1278
00:54:11,520 --> 00:54:14,800
curves correspond to different m so as

1279
00:54:14,800 --> 00:54:15,920
we can see

1280
00:54:15,920 --> 00:54:19,200
when we increase n from 100

1281
00:54:19,200 --> 00:54:23,040
to 500 okay which are these two curves

1282
00:54:23,040 --> 00:54:25,599
the certified accuracy increases

1283
00:54:25,599 --> 00:54:27,040
substantially

1284
00:54:27,040 --> 00:54:30,079
however when we further increase

1285
00:54:30,079 --> 00:54:32,800
m to be 1000 which corresponds to this

1286
00:54:32,800 --> 00:54:33,680
curve

1287
00:54:33,680 --> 00:54:35,440
the certified accuracy increases

1288
00:54:35,440 --> 00:54:37,280
marginally

1289
00:54:37,280 --> 00:54:40,240
so these results show that a moderate

1290
00:54:40,240 --> 00:54:41,599
number of global models

1291
00:54:41,599 --> 00:54:44,240
are enough for our method to achieve

1292
00:54:44,240 --> 00:54:46,960
good certified accuracy

1293
00:54:46,960 --> 00:54:49,520
to summarize initiative ensemble

1294
00:54:49,520 --> 00:54:50,640
federation is

1295
00:54:50,640 --> 00:54:53,359
probably secure against a bounded number

1296
00:54:53,359 --> 00:54:56,319
of malicious clients

1297
00:54:56,319 --> 00:54:58,640
and our method can achieve certified

1298
00:54:58,640 --> 00:54:59,440
accuracy

1299
00:54:59,440 --> 00:55:01,359
which is a lower bound of testing

1300
00:55:01,359 --> 00:55:03,119
accuracy no matter

1301
00:55:03,119 --> 00:55:07,359
what attacks the malicious clients use

1302
00:55:07,359 --> 00:55:10,559
so in this talk we discussed local model

1303
00:55:10,559 --> 00:55:11,359
project attacks

1304
00:55:11,359 --> 00:55:14,000
to federate learning and how to build

1305
00:55:14,000 --> 00:55:15,680
secure federated learning

1306
00:55:15,680 --> 00:55:18,400
via bootstrapping trust on the server

1307
00:55:18,400 --> 00:55:19,520
and

1308
00:55:19,520 --> 00:55:22,720
ensemble message finally i'd like to

1309
00:55:22,720 --> 00:55:24,799
thank my students and the collaborators

1310
00:55:24,799 --> 00:55:26,559
in this research

1311
00:55:26,559 --> 00:55:30,640
so that concludes my presentation

1312
00:55:31,119 --> 00:55:32,839
yeah man let's see do you have more

1313
00:55:32,839 --> 00:55:35,839
questions

1314
00:55:40,880 --> 00:55:44,079
okay so richard has one more it's not

1315
00:55:44,079 --> 00:55:45,520
like a question it's a comment

1316
00:55:45,520 --> 00:55:48,960
maybe update at the rate of

1317
00:55:48,960 --> 00:55:52,559
change of the long

1318
00:55:52,720 --> 00:55:56,160
zero that is to say update the

1319
00:55:56,160 --> 00:55:59,359
server model yeah yeah yeah and that was

1320
00:55:59,359 --> 00:56:00,640
part two

1321
00:56:00,640 --> 00:56:03,359
right right part two yes yes let it

1322
00:56:03,359 --> 00:56:05,520
update the server mode or the server's

1323
00:56:05,520 --> 00:56:06,640
training data set

1324
00:56:06,640 --> 00:56:10,079
or something yes yeah yeah yeah i think

1325
00:56:10,079 --> 00:56:10,400
that's

1326
00:56:10,400 --> 00:56:13,200
that's a good suggestion yeah i do think

1327
00:56:13,200 --> 00:56:13,599
uh

1328
00:56:13,599 --> 00:56:16,400
this is a very interesting direction

1329
00:56:16,400 --> 00:56:18,000
actually how to

1330
00:56:18,000 --> 00:56:21,119
how to update the

1331
00:56:21,119 --> 00:56:24,559
servers training data set yeah okay yeah

1332
00:56:24,559 --> 00:56:28,079
thanks for the comment richard

1333
00:56:29,680 --> 00:56:45,839
yeah let's see do i have more questions

1334
00:56:51,760 --> 00:56:54,799
okay yeah seems like we don't

1335
00:56:54,799 --> 00:56:58,240
have any more questions yeah i

1336
00:56:58,240 --> 00:57:01,040
i have uh some hands raised here okay

1337
00:57:01,040 --> 00:57:01,520
okay

1338
00:57:01,520 --> 00:57:04,160
i'm gonna somali i'm gonna turn on your

1339
00:57:04,160 --> 00:57:06,640
microphone

1340
00:57:10,799 --> 00:57:13,839
can you hear me yes yeah uh hi i'm

1341
00:57:13,839 --> 00:57:14,720
shamely chad

1342
00:57:14,720 --> 00:57:16,079
i'm an assistant professor at purdue

1343
00:57:16,079 --> 00:57:17,839
university and that was my uh

1344
00:57:17,839 --> 00:57:20,319
uh comment about updating the bootstrap

1345
00:57:20,319 --> 00:57:22,160
model i think this was fantastically

1346
00:57:22,160 --> 00:57:22,960
delivered

1347
00:57:22,960 --> 00:57:24,480
uh you know you had the attack and then

1348
00:57:24,480 --> 00:57:26,319
you you know showed how we can defend

1349
00:57:26,319 --> 00:57:27,280
against

1350
00:57:27,280 --> 00:57:29,359
a very malicious attack so for the

1351
00:57:29,359 --> 00:57:31,440
second point where you have this ability

1352
00:57:31,440 --> 00:57:32,400
to bootstrap

1353
00:57:32,400 --> 00:57:34,960
uh the server model by bootstrapping

1354
00:57:34,960 --> 00:57:36,799
what you have in terms of the clients

1355
00:57:36,799 --> 00:57:38,799
exposing different kinds of iid or

1356
00:57:38,799 --> 00:57:40,319
non-iid data sets

1357
00:57:40,319 --> 00:57:42,720
do you think that is kind of uh going

1358
00:57:42,720 --> 00:57:44,880
against the very philosophy of federated

1359
00:57:44,880 --> 00:57:46,559
learning especially if you start

1360
00:57:46,559 --> 00:57:48,400
kind of innovating and updating that

1361
00:57:48,400 --> 00:57:50,400
bootstrap model does that kind of go

1362
00:57:50,400 --> 00:57:52,319
against the fact that in federated

1363
00:57:52,319 --> 00:57:54,160
learning we don't want to do too much of

1364
00:57:54,160 --> 00:57:55,200
that so if you try

1365
00:57:55,200 --> 00:57:57,040
start trying to improve the accuracy of

1366
00:57:57,040 --> 00:57:58,480
that bootstrap model you might be

1367
00:57:58,480 --> 00:57:59,839
exposing more of the

1368
00:57:59,839 --> 00:58:03,119
data sets from the clients

1369
00:58:03,200 --> 00:58:04,570
you might um

1370
00:58:04,570 --> 00:58:06,319
[Music]

1371
00:58:06,319 --> 00:58:09,359
so let me make sure i understand your

1372
00:58:09,359 --> 00:58:13,119
comment but the server's training data

1373
00:58:13,119 --> 00:58:13,839
set is not

1374
00:58:13,839 --> 00:58:16,559
from the client the server collects the

1375
00:58:16,559 --> 00:58:17,440
the trinity

1376
00:58:17,440 --> 00:58:20,720
by itself i see

1377
00:58:20,720 --> 00:58:22,480
so uh yeah and that was my other

1378
00:58:22,480 --> 00:58:24,319
question you know how do you finish that

1379
00:58:24,319 --> 00:58:26,079
bootstrap data set at the

1380
00:58:26,079 --> 00:58:28,240
uh at the server like it would it be

1381
00:58:28,240 --> 00:58:30,160
kind of a snapshot of what the clients

1382
00:58:30,160 --> 00:58:30,799
are sending

1383
00:58:30,799 --> 00:58:32,400
so i guess there is like kind of a tug

1384
00:58:32,400 --> 00:58:34,240
of war there how would you collect

1385
00:58:34,240 --> 00:58:35,680
something that is

1386
00:58:35,680 --> 00:58:38,319
without kind of intruding into the data

1387
00:58:38,319 --> 00:58:39,280
space

1388
00:58:39,280 --> 00:58:41,680
yeah that's a good question so first of

1389
00:58:41,680 --> 00:58:42,480
all

1390
00:58:42,480 --> 00:58:45,520
i think the server should not

1391
00:58:45,520 --> 00:58:48,640
collect the data set from an untrusted

1392
00:58:48,640 --> 00:58:51,359
client because otherwise the data the

1393
00:58:51,359 --> 00:58:53,280
server trinity data may be pointed it's

1394
00:58:53,280 --> 00:58:54,720
like a data point

1395
00:58:54,720 --> 00:58:57,200
so so we argue that the server should

1396
00:58:57,200 --> 00:58:58,160
collect

1397
00:58:58,160 --> 00:59:01,440
a clean not poisoned training data set

1398
00:59:01,440 --> 00:59:04,160
by itself next for example google okay

1399
00:59:04,160 --> 00:59:06,319
let's say we use the m list the the

1400
00:59:06,319 --> 00:59:08,240
digit the the handwritten digits

1401
00:59:08,240 --> 00:59:09,599
recognition as an example

1402
00:59:09,599 --> 00:59:11,599
okay this is a probably a very reliable

1403
00:59:11,599 --> 00:59:13,760
example so let's say we want to learn a

1404
00:59:13,760 --> 00:59:15,920
federation only model to do handwritten

1405
00:59:15,920 --> 00:59:16,960
digit

1406
00:59:16,960 --> 00:59:20,160
recognition then google can for example

1407
00:59:20,160 --> 00:59:20,559
ask

1408
00:59:20,559 --> 00:59:23,760
its employees to handle write write some

1409
00:59:23,760 --> 00:59:26,720
digits and manually label them and then

1410
00:59:26,720 --> 00:59:28,400
google can use that data set

1411
00:59:28,400 --> 00:59:31,520
as its training data set

1412
00:59:31,520 --> 00:59:33,680
does this make sense so basically the

1413
00:59:33,680 --> 00:59:34,799
idea is that

1414
00:59:34,799 --> 00:59:37,280
the server is usually some uh some

1415
00:59:37,280 --> 00:59:38,480
service provider right

1416
00:59:38,480 --> 00:59:41,040
like some big company those companies

1417
00:59:41,040 --> 00:59:41,520
they have

1418
00:59:41,520 --> 00:59:43,440
employees they can ask their employees

1419
00:59:43,440 --> 00:59:46,480
to manual label or small training data

1420
00:59:46,480 --> 00:59:46,799
set

1421
00:59:46,799 --> 00:59:48,880
and our evaluation shows that this

1422
00:59:48,880 --> 00:59:50,480
server training data set

1423
00:59:50,480 --> 00:59:53,040
just can be very small okay the the

1424
00:59:53,040 --> 00:59:54,480
server does not need to collect

1425
00:59:54,480 --> 00:59:56,880
a huge training data set the server just

1426
00:59:56,880 --> 00:59:58,960
needs a very small tuning data set

1427
00:59:58,960 --> 01:00:01,920
so it can i mean manually label that

1428
01:00:01,920 --> 01:00:04,240
dataset by itself

1429
01:00:04,240 --> 01:00:06,240
yeah does that make sense yeah that

1430
01:00:06,240 --> 01:00:07,839
clarifies the things i was actually

1431
01:00:07,839 --> 01:00:09,760
thinking of having the ability to also

1432
01:00:09,760 --> 01:00:11,280
bootstrap from the clients to get

1433
01:00:11,280 --> 01:00:12,319
something that is a little more

1434
01:00:12,319 --> 01:00:13,680
representative of the

1435
01:00:13,680 --> 01:00:16,240
big wide world but your perspective

1436
01:00:16,240 --> 01:00:17,760
makes it a little cleaner

1437
01:00:17,760 --> 01:00:19,359
rather than getting into the space of

1438
01:00:19,359 --> 01:00:21,920
the client so thank you for clarifying

1439
01:00:21,920 --> 01:00:26,400
yeah okay thank you for the question of

1440
01:00:26,839 --> 01:00:29,839
course

1441
01:00:30,720 --> 01:00:33,760
let me see i think q and a has democracy

1442
01:00:33,760 --> 01:00:35,200
okay

1443
01:00:35,200 --> 01:00:37,440
one question does the bounded number of

1444
01:00:37,440 --> 01:00:39,680
malicious clients determine the probable

1445
01:00:39,680 --> 01:00:41,119
security levels

1446
01:00:41,119 --> 01:00:44,319
yes exactly actually

1447
01:00:44,319 --> 01:00:47,200
so the m star so i should make it more

1448
01:00:47,200 --> 01:00:48,240
clear

1449
01:00:48,240 --> 01:00:50,880
for n actually for any number of

1450
01:00:50,880 --> 01:00:53,119
medicinal clients we can derive the

1451
01:00:53,119 --> 01:00:55,280
certified security level m star

1452
01:00:55,280 --> 01:00:58,000
but if the number of malicious clients

1453
01:00:58,000 --> 01:01:00,160
is larger than some threshold the m star

1454
01:01:00,160 --> 01:01:01,359
will become

1455
01:01:01,359 --> 01:01:04,480
basically oh

1456
01:01:04,480 --> 01:01:07,040
yeah the m star will become basically

1457
01:01:07,040 --> 01:01:07,920
zero

1458
01:01:07,920 --> 01:01:11,359
something meaningless right right

1459
01:01:11,359 --> 01:01:14,480
yeah so the next question is what

1460
01:01:14,480 --> 01:01:18,319
is the overhead introduced by the

1461
01:01:18,319 --> 01:01:19,040
ensembl

1462
01:01:19,040 --> 01:01:22,079
setup yeah this is also a good question

1463
01:01:22,079 --> 01:01:25,119
so the question is because in our

1464
01:01:25,119 --> 01:01:26,799
ensemble federation learning we train

1465
01:01:26,799 --> 01:01:28,240
multiple global models

1466
01:01:28,240 --> 01:01:31,040
okay so uh one question is what is the

1467
01:01:31,040 --> 01:01:33,359
overhead

1468
01:01:33,359 --> 01:01:37,040
to train these n global models uh

1469
01:01:37,040 --> 01:01:40,400
there so if we uh

1470
01:01:40,400 --> 01:01:43,839
so i should see there is a trade-off

1471
01:01:43,839 --> 01:01:46,319
between the overhead and the probable

1472
01:01:46,319 --> 01:01:47,440
security

1473
01:01:47,440 --> 01:01:50,480
okay so why i see this because see

1474
01:01:50,480 --> 01:01:53,520
now i train in global models but

1475
01:01:53,520 --> 01:01:55,920
i can remember the federation learning

1476
01:01:55,920 --> 01:01:57,520
is an alternative process

1477
01:01:57,520 --> 01:02:00,079
let's see originally when i change just

1478
01:02:00,079 --> 01:02:01,119
one global model

1479
01:02:01,119 --> 01:02:05,039
i use 1000 iterations okay so this will

1480
01:02:05,039 --> 01:02:06,240
introduce some

1481
01:02:06,240 --> 01:02:07,920
overhead right communication cost and

1482
01:02:07,920 --> 01:02:09,440
the computation cost for the

1483
01:02:09,440 --> 01:02:12,799
for the clients now i train 10 global

1484
01:02:12,799 --> 01:02:13,839
models

1485
01:02:13,839 --> 01:02:17,920
however when training each global model

1486
01:02:17,920 --> 01:02:21,039
i just run 100 iterations

1487
01:02:21,039 --> 01:02:23,119
okay so i wrong less number of

1488
01:02:23,119 --> 01:02:24,799
iterations when i train

1489
01:02:24,799 --> 01:02:28,160
each global model such that the total

1490
01:02:28,160 --> 01:02:28,720
cost

1491
01:02:28,720 --> 01:02:30,400
okay the total communication and

1492
01:02:30,400 --> 01:02:32,079
computation cost

1493
01:02:32,079 --> 01:02:35,200
may be the same as training one

1494
01:02:35,200 --> 01:02:38,319
global model but when we do this

1495
01:02:38,319 --> 01:02:41,119
we may get a smaller certified accuracy

1496
01:02:41,119 --> 01:02:42,000
okay

1497
01:02:42,000 --> 01:02:44,079
so there is a tweet off if we want to

1498
01:02:44,079 --> 01:02:46,480
have a larger certified accuracy

1499
01:02:46,480 --> 01:02:49,119
we may train more iterations for each

1500
01:02:49,119 --> 01:02:50,400
global model

1501
01:02:50,400 --> 01:02:53,760
okay so i hope that answers your

1502
01:02:53,760 --> 01:02:56,160
question

1503
01:02:56,720 --> 01:02:58,960
in part so one more question in part

1504
01:02:58,960 --> 01:03:00,480
three do you have

1505
01:03:00,480 --> 01:03:04,000
any recommended strategy for choosing

1506
01:03:04,000 --> 01:03:05,680
the groups of clients

1507
01:03:05,680 --> 01:03:08,880
to train each of the local models

1508
01:03:08,880 --> 01:03:10,960
or do you just pick the group's uniform

1509
01:03:10,960 --> 01:03:12,400
at random yes

1510
01:03:12,400 --> 01:03:15,839
in order to at least for now

1511
01:03:15,839 --> 01:03:17,839
in order to derive the certified

1512
01:03:17,839 --> 01:03:20,000
security level we have to pick the

1513
01:03:20,000 --> 01:03:22,160
groups uniformly at random

1514
01:03:22,160 --> 01:03:26,399
when change the global models yeah

1515
01:03:34,240 --> 01:03:37,359
okay thanks neil that was a great talk

1516
01:03:37,359 --> 01:03:40,960
um thank you so i just want to remind

1517
01:03:40,960 --> 01:03:44,000
our viewers that um you know we do this

1518
01:03:44,000 --> 01:03:46,400
seminar every wednesday this summer at 1

1519
01:03:46,400 --> 01:03:50,119
30 and you can go to our website at

1520
01:03:50,119 --> 01:03:51,440
sirius.purdue.edu

1521
01:03:51,440 --> 01:03:54,559
and find uh this video pretty soon in

1522
01:03:54,559 --> 01:03:55,839
the next couple days and

1523
01:03:55,839 --> 01:03:58,960
any video that we've had in the past uh

1524
01:03:58,960 --> 01:04:02,000
going way back like almost 20 years of

1525
01:04:02,000 --> 01:04:04,079
security seminar videos so

1526
01:04:04,079 --> 01:04:07,039
thanks again neil take care yeah thanks

1527
01:04:07,039 --> 01:04:08,319
everyone

1528
01:04:08,319 --> 01:04:13,839
bye bye

1529
01:04:19,280 --> 01:04:21,359
you

