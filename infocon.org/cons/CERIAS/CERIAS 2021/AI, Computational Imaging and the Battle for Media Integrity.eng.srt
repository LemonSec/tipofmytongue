1
00:00:00,160 --> 00:00:03,760
welcome to all of the uh attendees

2
00:00:03,760 --> 00:00:07,680
for this uh last security seminar of the

3
00:00:07,680 --> 00:00:09,120
summer

4
00:00:09,120 --> 00:00:10,960
we've had a great series of speakers

5
00:00:10,960 --> 00:00:12,400
over the summer and

6
00:00:12,400 --> 00:00:16,520
we built up to this finale that um

7
00:00:16,520 --> 00:00:19,920
a friend of many of us colleague who's

8
00:00:19,920 --> 00:00:21,039
been in the field

9
00:00:21,039 --> 00:00:23,680
pioneered a number of things uh on the

10
00:00:23,680 --> 00:00:24,960
faculty at nyu

11
00:00:24,960 --> 00:00:28,160
but responsible for online programs real

12
00:00:28,160 --> 00:00:30,720
programs research

13
00:00:30,720 --> 00:00:33,200
student competitions conferences all

14
00:00:33,200 --> 00:00:34,640
kinds of things in the area

15
00:00:34,640 --> 00:00:37,520
of cyber security that have helped grow

16
00:00:37,520 --> 00:00:39,200
the field

17
00:00:39,200 --> 00:00:41,680
and he's going to share some thoughts

18
00:00:41,680 --> 00:00:43,120
with us today

19
00:00:43,120 --> 00:00:45,600
given that background on ai

20
00:00:45,600 --> 00:00:47,840
computational imaging and the battle

21
00:00:47,840 --> 00:00:51,120
for media integrity so uh

22
00:00:51,120 --> 00:00:54,160
welcome to nasir memen and please take

23
00:00:54,160 --> 00:00:55,440
it away

24
00:00:55,440 --> 00:00:57,120
thank you thank you ed you can hear me

25
00:00:57,120 --> 00:00:58,879
right

26
00:00:58,879 --> 00:01:02,399
yes and i'll share my screen

27
00:01:02,399 --> 00:01:06,000
to get started and go in presentation

28
00:01:06,000 --> 00:01:07,840
mode

29
00:01:07,840 --> 00:01:13,840
um okay oops sorry

30
00:01:15,520 --> 00:01:18,479
it's slow to respond uh so good

31
00:01:18,479 --> 00:01:20,000
afternoon everyone i guess it's

32
00:01:20,000 --> 00:01:22,799
it's afternoon there as well uh i'm

33
00:01:22,799 --> 00:01:24,799
speaking here from brooklyn new york

34
00:01:24,799 --> 00:01:28,080
and it's a very very hot day outside i

35
00:01:28,080 --> 00:01:30,000
don't know if it's the same

36
00:01:30,000 --> 00:01:33,280
you know in lafayette

37
00:01:33,280 --> 00:01:35,680
and i'm actually talking from my office

38
00:01:35,680 --> 00:01:37,040
i mean i'm back

39
00:01:37,040 --> 00:01:41,119
at work and it's fun to be back

40
00:01:41,119 --> 00:01:45,520
uh after such a long gap

41
00:01:45,520 --> 00:01:48,720
so this afternoon i'll i'll

42
00:01:48,720 --> 00:01:52,158
talk to you a little bit about

43
00:01:52,399 --> 00:01:56,240
what i guess people in the news

44
00:01:56,240 --> 00:01:58,399
these days are calling as the media

45
00:01:58,399 --> 00:02:00,399
integrity crisis

46
00:02:00,399 --> 00:02:03,520
although crisis may be a strong word but

47
00:02:03,520 --> 00:02:04,479
nevertheless

48
00:02:04,479 --> 00:02:07,600
issues that we are facing today and we

49
00:02:07,600 --> 00:02:08,239
expect

50
00:02:08,239 --> 00:02:10,080
that perhaps things will go worse as

51
00:02:10,080 --> 00:02:11,520
they go further further down

52
00:02:11,520 --> 00:02:14,080
the road and that we should be doing

53
00:02:14,080 --> 00:02:15,200
something about it

54
00:02:15,200 --> 00:02:18,480
before they they get worse right and so

55
00:02:18,480 --> 00:02:19,680
i'll kind of

56
00:02:19,680 --> 00:02:23,040
so i understand i very much know this

57
00:02:23,040 --> 00:02:26,400
the center it's uh right one of the

58
00:02:26,400 --> 00:02:27,040
things i

59
00:02:27,040 --> 00:02:29,680
started a center at nyu and i thought we

60
00:02:29,680 --> 00:02:31,700
should become like psorias right so

61
00:02:31,700 --> 00:02:32,879
[Music]

62
00:02:32,879 --> 00:02:34,800
and i i don't know what the great work

63
00:02:34,800 --> 00:02:37,280
in cyber security

64
00:02:37,280 --> 00:02:40,319
that that you do

65
00:02:40,720 --> 00:02:43,680
and i understand a lot of you could be

66
00:02:43,680 --> 00:02:44,319
from a

67
00:02:44,319 --> 00:02:47,599
computer science background at the same

68
00:02:47,599 --> 00:02:49,200
time i

69
00:02:49,200 --> 00:02:51,920
feel a bit nervous that you have someone

70
00:02:51,920 --> 00:02:53,840
like ed delp sitting out there listening

71
00:02:53,840 --> 00:02:55,920
to me who's a founder of this field

72
00:02:55,920 --> 00:02:58,800
really yeah he's

73
00:02:58,800 --> 00:03:00,959
been at it for a very long time and took

74
00:03:00,959 --> 00:03:02,000
and i've

75
00:03:02,000 --> 00:03:04,560
kind of worked with him and have learned

76
00:03:04,560 --> 00:03:05,519
from him

77
00:03:05,519 --> 00:03:09,360
as this field matured right so

78
00:03:09,360 --> 00:03:11,040
and it sits more in the signal

79
00:03:11,040 --> 00:03:12,879
processing kind of world

80
00:03:12,879 --> 00:03:15,920
and and so i'll try to kind of talk

81
00:03:15,920 --> 00:03:17,920
about things at a reasonably high level

82
00:03:17,920 --> 00:03:19,840
and also try to be sensitive to

83
00:03:19,840 --> 00:03:23,120
the fact that their computer scientists

84
00:03:23,120 --> 00:03:25,519
in their audience and and try to get

85
00:03:25,519 --> 00:03:27,040
across some main points

86
00:03:27,040 --> 00:03:30,959
right so basically

87
00:03:30,959 --> 00:03:34,480
i'll talk about what the issues that we

88
00:03:34,480 --> 00:03:35,760
face today briefly

89
00:03:35,760 --> 00:03:38,000
i don't want to dwell too much time on

90
00:03:38,000 --> 00:03:39,040
that

91
00:03:39,040 --> 00:03:41,120
talk a little bit more about some of the

92
00:03:41,120 --> 00:03:42,879
key things that we have done in media

93
00:03:42,879 --> 00:03:44,480
forensics right for

94
00:03:44,480 --> 00:03:47,760
almost two decades now some of the key

95
00:03:47,760 --> 00:03:49,120
techniques and what

96
00:03:49,120 --> 00:03:53,599
now is emerging in the sense of how

97
00:03:53,599 --> 00:03:56,400
imaging is going neural in the sense

98
00:03:56,400 --> 00:03:57,920
that a lot of

99
00:03:57,920 --> 00:04:00,080
machine learning techniques have really

100
00:04:00,080 --> 00:04:02,720
transformed computer vision and imaging

101
00:04:02,720 --> 00:04:05,760
and how that's impacting how can impact

102
00:04:05,760 --> 00:04:07,280
media forensics

103
00:04:07,280 --> 00:04:09,599
and and how perhaps we can leverage it

104
00:04:09,599 --> 00:04:09,760
to

105
00:04:09,760 --> 00:04:11,439
[Music]

106
00:04:11,439 --> 00:04:14,640
sort of provide authenticity uh

107
00:04:14,640 --> 00:04:17,680
to media so as i said

108
00:04:17,680 --> 00:04:20,478
media forensics is the issue of whether

109
00:04:20,478 --> 00:04:22,720
this media is authentic or not

110
00:04:22,720 --> 00:04:24,400
has been around for a long time because

111
00:04:24,400 --> 00:04:26,160
the one even when things were not

112
00:04:26,160 --> 00:04:26,800
digital

113
00:04:26,800 --> 00:04:28,560
right but especially when things went

114
00:04:28,560 --> 00:04:30,160
digital uh

115
00:04:30,160 --> 00:04:33,040
denies the advantages to having sort of

116
00:04:33,040 --> 00:04:34,400
images in digital form

117
00:04:34,400 --> 00:04:36,560
right you can it brings down the cost

118
00:04:36,560 --> 00:04:38,400
scale you can share them and all that

119
00:04:38,400 --> 00:04:39,120
stuff

120
00:04:39,120 --> 00:04:41,680
but it also allows them us to manipulate

121
00:04:41,680 --> 00:04:43,199
them change them

122
00:04:43,199 --> 00:04:46,240
and uh and it for a long time it was

123
00:04:46,240 --> 00:04:47,040
done for fun

124
00:04:47,040 --> 00:04:49,759
right uh when we started working on it

125
00:04:49,759 --> 00:04:50,560
uh

126
00:04:50,560 --> 00:04:52,320
people like ed and me like long time

127
00:04:52,320 --> 00:04:55,040
back in the early 2000s right you're

128
00:04:55,040 --> 00:04:56,960
more worried about law enforcement point

129
00:04:56,960 --> 00:04:58,880
of view right whether this image is

130
00:04:58,880 --> 00:05:01,919
submitted as evidence is real or not but

131
00:05:01,919 --> 00:05:03,039
today it's kind of

132
00:05:03,039 --> 00:05:04,880
now being used for misinformation and

133
00:05:04,880 --> 00:05:06,320
propaganda

134
00:05:06,320 --> 00:05:08,960
and also over the last few years it's

135
00:05:08,960 --> 00:05:09,600
also

136
00:05:09,600 --> 00:05:11,680
sort of there's disturbing trends of

137
00:05:11,680 --> 00:05:13,919
fake images being used in

138
00:05:13,919 --> 00:05:15,919
in in scientific in the scientific

139
00:05:15,919 --> 00:05:17,039
literature right

140
00:05:17,039 --> 00:05:19,440
and and ed is doing a lot of nice work

141
00:05:19,440 --> 00:05:20,479
on that

142
00:05:20,479 --> 00:05:23,039
uh scientific reports and for clinical

143
00:05:23,039 --> 00:05:24,960
trials and things of that sort

144
00:05:24,960 --> 00:05:28,800
so so and a lot of that

145
00:05:28,800 --> 00:05:31,520
is being driven or fueled by the fact

146
00:05:31,520 --> 00:05:32,160
that

147
00:05:32,160 --> 00:05:35,919
we have uh better capabilities

148
00:05:35,919 --> 00:05:39,600
right to to manipulate images ewing

149
00:05:39,600 --> 00:05:42,160
using machine learning techniques uh and

150
00:05:42,160 --> 00:05:44,000
it can be done in a shorter time

151
00:05:44,000 --> 00:05:48,400
at lower cost right so

152
00:05:48,400 --> 00:05:50,560
so what this has led to what some people

153
00:05:50,560 --> 00:05:53,199
call the democratization of synthetic

154
00:05:53,199 --> 00:05:54,720
media as well i mean

155
00:05:54,720 --> 00:05:56,800
synthetic media also has been around for

156
00:05:56,800 --> 00:05:58,960
a long time it's also known as hollywood

157
00:05:58,960 --> 00:06:01,520
right uh hollywood isn't the business of

158
00:06:01,520 --> 00:06:03,199
creating media that's

159
00:06:03,199 --> 00:06:06,720
synthetic right there's some

160
00:06:06,720 --> 00:06:08,800
and gives you the illusion of something

161
00:06:08,800 --> 00:06:11,280
being true and many times it's not

162
00:06:11,280 --> 00:06:15,280
and and uh so

163
00:06:15,919 --> 00:06:19,520
and and although uh image manipulation

164
00:06:19,520 --> 00:06:21,520
has been around for a long time

165
00:06:21,520 --> 00:06:24,639
uh the recent advances in machine

166
00:06:24,639 --> 00:06:26,240
learning have also

167
00:06:26,240 --> 00:06:29,360
made it possible for us to develop what

168
00:06:29,360 --> 00:06:30,560
we now know

169
00:06:30,560 --> 00:06:32,479
know as deep fakes and cheap fakes and

170
00:06:32,479 --> 00:06:33,600
you've seen

171
00:06:33,600 --> 00:06:36,639
all that stuff in in media and i won't

172
00:06:36,639 --> 00:06:40,240
repeat it right uh and and but now we

173
00:06:40,240 --> 00:06:41,360
are also

174
00:06:41,360 --> 00:06:43,199
like in the sense it's going to the

175
00:06:43,199 --> 00:06:44,720
point where when i'm

176
00:06:44,720 --> 00:06:47,280
saying democratization it's available as

177
00:06:47,280 --> 00:06:48,240
a service right

178
00:06:48,240 --> 00:06:50,160
uh service in the sense that companies

179
00:06:50,160 --> 00:06:51,360
out there both

180
00:06:51,360 --> 00:06:53,840
overground and underground whereby you

181
00:06:53,840 --> 00:06:54,720
can

182
00:06:54,720 --> 00:06:58,960
say hey i want uh a deep fake on

183
00:06:58,960 --> 00:07:01,199
nasa amendment doing this and for 50

184
00:07:01,199 --> 00:07:03,120
bucks 100 bucks 500 bucks right the

185
00:07:03,120 --> 00:07:04,000
prices

186
00:07:04,000 --> 00:07:05,680
depending on what you want uh you can

187
00:07:05,680 --> 00:07:07,280
actually get it done right so it's

188
00:07:07,280 --> 00:07:08,000
available

189
00:07:08,000 --> 00:07:11,120
as a service uh there's also an app

190
00:07:11,120 --> 00:07:11,759
there not

191
00:07:11,759 --> 00:07:14,400
not very good right but still the fact

192
00:07:14,400 --> 00:07:16,560
it's a starting point whereby

193
00:07:16,560 --> 00:07:19,840
i can replace myself right

194
00:07:19,840 --> 00:07:22,400
in real time on a video conference call

195
00:07:22,400 --> 00:07:23,199
so

196
00:07:23,199 --> 00:07:24,960
for all you know 10 years later when i'm

197
00:07:24,960 --> 00:07:26,720
speaking perhaps it's my postdoc

198
00:07:26,720 --> 00:07:28,080
speaking but

199
00:07:28,080 --> 00:07:30,160
because i'm busy in hawaii taking a

200
00:07:30,160 --> 00:07:32,080
vacation or something but

201
00:07:32,080 --> 00:07:35,919
uh actually uh my uh you can deep free

202
00:07:35,919 --> 00:07:36,400
can

203
00:07:36,400 --> 00:07:38,639
appear as if it's me speaking and i have

204
00:07:38,639 --> 00:07:39,680
somebody else do that

205
00:07:39,680 --> 00:07:42,800
instead right so we are we're getting to

206
00:07:42,800 --> 00:07:44,240
that point where

207
00:07:44,240 --> 00:07:46,160
it's just not hollywood and nation

208
00:07:46,160 --> 00:07:48,000
states that can do this

209
00:07:48,000 --> 00:07:51,120
but anybody can not almost anybody

210
00:07:51,120 --> 00:07:52,720
should be able to we'll be able to do

211
00:07:52,720 --> 00:07:54,639
that and that that's a problem

212
00:07:54,639 --> 00:07:57,039
right now of course nation states being

213
00:07:57,039 --> 00:07:58,160
able to do that at

214
00:07:58,160 --> 00:08:00,879
stake at scale uh to create

215
00:08:00,879 --> 00:08:02,319
misinformation

216
00:08:02,319 --> 00:08:04,319
uh leads to problems at least

217
00:08:04,319 --> 00:08:06,160
polarization in society it

218
00:08:06,160 --> 00:08:08,720
leads to deaths and people who have died

219
00:08:08,720 --> 00:08:10,840
in india because of fake images a fake

220
00:08:10,840 --> 00:08:12,240
video

221
00:08:12,240 --> 00:08:15,599
and and what do we do right so one of

222
00:08:15,599 --> 00:08:17,680
the answers to that is of course

223
00:08:17,680 --> 00:08:21,280
media forensics right uh media security

224
00:08:21,280 --> 00:08:23,599
broadly and media forensics as

225
00:08:23,599 --> 00:08:26,879
sort of a subset of media

226
00:08:26,879 --> 00:08:28,800
media forensic as a subset of media

227
00:08:28,800 --> 00:08:29,919
security

228
00:08:29,919 --> 00:08:32,240
right and and the kinds of questions

229
00:08:32,240 --> 00:08:34,159
that we've traditionally answered right

230
00:08:34,159 --> 00:08:36,240
over a long time

231
00:08:36,240 --> 00:08:39,039
was hey given a picture video is this

232
00:08:39,039 --> 00:08:40,958
real or is it

233
00:08:40,958 --> 00:08:43,440
generated using computer graphics and

234
00:08:43,440 --> 00:08:45,120
this question of graphics or video has

235
00:08:45,120 --> 00:08:47,040
been around for more than a decade

236
00:08:47,040 --> 00:08:50,080
graphics are real captured by a camera

237
00:08:50,080 --> 00:08:51,839
has been around by for more than a

238
00:08:51,839 --> 00:08:53,360
decade now

239
00:08:53,360 --> 00:08:55,680
but now with synthetic images that can

240
00:08:55,680 --> 00:08:57,360
be generated using machine learning

241
00:08:57,360 --> 00:08:58,080
techniques

242
00:08:58,080 --> 00:08:59,519
where the problem has gotten more

243
00:08:59,519 --> 00:09:01,360
difficult right

244
00:09:01,360 --> 00:09:04,240
and again a harder problem is has this

245
00:09:04,240 --> 00:09:06,240
photograph being manipulated

246
00:09:06,240 --> 00:09:09,600
right and this is hard because we do

247
00:09:09,600 --> 00:09:11,519
kind of quote encode and manipulate

248
00:09:11,519 --> 00:09:12,959
images right

249
00:09:12,959 --> 00:09:16,880
for benign purposes and what's the

250
00:09:16,880 --> 00:09:20,720
line between sort of

251
00:09:20,720 --> 00:09:22,560
malicious manipulation and benign

252
00:09:22,560 --> 00:09:24,080
manipulation when can you say it's

253
00:09:24,080 --> 00:09:26,480
authentic and when you say not

254
00:09:26,480 --> 00:09:29,839
that starts it's a blurry fuzzy line

255
00:09:29,839 --> 00:09:31,680
and so it becomes harder and of course

256
00:09:31,680 --> 00:09:33,760
if the manipulation is tiny

257
00:09:33,760 --> 00:09:35,680
it becomes harder to detect it so that's

258
00:09:35,680 --> 00:09:37,279
even harder right

259
00:09:37,279 --> 00:09:38,800
and then there's also the attribution

260
00:09:38,800 --> 00:09:40,720
problem that we've always talked about

261
00:09:40,720 --> 00:09:42,560
right so something about prominence

262
00:09:42,560 --> 00:09:44,640
where did this image come from

263
00:09:44,640 --> 00:09:47,120
where did the video come from uh in

264
00:09:47,120 --> 00:09:48,399
sense of which camera

265
00:09:48,399 --> 00:09:50,320
what date and time was it taken what

266
00:09:50,320 --> 00:09:51,839
place was it taken

267
00:09:51,839 --> 00:09:54,320
right things of that sort people have we

268
00:09:54,320 --> 00:09:56,880
have studied for a long time

269
00:09:56,880 --> 00:09:59,120
and and that's kind of what we've been

270
00:09:59,120 --> 00:10:00,560
doing in media forensics

271
00:10:00,560 --> 00:10:04,720
right and and why this essentially

272
00:10:04,720 --> 00:10:07,920
works is because uh and i'm i'll

273
00:10:07,920 --> 00:10:09,920
talk about images but a lot of what i

274
00:10:09,920 --> 00:10:11,760
say applies to video as well

275
00:10:11,760 --> 00:10:14,160
right and and essentially because when

276
00:10:14,160 --> 00:10:15,279
you manipulate any

277
00:10:15,279 --> 00:10:17,360
media and insert an object in it like

278
00:10:17,360 --> 00:10:18,320
like the scar

279
00:10:18,320 --> 00:10:21,040
out here i hope you can see my cursor

280
00:10:21,040 --> 00:10:21,680
right

281
00:10:21,680 --> 00:10:25,040
we go through certain sort of steps uh

282
00:10:25,040 --> 00:10:27,279
image processing steps and and those

283
00:10:27,279 --> 00:10:28,240
steps

284
00:10:28,240 --> 00:10:31,279
uh sort of leave some telltale artifacts

285
00:10:31,279 --> 00:10:33,120
right that allow us to potentially

286
00:10:33,120 --> 00:10:35,279
detect the fact that the image may have

287
00:10:35,279 --> 00:10:35,600
been

288
00:10:35,600 --> 00:10:38,240
detected and wait where do these

289
00:10:38,240 --> 00:10:40,079
artifacts stem from where do they come

290
00:10:40,079 --> 00:10:40,800
from

291
00:10:40,800 --> 00:10:44,000
well they come from the fact that the

292
00:10:44,000 --> 00:10:45,440
image is not produced

293
00:10:45,440 --> 00:10:47,600
magically right it's not that you press

294
00:10:47,600 --> 00:10:49,040
a button on the camera and boom

295
00:10:49,040 --> 00:10:51,040
an image is produced right there's

296
00:10:51,040 --> 00:10:52,640
actually a long and

297
00:10:52,640 --> 00:10:55,760
quite a computationally intensive uh

298
00:10:55,760 --> 00:10:58,880
processing pipeline that uh

299
00:10:58,880 --> 00:11:02,000
the sort of light from the scene uh

300
00:11:02,000 --> 00:11:03,120
which is then

301
00:11:03,120 --> 00:11:05,680
captured and converted right into this

302
00:11:05,680 --> 00:11:07,680
pixel value when you convert photons to

303
00:11:07,680 --> 00:11:10,000
electrons intensity when you measure

304
00:11:10,000 --> 00:11:11,839
and then you render that whole thing

305
00:11:11,839 --> 00:11:13,680
into a into a

306
00:11:13,680 --> 00:11:16,480
nice looking visually appealing image

307
00:11:16,480 --> 00:11:18,079
that that involves

308
00:11:18,079 --> 00:11:21,200
uh many different uh uh

309
00:11:21,200 --> 00:11:24,320
sort of image processing uh steps right

310
00:11:24,320 --> 00:11:25,279
and this is called

311
00:11:25,279 --> 00:11:28,320
the imaging pipeline right so that that

312
00:11:28,320 --> 00:11:30,640
happens within the camera

313
00:11:30,640 --> 00:11:32,720
and of course in the past you had to buy

314
00:11:32,720 --> 00:11:34,240
a big inexpensive camera

315
00:11:34,240 --> 00:11:37,360
to sort of have a

316
00:11:37,360 --> 00:11:40,000
reasonably powerful imaging pipeline but

317
00:11:40,000 --> 00:11:40,720
these days

318
00:11:40,720 --> 00:11:44,320
with uh with a sort of camera

319
00:11:44,320 --> 00:11:47,200
which is in your cell phone right uh you

320
00:11:47,200 --> 00:11:48,880
you actually have a lot of computational

321
00:11:48,880 --> 00:11:50,160
power in there to

322
00:11:50,160 --> 00:11:52,320
implement complicated sort of image

323
00:11:52,320 --> 00:11:53,839
pipelines right

324
00:11:53,839 --> 00:11:57,440
and and and these sort of leave certain

325
00:11:57,440 --> 00:12:00,240
characteristics broadly speaking right

326
00:12:00,240 --> 00:12:01,360
in the image

327
00:12:01,360 --> 00:12:03,360
and when you manipulate an image sort of

328
00:12:03,360 --> 00:12:05,920
the consistency of these characteristics

329
00:12:05,920 --> 00:12:08,320
gets disturbed and that's what we are we

330
00:12:08,320 --> 00:12:09,760
are doing we are doing statistical

331
00:12:09,760 --> 00:12:11,440
analysis and we are

332
00:12:11,440 --> 00:12:14,800
sort of trying to uh detect the fact

333
00:12:14,800 --> 00:12:16,720
that there is a break in consistency of

334
00:12:16,720 --> 00:12:18,240
these characteristics

335
00:12:18,240 --> 00:12:19,920
or or there's a certain type of

336
00:12:19,920 --> 00:12:21,760
consistency in this characteristics that

337
00:12:21,760 --> 00:12:22,560
tells me

338
00:12:22,560 --> 00:12:24,959
that it's a sony instead of a canon and

339
00:12:24,959 --> 00:12:26,320
things of that sort

340
00:12:26,320 --> 00:12:27,920
and this is all what we call signal

341
00:12:27,920 --> 00:12:30,399
processing and in purdue you have a

342
00:12:30,399 --> 00:12:32,320
phenomenal signal processing department

343
00:12:32,320 --> 00:12:33,920
with some very well

344
00:12:33,920 --> 00:12:36,480
accomplished people in there and and you

345
00:12:36,480 --> 00:12:36,959
can go

346
00:12:36,959 --> 00:12:39,760
learn a lot about about this from them

347
00:12:39,760 --> 00:12:40,720
right

348
00:12:40,720 --> 00:12:43,519
uh and

349
00:12:43,760 --> 00:12:47,040
so let me just give you for those of you

350
00:12:47,040 --> 00:12:49,279
who are new to media forensics

351
00:12:49,279 --> 00:12:51,360
and with apologies to those who know

352
00:12:51,360 --> 00:12:52,720
about all this

353
00:12:52,720 --> 00:12:54,560
let me just give you some simple example

354
00:12:54,560 --> 00:12:56,399
right one simple example which has been

355
00:12:56,399 --> 00:12:58,880
in my opinion quite successful

356
00:12:58,880 --> 00:13:00,320
something called we call sensor

357
00:13:00,320 --> 00:13:02,880
fingerprints right which uh

358
00:13:02,880 --> 00:13:05,399
or also the photo rounds photo response

359
00:13:05,399 --> 00:13:07,839
non-uniformity or prnu noise

360
00:13:07,839 --> 00:13:09,600
right and and i'll and there's some

361
00:13:09,600 --> 00:13:11,200
others also which i'll go through

362
00:13:11,200 --> 00:13:13,200
quickly but i'll talk about prnu in a

363
00:13:13,200 --> 00:13:14,480
bit more detail

364
00:13:14,480 --> 00:13:17,279
because intuitively it's sort of uh

365
00:13:17,279 --> 00:13:18,399
easier to capture

366
00:13:18,399 --> 00:13:22,720
right understand so so essentially

367
00:13:22,720 --> 00:13:25,680
the pr new noise essentially is kind of

368
00:13:25,680 --> 00:13:27,360
a noise pattern that

369
00:13:27,360 --> 00:13:29,839
gets embedded into every image taken as

370
00:13:29,839 --> 00:13:32,240
taken by a specific camera

371
00:13:32,240 --> 00:13:35,839
and this noise pattern is is very is

372
00:13:35,839 --> 00:13:37,760
unique i know unique is a strong word

373
00:13:37,760 --> 00:13:39,920
but quite unique

374
00:13:39,920 --> 00:13:42,240
right to the specific camera right in

375
00:13:42,240 --> 00:13:43,920
the sense that

376
00:13:43,920 --> 00:13:46,959
my iphone will have a different noise

377
00:13:46,959 --> 00:13:47,600
pattern

378
00:13:47,600 --> 00:13:49,440
as compared to your iphone although they

379
00:13:49,440 --> 00:13:50,880
will be the same model say bought from

380
00:13:50,880 --> 00:13:52,399
the same store same day

381
00:13:52,399 --> 00:13:54,800
whatever right two different instances

382
00:13:54,800 --> 00:13:56,560
of the device will have different noise

383
00:13:56,560 --> 00:13:57,519
patterns

384
00:13:57,519 --> 00:14:00,000
and that are essentially they come from

385
00:14:00,000 --> 00:14:01,040
the fact that

386
00:14:01,040 --> 00:14:03,199
the the underlying sensor right the

387
00:14:03,199 --> 00:14:04,639
imaging sensor

388
00:14:04,639 --> 00:14:07,199
right which uh when which senses uh

389
00:14:07,199 --> 00:14:07,680
light

390
00:14:07,680 --> 00:14:10,079
right and and converts those photons

391
00:14:10,079 --> 00:14:11,120
into

392
00:14:11,120 --> 00:14:13,839
light intensity electrons so that in in

393
00:14:13,839 --> 00:14:15,120
some measurable

394
00:14:15,120 --> 00:14:18,959
format right we uh we are able to

395
00:14:18,959 --> 00:14:21,199
that that response kind of varies

396
00:14:21,199 --> 00:14:22,320
because of

397
00:14:22,320 --> 00:14:25,360
no two physical sort of

398
00:14:25,360 --> 00:14:28,720
gizmos are exactly the same right and

399
00:14:28,720 --> 00:14:32,560
so there'll be a slight uh variation and

400
00:14:32,560 --> 00:14:35,040
a good analogy for that is what we call

401
00:14:35,040 --> 00:14:36,240
you you must have

402
00:14:36,240 --> 00:14:38,720
no you you would probably know about

403
00:14:38,720 --> 00:14:40,880
from reading detective novels from your

404
00:14:40,880 --> 00:14:42,880
childhood days right we called up about

405
00:14:42,880 --> 00:14:44,240
gun ballistics

406
00:14:44,240 --> 00:14:46,320
so when a bullet is fired through the

407
00:14:46,320 --> 00:14:47,600
gun right

408
00:14:47,600 --> 00:14:50,240
uh as the gun sort of traverses through

409
00:14:50,240 --> 00:14:51,600
the gun barrel

410
00:14:51,600 --> 00:14:53,920
each gun barrel all the same model make

411
00:14:53,920 --> 00:14:55,440
etc right

412
00:14:55,440 --> 00:14:58,320
has slightly different distortions in it

413
00:14:58,320 --> 00:14:59,920
deformations in it

414
00:14:59,920 --> 00:15:01,920
which causes unique scratches on the

415
00:15:01,920 --> 00:15:03,199
bullet as it

416
00:15:03,199 --> 00:15:05,519
sort of traverses through the barrel and

417
00:15:05,519 --> 00:15:07,360
then when you have a fired bullet

418
00:15:07,360 --> 00:15:09,440
right and you have a gun you can

419
00:15:09,440 --> 00:15:11,600
actually test it out fire 20 more

420
00:15:11,600 --> 00:15:12,000
bullets

421
00:15:12,000 --> 00:15:13,600
see what the scratch patterns that you

422
00:15:13,600 --> 00:15:15,600
get and compare

423
00:15:15,600 --> 00:15:18,000
that template scratch pattern with the

424
00:15:18,000 --> 00:15:20,000
bullet in question query bullet

425
00:15:20,000 --> 00:15:22,000
and say yep this bullet was indeed fired

426
00:15:22,000 --> 00:15:23,440
from this gun

427
00:15:23,440 --> 00:15:25,120
surprising thing as i said earlier is

428
00:15:25,120 --> 00:15:26,880
you can do this with cameras so you can

429
00:15:26,880 --> 00:15:27,920
give an image

430
00:15:27,920 --> 00:15:30,560
given a camera or you can say yep this

431
00:15:30,560 --> 00:15:32,560
image was taken from this camera

432
00:15:32,560 --> 00:15:34,399
and it's been quite successful and to my

433
00:15:34,399 --> 00:15:37,440
knowledge one of the

434
00:15:37,440 --> 00:15:40,000
few sort of forensic techniques that was

435
00:15:40,000 --> 00:15:41,519
accepted in a code of law

436
00:15:41,519 --> 00:15:45,120
right and the fbi studied it

437
00:15:45,120 --> 00:15:46,639
quite thoroughly and said this this

438
00:15:46,639 --> 00:15:48,800
works with high accuracy

439
00:15:48,800 --> 00:15:52,800
and and so not just attribution but it

440
00:15:52,800 --> 00:15:54,639
can also be

441
00:15:54,639 --> 00:15:58,160
used for sort of

442
00:15:58,160 --> 00:16:00,639
being able to tell whether an image is

443
00:16:00,639 --> 00:16:02,800
authentic or not so if you have a camera

444
00:16:02,800 --> 00:16:04,160
from which a particular

445
00:16:04,160 --> 00:16:06,399
image may have been taken and you have

446
00:16:06,399 --> 00:16:07,680
the image

447
00:16:07,680 --> 00:16:10,959
you can get the fingerprint of the

448
00:16:10,959 --> 00:16:11,600
camera

449
00:16:11,600 --> 00:16:13,839
match it and see if there's mismatch in

450
00:16:13,839 --> 00:16:15,120
a particular part

451
00:16:15,120 --> 00:16:18,240
you can say that hey this image this

452
00:16:18,240 --> 00:16:21,360
part of the image has been

453
00:16:21,360 --> 00:16:22,959
manipulate and there are other

454
00:16:22,959 --> 00:16:24,880
applications as well right that

455
00:16:24,880 --> 00:16:28,399
uh it can be used for so

456
00:16:28,399 --> 00:16:30,639
and and uh the noise pattern itself is

457
00:16:30,639 --> 00:16:32,000
what we call

458
00:16:32,000 --> 00:16:34,880
is non-linear right so you have the

459
00:16:34,880 --> 00:16:36,000
observed pixel value

460
00:16:36,000 --> 00:16:37,519
the measured pixel value within the

461
00:16:37,519 --> 00:16:39,759
camera but it's it's actually a function

462
00:16:39,759 --> 00:16:40,320
of

463
00:16:40,320 --> 00:16:42,560
the unobserved clean pixel value that

464
00:16:42,560 --> 00:16:44,560
came in through the lens right

465
00:16:44,560 --> 00:16:47,199
and that is then bettered by this by

466
00:16:47,199 --> 00:16:50,160
this prnu which is not additive it's

467
00:16:50,160 --> 00:16:52,000
multiplicative it's non-linear plus

468
00:16:52,000 --> 00:16:54,480
there's other sources of noise as well

469
00:16:54,480 --> 00:16:54,959
so

470
00:16:54,959 --> 00:16:57,680
there's a clean value x and then the

471
00:16:57,680 --> 00:16:59,440
value that you actually get when you get

472
00:16:59,440 --> 00:17:00,560
the photograph

473
00:17:00,560 --> 00:17:03,040
and this is where that prnu is sitting

474
00:17:03,040 --> 00:17:04,079
at right

475
00:17:04,079 --> 00:17:05,679
and and because of this it's kind of

476
00:17:05,679 --> 00:17:08,160
hard to remove and it survives many

477
00:17:08,160 --> 00:17:09,199
linear

478
00:17:09,199 --> 00:17:12,720
operations right and uh and and that has

479
00:17:12,720 --> 00:17:14,240
been the source of its uh

480
00:17:14,240 --> 00:17:17,599
success if you will uh and

481
00:17:17,599 --> 00:17:20,640
and the way it as i said it can be used

482
00:17:20,640 --> 00:17:21,439
for

483
00:17:21,439 --> 00:17:25,599
detecting photo manipulation as well so

484
00:17:25,599 --> 00:17:28,000
uh here is an image and and this the the

485
00:17:28,000 --> 00:17:30,000
circle red parts are the manipulated

486
00:17:30,000 --> 00:17:31,440
regions of the image

487
00:17:31,440 --> 00:17:34,799
and suppose you do have the camera from

488
00:17:34,799 --> 00:17:35,919
which this picture

489
00:17:35,919 --> 00:17:38,320
claims to have been taken you can

490
00:17:38,320 --> 00:17:39,919
extract that fingerprint

491
00:17:39,919 --> 00:17:41,679
and match it with the fingerprint in the

492
00:17:41,679 --> 00:17:43,760
image and there's certain parts these

493
00:17:43,760 --> 00:17:45,679
parts you'll see that the match is poor

494
00:17:45,679 --> 00:17:47,120
a low

495
00:17:47,120 --> 00:17:50,320
dark spot means a poor match right so

496
00:17:50,320 --> 00:17:52,400
that kind of pulls it out for you that

497
00:17:52,400 --> 00:17:53,919
these two are actually

498
00:17:53,919 --> 00:17:55,919
this parts are manipulated and you're

499
00:17:55,919 --> 00:17:57,520
able to localize

500
00:17:57,520 --> 00:17:59,280
the tampering that this was perhaps

501
00:17:59,280 --> 00:18:01,200
added and this the other this

502
00:18:01,200 --> 00:18:03,520
this was removed and this was added and

503
00:18:03,520 --> 00:18:05,679
you're able to detect both removal and

504
00:18:05,679 --> 00:18:08,400
addition of artifacts and of course

505
00:18:08,400 --> 00:18:09,760
there's a lot more to it how do you

506
00:18:09,760 --> 00:18:11,440
detect which part how do you grow those

507
00:18:11,440 --> 00:18:12,799
regions the different techniques

508
00:18:12,799 --> 00:18:15,919
that that can be used and it has some

509
00:18:15,919 --> 00:18:19,600
reasonable uh sort of accuracy

510
00:18:19,600 --> 00:18:22,559
in what we call localizing uh the

511
00:18:22,559 --> 00:18:24,480
manipulations right because you need to

512
00:18:24,480 --> 00:18:25,520
localize

513
00:18:25,520 --> 00:18:26,880
just saying whether an image is

514
00:18:26,880 --> 00:18:28,880
manipulated or not is not enough

515
00:18:28,880 --> 00:18:31,440
in many applications uh you want to

516
00:18:31,440 --> 00:18:33,440
actually be able to say what part of the

517
00:18:33,440 --> 00:18:35,600
image was manipulated

518
00:18:35,600 --> 00:18:38,799
and so this works great right

519
00:18:38,799 --> 00:18:40,640
but there's certain issues with it and

520
00:18:40,640 --> 00:18:42,320
and many of you who have a security

521
00:18:42,320 --> 00:18:43,360
mindset

522
00:18:43,360 --> 00:18:45,200
uh probably it's already jumped in your

523
00:18:45,200 --> 00:18:46,640
head saying what the heck if there's a

524
00:18:46,640 --> 00:18:48,720
fingerprint why can't i simply

525
00:18:48,720 --> 00:18:52,080
uh spoof it why can't i take an image

526
00:18:52,080 --> 00:18:54,400
extract its fingerprint and then put it

527
00:18:54,400 --> 00:18:56,160
on in another image take an image from

528
00:18:56,160 --> 00:18:58,320
nasir's camera

529
00:18:58,320 --> 00:19:01,600
and extract that fingerprint from it

530
00:19:01,600 --> 00:19:03,520
and then put it in some other image and

531
00:19:03,520 --> 00:19:05,280
claim that nasa's camera took it

532
00:19:05,280 --> 00:19:08,480
right yeah can be done right not that

533
00:19:08,480 --> 00:19:10,480
easy but by a skilled

534
00:19:10,480 --> 00:19:13,600
uh manipulator skilled person it

535
00:19:13,600 --> 00:19:16,400
is it can be done so that's that's an

536
00:19:16,400 --> 00:19:17,200
issue

537
00:19:17,200 --> 00:19:19,600
uh the other issue is the privacy leak

538
00:19:19,600 --> 00:19:20,640
right so

539
00:19:20,640 --> 00:19:23,360
you can't just post an image you can

540
00:19:23,360 --> 00:19:25,440
imagine many of the

541
00:19:25,440 --> 00:19:27,679
black lives matters or the other

542
00:19:27,679 --> 00:19:30,160
protests that you see sometimes right

543
00:19:30,160 --> 00:19:33,120
pictures that captured scenes that that

544
00:19:33,120 --> 00:19:34,640
we

545
00:19:34,640 --> 00:19:38,080
saw so maybe in some of the situations

546
00:19:38,080 --> 00:19:39,919
the folks who took it took the picture

547
00:19:39,919 --> 00:19:41,600
men don't want to be known that they

548
00:19:41,600 --> 00:19:43,679
took the picture for whatever

549
00:19:43,679 --> 00:19:45,760
uh for fear of reprisal or something of

550
00:19:45,760 --> 00:19:47,200
that sort right

551
00:19:47,200 --> 00:19:49,760
and but the presence of that fingerprint

552
00:19:49,760 --> 00:19:51,600
is a problem because it can actually end

553
00:19:51,600 --> 00:19:52,960
up revealing

554
00:19:52,960 --> 00:19:55,679
as to who took that picture in some

555
00:19:55,679 --> 00:19:57,520
situations that may not be

556
00:19:57,520 --> 00:20:00,799
ideal right also it's not very

557
00:20:00,799 --> 00:20:04,240
uh reliable if the image is dark

558
00:20:04,240 --> 00:20:06,240
it's more are textured it's more

559
00:20:06,240 --> 00:20:07,919
reliable in flat and bright

560
00:20:07,919 --> 00:20:10,640
parts of the image and and the precision

561
00:20:10,640 --> 00:20:12,320
of the localization that i showed you

562
00:20:12,320 --> 00:20:12,880
earlier

563
00:20:12,880 --> 00:20:15,360
is is kind of limited right the objects

564
00:20:15,360 --> 00:20:17,600
in that picture that we

565
00:20:17,600 --> 00:20:21,600
detected as manipulated were quite large

566
00:20:21,600 --> 00:20:23,520
right and perhaps one of the most

567
00:20:23,520 --> 00:20:25,840
problematic aspects of it it's that

568
00:20:25,840 --> 00:20:29,039
it is of limited robustness as you start

569
00:20:29,039 --> 00:20:31,200
compressing the image as you start

570
00:20:31,200 --> 00:20:33,120
scaling cropping the image and doing

571
00:20:33,120 --> 00:20:35,039
other transformations to it

572
00:20:35,039 --> 00:20:38,799
its accuracy starts dropping although

573
00:20:38,799 --> 00:20:40,320
it's it's better than many other

574
00:20:40,320 --> 00:20:42,960
forensics techniques but it it does stop

575
00:20:42,960 --> 00:20:44,320
tracking so there are

576
00:20:44,320 --> 00:20:47,679
those are the problems with it

577
00:20:47,679 --> 00:20:51,280
so uh and and and also

578
00:20:51,280 --> 00:20:52,960
it's not very scalable because a

579
00:20:52,960 --> 00:20:54,720
fingerprint can be very large

580
00:20:54,720 --> 00:20:57,679
right so that's one approach that has

581
00:20:57,679 --> 00:20:58,720
been successful

582
00:20:58,720 --> 00:21:02,480
and and even today people use it for

583
00:21:02,480 --> 00:21:05,039
image forensics analysis or video

584
00:21:05,039 --> 00:21:06,559
forensics analysis the

585
00:21:06,559 --> 00:21:09,600
paper is written literally every week on

586
00:21:09,600 --> 00:21:09,919
this

587
00:21:09,919 --> 00:21:12,960
still right there's an active area of

588
00:21:12,960 --> 00:21:14,880
research or improvements and things of

589
00:21:14,880 --> 00:21:16,000
that sort

590
00:21:16,000 --> 00:21:18,080
uh and then there are others which i'll

591
00:21:18,080 --> 00:21:20,080
quickly run through i apologize for them

592
00:21:20,080 --> 00:21:22,799
mindful of the time so one of the other

593
00:21:22,799 --> 00:21:23,840
things people look at

594
00:21:23,840 --> 00:21:26,880
is compression traces because when you

595
00:21:26,880 --> 00:21:28,320
manipulate an image you

596
00:21:28,320 --> 00:21:31,280
you you come you compress it again and

597
00:21:31,280 --> 00:21:32,880
the multiple compressions that take

598
00:21:32,880 --> 00:21:33,760
place

599
00:21:33,760 --> 00:21:36,159
and and so because of that certain parts

600
00:21:36,159 --> 00:21:37,919
of the image have different

601
00:21:37,919 --> 00:21:41,039
statistical characteristics uh as far as

602
00:21:41,039 --> 00:21:41,600
far as

603
00:21:41,600 --> 00:21:44,880
the jpeg sort of

604
00:21:44,880 --> 00:21:48,720
representation goes and based on that

605
00:21:48,720 --> 00:21:50,640
again you can

606
00:21:50,640 --> 00:21:53,520
detect a third way of detecting has been

607
00:21:53,520 --> 00:21:54,320
what we call

608
00:21:54,320 --> 00:21:58,400
copy move forgeries right so

609
00:21:58,400 --> 00:22:01,200
often it's done right where and these

610
00:22:01,200 --> 00:22:02,480
are some

611
00:22:02,480 --> 00:22:05,760
well publicized sort of images

612
00:22:05,760 --> 00:22:08,799
from the past where it was

613
00:22:08,799 --> 00:22:11,600
somebody manipulated an image to show

614
00:22:11,600 --> 00:22:12,000
more

615
00:22:12,000 --> 00:22:14,240
than what actually happened by simply

616
00:22:14,240 --> 00:22:15,600
copy pasting

617
00:22:15,600 --> 00:22:18,320
some uh some of the some parts of the

618
00:22:18,320 --> 00:22:19,039
scene to

619
00:22:19,039 --> 00:22:22,320
to another and and copy pasting also

620
00:22:22,320 --> 00:22:22,960
we've

621
00:22:22,960 --> 00:22:25,360
we've been reasonably good at detecting

622
00:22:25,360 --> 00:22:26,159
it because

623
00:22:26,159 --> 00:22:29,120
basically intuitively you uh for the

624
00:22:29,120 --> 00:22:30,480
computer scientist

625
00:22:30,480 --> 00:22:31,840
you do a comparison you do a

626
00:22:31,840 --> 00:22:33,520
cross-correlation if you will you want

627
00:22:33,520 --> 00:22:35,760
to be more efficient than that

628
00:22:35,760 --> 00:22:38,240
and you find areas that are very very

629
00:22:38,240 --> 00:22:39,679
match very well and

630
00:22:39,679 --> 00:22:41,679
and you say hey that's a problem right

631
00:22:41,679 --> 00:22:43,120
you can

632
00:22:43,120 --> 00:22:46,000
detect them and then finally them these

633
00:22:46,000 --> 00:22:47,520
days especially with machine learning

634
00:22:47,520 --> 00:22:48,400
techniques

635
00:22:48,400 --> 00:22:50,559
there are lots of approaches being used

636
00:22:50,559 --> 00:22:52,320
whereby you actually look at the

637
00:22:52,320 --> 00:22:54,000
image residuals after doing some

638
00:22:54,000 --> 00:22:57,200
filtering and many of those

639
00:22:57,200 --> 00:23:00,080
manipulation operations like compression

640
00:23:00,080 --> 00:23:01,440
or pasting or scaling

641
00:23:01,440 --> 00:23:03,919
or cropping or brightness etc all the

642
00:23:03,919 --> 00:23:06,480
adjustments that a manipulator has to do

643
00:23:06,480 --> 00:23:09,840
will result in kind of a perturbation of

644
00:23:09,840 --> 00:23:11,280
these residuals

645
00:23:11,280 --> 00:23:14,159
they will not be consistent and these

646
00:23:14,159 --> 00:23:15,200
days with

647
00:23:15,200 --> 00:23:16,880
in the past we were trying to develop

648
00:23:16,880 --> 00:23:19,840
models to detect them but these days

649
00:23:19,840 --> 00:23:21,120
these powerful machine learning

650
00:23:21,120 --> 00:23:24,400
detectors can actually pull them out

651
00:23:24,400 --> 00:23:28,000
pretty quickly right so examples for

652
00:23:28,000 --> 00:23:29,600
like this this part of the image has

653
00:23:29,600 --> 00:23:32,240
been cropped it has a different residual

654
00:23:32,240 --> 00:23:36,240
sort of pattern as opposed to the rest

655
00:23:36,240 --> 00:23:39,520
right and so this has been done but the

656
00:23:39,520 --> 00:23:41,440
problems of course are as i said before

657
00:23:41,440 --> 00:23:42,799
when i was talking about we are

658
00:23:42,799 --> 00:23:44,880
new they are sensitive to post

659
00:23:44,880 --> 00:23:46,240
processing

660
00:23:46,240 --> 00:23:49,679
uh they have been pretty bad quality for

661
00:23:49,679 --> 00:23:51,520
web content especially and that's where

662
00:23:51,520 --> 00:23:52,559
sometimes

663
00:23:52,559 --> 00:23:55,360
the images of interest are sitting right

664
00:23:55,360 --> 00:23:56,559
we want to

665
00:23:56,559 --> 00:23:59,440
uh we can and which we want to analyze

666
00:23:59,440 --> 00:24:00,159
right

667
00:24:00,159 --> 00:24:03,279
uh and and again by with someone who

668
00:24:03,279 --> 00:24:04,720
knows what they're doing

669
00:24:04,720 --> 00:24:06,960
can actually simply launder and do some

670
00:24:06,960 --> 00:24:07,760
filtering and

671
00:24:07,760 --> 00:24:09,919
and make many of these forensics

672
00:24:09,919 --> 00:24:11,120
techniques uh

673
00:24:11,120 --> 00:24:14,240
sort of less reliable right so this what

674
00:24:14,240 --> 00:24:17,039
we call counter forensics right is

675
00:24:17,039 --> 00:24:20,400
is is not that difficult right

676
00:24:20,400 --> 00:24:22,640
and for those of you again who are

677
00:24:22,640 --> 00:24:24,720
security minded i run into this

678
00:24:24,720 --> 00:24:27,200
question a lot saying hey i can defeat

679
00:24:27,200 --> 00:24:28,960
this i can defeat this so what's the

680
00:24:28,960 --> 00:24:30,080
point

681
00:24:30,080 --> 00:24:32,640
right forensics is a little bit

682
00:24:32,640 --> 00:24:33,440
different

683
00:24:33,440 --> 00:24:37,440
forensics is after the fact in forensics

684
00:24:37,440 --> 00:24:41,039
you are relying on the adversary

685
00:24:41,039 --> 00:24:44,320
to make some mistake right

686
00:24:44,320 --> 00:24:47,440
so you're putting the burden on

687
00:24:47,440 --> 00:24:51,520
not making the mistake on the adversary

688
00:24:51,520 --> 00:24:53,360
when you're trying to secure a system

689
00:24:53,360 --> 00:24:55,520
the burden actually unfortunately

690
00:24:55,520 --> 00:24:57,919
many of the ways many architectures many

691
00:24:57,919 --> 00:24:58,640
ways we have

692
00:24:58,640 --> 00:25:01,039
protecting systems sometimes the burden

693
00:25:01,039 --> 00:25:02,240
falls on us

694
00:25:02,240 --> 00:25:03,840
we have to make sure that there's no

695
00:25:03,840 --> 00:25:05,520
hole there's no there's no

696
00:25:05,520 --> 00:25:08,000
sort of weakness in in our security

697
00:25:08,000 --> 00:25:09,840
posture because all the

698
00:25:09,840 --> 00:25:12,720
attacker has to do is find one weakness

699
00:25:12,720 --> 00:25:13,200
right

700
00:25:13,200 --> 00:25:15,279
and they've compromised the system

701
00:25:15,279 --> 00:25:16,799
security is strongest was the weakest

702
00:25:16,799 --> 00:25:18,240
link or whatever

703
00:25:18,240 --> 00:25:20,480
forensics things flip around a bit in

704
00:25:20,480 --> 00:25:21,440
the sense that

705
00:25:21,440 --> 00:25:23,679
now we are the ones who are searching

706
00:25:23,679 --> 00:25:25,679
right we are the ones looking for traces

707
00:25:25,679 --> 00:25:26,640
of mistakes

708
00:25:26,640 --> 00:25:28,799
right we look and and if one thing

709
00:25:28,799 --> 00:25:30,400
doesn't work you try another you try

710
00:25:30,400 --> 00:25:32,000
another you try another and

711
00:25:32,000 --> 00:25:35,360
hopefully you'll find the mistake that

712
00:25:35,360 --> 00:25:38,480
the perpetrator actually made that will

713
00:25:38,480 --> 00:25:40,799
allow us then to give insight as to

714
00:25:40,799 --> 00:25:43,520
who what why etc the kind of questions

715
00:25:43,520 --> 00:25:45,520
that you're looking to answer

716
00:25:45,520 --> 00:25:48,640
in in forensics so just trying to give

717
00:25:48,640 --> 00:25:49,919
pushback to some of the very

718
00:25:49,919 --> 00:25:51,919
security-minded people saying hey

719
00:25:51,919 --> 00:25:53,760
yes there is counter forensics but that

720
00:25:53,760 --> 00:25:55,120
doesn't obviate

721
00:25:55,120 --> 00:25:56,880
the need for forensics techniques that

722
00:25:56,880 --> 00:25:58,159
doesn't invalidate

723
00:25:58,159 --> 00:25:59,520
forensics techniques they're still

724
00:25:59,520 --> 00:26:01,840
needed right

725
00:26:01,840 --> 00:26:04,960
and so so that

726
00:26:04,960 --> 00:26:06,640
sort of gives you a broad introduction

727
00:26:06,640 --> 00:26:08,240
it's almost half my talk right

728
00:26:08,240 --> 00:26:11,120
about about this whole field right and

729
00:26:11,120 --> 00:26:11,760
uh

730
00:26:11,760 --> 00:26:13,679
what has happened say but in the last

731
00:26:13,679 --> 00:26:15,919
two three four years five years

732
00:26:15,919 --> 00:26:18,400
right things have moved on at a very

733
00:26:18,400 --> 00:26:21,360
dramatic pace and it's all come from

734
00:26:21,360 --> 00:26:23,919
our ability to develop these these

735
00:26:23,919 --> 00:26:25,440
neural models and machine learning

736
00:26:25,440 --> 00:26:27,279
techniques right that have

737
00:26:27,279 --> 00:26:29,200
allowed us to do what we call

738
00:26:29,200 --> 00:26:30,400
computational

739
00:26:30,400 --> 00:26:32,560
photography and and

740
00:26:32,560 --> 00:26:33,760
[Music]

741
00:26:33,760 --> 00:26:36,320
we can now so essentially the camera

742
00:26:36,320 --> 00:26:38,400
pipeline that i showed you earlier

743
00:26:38,400 --> 00:26:41,679
uh slowly is being replaced by

744
00:26:41,679 --> 00:26:44,320
all kinds of neural models neural

745
00:26:44,320 --> 00:26:46,400
pipelines machine learning models inside

746
00:26:46,400 --> 00:26:47,840
the camera

747
00:26:47,840 --> 00:26:50,320
that can do amazing things right so all

748
00:26:50,320 --> 00:26:53,520
these hdr and things of that sort

749
00:26:53,520 --> 00:26:56,400
we can we can take an image and really

750
00:26:56,400 --> 00:26:56,880
uh

751
00:26:56,880 --> 00:27:00,880
create a a very realistic looking scene

752
00:27:00,880 --> 00:27:01,440
from it

753
00:27:01,440 --> 00:27:03,679
close to what might have originally

754
00:27:03,679 --> 00:27:07,039
might have originally hit the camera

755
00:27:07,039 --> 00:27:10,240
and we can produce images that are much

756
00:27:10,240 --> 00:27:12,080
better quality by using machine learning

757
00:27:12,080 --> 00:27:12,720
techniques

758
00:27:12,720 --> 00:27:16,159
right uh and and in fact i was talking

759
00:27:16,159 --> 00:27:17,039
to

760
00:27:17,039 --> 00:27:21,200
payment millenfar in one of our meetings

761
00:27:21,200 --> 00:27:24,320
forensics meetings ed was there as well

762
00:27:24,320 --> 00:27:27,200
and he was talking about how much of

763
00:27:27,200 --> 00:27:28,159
these things they do

764
00:27:28,159 --> 00:27:30,480
they actually take eight images for

765
00:27:30,480 --> 00:27:31,919
every single image

766
00:27:31,919 --> 00:27:33,919
right and they actually expect the kind

767
00:27:33,919 --> 00:27:35,440
to be shaking a bit

768
00:27:35,440 --> 00:27:37,039
because they get now eight different

769
00:27:37,039 --> 00:27:38,720
samplings of the scene

770
00:27:38,720 --> 00:27:40,320
and then they combine them together to

771
00:27:40,320 --> 00:27:42,559
get the final high quality picture right

772
00:27:42,559 --> 00:27:44,159
and the funny thing was that even if

773
00:27:44,159 --> 00:27:45,279
your hand is not shaking they

774
00:27:45,279 --> 00:27:47,120
deliberately shake the sensor a bit

775
00:27:47,120 --> 00:27:49,919
because they want to sample uh the image

776
00:27:49,919 --> 00:27:51,039
to get the higher

777
00:27:51,039 --> 00:27:53,679
resolution right and and he was saying

778
00:27:53,679 --> 00:27:55,440
that

779
00:27:55,440 --> 00:27:57,760
in five years he thinks that maybe a lot

780
00:27:57,760 --> 00:27:58,559
of that

781
00:27:58,559 --> 00:28:00,399
processing that takes place a lot of the

782
00:28:00,399 --> 00:28:02,240
components will be replaced by a machine

783
00:28:02,240 --> 00:28:03,200
learning model

784
00:28:03,200 --> 00:28:06,480
right within the camera so

785
00:28:06,480 --> 00:28:10,000
and and these things kind of make our

786
00:28:10,000 --> 00:28:12,720
assumptions about forensics techniques

787
00:28:12,720 --> 00:28:14,720
artifacts etc

788
00:28:14,720 --> 00:28:16,480
will make us revisit our assumptions

789
00:28:16,480 --> 00:28:18,240
we'll make us revisit our models we'll

790
00:28:18,240 --> 00:28:18,880
make us

791
00:28:18,880 --> 00:28:21,520
we'll make things hard harder for us

792
00:28:21,520 --> 00:28:22,880
good news is if you're a graduate

793
00:28:22,880 --> 00:28:23,679
student

794
00:28:23,679 --> 00:28:25,279
uh there are lots of problems that will

795
00:28:25,279 --> 00:28:26,880
come up and and you're you'll be in

796
00:28:26,880 --> 00:28:29,679
business for a long time

797
00:28:29,679 --> 00:28:31,679
and and the earlier prn you would start

798
00:28:31,679 --> 00:28:33,600
that i started talking about starts

799
00:28:33,600 --> 00:28:34,320
breaking

800
00:28:34,320 --> 00:28:36,880
right you you don't see that prn you

801
00:28:36,880 --> 00:28:37,600
quite well

802
00:28:37,600 --> 00:28:41,200
uh anymore when there's a lot of

803
00:28:41,200 --> 00:28:43,840
computation happening inside the camera

804
00:28:43,840 --> 00:28:44,640
itself

805
00:28:44,640 --> 00:28:47,760
before the final image is generated

806
00:28:47,760 --> 00:28:52,159
so so the idea then is that

807
00:28:52,159 --> 00:28:54,960
well perhaps this ability to compute

808
00:28:54,960 --> 00:28:56,240
inside the camera

809
00:28:56,240 --> 00:28:58,640
right and have neural models inside the

810
00:28:58,640 --> 00:29:01,200
camera perhaps we can leverage that

811
00:29:01,200 --> 00:29:04,960
to enable or aid forensics as well

812
00:29:04,960 --> 00:29:07,679
right so about two years back we started

813
00:29:07,679 --> 00:29:08,799
with that assumption

814
00:29:08,799 --> 00:29:12,080
and looked at what can be done a couple

815
00:29:12,080 --> 00:29:13,679
of things which i'll talk to you briefly

816
00:29:13,679 --> 00:29:14,159
about

817
00:29:14,159 --> 00:29:15,840
and then finally talk to you about

818
00:29:15,840 --> 00:29:17,360
something that we are currently working

819
00:29:17,360 --> 00:29:18,399
on

820
00:29:18,399 --> 00:29:20,720
which i think has some potential right

821
00:29:20,720 --> 00:29:21,679
and again

822
00:29:21,679 --> 00:29:23,679
it's the problem is not solved but i

823
00:29:23,679 --> 00:29:26,720
think has some potential

824
00:29:26,720 --> 00:29:29,840
so and and for those of you who might be

825
00:29:29,840 --> 00:29:31,039
interested

826
00:29:31,039 --> 00:29:33,600
in in sort of working in this area we

827
00:29:33,600 --> 00:29:34,720
have an open source

828
00:29:34,720 --> 00:29:39,279
open source toolbox right that

829
00:29:39,279 --> 00:29:42,399
will that allows you to simulate a

830
00:29:42,399 --> 00:29:45,039
neural imaging pipeline and optimize it

831
00:29:45,039 --> 00:29:46,000
for different things

832
00:29:46,000 --> 00:29:50,640
right so uh so that

833
00:29:50,640 --> 00:29:52,720
allows you to experiment and simulate

834
00:29:52,720 --> 00:29:53,919
and

835
00:29:53,919 --> 00:29:56,320
test out different techniques and we

836
00:29:56,320 --> 00:29:57,919
have all that available

837
00:29:57,919 --> 00:30:01,279
as an open source toolbox

838
00:30:01,600 --> 00:30:03,919
and i think there's a pointer to it at

839
00:30:03,919 --> 00:30:05,760
the end

840
00:30:05,760 --> 00:30:09,600
so so

841
00:30:09,600 --> 00:30:11,520
so the idea for one of the first ideas

842
00:30:11,520 --> 00:30:13,760
we we looked at is that okay

843
00:30:13,760 --> 00:30:16,640
suppose that the camera pipeline is

844
00:30:16,640 --> 00:30:18,399
replaced by a neural imaging pipeline

845
00:30:18,399 --> 00:30:20,559
it's just

846
00:30:20,559 --> 00:30:23,919
light in through the sensors the neural

847
00:30:23,919 --> 00:30:26,080
neural network and boom the final image

848
00:30:26,080 --> 00:30:28,320
i'm simplifying a bit but suppose

849
00:30:28,320 --> 00:30:31,440
we have that kind of architecture within

850
00:30:31,440 --> 00:30:33,200
the camera

851
00:30:33,200 --> 00:30:36,960
then can we optimize this neural network

852
00:30:36,960 --> 00:30:38,240
that takes the raw

853
00:30:38,240 --> 00:30:41,039
pixel data right the sense data the

854
00:30:41,039 --> 00:30:42,960
intensity values and finally produce

855
00:30:42,960 --> 00:30:43,440
that

856
00:30:43,440 --> 00:30:47,360
color image can we optimize that so that

857
00:30:47,360 --> 00:30:50,640
the image that is produced kind of has

858
00:30:50,640 --> 00:30:52,240
these artifacts in them

859
00:30:52,240 --> 00:30:54,159
right the traces that i talked about

860
00:30:54,159 --> 00:30:56,399
earlier right which were accidental

861
00:30:56,399 --> 00:30:58,960
in the earlier case okay we put in

862
00:30:58,960 --> 00:31:00,960
filtering and we put brightness and we

863
00:31:00,960 --> 00:31:01,360
put

864
00:31:01,360 --> 00:31:03,840
color demosaicing and all that stuff and

865
00:31:03,840 --> 00:31:05,279
whatever artifacts were there were

866
00:31:05,279 --> 00:31:06,240
accidental

867
00:31:06,240 --> 00:31:09,039
but can we now design artifacts and put

868
00:31:09,039 --> 00:31:10,559
them put in them

869
00:31:10,559 --> 00:31:13,120
that will improve our ability to do

870
00:31:13,120 --> 00:31:14,880
forensics analysis

871
00:31:14,880 --> 00:31:17,440
so we looked at like a pipeline of this

872
00:31:17,440 --> 00:31:19,120
form we have this new rule and then we

873
00:31:19,120 --> 00:31:20,000
have

874
00:31:20,000 --> 00:31:23,279
there is a reasonably and again

875
00:31:23,279 --> 00:31:26,480
available in the open source i think a

876
00:31:26,480 --> 00:31:28,320
forensics analysis network a neutral

877
00:31:28,320 --> 00:31:30,240
model that allows you to

878
00:31:30,240 --> 00:31:31,840
detect whether an image has been

879
00:31:31,840 --> 00:31:34,000
manipulated or not with some accuracy it

880
00:31:34,000 --> 00:31:36,000
has its limitations but still

881
00:31:36,000 --> 00:31:38,799
it's the best we have out there i think

882
00:31:38,799 --> 00:31:39,120
well

883
00:31:39,120 --> 00:31:41,600
at least for maybe things may have

884
00:31:41,600 --> 00:31:43,440
changed in a year or so but

885
00:31:43,440 --> 00:31:44,960
at least a couple of years or one year

886
00:31:44,960 --> 00:31:47,440
back it was the best we had

887
00:31:47,440 --> 00:31:50,640
and and then can we sort of

888
00:31:50,640 --> 00:31:53,039
train this neural model to produce

889
00:31:53,039 --> 00:31:54,240
images that

890
00:31:54,240 --> 00:31:56,559
work better with our forensics analysis

891
00:31:56,559 --> 00:31:57,279
right

892
00:31:57,279 --> 00:32:00,640
intuitively speaking that's what we were

893
00:32:00,640 --> 00:32:04,000
trying to do right and we showed again

894
00:32:04,000 --> 00:32:06,000
without getting into gory details we

895
00:32:06,000 --> 00:32:07,200
showed that yes

896
00:32:07,200 --> 00:32:09,760
right we can actually train the neural

897
00:32:09,760 --> 00:32:10,480
image model

898
00:32:10,480 --> 00:32:14,159
so that operations like sharpening

899
00:32:14,159 --> 00:32:16,159
gaussian filtering jpeg compression

900
00:32:16,159 --> 00:32:18,080
resampling etc

901
00:32:18,080 --> 00:32:20,799
we're detected with much higher accuracy

902
00:32:20,799 --> 00:32:22,720
more than double the accuracy

903
00:32:22,720 --> 00:32:25,360
as opposed to what was done with can

904
00:32:25,360 --> 00:32:27,360
right at a particular compression

905
00:32:27,360 --> 00:32:30,559
ratio right so so

906
00:32:30,559 --> 00:32:32,320
it can be done so that that gives us

907
00:32:32,320 --> 00:32:33,600
some kind of

908
00:32:33,600 --> 00:32:37,120
hope and and the motivation behind this

909
00:32:37,120 --> 00:32:37,679
kind of an

910
00:32:37,679 --> 00:32:39,279
approach and this kind of thinking was

911
00:32:39,279 --> 00:32:40,799
that you know

912
00:32:40,799 --> 00:32:42,880
we need to do something proactive

913
00:32:42,880 --> 00:32:43,570
detection

914
00:32:43,570 --> 00:32:44,640
[Music]

915
00:32:44,640 --> 00:32:47,039
is good detection is required dissection

916
00:32:47,039 --> 00:32:47,919
is is

917
00:32:47,919 --> 00:32:51,519
always has to be done uh but for certain

918
00:32:51,519 --> 00:32:53,919
type of content perhaps we want to be

919
00:32:53,919 --> 00:32:54,880
proactive

920
00:32:54,880 --> 00:32:56,880
right perhaps you want to put in

921
00:32:56,880 --> 00:32:58,799
artifacts in there that will help us

922
00:32:58,799 --> 00:33:01,200
detect so for example when we make a

923
00:33:01,200 --> 00:33:02,960
hundred dollar bill we just don't make a

924
00:33:02,960 --> 00:33:04,799
pretty picture with the president's face

925
00:33:04,799 --> 00:33:05,679
on it

926
00:33:05,679 --> 00:33:08,480
right we actually put in artifacts in

927
00:33:08,480 --> 00:33:08,880
there

928
00:33:08,880 --> 00:33:10,960
that will detect counterfeiting right

929
00:33:10,960 --> 00:33:12,559
with our knowledge of counterfeiting

930
00:33:12,559 --> 00:33:14,159
with the limitations of

931
00:33:14,159 --> 00:33:17,039
what counterfeiters can do we based on

932
00:33:17,039 --> 00:33:19,360
that we put in some artifacts in there

933
00:33:19,360 --> 00:33:22,080
which will help us detect counterfeits

934
00:33:22,080 --> 00:33:22,640
it's the

935
00:33:22,640 --> 00:33:23,919
same kind of thinking we're thinking

936
00:33:23,919 --> 00:33:25,919
okay what can we do that will help us

937
00:33:25,919 --> 00:33:29,519
detect right and again detection is not

938
00:33:29,519 --> 00:33:33,600
always 100 guaranteed but uh

939
00:33:33,600 --> 00:33:35,440
what can we do i mean how far can we get

940
00:33:35,440 --> 00:33:37,200
with this right and we showed that we

941
00:33:37,200 --> 00:33:38,480
could

942
00:33:38,480 --> 00:33:40,159
at least in this particular restricted

943
00:33:40,159 --> 00:33:41,519
simplified model

944
00:33:41,519 --> 00:33:45,360
right that the approach can potentially

945
00:33:45,360 --> 00:33:48,559
work right the

946
00:33:48,559 --> 00:33:52,000
second thing is then we looked at was

947
00:33:52,000 --> 00:33:55,039
okay um we can change the neural image

948
00:33:55,039 --> 00:33:56,720
pipeline but for that i need to convince

949
00:33:56,720 --> 00:33:58,159
sony to do that

950
00:33:58,159 --> 00:34:01,600
or apple to do that right uh can we do

951
00:34:01,600 --> 00:34:02,640
it outside that

952
00:34:02,640 --> 00:34:04,159
of course even there you could after the

953
00:34:04,159 --> 00:34:05,600
image is captured you can put in an

954
00:34:05,600 --> 00:34:07,120
artifact

955
00:34:07,120 --> 00:34:09,760
but but can we do that say with a social

956
00:34:09,760 --> 00:34:10,719
media platform

957
00:34:10,719 --> 00:34:12,639
when they when you're uploading an image

958
00:34:12,639 --> 00:34:13,839
to the platform

959
00:34:13,839 --> 00:34:16,159
they compress it and that can that

960
00:34:16,159 --> 00:34:18,079
compression technique be modified so

961
00:34:18,079 --> 00:34:18,879
that

962
00:34:18,879 --> 00:34:22,000
the resulting decompressed image

963
00:34:22,000 --> 00:34:25,918
is again forensics friendly if you will

964
00:34:25,918 --> 00:34:28,960
right because again what is happening is

965
00:34:28,960 --> 00:34:31,119
that even in the compression world

966
00:34:31,119 --> 00:34:35,040
uh a lot of the our vanilla

967
00:34:35,040 --> 00:34:36,639
compression techniques our traditional

968
00:34:36,639 --> 00:34:38,159
compression techniques

969
00:34:38,159 --> 00:34:41,359
are now we are developing our ex it's

970
00:34:41,359 --> 00:34:42,960
expected that perhaps they might be

971
00:34:42,960 --> 00:34:44,239
replaced by

972
00:34:44,239 --> 00:34:46,639
uh neural network-based compression

973
00:34:46,639 --> 00:34:48,320
techniques right

974
00:34:48,320 --> 00:34:51,599
and and uh so can we design

975
00:34:51,599 --> 00:34:53,520
a technique right that that actually

976
00:34:53,520 --> 00:34:54,879
neural network based compression

977
00:34:54,879 --> 00:34:55,918
technique so

978
00:34:55,918 --> 00:34:58,320
raw imaging neural network compressed

979
00:34:58,320 --> 00:34:59,040
image out

980
00:34:59,040 --> 00:35:01,280
right then neural network decompressed

981
00:35:01,280 --> 00:35:02,400
remedy

982
00:35:02,400 --> 00:35:06,240
so can we can we do that again

983
00:35:06,240 --> 00:35:09,359
simplifying things and and we showed

984
00:35:09,359 --> 00:35:10,079
that

985
00:35:10,079 --> 00:35:12,480
that can be done as well right we can

986
00:35:12,480 --> 00:35:15,839
design compression techniques

987
00:35:15,839 --> 00:35:18,400
give better performance than some of the

988
00:35:18,400 --> 00:35:20,720
other neural techniques out there

989
00:35:20,720 --> 00:35:22,880
and better performance than things like

990
00:35:22,880 --> 00:35:24,880
jpeg etc

991
00:35:24,880 --> 00:35:26,480
in terms of image quality and

992
00:35:26,480 --> 00:35:29,680
compression ratio right

993
00:35:29,680 --> 00:35:33,359
and also at the same time provide better

994
00:35:33,359 --> 00:35:36,480
uh forensics analysis uh capabilities so

995
00:35:36,480 --> 00:35:38,480
we can we can do that i mean again it's

996
00:35:38,480 --> 00:35:39,680
just a

997
00:35:39,680 --> 00:35:42,240
demonstration of a simulated in a

998
00:35:42,240 --> 00:35:45,200
simulated setting uh

999
00:35:45,200 --> 00:35:48,880
that there is some hope here as well

1000
00:35:48,880 --> 00:35:51,920
right we can even compress things with

1001
00:35:51,920 --> 00:35:55,040
forensics and mind and and

1002
00:35:55,040 --> 00:35:58,240
and again the the quality i won't go

1003
00:35:58,240 --> 00:35:59,680
into details but

1004
00:35:59,680 --> 00:36:03,920
believe me uh the quality is as good as

1005
00:36:03,920 --> 00:36:06,160
anything else and the bitrate also so it

1006
00:36:06,160 --> 00:36:07,200
compares

1007
00:36:07,200 --> 00:36:12,400
favorably in in terms of that single to

1008
00:36:12,400 --> 00:36:14,320
noise ratio and the amounts of bits that

1009
00:36:14,320 --> 00:36:15,119
you

1010
00:36:15,119 --> 00:36:18,160
that you spend right uh for more details

1011
00:36:18,160 --> 00:36:21,598
you can you can look at the paper

1012
00:36:22,079 --> 00:36:24,480
so we get pretty good rate distortion

1013
00:36:24,480 --> 00:36:27,839
trade-offs right compared to

1014
00:36:28,320 --> 00:36:31,520
and so let me just run through that

1015
00:36:31,520 --> 00:36:33,839
and the third approach we we started

1016
00:36:33,839 --> 00:36:35,440
taking

1017
00:36:35,440 --> 00:36:36,960
about a year back it's been a

1018
00:36:36,960 --> 00:36:38,720
challenging journey and i think we're

1019
00:36:38,720 --> 00:36:41,200
making some progress here

1020
00:36:41,200 --> 00:36:44,078
uh is

1021
00:36:44,560 --> 00:36:47,599
like saying okay so it is expected that

1022
00:36:47,599 --> 00:36:49,839
not just the imaging pipeline but now we

1023
00:36:49,839 --> 00:36:50,480
are also

1024
00:36:50,480 --> 00:36:53,839
have the capability to have a neural

1025
00:36:53,839 --> 00:36:54,400
network

1026
00:36:54,400 --> 00:36:58,720
in the sensor itself so the sensor that

1027
00:36:58,720 --> 00:37:01,119
light falls upon right and from which

1028
00:37:01,119 --> 00:37:03,040
the intensity of light is measured right

1029
00:37:03,040 --> 00:37:04,320
that sensing

1030
00:37:04,320 --> 00:37:07,440
element right pixel element

1031
00:37:07,440 --> 00:37:09,440
you can actually run a neural network on

1032
00:37:09,440 --> 00:37:11,280
that element itself

1033
00:37:11,280 --> 00:37:14,240
right a simple simple neural network and

1034
00:37:14,240 --> 00:37:16,320
so now we are right there at the time of

1035
00:37:16,320 --> 00:37:17,520
image capture

1036
00:37:17,520 --> 00:37:20,000
right when the image is being converted

1037
00:37:20,000 --> 00:37:20,839
from light

1038
00:37:20,839 --> 00:37:23,920
to uh digital form

1039
00:37:23,920 --> 00:37:27,280
right from optical to digital form uh

1040
00:37:27,280 --> 00:37:30,400
at that time itself can we insert some

1041
00:37:30,400 --> 00:37:31,440
fingerprint

1042
00:37:31,440 --> 00:37:34,720
can we insert something in it that will

1043
00:37:34,720 --> 00:37:37,440
allow us to detect that later on if that

1044
00:37:37,440 --> 00:37:39,760
image gets manipulated after that

1045
00:37:39,760 --> 00:37:42,880
right so and and

1046
00:37:42,880 --> 00:37:45,280
and such sensors are now sort of coming

1047
00:37:45,280 --> 00:37:46,720
up right people are are

1048
00:37:46,720 --> 00:37:49,920
talking about that and and and

1049
00:37:49,920 --> 00:37:51,599
why would we want to do that because the

1050
00:37:51,599 --> 00:37:53,440
prnu which was such a nice sense of

1051
00:37:53,440 --> 00:37:54,480
fingerprint

1052
00:37:54,480 --> 00:37:56,640
the problem that it's just accidental it

1053
00:37:56,640 --> 00:37:58,480
just was something that we happen to

1054
00:37:58,480 --> 00:38:00,800
benefit from right it just happens so

1055
00:38:00,800 --> 00:38:01,680
that

1056
00:38:01,680 --> 00:38:03,440
physical sensors are actually all

1057
00:38:03,440 --> 00:38:04,880
slightly different right no

1058
00:38:04,880 --> 00:38:08,640
two physical things are exactly the same

1059
00:38:08,640 --> 00:38:12,640
it and and and uh but now we have the

1060
00:38:12,640 --> 00:38:16,000
uh if we can actually

1061
00:38:16,000 --> 00:38:19,359
design one right rather than inheriting

1062
00:38:19,359 --> 00:38:19,680
one

1063
00:38:19,680 --> 00:38:22,720
or simply using one design one that is

1064
00:38:22,720 --> 00:38:24,720
optimized for certain

1065
00:38:24,720 --> 00:38:27,520
purposes certain applications uh perhaps

1066
00:38:27,520 --> 00:38:28,400
that will result

1067
00:38:28,400 --> 00:38:31,599
result in a better fingerprint right and

1068
00:38:31,599 --> 00:38:32,720
we can get the same

1069
00:38:32,720 --> 00:38:34,560
so we call them computational sensor

1070
00:38:34,560 --> 00:38:36,400
fingerprints

1071
00:38:36,400 --> 00:38:38,320
right and if you recall the problem of

1072
00:38:38,320 --> 00:38:40,880
the sensor fingerprint was first of all

1073
00:38:40,880 --> 00:38:43,119
every image from the earlier pr new

1074
00:38:43,119 --> 00:38:44,880
sensor fingerprint was

1075
00:38:44,880 --> 00:38:46,640
every image from that camera has the

1076
00:38:46,640 --> 00:38:48,560
same fingerprint right

1077
00:38:48,560 --> 00:38:51,280
uh that's an issue for it can be spoofed

1078
00:38:51,280 --> 00:38:52,320
it can be copied

1079
00:38:52,320 --> 00:38:56,000
privacy issues are there etc etc so

1080
00:38:56,000 --> 00:38:59,280
so but if you use a neural model to kind

1081
00:38:59,280 --> 00:39:01,520
of design and insert the sensor at the

1082
00:39:01,520 --> 00:39:02,720
time of capture

1083
00:39:02,720 --> 00:39:05,119
right uh you can imagine that perhaps

1084
00:39:05,119 --> 00:39:06,640
there is this

1085
00:39:06,640 --> 00:39:09,680
raw image content x right uh

1086
00:39:09,680 --> 00:39:14,240
taken before colored in mosaic

1087
00:39:14,240 --> 00:39:16,800
and things like that and and then you

1088
00:39:16,800 --> 00:39:17,440
what you do

1089
00:39:17,440 --> 00:39:20,640
is based on a secret key

1090
00:39:20,640 --> 00:39:23,680
saying uh based on the context what time

1091
00:39:23,680 --> 00:39:24,960
what place whatever

1092
00:39:24,960 --> 00:39:28,400
might be time location content etc right

1093
00:39:28,400 --> 00:39:32,160
that that the image was taken you create

1094
00:39:32,160 --> 00:39:35,040
right based on that you create a

1095
00:39:35,040 --> 00:39:36,240
fingerprint

1096
00:39:36,240 --> 00:39:38,000
which is then embedded in the image

1097
00:39:38,000 --> 00:39:40,640
before it goes into the imaging pipeline

1098
00:39:40,640 --> 00:39:42,960
and out then comes the final image so

1099
00:39:42,960 --> 00:39:44,079
can we do it

1100
00:39:44,079 --> 00:39:46,400
right here itself and then the image

1101
00:39:46,400 --> 00:39:47,760
goes through the channel

1102
00:39:47,760 --> 00:39:50,079
right people will upload it to facebook

1103
00:39:50,079 --> 00:39:51,599
instagram whatever

1104
00:39:51,599 --> 00:39:55,359
change it do this do that and then it it

1105
00:39:55,359 --> 00:39:58,640
sort of uh now the question arises

1106
00:39:58,640 --> 00:40:00,320
whether this image is authentic

1107
00:40:00,320 --> 00:40:03,839
or not authentic right uh well

1108
00:40:03,839 --> 00:40:07,040
if i there is metadata that goes along

1109
00:40:07,040 --> 00:40:09,920
and assuming the metadata is is there

1110
00:40:09,920 --> 00:40:10,720
right like

1111
00:40:10,720 --> 00:40:14,160
available right based on and now we have

1112
00:40:14,160 --> 00:40:16,240
an authentication service say that's our

1113
00:40:16,240 --> 00:40:18,400
model right we have an authentication

1114
00:40:18,400 --> 00:40:20,319
service in a very abstract level the

1115
00:40:20,319 --> 00:40:21,760
model

1116
00:40:21,760 --> 00:40:24,880
which can use the same key

1117
00:40:24,880 --> 00:40:28,640
context based on that uh can detect

1118
00:40:28,640 --> 00:40:31,440
that hey this image is actually

1119
00:40:31,440 --> 00:40:32,800
authentic or not

1120
00:40:32,800 --> 00:40:37,200
right so so the advantages are that

1121
00:40:37,200 --> 00:40:40,560
every fingerprint on every image on

1122
00:40:40,560 --> 00:40:42,400
every camera is different

1123
00:40:42,400 --> 00:40:45,520
right so spoofing starts becoming more

1124
00:40:45,520 --> 00:40:46,880
difficult

1125
00:40:46,880 --> 00:40:50,240
and now we have the luxury if you will

1126
00:40:50,240 --> 00:40:52,240
or the ability we have the opportunity

1127
00:40:52,240 --> 00:40:54,640
to to optimize it for whatever purpose

1128
00:40:54,640 --> 00:40:56,000
we want to

1129
00:40:56,000 --> 00:40:58,319
so what do we want to optimize for one

1130
00:40:58,319 --> 00:40:59,839
is like detection

1131
00:40:59,839 --> 00:41:03,040
right and yes when you optimize for

1132
00:41:03,040 --> 00:41:04,319
detection

1133
00:41:04,319 --> 00:41:07,280
right we show that uh we this

1134
00:41:07,280 --> 00:41:09,520
computational sensor fingerprints

1135
00:41:09,520 --> 00:41:13,520
do much better than a regular prnu

1136
00:41:13,520 --> 00:41:15,920
right a typical pr new computational

1137
00:41:15,920 --> 00:41:18,079
single finger sensor we can design

1138
00:41:18,079 --> 00:41:20,000
computational sensor fingerprints that

1139
00:41:20,000 --> 00:41:21,440
will do much better and again it's

1140
00:41:21,440 --> 00:41:22,160
through a

1141
00:41:22,160 --> 00:41:24,079
sort of a neural pipeline right there's

1142
00:41:24,079 --> 00:41:26,560
a feedback and you you design the

1143
00:41:26,560 --> 00:41:29,599
computational fingerprint

1144
00:41:30,720 --> 00:41:34,000
and so

1145
00:41:34,000 --> 00:41:36,319
just just elaborating on that as you can

1146
00:41:36,319 --> 00:41:37,520
see there's always a

1147
00:41:37,520 --> 00:41:39,359
false positive false negative sort of an

1148
00:41:39,359 --> 00:41:40,720
overlap out there but

1149
00:41:40,720 --> 00:41:44,079
uh with prnu which could be high at

1150
00:41:44,079 --> 00:41:46,560
certain compression ratios even at

1151
00:41:46,560 --> 00:41:48,720
these higher compression ratios we can

1152
00:41:48,720 --> 00:41:52,240
get good separation between

1153
00:41:52,480 --> 00:41:55,200
the false image and when there is no

1154
00:41:55,200 --> 00:41:56,400
fingerprint and when there is a

1155
00:41:56,400 --> 00:41:58,000
fingerprint

1156
00:41:58,000 --> 00:42:01,040
and and the visual impact is is minimal

1157
00:42:01,040 --> 00:42:03,920
although the interesting thing is that

1158
00:42:03,920 --> 00:42:04,560
whereas

1159
00:42:04,560 --> 00:42:08,079
something like a typical

1160
00:42:08,079 --> 00:42:10,400
traditional sense of fingerprint was

1161
00:42:10,400 --> 00:42:11,520
more or less kind of

1162
00:42:11,520 --> 00:42:14,319
random noise in some sense right it

1163
00:42:14,319 --> 00:42:16,480
turns out that the

1164
00:42:16,480 --> 00:42:18,319
computational sensor from fingerprints

1165
00:42:18,319 --> 00:42:19,760
we design

1166
00:42:19,760 --> 00:42:22,480
seem to have certain structure in them

1167
00:42:22,480 --> 00:42:24,000
and

1168
00:42:24,000 --> 00:42:27,440
i don't know what this structure is

1169
00:42:27,440 --> 00:42:29,040
implying or

1170
00:42:29,040 --> 00:42:31,520
i i don't know the answer to that but

1171
00:42:31,520 --> 00:42:32,079
clearly

1172
00:42:32,079 --> 00:42:34,240
they seem to be different it's just not

1173
00:42:34,240 --> 00:42:35,440
a but it's a

1174
00:42:35,440 --> 00:42:36,880
this is the kind of design that the

1175
00:42:36,880 --> 00:42:38,720
neural network came up with

1176
00:42:38,720 --> 00:42:41,760
right and and the problem with all these

1177
00:42:41,760 --> 00:42:43,359
machine learning techniques is sometimes

1178
00:42:43,359 --> 00:42:44,720
is there a black box

1179
00:42:44,720 --> 00:42:47,200
you don't get an explanation of why how

1180
00:42:47,200 --> 00:42:48,560
and we don't have one

1181
00:42:48,560 --> 00:42:51,760
and i and i know purdue has such a

1182
00:42:51,760 --> 00:42:53,440
stellar single processing department

1183
00:42:53,440 --> 00:42:56,240
perhaps someone will be able to

1184
00:42:56,240 --> 00:42:59,359
use this uh to get better theoretical

1185
00:42:59,359 --> 00:43:01,280
insights into what might be going on or

1186
00:43:01,280 --> 00:43:03,520
what's perfect to do

1187
00:43:03,520 --> 00:43:06,160
so even there you do have this issue of

1188
00:43:06,160 --> 00:43:08,240
saying okay now sir you have this

1189
00:43:08,240 --> 00:43:09,920
computational single fingerprint you

1190
00:43:09,920 --> 00:43:11,680
took your image in the camera

1191
00:43:11,680 --> 00:43:13,680
you it went through right right at the

1192
00:43:13,680 --> 00:43:15,440
time of capture you put the fingerprint

1193
00:43:15,440 --> 00:43:16,000
and now

1194
00:43:16,000 --> 00:43:17,839
i have your image with that thing

1195
00:43:17,839 --> 00:43:19,760
computational sensor fingerprint

1196
00:43:19,760 --> 00:43:22,079
but i know it's there what i can do is i

1197
00:43:22,079 --> 00:43:23,040
can

1198
00:43:23,040 --> 00:43:24,960
extract it by filtering extract the

1199
00:43:24,960 --> 00:43:26,880
residual and put it

1200
00:43:26,880 --> 00:43:30,000
into some into another image

1201
00:43:30,000 --> 00:43:31,760
right i can put it into another image

1202
00:43:31,760 --> 00:43:33,839
and claim and then make that image look

1203
00:43:33,839 --> 00:43:35,680
authentic as if it was taken in that

1204
00:43:35,680 --> 00:43:37,200
same context

1205
00:43:37,200 --> 00:43:40,319
yes it can be done right and we've

1206
00:43:40,319 --> 00:43:42,240
studied this and i won't be going into

1207
00:43:42,240 --> 00:43:44,000
details but we've studied this under

1208
00:43:44,000 --> 00:43:45,760
various various

1209
00:43:45,760 --> 00:43:48,640
threat models right uh as to what does

1210
00:43:48,640 --> 00:43:49,760
the attacker know

1211
00:43:49,760 --> 00:43:52,480
right attacker has black box error has

1212
00:43:52,480 --> 00:43:54,560
unlimited access to the detector

1213
00:43:54,560 --> 00:43:57,920
uh attacker has white box access to the

1214
00:43:57,920 --> 00:43:58,880
detector

1215
00:43:58,880 --> 00:44:01,599
attack attacker has a white boss box

1216
00:44:01,599 --> 00:44:03,440
access to the encoder

1217
00:44:03,440 --> 00:44:05,599
uh depending on that we've sort of

1218
00:44:05,599 --> 00:44:06,720
looked at

1219
00:44:06,720 --> 00:44:09,280
what kind of success the attacker could

1220
00:44:09,280 --> 00:44:09,920
have

1221
00:44:09,920 --> 00:44:12,160
but the real most realistic model we

1222
00:44:12,160 --> 00:44:14,160
were focusing on is the

1223
00:44:14,160 --> 00:44:17,359
attacker simply has ability to query the

1224
00:44:17,359 --> 00:44:19,040
detector a good number of times

1225
00:44:19,040 --> 00:44:22,160
right before in order to design that

1226
00:44:22,160 --> 00:44:22,800
spoofed

1227
00:44:22,800 --> 00:44:27,280
image right so

1228
00:44:27,760 --> 00:44:31,599
so that can be addressed as well uh

1229
00:44:31,599 --> 00:44:35,359
more recently we've shown that if we

1230
00:44:35,359 --> 00:44:37,280
do adversarial training so in the

1231
00:44:37,280 --> 00:44:38,480
training itself

1232
00:44:38,480 --> 00:44:41,359
you have these spoofed images also as a

1233
00:44:41,359 --> 00:44:42,160
part of it

1234
00:44:42,160 --> 00:44:45,280
right and

1235
00:44:45,280 --> 00:44:48,160
train the detector to design uh to be

1236
00:44:48,160 --> 00:44:50,480
able to tell the difference apart

1237
00:44:50,480 --> 00:44:52,400
we are able to do that right we are able

1238
00:44:52,400 --> 00:44:54,560
to reduce the success

1239
00:44:54,560 --> 00:44:57,359
right depending on the distortion right

1240
00:44:57,359 --> 00:44:59,119
a lot of distortion

1241
00:44:59,119 --> 00:45:02,960
does cause still

1242
00:45:02,960 --> 00:45:04,640
creates problems for the for the

1243
00:45:04,640 --> 00:45:06,079
detector uh

1244
00:45:06,079 --> 00:45:09,119
but depending on on that we are able to

1245
00:45:09,119 --> 00:45:12,319
uh protect against uh

1246
00:45:12,319 --> 00:45:16,480
some simple spoofing attacks as well

1247
00:45:16,560 --> 00:45:19,200
and we can take that one step further

1248
00:45:19,200 --> 00:45:19,760
right

1249
00:45:19,760 --> 00:45:22,319
uh we can in order to again raise the

1250
00:45:22,319 --> 00:45:23,040
bar

1251
00:45:23,040 --> 00:45:27,119
on on spoofing of this kind uh we can

1252
00:45:27,119 --> 00:45:30,839
uh actually make

1253
00:45:30,839 --> 00:45:34,160
that fingerprint dependent on the

1254
00:45:34,160 --> 00:45:35,119
content

1255
00:45:35,119 --> 00:45:37,119
so for the computer scientists in the in

1256
00:45:37,119 --> 00:45:39,280
the audience you can say you know i can

1257
00:45:39,280 --> 00:45:40,160
hash

1258
00:45:40,160 --> 00:45:43,359
the image and base i'll create my

1259
00:45:43,359 --> 00:45:45,280
fingerprint based on the hash of the

1260
00:45:45,280 --> 00:45:47,839
image so later on when i try that around

1261
00:45:47,839 --> 00:45:49,359
the image the hash is clearly going to

1262
00:45:49,359 --> 00:45:50,400
be different

1263
00:45:50,400 --> 00:45:53,440
and and now you now you won't be able to

1264
00:45:53,440 --> 00:45:55,839
do this right

1265
00:45:55,839 --> 00:45:59,119
so given the properties of these hashing

1266
00:45:59,119 --> 00:46:00,800
techniques

1267
00:46:00,800 --> 00:46:02,880
sounds good sounds interesting but the

1268
00:46:02,880 --> 00:46:04,880
problem is that images are strange

1269
00:46:04,880 --> 00:46:08,079
animals they can't be hashed that easily

1270
00:46:08,079 --> 00:46:09,920
because what are you hashing are you

1271
00:46:09,920 --> 00:46:12,319
hashing the digital representation of

1272
00:46:12,319 --> 00:46:13,520
the image

1273
00:46:13,520 --> 00:46:15,839
are you hashing the content right are

1274
00:46:15,839 --> 00:46:17,520
you hashing them the features

1275
00:46:17,520 --> 00:46:19,760
right and and it turns out that if

1276
00:46:19,760 --> 00:46:20,880
you're just hashing the digital

1277
00:46:20,880 --> 00:46:21,920
representation

1278
00:46:21,920 --> 00:46:24,480
if i change one pixel value the image

1279
00:46:24,480 --> 00:46:26,000
has really not changed and the hash is

1280
00:46:26,000 --> 00:46:26,960
different

1281
00:46:26,960 --> 00:46:29,200
so and when you're doing a query for

1282
00:46:29,200 --> 00:46:30,880
authenticity of course the image would

1283
00:46:30,880 --> 00:46:32,720
have gone through some changes

1284
00:46:32,720 --> 00:46:35,760
so question is can you design hash

1285
00:46:35,760 --> 00:46:37,839
functions that are

1286
00:46:37,839 --> 00:46:41,200
robust to certain manipulations

1287
00:46:41,200 --> 00:46:44,240
and not robust to too much manipulation

1288
00:46:44,240 --> 00:46:46,480
i know the problem is not very well

1289
00:46:46,480 --> 00:46:47,839
defined

1290
00:46:47,839 --> 00:46:50,400
but there's a notion of fuzzy hashing

1291
00:46:50,400 --> 00:46:51,200
being around

1292
00:46:51,200 --> 00:46:53,119
in in cryptography as well in a very

1293
00:46:53,119 --> 00:46:54,400
formal kind of

1294
00:46:54,400 --> 00:46:57,760
way right and and uh in

1295
00:46:57,760 --> 00:47:01,599
in this particular framework the formal

1296
00:47:01,599 --> 00:47:04,720
aspect of that is still weak right but

1297
00:47:04,720 --> 00:47:08,800
practical uh fuzzy hashing techniques or

1298
00:47:08,800 --> 00:47:10,800
we call robust hashing techniques excel

1299
00:47:10,800 --> 00:47:12,000
processing

1300
00:47:12,000 --> 00:47:14,880
have also been have have been developed

1301
00:47:14,880 --> 00:47:15,680
right

1302
00:47:15,680 --> 00:47:20,400
and mixing that in combining that in

1303
00:47:20,400 --> 00:47:22,960
with in and using that to create the

1304
00:47:22,960 --> 00:47:24,319
image fingerprint

1305
00:47:24,319 --> 00:47:27,760
is is giving us a

1306
00:47:27,760 --> 00:47:29,839
much better protection against attacks

1307
00:47:29,839 --> 00:47:31,520
and now suddenly the

1308
00:47:31,520 --> 00:47:33,839
bar for the attacker becomes much more

1309
00:47:33,839 --> 00:47:35,040
difficult

1310
00:47:35,040 --> 00:47:38,960
right and here's just a a baby example

1311
00:47:38,960 --> 00:47:40,480
right so here is

1312
00:47:40,480 --> 00:47:42,559
your test image compressed reasonably

1313
00:47:42,559 --> 00:47:44,400
bad and there's a simple cut and paste

1314
00:47:44,400 --> 00:47:46,160
we put it just for proof of concept

1315
00:47:46,160 --> 00:47:49,119
right and and and with the proposed

1316
00:47:49,119 --> 00:47:51,440
technique we can actually nail down

1317
00:47:51,440 --> 00:47:54,400
the manipulated region very well right

1318
00:47:54,400 --> 00:47:55,599
but with prnu

1319
00:47:55,599 --> 00:47:59,920
if we were just using prnu it's uh

1320
00:47:59,920 --> 00:48:01,680
it still says that the image is

1321
00:48:01,680 --> 00:48:03,040
potentially manipulated

1322
00:48:03,040 --> 00:48:05,680
large areas turn up red light up right

1323
00:48:05,680 --> 00:48:07,760
but but most of the areas that light up

1324
00:48:07,760 --> 00:48:10,880
are actually not been manipulated right

1325
00:48:10,880 --> 00:48:12,559
so we are able to

1326
00:48:12,559 --> 00:48:16,000
get improvements uh in that sense and uh

1327
00:48:16,000 --> 00:48:17,760
hopefully we'll be writing this up in a

1328
00:48:17,760 --> 00:48:19,040
month or so and

1329
00:48:19,040 --> 00:48:22,640
and making all this public but uh

1330
00:48:22,640 --> 00:48:24,480
i think it's an important step it's

1331
00:48:24,480 --> 00:48:26,000
still just a step

1332
00:48:26,000 --> 00:48:28,160
uh there's a lot more that should be

1333
00:48:28,160 --> 00:48:29,920
done can be done i think i

1334
00:48:29,920 --> 00:48:33,599
uh and needs to be done

1335
00:48:33,599 --> 00:48:37,200
so the lessons uh so far

1336
00:48:37,200 --> 00:48:39,520
uh i think the computational

1337
00:48:39,520 --> 00:48:40,720
capabilities

1338
00:48:40,720 --> 00:48:44,240
uh in intelligent vision sensors

1339
00:48:44,240 --> 00:48:46,480
uh perhaps allow us to develop novel

1340
00:48:46,480 --> 00:48:48,720
protection mechanisms

1341
00:48:48,720 --> 00:48:51,599
and it's our philosophy or at least in

1342
00:48:51,599 --> 00:48:52,720
our thinking

1343
00:48:52,720 --> 00:48:54,800
the earlier the protection is applied

1344
00:48:54,800 --> 00:48:56,240
the better as

1345
00:48:56,240 --> 00:48:57,920
the computer scientists in this field

1346
00:48:57,920 --> 00:48:59,920
understand you want to reduce the attack

1347
00:48:59,920 --> 00:49:01,040
surface

1348
00:49:01,040 --> 00:49:02,800
right so the earlier you put in right at

1349
00:49:02,800 --> 00:49:04,800
the time of capture

1350
00:49:04,800 --> 00:49:07,839
uh the better and we

1351
00:49:07,839 --> 00:49:09,520
showing that potentially sensor

1352
00:49:09,520 --> 00:49:11,599
fingerprints can be learned

1353
00:49:11,599 --> 00:49:15,040
right which are robust uh provide very

1354
00:49:15,040 --> 00:49:16,720
little image distortion right

1355
00:49:16,720 --> 00:49:20,880
and they also have some security

1356
00:49:20,880 --> 00:49:22,400
properties and of course the

1357
00:49:22,400 --> 00:49:25,520
the problem of course is giving

1358
00:49:25,520 --> 00:49:27,280
provable guarantees of some of these

1359
00:49:27,280 --> 00:49:28,960
security properties so

1360
00:49:28,960 --> 00:49:32,880
that appears to be very very very hard

1361
00:49:32,880 --> 00:49:35,119
but again many of our cryptographic

1362
00:49:35,119 --> 00:49:36,240
techniques that we use

1363
00:49:36,240 --> 00:49:39,920
don't have provable guarantees right but

1364
00:49:39,920 --> 00:49:44,559
we they have other sort of

1365
00:49:44,559 --> 00:49:46,079
sort of properties that give us

1366
00:49:46,079 --> 00:49:47,680
sufficient assurance that they are

1367
00:49:47,680 --> 00:49:49,040
secure and they've been around for a

1368
00:49:49,040 --> 00:49:50,319
long time

1369
00:49:50,319 --> 00:49:53,760
so i i will not say that they are secure

1370
00:49:53,760 --> 00:49:56,240
but they perhaps there is a way to get

1371
00:49:56,240 --> 00:49:58,720
security out of them right and time will

1372
00:49:58,720 --> 00:49:59,680
tell

1373
00:49:59,680 --> 00:50:03,359
right and and uh there is interestingly

1374
00:50:03,359 --> 00:50:06,640
other work going on in this area right

1375
00:50:06,640 --> 00:50:08,800
in from a different direction so adobe

1376
00:50:08,800 --> 00:50:10,880
is doing something very interesting

1377
00:50:10,880 --> 00:50:12,480
called the content authenticity

1378
00:50:12,480 --> 00:50:14,480
initiative but they're using the

1379
00:50:14,480 --> 00:50:16,319
traditional crypto mechanisms right

1380
00:50:16,319 --> 00:50:17,760
they're basically taking the image

1381
00:50:17,760 --> 00:50:18,720
hashing it

1382
00:50:18,720 --> 00:50:21,200
like a sha-1 sha-2 whatever they want to

1383
00:50:21,200 --> 00:50:22,400
use

1384
00:50:22,400 --> 00:50:24,000
and then doing a digital signature

1385
00:50:24,000 --> 00:50:25,839
attaching it and having a prominent

1386
00:50:25,839 --> 00:50:26,559
chain

1387
00:50:26,559 --> 00:50:28,800
using our normal integrity mechanisms

1388
00:50:28,800 --> 00:50:30,559
that have been

1389
00:50:30,559 --> 00:50:34,400
created in in cryptography

1390
00:50:34,400 --> 00:50:37,760
and as long as the image stays within

1391
00:50:37,760 --> 00:50:38,079
that

1392
00:50:38,079 --> 00:50:41,200
ecosystem these mechanisms are there

1393
00:50:41,200 --> 00:50:44,400
and you're able to say something about

1394
00:50:44,400 --> 00:50:46,079
the authenticity of the image of what

1395
00:50:46,079 --> 00:50:48,400
was done to it and who did what to it

1396
00:50:48,400 --> 00:50:50,720
etc so it remains within the

1397
00:50:50,720 --> 00:50:54,000
adobe ecosystem they they

1398
00:50:54,000 --> 00:50:57,680
are making the apis open and public

1399
00:50:57,680 --> 00:51:01,520
right this is not a commercial

1400
00:51:01,520 --> 00:51:04,000
initiative by them they actually want to

1401
00:51:04,000 --> 00:51:05,200
make it all public

1402
00:51:05,200 --> 00:51:08,240
open source so that people like you will

1403
00:51:08,240 --> 00:51:10,079
be able to develop

1404
00:51:10,079 --> 00:51:12,319
perhaps other interesting mechanisms and

1405
00:51:12,319 --> 00:51:14,640
simply digital signatures

1406
00:51:14,640 --> 00:51:18,160
traditional digital signatures uh and

1407
00:51:18,160 --> 00:51:20,240
and and so i think that's a that's an

1408
00:51:20,240 --> 00:51:22,079
exciting opportunity

1409
00:51:22,079 --> 00:51:25,040
and i don't see any contradiction or any

1410
00:51:25,040 --> 00:51:26,240
conflict between

1411
00:51:26,240 --> 00:51:28,240
the something like the computational

1412
00:51:28,240 --> 00:51:29,680
sensor fingerprint

1413
00:51:29,680 --> 00:51:32,000
and and what adobe is doing it can be

1414
00:51:32,000 --> 00:51:33,520
simply attached on

1415
00:51:33,520 --> 00:51:37,200
if needed as an additional

1416
00:51:37,200 --> 00:51:41,040
mechanism so and then jpeg also

1417
00:51:41,040 --> 00:51:43,119
is actually there's a working group jpeg

1418
00:51:43,119 --> 00:51:44,960
fake media i urge you to join

1419
00:51:44,960 --> 00:51:46,640
participate

1420
00:51:46,640 --> 00:51:49,119
they're looking at what can be done and

1421
00:51:49,119 --> 00:51:51,359
to standardize some of these protection

1422
00:51:51,359 --> 00:51:52,640
of prominence

1423
00:51:52,640 --> 00:51:54,000
mechanisms of course when they

1424
00:51:54,000 --> 00:51:56,880
standardize their sanitizing the syntax

1425
00:51:56,880 --> 00:51:59,200
uh because they want to keep the

1426
00:51:59,200 --> 00:52:00,480
algorithms themselves

1427
00:52:00,480 --> 00:52:03,200
open to to different people they want to

1428
00:52:03,200 --> 00:52:04,160
choke

1429
00:52:04,160 --> 00:52:07,440
uh innovation so but nevertheless

1430
00:52:07,440 --> 00:52:09,520
looking at

1431
00:52:09,520 --> 00:52:12,559
standardizing the syntax for that

1432
00:52:12,559 --> 00:52:16,839
so just just to end

1433
00:52:16,839 --> 00:52:20,559
up media forensics for a long time we

1434
00:52:20,559 --> 00:52:20,960
have

1435
00:52:20,960 --> 00:52:24,480
looked at uh detection

1436
00:52:24,480 --> 00:52:26,720
and as i said same thing in security for

1437
00:52:26,720 --> 00:52:28,400
a long time we were doing

1438
00:52:28,400 --> 00:52:32,559
intrusion detection right and

1439
00:52:32,559 --> 00:52:35,359
but detection has limitations right it

1440
00:52:35,359 --> 00:52:37,280
is needed it's absolutely needed

1441
00:52:37,280 --> 00:52:40,319
but uh we have to go beyond detection as

1442
00:52:40,319 --> 00:52:40,800
well

1443
00:52:40,800 --> 00:52:43,920
and uh in certain situations for certain

1444
00:52:43,920 --> 00:52:44,720
type of

1445
00:52:44,720 --> 00:52:47,839
content we perhaps need to explore

1446
00:52:47,839 --> 00:52:51,280
proactive methods and

1447
00:52:51,280 --> 00:52:54,240
which i loosely call as secure capture

1448
00:52:54,240 --> 00:52:55,599
so can we do

1449
00:52:55,599 --> 00:52:57,599
methods that allow us to do secure

1450
00:52:57,599 --> 00:52:59,839
capture

1451
00:52:59,839 --> 00:53:02,800
at the time of capture capture itself

1452
00:53:02,800 --> 00:53:05,119
something is done so that

1453
00:53:05,119 --> 00:53:07,839
manipulation becomes more difficult

1454
00:53:07,839 --> 00:53:10,319
after that

1455
00:53:10,319 --> 00:53:13,680
so let me stop at that i think i

1456
00:53:13,680 --> 00:53:17,520
went for 55 minutes instead of 50 but

1457
00:53:17,520 --> 00:53:21,520
uh didn't either license to do that he

1458
00:53:21,520 --> 00:53:23,200
said you can go a little bit over if you

1459
00:53:23,200 --> 00:53:23,839
want to

1460
00:53:23,839 --> 00:53:26,160
yep you have a number of comments and

1461
00:53:26,160 --> 00:53:27,040
questions

1462
00:53:27,040 --> 00:53:29,119
okay yeah i was seeing them pop up

1463
00:53:29,119 --> 00:53:30,640
because i was full screen i couldn't

1464
00:53:30,640 --> 00:53:33,520
stop and look at them

1465
00:53:33,520 --> 00:53:36,480
so there in zoom i stopped sharing

1466
00:53:36,480 --> 00:53:38,480
screen first

1467
00:53:38,480 --> 00:53:40,839
zoom i'll go to questions the eight

1468
00:53:40,839 --> 00:53:42,400
questions

1469
00:53:42,400 --> 00:53:46,240
what is truth uh yeah

1470
00:53:47,280 --> 00:53:50,160
that's what pontius pilate told jesus i

1471
00:53:50,160 --> 00:53:51,920
think what is truth

1472
00:53:51,920 --> 00:53:55,280
i don't know what truth is

1473
00:53:55,440 --> 00:53:58,559
and uh uh

1474
00:53:58,559 --> 00:54:01,040
that's a prf what is prf pseudo random

1475
00:54:01,040 --> 00:54:02,240
function

1476
00:54:02,240 --> 00:54:06,000
i i think the purdue research foundation

1477
00:54:06,000 --> 00:54:08,000
and i think you were talking about

1478
00:54:08,000 --> 00:54:09,280
produce

1479
00:54:09,280 --> 00:54:12,319
processing work and that really was

1480
00:54:12,319 --> 00:54:15,119
a delp's group

1481
00:54:15,839 --> 00:54:18,078
okay

1482
00:54:20,160 --> 00:54:24,000
yeah so these are comments more than

1483
00:54:24,000 --> 00:54:26,480
questions

1484
00:54:27,119 --> 00:54:29,200
so you can you can click answer live and

1485
00:54:29,200 --> 00:54:30,960
then done if you don't have anything to

1486
00:54:30,960 --> 00:54:31,680
say to it

1487
00:54:31,680 --> 00:54:34,240
correlation is not causation of course

1488
00:54:34,240 --> 00:54:35,680
right

1489
00:54:35,680 --> 00:54:39,119
would quantum computing improve

1490
00:54:39,119 --> 00:54:41,200
i don't know what quantum computing has

1491
00:54:41,200 --> 00:54:42,960
i can't see the connection with it but

1492
00:54:42,960 --> 00:54:44,480
that's because of my own limited

1493
00:54:44,480 --> 00:54:47,920
knowledge about quantum computing

1494
00:54:50,839 --> 00:54:52,720
so

1495
00:54:52,720 --> 00:54:56,559
arts what is that i have no idea

1496
00:54:56,559 --> 00:54:59,760
i i think dys is do it yourself

1497
00:54:59,760 --> 00:55:02,000
but i don't know i don't remember what

1498
00:55:02,000 --> 00:55:03,359
you were talking about when the question

1499
00:55:03,359 --> 00:55:06,319
was posed

1500
00:55:06,319 --> 00:55:09,760
uh overlap with cryptographic

1501
00:55:09,760 --> 00:55:10,799
authentication yes

1502
00:55:10,799 --> 00:55:13,760
indeed the only problem of course is

1503
00:55:13,760 --> 00:55:15,760
there's not a direct transfer

1504
00:55:15,760 --> 00:55:18,559
because in cryptographic authentication

1505
00:55:18,559 --> 00:55:19,839
your

1506
00:55:19,839 --> 00:55:21,920
authenticating the exact sequence of

1507
00:55:21,920 --> 00:55:23,920
bits and here it's content

1508
00:55:23,920 --> 00:55:26,960
whatever that content means right and so

1509
00:55:26,960 --> 00:55:30,000
that's what makes it a bit

1510
00:55:32,839 --> 00:55:34,240
harder

1511
00:55:34,240 --> 00:55:37,440
ed said nice talk so it said

1512
00:55:37,440 --> 00:55:41,200
uh can your ml approach be used to do a

1513
00:55:41,200 --> 00:55:44,799
rewater marking attack

1514
00:55:45,520 --> 00:55:47,599
so that is where we brought the content

1515
00:55:47,599 --> 00:55:48,960
hash in it

1516
00:55:48,960 --> 00:55:50,880
and we believe that the content hash

1517
00:55:50,880 --> 00:55:53,680
will make it difficult but we've not

1518
00:55:53,680 --> 00:55:56,960
spent time doing we will do that we've

1519
00:55:56,960 --> 00:55:58,160
not spent time

1520
00:55:58,160 --> 00:56:01,359
doing that yet and

1521
00:56:01,359 --> 00:56:03,760
but that's a great question i mean i i

1522
00:56:03,760 --> 00:56:05,359
of course

1523
00:56:05,359 --> 00:56:07,359
one has to look at that and of course

1524
00:56:07,359 --> 00:56:09,359
that's something we have to

1525
00:56:09,359 --> 00:56:12,480
kind of demonstrate that hey this is

1526
00:56:12,480 --> 00:56:14,400
more than demonstrate actually convince

1527
00:56:14,400 --> 00:56:16,160
people that this can't be done and we've

1528
00:56:16,160 --> 00:56:16,559
not

1529
00:56:16,559 --> 00:56:19,280
done that yet

1530
00:56:19,760 --> 00:56:21,920
i i i don't know ed you might have heard

1531
00:56:21,920 --> 00:56:24,000
this talk of mine before

1532
00:56:24,000 --> 00:56:27,040
i i'm trying to sort of

1533
00:56:27,040 --> 00:56:29,359
get the community to look at it more

1534
00:56:29,359 --> 00:56:31,680
because there's this

1535
00:56:31,680 --> 00:56:34,160
a lot of activity in detection and and

1536
00:56:34,160 --> 00:56:34,799
as i said

1537
00:56:34,799 --> 00:56:38,720
detection is needed but

1538
00:56:38,720 --> 00:56:42,319
i think the other side of it also needs

1539
00:56:42,319 --> 00:56:43,440
to be looked at

1540
00:56:43,440 --> 00:56:46,880
and because there's certain images

1541
00:56:46,880 --> 00:56:49,119
certain type of content like police body

1542
00:56:49,119 --> 00:56:50,480
cameras

1543
00:56:50,480 --> 00:56:53,040
right and things of that sort the

1544
00:56:53,040 --> 00:56:55,680
pictures that journalists take

1545
00:56:55,680 --> 00:56:57,760
we need better guarantees of their of

1546
00:56:57,760 --> 00:56:58,960
their prominence

1547
00:56:58,960 --> 00:57:03,040
right because otherwise uh once

1548
00:57:03,040 --> 00:57:05,839
fake media erodes trust in society there

1549
00:57:05,839 --> 00:57:06,640
has to be some

1550
00:57:06,640 --> 00:57:08,880
what i call islands of truth right some

1551
00:57:08,880 --> 00:57:11,599
some things that we can believe

1552
00:57:11,599 --> 00:57:14,079
and and to get that to build these

1553
00:57:14,079 --> 00:57:15,440
islands of truth

1554
00:57:15,440 --> 00:57:17,359
we need some proactive mechanisms i

1555
00:57:17,359 --> 00:57:19,598
think

1556
00:57:20,079 --> 00:57:22,240
i think one you skipped over is so

1557
00:57:22,240 --> 00:57:25,920
forensics all about securing evidence

1558
00:57:25,920 --> 00:57:26,480
really

1559
00:57:26,480 --> 00:57:30,160
no no securing evidence is more for the

1560
00:57:30,160 --> 00:57:31,839
what is called

1561
00:57:31,839 --> 00:57:36,400
the chain of custody right

1562
00:57:37,359 --> 00:57:40,559
and yes in the

1563
00:57:40,559 --> 00:57:42,640
forensics done by traditional law

1564
00:57:42,640 --> 00:57:43,680
enforcement

1565
00:57:43,680 --> 00:57:47,200
right uh where it is uh presented in a

1566
00:57:47,200 --> 00:57:48,480
court of law

1567
00:57:48,480 --> 00:57:51,520
uh chain of custody is is very important

1568
00:57:51,520 --> 00:57:55,040
right and and chain of custody can be

1569
00:57:55,040 --> 00:57:55,920
built in

1570
00:57:55,920 --> 00:57:58,400
right and we know how to do that when we

1571
00:57:58,400 --> 00:57:58,960
analyze

1572
00:57:58,960 --> 00:58:00,720
and this traditional model of analyzing

1573
00:58:00,720 --> 00:58:02,480
a disk and this and that

1574
00:58:02,480 --> 00:58:05,920
same things can be moved here forensics

1575
00:58:05,920 --> 00:58:07,440
is also about

1576
00:58:07,440 --> 00:58:09,920
developing techniques that tell us what

1577
00:58:09,920 --> 00:58:11,200
might have happened

1578
00:58:11,200 --> 00:58:14,799
right because so

1579
00:58:14,799 --> 00:58:18,000
so i think and my work goes more into

1580
00:58:18,000 --> 00:58:19,760
that direction

1581
00:58:19,760 --> 00:58:22,480
about recovering evidence finding

1582
00:58:22,480 --> 00:58:23,440
evidence

1583
00:58:23,440 --> 00:58:26,480
right and not simply

1584
00:58:26,480 --> 00:58:29,680
and of course protection is is important

1585
00:58:29,680 --> 00:58:32,880
right securing the evidence is important

1586
00:58:32,880 --> 00:58:35,520
but i my work more has been in the other

1587
00:58:35,520 --> 00:58:36,319
two

1588
00:58:36,319 --> 00:58:38,799
areas

1589
00:58:40,960 --> 00:58:43,280
lossy compression and image malleability

1590
00:58:43,280 --> 00:58:45,040
detection

1591
00:58:45,040 --> 00:58:48,640
yeah kind of that's what i said

1592
00:58:48,640 --> 00:58:50,240
you could spoof the pier and you yeah

1593
00:58:50,240 --> 00:58:51,839
yeah yeah and that's the whole point

1594
00:58:51,839 --> 00:58:53,119
between computational

1595
00:58:53,119 --> 00:58:54,480
sense of fingerprints we want to make

1596
00:58:54,480 --> 00:58:58,079
them hard to spoof

1597
00:58:59,040 --> 00:59:02,079
and ed said no i don't know what he said

1598
00:59:02,079 --> 00:59:05,839
nor to

1599
00:59:10,319 --> 00:59:14,160
well we've

1600
00:59:14,160 --> 00:59:17,040
hit the end of the time here thank you

1601
00:59:17,040 --> 00:59:18,160
so much

1602
00:59:18,160 --> 00:59:21,599
for visiting with us and presenting this

1603
00:59:21,599 --> 00:59:23,359
information this is a great way to end

1604
00:59:23,359 --> 00:59:25,200
out the summer series

1605
00:59:25,200 --> 00:59:26,960
i'd like to remind everybody who's

1606
00:59:26,960 --> 00:59:29,280
listening in the recording this will be

1607
00:59:29,280 --> 00:59:30,000
up

1608
00:59:30,000 --> 00:59:33,359
in a few days on our website

1609
00:59:33,359 --> 00:59:36,799
and starting in august

1610
00:59:36,799 --> 00:59:40,720
25th we'll have the first of our fall

1611
00:59:40,720 --> 00:59:44,480
security seminar series being presented

1612
00:59:44,480 --> 00:59:46,799
we very much appreciate everyone having

1613
00:59:46,799 --> 00:59:48,000
uh

1614
00:59:48,000 --> 00:59:49,920
uh spending some time to come in and

1615
00:59:49,920 --> 00:59:52,960
join us and uh is here maybe

1616
00:59:52,960 --> 00:59:54,880
sometime in the not too distant future

1617
00:59:54,880 --> 00:59:56,319
we can get you to visit us

1618
00:59:56,319 --> 00:59:59,599
in person sure and same for you same for

1619
00:59:59,599 --> 01:00:00,799
you ed

1620
01:00:00,799 --> 01:00:03,920
and others i i was not able to see the

1621
01:00:03,920 --> 01:00:05,839
names of participants but yeah

1622
01:00:05,839 --> 01:00:08,400
if you're in new york city look me up

1623
01:00:08,400 --> 01:00:09,280
all right all

1624
01:00:09,280 --> 01:00:11,200
have have a great afternoon and enjoy

1625
01:00:11,200 --> 01:00:12,319
the rest of your summer

1626
01:00:12,319 --> 01:00:15,839
yep thanks bye for now

1627
01:00:29,200 --> 01:00:31,279
you

