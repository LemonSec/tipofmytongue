1
00:00:00,320 --> 00:00:03,360
so i think it's like around uh 4 30. i

2
00:00:03,360 --> 00:00:04,960
will just turn off my camera

3
00:00:04,960 --> 00:00:08,480
and when hi you can just introduce

4
00:00:08,480 --> 00:00:11,840
the speaker today okay yeah sure so

5
00:00:11,840 --> 00:00:16,160
um i'm glad to introduce today's speaker

6
00:00:16,160 --> 00:00:19,279
um dr chamin yimin chen and he's

7
00:00:19,279 --> 00:00:20,800
currently a

8
00:00:20,800 --> 00:00:23,519
postdoctoral associate in the department

9
00:00:23,519 --> 00:00:26,640
of computer science at virginia tech

10
00:00:26,640 --> 00:00:29,119
and then he received this phd i think

11
00:00:29,119 --> 00:00:29,760
from

12
00:00:29,760 --> 00:00:33,600
arizona state university in 2018

13
00:00:33,600 --> 00:00:38,320
um his pt research focuses on the

14
00:00:38,320 --> 00:00:40,160
the security and the privacy in mobile

15
00:00:40,160 --> 00:00:42,800
computing and before that he received

16
00:00:42,800 --> 00:00:43,680
his

17
00:00:43,680 --> 00:00:47,120
best degree from the school of

18
00:00:47,120 --> 00:00:49,920
electronics engineering and the computer

19
00:00:49,920 --> 00:00:50,399
science

20
00:00:50,399 --> 00:00:53,840
at pecking university in 2010

21
00:00:53,840 --> 00:00:55,680
and his master's degree from the

22
00:00:55,680 --> 00:00:58,000
department of electrical engineering

23
00:00:58,000 --> 00:01:01,120
at chinese university of hong kong

24
00:01:01,120 --> 00:01:04,159
in 2013

25
00:01:04,159 --> 00:01:07,920
his research interest i think

26
00:01:08,479 --> 00:01:10,560
includes security and privacy in machine

27
00:01:10,560 --> 00:01:11,520
learning and

28
00:01:11,520 --> 00:01:13,600
meta learning and networking security

29
00:01:13,600 --> 00:01:15,360
and networked systems

30
00:01:15,360 --> 00:01:18,799
so um and also i think he's the

31
00:01:18,799 --> 00:01:22,000
is in the job market this year so um

32
00:01:22,000 --> 00:01:24,240
and looking for faculty positions so

33
00:01:24,240 --> 00:01:25,520
good luck

34
00:01:25,520 --> 00:01:28,960
for that um so um yeah

35
00:01:28,960 --> 00:01:32,400
i think um that's a quite a brief

36
00:01:32,400 --> 00:01:34,479
introduction to dr ching

37
00:01:34,479 --> 00:01:37,520
um with that i'll and i'll turn it over

38
00:01:37,520 --> 00:01:37,920
to

39
00:01:37,920 --> 00:01:41,040
to you dr chen

40
00:01:41,040 --> 00:01:43,119
thank you for the warm introduction

41
00:01:43,119 --> 00:01:44,320
professor

42
00:01:44,320 --> 00:01:47,040
and good afternoon and thank you for

43
00:01:47,040 --> 00:01:47,759
having me

44
00:01:47,759 --> 00:01:51,119
here today my name is ian chen

45
00:01:51,119 --> 00:01:53,360
and i'm currently a postdoc in the

46
00:01:53,360 --> 00:01:55,520
department of computer science

47
00:01:55,520 --> 00:01:59,360
at virginia tech today i'm honored to

48
00:01:59,360 --> 00:02:02,399
introduce my our research in

49
00:02:02,399 --> 00:02:05,200
machine learning security and privacy

50
00:02:05,200 --> 00:02:07,360
from the perspective of

51
00:02:07,360 --> 00:02:10,479
metal learning which is becoming an

52
00:02:10,479 --> 00:02:13,440
increasingly important topic in the era

53
00:02:13,440 --> 00:02:13,680
of

54
00:02:13,680 --> 00:02:16,720
artificial intelligence of things or we

55
00:02:16,720 --> 00:02:18,720
call it aiot

56
00:02:18,720 --> 00:02:22,800
and these are the joint research with my

57
00:02:22,800 --> 00:02:24,000
colleague

58
00:02:24,000 --> 00:02:28,000
miss miss ning wang miss yang ho

59
00:02:28,000 --> 00:02:32,640
and dr wen jingle at virginia tech

60
00:02:35,120 --> 00:02:38,080
so for today's talk i will first

61
00:02:38,080 --> 00:02:39,040
introduce the

62
00:02:39,040 --> 00:02:41,760
importance of metal learning perspective

63
00:02:41,760 --> 00:02:42,080
in

64
00:02:42,080 --> 00:02:44,560
machine learning security and privacy

65
00:02:44,560 --> 00:02:45,680
research

66
00:02:45,680 --> 00:02:48,720
then i will share the details of two of

67
00:02:48,720 --> 00:02:50,000
our ongoing research

68
00:02:50,000 --> 00:02:53,440
project the first one is about defending

69
00:02:53,440 --> 00:02:56,640
against uh data point only attacks

70
00:02:56,640 --> 00:02:59,760
on uh distributed learning models

71
00:02:59,760 --> 00:03:02,879
and the second one is about defending

72
00:03:02,879 --> 00:03:05,920
against a privacy attacks on

73
00:03:05,920 --> 00:03:09,200
federated metal learning systems

74
00:03:09,200 --> 00:03:12,000
i will leave some time at the end of for

75
00:03:12,000 --> 00:03:12,560
the q

76
00:03:12,560 --> 00:03:15,599
a and i hope today's talk

77
00:03:15,599 --> 00:03:18,080
can ignite and engage in and continue

78
00:03:18,080 --> 00:03:20,800
conversation

79
00:03:21,360 --> 00:03:24,959
so why is the meta learning perspective

80
00:03:24,959 --> 00:03:25,760
meaningful

81
00:03:25,760 --> 00:03:28,640
for our research in machine learning

82
00:03:28,640 --> 00:03:32,480
security and privacy

83
00:03:32,480 --> 00:03:35,519
to answer this question i will first uh

84
00:03:35,519 --> 00:03:37,200
briefly introduce

85
00:03:37,200 --> 00:03:40,400
so uh what meta learning is about we all

86
00:03:40,400 --> 00:03:40,879
know

87
00:03:40,879 --> 00:03:44,959
a machine learning model is to train to

88
00:03:44,959 --> 00:03:48,159
learn one specific task for example

89
00:03:48,159 --> 00:03:52,319
uh learn to do image classification

90
00:03:52,319 --> 00:03:56,239
and in comparison uh the high level idea

91
00:03:56,239 --> 00:03:57,599
of meta learning

92
00:03:57,599 --> 00:04:02,080
is to learn to learn from many tasks

93
00:04:02,080 --> 00:04:04,959
so for example in this figure we show

94
00:04:04,959 --> 00:04:05,519
that

95
00:04:05,519 --> 00:04:09,439
the robot here is exposed to

96
00:04:09,439 --> 00:04:12,799
100 learning tasks and through meta

97
00:04:12,799 --> 00:04:13,599
learning

98
00:04:13,599 --> 00:04:16,720
it learns some skills from all these

99
00:04:16,720 --> 00:04:17,440
tasks

100
00:04:17,440 --> 00:04:20,560
and becomes more confident to learn

101
00:04:20,560 --> 00:04:23,680
new tasks for example task 101

102
00:04:23,680 --> 00:04:28,240
and usually the skills it obtains

103
00:04:28,240 --> 00:04:31,120
uh common knowledge shall among the

104
00:04:31,120 --> 00:04:31,759
previous

105
00:04:31,759 --> 00:04:35,040
100 learning tasks

106
00:04:35,040 --> 00:04:37,360
this is the high-level idea of meta

107
00:04:37,360 --> 00:04:39,840
learning

108
00:04:40,479 --> 00:04:44,240
however in today i will only focus on a

109
00:04:44,240 --> 00:04:44,880
very

110
00:04:44,880 --> 00:04:47,520
small slice of metal learning i will

111
00:04:47,520 --> 00:04:48,240
focus

112
00:04:48,240 --> 00:04:51,199
on using metal learning as a

113
00:04:51,199 --> 00:04:52,960
pre-training technique

114
00:04:52,960 --> 00:04:56,400
for generic deep learning models

115
00:04:56,400 --> 00:04:59,680
and in this figure i illustrate the

116
00:04:59,680 --> 00:05:00,400
benefit

117
00:05:00,400 --> 00:05:02,880
of meta learning as a pre-training

118
00:05:02,880 --> 00:05:04,160
method

119
00:05:04,160 --> 00:05:07,520
so we can see here

120
00:05:07,520 --> 00:05:11,120
the blue circle is the parameter space

121
00:05:11,120 --> 00:05:14,400
of of our learning task and

122
00:05:14,400 --> 00:05:16,759
we all know that the most common

123
00:05:16,759 --> 00:05:18,720
initialization method

124
00:05:18,720 --> 00:05:22,080
is random initialization right and in

125
00:05:22,080 --> 00:05:23,120
this figure

126
00:05:23,120 --> 00:05:26,080
random initialization is the center of

127
00:05:26,080 --> 00:05:27,120
the circle

128
00:05:27,120 --> 00:05:30,479
here here and we assume

129
00:05:30,479 --> 00:05:33,520
that the r into star here

130
00:05:33,520 --> 00:05:36,840
denotes the parameters of our task after

131
00:05:36,840 --> 00:05:38,240
convergence

132
00:05:38,240 --> 00:05:41,600
so the net the machine learning training

133
00:05:41,600 --> 00:05:42,479
process

134
00:05:42,479 --> 00:05:45,440
is to move the parameters uh the

135
00:05:45,440 --> 00:05:46,479
parameters

136
00:05:46,479 --> 00:05:48,919
from this starting point from random

137
00:05:48,919 --> 00:05:50,639
initialization

138
00:05:50,639 --> 00:05:55,360
to the parameters at convergence

139
00:05:55,360 --> 00:05:58,800
and the intuition of meta learning is uh

140
00:05:58,800 --> 00:06:02,400
simple suppose uh

141
00:06:02,400 --> 00:06:06,479
suppose uh we have a lot of tasks for

142
00:06:06,479 --> 00:06:07,039
example

143
00:06:07,039 --> 00:06:10,319
task 1 to task 100 and

144
00:06:10,319 --> 00:06:16,080
they are similar to our new tasks here

145
00:06:16,240 --> 00:06:19,280
and very likely through a

146
00:06:19,280 --> 00:06:22,240
metal learning we we can obtain a

147
00:06:22,240 --> 00:06:24,720
starting point a starting point here

148
00:06:24,720 --> 00:06:27,840
that is closer to the uh to the point

149
00:06:27,840 --> 00:06:30,560
after conversion of our task

150
00:06:30,560 --> 00:06:34,000
so we can see that the keywords for

151
00:06:34,000 --> 00:06:37,680
metal learning as initialization method

152
00:06:37,680 --> 00:06:40,560
are common knowledge and also fast

153
00:06:40,560 --> 00:06:41,440
learning

154
00:06:41,440 --> 00:06:44,639
and these two will be are very important

155
00:06:44,639 --> 00:06:45,520
features

156
00:06:45,520 --> 00:06:49,840
in our research

157
00:06:51,759 --> 00:06:54,880
so now coming to part one of today's

158
00:06:54,880 --> 00:06:56,000
talk

159
00:06:56,000 --> 00:06:59,120
defending against data poisoning attacks

160
00:06:59,120 --> 00:07:03,199
on machine learning models

161
00:07:03,199 --> 00:07:06,639
first uh so why do we care about data

162
00:07:06,639 --> 00:07:08,720
poisoning attacks

163
00:07:08,720 --> 00:07:10,960
in fact they are of a big concern right

164
00:07:10,960 --> 00:07:12,080
now because

165
00:07:12,080 --> 00:07:14,880
data poisoning attacks usually cause

166
00:07:14,880 --> 00:07:18,319
severe damages to our society

167
00:07:18,319 --> 00:07:21,680
today we are we are using uh

168
00:07:21,680 --> 00:07:24,960
machine learning models in many uh in

169
00:07:24,960 --> 00:07:27,120
many sectors of our society

170
00:07:27,120 --> 00:07:29,680
for example we are using them for

171
00:07:29,680 --> 00:07:31,280
application

172
00:07:31,280 --> 00:07:34,880
including medical diagnosis

173
00:07:34,880 --> 00:07:38,039
autonomous driving and also

174
00:07:38,039 --> 00:07:42,400
institutional intrusion detection

175
00:07:42,400 --> 00:07:45,120
and data point only attacks are to

176
00:07:45,120 --> 00:07:46,160
insert

177
00:07:46,160 --> 00:07:48,960
quality samples into the training set of

178
00:07:48,960 --> 00:07:50,560
machine learning models

179
00:07:50,560 --> 00:07:54,879
and eventually harm the models

180
00:07:54,879 --> 00:07:58,879
we can see from the lower figures here

181
00:07:58,879 --> 00:08:02,879
a fewer poisonous samples can change the

182
00:08:02,879 --> 00:08:06,000
decision boundary of the vitamin machine

183
00:08:06,000 --> 00:08:07,199
learning model

184
00:08:07,199 --> 00:08:10,080
and cause wrong classification results

185
00:08:10,080 --> 00:08:10,479
on

186
00:08:10,479 --> 00:08:13,960
tested samples and we all know that

187
00:08:13,960 --> 00:08:15,120
misclassifications

188
00:08:15,120 --> 00:08:18,560
in some application could directly

189
00:08:18,560 --> 00:08:21,759
impact people's health and wealth wealth

190
00:08:21,759 --> 00:08:26,000
and wellness

191
00:08:26,000 --> 00:08:28,319
and what's more uh the threat of data

192
00:08:28,319 --> 00:08:29,520
pointing attacks

193
00:08:29,520 --> 00:08:32,719
also comes from that they are very easy

194
00:08:32,719 --> 00:08:33,440
to launch

195
00:08:33,440 --> 00:08:36,799
and they are also very effective

196
00:08:36,799 --> 00:08:40,399
on most machine learning models and we

197
00:08:40,399 --> 00:08:41,200
can see

198
00:08:41,200 --> 00:08:45,600
in the figure on the left here

199
00:08:45,600 --> 00:08:48,800
so in the area of iot we rely

200
00:08:48,800 --> 00:08:51,839
more and more on with most on remote

201
00:08:51,839 --> 00:08:53,040
nodes to collect

202
00:08:53,040 --> 00:08:56,160
trending data these remote nodes are

203
00:08:56,160 --> 00:08:58,640
usually iot devices

204
00:08:58,640 --> 00:09:02,000
and their numbers is growing rapidly

205
00:09:02,000 --> 00:09:03,839
over the time

206
00:09:03,839 --> 00:09:06,399
and the central server here aggregates

207
00:09:06,399 --> 00:09:07,680
this data

208
00:09:07,680 --> 00:09:12,000
and collect and conduct model training

209
00:09:12,000 --> 00:09:15,040
however such open nature of

210
00:09:15,040 --> 00:09:18,399
data collection process also allows

211
00:09:18,399 --> 00:09:21,680
attackers to act as remote node

212
00:09:21,680 --> 00:09:24,880
here and then launch data point only

213
00:09:24,880 --> 00:09:26,320
attacks

214
00:09:26,320 --> 00:09:29,440
for example the central server may

215
00:09:29,440 --> 00:09:32,160
publish a data collection tags of

216
00:09:32,160 --> 00:09:32,880
collecting

217
00:09:32,880 --> 00:09:36,160
images of digits of digits

218
00:09:36,160 --> 00:09:39,600
and but the attacker can instead simply

219
00:09:39,600 --> 00:09:42,640
upload image of letters to launch the

220
00:09:42,640 --> 00:09:45,519
data point on your attacks

221
00:09:45,519 --> 00:09:48,040
and considering there are so many

222
00:09:48,040 --> 00:09:50,800
vulnerabilities and cyber attacks

223
00:09:50,800 --> 00:09:54,160
on iot devices defending against data

224
00:09:54,160 --> 00:09:56,320
poisoning attacks have practical

225
00:09:56,320 --> 00:10:00,240
impact to our society

226
00:10:02,880 --> 00:10:05,519
the uniqueness of our approach to

227
00:10:05,519 --> 00:10:08,720
defending against data poisoning attack

228
00:10:08,720 --> 00:10:11,440
is that we try to tackle the challenging

229
00:10:11,440 --> 00:10:14,560
from the meta learning perspective

230
00:10:14,560 --> 00:10:18,320
so first uh i would like to

231
00:10:18,320 --> 00:10:20,880
so let's look at uh fundamentally what

232
00:10:20,880 --> 00:10:21,760
happens

233
00:10:21,760 --> 00:10:25,360
in a data poisoning attack

234
00:10:25,360 --> 00:10:28,480
as illustrated in the figure here in the

235
00:10:28,480 --> 00:10:30,320
figure here

236
00:10:30,320 --> 00:10:33,120
the purple dots and the green dots were

237
00:10:33,120 --> 00:10:35,600
present to different classes

238
00:10:35,600 --> 00:10:38,480
and the white curve is the decision

239
00:10:38,480 --> 00:10:41,839
boundary of the machine learning model

240
00:10:41,839 --> 00:10:44,959
and in a data point only attack

241
00:10:44,959 --> 00:10:47,839
the attacker may insert a few samples

242
00:10:47,839 --> 00:10:49,920
with modified labels

243
00:10:49,920 --> 00:10:53,600
for example on in the figure on the

244
00:10:53,600 --> 00:10:55,519
right here

245
00:10:55,519 --> 00:10:59,839
the attacker may modify at least

246
00:10:59,839 --> 00:11:03,200
these two uh purple dots into

247
00:11:03,200 --> 00:11:06,880
a green dots and at the same time uh

248
00:11:06,880 --> 00:11:10,560
she or he may modify the green dots

249
00:11:10,560 --> 00:11:15,200
the three green dots here to purple dots

250
00:11:15,200 --> 00:11:18,320
and then training on such a data set the

251
00:11:18,320 --> 00:11:20,320
decision boundary of the machine

252
00:11:20,320 --> 00:11:22,000
learning models

253
00:11:22,000 --> 00:11:24,720
is totally changed because it tried to

254
00:11:24,720 --> 00:11:26,079
accommodate

255
00:11:26,079 --> 00:11:29,600
at least fewer poisonous samples

256
00:11:29,600 --> 00:11:32,800
and this is also known as overfitting of

257
00:11:32,800 --> 00:11:35,519
deep learning models

258
00:11:35,519 --> 00:11:38,880
so compile uh the left and the right

259
00:11:38,880 --> 00:11:39,600
figure

260
00:11:39,600 --> 00:11:41,760
the machine learning model is very

261
00:11:41,760 --> 00:11:43,920
likely to misclassify

262
00:11:43,920 --> 00:11:47,600
the text sample for example on the left

263
00:11:47,600 --> 00:11:47,920
a

264
00:11:47,920 --> 00:11:51,360
list testing sample will be classified

265
00:11:51,360 --> 00:11:54,880
as the purple class however

266
00:11:54,880 --> 00:11:57,920
in the figure on the on the right here

267
00:11:57,920 --> 00:11:59,680
it will be classified as

268
00:11:59,680 --> 00:12:02,959
a green class

269
00:12:06,639 --> 00:12:10,760
and our intuition of using meta

270
00:12:10,760 --> 00:12:13,120
initialization is that

271
00:12:13,120 --> 00:12:16,000
meta learning as we mentioned is to

272
00:12:16,000 --> 00:12:16,959
extract

273
00:12:16,959 --> 00:12:19,680
common knowledge from the training set

274
00:12:19,680 --> 00:12:20,800
and therefore

275
00:12:20,800 --> 00:12:24,160
the initialized initialization model

276
00:12:24,160 --> 00:12:27,360
is much less affected by the poisonous

277
00:12:27,360 --> 00:12:31,519
samples and meanwhile the initialization

278
00:12:31,519 --> 00:12:32,240
model

279
00:12:32,240 --> 00:12:35,360
also helps model training to converge

280
00:12:35,360 --> 00:12:38,560
faster due to uh

281
00:12:38,560 --> 00:12:41,920
the property of initialization model

282
00:12:41,920 --> 00:12:45,040
and by doing so can further reduce

283
00:12:45,040 --> 00:12:48,079
over 15 as well

284
00:12:48,079 --> 00:12:51,800
so we can see that the

285
00:12:51,800 --> 00:12:54,800
initialization model from meta learning

286
00:12:54,800 --> 00:12:58,000
uh can have two uh property

287
00:12:58,000 --> 00:13:01,680
one is to uh reduce the overfitting

288
00:13:01,680 --> 00:13:04,959
and once one is to enable fast fit

289
00:13:04,959 --> 00:13:08,638
faster conversion

290
00:13:09,600 --> 00:13:13,600
so based on such a intuition we propose

291
00:13:13,600 --> 00:13:16,480
our framework here to defend against

292
00:13:16,480 --> 00:13:19,120
data poisoning attacks

293
00:13:19,120 --> 00:13:22,959
and we call our our approach net

294
00:13:22,959 --> 00:13:25,760
so as shown here uh deploying our

295
00:13:25,760 --> 00:13:26,480
approach

296
00:13:26,480 --> 00:13:29,360
as the defense mechanism is very

297
00:13:29,360 --> 00:13:30,639
straightforward

298
00:13:30,639 --> 00:13:35,360
uh it can be done in two steps first we

299
00:13:35,360 --> 00:13:38,160
put first we conduct uh meta learning we

300
00:13:38,160 --> 00:13:38,880
can that meant

301
00:13:38,880 --> 00:13:41,279
meta learning on the training set and

302
00:13:41,279 --> 00:13:42,000
then we can

303
00:13:42,000 --> 00:13:45,040
obtain a meta initialization model file

304
00:13:45,040 --> 00:13:46,959
here

305
00:13:46,959 --> 00:13:50,079
and we can see that the training the

306
00:13:50,079 --> 00:13:51,360
training data set here

307
00:13:51,360 --> 00:13:54,720
already have poisonous samples

308
00:13:54,720 --> 00:13:58,320
here then we will use the initialization

309
00:13:58,320 --> 00:13:59,279
model

310
00:13:59,279 --> 00:14:02,560
to initial our task and

311
00:14:02,560 --> 00:14:05,680
and and do and conduct with training

312
00:14:05,680 --> 00:14:06,560
using the

313
00:14:06,560 --> 00:14:10,638
original training set

314
00:14:11,120 --> 00:14:15,120
and from the from this trend model

315
00:14:15,120 --> 00:14:18,160
we have to output we here we can see

316
00:14:18,160 --> 00:14:21,760
we can see here the first output is the

317
00:14:21,760 --> 00:14:25,519
probability uh is the probability vector

318
00:14:25,519 --> 00:14:26,000
of the

319
00:14:26,000 --> 00:14:29,440
text of a text sample which indicates

320
00:14:29,440 --> 00:14:31,839
the probability

321
00:14:31,839 --> 00:14:35,519
of uh for different classes

322
00:14:35,519 --> 00:14:38,800
and then we also have another uh output

323
00:14:38,800 --> 00:14:40,000
here

324
00:14:40,000 --> 00:14:42,560
the output is the loss function of a

325
00:14:42,560 --> 00:14:43,120
text

326
00:14:43,120 --> 00:14:46,880
sample first we assume

327
00:14:46,880 --> 00:14:50,560
that the chain model here we obtain

328
00:14:50,560 --> 00:14:53,920
is not affected by the poisonous samples

329
00:14:53,920 --> 00:14:57,440
then naturally the loss function of a

330
00:14:57,440 --> 00:14:59,199
clean sample

331
00:14:59,199 --> 00:15:02,240
will be very small while the loss

332
00:15:02,240 --> 00:15:05,440
value of a poisonous sample is very

333
00:15:05,440 --> 00:15:06,160
large

334
00:15:06,160 --> 00:15:09,160
so we can simply use a threshold to

335
00:15:09,160 --> 00:15:10,320
differentiate

336
00:15:10,320 --> 00:15:14,560
a clean and poisonous sample

337
00:15:17,600 --> 00:15:21,199
after proposing our defense approaches

338
00:15:21,199 --> 00:15:23,839
we evaluated its performance on

339
00:15:23,839 --> 00:15:25,760
different applications

340
00:15:25,760 --> 00:15:28,959
including image classification

341
00:15:28,959 --> 00:15:31,920
document classification intrusion

342
00:15:31,920 --> 00:15:33,040
detection

343
00:15:33,040 --> 00:15:36,240
and malware detection

344
00:15:36,240 --> 00:15:39,279
and we use the most popular dataset in

345
00:15:39,279 --> 00:15:40,000
each state

346
00:15:40,000 --> 00:15:44,000
in each domain for evaluation

347
00:15:44,000 --> 00:15:47,199
and additionally we also compile

348
00:15:47,199 --> 00:15:51,600
our defense mechanism against to

349
00:15:51,600 --> 00:15:54,839
to baseline one is a random

350
00:15:54,839 --> 00:15:56,720
initialization method

351
00:15:56,720 --> 00:15:59,839
and the other is uh called

352
00:15:59,839 --> 00:16:03,360
dp method which is the state of the art

353
00:16:03,360 --> 00:16:06,839
to defense against data poisoning

354
00:16:06,839 --> 00:16:09,839
attacks

355
00:16:12,639 --> 00:16:15,680
and now let's see how effective our

356
00:16:15,680 --> 00:16:17,600
proposed attack is

357
00:16:17,600 --> 00:16:19,920
throughout this project we will use the

358
00:16:19,920 --> 00:16:21,360
area under

359
00:16:21,360 --> 00:16:25,120
precision we call curb score

360
00:16:25,120 --> 00:16:28,720
au aupr score

361
00:16:28,720 --> 00:16:32,480
as our evaluation metric and we can see

362
00:16:32,480 --> 00:16:35,360
from the two table here the detection

363
00:16:35,360 --> 00:16:36,000
results

364
00:16:36,000 --> 00:16:39,440
on poisonous samples in uh intrusion

365
00:16:39,440 --> 00:16:40,720
detection domain

366
00:16:40,720 --> 00:16:44,480
also on the left table and results in

367
00:16:44,480 --> 00:16:45,199
malware

368
00:16:45,199 --> 00:16:48,639
detection domain uh is uh in the

369
00:16:48,639 --> 00:16:53,440
right table and we and

370
00:16:53,440 --> 00:16:56,399
and for the and for the different magnet

371
00:16:56,399 --> 00:16:56,880
model

372
00:16:56,880 --> 00:17:00,320
here they correspond to

373
00:17:00,320 --> 00:17:04,000
uh to the uh to the magnet model with

374
00:17:04,000 --> 00:17:07,599
different meta training uh meta training

375
00:17:07,599 --> 00:17:08,959
iterations

376
00:17:08,959 --> 00:17:12,959
for example uh magnet 100 corresponds to

377
00:17:12,959 --> 00:17:16,079
our model with 100 meter training

378
00:17:16,079 --> 00:17:17,679
iteration

379
00:17:17,679 --> 00:17:21,760
and we can see that our models achieve

380
00:17:21,760 --> 00:17:22,559
higher

381
00:17:22,559 --> 00:17:26,240
aopi scores consistently in list two

382
00:17:26,240 --> 00:17:27,280
domain in this

383
00:17:27,280 --> 00:17:30,160
domain and if we look at the male well

384
00:17:30,160 --> 00:17:31,600
detection domain

385
00:17:31,600 --> 00:17:34,080
actually the the state of the rms the

386
00:17:34,080 --> 00:17:35,600
dpms

387
00:17:35,600 --> 00:17:38,559
achieve achieve a lower aopr score than

388
00:17:38,559 --> 00:17:40,799
the random initialized

389
00:17:40,799 --> 00:17:47,039
random initialization model

390
00:17:47,039 --> 00:17:50,640
and we also test our our approach in the

391
00:17:50,640 --> 00:17:52,559
computer vision domain

392
00:17:52,559 --> 00:17:55,120
to see if it can detect points in the

393
00:17:55,120 --> 00:17:55,840
samples

394
00:17:55,840 --> 00:17:59,039
uh from the attack as well and

395
00:17:59,039 --> 00:18:02,080
for the for the row here uh

396
00:18:02,080 --> 00:18:04,559
the row here is the ratio of poisonous

397
00:18:04,559 --> 00:18:06,400
samples in the training set

398
00:18:06,400 --> 00:18:10,160
in the training set and

399
00:18:10,160 --> 00:18:13,280
and the results here again show that

400
00:18:13,280 --> 00:18:16,480
our approach outperformed to baseline

401
00:18:16,480 --> 00:18:17,919
consistently

402
00:18:17,919 --> 00:18:20,640
so indicate that our approach can be

403
00:18:20,640 --> 00:18:22,080
applied to

404
00:18:22,080 --> 00:18:29,840
a wide range of machine learning models

405
00:18:32,320 --> 00:18:35,280
after introducing using my research on

406
00:18:35,280 --> 00:18:36,880
security attacks in

407
00:18:36,880 --> 00:18:40,400
in previous part naturally i would like

408
00:18:40,400 --> 00:18:41,440
to talk about

409
00:18:41,440 --> 00:18:43,440
privacy attacks on machine learning

410
00:18:43,440 --> 00:18:44,720
model as well

411
00:18:44,720 --> 00:18:48,000
so in part two i will discuss our

412
00:18:48,000 --> 00:18:50,320
research to defend against

413
00:18:50,320 --> 00:18:53,120
membership influence attacks on

414
00:18:53,120 --> 00:18:57,600
federated meta learning systems

415
00:18:57,600 --> 00:18:59,919
and remember that in part one we

416
00:18:59,919 --> 00:19:01,200
mentioned

417
00:19:01,200 --> 00:19:05,120
one notable trend of iot devices

418
00:19:05,120 --> 00:19:08,559
that in recent years

419
00:19:08,559 --> 00:19:12,720
iot devices are estimated to grow

420
00:19:12,720 --> 00:19:16,240
substantially and and the trend is not

421
00:19:16,240 --> 00:19:17,600
stopping

422
00:19:17,600 --> 00:19:20,799
actually another secular trend of iot

423
00:19:20,799 --> 00:19:24,080
devices is that their capabilities are

424
00:19:24,080 --> 00:19:26,480
increasingly diverse as well

425
00:19:26,480 --> 00:19:29,360
and this is actually very exciting

426
00:19:29,360 --> 00:19:30,559
because uh

427
00:19:30,559 --> 00:19:34,160
iot devices are used today are used

428
00:19:34,160 --> 00:19:37,120
not just for data collection but also

429
00:19:37,120 --> 00:19:38,080
for advanced

430
00:19:38,080 --> 00:19:41,760
tasks uh including learning

431
00:19:41,760 --> 00:19:45,440
so in other words the next generation of

432
00:19:45,440 --> 00:19:47,520
iot devices are

433
00:19:47,520 --> 00:19:50,320
actually expected to equip with ai or

434
00:19:50,320 --> 00:19:51,360
machine learning

435
00:19:51,360 --> 00:19:54,960
technologies and this uh transcends

436
00:19:54,960 --> 00:19:58,240
into what we call aiot

437
00:19:58,240 --> 00:20:00,640
error

438
00:20:01,360 --> 00:20:05,679
so in aiot we envision that

439
00:20:05,679 --> 00:20:08,480
federated metal learning may be the more

440
00:20:08,480 --> 00:20:10,000
suitable learning

441
00:20:10,000 --> 00:20:13,120
paradigm than the then the traditional

442
00:20:13,120 --> 00:20:14,480
federal learning

443
00:20:14,480 --> 00:20:17,200
than the traditional federal and the

444
00:20:17,200 --> 00:20:17,840
difference

445
00:20:17,840 --> 00:20:21,200
between these two learning paradigms

446
00:20:21,200 --> 00:20:24,320
is that the goal of federated meta

447
00:20:24,320 --> 00:20:28,000
learning is to learn an initialization

448
00:20:28,000 --> 00:20:29,120
model

449
00:20:29,120 --> 00:20:31,679
while federated learning is just to

450
00:20:31,679 --> 00:20:33,760
learn a machine learning model

451
00:20:33,760 --> 00:20:37,120
for one specific task

452
00:20:37,120 --> 00:20:40,400
so obviously the initialization model

453
00:20:40,400 --> 00:20:40,799
from

454
00:20:40,799 --> 00:20:44,720
federated metal learning has a much

455
00:20:44,720 --> 00:20:48,000
greater application potential because

456
00:20:48,000 --> 00:20:51,039
it can be used for many many tasks as

457
00:20:51,039 --> 00:20:51,679
long as

458
00:20:51,679 --> 00:20:55,679
they are similar and more importantly we

459
00:20:55,679 --> 00:20:58,880
have two other intuition for federated

460
00:20:58,880 --> 00:21:00,480
metal learning

461
00:21:00,480 --> 00:21:03,760
the first one is that um participants

462
00:21:03,760 --> 00:21:08,000
in uh in distributed learning system

463
00:21:08,000 --> 00:21:11,039
do not necessarily have a lot of data

464
00:21:11,039 --> 00:21:14,480
to train a local model uh which are

465
00:21:14,480 --> 00:21:15,840
meaningful

466
00:21:15,840 --> 00:21:18,480
for the global model in federated

467
00:21:18,480 --> 00:21:19,760
learning

468
00:21:19,760 --> 00:21:23,360
and in comparison uh federated metal

469
00:21:23,360 --> 00:21:24,080
learning

470
00:21:24,080 --> 00:21:27,440
does not have had such a requirement on

471
00:21:27,440 --> 00:21:30,960
participants data set i mean

472
00:21:30,960 --> 00:21:33,760
federated metal learning do not require

473
00:21:33,760 --> 00:21:36,480
other participants to have a lot of

474
00:21:36,480 --> 00:21:40,720
training data to generate a

475
00:21:40,720 --> 00:21:44,559
strong local model and

476
00:21:44,559 --> 00:21:47,600
the other uh the other intuition we have

477
00:21:47,600 --> 00:21:49,919
is that

478
00:21:49,919 --> 00:21:53,200
it's not wise to assume the sample

479
00:21:53,200 --> 00:21:54,480
distributions

480
00:21:54,480 --> 00:21:58,880
in all participants are almost the same

481
00:21:58,880 --> 00:22:02,880
and including uh the data distribution

482
00:22:02,880 --> 00:22:07,280
uh at the future model consumers

483
00:22:07,280 --> 00:22:10,480
so in this way we got it will make more

484
00:22:10,480 --> 00:22:11,039
sense

485
00:22:11,039 --> 00:22:15,120
that to assume that the participants

486
00:22:15,120 --> 00:22:16,240
data set

487
00:22:16,240 --> 00:22:19,520
only share some similarities and

488
00:22:19,520 --> 00:22:22,720
we can use federated metal learning

489
00:22:22,720 --> 00:22:26,400
to extract this similarity and

490
00:22:26,400 --> 00:22:30,000
and use them for for

491
00:22:30,000 --> 00:22:33,600
model initialization

492
00:22:34,320 --> 00:22:38,320
so um although federated metal learnings

493
00:22:38,320 --> 00:22:41,280
has the tip has some difference from

494
00:22:41,280 --> 00:22:43,039
federated learning

495
00:22:43,039 --> 00:22:46,159
uh one common feature among list two is

496
00:22:46,159 --> 00:22:46,640
that

497
00:22:46,640 --> 00:22:49,919
these two systems will be uh

498
00:22:49,919 --> 00:22:53,679
subject to privacy attacks such as a

499
00:22:53,679 --> 00:22:56,640
membership influence or property

500
00:22:56,640 --> 00:22:58,320
influencer tax

501
00:22:58,320 --> 00:23:01,520
and in our work we focus on

502
00:23:01,520 --> 00:23:04,559
membership influence attack in such

503
00:23:04,559 --> 00:23:06,240
attacks the attackers

504
00:23:06,240 --> 00:23:09,600
use the published initial initialization

505
00:23:09,600 --> 00:23:11,039
model

506
00:23:11,039 --> 00:23:14,240
initialization model to infer whether

507
00:23:14,240 --> 00:23:17,760
a specific user or a specific

508
00:23:17,760 --> 00:23:20,840
samples participate the meta training

509
00:23:20,840 --> 00:23:23,840
process

510
00:23:26,159 --> 00:23:30,080
and naturally to defend against

511
00:23:30,080 --> 00:23:33,120
membership influence attacks we

512
00:23:33,120 --> 00:23:37,120
we will use the popular differential

513
00:23:37,120 --> 00:23:38,000
privacy

514
00:23:38,000 --> 00:23:41,039
mechanism and and incorporate

515
00:23:41,039 --> 00:23:43,360
into the model training process of

516
00:23:43,360 --> 00:23:44,799
federated met

517
00:23:44,799 --> 00:23:48,080
meta learning systems

518
00:23:48,240 --> 00:23:50,799
and first i would like to uh i would

519
00:23:50,799 --> 00:23:53,440
like to briefly introduce the training

520
00:23:53,440 --> 00:23:54,559
process of

521
00:23:54,559 --> 00:23:58,159
federated meta learning system

522
00:23:58,159 --> 00:24:00,720
specifically uh we actually use one of

523
00:24:00,720 --> 00:24:02,159
the most popular

524
00:24:02,159 --> 00:24:05,120
meta learning schemes called memo in our

525
00:24:05,120 --> 00:24:08,000
in our system

526
00:24:08,000 --> 00:24:11,200
and we call that we have uh and we call

527
00:24:11,200 --> 00:24:12,000
that we have

528
00:24:12,000 --> 00:24:15,200
a participant and central server in

529
00:24:15,200 --> 00:24:18,320
a federated meta learning system

530
00:24:18,320 --> 00:24:22,159
and at the participant site she or he

531
00:24:22,159 --> 00:24:25,200
will conduct local learning will conduct

532
00:24:25,200 --> 00:24:26,320
local learning

533
00:24:26,320 --> 00:24:30,000
and obtain a local model for example

534
00:24:30,000 --> 00:24:34,080
theater i hear then

535
00:24:34,080 --> 00:24:36,880
then sure he will upload the local

536
00:24:36,880 --> 00:24:39,679
gradient to the central server here

537
00:24:39,679 --> 00:24:41,679
to the central server and the central

538
00:24:41,679 --> 00:24:43,520
server will aggregate

539
00:24:43,520 --> 00:24:46,960
many many gradients to update

540
00:24:46,960 --> 00:24:50,480
the uh the global model series

541
00:24:50,480 --> 00:24:54,640
here and at the end of meta training

542
00:24:54,640 --> 00:24:57,360
the the central server will publish the

543
00:24:57,360 --> 00:24:58,480
final meta

544
00:24:58,480 --> 00:25:01,919
model to the model consumer the model

545
00:25:01,919 --> 00:25:04,000
consumer will use the

546
00:25:04,000 --> 00:25:07,120
meta model theta to initial

547
00:25:07,120 --> 00:25:10,159
to initialize uh their own model

548
00:25:10,159 --> 00:25:13,679
and and do training on top list

549
00:25:13,679 --> 00:25:17,760
of of this metamodel

550
00:25:17,760 --> 00:25:21,520
so we can see that in this process

551
00:25:21,520 --> 00:25:25,200
in this process they they are actually

552
00:25:25,200 --> 00:25:27,840
two parts that could potentially lead to

553
00:25:27,840 --> 00:25:30,959
privacy leakage

554
00:25:31,279 --> 00:25:34,640
in the first case for case a

555
00:25:34,640 --> 00:25:37,679
the meta the the central server will

556
00:25:37,679 --> 00:25:38,960
publish

557
00:25:38,960 --> 00:25:42,240
the metamodel theater to the model

558
00:25:42,240 --> 00:25:43,360
consumers

559
00:25:43,360 --> 00:25:46,960
and the model consumer can use

560
00:25:46,960 --> 00:25:50,000
silt silta to infer

561
00:25:50,000 --> 00:25:54,080
to infer the existence of

562
00:25:54,080 --> 00:25:57,200
for example of satai the existence the

563
00:25:57,200 --> 00:26:00,720
existence of one specific task

564
00:26:00,720 --> 00:26:04,480
and for case b so we can see that the

565
00:26:04,480 --> 00:26:06,240
participants will

566
00:26:06,240 --> 00:26:09,520
will submit a silta siltai

567
00:26:09,520 --> 00:26:13,039
to the central server and actually the

568
00:26:13,039 --> 00:26:14,799
central server can

569
00:26:14,799 --> 00:26:19,200
use theta i to infer the existence of a

570
00:26:19,200 --> 00:26:21,840
specific training sample

571
00:26:21,840 --> 00:26:26,080
in setai so based on these two

572
00:26:26,080 --> 00:26:29,279
observations we propose to add

573
00:26:29,279 --> 00:26:31,279
differential privacy

574
00:26:31,279 --> 00:26:34,480
in list two parts in these two parts

575
00:26:34,480 --> 00:26:39,840
for to achieve privacy protection

576
00:26:42,159 --> 00:26:45,919
so so naturally we have two privacy

577
00:26:45,919 --> 00:26:49,360
levels for case a we call it

578
00:26:49,360 --> 00:26:53,840
a task level dp we have

579
00:26:53,840 --> 00:26:57,200
we use this uh list dp to pro

580
00:26:57,200 --> 00:27:00,320
to prevent the model consumers from

581
00:27:00,320 --> 00:27:03,360
informing membership from employee

582
00:27:03,360 --> 00:27:04,080
membership

583
00:27:04,080 --> 00:27:07,200
of a certain task of a certain type

584
00:27:07,200 --> 00:27:10,960
and for case b we have the dual level

585
00:27:10,960 --> 00:27:12,960
differential privacy

586
00:27:12,960 --> 00:27:16,159
and it will not only prevent the model

587
00:27:16,159 --> 00:27:17,200
consumer

588
00:27:17,200 --> 00:27:20,480
to infer each uh clt

589
00:27:20,480 --> 00:27:22,960
eye and you will also prevent the

590
00:27:22,960 --> 00:27:24,240
central server

591
00:27:24,240 --> 00:27:27,279
from info from informing

592
00:27:27,279 --> 00:27:30,240
the existence of a specific training

593
00:27:30,240 --> 00:27:31,120
sample

594
00:27:31,120 --> 00:27:35,520
contributing to each silta eye

595
00:27:35,520 --> 00:27:38,960
and corresponded uh to protect uh

596
00:27:38,960 --> 00:27:42,640
the privacy uh within silta we

597
00:27:42,640 --> 00:27:45,919
add a differential privacy to it here

598
00:27:45,919 --> 00:27:49,200
in this stage and and then

599
00:27:49,200 --> 00:27:52,720
to protect the privacy within celtai

600
00:27:52,720 --> 00:27:55,840
we we also add a differential privacy

601
00:27:55,840 --> 00:27:56,720
here

602
00:27:56,720 --> 00:28:03,279
uh to protect co2

603
00:28:03,279 --> 00:28:06,799
and after proposing our defense

604
00:28:06,799 --> 00:28:09,919
approaches we evaluate its performance

605
00:28:09,919 --> 00:28:13,440
on several more on the most popular

606
00:28:13,440 --> 00:28:16,720
data set in field short learning domain

607
00:28:16,720 --> 00:28:19,919
and additionally we also compile

608
00:28:19,919 --> 00:28:22,960
our our approach against

609
00:28:22,960 --> 00:28:27,760
uh against one baseline

610
00:28:27,760 --> 00:28:31,440
gbml and this is the state of the

611
00:28:31,440 --> 00:28:35,679
art of differential private federated

612
00:28:35,679 --> 00:28:40,000
metal learning schemes in the literature

613
00:28:40,000 --> 00:28:43,039
and the show and and the data sets we

614
00:28:43,039 --> 00:28:43,520
use

615
00:28:43,520 --> 00:28:46,559
in for our evaluation

616
00:28:46,559 --> 00:28:49,200
the first one is a ohmic law and the

617
00:28:49,200 --> 00:28:50,080
second one

618
00:28:50,080 --> 00:28:53,440
is a cipher fs and the third one

619
00:28:53,440 --> 00:28:59,840
is the mini image net

620
00:29:00,159 --> 00:29:03,200
now let's see how effective our our

621
00:29:03,200 --> 00:29:05,600
proposed approach is

622
00:29:05,600 --> 00:29:08,320
throughout this part we will use the

623
00:29:08,320 --> 00:29:09,760
in-way case short

624
00:29:09,760 --> 00:29:14,960
accuracy as our evaluation matches

625
00:29:14,960 --> 00:29:17,440
away k short denotes that the learning

626
00:29:17,440 --> 00:29:18,159
task

627
00:29:18,159 --> 00:29:20,240
for the for the specific machine

628
00:29:20,240 --> 00:29:23,200
learning machine learning model

629
00:29:23,200 --> 00:29:26,320
it has earned classes and each classes

630
00:29:26,320 --> 00:29:29,840
has a k samples for example uh

631
00:29:29,840 --> 00:29:32,080
five way one shot means that the machine

632
00:29:32,080 --> 00:29:33,360
learning model

633
00:29:33,360 --> 00:29:36,880
has five classes and in the trainings

634
00:29:36,880 --> 00:29:39,440
and in the training set each classes has

635
00:29:39,440 --> 00:29:40,080
only

636
00:29:40,080 --> 00:29:44,320
one training sample and sort the same to

637
00:29:44,320 --> 00:29:48,720
uh the other away casual setting

638
00:29:48,720 --> 00:29:53,440
and and here we evaluate different

639
00:29:53,440 --> 00:29:56,559
different methods including

640
00:29:56,559 --> 00:30:00,240
random initialization non-private

641
00:30:00,240 --> 00:30:05,200
and list 2 dpa gi and dpa gilr

642
00:30:05,200 --> 00:30:08,559
are the two methods we propose for

643
00:30:08,559 --> 00:30:11,919
case a and case b and also here we have

644
00:30:11,919 --> 00:30:14,399
the gbml

645
00:30:14,399 --> 00:30:16,720
and for the random initialization it

646
00:30:16,720 --> 00:30:18,720
means

647
00:30:18,720 --> 00:30:21,600
it means doing machine learning without

648
00:30:21,600 --> 00:30:22,480
any

649
00:30:22,480 --> 00:30:25,840
fuel approach future approach

650
00:30:25,840 --> 00:30:29,279
and for this non-private non-private one

651
00:30:29,279 --> 00:30:32,960
it means adopting our federating

652
00:30:32,960 --> 00:30:36,240
meta learning system without adding any

653
00:30:36,240 --> 00:30:38,240
differential privacy

654
00:30:38,240 --> 00:30:41,200
and we can see from this figure that

655
00:30:41,200 --> 00:30:43,919
when there's only a field trending set

656
00:30:43,919 --> 00:30:47,039
a field training sample at the

657
00:30:47,039 --> 00:30:48,640
participants

658
00:30:48,640 --> 00:30:51,840
actually we can only obtain a very

659
00:30:51,840 --> 00:30:55,120
low classification accuracy

660
00:30:55,120 --> 00:30:58,960
and then when we adopt some future

661
00:30:58,960 --> 00:31:02,320
learning method for example memo then

662
00:31:02,320 --> 00:31:05,120
the accuracy immediately goes up to very

663
00:31:05,120 --> 00:31:05,600
high

664
00:31:05,600 --> 00:31:09,440
here and of course

665
00:31:09,440 --> 00:31:13,519
when we would like to use some privacy

666
00:31:13,519 --> 00:31:15,200
protection mechanism

667
00:31:15,200 --> 00:31:18,399
we need to sacrifice a little bit

668
00:31:18,399 --> 00:31:21,600
the classification accuracy and we can

669
00:31:21,600 --> 00:31:22,559
see that

670
00:31:22,559 --> 00:31:25,679
for case a our accuracy will drop a

671
00:31:25,679 --> 00:31:26,640
little bit

672
00:31:26,640 --> 00:31:29,760
and if we would like to protect more

673
00:31:29,760 --> 00:31:32,880
privacy then the accuracy will

674
00:31:32,880 --> 00:31:35,919
also decrease will also

675
00:31:35,919 --> 00:31:39,279
decrease decrease further

676
00:31:39,279 --> 00:31:43,120
and however english in this figure we

677
00:31:43,120 --> 00:31:46,159
in this table we can see that our method

678
00:31:46,159 --> 00:31:46,960
can still

679
00:31:46,960 --> 00:31:50,399
achieve a higher accuracy

680
00:31:50,399 --> 00:31:53,440
than the state of the art and

681
00:31:53,440 --> 00:31:56,000
and folders and for the suite method for

682
00:31:56,000 --> 00:31:57,760
the three method with

683
00:31:57,760 --> 00:32:01,360
privacy protection their privacy

684
00:32:01,360 --> 00:32:04,719
batteries are similar

685
00:32:08,320 --> 00:32:11,760
and we also plot the

686
00:32:11,760 --> 00:32:15,039
the accuracy curve of different model on

687
00:32:15,039 --> 00:32:15,440
two

688
00:32:15,440 --> 00:32:18,080
data set one is the omegle and the other

689
00:32:18,080 --> 00:32:19,200
is the mini

690
00:32:19,200 --> 00:32:22,320
image net for a better

691
00:32:22,320 --> 00:32:26,480
illustration and we can see that

692
00:32:26,480 --> 00:32:29,600
we can see that of course the the

693
00:32:29,600 --> 00:32:32,080
non-private federated metal learning

694
00:32:32,080 --> 00:32:33,919
achieve the highest

695
00:32:33,919 --> 00:32:37,039
uh classification accuracy on list two

696
00:32:37,039 --> 00:32:38,320
data set

697
00:32:38,320 --> 00:32:42,799
and and still uh our two methods

698
00:32:42,799 --> 00:32:46,039
one is the dpati and the other is the

699
00:32:46,039 --> 00:32:47,600
dpatrl

700
00:32:47,600 --> 00:32:50,640
achieve higher accuracy than the state

701
00:32:50,640 --> 00:32:51,840
of the art

702
00:32:51,840 --> 00:32:54,960
and within our two methods a

703
00:32:54,960 --> 00:32:58,960
dpagr has a higher privacy because it

704
00:32:58,960 --> 00:33:00,000
only

705
00:33:00,000 --> 00:33:03,039
achieved the task level it does not

706
00:33:03,039 --> 00:33:04,240
prevent the

707
00:33:04,240 --> 00:33:07,679
central server from informing the

708
00:33:07,679 --> 00:33:10,399
existing of a specific sample

709
00:33:10,399 --> 00:33:11,919
contributing to

710
00:33:11,919 --> 00:33:15,679
a specific co2 eye

711
00:33:19,200 --> 00:33:22,399
yeah with that i'd like to open up for q

712
00:33:22,399 --> 00:33:22,799
a

713
00:33:22,799 --> 00:33:25,600
and i'd like to thank you all for having

714
00:33:25,600 --> 00:33:39,200
me here today

715
00:33:39,200 --> 00:33:43,679
all right uh you mean thank you so much

716
00:33:44,480 --> 00:33:48,000
it was a nice talk it was uh

717
00:33:48,000 --> 00:33:50,000
very related to the course that i am

718
00:33:50,000 --> 00:33:51,440
teaching

719
00:33:51,440 --> 00:33:54,799
okay really i'm teaching a security

720
00:33:54,799 --> 00:33:56,159
analytics course

721
00:33:56,159 --> 00:33:59,200
which we cover in the first half of the

722
00:33:59,200 --> 00:34:00,159
course

723
00:34:00,159 --> 00:34:02,399
the common machine learning algorithms

724
00:34:02,399 --> 00:34:03,519
and in the second

725
00:34:03,519 --> 00:34:07,039
half of the course basically we covered

726
00:34:07,039 --> 00:34:07,919
the

727
00:34:07,919 --> 00:34:10,480
recent attacks to the machine learning

728
00:34:10,480 --> 00:34:11,918
systems

729
00:34:11,918 --> 00:34:15,359
basically the systems that build

730
00:34:15,359 --> 00:34:19,119
on uh that that integrates machinery

731
00:34:19,119 --> 00:34:21,839
such as we cover evasion attacks

732
00:34:21,839 --> 00:34:23,119
eventually attack

733
00:34:23,119 --> 00:34:26,639
yes we cover you know like white bugs

734
00:34:26,639 --> 00:34:28,639
and black box attacks

735
00:34:28,639 --> 00:34:31,359
additionally we cover some privacy

736
00:34:31,359 --> 00:34:32,079
issues

737
00:34:32,079 --> 00:34:34,159
as you mentioned membership inference

738
00:34:34,159 --> 00:34:36,879
attack model inversion attack

739
00:34:36,879 --> 00:34:40,000
and also uh because it's a hot topic

740
00:34:40,000 --> 00:34:41,839
sometimes i cover

741
00:34:41,839 --> 00:34:44,239
differential privacy and generative

742
00:34:44,239 --> 00:34:45,760
adversarial networks

743
00:34:45,760 --> 00:34:48,800
as well okay so

744
00:34:48,800 --> 00:34:50,639
your research is like kind of like

745
00:34:50,639 --> 00:34:52,239
really related to

746
00:34:52,239 --> 00:34:53,839
you know like the course materials that

747
00:34:53,839 --> 00:34:55,520
i'm teaching

748
00:34:55,520 --> 00:34:59,440
okay sure um so we have some questions

749
00:34:59,440 --> 00:34:59,920
in the

750
00:34:59,920 --> 00:35:03,200
q a if you click you will be able to see

751
00:35:03,200 --> 00:35:04,320
the question

752
00:35:04,320 --> 00:35:08,960
okay so uh maybe i will first

753
00:35:08,960 --> 00:35:11,920
read the question to the audience i

754
00:35:11,920 --> 00:35:13,599
don't know if the audience can see the

755
00:35:13,599 --> 00:35:14,839
question

756
00:35:14,839 --> 00:35:17,680
uh and then i will try i will try to

757
00:35:17,680 --> 00:35:18,640
answer that so

758
00:35:18,640 --> 00:35:21,920
yeah uh and the

759
00:35:21,920 --> 00:35:24,960
the question is from chris uh clinton

760
00:35:24,960 --> 00:35:28,320
and uh and the question is what ibsen

761
00:35:28,320 --> 00:35:31,680
are you using with differential privacy

762
00:35:31,680 --> 00:35:35,440
and how are you calculating sensitivity

763
00:35:35,440 --> 00:35:37,839
for images

764
00:35:37,839 --> 00:35:41,599
yeah that's a good question and

765
00:35:41,599 --> 00:35:44,800
and for the uh we are we are using the

766
00:35:44,800 --> 00:35:47,760
epson delta differential privacy

767
00:35:47,760 --> 00:35:51,520
and if i remember that correctly for the

768
00:35:51,520 --> 00:35:55,200
for the epson we are using uh

769
00:35:55,200 --> 00:35:59,440
the values between

770
00:35:59,440 --> 00:36:03,119
between 10 to -4

771
00:36:03,119 --> 00:36:06,400
to 10 to uh to

772
00:36:06,400 --> 00:36:09,599
10 to -5 yeah

773
00:36:09,599 --> 00:36:13,680
10 to -4 to to 95 and for the

774
00:36:13,680 --> 00:36:16,720
for the delta uh i remember is

775
00:36:16,720 --> 00:36:20,880
uh between uh

776
00:36:20,880 --> 00:36:24,480
it should be less than 0.2 if

777
00:36:24,480 --> 00:36:27,119
if i remember that correctly and

778
00:36:27,119 --> 00:36:28,800
actually for the

779
00:36:28,800 --> 00:36:32,560
for the uh for the for the related work

780
00:36:32,560 --> 00:36:34,000
we are referring to the

781
00:36:34,000 --> 00:36:38,560
uh the gvml their privacy battery is uh

782
00:36:38,560 --> 00:36:41,760
the the epsilon is 10 to the power of

783
00:36:41,760 --> 00:36:45,200
uh 10 to the power -3 and

784
00:36:45,200 --> 00:36:49,440
and i i i did not remember their delta

785
00:36:49,440 --> 00:36:53,520
so i mean they are using a much larger

786
00:36:53,520 --> 00:36:57,200
privacy budget but as you as we can see

787
00:36:57,200 --> 00:36:58,079
in the figure

788
00:36:58,079 --> 00:37:01,920
their accuracy is not that good

789
00:37:02,880 --> 00:37:06,079
and okay and the sorry the second types

790
00:37:06,079 --> 00:37:06,960
are

791
00:37:06,960 --> 00:37:09,359
how are you calculating sensitivity for

792
00:37:09,359 --> 00:37:10,480
images

793
00:37:10,480 --> 00:37:14,079
actually let me see i will show my

794
00:37:14,079 --> 00:37:23,839
screen here

795
00:37:31,359 --> 00:37:35,520
yeah so traditionally they are

796
00:37:35,520 --> 00:37:38,800
there are three uh three parts that can

797
00:37:38,800 --> 00:37:42,160
that we can add differential privacy uh

798
00:37:42,160 --> 00:37:45,359
into model training process uh we can

799
00:37:45,359 --> 00:37:45,760
add

800
00:37:45,760 --> 00:37:48,720
a differential privacy in the training

801
00:37:48,720 --> 00:37:49,440
sample

802
00:37:49,440 --> 00:37:52,240
in the training samples and then add

803
00:37:52,240 --> 00:37:54,960
differential privacy in

804
00:37:54,960 --> 00:37:58,079
in the in in in the gradients

805
00:37:58,079 --> 00:38:00,800
in the gradient computation and the and

806
00:38:00,800 --> 00:38:01,520
the last

807
00:38:01,520 --> 00:38:04,480
position is to add at differential

808
00:38:04,480 --> 00:38:06,320
privacy at the output

809
00:38:06,320 --> 00:38:10,000
and the most uh the mainstream

810
00:38:10,000 --> 00:38:14,000
uh method to incorporate differential

811
00:38:14,000 --> 00:38:15,359
privacy is to

812
00:38:15,359 --> 00:38:19,920
to add them into the into the gradients

813
00:38:19,920 --> 00:38:23,440
so as as you can see here for example

814
00:38:23,440 --> 00:38:27,119
for the task level dp for start level db

815
00:38:27,119 --> 00:38:30,960
uh the gi here denotes the gradient from

816
00:38:30,960 --> 00:38:34,000
each node from each node and these are

817
00:38:34,000 --> 00:38:35,359
these pi is the

818
00:38:35,359 --> 00:38:39,200
this is the noise that will generate the

819
00:38:39,200 --> 00:38:42,000
differential privacy protection

820
00:38:42,000 --> 00:38:46,560
and and basically the two parameter here

821
00:38:46,560 --> 00:38:49,760
for the say here is uh for the say here

822
00:38:49,760 --> 00:38:51,920
is a normalization factor

823
00:38:51,920 --> 00:38:54,480
and then for the say here say is will be

824
00:38:54,480 --> 00:38:55,359
related to

825
00:38:55,359 --> 00:38:58,560
the uh sensitivity you are mentioning

826
00:38:58,560 --> 00:39:01,359
so we did not care about whether the

827
00:39:01,359 --> 00:39:02,000
input is

828
00:39:02,000 --> 00:39:05,280
image or our text or

829
00:39:05,280 --> 00:39:08,320
our arbitrary input uh we only look at

830
00:39:08,320 --> 00:39:09,760
the gradients here

831
00:39:09,760 --> 00:39:13,760
so we we can imagine that c should be

832
00:39:13,760 --> 00:39:14,960
proportional

833
00:39:14,960 --> 00:39:18,560
to the value of gi here because uh it

834
00:39:18,560 --> 00:39:19,200
will

835
00:39:19,200 --> 00:39:23,440
actually try to cover the impact of ti

836
00:39:23,440 --> 00:39:25,839
yeah i don't know if i answer the

837
00:39:25,839 --> 00:39:29,839
question or not

838
00:39:41,280 --> 00:39:44,880
let me see if there are other

839
00:39:44,880 --> 00:39:46,880
yeah professor chris clifton actually

840
00:39:46,880 --> 00:39:49,280
posted some more comments to the chat

841
00:39:49,280 --> 00:39:53,680
uh okay yeah for uh for the

842
00:39:53,680 --> 00:39:55,440
attention to the uh you know like the

843
00:39:55,440 --> 00:39:57,280
participants here

844
00:39:57,280 --> 00:39:59,680
essentially all right so you don't need

845
00:39:59,680 --> 00:40:00,480
to read that

846
00:40:00,480 --> 00:40:03,440
okay it's just an additional comment for

847
00:40:03,440 --> 00:40:04,960
your information

848
00:40:04,960 --> 00:40:08,880
thank you professor kington

849
00:40:08,880 --> 00:40:12,000
okay all right

850
00:40:12,000 --> 00:40:14,000
are there any other questions from the

851
00:40:14,000 --> 00:40:16,720
participants

852
00:40:22,319 --> 00:40:24,560
all right done uh you mean thank you so

853
00:40:24,560 --> 00:40:27,040
much again for your interesting talk

854
00:40:27,040 --> 00:40:31,119
and i hope that uh we see you personally

855
00:40:31,119 --> 00:40:34,160
yeah yeah it produced some time you know

856
00:40:34,160 --> 00:40:34,560
like

857
00:40:34,560 --> 00:40:36,160
probably probably like after the

858
00:40:36,160 --> 00:40:38,560
epidemic yeah actually i

859
00:40:38,560 --> 00:40:41,520
one of my family members went to purdue

860
00:40:41,520 --> 00:40:44,560
for his phd

861
00:40:44,720 --> 00:40:49,040
in information engineering department

862
00:40:49,040 --> 00:40:51,520
in industry engineering not information

863
00:40:51,520 --> 00:40:53,359
in industry engineering department

864
00:40:53,359 --> 00:40:56,400
i see industrial engineering yeah all

865
00:40:56,400 --> 00:40:57,680
right

866
00:40:57,680 --> 00:41:00,079
again thank you so much everyone for

867
00:41:00,079 --> 00:41:01,040
attending

868
00:41:01,040 --> 00:41:04,240
and i will see you next week in another

869
00:41:04,240 --> 00:41:07,359
series seminar

870
00:41:07,359 --> 00:41:11,119
thank you for having me here today take

871
00:41:18,839 --> 00:41:21,839
care

872
00:41:27,200 --> 00:41:29,279
you

