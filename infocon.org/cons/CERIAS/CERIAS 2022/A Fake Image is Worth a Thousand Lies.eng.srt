1
00:00:00,160 --> 00:00:02,240
good afternoon welcome to the serious

2
00:00:02,240 --> 00:00:04,720
weekly seminar today we're excited to

3
00:00:04,720 --> 00:00:07,759
welcome cindy shen uh dr shen is a

4
00:00:07,759 --> 00:00:10,320
professor of communication at uc davis

5
00:00:10,320 --> 00:00:12,719
her research focuses on understanding

6
00:00:12,719 --> 00:00:15,040
visual and misinformation

7
00:00:15,040 --> 00:00:17,840
today cindy's talk is titled a fake

8
00:00:17,840 --> 00:00:20,400
image is worth a thousand lies cindy

9
00:00:20,400 --> 00:00:22,000
thank you for joining us please take it

10
00:00:22,000 --> 00:00:23,039
away

11
00:00:23,039 --> 00:00:25,680
okay thank you so much mike uh for this

12
00:00:25,680 --> 00:00:28,320
very generous introduction and for

13
00:00:28,320 --> 00:00:30,560
inviting me to share my work with all of

14
00:00:30,560 --> 00:00:34,559
you today at the serious talk series

15
00:00:34,559 --> 00:00:37,280
all right so

16
00:00:37,280 --> 00:00:40,399
let me introduce the backdrop of the

17
00:00:40,399 --> 00:00:43,200
research presented here um

18
00:00:43,200 --> 00:00:45,920
i i'm sure some of you have heard um the

19
00:00:45,920 --> 00:00:48,879
term fake news uh so in 2018 the journal

20
00:00:48,879 --> 00:00:51,280
science published a review piece which

21
00:00:51,280 --> 00:00:53,520
seems to suggest that the study of

22
00:00:53,520 --> 00:00:56,079
fake news or misinformation has entered

23
00:00:56,079 --> 00:00:57,680
into prominence

24
00:00:57,680 --> 00:00:59,920
in this piece they gave a formal

25
00:00:59,920 --> 00:01:02,640
definition of fake news

26
00:01:02,640 --> 00:01:04,799
so the term fake news

27
00:01:04,799 --> 00:01:06,720
as i'm sure some of you know was

28
00:01:06,720 --> 00:01:09,680
populated by the presidential candidate

29
00:01:09,680 --> 00:01:11,280
donald trump

30
00:01:11,280 --> 00:01:13,840
so this this phrase fake news has

31
00:01:13,840 --> 00:01:16,720
entered into public vocabulary right so

32
00:01:16,720 --> 00:01:18,000
we could casually refer to

33
00:01:18,000 --> 00:01:21,360
misinformation as fake news it can be

34
00:01:21,360 --> 00:01:23,840
understood as fabricated information

35
00:01:23,840 --> 00:01:26,720
that mimics news media content informed

36
00:01:26,720 --> 00:01:28,880
but not in organizational process or

37
00:01:28,880 --> 00:01:31,759
intent and fake news outlets in turn

38
00:01:31,759 --> 00:01:34,159
left the news media's editorial norms

39
00:01:34,159 --> 00:01:36,400
and processes for ensuring the accuracy

40
00:01:36,400 --> 00:01:39,280
and credibility of information

41
00:01:39,280 --> 00:01:41,759
but at the same time the term fake news

42
00:01:41,759 --> 00:01:45,040
carries political bias as trump himself

43
00:01:45,040 --> 00:01:47,439
repeatedly used fake news to describe

44
00:01:47,439 --> 00:01:49,920
anything that does not support his

45
00:01:49,920 --> 00:01:52,640
agenda right so in this infamous tweet

46
00:01:52,640 --> 00:01:55,439
he slammed reputable news outlets such

47
00:01:55,439 --> 00:01:58,320
as new york times as fake news obviously

48
00:01:58,320 --> 00:02:00,320
in an attempt to use fake news as a

49
00:02:00,320 --> 00:02:02,240
political weapon

50
00:02:02,240 --> 00:02:05,680
another example is this tweet where

51
00:02:05,680 --> 00:02:08,239
i think it's leading up to the 2020

52
00:02:08,239 --> 00:02:10,959
election he claimed that the poll should

53
00:02:10,959 --> 00:02:12,239
be fake

54
00:02:12,239 --> 00:02:14,480
because it shows he was losing in

55
00:02:14,480 --> 00:02:17,280
pennsylvania so anything that is against

56
00:02:17,280 --> 00:02:20,080
his agenda was you know termed as fake

57
00:02:20,080 --> 00:02:21,680
news

58
00:02:21,680 --> 00:02:24,400
so that the the trust or credibility of

59
00:02:24,400 --> 00:02:27,040
that new source is damaged

60
00:02:27,040 --> 00:02:29,599
for these reasons um the scholarly

61
00:02:29,599 --> 00:02:32,160
community tend to think that fake news

62
00:02:32,160 --> 00:02:34,400
itself is not like a very

63
00:02:34,400 --> 00:02:37,040
academic or neutral term instead they

64
00:02:37,040 --> 00:02:39,440
prefer to use misinformation which

65
00:02:39,440 --> 00:02:42,080
describes false or misleading

66
00:02:42,080 --> 00:02:43,519
information

67
00:02:43,519 --> 00:02:46,640
and another term is disinformation which

68
00:02:46,640 --> 00:02:49,040
describes false information that is

69
00:02:49,040 --> 00:02:51,040
purposely spread

70
00:02:51,040 --> 00:02:52,800
to deceive people

71
00:02:52,800 --> 00:02:55,040
now i mentioned the distinction between

72
00:02:55,040 --> 00:02:56,800
this information and this information

73
00:02:56,800 --> 00:02:59,040
because this distinction sometimes comes

74
00:02:59,040 --> 00:03:02,000
up a lot when we talk about visual

75
00:03:02,000 --> 00:03:04,480
misinformation which we'll cover later

76
00:03:04,480 --> 00:03:05,599
in this talk

77
00:03:05,599 --> 00:03:07,760
and i sometimes get asked you know which

78
00:03:07,760 --> 00:03:10,400
is worse is it is misinformation worse

79
00:03:10,400 --> 00:03:12,400
or is this information worse which one

80
00:03:12,400 --> 00:03:15,280
is the bigger threat to our society my

81
00:03:15,280 --> 00:03:18,480
answer to this is that they both are of

82
00:03:18,480 --> 00:03:21,239
course intent is important but once

83
00:03:21,239 --> 00:03:23,840
misinformation enters into our

84
00:03:23,840 --> 00:03:25,920
information ecosystem

85
00:03:25,920 --> 00:03:27,680
everybody loses

86
00:03:27,680 --> 00:03:31,440
because we now bear a much bigger cost

87
00:03:31,440 --> 00:03:34,080
to our information verification our

88
00:03:34,080 --> 00:03:36,560
trust of information is compromised as a

89
00:03:36,560 --> 00:03:39,920
result now we have to carefully verify

90
00:03:39,920 --> 00:03:42,080
everything right and in this process

91
00:03:42,080 --> 00:03:46,080
truth becomes collateral damage

92
00:03:46,080 --> 00:03:48,400
and i probably do not have to preach to

93
00:03:48,400 --> 00:03:50,560
this crowd how detrimental

94
00:03:50,560 --> 00:03:53,040
misinformation can be it damages our

95
00:03:53,040 --> 00:03:55,440
trust in public institutions it

96
00:03:55,440 --> 00:03:57,519
threatens democracy and it literally

97
00:03:57,519 --> 00:03:59,599
kills people right

98
00:03:59,599 --> 00:04:02,000
and we also should add the compounding

99
00:04:02,000 --> 00:04:05,599
effect from online networks and research

100
00:04:05,599 --> 00:04:07,680
has shown that false information on

101
00:04:07,680 --> 00:04:10,959
twitter is retweeted by more people and

102
00:04:10,959 --> 00:04:13,680
far more rapidly than true information

103
00:04:13,680 --> 00:04:17,519
especially for politics

104
00:04:17,519 --> 00:04:20,560
so most existing work on this

105
00:04:20,560 --> 00:04:21,759
information

106
00:04:21,759 --> 00:04:24,800
focuses on textual misinformation

107
00:04:24,800 --> 00:04:26,720
but we all know that's not how people

108
00:04:26,720 --> 00:04:29,360
consume information these days

109
00:04:29,360 --> 00:04:31,680
people increasingly consume information

110
00:04:31,680 --> 00:04:33,680
in a multi-modal

111
00:04:33,680 --> 00:04:36,800
format so the motivation of my line of

112
00:04:36,800 --> 00:04:39,360
work is really simple i want to know how

113
00:04:39,360 --> 00:04:40,960
people process

114
00:04:40,960 --> 00:04:43,840
visual misinformation

115
00:04:43,840 --> 00:04:46,400
and decades of research has shown us

116
00:04:46,400 --> 00:04:47,280
that

117
00:04:47,280 --> 00:04:50,560
users process and perceive visuals

118
00:04:50,560 --> 00:04:53,680
fundamentally different than text

119
00:04:53,680 --> 00:04:56,560
visuals are more easily recalled

120
00:04:56,560 --> 00:04:58,400
shared and are more

121
00:04:58,400 --> 00:05:02,320
often more persuasive than words

122
00:05:02,320 --> 00:05:03,440
so

123
00:05:03,440 --> 00:05:06,080
we hear the saying a picture is worth a

124
00:05:06,080 --> 00:05:08,400
thousand words right which describe

125
00:05:08,400 --> 00:05:11,440
exactly that and i think in the context

126
00:05:11,440 --> 00:05:14,240
of misinformation the opposite also

127
00:05:14,240 --> 00:05:17,759
holds true hence my title a fake image

128
00:05:17,759 --> 00:05:21,120
is worth a thousand lies

129
00:05:21,120 --> 00:05:23,680
now images historically have been

130
00:05:23,680 --> 00:05:27,199
perceived as photographic proof of the

131
00:05:27,199 --> 00:05:29,120
depicted events

132
00:05:29,120 --> 00:05:31,199
we have expressions such as seeing is

133
00:05:31,199 --> 00:05:33,919
believing because photographs are

134
00:05:33,919 --> 00:05:36,479
convincing proof of things that happened

135
00:05:36,479 --> 00:05:39,360
or may be happening an example of this

136
00:05:39,360 --> 00:05:42,479
is the moon landing right the only way

137
00:05:42,479 --> 00:05:44,320
we knew it happened

138
00:05:44,320 --> 00:05:46,560
is the photographic evidence like this

139
00:05:46,560 --> 00:05:48,960
one otherwise how did you know that it

140
00:05:48,960 --> 00:05:52,479
happened right you don't because nobody

141
00:05:52,479 --> 00:05:54,000
was there

142
00:05:54,000 --> 00:05:55,919
physically right

143
00:05:55,919 --> 00:05:58,800
eyewitness saying uh when it happened so

144
00:05:58,800 --> 00:06:02,240
the only proof we have is the visual um

145
00:06:02,240 --> 00:06:04,479
evidence the footages the pictures and

146
00:06:04,479 --> 00:06:06,160
so on

147
00:06:06,160 --> 00:06:07,280
however

148
00:06:07,280 --> 00:06:09,840
while image manipulation technology

149
00:06:09,840 --> 00:06:12,720
could be used effectively with modern

150
00:06:12,720 --> 00:06:15,120
technology it's getting much easier to

151
00:06:15,120 --> 00:06:18,319
deceive and manipulate viewers

152
00:06:18,319 --> 00:06:21,039
using forgeries and these fake images

153
00:06:21,039 --> 00:06:23,520
can change beliefs cause distrust or

154
00:06:23,520 --> 00:06:26,880
manipulate emotions here i present you

155
00:06:26,880 --> 00:06:31,039
an example of this this is from the 2004

156
00:06:31,039 --> 00:06:33,520
u.s presidential election

157
00:06:33,520 --> 00:06:35,520
where an image of the presidential

158
00:06:35,520 --> 00:06:38,479
nominee john kerry on the left

159
00:06:38,479 --> 00:06:40,880
was circulating on the media

160
00:06:40,880 --> 00:06:43,440
he was sharing the stage with activist

161
00:06:43,440 --> 00:06:46,479
jane fonda on the right at the vietnam

162
00:06:46,479 --> 00:06:50,800
era anti-war rally back in the 70s

163
00:06:50,800 --> 00:06:53,120
now regardless of your political opinion

164
00:06:53,120 --> 00:06:54,960
on this picture

165
00:06:54,960 --> 00:06:57,199
this picture showed contrary to the

166
00:06:57,199 --> 00:06:59,919
image kerry wanted to present at that

167
00:06:59,919 --> 00:07:00,720
time

168
00:07:00,720 --> 00:07:03,599
okay so this picture was very damaging

169
00:07:03,599 --> 00:07:06,400
to his political career

170
00:07:06,400 --> 00:07:07,919
but

171
00:07:07,919 --> 00:07:09,759
this image was a composition and a

172
00:07:09,759 --> 00:07:13,360
visual forgery by a bush supporter

173
00:07:13,360 --> 00:07:15,919
the forgery was exposed because the two

174
00:07:15,919 --> 00:07:18,880
originals were found in archival sources

175
00:07:18,880 --> 00:07:20,720
but by the time the forgery was

176
00:07:20,720 --> 00:07:23,919
uncovered this image was circulating a

177
00:07:23,919 --> 00:07:26,720
lot and even appeared on new york times

178
00:07:26,720 --> 00:07:28,400
we all know that when new york times

179
00:07:28,400 --> 00:07:31,120
runs a retraction the story may not be

180
00:07:31,120 --> 00:07:33,840
viewed as widely as the original was

181
00:07:33,840 --> 00:07:35,759
right so the damage had already been

182
00:07:35,759 --> 00:07:36,560
done

183
00:07:36,560 --> 00:07:39,280
the point is human beings just simply

184
00:07:39,280 --> 00:07:42,000
lack the ability and the technical tools

185
00:07:42,000 --> 00:07:44,800
to tell the fake from the real images

186
00:07:44,800 --> 00:07:46,960
even big media outlets such as new york

187
00:07:46,960 --> 00:07:49,360
times will fall for it so with the wide

188
00:07:49,360 --> 00:07:51,520
availability of advanced photo editing

189
00:07:51,520 --> 00:07:54,639
tools we can no longer ensure that

190
00:07:54,639 --> 00:07:57,360
photographic records accurately reflect

191
00:07:57,360 --> 00:07:58,479
reality

192
00:07:58,479 --> 00:08:01,680
fake images can be extremely harmful and

193
00:08:01,680 --> 00:08:05,919
we have very few tools at our disposal

194
00:08:05,919 --> 00:08:07,599
here i want to show you another example

195
00:08:07,599 --> 00:08:10,400
of composition work the image display

196
00:08:10,400 --> 00:08:11,280
here

197
00:08:11,280 --> 00:08:14,160
of bikini wearing rifle tony sarah palin

198
00:08:14,160 --> 00:08:15,919
sarah palin was

199
00:08:15,919 --> 00:08:19,360
the 2008 republican

200
00:08:19,360 --> 00:08:21,440
vice president nominee

201
00:08:21,440 --> 00:08:23,680
so as the nomination was

202
00:08:23,680 --> 00:08:26,879
announced this photo was circulating on

203
00:08:26,879 --> 00:08:29,599
the internet but it was purely a digital

204
00:08:29,599 --> 00:08:32,719
manipulation it was created by pasting

205
00:08:32,719 --> 00:08:34,719
an image of her head

206
00:08:34,719 --> 00:08:36,320
onto the original

207
00:08:36,320 --> 00:08:38,640
photo of the bikini-clad woman holding a

208
00:08:38,640 --> 00:08:41,760
bb gun actually not a rifle

209
00:08:41,760 --> 00:08:43,839
sometimes even journalists at the big

210
00:08:43,839 --> 00:08:45,680
media outlets would commit photo

211
00:08:45,680 --> 00:08:48,480
manipulation knowingly as well

212
00:08:48,480 --> 00:08:51,120
so the economist a very reputable

213
00:08:51,120 --> 00:08:54,080
magazine ran a cover photo of president

214
00:08:54,080 --> 00:08:57,440
obama on louisiana beach this was around

215
00:08:57,440 --> 00:09:00,640
the bp oil spill incident in 2010

216
00:09:00,640 --> 00:09:03,040
but the original photo

217
00:09:03,040 --> 00:09:06,320
shows that obama was in fact not alone

218
00:09:06,320 --> 00:09:09,279
at all right so the altered image

219
00:09:09,279 --> 00:09:10,640
cropped out

220
00:09:10,640 --> 00:09:12,560
to other people

221
00:09:12,560 --> 00:09:13,440
okay

222
00:09:13,440 --> 00:09:14,800
um

223
00:09:14,800 --> 00:09:17,120
we can debate you know whether this

224
00:09:17,120 --> 00:09:19,519
should be done should not be done but

225
00:09:19,519 --> 00:09:21,440
the fact is that it was a phone

226
00:09:21,440 --> 00:09:22,399
manipulation

227
00:09:22,399 --> 00:09:24,959
to eliminate certain elements of the

228
00:09:24,959 --> 00:09:26,800
original image

229
00:09:26,800 --> 00:09:29,680
and sometimes a fake image does not

230
00:09:29,680 --> 00:09:31,760
necessarily involve deliberate

231
00:09:31,760 --> 00:09:34,800
manipulation of the image itself

232
00:09:34,800 --> 00:09:37,600
it involves mismatching the image and

233
00:09:37,600 --> 00:09:40,240
the context is supposed to describe

234
00:09:40,240 --> 00:09:41,920
i don't know how many of you have seen

235
00:09:41,920 --> 00:09:45,120
this video of biden speaking at events

236
00:09:45,120 --> 00:09:47,760
in iowa earlier this year

237
00:09:47,760 --> 00:09:50,880
um this video and you know the the tweet

238
00:09:50,880 --> 00:09:53,839
quickly went viral with the caption bird

239
00:09:53,839 --> 00:09:55,839
poop central by them

240
00:09:55,839 --> 00:09:57,600
do you know that this caption is

241
00:09:57,600 --> 00:10:00,800
actually wrong

242
00:10:01,200 --> 00:10:04,800
in fact biden was speaking in an indoor

243
00:10:04,800 --> 00:10:08,160
event so it's extremely unlikely for

244
00:10:08,160 --> 00:10:10,959
bird to come swooping in and you know

245
00:10:10,959 --> 00:10:12,640
drop on biting

246
00:10:12,640 --> 00:10:16,320
instead biden was standing right next to

247
00:10:16,320 --> 00:10:20,079
a giant pile of processed corn

248
00:10:20,079 --> 00:10:22,800
it's a corn product called the ddgs

249
00:10:22,800 --> 00:10:24,959
in fact photographs from the event show

250
00:10:24,959 --> 00:10:27,680
that the corn was actively falling on

251
00:10:27,680 --> 00:10:30,640
this pile during his speech

252
00:10:30,640 --> 00:10:33,200
and there are many eyewitnesses too for

253
00:10:33,200 --> 00:10:35,839
example um the agricultural news website

254
00:10:35,839 --> 00:10:38,880
a.g wired reported that you know the ceo

255
00:10:38,880 --> 00:10:41,360
jeff cooper was an eyewitness in the

256
00:10:41,360 --> 00:10:44,640
distiller's grains barn with biden and

257
00:10:44,640 --> 00:10:47,680
he confirmed it was definitely ddgs

258
00:10:47,680 --> 00:10:50,800
responsible for spotting dakota's level

259
00:10:50,800 --> 00:10:54,160
not bird dropping

260
00:10:54,160 --> 00:10:57,040
a similar example is this one

261
00:10:57,040 --> 00:10:59,519
circulated around during hurricane sandy

262
00:10:59,519 --> 00:11:02,079
at new york new york city some years ago

263
00:11:02,079 --> 00:11:03,920
in this photo lady liberty looks like

264
00:11:03,920 --> 00:11:05,920
she's under attack from sandy's massive

265
00:11:05,920 --> 00:11:08,320
storm search but really this is from the

266
00:11:08,320 --> 00:11:10,800
movie the day after tomorrow

267
00:11:10,800 --> 00:11:14,480
so the photo itself was not manipulated

268
00:11:14,480 --> 00:11:17,279
but it was used purposely in the wrong

269
00:11:17,279 --> 00:11:18,800
context

270
00:11:18,800 --> 00:11:22,320
and that's misinformation too

271
00:11:22,320 --> 00:11:24,959
um and lastly internet memes right

272
00:11:24,959 --> 00:11:26,720
internet memes often fall into the

273
00:11:26,720 --> 00:11:29,360
category of misinformation as well

274
00:11:29,360 --> 00:11:32,800
in this example the photo of einstein

275
00:11:32,800 --> 00:11:34,399
is real

276
00:11:34,399 --> 00:11:36,399
but the quote was not

277
00:11:36,399 --> 00:11:39,360
so again using a real and authentic

278
00:11:39,360 --> 00:11:42,640
image in the wrong context it creates

279
00:11:42,640 --> 00:11:46,160
visual misinformation

280
00:11:46,560 --> 00:11:48,720
so after so many examples

281
00:11:48,720 --> 00:11:51,680
my point being image veracity is

282
00:11:51,680 --> 00:11:54,640
contextual images or videos or other

283
00:11:54,640 --> 00:11:57,279
multimodal information has to be judged

284
00:11:57,279 --> 00:11:59,600
within a context and there are many

285
00:11:59,600 --> 00:12:02,000
types of visual misinformation some

286
00:12:02,000 --> 00:12:04,560
involved manipulated images but some

287
00:12:04,560 --> 00:12:06,480
involve perfectly authentic images

288
00:12:06,480 --> 00:12:09,600
placed in a wrong context

289
00:12:09,600 --> 00:12:11,920
and how do we define them so we did a

290
00:12:11,920 --> 00:12:14,240
comprehensive review and then we

291
00:12:14,240 --> 00:12:18,000
identified four ways we can create fake

292
00:12:18,000 --> 00:12:20,240
images or visual misinformation the

293
00:12:20,240 --> 00:12:22,399
first is composition it means putting

294
00:12:22,399 --> 00:12:24,720
two photos together maybe pasting one

295
00:12:24,720 --> 00:12:26,720
person's head onto another like the

296
00:12:26,720 --> 00:12:28,480
scarab palin case

297
00:12:28,480 --> 00:12:30,959
retouching means changing specific

298
00:12:30,959 --> 00:12:33,120
elements in the existing image for

299
00:12:33,120 --> 00:12:35,040
example someone might be holding a water

300
00:12:35,040 --> 00:12:37,519
cannon and then you can retouch it to

301
00:12:37,519 --> 00:12:39,600
make it look like a gun right

302
00:12:39,600 --> 00:12:42,639
elimination is about cropping out

303
00:12:42,639 --> 00:12:44,639
certain parts of the image like the

304
00:12:44,639 --> 00:12:47,200
obama example and the lastly

305
00:12:47,200 --> 00:12:49,360
misattribution is

306
00:12:49,360 --> 00:12:51,760
putting an image or video in the wrong

307
00:12:51,760 --> 00:12:52,959
context

308
00:12:52,959 --> 00:12:54,800
but the image or video itself doesn't

309
00:12:54,800 --> 00:12:56,880
have to be edited right so the byte and

310
00:12:56,880 --> 00:12:59,360
example the internet meme example they

311
00:12:59,360 --> 00:13:00,800
all fall into

312
00:13:00,800 --> 00:13:02,880
misattribution

313
00:13:02,880 --> 00:13:05,760
okay now we have defined fake or forged

314
00:13:05,760 --> 00:13:08,560
images the next question is whether we

315
00:13:08,560 --> 00:13:11,680
as human beings are able to detect it

316
00:13:11,680 --> 00:13:14,480
now let's go back to the moon landing

317
00:13:14,480 --> 00:13:15,519
picture

318
00:13:15,519 --> 00:13:18,880
how do we know it's real or fake

319
00:13:18,880 --> 00:13:21,360
well unfortunately we don't

320
00:13:21,360 --> 00:13:23,360
because the technology that allows for

321
00:13:23,360 --> 00:13:26,000
creating manipulated images has far

322
00:13:26,000 --> 00:13:28,639
outpaced technological methods for

323
00:13:28,639 --> 00:13:31,279
detecting such manipulations

324
00:13:31,279 --> 00:13:34,480
but there are some analysis methods that

325
00:13:34,480 --> 00:13:36,639
could detect forgeries from looking into

326
00:13:36,639 --> 00:13:40,000
the metadata uh looking into encryption

327
00:13:40,000 --> 00:13:41,920
or in this method developed by my

328
00:13:41,920 --> 00:13:44,560
collaborators in computer graphics by

329
00:13:44,560 --> 00:13:48,639
analyzing shadows and perspectives

330
00:13:48,639 --> 00:13:50,560
without going into too much technical

331
00:13:50,560 --> 00:13:53,360
detail this analysis shows that moon

332
00:13:53,360 --> 00:13:55,519
landing did actually happen

333
00:13:55,519 --> 00:13:57,760
but these methods are very complex and

334
00:13:57,760 --> 00:14:00,399
they're not accessible to the average

335
00:14:00,399 --> 00:14:03,360
internet user in fact the average

336
00:14:03,360 --> 00:14:05,920
internet users often assume they have

337
00:14:05,920 --> 00:14:08,320
the ability to identify forgeries but

338
00:14:08,320 --> 00:14:11,040
they're often wrong okay

339
00:14:11,040 --> 00:14:13,600
this is an infamous example here in 2016

340
00:14:13,600 --> 00:14:16,000
mike pence posted this image on twitter

341
00:14:16,000 --> 00:14:18,800
taken i think at a chili's restaurant

342
00:14:18,800 --> 00:14:21,760
with his wife who's dressed in black and

343
00:14:21,760 --> 00:14:24,399
his daughter who's dressed in white

344
00:14:24,399 --> 00:14:26,320
if i ask you to look at the image very

345
00:14:26,320 --> 00:14:27,680
closely

346
00:14:27,680 --> 00:14:31,199
do you think this image is real or fake

347
00:14:31,199 --> 00:14:34,959
can you spot anything wrong with it

348
00:14:35,760 --> 00:14:39,600
okay many internet users did claim that

349
00:14:39,600 --> 00:14:42,959
something was very wrong

350
00:14:42,959 --> 00:14:45,279
they immediately created a bus to say

351
00:14:45,279 --> 00:14:48,399
that the image is fake because

352
00:14:48,399 --> 00:14:50,800
while pence had a reflection in the

353
00:14:50,800 --> 00:14:52,240
mirror

354
00:14:52,240 --> 00:14:54,399
his daughter didn't

355
00:14:54,399 --> 00:14:56,880
how could that be how could that be some

356
00:14:56,880 --> 00:14:58,399
twitter users started calling her a

357
00:14:58,399 --> 00:15:00,959
vampire

358
00:15:01,680 --> 00:15:02,959
however

359
00:15:02,959 --> 00:15:05,279
as you can see in this analysis that the

360
00:15:05,279 --> 00:15:07,600
fact that her reflection is not apparent

361
00:15:07,600 --> 00:15:10,320
in the mirror is actually consistent

362
00:15:10,320 --> 00:15:12,720
with the geometry of the scene

363
00:15:12,720 --> 00:15:13,519
through

364
00:15:13,519 --> 00:15:15,760
analyzing reflection how reflection

365
00:15:15,760 --> 00:15:18,079
behaves under perspective projection

366
00:15:18,079 --> 00:15:20,079
right if we apply masks you can see that

367
00:15:20,079 --> 00:15:22,639
the tip of her nose should be behind

368
00:15:22,639 --> 00:15:24,480
pens that's why we don't see her

369
00:15:24,480 --> 00:15:26,000
reflection in the mirror because her

370
00:15:26,000 --> 00:15:28,720
reflection was blocked by pence's

371
00:15:28,720 --> 00:15:29,920
reflection

372
00:15:29,920 --> 00:15:31,279
so even when there is a growing

373
00:15:31,279 --> 00:15:33,120
awareness that images no longer

374
00:15:33,120 --> 00:15:35,600
represent an authentic proof of reality

375
00:15:35,600 --> 00:15:37,760
we're not very good at finding

376
00:15:37,760 --> 00:15:40,320
manipulations and therefore we are very

377
00:15:40,320 --> 00:15:42,800
vulnerable to them

378
00:15:42,800 --> 00:15:44,880
okay so now i have explained the

379
00:15:44,880 --> 00:15:47,600
rationale next i'm going to briefly talk

380
00:15:47,600 --> 00:15:50,240
about series of empirical studies

381
00:15:50,240 --> 00:15:52,639
carried out at my lab funded by the

382
00:15:52,639 --> 00:15:54,720
national science foundation and by the

383
00:15:54,720 --> 00:15:57,279
facebook integrity research awards

384
00:15:57,279 --> 00:16:00,000
we began by doing a focus group study

385
00:16:00,000 --> 00:16:02,000
which was published um in the proceeding

386
00:16:02,000 --> 00:16:04,079
of cayenne 2018

387
00:16:04,079 --> 00:16:06,800
and that study aimed to answer one

388
00:16:06,800 --> 00:16:09,279
question how do people evaluate image

389
00:16:09,279 --> 00:16:10,560
credibility

390
00:16:10,560 --> 00:16:14,000
so we created a lots of fake images

391
00:16:14,000 --> 00:16:16,880
using the you know four categories we

392
00:16:16,880 --> 00:16:19,519
just identified composition elimination

393
00:16:19,519 --> 00:16:21,440
retouching and misattribution

394
00:16:21,440 --> 00:16:23,680
and then we created lots of mockups

395
00:16:23,680 --> 00:16:26,320
based on these images so these mockups

396
00:16:26,320 --> 00:16:28,240
have the medium

397
00:16:28,240 --> 00:16:30,079
it purportedly can be from twitter or

398
00:16:30,079 --> 00:16:32,880
facebook or instagram and then it has an

399
00:16:32,880 --> 00:16:35,600
outlet let's say some of the images are

400
00:16:35,600 --> 00:16:37,759
from bbc somewhere from new york times

401
00:16:37,759 --> 00:16:42,240
etc and we also create a very very short

402
00:16:42,240 --> 00:16:43,360
caption

403
00:16:43,360 --> 00:16:45,759
and some endorsement cues

404
00:16:45,759 --> 00:16:47,680
so this is an example of one of the

405
00:16:47,680 --> 00:16:50,000
mock-ups here

406
00:16:50,000 --> 00:16:52,639
this visual post purportedly is from

407
00:16:52,639 --> 00:16:55,120
facebook it was published by

408
00:16:55,120 --> 00:16:58,560
the account of bbc world news it has a

409
00:16:58,560 --> 00:16:59,680
one-line

410
00:16:59,680 --> 00:17:01,360
caption

411
00:17:01,360 --> 00:17:05,039
and 439 people shared it etc to make it

412
00:17:05,039 --> 00:17:07,599
look like the way it should be when you

413
00:17:07,599 --> 00:17:10,799
encounter a image online

414
00:17:10,799 --> 00:17:12,799
and we conducted the study at two

415
00:17:12,799 --> 00:17:15,039
locations

416
00:17:15,039 --> 00:17:17,199
and our findings we found that our

417
00:17:17,199 --> 00:17:18,640
participants

418
00:17:18,640 --> 00:17:22,160
made judgments mostly based on non-image

419
00:17:22,160 --> 00:17:25,039
queues so in other words um

420
00:17:25,039 --> 00:17:26,880
when we show this mock-up to

421
00:17:26,880 --> 00:17:28,720
participants they may tell us this is

422
00:17:28,720 --> 00:17:32,440
real or fade

423
00:17:54,960 --> 00:17:58,559
are very poor at identifying fake images

424
00:17:58,559 --> 00:18:02,400
so that's the first study uh naturally

425
00:18:02,400 --> 00:18:04,000
we

426
00:18:04,000 --> 00:18:06,240
did a second study um

427
00:18:06,240 --> 00:18:08,000
following our first study and in this

428
00:18:08,000 --> 00:18:12,080
study we ran a large scale experiment

429
00:18:12,080 --> 00:18:14,400
to see what factors predict

430
00:18:14,400 --> 00:18:17,120
people's credibility judgments of online

431
00:18:17,120 --> 00:18:19,200
images

432
00:18:19,200 --> 00:18:20,960
and we borrow the heavily from

433
00:18:20,960 --> 00:18:24,000
credibility research in other settings

434
00:18:24,000 --> 00:18:27,280
and extend them into the image context

435
00:18:27,280 --> 00:18:29,360
so i'll go through all these factors we

436
00:18:29,360 --> 00:18:32,400
tested we looked at the source

437
00:18:32,400 --> 00:18:34,559
we used some very highly trustworthy

438
00:18:34,559 --> 00:18:37,440
source and some you know kind of sketchy

439
00:18:37,440 --> 00:18:40,240
sources we used humans as sources we

440
00:18:40,240 --> 00:18:42,400
also used the media organization

441
00:18:42,400 --> 00:18:45,120
such as new york times or buzzfeed as

442
00:18:45,120 --> 00:18:46,720
sources

443
00:18:46,720 --> 00:18:49,919
we looked at the source and media type

444
00:18:49,919 --> 00:18:52,000
is it website or social media or is it

445
00:18:52,000 --> 00:18:55,120
news organization versus individuals

446
00:18:55,120 --> 00:18:56,440
we looked at something called

447
00:18:56,440 --> 00:18:58,960
intermediaries and this is something i

448
00:18:58,960 --> 00:19:01,840
think that's very unique to social media

449
00:19:01,840 --> 00:19:04,080
platforms uh let's say

450
00:19:04,080 --> 00:19:06,720
in this example um the

451
00:19:06,720 --> 00:19:08,799
original source was this japanese sky

452
00:19:08,799 --> 00:19:10,400
guy

453
00:19:10,400 --> 00:19:11,840
you saw ku

454
00:19:11,840 --> 00:19:14,400
moziwa on twitter right

455
00:19:14,400 --> 00:19:16,240
but we actually don't really personally

456
00:19:16,240 --> 00:19:18,720
know this person or never heard of him

457
00:19:18,720 --> 00:19:21,120
but the fact that elon musk

458
00:19:21,120 --> 00:19:23,120
liked this tweet

459
00:19:23,120 --> 00:19:25,600
tells us something so in this case

460
00:19:25,600 --> 00:19:28,799
we are only able to access this tweet or

461
00:19:28,799 --> 00:19:30,559
know about this tweet

462
00:19:30,559 --> 00:19:31,679
due to

463
00:19:31,679 --> 00:19:34,160
this being liked or retweeted by elon

464
00:19:34,160 --> 00:19:36,840
musk so in this case elon musk is the

465
00:19:36,840 --> 00:19:40,559
intermediary source and then uh

466
00:19:40,559 --> 00:19:44,000
uh yusaku himself is the original source

467
00:19:44,000 --> 00:19:46,240
so we're interested in learning whether

468
00:19:46,240 --> 00:19:48,320
the intermediary endorsement from

469
00:19:48,320 --> 00:19:51,520
someone else would have any impact on

470
00:19:51,520 --> 00:19:53,600
the perceived credibility and what

471
00:19:53,600 --> 00:19:55,440
happens when the credibility of the

472
00:19:55,440 --> 00:19:57,840
source and the credibility of the

473
00:19:57,840 --> 00:20:02,240
intermediary are at odds with each other

474
00:20:02,240 --> 00:20:04,799
also we looked at the social buttons

475
00:20:04,799 --> 00:20:05,600
like

476
00:20:05,600 --> 00:20:08,320
like share favorite etc they're

477
00:20:08,320 --> 00:20:12,320
available on websites um and then

478
00:20:12,320 --> 00:20:14,720
research has shown that there is

479
00:20:14,720 --> 00:20:16,960
something called the bandwagon effect

480
00:20:16,960 --> 00:20:19,120
where people are more likely to agree

481
00:20:19,120 --> 00:20:21,440
with the perceived consensus from

482
00:20:21,440 --> 00:20:22,640
aggregate

483
00:20:22,640 --> 00:20:24,720
metrics such as oh so many people have

484
00:20:24,720 --> 00:20:26,640
liked it then it must be true something

485
00:20:26,640 --> 00:20:28,400
like that and we expect the same

486
00:20:28,400 --> 00:20:32,400
heuristic for online images

487
00:20:32,880 --> 00:20:35,200
we also looked at people's digital media

488
00:20:35,200 --> 00:20:38,000
literacy uh such as their photography

489
00:20:38,000 --> 00:20:40,080
skills their internet skills your social

490
00:20:40,080 --> 00:20:42,400
media experience and finally

491
00:20:42,400 --> 00:20:44,880
confirmation bias is a well established

492
00:20:44,880 --> 00:20:47,200
finding in the context of credibility

493
00:20:47,200 --> 00:20:49,440
judgment in that people are more likely

494
00:20:49,440 --> 00:20:51,840
to perceive something as credible if it

495
00:20:51,840 --> 00:20:54,000
confirms your existing beliefs and

496
00:20:54,000 --> 00:20:56,480
opinions

497
00:20:56,480 --> 00:20:58,000
all right so we did an experimental

498
00:20:58,000 --> 00:20:59,360
design

499
00:20:59,360 --> 00:21:03,200
with 28 conditions six images 28 markups

500
00:21:03,200 --> 00:21:05,280
for each image so that's a lot of

501
00:21:05,280 --> 00:21:06,480
conditions

502
00:21:06,480 --> 00:21:09,039
okay so this is uh briefly what we

503
00:21:09,039 --> 00:21:10,320
tested

504
00:21:10,320 --> 00:21:13,200
uh here are some example mockups

505
00:21:13,200 --> 00:21:16,720
here is a mock-up for a facebook post

506
00:21:16,720 --> 00:21:19,440
allegedly created by bill gates

507
00:21:19,440 --> 00:21:22,320
and then it was shared by npr right so

508
00:21:22,320 --> 00:21:24,000
bill gates would be the original source

509
00:21:24,000 --> 00:21:26,559
and npr would be the intermediary source

510
00:21:26,559 --> 00:21:28,960
and this image was modified to display

511
00:21:28,960 --> 00:21:32,320
airstrikes in syria

512
00:21:32,320 --> 00:21:34,799
so as you can see the mockup

513
00:21:34,799 --> 00:21:36,960
you know tries really hard to simulate

514
00:21:36,960 --> 00:21:40,080
the environment in which you encounter

515
00:21:40,080 --> 00:21:42,000
visual information

516
00:21:42,000 --> 00:21:45,840
on on social media sites

517
00:21:46,559 --> 00:21:49,440
uh here's another example this is a

518
00:21:49,440 --> 00:21:51,440
facebook post allegedly created by

519
00:21:51,440 --> 00:21:52,559
buzzfeed

520
00:21:52,559 --> 00:21:54,559
and shared by a generic person mark

521
00:21:54,559 --> 00:21:56,480
smith um

522
00:21:56,480 --> 00:22:00,240
supposedly it depicts a school in africa

523
00:22:00,240 --> 00:22:02,960
here's another one um a facebook post

524
00:22:02,960 --> 00:22:04,960
allegedly created by buzzfeed and

525
00:22:04,960 --> 00:22:06,000
showing

526
00:22:06,000 --> 00:22:08,240
um

527
00:22:08,559 --> 00:22:11,360
showing uh it's like a animal

528
00:22:11,360 --> 00:22:13,679
uh creation in the lab

529
00:22:13,679 --> 00:22:15,760
uh here's another example as you can see

530
00:22:15,760 --> 00:22:18,240
i mean the photoshop job we did is you

531
00:22:18,240 --> 00:22:20,320
know with varying degree of success we

532
00:22:20,320 --> 00:22:22,400
would say and we did it deliberately so

533
00:22:22,400 --> 00:22:24,799
that it's not like perfect photoshop job

534
00:22:24,799 --> 00:22:26,480
i i would say that if you look very

535
00:22:26,480 --> 00:22:28,480
closely you can kind of see that

536
00:22:28,480 --> 00:22:30,799
the photoshop job was not nice to be

537
00:22:30,799 --> 00:22:33,679
done and that was intentional

538
00:22:33,679 --> 00:22:36,000
okay so we looked uh we used

539
00:22:36,000 --> 00:22:38,240
interpreters in the united states asking

540
00:22:38,240 --> 00:22:40,880
them to use your desktop and we also did

541
00:22:40,880 --> 00:22:43,840
an excuse example for comparison

542
00:22:43,840 --> 00:22:46,320
our findings we found no effects for

543
00:22:46,320 --> 00:22:48,480
source trustworthiness source media type

544
00:22:48,480 --> 00:22:51,440
or intermediary or bandwagon what

545
00:22:51,440 --> 00:22:54,320
does have an effect are two things first

546
00:22:54,320 --> 00:22:56,960
is people's digital media literacy so if

547
00:22:56,960 --> 00:22:59,919
they have more experience doing digital

548
00:22:59,919 --> 00:23:02,000
photography if they

549
00:23:02,000 --> 00:23:04,400
have higher internet skills then they

550
00:23:04,400 --> 00:23:06,240
are more likely to be

551
00:23:06,240 --> 00:23:07,600
skeptical

552
00:23:07,600 --> 00:23:09,840
of the fake images

553
00:23:09,840 --> 00:23:11,679
um another thing that mattered is

554
00:23:11,679 --> 00:23:14,320
people's issue attitude so if they

555
00:23:14,320 --> 00:23:16,960
already kind of agree with whatever is

556
00:23:16,960 --> 00:23:19,919
depicting in the image post then they

557
00:23:19,919 --> 00:23:21,520
are more likely to think this is

558
00:23:21,520 --> 00:23:24,640
credible okay so to affirm whatever

559
00:23:24,640 --> 00:23:29,280
their prior attitude regarding an issue

560
00:23:29,280 --> 00:23:31,919
so with all these findings

561
00:23:31,919 --> 00:23:34,400
that led us to think

562
00:23:34,400 --> 00:23:37,360
well people seem to be pretty terrible

563
00:23:37,360 --> 00:23:38,480
um

564
00:23:38,480 --> 00:23:41,520
at uh judging the credibility of visual

565
00:23:41,520 --> 00:23:44,880
misinformation but what can we do

566
00:23:44,880 --> 00:23:48,000
to solve the misinformation a problem

567
00:23:48,000 --> 00:23:49,840
based on findings

568
00:23:49,840 --> 00:23:51,679
it seems there are two obvious choice

569
00:23:51,679 --> 00:23:55,760
here uh one is that we could um do

570
00:23:55,760 --> 00:23:58,640
something with the platforms uh what if

571
00:23:58,640 --> 00:24:00,799
we make some tweaks to the platforms

572
00:24:00,799 --> 00:24:02,240
that could help

573
00:24:02,240 --> 00:24:05,360
the other option we had is what if we do

574
00:24:05,360 --> 00:24:07,760
something to the users okay

575
00:24:07,760 --> 00:24:09,600
what if we educate them what if we

576
00:24:09,600 --> 00:24:12,159
design a low-cost intervention okay so

577
00:24:12,159 --> 00:24:15,200
these are exactly what we did in two

578
00:24:15,200 --> 00:24:16,880
follow-up studies

579
00:24:16,880 --> 00:24:19,840
so let's take a look at the next study

580
00:24:19,840 --> 00:24:21,120
in which we

581
00:24:21,120 --> 00:24:24,159
um try to see if we can do something to

582
00:24:24,159 --> 00:24:27,679
the platforms okay um so it involves

583
00:24:27,679 --> 00:24:30,080
using something called image forensic

584
00:24:30,080 --> 00:24:31,440
labeling

585
00:24:31,440 --> 00:24:34,240
so i'm sure a lot of you are somewhat

586
00:24:34,240 --> 00:24:36,159
familiar with fact checking websites

587
00:24:36,159 --> 00:24:39,200
like hoaxy politifact or snopes

588
00:24:39,200 --> 00:24:40,080
right

589
00:24:40,080 --> 00:24:43,360
these are existing fab checking services

590
00:24:43,360 --> 00:24:46,320
uh they're mostly about textual but they

591
00:24:46,320 --> 00:24:49,039
start to you know include visual

592
00:24:49,039 --> 00:24:50,880
misinformation in their fact checking

593
00:24:50,880 --> 00:24:53,760
services as well the problem of these

594
00:24:53,760 --> 00:24:56,000
services is that

595
00:24:56,000 --> 00:24:58,720
you have to come to them right so they

596
00:24:58,720 --> 00:25:01,840
are a third party uh so imagine a

597
00:25:01,840 --> 00:25:04,640
hundred users are exposed to

598
00:25:04,640 --> 00:25:07,520
misinformation let's say a fake picture

599
00:25:07,520 --> 00:25:10,080
how many of them do you think would

600
00:25:10,080 --> 00:25:12,960
willingly go to snopes or politifact or

601
00:25:12,960 --> 00:25:16,000
hoaxy and to check whether that image

602
00:25:16,000 --> 00:25:17,600
they just saw

603
00:25:17,600 --> 00:25:19,360
are authentic or not

604
00:25:19,360 --> 00:25:22,159
i bet it's not a hundred people right

605
00:25:22,159 --> 00:25:24,799
actually very few of them will willingly

606
00:25:24,799 --> 00:25:26,720
go to third-party websites and do the

607
00:25:26,720 --> 00:25:29,039
fact check themselves okay another

608
00:25:29,039 --> 00:25:31,279
problem with these sites is that they

609
00:25:31,279 --> 00:25:33,679
are after the exposure

610
00:25:33,679 --> 00:25:34,799
uh so

611
00:25:34,799 --> 00:25:37,520
um so for example you have to already

612
00:25:37,520 --> 00:25:39,120
see the

613
00:25:39,120 --> 00:25:40,080
um

614
00:25:40,080 --> 00:25:42,480
visual misinformation or the image or

615
00:25:42,480 --> 00:25:44,159
the visual post

616
00:25:44,159 --> 00:25:46,159
and then you will go to a third party

617
00:25:46,159 --> 00:25:48,720
site such as politifact to check its

618
00:25:48,720 --> 00:25:51,919
veracity but by the time you arrive at

619
00:25:51,919 --> 00:25:54,080
politifact you kind of already form an

620
00:25:54,080 --> 00:25:56,400
impression about the image or video

621
00:25:56,400 --> 00:25:58,240
we're trying you're trying to

622
00:25:58,240 --> 00:25:59,360
check

623
00:25:59,360 --> 00:26:02,559
oh so it's called post exposure

624
00:26:02,559 --> 00:26:06,080
and post exposure is not very efficient

625
00:26:06,080 --> 00:26:08,320
okay what if we

626
00:26:08,320 --> 00:26:10,640
make the platforms

627
00:26:10,640 --> 00:26:14,240
on which you consume information

628
00:26:14,240 --> 00:26:16,880
carries some of the fact-checking burden

629
00:26:16,880 --> 00:26:21,200
okay what if as you're exposed to

630
00:26:21,200 --> 00:26:25,520
information to images you can see

631
00:26:25,520 --> 00:26:28,799
a forensic label of that image at the

632
00:26:28,799 --> 00:26:32,400
same time okay so if we do that then it

633
00:26:32,400 --> 00:26:35,600
eliminates the problem of you know you

634
00:26:35,600 --> 00:26:38,080
have to have a prior exposure and it

635
00:26:38,080 --> 00:26:38,960
also

636
00:26:38,960 --> 00:26:41,520
eliminates the problem that you have to

637
00:26:41,520 --> 00:26:45,279
seek out a third party service right

638
00:26:45,279 --> 00:26:48,559
because here the forensic label

639
00:26:48,559 --> 00:26:52,240
would be already on the platform on

640
00:26:52,240 --> 00:26:54,559
which you're consuming use

641
00:26:54,559 --> 00:26:55,600
okay so

642
00:26:55,600 --> 00:26:58,080
we call this model a concurrent model of

643
00:26:58,080 --> 00:27:00,480
fact checking you make the credibility

644
00:27:00,480 --> 00:27:01,840
assessment

645
00:27:01,840 --> 00:27:03,520
as you

646
00:27:03,520 --> 00:27:08,000
are consuming the image or video

647
00:27:08,000 --> 00:27:10,640
so we get some inspiration from

648
00:27:10,640 --> 00:27:14,159
politifact's truth-o-meter okay i think

649
00:27:14,159 --> 00:27:15,520
it's a very very nice visual

650
00:27:15,520 --> 00:27:18,159
representation of where

651
00:27:18,159 --> 00:27:19,679
the information

652
00:27:19,679 --> 00:27:20,400
falls

653
00:27:20,400 --> 00:27:23,200
is it pants on fire is it mostly true is

654
00:27:23,200 --> 00:27:25,279
it half true etc so we designed

655
00:27:25,279 --> 00:27:28,400
something called a pixel meter

656
00:27:28,400 --> 00:27:31,919
so it models after trusometer and then

657
00:27:31,919 --> 00:27:34,480
we put this pictorial meter

658
00:27:34,480 --> 00:27:37,679
with our mockups we did a experiment

659
00:27:37,679 --> 00:27:41,600
very similar to the previous one

660
00:27:41,840 --> 00:27:44,080
and then

661
00:27:44,080 --> 00:27:45,919
results

662
00:27:45,919 --> 00:27:48,080
it shows that the forensic labels

663
00:27:48,080 --> 00:27:50,640
definitely work okay so the average

664
00:27:50,640 --> 00:27:53,440
image credibility ratings um

665
00:27:53,440 --> 00:27:56,640
differ by labeling condition the altered

666
00:27:56,640 --> 00:27:58,799
label which means the image was altered

667
00:27:58,799 --> 00:28:00,480
was doctored

668
00:28:00,480 --> 00:28:03,279
in the center it was significantly lower

669
00:28:03,279 --> 00:28:04,960
in credibility

670
00:28:04,960 --> 00:28:07,120
than the control condition

671
00:28:07,120 --> 00:28:10,640
while the unaltered label did not differ

672
00:28:10,640 --> 00:28:12,799
significantly from the control condition

673
00:28:12,799 --> 00:28:15,600
so in other words we

674
00:28:15,600 --> 00:28:17,919
our default is that everything is

675
00:28:17,919 --> 00:28:19,600
trustworthy

676
00:28:19,600 --> 00:28:21,760
okay everything is trustworthy

677
00:28:21,760 --> 00:28:23,760
but if you actually tell them you know

678
00:28:23,760 --> 00:28:25,679
in in terms of a label that this is

679
00:28:25,679 --> 00:28:27,760
altered this is photoshopped okay the

680
00:28:27,760 --> 00:28:30,559
credibility of that

681
00:28:30,559 --> 00:28:34,159
image is significantly lower than if it

682
00:28:34,159 --> 00:28:37,360
were unaltered or there is no labeling

683
00:28:37,360 --> 00:28:40,480
at all so labels definitely work

684
00:28:40,480 --> 00:28:43,760
um this figure shows the various factors

685
00:28:43,760 --> 00:28:45,840
associated with participants perceived

686
00:28:45,840 --> 00:28:48,320
image credibility rating as we would

687
00:28:48,320 --> 00:28:51,200
expect um

688
00:28:51,200 --> 00:28:52,559
people

689
00:28:52,559 --> 00:28:54,799
with a search on political ideology

690
00:28:54,799 --> 00:28:57,440
people with higher internet skills and

691
00:28:57,440 --> 00:29:00,240
people with pro-issue attitudes are more

692
00:29:00,240 --> 00:29:02,799
likely to

693
00:29:02,799 --> 00:29:06,080
think of something as more trustworthy

694
00:29:06,080 --> 00:29:08,159
um

695
00:29:08,159 --> 00:29:11,120
versus people with lower internet skills

696
00:29:11,120 --> 00:29:14,559
um and photography experience so that

697
00:29:14,559 --> 00:29:17,120
was consistent with our previous study

698
00:29:17,120 --> 00:29:19,279
as well so basically the more experience

699
00:29:19,279 --> 00:29:21,520
you have the more skeptical you become

700
00:29:21,520 --> 00:29:24,559
and then uh user features um yeah

701
00:29:24,559 --> 00:29:26,080
participants with higher digital media

702
00:29:26,080 --> 00:29:28,399
literacy they were more skeptical uh

703
00:29:28,399 --> 00:29:30,159
they're less likely to rate the images

704
00:29:30,159 --> 00:29:32,320
as very credible and images that aligns

705
00:29:32,320 --> 00:29:34,240
with participants pre-existing issue

706
00:29:34,240 --> 00:29:35,840
attitude were more likely to be

707
00:29:35,840 --> 00:29:38,320
perceived as credible okay so that's our

708
00:29:38,320 --> 00:29:41,600
study looking at the

709
00:29:41,600 --> 00:29:45,279
some tweaks we can do for the platforms

710
00:29:45,279 --> 00:29:47,520
the last study i want to talk about

711
00:29:47,520 --> 00:29:49,360
is what if we

712
00:29:49,360 --> 00:29:52,480
train the users okay so yes platforms

713
00:29:52,480 --> 00:29:54,320
yes we can make some tweaks but that

714
00:29:54,320 --> 00:29:56,480
doesn't necessarily mean that we can't

715
00:29:56,480 --> 00:29:58,320
do anything to the users

716
00:29:58,320 --> 00:30:01,200
what if we educate the users

717
00:30:01,200 --> 00:30:03,840
so how did we do it um this this project

718
00:30:03,840 --> 00:30:06,320
is ongoing it's using low-cost

719
00:30:06,320 --> 00:30:09,039
intervention to encourage fact-checking

720
00:30:09,039 --> 00:30:11,600
misattributed images it was partially

721
00:30:11,600 --> 00:30:15,679
funded by a facebook research award

722
00:30:15,679 --> 00:30:16,640
so

723
00:30:16,640 --> 00:30:19,840
the inspiration of this study

724
00:30:19,840 --> 00:30:21,200
was

725
00:30:21,200 --> 00:30:22,480
taken from

726
00:30:22,480 --> 00:30:24,960
a twitter feature i think twitter

727
00:30:24,960 --> 00:30:26,080
started rolling out this little

728
00:30:26,080 --> 00:30:27,520
intervention

729
00:30:27,520 --> 00:30:30,000
like maybe two or three years ago so

730
00:30:30,000 --> 00:30:31,520
whenever you're trying to share

731
00:30:31,520 --> 00:30:34,159
something a link on twitter it actually

732
00:30:34,159 --> 00:30:36,000
asks you to read the story before

733
00:30:36,000 --> 00:30:38,480
sharing okay all it is doing is to

734
00:30:38,480 --> 00:30:40,399
create some kind of friction for the

735
00:30:40,399 --> 00:30:43,039
dissemination of information so we

736
00:30:43,039 --> 00:30:44,720
thought you know what if we do the same

737
00:30:44,720 --> 00:30:46,240
for images

738
00:30:46,240 --> 00:30:47,679
um and this

739
00:30:47,679 --> 00:30:49,919
applies specifically to images that are

740
00:30:49,919 --> 00:30:53,039
misattributed or taken out of context

741
00:30:53,039 --> 00:30:56,320
right remember one of the categories of

742
00:30:56,320 --> 00:30:59,120
visual misinformation is images that are

743
00:30:59,120 --> 00:31:00,240
not

744
00:31:00,240 --> 00:31:03,679
edited but placed in the wrong context

745
00:31:03,679 --> 00:31:06,480
and this can be checked by using a

746
00:31:06,480 --> 00:31:08,399
reverse image search right so there's a

747
00:31:08,399 --> 00:31:10,559
tool for that

748
00:31:10,559 --> 00:31:12,799
so we decided to

749
00:31:12,799 --> 00:31:14,960
include like a little intervention to

750
00:31:14,960 --> 00:31:17,600
ask users to do every reverse image

751
00:31:17,600 --> 00:31:18,399
search

752
00:31:18,399 --> 00:31:20,720
okay it will be especially useful for

753
00:31:20,720 --> 00:31:23,760
this particular kind of miscaptioned

754
00:31:23,760 --> 00:31:24,880
images

755
00:31:24,880 --> 00:31:27,200
okay so we found our stimuli by

756
00:31:27,200 --> 00:31:29,279
scrapping uh scraping this website

757
00:31:29,279 --> 00:31:30,799
snopes.com

758
00:31:30,799 --> 00:31:32,640
snoops.com has this very interesting

759
00:31:32,640 --> 00:31:35,679
category called photography where it has

760
00:31:35,679 --> 00:31:38,960
a collection of dubious images of videos

761
00:31:38,960 --> 00:31:41,279
under false pretense

762
00:31:41,279 --> 00:31:44,640
uh so for example this is one of our

763
00:31:44,640 --> 00:31:46,880
stimuli material we used

764
00:31:46,880 --> 00:31:49,600
so this is that the image is the same it

765
00:31:49,600 --> 00:31:52,320
hasn't been photoshopped but in the

766
00:31:52,320 --> 00:31:54,880
first case um the caption says buses

767
00:31:54,880 --> 00:31:56,320
purchased by black lives matter

768
00:31:56,320 --> 00:31:58,960
supporters to transfer members to a riot

769
00:31:58,960 --> 00:32:02,240
in july 2020 which is wrong the caption

770
00:32:02,240 --> 00:32:04,480
was wrong and in the correct condition

771
00:32:04,480 --> 00:32:06,799
it's the toronto raptors going to nba

772
00:32:06,799 --> 00:32:09,200
bubbles in the black lives matter bus in

773
00:32:09,200 --> 00:32:12,880
july 2020 okay same image but different

774
00:32:12,880 --> 00:32:15,840
captions

775
00:32:16,399 --> 00:32:19,120
so we designed a tiny intervention by

776
00:32:19,120 --> 00:32:20,240
giving

777
00:32:20,240 --> 00:32:24,480
um our participants a little infographic

778
00:32:24,480 --> 00:32:26,640
called it's seen believing and in this

779
00:32:26,640 --> 00:32:29,679
we walk them through how they can use

780
00:32:29,679 --> 00:32:31,519
reverse image search

781
00:32:31,519 --> 00:32:33,200
on their desktop

782
00:32:33,200 --> 00:32:35,919
we divided them into three

783
00:32:35,919 --> 00:32:38,720
groups one is the active group

784
00:32:38,720 --> 00:32:41,840
these people would read the infographic

785
00:32:41,840 --> 00:32:44,559
and they have to complete a reverse

786
00:32:44,559 --> 00:32:46,960
image search task okay

787
00:32:46,960 --> 00:32:49,120
and they have to pass it the second

788
00:32:49,120 --> 00:32:51,519
group is the passive group in which they

789
00:32:51,519 --> 00:32:54,240
only read the infographic but there is

790
00:32:54,240 --> 00:32:55,279
no

791
00:32:55,279 --> 00:32:58,320
requirement for them to practice reverse

792
00:32:58,320 --> 00:32:59,519
image search

793
00:32:59,519 --> 00:33:02,080
and the third group is the control group

794
00:33:02,080 --> 00:33:03,919
they didn't have to do anything they

795
00:33:03,919 --> 00:33:07,039
didn't read any infographic etc

796
00:33:07,039 --> 00:33:09,519
so what we wanted to see is that whether

797
00:33:09,519 --> 00:33:12,799
the active and passive group

798
00:33:12,799 --> 00:33:14,720
would be better

799
00:33:14,720 --> 00:33:18,000
at discerning fake images

800
00:33:18,000 --> 00:33:21,600
than the controller

801
00:33:21,679 --> 00:33:24,720
okay so very briefly our results is that

802
00:33:24,720 --> 00:33:26,880
active intervention significantly

803
00:33:26,880 --> 00:33:28,880
increased intention

804
00:33:28,880 --> 00:33:31,279
of using reverse image search tools

805
00:33:31,279 --> 00:33:33,519
compared to the passive and the control

806
00:33:33,519 --> 00:33:36,320
group but neither active or passive

807
00:33:36,320 --> 00:33:38,399
intervention had an effect

808
00:33:38,399 --> 00:33:39,919
on credibility judgment or

809
00:33:39,919 --> 00:33:43,600
misinformation discernment

810
00:33:43,600 --> 00:33:47,039
so continuing on we are in the process

811
00:33:47,039 --> 00:33:49,760
of doing a follow-up study to up

812
00:33:49,760 --> 00:33:52,640
people's motivation even further by

813
00:33:52,640 --> 00:33:54,799
giving them money or

814
00:33:54,799 --> 00:33:58,159
online badges okay so that's kind of um

815
00:33:58,159 --> 00:34:00,240
what we're testing here what if we give

816
00:34:00,240 --> 00:34:02,480
people more incentive

817
00:34:02,480 --> 00:34:04,840
for them to carry out these reverse

818
00:34:04,840 --> 00:34:07,760
searches would that make the results

819
00:34:07,760 --> 00:34:09,040
better

820
00:34:09,040 --> 00:34:11,040
um and in order to make experimental

821
00:34:11,040 --> 00:34:13,599
condition as real as possible we hired a

822
00:34:13,599 --> 00:34:16,239
programmer to design a fully functional

823
00:34:16,239 --> 00:34:19,040
facebook lookalike as our experimental

824
00:34:19,040 --> 00:34:21,359
site uh everything here on the interface

825
00:34:21,359 --> 00:34:25,440
is clickable and it's fully functional

826
00:34:25,520 --> 00:34:28,719
uh but that study is still ongoing and i

827
00:34:28,719 --> 00:34:30,159
don't have the results to talk about

828
00:34:30,159 --> 00:34:32,639
just yet

829
00:34:33,280 --> 00:34:37,199
okay so um as we're approaching the 40

830
00:34:37,199 --> 00:34:39,760
minutes mark i want to kind of briefly

831
00:34:39,760 --> 00:34:42,159
summarize what i shared so far kind of

832
00:34:42,159 --> 00:34:43,918
all tie it together

833
00:34:43,918 --> 00:34:45,679
so um

834
00:34:45,679 --> 00:34:47,520
first question we discussed today is how

835
00:34:47,520 --> 00:34:49,520
do people judge the credibility of

836
00:34:49,520 --> 00:34:51,359
images right

837
00:34:51,359 --> 00:34:54,159
and then the first study

838
00:34:54,159 --> 00:34:56,800
we did in our group is that people are

839
00:34:56,800 --> 00:34:58,640
pretty bad at it

840
00:34:58,640 --> 00:35:00,640
you know they

841
00:35:00,640 --> 00:35:03,760
they're just very vulnerable true fake

842
00:35:03,760 --> 00:35:05,520
images they

843
00:35:05,520 --> 00:35:08,400
don't really know where to look when

844
00:35:08,400 --> 00:35:11,359
trying to judge the um veracity of the

845
00:35:11,359 --> 00:35:13,440
images and when they do they're often

846
00:35:13,440 --> 00:35:14,480
wrong

847
00:35:14,480 --> 00:35:17,280
and then in this second experiment when

848
00:35:17,280 --> 00:35:19,520
we systematically tested everything we

849
00:35:19,520 --> 00:35:21,280
found that decision

850
00:35:21,280 --> 00:35:24,560
of image judge credibility judgment is

851
00:35:24,560 --> 00:35:27,040
heavily influenced by people's digital

852
00:35:27,040 --> 00:35:28,800
media literacy their political

853
00:35:28,800 --> 00:35:31,119
affiliation and their prior issue

854
00:35:31,119 --> 00:35:33,040
attitude so in that people

855
00:35:33,040 --> 00:35:35,040
are have more digital media literacy

856
00:35:35,040 --> 00:35:37,359
they are you know very proficient at

857
00:35:37,359 --> 00:35:39,760
using various apps and internet they are

858
00:35:39,760 --> 00:35:42,880
much more skeptical than people who are

859
00:35:42,880 --> 00:35:45,760
not as well

860
00:35:45,760 --> 00:35:48,079
prepared

861
00:35:48,079 --> 00:35:49,920
and the second question we're trying to

862
00:35:49,920 --> 00:35:53,119
tackle is what can we do given these

863
00:35:53,119 --> 00:35:54,640
findings okay

864
00:35:54,640 --> 00:35:57,200
so again i shared two studies

865
00:35:57,200 --> 00:35:58,400
the third study

866
00:35:58,400 --> 00:36:01,599
it focuses on what the platforms can do

867
00:36:01,599 --> 00:36:02,880
right so

868
00:36:02,880 --> 00:36:03,760
we

869
00:36:03,760 --> 00:36:07,280
tested a forensic label of images on

870
00:36:07,280 --> 00:36:10,079
these platforms and it turns out the

871
00:36:10,079 --> 00:36:13,520
credibility the forensic labels do work

872
00:36:13,520 --> 00:36:14,480
okay

873
00:36:14,480 --> 00:36:16,960
people who are shown

874
00:36:16,960 --> 00:36:19,680
the photoshopped label and they would

875
00:36:19,680 --> 00:36:22,160
rage the images as significantly less

876
00:36:22,160 --> 00:36:25,280
credible than people in other conditions

877
00:36:25,280 --> 00:36:27,200
but is there anything else we can do to

878
00:36:27,200 --> 00:36:28,560
the users

879
00:36:28,560 --> 00:36:31,920
so here in the fourth study i shared

880
00:36:31,920 --> 00:36:35,280
we designed a low-cost intervention

881
00:36:35,280 --> 00:36:38,240
that teaches people how to do reverse

882
00:36:38,240 --> 00:36:39,599
image search

883
00:36:39,599 --> 00:36:41,680
that is particularly useful for

884
00:36:41,680 --> 00:36:42,960
identifying

885
00:36:42,960 --> 00:36:44,480
fake images

886
00:36:44,480 --> 00:36:46,480
that are misattributed like the image

887
00:36:46,480 --> 00:36:47,839
itself

888
00:36:47,839 --> 00:36:50,480
is fine but it was put in a wrong

889
00:36:50,480 --> 00:36:53,920
context and it turns out reverse image

890
00:36:53,920 --> 00:36:55,680
searches

891
00:36:55,680 --> 00:36:58,480
as we did in our little infographic have

892
00:36:58,480 --> 00:37:01,200
some usefulness um

893
00:37:01,200 --> 00:37:04,320
to combat visual misinformation

894
00:37:04,320 --> 00:37:07,680
okay so other than these two questions

895
00:37:07,680 --> 00:37:10,240
what else can we do in this line of

896
00:37:10,240 --> 00:37:11,520
research

897
00:37:11,520 --> 00:37:13,920
so there are other research themes and

898
00:37:13,920 --> 00:37:16,400
topics that i don't have time to cover

899
00:37:16,400 --> 00:37:18,160
today but they are equally very

900
00:37:18,160 --> 00:37:20,160
interesting so

901
00:37:20,160 --> 00:37:23,520
one area of research in this area is how

902
00:37:23,520 --> 00:37:26,960
do we counter visual misinformation okay

903
00:37:26,960 --> 00:37:29,760
how do we design effective campaigns

904
00:37:29,760 --> 00:37:32,720
against misinformation and there are two

905
00:37:32,720 --> 00:37:34,880
kind of schools of thought on this one

906
00:37:34,880 --> 00:37:37,599
is called inoculation uh it's sort of

907
00:37:37,599 --> 00:37:40,400
like a having a vaccine right so

908
00:37:40,400 --> 00:37:42,960
inoculation means that you tell people

909
00:37:42,960 --> 00:37:46,079
fact-checking information even before

910
00:37:46,079 --> 00:37:49,200
they get exposed to something okay

911
00:37:49,200 --> 00:37:51,119
okay so it's like getting a dose of

912
00:37:51,119 --> 00:37:54,480
carbon 19 vaccine before you get exposed

913
00:37:54,480 --> 00:37:56,240
to carbon 19

914
00:37:56,240 --> 00:37:58,960
and correction um is something that we

915
00:37:58,960 --> 00:38:01,280
do after the fact okay so it's more like

916
00:38:01,280 --> 00:38:04,160
treatment so once you get infected with

917
00:38:04,160 --> 00:38:06,079
the virus what do we do how do we

918
00:38:06,079 --> 00:38:07,520
counteract

919
00:38:07,520 --> 00:38:10,000
the harmful effects right so inoculation

920
00:38:10,000 --> 00:38:12,720
and correction can both be very useful

921
00:38:12,720 --> 00:38:14,560
in our battle against visual

922
00:38:14,560 --> 00:38:16,240
misinformation

923
00:38:16,240 --> 00:38:18,160
um the second interesting line of

924
00:38:18,160 --> 00:38:20,480
research is what about specific user

925
00:38:20,480 --> 00:38:21,520
groups

926
00:38:21,520 --> 00:38:23,680
okay so in this line of thinking we want

927
00:38:23,680 --> 00:38:27,040
to identify a specific user group that

928
00:38:27,040 --> 00:38:28,800
are most vulnerable to visual

929
00:38:28,800 --> 00:38:31,839
misinformation so that we can target our

930
00:38:31,839 --> 00:38:34,640
resources of fact-checking inoculation

931
00:38:34,640 --> 00:38:37,920
or correction towards uh these specific

932
00:38:37,920 --> 00:38:39,760
users only

933
00:38:39,760 --> 00:38:41,520
and then the third uh

934
00:38:41,520 --> 00:38:43,599
exciting line of research is to look at

935
00:38:43,599 --> 00:38:46,720
specific visual features for example

936
00:38:46,720 --> 00:38:48,079
color

937
00:38:48,079 --> 00:38:50,880
complexity facial presence and to see

938
00:38:50,880 --> 00:38:54,160
what feature or feature combinations

939
00:38:54,160 --> 00:38:56,560
are most indicative

940
00:38:56,560 --> 00:38:58,839
of credibility

941
00:38:58,839 --> 00:39:03,040
so i'd like to acknowledge all my

942
00:39:03,040 --> 00:39:05,520
amazing students and the collaborators

943
00:39:05,520 --> 00:39:07,760
and also the support from the national

944
00:39:07,760 --> 00:39:09,359
science foundation and facebook

945
00:39:09,359 --> 00:39:11,760
foundation integrity research award that

946
00:39:11,760 --> 00:39:14,400
partially fund the research i presented

947
00:39:14,400 --> 00:39:15,599
today

948
00:39:15,599 --> 00:39:17,839
and that's it thank you very much and

949
00:39:17,839 --> 00:39:21,760
now i'm very happy to take questions

950
00:39:21,760 --> 00:39:24,960
okay cindy thanks this was great um we

951
00:39:24,960 --> 00:39:27,200
have two questions here so i'm just

952
00:39:27,200 --> 00:39:29,760
going to read them to you

953
00:39:29,760 --> 00:39:32,000
how exactly does composition and

954
00:39:32,000 --> 00:39:34,079
retouching differ

955
00:39:34,079 --> 00:39:36,079
you're retouching example of

956
00:39:36,079 --> 00:39:39,440
manipulating a water cannon into a gun

957
00:39:39,440 --> 00:39:41,520
and it also involves superimposing the

958
00:39:41,520 --> 00:39:43,520
image of a gun over the original water

959
00:39:43,520 --> 00:39:46,640
cannon so yeah that's how how it's done

960
00:39:46,640 --> 00:39:49,599
right so i guess this is this is a great

961
00:39:49,599 --> 00:39:52,079
question thank you and i guess um i

962
00:39:52,079 --> 00:39:54,000
would i would think the main difference

963
00:39:54,000 --> 00:39:55,599
would be

964
00:39:55,599 --> 00:39:58,480
the degree of change so in the example

965
00:39:58,480 --> 00:40:01,119
of let's say a sarah palin

966
00:40:01,119 --> 00:40:03,040
sarah palin's head

967
00:40:03,040 --> 00:40:06,640
plus a bikini wearing woman's body that

968
00:40:06,640 --> 00:40:09,200
some you know you kind of need

969
00:40:09,200 --> 00:40:12,160
extensive change to the original picture

970
00:40:12,160 --> 00:40:14,960
by combining two or sometimes three or

971
00:40:14,960 --> 00:40:17,520
even more pictures together well i think

972
00:40:17,520 --> 00:40:20,320
retouching is more subtle and

973
00:40:20,320 --> 00:40:24,000
it only affects like a little part of

974
00:40:24,000 --> 00:40:25,839
the original image and i would say to

975
00:40:25,839 --> 00:40:27,440
some extent like

976
00:40:27,440 --> 00:40:30,000
um like zoom you know it could retouch

977
00:40:30,000 --> 00:40:32,240
your appearance so that that is

978
00:40:32,240 --> 00:40:35,440
retouching instead of composition i hope

979
00:40:35,440 --> 00:40:38,640
that makes sense

980
00:40:38,640 --> 00:40:40,960
yeah that's great another question here

981
00:40:40,960 --> 00:40:43,760
um and this goes back to the um

982
00:40:43,760 --> 00:40:46,160
uh you know experience equals more

983
00:40:46,160 --> 00:40:47,680
skepticism

984
00:40:47,680 --> 00:40:50,160
so what constitutes i think it was a

985
00:40:50,160 --> 00:40:52,079
bullet point what constitutes high

986
00:40:52,079 --> 00:40:53,680
internet skills

987
00:40:53,680 --> 00:40:56,640
oh that's a great question yeah so there

988
00:40:56,640 --> 00:40:58,240
are

989
00:40:58,240 --> 00:41:01,200
scales for those it's just like um you

990
00:41:01,200 --> 00:41:03,680
know you think of let's say um a

991
00:41:03,680 --> 00:41:06,319
personality test right where personality

992
00:41:06,319 --> 00:41:09,040
tests uh usually people would have to

993
00:41:09,040 --> 00:41:12,400
like take 30 40 questions and then

994
00:41:12,400 --> 00:41:14,880
with those questions they would compute

995
00:41:14,880 --> 00:41:17,040
the composite score and then determine

996
00:41:17,040 --> 00:41:19,040
you know you are you know high in

997
00:41:19,040 --> 00:41:21,760
agreeableness low in neuroticism

998
00:41:21,760 --> 00:41:24,720
something like that so we use a uh

999
00:41:24,720 --> 00:41:27,040
established scale

1000
00:41:27,040 --> 00:41:29,520
of people's internet skills

1001
00:41:29,520 --> 00:41:32,240
and the the scale was actually pretty

1002
00:41:32,240 --> 00:41:34,960
simple so what it does it it has let's

1003
00:41:34,960 --> 00:41:36,960
say 20 or 30

1004
00:41:36,960 --> 00:41:38,880
um statements

1005
00:41:38,880 --> 00:41:41,200
and then you could say oh this is true

1006
00:41:41,200 --> 00:41:44,400
this is false or i would ask you do you

1007
00:41:44,400 --> 00:41:46,079
know the answer to this question for

1008
00:41:46,079 --> 00:41:48,480
example what is an you know browser

1009
00:41:48,480 --> 00:41:49,440
cookie

1010
00:41:49,440 --> 00:41:51,920
or i would provide a statement a cookie

1011
00:41:51,920 --> 00:41:55,280
is this a vpn is that something like

1012
00:41:55,280 --> 00:41:56,880
that you know i i'm blanking on the

1013
00:41:56,880 --> 00:41:59,920
exact items yes but this is the

1014
00:41:59,920 --> 00:42:02,800
generally how it was done so then we

1015
00:42:02,800 --> 00:42:06,000
gave this battery of questions to each

1016
00:42:06,000 --> 00:42:07,440
participant

1017
00:42:07,440 --> 00:42:10,560
and then each one would have a score and

1018
00:42:10,560 --> 00:42:13,680
that score would determine their

1019
00:42:13,680 --> 00:42:16,400
digital media literacy

1020
00:42:16,400 --> 00:42:17,520
okay

1021
00:42:17,520 --> 00:42:20,160
uh we have another one um do you think

1022
00:42:20,160 --> 00:42:22,079
the research you've done would also line

1023
00:42:22,079 --> 00:42:24,160
up with deep fakes

1024
00:42:24,160 --> 00:42:25,040
yes

1025
00:42:25,040 --> 00:42:27,599
that's a great question um

1026
00:42:27,599 --> 00:42:29,359
and i've been thinking a lot about

1027
00:42:29,359 --> 00:42:31,040
defects as well

1028
00:42:31,040 --> 00:42:32,079
so

1029
00:42:32,079 --> 00:42:34,560
um one thing i think that's very much in

1030
00:42:34,560 --> 00:42:35,760
common

1031
00:42:35,760 --> 00:42:38,079
is a recurrent theme that people are

1032
00:42:38,079 --> 00:42:40,079
very bad at

1033
00:42:40,079 --> 00:42:43,520
detecting forgeries in image

1034
00:42:43,520 --> 00:42:46,160
and then with technology such as deep

1035
00:42:46,160 --> 00:42:47,200
fake

1036
00:42:47,200 --> 00:42:50,720
i think we are even more vulnerable

1037
00:42:50,720 --> 00:42:54,480
in the past um as i said images such as

1038
00:42:54,480 --> 00:42:57,040
the moon landing right images

1039
00:42:57,040 --> 00:42:59,680
by default people think it depicts the

1040
00:42:59,680 --> 00:43:01,040
true events

1041
00:43:01,040 --> 00:43:01,760
now

1042
00:43:01,760 --> 00:43:03,920
more and more people know photoshop

1043
00:43:03,920 --> 00:43:05,760
exists more and more people are using

1044
00:43:05,760 --> 00:43:09,119
photoshop right so to some extent people

1045
00:43:09,119 --> 00:43:12,800
starting to see that oh maybe an image

1046
00:43:12,800 --> 00:43:15,040
is not by default

1047
00:43:15,040 --> 00:43:18,079
a depiction of true events

1048
00:43:18,079 --> 00:43:18,960
okay

1049
00:43:18,960 --> 00:43:19,839
but

1050
00:43:19,839 --> 00:43:22,800
the same thought applies to video even

1051
00:43:22,800 --> 00:43:25,280
more right because i think video editing

1052
00:43:25,280 --> 00:43:28,240
software or the software to fabricate

1053
00:43:28,240 --> 00:43:31,200
video like deep fake is much less

1054
00:43:31,200 --> 00:43:33,359
available or accessible to the general

1055
00:43:33,359 --> 00:43:35,839
public that people will still very much

1056
00:43:35,839 --> 00:43:40,000
think that video oh it must be true okay

1057
00:43:40,000 --> 00:43:43,200
so to some extent i think deep fake is

1058
00:43:43,200 --> 00:43:45,440
an even bigger threat

1059
00:43:45,440 --> 00:43:46,400
than

1060
00:43:46,400 --> 00:43:48,000
fake images

1061
00:43:48,000 --> 00:43:50,640
my lab has been trying to make some

1062
00:43:50,640 --> 00:43:53,760
arrows into deep fake and so far

1063
00:43:53,760 --> 00:43:56,400
our impression is that yes if we are

1064
00:43:56,400 --> 00:43:59,920
vulnerable to fake images we are

1065
00:43:59,920 --> 00:44:03,200
extremely vulnerable to device

1066
00:44:03,200 --> 00:44:04,960
and i think we need we need

1067
00:44:04,960 --> 00:44:08,319
technological solutions as well as human

1068
00:44:08,319 --> 00:44:12,160
solutions um such as policy change such

1069
00:44:12,160 --> 00:44:13,040
as

1070
00:44:13,040 --> 00:44:14,000
you know

1071
00:44:14,000 --> 00:44:15,359
laws

1072
00:44:15,359 --> 00:44:16,240
to

1073
00:44:16,240 --> 00:44:17,359
kind of

1074
00:44:17,359 --> 00:44:19,680
keep the threat at bay otherwise we're

1075
00:44:19,680 --> 00:44:21,680
doomed

1076
00:44:21,680 --> 00:44:23,520
um i had just had a quick thought kind

1077
00:44:23,520 --> 00:44:26,640
of technology related to a point you

1078
00:44:26,640 --> 00:44:28,560
just said there but like like

1079
00:44:28,560 --> 00:44:30,800
are platforms using like

1080
00:44:30,800 --> 00:44:32,079
uh

1081
00:44:32,079 --> 00:44:35,119
detection to detect some of these uh

1082
00:44:35,119 --> 00:44:37,040
fakes like are they using like checking

1083
00:44:37,040 --> 00:44:39,440
for like fake accounts or bots or and

1084
00:44:39,440 --> 00:44:41,440
stuff like that is that happening too is

1085
00:44:41,440 --> 00:44:44,000
that like a tool that they have to yes

1086
00:44:44,000 --> 00:44:46,319
absolutely i i think there are some

1087
00:44:46,319 --> 00:44:48,800
technological solutions uh one is to go

1088
00:44:48,800 --> 00:44:50,800
after the source right so russian today

1089
00:44:50,800 --> 00:44:53,200
for example we know it post dubious

1090
00:44:53,200 --> 00:44:55,680
stuff so if it's from russian today then

1091
00:44:55,680 --> 00:44:57,680
we have to be on high alert doesn't

1092
00:44:57,680 --> 00:44:59,440
necessarily mean that everything that

1093
00:44:59,440 --> 00:45:00,880
published there

1094
00:45:00,880 --> 00:45:03,119
is fake otherwise the job will be too

1095
00:45:03,119 --> 00:45:05,520
easy i think the fake stuff is

1096
00:45:05,520 --> 00:45:07,599
especially dangerous

1097
00:45:07,599 --> 00:45:10,720
if you know it's published by

1098
00:45:10,720 --> 00:45:13,680
and you know a somewhat trustworthy

1099
00:45:13,680 --> 00:45:16,319
outlet right when they say if 100

1100
00:45:16,319 --> 00:45:18,079
everything is alive and actually that's

1101
00:45:18,079 --> 00:45:20,880
pretty easy but if 99 is true and

1102
00:45:20,880 --> 00:45:22,800
there's one percent of lie and that's

1103
00:45:22,800 --> 00:45:25,280
the most difficult challenge there so i

1104
00:45:25,280 --> 00:45:27,119
think we can definitely go after the

1105
00:45:27,119 --> 00:45:30,079
source and there's some um

1106
00:45:30,079 --> 00:45:32,319
technical solutions like perspective

1107
00:45:32,319 --> 00:45:34,960
analysis metadata analysis encryption

1108
00:45:34,960 --> 00:45:36,160
analysis

1109
00:45:36,160 --> 00:45:39,440
to directly detect the probability of

1110
00:45:39,440 --> 00:45:42,319
something being forged i think i've seen

1111
00:45:42,319 --> 00:45:44,800
some demonstrations of that but the

1112
00:45:44,800 --> 00:45:47,440
trouble is that they are not a hundred

1113
00:45:47,440 --> 00:45:50,720
percent reliable and not nothing is so

1114
00:45:50,720 --> 00:45:53,599
that puts a lot of pressure on platforms

1115
00:45:53,599 --> 00:45:54,480
um

1116
00:45:54,480 --> 00:45:56,800
i think uh analogy here is content

1117
00:45:56,800 --> 00:45:59,280
moderation right so let's say you know

1118
00:45:59,280 --> 00:46:02,240
facebook is trying to prevent uh child

1119
00:46:02,240 --> 00:46:04,400
pornography right but in that process it

1120
00:46:04,400 --> 00:46:07,599
cannot it's ai cannot identify a hundred

1121
00:46:07,599 --> 00:46:09,920
percent you know with a hundred percent

1122
00:46:09,920 --> 00:46:12,240
precision and recall it's just not

1123
00:46:12,240 --> 00:46:14,960
possible so you have to have some kind

1124
00:46:14,960 --> 00:46:17,760
of a human a human being you know

1125
00:46:17,760 --> 00:46:20,480
intervene and that creates you know a

1126
00:46:20,480 --> 00:46:23,760
lot of gray area a lot of space you know

1127
00:46:23,760 --> 00:46:25,040
for errors

1128
00:46:25,040 --> 00:46:27,440
and then you know lots of problems as

1129
00:46:27,440 --> 00:46:29,359
we've been reading the newspapers and so

1130
00:46:29,359 --> 00:46:32,160
on so i i see a lot of similarities

1131
00:46:32,160 --> 00:46:35,119
between you know detecting forgeries and

1132
00:46:35,119 --> 00:46:36,240
you know detecting some other

1133
00:46:36,240 --> 00:46:39,200
problematic content

1134
00:46:39,520 --> 00:46:42,000
another question here can we prove that

1135
00:46:42,000 --> 00:46:44,079
spending more time viewing real images

1136
00:46:44,079 --> 00:46:46,079
allows someone to detect fake images

1137
00:46:46,079 --> 00:46:49,119
more easily does viewing images

1138
00:46:49,119 --> 00:46:52,000
more often reduce their digital media

1139
00:46:52,000 --> 00:46:54,880
literacy so if viewing

1140
00:46:54,880 --> 00:46:57,520
if viewing fake images more often does

1141
00:46:57,520 --> 00:46:59,760
that reduce their literacy

1142
00:46:59,760 --> 00:47:01,520
so when they're being

1143
00:47:01,520 --> 00:47:02,640
you know

1144
00:47:02,640 --> 00:47:06,079
bombarded with more fake images do they

1145
00:47:06,079 --> 00:47:07,359
just

1146
00:47:07,359 --> 00:47:10,000
lose you know track of what's real i

1147
00:47:10,000 --> 00:47:11,359
guess

1148
00:47:11,359 --> 00:47:13,680
i'm not sure i think that's a very

1149
00:47:13,680 --> 00:47:14,800
interesting

1150
00:47:14,800 --> 00:47:17,200
projection but my

1151
00:47:17,200 --> 00:47:20,319
um i think my hunch is that um

1152
00:47:20,319 --> 00:47:22,319
uh i i don't know about images but my

1153
00:47:22,319 --> 00:47:24,400
hunch is that was textual information

1154
00:47:24,400 --> 00:47:26,720
let's say news stories if the more you

1155
00:47:26,720 --> 00:47:29,359
read fake news stories

1156
00:47:29,359 --> 00:47:31,520
uh my hunch is that the more likely

1157
00:47:31,520 --> 00:47:33,359
you're going to be vulnerable to more

1158
00:47:33,359 --> 00:47:34,960
fake news stories

1159
00:47:34,960 --> 00:47:37,119
uh maybe the opposite is also true the

1160
00:47:37,119 --> 00:47:40,640
more you read high quality news stories

1161
00:47:40,640 --> 00:47:42,880
maybe you'll develop an appreciation of

1162
00:47:42,880 --> 00:47:44,960
what a new story should be like and

1163
00:47:44,960 --> 00:47:47,040
maybe you can build some kind of defense

1164
00:47:47,040 --> 00:47:48,240
against

1165
00:47:48,240 --> 00:47:50,319
um fake news stories because i think

1166
00:47:50,319 --> 00:47:52,400
they're kind of pretty distinct markers

1167
00:47:52,400 --> 00:47:54,720
of a good news story like you know very

1168
00:47:54,720 --> 00:47:56,960
well researched fact check news story

1169
00:47:56,960 --> 00:47:58,000
versus

1170
00:47:58,000 --> 00:48:00,880
you know ai generated garbage right

1171
00:48:00,880 --> 00:48:03,440
i do not know whether this applies to

1172
00:48:03,440 --> 00:48:06,240
image but i think that's a interesting

1173
00:48:06,240 --> 00:48:07,839
very wonderful thought

1174
00:48:07,839 --> 00:48:12,000
yeah yeah okay another question here

1175
00:48:12,000 --> 00:48:12,960
um

1176
00:48:12,960 --> 00:48:15,599
sometimes misinformation sharing is more

1177
00:48:15,599 --> 00:48:18,400
about blindly sharing information and

1178
00:48:18,400 --> 00:48:20,560
being in the trend rather than actual

1179
00:48:20,560 --> 00:48:23,599
motivation for news dissemination

1180
00:48:23,599 --> 00:48:26,400
what are the ways that can be tackled uh

1181
00:48:26,400 --> 00:48:28,079
or what are the ways this can be tackled

1182
00:48:28,079 --> 00:48:30,880
since the incentive of badges

1183
00:48:30,880 --> 00:48:33,359
won't be that driving factor for a user

1184
00:48:33,359 --> 00:48:34,960
in this case

1185
00:48:34,960 --> 00:48:35,760
um

1186
00:48:35,760 --> 00:48:37,359
i i'm actually

1187
00:48:37,359 --> 00:48:39,680
not sure so one thing we are trying to

1188
00:48:39,680 --> 00:48:42,079
experiment in our current study is badge

1189
00:48:42,079 --> 00:48:44,000
right but as this

1190
00:48:44,000 --> 00:48:46,640
commenter said maybe batch is not what's

1191
00:48:46,640 --> 00:48:48,720
driving sharing either

1192
00:48:48,720 --> 00:48:51,440
um but you know i think as someone who

1193
00:48:51,440 --> 00:48:53,520
uses social media i'd like to share too

1194
00:48:53,520 --> 00:48:55,680
what you're trying to get there from

1195
00:48:55,680 --> 00:48:57,839
sharing something new and novel is to

1196
00:48:57,839 --> 00:49:00,240
gain a kind of social currency right and

1197
00:49:00,240 --> 00:49:02,640
i agree it's not always about oh like

1198
00:49:02,640 --> 00:49:04,240
you should read this but you know this

1199
00:49:04,240 --> 00:49:06,079
has some kind of value or maybe this is

1200
00:49:06,079 --> 00:49:08,319
funny or whatever so i would just

1201
00:49:08,319 --> 00:49:11,920
blindly share to my followers to gain

1202
00:49:11,920 --> 00:49:13,760
some kind of social currency what we're

1203
00:49:13,760 --> 00:49:15,280
trying to do with the badge is to

1204
00:49:15,280 --> 00:49:17,520
reverse engineer that social currency

1205
00:49:17,520 --> 00:49:18,960
but what if

1206
00:49:18,960 --> 00:49:22,079
um sharing reputable stuff also builds

1207
00:49:22,079 --> 00:49:23,599
social currency

1208
00:49:23,599 --> 00:49:26,880
um so that's that's our attempt at doing

1209
00:49:26,880 --> 00:49:27,680
that

1210
00:49:27,680 --> 00:49:30,480
but i have to admit i don't know the

1211
00:49:30,480 --> 00:49:32,240
secret of social currency i don't i

1212
00:49:32,240 --> 00:49:36,079
don't have it all figured out just yet

1213
00:49:36,079 --> 00:49:37,119
okay

1214
00:49:37,119 --> 00:49:38,880
all right great well yeah that's all the

1215
00:49:38,880 --> 00:49:41,200
questions here um yeah thank you again

1216
00:49:41,200 --> 00:49:43,119
really appreciate it and uh this talk

1217
00:49:43,119 --> 00:49:46,000
was was very very informative and you

1218
00:49:46,000 --> 00:49:47,839
know it's something that i'm always

1219
00:49:47,839 --> 00:49:49,200
reading about and this is this is

1220
00:49:49,200 --> 00:49:51,760
awesome i really love this talk so um

1221
00:49:51,760 --> 00:49:54,720
thank you so much uh thank you oh the

1222
00:49:54,720 --> 00:49:56,319
questions are wonderful thank you so

1223
00:49:56,319 --> 00:49:59,200
much great great um yeah well thanks

1224
00:49:59,200 --> 00:50:01,040
thanks again so um

1225
00:50:01,040 --> 00:50:02,960
uh attendees will we'll be back here

1226
00:50:02,960 --> 00:50:06,559
next week 4 30 uh same time so uh um

1227
00:50:06,559 --> 00:50:08,640
thanks again cindy uh stay safe thank

1228
00:50:08,640 --> 00:50:12,118
you mike bye

