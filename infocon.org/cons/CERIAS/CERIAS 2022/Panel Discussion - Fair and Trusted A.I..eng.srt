1
00:00:00,080 --> 00:00:02,560
welcome we have um

2
00:00:02,560 --> 00:00:05,040
an interesting panel coming up for you

3
00:00:05,040 --> 00:00:06,399
and uh

4
00:00:06,399 --> 00:00:07,520
kind of

5
00:00:07,520 --> 00:00:10,960
moving a little from

6
00:00:10,960 --> 00:00:14,160
traditional views of computer security

7
00:00:14,160 --> 00:00:17,840
and looking at some other other issues

8
00:00:17,840 --> 00:00:19,760
fairness which actually has a lot in

9
00:00:19,760 --> 00:00:21,840
common with data privacy

10
00:00:21,840 --> 00:00:25,199
uh and trustworthiness in ai which

11
00:00:25,199 --> 00:00:26,960
trust is something we think of a lot in

12
00:00:26,960 --> 00:00:28,640
computer security

13
00:00:28,640 --> 00:00:31,359
uh trustworthy ai

14
00:00:31,359 --> 00:00:33,920
some relationships some differences

15
00:00:33,920 --> 00:00:34,719
so

16
00:00:34,719 --> 00:00:37,040
i'm gonna just go through a couple of

17
00:00:37,040 --> 00:00:38,800
things uh

18
00:00:38,800 --> 00:00:41,040
before i introduce the panel

19
00:00:41,040 --> 00:00:43,040
uh

20
00:00:43,040 --> 00:00:46,000
you know what's the problem here

21
00:00:46,000 --> 00:00:47,039
well

22
00:00:47,039 --> 00:00:50,320
there's a lot of concerns about fairness

23
00:00:50,320 --> 00:00:53,520
trustworthiness of ai systems

24
00:00:53,520 --> 00:00:54,559
uh

25
00:00:54,559 --> 00:00:56,840
fairness ai systems

26
00:00:56,840 --> 00:01:01,520
exhibit recognizable biases

27
00:01:02,960 --> 00:01:05,600
amazon created a resume screener that

28
00:01:05,600 --> 00:01:08,960
was showing gender bias

29
00:01:09,439 --> 00:01:10,400
there's

30
00:01:10,400 --> 00:01:12,799
software being used to predict

31
00:01:12,799 --> 00:01:15,920
recidivism and that is being used in

32
00:01:15,920 --> 00:01:18,560
setting bail for example

33
00:01:18,560 --> 00:01:19,600
that

34
00:01:19,600 --> 00:01:20,720
is showing

35
00:01:20,720 --> 00:01:22,640
racial biases

36
00:01:22,640 --> 00:01:24,560
uh

37
00:01:24,560 --> 00:01:29,360
various various types of of biases here

38
00:01:29,360 --> 00:01:30,320
and

39
00:01:30,320 --> 00:01:31,840
trustworthiness

40
00:01:31,840 --> 00:01:36,079
ai systems we know can get it wrong

41
00:01:36,079 --> 00:01:38,000
and sometimes they get it wrong in ways

42
00:01:38,000 --> 00:01:40,000
that you know humans would perfectly

43
00:01:40,000 --> 00:01:41,759
succeed many of you see have seen the

44
00:01:41,759 --> 00:01:44,399
example of taking a stop sign

45
00:01:44,399 --> 00:01:46,079
putting a few stickers on it and all of

46
00:01:46,079 --> 00:01:47,680
a sudden the ai system thinks it's a

47
00:01:47,680 --> 00:01:49,759
speed limit sign

48
00:01:49,759 --> 00:01:51,360
um

49
00:01:51,360 --> 00:01:53,680
you know there's

50
00:01:53,680 --> 00:01:56,640
you know some some problems here

51
00:01:56,640 --> 00:01:58,479
contexts of this

52
00:01:58,479 --> 00:01:59,840
well

53
00:01:59,840 --> 00:02:02,000
trustworthiness has long been an issue

54
00:02:02,000 --> 00:02:04,000
in cyber security particularly in safety

55
00:02:04,000 --> 00:02:05,680
critical systems

56
00:02:05,680 --> 00:02:08,000
fairness and privacy turn out to have a

57
00:02:08,000 --> 00:02:10,878
lot in relation

58
00:02:12,239 --> 00:02:15,120
this fairness

59
00:02:15,280 --> 00:02:17,840
a lot of the work and the concern in ai

60
00:02:17,840 --> 00:02:20,480
has largely been in

61
00:02:20,480 --> 00:02:23,040
civil traditional civil rights spaces

62
00:02:23,040 --> 00:02:26,560
and those types of issues

63
00:02:26,720 --> 00:02:29,760
but are there other issues

64
00:02:29,760 --> 00:02:31,680
trustworthiness

65
00:02:31,680 --> 00:02:33,920
again has been primarily in safety

66
00:02:33,920 --> 00:02:36,800
critical systems use of ai and safety

67
00:02:36,800 --> 00:02:39,120
critical systems particularly autonomous

68
00:02:39,120 --> 00:02:40,239
vehicles

69
00:02:40,239 --> 00:02:42,400
the use of ai and autonomous vehicles

70
00:02:42,400 --> 00:02:43,200
has

71
00:02:43,200 --> 00:02:46,000
generated a lot of safety concerns

72
00:02:46,000 --> 00:02:47,599
so yeah but there's a couple of things

73
00:02:47,599 --> 00:02:49,599
to think about like you know are fair

74
00:02:49,599 --> 00:02:51,120
and trusted ai

75
00:02:51,120 --> 00:02:54,000
independent concepts do they really you

76
00:02:54,000 --> 00:02:55,680
know they really together

77
00:02:55,680 --> 00:02:57,920
we may get into some of that as we go

78
00:02:57,920 --> 00:02:59,440
into this

79
00:02:59,440 --> 00:03:00,959
uh

80
00:03:00,959 --> 00:03:03,120
as we go into this panel but let me

81
00:03:03,120 --> 00:03:05,200
start by introducing the panelists for

82
00:03:05,200 --> 00:03:06,879
those who don't know me i'm chris

83
00:03:06,879 --> 00:03:08,480
clifton i'm

84
00:03:08,480 --> 00:03:10,480
on the faculty here at purdue

85
00:03:10,480 --> 00:03:13,280
i've been part of sirius for

86
00:03:13,280 --> 00:03:15,200
over 20 years now

87
00:03:15,200 --> 00:03:17,360
and

88
00:03:17,360 --> 00:03:19,760
and actually i think

89
00:03:19,760 --> 00:03:22,239
my first interaction with sirius was in

90
00:03:22,239 --> 00:03:24,000
the previous millennium

91
00:03:24,000 --> 00:03:27,360
so uh it's been a while

92
00:03:27,360 --> 00:03:30,400
uh also with us we have a distinguished

93
00:03:30,400 --> 00:03:33,360
panel of people all of whom have

94
00:03:33,360 --> 00:03:34,560
purdue

95
00:03:34,560 --> 00:03:36,560
connections

96
00:03:36,560 --> 00:03:39,920
tom grunwald who is a senior vice

97
00:03:39,920 --> 00:03:42,959
president with blue waves ai got his phd

98
00:03:42,959 --> 00:03:46,080
here at purdue in 1976

99
00:03:46,080 --> 00:03:49,599
2007 was named an outstanding alum

100
00:03:49,599 --> 00:03:53,519
uh has a long history including faculty

101
00:03:53,519 --> 00:03:57,040
university of portland bell labs

102
00:03:57,040 --> 00:03:59,200
tel labs spectrum control

103
00:03:59,200 --> 00:04:00,319
westhall

104
00:04:00,319 --> 00:04:02,640
i'll let you go into a little on kind of

105
00:04:02,640 --> 00:04:04,159
your thoughts on these fairness

106
00:04:04,159 --> 00:04:06,560
trustworthiness issues and why this

107
00:04:06,560 --> 00:04:08,560
interests you okay thank you

108
00:04:08,560 --> 00:04:11,280
you know the our company bluewave ai

109
00:04:11,280 --> 00:04:12,239
labs

110
00:04:12,239 --> 00:04:14,959
generates software make software that's

111
00:04:14,959 --> 00:04:18,000
used in the nuclear industry as well as

112
00:04:18,000 --> 00:04:21,279
defense we do support for darpa as well

113
00:04:21,279 --> 00:04:22,720
and

114
00:04:22,720 --> 00:04:24,240
trustworthiness

115
00:04:24,240 --> 00:04:26,479
comes in a little different flavor

116
00:04:26,479 --> 00:04:28,880
with respect to nuclear because the

117
00:04:28,880 --> 00:04:31,680
folks who use our our software

118
00:04:31,680 --> 00:04:34,320
are using it to plan how they're going

119
00:04:34,320 --> 00:04:35,840
to operate

120
00:04:35,840 --> 00:04:38,240
the reactor for the next two years so we

121
00:04:38,240 --> 00:04:41,440
have to do a really thorough job

122
00:04:41,440 --> 00:04:43,199
of showing them

123
00:04:43,199 --> 00:04:44,320
that

124
00:04:44,320 --> 00:04:47,600
the software the predictions are really

125
00:04:47,600 --> 00:04:50,000
accurate and trustworthy

126
00:04:50,000 --> 00:04:52,320
and sometimes it's good to step back and

127
00:04:52,320 --> 00:04:55,120
ask yourself okay what is ai because ai

128
00:04:55,120 --> 00:04:56,800
is one of those words that can mean a

129
00:04:56,800 --> 00:04:58,320
lot of different things

130
00:04:58,320 --> 00:05:01,360
um primarily it's a blanket term for a

131
00:05:01,360 --> 00:05:03,360
lot of different techniques those of you

132
00:05:03,360 --> 00:05:05,280
who are here for the

133
00:05:05,280 --> 00:05:06,720
poster session

134
00:05:06,720 --> 00:05:07,759
saw that

135
00:05:07,759 --> 00:05:10,479
there there's an amazing set of talented

136
00:05:10,479 --> 00:05:12,320
people who are using ai in a lot of

137
00:05:12,320 --> 00:05:14,320
different ways and a lot of different ai

138
00:05:14,320 --> 00:05:15,440
techniques

139
00:05:15,440 --> 00:05:17,039
but basically

140
00:05:17,039 --> 00:05:20,000
aai is always

141
00:05:20,000 --> 00:05:24,560
supported or a reflection of the data

142
00:05:24,560 --> 00:05:26,560
there are a lot of different techniques

143
00:05:26,560 --> 00:05:29,759
but the data is really what determines

144
00:05:29,759 --> 00:05:32,400
how well the quote unquote ai is going

145
00:05:32,400 --> 00:05:35,120
to work and the data is primarily but

146
00:05:35,120 --> 00:05:38,400
not always what introduces the bias in

147
00:05:38,400 --> 00:05:40,880
our work we have to make sure that the

148
00:05:40,880 --> 00:05:44,479
data act that reflects the real way the

149
00:05:44,479 --> 00:05:47,360
reactors operate we take usually 20

150
00:05:47,360 --> 00:05:49,919
years of historical data to train a

151
00:05:49,919 --> 00:05:50,960
model

152
00:05:50,960 --> 00:05:54,320
to then give predictions

153
00:05:54,320 --> 00:05:56,880
so again we focus a lot on the data the

154
00:05:56,880 --> 00:06:00,240
other thing we worry about a lot is

155
00:06:00,240 --> 00:06:04,000
using the models in the regime that the

156
00:06:04,000 --> 00:06:07,520
data describes it it would be as if you

157
00:06:07,520 --> 00:06:09,440
were trying to

158
00:06:09,440 --> 00:06:10,960
select

159
00:06:10,960 --> 00:06:13,280
blue cars but you've never had a blue

160
00:06:13,280 --> 00:06:16,160
car in your training set

161
00:06:16,160 --> 00:06:18,639
there's also the difficulty sometimes

162
00:06:18,639 --> 00:06:21,759
with the models because you don't know

163
00:06:21,759 --> 00:06:24,319
exactly what the model is going to pick

164
00:06:24,319 --> 00:06:26,560
up on in some of these models actually

165
00:06:26,560 --> 00:06:28,479
the most complicated model in use right

166
00:06:28,479 --> 00:06:30,800
now has one trillion

167
00:06:30,800 --> 00:06:32,560
free parameters

168
00:06:32,560 --> 00:06:35,199
that are determined by the training data

169
00:06:35,199 --> 00:06:38,160
and there's a famous problem of uh

170
00:06:38,160 --> 00:06:40,800
an ai that's that was trying to to

171
00:06:40,800 --> 00:06:42,800
identify different animals

172
00:06:42,800 --> 00:06:45,520
it had problems with horses and when

173
00:06:45,520 --> 00:06:47,680
they looked into it they found out that

174
00:06:47,680 --> 00:06:48,639
oh

175
00:06:48,639 --> 00:06:51,919
the ai was training on the copyright

176
00:06:51,919 --> 00:06:53,520
that appeared in the horse pictures

177
00:06:53,520 --> 00:06:55,599
because the horse pictures that they

178
00:06:55,599 --> 00:06:58,400
used to train the ai all had a copyright

179
00:06:58,400 --> 00:07:00,160
on them but nobody else none of the

180
00:07:00,160 --> 00:07:01,680
other pictures did

181
00:07:01,680 --> 00:07:04,479
and so what that leads us to do in order

182
00:07:04,479 --> 00:07:08,400
to engender trust with our customers is

183
00:07:08,400 --> 00:07:10,840
we do a lot of work what are called

184
00:07:10,840 --> 00:07:13,120
explainability analyses

185
00:07:13,120 --> 00:07:15,759
trying to understand what features

186
00:07:15,759 --> 00:07:18,720
actually made the result come true and

187
00:07:18,720 --> 00:07:21,840
then that plus accuracy is what really

188
00:07:21,840 --> 00:07:24,639
gives our customers trust in what we do

189
00:07:24,639 --> 00:07:26,639
so it's a little different kind of trust

190
00:07:26,639 --> 00:07:29,280
than when you're dealing with people and

191
00:07:29,280 --> 00:07:31,280
i think most folks know

192
00:07:31,280 --> 00:07:33,360
that you know the europeans are

193
00:07:33,360 --> 00:07:35,280
significantly ahead of us

194
00:07:35,280 --> 00:07:37,039
in the whole trust thing they've got the

195
00:07:37,039 --> 00:07:40,240
gdpr which is about a hundred page set

196
00:07:40,240 --> 00:07:43,199
of regulations about what you do

197
00:07:43,199 --> 00:07:46,319
what you have to do when you use data

198
00:07:46,319 --> 00:07:49,039
that pertains human beings so

199
00:07:49,039 --> 00:07:51,520
that's sort of my introduction to how we

200
00:07:51,520 --> 00:07:53,919
look at trust

201
00:07:53,919 --> 00:07:57,360
okay also with us we have ashish kandu

202
00:07:57,360 --> 00:07:59,280
who is

203
00:07:59,280 --> 00:08:01,599
again purdue connections uh he's

204
00:08:01,599 --> 00:08:03,199
currently head of cyber security

205
00:08:03,199 --> 00:08:06,319
research at cisco

206
00:08:06,319 --> 00:08:10,080
but his phd is from purdue computer

207
00:08:10,080 --> 00:08:12,639
science in 2010

208
00:08:12,639 --> 00:08:14,720
for those of you tomorrow morning

209
00:08:14,720 --> 00:08:19,360
come to the awards and hear about uh

210
00:08:19,360 --> 00:08:21,520
and here are some of our awards

211
00:08:21,520 --> 00:08:24,319
ashish 10 years ago was

212
00:08:24,319 --> 00:08:26,479
11 years ago was a recipient of the

213
00:08:26,479 --> 00:08:29,758
serious diamond award

214
00:08:30,319 --> 00:08:33,039
he's previously been at ibm research and

215
00:08:33,039 --> 00:08:34,240
neuro

216
00:08:34,240 --> 00:08:35,839
unfortunately unable to join us in

217
00:08:35,839 --> 00:08:38,958
person today but is is with us so as she

218
00:08:38,958 --> 00:08:41,200
i'll let you give a little idea of

219
00:08:41,200 --> 00:08:43,120
fairness and trust and how that's of

220
00:08:43,120 --> 00:08:45,279
interest to you

221
00:08:45,279 --> 00:08:47,839
um thank you so much professor clifton

222
00:08:47,839 --> 00:08:50,480
um for your kind introduction and

223
00:08:50,480 --> 00:08:53,279
it is a pleasure and honor actually to

224
00:08:53,279 --> 00:08:57,279
be to be part of the series again

225
00:08:57,279 --> 00:08:59,920
um there's an alam series and cs alum

226
00:08:59,920 --> 00:09:02,080
from purdue

227
00:09:02,080 --> 00:09:05,279
and you know and i was a student of uh

228
00:09:05,279 --> 00:09:06,880
professor clifton

229
00:09:06,880 --> 00:09:09,839
uh it's a also a great honor to be part

230
00:09:09,839 --> 00:09:11,920
of the panel where he

231
00:09:11,920 --> 00:09:14,720
is moderating so uh thank you everyone

232
00:09:14,720 --> 00:09:15,920
again and

233
00:09:15,920 --> 00:09:18,080
the

234
00:09:18,080 --> 00:09:19,839
i think when we look at

235
00:09:19,839 --> 00:09:21,600
the important topic of fairness and

236
00:09:21,600 --> 00:09:23,600
trustworthy

237
00:09:23,600 --> 00:09:25,760
ness in ai

238
00:09:25,760 --> 00:09:28,480
right let me focus on primarily machine

239
00:09:28,480 --> 00:09:30,240
learning instead of going into the

240
00:09:30,240 --> 00:09:33,760
general area of ai

241
00:09:34,959 --> 00:09:36,640
you when you look at fairness right

242
00:09:36,640 --> 00:09:39,760
they're more about uh um something uh

243
00:09:39,760 --> 00:09:42,560
something relative like civil rights uh

244
00:09:42,560 --> 00:09:45,360
that change from geopolitical region to

245
00:09:45,360 --> 00:09:49,279
another region from time to time

246
00:09:49,360 --> 00:09:50,800
and uh

247
00:09:50,800 --> 00:09:53,839
whereas uh uh trustworthiness might

248
00:09:53,839 --> 00:09:56,240
is a close cousin but they are more that

249
00:09:56,240 --> 00:09:58,480
is more related to as

250
00:09:58,480 --> 00:10:00,880
as professor clifton and

251
00:10:00,880 --> 00:10:03,440
tom pointed out earlier right that

252
00:10:03,440 --> 00:10:06,000
they're more more related to security in

253
00:10:06,000 --> 00:10:08,720
some way in in trust can can we can you

254
00:10:08,720 --> 00:10:10,320
compromise

255
00:10:10,320 --> 00:10:11,360
a given

256
00:10:11,360 --> 00:10:14,000
machine learning system a model

257
00:10:14,000 --> 00:10:16,000
or it's uh

258
00:10:16,000 --> 00:10:18,480
or it's a real-time

259
00:10:18,480 --> 00:10:21,120
classification or prediction mechanism

260
00:10:21,120 --> 00:10:23,200
in some way that it is going to

261
00:10:23,200 --> 00:10:26,399
uh work uh in a

262
00:10:26,399 --> 00:10:28,560
in an unintended manner

263
00:10:28,560 --> 00:10:31,120
uh right so if you focus on little more

264
00:10:31,120 --> 00:10:34,720
unfair ai right

265
00:10:34,959 --> 00:10:36,959
it talks about essentially algorithmic

266
00:10:36,959 --> 00:10:37,920
bias

267
00:10:37,920 --> 00:10:39,920
uh with respect to

268
00:10:39,920 --> 00:10:41,920
certain fairness policies maybe that is

269
00:10:41,920 --> 00:10:43,600
the or regulatory requirement that is

270
00:10:43,600 --> 00:10:46,560
developed by a government or or certain

271
00:10:46,560 --> 00:10:48,880
certain civil rights body right you know

272
00:10:48,880 --> 00:10:50,240
like

273
00:10:50,240 --> 00:10:53,519
pay you know and gender equality

274
00:10:53,519 --> 00:10:54,150
or your

275
00:10:54,150 --> 00:10:55,279
[Music]

276
00:10:55,279 --> 00:10:58,320
voting rights for example

277
00:10:58,320 --> 00:11:01,360
or hiring decisions right

278
00:11:01,360 --> 00:11:03,120
uh but

279
00:11:03,120 --> 00:11:04,880
there is a general pro

280
00:11:04,880 --> 00:11:06,399
from my point of view the general

281
00:11:06,399 --> 00:11:07,760
problem there that

282
00:11:07,760 --> 00:11:09,920
when you talk about fairness in

283
00:11:09,920 --> 00:11:12,160
machine learning systems

284
00:11:12,160 --> 00:11:14,399
are we trying to make it

285
00:11:14,399 --> 00:11:17,360
fairer than what is in the world today

286
00:11:17,360 --> 00:11:18,399
right

287
00:11:18,399 --> 00:11:19,360
um

288
00:11:19,360 --> 00:11:21,760
or are we trying to match the fairness

289
00:11:21,760 --> 00:11:24,720
of of the fairness that we have in the

290
00:11:24,720 --> 00:11:26,640
world today or we plan to have in next

291
00:11:26,640 --> 00:11:29,760
few years and that is here comes the

292
00:11:29,760 --> 00:11:32,160
problem of uh now are you developing

293
00:11:32,160 --> 00:11:34,399
machine learning systems for this world

294
00:11:34,399 --> 00:11:35,200
or

295
00:11:35,200 --> 00:11:37,040
or another world

296
00:11:37,040 --> 00:11:38,959
that is in the future

297
00:11:38,959 --> 00:11:39,839
and

298
00:11:39,839 --> 00:11:41,440
in that context

299
00:11:41,440 --> 00:11:44,560
what does fairness in of ai means in

300
00:11:44,560 --> 00:11:45,839
metaverse

301
00:11:45,839 --> 00:11:46,720
okay

302
00:11:46,720 --> 00:11:49,920
that's that's something i'm currently

303
00:11:49,920 --> 00:11:52,639
working on to write a write a

304
00:11:52,639 --> 00:11:54,160
paper about

305
00:11:54,160 --> 00:11:56,000
what does it mean

306
00:11:56,000 --> 00:11:57,360
uh in in

307
00:11:57,360 --> 00:12:00,560
fairness uh and ethics of ai in in

308
00:12:00,560 --> 00:12:01,760
metagross

309
00:12:01,760 --> 00:12:04,399
of the avatars and other things

310
00:12:04,399 --> 00:12:05,360
so

311
00:12:05,360 --> 00:12:07,519
um

312
00:12:07,519 --> 00:12:08,880
and under some work that has been

313
00:12:08,880 --> 00:12:10,480
carried out there there are fairness

314
00:12:10,480 --> 00:12:13,120
metrics very well studied in from from

315
00:12:13,120 --> 00:12:15,279
the statistical

316
00:12:15,279 --> 00:12:17,519
standpoint

317
00:12:17,519 --> 00:12:18,639
there are

318
00:12:18,639 --> 00:12:21,360
so from a trustworthy point of view

319
00:12:21,360 --> 00:12:24,079
that the challenges are essentially can

320
00:12:24,079 --> 00:12:26,560
can the model be compromised right

321
00:12:26,560 --> 00:12:29,279
uh is it compromised what are the what

322
00:12:29,279 --> 00:12:32,079
are the cyber physical safety issues

323
00:12:32,079 --> 00:12:34,399
right how does what does adversarial

324
00:12:34,399 --> 00:12:36,959
machine learning play a role how does it

325
00:12:36,959 --> 00:12:39,360
play a role in in trustworthiness now

326
00:12:39,360 --> 00:12:42,160
can adversarial machine learning uh

327
00:12:42,160 --> 00:12:44,959
contribute to unfairness okay or

328
00:12:44,959 --> 00:12:47,120
fairness in machine learning systems

329
00:12:47,120 --> 00:12:49,519
that is you are they they they can

330
00:12:49,519 --> 00:12:51,680
they overlap with each other to give an

331
00:12:51,680 --> 00:12:54,399
example uh right

332
00:12:54,399 --> 00:12:56,880
imagine a map service right

333
00:12:56,880 --> 00:12:58,639
that you are you that one can use on

334
00:12:58,639 --> 00:13:01,680
there well the drive and and that map

335
00:13:01,680 --> 00:13:04,639
service is going to give directions to

336
00:13:04,639 --> 00:13:06,399
improve sales of

337
00:13:06,399 --> 00:13:09,360
an ev charging station electric

338
00:13:09,360 --> 00:13:10,800
battery charging station of the car

339
00:13:10,800 --> 00:13:12,800
right it's going to change it increase

340
00:13:12,800 --> 00:13:14,800
your distance or whatever right will you

341
00:13:14,800 --> 00:13:16,079
call it trusted

342
00:13:16,079 --> 00:13:19,440
ml or untrusted ml or unfair

343
00:13:19,440 --> 00:13:20,880
let me pause here

344
00:13:20,880 --> 00:13:23,279
and then

345
00:13:23,600 --> 00:13:26,320
we can come back to that later

346
00:13:26,320 --> 00:13:28,079
thank you

347
00:13:28,079 --> 00:13:29,120
thanks

348
00:13:29,120 --> 00:13:31,200
thanks ashish and

349
00:13:31,200 --> 00:13:33,519
the our other two panelists

350
00:13:33,519 --> 00:13:37,040
are currently affiliated with purdue and

351
00:13:37,040 --> 00:13:38,639
i've had the honor of working with both

352
00:13:38,639 --> 00:13:41,519
of them on a

353
00:13:42,000 --> 00:13:45,199
project funded by nsf and amazon on

354
00:13:45,199 --> 00:13:47,600
fairness in ai

355
00:13:47,600 --> 00:13:49,839
lindsey weinberg

356
00:13:49,839 --> 00:13:52,240
is an assistant professor in the honors

357
00:13:52,240 --> 00:13:53,519
college

358
00:13:53,519 --> 00:13:55,279
and uh i think

359
00:13:55,279 --> 00:13:58,160
has a history of kind of working

360
00:13:58,160 --> 00:14:00,639
cross-disciplinary just like being in an

361
00:14:00,639 --> 00:14:02,399
honors college which is

362
00:14:02,399 --> 00:14:04,000
kind of deals with multiple disciplines

363
00:14:04,000 --> 00:14:07,440
her phd from uc santa cruz

364
00:14:07,440 --> 00:14:09,920
was in history of consciousness

365
00:14:09,920 --> 00:14:11,680
uh she has

366
00:14:11,680 --> 00:14:13,920
the director i think founding director

367
00:14:13,920 --> 00:14:14,800
is the

368
00:14:14,800 --> 00:14:16,639
correct word for this of the tech

369
00:14:16,639 --> 00:14:19,680
justice lab here at purdue so let me

370
00:14:19,680 --> 00:14:22,720
turn it over to lindsay yeah

371
00:14:22,720 --> 00:14:24,720
can you folks hear me okay is this all

372
00:14:24,720 --> 00:14:26,720
right for folks on the back is that good

373
00:14:26,720 --> 00:14:29,279
okay awesome um thank you all so much

374
00:14:29,279 --> 00:14:31,120
for inviting me to speak with you i

375
00:14:31,120 --> 00:14:32,800
always love the opportunity to talk

376
00:14:32,800 --> 00:14:34,240
across disciplines and i'm of course

377
00:14:34,240 --> 00:14:35,600
honored to be up here with two of my

378
00:14:35,600 --> 00:14:37,440
colleagues on the grant that i'm on

379
00:14:37,440 --> 00:14:40,399
precisely on this idea of ai fairness um

380
00:14:40,399 --> 00:14:41,839
so i guess i'll just try and be very

381
00:14:41,839 --> 00:14:44,240
very quick about how i approach ai

382
00:14:44,240 --> 00:14:46,560
fairness as a feminist researcher

383
00:14:46,560 --> 00:14:48,079
and i think what my my training in

384
00:14:48,079 --> 00:14:49,680
feminist studies has taught me to think

385
00:14:49,680 --> 00:14:51,440
about is how do we center questions of

386
00:14:51,440 --> 00:14:53,519
power um and how do we think about the

387
00:14:53,519 --> 00:14:55,680
experiences of folks who are the most

388
00:14:55,680 --> 00:14:57,519
marginalized oftentimes in these

389
00:14:57,519 --> 00:14:58,399
conversations

390
00:14:58,399 --> 00:15:00,880
both in terms of who gets to participate

391
00:15:00,880 --> 00:15:03,519
in ai research who is most disparately

392
00:15:03,519 --> 00:15:06,480
impacted by ai systems whose voices and

393
00:15:06,480 --> 00:15:08,399
experiences tend to be left out of these

394
00:15:08,399 --> 00:15:10,320
discussions and really kind of immersing

395
00:15:10,320 --> 00:15:11,279
myself

396
00:15:11,279 --> 00:15:13,839
in those community experiences so i

397
00:15:13,839 --> 00:15:15,279
think there's a few ways we can start

398
00:15:15,279 --> 00:15:17,199
asking questions about ai fairness you

399
00:15:17,199 --> 00:15:18,639
know how do

400
00:15:18,639 --> 00:15:20,320
assumptions about what it means for a

401
00:15:20,320 --> 00:15:22,160
system to be fair get baked into these

402
00:15:22,160 --> 00:15:24,720
kinds of conversations to what extent

403
00:15:24,720 --> 00:15:26,639
are impacted communities at the table

404
00:15:26,639 --> 00:15:28,240
and able to give their input into

405
00:15:28,240 --> 00:15:30,399
exactly when we should decide an ai

406
00:15:30,399 --> 00:15:32,079
system is actually the best solution to

407
00:15:32,079 --> 00:15:34,399
a given problem um and i actually think

408
00:15:34,399 --> 00:15:36,560
there's two dominant ways in which ai

409
00:15:36,560 --> 00:15:38,959
fairness discourse actually perpetuates

410
00:15:38,959 --> 00:15:40,560
harm you know oftentimes we think of

411
00:15:40,560 --> 00:15:43,040
aspirations for ai fairness as achieving

412
00:15:43,040 --> 00:15:45,440
greater equality or greater fairness in

413
00:15:45,440 --> 00:15:47,199
society but i want to give you two

414
00:15:47,199 --> 00:15:48,959
examples to think about that i think

415
00:15:48,959 --> 00:15:49,680
help

416
00:15:49,680 --> 00:15:51,680
maybe temper claims that algorithmic

417
00:15:51,680 --> 00:15:52,800
fairness is something that we should

418
00:15:52,800 --> 00:15:54,399
always aspire to

419
00:15:54,399 --> 00:15:56,399
um so one example might be facial

420
00:15:56,399 --> 00:15:58,480
recognition technology right this is a

421
00:15:58,480 --> 00:16:00,800
technology with known racial and gender

422
00:16:00,800 --> 00:16:03,440
bias and within kind of conventional ai

423
00:16:03,440 --> 00:16:04,880
fairness thinking we might say okay

424
00:16:04,880 --> 00:16:06,639
let's solve for that racial and gender

425
00:16:06,639 --> 00:16:08,320
bias let's make sure that facial

426
00:16:08,320 --> 00:16:10,320
recognition technology works equally

427
00:16:10,320 --> 00:16:12,720
well across all race racial and gender

428
00:16:12,720 --> 00:16:13,920
groups right

429
00:16:13,920 --> 00:16:15,519
the problem with that type of approach

430
00:16:15,519 --> 00:16:17,360
to ai fairness is that when something

431
00:16:17,360 --> 00:16:19,040
like facial recognition technology is

432
00:16:19,040 --> 00:16:20,959
used to optimize the existing criminal

433
00:16:20,959 --> 00:16:23,199
legal system and that existing criminal

434
00:16:23,199 --> 00:16:25,519
legal system tends to mass incarcerate

435
00:16:25,519 --> 00:16:27,440
folks of color do we really want that

436
00:16:27,440 --> 00:16:29,040
system to be able to more accurately

437
00:16:29,040 --> 00:16:31,040
read folks of color spaces right when

438
00:16:31,040 --> 00:16:32,800
you listen to communities on the ground

439
00:16:32,800 --> 00:16:34,480
when you listen to communities of color

440
00:16:34,480 --> 00:16:36,480
doing activism work around the the

441
00:16:36,480 --> 00:16:38,240
prison industrial complex they don't

442
00:16:38,240 --> 00:16:40,800
necessarily want to be more legible to

443
00:16:40,800 --> 00:16:42,800
facial recognition technology and yet

444
00:16:42,800 --> 00:16:44,560
within the discourse around aspirations

445
00:16:44,560 --> 00:16:46,320
to algorithmic fairness we would assume

446
00:16:46,320 --> 00:16:49,120
that making frt fairer would would be a

447
00:16:49,120 --> 00:16:51,440
positive outcome right um just to give

448
00:16:51,440 --> 00:16:53,199
you one other example for thinking about

449
00:16:53,199 --> 00:16:55,360
this and this is a case study that uh dr

450
00:16:55,360 --> 00:16:57,519
clifton had brought up you know compass

451
00:16:57,519 --> 00:16:59,199
is an algorithm that's used to help

452
00:16:59,199 --> 00:17:01,120
inform judges decisions about the

453
00:17:01,120 --> 00:17:02,880
likelihood that a defendant will will

454
00:17:02,880 --> 00:17:04,480
become a repeat offender right so it's

455
00:17:04,480 --> 00:17:06,480
meant to guide judges in making these

456
00:17:06,480 --> 00:17:08,720
really difficult decisions it's probably

457
00:17:08,720 --> 00:17:10,640
a more accurate tool because it's based

458
00:17:10,640 --> 00:17:12,640
on you know historical data patterns

459
00:17:12,640 --> 00:17:14,480
right and yet we know that folks of

460
00:17:14,480 --> 00:17:16,160
color are disproportionately targeted

461
00:17:16,160 --> 00:17:17,760
within the prison industrial complex

462
00:17:17,760 --> 00:17:19,039
right so we also see that these

463
00:17:19,039 --> 00:17:21,280
decisions can optimize those existing

464
00:17:21,280 --> 00:17:22,480
biases

465
00:17:22,480 --> 00:17:24,160
if we were to fix compass right if we

466
00:17:24,160 --> 00:17:25,839
were to make sure that compass is

467
00:17:25,839 --> 00:17:27,679
equally predictably accurate for all

468
00:17:27,679 --> 00:17:29,760
racial and ethnic groups would that be a

469
00:17:29,760 --> 00:17:31,840
fairer tool and i think what folks on

470
00:17:31,840 --> 00:17:33,520
the ground might say is you're still

471
00:17:33,520 --> 00:17:35,600
optimizing a system that seeks to put

472
00:17:35,600 --> 00:17:37,039
folks in jail right you're still

473
00:17:37,039 --> 00:17:38,640
thinking about a problem formulation

474
00:17:38,640 --> 00:17:41,360
space where justice is conceived of how

475
00:17:41,360 --> 00:17:43,360
do we incarcerate people more fairly

476
00:17:43,360 --> 00:17:45,280
right and there's other ways to imagine

477
00:17:45,280 --> 00:17:46,400
a society

478
00:17:46,400 --> 00:17:48,080
of thinking about justice thinking about

479
00:17:48,080 --> 00:17:49,360
what do we do with people who commit

480
00:17:49,360 --> 00:17:51,039
harms in society that aren't rooted in

481
00:17:51,039 --> 00:17:52,960
mass incarceration right lots of folks

482
00:17:52,960 --> 00:17:54,799
make other kinds of arguments for ways

483
00:17:54,799 --> 00:17:56,640
we might solve these problems and yet

484
00:17:56,640 --> 00:17:58,400
the problem formulation space within the

485
00:17:58,400 --> 00:18:00,240
dominant discourse is how do we optimize

486
00:18:00,240 --> 00:18:01,919
the status quo how do we optimize this

487
00:18:01,919 --> 00:18:04,000
compass technology to be fair right so

488
00:18:04,000 --> 00:18:05,840
my my discipline teaches me to kind of

489
00:18:05,840 --> 00:18:07,360
um interrogate the kinds of

490
00:18:07,360 --> 00:18:09,200
epistemological assumptions we make

491
00:18:09,200 --> 00:18:11,440
about what ai fairness even means right

492
00:18:11,440 --> 00:18:13,120
and to maybe take a step back from those

493
00:18:13,120 --> 00:18:15,520
those discourses

494
00:18:15,520 --> 00:18:16,640
thank you

495
00:18:16,640 --> 00:18:18,480
and our final panelist is christopher

496
00:18:18,480 --> 00:18:20,320
yeomans who is

497
00:18:20,320 --> 00:18:22,640
professor and uh

498
00:18:22,640 --> 00:18:24,799
and head of the department of philosophy

499
00:18:24,799 --> 00:18:28,160
here at purdue uh

500
00:18:28,160 --> 00:18:31,039
his phd in riverside you see riverside

501
00:18:31,039 --> 00:18:33,200
so we have two people from uc two people

502
00:18:33,200 --> 00:18:34,320
from purdue

503
00:18:34,320 --> 00:18:36,080
and and the oddball here

504
00:18:36,080 --> 00:18:37,600
and uh

505
00:18:37,600 --> 00:18:39,200
uh so

506
00:18:39,200 --> 00:18:40,080
uh

507
00:18:40,080 --> 00:18:41,840
and christopher's also had the

508
00:18:41,840 --> 00:18:43,679
experience of being a humble fellow

509
00:18:43,679 --> 00:18:46,160
where he spent time at uh ludwig

510
00:18:46,160 --> 00:18:49,679
maximilian university in munchin munich

511
00:18:49,679 --> 00:18:52,160
that was in 2016.

512
00:18:52,160 --> 00:18:54,880
so i'll let you go ahead and

513
00:18:54,880 --> 00:18:55,919
give an idea of some of the things

514
00:18:55,919 --> 00:18:57,760
you're doing in fairness yeah sure i

515
00:18:57,760 --> 00:18:59,360
mean maybe i'll also speak a little to

516
00:18:59,360 --> 00:19:00,960
the disciplinary background so

517
00:19:00,960 --> 00:19:02,240
philosophers spend a lot of time

518
00:19:02,240 --> 00:19:03,840
thinking about what you might call

519
00:19:03,840 --> 00:19:06,000
natural intelligence right the kind of

520
00:19:06,000 --> 00:19:07,760
intelligence that we all

521
00:19:07,760 --> 00:19:10,240
seem to have or lack as the case may be

522
00:19:10,240 --> 00:19:12,240
on any given day

523
00:19:12,240 --> 00:19:13,360
and

524
00:19:13,360 --> 00:19:15,200
spoiler alert the result is it's really

525
00:19:15,200 --> 00:19:16,720
complicated

526
00:19:16,720 --> 00:19:20,000
so part of my interest in ai generally

527
00:19:20,000 --> 00:19:22,640
is wondering how any of this could be

528
00:19:22,640 --> 00:19:24,640
built from the ground up

529
00:19:24,640 --> 00:19:25,840
right and

530
00:19:25,840 --> 00:19:27,280
trying to see

531
00:19:27,280 --> 00:19:29,520
what insights might be derived from

532
00:19:29,520 --> 00:19:31,600
trying to do it that way

533
00:19:31,600 --> 00:19:34,960
the kinds of modeling that are involved

534
00:19:34,960 --> 00:19:36,320
but with a

535
00:19:36,320 --> 00:19:41,919
heavy dose of i think realism about

536
00:19:41,919 --> 00:19:46,160
the complexity and context sensitivity

537
00:19:46,160 --> 00:19:49,440
of all sorts of human judgments whether

538
00:19:49,440 --> 00:19:52,320
epistemic or normative judgments that is

539
00:19:52,320 --> 00:19:54,480
judgments about what is the case or

540
00:19:54,480 --> 00:19:56,160
judgments about what ought to be the

541
00:19:56,160 --> 00:19:58,480
case um and so

542
00:19:58,480 --> 00:20:01,440
uh even within philosophy i think

543
00:20:01,440 --> 00:20:05,200
i am on a continuum of people i'm on one

544
00:20:05,200 --> 00:20:07,760
end thinking that in particular

545
00:20:07,760 --> 00:20:10,240
our normative judgments our moral and

546
00:20:10,240 --> 00:20:13,360
our political judgments are incredibly

547
00:20:13,360 --> 00:20:15,120
complicated they're very context

548
00:20:15,120 --> 00:20:18,159
sensitive i'm a value pluralist i don't

549
00:20:18,159 --> 00:20:20,320
think that our moral or political values

550
00:20:20,320 --> 00:20:22,240
are reducible to

551
00:20:22,240 --> 00:20:25,679
freedom or equality or one single value

552
00:20:25,679 --> 00:20:27,280
i think we always have values that are

553
00:20:27,280 --> 00:20:29,760
trading off against each other and that

554
00:20:29,760 --> 00:20:31,039
there's

555
00:20:31,039 --> 00:20:33,679
no equation for what the right trade-off

556
00:20:33,679 --> 00:20:37,120
is particularly in any given situation

557
00:20:37,120 --> 00:20:38,000
um

558
00:20:38,000 --> 00:20:41,360
to come to fairness in particular i

559
00:20:41,360 --> 00:20:43,120
think it's notable

560
00:20:43,120 --> 00:20:45,520
that of all of the sort of

561
00:20:45,520 --> 00:20:48,480
terms for normative rightness that we

562
00:20:48,480 --> 00:20:51,600
have like if you put fair up against

563
00:20:51,600 --> 00:20:56,240
just good right moral i think fair at

564
00:20:56,240 --> 00:20:58,760
least in english is the most

565
00:20:58,760 --> 00:21:01,200
context-sensitive and the most local of

566
00:21:01,200 --> 00:21:04,080
all of those terms and the most likely

567
00:21:04,080 --> 00:21:05,679
to be tied to

568
00:21:05,679 --> 00:21:08,080
particular decisions in particular

569
00:21:08,080 --> 00:21:11,440
contexts and that raises particular

570
00:21:11,440 --> 00:21:13,679
issues of complexity for any attempt to

571
00:21:13,679 --> 00:21:16,400
automate this or to explain what an

572
00:21:16,400 --> 00:21:20,080
automated system is done with it um

573
00:21:20,080 --> 00:21:21,440
and

574
00:21:21,440 --> 00:21:22,960
so anyway those are sort of my

575
00:21:22,960 --> 00:21:24,480
expectations of what artificial

576
00:21:24,480 --> 00:21:26,000
intelligence will look like from

577
00:21:26,000 --> 00:21:27,760
thinking about what natural intelligence

578
00:21:27,760 --> 00:21:29,120
will look like

579
00:21:29,120 --> 00:21:30,799
i do think that there's an upside there

580
00:21:30,799 --> 00:21:33,120
which is i do think sometimes people in

581
00:21:33,120 --> 00:21:35,440
the ii community think

582
00:21:35,440 --> 00:21:38,559
you won't have found fair ai until you

583
00:21:38,559 --> 00:21:40,960
can eliminate disagreement

584
00:21:40,960 --> 00:21:43,679
about the decisions that are being made

585
00:21:43,679 --> 00:21:45,760
but uh let me tell you that's a fool's

586
00:21:45,760 --> 00:21:47,280
errand right you're not going to

587
00:21:47,280 --> 00:21:49,440
eliminate that kind of disagreement and

588
00:21:49,440 --> 00:21:51,360
it's more important to understand the

589
00:21:51,360 --> 00:21:53,520
quality of the disagreement and it's

590
00:21:53,520 --> 00:21:55,679
more important to understand the

591
00:21:55,679 --> 00:21:57,440
different groups and the different

592
00:21:57,440 --> 00:21:59,280
perspectives and the different interests

593
00:21:59,280 --> 00:22:01,360
that are involved in the disagreement

594
00:22:01,360 --> 00:22:03,600
and getting those people talking to each

595
00:22:03,600 --> 00:22:07,440
other about how as she put it to make it

596
00:22:07,440 --> 00:22:08,559
fairer

597
00:22:08,559 --> 00:22:12,960
right that's the goal right um and um

598
00:22:12,960 --> 00:22:16,480
right so maybe i'll just stop there

599
00:22:16,480 --> 00:22:18,640
well i'll tell you we've we've had some

600
00:22:18,640 --> 00:22:20,799
discussions amongst ourselves email

601
00:22:20,799 --> 00:22:22,720
discussions and came up with a few

602
00:22:22,720 --> 00:22:24,880
questions and so i'm gonna pose those

603
00:22:24,880 --> 00:22:26,640
and

604
00:22:26,640 --> 00:22:29,520
to get things started but you know as we

605
00:22:29,520 --> 00:22:30,880
go on if you've got

606
00:22:30,880 --> 00:22:33,360
questions to add or or

607
00:22:33,360 --> 00:22:35,280
you know things you want to post the

608
00:22:35,280 --> 00:22:37,600
panelists things you want to add

609
00:22:37,600 --> 00:22:39,679
please step up to the mic i'd like this

610
00:22:39,679 --> 00:22:41,840
to be a discussion i'm going to start

611
00:22:41,840 --> 00:22:43,039
off with the first

612
00:22:43,039 --> 00:22:45,200
and this kind of builds on something

613
00:22:45,200 --> 00:22:46,080
she's

614
00:22:46,080 --> 00:22:48,799
talked about in trustworthiness which is

615
00:22:48,799 --> 00:22:51,520
you know how much do we really need

616
00:22:51,520 --> 00:22:53,520
what about fairness why should we care

617
00:22:53,520 --> 00:22:56,559
that ai is fair even if humans aren't

618
00:22:56,559 --> 00:22:58,400
fair and ashish i'm going to put you on

619
00:22:58,400 --> 00:23:02,240
the spot to start this off

620
00:23:05,280 --> 00:23:09,559
up we go there we go

621
00:23:10,799 --> 00:23:13,520
can you hear me now

622
00:23:13,520 --> 00:23:15,440
yes we're good okay

623
00:23:15,440 --> 00:23:17,440
thank you project so

624
00:23:17,440 --> 00:23:20,320
yeah so why do we why do we need uh ai

625
00:23:20,320 --> 00:23:24,799
to be fair right so if you look at

626
00:23:26,000 --> 00:23:27,840
um

627
00:23:27,840 --> 00:23:30,799
if we look at uh

628
00:23:30,960 --> 00:23:32,240
the the the

629
00:23:32,240 --> 00:23:34,640
you know how how machine learning models

630
00:23:34,640 --> 00:23:37,039
or machine learning systems work right

631
00:23:37,039 --> 00:23:38,720
they are data driven primarily they are

632
00:23:38,720 --> 00:23:39,840
not a

633
00:23:39,840 --> 00:23:41,600
implementation of a deterministic

634
00:23:41,600 --> 00:23:42,720
algorithm

635
00:23:42,720 --> 00:23:44,960
that is

636
00:23:44,960 --> 00:23:47,120
being used there and it's going to be

637
00:23:47,120 --> 00:23:50,000
totally contact context independent um

638
00:23:50,000 --> 00:23:52,320
you know so so machine learning models

639
00:23:52,320 --> 00:23:53,279
are

640
00:23:53,279 --> 00:23:56,159
highly context sensitive with an and

641
00:23:56,159 --> 00:23:58,480
dependent on the data

642
00:23:58,480 --> 00:24:00,880
uh they're number one the data that is

643
00:24:00,880 --> 00:24:02,320
being used for training

644
00:24:02,320 --> 00:24:03,520
number two

645
00:24:03,520 --> 00:24:04,640
the

646
00:24:04,640 --> 00:24:06,799
way the training is carried out you know

647
00:24:06,799 --> 00:24:08,480
it's like

648
00:24:08,480 --> 00:24:10,799
there are there are work on going on

649
00:24:10,799 --> 00:24:13,279
determining even for the same type of

650
00:24:13,279 --> 00:24:15,919
data are there any other other variables

651
00:24:15,919 --> 00:24:18,080
that affect how the models here right

652
00:24:18,080 --> 00:24:20,559
then there are hyper parameter tuning

653
00:24:20,559 --> 00:24:22,640
you know uh

654
00:24:22,640 --> 00:24:25,760
so the weights are our

655
00:24:25,760 --> 00:24:28,240
important aspects of uh how the model

656
00:24:28,240 --> 00:24:30,080
will we have later even after training

657
00:24:30,080 --> 00:24:31,279
right

658
00:24:31,279 --> 00:24:33,840
so so

659
00:24:33,919 --> 00:24:35,760
those are different aspects and then

660
00:24:35,760 --> 00:24:38,799
there are aspects of at the run time

661
00:24:38,799 --> 00:24:41,200
when the model is let's say is is based

662
00:24:41,200 --> 00:24:43,039
on tensorflow right

663
00:24:43,039 --> 00:24:46,480
and you are going to run it on on

664
00:24:46,480 --> 00:24:49,760
nvidia gpus for example right but if

665
00:24:49,760 --> 00:24:52,240
there are

666
00:24:53,120 --> 00:24:56,480
the model doesn't execute properly right

667
00:24:56,480 --> 00:24:58,640
or there are certain other issues with

668
00:24:58,640 --> 00:25:00,320
respect to the run time

669
00:25:00,320 --> 00:25:02,240
maybe maybe

670
00:25:02,240 --> 00:25:04,640
we are looking at that the

671
00:25:04,640 --> 00:25:06,559
ai may not be may not be acting in a

672
00:25:06,559 --> 00:25:08,720
fair manner maybe the run time is not

673
00:25:08,720 --> 00:25:10,320
fair itself you know the model may be

674
00:25:10,320 --> 00:25:13,600
trained so so there is a necessity for

675
00:25:13,600 --> 00:25:16,080
looking at fairness of ai not just from

676
00:25:16,080 --> 00:25:18,480
algorithm algorithmic point of view not

677
00:25:18,480 --> 00:25:20,720
just from data bias which the industry

678
00:25:20,720 --> 00:25:22,320
has been looking at or the academy has

679
00:25:22,320 --> 00:25:24,240
been looking at they have to look at the

680
00:25:24,240 --> 00:25:25,440
entire

681
00:25:25,440 --> 00:25:27,919
life cycle of the model model governance

682
00:25:27,919 --> 00:25:29,360
you know

683
00:25:29,360 --> 00:25:32,640
our ai governance and try to see where

684
00:25:32,640 --> 00:25:34,400
fairness can be crafted and that is

685
00:25:34,400 --> 00:25:35,279
where

686
00:25:35,279 --> 00:25:36,480
key threats

687
00:25:36,480 --> 00:25:40,159
to fairness might come in you know

688
00:25:40,240 --> 00:25:42,640
and and fairness could also lead to

689
00:25:42,640 --> 00:25:45,440
privacy disclosures um

690
00:25:45,440 --> 00:25:47,760
you know so so there is there is a

691
00:25:47,760 --> 00:25:49,520
strong overlap between

692
00:25:49,520 --> 00:25:51,279
as you know civil rights privacy and

693
00:25:51,279 --> 00:25:54,840
everything similarly ai fairness and

694
00:25:54,840 --> 00:25:58,240
then privacy is also there and then that

695
00:25:58,240 --> 00:26:01,279
is why we need to have explainability

696
00:26:01,279 --> 00:26:02,559
and as

697
00:26:02,559 --> 00:26:04,320
tom pointed out

698
00:26:04,320 --> 00:26:05,840
earlier

699
00:26:05,840 --> 00:26:09,200
we need to determine what bias means

700
00:26:09,200 --> 00:26:12,159
what are the latent bias what are the

701
00:26:12,159 --> 00:26:14,720
explicit and implicit bias in the data

702
00:26:14,720 --> 00:26:17,600
or in the systems

703
00:26:17,760 --> 00:26:19,120
and and

704
00:26:19,120 --> 00:26:20,799
perhaps perhaps

705
00:26:20,799 --> 00:26:22,720
improve the fairness matrix that we have

706
00:26:22,720 --> 00:26:25,600
from just statistical properties to to

707
00:26:25,600 --> 00:26:27,600
properties with respect to

708
00:26:27,600 --> 00:26:28,960
computing

709
00:26:28,960 --> 00:26:30,640
or our

710
00:26:30,640 --> 00:26:33,200
runtime related properties

711
00:26:33,200 --> 00:26:34,320
so

712
00:26:34,320 --> 00:26:35,120
uh

713
00:26:35,120 --> 00:26:36,559
i think i think those are those are

714
00:26:36,559 --> 00:26:38,240
different aspects uh that we need to

715
00:26:38,240 --> 00:26:40,799
address and and governance is important

716
00:26:40,799 --> 00:26:42,400
there and regulatory compliance last

717
00:26:42,400 --> 00:26:44,080
point i'd like to bring out is that

718
00:26:44,080 --> 00:26:46,000
perhaps this this is where the the

719
00:26:46,000 --> 00:26:48,000
government and and the

720
00:26:48,000 --> 00:26:49,360
the federal

721
00:26:49,360 --> 00:26:51,760
uh machinery should come up with

722
00:26:51,760 --> 00:26:54,240
regulatory requirements on what it means

723
00:26:54,240 --> 00:26:57,600
by ai fairness that is for only u.s then

724
00:26:57,600 --> 00:27:00,480
then state rate fairness would change

725
00:27:00,480 --> 00:27:02,559
geopolitical fairness would change over

726
00:27:02,559 --> 00:27:05,360
time and and if you there are the what

727
00:27:05,360 --> 00:27:07,679
is going on right now right the ukraine

728
00:27:07,679 --> 00:27:10,240
um invasion

729
00:27:10,240 --> 00:27:12,080
what do you mean by fairness there you

730
00:27:12,080 --> 00:27:14,080
know there are there are drones that is

731
00:27:14,080 --> 00:27:15,360
being used

732
00:27:15,360 --> 00:27:18,080
uh there are there could be um perhaps

733
00:27:18,080 --> 00:27:21,559
robots on

734
00:27:22,960 --> 00:27:24,960
are they uh are they being tally

735
00:27:24,960 --> 00:27:26,720
operated remotely

736
00:27:26,720 --> 00:27:27,520
okay

737
00:27:27,520 --> 00:27:29,039
are they being operated uh

738
00:27:29,039 --> 00:27:30,480
semi-autonomously or are there

739
00:27:30,480 --> 00:27:32,640
autonomous weapons right

740
00:27:32,640 --> 00:27:34,240
so all those things bring out the

741
00:27:34,240 --> 00:27:36,720
question is are they going to target

742
00:27:36,720 --> 00:27:38,159
civilians

743
00:27:38,159 --> 00:27:41,039
um or are they going to target only

744
00:27:41,039 --> 00:27:43,600
military

745
00:27:45,760 --> 00:27:46,840
targets

746
00:27:46,840 --> 00:27:50,880
so uh so let me let me pause here

747
00:27:50,880 --> 00:27:53,440
and and uh you know

748
00:27:53,440 --> 00:27:54,880
see if there are other questions or

749
00:27:54,880 --> 00:27:57,200
thoughts

750
00:27:57,200 --> 00:27:59,600
you also want to pick up on this of you

751
00:27:59,600 --> 00:28:03,360
know why do we care that ai is fair

752
00:28:04,480 --> 00:28:07,200
i guess i would say we care that ai is

753
00:28:07,200 --> 00:28:09,360
fair for the same reasons we care

754
00:28:09,360 --> 00:28:12,640
about people being treated unfairly you

755
00:28:12,640 --> 00:28:15,039
know we we try not to tolerate that

756
00:28:15,039 --> 00:28:17,039
um and i know in europe you know they've

757
00:28:17,039 --> 00:28:18,399
got this

758
00:28:18,399 --> 00:28:21,520
data in a sense bill of rights and if a

759
00:28:21,520 --> 00:28:23,440
machine makes a decision about you in

760
00:28:23,440 --> 00:28:25,600
europe you at least have the right to

761
00:28:25,600 --> 00:28:28,159
know how that machine made that decision

762
00:28:28,159 --> 00:28:30,399
and that that actually happened

763
00:28:30,399 --> 00:28:32,559
and and so it's sort of

764
00:28:32,559 --> 00:28:34,399
like you would in an ordinary situation

765
00:28:34,399 --> 00:28:35,679
where you're just dealing with people

766
00:28:35,679 --> 00:28:37,919
you have recourse if somebody denied you

767
00:28:37,919 --> 00:28:39,360
for a loan you can pick up the phone and

768
00:28:39,360 --> 00:28:42,159
say hey tell me why i got denied so i

769
00:28:42,159 --> 00:28:44,159
can do something about it and i think at

770
00:28:44,159 --> 00:28:46,240
the lowest level of fairness

771
00:28:46,240 --> 00:28:48,240
ai ought to be constructed so that

772
00:28:48,240 --> 00:28:51,200
there's at least a recourse and a reason

773
00:28:51,200 --> 00:28:53,520
given

774
00:28:54,080 --> 00:28:55,600
anybody just follow i was gonna say

775
00:28:55,600 --> 00:28:57,919
apart the same thing um ai should be

776
00:28:57,919 --> 00:28:59,520
fair because people should be treated

777
00:28:59,520 --> 00:29:01,760
well yeah right because that um

778
00:29:01,760 --> 00:29:03,440
there's an additional

779
00:29:03,440 --> 00:29:05,840
reason i think which is

780
00:29:05,840 --> 00:29:09,679
ai applications are

781
00:29:09,679 --> 00:29:11,679
promising

782
00:29:11,679 --> 00:29:12,480
when

783
00:29:12,480 --> 00:29:14,559
uh at scale right when you've got

784
00:29:14,559 --> 00:29:18,320
massive data sets and you're trying to

785
00:29:18,320 --> 00:29:20,080
get a lot of information to make a lot

786
00:29:20,080 --> 00:29:22,559
of decisions consistently

787
00:29:22,559 --> 00:29:24,640
right that's i i think the the great

788
00:29:24,640 --> 00:29:27,200
promise of ai really is in crunching

789
00:29:27,200 --> 00:29:28,480
through that sort of thing and making

790
00:29:28,480 --> 00:29:31,520
those sorts of decisions at scale um

791
00:29:31,520 --> 00:29:35,600
decisions made at scale are precisely

792
00:29:35,600 --> 00:29:38,559
the sort of thing in human institutions

793
00:29:38,559 --> 00:29:41,360
that are hardest to explain that are

794
00:29:41,360 --> 00:29:44,000
hardest to make transparent and that are

795
00:29:44,000 --> 00:29:47,360
most likely to seem alienating to people

796
00:29:47,360 --> 00:29:50,640
even if you explain it at length right i

797
00:29:50,640 --> 00:29:51,840
mean so

798
00:29:51,840 --> 00:29:54,399
you know even human reasoning at scale

799
00:29:54,399 --> 00:29:56,240
can be very alienated thinking think

800
00:29:56,240 --> 00:29:58,240
about a supreme court decision these are

801
00:29:58,240 --> 00:30:00,080
the best reasoners in the world they'll

802
00:30:00,080 --> 00:30:02,399
spend a couple hundred pages explaining

803
00:30:02,399 --> 00:30:03,760
a decision

804
00:30:03,760 --> 00:30:04,720
um

805
00:30:04,720 --> 00:30:06,399
and it doesn't stop the disagreement

806
00:30:06,399 --> 00:30:07,760
right it doesn't stop the sense of

807
00:30:07,760 --> 00:30:09,520
alienation and so on but you can

808
00:30:09,520 --> 00:30:11,600
mitigate in that some way and i think

809
00:30:11,600 --> 00:30:13,600
the institutions that are going to be

810
00:30:13,600 --> 00:30:15,679
using ai applications to make these

811
00:30:15,679 --> 00:30:17,679
decisions at scale a lot of their

812
00:30:17,679 --> 00:30:20,080
legitimacy rests

813
00:30:20,080 --> 00:30:22,960
on at least having a commitment to

814
00:30:22,960 --> 00:30:26,159
making things fairer right uh or more

815
00:30:26,159 --> 00:30:28,640
transparent even though

816
00:30:28,640 --> 00:30:31,120
you have this scale that is inevitably

817
00:30:31,120 --> 00:30:34,080
going to lead to a kind of

818
00:30:34,080 --> 00:30:38,639
sorts of doubts right and and alienation

819
00:30:38,720 --> 00:30:41,279
i guess i can i can quickly quickly uh

820
00:30:41,279 --> 00:30:42,640
add thinking about what you folks are

821
00:30:42,640 --> 00:30:45,520
sharing is how often i see a gap between

822
00:30:45,520 --> 00:30:47,679
transparency and accountability um so

823
00:30:47,679 --> 00:30:50,399
it's one thing for an institution or an

824
00:30:50,399 --> 00:30:52,880
ai designer to explain every decision

825
00:30:52,880 --> 00:30:54,640
they made where the province of that

826
00:30:54,640 --> 00:30:57,279
data set was from and walk you know a

827
00:30:57,279 --> 00:30:59,279
person through exactly how that system

828
00:30:59,279 --> 00:31:00,880
was constructed and how decisions were

829
00:31:00,880 --> 00:31:02,880
made but transparency in and of itself

830
00:31:02,880 --> 00:31:04,320
doesn't guarantee that that person is

831
00:31:04,320 --> 00:31:06,640
then empowered to say hey i don't feel

832
00:31:06,640 --> 00:31:08,159
like that decision about me was actually

833
00:31:08,159 --> 00:31:09,600
fair i don't feel like that decision

834
00:31:09,600 --> 00:31:11,440
about me was right or even if it is

835
00:31:11,440 --> 00:31:12,799
right that i don't think that that

836
00:31:12,799 --> 00:31:14,159
system is actually how we should go

837
00:31:14,159 --> 00:31:16,559
about making judgments as a society um i

838
00:31:16,559 --> 00:31:18,399
think the simplest example of this that

839
00:31:18,399 --> 00:31:20,080
comes to mind is privacy policies right

840
00:31:20,080 --> 00:31:21,600
like you can imagine a company that has

841
00:31:21,600 --> 00:31:23,679
a really transparent really good privacy

842
00:31:23,679 --> 00:31:25,600
policy but if you need access to that

843
00:31:25,600 --> 00:31:27,039
platform if you need access to that

844
00:31:27,039 --> 00:31:29,279
service in order to apply for a job i

845
00:31:29,279 --> 00:31:30,880
mean do you really have a meaningful

846
00:31:30,880 --> 00:31:32,559
option to opt out of that system so i

847
00:31:32,559 --> 00:31:34,480
think it's always useful to link

848
00:31:34,480 --> 00:31:36,000
transparency and accountability and if

849
00:31:36,000 --> 00:31:37,600
you have one without the other it can

850
00:31:37,600 --> 00:31:39,519
actually shore up power for institutions

851
00:31:39,519 --> 00:31:41,360
as opposed to being a meaningful check

852
00:31:41,360 --> 00:31:43,918
and balance

853
00:31:44,840 --> 00:31:47,679
interesting um you know lindsay you

854
00:31:47,679 --> 00:31:50,799
brought up this question or this state

855
00:31:50,799 --> 00:31:53,279
i think term ai justice

856
00:31:53,279 --> 00:31:55,679
uh as opposed to fairness

857
00:31:55,679 --> 00:31:56,960
um

858
00:31:56,960 --> 00:31:58,480
would you care to elaborate is there a

859
00:31:58,480 --> 00:32:00,880
difference between ai justice and ai

860
00:32:00,880 --> 00:32:01,919
fairness

861
00:32:01,919 --> 00:32:03,279
yeah i'll try and be quick because i'm

862
00:32:03,279 --> 00:32:05,279
so curious uh to hear what the other

863
00:32:05,279 --> 00:32:07,919
folks to my right think um i i do think

864
00:32:07,919 --> 00:32:09,679
there's a distinction um and i think

865
00:32:09,679 --> 00:32:11,679
there's a distinction for a few reasons

866
00:32:11,679 --> 00:32:13,600
so i think that the terms fairness and

867
00:32:13,600 --> 00:32:16,480
justice have um different genealogies in

868
00:32:16,480 --> 00:32:18,960
terms of the how i see them come up um

869
00:32:18,960 --> 00:32:21,440
ai fairness i think when you historicize

870
00:32:21,440 --> 00:32:23,440
this this aspiration for it within

871
00:32:23,440 --> 00:32:26,159
institutions it comes from this earlier

872
00:32:26,159 --> 00:32:28,559
earlier period of actuarial risk

873
00:32:28,559 --> 00:32:30,480
and insurance institutions historically

874
00:32:30,480 --> 00:32:32,000
trying to make decisions about who

875
00:32:32,000 --> 00:32:34,000
should be entitled onto certain access

876
00:32:34,000 --> 00:32:36,320
to goods and resources and opportunities

877
00:32:36,320 --> 00:32:37,840
and then when we look at ai fairness

878
00:32:37,840 --> 00:32:39,760
discourse today it oftentimes takes

879
00:32:39,760 --> 00:32:40,840
these very

880
00:32:40,840 --> 00:32:43,440
mathematical parity-based metrics coming

881
00:32:43,440 --> 00:32:45,760
out of civil rights era legislation and

882
00:32:45,760 --> 00:32:46,799
there's been a tremendous amount of

883
00:32:46,799 --> 00:32:48,880
scholarship that has been critical of

884
00:32:48,880 --> 00:32:51,039
ideas of fairness within civil rights

885
00:32:51,039 --> 00:32:52,559
discourse and yet none of those

886
00:32:52,559 --> 00:32:54,159
criticisms have really been incorporated

887
00:32:54,159 --> 00:32:56,240
and attended to on to the ways ai

888
00:32:56,240 --> 00:32:58,000
fairness is being treated today right so

889
00:32:58,000 --> 00:32:59,600
i think that there's a there's a way in

890
00:32:59,600 --> 00:33:01,760
which um ai fairness discussions are

891
00:33:01,760 --> 00:33:04,640
relying on this very uh computational

892
00:33:04,640 --> 00:33:06,480
quantitative based approach that's

893
00:33:06,480 --> 00:33:08,960
grounded in statistical decision making

894
00:33:08,960 --> 00:33:10,640
that's been criticized outside the field

895
00:33:10,640 --> 00:33:11,919
and it hasn't really attended to those

896
00:33:11,919 --> 00:33:15,440
criticisms i think ai justice has not um

897
00:33:15,440 --> 00:33:17,440
has not received that type of treatment

898
00:33:17,440 --> 00:33:18,799
i don't think it's been co-opted

899
00:33:18,799 --> 00:33:20,399
institutionally in the same kinds of

900
00:33:20,399 --> 00:33:23,120
ways and i think justice has a different

901
00:33:23,120 --> 00:33:25,279
um genealogical lineage as well when we

902
00:33:25,279 --> 00:33:27,120
talk about justice we have to talk about

903
00:33:27,120 --> 00:33:29,039
racial justice or social justice or

904
00:33:29,039 --> 00:33:30,799
environmental justice it connects to a

905
00:33:30,799 --> 00:33:33,039
different set of intellectual traditions

906
00:33:33,039 --> 00:33:34,559
that i think provide more critical

907
00:33:34,559 --> 00:33:35,600
frameworks

908
00:33:35,600 --> 00:33:37,120
and i think justice makes us ask

909
00:33:37,120 --> 00:33:39,279
questions about power it makes us ask

910
00:33:39,279 --> 00:33:41,519
questions about who's included it makes

911
00:33:41,519 --> 00:33:43,840
us ask questions about uh resources and

912
00:33:43,840 --> 00:33:45,760
advantages and disadvantages in a way

913
00:33:45,760 --> 00:33:47,360
that i don't think ai fairness

914
00:33:47,360 --> 00:33:52,120
necessarily provokes us to do

915
00:33:52,640 --> 00:33:54,080
do you want to you look like you're

916
00:33:54,080 --> 00:33:58,158
ready to keep on sure no no no

917
00:33:58,559 --> 00:34:01,279
uh i got lots to say too i think in one

918
00:34:01,279 --> 00:34:02,960
sense i agree with lindsey in one sense

919
00:34:02,960 --> 00:34:05,120
i disagree so i also

920
00:34:05,120 --> 00:34:07,760
want to divide the concepts of fairness

921
00:34:07,760 --> 00:34:09,760
and justice um

922
00:34:09,760 --> 00:34:12,480
i do that and in fact at first it really

923
00:34:12,480 --> 00:34:14,159
puzzled me

924
00:34:14,159 --> 00:34:17,040
why we used the term fair why that came

925
00:34:17,040 --> 00:34:20,000
to be the term that we use to talk about

926
00:34:20,000 --> 00:34:22,639
a.i it's not really a technical term in

927
00:34:22,639 --> 00:34:24,000
philosophy and philosophy we read lots

928
00:34:24,000 --> 00:34:25,839
of books about justice about equality

929
00:34:25,839 --> 00:34:27,599
about freedom we don't really write a

930
00:34:27,599 --> 00:34:29,440
lot of books about fairness actually

931
00:34:29,440 --> 00:34:32,239
it's uh it tends to be treated as a kind

932
00:34:32,239 --> 00:34:35,520
of low budget uh

933
00:34:35,520 --> 00:34:38,719
parochial notion right

934
00:34:38,719 --> 00:34:40,719
yeah i've come to think of that as

935
00:34:40,719 --> 00:34:42,560
actually a

936
00:34:42,560 --> 00:34:44,320
feature rather than a bug when we're

937
00:34:44,320 --> 00:34:46,800
talking about ai because i think a lot

938
00:34:46,800 --> 00:34:50,560
of ai decisions can be evaluated along

939
00:34:50,560 --> 00:34:54,719
one one axis in precisely that way right

940
00:34:54,719 --> 00:34:58,079
um but that does precisely leave aside

941
00:34:58,079 --> 00:35:00,480
questions of justice which i do think of

942
00:35:00,480 --> 00:35:03,520
as as somewhat different um

943
00:35:03,520 --> 00:35:06,720
where i would disagree is i guess i want

944
00:35:06,720 --> 00:35:09,599
to think about justice as bigger

945
00:35:09,599 --> 00:35:12,240
questions about larger institutions in

946
00:35:12,240 --> 00:35:15,200
which ai decisions are embedded

947
00:35:15,200 --> 00:35:16,960
right so in a way

948
00:35:16,960 --> 00:35:19,200
um i think ai fairness is a good

949
00:35:19,200 --> 00:35:21,440
question i don't think ai justice is a

950
00:35:21,440 --> 00:35:23,839
good question but i think the justice of

951
00:35:23,839 --> 00:35:26,160
the penal system is a great question or

952
00:35:26,160 --> 00:35:27,839
the justice of capitalism is a good

953
00:35:27,839 --> 00:35:29,359
question or something like that but i

954
00:35:29,359 --> 00:35:31,359
would want to use justice and those that

955
00:35:31,359 --> 00:35:33,520
set of normative questions for the kind

956
00:35:33,520 --> 00:35:36,720
of bigger framework issues in which ai

957
00:35:36,720 --> 00:35:38,720
decisions are necessarily going to be

958
00:35:38,720 --> 00:35:40,800
embedded right now that doesn't mean

959
00:35:40,800 --> 00:35:41,680
that there isn't going to be an

960
00:35:41,680 --> 00:35:43,440
interesting interaction between the two

961
00:35:43,440 --> 00:35:45,280
i think

962
00:35:45,280 --> 00:35:46,880
oh

963
00:35:46,880 --> 00:35:49,359
did you want to speak up yeah i just one

964
00:35:49,359 --> 00:35:51,119
one comment it's a very nice discussion

965
00:35:51,119 --> 00:35:52,960
audio justice and uh

966
00:35:52,960 --> 00:35:54,480
lindsay and

967
00:35:54,480 --> 00:35:55,520
chris

968
00:35:55,520 --> 00:35:57,760
right so if you look at autonomous

969
00:35:57,760 --> 00:35:58,960
driving

970
00:35:58,960 --> 00:36:01,440
you know uh that is your air justice

971
00:36:01,440 --> 00:36:04,079
actually makes uh makes a lot of sense

972
00:36:04,079 --> 00:36:06,160
uh that if there is a there is a

973
00:36:06,160 --> 00:36:08,079
liability there is a crash

974
00:36:08,079 --> 00:36:11,760
uh who who is liable for that is the is

975
00:36:11,760 --> 00:36:14,800
our you know self-driving system on it

976
00:36:14,800 --> 00:36:16,880
or is the passenger

977
00:36:16,880 --> 00:36:19,359
or the or the driver on the wheel but is

978
00:36:19,359 --> 00:36:20,800
not driving that

979
00:36:20,800 --> 00:36:22,480
or the other person's right so i think

980
00:36:22,480 --> 00:36:23,680
there's a lot of

981
00:36:23,680 --> 00:36:26,800
questions and and can you can you really

982
00:36:26,800 --> 00:36:27,760
uh

983
00:36:27,760 --> 00:36:29,839
trust you know the trustworthiness come

984
00:36:29,839 --> 00:36:32,240
can you trust the explanability of the

985
00:36:32,240 --> 00:36:33,920
of the ai system say that hey this is

986
00:36:33,920 --> 00:36:36,640
why i crashed it you know but what tesla

987
00:36:36,640 --> 00:36:38,480
is telling and this is why my car

988
00:36:38,480 --> 00:36:41,480
crashed

989
00:36:43,119 --> 00:36:45,200
so i think ai justice is a good good

990
00:36:45,200 --> 00:36:47,040
thing good concept to

991
00:36:47,040 --> 00:36:48,640
follow up on

992
00:36:48,640 --> 00:36:51,119
yeah yeah

993
00:36:51,119 --> 00:36:54,160
so here's another um how is you know

994
00:36:54,160 --> 00:36:58,400
fairness related to ai ethics and let me

995
00:36:58,400 --> 00:37:00,480
get chris christopher up for our

996
00:37:00,480 --> 00:37:03,599
philosopher up for that yeah um

997
00:37:03,599 --> 00:37:06,960
uh so philosophers use ethics very

998
00:37:06,960 --> 00:37:09,760
broadly right generally we tend to use

999
00:37:09,760 --> 00:37:14,480
ethics as the giant grab bag in which

1000
00:37:14,480 --> 00:37:17,520
almost all sorts of normative questions

1001
00:37:17,520 --> 00:37:18,720
fall

1002
00:37:18,720 --> 00:37:20,400
maybe not aesthetic questions if you

1003
00:37:20,400 --> 00:37:22,160
think aesthetic questions or normative

1004
00:37:22,160 --> 00:37:25,119
questions but political questions of uh

1005
00:37:25,119 --> 00:37:26,960
political justice questions of

1006
00:37:26,960 --> 00:37:30,480
individual virtue uh questions of moral

1007
00:37:30,480 --> 00:37:32,000
responsibility

1008
00:37:32,000 --> 00:37:33,040
um

1009
00:37:33,040 --> 00:37:35,200
uh and so on uh

1010
00:37:35,200 --> 00:37:38,480
so i guess i think of

1011
00:37:38,480 --> 00:37:40,640
fairness as

1012
00:37:40,640 --> 00:37:44,320
one of the ethical dimensions

1013
00:37:44,320 --> 00:37:46,320
along which

1014
00:37:46,320 --> 00:37:48,280
we can

1015
00:37:48,280 --> 00:37:51,599
evaluate really anything at all but i

1016
00:37:51,599 --> 00:37:53,280
think particularly when we're talking

1017
00:37:53,280 --> 00:37:54,800
about fairness and maybe this would be

1018
00:37:54,800 --> 00:37:57,359
one additional thing that i'll say is um

1019
00:37:57,359 --> 00:37:58,880
i think generally speaking when we're

1020
00:37:58,880 --> 00:38:00,800
talking about fairness

1021
00:38:00,800 --> 00:38:02,960
we're not talking about an individual

1022
00:38:02,960 --> 00:38:04,640
decision at an individual time we're

1023
00:38:04,640 --> 00:38:06,640
talking about repeated decisions over

1024
00:38:06,640 --> 00:38:07,520
time

1025
00:38:07,520 --> 00:38:11,839
right when um for example in

1026
00:38:11,839 --> 00:38:13,920
families or in academic departments

1027
00:38:13,920 --> 00:38:15,440
people start to feel like things are

1028
00:38:15,440 --> 00:38:16,560
unfair

1029
00:38:16,560 --> 00:38:18,160
it's not generally because something

1030
00:38:18,160 --> 00:38:20,560
went against them one time it's they

1031
00:38:20,560 --> 00:38:22,640
feel like it always goes against them

1032
00:38:22,640 --> 00:38:24,640
right if people feel like well i lost

1033
00:38:24,640 --> 00:38:26,800
this one but i won the next one people

1034
00:38:26,800 --> 00:38:28,480
don't start to feel like the decisions

1035
00:38:28,480 --> 00:38:30,800
being made or unfair or people like feel

1036
00:38:30,800 --> 00:38:32,079
like they're being heard and then the

1037
00:38:32,079 --> 00:38:35,200
next time right um they they get some

1038
00:38:35,200 --> 00:38:37,040
influence or something like that so i

1039
00:38:37,040 --> 00:38:39,119
think fairness really points to these

1040
00:38:39,119 --> 00:38:42,240
iterated decision-making processes and

1041
00:38:42,240 --> 00:38:44,720
so within ethics if we're asking about

1042
00:38:44,720 --> 00:38:47,359
fairness i think that's really focusing

1043
00:38:47,359 --> 00:38:49,280
our concern

1044
00:38:49,280 --> 00:38:52,720
on the historical pattern of these

1045
00:38:52,720 --> 00:38:54,800
decision making

1046
00:38:54,800 --> 00:38:56,480
or the decisions that are being made

1047
00:38:56,480 --> 00:38:58,560
which is why for example in compass

1048
00:38:58,560 --> 00:38:59,359
right

1049
00:38:59,359 --> 00:39:02,160
if it had been a situation where um

1050
00:39:02,160 --> 00:39:04,880
african americans felt like uh they were

1051
00:39:04,880 --> 00:39:07,680
getting a great fair shake from the uh

1052
00:39:07,680 --> 00:39:10,400
uh criminal justice system before the ai

1053
00:39:10,400 --> 00:39:11,599
and then all of a sudden there was this

1054
00:39:11,599 --> 00:39:13,359
bug with the ai that would have been a

1055
00:39:13,359 --> 00:39:15,200
very different situation than feeling

1056
00:39:15,200 --> 00:39:17,280
like oh this is just a continuation of

1057
00:39:17,280 --> 00:39:18,640
the same problem

1058
00:39:18,640 --> 00:39:21,920
right so i think fairness unlike so i

1059
00:39:21,920 --> 00:39:24,480
think fairness

1060
00:39:24,480 --> 00:39:26,960
focuses our attention on these

1061
00:39:26,960 --> 00:39:29,599
historical decisions i think justice

1062
00:39:29,599 --> 00:39:32,000
focuses our attention on

1063
00:39:32,000 --> 00:39:34,960
structures and structural relationships

1064
00:39:34,960 --> 00:39:38,160
i think equality focuses our attention

1065
00:39:38,160 --> 00:39:40,480
on what you might call second personal

1066
00:39:40,480 --> 00:39:42,880
relationships right where i stand with

1067
00:39:42,880 --> 00:39:45,280
relation to you and so on so i think all

1068
00:39:45,280 --> 00:39:46,640
of these different

1069
00:39:46,640 --> 00:39:48,720
normative terms within ethics are like

1070
00:39:48,720 --> 00:39:51,440
different perspectives that you can pick

1071
00:39:51,440 --> 00:39:54,079
up to evaluate the phenomenon that

1072
00:39:54,079 --> 00:39:56,560
you're trying to um that you're

1073
00:39:56,560 --> 00:39:58,640
interested in and so that's why i think

1074
00:39:58,640 --> 00:40:00,320
there's a real value in distinguishing

1075
00:40:00,320 --> 00:40:01,920
fairness and justice because they get at

1076
00:40:01,920 --> 00:40:03,280
these different things and we should be

1077
00:40:03,280 --> 00:40:05,920
thinking about both

1078
00:40:06,240 --> 00:40:07,760
anybody else want to

1079
00:40:07,760 --> 00:40:09,119
chime in on this

1080
00:40:09,119 --> 00:40:10,800
you know i'll add a little wrinkle not

1081
00:40:10,800 --> 00:40:12,240
to that specific

1082
00:40:12,240 --> 00:40:15,920
discussion but when it came to um

1083
00:40:15,920 --> 00:40:18,240
you know fairness and justice with

1084
00:40:18,240 --> 00:40:21,839
respect to ai there there are generally

1085
00:40:21,839 --> 00:40:23,200
two types of

1086
00:40:23,200 --> 00:40:24,800
ai systems

1087
00:40:24,800 --> 00:40:26,960
some of them are autonomous and some

1088
00:40:26,960 --> 00:40:28,800
have human in the loop

1089
00:40:28,800 --> 00:40:30,560
and i think

1090
00:40:30,560 --> 00:40:33,040
the thing that scares most people are

1091
00:40:33,040 --> 00:40:35,520
the totally autonomous systems

1092
00:40:35,520 --> 00:40:36,880
where there's

1093
00:40:36,880 --> 00:40:38,400
no recourse

1094
00:40:38,400 --> 00:40:39,359
um

1095
00:40:39,359 --> 00:40:40,880
it's just going to do what it's going to

1096
00:40:40,880 --> 00:40:41,920
do

1097
00:40:41,920 --> 00:40:42,800
and

1098
00:40:42,800 --> 00:40:45,599
at least with human in the loop the ai

1099
00:40:45,599 --> 00:40:46,640
system

1100
00:40:46,640 --> 00:40:47,520
is

1101
00:40:47,520 --> 00:40:50,000
is advising a human who will make the

1102
00:40:50,000 --> 00:40:51,599
final decision

1103
00:40:51,599 --> 00:40:53,599
now i'm not sure where that goes with

1104
00:40:53,599 --> 00:40:56,480
respect to fairness and justice but um i

1105
00:40:56,480 --> 00:40:58,640
think there is a distinction there

1106
00:40:58,640 --> 00:41:02,319
that is pretty important because

1107
00:41:02,319 --> 00:41:05,040
if i have a biased ai system and there's

1108
00:41:05,040 --> 00:41:06,960
a human in the loop hopefully that

1109
00:41:06,960 --> 00:41:09,359
person might be able to recognize

1110
00:41:09,359 --> 00:41:11,760
the cases where it's bad but if it's too

1111
00:41:11,760 --> 00:41:13,839
completely autonomous

1112
00:41:13,839 --> 00:41:16,240
whether it's a weapon or it's just you

1113
00:41:16,240 --> 00:41:18,240
know institutional decisions

1114
00:41:18,240 --> 00:41:21,040
that's when things get a little worse i

1115
00:41:21,040 --> 00:41:24,160
think a little more scary

1116
00:41:25,760 --> 00:41:28,400
we almost want to

1117
00:41:28,400 --> 00:41:31,200
let me um since i see time's getting

1118
00:41:31,200 --> 00:41:32,480
shorter we've got a lot we still want to

1119
00:41:32,480 --> 00:41:33,599
discuss

1120
00:41:33,599 --> 00:41:36,000
oh we have a question here yeah so i

1121
00:41:36,000 --> 00:41:37,680
have a question about

1122
00:41:37,680 --> 00:41:39,760
how much of this is to do with the ai

1123
00:41:39,760 --> 00:41:42,560
versus our expectations of it

1124
00:41:42,560 --> 00:41:44,240
and i want to point to an example of a

1125
00:41:44,240 --> 00:41:47,440
paper that i wrote in 1999 remember that

1126
00:41:47,440 --> 00:41:49,440
uh it was called what's related

1127
00:41:49,440 --> 00:41:52,319
everything but your privacy and it was a

1128
00:41:52,319 --> 00:41:54,960
dissection of a button that showed up in

1129
00:41:54,960 --> 00:41:58,480
the netscape communicator version 4

1130
00:41:58,480 --> 00:42:01,359
browser no explanation but there was a

1131
00:42:01,359 --> 00:42:03,680
thing up there next to your url that

1132
00:42:03,680 --> 00:42:05,680
would have a butt said what's related

1133
00:42:05,680 --> 00:42:07,760
and if you clicked on it

1134
00:42:07,760 --> 00:42:10,319
then it would show you a

1135
00:42:10,319 --> 00:42:12,079
list of pages that

1136
00:42:12,079 --> 00:42:15,680
alexa decided were related

1137
00:42:15,680 --> 00:42:18,240
when you would click on

1138
00:42:18,240 --> 00:42:20,720
that button when looking at my paper

1139
00:42:20,720 --> 00:42:23,599
describing the what's related button and

1140
00:42:23,599 --> 00:42:25,440
the privacy implications and the

1141
00:42:25,440 --> 00:42:28,240
consequences of that technology it said

1142
00:42:28,240 --> 00:42:29,760
the number one related thing was the

1143
00:42:29,760 --> 00:42:33,200
unabomber manifesto

1144
00:42:34,000 --> 00:42:36,240
now of course i was outraged but at the

1145
00:42:36,240 --> 00:42:37,599
same time it's a no-brainer the

1146
00:42:37,599 --> 00:42:40,160
unabomber was warning about technology

1147
00:42:40,160 --> 00:42:42,240
run away so you could see how it would

1148
00:42:42,240 --> 00:42:43,920
be related but of course then somebody

1149
00:42:43,920 --> 00:42:46,960
looks at this and now here i show up i'm

1150
00:42:46,960 --> 00:42:49,040
i'm putting my hand up in court i'm i'm

1151
00:42:49,040 --> 00:42:50,960
being you know sworn in and all this and

1152
00:42:50,960 --> 00:42:52,480
now the other side is trying to destroy

1153
00:42:52,480 --> 00:42:54,240
me by saying that well aren't you

1154
00:42:54,240 --> 00:42:56,960
related to the unibomb so is this an

1155
00:42:56,960 --> 00:42:58,720
expectations problem and is it any

1156
00:42:58,720 --> 00:43:01,000
better today than it was

1157
00:43:01,000 --> 00:43:04,480
23 years ago

1158
00:43:05,920 --> 00:43:08,800
i want to read the papers

1159
00:43:08,800 --> 00:43:11,800
i

1160
00:43:12,800 --> 00:43:14,400
i'll maybe say one thing about this this

1161
00:43:14,400 --> 00:43:15,680
is a uh

1162
00:43:15,680 --> 00:43:17,520
may i uh

1163
00:43:17,520 --> 00:43:20,160
give me a minute um so uh historical

1164
00:43:20,160 --> 00:43:23,200
analogy right uh 18th century prussia

1165
00:43:23,200 --> 00:43:26,640
frederick the great pushes out the uh uh

1166
00:43:26,640 --> 00:43:28,319
the poison alga minus number the

1167
00:43:28,319 --> 00:43:30,400
depression general code right why does

1168
00:43:30,400 --> 00:43:32,960
he do this because he hates lawyers

1169
00:43:32,960 --> 00:43:35,359
and he thinks if you can just write the

1170
00:43:35,359 --> 00:43:38,240
law out clearly everybody will see what

1171
00:43:38,240 --> 00:43:41,599
it is and we won't need lawyers anymore

1172
00:43:41,599 --> 00:43:42,960
right they'll just they'll go into

1173
00:43:42,960 --> 00:43:45,200
something more productive right that's

1174
00:43:45,200 --> 00:43:46,400
sometimes i think the kind of

1175
00:43:46,400 --> 00:43:48,560
expectation we have about ai if we just

1176
00:43:48,560 --> 00:43:51,520
get the coding right the disagreements

1177
00:43:51,520 --> 00:43:53,119
would go away

1178
00:43:53,119 --> 00:43:55,040
right uh and then we can all be more

1179
00:43:55,040 --> 00:43:57,599
productive um but i don't think there's

1180
00:43:57,599 --> 00:44:00,000
any better chance that ai will make the

1181
00:44:00,000 --> 00:44:02,720
disagreements go away then a properly

1182
00:44:02,720 --> 00:44:04,480
written legal code will make lawyers go

1183
00:44:04,480 --> 00:44:05,920
away

1184
00:44:05,920 --> 00:44:07,520
i might want to just add one thing to

1185
00:44:07,520 --> 00:44:09,119
that when we were talking about ethics

1186
00:44:09,119 --> 00:44:10,000
and

1187
00:44:10,000 --> 00:44:13,520
fairness it's also culturally a little

1188
00:44:13,520 --> 00:44:15,359
different and i'll give you a quick

1189
00:44:15,359 --> 00:44:16,560
example

1190
00:44:16,560 --> 00:44:17,359
um

1191
00:44:17,359 --> 00:44:19,200
when i was working for tel labs i was in

1192
00:44:19,200 --> 00:44:21,280
charge of human resources and we had

1193
00:44:21,280 --> 00:44:23,200
this grand idea that we would have a tel

1194
00:44:23,200 --> 00:44:25,200
labs code of ethics

1195
00:44:25,200 --> 00:44:28,000
so i went to finland where we had eleven

1196
00:44:28,000 --> 00:44:29,599
hundred people working and i said we're

1197
00:44:29,599 --> 00:44:32,319
gonna give you this code of ethics and

1198
00:44:32,319 --> 00:44:34,000
you would think they'd all be okay with

1199
00:44:34,000 --> 00:44:36,319
it they said this was outrageous we

1200
00:44:36,319 --> 00:44:38,000
behave like this anyway why do you have

1201
00:44:38,000 --> 00:44:40,079
to write it down and tell us

1202
00:44:40,079 --> 00:44:42,400
and the other thing was that was you

1203
00:44:42,400 --> 00:44:44,560
know maybe a little more stark we

1204
00:44:44,560 --> 00:44:46,240
decided that every employee would get

1205
00:44:46,240 --> 00:44:49,200
stock options so we went to finland and

1206
00:44:49,200 --> 00:44:50,560
we said you're all going to get stock

1207
00:44:50,560 --> 00:44:53,440
options even the janitor and anybody is

1208
00:44:53,440 --> 00:44:55,839
going to get a stock option they said

1209
00:44:55,839 --> 00:44:57,359
you're already paying us we don't want

1210
00:44:57,359 --> 00:44:58,720
stock options

1211
00:44:58,720 --> 00:45:01,520
so it was pretty interesting just to see

1212
00:45:01,520 --> 00:45:03,599
their sense of fairness

1213
00:45:03,599 --> 00:45:05,599
was different than ours but it made it

1214
00:45:05,599 --> 00:45:07,680
made no sense at first to us until we

1215
00:45:07,680 --> 00:45:09,599
kind of got to know how their culture

1216
00:45:09,599 --> 00:45:12,079
operated

1217
00:45:13,599 --> 00:45:15,040
well i think uh

1218
00:45:15,040 --> 00:45:17,599
kind of on on fairness let me bring up

1219
00:45:17,599 --> 00:45:20,240
oh did you no i just saw that oh we have

1220
00:45:20,240 --> 00:45:22,240
yeah okay

1221
00:45:22,240 --> 00:45:25,200
just add one no no

1222
00:45:25,599 --> 00:45:28,000
i'd like to anybody can comment but i'd

1223
00:45:28,000 --> 00:45:30,240
like to direct the question to tom you

1224
00:45:30,240 --> 00:45:32,640
know your corporation the company that

1225
00:45:32,640 --> 00:45:34,880
that you're heading right now its core

1226
00:45:34,880 --> 00:45:37,359
technology is very ai based and used for

1227
00:45:37,359 --> 00:45:38,319
business

1228
00:45:38,319 --> 00:45:42,079
can you talk from the commercial side of

1229
00:45:42,079 --> 00:45:43,839
some of the challenges that you deal

1230
00:45:43,839 --> 00:45:45,680
with in making sure that

1231
00:45:45,680 --> 00:45:47,839
the services that you're providing to

1232
00:45:47,839 --> 00:45:50,000
other to your customers you know that

1233
00:45:50,000 --> 00:45:52,640
you're i'm making assumptions that

1234
00:45:52,640 --> 00:45:54,400
you're showing them how it can help them

1235
00:45:54,400 --> 00:45:56,079
with business

1236
00:45:56,079 --> 00:45:57,760
but obviously underlying is that

1237
00:45:57,760 --> 00:45:59,119
sometimes the customer is concerned

1238
00:45:59,119 --> 00:46:00,560
sometimes maybe they're not concerned

1239
00:46:00,560 --> 00:46:02,240
but your company obviously has to deal

1240
00:46:02,240 --> 00:46:05,040
with these fairness and trusted ai

1241
00:46:05,040 --> 00:46:06,960
implications of your product how do you

1242
00:46:06,960 --> 00:46:09,520
deal with this in the commercials

1243
00:46:09,520 --> 00:46:11,119
without getting into too much detail

1244
00:46:11,119 --> 00:46:13,280
there there's a whole process that

1245
00:46:13,280 --> 00:46:15,599
covers almost everything

1246
00:46:15,599 --> 00:46:17,680
that this conference covers it starts

1247
00:46:17,680 --> 00:46:21,839
with data privacy we have to ensure them

1248
00:46:21,839 --> 00:46:24,880
that we are storing their data

1249
00:46:24,880 --> 00:46:27,760
in a secure place

1250
00:46:27,760 --> 00:46:30,480
with hat which has encryption etc etc

1251
00:46:30,480 --> 00:46:32,400
that the data is encrypted in flight et

1252
00:46:32,400 --> 00:46:34,480
cetera et cetera and that only the folks

1253
00:46:34,480 --> 00:46:37,440
in our company that need to use that

1254
00:46:37,440 --> 00:46:39,440
data can see it

1255
00:46:39,440 --> 00:46:40,319
now

1256
00:46:40,319 --> 00:46:43,280
as an aside the data makes absolutely no

1257
00:46:43,280 --> 00:46:46,400
sense to anybody but they're concerned

1258
00:46:46,400 --> 00:46:48,560
okay so we do that

1259
00:46:48,560 --> 00:46:49,920
um

1260
00:46:49,920 --> 00:46:54,160
then we we we have a a pretty intense

1261
00:46:54,160 --> 00:46:56,400
discourse with them about

1262
00:46:56,400 --> 00:46:58,400
what the data means

1263
00:46:58,400 --> 00:47:00,319
what data might be corrupt et cetera et

1264
00:47:00,319 --> 00:47:02,400
cetera and then we go

1265
00:47:02,400 --> 00:47:05,359
to the modeling

1266
00:47:05,359 --> 00:47:09,200
portion where we have a pretty

1267
00:47:09,200 --> 00:47:11,440
rigorous i mean it takes five or six

1268
00:47:11,440 --> 00:47:13,119
months of

1269
00:47:13,119 --> 00:47:14,800
developing the models

1270
00:47:14,800 --> 00:47:17,520
testing them under every circumstance

1271
00:47:17,520 --> 00:47:20,960
finding out where the model doesn't work

1272
00:47:20,960 --> 00:47:23,839
and then doing um various types of

1273
00:47:23,839 --> 00:47:27,119
analyses that allow us to understand

1274
00:47:27,119 --> 00:47:28,720
what features

1275
00:47:28,720 --> 00:47:31,520
are driving the model and even to get

1276
00:47:31,520 --> 00:47:33,760
down to the level of

1277
00:47:33,760 --> 00:47:36,240
what features are driving the model in

1278
00:47:36,240 --> 00:47:38,640
certain parts of the reactor

1279
00:47:38,640 --> 00:47:40,240
because that allows them to take

1280
00:47:40,240 --> 00:47:43,280
remedial action if they have to so

1281
00:47:43,280 --> 00:47:46,000
it's a it's a process of

1282
00:47:46,000 --> 00:47:48,800
continued analysis

1283
00:47:48,800 --> 00:47:50,319
understanding

1284
00:47:50,319 --> 00:47:52,720
and then once we deploy the model to

1285
00:47:52,720 --> 00:47:54,160
them what they do

1286
00:47:54,160 --> 00:47:56,079
is there's a human in the loop their

1287
00:47:56,079 --> 00:47:59,520
engineers use our software to do

1288
00:47:59,520 --> 00:48:02,480
planning where they say okay how many

1289
00:48:02,480 --> 00:48:04,480
bundles of fuel do we need to buy what

1290
00:48:04,480 --> 00:48:08,079
kind will it be etc and then the system

1291
00:48:08,079 --> 00:48:10,079
tells them what that's going to do

1292
00:48:10,079 --> 00:48:11,760
within the reactor

1293
00:48:11,760 --> 00:48:14,800
and then once that happens we'll we'll

1294
00:48:14,800 --> 00:48:17,359
take any new data they give us and test

1295
00:48:17,359 --> 00:48:19,839
it against what the model predicted so

1296
00:48:19,839 --> 00:48:22,240
it's a it's a process of a lot of

1297
00:48:22,240 --> 00:48:23,440
testing

1298
00:48:23,440 --> 00:48:26,640
and a lot of deep analysis past the

1299
00:48:26,640 --> 00:48:28,960
point where we just give them a black

1300
00:48:28,960 --> 00:48:30,400
box model

1301
00:48:30,400 --> 00:48:32,800
does that answer your question i think

1302
00:48:32,800 --> 00:48:35,800
okay

1303
00:48:40,240 --> 00:48:42,880
things yeah things constantly

1304
00:48:42,880 --> 00:48:44,720
yeah

1305
00:48:44,720 --> 00:48:48,880
yeah things surprise us often because

1306
00:48:48,880 --> 00:48:50,800
sometimes they don't tell us everything

1307
00:48:50,800 --> 00:48:52,319
like for instance

1308
00:48:52,319 --> 00:48:54,640
we did this model that calculated how

1309
00:48:54,640 --> 00:48:56,079
much energy they would get out of their

1310
00:48:56,079 --> 00:48:58,240
fuel and they said oh by the way we

1311
00:48:58,240 --> 00:49:00,319
changed our measuring system 10 years

1312
00:49:00,319 --> 00:49:02,880
ago so half the data you have

1313
00:49:02,880 --> 00:49:06,079
is off by 20 well that explained why we

1314
00:49:06,079 --> 00:49:08,079
were concerned about our answer

1315
00:49:08,079 --> 00:49:10,000
and so it's a

1316
00:49:10,000 --> 00:49:11,760
it's a constant

1317
00:49:11,760 --> 00:49:14,559
kind of making sure things make sense

1318
00:49:14,559 --> 00:49:16,720
it's not specifically a fairness thing

1319
00:49:16,720 --> 00:49:18,720
it's more of a trust thing

1320
00:49:18,720 --> 00:49:22,000
where understanding and accuracy

1321
00:49:22,000 --> 00:49:25,440
leads to trust

1322
00:49:26,559 --> 00:49:28,319
just to continue with that

1323
00:49:28,319 --> 00:49:30,400
trustworthiness

1324
00:49:30,400 --> 00:49:31,680
you know

1325
00:49:31,680 --> 00:49:32,640
should we

1326
00:49:32,640 --> 00:49:34,400
you know why what does it mean to be

1327
00:49:34,400 --> 00:49:36,480
trustworthy why do we care is it is it

1328
00:49:36,480 --> 00:49:38,319
enough to be just better than what we

1329
00:49:38,319 --> 00:49:39,920
had before

1330
00:49:39,920 --> 00:49:42,000
or is that not good enough

1331
00:49:42,000 --> 00:49:44,359
well i i think that

1332
00:49:44,359 --> 00:49:48,000
trustworthiness has a different

1333
00:49:48,000 --> 00:49:51,280
level of or a slightly different meaning

1334
00:49:51,280 --> 00:49:53,280
when you're dealing with people than

1335
00:49:53,280 --> 00:49:55,200
when you're dealing with machines

1336
00:49:55,200 --> 00:49:56,400
because

1337
00:49:56,400 --> 00:49:58,160
you know if we make a mistake somebody's

1338
00:49:58,160 --> 00:49:59,359
gonna lose

1339
00:49:59,359 --> 00:50:01,119
a hundred thousand dollars

1340
00:50:01,119 --> 00:50:03,359
if a machine makes a mistake about a

1341
00:50:03,359 --> 00:50:04,480
person

1342
00:50:04,480 --> 00:50:05,680
they're gonna lose the opportunity to

1343
00:50:05,680 --> 00:50:09,119
buy a house or a car or maybe even

1344
00:50:09,119 --> 00:50:11,440
something worse is gonna happen to them

1345
00:50:11,440 --> 00:50:13,599
so i think um

1346
00:50:13,599 --> 00:50:16,319
trust in a in an industrial quote

1347
00:50:16,319 --> 00:50:18,880
setting is a little different than trust

1348
00:50:18,880 --> 00:50:21,200
in a you know where an ai is making

1349
00:50:21,200 --> 00:50:24,640
decisions about human beings

1350
00:50:24,640 --> 00:50:27,119
what about in your context so is that

1351
00:50:27,119 --> 00:50:28,319
you know if

1352
00:50:28,319 --> 00:50:29,359
did

1353
00:50:29,359 --> 00:50:30,800
if you go and say

1354
00:50:30,800 --> 00:50:32,000
you know our predictions are going to be

1355
00:50:32,000 --> 00:50:34,240
better than what you had before

1356
00:50:34,240 --> 00:50:36,240
is shouldn't that be good enough

1357
00:50:36,240 --> 00:50:38,720
well it's it's you know good enough is

1358
00:50:38,720 --> 00:50:41,119
never good enough and that's actually

1359
00:50:41,119 --> 00:50:44,880
you know what we do we are we constantly

1360
00:50:44,880 --> 00:50:48,000
try to improve the accuracy and

1361
00:50:48,000 --> 00:50:49,359
the certainty with which these

1362
00:50:49,359 --> 00:50:50,880
predictions are made

1363
00:50:50,880 --> 00:50:51,839
um

1364
00:50:51,839 --> 00:50:54,559
generally that depends on having more

1365
00:50:54,559 --> 00:50:55,520
data

1366
00:50:55,520 --> 00:50:58,240
i mean we we talk a lot about fancy ai

1367
00:50:58,240 --> 00:51:01,040
models and this and that but ultimately

1368
00:51:01,040 --> 00:51:03,760
you know 95 of the battle is do you have

1369
00:51:03,760 --> 00:51:05,520
the right data and if you interpret it

1370
00:51:05,520 --> 00:51:08,319
correctly

1371
00:51:08,319 --> 00:51:09,920
if i could chris i just wanna yeah i

1372
00:51:09,920 --> 00:51:10,960
just want i think this is such an

1373
00:51:10,960 --> 00:51:12,800
interesting example um and i wanted to

1374
00:51:12,800 --> 00:51:14,480
bring in back the question of power and

1375
00:51:14,480 --> 00:51:16,240
i think when you talk about ai

1376
00:51:16,240 --> 00:51:17,680
trustworthiness i think it's an

1377
00:51:17,680 --> 00:51:19,040
interesting question like whose trust

1378
00:51:19,040 --> 00:51:21,359
are you trying to earn um and who are

1379
00:51:21,359 --> 00:51:22,720
the stakeholders that you're trying to

1380
00:51:22,720 --> 00:51:24,480
appeal to and i think even thinking

1381
00:51:24,480 --> 00:51:25,839
about nuclear power right when you think

1382
00:51:25,839 --> 00:51:27,359
about uranium mining and you think about

1383
00:51:27,359 --> 00:51:29,119
other communities that are perhaps not

1384
00:51:29,119 --> 00:51:30,880
the folks you need to interact with on a

1385
00:51:30,880 --> 00:51:33,040
daily basis but thinking about ashish's

1386
00:51:33,040 --> 00:51:35,119
point about the entire life cycle of a

1387
00:51:35,119 --> 00:51:36,880
model the entire life cycle of the

1388
00:51:36,880 --> 00:51:39,119
system are there stakeholders or are

1389
00:51:39,119 --> 00:51:41,119
there folks who might feel like their

1390
00:51:41,119 --> 00:51:42,800
measure of a system's trustworthiness is

1391
00:51:42,800 --> 00:51:44,160
going to be very different than perhaps

1392
00:51:44,160 --> 00:51:45,839
the different stakeholders but who are

1393
00:51:45,839 --> 00:51:47,200
the people that we're actually asking

1394
00:51:47,200 --> 00:51:48,400
whether or not they find the system

1395
00:51:48,400 --> 00:51:50,480
trustworthy or not is a question that i

1396
00:51:50,480 --> 00:51:52,960
find myself returning to

1397
00:51:52,960 --> 00:51:55,920
and we're mainly asking

1398
00:51:55,920 --> 00:51:57,920
the people who

1399
00:51:57,920 --> 00:52:00,880
use the system and who manage the whole

1400
00:52:00,880 --> 00:52:02,400
energy process

1401
00:52:02,400 --> 00:52:04,400
and one of the outcomes

1402
00:52:04,400 --> 00:52:07,040
of the the things that we do

1403
00:52:07,040 --> 00:52:09,920
is they do get to use less fuel which

1404
00:52:09,920 --> 00:52:12,160
means there's less waste which means

1405
00:52:12,160 --> 00:52:14,240
there's less of a problem downstream in

1406
00:52:14,240 --> 00:52:16,960
storing that waste um but no we don't

1407
00:52:16,960 --> 00:52:18,720
interact with the uranium miners

1408
00:52:18,720 --> 00:52:19,760
although

1409
00:52:19,760 --> 00:52:21,520
um

1410
00:52:21,520 --> 00:52:23,760
we do interact a lot with the green

1411
00:52:23,760 --> 00:52:26,079
energy community who you know the whole

1412
00:52:26,079 --> 00:52:27,680
idea of reducing

1413
00:52:27,680 --> 00:52:31,359
co2 emissions and from that perspective

1414
00:52:31,359 --> 00:52:33,920
you know they they tend to

1415
00:52:33,920 --> 00:52:35,440
they're at least interested in what

1416
00:52:35,440 --> 00:52:37,040
we're doing if not

1417
00:52:37,040 --> 00:52:40,319
giving us their trust

1418
00:52:41,119 --> 00:52:44,240
i think to add to this discussion

1419
00:52:44,240 --> 00:52:46,800
um you know about the stakeholders and

1420
00:52:46,800 --> 00:52:48,319
links they pointed out

1421
00:52:48,319 --> 00:52:50,559
um there's a great point that is coming

1422
00:52:50,559 --> 00:52:53,040
for about use of decentralized trust

1423
00:52:53,040 --> 00:52:55,520
that we are actually ready to

1424
00:52:55,520 --> 00:52:56,480
adopt

1425
00:52:56,480 --> 00:52:59,440
uh the concept of decentralized trust in

1426
00:52:59,440 --> 00:53:01,839
an ai governance

1427
00:53:01,839 --> 00:53:04,720
for for trustworthiness of ai and there

1428
00:53:04,720 --> 00:53:06,559
are there are certain

1429
00:53:06,559 --> 00:53:10,000
startups are there of using blockchain

1430
00:53:10,000 --> 00:53:11,119
for

1431
00:53:11,119 --> 00:53:13,440
for aifrs and then

1432
00:53:13,440 --> 00:53:15,200
yeah trustworthiness

1433
00:53:15,200 --> 00:53:16,640
and and also

1434
00:53:16,640 --> 00:53:18,400
i'll connect that to something called

1435
00:53:18,400 --> 00:53:21,440
supply chain of fairness or supply chain

1436
00:53:21,440 --> 00:53:22,480
of trust

1437
00:53:22,480 --> 00:53:25,440
and ai uh ai aspect because when you

1438
00:53:25,440 --> 00:53:27,119
talk about this

1439
00:53:27,119 --> 00:53:29,040
the nuclear waste and related

1440
00:53:29,040 --> 00:53:30,319
stakeholders

1441
00:53:30,319 --> 00:53:32,559
right how does the supply chain look

1442
00:53:32,559 --> 00:53:34,160
like and what does the

1443
00:53:34,160 --> 00:53:37,040
related responsibilities lie there

1444
00:53:37,040 --> 00:53:38,880
and decision making as well as as well

1445
00:53:38,880 --> 00:53:42,079
as in making sure that it is sustainable

1446
00:53:42,079 --> 00:53:44,319
and it is and it is the right decision

1447
00:53:44,319 --> 00:53:46,720
to make

1448
00:53:50,960 --> 00:53:52,160
so

1449
00:53:52,160 --> 00:53:55,040
anyone else on that

1450
00:53:55,040 --> 00:53:57,440
what about um

1451
00:53:57,440 --> 00:53:59,200
you know we talked about these safety

1452
00:53:59,200 --> 00:54:02,000
critical systems for trust uh autonomous

1453
00:54:02,000 --> 00:54:05,520
vehicles uh nuclear power plants

1454
00:54:05,520 --> 00:54:07,200
where else i mean we have things that

1455
00:54:07,200 --> 00:54:09,119
really aren't kind of the safety

1456
00:54:09,119 --> 00:54:10,880
critical that you know something goes

1457
00:54:10,880 --> 00:54:13,280
wrong and people die

1458
00:54:13,280 --> 00:54:15,119
where we still need this trustworthiness

1459
00:54:15,119 --> 00:54:18,240
from our ai or are there areas where

1460
00:54:18,240 --> 00:54:20,880
ai trustworthiness maybe isn't a big

1461
00:54:20,880 --> 00:54:23,880
issue

1462
00:54:24,079 --> 00:54:25,200
any

1463
00:54:25,200 --> 00:54:28,319
ah so i'll just add quickly yeah

1464
00:54:28,319 --> 00:54:30,319
quickly there's a lot of work going on

1465
00:54:30,319 --> 00:54:31,839
in

1466
00:54:31,839 --> 00:54:33,280
healthcare

1467
00:54:33,280 --> 00:54:34,559
one is about

1468
00:54:34,559 --> 00:54:36,240
drug discovery

1469
00:54:36,240 --> 00:54:38,960
right so drug discovery is is an area

1470
00:54:38,960 --> 00:54:41,680
personalized treatment is an area where

1471
00:54:41,680 --> 00:54:43,280
ai trustworthiness is extremely

1472
00:54:43,280 --> 00:54:45,359
important because personalized treatment

1473
00:54:45,359 --> 00:54:47,599
where you want to make sure that

1474
00:54:47,599 --> 00:54:49,599
you know it is it is fair to everybody

1475
00:54:49,599 --> 00:54:51,440
you know it's not treating someone in an

1476
00:54:51,440 --> 00:54:54,400
unfair manner but it is also trusted

1477
00:54:54,400 --> 00:54:57,040
that it's not going to actually

1478
00:54:57,040 --> 00:54:59,839
recommend a drug to someone

1479
00:54:59,839 --> 00:55:02,640
or or a group of individuals because if

1480
00:55:02,640 --> 00:55:04,559
the system has been compromised through

1481
00:55:04,559 --> 00:55:06,480
through data poisoning attacks or

1482
00:55:06,480 --> 00:55:09,280
through uh through model poisoning or

1483
00:55:09,280 --> 00:55:10,319
something

1484
00:55:10,319 --> 00:55:13,200
right and and uh this is where

1485
00:55:13,200 --> 00:55:15,680
we need to be we need to see both

1486
00:55:15,680 --> 00:55:17,920
fairness and trustworthy how they can

1487
00:55:17,920 --> 00:55:21,640
how they can play together

1488
00:55:24,079 --> 00:55:26,319
anybody else's

1489
00:55:26,319 --> 00:55:27,760
thoughts on this

1490
00:55:27,760 --> 00:55:31,200
well i mean i'll just say um

1491
00:55:31,200 --> 00:55:33,440
i think in

1492
00:55:33,440 --> 00:55:37,359
any situation in which

1493
00:55:37,359 --> 00:55:40,000
an ai application is

1494
00:55:40,000 --> 00:55:42,240
making a decision

1495
00:55:42,240 --> 00:55:44,079
or substantially

1496
00:55:44,079 --> 00:55:46,400
influencing a decision trust and

1497
00:55:46,400 --> 00:55:48,400
fairness are going to be important

1498
00:55:48,400 --> 00:55:49,680
right because

1499
00:55:49,680 --> 00:55:51,839
any decision is going to involve a

1500
00:55:51,839 --> 00:55:55,319
distribution of costs and benefits

1501
00:55:55,319 --> 00:55:58,480
responsibilities and so on

1502
00:55:58,480 --> 00:55:59,599
and

1503
00:55:59,599 --> 00:56:01,599
even if there aren't strictly speaking

1504
00:56:01,599 --> 00:56:03,599
winners and losers for that kind of a

1505
00:56:03,599 --> 00:56:05,200
decision there's still going to be

1506
00:56:05,200 --> 00:56:06,880
people who wanted it to go a different

1507
00:56:06,880 --> 00:56:08,720
way and people who wanted it to go that

1508
00:56:08,720 --> 00:56:10,319
way

1509
00:56:10,319 --> 00:56:12,960
and as long as it's involved in those

1510
00:56:12,960 --> 00:56:15,880
sorts of decisions the

1511
00:56:15,880 --> 00:56:19,280
long-term legitimacy

1512
00:56:19,280 --> 00:56:22,880
of whatever institution is using the ai

1513
00:56:22,880 --> 00:56:25,200
application to make those decisions is

1514
00:56:25,200 --> 00:56:26,480
at stake

1515
00:56:26,480 --> 00:56:28,880
and the treatment of people within those

1516
00:56:28,880 --> 00:56:32,720
institutions is at stake i think

1517
00:56:34,160 --> 00:56:35,920
well i mean we're getting close to time

1518
00:56:35,920 --> 00:56:37,760
so i'm going to start off with kind of a

1519
00:56:37,760 --> 00:56:40,799
final question for each of the panelists

1520
00:56:40,799 --> 00:56:42,480
to

1521
00:56:42,480 --> 00:56:45,119
what what do you see as the greatest

1522
00:56:45,119 --> 00:56:48,079
threat or barrier to

1523
00:56:48,079 --> 00:56:49,040
fair

1524
00:56:49,040 --> 00:56:52,720
or trustworthiness or justice in

1525
00:56:52,720 --> 00:56:56,319
in a.i so let's start here with lindsay

1526
00:56:56,319 --> 00:56:59,119
sure uh so i guess i i'd name a few

1527
00:56:59,119 --> 00:57:01,520
things i think the first for me is a

1528
00:57:01,520 --> 00:57:03,440
lack of interdisciplinarity in general

1529
00:57:03,440 --> 00:57:05,760
and i think the way that uh folks who

1530
00:57:05,760 --> 00:57:07,440
are trained to build ai systems they're

1531
00:57:07,440 --> 00:57:09,839
not necessarily required to talk to

1532
00:57:09,839 --> 00:57:12,000
folks with domain expertise from outside

1533
00:57:12,000 --> 00:57:13,040
the field

1534
00:57:13,040 --> 00:57:15,280
ethics isn't rigorously incorporated

1535
00:57:15,280 --> 00:57:17,200
into the education of a lot of folks who

1536
00:57:17,200 --> 00:57:18,720
end up building ai systems i think

1537
00:57:18,720 --> 00:57:20,799
that's another uh reform that i could

1538
00:57:20,799 --> 00:57:23,599
imagine um i also think you know for me

1539
00:57:23,599 --> 00:57:25,359
personally i do believe that

1540
00:57:25,359 --> 00:57:27,359
diversifying who gets to participate in

1541
00:57:27,359 --> 00:57:29,200
the development of these systems and

1542
00:57:29,200 --> 00:57:31,760
also giving folks who are oftentimes the

1543
00:57:31,760 --> 00:57:34,079
most severely and desperately impacted

1544
00:57:34,079 --> 00:57:37,280
more agency more power more control at

1545
00:57:37,280 --> 00:57:39,680
all stages of the ai life cycle i think

1546
00:57:39,680 --> 00:57:41,520
that those type of changes would would

1547
00:57:41,520 --> 00:57:43,599
get us move the needle in significant

1548
00:57:43,599 --> 00:57:45,280
kinds of ways yeah

1549
00:57:45,280 --> 00:57:46,559
okay

1550
00:57:46,559 --> 00:57:47,920
yeah um

1551
00:57:47,920 --> 00:57:50,400
expectations right and it's connected to

1552
00:57:50,400 --> 00:57:52,880
some of the things lindsay just said um

1553
00:57:52,880 --> 00:57:55,760
thinking that ai the successful ai will

1554
00:57:55,760 --> 00:57:58,160
somehow be an end run around human

1555
00:57:58,160 --> 00:58:00,160
complexity and that's not going to

1556
00:58:00,160 --> 00:58:01,520
happen it's going to be a certain kind

1557
00:58:01,520 --> 00:58:03,440
of participation in

1558
00:58:03,440 --> 00:58:05,760
human complexity and it can be an

1559
00:58:05,760 --> 00:58:08,000
improvement of that kind of complexity

1560
00:58:08,000 --> 00:58:09,839
but it is not going to be an end run

1561
00:58:09,839 --> 00:58:14,000
around it nor should it attempt to do so

1562
00:58:14,000 --> 00:58:16,079
you know i i think um

1563
00:58:16,079 --> 00:58:18,160
you know the biggest threat i do think

1564
00:58:18,160 --> 00:58:21,200
it's also expectations i think there's a

1565
00:58:21,200 --> 00:58:24,559
a level of misconception about ai i mean

1566
00:58:24,559 --> 00:58:26,480
people they're i mean they're very

1567
00:58:26,480 --> 00:58:28,960
bright people who think that uh

1568
00:58:28,960 --> 00:58:30,559
the terminator is gonna come here from

1569
00:58:30,559 --> 00:58:31,680
the future

1570
00:58:31,680 --> 00:58:34,640
uh it's it's really not that capable i

1571
00:58:34,640 --> 00:58:36,319
mean at least now and for the

1572
00:58:36,319 --> 00:58:38,960
foreseeable future it's a really really

1573
00:58:38,960 --> 00:58:40,640
excellent calculator

1574
00:58:40,640 --> 00:58:42,319
and

1575
00:58:42,319 --> 00:58:44,240
and that's the way you know our team is

1576
00:58:44,240 --> 00:58:47,440
our team is all engineers who understand

1577
00:58:47,440 --> 00:58:49,599
the systems that they're working on who

1578
00:58:49,599 --> 00:58:51,520
use ai as a tool

1579
00:58:51,520 --> 00:58:54,960
but in in my realm the biggest threat to

1580
00:58:54,960 --> 00:58:59,119
ai is just data understanding data

1581
00:58:59,119 --> 00:59:02,319
and using it correctly and understanding

1582
00:59:02,319 --> 00:59:03,119
where

1583
00:59:03,119 --> 00:59:05,839
the data isn't really going to give you

1584
00:59:05,839 --> 00:59:07,119
a result

1585
00:59:07,119 --> 00:59:09,920
that is applicable in another regime

1586
00:59:09,920 --> 00:59:12,799
that you want it to be applicable in

1587
00:59:12,799 --> 00:59:13,599
um

1588
00:59:13,599 --> 00:59:16,240
so my experience is kind of in a sense

1589
00:59:16,240 --> 00:59:19,200
limited we don't deal with autonomous

1590
00:59:19,200 --> 00:59:22,480
systems etc

1591
00:59:26,559 --> 00:59:29,359
so i think one of the important

1592
00:59:29,359 --> 00:59:32,559
barrier is actually the data

1593
00:59:32,559 --> 00:59:33,760
to agree with

1594
00:59:33,760 --> 00:59:36,160
tom is that the the way data is

1595
00:59:36,160 --> 00:59:38,480
collected curated and

1596
00:59:38,480 --> 00:59:39,440
used

1597
00:59:39,440 --> 00:59:40,319
for

1598
00:59:40,319 --> 00:59:42,240
testing sorry training testing

1599
00:59:42,240 --> 00:59:45,520
validation what not right and that and

1600
00:59:45,520 --> 00:59:47,680
that also that that is also

1601
00:59:47,680 --> 00:59:49,200
um

1602
00:59:49,200 --> 00:59:51,680
is a complicated issue when we bring in

1603
00:59:51,680 --> 00:59:54,000
regulatory compliance like gdpr

1604
00:59:54,000 --> 00:59:57,280
hipaa uh also trying to play uh that

1605
00:59:57,280 --> 01:00:00,079
clears data islands you know so and when

1606
01:00:00,079 --> 01:00:02,079
we we build models in one data island

1607
01:00:02,079 --> 01:00:04,480
and then try to reuse it in another data

1608
01:00:04,480 --> 01:00:07,040
island and that that is going to cause

1609
01:00:07,040 --> 01:00:08,799
concern about fairness as well as

1610
01:00:08,799 --> 01:00:11,119
trustworthiness the second one is

1611
01:00:11,119 --> 01:00:13,520
actually bringing regulatory compliance

1612
01:00:13,520 --> 01:00:15,119
you know while we are talking regulatory

1613
01:00:15,119 --> 01:00:16,160
compliance

1614
01:00:16,160 --> 01:00:17,280
not helping

1615
01:00:17,280 --> 01:00:18,799
um

1616
01:00:18,799 --> 01:00:20,319
ai fairness and transparency but

1617
01:00:20,319 --> 01:00:23,359
regulative compliance should be there

1618
01:00:23,359 --> 01:00:24,960
under standardization should be there on

1619
01:00:24,960 --> 01:00:28,000
how such data is curated and and how

1620
01:00:28,000 --> 01:00:29,520
what what do you mean by fairness and

1621
01:00:29,520 --> 01:00:31,599
trustworthiness and the third one is a

1622
01:00:31,599 --> 01:00:33,359
decent decentralized

1623
01:00:33,359 --> 01:00:35,760
model of fairness and trust that is

1624
01:00:35,760 --> 01:00:37,680
essential that there are multiple

1625
01:00:37,680 --> 01:00:39,040
stakeholders

1626
01:00:39,040 --> 01:00:40,480
there should not be any centralized

1627
01:00:40,480 --> 01:00:41,599
authority

1628
01:00:41,599 --> 01:00:44,160
to to work on that and another work that

1629
01:00:44,160 --> 01:00:46,960
is ongoing with us is cisco research on

1630
01:00:46,960 --> 01:00:49,839
responsible ai uh we are we are looking

1631
01:00:49,839 --> 01:00:53,839
at some of the few of the things here

1632
01:00:53,839 --> 01:00:54,839
thank

1633
01:00:54,839 --> 01:00:56,559
you well

1634
01:00:56,559 --> 01:00:58,880
we're time here and need to move on to

1635
01:00:58,880 --> 01:01:01,119
the poster session so let me let's thank

1636
01:01:01,119 --> 01:01:02,720
our panelists i think this has been

1637
01:01:02,720 --> 01:01:05,720
wonderful

1638
01:01:18,000 --> 01:01:20,079
you

