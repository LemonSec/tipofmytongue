1
00:00:00,719 --> 00:00:04,380
so hello everyone welcome to the final

2
00:00:04,380 --> 00:00:07,460
security seminar of the semester

3
00:00:07,460 --> 00:00:09,179
and

4
00:00:09,179 --> 00:00:10,980
um hopefully you've been enjoying these

5
00:00:10,980 --> 00:00:13,740
as we've gone along the seminar will

6
00:00:13,740 --> 00:00:17,940
pick up again in January uh first

7
00:00:17,940 --> 00:00:21,119
scheduled session is January 11th when

8
00:00:21,119 --> 00:00:23,580
the semester resumes

9
00:00:23,580 --> 00:00:26,880
today's speaker do you close us out in a

10
00:00:26,880 --> 00:00:30,779
and a with a flourish is uh Julie Haney

11
00:00:30,779 --> 00:00:33,300
from the National Institute of Standards

12
00:00:33,300 --> 00:00:35,700
and technology which is otherwise known

13
00:00:35,700 --> 00:00:37,100
as nist

14
00:00:37,100 --> 00:00:39,540
and she's going to be speaking to us on

15
00:00:39,540 --> 00:00:41,940
a rather fascinating topic that's near

16
00:00:41,940 --> 00:00:44,579
and dear to me which is that users are

17
00:00:44,579 --> 00:00:45,899
not stupid

18
00:00:45,899 --> 00:00:48,719
it's six cyber security pitfalls

19
00:00:48,719 --> 00:00:50,039
overturned

20
00:00:50,039 --> 00:00:52,079
and um

21
00:00:52,079 --> 00:00:53,700
I think it will be a comfort to anyone

22
00:00:53,700 --> 00:00:55,620
watching this uh you're not stupid

23
00:00:55,620 --> 00:00:58,020
you're just sometimes misled by the

24
00:00:58,020 --> 00:00:59,300
technology

25
00:00:59,300 --> 00:01:02,280
so thank you very much Julie we look

26
00:01:02,280 --> 00:01:04,379
forward to hearing us

27
00:01:04,379 --> 00:01:06,659
thanks so much Scott appreciate the

28
00:01:06,659 --> 00:01:08,520
invitation this to speak with you all

29
00:01:08,520 --> 00:01:09,780
today

30
00:01:09,780 --> 00:01:12,360
um can everyone see my screen I'm

31
00:01:12,360 --> 00:01:15,960
assuming yep okay great

32
00:01:15,960 --> 00:01:17,880
um I'm very excited to talk with you all

33
00:01:17,880 --> 00:01:20,700
today about a topic that I'm quite

34
00:01:20,700 --> 00:01:23,340
passionate about and that's the human

35
00:01:23,340 --> 00:01:26,280
element of cyber security and and in

36
00:01:26,280 --> 00:01:28,680
particular some common human element

37
00:01:28,680 --> 00:01:31,200
pitfalls and misconceptions within the

38
00:01:31,200 --> 00:01:33,420
security community

39
00:01:33,420 --> 00:01:35,759
um so this is a version a slightly

40
00:01:35,759 --> 00:01:37,619
streamlined version of a talk that I

41
00:01:37,619 --> 00:01:39,479
gave at the RSA conference this past

42
00:01:39,479 --> 00:01:40,500
June

43
00:01:40,500 --> 00:01:43,200
and so the talk is very much aimed at

44
00:01:43,200 --> 00:01:45,540
practitioners who are the main audience

45
00:01:45,540 --> 00:01:49,140
at RSA but the talk is really based on

46
00:01:49,140 --> 00:01:51,180
kind of three foundations

47
00:01:51,180 --> 00:01:54,420
um so the first is research in the human

48
00:01:54,420 --> 00:01:56,759
element of cyber security so I'm going

49
00:01:56,759 --> 00:01:59,399
to be drawing a lot on that research

50
00:01:59,399 --> 00:02:01,200
um from a variety of research

51
00:02:01,200 --> 00:02:03,659
researchers including research that's

52
00:02:03,659 --> 00:02:06,420
being done in my own group

53
00:02:06,420 --> 00:02:09,660
um the other Foundation are observations

54
00:02:09,660 --> 00:02:11,038
in industry

55
00:02:11,038 --> 00:02:12,840
so things that are actually happening

56
00:02:12,840 --> 00:02:15,599
industry surveys

57
00:02:15,599 --> 00:02:18,180
um anecdotal information examples of

58
00:02:18,180 --> 00:02:20,040
things that are happening in the real

59
00:02:20,040 --> 00:02:21,060
world

60
00:02:21,060 --> 00:02:23,640
and then finally really the inspiration

61
00:02:23,640 --> 00:02:26,580
of this talk uh was my own prior

62
00:02:26,580 --> 00:02:29,879
experience having worked for over 20

63
00:02:29,879 --> 00:02:32,700
years as a security practitioner in the

64
00:02:32,700 --> 00:02:35,819
Department of Defense before I went back

65
00:02:35,819 --> 00:02:37,560
to grad school and got into research

66
00:02:37,560 --> 00:02:40,520
several years ago

67
00:02:40,819 --> 00:02:44,760
so we'll get started

68
00:02:44,760 --> 00:02:46,800
all right so this is just the typical

69
00:02:46,800 --> 00:02:48,660
government disclaimer I might mention

70
00:02:48,660 --> 00:02:50,519
some companies or products during my

71
00:02:50,519 --> 00:02:52,319
talk it doesn't mean nist thinks those

72
00:02:52,319 --> 00:02:54,480
are good or bad

73
00:02:54,480 --> 00:02:56,280
all right so I actually wanted to start

74
00:02:56,280 --> 00:02:58,140
with a little bit of information about

75
00:02:58,140 --> 00:02:59,940
my group

76
00:02:59,940 --> 00:03:02,160
um so at nist I'm in the information

77
00:03:02,160 --> 00:03:05,580
technology lab where our mission is to

78
00:03:05,580 --> 00:03:07,080
cultivate trust and information

79
00:03:07,080 --> 00:03:10,800
technology and Metrology and you can't

80
00:03:10,800 --> 00:03:13,920
have trust without considering that

81
00:03:13,920 --> 00:03:16,739
human in the loop so that's where my

82
00:03:16,739 --> 00:03:18,959
group comes in where the visualization

83
00:03:18,959 --> 00:03:20,940
and usability group

84
00:03:20,940 --> 00:03:23,580
and we really see ourselves as being the

85
00:03:23,580 --> 00:03:27,000
voice of people and their needs in a

86
00:03:27,000 --> 00:03:29,159
field that's otherwise viewed through

87
00:03:29,159 --> 00:03:32,700
this kind of Technology dominant lens

88
00:03:32,700 --> 00:03:36,360
and to look at the human and I.T we

89
00:03:36,360 --> 00:03:37,819
bring to bear

90
00:03:37,819 --> 00:03:40,739
insights from multiple disciplines um

91
00:03:40,739 --> 00:03:42,120
very much like

92
00:03:42,120 --> 00:03:43,940
um the serious group we are

93
00:03:43,940 --> 00:03:46,500
multi-disciplinary we have folks with

94
00:03:46,500 --> 00:03:48,720
backgrounds in computer science and

95
00:03:48,720 --> 00:03:51,780
cognitive science psychology human

96
00:03:51,780 --> 00:03:54,180
factors human computer interaction and

97
00:03:54,180 --> 00:03:55,620
cyber security

98
00:03:55,620 --> 00:03:57,900
and we work in a variety of different

99
00:03:57,900 --> 00:04:01,019
areas and the areas on the slide here

100
00:04:01,019 --> 00:04:03,959
are just any some examples of

101
00:04:03,959 --> 00:04:06,060
um of of things that we've done over the

102
00:04:06,060 --> 00:04:07,920
years and obviously today I'm going to

103
00:04:07,920 --> 00:04:09,900
be focusing on the usable cyber security

104
00:04:09,900 --> 00:04:12,480
program

105
00:04:12,480 --> 00:04:15,120
um and in that program we championed the

106
00:04:15,120 --> 00:04:18,418
human in cyber security we're very

107
00:04:18,418 --> 00:04:20,519
impact oriented

108
00:04:20,519 --> 00:04:23,520
um so very much applied research we want

109
00:04:23,520 --> 00:04:25,500
our research results to be able to

110
00:04:25,500 --> 00:04:27,300
inform

111
00:04:27,300 --> 00:04:29,340
um actionable guidance for the the folks

112
00:04:29,340 --> 00:04:30,900
that can actually make a difference that

113
00:04:30,900 --> 00:04:33,419
can take action on it so people like

114
00:04:33,419 --> 00:04:35,100
security and I.T professionals and

115
00:04:35,100 --> 00:04:37,500
policy makers and decision makers so

116
00:04:37,500 --> 00:04:39,419
that they will start considering the

117
00:04:39,419 --> 00:04:41,280
human element when they're making

118
00:04:41,280 --> 00:04:42,900
security decisions when they're

119
00:04:42,900 --> 00:04:44,699
developing security processes and

120
00:04:44,699 --> 00:04:47,180
products

121
00:04:48,540 --> 00:04:51,120
so these are just a few examples of some

122
00:04:51,120 --> 00:04:53,160
of the type of projects that we've

123
00:04:53,160 --> 00:04:56,580
worked on over the years in the past we

124
00:04:56,580 --> 00:04:58,560
started the usable security effort

125
00:04:58,560 --> 00:05:00,600
really with authentication so looking at

126
00:05:00,600 --> 00:05:04,919
passwords and smart card usage looking

127
00:05:04,919 --> 00:05:06,240
at

128
00:05:06,240 --> 00:05:07,919
um people security and privacy

129
00:05:07,919 --> 00:05:09,360
perceptions

130
00:05:09,360 --> 00:05:11,460
the first project I worked on at nist

131
00:05:11,460 --> 00:05:13,620
actually had to do with cryptographic

132
00:05:13,620 --> 00:05:15,540
development and how difficult it is for

133
00:05:15,540 --> 00:05:19,139
developers to implement crypto in

134
00:05:19,139 --> 00:05:20,639
products a lot of it because of

135
00:05:20,639 --> 00:05:24,180
unusability of uh crypto libraries

136
00:05:24,180 --> 00:05:27,000
and then more recently we've had a

137
00:05:27,000 --> 00:05:28,979
multi-year effort looking at children

138
00:05:28,979 --> 00:05:31,500
and their online security and privacy

139
00:05:31,500 --> 00:05:34,259
perceptions and and behaviors

140
00:05:34,259 --> 00:05:36,840
also phishing

141
00:05:36,840 --> 00:05:39,300
um looking at why some people click on

142
00:05:39,300 --> 00:05:40,800
phishing emails and why some people

143
00:05:40,800 --> 00:05:43,620
don't click and how do we determine the

144
00:05:43,620 --> 00:05:44,880
difficulty how do we measure the

145
00:05:44,880 --> 00:05:47,580
difficulty of phishing emails

146
00:05:47,580 --> 00:05:49,919
I've worked for a few years on multiple

147
00:05:49,919 --> 00:05:52,740
efforts around security adoption and

148
00:05:52,740 --> 00:05:55,740
more recently looking at security

149
00:05:55,740 --> 00:05:57,360
awareness and training in the US

150
00:05:57,360 --> 00:06:00,479
government so identifying challenges and

151
00:06:00,479 --> 00:06:02,460
approaches within the government for

152
00:06:02,460 --> 00:06:04,380
those and then we've done a couple

153
00:06:04,380 --> 00:06:06,360
studies with smart home security and

154
00:06:06,360 --> 00:06:10,400
privacy from the consumer perspective

155
00:06:11,000 --> 00:06:14,520
all right so now we'll jump into the the

156
00:06:14,520 --> 00:06:17,100
actual meat of the talk here

157
00:06:17,100 --> 00:06:19,320
um but I wanted to start talking about

158
00:06:19,320 --> 00:06:21,840
the human element of security

159
00:06:21,840 --> 00:06:24,960
um specifically two foundations of

160
00:06:24,960 --> 00:06:28,818
usability and usable security

161
00:06:30,180 --> 00:06:33,660
so this is the standard definition of

162
00:06:33,660 --> 00:06:36,479
usability it's an ISO standard

163
00:06:36,479 --> 00:06:38,340
um and it's quite a mouthful as many

164
00:06:38,340 --> 00:06:40,620
standards go it's the extent to which

165
00:06:40,620 --> 00:06:42,720
people can use systems products and

166
00:06:42,720 --> 00:06:45,180
services with effectiveness

167
00:06:45,180 --> 00:06:47,340
oh I'm sorry

168
00:06:47,340 --> 00:06:49,500
the extent to which a system product or

169
00:06:49,500 --> 00:06:52,020
service can be used by specified users

170
00:06:52,020 --> 00:06:54,300
to achieve specified goals with

171
00:06:54,300 --> 00:06:55,800
Effectiveness efficiency and

172
00:06:55,800 --> 00:06:58,080
satisfaction in the specified context of

173
00:06:58,080 --> 00:06:59,340
use

174
00:06:59,340 --> 00:07:01,020
um so let's break that down just a

175
00:07:01,020 --> 00:07:03,380
little bit

176
00:07:03,479 --> 00:07:06,180
so systems products or Services can be a

177
00:07:06,180 --> 00:07:07,680
lot of different things it could be

178
00:07:07,680 --> 00:07:10,380
traditional I.T like your devices

179
00:07:10,380 --> 00:07:13,380
software and services but it can also be

180
00:07:13,380 --> 00:07:15,840
processes so for example the steps

181
00:07:15,840 --> 00:07:18,300
involved in authenticating to a system

182
00:07:18,300 --> 00:07:21,419
and these can also be security policies

183
00:07:21,419 --> 00:07:23,280
or guidance documents or security

184
00:07:23,280 --> 00:07:25,979
training so any kind of output from

185
00:07:25,979 --> 00:07:27,960
security

186
00:07:27,960 --> 00:07:30,120
then we have the users those are just

187
00:07:30,120 --> 00:07:32,039
the humans involved in these

188
00:07:32,039 --> 00:07:33,479
interactions

189
00:07:33,479 --> 00:07:35,280
of course goals are just what people

190
00:07:35,280 --> 00:07:37,139
want to accomplish when they're using

191
00:07:37,139 --> 00:07:39,840
the systems products or services

192
00:07:39,840 --> 00:07:42,120
and then we get to the three components

193
00:07:42,120 --> 00:07:43,500
of usability

194
00:07:43,500 --> 00:07:45,960
so Effectiveness is all about whether

195
00:07:45,960 --> 00:07:48,000
people can successfully achieve their

196
00:07:48,000 --> 00:07:51,300
goals efficiency refers to the resources

197
00:07:51,300 --> 00:07:53,880
for example time or cognitive resources

198
00:07:53,880 --> 00:07:56,699
that are used to achieve those goals

199
00:07:56,699 --> 00:07:58,740
and satisfaction is really this

200
00:07:58,740 --> 00:08:01,860
intersection between someone's physical

201
00:08:01,860 --> 00:08:04,560
cognitive and emotional responses when

202
00:08:04,560 --> 00:08:06,240
they're using the system project or

203
00:08:06,240 --> 00:08:08,699
service and how well their needs and

204
00:08:08,699 --> 00:08:11,160
expectations are met

205
00:08:11,160 --> 00:08:14,280
and then finally context of use is a

206
00:08:14,280 --> 00:08:16,819
combination of several different things

207
00:08:16,819 --> 00:08:20,879
including the attributes of the user the

208
00:08:20,879 --> 00:08:23,039
characteristics of their tasks and goals

209
00:08:23,039 --> 00:08:25,080
and then the environments in which

210
00:08:25,080 --> 00:08:26,580
they're interacting with the technology

211
00:08:26,580 --> 00:08:28,199
so those could be technical or

212
00:08:28,199 --> 00:08:30,180
organizational or social or physical

213
00:08:30,180 --> 00:08:32,219
environments

214
00:08:32,219 --> 00:08:34,919
so we'll be revisiting these Concepts

215
00:08:34,919 --> 00:08:38,539
throughout the presentation

216
00:08:39,779 --> 00:08:42,839
okay so now on to usable security

217
00:08:42,839 --> 00:08:46,440
so way back in 2009 the Department of

218
00:08:46,440 --> 00:08:49,019
Homeland Security identified 11 hard

219
00:08:49,019 --> 00:08:50,459
problems and information security

220
00:08:50,459 --> 00:08:53,279
research and one of those was usable

221
00:08:53,279 --> 00:08:54,720
security

222
00:08:54,720 --> 00:08:56,700
but I think even though it's been a

223
00:08:56,700 --> 00:08:59,160
while what the report said is still very

224
00:08:59,160 --> 00:09:01,920
much relevant today and I I love this

225
00:09:01,920 --> 00:09:03,360
quote

226
00:09:03,360 --> 00:09:06,060
um the report said that security must be

227
00:09:06,060 --> 00:09:08,519
usable by persons ranging from

228
00:09:08,519 --> 00:09:10,860
non-technical users to experts and

229
00:09:10,860 --> 00:09:13,560
system administrators systems must be

230
00:09:13,560 --> 00:09:16,500
usable while maintaining Security in the

231
00:09:16,500 --> 00:09:18,959
absence of usable security there is

232
00:09:18,959 --> 00:09:22,500
ultimately no effective security

233
00:09:22,500 --> 00:09:25,080
and so when organizations fail to

234
00:09:25,080 --> 00:09:26,940
consider the human element there can be

235
00:09:26,940 --> 00:09:28,740
real consequences and I'm going to be

236
00:09:28,740 --> 00:09:32,959
talking about those consequences today

237
00:09:34,440 --> 00:09:37,019
so if the human element is so important

238
00:09:37,019 --> 00:09:40,140
why is it often overlooked well there's

239
00:09:40,140 --> 00:09:42,180
a lot of different reasons for that but

240
00:09:42,180 --> 00:09:45,779
I'm going to just briefly touch on four

241
00:09:45,779 --> 00:09:48,600
um so first of all the security field is

242
00:09:48,600 --> 00:09:50,899
just technology-centric by Nature

243
00:09:50,899 --> 00:09:53,279
technology is often viewed as the

244
00:09:53,279 --> 00:09:56,339
solution to security problems

245
00:09:56,339 --> 00:09:59,220
second many security folks have just

246
00:09:59,220 --> 00:10:00,920
never been trained in the human element

247
00:10:00,920 --> 00:10:03,480
it likely just wasn't part of their

248
00:10:03,480 --> 00:10:07,339
formal or their continuing education

249
00:10:07,500 --> 00:10:09,660
third taking a human-centric approach

250
00:10:09,660 --> 00:10:11,760
might be viewed as being resource

251
00:10:11,760 --> 00:10:14,880
intensive and as an impediment to

252
00:10:14,880 --> 00:10:18,240
getting security implemented efficiently

253
00:10:18,240 --> 00:10:20,880
and then finally Security Professionals

254
00:10:20,880 --> 00:10:23,160
May hold some misconceptions about the

255
00:10:23,160 --> 00:10:24,959
human element and the people they're

256
00:10:24,959 --> 00:10:26,940
ultimately supposed to be supporting

257
00:10:26,940 --> 00:10:30,899
which is really the focus of the talk

258
00:10:30,899 --> 00:10:32,820
um so let's start getting into those

259
00:10:32,820 --> 00:10:34,440
pitfalls

260
00:10:34,440 --> 00:10:36,779
um first a little caveat

261
00:10:36,779 --> 00:10:38,580
um I want to make sure everyone

262
00:10:38,580 --> 00:10:40,680
understands that this talk is really a

263
00:10:40,680 --> 00:10:43,980
no judgment Zone it's not intended to be

264
00:10:43,980 --> 00:10:46,440
overly critical of the security field or

265
00:10:46,440 --> 00:10:48,360
Security Professionals and it's

266
00:10:48,360 --> 00:10:50,880
particularly no judgment because I've

267
00:10:50,880 --> 00:10:53,760
been there myself I mentioned that I was

268
00:10:53,760 --> 00:10:55,700
a security practitioner for many years

269
00:10:55,700 --> 00:10:58,740
and especially early in my career I

270
00:10:58,740 --> 00:11:00,180
definitely made some mistakes and

271
00:11:00,180 --> 00:11:01,560
learned the hard way and saw other

272
00:11:01,560 --> 00:11:03,480
people learning the hard way and

273
00:11:03,480 --> 00:11:05,480
sometimes that was because we didn't

274
00:11:05,480 --> 00:11:08,160
consider the non-technical and the

275
00:11:08,160 --> 00:11:10,019
human-centric reasons why people and

276
00:11:10,019 --> 00:11:12,240
organizations don't always adopt

277
00:11:12,240 --> 00:11:14,820
security best practices so I really view

278
00:11:14,820 --> 00:11:18,360
these as Lessons Learned

279
00:11:18,360 --> 00:11:20,820
so for each Pitfall I'm going to

280
00:11:20,820 --> 00:11:22,740
describe what the pitfall is and then

281
00:11:22,740 --> 00:11:24,899
I'm going to provide an example which

282
00:11:24,899 --> 00:11:27,540
will either be a real world example or

283
00:11:27,540 --> 00:11:30,000
an interesting research finding and then

284
00:11:30,000 --> 00:11:32,100
I'm also going to offer some tips on how

285
00:11:32,100 --> 00:11:35,600
to overcome those pitfalls

286
00:11:37,079 --> 00:11:40,440
all right so Pitfall number one is

287
00:11:40,440 --> 00:11:43,399
assuming that users are stupid

288
00:11:43,399 --> 00:11:46,200
so over the years I've been in many

289
00:11:46,200 --> 00:11:47,940
conversations and overheard many

290
00:11:47,940 --> 00:11:49,260
conversations with Security

291
00:11:49,260 --> 00:11:52,860
Professionals who believe technology is

292
00:11:52,860 --> 00:11:54,839
the solution to all problems and that

293
00:11:54,839 --> 00:11:57,779
users are hopeless and they're stupid so

294
00:11:57,779 --> 00:11:59,519
we just need to tell them what to do and

295
00:11:59,519 --> 00:12:00,959
they just need to do it because we're

296
00:12:00,959 --> 00:12:03,000
the experts

297
00:12:03,000 --> 00:12:05,279
um and the reality is is that you know

298
00:12:05,279 --> 00:12:08,640
as as human beings we do make mistakes

299
00:12:08,640 --> 00:12:10,860
um but having this type of attitude can

300
00:12:10,860 --> 00:12:12,420
really backfire

301
00:12:12,420 --> 00:12:14,700
it ends up creating this Us Versus Them

302
00:12:14,700 --> 00:12:16,740
a situation this antagonistic

303
00:12:16,740 --> 00:12:18,660
relationship between Security

304
00:12:18,660 --> 00:12:20,880
Professionals and the people that

305
00:12:20,880 --> 00:12:23,700
they're really supposed to be supporting

306
00:12:23,700 --> 00:12:25,920
and in turn security folks can come

307
00:12:25,920 --> 00:12:27,839
across as being very arrogant and

308
00:12:27,839 --> 00:12:31,440
condescending and in general this type

309
00:12:31,440 --> 00:12:35,519
of attitude takes away user agency it's

310
00:12:35,519 --> 00:12:37,620
the opposite of empowering so instead of

311
00:12:37,620 --> 00:12:39,839
seeing them as as empowered and capable

312
00:12:39,839 --> 00:12:41,100
people

313
00:12:41,100 --> 00:12:43,019
um you know where we're labeling them as

314
00:12:43,019 --> 00:12:45,839
kind of hopeless and Clueless

315
00:12:45,839 --> 00:12:48,360
foreign

316
00:12:48,360 --> 00:12:51,420
so there's been a lot of research on

317
00:12:51,420 --> 00:12:53,880
security non-experts and their

318
00:12:53,880 --> 00:12:56,519
perceptions and actions that really

319
00:12:56,519 --> 00:12:59,279
shows that it it's not that users are

320
00:12:59,279 --> 00:13:01,560
stupid but rather they're often

321
00:13:01,560 --> 00:13:03,959
overwhelmed and they're ill-equipped and

322
00:13:03,959 --> 00:13:06,899
not necessarily through their own fault

323
00:13:06,899 --> 00:13:08,760
um so some of my colleagues at nist

324
00:13:08,760 --> 00:13:11,399
conducted a study a few years ago in

325
00:13:11,399 --> 00:13:12,959
which they interviewed general public

326
00:13:12,959 --> 00:13:15,360
people about their security perceptions

327
00:13:15,360 --> 00:13:18,240
and challenges and actions and they

328
00:13:18,240 --> 00:13:19,920
found that these people were suffering

329
00:13:19,920 --> 00:13:22,079
from something that they called security

330
00:13:22,079 --> 00:13:24,180
fatigue

331
00:13:24,180 --> 00:13:27,380
and so security fatigue is a sense of

332
00:13:27,380 --> 00:13:30,480
resignation and weariness and

333
00:13:30,480 --> 00:13:32,760
frustration or loss of control and

334
00:13:32,760 --> 00:13:35,639
people's responses to security and they

335
00:13:35,639 --> 00:13:39,120
identified as several reasons why people

336
00:13:39,120 --> 00:13:42,360
were suffering from the security fatigue

337
00:13:42,360 --> 00:13:44,700
so first of all for most people when

338
00:13:44,700 --> 00:13:47,160
they sit down at a computer security is

339
00:13:47,160 --> 00:13:51,000
not their primary task so security tasks

340
00:13:51,000 --> 00:13:53,639
can be quite disruptive to what is their

341
00:13:53,639 --> 00:13:56,160
primary task so they have to go through

342
00:13:56,160 --> 00:13:58,560
multiple steps to authenticate they have

343
00:13:58,560 --> 00:14:00,300
to deal with these security pop-up

344
00:14:00,300 --> 00:14:01,800
warnings

345
00:14:01,800 --> 00:14:04,200
and then an organizational contexts

346
00:14:04,200 --> 00:14:06,660
people might think that security is

347
00:14:06,660 --> 00:14:09,180
someone else's responsibility right it's

348
00:14:09,180 --> 00:14:11,880
those I.T guys the security nerds

349
00:14:11,880 --> 00:14:13,079
they're the ones that are supposed to be

350
00:14:13,079 --> 00:14:14,820
protecting me so I don't have to worry

351
00:14:14,820 --> 00:14:16,860
about it

352
00:14:16,860 --> 00:14:20,100
secondly most people are just not

353
00:14:20,100 --> 00:14:22,320
experts in this

354
00:14:22,320 --> 00:14:24,300
um they they don't necessarily

355
00:14:24,300 --> 00:14:26,339
understand the terminology they can't

356
00:14:26,339 --> 00:14:27,899
decipher

357
00:14:27,899 --> 00:14:31,220
um terms uh like the highly technical

358
00:14:31,220 --> 00:14:33,959
language and jargon that we tend to use

359
00:14:33,959 --> 00:14:36,300
and as security people we may have

360
00:14:36,300 --> 00:14:38,820
unrealistic expectations on what they

361
00:14:38,820 --> 00:14:41,220
understand and how well they can make

362
00:14:41,220 --> 00:14:43,380
sound decisions

363
00:14:43,380 --> 00:14:46,320
and then finally cognitive biases may

364
00:14:46,320 --> 00:14:48,600
come into play so for example people

365
00:14:48,600 --> 00:14:51,600
might suffer from an optimism bias where

366
00:14:51,600 --> 00:14:53,279
they might think well no one would want

367
00:14:53,279 --> 00:14:55,500
to Target me I'm not that interesting

368
00:14:55,500 --> 00:14:57,180
um we actually saw this quite a bit in

369
00:14:57,180 --> 00:14:59,940
our smart home research or an

370
00:14:59,940 --> 00:15:02,459
availability bias is also pretty common

371
00:15:02,459 --> 00:15:04,139
um where someone might think well I

372
00:15:04,139 --> 00:15:05,699
can't recall anything bad happening

373
00:15:05,699 --> 00:15:07,320
recently so I don't need to worry as

374
00:15:07,320 --> 00:15:08,220
much

375
00:15:08,220 --> 00:15:10,980
so these are all reasons why people are

376
00:15:10,980 --> 00:15:13,380
suffering from security fatigue and it's

377
00:15:13,380 --> 00:15:17,300
not that they're stupid or hopeless

378
00:15:18,240 --> 00:15:21,120
so some tips to overturn Pitfall number

379
00:15:21,120 --> 00:15:23,279
one

380
00:15:23,279 --> 00:15:24,779
first of all

381
00:15:24,779 --> 00:15:26,699
um I think it requires a shift in

382
00:15:26,699 --> 00:15:28,079
Attitude

383
00:15:28,079 --> 00:15:30,420
so instead of the blame game and

384
00:15:30,420 --> 00:15:33,380
scapegoating these non-expert users

385
00:15:33,380 --> 00:15:36,839
focus on empowering them to be active

386
00:15:36,839 --> 00:15:39,779
Partners in security and we need to take

387
00:15:39,779 --> 00:15:42,899
a bit of a step back to understand why

388
00:15:42,899 --> 00:15:45,540
people may be struggling it's not that

389
00:15:45,540 --> 00:15:47,279
they're bad it's not that they're stupid

390
00:15:47,279 --> 00:15:50,399
there's some real root cause for why

391
00:15:50,399 --> 00:15:53,600
they're having a challenge

392
00:15:54,060 --> 00:15:56,760
second building relationships and

393
00:15:56,760 --> 00:15:59,880
practicing empathy I think unfortunately

394
00:15:59,880 --> 00:16:01,920
empathy is something that tends to be

395
00:16:01,920 --> 00:16:05,040
lacking in the security Community we

396
00:16:05,040 --> 00:16:07,019
need to develop this definitely more

397
00:16:07,019 --> 00:16:09,839
move Beyond and US versus them type of

398
00:16:09,839 --> 00:16:13,139
mentality realize we're all human we're

399
00:16:13,139 --> 00:16:15,240
all influenced by our experiences goals

400
00:16:15,240 --> 00:16:17,279
and expertise

401
00:16:17,279 --> 00:16:19,680
and having some kind of positive

402
00:16:19,680 --> 00:16:21,600
relationship building that the users

403
00:16:21,600 --> 00:16:24,480
were supporting not only establishes

404
00:16:24,480 --> 00:16:26,579
Security Professionals own credibility

405
00:16:26,579 --> 00:16:28,860
and commitment but also helps them to

406
00:16:28,860 --> 00:16:31,019
better understand people's needs and

407
00:16:31,019 --> 00:16:33,420
their perspectives

408
00:16:33,420 --> 00:16:35,300
foreign

409
00:16:35,300 --> 00:16:39,360
Pitfall number two is not tailoring

410
00:16:39,360 --> 00:16:42,720
security Communications to the audience

411
00:16:42,720 --> 00:16:44,459
so Security Professionals have to

412
00:16:44,459 --> 00:16:46,920
communicate security information all the

413
00:16:46,920 --> 00:16:49,380
time so for example they may have to let

414
00:16:49,380 --> 00:16:51,420
people in the organization know about a

415
00:16:51,420 --> 00:16:54,240
new security policy or process they

416
00:16:54,240 --> 00:16:55,440
might be disseminating security

417
00:16:55,440 --> 00:16:58,380
awareness information or even trying to

418
00:16:58,380 --> 00:17:00,300
convince their leadership to invest more

419
00:17:00,300 --> 00:17:02,160
in security

420
00:17:02,160 --> 00:17:04,500
but security folks might have a really

421
00:17:04,500 --> 00:17:06,720
hard time putting themselves in other

422
00:17:06,720 --> 00:17:08,939
people's shoes and might suffer from

423
00:17:08,939 --> 00:17:10,919
something called The Curse of knowledge

424
00:17:10,919 --> 00:17:14,040
and so this is about how people who are

425
00:17:14,040 --> 00:17:15,780
experts in a field have a really

426
00:17:15,780 --> 00:17:18,839
difficult time explaining that field to

427
00:17:18,839 --> 00:17:20,939
non-experts and so Security

428
00:17:20,939 --> 00:17:22,799
Professionals might have a difficult

429
00:17:22,799 --> 00:17:25,919
time translating this highly technical

430
00:17:25,919 --> 00:17:28,980
information and jargon that that we use

431
00:17:28,980 --> 00:17:31,260
so often into a language that's

432
00:17:31,260 --> 00:17:33,120
understandable to their intended

433
00:17:33,120 --> 00:17:35,400
audience

434
00:17:35,400 --> 00:17:37,620
um they may also fail to account for

435
00:17:37,620 --> 00:17:39,020
differences

436
00:17:39,020 --> 00:17:41,700
within their audience um differences in

437
00:17:41,700 --> 00:17:43,559
people's security motivations their

438
00:17:43,559 --> 00:17:46,679
needs their knowledge or they may not

439
00:17:46,679 --> 00:17:48,299
appeal to what Their audience actually

440
00:17:48,299 --> 00:17:50,700
cares about in their day-to-day work in

441
00:17:50,700 --> 00:17:52,080
their lives

442
00:17:52,080 --> 00:17:53,760
um so for example I've and I've seen

443
00:17:53,760 --> 00:17:56,640
this happen quite a few times when

444
00:17:56,640 --> 00:17:59,460
talking to senior leadership or

445
00:17:59,460 --> 00:18:02,400
Executives security folks

446
00:18:02,400 --> 00:18:04,559
trying to explain a security concept

447
00:18:04,559 --> 00:18:07,980
tend to go very deep into the weeds and

448
00:18:07,980 --> 00:18:09,600
not provide the information that the

449
00:18:09,600 --> 00:18:11,520
executives actually need to make

450
00:18:11,520 --> 00:18:13,679
informed decisions so they they're just

451
00:18:13,679 --> 00:18:16,880
not speaking their language

452
00:18:18,240 --> 00:18:21,000
um so this is an example from my own

453
00:18:21,000 --> 00:18:22,620
um personal

454
00:18:22,620 --> 00:18:24,240
um background

455
00:18:24,240 --> 00:18:27,900
um so when I worked for DOD I started

456
00:18:27,900 --> 00:18:29,760
off doing network security evaluations

457
00:18:29,760 --> 00:18:31,799
so we would be invited by an

458
00:18:31,799 --> 00:18:33,000
organization

459
00:18:33,000 --> 00:18:35,580
um to go and basically find as many

460
00:18:35,580 --> 00:18:37,860
vulnerabilities as we could on their

461
00:18:37,860 --> 00:18:41,160
networks and at the end of our of our

462
00:18:41,160 --> 00:18:42,780
time on site we would give an out

463
00:18:42,780 --> 00:18:44,940
briefing and then about a month later we

464
00:18:44,940 --> 00:18:46,500
would deliver

465
00:18:46,500 --> 00:18:49,380
um a a thick and I'm not I'm not joking

466
00:18:49,380 --> 00:18:52,620
when I say 100 plus page report that

467
00:18:52,620 --> 00:18:54,360
included a description of every

468
00:18:54,360 --> 00:18:56,520
vulnerability we found what systems were

469
00:18:56,520 --> 00:18:58,679
affected and then recommendations on how

470
00:18:58,679 --> 00:19:00,660
to fix those issues

471
00:19:00,660 --> 00:19:03,240
and the report was very much geared

472
00:19:03,240 --> 00:19:05,280
toward the technical folks

473
00:19:05,280 --> 00:19:07,559
but then we started noticing that we

474
00:19:07,559 --> 00:19:09,840
would sometimes do repeat visits to the

475
00:19:09,840 --> 00:19:12,600
same site maybe a year or two later and

476
00:19:12,600 --> 00:19:15,000
they didn't always

477
00:19:15,000 --> 00:19:16,919
take our recommendations into account

478
00:19:16,919 --> 00:19:19,140
that things were often very much the

479
00:19:19,140 --> 00:19:21,900
same as as when we were there previously

480
00:19:21,900 --> 00:19:24,000
and so we really started thinking about

481
00:19:24,000 --> 00:19:27,419
well how can we make more of an impact

482
00:19:27,419 --> 00:19:30,240
and so we started looking at our reports

483
00:19:30,240 --> 00:19:32,940
and realized that what we were providing

484
00:19:32,940 --> 00:19:35,700
was not tailored for the people who

485
00:19:35,700 --> 00:19:38,460
actually made the decisions about where

486
00:19:38,460 --> 00:19:41,580
resources might be allocated

487
00:19:41,580 --> 00:19:44,640
um so those decision makers so we

488
00:19:44,640 --> 00:19:46,740
changed the way we did the reports we

489
00:19:46,740 --> 00:19:48,480
moved toward a shorter report format

490
00:19:48,480 --> 00:19:50,820
that resonated more with those decision

491
00:19:50,820 --> 00:19:52,860
makers and then just included the

492
00:19:52,860 --> 00:19:55,080
specifics in an appendix

493
00:19:55,080 --> 00:19:57,059
so in the main report we started using

494
00:19:57,059 --> 00:19:59,520
less technical language

495
00:19:59,520 --> 00:20:02,220
um and and information that could help

496
00:20:02,220 --> 00:20:04,160
with decision making like

497
00:20:04,160 --> 00:20:06,480
prioritizations severity of the

498
00:20:06,480 --> 00:20:08,880
vulnerabilities consequences and

499
00:20:08,880 --> 00:20:10,740
estimates of how much effort it would

500
00:20:10,740 --> 00:20:13,740
take to fix things

501
00:20:13,740 --> 00:20:15,419
foreign

502
00:20:15,419 --> 00:20:18,960
so overturning this pitfall

503
00:20:18,960 --> 00:20:21,600
so be context aware which goes back to

504
00:20:21,600 --> 00:20:23,700
that whole context of use that we talked

505
00:20:23,700 --> 00:20:26,220
about in the usability definition so

506
00:20:26,220 --> 00:20:28,799
really taking time to step back and

507
00:20:28,799 --> 00:20:31,919
understand who are your users what are

508
00:20:31,919 --> 00:20:33,900
their skill levels what do they care

509
00:20:33,900 --> 00:20:36,240
about what are the environments in which

510
00:20:36,240 --> 00:20:37,740
they operate

511
00:20:37,740 --> 00:20:40,980
and then being a translator

512
00:20:40,980 --> 00:20:41,760
um

513
00:20:41,760 --> 00:20:43,559
there's a lot to be learned actually

514
00:20:43,559 --> 00:20:45,960
from the risk communication field

515
00:20:45,960 --> 00:20:47,820
um the the health field the medical

516
00:20:47,820 --> 00:20:50,220
field has been doing this kind of risk

517
00:20:50,220 --> 00:20:53,640
communication for a long time and it

518
00:20:53,640 --> 00:20:55,679
talks a lot about tailoring those

519
00:20:55,679 --> 00:20:58,020
Communications to the intended audience

520
00:20:58,020 --> 00:21:00,840
making the guidance digestible and

521
00:21:00,840 --> 00:21:03,840
achievable for that audience and you

522
00:21:03,840 --> 00:21:05,820
often may need some kind of scaffolding

523
00:21:05,820 --> 00:21:08,280
to explain basic security concepts for

524
00:21:08,280 --> 00:21:10,260
some audiences

525
00:21:10,260 --> 00:21:13,080
making a personal connection can also

526
00:21:13,080 --> 00:21:15,720
really help especially with memorability

527
00:21:15,720 --> 00:21:18,840
so things like storytelling

528
00:21:18,840 --> 00:21:21,780
um referencing and recent events can all

529
00:21:21,780 --> 00:21:25,380
help overcome cognitive biases

530
00:21:25,380 --> 00:21:28,320
and then communicating not just you

531
00:21:28,320 --> 00:21:30,780
should do this but why should you do

532
00:21:30,780 --> 00:21:33,480
this so how does it impact someone's job

533
00:21:33,480 --> 00:21:36,780
or Mission why should they care and

534
00:21:36,780 --> 00:21:40,440
often if it's appropriate explaining

535
00:21:40,440 --> 00:21:42,720
non-security benefits in addition to the

536
00:21:42,720 --> 00:21:44,520
security benefits can really help

537
00:21:44,520 --> 00:21:47,400
motivate people

538
00:21:47,400 --> 00:21:51,539
next mix it up use different formats to

539
00:21:51,539 --> 00:21:53,820
disseminate security information because

540
00:21:53,820 --> 00:21:56,520
people have different preferences for

541
00:21:56,520 --> 00:21:59,220
how they receive information so for

542
00:21:59,220 --> 00:22:01,020
example a lot of security Communications

543
00:22:01,020 --> 00:22:04,740
get sent via email it's easy to do but

544
00:22:04,740 --> 00:22:06,720
how many people are just totally

545
00:22:06,720 --> 00:22:08,580
overwhelmed by email and just end up

546
00:22:08,580 --> 00:22:11,280
deleting it some people are more visual

547
00:22:11,280 --> 00:22:14,460
some people want things that are more in

548
00:22:14,460 --> 00:22:17,520
person or interactive so using a variety

549
00:22:17,520 --> 00:22:20,039
of different methods can can really help

550
00:22:20,039 --> 00:22:22,159
to get the word out

551
00:22:22,159 --> 00:22:25,919
and then finally enlist help so most of

552
00:22:25,919 --> 00:22:28,860
us are not Communications experts but in

553
00:22:28,860 --> 00:22:30,659
our organizations there probably are

554
00:22:30,659 --> 00:22:33,059
some communication experts

555
00:22:33,059 --> 00:22:34,919
um so reach out to them for help

556
00:22:34,919 --> 00:22:36,659
especially when you're crafting a

557
00:22:36,659 --> 00:22:38,520
message that you really want people to

558
00:22:38,520 --> 00:22:41,299
pay attention to

559
00:22:42,600 --> 00:22:45,179
all right Pitfall number three is

560
00:22:45,179 --> 00:22:47,640
unintentionally creating Insider threats

561
00:22:47,640 --> 00:22:50,280
due to poor usability

562
00:22:50,280 --> 00:22:53,640
so solutions that focus on security

563
00:22:53,640 --> 00:22:56,940
without considering usability can

564
00:22:56,940 --> 00:22:59,880
backfire so in a lot of environments

565
00:22:59,880 --> 00:23:01,980
users are already pushed to their limits

566
00:23:01,980 --> 00:23:05,280
they have a lot of time pressure they're

567
00:23:05,280 --> 00:23:07,620
juggling multiple tasks they have other

568
00:23:07,620 --> 00:23:10,679
distractions so unusable security can

569
00:23:10,679 --> 00:23:14,580
really increase user burden and that can

570
00:23:14,580 --> 00:23:17,220
result in the unwitting creation of

571
00:23:17,220 --> 00:23:20,580
Insider threats so users who are totally

572
00:23:20,580 --> 00:23:23,039
frustrated with security and they're

573
00:23:23,039 --> 00:23:25,020
more prone to making errors or making

574
00:23:25,020 --> 00:23:27,480
risky decisions and more likely to try

575
00:23:27,480 --> 00:23:31,100
less secure workarounds

576
00:23:31,500 --> 00:23:34,320
so the classic example of unusable

577
00:23:34,320 --> 00:23:37,080
security is passwords

578
00:23:37,080 --> 00:23:39,299
um I I am sure you are all intimately

579
00:23:39,299 --> 00:23:42,059
familiar with um a lot of these password

580
00:23:42,059 --> 00:23:43,679
policies that require a lot of

581
00:23:43,679 --> 00:23:45,480
complexity and you have to change your

582
00:23:45,480 --> 00:23:47,100
password frequently and you're not

583
00:23:47,100 --> 00:23:49,679
supposed to reuse your password and

584
00:23:49,679 --> 00:23:51,480
especially now that people have so many

585
00:23:51,480 --> 00:23:54,120
different accounts to maintain all of

586
00:23:54,120 --> 00:23:56,220
this complexity just really pushes them

587
00:23:56,220 --> 00:23:59,820
beyond their limits so to cope they

588
00:23:59,820 --> 00:24:01,799
resorted to practices that actually end

589
00:24:01,799 --> 00:24:04,880
up hampering security so for example

590
00:24:04,880 --> 00:24:07,020
writing their passwords in an

591
00:24:07,020 --> 00:24:08,700
unencrypted text file on their computer

592
00:24:08,700 --> 00:24:10,860
or reusing the same password across

593
00:24:10,860 --> 00:24:13,699
multiple accounts

594
00:24:13,919 --> 00:24:15,659
um another example

595
00:24:15,659 --> 00:24:18,179
um this is just one of the best stories

596
00:24:18,179 --> 00:24:19,500
I've heard

597
00:24:19,500 --> 00:24:23,580
um was in a reader submitted article in

598
00:24:23,580 --> 00:24:25,860
an IEEE newsletter I guess about a year

599
00:24:25,860 --> 00:24:27,480
ago now

600
00:24:27,480 --> 00:24:29,400
um so this reader wrote in and he he

601
00:24:29,400 --> 00:24:30,900
talked about how he worked in an

602
00:24:30,900 --> 00:24:33,419
organization that had mandated a

603
00:24:33,419 --> 00:24:35,400
screensaver kick in after five minutes

604
00:24:35,400 --> 00:24:37,880
of that inactivity for security purposes

605
00:24:37,880 --> 00:24:40,320
and this organization had a lot of

606
00:24:40,320 --> 00:24:41,700
scientists

607
00:24:41,700 --> 00:24:43,919
and he was a scientist and he said as a

608
00:24:43,919 --> 00:24:46,320
scientist he was often reading papers or

609
00:24:46,320 --> 00:24:48,780
doing other non-computer related tasks

610
00:24:48,780 --> 00:24:51,240
at his desk so the screen saver was

611
00:24:51,240 --> 00:24:53,460
activating many times throughout the day

612
00:24:53,460 --> 00:24:56,039
requiring him to re-authenticate each

613
00:24:56,039 --> 00:24:56,880
time

614
00:24:56,880 --> 00:24:59,220
so he became very frustrated with this

615
00:24:59,220 --> 00:25:02,940
and he decided that he was going to come

616
00:25:02,940 --> 00:25:05,760
up with a solution so he devised a

617
00:25:05,760 --> 00:25:07,440
method to automatically move the

618
00:25:07,440 --> 00:25:10,080
computer mouse to avoid the lockout

619
00:25:10,080 --> 00:25:13,679
so he ended up installing a watch with

620
00:25:13,679 --> 00:25:15,780
the sweep second hand under the mouse

621
00:25:15,780 --> 00:25:18,780
and the mouse had a motion detector and

622
00:25:18,780 --> 00:25:21,539
he was and it worked great and he was so

623
00:25:21,539 --> 00:25:22,620
proud

624
00:25:22,620 --> 00:25:25,500
that he told his colleagues and they

625
00:25:25,500 --> 00:25:28,320
implemented this great solution too and

626
00:25:28,320 --> 00:25:30,179
at the end of the article he proudly

627
00:25:30,179 --> 00:25:32,940
signed his name and said I got a lot of

628
00:25:32,940 --> 00:25:35,820
satisfaction from this achievement

629
00:25:35,820 --> 00:25:38,340
um so this is an example of the lengths

630
00:25:38,340 --> 00:25:40,679
that users will go to circumvent a

631
00:25:40,679 --> 00:25:43,500
security measure that doesn't take their

632
00:25:43,500 --> 00:25:45,720
needs into account and they are proud of

633
00:25:45,720 --> 00:25:48,000
it too

634
00:25:48,000 --> 00:25:52,440
all right so overturning this pitfall

635
00:25:52,440 --> 00:25:54,659
um first of all conduct some basic

636
00:25:54,659 --> 00:25:57,600
usability testing so you don't have to

637
00:25:57,600 --> 00:25:59,940
be a usability expert you don't have to

638
00:25:59,940 --> 00:26:02,640
conduct a formal usability test you can

639
00:26:02,640 --> 00:26:05,039
just pilot some proposed Solutions with

640
00:26:05,039 --> 00:26:07,159
a few representative users

641
00:26:07,159 --> 00:26:10,860
see where they're having troubles and

642
00:26:10,860 --> 00:26:12,600
then apply what you've learned toward

643
00:26:12,600 --> 00:26:15,120
improving the security solution

644
00:26:15,120 --> 00:26:16,279
foreign

645
00:26:16,279 --> 00:26:18,779
make it actionable

646
00:26:18,779 --> 00:26:20,940
don't just tell people to do things that

647
00:26:20,940 --> 00:26:23,700
are hard actually provide them tools and

648
00:26:23,700 --> 00:26:26,220
achievable guidance to help them make

649
00:26:26,220 --> 00:26:29,460
the right security decisions

650
00:26:29,460 --> 00:26:31,740
um so don't just have a long laundry

651
00:26:31,740 --> 00:26:33,900
list of to-do's with complicated steps

652
00:26:33,900 --> 00:26:35,940
break those recommendations and task

653
00:26:35,940 --> 00:26:40,200
down into manageable prioritized chunks

654
00:26:40,200 --> 00:26:42,419
and then offload the burden when

655
00:26:42,419 --> 00:26:44,940
possible so we all know that there are a

656
00:26:44,940 --> 00:26:46,980
lot of things that computers are much

657
00:26:46,980 --> 00:26:50,039
better at than people are so think about

658
00:26:50,039 --> 00:26:52,200
what can be done to lessen the burden on

659
00:26:52,200 --> 00:26:55,260
end users and put more of the processing

660
00:26:55,260 --> 00:26:59,400
on the back end on Computing systems

661
00:26:59,400 --> 00:27:02,279
so for example can more filtering be

662
00:27:02,279 --> 00:27:04,140
done at the mail server so that fewer

663
00:27:04,140 --> 00:27:07,580
phishing emails get delivered

664
00:27:08,760 --> 00:27:11,340
all right Pitfall number four is having

665
00:27:11,340 --> 00:27:13,740
too much security there is such a thing

666
00:27:13,740 --> 00:27:15,539
as too much security

667
00:27:15,539 --> 00:27:18,120
um and as security people though we tend

668
00:27:18,120 --> 00:27:20,039
to want everything to be as secure as

669
00:27:20,039 --> 00:27:22,620
possible because that's our job but

670
00:27:22,620 --> 00:27:25,340
sometimes that leads us to recommend a

671
00:27:25,340 --> 00:27:27,720
one-size-fits-all approach with the

672
00:27:27,720 --> 00:27:30,059
highest level of security and the most

673
00:27:30,059 --> 00:27:31,740
security tools

674
00:27:31,740 --> 00:27:35,100
but the most Secure Solutions might not

675
00:27:35,100 --> 00:27:37,440
be practical or necessary in every

676
00:27:37,440 --> 00:27:39,419
context and may end up having

677
00:27:39,419 --> 00:27:42,779
unanticipated consequences for users

678
00:27:42,779 --> 00:27:45,779
in addition research has shown that end

679
00:27:45,779 --> 00:27:48,299
users use stringent security measures as

680
00:27:48,299 --> 00:27:52,080
being counterproductive to their jobs

681
00:27:52,080 --> 00:27:55,740
also we have users being our it and

682
00:27:55,740 --> 00:27:58,020
security staff so overly complicated

683
00:27:58,020 --> 00:28:00,179
Security Solutions can negatively impact

684
00:28:00,179 --> 00:28:02,820
them as well

685
00:28:02,820 --> 00:28:05,279
and so an example of that

686
00:28:05,279 --> 00:28:08,159
um again from my my own um my own

687
00:28:08,159 --> 00:28:10,980
background my own past

688
00:28:10,980 --> 00:28:13,080
um when I was uh in the organization

689
00:28:13,080 --> 00:28:14,880
doing network security evaluations I

690
00:28:14,880 --> 00:28:16,380
also wrote

691
00:28:16,380 --> 00:28:18,600
um a lot of security guidance on

692
00:28:18,600 --> 00:28:21,659
Microsoft Windows systems

693
00:28:21,659 --> 00:28:24,480
and in those early days we recommended

694
00:28:24,480 --> 00:28:27,720
the most secure configurations and we

695
00:28:27,720 --> 00:28:29,820
tested them in the lab and they worked

696
00:28:29,820 --> 00:28:32,279
great but we really didn't anticipate

697
00:28:32,279 --> 00:28:33,900
that our recommendations might cause

698
00:28:33,900 --> 00:28:36,419
some serious issues in operational

699
00:28:36,419 --> 00:28:38,340
environments and end up putting

700
00:28:38,340 --> 00:28:41,100
additional burden on the people we were

701
00:28:41,100 --> 00:28:43,919
ultimately trying to help

702
00:28:43,919 --> 00:28:46,080
um so the example here that stood out

703
00:28:46,080 --> 00:28:48,900
for me was Windows event logs

704
00:28:48,900 --> 00:28:51,000
so logs are great from a security

705
00:28:51,000 --> 00:28:53,039
perspective for seeing potentially

706
00:28:53,039 --> 00:28:54,900
suspicious activities

707
00:28:54,900 --> 00:28:57,900
and now this is a while ago so my my

708
00:28:57,900 --> 00:29:00,000
knowledge of what's what the options are

709
00:29:00,000 --> 00:29:02,880
in Windows ferment logs is very dated

710
00:29:02,880 --> 00:29:05,159
but back then

711
00:29:05,159 --> 00:29:09,059
um you could choose not to log which we

712
00:29:09,059 --> 00:29:10,860
viewed as not being a good option from a

713
00:29:10,860 --> 00:29:12,480
security perspective

714
00:29:12,480 --> 00:29:14,940
or you could turn on logging with

715
00:29:14,940 --> 00:29:17,460
options to overwrite the logs as needed

716
00:29:17,460 --> 00:29:20,039
or you could configure the logs to not

717
00:29:20,039 --> 00:29:21,299
overwrite

718
00:29:21,299 --> 00:29:23,880
so again from a security perspective we

719
00:29:23,880 --> 00:29:25,860
didn't want the logs to be overwritten

720
00:29:25,860 --> 00:29:27,299
because we felt like well you know

721
00:29:27,299 --> 00:29:28,919
there's valuable information that could

722
00:29:28,919 --> 00:29:31,260
be lost so we ended up taking the

723
00:29:31,260 --> 00:29:33,899
Hardline approach and recommend that

724
00:29:33,899 --> 00:29:37,440
administrators do not uh set the logs to

725
00:29:37,440 --> 00:29:39,659
do not overwrite and that they would

726
00:29:39,659 --> 00:29:41,940
periodically save off the logs to a

727
00:29:41,940 --> 00:29:43,380
backup server before they actually

728
00:29:43,380 --> 00:29:45,899
filled up so ideally we thought people

729
00:29:45,899 --> 00:29:48,779
should automate that process

730
00:29:48,779 --> 00:29:51,480
so what ended up happening well the

731
00:29:51,480 --> 00:29:53,460
administrators weren't great at saving

732
00:29:53,460 --> 00:29:56,340
the logs off somewhere else so the logs

733
00:29:56,340 --> 00:29:57,720
filled up

734
00:29:57,720 --> 00:30:00,899
and when the logs filled up it turned

735
00:30:00,899 --> 00:30:03,419
out you got the dreaded blue screen of

736
00:30:03,419 --> 00:30:05,520
death you couldn't do anything until

737
00:30:05,520 --> 00:30:08,600
administrator manually cleared the logs

738
00:30:08,600 --> 00:30:11,159
essentially a self-imposed denial of

739
00:30:11,159 --> 00:30:13,740
service with lots of calls to help desk

740
00:30:13,740 --> 00:30:15,659
um I don't even know I I guess the blue

741
00:30:15,659 --> 00:30:17,220
screen of death still exists I haven't

742
00:30:17,220 --> 00:30:18,720
seen it in a long time which I guess is

743
00:30:18,720 --> 00:30:20,940
a good thing but this was like you know

744
00:30:20,940 --> 00:30:23,100
like it but it would just freeze like it

745
00:30:23,100 --> 00:30:24,299
you couldn't

746
00:30:24,299 --> 00:30:26,700
you couldn't you know rebooting wouldn't

747
00:30:26,700 --> 00:30:29,100
help so you had to have an administrator

748
00:30:29,100 --> 00:30:32,700
physically go to each system to clear it

749
00:30:32,700 --> 00:30:34,860
and so while this was a nuisance for an

750
00:30:34,860 --> 00:30:37,440
end user workstation it was considerably

751
00:30:37,440 --> 00:30:39,960
more impactful when it happened to the

752
00:30:39,960 --> 00:30:40,919
server

753
00:30:40,919 --> 00:30:43,860
so our attempt at recommending the most

754
00:30:43,860 --> 00:30:46,559
secure solution ended up adding much

755
00:30:46,559 --> 00:30:49,500
more burden for both administrators and

756
00:30:49,500 --> 00:30:52,399
the end users

757
00:30:53,700 --> 00:30:56,940
all right so overturning this pitfall

758
00:30:56,940 --> 00:30:59,460
um so first of all take a risk-based

759
00:30:59,460 --> 00:31:00,899
approach

760
00:31:00,899 --> 00:31:03,779
um avoid these kind of one-size-fits all

761
00:31:03,779 --> 00:31:05,820
most Secure Solutions and tailor the

762
00:31:05,820 --> 00:31:07,860
solutions to what is appropriate for

763
00:31:07,860 --> 00:31:10,860
that particular environment not every

764
00:31:10,860 --> 00:31:14,580
environment requires ultra high security

765
00:31:14,580 --> 00:31:17,220
and then be sure that you're supporting

766
00:31:17,220 --> 00:31:19,860
the capability of the users both the

767
00:31:19,860 --> 00:31:23,340
technical users and the end users so try

768
00:31:23,340 --> 00:31:25,860
not to put too much burden on your

769
00:31:25,860 --> 00:31:27,240
technical staff

770
00:31:27,240 --> 00:31:29,340
um try to select interoperable Solutions

771
00:31:29,340 --> 00:31:32,340
and increase automation to reduce

772
00:31:32,340 --> 00:31:34,799
complexity and increase usability for

773
00:31:34,799 --> 00:31:35,640
them

774
00:31:35,640 --> 00:31:37,620
and then from an end user perspective

775
00:31:37,620 --> 00:31:39,899
try to understand the current

776
00:31:39,899 --> 00:31:41,760
constraints and stresses of those end

777
00:31:41,760 --> 00:31:45,240
users and how adding additional or more

778
00:31:45,240 --> 00:31:47,100
stringent security processes might

779
00:31:47,100 --> 00:31:50,959
negatively impact their daily work

780
00:31:51,899 --> 00:31:54,899
all right Pitfall number five is when

781
00:31:54,899 --> 00:31:57,600
organizations default to using punitive

782
00:31:57,600 --> 00:31:59,640
measures and focusing on negative

783
00:31:59,640 --> 00:32:01,740
messaging to get users to take

784
00:32:01,740 --> 00:32:04,440
recommended security actions

785
00:32:04,440 --> 00:32:07,740
so we've already talked about how people

786
00:32:07,740 --> 00:32:09,600
have a lot of challenges when

787
00:32:09,600 --> 00:32:11,640
interacting with Security Solutions due

788
00:32:11,640 --> 00:32:14,520
to lack of usability and and they don't

789
00:32:14,520 --> 00:32:16,740
always have the knowledge

790
00:32:16,740 --> 00:32:18,960
but yet Security Professionals might

791
00:32:18,960 --> 00:32:21,720
hold unrealistic expectations that users

792
00:32:21,720 --> 00:32:23,880
should always make good decisions and

793
00:32:23,880 --> 00:32:27,059
then they punish them when they don't

794
00:32:27,059 --> 00:32:29,100
so while punitive measures can be

795
00:32:29,100 --> 00:32:31,679
appropriate in some situations in other

796
00:32:31,679 --> 00:32:34,320
situations focusing too much on the

797
00:32:34,320 --> 00:32:36,059
negative consequences might be

798
00:32:36,059 --> 00:32:38,700
counterproductive and there's been a

799
00:32:38,700 --> 00:32:40,620
fair amount of usable security research

800
00:32:40,620 --> 00:32:43,740
about fear appeals which is when you're

801
00:32:43,740 --> 00:32:45,899
trying to scare people into taking some

802
00:32:45,899 --> 00:32:48,480
kind of action by emphasizing the

803
00:32:48,480 --> 00:32:51,299
potential negative outcomes

804
00:32:51,299 --> 00:32:54,179
but these fear appeals while they may

805
00:32:54,179 --> 00:32:56,520
have some short-term behavioral effects

806
00:32:56,520 --> 00:32:59,820
they may ultimately elicit longer-term

807
00:32:59,820 --> 00:33:02,460
negative emotions towards security

808
00:33:02,460 --> 00:33:05,340
and most concerning just going directly

809
00:33:05,340 --> 00:33:07,679
to punitive measures may fail to really

810
00:33:07,679 --> 00:33:09,720
consider the root causes and the

811
00:33:09,720 --> 00:33:12,539
motivations between people's actions and

812
00:33:12,539 --> 00:33:14,580
their actual capacity of them to be able

813
00:33:14,580 --> 00:33:18,500
to avoid the incident in the first place

814
00:33:19,799 --> 00:33:21,960
so there's a lot of examples of punitive

815
00:33:21,960 --> 00:33:24,419
measures when it comes to phishing so

816
00:33:24,419 --> 00:33:26,039
most of you are probably familiar with

817
00:33:26,039 --> 00:33:28,980
phishing simulation exercises it's when

818
00:33:28,980 --> 00:33:31,620
organizations send out fake phishing

819
00:33:31,620 --> 00:33:34,019
emails to kind of test their Workforce

820
00:33:34,019 --> 00:33:36,179
do they click do they don't click and if

821
00:33:36,179 --> 00:33:37,919
they click then they they have some kind

822
00:33:37,919 --> 00:33:40,320
of kind of education for them to to tell

823
00:33:40,320 --> 00:33:41,700
them like this is why you shouldn't have

824
00:33:41,700 --> 00:33:43,260
clicked

825
00:33:43,260 --> 00:33:45,899
so I've heard of a company that had a

826
00:33:45,899 --> 00:33:48,240
phishing simulation three clicks and

827
00:33:48,240 --> 00:33:49,980
you're fired policy

828
00:33:49,980 --> 00:33:53,220
another company had a Wall of Shame in

829
00:33:53,220 --> 00:33:55,679
which the names of clickers were posted

830
00:33:55,679 --> 00:33:58,679
in a public area of the the company

831
00:33:58,679 --> 00:34:00,120
office

832
00:34:00,120 --> 00:34:03,120
but the fact is is that anyone can fall

833
00:34:03,120 --> 00:34:07,440
prey to a fish in certain circumstances

834
00:34:07,440 --> 00:34:08,879
and there's been a fair amount of

835
00:34:08,879 --> 00:34:10,980
research why some people click on

836
00:34:10,980 --> 00:34:13,440
phishing emails and why some don't

837
00:34:13,440 --> 00:34:16,020
and a few years ago my group looked at

838
00:34:16,020 --> 00:34:18,359
this problem with fishing stimulation

839
00:34:18,359 --> 00:34:21,239
exercises in our own organization

840
00:34:21,239 --> 00:34:23,760
and they found that in addition to

841
00:34:23,760 --> 00:34:25,859
typical fishing cues like spelling and

842
00:34:25,859 --> 00:34:27,719
grammatic layers or sense of urgency

843
00:34:27,719 --> 00:34:31,199
that the user context was a critical

844
00:34:31,199 --> 00:34:33,719
component so for example if there's a

845
00:34:33,719 --> 00:34:35,520
phishing email about something very

846
00:34:35,520 --> 00:34:38,219
relevant and in line with the duties of

847
00:34:38,219 --> 00:34:40,320
a person in their day-to-day work

848
00:34:40,320 --> 00:34:41,820
they're going to be more likely to click

849
00:34:41,820 --> 00:34:43,379
because they don't want to mess up their

850
00:34:43,379 --> 00:34:46,859
jobs so in the study there was an

851
00:34:46,859 --> 00:34:49,619
example of a financial group that had

852
00:34:49,619 --> 00:34:52,139
recently been late on an invoice and

853
00:34:52,139 --> 00:34:54,060
they had gotten some some negative

854
00:34:54,060 --> 00:34:55,800
feedback on that

855
00:34:55,800 --> 00:34:59,119
and a few weeks later one of the fishing

856
00:34:59,119 --> 00:35:01,980
simulation emails that sent out happened

857
00:35:01,980 --> 00:35:05,099
to be about an unpaid invoice and they

858
00:35:05,099 --> 00:35:06,540
didn't want to be late on another

859
00:35:06,540 --> 00:35:07,859
invoice

860
00:35:07,859 --> 00:35:09,359
um so there were several members of the

861
00:35:09,359 --> 00:35:10,920
group that clicked on the link in the

862
00:35:10,920 --> 00:35:12,599
phishing email

863
00:35:12,599 --> 00:35:13,980
um so yeah there's probably some things

864
00:35:13,980 --> 00:35:15,660
they could have done to kind of Step

865
00:35:15,660 --> 00:35:18,119
through and identify that as phishing

866
00:35:18,119 --> 00:35:19,560
but

867
00:35:19,560 --> 00:35:21,540
um they were really sensitized to these

868
00:35:21,540 --> 00:35:24,180
type of emails coming in and so should

869
00:35:24,180 --> 00:35:27,000
we punish them harshly for really trying

870
00:35:27,000 --> 00:35:29,040
to be diligent in their jobs that might

871
00:35:29,040 --> 00:35:32,700
not be the best best path to go down

872
00:35:32,700 --> 00:35:36,060
so overturning this pitfall

873
00:35:36,060 --> 00:35:38,880
um so to motivate people to take Action

874
00:35:38,880 --> 00:35:41,400
Security Professionals should honestly

875
00:35:41,400 --> 00:35:43,380
definitely honestly communicate with

876
00:35:43,380 --> 00:35:44,880
severity of the threat and the potential

877
00:35:44,880 --> 00:35:47,099
consequences while being careful though

878
00:35:47,099 --> 00:35:49,200
not to overstate these

879
00:35:49,200 --> 00:35:52,680
but in addition to motivation users have

880
00:35:52,680 --> 00:35:54,720
to have confidence in their ability to

881
00:35:54,720 --> 00:35:57,180
do something about the threat which

882
00:35:57,180 --> 00:35:59,940
requires them being provided with

883
00:35:59,940 --> 00:36:02,280
specific instructions and tools because

884
00:36:02,280 --> 00:36:04,380
if people don't feel like their actions

885
00:36:04,380 --> 00:36:06,060
will actually mitigate the threat

886
00:36:06,060 --> 00:36:10,040
they're less likely to choose to act

887
00:36:10,040 --> 00:36:12,660
and then don't rely on fear or

888
00:36:12,660 --> 00:36:14,280
punishment Alone

889
00:36:14,280 --> 00:36:16,980
um I did uh I mentioned I did a um I

890
00:36:16,980 --> 00:36:18,660
don't know if I mentioned I did but I I

891
00:36:18,660 --> 00:36:21,420
did a a study recently about security

892
00:36:21,420 --> 00:36:23,579
awareness programs in the government and

893
00:36:23,579 --> 00:36:26,220
there were several participants that

894
00:36:26,220 --> 00:36:28,920
talked about the importance of positive

895
00:36:28,920 --> 00:36:30,960
reinforcement and recognizing good

896
00:36:30,960 --> 00:36:33,000
security behaviors

897
00:36:33,000 --> 00:36:34,079
um so even

898
00:36:34,079 --> 00:36:35,339
um simple things like they had like

899
00:36:35,339 --> 00:36:37,260
little friendly competitions or they

900
00:36:37,260 --> 00:36:38,700
would give out small Trinkets and

901
00:36:38,700 --> 00:36:40,859
Rewards or even simple thank yous could

902
00:36:40,859 --> 00:36:42,599
really help

903
00:36:42,599 --> 00:36:44,839
and then being more collaborative and

904
00:36:44,839 --> 00:36:47,820
instructive rather than punitive

905
00:36:47,820 --> 00:36:49,920
um there's a book called fishing dark

906
00:36:49,920 --> 00:36:52,700
Waters that talks about one

907
00:36:52,700 --> 00:36:55,680
organization's successful paradigm shift

908
00:36:55,680 --> 00:36:58,079
in dealing with repeat phishing clickers

909
00:36:58,079 --> 00:37:00,480
and they moved from a very punitive

910
00:37:00,480 --> 00:37:01,740
stance to one that was more

911
00:37:01,740 --> 00:37:04,859
collaborative and one-on-one so working

912
00:37:04,859 --> 00:37:06,660
like meeting with people that were

913
00:37:06,660 --> 00:37:08,579
repeat clickers to understand you know

914
00:37:08,579 --> 00:37:10,320
why they were doing this and how they

915
00:37:10,320 --> 00:37:12,180
you know what did they need to be helped

916
00:37:12,180 --> 00:37:13,920
and they really saw marked improvement

917
00:37:13,920 --> 00:37:15,480
in their phishing responses and

918
00:37:15,480 --> 00:37:18,380
Reporting because of that

919
00:37:18,380 --> 00:37:21,839
and our final Pitfall is not considering

920
00:37:21,839 --> 00:37:24,480
user feedback and user-centric measures

921
00:37:24,480 --> 00:37:26,520
of effectiveness

922
00:37:26,520 --> 00:37:28,980
so from a technology perspective we know

923
00:37:28,980 --> 00:37:31,560
that getting good security metrics is a

924
00:37:31,560 --> 00:37:32,940
big challenge

925
00:37:32,940 --> 00:37:34,920
um so for example it's very difficult to

926
00:37:34,920 --> 00:37:38,040
measure security return on investment

927
00:37:38,040 --> 00:37:40,380
and in this technology dominant field

928
00:37:40,380 --> 00:37:43,140
though organizations might not consider

929
00:37:43,140 --> 00:37:45,359
that part of determining the success of

930
00:37:45,359 --> 00:37:47,400
their security efforts should be focused

931
00:37:47,400 --> 00:37:50,099
on Gathering data about how users

932
00:37:50,099 --> 00:37:52,079
behaviors and attitudes are being

933
00:37:52,079 --> 00:37:54,740
impacted

934
00:37:55,980 --> 00:37:58,380
so I mentioned I was studying government

935
00:37:58,380 --> 00:38:00,599
security awareness programs

936
00:38:00,599 --> 00:38:02,579
um in the government we're all required

937
00:38:02,579 --> 00:38:04,260
to complete annual security awareness

938
00:38:04,260 --> 00:38:06,780
training many of you might be required

939
00:38:06,780 --> 00:38:08,579
to complete similar training in your

940
00:38:08,579 --> 00:38:10,500
organization's

941
00:38:10,500 --> 00:38:13,460
and really the purpose of the training

942
00:38:13,460 --> 00:38:17,579
is to compel long-term and positive

943
00:38:17,579 --> 00:38:21,660
security behaviors in the workforce

944
00:38:21,660 --> 00:38:24,060
but in the government and as well as in

945
00:38:24,060 --> 00:38:26,640
other sectors there's a big emphasis on

946
00:38:26,640 --> 00:38:29,880
compliance with these training mandates

947
00:38:29,880 --> 00:38:31,619
um so you know did all employees

948
00:38:31,619 --> 00:38:33,599
complete the training yep then our

949
00:38:33,599 --> 00:38:35,520
security awareness training program is

950
00:38:35,520 --> 00:38:39,060
is a success

951
00:38:39,060 --> 00:38:41,700
and we did in in our study about these

952
00:38:41,700 --> 00:38:43,619
security awareness programs we confirmed

953
00:38:43,619 --> 00:38:46,680
that compliance for the majority was the

954
00:38:46,680 --> 00:38:48,900
most important indicator of success of

955
00:38:48,900 --> 00:38:51,119
their security awareness programs

956
00:38:51,119 --> 00:38:53,160
but the problem is is that compliance

957
00:38:53,160 --> 00:38:55,140
might not actually be effective in

958
00:38:55,140 --> 00:38:58,740
meeting the intended goals of actually

959
00:38:58,740 --> 00:39:01,320
changing employees behaviors and

960
00:39:01,320 --> 00:39:03,060
attitudes

961
00:39:03,060 --> 00:39:05,280
um so we know that employees often find

962
00:39:05,280 --> 00:39:07,079
security awareness training to be a

963
00:39:07,079 --> 00:39:09,480
boring check the box activity they race

964
00:39:09,480 --> 00:39:12,500
through to complete it I know I do

965
00:39:12,500 --> 00:39:15,060
so how much are they really retaining

966
00:39:15,060 --> 00:39:16,740
what are they getting out of it what's

967
00:39:16,740 --> 00:39:20,040
being translated into actual action we

968
00:39:20,040 --> 00:39:22,740
keep seeing the same user error security

969
00:39:22,740 --> 00:39:25,800
incidents year after year but we often

970
00:39:25,800 --> 00:39:28,079
have no idea what the training is

971
00:39:28,079 --> 00:39:30,480
accomplishing we don't know if there's

972
00:39:30,480 --> 00:39:32,099
gaps in the training are there other

973
00:39:32,099 --> 00:39:34,380
areas that we need to address do we need

974
00:39:34,380 --> 00:39:36,420
to do it differently and that's because

975
00:39:36,420 --> 00:39:38,220
we're not consistently measuring

976
00:39:38,220 --> 00:39:40,160
anything to tell us

977
00:39:40,160 --> 00:39:43,079
what what that training is actually

978
00:39:43,079 --> 00:39:45,440
doing

979
00:39:45,780 --> 00:39:48,599
all right so overturning this Pitfall

980
00:39:48,599 --> 00:39:50,460
um first of all you got to gather that

981
00:39:50,460 --> 00:39:53,579
user-centric data focusing on indicators

982
00:39:53,579 --> 00:39:55,680
of people's security attitudes and

983
00:39:55,680 --> 00:39:57,119
behaviors

984
00:39:57,119 --> 00:39:59,700
um so I like to think of um the first we

985
00:39:59,700 --> 00:40:02,779
identify the symptoms

986
00:40:02,880 --> 00:40:04,380
um that you know identifying that there

987
00:40:04,380 --> 00:40:06,300
is a problem to begin with so for

988
00:40:06,300 --> 00:40:08,820
example help desk calls can reveal

989
00:40:08,820 --> 00:40:10,500
things that users are really struggling

990
00:40:10,500 --> 00:40:13,140
with looking at user level security

991
00:40:13,140 --> 00:40:15,119
incidents like phishing clicks or

992
00:40:15,119 --> 00:40:17,640
security violations can inform us about

993
00:40:17,640 --> 00:40:19,680
where users might need more support or

994
00:40:19,680 --> 00:40:22,680
training or even better Solutions

995
00:40:22,680 --> 00:40:25,140
but then once you identify the symptoms

996
00:40:25,140 --> 00:40:28,740
you need to get to the root cause and to

997
00:40:28,740 --> 00:40:31,320
do that you have to understand the

998
00:40:31,320 --> 00:40:32,820
context that the users are working on

999
00:40:32,820 --> 00:40:34,920
and go straight to the source so go to

1000
00:40:34,920 --> 00:40:38,460
the users get their feedback

1001
00:40:38,460 --> 00:40:40,680
this could be done it doesn't have to be

1002
00:40:40,680 --> 00:40:42,720
again super formal or involved it could

1003
00:40:42,720 --> 00:40:45,540
be surveys it could be little focus

1004
00:40:45,540 --> 00:40:47,280
groups or one-on-one or small group

1005
00:40:47,280 --> 00:40:50,339
meetings providing a feedback mechanism

1006
00:40:50,339 --> 00:40:52,140
so that people can anonymously

1007
00:40:52,140 --> 00:40:54,180
communicate their security issues and

1008
00:40:54,180 --> 00:40:56,760
thoughts without fear of appraisal can

1009
00:40:56,760 --> 00:40:58,500
also really help

1010
00:40:58,500 --> 00:41:00,720
and then you need to use the data to

1011
00:41:00,720 --> 00:41:03,839
actually drive improvements and then

1012
00:41:03,839 --> 00:41:06,540
tell people that you took their feedback

1013
00:41:06,540 --> 00:41:08,280
into account

1014
00:41:08,280 --> 00:41:10,200
um and that you use that to help improve

1015
00:41:10,200 --> 00:41:12,060
the solutions because people want to

1016
00:41:12,060 --> 00:41:14,280
feel that they've been heard

1017
00:41:14,280 --> 00:41:15,720
um they want to feel that sense of

1018
00:41:15,720 --> 00:41:18,200
ownership

1019
00:41:19,200 --> 00:41:22,380
all right so those are the pitfalls so

1020
00:41:22,380 --> 00:41:25,740
just some parting thoughts

1021
00:41:25,740 --> 00:41:27,839
um so I really believe that considering

1022
00:41:27,839 --> 00:41:30,300
the human element ultimately leads to

1023
00:41:30,300 --> 00:41:32,520
what should be one of the big goals of a

1024
00:41:32,520 --> 00:41:34,500
security professional and that's

1025
00:41:34,500 --> 00:41:37,859
empowering users to be informed capable

1026
00:41:37,859 --> 00:41:41,099
and active Partners in security rather

1027
00:41:41,099 --> 00:41:44,300
than seeing them as hopeless or stupid

1028
00:41:44,300 --> 00:41:50,099
because security has enough problems in

1029
00:41:50,099 --> 00:41:52,320
which Security Professionals can't solve

1030
00:41:52,320 --> 00:41:55,220
all of those alone we need everyone else

1031
00:41:55,220 --> 00:41:59,099
to be empowered to work towards solving

1032
00:41:59,099 --> 00:42:01,680
some of those problems

1033
00:42:01,680 --> 00:42:03,060
all right

1034
00:42:03,060 --> 00:42:05,940
um with that I thank you very much my

1035
00:42:05,940 --> 00:42:08,160
contact information

1036
00:42:08,160 --> 00:42:11,160
um is on this slide there's also a URL

1037
00:42:11,160 --> 00:42:13,200
and a QR code to where usable cyber

1038
00:42:13,200 --> 00:42:14,880
security website

1039
00:42:14,880 --> 00:42:17,220
um we have all kinds of things on there

1040
00:42:17,220 --> 00:42:20,760
or Publications presentations

1041
00:42:20,760 --> 00:42:23,640
um there's videos on there blogs all

1042
00:42:23,640 --> 00:42:26,220
kinds of things so um I encourage you to

1043
00:42:26,220 --> 00:42:29,040
check it out and uh we always like to

1044
00:42:29,040 --> 00:42:30,900
hear feedback or ideas that you all

1045
00:42:30,900 --> 00:42:34,260
might have and I think we have a few

1046
00:42:34,260 --> 00:42:35,640
minutes

1047
00:42:35,640 --> 00:42:40,200
for questions we do we do thank you so

1048
00:42:40,200 --> 00:42:44,660
much that is a a great reminder I think

1049
00:42:44,660 --> 00:42:49,140
to our audience that the the technology

1050
00:42:49,140 --> 00:42:53,460
is important but so are the people and

1051
00:42:53,460 --> 00:42:58,920
that's really key to improving things

1052
00:42:58,920 --> 00:42:59,819
um

1053
00:42:59,819 --> 00:43:02,160
and it's great to know as well that nist

1054
00:43:02,160 --> 00:43:04,619
has a group continuing to work on these

1055
00:43:04,619 --> 00:43:06,660
issues

1056
00:43:06,660 --> 00:43:09,480
um I'll point out for the students who

1057
00:43:09,480 --> 00:43:12,359
are present and others who listen in on

1058
00:43:12,359 --> 00:43:14,160
this that

1059
00:43:14,160 --> 00:43:16,619
um nist does have

1060
00:43:16,619 --> 00:43:19,740
opportunities for interns and

1061
00:43:19,740 --> 00:43:22,380
occasionally employment

1062
00:43:22,380 --> 00:43:24,599
so if this is an area that interests you

1063
00:43:24,599 --> 00:43:26,760
if you have background in this that

1064
00:43:26,760 --> 00:43:28,859
would be another reason to contact uh

1065
00:43:28,859 --> 00:43:31,280
Julie's group

1066
00:43:31,280 --> 00:43:34,020
they they regularly produce if you're

1067
00:43:34,020 --> 00:43:36,480
not familiar with nist they they produce

1068
00:43:36,480 --> 00:43:38,760
some wonderful documents with guidance

1069
00:43:38,760 --> 00:43:42,180
on a variety of things of interest to us

1070
00:43:42,180 --> 00:43:43,800
or in the area of cyber security and

1071
00:43:43,800 --> 00:43:46,020
human factors but

1072
00:43:46,020 --> 00:43:49,160
they cover all kinds of

1073
00:43:49,160 --> 00:43:52,680
Technologies and policies do evaluations

1074
00:43:52,680 --> 00:43:56,700
for the government in general setting

1075
00:43:56,700 --> 00:43:58,380
standards on things like forensic

1076
00:43:58,380 --> 00:44:01,079
Technologies encryption is certainly a

1077
00:44:01,079 --> 00:44:03,300
big area they also have a very large

1078
00:44:03,300 --> 00:44:05,640
Metrology lab where they do work on time

1079
00:44:05,640 --> 00:44:07,980
and measurement they're the ones who

1080
00:44:07,980 --> 00:44:09,660
have the national standards on those

1081
00:44:09,660 --> 00:44:12,480
things it's a great organization and I

1082
00:44:12,480 --> 00:44:14,280
would encourage you to investigate if

1083
00:44:14,280 --> 00:44:16,980
you are not familiar with them

1084
00:44:16,980 --> 00:44:19,380
uh with that if people have questions

1085
00:44:19,380 --> 00:44:23,700
the Q a is open and that would be the

1086
00:44:23,700 --> 00:44:26,720
place where you should ask

1087
00:44:27,359 --> 00:44:29,480
um

1088
00:44:30,000 --> 00:44:32,760
and there is one right can you see that

1089
00:44:32,760 --> 00:44:36,540
Julia I yeah I can see it

1090
00:44:36,540 --> 00:44:39,240
um oh I'll read it out loud too

1091
00:44:39,240 --> 00:44:40,800
um do you have suggestions for getting

1092
00:44:40,800 --> 00:44:43,680
past the compliance Chasers Tech votes

1093
00:44:43,680 --> 00:44:46,800
Tech folks and c-suite managers are

1094
00:44:46,800 --> 00:44:48,660
equally guilty of this stopping the

1095
00:44:48,660 --> 00:44:51,420
moment they get the check check box on

1096
00:44:51,420 --> 00:44:53,520
an audit pushing everyone to do better

1097
00:44:53,520 --> 00:44:55,800
without alienating humans in the

1098
00:44:55,800 --> 00:44:58,400
organization

1099
00:44:58,619 --> 00:45:00,660
um great question

1100
00:45:00,660 --> 00:45:03,420
um the thing with it's compliance is a

1101
00:45:03,420 --> 00:45:06,720
really tricky issue because

1102
00:45:06,720 --> 00:45:09,420
um yeah I mean we're it's come they have

1103
00:45:09,420 --> 00:45:11,280
to comply it's a mandate they have to do

1104
00:45:11,280 --> 00:45:12,240
it

1105
00:45:12,240 --> 00:45:14,640
um so I'm not surprised that it it gets

1106
00:45:14,640 --> 00:45:17,160
um prioritized

1107
00:45:17,160 --> 00:45:19,859
um you know the thing with compliance is

1108
00:45:19,859 --> 00:45:22,800
it brings us all up to a minimum kind of

1109
00:45:22,800 --> 00:45:24,839
standard or Baseline

1110
00:45:24,839 --> 00:45:29,280
um so it's often better than nothing but

1111
00:45:29,280 --> 00:45:33,780
it's often uh misinterpreted or it's um

1112
00:45:33,780 --> 00:45:37,740
it's done to it doesn't fulfill the

1113
00:45:37,740 --> 00:45:41,460
intent of the compliance to begin with

1114
00:45:41,460 --> 00:45:44,880
um I don't I the suggestions I think

1115
00:45:44,880 --> 00:45:48,660
that I have is about how so it's not

1116
00:45:48,660 --> 00:45:51,720
just about the check box it's about how

1117
00:45:51,720 --> 00:45:54,599
doing better actually can impact

1118
00:45:54,599 --> 00:45:58,440
negative how how can positively impact

1119
00:45:58,440 --> 00:46:00,720
the organization

1120
00:46:00,720 --> 00:46:03,060
um and I think again putting things in

1121
00:46:03,060 --> 00:46:05,460
terms that the decision makers care

1122
00:46:05,460 --> 00:46:06,660
about

1123
00:46:06,660 --> 00:46:08,640
um so it's not just about oh I have to

1124
00:46:08,640 --> 00:46:10,740
check this box but okay if we do this

1125
00:46:10,740 --> 00:46:12,300
then

1126
00:46:12,300 --> 00:46:14,460
um these these positive outcomes could

1127
00:46:14,460 --> 00:46:16,560
happen for the organization or if I

1128
00:46:16,560 --> 00:46:18,480
don't do this or if I only do the

1129
00:46:18,480 --> 00:46:21,960
minimum this is what we're losing out on

1130
00:46:21,960 --> 00:46:24,780
um so again you know looking to see what

1131
00:46:24,780 --> 00:46:26,339
the decision makers in the organization

1132
00:46:26,339 --> 00:46:29,040
value and putting it in those type of

1133
00:46:29,040 --> 00:46:31,079
terms it's not it's definitely not an

1134
00:46:31,079 --> 00:46:32,819
easy

1135
00:46:32,819 --> 00:46:34,800
um and easing thing to solve it's like

1136
00:46:34,800 --> 00:46:36,900
yeah compliance gives us better you know

1137
00:46:36,900 --> 00:46:38,760
more than nothing but you know there's

1138
00:46:38,760 --> 00:46:40,800
pitfalls with that as well

1139
00:46:40,800 --> 00:46:44,520
if I may Julie let me suggest uh um

1140
00:46:44,520 --> 00:46:46,740
amplification of that

1141
00:46:46,740 --> 00:46:51,359
most compliance is centered around the

1142
00:46:51,359 --> 00:46:54,359
minimal necessary the the minimal

1143
00:46:54,359 --> 00:46:57,420
considered to be base protection

1144
00:46:57,420 --> 00:47:00,480
and if you're happy with

1145
00:47:00,480 --> 00:47:02,819
only doing the minimum

1146
00:47:02,819 --> 00:47:05,460
then uh

1147
00:47:05,460 --> 00:47:08,400
sure compliance is enough but if you

1148
00:47:08,400 --> 00:47:10,200
actually want to have Excellence if you

1149
00:47:10,200 --> 00:47:13,619
want to actually as as Julie said if you

1150
00:47:13,619 --> 00:47:16,920
want to actually do better and not miss

1151
00:47:16,920 --> 00:47:18,660
out on opportunities

1152
00:47:18,660 --> 00:47:21,599
then compliance alone is not enough you

1153
00:47:21,599 --> 00:47:23,579
actually have to understand the deeper

1154
00:47:23,579 --> 00:47:25,920
issues and do better

1155
00:47:25,920 --> 00:47:29,160
absolutely well said that's a great

1156
00:47:29,160 --> 00:47:32,460
I think I think we're we're both both in

1157
00:47:32,460 --> 00:47:34,200
line on that but yeah

1158
00:47:34,200 --> 00:47:36,900
uh and and that comes from experience we

1159
00:47:36,900 --> 00:47:38,579
both different experience different

1160
00:47:38,579 --> 00:47:41,040
audiences but but

1161
00:47:41,040 --> 00:47:42,900
it's very obvious if you've been out

1162
00:47:42,900 --> 00:47:46,220
there for a while yep

1163
00:47:46,260 --> 00:47:48,980
um other questions

1164
00:47:52,859 --> 00:47:54,960
I think it was such a clear presentation

1165
00:47:54,960 --> 00:47:57,660
that that's fine I think we're out of

1166
00:47:57,660 --> 00:47:58,980
time anyway

1167
00:47:58,980 --> 00:48:02,180
yeah well it's

1168
00:48:02,280 --> 00:48:04,260
it's also the case that is the last week

1169
00:48:04,260 --> 00:48:07,440
of classes uh there's a certain amount

1170
00:48:07,440 --> 00:48:10,319
of shell shock for many of our students

1171
00:48:10,319 --> 00:48:12,839
good good luck to you all with your

1172
00:48:12,839 --> 00:48:14,940
finals and everything all the students

1173
00:48:14,940 --> 00:48:17,339
that are on and and all the The Faculty

1174
00:48:17,339 --> 00:48:19,380
good luck with with your end of the

1175
00:48:19,380 --> 00:48:20,700
semester too

1176
00:48:20,700 --> 00:48:22,680
it is uh

1177
00:48:22,680 --> 00:48:25,980
it is a kind of a stressful uh 10 days

1178
00:48:25,980 --> 00:48:29,940
from here uh to the end but

1179
00:48:29,940 --> 00:48:31,980
um again let me tell everyone listening

1180
00:48:31,980 --> 00:48:32,940
in

1181
00:48:32,940 --> 00:48:34,980
um this is the end of the semester the

1182
00:48:34,980 --> 00:48:37,500
last uh talk for the semester we will

1183
00:48:37,500 --> 00:48:40,200
resume in January and you can visit the

1184
00:48:40,200 --> 00:48:43,099
webpage to see the lineup of speakers

1185
00:48:43,099 --> 00:48:46,319
I'll also note next year is the 25th

1186
00:48:46,319 --> 00:48:48,380
anniversary of the founding of Sirius

1187
00:48:48,380 --> 00:48:51,420
and that's a big milestone we would

1188
00:48:51,420 --> 00:48:53,940
encourage all of you to put our

1189
00:48:53,940 --> 00:48:57,119
Symposium March 28th and 29th on your

1190
00:48:57,119 --> 00:49:00,060
calendars to plan to attend

1191
00:49:00,060 --> 00:49:02,460
and help us celebrate we're really

1192
00:49:02,460 --> 00:49:06,240
looking forward to making that an event

1193
00:49:06,240 --> 00:49:08,940
and with that I want to say thank you so

1194
00:49:08,940 --> 00:49:10,740
much Julie and hopefully we'll get to

1195
00:49:10,740 --> 00:49:13,260
see you in person at some time uh you're

1196
00:49:13,260 --> 00:49:15,540
welcome to come out anytime and visit

1197
00:49:15,540 --> 00:49:19,020
with uh our faculty and students and see

1198
00:49:19,020 --> 00:49:21,119
what we have great thanks for having me

1199
00:49:21,119 --> 00:49:22,619
thanks everyone

1200
00:49:22,619 --> 00:49:24,000
goodbye everyone have a wonderful

1201
00:49:24,000 --> 00:49:26,660
holiday season

