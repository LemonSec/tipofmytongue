1
00:00:04,550 --> 00:00:10,830
so so good morning again so let's start

2
00:00:09,210 --> 00:00:14,430
it with let's start with the the panel

3
00:00:10,830 --> 00:00:21,060
discussion on a vehicle and autonomous

4
00:00:14,430 --> 00:00:23,100
of a thing security so so the overall

5
00:00:21,060 --> 00:00:26,400
scene or the kind of the the the the

6
00:00:23,100 --> 00:00:30,270
starting kind of opening as a story or a

7
00:00:26,400 --> 00:00:33,149
vision or observation is that cyber

8
00:00:30,270 --> 00:00:35,190
security is no longer about cyber or not

9
00:00:33,149 --> 00:00:38,399
just about cyber it's really about the

10
00:00:35,190 --> 00:00:42,510
security of a cyber eyes physical world

11
00:00:38,399 --> 00:00:45,420
or physical system so I always start

12
00:00:42,510 --> 00:00:47,550
with a bunch of kind of a recent stories

13
00:00:45,420 --> 00:00:50,910
that resonate with you know this

14
00:00:47,550 --> 00:00:52,559
observation last year at a kind of a

15
00:00:50,910 --> 00:00:56,160
very traditional Technical Research

16
00:00:52,559 --> 00:00:58,968
Conference in cyber security a group of

17
00:00:56,160 --> 00:01:02,038
researchers from UK and Germany

18
00:00:58,969 --> 00:01:05,460
demonstrated the feasibility of cloning

19
00:01:02,039 --> 00:01:11,340
a kind of a keyless remote entry remote

20
00:01:05,459 --> 00:01:13,408
control of a European made vehicle by

21
00:01:11,340 --> 00:01:17,400
just doing some reverse engineering on

22
00:01:13,409 --> 00:01:19,680
that original key and by listening to

23
00:01:17,400 --> 00:01:22,470
just one session of the interaction

24
00:01:19,680 --> 00:01:26,520
between the real key and the vehicle and

25
00:01:22,470 --> 00:01:28,619
then they can actually clone a remote

26
00:01:26,520 --> 00:01:30,810
maybe not as beautiful as the original

27
00:01:28,619 --> 00:01:33,450
one but equally powerful in remotely

28
00:01:30,810 --> 00:01:36,259
opening or you know turning on that

29
00:01:33,450 --> 00:01:39,360
vehicle remotely hundreds of feet away

30
00:01:36,259 --> 00:01:42,119
so this kind of sounds very scary and I

31
00:01:39,360 --> 00:01:45,000
heard that the the authors were faced

32
00:01:42,119 --> 00:01:48,060
two years ago they faced a lawsuit from

33
00:01:45,000 --> 00:01:51,030
that the European automaker and they

34
00:01:48,060 --> 00:01:52,890
ultimately were able to kind of a get

35
00:01:51,030 --> 00:01:55,350
out of that trouble and publish the

36
00:01:52,890 --> 00:01:58,140
paper last year so that would actually

37
00:01:55,350 --> 00:01:59,490
receive a lot of media coverage because

38
00:01:58,140 --> 00:02:01,560
this is kind of kind of highly

39
00:01:59,490 --> 00:02:04,140
unconventional not the traditional type

40
00:02:01,560 --> 00:02:07,069
of cybersecurity research papers that we

41
00:02:04,140 --> 00:02:10,259
read right out of conference like that

42
00:02:07,069 --> 00:02:14,040
and also many of us heard about some of

43
00:02:10,258 --> 00:02:16,920
the recent incidents like over Tesla

44
00:02:14,040 --> 00:02:19,679
they both have they both

45
00:02:16,920 --> 00:02:22,140
you know experience some accidents or

46
00:02:19,680 --> 00:02:24,690
incidents with their self-driving

47
00:02:22,140 --> 00:02:27,510
vehicle I believe in the case of the

48
00:02:24,690 --> 00:02:30,480
Tesla accident that was actually a fatal

49
00:02:27,510 --> 00:02:33,030
accident and in response you know in

50
00:02:30,480 --> 00:02:36,090
light of movers a recent act you know

51
00:02:33,030 --> 00:02:38,720
Auto autonomous driving vehicle accident

52
00:02:36,090 --> 00:02:42,209
they temporarily you know kind of a halt

53
00:02:38,720 --> 00:02:45,209
the program nice just to reveal the risk

54
00:02:42,209 --> 00:02:49,260
and the end and in the in the issues in

55
00:02:45,209 --> 00:02:51,900
that and I have two more stories and

56
00:02:49,260 --> 00:02:55,530
recently I read an article about hackers

57
00:02:51,900 --> 00:03:00,060
being able to hijack a high-end law

58
00:02:55,530 --> 00:03:03,830
enforcement UAV worth more than $40,000

59
00:03:00,060 --> 00:03:07,350
using a forty dollar commodity remote

60
00:03:03,830 --> 00:03:10,620
and they can do the hijacking from kind

61
00:03:07,350 --> 00:03:14,910
of one mile two mile away from that UAV

62
00:03:10,620 --> 00:03:16,769
and this is more local so my my research

63
00:03:14,910 --> 00:03:20,640
group has been working on a bunch of

64
00:03:16,769 --> 00:03:23,430
varying effort in studying the existing

65
00:03:20,640 --> 00:03:26,880
the vulnerabilities of commodity legacy

66
00:03:23,430 --> 00:03:29,190
control systems in UAVs and we found out

67
00:03:26,880 --> 00:03:31,530
that there exist many vulnerabilities

68
00:03:29,190 --> 00:03:33,920
not it not just in the program but also

69
00:03:31,530 --> 00:03:36,720
in the control models themselves and

70
00:03:33,920 --> 00:03:39,208
that actually opens the door for all

71
00:03:36,720 --> 00:03:41,130
kinds of a manipulation including you

72
00:03:39,209 --> 00:03:43,560
know hijacking or you know drone

73
00:03:41,130 --> 00:03:45,480
crashing basically you can just program

74
00:03:43,560 --> 00:03:48,239
the drone right once you get over the

75
00:03:45,480 --> 00:03:50,459
control and surprisingly if you look at

76
00:03:48,239 --> 00:03:52,500
the control program sometimes you don't

77
00:03:50,459 --> 00:03:55,010
even find any of those traditional

78
00:03:52,500 --> 00:03:59,280
program level vulnerability no overflow

79
00:03:55,010 --> 00:04:01,950
no control flow you know integrity

80
00:03:59,280 --> 00:04:03,570
violation nothing it's all semantics so

81
00:04:01,950 --> 00:04:05,339
now you know the more I study this

82
00:04:03,570 --> 00:04:07,829
problem the more realize that in order

83
00:04:05,340 --> 00:04:09,780
to study fibers physical security you

84
00:04:07,829 --> 00:04:11,579
really have to understand physics you

85
00:04:09,780 --> 00:04:15,209
really have to understand mechanics and

86
00:04:11,579 --> 00:04:17,250
control control theory not cyber not not

87
00:04:15,209 --> 00:04:19,140
hacking not the traditional kind of

88
00:04:17,250 --> 00:04:25,070
skills that the we and cyber security

89
00:04:19,140 --> 00:04:28,289
area would will have to gain so so we

90
00:04:25,070 --> 00:04:30,210
find it fascinating so that's why I did

91
00:04:28,289 --> 00:04:32,880
not hesitate a second to

92
00:04:30,210 --> 00:04:34,979
agree to serve as the moderate mod

93
00:04:32,880 --> 00:04:36,690
moderator of this panel so today we're

94
00:04:34,979 --> 00:04:40,349
going to talk about the security of the

95
00:04:36,690 --> 00:04:42,620
autonomous vehicle and more broadly a

96
00:04:40,349 --> 00:04:47,789
smart security of smart transportation

97
00:04:42,620 --> 00:04:50,849
system smart autonomous systems so my so

98
00:04:47,789 --> 00:04:52,740
I'm the moderator I am a professor in

99
00:04:50,849 --> 00:04:55,530
the computer science department

100
00:04:52,740 --> 00:04:59,720
I'm also the interim director of serious

101
00:04:55,530 --> 00:05:07,138
so I would like to quickly introduce my

102
00:04:59,720 --> 00:05:11,009
my panelists and first we have dr. Susan

103
00:05:07,139 --> 00:05:16,410
College she is a senior fellow at

104
00:05:11,009 --> 00:05:19,020
Raytheon and mr. Michael I know he is

105
00:05:16,410 --> 00:05:21,620
the group manager of autonomous product

106
00:05:19,020 --> 00:05:25,590
cyber security at General Motors

107
00:05:21,620 --> 00:05:28,319
mr. Charles tinkle he is the art he's an

108
00:05:25,590 --> 00:05:31,258
RD cyber security engineer at analog

109
00:05:28,319 --> 00:05:34,440
devices in Corporation and finally I

110
00:05:31,259 --> 00:05:36,180
have Professor Brendan Pitts assistant

111
00:05:34,440 --> 00:05:39,840
professor at the school of industrial

112
00:05:36,180 --> 00:05:43,080
engineering at Purdue so I will let each

113
00:05:39,840 --> 00:05:45,869
of our panelists to give a more specific

114
00:05:43,080 --> 00:05:48,389
introduction about their work and then I

115
00:05:45,870 --> 00:05:51,479
will let them give a kind of a position

116
00:05:48,389 --> 00:05:53,669
statement and I did prepare some kind of

117
00:05:51,479 --> 00:05:56,880
a questions that I would like each of

118
00:05:53,669 --> 00:05:58,560
them to respond to so maybe I would just

119
00:05:56,880 --> 00:06:00,240
briefly you know mention some of the

120
00:05:58,560 --> 00:06:02,880
questions and then I will open the floor

121
00:06:00,240 --> 00:06:04,770
for the audience to ask them more

122
00:06:02,880 --> 00:06:07,710
questions I hope to make this stuff

123
00:06:04,770 --> 00:06:09,479
really interactive and hope you know

124
00:06:07,710 --> 00:06:11,669
hopefully entertaining and maybe

125
00:06:09,479 --> 00:06:14,460
controversial I don't know so we'll see

126
00:06:11,669 --> 00:06:17,460
so I'm personally interested in knowing

127
00:06:14,460 --> 00:06:20,159
you know the comment and the vision from

128
00:06:17,460 --> 00:06:21,448
each of you on the panel the first one

129
00:06:20,159 --> 00:06:24,479
is what are some of the unique

130
00:06:21,449 --> 00:06:27,090
challenges right in a smart vehicle

131
00:06:24,479 --> 00:06:28,860
security and these are the I'm asking

132
00:06:27,090 --> 00:06:31,138
about the new challenges that do not

133
00:06:28,860 --> 00:06:33,240
exist in the traditional cybersecurity

134
00:06:31,139 --> 00:06:37,219
domain so what are the unique challenges

135
00:06:33,240 --> 00:06:39,599
in smart vehicle security and then I

136
00:06:37,219 --> 00:06:41,460
wonder I would like to know you know

137
00:06:39,599 --> 00:06:43,199
whether they feel optimistic or

138
00:06:41,460 --> 00:06:44,909
pessimistic so what is their outlook

139
00:06:43,199 --> 00:06:46,349
right in the sphere

140
00:06:44,909 --> 00:06:48,239
you know are there you know too many

141
00:06:46,349 --> 00:06:50,550
risk factors you know what shall we

142
00:06:48,239 --> 00:06:52,859
proceed despite all the little risks or

143
00:06:50,550 --> 00:06:53,369
you know issues or incidents that happen

144
00:06:52,860 --> 00:06:57,300
recently

145
00:06:53,369 --> 00:06:59,909
and finally how smart right do you want

146
00:06:57,300 --> 00:07:01,919
our autonomous vehicle to be you know

147
00:06:59,909 --> 00:07:03,869
should they be just fully automatic

148
00:07:01,919 --> 00:07:06,869
fully driverless or we want to have you

149
00:07:03,869 --> 00:07:08,269
know human intervention and finally what

150
00:07:06,869 --> 00:07:11,279
are some of the most promising

151
00:07:08,269 --> 00:07:13,800
approaches directions toward the

152
00:07:11,279 --> 00:07:17,189
ultimate security and reliability of

153
00:07:13,800 --> 00:07:19,289
smart vehicle so these are my personal

154
00:07:17,189 --> 00:07:21,839
questions I believe the audience will

155
00:07:19,289 --> 00:07:25,308
have more so lets you know what start

156
00:07:21,839 --> 00:07:29,879
with our panelists position statements

157
00:07:25,309 --> 00:07:31,399
Susan good morning can you hear me okay

158
00:07:29,879 --> 00:07:34,019
and does this work

159
00:07:31,399 --> 00:07:36,269
excellent so if you stopped me in the

160
00:07:34,019 --> 00:07:39,899
hallway at work and asked me how are you

161
00:07:36,269 --> 00:07:42,389
going to protect smart vehicles from

162
00:07:39,899 --> 00:07:46,019
doing damage I would just tell you

163
00:07:42,389 --> 00:07:51,029
off-the-cuff go look at FPE nist special

164
00:07:46,019 --> 00:07:52,829
publication 800-53 version 4 version 5

165
00:07:51,029 --> 00:07:54,929
is coming out soon this is a document

166
00:07:52,829 --> 00:07:57,839
commonly known as the risk management

167
00:07:54,929 --> 00:08:01,529
framework it's a really fabulous well

168
00:07:57,839 --> 00:08:03,360
put together document and essentially it

169
00:08:01,529 --> 00:08:05,610
it gives you a bunch of controls and

170
00:08:03,360 --> 00:08:07,469
ways of selecting controls I would say

171
00:08:05,610 --> 00:08:10,169
maybe you need to develop a vehicle

172
00:08:07,469 --> 00:08:11,789
overlay to tell you which which controls

173
00:08:10,169 --> 00:08:14,909
are important and so forth and you can

174
00:08:11,789 --> 00:08:18,058
go through all that but as The Economist

175
00:08:14,909 --> 00:08:20,639
had a special report a couple weeks ago

176
00:08:18,059 --> 00:08:21,990
they said we have to assume everything's

177
00:08:20,639 --> 00:08:23,519
going to be hackable we can do

178
00:08:21,990 --> 00:08:26,369
everything in the world to protect it

179
00:08:23,519 --> 00:08:29,029
but something that's important my

180
00:08:26,369 --> 00:08:31,259
position is let's start thinking about

181
00:08:29,029 --> 00:08:33,419
certainly we've got to require all that

182
00:08:31,259 --> 00:08:35,339
but what happens if ik attacked that's

183
00:08:33,419 --> 00:08:40,438
where we really need to focus some

184
00:08:35,339 --> 00:08:42,779
attention so what I'm looking at here

185
00:08:40,438 --> 00:08:44,670
and my friend my colleagues here from GM

186
00:08:42,779 --> 00:08:47,939
and some other companies probably know

187
00:08:44,670 --> 00:08:51,420
more about this than the cyber

188
00:08:47,939 --> 00:08:55,920
security's people do vehicle safety is

189
00:08:51,420 --> 00:08:56,849
very long it has been around for a long

190
00:08:55,920 --> 00:08:58,800
time

191
00:08:56,850 --> 00:09:01,890
there's been a lot of good work done in

192
00:08:58,800 --> 00:09:04,109
automotive industry in airplane industry

193
00:09:01,890 --> 00:09:05,699
and so forth we need to be able to

194
00:09:04,110 --> 00:09:10,460
figure out how to bring those two

195
00:09:05,700 --> 00:09:14,070
together this past summer I worked as a

196
00:09:10,460 --> 00:09:19,530
cybersecurity lead and I got feedback a

197
00:09:14,070 --> 00:09:21,660
couple some related pursuits where I got

198
00:09:19,530 --> 00:09:24,540
to work with some nuclear surety people

199
00:09:21,660 --> 00:09:27,240
and I showed in my RMF document I was so

200
00:09:24,540 --> 00:09:29,099
proud of my 462 pages for them this

201
00:09:27,240 --> 00:09:31,920
seems to be the table of contents for

202
00:09:29,100 --> 00:09:34,530
the documents they have to look at so so

203
00:09:31,920 --> 00:09:37,260
there's an awful lot that's been done

204
00:09:34,530 --> 00:09:38,640
and said about safety and surety and

205
00:09:37,260 --> 00:09:42,090
things like that and I think the

206
00:09:38,640 --> 00:09:44,490
cybersecurity community has been smarter

207
00:09:42,090 --> 00:09:47,070
on what's going on there but but the

208
00:09:44,490 --> 00:09:49,290
basics are here how do you protect the

209
00:09:47,070 --> 00:09:51,960
really you know critical safety

210
00:09:49,290 --> 00:09:54,870
equipment the lights the turn signals

211
00:09:51,960 --> 00:09:59,040
the brakes we all know about the famous

212
00:09:54,870 --> 00:10:01,020
Toyota unintended unintended

213
00:09:59,040 --> 00:10:03,689
acceleration control which turns out to

214
00:10:01,020 --> 00:10:06,060
be a problem with a flat fly-by-wire

215
00:10:03,690 --> 00:10:08,130
system you know the kill switch things

216
00:10:06,060 --> 00:10:09,449
like this you don't want you know kill

217
00:10:08,130 --> 00:10:12,150
switch sounds good to you certainly

218
00:10:09,450 --> 00:10:13,800
don't want the car killing when it's in

219
00:10:12,150 --> 00:10:15,030
the middle of fast-moving traffic for

220
00:10:13,800 --> 00:10:17,849
instance so we're going to have to

221
00:10:15,030 --> 00:10:20,130
figure out how to protect particularly

222
00:10:17,850 --> 00:10:23,100
protect those systems we need to come up

223
00:10:20,130 --> 00:10:25,110
with extensive monitoring put coolant

224
00:10:23,100 --> 00:10:28,980
you know along the lines of a collision

225
00:10:25,110 --> 00:10:30,990
may occur how can you avoid even if

226
00:10:28,980 --> 00:10:32,910
something manages to take over the car

227
00:10:30,990 --> 00:10:36,180
or their collision avoidance system

228
00:10:32,910 --> 00:10:38,490
sixes maybe counteract some of that look

229
00:10:36,180 --> 00:10:40,640
at post crash survivability which the

230
00:10:38,490 --> 00:10:43,470
car companies and the airplane companies

231
00:10:40,640 --> 00:10:45,510
my brother on a commercial side of

232
00:10:43,470 --> 00:10:47,580
Boeing who's also worked for GM has told

233
00:10:45,510 --> 00:10:49,800
me that he thought you know all the

234
00:10:47,580 --> 00:10:51,480
survivability at GM was crazy and then

235
00:10:49,800 --> 00:10:53,579
he got into the airplane world and when

236
00:10:51,480 --> 00:10:57,980
you look at all the standards on fire

237
00:10:53,580 --> 00:11:00,600
retardants and so forth and airplanes

238
00:10:57,980 --> 00:11:02,250
there's a lot of good work that has been

239
00:11:00,600 --> 00:11:04,770
done and you know we just need to look

240
00:11:02,250 --> 00:11:07,890
at making sure that works right we've

241
00:11:04,770 --> 00:11:09,760
had various people talk about supply

242
00:11:07,890 --> 00:11:13,240
chain security this is you know

243
00:11:09,760 --> 00:11:15,689
this is if you have an automated car we

244
00:11:13,240 --> 00:11:17,890
probably going to have to have some

245
00:11:15,690 --> 00:11:20,640
regulation that says you have to go get

246
00:11:17,890 --> 00:11:23,640
authorized part authorized service

247
00:11:20,640 --> 00:11:27,400
authorized inspections to make sure that

248
00:11:23,640 --> 00:11:29,949
your car isn't being hacked and so forth

249
00:11:27,400 --> 00:11:33,390
now what that doesn't say is if I leave

250
00:11:29,950 --> 00:11:36,160
my car parked in a parking garage and

251
00:11:33,390 --> 00:11:37,480
someone comes in and you know I mean

252
00:11:36,160 --> 00:11:38,740
we've seen the movies where they put

253
00:11:37,480 --> 00:11:42,250
bombs under the car right

254
00:11:38,740 --> 00:11:43,420
so what to stop someone from modifying

255
00:11:42,250 --> 00:11:46,480
the car when it's parked somewhere

256
00:11:43,420 --> 00:11:48,280
you're probably going to have to see

257
00:11:46,480 --> 00:11:50,890
maybe I got onto just two minutes one

258
00:11:48,280 --> 00:11:55,030
we're going to have to find ways to

259
00:11:50,890 --> 00:11:58,800
monitor our vehicles even manually

260
00:11:55,030 --> 00:12:01,500
driven vehicles you start using our

261
00:11:58,800 --> 00:12:05,740
anyone who knows anything about

262
00:12:01,500 --> 00:12:06,910
persistent surveillance in the audience

263
00:12:05,740 --> 00:12:09,310
probably knows that really our

264
00:12:06,910 --> 00:12:10,480
surveillance cameras everywhere so do we

265
00:12:09,310 --> 00:12:13,869
need to start looking at those

266
00:12:10,480 --> 00:12:17,350
surveillance cameras and start finding

267
00:12:13,870 --> 00:12:19,330
ways to detect malicious malicious

268
00:12:17,350 --> 00:12:21,760
behavior happening to some of these

269
00:12:19,330 --> 00:12:27,880
automatic are the manual consequence

270
00:12:21,760 --> 00:12:31,270
manually doing because I believe that as

271
00:12:27,880 --> 00:12:33,220
we move from person driven cars to

272
00:12:31,270 --> 00:12:35,260
automated driven cars the number of

273
00:12:33,220 --> 00:12:37,300
accidents that occur and the number of

274
00:12:35,260 --> 00:12:40,120
overall deaths is probably going going

275
00:12:37,300 --> 00:12:41,859
to go down substantially but when we do

276
00:12:40,120 --> 00:12:44,140
have accidents they're probably going to

277
00:12:41,860 --> 00:12:50,910
be far more horrific than what we're

278
00:12:44,140 --> 00:12:54,790
used to that's been proposed by various

279
00:12:50,910 --> 00:12:57,069
analysis out there so we're going to

280
00:12:54,790 --> 00:12:59,020
have to start looking at new way of

281
00:12:57,070 --> 00:13:03,730
protecting ourselves new ways of

282
00:12:59,020 --> 00:13:05,790
protecting this few vehicles we're going

283
00:13:03,730 --> 00:13:08,470
to have to change public safety and

284
00:13:05,790 --> 00:13:11,959
that's we're not going to be able to

285
00:13:08,470 --> 00:13:14,690
take public safety budgets and

286
00:13:11,960 --> 00:13:16,580
triple them or couple them to take

287
00:13:14,690 --> 00:13:20,260
account for these autonomous vehicles

288
00:13:16,580 --> 00:13:22,760
we're going to have to find savings

289
00:13:20,260 --> 00:13:24,770
where we're currently spending money and

290
00:13:22,760 --> 00:13:27,100
find new ways to spend money to do this

291
00:13:24,770 --> 00:13:31,010
I'd create surveillance and so forth and

292
00:13:27,100 --> 00:13:33,470
as we look at how things change I think

293
00:13:31,010 --> 00:13:34,910
the really hard part is going to be the

294
00:13:33,470 --> 00:13:36,740
transition there's going to be the

295
00:13:34,910 --> 00:13:39,680
current stage right now where we have

296
00:13:36,740 --> 00:13:42,680
Public Safety that's really a hundred

297
00:13:39,680 --> 00:13:45,170
percent aimed at manually driven cars in

298
00:13:42,680 --> 00:13:47,719
the future we're going to have probably

299
00:13:45,170 --> 00:13:49,490
a hundred percent autonomous driven cars

300
00:13:47,720 --> 00:13:50,720
and there's going to be this transition

301
00:13:49,490 --> 00:13:53,300
period where we're going to have both

302
00:13:50,720 --> 00:13:54,980
manually driven cars and autonomous

303
00:13:53,300 --> 00:13:56,689
driven cars on the road but if you think

304
00:13:54,980 --> 00:13:58,720
about a lot of what we do for public

305
00:13:56,690 --> 00:14:01,190
safety if you do driving through

306
00:13:58,720 --> 00:14:03,440
Lafayette you you're running into all

307
00:14:01,190 --> 00:14:04,910
these traffic cones because we you know

308
00:14:03,440 --> 00:14:06,440
we need to keep people out of these

309
00:14:04,910 --> 00:14:09,459
wings it could be once we have

310
00:14:06,440 --> 00:14:12,200
autonomous vehicles they just get sent

311
00:14:09,460 --> 00:14:14,750
restrictions to their computers and the

312
00:14:12,200 --> 00:14:16,400
computers won't let them drive in so we

313
00:14:14,750 --> 00:14:18,110
may be able to save a lot of money on

314
00:14:16,400 --> 00:14:20,000
you know placement a traffic cones

315
00:14:18,110 --> 00:14:22,520
question of traffic cones and speed

316
00:14:20,000 --> 00:14:24,920
tours and so forth by just pushing it

317
00:14:22,520 --> 00:14:28,390
out to the cars and maybe we can take

318
00:14:24,920 --> 00:14:31,670
some of this money that we're saving on

319
00:14:28,390 --> 00:14:33,560
routine stuff we do today and figure out

320
00:14:31,670 --> 00:14:37,219
how to better use it to protect the

321
00:14:33,560 --> 00:14:42,170
supply chain and to protect malicious

322
00:14:37,220 --> 00:14:43,640
behavior in the future and of course the

323
00:14:42,170 --> 00:14:47,089
real challenge is what are we going to

324
00:14:43,640 --> 00:14:50,140
do in the transition between automated

325
00:14:47,090 --> 00:14:53,500
cars and man Department automated cuffs

326
00:14:50,140 --> 00:14:53,500
with that off

327
00:14:54,320 --> 00:14:59,389
all right thanks Susan well the one

328
00:14:57,110 --> 00:15:01,730
thing I'll say about the traffic cones

329
00:14:59,389 --> 00:15:03,829
I'm always going to have my 72 Impala

330
00:15:01,730 --> 00:15:06,230
and I'm always going to expect those

331
00:15:03,829 --> 00:15:07,939
traffic cones to be out there so and I

332
00:15:06,230 --> 00:15:09,980
honestly I think that's GM stance as

333
00:15:07,940 --> 00:15:12,019
well we're not thinking we're not

334
00:15:09,980 --> 00:15:13,459
expecting any kind of changes in any

335
00:15:12,019 --> 00:15:16,220
kind of infrastructure or anything like

336
00:15:13,459 --> 00:15:17,959
that but I'll do my little soapbox I

337
00:15:16,220 --> 00:15:21,350
think I've answered all answers some of

338
00:15:17,959 --> 00:15:23,300
the questions that were raised a little

339
00:15:21,350 --> 00:15:26,269
brief quick excerpt about the EEI

340
00:15:23,300 --> 00:15:28,339
I did graduate at Purdue I actually went

341
00:15:26,269 --> 00:15:30,079
to school to be elected in electrical

342
00:15:28,339 --> 00:15:33,019
engineering I was expecting to come out

343
00:15:30,079 --> 00:15:34,790
of school working on like high voltage

344
00:15:33,019 --> 00:15:37,310
power line systems I was an electrician

345
00:15:34,790 --> 00:15:38,449
in my former life and I thought let's

346
00:15:37,310 --> 00:15:40,910
take to the next level and become an

347
00:15:38,449 --> 00:15:45,290
engineer might make more money and turns

348
00:15:40,910 --> 00:15:47,060
out it wasn't but any rate at any rate

349
00:15:45,290 --> 00:15:50,000
it was it was a good it was a good

350
00:15:47,060 --> 00:15:51,739
transition for me so long story short

351
00:15:50,000 --> 00:15:54,829
nine men died General Motors and now I'm

352
00:15:51,740 --> 00:15:57,740
working on cybersecurity in the in the

353
00:15:54,829 --> 00:15:59,359
autonomous space so my so I'll talk

354
00:15:57,740 --> 00:16:02,000
about my my area

355
00:15:59,360 --> 00:16:03,709
our CEO Mary Barra she has gone on

356
00:16:02,000 --> 00:16:05,120
record multiple times and said we're

357
00:16:03,709 --> 00:16:06,560
going to see more change in the next

358
00:16:05,120 --> 00:16:08,360
five or ten years in the automotive

359
00:16:06,560 --> 00:16:10,910
industry then we have seen in the last

360
00:16:08,360 --> 00:16:13,279
10 years and I personally I think

361
00:16:10,910 --> 00:16:15,319
nowhere is that true in autonomous and

362
00:16:13,279 --> 00:16:18,680
cybersecurity so we've got a lot of work

363
00:16:15,319 --> 00:16:21,229
to do in our space and she's made that

364
00:16:18,680 --> 00:16:23,060
very clear to us that she's reliant on

365
00:16:21,230 --> 00:16:25,639
both cybersecurity and autonomous so I

366
00:16:23,060 --> 00:16:27,138
get it from both angles and she's made

367
00:16:25,639 --> 00:16:29,839
it very clear that we have to be we have

368
00:16:27,139 --> 00:16:31,160
to win this in the segment so like we

369
00:16:29,839 --> 00:16:33,230
were saying earlier cybersecurity is new

370
00:16:31,160 --> 00:16:35,149
for us right at General Motors it's

371
00:16:33,230 --> 00:16:36,649
probably about 10 years old they'll

372
00:16:35,149 --> 00:16:38,779
believe it or not we've been working on

373
00:16:36,649 --> 00:16:40,760
for a while it's been really viable

374
00:16:38,779 --> 00:16:43,189
during the last five years where we've

375
00:16:40,760 --> 00:16:44,930
actually started to make some changes in

376
00:16:43,189 --> 00:16:49,099
tarp and into our products that you're

377
00:16:44,930 --> 00:16:50,630
that you're driving today but most of

378
00:16:49,100 --> 00:16:54,199
the stuff in this forum has not really

379
00:16:50,630 --> 00:16:56,089
been vehicle focused or really anything

380
00:16:54,199 --> 00:16:58,219
up here for our panel and that's fine

381
00:16:56,089 --> 00:17:00,769
right and we knew that going in we knew

382
00:16:58,220 --> 00:17:03,260
that you know if you look at the people

383
00:17:00,769 --> 00:17:05,480
that work in our organization very few

384
00:17:03,260 --> 00:17:07,579
of them have backgrounds and like what

385
00:17:05,480 --> 00:17:07,939
you guys have and that's fine because

386
00:17:07,579 --> 00:17:09,648
again

387
00:17:07,939 --> 00:17:10,429
like we were talking to you David you

388
00:17:09,648 --> 00:17:12,079
were saying earlier there's this

389
00:17:10,429 --> 00:17:13,759
technical deficit in the world and

390
00:17:12,079 --> 00:17:15,799
there's just not enough people to come

391
00:17:13,759 --> 00:17:18,230
in and one want to work in automotive

392
00:17:15,799 --> 00:17:21,349
and to have this knowledge to work in

393
00:17:18,230 --> 00:17:24,079
the in the IT space or in this security

394
00:17:21,349 --> 00:17:25,759
space so we've made do with what we have

395
00:17:24,079 --> 00:17:28,039
we've leaned on all the things that you

396
00:17:25,759 --> 00:17:29,809
guys designed and worked on we've taken

397
00:17:28,039 --> 00:17:32,779
it we've adapted it we've modified it

398
00:17:29,809 --> 00:17:34,340
we've scaled it down in many cases

399
00:17:32,779 --> 00:17:36,620
because a lot of the times are our

400
00:17:34,340 --> 00:17:38,480
computers that are in our car that run

401
00:17:36,620 --> 00:17:39,699
our cars are no bigger than these

402
00:17:38,480 --> 00:17:42,950
microphones right here and they're

403
00:17:39,700 --> 00:17:44,779
low-power they're low processor they

404
00:17:42,950 --> 00:17:47,149
don't have a lot of memory and you try

405
00:17:44,779 --> 00:17:48,470
to get a product engineer to put more

406
00:17:47,149 --> 00:17:51,799
memory in the controller it's like you

407
00:17:48,470 --> 00:17:54,820
know it's like you're asking the Pulis

408
00:17:51,799 --> 00:17:57,080
teeth or something no it's ridiculous so

409
00:17:54,820 --> 00:17:59,689
we do have that but we do have that

410
00:17:57,080 --> 00:18:02,379
challenge we also have the challenge

411
00:17:59,690 --> 00:18:05,659
that you know our supply chain if

412
00:18:02,379 --> 00:18:07,370
there's been out there for 20 50 years

413
00:18:05,659 --> 00:18:09,049
or whatever right and they don't know

414
00:18:07,370 --> 00:18:11,750
about security they've never definitely

415
00:18:09,049 --> 00:18:13,970
what looks security out what I did

416
00:18:11,750 --> 00:18:15,620
safety isn't that enough and also they

417
00:18:13,970 --> 00:18:17,960
so they don't understand the security

418
00:18:15,620 --> 00:18:20,059
side of it so General Motors I feel is

419
00:18:17,960 --> 00:18:22,190
probably leading the way of educating

420
00:18:20,059 --> 00:18:23,840
our supply chain which the rest of the

421
00:18:22,190 --> 00:18:25,879
industry of course benefits from that as

422
00:18:23,840 --> 00:18:27,408
well and we're also educating our own

423
00:18:25,879 --> 00:18:30,230
product engineering teams on how to

424
00:18:27,409 --> 00:18:32,059
become more security savvy I kind of

425
00:18:30,230 --> 00:18:33,950
think of it as like where safety was

426
00:18:32,059 --> 00:18:35,990
maybe 20 years ago nobody really thought

427
00:18:33,950 --> 00:18:37,820
about safety safety was kind of I will

428
00:18:35,990 --> 00:18:39,799
figure that out after we you know after

429
00:18:37,820 --> 00:18:41,509
we get it to work but that's not true

430
00:18:39,799 --> 00:18:44,269
today and I think somebody else

431
00:18:41,509 --> 00:18:46,429
mentioned yesterday that why do why do I

432
00:18:44,269 --> 00:18:48,710
have to tell people that I have to teach

433
00:18:46,429 --> 00:18:50,509
them how to do security it's not why do

434
00:18:48,710 --> 00:18:52,519
I have to make it their job well it's

435
00:18:50,509 --> 00:18:54,759
got to be everybody's job right we have

436
00:18:52,519 --> 00:18:56,899
to design insecurity into these designs

437
00:18:54,759 --> 00:18:59,570
otherwise it's not going to work and we

438
00:18:56,899 --> 00:19:01,850
realize that and we're slowly everybody

439
00:18:59,570 --> 00:19:05,299
refers to GM is this giant ship right it

440
00:19:01,850 --> 00:19:07,519
takes a lot to turn this giant ship and

441
00:19:05,299 --> 00:19:08,870
we're getting there just I've only been

442
00:19:07,519 --> 00:19:11,330
in this organization for about three

443
00:19:08,870 --> 00:19:13,189
years and I've seen a lot of a lot of

444
00:19:11,330 --> 00:19:15,470
acceptance

445
00:19:13,190 --> 00:19:17,269
you could argue maybe it's because we're

446
00:19:15,470 --> 00:19:19,279
beating them with a with a with a very

447
00:19:17,269 --> 00:19:21,280
big stick I know you have to do this but

448
00:19:19,279 --> 00:19:22,930
they are accepting it there

449
00:19:21,280 --> 00:19:27,399
start to realize that yeah you do have

450
00:19:22,930 --> 00:19:28,420
to do these kinds of things so one of

451
00:19:27,400 --> 00:19:30,040
the one of the particular areas that I

452
00:19:28,420 --> 00:19:31,750
was going to refer to and I didn't know

453
00:19:30,040 --> 00:19:34,210
how autonomous he wanted this this

454
00:19:31,750 --> 00:19:36,370
conversation to be but in one particular

455
00:19:34,210 --> 00:19:37,980
area that I that we really focus on and

456
00:19:36,370 --> 00:19:40,629
you kind of touched on it Susan was

457
00:19:37,980 --> 00:19:42,910
diagnostics so we have to make our

458
00:19:40,630 --> 00:19:45,880
vehicle serviceable you have to build

459
00:19:42,910 --> 00:19:47,260
the cars and as a result of publishing

460
00:19:45,880 --> 00:19:50,410
some of those some of that service

461
00:19:47,260 --> 00:19:52,780
information malicious people can take a

462
00:19:50,410 --> 00:19:54,700
hold of that stuff and do some bad

463
00:19:52,780 --> 00:19:56,530
things for it so another perfect example

464
00:19:54,700 --> 00:19:58,540
of where we're trying to educate our

465
00:19:56,530 --> 00:20:00,070
supply chains again as well as our own

466
00:19:58,540 --> 00:20:03,040
product they don't want to say like we

467
00:20:00,070 --> 00:20:06,669
are all-knowing up here in a General

468
00:20:03,040 --> 00:20:08,110
Motors but we got a week we got a we

469
00:20:06,670 --> 00:20:09,550
always tell my I always tell my

470
00:20:08,110 --> 00:20:11,949
engineers I should put yourself in that

471
00:20:09,550 --> 00:20:14,020
dark place now put yourself in like what

472
00:20:11,950 --> 00:20:15,910
could you do what could you do with that

473
00:20:14,020 --> 00:20:18,190
brake bleed I ignite akyuu know while

474
00:20:15,910 --> 00:20:20,200
they're going down the road so you you

475
00:20:18,190 --> 00:20:21,790
get your supply chain you get your

476
00:20:20,200 --> 00:20:25,270
manufacturing representatives you get

477
00:20:21,790 --> 00:20:26,520
your service representatives and you put

478
00:20:25,270 --> 00:20:29,470
them all in the room and say okay look

479
00:20:26,520 --> 00:20:31,120
here's what will happen or yes yes then

480
00:20:29,470 --> 00:20:33,400
you say well what if what if I execute

481
00:20:31,120 --> 00:20:34,479
that diagnostic at 50 miles an hour

482
00:20:33,400 --> 00:20:36,730
going down the road

483
00:20:34,480 --> 00:20:38,950
well nobody's going to do that well

484
00:20:36,730 --> 00:20:40,990
guess what we have connected cars now

485
00:20:38,950 --> 00:20:44,140
folks and and I get to audience

486
00:20:40,990 --> 00:20:45,550
participation who who can tell me the

487
00:20:44,140 --> 00:20:48,750
first year that we had a connected

488
00:20:45,550 --> 00:20:53,440
vehicle at General Motors raise a hand

489
00:20:48,750 --> 00:20:54,520
and I um I thought indeed but what was

490
00:20:53,440 --> 00:20:55,540
our first what year was our first

491
00:20:54,520 --> 00:21:01,379
connected car

492
00:20:55,540 --> 00:21:06,670
I hear 95 back there that's pretty close

493
00:21:01,380 --> 00:21:08,170
mm one more guess 98 so if you guys

494
00:21:06,670 --> 00:21:10,780
average it out you're pretty close it's

495
00:21:08,170 --> 00:21:13,000
96 so over 20 years now we've had

496
00:21:10,780 --> 00:21:14,830
connected cars and a lot of people don't

497
00:21:13,000 --> 00:21:16,660
realize that we've had a connected car

498
00:21:14,830 --> 00:21:18,939
you know did mean bike with anybody

499
00:21:16,660 --> 00:21:21,490
talking about this stuff 20 years ago no

500
00:21:18,940 --> 00:21:23,230
and that's why we happily went on and

501
00:21:21,490 --> 00:21:25,570
made our Diagnostics and we didn't care

502
00:21:23,230 --> 00:21:29,200
when or how or who was going to execute

503
00:21:25,570 --> 00:21:30,939
them now come 2017 right we have to

504
00:21:29,200 --> 00:21:32,680
worry about that so we get all those

505
00:21:30,940 --> 00:21:33,779
people in a room and we say okay well

506
00:21:32,680 --> 00:21:35,570
what if what if

507
00:21:33,779 --> 00:21:38,099
were to execute those Diagnostics and

508
00:21:35,570 --> 00:21:39,869
like he was saying it's not really it's

509
00:21:38,099 --> 00:21:41,939
not a security question anymore it's it

510
00:21:39,869 --> 00:21:45,330
becomes a risk question like well how do

511
00:21:41,940 --> 00:21:47,849
I can I not run this at 50 miles an hour

512
00:21:45,330 --> 00:21:49,769
well you need to be the car running does

513
00:21:47,849 --> 00:21:52,049
the car have to be idling can the car be

514
00:21:49,769 --> 00:21:54,059
in park you know so you talk through

515
00:21:52,049 --> 00:21:55,799
that and you throw in a set of

516
00:21:54,059 --> 00:21:59,158
rationalities or you throw in a set of

517
00:21:55,799 --> 00:22:00,658
conditions that still achieves the

518
00:21:59,159 --> 00:22:02,849
serviceability or the manufacturing

519
00:22:00,659 --> 00:22:04,979
ability of the vehicle and keeps your

520
00:22:02,849 --> 00:22:08,359
customer safe if by chance somebody were

521
00:22:04,979 --> 00:22:10,769
to execute that diagnostic remotely

522
00:22:08,359 --> 00:22:13,710
another another condition that we have

523
00:22:10,769 --> 00:22:15,419
that's in our in our field you know the

524
00:22:13,710 --> 00:22:17,129
technology guys all have in your pocket

525
00:22:15,419 --> 00:22:18,479
or sit on your table there's you're

526
00:22:17,129 --> 00:22:20,519
probably not going to have that thing in

527
00:22:18,479 --> 00:22:22,859
five years probably not even two years

528
00:22:20,519 --> 00:22:26,070
from now again I'm still driving my 72

529
00:22:22,859 --> 00:22:28,339
Impala convertible and I've got nine

530
00:22:26,070 --> 00:22:30,269
cars at home that I love to drive and

531
00:22:28,339 --> 00:22:32,759
I'm going to be driving those things

532
00:22:30,269 --> 00:22:34,589
forever so we have to develop these

533
00:22:32,759 --> 00:22:36,899
security measures in the Indy security

534
00:22:34,589 --> 00:22:39,889
designs that have to be viable in

535
00:22:36,899 --> 00:22:42,299
rock-solid for 20 30 years from now and

536
00:22:39,889 --> 00:22:44,039
so how do you get your supply chain to

537
00:22:42,299 --> 00:22:47,399
do that that's going to be a challenge

538
00:22:44,039 --> 00:22:50,099
for us we have a tough time just getting

539
00:22:47,399 --> 00:22:51,928
a supplier to recompile their software a

540
00:22:50,099 --> 00:22:53,668
year later after we launch a product

541
00:22:51,929 --> 00:22:55,019
because we found might not have been

542
00:22:53,669 --> 00:22:56,789
security related we might have said well

543
00:22:55,019 --> 00:22:59,149
you know the performance is not quite

544
00:22:56,789 --> 00:23:02,460
right so we need to recompile that code

545
00:22:59,149 --> 00:23:03,689
we don't have that code anymore right so

546
00:23:02,460 --> 00:23:05,339
we got it we got to start educator

547
00:23:03,690 --> 00:23:07,369
supply changes we have to set this up

548
00:23:05,339 --> 00:23:09,869
from the start with our contracts and an

549
00:23:07,369 --> 00:23:11,519
education that no you have to keep those

550
00:23:09,869 --> 00:23:13,408
tools somehow we got to figure out how

551
00:23:11,519 --> 00:23:14,339
to keep those tools keep those compilers

552
00:23:13,409 --> 00:23:16,799
in the state where they're going to

553
00:23:14,339 --> 00:23:18,989
execute or create and compile that same

554
00:23:16,799 --> 00:23:21,119
software exactly and then we're going to

555
00:23:18,989 --> 00:23:24,299
run into challenges with again with our

556
00:23:21,119 --> 00:23:25,619
tiny micros and memory size that we have

557
00:23:24,299 --> 00:23:28,679
at some point you're just going to reach

558
00:23:25,619 --> 00:23:30,178
a limit that you can't cram more stuff

559
00:23:28,679 --> 00:23:32,999
into that controller to fix those

560
00:23:30,179 --> 00:23:34,919
patches so OTA everybody says yeah OTA

561
00:23:32,999 --> 00:23:36,989
oh just just flash it just put new

562
00:23:34,919 --> 00:23:38,969
software it software's free

563
00:23:36,989 --> 00:23:40,679
it's not that easy right so we got ahead

564
00:23:38,969 --> 00:23:44,820
we had to work with our supply chain to

565
00:23:40,679 --> 00:23:46,990
make that kind of stuff happen one of it

566
00:23:44,820 --> 00:23:50,230
is a quick example on

567
00:23:46,990 --> 00:23:52,809
on trying to foresee into the future of

568
00:23:50,230 --> 00:23:55,750
how strong stuff is going to be no back

569
00:23:52,809 --> 00:23:57,850
in probably early 90s mid 90s some

570
00:23:55,750 --> 00:24:00,100
around there the emissions regulators

571
00:23:57,850 --> 00:24:01,919
said you know what oMG need to figure

572
00:24:00,100 --> 00:24:04,510
out a way to stop people from

573
00:24:01,920 --> 00:24:06,280
manipulating your controllers because

574
00:24:04,510 --> 00:24:07,570
people were doing it in the field you

575
00:24:06,280 --> 00:24:09,629
could call it hacking I mean nobody

576
00:24:07,570 --> 00:24:12,370
called it that at the time but they were

577
00:24:09,630 --> 00:24:14,380
you know ethical hackers at the time and

578
00:24:12,370 --> 00:24:17,169
they were just tweaking their prams to

579
00:24:14,380 --> 00:24:19,750
to get a couple miles per gallon extra

580
00:24:17,170 --> 00:24:22,390
or maybe ten extra horsepower

581
00:24:19,750 --> 00:24:24,160
maybe they bumped up the top speed on

582
00:24:22,390 --> 00:24:25,630
their car a little bit but in a sense

583
00:24:24,160 --> 00:24:27,820
they were hacking it and the government

584
00:24:25,630 --> 00:24:29,590
saw this or the admissions agency saw

585
00:24:27,820 --> 00:24:31,689
this and they said you guys got to stop

586
00:24:29,590 --> 00:24:33,399
that it's your problem now and you

587
00:24:31,690 --> 00:24:35,080
certified it to be with these conditions

588
00:24:33,400 --> 00:24:36,309
and now they're operating the car in

589
00:24:35,080 --> 00:24:38,830
these conditions and now you're no

590
00:24:36,309 --> 00:24:40,960
longer meeting or you may not be meeting

591
00:24:38,830 --> 00:24:42,610
your emission standards anymore so we

592
00:24:40,960 --> 00:24:45,309
came up at least General Motors came up

593
00:24:42,610 --> 00:24:49,540
with this brilliant two byte algorithm

594
00:24:45,309 --> 00:24:52,330
to to combat this and we put this on our

595
00:24:49,540 --> 00:24:53,950
programming routine and we said nobody's

596
00:24:52,330 --> 00:24:55,389
ever going to figure out 65,000

597
00:24:53,950 --> 00:24:59,440
combinations that it that should be good

598
00:24:55,390 --> 00:25:02,559
forever right well that's war now 2025

599
00:24:59,440 --> 00:25:04,210
years later you can pretty much find an

600
00:25:02,559 --> 00:25:06,220
end of algorithms like maybe I you

601
00:25:04,210 --> 00:25:07,900
probably loosely use the word algorithms

602
00:25:06,220 --> 00:25:11,080
they were just look up tables they're a

603
00:25:07,900 --> 00:25:13,390
really simple math from C value to key

604
00:25:11,080 --> 00:25:15,879
value no cryptography involved

605
00:25:13,390 --> 00:25:17,620
whatsoever but you can find those all

606
00:25:15,880 --> 00:25:19,450
over the internet now once you find once

607
00:25:17,620 --> 00:25:21,428
you calculate or find a couple of

608
00:25:19,450 --> 00:25:23,950
examples you can pretty much calculate

609
00:25:21,429 --> 00:25:26,080
all 65,000 combinations and people have

610
00:25:23,950 --> 00:25:27,820
posted that on the internet all the all

611
00:25:26,080 --> 00:25:29,230
these perfect you know line for line

612
00:25:27,820 --> 00:25:31,840
here's what you do to get ten extra

613
00:25:29,230 --> 00:25:35,679
horsepower out of your 97 Camaro with

614
00:25:31,840 --> 00:25:37,780
the lt1 or whatever so so that was kind

615
00:25:35,679 --> 00:25:39,790
of our first first attempt at it and

616
00:25:37,780 --> 00:25:41,980
then I think back on that I'm like okay

617
00:25:39,790 --> 00:25:45,428
the stuff that we're doing today now in

618
00:25:41,980 --> 00:25:47,620
in 2016-17 and coming up in our next

619
00:25:45,429 --> 00:25:50,890
architecture jump how good is it going

620
00:25:47,620 --> 00:25:52,178
to be no I hope it's good enough I hope

621
00:25:50,890 --> 00:25:53,860
with with the advanced cryptography

622
00:25:52,179 --> 00:25:55,600
algorithms and stuff that we're using

623
00:25:53,860 --> 00:25:57,490
now that if they're much more

624
00:25:55,600 --> 00:25:59,929
bulletproof than what the the low-tech

625
00:25:57,490 --> 00:26:01,520
stuff we were using in the past but

626
00:25:59,930 --> 00:26:04,760
I don't know I guess we'll have to wait

627
00:26:01,520 --> 00:26:08,690
and see but I don't wanna get too far

628
00:26:04,760 --> 00:26:12,010
ahead so so we were talking about the

629
00:26:08,690 --> 00:26:14,450
challenges with autonomous cars you know

630
00:26:12,010 --> 00:26:16,700
one of the biggest challenges I see it's

631
00:26:14,450 --> 00:26:20,090
not really a security issue but it's all

632
00:26:16,700 --> 00:26:22,010
the sensor sensor integration or what we

633
00:26:20,090 --> 00:26:22,879
call we call it sensor fusion I think so

634
00:26:22,010 --> 00:26:25,160
we have right we have all these

635
00:26:22,880 --> 00:26:27,920
different sensors looking at different

636
00:26:25,160 --> 00:26:31,160
things and the attack vectors that are

637
00:26:27,920 --> 00:26:32,750
coming in on those are are all new to us

638
00:26:31,160 --> 00:26:36,140
we've never seen this kind of stuff

639
00:26:32,750 --> 00:26:38,030
before you know in our simple mind you

640
00:26:36,140 --> 00:26:39,950
know I can I guess I can say this is a

641
00:26:38,030 --> 00:26:42,290
GM's already published fiction with this

642
00:26:39,950 --> 00:26:43,670
car but we have light our units right on

643
00:26:42,290 --> 00:26:45,290
the top of our car we thought well what

644
00:26:43,670 --> 00:26:47,300
happens if you point a laser pointer to

645
00:26:45,290 --> 00:26:49,040
it well that really tips it off what we

646
00:26:47,300 --> 00:26:51,050
found out that it's really not that easy

647
00:26:49,040 --> 00:26:53,180
so kind of like what you're talking

648
00:26:51,050 --> 00:26:56,600
about before we're doing you know reason

649
00:26:53,180 --> 00:26:58,790
like nist 800 - 800 - 30 risk assessment

650
00:26:56,600 --> 00:27:00,139
type philosophies and we're just going

651
00:26:58,790 --> 00:27:01,520
through it and going through all this

652
00:27:00,140 --> 00:27:03,050
stuff to find out what's really bad

653
00:27:01,520 --> 00:27:06,290
what's really not let's work on all the

654
00:27:03,050 --> 00:27:07,700
really bad stuff but there's a whole

655
00:27:06,290 --> 00:27:10,040
slew of stuff that i could like talk

656
00:27:07,700 --> 00:27:12,860
about for hours on the new use cases

657
00:27:10,040 --> 00:27:14,210
with autonomous cars the new technology

658
00:27:12,860 --> 00:27:16,250
that we've got to try and figure out how

659
00:27:14,210 --> 00:27:18,200
to secure and and i think one of my

660
00:27:16,250 --> 00:27:20,780
concerns like you were saying is this

661
00:27:18,200 --> 00:27:22,850
technology and the customer demand is

662
00:27:20,780 --> 00:27:26,960
moving faster than what we can secure it

663
00:27:22,850 --> 00:27:28,820
and and stay ahead of so I think that's

664
00:27:26,960 --> 00:27:32,000
my that's what keeps me up at night and

665
00:27:28,820 --> 00:27:38,990
I did start this job with a full head of

666
00:27:32,000 --> 00:27:42,200
dark hair y'all long just started a job

667
00:27:38,990 --> 00:27:47,960
but hopefully I'll have like go ahead of

668
00:27:42,200 --> 00:27:49,940
like a well alright oh but as I said I'm

669
00:27:47,960 --> 00:27:52,280
a new professor here in industrial

670
00:27:49,940 --> 00:27:54,140
engineering and I'm coming actually from

671
00:27:52,280 --> 00:27:56,060
a different side of things so my

672
00:27:54,140 --> 00:27:58,370
background is in human factors and

673
00:27:56,060 --> 00:28:01,280
ergonomics where we are looking at how

674
00:27:58,370 --> 00:28:06,429
do we design technologies interfaces

675
00:28:01,280 --> 00:28:06,430
display and got you in the back ok

676
00:28:06,960 --> 00:28:13,180
okay okay so how can we can year mean

677
00:28:10,690 --> 00:28:16,570
okay how do we design technology

678
00:28:13,180 --> 00:28:19,350
displays interfaces to support users to

679
00:28:16,570 --> 00:28:22,179
support their limitations but also

680
00:28:19,350 --> 00:28:24,760
capitalize on their strengths okay and I

681
00:28:22,180 --> 00:28:26,290
am primarily interested in human

682
00:28:24,760 --> 00:28:29,530
automation interaction which is the

683
00:28:26,290 --> 00:28:31,420
topic of this panel but in this idea of

684
00:28:29,530 --> 00:28:33,580
cognitive ergonomics so we're looking at

685
00:28:31,420 --> 00:28:36,670
how do we design things centered around

686
00:28:33,580 --> 00:28:38,980
how people think and of course in our

687
00:28:36,670 --> 00:28:40,570
field human factors and ergonomics for

688
00:28:38,980 --> 00:28:42,280
years now there's been a lot of talk

689
00:28:40,570 --> 00:28:46,210
about what is this going to look like

690
00:28:42,280 --> 00:28:48,250
cyber security possible hack end they

691
00:28:46,210 --> 00:28:50,410
talked about in aviation for example and

692
00:28:48,250 --> 00:28:52,660
now it is moving definitely into the

693
00:28:50,410 --> 00:28:54,130
direction of vehicles but one of the

694
00:28:52,660 --> 00:28:56,860
things that we've tried to do is really

695
00:28:54,130 --> 00:28:58,810
try to think about what can we learn

696
00:28:56,860 --> 00:29:01,060
what can we take away from other domains

697
00:28:58,810 --> 00:29:03,970
okay so if we look at a VA ssin for

698
00:29:01,060 --> 00:29:07,450
example when they introduced automated

699
00:29:03,970 --> 00:29:10,600
flight systems to cockpit there were two

700
00:29:07,450 --> 00:29:12,550
things there was this paper in the 70s

701
00:29:10,600 --> 00:29:14,560
that was called ironies of automation

702
00:29:12,550 --> 00:29:17,770
and there were two tasks that were left

703
00:29:14,560 --> 00:29:19,960
for humans to do once these highly

704
00:29:17,770 --> 00:29:22,690
automated systems were introduced that

705
00:29:19,960 --> 00:29:24,220
was to constantly monitor the system to

706
00:29:22,690 --> 00:29:26,200
make sure it was functioning the way

707
00:29:24,220 --> 00:29:28,420
it's supposed to and the other thing was

708
00:29:26,200 --> 00:29:31,810
to take over or to intervene whenever

709
00:29:28,420 --> 00:29:34,690
the automation got stuck or whenever it

710
00:29:31,810 --> 00:29:36,909
wasn't programmed to do things then the

711
00:29:34,690 --> 00:29:38,740
human needed to step in and so we're

712
00:29:36,910 --> 00:29:40,840
kind of using that same train of thought

713
00:29:38,740 --> 00:29:42,400
and especially in this interim period to

714
00:29:40,840 --> 00:29:45,550
think about what is that going to look

715
00:29:42,400 --> 00:29:47,440
like for autonomous vehicles and so in

716
00:29:45,550 --> 00:29:51,520
human factors and cognitive ergonomics

717
00:29:47,440 --> 00:29:53,620
we tried to use display design to divide

718
00:29:51,520 --> 00:29:55,240
people's attention terms of interruption

719
00:29:53,620 --> 00:29:57,879
management attention management how do

720
00:29:55,240 --> 00:29:59,440
we capture someone attention if they're

721
00:29:57,880 --> 00:30:01,330
doing a task how do we capture their

722
00:29:59,440 --> 00:30:03,940
attention can you have them switch

723
00:30:01,330 --> 00:30:05,500
perform that task and switch back to a

724
00:30:03,940 --> 00:30:08,800
class that they're doing okay and we can

725
00:30:05,500 --> 00:30:11,140
think about this for the hackin example

726
00:30:08,800 --> 00:30:12,970
let's just say that someone is driving

727
00:30:11,140 --> 00:30:15,280
down the highway and all of a sudden the

728
00:30:12,970 --> 00:30:16,960
vehicle is going in a direction that

729
00:30:15,280 --> 00:30:17,450
they didn't anticipate or that it wasn't

730
00:30:16,960 --> 00:30:20,899
program

731
00:30:17,450 --> 00:30:23,570
for so as human factors researchers one

732
00:30:20,899 --> 00:30:26,389
of our responsibilities is to figure out

733
00:30:23,570 --> 00:30:28,928
how do we number one detected a threat

734
00:30:26,389 --> 00:30:31,699
hat hacker or that a threat has occurred

735
00:30:28,929 --> 00:30:34,820
alert the person about what had happened

736
00:30:31,700 --> 00:30:36,320
and then if possible give them some kind

737
00:30:34,820 --> 00:30:38,090
of decision tool to help them overcome

738
00:30:36,320 --> 00:30:38,600
it okay and when I thought about this

739
00:30:38,090 --> 00:30:40,850
last night

740
00:30:38,600 --> 00:30:43,428
one of the things I was thinking about

741
00:30:40,850 --> 00:30:45,620
is the Chase Bank example so I have

742
00:30:43,429 --> 00:30:47,899
Chase and there have been two separate

743
00:30:45,620 --> 00:30:50,149
times where there was some fraudulent

744
00:30:47,899 --> 00:30:52,610
activity on my account and I immediately

745
00:30:50,149 --> 00:30:55,789
got alert I got an alert saying is this

746
00:30:52,610 --> 00:30:58,279
you if it is you know click yes if not

747
00:30:55,789 --> 00:31:01,490
then click no and then the account was

748
00:30:58,279 --> 00:31:03,169
completely frozen ok and then a new card

749
00:31:01,490 --> 00:31:06,250
had to be ordered so on and so forth and

750
00:31:03,169 --> 00:31:08,720
so my thinking is how can we now

751
00:31:06,250 --> 00:31:10,250
implement that into these these

752
00:31:08,720 --> 00:31:12,470
autonomous vehicles that kind of

753
00:31:10,250 --> 00:31:14,210
structure ok well we're not trusting the

754
00:31:12,470 --> 00:31:16,490
person to be able to detect a threat on

755
00:31:14,210 --> 00:31:18,429
their own but where the system as we

756
00:31:16,490 --> 00:31:21,470
continue to increase complexity and

757
00:31:18,429 --> 00:31:23,809
ending and these systems become more

758
00:31:21,470 --> 00:31:25,490
intelligent they will I believe get to

759
00:31:23,809 --> 00:31:27,559
the point where they can detect threat

760
00:31:25,490 --> 00:31:30,590
and then our question again as human

761
00:31:27,559 --> 00:31:33,649
factors specialists are is - how do we

762
00:31:30,590 --> 00:31:35,658
alert the person give them some kind of

763
00:31:33,649 --> 00:31:39,850
decision assistance decision support to

764
00:31:35,659 --> 00:31:42,200
where they can kind of recoup from that

765
00:31:39,850 --> 00:31:44,600
some other things I'm interested in more

766
00:31:42,200 --> 00:31:46,309
broadly are aging and Technology so a

767
00:31:44,600 --> 00:31:48,918
lot of the research that I'm pursuing

768
00:31:46,309 --> 00:31:53,480
has to do with as we get older again how

769
00:31:48,919 --> 00:31:55,700
do we how do we design for older adult

770
00:31:53,480 --> 00:31:56,960
limitations but also make use of the

771
00:31:55,700 --> 00:31:58,909
strengths that they have a lot of the

772
00:31:56,960 --> 00:31:59,600
times I think we don't really think

773
00:31:58,909 --> 00:32:01,220
about that

774
00:31:59,600 --> 00:32:03,559
but one of the questions that have come

775
00:32:01,220 --> 00:32:04,880
up for example people talk about how

776
00:32:03,559 --> 00:32:07,158
older adults don't want to use

777
00:32:04,880 --> 00:32:09,980
technologies they don't want to they

778
00:32:07,159 --> 00:32:11,870
don't want to pay bills online they

779
00:32:09,980 --> 00:32:13,820
don't want to use cell phones and that's

780
00:32:11,870 --> 00:32:15,860
definitely not true ok there is a

781
00:32:13,820 --> 00:32:17,899
segment of older adults who agree with

782
00:32:15,860 --> 00:32:19,309
that and say well you know what with all

783
00:32:17,899 --> 00:32:20,600
the things that I'm saying all the news

784
00:32:19,309 --> 00:32:21,860
with all of the things we're talking

785
00:32:20,600 --> 00:32:23,959
about it this symposium

786
00:32:21,860 --> 00:32:27,709
maybe I'll just continue to mail my

787
00:32:23,960 --> 00:32:29,539
bills paper paper paper wise right so

788
00:32:27,710 --> 00:32:30,590
that is some concern how do we build up

789
00:32:29,539 --> 00:32:32,320
trust and automate

790
00:32:30,590 --> 00:32:34,639
how do we build up trust in technology

791
00:32:32,320 --> 00:32:37,668
with them for example that's a research

792
00:32:34,640 --> 00:32:39,529
area people are looking at and then I

793
00:32:37,669 --> 00:32:41,960
just want to go back to this idea about

794
00:32:39,529 --> 00:32:45,289
not relying on people to be able to

795
00:32:41,960 --> 00:32:46,850
necessarily self detect threats okay as

796
00:32:45,289 --> 00:32:49,960
computer scientists as industrial

797
00:32:46,850 --> 00:32:53,178
engineers we really need to be able to

798
00:32:49,960 --> 00:32:56,480
do the best that we can to detect threat

799
00:32:53,179 --> 00:32:58,669
and inform the person so I think just

800
00:32:56,480 --> 00:33:00,080
going from here one thing I like about

801
00:32:58,669 --> 00:33:02,570
this job is that it is very

802
00:33:00,080 --> 00:33:04,039
collaborative and so this is not a

803
00:33:02,570 --> 00:33:06,559
problem that's going to be solved with

804
00:33:04,039 --> 00:33:08,510
just one field computer scientists won't

805
00:33:06,559 --> 00:33:10,340
be able to do it all alone human factors

806
00:33:08,510 --> 00:33:11,899
people won't be able to do it all along

807
00:33:10,340 --> 00:33:13,789
and so as long as we're bringing

808
00:33:11,899 --> 00:33:15,979
together those perspectives and thinking

809
00:33:13,789 --> 00:33:18,230
about how do we view the issue how do

810
00:33:15,980 --> 00:33:19,820
you view the issue and get centered

811
00:33:18,230 --> 00:33:24,190
around that I think we can make some

812
00:33:19,820 --> 00:33:24,189
good progress and I'll leave it at that

813
00:33:26,289 --> 00:33:33,799
how about now can you hear me now how

814
00:33:32,179 --> 00:33:35,809
about that I'm going to actually I'm

815
00:33:33,799 --> 00:33:41,240
going to head down there still this for

816
00:33:35,809 --> 00:33:42,529
me hear me okay just because I can't see

817
00:33:41,240 --> 00:33:43,580
the slides very well and I should be

818
00:33:42,529 --> 00:33:47,450
able to because I'm young but I have

819
00:33:43,580 --> 00:33:50,299
terrible eyesight so yeah so my name is

820
00:33:47,450 --> 00:33:52,549
Charleston Co I work in analog not

821
00:33:50,299 --> 00:33:54,590
really that important for this but I've

822
00:33:52,549 --> 00:33:57,740
been in the trenches for the past couple

823
00:33:54,590 --> 00:34:00,889
years looking at vehicle security and

824
00:33:57,740 --> 00:34:03,529
actually reverse engineering vehicles so

825
00:34:00,890 --> 00:34:05,120
I thought why not look at it from a

826
00:34:03,529 --> 00:34:08,449
little bit of historical perspective of

827
00:34:05,120 --> 00:34:10,969
where we've been speaking of security

828
00:34:08,449 --> 00:34:14,270
right you know Ford Model T and the

829
00:34:10,969 --> 00:34:16,219
DeLorean from a security perspective the

830
00:34:14,270 --> 00:34:17,989
threats kind of stay the same right

831
00:34:16,219 --> 00:34:21,230
we've just got a physical asset that you

832
00:34:17,989 --> 00:34:23,000
have to protect maybe you know

833
00:34:21,230 --> 00:34:25,580
convertible and someone could hop in and

834
00:34:23,000 --> 00:34:26,929
try to steal the car or you have to you

835
00:34:25,580 --> 00:34:29,629
know pick the lock the keys or whatever

836
00:34:26,929 --> 00:34:31,310
but the difference between these two

837
00:34:29,629 --> 00:34:33,109
really gets to be the functional safety

838
00:34:31,310 --> 00:34:35,179
aspect and that's you know something

839
00:34:33,109 --> 00:34:37,429
that was mentioned earlier is that you

840
00:34:35,179 --> 00:34:41,030
know functional safety for vehicles has

841
00:34:37,429 --> 00:34:43,580
improved drastically over the years but

842
00:34:41,030 --> 00:34:44,450
we look at today you know we still have

843
00:34:43,580 --> 00:34:47,449
that

844
00:34:44,449 --> 00:34:50,689
proved functional safety and the feature

845
00:34:47,449 --> 00:34:51,980
set has drastically improved as well you

846
00:34:50,690 --> 00:34:54,139
know we get advanced driver assistance

847
00:34:51,980 --> 00:34:56,839
service yeah advanced driver assistance

848
00:34:54,139 --> 00:34:59,180
systems but at the same time you know

849
00:34:56,839 --> 00:35:02,029
we're looking at Charlie Miller and

850
00:34:59,180 --> 00:35:04,390
Chris valasek and the G pack and you

851
00:35:02,030 --> 00:35:07,430
know the unintentional acceleration and

852
00:35:04,390 --> 00:35:09,890
the irony is you know working though EMS

853
00:35:07,430 --> 00:35:12,379
and getting really interesting questions

854
00:35:09,890 --> 00:35:14,690
from them about why do I care about

855
00:35:12,380 --> 00:35:17,089
security in some of these systems it's

856
00:35:14,690 --> 00:35:19,760
it really comes down to yes you have

857
00:35:17,089 --> 00:35:22,880
functional safety in my mind yes you

858
00:35:19,760 --> 00:35:25,250
have that but you security doesn't

859
00:35:22,880 --> 00:35:26,750
completely fall underneath that all

860
00:35:25,250 --> 00:35:28,760
right functional safety is about the

861
00:35:26,750 --> 00:35:31,970
vehicle in motion and protecting the

862
00:35:28,760 --> 00:35:34,940
inhabitants of the vehicle but what

863
00:35:31,970 --> 00:35:36,919
about when the vehicles off and you may

864
00:35:34,940 --> 00:35:39,829
have something that's you know calling

865
00:35:36,920 --> 00:35:41,390
home and getting new a diagnostic

866
00:35:39,829 --> 00:35:44,630
information or getting new you know

867
00:35:41,390 --> 00:35:46,368
software download or you know having to

868
00:35:44,630 --> 00:35:47,810
forbid a hacker breaking into it and

869
00:35:46,369 --> 00:35:49,160
installing ransomware and now you're

870
00:35:47,810 --> 00:35:52,910
paying $10 every single time you turn

871
00:35:49,160 --> 00:35:55,549
your car on that kind of terrifies me a

872
00:35:52,910 --> 00:35:58,399
little bit and you know the remote

873
00:35:55,550 --> 00:36:01,460
threats now have really increased as as

874
00:35:58,400 --> 00:36:04,730
the that the attack surface has grown is

875
00:36:01,460 --> 00:36:08,270
we get a lot of remote services for the

876
00:36:04,730 --> 00:36:09,380
vehicles but and I think this has been

877
00:36:08,270 --> 00:36:11,869
mentioned several times already

878
00:36:09,380 --> 00:36:14,180
IP security tools they don't map to this

879
00:36:11,869 --> 00:36:15,920
domain they don't make any sense here

880
00:36:14,180 --> 00:36:18,770
right you're talking about a cyber

881
00:36:15,920 --> 00:36:21,740
physical system not a network of web

882
00:36:18,770 --> 00:36:25,339
services even though they might be using

883
00:36:21,740 --> 00:36:26,839
something so yeah the threats here are a

884
00:36:25,339 --> 00:36:30,230
little bit different the loss is

885
00:36:26,839 --> 00:36:33,828
different right if a machine gets hacked

886
00:36:30,230 --> 00:36:35,300
in the current IT system you're losing

887
00:36:33,829 --> 00:36:37,089
maybe your intellectual property maybe

888
00:36:35,300 --> 00:36:41,000
your data no one's going to die

889
00:36:37,089 --> 00:36:43,849
hopefully but here you're talking about

890
00:36:41,000 --> 00:36:45,710
the potential loss of life and it gets

891
00:36:43,849 --> 00:36:47,810
really really scary at that point right

892
00:36:45,710 --> 00:36:51,440
because now you're not just talking

893
00:36:47,810 --> 00:36:53,180
about a car that had the ability for the

894
00:36:51,440 --> 00:36:55,960
driver to break and you know getting

895
00:36:53,180 --> 00:36:57,950
into the future with autonomous vehicles

896
00:36:55,960 --> 00:37:00,160
what about an attacker to

897
00:36:57,950 --> 00:37:02,930
and automating it to do their own will

898
00:37:00,160 --> 00:37:06,230
and driving it maybe accelerating it

899
00:37:02,930 --> 00:37:08,058
into some other vehicle or cutting off

900
00:37:06,230 --> 00:37:12,710
other vehicles and dangerous manner that

901
00:37:08,059 --> 00:37:14,599
humans would react poorly to and so you

902
00:37:12,710 --> 00:37:16,940
know what the future looks like you know

903
00:37:14,599 --> 00:37:19,309
si E's got their automation levels and

904
00:37:16,940 --> 00:37:21,799
we're getting close if not already

905
00:37:19,309 --> 00:37:25,400
getting there with Tesla and level five

906
00:37:21,799 --> 00:37:26,780
fully autonomous vehicles when human

907
00:37:25,400 --> 00:37:28,670
interaction is only necessary for

908
00:37:26,780 --> 00:37:31,880
turning the car on setting your

909
00:37:28,670 --> 00:37:33,740
destination and letting it go you know

910
00:37:31,880 --> 00:37:35,299
it seems really convenient but people

911
00:37:33,740 --> 00:37:37,368
don't pay attention and then you happen

912
00:37:35,299 --> 00:37:42,650
to get into automotive accidents that

913
00:37:37,369 --> 00:37:44,240
killed you referring specifically if not

914
00:37:42,650 --> 00:37:46,700
aware of the person that was killed in

915
00:37:44,240 --> 00:37:50,000
such laxen it was watching a video not

916
00:37:46,700 --> 00:37:51,799
paying attention to the road and then

917
00:37:50,000 --> 00:37:54,799
you get you know mobile apps are

918
00:37:51,799 --> 00:37:57,380
becoming more standard with vehicles of

919
00:37:54,799 --> 00:37:59,780
you know turning your car on unlocking

920
00:37:57,380 --> 00:38:01,730
it locating it that sort of thing and I

921
00:37:59,780 --> 00:38:02,809
can imagine at least that you know when

922
00:38:01,730 --> 00:38:04,520
we start getting fully autonomous

923
00:38:02,809 --> 00:38:06,589
vehicles you're going to be using your

924
00:38:04,520 --> 00:38:10,099
app like an uber app and you're gonna be

925
00:38:06,589 --> 00:38:11,599
calling it to your location what's to

926
00:38:10,099 --> 00:38:13,040
say that a hacker doesn't use that kind

927
00:38:11,599 --> 00:38:15,260
of a feature set to call it over the

928
00:38:13,040 --> 00:38:19,700
border and steal it

929
00:38:15,260 --> 00:38:21,700
right so the future kind of gets a

930
00:38:19,700 --> 00:38:23,720
little bit scary and concerning there

931
00:38:21,700 --> 00:38:24,890
and then we've got you know vehicle to

932
00:38:23,720 --> 00:38:26,890
everything yes it's been under

933
00:38:24,890 --> 00:38:30,650
development for the past several years

934
00:38:26,890 --> 00:38:32,480
but it's based off of stimuli from the

935
00:38:30,650 --> 00:38:34,190
infrastructure other vehicles to make

936
00:38:32,480 --> 00:38:35,630
decision granted that's not the only

937
00:38:34,190 --> 00:38:39,890
thing it uses to make decisions

938
00:38:35,630 --> 00:38:41,839
thankfully but if it began to use pieces

939
00:38:39,890 --> 00:38:43,430
and rely heavily on you know

940
00:38:41,839 --> 00:38:45,319
infrastructure to give it information

941
00:38:43,430 --> 00:38:47,808
about the traffic you could attacker

942
00:38:45,319 --> 00:38:49,460
could use that to reroute people through

943
00:38:47,809 --> 00:38:54,829
you know maybe a dangerous a

944
00:38:49,460 --> 00:38:55,940
neighborhood or you know lie about one

945
00:38:54,829 --> 00:38:59,390
of those vehicles that are there really

946
00:38:55,940 --> 00:39:02,049
aren't there and then you know we get to

947
00:38:59,390 --> 00:39:03,859
remote diagnostics and I think you know

948
00:39:02,049 --> 00:39:05,630
there's there's a lot of privacy

949
00:39:03,859 --> 00:39:08,119
concerns there right if someone could

950
00:39:05,630 --> 00:39:09,980
hack in and and pay attention to your

951
00:39:08,119 --> 00:39:11,300
Diagnostics they've gained enough

952
00:39:09,980 --> 00:39:13,670
information

953
00:39:11,300 --> 00:39:15,560
about the vehicle as long as they know

954
00:39:13,670 --> 00:39:19,340
where you started from they could build

955
00:39:15,560 --> 00:39:21,890
your entire path of travel right you

956
00:39:19,340 --> 00:39:24,560
have all of the inputs that the user

957
00:39:21,890 --> 00:39:25,850
gave to the vehicle even now or if it's

958
00:39:24,560 --> 00:39:27,500
fully autonomous you'd just be able to

959
00:39:25,850 --> 00:39:32,089
get where the target destination is and

960
00:39:27,500 --> 00:39:33,980
maybe even the GPS coordinate but you

961
00:39:32,090 --> 00:39:35,960
know how do we get to the future and I

962
00:39:33,980 --> 00:39:40,460
just you know some additional thoughts

963
00:39:35,960 --> 00:39:42,560
to add to it of automotive design we sit

964
00:39:40,460 --> 00:39:45,470
here thinking about security for it you

965
00:39:42,560 --> 00:39:48,170
know automotive is about five years out

966
00:39:45,470 --> 00:39:51,049
so if you're trying to design top down

967
00:39:48,170 --> 00:39:52,610
and you know integrating security in the

968
00:39:51,050 --> 00:39:54,950
design as part of the engineering

969
00:39:52,610 --> 00:39:57,470
process it's very difficult right

970
00:39:54,950 --> 00:39:59,990
because you know Miller valasek comes in

971
00:39:57,470 --> 00:40:01,609
and points out a really big problem and

972
00:39:59,990 --> 00:40:04,189
now we're trying to figure out how do we

973
00:40:01,610 --> 00:40:06,140
fit that in to the next model year so

974
00:40:04,190 --> 00:40:08,800
that we solve that problem in the future

975
00:40:06,140 --> 00:40:11,299
and now we're trying to wedge things in

976
00:40:08,800 --> 00:40:13,970
but for all other vehicles it's not

977
00:40:11,300 --> 00:40:18,290
going to really appear in the design for

978
00:40:13,970 --> 00:40:21,709
five years and then you can't oh yeah oh

979
00:40:18,290 --> 00:40:23,390
that so and then to Diagnostics this

980
00:40:21,710 --> 00:40:25,790
point us we have to remember right to

981
00:40:23,390 --> 00:40:27,080
repair loss which really makes things

982
00:40:25,790 --> 00:40:29,950
even more complicated because that

983
00:40:27,080 --> 00:40:33,230
requires Diagnostics to be open and

984
00:40:29,950 --> 00:40:35,600
allow anybody to do maintenance on their

985
00:40:33,230 --> 00:40:38,090
own vehicle so you know if we start

986
00:40:35,600 --> 00:40:40,220
adding security in and start resolving a

987
00:40:38,090 --> 00:40:42,860
lot of the really wide open

988
00:40:40,220 --> 00:40:46,490
vulnerabilities of being able to just

989
00:40:42,860 --> 00:40:48,080
replace firmware out on ECU's or not

990
00:40:46,490 --> 00:40:50,720
have signed code or something like that

991
00:40:48,080 --> 00:40:53,960
it how do you do it in such a way that

992
00:40:50,720 --> 00:40:56,029
the home user can modify their vehicle

993
00:40:53,960 --> 00:40:58,460
but only their vehicle and nobody else's

994
00:40:56,030 --> 00:41:01,370
vehicle and then is you know is there a

995
00:40:58,460 --> 00:41:05,090
way to define that the software running

996
00:41:01,370 --> 00:41:07,790
see you is part of the ECU and can't be

997
00:41:05,090 --> 00:41:09,890
replaced and thus you know part of the

998
00:41:07,790 --> 00:41:11,690
right to repair law then becomes well

999
00:41:09,890 --> 00:41:14,720
you can replace that ECU but not the

1000
00:41:11,690 --> 00:41:16,940
software that's on it so a little more

1001
00:41:14,720 --> 00:41:20,029
thought there and we'll Nitsa ever add

1002
00:41:16,940 --> 00:41:22,270
security to the five star rating you

1003
00:41:20,030 --> 00:41:24,810
know there's been discussion on that and

1004
00:41:22,270 --> 00:41:27,960
at the same time you know

1005
00:41:24,810 --> 00:41:29,880
assessment it's kind of subjective how

1006
00:41:27,960 --> 00:41:31,470
do you make it objective and how do you

1007
00:41:29,880 --> 00:41:33,720
make it objective in such a way that it

1008
00:41:31,470 --> 00:41:35,879
maps to all vehicle platforms in

1009
00:41:33,720 --> 00:41:38,299
existence right because you've got semis

1010
00:41:35,880 --> 00:41:39,930
which are different than you know

1011
00:41:38,300 --> 00:41:44,910
Corvettes which are going to be

1012
00:41:39,930 --> 00:41:48,690
different than an Impala from 19 what 72

1013
00:41:44,910 --> 00:41:50,279
72 and I also think you know the

1014
00:41:48,690 --> 00:41:52,620
business model is going to be challenged

1015
00:41:50,280 --> 00:41:54,420
here I know we're not necessarily

1016
00:41:52,620 --> 00:41:55,859
talking about business but I think the

1017
00:41:54,420 --> 00:41:57,420
business challenge can read this as well

1018
00:41:55,860 --> 00:42:00,110
and we challenged a little bit because

1019
00:41:57,420 --> 00:42:02,970
can't sell cars that we do normally

1020
00:42:00,110 --> 00:42:04,800
right it's now we have to start

1021
00:42:02,970 --> 00:42:07,259
packaging security in and security

1022
00:42:04,800 --> 00:42:08,520
updates for the life of vehicle and when

1023
00:42:07,260 --> 00:42:11,310
you think about the average last vehicle

1024
00:42:08,520 --> 00:42:13,560
is about 11 years right now becomes a

1025
00:42:11,310 --> 00:42:15,600
really hard problem to solve a what's

1026
00:42:13,560 --> 00:42:18,870
security going to look like in 11 years

1027
00:42:15,600 --> 00:42:22,110
you know where do you know we solved the

1028
00:42:18,870 --> 00:42:24,750
post quantum crypto problem in the

1029
00:42:22,110 --> 00:42:26,250
future and you know as was said earlier

1030
00:42:24,750 --> 00:42:29,490
I think it was Michael that mentioned it

1031
00:42:26,250 --> 00:42:31,020
the the fact that you know the

1032
00:42:29,490 --> 00:42:34,350
microcontrollers on the vehicles in the

1033
00:42:31,020 --> 00:42:36,870
ECU's are already maxed out now do we

1034
00:42:34,350 --> 00:42:39,660
have to start planning in for having the

1035
00:42:36,870 --> 00:42:41,460
ability to expand those they only have

1036
00:42:39,660 --> 00:42:45,509
maybe 50% utilization before we get

1037
00:42:41,460 --> 00:42:47,790
there and you know Microsoft and Apple

1038
00:42:45,510 --> 00:42:49,260
you know yes this is IT but they've

1039
00:42:47,790 --> 00:42:51,960
changed the way that you view software

1040
00:42:49,260 --> 00:42:54,390
you don't own it anymore you borrow it

1041
00:42:51,960 --> 00:42:55,890
and they will continue to provide you

1042
00:42:54,390 --> 00:42:59,370
security licenses you pay a subscription

1043
00:42:55,890 --> 00:43:01,230
fee and just a thought there I'm

1044
00:42:59,370 --> 00:43:04,529
optimistic that we can solve these

1045
00:43:01,230 --> 00:43:06,630
problems and ultimately get to a secure

1046
00:43:04,530 --> 00:43:07,920
vehicle platform but I think it's

1047
00:43:06,630 --> 00:43:10,500
something that's going to take a lot of

1048
00:43:07,920 --> 00:43:14,610
time and a lot of effort and needless to

1049
00:43:10,500 --> 00:43:17,730
say a lot of money and who gets to pay

1050
00:43:14,610 --> 00:43:20,640
for this you know piece is this do am

1051
00:43:17,730 --> 00:43:22,410
taking the cut or taking the hit which I

1052
00:43:20,640 --> 00:43:24,089
doubt or is it going to be the end

1053
00:43:22,410 --> 00:43:26,940
consumer that needs to really be

1054
00:43:24,090 --> 00:43:29,210
educated on understanding that I need to

1055
00:43:26,940 --> 00:43:31,890
pay for this because these feature sets

1056
00:43:29,210 --> 00:43:35,390
allow or hackers to do things they

1057
00:43:31,890 --> 00:43:35,390
weren't able to do in the past

1058
00:43:37,150 --> 00:43:45,230
thank you all and I think you have sent

1059
00:43:41,090 --> 00:43:48,350
a various levels of optimism for a

1060
00:43:45,230 --> 00:43:50,420
position statement so with the remaining

1061
00:43:48,350 --> 00:43:52,819
kind of the panel I would like to open

1062
00:43:50,420 --> 00:43:56,330
the floor so that I can so that the

1063
00:43:52,820 --> 00:44:00,170
audience can ask questions and so at

1064
00:43:56,330 --> 00:44:08,210
this time I would like to you know get

1065
00:44:00,170 --> 00:44:11,330
the questions from Charles you can't

1066
00:44:08,210 --> 00:44:14,150
answer this question my name is John

1067
00:44:11,330 --> 00:44:18,910
Walsh I'm with analog devices we are

1068
00:44:14,150 --> 00:44:23,060
going through a process internally to

1069
00:44:18,910 --> 00:44:26,470
develop a framework for a security

1070
00:44:23,060 --> 00:44:29,960
assurance program and we serve multiple

1071
00:44:26,470 --> 00:44:31,700
multiple sectors and one of the concerns

1072
00:44:29,960 --> 00:44:33,500
we have is we're looking at automotive

1073
00:44:31,700 --> 00:44:36,109
and other connected vehicles is

1074
00:44:33,500 --> 00:44:38,840
especially for some of these safety

1075
00:44:36,110 --> 00:44:42,380
critical functions what is the thought

1076
00:44:38,840 --> 00:44:45,490
process around I may detect that we have

1077
00:44:42,380 --> 00:44:48,800
an anomaly or we have a threat present

1078
00:44:45,490 --> 00:44:51,279
for example we get a break command and

1079
00:44:48,800 --> 00:44:55,250
the break a man goes to the brakes

1080
00:44:51,280 --> 00:44:58,010
what's the policy if we think it's it's

1081
00:44:55,250 --> 00:45:01,130
it may not be an authenticated command

1082
00:44:58,010 --> 00:45:03,380
so you know one of the big issues as

1083
00:45:01,130 --> 00:45:06,620
we're starting to get into this is it's

1084
00:45:03,380 --> 00:45:09,110
not just about security it's about do I

1085
00:45:06,620 --> 00:45:11,720
apply the brakes and if I don't apply

1086
00:45:09,110 --> 00:45:13,850
the brakes we may actually have a real

1087
00:45:11,720 --> 00:45:17,569
situation occurring and we have a

1088
00:45:13,850 --> 00:45:20,000
fatality occur because of security so

1089
00:45:17,570 --> 00:45:21,770
you know and we could talk about heart

1090
00:45:20,000 --> 00:45:26,420
monitors and all these other things

1091
00:45:21,770 --> 00:45:28,430
where functional safety versus critical

1092
00:45:26,420 --> 00:45:31,220
things you know and how do we address

1093
00:45:28,430 --> 00:45:34,759
those and what have you guys done in

1094
00:45:31,220 --> 00:45:36,830
terms of thinking through that and the

1095
00:45:34,760 --> 00:45:38,390
thought leadership when it comes to you

1096
00:45:36,830 --> 00:45:42,680
know how do we address those kind of

1097
00:45:38,390 --> 00:45:46,160
situations in our architectures just

1098
00:45:42,680 --> 00:45:48,740
I thought you said don't have me answer

1099
00:45:46,160 --> 00:45:50,180
it oh I guess I'll say guess that'll

1100
00:45:48,740 --> 00:45:52,339
look at one of the comics you said about

1101
00:45:50,180 --> 00:45:55,129
and you kind of answer your own question

1102
00:45:52,340 --> 00:45:56,600
and I paint in it right now I can't

1103
00:45:55,130 --> 00:45:58,490
necessarily do it with the technology we

1104
00:45:56,600 --> 00:46:00,860
have but it at some point we will have

1105
00:45:58,490 --> 00:46:02,350
authenticated messages right you just

1106
00:46:00,860 --> 00:46:04,610
like you said if I'm just going to

1107
00:46:02,350 --> 00:46:06,049
everybody probably here knows the can

1108
00:46:04,610 --> 00:46:08,360
networks on these vehicles is wide open

1109
00:46:06,050 --> 00:46:09,770
it's there's no security in it it's just

1110
00:46:08,360 --> 00:46:12,050
a wide oh it's a wild-west on there

1111
00:46:09,770 --> 00:46:13,730
right but there's ways we can

1112
00:46:12,050 --> 00:46:15,980
authenticate that stuff and we're moving

1113
00:46:13,730 --> 00:46:18,140
towards that we also built in

1114
00:46:15,980 --> 00:46:20,450
rationalities like you said like I was

1115
00:46:18,140 --> 00:46:22,520
saying earlier if you could suddenly get

1116
00:46:20,450 --> 00:46:23,960
this break command well where did it

1117
00:46:22,520 --> 00:46:26,150
come from did it come from it did it

1118
00:46:23,960 --> 00:46:27,740
come in from the right path because if

1119
00:46:26,150 --> 00:46:29,930
it came in for maybe what we'd consider

1120
00:46:27,740 --> 00:46:32,390
maybe our dirty side where our connected

1121
00:46:29,930 --> 00:46:34,279
components are I may not trust that guy

1122
00:46:32,390 --> 00:46:36,410
I'm not going to listen to that guy but

1123
00:46:34,280 --> 00:46:37,700
if it came in from this more private

1124
00:46:36,410 --> 00:46:39,890
network that has a little bit more

1125
00:46:37,700 --> 00:46:42,169
physical security on it maybe or a

1126
00:46:39,890 --> 00:46:44,420
little bit more redundancy on it I'll

1127
00:46:42,170 --> 00:46:45,620
listen to that guy but but the end

1128
00:46:44,420 --> 00:46:47,870
answer is it's got to be an

1129
00:46:45,620 --> 00:46:49,100
authenticated message and hope that you

1130
00:46:47,870 --> 00:46:51,500
have enough security around that

1131
00:46:49,100 --> 00:47:02,630
authentication that you believe it every

1132
00:46:51,500 --> 00:47:06,200
time okay so I agree I think that in the

1133
00:47:02,630 --> 00:47:09,140
DoD space there's a lot of thought on

1134
00:47:06,200 --> 00:47:12,680
this about unintended and attended

1135
00:47:09,140 --> 00:47:15,589
whenever whenever someone takes in no

1136
00:47:12,680 --> 00:47:18,799
unintended action ever happens and when

1137
00:47:15,590 --> 00:47:22,040
there is an appended action intended

1138
00:47:18,800 --> 00:47:23,840
accident it happens as well so it's both

1139
00:47:22,040 --> 00:47:25,340
the negative and positive sides there's

1140
00:47:23,840 --> 00:47:28,310
been a lot of thought around this but

1141
00:47:25,340 --> 00:47:32,420
ultimately it boils down to layers of

1142
00:47:28,310 --> 00:47:34,670
security you protect things you know at

1143
00:47:32,420 --> 00:47:38,710
every layer everything gets protected

1144
00:47:34,670 --> 00:47:41,990
all along the development cycle into

1145
00:47:38,710 --> 00:47:44,720
operation and maintenance and so forth

1146
00:47:41,990 --> 00:47:47,899
and when you build all the security then

1147
00:47:44,720 --> 00:47:49,970
ultimately your software has to do what

1148
00:47:47,900 --> 00:47:53,600
makes the most sense assuming you know

1149
00:47:49,970 --> 00:47:54,339
it's got a valid authenticated command

1150
00:47:53,600 --> 00:47:57,100
not

1151
00:47:54,340 --> 00:47:59,530
invalid authenticated command if I'm

1152
00:47:57,100 --> 00:48:04,569
making some throwing too many adjectives

1153
00:47:59,530 --> 00:48:07,170
data but there was a statement earlier

1154
00:48:04,570 --> 00:48:10,090
that with fully autonomous vehicles

1155
00:48:07,170 --> 00:48:12,070
someone expected that the frequency of

1156
00:48:10,090 --> 00:48:13,960
accidents would decrease but of the

1157
00:48:12,070 --> 00:48:15,790
accidents that remain the severity would

1158
00:48:13,960 --> 00:48:18,670
increase I was wondering what's the

1159
00:48:15,790 --> 00:48:22,540
intuition behind that that expectation

1160
00:48:18,670 --> 00:48:24,400
and do all the panelists agree you said

1161
00:48:22,540 --> 00:48:29,230
that maybe wouldn't I did I don't agree

1162
00:48:24,400 --> 00:48:31,260
with it no I'm basically I'm uh I read

1163
00:48:29,230 --> 00:48:34,330
The Economist and they've done a lot of

1164
00:48:31,260 --> 00:48:38,380
analysis on this and I think that's what

1165
00:48:34,330 --> 00:48:39,850
their conclusion has been ASET be you

1166
00:48:38,380 --> 00:48:41,740
know and I don't have their sources in

1167
00:48:39,850 --> 00:48:46,029
front of me that you can probably google

1168
00:48:41,740 --> 00:48:48,759
them you know the human error is going

1169
00:48:46,030 --> 00:48:50,250
to go way down one would think if a time

1170
00:48:48,760 --> 00:48:52,870
is over

1171
00:48:50,250 --> 00:48:54,310
it's more of an unknown when you have a

1172
00:48:52,870 --> 00:48:59,440
time with the correct crash that's

1173
00:48:54,310 --> 00:49:01,000
what's really going to occur yeah I read

1174
00:48:59,440 --> 00:49:03,270
and I would kind of disagree at least

1175
00:49:01,000 --> 00:49:05,620
from an autonomous vehicles perspective

1176
00:49:03,270 --> 00:49:07,060
you know we're not going to put a car on

1177
00:49:05,620 --> 00:49:10,630
the road that we don't feel safe to go

1178
00:49:07,060 --> 00:49:12,940
65 miles an hour so now can I stop some

1179
00:49:10,630 --> 00:49:15,640
crazy guy that wants an insurance claim

1180
00:49:12,940 --> 00:49:17,530
to slam into my car no and and I feel

1181
00:49:15,640 --> 00:49:19,629
bad about the uber story right because

1182
00:49:17,530 --> 00:49:21,190
could the uber car have gotten out of

1183
00:49:19,630 --> 00:49:23,080
the way of that I don't know but you

1184
00:49:21,190 --> 00:49:25,240
know it's unfortunate that that kind of

1185
00:49:23,080 --> 00:49:27,490
stuff hits the news and now it looks bad

1186
00:49:25,240 --> 00:49:29,649
from the autonomous perspective but I

1187
00:49:27,490 --> 00:49:32,560
just know that we're we know we've got a

1188
00:49:29,650 --> 00:49:33,790
very structured set of requirements so

1189
00:49:32,560 --> 00:49:35,799
we have to get to and we're slowly

1190
00:49:33,790 --> 00:49:37,570
ramping that up and if we don't feel

1191
00:49:35,800 --> 00:49:40,600
comfortable if we don't feel that that

1192
00:49:37,570 --> 00:49:42,700
car safer than a human on the road we're

1193
00:49:40,600 --> 00:49:45,520
not going to put it on the road and I

1194
00:49:42,700 --> 00:49:47,919
know that our mantra is going to be it's

1195
00:49:45,520 --> 00:49:50,400
going to have to be safer than another

1196
00:49:47,920 --> 00:49:52,690
than any other driver on the road I

1197
00:49:50,400 --> 00:49:54,490
don't think it'll be more horrific I

1198
00:49:52,690 --> 00:49:57,150
don't think it will be I don't think

1199
00:49:54,490 --> 00:50:00,009
well I think we'll see less accidents

1200
00:49:57,150 --> 00:50:01,270
but of the few that happen I don't think

1201
00:50:00,010 --> 00:50:03,760
there'll be any statistically

1202
00:50:01,270 --> 00:50:06,160
differences because it's I hope in my

1203
00:50:03,760 --> 00:50:07,539
mind that it's going to be not the

1204
00:50:06,160 --> 00:50:08,949
autonomous cars

1205
00:50:07,539 --> 00:50:11,079
you know it's going to be something else

1206
00:50:08,949 --> 00:50:14,829
hitting it something else out of the

1207
00:50:11,079 --> 00:50:15,759
blue that it couldn't avoid but I don't

1208
00:50:14,829 --> 00:50:20,789
think I don't think there'll be more

1209
00:50:15,759 --> 00:50:20,789
terrific okay thank you

1210
00:50:24,470 --> 00:50:28,259
Kurt Hadley with University of Illinois

1211
00:50:26,580 --> 00:50:28,680
first thank you thank you all for being

1212
00:50:28,260 --> 00:50:31,710
here

1213
00:50:28,680 --> 00:50:35,790
so my question as we look at autonomous

1214
00:50:31,710 --> 00:50:37,170
vehicles is that speed of reaction you

1215
00:50:35,790 --> 00:50:38,910
know we have farm equipment out there

1216
00:50:37,170 --> 00:50:41,310
now that's autonomous we have aircraft

1217
00:50:38,910 --> 00:50:43,680
that are autonomous but as a person who

1218
00:50:41,310 --> 00:50:45,360
spends a decent amount of time commuting

1219
00:50:43,680 --> 00:50:47,819
and has probably logged over 2 million

1220
00:50:45,360 --> 00:50:51,510
miles on the road now I look at those

1221
00:50:47,820 --> 00:50:53,520
bad decisions that people make whether

1222
00:50:51,510 --> 00:50:56,220
it is an evasive maneuver somebody

1223
00:50:53,520 --> 00:50:58,590
cutting them off in traffic idiot on the

1224
00:50:56,220 --> 00:51:01,470
phone whatever it might be and where do

1225
00:50:58,590 --> 00:51:05,220
we see that human machine interface that

1226
00:51:01,470 --> 00:51:13,680
helps those decisions improve and not

1227
00:51:05,220 --> 00:51:15,089
get worse if I have remembering what a

1228
00:51:13,680 --> 00:51:18,210
lot of people in this bill have been

1229
00:51:15,090 --> 00:51:21,540
talking about is that the human is still

1230
00:51:18,210 --> 00:51:24,180
there ok they're not the primary in 190

1231
00:51:21,540 --> 00:51:28,140
things even speed they're there but

1232
00:51:24,180 --> 00:51:30,000
they're there oh yeah they're there but

1233
00:51:28,140 --> 00:51:31,529
they're there in the backups case and I

1234
00:51:30,000 --> 00:51:32,880
know a lot of people hate probably need

1235
00:51:31,530 --> 00:51:35,310
to hear me say that but it's true and

1236
00:51:32,880 --> 00:51:36,990
we're saying in this interim period I'm

1237
00:51:35,310 --> 00:51:39,090
not talking about 20 years out when

1238
00:51:36,990 --> 00:51:41,729
we've got this thing under control and

1239
00:51:39,090 --> 00:51:43,290
it's possible that the human may not be

1240
00:51:41,730 --> 00:51:45,360
there that's what we're taught that

1241
00:51:43,290 --> 00:51:48,720
s'what people are looking to but we're

1242
00:51:45,360 --> 00:51:50,880
saying from now and for the next 20

1243
00:51:48,720 --> 00:51:52,680
years we're feeling that is critical to

1244
00:51:50,880 --> 00:51:55,860
leave the human there in terms of being

1245
00:51:52,680 --> 00:51:57,660
able to monitor and take over and just

1246
00:51:55,860 --> 00:52:00,660
just the mere fact of having them

1247
00:51:57,660 --> 00:52:03,690
sitting there monitoring is and having

1248
00:52:00,660 --> 00:52:05,009
the vehicle kind of do on its own will

1249
00:52:03,690 --> 00:52:07,530
reduce accidents like we're talking

1250
00:52:05,010 --> 00:52:09,600
about what we're thinking about is how

1251
00:52:07,530 --> 00:52:11,520
do we know if the person is engaged in a

1252
00:52:09,600 --> 00:52:13,680
different task if they're now have the

1253
00:52:11,520 --> 00:52:17,700
ability to text start to eat or to fall

1254
00:52:13,680 --> 00:52:19,319
asleep how do we create displays and I

1255
00:52:17,700 --> 00:52:21,450
didn't talk about this but we use these

1256
00:52:19,320 --> 00:52:24,870
multimodal displays basically a light

1257
00:52:21,450 --> 00:52:27,779
sound and vibrations okay to present

1258
00:52:24,870 --> 00:52:29,819
information how can we support situation

1259
00:52:27,780 --> 00:52:32,010
awareness so basically being able to

1260
00:52:29,820 --> 00:52:34,020
alert people at different times about

1261
00:52:32,010 --> 00:52:35,640
what the vehicle is doing to give them

1262
00:52:34,020 --> 00:52:36,790
enough time to take over so there have

1263
00:52:35,640 --> 00:52:38,049
been a lot of studies

1264
00:52:36,790 --> 00:52:39,880
that have looked at how long does it

1265
00:52:38,050 --> 00:52:41,770
take someone to take over control of a

1266
00:52:39,880 --> 00:52:45,520
vehicle and a lot of people are saying

1267
00:52:41,770 --> 00:52:48,670
that the time is so large in terms of in

1268
00:52:45,520 --> 00:52:50,650
terms of milliseconds that we can't get

1269
00:52:48,670 --> 00:52:52,720
it down short enough for a person to be

1270
00:52:50,650 --> 00:52:55,450
able to reliably take over and so what

1271
00:52:52,720 --> 00:52:57,279
we're saying is as our Gothamist well

1272
00:52:55,450 --> 00:52:59,169
then how can we keep the person engaged

1273
00:52:57,280 --> 00:53:00,850
how can we provide them with the right

1274
00:52:59,170 --> 00:53:02,770
situation awareness about what the

1275
00:53:00,850 --> 00:53:05,560
vehicle is doing and alert them

1276
00:53:02,770 --> 00:53:07,540
previously okay not not right when -

1277
00:53:05,560 --> 00:53:10,480
when - mode fails or when the automation

1278
00:53:07,540 --> 00:53:12,100
fails but how do we give them that and

1279
00:53:10,480 --> 00:53:15,330
enough time to be able to handle it I

1280
00:53:12,100 --> 00:53:15,330
don't know if the answers the question

1281
00:53:16,020 --> 00:53:20,140
well let me try I guess because you know

1282
00:53:18,460 --> 00:53:22,090
your question was about fully autonomous

1283
00:53:20,140 --> 00:53:23,140
vehicles and I'd say my CEO would

1284
00:53:22,090 --> 00:53:25,480
disagree that we're going to have

1285
00:53:23,140 --> 00:53:30,850
autonomous cars on the road driverless

1286
00:53:25,480 --> 00:53:33,280
well before before that but gives your

1287
00:53:30,850 --> 00:53:34,960
question of the reaction time so they're

1288
00:53:33,280 --> 00:53:37,510
telling us now again I don't design the

1289
00:53:34,960 --> 00:53:38,770
car or not I'm just hoping I'm securing

1290
00:53:37,510 --> 00:53:40,960
the car well enough but they're telling

1291
00:53:38,770 --> 00:53:43,780
us they think that our reaction time of

1292
00:53:40,960 --> 00:53:45,100
actually doing something like send the

1293
00:53:43,780 --> 00:53:47,770
steering command sending a break command

1294
00:53:45,100 --> 00:53:50,529
might be slightly less than what a what

1295
00:53:47,770 --> 00:53:51,940
a human could probably do it in where

1296
00:53:50,530 --> 00:53:53,830
we're going to make up that time however

1297
00:53:51,940 --> 00:53:56,020
is we're going to have a full view of

1298
00:53:53,830 --> 00:53:57,430
everything around us right so you don't

1299
00:53:56,020 --> 00:53:59,950
have eyes in the back your head I'm

1300
00:53:57,430 --> 00:54:01,600
probably do I've Drive a lot but I know

1301
00:53:59,950 --> 00:54:03,609
I can't see what's Bartlett coming up

1302
00:54:01,600 --> 00:54:05,620
behind me the autonomous car will and

1303
00:54:03,610 --> 00:54:07,690
it'll be able to ready and react

1304
00:54:05,620 --> 00:54:08,980
accordingly to that so that's where we

1305
00:54:07,690 --> 00:54:12,760
keep thinking that we're going to get a

1306
00:54:08,980 --> 00:54:14,470
better overall reaction time you know

1307
00:54:12,760 --> 00:54:16,180
one of one of the questions that was

1308
00:54:14,470 --> 00:54:18,279
asked a while back was am I going to be

1309
00:54:16,180 --> 00:54:19,870
stuck behind one of these cars wait for

1310
00:54:18,280 --> 00:54:22,540
it and decide if it needs to turn left

1311
00:54:19,870 --> 00:54:24,130
or right I'll be honking at it no no I

1312
00:54:22,540 --> 00:54:25,779
don't think so right unless unless it's

1313
00:54:24,130 --> 00:54:27,400
unless it's one of these orange barrels

1314
00:54:25,780 --> 00:54:29,170
or whatever of a tree fell in front of

1315
00:54:27,400 --> 00:54:31,570
it and it doesn't know what to do but

1316
00:54:29,170 --> 00:54:34,150
from a reaction perspective I think will

1317
00:54:31,570 --> 00:54:36,430
be better than the human and again we'll

1318
00:54:34,150 --> 00:54:38,950
have a much better we'll have a much

1319
00:54:36,430 --> 00:54:40,450
better input and process all that input

1320
00:54:38,950 --> 00:54:42,250
much quicker and better than what I

1321
00:54:40,450 --> 00:54:45,339
think a human can do from so many

1322
00:54:42,250 --> 00:54:47,380
different sources vision radar lidar all

1323
00:54:45,340 --> 00:54:50,490
that stuff and be a vehicle to

1324
00:54:47,380 --> 00:54:50,490
infrastructure whatever it may be

1325
00:54:50,840 --> 00:54:55,380
hi I'm from ATI as well I have a

1326
00:54:53,730 --> 00:54:58,380
question about the infrastructure I mean

1327
00:54:55,380 --> 00:55:00,630
do we have like you know autonomous

1328
00:54:58,380 --> 00:55:01,760
self-driving lanes or self parking

1329
00:55:00,630 --> 00:55:04,680
garages

1330
00:55:01,760 --> 00:55:07,410
you know the problem with like mixing

1331
00:55:04,680 --> 00:55:09,779
autonomous cars with like you know human

1332
00:55:07,410 --> 00:55:12,810
driven cars is that human mix love can

1333
00:55:09,780 --> 00:55:15,810
make love errors and that could possibly

1334
00:55:12,810 --> 00:55:18,180
cause a lot more accidents as well when

1335
00:55:15,810 --> 00:55:20,670
you put autonomous driving cars with

1336
00:55:18,180 --> 00:55:22,140
self-driving cars and you know I presume

1337
00:55:20,670 --> 00:55:23,970
that there must be a switch for someone

1338
00:55:22,140 --> 00:55:26,430
to be able to switch between letting the

1339
00:55:23,970 --> 00:55:28,140
car drive autonomously and taking things

1340
00:55:26,430 --> 00:55:30,990
over so you could make that switch

1341
00:55:28,140 --> 00:55:33,240
anytime or presume so do we have to

1342
00:55:30,990 --> 00:55:34,979
infrastructure to support that I mean

1343
00:55:33,240 --> 00:55:38,040
you see a lot of sci-fi movies you know

1344
00:55:34,980 --> 00:55:40,170
cars driving vertically as well where

1345
00:55:38,040 --> 00:55:43,170
are we in that I mean do we have a plan

1346
00:55:40,170 --> 00:55:45,540
to get there like so I know GM is not

1347
00:55:43,170 --> 00:55:47,280
relying on any of that so we're going to

1348
00:55:45,540 --> 00:55:49,770
work on making us we're making an

1349
00:55:47,280 --> 00:55:51,960
autonomous vehicle and yeah it's at

1350
00:55:49,770 --> 00:55:52,950
first it's going to have a driver in it

1351
00:55:51,960 --> 00:55:54,870
but we're not going to expect the

1352
00:55:52,950 --> 00:55:56,189
passenger to be doing any of that kind

1353
00:55:54,870 --> 00:55:58,680
of stuff the passenger is going to get

1354
00:55:56,190 --> 00:56:00,360
in go to whatever destination they want

1355
00:55:58,680 --> 00:56:02,669
to do and we're never going to expect

1356
00:56:00,360 --> 00:56:04,350
them to take over the wheel or anything

1357
00:56:02,670 --> 00:56:05,820
like that because like you said

1358
00:56:04,350 --> 00:56:07,200
especially cuz they're going to be in

1359
00:56:05,820 --> 00:56:09,120
the backseat they're not going to have

1360
00:56:07,200 --> 00:56:11,339
time to jump up there and do anything

1361
00:56:09,120 --> 00:56:12,299
that's will probably be passed out right

1362
00:56:11,340 --> 00:56:15,150
because that's why they're there in the

1363
00:56:12,300 --> 00:56:17,730
first place but we don't expect anything

1364
00:56:15,150 --> 00:56:19,710
like that to take place so we will have

1365
00:56:17,730 --> 00:56:21,930
like where I see that we're going to

1366
00:56:19,710 --> 00:56:24,810
have manual driving is like in the

1367
00:56:21,930 --> 00:56:26,669
service bay or if I need to inch the car

1368
00:56:24,810 --> 00:56:28,650
a little bit over to get to a charging

1369
00:56:26,670 --> 00:56:30,710
station or a fueling station or

1370
00:56:28,650 --> 00:56:34,520
something like that so that's where

1371
00:56:30,710 --> 00:56:37,890
humans will get in the vehicle through a

1372
00:56:34,520 --> 00:56:39,300
authorized connection you know we'll put

1373
00:56:37,890 --> 00:56:40,650
the car into a manual mode because again

1374
00:56:39,300 --> 00:56:42,390
we have to secure that right we don't

1375
00:56:40,650 --> 00:56:44,340
want anybody just walking up flipping it

1376
00:56:42,390 --> 00:56:46,680
to main and cold hey look what I got

1377
00:56:44,340 --> 00:56:48,300
so some authorized person will get in

1378
00:56:46,680 --> 00:56:50,759
the vehicle put it an annual get it

1379
00:56:48,300 --> 00:56:53,400
where it needs to go and then once it's

1380
00:56:50,760 --> 00:56:55,050
done with its charging or whatever it

1381
00:56:53,400 --> 00:56:57,059
gets turned back into autonomous mode

1382
00:56:55,050 --> 00:57:00,659
and then they walk away from it and

1383
00:56:57,059 --> 00:57:03,179
it's out in the wild for what so do you

1384
00:57:00,659 --> 00:57:05,630
expect us to see say autonomous driving

1385
00:57:03,179 --> 00:57:12,119
lanes in the future like carpool lanes

1386
00:57:05,630 --> 00:57:14,999
yeah I would assume so so I've read

1387
00:57:12,119 --> 00:57:16,769
about studies really aimed at how do you

1388
00:57:14,999 --> 00:57:18,808
resolve the traffic problem in this

1389
00:57:16,769 --> 00:57:23,428
country and basically they say if we

1390
00:57:18,809 --> 00:57:25,409
have automated cars we can close the gap

1391
00:57:23,429 --> 00:57:27,329
between cars so they could maybe be

1392
00:57:25,409 --> 00:57:29,249
driving along it to see five miles an

1393
00:57:27,329 --> 00:57:32,699
hour or two inches of separation between

1394
00:57:29,249 --> 00:57:35,999
them you know and that's a really scary

1395
00:57:32,699 --> 00:57:38,969
thought but to be honest the technology

1396
00:57:35,999 --> 00:57:41,879
would probably support it right so the

1397
00:57:38,969 --> 00:57:44,729
question is is as a society do we want

1398
00:57:41,880 --> 00:57:48,059
to spend the money it would cost to get

1399
00:57:44,729 --> 00:57:49,828
there and as a society you know the

1400
00:57:48,059 --> 00:57:54,269
question we're not even asking right now

1401
00:57:49,829 --> 00:57:55,469
it's legislation and it all gets to you

1402
00:57:54,269 --> 00:57:57,149
know if you're going to have an

1403
00:57:55,469 --> 00:57:58,769
autonomous car you're going to be

1404
00:57:57,150 --> 00:58:01,709
required to go through special

1405
00:57:58,769 --> 00:58:03,238
inspections I mean it seems that would

1406
00:58:01,709 --> 00:58:08,249
be a natural thing to do but will the

1407
00:58:03,239 --> 00:58:10,199
legislation keep up with the technology

1408
00:58:08,249 --> 00:58:12,390
or enable the technology to do

1409
00:58:10,199 --> 00:58:14,609
everything the technology can do and you

1410
00:58:12,390 --> 00:58:17,038
know to solve our traffic problems to

1411
00:58:14,609 --> 00:58:19,078
solve our safety problems and security

1412
00:58:17,039 --> 00:58:20,969
problems and so forth I would just say

1413
00:58:19,079 --> 00:58:23,369
talking about kind of making the analogy

1414
00:58:20,969 --> 00:58:24,809
to the HOV Lane there's nothing special

1415
00:58:23,369 --> 00:58:26,400
about those other than a few added

1416
00:58:24,809 --> 00:58:28,439
reflectors on the ground

1417
00:58:26,400 --> 00:58:31,319
I don't again I don't think if there was

1418
00:58:28,439 --> 00:58:33,149
to be a special autonomous lane if they

1419
00:58:31,319 --> 00:58:34,499
put some kind of infrastructure like a

1420
00:58:33,150 --> 00:58:36,539
vehicle to infrastructure a vehicle

1421
00:58:34,499 --> 00:58:37,859
vehicle in it that's fine we'll probably

1422
00:58:36,539 --> 00:58:39,749
utilize it but I don't

1423
00:58:37,859 --> 00:58:41,939
GM is not expecting anything like that

1424
00:58:39,749 --> 00:58:44,999
to happen it's we get if we get our own

1425
00:58:41,939 --> 00:58:46,379
lane great we'll take it take advantage

1426
00:58:44,999 --> 00:58:48,419
take advantage of it right yeah because

1427
00:58:46,380 --> 00:58:51,029
I used to live in California and it you

1428
00:58:48,419 --> 00:58:54,390
know we used to drive a lot between LA

1429
00:58:51,029 --> 00:58:56,009
and Bay Area and different talks about

1430
00:58:54,390 --> 00:58:57,779
building like a bullet train for years

1431
00:58:56,009 --> 00:58:59,669
and years and years I mean we'll be nice

1432
00:58:57,779 --> 00:59:03,210
to have an autonomous driving lane where

1433
00:58:59,669 --> 00:59:09,368
you know you catch all the korva issss

1434
00:59:03,210 --> 00:59:09,940
all right so yeah I I really enjoyed the

1435
00:59:09,369 --> 00:59:13,359
discussion

1436
00:59:09,940 --> 00:59:15,609
I learned my a lot myself I was actually

1437
00:59:13,359 --> 00:59:18,098
thinking of asking about your opinion on

1438
00:59:15,609 --> 00:59:19,950
third-party reverse engineering and

1439
00:59:18,099 --> 00:59:23,320
hacking or vetting of a commodity

1440
00:59:19,950 --> 00:59:25,330
autonomous vehicle system but we run out

1441
00:59:23,320 --> 00:59:27,970
of time so I would like to continue our

1442
00:59:25,330 --> 00:59:30,250
discussion of line and so this also

1443
00:59:27,970 --> 00:59:33,250
wraps up the panel discussion on

1444
00:59:30,250 --> 00:59:35,859
security on amass a vehicle I hope you

1445
00:59:33,250 --> 00:59:39,690
enjoy it and thank you very much and

1446
00:59:35,859 --> 00:59:39,690
thanks our first install Kennelly

