1
00:00:28,779 --> 00:00:33,890
hello and welcome to this week's

2
00:00:31,550 --> 00:00:36,350
security seminar from serious at Purdue

3
00:00:33,890 --> 00:00:39,769
University our speaker this week is

4
00:00:36,350 --> 00:00:41,269
Professor Bruno Ribeiro who is a CS

5
00:00:39,769 --> 00:00:43,670
professor in the computer science

6
00:00:41,269 --> 00:00:46,280
department here at Purdue Bruno take a

7
00:00:43,670 --> 00:00:51,019
Joe thanks for inviting me for to give a

8
00:00:46,280 --> 00:00:56,989
talk at serious so here I am going to

9
00:00:51,019 --> 00:00:59,780
move forward I hope that's ok welcome

10
00:00:56,989 --> 00:01:05,689
and so today i'll talk about predicting

11
00:00:59,780 --> 00:01:07,580
what users will do next so if you if you

12
00:01:05,689 --> 00:01:10,369
bear with me let me go through first a

13
00:01:07,580 --> 00:01:12,408
general introduction so here we have an

14
00:01:10,369 --> 00:01:15,200
agent that is walking around the world

15
00:01:12,409 --> 00:01:18,079
right we have for instance this agent is

16
00:01:15,200 --> 00:01:19,939
walking around lafayette looking for a

17
00:01:18,079 --> 00:01:24,020
coffee shop and then go to maybe chill

18
00:01:19,939 --> 00:01:28,758
restaurant or is try to buy some tablet

19
00:01:24,020 --> 00:01:32,509
in in a store like amazon or ebay or

20
00:01:28,759 --> 00:01:34,850
bestbuy or maybe it's browsing for music

21
00:01:32,510 --> 00:01:37,610
is building a music list and it's

22
00:01:34,850 --> 00:01:41,658
listening to songs in this music this in

23
00:01:37,610 --> 00:01:44,360
this website so what these problems have

24
00:01:41,659 --> 00:01:46,520
in common is that once the user which is

25
00:01:44,360 --> 00:01:50,990
a place you want to know what the user

26
00:01:46,520 --> 00:01:52,908
will do next and so this is the general

27
00:01:50,990 --> 00:01:56,320
motivation so for instance online

28
00:01:52,909 --> 00:02:02,119
shopping we have a let's say here

29
00:01:56,320 --> 00:02:04,330
tablets and laptops and you can go to to

30
00:02:02,119 --> 00:02:07,159
one of them let's say an e-reader and

31
00:02:04,330 --> 00:02:11,420
then after you click here maybe you go

32
00:02:07,159 --> 00:02:15,470
check how much it costs on Barnes and

33
00:02:11,420 --> 00:02:17,809
Noble is a different equipment gift card

34
00:02:15,470 --> 00:02:21,799
so these are the users browsing these

35
00:02:17,810 --> 00:02:23,989
websites and and so the motivation here

36
00:02:21,799 --> 00:02:25,819
is to predict given the weight where

37
00:02:23,989 --> 00:02:30,349
they are and what they have done that we

38
00:02:25,819 --> 00:02:32,480
know what they will do next so this is

39
00:02:30,349 --> 00:02:35,480
the general problem is called predicting

40
00:02:32,480 --> 00:02:37,429
user to Jack Therese okay so this is a

41
00:02:35,480 --> 00:02:40,220
user trajectory and a security

42
00:02:37,430 --> 00:02:41,870
motivation for this would be

43
00:02:40,220 --> 00:02:45,680
for instance you have some access access

44
00:02:41,870 --> 00:02:49,040
by patterns we have a regular user who's

45
00:02:45,680 --> 00:02:51,110
accessing some files in in its company

46
00:02:49,040 --> 00:02:53,030
and a malicious user that has a

47
00:02:51,110 --> 00:02:56,180
different pattern of accessing this

48
00:02:53,030 --> 00:02:59,240
these files right and you want to

49
00:02:56,180 --> 00:03:02,390
separate or group the users that are

50
00:02:59,240 --> 00:03:04,250
regular users are against the user there

51
00:03:02,390 --> 00:03:08,149
are malicious that they have some weird

52
00:03:04,250 --> 00:03:11,030
or different type of access pattern so

53
00:03:08,150 --> 00:03:16,010
just want to group users another

54
00:03:11,030 --> 00:03:19,490
scenario is a where you have a regular

55
00:03:16,010 --> 00:03:21,790
user and I VIP user ok and the VIP user

56
00:03:19,490 --> 00:03:24,440
has access to some other documents and

57
00:03:21,790 --> 00:03:26,450
the patterns that you see from a regular

58
00:03:24,440 --> 00:03:29,260
user in your company should be different

59
00:03:26,450 --> 00:03:32,390
than the ones that you see from the CEO

60
00:03:29,260 --> 00:03:34,399
but then this account is compromised and

61
00:03:32,390 --> 00:03:36,500
now the regular users starting to behave

62
00:03:34,400 --> 00:03:38,990
like the CEO get it trying to get access

63
00:03:36,500 --> 00:03:41,870
to this important information about the

64
00:03:38,990 --> 00:03:44,780
company and that's when you realize that

65
00:03:41,870 --> 00:03:46,310
the user changed behavior it's not like

66
00:03:44,780 --> 00:03:48,560
a regular user anymore it's very

67
00:03:46,310 --> 00:03:52,040
different it behaves more like the CEO

68
00:03:48,560 --> 00:03:54,530
and now you can identify this anomalous

69
00:03:52,040 --> 00:04:00,340
behavior ok so there are the two

70
00:03:54,530 --> 00:04:03,350
motivations in security for this and

71
00:04:00,340 --> 00:04:09,380
more personal motivation which often is

72
00:04:03,350 --> 00:04:12,650
why we do research is I had a new book

73
00:04:09,380 --> 00:04:16,850
and I was sharing this ebook with

74
00:04:12,650 --> 00:04:18,649
someone else and whenever I was browsing

75
00:04:16,850 --> 00:04:21,109
so this is the type of books that I

76
00:04:18,649 --> 00:04:24,500
would would appear of my recommendation

77
00:04:21,108 --> 00:04:27,340
it was easily science books and then and

78
00:04:24,500 --> 00:04:31,270
then because of a sherry someone I would

79
00:04:27,340 --> 00:04:33,890
receive a lot of recommendations of

80
00:04:31,270 --> 00:04:36,950
books that I'd was not too interested in

81
00:04:33,890 --> 00:04:39,710
and and what was upsetting to me was

82
00:04:36,950 --> 00:04:42,650
that I was I was I would keep browsing

83
00:04:39,710 --> 00:04:45,229
the book that our interests me and the

84
00:04:42,650 --> 00:04:47,750
website would never really learn that I

85
00:04:45,229 --> 00:04:49,070
am very different my trajectory is very

86
00:04:47,750 --> 00:04:51,470
different from this other person that

87
00:04:49,070 --> 00:04:54,060
was sharing the e-book with me the other

88
00:04:51,470 --> 00:04:56,880
person would do a trajectory over

89
00:04:54,060 --> 00:04:58,919
very different types of works and the

90
00:04:56,880 --> 00:05:01,110
machine learning algorithm that the

91
00:04:58,919 --> 00:05:03,330
company was using it was not keeping up

92
00:05:01,110 --> 00:05:09,780
with this understanding of the

93
00:05:03,330 --> 00:05:11,460
trajectories so so here is me browsing

94
00:05:09,780 --> 00:05:18,090
and of course I'm not browsing the other

95
00:05:11,460 --> 00:05:20,489
books so and then when I did I started

96
00:05:18,090 --> 00:05:22,380
looking into more into the problem and a

97
00:05:20,490 --> 00:05:25,530
problem that I was already interested of

98
00:05:22,380 --> 00:05:27,120
course but there were no scalable Big

99
00:05:25,530 --> 00:05:31,309
Data methods to predict this type of

100
00:05:27,120 --> 00:05:31,310
navigation this type of trajectories and

101
00:05:31,700 --> 00:05:37,380
I was surprised and that was a 2015

102
00:05:34,979 --> 00:05:39,330
surprises the fact that in 2015 we still

103
00:05:37,380 --> 00:05:42,659
didn't have any scalable rhythms for

104
00:05:39,330 --> 00:05:45,450
this type of a trajectory prediction so

105
00:05:42,660 --> 00:05:46,700
so there are reasons for that and I'll

106
00:05:45,450 --> 00:05:50,570
show you some of the challenges

107
00:05:46,700 --> 00:05:52,830
regarding learning user trajectories

108
00:05:50,570 --> 00:05:57,150
specifically because of user behavior

109
00:05:52,830 --> 00:05:59,460
and that also ties back to security the

110
00:05:57,150 --> 00:06:01,859
way that people access files in the way

111
00:05:59,460 --> 00:06:06,299
that they behave in a company is quite

112
00:06:01,860 --> 00:06:08,490
complex so let me give you an example so

113
00:06:06,300 --> 00:06:13,080
one of the first challenges here is

114
00:06:08,490 --> 00:06:16,770
account for stationarity what is what is

115
00:06:13,080 --> 00:06:20,180
that so you can think the trajectory of

116
00:06:16,770 --> 00:06:23,400
the user is a mixture of what we call

117
00:06:20,180 --> 00:06:27,780
stationary processes so what what's at

118
00:06:23,400 --> 00:06:30,989
issue stationary process is we can think

119
00:06:27,780 --> 00:06:33,780
of a process or stochastic process or

120
00:06:30,990 --> 00:06:36,539
behavior right that does not depend on

121
00:06:33,780 --> 00:06:42,270
wall clock time so if you are browsing

122
00:06:36,539 --> 00:06:44,460
on on Amazon in the morning for a

123
00:06:42,270 --> 00:06:46,560
computer or if you're browsing on Amazon

124
00:06:44,460 --> 00:06:48,750
in the afternoon for a computer you're

125
00:06:46,560 --> 00:06:50,400
browsing behavior would be the same

126
00:06:48,750 --> 00:06:52,860
right because you are trying to do the

127
00:06:50,400 --> 00:06:54,060
same thing so that doesn't depend on

128
00:06:52,860 --> 00:06:56,669
wall clock time and that's what

129
00:06:54,060 --> 00:06:59,400
stationarity mix and also the same thing

130
00:06:56,669 --> 00:07:00,690
if you'd browsing today you would have

131
00:06:59,400 --> 00:07:02,609
the same behavior as if you were

132
00:07:00,690 --> 00:07:05,010
browsing tomorrow right is the same

133
00:07:02,610 --> 00:07:06,330
behavior so that's what the stationary

134
00:07:05,010 --> 00:07:08,699
process

135
00:07:06,330 --> 00:07:11,099
the defeat that there is a more precise

136
00:07:08,699 --> 00:07:13,800
mathematical definition but i'll give

137
00:07:11,099 --> 00:07:15,599
you just intuition behind that so here

138
00:07:13,800 --> 00:07:18,689
we have a process right in which you are

139
00:07:15,599 --> 00:07:20,128
navigating over documents a a process be

140
00:07:18,689 --> 00:07:22,560
that you're navigating over documents

141
00:07:20,129 --> 00:07:25,979
and if you think about how users behave

142
00:07:22,560 --> 00:07:28,199
you you see that what they actually do

143
00:07:25,979 --> 00:07:31,560
is they at the sub point in time they

144
00:07:28,199 --> 00:07:33,330
start behaving like a and then another

145
00:07:31,560 --> 00:07:35,039
point in time they still behave like a

146
00:07:33,330 --> 00:07:37,530
for a while and then they quit the

147
00:07:35,039 --> 00:07:42,080
system right they stop they go home or

148
00:07:37,530 --> 00:07:45,210
they close a website and now they are

149
00:07:42,080 --> 00:07:48,448
behaving like process B is a different

150
00:07:45,210 --> 00:07:50,460
process so your behavior online or your

151
00:07:48,449 --> 00:07:54,449
behavior in general is a mixture of

152
00:07:50,460 --> 00:07:57,000
different of these processes that that

153
00:07:54,449 --> 00:07:58,919
don't don't really depend on exactly the

154
00:07:57,000 --> 00:08:00,360
date that you started some processes do

155
00:07:58,919 --> 00:08:02,068
but a lot of the process that we

156
00:08:00,360 --> 00:08:04,050
interested or not do not depend the

157
00:08:02,069 --> 00:08:10,340
exact time what time that you started

158
00:08:04,050 --> 00:08:13,740
things so the second challenge is

159
00:08:10,340 --> 00:08:15,779
evolving behavior of a time right so

160
00:08:13,740 --> 00:08:18,389
users may not be time hitter Virginia

161
00:08:15,779 --> 00:08:23,520
maybe thai military genius and what that

162
00:08:18,389 --> 00:08:27,240
means is that over time you may for

163
00:08:23,520 --> 00:08:31,318
instance if you like music you may start

164
00:08:27,240 --> 00:08:35,339
here in 2010 you may like classic rock a

165
00:08:31,319 --> 00:08:37,949
lot and US pop and then you're listening

166
00:08:35,339 --> 00:08:40,649
to this classic rock and and and you

167
00:08:37,948 --> 00:08:44,939
alternate between classic rock and pop

168
00:08:40,649 --> 00:08:46,649
so so this behavior over time is

169
00:08:44,940 --> 00:08:49,079
changing and maybe you add a bit of

170
00:08:46,649 --> 00:08:53,519
heavy metal and then you go back to pop

171
00:08:49,079 --> 00:08:56,790
and etc the same thing with in the other

172
00:08:53,519 --> 00:09:01,800
case here what one interesting aspect is

173
00:08:56,790 --> 00:09:03,839
that here time its originality doesn't

174
00:09:01,800 --> 00:09:06,149
mean that you will not go back to the

175
00:09:03,839 --> 00:09:07,680
same behavioral later in the future you

176
00:09:06,149 --> 00:09:09,690
may go back to the same behavior over

177
00:09:07,680 --> 00:09:13,979
the feet in the future but that can take

178
00:09:09,690 --> 00:09:19,950
a long time okay so what that so you can

179
00:09:13,980 --> 00:09:21,900
think of an evolving trajectory okay so

180
00:09:19,950 --> 00:09:24,960
an interesting point so this is

181
00:09:21,900 --> 00:09:27,750
annotated actually annotated data this

182
00:09:24,960 --> 00:09:30,510
is real data from some users at less FM

183
00:09:27,750 --> 00:09:33,420
and and it's interesting that the user

184
00:09:30,510 --> 00:09:35,100
here has us pop you see us pop is

185
00:09:33,420 --> 00:09:37,469
interested and then start listening to

186
00:09:35,100 --> 00:09:39,480
classic rock and then gives up on the

187
00:09:37,470 --> 00:09:43,260
pop and then just listen to classic rock

188
00:09:39,480 --> 00:09:46,410
for after that another one that is a one

189
00:09:43,260 --> 00:09:48,480
of the more interesting actually was a

190
00:09:46,410 --> 00:09:51,060
mixture of heavy metal pop in classic

191
00:09:48,480 --> 00:09:54,420
rock and then switch to Korean pop and

192
00:09:51,060 --> 00:09:58,229
never went back just listening to Korean

193
00:09:54,420 --> 00:10:04,380
pop I don't know when was maybe that's

194
00:09:58,230 --> 00:10:07,170
when sigh came came out I'm not sure ok

195
00:10:04,380 --> 00:10:10,140
so I may have been the last person to

196
00:10:07,170 --> 00:10:12,420
learn about this I had a friend who are

197
00:10:10,140 --> 00:10:16,520
like force force a visual anyways so

198
00:10:12,420 --> 00:10:18,900
here we have some the third challenge

199
00:10:16,520 --> 00:10:21,510
which is that these trajectories may be

200
00:10:18,900 --> 00:10:23,280
transient so let's say that you go

201
00:10:21,510 --> 00:10:24,750
through phases when you listen to music

202
00:10:23,280 --> 00:10:27,060
or you go through when you think about

203
00:10:24,750 --> 00:10:29,280
job the behavior of people accessing

204
00:10:27,060 --> 00:10:31,770
files on a company when they change job

205
00:10:29,280 --> 00:10:35,780
title they change their access behavior

206
00:10:31,770 --> 00:10:39,329
right so you expect for instance if you

207
00:10:35,780 --> 00:10:40,890
maybe like music like 6ish music and

208
00:10:39,330 --> 00:10:43,170
then you are just listening to sixties

209
00:10:40,890 --> 00:10:45,030
music there are three years in this is

210
00:10:43,170 --> 00:10:49,410
spam that you are really interested in

211
00:10:45,030 --> 00:10:51,810
60s music but then you decide to change

212
00:10:49,410 --> 00:10:56,459
as we saw people sometimes they change

213
00:10:51,810 --> 00:10:58,079
very very different styles and any now

214
00:10:56,460 --> 00:10:59,610
we have four years of heavy metal that

215
00:10:58,080 --> 00:11:01,710
you're really more interested in heavy

216
00:10:59,610 --> 00:11:03,900
metal than anything else and after that

217
00:11:01,710 --> 00:11:06,990
you go to your next musical phase and

218
00:11:03,900 --> 00:11:09,569
what is important in the transiency of

219
00:11:06,990 --> 00:11:13,470
these processes is that you will never

220
00:11:09,570 --> 00:11:16,050
go back to the 60s ok so you are done

221
00:11:13,470 --> 00:11:18,270
with that and now you move move move on

222
00:11:16,050 --> 00:11:20,040
that makes the learning of this

223
00:11:18,270 --> 00:11:21,750
trajectory is incredibly difficult right

224
00:11:20,040 --> 00:11:23,370
imagine that you have something that

225
00:11:21,750 --> 00:11:28,170
people are doing and they will never do

226
00:11:23,370 --> 00:11:32,070
again and in the other case where we

227
00:11:28,170 --> 00:11:33,860
have for instance here evolving

228
00:11:32,070 --> 00:11:35,810
trajectories right so your evolved

229
00:11:33,860 --> 00:11:37,730
over time so it's hard for you to learn

230
00:11:35,810 --> 00:11:39,920
what they are doing if it's not

231
00:11:37,730 --> 00:11:48,320
recording it tends to evolve and

232
00:11:39,920 --> 00:11:50,240
continually adapt so let me show you now

233
00:11:48,320 --> 00:11:55,850
what the data would look like for this

234
00:11:50,240 --> 00:11:58,339
type of problems so the data is a

235
00:11:55,850 --> 00:12:03,529
trajectory so we have a user and we have

236
00:11:58,339 --> 00:12:05,510
what they access and when okay fio a 50

237
00:12:03,529 --> 00:12:09,550
B if I will see and this is the observed

238
00:12:05,510 --> 00:12:14,060
trajectory data so this is all we have

239
00:12:09,550 --> 00:12:20,329
all we have is this and what I'm going

240
00:12:14,060 --> 00:12:22,430
to briefly explain is that in in machine

241
00:12:20,329 --> 00:12:24,920
learning and data mining what we'll do

242
00:12:22,430 --> 00:12:30,459
is observing this data we will come up

243
00:12:24,920 --> 00:12:32,870
with a model and then don't have to be

244
00:12:30,459 --> 00:12:34,699
I'm not going to explain exactly how the

245
00:12:32,870 --> 00:12:36,500
model works when you come up with a

246
00:12:34,700 --> 00:12:39,079
model that will learn from this data and

247
00:12:36,500 --> 00:12:41,600
then make predictions and group users

248
00:12:39,079 --> 00:12:45,040
according to their behavior okay so what

249
00:12:41,600 --> 00:12:48,500
I'm going to talk about now is how them

250
00:12:45,040 --> 00:12:50,839
the how to come up with these models

251
00:12:48,500 --> 00:12:56,089
what are the models and how these models

252
00:12:50,839 --> 00:12:57,890
work okay so there are a bunch of the

253
00:12:56,089 --> 00:13:01,070
you solutions to the problems if you are

254
00:12:57,890 --> 00:13:03,680
familiar with them so for instance

255
00:13:01,070 --> 00:13:06,829
matrix decomposition on a non-negative

256
00:13:03,680 --> 00:13:09,560
matrix factorization or singular value

257
00:13:06,829 --> 00:13:14,120
decomposition have multiple ways of the

258
00:13:09,560 --> 00:13:16,219
composing this user product matrices

259
00:13:14,120 --> 00:13:19,699
like so I have a user have a product and

260
00:13:16,220 --> 00:13:23,410
that's often how naively would recommend

261
00:13:19,699 --> 00:13:28,130
products to users okay and these are

262
00:13:23,410 --> 00:13:29,660
mostly done in the general ideas to do a

263
00:13:28,130 --> 00:13:32,300
low rank the composition over this

264
00:13:29,660 --> 00:13:35,120
matrix the problem with this approach is

265
00:13:32,300 --> 00:13:37,160
that assumes that things are homogeneous

266
00:13:35,120 --> 00:13:39,680
in time so the users are not changing

267
00:13:37,160 --> 00:13:42,339
their behavior and the behavior tends to

268
00:13:39,680 --> 00:13:49,349
record there is no transiency in the

269
00:13:42,339 --> 00:13:52,779
your behavior and another approach is

270
00:13:49,350 --> 00:13:57,240
the tenth of the compositions where you

271
00:13:52,779 --> 00:14:00,249
one of the the so a tensor is a

272
00:13:57,240 --> 00:14:02,319
multi-dimensional matrix so if you

273
00:14:00,249 --> 00:14:03,879
understand what a matrix is you

274
00:14:02,319 --> 00:14:05,589
basically understand what a tensor is

275
00:14:03,879 --> 00:14:08,410
it's just that you have one more

276
00:14:05,589 --> 00:14:11,230
dimension so a vector has one dimension

277
00:14:08,410 --> 00:14:13,600
a matrix will have two dimensions and a

278
00:14:11,230 --> 00:14:16,149
tensor has a third dimension you can

279
00:14:13,600 --> 00:14:18,519
think of it like a cube okay with data

280
00:14:16,149 --> 00:14:21,309
so one of the dimensions can be time

281
00:14:18,519 --> 00:14:24,779
right so we have okay can have user you

282
00:14:21,309 --> 00:14:27,309
can have product and you can have time

283
00:14:24,779 --> 00:14:29,499
what is interesting about terms of the

284
00:14:27,309 --> 00:14:32,139
composition with the same mode is that

285
00:14:29,499 --> 00:14:34,209
you can will see it at the end that you

286
00:14:32,139 --> 00:14:36,579
cannot recover hidden behavior from

287
00:14:34,209 --> 00:14:40,119
stationary processes so the fact that

288
00:14:36,579 --> 00:14:43,719
you are today you behavior is similar to

289
00:14:40,120 --> 00:14:45,699
the to your behavior tomorrow will will

290
00:14:43,720 --> 00:14:48,519
make it very hard for the tensor to

291
00:14:45,699 --> 00:14:52,329
learn your intrinsic properties of your

292
00:14:48,519 --> 00:14:55,660
trajectory something that you may have

293
00:14:52,329 --> 00:14:57,790
learned heard about our hidden Markov

294
00:14:55,660 --> 00:15:02,230
models who here has heard about hidden

295
00:14:57,790 --> 00:15:05,259
Markov models okay so we have a few so

296
00:15:02,230 --> 00:15:09,309
here the mark of models the idea is to

297
00:15:05,259 --> 00:15:14,860
model trajectory were very used a few

298
00:15:09,309 --> 00:15:18,189
years ago in learning how people right

299
00:15:14,860 --> 00:15:23,230
and also learning speech speech

300
00:15:18,189 --> 00:15:25,509
recognition so these models will model

301
00:15:23,230 --> 00:15:27,309
just a single type of trajectory it they

302
00:15:25,509 --> 00:15:29,290
are not meant for people with different

303
00:15:27,309 --> 00:15:30,809
objectives right we have the CEO we have

304
00:15:29,290 --> 00:15:34,089
the regular users we have the

305
00:15:30,809 --> 00:15:35,860
supervisors so this will be a different

306
00:15:34,089 --> 00:15:39,839
types of trajectory and these models are

307
00:15:35,860 --> 00:15:43,120
not really meant for this type of data

308
00:15:39,839 --> 00:15:45,449
okay so these are the name solutions

309
00:15:43,120 --> 00:15:49,529
that will not quite work well and

310
00:15:45,449 --> 00:15:53,949
recently they have been a number of

311
00:15:49,529 --> 00:15:55,710
solute other solutions better solutions

312
00:15:53,949 --> 00:15:59,910
to the problem

313
00:15:55,710 --> 00:16:02,910
and one of the most interesting ones is

314
00:15:59,910 --> 00:16:08,130
called latent mark of minbari was

315
00:16:02,910 --> 00:16:10,410
published at kdd in 2013 and there are

316
00:16:08,130 --> 00:16:12,410
many other variants of that multi

317
00:16:10,410 --> 00:16:14,699
correlated mark of embedding and

318
00:16:12,410 --> 00:16:18,079
personalised ranking elated mark of

319
00:16:14,700 --> 00:16:21,780
embody which is the fastest from our

320
00:16:18,080 --> 00:16:25,080
simulations with real data it seems to

321
00:16:21,780 --> 00:16:27,240
be the fastest and most accurate of for

322
00:16:25,080 --> 00:16:31,440
trajectory prediction that among all of

323
00:16:27,240 --> 00:16:33,270
them was published last year there are

324
00:16:31,440 --> 00:16:36,270
other methods like factorizing machines

325
00:16:33,270 --> 00:16:39,150
I'm not going to go into the details of

326
00:16:36,270 --> 00:16:41,220
how this method work i'll just say very

327
00:16:39,150 --> 00:16:43,829
vaguely that latent mark of embedding

328
00:16:41,220 --> 00:16:47,130
the idea is just to project your data

329
00:16:43,830 --> 00:16:51,470
into a latent space and then you learn

330
00:16:47,130 --> 00:16:51,470
the markov chain over that latent space

331
00:16:51,560 --> 00:16:56,699
ok so another common issues for this all

332
00:16:54,690 --> 00:16:59,370
these tools is that they are choose low

333
00:16:56,700 --> 00:17:08,430
for large data sets okay so you cannot

334
00:16:59,370 --> 00:17:15,569
learn a larger scale data so the east

335
00:17:08,430 --> 00:17:18,959
side that of our work here is that we

336
00:17:15,569 --> 00:17:20,730
understanding the constraining the

337
00:17:18,959 --> 00:17:22,920
constraint dynamics of the the

338
00:17:20,730 --> 00:17:25,890
trajectory is key to predict the future

339
00:17:22,920 --> 00:17:28,440
so if I knew if I could see where the

340
00:17:25,890 --> 00:17:30,630
agent is then it would be easier for me

341
00:17:28,440 --> 00:17:33,240
to predict where the agent will be next

342
00:17:30,630 --> 00:17:35,520
because you see if you are in Lafayette

343
00:17:33,240 --> 00:17:37,860
or if you are on a given website on a

344
00:17:35,520 --> 00:17:40,430
certain page there is that there it's

345
00:17:37,860 --> 00:17:42,719
hard for you to jump to some other place

346
00:17:40,430 --> 00:17:45,330
right for instance if you are in

347
00:17:42,720 --> 00:17:50,130
Lafayette you cannot have dinner in New

348
00:17:45,330 --> 00:17:51,949
York City that's very unlikely the same

349
00:17:50,130 --> 00:17:55,470
thing in a page so if you are on a page

350
00:17:51,950 --> 00:17:59,280
where you have a certain group it's hard

351
00:17:55,470 --> 00:18:03,090
for you to immediately after go to

352
00:17:59,280 --> 00:18:07,830
something completely different right so

353
00:18:03,090 --> 00:18:09,449
but what we see is just what the user

354
00:18:07,830 --> 00:18:11,789
did right so it was

355
00:18:09,450 --> 00:18:16,769
to Madonna and then went to listen to

356
00:18:11,789 --> 00:18:21,929
daft punk and what we want we'd like to

357
00:18:16,769 --> 00:18:24,179
do is from this trajectory observe

358
00:18:21,929 --> 00:18:27,240
trajectory we want to learn what were

359
00:18:24,179 --> 00:18:30,389
the constraints what would be given that

360
00:18:27,240 --> 00:18:32,880
you did most users did this type of

361
00:18:30,389 --> 00:18:36,090
transitions right from one artist to

362
00:18:32,880 --> 00:18:37,679
another artist or from one business to

363
00:18:36,090 --> 00:18:39,720
another business or from one file to

364
00:18:37,679 --> 00:18:41,789
another file you want to know what is

365
00:18:39,720 --> 00:18:43,919
the kind of space or constraints space

366
00:18:41,789 --> 00:18:46,019
that they were living in that forced

367
00:18:43,919 --> 00:18:49,649
them to to do this type of navigation

368
00:18:46,019 --> 00:18:50,970
and that's what we are going to learn

369
00:18:49,649 --> 00:18:57,418
from data we are going to learn this

370
00:18:50,970 --> 00:19:00,000
constrained space problem is the

371
00:18:57,419 --> 00:19:01,380
trajectories is really all we see we

372
00:19:00,000 --> 00:19:08,159
don't see the constraint in space we

373
00:19:01,380 --> 00:19:10,860
don't see the space so and that's what

374
00:19:08,159 --> 00:19:13,169
we are going to do is we start with the

375
00:19:10,860 --> 00:19:15,629
data and then we reverse-engineer the

376
00:19:13,169 --> 00:19:17,130
data to get to the constraints and the

377
00:19:15,630 --> 00:19:23,010
way that people are navigating over

378
00:19:17,130 --> 00:19:28,169
these constraints okay so for the

379
00:19:23,010 --> 00:19:30,350
experts it's it's a the idea is to learn

380
00:19:28,169 --> 00:19:33,779
a navigation process over a network

381
00:19:30,350 --> 00:19:34,830
using a nonparametric Bayes methods the

382
00:19:33,779 --> 00:19:37,110
learning is one hundred percent

383
00:19:34,830 --> 00:19:40,500
automatic so you just give the data and

384
00:19:37,110 --> 00:19:41,789
the method will give you the results you

385
00:19:40,500 --> 00:19:45,419
don't have to there are no parameters

386
00:19:41,789 --> 00:19:46,799
that you need a new tune no domain

387
00:19:45,419 --> 00:19:48,299
knowledge II don't need to understand

388
00:19:46,799 --> 00:19:50,070
anything about the data just throw the

389
00:19:48,299 --> 00:19:52,730
data at the model the model will create

390
00:19:50,070 --> 00:19:55,439
the constraints and the navigation and

391
00:19:52,730 --> 00:19:58,080
this work is the joint work with

392
00:19:55,440 --> 00:20:02,220
Figueredo almeida and for looters will

393
00:19:58,080 --> 00:20:10,980
be presented at dub dub dub this year in

394
00:20:02,220 --> 00:20:13,139
August if i'm not mistaken okay so let

395
00:20:10,980 --> 00:20:18,570
me give you some more details of how

396
00:20:13,139 --> 00:20:21,199
this works so the basic idea started

397
00:20:18,570 --> 00:20:21,200
with a

398
00:20:22,810 --> 00:20:28,300
with the idea of random walks of a

399
00:20:25,520 --> 00:20:33,530
random environment and and that idea

400
00:20:28,300 --> 00:20:36,620
really the first paper on this was in if

401
00:20:33,530 --> 00:20:39,500
i'm not mistaken 1934 from a russian

402
00:20:36,620 --> 00:20:43,820
mathematician describing a model for the

403
00:20:39,500 --> 00:20:47,630
DNA for the formation of DNA or copying

404
00:20:43,820 --> 00:20:50,389
in DNA and and this type of analysis

405
00:20:47,630 --> 00:20:52,580
gave us percolation theory so if you

406
00:20:50,390 --> 00:20:54,830
know percolation theory it came from

407
00:20:52,580 --> 00:20:59,570
these analysis of random walks on random

408
00:20:54,830 --> 00:21:01,550
environments so the constraint that i

409
00:20:59,570 --> 00:21:03,980
was talking about to be a graph with

410
00:21:01,550 --> 00:21:08,690
vertices the virtues are the products or

411
00:21:03,980 --> 00:21:11,390
the files and their edges are how likely

412
00:21:08,690 --> 00:21:14,090
it is that a given that the objective

413
00:21:11,390 --> 00:21:16,970
that you have right now that you would

414
00:21:14,090 --> 00:21:19,189
from one file go to the next file okay

415
00:21:16,970 --> 00:21:21,860
to another file or if we go from one

416
00:21:19,190 --> 00:21:23,510
business to another business or that you

417
00:21:21,860 --> 00:21:26,050
go from one song to another song for

418
00:21:23,510 --> 00:21:32,480
instance what would be this constraint

419
00:21:26,050 --> 00:21:34,370
this conditional behavior would be for

420
00:21:32,480 --> 00:21:35,750
instance you love have marrow so if

421
00:21:34,370 --> 00:21:37,790
you're listening to Metallica then

422
00:21:35,750 --> 00:21:41,960
probably the next one that you might be

423
00:21:37,790 --> 00:21:47,030
I don't maiden but the link weight or

424
00:21:41,960 --> 00:21:49,040
the edge to Madonna is very unlikely has

425
00:21:47,030 --> 00:21:52,420
very low weight a very low probability

426
00:21:49,040 --> 00:21:54,649
that you go from Metallica to Madonna

427
00:21:52,420 --> 00:21:56,390
you'll be surprised what you see the

428
00:21:54,650 --> 00:21:59,510
data actually which is very interesting

429
00:21:56,390 --> 00:22:03,950
but it is indeed it's very unlikely that

430
00:21:59,510 --> 00:22:06,890
you do that now we have such a graph and

431
00:22:03,950 --> 00:22:09,590
this graph defines one layer of these

432
00:22:06,890 --> 00:22:11,810
hidden constraints so if you like heavy

433
00:22:09,590 --> 00:22:15,889
metal then you have one graph over the

434
00:22:11,810 --> 00:22:17,510
whole set of songs that describe how

435
00:22:15,890 --> 00:22:20,660
people that like heavy metal will work

436
00:22:17,510 --> 00:22:22,460
on this network okay now if you like

437
00:22:20,660 --> 00:22:25,730
something else if you like pop then

438
00:22:22,460 --> 00:22:28,490
there is another graph that is how

439
00:22:25,730 --> 00:22:31,940
people walk on this disorder graph and

440
00:22:28,490 --> 00:22:33,920
this graph the way that will do is to

441
00:22:31,940 --> 00:22:36,220
say that these are these are created

442
00:22:33,920 --> 00:22:38,440
randomly these graphs are

443
00:22:36,220 --> 00:22:41,650
the generative model of this will be

444
00:22:38,440 --> 00:22:43,840
that these edges are distributed the

445
00:22:41,650 --> 00:22:45,580
edge weight of the strength of an edge

446
00:22:43,840 --> 00:22:49,080
will be distributed according to a gamma

447
00:22:45,580 --> 00:22:50,980
distribution it's just a way to

448
00:22:49,080 --> 00:22:52,750
probabilistically describe what you are

449
00:22:50,980 --> 00:22:58,570
seeing so we can reverse engineer it

450
00:22:52,750 --> 00:23:02,320
okay now the way that we were going to

451
00:22:58,570 --> 00:23:04,809
work then is to have let's say Alice she

452
00:23:02,320 --> 00:23:07,809
really prefers graph g 1 let's say heavy

453
00:23:04,809 --> 00:23:10,750
metal and then she does a random walk if

454
00:23:07,809 --> 00:23:13,000
you're familiar with page rank you want

455
00:23:10,750 --> 00:23:16,480
familiar with page rank have you heard

456
00:23:13,000 --> 00:23:18,909
about page rank so page rank is a random

457
00:23:16,480 --> 00:23:21,789
walk and then you randomly jump to a

458
00:23:18,909 --> 00:23:25,720
another node ok so what we are going to

459
00:23:21,789 --> 00:23:29,590
do is a similar to that in in that for a

460
00:23:25,720 --> 00:23:32,100
graph g 1 let's say heavy metal then she

461
00:23:29,590 --> 00:23:35,139
will be walking around the songs

462
00:23:32,100 --> 00:23:36,908
according to a random walk determined by

463
00:23:35,140 --> 00:23:40,590
the probability that you walk on a given

464
00:23:36,909 --> 00:23:46,780
edge is determined by the edge weight ok

465
00:23:40,590 --> 00:23:49,059
and then after a while and it gets bored

466
00:23:46,780 --> 00:23:52,210
and then randomly jumps to another thing

467
00:23:49,059 --> 00:23:54,879
that she likes let's say pop right and

468
00:23:52,210 --> 00:23:56,919
then she likes pop or if she likes other

469
00:23:54,880 --> 00:23:59,200
type of heavy metal she can also jump to

470
00:23:56,919 --> 00:24:00,549
other type of heavy metal so this is the

471
00:23:59,200 --> 00:24:02,440
model this is what we are trying to

472
00:24:00,549 --> 00:24:04,179
learn ok so we give a probabilistic

473
00:24:02,440 --> 00:24:06,990
description of this model and then try

474
00:24:04,179 --> 00:24:12,820
to learn the parameters of this model

475
00:24:06,990 --> 00:24:15,130
even more details so what we call the

476
00:24:12,820 --> 00:24:16,809
environment is a mixture of this latent

477
00:24:15,130 --> 00:24:19,179
structure that we are trying to learn

478
00:24:16,809 --> 00:24:21,520
and a navigation process on top of this

479
00:24:19,179 --> 00:24:23,880
structure the navigation process is

480
00:24:21,520 --> 00:24:27,250
given is a random walk with jumps and

481
00:24:23,880 --> 00:24:29,080
then what we assume is that using behave

482
00:24:27,250 --> 00:24:31,390
like this so there is a latent structure

483
00:24:29,080 --> 00:24:33,970
there is a navigation process and what

484
00:24:31,390 --> 00:24:36,340
we see is this trajectory segments right

485
00:24:33,970 --> 00:24:38,020
this we are going from Whitney Spears to

486
00:24:36,340 --> 00:24:40,539
something else a metallic and you are a

487
00:24:38,020 --> 00:24:42,720
maiden and etc and now we want to learn

488
00:24:40,539 --> 00:24:46,240
back what the user preferences are and

489
00:24:42,720 --> 00:24:50,050
the preferences are over the

490
00:24:46,240 --> 00:24:51,790
environments are learned as

491
00:24:50,050 --> 00:24:54,430
zooming a drizzly process over the

492
00:24:51,790 --> 00:24:56,350
different environment so you can have

493
00:24:54,430 --> 00:24:59,050
what what is interesting about assuming

494
00:24:56,350 --> 00:25:01,659
additional process is that you can have

495
00:24:59,050 --> 00:25:03,250
the method automatically learns how many

496
00:25:01,660 --> 00:25:05,080
of these environments are the

497
00:25:03,250 --> 00:25:07,660
consequence trains or this interest that

498
00:25:05,080 --> 00:25:11,620
the user have exist automatically don't

499
00:25:07,660 --> 00:25:15,280
have to tell that in advance okay and

500
00:25:11,620 --> 00:25:16,810
then you have whenever you have a jump

501
00:25:15,280 --> 00:25:19,930
from this random surfer so you're

502
00:25:16,810 --> 00:25:22,659
randomly surfing over this network let's

503
00:25:19,930 --> 00:25:23,980
say heavy metal you change not only the

504
00:25:22,660 --> 00:25:25,690
node but you also change the environment

505
00:25:23,980 --> 00:25:27,490
and you go to another preferred

506
00:25:25,690 --> 00:25:31,000
environment according to the probability

507
00:25:27,490 --> 00:25:34,330
so the 0.3 and 0.6 there are the

508
00:25:31,000 --> 00:25:36,040
probabilities that that Alice will jump

509
00:25:34,330 --> 00:25:38,260
to one of these other environments maybe

510
00:25:36,040 --> 00:25:43,810
stretch marrow maybe it's pot maybe

511
00:25:38,260 --> 00:25:46,450
something else that she prefers okay so

512
00:25:43,810 --> 00:25:52,300
how do we learn the model even more

513
00:25:46,450 --> 00:25:55,780
details we use Gibbs sampling and and

514
00:25:52,300 --> 00:25:57,430
the idea to make it fast is to use

515
00:25:55,780 --> 00:26:01,060
stochastic gradient descent for fast

516
00:25:57,430 --> 00:26:03,040
converges in mini batches so you suspect

517
00:26:01,060 --> 00:26:08,290
emerging research you learn the digital

518
00:26:03,040 --> 00:26:11,710
process and in a heuristic for learning

519
00:26:08,290 --> 00:26:14,170
trajectories over fixed size renewal

520
00:26:11,710 --> 00:26:19,720
points that makes this a learning

521
00:26:14,170 --> 00:26:24,220
process extremely fast let me show you

522
00:26:19,720 --> 00:26:26,740
the results with real datasets and first

523
00:26:24,220 --> 00:26:28,960
I'll show you the data sets we have

524
00:26:26,740 --> 00:26:31,600
three types of data here music streaming

525
00:26:28,960 --> 00:26:34,540
services where users are listening to

526
00:26:31,600 --> 00:26:37,179
songs location-based social networks

527
00:26:34,540 --> 00:26:40,389
like Foursquare and brightkite know if

528
00:26:37,180 --> 00:26:42,910
you're familiar Foursquare you choose

529
00:26:40,390 --> 00:26:45,850
which is something similar to Amazon

530
00:26:42,910 --> 00:26:49,840
people or bestbuy people are clicking on

531
00:26:45,850 --> 00:26:54,639
products and yes which is also a kind of

532
00:26:49,840 --> 00:26:57,550
internet radio station okay the largest

533
00:26:54,640 --> 00:26:59,950
data set is less FM groups where we have

534
00:26:57,550 --> 00:27:02,440
86 million transitions different

535
00:26:59,950 --> 00:27:03,760
transitions different jobs between

536
00:27:02,440 --> 00:27:08,370
different artists

537
00:27:03,760 --> 00:27:13,840
and 1.6 million artists in this data

538
00:27:08,370 --> 00:27:20,260
with about fifteen thousand users okay

539
00:27:13,840 --> 00:27:24,550
so how does it this idea or this method

540
00:27:20,260 --> 00:27:28,180
will do in this approach so what we'll

541
00:27:24,550 --> 00:27:31,180
see is the what we are looking here is

542
00:27:28,180 --> 00:27:34,210
in the x-axis the running time how long

543
00:27:31,180 --> 00:27:39,160
it takes to learn this model over the

544
00:27:34,210 --> 00:27:41,020
data and the y-axis is is a measure of

545
00:27:39,160 --> 00:27:42,730
accuracy how accurate you are in

546
00:27:41,020 --> 00:27:47,560
predicting what the next thing that the

547
00:27:42,730 --> 00:27:52,060
user will do okay so the best possible

548
00:27:47,560 --> 00:27:56,320
approach would be at the left upper

549
00:27:52,060 --> 00:28:00,490
corner okay you learn very fast and your

550
00:27:56,320 --> 00:28:03,159
accuracy is very good at the bottom is

551
00:28:00,490 --> 00:28:05,290
the worst at the bottom right is the

552
00:28:03,160 --> 00:28:09,670
worst where it's going to be very slow

553
00:28:05,290 --> 00:28:12,490
and your accuracy is not good so our

554
00:28:09,670 --> 00:28:17,370
approach what is interesting that our

555
00:28:12,490 --> 00:28:22,810
approach can learn the model and up to

556
00:28:17,370 --> 00:28:26,919
16 hours so the one thing to notice that

557
00:28:22,810 --> 00:28:29,200
the x axis is in log scale so here is

558
00:28:26,920 --> 00:28:32,620
one day and then you have here ten days

559
00:28:29,200 --> 00:28:34,420
and this is over ten days and we could

560
00:28:32,620 --> 00:28:37,600
not get their computing methods to

561
00:28:34,420 --> 00:28:39,130
finish in 10 days just trying to run

562
00:28:37,600 --> 00:28:41,980
over the same data set not even the

563
00:28:39,130 --> 00:28:45,070
smallest one one that our method takes

564
00:28:41,980 --> 00:28:49,860
one hour it takes more than ten days for

565
00:28:45,070 --> 00:28:52,570
the computing methods and multi lme our

566
00:28:49,860 --> 00:28:55,629
prediction is that it would take six

567
00:28:52,570 --> 00:28:58,419
years to learn over the largest data set

568
00:28:55,630 --> 00:29:00,310
and so that's that's one of the reasons

569
00:28:58,420 --> 00:29:04,050
why we don't see it rejected prediction

570
00:29:00,310 --> 00:29:07,350
in commercial services is because it's

571
00:29:04,050 --> 00:29:09,460
the methods have them being my

572
00:29:07,350 --> 00:29:12,669
hypothesis is that the method have been

573
00:29:09,460 --> 00:29:16,540
fast enough but our method now is fast

574
00:29:12,670 --> 00:29:17,650
enough so that to be fair right would

575
00:29:16,540 --> 00:29:20,470
have maybe maybe

576
00:29:17,650 --> 00:29:23,140
or so but they are very good it could be

577
00:29:20,470 --> 00:29:26,350
that the accuracy is better just that

578
00:29:23,140 --> 00:29:29,920
there is low but what is interesting is

579
00:29:26,350 --> 00:29:32,919
that if we if we cut this door the data

580
00:29:29,920 --> 00:29:35,350
set we subsample them and then try and

581
00:29:32,920 --> 00:29:39,430
then feed them to the drawer method into

582
00:29:35,350 --> 00:29:42,790
the other computer method we will see

583
00:29:39,430 --> 00:29:47,530
that our method which we call tribe flow

584
00:29:42,790 --> 00:29:51,129
is still significantly faster in this

585
00:29:47,530 --> 00:29:53,560
case here 46 times faster and always

586
00:29:51,130 --> 00:29:54,850
more accurate it's not only faster but

587
00:29:53,560 --> 00:30:01,750
it is more accurate than what's out

588
00:29:54,850 --> 00:30:05,740
there so and this is basically the best

589
00:30:01,750 --> 00:30:08,290
up competing approach is 46 times slower

590
00:30:05,740 --> 00:30:12,660
our methods 46 times faster and

591
00:30:08,290 --> 00:30:15,700
twenty-three percent more accurate so

592
00:30:12,660 --> 00:30:17,950
another interesting aspect i was talking

593
00:30:15,700 --> 00:30:20,920
about test of the composition right if

594
00:30:17,950 --> 00:30:23,200
you think about the tensor where the

595
00:30:20,920 --> 00:30:25,600
products are one of the modes the users

596
00:30:23,200 --> 00:30:29,680
are in the other mode and the third mode

597
00:30:25,600 --> 00:30:31,510
this time okay so as this cube and then

598
00:30:29,680 --> 00:30:34,750
the tensile composition we do composed

599
00:30:31,510 --> 00:30:37,510
is into three matrices this is an

600
00:30:34,750 --> 00:30:39,610
example of Tucker decomposition and a

601
00:30:37,510 --> 00:30:41,800
matrix in the middle there that is a

602
00:30:39,610 --> 00:30:47,679
matrix that make sure make sure the

603
00:30:41,800 --> 00:30:51,100
different majors other matrices so if we

604
00:30:47,680 --> 00:30:53,290
try to learn using tensors the hidden

605
00:30:51,100 --> 00:30:55,149
structure of this data the real hidden

606
00:30:53,290 --> 00:30:57,820
structure of the data so we have here

607
00:30:55,150 --> 00:31:03,420
generated data that is a coming from a

608
00:30:57,820 --> 00:31:03,419
stationary process and we use one of the

609
00:31:03,720 --> 00:31:14,620
one a very good method for learning over

610
00:31:10,030 --> 00:31:19,990
large datasets which is try mine and we

611
00:31:14,620 --> 00:31:23,469
see that this is the truth so the y axis

612
00:31:19,990 --> 00:31:27,790
is the type of the type of behavior that

613
00:31:23,470 --> 00:31:30,130
the the process has and the x axes are

614
00:31:27,790 --> 00:31:31,210
the user so there we have 10 users with

615
00:31:30,130 --> 00:31:34,450
one type of behavior

616
00:31:31,210 --> 00:31:37,120
for another type and etc so we have five

617
00:31:34,450 --> 00:31:40,000
different types and and then we we make

618
00:31:37,120 --> 00:31:42,399
them perform their trajectories over

619
00:31:40,000 --> 00:31:45,130
different points in time right like you

620
00:31:42,399 --> 00:31:46,899
like you do it today maybe another user

621
00:31:45,130 --> 00:31:50,140
that similar to you will do that

622
00:31:46,899 --> 00:31:51,879
trajectory tomorrow so if you try to

623
00:31:50,140 --> 00:31:53,830
apply it tends to the composition you

624
00:31:51,880 --> 00:31:56,380
see that you don't recover the behavior

625
00:31:53,830 --> 00:32:02,908
of the users very well but we stripe

626
00:31:56,380 --> 00:32:02,909
flow we can recover it quite well now

627
00:32:04,860 --> 00:32:10,809
there is another benefit of the benefits

628
00:32:07,899 --> 00:32:12,219
of our model and the idea that you're

629
00:32:10,809 --> 00:32:13,840
modeling this constraints and the

630
00:32:12,220 --> 00:32:17,289
navigation over this constrained space

631
00:32:13,840 --> 00:32:21,520
is better sense making so here we have

632
00:32:17,289 --> 00:32:25,840
foursquare data and in general what you

633
00:32:21,520 --> 00:32:28,029
will do is to have kind of a gaussian or

634
00:32:25,840 --> 00:32:30,520
some distribution of spatial

635
00:32:28,029 --> 00:32:31,960
distribution over your data to say well

636
00:32:30,520 --> 00:32:33,879
if you went in lafayette then you're

637
00:32:31,960 --> 00:32:38,080
probably going to go around the area

638
00:32:33,880 --> 00:32:41,830
right that area for the next for the

639
00:32:38,080 --> 00:32:45,399
next point of interest our method learns

640
00:32:41,830 --> 00:32:48,779
this constraint automatically from the

641
00:32:45,399 --> 00:32:51,580
data and what is interesting is that in

642
00:32:48,779 --> 00:32:55,860
the method outputs something like this

643
00:32:51,580 --> 00:32:59,379
it tells us that domestic flight so it

644
00:32:55,860 --> 00:33:02,408
appointed us to a group to constrain

645
00:32:59,380 --> 00:33:05,500
spatial constraint that our airports in

646
00:33:02,409 --> 00:33:07,480
the US with foursquare data and these

647
00:33:05,500 --> 00:33:10,840
airports are domestic airport so if you

648
00:33:07,480 --> 00:33:12,669
hit a hamburger in LaGuardia maybe you

649
00:33:10,840 --> 00:33:17,949
eat something you have a drink a coke

650
00:33:12,669 --> 00:33:19,630
and in Indianapolis see it's not special

651
00:33:17,950 --> 00:33:22,390
it's learned that there is there are

652
00:33:19,630 --> 00:33:24,610
some places that have this connection

653
00:33:22,390 --> 00:33:27,039
this constraint connection which are

654
00:33:24,610 --> 00:33:30,580
these airports local airport moreover it

655
00:33:27,039 --> 00:33:34,360
also found some other examples I'll give

656
00:33:30,580 --> 00:33:40,539
you one Pacific airports and the US and

657
00:33:34,360 --> 00:33:45,010
you UK hubs connecting Asia to to Europe

658
00:33:40,539 --> 00:33:48,430
and the US so if you have some

659
00:33:45,010 --> 00:33:51,520
food mcdonalds in new york city JFK it

660
00:33:48,430 --> 00:34:00,430
will predict that your next meal will be

661
00:33:51,520 --> 00:34:04,030
in beijing okay so another interesting

662
00:34:00,430 --> 00:34:06,880
aspect of this approach is to understand

663
00:34:04,030 --> 00:34:10,480
the data over multiple resolutions so

664
00:34:06,880 --> 00:34:13,149
here these data here that i was showing

665
00:34:10,480 --> 00:34:15,159
to you users were liking they like

666
00:34:13,149 --> 00:34:17,770
classic rock and they like korean pop

667
00:34:15,159 --> 00:34:20,889
and etc this is automatically learned by

668
00:34:17,770 --> 00:34:22,480
the method ok so this classification and

669
00:34:20,889 --> 00:34:25,270
the way in which people are changing

670
00:34:22,480 --> 00:34:27,820
things you can see that the method can

671
00:34:25,270 --> 00:34:33,780
tell you when it's decide to switch from

672
00:34:27,820 --> 00:34:37,530
pop to classic rock and these are the an

673
00:34:33,780 --> 00:34:42,220
example some examples of some of these

674
00:34:37,530 --> 00:34:44,530
hidden networks or hidden constraints

675
00:34:42,219 --> 00:34:48,040
that it learned so one hidden constraint

676
00:34:44,530 --> 00:34:55,860
if you like some movie soundtracks then

677
00:34:48,040 --> 00:34:59,350
you have this composers that of

678
00:34:55,860 --> 00:35:01,630
soundtracks movie soundtracks and if you

679
00:34:59,350 --> 00:35:06,640
like trash metal then you will go over

680
00:35:01,630 --> 00:35:08,800
this different bands I did not do that

681
00:35:06,640 --> 00:35:15,310
the student did because I don't really

682
00:35:08,800 --> 00:35:18,040
know many of these classifications so so

683
00:35:15,310 --> 00:35:19,750
iron maiden Slayer and etc so if you

684
00:35:18,040 --> 00:35:22,000
learned that these are these are

685
00:35:19,750 --> 00:35:25,780
constrained because users tend to do

686
00:35:22,000 --> 00:35:28,840
trajectory over this specific items

687
00:35:25,780 --> 00:35:31,990
together ok we have nine is rock new

688
00:35:28,840 --> 00:35:34,210
Verner green day and an electro house

689
00:35:31,990 --> 00:35:36,160
which is a very specific actually it

690
00:35:34,210 --> 00:35:39,490
divided in made it two different types

691
00:35:36,160 --> 00:35:41,290
of house and what I found online was

692
00:35:39,490 --> 00:35:48,220
that this seems to be electro house and

693
00:35:41,290 --> 00:35:49,870
this all automatically done so if you

694
00:35:48,220 --> 00:35:53,500
have any questions I'm happy to answer

695
00:35:49,870 --> 00:35:55,650
the questions I'm open to questions any

696
00:35:53,500 --> 00:35:55,650
questions

697
00:35:56,049 --> 00:36:01,599
or like you have ideas about dr. Tophet

698
00:35:58,660 --> 00:36:05,920
types of data or applications to this

699
00:36:01,599 --> 00:36:10,479
type of method yes so the model over

700
00:36:05,920 --> 00:36:14,019
here seems i did so model seems to learn

701
00:36:10,479 --> 00:36:16,569
by studying the tragic trees of oh right

702
00:36:14,019 --> 00:36:20,098
a user goes don't you think it'll raise

703
00:36:16,569 --> 00:36:26,109
privacy concerns with certain users

704
00:36:20,099 --> 00:36:28,719
privacy concern you see it in data

705
00:36:26,109 --> 00:36:35,319
mining we have a different view we want

706
00:36:28,719 --> 00:36:37,929
the data regarding learning how can you

707
00:36:35,319 --> 00:36:39,459
learn this privately right so an

708
00:36:37,929 --> 00:36:42,969
interesting aspect of this trajectory

709
00:36:39,459 --> 00:36:45,009
learning and and I have worked on this

710
00:36:42,969 --> 00:36:47,140
before so I know that it might you might

711
00:36:45,009 --> 00:36:48,969
be able to identify is that you don't

712
00:36:47,140 --> 00:36:52,959
really need to tell me anything about

713
00:36:48,969 --> 00:36:56,170
the item that the users are visiting you

714
00:36:52,959 --> 00:36:58,299
just say it's item number 32 it can

715
00:36:56,170 --> 00:37:00,880
totally anonymous the item number and

716
00:36:58,299 --> 00:37:05,349
you can anonymize the user and then I'll

717
00:37:00,880 --> 00:37:09,819
give you well item 32 4455 are together

718
00:37:05,349 --> 00:37:13,599
and user 155 tends to go over this items

719
00:37:09,819 --> 00:37:15,099
together of course if you you know that

720
00:37:13,599 --> 00:37:18,579
you can always d anonymize this type of

721
00:37:15,099 --> 00:37:21,279
data but it doesn't really need features

722
00:37:18,579 --> 00:37:23,859
of the user let's say the age and or or

723
00:37:21,279 --> 00:37:29,999
any other or where they live and etc

724
00:37:23,859 --> 00:37:29,999
just learns over this abstract concept

725
00:37:32,189 --> 00:37:45,368
and you have other questions yes

726
00:37:36,490 --> 00:37:47,379
oh yeah the microphone there I wonder

727
00:37:45,369 --> 00:37:49,570
what's the intuition behind you matter

728
00:37:47,380 --> 00:37:51,640
it's faster than others because from my

729
00:37:49,570 --> 00:37:54,010
pond I'm not very familiar with all the

730
00:37:51,640 --> 00:37:56,770
machine learning algorithms but to me

731
00:37:54,010 --> 00:37:59,830
it's like you are trying to find some

732
00:37:56,770 --> 00:38:02,470
latent factors while the others matter

733
00:37:59,830 --> 00:38:05,049
some of them also trying to use some

734
00:38:02,470 --> 00:38:08,470
inviting measures to project you the low

735
00:38:05,050 --> 00:38:10,450
dimension ok so I wonder why you

736
00:38:08,470 --> 00:38:12,910
you measure damn much faster than that

737
00:38:10,450 --> 00:38:15,730
is a very accurate description of what

738
00:38:12,910 --> 00:38:17,710
they're doing right so we if you can put

739
00:38:15,730 --> 00:38:20,230
a lot of the machine learning methods as

740
00:38:17,710 --> 00:38:22,960
trying to learn Lincoln factor and we

741
00:38:20,230 --> 00:38:25,570
are did trying to learn later factor but

742
00:38:22,960 --> 00:38:28,330
there is one interesting aspect if you

743
00:38:25,570 --> 00:38:31,450
give more structure to the problem then

744
00:38:28,330 --> 00:38:34,119
learning tends to be easier oh the more

745
00:38:31,450 --> 00:38:36,129
degrees of freedom you have it the more

746
00:38:34,119 --> 00:38:39,160
the space that you have to search it

747
00:38:36,130 --> 00:38:41,440
becomes a bit harder and and the

748
00:38:39,160 --> 00:38:44,049
interesting thing is that if you are if

749
00:38:41,440 --> 00:38:46,869
you have a good insight of what the

750
00:38:44,050 --> 00:38:48,640
structure is like you actually learn

751
00:38:46,869 --> 00:38:53,290
better than you would if you had

752
00:38:48,640 --> 00:38:56,140
something that was more generic and and

753
00:38:53,290 --> 00:38:58,119
so the big insight here is that users

754
00:38:56,140 --> 00:38:59,618
when they are doing this trajectory they

755
00:38:58,119 --> 00:39:02,770
are doing under this constrained space

756
00:38:59,619 --> 00:39:06,160
and and that in the way they are

757
00:39:02,770 --> 00:39:08,920
navigating we determine that way to be

758
00:39:06,160 --> 00:39:10,899
very simple it's like a random walk they

759
00:39:08,920 --> 00:39:17,250
are the hard part is just you learn what

760
00:39:10,900 --> 00:39:24,660
the constraint looks like ok while

761
00:39:17,250 --> 00:39:24,660
oftentimes what we'll do is to have the

762
00:39:24,990 --> 00:39:29,529
the the process in which they are

763
00:39:27,640 --> 00:39:33,129
navigating to be the complicated one and

764
00:39:29,530 --> 00:39:34,660
a space to be very simple so we we

765
00:39:33,130 --> 00:39:36,850
inverted the problem we said the

766
00:39:34,660 --> 00:39:41,230
navigation is very simple the space is a

767
00:39:36,850 --> 00:39:42,940
bit more complicated and and and and it

768
00:39:41,230 --> 00:39:45,100
seems to match very well the way that

769
00:39:42,940 --> 00:39:47,290
you would actually behave right it mats

770
00:39:45,100 --> 00:39:48,700
intuition of what what destruction the

771
00:39:47,290 --> 00:39:49,660
problem should be if you are in

772
00:39:48,700 --> 00:39:52,000
lafayette delhi

773
00:39:49,660 --> 00:39:55,240
have some special constraints that you

774
00:39:52,000 --> 00:39:57,220
can't get around and that will show up

775
00:39:55,240 --> 00:39:58,959
on your trajectory but what's

776
00:39:57,220 --> 00:40:02,109
interesting is that the special

777
00:39:58,960 --> 00:40:05,200
constrain is not just special in the

778
00:40:02,109 --> 00:40:07,180
sense of distance in miles you saw the

779
00:40:05,200 --> 00:40:10,180
airport's right if you are an airport

780
00:40:07,180 --> 00:40:12,368
your space constraints change from the

781
00:40:10,180 --> 00:40:14,980
airport you cannot go to lafayette very

782
00:40:12,369 --> 00:40:16,930
easily although it's closing distance

783
00:40:14,980 --> 00:40:25,319
but it's much easier to go to New York

784
00:40:16,930 --> 00:40:28,680
City right so that is I think one of the

785
00:40:25,319 --> 00:40:38,829
reasons why our method is so much faster

786
00:40:28,680 --> 00:40:41,770
thank you any other questions any ideas

787
00:40:38,829 --> 00:40:46,200
of data or behavior that you would like

788
00:40:41,770 --> 00:40:46,200
to capture security behavior

789
00:40:54,240 --> 00:40:59,600
okay I guess there are no other

790
00:40:57,060 --> 00:41:03,690
questions like to thank you for your

791
00:40:59,600 --> 00:41:05,190
time and I'm available offline also if

792
00:41:03,690 --> 00:41:08,510
you want to ask things off the

793
00:41:05,190 --> 00:41:08,510
microphone thank you

