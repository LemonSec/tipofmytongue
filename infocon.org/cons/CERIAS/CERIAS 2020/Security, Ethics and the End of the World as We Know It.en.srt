1
00:00:03,040 --> 00:00:08,480
all right so let's get it started

2
00:00:06,350 --> 00:00:12,050
it's my pleasure to introduce Douglas

3
00:00:08,480 --> 00:00:14,660
Rapp CISM is a cybersecurity catalyst

4
00:00:12,050 --> 00:00:17,000
throughout heath Korea you will find

5
00:00:14,660 --> 00:00:19,880
he's squarely at the center of countless

6
00:00:17,000 --> 00:00:22,670
firsts this includes writing the first

7
00:00:19,880 --> 00:00:26,029
state-level cyber incident response plan

8
00:00:22,670 --> 00:00:29,150
leading the critique e^x national

9
00:00:26,029 --> 00:00:31,640
exercise establishing the region 5 cyber

10
00:00:29,150 --> 00:00:34,070
protection team and establishing

11
00:00:31,640 --> 00:00:36,800
Indiana's first cyber working group

12
00:00:34,070 --> 00:00:40,190
which leader involved into both the

13
00:00:36,800 --> 00:00:43,339
Indiana cybersecurity execute executed X

14
00:00:40,190 --> 00:00:46,610
you executed Council and the cyber

15
00:00:43,340 --> 00:00:52,219
leadership Alliance Dawg has served as

16
00:00:46,610 --> 00:00:54,769
the adviser and authored Indiana's

17
00:00:52,219 --> 00:00:58,129
cybersecurity economic development plan

18
00:00:54,770 --> 00:01:01,100
he has started skilled and page

19
00:00:58,129 --> 00:01:04,339
cybersecurity businesses and serve as a

20
00:01:01,100 --> 00:01:09,850
consultant and intrapreneur in residence

21
00:01:04,339 --> 00:01:12,679
for Purdue University his most recent

22
00:01:09,850 --> 00:01:16,130
accomplishment accomplishment was

23
00:01:12,679 --> 00:01:18,560
creating a statewide emergency security

24
00:01:16,130 --> 00:01:21,798
workforce development program and raised

25
00:01:18,560 --> 00:01:24,590
three million dollars in commitments to

26
00:01:21,799 --> 00:01:26,659
student financial assistance dog is a

27
00:01:24,590 --> 00:01:29,539
published author international speaker

28
00:01:26,659 --> 00:01:32,810
and has testified before Congress on

29
00:01:29,539 --> 00:01:35,240
cyber security workforce development it

30
00:01:32,810 --> 00:01:37,969
decorated combat veteran his greatest

31
00:01:35,240 --> 00:01:40,279
accomplishment is having raised two

32
00:01:37,969 --> 00:01:42,589
amazing children and having been trusted

33
00:01:40,279 --> 00:01:46,098
with America's sons and daughters with

34
00:01:42,590 --> 00:01:49,219
further ado let's welcome Tom thanks

35
00:01:46,099 --> 00:01:50,989
Dave well thanks so I appreciate all of

36
00:01:49,219 --> 00:01:53,149
you coming out tonight because I it's

37
00:01:50,989 --> 00:01:54,709
4:30 in the afternoon which means that

38
00:01:53,149 --> 00:01:59,149
I'm probably standing between you and

39
00:01:54,709 --> 00:02:01,819
food and/or beer so and you're very very

40
00:01:59,149 --> 00:02:03,709
fortunate so you're very fortunate

41
00:02:01,819 --> 00:02:06,139
because this is the first time giving

42
00:02:03,709 --> 00:02:07,789
this presentation the next time this

43
00:02:06,139 --> 00:02:11,299
presentation will be given will be in

44
00:02:07,789 --> 00:02:12,349
Rwanda so here at the end next month so

45
00:02:11,299 --> 00:02:15,720
you are the guinea pigs

46
00:02:12,349 --> 00:02:18,450
so what you also have the

47
00:02:15,720 --> 00:02:21,359
you also have the dubious honor of being

48
00:02:18,450 --> 00:02:24,480
here for the unveiling of a new hallmark

49
00:02:21,360 --> 00:02:27,150
term which I'll get into I'll point it

50
00:02:24,480 --> 00:02:31,340
out and you'll be able to say I remember

51
00:02:27,150 --> 00:02:33,570
when I heard that for the first time so

52
00:02:31,340 --> 00:02:38,610
security ethics in the end of the world

53
00:02:33,570 --> 00:02:40,320
as we know it it's a pretty ominous

54
00:02:38,610 --> 00:02:41,940
title how many of you actually read the

55
00:02:40,320 --> 00:02:43,459
little summary what we were going to

56
00:02:41,940 --> 00:02:52,440
talk about tonight

57
00:02:43,460 --> 00:02:53,850
so okay all right one that's great so we

58
00:02:52,440 --> 00:02:56,490
live in a digital economy and that's

59
00:02:53,850 --> 00:02:59,489
certainly no surprise to anybody that's

60
00:02:56,490 --> 00:03:02,520
here and when we think about what a

61
00:02:59,490 --> 00:03:04,650
digital economy really is we have to

62
00:03:02,520 --> 00:03:07,320
look at how we interact economically

63
00:03:04,650 --> 00:03:12,120
with other cultures with the rest of the

64
00:03:07,320 --> 00:03:15,510
world many people have claimed the data

65
00:03:12,120 --> 00:03:18,300
is currency and there is some truth to

66
00:03:15,510 --> 00:03:21,120
that what the analogies that they use

67
00:03:18,300 --> 00:03:25,550
are a little bit outdated we've heard

68
00:03:21,120 --> 00:03:28,590
data as equivalent to black gold to oil

69
00:03:25,550 --> 00:03:33,650
but the fact of the matter is is once

70
00:03:28,590 --> 00:03:38,250
oil is used it's gone it is a finite

71
00:03:33,650 --> 00:03:40,739
it's a finite substance land we've heard

72
00:03:38,250 --> 00:03:43,830
it compared to land in the economic in

73
00:03:40,739 --> 00:03:46,410
the economic sense however again once

74
00:03:43,830 --> 00:03:48,660
land is built upon once that yet land is

75
00:03:46,410 --> 00:03:51,359
utilized it can't be reutilized

76
00:03:48,660 --> 00:03:54,840
until whatever is built on top of it is

77
00:03:51,360 --> 00:03:57,050
torn down so when you look at data data

78
00:03:54,840 --> 00:04:00,300
as currency data as something of value

79
00:03:57,050 --> 00:04:04,769
has an exponential factor to it and that

80
00:04:00,300 --> 00:04:09,239
is that data can be used multiple times

81
00:04:04,769 --> 00:04:13,980
and it can be used in infinite numbers

82
00:04:09,239 --> 00:04:16,019
of ways that provide value so as we

83
00:04:13,980 --> 00:04:18,149
enter the digital economy and the

84
00:04:16,019 --> 00:04:21,120
digital economy is really a global

85
00:04:18,149 --> 00:04:23,909
digital ecosystem where data is gathered

86
00:04:21,120 --> 00:04:25,680
organized and exchanged by network

87
00:04:23,910 --> 00:04:29,039
vendors for the purpose of deriving

88
00:04:25,680 --> 00:04:29,650
value from the accumulated information

89
00:04:29,039 --> 00:04:33,250
when we

90
00:04:29,650 --> 00:04:38,440
get it we look at look at the market

91
00:04:33,250 --> 00:04:41,110
share in 2015 one hundred and twenty two

92
00:04:38,440 --> 00:04:43,000
billion dollars in data analytics and

93
00:04:41,110 --> 00:04:48,270
the exportation of buying and selling of

94
00:04:43,000 --> 00:04:50,740
data the projected market share for 2022

95
00:04:48,270 --> 00:04:56,109
is two hundred and seventy four billion

96
00:04:50,740 --> 00:04:57,550
dollars around the world so you can see

97
00:04:56,110 --> 00:04:59,560
some of the quotes they're the most

98
00:04:57,550 --> 00:05:01,660
valuable valuable commodity of the

99
00:04:59,560 --> 00:05:04,990
digital age the most valuable resource

100
00:05:01,660 --> 00:05:07,840
in the world in an age where stock

101
00:05:04,990 --> 00:05:10,150
traders are locating their offices as

102
00:05:07,840 --> 00:05:13,560
close to the New York Stock Exchange as

103
00:05:10,150 --> 00:05:16,500
possible so that they can have a

104
00:05:13,560 --> 00:05:20,500
fragment of a second advantage in

105
00:05:16,500 --> 00:05:23,020
transmitting data to make a trade

106
00:05:20,500 --> 00:05:30,820
you can really start to understand the

107
00:05:23,020 --> 00:05:34,299
value of data so data as currency when

108
00:05:30,820 --> 00:05:37,090
we look at the new world as we

109
00:05:34,300 --> 00:05:41,560
transition and understand the value of

110
00:05:37,090 --> 00:05:45,210
data there are four factors that have

111
00:05:41,560 --> 00:05:50,200
been described by Harvard as factors

112
00:05:45,210 --> 00:05:54,130
that can define data wealth within

113
00:05:50,200 --> 00:05:56,440
countries so we look at volume the

114
00:05:54,130 --> 00:06:00,280
absolute amount of broadband consumed by

115
00:05:56,440 --> 00:06:02,229
a country and in doing this research the

116
00:06:00,280 --> 00:06:04,119
only the numbers that they were able to

117
00:06:02,229 --> 00:06:08,229
analyze really had to do with broadband

118
00:06:04,120 --> 00:06:11,680
consumption and utilization so it's it's

119
00:06:08,229 --> 00:06:13,840
impossible to track the transition of

120
00:06:11,680 --> 00:06:15,849
data back and forth around the world and

121
00:06:13,840 --> 00:06:19,000
really quantify it so when you look at

122
00:06:15,849 --> 00:06:21,130
how do you say a country is data rich or

123
00:06:19,000 --> 00:06:23,020
has the potential to be data rich so you

124
00:06:21,130 --> 00:06:24,880
look at volume you look at usage the

125
00:06:23,020 --> 00:06:27,849
number of active users that are on the

126
00:06:24,880 --> 00:06:31,539
internet you look at accessibility so

127
00:06:27,849 --> 00:06:37,419
how how easy is it to get a hold of that

128
00:06:31,539 --> 00:06:40,630
data and you look at complexity so what

129
00:06:37,419 --> 00:06:43,060
level of complexity can you perform with

130
00:06:40,630 --> 00:06:46,480
that digital activity

131
00:06:43,060 --> 00:06:50,500
and if you look at this this slide put

132
00:06:46,480 --> 00:06:55,060
together by Harvard you can see that

133
00:06:50,500 --> 00:06:57,490
United States's is way out front so when

134
00:06:55,060 --> 00:06:59,470
we look at companies like Indonesia or

135
00:06:57,490 --> 00:07:01,180
China or Russia and we put them in the

136
00:06:59,470 --> 00:07:05,680
context of these four criteria that

137
00:07:01,180 --> 00:07:08,830
we've we've defined volume some of those

138
00:07:05,680 --> 00:07:13,960
countries you can see have huge volumes

139
00:07:08,830 --> 00:07:17,020
of data but they limit accessibility so

140
00:07:13,960 --> 00:07:21,580
they limit accessibility and usage for

141
00:07:17,020 --> 00:07:23,650
individuals so not being one that wants

142
00:07:21,580 --> 00:07:28,030
to pick a fight with Harvard but I'm

143
00:07:23,650 --> 00:07:32,919
going to you and that is that I argue we

144
00:07:28,030 --> 00:07:36,960
cannot look at wealth in the context of

145
00:07:32,920 --> 00:07:39,790
nations anymore in a digital economy

146
00:07:36,960 --> 00:07:43,150
what we're seeing rise out of a digital

147
00:07:39,790 --> 00:07:46,180
economy is not what's what's important

148
00:07:43,150 --> 00:07:48,609
is not necessarily and completely the

149
00:07:46,180 --> 00:07:51,810
wealth of nations but what we're seeing

150
00:07:48,610 --> 00:07:55,300
is a rise of what I call data barons

151
00:07:51,810 --> 00:07:58,800
just like the old robber barons of the

152
00:07:55,300 --> 00:08:01,950
Industrial Revolution who accumulated

153
00:07:58,800 --> 00:08:06,280
massive quantities of wealth and drove

154
00:08:01,950 --> 00:08:08,500
innovation and drove progress what's

155
00:08:06,280 --> 00:08:11,590
arising right now what I argue is what's

156
00:08:08,500 --> 00:08:16,350
arising right now is really about seven

157
00:08:11,590 --> 00:08:19,840
companies around the world that have in

158
00:08:16,350 --> 00:08:21,370
essence these four criteria and I'll

159
00:08:19,840 --> 00:08:25,659
explain a little bit that these four

160
00:08:21,370 --> 00:08:29,140
criteria which make them and will make

161
00:08:25,660 --> 00:08:32,110
them more powerful than the government's

162
00:08:29,140 --> 00:08:35,110
and the confines of our national

163
00:08:32,110 --> 00:08:38,190
boundaries so when you look at these

164
00:08:35,110 --> 00:08:38,190
data Baron's

165
00:08:39,240 --> 00:08:50,070
you look at these companies that exist

166
00:08:43,270 --> 00:08:50,069
out there and they accumulate the wealth

167
00:08:51,770 --> 00:08:57,529
and access to the data who's collecting

168
00:08:54,500 --> 00:09:00,200
our data out there alright so who is

169
00:08:57,529 --> 00:09:02,360
actually looking who has the means to go

170
00:09:00,200 --> 00:09:07,190
out and collect all that data that

171
00:09:02,360 --> 00:09:09,140
exists out there so in 2015 the majority

172
00:09:07,190 --> 00:09:10,580
of the data that was collected in the

173
00:09:09,140 --> 00:09:12,050
world had been collected in the last two

174
00:09:10,580 --> 00:09:18,440
years that's grown exponentially since

175
00:09:12,050 --> 00:09:24,229
2015 so as we look at the seven major

176
00:09:18,440 --> 00:09:27,080
players in the ecosystem you really come

177
00:09:24,230 --> 00:09:30,980
up with two two major countries where

178
00:09:27,080 --> 00:09:34,160
those companies exist but again I make

179
00:09:30,980 --> 00:09:37,970
the argument that we are transitioning

180
00:09:34,160 --> 00:09:40,810
from the power of nations to the power

181
00:09:37,970 --> 00:09:45,110
of these companies as these companies

182
00:09:40,810 --> 00:09:49,040
continue to grow their capability to not

183
00:09:45,110 --> 00:09:53,000
only collect data but also to take that

184
00:09:49,040 --> 00:09:59,029
data and the algorithmics and invest in

185
00:09:53,000 --> 00:10:04,130
things like m LDL and AI and are using

186
00:09:59,029 --> 00:10:05,600
that data to generate wealth for their

187
00:10:04,130 --> 00:10:11,079
own individual companies or their own

188
00:10:05,600 --> 00:10:15,800
self-interests yes deep learning and

189
00:10:11,079 --> 00:10:17,930
artificial intelligence so when we look

190
00:10:15,800 --> 00:10:19,910
at these if you look at government's

191
00:10:17,930 --> 00:10:22,329
particularly our form of government our

192
00:10:19,910 --> 00:10:25,540
government is not in the business of

193
00:10:22,329 --> 00:10:28,250
selling data it's in the business of

194
00:10:25,540 --> 00:10:33,829
data that is freely accessible to

195
00:10:28,250 --> 00:10:35,420
everyone we live in a democracy so when

196
00:10:33,829 --> 00:10:37,279
we look at other forms of government

197
00:10:35,420 --> 00:10:39,649
look at China which is a fundamental

198
00:10:37,279 --> 00:10:43,399
difference all right in the United

199
00:10:39,649 --> 00:10:44,930
States we Revere innovation Silicon

200
00:10:43,399 --> 00:10:47,050
Valley is all about what's the next

201
00:10:44,930 --> 00:10:49,660
great thing what's the next great

202
00:10:47,050 --> 00:10:52,550
invention that's coming out there and

203
00:10:49,660 --> 00:10:55,969
god forbid you take somebody else's

204
00:10:52,550 --> 00:10:58,699
concept and improve upon it you're

205
00:10:55,970 --> 00:11:02,120
asking for a lawsuit in an amazingly

206
00:10:58,700 --> 00:11:05,960
litigious society but the fundamental

207
00:11:02,120 --> 00:11:10,010
difference with the entrepreneurship

208
00:11:05,960 --> 00:11:16,070
culture in places like China is the fact

209
00:11:10,010 --> 00:11:18,620
that anything that is if you can take

210
00:11:16,070 --> 00:11:21,410
something to make it better if you can

211
00:11:18,620 --> 00:11:24,530
take your adversaries product and make

212
00:11:21,410 --> 00:11:29,689
it better that's acceptable that's

213
00:11:24,530 --> 00:11:32,209
acceptable and the difference as we look

214
00:11:29,690 --> 00:11:35,600
at the two major players in in data

215
00:11:32,210 --> 00:11:40,120
particularly in use what's with AI which

216
00:11:35,600 --> 00:11:43,450
is the evolution of how we utilize data

217
00:11:40,120 --> 00:11:47,120
is that while we are concentrating on

218
00:11:43,450 --> 00:11:50,740
the next great innovation nations like

219
00:11:47,120 --> 00:11:53,750
China Indonesia Russia and others are

220
00:11:50,740 --> 00:11:56,360
taking that data and creating massive

221
00:11:53,750 --> 00:11:59,990
quantities of practitioners in the

222
00:11:56,360 --> 00:12:12,710
application of those technologies and

223
00:11:59,990 --> 00:12:14,860
algorithms so let's take a just there's

224
00:12:12,710 --> 00:12:18,620
two basic ways to really quantify

225
00:12:14,860 --> 00:12:22,870
artificial intelligence I'm gonna use

226
00:12:18,620 --> 00:12:26,840
the example that closely really mimics

227
00:12:22,870 --> 00:12:31,760
comparison to the human brain to humans

228
00:12:26,840 --> 00:12:33,590
so these types of AI you know reactive

229
00:12:31,760 --> 00:12:37,780
AI has been around for a while

230
00:12:33,590 --> 00:12:42,250
it's basically reacting to two input

231
00:12:37,780 --> 00:12:46,540
limited memory which is capable of

232
00:12:42,250 --> 00:12:49,670
capable of learning from repeated

233
00:12:46,540 --> 00:12:52,760
processes of historical data and using

234
00:12:49,670 --> 00:12:54,500
historical data to make decisions those

235
00:12:52,760 --> 00:12:57,230
two exist

236
00:12:54,500 --> 00:13:00,320
and the good news is is only those two

237
00:12:57,230 --> 00:13:04,460
really exist right now the theory of

238
00:13:00,320 --> 00:13:10,250
mind where AI is used to better

239
00:13:04,460 --> 00:13:12,080
understand entities by being able to

240
00:13:10,250 --> 00:13:13,670
determine what their needs are their

241
00:13:12,080 --> 00:13:16,250
emotions their beliefs and thought

242
00:13:13,670 --> 00:13:19,189
processes that's being worked on right

243
00:13:16,250 --> 00:13:21,009
now emotional AI is

244
00:13:19,190 --> 00:13:24,410
it's in the works people are doing it

245
00:13:21,009 --> 00:13:29,839
what exists only in theory right now is

246
00:13:24,410 --> 00:13:32,509
self-awareness so and I don't think I

247
00:13:29,839 --> 00:13:34,910
have to explain to you what you know

248
00:13:32,509 --> 00:13:37,819
what self-awareness is so keeping that

249
00:13:34,910 --> 00:13:39,139
in mind if you read the synopsis of what

250
00:13:37,819 --> 00:13:41,389
I was going to talk about or what I'm

251
00:13:39,139 --> 00:13:46,730
talking about tonight I talked about

252
00:13:41,389 --> 00:13:47,930
rogue ai's so mmm and this all what I've

253
00:13:46,730 --> 00:13:49,370
been talking about it's really kind of a

254
00:13:47,930 --> 00:13:53,599
prelude to what we're going to get into

255
00:13:49,370 --> 00:13:55,639
right now so a couple years ago

256
00:13:53,600 --> 00:13:59,149
Amazon decided that they were going to

257
00:13:55,639 --> 00:14:00,920
take an AI tool which they built and

258
00:13:59,149 --> 00:14:04,790
they were going to put it in Ravello

259
00:14:00,920 --> 00:14:06,349
revolutionize their human resources what

260
00:14:04,790 --> 00:14:10,189
ended up happening is you can see by the

261
00:14:06,350 --> 00:14:14,839
headline is the algorithm that AI came

262
00:14:10,189 --> 00:14:20,899
back and said don't hire females so now

263
00:14:14,839 --> 00:14:23,689
why is that so what does it take to have

264
00:14:20,899 --> 00:14:25,850
a really good AI anybody know what do

265
00:14:23,689 --> 00:14:30,889
you have to have in order to develop

266
00:14:25,850 --> 00:14:34,360
artificial intelligence training data

267
00:14:30,889 --> 00:14:36,740
lots of data massive quantities of data

268
00:14:34,360 --> 00:14:38,600
massive quantities of data so now

269
00:14:36,740 --> 00:14:41,709
remember we're talking about data as

270
00:14:38,600 --> 00:14:44,139
currency all of these things if we can

271
00:14:41,709 --> 00:14:46,279
if we gain a competitive advantage

272
00:14:44,139 --> 00:14:48,800
whether in business or in medical

273
00:14:46,279 --> 00:14:52,279
processes or in banking by the

274
00:14:48,800 --> 00:14:53,750
implementation of AI then in order to

275
00:14:52,279 --> 00:14:57,350
lead in that field you have to have

276
00:14:53,750 --> 00:14:58,339
massive quantities of data so which

277
00:14:57,350 --> 00:15:01,519
brings up some really interesting

278
00:14:58,339 --> 00:15:04,040
questions considering data and privacy

279
00:15:01,519 --> 00:15:06,259
right now is is a huge issue and

280
00:15:04,040 --> 00:15:10,399
particularly in the West not so much in

281
00:15:06,259 --> 00:15:11,990
the East so Amazon decided we're going

282
00:15:10,399 --> 00:15:13,899
to take AI and we're gonna we're going

283
00:15:11,990 --> 00:15:18,920
to use it to hire the absolute best

284
00:15:13,899 --> 00:15:22,069
candidates for our company so why do you

285
00:15:18,920 --> 00:15:26,300
think that the AI came back and said

286
00:15:22,069 --> 00:15:28,069
don't hire women the reason was as the

287
00:15:26,300 --> 00:15:30,500
data set was flawed it wasn't actually

288
00:15:28,069 --> 00:15:32,540
it wasn't flawed the data set was an

289
00:15:30,500 --> 00:15:38,680
accurate representation a

290
00:15:32,540 --> 00:15:42,319
historically what had been done so the

291
00:15:38,680 --> 00:15:47,689
technology sector historically has been

292
00:15:42,320 --> 00:15:51,170
dominated by men so the data set that

293
00:15:47,690 --> 00:15:53,810
was used to program the AI reflected

294
00:15:51,170 --> 00:15:58,189
that so that brings up a very

295
00:15:53,810 --> 00:16:02,140
interesting ethical question as we look

296
00:15:58,190 --> 00:16:10,000
towards the development of AI and as we

297
00:16:02,140 --> 00:16:10,000
gather and process data for applications

298
00:16:10,150 --> 00:16:16,819
if our datasets are skewed like that

299
00:16:13,900 --> 00:16:21,280
should we be altering those datasets to

300
00:16:16,820 --> 00:16:27,290
reflect the values that we aspire to

301
00:16:21,280 --> 00:16:30,980
versus what has historically happened so

302
00:16:27,290 --> 00:16:34,630
if you answer yes to that question then

303
00:16:30,980 --> 00:16:40,240
the next question is who gets to do that

304
00:16:34,630 --> 00:16:41,590
is it Amazon it's at Facebook is it

305
00:16:40,240 --> 00:16:47,990
Alibaba

306
00:16:41,590 --> 00:16:58,040
so Huawei so very very complex and very

307
00:16:47,990 --> 00:17:01,550
interesting ethical questions so we move

308
00:16:58,040 --> 00:17:05,599
further down this path of ethics and

309
00:17:01,550 --> 00:17:07,310
data and security and I ask you to look

310
00:17:05,599 --> 00:17:12,230
at three different definitions that I've

311
00:17:07,310 --> 00:17:16,010
put up here so notice the similarities

312
00:17:12,230 --> 00:17:19,280
in what they are some could argue that

313
00:17:16,010 --> 00:17:22,970
they are a gradation of the same scale

314
00:17:19,280 --> 00:17:28,520
so advertising is about making people

315
00:17:22,970 --> 00:17:31,370
aware of something brainwashing if you

316
00:17:28,520 --> 00:17:36,080
look at bullet point two is persuasion

317
00:17:31,370 --> 00:17:39,739
by propaganda or salesmanship all right

318
00:17:36,080 --> 00:17:41,540
and the big buzzword of one of the big

319
00:17:39,740 --> 00:17:45,080
buzz words of the day social engineering

320
00:17:41,540 --> 00:17:46,220
is the use of centralized planning in an

321
00:17:45,080 --> 00:17:47,629
attempt to manage

322
00:17:46,220 --> 00:17:52,370
change and regulate the future

323
00:17:47,630 --> 00:17:54,890
development and behavior of a society so

324
00:17:52,370 --> 00:17:58,189
I ask you to consider the techniques at

325
00:17:54,890 --> 00:18:01,640
which those three things are employed in

326
00:17:58,190 --> 00:18:07,940
modern society and where we're headed

327
00:18:01,640 --> 00:18:10,570
with those so as anybody's seen the

328
00:18:07,940 --> 00:18:13,909
great hack on Netflix

329
00:18:10,570 --> 00:18:15,309
okay little dramatized but definitely

330
00:18:13,909 --> 00:18:18,020
drives a point home

331
00:18:15,309 --> 00:18:19,520
Cambridge analytical has anybody really

332
00:18:18,020 --> 00:18:21,559
thought about what Cambridge analytic

333
00:18:19,520 --> 00:18:23,179
could did and really you heard a little

334
00:18:21,559 --> 00:18:25,668
bit about it during the elections and

335
00:18:23,179 --> 00:18:28,760
things like that but when you take a

336
00:18:25,669 --> 00:18:33,110
look at what Cambridge analytic actually

337
00:18:28,760 --> 00:18:37,070
did they collected over 5,000 data

338
00:18:33,110 --> 00:18:40,280
points on every undecided voter in the

339
00:18:37,070 --> 00:18:43,460
United States of America and created

340
00:18:40,280 --> 00:18:47,149
individual advertising plans for those

341
00:18:43,460 --> 00:18:50,360
folks for those people so now I ask you

342
00:18:47,150 --> 00:18:53,710
reflect back on that last slide on those

343
00:18:50,360 --> 00:18:57,500
three definitions was that advertising

344
00:18:53,710 --> 00:19:01,850
was it brainwashing was it social

345
00:18:57,500 --> 00:19:03,190
engineering where do those definitions

346
00:19:01,850 --> 00:19:06,379
begin and end

347
00:19:03,190 --> 00:19:11,929
so again other very important questions

348
00:19:06,380 --> 00:19:17,360
that influence ethics in a digital world

349
00:19:11,929 --> 00:19:19,880
that's being transformed so social

350
00:19:17,360 --> 00:19:25,699
engineering by another definition is the

351
00:19:19,880 --> 00:19:30,260
application of social science to to

352
00:19:25,700 --> 00:19:33,590
individuals to populations so the more

353
00:19:30,260 --> 00:19:37,520
we learn about what influences society

354
00:19:33,590 --> 00:19:40,760
and what people react to whether it's at

355
00:19:37,520 --> 00:19:43,908
the basic level you know maslov's

356
00:19:40,760 --> 00:19:47,720
hierarchy of needs to the way that

357
00:19:43,909 --> 00:19:49,820
individuals interact to the way that

358
00:19:47,720 --> 00:19:51,940
groups interact the way that nations

359
00:19:49,820 --> 00:19:58,070
interact the application and

360
00:19:51,940 --> 00:19:59,929
understanding of how people predictably

361
00:19:58,070 --> 00:20:04,340
will behave when in

362
00:19:59,929 --> 00:20:06,799
reduced with information companies like

363
00:20:04,340 --> 00:20:11,649
Cambridge analytical in others Bell

364
00:20:06,799 --> 00:20:13,908
pottinger there's another organization

365
00:20:11,649 --> 00:20:18,739
practice the theories of social

366
00:20:13,909 --> 00:20:21,499
engineering on whole nations all right

367
00:20:18,740 --> 00:20:24,799
they did it in Africa to a massive scale

368
00:20:21,499 --> 00:20:25,429
and to some extent it's still going on

369
00:20:24,799 --> 00:20:30,320
today

370
00:20:25,429 --> 00:20:33,559
Cambridge analytical used elections in

371
00:20:30,320 --> 00:20:38,840
Africa as the test case for what

372
00:20:33,559 --> 00:20:41,840
happened in the EU with brexit if you

373
00:20:38,840 --> 00:20:44,029
didn't know that and with the last

374
00:20:41,840 --> 00:20:47,240
presidential elections in the United

375
00:20:44,029 --> 00:20:50,600
States they were confident in their

376
00:20:47,240 --> 00:20:53,350
ability to accomplish what our last

377
00:20:50,600 --> 00:20:56,418
president wanted during his campaign

378
00:20:53,350 --> 00:21:02,178
because they had already done it using

379
00:20:56,419 --> 00:21:02,950
Africa as a test case now they're not

380
00:21:02,179 --> 00:21:06,559
the only ones

381
00:21:02,950 --> 00:21:10,279
so they got caught and they were really

382
00:21:06,559 --> 00:21:15,590
good at it as we speak

383
00:21:10,279 --> 00:21:21,919
now countries like Ethiopia Nigeria are

384
00:21:15,590 --> 00:21:27,649
using spyware to influence dissidents in

385
00:21:21,919 --> 00:21:29,509
our own countries to also influence

386
00:21:27,649 --> 00:21:37,699
people within their own boundaries on

387
00:21:29,509 --> 00:21:39,789
how they should vote Bell Pottinger

388
00:21:37,700 --> 00:21:41,860
actually had to formally a pileup

389
00:21:39,789 --> 00:21:45,529
pologize

390
00:21:41,860 --> 00:21:49,998
because they stirred up so much racial

391
00:21:45,529 --> 00:21:54,529
hostility on the grounds of fabricated

392
00:21:49,999 --> 00:21:55,700
in justices and inequalities they

393
00:21:54,529 --> 00:22:02,299
actually had to come out and publicly

394
00:21:55,700 --> 00:22:05,570
apologize in South Africa for what they

395
00:22:02,299 --> 00:22:13,220
had done so the African National

396
00:22:05,570 --> 00:22:14,030
Congress hired hacking teams RCS to do

397
00:22:13,220 --> 00:22:16,520
the same thing

398
00:22:14,030 --> 00:22:19,639
and while doing research for this speak

399
00:22:16,520 --> 00:22:28,160
a company that I routinely do business

400
00:22:19,640 --> 00:22:31,220
with sold spyware to Ethiopia or that

401
00:22:28,160 --> 00:22:35,660
was used for some of the same dubious

402
00:22:31,220 --> 00:22:40,450
efforts so we have to begin to ask these

403
00:22:35,660 --> 00:22:44,750
questions is it the responsibility of

404
00:22:40,450 --> 00:22:48,920
the company or individual that creates

405
00:22:44,750 --> 00:22:51,320
the product is it their responsibilities

406
00:22:48,920 --> 00:22:56,240
should they be held liable what is their

407
00:22:51,320 --> 00:23:00,620
ethical responsibility or is it how that

408
00:22:56,240 --> 00:23:03,590
tool is used so if you look at business

409
00:23:00,620 --> 00:23:06,280
ethics it is the responsibility of a

410
00:23:03,590 --> 00:23:10,129
business leader to maximize the profits

411
00:23:06,280 --> 00:23:13,129
for their shareholders by that

412
00:23:10,130 --> 00:23:14,770
justification there's no there's there's

413
00:23:13,130 --> 00:23:17,930
there's not a standard for fairness

414
00:23:14,770 --> 00:23:19,430
there's no decent standard for fairness

415
00:23:17,930 --> 00:23:22,040
out there I mean the last administration

416
00:23:19,430 --> 00:23:25,130
had to create a consumer fairness

417
00:23:22,040 --> 00:23:27,590
division you know because of the way

418
00:23:25,130 --> 00:23:31,640
that companies were interacting with

419
00:23:27,590 --> 00:23:33,740
with the public so you have to ask

420
00:23:31,640 --> 00:23:37,520
yourself ethically who's responsible for

421
00:23:33,740 --> 00:23:41,180
that is it the person or people or

422
00:23:37,520 --> 00:23:45,080
organization utilizing the spyware or is

423
00:23:41,180 --> 00:23:48,530
it the company that makes it so very

424
00:23:45,080 --> 00:23:49,909
interesting yes it's gonna bring up that

425
00:23:48,530 --> 00:23:53,690
you know we did have an answer to that

426
00:23:49,910 --> 00:23:56,450
years ago in South Africa and it was

427
00:23:53,690 --> 00:23:59,330
called divestiture that it was the

428
00:23:56,450 --> 00:24:01,600
responsibility of the company owners to

429
00:23:59,330 --> 00:24:06,379
say we're not going to own a company

430
00:24:01,600 --> 00:24:08,659
that does these things and guess what

431
00:24:06,380 --> 00:24:13,490
that affects the return to the

432
00:24:08,660 --> 00:24:15,580
shareholders and then the CEOs in their

433
00:24:13,490 --> 00:24:20,090
duty to the shareholders have to respond

434
00:24:15,580 --> 00:24:21,800
sure so you know I would say that there

435
00:24:20,090 --> 00:24:22,939
is probably a bit of a there's a

436
00:24:21,800 --> 00:24:25,940
difference between a Cambridge

437
00:24:22,940 --> 00:24:27,149
analytical and in the company that made

438
00:24:25,940 --> 00:24:28,769
the spyware in the

439
00:24:27,149 --> 00:24:32,699
fact that Cambridge analytic was all

440
00:24:28,769 --> 00:24:35,339
about the application and the intent the

441
00:24:32,700 --> 00:24:39,029
for the use of the products and

442
00:24:35,339 --> 00:24:41,339
analytics that it was doing whereas you

443
00:24:39,029 --> 00:24:45,629
could you know ethically you can argue

444
00:24:41,339 --> 00:24:46,918
you know you it's not the gun owner you

445
00:24:45,629 --> 00:24:48,869
know or it's not the gun it's the gun

446
00:24:46,919 --> 00:24:50,339
owner it's the intent of the gun owner

447
00:24:48,869 --> 00:24:55,678
and even to a certain extent our

448
00:24:50,339 --> 00:25:00,239
judicial system is as consideration for

449
00:24:55,679 --> 00:25:03,029
great consideration for intent so so

450
00:25:00,239 --> 00:25:04,289
that brings I mean it's a very I mean

451
00:25:03,029 --> 00:25:08,129
this is exactly I'm talking about it's a

452
00:25:04,289 --> 00:25:10,259
very interesting complex organ achill

453
00:25:08,129 --> 00:25:12,839
question so for sure but you know

454
00:25:10,259 --> 00:25:15,149
Cambridge analytical Bell Pottinger both

455
00:25:12,839 --> 00:25:18,349
went out of business when it was

456
00:25:15,149 --> 00:25:22,859
discovered what they were doing so

457
00:25:18,349 --> 00:25:24,389
however had they not gotten caught my

458
00:25:22,859 --> 00:25:27,389
question I mean look what they already

459
00:25:24,389 --> 00:25:28,859
did before they they got caught so did

460
00:25:27,389 --> 00:25:31,439
they really even get caught were they

461
00:25:28,859 --> 00:25:33,359
doing something that was illegal were

462
00:25:31,440 --> 00:25:36,029
they doing something that was immoral or

463
00:25:33,359 --> 00:25:40,859
unethical you know those are the

464
00:25:36,029 --> 00:25:42,509
questions we have to ask you know so but

465
00:25:40,859 --> 00:25:49,199
that's a that's a great court a great

466
00:25:42,509 --> 00:25:51,419
point so in my in my in my little

467
00:25:49,200 --> 00:25:56,119
write-up for this I talked about also

468
00:25:51,419 --> 00:26:01,190
about you know a fantasy world where

469
00:25:56,119 --> 00:26:04,289
millions of cameras track track eeeh and

470
00:26:01,190 --> 00:26:06,029
using predictive analytics they

471
00:26:04,289 --> 00:26:08,658
determine whether or not you're going to

472
00:26:06,029 --> 00:26:11,399
say something bad against the government

473
00:26:08,659 --> 00:26:16,649
and the facial recognition software

474
00:26:11,399 --> 00:26:19,649
picks you out of a crowd the storm

475
00:26:16,649 --> 00:26:23,879
troopers reach down into the grout crowd

476
00:26:19,649 --> 00:26:27,629
and grab you and then they take you away

477
00:26:23,879 --> 00:26:34,609
to a reprogramming camp so sounds pretty

478
00:26:27,629 --> 00:26:34,609
fantastic doesn't it so welcome to China

479
00:26:34,780 --> 00:26:43,000
so as we speak the Muslim minority in

480
00:26:39,250 --> 00:26:45,670
the north part of China the Chinese

481
00:26:43,000 --> 00:26:48,250
government is using facial recognition

482
00:26:45,670 --> 00:26:50,350
and predictive analysis and some of the

483
00:26:48,250 --> 00:26:52,090
criteria that was recently linked on how

484
00:26:50,350 --> 00:26:54,990
they're determining whether or not

485
00:26:52,090 --> 00:26:57,790
somebody is a dissident is as vague as

486
00:26:54,990 --> 00:27:00,100
they had a beard or once had a beard I

487
00:26:57,790 --> 00:27:03,100
mean this are real documents that are

488
00:27:00,100 --> 00:27:04,870
that are you can go and look at the

489
00:27:03,100 --> 00:27:09,699
criteria that they're using to select

490
00:27:04,870 --> 00:27:13,840
people so they're doing at some level

491
00:27:09,700 --> 00:27:17,530
predictive analysis on who might cause

492
00:27:13,840 --> 00:27:20,500
social unrest now understand that while

493
00:27:17,530 --> 00:27:22,210
we see this as potentially a crime

494
00:27:20,500 --> 00:27:25,660
against humanity against free will and

495
00:27:22,210 --> 00:27:29,500
democracy you have to understand the

496
00:27:25,660 --> 00:27:32,260
cultural mindset you know not everybody

497
00:27:29,500 --> 00:27:35,290
thinks like we do I know it's a crazy

498
00:27:32,260 --> 00:27:36,910
concept all right you know but not

499
00:27:35,290 --> 00:27:39,460
everybody thinks like the West

500
00:27:36,910 --> 00:27:40,810
particularly like the United States the

501
00:27:39,460 --> 00:27:43,270
United States was a country that was

502
00:27:40,810 --> 00:27:45,100
founded the city on the hill right we're

503
00:27:43,270 --> 00:27:47,710
supposed to be the great utopia for all

504
00:27:45,100 --> 00:27:49,120
the religious organizations that got

505
00:27:47,710 --> 00:27:50,950
kicked out every decent country in

506
00:27:49,120 --> 00:27:54,969
Europe so they were gonna come here and

507
00:27:50,950 --> 00:27:58,300
build this shining city on a hill all

508
00:27:54,970 --> 00:27:59,950
right but in some cultures and it's not

509
00:27:58,300 --> 00:28:02,340
right wrong or indifferent necessarily

510
00:27:59,950 --> 00:28:09,480
but in some cultures social stability is

511
00:28:02,340 --> 00:28:13,629
more important than individual rights so

512
00:28:09,480 --> 00:28:18,270
right now as I said the Muslim minority

513
00:28:13,630 --> 00:28:20,740
in northern China is being picked up

514
00:28:18,270 --> 00:28:23,290
they're being there's a huge massive

515
00:28:20,740 --> 00:28:26,230
intelligence collection data collection

516
00:28:23,290 --> 00:28:29,649
effort both a physical one with agents

517
00:28:26,230 --> 00:28:35,590
operating within the populations and the

518
00:28:29,650 --> 00:28:38,380
utilization of AI m and M L and D L in

519
00:28:35,590 --> 00:28:42,100
determining who the people are who are

520
00:28:38,380 --> 00:28:45,429
likely to become dissidents as I said

521
00:28:42,100 --> 00:28:47,909
facial recognition software millions and

522
00:28:45,429 --> 00:28:52,080
millions and millions of cameras

523
00:28:47,910 --> 00:28:56,150
all connected being able to identify

524
00:28:52,080 --> 00:28:59,520
those people and they're taken away to

525
00:28:56,150 --> 00:29:01,320
camps to be reprogrammed and this is an

526
00:28:59,520 --> 00:29:05,639
actual satellite photo of one of those

527
00:29:01,320 --> 00:29:09,330
camps so they exist

528
00:29:05,640 --> 00:29:11,810
the official message on this from the

529
00:29:09,330 --> 00:29:13,740
Chinese government and I feel like I'm

530
00:29:11,810 --> 00:29:17,490
don't feel like I'm picking on the

531
00:29:13,740 --> 00:29:22,860
Chinese so much but right now they're

532
00:29:17,490 --> 00:29:28,740
you know they are the the other AI world

533
00:29:22,860 --> 00:29:30,959
power so but the official the official

534
00:29:28,740 --> 00:29:35,580
line from the Chinese government is that

535
00:29:30,960 --> 00:29:37,890
they are you know teaching them how to

536
00:29:35,580 --> 00:29:40,230
better integrate was to integrate with

537
00:29:37,890 --> 00:29:42,180
society but the fact of the matter is is

538
00:29:40,230 --> 00:29:44,250
you have to ask yourself how does this

539
00:29:42,180 --> 00:29:46,800
different from other regimes that we've

540
00:29:44,250 --> 00:29:49,610
seen try and accomplish this with more

541
00:29:46,800 --> 00:29:52,379
brutal or less sophisticated methods

542
00:29:49,610 --> 00:29:57,209
like we saw in Cambodia and Vietnam and

543
00:29:52,380 --> 00:30:01,380
in other places even in Russia so so

544
00:29:57,210 --> 00:30:03,780
this fantastical environment you know

545
00:30:01,380 --> 00:30:05,760
where where you've got Tom Cruise

546
00:30:03,780 --> 00:30:11,370
running around trying to prevent crimes

547
00:30:05,760 --> 00:30:15,629
before they happen to some extent it's

548
00:30:11,370 --> 00:30:18,840
it's here it's here it's level to AI but

549
00:30:15,630 --> 00:30:21,480
it's here so the good news is if you go

550
00:30:18,840 --> 00:30:23,899
back to that that that classification

551
00:30:21,480 --> 00:30:28,230
for AI is we're still only at level 2

552
00:30:23,900 --> 00:30:29,820
which means right now is the time that

553
00:30:28,230 --> 00:30:31,770
we need to ask ourselves these

554
00:30:29,820 --> 00:30:37,379
incredibly hard questions about ethics

555
00:30:31,770 --> 00:30:40,889
and how we want our data to be utilized

556
00:30:37,380 --> 00:30:46,290
and how we want our algorithms to

557
00:30:40,890 --> 00:30:49,620
represent whatever values we choose so

558
00:30:46,290 --> 00:30:52,590
if this scares you and you think well

559
00:30:49,620 --> 00:30:56,520
hey you know at least we're not doing

560
00:30:52,590 --> 00:30:59,300
that in the US all right anybody ever

561
00:30:56,520 --> 00:30:59,300
heard of be linked

562
00:31:01,169 --> 00:31:05,799
okay

563
00:31:02,350 --> 00:31:08,320
so be link is the city of Indianapolis

564
00:31:05,799 --> 00:31:11,830
that's partnered with businesses and

565
00:31:08,320 --> 00:31:14,860
residents all through Marion County who

566
00:31:11,830 --> 00:31:17,439
can voluntarily connect to the police

567
00:31:14,860 --> 00:31:23,590
connect their security cameras to the

568
00:31:17,440 --> 00:31:27,700
police intelligence center so not quite

569
00:31:23,590 --> 00:31:31,720
a nation sponsored effort to subjugate a

570
00:31:27,700 --> 00:31:35,890
whole culture or society or religion but

571
00:31:31,720 --> 00:31:37,750
what I'm telling you is is that if we if

572
00:31:35,890 --> 00:31:40,660
we don't address the human ethical

573
00:31:37,750 --> 00:31:43,059
aspect of this it's not a leap too far

574
00:31:40,660 --> 00:31:45,900
that this technology that we're using on

575
00:31:43,059 --> 00:31:50,830
our own is utilized for a nefarious

576
00:31:45,900 --> 00:31:54,250
purpose right now this system is being

577
00:31:50,830 --> 00:31:57,370
used to solve crime you know and it's

578
00:31:54,250 --> 00:31:59,470
within the police department in

579
00:31:57,370 --> 00:32:04,330
Indianapolis they do predictive analysis

580
00:31:59,470 --> 00:32:07,210
on crime they absolutely do but my point

581
00:32:04,330 --> 00:32:13,540
to this is is it's not just China it's

582
00:32:07,210 --> 00:32:16,990
here - it's here - and we have not only

583
00:32:13,540 --> 00:32:19,540
a responsibility but an obligation to

584
00:32:16,990 --> 00:32:24,970
inform the people that control this

585
00:32:19,540 --> 00:32:27,309
technology of what values that we want

586
00:32:24,970 --> 00:32:33,880
for them to represent as a society we

587
00:32:27,309 --> 00:32:35,260
have to determine that so that brings up

588
00:32:33,880 --> 00:32:36,940
a really interesting question I've

589
00:32:35,260 --> 00:32:38,740
talked a lot about technology and the

590
00:32:36,940 --> 00:32:41,890
use of data and how people are using it

591
00:32:38,740 --> 00:32:43,450
and and it's a tool good or is it bad is

592
00:32:41,890 --> 00:32:48,610
that the way it's used is it not the way

593
00:32:43,450 --> 00:32:52,150
it's used and we've been talking about

594
00:32:48,610 --> 00:32:54,909
ethics for a long long time as as a race

595
00:32:52,150 --> 00:32:57,309
as a human race so you can see Immanuel

596
00:32:54,910 --> 00:32:59,980
Kant deciding whether or not one's

597
00:32:57,309 --> 00:33:01,830
actions are moral it should be

598
00:32:59,980 --> 00:33:08,070
considered what the universal benefit is

599
00:33:01,830 --> 00:33:11,199
so by Kant's theory if I were to steal I

600
00:33:08,070 --> 00:33:12,730
have to ask myself if I want to

601
00:33:11,200 --> 00:33:14,559
determine whether or not it's more

602
00:33:12,730 --> 00:33:15,910
or ethical I have to ask myself if

603
00:33:14,559 --> 00:33:17,010
everybody's told would that be a good

604
00:33:15,910 --> 00:33:20,309
thing or a bad thing

605
00:33:17,010 --> 00:33:23,890
and that's how Khan said we should

606
00:33:20,309 --> 00:33:28,960
determine what our ethics are so Karl

607
00:33:23,890 --> 00:33:31,450
Marx and Friedrich Engels they believe

608
00:33:28,960 --> 00:33:32,950
that you can't have a universal code of

609
00:33:31,450 --> 00:33:35,830
ethics and and again that's another

610
00:33:32,950 --> 00:33:37,600
question as nations nations use to

611
00:33:35,830 --> 00:33:40,270
decide what their ethics and values are

612
00:33:37,600 --> 00:33:42,309
but as we look in a digital economy and

613
00:33:40,270 --> 00:33:46,090
we look in a digital world those

614
00:33:42,309 --> 00:33:50,730
boundaries mean less and less they mean

615
00:33:46,090 --> 00:33:53,980
less you know you can experience the the

616
00:33:50,730 --> 00:33:57,309
life of somebody in Syria through

617
00:33:53,980 --> 00:33:58,540
Twitter to a great extent things that we

618
00:33:57,309 --> 00:34:01,270
could never access before that

619
00:33:58,540 --> 00:34:03,250
one-to-one human connection and and

620
00:34:01,270 --> 00:34:05,620
transmitting of experience and sharing

621
00:34:03,250 --> 00:34:07,179
of experience now doesn't recognize any

622
00:34:05,620 --> 00:34:11,888
borders nobody can control that anymore

623
00:34:07,179 --> 00:34:13,780
so we look at things we have to start

624
00:34:11,889 --> 00:34:16,409
looking at things from what what is

625
00:34:13,780 --> 00:34:18,970
Universal what are the universal values

626
00:34:16,409 --> 00:34:21,490
that we all have to subscribe to because

627
00:34:18,969 --> 00:34:22,959
as as all this data comes together and

628
00:34:21,489 --> 00:34:26,319
all these algorithms come together and

629
00:34:22,960 --> 00:34:29,859
right now AI level 2 is being used

630
00:34:26,320 --> 00:34:32,409
primarily in business and medical and

631
00:34:29,859 --> 00:34:35,109
finance and a few verticals all right

632
00:34:32,409 --> 00:34:38,950
and believe me we're we're a long way

633
00:34:35,109 --> 00:34:42,699
from self-actualization we're a long way

634
00:34:38,949 --> 00:34:45,129
from that long long long way but it's

635
00:34:42,699 --> 00:34:50,020
coming it's coming

636
00:34:45,129 --> 00:34:52,000
so so again the point is we need to

637
00:34:50,020 --> 00:34:55,210
start looking at universal values but

638
00:34:52,000 --> 00:34:58,060
Karl Marx and Frederick Engels they said

639
00:34:55,210 --> 00:35:01,840
there can't be any universal values

640
00:34:58,060 --> 00:35:03,790
because ethics and values are dependent

641
00:35:01,840 --> 00:35:05,440
upon the economic situation of the

642
00:35:03,790 --> 00:35:07,480
nation that it's in there's a certain

643
00:35:05,440 --> 00:35:11,260
amount of relativity that goes along

644
00:35:07,480 --> 00:35:14,350
with that what is now flawed in that

645
00:35:11,260 --> 00:35:18,670
theory again as we look at what is the

646
00:35:14,350 --> 00:35:22,150
importance of nations and borders so we

647
00:35:18,670 --> 00:35:24,910
are experiencing cultural one-to-one

648
00:35:22,150 --> 00:35:26,260
human contact on a massive scale through

649
00:35:24,910 --> 00:35:30,720
the revolution of data and

650
00:35:26,260 --> 00:35:35,730
technology right now so I challenged

651
00:35:30,720 --> 00:35:38,589
this theory in that it may have been

652
00:35:35,730 --> 00:35:42,490
accurate to some extent at one time but

653
00:35:38,590 --> 00:35:45,460
it's less and less accurate now so as we

654
00:35:42,490 --> 00:35:48,310
work towards convergence we are less and

655
00:35:45,460 --> 00:35:52,900
I may get I was an army for a long long

656
00:35:48,310 --> 00:35:55,140
time so this is this is not a betrayal

657
00:35:52,900 --> 00:35:57,670
of our nation but it's less and less

658
00:35:55,140 --> 00:36:00,330
important on an individual level whether

659
00:35:57,670 --> 00:36:05,470
or not we're Americans or Saudis or

660
00:36:00,330 --> 00:36:10,090
Chinese or Panamanian it's less and less

661
00:36:05,470 --> 00:36:12,819
important so Max Steiner the next

662
00:36:10,090 --> 00:36:17,590
evolution you know the common good is

663
00:36:12,820 --> 00:36:19,090
only an illusion because the only ethics

664
00:36:17,590 --> 00:36:22,330
that are benefits are the ones that

665
00:36:19,090 --> 00:36:24,790
benefit the self so if you go to the

666
00:36:22,330 --> 00:36:28,870
fact that we are all born naked alone

667
00:36:24,790 --> 00:36:31,779
and we all die alone his argument is is

668
00:36:28,870 --> 00:36:34,630
that you know ethics we can justify

669
00:36:31,780 --> 00:36:37,270
anything that becomes self-serving so

670
00:36:34,630 --> 00:36:40,960
and then Frederick Nietzsche everybody's

671
00:36:37,270 --> 00:36:42,940
phrase the favorite crazy person thought

672
00:36:40,960 --> 00:36:46,360
that everything that the powerful do is

673
00:36:42,940 --> 00:36:47,620
is moral so I don't know where you got

674
00:36:46,360 --> 00:36:49,330
that but I don't know where you got a

675
00:36:47,620 --> 00:36:52,830
lot of us is theories I'm sure there's

676
00:36:49,330 --> 00:37:01,930
something to it I just find it to be

677
00:36:52,830 --> 00:37:06,450
ridiculous so we look at the data

678
00:37:01,930 --> 00:37:09,910
revolution we look at the security of

679
00:37:06,450 --> 00:37:13,480
ourselves or personal information the

680
00:37:09,910 --> 00:37:15,520
way that we live democracy freedom think

681
00:37:13,480 --> 00:37:17,080
about think about democracy and social

682
00:37:15,520 --> 00:37:19,120
engineering there's something if you

683
00:37:17,080 --> 00:37:21,580
want to chew on it for a while you know

684
00:37:19,120 --> 00:37:24,720
are the choices you make really your

685
00:37:21,580 --> 00:37:31,120
choices anymore when we're applying

686
00:37:24,720 --> 00:37:35,890
social science theory to marketing so

687
00:37:31,120 --> 00:37:37,029
one could argue that those X amount of

688
00:37:35,890 --> 00:37:41,859
votes

689
00:37:37,030 --> 00:37:43,960
in the last election you know did they

690
00:37:41,859 --> 00:37:47,279
really vote the way that they wanted to

691
00:37:43,960 --> 00:37:56,140
or did we cross that social engineering

692
00:37:47,280 --> 00:37:58,060
brainwashing advertising boundary so

693
00:37:56,140 --> 00:38:01,990
there's another theory of moral

694
00:37:58,060 --> 00:38:06,490
development and I ask you to give some

695
00:38:01,990 --> 00:38:11,890
consideration to this as we evolve as a

696
00:38:06,490 --> 00:38:14,859
as humankind as as a race as the human

697
00:38:11,890 --> 00:38:18,819
race I ask you to keep this in mind can

698
00:38:14,859 --> 00:38:24,960
we evolve to a level of individual

699
00:38:18,820 --> 00:38:27,550
ethical standards that in an age where

700
00:38:24,960 --> 00:38:29,530
individuals can access power and

701
00:38:27,550 --> 00:38:32,260
knowledge at massive scales

702
00:38:29,530 --> 00:38:33,970
all right you no longer have to come to

703
00:38:32,260 --> 00:38:38,050
Purdue University to be the world's best

704
00:38:33,970 --> 00:38:41,589
hacker all right now I'm not I'm not

705
00:38:38,050 --> 00:38:44,530
saying don't go to Purdue University but

706
00:38:41,589 --> 00:38:46,930
my point is is that in the 80s we had a

707
00:38:44,530 --> 00:38:49,570
saying information wants to be free

708
00:38:46,930 --> 00:38:51,160
we are very idealistic and when the

709
00:38:49,570 --> 00:38:54,280
universities and the government owned

710
00:38:51,160 --> 00:38:56,049
the internet we would break into the

711
00:38:54,280 --> 00:38:57,880
Internet in order to gain access to

712
00:38:56,050 --> 00:39:00,040
knowledge and what we thought was that

713
00:38:57,880 --> 00:39:02,470
access to knowledge will allow people to

714
00:39:00,040 --> 00:39:04,720
raise themselves up out of their

715
00:39:02,470 --> 00:39:08,470
condition just like the printing press

716
00:39:04,720 --> 00:39:10,060
and the Gutenberg Bible you know we

717
00:39:08,470 --> 00:39:13,450
believe that the one thing we didn't

718
00:39:10,060 --> 00:39:16,359
take into account was that very very

719
00:39:13,450 --> 00:39:18,250
very basic ethical question and that is

720
00:39:16,359 --> 00:39:20,770
it's not knowledged it's inherently good

721
00:39:18,250 --> 00:39:22,119
or evil it's how you use it it's the

722
00:39:20,770 --> 00:39:24,099
application of that knowledge that's

723
00:39:22,119 --> 00:39:26,109
inherently good or evil so with

724
00:39:24,099 --> 00:39:30,250
individual access to massive quantities

725
00:39:26,109 --> 00:39:32,259
of data and with the convergence of all

726
00:39:30,250 --> 00:39:34,000
of this data and the refinement of

727
00:39:32,260 --> 00:39:37,330
algorithmics and the development of AI

728
00:39:34,000 --> 00:39:39,820
and all these things you know we have to

729
00:39:37,330 --> 00:39:42,609
ask ourselves do we need to evolve

730
00:39:39,820 --> 00:39:44,260
ethically as humans when we have the

731
00:39:42,609 --> 00:39:49,740
responsibility of having so much

732
00:39:44,260 --> 00:39:49,740
knowledge and power at our fingertips so

733
00:39:49,950 --> 00:39:57,328
it's a huge question and it's it's a

734
00:39:53,250 --> 00:39:59,190
very complex area and to be honest I

735
00:39:57,329 --> 00:40:00,809
haven't seen a lot of people looking at

736
00:39:59,190 --> 00:40:04,619
it

737
00:40:00,809 --> 00:40:06,690
there is a lot of questions about ethics

738
00:40:04,619 --> 00:40:11,369
within certain verticals and certain

739
00:40:06,690 --> 00:40:13,890
subjects in certain areas but you know I

740
00:40:11,369 --> 00:40:17,089
put up here this example of X answers

741
00:40:13,890 --> 00:40:22,288
universal principles of data ethics you

742
00:40:17,089 --> 00:40:26,520
know it is one of the very few stabs

743
00:40:22,289 --> 00:40:29,630
I've seen at someone or some industry or

744
00:40:26,520 --> 00:40:32,940
somebody in industry starting to say

745
00:40:29,630 --> 00:40:36,150
where does the responsibility lie does

746
00:40:32,940 --> 00:40:38,549
it lie with the with the coder does it

747
00:40:36,150 --> 00:40:41,309
lie with the you know doesn't lie with

748
00:40:38,549 --> 00:40:45,900
the company does it lie with the person

749
00:40:41,309 --> 00:40:51,539
who utilizes the tool and I just don't

750
00:40:45,900 --> 00:40:53,400
see a lot of work getting done in 48 the

751
00:40:51,539 --> 00:40:55,770
UN came up with the Universal

752
00:40:53,400 --> 00:41:00,029
Declaration of Human Rights

753
00:40:55,770 --> 00:41:03,230
how is that gone you know we still can't

754
00:41:00,029 --> 00:41:09,750
enforce Human Rights around the world

755
00:41:03,230 --> 00:41:12,510
you know so when we look at universal

756
00:41:09,750 --> 00:41:14,970
ethics or universal code of conduct and

757
00:41:12,510 --> 00:41:17,220
we do it through the cultural lens of

758
00:41:14,970 --> 00:41:21,779
different countries and different

759
00:41:17,220 --> 00:41:27,390
societies it becomes an incredibly

760
00:41:21,779 --> 00:41:31,470
complex ethical question so what makes

761
00:41:27,390 --> 00:41:33,598
one value system better than the other

762
00:41:31,470 --> 00:41:36,868
but what I can tell you is is with the

763
00:41:33,599 --> 00:41:38,250
interconnectivity of individuals this

764
00:41:36,869 --> 00:41:40,410
this question is going to become more

765
00:41:38,250 --> 00:41:42,359
and more important what are the

766
00:41:40,410 --> 00:41:46,288
universal values we can wait for it to

767
00:41:42,359 --> 00:41:50,160
evolve but can we wait for those

768
00:41:46,289 --> 00:41:53,640
universal values to evolve while we rush

769
00:41:50,160 --> 00:41:58,319
into technology like quantum computing

770
00:41:53,640 --> 00:42:02,680
and AI in other areas so I've seen some

771
00:41:58,319 --> 00:42:06,330
things out there global bioethics

772
00:42:02,680 --> 00:42:10,960
anybody anybody's seen anything on

773
00:42:06,330 --> 00:42:12,880
biohacking so CRISPR technology that you

774
00:42:10,960 --> 00:42:16,330
can buy kits on the internet now do it

775
00:42:12,880 --> 00:42:19,630
in your garage so we're now starting

776
00:42:16,330 --> 00:42:21,580
just now starting to convene global

777
00:42:19,630 --> 00:42:24,850
organizations to talk about these things

778
00:42:21,580 --> 00:42:29,980
and to help define what those ethical

779
00:42:24,850 --> 00:42:33,850
responsibilities are you know is

780
00:42:29,980 --> 00:42:36,010
withholding a cure because you don't

781
00:42:33,850 --> 00:42:37,480
know or a solution potential solution

782
00:42:36,010 --> 00:42:40,870
because you don't know the effects the

783
00:42:37,480 --> 00:42:45,280
long-term effects of it you know is that

784
00:42:40,870 --> 00:42:52,240
ethical versus unleashing unproven or

785
00:42:45,280 --> 00:42:54,220
untested technology out there so I've

786
00:42:52,240 --> 00:42:58,330
talked about kind of a lot of things at

787
00:42:54,220 --> 00:43:02,049
a really high level here and I hope that

788
00:42:58,330 --> 00:43:08,770
I'm giving you something to really chew

789
00:43:02,050 --> 00:43:11,140
on it's not what I'll be able to effect

790
00:43:08,770 --> 00:43:14,830
in the remainder of my time here on

791
00:43:11,140 --> 00:43:20,220
earth in this area will be will be

792
00:43:14,830 --> 00:43:23,500
pretty small but those of you that are

793
00:43:20,220 --> 00:43:25,870
still have many more footsteps on this

794
00:43:23,500 --> 00:43:28,450
earth these are the decisions that

795
00:43:25,870 --> 00:43:31,960
you're going to have to make these are

796
00:43:28,450 --> 00:43:37,029
the things that you will confront these

797
00:43:31,960 --> 00:43:38,590
are you will become the leaders and the

798
00:43:37,030 --> 00:43:41,380
researchers and the scientists and the

799
00:43:38,590 --> 00:43:45,760
coders and you will have to make those

800
00:43:41,380 --> 00:43:48,040
moral ethical decisions at some point on

801
00:43:45,760 --> 00:43:50,530
how your technology or how the

802
00:43:48,040 --> 00:43:52,240
technology in your life influences you

803
00:43:50,530 --> 00:43:54,190
and this decisions that you make your

804
00:43:52,240 --> 00:44:01,240
quality of life how it impacts other

805
00:43:54,190 --> 00:44:04,420
people so I would just ask you to the

806
00:44:01,240 --> 00:44:06,850
more complicated things get is to go

807
00:44:04,420 --> 00:44:09,700
back to the most simple things and that

808
00:44:06,850 --> 00:44:13,200
is how do we wish to consult ourselves

809
00:44:09,700 --> 00:44:15,359
as a human race how do we wish to treat

810
00:44:13,200 --> 00:44:18,859
each other

811
00:44:15,360 --> 00:44:22,140
how do we want the future to look and

812
00:44:18,860 --> 00:44:26,640
you know work on those work on those

813
00:44:22,140 --> 00:44:28,230
hard questions they're yours so that's

814
00:44:26,640 --> 00:44:30,180
all I have for tonight I appreciate all

815
00:44:28,230 --> 00:44:34,040
of you coming out I'll be happy to

816
00:44:30,180 --> 00:44:36,600
answer any questions that you might have

817
00:44:34,040 --> 00:44:44,520
I know that's a lot of really deep heavy

818
00:44:36,600 --> 00:44:46,650
stuff so Frank how do you think like the

819
00:44:44,520 --> 00:44:48,120
state and the federal government are

820
00:44:46,650 --> 00:44:51,360
kind of addressing some of these key

821
00:44:48,120 --> 00:44:54,750
issues they're not they're not right now

822
00:44:51,360 --> 00:44:57,000
the industry the industry is smart

823
00:44:54,750 --> 00:44:59,250
enough so it's been pretty interesting

824
00:44:57,000 --> 00:45:00,840
so specifically to AI we've heard

825
00:44:59,250 --> 00:45:03,240
several tech leaders prominent tech

826
00:45:00,840 --> 00:45:06,290
leaders come out and say actually say to

827
00:45:03,240 --> 00:45:10,109
the government we need to regulate this

828
00:45:06,290 --> 00:45:11,670
so what I would what I would say is

829
00:45:10,110 --> 00:45:13,950
there's a lot there's a lot of great

830
00:45:11,670 --> 00:45:15,270
people in Washington DC I mean I believe

831
00:45:13,950 --> 00:45:17,790
that there's a lot of people there that

832
00:45:15,270 --> 00:45:19,259
are trying to do the right thing but I

833
00:45:17,790 --> 00:45:21,570
don't know if there's a level of

834
00:45:19,260 --> 00:45:24,680
understanding about the effects of what

835
00:45:21,570 --> 00:45:27,960
technology and data and convergence

836
00:45:24,680 --> 00:45:32,580
singularity what that's bringing for the

837
00:45:27,960 --> 00:45:33,960
future and I'm not sure that the

838
00:45:32,580 --> 00:45:35,580
industry right now particularly in the

839
00:45:33,960 --> 00:45:40,710
United States is very interested in

840
00:45:35,580 --> 00:45:43,230
influencing privacy and how how I could

841
00:45:40,710 --> 00:45:45,240
segue to that is is the fact that with

842
00:45:43,230 --> 00:45:47,370
privacy that limits the data that can be

843
00:45:45,240 --> 00:45:50,009
collected then how it can be utilized to

844
00:45:47,370 --> 00:45:52,970
some extent in China they don't have

845
00:45:50,010 --> 00:45:56,370
those restrictions the the population

846
00:45:52,970 --> 00:46:00,020
freely gives up their data in order for

847
00:45:56,370 --> 00:46:02,400
convenience so and to some extent we do

848
00:46:00,020 --> 00:46:04,950
we give it up in Facebook and we give it

849
00:46:02,400 --> 00:46:06,510
up in in Amazon and their web browsers

850
00:46:04,950 --> 00:46:09,240
and all those things and those 15 page

851
00:46:06,510 --> 00:46:13,830
legal disclaimers that nobody ever reads

852
00:46:09,240 --> 00:46:17,720
but we all check off on so right now to

853
00:46:13,830 --> 00:46:20,400
my knowledge there are no international

854
00:46:17,720 --> 00:46:27,270
work groups or think tanks that are

855
00:46:20,400 --> 00:46:27,990
specifically looking at ethics and data

856
00:46:27,270 --> 00:46:29,850
in

857
00:46:27,990 --> 00:46:31,830
in the application II I'm sure there are

858
00:46:29,850 --> 00:46:34,110
some out there but I'm telling you

859
00:46:31,830 --> 00:46:38,940
there's it's just not it's our

860
00:46:34,110 --> 00:46:40,440
governments aren't looking at it oh is

861
00:46:38,940 --> 00:46:42,270
there anything in Israel they're doing

862
00:46:40,440 --> 00:46:45,420
that's unique or any best practices over

863
00:46:42,270 --> 00:46:47,759
there that we should be duplicating so

864
00:46:45,420 --> 00:46:51,600
again Israel is my experience with

865
00:46:47,760 --> 00:46:55,050
Israel is that they are an incredibly

866
00:46:51,600 --> 00:46:56,880
innovative society and I believe that

867
00:46:55,050 --> 00:46:57,420
innovation is born of three different

868
00:46:56,880 --> 00:46:59,480
things

869
00:46:57,420 --> 00:47:02,070
innovation is born out of research

870
00:46:59,480 --> 00:47:04,500
innovation is born out of businesses

871
00:47:02,070 --> 00:47:07,050
that do business daily that have to

872
00:47:04,500 --> 00:47:09,150
invent processes or niche naturally come

873
00:47:07,050 --> 00:47:12,330
up with processes that make their lives

874
00:47:09,150 --> 00:47:13,920
easier and then necessity I mean

875
00:47:12,330 --> 00:47:16,529
necessity is the mother of invention

876
00:47:13,920 --> 00:47:18,930
so specifically within Israel they're

877
00:47:16,530 --> 00:47:26,100
constantly at war particularly in

878
00:47:18,930 --> 00:47:29,580
cyberspace so they are very good at at

879
00:47:26,100 --> 00:47:32,130
technology - they're the second largest

880
00:47:29,580 --> 00:47:34,290
exporter of goods and services for cyber

881
00:47:32,130 --> 00:47:39,150
security in the world which is about 30

882
00:47:34,290 --> 00:47:41,400
times their weight per capita so I would

883
00:47:39,150 --> 00:47:43,140
say innovation is is what their their

884
00:47:41,400 --> 00:47:46,250
hallmark is and they've also figured out

885
00:47:43,140 --> 00:47:49,920
how to commercialize that innovation

886
00:47:46,250 --> 00:47:53,070
mostly through support through the State

887
00:47:49,920 --> 00:47:56,670
of Israel through the country so they've

888
00:47:53,070 --> 00:47:59,400
got a great great way of doing that but

889
00:47:56,670 --> 00:48:01,290
particularly geez I mean they've got all

890
00:47:59,400 --> 00:48:04,490
kinds of cool technologies a lot of

891
00:48:01,290 --> 00:48:06,920
water a lot of water technologies and

892
00:48:04,490 --> 00:48:08,959
because their needs over there and

893
00:48:06,920 --> 00:48:11,430
definitely a lot of security

894
00:48:08,960 --> 00:48:16,530
applications social media applications

895
00:48:11,430 --> 00:48:18,960
they love our cellphones so if you ever

896
00:48:16,530 --> 00:48:20,760
go to Israel you don't want anything on

897
00:48:18,960 --> 00:48:23,760
your cell phone to be owned by somebody

898
00:48:20,760 --> 00:48:30,390
else take a burner so it's just a fact

899
00:48:23,760 --> 00:48:33,240
of life so yeah so and you know again I

900
00:48:30,390 --> 00:48:34,740
talked a lot about about China and the

901
00:48:33,240 --> 00:48:38,368
cultural differences in the way they

902
00:48:34,740 --> 00:48:40,950
view things there but you know our

903
00:48:38,369 --> 00:48:44,160
allies are you know

904
00:48:40,950 --> 00:48:46,609
I won't say there are allies do similar

905
00:48:44,160 --> 00:48:49,950
things and we all spy on each other so

906
00:48:46,610 --> 00:48:52,820
you know the largest spy ring ever

907
00:48:49,950 --> 00:48:58,410
busted in the United States was Israel

908
00:48:52,820 --> 00:49:02,450
so anyway any other questions come on

909
00:48:58,410 --> 00:49:04,859
it's me yes I think they 60

910
00:49:02,450 --> 00:49:08,609
it depends what your reference frame

911
00:49:04,860 --> 00:49:11,220
when you're talking about say data or

912
00:49:08,610 --> 00:49:15,300
ovoid or whatever nowadays it's

913
00:49:11,220 --> 00:49:17,370
basically the say the dollars and the

914
00:49:15,300 --> 00:49:20,130
markers and it used to be some people

915
00:49:17,370 --> 00:49:23,310
they worship in the Golden Calf now they

916
00:49:20,130 --> 00:49:25,740
are worshiping worshiping the bull

917
00:49:23,310 --> 00:49:28,110
market now what's going on right now

918
00:49:25,740 --> 00:49:31,109
some people they somebody wrote a book

919
00:49:28,110 --> 00:49:32,490
about the Black Swan and right now it

920
00:49:31,110 --> 00:49:35,250
seems there's a flock of Black Swan

921
00:49:32,490 --> 00:49:38,790
which are coming in and basically you

922
00:49:35,250 --> 00:49:41,070
don't in the regular media they don't

923
00:49:38,790 --> 00:49:42,660
talk about it even though the whole

924
00:49:41,070 --> 00:49:45,000
bunch of thing going on particular in

925
00:49:42,660 --> 00:49:46,529
China and those people that are on China

926
00:49:45,000 --> 00:49:48,780
know it's gonna affect everybody all

927
00:49:46,530 --> 00:49:50,610
around the world that the parts that

928
00:49:48,780 --> 00:49:54,180
were made in China now they're closing

929
00:49:50,610 --> 00:49:56,550
factors in South Korea because there is

930
00:49:54,180 --> 00:49:59,430
a mess there is no tourists from China

931
00:49:56,550 --> 00:50:02,160
try not to go over there so they have to

932
00:49:59,430 --> 00:50:04,950
put money into that hotels and

933
00:50:02,160 --> 00:50:07,470
restaurants and everything so everybody

934
00:50:04,950 --> 00:50:09,899
is just printing money and it seems

935
00:50:07,470 --> 00:50:12,000
that's the only solution that most of

936
00:50:09,900 --> 00:50:13,320
the people that they have it just basic

937
00:50:12,000 --> 00:50:15,500
printing money more money more money

938
00:50:13,320 --> 00:50:21,360
more money which is gonna be sacrificing

939
00:50:15,500 --> 00:50:23,400
many for just a few very few no as I

940
00:50:21,360 --> 00:50:25,290
said you're talking about the oil oil is

941
00:50:23,400 --> 00:50:27,350
some necessity you have to use it it

942
00:50:25,290 --> 00:50:30,120
doesn't matter who you are where your

943
00:50:27,350 --> 00:50:32,910
money nowadays and the data that you are

944
00:50:30,120 --> 00:50:35,190
talking basically those people were

945
00:50:32,910 --> 00:50:37,620
paying for it those who were in the

946
00:50:35,190 --> 00:50:40,410
market and that's the first narrative

947
00:50:37,620 --> 00:50:42,299
what basically that's what the fake news

948
00:50:40,410 --> 00:50:44,480
is just projecting for everyone this is

949
00:50:42,300 --> 00:50:47,700
the only thing that you have to have and

950
00:50:44,480 --> 00:50:50,190
now we are getting to a point that is

951
00:50:47,700 --> 00:50:52,020
gonna be some kind of a problem big

952
00:50:50,190 --> 00:50:54,630
problem for the whole world

953
00:50:52,020 --> 00:50:56,549
now nobody discusses it because

954
00:50:54,630 --> 00:50:59,400
or if you say this something's gonna

955
00:50:56,549 --> 00:51:00,569
happen it's gonna be bad and whatever so

956
00:50:59,400 --> 00:51:02,910
I don't know what's gonna be your

957
00:51:00,569 --> 00:51:06,779
perspective on that one that in this

958
00:51:02,910 --> 00:51:10,618
particular segment of time this is what

959
00:51:06,779 --> 00:51:13,200
people think is important but it is not

960
00:51:10,619 --> 00:51:16,049
and they used to be a golden rule that

961
00:51:13,200 --> 00:51:17,999
they say don't do to others what that

962
00:51:16,049 --> 00:51:19,979
you don't want to them to do to you or

963
00:51:17,999 --> 00:51:21,868
do to others what they want them to do

964
00:51:19,979 --> 00:51:23,759
to you now they have changed it to

965
00:51:21,869 --> 00:51:26,819
golden rule is whoever has the gold is

966
00:51:23,759 --> 00:51:28,680
gonna settle so I don't know what you

967
00:51:26,819 --> 00:51:31,140
think about that now that's a great

968
00:51:28,680 --> 00:51:35,368
point so you know Gandhi had I think

969
00:51:31,140 --> 00:51:38,400
seven rules of seven rules that he

970
00:51:35,369 --> 00:51:39,809
talked about and about rights and human

971
00:51:38,400 --> 00:51:43,289
rights and things like that but he made

972
00:51:39,809 --> 00:51:45,269
the argument that if you know if you

973
00:51:43,289 --> 00:51:49,140
have the right the individual has the

974
00:51:45,269 --> 00:51:53,160
right to to exist to life then you also

975
00:51:49,140 --> 00:51:55,618
have the obligation to to not take

976
00:51:53,160 --> 00:51:58,680
somebody else's life so he was talking

977
00:51:55,619 --> 00:52:02,009
about the rights and obligations so

978
00:51:58,680 --> 00:52:04,828
within the world today we talk a lot

979
00:52:02,009 --> 00:52:05,940
about what our rights are but we don't

980
00:52:04,829 --> 00:52:11,729
talk a lot about what our obligations

981
00:52:05,940 --> 00:52:14,729
are so you know the greatest point of

982
00:52:11,729 --> 00:52:16,640
prosperity in the United States and you

983
00:52:14,729 --> 00:52:20,308
can actually fact check this was a time

984
00:52:16,640 --> 00:52:23,578
when when we taxed the largest companies

985
00:52:20,309 --> 00:52:25,859
the most so you know countries are

986
00:52:23,579 --> 00:52:30,119
experimenting with things like a living

987
00:52:25,859 --> 00:52:31,589
wage you know as we displace workers in

988
00:52:30,119 --> 00:52:33,839
the workforce which is coming with

989
00:52:31,589 --> 00:52:36,599
autonomy it's it's happening now with

990
00:52:33,839 --> 00:52:38,099
robotics as we do those things we're

991
00:52:36,599 --> 00:52:40,979
going to have increasingly amount of

992
00:52:38,099 --> 00:52:43,019
people that are displaced and our

993
00:52:40,979 --> 00:52:47,578
standard of living will continue to rise

994
00:52:43,019 --> 00:52:50,519
for those that can afford it so you know

995
00:52:47,579 --> 00:52:55,049
so again this takes me back full circle

996
00:52:50,519 --> 00:52:56,700
to you know how do we how do how do we

997
00:52:55,049 --> 00:52:59,549
wish to define ourselves as we move

998
00:52:56,700 --> 00:53:03,210
forward forward now I'm not going to

999
00:52:59,549 --> 00:53:04,890
talk politics and and and candidates and

1000
00:53:03,210 --> 00:53:07,650
stuff out there but you know in the

1001
00:53:04,890 --> 00:53:11,120
richest country in the world right now

1002
00:53:07,650 --> 00:53:14,610
we don't have a healthcare system that

1003
00:53:11,120 --> 00:53:20,970
takes care of the most disenfranchised

1004
00:53:14,610 --> 00:53:22,650
people in in our our country so what are

1005
00:53:20,970 --> 00:53:24,990
we going to do even more when people

1006
00:53:22,650 --> 00:53:27,900
don't have jobs you know when they're

1007
00:53:24,990 --> 00:53:29,310
replaced by economies or by autonomy and

1008
00:53:27,900 --> 00:53:32,070
things like that and then you look at

1009
00:53:29,310 --> 00:53:34,080
the gathering of wealth up there you

1010
00:53:32,070 --> 00:53:37,140
know businesses and people are

1011
00:53:34,080 --> 00:53:39,480
collecting wealth right now they're not

1012
00:53:37,140 --> 00:53:42,060
reinvesting it they're holding on to it

1013
00:53:39,480 --> 00:53:43,950
so if you look at the richest people in

1014
00:53:42,060 --> 00:53:46,410
in the world today they're not

1015
00:53:43,950 --> 00:53:48,180
redistributing that wealth they're

1016
00:53:46,410 --> 00:53:51,450
holding on to it and that's a phenomenon

1017
00:53:48,180 --> 00:53:56,040
that's that's unique to to some extent

1018
00:53:51,450 --> 00:54:01,890
to this time period so you know getting

1019
00:53:56,040 --> 00:54:04,170
back to what you're saying you know you

1020
00:54:01,890 --> 00:54:05,640
know the Krays first of all i'll be the

1021
00:54:04,170 --> 00:54:09,510
first admit i'm not a huge fan of

1022
00:54:05,640 --> 00:54:12,029
Silicon Valley because the craze out

1023
00:54:09,510 --> 00:54:13,950
there is you know these companies with a

1024
00:54:12,030 --> 00:54:16,320
mission that feel good and what's

1025
00:54:13,950 --> 00:54:18,000
happening is when they capitalize when

1026
00:54:16,320 --> 00:54:20,640
they take in all that money they get to

1027
00:54:18,000 --> 00:54:22,710
decide who the money goes to they get to

1028
00:54:20,640 --> 00:54:25,529
decide what charities they're they're

1029
00:54:22,710 --> 00:54:27,510
supporting you know individual giving is

1030
00:54:25,530 --> 00:54:30,840
dropping off but yet we as workers

1031
00:54:27,510 --> 00:54:33,470
expect our companies to contribute and

1032
00:54:30,840 --> 00:54:37,530
expect our companies to have a social

1033
00:54:33,470 --> 00:54:40,080
social mandate yet the people you know

1034
00:54:37,530 --> 00:54:42,240
to a great extent it's like you said the

1035
00:54:40,080 --> 00:54:44,790
people who control the gold make the

1036
00:54:42,240 --> 00:54:46,770
rules so and it goes back to Nietzsche

1037
00:54:44,790 --> 00:54:49,279
right who said that the rich and the

1038
00:54:46,770 --> 00:54:53,750
powerful anything they do is moral so

1039
00:54:49,280 --> 00:54:56,700
again very very complex questions but

1040
00:54:53,750 --> 00:55:02,120
absolutely the only I am a bit of an

1041
00:54:56,700 --> 00:55:05,819
optimist in the fact that with the this

1042
00:55:02,120 --> 00:55:08,490
human connection without the boundaries

1043
00:55:05,820 --> 00:55:11,370
and the vestiges of bureaucracies of

1044
00:55:08,490 --> 00:55:13,439
governments and I'm not an anarchist but

1045
00:55:11,370 --> 00:55:15,750
with the ability to connect one-on-one

1046
00:55:13,440 --> 00:55:17,550
with people and experience that you used

1047
00:55:15,750 --> 00:55:20,970
to only get through traveling to a

1048
00:55:17,550 --> 00:55:23,700
different country that we can achieve a

1049
00:55:20,970 --> 00:55:25,980
a higher level of understanding in human

1050
00:55:23,700 --> 00:55:27,870
needs I was a soldier for a long time

1051
00:55:25,980 --> 00:55:29,460
and what I can tell you is I've been to

1052
00:55:27,870 --> 00:55:32,400
some of the worst places in the entire

1053
00:55:29,460 --> 00:55:35,760
world and when I know to be true is that

1054
00:55:32,400 --> 00:55:37,500
when I sat with a Bedouin the Bedouin

1055
00:55:35,760 --> 00:55:41,160
wanted the same things that I wanted at

1056
00:55:37,500 --> 00:55:44,220
a very very basic level they wanted they

1057
00:55:41,160 --> 00:55:47,910
wanted safety they wanted they wanted

1058
00:55:44,220 --> 00:55:49,020
the ability to have some opportunity to

1059
00:55:47,910 --> 00:55:52,200
feed their family

1060
00:55:49,020 --> 00:55:53,520
you know those basic Maslow being I

1061
00:55:52,200 --> 00:55:56,669
don't know if that's a word but I'm

1062
00:55:53,520 --> 00:55:58,860
pointing it now but those basic needs

1063
00:55:56,670 --> 00:56:01,140
human needs they wanted those awesome

1064
00:55:58,860 --> 00:56:02,880
now all the trappings that came on top

1065
00:56:01,140 --> 00:56:04,980
of it you know come with societal

1066
00:56:02,880 --> 00:56:07,140
controls you know like what tribe do you

1067
00:56:04,980 --> 00:56:09,570
belong to what country are you you know

1068
00:56:07,140 --> 00:56:12,830
what's your religion say or whatever so

1069
00:56:09,570 --> 00:56:17,640
so a bit of an optimist in that that

1070
00:56:12,830 --> 00:56:20,330
one-to-one human connection that that

1071
00:56:17,640 --> 00:56:23,640
this next generation will be much more

1072
00:56:20,330 --> 00:56:25,650
evolved in that awareness than than we

1073
00:56:23,640 --> 00:56:28,950
were and then we can shake off some of

1074
00:56:25,650 --> 00:56:32,160
the trappings of you know I can't talk

1075
00:56:28,950 --> 00:56:35,870
to you because you're a communist you

1076
00:56:32,160 --> 00:56:40,950
know so I mean that's just stupid

1077
00:56:35,870 --> 00:56:42,660
so yeah sure they said some people they

1078
00:56:40,950 --> 00:56:44,669
believe that basically because of all

1079
00:56:42,660 --> 00:56:48,440
these gathering there might be some kind

1080
00:56:44,670 --> 00:56:51,030
of a and trash or something that maybe

1081
00:56:48,440 --> 00:56:52,860
the dog was below ten thousand or

1082
00:56:51,030 --> 00:56:54,570
whatever but then it's gonna recover and

1083
00:56:52,860 --> 00:56:57,180
then it's gonna go up to maybe fifty

1084
00:56:54,570 --> 00:56:58,560
sixty thousand and they say by that time

1085
00:56:57,180 --> 00:57:01,710
because of the way that they're behaving

1086
00:56:58,560 --> 00:57:03,779
and creating money probably it might be

1087
00:57:01,710 --> 00:57:05,400
sixty seventy thousand but the loaf of

1088
00:57:03,780 --> 00:57:08,010
bread is gonna be basically two thousand

1089
00:57:05,400 --> 00:57:09,810
dollars so what do you think about that

1090
00:57:08,010 --> 00:57:12,120
what they are saying about this

1091
00:57:09,810 --> 00:57:13,830
I think our economies are too hopelessly

1092
00:57:12,120 --> 00:57:16,470
entwined to allow that to happen anymore

1093
00:57:13,830 --> 00:57:19,710
I mean we bail out our system now

1094
00:57:16,470 --> 00:57:21,450
instead of allowing it to fail so as we

1095
00:57:19,710 --> 00:57:23,220
Teeter towards these things we'll take

1096
00:57:21,450 --> 00:57:27,299
the most extreme measures that we can to

1097
00:57:23,220 --> 00:57:29,459
keep our economies and in line but so I

1098
00:57:27,300 --> 00:57:32,610
don't I don't see a I don't see an

1099
00:57:29,460 --> 00:57:34,650
apocalyptic crash coming or with the

1100
00:57:32,610 --> 00:57:37,569
with the economy with any

1101
00:57:34,650 --> 00:57:40,030
mmm with with any of this I mean that

1102
00:57:37,569 --> 00:57:42,430
term too big to fail the whole world is

1103
00:57:40,030 --> 00:57:44,230
that way now I mean we can't go to war

1104
00:57:42,430 --> 00:57:46,210
with China we can't go to war with

1105
00:57:44,230 --> 00:57:49,660
Russia because our economies are too

1106
00:57:46,210 --> 00:57:50,530
hopelessly intertwined so and and I

1107
00:57:49,660 --> 00:57:54,578
think that's a good thing

1108
00:57:50,530 --> 00:57:57,339
so because that's basically what you are

1109
00:57:54,579 --> 00:57:59,380
saying the bible's and the blessed are

1110
00:57:57,339 --> 00:58:01,450
the peacemakers but what they are saying

1111
00:57:59,380 --> 00:58:03,400
over here are blessed by their war

1112
00:58:01,450 --> 00:58:05,828
mongers because you know all of these

1113
00:58:03,400 --> 00:58:08,470
industries if involving war they're just

1114
00:58:05,829 --> 00:58:11,049
going on and everybody believes in them

1115
00:58:08,470 --> 00:58:12,910
rather than believing in peacemaking so

1116
00:58:11,049 --> 00:58:16,150
as I said you might be an optimist I'm

1117
00:58:12,910 --> 00:58:18,098
also an optimist in general but the way

1118
00:58:16,150 --> 00:58:18,490
I look at these what's going on around

1119
00:58:18,099 --> 00:58:20,710
me

1120
00:58:18,490 --> 00:58:22,899
I just don't listen to the regular

1121
00:58:20,710 --> 00:58:24,940
numerous brainwashing you go a little

1122
00:58:22,900 --> 00:58:27,760
bit behind the news because when you

1123
00:58:24,940 --> 00:58:30,520
look at China they say 500 people 1,000

1124
00:58:27,760 --> 00:58:34,510
PPR 2,000 people have died and there may

1125
00:58:30,520 --> 00:58:36,819
be 75,000 or infected general you look

1126
00:58:34,510 --> 00:58:39,309
at it you don't close the industrial hub

1127
00:58:36,819 --> 00:58:41,109
of the world for maybe 2,000 people

1128
00:58:39,309 --> 00:58:42,880
75,000 is something like that so

1129
00:58:41,109 --> 00:58:44,950
something doesn't add up so that's what

1130
00:58:42,880 --> 00:58:46,059
I'm saying what I agree with you there

1131
00:58:44,950 --> 00:58:48,759
are a lot of things that you talked

1132
00:58:46,059 --> 00:58:51,460
about and they are basically major point

1133
00:58:48,760 --> 00:58:53,680
that everybody has to discuss with their

1134
00:58:51,460 --> 00:58:55,750
people around them and then decide what

1135
00:58:53,680 --> 00:58:59,470
they want to do yeah I'm kind of

1136
00:58:55,750 --> 00:59:01,119
encouraged I mean this this whole I

1137
00:58:59,470 --> 00:59:07,140
forget what the name of the virus is

1138
00:59:01,119 --> 00:59:09,430
that's that a corona virus so yeah well

1139
00:59:07,140 --> 00:59:11,140
somebody I heard somebody refer to it is

1140
00:59:09,430 --> 00:59:15,368
the kind of flu the other day I thought

1141
00:59:11,140 --> 00:59:16,779
it was rather interesting but I'm

1142
00:59:15,369 --> 00:59:20,829
actually kind of encouraged by it

1143
00:59:16,780 --> 00:59:22,210
because you know seeing the world

1144
00:59:20,829 --> 00:59:24,819
respond as something that's poached

1145
00:59:22,210 --> 00:59:28,299
potentially a pandemic you know and

1146
00:59:24,819 --> 00:59:31,150
respond in a coordinated fashion I think

1147
00:59:28,299 --> 00:59:35,230
is is encouraging and again as we become

1148
00:59:31,150 --> 00:59:37,630
more intertwined I think that we're

1149
00:59:35,230 --> 00:59:40,150
gonna we will see more of that when we

1150
00:59:37,630 --> 00:59:42,760
do see more of it now so we see world

1151
00:59:40,150 --> 00:59:47,619
reactions to two things that affect us

1152
00:59:42,760 --> 00:59:52,989
all so we can't afford not to so

1153
00:59:47,619 --> 00:59:55,809
did you have a question earlier so my

1154
00:59:52,989 --> 00:59:59,829
question is that so I completely agree

1155
00:59:55,809 --> 01:00:02,619
with the way we you you spoke about how

1156
00:59:59,829 --> 01:00:05,469
the corporates especially need to have

1157
01:00:02,619 --> 01:00:07,229
certain rules and ethics on how they

1158
01:00:05,469 --> 01:00:11,259
utilize our data which is available

1159
01:00:07,229 --> 01:00:14,968
publicly and I believe the acts like the

1160
01:00:11,259 --> 01:00:18,660
GDP are and the CCPA or maybe one of the

1161
01:00:14,969 --> 01:00:22,150
the forefront acts in allowing us to

1162
01:00:18,660 --> 01:00:23,589
properly set rules and guidelines on how

1163
01:00:22,150 --> 01:00:26,229
data has been utilized but I also

1164
01:00:23,589 --> 01:00:28,689
believe in the the sovereignty and the

1165
01:00:26,229 --> 01:00:31,058
freedom of data in the Internet and so

1166
01:00:28,689 --> 01:00:32,828
people always have a choice like you

1167
01:00:31,059 --> 01:00:34,689
said you spoke about we always we never

1168
01:00:32,829 --> 01:00:37,749
read the Terms and agreements where you

1169
01:00:34,689 --> 01:00:41,249
just checkoff on it and so what I'm

1170
01:00:37,749 --> 01:00:44,618
trying say is like shouldn't we try to

1171
01:00:41,249 --> 01:00:48,459
in this digital age shouldn't we try to

1172
01:00:44,619 --> 01:00:51,099
incorporate knowledge and how we should

1173
01:00:48,459 --> 01:00:53,529
use data and how we should be careful

1174
01:00:51,099 --> 01:00:56,289
with social media with with the Internet

1175
01:00:53,529 --> 01:00:58,630
as a whole into our education system and

1176
01:00:56,289 --> 01:01:01,209
tell people make sure people understand

1177
01:00:58,630 --> 01:01:03,519
the implications of things which go

1178
01:01:01,209 --> 01:01:07,538
wrong you're preaching you're preaching

1179
01:01:03,519 --> 01:01:11,288
brother I mean I I love that and you

1180
01:01:07,539 --> 01:01:15,130
know I always use the analogy back in

1181
01:01:11,289 --> 01:01:16,959
the back in the old days on on after

1182
01:01:15,130 --> 01:01:18,039
school and on Saturday mornings you know

1183
01:01:16,959 --> 01:01:20,259
we had a little public service

1184
01:01:18,039 --> 01:01:22,179
announcement announcement PSAs and they

1185
01:01:20,259 --> 01:01:23,890
still had some out there now but they

1186
01:01:22,179 --> 01:01:26,079
had a guy named Freddie kilowatt who was

1187
01:01:23,890 --> 01:01:28,029
like a little lightning bolt and I

1188
01:01:26,079 --> 01:01:29,380
learned from that cartoon not to stick

1189
01:01:28,029 --> 01:01:33,640
my finger in a light socket

1190
01:01:29,380 --> 01:01:35,589
you know why we don't do that with the

1191
01:01:33,640 --> 01:01:38,709
world that we live in now and where the

1192
01:01:35,589 --> 01:01:40,959
dangers really exist which is on the

1193
01:01:38,709 --> 01:01:42,549
internet you know why we don't have

1194
01:01:40,959 --> 01:01:46,269
those same type that same type of

1195
01:01:42,549 --> 01:01:51,249
education why digital citizenship is not

1196
01:01:46,269 --> 01:01:53,258
required in K through 12 is beyond me I

1197
01:01:51,249 --> 01:01:55,058
mean I fight that fight every day here

1198
01:01:53,259 --> 01:01:58,089
at the state of Indiana to try and

1199
01:01:55,059 --> 01:02:00,190
convince you know DoD and our DOA and

1200
01:01:58,089 --> 01:02:03,029
DWD and those guys to

1201
01:02:00,190 --> 01:02:05,410
to consider things like that so

1202
01:02:03,030 --> 01:02:06,819
absolutely I mean digital citizenship

1203
01:02:05,410 --> 01:02:08,740
there's there's countries doing great

1204
01:02:06,819 --> 01:02:11,170
things like Estonia is killing it right

1205
01:02:08,740 --> 01:02:12,368
now just in digital citizenship you know

1206
01:02:11,170 --> 01:02:13,960
where they're starting to really

1207
01:02:12,369 --> 01:02:19,270
understand what the responsibilities

1208
01:02:13,960 --> 01:02:25,800
aren't for that so but yeah so I'm with

1209
01:02:19,270 --> 01:02:31,329
you so any other questions all right

1210
01:02:25,800 --> 01:02:31,329
[Applause]

1211
01:02:38,160 --> 01:02:40,220
you

