1
00:00:20,610 --> 00:00:26,320
sorry for the delay welcome to the

2
00:00:23,350 --> 00:00:29,260
serious security seminar today it's my

3
00:00:26,320 --> 00:00:31,210
pleasure to introduce cushion pi cushion

4
00:00:29,260 --> 00:00:32,880
is a graduate student in the computer

5
00:00:31,210 --> 00:00:35,379
science department at Purdue University

6
00:00:32,880 --> 00:00:37,570
his research interests include data

7
00:00:35,379 --> 00:00:39,790
mining and security especially on

8
00:00:37,570 --> 00:00:41,710
solving security problems using both

9
00:00:39,790 --> 00:00:48,400
program analysis and machine learning

10
00:00:41,710 --> 00:00:50,860
techniques my name is coach in pay thank

11
00:00:48,400 --> 00:00:53,769
you for attending this talk today I will

12
00:00:50,860 --> 00:00:56,500
present our paper lips detecting

13
00:00:53,770 --> 00:01:00,219
camouflaged attacks with statistical

14
00:00:56,500 --> 00:01:02,879
learning guided by program analysis this

15
00:01:00,219 --> 00:01:05,770
is the joint work with John shoe goo

16
00:01:02,879 --> 00:01:07,990
chief and one professor lawsuit

17
00:01:05,770 --> 00:01:14,550
professor Zhang region and professor

18
00:01:07,990 --> 00:01:17,259
Suetonius ooh so today enterprise

19
00:01:14,550 --> 00:01:20,130
infrastructures are facing more severe

20
00:01:17,259 --> 00:01:23,679
cyber threats part by sophisticated

21
00:01:20,130 --> 00:01:28,119
attack techniques usually the cyber

22
00:01:23,679 --> 00:01:30,940
attack can be divided into two stages in

23
00:01:28,119 --> 00:01:33,670
the first stage the malicious adversary

24
00:01:30,940 --> 00:01:36,520
will try to intrude in the enterprise

25
00:01:33,670 --> 00:01:38,920
network they can do this either by

26
00:01:36,520 --> 00:01:41,590
exploiting system vulnerabilities or

27
00:01:38,920 --> 00:01:44,229
leveraging some social engineering

28
00:01:41,590 --> 00:01:47,399
techniques to treat the innocent users

29
00:01:44,229 --> 00:01:54,399
to perform harmful operations on

30
00:01:47,399 --> 00:01:56,890
intentionally then they will then they

31
00:01:54,399 --> 00:01:59,830
will implant their persistent malicious

32
00:01:56,890 --> 00:02:03,849
backdoors paler and payloads in do some

33
00:01:59,830 --> 00:02:06,640
victim machines in the second stage the

34
00:02:03,849 --> 00:02:09,580
hacker will infiltrate into the network

35
00:02:06,640 --> 00:02:12,430
again and connect to the back doors that

36
00:02:09,580 --> 00:02:14,709
they implant in the first stage then

37
00:02:12,430 --> 00:02:17,140
they could send the command and control

38
00:02:14,709 --> 00:02:19,480
to the malicious payload and control the

39
00:02:17,140 --> 00:02:22,238
victim assistance to achieve their goals

40
00:02:19,480 --> 00:02:25,420
such as stealing sensitive information

41
00:02:22,239 --> 00:02:28,120
from the machine in fact other systems

42
00:02:25,420 --> 00:02:31,689
or make the compromised machine one of

43
00:02:28,120 --> 00:02:33,340
the attackers BOTS to conduct DDoS

44
00:02:31,689 --> 00:02:39,760
distributed in

45
00:02:33,340 --> 00:02:42,160
of service attack malicious adversary

46
00:02:39,760 --> 00:02:44,379
always strive to make their attacks to

47
00:02:42,160 --> 00:02:46,480
be more stealthy and leave fewer

48
00:02:44,379 --> 00:02:50,290
footprints in the system that they

49
00:02:46,480 --> 00:02:52,720
attack typically they try to implant the

50
00:02:50,290 --> 00:02:56,560
malicious payload under the cover of

51
00:02:52,720 --> 00:02:58,720
some benign or mobile applications in

52
00:02:56,560 --> 00:03:01,920
order to evade the detection of sistan

53
00:02:58,720 --> 00:03:06,430
users we named such attacks as

54
00:03:01,920 --> 00:03:10,660
camouflaged attacks camouflaged attacks

55
00:03:06,430 --> 00:03:14,079
can be divided into two types the first

56
00:03:10,660 --> 00:03:17,049
type is the offline attack adversary in

57
00:03:14,079 --> 00:03:19,959
bed malicious logic within the binary of

58
00:03:17,049 --> 00:03:22,540
a legitimate application and can use

59
00:03:19,959 --> 00:03:26,260
some vision technique to trick the user

60
00:03:22,540 --> 00:03:29,620
to download and install it the malicious

61
00:03:26,260 --> 00:03:37,230
logic can run in parallel with benign

62
00:03:29,620 --> 00:03:40,569
logic the second type is online attack

63
00:03:37,230 --> 00:03:43,119
adversary can choose one running process

64
00:03:40,569 --> 00:03:45,548
that is vulnerable and inject the

65
00:03:43,120 --> 00:03:49,120
malicious logic directly into the

66
00:03:45,549 --> 00:03:51,910
applications memory space then they

67
00:03:49,120 --> 00:03:54,609
redirect some program path within the

68
00:03:51,910 --> 00:03:59,250
benign logic and execute the malicious

69
00:03:54,609 --> 00:03:59,250
payload within the same process context

70
00:04:00,450 --> 00:04:06,220
actually today camouflage attacks post

71
00:04:03,790 --> 00:04:08,858
some difficulties for traditional attack

72
00:04:06,220 --> 00:04:11,290
detection approaches because the

73
00:04:08,859 --> 00:04:14,680
recorded malicious and benign behaviors

74
00:04:11,290 --> 00:04:17,260
are mixed together such noise in the

75
00:04:14,680 --> 00:04:19,810
data set may greatly affect the accuracy

76
00:04:17,260 --> 00:04:22,780
of distinguishing malicious behavior

77
00:04:19,810 --> 00:04:29,680
from benign behavior for future

78
00:04:22,780 --> 00:04:32,260
detection generally attack detection two

79
00:04:29,680 --> 00:04:35,860
approaches in the literature can be

80
00:04:32,260 --> 00:04:38,469
classified into two categories program

81
00:04:35,860 --> 00:04:43,169
analysis based and statistical learning

82
00:04:38,470 --> 00:04:46,420
based both have their pros and cons for

83
00:04:43,169 --> 00:04:48,580
program analysis approaches

84
00:04:46,420 --> 00:04:51,790
it can generate a precise execution

85
00:04:48,580 --> 00:04:53,890
model but this is usually built upon

86
00:04:51,790 --> 00:04:57,430
heavyweight instrumentation on the

87
00:04:53,890 --> 00:05:00,520
program furthermore if the training data

88
00:04:57,430 --> 00:05:07,090
is not complete it may perform bad on

89
00:05:00,520 --> 00:05:09,580
the unseen testing data for statistical

90
00:05:07,090 --> 00:05:12,130
learning the advantage is that it is

91
00:05:09,580 --> 00:05:15,370
robust on dealing with unseen testing

92
00:05:12,130 --> 00:05:17,830
data but in order to generate this such

93
00:05:15,370 --> 00:05:19,780
model it needs domain knowledge to

94
00:05:17,830 --> 00:05:21,990
process the raw data to make it

95
00:05:19,780 --> 00:05:24,969
understandable by learning approach

96
00:05:21,990 --> 00:05:29,260
which is non-trivial in most of the

97
00:05:24,970 --> 00:05:31,150
learning process furthermore the

98
00:05:29,260 --> 00:05:35,950
performance of statistical learning

99
00:05:31,150 --> 00:05:38,340
approach is closely related to to the

100
00:05:35,950 --> 00:05:41,260
quality of the training data for example

101
00:05:38,340 --> 00:05:44,169
when facing the noisy they has that

102
00:05:41,260 --> 00:05:49,599
generated by the camouflaged attacks it

103
00:05:44,170 --> 00:05:52,240
may derive an inaccurate model let's

104
00:05:49,600 --> 00:05:56,580
explain what is exact the problem that

105
00:05:52,240 --> 00:05:59,950
will be introduced by the noisy data set

106
00:05:56,580 --> 00:06:02,500
in the first stage we could obtain the

107
00:05:59,950 --> 00:06:06,909
pure benign data set to provide positive

108
00:06:02,500 --> 00:06:10,990
training samples and then we could also

109
00:06:06,910 --> 00:06:13,710
obtain the data set contains camouflaged

110
00:06:10,990 --> 00:06:16,720
attacks as the negative training samples

111
00:06:13,710 --> 00:06:19,510
but because such negative training data

112
00:06:16,720 --> 00:06:23,860
contains both benign and malicious

113
00:06:19,510 --> 00:06:26,770
events in it which is denoted in the in

114
00:06:23,860 --> 00:06:28,930
that in the picture that positive sign

115
00:06:26,770 --> 00:06:31,240
is for the benign datas mo and the

116
00:06:28,930 --> 00:06:34,810
negative science for the malicious they

117
00:06:31,240 --> 00:06:37,660
resemble so finally we will get a

118
00:06:34,810 --> 00:06:41,230
misleading decision boundary that biased

119
00:06:37,660 --> 00:06:44,230
towards the positive side this decision

120
00:06:41,230 --> 00:06:47,440
boundary misclassify benign testing data

121
00:06:44,230 --> 00:06:50,220
as malicious in the future the intuitive

122
00:06:47,440 --> 00:06:54,040
solution to solve this problem is

123
00:06:50,220 --> 00:06:56,320
minimize the influence of benign data

124
00:06:54,040 --> 00:06:58,990
points within the negative data sets

125
00:06:56,320 --> 00:07:01,710
thus we can derive a correct decision

126
00:06:58,990 --> 00:07:01,710
boundary

127
00:07:03,370 --> 00:07:13,880
22 actually accurately classify the

128
00:07:07,730 --> 00:07:17,300
testing data in order to address the

129
00:07:13,880 --> 00:07:20,480
noisy data set problem we develop a new

130
00:07:17,300 --> 00:07:23,180
statistical learning by based attack

131
00:07:20,480 --> 00:07:25,430
detection system called lips which

132
00:07:23,180 --> 00:07:29,060
stands for learning enhanced with

133
00:07:25,430 --> 00:07:33,530
analysis of program support lips is

134
00:07:29,060 --> 00:07:36,950
inspired by a recent post vision of 92

135
00:07:33,530 --> 00:07:39,469
reason which promotes mutual enhancement

136
00:07:36,950 --> 00:07:45,770
between statistical learning and formal

137
00:07:39,470 --> 00:07:48,500
analysis reasoning lips integrate the

138
00:07:45,770 --> 00:07:51,260
capabilities from both program analysis

139
00:07:48,500 --> 00:07:53,630
and statistical learning camps to be

140
00:07:51,260 --> 00:07:56,270
more specific we leverage program

141
00:07:53,630 --> 00:07:58,640
analysis technique to refine the

142
00:07:56,270 --> 00:08:01,280
statistical learning model and finally

143
00:07:58,640 --> 00:08:03,860
we can boost the detection accuracy of

144
00:08:01,280 --> 00:08:09,289
learning approach for such camouflaged

145
00:08:03,860 --> 00:08:12,350
attacks through our investigation on the

146
00:08:09,290 --> 00:08:14,420
behavior of camouflaged attacks we find

147
00:08:12,350 --> 00:08:16,940
that usually benign and malicious

148
00:08:14,420 --> 00:08:19,550
program clusters separately in their

149
00:08:16,940 --> 00:08:24,910
code regions that are mapped in the

150
00:08:19,550 --> 00:08:24,910
memory we give a concrete example here

151
00:08:25,810 --> 00:08:32,479
the above one is control flow graph of a

152
00:08:29,600 --> 00:08:34,669
benign being a leader and the below one

153
00:08:32,479 --> 00:08:38,900
is control flow graph of a being editor

154
00:08:34,669 --> 00:08:41,360
that contains a malicious backdoor after

155
00:08:38,900 --> 00:08:44,000
comparing these two control flow graphs

156
00:08:41,360 --> 00:08:47,360
by Annoying the load with same address

157
00:08:44,000 --> 00:08:49,910
we find that benign sub graphs of these

158
00:08:47,360 --> 00:08:53,870
two control graphs are very similar to

159
00:08:49,910 --> 00:08:56,150
each other on the contrary the malicious

160
00:08:53,870 --> 00:08:58,960
sub graph in the second control growl

161
00:08:56,150 --> 00:09:03,110
flow graph is quite independent and

162
00:08:58,960 --> 00:09:06,020
isolated from other program paths the

163
00:09:03,110 --> 00:09:09,110
root cause is that parasitic malicious

164
00:09:06,020 --> 00:09:11,990
functionality is by nature not related

165
00:09:09,110 --> 00:09:14,040
to his hosts applications functionality

166
00:09:11,990 --> 00:09:16,889
thus we think that

167
00:09:14,040 --> 00:09:19,380
the code distance to the benign sub

168
00:09:16,889 --> 00:09:24,810
graph is a good measurement for the

169
00:09:19,380 --> 00:09:27,899
benignity of the code then we'll talk

170
00:09:24,810 --> 00:09:30,899
about the key idea of leaps we use the

171
00:09:27,899 --> 00:09:33,149
system event log and transform it into

172
00:09:30,899 --> 00:09:38,190
the training data set that is

173
00:09:33,149 --> 00:09:40,350
understandable by an early model we also

174
00:09:38,190 --> 00:09:42,680
derive the control flow graph

175
00:09:40,350 --> 00:09:45,930
information from the system event log

176
00:09:42,680 --> 00:09:49,589
one only has benign events and the other

177
00:09:45,930 --> 00:09:52,739
one contains camouflaged attack by

178
00:09:49,589 --> 00:09:54,870
comparing this to control flow graphs we

179
00:09:52,740 --> 00:09:58,380
assign corresponding weight for each

180
00:09:54,870 --> 00:10:01,050
data point in the negative data set this

181
00:09:58,380 --> 00:10:04,670
weight is based on the distance of code

182
00:10:01,050 --> 00:10:08,219
address to the benign control flow graph

183
00:10:04,670 --> 00:10:11,459
so with such weights in the data set we

184
00:10:08,220 --> 00:10:16,769
train a weighted SVM model for

185
00:10:11,459 --> 00:10:19,768
classification on future data so this is

186
00:10:16,769 --> 00:10:22,440
the overview of the system design in the

187
00:10:19,769 --> 00:10:26,370
system of leaps we have four major

188
00:10:22,440 --> 00:10:29,399
modules the stack petition module data

189
00:10:26,370 --> 00:10:31,740
pre-processing module control flow graph

190
00:10:29,399 --> 00:10:36,269
in feirense module and supervised

191
00:10:31,740 --> 00:10:42,240
learning module the input is the system

192
00:10:36,269 --> 00:10:44,010
event log it contains the recorded

193
00:10:42,240 --> 00:10:48,630
system events and their corresponding

194
00:10:44,010 --> 00:10:52,800
stack work trace through the stack

195
00:10:48,630 --> 00:10:57,480
Perdition module we partition each stack

196
00:10:52,800 --> 00:11:01,490
work trace into two parts the system

197
00:10:57,480 --> 00:11:04,230
stack trace and application stack trace

198
00:11:01,490 --> 00:11:07,740
the system stack traces the stack trace

199
00:11:04,230 --> 00:11:10,800
the system layer contains system level

200
00:11:07,740 --> 00:11:12,509
behavior of the program for example the

201
00:11:10,800 --> 00:11:16,050
shared library and the colonel

202
00:11:12,509 --> 00:11:20,279
information we use such system level

203
00:11:16,050 --> 00:11:22,290
behavior as our training and testing to

204
00:11:20,279 --> 00:11:26,410
distinguish benign and malicious

205
00:11:22,290 --> 00:11:30,009
behavior the application stack trace

206
00:11:26,410 --> 00:11:33,819
is the stacktrace within the application

207
00:11:30,009 --> 00:11:36,339
layer the application level program

208
00:11:33,819 --> 00:11:43,149
execution information is embedded inside

209
00:11:36,339 --> 00:11:46,449
the application stack trees through data

210
00:11:43,149 --> 00:11:48,939
pre-processing module we transform the

211
00:11:46,449 --> 00:11:51,670
log into the data set that can be used

212
00:11:48,939 --> 00:11:54,699
by the supervised learning module as we

213
00:11:51,670 --> 00:11:57,250
mentioned the negative data set is noisy

214
00:11:54,699 --> 00:12:02,378
because it contains both benign and

215
00:11:57,250 --> 00:12:04,750
malicious data so through the control

216
00:12:02,379 --> 00:12:06,459
flow graph inference module we derive

217
00:12:04,750 --> 00:12:08,829
the control flow graph from the

218
00:12:06,459 --> 00:12:10,899
application stack trace and assess the

219
00:12:08,829 --> 00:12:16,239
weight by comparing the control flow

220
00:12:10,899 --> 00:12:18,220
graph finally by assigning the weight to

221
00:12:16,240 --> 00:12:23,800
the training data set we build a

222
00:12:18,220 --> 00:12:28,500
weighted svn model this is the overview

223
00:12:23,800 --> 00:12:31,389
and the workflow of our system and then

224
00:12:28,500 --> 00:12:33,160
after discussing the overview let's

225
00:12:31,389 --> 00:12:36,339
inspect the details for the

226
00:12:33,160 --> 00:12:38,589
functionality of each module data

227
00:12:36,339 --> 00:12:41,769
pre-processing is the essential step

228
00:12:38,589 --> 00:12:44,649
before applying any learning model it

229
00:12:41,769 --> 00:12:46,899
requires the domain knowledge to extract

230
00:12:44,649 --> 00:12:50,079
the features from the raw data and

231
00:12:46,899 --> 00:12:53,110
discretize the data to make it adaptable

232
00:12:50,079 --> 00:12:56,979
to the nerdy model we instruct three

233
00:12:53,110 --> 00:13:01,779
features for each event the event type

234
00:12:56,980 --> 00:13:04,540
the library set and the function set for

235
00:13:01,779 --> 00:13:07,569
each event it has one to one mapping to

236
00:13:04,540 --> 00:13:09,939
the invent tag type but for each library

237
00:13:07,569 --> 00:13:12,279
set and functions that there are a

238
00:13:09,939 --> 00:13:15,490
sequence of library call and function

239
00:13:12,279 --> 00:13:18,339
call in the call stack so we need to

240
00:13:15,490 --> 00:13:20,589
further abstract this cause that

241
00:13:18,339 --> 00:13:25,779
sequence to a single value for each

242
00:13:20,589 --> 00:13:28,720
event which can be transformed as the

243
00:13:25,779 --> 00:13:31,180
input to the learning model hence we

244
00:13:28,720 --> 00:13:33,100
leverage the hierarchical clustering

245
00:13:31,180 --> 00:13:36,160
which is an unsupervised learning

246
00:13:33,100 --> 00:13:38,259
algorithm to group similar library

247
00:13:36,160 --> 00:13:40,120
function sets into one cluster and

248
00:13:38,259 --> 00:13:43,210
assign that cluster

249
00:13:40,120 --> 00:13:45,400
to the Christ corresponding library

250
00:13:43,210 --> 00:13:48,630
cluster number and function class the

251
00:13:45,400 --> 00:13:51,250
number we use the pairwise set

252
00:13:48,630 --> 00:13:54,070
dissimilarity as the measurements for

253
00:13:51,250 --> 00:14:00,040
distance and then apply the hierarchical

254
00:13:54,070 --> 00:14:02,290
clustering then we'll talk a little

255
00:14:00,040 --> 00:14:04,870
about the control flow graph inference

256
00:14:02,290 --> 00:14:07,569
module as we know traditionally people

257
00:14:04,870 --> 00:14:10,180
use system event log for performance

258
00:14:07,570 --> 00:14:13,980
debugging at the same time program

259
00:14:10,180 --> 00:14:17,560
execution information is also embedded

260
00:14:13,980 --> 00:14:20,140
implicitly within the log so the

261
00:14:17,560 --> 00:14:22,449
functionality of this module is to infer

262
00:14:20,140 --> 00:14:25,810
the control flow graph from the system

263
00:14:22,450 --> 00:14:29,140
event log in the previous slides as I

264
00:14:25,810 --> 00:14:31,029
mentioned we leverage the system stack

265
00:14:29,140 --> 00:14:33,670
trace to generate the data set for

266
00:14:31,029 --> 00:14:36,700
training and the testing here we mainly

267
00:14:33,670 --> 00:14:39,790
leverage the application stack trace

268
00:14:36,700 --> 00:14:42,760
attached to each event to invert the

269
00:14:39,790 --> 00:14:45,520
control flow graph as the event is

270
00:14:42,760 --> 00:14:48,310
predefined when triggered logging the

271
00:14:45,520 --> 00:14:51,520
call stack trace may not be complete

272
00:14:48,310 --> 00:14:53,859
which may not include each instructions

273
00:14:51,520 --> 00:14:56,800
and the basic block for the control flow

274
00:14:53,860 --> 00:14:58,990
graph but it is fine to reflect the

275
00:14:56,800 --> 00:15:04,089
general execution structure of the

276
00:14:58,990 --> 00:15:07,180
program here we give an example for

277
00:15:04,089 --> 00:15:10,209
control flow graph inference assume we

278
00:15:07,180 --> 00:15:13,510
have two adjacent events event one and

279
00:15:10,209 --> 00:15:16,029
event two in the log and both of them

280
00:15:13,510 --> 00:15:19,779
has their own application stack trace

281
00:15:16,029 --> 00:15:22,200
the first program path we can infer from

282
00:15:19,779 --> 00:15:25,330
the lock is the function invocation

283
00:15:22,200 --> 00:15:31,779
relation embedded on the stack we call

284
00:15:25,330 --> 00:15:34,660
it explicit path and then we also find

285
00:15:31,779 --> 00:15:37,360
that the first three addresses and on

286
00:15:34,660 --> 00:15:40,329
the stack are the same and they branch

287
00:15:37,360 --> 00:15:42,640
at the fourth address then we can infer

288
00:15:40,330 --> 00:15:46,120
that there exists some implicit path

289
00:15:42,640 --> 00:15:48,390
from address for to address six in this

290
00:15:46,120 --> 00:15:48,390
case

291
00:15:49,150 --> 00:15:53,920
so based on this information we derive

292
00:15:52,120 --> 00:15:56,590
the control flow graph from the system

293
00:15:53,920 --> 00:16:01,060
event log which is incomplete as I

294
00:15:56,590 --> 00:16:03,700
mentioned we derive a benign control

295
00:16:01,060 --> 00:16:06,640
flow graph from the benign log which is

296
00:16:03,700 --> 00:16:09,400
used for weight aside assignment for

297
00:16:06,640 --> 00:16:12,010
positive training data and derive a

298
00:16:09,400 --> 00:16:15,189
mixed control flow graph from the noisy

299
00:16:12,010 --> 00:16:20,050
mixed log which is used for weight

300
00:16:15,190 --> 00:16:22,720
assignment for negative training data we

301
00:16:20,050 --> 00:16:27,069
assign the wait for the negative data

302
00:16:22,720 --> 00:16:29,350
set based on the two criteria the first

303
00:16:27,070 --> 00:16:32,200
one is the program path that is close to

304
00:16:29,350 --> 00:16:36,250
benign control flow graph is more likely

305
00:16:32,200 --> 00:16:38,890
to be benign and the second one is

306
00:16:36,250 --> 00:16:41,110
program path that is far away from the

307
00:16:38,890 --> 00:16:43,449
benign control flow graph is more likely

308
00:16:41,110 --> 00:16:48,820
belonging to the implanted malicious

309
00:16:43,450 --> 00:16:52,330
payload we compare this to control flow

310
00:16:48,820 --> 00:16:57,030
graphs edge by edge for the edges that

311
00:16:52,330 --> 00:17:04,030
appear in the benign control flow graph

312
00:16:57,030 --> 00:17:06,819
we assign one to each wait for the edges

313
00:17:04,030 --> 00:17:09,520
that do not appear in the benign control

314
00:17:06,819 --> 00:17:12,250
flow graph but close to the benign nodes

315
00:17:09,520 --> 00:17:20,770
we assess their weight based on their

316
00:17:12,250 --> 00:17:23,440
distance to the adjacent nodes for edges

317
00:17:20,770 --> 00:17:28,889
that are totally far away from the

318
00:17:23,440 --> 00:17:28,890
benign nodes we assigned 0 as its weight

319
00:17:28,950 --> 00:17:35,800
with weight for each edges we calculate

320
00:17:33,100 --> 00:17:38,199
the weight for each corresponding event

321
00:17:35,800 --> 00:17:41,159
and assign a weight to the negative

322
00:17:38,200 --> 00:17:41,160
training data

323
00:17:44,120 --> 00:17:49,459
so the final step is to learn an

324
00:17:46,700 --> 00:17:52,250
accurate binary classifier from the

325
00:17:49,460 --> 00:17:55,730
training data we use the support vector

326
00:17:52,250 --> 00:18:00,050
machine as our classification model this

327
00:17:55,730 --> 00:18:04,670
is the formulation of the original svn

328
00:18:00,050 --> 00:18:07,220
with soft margin we revised the formula

329
00:18:04,670 --> 00:18:10,700
and introduced the weight of each data

330
00:18:07,220 --> 00:18:15,970
point in the training data into the

331
00:18:10,700 --> 00:18:15,970
formula and build a weighted svn model

332
00:18:17,110 --> 00:18:22,639
then let me talk about the

333
00:18:19,160 --> 00:18:26,240
implementation of our system for the

334
00:18:22,640 --> 00:18:29,390
system event logging we use the ET w

335
00:18:26,240 --> 00:18:32,330
which is stands for windows are event

336
00:18:29,390 --> 00:18:34,130
tracing for windows framework it's a

337
00:18:32,330 --> 00:18:37,250
general-purpose tracing engine

338
00:18:34,130 --> 00:18:40,610
introduced in Windows 2000 it can trace

339
00:18:37,250 --> 00:18:44,590
the system event a cross-layer from user

340
00:18:40,610 --> 00:18:48,199
space applications to Colonel components

341
00:18:44,590 --> 00:18:50,330
in our stack trace partition module they

342
00:18:48,200 --> 00:18:52,460
revert to the application stack trace

343
00:18:50,330 --> 00:18:55,340
which is for control flow graph in

344
00:18:52,460 --> 00:18:58,670
feirense and system stack trace for

345
00:18:55,340 --> 00:19:01,370
training and testing it can also enable

346
00:18:58,670 --> 00:19:05,390
stack working on each predefined

347
00:19:01,370 --> 00:19:08,570
assisting events for the statistical

348
00:19:05,390 --> 00:19:12,320
learning part we leverage the syfy for

349
00:19:08,570 --> 00:19:14,720
hierarchical clustering and leap svn to

350
00:19:12,320 --> 00:19:21,379
build our weighted supporter support

351
00:19:14,720 --> 00:19:25,340
vector machine we test our experiment on

352
00:19:21,380 --> 00:19:27,590
21 camouflaged attack samples there are

353
00:19:25,340 --> 00:19:30,530
the combination of five different host

354
00:19:27,590 --> 00:19:34,370
applications including such as Chrome

355
00:19:30,530 --> 00:19:36,590
Firefox notepad plus plus and three

356
00:19:34,370 --> 00:19:40,010
different malicious payload and three

357
00:19:36,590 --> 00:19:42,860
divin attack methods as the accuracy

358
00:19:40,010 --> 00:19:46,370
itself is always not enough to measure

359
00:19:42,860 --> 00:19:50,030
effectiveness of the detection model we

360
00:19:46,370 --> 00:19:53,959
use five different measurements ACC is

361
00:19:50,030 --> 00:19:56,149
the abbreviation of accuracy it measures

362
00:19:53,960 --> 00:19:57,770
the portion of the two results both the

363
00:19:56,150 --> 00:20:01,520
true positive and

364
00:19:57,770 --> 00:20:03,650
to- in the total health samples ppv

365
00:20:01,520 --> 00:20:07,820
refers to a positive predictive value

366
00:20:03,650 --> 00:20:10,550
also known as precision PPV measures the

367
00:20:07,820 --> 00:20:14,840
portion of actual benign samples in all

368
00:20:10,550 --> 00:20:18,110
predicted benign samples CPR is true

369
00:20:14,840 --> 00:20:20,300
positive rate also known as recall which

370
00:20:18,110 --> 00:20:22,879
measures the number of instances that

371
00:20:20,300 --> 00:20:26,930
are correctly classified as benign out

372
00:20:22,880 --> 00:20:29,840
of the total reopen instances similarly

373
00:20:26,930 --> 00:20:33,380
tlr refers to two negative rate it is

374
00:20:29,840 --> 00:20:36,679
also known as specificity similar to tpr

375
00:20:33,380 --> 00:20:38,720
TMR calculates out of the instances that

376
00:20:36,680 --> 00:20:41,390
are actually malicious the number of

377
00:20:38,720 --> 00:20:47,600
instances that are correctly classified

378
00:20:41,390 --> 00:20:51,110
as malicious mpv is negative predictive

379
00:20:47,600 --> 00:20:53,330
value similar to PPV mpv measures the

380
00:20:51,110 --> 00:20:55,580
portion of the actually malicious Sam

381
00:20:53,330 --> 00:20:58,689
hose out of the total predicted

382
00:20:55,580 --> 00:20:58,689
malicious symbols

383
00:21:06,680 --> 00:21:15,090
for each case in 21 test cases it

384
00:21:11,580 --> 00:21:17,879
consists of three datasets the pure

385
00:21:15,090 --> 00:21:20,610
benign dataset contains no attacks and

386
00:21:17,880 --> 00:21:23,790
is used as positive samples for our

387
00:21:20,610 --> 00:21:26,129
training the mixed dataset contains the

388
00:21:23,790 --> 00:21:30,060
camouflaged attacks and it also locks

389
00:21:26,130 --> 00:21:32,340
the host applications data the pure

390
00:21:30,060 --> 00:21:34,740
malicious data set only contains the

391
00:21:32,340 --> 00:21:37,709
malicious payload behavior we obtain

392
00:21:34,740 --> 00:21:40,380
these by manually extract and compel the

393
00:21:37,710 --> 00:21:45,290
malicious payload so this is data set is

394
00:21:40,380 --> 00:21:45,290
only used as a ground truth for testing

395
00:21:46,250 --> 00:21:52,650
in the training phase of our model for

396
00:21:50,160 --> 00:21:56,310
the positive samples we randomly select

397
00:21:52,650 --> 00:22:00,480
them from fifty percent pure benign data

398
00:21:56,310 --> 00:22:05,159
set for the negative samples we randomly

399
00:22:00,480 --> 00:22:07,140
select from mixed data set in the

400
00:22:05,160 --> 00:22:09,840
testing phase we test the positive

401
00:22:07,140 --> 00:22:12,320
samples randomly select from the rest of

402
00:22:09,840 --> 00:22:15,149
fifty percent pure benign data set and

403
00:22:12,320 --> 00:22:22,439
for negative samples we randomly select

404
00:22:15,150 --> 00:22:25,440
from the pure malicious data set we

405
00:22:22,440 --> 00:22:28,350
compare our results with both the

406
00:22:25,440 --> 00:22:32,100
original support vector machine model

407
00:22:28,350 --> 00:22:34,610
and system-level call graph model we

408
00:22:32,100 --> 00:22:38,399
find that our weighted svn model

409
00:22:34,610 --> 00:22:41,250
outperforms both svn and co graph model

410
00:22:38,400 --> 00:22:45,750
in all test cases on all five

411
00:22:41,250 --> 00:22:48,540
measurements this is the evaluation

412
00:22:45,750 --> 00:22:54,090
result which contains 30 offline

413
00:22:48,540 --> 00:22:56,430
infection cases the x-axis denotes the

414
00:22:54,090 --> 00:22:59,310
five different measurements and the

415
00:22:56,430 --> 00:23:03,510
y-axis do know the actual value of each

416
00:22:59,310 --> 00:23:07,740
measurements the red bar is our weighted

417
00:23:03,510 --> 00:23:10,980
SVM model the green bar is the original

418
00:23:07,740 --> 00:23:14,870
SVM model while the blue bar is the call

419
00:23:10,980 --> 00:23:14,870
graph based detection model

420
00:23:15,709 --> 00:23:22,019
this is the detection result comparison

421
00:23:18,899 --> 00:23:24,149
for online injection detection we can

422
00:23:22,019 --> 00:23:27,889
see the constant outperformance

423
00:23:24,149 --> 00:23:27,889
throughout the 21 cases

424
00:23:33,910 --> 00:23:42,080
here let me give a case study from our

425
00:23:37,670 --> 00:23:44,750
vet our evaluation in this case the host

426
00:23:42,080 --> 00:23:48,490
application is winged SCP and the

427
00:23:44,750 --> 00:23:51,860
malicious payload is a reverse TCP shell

428
00:23:48,490 --> 00:23:55,309
we leverage the tools on the metasploit

429
00:23:51,860 --> 00:23:59,919
framework to perform this attack we use

430
00:23:55,309 --> 00:24:03,770
the MSF payload to generate meterpreter

431
00:23:59,920 --> 00:24:06,679
payload this payload used the you

432
00:24:03,770 --> 00:24:09,100
reverse TCP as the connection for

433
00:24:06,679 --> 00:24:12,920
command and control you can perform

434
00:24:09,100 --> 00:24:15,620
keylogging file uploading taking

435
00:24:12,920 --> 00:24:18,559
screenshot and collecting password hash

436
00:24:15,620 --> 00:24:24,050
on the victim machine and the crack it

437
00:24:18,559 --> 00:24:27,230
offline then we use the MSF encode to

438
00:24:24,050 --> 00:24:32,000
encode the payload and embed it into a

439
00:24:27,230 --> 00:24:34,970
wing SCP binary for the detection

440
00:24:32,000 --> 00:24:37,660
accuracy we see it using call graph the

441
00:24:34,970 --> 00:24:41,300
rate is around seventy-five percent

442
00:24:37,660 --> 00:24:45,740
detection rate and four original SVM it

443
00:24:41,300 --> 00:24:49,370
is around eighty-six percent if we use

444
00:24:45,740 --> 00:24:51,350
weighted svn in leaves we can boost the

445
00:24:49,370 --> 00:24:55,610
accuracy accuracy to ninety-three

446
00:24:51,350 --> 00:25:00,919
percent for more results you can revert

447
00:24:55,610 --> 00:25:03,709
to our paper here we discuss some

448
00:25:00,920 --> 00:25:08,510
limitation of our system and what we

449
00:25:03,710 --> 00:25:11,540
want to explore in the future assume

450
00:25:08,510 --> 00:25:14,360
that the adversary can add the payload

451
00:25:11,540 --> 00:25:17,690
source code into the code base of the

452
00:25:14,360 --> 00:25:20,928
pinay application currently lips is not

453
00:25:17,690 --> 00:25:23,809
able to detect such attacks because all

454
00:25:20,929 --> 00:25:26,360
the benign code address will shift if

455
00:25:23,809 --> 00:25:29,600
you remember in our control flow graph

456
00:25:26,360 --> 00:25:33,040
inference one assumption is that the

457
00:25:29,600 --> 00:25:39,040
benign and camouflage attacks share some

458
00:25:33,040 --> 00:25:39,040
address for the benign execution path

459
00:25:39,100 --> 00:25:44,260
so we need to revise the control flow

460
00:25:41,620 --> 00:25:46,600
graph comparison algorithm we should not

461
00:25:44,260 --> 00:25:49,179
only consider the code address distance

462
00:25:46,600 --> 00:25:52,418
but also consider the shape of the

463
00:25:49,179 --> 00:25:55,720
control flow graph which may use the

464
00:25:52,419 --> 00:25:59,289
structure Nerney which is a hot topic in

465
00:25:55,720 --> 00:26:01,809
machine learning community currently we

466
00:25:59,289 --> 00:26:05,049
only considers the order of adjacent

467
00:26:01,809 --> 00:26:07,809
events but they may exist some causal

468
00:26:05,049 --> 00:26:12,250
relations between multiple events that

469
00:26:07,809 --> 00:26:14,200
are far away from each other in a log so

470
00:26:12,250 --> 00:26:16,270
we need to explore more learning

471
00:26:14,200 --> 00:26:19,090
approaches to review the hidden

472
00:26:16,270 --> 00:26:25,510
connections between them one possibility

473
00:26:19,090 --> 00:26:28,720
maybe the hidden Markov model so this is

474
00:26:25,510 --> 00:26:31,900
the conclusion for our paper we target

475
00:26:28,720 --> 00:26:34,090
the camouflaged attacks that brown the

476
00:26:31,900 --> 00:26:37,780
malicious payload under the cover of

477
00:26:34,090 --> 00:26:40,539
Samba lying applications such attacks

478
00:26:37,780 --> 00:26:43,658
can mislead the statistical learning

479
00:26:40,539 --> 00:26:47,440
approaches to generate a round decision

480
00:26:43,659 --> 00:26:49,750
boundary leaves integrates capabilities

481
00:26:47,440 --> 00:26:52,929
from program analysis to improve the

482
00:26:49,750 --> 00:26:55,390
results of statistical learning from the

483
00:26:52,929 --> 00:26:58,210
evaluation we see that leaves can

484
00:26:55,390 --> 00:27:00,730
effectively outperforms classification

485
00:26:58,210 --> 00:27:06,070
accuracy of traditional learning models

486
00:27:00,730 --> 00:27:12,990
on camouflaged attacks thank you for

487
00:27:06,070 --> 00:27:12,990
your listening any questions yeah

488
00:27:32,000 --> 00:27:39,180
when you showed us the data flow you

489
00:27:36,030 --> 00:27:42,060
said that you analyze the code section

490
00:27:39,180 --> 00:27:44,070
of the malware what do you do with the

491
00:27:42,060 --> 00:27:46,830
code that is hidden within the data

492
00:27:44,070 --> 00:28:00,060
section do you find that or do look into

493
00:27:46,830 --> 00:28:01,949
it zooming do you mean this your program

494
00:28:00,060 --> 00:28:03,899
yeah when when you create the data flow

495
00:28:01,950 --> 00:28:07,950
you said you go into the code space to

496
00:28:03,900 --> 00:28:09,630
find the flow of the program by

497
00:28:07,950 --> 00:28:12,240
analyzing some other malwares you have

498
00:28:09,630 --> 00:28:16,830
some of them hide some functionalities

499
00:28:12,240 --> 00:28:19,170
and other flows within the data section

500
00:28:16,830 --> 00:28:21,570
so do to do you take that into

501
00:28:19,170 --> 00:28:26,340
consideration or will it be considered

502
00:28:21,570 --> 00:28:30,600
part of a malware actually this are we

503
00:28:26,340 --> 00:28:32,520
we come here this malware with actually

504
00:28:30,600 --> 00:28:34,830
the mail way is a children application

505
00:28:32,520 --> 00:28:38,820
the application is benign in this self

506
00:28:34,830 --> 00:28:41,550
and we always run to two rounds the

507
00:28:38,820 --> 00:28:43,560
first one is to be nine runs and the

508
00:28:41,550 --> 00:28:48,440
second one we use the same machine and

509
00:28:43,560 --> 00:28:52,470
run the children children application up

510
00:28:48,440 --> 00:28:56,370
just after the benign one runs so we we

511
00:28:52,470 --> 00:28:58,290
can gain the address space of we can see

512
00:28:56,370 --> 00:29:01,370
that there are some sharing of the

513
00:28:58,290 --> 00:29:04,139
address space from these two runs and

514
00:29:01,370 --> 00:29:05,790
the remaining address in the second

515
00:29:04,140 --> 00:29:09,740
round which contains the malicious

516
00:29:05,790 --> 00:29:12,990
payload may have some address that have

517
00:29:09,740 --> 00:29:15,810
not been seen in the previous run which

518
00:29:12,990 --> 00:29:17,030
is the denial occasion another question

519
00:29:15,810 --> 00:29:21,750
I would have how would you deal with

520
00:29:17,030 --> 00:29:25,560
rats remote access Trojans and the third

521
00:29:21,750 --> 00:29:27,750
one would be time time constrain attacks

522
00:29:25,560 --> 00:29:30,149
I mean some people will write malware's

523
00:29:27,750 --> 00:29:31,260
the tauren in a specific time so if

524
00:29:30,150 --> 00:29:32,310
you're testing it against your

525
00:29:31,260 --> 00:29:33,840
application and that

526
00:29:32,310 --> 00:29:36,780
I'm is not that you'll not know that

527
00:29:33,840 --> 00:29:39,030
there is an extra execution or an extra

528
00:29:36,780 --> 00:29:41,280
function hit in somewhere yeah yeah that

529
00:29:39,030 --> 00:29:43,580
that's one of the limitation of our work

530
00:29:41,280 --> 00:29:46,920
and currently we are working on the

531
00:29:43,580 --> 00:29:50,820
spatial temporary dispersed log entries

532
00:29:46,920 --> 00:29:53,910
from very far away and a different light

533
00:29:50,820 --> 00:29:55,679
weight lock and yeah we are currently

534
00:29:53,910 --> 00:29:58,500
working on that that is one of the

535
00:29:55,680 --> 00:30:04,850
limitation in leaves actually thank you

536
00:29:58,500 --> 00:30:04,850
please any other questions

537
00:30:10,340 --> 00:30:17,330
if if you know thank you so much

