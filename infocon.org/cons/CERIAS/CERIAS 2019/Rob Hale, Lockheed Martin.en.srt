1
00:00:00,030 --> 00:00:06,960
welcome back our next speaker our let me

2
00:00:04,620 --> 00:00:09,690
change that our previous speaker was

3
00:00:06,960 --> 00:00:12,090
from MasterCard who is a member the

4
00:00:09,690 --> 00:00:13,678
organization is a member of this serious

5
00:00:12,090 --> 00:00:15,389
strategic partnership program for less

6
00:00:13,679 --> 00:00:17,160
than a year we've a lot going on with

7
00:00:15,389 --> 00:00:18,779
them and we're excited about that

8
00:00:17,160 --> 00:00:20,970
opportunity and what a great talk and

9
00:00:18,779 --> 00:00:23,698
we're happy to hear on the other end of

10
00:00:20,970 --> 00:00:26,698
the spectrum who we are also very happy

11
00:00:23,699 --> 00:00:28,859
is here but the relationship with Sirius

12
00:00:26,699 --> 00:00:31,050
goes back more than 20 years you know

13
00:00:28,859 --> 00:00:32,910
we're now celebrating our 21st year so

14
00:00:31,050 --> 00:00:36,480
our speaker now is from Lockheed Martin

15
00:00:32,910 --> 00:00:39,569
mr. Rob Hale or it's a dr. Rob here yet

16
00:00:36,480 --> 00:00:40,949
okay not yet okay so so Rob Hale who is

17
00:00:39,570 --> 00:00:43,050
a Lockheed Martin fellow with

18
00:00:40,950 --> 00:00:49,590
cybersecurity we're pleased you're here

19
00:00:43,050 --> 00:00:50,910
thanks Rob thank you okay so I put this

20
00:00:49,590 --> 00:00:53,789
on myself so I don't know if this is

21
00:00:50,910 --> 00:00:55,078
gonna work right or not since I've

22
00:00:53,789 --> 00:00:57,480
already screwed up passing my slides

23
00:00:55,079 --> 00:00:58,739
already once today I was actually going

24
00:00:57,480 --> 00:01:01,589
to ask how many people out here I've

25
00:00:58,739 --> 00:01:04,890
owned an f-35 I mean if you know I give

26
00:01:01,590 --> 00:01:08,820
them one of it we really couldn't after

27
00:01:04,890 --> 00:01:10,560
the thing in Japan here recently that's

28
00:01:08,820 --> 00:01:13,610
that's a joke that all my business

29
00:01:10,560 --> 00:01:16,560
development people are gonna hate I

30
00:01:13,610 --> 00:01:18,689
titled this this presentation about 20

31
00:01:16,560 --> 00:01:20,159
different titles because I really

32
00:01:18,689 --> 00:01:23,070
couldn't come up with the way I wanted

33
00:01:20,159 --> 00:01:25,939
to say it and I was watching watching

34
00:01:23,070 --> 00:01:29,008
TED Talks I like TED talks on various

35
00:01:25,939 --> 00:01:32,460
sundry things and there was one that was

36
00:01:29,009 --> 00:01:34,290
talking about clarity versus mystery and

37
00:01:32,460 --> 00:01:35,789
you know things like that I've been

38
00:01:34,290 --> 00:01:38,159
trying to try to do something that's

39
00:01:35,790 --> 00:01:40,380
gonna really you know cause the lasting

40
00:01:38,159 --> 00:01:42,540
memorable impression in ways other than

41
00:01:40,380 --> 00:01:46,170
ways that I've created lasting memorable

42
00:01:42,540 --> 00:01:47,610
impressions in the past and it talked

43
00:01:46,170 --> 00:01:49,049
about the importance of a title and I

44
00:01:47,610 --> 00:01:50,520
figure I screwed that up at the front so

45
00:01:49,049 --> 00:01:52,740
the rest of presentation is gonna go

46
00:01:50,520 --> 00:01:55,439
really well i titled it defense in the

47
00:01:52,740 --> 00:01:58,439
21st century in the 21st iteration of

48
00:01:55,439 --> 00:01:59,758
this because it really is that that's

49
00:01:58,439 --> 00:02:02,130
what we do at Lockheed we're a global

50
00:01:59,759 --> 00:02:05,520
security company it's no surprise to

51
00:02:02,130 --> 00:02:07,380
anyone we make airplanes that everything

52
00:02:05,520 --> 00:02:09,899
from cargo planes to weapons platforms

53
00:02:07,380 --> 00:02:13,240
we make missile defense systems we make

54
00:02:09,899 --> 00:02:15,800
a lot of things that are not necessarily

55
00:02:13,240 --> 00:02:18,620
defense-related from that standpoint we

56
00:02:15,800 --> 00:02:20,660
make we do a lot of work in the energy

57
00:02:18,620 --> 00:02:22,640
community and associated communities but

58
00:02:20,660 --> 00:02:25,010
our predominant business is in the

59
00:02:22,640 --> 00:02:26,779
global security industry and one of the

60
00:02:25,010 --> 00:02:30,679
biggest challenges that we have today is

61
00:02:26,780 --> 00:02:33,560
really in this how do we bring all of

62
00:02:30,680 --> 00:02:34,820
this data we do not lack for data how do

63
00:02:33,560 --> 00:02:37,880
we bring this together and how do we

64
00:02:34,820 --> 00:02:39,470
actually provide a meaningful defense as

65
00:02:37,880 --> 00:02:41,480
we go forward given our adversaries

66
00:02:39,470 --> 00:02:43,640
today now I will tell you right now I am

67
00:02:41,480 --> 00:02:45,320
NOT an expert in artificial intelligence

68
00:02:43,640 --> 00:02:48,500
machine learning neural networks or any

69
00:02:45,320 --> 00:02:49,760
of the associated practices thereof I've

70
00:02:48,500 --> 00:02:51,500
just have a lot of experience in

71
00:02:49,760 --> 00:02:54,019
building secure architectures I don't

72
00:02:51,500 --> 00:02:56,300
work with our IT security department we

73
00:02:54,020 --> 00:02:58,120
have an entirely great group of people

74
00:02:56,300 --> 00:03:01,310
to do that and keep us very very secure

75
00:02:58,120 --> 00:03:02,840
they do a fantastic job and I don't work

76
00:03:01,310 --> 00:03:04,010
with them I do advance research and

77
00:03:02,840 --> 00:03:06,050
development and trying to secure our

78
00:03:04,010 --> 00:03:07,429
weapons platforms and also the

79
00:03:06,050 --> 00:03:09,560
interconnection of those platforms and

80
00:03:07,430 --> 00:03:11,150
systems as we go forward so what I'm

81
00:03:09,560 --> 00:03:13,520
going to give you is what I wanted to

82
00:03:11,150 --> 00:03:14,780
talk about today is really what are the

83
00:03:13,520 --> 00:03:16,430
challenges that we're facing what are

84
00:03:14,780 --> 00:03:17,930
some of the things we're doing and just

85
00:03:16,430 --> 00:03:20,510
kind of leave some open questions with

86
00:03:17,930 --> 00:03:21,650
you to be thinking about and to be

87
00:03:20,510 --> 00:03:24,320
looking at because these are the things

88
00:03:21,650 --> 00:03:28,570
that were that we're dealing with I love

89
00:03:24,320 --> 00:03:31,549
the look let's try the right button

90
00:03:28,570 --> 00:03:33,079
there we go I started with Albert

91
00:03:31,550 --> 00:03:34,370
Einstein's picture because I loved his

92
00:03:33,080 --> 00:03:35,570
quote there were the quote that's at

93
00:03:34,370 --> 00:03:37,040
least attributed to him if you go to

94
00:03:35,570 --> 00:03:39,410
Snopes there's all sorts of questions as

95
00:03:37,040 --> 00:03:40,640
to whether or not it was him but where

96
00:03:39,410 --> 00:03:41,990
he was talking about I don't know what

97
00:03:40,640 --> 00:03:43,309
weapons World War 3 will be fought with

98
00:03:41,990 --> 00:03:46,250
but the war after that one will be

99
00:03:43,310 --> 00:03:47,510
fought with sticks and stones of course

100
00:03:46,250 --> 00:03:49,310
this was after World War two where he

101
00:03:47,510 --> 00:03:51,049
had the atomic bomb he had all of this

102
00:03:49,310 --> 00:03:52,280
work that he was doing with that and he

103
00:03:51,050 --> 00:03:54,620
was really talking about the fact that

104
00:03:52,280 --> 00:03:55,790
really the technological advantages or

105
00:03:54,620 --> 00:03:57,380
at least the way I interpreted the

106
00:03:55,790 --> 00:03:59,179
technological advantages are such that

107
00:03:57,380 --> 00:04:00,799
it's changing so much that we were going

108
00:03:59,180 --> 00:04:02,300
to have something so devastating or it

109
00:04:00,800 --> 00:04:04,850
could be so devastating that after that

110
00:04:02,300 --> 00:04:07,580
there's there's nothing nothing else I

111
00:04:04,850 --> 00:04:10,070
would like to kind of posit that well it

112
00:04:07,580 --> 00:04:12,680
may not be the weapon that is used in

113
00:04:10,070 --> 00:04:14,810
the next war the information is going to

114
00:04:12,680 --> 00:04:16,310
be the enabler of the weapons systems of

115
00:04:14,810 --> 00:04:22,540
the future and the weapon systems

116
00:04:16,310 --> 00:04:24,130
frankly of today okay there we go so

117
00:04:22,540 --> 00:04:25,480
the first question and that we deal with

118
00:04:24,130 --> 00:04:26,980
the should we be doing this now I'm

119
00:04:25,480 --> 00:04:28,660
going to punt on this I'll tell you that

120
00:04:26,980 --> 00:04:29,980
right up front because there's a lot of

121
00:04:28,660 --> 00:04:31,390
ethical concerns and questions and

122
00:04:29,980 --> 00:04:33,750
they're very viable ones that need to be

123
00:04:31,390 --> 00:04:36,190
talked about across all the communities

124
00:04:33,750 --> 00:04:39,940
there's legal ramifications there's a

125
00:04:36,190 --> 00:04:41,320
syllabic ramifications doing research

126
00:04:39,940 --> 00:04:43,360
and development the way I look at this

127
00:04:41,320 --> 00:04:45,010
question the way I the way I ask for a

128
00:04:43,360 --> 00:04:47,980
way I answer it is really this is

129
00:04:45,010 --> 00:04:49,840
somebody else going to do it the fact of

130
00:04:47,980 --> 00:04:52,870
the matter is technology has always been

131
00:04:49,840 --> 00:04:55,000
abused you go all the way back to the

132
00:04:52,870 --> 00:04:56,590
first guy that used a spear first guy

133
00:04:55,000 --> 00:04:59,620
used the spear goes you know I can use

134
00:04:56,590 --> 00:05:01,570
this to hunt elk or that deer and goes

135
00:04:59,620 --> 00:05:03,070
out you got a second guy maybe not as

136
00:05:01,570 --> 00:05:04,330
fast because I'm not as fast enough to

137
00:05:03,070 --> 00:05:05,530
catch that deer but I can sure enough

138
00:05:04,330 --> 00:05:07,960
use it to catch the guy that caught the

139
00:05:05,530 --> 00:05:09,640
deer so you have an abuse of the first

140
00:05:07,960 --> 00:05:11,469
the first thing they see use as far as

141
00:05:09,640 --> 00:05:13,360
technology is going we have the same

142
00:05:11,470 --> 00:05:15,640
thing today we have a lot of really good

143
00:05:13,360 --> 00:05:17,050
technology that gets abused we will

144
00:05:15,640 --> 00:05:19,479
continue to have technology that gets

145
00:05:17,050 --> 00:05:21,820
abused that is always going to happen

146
00:05:19,480 --> 00:05:23,530
and if it's something that we ignore or

147
00:05:21,820 --> 00:05:25,480
something that we that we refuse to look

148
00:05:23,530 --> 00:05:27,099
at because of the because of the

149
00:05:25,480 --> 00:05:28,450
concerns and the ethical considerations

150
00:05:27,100 --> 00:05:30,460
I'm not saying that we should punt on

151
00:05:28,450 --> 00:05:32,200
those I'm just saying that from an R&D

152
00:05:30,460 --> 00:05:35,169
perspective we should not turn away from

153
00:05:32,200 --> 00:05:36,610
those and use that as a as a system

154
00:05:35,170 --> 00:05:38,560
saying we just shouldn't address this if

155
00:05:36,610 --> 00:05:40,390
we refuse to address it it will be

156
00:05:38,560 --> 00:05:43,330
addressed by someone for us and we

157
00:05:40,390 --> 00:05:46,930
probably will not like the result so

158
00:05:43,330 --> 00:05:49,180
that's my soapbox for that going to that

159
00:05:46,930 --> 00:05:50,980
I really get to the point of intelligent

160
00:05:49,180 --> 00:05:52,900
machines because that's what we're

161
00:05:50,980 --> 00:05:55,240
starting to talk to talk about how do we

162
00:05:52,900 --> 00:05:56,620
get to a point where we've got all of

163
00:05:55,240 --> 00:05:58,000
these things connected together getting

164
00:05:56,620 --> 00:05:59,200
the information that they need and I'll

165
00:05:58,000 --> 00:06:01,480
go into a little bit about how lakis

166
00:05:59,200 --> 00:06:03,280
addressing it getting the information

167
00:06:01,480 --> 00:06:04,780
that they need how do we go about and

168
00:06:03,280 --> 00:06:06,400
manage this how intelligent will

169
00:06:04,780 --> 00:06:07,780
machines get there's a lot of different

170
00:06:06,400 --> 00:06:09,549
schools of thought on this by the way

171
00:06:07,780 --> 00:06:11,229
you have the Ray Kurzweil school but it

172
00:06:09,550 --> 00:06:12,880
talks about you know the idea of being

173
00:06:11,230 --> 00:06:14,410
able to map the human brain and you can

174
00:06:12,880 --> 00:06:15,820
upload your consciousness and everything

175
00:06:14,410 --> 00:06:17,670
and then you know download it back

176
00:06:15,820 --> 00:06:22,450
because it's just a computational

177
00:06:17,670 --> 00:06:24,040
computational issue I I've also seen

178
00:06:22,450 --> 00:06:27,099
arguments on the other side that you

179
00:06:24,040 --> 00:06:29,350
know it and somewhat philosophical that

180
00:06:27,100 --> 00:06:31,060
it's very very difficult to create

181
00:06:29,350 --> 00:06:33,520
something that is going to be is going

182
00:06:31,060 --> 00:06:35,200
to be better or more more capable than

183
00:06:33,520 --> 00:06:36,359
you so it becomes a question of moral

184
00:06:35,200 --> 00:06:44,320
philosophy at that

185
00:06:36,360 --> 00:06:50,980
there's a okay you know what I will

186
00:06:44,320 --> 00:06:52,750
figure this out at one point we already

187
00:06:50,980 --> 00:06:55,660
have discussions about whether or not

188
00:06:52,750 --> 00:06:57,340
the self-driving car we've seen we've

189
00:06:55,660 --> 00:06:59,980
seen those types of the questions about

190
00:06:57,340 --> 00:07:01,179
what about the question self-driving car

191
00:06:59,980 --> 00:07:02,710
makes it was a really great point that

192
00:07:01,180 --> 00:07:05,640
was made that a sticker on a stop sign

193
00:07:02,710 --> 00:07:08,950
can actually be fort that at the moment

194
00:07:05,640 --> 00:07:11,140
we have if anybody seen the Boston

195
00:07:08,950 --> 00:07:14,140
Dynamics videos awesome videos this is

196
00:07:11,140 --> 00:07:15,310
Atlas and mini dog they recently just

197
00:07:14,140 --> 00:07:16,870
did a thing where they took Atlas out

198
00:07:15,310 --> 00:07:20,050
and he ran through a park jumped over a

199
00:07:16,870 --> 00:07:21,700
log it was kind of cool autonomously how

200
00:07:20,050 --> 00:07:23,530
do we take something where we get to

201
00:07:21,700 --> 00:07:28,060
where we trust those without turning it

202
00:07:23,530 --> 00:07:29,830
into this or this that's always been the

203
00:07:28,060 --> 00:07:31,360
big question is when do we turn the

204
00:07:29,830 --> 00:07:32,409
decision-making over the machines when

205
00:07:31,360 --> 00:07:36,480
do the machines decided to take over

206
00:07:32,410 --> 00:07:39,820
decision-making of their own there's

207
00:07:36,480 --> 00:07:41,530
there's a lot of interesting research I

208
00:07:39,820 --> 00:07:44,010
know that's going on in the AI space but

209
00:07:41,530 --> 00:07:46,179
there really the question then becomes

210
00:07:44,010 --> 00:07:48,280
going back to almost Asimov's three

211
00:07:46,180 --> 00:07:50,470
rules of robotics how do we interact

212
00:07:48,280 --> 00:07:52,510
with these how do we how do we gate what

213
00:07:50,470 --> 00:07:54,070
these machines are able to do how do we

214
00:07:52,510 --> 00:07:55,690
gate what we're what we're able to do

215
00:07:54,070 --> 00:07:58,110
with means have been the purpose of

216
00:07:55,690 --> 00:08:00,550
human augmentation in such a way that

217
00:07:58,110 --> 00:08:01,990
we're able to defend against this type

218
00:08:00,550 --> 00:08:04,240
of thing and yet at the same time still

219
00:08:01,990 --> 00:08:07,780
enjoy the benefits of it I would submit

220
00:08:04,240 --> 00:08:09,880
that it's really a question of trust or

221
00:08:07,780 --> 00:08:12,880
if you don't like to trust it's a

222
00:08:09,880 --> 00:08:15,240
question of risk trust and risk are

223
00:08:12,880 --> 00:08:19,180
really the flip sides of the same coin

224
00:08:15,240 --> 00:08:22,540
we're not really good at managing risk

225
00:08:19,180 --> 00:08:24,790
in the cybersecurity arena yet we talk

226
00:08:22,540 --> 00:08:25,990
about it a lot we've got customers of

227
00:08:24,790 --> 00:08:27,490
demanda we got the risk management

228
00:08:25,990 --> 00:08:30,040
framework that's got risk in the title

229
00:08:27,490 --> 00:08:32,200
it must be great but we don't manage to

230
00:08:30,040 --> 00:08:34,270
risk we're managing to some sort of zero

231
00:08:32,200 --> 00:08:35,620
base concept that we have and yet in our

232
00:08:34,270 --> 00:08:38,319
daily lives we manage to risk all the

233
00:08:35,620 --> 00:08:40,750
time the earlier this morning we heard

234
00:08:38,320 --> 00:08:42,280
you know I locked my car why do I lock

235
00:08:40,750 --> 00:08:45,040
my car because there's a risk associated

236
00:08:42,280 --> 00:08:47,020
with it I take a look at it and just

237
00:08:45,040 --> 00:08:48,250
right now inherently I'm making the

238
00:08:47,020 --> 00:08:49,300
decision yeah it makes more sense to

239
00:08:48,250 --> 00:08:49,540
lock my car because I'm not willing to

240
00:08:49,300 --> 00:08:51,729
take

241
00:08:49,540 --> 00:08:52,990
the risk we do the same thing when we

242
00:08:51,730 --> 00:08:54,730
walk down the street we walk down the

243
00:08:52,990 --> 00:08:58,180
street you're looking at traffic coming

244
00:08:54,730 --> 00:08:59,620
by you come up on a red light you kind

245
00:08:58,180 --> 00:09:01,810
of look at traffic on either side to see

246
00:08:59,620 --> 00:09:04,029
if you can go it's a risk calculation we

247
00:09:01,810 --> 00:09:08,518
do it in every other area of it and yet

248
00:09:04,029 --> 00:09:10,959
we punt on it so far in cybersecurity

249
00:09:08,519 --> 00:09:13,750
not necessarily as professionals we all

250
00:09:10,959 --> 00:09:16,060
believe in it but getting it adopted and

251
00:09:13,750 --> 00:09:18,339
understood at a concept of level of

252
00:09:16,060 --> 00:09:19,660
acceptable risk I really like the point

253
00:09:18,339 --> 00:09:23,319
that was made earlier this morning by

254
00:09:19,660 --> 00:09:25,269
mr. English we're talking about how we

255
00:09:23,319 --> 00:09:26,860
can't trust these underlying machines we

256
00:09:25,269 --> 00:09:29,680
need to reintroduce the concept of

257
00:09:26,860 --> 00:09:33,880
skepticism part of the reason that the

258
00:09:29,680 --> 00:09:37,689
information operations worked with with

259
00:09:33,880 --> 00:09:39,189
the Russia in the in 2016 part of the

260
00:09:37,690 --> 00:09:41,980
reason that that worked was because we

261
00:09:39,190 --> 00:09:45,009
tend to as a society believe everything

262
00:09:41,980 --> 00:09:47,410
that comes out of that box if we saw it

263
00:09:45,009 --> 00:09:48,970
it must be true especially if it lines

264
00:09:47,410 --> 00:09:51,610
up with what we want to believe is true

265
00:09:48,970 --> 00:09:56,410
in the first place so we don't really

266
00:09:51,610 --> 00:09:57,880
have this concept yet of of risk based

267
00:09:56,410 --> 00:09:59,079
decision making when it comes to this

268
00:09:57,880 --> 00:10:01,389
play and that's something that I think I

269
00:09:59,079 --> 00:10:03,310
believe is a real a real key to getting

270
00:10:01,389 --> 00:10:06,220
to a future where we're able to use

271
00:10:03,310 --> 00:10:07,540
autonomous systems and AI systems it's

272
00:10:06,220 --> 00:10:10,209
particularly where they interact we're

273
00:10:07,540 --> 00:10:12,610
going to have to demonstrate that they

274
00:10:10,209 --> 00:10:14,589
have it they have a level of trust or a

275
00:10:12,610 --> 00:10:16,510
level of acceptable risk that we're

276
00:10:14,589 --> 00:10:19,839
willing to tolerate it and then by the

277
00:10:16,510 --> 00:10:21,250
way tolerated I've seen a matter of fact

278
00:10:19,839 --> 00:10:22,990
I would ask how many people out there

279
00:10:21,250 --> 00:10:24,670
have customers or have organizations

280
00:10:22,990 --> 00:10:26,589
that they're in that have this concept

281
00:10:24,670 --> 00:10:28,810
of yeah we we've got a level of

282
00:10:26,589 --> 00:10:30,190
acceptable risk and then want to shoot

283
00:10:28,810 --> 00:10:32,920
the messenger anytime something goes

284
00:10:30,190 --> 00:10:34,360
wrong I've got one hand in the back

285
00:10:32,920 --> 00:10:38,469
which means I've got a lot of people in

286
00:10:34,360 --> 00:10:43,810
denial that's okay I've been in denial

287
00:10:38,470 --> 00:10:45,160
for a long time so that's one of the

288
00:10:43,810 --> 00:10:46,209
things that we're trying to address one

289
00:10:45,160 --> 00:10:47,439
of the things we're trying to address in

290
00:10:46,209 --> 00:10:49,300
Lockheed is actually how do you get to a

291
00:10:47,439 --> 00:10:50,829
level of risk or a level of trust that

292
00:10:49,300 --> 00:10:53,139
says you know what the decision the

293
00:10:50,829 --> 00:10:54,430
machines gonna make on this is no worse

294
00:10:53,139 --> 00:10:56,860
than the average human being is going to

295
00:10:54,430 --> 00:10:58,810
make given the same data I mean

296
00:10:56,860 --> 00:11:01,839
realistically we want to try and drive

297
00:10:58,810 --> 00:11:03,160
it to zero but how many times have we

298
00:11:01,839 --> 00:11:04,899
made the right to spray Tris

299
00:11:03,160 --> 00:11:07,000
decision or how many times have we made

300
00:11:04,899 --> 00:11:08,470
a wrong risk decision and then had to

301
00:11:07,000 --> 00:11:10,930
figure out how to return from that

302
00:11:08,470 --> 00:11:15,220
that's what resiliency and robustness is

303
00:11:10,930 --> 00:11:18,729
all about how intelligent will machines

304
00:11:15,220 --> 00:11:20,230
get well anybody know what when this

305
00:11:18,730 --> 00:11:27,069
took place this is Kasparov and deep

306
00:11:20,230 --> 00:11:30,430
blue this is their second match 1997 22

307
00:11:27,069 --> 00:11:33,099
years ago this year deep blue actually

308
00:11:30,430 --> 00:11:34,930
defeated Garry Kasparov heard a great

309
00:11:33,100 --> 00:11:36,009
great talk by Garry Kasparov you just

310
00:11:34,930 --> 00:11:37,209
talk about the fact that one of the

311
00:11:36,009 --> 00:11:41,079
things was kind of interesting is his

312
00:11:37,209 --> 00:11:43,899
can his legal team actually negotiated

313
00:11:41,079 --> 00:11:45,339
to where they would get a record of all

314
00:11:43,899 --> 00:11:47,170
the games that deep blue played in

315
00:11:45,339 --> 00:11:49,209
competition for him to review before

316
00:11:47,170 --> 00:11:51,699
going into his competition the sum total

317
00:11:49,209 --> 00:11:53,469
of those games was zero it never played

318
00:11:51,699 --> 00:11:55,300
a competition game which denied them

319
00:11:53,470 --> 00:11:57,250
information he could use about learning

320
00:11:55,300 --> 00:12:00,569
how deep blue actually how its

321
00:11:57,250 --> 00:12:03,250
algorithms worked however deep blue

322
00:12:00,569 --> 00:12:07,540
basically destroyed him in that in that

323
00:12:03,250 --> 00:12:12,009
series that was 22 years ago fast

324
00:12:07,540 --> 00:12:14,199
forward 20 years we have alphago go is a

325
00:12:12,009 --> 00:12:16,860
much more complex version or complex

326
00:12:14,199 --> 00:12:20,469
game than chess and yet it we now have

327
00:12:16,860 --> 00:12:23,470
alphago defeating in 2017 yeah

328
00:12:20,470 --> 00:12:27,970
20 years later defeating the leading

329
00:12:23,470 --> 00:12:29,620
human in go 20 years now there were

330
00:12:27,970 --> 00:12:30,910
couple of technology changes and

331
00:12:29,620 --> 00:12:32,589
improvements over the course of 20 years

332
00:12:30,910 --> 00:12:34,149
there was a thing called the iPhone that

333
00:12:32,589 --> 00:12:37,870
came out there was a you know iPods

334
00:12:34,149 --> 00:12:39,819
things like that but it it goes to talk

335
00:12:37,870 --> 00:12:41,110
about how we're now starting to look at

336
00:12:39,819 --> 00:12:42,639
and how we're trying to train and we're

337
00:12:41,110 --> 00:12:45,189
training machines that are able to do

338
00:12:42,639 --> 00:12:47,829
distributed distributed Albertus

339
00:12:45,189 --> 00:12:50,740
tributed algorithms much much faster and

340
00:12:47,829 --> 00:12:54,489
make decisions based on not 100%

341
00:12:50,740 --> 00:12:56,740
information in order to get there so I

342
00:12:54,490 --> 00:12:59,079
would before I get to what Lockheed is

343
00:12:56,740 --> 00:13:01,420
doing so I would I would speculate and

344
00:12:59,079 --> 00:13:04,870
again it's speculation we're gonna run

345
00:13:01,420 --> 00:13:06,610
into this in an ever-increasing rate it

346
00:13:04,870 --> 00:13:08,199
took 20 years to get to this step I

347
00:13:06,610 --> 00:13:10,149
would I would submit that it's not a

348
00:13:08,199 --> 00:13:11,559
linear kind of growth in the end that

349
00:13:10,149 --> 00:13:12,490
technology in that learning level I

350
00:13:11,559 --> 00:13:14,980
would say that it's probably more

351
00:13:12,490 --> 00:13:16,780
exponential I don't have a factual basis

352
00:13:14,980 --> 00:13:19,450
for that or a statistical one

353
00:13:16,780 --> 00:13:21,339
those are not necessarily the same but I

354
00:13:19,450 --> 00:13:23,920
do have a belief that given the other

355
00:13:21,340 --> 00:13:25,240
patterns I've seen in technology growth

356
00:13:23,920 --> 00:13:27,209
that we're going to start to see more

357
00:13:25,240 --> 00:13:31,000
and more of this work

358
00:13:27,210 --> 00:13:32,140
so what's Lockheed doing that well we

359
00:13:31,000 --> 00:13:34,510
spent all last week

360
00:13:32,140 --> 00:13:35,740
in a forum gathered together talking

361
00:13:34,510 --> 00:13:37,210
about what are the hard problems that

362
00:13:35,740 --> 00:13:39,820
we've got coming forward so it actually

363
00:13:37,210 --> 00:13:42,820
this became a very timely timely

364
00:13:39,820 --> 00:13:45,940
discussion we're looking at an

365
00:13:42,820 --> 00:13:47,820
integrated battlefield we're looking at

366
00:13:45,940 --> 00:13:49,750
the fact that we've got sensors

367
00:13:47,820 --> 00:13:51,750
everywhere whether it be airborne

368
00:13:49,750 --> 00:13:54,100
whether it be space borne whether it be

369
00:13:51,750 --> 00:13:56,380
whether it be just a programmable logic

370
00:13:54,100 --> 00:13:58,510
controller node somewhere sitting there

371
00:13:56,380 --> 00:14:00,040
we have ground systems we have missile

372
00:13:58,510 --> 00:14:02,710
systems all these things talk but not

373
00:14:00,040 --> 00:14:04,870
necessarily to each other that is still

374
00:14:02,710 --> 00:14:07,000
a big capability gap that exists across

375
00:14:04,870 --> 00:14:08,440
the industry today now there are some

376
00:14:07,000 --> 00:14:10,030
translation things we're seeing a lot of

377
00:14:08,440 --> 00:14:12,550
movement across all of the defense

378
00:14:10,030 --> 00:14:14,319
industry in trying to bring those types

379
00:14:12,550 --> 00:14:17,530
of things together but there are there's

380
00:14:14,320 --> 00:14:18,670
a very basic concept of if these things

381
00:14:17,530 --> 00:14:20,829
are going to communicate to each other

382
00:14:18,670 --> 00:14:22,540
first we need to actually make them

383
00:14:20,830 --> 00:14:24,130
communicate with each other I mean be

384
00:14:22,540 --> 00:14:27,370
able to listen to each other and talk to

385
00:14:24,130 --> 00:14:29,560
each other secondly we're trying to take

386
00:14:27,370 --> 00:14:31,750
the the data that we're getting from all

387
00:14:29,560 --> 00:14:33,969
this that we're able to turn into

388
00:14:31,750 --> 00:14:36,430
individual pieces of information and say

389
00:14:33,970 --> 00:14:38,920
okay can we create a global situational

390
00:14:36,430 --> 00:14:41,079
awareness not for each of those machines

391
00:14:38,920 --> 00:14:42,939
they're sitting out here but can we

392
00:14:41,080 --> 00:14:46,300
create it in such a way that they can

393
00:14:42,940 --> 00:14:48,670
create a mission unique situational

394
00:14:46,300 --> 00:14:50,859
awareness if I've got a missile system

395
00:14:48,670 --> 00:14:52,780
sitting you know on the ground out here

396
00:14:50,860 --> 00:14:54,820
somewhere missile defense system it may

397
00:14:52,780 --> 00:14:56,860
not necessarily care if it's sitting in

398
00:14:54,820 --> 00:14:59,560
say Japan it may not necessarily care

399
00:14:56,860 --> 00:15:01,839
what's going on in Europe it may not be

400
00:14:59,560 --> 00:15:03,760
relevant to it but there may be aspects

401
00:15:01,839 --> 00:15:05,650
that would be relevant I'm not sure what

402
00:15:03,760 --> 00:15:08,020
and it would need to have access to be

403
00:15:05,650 --> 00:15:10,569
able to get that information as it as it

404
00:15:08,020 --> 00:15:13,870
needs it on demand we also need that in

405
00:15:10,570 --> 00:15:15,250
the sense that for this to work we're

406
00:15:13,870 --> 00:15:17,530
gonna have to push processing to the

407
00:15:15,250 --> 00:15:20,110
edge processing requirements are going

408
00:15:17,530 --> 00:15:21,550
to come much much more difficult one of

409
00:15:20,110 --> 00:15:24,839
the estimations that we have at a

410
00:15:21,550 --> 00:15:27,819
minimum it will be a teraflop four watt

411
00:15:24,839 --> 00:15:29,560
current processor technologies out there

412
00:15:27,820 --> 00:15:30,520
if you're looking at high really really

413
00:15:29,560 --> 00:15:33,760
high performance process

414
00:15:30,520 --> 00:15:37,990
are about a tenth of that right now

415
00:15:33,760 --> 00:15:39,550
a teraflop per watt so think 45 watts 44

416
00:15:37,990 --> 00:15:42,120
watch you're looking 44 teraflops of

417
00:15:39,550 --> 00:15:42,120
processing

418
00:15:42,209 --> 00:15:48,279
I'm sorry teraflop 4 watt per second you

419
00:15:46,959 --> 00:15:50,529
get to that kind of that kind of level

420
00:15:48,279 --> 00:15:52,990
of processing you've got is gonna take a

421
00:15:50,529 --> 00:15:55,149
hardware the hardware change but and

422
00:15:52,990 --> 00:15:56,620
needs to be focused to how do we start

423
00:15:55,149 --> 00:15:57,970
to integrate the systems together and

424
00:15:56,620 --> 00:16:00,279
get them to talk and understand each

425
00:15:57,970 --> 00:16:02,860
other and then how do we get them to do

426
00:16:00,279 --> 00:16:04,959
their own cognitive decision-making at

427
00:16:02,860 --> 00:16:06,730
those points how do we get them to

428
00:16:04,959 --> 00:16:09,130
understand the environment that they're

429
00:16:06,730 --> 00:16:12,520
living in themselves and understand from

430
00:16:09,130 --> 00:16:14,080
there how to how to trust the machines

431
00:16:12,520 --> 00:16:16,000
around them and what level of trust that

432
00:16:14,080 --> 00:16:17,760
is because it's also probably not going

433
00:16:16,000 --> 00:16:20,020
to be hundred-percent

434
00:16:17,760 --> 00:16:23,589
some of the other things that we that

435
00:16:20,020 --> 00:16:27,939
we've been doing and getting a little

436
00:16:23,589 --> 00:16:29,890
close on time okay I have to talk about

437
00:16:27,940 --> 00:16:31,180
this and this is alpha pilot alpha

438
00:16:29,890 --> 00:16:33,310
pilots one of the fun things that lucky

439
00:16:31,180 --> 00:16:34,959
gets to do unlike our new space scent

440
00:16:33,310 --> 00:16:38,890
cologne if anybody saw it out on April

441
00:16:34,959 --> 00:16:40,479
1st okay nobody did if you get a chance

442
00:16:38,890 --> 00:16:42,220
to use the Wayback Machine go back to

443
00:16:40,480 --> 00:16:44,140
April 1st read our space systems company

444
00:16:42,220 --> 00:16:46,630
space company came out with a new

445
00:16:44,140 --> 00:16:48,370
cologne that smelled like space it was

446
00:16:46,630 --> 00:16:50,860
this new this new joint venture that we

447
00:16:48,370 --> 00:16:52,690
were getting into and it was April

448
00:16:50,860 --> 00:16:55,020
Fool's Day they did make some samples

449
00:16:52,690 --> 00:16:58,180
though I'm trying to get my hands on it

450
00:16:55,020 --> 00:17:02,079
alpha pilot Lockheed Martin has joined

451
00:16:58,180 --> 00:17:05,559
joined up with Nvidia and the drone

452
00:17:02,079 --> 00:17:10,000
racing league to create an autonomous

453
00:17:05,559 --> 00:17:11,470
drone race where I forget how many teams

454
00:17:10,000 --> 00:17:13,540
were chosen we're in the third I think

455
00:17:11,470 --> 00:17:14,709
third round of down selects for that for

456
00:17:13,540 --> 00:17:17,379
the year there'll be a race at the end

457
00:17:14,709 --> 00:17:19,449
of the year autonomous race the Machine

458
00:17:17,380 --> 00:17:22,270
the drone itself will learn about the

459
00:17:19,449 --> 00:17:23,439
course as it flies it and will compete

460
00:17:22,270 --> 00:17:24,699
against other drones that are learning

461
00:17:23,439 --> 00:17:25,689
it and each of the teams has different

462
00:17:24,699 --> 00:17:26,980
algorithms they all get the same

463
00:17:25,689 --> 00:17:28,929
equipment is this equipment right here

464
00:17:26,980 --> 00:17:33,910
they get the exact same equipment exact

465
00:17:28,929 --> 00:17:36,820
same platform it's an Nvidia shoot I

466
00:17:33,910 --> 00:17:39,700
think it's a xaviar platform that

467
00:17:36,820 --> 00:17:43,030
they'll be using for for their

468
00:17:39,700 --> 00:17:43,299
programming purposes but that's that's

469
00:17:43,030 --> 00:17:44,739
going to

470
00:17:43,299 --> 00:17:46,749
happened later this year there's a 1

471
00:17:44,739 --> 00:17:50,529
million dollar prize to the winning the

472
00:17:46,749 --> 00:17:52,600
winning racer or race team and the first

473
00:17:50,529 --> 00:17:55,779
autonomous the very first autonomous

474
00:17:52,600 --> 00:17:57,908
drone that defeats a human pilot in a

475
00:17:55,779 --> 00:18:00,190
drone race actually gets another quarter

476
00:17:57,909 --> 00:18:02,049
of a million dollar bonus so it'll be

477
00:18:00,190 --> 00:18:03,340
something interesting the drone racing

478
00:18:02,049 --> 00:18:04,869
league is going to be televising it I

479
00:18:03,340 --> 00:18:07,809
believe that's on like the fifth or

480
00:18:04,869 --> 00:18:10,749
sixth or eighth ESPN channel that's out

481
00:18:07,809 --> 00:18:12,309
there but if you look at Lockheed

482
00:18:10,749 --> 00:18:15,190
website we'll probably have some sort of

483
00:18:12,309 --> 00:18:16,480
announcement about it this is actually

484
00:18:15,190 --> 00:18:19,950
an older technology this is called the

485
00:18:16,480 --> 00:18:22,899
mule mules designed to follow around

486
00:18:19,950 --> 00:18:25,419
combat troops and carry their bags form

487
00:18:22,899 --> 00:18:28,299
this is autonomous it's designed to

488
00:18:25,419 --> 00:18:29,919
actually focus in with them and they

489
00:18:28,299 --> 00:18:30,908
load it up and it follows them around as

490
00:18:29,919 --> 00:18:32,350
they go it's not necessarily

491
00:18:30,909 --> 00:18:34,330
pre-programmed was any particular route

492
00:18:32,350 --> 00:18:37,330
it's just an autonomous vehicle it's

493
00:18:34,330 --> 00:18:40,090
designed to help them with that this is

494
00:18:37,330 --> 00:18:42,279
one of our new one of our newer launches

495
00:18:40,090 --> 00:18:43,840
it's one of the Sikorsky had been

496
00:18:42,279 --> 00:18:48,720
working on for a while they actually

497
00:18:43,840 --> 00:18:51,449
showed an a helicopter drone land on a

498
00:18:48,720 --> 00:18:54,340
identify and land on a cargo ship

499
00:18:51,450 --> 00:18:56,529
actually was a forget what it was but a

500
00:18:54,340 --> 00:18:57,970
cargo ship out in a river from there

501
00:18:56,529 --> 00:19:01,269
they've actually now developed an a

502
00:18:57,970 --> 00:19:03,460
truly awe taught and aircraft unmanned

503
00:19:01,269 --> 00:19:05,769
helicopter and they've started working

504
00:19:03,460 --> 00:19:07,179
with unmanned unmanned teaming with

505
00:19:05,769 --> 00:19:09,220
different purposes so I've got a

506
00:19:07,179 --> 00:19:10,899
helicopter and I've got a drone and

507
00:19:09,220 --> 00:19:11,980
those two are able to communicate with

508
00:19:10,899 --> 00:19:15,070
each other and they're able to do

509
00:19:11,980 --> 00:19:16,509
conduct missions together and then go

510
00:19:15,070 --> 00:19:19,269
off and separately and then come back

511
00:19:16,509 --> 00:19:20,919
together it's a very fascinating time as

512
00:19:19,269 --> 00:19:22,590
part of one of the things that we've

513
00:19:20,919 --> 00:19:25,809
been looking at in addition to manned

514
00:19:22,590 --> 00:19:27,309
and unmanned collaboration unmanned

515
00:19:25,809 --> 00:19:30,129
unmanned collaboration of different

516
00:19:27,309 --> 00:19:31,928
types and this is actually one of the

517
00:19:30,129 --> 00:19:34,408
gets to my unmanned unmanned

518
00:19:31,929 --> 00:19:39,009
collaboration as well the ability for

519
00:19:34,409 --> 00:19:42,369
for drones to be unmanned unmanned

520
00:19:39,009 --> 00:19:43,929
wingmen for pilots take the number of

521
00:19:42,369 --> 00:19:46,178
pilots down that we have in the air that

522
00:19:43,929 --> 00:19:47,679
are at risk and actually bring in drones

523
00:19:46,179 --> 00:19:48,999
that can actually work with them to

524
00:19:47,679 --> 00:19:50,619
achieve different types of missions and

525
00:19:48,999 --> 00:19:53,649
have different types of drones that are

526
00:19:50,619 --> 00:19:54,959
able to assimilate different types of

527
00:19:53,649 --> 00:19:56,580
missions as they come through

528
00:19:54,960 --> 00:19:59,500
dynamically

529
00:19:56,580 --> 00:20:02,080
so with that I think I've given us about

530
00:19:59,500 --> 00:20:06,370
what five minutes or ten minutes for

531
00:20:02,080 --> 00:20:07,570
questions I've lost Joel so I'll end

532
00:20:06,370 --> 00:20:10,510
with a picture that's my favorite

533
00:20:07,570 --> 00:20:13,389
airplane I'll end with that yeah take

534
00:20:10,510 --> 00:20:21,760
any questions that you might have just a

535
00:20:13,390 --> 00:20:23,320
minute perfect so it answered all your

536
00:20:21,760 --> 00:20:25,720
questions about what Lockheed is doing

537
00:20:23,320 --> 00:20:28,389
in this AI space that's perfect yes sure

538
00:20:25,720 --> 00:20:30,100
I'm starting to read a little bit now

539
00:20:28,390 --> 00:20:32,520
about army futures command and their

540
00:20:30,100 --> 00:20:36,189
experimentation with multi-domain

541
00:20:32,520 --> 00:20:38,800
operations and so you know for example

542
00:20:36,190 --> 00:20:42,340
an artillery battalion on land can now

543
00:20:38,800 --> 00:20:45,419
see a ship that's ten miles offshore and

544
00:20:42,340 --> 00:20:50,770
that goes into your point about that

545
00:20:45,420 --> 00:20:52,780
multi is multi domain integration but

546
00:20:50,770 --> 00:20:55,629
will it I mean will it get to the point

547
00:20:52,780 --> 00:20:57,520
where the operator at the earth at in

548
00:20:55,630 --> 00:21:01,390
the artillery piece or the artillery

549
00:20:57,520 --> 00:21:03,940
missile just can't just can't handle all

550
00:21:01,390 --> 00:21:05,620
of the information they're seeing that

551
00:21:03,940 --> 00:21:06,700
it's a great question and it's one of

552
00:21:05,620 --> 00:21:07,959
the ones that we brought up as part of

553
00:21:06,700 --> 00:21:10,750
the hard problems and that gets back to

554
00:21:07,960 --> 00:21:13,060
that mission unique or mission mission

555
00:21:10,750 --> 00:21:14,500
specific situational awareness that

556
00:21:13,060 --> 00:21:16,990
we're looking at one of the problems

557
00:21:14,500 --> 00:21:18,040
with multi domain operations among many

558
00:21:16,990 --> 00:21:19,810
of the problems with multi domain

559
00:21:18,040 --> 00:21:21,210
operations is how do you keep from

560
00:21:19,810 --> 00:21:23,919
flooding the human with too much data

561
00:21:21,210 --> 00:21:27,820
how do you provide them information that

562
00:21:23,920 --> 00:21:31,090
is a what they need but also with an

563
00:21:27,820 --> 00:21:32,830
understanding of what with what their

564
00:21:31,090 --> 00:21:35,169
options are to a certain to a certain

565
00:21:32,830 --> 00:21:36,280
point we've got actually several

566
00:21:35,170 --> 00:21:38,200
different groups that are working on

567
00:21:36,280 --> 00:21:40,540
that we have a large focus on that

568
00:21:38,200 --> 00:21:42,580
common operating picture frankly just to

569
00:21:40,540 --> 00:21:44,260
see what that should look like how can

570
00:21:42,580 --> 00:21:46,120
you make a user find common operating

571
00:21:44,260 --> 00:21:48,760
picture that would be specific to be

572
00:21:46,120 --> 00:21:49,989
good for that going beyond that as we

573
00:21:48,760 --> 00:21:51,190
get into more the cognitive

574
00:21:49,990 --> 00:21:53,290
architectures one of the things we're

575
00:21:51,190 --> 00:21:56,470
looking at is how do we provide that

576
00:21:53,290 --> 00:21:58,720
same operator not just with that

577
00:21:56,470 --> 00:22:00,790
information but information us to hear

578
00:21:58,720 --> 00:22:02,800
recommendations and here's our here's

579
00:22:00,790 --> 00:22:05,800
our our confidence level and what those

580
00:22:02,800 --> 00:22:07,480
recommendations are and give them more

581
00:22:05,800 --> 00:22:09,780
that kind of thing one of the one of the

582
00:22:07,480 --> 00:22:12,330
things was brought up was ways

583
00:22:09,780 --> 00:22:13,830
waise is really really good for doing

584
00:22:12,330 --> 00:22:15,090
several different things right I mean

585
00:22:13,830 --> 00:22:17,070
how many people here are using it

586
00:22:15,090 --> 00:22:19,470
actively I use it because a it tells me

587
00:22:17,070 --> 00:22:22,139
where police are for where red light

588
00:22:19,470 --> 00:22:25,200
cameras are something that drives my

589
00:22:22,140 --> 00:22:26,670
wife nuts and then I use it because if

590
00:22:25,200 --> 00:22:29,520
something happens it'll dynamically

591
00:22:26,670 --> 00:22:32,280
reroute me well we need that same

592
00:22:29,520 --> 00:22:34,800
capability which already exists we need

593
00:22:32,280 --> 00:22:36,120
the same kind of capability in in the

594
00:22:34,800 --> 00:22:37,710
defense area to help with the similar

595
00:22:36,120 --> 00:22:40,500
similar kinds of things of being able to

596
00:22:37,710 --> 00:22:42,510
say to that same group okay here's the

597
00:22:40,500 --> 00:22:44,040
ship at the ship's action starting to go

598
00:22:42,510 --> 00:22:45,780
to this path so these options are going

599
00:22:44,040 --> 00:22:47,430
to disappear on you we now recommend

600
00:22:45,780 --> 00:22:50,970
that these recommendations move up with

601
00:22:47,430 --> 00:22:52,730
a certain level of confidence thank you

602
00:22:50,970 --> 00:22:55,650
sir

603
00:22:52,730 --> 00:22:57,990
can you comment at all on steps that

604
00:22:55,650 --> 00:23:03,030
Lockheed is taking to secure the AI part

605
00:22:57,990 --> 00:23:05,310
of the autonomous systems I came to a

606
00:23:03,030 --> 00:23:08,190
certain extent and what I can say is

607
00:23:05,310 --> 00:23:13,139
that we're looking into well number one

608
00:23:08,190 --> 00:23:15,540
we're looking at how how we go about

609
00:23:13,140 --> 00:23:17,160
sharing and how we go about sharing and

610
00:23:15,540 --> 00:23:18,720
or not sharing some of the information

611
00:23:17,160 --> 00:23:22,920
from the different types of systems so

612
00:23:18,720 --> 00:23:25,980
if you've got and I again I'm not an AI

613
00:23:22,920 --> 00:23:28,860
expert but if you've got neural networks

614
00:23:25,980 --> 00:23:30,030
doing one sort of processing and you've

615
00:23:28,860 --> 00:23:31,409
got machine learning that's been doing

616
00:23:30,030 --> 00:23:33,330
another set of learning and those types

617
00:23:31,410 --> 00:23:37,050
of things we're using we're now looking

618
00:23:33,330 --> 00:23:38,490
at ways that those that the training for

619
00:23:37,050 --> 00:23:40,740
example the training systems can be used

620
00:23:38,490 --> 00:23:42,330
to poison the training set so the

621
00:23:40,740 --> 00:23:44,180
training set now becomes skewed we're

622
00:23:42,330 --> 00:23:47,159
looking at ways to turn around and

623
00:23:44,180 --> 00:23:48,990
validate that or ways to take it's

624
00:23:47,160 --> 00:23:51,420
actually a lot easier in some cases

625
00:23:48,990 --> 00:23:53,190
because we know what the we know what

626
00:23:51,420 --> 00:23:55,440
the parameters for the operation of a

627
00:23:53,190 --> 00:23:56,700
certain system should be so we know that

628
00:23:55,440 --> 00:23:58,080
maybe there's only a certain set of

629
00:23:56,700 --> 00:23:59,640
commands or certain a set of traffic

630
00:23:58,080 --> 00:24:03,149
that should be going over this type or

631
00:23:59,640 --> 00:24:04,620
over this this data link and so we're

632
00:24:03,150 --> 00:24:05,940
able to profile that much easier and

633
00:24:04,620 --> 00:24:07,169
create training sets out of that so

634
00:24:05,940 --> 00:24:10,110
we're doing a lot of those types of

635
00:24:07,170 --> 00:24:12,210
things it is still a very very big open

636
00:24:10,110 --> 00:24:13,590
issue from especially the training side

637
00:24:12,210 --> 00:24:14,970
how do you trust your training data

638
00:24:13,590 --> 00:24:17,399
quite how do you trust it you're not

639
00:24:14,970 --> 00:24:21,520
training for the test to go ahead and

640
00:24:17,400 --> 00:24:22,570
you know and improve the year at 90%

641
00:24:21,520 --> 00:24:24,340
so that's one of the things that we're

642
00:24:22,570 --> 00:24:26,860
trying to do on it there's a lot of

643
00:24:24,340 --> 00:24:30,730
other activities in in our space systems

644
00:24:26,860 --> 00:24:32,949
company in particular and and I know in

645
00:24:30,730 --> 00:24:35,590
our missiles and fire command our fire

646
00:24:32,950 --> 00:24:37,480
control business area also working at

647
00:24:35,590 --> 00:24:41,439
that space we have a lot of a lot of

648
00:24:37,480 --> 00:24:43,030
work on how do we go back in and bring

649
00:24:41,440 --> 00:24:45,100
that and now second question that kind

650
00:24:43,030 --> 00:24:48,460
of comes out of that is how do we know

651
00:24:45,100 --> 00:24:50,050
that we are how do we know that we're

652
00:24:48,460 --> 00:24:51,550
actually on the right path of developing

653
00:24:50,050 --> 00:24:54,730
things and so we're doing a lot of

654
00:24:51,550 --> 00:24:56,260
really early-stage R&D with we've opened

655
00:24:54,730 --> 00:24:57,280
up a new AI and machine learning

656
00:24:56,260 --> 00:24:58,990
laboratory

657
00:24:57,280 --> 00:25:00,790
that's a virtual laboratory extends all

658
00:24:58,990 --> 00:25:03,460
the way across Lockheed Martin that's

659
00:25:00,790 --> 00:25:05,379
using that's basically being used right

660
00:25:03,460 --> 00:25:07,600
now for certain specific R&D projects

661
00:25:05,380 --> 00:25:08,950
just to help us learn better what types

662
00:25:07,600 --> 00:25:10,360
of techniques are better for which types

663
00:25:08,950 --> 00:25:12,760
and now look at trying to solve

664
00:25:10,360 --> 00:25:16,629
everything with one type of learning

665
00:25:12,760 --> 00:25:18,400
system thank you sir so this is a

666
00:25:16,630 --> 00:25:20,170
question from a very academic standpoint

667
00:25:18,400 --> 00:25:22,060
is in the academic world there's a lot

668
00:25:20,170 --> 00:25:24,370
of work going on in securing machine

669
00:25:22,060 --> 00:25:26,470
learning securing eia techniques how

670
00:25:24,370 --> 00:25:29,139
much of this to UC is ready for adoption

671
00:25:26,470 --> 00:25:31,660
in your mission critical systems and how

672
00:25:29,140 --> 00:25:35,400
much do you have to develop straight off

673
00:25:31,660 --> 00:25:37,660
off from scratch is a great question

674
00:25:35,400 --> 00:25:39,520
that's one of the biggest challenges we

675
00:25:37,660 --> 00:25:40,960
run into and when frankly we don't just

676
00:25:39,520 --> 00:25:43,290
run into with university research we

677
00:25:40,960 --> 00:25:45,610
also run into with laboratory research

678
00:25:43,290 --> 00:25:47,530
with some of the National Labs a lot of

679
00:25:45,610 --> 00:25:49,810
the research that we get will take

680
00:25:47,530 --> 00:25:51,460
things to a level which works in a

681
00:25:49,810 --> 00:25:52,840
certain isolated type of environment

682
00:25:51,460 --> 00:25:55,810
that hasn't been used or spread out to

683
00:25:52,840 --> 00:25:57,760
the kind of size extent to type a size

684
00:25:55,810 --> 00:26:00,300
or the types of complexity that we run

685
00:25:57,760 --> 00:26:02,740
into and so there generally is a lot of

686
00:26:00,300 --> 00:26:03,820
even if we've got notional use cases

687
00:26:02,740 --> 00:26:05,590
that we use when we're working

688
00:26:03,820 --> 00:26:06,909
collaborative projects with the with

689
00:26:05,590 --> 00:26:08,889
universities we're finding that it

690
00:26:06,910 --> 00:26:11,320
requires an awful lot of additional

691
00:26:08,890 --> 00:26:12,940
research after that fact internally to

692
00:26:11,320 --> 00:26:14,830
adapt it to a larger much larger system

693
00:26:12,940 --> 00:26:16,750
the principles are generally very very

694
00:26:14,830 --> 00:26:19,060
sound it's the how do we apply them to

695
00:26:16,750 --> 00:26:22,720
disparate weapon systems that frankly

696
00:26:19,060 --> 00:26:25,720
have been developed under under way

697
00:26:22,720 --> 00:26:27,790
different parameters a lot of it is a

698
00:26:25,720 --> 00:26:29,260
lot of it going forward is looking

699
00:26:27,790 --> 00:26:31,659
pretty good because we're able to kind

700
00:26:29,260 --> 00:26:33,550
of kind of normalize and take some of

701
00:26:31,660 --> 00:26:34,810
that and go forward very nicely but

702
00:26:33,550 --> 00:26:35,100
we've got an awful lot of equipment out

703
00:26:34,810 --> 00:26:37,860
there

704
00:26:35,100 --> 00:26:40,709
retrofit as well matter of fact you go

705
00:26:37,860 --> 00:26:42,659
back to some of some of the stuff that

706
00:26:40,710 --> 00:26:45,990
we have in in space has been around for

707
00:26:42,660 --> 00:26:47,640
decades and so we've got some issues

708
00:26:45,990 --> 00:26:50,580
there but it does take an awful lot

709
00:26:47,640 --> 00:26:52,289
generally I don't know I would I would

710
00:26:50,580 --> 00:26:53,340
almost look at Derek is here keeping me

711
00:26:52,289 --> 00:26:57,090
honest

712
00:26:53,340 --> 00:26:58,620
I would almost say we've it's at least a

713
00:26:57,090 --> 00:27:00,809
two-year effort for us to take something

714
00:26:58,620 --> 00:27:02,100
and then and that's a fast track if we

715
00:27:00,809 --> 00:27:04,730
were able to speed it now part of that

716
00:27:02,100 --> 00:27:08,070
speed of Lockheed

717
00:27:04,730 --> 00:27:14,070
thank you are there any other questions

718
00:27:08,070 --> 00:27:17,780
or comments why is that my favorite

719
00:27:14,070 --> 00:27:20,520
plane there's a lot of reasons for that

720
00:27:17,780 --> 00:27:22,500
part of it was when I was stationed at

721
00:27:20,520 --> 00:27:25,139
castle they were just they were they

722
00:27:22,500 --> 00:27:27,809
were just decommissioning the the sr-71s

723
00:27:25,140 --> 00:27:29,789
and we have one come in on to come in

724
00:27:27,809 --> 00:27:31,770
for static display and I'd always read

725
00:27:29,789 --> 00:27:35,669
about him and then seeing that thing in

726
00:27:31,770 --> 00:27:36,990
flight was just amazing now that's not

727
00:27:35,669 --> 00:27:39,240
to say that I don't like a lot of our

728
00:27:36,990 --> 00:27:41,429
other airplanes that are created both by

729
00:27:39,240 --> 00:27:43,650
Lockheed Boeing and Northrop I mean

730
00:27:41,429 --> 00:27:46,409
there's there's a lot of one's out there

731
00:27:43,650 --> 00:27:48,480
but this has been my favorite and the

732
00:27:46,409 --> 00:27:51,809
fact that we were in we were a kc-135

733
00:27:48,480 --> 00:27:54,539
base and kc-135 held the land the speed

734
00:27:51,809 --> 00:27:58,049
record Los Angeles to DC or Los Angeles

735
00:27:54,539 --> 00:28:00,179
in New York until the sr-71 broke it and

736
00:27:58,049 --> 00:28:02,429
it had been like 2 hours and 17 minutes

737
00:28:00,179 --> 00:28:06,659
the sr-71 did it in 68 minutes

738
00:28:02,429 --> 00:28:07,799
the same the same route so nice airplane

739
00:28:06,659 --> 00:28:12,150
if you ever get a chance to see it out

740
00:28:07,799 --> 00:28:13,350
in DC there's at the advertised e all

741
00:28:12,150 --> 00:28:16,190
right well thank you very much for your

742
00:28:13,350 --> 00:28:16,189
time I appreciate it

743
00:28:18,789 --> 00:28:23,869
thank you very much Rob so before we all

744
00:28:21,739 --> 00:28:28,789
break because we will have a 15-minute

745
00:28:23,869 --> 00:28:32,959
break now here I want any student in the

746
00:28:28,789 --> 00:28:35,600
room who is looking for a job for this

747
00:28:32,960 --> 00:28:39,529
summer or next summer meeting career or

748
00:28:35,600 --> 00:28:41,809
internship to raise your hand raise it

749
00:28:39,529 --> 00:28:43,489
high okay those of you who keep saying

750
00:28:41,809 --> 00:28:45,470
we're looking to hire your students look

751
00:28:43,489 --> 00:28:48,080
around the room see those with their

752
00:28:45,470 --> 00:28:50,239
hands up they are potential new hires

753
00:28:48,080 --> 00:28:52,369
for you okay now

754
00:28:50,239 --> 00:28:54,409
students put your hands down anybody in

755
00:28:52,369 --> 00:28:56,238
the room who is with a commercial

756
00:28:54,409 --> 00:28:58,340
government agency or any type of

757
00:28:56,239 --> 00:29:01,479
organization other than a university if

758
00:28:58,340 --> 00:29:04,928
your organization is looking to hire

759
00:29:01,479 --> 00:29:07,700
cyber security cyber physical systems

760
00:29:04,929 --> 00:29:11,149
students are new hires please raise your

761
00:29:07,700 --> 00:29:12,679
hand all right students now put a

762
00:29:11,149 --> 00:29:15,199
beeline to those people during this

763
00:29:12,679 --> 00:29:18,700
break very good okay we'll reconvene in

764
00:29:15,200 --> 00:29:18,700
15 minutes for the next panel

765
00:29:24,730 --> 00:29:26,790
you

