1
00:00:12,480 --> 00:00:13,679
[Music]

2
00:00:13,679 --> 00:00:17,119
alright so hello everyone and welcome to

3
00:00:17,119 --> 00:00:18,960
this session i'm andrea and i will be

4
00:00:18,960 --> 00:00:21,520
your host today i'm glad to welcome

5
00:00:21,520 --> 00:00:23,199
stefan today who is an associate

6
00:00:23,199 --> 00:00:25,119
professor at a red bound university in

7
00:00:25,119 --> 00:00:27,199
the netherlands his research interests

8
00:00:27,199 --> 00:00:29,279
are cryptography machine learning and

9
00:00:29,279 --> 00:00:31,599
evolutionary computation

10
00:00:31,599 --> 00:00:33,280
researchers have used machine learning

11
00:00:33,280 --> 00:00:34,960
in side channel analysis for more than

12
00:00:34,960 --> 00:00:37,600
10 years and deep learning for six years

13
00:00:37,600 --> 00:00:39,280
we witnessed significant progress

14
00:00:39,280 --> 00:00:41,200
showing clear benefits of using such

15
00:00:41,200 --> 00:00:44,559
techniques and today we will talk uh we

16
00:00:44,559 --> 00:00:46,559
will cover the most important challenges

17
00:00:46,559 --> 00:00:48,399
and some possible directions on how to

18
00:00:48,399 --> 00:00:50,800
solve them additionally we will also

19
00:00:50,800 --> 00:00:52,480
briefly discuss the most

20
00:00:52,480 --> 00:00:54,879
successful stories and how they improve

21
00:00:54,879 --> 00:00:57,280
the state of the art the presentation

22
00:00:57,280 --> 00:00:59,520
will consist of 30 minutes followed by

23
00:00:59,520 --> 00:01:02,000
by a 10-minute q a session if you have

24
00:01:02,000 --> 00:01:03,760
any questions please share them in the

25
00:01:03,760 --> 00:01:05,840
chat box and we will answer after the

26
00:01:05,840 --> 00:01:08,640
presentation is over uh you can start

27
00:01:08,640 --> 00:01:10,479
your presentation

28
00:01:10,479 --> 00:01:12,880
so thank you uh thank you very much for

29
00:01:12,880 --> 00:01:15,360
in the invitation and yeah the

30
00:01:15,360 --> 00:01:17,920
opportunity to talk a bit more about

31
00:01:17,920 --> 00:01:19,439
deep learning based side channel

32
00:01:19,439 --> 00:01:21,759
analysis and what are the challenges

33
00:01:21,759 --> 00:01:25,360
what are the perspectives we have so um

34
00:01:25,360 --> 00:01:28,159
i will i will start with uh with let's

35
00:01:28,159 --> 00:01:31,360
say basics where i will just give you a

36
00:01:31,360 --> 00:01:33,439
quick introduction to side channel

37
00:01:33,439 --> 00:01:35,200
attacks why do we care about deep

38
00:01:35,200 --> 00:01:37,680
learning for site channel analysis then

39
00:01:37,680 --> 00:01:39,680
i will show you a couple of

40
00:01:39,680 --> 00:01:41,759
recent results that

41
00:01:41,759 --> 00:01:42,720
that

42
00:01:42,720 --> 00:01:45,840
my group did that i feel are interesting

43
00:01:45,840 --> 00:01:47,840
something in chat

44
00:01:47,840 --> 00:01:51,200
okay and then i will finish with

45
00:01:51,200 --> 00:01:53,680
enumerating a number of challenges and

46
00:01:53,680 --> 00:01:57,040
there are many more challenges but i i

47
00:01:57,040 --> 00:01:58,079
let's say

48
00:01:58,079 --> 00:02:00,560
i said 10 is enough for this for this

49
00:02:00,560 --> 00:02:02,159
short talk and then there will be

50
00:02:02,159 --> 00:02:04,880
opportunity for us also to discuss why

51
00:02:04,880 --> 00:02:06,799
are those challenges important and what

52
00:02:06,799 --> 00:02:09,360
they actually mean to progress deep

53
00:02:09,360 --> 00:02:12,319
learning based sideshow analysis

54
00:02:12,319 --> 00:02:15,520
so uh outline of the talk profiling side

55
00:02:15,520 --> 00:02:18,239
channel 10 challenges and very short

56
00:02:18,239 --> 00:02:20,160
conclusions

57
00:02:20,160 --> 00:02:22,319
when we talk about side channel attacks

58
00:02:22,319 --> 00:02:24,400
so such an attacks are type of

59
00:02:24,400 --> 00:02:27,280
implementation attacks as such they do

60
00:02:27,280 --> 00:02:29,760
not aim at the weakness of the algorithm

61
00:02:29,760 --> 00:02:31,519
but on the implementation of the

62
00:02:31,519 --> 00:02:34,319
algorithm and side channel attacks are

63
00:02:34,319 --> 00:02:37,760
passive non-invasive attacks and today

64
00:02:37,760 --> 00:02:39,040
side channel

65
00:02:39,040 --> 00:02:41,280
attacks represent one of the most

66
00:02:41,280 --> 00:02:43,440
powerful categories of attacks on crypto

67
00:02:43,440 --> 00:02:44,959
devices

68
00:02:44,959 --> 00:02:47,519
in a similar way like we state this

69
00:02:47,519 --> 00:02:50,400
profiling attacks which is one category

70
00:02:50,400 --> 00:02:53,200
of side channel attacks represent

71
00:02:53,200 --> 00:02:55,200
probably the most powerful option for

72
00:02:55,200 --> 00:02:57,519
site channel analysis so if we compare

73
00:02:57,519 --> 00:02:58,239
with

74
00:02:58,239 --> 00:03:01,120
direct attacks like cpa

75
00:03:01,120 --> 00:03:04,080
profiling attacks have little bit more

76
00:03:04,080 --> 00:03:06,080
restriction from the perspective of the

77
00:03:06,080 --> 00:03:08,959
assumptions so profiling attacks assume

78
00:03:08,959 --> 00:03:11,120
the attacker has access to a copy of the

79
00:03:11,120 --> 00:03:15,200
device but if the attacker has it

80
00:03:15,200 --> 00:03:17,440
the option to make the attack becomes

81
00:03:17,440 --> 00:03:19,280
much more powerful

82
00:03:19,280 --> 00:03:21,840
and in the similar way like

83
00:03:21,840 --> 00:03:24,159
profiling attacks are the most uh

84
00:03:24,159 --> 00:03:27,519
powerful ones of sight general attacks

85
00:03:27,519 --> 00:03:28,799
deep learning

86
00:03:28,799 --> 00:03:30,879
represents today the most powerful

87
00:03:30,879 --> 00:03:33,200
option for profiling side channel of

88
00:03:33,200 --> 00:03:36,480
course this could be a little bit um

89
00:03:36,480 --> 00:03:40,319
controversial uh claim because

90
00:03:40,319 --> 00:03:42,560
let's say more traditional profiling

91
00:03:42,560 --> 00:03:44,720
attacks like template attack they are

92
00:03:44,720 --> 00:03:47,519
also very well established in industry

93
00:03:47,519 --> 00:03:51,040
in foreign certification labs while deep

94
00:03:51,040 --> 00:03:54,720
learning is used of course but still i

95
00:03:54,720 --> 00:03:58,080
would dare to say represent something

96
00:03:58,080 --> 00:03:59,920
much more

97
00:03:59,920 --> 00:04:01,280
interesting from the academic

98
00:04:01,280 --> 00:04:02,720
perspective so

99
00:04:02,720 --> 00:04:03,760
we as

100
00:04:03,760 --> 00:04:06,080
researchers are convinced deep learning

101
00:04:06,080 --> 00:04:08,720
attacks work and then they allow us to

102
00:04:08,720 --> 00:04:11,200
make different targets very efficiently

103
00:04:11,200 --> 00:04:14,400
but at the same time

104
00:04:14,400 --> 00:04:15,760
industry

105
00:04:15,760 --> 00:04:18,798
whether they their results follow

106
00:04:18,798 --> 00:04:21,279
the results from academia this is a

107
00:04:21,279 --> 00:04:23,759
little bit more questionable and i will

108
00:04:23,759 --> 00:04:27,600
discuss later what are possible doubts

109
00:04:27,600 --> 00:04:30,400
for what i'm saying

110
00:04:30,400 --> 00:04:31,840
okay

111
00:04:31,840 --> 00:04:34,800
so what are the profiling attacks

112
00:04:34,800 --> 00:04:37,520
well profiling attacks are two-stage

113
00:04:37,520 --> 00:04:40,320
attacks so they consist of two stages

114
00:04:40,320 --> 00:04:43,120
the first stage is called profaning

115
00:04:43,120 --> 00:04:45,280
and in that stage the adversary the

116
00:04:45,280 --> 00:04:46,400
attacker

117
00:04:46,400 --> 00:04:49,280
estimates leakage models for targeted

118
00:04:49,280 --> 00:04:50,560
computation

119
00:04:50,560 --> 00:04:53,120
and then in the second phase commonly

120
00:04:53,120 --> 00:04:55,360
known as the attack phase

121
00:04:55,360 --> 00:04:58,240
that model is used to extract secret

122
00:04:58,240 --> 00:05:00,080
information

123
00:05:00,080 --> 00:05:02,000
and then when we talk about profiling

124
00:05:02,000 --> 00:05:04,880
attacks a common example is the template

125
00:05:04,880 --> 00:05:06,560
attack

126
00:05:06,560 --> 00:05:09,520
presented in 2002 so already 20 years of

127
00:05:09,520 --> 00:05:13,120
the attack and it's very powerful

128
00:05:13,120 --> 00:05:14,240
but

129
00:05:14,240 --> 00:05:16,639
it's also the most powerful attack from

130
00:05:16,639 --> 00:05:17,919
the information

131
00:05:17,919 --> 00:05:20,320
point of view but really to reach that

132
00:05:20,320 --> 00:05:22,800
power of template attack some

133
00:05:22,800 --> 00:05:25,280
assumptions need to be fulfilled

134
00:05:25,280 --> 00:05:26,960
since in reality

135
00:05:26,960 --> 00:05:28,880
we do not have always that kind of

136
00:05:28,880 --> 00:05:31,120
assumptions fulfilled

137
00:05:31,120 --> 00:05:32,720
template attack

138
00:05:32,720 --> 00:05:35,759
often let's say loses on its power and

139
00:05:35,759 --> 00:05:38,000
we can think of other techniques that

140
00:05:38,000 --> 00:05:38,960
need

141
00:05:38,960 --> 00:05:41,360
possibly less assumptions that can

142
00:05:41,360 --> 00:05:43,600
provide a strong

143
00:05:43,600 --> 00:05:46,880
alternative and one group of techniques

144
00:05:46,880 --> 00:05:49,759
is machine learning deeply

145
00:05:49,759 --> 00:05:51,520
well immediately from the description

146
00:05:51,520 --> 00:05:53,919
when i said well uh profiling attacks

147
00:05:53,919 --> 00:05:56,000
consists of two phases so we have the

148
00:05:56,000 --> 00:05:57,919
profiling phase and we have the attack

149
00:05:57,919 --> 00:06:00,240
phase one could say okay this sounds

150
00:06:00,240 --> 00:06:02,400
very similar to supervised learning in

151
00:06:02,400 --> 00:06:05,199
machine learning and this is indeed true

152
00:06:05,199 --> 00:06:08,319
so what we call profiling is basically

153
00:06:08,319 --> 00:06:10,639
what we machine learning called training

154
00:06:10,639 --> 00:06:14,000
and what we call uh attacking is what we

155
00:06:14,000 --> 00:06:16,080
call machine learning test

156
00:06:16,080 --> 00:06:18,880
so indeed there is a natural connection

157
00:06:18,880 --> 00:06:21,440
between profiling attacks and supervised

158
00:06:21,440 --> 00:06:22,800
machine learning

159
00:06:22,800 --> 00:06:25,600
and that gives us the option to use all

160
00:06:25,600 --> 00:06:28,080
those many algorithms developed over

161
00:06:28,080 --> 00:06:30,720
decades from machine learning domain to

162
00:06:30,720 --> 00:06:32,000
actually mount

163
00:06:32,000 --> 00:06:35,600
very powerful side channel attacks

164
00:06:35,600 --> 00:06:38,000
that being said of course one could say

165
00:06:38,000 --> 00:06:41,280
yes but do we have that copy of a device

166
00:06:41,280 --> 00:06:44,160
we require for profiling phase

167
00:06:44,160 --> 00:06:46,400
uh this is a common assumption to be

168
00:06:46,400 --> 00:06:49,199
done of course attack like that is more

169
00:06:49,199 --> 00:06:51,680
difficult than direct attack

170
00:06:51,680 --> 00:06:54,800
but if we do manage to have a copy of

171
00:06:54,800 --> 00:06:56,960
the device and if we do manage to build

172
00:06:56,960 --> 00:06:58,319
a good model

173
00:06:58,319 --> 00:06:59,199
then

174
00:06:59,199 --> 00:07:02,240
our attack performance can be extremely

175
00:07:02,240 --> 00:07:04,080
good

176
00:07:04,080 --> 00:07:06,960
so this is a general scheme how prof all

177
00:07:06,960 --> 00:07:09,280
the profiling attacks would work so we

178
00:07:09,280 --> 00:07:13,199
have two devices device under attack and

179
00:07:13,199 --> 00:07:16,560
identical copy for that device then we

180
00:07:16,560 --> 00:07:20,080
have our pc to build the profiling model

181
00:07:20,080 --> 00:07:21,840
for instance our machine learning or

182
00:07:21,840 --> 00:07:23,039
template

183
00:07:23,039 --> 00:07:25,039
and then we feed many different

184
00:07:25,039 --> 00:07:27,440
configurations of plain text keys

185
00:07:27,440 --> 00:07:29,759
to identical copy we observe the

186
00:07:29,759 --> 00:07:32,639
measurements and we build a model of a

187
00:07:32,639 --> 00:07:37,360
device once we have a model we attack

188
00:07:37,360 --> 00:07:40,080
the device under attack where we have

189
00:07:40,080 --> 00:07:42,720
again the plain text info but also we

190
00:07:42,720 --> 00:07:44,879
have our model we again observe the

191
00:07:44,879 --> 00:07:47,759
measurements based on that information

192
00:07:47,759 --> 00:07:49,199
we guess the key

193
00:07:49,199 --> 00:07:51,840
the better the model the better the kick

194
00:07:51,840 --> 00:07:53,919
is

195
00:07:53,919 --> 00:07:57,440
so going a little bit more um

196
00:07:57,440 --> 00:07:59,840
more formal for deep learning we could

197
00:07:59,840 --> 00:08:02,639
also uh devise this deep learning based

198
00:08:02,639 --> 00:08:06,160
side channel in a setup like this so

199
00:08:06,160 --> 00:08:08,160
we would start with oscilloscope with

200
00:08:08,160 --> 00:08:09,280
the device

201
00:08:09,280 --> 00:08:11,440
uh that we want to attack be able to

202
00:08:11,440 --> 00:08:13,440
obtain the raw measurements and

203
00:08:13,440 --> 00:08:16,720
basically this is let's say engineering

204
00:08:16,720 --> 00:08:18,160
phase there is

205
00:08:18,160 --> 00:08:20,080
not so much

206
00:08:20,080 --> 00:08:22,240
differentiation what you can do i mean

207
00:08:22,240 --> 00:08:24,639
it's well-known procedure how to do the

208
00:08:24,639 --> 00:08:27,520
data acquisition and this is standard

209
00:08:27,520 --> 00:08:31,440
and in academia mostly this step is even

210
00:08:31,440 --> 00:08:33,839
not taken but researchers just take the

211
00:08:33,839 --> 00:08:36,320
data sets that are publicly available

212
00:08:36,320 --> 00:08:38,240
then we come to a step of data

213
00:08:38,240 --> 00:08:40,799
pre-processing where we would clean

214
00:08:40,799 --> 00:08:42,559
noise remove

215
00:08:42,559 --> 00:08:45,440
uncomplete measurements and so on

216
00:08:45,440 --> 00:08:47,440
then we would come to the phase of

217
00:08:47,440 --> 00:08:49,120
feature engineering

218
00:08:49,120 --> 00:08:51,120
where for simpler techniques for

219
00:08:51,120 --> 00:08:53,200
instance we would

220
00:08:53,200 --> 00:08:55,440
find the most informative features that

221
00:08:55,440 --> 00:08:56,880
we wanna use

222
00:08:56,880 --> 00:08:59,040
while for deep learning we would

223
00:08:59,040 --> 00:09:01,440
hopefully work with as many features as

224
00:09:01,440 --> 00:09:03,839
possible and leave it for deep learning

225
00:09:03,839 --> 00:09:06,480
to find what are the important features

226
00:09:06,480 --> 00:09:08,560
next step we need to select the

227
00:09:08,560 --> 00:09:10,640
algorithm so

228
00:09:10,640 --> 00:09:12,560
in deep learning context what kind of

229
00:09:12,560 --> 00:09:14,399
neural network do you want to use for

230
00:09:14,399 --> 00:09:16,080
instance do you want to use multi-layer

231
00:09:16,080 --> 00:09:18,160
perceptrons or convolutional neural

232
00:09:18,160 --> 00:09:20,959
networks as the two most common examples

233
00:09:20,959 --> 00:09:24,640
today after we select the algorithm we

234
00:09:24,640 --> 00:09:27,360
select all the hyper parameters

235
00:09:27,360 --> 00:09:29,360
we reach the model training phase where

236
00:09:29,360 --> 00:09:32,240
we are building our model

237
00:09:32,240 --> 00:09:34,800
once we have our model we are ready to

238
00:09:34,800 --> 00:09:37,279
do the attack evaluation where we use

239
00:09:37,279 --> 00:09:39,519
the model and we

240
00:09:39,519 --> 00:09:43,200
guess the key and based on on the

241
00:09:43,200 --> 00:09:45,839
how well did we get the key we estimate

242
00:09:45,839 --> 00:09:48,240
the performance with some of the side

243
00:09:48,240 --> 00:09:51,279
channel metrics like kiran like guessing

244
00:09:51,279 --> 00:09:54,480
entropy like success rate

245
00:09:54,480 --> 00:09:56,880
okay so from from this

246
00:09:56,880 --> 00:09:59,760
number of steps from a to f the question

247
00:09:59,760 --> 00:10:03,120
we can immediately try to understand is

248
00:10:03,120 --> 00:10:04,640
what phases

249
00:10:04,640 --> 00:10:06,720
are the most important

250
00:10:06,720 --> 00:10:07,600
um

251
00:10:07,600 --> 00:10:08,720
form

252
00:10:08,720 --> 00:10:10,320
for deep learning based functional

253
00:10:10,320 --> 00:10:13,040
analysis well what phases are

254
00:10:13,040 --> 00:10:14,079
let's say

255
00:10:14,079 --> 00:10:17,440
less relevant or less explored but up to

256
00:10:17,440 --> 00:10:20,800
the moment so first of all raw data yeah

257
00:10:20,800 --> 00:10:22,560
like i already said there is almost

258
00:10:22,560 --> 00:10:25,279
nothing there so we collect the data and

259
00:10:25,279 --> 00:10:29,120
that is all so this is nothing specific

260
00:10:29,120 --> 00:10:31,600
data pre-processing is currently

261
00:10:31,600 --> 00:10:35,600
something that is very much neglected in

262
00:10:35,600 --> 00:10:36,959
machine learning deep learning

263
00:10:36,959 --> 00:10:39,760
perspective where there are only few

264
00:10:39,760 --> 00:10:42,640
works where for instance people would do

265
00:10:42,640 --> 00:10:45,920
data augmentation so maybe build more

266
00:10:45,920 --> 00:10:47,760
more measurements synthetic measurements

267
00:10:47,760 --> 00:10:50,560
to increase the performance or maybe

268
00:10:50,560 --> 00:10:52,959
remove the noise remove the counter

269
00:10:52,959 --> 00:10:55,120
measures but all in all

270
00:10:55,120 --> 00:10:59,519
this phase is very much neglected

271
00:10:59,519 --> 00:11:02,240
next is feature engineering like already

272
00:11:02,240 --> 00:11:03,279
said

273
00:11:03,279 --> 00:11:05,200
in this phase we would select the most

274
00:11:05,200 --> 00:11:07,519
important features or

275
00:11:07,519 --> 00:11:09,360
we would leave it for neural network to

276
00:11:09,360 --> 00:11:11,839
decide what are the important features

277
00:11:11,839 --> 00:11:14,240
currently

278
00:11:14,240 --> 00:11:16,640
it's less clear how important is the

279
00:11:16,640 --> 00:11:19,760
future engineering simple reason is this

280
00:11:19,760 --> 00:11:21,680
at the moment when we started doing deep

281
00:11:21,680 --> 00:11:23,839
learning immediately we started with the

282
00:11:23,839 --> 00:11:26,320
arguments deep learning can do implicit

283
00:11:26,320 --> 00:11:30,640
feature selection as such we do not need

284
00:11:30,640 --> 00:11:33,680
but interestingly despite those

285
00:11:33,680 --> 00:11:35,120
sentences

286
00:11:35,120 --> 00:11:36,959
people commonly

287
00:11:36,959 --> 00:11:38,959
do

288
00:11:38,959 --> 00:11:40,640
let's say feature engineering in the

289
00:11:40,640 --> 00:11:42,959
sense that they select a window of

290
00:11:42,959 --> 00:11:45,600
features they consider import

291
00:11:45,600 --> 00:11:48,320
so while we commonly say yes we do not

292
00:11:48,320 --> 00:11:50,320
need feature engineering i always find

293
00:11:50,320 --> 00:11:51,839
it interesting but

294
00:11:51,839 --> 00:11:55,839
i believe 90 of papers still has been

295
00:11:55,839 --> 00:11:58,079
challenging

296
00:11:58,079 --> 00:12:00,560
then algorithm selection model training

297
00:12:00,560 --> 00:12:01,760
there is

298
00:12:01,760 --> 00:12:04,320
most of the works consider this so what

299
00:12:04,320 --> 00:12:06,399
is the good architecture what are the

300
00:12:06,399 --> 00:12:09,040
important hyper parameters how to tune

301
00:12:09,040 --> 00:12:11,200
them can we do automated tuning do we

302
00:12:11,200 --> 00:12:14,320
need to do uh tuning at all so this is

303
00:12:14,320 --> 00:12:16,079
this is uh

304
00:12:16,079 --> 00:12:18,560
questions uh that we still don't know

305
00:12:18,560 --> 00:12:20,160
good answers

306
00:12:20,160 --> 00:12:22,240
we know many answers we know many good

307
00:12:22,240 --> 00:12:25,200
results but we are still missing all the

308
00:12:25,200 --> 00:12:26,800
information to build the complete

309
00:12:26,800 --> 00:12:28,639
picture and finally the attack

310
00:12:28,639 --> 00:12:30,959
evaluation where we know very well how

311
00:12:30,959 --> 00:12:33,279
to evaluate the sideshield performance

312
00:12:33,279 --> 00:12:36,639
but what we do not know all the time is

313
00:12:36,639 --> 00:12:40,320
how to map the side channel performance

314
00:12:40,320 --> 00:12:42,240
with the machine learning metrics we

315
00:12:42,240 --> 00:12:43,839
commonly measure

316
00:12:43,839 --> 00:12:46,880
during the training process

317
00:12:46,880 --> 00:12:47,760
okay

318
00:12:47,760 --> 00:12:50,399
so let me let me give you

319
00:12:50,399 --> 00:12:53,360
very quickly couple of topics connected

320
00:12:53,360 --> 00:12:56,240
with feature engineering that i consider

321
00:12:56,240 --> 00:12:58,079
to be interesting

322
00:12:58,079 --> 00:13:00,639
so like i already said

323
00:13:00,639 --> 00:13:01,600
uh

324
00:13:01,600 --> 00:13:03,360
while we say we do not need feature

325
00:13:03,360 --> 00:13:05,279
engineering for deep learning inside

326
00:13:05,279 --> 00:13:08,000
channel still we kind of do it at least

327
00:13:08,000 --> 00:13:09,920
from academic perspective

328
00:13:09,920 --> 00:13:12,320
but then the question remains

329
00:13:12,320 --> 00:13:14,959
can we actually get better performance

330
00:13:14,959 --> 00:13:17,839
if we do not do future selection if we

331
00:13:17,839 --> 00:13:20,160
leave for deep learning to do it on its

332
00:13:20,160 --> 00:13:21,040
own

333
00:13:21,040 --> 00:13:24,000
or would there be maybe consequences

334
00:13:24,000 --> 00:13:25,440
of a

335
00:13:25,440 --> 00:13:27,519
decision like that in the sense that the

336
00:13:27,519 --> 00:13:30,320
performance would decrease or maybe the

337
00:13:30,320 --> 00:13:32,480
performance would increase but we would

338
00:13:32,480 --> 00:13:35,600
pay that with the necessity to having

339
00:13:35,600 --> 00:13:37,600
much more complicated neural network

340
00:13:37,600 --> 00:13:38,959
architectures

341
00:13:38,959 --> 00:13:41,519
and this is also very important if if

342
00:13:41,519 --> 00:13:43,120
you are uh

343
00:13:43,120 --> 00:13:45,519
checking and um up to date with the

344
00:13:45,519 --> 00:13:47,519
latest results for deep learning based

345
00:13:47,519 --> 00:13:50,240
action analysis you will notice that

346
00:13:50,240 --> 00:13:52,959
actually the most of neural networks

347
00:13:52,959 --> 00:13:56,079
that we use are very simple very small

348
00:13:56,079 --> 00:13:59,040
so cut only a couple of layers and we

349
00:13:59,040 --> 00:14:00,480
break the tag

350
00:14:00,480 --> 00:14:04,160
so the question is if we now use much

351
00:14:04,160 --> 00:14:06,800
bigger traces would we pay this with

352
00:14:06,800 --> 00:14:10,320
performance so having worse attacks

353
00:14:10,320 --> 00:14:12,959
or and would be paid with the size of

354
00:14:12,959 --> 00:14:15,440
the neural network we need to use

355
00:14:15,440 --> 00:14:17,279
and this is the question we've also

356
00:14:17,279 --> 00:14:19,760
tried recently to answer because there

357
00:14:19,760 --> 00:14:23,199
was a paper 2021 doing some experiments

358
00:14:23,199 --> 00:14:26,399
with asca and oscar is the

359
00:14:26,399 --> 00:14:29,040
among other datasets the datasets that

360
00:14:29,040 --> 00:14:31,760
are commonly used in academic research

361
00:14:31,760 --> 00:14:34,399
and actually the the authors

362
00:14:34,399 --> 00:14:36,560
so the paper is called pay attention to

363
00:14:36,560 --> 00:14:38,720
raw traces uh

364
00:14:38,720 --> 00:14:40,880
from chess last year and the authors

365
00:14:40,880 --> 00:14:43,360
showed that when you use raw traces you

366
00:14:43,360 --> 00:14:45,760
can get much better attack performance

367
00:14:45,760 --> 00:14:48,320
so you can break target with less attack

368
00:14:48,320 --> 00:14:49,920
traces

369
00:14:49,920 --> 00:14:51,120
but

370
00:14:51,120 --> 00:14:53,279
in a way they paid a price for doing

371
00:14:53,279 --> 00:14:55,760
that because they required a

372
00:14:55,760 --> 00:14:58,480
significantly more complex architectures

373
00:14:58,480 --> 00:15:02,240
around even up to around 50 layers

374
00:15:02,240 --> 00:15:04,240
and while for many domains 50 layers

375
00:15:04,240 --> 00:15:05,839
will not be allowed

376
00:15:05,839 --> 00:15:07,600
for us for side channel this is

377
00:15:07,600 --> 00:15:09,680
significantly larger than we are

378
00:15:09,680 --> 00:15:11,519
commonly used to see

379
00:15:11,519 --> 00:15:13,920
and immediately one can ask the question

380
00:15:13,920 --> 00:15:15,519
since we do not know

381
00:15:15,519 --> 00:15:16,320
how

382
00:15:16,320 --> 00:15:18,480
what are the important hyper parameters

383
00:15:18,480 --> 00:15:21,519
how to do a good hyperparameter tuning

384
00:15:21,519 --> 00:15:24,160
having so much bigger architecture comes

385
00:15:24,160 --> 00:15:26,560
with the price of maybe having much

386
00:15:26,560 --> 00:15:29,199
longer tuning process

387
00:15:29,199 --> 00:15:32,000
so recently we tried to devise couple of

388
00:15:32,000 --> 00:15:35,279
interesting scenarios that make sense to

389
00:15:35,279 --> 00:15:37,839
evaluate in the context of deep learning

390
00:15:37,839 --> 00:15:39,040
and we

391
00:15:39,040 --> 00:15:40,959
recognized three scenarios will be

392
00:15:40,959 --> 00:15:42,320
called

393
00:15:42,320 --> 00:15:45,440
airpoi oppo nopoi so

394
00:15:45,440 --> 00:15:47,519
points of interest selection optimized

395
00:15:47,519 --> 00:15:49,120
non-optimized

396
00:15:49,120 --> 00:15:52,639
uh restricted so basically it means do

397
00:15:52,639 --> 00:15:54,480
you assume you know

398
00:15:54,480 --> 00:15:56,880
the for instance the mask and do you

399
00:15:56,880 --> 00:16:00,320
know where the main snr peaks

400
00:16:00,320 --> 00:16:03,360
so in in a way this gives us from from

401
00:16:03,360 --> 00:16:05,759
the easiest scenario from the evaluation

402
00:16:05,759 --> 00:16:07,920
perspective from the evaluator knowing

403
00:16:07,920 --> 00:16:10,560
the details all the way to not query

404
00:16:10,560 --> 00:16:12,560
where the evaluator doesn't know

405
00:16:12,560 --> 00:16:15,839
anything and is kind of forced to work

406
00:16:15,839 --> 00:16:18,399
with raw traces

407
00:16:18,399 --> 00:16:21,279
and what we actually saw is that working

408
00:16:21,279 --> 00:16:22,959
with

409
00:16:22,959 --> 00:16:25,040
more features so something that looks

410
00:16:25,040 --> 00:16:27,519
like raw traces or at least has

411
00:16:27,519 --> 00:16:29,360
significantly more features than

412
00:16:29,360 --> 00:16:32,079
commonly used actually improves attack

413
00:16:32,079 --> 00:16:34,800
performance significantly

414
00:16:34,800 --> 00:16:37,120
so you can see here that we can break

415
00:16:37,120 --> 00:16:38,240
the target

416
00:16:38,240 --> 00:16:41,759
and this is a random key data set

417
00:16:41,759 --> 00:16:43,839
with and without uh

418
00:16:43,839 --> 00:16:47,680
uh data augmentation with even a single

419
00:16:47,680 --> 00:16:49,600
attack trace

420
00:16:49,600 --> 00:16:53,680
so we could break ascot which was

421
00:16:53,680 --> 00:16:56,160
realistically speaking uh speaking up to

422
00:16:56,160 --> 00:16:57,360
recently

423
00:16:57,360 --> 00:16:59,759
a data set that was considered

424
00:16:59,759 --> 00:17:01,920
a good good uh

425
00:17:01,920 --> 00:17:04,000
benchmark to assess the performance you

426
00:17:04,000 --> 00:17:06,240
can break it with a single trace

427
00:17:06,240 --> 00:17:08,160
and of course this is nothing new from

428
00:17:08,160 --> 00:17:09,760
the practical perspective whether you

429
00:17:09,760 --> 00:17:12,559
break it in one trace or five traces or

430
00:17:12,559 --> 00:17:15,280
10 20 it's not so important

431
00:17:15,280 --> 00:17:18,079
what is important that we needed only

432
00:17:18,079 --> 00:17:20,720
extremely small architecture to do this

433
00:17:20,720 --> 00:17:21,679
so

434
00:17:21,679 --> 00:17:22,559
while

435
00:17:22,559 --> 00:17:24,959
like i said already recent work said 50

436
00:17:24,959 --> 00:17:26,480
layers maybe

437
00:17:26,480 --> 00:17:28,400
actually we managed to do this with only

438
00:17:28,400 --> 00:17:30,400
two or three hidden layers

439
00:17:30,400 --> 00:17:32,960
so we do see that even if when we work

440
00:17:32,960 --> 00:17:35,840
with a huge number of features

441
00:17:35,840 --> 00:17:38,160
we can use still extremely small neural

442
00:17:38,160 --> 00:17:40,720
networks and get amazingly well

443
00:17:40,720 --> 00:17:42,080
performed

444
00:17:42,080 --> 00:17:45,520
so it does seem that more features is

445
00:17:45,520 --> 00:17:46,400
better

446
00:17:46,400 --> 00:17:48,880
and at least at the moment it's it does

447
00:17:48,880 --> 00:17:51,679
seem that we do not need to pay huge

448
00:17:51,679 --> 00:17:54,880
price in the architecture size to use

449
00:17:54,880 --> 00:17:57,280
that additional information coming from

450
00:17:57,280 --> 00:18:00,080
all teachers

451
00:18:00,080 --> 00:18:02,240
continuing with the with the

452
00:18:02,240 --> 00:18:05,120
topic of features a similar question you

453
00:18:05,120 --> 00:18:06,720
can ask

454
00:18:06,720 --> 00:18:07,600
okay

455
00:18:07,600 --> 00:18:09,360
features are definitely important

456
00:18:09,360 --> 00:18:11,360
depending how we select features or do

457
00:18:11,360 --> 00:18:13,280
we select them we get

458
00:18:13,280 --> 00:18:15,919
good or bad performance can we select

459
00:18:15,919 --> 00:18:19,039
features in such a way that we can make

460
00:18:19,039 --> 00:18:22,160
other profiling techniques also better

461
00:18:22,160 --> 00:18:24,799
performed so see think of it in this

462
00:18:24,799 --> 00:18:25,760
setup

463
00:18:25,760 --> 00:18:28,160
can we do feature selection for instance

464
00:18:28,160 --> 00:18:30,640
for template attack that would make

465
00:18:30,640 --> 00:18:33,919
template attack uh rival the

466
00:18:33,919 --> 00:18:36,000
performance deep learning

467
00:18:36,000 --> 00:18:38,799
and actually if we do uh feature

468
00:18:38,799 --> 00:18:40,720
selection with the

469
00:18:40,720 --> 00:18:43,200
with the techniques from deep learning

470
00:18:43,200 --> 00:18:45,919
we can reach that kind of performance

471
00:18:45,919 --> 00:18:48,080
what did we do we used similarity

472
00:18:48,080 --> 00:18:50,640
learning so where the goal is to learn a

473
00:18:50,640 --> 00:18:53,200
similarity function measuring how

474
00:18:53,200 --> 00:18:55,840
similar how related to objects are and

475
00:18:55,840 --> 00:18:58,000
then we use the concept of triplet

476
00:18:58,000 --> 00:19:00,320
learning where a triplet would consist

477
00:19:00,320 --> 00:19:02,559
of three samples positive and chord

478
00:19:02,559 --> 00:19:03,600
negative

479
00:19:03,600 --> 00:19:05,679
positive and anchor would have the same

480
00:19:05,679 --> 00:19:06,720
label

481
00:19:06,720 --> 00:19:08,080
while

482
00:19:08,080 --> 00:19:09,440
that label would be different from

483
00:19:09,440 --> 00:19:11,679
negative sample and we would get a

484
00:19:11,679 --> 00:19:13,360
setting like that

485
00:19:13,360 --> 00:19:16,240
we obtained linked traces we have labels

486
00:19:16,240 --> 00:19:19,120
we feed them to the treatment network

487
00:19:19,120 --> 00:19:22,480
we reach some embeddings so some

488
00:19:22,480 --> 00:19:23,360
some

489
00:19:23,360 --> 00:19:25,840
latent space representation of our

490
00:19:25,840 --> 00:19:28,720
features and those features that also

491
00:19:28,720 --> 00:19:32,400
use the labels so do not uh mix it for

492
00:19:32,400 --> 00:19:34,320
instance without encoders which is

493
00:19:34,320 --> 00:19:36,400
unsupervised

494
00:19:36,400 --> 00:19:39,039
this is a supervised technique so we add

495
00:19:39,039 --> 00:19:41,280
the information about the label into

496
00:19:41,280 --> 00:19:43,679
selection of most important features

497
00:19:43,679 --> 00:19:46,880
once we feed those into the attack

498
00:19:46,880 --> 00:19:50,559
mechanism we can actually see

499
00:19:50,559 --> 00:19:52,559
we can reach temperate attack

500
00:19:52,559 --> 00:19:55,600
performance something that is

501
00:19:55,600 --> 00:19:58,400
very similar to state of the art so here

502
00:19:58,400 --> 00:20:01,679
i just give a couple of um

503
00:20:01,679 --> 00:20:05,039
slides uh comparison so well not all the

504
00:20:05,039 --> 00:20:07,039
numbers are here because

505
00:20:07,039 --> 00:20:10,320
the the comparison is not fully fair but

506
00:20:10,320 --> 00:20:12,720
for instance methodology paper so this

507
00:20:12,720 --> 00:20:14,320
was the paper that

508
00:20:14,320 --> 00:20:15,840
proposed the methodology for

509
00:20:15,840 --> 00:20:18,240
convolutional networks for

510
00:20:18,240 --> 00:20:21,280
deep learning bo by asian optimization

511
00:20:21,280 --> 00:20:23,600
for automated hyphen parameter tuning

512
00:20:23,600 --> 00:20:26,799
are rl for reinforcement learning and

513
00:20:26,799 --> 00:20:31,520
then different techniques how to do um

514
00:20:31,520 --> 00:20:33,440
how to do feature reduction in a more

515
00:20:33,440 --> 00:20:35,520
traditional way so either principal

516
00:20:35,520 --> 00:20:37,039
component analysis or linear

517
00:20:37,039 --> 00:20:40,640
discriminant analysis or sosd or outer

518
00:20:40,640 --> 00:20:43,440
encoder so we can see that

519
00:20:43,440 --> 00:20:44,720
the

520
00:20:44,720 --> 00:20:46,880
smart feature selection

521
00:20:46,880 --> 00:20:49,280
coupled with template attack can

522
00:20:49,280 --> 00:20:51,280
actually give you even better

523
00:20:51,280 --> 00:20:53,520
performance than

524
00:20:53,520 --> 00:20:55,360
using

525
00:20:55,360 --> 00:20:58,720
powerful deep learning techniques or

526
00:20:58,720 --> 00:21:01,200
some other feature selection techniques

527
00:21:01,200 --> 00:21:03,280
combined for instance with template

528
00:21:03,280 --> 00:21:05,600
attack

529
00:21:05,600 --> 00:21:08,720
hyphen means the uh we could not break

530
00:21:08,720 --> 00:21:11,360
the target with the number of attack

531
00:21:11,360 --> 00:21:14,000
traces we had at our disposal so you can

532
00:21:14,000 --> 00:21:16,159
see here indeed

533
00:21:16,159 --> 00:21:18,480
it's not only that we can use deep

534
00:21:18,480 --> 00:21:21,120
learning to do powerful attacks but we

535
00:21:21,120 --> 00:21:23,520
could also use deep learning to make

536
00:21:23,520 --> 00:21:25,919
better feature selection that allows us

537
00:21:25,919 --> 00:21:28,240
to use even simpler techniques

538
00:21:28,240 --> 00:21:32,240
and still result in powerful attacks

539
00:21:32,240 --> 00:21:33,200
um

540
00:21:33,200 --> 00:21:35,520
that being said let me then just give

541
00:21:35,520 --> 00:21:37,520
you very very

542
00:21:37,520 --> 00:21:38,799
brief uh

543
00:21:38,799 --> 00:21:39,679
pitch

544
00:21:39,679 --> 00:21:42,000
uh if you like deep learning sites and

545
00:21:42,000 --> 00:21:44,799
analysis and maybe you do not know from

546
00:21:44,799 --> 00:21:46,559
where to start because there are so many

547
00:21:46,559 --> 00:21:49,360
techniques so many things to consider

548
00:21:49,360 --> 00:21:50,159
uh

549
00:21:50,159 --> 00:21:52,000
we recently published

550
00:21:52,000 --> 00:21:55,200
open source framework called ic and

551
00:21:55,200 --> 00:21:56,960
it's intended for deep learning based

552
00:21:56,960 --> 00:21:58,400
side channel analysis

553
00:21:58,400 --> 00:22:01,039
it has also some graphic user interface

554
00:22:01,039 --> 00:22:04,240
integrated database so state-of-the-art

555
00:22:04,240 --> 00:22:06,799
results you can generate

556
00:22:06,799 --> 00:22:09,840
reproducible scripts so even

557
00:22:09,840 --> 00:22:11,840
scripts that

558
00:22:11,840 --> 00:22:14,400
store all the all the random values

559
00:22:14,400 --> 00:22:16,720
everything so that indeed every time you

560
00:22:16,720 --> 00:22:18,960
run your experiment you get exactly the

561
00:22:18,960 --> 00:22:20,559
same result

562
00:22:20,559 --> 00:22:21,360
and

563
00:22:21,360 --> 00:22:24,240
of course teamwork and so on and so on

564
00:22:24,240 --> 00:22:26,880
so if you if you like the opportunity to

565
00:22:26,880 --> 00:22:28,720
use a tool like this

566
00:22:28,720 --> 00:22:31,600
uh we we have the version currently it's

567
00:22:31,600 --> 00:22:32,640
already

568
00:22:32,640 --> 00:22:35,520
let's say 0.2

569
00:22:35,520 --> 00:22:36,640
and

570
00:22:36,640 --> 00:22:39,520
you can you can download it uh there is

571
00:22:39,520 --> 00:22:42,640
also nice documentation if you think the

572
00:22:42,640 --> 00:22:45,200
tool is cool but is missing something

573
00:22:45,200 --> 00:22:48,159
important send us mail we will we are

574
00:22:48,159 --> 00:22:50,240
also adding new functionalities all the

575
00:22:50,240 --> 00:22:53,440
time so we will do our best to add new

576
00:22:53,440 --> 00:22:56,320
functionalities but community considers

577
00:22:56,320 --> 00:22:58,559
import

578
00:22:58,559 --> 00:23:01,120
let's uh yeah this is just a graphical

579
00:23:01,120 --> 00:23:03,760
depiction of what the tool can do so we

580
00:23:03,760 --> 00:23:06,159
can do quite a lot basically all those

581
00:23:06,159 --> 00:23:09,039
things i talked up to now so hyper

582
00:23:09,039 --> 00:23:10,799
parameter tuning different models

583
00:23:10,799 --> 00:23:13,600
different metrics uh visualization data

584
00:23:13,600 --> 00:23:16,480
augmentation all those things are

585
00:23:16,480 --> 00:23:19,280
possible within it

586
00:23:19,280 --> 00:23:21,679
finally let me tell you very shortly

587
00:23:21,679 --> 00:23:23,679
what are the possible challenges what

588
00:23:23,679 --> 00:23:25,760
what are the things we as the community

589
00:23:25,760 --> 00:23:28,240
need to address in next one

590
00:23:28,240 --> 00:23:29,520
five years

591
00:23:29,520 --> 00:23:30,320
so

592
00:23:30,320 --> 00:23:32,960
first we are missing a functional

593
00:23:32,960 --> 00:23:35,280
approach for the unsupervised deep

594
00:23:35,280 --> 00:23:37,440
learning based strike channel analysis

595
00:23:37,440 --> 00:23:40,159
what we do up now is supervised

596
00:23:40,159 --> 00:23:42,640
and there are some attempts towards

597
00:23:42,640 --> 00:23:45,279
unsupervised deep learning first one the

598
00:23:45,279 --> 00:23:48,840
2019 paper by benjamin timon

599
00:23:48,840 --> 00:23:52,960
but uh when we talk about practicality

600
00:23:52,960 --> 00:23:57,840
the cost of to do that kind of attack

601
00:23:57,840 --> 00:24:00,799
is maybe too significant to be always

602
00:24:00,799 --> 00:24:01,919
practical

603
00:24:01,919 --> 00:24:03,600
so we need

604
00:24:03,600 --> 00:24:05,440
more functional approaches for

605
00:24:05,440 --> 00:24:07,279
unsupervised deployment

606
00:24:07,279 --> 00:24:09,840
any new results in this domain will be

607
00:24:09,840 --> 00:24:12,240
in my opinion extremely important for

608
00:24:12,240 --> 00:24:14,080
the community

609
00:24:14,080 --> 00:24:16,640
second challenge is that

610
00:24:16,640 --> 00:24:18,559
most of the targets that we are

611
00:24:18,559 --> 00:24:20,960
constantly considering

612
00:24:20,960 --> 00:24:24,000
are software implementations offering

613
00:24:24,000 --> 00:24:26,559
limited countermeasures

614
00:24:26,559 --> 00:24:27,840
as such

615
00:24:27,840 --> 00:24:30,400
we commonly report deep learning working

616
00:24:30,400 --> 00:24:34,240
amazingly nice breaking targets easily

617
00:24:34,240 --> 00:24:35,919
but when you talk with people from

618
00:24:35,919 --> 00:24:38,480
industry they will use much

619
00:24:38,480 --> 00:24:41,200
more difficult targets hardware devices

620
00:24:41,200 --> 00:24:43,360
stronger countermeasures and commonly

621
00:24:43,360 --> 00:24:45,679
then you reach a huge car where we as

622
00:24:45,679 --> 00:24:48,080
the academic community say yes deep

623
00:24:48,080 --> 00:24:50,159
learning works amazingly and we break

624
00:24:50,159 --> 00:24:52,000
whatever we want

625
00:24:52,000 --> 00:24:54,000
and industry is saying yeah yeah deep

626
00:24:54,000 --> 00:24:56,400
learning is very interesting but for us

627
00:24:56,400 --> 00:24:57,440
it's

628
00:24:57,440 --> 00:25:00,480
a crazy amount of work to make anything

629
00:25:00,480 --> 00:25:03,120
happen and still we do not get

630
00:25:03,120 --> 00:25:06,559
not even close nice performance as as

631
00:25:06,559 --> 00:25:09,360
you claim to be possible so there is

632
00:25:09,360 --> 00:25:11,919
quite a big gap we need to see how to

633
00:25:11,919 --> 00:25:13,440
how to resolve

634
00:25:13,440 --> 00:25:16,880
the third challenge is data augmentation

635
00:25:16,880 --> 00:25:19,760
so do we need synthetic measurements

636
00:25:19,760 --> 00:25:22,400
to improve the performance

637
00:25:22,400 --> 00:25:24,320
well ideally

638
00:25:24,320 --> 00:25:26,480
i would say the answer seems to be yes

639
00:25:26,480 --> 00:25:28,000
because there is already a number of

640
00:25:28,000 --> 00:25:30,720
papers during data limitation and we

641
00:25:30,720 --> 00:25:32,880
always see improved results

642
00:25:32,880 --> 00:25:35,679
but strangely still it does not seem

643
00:25:35,679 --> 00:25:37,200
very well

644
00:25:37,200 --> 00:25:39,919
accepted in the community so

645
00:25:39,919 --> 00:25:41,919
not all the papers consider that they

646
00:25:41,919 --> 00:25:44,080
need data augmentation and indeed one

647
00:25:44,080 --> 00:25:46,799
could say well if i break the target do

648
00:25:46,799 --> 00:25:49,360
i need data augmentation

649
00:25:49,360 --> 00:25:51,279
from other side one could say yes you

650
00:25:51,279 --> 00:25:53,039
break the target but maybe you would

651
00:25:53,039 --> 00:25:55,919
break it with even much less traces if

652
00:25:55,919 --> 00:25:58,320
you did data augmentation so we we are

653
00:25:58,320 --> 00:26:01,520
still missing clear results from that

654
00:26:01,520 --> 00:26:04,159
do we need feature engineering if yes in

655
00:26:04,159 --> 00:26:05,760
what form

656
00:26:05,760 --> 00:26:08,960
the answer seems to be more and more

657
00:26:08,960 --> 00:26:11,600
no we do not need feature engineering if

658
00:26:11,600 --> 00:26:14,480
we want to use deep learning and yes we

659
00:26:14,480 --> 00:26:17,120
need very smart feature engineering if

660
00:26:17,120 --> 00:26:20,000
you want to use simpler techniques

661
00:26:20,000 --> 00:26:22,480
like them

662
00:26:23,200 --> 00:26:25,200
next

663
00:26:25,200 --> 00:26:27,520
since we are in deep learning center

664
00:26:27,520 --> 00:26:28,400
uh

665
00:26:28,400 --> 00:26:30,480
we want to know how to build good neural

666
00:26:30,480 --> 00:26:31,919
networks

667
00:26:31,919 --> 00:26:34,240
we need efficient guidelines how to

668
00:26:34,240 --> 00:26:36,400
build those networks so people currently

669
00:26:36,400 --> 00:26:39,039
use random search and then just search

670
00:26:39,039 --> 00:26:40,559
search search until

671
00:26:40,559 --> 00:26:41,760
they're happy

672
00:26:41,760 --> 00:26:43,840
or they use by asian optimization or

673
00:26:43,840 --> 00:26:45,760
they use reinforcement learning or they

674
00:26:45,760 --> 00:26:47,760
use genetic algorithms

675
00:26:47,760 --> 00:26:50,320
all those techniques seem to work

676
00:26:50,320 --> 00:26:53,760
but people still require better guidance

677
00:26:53,760 --> 00:26:56,240
ideally methodologies so we already have

678
00:26:56,240 --> 00:26:58,960
some methodologies but more work is

679
00:26:58,960 --> 00:27:01,600
needed especially when we consider

680
00:27:01,600 --> 00:27:03,520
realistic targets

681
00:27:03,520 --> 00:27:06,559
why is this so important well as long as

682
00:27:06,559 --> 00:27:08,640
we work with extremely small neural

683
00:27:08,640 --> 00:27:11,200
networks like we commonly do

684
00:27:11,200 --> 00:27:13,440
one could say it is not very important

685
00:27:13,440 --> 00:27:16,080
because we can get great results with

686
00:27:16,080 --> 00:27:18,840
small number of unique experiments

687
00:27:18,840 --> 00:27:21,120
efficiently but if you really need to

688
00:27:21,120 --> 00:27:23,279
increase our neural networks to be very

689
00:27:23,279 --> 00:27:24,240
large

690
00:27:24,240 --> 00:27:26,399
then the hyperparameter tuning will

691
00:27:26,399 --> 00:27:28,399
become extremely expensive

692
00:27:28,399 --> 00:27:30,960
and then having guidance how to do it

693
00:27:30,960 --> 00:27:32,159
would make

694
00:27:32,159 --> 00:27:35,200
practically huge difference

695
00:27:35,200 --> 00:27:37,360
connected question is

696
00:27:37,360 --> 00:27:39,360
what are the important hyper parameters

697
00:27:39,360 --> 00:27:40,960
there are so many different types of

698
00:27:40,960 --> 00:27:43,120
parameters for neural networks are they

699
00:27:43,120 --> 00:27:45,919
all important or can we just say well i

700
00:27:45,919 --> 00:27:47,600
will always use

701
00:27:47,600 --> 00:27:50,080
rel activation function and i don't need

702
00:27:50,080 --> 00:27:51,679
even to check what's happening with

703
00:27:51,679 --> 00:27:53,360
other ones

704
00:27:53,360 --> 00:27:55,279
do we require custom neural network

705
00:27:55,279 --> 00:27:57,760
elements so for instance recently we

706
00:27:57,760 --> 00:28:00,240
started a trend we started seeing a

707
00:28:00,240 --> 00:28:02,480
trend in side channel and deep learning

708
00:28:02,480 --> 00:28:04,960
to make side channel specific

709
00:28:04,960 --> 00:28:07,279
loss functions and the results are very

710
00:28:07,279 --> 00:28:08,880
good

711
00:28:08,880 --> 00:28:11,440
do we need more elements like that

712
00:28:11,440 --> 00:28:13,840
can we build universal models so can we

713
00:28:13,840 --> 00:28:15,840
build architectures that are good

714
00:28:15,840 --> 00:28:19,360
against various data sets not only

715
00:28:19,360 --> 00:28:21,919
one dataset one architecture

716
00:28:21,919 --> 00:28:23,679
next one

717
00:28:23,679 --> 00:28:25,440
explainability

718
00:28:25,440 --> 00:28:28,559
so why do we do attacks we do attacks to

719
00:28:28,559 --> 00:28:29,760
understand

720
00:28:29,760 --> 00:28:31,760
the security of a device

721
00:28:31,760 --> 00:28:34,480
but if the attack is successful the

722
00:28:34,480 --> 00:28:36,640
natural answer should be

723
00:28:36,640 --> 00:28:39,200
okay the attack is successful because of

724
00:28:39,200 --> 00:28:42,320
this and now we can do that to make it

725
00:28:42,320 --> 00:28:43,679
more secure

726
00:28:43,679 --> 00:28:45,600
but with deep learning we are missing

727
00:28:45,600 --> 00:28:47,039
this step

728
00:28:47,039 --> 00:28:49,840
we say well we broke the target but we

729
00:28:49,840 --> 00:28:51,600
don't understand what the neural network

730
00:28:51,600 --> 00:28:54,000
did why it broke the target without that

731
00:28:54,000 --> 00:28:55,919
understanding we don't know how to make

732
00:28:55,919 --> 00:28:57,440
better design

733
00:28:57,440 --> 00:29:00,320
so we need explainability

734
00:29:00,320 --> 00:29:02,640
and of course one big challenge for

735
00:29:02,640 --> 00:29:04,559
anyone especially new people coming to

736
00:29:04,559 --> 00:29:06,640
the domain there are so many things

737
00:29:06,640 --> 00:29:10,960
already done so from 2016 end of 2016

738
00:29:10,960 --> 00:29:13,760
there is already more than 200 papers on

739
00:29:13,760 --> 00:29:16,320
deep learning based on general analysis

740
00:29:16,320 --> 00:29:19,840
for someone who wants to use important

741
00:29:19,840 --> 00:29:21,279
and

742
00:29:21,279 --> 00:29:22,159
good

743
00:29:22,159 --> 00:29:23,600
options

744
00:29:23,600 --> 00:29:25,440
and when i say good i mean

745
00:29:25,440 --> 00:29:28,320
efficient from the practical perspective

746
00:29:28,320 --> 00:29:30,720
it's a lot of work to understand where

747
00:29:30,720 --> 00:29:35,200
to start what to take what to change

748
00:29:35,200 --> 00:29:37,760
and finally to conclude

749
00:29:37,760 --> 00:29:39,520
i would just say deep learning is

750
00:29:39,520 --> 00:29:41,600
efficient and powerful option for side

751
00:29:41,600 --> 00:29:43,919
channel analysis i think with all the

752
00:29:43,919 --> 00:29:47,039
results we have up now this is clear

753
00:29:47,039 --> 00:29:48,960
and current results are very very

754
00:29:48,960 --> 00:29:51,440
promising as we can break protected

755
00:29:51,440 --> 00:29:53,440
targets even with very small

756
00:29:53,440 --> 00:29:55,039
architecture

757
00:29:55,039 --> 00:29:57,840
what is less clear is how would the

758
00:29:57,840 --> 00:30:00,080
approach that we are currently using all

759
00:30:00,080 --> 00:30:02,159
the approaches we are currently using

760
00:30:02,159 --> 00:30:05,679
scale for more realistic targets

761
00:30:05,679 --> 00:30:08,640
and of course i again i repeat the big

762
00:30:08,640 --> 00:30:10,720
challenges are unsupervised deep

763
00:30:10,720 --> 00:30:12,960
learning functional unsupervised deep

764
00:30:12,960 --> 00:30:15,120
learning based social analysis

765
00:30:15,120 --> 00:30:17,039
and explainable

766
00:30:17,039 --> 00:30:19,679
ai explainable machine learning for side

767
00:30:19,679 --> 00:30:21,279
channel analysis

768
00:30:21,279 --> 00:30:22,720
that being said

769
00:30:22,720 --> 00:30:24,960
thank you very much for your attention

770
00:30:24,960 --> 00:30:27,440
and if there are any questions i will be

771
00:30:27,440 --> 00:30:29,919
happy to answer or at least try

772
00:30:29,919 --> 00:30:32,480
answering

773
00:30:32,960 --> 00:30:37,080
thank you very much for the presentation

774
00:30:37,120 --> 00:30:38,880
i would like to encourage everyone to

775
00:30:38,880 --> 00:30:40,640
share their questions in the chat box if

776
00:30:40,640 --> 00:30:43,279
they have any

777
00:30:49,200 --> 00:30:52,080
probably it was a lot to digest in one

778
00:30:52,080 --> 00:30:53,360
go

779
00:30:53,360 --> 00:30:55,120
sorry

780
00:30:55,120 --> 00:30:58,239
oh there is something

781
00:30:59,039 --> 00:31:02,480
why is the q in green

782
00:31:02,480 --> 00:31:04,960
why is q in green

783
00:31:04,960 --> 00:31:07,600
well because i uh um

784
00:31:07,600 --> 00:31:09,440
that's not a good question for me

785
00:31:09,440 --> 00:31:12,880
actually i stole this uh question slide

786
00:31:12,880 --> 00:31:15,360
from uh from a colleague so i should ask

787
00:31:15,360 --> 00:31:19,840
him why he put uh q in green

788
00:31:20,799 --> 00:31:23,200
all right

789
00:31:24,480 --> 00:31:26,159
are there any metrics

790
00:31:26,159 --> 00:31:28,880
for dl side channel resistance like

791
00:31:28,880 --> 00:31:30,799
remaining

792
00:31:30,799 --> 00:31:32,799
entropy

793
00:31:32,799 --> 00:31:34,080
um

794
00:31:34,080 --> 00:31:37,519
well there are uh there are uh metrics

795
00:31:37,519 --> 00:31:39,360
um

796
00:31:39,360 --> 00:31:42,240
in general that we can somehow combine

797
00:31:42,240 --> 00:31:44,640
deep learning and

798
00:31:44,640 --> 00:31:46,960
side channel like perceived information

799
00:31:46,960 --> 00:31:49,360
for instance so this is the

800
00:31:49,360 --> 00:31:51,840
something very much connected with the

801
00:31:51,840 --> 00:31:54,880
categorical cross entropy so we could

802
00:31:54,880 --> 00:31:57,679
say receive information from one side on

803
00:31:57,679 --> 00:32:00,000
side channel and categorical cross

804
00:32:00,000 --> 00:32:02,159
entropy on the other side

805
00:32:02,159 --> 00:32:04,480
as as

806
00:32:04,480 --> 00:32:06,720
machine learning so there is a match

807
00:32:06,720 --> 00:32:08,320
between those two

808
00:32:08,320 --> 00:32:09,200
but

809
00:32:09,200 --> 00:32:11,039
for deep learning side channel

810
00:32:11,039 --> 00:32:12,799
resistance

811
00:32:12,799 --> 00:32:14,399
we do not have

812
00:32:14,399 --> 00:32:16,960
as far as i know something specific we

813
00:32:16,960 --> 00:32:19,919
just have the notion of successfulness

814
00:32:19,919 --> 00:32:22,960
of attack so then we could say well the

815
00:32:22,960 --> 00:32:24,960
resistance is the absence of

816
00:32:24,960 --> 00:32:26,559
successfulness

817
00:32:26,559 --> 00:32:30,240
but i i am not aware of any any very

818
00:32:30,240 --> 00:32:32,159
specific metrics beyond those

819
00:32:32,159 --> 00:32:34,799
information theoretic metrics saying how

820
00:32:34,799 --> 00:32:39,000
successful the attack is

821
00:32:42,380 --> 00:32:44,640
[Music]

822
00:32:44,640 --> 00:32:46,240
could this deep learning based side

823
00:32:46,240 --> 00:32:48,080
channel be applied for other attacks

824
00:32:48,080 --> 00:32:50,240
like identifying instructions being

825
00:32:50,240 --> 00:32:53,200
executed in a cpu from the noise

826
00:32:53,200 --> 00:32:56,399
raw data this is very good question

827
00:32:56,399 --> 00:32:59,440
to be honest i do not know

828
00:32:59,440 --> 00:33:02,240
let me explain a bit more uh

829
00:33:02,240 --> 00:33:05,279
intuitively i would say yes

830
00:33:05,279 --> 00:33:06,960
it could be

831
00:33:06,960 --> 00:33:09,200
i know many people tried things like

832
00:33:09,200 --> 00:33:14,480
that and many people more or less failed

833
00:33:14,480 --> 00:33:16,640
the the cpu

834
00:33:16,640 --> 00:33:19,279
so the data coming from cpu

835
00:33:19,279 --> 00:33:21,519
is commonly extremely

836
00:33:21,519 --> 00:33:24,480
long traces measurements and then you

837
00:33:24,480 --> 00:33:27,120
get in a set up where you need to use

838
00:33:27,120 --> 00:33:29,360
extremely

839
00:33:29,360 --> 00:33:31,919
large neural network architectures and

840
00:33:31,919 --> 00:33:35,679
then it seems we are still a bit lacking

841
00:33:35,679 --> 00:33:37,440
the knowledge

842
00:33:37,440 --> 00:33:39,760
in the in the side channel security

843
00:33:39,760 --> 00:33:41,200
community

844
00:33:41,200 --> 00:33:43,360
how to use powerful architectures for

845
00:33:43,360 --> 00:33:44,799
things like that

846
00:33:44,799 --> 00:33:47,440
so intuitively i would say the answer

847
00:33:47,440 --> 00:33:49,279
should be yes because anyway deep

848
00:33:49,279 --> 00:33:51,919
learning doesn't know what the cpu data

849
00:33:51,919 --> 00:33:53,919
versus what is power

850
00:33:53,919 --> 00:33:57,120
measurement or electromagnetic radiation

851
00:33:57,120 --> 00:33:58,720
it is more

852
00:33:58,720 --> 00:34:02,640
the quality of the information contained

853
00:34:02,640 --> 00:34:05,519
in the traces and how complex

854
00:34:05,519 --> 00:34:07,200
is that information

855
00:34:07,200 --> 00:34:09,839
so the less quality of a measurement we

856
00:34:09,839 --> 00:34:12,320
require

857
00:34:12,320 --> 00:34:14,800
more more data

858
00:34:14,800 --> 00:34:15,679
and

859
00:34:15,679 --> 00:34:17,839
the more data there is we require

860
00:34:17,839 --> 00:34:20,800
possibly more complex architectures that

861
00:34:20,800 --> 00:34:23,199
are able to model that complex

862
00:34:23,199 --> 00:34:26,000
relationships

863
00:34:26,719 --> 00:34:28,159
um

864
00:34:28,159 --> 00:34:29,199
so

865
00:34:29,199 --> 00:34:32,159
i this is something i think in general

866
00:34:32,159 --> 00:34:36,239
will be uh covered more in future i

867
00:34:36,239 --> 00:34:38,399
think we will be seeing more of those

868
00:34:38,399 --> 00:34:41,359
cpu based attacks and deep learning in

869
00:34:41,359 --> 00:34:43,440
future there are a couple of works in

870
00:34:43,440 --> 00:34:46,960
last year or so and i believe more and

871
00:34:46,960 --> 00:34:50,480
more will show up um

872
00:34:50,480 --> 00:34:52,719
so now the question is uh there is

873
00:34:52,719 --> 00:34:54,560
limited research with count dimensions

874
00:34:54,560 --> 00:34:56,159
but any

875
00:34:56,159 --> 00:34:58,560
any idea yet which might be strong or

876
00:34:58,560 --> 00:35:00,560
weak timing randomization may be big due

877
00:35:00,560 --> 00:35:03,440
to time environments indeed indeed

878
00:35:03,440 --> 00:35:05,440
so

879
00:35:05,440 --> 00:35:08,160
based on the experiments we did

880
00:35:08,160 --> 00:35:10,000
hiding

881
00:35:10,000 --> 00:35:13,599
doesn't seem as powerful as masking

882
00:35:13,599 --> 00:35:16,480
although of course that also depends on

883
00:35:16,480 --> 00:35:19,200
the on the level of hiding so if we hide

884
00:35:19,200 --> 00:35:21,040
in the amplitude domain

885
00:35:21,040 --> 00:35:23,680
and then we add crazy amount of noise of

886
00:35:23,680 --> 00:35:25,359
course that is difficult

887
00:35:25,359 --> 00:35:27,839
if we uh hide in the timing domain and

888
00:35:27,839 --> 00:35:29,800
we add crazy amount of

889
00:35:29,800 --> 00:35:31,839
de-synchronization that is of course

890
00:35:31,839 --> 00:35:32,880
difficult

891
00:35:32,880 --> 00:35:35,040
in general what we see

892
00:35:35,040 --> 00:35:37,520
timing various convolutional networks

893
00:35:37,520 --> 00:35:39,599
fight very nicely against timing

894
00:35:39,599 --> 00:35:41,720
randomization and global

895
00:35:41,720 --> 00:35:44,960
desynchronization is more easy to

896
00:35:44,960 --> 00:35:47,920
defeat than local desynchronization so

897
00:35:47,920 --> 00:35:49,680
this synchronization

898
00:35:49,680 --> 00:35:52,480
is easy while jitter or random delay

899
00:35:52,480 --> 00:35:54,800
interrupts are more difficult

900
00:35:54,800 --> 00:35:57,280
similarly on masking side boolean

901
00:35:57,280 --> 00:35:59,760
masking well at least first order

902
00:35:59,760 --> 00:36:02,720
masking seems relatively easy higher

903
00:36:02,720 --> 00:36:06,160
order maskings are actually not tested

904
00:36:06,160 --> 00:36:06,960
so

905
00:36:06,960 --> 00:36:10,400
difficult to say and also there is the

906
00:36:10,400 --> 00:36:11,760
uh the

907
00:36:11,760 --> 00:36:14,240
additive masking so multiplicative

908
00:36:14,240 --> 00:36:15,440
something

909
00:36:15,440 --> 00:36:18,480
that seems to be much more difficult so

910
00:36:18,480 --> 00:36:21,599
for instance if we compare ascot v1 that

911
00:36:21,599 --> 00:36:23,440
has boonian masking

912
00:36:23,440 --> 00:36:27,440
and ascot v2 that has offline

913
00:36:27,440 --> 00:36:28,560
masking

914
00:36:28,560 --> 00:36:30,960
seems the offline one to be much more

915
00:36:30,960 --> 00:36:33,520
difficult because

916
00:36:33,520 --> 00:36:36,160
the operation there is multiplication

917
00:36:36,160 --> 00:36:38,480
and for neural network to

918
00:36:38,480 --> 00:36:41,280
to learn the multiplication operation

919
00:36:41,280 --> 00:36:42,320
that is

920
00:36:42,320 --> 00:36:45,200
intuitively significantly more difficult

921
00:36:45,200 --> 00:36:47,119
than just xor

922
00:36:47,119 --> 00:36:49,920
but some very precise

923
00:36:49,920 --> 00:36:52,160
evaluation of different countermeasures

924
00:36:52,160 --> 00:36:52,839
and

925
00:36:52,839 --> 00:36:56,240
gradation uh we do not have to come more

926
00:36:56,240 --> 00:36:58,960
into intuitions

927
00:36:58,960 --> 00:37:00,960
data mutations seem valuable but how

928
00:37:00,960 --> 00:37:02,880
best to make the model flexible across

929
00:37:02,880 --> 00:37:04,560
parts implementation with different

930
00:37:04,560 --> 00:37:06,960
process variation yeah this is very good

931
00:37:06,960 --> 00:37:10,800
very good question so it depends

932
00:37:10,800 --> 00:37:12,400
um

933
00:37:12,400 --> 00:37:14,560
it very much depends so this is

934
00:37:14,560 --> 00:37:17,599
connected also with portability uh what

935
00:37:17,599 --> 00:37:20,960
does seem as a very good option

936
00:37:20,960 --> 00:37:23,440
for for data augmentation would be maybe

937
00:37:23,440 --> 00:37:24,560
to use

938
00:37:24,560 --> 00:37:27,599
generative adversarial networks where

939
00:37:27,599 --> 00:37:30,560
they would be used um not only to make

940
00:37:30,560 --> 00:37:33,040
new measurements but also

941
00:37:33,040 --> 00:37:34,400
we could make

942
00:37:34,400 --> 00:37:38,640
more powerful uh loss functions so uh

943
00:37:38,640 --> 00:37:42,000
that also incorporate the the difference

944
00:37:42,000 --> 00:37:44,720
so to to mimic different devices that

945
00:37:44,720 --> 00:37:46,720
could be one option for process

946
00:37:46,720 --> 00:37:51,040
variation to to use guns to produce

947
00:37:51,040 --> 00:37:53,040
measurements that

948
00:37:53,040 --> 00:37:56,720
are intentionally similar in the sense

949
00:37:56,720 --> 00:37:58,800
similar like they're coming from a

950
00:37:58,800 --> 00:38:00,400
similar device

951
00:38:00,400 --> 00:38:02,240
so i think that would be one very

952
00:38:02,240 --> 00:38:03,680
interesting quote

953
00:38:03,680 --> 00:38:04,880
what is the state-of-the-art

954
00:38:04,880 --> 00:38:06,480
inexplicability of people learning

955
00:38:06,480 --> 00:38:09,599
sideshow or attacks

956
00:38:09,760 --> 00:38:11,200
state-of-the-art

957
00:38:11,200 --> 00:38:14,240
seems almost nothing there is a recent

958
00:38:14,240 --> 00:38:16,560
paper on great books

959
00:38:16,560 --> 00:38:17,680
masking

960
00:38:17,680 --> 00:38:20,800
by people from uh nouveau i don't know

961
00:38:20,800 --> 00:38:21,680
uh

962
00:38:21,680 --> 00:38:24,000
they do some work there there is also

963
00:38:24,000 --> 00:38:25,520
one paper

964
00:38:25,520 --> 00:38:27,839
from my group from last year where we

965
00:38:27,839 --> 00:38:29,680
used actually ablation technique to

966
00:38:29,680 --> 00:38:32,400
understand hiding countermeasures

967
00:38:32,400 --> 00:38:36,240
and it seems to work quite nicely so

968
00:38:36,240 --> 00:38:38,560
what is the ablation it's the process

969
00:38:38,560 --> 00:38:40,960
like in

970
00:38:40,960 --> 00:38:43,760
operations of brain where you see what

971
00:38:43,760 --> 00:38:46,720
is the influence of a small change on a

972
00:38:46,720 --> 00:38:48,320
behavior for instance when you watch

973
00:38:48,320 --> 00:38:50,640
some movie with operation on a brain you

974
00:38:50,640 --> 00:38:52,079
see a person

975
00:38:52,079 --> 00:38:54,640
playing a guitar and then they touch one

976
00:38:54,640 --> 00:38:57,920
part of the brain and then person stops

977
00:38:57,920 --> 00:38:59,680
playing because the person does not

978
00:38:59,680 --> 00:39:02,320
remember anymore how to play so ablation

979
00:39:02,320 --> 00:39:04,560
for neural networks is something similar

980
00:39:04,560 --> 00:39:07,280
you randomly disconnect kill

981
00:39:07,280 --> 00:39:09,680
some neurons and see what what will be

982
00:39:09,680 --> 00:39:12,400
the output if the output is the same

983
00:39:12,400 --> 00:39:14,640
then basically it means that part of

984
00:39:14,640 --> 00:39:16,480
neural network was not doing anything

985
00:39:16,480 --> 00:39:19,520
important if the output changes a lot it

986
00:39:19,520 --> 00:39:22,079
means it was important so based on that

987
00:39:22,079 --> 00:39:24,480
we could actually build

988
00:39:24,480 --> 00:39:27,760
a ablation mechanism for explainability

989
00:39:27,760 --> 00:39:30,000
but actually told us for different

990
00:39:30,000 --> 00:39:31,920
hiding counter measures

991
00:39:31,920 --> 00:39:33,359
which are easier which are more

992
00:39:33,359 --> 00:39:35,520
difficult and indeed we see

993
00:39:35,520 --> 00:39:38,640
we saw that if we use something like

994
00:39:38,640 --> 00:39:42,160
small desynchronization neural networks

995
00:39:42,160 --> 00:39:44,800
handle small desynchronization already

996
00:39:44,800 --> 00:39:47,839
in first layers so already first layers

997
00:39:47,839 --> 00:39:49,680
are sufficient to break

998
00:39:49,680 --> 00:39:51,760
that kind of countermeasure but if we

999
00:39:51,760 --> 00:39:54,160
add something like jitter that is local

1000
00:39:54,160 --> 00:39:56,320
desynchronization more difficult to beat

1001
00:39:56,320 --> 00:39:58,079
you can see the more and more of

1002
00:39:58,079 --> 00:40:00,000
activity of neural network

1003
00:40:00,000 --> 00:40:02,160
happens in deeper layers which means

1004
00:40:02,160 --> 00:40:04,560
neural network needs to combine more

1005
00:40:04,560 --> 00:40:07,839
things uh do a smarter feature

1006
00:40:07,839 --> 00:40:10,240
extraction to handle more those more

1007
00:40:10,240 --> 00:40:13,040
difficult countermeasures so i would

1008
00:40:13,040 --> 00:40:14,240
call this

1009
00:40:14,240 --> 00:40:15,599
current state of the art in

1010
00:40:15,599 --> 00:40:17,680
explainability of deep learning side

1011
00:40:17,680 --> 00:40:19,839
channel analysis although honestly i

1012
00:40:19,839 --> 00:40:22,160
think we are still very far from

1013
00:40:22,160 --> 00:40:24,640
anything that can be really used in

1014
00:40:24,640 --> 00:40:27,040
practice

1015
00:40:31,119 --> 00:40:32,720
all right thank you very much for

1016
00:40:32,720 --> 00:40:35,760
answering these questions

1017
00:40:35,760 --> 00:40:38,480
um i don't think we have any other

1018
00:40:38,480 --> 00:40:40,640
questions so

1019
00:40:40,640 --> 00:40:42,480
i would like to thank you for the

1020
00:40:42,480 --> 00:40:45,280
presentation and for being with us today

1021
00:40:45,280 --> 00:40:47,359
and

1022
00:40:47,359 --> 00:40:49,440
guys if you like the presentation today

1023
00:40:49,440 --> 00:40:51,040
and you would like to learn more about

1024
00:40:51,040 --> 00:40:52,960
these topics stephan will also deliver a

1025
00:40:52,960 --> 00:40:56,319
training this june in us in santa clara

1026
00:40:56,319 --> 00:40:58,800
at hardware io where he will cover not

1027
00:40:58,800 --> 00:41:00,480
only theory but a lot of hands-on

1028
00:41:00,480 --> 00:41:02,000
activities as well

1029
00:41:02,000 --> 00:41:04,160
now i would like to thank you for your

1030
00:41:04,160 --> 00:41:06,160
attendance and i hope you enjoyed the

1031
00:41:06,160 --> 00:41:07,760
presentation and i wish you a nice

1032
00:41:07,760 --> 00:41:09,680
evening thank you all

1033
00:41:09,680 --> 00:41:11,920
have a nice evening for day

1034
00:41:11,920 --> 00:41:13,440
yes bye

1035
00:41:13,440 --> 00:41:16,440
bye

