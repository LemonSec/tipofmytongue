1
00:00:12,460 --> 00:00:15,440
hey everyone thank you for joining us

2
00:00:15,440 --> 00:00:15,889
today

3
00:00:15,889 --> 00:00:18,380
my name is anthrax and that would be our

4
00:00:18,380 --> 00:00:21,470
host for the day at Hardware dot IO we

5
00:00:21,470 --> 00:00:23,779
come together to put our brains into

6
00:00:23,779 --> 00:00:25,669
discussing the latest innovation in

7
00:00:25,669 --> 00:00:28,669
attacking and defending Hardware the

8
00:00:28,669 --> 00:00:30,770
objective of the conference revolves

9
00:00:30,770 --> 00:00:33,500
around key concerns related to hardware

10
00:00:33,500 --> 00:00:36,829
wrong way and related protocols today's

11
00:00:36,829 --> 00:00:39,290
webinar is going to talk about denoising

12
00:00:39,290 --> 00:00:43,190
of tempest images on efficient optical

13
00:00:43,190 --> 00:00:45,800
character recognition by Santiago and

14
00:00:45,800 --> 00:00:48,710
yuan they put our researchers at

15
00:00:48,710 --> 00:00:50,930
technology and innovation Institute of

16
00:00:50,930 --> 00:00:55,100
Abu Dhabi so a quick announcement for

17
00:00:55,100 --> 00:00:57,920
the format of the webinar 30 minutes

18
00:00:57,920 --> 00:00:59,810
would be the presentation and 10 mins

19
00:00:59,810 --> 00:01:01,400
would be given for Q&A

20
00:01:01,400 --> 00:01:03,890
all those participants who have joined

21
00:01:03,890 --> 00:01:05,810
us and would like to ask questions to

22
00:01:05,810 --> 00:01:08,180
our speakers you could ask your

23
00:01:08,180 --> 00:01:10,250
questions in the chat option on Zoom and

24
00:01:10,250 --> 00:01:12,259
they would be answered at the end of the

25
00:01:12,259 --> 00:01:13,039
presentation

26
00:01:13,039 --> 00:01:15,469
I would like to also thank both our

27
00:01:15,469 --> 00:01:17,810
speakers to present this latest research

28
00:01:17,810 --> 00:01:20,569
of theirs on our hardware or tile

29
00:01:20,569 --> 00:01:23,889
platform because it's yet not published

30
00:01:23,889 --> 00:01:26,509
completely in a journal there's a

31
00:01:26,509 --> 00:01:30,499
pretext available for download I forget

32
00:01:30,499 --> 00:01:32,929
the publication house but I will get

33
00:01:32,929 --> 00:01:34,670
back to you on that very soon

34
00:01:34,670 --> 00:01:37,630
quick announcement about our upcoming

35
00:01:37,630 --> 00:01:40,219
webinars we have two webinar scheduled

36
00:01:40,219 --> 00:01:42,679
this in this month on the 6th of July by

37
00:01:42,679 --> 00:01:45,530
ma and followed by 30th of July by Alex

38
00:01:45,530 --> 00:01:47,659
so if you're interested in this topic

39
00:01:47,659 --> 00:01:51,200
please join us as well as after this

40
00:01:51,200 --> 00:01:53,569
presentation we expect you to give us a

41
00:01:53,569 --> 00:01:56,179
feedback so that whatever your life what

42
00:01:56,179 --> 00:01:58,880
you're then like that will that was

43
00:01:58,880 --> 00:02:02,779
improve for future webinars and last we

44
00:02:02,779 --> 00:02:04,639
have our upcoming trainings for

45
00:02:04,639 --> 00:02:06,439
schedules in the month of September we

46
00:02:06,439 --> 00:02:09,169
have nine amazing hard way researchers

47
00:02:09,169 --> 00:02:11,480
presenting and delivering and sharing

48
00:02:11,480 --> 00:02:13,340
your knowledge to the community so

49
00:02:13,340 --> 00:02:17,450
please join us for now yuan and Santiago

50
00:02:17,450 --> 00:02:21,470
the stage is yours ok thank you very

51
00:02:21,470 --> 00:02:22,440
much

52
00:02:22,440 --> 00:02:25,320
and Trish so good evening good evening

53
00:02:25,320 --> 00:02:34,560
everyone I will share my screen so

54
00:02:34,560 --> 00:02:44,850
please excuse me sorry okay so as you

55
00:02:44,850 --> 00:02:47,250
said today's webinar is called deep

56
00:02:47,250 --> 00:02:49,560
learning based on the noising of tempest

57
00:02:49,560 --> 00:02:51,960
images for efficient optical character

58
00:02:51,960 --> 00:02:55,410
recognition this consists on two parts

59
00:02:55,410 --> 00:02:58,200
the first part is going to be presented

60
00:02:58,200 --> 00:03:01,170
by me and the second part by one so in

61
00:03:01,170 --> 00:03:03,480
this first part I'm going to present a

62
00:03:03,480 --> 00:03:05,820
little review of what is tempest the

63
00:03:05,820 --> 00:03:10,050
history of tempest and how we can cook

64
00:03:10,050 --> 00:03:13,670
we can reconstruct a video frames from

65
00:03:13,670 --> 00:03:17,670
remote video interfaces a remote display

66
00:03:17,670 --> 00:03:20,820
monitor so the first part from

67
00:03:20,820 --> 00:03:23,220
electromagnetic emanations back to video

68
00:03:23,220 --> 00:03:26,610
frames something about the history of

69
00:03:26,610 --> 00:03:29,540
tempest tempest is an acronym for

70
00:03:29,540 --> 00:03:31,650
telecommunications electronics materials

71
00:03:31,650 --> 00:03:33,930
protected from emanating spurious

72
00:03:33,930 --> 00:03:36,930
transmissions it is a United States

73
00:03:36,930 --> 00:03:39,840
National Security Agency specification

74
00:03:39,840 --> 00:03:44,580
and NATO certification and it

75
00:03:44,580 --> 00:03:46,830
established the requirements for

76
00:03:46,830 --> 00:03:51,170
military equipment protection against

77
00:03:51,170 --> 00:03:54,000
unintentional electromagnetic emanations

78
00:03:54,000 --> 00:03:56,040
zone and Liberation's

79
00:03:56,040 --> 00:03:58,440
so it has been known by military

80
00:03:58,440 --> 00:04:00,959
organizations as early as sixth as the

81
00:04:00,959 --> 00:04:04,730
60s of the past century that

82
00:04:04,730 --> 00:04:10,050
electromechanical devices emanate when

83
00:04:10,050 --> 00:04:13,489
intended electromagnetic signals or

84
00:04:13,489 --> 00:04:16,918
sometimes also sound and physical

85
00:04:16,918 --> 00:04:22,340
vibration and with from these emanations

86
00:04:22,460 --> 00:04:25,620
information can be recovered so this is

87
00:04:25,620 --> 00:04:29,340
an info a security leakage this was

88
00:04:29,340 --> 00:04:34,169
known by the open public in 1984 when

89
00:04:34,169 --> 00:04:35,740
the Swedish government

90
00:04:35,740 --> 00:04:40,210
issued a thin book page booklet with

91
00:04:40,210 --> 00:04:44,260
technical details of how this phenomena

92
00:04:44,260 --> 00:04:49,120
worked at the time and in 1985 a BIM

93
00:04:49,120 --> 00:04:51,340
Banach publish an unclassified technical

94
00:04:51,340 --> 00:04:55,120
analysis of the security risks on CRT

95
00:04:55,120 --> 00:04:59,230
monitors in air in the early 2000s

96
00:04:59,230 --> 00:05:02,200
Marcus Kuhn demonstrated that this also

97
00:05:02,200 --> 00:05:10,060
happened on LCD monitors so whole is

98
00:05:10,060 --> 00:05:12,580
what is happening so we are connecting

99
00:05:12,580 --> 00:05:16,540
an external display monitor with a video

100
00:05:16,540 --> 00:05:20,950
interface the two most known build

101
00:05:20,950 --> 00:05:22,840
interfaces at the moment are busy and

102
00:05:22,840 --> 00:05:27,190
HDMI and the information that is

103
00:05:27,190 --> 00:05:29,440
traveling through the cable from the PC

104
00:05:29,440 --> 00:05:32,020
to the display monitor is also

105
00:05:32,020 --> 00:05:34,470
transmitting electromagnetic

106
00:05:34,470 --> 00:05:37,710
electromagnetic waves if we capture this

107
00:05:37,710 --> 00:05:42,310
this electromagnetic emanations we can

108
00:05:42,310 --> 00:05:43,870
reconstruct video frames

109
00:05:43,870 --> 00:05:47,710
I want to splain you how this is done

110
00:05:47,710 --> 00:05:52,300
but I will explain you with BGA I think

111
00:05:52,300 --> 00:05:54,550
from my point of view it's easily easily

112
00:05:54,550 --> 00:05:57,250
understandable this phenomena from the

113
00:05:57,250 --> 00:05:59,500
VGA point of view but this also happens

114
00:05:59,500 --> 00:06:03,310
with HDMI this phenomena happens with

115
00:06:03,310 --> 00:06:05,320
this point of interfaces and also with

116
00:06:05,320 --> 00:06:10,600
our other interfaces like D V I so how

117
00:06:10,600 --> 00:06:14,080
this is happening we have this connector

118
00:06:14,080 --> 00:06:18,160
a VGA connector the main pins of this

119
00:06:18,160 --> 00:06:19,990
connector are the blue green and red

120
00:06:19,990 --> 00:06:24,130
channels also we have the result of

121
00:06:24,130 --> 00:06:25,690
synchronization pulls the vertical

122
00:06:25,690 --> 00:06:28,120
synchronization poles and for example

123
00:06:28,120 --> 00:06:30,120
let's assume that we are transmitted

124
00:06:30,120 --> 00:06:38,700
full HD resolution of 1920 pixels per

125
00:06:38,700 --> 00:06:45,390
1080 lines at a frame rate of 60 Hertz

126
00:06:45,390 --> 00:06:48,460
if there is this is our solution and

127
00:06:48,460 --> 00:06:49,590
this is our

128
00:06:49,590 --> 00:06:52,590
fresh deflation rate the pixel clock in

129
00:06:52,590 --> 00:06:55,890
this standard is going to be one hundred

130
00:06:55,890 --> 00:06:59,330
and forty eight point five megahertz

131
00:06:59,330 --> 00:07:02,010
this frequency is important we have to

132
00:07:02,010 --> 00:07:04,980
have in mind this frequency but in

133
00:07:04,980 --> 00:07:08,040
reality we are seeing only a portion of

134
00:07:08,040 --> 00:07:10,530
the screen a portion of this data

135
00:07:10,530 --> 00:07:12,990
because in reality the total quantity of

136
00:07:12,990 --> 00:07:16,110
data that is being transmitted is 2200

137
00:07:16,110 --> 00:07:24,330
pixels in in width and 1125 lines in

138
00:07:24,330 --> 00:07:27,030
hive so why is this happening because

139
00:07:27,030 --> 00:07:30,810
the display monitor needs this like

140
00:07:30,810 --> 00:07:33,540
blank spaces to synchronize its internal

141
00:07:33,540 --> 00:07:35,570
clocks so we have this horizontal

142
00:07:35,570 --> 00:07:38,640
backwards this horizontal front porch

143
00:07:38,640 --> 00:07:40,310
this horizontal synchronization

144
00:07:40,310 --> 00:07:42,780
synchronization pulls the vertical back

145
00:07:42,780 --> 00:07:45,060
porch and the vertical front porch and a

146
00:07:45,060 --> 00:07:47,940
vertical synchronization pulls so in

147
00:07:47,940 --> 00:07:49,680
total we are having this amount of

148
00:07:49,680 --> 00:07:52,040
information okay

149
00:07:52,040 --> 00:07:54,240
also it is important to have in mind

150
00:07:54,240 --> 00:07:58,410
that BG is an analog transmit and analog

151
00:07:58,410 --> 00:08:00,750
video interface that means that each

152
00:08:00,750 --> 00:08:04,640
channel is transmitting from 0 volts to

153
00:08:04,640 --> 00:08:07,410
0.7 volts sometimes larger more but

154
00:08:07,410 --> 00:08:10,590
normally this is this is a standard and

155
00:08:10,590 --> 00:08:14,940
to transmit up a middle tone red we will

156
00:08:14,940 --> 00:08:19,080
have to have go into the middle of the

157
00:08:19,080 --> 00:08:20,670
range for example here we are

158
00:08:20,670 --> 00:08:26,250
transmitting 0.35 bolts and green and

159
00:08:26,250 --> 00:08:28,910
blue channels must remain at zero volts

160
00:08:28,910 --> 00:08:32,400
if we want to transmit a pure red tone

161
00:08:32,400 --> 00:08:36,809
and we have to go to 0.7 volts and green

162
00:08:36,809 --> 00:08:39,059
and blue again have to remain at zero

163
00:08:39,059 --> 00:08:42,000
and this happens also with green and

164
00:08:42,000 --> 00:08:44,250
blue channels so the combination of

165
00:08:44,250 --> 00:08:46,770
these three channels and enraging from

166
00:08:46,770 --> 00:08:49,410
these zero volts to 0.7 volts is the

167
00:08:49,410 --> 00:08:52,680
weak is what makes the pearance of the

168
00:08:52,680 --> 00:08:55,440
of colors in each pixels in each pixel

169
00:08:55,440 --> 00:08:59,460
so we are appealing this line by line by

170
00:08:59,460 --> 00:09:02,340
line in kuala scholar rasterize it be

171
00:09:02,340 --> 00:09:03,300
your interface

172
00:09:03,300 --> 00:09:07,560
so we here we can see that we go to 1080

173
00:09:07,560 --> 00:09:11,870
lines but we have also 36 lines for the

174
00:09:11,870 --> 00:09:15,720
vertical back porch and 4 lines for the

175
00:09:15,720 --> 00:09:18,960
front vertical front porch and 5 lines

176
00:09:18,960 --> 00:09:21,480
for TC Clint synchronization pulls in

177
00:09:21,480 --> 00:09:23,430
this standard for example the pixel time

178
00:09:23,430 --> 00:09:26,880
is more or less 6.7 nanoseconds

179
00:09:26,880 --> 00:09:31,620
a line is 40 micro seconds a 4:14 excuse

180
00:09:31,620 --> 00:09:33,900
me microseconds and at in the total

181
00:09:33,900 --> 00:09:38,490
frame is going to be transmitted in 16

182
00:09:38,490 --> 00:09:42,720
milliseconds here we can see also what

183
00:09:42,720 --> 00:09:45,020
is happening when we are transmitting a

184
00:09:45,020 --> 00:09:48,210
grayscale from black to white for

185
00:09:48,210 --> 00:09:49,830
example in white we have the three

186
00:09:49,830 --> 00:09:53,360
channels going to the top of 0.7 volts

187
00:09:53,360 --> 00:09:59,280
but but in reality this is very real we

188
00:09:59,280 --> 00:10:01,310
win we will never achieve these square

189
00:10:01,310 --> 00:10:05,610
shapes of the signals so reality every

190
00:10:05,610 --> 00:10:08,700
pixel has more or less these different

191
00:10:08,700 --> 00:10:11,850
forums and the pixel shape is something

192
00:10:11,850 --> 00:10:14,040
we normally don't know for example here

193
00:10:14,040 --> 00:10:16,350
we have two manufacturers all different

194
00:10:16,350 --> 00:10:20,070
graphic cards and for example the green

195
00:10:20,070 --> 00:10:23,070
and the black shapes are for our throat

196
00:10:23,070 --> 00:10:24,300
meeting at exactly the same frequency

197
00:10:24,300 --> 00:10:26,610
same resolution and not transmitting

198
00:10:26,610 --> 00:10:28,770
exactly the same information but you can

199
00:10:28,770 --> 00:10:30,870
see that the pixel shape is really

200
00:10:30,870 --> 00:10:33,570
different why is this happening because

201
00:10:33,570 --> 00:10:36,260
from the manufacturers point of view

202
00:10:36,260 --> 00:10:38,490
what it is important is to ship

203
00:10:38,490 --> 00:10:41,280
appraises voltage at a precise time the

204
00:10:41,280 --> 00:10:43,230
rest of the pixel shape it really

205
00:10:43,230 --> 00:10:48,540
doesn't matter so this is something

206
00:10:48,540 --> 00:10:50,430
important to have in mind because at the

207
00:10:50,430 --> 00:10:53,280
end we are dealing with a very complex

208
00:10:53,280 --> 00:10:56,340
system what are the difficulties for

209
00:10:56,340 --> 00:11:00,090
capturing the video frames a first of

210
00:11:00,090 --> 00:11:02,100
all we are we all really know the

211
00:11:02,100 --> 00:11:04,110
spectral density of the source video

212
00:11:04,110 --> 00:11:07,410
signal hey we don't know the pixel shape

213
00:11:07,410 --> 00:11:11,700
we don't know how what data is being

214
00:11:11,700 --> 00:11:14,330
transmitted at that point in time and

215
00:11:14,330 --> 00:11:17,130
this also is barra into all the

216
00:11:17,130 --> 00:11:22,470
time so this is a one difficulty also we

217
00:11:22,470 --> 00:11:24,270
don't know the characteristics of the

218
00:11:24,270 --> 00:11:25,890
transmission line we have we know we

219
00:11:25,890 --> 00:11:29,340
have a cable but this cable normally

220
00:11:29,340 --> 00:11:33,420
what is in in length in longitude and we

221
00:11:33,420 --> 00:11:35,490
cannot characterize it and we all know

222
00:11:35,490 --> 00:11:39,870
how this acts as an antenna also and the

223
00:11:39,870 --> 00:11:42,300
signal symbols in this process are very

224
00:11:42,300 --> 00:11:45,750
low power as I told you the peak voltage

225
00:11:45,750 --> 00:11:50,400
for a wide-scale is 0.7 volts and this

226
00:11:50,400 --> 00:11:55,380
grid impedance in the system are more or

227
00:11:55,380 --> 00:11:57,860
less in the in the range of milliwatts

228
00:11:57,860 --> 00:12:00,530
so also we have electromagnetic

229
00:12:00,530 --> 00:12:03,450
interference from the Biderman and we

230
00:12:03,450 --> 00:12:06,780
also have a transfer function function

231
00:12:06,780 --> 00:12:10,170
that we normally don't know now we add

232
00:12:10,170 --> 00:12:13,800
the at the receiver also we are

233
00:12:13,800 --> 00:12:15,900
transmitting Tricia a we have three

234
00:12:15,900 --> 00:12:18,270
channels that are transmitting it's one

235
00:12:18,270 --> 00:12:22,740
each a sequence of data information and

236
00:12:22,740 --> 00:12:25,890
this probably goes to a construct or

237
00:12:25,890 --> 00:12:29,610
destructive interference so these are

238
00:12:29,610 --> 00:12:31,380
the difficulties that make the since

239
00:12:31,380 --> 00:12:35,940
system work like work like a block black

240
00:12:35,940 --> 00:12:38,760
box but we also have some advantages for

241
00:12:38,760 --> 00:12:41,580
example we know that there is a

242
00:12:41,580 --> 00:12:44,120
streaming a constant streaming of video

243
00:12:44,120 --> 00:12:46,740
signals at the moment the display

244
00:12:46,740 --> 00:12:49,740
monitor is going to stop we will not we

245
00:12:49,740 --> 00:12:52,650
know that if we are seeing information

246
00:12:52,650 --> 00:12:55,260
we are between video in the explained

247
00:12:55,260 --> 00:12:59,370
monitor is because there are information

248
00:12:59,370 --> 00:13:02,880
traveling through the cable also we know

249
00:13:02,880 --> 00:13:04,470
that there is an underline central

250
00:13:04,470 --> 00:13:08,010
frequency and this frequency for the

251
00:13:08,010 --> 00:13:12,120
specific case of the full HD is again

252
00:13:12,120 --> 00:13:14,970
one hundred forty eight point seven

253
00:13:14,970 --> 00:13:20,570
megahertz at 60 Hertz of refresh rate

254
00:13:20,570 --> 00:13:24,930
this frequency we also know we it can

255
00:13:24,930 --> 00:13:29,520
prove that the spectrum of the baseband

256
00:13:29,520 --> 00:13:30,770
signal

257
00:13:30,770 --> 00:13:33,380
is going to be repeated itself at the

258
00:13:33,380 --> 00:13:36,790
harmonics of the of this pixel frequency

259
00:13:36,790 --> 00:13:39,830
and we also know that the one width of

260
00:13:39,830 --> 00:13:42,140
the received signal is suspected to be

261
00:13:42,140 --> 00:13:45,940
less or equal of of this frequency

262
00:13:45,940 --> 00:13:48,980
finally wheaton we also know that there

263
00:13:48,980 --> 00:13:51,170
is a correlation ship between the

264
00:13:51,170 --> 00:13:52,810
amplitude of the signal and the

265
00:13:52,810 --> 00:13:57,050
intensity of the pixels in the

266
00:13:57,050 --> 00:13:58,730
transmission in the india in the

267
00:13:58,730 --> 00:14:02,029
transmitted data this correlation ship

268
00:14:02,029 --> 00:14:05,390
means that it is not equal but from this

269
00:14:05,390 --> 00:14:10,040
we can reconstruct behaving so what tool

270
00:14:10,040 --> 00:14:13,160
we are using for this and normally we we

271
00:14:13,160 --> 00:14:15,860
can use an SDR the main reasons are

272
00:14:15,860 --> 00:14:18,200
because obviously because they are cheap

273
00:14:18,200 --> 00:14:20,720
but also because they have they can be

274
00:14:20,720 --> 00:14:23,209
tuned in a wide range of frequencies

275
00:14:23,209 --> 00:14:25,730
from 1 megahertz to 60 Hertz for example

276
00:14:25,730 --> 00:14:29,240
this one and they also have a lesson

277
00:14:29,240 --> 00:14:32,120
bandwidth for these tags mmm for example

278
00:14:32,120 --> 00:14:37,029
this one has a 20 megahertz of bandwidth

279
00:14:37,029 --> 00:14:39,890
maybe this is not enough to recover the

280
00:14:39,890 --> 00:14:42,770
whole spectral density of the base base

281
00:14:42,770 --> 00:14:47,810
band signal but it allow us to recover

282
00:14:47,810 --> 00:14:52,070
part of the information so we did a

283
00:14:52,070 --> 00:14:54,020
little experiment here in green I am

284
00:14:54,020 --> 00:14:56,890
showing you the harmonics of this

285
00:14:56,890 --> 00:15:03,320
frequency of foolish D again 148 point 5

286
00:15:03,320 --> 00:15:05,570
megahertz here is the field the second

287
00:15:05,570 --> 00:15:08,060
theorem so in a harmonic and in blue I

288
00:15:08,060 --> 00:15:10,100
am showing you the quality of the

289
00:15:10,100 --> 00:15:12,529
recovered video frames so at the phaser

290
00:15:12,529 --> 00:15:15,279
money we can see that we can recover

291
00:15:15,279 --> 00:15:18,529
somehow the information also the quality

292
00:15:18,529 --> 00:15:21,680
is not very good at the second and third

293
00:15:21,680 --> 00:15:22,399
harmonic

294
00:15:22,399 --> 00:15:25,670
we are not easily we can not recover

295
00:15:25,670 --> 00:15:27,560
easily the information may be there is

296
00:15:27,560 --> 00:15:29,270
there is interference in that part of

297
00:15:29,270 --> 00:15:31,820
the spectrum but at the fourth and fifth

298
00:15:31,820 --> 00:15:34,040
harmonic the information can be

299
00:15:34,040 --> 00:15:37,730
recovered easily who does the look this

300
00:15:37,730 --> 00:15:41,540
look here we have a reference image here

301
00:15:41,540 --> 00:15:43,769
is how it looks at

302
00:15:43,769 --> 00:15:48,660
at recording at these Philharmonic and

303
00:15:48,660 --> 00:15:50,279
you can see it there is information

304
00:15:50,279 --> 00:15:52,889
there although it is not very good and

305
00:15:52,889 --> 00:15:56,040
the second ceremony you cannot see

306
00:15:56,040 --> 00:15:58,170
anything but at the fourth and fifth

307
00:15:58,170 --> 00:16:01,079
harmonic harmonic you really can see

308
00:16:01,079 --> 00:16:04,170
that there is a reconstruction of the

309
00:16:04,170 --> 00:16:08,839
image there okay so how this is done

310
00:16:08,839 --> 00:16:11,069
here I'm showing you the spectral

311
00:16:11,069 --> 00:16:18,360
density of the SDR centred at 610

312
00:16:18,360 --> 00:16:22,559
megahertz and you can see that this is

313
00:16:22,559 --> 00:16:25,529
all the energy that is being transmitted

314
00:16:25,529 --> 00:16:27,869
by the video interface why do we know

315
00:16:27,869 --> 00:16:31,489
this because here in this waterfall plot

316
00:16:31,489 --> 00:16:34,619
we can see how this was transmitting

317
00:16:34,619 --> 00:16:38,879
information here we turn off the play

318
00:16:38,879 --> 00:16:41,730
monitor and here we turn it on again and

319
00:16:41,730 --> 00:16:45,540
we can is evidence that the energy goes

320
00:16:45,540 --> 00:16:48,839
away and comes back so how do we recover

321
00:16:48,839 --> 00:16:52,699
this and as I told you the amplitude the

322
00:16:52,699 --> 00:16:55,529
envelope of the baseband signal is

323
00:16:55,529 --> 00:16:58,139
related to the intensity of the pixels

324
00:16:58,139 --> 00:17:02,069
so at the end what we're doing is an and

325
00:17:02,069 --> 00:17:04,829
AM the modulation and this is something

326
00:17:04,829 --> 00:17:06,869
that is trivial on an SDR because in and

327
00:17:06,869 --> 00:17:09,480
as here to obtain the amplitude of a

328
00:17:09,480 --> 00:17:12,689
baseband signal we only have to obtain

329
00:17:12,689 --> 00:17:15,119
the amplitude of the in face and color

330
00:17:15,119 --> 00:17:22,380
to information of the of the SDR so once

331
00:17:22,380 --> 00:17:27,329
we have our set of amplitudes we have to

332
00:17:27,329 --> 00:17:28,740
go to a process that is called Rho

333
00:17:28,740 --> 00:17:30,990
sampling for example imagine we have

334
00:17:30,990 --> 00:17:33,600
this reference image and in this

335
00:17:33,600 --> 00:17:37,770
reference image if we are sampling at 20

336
00:17:37,770 --> 00:17:40,529
megahertz if you do the maths you will

337
00:17:40,529 --> 00:17:42,870
reach to a point that per line we are

338
00:17:42,870 --> 00:17:48,240
obtaining 148 samples but in reality we

339
00:17:48,240 --> 00:17:51,149
know that one line is composed of two

340
00:17:51,149 --> 00:17:56,220
thousand two hundred pixels so how do we

341
00:17:56,220 --> 00:17:57,539
convert this buffer

342
00:17:57,539 --> 00:17:59,669
into this buffer okay it's a matter or

343
00:17:59,669 --> 00:18:02,639
something where we have to feed the data

344
00:18:02,639 --> 00:18:07,080
in this buffer maybe repeat for example

345
00:18:07,080 --> 00:18:12,419
this may be something like a fourth 15

346
00:18:12,419 --> 00:18:15,119
times but somewhere but maybe the first

347
00:18:15,119 --> 00:18:18,059
sample 15 times the second sample 14

348
00:18:18,059 --> 00:18:19,889
times this is done very carefully if you

349
00:18:19,889 --> 00:18:22,289
don't do these very carefully in a

350
00:18:22,289 --> 00:18:24,840
synchronized way you will not be able to

351
00:18:24,840 --> 00:18:29,129
obtain your video frames here you can

352
00:18:29,129 --> 00:18:31,470
see after the resampling a process we

353
00:18:31,470 --> 00:18:32,940
can see that there is something there

354
00:18:32,940 --> 00:18:34,710
but it is very ugly

355
00:18:34,710 --> 00:18:39,090
it is terrible how do we improve this we

356
00:18:39,090 --> 00:18:42,109
use a serial of post-processing steps

357
00:18:42,109 --> 00:18:45,359
where for example we use low-pass

358
00:18:45,359 --> 00:18:47,909
filtering out again frame averaging and

359
00:18:47,909 --> 00:18:51,570
we go from this to this which is a lot

360
00:18:51,570 --> 00:18:56,849
better so anyway at this point the frame

361
00:18:56,849 --> 00:19:00,389
is still not synchronized because we are

362
00:19:00,389 --> 00:19:02,879
not seeing you can see it is not

363
00:19:02,879 --> 00:19:04,470
organized that we have to find this

364
00:19:04,470 --> 00:19:07,259
point and after finding this point we

365
00:19:07,259 --> 00:19:09,570
reorganize the information we start to

366
00:19:09,570 --> 00:19:12,119
explain this plain that at the beginning

367
00:19:12,119 --> 00:19:15,929
of this point so now here I'm showing

368
00:19:15,929 --> 00:19:19,019
you a little spring man with it they say

369
00:19:19,019 --> 00:19:22,559
that the setup here it shows an HDMI

370
00:19:22,559 --> 00:19:26,099
interface and we can see that at 5

371
00:19:26,099 --> 00:19:29,690
meters we were recovering video frames

372
00:19:29,690 --> 00:19:32,840
it is everything here that we can see

373
00:19:32,840 --> 00:19:37,710
what the display monitor X is is showing

374
00:19:37,710 --> 00:19:42,869
so the conclusions of this first part

375
00:19:42,869 --> 00:19:47,309
are we cannot recover color but this

376
00:19:47,309 --> 00:19:49,169
doesn't matter because for the covering

377
00:19:49,169 --> 00:19:53,009
text the the system is very suitable

378
00:19:53,009 --> 00:19:56,399
because normally text is a high contrast

379
00:19:56,399 --> 00:20:01,080
information you know text is black in in

380
00:20:01,080 --> 00:20:04,889
front of white background and this tool

381
00:20:04,889 --> 00:20:07,499
will work very good for this so having

382
00:20:07,499 --> 00:20:09,899
this in mind I give the floor to hyung

383
00:20:09,899 --> 00:20:11,220
who is going to explain

384
00:20:11,220 --> 00:20:14,010
we can recover text from this so thank

385
00:20:14,010 --> 00:20:20,880
you very much okay thanks Santiago I'm

386
00:20:20,880 --> 00:20:24,169
gonna show my screen now

387
00:20:27,130 --> 00:20:31,150
okay so hello everyone again thanks for

388
00:20:31,150 --> 00:20:35,980
attending this webinar now I will that

389
00:20:35,980 --> 00:20:38,350
is that I will show you how to process

390
00:20:38,350 --> 00:20:41,890
the intercepted images and so that we

391
00:20:41,890 --> 00:20:44,890
can obtain sharp images from which we

392
00:20:44,890 --> 00:20:46,410
can actually extract information

393
00:20:46,410 --> 00:20:51,130
particular text information so I will

394
00:20:51,130 --> 00:20:53,740
start by explaining first most important

395
00:20:53,740 --> 00:20:56,680
concepts of this deep learning and why

396
00:20:56,680 --> 00:20:59,470
we are deciding to go with a data-driven

397
00:20:59,470 --> 00:21:02,740
approach then I will go through the work

398
00:21:02,740 --> 00:21:05,860
with conductor to capture the data train

399
00:21:05,860 --> 00:21:08,740
and evaluate the models that we've been

400
00:21:08,740 --> 00:21:11,620
working with to pre-process the images

401
00:21:11,620 --> 00:21:14,050
so that we at the end can extract

402
00:21:14,050 --> 00:21:19,720
information from these images so first I

403
00:21:19,720 --> 00:21:21,010
would like to start by briefly

404
00:21:21,010 --> 00:21:23,560
explaining the classic approach to

405
00:21:23,560 --> 00:21:25,990
programming with a with a simple example

406
00:21:25,990 --> 00:21:28,480
in the field of image processing let's

407
00:21:28,480 --> 00:21:32,350
say we want to segment or extract the

408
00:21:32,350 --> 00:21:35,520
position of this red ball in the image

409
00:21:35,520 --> 00:21:38,410
we can do this simply by taking the

410
00:21:38,410 --> 00:21:40,840
information from the image that is in

411
00:21:40,840 --> 00:21:42,550
the three color channel that are

412
00:21:42,550 --> 00:21:45,280
represented by three matrices and

413
00:21:45,280 --> 00:21:49,620
defined fixed thresholds so that we can

414
00:21:49,620 --> 00:21:52,390
extract the color and then calculate the

415
00:21:52,390 --> 00:21:55,420
center of the particular color this is

416
00:21:55,420 --> 00:21:57,880
something that works pretty well that

417
00:21:57,880 --> 00:22:01,960
can be done easily but then what happens

418
00:22:01,960 --> 00:22:03,910
if we want to for example detect another

419
00:22:03,910 --> 00:22:06,610
color or if the lighting conditions

420
00:22:06,610 --> 00:22:09,100
change there's a lot of things that can

421
00:22:09,100 --> 00:22:11,590
change in this setting and the threshold

422
00:22:11,590 --> 00:22:14,050
that we defined from the our knowledge

423
00:22:14,050 --> 00:22:16,210
of the image and of the color that we

424
00:22:16,210 --> 00:22:18,940
wanted to get will stop applying so this

425
00:22:18,940 --> 00:22:21,130
is something that happens a lot in

426
00:22:21,130 --> 00:22:23,130
real-world problems and in particular in

427
00:22:23,130 --> 00:22:26,070
computer vision problems

428
00:22:26,070 --> 00:22:29,050
so now following this classical paradigm

429
00:22:29,050 --> 00:22:31,990
in a particular case several approaches

430
00:22:31,990 --> 00:22:34,840
have been studied in the field of image

431
00:22:34,840 --> 00:22:37,600
denoising in general these approaches

432
00:22:37,600 --> 00:22:39,970
rely on the knowledge that we have

433
00:22:39,970 --> 00:22:42,940
of the particular images and of the

434
00:22:42,940 --> 00:22:45,549
noise distribution that we want to

435
00:22:45,549 --> 00:22:49,299
eliminate so there is one of the state

436
00:22:49,299 --> 00:22:51,129
of the art classical non learn

437
00:22:51,129 --> 00:22:53,350
approaches that's called collaborative

438
00:22:53,350 --> 00:22:57,100
filtering and we can see how this works

439
00:22:57,100 --> 00:22:59,470
in in these images it works pretty well

440
00:22:59,470 --> 00:23:03,039
for the removal of additive white

441
00:23:03,039 --> 00:23:06,009
Gaussian noise but if we try to process

442
00:23:06,009 --> 00:23:08,620
our intercepted images with this we get

443
00:23:08,620 --> 00:23:11,440
some pretty poor results and this is due

444
00:23:11,440 --> 00:23:13,269
to the fact that these images are

445
00:23:13,269 --> 00:23:15,669
distorted by several effects that are

446
00:23:15,669 --> 00:23:19,419
not only additive Gaussian noise and we

447
00:23:19,419 --> 00:23:20,909
don't know the distribution of these

448
00:23:20,909 --> 00:23:25,090
effects so this doesn't work in general

449
00:23:25,090 --> 00:23:29,710
for our particular case um then if we

450
00:23:29,710 --> 00:23:32,710
were for example to take one of the text

451
00:23:32,710 --> 00:23:35,950
image and additive white Gaussian noise

452
00:23:35,950 --> 00:23:38,409
with a distribution that we know then

453
00:23:38,409 --> 00:23:40,149
this algorithm would be really good at

454
00:23:40,149 --> 00:23:42,549
eliminating that noise it's only because

455
00:23:42,549 --> 00:23:44,710
we synthetically added a known

456
00:23:44,710 --> 00:23:47,830
distribution of Y tau from X so this one

457
00:23:47,830 --> 00:23:50,220
applied for four images that's why

458
00:23:50,220 --> 00:23:52,899
that's the main motivation to go with a

459
00:23:52,899 --> 00:23:58,299
data-driven approach and so different

460
00:23:58,299 --> 00:24:01,539
from the paradigm of defining hard-coded

461
00:24:01,539 --> 00:24:04,779
rules we have the paradigm of much

462
00:24:04,779 --> 00:24:11,559
learning where what we do is based on

463
00:24:11,559 --> 00:24:16,870
data we train or adapt mathematical

464
00:24:16,870 --> 00:24:18,549
model which can be a neural network for

465
00:24:18,549 --> 00:24:21,039
example we are that we train its

466
00:24:21,039 --> 00:24:25,139
parameters so that it performs a task or

467
00:24:25,139 --> 00:24:28,179
resembles a function that is described

468
00:24:28,179 --> 00:24:30,940
by the data that we're giving in so for

469
00:24:30,940 --> 00:24:34,210
example we had the problem common

470
00:24:34,210 --> 00:24:37,629
problem in in image processing in

471
00:24:37,629 --> 00:24:39,610
computer vision which is called image

472
00:24:39,610 --> 00:24:43,000
segmentation which basically consists in

473
00:24:43,000 --> 00:24:46,120
taking an image and labeling each pixel

474
00:24:46,120 --> 00:24:48,960
depending on whether it corresponds to

475
00:24:48,960 --> 00:24:51,759
let's say a person or a motorcycle or

476
00:24:51,759 --> 00:24:53,950
any kind of object

477
00:24:53,950 --> 00:24:55,929
so this is a problem that would be

478
00:24:55,929 --> 00:24:58,240
really hard to solve by classical

479
00:24:58,240 --> 00:25:02,049
approaches so what we do here is we take

480
00:25:02,049 --> 00:25:05,200
images then we pass it through a madman

481
00:25:05,200 --> 00:25:08,169
so Network which initially has random

482
00:25:08,169 --> 00:25:10,990
parameters random waves then it will

483
00:25:10,990 --> 00:25:12,340
perform a prediction which will be

484
00:25:12,340 --> 00:25:14,380
really poor then we can compare that

485
00:25:14,380 --> 00:25:16,480
prediction with data that has been

486
00:25:16,480 --> 00:25:20,620
previously labeled then we compute an

487
00:25:20,620 --> 00:25:22,870
error or loss and through a learning

488
00:25:22,870 --> 00:25:25,179
algorithm or an optimization algorithm

489
00:25:25,179 --> 00:25:27,909
we update the weights of the neural

490
00:25:27,909 --> 00:25:30,760
network so that it does a better

491
00:25:30,760 --> 00:25:32,830
prediction so the idea behind this is

492
00:25:32,830 --> 00:25:35,830
that the more data that we show to the

493
00:25:35,830 --> 00:25:38,080
model the battery will start performing

494
00:25:38,080 --> 00:25:40,019
all of the better it will start

495
00:25:40,019 --> 00:25:44,470
resembling the data and this works

496
00:25:44,470 --> 00:25:46,899
pretty well in a lot of for a lot of

497
00:25:46,899 --> 00:25:50,429
problems and has made it possible to

498
00:25:50,429 --> 00:25:53,320
tackle problems whose complexity was

499
00:25:53,320 --> 00:25:55,299
impacts it was impossible to attack with

500
00:25:55,299 --> 00:25:59,500
classical approaches so for example this

501
00:25:59,500 --> 00:26:02,980
tweet says gradient descent can write

502
00:26:02,980 --> 00:26:05,679
code better than you in some cases or in

503
00:26:05,679 --> 00:26:07,510
a lot of cases that is actually true

504
00:26:07,510 --> 00:26:09,340
but of course my coloring is not a

505
00:26:09,340 --> 00:26:13,149
silver bullet to be it's mostly limited

506
00:26:13,149 --> 00:26:16,360
to certain domains and regularly those

507
00:26:16,360 --> 00:26:19,510
were capturing large amounts of data is

508
00:26:19,510 --> 00:26:22,360
feasible or and where the task

509
00:26:22,360 --> 00:26:24,820
complexity justifies capturing all these

510
00:26:24,820 --> 00:26:31,570
big amounts of data so now we have like

511
00:26:31,570 --> 00:26:33,549
I mentioned before what one of the most

512
00:26:33,549 --> 00:26:36,669
common models that are trained through

513
00:26:36,669 --> 00:26:38,860
many learning are neural networks and to

514
00:26:38,860 --> 00:26:40,269
understand their networks we must first

515
00:26:40,269 --> 00:26:42,820
understand the perception which is

516
00:26:42,820 --> 00:26:44,830
simply a mathematical model or a

517
00:26:44,830 --> 00:26:46,720
mathematical function which computes a

518
00:26:46,720 --> 00:26:49,840
weighted sum of a series of inputs and

519
00:26:49,840 --> 00:26:52,029
the capacities sum through what's called

520
00:26:52,029 --> 00:26:54,190
an activation function and there behind

521
00:26:54,190 --> 00:26:56,580
behind this weighted sum is that we can

522
00:26:56,580 --> 00:27:00,779
tweak these parameters so that they

523
00:27:00,779 --> 00:27:03,190
perform the function that we want them

524
00:27:03,190 --> 00:27:06,490
to as good as possible so a really

525
00:27:06,490 --> 00:27:07,330
simple example

526
00:27:07,330 --> 00:27:10,059
for this is linear regression we we

527
00:27:10,059 --> 00:27:11,769
train these two parameters which are the

528
00:27:11,769 --> 00:27:14,559
slope of the core and intersection with

529
00:27:14,559 --> 00:27:19,539
the y-axis so that they are as close to

530
00:27:19,539 --> 00:27:23,470
a given data as possible and something

531
00:27:23,470 --> 00:27:26,559
that's really commonly done also another

532
00:27:26,559 --> 00:27:28,330
problem that can be solved through one

533
00:27:28,330 --> 00:27:30,369
of these perceptions is the promote

534
00:27:30,369 --> 00:27:33,070
linear classification so we in the same

535
00:27:33,070 --> 00:27:34,809
way we will try to find the parameters

536
00:27:34,809 --> 00:27:36,600
that describe a binary decision line

537
00:27:36,600 --> 00:27:40,149
finally some really important part the

538
00:27:40,149 --> 00:27:41,830
perception is the Sigma is the

539
00:27:41,830 --> 00:27:43,629
activation function on one example of

540
00:27:43,629 --> 00:27:45,179
this is for example the Sigma function

541
00:27:45,179 --> 00:27:48,210
which is widely used for example to

542
00:27:48,210 --> 00:27:51,129
transform this course that we can get

543
00:27:51,129 --> 00:27:53,440
from from these equation that defines

544
00:27:53,440 --> 00:27:56,019
the decision boundary to something that

545
00:27:56,019 --> 00:27:59,499
is closer to a probability is bounded

546
00:27:59,499 --> 00:28:04,480
between 0 on 1 so that if we get a data

547
00:28:04,480 --> 00:28:07,419
point that is on the line for example it

548
00:28:07,419 --> 00:28:09,580
would give us a score of 0 and it would

549
00:28:09,580 --> 00:28:12,460
give us a probability of 0.5 which makes

550
00:28:12,460 --> 00:28:14,259
sense if we have two classes fifty

551
00:28:14,259 --> 00:28:15,879
percent probability of belonging to a

552
00:28:15,879 --> 00:28:18,940
certain class and then if their score is

553
00:28:18,940 --> 00:28:20,619
large then it will give us a body closer

554
00:28:20,619 --> 00:28:25,299
to 1 or 100 percent but what happens

555
00:28:25,299 --> 00:28:28,029
when the phenomena that we want to model

556
00:28:28,029 --> 00:28:31,299
with these neural networks are more

557
00:28:31,299 --> 00:28:36,399
complex and have nominees one is well

558
00:28:36,399 --> 00:28:38,950
it's not is basically adding more of

559
00:28:38,950 --> 00:28:41,460
these units more of these neurons and

560
00:28:41,460 --> 00:28:45,279
perform and do have several layers of

561
00:28:45,279 --> 00:28:47,019
this so that we can achieve these

562
00:28:47,019 --> 00:28:52,359
nonlinearities and to train these models

563
00:28:52,359 --> 00:28:55,720
the the procedure basically can be

564
00:28:55,720 --> 00:28:58,899
summarized in first computing a loss

565
00:28:58,899 --> 00:29:01,480
function that tells us how good our

566
00:29:01,480 --> 00:29:07,509
model is is describes the data how close

567
00:29:07,509 --> 00:29:10,600
it is to the data and then we use

568
00:29:10,600 --> 00:29:13,179
gradient descent for that we calculate

569
00:29:13,179 --> 00:29:15,309
the gradient of the loss function with

570
00:29:15,309 --> 00:29:17,259
respect to the parameters of the model

571
00:29:17,259 --> 00:29:20,419
and that will tell us that

572
00:29:20,419 --> 00:29:21,950
correction in which the weights should

573
00:29:21,950 --> 00:29:24,830
be updated so that the model performs

574
00:29:24,830 --> 00:29:30,320
better or or it has a smaller loss so

575
00:29:30,320 --> 00:29:32,059
that's that can be seen in these figure

576
00:29:32,059 --> 00:29:34,039
here for example we start with random

577
00:29:34,039 --> 00:29:36,799
weights and a high loss that means our

578
00:29:36,799 --> 00:29:39,529
model performs poorly and then as we

579
00:29:39,529 --> 00:29:41,600
move on it starts moving in the

580
00:29:41,600 --> 00:29:45,679
direction where the loss decreases until

581
00:29:45,679 --> 00:29:48,320
it reaches an optimal point of course

582
00:29:48,320 --> 00:29:49,580
this is not always that simple

583
00:29:49,580 --> 00:29:52,879
there can be local minima or other

584
00:29:52,879 --> 00:29:55,580
effects that that affect this but that's

585
00:29:55,580 --> 00:29:58,399
the basic principle now that we

586
00:29:58,399 --> 00:30:00,470
discussed neural networks how do we use

587
00:30:00,470 --> 00:30:03,489
them for image processing usually it

588
00:30:03,489 --> 00:30:06,529
well it becomes almost impossible to use

589
00:30:06,529 --> 00:30:08,570
this type of more to their perceptions

590
00:30:08,570 --> 00:30:10,700
for images mostly because it will

591
00:30:10,700 --> 00:30:12,590
require a large amount of parameters and

592
00:30:12,590 --> 00:30:16,340
also there's no important fact and

593
00:30:16,340 --> 00:30:18,379
instead statistics of natural images

594
00:30:18,379 --> 00:30:21,409
away in variants which are intrinsic

595
00:30:21,409 --> 00:30:23,239
correlations among pixels that means

596
00:30:23,239 --> 00:30:25,730
that if we have an image of a cat and we

597
00:30:25,730 --> 00:30:27,259
move the cat inside the image it will

598
00:30:27,259 --> 00:30:30,080
still be a cat or if we rotate the cat

599
00:30:30,080 --> 00:30:32,029
it will still be a cat so that's where

600
00:30:32,029 --> 00:30:34,309
this operation emerges and this type of

601
00:30:34,309 --> 00:30:36,080
neural networks merge the convolutional

602
00:30:36,080 --> 00:30:39,710
neural networks the commotion works I'm

603
00:30:39,710 --> 00:30:41,749
going to eat a bad it can be it's

604
00:30:41,749 --> 00:30:44,149
depicted in these figure here will have

605
00:30:44,149 --> 00:30:45,679
what's called a convolutional kernel

606
00:30:45,679 --> 00:30:48,529
that slides through the input or through

607
00:30:48,529 --> 00:30:51,590
the image basically and computes output

608
00:30:51,590 --> 00:30:54,350
and it does so sharing weights among the

609
00:30:54,350 --> 00:30:56,419
whole image which reduces the parameters

610
00:30:56,419 --> 00:30:58,369
and takes advantage about the fact that

611
00:30:58,369 --> 00:31:01,279
of there's these intrinsic correlations

612
00:31:01,279 --> 00:31:05,869
between pixels so basically what

613
00:31:05,869 --> 00:31:07,639
convolutional neural networks end up

614
00:31:07,639 --> 00:31:09,230
doing is learning hierarchical

615
00:31:09,230 --> 00:31:13,639
representations of the images so for

616
00:31:13,639 --> 00:31:15,409
example here in this image

617
00:31:15,409 --> 00:31:17,059
classification task they'll learn to

618
00:31:17,059 --> 00:31:20,269
first detect edges then detect more

619
00:31:20,269 --> 00:31:21,200
complex shapes

620
00:31:21,200 --> 00:31:25,639
blobs and then at the end in a deeper

621
00:31:25,639 --> 00:31:27,980
layer they will be able to take things

622
00:31:27,980 --> 00:31:31,159
like eyes or mouth or other types of

623
00:31:31,159 --> 00:31:34,290
objects and by extracting these features

624
00:31:34,290 --> 00:31:36,450
then we can do real interesting tasks

625
00:31:36,450 --> 00:31:40,380
like image classification and this is

626
00:31:40,380 --> 00:31:43,010
one of the things that make this type of

627
00:31:43,010 --> 00:31:46,680
models to explode basically and to have

628
00:31:46,680 --> 00:31:47,970
a lot of people using them in the

629
00:31:47,970 --> 00:31:51,450
Philips computer vision is that in 2012

630
00:31:51,450 --> 00:31:54,720
and the progress and the advancements of

631
00:31:54,720 --> 00:31:58,350
GPUs made it possible to use these large

632
00:31:58,350 --> 00:32:00,330
models that emotional networks to

633
00:32:00,330 --> 00:32:03,600
process images and it allows to allow to

634
00:32:03,600 --> 00:32:08,580
increase a lot the performance of these

635
00:32:08,580 --> 00:32:10,920
type of models of image because any

636
00:32:10,920 --> 00:32:13,560
modification test so since then

637
00:32:13,560 --> 00:32:16,140
convolution learners have taken over the

638
00:32:16,140 --> 00:32:19,080
field of computer vision basically ok so

639
00:32:19,080 --> 00:32:23,010
now after discussing these topics we go

640
00:32:23,010 --> 00:32:26,780
back to to to what we actually do to

641
00:32:26,780 --> 00:32:29,160
remove the noise out of this images

642
00:32:29,160 --> 00:32:33,420
which is what we care about so we follow

643
00:32:33,420 --> 00:32:36,270
is we first capture the data and for

644
00:32:36,270 --> 00:32:39,600
this we do is we have these images these

645
00:32:39,600 --> 00:32:42,420
base images that are displayed in a in a

646
00:32:42,420 --> 00:32:46,170
monitor and they have these calibration

647
00:32:46,170 --> 00:32:47,300
patterns here

648
00:32:47,300 --> 00:32:49,830
then we intercept these images through

649
00:32:49,830 --> 00:32:52,950
the process explained by Santiago after

650
00:32:52,950 --> 00:32:54,480
we find the Centers of these images

651
00:32:54,480 --> 00:32:59,190
through trash folding and we align the

652
00:32:59,190 --> 00:33:01,530
Centers of the intercepted and the

653
00:33:01,530 --> 00:33:03,590
original image this allows us to get

654
00:33:03,590 --> 00:33:07,530
aligned pairs of images which is really

655
00:33:07,530 --> 00:33:08,730
important because originally these

656
00:33:08,730 --> 00:33:11,190
images were misaligned displaced and

657
00:33:11,190 --> 00:33:15,030
resized and then we will we do something

658
00:33:15,030 --> 00:33:16,830
called data augmentation which allows us

659
00:33:16,830 --> 00:33:19,380
to have more data and more variability

660
00:33:19,380 --> 00:33:21,720
inside between some of the data so that

661
00:33:21,720 --> 00:33:24,540
we can our model is a even better at

662
00:33:24,540 --> 00:33:28,830
generalizing for new types of data mmm

663
00:33:28,830 --> 00:33:32,340
then once we have this data we what we

664
00:33:32,340 --> 00:33:35,040
do is basically trainer model and in a

665
00:33:35,040 --> 00:33:37,530
fashion pretty similar to what I

666
00:33:37,530 --> 00:33:41,070
explained earlier is we have this set of

667
00:33:41,070 --> 00:33:42,890
nice images we pass them through the

668
00:33:42,890 --> 00:33:45,900
neural network this gives a prediction

669
00:33:45,900 --> 00:33:46,490
which

670
00:33:46,490 --> 00:33:47,930
at the beginning really bad then we

671
00:33:47,930 --> 00:33:50,330
compare that with the original images

672
00:33:50,330 --> 00:33:53,180
and then these gives us an error we

673
00:33:53,180 --> 00:33:55,160
calculate the gradient and then we do

674
00:33:55,160 --> 00:33:56,450
gradient descent to update the

675
00:33:56,450 --> 00:33:58,310
parameters of the model and it starts

676
00:33:58,310 --> 00:33:59,870
doing better and better and this process

677
00:33:59,870 --> 00:34:03,680
repeats for all the data a lot of times

678
00:34:03,680 --> 00:34:07,430
and then once we have this model what we

679
00:34:07,430 --> 00:34:08,929
can do is inference which is basically

680
00:34:08,929 --> 00:34:11,360
showing you images to the model and it

681
00:34:11,360 --> 00:34:14,120
will perform the task on these new

682
00:34:14,120 --> 00:34:17,719
images the connection test the first

683
00:34:17,719 --> 00:34:20,179
architecture we tested was based on the

684
00:34:20,179 --> 00:34:23,510
work by MA on deep learning based scan

685
00:34:23,510 --> 00:34:25,639
texture blurring and these basically

686
00:34:25,639 --> 00:34:30,580
performs pixel level regression so

687
00:34:30,580 --> 00:34:35,600
bypassing the noisy and original images

688
00:34:35,600 --> 00:34:41,750
this model learns the inverse of the

689
00:34:41,750 --> 00:34:45,679
noise transformation that order yeah the

690
00:34:45,679 --> 00:34:47,330
nice transformation that the images are

691
00:34:47,330 --> 00:34:49,820
subject to due to the interception

692
00:34:49,820 --> 00:34:52,370
process so it does the inverse and then

693
00:34:52,370 --> 00:34:57,110
it predicts the original image from the

694
00:34:57,110 --> 00:34:59,600
noisy images since we're dealing with

695
00:34:59,600 --> 00:35:05,810
regression here we use mean minimum

696
00:35:05,810 --> 00:35:09,140
square error loss on a pixel level and

697
00:35:09,140 --> 00:35:12,590
we can see how these loss decreases when

698
00:35:12,590 --> 00:35:14,000
we train this model so that means this

699
00:35:14,000 --> 00:35:18,170
model is actually learning to make the

700
00:35:18,170 --> 00:35:20,150
pixels of the outputs more and more

701
00:35:20,150 --> 00:35:22,790
similar to the original to the original

702
00:35:22,790 --> 00:35:26,750
pixels and so particular is about this

703
00:35:26,750 --> 00:35:29,900
model are the it uses residual receiver

704
00:35:29,900 --> 00:35:31,700
connections which is something really

705
00:35:31,700 --> 00:35:36,400
important in most of this type of models

706
00:35:36,400 --> 00:35:39,500
to allow to allow us to train these

707
00:35:39,500 --> 00:35:43,660
these big models without having the

708
00:35:43,660 --> 00:35:46,130
gradient die in the back propagation

709
00:35:46,130 --> 00:35:48,500
process which is a particularity of

710
00:35:48,500 --> 00:35:51,980
these large models so these residual

711
00:35:51,980 --> 00:35:54,920
connections are really important and the

712
00:35:54,920 --> 00:35:57,470
second model that we tried was is called

713
00:35:57,470 --> 00:35:59,150
FC Hartmann and he's based on the world

714
00:35:59,150 --> 00:36:00,320
by child on if he

715
00:36:00,320 --> 00:36:03,260
image classification it basically has

716
00:36:03,260 --> 00:36:05,510
that encoder/decoder structure this

717
00:36:05,510 --> 00:36:08,540
means that we take the image and we pass

718
00:36:08,540 --> 00:36:10,670
it through a series of convolutions that

719
00:36:10,670 --> 00:36:14,750
reduce the dimensionality till we have a

720
00:36:14,750 --> 00:36:17,360
feature map which is way smaller than

721
00:36:17,360 --> 00:36:20,030
the image and summarizes the important

722
00:36:20,030 --> 00:36:22,130
information about the image which is in

723
00:36:22,130 --> 00:36:24,530
this case the pixels that are texts for

724
00:36:24,530 --> 00:36:28,490
us and then we pass it we do a

725
00:36:28,490 --> 00:36:33,250
decomposition process to sample again

726
00:36:33,250 --> 00:36:37,730
this feature map into an image at the

727
00:36:37,730 --> 00:36:42,440
end there we have is pixel labels so it

728
00:36:42,440 --> 00:36:44,810
will tell us what each pixel what class

729
00:36:44,810 --> 00:36:47,330
each pixel belongs to whether it belongs

730
00:36:47,330 --> 00:36:49,040
to a character or it belongs to the

731
00:36:49,040 --> 00:36:51,650
background and we use cross-entropy loss

732
00:36:51,650 --> 00:36:56,870
in this case that's the last we use and

733
00:36:56,870 --> 00:36:59,240
we can see in the plots how first loss

734
00:36:59,240 --> 00:37:01,070
is decreased and how also they are

735
00:37:01,070 --> 00:37:04,070
curacy for each one of the classes is

736
00:37:04,070 --> 00:37:06,320
incremented that means we're at the end

737
00:37:06,320 --> 00:37:09,560
we're detecting pixels that belong to

738
00:37:09,560 --> 00:37:12,230
the characters with a 60 percent

739
00:37:12,230 --> 00:37:16,070
accuracy which is we'll see what that

740
00:37:16,070 --> 00:37:18,700
allows us to do in the next slides

741
00:37:18,700 --> 00:37:21,650
finally once we have these models that

742
00:37:21,650 --> 00:37:24,950
are able to be noise intercepted images

743
00:37:24,950 --> 00:37:27,710
we're passing through why we use open

744
00:37:27,710 --> 00:37:31,490
sewers OCR model also see our algorithm

745
00:37:31,490 --> 00:37:34,790
which is called tesseract and this one

746
00:37:34,790 --> 00:37:37,190
is basically has four main steps

747
00:37:37,190 --> 00:37:39,350
the first one is adaptive tree falling

748
00:37:39,350 --> 00:37:42,380
which is probably what he has the most

749
00:37:42,380 --> 00:37:44,560
trouble with when we passive the

750
00:37:44,560 --> 00:37:46,490
intercepted images because they're

751
00:37:46,490 --> 00:37:49,250
really noisy then it does officially our

752
00:37:49,250 --> 00:37:50,660
analysis to find worth it

753
00:37:50,660 --> 00:37:53,720
text is where issue each line is and

754
00:37:53,720 --> 00:37:56,570
then it does a baseline feeding to

755
00:37:56,570 --> 00:37:59,480
correct this cue of the images and then

756
00:37:59,480 --> 00:38:03,350
it passes that through an LS TM which is

757
00:38:03,350 --> 00:38:05,990
another type of neural network of

758
00:38:05,990 --> 00:38:08,930
recurrent neural network that also

759
00:38:08,930 --> 00:38:12,320
slides through the text detecting each

760
00:38:12,320 --> 00:38:13,500
character and

761
00:38:13,500 --> 00:38:15,180
particularity here is that each

762
00:38:15,180 --> 00:38:17,730
character detective serves as an equal

763
00:38:17,730 --> 00:38:21,390
to the next step so that text has

764
00:38:21,390 --> 00:38:28,349
certain yeah it has so that the output

765
00:38:28,349 --> 00:38:30,869
of the previous of the Korean character

766
00:38:30,869 --> 00:38:33,210
being detected is an input to that for

767
00:38:33,210 --> 00:38:35,060
for detecting the next character and

768
00:38:35,060 --> 00:38:38,780
then these are the results we obtain

769
00:38:38,780 --> 00:38:41,490
we're measuring how good our denoising

770
00:38:41,490 --> 00:38:44,820
is using peak signal-to-noise ratio so

771
00:38:44,820 --> 00:38:48,270
we start with an average SNR of seven

772
00:38:48,270 --> 00:38:50,160
point seven point three six three and

773
00:38:50,160 --> 00:38:54,080
these decreases this improves using the

774
00:38:54,080 --> 00:38:55,770
convolutional neural network the first

775
00:38:55,770 --> 00:38:58,740
solution in our network to nine point

776
00:38:58,740 --> 00:39:01,080
seven but it's even better with the FC

777
00:39:01,080 --> 00:39:04,859
partner to this level of twelve point

778
00:39:04,859 --> 00:39:06,510
four and we can see some examples here

779
00:39:06,510 --> 00:39:10,020
of how different types of input

780
00:39:10,020 --> 00:39:14,390
intercepted images can be denoise

781
00:39:14,390 --> 00:39:18,720
finally this allows us to perform OCR

782
00:39:18,720 --> 00:39:22,170
and the metric that the metrics that we

783
00:39:22,170 --> 00:39:24,300
use here are basically precision recall

784
00:39:24,300 --> 00:39:29,609
f1 score and accuracy and we can see how

785
00:39:29,609 --> 00:39:32,580
there are original images of course have

786
00:39:32,580 --> 00:39:35,119
a really high accuracy and f1 score

787
00:39:35,119 --> 00:39:39,359
these decreases normally when we do when

788
00:39:39,359 --> 00:39:41,400
we try to do OCR with the intercepted

789
00:39:41,400 --> 00:39:43,950
images it can barely detect any

790
00:39:43,950 --> 00:39:47,430
character and then it improves a little

791
00:39:47,430 --> 00:39:50,250
with the first completion method where

792
00:39:50,250 --> 00:39:53,099
we we saw if there's only improves along

793
00:39:53,099 --> 00:39:56,640
with the FC hartnett so our regression

794
00:39:56,640 --> 00:40:03,150
your network or we can run the worse way

795
00:40:03,150 --> 00:40:05,609
better at at this task in this

796
00:40:05,609 --> 00:40:07,830
particular case and it's even faster

797
00:40:07,830 --> 00:40:12,030
because it's it's a smaller network at

798
00:40:12,030 --> 00:40:17,339
the end we can pass new images the noise

799
00:40:17,339 --> 00:40:19,680
them and extract actual text from all of

800
00:40:19,680 --> 00:40:22,530
these images as you can see in these

801
00:40:22,530 --> 00:40:24,589
examples

802
00:40:25,470 --> 00:40:28,680
so we have to wrap up the traditional

803
00:40:28,680 --> 00:40:30,859
methods make it difficult to process

804
00:40:30,859 --> 00:40:35,580
these intercepted images in our work we

805
00:40:35,580 --> 00:40:38,910
have created a a set of 18 around 18,000

806
00:40:38,910 --> 00:40:43,550
short and we see image pairs are aligned

807
00:40:43,550 --> 00:40:46,950
two models were trained first the

808
00:40:46,950 --> 00:40:49,170
denoising the blurring conversion neural

809
00:40:49,170 --> 00:40:51,240
network which performs axilla

810
00:40:51,240 --> 00:40:54,330
progression and then the FC heart net

811
00:40:54,330 --> 00:40:56,040
which performs pixel level

812
00:40:56,040 --> 00:40:59,609
classification the results as we saw

813
00:40:59,609 --> 00:41:03,630
improve the Pearson R and the best

814
00:41:03,630 --> 00:41:05,640
results we obtain them for the FC

815
00:41:05,640 --> 00:41:08,130
Hartnett and finally this allows us to

816
00:41:08,130 --> 00:41:11,070
perform character recognition and the

817
00:41:11,070 --> 00:41:13,530
results can be seen in the improvement

818
00:41:13,530 --> 00:41:17,460
in the in the f1 score and accuracy in

819
00:41:17,460 --> 00:41:20,580
the character recognition and something

820
00:41:20,580 --> 00:41:22,530
that these proofs is that information

821
00:41:22,530 --> 00:41:24,930
can be effectively retrieved from e/m

822
00:41:24,930 --> 00:41:27,720
emanations hence this is today an active

823
00:41:27,720 --> 00:41:32,700
security threat these are some of the

824
00:41:32,700 --> 00:41:35,900
references and I thank you a lot for

825
00:41:35,900 --> 00:41:40,280
your patience and your attention

826
00:41:40,280 --> 00:41:43,589
thank you very much Santiago anyone for

827
00:41:43,589 --> 00:41:48,750
this amazing presentation we have few

828
00:41:48,750 --> 00:41:52,260
questions in the chat but you know I

829
00:41:52,260 --> 00:41:53,670
would want you to keep it short and

830
00:41:53,670 --> 00:41:55,800
quick maybe like in five minutes you

831
00:41:55,800 --> 00:41:57,510
could answer those questions if you have

832
00:41:57,510 --> 00:42:00,180
because just to keep the interest of

833
00:42:00,180 --> 00:42:01,859
time so you could have a look at the

834
00:42:01,859 --> 00:42:06,359
questions yeah yeah I have look what

835
00:42:06,359 --> 00:42:08,970
some of the questions so first question

836
00:42:08,970 --> 00:42:11,640
I hope can we protect the hard work from

837
00:42:11,640 --> 00:42:14,970
the stated attacks please share the

838
00:42:14,970 --> 00:42:16,800
common standards an operating procedure

839
00:42:16,800 --> 00:42:19,170
squish are effective against RF attack

840
00:42:19,170 --> 00:42:22,480
so I will say that

841
00:42:22,480 --> 00:42:25,300
maybe shielding filters in past luncheon

842
00:42:25,300 --> 00:42:28,030
and hardware level between the cables

843
00:42:28,030 --> 00:42:30,579
and the impedance of the screens and in

844
00:42:30,579 --> 00:42:32,710
pants of the busier interface of the

845
00:42:32,710 --> 00:42:35,079
computer are the main tools you have to

846
00:42:35,079 --> 00:42:39,130
protect against this as I told you

847
00:42:39,130 --> 00:42:42,160
tempest standard is a military standard

848
00:42:42,160 --> 00:42:46,240
and it is known Ultimo also small most

849
00:42:46,240 --> 00:42:50,349
of it is secret it is known that this

850
00:42:50,349 --> 00:42:53,890
equipment is very expensive so you

851
00:42:53,890 --> 00:42:57,490
cannot protect as a military as a

852
00:42:57,490 --> 00:43:00,099
military level protection but maybe in

853
00:43:00,099 --> 00:43:02,940
the short answer is maybe if you have a

854
00:43:02,940 --> 00:43:06,640
next branch display monitor go with the

855
00:43:06,640 --> 00:43:11,710
excellent cable that will help you okay

856
00:43:11,710 --> 00:43:14,440
how far was the air from the cable as I

857
00:43:14,440 --> 00:43:17,040
showed you it was five meters a

858
00:43:17,040 --> 00:43:19,690
curriculum and we used to recreate the

859
00:43:19,690 --> 00:43:22,030
struts of information I told you it was

860
00:43:22,030 --> 00:43:25,930
an easy are the horror horror you

861
00:43:25,930 --> 00:43:28,720
capturing the HDMI signal through the

862
00:43:28,720 --> 00:43:31,000
hot air F it is very strange because

863
00:43:31,000 --> 00:43:33,190
capturing the wire signal seems not

864
00:43:33,190 --> 00:43:37,089
possible to hack RF it is possible maybe

865
00:43:37,089 --> 00:43:40,390
and when you are thinking HDMI as you're

866
00:43:40,390 --> 00:43:43,390
thinking in the digital interface and

867
00:43:43,390 --> 00:43:45,970
will it is true that the clock of this

868
00:43:45,970 --> 00:43:48,579
digital interface is 10 times faster

869
00:43:48,579 --> 00:43:51,819
than taking a pixel frequency for

870
00:43:51,819 --> 00:43:55,089
example if I have full HD we are

871
00:43:55,089 --> 00:43:57,819
transmitted at the pixel clock frequency

872
00:43:57,819 --> 00:44:04,270
was 148 measures and for transmitted of

873
00:44:04,270 --> 00:44:06,510
transmitting and at this resolution on

874
00:44:06,510 --> 00:44:12,540
HDMI we have to achieve a clock rate of

875
00:44:12,540 --> 00:44:15,339
1.5 gigahertz something like that

876
00:44:15,339 --> 00:44:18,490
but anyway that that's not what mean

877
00:44:18,490 --> 00:44:21,010
what we care about because we are anyway

878
00:44:21,010 --> 00:44:26,200
trying to get pixel shapes so again this

879
00:44:26,200 --> 00:44:28,210
works the same as VGA so the language

880
00:44:28,210 --> 00:44:30,490
will be the same and I can assure that

881
00:44:30,490 --> 00:44:33,380
with these words also on HDMI

882
00:44:33,380 --> 00:44:38,029
so what happens when we are there are

883
00:44:38,029 --> 00:44:41,509
multiple screens in the salon okay with

884
00:44:41,509 --> 00:44:45,440
multiple screens that depends on then

885
00:44:45,440 --> 00:44:48,049
there on the antenna if we are not using

886
00:44:48,049 --> 00:44:51,200
a directive antenna probably there will

887
00:44:51,200 --> 00:44:53,529
be constructive and destructive

888
00:44:53,529 --> 00:44:57,229
interference between the several display

889
00:44:57,229 --> 00:45:00,109
monitors and several video interfaces

890
00:45:00,109 --> 00:45:02,690
that are transmitted at that time but if

891
00:45:02,690 --> 00:45:04,759
we are using a detective antenna this

892
00:45:04,759 --> 00:45:07,099
could work this is an experiment we are

893
00:45:07,099 --> 00:45:11,930
going to do in a near future so can you

894
00:45:11,930 --> 00:45:14,299
please share the stud bust a guideline

895
00:45:14,299 --> 00:45:16,999
with equipment part numbers okay the

896
00:45:16,999 --> 00:45:19,759
clip - I told you was an SDR you can use

897
00:45:19,759 --> 00:45:24,259
a hack RF and there is an open-source

898
00:45:24,259 --> 00:45:26,359
tool you can use for this it is very

899
00:45:26,359 --> 00:45:29,499
known we're developing our own tools but

900
00:45:29,499 --> 00:45:33,739
hey I don't know if I can share the the

901
00:45:33,739 --> 00:45:36,799
open-source tool you tell me if this is

902
00:45:36,799 --> 00:45:39,829
possible in the chat it's okay yes you

903
00:45:39,829 --> 00:45:44,900
can yeah I wish I will share it in the

904
00:45:44,900 --> 00:45:50,930
Mon in a moment so what is the minimum

905
00:45:50,930 --> 00:45:53,539
size for okay does I things a question

906
00:45:53,539 --> 00:45:55,950
for one

907
00:45:55,950 --> 00:45:57,630
you

908
00:45:57,630 --> 00:46:00,089
what is the minimum size one you can

909
00:46:00,089 --> 00:46:03,180
recognize with the FC Hornet um this is

910
00:46:03,180 --> 00:46:06,180
a tricky one because the minimum size we

911
00:46:06,180 --> 00:46:09,779
recognized was 30 points and that's a

912
00:46:09,779 --> 00:46:13,980
little big but it depends a lot on the

913
00:46:13,980 --> 00:46:18,990
hardware you used to capture the actual

914
00:46:18,990 --> 00:46:21,450
images because if you have a really good

915
00:46:21,450 --> 00:46:23,609
hardware and you work a lot on that part

916
00:46:23,609 --> 00:46:25,710
then you will have a really good

917
00:46:25,710 --> 00:46:27,210
reconstruction otherwise you will have

918
00:46:27,210 --> 00:46:28,740
so much noise that it will be impossible

919
00:46:28,740 --> 00:46:32,009
to recover any data thus the snr would

920
00:46:32,009 --> 00:46:36,990
be too bad so it depends more on the on

921
00:46:36,990 --> 00:46:40,740
that on the snr than on the on the

922
00:46:40,740 --> 00:46:43,950
actual size that the limit is more on on

923
00:46:43,950 --> 00:46:46,470
what on what type of data we can capture

924
00:46:46,470 --> 00:46:50,009
with this setup and the next question

925
00:46:50,009 --> 00:46:53,220
was is it better no what is the minimum

926
00:46:53,220 --> 00:46:56,430
size on it already I think this was for

927
00:46:56,430 --> 00:47:02,039
you to do yeah is there a better and

928
00:47:02,039 --> 00:47:04,559
more secure standard other than HDMI and

929
00:47:04,559 --> 00:47:09,930
VGA I will say no I think these

930
00:47:09,930 --> 00:47:12,150
principles are the same for all of them

931
00:47:12,150 --> 00:47:17,339
I have not tested but for example VGA

932
00:47:17,339 --> 00:47:21,539
and HDMI are so different and anyway we

933
00:47:21,539 --> 00:47:24,950
can reconstruct via frames so I will say

934
00:47:24,950 --> 00:47:28,650
there is no secure there is no small

935
00:47:28,650 --> 00:47:31,319
secure being interface for this against

936
00:47:31,319 --> 00:47:36,450
this this is the last question that you

937
00:47:36,450 --> 00:47:41,400
have yeah okay thank you very much thank

938
00:47:41,400 --> 00:47:43,380
you everybody who joined us for today's

939
00:47:43,380 --> 00:47:45,839
webinar amazing presentation Santiago

940
00:47:45,839 --> 00:47:49,619
and you are all those who registered

941
00:47:49,619 --> 00:47:51,480
today and attended this amazing webinar

942
00:47:51,480 --> 00:47:53,910
I request you all to please send us your

943
00:47:53,910 --> 00:47:56,430
feedback and what would you like to you

944
00:47:56,430 --> 00:47:58,799
in the future series of hardware or i/o

945
00:47:58,799 --> 00:48:01,380
webinar I'm pasting the link of the

946
00:48:01,380 --> 00:48:03,750
feedback in the chat option so you can

947
00:48:03,750 --> 00:48:08,039
quickly use it and this presentation is

948
00:48:08,039 --> 00:48:09,660
recorded

949
00:48:09,660 --> 00:48:11,640
and it would be available on our YouTube

950
00:48:11,640 --> 00:48:14,700
channel after some editing work as well

951
00:48:14,700 --> 00:48:18,030
so yeah thank you all and stay safe

952
00:48:18,030 --> 00:48:23,240
but thank you thank you bye bye

