1
00:00:01,680 --> 00:00:04,319
hello everyone my name is john lochen a

2
00:00:04,319 --> 00:00:06,640
ph student here at the jajang university

3
00:00:06,640 --> 00:00:09,440
china i'm glad to be here to present our

4
00:00:09,440 --> 00:00:11,679
work about deep learning model copyright

5
00:00:11,679 --> 00:00:13,360
protection

6
00:00:13,360 --> 00:00:15,519
deep learning models have helped us so

7
00:00:15,519 --> 00:00:17,920
many real world problems and facilitate

8
00:00:17,920 --> 00:00:19,199
our lives

9
00:00:19,199 --> 00:00:21,600
for example we'll see a plant you don't

10
00:00:21,600 --> 00:00:23,840
know you can take a picture and use a

11
00:00:23,840 --> 00:00:27,199
nap to identify what species it is

12
00:00:27,199 --> 00:00:29,599
however training such high performance

13
00:00:29,599 --> 00:00:31,599
deep learning models it's by no means

14
00:00:31,599 --> 00:00:33,920
trivial which requires not only

15
00:00:33,920 --> 00:00:35,920
large-scale datasets but also

16
00:00:35,920 --> 00:00:38,320
significant computational and expert

17
00:00:38,320 --> 00:00:39,680
resources

18
00:00:39,680 --> 00:00:42,160
as a result deep learning models have

19
00:00:42,160 --> 00:00:44,719
become one of the most valuable assets

20
00:00:44,719 --> 00:00:47,520
in modern artificial intelligence

21
00:00:47,520 --> 00:00:50,079
unauthorized duplication or reproduction

22
00:00:50,079 --> 00:00:51,920
of deep learning models can lead to

23
00:00:51,920 --> 00:00:54,559
copyright infringement and cause huge

24
00:00:54,559 --> 00:00:57,600
economic losses to model owners calling

25
00:00:57,600 --> 00:00:59,600
for effective copyright protection

26
00:00:59,600 --> 00:01:01,440
techniques

27
00:01:01,440 --> 00:01:03,440
here we consider a typical attack

28
00:01:03,440 --> 00:01:05,840
defense setting with two parties

29
00:01:05,840 --> 00:01:08,400
the defender and the adversary

30
00:01:08,400 --> 00:01:10,720
here the defender is a model owner who

31
00:01:10,720 --> 00:01:12,479
trains a deep learning model using

32
00:01:12,479 --> 00:01:14,320
private resources

33
00:01:14,320 --> 00:01:16,640
the adversary attempts to steer a copy

34
00:01:16,640 --> 00:01:18,880
of this model aiming to mimic its

35
00:01:18,880 --> 00:01:21,600
functionality while evading detection

36
00:01:21,600 --> 00:01:23,680
and the defender hopes to recognize

37
00:01:23,680 --> 00:01:26,479
motor copies that is model ownership

38
00:01:26,479 --> 00:01:27,920
verification

39
00:01:27,920 --> 00:01:30,159
this process should be robust and

40
00:01:30,159 --> 00:01:31,920
efficient

41
00:01:31,920 --> 00:01:34,320
we identify three common threads to deep

42
00:01:34,320 --> 00:01:36,640
learning model copyright model fine

43
00:01:36,640 --> 00:01:38,840
tuning model pruning and model

44
00:01:38,840 --> 00:01:41,520
extraction for functioning and pruning

45
00:01:41,520 --> 00:01:43,439
the anniversary has full knowledge of

46
00:01:43,439 --> 00:01:45,600
the victim model including model

47
00:01:45,600 --> 00:01:48,000
architecture and parameters

48
00:01:48,000 --> 00:01:50,399
while for extraction the adversary can

49
00:01:50,399 --> 00:01:52,479
only query the victim model for

50
00:01:52,479 --> 00:01:53,680
predictions

51
00:01:53,680 --> 00:01:56,640
to achieve this the adversary first

52
00:01:56,640 --> 00:01:59,360
obtains an annotated data set by

53
00:01:59,360 --> 00:02:01,360
querying the victim model for set of

54
00:02:01,360 --> 00:02:04,320
auxiliary samples then change a copy of

55
00:02:04,320 --> 00:02:06,560
the victim model on this annotated

56
00:02:06,560 --> 00:02:08,399
dataset

57
00:02:08,399 --> 00:02:10,479
a number of watermarking techniques have

58
00:02:10,479 --> 00:02:12,640
been proposed to protect the copyright

59
00:02:12,640 --> 00:02:14,640
of deep learning models

60
00:02:14,640 --> 00:02:16,720
similar to traditional multimedia

61
00:02:16,720 --> 00:02:19,040
watermarking differently watermarking

62
00:02:19,040 --> 00:02:21,440
works in two steps embedding another

63
00:02:21,440 --> 00:02:24,560
verification in the embedding step the

64
00:02:24,560 --> 00:02:27,120
owner embeds a secret watermark into the

65
00:02:27,120 --> 00:02:29,120
model during the training process

66
00:02:29,120 --> 00:02:31,040
depending on how much knowledge of the

67
00:02:31,040 --> 00:02:32,959
model is available in the verification

68
00:02:32,959 --> 00:02:35,760
step existing watermarking methods can

69
00:02:35,760 --> 00:02:39,040
be broadly categorized into two classes

70
00:02:39,040 --> 00:02:41,200
while box methods for the case were

71
00:02:41,200 --> 00:02:44,319
model parameters available and black box

72
00:02:44,319 --> 00:02:46,239
methods where only predictions of the

73
00:02:46,239 --> 00:02:48,319
model can be acquired

74
00:02:48,319 --> 00:02:50,239
white box water marking bears a

75
00:02:50,239 --> 00:02:52,640
pre-designed signature for example a

76
00:02:52,640 --> 00:02:55,200
spherical beads into the parameter space

77
00:02:55,200 --> 00:02:57,920
of the model via certain regularization

78
00:02:57,920 --> 00:03:00,560
terms the ownership could be claimed

79
00:03:00,560 --> 00:03:02,480
when the extracted signature from a

80
00:03:02,480 --> 00:03:04,640
suspect model is similar to that of the

81
00:03:04,640 --> 00:03:06,000
owner model

82
00:03:06,000 --> 00:03:08,159
black box watermarking usually leverages

83
00:03:08,159 --> 00:03:10,480
backdoor attacks to implant a watermark

84
00:03:10,480 --> 00:03:12,800
pattern into the owner model by training

85
00:03:12,800 --> 00:03:15,200
the model with a set of vector examples

86
00:03:15,200 --> 00:03:17,680
relatable to a secret class

87
00:03:17,680 --> 00:03:19,760
the ownership can then be claimed when

88
00:03:19,760 --> 00:03:22,000
the defender creates the suspect model

89
00:03:22,000 --> 00:03:24,239
for example attached with the watermark

90
00:03:24,239 --> 00:03:26,879
trigger and receives the security class

91
00:03:26,879 --> 00:03:28,720
as predictions

92
00:03:28,720 --> 00:03:31,280
however these methods still suffer from

93
00:03:31,280 --> 00:03:32,879
certain weaknesses

94
00:03:32,879 --> 00:03:35,519
arguably the most concerning one is that

95
00:03:35,519 --> 00:03:38,000
they are invasive that is they need to

96
00:03:38,000 --> 00:03:39,599
tamper with the normal training

97
00:03:39,599 --> 00:03:42,560
procedure to embed watermark which may

98
00:03:42,560 --> 00:03:45,280
compromise motor utility or introduce

99
00:03:45,280 --> 00:03:48,879
new security threats into the model

100
00:03:48,879 --> 00:03:51,840
thus in this work representative judge a

101
00:03:51,840 --> 00:03:53,680
testing framework that produce

102
00:03:53,680 --> 00:03:56,000
supporting evidence to determine whether

103
00:03:56,000 --> 00:03:58,319
suspected model is a copy of a victim

104
00:03:58,319 --> 00:03:59,439
model

105
00:03:59,439 --> 00:04:02,159
discharge consists of three components

106
00:04:02,159 --> 00:04:04,640
at the test case generation step it

107
00:04:04,640 --> 00:04:06,720
selects a set of seeds from the input

108
00:04:06,720 --> 00:04:09,280
data set and carefully generate a set of

109
00:04:09,280 --> 00:04:11,360
extreme test cases

110
00:04:11,360 --> 00:04:14,080
based on the test cases generated deep

111
00:04:14,080 --> 00:04:16,000
judge computes the distance goals

112
00:04:16,000 --> 00:04:18,000
defined by the testing matrix between

113
00:04:18,000 --> 00:04:20,478
the suspect and the victim models

114
00:04:20,478 --> 00:04:22,320
the final judgment of whether the

115
00:04:22,320 --> 00:04:24,880
suspected model is a copy of the victim

116
00:04:24,880 --> 00:04:27,040
can be made via thresholding and a

117
00:04:27,040 --> 00:04:28,880
voting mechanism

118
00:04:28,880 --> 00:04:31,120
we first introduced the testing metrics

119
00:04:31,120 --> 00:04:33,919
for two different settings respectively

120
00:04:33,919 --> 00:04:36,720
white box and blackbox in the white box

121
00:04:36,720 --> 00:04:39,199
setting deep charge has full access to

122
00:04:39,199 --> 00:04:41,520
the internals and the final probability

123
00:04:41,520 --> 00:04:43,919
vectors of the suspected model

124
00:04:43,919 --> 00:04:45,919
while in the black box setting deep

125
00:04:45,919 --> 00:04:48,240
judge can only query the suspected model

126
00:04:48,240 --> 00:04:50,960
to obtain the probability vectors of the

127
00:04:50,960 --> 00:04:53,440
predicted labels

128
00:04:53,440 --> 00:04:55,520
we first introduce a metric at the

129
00:04:55,520 --> 00:04:58,320
property level model properties can be

130
00:04:58,320 --> 00:05:00,639
used to characterize the similarity

131
00:05:00,639 --> 00:05:03,440
between models and we define the robotic

132
00:05:03,440 --> 00:05:05,840
distance rob d to measure the difference

133
00:05:05,840 --> 00:05:08,639
in adverse release between two models

134
00:05:08,639 --> 00:05:10,800
the motivation is that the robotness of

135
00:05:10,800 --> 00:05:12,720
a model is closely related to the

136
00:05:12,720 --> 00:05:14,800
decision boundary that the model runs

137
00:05:14,800 --> 00:05:17,440
through its unique optimization process

138
00:05:17,440 --> 00:05:19,680
and can be considered as a special

139
00:05:19,680 --> 00:05:21,600
fingerprint of the model

140
00:05:21,600 --> 00:05:23,680
if the model has a smaller distance on

141
00:05:23,680 --> 00:05:26,000
this metric it means that the more

142
00:05:26,000 --> 00:05:28,479
likely it is a stolen copy of the source

143
00:05:28,479 --> 00:05:29,759
model

144
00:05:29,759 --> 00:05:31,919
neural level metrics are suitable for

145
00:05:31,919 --> 00:05:34,240
white box testing scenarios where the

146
00:05:34,240 --> 00:05:36,639
internal layers output of the model is

147
00:05:36,639 --> 00:05:39,840
accessible intuitively the output of

148
00:05:39,840 --> 00:05:42,240
each neuron in a model follows its own

149
00:05:42,240 --> 00:05:44,960
statistical distribution and the neural

150
00:05:44,960 --> 00:05:47,680
output in different models should vary

151
00:05:47,680 --> 00:05:51,120
motivated by this divide uses the output

152
00:05:51,120 --> 00:05:53,280
status of neurons to capture the

153
00:05:53,280 --> 00:05:55,520
difference between two models and

154
00:05:55,520 --> 00:05:57,360
defines the following two neuron level

155
00:05:57,360 --> 00:06:00,000
metrics neuron output distance and the

156
00:06:00,000 --> 00:06:02,240
neural activation distance

157
00:06:02,240 --> 00:06:04,560
led measures average neural output

158
00:06:04,560 --> 00:06:06,880
difference between two models over a

159
00:06:06,880 --> 00:06:10,319
given set of tester cases while nad

160
00:06:10,319 --> 00:06:12,400
measures the difference in activation

161
00:06:12,400 --> 00:06:15,600
status activated or not activated

162
00:06:15,600 --> 00:06:17,759
between the neurons

163
00:06:17,759 --> 00:06:20,000
the layer wise matrix in deep charge

164
00:06:20,000 --> 00:06:22,400
taking into account output values of the

165
00:06:22,400 --> 00:06:24,960
entire layer in deep learning model

166
00:06:24,960 --> 00:06:27,520
compared with neural level metrics layer

167
00:06:27,520 --> 00:06:30,080
level metrics provide a full scale view

168
00:06:30,080 --> 00:06:31,919
of the intermediate layer output

169
00:06:31,919 --> 00:06:34,479
difference between two models json

170
00:06:34,479 --> 00:06:36,639
channel distance is a matrix that

171
00:06:36,639 --> 00:06:38,639
measures the similarity of two

172
00:06:38,639 --> 00:06:41,600
probability distributions and a small

173
00:06:41,600 --> 00:06:44,000
jsd value implies the two distributions

174
00:06:44,000 --> 00:06:47,039
are very similar jsd is designed for the

175
00:06:47,039 --> 00:06:49,759
output layer specifically

176
00:06:49,759 --> 00:06:51,919
to fully exercise the testing matrix

177
00:06:51,919 --> 00:06:54,400
defined above we need to magnify the

178
00:06:54,400 --> 00:06:56,720
similarities between a positive suspect

179
00:06:56,720 --> 00:06:59,199
and the victim model while minimizing

180
00:06:59,199 --> 00:07:01,759
the similarities of a negative suspect

181
00:07:01,759 --> 00:07:04,880
to the victim model in deep charge this

182
00:07:04,880 --> 00:07:06,960
is achieved by small test case

183
00:07:06,960 --> 00:07:08,800
generation methods

184
00:07:08,800 --> 00:07:11,199
meanwhile test case generation should

185
00:07:11,199 --> 00:07:13,360
respect the model accessibility in

186
00:07:13,360 --> 00:07:15,280
different defense settings

187
00:07:15,280 --> 00:07:17,680
blackbox and whitebox

188
00:07:17,680 --> 00:07:19,520
in the blackbox setting

189
00:07:19,520 --> 00:07:21,599
we populated the test set using

190
00:07:21,599 --> 00:07:24,160
adversarial inputs generated by existing

191
00:07:24,160 --> 00:07:26,639
anniversary attack methods on the victim

192
00:07:26,639 --> 00:07:27,599
model

193
00:07:27,599 --> 00:07:29,440
the left figure illustrates the

194
00:07:29,440 --> 00:07:32,080
rationale behind fine-tuned and proved

195
00:07:32,080 --> 00:07:34,479
model copies are directly derived from

196
00:07:34,479 --> 00:07:37,039
the victim model starts to share similar

197
00:07:37,039 --> 00:07:39,199
decision boundaries as a victim model

198
00:07:39,199 --> 00:07:41,840
however the negative suspect models are

199
00:07:41,840 --> 00:07:44,000
trained from scratch on different data

200
00:07:44,000 --> 00:07:46,800
always different initializations such

201
00:07:46,800 --> 00:07:49,199
having minimum or no overlapping with

202
00:07:49,199 --> 00:07:51,919
the victim model's decision boundary

203
00:07:51,919 --> 00:07:53,759
although the extracted models by

204
00:07:53,759 --> 00:07:55,840
extraction attacks are trained from

205
00:07:55,840 --> 00:07:58,400
scratch by the adversary but they will

206
00:07:58,400 --> 00:08:00,319
gradually mimic the digital boundary of

207
00:08:00,319 --> 00:08:02,319
the victim model

208
00:08:02,319 --> 00:08:04,560
in the white box setting the internals

209
00:08:04,560 --> 00:08:07,440
of the setback model are accessible thus

210
00:08:07,440 --> 00:08:09,520
a more fine-grained approach for test

211
00:08:09,520 --> 00:08:12,000
case generation becomes feasible as

212
00:08:12,000 --> 00:08:14,400
shown in the figure given a seed input

213
00:08:14,400 --> 00:08:16,560
and specified layer deep charge

214
00:08:16,560 --> 00:08:19,120
generates one test case for each neuron

215
00:08:19,120 --> 00:08:21,280
and the corner case of the neurons

216
00:08:21,280 --> 00:08:24,800
activation is our particular interest

217
00:08:24,800 --> 00:08:27,280
the judgment mechanism of deep judge has

218
00:08:27,280 --> 00:08:30,240
two steps thresholding and voting the

219
00:08:30,240 --> 00:08:32,240
thresholding step determines a proper

220
00:08:32,240 --> 00:08:34,399
threshold for each testing metric based

221
00:08:34,399 --> 00:08:36,799
on the statistics the voting step

222
00:08:36,799 --> 00:08:39,839
examines a suspected model against each

223
00:08:39,839 --> 00:08:42,399
metric and gives it a positive vote if

224
00:08:42,399 --> 00:08:44,399
its distance to the victim model is

225
00:08:44,399 --> 00:08:47,120
lower than the threshold of that metric

226
00:08:47,120 --> 00:08:49,839
the lower measured metric value the more

227
00:08:49,839 --> 00:08:52,160
likely the suspected model is a copy of

228
00:08:52,160 --> 00:08:53,279
the victim

229
00:08:53,279 --> 00:08:55,519
based on the voting result we can get

230
00:08:55,519 --> 00:08:57,120
answer for the question in our paper

231
00:08:57,120 --> 00:09:00,560
title copyright yes or no

232
00:09:00,560 --> 00:09:02,720
we run the experiments on three match

233
00:09:02,720 --> 00:09:04,880
classification datasets and one audio

234
00:09:04,880 --> 00:09:07,920
recognition dataset the models used for

235
00:09:07,920 --> 00:09:10,880
the four data sets are summarizing table

236
00:09:10,880 --> 00:09:12,320
including three convolutional

237
00:09:12,320 --> 00:09:14,800
architectures and one recurrent neural

238
00:09:14,800 --> 00:09:15,839
network

239
00:09:15,839 --> 00:09:18,080
positive suspect models are derived from

240
00:09:18,080 --> 00:09:20,240
the victim models via fine-tuning

241
00:09:20,240 --> 00:09:22,800
probing or extraction these models are

242
00:09:22,800 --> 00:09:24,800
considered as stolen copies of the

243
00:09:24,800 --> 00:09:27,440
owner's mixing model div judge should

244
00:09:27,440 --> 00:09:29,760
provide evidence for the victim to claim

245
00:09:29,760 --> 00:09:31,040
ownership

246
00:09:31,040 --> 00:09:32,959
negative suspect models are trained

247
00:09:32,959 --> 00:09:34,880
independently using different training

248
00:09:34,880 --> 00:09:37,600
data or different initializations the

249
00:09:37,600 --> 00:09:39,920
negative suspect models serve as the

250
00:09:39,920 --> 00:09:41,760
control group to show that deep judge

251
00:09:41,760 --> 00:09:44,160
will not claim wrong ownership

252
00:09:44,160 --> 00:09:45,839
as model fun tuning and pruning

253
00:09:45,839 --> 00:09:47,600
strengths are similar in processing the

254
00:09:47,600 --> 00:09:50,000
victim model we discuss them here

255
00:09:50,000 --> 00:09:52,240
together the results are presented

256
00:09:52,240 --> 00:09:54,480
separately for blackbox and whitebox

257
00:09:54,480 --> 00:09:56,959
setting for the blackbox setting for

258
00:09:56,959 --> 00:09:58,800
each metric the values below the

259
00:09:58,800 --> 00:10:01,600
threshold indicating copy are both

260
00:10:01,600 --> 00:10:03,760
threshold indicating node copy a

261
00:10:03,760 --> 00:10:06,560
highlight in red and green respectively

262
00:10:06,560 --> 00:10:08,959
as we can see podium models are all

263
00:10:08,959 --> 00:10:11,200
marked by reds and negatives are all

264
00:10:11,200 --> 00:10:13,680
marked by green that means the final

265
00:10:13,680 --> 00:10:15,200
judgment made by deep judge are

266
00:10:15,200 --> 00:10:17,600
identical to the ground

267
00:10:17,600 --> 00:10:19,440
the results of white box stating are

268
00:10:19,440 --> 00:10:22,000
similar to black box and it is not

269
00:10:22,000 --> 00:10:24,399
surprising as white box testing can

270
00:10:24,399 --> 00:10:26,240
collect more fine-grained information

271
00:10:26,240 --> 00:10:28,880
from the suspected models

272
00:10:28,880 --> 00:10:30,800
to better understand the power of deep

273
00:10:30,800 --> 00:10:33,200
judge we combine the results for each

274
00:10:33,200 --> 00:10:35,680
suspect model into a single reader chat

275
00:10:35,680 --> 00:10:36,959
in this figure

276
00:10:36,959 --> 00:10:38,800
we use the orange line for the positive

277
00:10:38,800 --> 00:10:40,880
suspect models and the blue line for the

278
00:10:40,880 --> 00:10:42,000
negatives

279
00:10:42,000 --> 00:10:43,680
each dimension of the reader chart

280
00:10:43,680 --> 00:10:46,000
corresponds to a similarity score given

281
00:10:46,000 --> 00:10:47,920
by one testing matrix

282
00:10:47,920 --> 00:10:50,000
the larger the value the more similar

283
00:10:50,000 --> 00:10:52,720
the suspected model to the victim thus

284
00:10:52,720 --> 00:10:55,120
the field area could be viewed as

285
00:10:55,120 --> 00:10:56,880
accumulated supporting evidence by

286
00:10:56,880 --> 00:10:59,200
typical matrix for determining whether

287
00:10:59,200 --> 00:11:02,000
the suspect model is a stolen copy

288
00:11:02,000 --> 00:11:04,800
clearly deep drive is able to accurately

289
00:11:04,800 --> 00:11:06,800
distinguish positive suspects from

290
00:11:06,800 --> 00:11:09,040
negative ones

291
00:11:09,040 --> 00:11:11,120
here we compare deep charge with three

292
00:11:11,120 --> 00:11:13,120
copyright defense methods

293
00:11:13,120 --> 00:11:15,440
in short deep charge performs better in

294
00:11:15,440 --> 00:11:18,000
the black box setting and comparably in

295
00:11:18,000 --> 00:11:19,920
the white box setting against the model

296
00:11:19,920 --> 00:11:22,399
for tuning and pruning attacks while no

297
00:11:22,399 --> 00:11:25,040
tampering with model training

298
00:11:25,040 --> 00:11:27,519
model extraction also known as model

299
00:11:27,519 --> 00:11:29,440
student it's considered to be a more

300
00:11:29,440 --> 00:11:31,120
challenging threat to deep learning

301
00:11:31,120 --> 00:11:32,399
copyright

302
00:11:32,399 --> 00:11:34,560
we consider model extraction with two

303
00:11:34,560 --> 00:11:36,640
different types of supporting data

304
00:11:36,640 --> 00:11:39,200
auxiliary and synthetic compared to

305
00:11:39,200 --> 00:11:41,120
model functioning and pruning the

306
00:11:41,120 --> 00:11:43,360
average property and gsd values on the

307
00:11:43,360 --> 00:11:46,079
extracted models are relatively larger

308
00:11:46,079 --> 00:11:47,920
meaning that the decision boundaries of

309
00:11:47,920 --> 00:11:50,160
the extracting models are more different

310
00:11:50,160 --> 00:11:52,240
from that of the victim model

311
00:11:52,240 --> 00:11:54,720
nonetheless the true matrix can still

312
00:11:54,720 --> 00:11:57,120
reveal the unique similarities smaller

313
00:11:57,120 --> 00:11:59,600
values of the extracted models to the

314
00:11:59,600 --> 00:12:02,240
victim model the better the extraction

315
00:12:02,240 --> 00:12:04,639
that is higher accuracy of the extracted

316
00:12:04,639 --> 00:12:07,760
model the lower the two metric values

317
00:12:07,760 --> 00:12:10,079
this indicates that the extracting model

318
00:12:10,079 --> 00:12:12,480
behaves more similarly to the victim as

319
00:12:12,480 --> 00:12:14,160
its digital boundary gradually

320
00:12:14,160 --> 00:12:16,320
approaching that of the victim this is

321
00:12:16,320 --> 00:12:18,720
further validated in the bottom figure

322
00:12:18,720 --> 00:12:20,720
which shows the evolution of the

323
00:12:20,720 --> 00:12:22,959
extraction process

324
00:12:22,959 --> 00:12:25,279
we explore potential adaptive attacks to

325
00:12:25,279 --> 00:12:27,040
discharge based on the diversity

326
00:12:27,040 --> 00:12:28,240
knowledge

327
00:12:28,240 --> 00:12:30,959
in the first setting the adversary has

328
00:12:30,959 --> 00:12:32,639
full knowledge of deep charge including

329
00:12:32,639 --> 00:12:34,880
the testing metrics and the secret test

330
00:12:34,880 --> 00:12:37,440
cases and adversary functioning is

331
00:12:37,440 --> 00:12:38,480
considered

332
00:12:38,480 --> 00:12:40,720
which makes its test case into clean

333
00:12:40,720 --> 00:12:43,200
data and fine-tunes the stolen model in

334
00:12:43,200 --> 00:12:45,440
the second setting the adversary only

335
00:12:45,440 --> 00:12:48,320
knows 30 metrics two adaptive attacks

336
00:12:48,320 --> 00:12:50,800
are considered blind adversary training

337
00:12:50,800 --> 00:12:53,279
targeting of black box testing and a

338
00:12:53,279 --> 00:12:55,279
general transfer learning attack on

339
00:12:55,279 --> 00:12:57,279
white box testing

340
00:12:57,279 --> 00:12:59,279
we find that deep charge are partially

341
00:12:59,279 --> 00:13:02,079
broken under these adaptive attacks

342
00:13:02,079 --> 00:13:04,240
that is some metric values or party

343
00:13:04,240 --> 00:13:06,800
models unmarked by green but the final

344
00:13:06,800 --> 00:13:08,720
judgments are still correct

345
00:13:08,720 --> 00:13:10,959
moreover deep judge can recover the

346
00:13:10,959 --> 00:13:12,959
performance with new test cases

347
00:13:12,959 --> 00:13:15,440
generated with different seeds

348
00:13:15,440 --> 00:13:18,240
and lastly we make a brief summary and

349
00:13:18,240 --> 00:13:20,480
discussion about our work

350
00:13:20,480 --> 00:13:23,040
the core of deep judge is a family of

351
00:13:23,040 --> 00:13:24,959
multi-level testing metrics that

352
00:13:24,959 --> 00:13:27,040
characterize different aspects of

353
00:13:27,040 --> 00:13:29,680
similarities between models efficient

354
00:13:29,680 --> 00:13:31,760
and flexible test case generation

355
00:13:31,760 --> 00:13:34,399
methods are also developed to help boost

356
00:13:34,399 --> 00:13:36,959
the discriminating power of the testing

357
00:13:36,959 --> 00:13:38,160
metrics

358
00:13:38,160 --> 00:13:40,720
as a generic testing framework new

359
00:13:40,720 --> 00:13:43,040
testing metrics or test case generation

360
00:13:43,040 --> 00:13:45,360
methods can be incorporated into deep

361
00:13:45,360 --> 00:13:48,320
charge to help define future threats

362
00:13:48,320 --> 00:13:50,480
although watermarking requires change to

363
00:13:50,480 --> 00:13:52,639
the victim model it also provides

364
00:13:52,639 --> 00:13:54,800
functionalities beyond what testing can

365
00:13:54,800 --> 00:13:55,600
check

366
00:13:55,600 --> 00:13:58,160
such as embedding the owner identity for

367
00:13:58,160 --> 00:14:00,720
example signature or logos into the

368
00:14:00,720 --> 00:14:03,360
model we anticipate a long-running arms

369
00:14:03,360 --> 00:14:05,199
race between the motor owners and the

370
00:14:05,199 --> 00:14:06,639
adversaries

371
00:14:06,639 --> 00:14:08,880
where both water marking fingerprinting

372
00:14:08,880 --> 00:14:10,880
and the testing techniques are essential

373
00:14:10,880 --> 00:14:14,079
for trained effective defense

374
00:14:14,079 --> 00:14:17,839
that's all thanks for listening

375
00:14:22,800 --> 00:14:25,839
okay so we've got the giallo uh on zoom

376
00:14:25,839 --> 00:14:29,399
are there any questions

377
00:14:38,880 --> 00:14:39,920
watermark

378
00:14:39,920 --> 00:14:40,639
oh

379
00:14:40,639 --> 00:14:43,360
yeah wait okay jello can you hear

380
00:14:43,360 --> 00:14:44,880
what's being said yeah

381
00:14:44,880 --> 00:14:46,399
can you hear me

382
00:14:46,399 --> 00:14:48,160
yes yes okay thank you for the

383
00:14:48,160 --> 00:14:50,560
presentation so i was wondering your

384
00:14:50,560 --> 00:14:52,880
white box watermarking scheme first of

385
00:14:52,880 --> 00:14:54,880
all is it a watermark does it change the

386
00:14:54,880 --> 00:14:56,800
model or is it a fingerprint so it

387
00:14:56,800 --> 00:14:59,839
doesn't change the model

388
00:15:02,780 --> 00:15:04,560
[Music]

389
00:15:04,560 --> 00:15:07,680
can be a good pattern oh of course so

390
00:15:07,680 --> 00:15:10,240
your white box watermark does it modify

391
00:15:10,240 --> 00:15:13,800
the model's weights

392
00:15:14,910 --> 00:15:16,240
[Music]

393
00:15:16,240 --> 00:15:18,480
while box watermarking

394
00:15:18,480 --> 00:15:19,680
would

395
00:15:19,680 --> 00:15:21,199
would it change the

396
00:15:21,199 --> 00:15:24,000
model parameters

397
00:15:24,000 --> 00:15:25,600
okay so so you actually change it so i

398
00:15:25,600 --> 00:15:27,680
was wondering for your watermark if you

399
00:15:27,680 --> 00:15:30,560
evaluate it um because you look at the

400
00:15:30,560 --> 00:15:31,839
feature

401
00:15:31,839 --> 00:15:34,079
sorry the distribution of every every

402
00:15:34,079 --> 00:15:36,639
neuron right

403
00:15:36,639 --> 00:15:38,880
so i was wondering if you evaluated a

404
00:15:38,880 --> 00:15:41,600
shuffling attack because we know that

405
00:15:41,600 --> 00:15:43,360
neural networks are

406
00:15:43,360 --> 00:15:46,079
permutation and variant and there's been

407
00:15:46,079 --> 00:15:48,480
recently research coming out showing

408
00:15:48,480 --> 00:15:50,079
that a lot of the white box water

409
00:15:50,079 --> 00:15:51,360
marking schemes

410
00:15:51,360 --> 00:15:52,639
are not

411
00:15:52,639 --> 00:15:54,480
resistant or not robust

412
00:15:54,480 --> 00:15:56,160
against these

413
00:15:56,160 --> 00:15:58,000
feature shuffling attacks or future

414
00:15:58,000 --> 00:15:59,839
permutation attacks and i was just

415
00:15:59,839 --> 00:16:01,839
wondering if you evaluated those

416
00:16:01,839 --> 00:16:05,600
uh in your white box watermark

417
00:16:06,000 --> 00:16:08,399
no we have we haven't we have not

418
00:16:08,399 --> 00:16:11,199
studied this

419
00:16:11,519 --> 00:16:14,079
do you think but okay

420
00:16:14,079 --> 00:16:17,279
uh and i i should correct my answer uh

421
00:16:17,279 --> 00:16:20,560
before that our our white white box

422
00:16:20,560 --> 00:16:22,880
testing does not

423
00:16:22,880 --> 00:16:25,600
change the model parameters because it

424
00:16:25,600 --> 00:16:27,440
is uh

425
00:16:27,440 --> 00:16:29,519
because it's a

426
00:16:29,519 --> 00:16:30,880
passive

427
00:16:30,880 --> 00:16:34,000
passive protection techniques that

428
00:16:34,000 --> 00:16:36,720
we we generate a test case uh on the

429
00:16:36,720 --> 00:16:38,399
victim mode and the test

430
00:16:38,399 --> 00:16:41,680
the distance while uh white box water

431
00:16:41,680 --> 00:16:44,079
marking uh indeed changes the model

432
00:16:44,079 --> 00:16:46,079
parameters and

433
00:16:46,079 --> 00:16:48,720
then you you mentioned the uh neural

434
00:16:48,720 --> 00:16:53,839
shuffle attack yes uh it is uh

435
00:16:53,839 --> 00:16:57,199
it is a great trend great challenges to

436
00:16:57,199 --> 00:17:00,160
existing uh white box watermarking and

437
00:17:00,160 --> 00:17:02,320
and i have noticed that

438
00:17:02,320 --> 00:17:06,079
recently there is uh uh some works uh

439
00:17:06,079 --> 00:17:10,640
uh about uh uh neuroscience uh uh neural

440
00:17:10,640 --> 00:17:11,919
shopping that

441
00:17:11,919 --> 00:17:14,079
uh

442
00:17:14,480 --> 00:17:18,599
that indeed in

443
00:17:18,720 --> 00:17:22,720
uh indeed a neural shopping it's uh uh

444
00:17:22,720 --> 00:17:25,439
it's difficult to define for existing

445
00:17:25,439 --> 00:17:28,640
watermarking uh techniques since it's uh

446
00:17:28,640 --> 00:17:33,120
based on directly uh calculating uh

447
00:17:33,120 --> 00:17:35,280
the weight parameters

448
00:17:35,280 --> 00:17:37,120
uh

449
00:17:37,120 --> 00:17:39,520
and and

450
00:17:39,520 --> 00:17:40,880
and we think

451
00:17:40,880 --> 00:17:41,679
uh

452
00:17:41,679 --> 00:17:47,080
i think there is a possibility uh

453
00:17:47,600 --> 00:17:49,840
a positive a possible

454
00:17:49,840 --> 00:17:51,280
method to

455
00:17:51,280 --> 00:17:54,160
mitigate this problem since uh

456
00:17:54,160 --> 00:17:56,640
we can

457
00:17:56,960 --> 00:17:59,600
based on the neuron output this statics

458
00:17:59,600 --> 00:18:00,960
we can

459
00:18:00,960 --> 00:18:02,240
we can

460
00:18:02,240 --> 00:18:05,200
rank the neurons by its uh output output

461
00:18:05,200 --> 00:18:08,960
distribution and if the attacker

462
00:18:08,960 --> 00:18:13,039
shuffled the neurons we we could

463
00:18:13,039 --> 00:18:15,520
we could uh

464
00:18:15,520 --> 00:18:19,039
re-rank the neurons by their uh

465
00:18:19,039 --> 00:18:20,640
statics

466
00:18:20,640 --> 00:18:22,240
before uh

467
00:18:22,240 --> 00:18:23,520
we

468
00:18:23,520 --> 00:18:24,559
we

469
00:18:24,559 --> 00:18:27,280
extracted before the

470
00:18:27,280 --> 00:18:29,200
before the attack uh

471
00:18:29,200 --> 00:18:31,039
that might be a

472
00:18:31,039 --> 00:18:32,960
possible way to

473
00:18:32,960 --> 00:18:35,039
and we we

474
00:18:35,039 --> 00:18:37,200
and

475
00:18:37,280 --> 00:18:38,720
and we hope to

476
00:18:38,720 --> 00:18:42,400
do more studies about this problem

477
00:18:42,400 --> 00:18:44,720
yes okay thank you thank you

478
00:18:44,720 --> 00:18:46,799
thank you jailor uh are there any other

479
00:18:46,799 --> 00:18:49,200
questions

480
00:18:49,520 --> 00:18:51,039
if they aren't well thanks so much

481
00:18:51,039 --> 00:18:56,360
jailor it was a great talk thanks thanks

