1
00:00:00,960 --> 00:00:03,439
hello everyone my name is rihaman i'm

2
00:00:03,439 --> 00:00:05,440
honored to be here to present our work

3
00:00:05,440 --> 00:00:08,240
on behalf of all my courses

4
00:00:08,240 --> 00:00:10,559
transfer attacks has been harassing deep

5
00:00:10,559 --> 00:00:12,559
neural networks for a long time the

6
00:00:12,559 --> 00:00:14,240
attacker needs to prepare a surrogate

7
00:00:14,240 --> 00:00:17,359
model on the cd images and perform a

8
00:00:17,359 --> 00:00:19,680
watchbox attack on surrogate model to

9
00:00:19,680 --> 00:00:21,600
get adversarial examples that can

10
00:00:21,600 --> 00:00:24,240
deceive the surrogate model they hope

11
00:00:24,240 --> 00:00:24,960
that

12
00:00:24,960 --> 00:00:27,920
the adversarial examples are able to

13
00:00:27,920 --> 00:00:30,160
transfer to the unseen targets

14
00:00:30,160 --> 00:00:32,320
so that these on-scene targets are

15
00:00:32,320 --> 00:00:35,120
deceived as well

16
00:00:35,120 --> 00:00:36,880
therefore transfer attacks allows the

17
00:00:36,880 --> 00:00:39,200
attackers to perform adversarial attacks

18
00:00:39,200 --> 00:00:42,079
in black box scenarios in a real world

19
00:00:42,079 --> 00:00:44,160
transfer attack the targets are usually

20
00:00:44,160 --> 00:00:46,879
the cloud models such as amazon bandu

21
00:00:46,879 --> 00:00:49,680
ali and google

22
00:00:49,680 --> 00:00:51,840
transfer tech has attracted intensive

23
00:00:51,840 --> 00:00:54,640
research in recent years leading to rich

24
00:00:54,640 --> 00:00:56,960
conclusions in the lab environment

25
00:00:56,960 --> 00:00:59,440
however there is no systematic study in

26
00:00:59,440 --> 00:01:00,719
the real world

27
00:01:00,719 --> 00:01:02,960
the question is why can't we simply

28
00:01:02,960 --> 00:01:05,040
generalize the lab conclusions to the

29
00:01:05,040 --> 00:01:06,159
real world

30
00:01:06,159 --> 00:01:07,600
this is because there are many

31
00:01:07,600 --> 00:01:09,520
differences between the lab targets and

32
00:01:09,520 --> 00:01:11,040
the real targets

33
00:01:11,040 --> 00:01:14,000
first the real model is far more complex

34
00:01:14,000 --> 00:01:16,080
because they are required to process

35
00:01:16,080 --> 00:01:18,640
much more complex tasks

36
00:01:18,640 --> 00:01:20,960
second the real model is better trained

37
00:01:20,960 --> 00:01:24,080
with larger data sets and more resources

38
00:01:24,080 --> 00:01:24,960
as

39
00:01:24,960 --> 00:01:27,439
which is not affordable by many local

40
00:01:27,439 --> 00:01:28,799
targets

41
00:01:28,799 --> 00:01:31,759
so serial input is high resolution but

42
00:01:31,759 --> 00:01:34,159
many previous app conclusions are drawn

43
00:01:34,159 --> 00:01:36,479
from low resolution images such as

44
00:01:36,479 --> 00:01:40,240
cipher which is only 32x32

45
00:01:40,240 --> 00:01:42,479
in addition the real model admits

46
00:01:42,479 --> 00:01:44,560
different shapes

47
00:01:44,560 --> 00:01:46,159
suggesting sets are different to

48
00:01:46,159 --> 00:01:48,399
pre-processing techniques which are not

49
00:01:48,399 --> 00:01:51,759
transparent to the attackers applied

50
00:01:51,759 --> 00:01:54,240
last serial article structure is more

51
00:01:54,240 --> 00:01:57,360
ambiguous for example the left image

52
00:01:57,360 --> 00:02:00,000
which is taken from minutes is predicted

53
00:02:00,000 --> 00:02:03,200
to be text number or symbol but actually

54
00:02:03,200 --> 00:02:05,200
we are expecting a prediction of digit

55
00:02:05,200 --> 00:02:07,200
zero

56
00:02:07,200 --> 00:02:09,758
in addition the red image which is taken

57
00:02:09,758 --> 00:02:11,599
from image nets

58
00:02:11,599 --> 00:02:14,239
is predicted to be sports sphere and

59
00:02:14,239 --> 00:02:16,319
baseball however so grandchildren's

60
00:02:16,319 --> 00:02:20,238
baseball does not get the highest score

61
00:02:21,040 --> 00:02:23,120
we perform a systematic evaluation of

62
00:02:23,120 --> 00:02:25,760
the transfer under transfer attacks in

63
00:02:25,760 --> 00:02:27,440
the real settings

64
00:02:27,440 --> 00:02:30,160
we identify the ambiguity in the success

65
00:02:30,160 --> 00:02:33,200
criteria of a real transfer attack and

66
00:02:33,200 --> 00:02:36,160
propose corresponding solutions

67
00:02:36,160 --> 00:02:37,680
based on these

68
00:02:37,680 --> 00:02:40,480
techniques we explore possible factors

69
00:02:40,480 --> 00:02:42,959
that can affect a real transfer attack

70
00:02:42,959 --> 00:02:47,120
using statistically sufficient settings

71
00:02:47,120 --> 00:02:49,680
our evaluation goes like this this is

72
00:02:49,680 --> 00:02:51,920
our pipeline of a real transfer attack

73
00:02:51,920 --> 00:02:53,440
the only difference between a real

74
00:02:53,440 --> 00:02:56,000
transfer tech and a lab transfer tech is

75
00:02:56,000 --> 00:02:58,480
that the target model is not a cloud

76
00:02:58,480 --> 00:03:01,200
model and the response contains multiple

77
00:03:01,200 --> 00:03:03,680
levels of ambiguities

78
00:03:03,680 --> 00:03:05,519
more specifically there are two

79
00:03:05,519 --> 00:03:08,319
ambiguities in success criteria the

80
00:03:08,319 --> 00:03:11,200
first one is called class inconsistency

81
00:03:11,200 --> 00:03:14,159
there are three types of inconsistent

82
00:03:14,159 --> 00:03:15,840
inconsistencies

83
00:03:15,840 --> 00:03:18,080
first the prediction could be more

84
00:03:18,080 --> 00:03:20,720
specific than the ground truth label for

85
00:03:20,720 --> 00:03:23,280
example a image could be predicted as

86
00:03:23,280 --> 00:03:25,920
gun or knife compared to the conscious

87
00:03:25,920 --> 00:03:27,840
labor weapon

88
00:03:27,840 --> 00:03:30,319
second it could be more general as a

89
00:03:30,319 --> 00:03:32,319
ground truth label for example as we

90
00:03:32,319 --> 00:03:34,799
have already seen before sports for

91
00:03:34,799 --> 00:03:37,440
grantor's label baseball

92
00:03:37,440 --> 00:03:39,360
third it's going to use different names

93
00:03:39,360 --> 00:03:42,000
to describe the same object

94
00:03:42,000 --> 00:03:44,840
such as using different

95
00:03:44,840 --> 00:03:47,360
languages the other problem is called

96
00:03:47,360 --> 00:03:49,920
multiple prediction problem for example

97
00:03:49,920 --> 00:03:52,319
here is an exam here is an image

98
00:03:52,319 --> 00:03:54,480
containing a computer a cat and a

99
00:03:54,480 --> 00:03:56,879
keyboard the local ground choose label

100
00:03:56,879 --> 00:04:00,400
is keyboard but the cloud pretty awesome

101
00:04:00,400 --> 00:04:02,640
so they are not mistakes which prevents

102
00:04:02,640 --> 00:04:03,760
us from

103
00:04:03,760 --> 00:04:07,518
applying the traditional metrics

104
00:04:08,159 --> 00:04:10,239
to address the class inconsistency

105
00:04:10,239 --> 00:04:12,319
problem we manually build an equivalence

106
00:04:12,319 --> 00:04:14,480
dictionary for each platform from the

107
00:04:14,480 --> 00:04:16,959
predictions of cd images we use the

108
00:04:16,959 --> 00:04:19,519
prediction of seed images to avoid to

109
00:04:19,519 --> 00:04:21,440
avoid potential

110
00:04:21,440 --> 00:04:24,320
bias introduced by adversarial examples

111
00:04:24,320 --> 00:04:26,800
for example for the ground truth label

112
00:04:26,800 --> 00:04:28,720
engine we use the

113
00:04:28,720 --> 00:04:31,360
equivalence labels containing motor

114
00:04:31,360 --> 00:04:34,000
motorcycle engine etc

115
00:04:34,000 --> 00:04:35,759
this labels in the dictionary is

116
00:04:35,759 --> 00:04:37,680
considered equivalent to the ground to

117
00:04:37,680 --> 00:04:39,759
stable engine

118
00:04:39,759 --> 00:04:41,360
to address the multiple prediction

119
00:04:41,360 --> 00:04:43,199
problem we use the cutting extraction

120
00:04:43,199 --> 00:04:46,080
method our precision our principle is

121
00:04:46,080 --> 00:04:48,160
that the cutting threshold should filter

122
00:04:48,160 --> 00:04:49,919
out the most predictions while

123
00:04:49,919 --> 00:04:52,800
maintaining accuracy on scene images

124
00:04:52,800 --> 00:04:54,720
predictions with a score smaller than

125
00:04:54,720 --> 00:04:56,720
threshold are excluded from the

126
00:04:56,720 --> 00:04:59,360
evaluation

127
00:05:00,000 --> 00:05:03,280
therefore based on these techniques we

128
00:05:03,280 --> 00:05:05,840
modify the evaluation matches as follows

129
00:05:05,840 --> 00:05:08,639
if none of the equivalent labels of the

130
00:05:08,639 --> 00:05:11,039
ground choose label is in the prediction

131
00:05:11,039 --> 00:05:14,160
then we cause a is misclassified

132
00:05:14,160 --> 00:05:16,479
in addition if any of the occurrence

133
00:05:16,479 --> 00:05:18,240
levels of the ground truth label is

134
00:05:18,240 --> 00:05:22,160
infiltration then we causes a match

135
00:05:22,160 --> 00:05:24,400
so the misconception rate is an

136
00:05:24,400 --> 00:05:27,280
extension of the traditional uh transfer

137
00:05:27,280 --> 00:05:30,160
metrics for untargeted transfer attack

138
00:05:30,160 --> 00:05:32,720
but and the matching rate is an

139
00:05:32,720 --> 00:05:35,280
extension of transfer rates in a

140
00:05:35,280 --> 00:05:37,759
targeted transfer attack

141
00:05:37,759 --> 00:05:39,759
in the paper we also measured the

142
00:05:39,759 --> 00:05:41,600
transfer rates in general classification

143
00:05:41,600 --> 00:05:43,520
tasks but for gravity in the

144
00:05:43,520 --> 00:05:46,160
presentation we omit these results this

145
00:05:46,160 --> 00:05:48,320
results are consistent to the object

146
00:05:48,320 --> 00:05:50,400
classification tasks

147
00:05:50,400 --> 00:05:52,639
in addition we also omits the

148
00:05:52,639 --> 00:05:54,560
experimental details here and only

149
00:05:54,560 --> 00:05:57,840
presents the conclusions

150
00:05:58,000 --> 00:06:00,400
so first to consider the picture is the

151
00:06:00,400 --> 00:06:03,039
platform robustness we found that the

152
00:06:03,039 --> 00:06:05,280
kernel models are not unbreakable on the

153
00:06:05,280 --> 00:06:07,919
transfer tax even if the attack status

154
00:06:07,919 --> 00:06:11,600
absolute attack uniform at random

155
00:06:11,600 --> 00:06:13,600
we found that all transfer rates are

156
00:06:13,600 --> 00:06:15,440
significantly positive which is

157
00:06:15,440 --> 00:06:17,600
different to the left conclusions of new

158
00:06:17,600 --> 00:06:19,039
et cetera

159
00:06:19,039 --> 00:06:20,720
that the

160
00:06:20,720 --> 00:06:24,479
targeted attacks almost never transfer

161
00:06:24,479 --> 00:06:27,120
in addition we also find that targets

162
00:06:27,120 --> 00:06:29,120
with higher accuracy are possible to be

163
00:06:29,120 --> 00:06:31,919
less robust to transfer attacks all

164
00:06:31,919 --> 00:06:34,160
these steps suggest that transfer attack

165
00:06:34,160 --> 00:06:36,319
in the real world cannot be overlooked

166
00:06:36,319 --> 00:06:38,880
as they have not non trivial success

167
00:06:38,880 --> 00:06:40,160
rates but

168
00:06:40,160 --> 00:06:43,600
ignorable marginal costs

169
00:06:43,600 --> 00:06:45,600
the second property as the second fact

170
00:06:45,600 --> 00:06:48,400
factor we consider is pre-training

171
00:06:48,400 --> 00:06:50,160
we found that preaching improves the

172
00:06:50,160 --> 00:06:51,919
matching rate but decrease the mass

173
00:06:51,919 --> 00:06:53,280
cascading rate

174
00:06:53,280 --> 00:06:55,360
this actually contributes the common

175
00:06:55,360 --> 00:06:58,400
notion of motor similarity as if we

176
00:06:58,400 --> 00:07:01,039
assume protein improves similarity then

177
00:07:01,039 --> 00:07:03,039
transfer rates should all be improved

178
00:07:03,039 --> 00:07:05,360
otherwise this should be decreased

179
00:07:05,360 --> 00:07:07,599
similar phenomena is observed for other

180
00:07:07,599 --> 00:07:09,440
factors as well so this is not a

181
00:07:09,440 --> 00:07:10,960
coincidence

182
00:07:10,960 --> 00:07:13,440
this suggests that defining similarity

183
00:07:13,440 --> 00:07:17,840
for model 6 is extremely difficult

184
00:07:18,479 --> 00:07:20,080
the third factor we consider is

185
00:07:20,080 --> 00:07:22,160
adversarial algorithms

186
00:07:22,160 --> 00:07:24,080
which is also the common focus of

187
00:07:24,080 --> 00:07:26,000
previous studies

188
00:07:26,000 --> 00:07:28,639
we found that strong algorithms such as

189
00:07:28,639 --> 00:07:31,840
w2 can have weak transferability in

190
00:07:31,840 --> 00:07:34,880
contrast the weak algorithm fgsm

191
00:07:34,880 --> 00:07:38,080
achieves the best transfer rates

192
00:07:38,080 --> 00:07:39,840
this is consistent to the finding of

193
00:07:39,840 --> 00:07:43,599
sewage central in the lab environments

194
00:07:43,599 --> 00:07:46,240
we also found that iterative algorithms

195
00:07:46,240 --> 00:07:48,240
transfer less than their single step

196
00:07:48,240 --> 00:07:50,000
counterparts

197
00:07:50,000 --> 00:07:52,560
this stress set probably most the most

198
00:07:52,560 --> 00:07:55,280
transferable information is a gradient

199
00:07:55,280 --> 00:07:58,000
with regard to the same image as other

200
00:07:58,000 --> 00:07:59,199
attacks

201
00:07:59,199 --> 00:08:00,960
use implies

202
00:08:00,960 --> 00:08:03,840
as information but also but still get

203
00:08:03,840 --> 00:08:06,960
worse transfer rates

204
00:08:07,039 --> 00:08:09,360
we also consider the transfer as a

205
00:08:09,360 --> 00:08:11,759
surrogate complexity

206
00:08:11,759 --> 00:08:13,440
as we are in the context of neural

207
00:08:13,440 --> 00:08:15,520
networks we define the

208
00:08:15,520 --> 00:08:17,840
surrogate complexity by the depths of

209
00:08:17,840 --> 00:08:20,479
neural networks we found that this

210
00:08:20,479 --> 00:08:23,280
complexity have has a non-monotonic

211
00:08:23,280 --> 00:08:26,240
effect on the transferability

212
00:08:26,240 --> 00:08:28,720
at fabricates with a suitable depth what

213
00:08:28,720 --> 00:08:30,960
performs both simpler and more complex

214
00:08:30,960 --> 00:08:33,120
counterparts

215
00:08:33,120 --> 00:08:35,200
this suggests that probably there are

216
00:08:35,200 --> 00:08:37,760
optimal capacity for surrogates

217
00:08:37,760 --> 00:08:40,080
which should depend on the task and the

218
00:08:40,080 --> 00:08:42,479
target

219
00:08:43,039 --> 00:08:45,279
we also discussed the architecture we

220
00:08:45,279 --> 00:08:47,200
found that all surrogate architectures

221
00:08:47,200 --> 00:08:49,600
have similar sub transfer rates

222
00:08:49,600 --> 00:08:51,920
this is different to the suit central's

223
00:08:51,920 --> 00:08:54,480
conclusion that vgg transfers well where

224
00:08:54,480 --> 00:08:57,360
other architectures almost transfer

225
00:08:57,360 --> 00:08:59,839
this just said in a real transfer attack

226
00:08:59,839 --> 00:09:02,240
no preference for surrogate architecture

227
00:09:02,240 --> 00:09:03,519
is

228
00:09:03,519 --> 00:09:06,160
to exist

229
00:09:07,519 --> 00:09:09,360
we also discussed the sampling

230
00:09:09,360 --> 00:09:11,680
properties as a vector

231
00:09:11,680 --> 00:09:13,920
the first property we did we discussed

232
00:09:13,920 --> 00:09:16,560
is the measured norm of the perturbation

233
00:09:16,560 --> 00:09:18,959
we found that l2 norm shows strong

234
00:09:18,959 --> 00:09:21,519
correlation to the transference while l

235
00:09:21,519 --> 00:09:23,680
infinity norm roughly has no correlation

236
00:09:23,680 --> 00:09:26,000
to transfer rates if we

237
00:09:26,000 --> 00:09:29,839
extract the effect of l2 norm

238
00:09:29,839 --> 00:09:32,320
we also find that increasing l2 norm

239
00:09:32,320 --> 00:09:34,480
while keeping our infinity norm fixed

240
00:09:34,480 --> 00:09:37,279
can greatly increase the transferability

241
00:09:37,279 --> 00:09:40,640
while the upset is generally not true

242
00:09:40,640 --> 00:09:42,560
since stress said transfer attacks

243
00:09:42,560 --> 00:09:44,560
prefer the dense perturbations than its

244
00:09:44,560 --> 00:09:47,119
passwords

245
00:09:48,000 --> 00:09:50,000
we consider conflict adversarial

246
00:09:50,000 --> 00:09:51,519
confidence as well

247
00:09:51,519 --> 00:09:54,160
uh two different definitions of

248
00:09:54,160 --> 00:09:56,240
adversarial confidence are considered

249
00:09:56,240 --> 00:09:57,839
the first one is called scaling

250
00:09:57,839 --> 00:10:00,080
sensitive copper which is exactly the

251
00:10:00,080 --> 00:10:02,000
same copper value defined in the cw

252
00:10:02,000 --> 00:10:03,600
attack

253
00:10:03,600 --> 00:10:05,600
we also consider skilling insensitive

254
00:10:05,600 --> 00:10:07,360
copper which is defined to be the

255
00:10:07,360 --> 00:10:09,519
indifferent should be the difference in

256
00:10:09,519 --> 00:10:11,760
the soft mass output which is the most

257
00:10:11,760 --> 00:10:13,839
likely class and the second most likely

258
00:10:13,839 --> 00:10:15,360
class

259
00:10:15,360 --> 00:10:17,680
the experimental results show that the

260
00:10:17,680 --> 00:10:20,079
correlation between ssk and the transfer

261
00:10:20,079 --> 00:10:21,839
rate is not significant

262
00:10:21,839 --> 00:10:24,720
on the contrary smk shows a very

263
00:10:24,720 --> 00:10:26,640
significant correlation to transfer

264
00:10:26,640 --> 00:10:28,000
rates

265
00:10:28,000 --> 00:10:30,320
in addition we also observed that

266
00:10:30,320 --> 00:10:32,959
increasing ssk for cw attack does not

267
00:10:32,959 --> 00:10:35,360
increase transfer rates in many cases

268
00:10:35,360 --> 00:10:37,760
which is upset to the original paper

269
00:10:37,760 --> 00:10:39,279
which concludes that in the level

270
00:10:39,279 --> 00:10:41,680
environments this should increase the

271
00:10:41,680 --> 00:10:43,760
transfer rates

272
00:10:43,760 --> 00:10:46,160
therefore we conclude that in our real

273
00:10:46,160 --> 00:10:48,560
transfer attack slk is a better

274
00:10:48,560 --> 00:10:52,959
instrument for transferability than ssk

275
00:10:53,360 --> 00:10:55,360
we also considered the intrinsic

276
00:10:55,360 --> 00:10:57,680
classical hardness

277
00:10:57,680 --> 00:10:59,760
we found that out of several examples

278
00:10:59,760 --> 00:11:01,839
generating the foreseen images that are

279
00:11:01,839 --> 00:11:04,720
misconceptible have transfers better

280
00:11:04,720 --> 00:11:06,880
than a is generated from correctly

281
00:11:06,880 --> 00:11:09,440
classified images

282
00:11:09,440 --> 00:11:11,760
the experiment shows that for all the

283
00:11:11,760 --> 00:11:13,920
adversarial algorithms and all their

284
00:11:13,920 --> 00:11:16,560
targets the former transfers at least as

285
00:11:16,560 --> 00:11:18,160
good as the letter

286
00:11:18,160 --> 00:11:20,480
and in many cases the former has much

287
00:11:20,480 --> 00:11:24,000
larger transfer rates than the letter

288
00:11:24,000 --> 00:11:26,800
this suggests that thin images that are

289
00:11:26,800 --> 00:11:28,800
harder to classify are easier for

290
00:11:28,800 --> 00:11:31,599
transfer tags

291
00:11:32,240 --> 00:11:34,399
there are more observations experimental

292
00:11:34,399 --> 00:11:36,880
results and analysis in paper we

293
00:11:36,880 --> 00:11:38,959
strongly recommend the audience to refer

294
00:11:38,959 --> 00:11:41,839
to our paper for more

295
00:11:41,839 --> 00:11:46,040
thank you very much for your attention

