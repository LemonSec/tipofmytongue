1
00:00:01,199 --> 00:00:04,720
hello everyone i'm chile from cespa

2
00:00:04,720 --> 00:00:06,879
today i'm going to share with you this

3
00:00:06,879 --> 00:00:09,519
work model stealing attack against

4
00:00:09,519 --> 00:00:12,320
inductive rock neural network

5
00:00:12,320 --> 00:00:15,120
this is a joint work with yun ufe and

6
00:00:15,120 --> 00:00:19,279
young let's get started

7
00:00:20,160 --> 00:00:22,560
nowadays machine learning has shown

8
00:00:22,560 --> 00:00:24,880
remarkable performance on various

9
00:00:24,880 --> 00:00:26,480
applications

10
00:00:26,480 --> 00:00:29,840
for example alpha 4 shows great ability

11
00:00:29,840 --> 00:00:33,440
in predicting the protein's architecture

12
00:00:33,440 --> 00:00:36,640
an open ai published gpt3 that can

13
00:00:36,640 --> 00:00:38,960
automatically write emails and business

14
00:00:38,960 --> 00:00:41,840
proposals or even writing creative

15
00:00:41,840 --> 00:00:44,320
fiction

16
00:00:44,320 --> 00:00:46,719
and a github compiler can be used for

17
00:00:46,719 --> 00:00:49,440
code auto generation they can convert

18
00:00:49,440 --> 00:00:51,840
common to code and autofill for

19
00:00:51,840 --> 00:00:54,960
repetitive codes

20
00:00:55,680 --> 00:00:59,039
in the real world many data a graph

21
00:00:59,039 --> 00:01:02,000
which has a combinatorial structure have

22
00:01:02,000 --> 00:01:04,799
arbitrary size and contains multi-modal

23
00:01:04,799 --> 00:01:06,159
information

24
00:01:06,159 --> 00:01:08,560
for example social network protein

25
00:01:08,560 --> 00:01:11,600
architecture and knowledge graph

26
00:01:11,600 --> 00:01:14,400
to better leverage the graph information

27
00:01:14,400 --> 00:01:16,880
a new type of machine learning model

28
00:01:16,880 --> 00:01:19,040
called graph neural network have been

29
00:01:19,040 --> 00:01:21,600
proposed

30
00:01:22,000 --> 00:01:24,799
to trend a graph neural network we first

31
00:01:24,799 --> 00:01:27,520
need a graph data set that contains node

32
00:01:27,520 --> 00:01:28,640
feature

33
00:01:28,640 --> 00:01:31,520
and a connection between different nodes

34
00:01:31,520 --> 00:01:33,840
the model can be trained for different

35
00:01:33,840 --> 00:01:34,799
tasks

36
00:01:34,799 --> 00:01:37,200
such as node classification link

37
00:01:37,200 --> 00:01:41,520
prediction graph classification etc

38
00:01:41,520 --> 00:01:43,360
in this work we focus on node

39
00:01:43,360 --> 00:01:45,520
classification tasks

40
00:01:45,520 --> 00:01:47,840
we'll introduce two settings of graph

41
00:01:47,840 --> 00:01:49,759
neural nets

42
00:01:49,759 --> 00:01:51,680
in a transductive setting

43
00:01:51,680 --> 00:01:53,920
the test graph is the same as the

44
00:01:53,920 --> 00:01:55,280
training graph

45
00:01:55,280 --> 00:01:57,759
and the graph neural net is able to

46
00:01:57,759 --> 00:02:02,320
predict the class of unlabeled nodes

47
00:02:02,320 --> 00:02:04,960
a more general setting is the inductive

48
00:02:04,960 --> 00:02:05,920
setting

49
00:02:05,920 --> 00:02:08,800
where the model can generalize to other

50
00:02:08,800 --> 00:02:11,520
unseen graphs with some type of node

51
00:02:11,520 --> 00:02:13,599
features

52
00:02:13,599 --> 00:02:15,680
in this paper we focus on node

53
00:02:15,680 --> 00:02:17,920
classification test in the inductive

54
00:02:17,920 --> 00:02:20,920
setting

55
00:02:22,000 --> 00:02:24,720
so a graph neural net can be represented

56
00:02:24,720 --> 00:02:26,560
as two parts

57
00:02:26,560 --> 00:02:28,080
an encoder

58
00:02:28,080 --> 00:02:30,720
and a classifier

59
00:02:30,720 --> 00:02:32,640
the prediction generated from the

60
00:02:32,640 --> 00:02:34,800
classifier can be used for node

61
00:02:34,800 --> 00:02:38,160
classification task for example

62
00:02:38,160 --> 00:02:40,640
meanwhile the middle layer embedding can

63
00:02:40,640 --> 00:02:43,200
be used for different dungeon tasks such

64
00:02:43,200 --> 00:02:46,400
as community detection

65
00:02:46,400 --> 00:02:47,440
also

66
00:02:47,440 --> 00:02:49,680
the embedding can be further used to

67
00:02:49,680 --> 00:02:52,080
generate the disney production for

68
00:02:52,080 --> 00:02:55,440
visualization purpose

69
00:02:57,360 --> 00:02:59,519
draft neural nets have been applied by

70
00:02:59,519 --> 00:03:02,319
many big companies like google meta and

71
00:03:02,319 --> 00:03:04,000
twitter

72
00:03:04,000 --> 00:03:06,480
once the model is trained it can

73
00:03:06,480 --> 00:03:09,680
generate different type of response

74
00:03:09,680 --> 00:03:12,239
for example prediction embedding disney

75
00:03:12,239 --> 00:03:16,239
projection as we mentioned before

76
00:03:17,360 --> 00:03:19,519
despite being powerful the data

77
00:03:19,519 --> 00:03:22,319
collection and the training process is

78
00:03:22,319 --> 00:03:25,440
also expensive as it may takes a lot of

79
00:03:25,440 --> 00:03:28,560
effort in collecting the graph data and

80
00:03:28,560 --> 00:03:32,480
tuning the hyper parameter of the model

81
00:03:32,480 --> 00:03:35,280
after that the trend model can be shared

82
00:03:35,280 --> 00:03:40,239
via cloud platform for commercial usage

83
00:03:41,920 --> 00:03:43,680
for a benign user

84
00:03:43,680 --> 00:03:46,959
he can leverage a query graph to query

85
00:03:46,959 --> 00:03:50,239
the api of the model and obtain the

86
00:03:50,239 --> 00:03:53,040
corresponding response to view dungeon

87
00:03:53,040 --> 00:03:55,840
applications

88
00:03:56,959 --> 00:03:58,400
in this paper

89
00:03:58,400 --> 00:04:01,360
we consider an adversary whose goal is

90
00:04:01,360 --> 00:04:04,319
to steal the functionality of the gm

91
00:04:04,319 --> 00:04:06,400
model

92
00:04:06,400 --> 00:04:09,040
concretely the anniversary query the

93
00:04:09,040 --> 00:04:12,319
model with a query graph and obtain the

94
00:04:12,319 --> 00:04:14,400
corresponding output to train a

95
00:04:14,400 --> 00:04:15,920
surrogate model

96
00:04:15,920 --> 00:04:17,918
that has similar performance and

97
00:04:17,918 --> 00:04:22,079
behaviors as the target model

98
00:04:23,520 --> 00:04:26,080
the adversary has two goals

99
00:04:26,080 --> 00:04:29,199
step and reconnaissance

100
00:04:29,199 --> 00:04:32,000
the goal of the sap adversary is to

101
00:04:32,000 --> 00:04:34,720
build a surrogate model that match the

102
00:04:34,720 --> 00:04:37,759
accuracy of the target model

103
00:04:37,759 --> 00:04:40,320
the staff adversary's motivation is

104
00:04:40,320 --> 00:04:43,280
compromising the intellectual property

105
00:04:43,280 --> 00:04:46,160
and violating the confidentiality of the

106
00:04:46,160 --> 00:04:48,880
target model

107
00:04:48,880 --> 00:04:51,280
on the other hand the reconnaissance

108
00:04:51,280 --> 00:04:54,080
anniversary aimed to build a surrogate

109
00:04:54,080 --> 00:04:57,440
model that closely match the behavior of

110
00:04:57,440 --> 00:04:59,680
the target model

111
00:04:59,680 --> 00:05:02,400
that is the surrogate model seek an

112
00:05:02,400 --> 00:05:04,960
agreement to the target model on any

113
00:05:04,960 --> 00:05:06,639
input

114
00:05:06,639 --> 00:05:09,680
this enable the adversary to leverage it

115
00:05:09,680 --> 00:05:12,160
as a stepping stone to launch further

116
00:05:12,160 --> 00:05:14,000
attacks

117
00:05:14,000 --> 00:05:16,400
for instance the university can craft

118
00:05:16,400 --> 00:05:19,039
adversarial examples using the surrogate

119
00:05:19,039 --> 00:05:21,280
model instead of

120
00:05:21,280 --> 00:05:24,000
risking potentially detectable queries

121
00:05:24,000 --> 00:05:26,880
to the target model

122
00:05:28,240 --> 00:05:31,759
now let's talk about the attack taxonomy

123
00:05:31,759 --> 00:05:34,720
we consider the adversary has the query

124
00:05:34,720 --> 00:05:37,440
access to the black box target model

125
00:05:37,440 --> 00:05:39,600
and can obtain the response such as

126
00:05:39,600 --> 00:05:43,600
prediction embedding and projection

127
00:05:43,600 --> 00:05:45,919
and the anniversary may also have a

128
00:05:45,919 --> 00:05:48,800
query graph which has an adjacency

129
00:05:48,800 --> 00:05:49,840
matrix

130
00:05:49,840 --> 00:05:53,440
and all nodes features

131
00:05:53,440 --> 00:05:55,759
in a more extreme case

132
00:05:55,759 --> 00:05:58,800
the adjacency matrix is not accessible

133
00:05:58,800 --> 00:06:00,880
and the anniversary need to reconstruct

134
00:06:00,880 --> 00:06:02,639
the graph structure

135
00:06:02,639 --> 00:06:06,240
with the node feature first

136
00:06:06,240 --> 00:06:08,720
we propose two type of attacks

137
00:06:08,720 --> 00:06:11,360
where type 1 attack has the adjacency

138
00:06:11,360 --> 00:06:14,160
matrix and type 2 attack does not have

139
00:06:14,160 --> 00:06:16,319
it

140
00:06:17,120 --> 00:06:19,360
and for each type of attack the target

141
00:06:19,360 --> 00:06:22,000
model has three type of response

142
00:06:22,000 --> 00:06:26,319
which in total result in six attacks

143
00:06:28,080 --> 00:06:31,120
we then discuss our attack pipeline

144
00:06:31,120 --> 00:06:33,120
given a target model

145
00:06:33,120 --> 00:06:35,360
the attacker first need to reconstruct

146
00:06:35,360 --> 00:06:38,080
the graph structure of the query graph

147
00:06:38,080 --> 00:06:40,319
if they don't have it

148
00:06:40,319 --> 00:06:42,800
then the query graph can be used to

149
00:06:42,800 --> 00:06:44,960
query the target model

150
00:06:44,960 --> 00:06:47,919
and obtain the corresponding response

151
00:06:47,919 --> 00:06:49,280
such as

152
00:06:49,280 --> 00:06:53,440
a prediction embedding or projections

153
00:06:53,440 --> 00:06:54,400
then

154
00:06:54,400 --> 00:06:57,039
we use this query graph as well as

155
00:06:57,039 --> 00:07:00,319
response to trend the first part of the

156
00:07:00,319 --> 00:07:03,440
surrogate model leverage the label of

157
00:07:03,440 --> 00:07:05,919
nodes on the query graph to trend the

158
00:07:05,919 --> 00:07:08,160
second part of the circuit model to form

159
00:07:08,160 --> 00:07:12,440
a node classification model

160
00:07:17,039 --> 00:07:20,240
in our experiment we evaluate our attack

161
00:07:20,240 --> 00:07:22,960
using three popular gm models that is

162
00:07:22,960 --> 00:07:25,520
gene get and sage

163
00:07:25,520 --> 00:07:28,479
and we evaluate on six data sets

164
00:07:28,479 --> 00:07:30,960
regarding the configuration

165
00:07:30,960 --> 00:07:33,840
we randomly select subgroup with 20 of

166
00:07:33,840 --> 00:07:36,080
node as the training graph

167
00:07:36,080 --> 00:07:38,400
and we sample subgraph with 30 percent

168
00:07:38,400 --> 00:07:40,560
of nodes at the query graph

169
00:07:40,560 --> 00:07:42,880
we later we show that the preview graph

170
00:07:42,880 --> 00:07:44,960
are still effective with much fewer

171
00:07:44,960 --> 00:07:46,879
nodes

172
00:07:46,879 --> 00:07:49,520
and then we sample a subgraph with 50 of

173
00:07:49,520 --> 00:07:51,280
the node as the testing graph to

174
00:07:51,280 --> 00:07:54,560
evaluate the performance

175
00:07:54,560 --> 00:07:57,120
regarding evaluation metric we use

176
00:07:57,120 --> 00:07:59,759
accuracy and fidelity where accuracy

177
00:07:59,759 --> 00:08:01,919
measured the performance on the original

178
00:08:01,919 --> 00:08:03,759
classification task

179
00:08:03,759 --> 00:08:06,160
and fidelity measured agreement between

180
00:08:06,160 --> 00:08:10,520
target and surrogate models

181
00:08:12,080 --> 00:08:14,560
we first show the performance of the

182
00:08:14,560 --> 00:08:16,960
original classification task

183
00:08:16,960 --> 00:08:20,319
we can observe that all gm model achieve

184
00:08:20,319 --> 00:08:23,360
good performance on all data sets

185
00:08:23,360 --> 00:08:25,360
which demonstrate that jointly

186
00:08:25,360 --> 00:08:27,759
considering node feature and graph

187
00:08:27,759 --> 00:08:29,520
structure are effective for

188
00:08:29,520 --> 00:08:32,400
classification

189
00:08:33,679 --> 00:08:35,839
we then show the performance of type 1

190
00:08:35,839 --> 00:08:36,880
attacks

191
00:08:36,880 --> 00:08:39,279
as we can see from table 5

192
00:08:39,279 --> 00:08:42,000
type 1 attack can be used sorry model

193
00:08:42,000 --> 00:08:44,000
close to the target model given the

194
00:08:44,000 --> 00:08:47,279
response is prediction or the node

195
00:08:47,279 --> 00:08:49,839
embeddings

196
00:08:50,480 --> 00:08:53,120
take the pubmed dataset as an example

197
00:08:53,120 --> 00:08:55,920
the target model achieves more than 0.9

198
00:08:55,920 --> 00:08:58,560
accuracy score while the surrogate model

199
00:08:58,560 --> 00:09:01,279
can consistently achieve at least like

200
00:09:01,279 --> 00:09:03,920
87 accuracy score

201
00:09:03,920 --> 00:09:07,760
these represent approximately 4 accuracy

202
00:09:07,760 --> 00:09:08,560
drop

203
00:09:08,560 --> 00:09:12,560
in all case compared to the target model

204
00:09:12,560 --> 00:09:14,880
we can also observe that type 1 attack

205
00:09:14,880 --> 00:09:17,120
can build circuit model that offer

206
00:09:17,120 --> 00:09:20,720
usable accuracy even if the response is

207
00:09:20,720 --> 00:09:25,680
only a two-dimensional disney projection

208
00:09:28,240 --> 00:09:30,640
we further conduct a study to show the

209
00:09:30,640 --> 00:09:33,040
similarity of the target and surrogate

210
00:09:33,040 --> 00:09:34,839
model

211
00:09:34,839 --> 00:09:37,760
concretely we first randomly select a

212
00:09:37,760 --> 00:09:40,560
query graph to query both the target and

213
00:09:40,560 --> 00:09:43,200
surrogate model and obtain the embedding

214
00:09:43,200 --> 00:09:45,120
as the response

215
00:09:45,120 --> 00:09:47,360
then we projected them into

216
00:09:47,360 --> 00:09:51,839
two-dimensional space using disney

217
00:09:52,640 --> 00:09:55,200
as we can see from this figure for both

218
00:09:55,200 --> 00:09:57,839
target and surrogate model they are able

219
00:09:57,839 --> 00:10:00,959
to separate nodes from different classes

220
00:10:00,959 --> 00:10:03,839
which are denoted with different colors

221
00:10:03,839 --> 00:10:05,839
we can see that different class lie in

222
00:10:05,839 --> 00:10:09,440
different regions on the figure

223
00:10:10,320 --> 00:10:11,360
also

224
00:10:11,360 --> 00:10:13,760
for embedding with the same class

225
00:10:13,760 --> 00:10:15,920
generated from target and surrogate

226
00:10:15,920 --> 00:10:18,560
model which are market as triangle and

227
00:10:18,560 --> 00:10:21,040
cross respectively

228
00:10:21,040 --> 00:10:24,160
most of them lie in the same area which

229
00:10:24,160 --> 00:10:26,720
shows that the node embedding generated

230
00:10:26,720 --> 00:10:29,360
from the target and surrogate model are

231
00:10:29,360 --> 00:10:33,480
closed with each other

232
00:10:35,760 --> 00:10:38,880
we then relax assumption of both target

233
00:10:38,880 --> 00:10:41,040
and surrogate models architecture and

234
00:10:41,040 --> 00:10:43,200
show the attack performance

235
00:10:43,200 --> 00:10:45,440
we can observe that attack performance

236
00:10:45,440 --> 00:10:47,600
are very stable

237
00:10:47,600 --> 00:10:50,000
for example in figure 2c

238
00:10:50,000 --> 00:10:52,079
when a target model is sage

239
00:10:52,079 --> 00:10:54,000
the surrogate model can still achieve

240
00:10:54,000 --> 00:10:57,519
like 0.88 accuracy with gene as its

241
00:10:57,519 --> 00:10:59,440
architecture

242
00:10:59,440 --> 00:11:01,760
which further shows our attack is very

243
00:11:01,760 --> 00:11:03,760
robust against different model

244
00:11:03,760 --> 00:11:06,480
architecture

245
00:11:07,839 --> 00:11:10,160
we then compare the performance of type

246
00:11:10,160 --> 00:11:12,480
1 attack and type 2 attacks

247
00:11:12,480 --> 00:11:14,560
here the type 2 attack means we need to

248
00:11:14,560 --> 00:11:16,640
reconstruct the graph structure first

249
00:11:16,640 --> 00:11:18,880
and then use the reconstruct graph to

250
00:11:18,880 --> 00:11:22,000
query the target model

251
00:11:22,320 --> 00:11:23,200
so

252
00:11:23,200 --> 00:11:25,360
in this figure the first row denotes the

253
00:11:25,360 --> 00:11:27,680
type 1 attack and the second row denotes

254
00:11:27,680 --> 00:11:30,640
the type 2 attacks we find that type 1

255
00:11:30,640 --> 00:11:34,399
and type 2 attack reach almost the same

256
00:11:34,399 --> 00:11:35,600
performance

257
00:11:35,600 --> 00:11:37,920
and even in some case type 2 attack

258
00:11:37,920 --> 00:11:40,240
performs better

259
00:11:40,240 --> 00:11:41,519
for instance

260
00:11:41,519 --> 00:11:44,560
the surrogate model accuracy is 0.84 and

261
00:11:44,560 --> 00:11:47,440
0.83 respectively

262
00:11:47,440 --> 00:11:49,760
in this case

263
00:11:49,760 --> 00:11:50,560
so

264
00:11:50,560 --> 00:11:52,800
we suspect the reason is that the road

265
00:11:52,800 --> 00:11:55,519
graph are not always optimal for the

266
00:11:55,519 --> 00:11:58,160
duncan task for different reasons

267
00:11:58,160 --> 00:12:00,800
for example the raw graph may contain

268
00:12:00,800 --> 00:12:03,680
noisy or incomplete information due to

269
00:12:03,680 --> 00:12:06,079
the error-prone data collection or the

270
00:12:06,079 --> 00:12:08,320
structure do not reflect ideal graph

271
00:12:08,320 --> 00:12:11,120
topology after feature extraction and

272
00:12:11,120 --> 00:12:13,920
transformation

273
00:12:13,920 --> 00:12:16,000
the query graph learned by the graph

274
00:12:16,000 --> 00:12:18,399
reconstruction framework in our type 2

275
00:12:18,399 --> 00:12:20,560
attack are optimized toward the

276
00:12:20,560 --> 00:12:22,560
downstream tasks for example node

277
00:12:22,560 --> 00:12:24,079
classification

278
00:12:24,079 --> 00:12:26,320
and this may achieve better performance

279
00:12:26,320 --> 00:12:29,200
in some cases

280
00:12:30,560 --> 00:12:32,639
we then study the effect of graph

281
00:12:32,639 --> 00:12:35,040
reconstruction method to the final

282
00:12:35,040 --> 00:12:36,560
attack performance

283
00:12:36,560 --> 00:12:39,519
we find that compared to random and k n

284
00:12:39,519 --> 00:12:41,279
graph reconstruction

285
00:12:41,279 --> 00:12:43,839
the idgl reconstruction achieved better

286
00:12:43,839 --> 00:12:45,440
performance

287
00:12:45,440 --> 00:12:48,160
this indicates that an effective graph

288
00:12:48,160 --> 00:12:50,480
reconstruction master does benefit the

289
00:12:50,480 --> 00:12:53,839
final attack performance

290
00:12:54,639 --> 00:12:55,839
later

291
00:12:55,839 --> 00:12:58,320
we show the size how the size of the

292
00:12:58,320 --> 00:13:01,760
query graph affect attack performance

293
00:13:01,760 --> 00:13:04,240
as we can see from figure 6

294
00:13:04,240 --> 00:13:07,040
the attack performance are very stable

295
00:13:07,040 --> 00:13:09,600
and our attack are still effective with

296
00:13:09,600 --> 00:13:11,600
only three percent of node in the data

297
00:13:11,600 --> 00:13:14,320
set to form the query graph

298
00:13:14,320 --> 00:13:16,639
this further shows the severe risk of

299
00:13:16,639 --> 00:13:18,839
models dealing attack against

300
00:13:18,839 --> 00:13:21,920
gn because the anniversary can basically

301
00:13:21,920 --> 00:13:24,480
leverage much less data to launch an

302
00:13:24,480 --> 00:13:27,440
effective attack

303
00:13:29,120 --> 00:13:32,000
we explore a straightforward defense

304
00:13:32,000 --> 00:13:34,000
that is adding gaussian noise to the

305
00:13:34,000 --> 00:13:36,959
response of gns as we can see from

306
00:13:36,959 --> 00:13:39,920
figure 7 the gaussian noise only

307
00:13:39,920 --> 00:13:42,480
slightly affect the performance of the

308
00:13:42,480 --> 00:13:45,440
surrogate model

309
00:13:45,519 --> 00:13:47,600
so which means more effective defense

310
00:13:47,600 --> 00:13:50,240
are still needed in the future we plan

311
00:13:50,240 --> 00:13:52,480
to investigate more effective defense

312
00:13:52,480 --> 00:13:54,560
based on differential privacy and

313
00:13:54,560 --> 00:13:57,760
watermarking technique

314
00:13:59,279 --> 00:14:01,839
for a brief summary of our work we

315
00:14:01,839 --> 00:14:04,079
performed the first model ceiling attack

316
00:14:04,079 --> 00:14:06,800
against inductive graph neural nets and

317
00:14:06,800 --> 00:14:10,240
we propose a systematic design of threat

318
00:14:10,240 --> 00:14:13,199
model and different attack scenarios

319
00:14:13,199 --> 00:14:15,760
and our attack have no different type of

320
00:14:15,760 --> 00:14:19,519
response and achieve strong performance

321
00:14:19,519 --> 00:14:22,399
our code is available at this link

322
00:14:22,399 --> 00:14:24,959
this is all of my talk and are happy to

323
00:14:24,959 --> 00:14:28,439
take your question

324
00:14:30,920 --> 00:14:34,969
[Applause]

325
00:14:36,639 --> 00:14:38,720
great thank you very much and if i talk

326
00:14:38,720 --> 00:14:41,279
uh we can see you uh

327
00:14:41,279 --> 00:14:43,519
and probably hear you yep i hear you as

328
00:14:43,519 --> 00:14:45,440
well anyone has questions yeah please

329
00:14:45,440 --> 00:14:47,680
come to the mic because this way it can

330
00:14:47,680 --> 00:14:50,880
be heard remotely as well

331
00:14:51,279 --> 00:14:53,199
hi interesting talk

332
00:14:53,199 --> 00:14:56,160
so how many queries do you need in order

333
00:14:56,160 --> 00:14:59,519
to build a reasonably accurate surrogate

334
00:14:59,519 --> 00:15:01,839
model

335
00:15:01,920 --> 00:15:04,240
uh yes thanks for question so actually

336
00:15:04,240 --> 00:15:06,959
in our experiment we see that maybe we

337
00:15:06,959 --> 00:15:08,720
can only use like

338
00:15:08,720 --> 00:15:10,639
only three percent of node in the data

339
00:15:10,639 --> 00:15:13,680
set that we are able to build a usable

340
00:15:13,680 --> 00:15:15,199
surrogate model that achieves similar

341
00:15:15,199 --> 00:15:19,599
accuracy or fidelity as a target model

342
00:15:24,399 --> 00:15:26,240
i'm not sure if you heard the question

343
00:15:26,240 --> 00:15:28,959
also um

344
00:15:29,120 --> 00:15:31,199
so so my question about the number of

345
00:15:31,199 --> 00:15:32,959
queries that you need to make to the

346
00:15:32,959 --> 00:15:34,639
original model in order to build the

347
00:15:34,639 --> 00:15:37,199
surrogate model it's not the the size of

348
00:15:37,199 --> 00:15:39,279
the query

349
00:15:39,279 --> 00:15:41,279
the number of times that you need to

350
00:15:41,279 --> 00:15:43,839
query the model

351
00:15:43,839 --> 00:15:46,399
uh actually the number of times is

352
00:15:46,399 --> 00:15:48,079
proportional to the total number of

353
00:15:48,079 --> 00:15:49,920
nodes because for each node you only

354
00:15:49,920 --> 00:15:51,519
need to query the target model once

355
00:15:51,519 --> 00:15:52,399
right

356
00:15:52,399 --> 00:15:54,240
so after you get the prediction that you

357
00:15:54,240 --> 00:15:56,560
can leverage this graph as well as the

358
00:15:56,560 --> 00:15:58,639
prediction to to train your surrogate

359
00:15:58,639 --> 00:16:00,959
model

360
00:16:01,440 --> 00:16:02,959
so

361
00:16:02,959 --> 00:16:05,600
three percent do you mean of

362
00:16:05,600 --> 00:16:07,839
the nodes

363
00:16:07,839 --> 00:16:09,519
like you only need three percent of

364
00:16:09,519 --> 00:16:11,600
query to train

365
00:16:11,600 --> 00:16:13,199
let's say when you trade your target

366
00:16:13,199 --> 00:16:16,000
model you need 100 of query but when you

367
00:16:16,000 --> 00:16:18,399
train your um let's say circuit model

368
00:16:18,399 --> 00:16:22,480
you only need three percent of query

369
00:16:22,480 --> 00:16:24,800
okay

370
00:16:27,440 --> 00:16:31,199
thank you um any other questions

371
00:16:31,199 --> 00:16:32,560
okay i actually had our questions

372
00:16:32,560 --> 00:16:33,920
because you mentioned uh differential

373
00:16:33,920 --> 00:16:35,920
privacy as a way to defend against this

374
00:16:35,920 --> 00:16:38,959
attack i was just wondering why like why

375
00:16:38,959 --> 00:16:42,399
you think it would be effective and yeah

376
00:16:42,399 --> 00:16:44,880
yeah it's a very good question so

377
00:16:44,880 --> 00:16:46,959
actually when we talking about

378
00:16:46,959 --> 00:16:49,680
differential privacy it may be able to

379
00:16:49,680 --> 00:16:51,920
hide some let's say we can use

380
00:16:51,920 --> 00:16:53,839
differential privacy to hide some age

381
00:16:53,839 --> 00:16:55,519
information so that

382
00:16:55,519 --> 00:16:58,480
maybe as we can also as we also see in

383
00:16:58,480 --> 00:16:59,839
the paper that

384
00:16:59,839 --> 00:17:02,399
the graph reconstruction method is

385
00:17:02,399 --> 00:17:06,240
very um essential or very important to

386
00:17:06,240 --> 00:17:08,400
to get a good attack performance like

387
00:17:08,400 --> 00:17:09,679
you you need to

388
00:17:09,679 --> 00:17:12,319
reconstruct the graph perfectly

389
00:17:12,319 --> 00:17:16,240
or nearly perfectly to to get a

390
00:17:16,240 --> 00:17:19,599
well-performed circuit model but if you

391
00:17:19,599 --> 00:17:21,439
leverage differential privacy let's say

392
00:17:21,439 --> 00:17:23,039
when you train your target model they

393
00:17:23,039 --> 00:17:23,919
may

394
00:17:23,919 --> 00:17:24,880
um

395
00:17:24,880 --> 00:17:26,640
let's say they may

396
00:17:26,640 --> 00:17:27,679
um

397
00:17:27,679 --> 00:17:29,919
forbid you to predict the h connection

398
00:17:29,919 --> 00:17:31,280
between different nodes so that the

399
00:17:31,280 --> 00:17:33,600
reconstruct graph may have lower quality

400
00:17:33,600 --> 00:17:35,679
and that make your attack less effective

401
00:17:35,679 --> 00:17:38,320
yeah oh i see so yeah so you would use

402
00:17:38,320 --> 00:17:40,160
differential privacy on the actual model

403
00:17:40,160 --> 00:17:42,240
um protection as opposed to training

404
00:17:42,240 --> 00:17:44,840
data because there is an intersection

405
00:17:44,840 --> 00:17:47,600
between yeah but it may also

406
00:17:47,600 --> 00:17:48,799
let's say

407
00:17:48,799 --> 00:17:52,400
decrease the target model's utility as

408
00:17:52,400 --> 00:17:54,880
for example if you reduce the total

409
00:17:54,880 --> 00:17:57,919
number of h let's say that the the

410
00:17:57,919 --> 00:17:59,679
original target model may take less

411
00:17:59,679 --> 00:18:02,080
information from its neighbor and give a

412
00:18:02,080 --> 00:18:03,840
less concrete prediction or less

413
00:18:03,840 --> 00:18:06,880
correctly prediction yeah

414
00:18:06,880 --> 00:18:08,960
thank you for clarifying chile any other

415
00:18:08,960 --> 00:18:11,280
questions personally

416
00:18:11,280 --> 00:18:12,799
okay thank you very much shinley for

417
00:18:12,799 --> 00:18:14,000
joining us

418
00:18:14,000 --> 00:18:14,830
[Music]

419
00:18:14,830 --> 00:18:18,830
[Applause]

