1
00:00:00,640 --> 00:00:01,760
great

2
00:00:01,760 --> 00:00:03,360
hello everyone my name is eric pauley

3
00:00:03,360 --> 00:00:05,440
i'm a phd student at penn state and the

4
00:00:05,440 --> 00:00:06,879
work i'll be discussing today which is

5
00:00:06,879 --> 00:00:08,559
in collaboration with my colleagues and

6
00:00:08,559 --> 00:00:11,200
my advisor dr patrick mcdaniel explores

7
00:00:11,200 --> 00:00:13,280
the risks of ip address reuse on public

8
00:00:13,280 --> 00:00:14,880
clouds

9
00:00:14,880 --> 00:00:15,679
now

10
00:00:15,679 --> 00:00:17,359
public clouds have revolutionized the

11
00:00:17,359 --> 00:00:18,960
way that organizations manage their

12
00:00:18,960 --> 00:00:20,480
computing infrastructure

13
00:00:20,480 --> 00:00:22,560
by eliminating upfront costs and

14
00:00:22,560 --> 00:00:24,560
offering near limitless scalability

15
00:00:24,560 --> 00:00:26,320
public clouds have become the de facto

16
00:00:26,320 --> 00:00:28,000
deployment method

17
00:00:28,000 --> 00:00:29,439
and the way that they achieve this is

18
00:00:29,439 --> 00:00:31,760
through resource sharing resources are

19
00:00:31,760 --> 00:00:33,440
provisioned onto shared underlying

20
00:00:33,440 --> 00:00:35,440
infrastructure and those resources are

21
00:00:35,440 --> 00:00:37,440
reused once cloud tenants no longer need

22
00:00:37,440 --> 00:00:38,320
them

23
00:00:38,320 --> 00:00:40,480
however as we've seen in the past this

24
00:00:40,480 --> 00:00:41,600
research sharing is not without

25
00:00:41,600 --> 00:00:42,879
downsides

26
00:00:42,879 --> 00:00:44,719
existing works have shown that memory

27
00:00:44,719 --> 00:00:46,960
compute and storage sharing can cause

28
00:00:46,960 --> 00:00:48,879
vulnerabilities when used in this shared

29
00:00:48,879 --> 00:00:51,039
setting however what we wanted to

30
00:00:51,039 --> 00:00:53,280
explore was how the specific resource

31
00:00:53,280 --> 00:00:55,600
life cycle of public clouds i.e resource

32
00:00:55,600 --> 00:00:57,360
reuse could have negative security

33
00:00:57,360 --> 00:00:59,120
implications

34
00:00:59,120 --> 00:01:00,719
and what we found was that this resource

35
00:01:00,719 --> 00:01:03,520
use could represent a real vulnerability

36
00:01:03,520 --> 00:01:05,680
consider the following scenario

37
00:01:05,680 --> 00:01:07,760
a tenant configures resources on the

38
00:01:07,760 --> 00:01:09,280
public cloud and assigns them ip

39
00:01:09,280 --> 00:01:11,840
addresses and they create configuration

40
00:01:11,840 --> 00:01:13,680
that causes clients to connect to those

41
00:01:13,680 --> 00:01:14,799
resources

42
00:01:14,799 --> 00:01:16,400
this also establishes a trust

43
00:01:16,400 --> 00:01:19,280
association with the resources

44
00:01:19,280 --> 00:01:21,360
next when a tenant no longer needs the

45
00:01:21,360 --> 00:01:23,360
resources they release them back to the

46
00:01:23,360 --> 00:01:25,439
cloud provider and the same ip address

47
00:01:25,439 --> 00:01:27,439
can be given to another tenant

48
00:01:27,439 --> 00:01:29,680
if the original tenant no longer is

49
00:01:29,680 --> 00:01:32,720
unable to remove configuration or no

50
00:01:32,720 --> 00:01:34,320
does not properly remove configuration

51
00:01:34,320 --> 00:01:36,400
that refers to the address we say that

52
00:01:36,400 --> 00:01:38,799
that configuration is latent

53
00:01:38,799 --> 00:01:40,400
clients will continue to connect to the

54
00:01:40,400 --> 00:01:42,159
address because of it

55
00:01:42,159 --> 00:01:44,479
if an adversary receives that ip and

56
00:01:44,479 --> 00:01:46,240
attempts to receive the data the

57
00:01:46,240 --> 00:01:48,079
sensitive data intended for that address

58
00:01:48,079 --> 00:01:49,920
intended for previous tenants they're

59
00:01:49,920 --> 00:01:51,520
said to be performing a cloud squatting

60
00:01:51,520 --> 00:01:53,119
attack

61
00:01:53,119 --> 00:01:54,720
in order to measure the prevalence and

62
00:01:54,720 --> 00:01:56,640
severity of cloud squatting we deployed

63
00:01:56,640 --> 00:01:58,320
a custom internet telescope to amazon

64
00:01:58,320 --> 00:02:01,759
web services for over 100 days in 2021

65
00:02:01,759 --> 00:02:03,439
the system is as follows

66
00:02:03,439 --> 00:02:05,200
note that although we use amazon web

67
00:02:05,200 --> 00:02:07,439
services terminology here the system and

68
00:02:07,439 --> 00:02:10,160
the implications are generalizable

69
00:02:10,160 --> 00:02:12,239
we deploy ec2 instances to amazon's

70
00:02:12,239 --> 00:02:14,560
public cloud and have tcp handlers that

71
00:02:14,560 --> 00:02:16,720
collects traffic on all ports it does

72
00:02:16,720 --> 00:02:18,480
preliminary analysis of this that needs

73
00:02:18,480 --> 00:02:20,319
to be done in real time and then also

74
00:02:20,319 --> 00:02:22,080
records the traffic for use in future

75
00:02:22,080 --> 00:02:23,920
analysis

76
00:02:23,920 --> 00:02:25,760
clients connect to these addresses and

77
00:02:25,760 --> 00:02:27,760
send data potentially data intended for

78
00:02:27,760 --> 00:02:29,920
previous tenants

79
00:02:29,920 --> 00:02:31,920
next in order to gain high coverage of

80
00:02:31,920 --> 00:02:34,160
aws's cloud we scale this up using a

81
00:02:34,160 --> 00:02:36,080
spot fleet across a variety of

82
00:02:36,080 --> 00:02:38,640
availability zones including 3 million

83
00:02:38,640 --> 00:02:40,160
servers provisioned throughout the study

84
00:02:40,160 --> 00:02:42,959
in amazon web services usd 1 region

85
00:02:42,959 --> 00:02:45,120
as a result we received over 500 million

86
00:02:45,120 --> 00:02:46,720
network sessions spanning half a

87
00:02:46,720 --> 00:02:48,239
terabyte of data

88
00:02:48,239 --> 00:02:49,920
we store this data encrypted in an

89
00:02:49,920 --> 00:02:52,239
amazon s3 bucket and then downloaded for

90
00:02:52,239 --> 00:02:54,640
future offline analysis

91
00:02:54,640 --> 00:02:56,480
through this analysis we found that we

92
00:02:56,480 --> 00:02:58,959
received over 1.5 million unique ip

93
00:02:58,959 --> 00:03:00,879
addresses from the cloud each studied

94
00:03:00,879 --> 00:03:02,720
for a period of time

95
00:03:02,720 --> 00:03:04,959
and as we'll see in a little bit this is

96
00:03:04,959 --> 00:03:07,760
over 56 of the available ip addresses in

97
00:03:07,760 --> 00:03:09,360
the pool

98
00:03:09,360 --> 00:03:11,760
now unlike existing works that primarily

99
00:03:11,760 --> 00:03:14,080
focus on dns we study the effects and

100
00:03:14,080 --> 00:03:16,080
configurations referring to ip address

101
00:03:16,080 --> 00:03:18,640
reuse directly and so we discover a new

102
00:03:18,640 --> 00:03:20,319
dimension of vulnerabilities across

103
00:03:20,319 --> 00:03:22,319
different types of configuration as we

104
00:03:22,319 --> 00:03:24,879
can see in the stars in the figure

105
00:03:24,879 --> 00:03:26,879
as a result of our study we found that

106
00:03:26,879 --> 00:03:28,319
cloud squatting is

107
00:03:28,319 --> 00:03:30,000
prevalent and in some cases causes

108
00:03:30,000 --> 00:03:32,640
highly exploitable latent configurations

109
00:03:32,640 --> 00:03:34,480
we saw cloud services and third-party

110
00:03:34,480 --> 00:03:35,440
services

111
00:03:35,440 --> 00:03:36,959
as novel sources of latent

112
00:03:36,959 --> 00:03:39,360
configurations in our study

113
00:03:39,360 --> 00:03:41,519
for cloud search for cloud services we

114
00:03:41,519 --> 00:03:43,280
found over 5 million messages received

115
00:03:43,280 --> 00:03:45,120
by our telescope intended for other

116
00:03:45,120 --> 00:03:46,959
tenants across four different cloud

117
00:03:46,959 --> 00:03:48,239
services

118
00:03:48,239 --> 00:03:50,480
on third party services we received over

119
00:03:50,480 --> 00:03:52,239
three million messages intent or

120
00:03:52,239 --> 00:03:54,159
potentially intended for other tenants

121
00:03:54,159 --> 00:03:55,760
across numerous services such as

122
00:03:55,760 --> 00:03:58,640
databases caches and other apis

123
00:03:58,640 --> 00:04:01,040
finally we use the domain name system to

124
00:04:01,040 --> 00:04:02,319
attribute a subset of these

125
00:04:02,319 --> 00:04:03,920
vulnerabilities to specific

126
00:04:03,920 --> 00:04:06,959
organizations finding that over 5400

127
00:04:06,959 --> 00:04:08,159
organizations were potentially

128
00:04:08,159 --> 00:04:10,480
vulnerable including 23 top thousand

129
00:04:10,480 --> 00:04:12,560
websites in our study of just one region

130
00:04:12,560 --> 00:04:14,799
of one cloud provider

131
00:04:14,799 --> 00:04:16,478
the types of data that we received as a

132
00:04:16,478 --> 00:04:18,238
result of this study were diverse but

133
00:04:18,238 --> 00:04:19,759
they included several classes of

134
00:04:19,759 --> 00:04:21,918
sensitive data including financial

135
00:04:21,918 --> 00:04:24,240
personal and location data credentials

136
00:04:24,240 --> 00:04:26,479
and user uploaded images and in some

137
00:04:26,479 --> 00:04:28,400
cases the ability to execute code in

138
00:04:28,400 --> 00:04:30,000
privileged positions on customer

139
00:04:30,000 --> 00:04:31,919
machines

140
00:04:31,919 --> 00:04:33,600
let's look at how we measure ip address

141
00:04:33,600 --> 00:04:35,280
for use from the bottom up

142
00:04:35,280 --> 00:04:36,960
as we said we first provision servers

143
00:04:36,960 --> 00:04:39,600
and collect traffic next we hypothesize

144
00:04:39,600 --> 00:04:41,759
various configuration types

145
00:04:41,759 --> 00:04:43,680
create filters and run those filters to

146
00:04:43,680 --> 00:04:45,199
identify traffic referring to those

147
00:04:45,199 --> 00:04:47,120
configurations and identify

148
00:04:47,120 --> 00:04:49,919
vulnerabilities and vulnerable parties

149
00:04:49,919 --> 00:04:51,520
before we move on to configuration types

150
00:04:51,520 --> 00:04:52,960
let's discuss a little bit more about

151
00:04:52,960 --> 00:04:55,120
our data collection

152
00:04:55,120 --> 00:04:57,440
now because we'd like to characterize ip

153
00:04:57,440 --> 00:04:59,199
address reuse directly we need to

154
00:04:59,199 --> 00:05:01,759
understand a little more about the ipool

155
00:05:01,759 --> 00:05:03,440
to do this we take advantage of the

156
00:05:03,440 --> 00:05:05,199
scale of our experiment

157
00:05:05,199 --> 00:05:07,360
because we provision so many servers

158
00:05:07,360 --> 00:05:08,960
it's an inevitability that we receive

159
00:05:08,960 --> 00:05:10,560
the same ip address from the cloud

160
00:05:10,560 --> 00:05:12,639
provider multiple times

161
00:05:12,639 --> 00:05:14,639
in order to analyze this we measure the

162
00:05:14,639 --> 00:05:16,320
time period between when we receive the

163
00:05:16,320 --> 00:05:18,800
same ip address multiple times this time

164
00:05:18,800 --> 00:05:21,039
between reuse is shown in this histogram

165
00:05:21,039 --> 00:05:23,919
x-axis is hours between reuse and y-axis

166
00:05:23,919 --> 00:05:25,520
is a number of instances where we saw

167
00:05:25,520 --> 00:05:28,159
that behavior

168
00:05:28,479 --> 00:05:30,560
zooming in on this or

169
00:05:30,560 --> 00:05:32,320
from this graph we can see that ip

170
00:05:32,320 --> 00:05:33,919
address reviews follows a poisson

171
00:05:33,919 --> 00:05:36,160
process showing that ip addresses are

172
00:05:36,160 --> 00:05:37,840
pseudo-randomly sampled from the pool of

173
00:05:37,840 --> 00:05:39,360
available addresses

174
00:05:39,360 --> 00:05:41,280
subsequent conversations with amazon

175
00:05:41,280 --> 00:05:42,960
confirmed this

176
00:05:42,960 --> 00:05:44,400
zooming in on the first bin of this

177
00:05:44,400 --> 00:05:46,800
histogram we can see that ip addresses

178
00:05:46,800 --> 00:05:48,479
can be reused by the cloud provider

179
00:05:48,479 --> 00:05:50,240
across tenants in as little as 30

180
00:05:50,240 --> 00:05:52,000
minutes after they're released meaning

181
00:05:52,000 --> 00:05:53,520
there's little time for tenants to

182
00:05:53,520 --> 00:05:55,360
correct latent configuration before

183
00:05:55,360 --> 00:05:57,280
adversaries can receive the associated

184
00:05:57,280 --> 00:06:00,080
ip address and exploit the traffic

185
00:06:00,080 --> 00:06:01,919
next we wanted to understand the size of

186
00:06:01,919 --> 00:06:03,919
the ip pool and how easily an adversary

187
00:06:03,919 --> 00:06:06,240
could cover the pool to do this we use

188
00:06:06,240 --> 00:06:08,319
an open population estimate technique

189
00:06:08,319 --> 00:06:10,000
widely used for wildlife studies but

190
00:06:10,000 --> 00:06:11,680
also applicable here

191
00:06:11,680 --> 00:06:13,600
the statistic gives us an estimate of

192
00:06:13,600 --> 00:06:15,280
the number of ip addresses in the pool

193
00:06:15,280 --> 00:06:17,280
throughout the study period and also an

194
00:06:17,280 --> 00:06:19,199
estimate of the percentage of those ips

195
00:06:19,199 --> 00:06:21,919
that our telescope was able to observe

196
00:06:21,919 --> 00:06:23,759
as a result of this we find that we

197
00:06:23,759 --> 00:06:26,160
observe over 56 percent of the total ip

198
00:06:26,160 --> 00:06:27,919
addresses available in the pool and an

199
00:06:27,919 --> 00:06:29,840
adversary could do similarly

200
00:06:29,840 --> 00:06:31,919
this leads us to a couple of conclusions

201
00:06:31,919 --> 00:06:33,759
as we saw at the pseudorandom nature of

202
00:06:33,759 --> 00:06:35,919
allocation allows adversaries to easily

203
00:06:35,919 --> 00:06:38,240
gain high coverage of the pool and

204
00:06:38,240 --> 00:06:39,600
there's little time for tenants to

205
00:06:39,600 --> 00:06:41,520
correct latent configuration before

206
00:06:41,520 --> 00:06:45,280
adversaries can discover and exploit it

207
00:06:45,280 --> 00:06:46,720
next let's look at the configuration

208
00:06:46,720 --> 00:06:49,440
types that we studied in our measurement

209
00:06:49,440 --> 00:06:51,360
now as we said clients reference

210
00:06:51,360 --> 00:06:53,440
configuration to know which ip addresses

211
00:06:53,440 --> 00:06:55,919
to connect to trust and send data and

212
00:06:55,919 --> 00:06:57,440
this configuration can take multiple

213
00:06:57,440 --> 00:06:59,440
classes that we studied

214
00:06:59,440 --> 00:07:02,160
the first is cloud service traffic cloud

215
00:07:02,160 --> 00:07:04,000
services are managed offerings by the

216
00:07:04,000 --> 00:07:05,919
cloud provider and while they don't run

217
00:07:05,919 --> 00:07:07,599
on tenant machines they can be

218
00:07:07,599 --> 00:07:09,360
configured to connect to tenant ip

219
00:07:09,360 --> 00:07:10,479
addresses

220
00:07:10,479 --> 00:07:12,160
for instance amazon's simple

221
00:07:12,160 --> 00:07:14,319
notification service or sns

222
00:07:14,319 --> 00:07:15,919
automatically delivers messages to

223
00:07:15,919 --> 00:07:18,720
subscriptions to http endpoints on ip

224
00:07:18,720 --> 00:07:21,039
addresses

225
00:07:21,039 --> 00:07:23,199
next we have third-party services

226
00:07:23,199 --> 00:07:25,120
third-party services can be any client

227
00:07:25,120 --> 00:07:27,840
software service or device that connects

228
00:07:27,840 --> 00:07:30,479
to ip addresses due to configuration for

229
00:07:30,479 --> 00:07:32,560
instance we saw web servers connecting

230
00:07:32,560 --> 00:07:34,240
to our ip addresses and attempting to

231
00:07:34,240 --> 00:07:36,400
access databases where they would either

232
00:07:36,400 --> 00:07:38,160
send or attempt to receive sensitive

233
00:07:38,160 --> 00:07:39,919
information

234
00:07:39,919 --> 00:07:41,840
finally we use configurations pulled

235
00:07:41,840 --> 00:07:43,680
from the domain name system to attribute

236
00:07:43,680 --> 00:07:45,599
a subset of these vulnerabilities to

237
00:07:45,599 --> 00:07:47,759
specific organizations

238
00:07:47,759 --> 00:07:51,440
let's start by looking at cloud services

239
00:07:51,520 --> 00:07:53,520
now what we'd like to do here is filter

240
00:07:53,520 --> 00:07:54,960
traffic to that source from cloud

241
00:07:54,960 --> 00:07:56,400
services and to do that we take

242
00:07:56,400 --> 00:07:57,919
advantage of a couple of unique

243
00:07:57,919 --> 00:08:00,319
properties of cloud service traffic

244
00:08:00,319 --> 00:08:01,919
the first is that this traffic is

245
00:08:01,919 --> 00:08:03,919
sourced from an amazon or cloud provider

246
00:08:03,919 --> 00:08:06,240
verified ip address and so we can filter

247
00:08:06,240 --> 00:08:09,520
to these ip addresses in a sound way

248
00:08:09,520 --> 00:08:11,520
next we can identify the service and

249
00:08:11,520 --> 00:08:14,240
associate a tenant from http headers and

250
00:08:14,240 --> 00:08:16,000
an adversary could do likewise to

251
00:08:16,000 --> 00:08:17,360
discover which tenants they'd like to

252
00:08:17,360 --> 00:08:18,720
exploit

253
00:08:18,720 --> 00:08:20,560
based on these filtering techniques we

254
00:08:20,560 --> 00:08:22,720
discover four different cloud services

255
00:08:22,720 --> 00:08:25,039
that leaked sensitive data intended for

256
00:08:25,039 --> 00:08:26,400
other tenants

257
00:08:26,400 --> 00:08:28,080
for example we received financial

258
00:08:28,080 --> 00:08:31,039
financial personal and location history

259
00:08:31,039 --> 00:08:33,279
in one case a government social services

260
00:08:33,279 --> 00:08:35,440
organization leaked sensitive data over

261
00:08:35,440 --> 00:08:37,440
sns to our telescope including

262
00:08:37,440 --> 00:08:39,120
personally identifiable information

263
00:08:39,120 --> 00:08:41,039
about their about their users i.e

264
00:08:41,039 --> 00:08:42,958
anything on a driver's license and

265
00:08:42,958 --> 00:08:44,399
real-time location history from

266
00:08:44,399 --> 00:08:46,480
vulnerable users

267
00:08:46,480 --> 00:08:48,800
in another case cloudfront endpoints

268
00:08:48,800 --> 00:08:51,279
delivered requests for javascript files

269
00:08:51,279 --> 00:08:53,120
responding to these requests would allow

270
00:08:53,120 --> 00:08:55,600
an adversary to run data in a privileged

271
00:08:55,600 --> 00:08:57,519
fashion on main on

272
00:08:57,519 --> 00:09:00,160
popular websites

273
00:09:00,160 --> 00:09:02,640
next let's look at third-party services

274
00:09:02,640 --> 00:09:04,959
now unlike cloud services third-party

275
00:09:04,959 --> 00:09:06,560
services are as diverse as those

276
00:09:06,560 --> 00:09:08,399
deployed on the cloud provider itself

277
00:09:08,399 --> 00:09:10,080
and the configurations used in these

278
00:09:10,080 --> 00:09:12,160
services are likewise diverse they can

279
00:09:12,160 --> 00:09:13,920
be as complicated as a distributed

280
00:09:13,920 --> 00:09:15,600
configuration management system such as

281
00:09:15,600 --> 00:09:17,920
kubernetes or they could be as simple as

282
00:09:17,920 --> 00:09:19,920
a sticky note on an employee's monitor

283
00:09:19,920 --> 00:09:21,600
saying which ip address to connect to to

284
00:09:21,600 --> 00:09:23,360
upload invoices

285
00:09:23,360 --> 00:09:25,519
because of this our analysis needs to be

286
00:09:25,519 --> 00:09:27,279
general

287
00:09:27,279 --> 00:09:29,279
to the goal here is that we'd like to

288
00:09:29,279 --> 00:09:31,200
filter out illegitimate or scanner

289
00:09:31,200 --> 00:09:33,120
traffic and analyze the remaining share

290
00:09:33,120 --> 00:09:35,120
which is likely to be legitimate

291
00:09:35,120 --> 00:09:37,360
to achieve this we take advantage of a

292
00:09:37,360 --> 00:09:38,959
series of filters that operate at

293
00:09:38,959 --> 00:09:42,080
various levels of the protocol hierarchy

294
00:09:42,080 --> 00:09:44,160
now due to the scale of the data we

295
00:09:44,160 --> 00:09:45,760
collected and the generality of our

296
00:09:45,760 --> 00:09:47,680
needed analysis this required the

297
00:09:47,680 --> 00:09:49,279
development of new traffic filtering

298
00:09:49,279 --> 00:09:51,120
techniques that were sensitive to the

299
00:09:51,120 --> 00:09:53,040
specific measurement scenario as we'll

300
00:09:53,040 --> 00:09:54,399
see

301
00:09:54,399 --> 00:09:56,160
the first step that we took was network

302
00:09:56,160 --> 00:09:57,760
filtering using publicly available

303
00:09:57,760 --> 00:10:00,000
embedded ip block lists this eliminated

304
00:10:00,000 --> 00:10:02,079
a majority of the traffic

305
00:10:02,079 --> 00:10:04,160
next we perform transport filtering

306
00:10:04,160 --> 00:10:05,279
taking advantage of the unique

307
00:10:05,279 --> 00:10:08,000
characteristics of our cloud measurement

308
00:10:08,000 --> 00:10:09,920
because we receive and measure data on

309
00:10:09,920 --> 00:10:11,920
so many ip addresses it is an

310
00:10:11,920 --> 00:10:14,320
inevitability or a near inevitability

311
00:10:14,320 --> 00:10:17,120
that scanners in scanning providers will

312
00:10:17,120 --> 00:10:19,360
access multiple ip addresses in our

313
00:10:19,360 --> 00:10:20,959
telescope over the course of the study

314
00:10:20,959 --> 00:10:22,560
or multiple ports

315
00:10:22,560 --> 00:10:25,200
and so we can take advantage of this the

316
00:10:25,200 --> 00:10:27,040
figure on the right is a histogram on

317
00:10:27,040 --> 00:10:29,680
the x-axis being number of ips contacted

318
00:10:29,680 --> 00:10:32,320
and y-axis number of ports contacted of

319
00:10:32,320 --> 00:10:33,680
all of the connections seen throughout

320
00:10:33,680 --> 00:10:35,279
our experiment

321
00:10:35,279 --> 00:10:37,600
as we can see the vast majority of

322
00:10:37,600 --> 00:10:39,440
connections come from ip addresses that

323
00:10:39,440 --> 00:10:42,160
contacted a large number of ips or ports

324
00:10:42,160 --> 00:10:44,399
on our telescope and so

325
00:10:44,399 --> 00:10:46,399
by filtering traffic to just the bottom

326
00:10:46,399 --> 00:10:48,240
left corner of this histogram we can

327
00:10:48,240 --> 00:10:50,079
eliminate a vast majority of scanner

328
00:10:50,079 --> 00:10:51,839
traffic

329
00:10:51,839 --> 00:10:53,519
next we filter at the session and

330
00:10:53,519 --> 00:10:55,680
application layers using existing

331
00:10:55,680 --> 00:10:57,279
network intrusion detection systems such

332
00:10:57,279 --> 00:10:59,200
as snort and custom rules that are

333
00:10:59,200 --> 00:11:01,040
designed to eliminate other exploit

334
00:11:01,040 --> 00:11:04,160
illegitimate and uninteresting traffic

335
00:11:04,160 --> 00:11:06,160
as a result of these filtering steps we

336
00:11:06,160 --> 00:11:08,320
identified a wide variety of third-party

337
00:11:08,320 --> 00:11:10,079
services delivering sensitive data to

338
00:11:10,079 --> 00:11:11,839
our telescope and potentially to

339
00:11:11,839 --> 00:11:13,200
adversaries

340
00:11:13,200 --> 00:11:15,120
as we mentioned we saw database and cash

341
00:11:15,120 --> 00:11:16,959
traffic but we also saw financial

342
00:11:16,959 --> 00:11:19,200
service traffic logging analytics and

343
00:11:19,200 --> 00:11:22,079
advertising networks and other apis

344
00:11:22,079 --> 00:11:23,680
in the case of financial traffic for

345
00:11:23,680 --> 00:11:25,680
instance we saw connections utilizing

346
00:11:25,680 --> 00:11:27,360
the financial information exchange

347
00:11:27,360 --> 00:11:29,519
protocol which underpins stock and other

348
00:11:29,519 --> 00:11:31,839
equity markets

349
00:11:31,839 --> 00:11:33,680
we saw logging and analytics data

350
00:11:33,680 --> 00:11:35,279
containing personally identifiable

351
00:11:35,279 --> 00:11:37,600
information of mobile app users and

352
00:11:37,600 --> 00:11:39,680
device credentials or rather device

353
00:11:39,680 --> 00:11:41,600
metadata

354
00:11:41,600 --> 00:11:44,480
we also saw other apis and web books

355
00:11:44,480 --> 00:11:46,240
these allow services on the internet to

356
00:11:46,240 --> 00:11:48,079
communicate with each other

357
00:11:48,079 --> 00:11:50,480
for instance we saw instances of one

358
00:11:50,480 --> 00:11:52,399
source control management platform

359
00:11:52,399 --> 00:11:54,320
delivering metadata on private source

360
00:11:54,320 --> 00:11:56,959
code and credentials to ci servers

361
00:11:56,959 --> 00:11:59,279
in another case we saw a retired

362
00:11:59,279 --> 00:12:02,560
endpoint used by web browsers to

363
00:12:02,560 --> 00:12:04,079
to get search results or search

364
00:12:04,079 --> 00:12:05,360
suggestions and these contain

365
00:12:05,360 --> 00:12:07,120
potentially sensitive private browsing

366
00:12:07,120 --> 00:12:09,040
data

367
00:12:09,040 --> 00:12:11,120
as you can see the services or the

368
00:12:11,120 --> 00:12:12,800
configurations we saw under third party

369
00:12:12,800 --> 00:12:15,120
services were diverse but the result was

370
00:12:15,120 --> 00:12:18,560
the leakage of highly sensitive data

371
00:12:18,560 --> 00:12:20,000
finally we'd like to attribute

372
00:12:20,000 --> 00:12:21,839
vulnerabilities to organizations using

373
00:12:21,839 --> 00:12:23,839
the domain name system to do this we

374
00:12:23,839 --> 00:12:25,680
pulled banner information from requests

375
00:12:25,680 --> 00:12:28,079
that identified it such as http or tls

376
00:12:28,079 --> 00:12:29,040
traffic

377
00:12:29,040 --> 00:12:31,360
as a result we found over 5400

378
00:12:31,360 --> 00:12:34,079
vulnerable domains into including 23 top

379
00:12:34,079 --> 00:12:36,720
thousand websites in many cases these

380
00:12:36,720 --> 00:12:38,560
domains had several or even over a

381
00:12:38,560 --> 00:12:40,720
hundred vulnerable subdomains showing

382
00:12:40,720 --> 00:12:42,320
that the problem is both broad and deep

383
00:12:42,320 --> 00:12:45,279
across these organizations

384
00:12:45,279 --> 00:12:47,360
in our sample we saw a wide variety of

385
00:12:47,360 --> 00:12:49,120
affected organizations across both

386
00:12:49,120 --> 00:12:50,959
industry academic and government

387
00:12:50,959 --> 00:12:53,839
settings and as we can see from the data

388
00:12:53,839 --> 00:12:55,519
even organizations with security

389
00:12:55,519 --> 00:12:59,440
subgroups aren't necessarily immune

390
00:13:00,160 --> 00:13:02,560
naturally in seeing our findings our

391
00:13:02,560 --> 00:13:04,240
first step was to disclose our results

392
00:13:04,240 --> 00:13:05,920
directly to amazon and other cloud

393
00:13:05,920 --> 00:13:08,000
providers and the vast majority of our

394
00:13:08,000 --> 00:13:09,760
disclosure process was coordinated

395
00:13:09,760 --> 00:13:11,519
through amazon in order to achieve the

396
00:13:11,519 --> 00:13:13,600
highest coverage in fact amazon took

397
00:13:13,600 --> 00:13:16,320
additional steps as we'll see later

398
00:13:16,320 --> 00:13:18,320
however we also directly reached out to

399
00:13:18,320 --> 00:13:20,240
a select group of tenants and perform

400
00:13:20,240 --> 00:13:22,240
disclosure interviews and surveys to

401
00:13:22,240 --> 00:13:24,320
reveal root causes which we found

402
00:13:24,320 --> 00:13:27,120
broadly fell into three categories

403
00:13:27,120 --> 00:13:29,120
the first is issues with integration

404
00:13:29,120 --> 00:13:31,519
what we refer to as lift and shift

405
00:13:31,519 --> 00:13:33,120
lift and shift is the technique of

406
00:13:33,120 --> 00:13:34,800
taking existing on-premises

407
00:13:34,800 --> 00:13:36,800
infrastructure from data centers and

408
00:13:36,800 --> 00:13:39,040
moving it to a public cloud provider

409
00:13:39,040 --> 00:13:40,399
often this brings with it the

410
00:13:40,399 --> 00:13:42,320
assumptions of the private data center

411
00:13:42,320 --> 00:13:43,920
including lack of consideration for

412
00:13:43,920 --> 00:13:46,399
service decommissioning

413
00:13:46,399 --> 00:13:48,320
next we saw failure to manage

414
00:13:48,320 --> 00:13:49,760
integration or failure to manage

415
00:13:49,760 --> 00:13:52,000
configuration and lack of oversight

416
00:13:52,000 --> 00:13:54,000
in this we saw that many organizations

417
00:13:54,000 --> 00:13:55,920
lacked a centralized overview of their

418
00:13:55,920 --> 00:13:58,079
cloud infrastructure where many subunits

419
00:13:58,079 --> 00:13:59,519
of the organization would have their own

420
00:13:59,519 --> 00:14:02,079
cloud presences in one case an academic

421
00:14:02,079 --> 00:14:03,760
institution had over 100

422
00:14:03,760 --> 00:14:05,600
sub-organizations with independent cloud

423
00:14:05,600 --> 00:14:07,040
infrastructure

424
00:14:07,040 --> 00:14:08,880
and as a result it's difficult to

425
00:14:08,880 --> 00:14:10,480
enforce best practices such as

426
00:14:10,480 --> 00:14:12,720
decommissioning considerations on those

427
00:14:12,720 --> 00:14:15,040
sub-organizations

428
00:14:15,040 --> 00:14:17,199
finally we saw insufficient lack of or

429
00:14:17,199 --> 00:14:19,440
broken automation here we saw

430
00:14:19,440 --> 00:14:21,199
organizations manually deploying

431
00:14:21,199 --> 00:14:22,800
instances to the cloud and failing to

432
00:14:22,800 --> 00:14:24,959
consider decommissioning or we saw

433
00:14:24,959 --> 00:14:27,519
custom or bespoke deployment strategies

434
00:14:27,519 --> 00:14:28,639
that also did not consider

435
00:14:28,639 --> 00:14:31,120
decommissioning

436
00:14:31,120 --> 00:14:32,639
this brings us to our defenses and

437
00:14:32,639 --> 00:14:34,160
mitigations that can be taken against

438
00:14:34,160 --> 00:14:35,199
this issue

439
00:14:35,199 --> 00:14:37,440
now as we saw cloud squatting results

440
00:14:37,440 --> 00:14:39,680
from a combination of resource reuse and

441
00:14:39,680 --> 00:14:42,320
latent configuration and so our defenses

442
00:14:42,320 --> 00:14:44,720
mirror this taxonomy

443
00:14:44,720 --> 00:14:46,480
in our paper we discuss and introduce a

444
00:14:46,480 --> 00:14:48,639
variety of new defenses that both cloud

445
00:14:48,639 --> 00:14:50,320
tenants and providers can take to

446
00:14:50,320 --> 00:14:51,920
protect customer data

447
00:14:51,920 --> 00:14:53,440
however the strategy that i'd like to

448
00:14:53,440 --> 00:14:54,959
talk a little more about is ip

449
00:14:54,959 --> 00:14:56,720
allocation policy

450
00:14:56,720 --> 00:14:59,199
we hypothesized that by changing the way

451
00:14:59,199 --> 00:15:00,880
that ip addresses are allocated on the

452
00:15:00,880 --> 00:15:02,800
public cloud providers could greatly

453
00:15:02,800 --> 00:15:04,560
reduce the ability of adversaries to

454
00:15:04,560 --> 00:15:06,079
measure the pool

455
00:15:06,079 --> 00:15:08,639
and so we generated or we created a new

456
00:15:08,639 --> 00:15:10,800
technique which we call iptagging

457
00:15:10,800 --> 00:15:12,800
iptagging uses heuristics to

458
00:15:12,800 --> 00:15:14,880
automatically segment the ip pool across

459
00:15:14,880 --> 00:15:16,880
tenants reducing tenant's ability to

460
00:15:16,880 --> 00:15:18,800
measure new ip addresses

461
00:15:18,800 --> 00:15:20,399
based on the metrics that we measured in

462
00:15:20,399 --> 00:15:22,399
our initial study of cloud providers we

463
00:15:22,399 --> 00:15:24,480
created a simulator that simulates ip

464
00:15:24,480 --> 00:15:26,480
address allocation in a virtual public

465
00:15:26,480 --> 00:15:27,760
cloud

466
00:15:27,760 --> 00:15:29,440
as a result of this and analyzing our

467
00:15:29,440 --> 00:15:31,600
new technique we see that iptagging

468
00:15:31,600 --> 00:15:33,839
vastly reduces the number of unique ips

469
00:15:33,839 --> 00:15:35,839
observable by an adversary protecting

470
00:15:35,839 --> 00:15:39,040
user data among other benefits

471
00:15:39,040 --> 00:15:40,560
in addition to the defenses that we

472
00:15:40,560 --> 00:15:42,639
propose amazon has taken actions to

473
00:15:42,639 --> 00:15:45,519
protect users on their cloud platform

474
00:15:45,519 --> 00:15:47,360
they've introduced new configuration

475
00:15:47,360 --> 00:15:49,120
alerts on the console that will

476
00:15:49,120 --> 00:15:50,959
automatically alert users when creating

477
00:15:50,959 --> 00:15:53,199
configurations that could become latent

478
00:15:53,199 --> 00:15:54,959
such as direct references to ipa

479
00:15:54,959 --> 00:15:56,560
addresses

480
00:15:56,560 --> 00:15:58,320
they've also taken our findings and

481
00:15:58,320 --> 00:15:59,759
performed scanning and disclosure

482
00:15:59,759 --> 00:16:01,680
process using control plane information

483
00:16:01,680 --> 00:16:03,440
across all regions

484
00:16:03,440 --> 00:16:05,199
whereas our study looked at one region

485
00:16:05,199 --> 00:16:07,040
of a single public cloud provider

486
00:16:07,040 --> 00:16:08,480
amazon's scandal of control plane

487
00:16:08,480 --> 00:16:10,000
information revealed that similar

488
00:16:10,000 --> 00:16:12,000
problems exist across all regions

489
00:16:12,000 --> 00:16:14,000
showing the problem is general

490
00:16:14,000 --> 00:16:15,839
they also introduce automated policy

491
00:16:15,839 --> 00:16:18,000
enforcement allowing organizations to

492
00:16:18,000 --> 00:16:20,639
enforce best practices from the top down

493
00:16:20,639 --> 00:16:22,639
further they document these practices

494
00:16:22,639 --> 00:16:25,120
using new documentation to introduce ip

495
00:16:25,120 --> 00:16:27,440
hygiene

496
00:16:27,440 --> 00:16:29,279
cloud squatting represents a serious and

497
00:16:29,279 --> 00:16:30,639
widespread vulnerability for

498
00:16:30,639 --> 00:16:32,160
infrastructure deployed on the public

499
00:16:32,160 --> 00:16:34,160
cloud our measurement study found that

500
00:16:34,160 --> 00:16:35,920
latent configuration vulnerabilities

501
00:16:35,920 --> 00:16:38,079
were widespread across a wide variety of

502
00:16:38,079 --> 00:16:39,360
services

503
00:16:39,360 --> 00:16:41,839
further existing cloud services may not

504
00:16:41,839 --> 00:16:44,880
sufficiently protect user data

505
00:16:44,880 --> 00:16:46,720
cloud squatting is also deeply practical

506
00:16:46,720 --> 00:16:49,120
for an adversary because ip addresses

507
00:16:49,120 --> 00:16:51,199
are allocated pseudorandomly adversaries

508
00:16:51,199 --> 00:16:52,720
can easily sample the pool with high

509
00:16:52,720 --> 00:16:53,839
coverage

510
00:16:53,839 --> 00:16:56,000
for instance our measurement study which

511
00:16:56,000 --> 00:16:58,480
covered over 1.5 million unique ips from

512
00:16:58,480 --> 00:17:00,320
a major cloud provider was performed at

513
00:17:00,320 --> 00:17:03,040
a cost of only 2 000 us dollars

514
00:17:03,040 --> 00:17:05,119
finally this generalizat this

515
00:17:05,119 --> 00:17:06,799
vulnerability generalizes to other

516
00:17:06,799 --> 00:17:08,720
providers in our paper we analyze how

517
00:17:08,720 --> 00:17:11,599
the other offerings could be vulnerable

518
00:17:11,599 --> 00:17:13,520
however there is hope cloud squatting

519
00:17:13,520 --> 00:17:15,039
can be prevented via judicious

520
00:17:15,039 --> 00:17:16,720
configuration management and reduction

521
00:17:16,720 --> 00:17:18,559
of ip address reuse

522
00:17:18,559 --> 00:17:19,919
thank you for your attention and thank

523
00:17:19,919 --> 00:17:21,280
you to the national science foundation

524
00:17:21,280 --> 00:17:22,720
for funding this work with that i'll

525
00:17:22,720 --> 00:17:23,970
take any questions

526
00:17:23,970 --> 00:17:30,799
[Applause]

527
00:17:30,799 --> 00:17:32,960
thank you very much for the presentation

528
00:17:32,960 --> 00:17:35,280
do we have questions from the audience

529
00:17:35,280 --> 00:17:38,240
yes please go ahead

530
00:17:41,440 --> 00:17:44,000
hey uh thanks for the talk uh i've got a

531
00:17:44,000 --> 00:17:45,600
question so you you've mentioned a

532
00:17:45,600 --> 00:17:47,840
couple pretty uh concerning um

533
00:17:47,840 --> 00:17:49,919
communications that weren't you know you

534
00:17:49,919 --> 00:17:51,440
weren't intended to see

535
00:17:51,440 --> 00:17:53,760
uh including financial information

536
00:17:53,760 --> 00:17:55,679
was this a common occurrence or do you

537
00:17:55,679 --> 00:17:58,080
just find a couple examples of these

538
00:17:58,080 --> 00:17:58,960
um

539
00:17:58,960 --> 00:18:01,280
so in some cases we can definitely uh

540
00:18:01,280 --> 00:18:03,039
sort of categorize the prevalence here

541
00:18:03,039 --> 00:18:04,720
in the case of domain name for instance

542
00:18:04,720 --> 00:18:07,440
we saw the 5400 in the case of cloud

543
00:18:07,440 --> 00:18:09,039
services we can look at the number of

544
00:18:09,039 --> 00:18:10,720
tenants or number of ip addresses that

545
00:18:10,720 --> 00:18:12,559
were vulnerable as you may have seen in

546
00:18:12,559 --> 00:18:14,880
the figure that was over 20 i believe 23

547
00:18:14,880 --> 00:18:17,200
000 ip addresses for

548
00:18:17,200 --> 00:18:18,720
sns for instance so that would be a

549
00:18:18,720 --> 00:18:20,720
widespread vulnerability

550
00:18:20,720 --> 00:18:22,480
in the case of third party services it's

551
00:18:22,480 --> 00:18:24,320
a little more difficult to quantify

552
00:18:24,320 --> 00:18:26,640
however sort of anecdotally we saw that

553
00:18:26,640 --> 00:18:28,160
this was a large number of affected

554
00:18:28,160 --> 00:18:30,880
organizations

555
00:18:31,039 --> 00:18:32,640
thank you

556
00:18:32,640 --> 00:18:35,360
hi eric a great talk thank you so one of

557
00:18:35,360 --> 00:18:37,039
the defenses that you showed basically

558
00:18:37,039 --> 00:18:39,760
says that you can just use ipv6 which i

559
00:18:39,760 --> 00:18:41,760
would think is the most practical

560
00:18:41,760 --> 00:18:44,080
solution to this situation but given

561
00:18:44,080 --> 00:18:45,760
that there are so many deployment issues

562
00:18:45,760 --> 00:18:47,520
with ipv6 it's not being deployed very

563
00:18:47,520 --> 00:18:49,600
quickly so what are your thoughts about

564
00:18:49,600 --> 00:18:51,039
that defense because

565
00:18:51,039 --> 00:18:53,520
with ipv6 you basically do not need to

566
00:18:53,520 --> 00:18:55,600
reuse an ip address ever at the address

567
00:18:55,600 --> 00:18:58,080
space provided to one host your slash 64

568
00:18:58,080 --> 00:18:59,840
which is practically just unlimited so

569
00:18:59,840 --> 00:19:02,559
why not just go to that instead of uh

570
00:19:02,559 --> 00:19:04,640
defenses like ib tagging or something

571
00:19:04,640 --> 00:19:07,280
like that that's a great question so we

572
00:19:07,280 --> 00:19:09,919
see ipv6 as having sort of two realms

573
00:19:09,919 --> 00:19:11,360
one which in which it's applicable and

574
00:19:11,360 --> 00:19:13,760
one in which for legacy reasons it it

575
00:19:13,760 --> 00:19:15,919
won't be fully applicable any time soon

576
00:19:15,919 --> 00:19:18,640
for internal service traffic um traffic

577
00:19:18,640 --> 00:19:20,080
between services owned by a single

578
00:19:20,080 --> 00:19:22,400
tenant where ipv6 can be deployed across

579
00:19:22,400 --> 00:19:23,520
the entire

580
00:19:23,520 --> 00:19:26,000
entire cloud presence ipv6 is a really

581
00:19:26,000 --> 00:19:27,919
compelling solution there most cloud

582
00:19:27,919 --> 00:19:30,559
providers now support ipv6 endpoints so

583
00:19:30,559 --> 00:19:32,640
if you can do all your routing over ipv6

584
00:19:32,640 --> 00:19:34,480
that will solve a lot of your problems

585
00:19:34,480 --> 00:19:36,400
however for public client-facing

586
00:19:36,400 --> 00:19:38,960
services i do agree that um we're sort

587
00:19:38,960 --> 00:19:41,120
of stuck with ipv4 routing now so that's

588
00:19:41,120 --> 00:19:44,320
why these other solutions especially um

589
00:19:44,320 --> 00:19:46,320
especially the solution of ip tagging

590
00:19:46,320 --> 00:19:48,400
because it is fully backwards compatible

591
00:19:48,400 --> 00:19:49,760
um sort of offers a compelling

592
00:19:49,760 --> 00:19:50,880
contribution

593
00:19:50,880 --> 00:19:54,080
all right thank you thank you

594
00:19:54,320 --> 00:19:58,880
so i i have a curiosity actually

595
00:19:58,880 --> 00:20:01,120
i don't know if you mentioned it but how

596
00:20:01,120 --> 00:20:03,280
much time did it take roughly to conduct

597
00:20:03,280 --> 00:20:07,360
the study and also i'm curious about

598
00:20:07,360 --> 00:20:08,960
a little bit about the reaction of the

599
00:20:08,960 --> 00:20:11,200
cloud providers and maybe the response

600
00:20:11,200 --> 00:20:13,440
time in taking actions or

601
00:20:13,440 --> 00:20:15,280
responding to you yeah

602
00:20:15,280 --> 00:20:18,240
yeah so um the the period of the study

603
00:20:18,240 --> 00:20:20,480
is sort of somewhat indefinite um for

604
00:20:20,480 --> 00:20:22,960
the initial data analysis as part of

605
00:20:22,960 --> 00:20:24,400
this presentation and the paper we

606
00:20:24,400 --> 00:20:25,919
performed the study over i believe about

607
00:20:25,919 --> 00:20:27,760
107 days

608
00:20:27,760 --> 00:20:29,280
in terms of cloud provider reactions

609
00:20:29,280 --> 00:20:31,039
we've been very very pleased with

610
00:20:31,039 --> 00:20:33,039
especially amazon's reaction in terms of

611
00:20:33,039 --> 00:20:35,840
uh sort of you know responding to us

612
00:20:35,840 --> 00:20:37,760
reaching out with them and and

613
00:20:37,760 --> 00:20:39,039
implementing

614
00:20:39,039 --> 00:20:40,640
new best practices

615
00:20:40,640 --> 00:20:43,200
um in terms of the total measurement

616
00:20:43,200 --> 00:20:45,200
study collection is actually sort of

617
00:20:45,200 --> 00:20:46,880
ongoing and we're continuing to see new

618
00:20:46,880 --> 00:20:49,280
and interesting trends in this space and

619
00:20:49,280 --> 00:20:50,559
the

620
00:20:50,559 --> 00:20:52,799
the sort of in-situ cloud provider

621
00:20:52,799 --> 00:20:54,559
internet telescope strategy is also

622
00:20:54,559 --> 00:20:56,240
potentially leading to new insights in

623
00:20:56,240 --> 00:20:58,960
other spaces

624
00:20:59,679 --> 00:21:02,400
we have a question from home our time is

625
00:21:02,400 --> 00:21:05,200
help so maybe if you could go to hop in

626
00:21:05,200 --> 00:21:07,200
and answer that it would be great

627
00:21:07,200 --> 00:21:08,960
absolutely let's let's thank our speaker

628
00:21:08,960 --> 00:21:10,240
again

629
00:21:10,240 --> 00:21:15,839
[Applause]

