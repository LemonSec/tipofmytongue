1
00:00:00,160 --> 00:00:01,040
thank you

2
00:00:01,040 --> 00:00:03,040
thank you very much um

3
00:00:03,040 --> 00:00:05,680
this is the presentation for our uh work

4
00:00:05,680 --> 00:00:07,040
property inference from poisoning

5
00:00:07,040 --> 00:00:09,040
outside knowledgebar from princeton

6
00:00:09,040 --> 00:00:10,880
university and this is a joint work with

7
00:00:10,880 --> 00:00:12,719
asia gauche and melissa chase from

8
00:00:12,719 --> 00:00:16,239
microsoft research

9
00:00:16,239 --> 00:00:18,080
in privacy-preserving machine learning

10
00:00:18,080 --> 00:00:20,880
we often deal with a setting where there

11
00:00:20,880 --> 00:00:23,519
are multiple data providers who want to

12
00:00:23,519 --> 00:00:26,160
train a joint model on the union of

13
00:00:26,160 --> 00:00:28,800
their data by interacting with each

14
00:00:28,800 --> 00:00:31,439
other and potentially a learning a

15
00:00:31,439 --> 00:00:33,920
central learner

16
00:00:33,920 --> 00:00:35,680
they need this whole process to be

17
00:00:35,680 --> 00:00:37,200
privacy preserving meaning that an

18
00:00:37,200 --> 00:00:40,000
adversary who can observe

19
00:00:40,000 --> 00:00:42,800
the steps of computation as well as the

20
00:00:42,800 --> 00:00:44,719
final trained model cannot infer

21
00:00:44,719 --> 00:00:46,559
sensitive information about the data

22
00:00:46,559 --> 00:00:47,760
sets

23
00:00:47,760 --> 00:00:49,360
of these parties

24
00:00:49,360 --> 00:00:51,600
this setting is usually referred to as

25
00:00:51,600 --> 00:00:54,000
honest or semi-honest privacy where the

26
00:00:54,000 --> 00:00:58,199
adversary is only an observer

27
00:00:58,239 --> 00:01:00,559
there is a stronger notion of

28
00:01:00,559 --> 00:01:03,359
privacy referred to as malicious privacy

29
00:01:03,359 --> 00:01:06,000
where the adversary can collude with

30
00:01:06,000 --> 00:01:08,080
some of the data providers

31
00:01:08,080 --> 00:01:10,400
and make them deviate from the protocol

32
00:01:10,400 --> 00:01:12,400
with the goal of inferring more

33
00:01:12,400 --> 00:01:14,159
sensitive information about the data of

34
00:01:14,159 --> 00:01:15,280
other data

35
00:01:15,280 --> 00:01:17,840
providers

36
00:01:17,840 --> 00:01:19,759
what are the solutions that we have in

37
00:01:19,759 --> 00:01:21,759
such scenarios well

38
00:01:21,759 --> 00:01:24,159
we have a strong cryptographic tools

39
00:01:24,159 --> 00:01:27,119
that enable us to eliminate the concern

40
00:01:27,119 --> 00:01:29,600
about the information leakage from

41
00:01:29,600 --> 00:01:31,840
computation

42
00:01:31,840 --> 00:01:32,960
even in the

43
00:01:32,960 --> 00:01:35,439
malicious privacy regime using the right

44
00:01:35,439 --> 00:01:37,520
cryptographic primitive enables us to

45
00:01:37,520 --> 00:01:39,040
consider

46
00:01:39,040 --> 00:01:41,680
the whole training as a black box that

47
00:01:41,680 --> 00:01:44,240
just outputs the model

48
00:01:44,240 --> 00:01:46,079
the only cost that we have to pay to

49
00:01:46,079 --> 00:01:48,560
upgrade to malicious privacy is to use

50
00:01:48,560 --> 00:01:50,799
stronger crypto primitives

51
00:01:50,799 --> 00:01:55,200
that are usually less efficient

52
00:01:55,200 --> 00:01:57,600
although this is fantastic we still have

53
00:01:57,600 --> 00:01:59,200
another

54
00:01:59,200 --> 00:02:01,040
source of leakage that we have to worry

55
00:02:01,040 --> 00:02:02,079
about

56
00:02:02,079 --> 00:02:03,840
which is the leakage from the train

57
00:02:03,840 --> 00:02:05,600
model

58
00:02:05,600 --> 00:02:07,840
to measure this leakage there are works

59
00:02:07,840 --> 00:02:10,000
that evaluate that proposed techniques

60
00:02:10,000 --> 00:02:12,160
to evaluate the leakage of these trained

61
00:02:12,160 --> 00:02:14,640
models but most of them are only

62
00:02:14,640 --> 00:02:16,959
designed for the honest setting

63
00:02:16,959 --> 00:02:18,319
and

64
00:02:18,319 --> 00:02:21,440
the motivating question in

65
00:02:21,440 --> 00:02:23,599
our work is to understand if there is a

66
00:02:23,599 --> 00:02:26,560
gap between uh honest and malicious

67
00:02:26,560 --> 00:02:29,280
privacy in terms of the

68
00:02:29,280 --> 00:02:32,640
the leakage from these models

69
00:02:34,800 --> 00:02:36,640
the the leakage model that we consider

70
00:02:36,640 --> 00:02:40,080
is property inference uh

71
00:02:40,080 --> 00:02:42,400
where where we have a data set uh

72
00:02:42,400 --> 00:02:44,080
consists of a bunch of features and a

73
00:02:44,080 --> 00:02:45,760
label and we have this training

74
00:02:45,760 --> 00:02:47,840
algorithm that output some model

75
00:02:47,840 --> 00:02:49,200
this data set

76
00:02:49,200 --> 00:02:51,599
supposedly has a sensitive property that

77
00:02:51,599 --> 00:02:53,680
the attacker tries to infer some

78
00:02:53,680 --> 00:02:55,120
statistics about

79
00:02:55,120 --> 00:02:58,080
for example uh imagine this data set is

80
00:02:58,080 --> 00:03:00,000
an email data set and the adversary

81
00:03:00,000 --> 00:03:02,000
wants to find out how many emails in the

82
00:03:02,000 --> 00:03:06,400
status that have a certain word in it

83
00:03:09,280 --> 00:03:10,879
other than this

84
00:03:10,879 --> 00:03:11,840
this

85
00:03:11,840 --> 00:03:14,080
statistics being sensitive

86
00:03:14,080 --> 00:03:16,319
i'm going to give you two other reasons

87
00:03:16,319 --> 00:03:17,120
to

88
00:03:17,120 --> 00:03:18,640
convince you that this is an important

89
00:03:18,640 --> 00:03:21,040
thread model to consider first of all

90
00:03:21,040 --> 00:03:23,120
it's equivalent to the statistical query

91
00:03:23,120 --> 00:03:25,120
model of nissim at all where they show

92
00:03:25,120 --> 00:03:27,519
that if you make enough of these queries

93
00:03:27,519 --> 00:03:29,280
you can actually reconstruct the whole

94
00:03:29,280 --> 00:03:32,080
dataset

95
00:03:32,080 --> 00:03:33,440
the other reason is this is a

96
00:03:33,440 --> 00:03:35,440
generalization of the well studied

97
00:03:35,440 --> 00:03:37,760
membership inference attack where the

98
00:03:37,760 --> 00:03:39,920
adversary wants to infer

99
00:03:39,920 --> 00:03:41,760
whether a specific instance in the

100
00:03:41,760 --> 00:03:43,920
training set or not so if you design

101
00:03:43,920 --> 00:03:46,239
your property carefully this property

102
00:03:46,239 --> 00:03:49,120
inference the tag will render into a

103
00:03:49,120 --> 00:03:52,319
membership infrastructure

104
00:03:53,599 --> 00:03:54,959
in this work we

105
00:03:54,959 --> 00:03:57,280
look at an extension of this thread

106
00:03:57,280 --> 00:03:58,319
model

107
00:03:58,319 --> 00:04:01,200
and leakage to to make it work in the

108
00:04:01,200 --> 00:04:03,760
malicious setting so what we consider is

109
00:04:03,760 --> 00:04:06,159
an adversary who additionally additional

110
00:04:06,159 --> 00:04:08,560
to observing the model it can also

111
00:04:08,560 --> 00:04:11,120
poison uh the training set by injecting

112
00:04:11,120 --> 00:04:12,840
some malicious

113
00:04:12,840 --> 00:04:15,920
inputs and we ask if this

114
00:04:15,920 --> 00:04:18,399
these kind of poisoning attacks can help

115
00:04:18,399 --> 00:04:22,239
property inference leakage

116
00:04:27,199 --> 00:04:30,720
in this world we propose a new attack uh

117
00:04:30,720 --> 00:04:34,400
for property inference that uses poison

118
00:04:34,400 --> 00:04:36,000
and

119
00:04:36,000 --> 00:04:38,800
in summary uh our this is this this this

120
00:04:38,800 --> 00:04:40,960
figure shows our result where this

121
00:04:40,960 --> 00:04:42,560
x-axis is

122
00:04:42,560 --> 00:04:44,639
the poison ratio and y-axis is the

123
00:04:44,639 --> 00:04:47,199
attack accuracy and as you see poisoning

124
00:04:47,199 --> 00:04:49,199
can significantly increase the leakage

125
00:04:49,199 --> 00:04:50,479
um

126
00:04:50,479 --> 00:04:52,960
i'm going to describe in in the next

127
00:04:52,960 --> 00:04:53,919
slide

128
00:04:53,919 --> 00:04:56,320
what we mean by attack accuracy how our

129
00:04:56,320 --> 00:04:59,280
attack works and

130
00:04:59,280 --> 00:05:01,120
and finally some details about the

131
00:05:01,120 --> 00:05:03,680
experiments

132
00:05:06,400 --> 00:05:08,720
to to to talk about attack accuracy we

133
00:05:08,720 --> 00:05:11,039
have to formally define what we mean

134
00:05:11,039 --> 00:05:13,759
here so we define this as a security

135
00:05:13,759 --> 00:05:15,520
game between a challenger and an

136
00:05:15,520 --> 00:05:18,000
adversary so our security game has

137
00:05:18,000 --> 00:05:20,479
multiple parameters

138
00:05:20,479 --> 00:05:23,199
two distributions d and d prime we have

139
00:05:23,199 --> 00:05:25,120
a property f which is the sensitive

140
00:05:25,120 --> 00:05:27,039
property that adversary wants to attack

141
00:05:27,039 --> 00:05:29,120
and it has the condition that the first

142
00:05:29,120 --> 00:05:31,440
distribution has more probability for

143
00:05:31,440 --> 00:05:33,360
for that property versus the other

144
00:05:33,360 --> 00:05:35,199
distribution and we also have this

145
00:05:35,199 --> 00:05:36,880
learning algorithm

146
00:05:36,880 --> 00:05:39,280
the security guard game goes as follows

147
00:05:39,280 --> 00:05:41,919
the first step adversary first

148
00:05:41,919 --> 00:05:43,759
selects a poison set

149
00:05:43,759 --> 00:05:46,160
and sends it to the challenger

150
00:05:46,160 --> 00:05:48,320
the challenger will randomly select a

151
00:05:48,320 --> 00:05:50,880
bit b and based on that bit selects one

152
00:05:50,880 --> 00:05:53,520
of these two distributions d and d prime

153
00:05:53,520 --> 00:05:56,000
and sample the data set from the

154
00:05:56,000 --> 00:05:57,440
distribution

155
00:05:57,440 --> 00:05:59,759
and after that it will just

156
00:05:59,759 --> 00:06:01,840
calculate the union of the poison set

157
00:06:01,840 --> 00:06:04,639
and the sample train a model and now the

158
00:06:04,639 --> 00:06:07,280
adversary gets to query this model

159
00:06:07,280 --> 00:06:08,960
and get the result back and it can

160
00:06:08,960 --> 00:06:10,720
repeat this process as many time in

161
00:06:10,720 --> 00:06:13,199
adaptive way

162
00:06:13,199 --> 00:06:15,840
finally the adversary is uh outputting a

163
00:06:15,840 --> 00:06:18,319
bit b prime and the success of the

164
00:06:18,319 --> 00:06:21,039
adversary is measured by the probability

165
00:06:21,039 --> 00:06:22,160
that

166
00:06:22,160 --> 00:06:24,240
b is equal to b prime

167
00:06:24,240 --> 00:06:26,160
and we refer to this as accuracy of the

168
00:06:26,160 --> 00:06:28,479
address

169
00:06:31,600 --> 00:06:33,840
our adversarial model

170
00:06:33,840 --> 00:06:35,440
has some assumptions we assume the

171
00:06:35,440 --> 00:06:37,199
adversary knows the distributions d and

172
00:06:37,199 --> 00:06:39,440
d prime and we assume the adversary

173
00:06:39,440 --> 00:06:41,759
knows the learning algorithm l

174
00:06:41,759 --> 00:06:43,919
the data distribution assumption might

175
00:06:43,919 --> 00:06:46,880
sound very strong to some people but

176
00:06:46,880 --> 00:06:48,560
we have settings that relax this

177
00:06:48,560 --> 00:06:50,240
assumption and in our experiments we

178
00:06:50,240 --> 00:06:52,720
show that sampling from a proxy

179
00:06:52,720 --> 00:06:54,560
distribution can have the same effect

180
00:06:54,560 --> 00:06:56,240
i'm not going to talk about that sitting

181
00:06:56,240 --> 00:06:58,479
here but we have more experiments in the

182
00:06:58,479 --> 00:07:00,479
paper for

183
00:07:00,479 --> 00:07:02,560
relaxing this assumption

184
00:07:02,560 --> 00:07:04,400
we also assume that the adversary has

185
00:07:04,400 --> 00:07:07,039
access to the model voting we can in the

186
00:07:07,039 --> 00:07:08,560
black box way

187
00:07:08,560 --> 00:07:10,160
the white box setting is also well

188
00:07:10,160 --> 00:07:11,840
defined but we are interested in the

189
00:07:11,840 --> 00:07:14,000
black box setting where the adversary

190
00:07:14,000 --> 00:07:16,319
only queries the model and it only gets

191
00:07:16,319 --> 00:07:18,479
the label back we don't even give it the

192
00:07:18,479 --> 00:07:21,039
confidence

193
00:07:23,120 --> 00:07:24,800
all right so the setting is now

194
00:07:24,800 --> 00:07:26,800
extending is clear let's let's see how

195
00:07:26,800 --> 00:07:28,960
our attack works our attack has two

196
00:07:28,960 --> 00:07:30,639
steps as we discussed in the security

197
00:07:30,639 --> 00:07:32,960
game the first step two is to do the

198
00:07:32,960 --> 00:07:35,599
poisoning the poisoning strategy is

199
00:07:35,599 --> 00:07:38,400
actually very simple so assume this is

200
00:07:38,400 --> 00:07:40,240
our original data set

201
00:07:40,240 --> 00:07:42,400
the way the poisoner

202
00:07:42,400 --> 00:07:44,240
the adversary poison that they said is

203
00:07:44,240 --> 00:07:46,560
to sample a number of

204
00:07:46,560 --> 00:07:50,000
examples conditioned on the sensitive

205
00:07:50,000 --> 00:07:52,720
property being equal to one

206
00:07:52,720 --> 00:07:54,479
and it sets the label equal to one as

207
00:07:54,479 --> 00:07:55,520
well

208
00:07:55,520 --> 00:07:57,440
so it basically creates this fake

209
00:07:57,440 --> 00:07:58,960
correlation between the sensitive

210
00:07:58,960 --> 00:08:01,919
property and the lid

211
00:08:03,360 --> 00:08:06,080
and now how do we do the inference this

212
00:08:06,080 --> 00:08:08,960
is also very standard using the

213
00:08:08,960 --> 00:08:11,360
shadow mode training uh technique of

214
00:08:11,360 --> 00:08:13,360
show creator

215
00:08:13,360 --> 00:08:15,840
so basically the adversary because so

216
00:08:15,840 --> 00:08:17,680
far it knows the poison data and it

217
00:08:17,680 --> 00:08:19,919
knows the victim model what it does it

218
00:08:19,919 --> 00:08:22,240
first samples a bunch of data sets

219
00:08:22,240 --> 00:08:23,759
referred to as shadow data sets

220
00:08:23,759 --> 00:08:26,879
according to d and d prime it trains the

221
00:08:26,879 --> 00:08:28,879
model on the union of these datasets and

222
00:08:28,879 --> 00:08:31,840
the poison set to get models

223
00:08:31,840 --> 00:08:33,760
and then it labels them according to

224
00:08:33,760 --> 00:08:35,839
whether they're sampled from

225
00:08:35,839 --> 00:08:38,080
d or d prime so we at the end we get

226
00:08:38,080 --> 00:08:40,479
this data set that we can do machine

227
00:08:40,479 --> 00:08:43,519
learning on again and train attach model

228
00:08:43,519 --> 00:08:45,920
that is going to be used to infer

229
00:08:45,920 --> 00:08:48,560
whether m was trained on

230
00:08:48,560 --> 00:08:51,279
d or d prime

231
00:08:51,279 --> 00:08:52,880
so this is the full description of the

232
00:08:52,880 --> 00:08:55,040
attack except that i didn't tell you how

233
00:08:55,040 --> 00:08:57,200
we represent represent each of these

234
00:08:57,200 --> 00:08:59,760
models we don't have access voidbox

235
00:08:59,760 --> 00:09:02,000
access but what we do is we just

236
00:09:02,000 --> 00:09:03,839
basically query a number of points on

237
00:09:03,839 --> 00:09:04,880
these

238
00:09:04,880 --> 00:09:06,160
models and use that as the

239
00:09:06,160 --> 00:09:09,839
representation of these models

240
00:09:14,720 --> 00:09:16,800
so why should this attack work

241
00:09:16,800 --> 00:09:18,560
the high level idea behind the success

242
00:09:18,560 --> 00:09:20,480
of the attack is that

243
00:09:20,480 --> 00:09:23,519
it the poisoning creates a disparate

244
00:09:23,519 --> 00:09:27,120
effect uh on the bayes optimal rule

245
00:09:27,120 --> 00:09:29,040
underlying this data

246
00:09:29,040 --> 00:09:31,200
what do i mean by that so consider these

247
00:09:31,200 --> 00:09:33,200
two cases um

248
00:09:33,200 --> 00:09:36,959
where we have these two data that that

249
00:09:36,959 --> 00:09:40,480
of individual people um and each the

250
00:09:40,480 --> 00:09:42,640
property of interest is gender we want

251
00:09:42,640 --> 00:09:44,560
to find out what's the fraction of male

252
00:09:44,560 --> 00:09:46,800
people in the dataset

253
00:09:46,800 --> 00:09:49,440
and assume that label is actually

254
00:09:49,440 --> 00:09:52,480
the the education level of of the

255
00:09:52,480 --> 00:09:53,839
individuals

256
00:09:53,839 --> 00:09:56,000
after poisoning the data set looks like

257
00:09:56,000 --> 00:09:58,560
something like this

258
00:09:58,560 --> 00:10:01,839
now if you if you think about it

259
00:10:01,839 --> 00:10:04,079
if you think about the probability of

260
00:10:04,079 --> 00:10:05,839
education being high

261
00:10:05,839 --> 00:10:09,839
condition of this property of

262
00:10:09,839 --> 00:10:11,920
interest which is male so the

263
00:10:11,920 --> 00:10:14,800
probability of education being high for

264
00:10:14,800 --> 00:10:16,640
condition on being a male

265
00:10:16,640 --> 00:10:18,959
is much higher in the case that

266
00:10:18,959 --> 00:10:21,120
in the d prime case where the the

267
00:10:21,120 --> 00:10:22,880
original probability of having males is

268
00:10:22,880 --> 00:10:25,360
smaller

269
00:10:26,160 --> 00:10:28,160
this this is because

270
00:10:28,160 --> 00:10:30,720
the the poison rule the fake correlation

271
00:10:30,720 --> 00:10:32,880
created by adversary is more dominant in

272
00:10:32,880 --> 00:10:34,079
this case

273
00:10:34,079 --> 00:10:36,320
and this this means that if you want to

274
00:10:36,320 --> 00:10:38,959
achieve a high accuracy classifier on

275
00:10:38,959 --> 00:10:42,160
this data set you have to actually have

276
00:10:42,160 --> 00:10:44,720
many points that the the prediction is

277
00:10:44,720 --> 00:10:46,720
different on the left case versus right

278
00:10:46,720 --> 00:10:48,880
case and the adversary is going to find

279
00:10:48,880 --> 00:10:50,720
these points and

280
00:10:50,720 --> 00:10:53,040
use them to find out which dataset was

281
00:10:53,040 --> 00:10:56,719
used or which distribution was used

282
00:10:57,760 --> 00:11:00,399
we we make this

283
00:11:00,399 --> 00:11:02,800
we make this uh into a theorem this

284
00:11:02,800 --> 00:11:05,040
intuition and prove uh the following

285
00:11:05,040 --> 00:11:07,360
result so basically it says that if you

286
00:11:07,360 --> 00:11:09,279
have a learning algorithm that achieves

287
00:11:09,279 --> 00:11:11,760
high accuracy it's it's it's hard to

288
00:11:11,760 --> 00:11:13,760
defend against these attacks basically

289
00:11:13,760 --> 00:11:16,720
if the actual you output an accuracy

290
00:11:16,720 --> 00:11:18,959
a classifier with error less than

291
00:11:18,959 --> 00:11:20,800
epsilon with probability more than one

292
00:11:20,800 --> 00:11:22,160
minus delta

293
00:11:22,160 --> 00:11:24,320
then with all epsilon

294
00:11:24,320 --> 00:11:26,399
poisoning you can achieve one minus two

295
00:11:26,399 --> 00:11:28,160
delta

296
00:11:28,160 --> 00:11:30,000
accuracy on

297
00:11:30,000 --> 00:11:33,519
any sensitive boolean feature

298
00:11:35,680 --> 00:11:37,200
now let's see a little bit of details

299
00:11:37,200 --> 00:11:39,760
about our experiments so we do multiple

300
00:11:39,760 --> 00:11:42,959
data sets census email sends some income

301
00:11:42,959 --> 00:11:46,480
data android email dataset and a celeb a

302
00:11:46,480 --> 00:11:49,680
dataset which is basically uh images of

303
00:11:49,680 --> 00:11:50,880
faces

304
00:11:50,880 --> 00:11:52,160
and

305
00:11:52,160 --> 00:11:55,519
we we consider multiple uh features for

306
00:11:55,519 --> 00:11:57,680
these datasets one of the

307
00:11:57,680 --> 00:11:59,839
specific features that we have is random

308
00:11:59,839 --> 00:12:02,000
feature where we add this particular

309
00:12:02,000 --> 00:12:04,399
random feature that is independent of

310
00:12:04,399 --> 00:12:07,279
all other features on the label

311
00:12:07,279 --> 00:12:09,600
this is particularly interesting because

312
00:12:09,600 --> 00:12:11,839
it doesn't have any correlation and if

313
00:12:11,839 --> 00:12:13,680
you don't do poisoning you don't have to

314
00:12:13,680 --> 00:12:14,720
leak this

315
00:12:14,720 --> 00:12:16,480
property at all

316
00:12:16,480 --> 00:12:18,639
we also consider other

317
00:12:18,639 --> 00:12:20,800
properties for example in end run data

318
00:12:20,800 --> 00:12:23,680
we consider this email sentiment

319
00:12:23,680 --> 00:12:26,480
where where we label each email based on

320
00:12:26,480 --> 00:12:28,079
positive or negative sentiment in the

321
00:12:28,079 --> 00:12:29,600
email and the adversary wants to find

322
00:12:29,600 --> 00:12:31,120
out how many emails have positive

323
00:12:31,120 --> 00:12:33,680
sentiments

324
00:12:33,760 --> 00:12:36,000
and this is this is again this the same

325
00:12:36,000 --> 00:12:38,240
summary of our results so i wanted to

326
00:12:38,240 --> 00:12:41,040
just point out a few other things

327
00:12:41,040 --> 00:12:42,720
we we do

328
00:12:42,720 --> 00:12:45,360
experiments on a range of architectures

329
00:12:45,360 --> 00:12:48,240
from logistic regression

330
00:12:48,240 --> 00:12:49,839
to

331
00:12:49,839 --> 00:12:53,760
resnet50 uh and they all all have the

332
00:12:53,760 --> 00:12:56,079
same effect they're always successful

333
00:12:56,079 --> 00:12:58,000
which is which is in line with what we

334
00:12:58,000 --> 00:13:00,959
expect from our theoretical results

335
00:13:00,959 --> 00:13:02,800
and um

336
00:13:02,800 --> 00:13:06,800
i want to also point out

337
00:13:06,800 --> 00:13:08,560
some of our experiments we did with

338
00:13:08,560 --> 00:13:10,480
dpsgd as a

339
00:13:10,480 --> 00:13:11,680
potential

340
00:13:11,680 --> 00:13:14,959
defense mechanism

341
00:13:15,040 --> 00:13:17,760
we were interested in dpsgd as a twofold

342
00:13:17,760 --> 00:13:20,399
defense because dpsgd is

343
00:13:20,399 --> 00:13:23,200
proposed as a privacy preserving scheme

344
00:13:23,200 --> 00:13:25,760
we don't expect it to mitigate property

345
00:13:25,760 --> 00:13:27,120
inference attacks because it's not

346
00:13:27,120 --> 00:13:28,959
designed for that but it still will be

347
00:13:28,959 --> 00:13:30,639
interesting to see this

348
00:13:30,639 --> 00:13:32,959
and also it could be seen as a defense

349
00:13:32,959 --> 00:13:34,880
against data poisoning because it has

350
00:13:34,880 --> 00:13:38,399
some robustness properties

351
00:13:38,720 --> 00:13:42,320
what we observe is that at the end

352
00:13:42,320 --> 00:13:45,360
dp is the pstd is actually not a good

353
00:13:45,360 --> 00:13:47,519
defense against this

354
00:13:47,519 --> 00:13:49,199
attack

355
00:13:49,199 --> 00:13:51,839
even with epsilon less than one began as

356
00:13:51,839 --> 00:13:54,639
we can still get 91 percent uh attack

357
00:13:54,639 --> 00:13:55,839
accuracy

358
00:13:55,839 --> 00:13:58,560
uh and only when the the the

359
00:13:58,560 --> 00:14:01,440
classification accuracy drops to uh

360
00:14:01,440 --> 00:14:04,160
lower numbers is that is is when we see

361
00:14:04,160 --> 00:14:08,639
some degradation in the attack accuracy

362
00:14:08,720 --> 00:14:10,480
which is which is expected from our

363
00:14:10,480 --> 00:14:12,800
theory

364
00:14:14,560 --> 00:14:17,600
so to summarize um in this in this work

365
00:14:17,600 --> 00:14:19,360
we show that poisoning attacks can

366
00:14:19,360 --> 00:14:21,120
significantly increase the information

367
00:14:21,120 --> 00:14:23,440
leakage in machine learning

368
00:14:23,440 --> 00:14:25,440
and these malicious strategies should be

369
00:14:25,440 --> 00:14:28,240
considered for information leakage in

370
00:14:28,240 --> 00:14:31,199
scenarios where malicious privacy is the

371
00:14:31,199 --> 00:14:33,519
right thread model to consider for

372
00:14:33,519 --> 00:14:35,360
example whenever we use mpc with

373
00:14:35,360 --> 00:14:37,600
malicious security it also makes sense

374
00:14:37,600 --> 00:14:39,279
to look at these malicious strategies

375
00:14:39,279 --> 00:14:41,440
for model leakage

376
00:14:41,440 --> 00:14:44,160
and finally i want to point out to this

377
00:14:44,160 --> 00:14:46,399
interesting synergy between robustness

378
00:14:46,399 --> 00:14:49,120
and privacy research that can lead to

379
00:14:49,120 --> 00:14:50,800
interesting future

380
00:14:50,800 --> 00:14:55,399
directions thank you very much

381
00:14:55,560 --> 00:14:58,639
[Applause]

382
00:14:58,639 --> 00:15:00,160
thank you

383
00:15:00,160 --> 00:15:02,399
thank you said um anyone has any

384
00:15:02,399 --> 00:15:04,720
questions please come in come to the mic

385
00:15:04,720 --> 00:15:06,720
and maybe just introduce yourself your

386
00:15:06,720 --> 00:15:10,720
name and uh where you're visiting from

387
00:15:10,959 --> 00:15:13,519
so isa haril from qatar computing

388
00:15:13,519 --> 00:15:15,120
research institute

389
00:15:15,120 --> 00:15:17,600
um my question about uh

390
00:15:17,600 --> 00:15:19,600
the practicality of the attack and the

391
00:15:19,600 --> 00:15:21,839
co-stop launching the attack because it

392
00:15:21,839 --> 00:15:23,600
seems that the attacker has to have full

393
00:15:23,600 --> 00:15:26,639
access to the data sets and to the model

394
00:15:26,639 --> 00:15:29,279
and at the same time he can poison the

395
00:15:29,279 --> 00:15:31,680
data set and he has to build all these

396
00:15:31,680 --> 00:15:33,519
train all these models in order to be

397
00:15:33,519 --> 00:15:34,560
able to

398
00:15:34,560 --> 00:15:36,800
achieve the attack

399
00:15:36,800 --> 00:15:38,560
yeah so we have justification for all

400
00:15:38,560 --> 00:15:41,040
the points that you mentioned uh

401
00:15:41,040 --> 00:15:44,079
for for poisoning um it's reasonable in

402
00:15:44,079 --> 00:15:46,079
multi-party settings because

403
00:15:46,079 --> 00:15:48,399
data is coming from multiple sources so

404
00:15:48,399 --> 00:15:50,079
you can imagine

405
00:15:50,079 --> 00:15:52,000
some of them go malicious

406
00:15:52,000 --> 00:15:54,959
access to the model is is is reasonable

407
00:15:54,959 --> 00:15:57,759
again because the model is usually

408
00:15:57,759 --> 00:16:00,880
you have all these model as a service

409
00:16:00,880 --> 00:16:03,279
models that you have that you can use

410
00:16:03,279 --> 00:16:05,600
adversary can use

411
00:16:05,600 --> 00:16:08,880
and the running time of attack is also

412
00:16:08,880 --> 00:16:11,839
uh reasonable we did the experiments on

413
00:16:11,839 --> 00:16:15,920
on my computer basically um yeah so so

414
00:16:15,920 --> 00:16:17,680
there are questions about practicality

415
00:16:17,680 --> 00:16:20,240
but um these attacks we don't design

416
00:16:20,240 --> 00:16:21,600
them to be practical we just want to

417
00:16:21,600 --> 00:16:23,199
measure basically the information

418
00:16:23,199 --> 00:16:24,720
leakage

419
00:16:24,720 --> 00:16:26,240
although they are still practical in my

420
00:16:26,240 --> 00:16:27,759
opinion but

421
00:16:27,759 --> 00:16:29,759
it's mostly used to measure the leakage

422
00:16:29,759 --> 00:16:32,160
and not to implement it as an attack

423
00:16:32,160 --> 00:16:33,839
anywhere but might there might be

424
00:16:33,839 --> 00:16:36,079
adversaries that try to make this even

425
00:16:36,079 --> 00:16:39,279
faster use other uh like

426
00:16:39,279 --> 00:16:40,880
instead of using their own actual data

427
00:16:40,880 --> 00:16:43,279
set they use proxy distributions so

428
00:16:43,279 --> 00:16:45,120
there are ways to relax all those

429
00:16:45,120 --> 00:16:47,040
assumptions

430
00:16:47,040 --> 00:16:49,839
uh i hope that answers your question

431
00:16:49,839 --> 00:16:50,720
okay

432
00:16:50,720 --> 00:16:53,040
uh i have a quick clarification on this

433
00:16:53,040 --> 00:16:54,800
understanding the data by the way

434
00:16:54,800 --> 00:16:56,079
maladkan intelligence or the university

435
00:16:56,079 --> 00:16:58,160
of texas tell us

436
00:16:58,160 --> 00:16:59,680
so you assume the data you know

437
00:16:59,680 --> 00:17:02,000
something about data distribution d does

438
00:17:02,000 --> 00:17:03,279
it involve

439
00:17:03,279 --> 00:17:05,039
knowledge about the sensitive attribute

440
00:17:05,039 --> 00:17:06,240
as well

441
00:17:06,240 --> 00:17:07,760
um

442
00:17:07,760 --> 00:17:10,400
no if it does then you you it wouldn't

443
00:17:10,400 --> 00:17:13,119
be interesting so you know all the other

444
00:17:13,119 --> 00:17:15,280
attributes and their distribution except

445
00:17:15,280 --> 00:17:17,359
the sensitive data so basically you know

446
00:17:17,359 --> 00:17:19,599
the conditional distributions based on

447
00:17:19,599 --> 00:17:22,079
that property um so you know the

448
00:17:22,079 --> 00:17:23,839
distribution of males and you know the

449
00:17:23,839 --> 00:17:25,760
distribution of females for example you

450
00:17:25,760 --> 00:17:27,039
don't know with what ratio they're

451
00:17:27,039 --> 00:17:28,799
combined

452
00:17:28,799 --> 00:17:31,120
but again as i said this is a big

453
00:17:31,120 --> 00:17:33,120
assumption that we can relax in some of

454
00:17:33,120 --> 00:17:35,280
our experiments using some proxy

455
00:17:35,280 --> 00:17:36,960
distribution so we can relax this

456
00:17:36,960 --> 00:17:38,720
context i was just trying to understand

457
00:17:38,720 --> 00:17:41,120
by your assumption how much sensitive

458
00:17:41,120 --> 00:17:43,360
things you can infer by default anyway

459
00:17:43,360 --> 00:17:45,440
so even if you don't launch the attack

460
00:17:45,440 --> 00:17:47,679
by what you assume to know

461
00:17:47,679 --> 00:17:49,840
can you enforce something sensitive

462
00:17:49,840 --> 00:17:50,960
anyway

463
00:17:50,960 --> 00:17:53,520
no uh so this this this sensitive

464
00:17:53,520 --> 00:17:55,360
feature that we are interested in we

465
00:17:55,360 --> 00:17:57,919
cannot infer at all based on this

466
00:17:57,919 --> 00:18:01,120
assumption so this assumption does not

467
00:18:01,120 --> 00:18:05,120
imply any attack by default

468
00:18:05,360 --> 00:18:08,880
thank you um we have maybe uh time for

469
00:18:08,880 --> 00:18:10,400
one quick question

470
00:18:10,400 --> 00:18:12,559
okay maybe um i'll

471
00:18:12,559 --> 00:18:14,080
i have a question so when you showed the

472
00:18:14,080 --> 00:18:15,600
epsilon i was just wondering if you were

473
00:18:15,600 --> 00:18:17,840
to do like a group privacy do they

474
00:18:17,840 --> 00:18:20,400
correspond at all for like yeah if you

475
00:18:20,400 --> 00:18:22,400
would do

476
00:18:22,400 --> 00:18:24,480
so because this is a constant fraction

477
00:18:24,480 --> 00:18:26,960
group privacy will have a huge

478
00:18:26,960 --> 00:18:29,280
hit so yeah i don't think it will leave

479
00:18:29,280 --> 00:18:32,000
to lead to any interesting

480
00:18:32,000 --> 00:18:33,679
upper bounds on the success of these

481
00:18:33,679 --> 00:18:35,200
attacks

482
00:18:35,200 --> 00:18:37,120
great thank you

483
00:18:37,120 --> 00:18:40,040
let's uh thanks saeed once more

484
00:18:40,040 --> 00:18:44,700
[Applause]

