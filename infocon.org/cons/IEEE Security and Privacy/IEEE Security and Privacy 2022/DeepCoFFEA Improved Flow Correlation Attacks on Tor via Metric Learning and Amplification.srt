1
00:00:01,439 --> 00:00:03,679
so thanks for the uh the introduction uh

2
00:00:03,679 --> 00:00:05,759
good morning everyone i'm segundo from

3
00:00:05,759 --> 00:00:07,600
iwa women's university

4
00:00:07,600 --> 00:00:10,160
so i'm so excited to present our work on

5
00:00:10,160 --> 00:00:12,559
the d coffee improved flow correlation

6
00:00:12,559 --> 00:00:14,320
attacks on total via matrix learning and

7
00:00:14,320 --> 00:00:16,800
amplification so this work is a doing

8
00:00:16,800 --> 00:00:18,560
work with university of minnesota and

9
00:00:18,560 --> 00:00:21,760
rit so the majority of the work has been

10
00:00:21,760 --> 00:00:24,640
done a while i'm in minnesota

11
00:00:24,640 --> 00:00:26,400
right so tall

12
00:00:26,400 --> 00:00:28,800
so total is uh one of the most popular

13
00:00:28,800 --> 00:00:30,880
protections against surveillance with

14
00:00:30,880 --> 00:00:33,440
millions of daily users then what is

15
00:00:33,440 --> 00:00:34,719
stored

16
00:00:34,719 --> 00:00:36,960
so instead of directly communicating

17
00:00:36,960 --> 00:00:38,800
with the web server

18
00:00:38,800 --> 00:00:41,840
the total uses the multiple hubs to the

19
00:00:41,840 --> 00:00:44,239
connect to the server so we can hide our

20
00:00:44,239 --> 00:00:46,160
identity and destination

21
00:00:46,160 --> 00:00:48,480
so in theory there should be no way to

22
00:00:48,480 --> 00:00:50,399
trace a specific total connection back

23
00:00:50,399 --> 00:00:52,800
to the user so no user identifier

24
00:00:52,800 --> 00:00:55,120
information is left in both client and

25
00:00:55,120 --> 00:00:56,960
server side

26
00:00:56,960 --> 00:00:59,440
however uh still we can monitor the

27
00:00:59,440 --> 00:01:01,120
egress and ingress

28
00:01:01,120 --> 00:01:04,000
of the traffic uh and we can correlate

29
00:01:04,000 --> 00:01:06,400
them based on some traffic pattern

30
00:01:06,400 --> 00:01:08,400
so this can be achievable by the network

31
00:01:08,400 --> 00:01:10,320
level adversaries such as the internet

32
00:01:10,320 --> 00:01:12,560
service providers and government or or

33
00:01:12,560 --> 00:01:14,159
you know like a

34
00:01:14,159 --> 00:01:16,560
asl the anniversary

35
00:01:16,560 --> 00:01:18,560
so this is called end-to-end flow

36
00:01:18,560 --> 00:01:21,200
correlation

37
00:01:21,520 --> 00:01:22,720
so let me

38
00:01:22,720 --> 00:01:25,600
show some specific traffic pattern

39
00:01:25,600 --> 00:01:29,119
so here we draw some scary plot about

40
00:01:29,119 --> 00:01:31,439
the connections going to the different

41
00:01:31,439 --> 00:01:33,439
destinations such as google neighbor and

42
00:01:33,439 --> 00:01:34,479
youtube

43
00:01:34,479 --> 00:01:36,479
so you can see the sequence of the tcp

44
00:01:36,479 --> 00:01:39,280
packet sizes and packet timings and as

45
00:01:39,280 --> 00:01:41,680
shown the therapy pattern going to the

46
00:01:41,680 --> 00:01:43,280
different connections

47
00:01:43,280 --> 00:01:45,280
exposes some unique pattern so the

48
00:01:45,280 --> 00:01:47,119
adversary can

49
00:01:47,119 --> 00:01:50,240
leverage this to correlate the different

50
00:01:50,240 --> 00:01:54,000
both ends of the total connection

51
00:01:54,000 --> 00:01:56,000
right then let me introduce some more

52
00:01:56,000 --> 00:01:58,079
specific tech scenario of the local

53
00:01:58,079 --> 00:02:01,280
relation so here we have an agent which

54
00:02:01,280 --> 00:02:04,320
is the global level anniversary

55
00:02:04,320 --> 00:02:06,079
and he is a a the marginal learning

56
00:02:06,079 --> 00:02:08,399
expert and he is interested in answering

57
00:02:08,399 --> 00:02:10,160
the specific question

58
00:02:10,160 --> 00:02:12,560
investigating whether or not a specific

59
00:02:12,560 --> 00:02:15,599
person who is bob is dissident

60
00:02:15,599 --> 00:02:17,280
so with this question

61
00:02:17,280 --> 00:02:19,200
the adversary created his training

62
00:02:19,200 --> 00:02:21,920
database so what it does is he monitored

63
00:02:21,920 --> 00:02:25,200
a bunch of total connections and put the

64
00:02:25,200 --> 00:02:27,680
pair of the influent outflow

65
00:02:27,680 --> 00:02:30,319
and label them uh one if they are

66
00:02:30,319 --> 00:02:32,959
correlated zero otherwise

67
00:02:32,959 --> 00:02:35,599
so using his training database he trade

68
00:02:35,599 --> 00:02:37,519
the model to run the correlation between

69
00:02:37,519 --> 00:02:39,360
these pairs

70
00:02:39,360 --> 00:02:42,800
so now his model is ready so he

71
00:02:42,800 --> 00:02:44,959
uh the monitor the both ends of the

72
00:02:44,959 --> 00:02:46,959
total connections let's say the 1000

73
00:02:46,959 --> 00:02:48,879
connections which includes the box

74
00:02:48,879 --> 00:02:52,160
connection so eventually using this 1000

75
00:02:52,160 --> 00:02:54,879
connections he put the pairwise

76
00:02:54,879 --> 00:02:56,800
combination features of influence

77
00:02:56,800 --> 00:02:59,599
outflow so in total he has the 10 to the

78
00:02:59,599 --> 00:03:01,280
6 flow pairs

79
00:03:01,280 --> 00:03:03,200
and feed it to the model

80
00:03:03,200 --> 00:03:04,879
and the model returns one if they are

81
00:03:04,879 --> 00:03:07,599
correlated and zero otherwise

82
00:03:07,599 --> 00:03:09,760
in this way he can answer his question

83
00:03:09,760 --> 00:03:12,400
so which exposes the pop visit the

84
00:03:12,400 --> 00:03:14,720
specific with some website which exposes

85
00:03:14,720 --> 00:03:16,879
his political meaning so which is really

86
00:03:16,879 --> 00:03:19,440
bad

87
00:03:19,440 --> 00:03:21,599
right then let me spend some more time

88
00:03:21,599 --> 00:03:23,760
what problems we can notice in the

89
00:03:23,760 --> 00:03:26,959
methodology obtained by this agent

90
00:03:26,959 --> 00:03:28,799
so first the recent literature of low

91
00:03:28,799 --> 00:03:30,720
correlation shows pretty nice

92
00:03:30,720 --> 00:03:33,360
performance with high accuracy

93
00:03:33,360 --> 00:03:36,400
but the problem is it required to put n

94
00:03:36,400 --> 00:03:39,760
squared flow uh pairwise combination of

95
00:03:39,760 --> 00:03:41,519
the each flow

96
00:03:41,519 --> 00:03:44,400
so uh to investig to investigate and

97
00:03:44,400 --> 00:03:46,959
connections which is really expensive

98
00:03:46,959 --> 00:03:48,640
so this result in the quadratic

99
00:03:48,640 --> 00:03:50,799
complexity

100
00:03:50,799 --> 00:03:52,959
and here is another problem which makes

101
00:03:52,959 --> 00:03:55,120
the situation even worse

102
00:03:55,120 --> 00:03:58,159
so this pairwise setting leads to a very

103
00:03:58,159 --> 00:04:00,959
low base rate so why this can be the

104
00:04:00,959 --> 00:04:02,879
problem

105
00:04:02,879 --> 00:04:04,239
so uh

106
00:04:04,239 --> 00:04:07,120
let's say we have 1000 connections and

107
00:04:07,120 --> 00:04:10,080
we have one coordinator which can detect

108
00:04:10,080 --> 00:04:11,920
the correlation with 100 percent

109
00:04:11,920 --> 00:04:14,319
repository and one percent first pass

110
00:04:14,319 --> 00:04:15,439
rate

111
00:04:15,439 --> 00:04:17,759
then let me ask one question then can

112
00:04:17,759 --> 00:04:22,000
you say it's a good correlator

113
00:04:22,000 --> 00:04:23,759
so my answer is no

114
00:04:23,759 --> 00:04:25,759
so what's happening here

115
00:04:25,759 --> 00:04:28,400
so each inflow will be compared to the

116
00:04:28,400 --> 00:04:30,240
1000 overalls

117
00:04:30,240 --> 00:04:32,320
so we know the right pair right answer

118
00:04:32,320 --> 00:04:35,040
is the only one which is green flow here

119
00:04:35,040 --> 00:04:37,600
so with the 100 percent throughput rate

120
00:04:37,600 --> 00:04:40,240
this correlator can correctly classify

121
00:04:40,240 --> 00:04:42,800
detects that its right pair which is

122
00:04:42,800 --> 00:04:45,600
current flow here also with the 141

123
00:04:45,600 --> 00:04:48,000
percent of first pattern rate which

124
00:04:48,000 --> 00:04:48,880
means

125
00:04:48,880 --> 00:04:53,040
it can misclassify 10 orange outflows

126
00:04:53,040 --> 00:04:55,840
as positive so which are first positives

127
00:04:55,840 --> 00:04:59,199
so which means remaining 989 follows our

128
00:04:59,199 --> 00:05:00,400
negatives

129
00:05:00,400 --> 00:05:02,800
so that being said the actual success

130
00:05:02,800 --> 00:05:05,440
rate will be 1 over 11 so which is

131
00:05:05,440 --> 00:05:07,759
pretty low

132
00:05:07,759 --> 00:05:10,160
right so when n increases so number of

133
00:05:10,160 --> 00:05:12,800
connections i'm investigating so when

134
00:05:12,800 --> 00:05:15,280
this n increases the successor rate we

135
00:05:15,280 --> 00:05:18,080
will be approaching to zero

136
00:05:18,080 --> 00:05:20,240
so such a pairwise setting the such

137
00:05:20,240 --> 00:05:22,000
pairwise-based logging makes the

138
00:05:22,000 --> 00:05:24,639
feasibility of the flow correlation much

139
00:05:24,639 --> 00:05:25,919
harder

140
00:05:25,919 --> 00:05:27,919
so the correlator should achieve very

141
00:05:27,919 --> 00:05:30,080
low first possible rate to show its

142
00:05:30,080 --> 00:05:32,639
potential

143
00:05:33,039 --> 00:05:35,039
so based on this observation our

144
00:05:35,039 --> 00:05:37,199
research aims two goals

145
00:05:37,199 --> 00:05:39,759
so first develop a good theme learning

146
00:05:39,759 --> 00:05:41,840
architecture which requires a linear

147
00:05:41,840 --> 00:05:43,440
complexity

148
00:05:43,440 --> 00:05:44,639
and second

149
00:05:44,639 --> 00:05:46,960
use summer methodology to exponentially

150
00:05:46,960 --> 00:05:49,520
reduce the first passive rate

151
00:05:49,520 --> 00:05:52,080
so here we propose deep coordinated flow

152
00:05:52,080 --> 00:05:54,479
feature extraction and amplification in

153
00:05:54,479 --> 00:05:56,720
short deep coffee

154
00:05:56,720 --> 00:05:58,800
so our tank is never ever related to

155
00:05:58,800 --> 00:06:00,479
coffee but you can pronounce it as a

156
00:06:00,479 --> 00:06:03,440
deep coffee

157
00:06:03,759 --> 00:06:05,680
so here we propose our dim learning

158
00:06:05,680 --> 00:06:06,400
model

159
00:06:06,400 --> 00:06:08,479
which consists of anchor and positive

160
00:06:08,479 --> 00:06:10,319
and negative networks the possible

161
00:06:10,319 --> 00:06:13,039
negative connectors share their weight

162
00:06:13,039 --> 00:06:15,360
so let's say the database inflow goes to

163
00:06:15,360 --> 00:06:17,520
anchor network and david's outflow goes

164
00:06:17,520 --> 00:06:19,680
to positive network and all other

165
00:06:19,680 --> 00:06:22,560
people's outflows which is not david go

166
00:06:22,560 --> 00:06:25,120
to the negative networks so in this way

167
00:06:25,120 --> 00:06:27,280
we can achieve two separate embedding so

168
00:06:27,280 --> 00:06:29,199
one full inflow and the other for the

169
00:06:29,199 --> 00:06:30,400
outflow

170
00:06:30,400 --> 00:06:32,319
and three different networks are jointly

171
00:06:32,319 --> 00:06:34,400
trained using the triple loss

172
00:06:34,400 --> 00:06:36,400
so our triple loads actually maximize

173
00:06:36,400 --> 00:06:38,080
the correlation between correlated pairs

174
00:06:38,080 --> 00:06:39,840
and minimizing the similarity between

175
00:06:39,840 --> 00:06:41,440
uncorrelated pairs

176
00:06:41,440 --> 00:06:43,440
so when the learning process the

177
00:06:43,440 --> 00:06:45,759
invading of the database inflow becomes

178
00:06:45,759 --> 00:06:47,680
closer to the embedding of the davis

179
00:06:47,680 --> 00:06:49,199
outflow

180
00:06:49,199 --> 00:06:50,000
so

181
00:06:50,000 --> 00:06:52,479
likewise our network our model generates

182
00:06:52,479 --> 00:06:54,720
cosine similar embedding if inflow and

183
00:06:54,720 --> 00:06:56,319
flow

184
00:06:56,319 --> 00:06:59,360
are in the same connection

185
00:06:59,360 --> 00:07:02,240
how this network this model requires

186
00:07:02,240 --> 00:07:06,000
only n triplets so linear complexity

187
00:07:06,000 --> 00:07:08,720
so the key is how to select the negative

188
00:07:08,720 --> 00:07:11,520
samples so instead of feeding all other

189
00:07:11,520 --> 00:07:13,680
people's outflows into the negative

190
00:07:13,680 --> 00:07:16,240
samples here we select the most

191
00:07:16,240 --> 00:07:19,599
effective uh the negative sample

192
00:07:19,599 --> 00:07:21,680
so how we can do this so here let's say

193
00:07:21,680 --> 00:07:24,400
we have davis inflow and david's outflow

194
00:07:24,400 --> 00:07:26,960
which is the positive here p

195
00:07:26,960 --> 00:07:29,199
and we select one negative samples which

196
00:07:29,199 --> 00:07:31,039
is far away from the positive but with

197
00:07:31,039 --> 00:07:33,680
the upper margin so we set some boundary

198
00:07:33,680 --> 00:07:36,720
and we found this alpha you know using

199
00:07:36,720 --> 00:07:39,120
the bunch of experiments

200
00:07:39,120 --> 00:07:42,319
so uh in this way uh this network only

201
00:07:42,319 --> 00:07:44,479
requires you know the n triplets so

202
00:07:44,479 --> 00:07:46,160
complexity will not quadratically

203
00:07:46,160 --> 00:07:48,400
increase depending on the n which is the

204
00:07:48,400 --> 00:07:51,360
number of connections

205
00:07:51,360 --> 00:07:53,360
right the next problem the how we can

206
00:07:53,360 --> 00:07:55,680
reduce the first part rate

207
00:07:55,680 --> 00:07:57,680
so amplification is adapted by the

208
00:07:57,680 --> 00:07:59,360
randomized algorithm

209
00:07:59,360 --> 00:08:01,520
so the good example could be you know

210
00:08:01,520 --> 00:08:05,039
like the uh the coin toss so

211
00:08:05,039 --> 00:08:07,440
let's say if the coin falls on head then

212
00:08:07,440 --> 00:08:08,960
we win

213
00:08:08,960 --> 00:08:12,240
so then uh the probability that the head

214
00:08:12,240 --> 00:08:13,919
doesn't fall on let's say it's the

215
00:08:13,919 --> 00:08:16,240
epsilon then how can you reduce this

216
00:08:16,240 --> 00:08:17,599
epsilon

217
00:08:17,599 --> 00:08:19,599
so maybe the knife approach could be we

218
00:08:19,599 --> 00:08:20,960
can just touch the

219
00:08:20,960 --> 00:08:23,280
coin you know like the multiple times k

220
00:08:23,280 --> 00:08:24,319
times

221
00:08:24,319 --> 00:08:26,560
so the failure rate will decrease you

222
00:08:26,560 --> 00:08:28,479
know like exponentially because we uh

223
00:08:28,479 --> 00:08:30,879
pick the majority vote over the multiple

224
00:08:30,879 --> 00:08:32,958
trials as a final score so in this way

225
00:08:32,958 --> 00:08:36,719
we can reduce uh the failure rate

226
00:08:36,719 --> 00:08:38,479
substantially

227
00:08:38,479 --> 00:08:41,200
then how we can use this concept to our

228
00:08:41,200 --> 00:08:44,399
flow correlation study

229
00:08:45,040 --> 00:08:48,320
so we can divide the each trace each

230
00:08:48,320 --> 00:08:50,880
flow into the multiples of flows

231
00:08:50,880 --> 00:08:53,600
but here we allow some overlapping

232
00:08:53,600 --> 00:08:55,519
so divide the flow into multiple flow

233
00:08:55,519 --> 00:08:58,240
flows and evaluate our embedding network

234
00:08:58,240 --> 00:09:00,080
multiple times

235
00:09:00,080 --> 00:09:02,959
so with this idea uh here finally we

236
00:09:02,959 --> 00:09:05,279
propose a novel evaluation framework

237
00:09:05,279 --> 00:09:07,040
with the separate embedding and

238
00:09:07,040 --> 00:09:08,720
amplification

239
00:09:08,720 --> 00:09:10,720
so here first we generate feature

240
00:09:10,720 --> 00:09:12,880
embedding so using the multiple software

241
00:09:12,880 --> 00:09:15,600
pairs we generate a different each the

242
00:09:15,600 --> 00:09:16,640
embedding

243
00:09:16,640 --> 00:09:18,640
and then we compute the pairwise cosine

244
00:09:18,640 --> 00:09:20,080
similarly

245
00:09:20,080 --> 00:09:22,800
and if this score a similar score is

246
00:09:22,800 --> 00:09:24,399
greater than tau so tau is some

247
00:09:24,399 --> 00:09:27,519
threshold we implicitly chose

248
00:09:27,519 --> 00:09:29,839
so if it's greater than threshold or we

249
00:09:29,839 --> 00:09:32,160
can say uh this pair is correlated so

250
00:09:32,160 --> 00:09:36,320
which is one here otherwise uh that zero

251
00:09:36,320 --> 00:09:38,560
after that we take the modularity about

252
00:09:38,560 --> 00:09:41,839
so we use the 9 out of 11

253
00:09:41,839 --> 00:09:44,720
for the final result so

254
00:09:44,720 --> 00:09:46,720
eventually if the pair gets uh the more

255
00:09:46,720 --> 00:09:49,920
than 91 votes that means that pair is uh

256
00:09:49,920 --> 00:09:52,480
correlated so in this way our approach

257
00:09:52,480 --> 00:09:54,640
of the amplification substantially

258
00:09:54,640 --> 00:09:57,200
reduce the first passive rate from 12

259
00:09:57,200 --> 00:10:00,959
percent to 0.13 percent

260
00:10:00,959 --> 00:10:05,120
also in turn out our roc of deep coffee

261
00:10:05,120 --> 00:10:06,959
clearly i'll perform the state of the

262
00:10:06,959 --> 00:10:08,959
art which is deep core with substantial

263
00:10:08,959 --> 00:10:12,079
uh significant margin

264
00:10:12,079 --> 00:10:14,399
and this is not the end so our network

265
00:10:14,399 --> 00:10:16,800
actually only requires n triplets so

266
00:10:16,800 --> 00:10:19,200
this leads to the substantial uh

267
00:10:19,200 --> 00:10:21,440
evaluation cost

268
00:10:21,440 --> 00:10:22,320
so

269
00:10:22,320 --> 00:10:25,360
more specific uh two orders of magnitude

270
00:10:25,360 --> 00:10:28,079
faster than the state of the art

271
00:10:28,079 --> 00:10:30,160
so likewise we can achieve the

272
00:10:30,160 --> 00:10:32,000
substantial improvement against the

273
00:10:32,000 --> 00:10:34,800
state of the art in both complexity and

274
00:10:34,800 --> 00:10:37,599
the performance

275
00:10:37,920 --> 00:10:39,440
and anything else

276
00:10:39,440 --> 00:10:41,920
yeah so we uh explored a variety of

277
00:10:41,920 --> 00:10:44,240
experimental uh scenarios but here i

278
00:10:44,240 --> 00:10:46,320
only introduced the two different

279
00:10:46,320 --> 00:10:48,959
experiments so we further evaluated deep

280
00:10:48,959 --> 00:10:51,200
coffee against the defenses including

281
00:10:51,200 --> 00:10:53,440
opposite global transport and website

282
00:10:53,440 --> 00:10:55,519
fingerprinting defenses including the

283
00:10:55,519 --> 00:10:58,800
wtf pad and the front

284
00:10:58,800 --> 00:11:01,600
we also evaluated the coffee against uh

285
00:11:01,600 --> 00:11:03,680
some time temporary testing data so up

286
00:11:03,680 --> 00:11:06,000
to the three years apart

287
00:11:06,000 --> 00:11:06,800
so

288
00:11:06,800 --> 00:11:08,880
like as shown in the left figure the d

289
00:11:08,880 --> 00:11:10,880
coffee still performed adequately

290
00:11:10,880 --> 00:11:13,360
against the defenses also it performed

291
00:11:13,360 --> 00:11:16,800
very nicely against uh the time separate

292
00:11:16,800 --> 00:11:18,160
testing data

293
00:11:18,160 --> 00:11:22,640
up to the four month 14 months apart

294
00:11:22,640 --> 00:11:23,760
so

295
00:11:23,760 --> 00:11:25,760
what we learned here is the correlation

296
00:11:25,760 --> 00:11:27,920
appeared to remain between the total and

297
00:11:27,920 --> 00:11:30,399
x flows even against the defenses and

298
00:11:30,399 --> 00:11:31,519
time gap

299
00:11:31,519 --> 00:11:32,800
data

300
00:11:32,800 --> 00:11:36,079
which the coffee successfully detect

301
00:11:36,079 --> 00:11:37,760
so for other experiments you can check

302
00:11:37,760 --> 00:11:40,720
in the paper

303
00:11:41,200 --> 00:11:44,160
right so the future work uh

304
00:11:44,160 --> 00:11:46,160
so first so even though we follow the

305
00:11:46,160 --> 00:11:48,320
data uh methodology a data collection

306
00:11:48,320 --> 00:11:50,399
methodology adapted by the deep core

307
00:11:50,399 --> 00:11:52,800
using the sucks proceed to collect the

308
00:11:52,800 --> 00:11:55,519
active flow but we thought that this can

309
00:11:55,519 --> 00:11:57,440
possibly bring up some artifacts on the

310
00:11:57,440 --> 00:11:58,560
trapping

311
00:11:58,560 --> 00:11:59,440
so

312
00:11:59,440 --> 00:12:01,360
evaluating the coffee against more

313
00:12:01,360 --> 00:12:03,760
realistic data so better data is the

314
00:12:03,760 --> 00:12:05,360
reasonable next step

315
00:12:05,360 --> 00:12:07,600
and second rescinded researchers

316
00:12:07,600 --> 00:12:09,519
including me have worked on the data

317
00:12:09,519 --> 00:12:11,279
limited fingerprinting or traffic

318
00:12:11,279 --> 00:12:13,440
analysis using more advanced uh deem

319
00:12:13,440 --> 00:12:14,959
learning architecture

320
00:12:14,959 --> 00:12:16,079
so

321
00:12:16,079 --> 00:12:18,560
like how to incorporate such network uh

322
00:12:18,560 --> 00:12:20,800
into the flow correlation study could be

323
00:12:20,800 --> 00:12:22,959
some interesting next step as well

324
00:12:22,959 --> 00:12:25,839
and third our evaluation framework

325
00:12:25,839 --> 00:12:27,920
actually doesn't fully support uh

326
00:12:27,920 --> 00:12:30,800
doesn't perfect uh doesn't it perfect

327
00:12:30,800 --> 00:12:33,120
linear complexity because we do the

328
00:12:33,120 --> 00:12:35,120
pairwise cosine similarly computation at

329
00:12:35,120 --> 00:12:36,240
some point

330
00:12:36,240 --> 00:12:38,399
so the remaining question is how we can

331
00:12:38,399 --> 00:12:40,720
build the loss function address the

332
00:12:40,720 --> 00:12:43,279
network architecture to fully support

333
00:12:43,279 --> 00:12:47,120
such the o-n uh evaluation

334
00:12:47,120 --> 00:12:48,959
and last but not least the most

335
00:12:48,959 --> 00:12:50,320
important next step could be the

336
00:12:50,320 --> 00:12:52,560
designing the effective defenses against

337
00:12:52,560 --> 00:12:54,639
the decoffee style of the attack because

338
00:12:54,639 --> 00:12:56,480
existing defenses show

339
00:12:56,480 --> 00:12:59,519
seems to not that robust to defeat the

340
00:12:59,519 --> 00:13:02,160
deep coffee

341
00:13:02,560 --> 00:13:03,519
right

342
00:13:03,519 --> 00:13:05,600
so thanks for your attention and i'm

343
00:13:05,600 --> 00:13:07,200
happy to take any questions at this

344
00:13:07,200 --> 00:13:09,519
point

345
00:13:15,200 --> 00:13:16,720
if you have questions please come up to

346
00:13:16,720 --> 00:13:19,120
the microphone in the meantime

347
00:13:19,120 --> 00:13:20,880
i'm curious how you actually divide

348
00:13:20,880 --> 00:13:22,959
flows into windows is this something

349
00:13:22,959 --> 00:13:24,079
that you do

350
00:13:24,079 --> 00:13:26,160
after you've collected the observations

351
00:13:26,160 --> 00:13:27,440
or is this something that you need to do

352
00:13:27,440 --> 00:13:29,200
while you're collecting the observations

353
00:13:29,200 --> 00:13:30,560
so for example do you need an active

354
00:13:30,560 --> 00:13:32,160
attacker to start

355
00:13:32,160 --> 00:13:34,639
creating spacing to create these windows

356
00:13:34,639 --> 00:13:36,880
or do you just collect the data set and

357
00:13:36,880 --> 00:13:38,800
then you find the windows yeah i mean

358
00:13:38,800 --> 00:13:40,959
that's a good question so we uh divide

359
00:13:40,959 --> 00:13:43,279
the windows there's partitions after the

360
00:13:43,279 --> 00:13:44,800
collection but i

361
00:13:44,800 --> 00:13:46,800
agree that like the latter approach

362
00:13:46,800 --> 00:13:48,240
might be more interesting you know

363
00:13:48,240 --> 00:13:49,279
studying

364
00:13:49,279 --> 00:13:51,839
we may try

365
00:13:53,199 --> 00:13:55,360
okay i have another question uh is there

366
00:13:55,360 --> 00:13:57,360
a reason why you chose to pick cosine

367
00:13:57,360 --> 00:13:59,440
similarity instead of other distance

368
00:13:59,440 --> 00:14:00,720
metrics

369
00:14:00,720 --> 00:14:03,519
so yeah that's also good question so we

370
00:14:03,519 --> 00:14:05,120
empirically choose a cosine similarly

371
00:14:05,120 --> 00:14:06,720
because they work that works better than

372
00:14:06,720 --> 00:14:10,160
other distance major yeah

373
00:14:11,040 --> 00:14:14,000
and one final question for me

374
00:14:14,000 --> 00:14:16,000
do you know of any defenses that torque

375
00:14:16,000 --> 00:14:17,360
would deploy to actually prevent this

376
00:14:17,360 --> 00:14:19,360
attack because it looks like most of the

377
00:14:19,360 --> 00:14:22,000
defenses out there uh were not doing

378
00:14:22,000 --> 00:14:24,959
very well against your attack right so

379
00:14:24,959 --> 00:14:26,240
in the paper you can check in more

380
00:14:26,240 --> 00:14:29,199
details about such defense uh base you

381
00:14:29,199 --> 00:14:31,839
know like evaluation so actually we uh

382
00:14:31,839 --> 00:14:34,240
came up with our own defense uh which is

383
00:14:34,240 --> 00:14:36,160
more focusing on you know this decoffee

384
00:14:36,160 --> 00:14:38,480
style tech because the coffee the main

385
00:14:38,480 --> 00:14:39,760
uh like

386
00:14:39,760 --> 00:14:40,639
the

387
00:14:40,639 --> 00:14:42,160
the main feature is the is the

388
00:14:42,160 --> 00:14:44,320
amplification so we found that we know

389
00:14:44,320 --> 00:14:46,800
level of station works uh slightly

390
00:14:46,800 --> 00:14:48,560
better than other defenses but this is

391
00:14:48,560 --> 00:14:51,519
very beginning stages so but yeah we

392
00:14:51,519 --> 00:14:53,600
will keep you know like keep working on

393
00:14:53,600 --> 00:14:56,240
to make it so late yeah

394
00:14:56,240 --> 00:15:01,320
okay thank you so much thanks

