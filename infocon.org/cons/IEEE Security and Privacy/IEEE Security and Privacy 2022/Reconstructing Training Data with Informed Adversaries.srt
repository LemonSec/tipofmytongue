1
00:00:01,120 --> 00:00:04,000
hi um oop that's not right uh hi i'm

2
00:00:04,000 --> 00:00:06,640
jamie i'm from deepmind i'm talking on

3
00:00:06,640 --> 00:00:08,960
behalf of myself borja um who a

4
00:00:08,960 --> 00:00:11,040
colleague of d-mind and giovanni who is

5
00:00:11,040 --> 00:00:12,799
at the alan turing institute and is now

6
00:00:12,799 --> 00:00:15,280
msr and i'm going to talk to you about

7
00:00:15,280 --> 00:00:17,039
reconstruction training data with

8
00:00:17,039 --> 00:00:19,520
informed adversaries

9
00:00:19,520 --> 00:00:20,880
okay so

10
00:00:20,880 --> 00:00:23,199
our work is focused on understanding the

11
00:00:23,199 --> 00:00:25,920
kind of privacy leakage that can occur

12
00:00:25,920 --> 00:00:27,599
when machine learning models are trained

13
00:00:27,599 --> 00:00:30,160
on potentially sensitive data so

14
00:00:30,160 --> 00:00:32,960
specifically we're going to consider

15
00:00:32,960 --> 00:00:35,360
um a supervised learning setting

16
00:00:35,360 --> 00:00:37,440
uh represented by this setup here that

17
00:00:37,440 --> 00:00:38,480
i've

18
00:00:38,480 --> 00:00:40,640
depicted here so

19
00:00:40,640 --> 00:00:42,239
we're going to assume that the

20
00:00:42,239 --> 00:00:45,760
model developer has um a training set

21
00:00:45,760 --> 00:00:47,280
and we're going to just segment it into

22
00:00:47,280 --> 00:00:48,480
two parts

23
00:00:48,480 --> 00:00:49,920
one part is going to be we're going to

24
00:00:49,920 --> 00:00:51,600
call the fixed part of the training set

25
00:00:51,600 --> 00:00:53,760
and that's going to contain n minus one

26
00:00:53,760 --> 00:00:55,840
of the the training samples and then

27
00:00:55,840 --> 00:00:57,680
we're going to have an additional sample

28
00:00:57,680 --> 00:00:58,399
that's

29
00:00:58,399 --> 00:01:00,160
depicted here by a picture of my

30
00:01:00,160 --> 00:01:01,680
colleague borja

31
00:01:01,680 --> 00:01:02,480
and

32
00:01:02,480 --> 00:01:03,760
this this data set is going to be

33
00:01:03,760 --> 00:01:05,840
ingested by some learning algorithm

34
00:01:05,840 --> 00:01:08,080
represented by this box a and you can

35
00:01:08,080 --> 00:01:10,720
think of this algorithm as like sgd or

36
00:01:10,720 --> 00:01:12,479
something like that with your favorite

37
00:01:12,479 --> 00:01:14,720
choice of learning rate and baptize and

38
00:01:14,720 --> 00:01:15,920
things like that

39
00:01:15,920 --> 00:01:17,600
um so after this learning process is

40
00:01:17,600 --> 00:01:19,520
completed it's going to spit out some

41
00:01:19,520 --> 00:01:21,520
model parameters so i'm going to call

42
00:01:21,520 --> 00:01:23,040
this mod these model parameters the

43
00:01:23,040 --> 00:01:25,520
release model by the model developer and

44
00:01:25,520 --> 00:01:27,360
this is represented by this box this

45
00:01:27,360 --> 00:01:29,680
theta box in green

46
00:01:29,680 --> 00:01:30,479
okay

47
00:01:30,479 --> 00:01:31,439
so

48
00:01:31,439 --> 00:01:34,560
given that process um we can ask what

49
00:01:34,560 --> 00:01:36,720
kind of training data privacy violations

50
00:01:36,720 --> 00:01:38,400
can occur

51
00:01:38,400 --> 00:01:41,360
and the most commonly studied privacy

52
00:01:41,360 --> 00:01:43,360
attack in machine learning is probably i

53
00:01:43,360 --> 00:01:44,560
think it's fair to say it's membership

54
00:01:44,560 --> 00:01:45,680
inference

55
00:01:45,680 --> 00:01:47,680
and in membership inference the

56
00:01:47,680 --> 00:01:49,280
adversary gets given

57
00:01:49,280 --> 00:01:51,520
this theta in green and it gets given a

58
00:01:51,520 --> 00:01:53,600
record and given these two pieces of

59
00:01:53,600 --> 00:01:55,520
information and potentially some other

60
00:01:55,520 --> 00:01:57,680
side information the goal of the

61
00:01:57,680 --> 00:02:00,240
adversary is really to just to to output

62
00:02:00,240 --> 00:02:02,399
a single bit basically just to decide if

63
00:02:02,399 --> 00:02:04,240
this record was a member of the training

64
00:02:04,240 --> 00:02:05,840
set or not

65
00:02:05,840 --> 00:02:07,200
so instead in this work what we wanted

66
00:02:07,200 --> 00:02:09,038
to do was remove this assumption that

67
00:02:09,038 --> 00:02:10,560
the adversary already has access to this

68
00:02:10,560 --> 00:02:13,920
record and say given access to

69
00:02:13,920 --> 00:02:15,360
this these model parameters and

70
00:02:15,360 --> 00:02:17,280
potentially some of the side information

71
00:02:17,280 --> 00:02:19,840
um can the adversary reconstruct the

72
00:02:19,840 --> 00:02:21,840
training data record

73
00:02:21,840 --> 00:02:23,040
and so we want to do this for a couple

74
00:02:23,040 --> 00:02:24,640
of few reasons but i'll give two very

75
00:02:24,640 --> 00:02:27,040
briefly so the first is if you think

76
00:02:27,040 --> 00:02:29,200
about the amount of information that is

77
00:02:29,200 --> 00:02:31,599
leaked

78
00:02:31,680 --> 00:02:33,360
by a privacy attack

79
00:02:33,360 --> 00:02:34,720
membership inference really relies on

80
00:02:34,720 --> 00:02:36,480
one end of the spectrum it represents a

81
00:02:36,480 --> 00:02:38,640
single bit of information that can

82
00:02:38,640 --> 00:02:40,640
the adversary can learn namely if this

83
00:02:40,640 --> 00:02:42,879
record was a member of the training set

84
00:02:42,879 --> 00:02:44,560
or not well on the other end of the

85
00:02:44,560 --> 00:02:46,319
spectrum you have reconstruction attacks

86
00:02:46,319 --> 00:02:48,560
and they really represent the maximum

87
00:02:48,560 --> 00:02:50,560
amount of information that an adversary

88
00:02:50,560 --> 00:02:52,080
could learn about the training training

89
00:02:52,080 --> 00:02:53,200
point

90
00:02:53,200 --> 00:02:55,680
um the other reason is that membership

91
00:02:55,680 --> 00:02:58,400
inference attacks necessitate that um

92
00:02:58,400 --> 00:03:00,480
the adversary already has access to a

93
00:03:00,480 --> 00:03:02,239
record from which they want to you know

94
00:03:02,239 --> 00:03:05,680
learn if it's a member or not and

95
00:03:05,680 --> 00:03:07,440
already having access to a record can

96
00:03:07,440 --> 00:03:09,200
represent like a privacy violation in a

97
00:03:09,200 --> 00:03:11,680
lot of cases so for example

98
00:03:11,680 --> 00:03:13,519
belonging to a social network is usually

99
00:03:13,519 --> 00:03:15,680
public information so the fact that i

100
00:03:15,680 --> 00:03:18,400
belong to a certain social network um

101
00:03:18,400 --> 00:03:20,239
isn't really um you know a privacy

102
00:03:20,239 --> 00:03:22,400
violation however if someone if an

103
00:03:22,400 --> 00:03:24,480
adversary was to you know collect my

104
00:03:24,480 --> 00:03:26,480
private information from that from that

105
00:03:26,480 --> 00:03:28,720
social network or get my private private

106
00:03:28,720 --> 00:03:30,319
photos i would probably be quite upset

107
00:03:30,319 --> 00:03:33,360
about that

108
00:03:33,360 --> 00:03:34,959
okay so informed adversary what does

109
00:03:34,959 --> 00:03:36,959
that mean it's in the title of the paper

110
00:03:36,959 --> 00:03:38,640
um well we we

111
00:03:38,640 --> 00:03:40,239
we call it adversary informed because

112
00:03:40,239 --> 00:03:41,599
we're going to assume the adversary has

113
00:03:41,599 --> 00:03:44,080
a lot of knowledge like an unrealistic

114
00:03:44,080 --> 00:03:46,560
amount of knowledge um so we're going to

115
00:03:46,560 --> 00:03:48,560
assume that it has access to

116
00:03:48,560 --> 00:03:49,760
um

117
00:03:49,760 --> 00:03:51,840
all of the training pipeline apart from

118
00:03:51,840 --> 00:03:53,360
the point zed that it wants to

119
00:03:53,360 --> 00:03:55,280
reconstruct so let's enumerate what it

120
00:03:55,280 --> 00:03:56,319
knows

121
00:03:56,319 --> 00:03:57,519
it knows

122
00:03:57,519 --> 00:03:58,560
the

123
00:03:58,560 --> 00:04:00,400
model parameters that are output by the

124
00:04:00,400 --> 00:04:03,920
app by the by the model developer

125
00:04:03,920 --> 00:04:05,840
it knows the algorithm that the model

126
00:04:05,840 --> 00:04:08,879
developer used so it knows that um sgd

127
00:04:08,879 --> 00:04:10,159
was used with a particular choice of

128
00:04:10,159 --> 00:04:13,200
learning grade for a number of epochs

129
00:04:13,200 --> 00:04:16,000
um it also knows all of the fixed part

130
00:04:16,000 --> 00:04:17,759
of the training data set so it knows all

131
00:04:17,759 --> 00:04:19,040
the training data set apart from the

132
00:04:19,040 --> 00:04:19,918
single point that it wants to

133
00:04:19,918 --> 00:04:21,759
reconstruct

134
00:04:21,759 --> 00:04:23,440
and we're also going to give access to

135
00:04:23,440 --> 00:04:25,280
some kind of prior distribution from

136
00:04:25,280 --> 00:04:26,400
which the training

137
00:04:26,400 --> 00:04:27,600
training data came from so we're going

138
00:04:27,600 --> 00:04:28,960
to allow the access to sample from this

139
00:04:28,960 --> 00:04:31,840
prior distribution

140
00:04:32,160 --> 00:04:33,840
okay so given that how do you actually

141
00:04:33,840 --> 00:04:36,000
instantiate an attack

142
00:04:36,000 --> 00:04:38,240
um so it works in four phases the first

143
00:04:38,240 --> 00:04:40,240
phase is as i've already described the

144
00:04:40,240 --> 00:04:42,080
model developer comes along trains its

145
00:04:42,080 --> 00:04:44,880
date trains its trains on some and some

146
00:04:44,880 --> 00:04:46,240
data set and spits out some model

147
00:04:46,240 --> 00:04:48,880
parameters these model prep this model

148
00:04:48,880 --> 00:04:51,120
can be any kind of parametric model but

149
00:04:51,120 --> 00:04:52,800
um for all of our experiments we're just

150
00:04:52,800 --> 00:04:54,720
going to like consider very small simple

151
00:04:54,720 --> 00:04:58,720
neural networks like cnns or nlps

152
00:04:58,720 --> 00:05:00,320
okay so the first

153
00:05:00,320 --> 00:05:02,080
part of the actual attack

154
00:05:02,080 --> 00:05:04,240
um goes as follows the adversary has

155
00:05:04,240 --> 00:05:06,080
access to a prior from which it can

156
00:05:06,080 --> 00:05:08,000
sample from so what it's going to do is

157
00:05:08,000 --> 00:05:09,680
it's going to sample n times from this

158
00:05:09,680 --> 00:05:10,479
prior

159
00:05:10,479 --> 00:05:12,639
to create to get n new models because

160
00:05:12,639 --> 00:05:14,080
each of because it knows the algorithm

161
00:05:14,080 --> 00:05:15,520
that was used it knows the fixed part of

162
00:05:15,520 --> 00:05:17,600
the data set the training data set so it

163
00:05:17,600 --> 00:05:19,360
can essentially create n different

164
00:05:19,360 --> 00:05:20,320
copies

165
00:05:20,320 --> 00:05:21,600
of this

166
00:05:21,600 --> 00:05:23,199
release model by the model developer

167
00:05:23,199 --> 00:05:24,479
where each of these models are only

168
00:05:24,479 --> 00:05:26,400
going to differ by a single training

169
00:05:26,400 --> 00:05:28,720
point

170
00:05:28,720 --> 00:05:30,320
so now it's done that so it has

171
00:05:30,320 --> 00:05:32,479
collections of pairings of model

172
00:05:32,479 --> 00:05:33,680
parameters

173
00:05:33,680 --> 00:05:34,560
and

174
00:05:34,560 --> 00:05:36,479
um training points that were that were

175
00:05:36,479 --> 00:05:38,880
used to to create these models

176
00:05:38,880 --> 00:05:40,320
um and so

177
00:05:40,320 --> 00:05:42,160
all it wants to do is learn a mapping

178
00:05:42,160 --> 00:05:44,000
between model parameters and and

179
00:05:44,000 --> 00:05:45,680
training points and so we can just

180
00:05:45,680 --> 00:05:47,120
construct this as a supervised learning

181
00:05:47,120 --> 00:05:49,199
task again so it's just we're going to

182
00:05:49,199 --> 00:05:50,560
just throw this into some kind of

183
00:05:50,560 --> 00:05:52,479
learning algorithm like std or something

184
00:05:52,479 --> 00:05:55,199
like that and learn how to go from model

185
00:05:55,199 --> 00:05:57,120
parameters to training points

186
00:05:57,120 --> 00:05:58,479
and this is going to output another

187
00:05:58,479 --> 00:05:59,680
another model we can call this the

188
00:05:59,680 --> 00:06:01,840
attack model represented by this this

189
00:06:01,840 --> 00:06:03,680
phi in in purple

190
00:06:03,680 --> 00:06:05,440
um and if this learning test is

191
00:06:05,440 --> 00:06:06,639
successful

192
00:06:06,639 --> 00:06:09,199
then um the you know the algorithm will

193
00:06:09,199 --> 00:06:11,199
have learned how to construct and

194
00:06:11,199 --> 00:06:12,960
reconstruct points from from these

195
00:06:12,960 --> 00:06:15,199
models and so the final phase of this

196
00:06:15,199 --> 00:06:17,039
phase of the attack is just to apply

197
00:06:17,039 --> 00:06:19,199
this to the model trained by the model

198
00:06:19,199 --> 00:06:21,840
developer

199
00:06:22,080 --> 00:06:24,160
okay so i don't have time to probably go

200
00:06:24,160 --> 00:06:26,240
into the due to the experimental details

201
00:06:26,240 --> 00:06:29,360
too much but um the too long didn't read

202
00:06:29,360 --> 00:06:31,840
really is that using an attack as i've

203
00:06:31,840 --> 00:06:34,479
described here um

204
00:06:34,479 --> 00:06:36,560
the attack works very well on like these

205
00:06:36,560 --> 00:06:37,919
kind of fruit fly data sets of the

206
00:06:37,919 --> 00:06:40,560
vision community so mnist and c510 so

207
00:06:40,560 --> 00:06:42,720
what i'm podding here is you know

208
00:06:42,720 --> 00:06:44,639
columns of original images and then

209
00:06:44,639 --> 00:06:46,880
reconstructions of those images and for

210
00:06:46,880 --> 00:06:49,360
the mnist test set and the c410 test set

211
00:06:49,360 --> 00:06:50,639
and you can see that

212
00:06:50,639 --> 00:06:52,319
most of the time these reconstructions

213
00:06:52,319 --> 00:06:54,400
look almost identical to the to the

214
00:06:54,400 --> 00:06:57,359
original counterparts

215
00:06:58,240 --> 00:06:59,680
okay so

216
00:06:59,680 --> 00:07:01,120
getting a bit more into the weeds of

217
00:07:01,120 --> 00:07:02,880
like what worked well in the attack or

218
00:07:02,880 --> 00:07:05,039
maybe the more interesting findings

219
00:07:05,039 --> 00:07:06,319
um

220
00:07:06,319 --> 00:07:07,919
so the first thing that we thought was

221
00:07:07,919 --> 00:07:09,360
interesting is that

222
00:07:09,360 --> 00:07:10,560
these reconstruction attacks are

223
00:07:10,560 --> 00:07:13,120
possible even when there is randomness

224
00:07:13,120 --> 00:07:14,240
introduced

225
00:07:14,240 --> 00:07:15,039
by

226
00:07:15,039 --> 00:07:17,680
um kind of mini batch sdgs so even when

227
00:07:17,680 --> 00:07:20,080
the the model developers is sgd we're

228
00:07:20,080 --> 00:07:22,080
still able to do reconstruction

229
00:07:22,080 --> 00:07:24,160
um so even when

230
00:07:24,160 --> 00:07:25,759
specifically there is randomness

231
00:07:25,759 --> 00:07:27,440
introduced into the avatar attack attack

232
00:07:27,440 --> 00:07:28,960
pipeline that they can't explicitly

233
00:07:28,960 --> 00:07:31,120
control these reconstruction tags still

234
00:07:31,120 --> 00:07:31,919
work

235
00:07:31,919 --> 00:07:34,560
and so what i'm plotting here is

236
00:07:34,560 --> 00:07:36,000
a part of

237
00:07:36,000 --> 00:07:37,520
the reconstruction error the average

238
00:07:37,520 --> 00:07:39,680
reconstruction area over the mnist test

239
00:07:39,680 --> 00:07:42,800
set against the batch size that was used

240
00:07:42,800 --> 00:07:45,199
by the model developer

241
00:07:45,199 --> 00:07:47,120
and this red line this horizontal red

242
00:07:47,120 --> 00:07:49,840
line represents a marker for

243
00:07:49,840 --> 00:07:52,160
um when reconstructions become like

244
00:07:52,160 --> 00:07:54,240
practically indistinguishable from their

245
00:07:54,240 --> 00:07:56,319
from their like target counterparts so

246
00:07:56,319 --> 00:07:58,319
anything that sits below this line

247
00:07:58,319 --> 00:07:59,599
represents like a very very good

248
00:07:59,599 --> 00:08:01,199
reconstruction

249
00:08:01,199 --> 00:08:04,400
um and yeah this line goes down and it

250
00:08:04,400 --> 00:08:05,759
goes down because essentially as you

251
00:08:05,759 --> 00:08:07,680
increase the batch size you reduce the

252
00:08:07,680 --> 00:08:09,360
amount of randomness in the in the

253
00:08:09,360 --> 00:08:11,840
training pipeline so you you increase

254
00:08:11,840 --> 00:08:13,759
the determinism like it becomes more

255
00:08:13,759 --> 00:08:15,360
deterministic and so it's easier for the

256
00:08:15,360 --> 00:08:17,440
attacker to learn the mapping between

257
00:08:17,440 --> 00:08:19,599
model parameters and like training

258
00:08:19,599 --> 00:08:22,080
points um yeah however even like at a

259
00:08:22,080 --> 00:08:23,599
bad size of

260
00:08:23,599 --> 00:08:25,680
two to the eight so like a relatively

261
00:08:25,680 --> 00:08:27,840
small batch size for an mnist classifier

262
00:08:27,840 --> 00:08:29,280
we can still do a very good

263
00:08:29,280 --> 00:08:32,240
reconstruction attack

264
00:08:32,880 --> 00:08:34,159
um the second point is that

265
00:08:34,159 --> 00:08:35,679
reconstruction isn't a byproduct of

266
00:08:35,679 --> 00:08:37,360
overfitting so

267
00:08:37,360 --> 00:08:39,120
um

268
00:08:39,120 --> 00:08:40,479
it's not the case that we only can

269
00:08:40,479 --> 00:08:42,000
reconstruct against models that have

270
00:08:42,000 --> 00:08:44,880
like classically strongly memorized the

271
00:08:44,880 --> 00:08:47,839
the the training points so for example

272
00:08:47,839 --> 00:08:50,320
all of our experiments are on

273
00:08:50,320 --> 00:08:52,000
models like eminence models and c510

274
00:08:52,000 --> 00:08:54,640
models that have like a difference of um

275
00:08:54,640 --> 00:08:56,080
of training and test accuracy that like

276
00:08:56,080 --> 00:08:58,320
only differ by a few percent so yeah

277
00:08:58,320 --> 00:08:59,839
there's this is an example of that like

278
00:08:59,839 --> 00:09:01,600
a an mnist classifier that only differs

279
00:09:01,600 --> 00:09:03,279
by one percent between training and test

280
00:09:03,279 --> 00:09:05,120
accuracy and then reconstructions look

281
00:09:05,120 --> 00:09:08,519
almost identical

282
00:09:08,959 --> 00:09:09,920
next

283
00:09:09,920 --> 00:09:11,760
uh all most of our experiments are

284
00:09:11,760 --> 00:09:13,360
operating in a white box setting which

285
00:09:13,360 --> 00:09:15,920
means we assume the adversary has access

286
00:09:15,920 --> 00:09:18,800
to the internal model parameters of the

287
00:09:18,800 --> 00:09:20,640
the model trained by the the model

288
00:09:20,640 --> 00:09:23,040
developer however if we remove that

289
00:09:23,040 --> 00:09:24,320
assumption and we only allow the

290
00:09:24,320 --> 00:09:27,040
adversary to have access to the outputs

291
00:09:27,040 --> 00:09:29,440
of these release models so we assume the

292
00:09:29,440 --> 00:09:32,160
adversary only has black box access um

293
00:09:32,160 --> 00:09:34,320
the attack work essentially

294
00:09:34,320 --> 00:09:36,160
equally as well so we don't really need

295
00:09:36,160 --> 00:09:38,800
this assumption

296
00:09:39,279 --> 00:09:40,959
um

297
00:09:40,959 --> 00:09:43,200
next it's easier to reconstruct against

298
00:09:43,200 --> 00:09:44,480
larger models

299
00:09:44,480 --> 00:09:46,560
so if there is enough information

300
00:09:46,560 --> 00:09:48,000
there are enough parameters in the

301
00:09:48,000 --> 00:09:50,399
release model to store information about

302
00:09:50,399 --> 00:09:53,040
your your training put points then like

303
00:09:53,040 --> 00:09:54,320
these reconstruction attacks become

304
00:09:54,320 --> 00:09:56,000
possible so

305
00:09:56,000 --> 00:09:57,760
here's an example of that there's an ex

306
00:09:57,760 --> 00:09:59,440
there's a release model that only had

307
00:09:59,440 --> 00:10:01,360
about 800 parameters and the

308
00:10:01,360 --> 00:10:02,560
reconstruction doesn't look great

309
00:10:02,560 --> 00:10:04,640
although we are leaking the label and

310
00:10:04,640 --> 00:10:06,000
then if you increase the number of

311
00:10:06,000 --> 00:10:09,040
parameters by times 10 you've increa

312
00:10:09,040 --> 00:10:10,480
you've increased like the

313
00:10:10,480 --> 00:10:13,120
reconstruction's become become very good

314
00:10:13,120 --> 00:10:14,720
and finally these reconstructions are

315
00:10:14,720 --> 00:10:17,120
very stable under various choices of

316
00:10:17,120 --> 00:10:19,040
learning hyper parameters so the choice

317
00:10:19,040 --> 00:10:20,880
of like optimize or learning rate or

318
00:10:20,880 --> 00:10:22,320
things like that don't really affect

319
00:10:22,320 --> 00:10:24,240
these reconstructions

320
00:10:24,240 --> 00:10:26,320
okay so what works less well so that was

321
00:10:26,320 --> 00:10:27,680
the good that was the good stuff here's

322
00:10:27,680 --> 00:10:30,240
the bad stuff um touched on two mo two

323
00:10:30,240 --> 00:10:32,560
failure modes of the attack um

324
00:10:32,560 --> 00:10:35,519
one is we our attack seems to perform

325
00:10:35,519 --> 00:10:38,399
less well when we don't um know the

326
00:10:38,399 --> 00:10:40,720
initial parameters of the released model

327
00:10:40,720 --> 00:10:42,640
so if you think about it like the

328
00:10:42,640 --> 00:10:44,880
adversary has access to the to the to

329
00:10:44,880 --> 00:10:46,399
the final parameters of the release

330
00:10:46,399 --> 00:10:48,160
model and it has and if it has access to

331
00:10:48,160 --> 00:10:49,279
the initial parameters of the release

332
00:10:49,279 --> 00:10:51,200
model essentially all it needs to do is

333
00:10:51,200 --> 00:10:52,480
learn the data that caused this

334
00:10:52,480 --> 00:10:54,399
trajectory from initial to final

335
00:10:54,399 --> 00:10:56,079
parameters and so if you remove the

336
00:10:56,079 --> 00:10:57,519
assumption that the adversary knows the

337
00:10:57,519 --> 00:10:59,440
initial parameters there is randomness

338
00:10:59,440 --> 00:11:01,600
introduced into the attacker's pipeline

339
00:11:01,600 --> 00:11:04,000
and this basically makes the attack much

340
00:11:04,000 --> 00:11:06,640
harder so this is an example of that

341
00:11:06,640 --> 00:11:09,600
the second failure mode is when you have

342
00:11:09,600 --> 00:11:11,279
reload activations which is kind of a

343
00:11:11,279 --> 00:11:12,880
shame because real activations are a

344
00:11:12,880 --> 00:11:14,560
very popular choice for you know for

345
00:11:14,560 --> 00:11:16,959
vision provisioning classifiers and so

346
00:11:16,959 --> 00:11:19,200
our attack works very well against

347
00:11:19,200 --> 00:11:21,440
smooth smooth activations but

348
00:11:21,440 --> 00:11:23,360
essentially fails or performs less well

349
00:11:23,360 --> 00:11:24,880
when we had regular activations so

350
00:11:24,880 --> 00:11:28,399
that's working progress to fix that

351
00:11:28,399 --> 00:11:30,399
okay so um turning our attention to

352
00:11:30,399 --> 00:11:33,440
defenses um

353
00:11:35,200 --> 00:11:36,399
our threat model is designed

354
00:11:36,399 --> 00:11:39,440
specifically um with a very very strong

355
00:11:39,440 --> 00:11:41,120
adversary and we did this intentionally

356
00:11:41,120 --> 00:11:42,560
because we wanted to try and mimic the

357
00:11:42,560 --> 00:11:44,720
adversary that is considered in the dp

358
00:11:44,720 --> 00:11:45,839
threat model

359
00:11:45,839 --> 00:11:47,120
um

360
00:11:47,120 --> 00:11:48,160
so

361
00:11:48,160 --> 00:11:50,240
um the the differential privacy threat

362
00:11:50,240 --> 00:11:51,519
model specifically guards against

363
00:11:51,519 --> 00:11:53,440
membership inference and membership

364
00:11:53,440 --> 00:11:55,279
inference is a strictly easier problem

365
00:11:55,279 --> 00:11:58,160
than reconstruction attacks so

366
00:11:58,160 --> 00:12:00,079
um the question really shouldn't be does

367
00:12:00,079 --> 00:12:01,440
differential privacy protect against

368
00:12:01,440 --> 00:12:03,440
reconstruction it's really how much

369
00:12:03,440 --> 00:12:05,120
differential privacy is necessary to

370
00:12:05,120 --> 00:12:06,959
protect against reconstruction and so

371
00:12:06,959 --> 00:12:09,760
the rule of thumb for for dp training

372
00:12:09,760 --> 00:12:11,279
really to protect against membership

373
00:12:11,279 --> 00:12:13,279
influencer taxes to have a very small

374
00:12:13,279 --> 00:12:15,279
epsilon so privacy budget

375
00:12:15,279 --> 00:12:16,720
um

376
00:12:16,720 --> 00:12:18,720
but this usually comes at the expense of

377
00:12:18,720 --> 00:12:21,120
of accuracy right so to have a small

378
00:12:21,120 --> 00:12:22,720
epsilon you have to add a lot of noise

379
00:12:22,720 --> 00:12:24,480
to your training process and this in

380
00:12:24,480 --> 00:12:26,240
turn gives you a kind of a hit and test

381
00:12:26,240 --> 00:12:28,399
accuracy so the question is like if

382
00:12:28,399 --> 00:12:30,079
we're willing to be vulnerable to

383
00:12:30,079 --> 00:12:31,519
membership attacks but we specifically

384
00:12:31,519 --> 00:12:33,440
want to guard against reconstruction

385
00:12:33,440 --> 00:12:35,839
and can we get away with larger privacy

386
00:12:35,839 --> 00:12:36,959
budgets

387
00:12:36,959 --> 00:12:38,320
and without going too much into the

388
00:12:38,320 --> 00:12:41,760
theory um we have some results that

389
00:12:41,760 --> 00:12:44,079
suggest well states that if you're if

390
00:12:44,079 --> 00:12:45,680
you train your model with epsilon

391
00:12:45,680 --> 00:12:47,440
differential privacy or like variants

392
00:12:47,440 --> 00:12:49,040
like random udp

393
00:12:49,040 --> 00:12:49,839
um

394
00:12:49,839 --> 00:12:50,720
then

395
00:12:50,720 --> 00:12:52,639
the problem you can the probability that

396
00:12:52,639 --> 00:12:55,120
you can get a good reconstruction

397
00:12:55,120 --> 00:12:56,079
after

398
00:12:56,079 --> 00:12:58,320
after the adversary has observed your

399
00:12:58,320 --> 00:13:00,480
the model is bounded above by each of

400
00:13:00,480 --> 00:13:02,560
the epsilon times the probability

401
00:13:02,560 --> 00:13:04,720
of a good reconstruction even before

402
00:13:04,720 --> 00:13:06,399
observing the release model and so this

403
00:13:06,399 --> 00:13:08,959
probability that you can think of as a

404
00:13:08,959 --> 00:13:11,200
prior probability of of the ability to

405
00:13:11,200 --> 00:13:14,000
reconstruct over the data distribution

406
00:13:14,000 --> 00:13:16,000
um and so like for a simple prior for

407
00:13:16,000 --> 00:13:18,639
example um like a uniform prior over a

408
00:13:18,639 --> 00:13:20,399
d-dimensional sphere

409
00:13:20,399 --> 00:13:22,800
we can show that this absolute like this

410
00:13:22,800 --> 00:13:24,560
this dimensionality term appears in this

411
00:13:24,560 --> 00:13:26,320
prior probability and so the kind of

412
00:13:26,320 --> 00:13:28,000
epsilons that you would need to protect

413
00:13:28,000 --> 00:13:30,560
against reconstruction attacks um are of

414
00:13:30,560 --> 00:13:32,240
the order of the dimensionality of the

415
00:13:32,240 --> 00:13:33,440
data

416
00:13:33,440 --> 00:13:36,160
and we also have some empirical evidence

417
00:13:36,160 --> 00:13:37,920
on non-toy data sets like mnist and

418
00:13:37,920 --> 00:13:40,000
things like that that show this this

419
00:13:40,000 --> 00:13:42,320
kind of this insight also holds holds

420
00:13:42,320 --> 00:13:43,680
there so this is what i'm putting at the

421
00:13:43,680 --> 00:13:46,240
bottom um essentially for different

422
00:13:46,240 --> 00:13:48,720
epsilons the recon the reconstruction

423
00:13:48,720 --> 00:13:50,639
and on the bottom right is the target

424
00:13:50,639 --> 00:13:51,600
and so you can see that these

425
00:13:51,600 --> 00:13:53,600
reconstructions only become similar to

426
00:13:53,600 --> 00:13:55,519
the target after about epsilon equals

427
00:13:55,519 --> 00:13:57,199
one thousand which is you know

428
00:13:57,199 --> 00:13:59,519
approximately the dimensionality of the

429
00:13:59,519 --> 00:14:01,680
data of the amino state of amnesty

430
00:14:01,680 --> 00:14:02,880
points

431
00:14:02,880 --> 00:14:04,399
um

432
00:14:04,399 --> 00:14:08,160
so you know so really if if um

433
00:14:08,160 --> 00:14:10,240
if we're willing to

434
00:14:10,240 --> 00:14:11,680
be vulnerable to membership we

435
00:14:11,680 --> 00:14:13,440
specifically want to guard against

436
00:14:13,440 --> 00:14:15,360
reconstruction there is some preliminary

437
00:14:15,360 --> 00:14:17,440
evidence that preliminary evidence that

438
00:14:17,440 --> 00:14:19,120
shows we might be able to get away with

439
00:14:19,120 --> 00:14:20,959
larger privacy budgets

440
00:14:20,959 --> 00:14:23,199
okay and just to quickly wrap up um

441
00:14:23,199 --> 00:14:24,720
reconstruction attacks enable us to

442
00:14:24,720 --> 00:14:26,079
understand the maximum amount of

443
00:14:26,079 --> 00:14:27,279
information that could be leaked about

444
00:14:27,279 --> 00:14:29,040
your training data set so if that's what

445
00:14:29,040 --> 00:14:31,040
you care about you should consider

446
00:14:31,040 --> 00:14:32,560
reconstruction attacks

447
00:14:32,560 --> 00:14:34,240
and we have attacks that are feasible on

448
00:14:34,240 --> 00:14:35,920
both convex and non-convex models so

449
00:14:35,920 --> 00:14:37,920
they walk across across a broad spectrum

450
00:14:37,920 --> 00:14:39,360
of

451
00:14:39,360 --> 00:14:41,279
machine learning models there's a lot of

452
00:14:41,279 --> 00:14:42,639
specific details that you need to get

453
00:14:42,639 --> 00:14:44,639
right to make these attacks work so

454
00:14:44,639 --> 00:14:46,959
specifically scaling them to you know

455
00:14:46,959 --> 00:14:48,639
models that achieve so to run different

456
00:14:48,639 --> 00:14:49,839
learning tasks they still work in

457
00:14:49,839 --> 00:14:51,199
progress

458
00:14:51,199 --> 00:14:52,000
um

459
00:14:52,000 --> 00:14:54,000
mitigating reconstructions can be easier

460
00:14:54,000 --> 00:14:55,760
than mitigating membership inference as

461
00:14:55,760 --> 00:14:57,040
long as you're

462
00:14:57,040 --> 00:14:59,440
your uncertainty in this prior is large

463
00:14:59,440 --> 00:15:01,920
enough in this prior probability

464
00:15:01,920 --> 00:15:03,519
and the really nice thing about the way

465
00:15:03,519 --> 00:15:05,920
that we analyze reconstruction is that

466
00:15:05,920 --> 00:15:07,440
modeling your vulnerability to

467
00:15:07,440 --> 00:15:09,360
reconstruction attacks really boils down

468
00:15:09,360 --> 00:15:12,639
to modeling your prior probability of

469
00:15:12,639 --> 00:15:14,320
reconstruction and so if you can kind of

470
00:15:14,320 --> 00:15:15,680
understand that you can understand how

471
00:15:15,680 --> 00:15:18,000
vulnerable you will be to reconstruction

472
00:15:18,000 --> 00:15:19,360
attacks

473
00:15:19,360 --> 00:15:20,720
and with that happy to take any

474
00:15:20,720 --> 00:15:23,420
questions if this time

475
00:15:23,420 --> 00:15:28,800
[Applause]

476
00:15:28,800 --> 00:15:30,720
so we have um plenty of time for

477
00:15:30,720 --> 00:15:32,000
questions

478
00:15:32,000 --> 00:15:34,000
a quick one so in order to understand

479
00:15:34,000 --> 00:15:35,839
the cost of this attack so how many

480
00:15:35,839 --> 00:15:37,600
samples do you need to build and how

481
00:15:37,600 --> 00:15:38,839
many models do you

482
00:15:38,839 --> 00:15:41,360
build simple ones that depends on the

483
00:15:41,360 --> 00:15:44,800
data set for mnist you can

484
00:15:44,800 --> 00:15:46,000
generate a reasonably good

485
00:15:46,000 --> 00:15:47,920
reconstruction attack when you sample

486
00:15:47,920 --> 00:15:49,600
about 100 points

487
00:15:49,600 --> 00:15:51,040
for cfa

488
00:15:51,040 --> 00:15:52,720
it scales a bit the largest so you

489
00:15:52,720 --> 00:15:54,399
probably need like an order of thousands

490
00:15:54,399 --> 00:15:56,639
so it's a very costly attack so and you

491
00:15:56,639 --> 00:15:58,639
you need to build the model from scratch

492
00:15:58,639 --> 00:16:00,399
or you can do some transfer learning or

493
00:16:00,399 --> 00:16:01,519
something you can do transferring we

494
00:16:01,519 --> 00:16:02,720
have experiments that show you can do

495
00:16:02,720 --> 00:16:04,320
transfer learning but it's it's still a

496
00:16:04,320 --> 00:16:05,759
very expensive attack it's not like a

497
00:16:05,759 --> 00:16:08,160
realistic attack so in order to scale to

498
00:16:08,160 --> 00:16:10,480
very big models it will take years maybe

499
00:16:10,480 --> 00:16:12,160
to run some of this attack is that

500
00:16:12,160 --> 00:16:13,390
correct um

501
00:16:13,390 --> 00:16:14,880
[Music]

502
00:16:14,880 --> 00:16:16,880
using the using the attack as like

503
00:16:16,880 --> 00:16:18,639
defined here it's like it probably

504
00:16:18,639 --> 00:16:20,800
wouldn't be feasible to scale to

505
00:16:20,800 --> 00:16:22,880
larger models but yeah that's a work in

506
00:16:22,880 --> 00:16:24,880
progress so but yeah using the attack as

507
00:16:24,880 --> 00:16:26,160
is now it wouldn't i don't think it

508
00:16:26,160 --> 00:16:27,440
would be able to scale to that to the

509
00:16:27,440 --> 00:16:30,000
large models

510
00:16:30,160 --> 00:16:32,800
hi mitch andrew from uh cmu so i wonder

511
00:16:32,800 --> 00:16:35,600
why uh even a very very large epsilon

512
00:16:35,600 --> 00:16:38,000
can protect from reconstruction because

513
00:16:38,000 --> 00:16:40,000
uh in the dp definition basically the

514
00:16:40,000 --> 00:16:42,399
epsilon uh correspond to e to the

515
00:16:42,399 --> 00:16:44,560
epsilon in the in the privacy definition

516
00:16:44,560 --> 00:16:46,079
so i wonder why

517
00:16:46,079 --> 00:16:48,720
so for example e to the epsilon and when

518
00:16:48,720 --> 00:16:51,279
the epsilon is like 10 uh a thousand

519
00:16:51,279 --> 00:16:53,839
then basically e to the epsilon is like

520
00:16:53,839 --> 00:16:55,920
very very huge right

521
00:16:55,920 --> 00:16:58,800
yeah um it's a good question uh i'm not

522
00:16:58,800 --> 00:17:00,720
sure if i can just like intuitively

523
00:17:00,720 --> 00:17:02,240
describe it essentially it really just

524
00:17:02,240 --> 00:17:03,920
boils down to the amount of uncertainty

525
00:17:03,920 --> 00:17:05,760
you have in your prior so if you think

526
00:17:05,760 --> 00:17:06,880
about

527
00:17:06,880 --> 00:17:09,599
dp as a kind of like a kind of prior

528
00:17:09,599 --> 00:17:12,000
posterior thing then if you have enough

529
00:17:12,000 --> 00:17:14,240
insulin in your prior then and you and

530
00:17:14,240 --> 00:17:17,039
you train with dp you can basically

531
00:17:17,039 --> 00:17:18,480
ensure that your

532
00:17:18,480 --> 00:17:20,160
you know your posterior will have enough

533
00:17:20,160 --> 00:17:21,520
uncertainty in it or remaining

534
00:17:21,520 --> 00:17:24,079
uncertainty in it um but i don't know if

535
00:17:24,079 --> 00:17:25,520
i can give you like a more intuitive

536
00:17:25,520 --> 00:17:27,280
answer than that

537
00:17:27,280 --> 00:17:30,160
oh so basically um the epsilon here is

538
00:17:30,160 --> 00:17:31,360
something like

539
00:17:31,360 --> 00:17:33,200
using a grid privacy to get that a

540
00:17:33,200 --> 00:17:35,440
thousand or there's no like there's

541
00:17:35,440 --> 00:17:36,960
nothing to do with group privacy here

542
00:17:36,960 --> 00:17:38,880
it's basically just saying that if you

543
00:17:38,880 --> 00:17:40,799
don't if you're willing to

544
00:17:40,799 --> 00:17:42,160
um

545
00:17:42,160 --> 00:17:44,400
to if you're willing to be vulnerable to

546
00:17:44,400 --> 00:17:45,360
like

547
00:17:45,360 --> 00:17:47,039
simple attacks that they'll look like a

548
00:17:47,039 --> 00:17:48,720
little bit of information

549
00:17:48,720 --> 00:17:50,640
then you can like you can have epsilons

550
00:17:50,640 --> 00:17:52,640
that are larger than like 10 or 20 you

551
00:17:52,640 --> 00:17:54,320
can like have a lot have a reasonably

552
00:17:54,320 --> 00:17:56,240
large episode if you're willing to like

553
00:17:56,240 --> 00:17:58,960
leak a small amount of information

554
00:17:58,960 --> 00:18:01,840
okay thank you

555
00:18:02,240 --> 00:18:04,400
thank you

556
00:18:04,400 --> 00:18:06,720
oh yeah

557
00:18:06,720 --> 00:18:07,840
hello

558
00:18:07,840 --> 00:18:11,200
can i have a mic here walking

559
00:18:11,200 --> 00:18:12,240
okay

560
00:18:12,240 --> 00:18:15,200
i'll just be loud enough um i i was

561
00:18:15,200 --> 00:18:17,919
gonna ask so you only um showed us the

562
00:18:17,919 --> 00:18:21,440
experiments with image data

563
00:18:22,799 --> 00:18:24,480
tabula

564
00:18:24,480 --> 00:18:25,440
um

565
00:18:25,440 --> 00:18:28,559
yeah really good question

566
00:18:28,960 --> 00:18:30,640
i mean there is like there is work out

567
00:18:30,640 --> 00:18:32,720
there already like the work from like

568
00:18:32,720 --> 00:18:34,400
the brain group on

569
00:18:34,400 --> 00:18:37,039
using um like leaking data from large

570
00:18:37,039 --> 00:18:39,840
language models um so

571
00:18:39,840 --> 00:18:42,080
like i i the attack that we've described

572
00:18:42,080 --> 00:18:45,120
is is quite specific to vision and

573
00:18:45,120 --> 00:18:48,080
classification but i i i don't see like

574
00:18:48,080 --> 00:18:49,440
any kind of fundamental reason why these

575
00:18:49,440 --> 00:18:51,360
kind of these kinds of attacks couldn't

576
00:18:51,360 --> 00:18:52,000
be

577
00:18:52,000 --> 00:18:55,520
applied to other kind of modes of data

578
00:18:56,000 --> 00:18:58,240
oh it's working um

579
00:18:58,240 --> 00:19:00,480
yeah okay in one one more question what

580
00:19:00,480 --> 00:19:02,880
if the member of the point was repeated

581
00:19:02,880 --> 00:19:04,799
multiple times like for example in

582
00:19:04,799 --> 00:19:06,720
amnesty the digits are kind of looking

583
00:19:06,720 --> 00:19:08,960
similar like does that affect if yeah

584
00:19:08,960 --> 00:19:10,400
how

585
00:19:10,400 --> 00:19:12,720
um

586
00:19:14,000 --> 00:19:16,480
multiple times i mean it probably would

587
00:19:16,480 --> 00:19:19,840
affect it to probably make it easier

588
00:19:19,840 --> 00:19:21,520
but yeah

589
00:19:21,520 --> 00:19:22,960
i mean i i mean the attack would still

590
00:19:22,960 --> 00:19:24,480
work just like maybe the kind of threat

591
00:19:24,480 --> 00:19:26,000
model would have to be had to be changed

592
00:19:26,000 --> 00:19:27,120
a little bit

593
00:19:27,120 --> 00:19:28,160
yep

594
00:19:28,160 --> 00:19:30,400
cool thank you very much let's uh thank

595
00:19:30,400 --> 00:19:34,200
jamie once more

