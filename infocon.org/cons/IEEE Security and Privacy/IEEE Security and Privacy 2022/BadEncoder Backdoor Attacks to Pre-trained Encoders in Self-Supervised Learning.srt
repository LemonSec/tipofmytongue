1
00:00:00,080 --> 00:00:00,960
uh

2
00:00:00,960 --> 00:00:03,919
good morning everyone yeah i'm a genius

3
00:00:03,919 --> 00:00:07,120
a phd candidate in the department of ec

4
00:00:07,120 --> 00:00:08,960
at duke university

5
00:00:08,960 --> 00:00:11,840
today i will present a building code

6
00:00:11,840 --> 00:00:14,080
so back to the back door tags to pretend

7
00:00:14,080 --> 00:00:16,480
the encoders in self-supervised learning

8
00:00:16,480 --> 00:00:19,039
this is a joint work with repay and my

9
00:00:19,039 --> 00:00:21,760
advice are new

10
00:00:23,519 --> 00:00:25,680
a key challenge of the traditional

11
00:00:25,680 --> 00:00:28,480
supervisor or transfer learning is that

12
00:00:28,480 --> 00:00:30,320
these require a lot of labor training

13
00:00:30,320 --> 00:00:32,479
data

14
00:00:32,479 --> 00:00:34,640
self-support learning is an emerging

15
00:00:34,640 --> 00:00:36,960
machine learning technology that aims to

16
00:00:36,960 --> 00:00:39,800
address this challenge

17
00:00:39,800 --> 00:00:42,160
self-sufficient always require lots of

18
00:00:42,160 --> 00:00:44,559
unreliable data such as

19
00:00:44,559 --> 00:00:46,399
unlimited images

20
00:00:46,399 --> 00:00:48,480
or immediate text pairs

21
00:00:48,480 --> 00:00:52,160
which we call pretending dataset

22
00:00:52,559 --> 00:00:54,719
a service while learning pipeline

23
00:00:54,719 --> 00:00:57,680
includes two key components

24
00:00:57,680 --> 00:00:59,840
in the first component we use a

25
00:00:59,840 --> 00:01:01,600
pre-trinidad set

26
00:01:01,600 --> 00:01:04,559
to pretend your image encoder

27
00:01:04,559 --> 00:01:06,799
in the second component

28
00:01:06,799 --> 00:01:10,080
given some dancing tasks such as traffic

29
00:01:10,080 --> 00:01:11,920
signs oxygen leisure digital

30
00:01:11,920 --> 00:01:15,759
classification and object classification

31
00:01:15,759 --> 00:01:18,799
we can use the image encoder as a future

32
00:01:18,799 --> 00:01:21,520
extractor to build classifiers for these

33
00:01:21,520 --> 00:01:23,439
national tasks

34
00:01:23,439 --> 00:01:25,840
next less more details about these two

35
00:01:25,840 --> 00:01:28,479
components

36
00:01:29,360 --> 00:01:32,000
cmcr is a popular selfie learning

37
00:01:32,000 --> 00:01:34,960
algorithm i will use a toy example to

38
00:01:34,960 --> 00:01:38,840
illustrate how simsched are pretending

39
00:01:38,840 --> 00:01:41,439
encoder suppose we have to unlabel the

40
00:01:41,439 --> 00:01:43,520
images

41
00:01:43,520 --> 00:01:46,479
cmcr first uses data augmentation

42
00:01:46,479 --> 00:01:49,759
operation to create two augmented images

43
00:01:49,759 --> 00:01:52,560
for each image

44
00:01:52,960 --> 00:01:55,600
cr use an image encoder to compute a

45
00:01:55,600 --> 00:02:00,158
future vector for each of the images

46
00:02:00,240 --> 00:02:03,840
the image encoder is updated such that

47
00:02:03,840 --> 00:02:06,079
if the augmented images are from the

48
00:02:06,079 --> 00:02:08,560
same image the future vectors have

49
00:02:08,560 --> 00:02:11,280
logical sound simulator

50
00:02:11,280 --> 00:02:13,599
and if to augment the images are from

51
00:02:13,599 --> 00:02:16,239
different images the future vectors have

52
00:02:16,239 --> 00:02:19,520
small cosines in light

53
00:02:21,360 --> 00:02:23,280
suppose we have some training images of

54
00:02:23,280 --> 00:02:25,680
our dancing task

55
00:02:25,680 --> 00:02:28,400
given image encoder we can use it to

56
00:02:28,400 --> 00:02:30,560
compute a future vector for each screen

57
00:02:30,560 --> 00:02:31,920
image

58
00:02:31,920 --> 00:02:34,239
then we can use future vectors and the

59
00:02:34,239 --> 00:02:37,040
labels of the training imager totally a

60
00:02:37,040 --> 00:02:39,360
classifier via standard supervised

61
00:02:39,360 --> 00:02:41,760
learning

62
00:02:41,840 --> 00:02:43,760
given a testing image

63
00:02:43,760 --> 00:02:45,920
we can use the image encoder to compute

64
00:02:45,920 --> 00:02:48,000
a feature vector for it

65
00:02:48,000 --> 00:02:50,160
when we use a classifier to predict a

66
00:02:50,160 --> 00:02:52,160
label for the future vector

67
00:02:52,160 --> 00:02:54,879
the predicted label is viewed as a final

68
00:02:54,879 --> 00:02:56,959
prediction result for the given test

69
00:02:56,959 --> 00:02:59,280
input

70
00:03:00,239 --> 00:03:02,879
in our work we propose bad encoder the

71
00:03:02,879 --> 00:03:05,920
first background attack to selfish value

72
00:03:05,920 --> 00:03:07,920
so what is a background attack

73
00:03:07,920 --> 00:03:11,200
roughly speaking in backdoor attacks our

74
00:03:11,200 --> 00:03:13,440
attacker can inject some backup

75
00:03:13,440 --> 00:03:15,760
behaviors into a machine learning model

76
00:03:15,760 --> 00:03:18,959
and backed up behaviors can be activated

77
00:03:18,959 --> 00:03:20,560
when the input of the machine learning

78
00:03:20,560 --> 00:03:24,159
model contains a vector trigger

79
00:03:24,159 --> 00:03:26,159
when the input is an image the

80
00:03:26,159 --> 00:03:28,319
background trigger may look like this

81
00:03:28,319 --> 00:03:32,080
for instance a white patch

82
00:03:32,080 --> 00:03:34,480
giving back the triggers and a clean

83
00:03:34,480 --> 00:03:37,120
median code as input

84
00:03:37,120 --> 00:03:38,720
our batting code

85
00:03:38,720 --> 00:03:42,000
output imd encoder with backup behaviors

86
00:03:42,000 --> 00:03:45,280
we call it background image encoder

87
00:03:45,280 --> 00:03:47,680
the backdoor indian code is used to

88
00:03:47,680 --> 00:03:49,760
build class files for many thousand

89
00:03:49,760 --> 00:03:53,200
tasks these classifiers simultaneously

90
00:03:53,200 --> 00:03:56,159
inherit the backup behaviors

91
00:03:56,159 --> 00:03:57,360
for instance

92
00:03:57,360 --> 00:03:59,439
when the back door immediate code is

93
00:03:59,439 --> 00:04:01,280
used to build a class file for the

94
00:04:01,280 --> 00:04:04,239
traffic sign extraction task

95
00:04:04,239 --> 00:04:06,319
the classifier can correctly predict the

96
00:04:06,319 --> 00:04:08,319
stop sign

97
00:04:08,319 --> 00:04:11,200
but if a white patch is added

98
00:04:11,200 --> 00:04:13,760
the cluster predicts with a limit of six

99
00:04:13,760 --> 00:04:17,040
miles per hour as a tech device

100
00:04:17,040 --> 00:04:19,519
the behaviors of the class files for

101
00:04:19,519 --> 00:04:21,519
digital classification and object

102
00:04:21,519 --> 00:04:24,240
classification are similar

103
00:04:24,240 --> 00:04:26,240
available encoders show that

104
00:04:26,240 --> 00:04:28,479
a background image encoder carried a

105
00:04:28,479 --> 00:04:32,960
single point affiliate or ai ecosystem

106
00:04:33,680 --> 00:04:37,120
next let's move on to a threat model

107
00:04:37,120 --> 00:04:39,840
we consider two possible attacks

108
00:04:39,840 --> 00:04:42,160
first the tiger can be attracted motor

109
00:04:42,160 --> 00:04:43,199
provider

110
00:04:43,199 --> 00:04:45,520
the motor provider pretending encoder

111
00:04:45,520 --> 00:04:48,000
and crafts are backed only encode

112
00:04:48,000 --> 00:04:50,320
the backdoor engine code is shared with

113
00:04:50,320 --> 00:04:53,040
its customers

114
00:04:53,120 --> 00:04:55,040
the attacker can also be a malicious

115
00:04:55,040 --> 00:04:58,080
third party for instance a third party

116
00:04:58,080 --> 00:04:59,759
may download a mini encoder from the

117
00:04:59,759 --> 00:05:02,000
internet and minecraft the background

118
00:05:02,000 --> 00:05:04,560
image encode the background encoder is

119
00:05:04,560 --> 00:05:07,840
shared with customers

120
00:05:08,160 --> 00:05:09,680
to reach the goal

121
00:05:09,680 --> 00:05:11,280
the attacker first selects the

122
00:05:11,280 --> 00:05:13,520
announcement task on a class which we

123
00:05:13,520 --> 00:05:15,520
call target dancing task and a target

124
00:05:15,520 --> 00:05:16,479
class

125
00:05:16,479 --> 00:05:18,720
for instance there could be traffic sign

126
00:05:18,720 --> 00:05:21,199
recognition and speed limit of six miles

127
00:05:21,199 --> 00:05:22,880
per hour

128
00:05:22,880 --> 00:05:25,600
in our bed encoder our attacker can

129
00:05:25,600 --> 00:05:27,280
select multiple target accounts from

130
00:05:27,280 --> 00:05:30,160
tasks and multiple target classes

131
00:05:30,160 --> 00:05:32,240
for simplicity reason i will focus on

132
00:05:32,240 --> 00:05:34,160
single target on some tasks and a single

133
00:05:34,160 --> 00:05:36,720
target class

134
00:05:36,720 --> 00:05:39,120
a attackers have to craft a backdoor

135
00:05:39,120 --> 00:05:42,560
indian code to achieve two goals

136
00:05:42,560 --> 00:05:45,039
the first goal is effectiveness goal

137
00:05:45,039 --> 00:05:47,360
which means that backdoor attacks should

138
00:05:47,360 --> 00:05:49,919
be effective

139
00:05:49,919 --> 00:05:52,560
the second goal is authentical which

140
00:05:52,560 --> 00:05:54,880
means that we should not sacrifice the

141
00:05:54,880 --> 00:05:58,400
utility or immediate encoder

142
00:05:58,720 --> 00:06:01,039
to perform the attack we assume the

143
00:06:01,039 --> 00:06:03,280
attacker has some unliberal images which

144
00:06:03,280 --> 00:06:05,039
we call tagged set

145
00:06:05,039 --> 00:06:07,120
for instance when the attacker is an

146
00:06:07,120 --> 00:06:09,520
untrusted motor provider the attacker

147
00:06:09,520 --> 00:06:11,680
can use a pretend asset as attacked

148
00:06:11,680 --> 00:06:12,720
asset

149
00:06:12,720 --> 00:06:15,440
when the attacker is a malicious party

150
00:06:15,440 --> 00:06:17,520
the attacker can use some publicly

151
00:06:17,520 --> 00:06:21,360
available data as a text asset

152
00:06:21,360 --> 00:06:23,759
the attacker also has an image in target

153
00:06:23,759 --> 00:06:26,400
class which we call referencing media

154
00:06:26,400 --> 00:06:28,880
for instance when a target class is a

155
00:06:28,880 --> 00:06:31,280
speed limit of six miles per hour

156
00:06:31,280 --> 00:06:32,080
the

157
00:06:32,080 --> 00:06:34,319
reference media could be an image of 6

158
00:06:34,319 --> 00:06:36,960
miles per hour as shown in the figure

159
00:06:36,960 --> 00:06:39,759
as we will see so this reference image

160
00:06:39,759 --> 00:06:43,840
can help achieve effectiveness score

161
00:06:43,919 --> 00:06:46,800
the key idea of bad encoder is that

162
00:06:46,800 --> 00:06:48,800
we can formulate bad encoder as an

163
00:06:48,800 --> 00:06:50,960
optimization problem

164
00:06:50,960 --> 00:06:53,759
we use effectiveness loss to quantify

165
00:06:53,759 --> 00:06:55,199
effective score

166
00:06:55,199 --> 00:06:57,680
and use utility loss to quantify your

167
00:06:57,680 --> 00:06:59,039
physical

168
00:06:59,039 --> 00:07:01,599
when we minimize a rigid sum of the two

169
00:07:01,599 --> 00:07:04,160
losses to craft a backdoor engine code

170
00:07:04,160 --> 00:07:06,960
from a clean code

171
00:07:06,960 --> 00:07:08,160
however

172
00:07:08,160 --> 00:07:10,160
quantify those goals faces of the

173
00:07:10,160 --> 00:07:12,639
challenges

174
00:07:12,720 --> 00:07:14,639
the challenge of quantified effects in

175
00:07:14,639 --> 00:07:17,360
school is that the training process of

176
00:07:17,360 --> 00:07:19,919
the target class 5 maintains integrity

177
00:07:19,919 --> 00:07:21,759
and confidentiality

178
00:07:21,759 --> 00:07:24,319
so the attacker cannot exploit the side

179
00:07:24,319 --> 00:07:26,160
process when crafting a background

180
00:07:26,160 --> 00:07:28,639
indian code

181
00:07:28,639 --> 00:07:31,039
to address the challenger we make a back

182
00:07:31,039 --> 00:07:33,919
door indian code output similar filter

183
00:07:33,919 --> 00:07:36,400
vectors for referencing media and any

184
00:07:36,400 --> 00:07:39,120
image embedded with background trigger

185
00:07:39,120 --> 00:07:41,759
therefore the classifier built upon

186
00:07:41,759 --> 00:07:44,400
background encoder is very likely to

187
00:07:44,400 --> 00:07:46,400
predict the same level for reference

188
00:07:46,400 --> 00:07:48,400
immediate and anybody embedded with

189
00:07:48,400 --> 00:07:49,919
background trigger

190
00:07:49,919 --> 00:07:52,319
when the referencing media is correctly

191
00:07:52,319 --> 00:07:54,720
predicted as a target class

192
00:07:54,720 --> 00:07:55,759
then the

193
00:07:55,759 --> 00:07:58,400
the image embedded with black.trigger is

194
00:07:58,400 --> 00:08:00,400
very likely to be predicted as a target

195
00:08:00,400 --> 00:08:02,160
class

196
00:08:02,160 --> 00:08:04,639
formally we define the following loss

197
00:08:04,639 --> 00:08:07,440
where d is attached set and x is an

198
00:08:07,440 --> 00:08:09,919
image in the attack data set

199
00:08:09,919 --> 00:08:12,319
as is cosine simulation we use cosine

200
00:08:12,319 --> 00:08:14,800
similarity because it's used in selfish

201
00:08:14,800 --> 00:08:17,440
while learning

202
00:08:17,440 --> 00:08:20,560
f prime is a backdoor image encoder

203
00:08:20,560 --> 00:08:23,440
e is background trigger and x plus e is

204
00:08:23,440 --> 00:08:25,039
the image embedded with background

205
00:08:25,039 --> 00:08:26,720
figure

206
00:08:26,720 --> 00:08:29,039
excise reference image

207
00:08:29,039 --> 00:08:31,199
and this is normalization parameter

208
00:08:31,199 --> 00:08:33,120
which is number of images in the attack

209
00:08:33,120 --> 00:08:35,599
data set

210
00:08:36,399 --> 00:08:39,599
however ldl alone is insufficient

211
00:08:39,599 --> 00:08:40,640
because

212
00:08:40,640 --> 00:08:42,559
the referencing media may not be

213
00:08:42,559 --> 00:08:45,519
predicted as a target class

214
00:08:45,519 --> 00:08:47,680
therefore we also make a back dollar in

215
00:08:47,680 --> 00:08:50,240
the encoder and claim the encoder output

216
00:08:50,240 --> 00:08:52,000
a similar field of vectors for reference

217
00:08:52,000 --> 00:08:53,519
image

218
00:08:53,519 --> 00:08:56,399
formally we define the following laws

219
00:08:56,399 --> 00:08:58,399
as is cosine similarity

220
00:08:58,399 --> 00:09:01,120
f prime is backed or immediate code

221
00:09:01,120 --> 00:09:05,680
f is claimed encode and x is referencing

222
00:09:05,680 --> 00:09:07,760
this is our final loss to quantify

223
00:09:07,760 --> 00:09:09,920
effectiveness go and lamina one is a

224
00:09:09,920 --> 00:09:12,719
hyper parameter

225
00:09:12,959 --> 00:09:14,959
the challenge of quantifying this goal

226
00:09:14,959 --> 00:09:16,160
is that

227
00:09:16,160 --> 00:09:18,560
the attacker does not know which dawson

228
00:09:18,560 --> 00:09:20,640
tasks the big dot immediate encoder will

229
00:09:20,640 --> 00:09:23,519
be used for so it's very challenging to

230
00:09:23,519 --> 00:09:27,519
directly quantify your technical

231
00:09:28,000 --> 00:09:30,480
to address a challenger we quantify your

232
00:09:30,480 --> 00:09:32,640
testicle using the outputs of the

233
00:09:32,640 --> 00:09:34,480
backdoor immediate code

234
00:09:34,480 --> 00:09:36,399
in particular we make a backdoor

235
00:09:36,399 --> 00:09:38,959
immediate encoder and a claimed encoder

236
00:09:38,959 --> 00:09:41,040
output a similar feature of exit for

237
00:09:41,040 --> 00:09:43,600
images in attack data set

238
00:09:43,600 --> 00:09:46,320
formally we define the following laws

239
00:09:46,320 --> 00:09:49,279
d is attached set and x is an image in

240
00:09:49,279 --> 00:09:50,880
the text set

241
00:09:50,880 --> 00:09:53,440
as is cosine simulation

242
00:09:53,440 --> 00:09:56,399
f prime is a backdoor immediate code

243
00:09:56,399 --> 00:09:59,600
f is a clean median code

244
00:09:59,600 --> 00:10:03,279
our friend optimization problem is as

245
00:10:03,279 --> 00:10:03,760
1 follows

246
00:10:03,760 --> 00:10:06,800
lambda 2 are to have a parameters

247
00:10:06,800 --> 00:10:09,279
we can use gradient descent to resolve

248
00:10:09,279 --> 00:10:11,440
this optimization problem to craft a

249
00:10:11,440 --> 00:10:13,600
background imaging code from a clean

250
00:10:13,600 --> 00:10:15,839
code

251
00:10:16,000 --> 00:10:19,360
next let's see some evaluation results

252
00:10:19,360 --> 00:10:22,079
we use solvent as opportunity asset and

253
00:10:22,079 --> 00:10:23,120
using

254
00:10:23,120 --> 00:10:24,720
a popular self-supervised learning

255
00:10:24,720 --> 00:10:26,880
algorithm to pretend encoded on the

256
00:10:26,880 --> 00:10:28,880
protein data set

257
00:10:28,880 --> 00:10:31,839
we use gtsrb as version as children as

258
00:10:31,839 --> 00:10:34,160
dancing tasks which is respectfully

259
00:10:34,160 --> 00:10:36,880
represent traffic scientists digital

260
00:10:36,880 --> 00:10:40,160
classification and object classification

261
00:10:40,160 --> 00:10:42,320
we use a fully connected news network as

262
00:10:42,320 --> 00:10:44,880
a cluster

263
00:10:45,120 --> 00:10:48,000
in our attack we need to specify

264
00:10:48,000 --> 00:10:49,519
a target set

265
00:10:49,519 --> 00:10:52,800
referencing media and hyper parameters

266
00:10:52,800 --> 00:10:55,200
in this talk we use a preceding data set

267
00:10:55,200 --> 00:10:56,959
as a text asset

268
00:10:56,959 --> 00:10:58,880
and we collect one image from the

269
00:10:58,880 --> 00:11:01,200
internet as a reference image

270
00:11:01,200 --> 00:11:04,240
and set lamina 1 and lambda2 to be one

271
00:11:04,240 --> 00:11:06,079
we also consider other settings and the

272
00:11:06,079 --> 00:11:09,680
details can be found in our paper

273
00:11:09,760 --> 00:11:12,160
we use a text success reader to evaluate

274
00:11:12,160 --> 00:11:15,519
the effectiveness of betting code

275
00:11:15,519 --> 00:11:17,760
suppose we have some clean test images

276
00:11:17,760 --> 00:11:20,640
of the target announcement task

277
00:11:20,640 --> 00:11:22,640
we can add a background trigger to each

278
00:11:22,640 --> 00:11:24,800
of them

279
00:11:24,800 --> 00:11:27,040
then we use a classifier that is built

280
00:11:27,040 --> 00:11:29,839
upon magnolia encoder to predict the

281
00:11:29,839 --> 00:11:31,680
label for each image of its background

282
00:11:31,680 --> 00:11:33,519
trigger

283
00:11:33,519 --> 00:11:36,320
attack success rate is defined as a

284
00:11:36,320 --> 00:11:38,160
fraction of images with background

285
00:11:38,160 --> 00:11:40,640
trigger that are predicted as a target

286
00:11:40,640 --> 00:11:43,640
class

287
00:11:43,680 --> 00:11:45,760
this table shows that a bad encoder is

288
00:11:45,760 --> 00:11:48,240
effective where the first column is

289
00:11:48,240 --> 00:11:50,480
targeted down some tasks selected by the

290
00:11:50,480 --> 00:11:52,560
attacker when crafting a backup imaging

291
00:11:52,560 --> 00:11:53,920
code

292
00:11:53,920 --> 00:11:56,480
as we can see better encoder can achieve

293
00:11:56,480 --> 00:11:58,480
very high attack success rate which

294
00:11:58,480 --> 00:12:02,720
means that better encoder is effective

295
00:12:02,720 --> 00:12:05,040
we use clean accuracy and back door

296
00:12:05,040 --> 00:12:07,519
accuracy to evaluate the utility or

297
00:12:07,519 --> 00:12:09,440
clean immediate code and back door image

298
00:12:09,440 --> 00:12:11,360
encode

299
00:12:11,360 --> 00:12:14,880
suppose we have some clean text images

300
00:12:14,880 --> 00:12:16,959
we can use a class bar that is built

301
00:12:16,959 --> 00:12:19,440
upon claims encoder to predict a label

302
00:12:19,440 --> 00:12:21,839
for each image

303
00:12:21,839 --> 00:12:24,079
clear accuracy is defined as a

304
00:12:24,079 --> 00:12:26,000
fractional images that are correctly

305
00:12:26,000 --> 00:12:28,560
predicted

306
00:12:28,560 --> 00:12:31,200
similarly we can use a cluster that is

307
00:12:31,200 --> 00:12:33,600
built upon background imaging code to

308
00:12:33,600 --> 00:12:36,320
predict a label for each image

309
00:12:36,320 --> 00:12:38,959
and backward accuracy is defined as a

310
00:12:38,959 --> 00:12:40,720
fraction or immediate that are correctly

311
00:12:40,720 --> 00:12:43,279
predicted

312
00:12:43,680 --> 00:12:45,440
this table shows that bad encoder

313
00:12:45,440 --> 00:12:47,200
materiality

314
00:12:47,200 --> 00:12:49,519
the second column is done some tasks

315
00:12:49,519 --> 00:12:52,079
that we evaluate utility or claims

316
00:12:52,079 --> 00:12:54,800
encoder and vectoring encoder

317
00:12:54,800 --> 00:12:57,519
as we can see vector accuracy is similar

318
00:12:57,519 --> 00:12:59,760
to clear accuracy which means that

319
00:12:59,760 --> 00:13:03,360
better encoder maintains utility

320
00:13:03,440 --> 00:13:05,920
we also evaluate a better encoder on

321
00:13:05,920 --> 00:13:08,000
using a real-world case study

322
00:13:08,000 --> 00:13:10,240
we apply that encoder to open eyes

323
00:13:10,240 --> 00:13:12,959
immediate code which is present on 400

324
00:13:12,959 --> 00:13:15,519
milli image taxpayers collected from the

325
00:13:15,519 --> 00:13:16,800
internet

326
00:13:16,800 --> 00:13:19,040
this pretending data is not publicly

327
00:13:19,040 --> 00:13:21,920
available so the attacker can use the

328
00:13:21,920 --> 00:13:25,760
imageness dataset as a text set

329
00:13:25,760 --> 00:13:28,000
and similarly we collect one image from

330
00:13:28,000 --> 00:13:30,320
the internet as a reference media and

331
00:13:30,320 --> 00:13:34,399
say lamina 1 and lambda2 to b1

332
00:13:34,399 --> 00:13:36,720
this table shows our results

333
00:13:36,720 --> 00:13:39,519
we have two observations

334
00:13:39,519 --> 00:13:42,399
first a text success rate is very high

335
00:13:42,399 --> 00:13:44,240
which means that bad encoder is

336
00:13:44,240 --> 00:13:46,480
effective

337
00:13:46,480 --> 00:13:47,440
second

338
00:13:47,440 --> 00:13:49,760
brexit accuracy is similar to cleaner

339
00:13:49,760 --> 00:13:52,320
accuracy which means that better encoder

340
00:13:52,320 --> 00:13:55,279
maintenance activity

341
00:13:56,079 --> 00:13:58,560
we also generalize existing differences

342
00:13:58,560 --> 00:14:00,720
for background attacks to classify to

343
00:14:00,720 --> 00:14:03,040
define against our betting code

344
00:14:03,040 --> 00:14:05,440
we consider empirical defenses and

345
00:14:05,440 --> 00:14:07,199
approval defense

346
00:14:07,199 --> 00:14:10,160
for empirical defense we consider user

347
00:14:10,160 --> 00:14:13,440
cleanse and mntd which are twisted or

348
00:14:13,440 --> 00:14:15,440
other methods to detect whether

349
00:14:15,440 --> 00:14:18,240
classifier is backed or not

350
00:14:18,240 --> 00:14:20,720
for proof of defense we consider patch

351
00:14:20,720 --> 00:14:23,279
guard which can build a classifier with

352
00:14:23,279 --> 00:14:25,040
former security guarantees against the

353
00:14:25,040 --> 00:14:26,560
backdoor attacks

354
00:14:26,560 --> 00:14:28,560
due to some constraint i only show the

355
00:14:28,560 --> 00:14:30,800
conclusion here and omit the expanded

356
00:14:30,800 --> 00:14:32,959
results

357
00:14:32,959 --> 00:14:35,279
we find that neutral cleans cannot

358
00:14:35,279 --> 00:14:37,519
detect bacterium gain code

359
00:14:37,519 --> 00:14:40,000
and the detection accuracy of mntd is

360
00:14:40,000 --> 00:14:42,480
closer to random gases

361
00:14:42,480 --> 00:14:44,959
our results show that hedge guard

362
00:14:44,959 --> 00:14:47,199
provides insufficient robust guarantees

363
00:14:47,199 --> 00:14:49,839
against wedding code

364
00:14:49,839 --> 00:14:51,600
in summary

365
00:14:51,600 --> 00:14:53,199
several learning is vulnerable to

366
00:14:53,199 --> 00:14:55,279
background attacks

367
00:14:55,279 --> 00:14:58,160
we find that insecure encoders lead to a

368
00:14:58,160 --> 00:15:01,760
single point ophelia or ai ecosystem

369
00:15:01,760 --> 00:15:04,480
equation defenses are insufficient so we

370
00:15:04,480 --> 00:15:06,560
need a secure self-supervised learning

371
00:15:06,560 --> 00:15:07,680
algorithm

372
00:15:07,680 --> 00:15:09,680
thank you for listening yeah i'm happy

373
00:15:09,680 --> 00:15:13,100
to take any questions

374
00:15:13,100 --> 00:15:17,839
[Applause]

375
00:15:17,839 --> 00:15:21,760
right so we have time for questions

376
00:15:24,560 --> 00:15:27,040
please go to the mic state your name

377
00:15:27,040 --> 00:15:28,800
and affiliation

378
00:15:28,800 --> 00:15:31,519
uh hi thanks for the nice talk uh this

379
00:15:31,519 --> 00:15:33,360
is virat from umass

380
00:15:33,360 --> 00:15:36,000
uh so what data augmentations did you

381
00:15:36,000 --> 00:15:39,199
use for the like sim clr did you try

382
00:15:39,199 --> 00:15:42,079
strong augmentations or have you sort of

383
00:15:42,079 --> 00:15:43,600
experimented with different

384
00:15:43,600 --> 00:15:46,880
augmentations maybe what's grc

385
00:15:46,880 --> 00:15:49,120
uh like what data augmentations did you

386
00:15:49,120 --> 00:15:52,240
use in the sim clr uh because you mean

387
00:15:52,240 --> 00:15:53,680
like augmentation

388
00:15:53,680 --> 00:15:56,240
yeah yeah date yeah augmentations okay

389
00:15:56,240 --> 00:15:58,560
so for layers for

390
00:15:58,560 --> 00:16:00,880
like in cbci is a like a data

391
00:16:00,880 --> 00:16:03,040
organization operation is a composition

392
00:16:03,040 --> 00:16:06,639
of a series of like operations like a

393
00:16:06,639 --> 00:16:08,399
randomly decider crop

394
00:16:08,399 --> 00:16:10,880
gotcha blur right and college it and so

395
00:16:10,880 --> 00:16:13,279
on right so actually we use its original

396
00:16:13,279 --> 00:16:15,519
version yeah how you see that

397
00:16:15,519 --> 00:16:17,680
yeah you can go ahead uh so yeah thanks

398
00:16:17,680 --> 00:16:20,160
for a follow-up question i think

399
00:16:20,160 --> 00:16:21,920
most of the original sim clr

400
00:16:21,920 --> 00:16:24,720
augmentations are weak augmentations

401
00:16:24,720 --> 00:16:27,040
uh like weak data augmentations but in

402
00:16:27,040 --> 00:16:28,720
if have you tried like strong

403
00:16:28,720 --> 00:16:30,560
augmentations like which have

404
00:16:30,560 --> 00:16:33,920
random crop or auto augment uh based uh

405
00:16:33,920 --> 00:16:36,639
contrastive learning approaches yeah so

406
00:16:36,639 --> 00:16:38,560
you are talking about like whether our

407
00:16:38,560 --> 00:16:40,560
method can be generalized yeah yeah yeah

408
00:16:40,560 --> 00:16:43,680
exactly yeah i think yeah so

409
00:16:43,680 --> 00:16:45,839
i think by design

410
00:16:45,839 --> 00:16:49,040
actually bad encoder does not rely on

411
00:16:49,040 --> 00:16:51,279
how the encoder is between

412
00:16:51,279 --> 00:16:53,839
right so you can see this how these laws

413
00:16:53,839 --> 00:16:56,000
are defined right it does not involve

414
00:16:56,000 --> 00:16:58,959
the like the property rights or some

415
00:16:58,959 --> 00:17:01,759
property of the income so i think

416
00:17:01,759 --> 00:17:05,119
in our evaluation we use simsched but i

417
00:17:05,119 --> 00:17:08,079
think it can be directly applied to many

418
00:17:08,079 --> 00:17:10,959
other encoders but how it will perform

419
00:17:10,959 --> 00:17:13,679
yeah so we to be honest we currently

420
00:17:13,679 --> 00:17:18,319
don't know yeah okay good thanks

421
00:17:18,319 --> 00:17:21,520
hi gory talk uh young chi from purdue so

422
00:17:21,520 --> 00:17:23,919
i can understand why like existing

423
00:17:23,919 --> 00:17:26,079
defense like neuro cleans could not just

424
00:17:26,079 --> 00:17:29,440
directly detect the encoder but if the

425
00:17:29,440 --> 00:17:31,280
user actually trims the trailer

426
00:17:31,280 --> 00:17:33,600
classifier for their tasks and they

427
00:17:33,600 --> 00:17:35,200
combine this trailer with the encoder

428
00:17:35,200 --> 00:17:38,400
it's essentially just a typical ml model

429
00:17:38,400 --> 00:17:41,120
then why cannot those defense defend

430
00:17:41,120 --> 00:17:42,400
this

431
00:17:42,400 --> 00:17:43,520
whole

432
00:17:43,520 --> 00:17:45,440
ml model

433
00:17:45,440 --> 00:17:47,679
yeah thanks for the question so

434
00:17:47,679 --> 00:17:49,919
basically you you are saying like uh

435
00:17:49,919 --> 00:17:52,160
like we have an encoder right it's like

436
00:17:52,160 --> 00:17:54,559
a future extractor and built up on this

437
00:17:54,559 --> 00:17:56,240
field exchange yeah

438
00:17:56,240 --> 00:17:57,919
for users to use this encoder they need

439
00:17:57,919 --> 00:17:58,840
to build

440
00:17:58,840 --> 00:18:01,200
up classifier right yeah classifier and

441
00:18:01,200 --> 00:18:02,960
they combine these two i think it's

442
00:18:02,960 --> 00:18:05,200
still kind of like similar to what

443
00:18:05,200 --> 00:18:08,400
existing mml model is and

444
00:18:08,400 --> 00:18:10,880
uh mntd or neural clean should be able

445
00:18:10,880 --> 00:18:14,400
to detect triggers in in this scenario

446
00:18:14,400 --> 00:18:16,720
not necessarily right as you can see

447
00:18:16,720 --> 00:18:18,559
first like we

448
00:18:18,559 --> 00:18:20,799
actually evaluate those defenses right

449
00:18:20,799 --> 00:18:23,360
we find actually they cannot detect them

450
00:18:23,360 --> 00:18:25,600
yeah i i think those defenses cannot

451
00:18:25,600 --> 00:18:28,559
detect just given the encoder because

452
00:18:28,559 --> 00:18:31,600
there is no classes but even with the

453
00:18:31,600 --> 00:18:34,959
class trigger classifier those

454
00:18:35,120 --> 00:18:37,039
those scanning methods cut still cannot

455
00:18:37,039 --> 00:18:39,760
be tagged back back doors yeah sorry for

456
00:18:39,760 --> 00:18:43,440
the confusion yeah so basically when we

457
00:18:43,440 --> 00:18:45,840
like for example when we apply neutral

458
00:18:45,840 --> 00:18:47,280
cleanse to

459
00:18:47,280 --> 00:18:48,559
basically we generalize the neural

460
00:18:48,559 --> 00:18:51,200
cleanse to detect whether

461
00:18:51,200 --> 00:18:54,320
like our bed encoder right so actually

462
00:18:54,320 --> 00:18:56,320
we apply the

463
00:18:56,320 --> 00:18:59,120
neural cleanse to a composition of like

464
00:18:59,120 --> 00:19:01,840
the encoder and some transform clasp so

465
00:19:01,840 --> 00:19:04,960
actually we detect the clasp not our

466
00:19:04,960 --> 00:19:08,240
encoder oh so even with the classifier

467
00:19:08,240 --> 00:19:11,760
yes still cannot exactly yeah uh can you

468
00:19:11,760 --> 00:19:13,840
provide some intuition why neuroclean

469
00:19:13,840 --> 00:19:16,000
still cannot because that's that should

470
00:19:16,000 --> 00:19:17,200
be the same

471
00:19:17,200 --> 00:19:18,960
kind of similar to the model neuroplanes

472
00:19:18,960 --> 00:19:20,720
evaluated on

473
00:19:20,720 --> 00:19:22,799
yeah i think for its kind of version

474
00:19:22,799 --> 00:19:25,280
maybe it cannot be detected because it's

475
00:19:25,280 --> 00:19:27,520
maybe tailored to classifier right

476
00:19:27,520 --> 00:19:29,600
you'll see how when we use a training

477
00:19:29,600 --> 00:19:31,520
data set or like a big dollar tree

478
00:19:31,520 --> 00:19:33,120
that's a touching or classifier right

479
00:19:33,120 --> 00:19:34,720
with applied standards to provide

480
00:19:34,720 --> 00:19:37,840
learning but in whatever in

481
00:19:37,840 --> 00:19:39,679
right we first use

482
00:19:39,679 --> 00:19:40,799
like uh

483
00:19:40,799 --> 00:19:43,280
encoder to generate some future vectors

484
00:19:43,280 --> 00:19:45,919
and advancing ex class file right i

485
00:19:45,919 --> 00:19:48,160
think maybe it's due to a mechanism

486
00:19:48,160 --> 00:19:50,400
difference but i think it's very

487
00:19:50,400 --> 00:19:52,720
interesting to further generalize or

488
00:19:52,720 --> 00:19:54,799
something like a depth neural cleanse

489
00:19:54,799 --> 00:19:57,360
somehow like for example we make a

490
00:19:57,360 --> 00:20:01,200
like modified loss right then tell it to

491
00:20:01,200 --> 00:20:03,600
encode so maybe we can directly detect

492
00:20:03,600 --> 00:20:07,439
the encoders rather than the class

493
00:20:19,200 --> 00:20:22,240
uh my available is kind of similar as a

494
00:20:22,240 --> 00:20:23,120
former

495
00:20:23,120 --> 00:20:25,919
uh former one so uh can you comment a

496
00:20:25,919 --> 00:20:29,360
little bit more on like uh how then the

497
00:20:29,360 --> 00:20:31,039
uh self super

498
00:20:31,039 --> 00:20:32,799
supervised learning

499
00:20:32,799 --> 00:20:34,960
uh backdoor detection is different from

500
00:20:34,960 --> 00:20:38,640
you know like uh the uh

501
00:20:38,640 --> 00:20:40,240
better steady

502
00:20:40,240 --> 00:20:43,039
classifier uh vector detections

503
00:20:43,039 --> 00:20:45,200
so you're talking about like the

504
00:20:45,200 --> 00:20:47,360
difference between like detecting better

505
00:20:47,360 --> 00:20:49,120
encoders

506
00:20:49,120 --> 00:20:51,919
do you have any answers on this yes

507
00:20:51,919 --> 00:20:52,880
i think

508
00:20:52,880 --> 00:20:55,120
for

509
00:20:55,679 --> 00:20:58,000
yeah that's a good question yeah so for

510
00:20:58,000 --> 00:21:01,039
detecting like a classifier right so i

511
00:21:01,039 --> 00:21:03,760
think for example classify it may have

512
00:21:03,760 --> 00:21:06,559
like some confidence goals right so

513
00:21:06,559 --> 00:21:08,240
based on this confidence guard we make

514
00:21:08,240 --> 00:21:10,320
some predictions basically we take the

515
00:21:10,320 --> 00:21:12,799
index with the largest confidence score

516
00:21:12,799 --> 00:21:16,799
right so i think one they design those

517
00:21:16,799 --> 00:21:20,080
like defenses they basically

518
00:21:20,080 --> 00:21:22,799
like once the like the target the

519
00:21:22,799 --> 00:21:24,640
confidence of the target class is

520
00:21:24,640 --> 00:21:26,880
largest range will be predicted as a

521
00:21:26,880 --> 00:21:29,200
target class right so they do not tell

522
00:21:29,200 --> 00:21:31,039
us about the specific value of the

523
00:21:31,039 --> 00:21:33,520
confidence scores right but you see when

524
00:21:33,520 --> 00:21:36,720
we design our betting code basically we

525
00:21:36,720 --> 00:21:40,159
make the encoder have a specific produce

526
00:21:40,159 --> 00:21:43,120
a specific future vector right

527
00:21:43,120 --> 00:21:45,760
but i think in that way so we need you

528
00:21:45,760 --> 00:21:48,080
to care about his specific venue so i

529
00:21:48,080 --> 00:21:51,520
think that's maybe one differences

530
00:21:51,520 --> 00:21:54,879
okay thank you yeah thank you

531
00:21:54,960 --> 00:21:56,960
so i guess that we are all i'm holding

532
00:21:56,960 --> 00:21:59,200
you all for lunch and

533
00:21:59,200 --> 00:22:01,120
we're finishing the session and thanks

534
00:22:01,120 --> 00:22:02,720
to speaker once again thank you thank

535
00:22:02,720 --> 00:22:06,360
you thank you

