1
00:00:01,360 --> 00:00:03,600
so um what's up everybody my name is

2
00:00:03,600 --> 00:00:05,759
ming shinzo i'm a phd student from

3
00:00:05,759 --> 00:00:08,000
carnegie mellon university today i'm

4
00:00:08,000 --> 00:00:09,760
very happy to share with you our paper

5
00:00:09,760 --> 00:00:11,599
locally differentially private sparse

6
00:00:11,599 --> 00:00:13,280
vector aggregation here are my

7
00:00:13,280 --> 00:00:15,920
co-authors tian hawang huber chen julia

8
00:00:15,920 --> 00:00:18,560
and elaine

9
00:00:18,560 --> 00:00:20,560
so what is private vector aggregation

10
00:00:20,560 --> 00:00:22,960
well imagine that you have n client

11
00:00:22,960 --> 00:00:24,560
each client could have some local

12
00:00:24,560 --> 00:00:25,920
private data

13
00:00:25,920 --> 00:00:28,000
and then they represent their data using

14
00:00:28,000 --> 00:00:30,320
d-dimensional vector and we assume those

15
00:00:30,320 --> 00:00:33,680
values are in negative one and one

16
00:00:33,680 --> 00:00:34,960
and now the server is trying to

17
00:00:34,960 --> 00:00:36,880
aggregate those vectors and figure out

18
00:00:36,880 --> 00:00:40,800
what is the summation of those vectors

19
00:00:40,800 --> 00:00:43,440
to protect their own privacy the client

20
00:00:43,440 --> 00:00:44,960
will use some privacy preserving

21
00:00:44,960 --> 00:00:47,680
encodings to encode their local vector

22
00:00:47,680 --> 00:00:51,440
and upload the encodings to the server

23
00:00:51,440 --> 00:00:53,360
then the server will run some estimation

24
00:00:53,360 --> 00:00:56,239
algorithm on those encodings and output

25
00:00:56,239 --> 00:00:58,559
the estimation of the sum vector so this

26
00:00:58,559 --> 00:01:01,280
is pretty straightforward

27
00:01:01,280 --> 00:01:03,520
and why would we do something like this

28
00:01:03,520 --> 00:01:05,040
it turns out that

29
00:01:05,040 --> 00:01:06,799
private vector aggregation can be very

30
00:01:06,799 --> 00:01:09,119
useful in private data analytics it can

31
00:01:09,119 --> 00:01:10,320
be used for

32
00:01:10,320 --> 00:01:12,159
for example item frequency estimation

33
00:01:12,159 --> 00:01:14,960
can be used for hypothesis testing

34
00:01:14,960 --> 00:01:17,840
and even for clustering and it also has

35
00:01:17,840 --> 00:01:19,520
a lot of real-world applications for

36
00:01:19,520 --> 00:01:21,439
example browser data collection

37
00:01:21,439 --> 00:01:23,360
advertisement system and federated

38
00:01:23,360 --> 00:01:26,000
learning

39
00:01:26,000 --> 00:01:27,439
there are a lot of companies are trying

40
00:01:27,439 --> 00:01:30,159
to do similar things for example google

41
00:01:30,159 --> 00:01:33,520
mozilla meta and apple

42
00:01:33,520 --> 00:01:36,079
in our paper we focus on sparse vectors

43
00:01:36,079 --> 00:01:38,799
so let's say we have a movie rating

44
00:01:38,799 --> 00:01:41,119
aggregation task such that we are

45
00:01:41,119 --> 00:01:43,600
aggregating the user's rating to those

46
00:01:43,600 --> 00:01:45,680
movie

47
00:01:45,680 --> 00:01:47,520
and let's say we have 10 to the five

48
00:01:47,520 --> 00:01:51,280
clients and 20 000 movies and each user

49
00:01:51,280 --> 00:01:53,200
will only raise about 10 movies

50
00:01:53,200 --> 00:01:54,799
something like that

51
00:01:54,799 --> 00:01:57,040
and if we compile each user's rating to

52
00:01:57,040 --> 00:01:58,479
all those movies

53
00:01:58,479 --> 00:02:01,920
the veteran could be very very sparse

54
00:02:01,920 --> 00:02:03,840
another great example is the

55
00:02:03,840 --> 00:02:05,920
sparsification technique for federated

56
00:02:05,920 --> 00:02:08,560
learning in federated learning the user

57
00:02:08,560 --> 00:02:10,399
will update their local gradient to the

58
00:02:10,399 --> 00:02:12,000
central server for some machine learning

59
00:02:12,000 --> 00:02:12,959
training

60
00:02:12,959 --> 00:02:15,840
but here when they update their gradient

61
00:02:15,840 --> 00:02:17,520
vector to the server they will apply a

62
00:02:17,520 --> 00:02:20,000
random masking to zero out some of the

63
00:02:20,000 --> 00:02:22,959
coordinates in the vector and their

64
00:02:22,959 --> 00:02:25,280
gradient now is very very sparse and

65
00:02:25,280 --> 00:02:27,360
previous paper have shown that with this

66
00:02:27,360 --> 00:02:29,280
technique the client could save a lot of

67
00:02:29,280 --> 00:02:31,360
communication cost while the central

68
00:02:31,360 --> 00:02:33,280
server can still train a pretty good

69
00:02:33,280 --> 00:02:36,160
machine learning model

70
00:02:36,160 --> 00:02:38,400
so in our paper we focus on k-sports

71
00:02:38,400 --> 00:02:40,239
vector it means there are most k

72
00:02:40,239 --> 00:02:42,879
non-zero coordinates in the same vector

73
00:02:42,879 --> 00:02:43,840
and

74
00:02:43,840 --> 00:02:45,519
we assume different clients could have

75
00:02:45,519 --> 00:02:48,879
different sparsity pattern

76
00:02:49,040 --> 00:02:50,720
there is actually a tension between

77
00:02:50,720 --> 00:02:52,640
communication versus the estimation

78
00:02:52,640 --> 00:02:54,800
error for private vector aggregation

79
00:02:54,800 --> 00:02:55,760
problem

80
00:02:55,760 --> 00:02:57,760
especially the sparse case

81
00:02:57,760 --> 00:03:01,360
the canonical way to handle a vector in

82
00:03:01,360 --> 00:03:03,280
privacy literature is the gaussian

83
00:03:03,280 --> 00:03:05,280
mechanism but the gaussian mechanism

84
00:03:05,280 --> 00:03:06,720
will add

85
00:03:06,720 --> 00:03:08,720
gaussian noise to each coordinates in

86
00:03:08,720 --> 00:03:11,200
the same vector in the same vector

87
00:03:11,200 --> 00:03:13,519
making the output not sparse anymore so

88
00:03:13,519 --> 00:03:15,440
we actually lose the advantage of the

89
00:03:15,440 --> 00:03:17,280
communication

90
00:03:17,280 --> 00:03:19,599
there is also some central sparse vector

91
00:03:19,599 --> 00:03:21,200
releasing technique but they are more

92
00:03:21,200 --> 00:03:23,040
suitable in the central setting instead

93
00:03:23,040 --> 00:03:24,959
of the aggregation setting there are

94
00:03:24,959 --> 00:03:27,760
also methods for dense vectors but those

95
00:03:27,760 --> 00:03:30,560
methods are not utilizing sparsity very

96
00:03:30,560 --> 00:03:34,000
well making them have non-optimal error

97
00:03:34,000 --> 00:03:36,239
so our target is to have a low

98
00:03:36,239 --> 00:03:38,080
communication and low estimation

99
00:03:38,080 --> 00:03:40,480
algorithm

100
00:03:40,480 --> 00:03:42,480
to formally define the privacy we use a

101
00:03:42,480 --> 00:03:44,319
pretty standard local differential

102
00:03:44,319 --> 00:03:46,640
privacy definition it says for any

103
00:03:46,640 --> 00:03:48,879
neighboring vectors x and x prime

104
00:03:48,879 --> 00:03:50,720
their privacy preserving encodings

105
00:03:50,720 --> 00:03:53,360
should be very similar

106
00:03:53,360 --> 00:03:54,720
for those who are not familiar with

107
00:03:54,720 --> 00:03:56,400
defeat just think about those

108
00:03:56,400 --> 00:04:00,239
distribution are very very close

109
00:04:00,239 --> 00:04:01,840
actually with different neighboring

110
00:04:01,840 --> 00:04:03,760
definition we could have different level

111
00:04:03,760 --> 00:04:04,959
of privacy

112
00:04:04,959 --> 00:04:06,799
for example a weaker definition is

113
00:04:06,799 --> 00:04:08,720
called the event level privacy

114
00:04:08,720 --> 00:04:11,200
is that for any two vectors such that

115
00:04:11,200 --> 00:04:14,080
only differ in one entry then we require

116
00:04:14,080 --> 00:04:15,200
their

117
00:04:15,200 --> 00:04:17,839
encodings to be very similar

118
00:04:17,839 --> 00:04:20,079
it is called event level because

119
00:04:20,079 --> 00:04:21,600
we can sometimes think about each

120
00:04:21,600 --> 00:04:23,840
coordinate in the same vector can be

121
00:04:23,840 --> 00:04:26,320
thinking about some individual event in

122
00:04:26,320 --> 00:04:28,720
this case when the adversary observe the

123
00:04:28,720 --> 00:04:30,960
encodings it cannot distinguish for that

124
00:04:30,960 --> 00:04:33,520
particular location whether it is a or

125
00:04:33,520 --> 00:04:36,240
whether it is b

126
00:04:36,639 --> 00:04:38,800
a stronger level of privacy is called a

127
00:04:38,800 --> 00:04:41,120
user level privacy it says for any two

128
00:04:41,120 --> 00:04:43,759
case bars vectors their encodings should

129
00:04:43,759 --> 00:04:46,000
be very similar in this case basically

130
00:04:46,000 --> 00:04:48,720
the adversary cannot sure about anything

131
00:04:48,720 --> 00:04:51,520
from the original input

132
00:04:51,520 --> 00:04:53,280
in our paper we actually use a more

133
00:04:53,280 --> 00:04:55,680
unified privacy definition it is called

134
00:04:55,680 --> 00:04:58,400
the lldp it says for any two vectors x

135
00:04:58,400 --> 00:05:00,800
and x prime if their l1 distance is

136
00:05:00,800 --> 00:05:03,120
bounded then their encoding should be

137
00:05:03,120 --> 00:05:05,280
very similar and actually this unified

138
00:05:05,280 --> 00:05:07,759
prophecy definition is pretty general

139
00:05:07,759 --> 00:05:10,479
for example l equals 2 implies event

140
00:05:10,479 --> 00:05:13,840
level privacy and l equals to 2k implies

141
00:05:13,840 --> 00:05:15,840
user level privacy and in the middle

142
00:05:15,840 --> 00:05:18,160
ground we have more flexible privacy

143
00:05:18,160 --> 00:05:19,919
definition

144
00:05:19,919 --> 00:05:22,160
now let's talk about some algorithm

145
00:05:22,160 --> 00:05:24,800
design

146
00:05:24,800 --> 00:05:26,160
in this talk i would try to make it as

147
00:05:26,160 --> 00:05:27,919
simple as possible so we're going to

148
00:05:27,919 --> 00:05:31,440
assume those inputs are all 01 vectors

149
00:05:31,440 --> 00:05:33,039
i'm going to start from one sparse

150
00:05:33,039 --> 00:05:35,360
encoding and i'm going to talk about how

151
00:05:35,360 --> 00:05:37,280
we construct a strongman scheme and how

152
00:05:37,280 --> 00:05:39,360
we use our basic scheme to overcome the

153
00:05:39,360 --> 00:05:40,800
drawbacks of those

154
00:05:40,800 --> 00:05:42,800
strongman scheme i'm not going to cover

155
00:05:42,800 --> 00:05:44,479
our full scheme here and i'm not going

156
00:05:44,479 --> 00:05:48,400
to cover detailed theoretical analysis

157
00:05:48,400 --> 00:05:51,120
so let's start from a very very basic

158
00:05:51,120 --> 00:05:53,280
thing that is the one sparse and even

159
00:05:53,280 --> 00:05:54,960
non-private encoding

160
00:05:54,960 --> 00:05:57,280
now the encoder is given

161
00:05:57,280 --> 00:06:00,080
an input x and it's a one sparse

162
00:06:00,080 --> 00:06:02,400
zero one vector so let's say in location

163
00:06:02,400 --> 00:06:05,280
i there is a one out there

164
00:06:05,280 --> 00:06:07,600
so the encoder will sample a binary hash

165
00:06:07,600 --> 00:06:10,160
s and the binary hash will output zero

166
00:06:10,160 --> 00:06:12,080
uh will output plus one or negative one

167
00:06:12,080 --> 00:06:13,840
for each coordinate

168
00:06:13,840 --> 00:06:15,680
and the inco the encoding scheme is

169
00:06:15,680 --> 00:06:17,199
pretty simple

170
00:06:17,199 --> 00:06:19,440
that would simply include the hash

171
00:06:19,440 --> 00:06:21,840
values for that particular location i

172
00:06:21,840 --> 00:06:24,000
and also the whole description about the

173
00:06:24,000 --> 00:06:25,520
binary hash

174
00:06:25,520 --> 00:06:27,360
it turns out that we can actually use a

175
00:06:27,360 --> 00:06:30,160
random c to represent the binary hash

176
00:06:30,160 --> 00:06:33,600
so the encoder is very efficient

177
00:06:33,600 --> 00:06:35,840
for the decoder it simply take this

178
00:06:35,840 --> 00:06:37,039
encoding

179
00:06:37,039 --> 00:06:39,680
and it simply take the hash bit and

180
00:06:39,680 --> 00:06:42,639
multiply that bit to the whole hash

181
00:06:42,639 --> 00:06:43,759
vector

182
00:06:43,759 --> 00:06:46,560
and we call this output as our decoded

183
00:06:46,560 --> 00:06:48,400
vector

184
00:06:48,400 --> 00:06:50,880
so what really happens here is that the

185
00:06:50,880 --> 00:06:52,960
decoder will have a very nice output

186
00:06:52,960 --> 00:06:54,160
distribution

187
00:06:54,160 --> 00:06:56,639
to the location i because we take the

188
00:06:56,639 --> 00:06:59,199
hash value from location i and multiply

189
00:06:59,199 --> 00:07:01,120
again to the whole vector so to the

190
00:07:01,120 --> 00:07:03,520
location i it is guaranteed to be s i

191
00:07:03,520 --> 00:07:05,039
multiplied by s i

192
00:07:05,039 --> 00:07:07,360
and because our binary has only output

193
00:07:07,360 --> 00:07:09,280
negative one or one so the

194
00:07:09,280 --> 00:07:11,680
multiplication will definitely be one

195
00:07:11,680 --> 00:07:14,240
for any other location j

196
00:07:14,240 --> 00:07:15,120
the

197
00:07:15,120 --> 00:07:18,400
output will be s i multiplied by sj and

198
00:07:18,400 --> 00:07:20,479
if we assume the binary hash is

199
00:07:20,479 --> 00:07:23,199
uniformly random then this value is a

200
00:07:23,199 --> 00:07:25,520
random bit it is plus one or negative

201
00:07:25,520 --> 00:07:27,599
one with probability of half so it's

202
00:07:27,599 --> 00:07:29,199
pretty nice

203
00:07:29,199 --> 00:07:31,599
so the server aggregation is very simple

204
00:07:31,599 --> 00:07:33,280
that i simply sum up all the decoded

205
00:07:33,280 --> 00:07:34,400
vectors

206
00:07:34,400 --> 00:07:36,639
and here's a very very simple analysis

207
00:07:36,639 --> 00:07:38,960
for example let's say only client one

208
00:07:38,960 --> 00:07:42,479
has a valid one in location i and client

209
00:07:42,479 --> 00:07:44,720
two to client n we all have zero in that

210
00:07:44,720 --> 00:07:46,479
location

211
00:07:46,479 --> 00:07:48,720
after the encoding and decoding process

212
00:07:48,720 --> 00:07:50,879
client one is guaranteed to output a one

213
00:07:50,879 --> 00:07:52,800
in that location and client two to

214
00:07:52,800 --> 00:07:54,720
client end without the random bit and

215
00:07:54,720 --> 00:07:57,280
the summation of all those values

216
00:07:57,280 --> 00:08:00,000
is exactly one plus a random walk of

217
00:08:00,000 --> 00:08:02,319
plus one or negative one with step n

218
00:08:02,319 --> 00:08:04,240
minus one

219
00:08:04,240 --> 00:08:06,080
and this analysis actually applies to

220
00:08:06,080 --> 00:08:07,759
every other location

221
00:08:07,759 --> 00:08:10,639
so um for any location i the output

222
00:08:10,639 --> 00:08:13,199
would be the true count plus a plus one

223
00:08:13,199 --> 00:08:14,960
negative one random walk with the count

224
00:08:14,960 --> 00:08:17,440
of n minus the true count

225
00:08:17,440 --> 00:08:19,599
and this random work part is exactly the

226
00:08:19,599 --> 00:08:20,479
error

227
00:08:20,479 --> 00:08:22,240
and with a very standard concentration

228
00:08:22,240 --> 00:08:24,479
bound we can prove that it is square

229
00:08:24,479 --> 00:08:27,120
root n roughly

230
00:08:27,120 --> 00:08:29,199
how to make the previous scheme private

231
00:08:29,199 --> 00:08:32,080
it is also not that difficult

232
00:08:32,080 --> 00:08:34,399
now we are not directly

233
00:08:34,399 --> 00:08:37,519
include the hash value in

234
00:08:37,519 --> 00:08:39,679
in the encoding but now we are flipping

235
00:08:39,679 --> 00:08:42,240
that bit with some small probability and

236
00:08:42,240 --> 00:08:43,599
this technique is called the randomized

237
00:08:43,599 --> 00:08:44,800
response

238
00:08:44,800 --> 00:08:47,120
it says if we flip that bit with

239
00:08:47,120 --> 00:08:49,920
probability one over e to the epsilon

240
00:08:49,920 --> 00:08:52,480
plus one then the whole scheme satisfy

241
00:08:52,480 --> 00:08:55,519
epsilon zero ldp and actually in the one

242
00:08:55,519 --> 00:08:57,920
square case event level or user level is

243
00:08:57,920 --> 00:09:00,560
equivalent

244
00:09:01,120 --> 00:09:03,839
so now um we can construct a very

245
00:09:03,839 --> 00:09:05,600
natural stroman scheme from the previous

246
00:09:05,600 --> 00:09:07,040
one square scheme

247
00:09:07,040 --> 00:09:10,720
so now we are given a k-sparse vector

248
00:09:10,720 --> 00:09:13,519
so we can just think about it as

249
00:09:13,519 --> 00:09:15,760
k-virtual clients and at all virtual

250
00:09:15,760 --> 00:09:18,560
clients we hold a single sparse vector

251
00:09:18,560 --> 00:09:20,640
we simply use the previous one sparse

252
00:09:20,640 --> 00:09:22,720
private encoding to encode all those

253
00:09:22,720 --> 00:09:24,000
vectors

254
00:09:24,000 --> 00:09:25,600
and then send all the encodings to the

255
00:09:25,600 --> 00:09:27,760
server this theme actually satisfy event

256
00:09:27,760 --> 00:09:29,839
level ldp

257
00:09:29,839 --> 00:09:31,760
the server simply computes some vectors

258
00:09:31,760 --> 00:09:34,080
of all those can virtual clients and the

259
00:09:34,080 --> 00:09:36,160
error will be roughly square root k

260
00:09:36,160 --> 00:09:39,360
times n but this is actually not optimal

261
00:09:39,360 --> 00:09:40,959
and we can actually get rid of that

262
00:09:40,959 --> 00:09:43,360
squared k factor here

263
00:09:43,360 --> 00:09:45,360
and here's how we do that it is our

264
00:09:45,360 --> 00:09:48,160
starting point of our aggregate design

265
00:09:48,160 --> 00:09:50,800
so in the event level private scheme

266
00:09:50,800 --> 00:09:54,560
our encoder is given a case based vector

267
00:09:54,560 --> 00:09:57,839
and we again use the binary s to hash

268
00:09:57,839 --> 00:09:59,600
all those nonzero coordinates into a

269
00:09:59,600 --> 00:10:01,519
random bit

270
00:10:01,519 --> 00:10:03,120
and then we're using another hash called

271
00:10:03,120 --> 00:10:04,720
the bucket hash to hash them into the

272
00:10:04,720 --> 00:10:06,959
bucket and if multiple values are mapped

273
00:10:06,959 --> 00:10:08,560
into the same bucket then we simply add

274
00:10:08,560 --> 00:10:10,720
them up

275
00:10:10,720 --> 00:10:12,160
then we add some laplace and noise to

276
00:10:12,160 --> 00:10:14,000
protect the privacy

277
00:10:14,000 --> 00:10:15,920
and then our encoding will include the

278
00:10:15,920 --> 00:10:18,800
noisy version of the buckets and the

279
00:10:18,800 --> 00:10:21,040
description about the binary hash and

280
00:10:21,040 --> 00:10:23,839
also the bucket hash

281
00:10:23,839 --> 00:10:25,760
the decoder will simply take this

282
00:10:25,760 --> 00:10:26,800
encoding

283
00:10:26,800 --> 00:10:29,920
it will simply first use the bucket hash

284
00:10:29,920 --> 00:10:32,959
to reverse the value from

285
00:10:32,959 --> 00:10:35,680
the buckets to the vector

286
00:10:35,680 --> 00:10:39,200
and then multiply it to

287
00:10:39,200 --> 00:10:40,000
the

288
00:10:40,000 --> 00:10:41,680
hash vector again

289
00:10:41,680 --> 00:10:45,200
then we call this our output vector

290
00:10:45,200 --> 00:10:47,440
and the server aggregation is the same

291
00:10:47,440 --> 00:10:49,200
that it simply sums up all the decoded

292
00:10:49,200 --> 00:10:51,200
vectors and with a very similar error

293
00:10:51,200 --> 00:10:52,240
analysis

294
00:10:52,240 --> 00:10:54,480
we can actually show that the error will

295
00:10:54,480 --> 00:10:56,399
be roughly a plus one or negative one

296
00:10:56,399 --> 00:10:58,560
random walk with a number of hash

297
00:10:58,560 --> 00:11:01,279
collision and because now we're using

298
00:11:01,279 --> 00:11:03,440
enough buckets so we can avoid many of

299
00:11:03,440 --> 00:11:05,200
the hash collisions and the arrow will

300
00:11:05,200 --> 00:11:06,800
be roughly

301
00:11:06,800 --> 00:11:10,160
squared n and here we just improve the

302
00:11:10,160 --> 00:11:12,839
previous scheme by skirt k

303
00:11:12,839 --> 00:11:15,519
factor well the most trickier part is

304
00:11:15,519 --> 00:11:17,120
that how we extend the previous scheme

305
00:11:17,120 --> 00:11:19,920
to lldp it turns out that if we use the

306
00:11:19,920 --> 00:11:22,000
naive composition theorem or advanced

307
00:11:22,000 --> 00:11:23,519
composition theorem

308
00:11:23,519 --> 00:11:26,480
it could lead to sub-optimal parameters

309
00:11:26,480 --> 00:11:27,760
and here

310
00:11:27,760 --> 00:11:29,920
we realize that we can actually tune how

311
00:11:29,920 --> 00:11:31,600
many buckets we're going to use and

312
00:11:31,600 --> 00:11:33,120
what's the laplace and noise we're going

313
00:11:33,120 --> 00:11:35,920
to add here

314
00:11:36,800 --> 00:11:38,959
so here's the key observation

315
00:11:38,959 --> 00:11:41,200
let's say two vectors and let's say

316
00:11:41,200 --> 00:11:43,200
originally their l1 distance is like

317
00:11:43,200 --> 00:11:44,880
here it's three

318
00:11:44,880 --> 00:11:47,680
and boom we have our

319
00:11:47,680 --> 00:11:49,120
hashing process

320
00:11:49,120 --> 00:11:51,920
and magically in the final two bucket we

321
00:11:51,920 --> 00:11:53,680
see the l1 distance is actually much

322
00:11:53,680 --> 00:11:54,800
smaller

323
00:11:54,800 --> 00:11:56,639
so what really happens here

324
00:11:56,639 --> 00:11:58,639
it turns out that the hash collision

325
00:11:58,639 --> 00:12:01,200
make the l1 distance smaller because if

326
00:12:01,200 --> 00:12:03,040
multiple coordinates are mapped into the

327
00:12:03,040 --> 00:12:05,200
same bucket and originally there could

328
00:12:05,200 --> 00:12:07,120
be some difference in those coordinates

329
00:12:07,120 --> 00:12:08,399
and now they're all mapped in the same

330
00:12:08,399 --> 00:12:10,320
bucket they could be positive or

331
00:12:10,320 --> 00:12:11,920
negative and now they're mixed up

332
00:12:11,920 --> 00:12:13,839
together so they could cancel each other

333
00:12:13,839 --> 00:12:14,639
out

334
00:12:14,639 --> 00:12:17,200
so this makes the l1 distance smaller

335
00:12:17,200 --> 00:12:19,040
this is for those who are familiar with

336
00:12:19,040 --> 00:12:20,639
dp this is something like we are

337
00:12:20,639 --> 00:12:22,839
compressing um the

338
00:12:22,839 --> 00:12:25,600
sensitivity and if the l1 distance is

339
00:12:25,600 --> 00:12:27,760
compressed we can actually add smaller

340
00:12:27,760 --> 00:12:30,240
noise to the final bucket

341
00:12:30,240 --> 00:12:32,240
and smaller noise just means smaller

342
00:12:32,240 --> 00:12:33,360
error

343
00:12:33,360 --> 00:12:36,240
with this idea in mind we derive some

344
00:12:36,240 --> 00:12:38,880
parameters and we derive it for low

345
00:12:38,880 --> 00:12:40,880
privacy regime and high privacy region

346
00:12:40,880 --> 00:12:42,399
this part is more technical so i'm going

347
00:12:42,399 --> 00:12:44,000
to skip this

348
00:12:44,000 --> 00:12:46,079
and here's our final theoretical result

349
00:12:46,079 --> 00:12:48,240
for n clients d dimensional k sparse

350
00:12:48,240 --> 00:12:49,680
vector aggregation

351
00:12:49,680 --> 00:12:53,360
and we assume the epsilon is a constant

352
00:12:53,360 --> 00:12:55,120
then in both the event level case and

353
00:12:55,120 --> 00:12:57,440
the user level case our scheme will have

354
00:12:57,440 --> 00:12:59,279
roughly the same communication cost as a

355
00:12:59,279 --> 00:13:01,120
previous result

356
00:13:01,120 --> 00:13:03,279
and our scheme will have

357
00:13:03,279 --> 00:13:04,839
squared k times

358
00:13:04,839 --> 00:13:07,519
error smaller error compared to previous

359
00:13:07,519 --> 00:13:08,560
result

360
00:13:08,560 --> 00:13:12,000
and our scheme actually have the optimal

361
00:13:12,000 --> 00:13:13,120
error

362
00:13:13,120 --> 00:13:14,880
here are two lower bounds the lower

363
00:13:14,880 --> 00:13:18,240
bound for event level ldp is an o result

364
00:13:18,240 --> 00:13:20,720
and the user level ldb part is a new

365
00:13:20,720 --> 00:13:23,040
result in our paper so this the proof

366
00:13:23,040 --> 00:13:25,279
for the lower bound is more technical so

367
00:13:25,279 --> 00:13:27,040
i'm going to skip here for those who are

368
00:13:27,040 --> 00:13:30,160
interested please take a look

369
00:13:30,240 --> 00:13:32,079
here are some experimental results for

370
00:13:32,079 --> 00:13:33,920
this synthetic data set we have ten to

371
00:13:33,920 --> 00:13:35,680
the five client four thousand and ninety

372
00:13:35,680 --> 00:13:38,160
six dimensional vector we're actually

373
00:13:38,160 --> 00:13:40,480
changing this varsity k from very sparse

374
00:13:40,480 --> 00:13:42,399
to very dense we're plotting the l

375
00:13:42,399 --> 00:13:46,000
infinity error here and um our scheme is

376
00:13:46,000 --> 00:13:47,680
the lowest line here

377
00:13:47,680 --> 00:13:49,440
so you can see our scheme actually

378
00:13:49,440 --> 00:13:51,760
outperform other schemes and in a

379
00:13:51,760 --> 00:13:54,079
typical setting where it is 64 sparse so

380
00:13:54,079 --> 00:13:56,560
it's a pretty sparse vector and given

381
00:13:56,560 --> 00:13:59,040
the event level privacy constraint our

382
00:13:59,040 --> 00:14:00,639
scheme shows seven times the reduction

383
00:14:00,639 --> 00:14:02,959
in the l infinity error and the

384
00:14:02,959 --> 00:14:05,279
communication cost per client is only 68

385
00:14:05,279 --> 00:14:07,839
bytes so it's pretty efficient

386
00:14:07,839 --> 00:14:09,839
and for a movie data set

387
00:14:09,839 --> 00:14:11,920
we have a pretty big data set and our

388
00:14:11,920 --> 00:14:13,519
scheme shows four times eight times

389
00:14:13,519 --> 00:14:16,240
reduction in the alien era and our

390
00:14:16,240 --> 00:14:18,639
scheme has the low uh the smallest

391
00:14:18,639 --> 00:14:20,639
communication cost

392
00:14:20,639 --> 00:14:22,959
so yeah that's it um here are some final

393
00:14:22,959 --> 00:14:25,120
takeaway notes sparse vector aggregation

394
00:14:25,120 --> 00:14:27,199
is a very general and powerful primitive

395
00:14:27,199 --> 00:14:28,720
and our algorithm has efficient

396
00:14:28,720 --> 00:14:31,120
communication and nearly optimal error

397
00:14:31,120 --> 00:14:32,800
so for the future work we would like to

398
00:14:32,800 --> 00:14:34,560
do sparse vector aggregation under

399
00:14:34,560 --> 00:14:36,399
different privacy models and we would

400
00:14:36,399 --> 00:14:38,480
like to apply our technique to machine

401
00:14:38,480 --> 00:14:40,000
learning models

402
00:14:40,000 --> 00:14:41,920
so yeah thank you and i very happy to

403
00:14:41,920 --> 00:14:43,680
questions

404
00:14:43,680 --> 00:14:48,000
[Applause]

405
00:14:48,000 --> 00:14:50,160
great thanks for the very nice talk so

406
00:14:50,160 --> 00:14:52,160
are there any questions in the room

407
00:14:52,160 --> 00:14:56,120
please go up to the podium

408
00:14:58,240 --> 00:15:01,360
so i guess oh go for it yeah question um

409
00:15:01,360 --> 00:15:03,040
i'm curious about whether you have to

410
00:15:03,040 --> 00:15:04,399
make any assumptions about the hash

411
00:15:04,399 --> 00:15:07,120
function when you reason about

412
00:15:07,120 --> 00:15:08,800
the reduction

413
00:15:08,800 --> 00:15:10,839
in the l1

414
00:15:10,839 --> 00:15:13,279
sensitivity um

415
00:15:13,279 --> 00:15:15,839
yeah so in actually in our paper we

416
00:15:15,839 --> 00:15:19,440
generally assume um random oracle but uh

417
00:15:19,440 --> 00:15:21,519
we actually have so you can actually use

418
00:15:21,519 --> 00:15:23,440
pseudorandom function so this is what

419
00:15:23,440 --> 00:15:24,800
technology

420
00:15:24,800 --> 00:15:27,760
was done and

421
00:15:27,760 --> 00:15:29,519
we need to assume the pseudorandom

422
00:15:29,519 --> 00:15:31,759
function has some

423
00:15:31,759 --> 00:15:34,160
we actually have some more theoretical

424
00:15:34,160 --> 00:15:36,639
result in our paper that we can actually

425
00:15:36,639 --> 00:15:37,839
use

426
00:15:37,839 --> 00:15:39,920
something like

427
00:15:39,920 --> 00:15:43,120
smaller independency so we know that prf

428
00:15:43,120 --> 00:15:44,720
or something like that we'll have like

429
00:15:44,720 --> 00:15:45,440
some

430
00:15:45,440 --> 00:15:47,839
k independency or something like that so

431
00:15:47,839 --> 00:15:49,759
actually there's some previous papers

432
00:15:49,759 --> 00:15:52,079
show that if you only need a load

433
00:15:52,079 --> 00:15:54,320
balancing you don't need full

434
00:15:54,320 --> 00:15:56,399
independency you can actually need uh

435
00:15:56,399 --> 00:15:58,720
something like log rhythmic independency

436
00:15:58,720 --> 00:16:00,639
and it is enough

437
00:16:00,639 --> 00:16:03,360
so but this part is more technical so

438
00:16:03,360 --> 00:16:06,480
yeah very cool thanks yeah thank you

439
00:16:06,480 --> 00:16:09,040
great there additional questions

440
00:16:09,040 --> 00:16:10,959
so i have one so

441
00:16:10,959 --> 00:16:12,800
uh can you say a little bit about if you

442
00:16:12,800 --> 00:16:14,560
wanted to extend your work into

443
00:16:14,560 --> 00:16:16,480
computing statistical measures on sparse

444
00:16:16,480 --> 00:16:18,639
vectors do your techniques generalize or

445
00:16:18,639 --> 00:16:20,959
what would be needed to support more

446
00:16:20,959 --> 00:16:23,120
expressive queries than just aggregation

447
00:16:23,120 --> 00:16:24,320
um

448
00:16:24,320 --> 00:16:26,720
do you mean we can do some statistical

449
00:16:26,720 --> 00:16:28,880
analysis and suppose you want to do more

450
00:16:28,880 --> 00:16:30,959
compute more complicated functions on

451
00:16:30,959 --> 00:16:32,320
sparse vectors

452
00:16:32,320 --> 00:16:35,360
oh i see uh

453
00:16:35,360 --> 00:16:38,000
yeah maybe we need different results but

454
00:16:38,000 --> 00:16:41,040
the encoding scheme itself is um

455
00:16:41,040 --> 00:16:42,720
is there so you can actually use that

456
00:16:42,720 --> 00:16:44,639
scheme to encode the vectors and still

457
00:16:44,639 --> 00:16:46,880
send it to the server but because now

458
00:16:46,880 --> 00:16:48,720
we're doing a very simple aggregation

459
00:16:48,720 --> 00:16:51,600
task such that it can simply be

460
00:16:51,600 --> 00:16:54,240
add up together and some

461
00:16:54,240 --> 00:16:55,600
the error will be canceled out of each

462
00:16:55,600 --> 00:16:57,120
other but if we are doing more

463
00:16:57,120 --> 00:16:59,519
complicated tasks we need to analyze

464
00:16:59,519 --> 00:17:01,759
what kinds of tests we are doing and we

465
00:17:01,759 --> 00:17:04,079
need to analyze what kinds of error

466
00:17:04,079 --> 00:17:05,679
we're coming from

467
00:17:05,679 --> 00:17:08,559
the encoding so so the encoding so here

468
00:17:08,559 --> 00:17:11,039
we are doing a very simple summation so

469
00:17:11,039 --> 00:17:12,480
the analysis

470
00:17:12,480 --> 00:17:15,919
um will be much similar much simpler

471
00:17:15,919 --> 00:17:16,720
than

472
00:17:16,720 --> 00:17:19,199
maybe more complicated task

473
00:17:19,199 --> 00:17:21,360
so

474
00:17:21,439 --> 00:17:24,720
are there any other questions

475
00:17:25,760 --> 00:17:27,039
if not then let's thank the speaker

476
00:17:27,039 --> 00:17:30,919
again thank you

