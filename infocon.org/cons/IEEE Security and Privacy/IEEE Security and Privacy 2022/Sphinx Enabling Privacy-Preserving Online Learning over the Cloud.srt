1
00:00:01,120 --> 00:00:03,600
my name is han tian a phd student from

2
00:00:03,600 --> 00:00:05,600
hong kong university of science and

3
00:00:05,600 --> 00:00:06,879
technology

4
00:00:06,879 --> 00:00:08,800
today i would like to introduce our

5
00:00:08,800 --> 00:00:11,360
research which introduces new privacy

6
00:00:11,360 --> 00:00:13,200
preserving online learning framework

7
00:00:13,200 --> 00:00:14,639
named sphinx

8
00:00:14,639 --> 00:00:16,960
with things we can upload our data to

9
00:00:16,960 --> 00:00:18,960
the cloud for machine learning training

10
00:00:18,960 --> 00:00:21,920
and inference services without exposing

11
00:00:21,920 --> 00:00:23,600
our user privacy

12
00:00:23,600 --> 00:00:25,599
nowadays machine learning has become the

13
00:00:25,599 --> 00:00:27,920
fundamental infrastructure and core

14
00:00:27,920 --> 00:00:30,480
functionality for many applications

15
00:00:30,480 --> 00:00:32,960
however users have to upload their

16
00:00:32,960 --> 00:00:34,880
individual data to the cloud which

17
00:00:34,880 --> 00:00:36,880
raises privacy consent

18
00:00:36,880 --> 00:00:38,960
our goal is to provide a practical

19
00:00:38,960 --> 00:00:40,640
privacy preserving online learning

20
00:00:40,640 --> 00:00:43,040
service over the cloud where the user

21
00:00:43,040 --> 00:00:45,520
can incrementally update the model and

22
00:00:45,520 --> 00:00:47,680
perform prediction tasks without

23
00:00:47,680 --> 00:00:50,000
worrying about privacy issues

24
00:00:50,000 --> 00:00:52,320
the online setting is as shown in the

25
00:00:52,320 --> 00:00:53,199
figure

26
00:00:53,199 --> 00:00:54,800
the model should keep updated

27
00:00:54,800 --> 00:00:57,600
continuously with new training data and

28
00:00:57,600 --> 00:00:59,680
is unable to perform real-time inference

29
00:00:59,680 --> 00:01:02,320
for user demands during the procedure

30
00:01:02,320 --> 00:01:04,479
they upload the input data as well as

31
00:01:04,479 --> 00:01:06,799
the model or should be protected against

32
00:01:06,799 --> 00:01:09,200
the service provider

33
00:01:09,200 --> 00:01:11,920
to ensure privacy guarantees researchers

34
00:01:11,920 --> 00:01:13,119
have proposed a lot of

35
00:01:13,119 --> 00:01:14,560
privacy-preserving machine learning

36
00:01:14,560 --> 00:01:16,880
algorithms either for training or

37
00:01:16,880 --> 00:01:18,080
influence

38
00:01:18,080 --> 00:01:20,159
however to the best of our knowledge

39
00:01:20,159 --> 00:01:22,159
none of them can provide online learning

40
00:01:22,159 --> 00:01:24,320
capability over the cloud

41
00:01:24,320 --> 00:01:26,240
approaches for training either are

42
00:01:26,240 --> 00:01:28,960
inefficient or make extra absorptions

43
00:01:28,960 --> 00:01:31,680
such as multiple non-convoluting servers

44
00:01:31,680 --> 00:01:33,840
or trusty third party

45
00:01:33,840 --> 00:01:36,159
approaches only for inference are fast

46
00:01:36,159 --> 00:01:38,479
however they generally assume a plain

47
00:01:38,479 --> 00:01:41,040
public model on the server which is not

48
00:01:41,040 --> 00:01:43,040
available for training where the model

49
00:01:43,040 --> 00:01:45,360
parameters will be updated and need to

50
00:01:45,360 --> 00:01:47,680
be protected as well

51
00:01:47,680 --> 00:01:50,079
to fill the gap between solutions with

52
00:01:50,079 --> 00:01:52,880
different cryptographic tools let's take

53
00:01:52,880 --> 00:01:55,600
a closer look at previous approaches

54
00:01:55,600 --> 00:01:58,320
on one hand in order to protect the

55
00:01:58,320 --> 00:02:01,040
trend model approaches for training use

56
00:02:01,040 --> 00:02:03,439
cryptographic tools to encrypt the modal

57
00:02:03,439 --> 00:02:04,960
parameters

58
00:02:04,960 --> 00:02:07,439
thus the model training and inference

59
00:02:07,439 --> 00:02:10,080
require irrespective operations between

60
00:02:10,080 --> 00:02:12,000
gravitational inputs and incremental

61
00:02:12,000 --> 00:02:15,280
multiple parameters which is often slow

62
00:02:15,280 --> 00:02:17,360
on the other hand privacy preserving

63
00:02:17,360 --> 00:02:19,599
inference approaches assume a public

64
00:02:19,599 --> 00:02:22,640
model stored as plain text in the server

65
00:02:22,640 --> 00:02:24,480
thus they only need to consider to

66
00:02:24,480 --> 00:02:26,400
optimize the operation between

67
00:02:26,400 --> 00:02:28,400
incremental inputs and plan model

68
00:02:28,400 --> 00:02:31,360
parameters which is often faster

69
00:02:31,360 --> 00:02:33,519
therefore there is a conflict between

70
00:02:33,519 --> 00:02:35,920
achieving fast training and inference

71
00:02:35,920 --> 00:02:39,040
and preserving motor privacy

72
00:02:39,040 --> 00:02:40,720
our solution is to introduce

73
00:02:40,720 --> 00:02:42,640
differential privacy differential

74
00:02:42,640 --> 00:02:44,400
privacy is a privacy preserving

75
00:02:44,400 --> 00:02:47,440
technique that aids random noise on data

76
00:02:47,440 --> 00:02:49,680
to confuse the attackers

77
00:02:49,680 --> 00:02:52,000
differential privacy protects messages

78
00:02:52,000 --> 00:02:54,640
without cryptographic methods and use

79
00:02:54,640 --> 00:02:57,360
plain text instead of cipher text

80
00:02:57,360 --> 00:02:59,680
thus we found that it's a suitable tool

81
00:02:59,680 --> 00:03:01,280
to fill in the gap

82
00:03:01,280 --> 00:03:03,360
if we can protect the model as plain

83
00:03:03,360 --> 00:03:06,159
text use differential privacy maybe we

84
00:03:06,159 --> 00:03:08,319
can bridge the two types of approaches

85
00:03:08,319 --> 00:03:11,760
together for an online learning solution

86
00:03:11,760 --> 00:03:14,159
however differential privacy also has

87
00:03:14,159 --> 00:03:15,840
its drawbacks

88
00:03:15,840 --> 00:03:18,480
current differential based measures a

89
00:03:18,480 --> 00:03:20,720
zoomer trusted data aggregator to add

90
00:03:20,720 --> 00:03:23,200
noise and have been criticized for its

91
00:03:23,200 --> 00:03:25,280
unsatisfied trade-off between model

92
00:03:25,280 --> 00:03:28,239
performance and privacy preservation

93
00:03:28,239 --> 00:03:30,640
specifically attackers can infer the

94
00:03:30,640 --> 00:03:32,799
training data from the perturbed modal

95
00:03:32,799 --> 00:03:34,159
gradients

96
00:03:34,159 --> 00:03:36,720
sphinx proposed hybrid protocols based

97
00:03:36,720 --> 00:03:38,159
on differential privacy and

98
00:03:38,159 --> 00:03:40,640
cryptographic tools to solve the above

99
00:03:40,640 --> 00:03:41,840
problems

100
00:03:41,840 --> 00:03:43,760
it strike a balance between model

101
00:03:43,760 --> 00:03:46,400
performance computational efficiency and

102
00:03:46,400 --> 00:03:48,799
privacy preservation

103
00:03:48,799 --> 00:03:52,080
here let's see how swings works

104
00:03:52,080 --> 00:03:54,799
at its core sphinx divides each linear

105
00:03:54,799 --> 00:03:57,280
layer in deep neural networks into two

106
00:03:57,280 --> 00:04:00,040
parts and protects them with different

107
00:04:00,040 --> 00:04:02,239
privacy-preserving techniques

108
00:04:02,239 --> 00:04:04,720
given one input sample the forwarding

109
00:04:04,720 --> 00:04:07,040
operation for each convolutional or

110
00:04:07,040 --> 00:04:09,840
fully connected layer can be formalized

111
00:04:09,840 --> 00:04:11,519
as this equation

112
00:04:11,519 --> 00:04:14,720
where w is a linear weight matrix b is

113
00:04:14,720 --> 00:04:17,440
the bias the f denotes the activation

114
00:04:17,440 --> 00:04:20,478
function plus some extra function such

115
00:04:20,478 --> 00:04:22,160
as polling function

116
00:04:22,160 --> 00:04:24,400
we call all the w in the model the

117
00:04:24,400 --> 00:04:27,040
linear components and the b the bios

118
00:04:27,040 --> 00:04:28,400
components

119
00:04:28,400 --> 00:04:31,360
sphinx encrypts all the bias components

120
00:04:31,360 --> 00:04:34,240
with homomorphic encryption and perturbs

121
00:04:34,240 --> 00:04:36,479
the linear components with differential

122
00:04:36,479 --> 00:04:39,199
privacy therefore the best components

123
00:04:39,199 --> 00:04:41,680
stored on the server are several texts

124
00:04:41,680 --> 00:04:43,759
and the linear components are stored in

125
00:04:43,759 --> 00:04:45,759
noisy plan texts

126
00:04:45,759 --> 00:04:48,320
this coil feature enables fast training

127
00:04:48,320 --> 00:04:51,360
and inference protocol designs

128
00:04:51,360 --> 00:04:53,199
as shown in the figure

129
00:04:53,199 --> 00:04:55,360
the w in those white boxes are the

130
00:04:55,360 --> 00:04:57,520
linear components in plain text and

131
00:04:57,520 --> 00:04:59,680
those biases in blue boxes are

132
00:04:59,680 --> 00:05:01,759
incorporated by us components

133
00:05:01,759 --> 00:05:04,720
fddos the activation and extra function

134
00:05:04,720 --> 00:05:06,800
during the training procedure the client

135
00:05:06,800 --> 00:05:09,120
will encrypt the training data and then

136
00:05:09,120 --> 00:05:11,360
send it to the server as the input of

137
00:05:11,360 --> 00:05:12,639
the first layer

138
00:05:12,639 --> 00:05:14,240
and then the forwarding and back

139
00:05:14,240 --> 00:05:16,400
propagation procedures involve the

140
00:05:16,400 --> 00:05:18,639
homomorphic operations between the

141
00:05:18,639 --> 00:05:20,479
encrypted features and the two

142
00:05:20,479 --> 00:05:21,680
components

143
00:05:21,680 --> 00:05:23,759
after the gradients are calculated we

144
00:05:23,759 --> 00:05:25,680
send the encrypted linear gradients back

145
00:05:25,680 --> 00:05:28,160
to the client to apply different privacy

146
00:05:28,160 --> 00:05:30,560
mechanism by adding noise

147
00:05:30,560 --> 00:05:33,039
these noisy decorative gradients are

148
00:05:33,039 --> 00:05:34,800
then sent back to updated model

149
00:05:34,800 --> 00:05:37,360
parameters on the server

150
00:05:37,360 --> 00:05:38,960
the following and by propagation

151
00:05:38,960 --> 00:05:40,960
operations are shown here

152
00:05:40,960 --> 00:05:44,160
we can see with the w unencrypted the

153
00:05:44,160 --> 00:05:46,639
homomorphic matrix multiplications

154
00:05:46,639 --> 00:05:49,039
between ciphertexts are replaced with

155
00:05:49,039 --> 00:05:51,039
multiplications between ciphertext and

156
00:05:51,039 --> 00:05:54,160
plain text which is several times faster

157
00:05:54,160 --> 00:05:58,080
and results in faster training speed

158
00:05:58,080 --> 00:06:00,240
the server trends neural networks with

159
00:06:00,240 --> 00:06:02,479
stochastic gradient descent

160
00:06:02,479 --> 00:06:04,560
it iterates on many badges of the

161
00:06:04,560 --> 00:06:06,720
training data set to update model

162
00:06:06,720 --> 00:06:08,080
parameters

163
00:06:08,080 --> 00:06:10,400
at its train step the client takes a

164
00:06:10,400 --> 00:06:12,080
random sample batch

165
00:06:12,080 --> 00:06:13,680
with size b

166
00:06:13,680 --> 00:06:16,479
equates and sends it to the server

167
00:06:16,479 --> 00:06:18,639
based on the input batch the server

168
00:06:18,639 --> 00:06:20,639
computes the encrypted gradients

169
00:06:20,639 --> 00:06:22,639
following the forwarding and propagation

170
00:06:22,639 --> 00:06:25,039
formulas described before

171
00:06:25,039 --> 00:06:27,520
for the encrypted bias components the

172
00:06:27,520 --> 00:06:29,840
server can aggregate and averages the

173
00:06:29,840 --> 00:06:32,400
gradients locally and directly updates

174
00:06:32,400 --> 00:06:34,800
the model parameters for the linear

175
00:06:34,800 --> 00:06:36,800
components the server sends their

176
00:06:36,800 --> 00:06:38,800
gradients back to the client for

177
00:06:38,800 --> 00:06:40,000
decryption

178
00:06:40,000 --> 00:06:41,759
the client decreases them with the

179
00:06:41,759 --> 00:06:44,479
private key clips the gradients and

180
00:06:44,479 --> 00:06:47,520
gaussian noise on the average gradients

181
00:06:47,520 --> 00:06:49,919
the perturbed gradients are then sent

182
00:06:49,919 --> 00:06:52,319
back to the server to update the linear

183
00:06:52,319 --> 00:06:54,080
components

184
00:06:54,080 --> 00:06:56,479
with the help of homomorphic encryption

185
00:06:56,479 --> 00:06:58,400
we can apply differential privacy on the

186
00:06:58,400 --> 00:07:01,520
gradients without a trusted aggregator

187
00:07:01,520 --> 00:07:03,039
the reason is that

188
00:07:03,039 --> 00:07:05,360
generally to add noise on aggregate

189
00:07:05,360 --> 00:07:07,520
gradients we need a trusted data

190
00:07:07,520 --> 00:07:10,080
aggregator to aggregate the gradients

191
00:07:10,080 --> 00:07:12,639
and apply different privacy noise as

192
00:07:12,639 --> 00:07:14,479
shown on the left here

193
00:07:14,479 --> 00:07:17,039
however in our framework the input data

194
00:07:17,039 --> 00:07:19,440
as well as the gradients are processed

195
00:07:19,440 --> 00:07:22,240
homomorphically and thus are not exposed

196
00:07:22,240 --> 00:07:23,520
to the server

197
00:07:23,520 --> 00:07:25,360
moreover the differential privacy

198
00:07:25,360 --> 00:07:28,479
mechanism is applied on the client side

199
00:07:28,479 --> 00:07:30,240
so only the part two of the gradients

200
00:07:30,240 --> 00:07:32,880
are exposed to the server and we apply

201
00:07:32,880 --> 00:07:35,039
differential privacy without the trusted

202
00:07:35,039 --> 00:07:38,800
party as shown on the right

203
00:07:38,800 --> 00:07:41,120
we also design specific protocol for

204
00:07:41,120 --> 00:07:44,080
inference based on secret sharing by

205
00:07:44,080 --> 00:07:46,160
storing the linear components as plain

206
00:07:46,160 --> 00:07:47,039
text

207
00:07:47,039 --> 00:07:49,599
we allow the use of techniques used in

208
00:07:49,599 --> 00:07:51,840
previous inference protocols that were

209
00:07:51,840 --> 00:07:54,240
only available with the assumption of a

210
00:07:54,240 --> 00:07:56,240
public plan model

211
00:07:56,240 --> 00:07:58,000
here we give an overview of our

212
00:07:58,000 --> 00:08:00,160
inference protocol which is split into

213
00:08:00,160 --> 00:08:01,599
two phases

214
00:08:01,599 --> 00:08:04,160
the preprocessing phase is the dependent

215
00:08:04,160 --> 00:08:07,599
of the user input for every linear layer

216
00:08:07,599 --> 00:08:10,240
the client generates a random mask r

217
00:08:10,240 --> 00:08:12,960
encrypted and sent into the server

218
00:08:12,960 --> 00:08:15,360
the server generates a mask ace and

219
00:08:15,360 --> 00:08:17,919
performs homomorphic mean operation to

220
00:08:17,919 --> 00:08:19,759
generate w

221
00:08:19,759 --> 00:08:23,039
times r plus b minus s and sends it back

222
00:08:23,039 --> 00:08:24,560
to the client

223
00:08:24,560 --> 00:08:26,319
now the two parties hold the secret

224
00:08:26,319 --> 00:08:29,919
share of w times r plus b

225
00:08:29,919 --> 00:08:32,000
in the online phase with the user input

226
00:08:32,000 --> 00:08:35,279
x the client minor seats with the mask

227
00:08:35,279 --> 00:08:38,159
are prepared in the preprocessing phase

228
00:08:38,159 --> 00:08:40,719
and sends it to the server the server

229
00:08:40,719 --> 00:08:43,440
performs linear operation in plain text

230
00:08:43,440 --> 00:08:46,240
by only multiplying w notice that we

231
00:08:46,240 --> 00:08:48,880
don't need to add several text b

232
00:08:48,880 --> 00:08:50,880
now the two parties hold the secret

233
00:08:50,880 --> 00:08:54,399
share of w times x plus d

234
00:08:54,399 --> 00:08:56,080
the server then

235
00:08:56,080 --> 00:08:59,040
sends its share back to the clients so

236
00:08:59,040 --> 00:09:01,680
the clients can add them together to get

237
00:09:01,680 --> 00:09:03,920
the linear operation results

238
00:09:03,920 --> 00:09:06,160
the interface is free of homomorphic

239
00:09:06,160 --> 00:09:08,320
cooperation and thus can achieve very

240
00:09:08,320 --> 00:09:10,640
fast online inference response

241
00:09:10,640 --> 00:09:12,880
the influenced data is protected with

242
00:09:12,880 --> 00:09:15,279
secret sharing and thus has a higher

243
00:09:15,279 --> 00:09:17,680
privacy preservation level than the

244
00:09:17,680 --> 00:09:19,760
training data which is partially

245
00:09:19,760 --> 00:09:22,800
protected with differential privacy

246
00:09:22,800 --> 00:09:25,279
moreover we also propose several system

247
00:09:25,279 --> 00:09:27,519
optimizations for sphinx to further

248
00:09:27,519 --> 00:09:30,320
accelerate the model training procedure

249
00:09:30,320 --> 00:09:32,000
the first optimization is faster

250
00:09:32,000 --> 00:09:34,160
homomorphic multiplication

251
00:09:34,160 --> 00:09:36,800
we use ckks as a homomorphic encryption

252
00:09:36,800 --> 00:09:39,200
scheme where after every several thanks

253
00:09:39,200 --> 00:09:40,880
to several texas multiplication

254
00:09:40,880 --> 00:09:42,839
operation we're scaling and

255
00:09:42,839 --> 00:09:45,600
relinearization operations are needed to

256
00:09:45,600 --> 00:09:47,920
enable further multiplication

257
00:09:47,920 --> 00:09:50,320
sphinx deliberately refines the

258
00:09:50,320 --> 00:09:52,720
homomorphic operation behaviors for

259
00:09:52,720 --> 00:09:54,800
every operation in the forwarding and

260
00:09:54,800 --> 00:09:57,920
the back propagation phases to avoid or

261
00:09:57,920 --> 00:10:00,399
delay rescaling and really analyzation

262
00:10:00,399 --> 00:10:02,720
as much as possible the details are

263
00:10:02,720 --> 00:10:04,560
given in the paper

264
00:10:04,560 --> 00:10:07,040
second right now because homomorphic

265
00:10:07,040 --> 00:10:08,880
encryption cannot handle nonlinear

266
00:10:08,880 --> 00:10:09,839
function

267
00:10:09,839 --> 00:10:12,079
other than polynomials we need the

268
00:10:12,079 --> 00:10:14,560
client's help to evaluate the nonlinear

269
00:10:14,560 --> 00:10:16,880
activation functions

270
00:10:16,880 --> 00:10:18,880
because the activation function inputs

271
00:10:18,880 --> 00:10:20,959
and the outputs in the forwarding phase

272
00:10:20,959 --> 00:10:22,959
are usually also needed again in the

273
00:10:22,959 --> 00:10:25,120
back propagation phase to calculate the

274
00:10:25,120 --> 00:10:27,600
gradients spence adopts a forward

275
00:10:27,600 --> 00:10:29,839
propagation cache in the forwarding

276
00:10:29,839 --> 00:10:32,079
phase it calculates the gradients of the

277
00:10:32,079 --> 00:10:34,320
activation functions beforehand and

278
00:10:34,320 --> 00:10:36,160
stores them in the cache

279
00:10:36,160 --> 00:10:38,720
these stored gradients are then directly

280
00:10:38,720 --> 00:10:41,200
used in the back propagation phase to

281
00:10:41,200 --> 00:10:43,120
avoid duplicate computations and

282
00:10:43,120 --> 00:10:47,279
communications between the two parties

283
00:10:47,279 --> 00:10:49,200
the final two small optimizations are

284
00:10:49,200 --> 00:10:50,880
called zero decryption and zero

285
00:10:50,880 --> 00:10:52,320
encryption

286
00:10:52,320 --> 00:10:54,399
in the training protocol decryption and

287
00:10:54,399 --> 00:10:56,800
array encryption occur in every

288
00:10:56,800 --> 00:10:59,519
ciphertext sent to the client either for

289
00:10:59,519 --> 00:11:01,920
nonlinear activation function evaluation

290
00:11:01,920 --> 00:11:04,240
or differential privacy mechanism

291
00:11:04,240 --> 00:11:06,079
because we realize that no more

292
00:11:06,079 --> 00:11:08,399
arithmetic operations are needed for

293
00:11:08,399 --> 00:11:11,600
these send ciphertexts before decryption

294
00:11:11,600 --> 00:11:14,240
we adopt zero decryption to reduce the

295
00:11:14,240 --> 00:11:16,320
level of physics ready to be sent

296
00:11:16,320 --> 00:11:18,240
several times to zero

297
00:11:18,240 --> 00:11:20,399
here the level determines the number of

298
00:11:20,399 --> 00:11:22,880
homomorphic multiplication allowed as

299
00:11:22,880 --> 00:11:25,839
well as the size of the ciphertext

300
00:11:25,839 --> 00:11:28,880
so by reducing the level to zero we

301
00:11:28,880 --> 00:11:31,040
results in a much smaller several taxes

302
00:11:31,040 --> 00:11:33,440
size during the communication

303
00:11:33,440 --> 00:11:35,279
for the array encryption part on the

304
00:11:35,279 --> 00:11:37,839
client side we realize that homomorphic

305
00:11:37,839 --> 00:11:39,760
addition between ciphertext and the

306
00:11:39,760 --> 00:11:42,000
plaintext is much faster than the

307
00:11:42,000 --> 00:11:44,160
encryption operation as shown in the

308
00:11:44,160 --> 00:11:45,360
table here

309
00:11:45,360 --> 00:11:47,920
thus sphinx adopts an encryption method

310
00:11:47,920 --> 00:11:50,160
we called zero encryption

311
00:11:50,160 --> 00:11:52,240
in the offline pre-processing phase the

312
00:11:52,240 --> 00:11:55,040
client prepared a stream of zero cell

313
00:11:55,040 --> 00:11:57,760
text by encrypting zeros

314
00:11:57,760 --> 00:11:59,600
which works as one time past several

315
00:11:59,600 --> 00:12:00,639
texts

316
00:12:00,639 --> 00:12:02,399
so in the ifs

317
00:12:02,399 --> 00:12:04,560
once the client need to encrypt a

318
00:12:04,560 --> 00:12:05,519
message

319
00:12:05,519 --> 00:12:08,240
it encrypted the plain text by directly

320
00:12:08,240 --> 00:12:10,639
adding it with a prepared zero seven

321
00:12:10,639 --> 00:12:11,519
test

322
00:12:11,519 --> 00:12:13,680
thus shortening the encryption time in

323
00:12:13,680 --> 00:12:15,920
the online phase

324
00:12:15,920 --> 00:12:18,320
we have implemented the sphinx with c

325
00:12:18,320 --> 00:12:20,639
plus pass based on the c life with

326
00:12:20,639 --> 00:12:23,120
machine learning framework key a n

327
00:12:23,120 --> 00:12:25,920
we use the ckkshv scheme

328
00:12:25,920 --> 00:12:29,279
implemented in cr 3.6

329
00:12:29,279 --> 00:12:31,680
we evaluated strings on two famous data

330
00:12:31,680 --> 00:12:34,639
sets amnes and server 10 with two

331
00:12:34,639 --> 00:12:36,240
convolutional neural network

332
00:12:36,240 --> 00:12:38,959
architectures

333
00:12:38,959 --> 00:12:40,959
for the privacy preserving training we

334
00:12:40,959 --> 00:12:42,639
compiled this pure homomorphic

335
00:12:42,639 --> 00:12:45,120
encryption training protocols we found

336
00:12:45,120 --> 00:12:47,920
that sphinx achieves 35 times less

337
00:12:47,920 --> 00:12:50,160
training time and five times lower

338
00:12:50,160 --> 00:12:52,160
communication cost for both neural

339
00:12:52,160 --> 00:12:54,959
networks under two data sets than the

340
00:12:54,959 --> 00:12:58,560
pure homomorphic encryption method

341
00:12:58,560 --> 00:13:01,120
for private inference we compared with

342
00:13:01,120 --> 00:13:02,959
both pure homomorphic encryption

343
00:13:02,959 --> 00:13:05,200
measures as well as those private

344
00:13:05,200 --> 00:13:06,720
inference particles

345
00:13:06,720 --> 00:13:09,120
we found that sphinx achieves real-time

346
00:13:09,120 --> 00:13:11,600
influence in the online phase which is

347
00:13:11,600 --> 00:13:14,000
four orders of magnitude faster compared

348
00:13:14,000 --> 00:13:16,800
to the pure homomorphic reaction method

349
00:13:16,800 --> 00:13:18,880
similar with private inference protocol

350
00:13:18,880 --> 00:13:20,639
such as their fee

351
00:13:20,639 --> 00:13:23,360
however definitely assume a public plan

352
00:13:23,360 --> 00:13:25,519
model on the server

353
00:13:25,519 --> 00:13:26,959
for the evaluation of differential

354
00:13:26,959 --> 00:13:29,120
privacy mechanism used in the training

355
00:13:29,120 --> 00:13:31,360
protocol we draw the trade-off curve

356
00:13:31,360 --> 00:13:33,440
between model accuracy and different

357
00:13:33,440 --> 00:13:35,600
privacy level with different datasets

358
00:13:35,600 --> 00:13:38,079
and hyperparameters we found that given

359
00:13:38,079 --> 00:13:40,639
the same privacy budget sphinx achieves

360
00:13:40,639 --> 00:13:42,800
a similar model accuracy as the pure

361
00:13:42,800 --> 00:13:44,959
differential privacy training algorithm

362
00:13:44,959 --> 00:13:48,399
without a trustworthy server

363
00:13:48,399 --> 00:13:50,399
finally we use the practical

364
00:13:50,399 --> 00:13:53,199
reconstruction attacks to attack sphinx

365
00:13:53,199 --> 00:13:56,160
and pure differential privacy solutions

366
00:13:56,160 --> 00:13:58,399
the measure tried to restore the input

367
00:13:58,399 --> 00:14:01,199
images from the noisy gradient the

368
00:14:01,199 --> 00:14:03,680
result showed that even with similar

369
00:14:03,680 --> 00:14:05,920
theoretical privacy cost compared to

370
00:14:05,920 --> 00:14:08,560
different privacy solutions sphinx can

371
00:14:08,560 --> 00:14:11,199
achieve a significantly stronger defense

372
00:14:11,199 --> 00:14:13,600
against practical attacks

373
00:14:13,600 --> 00:14:15,839
because it protects parts of the model

374
00:14:15,839 --> 00:14:17,120
parameters

375
00:14:17,120 --> 00:14:19,519
the best components with homomorphic

376
00:14:19,519 --> 00:14:22,880
encryption against the attackers

377
00:14:22,880 --> 00:14:25,440
okay so the summaries we present sphinx

378
00:14:25,440 --> 00:14:27,680
a hybrid privacy preserving our learning

379
00:14:27,680 --> 00:14:30,480
framework sphinx has significantly

380
00:14:30,480 --> 00:14:32,399
accelerated the privacy preserving

381
00:14:32,399 --> 00:14:34,560
training and influence speed thus

382
00:14:34,560 --> 00:14:36,240
enabling privacy preserving online

383
00:14:36,240 --> 00:14:38,399
learning service over the cloud the

384
00:14:38,399 --> 00:14:40,320
paper is online and we will release the

385
00:14:40,320 --> 00:14:45,800
code soon so that's all thank you

386
00:14:50,079 --> 00:14:52,240
all right any questions please come up

387
00:14:52,240 --> 00:14:55,639
to the microphone

388
00:14:58,000 --> 00:15:00,959
hi nice talk i'm ming xin from cmu so i

389
00:15:00,959 --> 00:15:03,600
wonder what's the sigma in your ppt

390
00:15:03,600 --> 00:15:06,320
because generally we have

391
00:15:06,320 --> 00:15:07,440
um

392
00:15:07,440 --> 00:15:09,760
like epsilon delta differential privacy

393
00:15:09,760 --> 00:15:12,320
where epsilon is smaller it means higher

394
00:15:12,320 --> 00:15:14,959
privacy but in your slides it seems it's

395
00:15:14,959 --> 00:15:17,439
reversed

396
00:15:20,480 --> 00:15:22,320
you mean you mean the third you mean in

397
00:15:22,320 --> 00:15:23,600
the

398
00:15:23,600 --> 00:15:26,399
differential privacy evaluation the data

399
00:15:26,399 --> 00:15:28,639
yeah

400
00:15:34,959 --> 00:15:36,720
could you repeat your question again

401
00:15:36,720 --> 00:15:38,480
because uh

402
00:15:38,480 --> 00:15:41,040
obviously no

403
00:15:41,199 --> 00:15:44,560
so um what's your privacy parameters

404
00:15:44,560 --> 00:15:46,480
so what does that

405
00:15:46,480 --> 00:15:49,120
sigma mean

406
00:15:49,279 --> 00:15:50,079
oh

407
00:15:50,079 --> 00:15:52,959
oh the the sigma the sigma here means

408
00:15:52,959 --> 00:15:56,320
the you know because for a differential

409
00:15:56,320 --> 00:15:59,839
privacy level on the original one we

410
00:15:59,839 --> 00:16:02,320
only have epsilon writes the epsilon

411
00:16:02,320 --> 00:16:04,560
parameter but um

412
00:16:04,560 --> 00:16:06,079
afterwards

413
00:16:06,079 --> 00:16:08,560
and but and later there's another

414
00:16:08,560 --> 00:16:11,519
differential privacy um mechanism based

415
00:16:11,519 --> 00:16:15,279
on two um parameters epsilon and dirt

416
00:16:15,279 --> 00:16:17,759
has so the data means that there may be

417
00:16:17,759 --> 00:16:20,639
a small probability that uh

418
00:16:20,639 --> 00:16:21,600
the

419
00:16:21,600 --> 00:16:23,920
the information will be linked

420
00:16:23,920 --> 00:16:24,959
so

421
00:16:24,959 --> 00:16:28,240
and i mean you know ipster means the uh

422
00:16:28,240 --> 00:16:30,800
the similar the similarity between q

423
00:16:30,800 --> 00:16:32,800
distributions right

424
00:16:32,800 --> 00:16:35,199
that one is a very small probability

425
00:16:35,199 --> 00:16:37,920
that the two uh

426
00:16:37,920 --> 00:16:39,839
the two probably the

427
00:16:39,839 --> 00:16:42,560
description the distribution may be

428
00:16:42,560 --> 00:16:44,800
uh totally different so

429
00:16:44,800 --> 00:16:47,040
and the details for the dts maybe you

430
00:16:47,040 --> 00:16:50,800
can refer to some um different privacy

431
00:16:50,800 --> 00:16:51,839
papers

432
00:16:51,839 --> 00:16:53,920
okay yeah we can talk that later

433
00:16:53,920 --> 00:16:55,759
yeah

434
00:16:55,759 --> 00:16:58,639
hello i'm from uc berkeley i have two

435
00:16:58,639 --> 00:17:00,480
questions the first one is that is your

436
00:17:00,480 --> 00:17:02,000
approach not suitable for a setting

437
00:17:02,000 --> 00:17:04,880
where the server's model is proprietary

438
00:17:04,880 --> 00:17:06,799
and clients cannot see it because i

439
00:17:06,799 --> 00:17:08,559
think in your approach you send back the

440
00:17:08,559 --> 00:17:10,079
gradients to the client and client is

441
00:17:10,079 --> 00:17:11,839
responsible for adding the

442
00:17:11,839 --> 00:17:13,760
noise and the client does the decryption

443
00:17:13,760 --> 00:17:16,319
so client basically sees

444
00:17:16,319 --> 00:17:18,559
the weights and everything so

445
00:17:18,559 --> 00:17:20,319
it kind of seems that it won't work in a

446
00:17:20,319 --> 00:17:22,720
setting where you

447
00:17:22,720 --> 00:17:24,480
have a propriety model and you don't

448
00:17:24,480 --> 00:17:25,439
want to share it with us with the

449
00:17:25,439 --> 00:17:27,119
clients right

450
00:17:27,119 --> 00:17:28,160
yeah yeah

451
00:17:28,160 --> 00:17:29,520
and then how do you how do you compute

452
00:17:29,520 --> 00:17:31,760
activations or encrypted data like did

453
00:17:31,760 --> 00:17:35,120
you do you actually change your model to

454
00:17:35,120 --> 00:17:39,280
make all the activation functions linear

455
00:17:40,880 --> 00:17:42,720
yeah okay so so for the first question

456
00:17:42,720 --> 00:17:45,600
so in our use case um

457
00:17:45,600 --> 00:17:46,559
because

458
00:17:46,559 --> 00:17:49,440
the model is updated by the data

459
00:17:49,440 --> 00:17:50,720
uploaded by

460
00:17:50,720 --> 00:17:51,520
um

461
00:17:51,520 --> 00:17:53,760
by the client so here in fact we will

462
00:17:53,760 --> 00:17:56,240
assume that we do not protect the model

463
00:17:56,240 --> 00:17:58,559
against the client because all the data

464
00:17:58,559 --> 00:18:00,400
comes from the client so

465
00:18:00,400 --> 00:18:01,600
um

466
00:18:01,600 --> 00:18:04,320
you know so so here we associate it's

467
00:18:04,320 --> 00:18:07,440
okay for our 18 hours an hour to for the

468
00:18:07,440 --> 00:18:08,880
client to see the

469
00:18:08,880 --> 00:18:11,600
gradients so and for the second question

470
00:18:11,600 --> 00:18:15,120
um the activation here in fact it is

471
00:18:15,120 --> 00:18:18,880
we use a client aid model so because we

472
00:18:18,880 --> 00:18:21,520
we denote some some researchers and

473
00:18:21,520 --> 00:18:23,039
transform the activation into

474
00:18:23,039 --> 00:18:25,120
polynomials but here we just use the

475
00:18:25,120 --> 00:18:27,440
original polynomial of we just use the

476
00:18:27,440 --> 00:18:29,600
problem original activation functions

477
00:18:29,600 --> 00:18:32,960
and we and and the server need to send

478
00:18:32,960 --> 00:18:35,440
this um

479
00:18:35,440 --> 00:18:38,480
sentences data back to the client and

480
00:18:38,480 --> 00:18:42,000
the client will decrypt them and perform

481
00:18:42,000 --> 00:18:43,919
activations on them and

482
00:18:43,919 --> 00:18:46,080
encrypt them again and send them back to

483
00:18:46,080 --> 00:18:49,600
the to server and this is in fact uh

484
00:18:49,600 --> 00:18:51,280
um

485
00:18:51,280 --> 00:18:54,080
and and it will be improved if the

486
00:18:54,080 --> 00:18:56,480
if this problem of homomorphic

487
00:18:56,480 --> 00:18:58,559
incorruption will be solved uh in the

488
00:18:58,559 --> 00:19:02,799
future yeah yeah cool got it

489
00:19:07,360 --> 00:19:09,520
all right so nice talk so i have a quick

490
00:19:09,520 --> 00:19:12,000
question so looks like in your protocol

491
00:19:12,000 --> 00:19:13,840
the client needs to be

492
00:19:13,840 --> 00:19:16,720
online during training

493
00:19:16,720 --> 00:19:17,520
so

494
00:19:17,520 --> 00:19:19,919
do you have any numbers on how much

495
00:19:19,919 --> 00:19:21,679
computation the client needs to be done

496
00:19:21,679 --> 00:19:23,120
compared to

497
00:19:23,120 --> 00:19:25,280
if the all the training was happening in

498
00:19:25,280 --> 00:19:27,919
client itself on without doing any

499
00:19:27,919 --> 00:19:29,200
homophobic encryption or differential

500
00:19:29,200 --> 00:19:31,679
privacy

501
00:19:31,760 --> 00:19:34,880
uh so you mean the converge time of the

502
00:19:34,880 --> 00:19:35,840
model

503
00:19:35,840 --> 00:19:37,679
so the so now the client is doing some

504
00:19:37,679 --> 00:19:39,919
computation during training

505
00:19:39,919 --> 00:19:40,799
so

506
00:19:40,799 --> 00:19:42,320
the other scenario is alternative

507
00:19:42,320 --> 00:19:45,200
scenario is there is no server at all

508
00:19:45,200 --> 00:19:46,960
or the server is only doing some storage

509
00:19:46,960 --> 00:19:49,039
so the training can happen in client

510
00:19:49,039 --> 00:19:50,640
itself right without doing any

511
00:19:50,640 --> 00:19:53,200
homomorphic encryption

512
00:19:53,200 --> 00:19:54,480
so what's

513
00:19:54,480 --> 00:19:55,360
the

514
00:19:55,360 --> 00:19:56,400
time

515
00:19:56,400 --> 00:19:58,320
without so so compared to that

516
00:19:58,320 --> 00:19:59,760
computational time we expect the

517
00:19:59,760 --> 00:20:01,520
client's computation time in this case

518
00:20:01,520 --> 00:20:03,679
should be smaller than

519
00:20:03,679 --> 00:20:06,240
the plane training on client itself

520
00:20:06,240 --> 00:20:08,480
right

521
00:20:09,039 --> 00:20:13,039
uh yes in fact it is true um

522
00:20:13,039 --> 00:20:14,720
uh

523
00:20:14,720 --> 00:20:16,799
but uh

524
00:20:16,799 --> 00:20:18,880
i say i i i

525
00:20:18,880 --> 00:20:20,720
do not have the the

526
00:20:20,720 --> 00:20:22,159
data now the

527
00:20:22,159 --> 00:20:25,200
i think we have done that before but uh

528
00:20:25,200 --> 00:20:27,360
we haven't put that kind of

529
00:20:27,360 --> 00:20:30,320
data in our paper before

530
00:20:30,320 --> 00:20:32,960
um maybe you can send me an email and we

531
00:20:32,960 --> 00:20:34,960
can you know

532
00:20:34,960 --> 00:20:38,240
send this data to you about yeah so

533
00:20:38,240 --> 00:20:40,240
okay so but it is

534
00:20:40,240 --> 00:20:42,640
so your case client is now computation

535
00:20:42,640 --> 00:20:43,679
is much

536
00:20:43,679 --> 00:20:44,640
is much

537
00:20:44,640 --> 00:20:46,320
lower than doing the training on the

538
00:20:46,320 --> 00:20:48,320
client itself right yeah yeah i agree

539
00:20:48,320 --> 00:20:50,320
with you so yeah

540
00:20:50,320 --> 00:20:52,720
but here because it's not our privacy

541
00:20:52,720 --> 00:20:55,360
consent so

542
00:20:57,679 --> 00:20:59,919
all righty thank you so much for joining

543
00:20:59,919 --> 00:21:02,240
us remotely and for answering questions

544
00:21:02,240 --> 00:21:03,919
so let's give the speaker one last round

545
00:21:03,919 --> 00:21:07,000
of applause

