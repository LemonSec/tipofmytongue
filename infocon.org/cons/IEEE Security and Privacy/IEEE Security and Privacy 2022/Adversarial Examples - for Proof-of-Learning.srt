1
00:00:01,280 --> 00:00:03,120
i have the plethora of delivering our

2
00:00:03,120 --> 00:00:05,839
work titled adversarial examples for

3
00:00:05,839 --> 00:00:07,359
proof of learning

4
00:00:07,359 --> 00:00:09,760
what motivated our work was two major

5
00:00:09,760 --> 00:00:12,240
challenges in machine learning the first

6
00:00:12,240 --> 00:00:13,759
is a challenge of machine learning

7
00:00:13,759 --> 00:00:15,920
intellectual property resolution which

8
00:00:15,920 --> 00:00:18,400
asks the question how can a model owner

9
00:00:18,400 --> 00:00:20,720
prove that they produce the model this

10
00:00:20,720 --> 00:00:23,119
is an open and difficult challenge in ml

11
00:00:23,119 --> 00:00:25,039
that rather is due to the popular model

12
00:00:25,039 --> 00:00:26,800
extraction attack

13
00:00:26,800 --> 00:00:29,439
given some publicly hosted model an

14
00:00:29,439 --> 00:00:31,439
adversary employing this attack can

15
00:00:31,439 --> 00:00:33,680
reverse engineering a model by covering

16
00:00:33,680 --> 00:00:34,719
it

17
00:00:34,719 --> 00:00:37,120
here an arbitrator may be needed to

18
00:00:37,120 --> 00:00:39,680
facilitate ownership resolution

19
00:00:39,680 --> 00:00:41,200
the second challenge surrounds

20
00:00:41,200 --> 00:00:43,360
presenting workers in distributed and

21
00:00:43,360 --> 00:00:45,680
collaborative learning setups which are

22
00:00:45,680 --> 00:00:47,520
of growing popularity in the machine

23
00:00:47,520 --> 00:00:49,120
learning community

24
00:00:49,120 --> 00:00:51,039
here we see a central server with

25
00:00:51,039 --> 00:00:53,680
distributed workers across the bottom

26
00:00:53,680 --> 00:00:55,680
we pounded what happened when balancing

27
00:00:55,680 --> 00:00:58,000
workers deviated from the designated

28
00:00:58,000 --> 00:01:00,239
collaborative protocol and then asked

29
00:01:00,239 --> 00:01:02,640
ourselves is there a way to verify

30
00:01:02,640 --> 00:01:05,360
computations made by distributed workers

31
00:01:05,360 --> 00:01:07,520
there are literally related works that

32
00:01:07,520 --> 00:01:09,439
address these questions of ownership

33
00:01:09,439 --> 00:01:11,360
resolution and presenting workers in

34
00:01:11,360 --> 00:01:13,200
collaborative learning

35
00:01:13,200 --> 00:01:14,880
these are watermarking defenses and

36
00:01:14,880 --> 00:01:16,640
fingerprinting for the former are

37
00:01:16,640 --> 00:01:19,439
verifiable computations for the latter

38
00:01:19,439 --> 00:01:21,280
however both water marking and

39
00:01:21,280 --> 00:01:23,040
fingerprinting leads to unfavorable

40
00:01:23,040 --> 00:01:25,280
tradeoffs between their success and the

41
00:01:25,280 --> 00:01:27,759
model utility meaning that it can be

42
00:01:27,759 --> 00:01:29,920
difficult to employ these solutions

43
00:01:29,920 --> 00:01:31,759
while attaining similar performance

44
00:01:31,759 --> 00:01:33,040
models

45
00:01:33,040 --> 00:01:34,560
they also require training time

46
00:01:34,560 --> 00:01:36,880
modifications limiting their general

47
00:01:36,880 --> 00:01:39,119
applicability

48
00:01:39,119 --> 00:01:41,200
verifiable computations require

49
00:01:41,200 --> 00:01:43,439
inference time alterations such as

50
00:01:43,439 --> 00:01:45,759
changes to architectures or restrictions

51
00:01:45,759 --> 00:01:48,240
on what operations can be used

52
00:01:48,240 --> 00:01:50,159
prior approaches towards solving the

53
00:01:50,159 --> 00:01:52,320
aforementioned problems have several

54
00:01:52,320 --> 00:01:53,600
shortcomings

55
00:01:53,600 --> 00:01:55,840
to create a verifiable proof for their

56
00:01:55,840 --> 00:01:58,240
efforts in training a model ja atta

57
00:01:58,240 --> 00:02:00,880
proposed a concept named proof of

58
00:02:00,880 --> 00:02:02,240
learning

59
00:02:02,240 --> 00:02:04,799
with these verifiable proofs arbitrators

60
00:02:04,799 --> 00:02:06,640
can use these proofs to resolve

61
00:02:06,640 --> 00:02:09,199
ownership conflicts all central servers

62
00:02:09,199 --> 00:02:11,440
could use them to verify if delegated

63
00:02:11,440 --> 00:02:14,800
computation was performed correctly

64
00:02:14,800 --> 00:02:17,440
proof of learning of pol is defined as a

65
00:02:17,440 --> 00:02:18,680
table

66
00:02:18,680 --> 00:02:20,239
w-i-h-a

67
00:02:20,239 --> 00:02:23,440
where w is the sequence of modulates i

68
00:02:23,440 --> 00:02:26,000
is a set of data indices used to learn

69
00:02:26,000 --> 00:02:28,400
the width and a is the accelerated

70
00:02:28,400 --> 00:02:30,239
information that is necessary for

71
00:02:30,239 --> 00:02:31,680
training

72
00:02:31,680 --> 00:02:33,760
note that this is enough for producing

73
00:02:33,760 --> 00:02:35,360
and verifying a

74
00:02:35,360 --> 00:02:37,040
for a module that is trained on a

75
00:02:37,040 --> 00:02:39,440
publicly available data set

76
00:02:39,440 --> 00:02:41,840
if the training data is private however

77
00:02:41,840 --> 00:02:43,760
approval is required to provide

78
00:02:43,760 --> 00:02:46,000
signature of the data points an

79
00:02:46,000 --> 00:02:48,959
adversary might wish to prove a purell

80
00:02:48,959 --> 00:02:51,120
proof by spending less computation and

81
00:02:51,120 --> 00:02:53,760
storage than that made by private in

82
00:02:53,760 --> 00:02:56,160
generating the pure proof

83
00:02:56,160 --> 00:02:59,040
by stochastic spoofing adversary could

84
00:02:59,040 --> 00:03:02,720
produce a valid pol for fwt but it is

85
00:03:02,720 --> 00:03:05,440
different from the one produced by provo

86
00:03:05,440 --> 00:03:08,560
which means f w t prime equals f w t

87
00:03:08,560 --> 00:03:12,239
well t prime less than t

88
00:03:13,040 --> 00:03:15,840
pol assumes that an adversary is capable

89
00:03:15,840 --> 00:03:17,840
of the following capabilities

90
00:03:17,840 --> 00:03:20,239
first it has full knowledge of the model

91
00:03:20,239 --> 00:03:22,400
architecture moderates loss function and

92
00:03:22,400 --> 00:03:24,159
other hyper parameters

93
00:03:24,159 --> 00:03:26,400
second a has four access to the training

94
00:03:26,400 --> 00:03:29,200
data set d and can modify it and a has

95
00:03:29,200 --> 00:03:30,879
no knowledge of proven

96
00:03:30,879 --> 00:03:33,120
strategies about batching parameter

97
00:03:33,120 --> 00:03:35,599
initialization random generation and so

98
00:03:35,599 --> 00:03:36,480
on

99
00:03:36,480 --> 00:03:38,159
note that the second assumption is

100
00:03:38,159 --> 00:03:40,560
essential in our text because our

101
00:03:40,560 --> 00:03:43,040
methods are based on the adversarial

102
00:03:43,040 --> 00:03:45,680
examples of the origin data

103
00:03:45,680 --> 00:03:47,920
for the creation of a proof of learning

104
00:03:47,920 --> 00:03:50,720
proof it is equivalent to recording the

105
00:03:50,720 --> 00:03:52,319
chain of the updates

106
00:03:52,319 --> 00:03:55,040
however to prevent the size of the proof

107
00:03:55,040 --> 00:03:57,120
from being prohibitively large the

108
00:03:57,120 --> 00:03:59,280
approval can record the reads only arrow

109
00:03:59,280 --> 00:04:02,959
k steps then p1 p2 until pk minus 1 are

110
00:04:02,959 --> 00:04:07,200
removed leaving only p0 pk p2k and so on

111
00:04:07,200 --> 00:04:09,200
this is done for every epic of the

112
00:04:09,200 --> 00:04:11,120
training

113
00:04:11,120 --> 00:04:13,439
the verification mechanism of the proof

114
00:04:13,439 --> 00:04:16,560
of learning is divided into two parts

115
00:04:16,560 --> 00:04:19,120
first for the initial state of the proof

116
00:04:19,120 --> 00:04:22,479
p0 to prevent adversary using a stolen

117
00:04:22,479 --> 00:04:25,040
model as initial state the chemical of

118
00:04:25,040 --> 00:04:28,240
smaranov test is used to test if a set

119
00:04:28,240 --> 00:04:30,880
of values is sampled from a certain

120
00:04:30,880 --> 00:04:32,800
distributions

121
00:04:32,800 --> 00:04:35,919
second stepwise verification is used to

122
00:04:35,919 --> 00:04:37,919
verify the rest part

123
00:04:37,919 --> 00:04:40,320
in high level the verifiable verifies

124
00:04:40,320 --> 00:04:42,720
approved by reproducing the training and

125
00:04:42,720 --> 00:04:45,520
say if they get the same final model

126
00:04:45,520 --> 00:04:47,360
when training a model from the same

127
00:04:47,360 --> 00:04:50,479
initial state and use the same data for

128
00:04:50,479 --> 00:04:53,120
error key step the verifier check if the

129
00:04:53,120 --> 00:04:55,360
reproduced model is closed to the

130
00:04:55,360 --> 00:04:58,639
corresponding model state in the proof

131
00:04:58,639 --> 00:05:01,199
if so the verifier loads the motor state

132
00:05:01,199 --> 00:05:03,520
from the proof and try to reproduce the

133
00:05:03,520 --> 00:05:06,479
next k step based on this state and so

134
00:05:06,479 --> 00:05:08,320
on

135
00:05:08,320 --> 00:05:11,120
by estimating the entropy of the k step

136
00:05:11,120 --> 00:05:13,199
of time t which called epsilon

137
00:05:13,199 --> 00:05:14,560
reproduction t

138
00:05:14,560 --> 00:05:17,759
and set a data that much less than d ref

139
00:05:17,759 --> 00:05:19,840
where draft is the entropy when

140
00:05:19,840 --> 00:05:22,240
retraining the model it is easy to

141
00:05:22,240 --> 00:05:25,280
verify the proof

142
00:05:25,280 --> 00:05:27,919
now let me start to discuss our attack

143
00:05:27,919 --> 00:05:29,199
intuition

144
00:05:29,199 --> 00:05:30,880
note that the verification is to

145
00:05:30,880 --> 00:05:32,800
reproduce part of the model with given

146
00:05:32,800 --> 00:05:35,440
start model and data what if we could

147
00:05:35,440 --> 00:05:36,880
generate certain weights with real

148
00:05:36,880 --> 00:05:38,479
well-designed data

149
00:05:38,479 --> 00:05:40,880
if so we can generate a sequence of

150
00:05:40,880 --> 00:05:43,520
intentional weeds leading to wt which

151
00:05:43,520 --> 00:05:45,520
could be much shorter than the original

152
00:05:45,520 --> 00:05:46,720
sequence

153
00:05:46,720 --> 00:05:49,039
an adversarial example is an instance

154
00:05:49,039 --> 00:05:50,960
added with small and intentional

155
00:05:50,960 --> 00:05:53,199
preservations so that a machine learning

156
00:05:53,199 --> 00:05:56,400
model will make a false prediction on it

157
00:05:56,400 --> 00:05:58,720
in a similar way as optimizing an

158
00:05:58,720 --> 00:06:01,199
adversarial example we could make an

159
00:06:01,199 --> 00:06:03,759
arbitrarily chosen data point generate a

160
00:06:03,759 --> 00:06:04,840
given

161
00:06:04,840 --> 00:06:07,919
model take the step from w0 as an

162
00:06:07,919 --> 00:06:10,560
example like normal training process we

163
00:06:10,560 --> 00:06:13,840
compute the loss and gradient dw

164
00:06:13,840 --> 00:06:16,319
using gradient descent to get next with

165
00:06:16,319 --> 00:06:20,880
w0 plus eta dw0 which is denoted as w1

166
00:06:20,880 --> 00:06:21,680
head

167
00:06:21,680 --> 00:06:23,919
and minimize the distance between this

168
00:06:23,919 --> 00:06:28,080
weight and the given weight w1

169
00:06:28,160 --> 00:06:30,880
similar to the generation of adversarial

170
00:06:30,880 --> 00:06:32,080
examples

171
00:06:32,080 --> 00:06:34,560
we only optimize the perturbation and in

172
00:06:34,560 --> 00:06:36,639
the same time we minimize the

173
00:06:36,639 --> 00:06:39,600
perturbation itself

174
00:06:39,600 --> 00:06:41,840
by optimizing the perturbation to find

175
00:06:41,840 --> 00:06:44,880
the minimum of this objective function

176
00:06:44,880 --> 00:06:48,639
w1 head is getting closer to w1 and the

177
00:06:48,639 --> 00:06:52,400
perturbation is getting smaller

178
00:06:53,599 --> 00:06:57,280
finally w1 head equals to w1 and the

179
00:06:57,280 --> 00:06:59,520
perturbation is small enough to be

180
00:06:59,520 --> 00:07:02,080
indistinguishable

181
00:07:02,080 --> 00:07:04,160
this progress of generating their

182
00:07:04,160 --> 00:07:06,639
perturbation is called adversarial

183
00:07:06,639 --> 00:07:08,400
optimization

184
00:07:08,400 --> 00:07:11,039
however each adversarial optimization

185
00:07:11,039 --> 00:07:13,599
needs a lot of computation and the total

186
00:07:13,599 --> 00:07:16,240
computation is very large even if we

187
00:07:16,240 --> 00:07:19,680
design a shorter sequence of weights

188
00:07:19,680 --> 00:07:22,319
to accelerate adversarial optimization

189
00:07:22,319 --> 00:07:24,800
instead of sampling models randomly we

190
00:07:24,800 --> 00:07:26,720
sampled the intermediate modal weights

191
00:07:26,720 --> 00:07:29,680
such that the distance between wt and wt

192
00:07:29,680 --> 00:07:32,639
minus k less than delta and much less

193
00:07:32,639 --> 00:07:35,360
than epsilon reproduction t where t mod

194
00:07:35,360 --> 00:07:38,319
k equals zero

195
00:07:39,120 --> 00:07:41,759
practically we can initialize the width

196
00:07:41,759 --> 00:07:44,080
after w0 by equally dividing the

197
00:07:44,080 --> 00:07:46,960
distance between w0 and wt

198
00:07:46,960 --> 00:07:49,599
if the number of steps first proving for

199
00:07:49,599 --> 00:07:52,720
example t prime is large enough in other

200
00:07:52,720 --> 00:07:55,440
words there are enough wt's the

201
00:07:55,440 --> 00:07:58,240
condition distance between wt and wt

202
00:07:58,240 --> 00:08:01,199
minus k less than delta can be trivially

203
00:08:01,199 --> 00:08:03,039
satisfied

204
00:08:03,039 --> 00:08:05,840
another major change in basic attack is

205
00:08:05,840 --> 00:08:08,240
that a optimize the perturbation by

206
00:08:08,240 --> 00:08:13,280
minimize dw0 plus perturbation

207
00:08:13,280 --> 00:08:15,599
these two changes make the adverse0

208
00:08:15,599 --> 00:08:18,879
optimization become easier to converge

209
00:08:18,879 --> 00:08:21,840
normally it takes no more than 10 steps

210
00:08:21,840 --> 00:08:24,160
of advanced server optimization to get a

211
00:08:24,160 --> 00:08:26,319
convergence result

212
00:08:26,319 --> 00:08:28,960
notice that in basic attack we have to

213
00:08:28,960 --> 00:08:31,520
generate adversarial examples for every

214
00:08:31,520 --> 00:08:32,479
step

215
00:08:32,479 --> 00:08:34,958
an advanced attack is proposed to

216
00:08:34,958 --> 00:08:36,880
optimize all key batteries of data

217
00:08:36,880 --> 00:08:39,120
points together

218
00:08:39,120 --> 00:08:41,760
in basic attack we need adversarial

219
00:08:41,760 --> 00:08:45,440
optimization for w0 to get w1 for w1 to

220
00:08:45,440 --> 00:08:48,240
get w2 until we get wk

221
00:08:48,240 --> 00:08:51,120
in fact we could optimize the key steps

222
00:08:51,120 --> 00:08:54,160
of advanced zero optimization ones

223
00:08:54,160 --> 00:08:57,600
from w0 to wk directly

224
00:08:57,600 --> 00:08:59,760
we formally prove that the extra error

225
00:08:59,760 --> 00:09:03,680
cost by advanced attack is acceptable

226
00:09:03,680 --> 00:09:05,839
the details of the proof are discussed

227
00:09:05,839 --> 00:09:08,160
in the paper

228
00:09:08,160 --> 00:09:10,640
then we can compare the cost of the

229
00:09:10,640 --> 00:09:13,279
gradient calculations in the por proof

230
00:09:13,279 --> 00:09:15,600
and in our attacks

231
00:09:15,600 --> 00:09:18,160
in the purell proof it needs three times

232
00:09:18,160 --> 00:09:20,080
of iteration

233
00:09:20,080 --> 00:09:23,440
in basic attack it needs t prime plus t

234
00:09:23,440 --> 00:09:26,480
primes time n times adder times of

235
00:09:26,480 --> 00:09:28,320
gradient calculations

236
00:09:28,320 --> 00:09:31,200
where n equals 10 and adverse zero

237
00:09:31,200 --> 00:09:33,440
optimization needs three times of

238
00:09:33,440 --> 00:09:35,600
gradient calculations

239
00:09:35,600 --> 00:09:38,480
in advanced attack it needs t prime

240
00:09:38,480 --> 00:09:41,760
divided by k plus t prime divided by k

241
00:09:41,760 --> 00:09:44,720
times n times edible times of gradient

242
00:09:44,720 --> 00:09:46,160
calculations

243
00:09:46,160 --> 00:09:49,600
same with basic attack n equals 10 and l

244
00:09:49,600 --> 00:09:52,399
equals three

245
00:09:52,399 --> 00:09:55,839
impractical t is said to be 78 000 in

246
00:09:55,839 --> 00:09:57,440
pure l proof

247
00:09:57,440 --> 00:10:01,680
so we set t prime less than 2516

248
00:10:01,680 --> 00:10:04,399
for basic attack and the t prime less

249
00:10:04,399 --> 00:10:05,320
than

250
00:10:05,320 --> 00:10:06,880
251

251
00:10:06,880 --> 00:10:09,760
600 for advanced attack

252
00:10:09,760 --> 00:10:12,480
can provide us proof with less number of

253
00:10:12,480 --> 00:10:15,040
gradient calculations compared to the

254
00:10:15,040 --> 00:10:17,279
pol proof

255
00:10:17,279 --> 00:10:19,920
now let us see some evaluations we

256
00:10:19,920 --> 00:10:22,240
conduct our tags on separation cipher

257
00:10:22,240 --> 00:10:26,079
100 and a subset of imagenet dataset and

258
00:10:26,079 --> 00:10:29,279
we show our evaluations on safer 100 as

259
00:10:29,279 --> 00:10:31,440
examples

260
00:10:31,440 --> 00:10:33,760
for basic attack we can see that the

261
00:10:33,760 --> 00:10:35,760
time of attack and the time of pure

262
00:10:35,760 --> 00:10:38,320
proof generation crosses when t prime

263
00:10:38,320 --> 00:10:40,320
equals 1700

264
00:10:40,320 --> 00:10:42,079
and the other two cross points are

265
00:10:42,079 --> 00:10:45,200
cosine norm and l1 ohm epsilon at 1300

266
00:10:45,200 --> 00:10:47,279
and 700 respectively

267
00:10:47,279 --> 00:10:49,839
so setting t prime smaller than 700

268
00:10:49,839 --> 00:10:52,320
where larger than 1300 can be a valid

269
00:10:52,320 --> 00:10:53,200
attack

270
00:10:53,200 --> 00:10:55,760
for advanced attack t prime smaller than

271
00:10:55,760 --> 00:10:58,399
4000 is effective for spoof

272
00:10:58,399 --> 00:11:00,079
and the other two cross points are

273
00:11:00,079 --> 00:11:03,440
cosine knob and l1 ohm epsilon at 1300

274
00:11:03,440 --> 00:11:05,760
and 700 respectively

275
00:11:05,760 --> 00:11:08,640
so to generate a valid proof we need to

276
00:11:08,640 --> 00:11:11,120
set t prime smaller than 4000 we are

277
00:11:11,120 --> 00:11:14,079
larger than 1300.

278
00:11:14,079 --> 00:11:16,480
recall that it is assumed that a has

279
00:11:16,480 --> 00:11:18,560
four access to the training data set and

280
00:11:18,560 --> 00:11:20,160
can modify it

281
00:11:20,160 --> 00:11:22,959
an implicit assumption is that v does

282
00:11:22,959 --> 00:11:25,360
not know the data set beforehand

283
00:11:25,360 --> 00:11:27,600
otherwise the attack can easily define

284
00:11:27,600 --> 00:11:29,760
it by checking the integrity of the data

285
00:11:29,760 --> 00:11:31,279
set

286
00:11:31,279 --> 00:11:33,839
this assumption is realistic

287
00:11:33,839 --> 00:11:35,680
consider the scenario where two

288
00:11:35,680 --> 00:11:38,640
hospitals share data with each other

289
00:11:38,640 --> 00:11:40,880
so that they can treat models separately

290
00:11:40,880 --> 00:11:43,839
for online diagnosis

291
00:11:44,240 --> 00:11:46,560
suppose hospital 1 trains a good model

292
00:11:46,560 --> 00:11:48,720
and hospital 2 which is the attacker

293
00:11:48,720 --> 00:11:50,639
wants to claim the ownership of this

294
00:11:50,639 --> 00:11:51,839
model

295
00:11:51,839 --> 00:11:54,639
then hospital 1 provides a purell proof

296
00:11:54,639 --> 00:11:56,800
and hospital 2 provides a pure else

297
00:11:56,800 --> 00:11:58,079
proof

298
00:11:58,079 --> 00:12:00,240
in this example a knows the training

299
00:12:00,240 --> 00:12:02,720
data set but ray does not

300
00:12:02,720 --> 00:12:05,440
however we could still check if one data

301
00:12:05,440 --> 00:12:07,920
set is an adversarial perturbation of

302
00:12:07,920 --> 00:12:09,600
the other

303
00:12:09,600 --> 00:12:12,800
this may introduce a new armatures

304
00:12:12,800 --> 00:12:15,680
it would need to evade both pol and this

305
00:12:15,680 --> 00:12:17,600
second verifier

306
00:12:17,600 --> 00:12:19,680
to this end we evaluate the

307
00:12:19,680 --> 00:12:22,399
effectiveness of our attacks onto

308
00:12:22,399 --> 00:12:25,680
non-overlapping datasets

309
00:12:25,680 --> 00:12:28,639
we split the dataset civerton into two

310
00:12:28,639 --> 00:12:30,880
non-overlapping datasets

311
00:12:30,880 --> 00:12:32,880
the approval and

312
00:12:32,880 --> 00:12:35,680
adversary generate proof and spoof with

313
00:12:35,680 --> 00:12:38,480
these two different datasets in this

314
00:12:38,480 --> 00:12:39,279
case

315
00:12:39,279 --> 00:12:41,839
verify can never know which data set is

316
00:12:41,839 --> 00:12:44,959
an adversarial perturbation

317
00:12:44,959 --> 00:12:46,880
here we see the two cross points for

318
00:12:46,880 --> 00:12:50,320
basic attack is t prime equals 5000 for

319
00:12:50,320 --> 00:12:52,399
the time evaluation

320
00:12:52,399 --> 00:12:56,079
and t prime equals 600 for the infinity

321
00:12:56,079 --> 00:12:58,320
norm

322
00:12:58,320 --> 00:13:01,200
so we can prove pl when t prime smaller

323
00:13:01,200 --> 00:13:06,600
than 5000 and larger than 600.

324
00:13:07,600 --> 00:13:10,560
for advanced attack the cross prongs of

325
00:13:10,560 --> 00:13:13,360
time evaluation increases to t prime

326
00:13:13,360 --> 00:13:16,399
equals 5900

327
00:13:16,399 --> 00:13:19,680
and it reminds t prime equals 600 for

328
00:13:19,680 --> 00:13:22,079
the infinity norm

329
00:13:22,079 --> 00:13:26,079
so t prime can be smaller than 5900

330
00:13:26,079 --> 00:13:28,880
we are larger than 600 for advanced

331
00:13:28,880 --> 00:13:31,880
attack

332
00:13:34,160 --> 00:13:36,079
there are still two countermeasures for

333
00:13:36,079 --> 00:13:38,800
our attack the first is to design a more

334
00:13:38,800 --> 00:13:41,839
strict selection of cirrus route

335
00:13:41,839 --> 00:13:44,320
as we observed in our experiments

336
00:13:44,320 --> 00:13:46,160
epsilon reproduction in the earlier

337
00:13:46,160 --> 00:13:48,399
training stage is usually larger than

338
00:13:48,399 --> 00:13:50,320
that in the later stage

339
00:13:50,320 --> 00:13:52,560
because the model converges in the later

340
00:13:52,560 --> 00:13:53,760
stage

341
00:13:53,760 --> 00:13:56,160
on the other hand epsilon reproduction

342
00:13:56,160 --> 00:13:59,760
remains in the same level in hours proof

343
00:13:59,760 --> 00:14:01,519
a more sophisticated way would be

344
00:14:01,519 --> 00:14:03,360
dynamically choosing the verification

345
00:14:03,360 --> 00:14:05,440
thresholds according to the stage of

346
00:14:05,440 --> 00:14:06,800
model training

347
00:14:06,800 --> 00:14:08,720
choose larger thresholds for the early

348
00:14:08,720 --> 00:14:10,800
stage and choose smaller structures for

349
00:14:10,800 --> 00:14:12,399
the later stage

350
00:14:12,399 --> 00:14:14,639
however we can also set the difference

351
00:14:14,639 --> 00:14:17,839
between wt and wt minus k closer in the

352
00:14:17,839 --> 00:14:20,000
later stage to circumference this

353
00:14:20,000 --> 00:14:21,519
quantum error

354
00:14:21,519 --> 00:14:23,360
another quantum error is verifiable

355
00:14:23,360 --> 00:14:24,560
computation

356
00:14:24,560 --> 00:14:27,440
we can use spacey to build a secure pol

357
00:14:27,440 --> 00:14:28,720
mechanism

358
00:14:28,720 --> 00:14:30,880
during training model approval generates

359
00:14:30,880 --> 00:14:32,880
a basic proof proving that the final

360
00:14:32,880 --> 00:14:35,600
model wt was resulted by running the

361
00:14:35,600 --> 00:14:37,519
training algorithm on the initial model

362
00:14:37,519 --> 00:14:40,240
w0 and the data set d

363
00:14:40,240 --> 00:14:42,959
however it has to divert on log and

364
00:14:42,959 --> 00:14:46,240
computation to generate a valid vc proof

365
00:14:46,240 --> 00:14:48,880
which is almost equal as t

366
00:14:48,880 --> 00:14:51,120
this mechanism is valid but it will

367
00:14:51,120 --> 00:14:53,839
introduce an overwhelming overhead

368
00:14:53,839 --> 00:14:58,120
thank you for listening our presentation

369
00:15:06,160 --> 00:15:08,639
okay thank you questions

370
00:15:08,639 --> 00:15:11,279
hi nicola papelno from the university of

371
00:15:11,279 --> 00:15:14,560
toronto thank you for the presentation

372
00:15:14,560 --> 00:15:17,120
in the original pol paper we described a

373
00:15:17,120 --> 00:15:20,000
variant where the data is hashed in the

374
00:15:20,000 --> 00:15:22,079
proof could you comment whether your

375
00:15:22,079 --> 00:15:26,000
attack is still applicable in that case

376
00:15:26,000 --> 00:15:27,519
uh

377
00:15:27,519 --> 00:15:29,839
in our works we just

378
00:15:29,839 --> 00:15:31,440
neglected the

379
00:15:31,440 --> 00:15:32,480
the

380
00:15:32,480 --> 00:15:33,759
hash

381
00:15:33,759 --> 00:15:35,279
for some

382
00:15:35,279 --> 00:15:38,279
simplifying

383
00:15:43,839 --> 00:15:46,079
uh

384
00:15:46,160 --> 00:15:49,839
so uh you know what in our works we

385
00:15:49,839 --> 00:15:52,480
follow them for the simply

386
00:15:52,480 --> 00:15:54,800
we we neglect the

387
00:15:54,800 --> 00:15:57,600
hash because we think that

388
00:15:57,600 --> 00:15:59,279
uh

389
00:15:59,279 --> 00:16:01,360
since the dataset are

390
00:16:01,360 --> 00:16:02,959
all

391
00:16:02,959 --> 00:16:03,920
uh

392
00:16:03,920 --> 00:16:07,600
unknown to the verifier so we can build

393
00:16:07,600 --> 00:16:08,880
a

394
00:16:08,880 --> 00:16:10,480
harsh uh

395
00:16:10,480 --> 00:16:11,839
simply

396
00:16:11,839 --> 00:16:14,560
so that's the reason we neglect

397
00:16:14,560 --> 00:16:16,639
the techniques

398
00:16:16,639 --> 00:16:17,839
okay so

399
00:16:17,839 --> 00:16:19,759
i guess the reason i was asking is that

400
00:16:19,759 --> 00:16:21,759
in the paper we described a mechanism

401
00:16:21,759 --> 00:16:22,720
for

402
00:16:22,720 --> 00:16:25,440
the proof to contain a hash so that

403
00:16:25,440 --> 00:16:27,600
the verifier would be the only one given

404
00:16:27,600 --> 00:16:30,959
access to the data when needed on demand

405
00:16:30,959 --> 00:16:33,040
and the adversary would not have access

406
00:16:33,040 --> 00:16:35,199
to the data itself

407
00:16:35,199 --> 00:16:37,199
and i guess for context for people in

408
00:16:37,199 --> 00:16:39,360
the audience i tried to reach out to you

409
00:16:39,360 --> 00:16:41,279
to reproduce the results

410
00:16:41,279 --> 00:16:43,360
and we were not able to so we wrote a

411
00:16:43,360 --> 00:16:44,839
blog post on

412
00:16:44,839 --> 00:16:46,399
cleverhans.io

413
00:16:46,399 --> 00:16:47,920
where we explained why we cannot

414
00:16:47,920 --> 00:16:51,839
reproduce the results in your paper

415
00:16:52,720 --> 00:16:54,320
okay i

416
00:16:54,320 --> 00:16:56,240
i have

417
00:16:56,240 --> 00:17:00,320
i have shared our code on the github

418
00:17:00,320 --> 00:17:02,480
so

419
00:17:02,800 --> 00:17:05,520
so you can reproduce our results with

420
00:17:05,520 --> 00:17:08,480
the the code i shared online

421
00:17:08,480 --> 00:17:11,119
i'll be happy to to try the last time i

422
00:17:11,119 --> 00:17:12,959
wrote to you told me the code is messy

423
00:17:12,959 --> 00:17:13,839
and

424
00:17:13,839 --> 00:17:16,000
didn't respond okay

425
00:17:16,000 --> 00:17:19,240
thank you

426
00:17:21,679 --> 00:17:24,319
other questions

427
00:17:28,880 --> 00:17:30,799
okay i'll ask one

428
00:17:30,799 --> 00:17:33,039
in you describe the adversarial examples

429
00:17:33,039 --> 00:17:35,919
by making imperceptible perturbations

430
00:17:35,919 --> 00:17:38,880
to valid images

431
00:17:38,880 --> 00:17:42,000
i'm curious if it would be

432
00:17:42,000 --> 00:17:44,400
allow the adversary more ability to just

433
00:17:44,400 --> 00:17:46,080
make the images be completely

434
00:17:46,080 --> 00:17:48,160
arbitrarily noisy

435
00:17:48,160 --> 00:17:49,520
instead of having to be adversarial

436
00:17:49,520 --> 00:17:51,200
modifications of clean images is the

437
00:17:51,200 --> 00:17:54,080
reason why you why you added this like

438
00:17:54,080 --> 00:17:57,678
sort of similarity distance as well

439
00:17:58,840 --> 00:18:03,120
uh are you asking about the

440
00:18:03,120 --> 00:18:04,880
the picture

441
00:18:04,880 --> 00:18:06,480
the picture's quality

442
00:18:06,480 --> 00:18:08,240
yeah so so you require that the training

443
00:18:08,240 --> 00:18:11,200
data actually looks visually good

444
00:18:11,200 --> 00:18:13,440
yes um is it

445
00:18:13,440 --> 00:18:14,880
other than the fact that a human might

446
00:18:14,880 --> 00:18:16,960
look at the training data set

447
00:18:16,960 --> 00:18:19,440
like the proof would the proof check

448
00:18:19,440 --> 00:18:21,200
correctly even if the images didn't look

449
00:18:21,200 --> 00:18:23,120
high quality

450
00:18:23,120 --> 00:18:25,360
uh

451
00:18:25,440 --> 00:18:27,360
so the verifier will

452
00:18:27,360 --> 00:18:28,559
check the

453
00:18:28,559 --> 00:18:29,280
uh

454
00:18:29,280 --> 00:18:31,200
will check the data

455
00:18:31,200 --> 00:18:33,760
so since since the verify can get the

456
00:18:33,760 --> 00:18:35,360
data so

457
00:18:35,360 --> 00:18:37,840
and

458
00:18:37,919 --> 00:18:39,039
check

459
00:18:39,039 --> 00:18:40,160
check the

460
00:18:40,160 --> 00:18:43,520
the intimate intermediate models and

461
00:18:43,520 --> 00:18:45,120
their input so

462
00:18:45,120 --> 00:18:46,880
we make the

463
00:18:46,880 --> 00:18:48,400
pictures

464
00:18:48,400 --> 00:18:50,480
very very good looking

465
00:18:50,480 --> 00:18:51,840
otherwise

466
00:18:51,840 --> 00:18:54,480
they will tell the difference from

467
00:18:54,480 --> 00:18:56,880
the from the generated

468
00:18:56,880 --> 00:18:58,799
pictures

469
00:18:58,799 --> 00:19:00,320
okay so yeah so you're worried about

470
00:19:00,320 --> 00:19:02,320
like manual inspection of the data

471
00:19:02,320 --> 00:19:04,559
yes

472
00:19:07,520 --> 00:19:10,760
more questions

473
00:19:14,240 --> 00:19:15,919
sure i'll ask one more then

474
00:19:15,919 --> 00:19:17,120
which i know that you sort of have a

475
00:19:17,120 --> 00:19:18,720
comment on in the paper

476
00:19:18,720 --> 00:19:20,080
i know the original proof of learning

477
00:19:20,080 --> 00:19:22,080
paper also talks about sort of

478
00:19:22,080 --> 00:19:24,880
randomness across gpus

479
00:19:24,880 --> 00:19:26,320
and how sometimes it can be harder on

480
00:19:26,320 --> 00:19:27,840
some hardware but not another hardware

481
00:19:27,840 --> 00:19:29,679
if maybe you want to comment on

482
00:19:29,679 --> 00:19:32,080
on that

483
00:19:32,320 --> 00:19:33,280
uh

484
00:19:33,280 --> 00:19:36,000
yes the different handlers will

485
00:19:36,000 --> 00:19:39,120
have have different capabilities so it's

486
00:19:39,120 --> 00:19:42,080
especially for the advanced attack it

487
00:19:42,080 --> 00:19:44,480
relies greatly on the

488
00:19:44,480 --> 00:19:46,640
handler's capabilities

489
00:19:46,640 --> 00:19:47,520
for

490
00:19:47,520 --> 00:19:49,520
the the larger

491
00:19:49,520 --> 00:19:50,240
uh

492
00:19:50,240 --> 00:19:54,320
for the 32 the 66 72j

493
00:19:54,320 --> 00:19:56,400
capacity will carry

494
00:19:56,400 --> 00:19:57,360
a

495
00:19:57,360 --> 00:19:59,200
more faster

496
00:19:59,200 --> 00:20:01,840
uh optimization

497
00:20:01,840 --> 00:20:03,200
for

498
00:20:03,200 --> 00:20:06,960
uh so in experiments because the limit

499
00:20:06,960 --> 00:20:07,760
of

500
00:20:07,760 --> 00:20:11,039
the capability of gpus we divided the

501
00:20:11,039 --> 00:20:13,039
advanced tax

502
00:20:13,039 --> 00:20:14,240
in fact

503
00:20:14,240 --> 00:20:17,760
we developed divided the attacks in

504
00:20:17,760 --> 00:20:21,840
about 50 25 different steps

505
00:20:21,840 --> 00:20:24,080
so

506
00:20:24,159 --> 00:20:28,320
the gpus are very important

507
00:20:28,480 --> 00:20:31,520
for our attacks

508
00:20:33,600 --> 00:20:34,880
okay let's thank the speaker one more

509
00:20:34,880 --> 00:20:37,880
time

