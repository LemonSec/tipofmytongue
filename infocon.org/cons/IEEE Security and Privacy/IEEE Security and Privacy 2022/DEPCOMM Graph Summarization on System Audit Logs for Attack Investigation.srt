1
00:00:02,639 --> 00:00:03,840
afternoon

2
00:00:03,840 --> 00:00:06,720
the title of my report is deep calm

3
00:00:06,720 --> 00:00:08,880
graph summarization of system product

4
00:00:08,880 --> 00:00:12,240
logs for attack investigation

5
00:00:12,240 --> 00:00:14,559
consensus analysis generates a

6
00:00:14,559 --> 00:00:17,440
dependency graph from system audit logs

7
00:00:17,440 --> 00:00:19,920
which has been an important solution for

8
00:00:19,920 --> 00:00:22,800
attack investigation

9
00:00:22,800 --> 00:00:25,199
despite the promising early results

10
00:00:25,199 --> 00:00:27,760
causality analysis often produces a

11
00:00:27,760 --> 00:00:30,320
large graph and is a daunting task for

12
00:00:30,320 --> 00:00:33,120
security areas to expect such a large

13
00:00:33,120 --> 00:00:34,960
graph

14
00:00:34,960 --> 00:00:38,320
the challenges and key insights is

15
00:00:38,320 --> 00:00:41,160
firstly dependency graph is a type of

16
00:00:41,160 --> 00:00:43,520
heterogeneous web

17
00:00:43,520 --> 00:00:46,320
system activities are often represented

18
00:00:46,320 --> 00:00:48,719
as a set of process nodes that have

19
00:00:48,719 --> 00:00:51,199
either strong correlation with each

20
00:00:51,199 --> 00:00:54,480
other or data dependencies through some

21
00:00:54,480 --> 00:00:56,239
resource nodes

22
00:00:56,239 --> 00:00:59,680
we refer to this type of closely related

23
00:00:59,680 --> 00:01:03,039
processes as intimate processes

24
00:01:03,039 --> 00:01:05,680
thus to address the challenge deepcom

25
00:01:05,680 --> 00:01:08,159
partitions a dependency graph into

26
00:01:08,159 --> 00:01:10,960
process-centric communities where each

27
00:01:10,960 --> 00:01:14,159
community includes a group of intimate

28
00:01:14,159 --> 00:01:15,280
processes

29
00:01:15,280 --> 00:01:18,159
and the system resources accessed by

30
00:01:18,159 --> 00:01:21,119
these processes

31
00:01:21,119 --> 00:01:22,400
secondly

32
00:01:22,400 --> 00:01:24,720
they are often caused by less important

33
00:01:24,720 --> 00:01:27,840
and repetitive system activities such as

34
00:01:27,840 --> 00:01:30,320
backup file updates

35
00:01:30,320 --> 00:01:33,040
deepcom can identify patterns of

36
00:01:33,040 --> 00:01:36,000
repetitive edges and compress them

37
00:01:36,000 --> 00:01:38,880
inside the communities

38
00:01:38,880 --> 00:01:42,479
thirdly major system activities such as

39
00:01:42,479 --> 00:01:45,680
compressive fires and attack behaviors

40
00:01:45,680 --> 00:01:47,840
such as leaking data are often

41
00:01:47,840 --> 00:01:50,960
represented as information flows

42
00:01:50,960 --> 00:01:52,720
connecting communities

43
00:01:52,720 --> 00:01:54,479
and refers as

44
00:01:54,479 --> 00:01:56,240
infopaths

45
00:01:56,240 --> 00:01:59,520
deep com prioritize info paths that are

46
00:01:59,520 --> 00:02:02,399
more likely to represent major system

47
00:02:02,399 --> 00:02:05,439
activities and use them as the community

48
00:02:05,439 --> 00:02:07,759
summary

49
00:02:07,759 --> 00:02:10,639
to address the challenge we propose a

50
00:02:10,639 --> 00:02:13,840
framework deep cop for summarizing the

51
00:02:13,840 --> 00:02:16,640
dependency graph for facilitating attack

52
00:02:16,640 --> 00:02:18,400
recovery

53
00:02:18,400 --> 00:02:20,080
firstly

54
00:02:20,080 --> 00:02:23,280
deepcom applies causality analysis on a

55
00:02:23,280 --> 00:02:24,879
point of interest

56
00:02:24,879 --> 00:02:27,120
event to generate

57
00:02:27,120 --> 00:02:29,120
a dependency graph

58
00:02:29,120 --> 00:02:32,800
secondly merges repetitive edges and

59
00:02:32,800 --> 00:02:35,599
filter read only fires

60
00:02:35,599 --> 00:02:39,360
thirdly partitions a dependency graph as

61
00:02:39,360 --> 00:02:41,840
process-centric communities

62
00:02:41,840 --> 00:02:42,959
and then

63
00:02:42,959 --> 00:02:44,400
compresses

64
00:02:44,400 --> 00:02:47,840
repetitive edges inside a community

65
00:02:47,840 --> 00:02:51,680
lastly identifies top-ranked info paths

66
00:02:51,680 --> 00:02:54,879
as graph summary

67
00:02:54,879 --> 00:02:57,040
for the process-centric community

68
00:02:57,040 --> 00:02:58,640
detection

69
00:02:58,640 --> 00:03:01,680
deepcom first performs random walks on

70
00:03:01,680 --> 00:03:06,159
each process node to obtain work roots

71
00:03:06,159 --> 00:03:09,840
we propose hierarchical work schemes

72
00:03:09,840 --> 00:03:13,040
leverages both local neighbors and the

73
00:03:13,040 --> 00:03:16,239
global process lineage trees to choose

74
00:03:16,239 --> 00:03:18,400
the work roots

75
00:03:18,400 --> 00:03:19,440
then

76
00:03:19,440 --> 00:03:21,680
deepcom computes

77
00:03:21,680 --> 00:03:24,560
the behavior representation for each

78
00:03:24,560 --> 00:03:28,480
process node p by vectorizing these rock

79
00:03:28,480 --> 00:03:30,159
rules

80
00:03:30,159 --> 00:03:34,319
specifically we first make an analogy

81
00:03:34,319 --> 00:03:37,840
by regarding nodes in a dependency graph

82
00:03:37,840 --> 00:03:41,280
as words and walk rules as ordered

83
00:03:41,280 --> 00:03:44,080
sequences of words

84
00:03:44,080 --> 00:03:46,560
deepcom learns the process node

85
00:03:46,560 --> 00:03:49,440
representation from all work rules

86
00:03:49,440 --> 00:03:50,879
containing p

87
00:03:50,879 --> 00:03:54,000
by skip gram a widely used board

88
00:03:54,000 --> 00:03:57,680
representation learning algorithm

89
00:03:57,680 --> 00:04:01,599
next deep com clusters process nodes

90
00:04:01,599 --> 00:04:04,720
based on their behavior representation

91
00:04:04,720 --> 00:04:07,920
to compute the overlapping clustering

92
00:04:07,920 --> 00:04:10,480
for processed nodes based on their

93
00:04:10,480 --> 00:04:12,720
behavior representations

94
00:04:12,720 --> 00:04:17,199
deepcom employs a soft clustering method

95
00:04:17,199 --> 00:04:18,399
fcm

96
00:04:18,399 --> 00:04:22,000
that outputs the membership degree of

97
00:04:22,000 --> 00:04:26,080
each process node in each cluster

98
00:04:26,080 --> 00:04:29,520
lastly deep come as the accessed

99
00:04:29,520 --> 00:04:31,040
resource notes

100
00:04:31,040 --> 00:04:34,080
to the detected communities according to

101
00:04:34,080 --> 00:04:38,639
the connection with the process nodes

102
00:04:38,720 --> 00:04:41,280
for the community compression

103
00:04:41,280 --> 00:04:44,639
deepcom processes the process-based

104
00:04:44,639 --> 00:04:47,440
pattern compression namely repetitive

105
00:04:47,440 --> 00:04:51,120
activities that spawn same set of

106
00:04:51,120 --> 00:04:55,199
processes to process some resources

107
00:04:55,199 --> 00:04:58,160
at the same time deepcom processes the

108
00:04:58,160 --> 00:05:00,960
resource-based pattern compression

109
00:05:00,960 --> 00:05:04,000
namely resources that are repetitively

110
00:05:04,000 --> 00:05:08,639
accessed by a same set of processes

111
00:05:08,639 --> 00:05:11,199
this is an example community dependency

112
00:05:11,199 --> 00:05:12,400
graph

113
00:05:12,400 --> 00:05:15,360
deepcom partitions the dependency graph

114
00:05:15,360 --> 00:05:17,600
to 10 communities

115
00:05:17,600 --> 00:05:22,639
the complete dependency graph has 1038

116
00:05:22,639 --> 00:05:24,520
nodes and

117
00:05:24,520 --> 00:05:26,960
4039 edges

118
00:05:26,960 --> 00:05:29,759
red dashed frames are the communities

119
00:05:29,759 --> 00:05:32,720
with attack related events

120
00:05:32,720 --> 00:05:35,600
blue dashed frames are the communities

121
00:05:35,600 --> 00:05:39,280
with only normal events blue nodes

122
00:05:39,280 --> 00:05:42,320
represent nodes that belong to multiple

123
00:05:42,320 --> 00:05:44,080
communities

124
00:05:44,080 --> 00:05:47,120
after compression the number of edges of

125
00:05:47,120 --> 00:05:51,199
the community seat 9 decreases to 33

126
00:05:51,199 --> 00:05:53,360
from 108

127
00:05:53,360 --> 00:05:56,919
about 69.4

128
00:05:57,120 --> 00:05:58,800
compression rate

129
00:05:58,800 --> 00:06:00,319
similarly

130
00:06:00,319 --> 00:06:04,000
the community c9 is compressed into two

131
00:06:04,000 --> 00:06:10,319
edges from 58 edges about 96.5

132
00:06:10,319 --> 00:06:13,280
compression rate

133
00:06:13,919 --> 00:06:17,039
for the community summarization

134
00:06:17,039 --> 00:06:18,880
deepcom extracts

135
00:06:18,880 --> 00:06:22,400
the info path that is more likely to

136
00:06:22,400 --> 00:06:25,440
represent major system activities or

137
00:06:25,440 --> 00:06:28,400
attack activities

138
00:06:28,400 --> 00:06:30,080
specifically

139
00:06:30,080 --> 00:06:34,479
deepcom uses dfs to find longest paths

140
00:06:34,479 --> 00:06:37,840
from input nodes to output nodes

141
00:06:37,840 --> 00:06:40,880
where the input nodes represent

142
00:06:40,880 --> 00:06:43,520
the incoming information flow for a

143
00:06:43,520 --> 00:06:44,800
community

144
00:06:44,800 --> 00:06:46,960
namely the target node

145
00:06:46,960 --> 00:06:49,599
of an inter-community edge

146
00:06:49,599 --> 00:06:52,240
and the output node represents the

147
00:06:52,240 --> 00:06:54,720
outgoing information flow for a

148
00:06:54,720 --> 00:06:56,000
community

149
00:06:56,000 --> 00:06:58,840
namely the source node of an

150
00:06:58,840 --> 00:07:02,240
intercommunity edge

151
00:07:02,240 --> 00:07:06,000
however there are many info paths

152
00:07:06,000 --> 00:07:08,960
it is necessary to prioritize this info

153
00:07:08,960 --> 00:07:10,319
path

154
00:07:10,319 --> 00:07:13,759
deepcom computes the prioritization

155
00:07:13,759 --> 00:07:17,199
score for every info path based on 4

156
00:07:17,199 --> 00:07:18,880
weighted features

157
00:07:18,880 --> 00:07:21,280
including poi event

158
00:07:21,280 --> 00:07:23,680
input or output type

159
00:07:23,680 --> 00:07:27,520
event uniqueness and time spam

160
00:07:27,520 --> 00:07:30,479
according to the assigned priority

161
00:07:30,479 --> 00:07:33,759
scores we source the info path and

162
00:07:33,759 --> 00:07:37,759
select the top and pass as the summary

163
00:07:37,759 --> 00:07:40,319
where security analysts have the

164
00:07:40,319 --> 00:07:41,840
flexibility

165
00:07:41,840 --> 00:07:45,840
to choose the value of n

166
00:07:46,319 --> 00:07:49,520
this is an example summary graph

167
00:07:49,520 --> 00:07:52,960
summary for each community includes

168
00:07:52,960 --> 00:07:54,720
master process

169
00:07:54,720 --> 00:07:59,759
time span and top ranked info infopath

170
00:07:59,759 --> 00:08:03,520
the evaluation data set includes six

171
00:08:03,520 --> 00:08:06,160
attacks performed in our test

172
00:08:06,160 --> 00:08:07,360
environment

173
00:08:07,360 --> 00:08:10,400
deployed with system monitoring tools

174
00:08:10,400 --> 00:08:14,879
and eight attacks in the wrtc data set

175
00:08:14,879 --> 00:08:17,919
where the collected data set contains

176
00:08:17,919 --> 00:08:21,840
about 100 million events for three days

177
00:08:21,840 --> 00:08:26,240
and the wrtc data set contains about 15

178
00:08:26,240 --> 00:08:27,919
million events

179
00:08:27,919 --> 00:08:30,879
deepcom partitions the dependency graphs

180
00:08:30,879 --> 00:08:33,519
into 18.4

181
00:08:33,519 --> 00:08:35,839
communities on average

182
00:08:35,839 --> 00:08:38,000
compared with the original dependency

183
00:08:38,000 --> 00:08:42,080
graphs it is 70.7 times smaller

184
00:08:42,080 --> 00:08:44,640
these results indicate that with the

185
00:08:44,640 --> 00:08:46,160
much smaller

186
00:08:46,160 --> 00:08:49,040
number of communities it is feasible to

187
00:08:49,040 --> 00:08:51,279
visualize all the communities or

188
00:08:51,279 --> 00:08:54,320
security analysts to easily see the

189
00:08:54,320 --> 00:08:56,880
overview of all the related system

190
00:08:56,880 --> 00:08:58,640
activities

191
00:08:58,640 --> 00:09:01,760
each community size is relatively small

192
00:09:01,760 --> 00:09:03,160
and has

193
00:09:03,160 --> 00:09:06,880
15.7 nodes on average which greatly

194
00:09:06,880 --> 00:09:10,320
reduces security analyst efforts in

195
00:09:10,320 --> 00:09:13,440
inspecting each community

196
00:09:13,440 --> 00:09:15,680
compared with the original dependency

197
00:09:15,680 --> 00:09:19,120
graphs these results also show that

198
00:09:19,120 --> 00:09:21,279
the community compression is quite

199
00:09:21,279 --> 00:09:23,839
effective in compressing the redundant

200
00:09:23,839 --> 00:09:24,959
edges

201
00:09:24,959 --> 00:09:28,920
reducing 216.4

202
00:09:29,040 --> 00:09:31,760
redundant edges on average for each

203
00:09:31,760 --> 00:09:34,319
community

204
00:09:34,720 --> 00:09:37,440
we compare the number of events in the

205
00:09:37,440 --> 00:09:41,120
top 1 top 2 and top 3 info paths for all

206
00:09:41,120 --> 00:09:43,680
the communities with the events

207
00:09:43,680 --> 00:09:46,000
identified by nodas

208
00:09:46,000 --> 00:09:49,279
top 3 info paths of deep calm have

209
00:09:49,279 --> 00:09:53,440
averagely 21 times less edges than no

210
00:09:53,440 --> 00:09:54,560
does

211
00:09:54,560 --> 00:09:55,760
in addition

212
00:09:55,760 --> 00:10:00,480
a community has 4.3 input nodes and 3.9

213
00:10:00,480 --> 00:10:02,880
output nodes 4 means

214
00:10:02,880 --> 00:10:06,800
15 15.7 info pass on average

215
00:10:06,800 --> 00:10:09,680
we manually inspect the top three info

216
00:10:09,680 --> 00:10:12,720
paths for each community and confirm

217
00:10:12,720 --> 00:10:14,800
that the top two infrapaths are

218
00:10:14,800 --> 00:10:17,440
sufficient to represent system

219
00:10:17,440 --> 00:10:20,560
activities and attack behaviors

220
00:10:20,560 --> 00:10:23,920
that is we only need to inspect

221
00:10:23,920 --> 00:10:25,920
12.7

222
00:10:25,920 --> 00:10:30,399
percent of the extracted info path

223
00:10:30,399 --> 00:10:32,560
we compare deepcom with nine

224
00:10:32,560 --> 00:10:35,120
state-of-the-art community detection

225
00:10:35,120 --> 00:10:38,160
algorithms to show the effectiveness of

226
00:10:38,160 --> 00:10:39,920
deepcom's

227
00:10:39,920 --> 00:10:42,720
community detection technique

228
00:10:42,720 --> 00:10:44,880
we use f1 score

229
00:10:44,880 --> 00:10:48,240
to evaluate the overall correspondence

230
00:10:48,240 --> 00:10:50,959
between the detected communities and the

231
00:10:50,959 --> 00:10:54,320
ground truth communities labeled by us

232
00:10:54,320 --> 00:10:57,680
the results show that f1 score achieved

233
00:10:57,680 --> 00:11:01,839
by deepcom is averagely 2.29

234
00:11:01,839 --> 00:11:04,399
times higher than those achieved by the

235
00:11:04,399 --> 00:11:06,000
baselines

236
00:11:06,000 --> 00:11:07,519
this shows that

237
00:11:07,519 --> 00:11:10,880
our community detection algorithms is

238
00:11:10,880 --> 00:11:12,079
effective

239
00:11:12,079 --> 00:11:14,480
to detect the process-centric

240
00:11:14,480 --> 00:11:16,160
communities

241
00:11:16,160 --> 00:11:17,920
furthermore

242
00:11:17,920 --> 00:11:20,640
even though deepcom and deep walk

243
00:11:20,640 --> 00:11:24,560
both use skip ramp to learn the node

244
00:11:24,560 --> 00:11:27,680
representation from the work rules

245
00:11:27,680 --> 00:11:33,440
deepcom outperforms deep work by 1.65

246
00:11:33,440 --> 00:11:35,680
times on average

247
00:11:35,680 --> 00:11:37,360
this shows that

248
00:11:37,360 --> 00:11:39,440
deepcom's hierarchical

249
00:11:39,440 --> 00:11:42,480
work themes are more effective than the

250
00:11:42,480 --> 00:11:46,240
random walk scheme adopted by deep walk

251
00:11:46,240 --> 00:11:50,399
which frees each node equally

252
00:11:50,720 --> 00:11:54,000
the box clocks in figure show

253
00:11:54,000 --> 00:11:56,639
the distributions of the compression

254
00:11:56,639 --> 00:11:58,880
rates for the nose and edges

255
00:11:58,880 --> 00:12:00,480
respectively

256
00:12:00,480 --> 00:12:02,959
the number of nodes and the number of

257
00:12:02,959 --> 00:12:06,000
edges for our community are reduced

258
00:12:06,000 --> 00:12:07,279
averagely

259
00:12:07,279 --> 00:12:08,360
by

260
00:12:08,360 --> 00:12:12,240
38.4 percent and forty four point seven

261
00:12:12,240 --> 00:12:15,600
percent respectively with the maximum

262
00:12:15,600 --> 00:12:16,800
reduction

263
00:12:16,800 --> 00:12:20,240
being ninety seven point three percent

264
00:12:20,240 --> 00:12:23,440
for the nose and ninety eight point

265
00:12:23,440 --> 00:12:24,399
percent

266
00:12:24,399 --> 00:12:26,560
for the edges

267
00:12:26,560 --> 00:12:29,760
in addition we verify that the info

268
00:12:29,760 --> 00:12:33,040
paths are not changed after compression

269
00:12:33,040 --> 00:12:36,639
and still preserves the semantics for

270
00:12:36,639 --> 00:12:41,120
the task represented by a community

271
00:12:41,120 --> 00:12:43,760
this is my report if you are interested

272
00:12:43,760 --> 00:12:47,040
in more details please read my paper

273
00:12:47,040 --> 00:12:51,399
thank you for listening to me

274
00:12:59,200 --> 00:13:01,279
sorry i'm gonna use this

275
00:13:01,279 --> 00:13:05,880
do we have questions from the audience

276
00:13:08,480 --> 00:13:09,370
i do have

277
00:13:09,370 --> 00:13:11,040
[Music]

278
00:13:11,040 --> 00:13:13,760
sorry maybe it's creating

279
00:13:13,760 --> 00:13:16,000
echo with the other microphone

280
00:13:16,000 --> 00:13:19,519
i do have a curiosity

281
00:13:19,519 --> 00:13:22,160
uh um

282
00:13:22,160 --> 00:13:24,320
when you when you do this community

283
00:13:24,320 --> 00:13:26,399
detection

284
00:13:26,399 --> 00:13:27,360
uh

285
00:13:27,360 --> 00:13:29,760
don't you uh when and when you evaluate

286
00:13:29,760 --> 00:13:32,480
it on the large data sets don't you need

287
00:13:32,480 --> 00:13:34,399
some sort of

288
00:13:34,399 --> 00:13:37,920
manual analysis to verify that

289
00:13:37,920 --> 00:13:39,120
this

290
00:13:39,120 --> 00:13:41,360
compressed representation

291
00:13:41,360 --> 00:13:44,480
is actually meaningful

292
00:13:46,800 --> 00:13:48,959
sorry i i should have also checked that

293
00:13:48,959 --> 00:13:50,320
our

294
00:13:50,320 --> 00:13:52,000
authors are

295
00:13:52,000 --> 00:13:54,800
can you hear us

296
00:13:55,920 --> 00:13:58,639
yeah yes

297
00:14:01,360 --> 00:14:04,639
should i ask the question

298
00:14:04,639 --> 00:14:07,920
should i ask the question again

299
00:14:09,600 --> 00:14:13,320
so can you hear us

300
00:14:16,000 --> 00:14:17,600
can you hear us

301
00:14:17,600 --> 00:14:19,360
yes yes

302
00:14:19,360 --> 00:14:21,360
oh yeah okay great yeah

303
00:14:21,360 --> 00:14:23,360
so you were asking about how we connect

304
00:14:23,360 --> 00:14:26,000
the ground tools right

305
00:14:26,000 --> 00:14:28,480
yeah like how you i mean

306
00:14:28,480 --> 00:14:30,160
do you evaluate with some manual

307
00:14:30,160 --> 00:14:32,720
analysis also the quality of the

308
00:14:32,720 --> 00:14:35,040
communities that are detected

309
00:14:35,040 --> 00:14:37,600
yes yes in fact we

310
00:14:37,600 --> 00:14:40,320
we collect the we actually try our best

311
00:14:40,320 --> 00:14:42,079
to collect a high quality ground choose

312
00:14:42,079 --> 00:14:45,279
data set for the the communities so we

313
00:14:45,279 --> 00:14:47,440
first do them like uh

314
00:14:47,440 --> 00:14:49,680
like you realistic search like using the

315
00:14:49,680 --> 00:14:51,120
keywords and the other find out the

316
00:14:51,120 --> 00:14:53,760
related process and we manually contract

317
00:14:53,760 --> 00:14:56,880
those communities to better uh improve

318
00:14:56,880 --> 00:14:58,800
our classrooms we also

319
00:14:58,800 --> 00:15:01,839
invite uh some three like uh

320
00:15:01,839 --> 00:15:04,800
independent computer system research uh

321
00:15:04,800 --> 00:15:07,279
researchers who has more than ten years

322
00:15:07,279 --> 00:15:09,279
research experience in this field ask

323
00:15:09,279 --> 00:15:11,760
them to help us verify our ground tools

324
00:15:11,760 --> 00:15:14,160
and we also revise our crowdsource based

325
00:15:14,160 --> 00:15:16,399
on their feedback

326
00:15:16,399 --> 00:15:19,600
okay and do you think that this approach

327
00:15:19,600 --> 00:15:22,959
could be also useful for

328
00:15:22,959 --> 00:15:25,199
i don't know for example in our analysis

329
00:15:25,199 --> 00:15:28,079
or even for

330
00:15:28,320 --> 00:15:30,959
using community detection on actual code

331
00:15:30,959 --> 00:15:33,680
or is it something more specific to the

332
00:15:33,680 --> 00:15:35,759
logs

333
00:15:35,759 --> 00:15:38,079
oh thank you for the question yeah so in

334
00:15:38,079 --> 00:15:40,880
fact right now we use the uh approaches

335
00:15:40,880 --> 00:15:44,240
to help uh understanding the uh the

336
00:15:44,240 --> 00:15:47,360
province uh province graph and we have a

337
00:15:47,360 --> 00:15:49,519
show one application by integrating with

338
00:15:49,519 --> 00:15:52,720
the hormones so which would be the

339
00:15:52,720 --> 00:15:56,639
attack detection approaches so the way

340
00:15:56,639 --> 00:15:58,720
how we integrate them is that

341
00:15:58,720 --> 00:16:00,560
by using their rule-based approaches

342
00:16:00,560 --> 00:16:01,920
we'll be able to highlight which

343
00:16:01,920 --> 00:16:04,160
community is attack-related which

344
00:16:04,160 --> 00:16:07,040
communities attack are irrelevant so

345
00:16:07,040 --> 00:16:09,360
that you can combine our process with

346
00:16:09,360 --> 00:16:11,199
detection technique then we will be able

347
00:16:11,199 --> 00:16:12,560
to better

348
00:16:12,560 --> 00:16:14,480
help those detection technique to

349
00:16:14,480 --> 00:16:18,240
explain why this is attack related

350
00:16:18,240 --> 00:16:20,399
oh okay and

351
00:16:20,399 --> 00:16:22,880
in the tables when you are showing the

352
00:16:22,880 --> 00:16:28,000
f1 score how you are automatically um

353
00:16:28,000 --> 00:16:29,360
i don't know how you are automatically

354
00:16:29,360 --> 00:16:31,279
measuring it

355
00:16:31,279 --> 00:16:33,759
how you work

356
00:16:34,320 --> 00:16:36,399
so we construct the ground source as i

357
00:16:36,399 --> 00:16:37,759
just explained

358
00:16:37,759 --> 00:16:40,399
and then we measure the f1 by comparing

359
00:16:40,399 --> 00:16:43,279
how whether we detect a community as the

360
00:16:43,279 --> 00:16:46,560
ground source if like we misclassified a

361
00:16:46,560 --> 00:16:48,399
process into another community we'll

362
00:16:48,399 --> 00:16:50,560
consider it's the fourth positive

363
00:16:50,560 --> 00:16:52,959
sorry our first negative

364
00:16:52,959 --> 00:16:54,560
okay

365
00:16:54,560 --> 00:16:57,120
thank you let me check if we have

366
00:16:57,120 --> 00:17:01,560
questions from the audience at home

367
00:17:09,359 --> 00:17:11,520
no it looks like we don't have more

368
00:17:11,520 --> 00:17:13,839
questions so thank you very much for

369
00:17:13,839 --> 00:17:15,760
attending remotely and for the

370
00:17:15,760 --> 00:17:18,240
interesting token work thank you thank

371
00:17:18,240 --> 00:17:21,959
you thank you

