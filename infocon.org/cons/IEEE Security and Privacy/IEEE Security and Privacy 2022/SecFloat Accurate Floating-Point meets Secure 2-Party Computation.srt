1
00:00:02,159 --> 00:00:02,960
okay

2
00:00:02,960 --> 00:00:05,040
thanks dominic for the introduction so

3
00:00:05,040 --> 00:00:07,759
hi everyone i'm diveshwar and in this

4
00:00:07,759 --> 00:00:09,440
presentation i'll be talking about sec

5
00:00:09,440 --> 00:00:11,679
float which is a two pc library for

6
00:00:11,679 --> 00:00:13,440
accurate floating point

7
00:00:13,440 --> 00:00:14,880
this is a joint work with maxim

8
00:00:14,880 --> 00:00:17,360
colleagues from microsoft research

9
00:00:17,360 --> 00:00:19,840
[Music]

10
00:00:19,840 --> 00:00:21,680
so let's get started

11
00:00:21,680 --> 00:00:24,080
okay so first of all what is two pc so

12
00:00:24,080 --> 00:00:25,920
two pc is a very powerful cryptographic

13
00:00:25,920 --> 00:00:27,519
primitive which allows two mutually

14
00:00:27,519 --> 00:00:29,359
distrusting parties to evaluate

15
00:00:29,359 --> 00:00:30,800
arbitrary functions on their secret

16
00:00:30,800 --> 00:00:32,399
inputs without revealing anything beyond

17
00:00:32,399 --> 00:00:34,000
the output

18
00:00:34,000 --> 00:00:35,760
so although like this f can be an

19
00:00:35,760 --> 00:00:37,360
arbitrary program floating point

20
00:00:37,360 --> 00:00:39,920
programs are of special interest because

21
00:00:39,920 --> 00:00:41,680
uh floating point is used widely in

22
00:00:41,680 --> 00:00:44,399
software for for emulating real

23
00:00:44,399 --> 00:00:46,719
like computation or real numbers

24
00:00:46,719 --> 00:00:48,079
so this brings a natural question

25
00:00:48,079 --> 00:00:50,480
forward is that in that uh how to

26
00:00:50,480 --> 00:00:51,840
securely evaluate floating point

27
00:00:51,840 --> 00:00:53,039
programs

28
00:00:53,039 --> 00:00:54,879
so currently there are two options the

29
00:00:54,879 --> 00:00:56,879
first one is to

30
00:00:56,879 --> 00:00:59,680
take two pc protocols for floating point

31
00:00:59,680 --> 00:01:01,600
and this option has some nice properties

32
00:01:01,600 --> 00:01:02,879
for instance it's a plug-and-play

33
00:01:02,879 --> 00:01:05,840
solution from plug and play i mean that

34
00:01:05,840 --> 00:01:07,200
in your floating point program you have

35
00:01:07,200 --> 00:01:08,560
floating point operations you can just

36
00:01:08,560 --> 00:01:09,760
replace them with the corresponding to

37
00:01:09,760 --> 00:01:11,040
bc protocol

38
00:01:11,040 --> 00:01:13,200
and this option also is accurate because

39
00:01:13,200 --> 00:01:15,280
you're not changing f so you're exactly

40
00:01:15,280 --> 00:01:16,479
computing f

41
00:01:16,479 --> 00:01:18,560
but the problem is that the overheads

42
00:01:18,560 --> 00:01:20,240
are interactable or it's like widely

43
00:01:20,240 --> 00:01:21,680
believed that they are completely

44
00:01:21,680 --> 00:01:23,119
intractable so no one really looks at

45
00:01:23,119 --> 00:01:25,280
this option that much the second option

46
00:01:25,280 --> 00:01:26,479
is to consider fixed point

47
00:01:26,479 --> 00:01:27,759
approximations which have great

48
00:01:27,759 --> 00:01:29,520
performance so that's where most of the

49
00:01:29,520 --> 00:01:30,799
focus has been

50
00:01:30,799 --> 00:01:32,640
but uh there's this flow to fix

51
00:01:32,640 --> 00:01:33,840
conversion which is required for

52
00:01:33,840 --> 00:01:35,439
floating point programs which is pretty

53
00:01:35,439 --> 00:01:38,000
janky so in the sense like first this

54
00:01:38,000 --> 00:01:39,840
leads to a precision loss because you're

55
00:01:39,840 --> 00:01:41,280
now

56
00:01:41,280 --> 00:01:43,280
not computing f you're computing f prime

57
00:01:43,280 --> 00:01:45,680
an approximation the second problem is

58
00:01:45,680 --> 00:01:47,840
that this conversion is only available

59
00:01:47,840 --> 00:01:49,840
for a few applications like

60
00:01:49,840 --> 00:01:51,759
machine learning for instance and

61
00:01:51,759 --> 00:01:53,200
there's no general tool which you can

62
00:01:53,200 --> 00:01:55,680
use to go from a floating point program

63
00:01:55,680 --> 00:01:57,920
to a fixed point approximation uh the

64
00:01:57,920 --> 00:01:59,439
third problem is that this process is

65
00:01:59,439 --> 00:02:01,360
quite painstaking in that if you have

66
00:02:01,360 --> 00:02:03,040
ever worked with fixed point code you

67
00:02:03,040 --> 00:02:05,040
would like you would realize that it's a

68
00:02:05,040 --> 00:02:06,799
debugging nightmare to like deal with it

69
00:02:06,799 --> 00:02:08,479
you have to choose scale but which

70
00:02:08,479 --> 00:02:10,560
there's there are a lot of moving parts

71
00:02:10,560 --> 00:02:12,800
and finally uh the an efficient

72
00:02:12,800 --> 00:02:14,640
conversion is not even always possible

73
00:02:14,640 --> 00:02:16,319
from fluid to fix so you could even end

74
00:02:16,319 --> 00:02:18,319
up with a fixed point approximation

75
00:02:18,319 --> 00:02:19,680
which is not efficient this does not

76
00:02:19,680 --> 00:02:21,120
even make sense

77
00:02:21,120 --> 00:02:23,440
okay so despite all of these problems

78
00:02:23,440 --> 00:02:25,200
most of the focus has been on this

79
00:02:25,200 --> 00:02:27,200
option too but in this work we take a

80
00:02:27,200 --> 00:02:28,640
different approach and focus on option

81
00:02:28,640 --> 00:02:31,519
one and actually show that uh floating

82
00:02:31,519 --> 00:02:34,480
point two pc can actually be practical

83
00:02:34,480 --> 00:02:36,239
okay so let's look at the lay of the

84
00:02:36,239 --> 00:02:38,720
land so uh these are the like what are

85
00:02:38,720 --> 00:02:40,239
the existing solutions for secure

86
00:02:40,239 --> 00:02:42,160
floating point right now so the first

87
00:02:42,160 --> 00:02:44,319
option is to take a clear exploding

88
00:02:44,319 --> 00:02:46,319
point library which which we like which

89
00:02:46,319 --> 00:02:48,560
has nothing to do with security and run

90
00:02:48,560 --> 00:02:51,360
it with some generic two pc uh protocol

91
00:02:51,360 --> 00:02:53,280
for an example of this is soft float and

92
00:02:53,280 --> 00:02:55,920
you run this with cbm cgc so as you'd

93
00:02:55,920 --> 00:02:58,239
expect the precision is as you'd like as

94
00:02:58,239 --> 00:02:59,680
precise as standard float but the

95
00:02:59,680 --> 00:03:01,280
performance is pretty terrible with a

96
00:03:01,280 --> 00:03:03,360
semi-honest two pc back-end and the

97
00:03:03,360 --> 00:03:04,879
reason for this is first that you're

98
00:03:04,879 --> 00:03:06,879
using a generic two pc and the other

99
00:03:06,879 --> 00:03:09,040
problem is that these floating point uh

100
00:03:09,040 --> 00:03:10,640
algorithms were not designed keeping two

101
00:03:10,640 --> 00:03:13,280
pc in mind they were designed for cpus

102
00:03:13,280 --> 00:03:15,440
the second option is to

103
00:03:15,440 --> 00:03:17,200
consider prior work specifically for

104
00:03:17,200 --> 00:03:19,440
secure floating point and while these

105
00:03:19,440 --> 00:03:21,440
works have a much better performance

106
00:03:21,440 --> 00:03:24,480
like 300x better performance than the

107
00:03:24,480 --> 00:03:26,480
the generic 2pc solution the precision

108
00:03:26,480 --> 00:03:28,400
is very poor so for instance for a

109
00:03:28,400 --> 00:03:30,799
simple example like x by sine pi x where

110
00:03:30,799 --> 00:03:33,120
x is close to zero the the like the

111
00:03:33,120 --> 00:03:35,360
state-of-the-art uh outputs a division

112
00:03:35,360 --> 00:03:37,120
by zero error while the output should

113
00:03:37,120 --> 00:03:39,120
have been one by five

114
00:03:39,120 --> 00:03:41,440
so clearly both options are inadequate

115
00:03:41,440 --> 00:03:43,680
and this is where uh sec float comes in

116
00:03:43,680 --> 00:03:45,680
and it provides um

117
00:03:45,680 --> 00:03:47,680
like it's pro it provides protocols

118
00:03:47,680 --> 00:03:49,519
which are as precise as standard float

119
00:03:49,519 --> 00:03:51,280
and the performance is also even better

120
00:03:51,280 --> 00:03:52,400
compared to

121
00:03:52,400 --> 00:03:55,680
the imprecise floating point works

122
00:03:55,680 --> 00:03:58,080
so in more detail uh

123
00:03:58,080 --> 00:04:00,159
we provide novel two pc friendly and

124
00:04:00,159 --> 00:04:01,439
provably precise floating point

125
00:04:01,439 --> 00:04:03,280
algorithms which are up to seven orders

126
00:04:03,280 --> 00:04:05,280
of magnitude more precise than prior

127
00:04:05,280 --> 00:04:07,599
work on secure floating point

128
00:04:07,599 --> 00:04:09,439
and we implement these algorithms in a

129
00:04:09,439 --> 00:04:11,200
semi honest two pc library called sec

130
00:04:11,200 --> 00:04:13,120
float which provides up to two orders of

131
00:04:13,120 --> 00:04:14,640
magnitude

132
00:04:14,640 --> 00:04:16,639
better performance compared even to the

133
00:04:16,639 --> 00:04:18,238
imprecise prior work

134
00:04:18,238 --> 00:04:20,079
and we to demonstrate the practicality

135
00:04:20,079 --> 00:04:21,358
of sec float we consider two

136
00:04:21,358 --> 00:04:23,199
applications the first is privacy

137
00:04:23,199 --> 00:04:25,040
preserving proximity testing and the

138
00:04:25,040 --> 00:04:26,560
second is secure machine learning

139
00:04:26,560 --> 00:04:28,000
inference and for both of these

140
00:04:28,000 --> 00:04:29,520
applications we show

141
00:04:29,520 --> 00:04:31,280
nice improvements in performance as well

142
00:04:31,280 --> 00:04:32,880
as precision

143
00:04:32,880 --> 00:04:34,880
okay so for the rest of this talk this

144
00:04:34,880 --> 00:04:37,280
is my agenda i'll first go over sec

145
00:04:37,280 --> 00:04:39,120
floors to pc friendly design

146
00:04:39,120 --> 00:04:40,240
and i'll

147
00:04:40,240 --> 00:04:42,080
go over the primitive operations as well

148
00:04:42,080 --> 00:04:44,080
as math operations

149
00:04:44,080 --> 00:04:45,440
next i'll go over a part of our

150
00:04:45,440 --> 00:04:46,880
evaluation starting with micro

151
00:04:46,880 --> 00:04:49,199
benchmarks and then i'll focus on one of

152
00:04:49,199 --> 00:04:50,720
our application which is secure ml

153
00:04:50,720 --> 00:04:52,720
inference and for this application i'll

154
00:04:52,720 --> 00:04:54,639
show that fixed point approximations are

155
00:04:54,639 --> 00:04:57,280
inadequate and the floating point that

156
00:04:57,280 --> 00:04:59,120
we design is practical

157
00:04:59,120 --> 00:05:00,400
and finally i'll conclude with some

158
00:05:00,400 --> 00:05:02,160
directions for future work

159
00:05:02,160 --> 00:05:03,600
okay

160
00:05:03,600 --> 00:05:05,280
so first starting with the two pc

161
00:05:05,280 --> 00:05:07,120
friendly design so in this section i'll

162
00:05:07,120 --> 00:05:09,280
basically compare and contrast with the

163
00:05:09,280 --> 00:05:12,160
existing standard libraries i'll mention

164
00:05:12,160 --> 00:05:13,919
how they like what properties they have

165
00:05:13,919 --> 00:05:15,440
and how we design secular to be

166
00:05:15,440 --> 00:05:16,479
different

167
00:05:16,479 --> 00:05:18,400
okay so first looking at primitive

168
00:05:18,400 --> 00:05:19,840
operations

169
00:05:19,840 --> 00:05:20,639
so

170
00:05:20,639 --> 00:05:22,400
in the standard libraries for primitive

171
00:05:22,400 --> 00:05:23,440
operations

172
00:05:23,440 --> 00:05:25,680
more conditional statements are involved

173
00:05:25,680 --> 00:05:27,840
this is fine because in cpus you only

174
00:05:27,840 --> 00:05:29,360
have to execute one branch so it does

175
00:05:29,360 --> 00:05:30,479
not matter

176
00:05:30,479 --> 00:05:32,960
but in two pc all conditional branches

177
00:05:32,960 --> 00:05:34,720
have to be evaluated because any branch

178
00:05:34,720 --> 00:05:36,000
which is not evaluated would leak

179
00:05:36,000 --> 00:05:37,199
information

180
00:05:37,199 --> 00:05:38,880
so we designed sec flow to have fewer

181
00:05:38,880 --> 00:05:41,120
conditional statements

182
00:05:41,120 --> 00:05:42,560
the second point of difference is that

183
00:05:42,560 --> 00:05:44,000
standard libraries have larger than

184
00:05:44,000 --> 00:05:46,800
standard bit widths like 16 32 and 64.

185
00:05:46,800 --> 00:05:50,240
this is uh great because 16 32 and 64

186
00:05:50,240 --> 00:05:52,800
have native support on cpus so they are

187
00:05:52,800 --> 00:05:54,960
fine using them but we designed sec flow

188
00:05:54,960 --> 00:05:57,039
to have smaller and non-standard bit

189
00:05:57,039 --> 00:05:59,680
widths like 10 instead of 16 17 17

190
00:05:59,680 --> 00:06:02,319
instead of 32 and so on and this is and

191
00:06:02,319 --> 00:06:04,240
we do this without any loss in precision

192
00:06:04,240 --> 00:06:05,759
so we are guaranteeing the same

193
00:06:05,759 --> 00:06:07,919
precision and this is important because

194
00:06:07,919 --> 00:06:09,840
two pc cost case linearly with bit width

195
00:06:09,840 --> 00:06:11,440
so the smaller the bit width is the more

196
00:06:11,440 --> 00:06:13,199
performance you get

197
00:06:13,199 --> 00:06:16,160
and uh with these changes seclude offers

198
00:06:16,160 --> 00:06:19,199
a 2.4 x to 6.1 x improvement over soft

199
00:06:19,199 --> 00:06:20,960
load which is a standard library with

200
00:06:20,960 --> 00:06:22,720
the same back end

201
00:06:22,720 --> 00:06:25,440
and after these like with these

202
00:06:25,440 --> 00:06:27,520
improvements the relative cost of these

203
00:06:27,520 --> 00:06:30,240
operations are as follows so addition is

204
00:06:30,240 --> 00:06:32,639
11 times more expensive than comparison

205
00:06:32,639 --> 00:06:35,039
so looking at these costs you would you

206
00:06:35,039 --> 00:06:36,800
realize that you would not see the same

207
00:06:36,800 --> 00:06:40,000
performance characteristic in cpus

208
00:06:40,000 --> 00:06:40,800
so

209
00:06:40,800 --> 00:06:42,639
so since like the two pc relative cost

210
00:06:42,639 --> 00:06:45,280
is very different in two pc compared to

211
00:06:45,280 --> 00:06:46,319
cpus

212
00:06:46,319 --> 00:06:48,560
our design of math operations is also

213
00:06:48,560 --> 00:06:50,080
like very different

214
00:06:50,080 --> 00:06:52,160
because uh math operations ultimately

215
00:06:52,160 --> 00:06:54,479
rely on primitive operations

216
00:06:54,479 --> 00:06:56,160
okay and the final point of difference

217
00:06:56,160 --> 00:06:58,000
in primitive operation is that standard

218
00:06:58,000 --> 00:06:59,599
libraries have specialized algorithms

219
00:06:59,599 --> 00:07:00,800
for different

220
00:07:00,800 --> 00:07:02,720
ieee formats

221
00:07:02,720 --> 00:07:05,039
and in contrast seclude has one general

222
00:07:05,039 --> 00:07:06,560
algorithm for arbitrary floating point

223
00:07:06,560 --> 00:07:08,560
representation and this generality is

224
00:07:08,560 --> 00:07:10,400
quite useful in designing our math

225
00:07:10,400 --> 00:07:11,759
floating point algorithms as i'll

226
00:07:11,759 --> 00:07:14,240
discuss in the next slide

227
00:07:14,240 --> 00:07:15,440
okay

228
00:07:15,440 --> 00:07:18,319
so um looking at uh now moving on to the

229
00:07:18,319 --> 00:07:20,000
math operation so our math operations

230
00:07:20,000 --> 00:07:21,919
also follow this three-step procedure

231
00:07:21,919 --> 00:07:23,759
like standard libraries first there's

232
00:07:23,759 --> 00:07:25,520
the range reduction step which reduces

233
00:07:25,520 --> 00:07:27,280
the range of the input then there's a

234
00:07:27,280 --> 00:07:28,800
polynomial evaluation on that and

235
00:07:28,800 --> 00:07:29,759
finally

236
00:07:29,759 --> 00:07:31,759
the output of polynomial evaluation is

237
00:07:31,759 --> 00:07:34,080
adjusted in output compensation step

238
00:07:34,080 --> 00:07:35,680
so we don't have to go into the details

239
00:07:35,680 --> 00:07:37,280
of all three steps but the important

240
00:07:37,280 --> 00:07:39,280
point is that all steps introduce some

241
00:07:39,280 --> 00:07:41,120
error into the computation so we have to

242
00:07:41,120 --> 00:07:42,880
do this computation in some higher

243
00:07:42,880 --> 00:07:45,280
precision so that the errors which are

244
00:07:45,280 --> 00:07:48,720
introduced can actually be accounted for

245
00:07:48,720 --> 00:07:50,560
and i also have this relative cost table

246
00:07:50,560 --> 00:07:52,160
from the previous slide just for

247
00:07:52,160 --> 00:07:53,199
reference

248
00:07:53,199 --> 00:07:55,759
okay so first the standard libraries for

249
00:07:55,759 --> 00:07:57,599
math operation use 64-bit doubles for

250
00:07:57,599 --> 00:07:58,960
the intermediate operations because they

251
00:07:58,960 --> 00:08:00,479
have native support

252
00:08:00,479 --> 00:08:03,120
but the 64-bit is actually quite large

253
00:08:03,120 --> 00:08:05,440
and this is expensive for two pc

254
00:08:05,440 --> 00:08:07,680
so in contrast we use a custom 36-bit

255
00:08:07,680 --> 00:08:08,879
floating point representation for

256
00:08:08,879 --> 00:08:11,280
intermediate operation which is uh

257
00:08:11,280 --> 00:08:12,400
greater than two times better in

258
00:08:12,400 --> 00:08:14,319
performance compared to doubled

259
00:08:14,319 --> 00:08:16,960
and as a result of using this uh much

260
00:08:16,960 --> 00:08:20,560
more much like much lower precision uh

261
00:08:20,560 --> 00:08:22,960
enter representation we have to design

262
00:08:22,960 --> 00:08:24,160
normal range reduction techniques

263
00:08:24,160 --> 00:08:25,520
because the existing range reduction

264
00:08:25,520 --> 00:08:27,360
techniques have way too much error for

265
00:08:27,360 --> 00:08:28,240
this

266
00:08:28,240 --> 00:08:31,280
in the the this imprecise uh

267
00:08:31,280 --> 00:08:32,719
representation

268
00:08:32,719 --> 00:08:35,039
and uh so i was talking about generality

269
00:08:35,039 --> 00:08:37,039
before so this 36-bit floating point

270
00:08:37,039 --> 00:08:38,719
representation is not supported by any

271
00:08:38,719 --> 00:08:40,479
standard library and they don't have a

272
00:08:40,479 --> 00:08:42,240
general algorithm but the general

273
00:08:42,240 --> 00:08:44,240
algorithm that we have directly

274
00:08:44,240 --> 00:08:48,080
makes it possible to do these operations

275
00:08:48,080 --> 00:08:49,440
the next point is that standard

276
00:08:49,440 --> 00:08:51,440
libraries have high degree polynomials

277
00:08:51,440 --> 00:08:53,920
so they involve a many floating point

278
00:08:53,920 --> 00:08:55,680
additions and multiplication as as you

279
00:08:55,680 --> 00:08:56,959
can see in the table they are quite

280
00:08:56,959 --> 00:08:58,240
expensive

281
00:08:58,240 --> 00:09:00,720
so we instead use low degree splines

282
00:09:00,720 --> 00:09:02,080
and uh

283
00:09:02,080 --> 00:09:03,680
so basically we have replaced most of

284
00:09:03,680 --> 00:09:04,800
the floating point additions and

285
00:09:04,800 --> 00:09:05,920
multiplication floating point

286
00:09:05,920 --> 00:09:08,399
comparisons which you can see are much

287
00:09:08,399 --> 00:09:10,320
cheaper comparatively

288
00:09:10,320 --> 00:09:13,040
and more moreover like uh in addition to

289
00:09:13,040 --> 00:09:15,279
using 40 point comparisons we carefully

290
00:09:15,279 --> 00:09:17,360
choose the spline knots in our splines

291
00:09:17,360 --> 00:09:19,680
so that to identify the interval in

292
00:09:19,680 --> 00:09:21,360
which the input is lying we only have to

293
00:09:21,360 --> 00:09:23,680
do integer comparison on less than equal

294
00:09:23,680 --> 00:09:25,279
to eight bits instead of doing

295
00:09:25,279 --> 00:09:26,880
full-fledged 36-bit floating point

296
00:09:26,880 --> 00:09:28,880
comparisons

297
00:09:28,880 --> 00:09:30,480
and even for the remaining floating

298
00:09:30,480 --> 00:09:32,160
point addition multiplication we use

299
00:09:32,160 --> 00:09:33,839
cheaper imprecise variance so we

300
00:09:33,839 --> 00:09:35,360
realized that for intermediate

301
00:09:35,360 --> 00:09:37,120
computations we don't need the most

302
00:09:37,120 --> 00:09:38,959
precise version of these arithmetic

303
00:09:38,959 --> 00:09:40,399
operations

304
00:09:40,399 --> 00:09:42,480
finally the range reduction in standard

305
00:09:42,480 --> 00:09:44,240
libraries use floating point operations

306
00:09:44,240 --> 00:09:46,240
because they are natively supported

307
00:09:46,240 --> 00:09:48,720
but we use we designed range reduction

308
00:09:48,720 --> 00:09:50,000
to only use fixed point operations

309
00:09:50,000 --> 00:09:51,360
because they're because they're cheaper

310
00:09:51,360 --> 00:09:53,920
with uh two pc

311
00:09:53,920 --> 00:09:54,959
okay

312
00:09:54,959 --> 00:09:57,680
now moving on to the evaluation

313
00:09:57,680 --> 00:09:59,040
so first let's look at the micro

314
00:09:59,040 --> 00:10:00,560
benchmarks so i have the micro

315
00:10:00,560 --> 00:10:01,920
benchmarks for a primitive operation

316
00:10:01,920 --> 00:10:03,360
which is multiplication and a math

317
00:10:03,360 --> 00:10:05,360
operation which is sine

318
00:10:05,360 --> 00:10:07,040
and here uh first let's look at the

319
00:10:07,040 --> 00:10:08,480
black rows which

320
00:10:08,480 --> 00:10:09,680
represent the

321
00:10:09,680 --> 00:10:11,600
the precise works so here for

322
00:10:11,600 --> 00:10:13,600
multiplication we have a 46 improvement

323
00:10:13,600 --> 00:10:16,160
46x improvement in runtime and for sine

324
00:10:16,160 --> 00:10:17,279
the improvement is three orders of

325
00:10:17,279 --> 00:10:18,640
magnitude compared to the precise

326
00:10:18,640 --> 00:10:21,360
baseline and in red i have the imprecise

327
00:10:21,360 --> 00:10:23,920
baseline over which we still have 4x

328
00:10:23,920 --> 00:10:25,680
improvement

329
00:10:25,680 --> 00:10:27,200
in terms of rounds

330
00:10:27,200 --> 00:10:29,360
the the prior works can be evaluated

331
00:10:29,360 --> 00:10:31,440
even either with garble circuits or with

332
00:10:31,440 --> 00:10:33,920
gmw so if it's gobble circus then the

333
00:10:33,920 --> 00:10:36,480
rounds are just two but for but compared

334
00:10:36,480 --> 00:10:38,160
to the gmw baseline our rounds are

335
00:10:38,160 --> 00:10:40,720
comparable

336
00:10:40,720 --> 00:10:43,519
okay now let's look at our application

337
00:10:43,519 --> 00:10:45,680
so we look at secure machine learning

338
00:10:45,680 --> 00:10:47,120
inference in the context of privacy

339
00:10:47,120 --> 00:10:48,959
preserving web advertising which which

340
00:10:48,959 --> 00:10:50,800
has recently garnered a lot of attention

341
00:10:50,800 --> 00:10:52,800
from google and microsoft

342
00:10:52,800 --> 00:10:55,200
and for this application uh

343
00:10:55,200 --> 00:10:56,640
one model which is used is this

344
00:10:56,640 --> 00:10:58,640
industrial relevance model which is a

345
00:10:58,640 --> 00:11:00,000
fully connected neural network with

346
00:11:00,000 --> 00:11:02,160
three hidden layers

347
00:11:02,160 --> 00:11:05,120
and when we took this model and ran this

348
00:11:05,120 --> 00:11:06,800
state-of-the-art flow to fix converter

349
00:11:06,800 --> 00:11:08,560
on this model we realized that it failed

350
00:11:08,560 --> 00:11:09,600
completely

351
00:11:09,600 --> 00:11:11,519
so from failing completely i mean that

352
00:11:11,519 --> 00:11:13,360
for each scale so this converter tried

353
00:11:13,360 --> 00:11:14,800
each scale and for each scale the fixed

354
00:11:14,800 --> 00:11:16,880
point model output was garbage so for

355
00:11:16,880 --> 00:11:19,360
low uh low for smaller scales the

356
00:11:19,360 --> 00:11:21,040
precision was too low and for larger

357
00:11:21,040 --> 00:11:23,839
scales there was an overflow in 64 bits

358
00:11:23,839 --> 00:11:25,920
and this was happening because the

359
00:11:25,920 --> 00:11:27,760
weights the dynamic range of weights was

360
00:11:27,760 --> 00:11:30,240
quite large from 10 to the minus 7 to 10

361
00:11:30,240 --> 00:11:31,839
and the range for the intermediate

362
00:11:31,839 --> 00:11:34,480
activation was even larger

363
00:11:34,480 --> 00:11:36,720
in contrast when we ran the same model

364
00:11:36,720 --> 00:11:38,880
with sec float we could evaluate it

365
00:11:38,880 --> 00:11:40,880
without any of this

366
00:11:40,880 --> 00:11:42,480
like scale determination or anything we

367
00:11:42,480 --> 00:11:44,160
could run it directly without any

368
00:11:44,160 --> 00:11:46,880
accuracy loss in just 18 seconds and for

369
00:11:46,880 --> 00:11:49,279
with 4.7 gb of communication

370
00:11:49,279 --> 00:11:51,839
uh if we uh we also

371
00:11:51,839 --> 00:11:53,680
benchmarked how much communication would

372
00:11:53,680 --> 00:11:55,839
be required with a prior work and

373
00:11:55,839 --> 00:11:58,079
floating point which is also imprecise

374
00:11:58,079 --> 00:11:59,839
the the the difference in communication

375
00:11:59,839 --> 00:12:02,000
was 8.8 x

376
00:12:02,000 --> 00:12:03,600
so this shows that sex fluid enables

377
00:12:03,600 --> 00:12:05,120
practical secure inference of such

378
00:12:05,120 --> 00:12:07,279
models for the first time

379
00:12:07,279 --> 00:12:08,320
okay

380
00:12:08,320 --> 00:12:09,120
and

381
00:12:09,120 --> 00:12:11,920
in summary uh we designed novel two pc

382
00:12:11,920 --> 00:12:13,200
friendly and precise floating point

383
00:12:13,200 --> 00:12:14,399
algorithms which offer large

384
00:12:14,399 --> 00:12:16,079
improvements over prior work

385
00:12:16,079 --> 00:12:17,760
up to seven orders in precision and up

386
00:12:17,760 --> 00:12:20,000
to three orders in performance

387
00:12:20,000 --> 00:12:21,760
this shows that precise floating point

388
00:12:21,760 --> 00:12:23,200
precise secure floating point can

389
00:12:23,200 --> 00:12:25,279
actually have practical performance

390
00:12:25,279 --> 00:12:27,120
um although we have taken a like a

391
00:12:27,120 --> 00:12:28,720
significant step forward in this

392
00:12:28,720 --> 00:12:30,240
direction

393
00:12:30,240 --> 00:12:31,680
a lot of work is still needed for wide

394
00:12:31,680 --> 00:12:32,959
adoption so that it can be made

395
00:12:32,959 --> 00:12:35,279
practical for a lot of applications so i

396
00:12:35,279 --> 00:12:36,839
have some directions for future work

397
00:12:36,839 --> 00:12:40,560
here first is that extending a sec flow

398
00:12:40,560 --> 00:12:42,560
to be maliciously secure or to the

399
00:12:42,560 --> 00:12:44,800
n-party situation it's actually pretty

400
00:12:44,800 --> 00:12:46,720
straightforward because all of our

401
00:12:46,720 --> 00:12:48,240
algorithms are

402
00:12:48,240 --> 00:12:49,839
just use simple

403
00:12:49,839 --> 00:12:51,760
operations which are supported by any of

404
00:12:51,760 --> 00:12:54,399
the mpc or maliciously secure npc or two

405
00:12:54,399 --> 00:12:56,639
pc libraries the second is to improve

406
00:12:56,639 --> 00:12:57,680
the rounds of our floating point

407
00:12:57,680 --> 00:12:59,360
protocol so we did not focus that much

408
00:12:59,360 --> 00:13:01,040
on improving grounds in this work and

409
00:13:01,040 --> 00:13:02,880
that could be an interesting direction

410
00:13:02,880 --> 00:13:04,639
the third is to extend math operations

411
00:13:04,639 --> 00:13:06,399
to 64 64-bit doubles so our primitive

412
00:13:06,399 --> 00:13:07,440
operations support arbitrary

413
00:13:07,440 --> 00:13:09,200
representations but our math operations

414
00:13:09,200 --> 00:13:11,440
are limited to 32-bit float

415
00:13:11,440 --> 00:13:13,200
and finally uh

416
00:13:13,200 --> 00:13:15,120
we think that sec flow can be useful in

417
00:13:15,120 --> 00:13:16,480
other areas of privacy preserving

418
00:13:16,480 --> 00:13:17,839
machine learning

419
00:13:17,839 --> 00:13:19,360
that's it and

420
00:13:19,360 --> 00:13:20,560
our paper and code are publicly

421
00:13:20,560 --> 00:13:22,320
available and we hope you check them out

422
00:13:22,320 --> 00:13:25,839
that's it thank you for your attention

423
00:13:29,600 --> 00:13:31,920
okay

424
00:13:31,920 --> 00:13:34,240
so any questions yeah all right thank

425
00:13:34,240 --> 00:13:36,320
you so much awesome talk any questions

426
00:13:36,320 --> 00:13:38,480
from the audience

427
00:13:38,480 --> 00:13:40,959
yeah so this is a bit of a broad

428
00:13:40,959 --> 00:13:43,519
question um but i'm curious why you

429
00:13:43,519 --> 00:13:45,120
think

430
00:13:45,120 --> 00:13:47,760
in your setting and in the typical cpu

431
00:13:47,760 --> 00:13:49,519
hardware setting you end up with such

432
00:13:49,519 --> 00:13:51,600
different cost profiles because at least

433
00:13:51,600 --> 00:13:53,120
somewhat naively i'm thinking of both of

434
00:13:53,120 --> 00:13:55,279
these settings is requiring a boolean

435
00:13:55,279 --> 00:13:56,720
circuit

436
00:13:56,720 --> 00:13:58,320
so yeah why the huge difference in the

437
00:13:58,320 --> 00:14:00,480
class profile

438
00:14:00,480 --> 00:14:03,680
so um i think that the difference

439
00:14:03,680 --> 00:14:05,600
majorly comes from so in the floating

440
00:14:05,600 --> 00:14:07,120
point floating point is really expensive

441
00:14:07,120 --> 00:14:09,440
because of a normalization operation so

442
00:14:09,440 --> 00:14:12,399
you basically so normally in into pc you

443
00:14:12,399 --> 00:14:14,480
would do uh arithmetic operations in an

444
00:14:14,480 --> 00:14:16,160
arithmetic domain and for boolean

445
00:14:16,160 --> 00:14:17,360
operations you would go to a boolean

446
00:14:17,360 --> 00:14:20,560
domain and there i like i guess in cpu

447
00:14:20,560 --> 00:14:21,920
that is just as good because you can

448
00:14:21,920 --> 00:14:23,199
have both operations natively

449
00:14:23,199 --> 00:14:25,199
implemented and there's no not that big

450
00:14:25,199 --> 00:14:26,880
of a conversion cost but for

451
00:14:26,880 --> 00:14:28,480
normalization in two pc is actually

452
00:14:28,480 --> 00:14:30,320
quite expensive so for addition you saw

453
00:14:30,320 --> 00:14:33,120
it was 11c right 11 times i think around

454
00:14:33,120 --> 00:14:34,639
10 of that comes from just a

455
00:14:34,639 --> 00:14:36,480
normalization like it's it's a very

456
00:14:36,480 --> 00:14:38,079
expensive operation which requires

457
00:14:38,079 --> 00:14:40,000
determining the the most significant

458
00:14:40,000 --> 00:14:42,079
non-zero bit and that is expensive into

459
00:14:42,079 --> 00:14:43,760
pc and i think future works can improve

460
00:14:43,760 --> 00:14:45,600
that and actually reduce

461
00:14:45,600 --> 00:14:48,240
the huge gap right that exists right now

462
00:14:48,240 --> 00:14:51,959
i see yeah thanks

463
00:14:58,320 --> 00:15:01,600
what's up dan what

464
00:15:02,000 --> 00:15:03,839
that's it um

465
00:15:03,839 --> 00:15:05,839
then if there aren't any more questions

466
00:15:05,839 --> 00:15:07,760
from the audience i don't think oh wait

467
00:15:07,760 --> 00:15:09,600
yeah from uh one

468
00:15:09,600 --> 00:15:11,600
on the chat no never mind nothing from

469
00:15:11,600 --> 00:15:14,000
the chat um then i would maybe like ask

470
00:15:14,000 --> 00:15:16,079
one of my questions so you mentioned

471
00:15:16,079 --> 00:15:18,639
maybe the extension to double precision

472
00:15:18,639 --> 00:15:20,320
uh could you just like go a bit more

473
00:15:20,320 --> 00:15:22,160
into that what you would like plan on

474
00:15:22,160 --> 00:15:24,000
doing and what do you think would be

475
00:15:24,000 --> 00:15:24,800
like

476
00:15:24,800 --> 00:15:26,880
successful there so i think like that

477
00:15:26,880 --> 00:15:28,399
would be a bit challenging with our

478
00:15:28,399 --> 00:15:30,560
approach so right now for 32-bit floats

479
00:15:30,560 --> 00:15:32,959
we are doing an exhaustive evaluation so

480
00:15:32,959 --> 00:15:34,720
we are basically testing like that's how

481
00:15:34,720 --> 00:15:35,920
we formally verify that our

482
00:15:35,920 --> 00:15:37,600
implementations are correct we go over

483
00:15:37,600 --> 00:15:39,279
all possible outputs and see that they

484
00:15:39,279 --> 00:15:41,600
work but that's impossible to do for 64

485
00:15:41,600 --> 00:15:43,759
bit doubles right now so we need i think

486
00:15:43,759 --> 00:15:46,560
like some uh nice uh

487
00:15:46,560 --> 00:15:47,839
like advancement is required in

488
00:15:47,839 --> 00:15:49,519
verification technologies to actually

489
00:15:49,519 --> 00:15:51,360
make it happen for 64-bit doubles i

490
00:15:51,360 --> 00:15:53,120
think that's a challenge even for uh

491
00:15:53,120 --> 00:15:54,720
even in the floating point community to

492
00:15:54,720 --> 00:15:57,120
make it happen

493
00:15:57,120 --> 00:15:58,959
all right thank you so much uh awesome

494
00:15:58,959 --> 00:16:03,239
talk give it up again for the watch

