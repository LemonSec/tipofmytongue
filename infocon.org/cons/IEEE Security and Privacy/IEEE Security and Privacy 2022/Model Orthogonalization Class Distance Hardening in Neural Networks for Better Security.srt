1
00:00:00,399 --> 00:00:01,280
please

2
00:00:01,280 --> 00:00:03,840
good afternoon i'm guan hong from purdue

3
00:00:03,840 --> 00:00:05,120
university

4
00:00:05,120 --> 00:00:07,680
i'm going to talk about our work model

5
00:00:07,680 --> 00:00:10,080
of organization cast distance hardening

6
00:00:10,080 --> 00:00:13,759
in your networks for better security

7
00:00:13,759 --> 00:00:15,759
backdoor attacks are prominent threats

8
00:00:15,759 --> 00:00:17,680
to deep learning models

9
00:00:17,680 --> 00:00:20,480
suppose we have an image of a dog and a

10
00:00:20,480 --> 00:00:22,800
normal model will predict it as a dog

11
00:00:22,800 --> 00:00:24,400
apparently

12
00:00:24,400 --> 00:00:27,039
however an attacker can inject a small

13
00:00:27,039 --> 00:00:29,599
bacterial trigger on the image and use

14
00:00:29,599 --> 00:00:32,320
this to trim a new model

15
00:00:32,320 --> 00:00:34,559
this new model will misclassify any

16
00:00:34,559 --> 00:00:36,800
inputs with the trigger to the target

17
00:00:36,800 --> 00:00:39,120
label fish

18
00:00:39,120 --> 00:00:41,120
there are a body of defense techniques

19
00:00:41,120 --> 00:00:43,040
against the backdoor attacks for

20
00:00:43,040 --> 00:00:45,360
instance backdoor scanning aims to

21
00:00:45,360 --> 00:00:47,440
determine whether back a model is

22
00:00:47,440 --> 00:00:49,760
factored or not using a small set of

23
00:00:49,760 --> 00:00:51,680
cleaning samples

24
00:00:51,680 --> 00:00:53,680
backdoor input detection

25
00:00:53,680 --> 00:00:55,840
rejects inputs with a backward trigger

26
00:00:55,840 --> 00:00:57,360
on the fly

27
00:00:57,360 --> 00:01:00,640
and back to elimination aims to

28
00:01:00,640 --> 00:01:03,039
try to remove those injected vectors in

29
00:01:03,039 --> 00:01:04,720
the model

30
00:01:04,720 --> 00:01:06,960
all these defense techniques

31
00:01:06,960 --> 00:01:10,960
mainly focus on injection backdoors

32
00:01:11,040 --> 00:01:13,280
as we have seen earlier the injector

33
00:01:13,280 --> 00:01:15,360
vectors are intentionally added to the

34
00:01:15,360 --> 00:01:17,840
model by an attacker

35
00:01:17,840 --> 00:01:20,640
however we can also find vectors in

36
00:01:20,640 --> 00:01:22,640
benign models called the natural

37
00:01:22,640 --> 00:01:24,400
backdoor

38
00:01:24,400 --> 00:01:27,680
for example we downloaded a model from

39
00:01:27,680 --> 00:01:31,119
keras model zoom and use an existing

40
00:01:31,119 --> 00:01:33,280
trigger generation method to generate a

41
00:01:33,280 --> 00:01:35,680
trigger for turtle images

42
00:01:35,680 --> 00:01:38,159
the general trigger is very small even

43
00:01:38,159 --> 00:01:41,759
smaller than the above injected vector

44
00:01:41,759 --> 00:01:44,720
when we add this trigger on any turtle

45
00:01:44,720 --> 00:01:47,439
images and feed it to the benign bundle

46
00:01:47,439 --> 00:01:50,320
it will always predict a bird with high

47
00:01:50,320 --> 00:01:51,920
confidence

48
00:01:51,920 --> 00:01:54,159
this means both injected backdoor and

49
00:01:54,159 --> 00:01:56,399
the natural vectors are severe security

50
00:01:56,399 --> 00:01:59,280
problems and we need to improve model's

51
00:01:59,280 --> 00:02:02,560
robotness against them

52
00:02:02,560 --> 00:02:04,799
before we go into details about how to

53
00:02:04,799 --> 00:02:06,719
harden a model

54
00:02:06,719 --> 00:02:09,038
let's first define a few notions for the

55
00:02:09,038 --> 00:02:11,280
ease of a discussion

56
00:02:11,280 --> 00:02:14,239
a backdoor is a transformation that has

57
00:02:14,239 --> 00:02:18,160
a trigger on the input of class a

58
00:02:18,160 --> 00:02:20,480
the transform the input will

59
00:02:20,480 --> 00:02:21,480
cause

60
00:02:21,480 --> 00:02:24,640
misclassification to a class b on model

61
00:02:24,640 --> 00:02:26,000
m

62
00:02:26,000 --> 00:02:29,040
the background size is the lp norm of

63
00:02:29,040 --> 00:02:31,599
the change on the input

64
00:02:31,599 --> 00:02:34,239
backdoors are usually small and stealthy

65
00:02:34,239 --> 00:02:36,879
which can expose the characteristics of

66
00:02:36,879 --> 00:02:37,840
the

67
00:02:37,840 --> 00:02:41,280
between a pair of classes

68
00:02:41,280 --> 00:02:43,920
we define the lp norm of the minimum

69
00:02:43,920 --> 00:02:46,720
vector from a victim class a to a target

70
00:02:46,720 --> 00:02:50,480
class b they are discard distance

71
00:02:50,480 --> 00:02:53,920
this is a trigger from turtle to bird

72
00:02:53,920 --> 00:02:57,120
this it is the minimum vector and hence

73
00:02:57,120 --> 00:02:59,040
the gas distance

74
00:02:59,040 --> 00:03:01,120
we can also generate a much larger

75
00:03:01,120 --> 00:03:02,319
trigger

76
00:03:02,319 --> 00:03:04,560
which cannot be considered as the

77
00:03:04,560 --> 00:03:06,400
distance

78
00:03:06,400 --> 00:03:08,959
the distance can is directional and it

79
00:03:08,959 --> 00:03:12,239
can be different for the two directions

80
00:03:12,239 --> 00:03:15,519
see the distance from bird to turtle is

81
00:03:15,519 --> 00:03:18,239
different and much larger than the other

82
00:03:18,239 --> 00:03:20,720
direction

83
00:03:20,800 --> 00:03:23,599
the cast distance hence measures the

84
00:03:23,599 --> 00:03:27,280
robustness to vector attacks

85
00:03:27,360 --> 00:03:29,840
here we study how to enlarge cost

86
00:03:29,840 --> 00:03:33,840
distance for improved model robustness

87
00:03:33,840 --> 00:03:36,480
suppose we have two classes greek and

88
00:03:36,480 --> 00:03:37,519
blue

89
00:03:37,519 --> 00:03:39,519
natural training use a straight line to

90
00:03:39,519 --> 00:03:41,440
separate them

91
00:03:41,440 --> 00:03:44,480
this star denotes the center of a sample

92
00:03:44,480 --> 00:03:45,760
mass

93
00:03:45,760 --> 00:03:48,080
the orange line denotes the backdoor

94
00:03:48,080 --> 00:03:50,640
flipping samples to the other side

95
00:03:50,640 --> 00:03:53,280
the length of orange lines denotes the

96
00:03:53,280 --> 00:03:54,640
cast distance

97
00:03:54,640 --> 00:03:57,040
we can see that the distance are small

98
00:03:57,040 --> 00:03:59,120
meaning the samples are close to the

99
00:03:59,120 --> 00:04:01,360
decision boundary

100
00:04:01,360 --> 00:04:03,760
a street forward proposal will be

101
00:04:03,760 --> 00:04:05,680
adversarial training

102
00:04:05,680 --> 00:04:08,239
it aims to have a consistent prediction

103
00:04:08,239 --> 00:04:12,640
for example within the lp norm ball

104
00:04:12,640 --> 00:04:15,840
this shows the training procedure

105
00:04:15,840 --> 00:04:17,839
and the decision manager after

106
00:04:17,839 --> 00:04:20,079
adversarial training

107
00:04:20,079 --> 00:04:22,880
however it is not sufficient for

108
00:04:22,880 --> 00:04:25,520
distance enlargement

109
00:04:25,520 --> 00:04:28,479
now let's see our goal this is the

110
00:04:28,479 --> 00:04:30,800
original boundary and

111
00:04:30,800 --> 00:04:33,520
distances

112
00:04:34,000 --> 00:04:36,000
from the math point of view suppose

113
00:04:36,000 --> 00:04:38,160
there is a line linking the two class

114
00:04:38,160 --> 00:04:39,520
centers

115
00:04:39,520 --> 00:04:40,320
the

116
00:04:40,320 --> 00:04:41,120
the

117
00:04:41,120 --> 00:04:43,680
largest distance can be achieved

118
00:04:43,680 --> 00:04:46,000
when the boundary is perpendicular to

119
00:04:46,000 --> 00:04:49,440
the line here

120
00:04:51,120 --> 00:04:54,000
our goal is hence to improve the model

121
00:04:54,000 --> 00:04:56,479
robustness against vector attacks by

122
00:04:56,479 --> 00:04:59,120
energy pairwise class distances while

123
00:04:59,120 --> 00:05:02,320
maintaining normal functionalities

124
00:05:02,320 --> 00:05:04,160
when you remove injected battles in

125
00:05:04,160 --> 00:05:06,560
points and models we assume only five

126
00:05:06,560 --> 00:05:10,000
percent of the train data are available

127
00:05:10,000 --> 00:05:12,880
we consider static trigger that does not

128
00:05:12,880 --> 00:05:16,240
change regarding the input such as patch

129
00:05:16,240 --> 00:05:17,759
vectors

130
00:05:17,759 --> 00:05:19,520
and a label specific

131
00:05:19,520 --> 00:05:20,800
that

132
00:05:20,800 --> 00:05:22,960
aims to flip the predictions of the

133
00:05:22,960 --> 00:05:24,560
samples from a

134
00:05:24,560 --> 00:05:27,680
victim class to a target class

135
00:05:27,680 --> 00:05:29,199
both injected backdoors and

136
00:05:29,199 --> 00:05:30,960
naturobacters are

137
00:05:30,960 --> 00:05:34,880
harmful we handle both cases

138
00:05:34,880 --> 00:05:36,639
a straightforward idea of model

139
00:05:36,639 --> 00:05:38,800
hardening is as follows

140
00:05:38,800 --> 00:05:42,240
given a model and is stringing the data

141
00:05:42,240 --> 00:05:44,080
we randomly select the pair and the

142
00:05:44,080 --> 00:05:45,840
generator trigger

143
00:05:45,840 --> 00:05:48,320
the normal sample and the sample with

144
00:05:48,320 --> 00:05:50,880
injected trig are used to train the

145
00:05:50,880 --> 00:05:53,919
model it finally outputs a hardened

146
00:05:53,919 --> 00:05:55,360
model

147
00:05:55,360 --> 00:05:58,880
this idea however has a few problems

148
00:05:58,880 --> 00:06:02,560
the first problem is that

149
00:06:02,800 --> 00:06:05,039
not all the cost pairs have the same

150
00:06:05,039 --> 00:06:08,960
potential for distance improving

151
00:06:08,960 --> 00:06:11,440
this figure shows the distance change

152
00:06:11,440 --> 00:06:13,199
for three class pairs

153
00:06:13,199 --> 00:06:14,800
during training

154
00:06:14,800 --> 00:06:18,240
the x-axis is the number of training

155
00:06:18,240 --> 00:06:20,880
iterations and the y-axis is the class

156
00:06:20,880 --> 00:06:22,479
distance

157
00:06:22,479 --> 00:06:24,960
let's take a look at this pair the end

158
00:06:24,960 --> 00:06:26,160
holes

159
00:06:26,160 --> 00:06:28,560
they are similar in humans eyes

160
00:06:28,560 --> 00:06:31,919
compared to the other two class pairs

161
00:06:31,919 --> 00:06:33,680
the distance does not

162
00:06:33,680 --> 00:06:36,639
seem to change a lot during training

163
00:06:36,639 --> 00:06:39,199
for another pair bird and horse

164
00:06:39,199 --> 00:06:41,759
visually they are quite different

165
00:06:41,759 --> 00:06:42,960
the distance

166
00:06:42,960 --> 00:06:45,600
is improved greatly

167
00:06:45,600 --> 00:06:49,280
however it saturates after a number of a

168
00:06:49,280 --> 00:06:52,479
certain number of iterations

169
00:06:52,479 --> 00:06:55,360
if we treat each pair to their maximum

170
00:06:55,360 --> 00:06:58,319
clearly it is not cost efficient

171
00:06:58,319 --> 00:07:00,840
it has quadratic

172
00:07:00,840 --> 00:07:03,919
complexity if we randomly select a pair

173
00:07:03,919 --> 00:07:07,520
at each iteration it is also not ideal

174
00:07:07,520 --> 00:07:10,240
as we have seen here some pairs may not

175
00:07:10,240 --> 00:07:12,639
improve during training

176
00:07:12,639 --> 00:07:15,039
hence we need a better way to select the

177
00:07:15,039 --> 00:07:18,719
cost pairs for hardening

178
00:07:18,880 --> 00:07:20,960
but at the beginning of training we have

179
00:07:20,960 --> 00:07:23,759
no idea which pair is promising

180
00:07:23,759 --> 00:07:26,720
a straightforward idea is to run a few

181
00:07:26,720 --> 00:07:29,680
runs of optimization for every pair and

182
00:07:29,680 --> 00:07:32,800
use the distance for further improvement

183
00:07:32,800 --> 00:07:35,360
but it is quite expensive

184
00:07:35,360 --> 00:07:37,919
we introduce an approximation with

185
00:07:37,919 --> 00:07:40,720
linear complexity

186
00:07:40,720 --> 00:07:43,199
specifically we leverage one round of

187
00:07:43,199 --> 00:07:46,000
universal backdoor generation to probe

188
00:07:46,000 --> 00:07:49,840
the potential of each source class

189
00:07:49,840 --> 00:07:51,919
the universal factor aims to flip a

190
00:07:51,919 --> 00:07:54,960
small set of samples from all classes to

191
00:07:54,960 --> 00:07:57,039
a tag class

192
00:07:57,039 --> 00:07:59,840
we then observe the last change of each

193
00:07:59,840 --> 00:08:02,479
class to the target

194
00:08:02,479 --> 00:08:05,039
the difficulty of flipping samples using

195
00:08:05,039 --> 00:08:07,520
the universal trigger indicates how

196
00:08:07,520 --> 00:08:10,799
promising a pair is

197
00:08:11,360 --> 00:08:14,639
this shows how the warm up comes in our

198
00:08:14,639 --> 00:08:17,680
hardening pipeline

199
00:08:17,759 --> 00:08:20,319
the warm-up is the starting point

200
00:08:20,319 --> 00:08:22,879
with the training moving forward we have

201
00:08:22,879 --> 00:08:26,400
more information which can be leveraged

202
00:08:26,400 --> 00:08:29,280
we use a scheduler to select a

203
00:08:29,280 --> 00:08:32,000
multi-promising pair for hardening based

204
00:08:32,000 --> 00:08:35,120
on a historical view view of a distance

205
00:08:35,120 --> 00:08:37,599
improvement

206
00:08:37,599 --> 00:08:39,039
specifically

207
00:08:39,039 --> 00:08:41,919
we make use of accumulated cost distance

208
00:08:41,919 --> 00:08:45,920
change to update each pair's potential

209
00:08:45,920 --> 00:08:48,160
which is the relative improvement of the

210
00:08:48,160 --> 00:08:52,080
current distance over the previous one

211
00:08:52,080 --> 00:08:54,399
the local improvement is then combined

212
00:08:54,399 --> 00:08:57,680
with the results from the warm-up

213
00:08:57,680 --> 00:08:59,200
the schedule is

214
00:08:59,200 --> 00:09:03,519
is then added in hour of hardening

215
00:09:03,519 --> 00:09:05,600
the second problem the previous

216
00:09:05,600 --> 00:09:08,240
straightforward hardening idea is that

217
00:09:08,240 --> 00:09:10,800
training two direct along two directions

218
00:09:10,800 --> 00:09:13,440
of a pair may cancel each other out

219
00:09:13,440 --> 00:09:15,440
let's take a look at the

220
00:09:15,440 --> 00:09:18,080
decision boundary if we train the two

221
00:09:18,080 --> 00:09:20,320
directions separately

222
00:09:20,320 --> 00:09:23,440
if we push the distance from the green

223
00:09:23,440 --> 00:09:25,839
side the boundary moves to the other

224
00:09:25,839 --> 00:09:28,480
side which leads to a smaller distance

225
00:09:28,480 --> 00:09:29,519
for the

226
00:09:29,519 --> 00:09:31,519
blue class

227
00:09:31,519 --> 00:09:33,440
if we push the distance from the blue

228
00:09:33,440 --> 00:09:36,800
side the boundary moves back to its

229
00:09:36,800 --> 00:09:38,560
original position

230
00:09:38,560 --> 00:09:41,839
the distance are not enlarged for both

231
00:09:41,839 --> 00:09:43,760
classes

232
00:09:43,760 --> 00:09:45,680
now let's take a look at

233
00:09:45,680 --> 00:09:47,200
training the two directions

234
00:09:47,200 --> 00:09:49,360
simultaneously

235
00:09:49,360 --> 00:09:51,920
the puncture is rotated as we are

236
00:09:51,920 --> 00:09:53,839
pushing it from the two directions

237
00:09:53,839 --> 00:09:56,080
together

238
00:09:56,080 --> 00:09:57,920
when the training converges

239
00:09:57,920 --> 00:10:00,720
we have the largest distance for the two

240
00:10:00,720 --> 00:10:01,839
directions

241
00:10:01,839 --> 00:10:03,440
the bound the boundary is a

242
00:10:03,440 --> 00:10:06,079
perpendicular to the line linking the

243
00:10:06,079 --> 00:10:09,199
two class centers

244
00:10:09,920 --> 00:10:12,880
we hence apply symmetric hardening which

245
00:10:12,880 --> 00:10:15,120
hardens the two directions of a pair

246
00:10:15,120 --> 00:10:17,200
simultaneously

247
00:10:17,200 --> 00:10:19,360
we propose a two-sided backdoor

248
00:10:19,360 --> 00:10:23,120
generation which introduces two triggers

249
00:10:23,120 --> 00:10:24,800
and the stamps on

250
00:10:24,800 --> 00:10:28,800
one on each direction of a pair

251
00:10:29,360 --> 00:10:31,200
we generate the two vectors

252
00:10:31,200 --> 00:10:33,920
triggers simultaneously

253
00:10:33,920 --> 00:10:36,240
backdoors for the two directions are

254
00:10:36,240 --> 00:10:38,160
then added for the

255
00:10:38,160 --> 00:10:40,719
hardening

256
00:10:41,839 --> 00:10:44,720
we conduct the evaluation on a set of

257
00:10:44,720 --> 00:10:48,240
standard data sets and models

258
00:10:48,240 --> 00:10:50,560
we also download the 89 pre-trained

259
00:10:50,560 --> 00:10:54,479
models for the evaluation

260
00:10:55,120 --> 00:10:57,040
the base science you

261
00:10:57,040 --> 00:10:59,040
could include using universal vectors

262
00:10:59,040 --> 00:11:01,680
for hardening which is an extension of

263
00:11:01,680 --> 00:11:03,600
an existing work

264
00:11:03,600 --> 00:11:05,920
we also evaluated the performance of a

265
00:11:05,920 --> 00:11:08,720
universal adversary perturbation and a

266
00:11:08,720 --> 00:11:11,839
two bacterial removal

267
00:11:12,839 --> 00:11:15,519
techniques the relative improvement of

268
00:11:15,519 --> 00:11:18,720
pairwise class distance for all pairs is

269
00:11:18,720 --> 00:11:21,120
used as the metric

270
00:11:21,120 --> 00:11:23,120
we study naturally trained and

271
00:11:23,120 --> 00:11:26,320
adversarially trained models

272
00:11:26,320 --> 00:11:28,800
this this table shows the results for

273
00:11:28,800 --> 00:11:30,800
naturally trained models

274
00:11:30,800 --> 00:11:33,279
this first column shows different

275
00:11:33,279 --> 00:11:35,760
hardening techniques the second second

276
00:11:35,760 --> 00:11:38,880
column column the test accuracy the

277
00:11:38,880 --> 00:11:41,440
third column the training time and the

278
00:11:41,440 --> 00:11:45,200
fourth column the average cast distance

279
00:11:45,200 --> 00:11:47,200
the last column shows the relative

280
00:11:47,200 --> 00:11:49,839
improvement

281
00:11:50,160 --> 00:11:52,639
we have we can see that mouse has the

282
00:11:52,639 --> 00:11:55,120
largest distance improvement with

283
00:11:55,120 --> 00:11:57,839
reasonable time cost and the accuracy

284
00:11:57,839 --> 00:12:00,079
drop

285
00:12:00,480 --> 00:12:02,720
the observations on adversarially

286
00:12:02,720 --> 00:12:05,040
trained models are similar

287
00:12:05,040 --> 00:12:06,959
mars had the largest distance

288
00:12:06,959 --> 00:12:08,320
improvement

289
00:12:08,320 --> 00:12:11,360
with limited accuracy degradation

290
00:12:11,360 --> 00:12:13,839
and there is no robustness job on

291
00:12:13,839 --> 00:12:16,320
average

292
00:12:16,880 --> 00:12:20,079
we also apply mouths in a downstream

293
00:12:20,079 --> 00:12:22,480
application of eliminating injected

294
00:12:22,480 --> 00:12:24,240
backdoors

295
00:12:24,240 --> 00:12:26,720
moss can reduce the attack success rate

296
00:12:26,720 --> 00:12:30,320
from almost 100 percent to 1

297
00:12:30,320 --> 00:12:34,240
for 15 time backdoor models

298
00:12:34,399 --> 00:12:37,040
in comparison the baselines can only

299
00:12:37,040 --> 00:12:38,880
reduce to 27

300
00:12:38,880 --> 00:12:41,519
at most

301
00:12:41,760 --> 00:12:45,839
our implementation is available online

302
00:12:45,839 --> 00:12:50,639
here is a list of our related work

303
00:12:50,800 --> 00:12:53,839
in conclusion we propose a model a

304
00:12:53,839 --> 00:12:56,079
normal model hardening technique mods

305
00:12:56,079 --> 00:12:58,240
that can improve model's robustness

306
00:12:58,240 --> 00:13:01,519
against against the bacterial attacks

307
00:13:01,519 --> 00:13:02,880
moss can

308
00:13:02,880 --> 00:13:06,800
greatly improve class distances by 177

309
00:13:06,800 --> 00:13:09,200
on average with

310
00:13:09,200 --> 00:13:13,920
limited accuracy loss for 48 models

311
00:13:13,920 --> 00:13:16,880
it also reduces the attack success rate

312
00:13:16,880 --> 00:13:18,839
of injected backdoors by

313
00:13:18,839 --> 00:13:21,760
99 outperforming state-of-the-art

314
00:13:21,760 --> 00:13:24,639
removal techniques

315
00:13:24,639 --> 00:13:26,800
thank you i'm happy to take any

316
00:13:26,800 --> 00:13:29,800
questions

317
00:13:30,570 --> 00:13:34,029
[Applause]

318
00:13:34,480 --> 00:13:35,600
you have any questions please stop the

319
00:13:35,600 --> 00:13:38,000
microphone

320
00:13:44,399 --> 00:13:47,120
how does that trigger type for example

321
00:13:47,120 --> 00:13:49,360
being semantic trigger or adaptive

322
00:13:49,360 --> 00:13:51,920
trigger affect this

323
00:13:51,920 --> 00:13:55,440
harding process that you have

324
00:13:55,440 --> 00:13:57,920
so actually during the hardening we do

325
00:13:57,920 --> 00:13:59,600
not separate

326
00:13:59,600 --> 00:14:01,839
differentiate the semantic trigger or

327
00:14:01,839 --> 00:14:04,639
like just random pixel triggers

328
00:14:04,639 --> 00:14:07,199
we just consider they are all harmful

329
00:14:07,199 --> 00:14:10,399
that will make the model uh vulnerable

330
00:14:10,399 --> 00:14:13,440
so we use those both triggers to like

331
00:14:13,440 --> 00:14:15,440
those different types of trigger to

332
00:14:15,440 --> 00:14:17,040
enlarge smaller

333
00:14:17,040 --> 00:14:18,959
cast distance which can improve its

334
00:14:18,959 --> 00:14:23,040
robustness against vector attacks

335
00:14:26,079 --> 00:14:28,399
so you harden it targeting certain

336
00:14:28,399 --> 00:14:30,000
trigger or it doesn't matter which

337
00:14:30,000 --> 00:14:33,199
trigger which back door he stole can can

338
00:14:33,199 --> 00:14:34,560
do or

339
00:14:34,560 --> 00:14:36,959
defend against any any any type of

340
00:14:36,959 --> 00:14:37,920
trigger

341
00:14:37,920 --> 00:14:39,519
even unforeseen

342
00:14:39,519 --> 00:14:41,440
or even the one that you did not use in

343
00:14:41,440 --> 00:14:43,519
your training

344
00:14:43,519 --> 00:14:44,480
so

345
00:14:44,480 --> 00:14:47,519
uh so our uh our hardened technique will

346
00:14:47,519 --> 00:14:49,600
improve that this which measures the

347
00:14:49,600 --> 00:14:51,920
size of the trigger so

348
00:14:51,920 --> 00:14:54,160
during the evaluation we only consider

349
00:14:54,160 --> 00:14:56,399
like how large the trigger will be after

350
00:14:56,399 --> 00:14:58,639
the model is hardened that means like

351
00:14:58,639 --> 00:15:00,639
how robust the model will get those kind

352
00:15:00,639 --> 00:15:03,199
of attacks so it's kind of like a metric

353
00:15:03,199 --> 00:15:05,199
to measure the robustness

354
00:15:05,199 --> 00:15:07,839
so it does not um

355
00:15:07,839 --> 00:15:10,160
like measure the semantic of the tree

356
00:15:10,160 --> 00:15:11,519
it's more like a

357
00:15:11,519 --> 00:15:14,639
measure like how hard it is to generate

358
00:15:14,639 --> 00:15:16,320
a trigger

359
00:15:16,320 --> 00:15:19,760
does that answer your question

360
00:15:21,680 --> 00:15:24,399
uh hi so i'm shinda from universal

361
00:15:24,399 --> 00:15:26,480
warlord so i just have one question so

362
00:15:26,480 --> 00:15:28,800
have you tested your method against uh

363
00:15:28,800 --> 00:15:31,040
adverse example attacks because it seems

364
00:15:31,040 --> 00:15:31,839
like

365
00:15:31,839 --> 00:15:34,000
by enlarging the distance between the

366
00:15:34,000 --> 00:15:36,160
test sample and the boundary

367
00:15:36,160 --> 00:15:38,480
will prevent some of these adverse

368
00:15:38,480 --> 00:15:41,839
attack attacks uh yes we have evaluated

369
00:15:41,839 --> 00:15:42,560
the

370
00:15:42,560 --> 00:15:45,199
result on the adversary attacks actually

371
00:15:45,199 --> 00:15:46,639
because

372
00:15:46,639 --> 00:15:47,759
for our

373
00:15:47,759 --> 00:15:50,320
generation method it's like focus on the

374
00:15:50,320 --> 00:15:52,480
um the universal

375
00:15:52,480 --> 00:15:54,800
kind of like the overall distance from

376
00:15:54,800 --> 00:15:56,720
the center of the

377
00:15:56,720 --> 00:15:59,759
sample mass to the boundary so sometimes

378
00:15:59,759 --> 00:16:02,399
for those like a local perturbations on

379
00:16:02,399 --> 00:16:04,320
the input it will still cause the

380
00:16:04,320 --> 00:16:06,880
adverse vulnerabilities to model

381
00:16:06,880 --> 00:16:10,480
but our techniques is like applicable to

382
00:16:10,480 --> 00:16:12,160
different types of models like natural

383
00:16:12,160 --> 00:16:14,240
nutrient models also adversarial trained

384
00:16:14,240 --> 00:16:17,279
models so we evaluate our method on the

385
00:16:17,279 --> 00:16:19,600
anniversary training models it can

386
00:16:19,600 --> 00:16:22,160
improve the cast distance like those

387
00:16:22,160 --> 00:16:24,560
universal triggers and also maintaining

388
00:16:24,560 --> 00:16:26,959
the original like the robustness of the

389
00:16:26,959 --> 00:16:30,160
model so if we want to have a robust

390
00:16:30,160 --> 00:16:32,399
model against both adverse attacks and

391
00:16:32,399 --> 00:16:35,199
also back to attacks we can

392
00:16:35,199 --> 00:16:37,360
apply like a combining the adversarial

393
00:16:37,360 --> 00:16:40,320
training and the the our uh technique so

394
00:16:40,320 --> 00:16:43,120
we have like have a overall robust model

395
00:16:43,120 --> 00:16:45,920
against those type of attacks

396
00:16:45,920 --> 00:16:47,920
that's answer your question yeah thank

397
00:16:47,920 --> 00:16:50,079
you thank you

398
00:16:50,079 --> 00:16:53,199
maybe one last question then um

399
00:16:53,199 --> 00:16:54,639
supposing that i only cared about

400
00:16:54,639 --> 00:16:56,480
certain class pairs

401
00:16:56,480 --> 00:16:58,639
is it possible to maybe restrict your

402
00:16:58,639 --> 00:17:01,360
defense to make it really really hard to

403
00:17:01,360 --> 00:17:02,959
go from one class to a different class

404
00:17:02,959 --> 00:17:04,880
i'm thinking maybe it's more important

405
00:17:04,880 --> 00:17:07,679
that i can't turn my malware file benign

406
00:17:07,679 --> 00:17:09,839
than going from a benign file to malware

407
00:17:09,839 --> 00:17:11,119
can you set up

408
00:17:11,119 --> 00:17:13,439
the the like your problem formulation in

409
00:17:13,439 --> 00:17:15,280
such a way that you can improve certain

410
00:17:15,280 --> 00:17:16,959
classes more than others that might be

411
00:17:16,959 --> 00:17:18,959
one benefit

412
00:17:18,959 --> 00:17:21,919
yeah that's possible so for the image uh

413
00:17:21,919 --> 00:17:23,679
classification actually we observe that

414
00:17:23,679 --> 00:17:25,359
the different uh class pairs have

415
00:17:25,359 --> 00:17:27,839
different potentials for some pairs

416
00:17:27,839 --> 00:17:29,919
actually they are very vulnerable sorry

417
00:17:29,919 --> 00:17:33,120
they are very close like a from humans

418
00:17:33,120 --> 00:17:35,039
perspective though so that means for

419
00:17:35,039 --> 00:17:38,320
those kind of uh pairs uh if how even

420
00:17:38,320 --> 00:17:40,559
though we like trying to like improve

421
00:17:40,559 --> 00:17:42,720
their distance eventually they will not

422
00:17:42,720 --> 00:17:44,720
be improved because like uh from the

423
00:17:44,720 --> 00:17:46,799
data distribution point of view they

424
00:17:46,799 --> 00:17:48,960
will always be that codes but like for

425
00:17:48,960 --> 00:17:51,600
some cases like they are quite different

426
00:17:51,600 --> 00:17:53,840
then we can have like a

427
00:17:53,840 --> 00:17:55,760
much larger distance that i can improve

428
00:17:55,760 --> 00:17:58,080
like the case that you mentioned earlier

429
00:17:58,080 --> 00:18:01,840
from the to but i mod benign like

430
00:18:01,840 --> 00:18:03,760
subtourists there's a possibility that

431
00:18:03,760 --> 00:18:06,640
we can like enlarge really enlarge their

432
00:18:06,640 --> 00:18:08,480
distance so it's not possible to flip

433
00:18:08,480 --> 00:18:09,440
them

434
00:18:09,440 --> 00:18:11,760
yeah so here we can see like the two

435
00:18:11,760 --> 00:18:13,280
directions from like

436
00:18:13,280 --> 00:18:14,880
the case you mentioned like from where

437
00:18:14,880 --> 00:18:16,960
malware and the two binary and in the

438
00:18:16,960 --> 00:18:19,200
the case there we probably more care

439
00:18:19,200 --> 00:18:21,440
about it from milwaukee to bernard so we

440
00:18:21,440 --> 00:18:24,160
can directly just enlarge that direction

441
00:18:24,160 --> 00:18:26,400
not like both direction because that

442
00:18:26,400 --> 00:18:28,720
direction is more important for our

443
00:18:28,720 --> 00:18:30,080
scenario

444
00:18:30,080 --> 00:18:31,600
thank you

445
00:18:31,600 --> 00:18:36,120
let's thank the speaker again thank you

