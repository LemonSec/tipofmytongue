1
00:00:01,199 --> 00:00:03,760
hi my name is hamza i'm a research

2
00:00:03,760 --> 00:00:06,319
scientist at google today i will be

3
00:00:06,319 --> 00:00:08,080
presenting disjoint work with my

4
00:00:08,080 --> 00:00:11,360
collaborators sai rishab animesh nina

5
00:00:11,360 --> 00:00:13,840
titled heart a deep learning system for

6
00:00:13,840 --> 00:00:17,199
navigating privacy feedback at scale

7
00:00:17,199 --> 00:00:18,880
as we all know there have been

8
00:00:18,880 --> 00:00:21,199
significant efforts lately

9
00:00:21,199 --> 00:00:22,720
to improve the developer to user

10
00:00:22,720 --> 00:00:25,439
communication channel one clear example

11
00:00:25,439 --> 00:00:27,279
is the data safety section on the play

12
00:00:27,279 --> 00:00:29,920
store where users can see how their data

13
00:00:29,920 --> 00:00:32,399
is being used and shared

14
00:00:32,399 --> 00:00:34,800
however the reverse user to developer

15
00:00:34,800 --> 00:00:37,200
channel remains underexplored in this

16
00:00:37,200 --> 00:00:39,040
work we asked the question

17
00:00:39,040 --> 00:00:40,000
of

18
00:00:40,000 --> 00:00:41,920
how can we create a better user

19
00:00:41,920 --> 00:00:44,719
developer channel to communicate privacy

20
00:00:44,719 --> 00:00:46,320
concerns

21
00:00:46,320 --> 00:00:48,559
and we answer that question with heart

22
00:00:48,559 --> 00:00:50,960
hark takes the set of reviews submitted

23
00:00:50,960 --> 00:00:53,520
on the play store as an input this can

24
00:00:53,520 --> 00:00:55,600
be about any topic not only about

25
00:00:55,600 --> 00:00:56,800
privacy

26
00:00:56,800 --> 00:00:59,039
we built a fully automated pipeline that

27
00:00:59,039 --> 00:01:01,600
consumes these reviews and outputs and

28
00:01:01,600 --> 00:01:04,080
quantifies the top privacy keys for

29
00:01:04,080 --> 00:01:06,320
example here it says that the unneeded

30
00:01:06,320 --> 00:01:10,080
access is present in 546k reviews

31
00:01:10,080 --> 00:01:12,320
the top fine-grained issue in that theme

32
00:01:12,320 --> 00:01:14,640
is the unneeded location access with 89

33
00:01:14,640 --> 00:01:15,600
kgb

34
00:01:15,600 --> 00:01:18,240
to illustrate this issue hark allows

35
00:01:18,240 --> 00:01:19,920
discovering high quality reviews

36
00:01:19,920 --> 00:01:21,759
diversified across the emotions

37
00:01:21,759 --> 00:01:24,159
dimension like angry reviews confused

38
00:01:24,159 --> 00:01:26,320
reviews etc

39
00:01:26,320 --> 00:01:28,640
one might ask why is the developer why

40
00:01:28,640 --> 00:01:30,640
is the user the developer channel

41
00:01:30,640 --> 00:01:32,720
importing

42
00:01:32,720 --> 00:01:34,159
previous works suggest that the

43
00:01:34,159 --> 00:01:36,320
developers who are made aware of privacy

44
00:01:36,320 --> 00:01:39,200
reviews tend to carry out late updates

45
00:01:39,200 --> 00:01:41,040
in the permissions context nudging

46
00:01:41,040 --> 00:01:42,799
developers results and reducing

47
00:01:42,799 --> 00:01:44,799
unnecessary permissions

48
00:01:44,799 --> 00:01:46,320
it has been also shown that there is a

49
00:01:46,320 --> 00:01:48,079
correlation between low app ratings and

50
00:01:48,079 --> 00:01:50,000
negative privacy reviews

51
00:01:50,000 --> 00:01:52,079
which can serve as a motivation for

52
00:01:52,079 --> 00:01:54,720
developers to act

53
00:01:54,720 --> 00:01:56,880
now we provide an overview of hark's

54
00:01:56,880 --> 00:01:59,520
component in the following slide

55
00:01:59,520 --> 00:02:02,000
hard works in multiple stages given a

56
00:02:02,000 --> 00:02:03,040
set

57
00:02:03,040 --> 00:02:04,479
of reviews

58
00:02:04,479 --> 00:02:06,960
and the level of app developer or even

59
00:02:06,960 --> 00:02:08,800
the store category

60
00:02:08,800 --> 00:02:11,038
we have a privacy classifier that finds

61
00:02:11,038 --> 00:02:12,720
the privacy feedback

62
00:02:12,720 --> 00:02:15,200
the feedback is then fed into an issue

63
00:02:15,200 --> 00:02:17,360
generation model that produces

64
00:02:17,360 --> 00:02:19,520
fine-grained issues representing each

65
00:02:19,520 --> 00:02:20,959
review

66
00:02:20,959 --> 00:02:21,760
then

67
00:02:21,760 --> 00:02:23,680
these issues are grouped into higher

68
00:02:23,680 --> 00:02:24,959
level themes

69
00:02:24,959 --> 00:02:27,840
that are easily glanced

70
00:02:27,840 --> 00:02:29,599
to make it easy to navigate through the

71
00:02:29,599 --> 00:02:32,560
feedback we have two additional models

72
00:02:32,560 --> 00:02:34,879
the first is an emotions classifier that

73
00:02:34,879 --> 00:02:37,040
assigns one of 28 emotions to each

74
00:02:37,040 --> 00:02:39,200
review allowing developers to filter

75
00:02:39,200 --> 00:02:40,959
data by emotions

76
00:02:40,959 --> 00:02:43,280
we also have a feedback quality scoring

77
00:02:43,280 --> 00:02:45,680
model that surfaces the highest quality

78
00:02:45,680 --> 00:02:47,519
reviews to the top

79
00:02:47,519 --> 00:02:49,680
we then can perform a variety of trend

80
00:02:49,680 --> 00:02:51,599
analysis on top of the outputs from

81
00:02:51,599 --> 00:02:53,519
these models

82
00:02:53,519 --> 00:02:55,760
here's an example of how reviews look

83
00:02:55,760 --> 00:02:57,360
like they can be very informal and

84
00:02:57,360 --> 00:02:59,680
commonly have spelling errors

85
00:02:59,680 --> 00:03:01,599
our privacy classifier sifts through the

86
00:03:01,599 --> 00:03:03,760
data to extract the privacy reviews and

87
00:03:03,760 --> 00:03:06,400
exclude the non-privacy ones

88
00:03:06,400 --> 00:03:08,400
our issue generation model assigns each

89
00:03:08,400 --> 00:03:10,319
review one or more fine-grained issue

90
00:03:10,319 --> 00:03:13,280
tags for example unneeded contact access

91
00:03:13,280 --> 00:03:15,040
is assigned to the review saying why

92
00:03:15,040 --> 00:03:18,879
does it need access to my contacts now

93
00:03:18,879 --> 00:03:21,599
next issues are clustered into themes

94
00:03:21,599 --> 00:03:23,760
here we abstract the combination of

95
00:03:23,760 --> 00:03:25,519
height photos hide videos and height

96
00:03:25,519 --> 00:03:27,519
files into content hiding

97
00:03:27,519 --> 00:03:29,120
all of this is done automatically

98
00:03:29,120 --> 00:03:30,400
without being restricted to a

99
00:03:30,400 --> 00:03:34,159
predetermined set of issues or themes

100
00:03:34,159 --> 00:03:36,000
so we start by describing the privacy

101
00:03:36,000 --> 00:03:37,440
classifier

102
00:03:37,440 --> 00:03:39,519
we want to differentiate between privacy

103
00:03:39,519 --> 00:03:41,599
and not privacy feedback but the main

104
00:03:41,599 --> 00:03:43,360
challenge is that the data for this

105
00:03:43,360 --> 00:03:46,319
problem is severely imbalanced previous

106
00:03:46,319 --> 00:03:48,080
works estimate that reviews about

107
00:03:48,080 --> 00:03:50,959
privacy are less than 0.5 percent of the

108
00:03:50,959 --> 00:03:52,799
reviews

109
00:03:52,799 --> 00:03:54,799
some approaches try to mitigate this by

110
00:03:54,799 --> 00:03:56,480
searching via keywords regular

111
00:03:56,480 --> 00:03:58,239
expressions and labeling data that is

112
00:03:58,239 --> 00:04:00,560
surfaced

113
00:04:00,560 --> 00:04:02,400
this leads to

114
00:04:02,400 --> 00:04:05,120
the model overfitting on the presence or

115
00:04:05,120 --> 00:04:07,920
absence of these expressions

116
00:04:07,920 --> 00:04:10,560
our approach is to use a natural natural

117
00:04:10,560 --> 00:04:12,799
language inference model for retrieving

118
00:04:12,799 --> 00:04:14,560
semantically relevant data to the

119
00:04:14,560 --> 00:04:16,639
privacy domain

120
00:04:16,639 --> 00:04:19,440
we guide our retrieval with concepts

121
00:04:19,440 --> 00:04:21,918
selected from privacy taxonomy the

122
00:04:21,918 --> 00:04:23,919
advantage of this is that it can

123
00:04:23,919 --> 00:04:26,479
generalize well beyond keywords

124
00:04:26,479 --> 00:04:29,120
it can also achieve a higher coverage of

125
00:04:29,120 --> 00:04:31,680
a variety of privacy concepts

126
00:04:31,680 --> 00:04:33,040
now as i mentioned we leverage a

127
00:04:33,040 --> 00:04:34,880
well-known task in nlp which is natural

128
00:04:34,880 --> 00:04:36,960
language inference in this task we are

129
00:04:36,960 --> 00:04:39,919
given a premise and a hypothesis and we

130
00:04:39,919 --> 00:04:41,600
want to determine if the hypothesis is

131
00:04:41,600 --> 00:04:43,840
true false or undetermined given the

132
00:04:43,840 --> 00:04:45,040
premise

133
00:04:45,040 --> 00:04:47,360
let's take a generic example houston is

134
00:04:47,360 --> 00:04:48,960
freezing and dry right now if my

135
00:04:48,960 --> 00:04:50,960
hypothesis is that the city is really

136
00:04:50,960 --> 00:04:52,320
humid now

137
00:04:52,320 --> 00:04:53,840
there is a contradiction

138
00:04:53,840 --> 00:04:56,240
my hypothesis is that it's cold then

139
00:04:56,240 --> 00:04:58,560
that's true we say there's entertainment

140
00:04:58,560 --> 00:05:00,960
if my hypothesis is that it's crowded

141
00:05:00,960 --> 00:05:02,960
then the verdict is neutral there's

142
00:05:02,960 --> 00:05:05,039
nothing proving either way

143
00:05:05,039 --> 00:05:08,080
our idea is to use a generic model

144
00:05:08,080 --> 00:05:10,160
trained on analyte data sets to sample

145
00:05:10,160 --> 00:05:14,400
data matching privacy hypotheses

146
00:05:14,400 --> 00:05:16,320
let's see how we do that we started from

147
00:05:16,320 --> 00:05:17,600
a couple of well-known privacy

148
00:05:17,600 --> 00:05:20,240
taxonomies solos taxonomy of privacy

149
00:05:20,240 --> 00:05:22,400
violations and wangs and cops as

150
00:05:22,400 --> 00:05:25,199
taxonomy enhancing technologies

151
00:05:25,199 --> 00:05:27,120
from each of these we extracted the

152
00:05:27,120 --> 00:05:29,120
concepts that apply to the context of

153
00:05:29,120 --> 00:05:30,400
app reviews

154
00:05:30,400 --> 00:05:32,880
we also added generic privacy concepts

155
00:05:32,880 --> 00:05:34,639
for general dimensions of negative or

156
00:05:34,639 --> 00:05:37,039
positive privacy issues

157
00:05:37,039 --> 00:05:39,039
next we came up with one or more

158
00:05:39,039 --> 00:05:41,440
hypotheses for each concept

159
00:05:41,440 --> 00:05:43,680
the goal is not to be comprehensive but

160
00:05:43,680 --> 00:05:45,919
to ensure that we have examples of each

161
00:05:45,919 --> 00:05:46,960
concept

162
00:05:46,960 --> 00:05:49,759
when we label data

163
00:05:49,759 --> 00:05:51,440
for instance for the concept use

164
00:05:51,440 --> 00:05:52,479
limitation

165
00:05:52,479 --> 00:05:54,400
one hypothesis would be the data is

166
00:05:54,400 --> 00:05:57,919
being used for unexpected purposes

167
00:05:57,919 --> 00:06:00,160
finally we apply this idea by checking

168
00:06:00,160 --> 00:06:03,039
where each hypothesis applies

169
00:06:03,039 --> 00:06:06,000
within a corpus of 9 million reviews for

170
00:06:06,000 --> 00:06:08,400
example these are two cases with high

171
00:06:08,400 --> 00:06:10,479
entailment score for the disclosure

172
00:06:10,479 --> 00:06:11,520
console

173
00:06:11,520 --> 00:06:13,520
notice that neither of them has the word

174
00:06:13,520 --> 00:06:14,720
disclosure

175
00:06:14,720 --> 00:06:16,720
the first review doesn't even share a

176
00:06:16,720 --> 00:06:19,120
single word with the hypothesis but the

177
00:06:19,120 --> 00:06:21,120
entailment relationship is detected by

178
00:06:21,120 --> 00:06:22,720
this model

179
00:06:22,720 --> 00:06:25,440
our approach is to use the entailment

180
00:06:25,440 --> 00:06:28,160
score as a way to get a pseudo

181
00:06:28,160 --> 00:06:30,560
balanced candidate set for for data

182
00:06:30,560 --> 00:06:31,840
labeling

183
00:06:31,840 --> 00:06:34,479
reviews with high entailment scores

184
00:06:34,479 --> 00:06:36,400
for some hypotheses

185
00:06:36,400 --> 00:06:38,720
are potentially privacy delayed

186
00:06:38,720 --> 00:06:40,720
those with low entailment score for all

187
00:06:40,720 --> 00:06:44,560
hypotheses are potentially not privacy

188
00:06:44,560 --> 00:06:46,560
the candidate set is then passed to

189
00:06:46,560 --> 00:06:49,199
human annotators

190
00:06:49,199 --> 00:06:51,199
we set up the task for manually

191
00:06:51,199 --> 00:06:52,960
annotating this data

192
00:06:52,960 --> 00:06:56,479
each data item was labeled by five

193
00:06:56,479 --> 00:06:58,400
crowdsourcing annotators who were

194
00:06:58,400 --> 00:07:00,880
provided with clear guidance and what to

195
00:07:00,880 --> 00:07:03,840
consider as privacy related

196
00:07:03,840 --> 00:07:05,360
finally this

197
00:07:05,360 --> 00:07:08,080
almost balanced data set is split into

198
00:07:08,080 --> 00:07:09,680
training and testing data for our

199
00:07:09,680 --> 00:07:12,639
privacy classifier training

200
00:07:12,639 --> 00:07:14,960
we now move to the training part we use

201
00:07:14,960 --> 00:07:17,759
the t5 model as a model of choice it's a

202
00:07:17,759 --> 00:07:20,160
unified texture text model where the

203
00:07:20,160 --> 00:07:22,000
input and the output are always text

204
00:07:22,000 --> 00:07:23,440
strings this has been used for

205
00:07:23,440 --> 00:07:25,680
classification summarization translation

206
00:07:25,680 --> 00:07:26,800
etc

207
00:07:26,800 --> 00:07:28,479
and has been leading the natural

208
00:07:28,479 --> 00:07:30,639
language understanding benchmarks

209
00:07:30,639 --> 00:07:32,160
recently

210
00:07:32,160 --> 00:07:34,000
since it is using the text-to-text

211
00:07:34,000 --> 00:07:35,120
paradigm

212
00:07:35,120 --> 00:07:37,440
we also use it for other problems in

213
00:07:37,440 --> 00:07:39,039
this work

214
00:07:39,039 --> 00:07:41,039
we apply it in the privacy case by

215
00:07:41,039 --> 00:07:42,960
taking inputs such as

216
00:07:42,960 --> 00:07:44,639
always spying on me with the word

217
00:07:44,639 --> 00:07:46,319
privacy as the target

218
00:07:46,319 --> 00:07:48,639
as well as inputs like it's always

219
00:07:48,639 --> 00:07:50,240
playing with me with the word not

220
00:07:50,240 --> 00:07:52,720
privacy as the target

221
00:07:52,720 --> 00:07:55,120
to evaluate our approach we investigated

222
00:07:55,120 --> 00:07:56,479
two dimensions

223
00:07:56,479 --> 00:07:58,479
first we wanted we wanted to show the

224
00:07:58,479 --> 00:08:00,800
importance of better data sampling

225
00:08:00,800 --> 00:08:03,039
so we compare models trained on our own

226
00:08:03,039 --> 00:08:04,080
data

227
00:08:04,080 --> 00:08:06,319
versus those trained on what we call the

228
00:08:06,319 --> 00:08:09,360
exe data which is um data sampled via

229
00:08:09,360 --> 00:08:11,360
regular expressions guided

230
00:08:11,360 --> 00:08:13,680
by a set of privacy concepts and created

231
00:08:13,680 --> 00:08:16,000
by mimaeta

232
00:08:16,000 --> 00:08:18,160
we also investigate the model choice in

233
00:08:18,160 --> 00:08:20,479
addition to the t511 billion model we

234
00:08:20,479 --> 00:08:22,479
evaluate the roberta large model that is

235
00:08:22,479 --> 00:08:25,280
smaller than the t511b model but still

236
00:08:25,280 --> 00:08:27,039
has been achieving strong results on

237
00:08:27,039 --> 00:08:29,120
various classification tasks

238
00:08:29,120 --> 00:08:31,039
we additionally compare against an svm

239
00:08:31,039 --> 00:08:33,360
model based on a bag of words in an

240
00:08:33,360 --> 00:08:35,200
attempt to reproduce the model used in a

241
00:08:35,200 --> 00:08:37,839
previous work

242
00:08:38,000 --> 00:08:40,559
on this plot we show the roc curves for

243
00:08:40,559 --> 00:08:42,640
the various models when evaluated on the

244
00:08:42,640 --> 00:08:44,959
hard test cell we can observe that the

245
00:08:44,959 --> 00:08:46,880
d511b model

246
00:08:46,880 --> 00:08:51,360
has 0.17 absolute increase in auc

247
00:08:51,360 --> 00:08:53,680
when replacing x-ray data with heart

248
00:08:53,680 --> 00:08:54,720
data

249
00:08:54,720 --> 00:08:56,480
a similar trend is observed in the

250
00:08:56,480 --> 00:08:58,320
roberta large model

251
00:08:58,320 --> 00:09:00,640
indicating the importance of the data

252
00:09:00,640 --> 00:09:03,279
sampling strategy we adopted

253
00:09:03,279 --> 00:09:06,000
we can also observe that the svm model

254
00:09:06,000 --> 00:09:07,519
fails to learn the nuances of the

255
00:09:07,519 --> 00:09:10,560
problem and is limited due to relying on

256
00:09:10,560 --> 00:09:12,800
explicit and ground so deep learning

257
00:09:12,800 --> 00:09:15,279
models leverage transfer learning and

258
00:09:15,279 --> 00:09:17,200
the larger the model the better the

259
00:09:17,200 --> 00:09:19,360
results are with the same training data

260
00:09:19,360 --> 00:09:21,920
in our case

261
00:09:21,920 --> 00:09:23,440
now that we have narrowed down the

262
00:09:23,440 --> 00:09:25,360
feedback that the privacy wants we move

263
00:09:25,360 --> 00:09:28,480
towards generating the user issues

264
00:09:28,480 --> 00:09:29,440
um

265
00:09:29,440 --> 00:09:31,360
we want to generate one or more issues

266
00:09:31,360 --> 00:09:32,560
per review

267
00:09:32,560 --> 00:09:34,160
the main challenge is how can we

268
00:09:34,160 --> 00:09:37,040
generate issues that are not necessarily

269
00:09:37,040 --> 00:09:39,600
seen before while still being concise

270
00:09:39,600 --> 00:09:42,080
consistent and fine-grained

271
00:09:42,080 --> 00:09:43,600
some previous approaches relied on

272
00:09:43,600 --> 00:09:45,920
classification we categorized a review

273
00:09:45,920 --> 00:09:48,560
from a set of limited options

274
00:09:48,560 --> 00:09:50,560
other works relied on identifying the

275
00:09:50,560 --> 00:09:52,880
key phrases in the review and

276
00:09:52,880 --> 00:09:54,800
considering that to be the issue or the

277
00:09:54,800 --> 00:09:57,279
requirement these approaches are limited

278
00:09:57,279 --> 00:09:59,600
since classifiers can only get one of

279
00:09:59,600 --> 00:10:02,079
the predetermined labels and key phrases

280
00:10:02,079 --> 00:10:05,040
do not recur across reviews

281
00:10:05,040 --> 00:10:06,959
our approach is different we take an

282
00:10:06,959 --> 00:10:09,120
abstractive generation approach we

283
00:10:09,120 --> 00:10:10,880
developed a generative model that is

284
00:10:10,880 --> 00:10:12,640
trained to produce issues in the style

285
00:10:12,640 --> 00:10:15,440
of succinct labels that are either new

286
00:10:15,440 --> 00:10:17,200
or have been seen before

287
00:10:17,200 --> 00:10:19,040
the advantage is that the outputs are

288
00:10:19,040 --> 00:10:21,360
consistently worth the issues that occur

289
00:10:21,360 --> 00:10:24,720
across reviews moreover it accounts for

290
00:10:24,720 --> 00:10:26,800
newly emerging issues

291
00:10:26,800 --> 00:10:29,120
we formulate the task as text generation

292
00:10:29,120 --> 00:10:31,519
task in d5 the same model we use for the

293
00:10:31,519 --> 00:10:33,680
privacy classifier we include the

294
00:10:33,680 --> 00:10:36,240
reviews text in the input so it would be

295
00:10:36,240 --> 00:10:38,480
as you see it here we include the list

296
00:10:38,480 --> 00:10:40,320
of issues we want

297
00:10:40,320 --> 00:10:41,120
um

298
00:10:41,120 --> 00:10:42,800
in the target

299
00:10:42,800 --> 00:10:44,720
then we fine tune the t5 model to

300
00:10:44,720 --> 00:10:46,480
generate such targets

301
00:10:46,480 --> 00:10:47,920
you can find more details about

302
00:10:47,920 --> 00:10:49,760
evaluating this model

303
00:10:49,760 --> 00:10:52,160
in the paper

304
00:10:52,160 --> 00:10:54,800
we briefly now describe the

305
00:10:54,800 --> 00:10:57,120
theme generation part

306
00:10:57,120 --> 00:10:58,320
you might recall that we want to

307
00:10:58,320 --> 00:10:59,839
summarize the fine fine-grained issues

308
00:10:59,839 --> 00:11:01,839
into high-level themes

309
00:11:01,839 --> 00:11:04,000
we won't go into details of the process

310
00:11:04,000 --> 00:11:05,519
in this talk you can check them in the

311
00:11:05,519 --> 00:11:06,480
paper

312
00:11:06,480 --> 00:11:08,320
but here's an overview

313
00:11:08,320 --> 00:11:10,399
given a set of issues

314
00:11:10,399 --> 00:11:12,720
in our corpus we first perform a

315
00:11:12,720 --> 00:11:14,640
clustering of issues based on their

316
00:11:14,640 --> 00:11:16,880
embeddings this gives us a set of issue

317
00:11:16,880 --> 00:11:18,079
clusters

318
00:11:18,079 --> 00:11:20,320
to avoid the manual work of interpreting

319
00:11:20,320 --> 00:11:22,800
what these poops are about you also have

320
00:11:22,800 --> 00:11:25,279
a theme title generation model that

321
00:11:25,279 --> 00:11:27,839
takes the most frequent issues per group

322
00:11:27,839 --> 00:11:31,360
and produces theme targets

323
00:11:31,360 --> 00:11:34,079
now how do we get these theme titles and

324
00:11:34,079 --> 00:11:35,839
you might have guessed it by now but we

325
00:11:35,839 --> 00:11:37,920
also use an abstractive title generation

326
00:11:37,920 --> 00:11:39,360
approach

327
00:11:39,360 --> 00:11:41,760
by combining a custom data set with a d5

328
00:11:41,760 --> 00:11:42,959
model

329
00:11:42,959 --> 00:11:44,720
the custom data set looks like this we

330
00:11:44,720 --> 00:11:46,800
have a list of issues in the input and

331
00:11:46,800 --> 00:11:49,200
we have the theme title as the target

332
00:11:49,200 --> 00:11:51,040
then we train the model on such a data

333
00:11:51,040 --> 00:11:53,760
set and evaluate it against baselines

334
00:11:53,760 --> 00:11:55,920
via a comparative crowdsourcing

335
00:11:55,920 --> 00:11:58,479
experiment

336
00:11:58,720 --> 00:12:00,639
so after building this pipeline we

337
00:12:00,639 --> 00:12:02,880
applied it at scale

338
00:12:02,880 --> 00:12:04,560
we applied our methodology to the

339
00:12:04,560 --> 00:12:06,399
largest data set of play store reviews

340
00:12:06,399 --> 00:12:09,440
debate including 626 million reviews

341
00:12:09,440 --> 00:12:12,560
coming from 1.3 million

342
00:12:12,560 --> 00:12:15,040
with our privacy classifier we find that

343
00:12:15,040 --> 00:12:17,120
around 6 million reviews were classified

344
00:12:17,120 --> 00:12:18,399
as privacy

345
00:12:18,399 --> 00:12:20,240
across these reviews we identified more

346
00:12:20,240 --> 00:12:22,079
than 300 high-level themes with more

347
00:12:22,079 --> 00:12:25,760
than 1 000 reviews each

348
00:12:25,760 --> 00:12:28,160
we conducted several types of analysis

349
00:12:28,160 --> 00:12:30,959
on this corpus in this mosaic plot we

350
00:12:30,959 --> 00:12:32,480
showed the top 10 themes across the

351
00:12:32,480 --> 00:12:35,200
whole corpus split by emotions

352
00:12:35,200 --> 00:12:36,959
we can see that unneeded access is the

353
00:12:36,959 --> 00:12:39,360
top theme with over a half a million

354
00:12:39,360 --> 00:12:40,399
reviews

355
00:12:40,399 --> 00:12:42,320
this is followed by generic privacy

356
00:12:42,320 --> 00:12:44,000
concerns and personal information

357
00:12:44,000 --> 00:12:45,120
privacy

358
00:12:45,120 --> 00:12:46,880
you can also see interesting trends

359
00:12:46,880 --> 00:12:49,360
across emotions too users are mostly

360
00:12:49,360 --> 00:12:52,240
angry about the unneeded axes however

361
00:12:52,240 --> 00:12:54,639
the theme content hiding is dominated by

362
00:12:54,639 --> 00:12:55,839
joy

363
00:12:55,839 --> 00:12:57,519
the presence of positive emotions in

364
00:12:57,519 --> 00:12:59,600
these themes represents users that are

365
00:12:59,600 --> 00:13:01,360
expressing their content with the

366
00:13:01,360 --> 00:13:03,360
absence of privacy invading behavior

367
00:13:03,360 --> 00:13:04,959
sometimes

368
00:13:04,959 --> 00:13:06,480
this aspect is typically

369
00:13:06,480 --> 00:13:08,320
underrepresented in previous work and

370
00:13:08,320 --> 00:13:10,240
can act as a positive reinforcement for

371
00:13:10,240 --> 00:13:12,079
developers

372
00:13:12,079 --> 00:13:14,079
next we will drill down

373
00:13:14,079 --> 00:13:16,800
into the top fine grained issues within

374
00:13:16,800 --> 00:13:19,839
the spying concerns

375
00:13:19,839 --> 00:13:22,480
we can see a variety of aspects such as

376
00:13:22,480 --> 00:13:24,399
general spying mentions

377
00:13:24,399 --> 00:13:26,720
tracking spying on spouse spying camera

378
00:13:26,720 --> 00:13:28,720
detection etc it's

379
00:13:28,720 --> 00:13:30,560
surprising that some of these issues

380
00:13:30,560 --> 00:13:32,639
such as spying on spouse or partner get

381
00:13:32,639 --> 00:13:34,720
a lot of joyful emotions

382
00:13:34,720 --> 00:13:36,560
and it would be interesting to delve

383
00:13:36,560 --> 00:13:39,600
deeper into them in future research

384
00:13:39,600 --> 00:13:42,880
to further show how one can go um

385
00:13:42,880 --> 00:13:44,959
into details about each issue we will

386
00:13:44,959 --> 00:13:49,600
further zoom into the tracking app issue

387
00:13:49,600 --> 00:13:51,279
hark enables us to check the high

388
00:13:51,279 --> 00:13:52,800
quality codes from this issue

389
00:13:52,800 --> 00:13:55,279
diversified across emotions

390
00:13:55,279 --> 00:13:57,279
here we just show four emotions but you

391
00:13:57,279 --> 00:13:59,199
can see more examples in the paper with

392
00:13:59,199 --> 00:14:02,320
other emotions the idea is that it's

393
00:14:02,320 --> 00:14:04,000
possible to build various ways of

394
00:14:04,000 --> 00:14:06,720
prioritizing issues on top of heart via

395
00:14:06,720 --> 00:14:09,760
these filters that emotions classifiers

396
00:14:09,760 --> 00:14:12,079
provide

397
00:14:12,160 --> 00:14:14,480
in summary heart is an end-to-end system

398
00:14:14,480 --> 00:14:16,480
for extracting and analyzing privacy

399
00:14:16,480 --> 00:14:17,519
reviews

400
00:14:17,519 --> 00:14:19,199
we achieved three requirements that we

401
00:14:19,199 --> 00:14:20,320
aimed for

402
00:14:20,320 --> 00:14:23,360
topical diversity

403
00:14:23,360 --> 00:14:25,279
high coverage of privacy concepts

404
00:14:25,279 --> 00:14:26,800
glanceability

405
00:14:26,800 --> 00:14:28,639
developers can understand the gist of

406
00:14:28,639 --> 00:14:30,240
topics without having to read all the

407
00:14:30,240 --> 00:14:33,199
reviews and navigability developers get

408
00:14:33,199 --> 00:14:34,959
the high level picture and can delve

409
00:14:34,959 --> 00:14:37,600
deeper into issues on demand

410
00:14:37,600 --> 00:14:39,519
thanks again for your attention and for

411
00:14:39,519 --> 00:14:40,720
organizers

412
00:14:40,720 --> 00:14:42,560
for creating the opportunity to present

413
00:14:42,560 --> 00:14:43,760
remotely

414
00:14:43,760 --> 00:14:46,160
for those who are hindered from coming

415
00:14:46,160 --> 00:14:48,240
by the u.s visa system

416
00:14:48,240 --> 00:14:50,560
and finally a shout out that our team is

417
00:14:50,560 --> 00:14:51,440
hired

418
00:14:51,440 --> 00:14:53,839
feel free to reach out to my colleagues

419
00:14:53,839 --> 00:14:56,000
psy and animesh who are available in

420
00:14:56,000 --> 00:14:57,040
person

421
00:14:57,040 --> 00:14:58,959
to answer your questions about the paper

422
00:14:58,959 --> 00:15:00,320
tube

423
00:15:00,320 --> 00:15:02,639
thanks

424
00:15:03,610 --> 00:15:07,360
[Applause]

425
00:15:07,360 --> 00:15:10,079
all right so first i want to apologize i

426
00:15:10,079 --> 00:15:12,160
unfortunately swapped the content of the

427
00:15:12,160 --> 00:15:14,480
last two papers so this was obviously

428
00:15:14,480 --> 00:15:15,839
not about differential privacy and

429
00:15:15,839 --> 00:15:17,600
homomorphic encryption it was about

430
00:15:17,600 --> 00:15:19,120
using nlp

431
00:15:19,120 --> 00:15:21,120
to uh categorize

432
00:15:21,120 --> 00:15:22,320
uh different

433
00:15:22,320 --> 00:15:24,639
sentiments and and language related to

434
00:15:24,639 --> 00:15:26,800
privacy so you take well you can ask

435
00:15:26,800 --> 00:15:27,839
questions

436
00:15:27,839 --> 00:15:30,480
any questions

437
00:15:39,600 --> 00:15:40,480
hello

438
00:15:40,480 --> 00:15:43,120
uh thanks a little talk um

439
00:15:43,120 --> 00:15:46,079
one thing i had in mind uh was how did

440
00:15:46,079 --> 00:15:47,680
you so it seems like there's three

441
00:15:47,680 --> 00:15:49,759
levels of hierarchies the reviews the

442
00:15:49,759 --> 00:15:51,839
the issues i think you call it and then

443
00:15:51,839 --> 00:15:53,680
the high level theme

444
00:15:53,680 --> 00:15:56,800
why three and not not four or or two

445
00:15:56,800 --> 00:15:58,320
what was the

446
00:15:58,320 --> 00:16:01,279
well so we essentially wanted to balance

447
00:16:01,279 --> 00:16:04,160
out uh the amount of time uh or the

448
00:16:04,160 --> 00:16:05,199
information that we shared with the

449
00:16:05,199 --> 00:16:07,440
developers and also

450
00:16:07,440 --> 00:16:08,720
give them the details that they would

451
00:16:08,720 --> 00:16:11,440
like to do we felt like this was the uh

452
00:16:11,440 --> 00:16:13,279
best option that was possible like we

453
00:16:13,279 --> 00:16:15,040
initially we thought we can just have an

454
00:16:15,040 --> 00:16:17,759
issues to uh reviews as as just a two

455
00:16:17,759 --> 00:16:19,680
level thing but then the number of

456
00:16:19,680 --> 00:16:20,800
issues that we were generating were too

457
00:16:20,800 --> 00:16:23,199
many to handle and even if you remember

458
00:16:23,199 --> 00:16:25,360
one of the slide we had like about

459
00:16:25,360 --> 00:16:26,399
um

460
00:16:26,399 --> 00:16:28,639
500 or so themes in total that we had

461
00:16:28,639 --> 00:16:30,959
identified across the full data set so

462
00:16:30,959 --> 00:16:33,040
if you pick a particular app an app

463
00:16:33,040 --> 00:16:34,959
might have like 30 or 40 themes which

464
00:16:34,959 --> 00:16:37,519
was easy to scan into

465
00:16:37,519 --> 00:16:39,360
and also if they wish they could dive

466
00:16:39,360 --> 00:16:40,800
deeper into it so it was like a fine

467
00:16:40,800 --> 00:16:42,399
balance between how many we show the

468
00:16:42,399 --> 00:16:44,240
developer and also at the same time give

469
00:16:44,240 --> 00:16:47,120
them the option to dive deeper if needed

470
00:16:47,120 --> 00:16:49,759
so it seems like the high the number of

471
00:16:49,759 --> 00:16:52,240
highest level themes per app was kind of

472
00:16:52,240 --> 00:16:53,120
the

473
00:16:53,120 --> 00:16:55,600
determiner of how many exactly got it

474
00:16:55,600 --> 00:16:57,759
all right thanks

475
00:16:57,759 --> 00:16:59,519
so i have a question so are there

476
00:16:59,519 --> 00:17:02,560
situations where uh different users uh

477
00:17:02,560 --> 00:17:04,480
can for example express the same thing

478
00:17:04,480 --> 00:17:06,640
using different language and then this

479
00:17:06,640 --> 00:17:08,079
ends up being mapped to different

480
00:17:08,079 --> 00:17:10,240
categories and so at the end of the day

481
00:17:10,240 --> 00:17:12,240
you don't actually get

482
00:17:12,240 --> 00:17:14,720
the right counting uh so if you want to

483
00:17:14,720 --> 00:17:16,720
prioritize things it doesn't quite work

484
00:17:16,720 --> 00:17:18,959
out yeah that can definitely happen but

485
00:17:18,959 --> 00:17:21,679
uh that problem is partly eradicated

486
00:17:21,679 --> 00:17:24,240
where we group the issues into themes

487
00:17:24,240 --> 00:17:26,079
uh probably might not realize this in

488
00:17:26,079 --> 00:17:28,079
one of the spying themes that we had

489
00:17:28,079 --> 00:17:29,679
seen like the fangun issues were

490
00:17:29,679 --> 00:17:30,880
slightly different like one is like

491
00:17:30,880 --> 00:17:32,480
spying spouse one other is general

492
00:17:32,480 --> 00:17:34,480
spying where there could be an overlap

493
00:17:34,480 --> 00:17:35,840
of the topic but when we do the

494
00:17:35,840 --> 00:17:37,360
aggregation at the issue level this

495
00:17:37,360 --> 00:17:39,120
problem would diminish to a large extent

496
00:17:39,120 --> 00:17:41,120
so we get to count them accurately as

497
00:17:41,120 --> 00:17:43,600
much as possible i see and then uh one

498
00:17:43,600 --> 00:17:45,679
other question is you seem to be using a

499
00:17:45,679 --> 00:17:47,360
lot of techniques that are common in the

500
00:17:47,360 --> 00:17:50,480
nlp uh literature such as summarization

501
00:17:50,480 --> 00:17:51,679
uh

502
00:17:51,679 --> 00:17:54,000
and finding emotions uh did you have to

503
00:17:54,000 --> 00:17:55,520
do adjustments to any of those

504
00:17:55,520 --> 00:17:57,039
techniques or you just can just use the

505
00:17:57,039 --> 00:17:58,720
black box mechanisms themselves and

506
00:17:58,720 --> 00:18:00,720
combine them so we had to fine-tune the

507
00:18:00,720 --> 00:18:02,799
models to our use case because if you

508
00:18:02,799 --> 00:18:04,880
were to use them as is out of the box

509
00:18:04,880 --> 00:18:06,960
they don't apply well to the

510
00:18:06,960 --> 00:18:09,200
data set that we are working with so we

511
00:18:09,200 --> 00:18:10,880
saw a lot of noise creeping into the

512
00:18:10,880 --> 00:18:12,720
system so we had to fine-tune these

513
00:18:12,720 --> 00:18:14,720
models to our use case so that's just to

514
00:18:14,720 --> 00:18:16,160
get better results of course if you were

515
00:18:16,160 --> 00:18:17,679
to use them out of the box instead of

516
00:18:17,679 --> 00:18:19,840
getting something like 90 auc we might

517
00:18:19,840 --> 00:18:22,320
get like 70 which is still good but fine

518
00:18:22,320 --> 00:18:23,760
tuning might give us the best results

519
00:18:23,760 --> 00:18:26,160
possible okay thank you so much uh let's

520
00:18:26,160 --> 00:18:28,220
thank the authors once again

521
00:18:28,220 --> 00:18:32,819
[Applause]

