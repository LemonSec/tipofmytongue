1
00:00:00,560 --> 00:00:02,560
okay thank you uh yeah so very excited

2
00:00:02,560 --> 00:00:04,720
to be here to talk about our paper on

3
00:00:04,720 --> 00:00:06,240
membership infant attacks from first

4
00:00:06,240 --> 00:00:07,520
principles

5
00:00:07,520 --> 00:00:08,800
this is joint work with my colleagues

6
00:00:08,800 --> 00:00:11,280
now of all of whom are at google

7
00:00:11,280 --> 00:00:14,000
so a membership info attack asks a very

8
00:00:14,000 --> 00:00:16,640
simple question which is was some

9
00:00:16,640 --> 00:00:18,400
particular machine learning model

10
00:00:18,400 --> 00:00:20,400
trained on a given example

11
00:00:20,400 --> 00:00:22,240
recall that the way that machine

12
00:00:22,240 --> 00:00:23,840
learning models are trained is you have

13
00:00:23,840 --> 00:00:25,680
some training data set

14
00:00:25,680 --> 00:00:27,519
and then from here you use some

15
00:00:27,519 --> 00:00:29,119
algorithm like stochastic gradient

16
00:00:29,119 --> 00:00:31,439
descent to get some machine learning

17
00:00:31,439 --> 00:00:33,120
model out of this

18
00:00:33,120 --> 00:00:34,880
and all that we ask for membership

19
00:00:34,880 --> 00:00:36,160
inference

20
00:00:36,160 --> 00:00:38,320
is you know we have this image maybe of

21
00:00:38,320 --> 00:00:39,360
a cat

22
00:00:39,360 --> 00:00:41,680
was this image any of these images

23
00:00:41,680 --> 00:00:44,079
contained in the training data set it's

24
00:00:44,079 --> 00:00:46,000
a very simple question

25
00:00:46,000 --> 00:00:48,079
and there's a lot of reasons why you

26
00:00:48,079 --> 00:00:49,120
might want to be able to answer this

27
00:00:49,120 --> 00:00:50,160
question

28
00:00:50,160 --> 00:00:51,360
they range from everything from just

29
00:00:51,360 --> 00:00:53,840
curiosity maybe i want to know what some

30
00:00:53,840 --> 00:00:55,520
company trained their model on did they

31
00:00:55,520 --> 00:00:56,879
use my data

32
00:00:56,879 --> 00:00:59,359
to questions of reconnaissance maybe i

33
00:00:59,359 --> 00:01:01,039
want to if i know what data they trained

34
00:01:01,039 --> 00:01:02,480
on maybe i can attack them better with

35
00:01:02,480 --> 00:01:04,080
some future attack

36
00:01:04,080 --> 00:01:05,438
to

37
00:01:05,438 --> 00:01:07,200
motivations like data extraction it

38
00:01:07,200 --> 00:01:08,640
turns out that membership infrared

39
00:01:08,640 --> 00:01:10,560
attacks are the building block that we

40
00:01:10,560 --> 00:01:12,720
use to actually extract data from

41
00:01:12,720 --> 00:01:14,560
trained models and so better membership

42
00:01:14,560 --> 00:01:16,000
info attacks allow us to do the data

43
00:01:16,000 --> 00:01:18,320
extraction better or finally even i

44
00:01:18,320 --> 00:01:20,000
might want to audit the correctness of

45
00:01:20,000 --> 00:01:22,000
some privacy algorithm and i might use

46
00:01:22,000 --> 00:01:23,520
the attack in order to ensure that the

47
00:01:23,520 --> 00:01:25,759
algorithm is actually as private as it's

48
00:01:25,759 --> 00:01:28,320
claimed to be

49
00:01:28,320 --> 00:01:29,200
so

50
00:01:29,200 --> 00:01:31,920
in this talk i'm going to present to you

51
00:01:31,920 --> 00:01:33,759
a first principles approach to designing

52
00:01:33,759 --> 00:01:35,840
membership attacks where we try and

53
00:01:35,840 --> 00:01:37,520
reconsider this problem and just say

54
00:01:37,520 --> 00:01:38,799
what do we think is the right way of

55
00:01:38,799 --> 00:01:40,159
performing these kinds of attacks and

56
00:01:40,159 --> 00:01:41,439
hopefully develop something that's both

57
00:01:41,439 --> 00:01:44,640
simpler and also more powerful

58
00:01:44,640 --> 00:01:46,479
okay so

59
00:01:46,479 --> 00:01:48,240
the basic membership inference setup

60
00:01:48,240 --> 00:01:49,680
goes like this you know you want to

61
00:01:49,680 --> 00:01:52,479
predict the probability that you believe

62
00:01:52,479 --> 00:01:54,000
that some example was in the training

63
00:01:54,000 --> 00:01:55,360
data set for some machine machine

64
00:01:55,360 --> 00:01:56,799
learning model and we'll call it

65
00:01:56,799 --> 00:01:58,719
quantity a and then quantity b is the

66
00:01:58,719 --> 00:02:00,079
inverse the probability you think it's

67
00:02:00,079 --> 00:02:01,920
not in the data set

68
00:02:01,920 --> 00:02:03,600
and then you know trivially if quantity

69
00:02:03,600 --> 00:02:05,040
a is greater than quantity b you think

70
00:02:05,040 --> 00:02:06,479
that you say yes it's a member unless

71
00:02:06,479 --> 00:02:09,038
you say non-member the interesting

72
00:02:09,038 --> 00:02:10,800
question then is how do you compute

73
00:02:10,800 --> 00:02:12,640
these quantities here and for this let

74
00:02:12,640 --> 00:02:15,040
me just zoom in on one of these because

75
00:02:15,040 --> 00:02:17,040
the other one is symmetric

76
00:02:17,040 --> 00:02:18,480
and ask how we calculate this

77
00:02:18,480 --> 00:02:20,480
probability that something is actually

78
00:02:20,480 --> 00:02:23,040
in the training data set

79
00:02:23,040 --> 00:02:24,080
what we're going to do here is

80
00:02:24,080 --> 00:02:25,120
essentially we're just going to treat

81
00:02:25,120 --> 00:02:26,480
this you know

82
00:02:26,480 --> 00:02:28,080
like any modeling problem and we'll try

83
00:02:28,080 --> 00:02:29,760
and first frame it like this way we'll

84
00:02:29,760 --> 00:02:31,760
say like what is the probability of

85
00:02:31,760 --> 00:02:33,280
observing these particular neural

86
00:02:33,280 --> 00:02:34,560
network weights

87
00:02:34,560 --> 00:02:36,560
under this distribution

88
00:02:36,560 --> 00:02:38,160
of all possible models that we could

89
00:02:38,160 --> 00:02:39,760
have seen

90
00:02:39,760 --> 00:02:42,480
given that it was trained on this image

91
00:02:42,480 --> 00:02:44,160
this clearly is the right thing to do

92
00:02:44,160 --> 00:02:46,080
sort of by definition this is what this

93
00:02:46,080 --> 00:02:48,080
equation actually means the problem is

94
00:02:48,080 --> 00:02:50,160
that you can't actually compute this

95
00:02:50,160 --> 00:02:52,080
like neural networks are millions or

96
00:02:52,080 --> 00:02:53,920
billions of dimensional objects you

97
00:02:53,920 --> 00:02:55,280
can't just ask what is the probability

98
00:02:55,280 --> 00:02:57,360
of this billion dimensional object

99
00:02:57,360 --> 00:02:59,519
through some simple statistics

100
00:02:59,519 --> 00:03:00,879
so there's this challenge here right

101
00:03:00,879 --> 00:03:02,000
like you can't enumerate this

102
00:03:02,000 --> 00:03:03,440
distribution space

103
00:03:03,440 --> 00:03:05,519
and so directly asking this question is

104
00:03:05,519 --> 00:03:07,040
not going to work

105
00:03:07,040 --> 00:03:08,560
so we'll have to do something slightly

106
00:03:08,560 --> 00:03:09,680
different

107
00:03:09,680 --> 00:03:11,280
so what are we going to do

108
00:03:11,280 --> 00:03:12,959
well instead i'll give you sort of one

109
00:03:12,959 --> 00:03:14,640
piece of background which is something

110
00:03:14,640 --> 00:03:17,200
that's called a loss function a loss

111
00:03:17,200 --> 00:03:19,040
function allows me to answer the

112
00:03:19,040 --> 00:03:21,920
question how wrong was some model on an

113
00:03:21,920 --> 00:03:22,800
input

114
00:03:22,800 --> 00:03:24,239
it's going to return from me a single

115
00:03:24,239 --> 00:03:26,560
scalar floating point value which is a

116
00:03:26,560 --> 00:03:28,480
measure of how badly the model performs

117
00:03:28,480 --> 00:03:31,120
on this input and so what we can do is

118
00:03:31,120 --> 00:03:33,840
we can take this correct formulation

119
00:03:33,840 --> 00:03:35,120
which is what we would like to be able

120
00:03:35,120 --> 00:03:37,440
to compute but we can't and instead

121
00:03:37,440 --> 00:03:39,440
measure it somewhat slightly differently

122
00:03:39,440 --> 00:03:41,440
and instead measure

123
00:03:41,440 --> 00:03:44,000
in the probability of observing this

124
00:03:44,000 --> 00:03:46,959
loss value for the model on this image

125
00:03:46,959 --> 00:03:49,680
under the distribution over all losses

126
00:03:49,680 --> 00:03:51,040
that we might see

127
00:03:51,040 --> 00:03:53,599
when training models on this image

128
00:03:53,599 --> 00:03:54,799
and the reason this is easier to

129
00:03:54,799 --> 00:03:56,799
calculate is because now we're only

130
00:03:56,799 --> 00:03:58,480
asking for the probability of observing

131
00:03:58,480 --> 00:04:00,799
a single floating point value

132
00:04:00,799 --> 00:04:02,080
this is something that's easy to

133
00:04:02,080 --> 00:04:04,560
calculate and so it's much sort of more

134
00:04:04,560 --> 00:04:06,720
feasible and like i say it's easy to

135
00:04:06,720 --> 00:04:08,560
calculate but like you know i still need

136
00:04:08,560 --> 00:04:10,480
to tell you how we do it and that's what

137
00:04:10,480 --> 00:04:11,840
i'm going to walk through next is how we

138
00:04:11,840 --> 00:04:15,200
actually calculate these probabilities

139
00:04:15,200 --> 00:04:16,798
okay so what i'm going to do is i'm

140
00:04:16,798 --> 00:04:17,759
going to take inspiration from this

141
00:04:17,759 --> 00:04:19,279
recent paper that performs the following

142
00:04:19,279 --> 00:04:20,478
experiment

143
00:04:20,478 --> 00:04:22,800
let's take the training data set

144
00:04:22,800 --> 00:04:25,120
and we're going to randomly mask out

145
00:04:25,120 --> 00:04:27,440
like half of the training examples

146
00:04:27,440 --> 00:04:28,800
and i'm going to use this to train a

147
00:04:28,800 --> 00:04:30,560
machine learning model

148
00:04:30,560 --> 00:04:32,800
i'll take this model and i'll save it

149
00:04:32,800 --> 00:04:34,800
over the side and remember what data it

150
00:04:34,800 --> 00:04:36,479
was trained on

151
00:04:36,479 --> 00:04:38,000
then i'll take the same training data

152
00:04:38,000 --> 00:04:39,360
set and i'll randomly mask out a

153
00:04:39,360 --> 00:04:41,759
different 50 of the data

154
00:04:41,759 --> 00:04:43,680
and then i'll save this model over the

155
00:04:43,680 --> 00:04:45,680
side and do this again you know take the

156
00:04:45,680 --> 00:04:47,680
training data set mask out half of the

157
00:04:47,680 --> 00:04:49,759
training examples train a model and now

158
00:04:49,759 --> 00:04:50,960
i'm going to have a huge number of

159
00:04:50,960 --> 00:04:52,560
models

160
00:04:52,560 --> 00:04:54,240
now i do that i take all of these models

161
00:04:54,240 --> 00:04:55,520
that i've trained

162
00:04:55,520 --> 00:04:58,400
and i partitioned them into two sets

163
00:04:58,400 --> 00:05:00,479
on the left i have the models that saw a

164
00:05:00,479 --> 00:05:02,320
particular image during training and on

165
00:05:02,320 --> 00:05:03,600
the right i have the models that didn't

166
00:05:03,600 --> 00:05:06,240
see this image during training

167
00:05:06,240 --> 00:05:07,120
okay

168
00:05:07,120 --> 00:05:08,960
now what i can do from here

169
00:05:08,960 --> 00:05:11,199
is i can zoom in on the one on the left

170
00:05:11,199 --> 00:05:14,080
and say okay for each of these models

171
00:05:14,080 --> 00:05:16,720
what was their loss on the image that

172
00:05:16,720 --> 00:05:17,919
i'm interested in

173
00:05:17,919 --> 00:05:20,000
i can replace this with the scalar value

174
00:05:20,000 --> 00:05:22,080
loss and this gives me a distribution

175
00:05:22,080 --> 00:05:25,360
over losses i can then go and i can ask

176
00:05:25,360 --> 00:05:27,280
exactly the same question

177
00:05:27,280 --> 00:05:29,360
for the other set

178
00:05:29,360 --> 00:05:31,520
of models that didn't see some image i

179
00:05:31,520 --> 00:05:33,199
take these models i get the single scale

180
00:05:33,199 --> 00:05:34,560
or value loss

181
00:05:34,560 --> 00:05:36,720
and i get a new distribution

182
00:05:36,720 --> 00:05:38,000
and now what i can do is i can compare

183
00:05:38,000 --> 00:05:39,199
these distributions i can plot them on

184
00:05:39,199 --> 00:05:41,039
the same scale and you can see that

185
00:05:41,039 --> 00:05:42,800
they're very very separate like the

186
00:05:42,800 --> 00:05:44,400
distribution of losses that you happen

187
00:05:44,400 --> 00:05:45,759
to see when an image is in the training

188
00:05:45,759 --> 00:05:48,320
data set is very different from this

189
00:05:48,320 --> 00:05:50,160
distribution of losses when some image

190
00:05:50,160 --> 00:05:52,080
is not in the training data set which

191
00:05:52,080 --> 00:05:53,919
intuitively allows us to say that it

192
00:05:53,919 --> 00:05:55,919
should be easy to attack this image

193
00:05:55,919 --> 00:05:58,080
because we can check is it more similar

194
00:05:58,080 --> 00:05:59,600
to the blue distribution or the orange

195
00:05:59,600 --> 00:06:01,520
distribution and then directly fairly

196
00:06:01,520 --> 00:06:03,120
easily read off whether or not we think

197
00:06:03,120 --> 00:06:04,639
this is a member

198
00:06:04,639 --> 00:06:06,800
and this works sort of by eye test but

199
00:06:06,800 --> 00:06:07,919
we'd like to be able to have something

200
00:06:07,919 --> 00:06:10,080
we can actually measure in practice so

201
00:06:10,080 --> 00:06:11,520
what are we going to do here like we're

202
00:06:11,520 --> 00:06:12,720
going to need some way of actually

203
00:06:12,720 --> 00:06:15,199
measuring these distributions

204
00:06:15,199 --> 00:06:16,080
okay

205
00:06:16,080 --> 00:06:17,280
so in general whenever you have to

206
00:06:17,280 --> 00:06:18,720
measure distributions there's two things

207
00:06:18,720 --> 00:06:20,319
you can do you can come up with a

208
00:06:20,319 --> 00:06:22,880
non-parametric approach which just

209
00:06:22,880 --> 00:06:24,400
gathers lots of data and then just

210
00:06:24,400 --> 00:06:26,319
directly look at the data the problem

211
00:06:26,319 --> 00:06:28,000
with non-parametric approaches is they

212
00:06:28,000 --> 00:06:30,400
require training lots and lots of models

213
00:06:30,400 --> 00:06:32,560
now to make these figures in this talk

214
00:06:32,560 --> 00:06:34,080
i've done that i've trained 10 000

215
00:06:34,080 --> 00:06:35,840
models but in general this is not

216
00:06:35,840 --> 00:06:37,199
something you can reasonably ask people

217
00:06:37,199 --> 00:06:39,600
to do especially for big models

218
00:06:39,600 --> 00:06:40,400
so

219
00:06:40,400 --> 00:06:41,600
we're not going to take this approach

220
00:06:41,600 --> 00:06:42,880
because it's going to require training

221
00:06:42,880 --> 00:06:44,720
just far too many models

222
00:06:44,720 --> 00:06:45,840
instead what we're going to do is we're

223
00:06:45,840 --> 00:06:47,759
going to take a parametric approach

224
00:06:47,759 --> 00:06:49,360
which is going to allow us to train many

225
00:06:49,360 --> 00:06:51,360
many fewer models but still estimate the

226
00:06:51,360 --> 00:06:53,520
distributions fairly closely

227
00:06:53,520 --> 00:06:55,120
in particular we'll need something like

228
00:06:55,120 --> 00:06:58,000
16 32 models trained

229
00:06:58,000 --> 00:06:59,360
okay

230
00:06:59,360 --> 00:07:01,759
so what this can actually let us do

231
00:07:01,759 --> 00:07:03,360
is estimate the distributions but i need

232
00:07:03,360 --> 00:07:04,800
some way to actually estimate these

233
00:07:04,800 --> 00:07:06,240
distributions correctly you can't just

234
00:07:06,240 --> 00:07:08,560
throw a gaussian on top of anything

235
00:07:08,560 --> 00:07:09,599
and so you need to have some kind of

236
00:07:09,599 --> 00:07:10,639
care that you're actually going to model

237
00:07:10,639 --> 00:07:11,919
this with some correct parametric

238
00:07:11,919 --> 00:07:14,479
distribution this requires a lot of care

239
00:07:14,479 --> 00:07:16,160
for this talk i've already done all that

240
00:07:16,160 --> 00:07:17,840
for you and like these things actually

241
00:07:17,840 --> 00:07:20,160
fit gaussians very nicely

242
00:07:20,160 --> 00:07:21,759
maybe a big part of our paper is

243
00:07:21,759 --> 00:07:23,120
actually how to make these things fit

244
00:07:23,120 --> 00:07:24,400
gaussians

245
00:07:24,400 --> 00:07:26,160
and for details i encourage you to take

246
00:07:26,160 --> 00:07:28,160
a look in paper for that but if you do

247
00:07:28,160 --> 00:07:29,520
it all correctly then you end up with

248
00:07:29,520 --> 00:07:31,360
two very nice gaussians and what you can

249
00:07:31,360 --> 00:07:33,520
do is you can look like where these two

250
00:07:33,520 --> 00:07:35,039
lines cross for the galaxies and this

251
00:07:35,039 --> 00:07:36,639
gives you a threshold

252
00:07:36,639 --> 00:07:39,039
where if the loss is to the left on this

253
00:07:39,039 --> 00:07:40,240
blue region then we say that it's a

254
00:07:40,240 --> 00:07:41,759
member and if it's the right of this

255
00:07:41,759 --> 00:07:44,639
threshold we say it's a non-member

256
00:07:44,639 --> 00:07:47,440
okay so this is what happens for you

257
00:07:47,440 --> 00:07:49,680
know one particular image of this cat

258
00:07:49,680 --> 00:07:51,039
but like what would it look like for a

259
00:07:51,039 --> 00:07:52,800
different image well all you do is you

260
00:07:52,800 --> 00:07:53,919
repeat this process where you

261
00:07:53,919 --> 00:07:55,680
re-partition the models under the new

262
00:07:55,680 --> 00:07:57,919
distributions of the new image you care

263
00:07:57,919 --> 00:08:00,240
about and recompute these statistics and

264
00:08:00,240 --> 00:08:01,840
you'll get a new loss distribution you

265
00:08:01,840 --> 00:08:03,680
can recompute this again so for example

266
00:08:03,680 --> 00:08:05,360
for this cat image you know the decision

267
00:08:05,360 --> 00:08:06,639
threshold is somewhere around a lost

268
00:08:06,639 --> 00:08:08,240
value of seven

269
00:08:08,240 --> 00:08:09,199
um

270
00:08:09,199 --> 00:08:10,800
but here you know we have these airplane

271
00:08:10,800 --> 00:08:12,160
images where i've just recomputed the

272
00:08:12,160 --> 00:08:14,720
statistics and i can you know we look at

273
00:08:14,720 --> 00:08:16,960
these gaussians and see the decision

274
00:08:16,960 --> 00:08:18,639
threshold here this time is somewhere

275
00:08:18,639 --> 00:08:20,479
around one so like this is very

276
00:08:20,479 --> 00:08:21,759
important observation from the paper

277
00:08:21,759 --> 00:08:23,680
which is different images have very

278
00:08:23,680 --> 00:08:26,720
different histograms and so it's easy to

279
00:08:26,720 --> 00:08:29,280
distinguish between both this airplane

280
00:08:29,280 --> 00:08:30,879
and for the previous cat but they had

281
00:08:30,879 --> 00:08:32,399
different decision thresholds and so any

282
00:08:32,399 --> 00:08:33,919
attack needs to

283
00:08:33,919 --> 00:08:35,279
be allowed to have these different

284
00:08:35,279 --> 00:08:36,640
decision thresholds and this is a big

285
00:08:36,640 --> 00:08:37,919
thing that many prior attacks didn't

286
00:08:37,919 --> 00:08:39,440
allow

287
00:08:39,440 --> 00:08:41,039
okay so again you know you can model

288
00:08:41,039 --> 00:08:42,719
this as a gaussian too and one final

289
00:08:42,719 --> 00:08:44,800
thing that we can do that's nice with

290
00:08:44,800 --> 00:08:46,880
our technique is let me just zoom into

291
00:08:46,880 --> 00:08:48,399
like this region of interest down in the

292
00:08:48,399 --> 00:08:49,440
middle

293
00:08:49,440 --> 00:08:51,440
we can not only get a threshold which is

294
00:08:51,440 --> 00:08:53,120
where these two values cross

295
00:08:53,120 --> 00:08:55,680
but if i take any arbitrary point i can

296
00:08:55,680 --> 00:08:57,839
look at like the ratio of the blue line

297
00:08:57,839 --> 00:08:59,920
to the ratio of the orange

298
00:08:59,920 --> 00:09:01,920
and we'll tell you the probability that

299
00:09:01,920 --> 00:09:03,839
i think i'm right in my answer and this

300
00:09:03,839 --> 00:09:05,360
is really really powerful because not

301
00:09:05,360 --> 00:09:07,120
only can i be correct most of the time

302
00:09:07,120 --> 00:09:08,399
but when i'm wrong

303
00:09:08,399 --> 00:09:10,399
i'll have low confidence in my answers

304
00:09:10,399 --> 00:09:11,760
so i'll know that i'm wrong and it's

305
00:09:11,760 --> 00:09:13,120
going to be make the results actually

306
00:09:13,120 --> 00:09:14,880
much stronger

307
00:09:14,880 --> 00:09:17,200
okay um so yeah like with that let's

308
00:09:17,200 --> 00:09:18,240
let's actually start talking about some

309
00:09:18,240 --> 00:09:20,959
results which is how well we do

310
00:09:20,959 --> 00:09:22,640
and you know as you might expect given

311
00:09:22,640 --> 00:09:24,080
that we have the paper here our results

312
00:09:24,080 --> 00:09:25,600
are sort of as good as or better than

313
00:09:25,600 --> 00:09:26,880
most prior work

314
00:09:26,880 --> 00:09:29,279
um this result um

315
00:09:29,279 --> 00:09:30,800
is sort of just like the average

316
00:09:30,800 --> 00:09:32,080
accuracy and you see like you know our

317
00:09:32,080 --> 00:09:33,760
numbers are bigger or something

318
00:09:33,760 --> 00:09:35,600
but the problem is that this is an

319
00:09:35,600 --> 00:09:37,279
average case number like this is what

320
00:09:37,279 --> 00:09:38,800
people have been using in the past this

321
00:09:38,800 --> 00:09:41,120
is an attack success rate on average

322
00:09:41,120 --> 00:09:43,519
over an entire training data set

323
00:09:43,519 --> 00:09:44,959
i don't think that this is a very good

324
00:09:44,959 --> 00:09:46,160
number

325
00:09:46,160 --> 00:09:47,839
average case numbers don't work very

326
00:09:47,839 --> 00:09:50,000
well with privacy for example if i could

327
00:09:50,000 --> 00:09:51,920
say i know for all of you in the

328
00:09:51,920 --> 00:09:52,959
audience

329
00:09:52,959 --> 00:09:54,640
one digit of each of your credit card

330
00:09:54,640 --> 00:09:55,680
numbers

331
00:09:55,680 --> 00:09:57,360
this would not be very informative i

332
00:09:57,360 --> 00:09:58,720
know six percent of your credit card

333
00:09:58,720 --> 00:10:00,640
number's on average but one digit of

334
00:10:00,640 --> 00:10:02,079
each of your numbers is much less

335
00:10:02,079 --> 00:10:04,320
important than i know the credit card

336
00:10:04,320 --> 00:10:05,760
number exactly of all of you in the

337
00:10:05,760 --> 00:10:07,519
left-most column like that would be a

338
00:10:07,519 --> 00:10:09,200
much more powerful attack but the

339
00:10:09,200 --> 00:10:10,640
average they're still the same like they

340
00:10:10,640 --> 00:10:12,320
report the same quantity

341
00:10:12,320 --> 00:10:13,519
so what we want to do is we want to have

342
00:10:13,519 --> 00:10:15,680
a metric which allows us to distinguish

343
00:10:15,680 --> 00:10:17,360
between these two facts you can have

344
00:10:17,360 --> 00:10:18,880
attacks that are good on average but not

345
00:10:18,880 --> 00:10:20,880
good at identifying any one particular

346
00:10:20,880 --> 00:10:23,040
person as being vulnerable

347
00:10:23,040 --> 00:10:24,880
so for doing this let me sort of zoom in

348
00:10:24,880 --> 00:10:27,040
on this one attack on the top which has

349
00:10:27,040 --> 00:10:29,760
the second highest average accuracy and

350
00:10:29,760 --> 00:10:31,760
works quite well on average and what i'm

351
00:10:31,760 --> 00:10:32,800
going to do is i'm going to show you for

352
00:10:32,800 --> 00:10:35,600
this attack what the roc curve looks

353
00:10:35,600 --> 00:10:38,320
like what the rc curve shows it's a true

354
00:10:38,320 --> 00:10:40,480
positive first false positive evaluation

355
00:10:40,480 --> 00:10:41,839
of this attack

356
00:10:41,839 --> 00:10:44,240
so you can think for any given point on

357
00:10:44,240 --> 00:10:46,959
this line this corresponds to a given

358
00:10:46,959 --> 00:10:48,399
true positive rate and a given false

359
00:10:48,399 --> 00:10:49,839
positive rate so this attack at this

360
00:10:49,839 --> 00:10:51,760
point you know has some high true

361
00:10:51,760 --> 00:10:53,120
positive rate but the false positive is

362
00:10:53,120 --> 00:10:54,079
quite high

363
00:10:54,079 --> 00:10:55,839
note that the way that rfc curves work

364
00:10:55,839 --> 00:10:57,519
it's trivial to be anywhere on the upper

365
00:10:57,519 --> 00:10:59,120
right just make your attack always

366
00:10:59,120 --> 00:11:01,279
return yes you know 100 true positive

367
00:11:01,279 --> 00:11:02,480
rate of course you make all the errors

368
00:11:02,480 --> 00:11:04,000
on false positives and similarly it's

369
00:11:04,000 --> 00:11:05,680
trivial to be on the bottom left just

370
00:11:05,680 --> 00:11:06,959
never say anything as a member and

371
00:11:06,959 --> 00:11:08,079
you'll have zero percent false positive

372
00:11:08,079 --> 00:11:09,519
rate of course you have zero for the

373
00:11:09,519 --> 00:11:11,200
true positive rate so what you want is

374
00:11:11,200 --> 00:11:13,279
something to the upper left region

375
00:11:13,279 --> 00:11:15,120
and the thing that we notice here is

376
00:11:15,120 --> 00:11:16,800
that for this attack

377
00:11:16,800 --> 00:11:18,560
actually at the low false positive rate

378
00:11:18,560 --> 00:11:20,079
regime the area where you're actually

379
00:11:20,079 --> 00:11:21,760
able to say something specific about a

380
00:11:21,760 --> 00:11:24,000
few individuals this attack performs

381
00:11:24,000 --> 00:11:26,320
about as well as random chance

382
00:11:26,320 --> 00:11:28,720
it doesn't actually do a powerful attack

383
00:11:28,720 --> 00:11:30,079
at this low false positive rate and this

384
00:11:30,079 --> 00:11:32,160
is the regime that lets us say something

385
00:11:32,160 --> 00:11:34,560
particular about individual users

386
00:11:34,560 --> 00:11:36,640
okay despite the fact that remember on

387
00:11:36,640 --> 00:11:38,160
average this attack worked very very

388
00:11:38,160 --> 00:11:39,519
well

389
00:11:39,519 --> 00:11:41,360
to make this very explicit what i'm

390
00:11:41,360 --> 00:11:42,480
going to do is i'm going to take this

391
00:11:42,480 --> 00:11:44,640
plot and i'm going to instead of showing

392
00:11:44,640 --> 00:11:46,880
you this in this this normal scale i'm

393
00:11:46,880 --> 00:11:48,160
going to show you this in a log log

394
00:11:48,160 --> 00:11:49,120
scale

395
00:11:49,120 --> 00:11:50,480
and this is just fairly standard in

396
00:11:50,480 --> 00:11:52,079
security and malware evaluations these

397
00:11:52,079 --> 00:11:53,360
kinds of things we like to report log

398
00:11:53,360 --> 00:11:54,959
log scales just to zoom in on the lower

399
00:11:54,959 --> 00:11:57,040
left region and so i'm showing the same

400
00:11:57,040 --> 00:11:58,720
data but you can see that like at false

401
00:11:58,720 --> 00:12:00,880
positive rates less than like 10 percent

402
00:12:00,880 --> 00:12:03,760
the attack is roughly random guessing

403
00:12:03,760 --> 00:12:05,760
okay and it's not true for like this one

404
00:12:05,760 --> 00:12:07,440
attack i'm not sort of singling that one

405
00:12:07,440 --> 00:12:08,560
attack out

406
00:12:08,560 --> 00:12:10,320
for all of the attacks that perform well

407
00:12:10,320 --> 00:12:11,680
on average

408
00:12:11,680 --> 00:12:13,839
they have very very poor success rates

409
00:12:13,839 --> 00:12:16,079
at low false positive rates

410
00:12:16,079 --> 00:12:17,920
okay so what about the other attacks

411
00:12:17,920 --> 00:12:19,440
there are others of them in this table

412
00:12:19,440 --> 00:12:21,120
so that's for these ones you know there

413
00:12:21,120 --> 00:12:22,720
are some other attacks here notice these

414
00:12:22,720 --> 00:12:25,120
ones perform less well so you'll take

415
00:12:25,120 --> 00:12:26,480
the first couple they all tightly

416
00:12:26,480 --> 00:12:28,399
clustered on 59 percent of tax success

417
00:12:28,399 --> 00:12:31,519
rates these ones are like 56 53 so

418
00:12:31,519 --> 00:12:33,600
they're less good on average

419
00:12:33,600 --> 00:12:35,120
but when you look at these attacks that

420
00:12:35,120 --> 00:12:36,959
were previously published you find that

421
00:12:36,959 --> 00:12:39,120
they have a much higher success rate at

422
00:12:39,120 --> 00:12:40,800
low false positive rate

423
00:12:40,800 --> 00:12:42,320
so this is one of the observations from

424
00:12:42,320 --> 00:12:43,680
our paper is that there are lots of

425
00:12:43,680 --> 00:12:44,880
attacks that already exist in the

426
00:12:44,880 --> 00:12:46,800
literature which can succeed at this

427
00:12:46,800 --> 00:12:48,639
metric they just haven't been evaluated

428
00:12:48,639 --> 00:12:50,480
this way before and these attacks we

429
00:12:50,480 --> 00:12:53,120
argue are better despite the fact that

430
00:12:53,120 --> 00:12:55,600
on average they do a little less well

431
00:12:55,600 --> 00:12:56,480
okay

432
00:12:56,480 --> 00:12:58,240
and so the maybe the surprising

433
00:12:58,240 --> 00:12:59,920
contribution from our paper is that not

434
00:12:59,920 --> 00:13:02,560
only do we have numbers that are more

435
00:13:02,560 --> 00:13:04,560
accurate on average so do we beat the

436
00:13:04,560 --> 00:13:07,360
best average case numbers we also beat

437
00:13:07,360 --> 00:13:09,360
the best true positive rate at low false

438
00:13:09,360 --> 00:13:11,440
positive rate numbers so the attack that

439
00:13:11,440 --> 00:13:12,720
we have is sort of better no matter the

440
00:13:12,720 --> 00:13:14,399
metric that you care about it's doing

441
00:13:14,399 --> 00:13:15,920
the right thing and because it's doing

442
00:13:15,920 --> 00:13:17,120
the right thing it's easier to more

443
00:13:17,120 --> 00:13:19,440
correctly model everything

444
00:13:19,440 --> 00:13:20,720
okay

445
00:13:20,720 --> 00:13:22,240
so this is the main set of results i'm

446
00:13:22,240 --> 00:13:24,160
going to present to you for this talk of

447
00:13:24,160 --> 00:13:27,200
course in in the paper we have all kinds

448
00:13:27,200 --> 00:13:29,519
of other analysis that talks about what

449
00:13:29,519 --> 00:13:30,800
happens

450
00:13:30,800 --> 00:13:32,880
on different data sets what happens we

451
00:13:32,880 --> 00:13:35,600
do an ablation study across why why what

452
00:13:35,600 --> 00:13:37,360
we have is better than what prior work

453
00:13:37,360 --> 00:13:38,880
you know what pieces are most contribute

454
00:13:38,880 --> 00:13:40,639
to the attack success rate we try and

455
00:13:40,639 --> 00:13:42,480
understand why some examples are more

456
00:13:42,480 --> 00:13:43,839
non-private than other examples what's

457
00:13:43,839 --> 00:13:45,839
going to make this happen i'd encourage

458
00:13:45,839 --> 00:13:47,519
you to look for all of these details and

459
00:13:47,519 --> 00:13:48,399
more

460
00:13:48,399 --> 00:13:50,320
for the paper for details

461
00:13:50,320 --> 00:13:52,639
so with that uh let me give sort of

462
00:13:52,639 --> 00:13:57,360
maybe now a brief conclusion um maybe uh

463
00:13:57,360 --> 00:13:58,959
this was not my conclusion welcome to

464
00:13:58,959 --> 00:14:01,440
ice for police s p okay uh if we can get

465
00:14:01,440 --> 00:14:03,839
it back

466
00:14:10,079 --> 00:14:12,000
should i keep going

467
00:14:12,000 --> 00:14:14,399
okay i'll keep going i hold up only one

468
00:14:14,399 --> 00:14:16,079
slide that's my favorite slide okay here

469
00:14:16,079 --> 00:14:18,319
we go

470
00:14:19,279 --> 00:14:21,279
okay okay here's my conclusion um

471
00:14:21,279 --> 00:14:22,480
basically the conclusion may be somewhat

472
00:14:22,480 --> 00:14:24,480
controversial um everything we know

473
00:14:24,480 --> 00:14:25,920
about membership members attack results

474
00:14:25,920 --> 00:14:27,199
might be wrong

475
00:14:27,199 --> 00:14:28,800
um okay so let me say what i mean by

476
00:14:28,800 --> 00:14:30,480
this because i mean something particular

477
00:14:30,480 --> 00:14:32,079
people have written lots of papers

478
00:14:32,079 --> 00:14:33,839
before that say things like

479
00:14:33,839 --> 00:14:36,160
weight decay does not help

480
00:14:36,160 --> 00:14:38,000
with membership members with ex it's not

481
00:14:38,000 --> 00:14:39,839
a good defense

482
00:14:39,839 --> 00:14:43,120
this result is true insofar as weight

483
00:14:43,120 --> 00:14:46,160
decay does not help on average defending

484
00:14:46,160 --> 00:14:47,600
its membership inference attacks that

485
00:14:47,600 --> 00:14:50,160
work by using a single global threshold

486
00:14:50,160 --> 00:14:52,079
that is a true result

487
00:14:52,079 --> 00:14:53,680
whether or not that result is

488
00:14:53,680 --> 00:14:54,959
interesting

489
00:14:54,959 --> 00:14:57,279
is a separate question like i argue that

490
00:14:57,279 --> 00:14:59,360
what we should be asking is

491
00:14:59,360 --> 00:15:01,680
is weight decay a valid defense at low

492
00:15:01,680 --> 00:15:03,199
false positive rates for the best

493
00:15:03,199 --> 00:15:04,639
attacks that we have

494
00:15:04,639 --> 00:15:05,839
and that's something that now we no

495
00:15:05,839 --> 00:15:07,680
longer know the answer to basically all

496
00:15:07,680 --> 00:15:09,360
of the results in the literature look

497
00:15:09,360 --> 00:15:11,199
exclusively at this average case number

498
00:15:11,199 --> 00:15:13,360
for the most part and use attacks that

499
00:15:13,360 --> 00:15:14,959
are quite poor

500
00:15:14,959 --> 00:15:16,399
and so for the most part we're going to

501
00:15:16,399 --> 00:15:17,839
have to reconsider many of these things

502
00:15:17,839 --> 00:15:19,760
that we think we know are true in order

503
00:15:19,760 --> 00:15:21,839
to see if they still remain true

504
00:15:21,839 --> 00:15:23,120
at low false positive rates with

505
00:15:23,120 --> 00:15:24,800
stronger attacks whether or not they do

506
00:15:24,800 --> 00:15:26,240
i think you know maybe it's slightly

507
00:15:26,240 --> 00:15:27,760
more biased to that they actually the

508
00:15:27,760 --> 00:15:30,000
results transfer but but we don't know

509
00:15:30,000 --> 00:15:31,519
and it's not just defenses you know

510
00:15:31,519 --> 00:15:32,720
there have been things like you know

511
00:15:32,720 --> 00:15:34,160
i've written a paper on how to do

512
00:15:34,160 --> 00:15:36,000
membership inference attacks with only

513
00:15:36,000 --> 00:15:37,519
being able to have hard label query

514
00:15:37,519 --> 00:15:38,880
access to a model

515
00:15:38,880 --> 00:15:40,880
but we evaluated this only on average

516
00:15:40,880 --> 00:15:42,320
case numbers does it work at low false

517
00:15:42,320 --> 00:15:44,000
positive rates i don't know maybe it

518
00:15:44,000 --> 00:15:45,199
does maybe it doesn't someone's going to

519
00:15:45,199 --> 00:15:46,399
need to actually go and reinvestigate

520
00:15:46,399 --> 00:15:47,440
this

521
00:15:47,440 --> 00:15:48,959
and notice please like there is like a

522
00:15:48,959 --> 00:15:50,560
huge asterisk here like i don't want to

523
00:15:50,560 --> 00:15:52,880
dismiss you know all of prior work as

524
00:15:52,880 --> 00:15:54,800
being wrong i just think we're going to

525
00:15:54,800 --> 00:15:57,040
have to be very careful about what

526
00:15:57,040 --> 00:15:58,560
trusting which numbers we have from

527
00:15:58,560 --> 00:16:00,079
prior work in order to make sure that

528
00:16:00,079 --> 00:16:01,759
that we can actually trust um what we're

529
00:16:01,759 --> 00:16:02,880
doing here

530
00:16:02,880 --> 00:16:04,639
so with that um you know thank you very

531
00:16:04,639 --> 00:16:08,920
much and happy to take any questions

532
00:16:14,399 --> 00:16:16,560
yeah thanks nicola

533
00:16:16,560 --> 00:16:18,399
very interesting talk my question is

534
00:16:18,399 --> 00:16:20,560
that do you require to know the training

535
00:16:20,560 --> 00:16:22,959
that is it except the sample that you're

536
00:16:22,959 --> 00:16:24,880
looking for

537
00:16:24,880 --> 00:16:26,800
yeah okay so let me sort of make sure i

538
00:16:26,800 --> 00:16:27,839
understand the question right do i need

539
00:16:27,839 --> 00:16:29,360
to be able to train on the example that

540
00:16:29,360 --> 00:16:31,680
i want to evaluate on um for the full

541
00:16:31,680 --> 00:16:33,839
exact as i've presented in the talk yes

542
00:16:33,839 --> 00:16:35,600
we need to train a model on this image

543
00:16:35,600 --> 00:16:37,839
this is we call an online attack where

544
00:16:37,839 --> 00:16:39,440
we assume we can sort of get the point

545
00:16:39,440 --> 00:16:40,480
and then train a bunch of models and

546
00:16:40,480 --> 00:16:42,399
then give an answer we also in the paper

547
00:16:42,399 --> 00:16:44,639
have an offline attack that sort of

548
00:16:44,639 --> 00:16:46,399
removes this assumption

549
00:16:46,399 --> 00:16:50,240
it performs almost as well um i didn't

550
00:16:50,240 --> 00:16:52,079
give the figure here but we have some

551
00:16:52,079 --> 00:16:53,600
figures in the paper that show how well

552
00:16:53,600 --> 00:16:55,040
the attack works in the offline setting

553
00:16:55,040 --> 00:16:56,399
you lose a couple percentage points of

554
00:16:56,399 --> 00:16:59,680
accuracy but it's almost as good

555
00:16:59,920 --> 00:17:01,680
all right first off awesome talk um i

556
00:17:01,680 --> 00:17:03,040
feel like every like privacy paper

557
00:17:03,040 --> 00:17:04,959
should not include disclaimers for that

558
00:17:04,959 --> 00:17:07,199
stuff um my question is

559
00:17:07,199 --> 00:17:09,280
assuming i would now uh use your

560
00:17:09,280 --> 00:17:10,959
approach for let's say dogs instead of

561
00:17:10,959 --> 00:17:13,039
cats what are like some of the pitfalls

562
00:17:13,039 --> 00:17:14,720
i should look out for

563
00:17:14,720 --> 00:17:16,720
what are some of the pitfalls for using

564
00:17:16,720 --> 00:17:18,319
the approach in general yeah i see okay

565
00:17:18,319 --> 00:17:19,679
so okay so why would you not use this

566
00:17:19,679 --> 00:17:21,280
okay great there's a bunch of reasons um

567
00:17:21,280 --> 00:17:22,640
the first of which is it requires

568
00:17:22,640 --> 00:17:25,359
training several models um the the nice

569
00:17:25,359 --> 00:17:26,720
thing about the simple attacks is you

570
00:17:26,720 --> 00:17:28,240
train one model you compute one global

571
00:17:28,240 --> 00:17:29,600
threshold and you're done

572
00:17:29,600 --> 00:17:31,440
um in order for this attack to work we

573
00:17:31,440 --> 00:17:32,400
need to train

574
00:17:32,400 --> 00:17:34,720
at least eight models

575
00:17:34,720 --> 00:17:36,160
in some settings training eight models

576
00:17:36,160 --> 00:17:37,440
is easy

577
00:17:37,440 --> 00:17:38,799
in other settings in machine learning

578
00:17:38,799 --> 00:17:40,240
space where people have training models

579
00:17:40,240 --> 00:17:42,320
with 500 billion parameters and it costs

580
00:17:42,320 --> 00:17:44,320
you know 10 million dollars telling them

581
00:17:44,320 --> 00:17:45,919
please do this seven more times so i can

582
00:17:45,919 --> 00:17:47,600
tell the privacy of this method they'll

583
00:17:47,600 --> 00:17:48,799
say no

584
00:17:48,799 --> 00:17:51,039
um so we need this is the main thing is

585
00:17:51,039 --> 00:17:53,280
we need sort of a more computation cost

586
00:17:53,280 --> 00:17:54,480
there are other factors that might

587
00:17:54,480 --> 00:17:56,960
contribute in the future because this is

588
00:17:56,960 --> 00:17:59,280
a slightly more involved setup there are

589
00:17:59,280 --> 00:18:01,840
more places where things might go wrong

590
00:18:01,840 --> 00:18:03,679
it's very easy to look at the loss and

591
00:18:03,679 --> 00:18:05,360
for a global threshold and say less than

592
00:18:05,360 --> 00:18:07,280
less than or greater than some value and

593
00:18:07,280 --> 00:18:08,880
so if people use this for like

594
00:18:08,880 --> 00:18:11,120
evaluating its defenses

595
00:18:11,120 --> 00:18:13,600
it's possible it'll be easier to

596
00:18:13,600 --> 00:18:15,760
accidentally fool yourself in getting

597
00:18:15,760 --> 00:18:17,280
something wrong when using it as a

598
00:18:17,280 --> 00:18:18,880
defensive evaluation tool we've seen

599
00:18:18,880 --> 00:18:20,640
this a lot in adversarial examples where

600
00:18:20,640 --> 00:18:22,559
more sophisticated attacks do better but

601
00:18:22,559 --> 00:18:24,480
people have a harder time applying them

602
00:18:24,480 --> 00:18:26,000
and so i worry that this may happen in

603
00:18:26,000 --> 00:18:27,520
the future but for right now at least we

604
00:18:27,520 --> 00:18:28,799
don't have you don't know where these

605
00:18:28,799 --> 00:18:30,720
failure cases might occur but um it's

606
00:18:30,720 --> 00:18:33,039
possible that they might exist

607
00:18:33,039 --> 00:18:36,400
awesome answer thank you

608
00:18:37,919 --> 00:18:39,520
hi thank you for the great talk i've

609
00:18:39,520 --> 00:18:41,200
just a very brief question concerning

610
00:18:41,200 --> 00:18:43,520
the basically what you've done with this

611
00:18:43,520 --> 00:18:46,080
masking device so you've always just

612
00:18:46,080 --> 00:18:48,080
said okay i'm going to mask 50 of the

613
00:18:48,080 --> 00:18:49,760
images and you know half the time the

614
00:18:49,760 --> 00:18:51,520
cat image is in there half the time it

615
00:18:51,520 --> 00:18:53,200
isn't but i mean in principle you could

616
00:18:53,200 --> 00:18:55,039
do different stuff right you could just

617
00:18:55,039 --> 00:18:57,600
artificially construct 50 50 um split

618
00:18:57,600 --> 00:18:59,679
ups putting in the cat image putting not

619
00:18:59,679 --> 00:19:01,440
in the cat image and then

620
00:19:01,440 --> 00:19:03,600
looking for smaller samples then 50

621
00:19:03,600 --> 00:19:05,360
looking for larger stuff

622
00:19:05,360 --> 00:19:09,280
taking some images multiple times um so

623
00:19:09,280 --> 00:19:10,799
uh what are your insights concerning

624
00:19:10,799 --> 00:19:14,000
this how does this uh influence the um

625
00:19:14,000 --> 00:19:16,240
the performance and i mean

626
00:19:16,240 --> 00:19:18,320
perhaps even uh this also connects to

627
00:19:18,320 --> 00:19:19,760
the diversity of the images right

628
00:19:19,760 --> 00:19:21,440
sometimes twenty percent might suffice

629
00:19:21,440 --> 00:19:22,799
sometimes not so what are your insights

630
00:19:22,799 --> 00:19:23,919
on that yeah this is a good question so

631
00:19:23,919 --> 00:19:26,640
yeah we choose 50 um this is not sort of

632
00:19:26,640 --> 00:19:28,080
fundamental to the attack in particular

633
00:19:28,080 --> 00:19:29,919
we have some numbers where we we sample

634
00:19:29,919 --> 00:19:32,080
with a different threshold so we

635
00:19:32,080 --> 00:19:33,600
actually attack some real models where

636
00:19:33,600 --> 00:19:35,200
people train on the entire cifar10

637
00:19:35,200 --> 00:19:36,880
training data set

638
00:19:36,880 --> 00:19:39,360
and here what we do is we take the test

639
00:19:39,360 --> 00:19:40,799
set and we train partly on the test set

640
00:19:40,799 --> 00:19:42,240
pull in the training set and what this

641
00:19:42,240 --> 00:19:43,520
means is because of the ratio of these

642
00:19:43,520 --> 00:19:44,880
two data sets we sample from the

643
00:19:44,880 --> 00:19:47,280
training data set with five sixths

644
00:19:47,280 --> 00:19:49,120
probability each sample and the attack

645
00:19:49,120 --> 00:19:50,960
works basically the same you'll just get

646
00:19:50,960 --> 00:19:52,400
more values in the histogram

647
00:19:52,400 --> 00:19:54,000
corresponding to when it was in versus

648
00:19:54,000 --> 00:19:55,440
when it was out

649
00:19:55,440 --> 00:19:57,600
the number here doesn't actually i mean

650
00:19:57,600 --> 00:19:59,200
50 is just the best number to choose if

651
00:19:59,200 --> 00:20:00,799
you want to change train as few few

652
00:20:00,799 --> 00:20:02,400
models as possible

653
00:20:02,400 --> 00:20:03,919
if you want to make sure that your

654
00:20:03,919 --> 00:20:05,760
models train on at least 80 of the data

655
00:20:05,760 --> 00:20:06,880
you can do this you just may need to

656
00:20:06,880 --> 00:20:08,640
train more models because you now have

657
00:20:08,640 --> 00:20:10,159
20 of the models that don't and so you

658
00:20:10,159 --> 00:20:12,080
still need at least like five models to

659
00:20:12,080 --> 00:20:13,440
compute a normal distribution and so

660
00:20:13,440 --> 00:20:14,720
you'll need to train more models in

661
00:20:14,720 --> 00:20:16,720
general but the number here is is not

662
00:20:16,720 --> 00:20:18,559
hugely sensitive

663
00:20:18,559 --> 00:20:21,720
thank you

664
00:20:22,799 --> 00:20:25,120
hi mahmoud arun from uc davis amazing

665
00:20:25,120 --> 00:20:26,320
presentation

666
00:20:26,320 --> 00:20:28,640
i was wondering if this attack could be

667
00:20:28,640 --> 00:20:30,720
adapted on the problem of attribute

668
00:20:30,720 --> 00:20:32,240
inference as well

669
00:20:32,240 --> 00:20:33,919
where the possibilities for privacy

670
00:20:33,919 --> 00:20:36,240
concerns are much greater yes um no

671
00:20:36,240 --> 00:20:37,760
attributive is yes we think this can be

672
00:20:37,760 --> 00:20:39,679
directly applied in particular there's a

673
00:20:39,679 --> 00:20:41,039
paper we've put out that doesn't do

674
00:20:41,039 --> 00:20:42,799
attribute influence by itself but it

675
00:20:42,799 --> 00:20:43,679
does

676
00:20:43,679 --> 00:20:45,360
this sort of new clever attack of

677
00:20:45,360 --> 00:20:47,039
poisoning in order to improve various

678
00:20:47,039 --> 00:20:48,400
downstream attacks

679
00:20:48,400 --> 00:20:50,640
and we show that we can use this attack

680
00:20:50,640 --> 00:20:52,559
to do as a baseline much much better on

681
00:20:52,559 --> 00:20:53,840
attribute inference and then if you're

682
00:20:53,840 --> 00:20:55,120
willing to do some additional

683
00:20:55,120 --> 00:20:56,159
assumptions we can make it go even

684
00:20:56,159 --> 00:20:57,360
further so one of the reasons we like

685
00:20:57,360 --> 00:20:59,679
this result is because it's general

686
00:20:59,679 --> 00:21:01,039
across not only membership influence but

687
00:21:01,039 --> 00:21:02,240
attribute inference and we show it can

688
00:21:02,240 --> 00:21:03,520
be helpful for data extraction and a

689
00:21:03,520 --> 00:21:04,960
couple other things so we think it's a

690
00:21:04,960 --> 00:21:06,480
very general approach

691
00:21:06,480 --> 00:21:09,520
all right thank you yeah

692
00:21:13,039 --> 00:21:15,440
right maybe as a last question uh to

693
00:21:15,440 --> 00:21:17,120
bring the session together with the

694
00:21:17,120 --> 00:21:19,840
usability and the attacks um

695
00:21:19,840 --> 00:21:23,120
what are like ideas to improve usability

696
00:21:23,120 --> 00:21:24,960
or issues if if there's anything you

697
00:21:24,960 --> 00:21:27,600
want to talk about to use use it sorry

698
00:21:27,600 --> 00:21:29,360
to improve usability of what

699
00:21:29,360 --> 00:21:32,480
a fewer presentation topic i see to

700
00:21:32,480 --> 00:21:34,240
improve the usability of this attack

701
00:21:34,240 --> 00:21:35,760
yeah no i think yeah the biggest thing

702
00:21:35,760 --> 00:21:36,880
right now is

703
00:21:36,880 --> 00:21:38,559
making it easier to without having to

704
00:21:38,559 --> 00:21:39,919
train more models

705
00:21:39,919 --> 00:21:43,280
this is i think the big thing that

706
00:21:43,280 --> 00:21:44,720
if someone told me they couldn't apply

707
00:21:44,720 --> 00:21:46,720
this approach probably the number one

708
00:21:46,720 --> 00:21:47,840
reason i think it would be is because

709
00:21:47,840 --> 00:21:49,919
they have to train many more models and

710
00:21:49,919 --> 00:21:51,200
i i would like to be able to see

711
00:21:51,200 --> 00:21:53,200
approach that don't require this i'm

712
00:21:53,200 --> 00:21:54,480
a little skeptical here that this will

713
00:21:54,480 --> 00:21:56,559
be easy just because fundamentally the

714
00:21:56,559 --> 00:21:57,919
way our approach works is by modeling

715
00:21:57,919 --> 00:21:59,679
things with gaussians and or

716
00:21:59,679 --> 00:22:01,280
distributions in order to make a

717
00:22:01,280 --> 00:22:02,960
prediction of a modeling you need at

718
00:22:02,960 --> 00:22:05,440
least two samples if not you know four

719
00:22:05,440 --> 00:22:06,799
or something and so

720
00:22:06,799 --> 00:22:08,400
i think it's going to be interesting but

721
00:22:08,400 --> 00:22:12,880
uh hopefully we can do better there yeah

722
00:22:12,880 --> 00:22:14,640
thank you it doesn't seem like there are

723
00:22:14,640 --> 00:22:16,320
more questions so thank you again

724
00:22:16,320 --> 00:22:18,159
dealers for your talk

725
00:22:18,159 --> 00:22:21,280
uh thanks everyone

726
00:22:21,280 --> 00:22:24,440
thank you

