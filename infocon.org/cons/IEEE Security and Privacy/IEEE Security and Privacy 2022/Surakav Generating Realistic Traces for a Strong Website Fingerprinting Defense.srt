1
00:00:01,439 --> 00:00:03,919
hi everyone i am

2
00:00:03,919 --> 00:00:06,000
i'm from the hong kong university of

3
00:00:06,000 --> 00:00:08,880
science and technology it is my pleasure

4
00:00:08,880 --> 00:00:11,120
to talk to you today about our work

5
00:00:11,120 --> 00:00:14,000
surakov generating realistic traces for

6
00:00:14,000 --> 00:00:16,320
strong website fingerprinting defense

7
00:00:16,320 --> 00:00:18,560
this is a joint work with wu chi zhang

8
00:00:18,560 --> 00:00:20,880
and my supervisors charles jung and tao

9
00:00:20,880 --> 00:00:23,119
wang

10
00:00:23,199 --> 00:00:24,880
more and more people have been turned

11
00:00:24,880 --> 00:00:27,119
into privacy enhancing communication

12
00:00:27,119 --> 00:00:30,480
tools like tor to access the internet

13
00:00:30,480 --> 00:00:33,120
however a local eavesdropper can launch

14
00:00:33,120 --> 00:00:35,520
a traffic analysis attack known as

15
00:00:35,520 --> 00:00:37,920
website fingerprinting to the anonymized

16
00:00:37,920 --> 00:00:40,160
user

17
00:00:40,160 --> 00:00:43,360
in nature it is a classification problem

18
00:00:43,360 --> 00:00:45,280
where the attacker uses a machine

19
00:00:45,280 --> 00:00:47,680
learning or deep learning technique to

20
00:00:47,680 --> 00:00:50,399
infer the web pages by timing and size

21
00:00:50,399 --> 00:00:53,680
information in the package sequences

22
00:00:53,680 --> 00:00:55,760
modern attacks have achieved more than

23
00:00:55,760 --> 00:00:58,719
90 percent prediction accuracy posing a

24
00:00:58,719 --> 00:01:01,440
great threat

25
00:01:01,520 --> 00:01:04,080
to address this problem several defenses

26
00:01:04,080 --> 00:01:06,720
have been proposed

27
00:01:06,720 --> 00:01:09,280
for example lightweight defenses such as

28
00:01:09,280 --> 00:01:12,560
wtf pad and front rely on the randomness

29
00:01:12,560 --> 00:01:15,360
of the way they inject dummy packets

30
00:01:15,360 --> 00:01:18,080
they do not delay user packets so they

31
00:01:18,080 --> 00:01:19,920
incur low overhead

32
00:01:19,920 --> 00:01:22,000
however they do not provide strong

33
00:01:22,000 --> 00:01:25,759
protection against deep learning attacks

34
00:01:25,759 --> 00:01:28,640
by contrast walkie-talkie and tamara

35
00:01:28,640 --> 00:01:30,880
force users to load pages in some

36
00:01:30,880 --> 00:01:32,640
predefined patterns

37
00:01:32,640 --> 00:01:37,119
they are effective but hard to implement

38
00:01:37,119 --> 00:01:39,360
we try to look deeper into the designs

39
00:01:39,360 --> 00:01:42,159
of those effective defenses and identify

40
00:01:42,159 --> 00:01:44,320
their problems

41
00:01:44,320 --> 00:01:47,280
we find that timer forces packets to be

42
00:01:47,280 --> 00:01:50,000
sent at a constant rate totally ignoring

43
00:01:50,000 --> 00:01:51,920
the characteristics of loading different

44
00:01:51,920 --> 00:01:52,960
pages

45
00:01:52,960 --> 00:01:56,079
leading to large overhead

46
00:01:56,079 --> 00:01:58,880
while walkie-talkie reduces the overhead

47
00:01:58,880 --> 00:02:00,880
by choosing a suitable sending pattern

48
00:02:00,880 --> 00:02:03,759
for each each page it assumes that we

49
00:02:03,759 --> 00:02:05,840
can know the pattern of the web pages in

50
00:02:05,840 --> 00:02:07,040
advance

51
00:02:07,040 --> 00:02:10,639
which is hard to achieve in reality

52
00:02:10,639 --> 00:02:12,879
to fix these problems while holding the

53
00:02:12,879 --> 00:02:15,440
defense effectiveness we come up with

54
00:02:15,440 --> 00:02:18,239
the idea of using a generative model to

55
00:02:18,239 --> 00:02:20,400
generate diverse standing patterns for

56
00:02:20,400 --> 00:02:23,200
us

57
00:02:23,200 --> 00:02:25,280
therefore we propose a new defense

58
00:02:25,280 --> 00:02:26,959
called surah curve

59
00:02:26,959 --> 00:02:29,040
the general idea is to leverage a

60
00:02:29,040 --> 00:02:31,360
generative model that learns a variety

61
00:02:31,360 --> 00:02:33,599
of sending patterns to guide the package

62
00:02:33,599 --> 00:02:35,040
sending process

63
00:02:35,040 --> 00:02:37,680
under the defense of surakov even if you

64
00:02:37,680 --> 00:02:39,840
load the same page several times the

65
00:02:39,840 --> 00:02:41,840
data will be tunneled by different

66
00:02:41,840 --> 00:02:44,400
sending patterns which greatly lowers

67
00:02:44,400 --> 00:02:47,920
the attacker's success rate

68
00:02:48,560 --> 00:02:50,959
to achieve our goal we first need to

69
00:02:50,959 --> 00:02:52,640
train a generator

70
00:02:52,640 --> 00:02:55,280
there are two issues we need to consider

71
00:02:55,280 --> 00:02:57,200
the first is what pattern should we

72
00:02:57,200 --> 00:02:58,640
mimic

73
00:02:58,640 --> 00:03:00,480
we decide to directly learn from the

74
00:03:00,480 --> 00:03:03,599
burst sequences of loading railway pages

75
00:03:03,599 --> 00:03:05,920
the intuition behind this is the sending

76
00:03:05,920 --> 00:03:07,680
patterns should capture the general

77
00:03:07,680 --> 00:03:10,800
characteristics of a normal page load so

78
00:03:10,800 --> 00:03:12,720
that following these patterns won't

79
00:03:12,720 --> 00:03:15,599
incur much unnecessary overhead

80
00:03:15,599 --> 00:03:17,920
and there are already a few large data

81
00:03:17,920 --> 00:03:20,239
sets from previous studies that can be

82
00:03:20,239 --> 00:03:23,200
used at the training source

83
00:03:23,200 --> 00:03:25,599
the second question is what generative

84
00:03:25,599 --> 00:03:27,840
model should we use

85
00:03:27,840 --> 00:03:29,599
we choose to use a generative

86
00:03:29,599 --> 00:03:32,560
adversarial network namely gang as it

87
00:03:32,560 --> 00:03:34,239
was shown to be able to generate

88
00:03:34,239 --> 00:03:37,040
realistic traces with more diversity in

89
00:03:37,040 --> 00:03:40,720
our preliminary experiments

90
00:03:40,959 --> 00:03:43,040
since there is no ready model for our

91
00:03:43,040 --> 00:03:47,040
task we build the game from scratch

92
00:03:47,040 --> 00:03:49,760
first we have a generator which is a

93
00:03:49,760 --> 00:03:51,920
multi-layer perceptron to generate fake

94
00:03:51,920 --> 00:03:53,120
traces

95
00:03:53,120 --> 00:03:55,760
the input is a label of a web page and

96
00:03:55,760 --> 00:03:58,480
random noise it tries to generate a

97
00:03:58,480 --> 00:04:00,720
trace of this page

98
00:04:00,720 --> 00:04:03,200
then both real and fake traces will be

99
00:04:03,200 --> 00:04:05,200
fed into the discriminator

100
00:04:05,200 --> 00:04:08,080
the discriminator is asked to tell them

101
00:04:08,080 --> 00:04:10,560
apart by answering whether the trace is

102
00:04:10,560 --> 00:04:12,080
real or fake

103
00:04:12,080 --> 00:04:14,159
this feedback will then be passed back

104
00:04:14,159 --> 00:04:16,959
to the generator to improve its ability

105
00:04:16,959 --> 00:04:19,519
to craft realistic traces

106
00:04:19,519 --> 00:04:21,759
however since network traces are

107
00:04:21,759 --> 00:04:24,479
complicated we further introduce a novel

108
00:04:24,479 --> 00:04:27,199
observer to refine tracecrafting

109
00:04:27,199 --> 00:04:29,120
the observer is a pre-trained website

110
00:04:29,120 --> 00:04:31,280
fingerprinting model for those fake

111
00:04:31,280 --> 00:04:33,199
traces that successfully for the

112
00:04:33,199 --> 00:04:35,520
discriminator we further query the

113
00:04:35,520 --> 00:04:37,280
observer to see whether they are

114
00:04:37,280 --> 00:04:40,720
classified into the expected class

115
00:04:40,720 --> 00:04:42,800
then the observer will give this

116
00:04:42,800 --> 00:04:45,520
feedback to the generator so the final

117
00:04:45,520 --> 00:04:47,919
training loss is the linear combination

118
00:04:47,919 --> 00:04:49,759
of the feedback from both the

119
00:04:49,759 --> 00:04:52,479
discriminator and the observer

120
00:04:52,479 --> 00:04:55,199
we introduce a parameter alpha here to

121
00:04:55,199 --> 00:04:58,639
balance these two losses

122
00:04:59,520 --> 00:05:02,080
after this adversarial training process

123
00:05:02,080 --> 00:05:04,400
we can get a generator that can output

124
00:05:04,400 --> 00:05:06,080
realistic traces

125
00:05:06,080 --> 00:05:08,240
we choose to use reverse dataset to

126
00:05:08,240 --> 00:05:10,720
train the model as it is one of the

127
00:05:10,720 --> 00:05:13,039
largest existing datasets

128
00:05:13,039 --> 00:05:16,400
we randomly pick 100 classes for each

129
00:05:16,400 --> 00:05:20,160
class we pick 1000 instances

130
00:05:20,160 --> 00:05:22,400
we use water stain distance to monitor

131
00:05:22,400 --> 00:05:24,960
the training process versus staying

132
00:05:24,960 --> 00:05:27,360
distance is helpful in estimating the

133
00:05:27,360 --> 00:05:29,280
distance between two probability

134
00:05:29,280 --> 00:05:30,639
distributions

135
00:05:30,639 --> 00:05:32,479
in our case we try to make the

136
00:05:32,479 --> 00:05:35,520
distribution of fake traces as close to

137
00:05:35,520 --> 00:05:37,680
the distribution of real traces as

138
00:05:37,680 --> 00:05:39,039
possible

139
00:05:39,039 --> 00:05:41,360
we find that after the training the

140
00:05:41,360 --> 00:05:43,440
vertical standing distance is reduced

141
00:05:43,440 --> 00:05:47,199
from 0.9 to 0.02

142
00:05:47,199 --> 00:05:49,440
to visualize this change we plot the

143
00:05:49,440 --> 00:05:51,919
mean burst sizes of the generated traces

144
00:05:51,919 --> 00:05:53,840
of the same class

145
00:05:53,840 --> 00:05:55,919
as shown in this figure we use a

146
00:05:55,919 --> 00:05:58,240
positive number to represent an outgoing

147
00:05:58,240 --> 00:06:00,720
burst and a negative number to represent

148
00:06:00,720 --> 00:06:02,319
an incoming burst

149
00:06:02,319 --> 00:06:04,720
after one epoch the fake trace is

150
00:06:04,720 --> 00:06:06,880
different from the real one

151
00:06:06,880 --> 00:06:10,080
but after 500 epochs the fake traces are

152
00:06:10,080 --> 00:06:12,560
statistically close to the real ones as

153
00:06:12,560 --> 00:06:15,919
the red line is close to the black line

154
00:06:15,919 --> 00:06:19,039
last but not least we find that 90 of

155
00:06:19,039 --> 00:06:21,520
the generated traces are classified into

156
00:06:21,520 --> 00:06:24,960
their expected classes

157
00:06:25,840 --> 00:06:28,240
after we obtain the generator we can

158
00:06:28,240 --> 00:06:31,199
start to send packets based on it

159
00:06:31,199 --> 00:06:34,639
we first generate a trace t then the

160
00:06:34,639 --> 00:06:36,800
client and the proxy will take turns

161
00:06:36,800 --> 00:06:39,440
sending births based on t

162
00:06:39,440 --> 00:06:42,080
the client controls the timing the time

163
00:06:42,080 --> 00:06:43,919
gaps are randomly sampled from a

164
00:06:43,919 --> 00:06:45,199
distribution

165
00:06:45,199 --> 00:06:47,280
we learn such a distribution from real

166
00:06:47,280 --> 00:06:49,440
data sets using kernel density

167
00:06:49,440 --> 00:06:50,960
estimation

168
00:06:50,960 --> 00:06:53,680
by using random time gaps we can leak as

169
00:06:53,680 --> 00:06:56,160
little time information as possible

170
00:06:56,160 --> 00:06:58,639
meanwhile we guarantee these time gaps

171
00:06:58,639 --> 00:07:01,360
follow the general loading pattern

172
00:07:01,360 --> 00:07:03,759
however we do not strictly follow the

173
00:07:03,759 --> 00:07:06,400
sending patterns as this strategy may

174
00:07:06,400 --> 00:07:08,560
lead to high overhead just like the

175
00:07:08,560 --> 00:07:10,319
previous defense does

176
00:07:10,319 --> 00:07:13,199
therefore we propose two mechanisms that

177
00:07:13,199 --> 00:07:15,599
dynamically adjust the sending patterns

178
00:07:15,599 --> 00:07:18,400
in real time

179
00:07:18,400 --> 00:07:20,560
the first mechanism is burst size

180
00:07:20,560 --> 00:07:21,680
adjustment

181
00:07:21,680 --> 00:07:24,400
we introduce a parameter delta here and

182
00:07:24,400 --> 00:07:27,520
can compute two boundary values given a

183
00:07:27,520 --> 00:07:31,199
required first size b fake then at the

184
00:07:31,199 --> 00:07:33,440
same time if the size of the buffered

185
00:07:33,440 --> 00:07:36,319
data is lower than the lower bound value

186
00:07:36,319 --> 00:07:38,240
we will adjust the burst size to the

187
00:07:38,240 --> 00:07:41,199
lower bound with some dummy packets

188
00:07:41,199 --> 00:07:43,840
similarly if the size of buffered data

189
00:07:43,840 --> 00:07:46,240
is larger than the upper bound we will

190
00:07:46,240 --> 00:07:48,479
hold some real data and only send a

191
00:07:48,479 --> 00:07:50,960
burst of upper bound size

192
00:07:50,960 --> 00:07:53,520
if the burst size is within the range of

193
00:07:53,520 --> 00:07:56,080
delta in other words is close to the

194
00:07:56,080 --> 00:07:58,800
required fake burst we will directly

195
00:07:58,800 --> 00:08:02,720
send out this burst without modification

196
00:08:02,720 --> 00:08:05,280
a smaller delta will respect the

197
00:08:05,280 --> 00:08:07,360
required sending pattern more than the

198
00:08:07,360 --> 00:08:10,479
real pattern and vice versa

199
00:08:10,479 --> 00:08:12,240
by taking both patterns into

200
00:08:12,240 --> 00:08:14,879
consideration such a mechanism provides

201
00:08:14,879 --> 00:08:17,120
more flexibility for users with

202
00:08:17,120 --> 00:08:20,800
different security preferences

203
00:08:21,199 --> 00:08:23,759
to further reduce the data overhead we

204
00:08:23,759 --> 00:08:25,680
introduce another mechanism called

205
00:08:25,680 --> 00:08:27,440
random response

206
00:08:27,440 --> 00:08:29,919
we allow the proxy to skip sending a

207
00:08:29,919 --> 00:08:32,399
total fake burst when there is no real

208
00:08:32,399 --> 00:08:35,919
data in the buffer with a 50 chance

209
00:08:35,919 --> 00:08:37,839
we only apply this mechanism on the

210
00:08:37,839 --> 00:08:40,399
proxy side because we usually have much

211
00:08:40,399 --> 00:08:42,799
larger bursts on the proxy side which

212
00:08:42,799 --> 00:08:44,880
mainly contributes to the final data

213
00:08:44,880 --> 00:08:47,360
overhead

214
00:08:47,440 --> 00:08:50,320
to evaluate our defense we consider the

215
00:08:50,320 --> 00:08:52,560
open world setting where the attacker

216
00:08:52,560 --> 00:08:55,440
monitors a list of 100 web pages

217
00:08:55,440 --> 00:08:58,480
the client may also visit other 60 000

218
00:08:58,480 --> 00:08:59,839
pages

219
00:08:59,839 --> 00:09:02,080
all those pages are from the tranquil

220
00:09:02,080 --> 00:09:05,200
list collected in january 2021

221
00:09:05,200 --> 00:09:07,200
the attacker's goal is to find out

222
00:09:07,200 --> 00:09:09,200
whether the client is visiting any

223
00:09:09,200 --> 00:09:13,360
monitor page and which is it

224
00:09:14,720 --> 00:09:17,440
as simulation could be inaccurate we

225
00:09:17,440 --> 00:09:19,760
decide to implement each defense and

226
00:09:19,760 --> 00:09:21,600
directly measure their performance in

227
00:09:21,600 --> 00:09:23,600
the realtor network

228
00:09:23,600 --> 00:09:25,760
to achieve that we implement each

229
00:09:25,760 --> 00:09:28,000
defense as plugable transport

230
00:09:28,000 --> 00:09:30,560
in this case the entry node will be our

231
00:09:30,560 --> 00:09:33,120
cooperating proxy to help forward real

232
00:09:33,120 --> 00:09:35,760
packets and deal with dummy packets

233
00:09:35,760 --> 00:09:38,240
the data from the top process will be

234
00:09:38,240 --> 00:09:39,920
reshaped according to the defense

235
00:09:39,920 --> 00:09:42,080
protocol before they are sent on to the

236
00:09:42,080 --> 00:09:43,600
connection

237
00:09:43,600 --> 00:09:46,640
we rent several servers on asu we place

238
00:09:46,640 --> 00:09:48,959
the user in hong kong and the entry node

239
00:09:48,959 --> 00:09:51,440
in the us to create some physical

240
00:09:51,440 --> 00:09:53,920
distance

241
00:09:54,080 --> 00:09:56,240
we first show the performance of sewer

242
00:09:56,240 --> 00:09:58,480
curve against the different attacks

243
00:09:58,480 --> 00:10:00,800
we present sure curve as two overhead

244
00:10:00,800 --> 00:10:03,600
levels by adjusting the parameter delta

245
00:10:03,600 --> 00:10:07,360
to 0.4 and 0.6 respectively

246
00:10:07,360 --> 00:10:09,519
as a baseline all the attacks can

247
00:10:09,519 --> 00:10:11,920
achieve over 90 percent accuracy on the

248
00:10:11,920 --> 00:10:14,079
undefended dataset

249
00:10:14,079 --> 00:10:16,640
as shown in this figure sure curve is

250
00:10:16,640 --> 00:10:20,160
very effective against kfp and qmu even

251
00:10:20,160 --> 00:10:22,480
under the lightweight setting

252
00:10:22,480 --> 00:10:24,480
for stronger attacks like df and

253
00:10:24,480 --> 00:10:26,959
tick-tock circle is rather effective

254
00:10:26,959 --> 00:10:30,399
under the heavyweight setting

255
00:10:30,640 --> 00:10:33,040
next we compare surcoff with other

256
00:10:33,040 --> 00:10:35,360
defenses against the strongest attack

257
00:10:35,360 --> 00:10:36,880
tick tock

258
00:10:36,880 --> 00:10:39,760
as we can see surakov heavy has the best

259
00:10:39,760 --> 00:10:42,240
performance among all the defenses it

260
00:10:42,240 --> 00:10:44,160
reduces the true positive rate of the

261
00:10:44,160 --> 00:10:46,800
attacker from 97 percent to only 6

262
00:10:46,800 --> 00:10:48,079
percent

263
00:10:48,079 --> 00:10:50,720
circled light on the other hand achieves

264
00:10:50,720 --> 00:10:53,200
a similar protection level as front

265
00:10:53,200 --> 00:10:55,040
reducing the true possible rate of the

266
00:10:55,040 --> 00:10:58,959
attacker by nearly 60 percent

267
00:10:59,200 --> 00:11:01,680
when it comes to the overhead we find

268
00:11:01,680 --> 00:11:05,040
that compared to front surcoff saves 42

269
00:11:05,040 --> 00:11:07,200
percent data overhead to achieve a

270
00:11:07,200 --> 00:11:09,120
similar protection rate

271
00:11:09,120 --> 00:11:12,240
or offers much more robust protection at

272
00:11:12,240 --> 00:11:15,040
a similar overhead level

273
00:11:15,040 --> 00:11:17,600
when we compare surcoff to tamron which

274
00:11:17,600 --> 00:11:19,600
was considered the strongest defense

275
00:11:19,600 --> 00:11:22,320
before we find that sure curve requires

276
00:11:22,320 --> 00:11:24,720
40 percent less data overhead and 10

277
00:11:24,720 --> 00:11:27,279
percent less time overhead to achieve an

278
00:11:27,279 --> 00:11:30,880
even lower true positive rate

279
00:11:31,920 --> 00:11:34,640
modern defense evaluation also includes

280
00:11:34,640 --> 00:11:37,040
the information leakage analysis which

281
00:11:37,040 --> 00:11:39,279
examines the amount of information an

282
00:11:39,279 --> 00:11:41,279
attacker can learn from a specific

283
00:11:41,279 --> 00:11:42,399
feature

284
00:11:42,399 --> 00:11:45,240
we conduct the same analysis using the

285
00:11:45,240 --> 00:11:48,480
wefd framework and plot the cumulative

286
00:11:48,480 --> 00:11:50,880
distribution function of the information

287
00:11:50,880 --> 00:11:53,200
leakage for the top 100 informative

288
00:11:53,200 --> 00:11:54,320
features

289
00:11:54,320 --> 00:11:56,800
we find that surcoff leaks the least

290
00:11:56,800 --> 00:11:59,279
bits of information for these features

291
00:11:59,279 --> 00:12:02,399
among other defenses

292
00:12:03,279 --> 00:12:05,920
last but not least since our defense

293
00:12:05,920 --> 00:12:08,800
utilizes a generator we also care about

294
00:12:08,800 --> 00:12:11,279
its distribution cost

295
00:12:11,279 --> 00:12:13,360
suppose the top directory servers are

296
00:12:13,360 --> 00:12:15,440
responsible for distributing the trained

297
00:12:15,440 --> 00:12:18,399
model according to our estimation it

298
00:12:18,399 --> 00:12:20,639
only adds one to eight percent extra

299
00:12:20,639 --> 00:12:23,120
bandwidth overhead to the existing

300
00:12:23,120 --> 00:12:25,519
bandwidth consumption depending on how

301
00:12:25,519 --> 00:12:27,600
frequently and how many users will

302
00:12:27,600 --> 00:12:30,639
update the model

303
00:12:30,959 --> 00:12:33,760
to sum up in this study we proposed a

304
00:12:33,760 --> 00:12:35,519
strong website fingerprinting defense

305
00:12:35,519 --> 00:12:38,160
called surcob surcoff leverages a

306
00:12:38,160 --> 00:12:40,240
self-designed generative adversarial

307
00:12:40,240 --> 00:12:42,480
model to generate realistic sending

308
00:12:42,480 --> 00:12:43,519
patterns

309
00:12:43,519 --> 00:12:45,200
it tunnels real data through these

310
00:12:45,200 --> 00:12:47,279
patterns while also applying two

311
00:12:47,279 --> 00:12:49,279
mechanisms to bring about more

312
00:12:49,279 --> 00:12:52,399
randomness and to reduce the overhead if

313
00:12:52,399 --> 00:12:54,560
you are interested in our project you

314
00:12:54,560 --> 00:12:56,240
can find

315
00:12:56,240 --> 00:12:57,200
it

316
00:12:57,200 --> 00:13:00,880
thanks for listening to this talk

317
00:13:01,200 --> 00:13:02,639
thanks

318
00:13:02,639 --> 00:13:04,800
speaker

319
00:13:04,800 --> 00:13:07,800
hi

320
00:13:08,000 --> 00:13:10,560
questions

321
00:13:11,360 --> 00:13:13,200
no questions

322
00:13:13,200 --> 00:13:14,480
okay

323
00:13:14,480 --> 00:13:15,519
okay so

324
00:13:15,519 --> 00:13:17,519
is he online

325
00:13:17,519 --> 00:13:21,360
yes i am okay sorry uh good uh so i have

326
00:13:21,360 --> 00:13:23,600
one question actually not my error great

327
00:13:23,600 --> 00:13:26,000
talk actually uh very interesting but uh

328
00:13:26,000 --> 00:13:27,760
i was wondering

329
00:13:27,760 --> 00:13:30,160
so you train actually again on the data

330
00:13:30,160 --> 00:13:32,880
set it was collected in 2017. so is this

331
00:13:32,880 --> 00:13:35,279
something that needs to be retrained is

332
00:13:35,279 --> 00:13:37,040
it something that uh

333
00:13:37,040 --> 00:13:39,199
affects in some way you know the

334
00:13:39,199 --> 00:13:42,480
uh the packets to generate or is

335
00:13:42,480 --> 00:13:45,440
i don't know did you try

336
00:13:45,440 --> 00:13:49,279
um so um since we don't uh actually we

337
00:13:49,279 --> 00:13:50,399
trained

338
00:13:50,399 --> 00:13:52,959
on two different data sets which is

339
00:13:52,959 --> 00:13:55,279
rumors that set and another is sure

340
00:13:55,279 --> 00:13:57,199
names dataset

341
00:13:57,199 --> 00:14:00,079
and i think um they

342
00:14:00,079 --> 00:14:03,279
show similar performance and also we try

343
00:14:03,279 --> 00:14:06,480
to use for example the first 100

344
00:14:06,480 --> 00:14:09,279
not the first randomly choose uh 100

345
00:14:09,279 --> 00:14:12,160
classes in rumors dataset and

346
00:14:12,160 --> 00:14:14,320
or choose like

347
00:14:14,320 --> 00:14:17,680
500 classes in the data set and try the

348
00:14:17,680 --> 00:14:21,359
performance and they perform

349
00:14:21,680 --> 00:14:24,639
like the results are similar so i guess

350
00:14:24,639 --> 00:14:28,000
maybe it will have like minor effect on

351
00:14:28,000 --> 00:14:30,480
the defense performance but it is

352
00:14:30,480 --> 00:14:33,040
possible uh that it could affect the

353
00:14:33,040 --> 00:14:35,839
performance in some way if um

354
00:14:35,839 --> 00:14:38,560
you know the collection year is too far

355
00:14:38,560 --> 00:14:39,680
away from

356
00:14:39,680 --> 00:14:42,560
the year when you test the defense

357
00:14:42,560 --> 00:14:46,160
but um we don't test that like we don't

358
00:14:46,160 --> 00:14:49,519
have existing desks that um have such

359
00:14:49,519 --> 00:14:52,399
long time gap yeah we explore some

360
00:14:52,399 --> 00:14:54,000
possibility here

361
00:14:54,000 --> 00:14:55,920
okay thank you so basically you don't

362
00:14:55,920 --> 00:14:57,600
expect you know that you need to retrain

363
00:14:57,600 --> 00:14:59,920
the model very often maybe it's possible

364
00:14:59,920 --> 00:15:02,000
but it seems like you know uh it doesn't

365
00:15:02,000 --> 00:15:04,800
affect too much correct

366
00:15:04,800 --> 00:15:06,000
yeah yeah

367
00:15:06,000 --> 00:15:08,000
um and also we

368
00:15:08,000 --> 00:15:09,120
um

369
00:15:09,120 --> 00:15:11,440
yeah the the training cost is

370
00:15:11,440 --> 00:15:14,240
actually not that large like it's six

371
00:15:14,240 --> 00:15:15,360
hours

372
00:15:15,360 --> 00:15:18,000
to train the model so even if you want

373
00:15:18,000 --> 00:15:19,680
to train it like frequently i don't

374
00:15:19,680 --> 00:15:21,839
think it is a problem

375
00:15:21,839 --> 00:15:23,519
okay we have a question in the room yeah

376
00:15:23,519 --> 00:15:25,920
so hamas university of iowa so one of

377
00:15:25,920 --> 00:15:27,680
the problems that thor faces right now

378
00:15:27,680 --> 00:15:29,920
is that there's three hops between uh

379
00:15:29,920 --> 00:15:31,839
the client and the end website which

380
00:15:31,839 --> 00:15:34,320
makes the travel packet the packet

381
00:15:34,320 --> 00:15:35,680
travel a lot more distance than it's

382
00:15:35,680 --> 00:15:37,759
supposed to so it increases the latency

383
00:15:37,759 --> 00:15:39,680
that the packet takes from the client to

384
00:15:39,680 --> 00:15:42,560
the end server so now that the packets

385
00:15:42,560 --> 00:15:44,880
are somewhat delayed in some instances

386
00:15:44,880 --> 00:15:47,360
do you have an idea of what the increase

387
00:15:47,360 --> 00:15:49,279
in latency might be and how it might

388
00:15:49,279 --> 00:15:51,759
affect how users

389
00:15:51,759 --> 00:15:53,440
go away from tor or

390
00:15:53,440 --> 00:15:56,800
how they might still use it

391
00:15:57,440 --> 00:16:01,920
um yeah so the time the latency is one

392
00:16:01,920 --> 00:16:03,600
manufacturer we need to consider when

393
00:16:03,600 --> 00:16:07,680
design a defense and in our case we met

394
00:16:07,680 --> 00:16:10,240
we measured the overhead and it

395
00:16:10,240 --> 00:16:11,680
increased about

396
00:16:11,680 --> 00:16:12,560
um

397
00:16:12,560 --> 00:16:15,040
16 to 18

398
00:16:15,040 --> 00:16:16,240
on average

399
00:16:16,240 --> 00:16:17,839
so

400
00:16:17,839 --> 00:16:18,720
uh

401
00:16:18,720 --> 00:16:20,880
i don't think it's a large

402
00:16:20,880 --> 00:16:23,519
time overheads if we consider

403
00:16:23,519 --> 00:16:26,880
a it is an effective defense and also we

404
00:16:26,880 --> 00:16:28,959
have actually another

405
00:16:28,959 --> 00:16:31,120
hyper parameter in the defense which

406
00:16:31,120 --> 00:16:33,839
controls that the maximum delay you will

407
00:16:33,839 --> 00:16:35,120
have

408
00:16:35,120 --> 00:16:37,279
every time you send out a burst

409
00:16:37,279 --> 00:16:39,199
yeah and if you're

410
00:16:39,199 --> 00:16:41,279
very sensitive to the time overhead

411
00:16:41,279 --> 00:16:43,920
actually for the user um he can choose

412
00:16:43,920 --> 00:16:46,720
to set it to a low value so that you

413
00:16:46,720 --> 00:16:48,160
will get a

414
00:16:48,160 --> 00:16:50,560
response in a

415
00:16:50,560 --> 00:16:51,600
shorter

416
00:16:51,600 --> 00:16:53,519
time period yeah

417
00:16:53,519 --> 00:16:55,360
so that you can reduce the time overhead

418
00:16:55,360 --> 00:16:56,320
further

419
00:16:56,320 --> 00:16:59,199
and also was this evaluated over just uh

420
00:16:59,199 --> 00:17:01,040
two geographic locations so i get

421
00:17:01,040 --> 00:17:03,199
imagine it's just hong kong in the us

422
00:17:03,199 --> 00:17:04,959
did you try it over different geographic

423
00:17:04,959 --> 00:17:07,199
locations for example longer distances

424
00:17:07,199 --> 00:17:09,199
and that might increase the latency that

425
00:17:09,199 --> 00:17:11,280
you would see

426
00:17:11,280 --> 00:17:13,599
unfortunately no

427
00:17:13,599 --> 00:17:14,959
okay thank you

428
00:17:14,959 --> 00:17:18,760
yeah thank you

429
00:17:19,760 --> 00:17:24,119
okay so let's thank the speaker again

