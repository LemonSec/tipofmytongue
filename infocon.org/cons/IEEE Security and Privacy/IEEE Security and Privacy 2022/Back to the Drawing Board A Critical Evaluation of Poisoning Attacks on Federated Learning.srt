1
00:00:00,480 --> 00:00:03,199
uh hi everyone i'm virat and i'm very

2
00:00:03,199 --> 00:00:05,279
happy to present our work uh back to the

3
00:00:05,279 --> 00:00:06,879
drawing board a critical analysis of

4
00:00:06,879 --> 00:00:08,880
poisoning attacks on production

5
00:00:08,880 --> 00:00:11,040
federated learning so this is a joint

6
00:00:11,040 --> 00:00:13,759
work with my advisor amir at umass

7
00:00:13,759 --> 00:00:15,440
amherst and my collaborators peter

8
00:00:15,440 --> 00:00:18,400
cairos and daniel ramich add google

9
00:00:18,400 --> 00:00:20,080
research

10
00:00:20,080 --> 00:00:22,080
so as we all know in traditional machine

11
00:00:22,080 --> 00:00:24,240
learning a server or a service provider

12
00:00:24,240 --> 00:00:26,880
collects data from a bunch of users and

13
00:00:26,880 --> 00:00:28,720
trains a machine learning model in a

14
00:00:28,720 --> 00:00:30,400
centralized fashion

15
00:00:30,400 --> 00:00:32,000
but this raises a lot of privacy

16
00:00:32,000 --> 00:00:34,079
concerns because the users have to share

17
00:00:34,079 --> 00:00:36,559
their data in plain text to with a

18
00:00:36,559 --> 00:00:38,399
untrusted and potentially malicious

19
00:00:38,399 --> 00:00:39,440
server

20
00:00:39,440 --> 00:00:41,760
also the server has to bear the entire

21
00:00:41,760 --> 00:00:44,320
cost of computation

22
00:00:44,320 --> 00:00:46,079
so if edited learning addresses some of

23
00:00:46,079 --> 00:00:48,320
these issues to a certain extent

24
00:00:48,320 --> 00:00:50,640
by allowing the clients or users to

25
00:00:50,640 --> 00:00:54,079
collaborate via a central server

26
00:00:54,079 --> 00:00:56,000
and while keeping their data local and

27
00:00:56,000 --> 00:00:58,879
therefore private to some extent

28
00:00:58,879 --> 00:01:01,280
uh specifically the server broadcasts a

29
00:01:01,280 --> 00:01:04,080
global model to a subset of clients in

30
00:01:04,080 --> 00:01:06,159
nfl round these clients then train the

31
00:01:06,159 --> 00:01:08,799
model locally and compute a model update

32
00:01:08,799 --> 00:01:11,040
and then send this model update back to

33
00:01:11,040 --> 00:01:13,280
the server the server then

34
00:01:13,280 --> 00:01:15,280
aggregates all of these updates and

35
00:01:15,280 --> 00:01:17,759
updates the global model to be used for

36
00:01:17,759 --> 00:01:20,320
the following fl rounds

37
00:01:20,320 --> 00:01:22,159
so there are a lot of benefits of edited

38
00:01:22,159 --> 00:01:23,840
learning and consequently it has been

39
00:01:23,840 --> 00:01:26,560
adopted by many industrial leading

40
00:01:26,560 --> 00:01:28,560
industrial applications such as google's

41
00:01:28,560 --> 00:01:29,920
keyboard

42
00:01:29,920 --> 00:01:33,360
siri alexa etc

43
00:01:33,360 --> 00:01:35,119
so there are two major flavors of

44
00:01:35,119 --> 00:01:37,040
federated learning one is cross device

45
00:01:37,040 --> 00:01:39,680
fl and one is cross silo

46
00:01:39,680 --> 00:01:41,759
uh in cross device fl the number of

47
00:01:41,759 --> 00:01:44,240
clients are very large for it can be

48
00:01:44,240 --> 00:01:46,479
anywhere from thousands to billions and

49
00:01:46,479 --> 00:01:48,320
only a very small fraction of the

50
00:01:48,320 --> 00:01:51,280
clients participate in each fl round on

51
00:01:51,280 --> 00:01:53,520
the other end in cross silo fill

52
00:01:53,520 --> 00:01:55,439
the number of clients are rather small

53
00:01:55,439 --> 00:01:57,680
maybe in hundreds but all of them

54
00:01:57,680 --> 00:02:01,280
participate in each fl round

55
00:02:01,280 --> 00:02:03,040
so as we can see federated learning is a

56
00:02:03,040 --> 00:02:04,799
distributed system and therefore

57
00:02:04,799 --> 00:02:07,439
unfortunately it can have an adversary

58
00:02:07,439 --> 00:02:09,679
who compromises some of the clients or

59
00:02:09,679 --> 00:02:11,760
owns some of the clients

60
00:02:11,760 --> 00:02:13,040
in order to

61
00:02:13,040 --> 00:02:15,440
poison the model and reduces reduce the

62
00:02:15,440 --> 00:02:17,040
performance of the global model in

63
00:02:17,040 --> 00:02:18,640
certain fashion

64
00:02:18,640 --> 00:02:20,400
so these attacks that we call poisoning

65
00:02:20,400 --> 00:02:22,400
attacks can have severe consequences

66
00:02:22,400 --> 00:02:24,879
ranging from a simple network interest

67
00:02:24,879 --> 00:02:26,319
detection system

68
00:02:26,319 --> 00:02:28,959
misclassifying a malware as a benign

69
00:02:28,959 --> 00:02:31,680
software to an autonomous vehicle

70
00:02:31,680 --> 00:02:34,080
labeling a stop sign as a go sign which

71
00:02:34,080 --> 00:02:38,879
can lead to accidents and even worse

72
00:02:38,879 --> 00:02:40,480
so consequently there has been a

73
00:02:40,480 --> 00:02:42,080
significant literature dedicated to

74
00:02:42,080 --> 00:02:43,680
understanding poisoning attacks on

75
00:02:43,680 --> 00:02:46,560
federated learning from both attacks and

76
00:02:46,560 --> 00:02:48,239
defenses perspective

77
00:02:48,239 --> 00:02:50,000
there are a lot of attacks which are of

78
00:02:50,000 --> 00:02:52,080
various types and have made quite

79
00:02:52,080 --> 00:02:53,840
alarming claims about

80
00:02:53,840 --> 00:02:55,760
or against the robustness of related

81
00:02:55,760 --> 00:02:56,720
learning

82
00:02:56,720 --> 00:02:58,480
and also there are defenses which try to

83
00:02:58,480 --> 00:03:00,800
mitigate these attacks in different

84
00:03:00,800 --> 00:03:02,080
threat models

85
00:03:02,080 --> 00:03:04,239
and these defenses have various costs

86
00:03:04,239 --> 00:03:07,040
and guarantees

87
00:03:07,040 --> 00:03:09,200
but despite so much work uh from

88
00:03:09,200 --> 00:03:11,760
academia these uh leading

89
00:03:11,760 --> 00:03:13,519
applications of federated learning don't

90
00:03:13,519 --> 00:03:14,879
seem to worry too much about the

91
00:03:14,879 --> 00:03:17,360
poisoning attacks and uh because they

92
00:03:17,360 --> 00:03:18,959
have not

93
00:03:18,959 --> 00:03:21,200
implemented any of the differences from

94
00:03:21,200 --> 00:03:22,879
the literature in the referral

95
00:03:22,879 --> 00:03:25,360
applications so in this work we ask a

96
00:03:25,360 --> 00:03:27,680
key question that is poisoning really a

97
00:03:27,680 --> 00:03:29,840
threat to production fl

98
00:03:29,840 --> 00:03:32,720
so by products nfl we mean any federated

99
00:03:32,720 --> 00:03:35,040
learning algorithm that has been

100
00:03:35,040 --> 00:03:37,920
implemented in a practical real-world fl

101
00:03:37,920 --> 00:03:40,480
application

102
00:03:40,879 --> 00:03:42,799
so uh in order to answer this question

103
00:03:42,799 --> 00:03:45,120
for the talk i'll uh i'll follow this

104
00:03:45,120 --> 00:03:46,879
outline

105
00:03:46,879 --> 00:03:48,319
so as i mentioned before there has been

106
00:03:48,319 --> 00:03:51,280
a significant prior work on poisoning

107
00:03:51,280 --> 00:03:53,680
understanding pausing in fl so we first

108
00:03:53,680 --> 00:03:55,360
as a first step into our work we tried

109
00:03:55,360 --> 00:03:57,439
to systemize the poisoning

110
00:03:57,439 --> 00:04:00,159
threat models in federated learning in

111
00:04:00,159 --> 00:04:02,799
particular we identify uh three main

112
00:04:02,799 --> 00:04:04,720
dimensions of the threat model which are

113
00:04:04,720 --> 00:04:06,560
adversaries objective adversaries

114
00:04:06,560 --> 00:04:10,080
knowledge and their capability

115
00:04:10,080 --> 00:04:12,080
adversary's objective can either be

116
00:04:12,080 --> 00:04:14,480
targeted poisoning backdoor poisoning or

117
00:04:14,480 --> 00:04:16,320
untargeted poisoning and backdoor

118
00:04:16,320 --> 00:04:18,079
poisoning can be further divided into

119
00:04:18,079 --> 00:04:21,918
semantic or artificial bacters

120
00:04:21,918 --> 00:04:24,160
in terms of adversaries knowledge

121
00:04:24,160 --> 00:04:26,639
of the global model parameters we

122
00:04:26,639 --> 00:04:28,560
identify two types one is white box

123
00:04:28,560 --> 00:04:30,960
access or no box access in case of the

124
00:04:30,960 --> 00:04:33,280
white box access the adversary has the

125
00:04:33,280 --> 00:04:35,360
full knowledge of the model which

126
00:04:35,360 --> 00:04:37,759
includes its parameters architecture

127
00:04:37,759 --> 00:04:41,280
outputs in all of the fl rounds

128
00:04:41,280 --> 00:04:43,120
on the other hand in no box knowledge

129
00:04:43,120 --> 00:04:45,040
the adversary knows none of these things

130
00:04:45,040 --> 00:04:48,320
for any of the fl rounds

131
00:04:48,320 --> 00:04:49,840
in terms of adversary's knowledge of the

132
00:04:49,840 --> 00:04:52,240
benign data that is used to train their

133
00:04:52,240 --> 00:04:54,400
film model we identify either a full

134
00:04:54,400 --> 00:04:55,919
knowledge case or a partial knowledge

135
00:04:55,919 --> 00:04:58,160
case so in case of full knowledge

136
00:04:58,160 --> 00:04:59,759
adversity knows

137
00:04:59,759 --> 00:05:01,600
the data or benign data of all the

138
00:05:01,600 --> 00:05:03,120
clients in fl which is kind of

139
00:05:03,120 --> 00:05:04,960
impractical but it has been used

140
00:05:04,960 --> 00:05:07,199
previously or partial knowledge where

141
00:05:07,199 --> 00:05:09,919
adversely knows the benign data only of

142
00:05:09,919 --> 00:05:12,800
the compromised clients

143
00:05:12,800 --> 00:05:14,880
finally in terms of adversaries ability

144
00:05:14,880 --> 00:05:16,800
to access the compromised client devices

145
00:05:16,800 --> 00:05:19,039
we identify two types one is model

146
00:05:19,039 --> 00:05:21,360
poisoning another is data poisoning in

147
00:05:21,360 --> 00:05:23,520
case of model poisoning the adversary is

148
00:05:23,520 --> 00:05:25,919
smart technically savvy and therefore

149
00:05:25,919 --> 00:05:27,919
they can break into the device of

150
00:05:27,919 --> 00:05:30,160
compromised clients by circumventing the

151
00:05:30,160 --> 00:05:32,479
security protocols and access the model

152
00:05:32,479 --> 00:05:35,120
parameters and directly manipulate the

153
00:05:35,120 --> 00:05:36,880
poisson model updates

154
00:05:36,880 --> 00:05:39,039
in case of data poisoning however the

155
00:05:39,039 --> 00:05:40,240
adversary is

156
00:05:40,240 --> 00:05:42,479
rather naive so they cannot

157
00:05:42,479 --> 00:05:44,960
break into the device so only can

158
00:05:44,960 --> 00:05:46,479
indirectly manipulate these model

159
00:05:46,479 --> 00:05:48,639
updates via data poisoning

160
00:05:48,639 --> 00:05:51,120
so as we can imagine data poisoning is

161
00:05:51,120 --> 00:05:52,560
leads to rather

162
00:05:52,560 --> 00:05:53,520
naive

163
00:05:53,520 --> 00:05:56,160
less poisonous model updates but because

164
00:05:56,160 --> 00:05:57,919
it is a simple kind of poisoning the

165
00:05:57,919 --> 00:05:59,280
number of

166
00:05:59,280 --> 00:06:03,039
compromised clients can be very large

167
00:06:03,360 --> 00:06:05,280
so we have a lot of discussion

168
00:06:05,280 --> 00:06:07,520
on the systemization systemization in

169
00:06:07,520 --> 00:06:10,000
our main paper but for conciseness i'll

170
00:06:10,000 --> 00:06:11,520
move on to our

171
00:06:11,520 --> 00:06:13,840
next topic that is takeaways from our

172
00:06:13,840 --> 00:06:16,080
systemization so the first takeaway is

173
00:06:16,080 --> 00:06:18,400
about the gap that we observe in the

174
00:06:18,400 --> 00:06:21,680
theory and practice of fl poisoning

175
00:06:21,680 --> 00:06:24,240
again we observe a lot of gaps and have

176
00:06:24,240 --> 00:06:26,240
discussed them in the paper but

177
00:06:26,240 --> 00:06:27,759
due to time constraints i'll focus on

178
00:06:27,759 --> 00:06:29,759
two major issues that we think should be

179
00:06:29,759 --> 00:06:33,120
addressed as soon as possible

180
00:06:33,120 --> 00:06:34,639
so the first uh

181
00:06:34,639 --> 00:06:37,440
gap or first issue that we observe is

182
00:06:37,440 --> 00:06:39,199
the use of rather impractical threat

183
00:06:39,199 --> 00:06:41,680
models in prior works for example a

184
00:06:41,680 --> 00:06:43,759
common threat model used in both attacks

185
00:06:43,759 --> 00:06:46,000
and defenses work is model poisoning

186
00:06:46,000 --> 00:06:47,600
attacks and cross-silo federated

187
00:06:47,600 --> 00:06:48,560
learning

188
00:06:48,560 --> 00:06:50,720
so why is this an impractical threat

189
00:06:50,720 --> 00:06:53,120
model because silos are generally very

190
00:06:53,120 --> 00:06:55,280
large organizations like banks or

191
00:06:55,280 --> 00:06:58,000
hospitals or universities who have their

192
00:06:58,000 --> 00:06:59,919
software security software's

193
00:06:59,919 --> 00:07:02,240
professionally maintained therefore even

194
00:07:02,240 --> 00:07:03,840
if an adversary is assumed to be very

195
00:07:03,840 --> 00:07:05,599
technically savvy they cannot just

196
00:07:05,599 --> 00:07:08,080
easily break into the devices of these

197
00:07:08,080 --> 00:07:10,560
silos that host the global models in

198
00:07:10,560 --> 00:07:13,039
order to poison or in order to mount a

199
00:07:13,039 --> 00:07:15,599
model poisoning attack

200
00:07:15,599 --> 00:07:17,360
instead we believe a more practical

201
00:07:17,360 --> 00:07:19,360
threat model would be data poisoning in

202
00:07:19,360 --> 00:07:22,080
cross sell fl where the adversary

203
00:07:22,080 --> 00:07:23,919
compromises some of the users who

204
00:07:23,919 --> 00:07:27,039
contribute their data to a silo so the

205
00:07:27,039 --> 00:07:29,280
adversary can manipulate the data of

206
00:07:29,280 --> 00:07:31,840
these users and then share it with the

207
00:07:31,840 --> 00:07:34,560
silo to compromise it

208
00:07:34,560 --> 00:07:37,120
another major issue is the use of very

209
00:07:37,120 --> 00:07:39,039
large percentages of compromised clients

210
00:07:39,039 --> 00:07:41,440
in prior works anywhere between five to

211
00:07:41,440 --> 00:07:43,280
25 percentages

212
00:07:43,280 --> 00:07:46,240
so if we assume uh in a typical cross

213
00:07:46,240 --> 00:07:48,240
device as a federated learning

214
00:07:48,240 --> 00:07:50,400
application like google g port which has

215
00:07:50,400 --> 00:07:52,479
say one billion clients

216
00:07:52,479 --> 00:07:55,120
uh and if the cost of compromising a

217
00:07:55,120 --> 00:07:56,720
client is assumed to be around ten

218
00:07:56,720 --> 00:07:59,440
dollars then just to compromise five

219
00:07:59,440 --> 00:08:01,120
percent of the clients the cost of an

220
00:08:01,120 --> 00:08:03,440
attack would be 500 million dollars

221
00:08:03,440 --> 00:08:05,759
which is quite impractical in most of

222
00:08:05,759 --> 00:08:06,639
the

223
00:08:06,639 --> 00:08:09,280
real world settings

224
00:08:09,280 --> 00:08:10,720
the second major takeaway from our

225
00:08:10,720 --> 00:08:13,280
systemization is that untargeted attacks

226
00:08:13,280 --> 00:08:16,160
are we believe the most severe threat to

227
00:08:16,160 --> 00:08:17,680
production fl

228
00:08:17,680 --> 00:08:19,759
so first of all untargeted attacks aim

229
00:08:19,759 --> 00:08:22,960
to misclassify most or all of the

230
00:08:22,960 --> 00:08:25,199
inputs to the global model at test or

231
00:08:25,199 --> 00:08:26,800
inference time

232
00:08:26,800 --> 00:08:28,720
so why is this the most severe threat

233
00:08:28,720 --> 00:08:30,000
model

234
00:08:30,000 --> 00:08:31,840
so first of all untargeted poisoning

235
00:08:31,840 --> 00:08:34,799
aims to impact all or most of the users

236
00:08:34,799 --> 00:08:37,039
in federated learning as compared to

237
00:08:37,039 --> 00:08:39,279
targeted or backdoor attacks and

238
00:08:39,279 --> 00:08:41,360
therefore they can harm the experience

239
00:08:41,360 --> 00:08:42,880
of the federated learning application

240
00:08:42,880 --> 00:08:44,159
for many more

241
00:08:44,159 --> 00:08:46,640
users

242
00:08:46,880 --> 00:08:49,040
the second issue or second reason is

243
00:08:49,040 --> 00:08:51,120
that untargeted poisoning is quite

244
00:08:51,120 --> 00:08:53,200
difficult to detect in practice due to

245
00:08:53,200 --> 00:08:55,200
the absence of benchmarks so what we

246
00:08:55,200 --> 00:08:57,760
mean by that is consider an untargeted

247
00:08:57,760 --> 00:09:00,560
attack that reduces the accuracy of

248
00:09:00,560 --> 00:09:04,080
the global model by just few percentages

249
00:09:04,080 --> 00:09:05,839
so this can go unnoticed by the server

250
00:09:05,839 --> 00:09:07,600
because there are no benchmarks

251
00:09:07,600 --> 00:09:09,600
but this small reduction in accuracy can

252
00:09:09,600 --> 00:09:12,080
have arbitrary impact on an arbitrary

253
00:09:12,080 --> 00:09:14,880
state of users in federated learning

254
00:09:14,880 --> 00:09:18,320
which again can harm their experience

255
00:09:18,320 --> 00:09:20,240
so therefore in this work we

256
00:09:20,240 --> 00:09:22,880
primarily focus on untargeted attacks

257
00:09:22,880 --> 00:09:24,480
and their evaluation on federated

258
00:09:24,480 --> 00:09:26,800
learning under practical threat models

259
00:09:26,800 --> 00:09:28,720
which are guided by our systemization

260
00:09:28,720 --> 00:09:30,320
from the first part

261
00:09:30,320 --> 00:09:32,160
but instead of just simply evaluating

262
00:09:32,160 --> 00:09:34,160
the existing attacks we improve the

263
00:09:34,160 --> 00:09:36,320
existing attacks or provide improved

264
00:09:36,320 --> 00:09:38,000
attacks that

265
00:09:38,000 --> 00:09:41,279
that i will talk about now

266
00:09:41,360 --> 00:09:43,200
so the intuition behind our attacks is

267
00:09:43,200 --> 00:09:44,480
to find

268
00:09:44,480 --> 00:09:45,760
a set of

269
00:09:45,760 --> 00:09:47,839
malicious updates in the space of benign

270
00:09:47,839 --> 00:09:50,320
updates that maximize the distances

271
00:09:50,320 --> 00:09:52,720
between the benign and the malicious

272
00:09:52,720 --> 00:09:53,839
aggregates

273
00:09:53,839 --> 00:09:55,600
now the space of benign updates is

274
00:09:55,600 --> 00:09:57,839
decided by the target aggregation rule

275
00:09:57,839 --> 00:10:01,440
or defense that we want to break

276
00:10:01,519 --> 00:10:03,040
based on this intuition we provide a

277
00:10:03,040 --> 00:10:04,800
general optimization problem in the

278
00:10:04,800 --> 00:10:06,480
paper where we compute the benign

279
00:10:06,480 --> 00:10:08,720
aggregate using the benign data that the

280
00:10:08,720 --> 00:10:11,040
adversary has for example in partial or

281
00:10:11,040 --> 00:10:13,279
full knowledge case and then we find a

282
00:10:13,279 --> 00:10:15,279
poisoned update set of poison updates

283
00:10:15,279 --> 00:10:16,959
that maximize this distance between

284
00:10:16,959 --> 00:10:20,560
poison and benign aggregates

285
00:10:20,720 --> 00:10:22,320
so i'll not go into the details of our

286
00:10:22,320 --> 00:10:24,480
attacks but give an intuition again

287
00:10:24,480 --> 00:10:26,079
so in our data poisoning attack we

288
00:10:26,079 --> 00:10:28,720
exploit the fact that server has no

289
00:10:28,720 --> 00:10:31,360
visibility into the data on compromised

290
00:10:31,360 --> 00:10:33,680
client devices and in particular they

291
00:10:33,680 --> 00:10:35,760
don't have any visibility into the size

292
00:10:35,760 --> 00:10:37,200
of the data

293
00:10:37,200 --> 00:10:39,600
what we observe is the more mislabeled

294
00:10:39,600 --> 00:10:41,839
data that we use to compute these

295
00:10:41,839 --> 00:10:43,920
poisoned updates the more poisonous or

296
00:10:43,920 --> 00:10:46,480
more impactful these updates become

297
00:10:46,480 --> 00:10:48,000
and in order to

298
00:10:48,000 --> 00:10:50,160
circumvent the target aggregation rule

299
00:10:50,160 --> 00:10:52,720
or target defense we just need to adjust

300
00:10:52,720 --> 00:10:56,000
the size of mislabeled data

301
00:10:56,000 --> 00:10:57,440
in our model poisoning attacks we

302
00:10:57,440 --> 00:10:59,519
exploit the direct access that adversary

303
00:10:59,519 --> 00:11:01,040
has to the model parameters and

304
00:11:01,040 --> 00:11:03,440
therefore use gradient ascent to

305
00:11:03,440 --> 00:11:05,519
increase the loss of the final model

306
00:11:05,519 --> 00:11:07,680
update on some benign data

307
00:11:07,680 --> 00:11:10,240
and then finally we scale this update in

308
00:11:10,240 --> 00:11:11,680
order to circumvent the given

309
00:11:11,680 --> 00:11:13,760
aggregation rule or the target defense

310
00:11:13,760 --> 00:11:16,640
that we want to break

311
00:11:17,600 --> 00:11:20,320
now i'll uh move on to discuss some of

312
00:11:20,320 --> 00:11:23,040
the key results of our evaluations of

313
00:11:23,040 --> 00:11:25,200
the robustness of edited learning under

314
00:11:25,200 --> 00:11:26,800
the more practical threat models which

315
00:11:26,800 --> 00:11:28,720
are again guided by our systemization

316
00:11:28,720 --> 00:11:31,600
from the first part

317
00:11:31,920 --> 00:11:34,160
so first we evaluate non-robust

318
00:11:34,160 --> 00:11:36,240
federated learning non-robust cross

319
00:11:36,240 --> 00:11:37,680
device federated learning with an

320
00:11:37,680 --> 00:11:39,760
average aggregation rule

321
00:11:39,760 --> 00:11:42,160
so in production across device fl with

322
00:11:42,160 --> 00:11:44,480
billions of devices we believe that the

323
00:11:44,480 --> 00:11:46,560
percentages or we argue that but the

324
00:11:46,560 --> 00:11:49,200
percentages of compromised clients are

325
00:11:49,200 --> 00:11:52,240
about less than 0.01 for the model

326
00:11:52,240 --> 00:11:54,240
poisoning attacks while they are less

327
00:11:54,240 --> 00:11:56,959
than about 0.1 percent for the data

328
00:11:56,959 --> 00:11:59,760
poisoning attacks uh so again due to

329
00:11:59,760 --> 00:12:01,680
time constraints i will not discuss why

330
00:12:01,680 --> 00:12:04,399
are these numbers um

331
00:12:04,399 --> 00:12:06,399
should be what they are but in the paper

332
00:12:06,399 --> 00:12:08,800
we have an extensive discussion on this

333
00:12:08,800 --> 00:12:11,600
so we use an attack impact metric which

334
00:12:11,600 --> 00:12:13,440
to assess the

335
00:12:13,440 --> 00:12:15,040
impact of our attacks

336
00:12:15,040 --> 00:12:16,880
which measures the reduction in the

337
00:12:16,880 --> 00:12:19,200
global model's accuracy due to an attack

338
00:12:19,200 --> 00:12:21,200
as compared to the benign setting

339
00:12:21,200 --> 00:12:23,120
without any adversary

340
00:12:23,120 --> 00:12:24,480
so we observe that with these

341
00:12:24,480 --> 00:12:26,959
percentages of compromised clients even

342
00:12:26,959 --> 00:12:28,399
the non-robust credited learning

343
00:12:28,399 --> 00:12:30,639
achieves very high accuracies which is

344
00:12:30,639 --> 00:12:32,560
in stark contrast to the prior belief

345
00:12:32,560 --> 00:12:33,839
that

346
00:12:33,839 --> 00:12:35,839
the non-robust federated learning with

347
00:12:35,839 --> 00:12:38,560
average aggregation rule can be broken

348
00:12:38,560 --> 00:12:41,680
even by a single compromise client

349
00:12:41,680 --> 00:12:43,440
so we believe that this intrinsic

350
00:12:43,440 --> 00:12:46,560
robustness of cross device fl is because

351
00:12:46,560 --> 00:12:48,160
with these small percentages are

352
00:12:48,160 --> 00:12:50,240
practical percentages of compromised

353
00:12:50,240 --> 00:12:52,639
clients in most of the fl rounds only

354
00:12:52,639 --> 00:12:54,639
benign clients participate and therefore

355
00:12:54,639 --> 00:12:56,800
they can easily cancel any negative

356
00:12:56,800 --> 00:12:59,200
impact that the compromised clients have

357
00:12:59,200 --> 00:13:02,560
had on the global model

358
00:13:02,560 --> 00:13:04,800
we observe we make a similar observation

359
00:13:04,800 --> 00:13:07,279
for cross-silo federated learning with

360
00:13:07,279 --> 00:13:10,800
non-robust average aggregation rule

361
00:13:10,800 --> 00:13:12,800
when we evaluate it using our

362
00:13:12,800 --> 00:13:14,800
state-of-the-art data poisoning attacks

363
00:13:14,800 --> 00:13:16,959
in fact even at a very high percentages

364
00:13:16,959 --> 00:13:18,399
of compromised clients there is

365
00:13:18,399 --> 00:13:20,800
absolutely no impact on the accuracy of

366
00:13:20,800 --> 00:13:22,720
cross style of edited learning

367
00:13:22,720 --> 00:13:25,279
so as i discussed before uh the model

368
00:13:25,279 --> 00:13:27,279
poisoning attacks are not really

369
00:13:27,279 --> 00:13:30,240
practical in cross silo fl

370
00:13:30,240 --> 00:13:32,240
so again the this this intrinsic

371
00:13:32,240 --> 00:13:34,639
robustness is because of batching and

372
00:13:34,639 --> 00:13:37,519
the benign data that these cross silos

373
00:13:37,519 --> 00:13:39,680
have can easily cancel the negative

374
00:13:39,680 --> 00:13:41,680
impact of the poison data on the global

375
00:13:41,680 --> 00:13:44,000
model

376
00:13:44,480 --> 00:13:46,560
next we evaluate cross device federated

377
00:13:46,560 --> 00:13:49,120
learning with robust aggregation rules

378
00:13:49,120 --> 00:13:51,440
and we observe that none of the attacks

379
00:13:51,440 --> 00:13:53,199
data or model poisoning have any

380
00:13:53,199 --> 00:13:55,440
noticeable impact on the cross device

381
00:13:55,440 --> 00:13:58,240
federated landing with defenses such as

382
00:13:58,240 --> 00:14:00,399
non-bounding or multi-chrome or crimp

383
00:14:00,399 --> 00:14:02,560
beam even when the percentages of

384
00:14:02,560 --> 00:14:04,399
compromised clients are as high as one

385
00:14:04,399 --> 00:14:05,360
percent

386
00:14:05,360 --> 00:14:06,240
again

387
00:14:06,240 --> 00:14:08,560
the the low attack impact is because in

388
00:14:08,560 --> 00:14:10,240
most of the fell rounds only benign

389
00:14:10,240 --> 00:14:12,399
clients participate and now on top of it

390
00:14:12,399 --> 00:14:14,639
we have these defenses which try to

391
00:14:14,639 --> 00:14:18,720
mitigate the impact of poisoned updates

392
00:14:18,800 --> 00:14:21,120
moreover the robustness of these

393
00:14:21,120 --> 00:14:22,880
robustness of federated learning with

394
00:14:22,880 --> 00:14:25,040
robust aggregation rules or defenses

395
00:14:25,040 --> 00:14:27,199
persists even when we run federated

396
00:14:27,199 --> 00:14:29,360
learning for a very long number a very

397
00:14:29,360 --> 00:14:31,040
large number of

398
00:14:31,040 --> 00:14:33,519
rounds in the presence of adversary this

399
00:14:33,519 --> 00:14:35,120
is very important thing because

400
00:14:35,120 --> 00:14:36,639
federated learning is a continuous

401
00:14:36,639 --> 00:14:38,399
learning process it doesn't stop it

402
00:14:38,399 --> 00:14:40,079
keeps learning with

403
00:14:40,079 --> 00:14:42,160
new data and new clients as they join in

404
00:14:42,160 --> 00:14:45,839
a federation learning system

405
00:14:47,920 --> 00:14:49,920
finally as i noted before there are

406
00:14:49,920 --> 00:14:52,399
numerous differences that exist

407
00:14:52,399 --> 00:14:54,639
against federated learning poisoning but

408
00:14:54,639 --> 00:14:56,480
very interestingly what we observe in

409
00:14:56,480 --> 00:14:58,880
our evaluations is that the simple and

410
00:14:58,880 --> 00:15:00,160
low-cost

411
00:15:00,160 --> 00:15:02,240
defenses like bounding the norms of the

412
00:15:02,240 --> 00:15:04,720
updates or bounding the local data set

413
00:15:04,720 --> 00:15:06,639
sizes of clients are sufficient to

414
00:15:06,639 --> 00:15:08,720
protect federated learning against

415
00:15:08,720 --> 00:15:12,480
untargeted poisoning attacks in practice

416
00:15:12,480 --> 00:15:14,720
so this observation questions whether we

417
00:15:14,720 --> 00:15:15,920
really need

418
00:15:15,920 --> 00:15:17,920
the costly and more difficult to

419
00:15:17,920 --> 00:15:20,399
implement our defenses in practice to

420
00:15:20,399 --> 00:15:23,040
protect federal learning and if we need

421
00:15:23,040 --> 00:15:25,040
them then we need to be more specific

422
00:15:25,040 --> 00:15:26,800
about the threat models that we want

423
00:15:26,800 --> 00:15:29,199
them in

424
00:15:29,360 --> 00:15:31,759
so to summarize in this work our goal

425
00:15:31,759 --> 00:15:33,360
was to investigate whether poisoning

426
00:15:33,360 --> 00:15:34,880
attacks are really a threat to

427
00:15:34,880 --> 00:15:37,199
production fl

428
00:15:37,199 --> 00:15:38,800
in order to answer this we first

429
00:15:38,800 --> 00:15:40,800
systemized the threat models of

430
00:15:40,800 --> 00:15:42,959
federated learning poisoning attacks and

431
00:15:42,959 --> 00:15:44,959
we designed improved model and data

432
00:15:44,959 --> 00:15:46,560
poisoning attacks against federated

433
00:15:46,560 --> 00:15:49,120
learning and then we evaluated our

434
00:15:49,120 --> 00:15:51,120
improved and existing poisoning attacks

435
00:15:51,120 --> 00:15:53,600
on federated learning under practical

436
00:15:53,600 --> 00:15:55,759
threat models that are guided by our

437
00:15:55,759 --> 00:15:59,040
systemization from the first part

438
00:15:59,040 --> 00:16:00,959
and couple of key takeaways from my talk

439
00:16:00,959 --> 00:16:03,040
are federated learning we believe is

440
00:16:03,040 --> 00:16:05,120
much more robust to poisoning attacks

441
00:16:05,120 --> 00:16:06,079
than

442
00:16:06,079 --> 00:16:09,040
believed previously and the simple and

443
00:16:09,040 --> 00:16:10,639
low cost differences such as bounding

444
00:16:10,639 --> 00:16:12,560
the norms or bounding the local data set

445
00:16:12,560 --> 00:16:14,560
sizes are

446
00:16:14,560 --> 00:16:16,560
enough in practice to protect federated

447
00:16:16,560 --> 00:16:19,440
learning in most of the settings

448
00:16:19,440 --> 00:16:21,600
up thank you that's that's all and i'm

449
00:16:21,600 --> 00:16:23,420
happy to take any questions

450
00:16:23,420 --> 00:16:27,899
[Applause]

451
00:16:28,240 --> 00:16:29,519
thank you

452
00:16:29,519 --> 00:16:33,639
questions please sub to the microphone

453
00:16:40,959 --> 00:16:42,880
so my question is do you assume that the

454
00:16:42,880 --> 00:16:44,800
clients are iids

455
00:16:44,800 --> 00:16:47,759
what what do you have the same insights

456
00:16:47,759 --> 00:16:49,199
if you have

457
00:16:49,199 --> 00:16:51,519
personalized federated learning center

458
00:16:51,519 --> 00:16:52,560
setup

459
00:16:52,560 --> 00:16:54,399
no the we assume that it's a

460
00:16:54,399 --> 00:16:56,560
heterogeneous environment uh like the

461
00:16:56,560 --> 00:16:58,959
clients are not iid their data sets are

462
00:16:58,959 --> 00:17:01,360
not iid distributed they are pretty

463
00:17:01,360 --> 00:17:02,880
heterogeneous

464
00:17:02,880 --> 00:17:04,720
but then how do you

465
00:17:04,720 --> 00:17:06,319
measure performance it is the

466
00:17:06,319 --> 00:17:08,240
performance at the server or maybe the

467
00:17:08,240 --> 00:17:09,839
performance at the end declines maybe

468
00:17:09,839 --> 00:17:12,400
it's good for one client oh yeah in this

469
00:17:12,400 --> 00:17:14,160
uh oh that's a good question so in this

470
00:17:14,160 --> 00:17:15,839
case we measure the performance of the

471
00:17:15,839 --> 00:17:17,839
global model and some held out data

472
00:17:17,839 --> 00:17:19,839
because that is how the performance is

473
00:17:19,839 --> 00:17:23,839
generally measured on the server side

474
00:17:26,720 --> 00:17:28,559
hi uh georgios very northeastern

475
00:17:28,559 --> 00:17:30,960
university a very interesting talk

476
00:17:30,960 --> 00:17:33,120
i have a question could you please

477
00:17:33,120 --> 00:17:35,799
elaborate a little more on why

478
00:17:35,799 --> 00:17:38,880
untargeted attacks are considered more

479
00:17:38,880 --> 00:17:41,600
uh impactful because it seems a little

480
00:17:41,600 --> 00:17:43,919
counterintuitive uh that

481
00:17:43,919 --> 00:17:46,160
the untargeted version of the attack

482
00:17:46,160 --> 00:17:47,280
would be

483
00:17:47,280 --> 00:17:48,480
more relevant right because you're

484
00:17:48,480 --> 00:17:51,440
reducing the global model accuracy by a

485
00:17:51,440 --> 00:17:52,799
little bit

486
00:17:52,799 --> 00:17:55,440
um for all clients instead of reducing

487
00:17:55,440 --> 00:17:56,880
it by a lot for

488
00:17:56,880 --> 00:17:59,280
fewer clients so why would you consider

489
00:17:59,280 --> 00:18:02,240
that more relevant yeah the main reason

490
00:18:02,240 --> 00:18:04,640
we believe that uh and targeted attacks

491
00:18:04,640 --> 00:18:06,160
they want to impact a very large

492
00:18:06,160 --> 00:18:08,160
population and an arbitrary set of

493
00:18:08,160 --> 00:18:10,480
clients can uh get affected

494
00:18:10,480 --> 00:18:13,360
and uh even if the accuracy reduction in

495
00:18:13,360 --> 00:18:15,679
the global model is not a lot it can

496
00:18:15,679 --> 00:18:18,000
still impact a very large population of

497
00:18:18,000 --> 00:18:20,400
the clients and the clients which are on

498
00:18:20,400 --> 00:18:21,919
the head of the distributions like the

499
00:18:21,919 --> 00:18:24,000
rest of the attacks like back doors are

500
00:18:24,000 --> 00:18:26,320
targeted they are generally effective on

501
00:18:26,320 --> 00:18:28,480
the tail of the distribution

502
00:18:28,480 --> 00:18:30,799
which are very small population

503
00:18:30,799 --> 00:18:32,480
but the untargeted attacks they can be

504
00:18:32,480 --> 00:18:33,840
impactful even at the head of the

505
00:18:33,840 --> 00:18:36,240
distribution and therefore we think that

506
00:18:36,240 --> 00:18:38,160
and head of the distribution is the crux

507
00:18:38,160 --> 00:18:40,480
that's the majority of the population i

508
00:18:40,480 --> 00:18:42,960
see a quick follow-up but for instance

509
00:18:42,960 --> 00:18:46,160
if uh let's say i am in the head of the

510
00:18:46,160 --> 00:18:48,000
distribution and my phone

511
00:18:48,000 --> 00:18:50,559
accuracy um prediction on the next word

512
00:18:50,559 --> 00:18:53,280
is dropped by five percent if it was

513
00:18:53,280 --> 00:18:54,880
close to 100

514
00:18:54,880 --> 00:18:57,520
would i really be impacted that much by

515
00:18:57,520 --> 00:18:58,320
this

516
00:18:58,320 --> 00:19:00,559
small five percent drop

517
00:19:00,559 --> 00:19:03,280
yeah i i think yeah that's that's a good

518
00:19:03,280 --> 00:19:06,080
question in in theory or in like

519
00:19:06,080 --> 00:19:08,880
empirical experiments i think it may not

520
00:19:08,880 --> 00:19:11,120
seem like much but in production fl when

521
00:19:11,120 --> 00:19:12,960
we have like a very large number of

522
00:19:12,960 --> 00:19:14,720
clients and these companies are uh

523
00:19:14,720 --> 00:19:16,480
fighting for one percent accuracy as

524
00:19:16,480 --> 00:19:18,240
well like so it's it's a big deal that

525
00:19:18,240 --> 00:19:20,480
even a small reduction in accuracy can

526
00:19:20,480 --> 00:19:23,039
have large impact for for the product

527
00:19:23,039 --> 00:19:24,960
again thank you very much

528
00:19:24,960 --> 00:19:26,240
unfortunately it's all the time we have

529
00:19:26,240 --> 00:19:27,679
questions for this paper but please feel

530
00:19:27,679 --> 00:19:29,600
free um to ask the speaker let's thank

531
00:19:29,600 --> 00:19:33,240
the speaker one more time

