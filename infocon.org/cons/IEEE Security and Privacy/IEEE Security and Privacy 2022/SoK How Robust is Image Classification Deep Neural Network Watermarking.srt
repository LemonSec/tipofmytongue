1
00:00:01,520 --> 00:00:03,679
nice yes hello welcome to my talk my

2
00:00:03,679 --> 00:00:05,200
name is lucas and it's not

3
00:00:05,200 --> 00:00:07,440
fingerprinting but watermarking

4
00:00:07,440 --> 00:00:09,519
um and this was work done at the

5
00:00:09,519 --> 00:00:12,639
university of waterloo with edward jung

6
00:00:12,639 --> 00:00:14,480
schindler lee and supervised by florian

7
00:00:14,480 --> 00:00:15,759
kirschbaum

8
00:00:15,759 --> 00:00:17,520
so what is deep neural network

9
00:00:17,520 --> 00:00:20,000
watermarking imagine you're a company

10
00:00:20,000 --> 00:00:22,160
and you have training data and you want

11
00:00:22,160 --> 00:00:24,880
to train a model on this training data

12
00:00:24,880 --> 00:00:27,840
and this is a process that outputs a

13
00:00:27,840 --> 00:00:30,000
model and then you want to share this

14
00:00:30,000 --> 00:00:31,920
model publicly so you want to upload it

15
00:00:31,920 --> 00:00:33,840
for example on model hub

16
00:00:33,840 --> 00:00:35,920
now you don't want to upload it just as

17
00:00:35,920 --> 00:00:37,920
it is but you want to upload it with a

18
00:00:37,920 --> 00:00:40,559
usage agreement for example you will

19
00:00:40,559 --> 00:00:43,120
allow academic purposes but you will not

20
00:00:43,120 --> 00:00:45,920
allow model misuse and model misuse in

21
00:00:45,920 --> 00:00:48,320
this case can be have quite a lot of

22
00:00:48,320 --> 00:00:50,480
meanings for example one of the meanings

23
00:00:50,480 --> 00:00:52,879
that we investigate is commercialization

24
00:00:52,879 --> 00:00:54,559
where you won't allow your model to be

25
00:00:54,559 --> 00:00:57,440
commercialized but it can also be other

26
00:00:57,440 --> 00:00:59,440
cases of model misuse for example what

27
00:00:59,440 --> 00:01:00,960
we just heard

28
00:01:00,960 --> 00:01:04,000
misuse with language models one possible

29
00:01:04,000 --> 00:01:06,720
defense would be to watermark the model

30
00:01:06,720 --> 00:01:09,040
and make it detectable that oh you used

31
00:01:09,040 --> 00:01:12,560
our model you can't do that and this is

32
00:01:12,560 --> 00:01:14,960
a faulty service

33
00:01:14,960 --> 00:01:17,600
so if we have an adversary that

34
00:01:17,600 --> 00:01:19,759
downloads the model from model hub and

35
00:01:19,759 --> 00:01:22,720
then deploys it publicly the question is

36
00:01:22,720 --> 00:01:25,840
how can we tell that they used our model

37
00:01:25,840 --> 00:01:27,680
and that is the question is called

38
00:01:27,680 --> 00:01:29,840
provenance verification

39
00:01:29,840 --> 00:01:31,439
this is the question that watermarking

40
00:01:31,439 --> 00:01:34,880
seeks to answer so in watermarking it's

41
00:01:34,880 --> 00:01:37,119
an addition to the training process

42
00:01:37,119 --> 00:01:40,000
where we embed a message into the

43
00:01:40,000 --> 00:01:41,520
trained

44
00:01:41,520 --> 00:01:42,479
model

45
00:01:42,479 --> 00:01:45,680
that makes it recognizable in the other

46
00:01:45,680 --> 00:01:48,320
deployment from the adversary

47
00:01:48,320 --> 00:01:51,920
so our work studies how reliable water

48
00:01:51,920 --> 00:01:53,759
marking is in practice

49
00:01:53,759 --> 00:01:55,759
so we have five contributions of our

50
00:01:55,759 --> 00:01:57,920
paper that i'm going to talk about so

51
00:01:57,920 --> 00:01:59,040
first we

52
00:01:59,040 --> 00:02:01,600
talk about the taxonomy of watermarking

53
00:02:01,600 --> 00:02:03,520
so we looked at a lot of schemes that we

54
00:02:03,520 --> 00:02:05,360
found in

55
00:02:05,360 --> 00:02:07,840
research that we found published and we

56
00:02:07,840 --> 00:02:09,598
categorized them in different into

57
00:02:09,598 --> 00:02:11,280
different categories

58
00:02:11,280 --> 00:02:13,680
then we look at novel adaptive attacks

59
00:02:13,680 --> 00:02:15,920
against a subset of these watermarks

60
00:02:15,920 --> 00:02:18,319
and we have a large empirical study

61
00:02:18,319 --> 00:02:20,319
where we pit a lot of attacks against a

62
00:02:20,319 --> 00:02:23,520
lot of defenses on multiple data sets

63
00:02:23,520 --> 00:02:25,280
and we drive at

64
00:02:25,280 --> 00:02:28,800
guidelines on how robustness

65
00:02:28,800 --> 00:02:31,680
evaluation can be improved in practice

66
00:02:31,680 --> 00:02:33,440
and finally all of our source code is

67
00:02:33,440 --> 00:02:36,319
open source on github and i will provide

68
00:02:36,319 --> 00:02:38,879
a link at the very end

69
00:02:38,879 --> 00:02:41,360
so first what is watermarking so

70
00:02:41,360 --> 00:02:43,360
watermarking can be defined by two

71
00:02:43,360 --> 00:02:45,599
functions namely the embedding and the

72
00:02:45,599 --> 00:02:47,680
extraction function so during the

73
00:02:47,680 --> 00:02:50,239
embedding you have a source model a

74
00:02:50,239 --> 00:02:52,319
message and a key the key can be

75
00:02:52,319 --> 00:02:53,840
anything can be a vector it can be

76
00:02:53,840 --> 00:02:55,360
images whatever

77
00:02:55,360 --> 00:02:56,879
and that is the input to the embedding

78
00:02:56,879 --> 00:02:58,959
function which then outputs a marked

79
00:02:58,959 --> 00:02:59,840
model

80
00:02:59,840 --> 00:03:02,159
so this marked model then contains a

81
00:03:02,159 --> 00:03:04,319
message that can be extracted via the

82
00:03:04,319 --> 00:03:05,920
extraction function

83
00:03:05,920 --> 00:03:08,000
so in the extraction function it takes

84
00:03:08,000 --> 00:03:09,680
as an input the marked model and the

85
00:03:09,680 --> 00:03:12,480
watermarking key and it outputs the

86
00:03:12,480 --> 00:03:13,840
message

87
00:03:13,840 --> 00:03:15,680
and there's a simple verification

88
00:03:15,680 --> 00:03:17,920
process where we speak of the watermark

89
00:03:17,920 --> 00:03:22,000
accuracy which verifies if our extracted

90
00:03:22,000 --> 00:03:22,959
message

91
00:03:22,959 --> 00:03:25,760
and our embedded message if they match

92
00:03:25,760 --> 00:03:28,000
if they match up to a certain degree we

93
00:03:28,000 --> 00:03:29,760
say that this model was stolen and this

94
00:03:29,760 --> 00:03:32,879
was our model and check if it was maybe

95
00:03:32,879 --> 00:03:34,720
misused

96
00:03:34,720 --> 00:03:37,440
okay so we looked at a lot of schemes as

97
00:03:37,440 --> 00:03:40,159
i said and we derived a taxonomy for

98
00:03:40,159 --> 00:03:42,640
them so we have four we have found four

99
00:03:42,640 --> 00:03:43,920
categories

100
00:03:43,920 --> 00:03:45,280
so one is

101
00:03:45,280 --> 00:03:47,840
one type of watermarking schemes we call

102
00:03:47,840 --> 00:03:49,920
white box watermarking schemes

103
00:03:49,920 --> 00:03:52,959
here the defender has access to the

104
00:03:52,959 --> 00:03:55,599
weights during verification so they take

105
00:03:55,599 --> 00:03:57,200
the watermarking key

106
00:03:57,200 --> 00:04:00,159
they take the what the model's weights

107
00:04:00,159 --> 00:04:02,000
and then from that they derive the

108
00:04:02,000 --> 00:04:03,040
message

109
00:04:03,040 --> 00:04:05,120
the other three watermarking schemes are

110
00:04:05,120 --> 00:04:07,439
so called black box watermarking schemes

111
00:04:07,439 --> 00:04:10,480
where the defender to verify only has

112
00:04:10,480 --> 00:04:12,879
access to the images

113
00:04:12,879 --> 00:04:15,360
and the model as a service so they would

114
00:04:15,360 --> 00:04:18,000
send an image to the model and get the

115
00:04:18,000 --> 00:04:19,759
classification of that image back but

116
00:04:19,759 --> 00:04:21,040
nothing more

117
00:04:21,040 --> 00:04:22,800
so the first of these black box water

118
00:04:22,800 --> 00:04:24,880
marking schemes we call model

119
00:04:24,880 --> 00:04:27,199
independent water marking so here the

120
00:04:27,199 --> 00:04:29,040
gist this is like one very simplified

121
00:04:29,040 --> 00:04:31,280
scheme is we have a watermarking key

122
00:04:31,280 --> 00:04:33,360
which is just some images randomly

123
00:04:33,360 --> 00:04:35,520
sampled let's say from the internet some

124
00:04:35,520 --> 00:04:37,840
randomly sample target labels and we

125
00:04:37,840 --> 00:04:40,080
teach the model we backdoor the model

126
00:04:40,080 --> 00:04:42,639
essentially to predict a certain label

127
00:04:42,639 --> 00:04:45,120
for each image if the stolen model

128
00:04:45,120 --> 00:04:47,360
predicts the same label we interpret

129
00:04:47,360 --> 00:04:50,080
that as a match and we decode that as a

130
00:04:50,080 --> 00:04:51,919
one in the message

131
00:04:51,919 --> 00:04:53,360
then there's model dependent

132
00:04:53,360 --> 00:04:54,800
watermarking

133
00:04:54,800 --> 00:04:56,960
that mostly most of the schemes that we

134
00:04:56,960 --> 00:04:59,600
looked at depend on adversarial examples

135
00:04:59,600 --> 00:05:02,000
here the defender would adversarially

136
00:05:02,000 --> 00:05:04,320
train the source model and then we would

137
00:05:04,320 --> 00:05:06,560
check if the deployed model is

138
00:05:06,560 --> 00:05:08,400
vulnerable is vulnerable to the

139
00:05:08,400 --> 00:05:10,880
adversarial attack or not if the

140
00:05:10,880 --> 00:05:13,120
deployed model is not vulnerable to the

141
00:05:13,120 --> 00:05:14,080
attack

142
00:05:14,080 --> 00:05:16,400
then we decode that as a one and

143
00:05:16,400 --> 00:05:18,720
otherwise as a zero in the message

144
00:05:18,720 --> 00:05:21,120
and finally there's active water marking

145
00:05:21,120 --> 00:05:23,039
so active water marking is purely a

146
00:05:23,039 --> 00:05:25,759
post-processing step to the model where

147
00:05:25,759 --> 00:05:26,880
the

148
00:05:26,880 --> 00:05:28,880
watermarking function has access both to

149
00:05:28,880 --> 00:05:31,120
the input and all intermediate

150
00:05:31,120 --> 00:05:33,600
activations and then it outputs a label

151
00:05:33,600 --> 00:05:35,120
and then with a certain probability it

152
00:05:35,120 --> 00:05:37,759
can output a false label and then we can

153
00:05:37,759 --> 00:05:39,759
see if someone trains a model on these

154
00:05:39,759 --> 00:05:41,440
false labels

155
00:05:41,440 --> 00:05:44,240
if they stole our model

156
00:05:44,240 --> 00:05:46,479
so in total as i said we looked at 11

157
00:05:46,479 --> 00:05:48,160
watermarking schemes that are listed

158
00:05:48,160 --> 00:05:50,160
here unfortunately i cannot go into

159
00:05:50,160 --> 00:05:52,320
detail for all of them but they all

160
00:05:52,320 --> 00:05:54,880
depend on different functionalities to

161
00:05:54,880 --> 00:05:57,039
claim robustness

162
00:05:57,039 --> 00:05:59,039
okay so the same

163
00:05:59,039 --> 00:06:01,759
taxonomy also exists for the attacker so

164
00:06:01,759 --> 00:06:03,440
now we have an attacker that took a

165
00:06:03,440 --> 00:06:05,360
model that downloaded a model and they

166
00:06:05,360 --> 00:06:07,120
want to corrupt the message that a

167
00:06:07,120 --> 00:06:09,680
defender would read from the model

168
00:06:09,680 --> 00:06:11,840
so here we have three categories so the

169
00:06:11,840 --> 00:06:14,319
first are input pre-processing attacks

170
00:06:14,319 --> 00:06:16,479
where the attacker takes the image

171
00:06:16,479 --> 00:06:18,800
perturbs it and then passes it forward

172
00:06:18,800 --> 00:06:21,440
to the stolen model and here for example

173
00:06:21,440 --> 00:06:22,960
we have input noising where they just

174
00:06:22,960 --> 00:06:25,440
add random noise or input smoothing

175
00:06:25,440 --> 00:06:28,319
where they blur the image effectively

176
00:06:28,319 --> 00:06:30,800
then we have model modification attacks

177
00:06:30,800 --> 00:06:31,600
which

178
00:06:31,600 --> 00:06:33,199
some of the well-known ones might be

179
00:06:33,199 --> 00:06:34,960
neural cleanse for those working with

180
00:06:34,960 --> 00:06:37,440
back doors or neural laundering which is

181
00:06:37,440 --> 00:06:41,680
specifically tailored against watermarks

182
00:06:41,680 --> 00:06:43,520
and finally we have model extraction

183
00:06:43,520 --> 00:06:45,680
attacks where the attacker takes the

184
00:06:45,680 --> 00:06:48,080
model he downloads the the stolen model

185
00:06:48,080 --> 00:06:50,560
and then uses a distillation a knowledge

186
00:06:50,560 --> 00:06:52,479
distillation approach to train a

187
00:06:52,479 --> 00:06:54,479
different model so model extraction

188
00:06:54,479 --> 00:06:56,319
attacks they can even change the model

189
00:06:56,319 --> 00:06:58,400
architecture and we still want to be

190
00:06:58,400 --> 00:07:01,520
able to tell that the information from

191
00:07:01,520 --> 00:07:05,520
our model was used in the stolen model

192
00:07:05,520 --> 00:07:08,560
so we in the paper we describe

193
00:07:08,560 --> 00:07:11,199
adaptive attacks and three of them are

194
00:07:11,199 --> 00:07:12,639
listed here

195
00:07:12,639 --> 00:07:13,520
and

196
00:07:13,520 --> 00:07:16,560
i will be showing the results in

197
00:07:16,560 --> 00:07:18,800
subsequent slides so we need to answer

198
00:07:18,800 --> 00:07:21,199
two questions essentially so first what

199
00:07:21,199 --> 00:07:23,440
does it mean to be robust like i talked

200
00:07:23,440 --> 00:07:26,160
about the watermark accuracy but how

201
00:07:26,160 --> 00:07:27,840
large does the watermark accuracy have

202
00:07:27,840 --> 00:07:30,800
to be so here we see two distributions

203
00:07:30,800 --> 00:07:33,199
on the x-axis is the watermark accuracy

204
00:07:33,199 --> 00:07:35,360
and on the y-axis is the probability of

205
00:07:35,360 --> 00:07:38,479
the of observing this watermark accuracy

206
00:07:38,479 --> 00:07:40,319
so we see that both these functions

207
00:07:40,319 --> 00:07:42,639
they're separable now if we see a model

208
00:07:42,639 --> 00:07:44,240
in the wild and it has a watermark

209
00:07:44,240 --> 00:07:46,000
accuracy lower than the decision

210
00:07:46,000 --> 00:07:48,400
threshold we say it wasn't stolen

211
00:07:48,400 --> 00:07:50,160
otherwise it is stolen

212
00:07:50,160 --> 00:07:52,639
but determining this

213
00:07:52,639 --> 00:07:55,039
unmodified un

214
00:07:55,039 --> 00:07:57,520
unmarked watermark accuracy

215
00:07:57,520 --> 00:07:59,199
can be very hard

216
00:07:59,199 --> 00:08:01,520
if the watermark uses things like

217
00:08:01,520 --> 00:08:03,360
adversarial attacks because with

218
00:08:03,360 --> 00:08:05,440
adversarial attacks we have properties

219
00:08:05,440 --> 00:08:07,680
such as transferability and then it's

220
00:08:07,680 --> 00:08:09,520
very hard to know how unmarked models

221
00:08:09,520 --> 00:08:11,039
react to this

222
00:08:11,039 --> 00:08:12,960
and many schemes that we evaluated they

223
00:08:12,960 --> 00:08:16,639
didn't give a method to evaluate

224
00:08:16,639 --> 00:08:20,000
or to determine the decision threshold

225
00:08:20,000 --> 00:08:22,560
so what we do is we created a generic

226
00:08:22,560 --> 00:08:24,720
method where we train a lot of unmarked

227
00:08:24,720 --> 00:08:27,360
models and then empirically measure

228
00:08:27,360 --> 00:08:30,000
the decision thresholds and what we find

229
00:08:30,000 --> 00:08:31,759
is that there's large differences across

230
00:08:31,759 --> 00:08:34,479
data sets for a single scheme so on the

231
00:08:34,479 --> 00:08:36,399
x-axis we see all the schemes the 11

232
00:08:36,399 --> 00:08:37,919
schemes that we look at

233
00:08:37,919 --> 00:08:41,120
and also for between schemes we also see

234
00:08:41,120 --> 00:08:43,120
large differences so here gr has a very

235
00:08:43,120 --> 00:08:45,839
low decision threshold whereas frontier

236
00:08:45,839 --> 00:08:48,640
stitching or fs has a very high one so

237
00:08:48,640 --> 00:08:50,640
now we take all these values and in

238
00:08:50,640 --> 00:08:52,240
order to compare the schemes with each

239
00:08:52,240 --> 00:08:54,959
other we scale all the values according

240
00:08:54,959 --> 00:08:57,440
to this decision threshold to 0.5

241
00:08:57,440 --> 00:08:59,839
meaning if we measure watermark accuracy

242
00:08:59,839 --> 00:09:02,800
lower than 0.5 or 50 percent

243
00:09:02,800 --> 00:09:06,560
we say that it the scheme is not robust

244
00:09:06,560 --> 00:09:08,800
okay the second question that we need to

245
00:09:08,800 --> 00:09:10,560
answer is

246
00:09:10,560 --> 00:09:13,279
what setting for the defender and what

247
00:09:13,279 --> 00:09:15,279
setting for the attacker

248
00:09:15,279 --> 00:09:18,560
do we boil robustness down to so a

249
00:09:18,560 --> 00:09:20,640
defender for example can instantiate a

250
00:09:20,640 --> 00:09:23,120
watermark with different parameters now

251
00:09:23,120 --> 00:09:25,600
what if one set of parameters leads to a

252
00:09:25,600 --> 00:09:27,440
different optimal strategy for the

253
00:09:27,440 --> 00:09:29,600
attacker so this is where the nash

254
00:09:29,600 --> 00:09:31,200
equilibrium comes in

255
00:09:31,200 --> 00:09:32,560
so we

256
00:09:32,560 --> 00:09:35,120
design a payoff matrix where we write

257
00:09:35,120 --> 00:09:36,560
the test accuracy

258
00:09:36,560 --> 00:09:38,480
for successful attacks so attacks that

259
00:09:38,480 --> 00:09:40,880
remove the watermark and otherwise we

260
00:09:40,880 --> 00:09:43,600
write minus infinity and then we compute

261
00:09:43,600 --> 00:09:45,839
the the nash equilibrium

262
00:09:45,839 --> 00:09:47,680
for the attacker and the defender which

263
00:09:47,680 --> 00:09:49,760
is their optimal strategies and then we

264
00:09:49,760 --> 00:09:52,800
look at two scenarios so one scenario is

265
00:09:52,800 --> 00:09:55,040
one scheme versus all attacks so here

266
00:09:55,040 --> 00:09:57,519
the attacker knows the defense and the

267
00:09:57,519 --> 00:10:00,640
second is all schemes versus all attacks

268
00:10:00,640 --> 00:10:01,680
where

269
00:10:01,680 --> 00:10:03,760
the attack neither the attacker nor the

270
00:10:03,760 --> 00:10:06,480
defender know which strategies was used

271
00:10:06,480 --> 00:10:08,959
by the other party

272
00:10:08,959 --> 00:10:12,320
so now our empirical analysis starts so

273
00:10:12,320 --> 00:10:14,240
we i will only present the results on

274
00:10:14,240 --> 00:10:16,640
imagenet which is a large image

275
00:10:16,640 --> 00:10:18,560
classification data set with a thousand

276
00:10:18,560 --> 00:10:22,480
classes and 1.28 million images

277
00:10:22,480 --> 00:10:24,240
so what we see first is for the model

278
00:10:24,240 --> 00:10:27,279
extraction attacks we need a lot of data

279
00:10:27,279 --> 00:10:28,959
to steal a model with a high test

280
00:10:28,959 --> 00:10:31,040
accuracy so that's on the x axis we have

281
00:10:31,040 --> 00:10:33,200
the amount of unlabeled training data

282
00:10:33,200 --> 00:10:34,800
and on the y-axis we have the test

283
00:10:34,800 --> 00:10:37,920
accuracy and the blue box is an

284
00:10:37,920 --> 00:10:41,040
acceptable quote-unquote acceptable

285
00:10:41,040 --> 00:10:44,480
test accuracy so we see that

286
00:10:44,480 --> 00:10:46,399
for model modification attacks we limit

287
00:10:46,399 --> 00:10:48,720
the attacker to about a third of the

288
00:10:48,720 --> 00:10:51,279
unlabeled training data whereas

289
00:10:51,279 --> 00:10:53,920
the model extraction attacker has access

290
00:10:53,920 --> 00:10:56,079
to all of the unlabeled training data

291
00:10:56,079 --> 00:10:57,760
because if they had less they couldn't

292
00:10:57,760 --> 00:10:58,880
train

293
00:10:58,880 --> 00:11:00,000
they couldn't train a model they

294
00:11:00,000 --> 00:11:01,440
couldn't extract it with high enough

295
00:11:01,440 --> 00:11:02,800
accuracy

296
00:11:02,800 --> 00:11:03,600
so

297
00:11:03,600 --> 00:11:04,959
we look at the

298
00:11:04,959 --> 00:11:06,640
schemes and the embedding times of the

299
00:11:06,640 --> 00:11:09,279
water marking schemes so in general we

300
00:11:09,279 --> 00:11:11,360
observe that embedding is not a problem

301
00:11:11,360 --> 00:11:14,079
it's very fast so on the x-axis is

302
00:11:14,079 --> 00:11:16,399
always the time and on the left left

303
00:11:16,399 --> 00:11:18,959
graph we have at most one and a half

304
00:11:18,959 --> 00:11:22,000
hours which is absolutely fast

305
00:11:22,000 --> 00:11:23,920
whereas for the attacker they need a lot

306
00:11:23,920 --> 00:11:26,079
more time to run some of these attacks

307
00:11:26,079 --> 00:11:28,240
so the dashed vertical line is the

308
00:11:28,240 --> 00:11:30,480
training time of the defender from

309
00:11:30,480 --> 00:11:33,120
scratch and we can see that some attacks

310
00:11:33,120 --> 00:11:35,440
take more time more training time than

311
00:11:35,440 --> 00:11:37,279
the attack than the defender used to

312
00:11:37,279 --> 00:11:39,360
train their model from scratch

313
00:11:39,360 --> 00:11:40,079
so

314
00:11:40,079 --> 00:11:42,079
the next thing we're looking at is how

315
00:11:42,079 --> 00:11:43,519
much does

316
00:11:43,519 --> 00:11:45,279
the embedding

317
00:11:45,279 --> 00:11:47,839
impact the test accuracy of the model

318
00:11:47,839 --> 00:11:50,079
and we see that generally this is also

319
00:11:50,079 --> 00:11:52,160
good but it again depends a little bit

320
00:11:52,160 --> 00:11:55,200
on the use case so for our cases we saw

321
00:11:55,200 --> 00:11:57,200
mostly a drop of less than one percent

322
00:11:57,200 --> 00:12:00,000
of test accuracy except for one scheme

323
00:12:00,000 --> 00:12:00,800
which

324
00:12:00,800 --> 00:12:03,279
at three and a half i believe

325
00:12:03,279 --> 00:12:06,320
percent lower test accuracy

326
00:12:06,320 --> 00:12:07,279
okay

327
00:12:07,279 --> 00:12:09,519
our main result of this paper

328
00:12:09,519 --> 00:12:11,440
is that a lot of the schemes or all of

329
00:12:11,440 --> 00:12:14,000
the schemes that we observed were not

330
00:12:14,000 --> 00:12:16,240
robust in practice against a highly

331
00:12:16,240 --> 00:12:18,560
capable attacker so on the x-axis we

332
00:12:18,560 --> 00:12:20,320
have what we call the stealing loss

333
00:12:20,320 --> 00:12:21,279
which is

334
00:12:21,279 --> 00:12:22,320
the loss

335
00:12:22,320 --> 00:12:24,800
that the attacker has when he steals the

336
00:12:24,800 --> 00:12:25,680
model

337
00:12:25,680 --> 00:12:28,079
and on the y-axis we have the watermark

338
00:12:28,079 --> 00:12:30,880
accuracy and as i said it's scaled so

339
00:12:30,880 --> 00:12:34,560
everything that's lower than 0.5

340
00:12:34,560 --> 00:12:36,240
means it's not robust and none of the

341
00:12:36,240 --> 00:12:39,360
schemes is robust and the best scheme

342
00:12:39,360 --> 00:12:41,600
incurs a test accuracy drop of three

343
00:12:41,600 --> 00:12:43,120
percent

344
00:12:43,120 --> 00:12:44,320
so

345
00:12:44,320 --> 00:12:46,240
we we also see that the attacker doesn't

346
00:12:46,240 --> 00:12:47,279
need to

347
00:12:47,279 --> 00:12:49,279
doesn't need to spend a lot of runtime

348
00:12:49,279 --> 00:12:50,959
to remove these attacks especially our

349
00:12:50,959 --> 00:12:53,279
adaptive attacks are really fast at

350
00:12:53,279 --> 00:12:55,200
removing the watermark and there's only

351
00:12:55,200 --> 00:12:57,279
two schemes where the only attacks that

352
00:12:57,279 --> 00:12:59,040
remove the watermark are model

353
00:12:59,040 --> 00:13:00,639
extraction attacks namely transfer

354
00:13:00,639 --> 00:13:02,160
learning attacks

355
00:13:02,160 --> 00:13:03,279
so

356
00:13:03,279 --> 00:13:05,200
as a takeaway

357
00:13:05,200 --> 00:13:06,480
robustness

358
00:13:06,480 --> 00:13:08,560
is very hard to achieve and it

359
00:13:08,560 --> 00:13:10,880
it might be impossible to achieve

360
00:13:10,880 --> 00:13:13,440
robustness of watermarking against this

361
00:13:13,440 --> 00:13:15,680
highly capable attacker so for

362
00:13:15,680 --> 00:13:18,320
robustness it's it's crucial that we

363
00:13:18,320 --> 00:13:21,279
define the capabilities of the attacker

364
00:13:21,279 --> 00:13:22,079
very

365
00:13:22,079 --> 00:13:23,760
accurately

366
00:13:23,760 --> 00:13:26,240
so we have four guidelines now in this

367
00:13:26,240 --> 00:13:27,519
presentation in the paper there's some

368
00:13:27,519 --> 00:13:29,680
more so check that out if you're

369
00:13:29,680 --> 00:13:31,680
if you want more details so the first

370
00:13:31,680 --> 00:13:34,639
one is always derive a method to compute

371
00:13:34,639 --> 00:13:37,040
the decision threshold that is a major

372
00:13:37,040 --> 00:13:40,000
component of claiming robustness is to

373
00:13:40,000 --> 00:13:42,000
exactly specify

374
00:13:42,000 --> 00:13:44,079
what the decision threshold is

375
00:13:44,079 --> 00:13:45,760
second show limitations of your

376
00:13:45,760 --> 00:13:47,600
watermark break it

377
00:13:47,600 --> 00:13:50,480
right like assume a very strong attacker

378
00:13:50,480 --> 00:13:52,079
and break the watermark to show up the

379
00:13:52,079 --> 00:13:54,000
limitations none of the surveyed

380
00:13:54,000 --> 00:13:57,680
watermarks was robust and yours probably

381
00:13:57,680 --> 00:14:00,079
isn't either given that there's a highly

382
00:14:00,079 --> 00:14:01,680
capable attacker

383
00:14:01,680 --> 00:14:03,360
state assumption about the attacker's

384
00:14:03,360 --> 00:14:05,040
data set we found that an attacker with

385
00:14:05,040 --> 00:14:07,600
a lot of data also has a lot of power

386
00:14:07,600 --> 00:14:09,760
and this is always unlabeled data

387
00:14:09,760 --> 00:14:11,440
and finally we

388
00:14:11,440 --> 00:14:12,959
we would

389
00:14:12,959 --> 00:14:15,279
prefer to see an ablation study with the

390
00:14:15,279 --> 00:14:17,279
nash equilibrium foreign attacker to get

391
00:14:17,279 --> 00:14:19,360
a complete

392
00:14:19,360 --> 00:14:22,480
view of the robustness of the data set

393
00:14:22,480 --> 00:14:24,160
of the watermark sorry

394
00:14:24,160 --> 00:14:26,560
and that concludes our presentation

395
00:14:26,560 --> 00:14:28,320
feel free to check out our source code

396
00:14:28,320 --> 00:14:30,880
on github and i'm happy to receive any

397
00:14:30,880 --> 00:14:33,380
questions

398
00:14:33,380 --> 00:14:39,950
[Applause]

399
00:14:45,760 --> 00:14:48,720
hello um haroon from uc davis

400
00:14:48,720 --> 00:14:50,480
in the past i've come across some papers

401
00:14:50,480 --> 00:14:52,720
that have talked about watermarking as a

402
00:14:52,720 --> 00:14:54,399
clean label attack if you're familiar

403
00:14:54,399 --> 00:14:56,480
with poison frogs

404
00:14:56,480 --> 00:14:57,279
and

405
00:14:57,279 --> 00:14:59,440
my question is that in an attempt to

406
00:14:59,440 --> 00:15:01,440
make watermarking the model more robust

407
00:15:01,440 --> 00:15:03,440
so the model can't be stolen

408
00:15:03,440 --> 00:15:04,720
are you actually empowering these

409
00:15:04,720 --> 00:15:07,680
attacks to become better as well

410
00:15:07,680 --> 00:15:10,560
um so i'm not familiar with poison frogs

411
00:15:10,560 --> 00:15:12,959
so this was an attack demonstrated by

412
00:15:12,959 --> 00:15:15,760
like a paper in 2012 which was you could

413
00:15:15,760 --> 00:15:18,079
manipulate an image and an online model

414
00:15:18,079 --> 00:15:19,519
would then pick up that image which

415
00:15:19,519 --> 00:15:22,160
would poison its internal whatever it's

416
00:15:22,160 --> 00:15:24,000
learned so far they demonstrate that

417
00:15:24,000 --> 00:15:26,959
they could get some resnet model to

418
00:15:26,959 --> 00:15:29,759
see images of dogs as frogs

419
00:15:29,759 --> 00:15:31,680
oh i think so then they basically

420
00:15:31,680 --> 00:15:33,199
manipulate the image to like introduce a

421
00:15:33,199 --> 00:15:35,519
watermark in it which would mislead them

422
00:15:35,519 --> 00:15:36,880
with classifier

423
00:15:36,880 --> 00:15:38,480
and the question is if we should make

424
00:15:38,480 --> 00:15:40,800
attacks better in order to evaluate

425
00:15:40,800 --> 00:15:42,160
robustness if you're making it more

426
00:15:42,160 --> 00:15:45,040
robust with this study does that

427
00:15:45,040 --> 00:15:46,800
inadvertently make those attacks more

428
00:15:46,800 --> 00:15:48,639
robust as well

429
00:15:48,639 --> 00:15:51,040
um so i think one of the i think the

430
00:15:51,040 --> 00:15:52,959
limitations aren't really the attacks i

431
00:15:52,959 --> 00:15:54,800
do think that adaptive attacks i mean

432
00:15:54,800 --> 00:15:57,040
they're very hard to find you cannot ask

433
00:15:57,040 --> 00:15:58,480
someone who

434
00:15:58,480 --> 00:16:01,519
who designs and publishes a defense to

435
00:16:01,519 --> 00:16:04,800
cover all the adaptive attacks so what

436
00:16:04,800 --> 00:16:07,680
we would ask of them is to look at the

437
00:16:07,680 --> 00:16:09,920
adaptive attacks that we know

438
00:16:09,920 --> 00:16:11,519
exist in practice

439
00:16:11,519 --> 00:16:13,920
choose appropriate parameters ideally an

440
00:16:13,920 --> 00:16:16,079
ablation study over them and then show

441
00:16:16,079 --> 00:16:18,240
the robustness for it but actually it's

442
00:16:18,240 --> 00:16:20,240
an it's a very important point

443
00:16:20,240 --> 00:16:22,079
a takeaway from this paper is not

444
00:16:22,079 --> 00:16:24,639
necessarily that you have to redesign

445
00:16:24,639 --> 00:16:26,959
our study like take all attacks that we

446
00:16:26,959 --> 00:16:29,279
looked at and then run them against the

447
00:16:29,279 --> 00:16:30,639
watermark that you're designing i think

448
00:16:30,639 --> 00:16:32,560
that would put a lot of strain on on

449
00:16:32,560 --> 00:16:34,880
anyone wanting to propose a new defense

450
00:16:34,880 --> 00:16:37,360
i think the takeaway is more that we

451
00:16:37,360 --> 00:16:40,480
showed which attacks particularly

452
00:16:40,480 --> 00:16:43,440
break a subset of these watermarks or or

453
00:16:43,440 --> 00:16:46,079
watermark specifically so

454
00:16:46,079 --> 00:16:46,959
for

455
00:16:46,959 --> 00:16:49,199
for us the takeaway should be

456
00:16:49,199 --> 00:16:51,920
look where your water like what what are

457
00:16:51,920 --> 00:16:53,759
the closest other watermarks to your

458
00:16:53,759 --> 00:16:56,320
watermark look what they are vulnerable

459
00:16:56,320 --> 00:16:58,639
to and then show your performance

460
00:16:58,639 --> 00:17:00,720
specifically against that subset of

461
00:17:00,720 --> 00:17:02,720
attacks

462
00:17:02,720 --> 00:17:05,199
okay thank you

463
00:17:05,199 --> 00:17:08,480
all right uh if no more questions let's

464
00:17:08,480 --> 00:17:10,210
thank nails again

465
00:17:10,210 --> 00:17:15,789
[Applause]

