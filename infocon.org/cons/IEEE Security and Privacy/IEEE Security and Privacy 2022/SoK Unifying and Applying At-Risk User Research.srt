1
00:00:01,439 --> 00:00:02,960
all right

2
00:00:02,960 --> 00:00:04,640
thanks everybody and thanks for that

3
00:00:04,640 --> 00:00:06,640
introduction it's a pleasure to be with

4
00:00:06,640 --> 00:00:08,880
you here today to discuss a framework

5
00:00:08,880 --> 00:00:11,040
for unifying at risk as a research i'm

6
00:00:11,040 --> 00:00:12,000
noel

7
00:00:12,000 --> 00:00:13,759
uh before i start i'd like to thank my

8
00:00:13,759 --> 00:00:15,599
colleagues at the university of maryland

9
00:00:15,599 --> 00:00:18,080
caitlin omer nathan and my advisor

10
00:00:18,080 --> 00:00:20,400
michelle and our wonderful colleagues at

11
00:00:20,400 --> 00:00:23,519
google as well tara sunny patrick mania

12
00:00:23,519 --> 00:00:25,439
and kurt

13
00:00:25,439 --> 00:00:27,840
so at-risk users by the definition of

14
00:00:27,840 --> 00:00:30,960
rsok are people who experience risk

15
00:00:30,960 --> 00:00:32,800
factors like societal factors

16
00:00:32,800 --> 00:00:35,600
relationships or personal circumstances

17
00:00:35,600 --> 00:00:37,840
that augment or amplify their chances of

18
00:00:37,840 --> 00:00:39,760
being digitally attacked compared to

19
00:00:39,760 --> 00:00:41,200
general users

20
00:00:41,200 --> 00:00:44,239
and or suffering disproportionate harms

21
00:00:44,239 --> 00:00:46,000
so the question is therefore how do we

22
00:00:46,000 --> 00:00:49,200
design for these users the common advice

23
00:00:49,200 --> 00:00:51,199
in the literature on this topic

24
00:00:51,199 --> 00:00:53,199
is to consider at-risk users in the

25
00:00:53,199 --> 00:00:55,760
research and technology design processes

26
00:00:55,760 --> 00:00:57,520
but that can be bewildering given the

27
00:00:57,520 --> 00:00:59,039
variety and scope of the needs that

28
00:00:59,039 --> 00:01:00,960
these users have regardless of the

29
00:01:00,960 --> 00:01:03,520
desire of researches and technologists

30
00:01:03,520 --> 00:01:05,280
to do so

31
00:01:05,280 --> 00:01:07,760
as an example of differing needs compare

32
00:01:07,760 --> 00:01:10,560
the circumstances of a survivor of

33
00:01:10,560 --> 00:01:12,400
intermittent partner abuse

34
00:01:12,400 --> 00:01:14,880
and a high profile journalist

35
00:01:14,880 --> 00:01:17,200
so a survivor of intimate partner abuse

36
00:01:17,200 --> 00:01:19,119
has a direct relationship with their

37
00:01:19,119 --> 00:01:21,360
attacker who therefore has intimate

38
00:01:21,360 --> 00:01:24,000
physical access to their devices and the

39
00:01:24,000 --> 00:01:26,560
goal of the survivor is to achieve

40
00:01:26,560 --> 00:01:28,400
most of the time some kind of permitted

41
00:01:28,400 --> 00:01:30,320
separation from this person both

42
00:01:30,320 --> 00:01:32,880
physically and digitally

43
00:01:32,880 --> 00:01:35,200
in comparison though a high-profile

44
00:01:35,200 --> 00:01:36,960
journalist might have a highly resourced

45
00:01:36,960 --> 00:01:38,640
adversary like a nation-state who

46
00:01:38,640 --> 00:01:41,280
doesn't want them to publish x y or z

47
00:01:41,280 --> 00:01:42,880
they may also have access to

48
00:01:42,880 --> 00:01:44,320
institutional support through their

49
00:01:44,320 --> 00:01:47,119
newsroom and they might have worked with

50
00:01:47,119 --> 00:01:49,119
sources who have their own

51
00:01:49,119 --> 00:01:50,960
requirements for communication which

52
00:01:50,960 --> 00:01:52,479
could limit the options that they can

53
00:01:52,479 --> 00:01:54,880
use to protect themselves

54
00:01:54,880 --> 00:01:57,840
so accordingly we present a synthesis of

55
00:01:57,840 --> 00:01:59,520
this work from the past seven or eight

56
00:01:59,520 --> 00:02:01,439
years in order to guide research and

57
00:02:01,439 --> 00:02:03,680
tech development in the future so how do

58
00:02:03,680 --> 00:02:04,880
we do this

59
00:02:04,880 --> 00:02:06,960
uh we in order to create the synthesis

60
00:02:06,960 --> 00:02:09,360
we collected security privacy and hci

61
00:02:09,360 --> 00:02:13,800
papers from 2016 to 2020 resulting in

62
00:02:13,800 --> 00:02:17,280
6534 papers we wanted to cast a wide net

63
00:02:17,280 --> 00:02:19,040
at this stage in the process to fetch as

64
00:02:19,040 --> 00:02:20,840
many potentially relevant papers as

65
00:02:20,840 --> 00:02:24,239
possible we then iteratively selected a

66
00:02:24,239 --> 00:02:25,920
subset of these papers that directly

67
00:02:25,920 --> 00:02:28,000
studied at-risk users

68
00:02:28,000 --> 00:02:30,319
first by reviewing titles and abstracts

69
00:02:30,319 --> 00:02:31,920
and then by discussing potentially

70
00:02:31,920 --> 00:02:34,080
relevant papers as a team we wanted to

71
00:02:34,080 --> 00:02:36,400
focus on papers that dealt specifically

72
00:02:36,400 --> 00:02:38,480
with at-risk users in the context of

73
00:02:38,480 --> 00:02:40,560
their digital safety concerns this

74
00:02:40,560 --> 00:02:42,879
resulted in our final data set of 95

75
00:02:42,879 --> 00:02:44,400
papers

76
00:02:44,400 --> 00:02:46,560
then we performed thematic analysis on

77
00:02:46,560 --> 00:02:48,800
our final data set in order to develop

78
00:02:48,800 --> 00:02:52,160
our framework which i will now describe

79
00:02:52,160 --> 00:02:53,680
so here's a summary of the at-risk

80
00:02:53,680 --> 00:02:56,000
frameworks major components first we

81
00:02:56,000 --> 00:02:57,840
identify the factors that augment or

82
00:02:57,840 --> 00:02:59,680
amplify digital safety risk for these

83
00:02:59,680 --> 00:03:02,000
users which are divided into three broad

84
00:03:02,000 --> 00:03:03,120
categories

85
00:03:03,120 --> 00:03:05,200
broad societal factors specific

86
00:03:05,200 --> 00:03:07,519
relationships that expose them to risk

87
00:03:07,519 --> 00:03:09,519
and a variety of personal circumstances

88
00:03:09,519 --> 00:03:12,000
that elevate digital safety risk

89
00:03:12,000 --> 00:03:14,159
we also identified three categories of

90
00:03:14,159 --> 00:03:16,080
protective practices and strategies

91
00:03:16,080 --> 00:03:18,080
at-risk users employed to ameliorate

92
00:03:18,080 --> 00:03:20,560
those risks strategies that rely on

93
00:03:20,560 --> 00:03:22,480
their social networks and connections

94
00:03:22,480 --> 00:03:25,120
reduces seeing the use of or distancing

95
00:03:25,120 --> 00:03:27,280
themselves from technology and

96
00:03:27,280 --> 00:03:30,959
specifical technical solutions and tools

97
00:03:30,959 --> 00:03:32,640
while the individual practices in these

98
00:03:32,640 --> 00:03:34,239
categories will change over time

99
00:03:34,239 --> 00:03:36,159
particularly technical solutions we

100
00:03:36,159 --> 00:03:37,920
believe these larger categories are

101
00:03:37,920 --> 00:03:40,080
comprehensive

102
00:03:40,080 --> 00:03:41,840
so now i'm going to go into some more

103
00:03:41,840 --> 00:03:44,080
detail about the contextual risk factors

104
00:03:44,080 --> 00:03:46,239
we identified a comprehensive set of 10

105
00:03:46,239 --> 00:03:48,080
of these in these three categories and

106
00:03:48,080 --> 00:03:49,519
on the following slides i'll discuss

107
00:03:49,519 --> 00:03:50,720
each of them

108
00:03:50,720 --> 00:03:52,640
so first our

109
00:03:52,640 --> 00:03:54,319
first set of contextual risk factors are

110
00:03:54,319 --> 00:03:56,080
the societal factors i mentioned so

111
00:03:56,080 --> 00:03:58,560
broad social or political considerations

112
00:03:58,560 --> 00:04:01,519
that impact one's digital safety

113
00:04:01,519 --> 00:04:03,519
the three factors in this category are

114
00:04:03,519 --> 00:04:05,599
legal or political where laws and

115
00:04:05,599 --> 00:04:07,280
policies of government or political

116
00:04:07,280 --> 00:04:09,439
activity have an impact on your digital

117
00:04:09,439 --> 00:04:10,959
safety profile

118
00:04:10,959 --> 00:04:13,040
marginalization can have an impact on

119
00:04:13,040 --> 00:04:14,400
this particularly in the realm of

120
00:04:14,400 --> 00:04:16,959
harassment but not exclusively

121
00:04:16,959 --> 00:04:19,358
and uh particularly restrictive social

122
00:04:19,358 --> 00:04:21,680
norms might actually impact uh what

123
00:04:21,680 --> 00:04:23,280
you're expected to do or what you're

124
00:04:23,280 --> 00:04:24,560
able to do

125
00:04:24,560 --> 00:04:27,680
in order uh to negotiate digital safety

126
00:04:27,680 --> 00:04:29,919
with others i'll emphasize that these

127
00:04:29,919 --> 00:04:32,160
are some of the examples of populations

128
00:04:32,160 --> 00:04:34,000
that experience not every single one i

129
00:04:34,000 --> 00:04:35,520
encourage you to check out our paper for

130
00:04:35,520 --> 00:04:37,440
more details on that

131
00:04:37,440 --> 00:04:39,440
the second category of contextual risk

132
00:04:39,440 --> 00:04:41,360
factors are the specific relationships

133
00:04:41,360 --> 00:04:43,840
that impact user digital safety

134
00:04:43,840 --> 00:04:45,520
first having a direct relationship with

135
00:04:45,520 --> 00:04:47,520
the attacker as in the case for example

136
00:04:47,520 --> 00:04:50,080
of intimate partner abuse

137
00:04:50,080 --> 00:04:52,240
dramatically changes one's digital

138
00:04:52,240 --> 00:04:53,759
safety position

139
00:04:53,759 --> 00:04:55,600
having reliance on a third party even

140
00:04:55,600 --> 00:04:57,520
one that is beneficial can also have an

141
00:04:57,520 --> 00:04:59,600
impact on this think of you know this is

142
00:04:59,600 --> 00:05:01,280
increasing the attack surface for the

143
00:05:01,280 --> 00:05:02,800
individual or

144
00:05:02,800 --> 00:05:05,280
maybe some sort of impersonation can be

145
00:05:05,280 --> 00:05:06,720
an issue there

146
00:05:06,720 --> 00:05:09,440
and then some users have access to other

147
00:05:09,440 --> 00:05:11,360
at-risk users like journalists and their

148
00:05:11,360 --> 00:05:13,600
sources so the journalists themselves

149
00:05:13,600 --> 00:05:15,199
may not be the direct target but they

150
00:05:15,199 --> 00:05:17,039
might receive particular targeting

151
00:05:17,039 --> 00:05:18,639
because someone wants to get access to

152
00:05:18,639 --> 00:05:19,919
the source that they're communicating

153
00:05:19,919 --> 00:05:21,680
with

154
00:05:21,680 --> 00:05:23,440
finally a variety of personal

155
00:05:23,440 --> 00:05:25,199
circumstances might contribute to these

156
00:05:25,199 --> 00:05:27,120
users digital safety

157
00:05:27,120 --> 00:05:29,039
so particular prominence

158
00:05:29,039 --> 00:05:30,479
can expose one to more risk the more

159
00:05:30,479 --> 00:05:32,560
people know who you are the more likely

160
00:05:32,560 --> 00:05:34,639
you might be to be targeted particular

161
00:05:34,639 --> 00:05:36,800
resource or time constraints can have a

162
00:05:36,800 --> 00:05:38,560
significant impact on digital safety

163
00:05:38,560 --> 00:05:40,800
particularly in the areas of recovering

164
00:05:40,800 --> 00:05:43,120
uh from negative incidents

165
00:05:43,120 --> 00:05:46,000
underserved accessibility needs are

166
00:05:46,000 --> 00:05:47,840
a problem across all of technology and

167
00:05:47,840 --> 00:05:49,759
digital safety is not exempt from that

168
00:05:49,759 --> 00:05:51,039
by any means

169
00:05:51,039 --> 00:05:52,720
and finally having access to some sort

170
00:05:52,720 --> 00:05:54,880
of specific sensitive resource much like

171
00:05:54,880 --> 00:05:56,479
having access to

172
00:05:56,479 --> 00:05:58,319
other at-risk users might put you at

173
00:05:58,319 --> 00:05:59,840
greater risk

174
00:05:59,840 --> 00:06:02,160
we also observed that these contextual

175
00:06:02,160 --> 00:06:04,720
factors can interact with one another so

176
00:06:04,720 --> 00:06:07,360
for example resource or time constraints

177
00:06:07,360 --> 00:06:08,960
made it harder to cope with other risk

178
00:06:08,960 --> 00:06:10,080
factors

179
00:06:10,080 --> 00:06:11,840
underserved accessibility needs and

180
00:06:11,840 --> 00:06:14,560
reliance on a third party often combined

181
00:06:14,560 --> 00:06:17,120
and prominence added focused targeting

182
00:06:17,120 --> 00:06:18,639
to other risk factors which i will

183
00:06:18,639 --> 00:06:22,000
explain now in more detail as an example

184
00:06:22,000 --> 00:06:23,360
so

185
00:06:23,360 --> 00:06:24,479
uh

186
00:06:24,479 --> 00:06:26,800
an example from learner at all from kai

187
00:06:26,800 --> 00:06:29,759
2020 is that an lgbtq person who

188
00:06:29,759 --> 00:06:31,919
experiences marginalization

189
00:06:31,919 --> 00:06:34,400
and and maybe harassment as a result

190
00:06:34,400 --> 00:06:36,400
who then becomes prominent due to their

191
00:06:36,400 --> 00:06:39,520
activism might experience much

192
00:06:39,520 --> 00:06:41,440
highly much more highly targeted

193
00:06:41,440 --> 00:06:43,280
harassment as a result of those two

194
00:06:43,280 --> 00:06:45,440
factors interacting with one another

195
00:06:45,440 --> 00:06:48,000
creating a sort of a new experience that

196
00:06:48,000 --> 00:06:49,840
is outside either one of those

197
00:06:49,840 --> 00:06:51,919
individually

198
00:06:51,919 --> 00:06:53,360
so now i'm going to discuss the

199
00:06:53,360 --> 00:06:56,080
protective practices in some more detail

200
00:06:56,080 --> 00:06:59,680
so the this part of our framework is

201
00:06:59,680 --> 00:07:01,360
talking about what at-risk users

202
00:07:01,360 --> 00:07:03,360
currently do about digital safety

203
00:07:03,360 --> 00:07:05,520
threats which emerge from the contextual

204
00:07:05,520 --> 00:07:07,599
risk factors that they experience and

205
00:07:07,599 --> 00:07:09,599
are neither mutually exclusive nor

206
00:07:09,599 --> 00:07:12,560
necessarily ideal however understanding

207
00:07:12,560 --> 00:07:14,560
current protective practices allows us

208
00:07:14,560 --> 00:07:16,319
to gain important insights into the

209
00:07:16,319 --> 00:07:18,560
priorities and needs of these users even

210
00:07:18,560 --> 00:07:20,639
if those protections are not necessarily

211
00:07:20,639 --> 00:07:22,400
perfect

212
00:07:22,400 --> 00:07:24,479
so some users protected themselves by

213
00:07:24,479 --> 00:07:26,240
using social connections to overcome

214
00:07:26,240 --> 00:07:28,800
digital safety threats for example one

215
00:07:28,800 --> 00:07:30,479
of the several social strategies we

216
00:07:30,479 --> 00:07:32,080
observed is that some users would seek

217
00:07:32,080 --> 00:07:33,840
help from trusted others like family

218
00:07:33,840 --> 00:07:36,080
friends or certain trusted organizations

219
00:07:36,080 --> 00:07:38,240
for assistance with digital safety and

220
00:07:38,240 --> 00:07:40,319
as another example some users will

221
00:07:40,319 --> 00:07:42,160
preemptively disclose sensitive

222
00:07:42,160 --> 00:07:43,759
information in order to control their

223
00:07:43,759 --> 00:07:45,440
self-narrative and prevent this

224
00:07:45,440 --> 00:07:47,199
information being used against them in

225
00:07:47,199 --> 00:07:49,039
the future

226
00:07:49,039 --> 00:07:51,360
the second category of protective

227
00:07:51,360 --> 00:07:53,599
practices was limiting technology use

228
00:07:53,599 --> 00:07:54,960
rather than engaging with other

229
00:07:54,960 --> 00:07:57,360
solutions which was incredibly pervasive

230
00:07:57,360 --> 00:07:59,360
across our data set some examples

231
00:07:59,360 --> 00:08:01,599
included users physically destroying

232
00:08:01,599 --> 00:08:03,360
compromised devices when they did not

233
00:08:03,360 --> 00:08:05,680
know how to make those devices safe

234
00:08:05,680 --> 00:08:08,080
and uh reducing online participation

235
00:08:08,080 --> 00:08:09,520
through self-censorship in order to

236
00:08:09,520 --> 00:08:12,240
prevent protect themselves from abuse or

237
00:08:12,240 --> 00:08:15,280
perhaps heighten surveillance

238
00:08:15,280 --> 00:08:17,199
finally of course at risk users employ a

239
00:08:17,199 --> 00:08:18,960
variety of tools and technologies for

240
00:08:18,960 --> 00:08:20,800
digital safety protection with varying

241
00:08:20,800 --> 00:08:23,360
levels of effectiveness and impact

242
00:08:23,360 --> 00:08:25,759
some users in our data set do and dude

243
00:08:25,759 --> 00:08:26,479
you

244
00:08:26,479 --> 00:08:29,039
do indeed use secure communication tools

245
00:08:29,039 --> 00:08:31,199
like encrypted messaging apps or even

246
00:08:31,199 --> 00:08:34,080
pgp and some users also employ tracking

247
00:08:34,080 --> 00:08:36,000
and monitoring applications often to

248
00:08:36,000 --> 00:08:37,839
support others like think of you know

249
00:08:37,839 --> 00:08:40,159
parents and teenagers or people who are

250
00:08:40,159 --> 00:08:41,599
caretakers of

251
00:08:41,599 --> 00:08:44,240
older adults

252
00:08:44,480 --> 00:08:46,560
so during the thematic analysis that led

253
00:08:46,560 --> 00:08:48,800
to the development of this framework we

254
00:08:48,800 --> 00:08:50,800
also identified a set of barriers that

255
00:08:50,800 --> 00:08:52,800
prevented at-risk users from adopting

256
00:08:52,800 --> 00:08:55,120
protective practices as informed by the

257
00:08:55,120 --> 00:08:56,720
particular contextual risk factors that

258
00:08:56,720 --> 00:08:58,160
they experience

259
00:08:58,160 --> 00:09:00,399
the first example of this uh were

260
00:09:00,399 --> 00:09:02,080
priorities that competed with digital

261
00:09:02,080 --> 00:09:04,480
safety collins has provided us already

262
00:09:04,480 --> 00:09:06,000
in this session a perfect example of

263
00:09:06,000 --> 00:09:07,279
that

264
00:09:07,279 --> 00:09:09,760
so as as we know for for everybody

265
00:09:09,760 --> 00:09:12,560
digital safety is not always uh and

266
00:09:12,560 --> 00:09:14,720
cannot always be the top priority this

267
00:09:14,720 --> 00:09:16,480
is true for people in general and

268
00:09:16,480 --> 00:09:18,959
especially for at-risk users uh for

269
00:09:18,959 --> 00:09:21,440
example some users may be motivated to

270
00:09:21,440 --> 00:09:23,120
use potentially risky modes of

271
00:09:23,120 --> 00:09:25,200
communication in order to participate in

272
00:09:25,200 --> 00:09:27,440
activist communities despite the dangers

273
00:09:27,440 --> 00:09:28,320
that they could face if this

274
00:09:28,320 --> 00:09:29,920
communication was compromised in some

275
00:09:29,920 --> 00:09:30,720
way

276
00:09:30,720 --> 00:09:32,560
as a human rights activist memorably

277
00:09:32,560 --> 00:09:35,680
said in marsec and pax in 2017

278
00:09:35,680 --> 00:09:37,680
should i spend half a day figuring out

279
00:09:37,680 --> 00:09:41,440
digital security or do work

280
00:09:41,440 --> 00:09:44,480
um as another example uh journalists

281
00:09:44,480 --> 00:09:46,640
often described uh

282
00:09:46,640 --> 00:09:48,560
there we go journalists often described

283
00:09:48,560 --> 00:09:50,320
using less secure methods of

284
00:09:50,320 --> 00:09:52,320
communication with sources because the

285
00:09:52,320 --> 00:09:54,000
need to communicate in a way that worked

286
00:09:54,000 --> 00:09:56,000
for those sources outweighed the privacy

287
00:09:56,000 --> 00:09:58,399
concerns that they might have had

288
00:09:58,399 --> 00:09:59,760
regardless of what their personal

289
00:09:59,760 --> 00:10:02,000
preferences were

290
00:10:02,000 --> 00:10:03,680
our second major barrier was lack of

291
00:10:03,680 --> 00:10:05,760
knowledge or experience there are many

292
00:10:05,760 --> 00:10:07,200
examples in the literature which

293
00:10:07,200 --> 00:10:08,959
demonstrate users have limited digital

294
00:10:08,959 --> 00:10:11,279
safety knowledge on top of general

295
00:10:11,279 --> 00:10:13,360
knowledge though at-risk users need to

296
00:10:13,360 --> 00:10:15,920
implement robust protections in order to

297
00:10:15,920 --> 00:10:18,000
combat sophisticated attackers like

298
00:10:18,000 --> 00:10:20,000
nation states or counter intimate

299
00:10:20,000 --> 00:10:22,160
threats as i've already described and

300
00:10:22,160 --> 00:10:24,000
they often are required to do so under

301
00:10:24,000 --> 00:10:26,720
severe uh resource and time constraints

302
00:10:26,720 --> 00:10:28,560
for example not being able to get a new

303
00:10:28,560 --> 00:10:30,640
device that has like the most up-to-date

304
00:10:30,640 --> 00:10:32,720
security measures or something like that

305
00:10:32,720 --> 00:10:34,399
lack of knowledge of resources makes all

306
00:10:34,399 --> 00:10:36,640
of the challenges that these users face

307
00:10:36,640 --> 00:10:38,720
even worse because they have difficulty

308
00:10:38,720 --> 00:10:41,200
sometimes dealing with them

309
00:10:41,200 --> 00:10:43,200
finally we noticed that at-risk users

310
00:10:43,200 --> 00:10:45,760
threat models often broke some common

311
00:10:45,760 --> 00:10:48,000
design assumptions in south asia for

312
00:10:48,000 --> 00:10:49,920
example it is very common for families

313
00:10:49,920 --> 00:10:51,680
to share devices and accounts due to

314
00:10:51,680 --> 00:10:53,760
strong social norms violating this idea

315
00:10:53,760 --> 00:10:56,399
that one device has one set of accounts

316
00:10:56,399 --> 00:10:59,200
on it which corresponds to one person

317
00:10:59,200 --> 00:11:01,360
another example is technology access for

318
00:11:01,360 --> 00:11:03,760
example multi-factor authentication that

319
00:11:03,760 --> 00:11:05,600
depends on a mobile phone could be

320
00:11:05,600 --> 00:11:07,279
inaccessible to people experiencing

321
00:11:07,279 --> 00:11:08,640
homelessness who are unable to

322
00:11:08,640 --> 00:11:10,480
consistently pay a phone bill so they

323
00:11:10,480 --> 00:11:12,000
would have to turn it off and be less

324
00:11:12,000 --> 00:11:13,920
secure

325
00:11:13,920 --> 00:11:16,079
finally although digital safety options

326
00:11:16,079 --> 00:11:17,680
are that are too numerous may be

327
00:11:17,680 --> 00:11:20,320
difficult to use some at-risk users have

328
00:11:20,320 --> 00:11:22,399
highly contextual digital safety needs

329
00:11:22,399 --> 00:11:24,480
which leads to these users needing

330
00:11:24,480 --> 00:11:26,640
direct granular control of the security

331
00:11:26,640 --> 00:11:28,720
settings of their devices survivors of

332
00:11:28,720 --> 00:11:30,000
intimate partner

333
00:11:30,000 --> 00:11:32,720
abuse for example need to very carefully

334
00:11:32,720 --> 00:11:34,720
navigate the nuances of their

335
00:11:34,720 --> 00:11:36,399
relationship with the attacker that they

336
00:11:36,399 --> 00:11:38,800
face and so they need granular access to

337
00:11:38,800 --> 00:11:41,440
these controls

338
00:11:41,440 --> 00:11:43,200
so the question remains i've been

339
00:11:43,200 --> 00:11:44,640
talking about this framework for 11

340
00:11:44,640 --> 00:11:46,720
minutes what is it for

341
00:11:46,720 --> 00:11:49,680
uh we believe that uh in the research

342
00:11:49,680 --> 00:11:52,000
space uh this framework can help us

343
00:11:52,000 --> 00:11:54,079
identify populations that might benefit

344
00:11:54,079 --> 00:11:55,600
from in-depth study like the ones

345
00:11:55,600 --> 00:11:57,279
described in our data set and to

346
00:11:57,279 --> 00:11:58,959
continue to investigate populations that

347
00:11:58,959 --> 00:12:01,040
have already been studied for example

348
00:12:01,040 --> 00:12:02,880
real estate agents have access to a

349
00:12:02,880 --> 00:12:05,279
sensitive resource as depicted in some

350
00:12:05,279 --> 00:12:07,279
news media and maybe specifically at

351
00:12:07,279 --> 00:12:08,959
higher risk because of that but that's

352
00:12:08,959 --> 00:12:10,560
not currently represented to my

353
00:12:10,560 --> 00:12:12,720
knowledge in the literature

354
00:12:12,720 --> 00:12:15,360
we also believe that this uh framework

355
00:12:15,360 --> 00:12:18,000
uh can help us

356
00:12:18,000 --> 00:12:20,079
identify the interactions that i was uh

357
00:12:20,079 --> 00:12:21,760
talking about earlier

358
00:12:21,760 --> 00:12:23,680
we know that this happens

359
00:12:23,680 --> 00:12:25,600
but we don't yet know enough about which

360
00:12:25,600 --> 00:12:27,279
contextual factors interact how this

361
00:12:27,279 --> 00:12:29,040
happens and what the end result of those

362
00:12:29,040 --> 00:12:31,360
interactions can be

363
00:12:31,360 --> 00:12:32,959
we also believe that this framework can

364
00:12:32,959 --> 00:12:34,720
be used to guide how this research could

365
00:12:34,720 --> 00:12:37,040
be conducted for instance one might ask

366
00:12:37,040 --> 00:12:38,720
about all of the contextual factors when

367
00:12:38,720 --> 00:12:40,160
interviewing a new population to make

368
00:12:40,160 --> 00:12:42,480
sure nothing is missed one challenge and

369
00:12:42,480 --> 00:12:44,240
identifying that uh working through this

370
00:12:44,240 --> 00:12:45,600
data set is all this research is

371
00:12:45,600 --> 00:12:47,040
conducted in different ways so looking

372
00:12:47,040 --> 00:12:48,560
for what we were looking for was

373
00:12:48,560 --> 00:12:50,800
sometimes a challenge

374
00:12:50,800 --> 00:12:52,240
we also believe this framework can be

375
00:12:52,240 --> 00:12:54,720
useful for technology creators uh this

376
00:12:54,720 --> 00:12:56,079
framework can be used to consider

377
00:12:56,079 --> 00:12:58,079
at-risk users at scale as mentioned at

378
00:12:58,079 --> 00:13:00,079
the beginning of this talk considering

379
00:13:00,079 --> 00:13:02,320
dozens of populations is in the design

380
00:13:02,320 --> 00:13:04,800
process is of course very difficult by

381
00:13:04,800 --> 00:13:06,880
providing this framework we simplify the

382
00:13:06,880 --> 00:13:08,399
important process of thinking through

383
00:13:08,399 --> 00:13:10,320
potential risks and needs of different

384
00:13:10,320 --> 00:13:11,839
populations

385
00:13:11,839 --> 00:13:13,760
the framework can also be used to help

386
00:13:13,760 --> 00:13:16,000
point technology creators to areas where

387
00:13:16,000 --> 00:13:18,480
improving usability and accessibility

388
00:13:18,480 --> 00:13:20,720
might be most useful and help prioritize

389
00:13:20,720 --> 00:13:22,240
that as demonstrated by our

390
00:13:22,240 --> 00:13:26,320
systemization of protective practices

391
00:13:27,360 --> 00:13:29,360
we also know that of course there's no

392
00:13:29,360 --> 00:13:31,120
such thing as perfect digital safety and

393
00:13:31,120 --> 00:13:33,440
so every new technology has trade-offs

394
00:13:33,440 --> 00:13:35,279
however some of the practices that we

395
00:13:35,279 --> 00:13:37,440
identified like distancing behaviors

396
00:13:37,440 --> 00:13:39,360
have significant downside like reduced

397
00:13:39,360 --> 00:13:40,639
participation

398
00:13:40,639 --> 00:13:43,279
careful evaluation of potential designs

399
00:13:43,279 --> 00:13:45,279
using our foreign framework can help

400
00:13:45,279 --> 00:13:46,959
technology teams reason about how to

401
00:13:46,959 --> 00:13:50,240
balance these various risks and benefits

402
00:13:50,240 --> 00:13:52,399
so to sum it up we developed a framework

403
00:13:52,399 --> 00:13:53,839
for understanding

404
00:13:53,839 --> 00:13:56,000
at-risk users which enumerates 10

405
00:13:56,000 --> 00:13:57,839
contextual risk factors and three

406
00:13:57,839 --> 00:14:00,240
categories of protective practices

407
00:14:00,240 --> 00:14:01,920
this framework also reveals some

408
00:14:01,920 --> 00:14:03,760
barriers that at-risk users face to

409
00:14:03,760 --> 00:14:05,760
implementing secure behaviors

410
00:14:05,760 --> 00:14:07,680
and we provide a direction for research

411
00:14:07,680 --> 00:14:09,839
and technology moving forward uh in this

412
00:14:09,839 --> 00:14:12,320
space i would like to thank all of these

413
00:14:12,320 --> 00:14:14,079
uh organizations for making this

414
00:14:14,079 --> 00:14:16,880
research possible and you're more than

415
00:14:16,880 --> 00:14:18,880
welcome to send me an email at this

416
00:14:18,880 --> 00:14:20,480
email address or come chat with me after

417
00:14:20,480 --> 00:14:21,519
the session

418
00:14:21,519 --> 00:14:23,760
and i'm now happy to take questions

419
00:14:23,760 --> 00:14:26,920
thank you

420
00:14:27,400 --> 00:14:31,640
[Applause]

421
00:14:31,839 --> 00:14:35,040
questions for our speaker

422
00:14:38,480 --> 00:14:39,360
hi

423
00:14:39,360 --> 00:14:41,120
great talk

424
00:14:41,120 --> 00:14:43,199
i was wondering how do you define

425
00:14:43,199 --> 00:14:46,079
at-risk users like if you were to

426
00:14:46,079 --> 00:14:47,920
define it i know there's a framework but

427
00:14:47,920 --> 00:14:50,160
just in general

428
00:14:50,160 --> 00:14:52,240
what would you say is at-risk

429
00:14:52,240 --> 00:14:54,240
users

430
00:14:54,240 --> 00:14:55,839
so this would be what i would consider

431
00:14:55,839 --> 00:14:57,199
to be our

432
00:14:57,199 --> 00:14:59,920
final definition um

433
00:14:59,920 --> 00:15:02,480
which is sort of deliberately broad we

434
00:15:02,480 --> 00:15:05,360
don't really want to like it is sort of

435
00:15:05,360 --> 00:15:07,519
a thorny question is like who is at risk

436
00:15:07,519 --> 00:15:10,079
enough kind of to be to be considered in

437
00:15:10,079 --> 00:15:11,360
this framework

438
00:15:11,360 --> 00:15:12,480
um

439
00:15:12,480 --> 00:15:14,959
but maybe almost tautologically i would

440
00:15:14,959 --> 00:15:16,880
say that someone who experienced some is

441
00:15:16,880 --> 00:15:18,880
some of these contextual risk factors

442
00:15:18,880 --> 00:15:20,800
would be candidates so like usually

443
00:15:20,800 --> 00:15:22,079
though we consider

444
00:15:22,079 --> 00:15:23,600
more likely to be attacked for some

445
00:15:23,600 --> 00:15:25,920
reason some characteristic or the harms

446
00:15:25,920 --> 00:15:27,839
that they experience which may be the

447
00:15:27,839 --> 00:15:30,000
same harms as anybody else might be

448
00:15:30,000 --> 00:15:32,320
disproportionate in their scope because

449
00:15:32,320 --> 00:15:35,040
of their particular circumstances like

450
00:15:35,040 --> 00:15:36,480
um

451
00:15:36,480 --> 00:15:37,920
like if my

452
00:15:37,920 --> 00:15:40,560
home address was leaked if i was doxxed

453
00:15:40,560 --> 00:15:41,680
um

454
00:15:41,680 --> 00:15:43,600
my experience of that would be very

455
00:15:43,600 --> 00:15:44,800
different from somebody who has

456
00:15:44,800 --> 00:15:47,279
experienced partner abuse because of the

457
00:15:47,279 --> 00:15:49,920
circumstance right like that result is

458
00:15:49,920 --> 00:15:51,519
much more devastating like it's not

459
00:15:51,519 --> 00:15:53,440
pleasant for me as someone who has not

460
00:15:53,440 --> 00:15:55,920
had that experience but the end result

461
00:15:55,920 --> 00:15:58,079
is is much worse uh in some of these

462
00:15:58,079 --> 00:15:59,680
contexts

463
00:15:59,680 --> 00:16:02,000
okay i had a follow-up question sure so

464
00:16:02,000 --> 00:16:04,800
um by digitally attacked do you mean

465
00:16:04,800 --> 00:16:07,040
just attacks that happen online and stay

466
00:16:07,040 --> 00:16:08,800
online or do you mean ones that could

467
00:16:08,800 --> 00:16:11,600
also lead to offline consequences

468
00:16:11,600 --> 00:16:13,519
uh i wouldn't i would include both of

469
00:16:13,519 --> 00:16:15,600
those but we were focused mostly on the

470
00:16:15,600 --> 00:16:18,160
digital safety domain so uh our

471
00:16:18,160 --> 00:16:20,639
conferences that we looked at were both

472
00:16:20,639 --> 00:16:22,880
uh major security privacy conferences as

473
00:16:22,880 --> 00:16:25,199
well as the hci space generally but some

474
00:16:25,199 --> 00:16:27,279
of the papers in our data set

475
00:16:27,279 --> 00:16:29,279
certainly cross those lines like work

476
00:16:29,279 --> 00:16:31,360
again uh work on intimate partner abuse

477
00:16:31,360 --> 00:16:33,120
that it's a it's a flu like digital

478
00:16:33,120 --> 00:16:34,880
mediation was was an important part of

479
00:16:34,880 --> 00:16:37,759
that but obviously has so in person

480
00:16:37,759 --> 00:16:40,959
ramifications as well

481
00:16:43,120 --> 00:16:46,800
all right any more questions

482
00:16:49,120 --> 00:16:52,639
let me check the chat

483
00:16:55,199 --> 00:16:56,959
no all right then it looks like we're

484
00:16:56,959 --> 00:17:01,638
done thank you again thank you

