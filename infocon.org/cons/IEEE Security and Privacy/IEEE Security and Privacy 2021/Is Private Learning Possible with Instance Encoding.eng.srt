1
00:00:00,480 --> 00:00:02,240
hi everyone on site knowledgefire this

2
00:00:02,240 --> 00:00:04,000
is the presentation for our work is

3
00:00:04,000 --> 00:00:05,680
private learning possible with instance

4
00:00:05,680 --> 00:00:06,799
encoding

5
00:00:06,799 --> 00:00:08,240
i'll do the first half of the talk and

6
00:00:08,240 --> 00:00:09,599
then florian will continue with the

7
00:00:09,599 --> 00:00:12,160
second half

8
00:00:13,440 --> 00:00:15,040
the process of machine learning starts

9
00:00:15,040 --> 00:00:16,480
with a training set that is labeled

10
00:00:16,480 --> 00:00:18,400
according to some concept function c

11
00:00:18,400 --> 00:00:20,000
and the goal of the learning algorithm

12
00:00:20,000 --> 00:00:21,680
is to look at this training set

13
00:00:21,680 --> 00:00:23,920
and find a good approximation of that

14
00:00:23,920 --> 00:00:25,760
concept function

15
00:00:25,760 --> 00:00:27,359
so this process can leak some

16
00:00:27,359 --> 00:00:29,679
information about the training set used

17
00:00:29,679 --> 00:00:30,640
and the

18
00:00:30,640 --> 00:00:32,558
study of privacy preserving machine

19
00:00:32,558 --> 00:00:34,800
learning aims at finding

20
00:00:34,800 --> 00:00:36,960
private algorithms that can minimize

21
00:00:36,960 --> 00:00:39,760
such leakage

22
00:00:41,120 --> 00:00:43,760
instance encoding is one specific idea

23
00:00:43,760 --> 00:00:45,280
that could be used for

24
00:00:45,280 --> 00:00:47,520
uh achieving privacy preserving uh

25
00:00:47,520 --> 00:00:48,719
machine learning

26
00:00:48,719 --> 00:00:51,120
where we have an additional encoding

27
00:00:51,120 --> 00:00:53,520
step added to machine learning pipeline

28
00:00:53,520 --> 00:00:55,360
so before applying the learning

29
00:00:55,360 --> 00:00:57,360
algorithm we first encode the training

30
00:00:57,360 --> 00:00:57,680
set

31
00:00:57,680 --> 00:01:00,239
into this intermediate object that we

32
00:01:00,239 --> 00:01:02,160
expect to be private

33
00:01:02,160 --> 00:01:04,959
by achieving some notion of privacy for

34
00:01:04,959 --> 00:01:06,080
example we might

35
00:01:06,080 --> 00:01:09,200
want the encodings to be non invertible

36
00:01:09,200 --> 00:01:11,280
or we might require the encodings to

37
00:01:11,280 --> 00:01:13,840
have some stronger indistinguishability

38
00:01:13,840 --> 00:01:15,920
guarantees

39
00:01:15,920 --> 00:01:18,080
but in both types of definitions no

40
00:01:18,080 --> 00:01:19,920
matter how we define privacy

41
00:01:19,920 --> 00:01:21,840
the learning algorithm here is expected

42
00:01:21,840 --> 00:01:23,439
to just learn by looking

43
00:01:23,439 --> 00:01:27,839
at the encodings

44
00:01:27,920 --> 00:01:30,159
why would this be nice so such an

45
00:01:30,159 --> 00:01:31,759
encoding could be really useful to

46
00:01:31,759 --> 00:01:32,159
achieve

47
00:01:32,159 --> 00:01:34,960
two types of privacy at once that are

48
00:01:34,960 --> 00:01:36,720
both proven to be very

49
00:01:36,720 --> 00:01:39,200
challenging to achieve so it can achieve

50
00:01:39,200 --> 00:01:40,640
private computation

51
00:01:40,640 --> 00:01:42,560
because these encodings are supposed to

52
00:01:42,560 --> 00:01:44,640
not reveal information about the inputs

53
00:01:44,640 --> 00:01:45,119
and

54
00:01:45,119 --> 00:01:47,040
the computation could still be done on

55
00:01:47,040 --> 00:01:48,960
top of them so this means in the

56
00:01:48,960 --> 00:01:50,720
multi-party learning scenarios that

57
00:01:50,720 --> 00:01:51,600
privacy is

58
00:01:51,600 --> 00:01:55,040
important such as healthcare the

59
00:01:55,040 --> 00:01:58,079
the data set owners can just reveal or

60
00:01:58,079 --> 00:02:00,399
share the encodings of the data without

61
00:02:00,399 --> 00:02:01,680
worrying about

62
00:02:01,680 --> 00:02:06,159
extra privacy leakage

63
00:02:06,159 --> 00:02:08,479
it can also provide private outcome like

64
00:02:08,479 --> 00:02:10,800
what we have in differential privacy

65
00:02:10,800 --> 00:02:12,560
this is again because the algorithm is

66
00:02:12,560 --> 00:02:14,720
done is is being applied on top of the

67
00:02:14,720 --> 00:02:16,160
encoded data

68
00:02:16,160 --> 00:02:17,920
and no matter what the algorithm does

69
00:02:17,920 --> 00:02:19,520
and what the output of the algorithm

70
00:02:19,520 --> 00:02:22,640
is the final output will be at least

71
00:02:22,640 --> 00:02:25,360
as private as the encodings and

72
00:02:25,360 --> 00:02:27,040
everything will boil down to just

73
00:02:27,040 --> 00:02:30,080
analyzing the privacy of the code

74
00:02:30,080 --> 00:02:32,480
so if we have such a private encoding

75
00:02:32,480 --> 00:02:34,319
these two types of privacy will be

76
00:02:34,319 --> 00:02:37,120
obtained immediately

77
00:02:37,120 --> 00:02:38,800
one instantiation of such instance

78
00:02:38,800 --> 00:02:40,319
encoding mechanism

79
00:02:40,319 --> 00:02:42,560
is insta hi that was introduced in last

80
00:02:42,560 --> 00:02:44,959
year icl

81
00:02:44,959 --> 00:02:47,519
so inside propose a way for achieving

82
00:02:47,519 --> 00:02:48,480
privacy

83
00:02:48,480 --> 00:02:50,480
without sacrificing accuracy or

84
00:02:50,480 --> 00:02:52,640
efficiency and they actually win

85
00:02:52,640 --> 00:02:55,280
a beloved's price for their contribution

86
00:02:55,280 --> 00:02:57,360
however they act their privacy of their

87
00:02:57,360 --> 00:02:58,319
skin is not

88
00:02:58,319 --> 00:03:02,080
well understood yet so in this world

89
00:03:02,080 --> 00:03:04,480
one of our goals was to first understand

90
00:03:04,480 --> 00:03:05,599
how private this

91
00:03:05,599 --> 00:03:07,519
instahyde algorithm is and we also

92
00:03:07,519 --> 00:03:09,680
wanted to have a more general treatment

93
00:03:09,680 --> 00:03:10,640
of this

94
00:03:10,640 --> 00:03:14,959
privacy to true instance encoding idea

95
00:03:14,959 --> 00:03:16,720
so for first question you provide an

96
00:03:16,720 --> 00:03:18,400
attack that breaks the challenge data

97
00:03:18,400 --> 00:03:20,959
set that the instahype authors provide

98
00:03:20,959 --> 00:03:22,720
and florian will talk more about this

99
00:03:22,720 --> 00:03:24,799
challenge but here i'm gonna

100
00:03:24,799 --> 00:03:27,440
give you a high-level idea of how our

101
00:03:27,440 --> 00:03:29,360
attack works

102
00:03:29,360 --> 00:03:31,440
but before that let me give you a

103
00:03:31,440 --> 00:03:33,280
summary of the instahyde's

104
00:03:33,280 --> 00:03:36,400
encoding algorithm so this algorithm has

105
00:03:36,400 --> 00:03:38,159
access to two datasets

106
00:03:38,159 --> 00:03:40,319
one private dataset that is a labeled

107
00:03:40,319 --> 00:03:42,799
dataset as a source of learning

108
00:03:42,799 --> 00:03:45,360
and there is also a public dataset that

109
00:03:45,360 --> 00:03:47,360
is not labeled and we don't

110
00:03:47,360 --> 00:03:49,040
care about its privacy and it's only

111
00:03:49,040 --> 00:03:52,720
there to help the encoder

112
00:03:53,040 --> 00:03:55,599
so how is the encoding done so first the

113
00:03:55,599 --> 00:03:56,799
encoding algorithm

114
00:03:56,799 --> 00:03:58,480
samples a bunch of images from the

115
00:03:58,480 --> 00:04:00,879
public and private data set and combines

116
00:04:00,879 --> 00:04:02,879
them linearly together to get this

117
00:04:02,879 --> 00:04:04,720
combination

118
00:04:04,720 --> 00:04:06,640
and after that there is this masking

119
00:04:06,640 --> 00:04:08,319
layer where

120
00:04:08,319 --> 00:04:11,360
each pixel in this image has its sign

121
00:04:11,360 --> 00:04:14,720
flipped with a random randomly with

122
00:04:14,720 --> 00:04:15,840
probability half

123
00:04:15,840 --> 00:04:18,639
so after that we get this image that

124
00:04:18,639 --> 00:04:20,959
doesn't look like anything to human

125
00:04:20,959 --> 00:04:22,960
but if you gather enough of them and

126
00:04:22,960 --> 00:04:25,360
feed that to a learning algorithm

127
00:04:25,360 --> 00:04:27,759
it still is able to learn a good

128
00:04:27,759 --> 00:04:29,759
classifier out of these encodings which

129
00:04:29,759 --> 00:04:33,040
is surprising on its own

130
00:04:33,360 --> 00:04:37,040
now how do we attach these encodings

131
00:04:37,040 --> 00:04:38,800
so we have this challenge dataset that

132
00:04:38,800 --> 00:04:41,040
is a bunch of encodings and we are

133
00:04:41,040 --> 00:04:42,560
supposed to

134
00:04:42,560 --> 00:04:44,639
invert them to the original images that

135
00:04:44,639 --> 00:04:47,680
were used to produce them

136
00:04:47,680 --> 00:04:49,520
so the first step of attack is the

137
00:04:49,520 --> 00:04:50,880
clustering step

138
00:04:50,880 --> 00:04:53,120
where the goal is to match these

139
00:04:53,120 --> 00:04:55,280
encodings together based on which is

140
00:04:55,280 --> 00:04:58,560
which image they are encoding so we

141
00:04:58,560 --> 00:05:00,400
train a neural network to do this

142
00:05:00,400 --> 00:05:01,759
clustering for us

143
00:05:01,759 --> 00:05:03,600
and after that we have this recovery

144
00:05:03,600 --> 00:05:05,280
step where we use the

145
00:05:05,280 --> 00:05:07,199
the cluster information to come up with

146
00:05:07,199 --> 00:05:10,320
the image corresponding to each cluster

147
00:05:10,320 --> 00:05:12,080
after these two steps we can actually

148
00:05:12,080 --> 00:05:14,720
get a very good recovery of the images

149
00:05:14,720 --> 00:05:16,880
i'm not going to elaborate more of about

150
00:05:16,880 --> 00:05:18,639
the deals of these steps but i suggest

151
00:05:18,639 --> 00:05:21,600
you to have a look at the paper for that

152
00:05:21,600 --> 00:05:24,800
but here is a a sample of uh

153
00:05:24,800 --> 00:05:26,400
what our attack they are so the

154
00:05:26,400 --> 00:05:28,400
extracted images that you see here are

155
00:05:28,400 --> 00:05:30,400
very close to the original images that

156
00:05:30,400 --> 00:05:31,919
were used to encode the

157
00:05:31,919 --> 00:05:33,919
uh challenged interest in the instant

158
00:05:33,919 --> 00:05:36,400
challenge so this shows that instahyde

159
00:05:36,400 --> 00:05:38,880
is not as private as first stop

160
00:05:38,880 --> 00:05:41,280
and we basically have a complete break

161
00:05:41,280 --> 00:05:44,080
of their challenge

162
00:05:44,720 --> 00:05:46,240
additional to our attack against

163
00:05:46,240 --> 00:05:48,240
instahart we also have some theoretical

164
00:05:48,240 --> 00:05:50,240
results showing some barriers against

165
00:05:50,240 --> 00:05:50,960
achieving

166
00:05:50,960 --> 00:05:54,799
privacy through instance encoding

167
00:05:55,680 --> 00:05:57,440
as we want to prove theorems here we

168
00:05:57,440 --> 00:05:59,600
have to work with formal definitions of

169
00:05:59,600 --> 00:06:01,440
privacy for instance encoding and

170
00:06:01,440 --> 00:06:03,520
instahype doesn't come with any formal

171
00:06:03,520 --> 00:06:04,319
definition

172
00:06:04,319 --> 00:06:07,440
so we had to define our own so we have

173
00:06:07,440 --> 00:06:08,880
this protocol

174
00:06:08,880 --> 00:06:11,759
with encoding and we also have a fixed

175
00:06:11,759 --> 00:06:13,120
concept function

176
00:06:13,120 --> 00:06:14,720
for example you can think of the dog

177
00:06:14,720 --> 00:06:16,800
versus cat concept function

178
00:06:16,800 --> 00:06:18,880
and we define a security game for this

179
00:06:18,880 --> 00:06:20,479
setting

180
00:06:20,479 --> 00:06:22,720
so in our security game the first step

181
00:06:22,720 --> 00:06:24,560
the adversary selects two

182
00:06:24,560 --> 00:06:26,400
instances that are labeled the same

183
00:06:26,400 --> 00:06:28,720
according to the constant function

184
00:06:28,720 --> 00:06:30,479
so for example it should be either two

185
00:06:30,479 --> 00:06:32,720
dark photos or two platforms

186
00:06:32,720 --> 00:06:35,280
and then the challenger will encode one

187
00:06:35,280 --> 00:06:36,639
of these two images

188
00:06:36,639 --> 00:06:38,319
or instances and sends it to the

189
00:06:38,319 --> 00:06:40,479
adversary and now the adversary has to

190
00:06:40,479 --> 00:06:42,639
decide which one of the

191
00:06:42,639 --> 00:06:44,479
images will use to come up with this

192
00:06:44,479 --> 00:06:46,560
encoding

193
00:06:46,560 --> 00:06:48,880
so the advantage of the adversary here

194
00:06:48,880 --> 00:06:50,080
is the probability

195
00:06:50,080 --> 00:06:52,240
of guessing correctly which image was

196
00:06:52,240 --> 00:06:54,639
encoded

197
00:06:54,639 --> 00:06:56,160
note that here we are forcing the

198
00:06:56,160 --> 00:06:58,080
adversary to submit two images with the

199
00:06:58,080 --> 00:07:00,479
same label because we don't want

200
00:07:00,479 --> 00:07:03,039
the advantage to increase by leveraging

201
00:07:03,039 --> 00:07:03,680
the

202
00:07:03,680 --> 00:07:05,599
label information that exists in the

203
00:07:05,599 --> 00:07:08,159
encodings

204
00:07:09,199 --> 00:07:11,840
now that we have our privacy definition

205
00:07:11,840 --> 00:07:13,440
we can ask this question

206
00:07:13,440 --> 00:07:15,280
can we achieve privacy and accuracy

207
00:07:15,280 --> 00:07:17,919
together using instance and code

208
00:07:17,919 --> 00:07:19,759
it's important that we want to achieve

209
00:07:19,759 --> 00:07:21,599
them together because achieving privacy

210
00:07:21,599 --> 00:07:24,960
alone is not a hard task

211
00:07:24,960 --> 00:07:26,800
but there are two ways that we can

212
00:07:26,800 --> 00:07:29,520
define accuracy here

213
00:07:29,520 --> 00:07:32,479
the first way is by defining accuracy on

214
00:07:32,479 --> 00:07:34,800
top of the encoded data

215
00:07:34,800 --> 00:07:36,720
so in this setting we have our first

216
00:07:36,720 --> 00:07:38,560
barrier that says that if a learning

217
00:07:38,560 --> 00:07:40,319
protocol with encoding can achieve

218
00:07:40,319 --> 00:07:41,680
accuracy half plus

219
00:07:41,680 --> 00:07:45,440
epsilon on two uh for two different

220
00:07:45,440 --> 00:07:47,120
concept functions

221
00:07:47,120 --> 00:07:48,560
then there is a polynomial time

222
00:07:48,560 --> 00:07:50,639
adversary that can break the security

223
00:07:50,639 --> 00:07:51,039
gain

224
00:07:51,039 --> 00:07:53,599
for both of c1 c2 with advantage half

225
00:07:53,599 --> 00:07:54,479
plus omega

226
00:07:54,479 --> 00:07:58,879
epsilon so the key idea here is that

227
00:07:58,879 --> 00:08:02,720
if the algorithm uh is going to achieve

228
00:08:02,720 --> 00:08:05,840
high accuracy on the encoded data

229
00:08:05,840 --> 00:08:07,599
this means that the encodings should

230
00:08:07,599 --> 00:08:10,879
have information about both c1 and c2

231
00:08:10,879 --> 00:08:13,759
uh in there so this means the adversary

232
00:08:13,759 --> 00:08:14,560
can actually

233
00:08:14,560 --> 00:08:16,400
use the information about one of the

234
00:08:16,400 --> 00:08:18,400
concepts to break the security game for

235
00:08:18,400 --> 00:08:20,960
the other one

236
00:08:20,960 --> 00:08:23,120
so this idea only works if we define

237
00:08:23,120 --> 00:08:24,960
accuracy on encoded data

238
00:08:24,960 --> 00:08:27,199
so we have another theorem about the

239
00:08:27,199 --> 00:08:28,720
case that you define accuracy on

240
00:08:28,720 --> 00:08:30,000
original data

241
00:08:30,000 --> 00:08:32,399
so in this setting we have a very

242
00:08:32,399 --> 00:08:33,440
similar theorem

243
00:08:33,440 --> 00:08:35,440
but with the difference that our

244
00:08:35,440 --> 00:08:36,799
advantage here

245
00:08:36,799 --> 00:08:39,440
depends on the sample complexity so the

246
00:08:39,440 --> 00:08:41,839
advantage of adversaries here is

247
00:08:41,839 --> 00:08:44,000
half plus one over n where n is the

248
00:08:44,000 --> 00:08:46,240
number of examples in the training

249
00:08:46,240 --> 00:08:47,839
so this is a slightly worse than the

250
00:08:47,839 --> 00:08:50,320
previous bound but the still sufficient

251
00:08:50,320 --> 00:08:51,200
to prove

252
00:08:51,200 --> 00:08:54,399
impossibility of achieving cryptographic

253
00:08:54,399 --> 00:08:57,040
levels of privacy for instance encoding

254
00:08:57,040 --> 00:09:00,160
together with accuracy

255
00:09:00,160 --> 00:09:01,760
so that was the first part of the talk

256
00:09:01,760 --> 00:09:04,720
i'll leave the rest to florian

257
00:09:04,720 --> 00:09:06,320
for the second part of this talk we'll

258
00:09:06,320 --> 00:09:07,839
look a bit more closely at the insta

259
00:09:07,839 --> 00:09:09,519
hype challenge that was released by the

260
00:09:09,519 --> 00:09:10,640
offers

261
00:09:10,640 --> 00:09:12,320
releasing security challenge is not

262
00:09:12,320 --> 00:09:14,000
uncommon in machine learning security

263
00:09:14,000 --> 00:09:14,959
these days

264
00:09:14,959 --> 00:09:16,399
i will now discuss some of the goals

265
00:09:16,399 --> 00:09:18,080
that we should strive for when designing

266
00:09:18,080 --> 00:09:19,839
security challenges and the role they

267
00:09:19,839 --> 00:09:20,560
should play in

268
00:09:20,560 --> 00:09:23,360
arguing security of the system the main

269
00:09:23,360 --> 00:09:24,959
message i want you to take away here is

270
00:09:24,959 --> 00:09:26,720
that a security channel should never be

271
00:09:26,720 --> 00:09:28,399
used as a substitute for a formal

272
00:09:28,399 --> 00:09:30,080
security claim

273
00:09:30,080 --> 00:09:32,000
in fact when we design a secure system

274
00:09:32,000 --> 00:09:33,839
the very first thing we should do is to

275
00:09:33,839 --> 00:09:35,600
provide a falsifiable claim of the

276
00:09:35,600 --> 00:09:37,200
security goals that the system is meant

277
00:09:37,200 --> 00:09:37,839
to achieve

278
00:09:37,839 --> 00:09:40,080
within a formally defined threat model

279
00:09:40,080 --> 00:09:41,680
this is the single most important step

280
00:09:41,680 --> 00:09:43,040
if we want to treat security as a

281
00:09:43,040 --> 00:09:44,640
scientific discipline

282
00:09:44,640 --> 00:09:46,240
after that it's of course great if we

283
00:09:46,240 --> 00:09:47,920
can prove something about the security

284
00:09:47,920 --> 00:09:48,480
claims

285
00:09:48,480 --> 00:09:49,920
but that isn't always feasible and

286
00:09:49,920 --> 00:09:52,320
that's okay instead of a formal proof or

287
00:09:52,320 --> 00:09:53,519
maybe in addition to one

288
00:09:53,519 --> 00:09:55,040
we can also publish a challenge to

289
00:09:55,040 --> 00:09:56,880
invite the community to try and attack

290
00:09:56,880 --> 00:09:58,480
our system

291
00:09:58,480 --> 00:10:00,560
and such a challenge should be designed

292
00:10:00,560 --> 00:10:02,560
to capture a necessary condition for the

293
00:10:02,560 --> 00:10:03,760
system's security

294
00:10:03,760 --> 00:10:05,440
so that if someone solves a challenge

295
00:10:05,440 --> 00:10:07,760
this implies that the system is insecure

296
00:10:07,760 --> 00:10:09,680
the converse is not always true usually

297
00:10:09,680 --> 00:10:11,519
the failure to solve a challenge is not

298
00:10:11,519 --> 00:10:13,120
enough to convince us of a system

299
00:10:13,120 --> 00:10:14,880
security

300
00:10:14,880 --> 00:10:16,800
let's look at a specific example that

301
00:10:16,800 --> 00:10:18,720
everyone's probably familiar with rsa

302
00:10:18,720 --> 00:10:19,600
encryption

303
00:10:19,600 --> 00:10:21,360
today we have strong formal security

304
00:10:21,360 --> 00:10:22,640
claims for rsa

305
00:10:22,640 --> 00:10:24,800
for example that it is indistinguishable

306
00:10:24,800 --> 00:10:26,640
under chosen ciphertext attacks with

307
00:10:26,640 --> 00:10:28,160
proper padding

308
00:10:28,160 --> 00:10:30,000
but note that this security claim cannot

309
00:10:30,000 --> 00:10:31,680
be directly captured by a static

310
00:10:31,680 --> 00:10:33,200
challenge because it involves multiple

311
00:10:33,200 --> 00:10:34,640
rounds of interaction between an

312
00:10:34,640 --> 00:10:36,959
adversary and victim

313
00:10:36,959 --> 00:10:38,959
but stating challenges are still very

314
00:10:38,959 --> 00:10:40,560
useful to measure and drive research

315
00:10:40,560 --> 00:10:41,120
progress

316
00:10:41,120 --> 00:10:43,200
for example two decades ago rsa

317
00:10:43,200 --> 00:10:44,560
published a suite of factoring

318
00:10:44,560 --> 00:10:45,440
challenges

319
00:10:45,440 --> 00:10:47,279
and these challenges satisfy our earlier

320
00:10:47,279 --> 00:10:49,360
stated goal and that solving them

321
00:10:49,360 --> 00:10:51,680
indeed would falsify rsa's formal

322
00:10:51,680 --> 00:10:53,120
security guarantees

323
00:10:53,120 --> 00:10:54,480
but we don't know if the converse is

324
00:10:54,480 --> 00:10:56,399
true it might be possible to break rsa

325
00:10:56,399 --> 00:10:59,519
in other ways than the factory

326
00:10:59,519 --> 00:11:01,839
while such challenges can be useful it's

327
00:11:01,839 --> 00:11:03,360
also very well accepted within the

328
00:11:03,360 --> 00:11:05,360
crypto community that a static challenge

329
00:11:05,360 --> 00:11:07,440
is in itself not sufficient

330
00:11:07,440 --> 00:11:09,440
as a measure of security even if the

331
00:11:09,440 --> 00:11:11,040
challenge is hard to solve

332
00:11:11,040 --> 00:11:12,800
because by that metric the cipher used

333
00:11:12,800 --> 00:11:14,560
by the zodiac killer could even be

334
00:11:14,560 --> 00:11:15,760
considered secure

335
00:11:15,760 --> 00:11:18,480
since it took over 50 years to crack it

336
00:11:18,480 --> 00:11:20,000
but it definitely isn't considered

337
00:11:20,000 --> 00:11:21,920
secure according to any modern notion of

338
00:11:21,920 --> 00:11:24,480
cryptographic security

339
00:11:24,480 --> 00:11:26,000
so let's now talk about the instant

340
00:11:26,000 --> 00:11:27,440
height challenge which was released to

341
00:11:27,440 --> 00:11:28,560
test the security

342
00:11:28,560 --> 00:11:30,880
of the instahyde protocol this challenge

343
00:11:30,880 --> 00:11:32,800
consists of 100 images that were each

344
00:11:32,800 --> 00:11:35,120
encoded 50 times using some undisclosed

345
00:11:35,120 --> 00:11:36,399
public dataset

346
00:11:36,399 --> 00:11:37,760
and to solve the challenge we have to

347
00:11:37,760 --> 00:11:39,680
reconstruct the close approximation of

348
00:11:39,680 --> 00:11:42,160
these 100 private images

349
00:11:42,160 --> 00:11:43,839
we use the attack that they presented

350
00:11:43,839 --> 00:11:45,360
earlier to solve this challenge and we

351
00:11:45,360 --> 00:11:45,760
cover

352
00:11:45,760 --> 00:11:48,640
all the private images so according to

353
00:11:48,640 --> 00:11:50,000
the goal we stated earlier for a

354
00:11:50,000 --> 00:11:51,120
security challenge

355
00:11:51,120 --> 00:11:52,720
this should mean that our attack

356
00:11:52,720 --> 00:11:54,880
invalidates instahid security claims

357
00:11:54,880 --> 00:11:56,320
right

358
00:11:56,320 --> 00:11:58,000
it turns out that no attack can actually

359
00:11:58,000 --> 00:11:59,760
do that because the insta hype protocol

360
00:11:59,760 --> 00:12:01,440
does not come with any formal and

361
00:12:01,440 --> 00:12:03,279
falsifiable security claims

362
00:12:03,279 --> 00:12:06,160
all we have is this challenge and this

363
00:12:06,160 --> 00:12:07,440
is a major problem because in the

364
00:12:07,440 --> 00:12:09,279
absence of the formal security model

365
00:12:09,279 --> 00:12:10,639
the only thing we're left with is the

366
00:12:10,639 --> 00:12:11,920
threat model that the challenge

367
00:12:11,920 --> 00:12:13,279
implicitly defines

368
00:12:13,279 --> 00:12:14,959
namely that an attacker gets access to

369
00:12:14,959 --> 00:12:16,800
50 encodings and has to recover the

370
00:12:16,800 --> 00:12:18,320
private images

371
00:12:18,320 --> 00:12:19,440
but this threat model is a bit

372
00:12:19,440 --> 00:12:21,519
simplistic and to make things worse

373
00:12:21,519 --> 00:12:22,720
there are actually ways in which the

374
00:12:22,720 --> 00:12:24,480
challenge makes the attacker's job both

375
00:12:24,480 --> 00:12:25,200
easier

376
00:12:25,200 --> 00:12:27,600
and harder than in an actual deployment

377
00:12:27,600 --> 00:12:29,279
of instaheim

378
00:12:29,279 --> 00:12:31,120
for example a real adversary would

379
00:12:31,120 --> 00:12:32,880
probably have some prior knowledge about

380
00:12:32,880 --> 00:12:34,959
the type of data that is being encoded

381
00:12:34,959 --> 00:12:37,680
and what public dataset is being used

382
00:12:37,680 --> 00:12:39,040
the adversary might also be able to

383
00:12:39,040 --> 00:12:41,200
actively participate in the protocol

384
00:12:41,200 --> 00:12:43,040
and aim for a slightly weaker goal than

385
00:12:43,040 --> 00:12:44,560
full reconstruction

386
00:12:44,560 --> 00:12:46,079
the challenge doesn't allow for these

387
00:12:46,079 --> 00:12:47,680
things and so in this sense solving the

388
00:12:47,680 --> 00:12:48,480
challenge is

389
00:12:48,480 --> 00:12:50,000
harder than breaking the instahype

390
00:12:50,000 --> 00:12:52,399
protocol which is fine

391
00:12:52,399 --> 00:12:53,760
the bigger issue is that the challenge

392
00:12:53,760 --> 00:12:55,360
also makes some things too easy for the

393
00:12:55,360 --> 00:12:56,000
attacker

394
00:12:56,000 --> 00:12:58,880
such as giving direct access to many

395
00:12:58,880 --> 00:13:00,800
encoded inputs computed over a small

396
00:13:00,800 --> 00:13:02,000
data set

397
00:13:02,000 --> 00:13:03,519
and our main attack on the challenge

398
00:13:03,519 --> 00:13:04,839
obviously exploits all of these

399
00:13:04,839 --> 00:13:06,240
capabilities

400
00:13:06,240 --> 00:13:07,519
but this also means that whether the

401
00:13:07,519 --> 00:13:09,200
challenge gets solved or not cannot

402
00:13:09,200 --> 00:13:10,959
conclusively tell us anything about

403
00:13:10,959 --> 00:13:12,399
insta height security

404
00:13:12,399 --> 00:13:15,360
in a different threat model for example

405
00:13:15,360 --> 00:13:17,200
we also consider a slightly different

406
00:13:17,200 --> 00:13:18,639
threat model in our paper

407
00:13:18,639 --> 00:13:20,639
where the adversary does have some

408
00:13:20,639 --> 00:13:22,320
auxiliary knowledge on the data that

409
00:13:22,320 --> 00:13:24,000
instahype protects and in particular we

410
00:13:24,000 --> 00:13:25,200
assume the adversary has

411
00:13:25,200 --> 00:13:27,839
access to the public data set and under

412
00:13:27,839 --> 00:13:29,440
these assumptions we give a very

413
00:13:29,440 --> 00:13:31,360
efficient attack that scales linearly

414
00:13:31,360 --> 00:13:33,360
with the size of the encoded data

415
00:13:33,360 --> 00:13:35,440
and can recover a private image given a

416
00:13:35,440 --> 00:13:37,680
single encoding

417
00:13:37,680 --> 00:13:39,600
in many ways this attack might be more

418
00:13:39,600 --> 00:13:41,279
realistic than the one we use to solve

419
00:13:41,279 --> 00:13:42,160
the challenge

420
00:13:42,160 --> 00:13:43,600
but this attack fails to solve the

421
00:13:43,600 --> 00:13:45,199
challenge because it operates in a

422
00:13:45,199 --> 00:13:47,199
different thread model

423
00:13:47,199 --> 00:13:48,639
so this brings us back to our main

424
00:13:48,639 --> 00:13:50,480
takeaway here that a challenge is really

425
00:13:50,480 --> 00:13:51,920
not a substitute for

426
00:13:51,920 --> 00:13:53,839
formal security claims in a well-defined

427
00:13:53,839 --> 00:13:55,440
threat model of course

428
00:13:55,440 --> 00:13:56,959
this doesn't mean things would have been

429
00:13:56,959 --> 00:13:58,880
better if instahyde hadn't come with a

430
00:13:58,880 --> 00:13:59,600
challenge

431
00:13:59,600 --> 00:14:01,360
publishing a well-designed challenge is

432
00:14:01,360 --> 00:14:03,120
always a good thing but we

433
00:14:03,120 --> 00:14:04,880
always first need formal security

434
00:14:04,880 --> 00:14:07,120
definitions and security claims to base

435
00:14:07,120 --> 00:14:09,120
the challenge on

436
00:14:09,120 --> 00:14:10,720
taking a step back i'll conclude the

437
00:14:10,720 --> 00:14:12,639
talk by noting that many of the issues

438
00:14:12,639 --> 00:14:14,240
that are work raised for instance

439
00:14:14,240 --> 00:14:16,800
encoding schemes are not uncommon in the

440
00:14:16,800 --> 00:14:18,079
broader space of machine learning

441
00:14:18,079 --> 00:14:19,680
security today

442
00:14:19,680 --> 00:14:21,120
and that there are many systems that are

443
00:14:21,120 --> 00:14:22,959
being designed and proposed for various

444
00:14:22,959 --> 00:14:25,040
security properties of machine learning

445
00:14:25,040 --> 00:14:26,639
but these properties are often not

446
00:14:26,639 --> 00:14:28,240
defined very formally

447
00:14:28,240 --> 00:14:30,560
but rather expressed as informal

448
00:14:30,560 --> 00:14:32,720
desiderata about the costs of successful

449
00:14:32,720 --> 00:14:33,600
attacks

450
00:14:33,600 --> 00:14:35,519
and in turn the evaluations of these

451
00:14:35,519 --> 00:14:37,120
systems are also often

452
00:14:37,120 --> 00:14:39,120
incomplete and typically limited to

453
00:14:39,120 --> 00:14:41,920
testing that prior attacks fail

454
00:14:41,920 --> 00:14:43,360
i think that these problems are exactly

455
00:14:43,360 --> 00:14:44,720
what people in this audience are

456
00:14:44,720 --> 00:14:46,560
uniquely qualified to tackle

457
00:14:46,560 --> 00:14:48,639
if you like attacks that's good news for

458
00:14:48,639 --> 00:14:50,079
you because there's currently way more

459
00:14:50,079 --> 00:14:51,920
new systems being proposed here that can

460
00:14:51,920 --> 00:14:53,920
be rigorously evaluated

461
00:14:53,920 --> 00:14:55,680
and if you like formal security and

462
00:14:55,680 --> 00:14:57,120
coming up with nice definitions and

463
00:14:57,120 --> 00:14:58,000
threat models

464
00:14:58,000 --> 00:14:59,839
there is also a huge amount of cool work

465
00:14:59,839 --> 00:15:01,199
to be done in this area

466
00:15:01,199 --> 00:15:03,040
towards the goal of a much more rigorous

467
00:15:03,040 --> 00:15:04,639
and formally grounded approach to

468
00:15:04,639 --> 00:15:06,399
machine learning security

469
00:15:06,399 --> 00:15:09,839
thank you

