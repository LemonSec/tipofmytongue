1
00:00:00,480 --> 00:00:02,240
hello everyone thank you for attending

2
00:00:02,240 --> 00:00:04,319
this session i am so excited to be here

3
00:00:04,319 --> 00:00:06,000
today to chat with you about some recent

4
00:00:06,000 --> 00:00:07,919
work we've been doing in unpacking and

5
00:00:07,919 --> 00:00:10,240
understanding online hate and harassment

6
00:00:10,240 --> 00:00:11,679
this work is done in collaboration with

7
00:00:11,679 --> 00:00:13,280
some awesome folks from google

8
00:00:13,280 --> 00:00:16,400
uiuc stanford cornell tech nyu

9
00:00:16,400 --> 00:00:19,680
ucl and bu now before we begin

10
00:00:19,680 --> 00:00:21,199
i want to just briefly mention that this

11
00:00:21,199 --> 00:00:22,880
presentation will be talking about

12
00:00:22,880 --> 00:00:24,720
potentially difficult experiences so if

13
00:00:24,720 --> 00:00:25,519
you want to pause

14
00:00:25,519 --> 00:00:27,439
or skip over this video now is a good

15
00:00:27,439 --> 00:00:30,160
opportunity to do so

16
00:00:30,160 --> 00:00:32,640
okay let's get started so if you spend

17
00:00:32,640 --> 00:00:34,160
even a little bit of time on the

18
00:00:34,160 --> 00:00:34,719
internet

19
00:00:34,719 --> 00:00:36,719
you've probably heard about increasingly

20
00:00:36,719 --> 00:00:38,480
toxic online environments

21
00:00:38,480 --> 00:00:40,079
now without additional context you might

22
00:00:40,079 --> 00:00:41,680
think hate and harassment experiences

23
00:00:41,680 --> 00:00:44,239
mostly consist of bullying or trolling

24
00:00:44,239 --> 00:00:46,079
but actually there is an increasingly

25
00:00:46,079 --> 00:00:48,160
diverse set of complicated ways

26
00:00:48,160 --> 00:00:49,920
that folks are being targeted by hate

27
00:00:49,920 --> 00:00:51,440
and harassment online

28
00:00:51,440 --> 00:00:54,320
so for example in 2016 leslie jones

29
00:00:54,320 --> 00:00:56,000
famous for her roles on snl

30
00:00:56,000 --> 00:00:58,160
was the target of a coordinated campaign

31
00:00:58,160 --> 00:00:59,760
of toxic comments and hate

32
00:00:59,760 --> 00:01:01,680
wherein thousands of internet accounts

33
00:01:01,680 --> 00:01:03,039
sent her threatening messages

34
00:01:03,039 --> 00:01:04,879
with the eventual result of her leaving

35
00:01:04,879 --> 00:01:07,520
social media platforms entirely

36
00:01:07,520 --> 00:01:09,119
and in another example the twitch

37
00:01:09,119 --> 00:01:11,119
streamer nate hill was swatted

38
00:01:11,119 --> 00:01:12,720
while streaming which is when an

39
00:01:12,720 --> 00:01:14,720
attacker maliciously calls law

40
00:01:14,720 --> 00:01:16,479
enforcement to a target's address

41
00:01:16,479 --> 00:01:18,479
under false pretenses of abuse or

42
00:01:18,479 --> 00:01:19,920
misconduct

43
00:01:19,920 --> 00:01:21,680
and these are not just attacks that

44
00:01:21,680 --> 00:01:23,520
impact high profile people

45
00:01:23,520 --> 00:01:24,960
studies from the pew research center in

46
00:01:24,960 --> 00:01:26,560
microsoft tell us that these attacks are

47
00:01:26,560 --> 00:01:28,960
widespread with an estimated 41 percent

48
00:01:28,960 --> 00:01:30,000
of people in the us

49
00:01:30,000 --> 00:01:32,880
and 40 of people globally reporting alma

50
00:01:32,880 --> 00:01:34,400
experiencing online hate and harassment

51
00:01:34,400 --> 00:01:35,680
in some ways

52
00:01:35,680 --> 00:01:37,520
and these kinds of online experiences

53
00:01:37,520 --> 00:01:39,520
often have devastating effects

54
00:01:39,520 --> 00:01:42,240
on internet users mental emotional and

55
00:01:42,240 --> 00:01:43,680
physical health

56
00:01:43,680 --> 00:01:45,280
i've described just a handful of the

57
00:01:45,280 --> 00:01:46,960
kinds of attacks that fall within scope

58
00:01:46,960 --> 00:01:47,439
here

59
00:01:47,439 --> 00:01:49,439
so what is it that ties all of these

60
00:01:49,439 --> 00:01:51,680
internet experiences together

61
00:01:51,680 --> 00:01:54,880
well it's the intent of the attacker

62
00:01:54,880 --> 00:01:56,240
and the intent of the attacker here is

63
00:01:56,240 --> 00:01:58,240
to inflict emotional harm

64
00:01:58,240 --> 00:02:00,799
to instill a sense of fear or to control

65
00:02:00,799 --> 00:02:02,880
the target of the abuse in some way

66
00:02:02,880 --> 00:02:04,560
and this is a marked difference than the

67
00:02:04,560 --> 00:02:06,640
traditional forms of abuse the security

68
00:02:06,640 --> 00:02:08,959
community has historically dealt with

69
00:02:08,959 --> 00:02:10,720
incentives for what i'll call classic

70
00:02:10,720 --> 00:02:12,000
abuse like spam

71
00:02:12,000 --> 00:02:14,160
phishing and ddos is often profit

72
00:02:14,160 --> 00:02:15,200
motivated

73
00:02:15,200 --> 00:02:17,120
but online abuse how internet

74
00:02:17,120 --> 00:02:19,200
adversaries use technology to harm

75
00:02:19,200 --> 00:02:20,080
people

76
00:02:20,080 --> 00:02:22,640
is changing and over the course of this

77
00:02:22,640 --> 00:02:23,120
talk

78
00:02:23,120 --> 00:02:24,720
i'm going to make the argument that we

79
00:02:24,720 --> 00:02:26,160
the security community

80
00:02:26,160 --> 00:02:28,959
can and should step up to address online

81
00:02:28,959 --> 00:02:30,959
hate and harassment as a core security

82
00:02:30,959 --> 00:02:31,920
problem

83
00:02:31,920 --> 00:02:33,280
now i'll add one thing here which is

84
00:02:33,280 --> 00:02:34,560
that the attacks that we'll be talking

85
00:02:34,560 --> 00:02:35,040
about

86
00:02:35,040 --> 00:02:37,920
are enabled in some part by technology

87
00:02:37,920 --> 00:02:38,879
and as such

88
00:02:38,879 --> 00:02:40,959
we'll be discussing how technology can

89
00:02:40,959 --> 00:02:42,080
play a role

90
00:02:42,080 --> 00:02:45,040
in the solution so to start reasoning

91
00:02:45,040 --> 00:02:45,680
about this

92
00:02:45,680 --> 00:02:47,760
we went to the computing literature we

93
00:02:47,760 --> 00:02:49,599
examined the last five years of research

94
00:02:49,599 --> 00:02:50,959
on online hate and harassment from

95
00:02:50,959 --> 00:02:51,519
oakland

96
00:02:51,519 --> 00:02:55,920
usernet security ccs kai cscw and icwsm

97
00:02:55,920 --> 00:02:58,000
which are prominent hci conferences

98
00:02:58,000 --> 00:03:01,280
the web conference soups and imc

99
00:03:01,280 --> 00:03:03,760
we use this foundation as a set of seed

100
00:03:03,760 --> 00:03:04,640
papers to find

101
00:03:04,640 --> 00:03:06,159
other related work from other

102
00:03:06,159 --> 00:03:07,920
disciplines in social sciences

103
00:03:07,920 --> 00:03:09,680
and then expanded that set to also

104
00:03:09,680 --> 00:03:11,599
include major news events from the last

105
00:03:11,599 --> 00:03:12,400
several years

106
00:03:12,400 --> 00:03:15,200
to help contextualize the research work

107
00:03:15,200 --> 00:03:15,920
in total

108
00:03:15,920 --> 00:03:18,560
we reviewed over 150 research papers and

109
00:03:18,560 --> 00:03:20,080
news articles and online hate and

110
00:03:20,080 --> 00:03:21,360
harassment

111
00:03:21,360 --> 00:03:23,040
and through that process we started to

112
00:03:23,040 --> 00:03:24,799
build out a threat model

113
00:03:24,799 --> 00:03:26,560
defining the targets of the abuse and

114
00:03:26,560 --> 00:03:28,720
the attackers that perpetrate it

115
00:03:28,720 --> 00:03:30,799
the targets in abusive attacks can be an

116
00:03:30,799 --> 00:03:32,400
individual but can also be

117
00:03:32,400 --> 00:03:34,239
groups of people and oftentimes these

118
00:03:34,239 --> 00:03:36,239
are groups historically at risk of broad

119
00:03:36,239 --> 00:03:38,159
harassment like religious minorities

120
00:03:38,159 --> 00:03:40,879
or members of the lgbtq community and

121
00:03:40,879 --> 00:03:43,200
again an attacker's main goal here is to

122
00:03:43,200 --> 00:03:45,360
emotionally harm or coercively control

123
00:03:45,360 --> 00:03:46,080
the target

124
00:03:46,080 --> 00:03:48,239
and given that broad goal attackers can

125
00:03:48,239 --> 00:03:50,000
often take on many different shapes

126
00:03:50,000 --> 00:03:51,599
depending on the kind of attack they

127
00:03:51,599 --> 00:03:52,480
want to conduct

128
00:03:52,480 --> 00:03:54,720
so for example on the one end an

129
00:03:54,720 --> 00:03:55,599
attacker could be

130
00:03:55,599 --> 00:03:58,159
highly privileged like a spouse family

131
00:03:58,159 --> 00:03:59,280
or peers who have

132
00:03:59,280 --> 00:04:01,439
regular or consistent access to a

133
00:04:01,439 --> 00:04:03,360
target's devices and in some cases

134
00:04:03,360 --> 00:04:05,920
their internet connection altogether on

135
00:04:05,920 --> 00:04:08,159
the other end we have seemingly faceless

136
00:04:08,159 --> 00:04:08,799
anonymous

137
00:04:08,799 --> 00:04:11,120
internet mobs who although maybe easy to

138
00:04:11,120 --> 00:04:12,319
ignore independently

139
00:04:12,319 --> 00:04:14,159
might use their collective power to

140
00:04:14,159 --> 00:04:16,160
bombard and overwhelm a target with

141
00:04:16,160 --> 00:04:17,918
toxic messages

142
00:04:17,918 --> 00:04:19,600
in our threat model we make no

143
00:04:19,600 --> 00:04:21,040
assumption about the capabilities of

144
00:04:21,040 --> 00:04:21,680
attackers

145
00:04:21,680 --> 00:04:23,280
any communication channel between the

146
00:04:23,280 --> 00:04:25,840
attacker and target or any protections

147
00:04:25,840 --> 00:04:27,120
available to targets

148
00:04:27,120 --> 00:04:29,600
and we do this because online hate and

149
00:04:29,600 --> 00:04:30,880
harassment attacks

150
00:04:30,880 --> 00:04:32,880
often take on many different forms and

151
00:04:32,880 --> 00:04:34,639
each contains a unique set

152
00:04:34,639 --> 00:04:36,400
of properties and criterion that

153
00:04:36,400 --> 00:04:38,240
differentiate them from one another

154
00:04:38,240 --> 00:04:40,000
so through our literature review we

155
00:04:40,000 --> 00:04:41,919
identified three broad criteria that

156
00:04:41,919 --> 00:04:43,280
capture the hate and harassment attacks

157
00:04:43,280 --> 00:04:44,000
we observed

158
00:04:44,000 --> 00:04:45,520
these pertain to the intended audience

159
00:04:45,520 --> 00:04:47,520
of the attack the medium that the attack

160
00:04:47,520 --> 00:04:48,240
takes on

161
00:04:48,240 --> 00:04:50,240
and the fundamental capabilities of the

162
00:04:50,240 --> 00:04:51,520
attacker

163
00:04:51,520 --> 00:04:53,120
so let me go into detail and provide

164
00:04:53,120 --> 00:04:54,880
some examples so in the audience

165
00:04:54,880 --> 00:04:55,680
category

166
00:04:55,680 --> 00:04:57,600
our two criterion are whether the attack

167
00:04:57,600 --> 00:04:59,919
is intended to be seen by the target

168
00:04:59,919 --> 00:05:02,080
or intended to be seen by an audience or

169
00:05:02,080 --> 00:05:03,120
by the public

170
00:05:03,120 --> 00:05:05,280
so for example online bullying or

171
00:05:05,280 --> 00:05:07,919
trolling is used specifically to inflict

172
00:05:07,919 --> 00:05:10,160
direct emotional harm to the target and

173
00:05:10,160 --> 00:05:11,520
so is an example of being

174
00:05:11,520 --> 00:05:13,680
explicitly intended to be seen by that

175
00:05:13,680 --> 00:05:14,800
target

176
00:05:14,800 --> 00:05:16,479
doxxing on the other hand which is when

177
00:05:16,479 --> 00:05:18,160
an attacker searches for and then

178
00:05:18,160 --> 00:05:19,039
publishes

179
00:05:19,039 --> 00:05:21,120
private or identifying information about

180
00:05:21,120 --> 00:05:22,720
a target to the public internet

181
00:05:22,720 --> 00:05:25,280
gains its power from disseminating data

182
00:05:25,280 --> 00:05:25,919
widely

183
00:05:25,919 --> 00:05:27,440
and so is an example of needing to be

184
00:05:27,440 --> 00:05:29,360
seen by an audience

185
00:05:29,360 --> 00:05:30,960
we know that these categories are not

186
00:05:30,960 --> 00:05:32,800
mutually exclusive so though the main

187
00:05:32,800 --> 00:05:34,880
objective of online bullying may be to

188
00:05:34,880 --> 00:05:37,199
target an individual the attacker may

189
00:05:37,199 --> 00:05:38,960
also want to encourage others to bully

190
00:05:38,960 --> 00:05:39,840
that individual

191
00:05:39,840 --> 00:05:41,759
which can then benefit from their

192
00:05:41,759 --> 00:05:43,280
attacks being seen

193
00:05:43,280 --> 00:05:46,400
by the public also important

194
00:05:46,400 --> 00:05:47,919
distinguishing these kinds of attacks

195
00:05:47,919 --> 00:05:49,360
are the medium used to deliver the

196
00:05:49,360 --> 00:05:51,520
attacks so not every attack uses some

197
00:05:51,520 --> 00:05:53,680
kind of media for example text images or

198
00:05:53,680 --> 00:05:54,160
videos

199
00:05:54,160 --> 00:05:55,919
to harass someone so for example

200
00:05:55,919 --> 00:05:57,840
swatting which leverages an external

201
00:05:57,840 --> 00:06:00,000
party to inflict harm and often has

202
00:06:00,000 --> 00:06:03,039
no direct link between the attacker and

203
00:06:03,039 --> 00:06:05,280
the target of the abuse

204
00:06:05,280 --> 00:06:07,520
and finally attackers often require

205
00:06:07,520 --> 00:06:08,960
different capabilities to conduct

206
00:06:08,960 --> 00:06:10,319
different kinds of attacks so an

207
00:06:10,319 --> 00:06:11,520
attacker may need to

208
00:06:11,520 --> 00:06:13,600
deceive an audience to conduct attacks

209
00:06:13,600 --> 00:06:15,600
like an impersonated profile or a deep

210
00:06:15,600 --> 00:06:16,160
fake

211
00:06:16,160 --> 00:06:17,280
which takes the likeness of an

212
00:06:17,280 --> 00:06:19,520
individual and has them say or do things

213
00:06:19,520 --> 00:06:20,000
that never

214
00:06:20,000 --> 00:06:22,479
happened similarly an attacker may need

215
00:06:22,479 --> 00:06:24,560
to deceive a third party authority

216
00:06:24,560 --> 00:06:26,639
like a platform in the case of false

217
00:06:26,639 --> 00:06:28,160
abuse reporting

218
00:06:28,160 --> 00:06:30,160
other attacks may require amplifications

219
00:06:30,160 --> 00:06:32,479
for example attacks like raiding or dog

220
00:06:32,479 --> 00:06:33,199
piling

221
00:06:33,199 --> 00:06:35,039
that require hundreds of accounts to

222
00:06:35,039 --> 00:06:37,280
target an individual all at once

223
00:06:37,280 --> 00:06:40,080
require amplification and finally

224
00:06:40,080 --> 00:06:41,600
attacks may require some privileged

225
00:06:41,600 --> 00:06:43,280
access information so for example

226
00:06:43,280 --> 00:06:45,039
surveillance attacks often require

227
00:06:45,039 --> 00:06:46,960
direct access to targets devices or

228
00:06:46,960 --> 00:06:48,800
internet connection that most remote

229
00:06:48,800 --> 00:06:49,759
adversaries

230
00:06:49,759 --> 00:06:52,560
would not have access to through our

231
00:06:52,560 --> 00:06:53,520
systematization

232
00:06:53,520 --> 00:06:56,000
we came up with seven broad classes of

233
00:06:56,000 --> 00:06:57,520
online hate and harassment attacks

234
00:06:57,520 --> 00:06:59,919
that capture the 38 attacks that we

235
00:06:59,919 --> 00:07:01,759
observed in our literature review now

236
00:07:01,759 --> 00:07:03,120
you might have picked up on this in the

237
00:07:03,120 --> 00:07:04,639
way that we're describing each attack

238
00:07:04,639 --> 00:07:05,120
but

239
00:07:05,120 --> 00:07:07,120
tying this back to security we also

240
00:07:07,120 --> 00:07:08,639
describe how each

241
00:07:08,639 --> 00:07:11,199
attack violates a fundamental computer

242
00:07:11,199 --> 00:07:12,560
security principle either

243
00:07:12,560 --> 00:07:14,840
confidentiality integrity or

244
00:07:14,840 --> 00:07:16,000
availability

245
00:07:16,000 --> 00:07:17,520
so for example with attacks like

246
00:07:17,520 --> 00:07:19,199
bullying or trolling which fall into the

247
00:07:19,199 --> 00:07:20,880
toxic content attack type

248
00:07:20,880 --> 00:07:22,479
this targets the availability of the

249
00:07:22,479 --> 00:07:24,639
system to users so if a regular

250
00:07:24,639 --> 00:07:26,639
authorized user can't use the system due

251
00:07:26,639 --> 00:07:28,000
to constant harassment

252
00:07:28,000 --> 00:07:30,479
you've got an availability problem and

253
00:07:30,479 --> 00:07:32,240
in the case of content leakage so for

254
00:07:32,240 --> 00:07:33,599
example doxing or

255
00:07:33,599 --> 00:07:35,840
leaking non-consensual intimate imagery

256
00:07:35,840 --> 00:07:37,440
you're breaching the confidentiality of

257
00:07:37,440 --> 00:07:38,720
the target

258
00:07:38,720 --> 00:07:40,639
impersonation attacks muddy the

259
00:07:40,639 --> 00:07:41,759
integrity of the system

260
00:07:41,759 --> 00:07:43,199
making it harder to believe that what

261
00:07:43,199 --> 00:07:44,560
you're seeing on the platform is

262
00:07:44,560 --> 00:07:45,599
actually accurate

263
00:07:45,599 --> 00:07:48,080
and trustworthy and going one step

264
00:07:48,080 --> 00:07:50,000
further each of these attacks also has

265
00:07:50,000 --> 00:07:52,879
analogs to classic abuse so in one

266
00:07:52,879 --> 00:07:55,039
example with an attack like overloading

267
00:07:55,039 --> 00:07:56,800
where a target is overwhelmed by

268
00:07:56,800 --> 00:07:58,240
unwanted notifications

269
00:07:58,240 --> 00:08:00,080
and messages often from hundreds of

270
00:08:00,080 --> 00:08:01,280
different accounts

271
00:08:01,280 --> 00:08:04,160
well this feels a lot like for-profit

272
00:08:04,160 --> 00:08:05,120
ddos

273
00:08:05,120 --> 00:08:06,879
and we as a security community have

274
00:08:06,879 --> 00:08:08,960
studied botnets and ddos and the way

275
00:08:08,960 --> 00:08:10,960
that coordination happens

276
00:08:10,960 --> 00:08:12,800
on the internet many of those techniques

277
00:08:12,800 --> 00:08:14,160
can be great places

278
00:08:14,160 --> 00:08:15,440
and starting to understand the

279
00:08:15,440 --> 00:08:17,360
overloading problem

280
00:08:17,360 --> 00:08:19,280
now we go into much more detail about

281
00:08:19,280 --> 00:08:20,879
these attacks where they fall into the

282
00:08:20,879 --> 00:08:21,759
taxonomy

283
00:08:21,759 --> 00:08:23,280
and the specific connections to classic

284
00:08:23,280 --> 00:08:25,199
abuse in the paper but if there's one

285
00:08:25,199 --> 00:08:26,800
big thing i want you to take away

286
00:08:26,800 --> 00:08:28,160
from going through this exercise of

287
00:08:28,160 --> 00:08:29,919
systematizing each attack

288
00:08:29,919 --> 00:08:31,759
it's that online and harassment attacks

289
00:08:31,759 --> 00:08:33,599
are just as diverse

290
00:08:33,599 --> 00:08:35,919
as traditional online abuse attacks and

291
00:08:35,919 --> 00:08:37,200
so there's not going to be one single

292
00:08:37,200 --> 00:08:37,839
solution

293
00:08:37,839 --> 00:08:39,919
no magic machine learning classifier no

294
00:08:39,919 --> 00:08:40,958
easy fix

295
00:08:40,958 --> 00:08:42,479
to address all of these challenges

296
00:08:42,479 --> 00:08:44,800
simultaneously the only way to address

297
00:08:44,800 --> 00:08:45,279
it

298
00:08:45,279 --> 00:08:47,519
is a concerted effort from industry and

299
00:08:47,519 --> 00:08:49,440
the research community to study

300
00:08:49,440 --> 00:08:53,120
understand and combat these problems

301
00:08:53,120 --> 00:08:55,760
so we talked about attacks what we next

302
00:08:55,760 --> 00:08:56,320
turn to

303
00:08:56,320 --> 00:08:57,839
is understanding how prevalent these

304
00:08:57,839 --> 00:08:59,920
attacks are understanding what people

305
00:08:59,920 --> 00:09:01,440
are experiencing today

306
00:09:01,440 --> 00:09:03,360
can inform where we as a research

307
00:09:03,360 --> 00:09:04,800
community might want to focus

308
00:09:04,800 --> 00:09:07,680
our initial attention and our effort so

309
00:09:07,680 --> 00:09:08,959
to do this we developed a survey

310
00:09:08,959 --> 00:09:09,600
instrument

311
00:09:09,600 --> 00:09:11,200
that surveyed approximately a thousand

312
00:09:11,200 --> 00:09:12,959
participants from 22 different countries

313
00:09:12,959 --> 00:09:14,320
for three years

314
00:09:14,320 --> 00:09:15,760
and asked them about their hate and

315
00:09:15,760 --> 00:09:17,600
harassment experiences

316
00:09:17,600 --> 00:09:19,120
we solicited these participants through

317
00:09:19,120 --> 00:09:20,640
an expert panel from a leading market

318
00:09:20,640 --> 00:09:21,519
research firm

319
00:09:21,519 --> 00:09:23,040
who also handled country-specific

320
00:09:23,040 --> 00:09:24,640
translations and identified what

321
00:09:24,640 --> 00:09:25,440
information

322
00:09:25,440 --> 00:09:26,720
we could safely collect from

323
00:09:26,720 --> 00:09:28,640
participants around the world

324
00:09:28,640 --> 00:09:30,480
and we note that for maximal coverage of

325
00:09:30,480 --> 00:09:31,600
countries around the world some

326
00:09:31,600 --> 00:09:33,040
countries don't appear in all three

327
00:09:33,040 --> 00:09:33,839
years of data

328
00:09:33,839 --> 00:09:34,959
and we exclude those when we're

329
00:09:34,959 --> 00:09:37,440
comparing year-over-year trends

330
00:09:37,440 --> 00:09:40,240
our core question uh was for

331
00:09:40,240 --> 00:09:41,680
participants was have you ever

332
00:09:41,680 --> 00:09:42,959
personally experienced

333
00:09:42,959 --> 00:09:45,600
x online where x contained a varied list

334
00:09:45,600 --> 00:09:47,040
of hate and harassment experiences

335
00:09:47,040 --> 00:09:49,200
informed by similar prior work in a

336
00:09:49,200 --> 00:09:50,160
space

337
00:09:50,160 --> 00:09:51,760
and finally we collected demographic

338
00:09:51,760 --> 00:09:53,440
data for example their gender whether

339
00:09:53,440 --> 00:09:54,959
they identified as lgbtq

340
00:09:54,959 --> 00:09:57,920
their age and their social media usage

341
00:09:57,920 --> 00:09:58,399
so

342
00:09:58,399 --> 00:10:00,240
what attacks are people around the world

343
00:10:00,240 --> 00:10:01,440
experiencing

344
00:10:01,440 --> 00:10:02,640
what i'm showing you here is the

345
00:10:02,640 --> 00:10:04,320
fraction of participants globally that

346
00:10:04,320 --> 00:10:06,079
noted that they had experienced one of

347
00:10:06,079 --> 00:10:07,120
these attacks

348
00:10:07,120 --> 00:10:08,959
and what we found is that the most

349
00:10:08,959 --> 00:10:10,480
prevalent attack participants in our

350
00:10:10,480 --> 00:10:11,440
service experienced

351
00:10:11,440 --> 00:10:14,399
were toxic content attacks 19 of

352
00:10:14,399 --> 00:10:15,920
participants were insulted

353
00:10:15,920 --> 00:10:18,959
or treated unkindly followed by 16 who

354
00:10:18,959 --> 00:10:20,560
had someone make hateful comments

355
00:10:20,560 --> 00:10:23,200
and 14 said they'd been called offensive

356
00:10:23,200 --> 00:10:24,720
names

357
00:10:24,720 --> 00:10:26,800
this is followed by a smaller prevalence

358
00:10:26,800 --> 00:10:27,839
but often more

359
00:10:27,839 --> 00:10:30,000
severe abusive experiences so for

360
00:10:30,000 --> 00:10:32,240
example six percent of participants

361
00:10:32,240 --> 00:10:34,160
have their online accounts hacked by

362
00:10:34,160 --> 00:10:35,279
somebody that they know

363
00:10:35,279 --> 00:10:36,800
that this is akin to a privileged

364
00:10:36,800 --> 00:10:38,880
attacker or an inside attacker

365
00:10:38,880 --> 00:10:40,480
and four percent of participants had

366
00:10:40,480 --> 00:10:42,320
someone use spyware to monitor their

367
00:10:42,320 --> 00:10:43,200
activities

368
00:10:43,200 --> 00:10:44,800
highlighting the diversity of harassment

369
00:10:44,800 --> 00:10:46,320
experiences that exist

370
00:10:46,320 --> 00:10:49,360
across the world going further than that

371
00:10:49,360 --> 00:10:50,959
it turns out that depending on where you

372
00:10:50,959 --> 00:10:52,000
live in the world

373
00:10:52,000 --> 00:10:53,360
your experiences with online hate and

374
00:10:53,360 --> 00:10:55,360
harassment vary so for example

375
00:10:55,360 --> 00:10:57,839
more than 70 of participants from kenya

376
00:10:57,839 --> 00:10:59,680
and nigeria reported experiencing online

377
00:10:59,680 --> 00:11:00,640
hate and harassment

378
00:11:00,640 --> 00:11:02,399
compared to approximately 40 of

379
00:11:02,399 --> 00:11:04,880
participants from the united states

380
00:11:04,880 --> 00:11:06,800
and of course it's not just where you

381
00:11:06,800 --> 00:11:08,160
live that define your harassment

382
00:11:08,160 --> 00:11:08,959
experiences

383
00:11:08,959 --> 00:11:11,279
it's also shaped by the multitude of

384
00:11:11,279 --> 00:11:12,560
your identities

385
00:11:12,560 --> 00:11:14,000
so to measure this we modeled

386
00:11:14,000 --> 00:11:15,839
experiencing any form of hate and

387
00:11:15,839 --> 00:11:17,920
harassment as a binomial distribution

388
00:11:17,920 --> 00:11:19,839
and essentially performed a regression

389
00:11:19,839 --> 00:11:21,600
with categorical demographic data as

390
00:11:21,600 --> 00:11:22,320
inputs

391
00:11:22,320 --> 00:11:23,600
and for the 12 countries we had year

392
00:11:23,600 --> 00:11:25,279
over year data we also modeled

393
00:11:25,279 --> 00:11:27,360
harassment experiences as a function

394
00:11:27,360 --> 00:11:30,240
of the year what we found was that the

395
00:11:30,240 --> 00:11:32,079
odds that young participants

396
00:11:32,079 --> 00:11:35,279
those ages 18 to 24 and those who

397
00:11:35,279 --> 00:11:37,760
identified as lgbtq experiencing online

398
00:11:37,760 --> 00:11:38,480
harassment

399
00:11:38,480 --> 00:11:41,279
was 1.9 to 4 times higher than reference

400
00:11:41,279 --> 00:11:43,279
groups in each category

401
00:11:43,279 --> 00:11:44,800
and to make matters worse hate

402
00:11:44,800 --> 00:11:46,640
harassment experiences are increasing

403
00:11:46,640 --> 00:11:47,839
year over year

404
00:11:47,839 --> 00:11:49,920
compared to 2016 the first year in our

405
00:11:49,920 --> 00:11:52,000
survey the odds of experiencing hate and

406
00:11:52,000 --> 00:11:52,720
harassment

407
00:11:52,720 --> 00:11:56,079
globally in 2017 increased by 1.2 times

408
00:11:56,079 --> 00:11:57,279
and the odds of experiencing hate and

409
00:11:57,279 --> 00:11:59,920
harassment in 2018 increased by 1.3

410
00:11:59,920 --> 00:12:00,800
times

411
00:12:00,800 --> 00:12:03,200
and recent research from pew in 2021

412
00:12:03,200 --> 00:12:04,160
suggests that

413
00:12:04,160 --> 00:12:06,560
even if the experiences themselves are

414
00:12:06,560 --> 00:12:08,320
not drastically changing

415
00:12:08,320 --> 00:12:10,959
in prevalence over time the intensity of

416
00:12:10,959 --> 00:12:12,160
those experiences

417
00:12:12,160 --> 00:12:13,600
and the effects that they seem to have

418
00:12:13,600 --> 00:12:17,279
on internet users are actually deepening

419
00:12:17,279 --> 00:12:19,600
so what we're seeing then is not only

420
00:12:19,600 --> 00:12:20,880
that there's not going to be one

421
00:12:20,880 --> 00:12:22,320
technical solution to online hate and

422
00:12:22,320 --> 00:12:23,120
harassment

423
00:12:23,120 --> 00:12:25,120
but that designing defenses has to take

424
00:12:25,120 --> 00:12:27,040
into account the online

425
00:12:27,040 --> 00:12:29,200
diverse experiences that internet users

426
00:12:29,200 --> 00:12:31,440
have so understanding that first

427
00:12:31,440 --> 00:12:32,880
and then baking that into our design

428
00:12:32,880 --> 00:12:34,399
recommendations is going to be an

429
00:12:34,399 --> 00:12:35,279
important step

430
00:12:35,279 --> 00:12:37,680
in building defenses that work for all

431
00:12:37,680 --> 00:12:40,000
internet users

432
00:12:40,000 --> 00:12:42,240
so i want to conclude with a discussion

433
00:12:42,240 --> 00:12:43,680
of some of the research directions

434
00:12:43,680 --> 00:12:45,680
that we think can have immediate impact

435
00:12:45,680 --> 00:12:47,440
and that the security and anti-abuse

436
00:12:47,440 --> 00:12:49,519
communities already have some experience

437
00:12:49,519 --> 00:12:50,399
with

438
00:12:50,399 --> 00:12:52,079
so in the paper we outline five

439
00:12:52,079 --> 00:12:53,279
different research areas that are

440
00:12:53,279 --> 00:12:53,920
informed

441
00:12:53,920 --> 00:12:56,320
by both the literature and what industry

442
00:12:56,320 --> 00:12:58,240
is trying right now to curb hate and

443
00:12:58,240 --> 00:12:59,760
harassment on their platforms

444
00:12:59,760 --> 00:13:02,480
so these are nudges moderation automated

445
00:13:02,480 --> 00:13:03,279
detection

446
00:13:03,279 --> 00:13:05,839
conscious design and a focus on policies

447
00:13:05,839 --> 00:13:07,200
and education

448
00:13:07,200 --> 00:13:08,480
so we go into much more detail about

449
00:13:08,480 --> 00:13:10,079
these areas in the paper but the one

450
00:13:10,079 --> 00:13:11,440
thing i'd like to call out is that the

451
00:13:11,440 --> 00:13:12,639
security community

452
00:13:12,639 --> 00:13:14,480
already has a lot of experience working

453
00:13:14,480 --> 00:13:16,800
with these kinds of interventions

454
00:13:16,800 --> 00:13:18,720
we can rely on our experiences with

455
00:13:18,720 --> 00:13:20,240
security indicators and security

456
00:13:20,240 --> 00:13:20,959
warnings

457
00:13:20,959 --> 00:13:22,880
our expertise in building automated

458
00:13:22,880 --> 00:13:24,800
anomaly detection or automated systems

459
00:13:24,800 --> 00:13:26,160
for ddos protection

460
00:13:26,160 --> 00:13:28,320
and even draw on our work in evaluating

461
00:13:28,320 --> 00:13:29,839
security advice

462
00:13:29,839 --> 00:13:33,440
to help in this new space of course

463
00:13:33,440 --> 00:13:35,200
any of these solutions are not without

464
00:13:35,200 --> 00:13:36,480
their challenges

465
00:13:36,480 --> 00:13:38,399
so in giving users more control we also

466
00:13:38,399 --> 00:13:40,800
run the risk of burdening them

467
00:13:40,800 --> 00:13:42,560
in building better tools for moderation

468
00:13:42,560 --> 00:13:44,720
we have to balance and grapple with the

469
00:13:44,720 --> 00:13:46,639
risk of censorship of thought

470
00:13:46,639 --> 00:13:48,399
and as we move to a more end-to-end

471
00:13:48,399 --> 00:13:49,680
encrypted world

472
00:13:49,680 --> 00:13:51,920
we have to question how we balance user

473
00:13:51,920 --> 00:13:52,880
privacy

474
00:13:52,880 --> 00:13:55,360
with platform accountability the last

475
00:13:55,360 --> 00:13:56,720
thing i'll say is despite all of these

476
00:13:56,720 --> 00:13:57,680
similarities

477
00:13:57,680 --> 00:13:59,680
modern abuse is different than classic

478
00:13:59,680 --> 00:14:00,959
abuse in the

479
00:14:00,959 --> 00:14:03,680
harms the harms are broader and have the

480
00:14:03,680 --> 00:14:06,000
potential to impact every aspect of a

481
00:14:06,000 --> 00:14:07,360
person's life

482
00:14:07,360 --> 00:14:09,360
keeping this in mind and maintaining

483
00:14:09,360 --> 00:14:11,199
respect for both the targets of the

484
00:14:11,199 --> 00:14:11,920
abuse

485
00:14:11,920 --> 00:14:14,160
and those that are doing the work will

486
00:14:14,160 --> 00:14:17,760
be very important as we move forward

487
00:14:17,760 --> 00:14:20,160
so to wrap up we talked about how online

488
00:14:20,160 --> 00:14:21,199
abuse is changing

489
00:14:21,199 --> 00:14:22,959
and that new forms of online abuse hate

490
00:14:22,959 --> 00:14:24,639
and harassment can be thought of as a

491
00:14:24,639 --> 00:14:25,519
security problem

492
00:14:25,519 --> 00:14:27,760
that we should work towards tackling it

493
00:14:27,760 --> 00:14:28,720
second online

494
00:14:28,720 --> 00:14:30,560
harassment is a pernicious problem that

495
00:14:30,560 --> 00:14:32,399
is growing over time and especially

496
00:14:32,399 --> 00:14:34,079
dangerous to some internet users

497
00:14:34,079 --> 00:14:35,839
highlighting a need to design techniques

498
00:14:35,839 --> 00:14:37,839
that can support all internet users and

499
00:14:37,839 --> 00:14:39,360
not just a few

500
00:14:39,360 --> 00:14:41,199
and finally many of the techniques and

501
00:14:41,199 --> 00:14:43,360
defenses that are either already in use

502
00:14:43,360 --> 00:14:44,399
or being piloted

503
00:14:44,399 --> 00:14:45,839
are those that are well studied by the

504
00:14:45,839 --> 00:14:48,079
security community and we can and should

505
00:14:48,079 --> 00:14:49,519
draw on our past experiences and

506
00:14:49,519 --> 00:14:50,880
tackling online abuse

507
00:14:50,880 --> 00:14:53,600
to address these new challenges with

508
00:14:53,600 --> 00:14:54,079
that

509
00:14:54,079 --> 00:14:56,000
i'll conclude i'll leave my information

510
00:14:56,000 --> 00:14:57,199
up here for those that have questions or

511
00:14:57,199 --> 00:14:58,639
want to contact me

512
00:14:58,639 --> 00:15:01,519
thanks again for listening

