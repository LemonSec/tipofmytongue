1
00:00:00,320 --> 00:00:02,240
hello everyone i'm xiwi lee from

2
00:00:02,240 --> 00:00:03,919
columbia university

3
00:00:03,919 --> 00:00:05,920
today i'm going to present our work

4
00:00:05,920 --> 00:00:07,040
about a secure

5
00:00:07,040 --> 00:00:09,360
and formally verified linux kvm

6
00:00:09,360 --> 00:00:11,040
hypervisor

7
00:00:11,040 --> 00:00:12,719
the shift to cloud computing has been

8
00:00:12,719 --> 00:00:15,200
one of the most significant tech trends

9
00:00:15,200 --> 00:00:17,199
of the past few years due to the

10
00:00:17,199 --> 00:00:19,279
increasing demands in online services

11
00:00:19,279 --> 00:00:22,480
and the support for scalability cloud

12
00:00:22,480 --> 00:00:23,359
computing

13
00:00:23,359 --> 00:00:25,359
is supported by a technology called

14
00:00:25,359 --> 00:00:26,720
virtualization

15
00:00:26,720 --> 00:00:28,880
that allows multiple virtual machines to

16
00:00:28,880 --> 00:00:31,279
run on a single piece of hardware

17
00:00:31,279 --> 00:00:33,360
these virtual machine center resources

18
00:00:33,360 --> 00:00:35,520
are managed by a privileged software

19
00:00:35,520 --> 00:00:37,440
called hypervisor

20
00:00:37,440 --> 00:00:40,559
popular hypervisors include kvm xen

21
00:00:40,559 --> 00:00:42,800
and hyper-v are deployed by the cloud

22
00:00:42,800 --> 00:00:44,079
vendors on server

23
00:00:44,079 --> 00:00:47,840
multiprocessor hardware hypervisors are

24
00:00:47,840 --> 00:00:49,440
increasingly complicated

25
00:00:49,440 --> 00:00:51,039
to support the performance and

26
00:00:51,039 --> 00:00:52,800
functionality required by cloud

27
00:00:52,800 --> 00:00:54,640
computing workloads

28
00:00:54,640 --> 00:00:57,120
the popular hypervisors include a full

29
00:00:57,120 --> 00:00:59,359
operating system kernel to reuse its

30
00:00:59,359 --> 00:01:02,239
existing functionality however the

31
00:01:02,239 --> 00:01:04,559
increasing complexity in hypervisors

32
00:01:04,559 --> 00:01:07,840
results in more bugs the security of

33
00:01:07,840 --> 00:01:08,720
users data

34
00:01:08,720 --> 00:01:11,040
in cloud virtual machines thus depends

35
00:01:11,040 --> 00:01:12,880
on this complicated and privileged

36
00:01:12,880 --> 00:01:14,720
hypervisor code base

37
00:01:14,720 --> 00:01:16,720
in which attackers that exploit

38
00:01:16,720 --> 00:01:19,040
vulnerabilities in the hypervisor

39
00:01:19,040 --> 00:01:21,360
can gain on fatal access to all hosts

40
00:01:21,360 --> 00:01:23,759
the vm's data

41
00:01:23,759 --> 00:01:26,080
one way to address this security issue

42
00:01:26,080 --> 00:01:28,159
is to formally verify the hypervisor

43
00:01:28,159 --> 00:01:29,119
code base

44
00:01:29,119 --> 00:01:31,600
ensuring it contains no vulnerabilities

45
00:01:31,600 --> 00:01:34,000
and protects virtual machine data

46
00:01:34,000 --> 00:01:36,159
however previous formal verification

47
00:01:36,159 --> 00:01:39,119
research focuses on simplistic systems

48
00:01:39,119 --> 00:01:41,360
designed for verification with limited

49
00:01:41,360 --> 00:01:42,960
functionality

50
00:01:42,960 --> 00:01:46,000
for instance both seo4 and cerakos do

51
00:01:46,000 --> 00:01:46,799
not support

52
00:01:46,799 --> 00:01:49,200
common hypervisor features such as

53
00:01:49,200 --> 00:01:51,600
multiprocessor virtual machines

54
00:01:51,600 --> 00:01:54,079
therefore these systems are not used in

55
00:01:54,079 --> 00:01:56,240
practice in the cloud

56
00:01:56,240 --> 00:01:58,719
in addition it requires significant

57
00:01:58,719 --> 00:02:00,560
effort to formally verify

58
00:02:00,560 --> 00:02:02,880
these simplistic systems with only few

59
00:02:02,880 --> 00:02:04,399
thousand lines

60
00:02:04,399 --> 00:02:06,960
in contrast commodity hypervisors like

61
00:02:06,960 --> 00:02:07,759
kvm

62
00:02:07,759 --> 00:02:09,758
includes more than 2 million slides of

63
00:02:09,758 --> 00:02:11,440
code

64
00:02:11,440 --> 00:02:13,680
the existing verification approaches are

65
00:02:13,680 --> 00:02:14,720
intractable for

66
00:02:14,720 --> 00:02:18,480
commodity hypervisors like kvm

67
00:02:18,480 --> 00:02:21,040
our hypothesis is instead of building

68
00:02:21,040 --> 00:02:23,120
new formally verified systems from

69
00:02:23,120 --> 00:02:24,080
scratch

70
00:02:24,080 --> 00:02:26,800
modest changes to the existing commodity

71
00:02:26,800 --> 00:02:29,440
systems like the kvm hypervisor

72
00:02:29,440 --> 00:02:31,599
can make it possible to verify their

73
00:02:31,599 --> 00:02:33,680
security properties

74
00:02:33,680 --> 00:02:35,920
and given the changes are modest we

75
00:02:35,920 --> 00:02:38,160
could thus retain the system's overall

76
00:02:38,160 --> 00:02:39,120
functionality

77
00:02:39,120 --> 00:02:42,480
and performance based on the hypothesis

78
00:02:42,480 --> 00:02:45,040
we introduced micro verification and

79
00:02:45,040 --> 00:02:45,760
approach to

80
00:02:45,760 --> 00:02:48,319
enable formal verification of security

81
00:02:48,319 --> 00:02:51,040
properties of the commodity systems

82
00:02:51,040 --> 00:02:53,360
micro verification reduces the proof

83
00:02:53,360 --> 00:02:55,599
effort by retrofitting a commodity

84
00:02:55,599 --> 00:02:57,440
system into a small core

85
00:02:57,440 --> 00:03:00,080
and a set of untrusted services so that

86
00:03:00,080 --> 00:03:01,519
it is possible to prove

87
00:03:01,519 --> 00:03:03,680
properties of the entire system by

88
00:03:03,680 --> 00:03:06,400
verifying the core alone

89
00:03:06,400 --> 00:03:08,400
we have applied micro verification to

90
00:03:08,400 --> 00:03:11,040
kvm to verify for the first time

91
00:03:11,040 --> 00:03:13,680
a commodity hypervisor protects vm

92
00:03:13,680 --> 00:03:14,800
confidentiality

93
00:03:14,800 --> 00:03:17,280
and integrity while preserving its

94
00:03:17,280 --> 00:03:18,720
overall feature set

95
00:03:18,720 --> 00:03:22,159
and performance i now discuss how we use

96
00:03:22,159 --> 00:03:24,400
micro verification to retrofit kvm

97
00:03:24,400 --> 00:03:27,440
to simplify verification effort

98
00:03:27,440 --> 00:03:29,599
micro verification leverages micro

99
00:03:29,599 --> 00:03:30,720
kernel design

100
00:03:30,720 --> 00:03:33,760
to retrofit kvm into sckvm

101
00:03:33,760 --> 00:03:36,480
which includes a small hypervisor core k

102
00:03:36,480 --> 00:03:37,040
core

103
00:03:37,040 --> 00:03:39,760
that protects vm data in cpu and memory

104
00:03:39,760 --> 00:03:41,760
which we verify

105
00:03:41,760 --> 00:03:44,560
sdk vm also includes a large set of

106
00:03:44,560 --> 00:03:46,720
untrusted hypervisor services

107
00:03:46,720 --> 00:03:49,360
k-serve that includes the linux kernel

108
00:03:49,360 --> 00:03:50,879
integrated with kvm

109
00:03:50,879 --> 00:03:52,720
to provide complex virtualization

110
00:03:52,720 --> 00:03:54,799
functionality

111
00:03:54,799 --> 00:03:57,439
sckvm leverages hardware virtualization

112
00:03:57,439 --> 00:03:58,000
features

113
00:03:58,000 --> 00:04:00,400
available on modern architectures to

114
00:04:00,400 --> 00:04:02,480
simplify the retrofit

115
00:04:02,480 --> 00:04:05,680
our sdk vm implementation supports our

116
00:04:05,680 --> 00:04:08,640
virtualization extensions as shown in

117
00:04:08,640 --> 00:04:09,040
the

118
00:04:09,040 --> 00:04:11,680
figure here sdk vm the privileges

119
00:04:11,680 --> 00:04:14,000
k-serve to arm's kernel mode

120
00:04:14,000 --> 00:04:17,279
e01 and runs k-core in e02

121
00:04:17,279 --> 00:04:20,238
a higher privileged hypervisor mode

122
00:04:20,238 --> 00:04:21,600
running in e02

123
00:04:21,600 --> 00:04:23,759
allows k-core to use the hardware

124
00:04:23,759 --> 00:04:25,280
virtualization features

125
00:04:25,280 --> 00:04:29,280
to protect vm data to protect vm memory

126
00:04:29,280 --> 00:04:32,400
k-core leverages nested page tables next

127
00:04:32,400 --> 00:04:33,759
the page tables

128
00:04:33,759 --> 00:04:35,280
were introduced by hardware

129
00:04:35,280 --> 00:04:37,600
virtualization extensions to enable

130
00:04:37,600 --> 00:04:40,160
a hypervisor to efficiently control what

131
00:04:40,160 --> 00:04:41,360
physical memory is

132
00:04:41,360 --> 00:04:44,560
accessible to virtual machine k-core not

133
00:04:44,560 --> 00:04:46,080
only uses the nested

134
00:04:46,080 --> 00:04:49,040
page tables to restrict the vms but also

135
00:04:49,040 --> 00:04:51,280
the hypervisor's accesses to physical

136
00:04:51,280 --> 00:04:52,560
memory

137
00:04:52,560 --> 00:04:54,720
in our implementation for arm k-core

138
00:04:54,720 --> 00:04:56,960
manages arms nested page tables

139
00:04:56,960 --> 00:05:00,400
stage 2 page tables k-core enables

140
00:05:00,400 --> 00:05:02,800
nested page tables when running k-surf

141
00:05:02,800 --> 00:05:05,280
and manages k-serve's nested page table

142
00:05:05,280 --> 00:05:07,759
to ensure the page table does not map to

143
00:05:07,759 --> 00:05:11,520
k-cores or a vm's private memory

144
00:05:11,520 --> 00:05:14,639
to simplify k-core sdk vm relies on

145
00:05:14,639 --> 00:05:17,360
k-serve for memory allocation

146
00:05:17,360 --> 00:05:19,919
k-core uses an identity mapping and

147
00:05:19,919 --> 00:05:21,840
k-serves nested page table

148
00:05:21,840 --> 00:05:24,320
so that a physical address viewed by

149
00:05:24,320 --> 00:05:24,960
k-serve

150
00:05:24,960 --> 00:05:27,840
is the same as that on the hardware this

151
00:05:27,840 --> 00:05:30,080
makes it possible for k-serve to reuse

152
00:05:30,080 --> 00:05:32,080
kvm's existing memory management

153
00:05:32,080 --> 00:05:35,280
functionality for allocating membrane

154
00:05:35,280 --> 00:05:38,400
to protect vm's data stored in cpu

155
00:05:38,400 --> 00:05:41,360
k-core ensures k-serve cannot access vm

156
00:05:41,360 --> 00:05:43,440
cpu data on the hardware

157
00:05:43,440 --> 00:05:46,240
when the vm is running k-core ensures

158
00:05:46,240 --> 00:05:48,639
only the vm itself has access to the cpu

159
00:05:48,639 --> 00:05:50,320
data on our hardware

160
00:05:50,320 --> 00:05:52,880
and when the vm is not running k-core

161
00:05:52,880 --> 00:05:56,240
saves the vm's cpu to its private memory

162
00:05:56,240 --> 00:05:59,280
thus in both cases the vm's cpu is

163
00:05:59,280 --> 00:06:02,639
protected here i use an example to show

164
00:06:02,639 --> 00:06:02,960
how

165
00:06:02,960 --> 00:06:05,440
k-core protects vm data when handling a

166
00:06:05,440 --> 00:06:08,880
page file from a vm's nested page table

167
00:06:08,880 --> 00:06:10,880
kcoor controls the hardware to trap the

168
00:06:10,880 --> 00:06:12,479
page file to itself

169
00:06:12,479 --> 00:06:14,479
which then switches to k-serve to

170
00:06:14,479 --> 00:06:17,440
allocate a new page for the faulty vm

171
00:06:17,440 --> 00:06:20,000
before entering k-serve k-core contact

172
00:06:20,000 --> 00:06:22,080
switches the hardware to k-surf

173
00:06:22,080 --> 00:06:24,639
ensuring that k-serve cannot access vm

174
00:06:24,639 --> 00:06:27,520
cpu data on the hardware

175
00:06:27,520 --> 00:06:30,479
after k-surf finishes memory allocation

176
00:06:30,479 --> 00:06:32,880
k-core unmapped the newly allocated page

177
00:06:32,880 --> 00:06:34,000
by k-serve from

178
00:06:34,000 --> 00:06:36,479
k-serve's nested page table ensuring

179
00:06:36,479 --> 00:06:39,520
that k-serve has no access to the page

180
00:06:39,520 --> 00:06:41,919
k-core then maps the page to the vm's

181
00:06:41,919 --> 00:06:43,280
nested page table

182
00:06:43,280 --> 00:06:46,639
to resolve the page fault finally k-core

183
00:06:46,639 --> 00:06:49,120
restores vm's cpu registers

184
00:06:49,120 --> 00:06:50,960
stored in its memory to the hardware

185
00:06:50,960 --> 00:06:53,520
before entering vm

186
00:06:53,520 --> 00:06:55,680
now that after the retrofit we have

187
00:06:55,680 --> 00:06:58,240
significantly reduced kvm's code base we

188
00:06:58,240 --> 00:06:59,520
need to prove

189
00:06:59,520 --> 00:07:02,080
i will then discuss how we verify that

190
00:07:02,080 --> 00:07:03,759
the kvm implementation

191
00:07:03,759 --> 00:07:07,680
protects vm data we first verify

192
00:07:07,680 --> 00:07:09,120
the functional correctness of the

193
00:07:09,120 --> 00:07:11,759
concurrent k-core implementation

194
00:07:11,759 --> 00:07:13,440
ensuring that the implementation

195
00:07:13,440 --> 00:07:16,319
contains no vulnerabilities

196
00:07:16,319 --> 00:07:18,479
we defined a specification that

197
00:07:18,479 --> 00:07:20,880
describes the behavior of k-core

198
00:07:20,880 --> 00:07:24,000
improved refinement verifying that

199
00:07:24,000 --> 00:07:26,319
k-core's implementation refines its

200
00:07:26,319 --> 00:07:28,800
specification

201
00:07:28,800 --> 00:07:30,639
since the implementation refines the

202
00:07:30,639 --> 00:07:32,160
specification

203
00:07:32,160 --> 00:07:34,400
we can use the specification to prove

204
00:07:34,400 --> 00:07:36,720
security properties of k-core

205
00:07:36,720 --> 00:07:38,319
which is easier than using the

206
00:07:38,319 --> 00:07:40,400
implementation directly

207
00:07:40,400 --> 00:07:42,520
specifically we show that the

208
00:07:42,520 --> 00:07:43,680
confidentiality

209
00:07:43,680 --> 00:07:46,720
and integrity of vm data is protected

210
00:07:46,720 --> 00:07:47,120
for

211
00:07:47,120 --> 00:07:50,000
any implementation of k-serve

212
00:07:50,000 --> 00:07:52,400
interacting with k-core

213
00:07:52,400 --> 00:07:55,599
therefore the security properties hold

214
00:07:55,599 --> 00:07:59,599
for the entire hypervisor

215
00:07:59,599 --> 00:08:01,280
to further reduce the verification

216
00:08:01,280 --> 00:08:03,039
effort we employed a

217
00:08:03,039 --> 00:08:05,120
layered approach that enables us to

218
00:08:05,120 --> 00:08:07,199
decompose the verification of k-core

219
00:08:07,199 --> 00:08:08,879
into simpler components

220
00:08:08,879 --> 00:08:12,240
that are easier to prove to verify the

221
00:08:12,240 --> 00:08:13,120
correctness of k

222
00:08:13,120 --> 00:08:15,680
core start from the bottom layer we

223
00:08:15,680 --> 00:08:17,680
incrementally prove the implementation

224
00:08:17,680 --> 00:08:18,639
in each layer

225
00:08:18,639 --> 00:08:21,759
refines its specification and gradually

226
00:08:21,759 --> 00:08:23,919
prove that k-core's top layer

227
00:08:23,919 --> 00:08:26,240
which includes the exception and hyper

228
00:08:26,240 --> 00:08:27,360
call handlers

229
00:08:27,360 --> 00:08:30,400
is refined by a stack of layers

230
00:08:30,400 --> 00:08:33,279
a key issue is refinement may hide

231
00:08:33,279 --> 00:08:35,279
intermediate information leakage

232
00:08:35,279 --> 00:08:37,599
and do not preserve security properties

233
00:08:37,599 --> 00:08:39,039
like confidentiality

234
00:08:39,039 --> 00:08:42,479
and integrity i use a nested page table

235
00:08:42,479 --> 00:08:44,880
example to illustrate the problem

236
00:08:44,880 --> 00:08:47,519
a multi-processor vm has shared nested

237
00:08:47,519 --> 00:08:48,720
page tables

238
00:08:48,720 --> 00:08:51,120
that is the page tables are accessible

239
00:08:51,120 --> 00:08:53,279
across multiple cpus

240
00:08:53,279 --> 00:08:55,120
therefore logs are used to protect

241
00:08:55,120 --> 00:08:57,279
hypervisors concurrent accesses to the

242
00:08:57,279 --> 00:08:59,760
nested page tables

243
00:08:59,760 --> 00:09:01,440
although the hypervisors access these

244
00:09:01,440 --> 00:09:02,959
students nested page tables are

245
00:09:02,959 --> 00:09:04,640
protected by logs

246
00:09:04,640 --> 00:09:06,880
a multi-processor vm can access the

247
00:09:06,880 --> 00:09:09,200
nested page table anytime

248
00:09:09,200 --> 00:09:12,399
as shown in the diagram the vm on cpu 1

249
00:09:12,399 --> 00:09:14,560
can read the guest physical address

250
00:09:14,560 --> 00:09:16,080
within the critical section

251
00:09:16,080 --> 00:09:19,120
of set mpt on cpu 0

252
00:09:19,120 --> 00:09:21,519
using the shared nested page table via

253
00:09:21,519 --> 00:09:23,519
the mmu concurrently

254
00:09:23,519 --> 00:09:26,800
bypassing logs consider an

255
00:09:26,800 --> 00:09:28,959
incorrect set mpt implementation here

256
00:09:28,959 --> 00:09:31,040
that makes an erroneous update

257
00:09:31,040 --> 00:09:33,360
to map the guest physical address to an

258
00:09:33,360 --> 00:09:34,880
address pa-1

259
00:09:34,880 --> 00:09:37,839
that corresponds to a private page owned

260
00:09:37,839 --> 00:09:39,839
by another vm

261
00:09:39,839 --> 00:09:42,399
assuming the hypervisor on cpu 0 calls

262
00:09:42,399 --> 00:09:44,240
this incorrect implementation

263
00:09:44,240 --> 00:09:47,760
as shown in the diagram the vm on cpu 1

264
00:09:47,760 --> 00:09:50,399
using the same nested page table that

265
00:09:50,399 --> 00:09:52,240
cpu0 updated

266
00:09:52,240 --> 00:09:56,080
can access the page pa1 at time t2

267
00:09:56,080 --> 00:09:59,200
which causes information leakage

268
00:09:59,200 --> 00:10:02,160
the goal for proving set mpt is to show

269
00:10:02,160 --> 00:10:02,720
that the

270
00:10:02,720 --> 00:10:04,959
implementation executing in the

271
00:10:04,959 --> 00:10:06,880
multi-processor environment

272
00:10:06,880 --> 00:10:09,600
refines the specification that specifies

273
00:10:09,600 --> 00:10:11,920
an atomic page table map

274
00:10:11,920 --> 00:10:14,880
so we can use the atomic specification

275
00:10:14,880 --> 00:10:15,200
that

276
00:10:15,200 --> 00:10:17,360
encapsulates the concurrent behaviors of

277
00:10:17,360 --> 00:10:18,240
the implementation

278
00:10:18,240 --> 00:10:21,440
for further reasoning

279
00:10:21,440 --> 00:10:24,000
however previous refinement approach

280
00:10:24,000 --> 00:10:24,560
hides

281
00:10:24,560 --> 00:10:26,880
the intermediate updates and do not

282
00:10:26,880 --> 00:10:28,800
distinguish between the correct

283
00:10:28,800 --> 00:10:31,839
and incorrect implementation for set mpt

284
00:10:31,839 --> 00:10:32,399
and would

285
00:10:32,399 --> 00:10:34,959
end up refining the two implementations

286
00:10:34,959 --> 00:10:37,360
to the same atomic spec

287
00:10:37,360 --> 00:10:39,279
hiding information leakage and not

288
00:10:39,279 --> 00:10:41,920
preserving security

289
00:10:41,920 --> 00:10:44,480
we introduced security precision layers

290
00:10:44,480 --> 00:10:45,200
to ensure

291
00:10:45,200 --> 00:10:47,120
refinement does not hide information

292
00:10:47,120 --> 00:10:48,880
leakage and preserves

293
00:10:48,880 --> 00:10:52,240
security properties security preserving

294
00:10:52,240 --> 00:10:52,880
layers

295
00:10:52,880 --> 00:10:55,279
employ transparent trace refinement to

296
00:10:55,279 --> 00:10:57,760
track intermediate updates

297
00:10:57,760 --> 00:10:59,680
transparent trace refinement ensures

298
00:10:59,680 --> 00:11:01,200
that the implementation

299
00:11:01,200 --> 00:11:04,000
reveals at most as much information as

300
00:11:04,000 --> 00:11:05,760
its specification

301
00:11:05,760 --> 00:11:07,839
such that the refinement can preserve

302
00:11:07,839 --> 00:11:09,440
security

303
00:11:09,440 --> 00:11:11,519
using security preserving layers we

304
00:11:11,519 --> 00:11:12,800
proved that the correct

305
00:11:12,800 --> 00:11:14,800
set mpt implementation refines the

306
00:11:14,800 --> 00:11:16,480
atomic specification

307
00:11:16,480 --> 00:11:18,560
but the incorrect implementation that

308
00:11:18,560 --> 00:11:22,000
includes multiple updates does not

309
00:11:22,000 --> 00:11:25,360
we prove sdk vm's protection of vm data

310
00:11:25,360 --> 00:11:27,920
over kcor's specification at its top

311
00:11:27,920 --> 00:11:28,640
layer

312
00:11:28,640 --> 00:11:31,279
for all principles including k-serves or

313
00:11:31,279 --> 00:11:33,839
vm's interaction with k-core

314
00:11:33,839 --> 00:11:35,920
using security preserving layers we

315
00:11:35,920 --> 00:11:38,000
guarantee that the security properties

316
00:11:38,000 --> 00:11:38,880
we verify

317
00:11:38,880 --> 00:11:41,600
over the spec holds on the

318
00:11:41,600 --> 00:11:44,079
implementation

319
00:11:44,079 --> 00:11:47,120
we formulate sckvm's vm security

320
00:11:47,120 --> 00:11:50,000
guarantees in terms of not interference

321
00:11:50,000 --> 00:11:52,320
the intuition here is we want to show

322
00:11:52,320 --> 00:11:53,519
that one principle's

323
00:11:53,519 --> 00:11:56,160
execution will not infer or affect

324
00:11:56,160 --> 00:11:57,040
others data

325
00:11:57,040 --> 00:12:00,000
so that the confidentiality and

326
00:12:00,000 --> 00:12:00,800
integrity of

327
00:12:00,800 --> 00:12:04,560
vm data is protected specifically we

328
00:12:04,560 --> 00:12:05,279
verify

329
00:12:05,279 --> 00:12:07,440
non-interference over the machine states

330
00:12:07,440 --> 00:12:10,320
that contains a given principles data

331
00:12:10,320 --> 00:12:13,200
like the portion for vma and b shown

332
00:12:13,200 --> 00:12:15,839
here in the diagram

333
00:12:15,839 --> 00:12:18,639
using micro verification we verified our

334
00:12:18,639 --> 00:12:20,480
sckvm implementation

335
00:12:20,480 --> 00:12:22,320
protects the confidentiality and

336
00:12:22,320 --> 00:12:24,720
integrity of vm data

337
00:12:24,720 --> 00:12:27,839
the k-core implementation in sdk vm is

338
00:12:27,839 --> 00:12:30,639
marginalized into 34 security preserving

339
00:12:30,639 --> 00:12:31,600
layers

340
00:12:31,600 --> 00:12:34,639
which as shown in the table includes 3.8

341
00:12:34,639 --> 00:12:36,880
k lines of c and assembly code that we

342
00:12:36,880 --> 00:12:38,480
verified

343
00:12:38,480 --> 00:12:41,200
as shown in the table k core includes

344
00:12:41,200 --> 00:12:44,000
the verified hacker library to support

345
00:12:44,000 --> 00:12:47,040
crypto functionality this also shows

346
00:12:47,040 --> 00:12:48,800
that the retrofit required

347
00:12:48,800 --> 00:12:51,519
modest effort as other than hackhole the

348
00:12:51,519 --> 00:12:52,880
rest of it only requires

349
00:12:52,880 --> 00:12:55,440
roughly 3k additional lines of code to

350
00:12:55,440 --> 00:12:57,200
kvm

351
00:12:57,200 --> 00:13:00,079
the proofs for sdk vm are written in cog

352
00:13:00,079 --> 00:13:01,600
and machine checked

353
00:13:01,600 --> 00:13:03,519
the implementation effort for our proofs

354
00:13:03,519 --> 00:13:04,720
as shown in the table

355
00:13:04,720 --> 00:13:06,720
is mostly for proving the correctness of

356
00:13:06,720 --> 00:13:07,839
k-core

357
00:13:07,839 --> 00:13:09,200
which includes the challenging

358
00:13:09,200 --> 00:13:11,279
refinement proofs for multi-level page

359
00:13:11,279 --> 00:13:12,720
tables

360
00:13:12,720 --> 00:13:15,680
in total of 6k lines of code were for

361
00:13:15,680 --> 00:13:16,880
k-course

362
00:13:16,880 --> 00:13:19,360
layer specifications which most of them

363
00:13:19,360 --> 00:13:20,000
were for

364
00:13:20,000 --> 00:13:22,160
the intermediate layers to help us do

365
00:13:22,160 --> 00:13:23,200
the refinement

366
00:13:23,200 --> 00:13:26,399
and only 1.7 k lines of code were for

367
00:13:26,399 --> 00:13:28,240
the top layer specification

368
00:13:28,240 --> 00:13:30,480
which specifies all of k core's

369
00:13:30,480 --> 00:13:32,320
behaviors

370
00:13:32,320 --> 00:13:34,560
are formally verified sdkvm

371
00:13:34,560 --> 00:13:35,519
implementation

372
00:13:35,519 --> 00:13:37,760
supports comprehensive virtualization

373
00:13:37,760 --> 00:13:38,560
features

374
00:13:38,560 --> 00:13:42,160
as shown here inherited from kvm while

375
00:13:42,160 --> 00:13:45,519
providing verified vm protection

376
00:13:45,519 --> 00:13:48,959
in contrast scl4 and certicos provide

377
00:13:48,959 --> 00:13:51,680
none of these features with verified vm

378
00:13:51,680 --> 00:13:53,680
protection

379
00:13:53,680 --> 00:13:55,519
we tested application workloads as

380
00:13:55,519 --> 00:13:57,839
listed in a table on real arm server

381
00:13:57,839 --> 00:14:00,240
class hardware running sdk vm

382
00:14:00,240 --> 00:14:03,279
the workloads include a mix of cpu and i

383
00:14:03,279 --> 00:14:04,880
o intensive benchmarks

384
00:14:04,880 --> 00:14:07,120
which some of them are commonly used

385
00:14:07,120 --> 00:14:08,320
server applications

386
00:14:08,320 --> 00:14:12,320
including apache memcache d and my sql

387
00:14:12,320 --> 00:14:14,639
the graph shows vm performance using

388
00:14:14,639 --> 00:14:16,000
baseline kvm and

389
00:14:16,000 --> 00:14:18,800
sckvm the results in a graph are

390
00:14:18,800 --> 00:14:19,839
normalized to

391
00:14:19,839 --> 00:14:22,399
bare metal execution so lower numbers

392
00:14:22,399 --> 00:14:24,560
mean better performance

393
00:14:24,560 --> 00:14:26,720
the results show modest overhead in

394
00:14:26,720 --> 00:14:29,600
overall on sdk vm compared to a modified

395
00:14:29,600 --> 00:14:31,440
kvm

396
00:14:31,440 --> 00:14:34,399
and here i summarize my presentation

397
00:14:34,399 --> 00:14:36,160
using micro verification

398
00:14:36,160 --> 00:14:39,360
we made small changes to kvm and verify

399
00:14:39,360 --> 00:14:41,120
that the kvm implementation

400
00:14:41,120 --> 00:14:43,360
protects the confidentiality and

401
00:14:43,360 --> 00:14:45,839
integrity of vm data

402
00:14:45,839 --> 00:14:47,920
because the changes required to retrofit

403
00:14:47,920 --> 00:14:49,760
kvm are modest

404
00:14:49,760 --> 00:14:52,399
the implementation retains kvm's overall

405
00:14:52,399 --> 00:14:53,680
commodity feature set

406
00:14:53,680 --> 00:14:56,399
and performance artwork is the first

407
00:14:56,399 --> 00:14:58,160
machine checked security proof

408
00:14:58,160 --> 00:15:00,079
for a commodity multiprocessor

409
00:15:00,079 --> 00:15:01,839
hypervisor

410
00:15:01,839 --> 00:15:04,160
and that'll be all for my presentation

411
00:15:04,160 --> 00:15:06,079
thanks for listening i'm now happy to

412
00:15:06,079 --> 00:15:06,560
answer

413
00:15:06,560 --> 00:15:09,279
questions

