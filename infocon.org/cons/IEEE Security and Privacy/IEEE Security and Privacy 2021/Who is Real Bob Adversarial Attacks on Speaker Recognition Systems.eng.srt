1
00:00:00,610 --> 00:00:01,799
Hello everyone!

2
00:00:01,800 --> 00:00:03,332
Welcome to my talk.

3
00:00:03,332 --> 00:00:07,530
My name is Guangke Chen, from ShanghaiTech University in Shanghai, China.

4
00:00:07,530 --> 00:00:11,533
Today I will be talking about our paper "Who is Real Bob?

5
00:00:11,534 --> 00:00:15,039
Adversarial Attacks on Speaker Recognition Systems".

6
00:00:15,039 --> 00:00:24,200
It is a joint work with Sen Chen, Lingling Fan, Xiaoning Du, Zhe Zhao, Fu Song and Yang Liu.

7
00:00:24,230 --> 00:00:29,600
As the title of our paper implies, the topic of our paper is adversarial attacks

8
00:00:29,600 --> 00:00:32,279
on speaker recognition systems.

9
00:00:32,279 --> 00:00:39,034
So first let me briefly introduce the basic concepts of speaker recognition.

10
00:00:39,034 --> 00:00:44,367
Speaker recognition is an automatic technique that recognizes a person's identity

11
00:00:44,367 --> 00:00:48,070
based on his or her speeches.

12
00:00:48,070 --> 00:00:54,630
Nowadays, speaker recognition has lots of applications, including voice assistant wake up,

13
00:00:54,630 --> 00:01:01,034
such as Hey Siri, personalized service on smart home,

14
00:01:01,034 --> 00:01:09,400
identity authentication in financial transaction, and application log in by voice print.

15
00:01:09,400 --> 00:01:16,890
As can be seen above, speaker recognition system is mostly used in safety-critical scenario.

16
00:01:16,890 --> 00:01:23,900
So once the system is broken, it will cause property damage, regulation degrade,

17
00:01:23,900 --> 00:01:27,710
sensitive information leak and so on.

18
00:01:27,710 --> 00:01:34,210
So the security of speaker recognition systems is very important.

19
00:01:34,210 --> 00:01:39,949
The mainstream implementation of speaker recognition is machine learning.

20
00:01:39,950 --> 00:01:45,229
However machine learning has been demonstrated to be vulnerable to adversarial examples.

21
00:01:45,229 --> 00:01:50,009
I think most of you are quite familiar with this picture.

22
00:01:50,009 --> 00:01:55,229
The benign example contains a panda,
and the machine learning model correctly recognizes

23
00:01:55,229 --> 00:01:58,390
the benign example as a panda.

24
00:01:58,390 --> 00:02:04,440
The attacker adds perturbation to the benign example to compute an adversarial example.

25
00:02:04,440 --> 00:02:08,380
Although the adversarial example still looks like a panda to human users,

26
00:02:08,380 --> 00:02:12,799
the machine learning model recognizes the adversarial example as a gibbon

27
00:02:12,800 --> 00:02:14,660
with very high confidence.

28
00:02:14,660 --> 00:02:22,320
The picture on the right is an example of adversarial attack in speech-to-text domain.

29
00:02:22,320 --> 00:02:25,040
So here there is a natural question.

30
00:02:25,040 --> 00:02:30,040
Is adversarial attack practical on speaker recognition systems?

31
00:02:30,040 --> 00:02:34,060
We answer this question by proposing an attack called FAKEBOB.

32
00:02:34,060 --> 00:02:36,090
FAKEBOB is black-box.

33
00:02:36,090 --> 00:02:39,980
It can be applied to general speaker recognition task.

34
00:02:39,980 --> 00:02:43,899
It remains effective on commercial speaker recognition systems.

35
00:02:43,900 --> 00:02:47,320
It is effective in over-the-air attack.

36
00:02:47,320 --> 00:02:52,599
Before I present the detail of our attack
FAKEBOB, let's have a look at the threat model

37
00:02:52,599 --> 00:02:55,340
we use in our paper.

38
00:02:55,340 --> 00:02:57,660
There is a victim speaker model in this picture.

39
00:02:57,660 --> 00:03:05,800
In normal case, a piece of voice from an illegal speaker will be rejected by the victim's speaker model.

40
00:03:05,810 --> 00:03:11,230
The attacker adds perturbation to the original voice to compute an adversarial voice.

41
00:03:11,230 --> 00:03:16,060
The adversarial voice can successfully pass the voice authentication of the victim speaker model,

42
00:03:16,060 --> 00:03:20,290
that is, being accepted by this model.

43
00:03:20,290 --> 00:03:25,929
So the attacker can gain access to privilege of the victim.

44
00:03:25,930 --> 00:03:32,780
The attacker has no any information about the model internal structure or parameters.

45
00:03:32,780 --> 00:03:39,834
He is limited to query the victim speaker model, that is, provide some inputs and obtain the outputs.

46
00:03:39,849 --> 00:03:44,768
This is the overview of our attack FAKEBOB.

47
00:03:44,769 --> 00:03:48,150
There are four major components.

48
00:03:48,150 --> 00:03:56,360
Firstly, since the generation of adversarial voices is formulated as an optimization problem,

49
00:03:56,360 --> 00:03:59,920
the first thing we need to do is to design the loss function.

50
00:03:59,920 --> 00:04:07,100
Our goal is that the loss function should be a good indicator of whether the attack succeeds or not.

51
00:04:07,110 --> 00:04:13,590
That is, the loss function is less than or equal to zero if and only if the attack succeeds.

52
00:04:13,590 --> 00:04:21,289
After several trial, we design the loss function based on the scoring and decision-making mechanism

53
00:04:21,289 --> 00:04:23,620
of speaker recognition systems.

54
00:04:23,620 --> 00:04:29,700
For example, because the open-set identification task makes the decision based on a pre-defined threshold,

55
00:04:29,700 --> 00:04:34,734
we incorporate this threshold into the loss function.

56
00:04:34,740 --> 00:04:39,939
In addition, since different tasks have different scoring and decision-making mechanisms,

57
00:04:39,939 --> 00:04:44,259
we design different loss functions for different tasks.

58
00:04:44,259 --> 00:04:53,267
Secondly unlike image classification, speaker recognition systems make decisions based on a pre-defined threshold.

59
00:04:53,289 --> 00:04:58,034
Only when the maximum score of adversarial voices is larger than the threshold

60
00:04:58,034 --> 00:05:00,990
can the attack succeed.

61
00:05:00,990 --> 00:05:05,759
Unfortunately this threshold is unknown to the attacker.

62
00:05:05,759 --> 00:05:10,289
To address this problem, we propose a threshold estimation algorithm.

63
00:05:10,289 --> 00:05:17,659
Once we obtain the estimated threshold, we replace the true threshold in the loss function

64
00:05:17,659 --> 00:05:20,099
with the estimated one.

65
00:05:20,100 --> 00:05:23,360
Our estimated threshold is larger than the true threshold.

66
00:05:23,360 --> 00:05:24,979
This guarantees that the attack will succeed.

67
00:05:24,979 --> 00:05:32,229
At the same time, our estimated threshold is not too far from the true threshold, 

68
00:05:32,229 --> 00:05:36,590
so the attack will not be too expensive.

69
00:05:36,590 --> 00:05:42,508
In white-box attack, the attacker usually uses backpropagation to obtain the exact gradient

70
00:05:42,509 --> 00:05:46,169
which is then used to solve the optimization problem.

71
00:05:46,169 --> 00:05:52,200
However backpropagation is not available in black box attack because the attacker

72
00:05:52,200 --> 00:05:57,590
has no information about model structures or parameters.

73
00:05:57,590 --> 00:06:03,700
So here we use Natural Evolution Strategy based gradient estimation method to estimate

74
00:06:03,700 --> 00:06:07,840
the gradient instead of using the exact gradient.

75
00:06:07,840 --> 00:06:14,498
This method only relies on scores and decisions returned by victim speaker model

76
00:06:14,499 --> 00:06:17,099
when we query the model.

77
00:06:17,099 --> 00:06:21,128
This allows us to achieve black-box attack.

78
00:06:21,129 --> 00:06:28,289
After we obtain the estimated gradient information, we can use it to solve the optimization problem

79
00:06:28,289 --> 00:06:34,034
by gradient descent, and finally we can obtain the adversarial voices.

80
00:06:34,034 --> 00:06:40,810
Adversarial attack in speech domain can be divided into API attack and over-the-air attack.

81
00:06:40,810 --> 00:06:49,067
API attack means the attacker directly feeds the adversarial voices to the model by the model's API.

82
00:06:49,080 --> 00:06:54,599
In contrast, to launch over-the-air attack, the attacker needs to play the adversarial

83
00:06:54,599 --> 00:06:59,300
voices by some loudspeakers, then the adversarial voices are transmitted

84
00:06:59,300 --> 00:07:03,034
in the air channel, and finally received by the microphones

85
00:07:03,034 --> 00:07:08,490
of speaker recognition devices.

86
00:07:08,490 --> 00:07:12,449
Over the attack is more challenging than API attack.

87
00:07:12,449 --> 00:07:19,800
The reason is that the noise introduced in  the air channel will make the attack lose effectiveness.

88
00:07:19,800 --> 00:07:26,800
Previous work solves this problem by modeling the noise during the generation of adversarial voices.

89
00:07:26,800 --> 00:07:32,699
The limitation of this solution is that it is somehow environment and device dependent.

90
00:07:32,699 --> 00:07:39,259
Instead, our solution is to simply improve the confidence of the adversarial voices.

91
00:07:39,259 --> 00:07:47,534
Although our solution is a little simple, our experimental result shows that our solution is quite effective.

92
00:07:47,559 --> 00:07:52,990
Okay, now let's turn to the experimental results.

93
00:07:52,990 --> 00:08:01,367
On open-source systems, our attack FAKEBOB can achieve nearly 100% attack successful rate.

94
00:08:01,369 --> 00:08:08,330
We also evaluate our attack FAKEBOB on two commercial speaker recognition systems.

95
00:08:08,330 --> 00:08:11,300
The first one is Talentedsoft.

96
00:08:11,300 --> 00:08:17,134
Because Talentedsoft returns both scores and decisions after we query the victim speaker model,

97
00:08:17,134 --> 00:08:23,067
we directly generate adversarial voices on this system.

98
00:08:23,099 --> 00:08:26,878
Our attack achieves 100% attack successful rate,

99
00:08:26,879 --> 00:08:32,089
and the average number of queries is two thousand and five hundred.

100
00:08:32,089 --> 00:08:34,909
The second one is Microsoft Azure.

101
00:08:34,909 --> 00:08:41,280
Because Microsoft Azure only returns decisions after we query the victim's speaker model,

102
00:08:41,280 --> 00:08:46,610
we use transfer attack to attack this system.

103
00:08:46,610 --> 00:08:51,519
Our attack FAKEBOB achieves 26% attack successful rate on the victim's speaker model,

104
00:08:51,519 --> 00:08:56,199
much lower than that on Talentedsoft.

105
00:08:56,199 --> 00:09:03,170
The reason is that transfer attack inherently suffers from low attack successful rate.

106
00:09:03,170 --> 00:09:10,034
For over-the-air attack, our attack remains effective

107
00:09:10,034 --> 00:09:15,220
across different distances between the loudspeaker and the microphone.

108
00:09:15,220 --> 00:09:20,360
When the distance between loudspeaker and microphone is no more than two meters,

109
00:09:20,360 --> 00:09:25,600
our attack FAKEBOB can achieve at least 70% attack successful rate.

110
00:09:25,600 --> 00:09:34,940
We consider three different types of loudspeakers, laptop, portable speaker, and broadcast equipment.

111
00:09:34,940 --> 00:09:37,640
We also consider two different microphones.

112
00:09:37,640 --> 00:09:41,720
The first one is the microphone on ISO system.

113
00:09:41,720 --> 00:09:46,529
The other one is the microphone on Android system.

114
00:09:46,529 --> 00:09:52,019
Totally we have six different combinations of loudspeakers and microphones.

115
00:09:52,019 --> 00:09:59,267
No matter what the combination is, our attack FAKEBOB can achieve at least 70% attack successful rate.

116
00:09:59,269 --> 00:10:06,319
This shows that our attack is device independent to some extent.

117
00:10:06,319 --> 00:10:12,329
We simulate different acoustic environments by introducing different types of noises,

118
00:10:12,329 --> 00:10:17,459
such as white noise, bus noise, restaurant noise, and music noise.

119
00:10:17,459 --> 00:10:22,967
When the noise is less than 60dB, our
attack FAKEBOB can achieve

120
00:10:22,967 --> 00:10:26,870
at least 48% attack successful rate.

121
00:10:26,870 --> 00:10:32,579
This shows that our attack is environment independent to some extent.

122
00:10:32,579 --> 00:10:36,819
Adversarial attack should not only make the machine learning model misbehave,

123
00:10:36,819 --> 00:10:40,410
but should also be imperceptible for human beings,

124
00:10:40,410 --> 00:10:45,750
so we need to evaluate the imperceptibility of our attack FAKEBOB.

125
00:10:45,750 --> 00:10:51,040
However imperceptibility has different meanings in different domains.

126
00:10:51,040 --> 00:10:57,534
For image classification, imperceptibility means humans recognize the original and adversarial images

127
00:10:57,534 --> 00:11:00,819
as the same objects.

128
00:11:00,819 --> 00:11:06,800
For speech-to-text, imperceptibility means the user hears the same sentence

129
00:11:06,800 --> 00:11:09,930
in the original and adversarial speeches.

130
00:11:09,930 --> 00:11:17,430
So first we have to define what imperceptibility means in speaker recognition.

131
00:11:17,430 --> 00:11:21,939
Suppose Alice is the source speaker, and she utters a voice.

132
00:11:21,940 --> 00:11:28,439
The attacker adds perturbation to the original voice to compute an adversarial voice.

133
00:11:28,439 --> 00:11:34,910
Bob's speaker model thinks the adversarial voice is uttered by Bob himself.

134
00:11:34,910 --> 00:11:39,699
However a third person thinks the adversarial voice is from Alice

135
00:11:39,700 --> 00:11:42,339
when listening to this voice.

136
00:11:42,339 --> 00:11:48,340
And this is the imperceptibility in speaker recognition.

137
00:11:48,340 --> 00:11:57,939
We conduct a quantity analysis of imperceptibility of our attack FAKEBOB.

138
00:11:57,939 --> 00:12:00,534
Our question is that how many people think

139
00:12:00,534 --> 00:12:05,689
adversarial and original voices are uttered by the same speaker.

140
00:12:05,690 --> 00:12:11,189
To get the answer, we carry our a human study on Amazon Mechanical Turk.

141
00:12:11,189 --> 00:12:18,990
For API attack, 64.9% of people think adversarial and original voices

142
00:12:18,990 --> 00:12:26,034
are uttered by the same speaker, indicating good imperceptibility of our attack FAKEBOB.

143
00:12:26,034 --> 00:12:35,079
For over-the-air attack, the result is 34%, lower than that of API attack.

144
00:12:35,079 --> 00:12:39,034
This is not surprising, because to launch over-the-air attack, 

145
00:12:39,034 --> 00:12:42,499
we need to improve the confidence of adversarial voices.

146
00:12:42,499 --> 00:12:46,880
This will introduce larger perturbation to the adversarial voices.

147
00:12:46,880 --> 00:12:51,300
So the imperceptibility becomes worse.

148
00:12:51,300 --> 00:12:54,359
Okay, that's all for today's presentation.

149
00:12:54,360 --> 00:12:57,170
Thanks for your attention.

150
00:12:57,170 --> 00:13:02,998
If you want to know more about our attack FAKEBOB, you can explore our official website.

151
00:13:02,999 --> 00:13:09,160
In addition, we have released our code for reproducing our experimental results.

152
00:13:09,160 --> 00:13:12,000
Thanks for listening again. Bye bye.

