1
00:00:00,560 --> 00:00:03,280
hi my name is hari and today i'll be

2
00:00:03,280 --> 00:00:05,359
talking about a new black box attack

3
00:00:05,359 --> 00:00:07,520
that our team has developed that can

4
00:00:07,520 --> 00:00:09,519
exploit both speech recognition

5
00:00:09,519 --> 00:00:12,960
and voice identification systems so

6
00:00:12,960 --> 00:00:15,200
attacks against ml models or adversarial

7
00:00:15,200 --> 00:00:17,119
ml has gained a lot of traction

8
00:00:17,119 --> 00:00:19,279
in the past few years the goal of the

9
00:00:19,279 --> 00:00:20,880
attack is to come up with inputs that

10
00:00:20,880 --> 00:00:21,680
the human

11
00:00:21,680 --> 00:00:23,119
and the ml model will interpret

12
00:00:23,119 --> 00:00:25,279
differently in the case

13
00:00:25,279 --> 00:00:27,359
of attacks against voice systems you

14
00:00:27,359 --> 00:00:29,920
start off with an audio sample

15
00:00:29,920 --> 00:00:31,359
when this audio sample is given to the

16
00:00:31,359 --> 00:00:33,440
human the human hears the benign command

17
00:00:33,440 --> 00:00:34,160
which is

18
00:00:34,160 --> 00:00:36,480
what's the weather but when the same

19
00:00:36,480 --> 00:00:37,920
audio sample is given

20
00:00:37,920 --> 00:00:40,000
to the voice assistant in this case a

21
00:00:40,000 --> 00:00:41,280
siri

22
00:00:41,280 --> 00:00:42,719
the voice assistant will output a

23
00:00:42,719 --> 00:00:44,559
malicious command which is unlock the

24
00:00:44,559 --> 00:00:46,399
door

25
00:00:46,399 --> 00:00:48,480
now this is an example of a target

26
00:00:48,480 --> 00:00:50,960
attack where the attacker wants to force

27
00:00:50,960 --> 00:00:52,239
the voice assistant

28
00:00:52,239 --> 00:00:55,520
to output a target transcription and if

29
00:00:55,520 --> 00:00:57,039
you were to look at the existing

30
00:00:57,039 --> 00:00:57,760
literature

31
00:00:57,760 --> 00:00:59,760
there has been a lot of really solid

32
00:00:59,760 --> 00:01:01,359
work that has been done

33
00:01:01,359 --> 00:01:04,640
in the space of targeted attacks but

34
00:01:04,640 --> 00:01:07,760
what about untargeted ones here the

35
00:01:07,760 --> 00:01:09,680
attacker wants the model to output

36
00:01:09,680 --> 00:01:10,880
anything but

37
00:01:10,880 --> 00:01:14,320
the original output and unfortunately

38
00:01:14,320 --> 00:01:15,600
there hasn't been

39
00:01:15,600 --> 00:01:18,320
any work done in this space this is

40
00:01:18,320 --> 00:01:19,759
quite unfortunate because there are a

41
00:01:19,759 --> 00:01:21,040
lot of applications where

42
00:01:21,040 --> 00:01:23,920
untargeted attacks might be very useful

43
00:01:23,920 --> 00:01:24,720
so

44
00:01:24,720 --> 00:01:26,400
think about building robust audio

45
00:01:26,400 --> 00:01:29,200
captures recent work has shown that boss

46
00:01:29,200 --> 00:01:31,439
use speech recognition services to

47
00:01:31,439 --> 00:01:32,400
identify

48
00:01:32,400 --> 00:01:34,159
and transcribe audio captures with very

49
00:01:34,159 --> 00:01:35,520
high accuracy

50
00:01:35,520 --> 00:01:37,680
so using these untargeted attacks to

51
00:01:37,680 --> 00:01:39,200
make audio captures robust

52
00:01:39,200 --> 00:01:41,119
transcription would be real useful

53
00:01:41,119 --> 00:01:42,880
against these bots

54
00:01:42,880 --> 00:01:45,439
or consider the case of evading mass

55
00:01:45,439 --> 00:01:48,000
telephony surveillance networks

56
00:01:48,000 --> 00:01:50,079
these networks record and transcribe

57
00:01:50,079 --> 00:01:51,920
millions of hours of phone call data to

58
00:01:51,920 --> 00:01:53,119
perform surveillance

59
00:01:53,119 --> 00:01:56,000
on their populations or even evading

60
00:01:56,000 --> 00:01:57,439
content id systems

61
00:01:57,439 --> 00:02:00,479
such as that on youtube um

62
00:02:00,479 --> 00:02:02,159
you might want to upload a proprietary

63
00:02:02,159 --> 00:02:03,840
music file on youtube without being

64
00:02:03,840 --> 00:02:05,040
flagged

65
00:02:05,040 --> 00:02:07,119
now these are just a few examples each

66
00:02:07,119 --> 00:02:08,080
each

67
00:02:08,080 --> 00:02:10,720
of which deserves multiple papers and

68
00:02:10,720 --> 00:02:12,400
clearly there's a need for untargeted

69
00:02:12,400 --> 00:02:14,000
attacks in this space which

70
00:02:14,000 --> 00:02:15,520
current literature doesn't actually

71
00:02:15,520 --> 00:02:17,760
fulfill

72
00:02:17,760 --> 00:02:19,200
so i'm sure some of you might be

73
00:02:19,200 --> 00:02:21,360
thinking well why can't we just use

74
00:02:21,360 --> 00:02:24,000
those targeted attacks instead

75
00:02:24,000 --> 00:02:26,480
well that's because target attacks have

76
00:02:26,480 --> 00:02:28,319
some severe limitations

77
00:02:28,319 --> 00:02:30,800
they often require white box access

78
00:02:30,800 --> 00:02:32,000
which an attacker

79
00:02:32,000 --> 00:02:34,720
might not have in the real world they

80
00:02:34,720 --> 00:02:35,040
are

81
00:02:35,040 --> 00:02:36,879
not real time in fact they're really

82
00:02:36,879 --> 00:02:38,959
slow because they require thousands of

83
00:02:38,959 --> 00:02:42,160
queries to the target system

84
00:02:42,160 --> 00:02:45,120
and to add to that samples that these

85
00:02:45,120 --> 00:02:47,040
attacks produce do not actually transfer

86
00:02:47,040 --> 00:02:47,760
to

87
00:02:47,760 --> 00:02:50,800
different models and lastly

88
00:02:50,800 --> 00:02:52,959
these attack these attack samples are

89
00:02:52,959 --> 00:02:54,720
very sensitive to noise

90
00:02:54,720 --> 00:02:57,360
and won't be able to operate in robot in

91
00:02:57,360 --> 00:02:58,959
lossy environments such as that of the

92
00:02:58,959 --> 00:03:01,200
telephony network

93
00:03:01,200 --> 00:03:03,840
so what do we do to fill in this gap in

94
00:03:03,840 --> 00:03:04,560
this paper

95
00:03:04,560 --> 00:03:07,680
we propose a generic attack algorithm

96
00:03:07,680 --> 00:03:09,840
that can create transferable adversarial

97
00:03:09,840 --> 00:03:11,040
samples

98
00:03:11,040 --> 00:03:12,720
these samples sound like the original to

99
00:03:12,720 --> 00:03:14,319
the human ear but can

100
00:03:14,319 --> 00:03:16,560
force a voice model to produce an

101
00:03:16,560 --> 00:03:18,640
untargeted output

102
00:03:18,640 --> 00:03:20,319
the attack can work against both speech

103
00:03:20,319 --> 00:03:22,400
recognition and voice identification

104
00:03:22,400 --> 00:03:23,280
systems

105
00:03:23,280 --> 00:03:28,400
and can succeed in black box settings

106
00:03:29,599 --> 00:03:31,120
but before we talk about the attack

107
00:03:31,120 --> 00:03:32,799
itself let's look at the voice system

108
00:03:32,799 --> 00:03:35,360
pipeline that we'll be exploiting

109
00:03:35,360 --> 00:03:38,239
so we start off with an audio sample the

110
00:03:38,239 --> 00:03:39,599
audio sample is passed on to the

111
00:03:39,599 --> 00:03:41,280
pre-processing stage

112
00:03:41,280 --> 00:03:43,920
here we apply low pass filters to remove

113
00:03:43,920 --> 00:03:45,680
any sort of high frequency

114
00:03:45,680 --> 00:03:48,879
or rudimentary noise from the audio

115
00:03:48,879 --> 00:03:50,560
next the audio is passed to the feature

116
00:03:50,560 --> 00:03:52,640
extraction phase which is designed to

117
00:03:52,640 --> 00:03:54,879
emulate the human ear

118
00:03:54,879 --> 00:03:56,720
it uses signal processing algorithms to

119
00:03:56,720 --> 00:03:58,159
extract features that the human ear

120
00:03:58,159 --> 00:04:00,080
finds most important

121
00:04:00,080 --> 00:04:01,439
these are known as feature vectors which

122
00:04:01,439 --> 00:04:03,200
are then passed on to the model

123
00:04:03,200 --> 00:04:04,799
and the model produces an output

124
00:04:04,799 --> 00:04:06,879
transcript

125
00:04:06,879 --> 00:04:09,599
now most attacks in the adversarial ml

126
00:04:09,599 --> 00:04:10,080
space

127
00:04:10,080 --> 00:04:11,920
target the model inference stage of the

128
00:04:11,920 --> 00:04:13,599
pipeline

129
00:04:13,599 --> 00:04:15,599
rather than attacking the model itself

130
00:04:15,599 --> 00:04:17,120
we attack the feature extraction

131
00:04:17,120 --> 00:04:19,120
phase specifically the signal processing

132
00:04:19,120 --> 00:04:20,880
algorithms it uses

133
00:04:20,880 --> 00:04:22,240
these are known as signal processing

134
00:04:22,240 --> 00:04:25,280
attacks specifically in this paper we

135
00:04:25,280 --> 00:04:26,000
exploit how

136
00:04:26,000 --> 00:04:28,240
frequencies the human ear does not find

137
00:04:28,240 --> 00:04:29,199
important

138
00:04:29,199 --> 00:04:31,680
and might not even hear are very

139
00:04:31,680 --> 00:04:34,000
important for model inference

140
00:04:34,000 --> 00:04:36,960
so when we remove these frequencies this

141
00:04:36,960 --> 00:04:37,600
not this

142
00:04:37,600 --> 00:04:39,600
might not impact human intelligibility

143
00:04:39,600 --> 00:04:40,639
at all

144
00:04:40,639 --> 00:04:42,400
but doing so will actually confuse the

145
00:04:42,400 --> 00:04:45,840
model into producing an incorrect output

146
00:04:45,840 --> 00:04:48,560
so by attacking the feature extraction

147
00:04:48,560 --> 00:04:49,280
phase

148
00:04:49,280 --> 00:04:51,600
we really don't need to know anything

149
00:04:51,600 --> 00:04:53,199
about the underlying model

150
00:04:53,199 --> 00:04:56,080
the model is irrelevant and this makes

151
00:04:56,080 --> 00:04:56,880
our attack

152
00:04:56,880 --> 00:05:00,080
successful in black box settings so how

153
00:05:00,080 --> 00:05:02,000
does the attack work anyway well

154
00:05:02,000 --> 00:05:03,600
we start off with an audio sample that

155
00:05:03,600 --> 00:05:05,600
we want to pretend and pass it to the

156
00:05:05,600 --> 00:05:07,199
attack algorithm

157
00:05:07,199 --> 00:05:08,639
now the first step of the attack

158
00:05:08,639 --> 00:05:10,240
algorithm is to break the audio

159
00:05:10,240 --> 00:05:12,479
insurance subsequent frequencies

160
00:05:12,479 --> 00:05:14,000
and one of the most popular methods for

161
00:05:14,000 --> 00:05:15,440
this task is the discrete fourier

162
00:05:15,440 --> 00:05:16,000
transform

163
00:05:16,000 --> 00:05:19,120
or the dft so the dft tells us which

164
00:05:19,120 --> 00:05:19,840
frequencies

165
00:05:19,840 --> 00:05:22,560
make up an input signal and you can see

166
00:05:22,560 --> 00:05:25,039
the dfd output on your screen right now

167
00:05:25,039 --> 00:05:26,880
on the x-axis you can see the different

168
00:05:26,880 --> 00:05:28,080
frequencies

169
00:05:28,080 --> 00:05:29,919
and on the y-axis you can see the

170
00:05:29,919 --> 00:05:32,240
corresponding intensities

171
00:05:32,240 --> 00:05:34,400
now we hypothesize that frequencies with

172
00:05:34,400 --> 00:05:36,400
low intensities are not important to the

173
00:05:36,400 --> 00:05:37,280
human ear

174
00:05:37,280 --> 00:05:39,120
but are in fact important to the model

175
00:05:39,120 --> 00:05:40,800
for correct transcription

176
00:05:40,800 --> 00:05:43,360
so we remove these frequencies hoping to

177
00:05:43,360 --> 00:05:44,720
confuse the model

178
00:05:44,720 --> 00:05:46,240
but the question you might ask yourself

179
00:05:46,240 --> 00:05:48,080
as well which frequencies should you

180
00:05:48,080 --> 00:05:49,120
remove

181
00:05:49,120 --> 00:05:51,440
and for that we employ a binary search

182
00:05:51,440 --> 00:05:52,960
approach

183
00:05:52,960 --> 00:05:55,360
so we set a threshold at fifty percent

184
00:05:55,360 --> 00:05:57,680
of the maximum intensity in the signal

185
00:05:57,680 --> 00:05:59,120
all the frequency that have less

186
00:05:59,120 --> 00:06:01,120
intensity than the threshold

187
00:06:01,120 --> 00:06:04,080
shown in green right now are discarded

188
00:06:04,080 --> 00:06:05,440
which means that we just set them to

189
00:06:05,440 --> 00:06:06,639
zero

190
00:06:06,639 --> 00:06:08,639
whatever frequencies are left shown in

191
00:06:08,639 --> 00:06:10,560
red are used to reconstruct

192
00:06:10,560 --> 00:06:12,639
a prepared audio sample which is given

193
00:06:12,639 --> 00:06:15,039
to the model and the model outputs

194
00:06:15,039 --> 00:06:18,240
a garbage transcription now

195
00:06:18,240 --> 00:06:19,600
some of you might be thinking to

196
00:06:19,600 --> 00:06:22,240
yourself right now if you throw out

197
00:06:22,240 --> 00:06:24,000
half the frequencies won't that

198
00:06:24,000 --> 00:06:27,199
significantly degrade the audio quality

199
00:06:27,199 --> 00:06:29,039
well that that's correct yes that's

200
00:06:29,039 --> 00:06:31,440
right so we want to make sure we throw

201
00:06:31,440 --> 00:06:31,680
out

202
00:06:31,680 --> 00:06:34,240
as few frequencies as possible while

203
00:06:34,240 --> 00:06:36,880
still being able to evade the model

204
00:06:36,880 --> 00:06:38,240
and this is because throwing fewer

205
00:06:38,240 --> 00:06:40,400
frequencies allows us to maintain high

206
00:06:40,400 --> 00:06:42,479
audio quality

207
00:06:42,479 --> 00:06:44,720
therefore we will iterate to the next

208
00:06:44,720 --> 00:06:46,400
step of the minus search that is

209
00:06:46,400 --> 00:06:48,080
start off with the original audio sample

210
00:06:48,080 --> 00:06:51,280
again take the dft

211
00:06:51,280 --> 00:06:53,360
now instead of using the 50 threshold

212
00:06:53,360 --> 00:06:55,120
like we did last time we'll reduce it to

213
00:06:55,120 --> 00:06:57,199
25

214
00:06:57,199 --> 00:06:59,440
we'll figure out which frequency is set

215
00:06:59,440 --> 00:07:01,840
to zero which is uh the ones in green uh

216
00:07:01,840 --> 00:07:03,360
which fall below the 25

217
00:07:03,360 --> 00:07:06,400
threshold set them to zero reconstruct

218
00:07:06,400 --> 00:07:07,280
the audio

219
00:07:07,280 --> 00:07:08,800
pass it to the model and the model

220
00:07:08,800 --> 00:07:10,080
outputs a different garbage

221
00:07:10,080 --> 00:07:11,759
transcription

222
00:07:11,759 --> 00:07:13,680
now we will keep repeating this process

223
00:07:13,680 --> 00:07:15,759
until we reach the smallest threshold

224
00:07:15,759 --> 00:07:17,120
for which the models

225
00:07:17,120 --> 00:07:18,880
output the model outputs the wrong

226
00:07:18,880 --> 00:07:20,800
transcript

227
00:07:20,800 --> 00:07:23,680
so what now that we understand how the

228
00:07:23,680 --> 00:07:24,560
attack works

229
00:07:24,560 --> 00:07:26,479
we need to think about what part of the

230
00:07:26,479 --> 00:07:28,479
audio we want to pretend

231
00:07:28,479 --> 00:07:30,479
now traditionally attacks in the audio

232
00:07:30,479 --> 00:07:32,240
space protect the full width of the

233
00:07:32,240 --> 00:07:33,599
audio sample

234
00:07:33,599 --> 00:07:35,199
and this isn't great because you're

235
00:07:35,199 --> 00:07:37,599
introducing the noise across the entire

236
00:07:37,599 --> 00:07:38,800
signal

237
00:07:38,800 --> 00:07:40,479
which significantly impacts audio

238
00:07:40,479 --> 00:07:42,000
quality

239
00:07:42,000 --> 00:07:43,840
but what if we want to perturb only a

240
00:07:43,840 --> 00:07:46,000
tiny fraction of the audio and

241
00:07:46,000 --> 00:07:48,240
still be able to fold the model that

242
00:07:48,240 --> 00:07:49,840
will give us both advantages at the same

243
00:07:49,840 --> 00:07:50,479
time

244
00:07:50,479 --> 00:07:53,680
high audio quality and attack success

245
00:07:53,680 --> 00:07:55,199
we can perform this sort of perturbation

246
00:07:55,199 --> 00:07:57,199
at different granularities we can do it

247
00:07:57,199 --> 00:07:58,000
at the word level

248
00:07:58,000 --> 00:08:00,400
where we only perturb a single word or

249
00:08:00,400 --> 00:08:02,160
every every few words

250
00:08:02,160 --> 00:08:04,639
or we can we can we can even perturb at

251
00:08:04,639 --> 00:08:06,319
even a more finer green level which is

252
00:08:06,319 --> 00:08:08,080
at the phoneme level which is

253
00:08:08,080 --> 00:08:11,039
where you only protect a single phoneme

254
00:08:11,039 --> 00:08:12,479
but what's interesting to note here is

255
00:08:12,479 --> 00:08:14,000
that perturbing the audio samples at

256
00:08:14,000 --> 00:08:15,520
this granularity

257
00:08:15,520 --> 00:08:17,280
distorts a large chunk of the

258
00:08:17,280 --> 00:08:19,199
transcription not just the unit we are

259
00:08:19,199 --> 00:08:20,160
pertaining

260
00:08:20,160 --> 00:08:21,840
and that's because voice systems are

261
00:08:21,840 --> 00:08:23,199
context dependent

262
00:08:23,199 --> 00:08:24,879
they use information about previous

263
00:08:24,879 --> 00:08:27,280
timestamps to make an inference about

264
00:08:27,280 --> 00:08:28,560
the current one

265
00:08:28,560 --> 00:08:30,800
so perturbing a few timestamps can

266
00:08:30,800 --> 00:08:33,599
impact the entire transcript

267
00:08:33,599 --> 00:08:36,000
we perform both levels of attack phoneme

268
00:08:36,000 --> 00:08:37,599
and word in the paper but we'll focus on

269
00:08:37,599 --> 00:08:40,640
the phoneme level attack in the stock

270
00:08:40,640 --> 00:08:42,399
to see the effect of the phone level

271
00:08:42,399 --> 00:08:44,800
attack we use the timid data set

272
00:08:44,800 --> 00:08:47,360
we pull out individual sentences and

273
00:08:47,360 --> 00:08:48,000
perturb

274
00:08:48,000 --> 00:08:50,160
one phoneme at a time and observe the

275
00:08:50,160 --> 00:08:53,040
corresponding change in transcription

276
00:08:53,040 --> 00:08:55,440
we measure this change using the cosine

277
00:08:55,440 --> 00:08:56,880
similarity score

278
00:08:56,880 --> 00:08:59,040
the larger the cosine score the larger

279
00:08:59,040 --> 00:09:01,279
the change in transcription

280
00:09:01,279 --> 00:09:03,200
now we can see that how changing a

281
00:09:03,200 --> 00:09:05,440
single phoneme in this case a valve

282
00:09:05,440 --> 00:09:08,000
for google has an average cosine

283
00:09:08,000 --> 00:09:10,480
similarity score of 0.8

284
00:09:10,480 --> 00:09:12,880
this means that changing just a single

285
00:09:12,880 --> 00:09:13,519
vowel

286
00:09:13,519 --> 00:09:16,640
can actually distort the model's output

287
00:09:16,640 --> 00:09:21,120
by 80 but why is this actually happening

288
00:09:21,120 --> 00:09:23,680
well as i mentioned earlier voice models

289
00:09:23,680 --> 00:09:25,120
use context

290
00:09:25,120 --> 00:09:27,760
so perturbing just a single time step

291
00:09:27,760 --> 00:09:29,600
carries the error into all future time

292
00:09:29,600 --> 00:09:30,240
step

293
00:09:30,240 --> 00:09:33,600
ruining the transcription so

294
00:09:33,600 --> 00:09:35,360
this actually brings me to a very

295
00:09:35,360 --> 00:09:37,279
important observation and one major

296
00:09:37,279 --> 00:09:39,920
advantage of using our attack

297
00:09:39,920 --> 00:09:41,440
the attacker doesn't need to protect the

298
00:09:41,440 --> 00:09:43,360
entire audio in fact

299
00:09:43,360 --> 00:09:45,920
perturbing one vowel or any phoneme for

300
00:09:45,920 --> 00:09:47,600
that matter can cause a large chunk of

301
00:09:47,600 --> 00:09:50,399
the audio to mistranscribe

302
00:09:50,399 --> 00:09:54,480
i know so this slide shows the

303
00:09:54,480 --> 00:09:56,000
impact of our attack on model

304
00:09:56,000 --> 00:09:58,480
transcription so in the case of google

305
00:09:58,480 --> 00:09:59,440
when you perturb

306
00:09:59,440 --> 00:10:02,720
the o in no evil it actually forces

307
00:10:02,720 --> 00:10:05,040
the google speech api to output keenan's

308
00:10:05,040 --> 00:10:06,720
bill instead of the original phrase

309
00:10:06,720 --> 00:10:07,360
which is no

310
00:10:07,360 --> 00:10:10,000
evil and that's where our paper got the

311
00:10:10,000 --> 00:10:10,399
name

312
00:10:10,399 --> 00:10:13,600
hey no evil ck as well

313
00:10:13,600 --> 00:10:17,279
now we see an extreme case for wit we're

314
00:10:17,279 --> 00:10:20,079
that a single g actually ruins the

315
00:10:20,079 --> 00:10:22,240
entire transcription of the audio

316
00:10:22,240 --> 00:10:23,760
forcing the model to output just a

317
00:10:23,760 --> 00:10:26,480
single word nope

318
00:10:26,480 --> 00:10:27,680
so let's quickly go over the

319
00:10:27,680 --> 00:10:30,079
capabilities our tag provides

320
00:10:30,079 --> 00:10:32,000
it can force speech recognition models

321
00:10:32,000 --> 00:10:33,839
to output random text

322
00:10:33,839 --> 00:10:35,760
and speaker recognition models to output

323
00:10:35,760 --> 00:10:37,200
random users

324
00:10:37,200 --> 00:10:38,640
and if you're interested in our speaker

325
00:10:38,640 --> 00:10:40,320
recognition experiments you should

326
00:10:40,320 --> 00:10:42,320
definitely check out the paper

327
00:10:42,320 --> 00:10:45,040
we tested more than 1200 attack audio

328
00:10:45,040 --> 00:10:46,000
samples

329
00:10:46,000 --> 00:10:47,839
in the case of speaker ignition models

330
00:10:47,839 --> 00:10:50,480
and about 20 speakers

331
00:10:50,480 --> 00:10:52,880
in the case of speaker ignition models

332
00:10:52,880 --> 00:10:54,320
overall our attack takes

333
00:10:54,320 --> 00:10:56,720
less than 15 queries to the model to

334
00:10:56,720 --> 00:10:57,360
generate

335
00:10:57,360 --> 00:10:59,200
an adversarial sample and that's

336
00:10:59,200 --> 00:11:01,040
primarily because of the binary search

337
00:11:01,040 --> 00:11:02,880
approach that we employ

338
00:11:02,880 --> 00:11:04,640
and because it takes so little time to

339
00:11:04,640 --> 00:11:07,120
craft adversarial samples we were able

340
00:11:07,120 --> 00:11:09,600
to successfully exploit

341
00:11:09,600 --> 00:11:13,279
models from google facebook

342
00:11:13,279 --> 00:11:16,959
and many other models

343
00:11:16,959 --> 00:11:19,200
so in essence by attacking the feature

344
00:11:19,200 --> 00:11:21,040
extraction stage of the speech ignition

345
00:11:21,040 --> 00:11:21,920
pipeline

346
00:11:21,920 --> 00:11:24,079
instead of the model itself we can

347
00:11:24,079 --> 00:11:25,839
successfully exploit

348
00:11:25,839 --> 00:11:29,600
any black box voice system

349
00:11:29,600 --> 00:11:31,680
now i'm sure some of you are shouting at

350
00:11:31,680 --> 00:11:32,959
your screens right now

351
00:11:32,959 --> 00:11:34,560
what does the audio even sound like

352
00:11:34,560 --> 00:11:37,519
hardy well yo it is finally over

353
00:11:37,519 --> 00:11:40,000
it's demo time i'm going to play a

354
00:11:40,000 --> 00:11:41,440
benign audio sample for you

355
00:11:41,440 --> 00:11:43,440
and try to make out the text in the

356
00:11:43,440 --> 00:11:44,640
audio

357
00:11:44,640 --> 00:11:47,519
the emperor had a main temper so when we

358
00:11:47,519 --> 00:11:49,600
gave this audio sample to google

359
00:11:49,600 --> 00:11:51,120
google output the transcription the

360
00:11:51,120 --> 00:11:53,120
emperor had a mean temper which is the

361
00:11:53,120 --> 00:11:54,320
correct output

362
00:11:54,320 --> 00:11:56,160
so let's say the audio sample one more

363
00:11:56,160 --> 00:11:59,279
time the emperor had a main temper

364
00:11:59,279 --> 00:12:01,279
now we took this audio file and we

365
00:12:01,279 --> 00:12:03,200
perturbed it so now you'll hear the

366
00:12:03,200 --> 00:12:04,880
perturbed version of the audio

367
00:12:04,880 --> 00:12:07,360
the emperor had a main temper now the

368
00:12:07,360 --> 00:12:08,880
difference between the original

369
00:12:08,880 --> 00:12:10,639
and this prettier audio sample are

370
00:12:10,639 --> 00:12:12,240
almost inaudible

371
00:12:12,240 --> 00:12:14,160
however when we gave this perturbed

372
00:12:14,160 --> 00:12:15,680
sample to google

373
00:12:15,680 --> 00:12:19,519
it output semper hanuman temple

374
00:12:19,519 --> 00:12:21,279
and this kind of illustrates how our

375
00:12:21,279 --> 00:12:23,360
attack maintained audio quality while

376
00:12:23,360 --> 00:12:26,560
still successfully fooling the model

377
00:12:26,560 --> 00:12:28,800
to further understand the impact of the

378
00:12:28,800 --> 00:12:30,639
attack on audio quality we conducted a

379
00:12:30,639 --> 00:12:31,920
user study

380
00:12:31,920 --> 00:12:34,000
each participant was asked to listen to

381
00:12:34,000 --> 00:12:36,480
and transcribe perturbed audio samples

382
00:12:36,480 --> 00:12:38,160
so for example one audio sample we gave

383
00:12:38,160 --> 00:12:39,839
listeners contained the command

384
00:12:39,839 --> 00:12:43,040
how are you how's work going this

385
00:12:43,040 --> 00:12:44,800
was a perturb sample so when we gave it

386
00:12:44,800 --> 00:12:46,560
to the voice system it would output the

387
00:12:46,560 --> 00:12:47,680
incorrect transcript

388
00:12:47,680 --> 00:12:50,720
how are you post more thorough

389
00:12:50,720 --> 00:12:52,399
to measure the impact on audio quality

390
00:12:52,399 --> 00:12:54,240
we and as participants

391
00:12:54,240 --> 00:12:56,880
for their respective transcriptions so

392
00:12:56,880 --> 00:12:58,560
higher transcription accuracy means

393
00:12:58,560 --> 00:13:00,639
higher audio quality

394
00:13:00,639 --> 00:13:03,360
our results show that on average the

395
00:13:03,360 --> 00:13:05,120
transcription accuracy for benign audio

396
00:13:05,120 --> 00:13:07,839
samples is around 98

397
00:13:07,839 --> 00:13:10,000
and the accuracy for not for attack

398
00:13:10,000 --> 00:13:11,200
audio sample is around

399
00:13:11,200 --> 00:13:14,800
96 so this shows that

400
00:13:14,800 --> 00:13:16,720
shows us that the human interpretability

401
00:13:16,720 --> 00:13:17,920
of attack audio

402
00:13:17,920 --> 00:13:19,839
is very similar to that of benign audio

403
00:13:19,839 --> 00:13:21,040
samples

404
00:13:21,040 --> 00:13:23,680
so effectively the attack is able to

405
00:13:23,680 --> 00:13:25,120
maintain audio quality

406
00:13:25,120 --> 00:13:28,639
and still fool the model now

407
00:13:28,639 --> 00:13:30,320
in case you slept through this entire

408
00:13:30,320 --> 00:13:32,720
talk let's just go over the takeaways

409
00:13:32,720 --> 00:13:34,320
really quickly

410
00:13:34,320 --> 00:13:36,639
in this paper we present a simple

411
00:13:36,639 --> 00:13:37,680
efficient

412
00:13:37,680 --> 00:13:40,000
and untargeted attack that can exploit

413
00:13:40,000 --> 00:13:42,639
any voice processing system

414
00:13:42,639 --> 00:13:45,040
because we target the feature extraction

415
00:13:45,040 --> 00:13:45,680
station

416
00:13:45,680 --> 00:13:48,399
of the pipeline the model is just

417
00:13:48,399 --> 00:13:49,519
irrelevant

418
00:13:49,519 --> 00:13:51,760
and as a consequence all the systems we

419
00:13:51,760 --> 00:13:52,639
tested

420
00:13:52,639 --> 00:13:55,920
are in fact vulnerable we showed that we

421
00:13:55,920 --> 00:13:57,920
can exploit both speech recognition

422
00:13:57,920 --> 00:14:00,720
and speaker recognition models and as a

423
00:14:00,720 --> 00:14:02,320
consequence we can achieve the same

424
00:14:02,320 --> 00:14:04,639
goals as traditional adversarial ml

425
00:14:04,639 --> 00:14:08,839
attacks and at this point i'll take easy

426
00:14:08,839 --> 00:14:11,839
questions

