1
00:00:00,240 --> 00:00:03,920
hello my name is christopher and i have the 
pleasure of beginning this presentation in our

2
00:00:03,920 --> 00:00:09,200
paper titled machine on learning later you'll hear 
from my fellow co-authors baiwu hengrui and lucas

3
00:00:10,240 --> 00:00:15,200
let's begin by quickly going over a few motivating 
observations that we had on machine learning our

4
00:00:15,200 --> 00:00:19,520
first observation is that machine learning models 
are rapidly growing in their number of parameters

5
00:00:19,520 --> 00:00:23,920
observing this graph the x-axis shows the 
model computations or the prediction efficiency

6
00:00:23,920 --> 00:00:28,160
and the y-axis the model accuracy these can 
be roughly correlated with the model size

7
00:00:28,160 --> 00:00:32,159
in terms of the number of parameters shown 
as the size of the circle models now have

8
00:00:32,159 --> 00:00:36,000
many millions of parameters often exceeding the 
size of the data set that they are trained on

9
00:00:36,640 --> 00:00:40,800
this over parameterization combined with the 
incremental and random nature of stochastic

10
00:00:40,800 --> 00:00:45,839
gradient descent can lead to unintended behaviors 
and privacy concerns such as the memorization of

11
00:00:45,840 --> 00:00:50,400
data in particular it makes it difficult to 
determine the exact influence of a data point

12
00:00:50,400 --> 00:00:55,519
on these parameters thus our first observation is 
that there is a complex interplay between the data

13
00:00:55,520 --> 00:01:00,800
and the learn's model parameters recently many 
new privacy legislation called for more clarity

14
00:01:00,800 --> 00:01:05,519
around data privacy users are now empowered to 
remove their data which impacts machine learning

15
00:01:05,519 --> 00:01:09,920
as the model may have already memorized some or 
all of the data's contributions during training

16
00:01:10,640 --> 00:01:14,480
much of these legislation come at a time when 
tech experts and companies are not ready to

17
00:01:14,480 --> 00:01:18,480
facilitate their requirements all of this 
points to a disconnect between the needs of

18
00:01:18,480 --> 00:01:22,560
the people imposed through legal experts 
and what technology experts can provide

19
00:01:24,240 --> 00:01:27,759
our observations are that there is a lacking 
synergy between legal and tech experts

20
00:01:27,760 --> 00:01:31,680
and that there's a complex interplay between 
data and the learned parameters this creates a

21
00:01:31,680 --> 00:01:35,840
concrete problem how do we unlearn from already 
trained machine learning models such as deep

22
00:01:35,840 --> 00:01:41,280
neural networks so the removal guarantee is easy 
to comprehend let us now define the problem of

23
00:01:41,280 --> 00:01:46,000
machine learning say that we have some data and 
we train a model on it with some randomness we

24
00:01:46,000 --> 00:01:49,680
may learn a different model after training 
giving us a distribution of possible models

25
00:01:50,560 --> 00:01:54,160
after getting some more data shown in 
blue we train a new model on the aggregate

26
00:01:54,160 --> 00:01:59,440
of all of our data imagine the owners of the blue 
data now request for on learning our goal is to

27
00:01:59,440 --> 00:02:03,679
then unlearn from this model so that the unlearned 
model is equivalent to what we would have obtained

28
00:02:03,680 --> 00:02:08,479
without ever training on that data point in the 
first place because of random reinitialization

29
00:02:08,479 --> 00:02:13,519
our unlearned model may be different concretely 
we need the distribution of possible models after

30
00:02:13,520 --> 00:02:18,000
unlearning to be equivalent to those randomly 
initialized and trained without the unlearned data

31
00:02:20,080 --> 00:02:22,480
we now turn to solutions for machine learning

32
00:02:23,200 --> 00:02:27,679
there are two major solutions in prime works the 
first is differentially private learning in this

33
00:02:27,680 --> 00:02:32,880
case we treat a model so that its predictions do 
not strongly depend on the data however to achieve

34
00:02:32,880 --> 00:02:37,600
complete unlearning in this case we require our 
differential private parameter epsilon to be zero

35
00:02:37,600 --> 00:02:43,120
this prevents learning the second solution 
is statistical query learning of tau ital

36
00:02:43,120 --> 00:02:47,840
statistical query learning is a formulation 
where a model learns by asking aggregate queries

37
00:02:48,560 --> 00:02:52,720
however this solution only works for simple 
models like nail base and do not apply to

38
00:02:52,720 --> 00:02:58,160
more complex models like deep neural networks we 
can also only handle a limited number of queries

39
00:02:59,360 --> 00:03:04,400
given the drawback of solutions from fireworks we 
take a step back to reconsider the naive solution

40
00:03:04,400 --> 00:03:09,120
of machine learning namely first remove the 
data points we want to unlearn from the data set

41
00:03:09,120 --> 00:03:14,800
and then retrieve the model from scratch with a 
new data set this solution is simple and intuitive

42
00:03:15,360 --> 00:03:20,640
and works for all kinds of models including deep 
neural networks however it is extremely slow due

43
00:03:20,640 --> 00:03:26,559
to retraining on a fall data set in this work we 
aim to improve on the computational efficiency

44
00:03:26,560 --> 00:03:34,800
of this naive solution we propose shortly isolated 
slice and aggregated training also known as sisa

45
00:03:36,080 --> 00:03:42,320
imagine we have a large data d the first step of 
sisa is to split this data set into s shards this

46
00:03:42,320 --> 00:03:48,160
will result in s non-overlapping data sets then 
we train a separate model and sharp in isolation

47
00:03:48,720 --> 00:03:54,720
thus each model only sees one shard of data 
reducing its data dependency scope the final model

48
00:03:54,720 --> 00:03:59,920
outputs the aggregate of each model's prediction 
like an example this helps recover accuracy

49
00:03:59,920 --> 00:04:06,559
loss in sharding to further optimize retraining 
time we introduce slicing during model training

50
00:04:06,560 --> 00:04:12,240
the idea works as follows normally each output of 
stochastic green descent will pass over the whole

51
00:04:12,240 --> 00:04:18,480
data set instead we slice the chart prior to the 
training and incrementally train our most slices

52
00:04:18,480 --> 00:04:24,720
as training progresses imagine we have r slices 
in a chart which are only on the first slice for

53
00:04:24,720 --> 00:04:30,000
some number of epochs with an immediately saved 
checkpoint then we turn on the first and second

54
00:04:30,000 --> 00:04:35,520
size save as checkpoint and so on and finally 
we turn on all our slices and save checkpoints

55
00:04:36,640 --> 00:04:41,680
this means that when we unlearn we can leverage 
past safe model checkpoints depending on where the

56
00:04:41,680 --> 00:04:47,200
data was seen to honor the red data point showing 
the figure we only need to retrieve one model

57
00:04:47,840 --> 00:04:52,560
on the bottom portion of one chart 
with sisa we can achieve at least

58
00:04:52,560 --> 00:04:56,720
s times speed up for unlearning any data 
point where s is the number of shards

59
00:04:59,520 --> 00:05:03,599
in system training scheme there are three hyper 
parameters that we can tune to achieve desired

60
00:05:03,600 --> 00:05:08,400
unlearning performance first if the number of 
shards is increased we can have higher speed

61
00:05:08,400 --> 00:05:14,400
up for unlearning with a cost of lower accuracy 
second if we have more sizes in each chart we

62
00:05:14,400 --> 00:05:19,120
can also obtain faster retraining speed during 
our learning but require more storage space for

63
00:05:19,120 --> 00:05:24,640
checkpoints and third we can swap in different 
aggregation strategies for better performance

64
00:05:27,040 --> 00:05:32,560
we outlined four advantages of sysap first csi 
is generally applicable to all kinds of models

65
00:05:32,560 --> 00:05:37,840
including statistical models and deep learning 
models second sysa offers an intuitive definition

66
00:05:37,840 --> 00:05:43,520
of our learning of an end of any data point 
system also provides probable and optimal

67
00:05:43,520 --> 00:05:48,080
way to unlearn data points from machine learning 
models for either the data owner or the regulator

68
00:05:49,120 --> 00:05:53,760
on the other side having multiple models 
trained with destroying the data set up in cesar

69
00:05:53,760 --> 00:05:58,159
may cause models to disagree with each other 
and subsequently reduces overall accuracy

70
00:05:59,200 --> 00:06:04,000
each model is also a weak learner which 
may not perform well on complex tasks

71
00:06:04,000 --> 00:06:09,600
or even over fade the small shard of 
training data we now turn to evaluation

72
00:06:11,680 --> 00:06:17,200
first let me describe the experimental setup for 
now we assume the unknown requests are uniformly

73
00:06:17,200 --> 00:06:22,479
distributed across certain slices and later on 
we relax this assumption the experiments are

74
00:06:22,480 --> 00:06:27,360
down six data sets including a learning from 
three easy tasks like purchase analyze region

75
00:06:27,360 --> 00:06:33,440
and three hard ones such as imagenet a key aspect 
of our evaluation is how we validate time which is

76
00:06:33,440 --> 00:06:38,880
the main notion of interest for unlearning in this 
section if not specified we'll be using unlikely

77
00:06:38,880 --> 00:06:43,760
time to report our results which is defined to 
be number of samples in the training data set

78
00:06:43,760 --> 00:06:47,760
the reason of doing so is that when we divide 
the proposed message we disagree with the

79
00:06:47,760 --> 00:06:52,320
training to multiple machines the workload time 
is highly dependent on the hardware so it becomes

80
00:06:52,320 --> 00:06:58,240
challenging for us to log the time consistently we 
made an observation here that if the same hardware

81
00:06:58,240 --> 00:07:02,480
is used workload training time is linear to 
the number of samples in the training side

82
00:07:02,480 --> 00:07:07,520
as shown in the figure the measurements obtain an 
increment of 10 percent of data set size and the

83
00:07:07,520 --> 00:07:12,479
experiment is repeated five times and we plot the 
mean and variance which is too small to be seen

84
00:07:13,600 --> 00:07:18,800
as mentioned earlier cisa stand for sharded 
isolated sliced and aggregated so let me

85
00:07:18,800 --> 00:07:22,880
start by describing the impact of sharding 
on accuracy and returning time we divide the

86
00:07:22,880 --> 00:07:28,240
whole data set into ice charts depending on the 
number of alien requests this request will fall

87
00:07:28,240 --> 00:07:34,160
in one or more of the charts and we will have to 
return all corresponding models of the symbol in

88
00:07:34,160 --> 00:07:38,640
experiments we measure the validation accuracy 
and analytical returning time with respect to

89
00:07:38,640 --> 00:07:44,560
different number of unlearning requests to compare 
the performance we also introduce two baselines

90
00:07:44,560 --> 00:07:49,200
the first one is called batch k in which there is 
only one model trained on the whole data set with

91
00:07:49,200 --> 00:07:54,000
wait time here there is key any requests and then 
return the model with these key samples removed

92
00:07:54,640 --> 00:07:59,919
the second baseline is called rs fraction here we 
also split the whole data set into s charts but

93
00:07:59,920 --> 00:08:05,360
only turn the model on one of the shards so for 
each and any requests there are two possible cases

94
00:08:05,360 --> 00:08:09,840
first is having the chart that we use during print 
the model in this case we will need to return the

95
00:08:09,840 --> 00:08:15,520
model in the either case to request hyphen in 
a chart that is not used for training then we

96
00:08:15,520 --> 00:08:20,960
don't need to do anything about it we present 
our results to answer the following questions

97
00:08:21,680 --> 00:08:26,240
does increasing number of shots improve 
returning speedups and by doing so when

98
00:08:26,240 --> 00:08:31,200
does accuracy degrade too much in the figure 
we plot returning time accuracy and number

99
00:08:31,200 --> 00:08:37,439
of unlearning requirements for isolation each 
curve is represent one unlearning method the

100
00:08:37,440 --> 00:08:41,679
black curve is a batch k baseline since there 
are three curves for six and one wise fraction

101
00:08:41,679 --> 00:08:47,280
baseline respectively each with a different value 
of s it can be seen that the batch keyboard has

102
00:08:47,280 --> 00:08:52,800
the best accuracy since it used the entire 
data set guyser but it's also the slowest one

103
00:08:52,800 --> 00:08:57,599
on the other hand the vowel wise fraction baseline 
is very fast to region but the degrading accuracy

104
00:08:57,600 --> 00:09:03,120
is significant compared to the two baselines 
these are provide a better trade-off it is faster

105
00:09:03,120 --> 00:09:09,040
to reach one than bench by 2k and hides better 
accuracies among my overalls also as you can see

106
00:09:09,760 --> 00:09:14,560
the speed up exists as long as the number of 
underlining requests is smaller than three times

107
00:09:14,560 --> 00:09:19,199
number of shards the trend discuss 
holds for other data sets as well

108
00:09:20,320 --> 00:09:25,440
we now turn to evil read the slicing procedure 
recall that in the slicing setting at its step

109
00:09:25,440 --> 00:09:30,400
we reused the model learned of the previous slices 
in a rectum manner we fine-tune it on the union of

110
00:09:30,400 --> 00:09:34,720
the previous slices and the new slice to obtain a 
new checkpoint that is the starting point for the

111
00:09:34,720 --> 00:09:40,160
following step this has consequences on how the 
requests are handled whenever a request sets a

112
00:09:40,160 --> 00:09:45,520
specific slice we need to retrain from that slice 
onward as all the following modules are affected

113
00:09:45,520 --> 00:09:49,520
as training with few slices is faster than 
training with whole data set we adapt the

114
00:09:49,520 --> 00:09:53,760
number of epochs to the number of slices we use in 
order to ensure that the model is trained for the

115
00:09:53,760 --> 00:09:59,439
same amount of time as it would without slicing we 
study the impact of slicing first by measuring the

116
00:09:59,440 --> 00:10:04,400
experimental accuracy with respect to the number 
of epochs and second by computing the analytical

117
00:10:04,400 --> 00:10:10,560
returning time in conjunction with sharding to 
illustrate the impact of slicing on the accuracy

118
00:10:10,560 --> 00:10:15,359
we present a graph of the accuracy with respect to 
the number of training epochs for the svhn dataset

119
00:10:17,360 --> 00:10:21,680
we first noticed that slice models apparently 
take more epochs to converge as the number of

120
00:10:21,680 --> 00:10:26,560
slices increases but this is actually an artifact 
due to the adaptation of the training procedure

121
00:10:26,560 --> 00:10:32,160
described earlier besides it turns out there is 
no degradation of the accuracy attributable to the

122
00:10:32,160 --> 00:10:38,160
sizing procedure at the sufficient training 
results for the datasets such as purchase

123
00:10:38,160 --> 00:10:44,240
displayed on the right are comparable we now turn 
to the impact of slicing in terms of returning

124
00:10:44,240 --> 00:10:49,120
time we are especially interested in knowing what 
is there to play between shouting and slicing

125
00:10:50,080 --> 00:10:54,480
in the experiment that follows we focus on 
the sequential setting which means requests

126
00:10:54,480 --> 00:10:59,280
are processed one by one we represent the 
speed up vertical axis obtained by both

127
00:10:59,280 --> 00:11:04,640
sharding bottom right access and slicing bottom 
left access in a 3d plot for the svhn dataset

128
00:11:05,680 --> 00:11:10,479
as we saw earlier in the talk shouting provides a 
steadily improving speed up as we add more shots

129
00:11:12,400 --> 00:11:16,800
conversely slicing shows a saturated 
behavior after a certain amount of slices

130
00:11:17,360 --> 00:11:21,760
increasing the number of slices this has 
diminishing returns beyond a few slices

131
00:11:24,160 --> 00:11:30,640
we observe we observe civilian phenomena with 
other data sets slicing provides an additional

132
00:11:30,640 --> 00:11:36,080
1.5 time speed up with respect to sharding in 
the best case in other words when saturating

133
00:11:36,080 --> 00:11:40,640
the effect seems magnified for higher number 
of shots as the speed up is already significant

134
00:11:41,760 --> 00:11:46,800
the choice of an aggregation strategy also has 
a significant impact on the accuracy of shorting

135
00:11:46,800 --> 00:11:51,040
our experiments indicate that we can obtain 
a consistently significant accuracy boost

136
00:11:51,040 --> 00:11:56,719
by using an aggregation strategy over the logits 
instead of the naive majority vote over the labels

137
00:11:56,720 --> 00:12:01,520
you may learn more about this by reading our 
paper we now ask the question can seems to do

138
00:12:01,520 --> 00:12:06,079
even better in terms of retraining efficiency if 
we have a priori knowledge of a user's probability

139
00:12:06,080 --> 00:12:10,720
to request foreign learning the service provider 
could likely ascertain the required information

140
00:12:10,720 --> 00:12:16,160
from auxiliary data for instance how they use the 
data what the local perception of that data use is

141
00:12:16,160 --> 00:12:21,839
and if they've had any prior data misuse incidents 
now imagine we have two types of users with ground

142
00:12:21,840 --> 00:12:27,200
truth data that one group is likely to unlearn 
and the other being unlikely to unlearn our prior

143
00:12:27,200 --> 00:12:31,840
thesis sharing strategy is uniform so we ignore 
this information and assume they're all equally

144
00:12:31,840 --> 00:12:36,960
likely to unlearn the image to the right shows a 
sample sharding of the user data because the data

145
00:12:36,960 --> 00:12:40,880
from likely to learn users are spread uniformly 
we have a higher probability of hitting more

146
00:12:40,880 --> 00:12:45,920
shards thus reducing the efficiency gains but 
an adaptive czar approach can take estimates of

147
00:12:45,920 --> 00:12:49,920
the ground truth probability and put likely to 
unlearned data into a minimal number of shards

148
00:12:50,480 --> 00:12:55,280
then whether the data is unlearned in a stream or 
a batch fewer shards are affected improving sisa

149
00:12:55,280 --> 00:13:01,360
efficiency further we generalize this approach as 
distribution aware starting our goal is twofold

150
00:13:01,360 --> 00:13:05,760
to maximize shard size so as to improve model 
accuracy but to minimize the chance that a shard

151
00:13:05,760 --> 00:13:10,319
or group of users has an unlearning request 
these schools are in direct opposition so to

152
00:13:10,320 --> 00:13:14,400
find a good middle ground we will assume that each 
user requesting foreign learning is an independent

153
00:13:14,400 --> 00:13:18,880
bernoulli trial akin to a weighted coin flip 
this means that we have some probability p

154
00:13:18,880 --> 00:13:22,560
potentially different for each user that they 
will request unlearning in a given time interval

155
00:13:23,360 --> 00:13:27,360
with each user having a different probability of 
unlearning it becomes difficult to calculate the

156
00:13:27,360 --> 00:13:32,240
chance that anyone from the group will unlearn our 
key insight is that if each user is a bernoulli

157
00:13:32,240 --> 00:13:36,000
trial then the distribution over the group 
follows up for some binomial distribution

158
00:13:36,880 --> 00:13:40,800
we can now calculate the expected chance 
that any user on learns in our time period

159
00:13:40,800 --> 00:13:44,880
and the calculation is quite simple it's 
the average probability across all users

160
00:13:44,880 --> 00:13:49,520
if we assume one time period for n then we 
can ignore it using this the distribution

161
00:13:49,520 --> 00:13:53,680
aware starting approach is as follows add 
users to the group represented by chi sub

162
00:13:53,680 --> 00:13:58,079
i until its expectation hits some constant 
c the smaller c is the less chance that

163
00:13:58,080 --> 00:14:01,840
that shard will need on learning but also the 
smaller its size reducing the model accuracy

164
00:14:03,040 --> 00:14:06,880
we implemented the distribution aware shining 
approach on a synthetic data set modeled from

165
00:14:06,880 --> 00:14:11,520
unlearning requests for search engines under the 
right to be forgotten the x-axis of this figure

166
00:14:11,520 --> 00:14:16,240
shows a stream of unlearning requests and the 
y-axis the analytical retraining time the top

167
00:14:16,240 --> 00:14:20,400
line is our benchmark the uniforms use a strategy 
and the bottom line is our distribution aware

168
00:14:20,400 --> 00:14:25,360
sharding approach using the poisson binomial 
distribution there are two major insights the

169
00:14:25,360 --> 00:14:30,400
adaptive strategy is never worse and it on average 
reduces the analytical returning time as desired

170
00:14:31,440 --> 00:14:36,080
our takeaways are that sisa provides a guaranteed 
and provable unlearning strategy that is easy to

171
00:14:36,080 --> 00:14:41,040
understand by non-experts it reduces retraining 
time consistently with minimal overhead in both

172
00:14:41,040 --> 00:14:46,400
storage and training algorithm changes further is 
applicable to any model train by gradient descent

173
00:14:46,400 --> 00:14:49,280
and we can leverage knowledge of the 
distribution of our learning requests

174
00:14:49,280 --> 00:14:54,079
to further improve on learning efficiency thank 
you we're happy to accept any of your questions

