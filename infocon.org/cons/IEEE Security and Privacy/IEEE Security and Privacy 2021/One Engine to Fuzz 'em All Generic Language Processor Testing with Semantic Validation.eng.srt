1
00:00:02,080 --> 00:00:07,360
Hello everyone welcome to our 
talk one engine to fuzz them all:

2
00:00:07,360 --> 00:00:11,520
generic language processor 
testing with semantic validation.

3
00:00:13,280 --> 00:00:19,360
Language processors translate programs written 
in high-level languages to low-level machine  

4
00:00:19,360 --> 00:00:29,840
code. we use lots of them every day. For example a 
browser contains processes of javascript html etc.

5
00:00:31,200 --> 00:00:37,760
Language processors should be correct otherwise 
even correct source programs will be translated  

6
00:00:37,760 --> 00:00:44,480
into malfunctional codes which might lead to 
incorrect functionalities and even security holes.

7
00:00:46,720 --> 00:00:51,360
Therefore fuzzing has been widely 
applied to improve the correctness  

8
00:00:51,360 --> 00:00:59,360
of language processors.  However fuzzing language 
processors is hard because to reach the deep logic  

9
00:00:59,360 --> 00:01:03,440
the generated input should be 
correct in syntax and semantics.  

10
00:01:04,879 --> 00:01:12,000
Otherwise a syntax error will cause a bailout 
during the syntax processing at the front end  

11
00:01:12,000 --> 00:01:17,040
and as the semantic error will cause a bailout 
during semantic actions at the backend.

12
00:01:19,920 --> 00:01:27,360
This put us in a dilemma. Nowadays we have 
over 770 real-world programming languages  

13
00:01:27,920 --> 00:01:35,440
each of them has unique features.  Generic fuzzers 
can easily apply to different languages but they  

14
00:01:35,440 --> 00:01:43,920
cannot generate high quality inputs. Specialized fuzzers are 
effective for one languages but they require  

15
00:01:43,920 --> 00:01:52,160
huge development effort to adopt on new languages.
For example given the seed input generic fuzzers  

16
00:01:52,160 --> 00:01:58,640
can only adopt simple mutations such as bitflip 
and the syntax correctness is not guaranteed  

17
00:01:59,520 --> 00:02:05,280
we can add the support of grammar and the fuzzers 
can generate syntactically correct test cases  

18
00:02:05,840 --> 00:02:12,720
but the semantic correctness is still not guaranteed.
We can further add the support of semantics  

19
00:02:13,840 --> 00:02:21,840
but after that the fuzzers will not be generic 
anymore so the question is: can we achieve both?

20
00:02:24,480 --> 00:02:32,959
to find the answers we manually check 1500 invalid 
inputs generated by existing language fuzzers and  

21
00:02:32,960 --> 00:02:40,160
find that the errors actually follow some patterns.
Among these inputs, syntax errors are few  

22
00:02:40,160 --> 00:02:47,440
because advanced fuzzers can utilize the grammar to 
help input generations. Most of them are semantic  

23
00:02:47,440 --> 00:02:54,320
errors. The common root cause is that they use 
the definition of wrong types and scopes. For  

24
00:02:54,320 --> 00:03:03,280
example at line two we are assigning a string to a 
variable of type integer so the type mismatches at  

25
00:03:03,280 --> 00:03:13,680
line 3 both a and b are integer types but b's scope 
starts from line 4 so the scope is wrong. If we can  

26
00:03:13,680 --> 00:03:20,560
avoid all these errors, the language correctness 
of the generated input will be greatly improved.  

27
00:03:22,320 --> 00:03:29,200
Therefore, we propose polyglot a generic and 
effective fuzzing framework which focuses  

28
00:03:29,200 --> 00:03:35,839
on reducing the common semantic errors. It is 
generic because it can easily apply to different  

29
00:03:35,840 --> 00:03:43,440
programming languages.  It is effective because 
it can generate high quality inputs. Polyglot  

30
00:03:43,440 --> 00:03:50,000
achieved its goals by three steps: first the 
frontend generation generates an IR translator  

31
00:03:50,000 --> 00:03:57,760
to convert the source program into an ir program.
next the constrained mutation mutates the ir  

32
00:03:57,760 --> 00:04:05,519
program to get new variants then the semantic 
validation fixes the semantic errors in the programs  

33
00:04:06,800 --> 00:04:12,080
last we convert the programs back to 
a source program and feed it into the  

34
00:04:12,080 --> 00:04:17,840
fuzzing engine to find bugs. Now let's take 
a deeper look at how these steps work  

35
00:04:19,760 --> 00:04:26,480
the goal of frontend generation is to translate 
language specific source programs into language  

36
00:04:26,480 --> 00:04:33,760
agnostic ir programs. We propose an ir 
that has syntax types and semantic properties  

37
00:04:34,720 --> 00:04:40,880
the syntax types represent the syntactic
structure of the source code and the semantic  

38
00:04:40,880 --> 00:04:45,600
properties carry information about 
the definitions scopes and types  

39
00:04:46,480 --> 00:04:55,040
these properties are predefined by us and focus
on handle the common language features. For example,  

40
00:04:55,040 --> 00:05:00,480
an ir with property function-def means 
the ir refers to a function definition.

41
00:05:02,240 --> 00:05:07,920
To make this work users should provide the 
BNF grammars and the semantic annotations

42
00:05:07,920 --> 00:05:14,800
of the language. Here's an example. On 
the left is the bnf grammar for C  

43
00:05:14,800 --> 00:05:22,080
function definition and on the right is the 
related semantic annotations. These annotations  

44
00:05:22,080 --> 00:05:30,000
tell us which symbol in the BNF grammar has semantic 
properties. With these when the source program  

45
00:05:30,000 --> 00:05:35,840
is translated into the ir program the translator 
will automatically assign those properties to  

46
00:05:35,840 --> 00:05:43,359
the corresponding ir. For example we know that a 
function body in c program will create a new scope  

47
00:05:44,000 --> 00:05:48,800
so in the annotations we add the 
property NewScope for function body  

48
00:05:50,240 --> 00:05:56,160
and when the function body in the source 
program is translated into the ir which is ir7  

49
00:05:56,720 --> 00:06:05,120
those properties in the semantic annotation will 
automatically assign to ir7. After the translation

50
00:06:05,120 --> 00:06:11,520
we can now base our mutations and analyses on 
the uniform ir regardless of the underlying  

51
00:06:11,520 --> 00:06:19,840
source language. For better demonstration we 
will still use source program in our example.

52
00:06:20,800 --> 00:06:27,040
next is the constrained mutation whose goal is to 
preserve the correctness of the modified part of  

53
00:06:27,040 --> 00:06:35,200
the programs. It has two strategies first is the 
type-based mutation which can preserve the syntax  

54
00:06:35,200 --> 00:06:42,880
correctness of the whole program. what do we do we 
modify irs with those of the same syntactic types  

55
00:06:43,920 --> 00:06:52,000
for example we can replace a < 0  with 
an ir b > 1 because they are both of  

56
00:06:52,000 --> 00:06:58,720
type expressions and after the mutations we 
can see that the syntactic correctness of the  

57
00:06:58,720 --> 00:07:07,120
whole program is still preserved. Next is the 
local mutation which can preserve the semantic  

58
00:07:07,120 --> 00:07:15,120
correctness of the unmutated part. It requires 
to apply mutation only on irs with local effect  

59
00:07:15,840 --> 00:07:22,320
which include irs that don't contain new 
definitions or irs that creates a new scope  

60
00:07:23,120 --> 00:07:30,960
For example at line 1 we define a new 
variable a and line 2 uses it. At line four  

61
00:07:30,960 --> 00:07:38,239
we define the variable d but line three to six 
as a whole is a block and create a new scope  

62
00:07:39,120 --> 00:07:45,120
therefore we cannot modify line one 
because this will invalidate line 2  

63
00:07:46,000 --> 00:07:57,920
but we can replace line 3 to 6 as a whole and 
after that line 1 and 2 will not be affected  

64
00:07:57,920 --> 00:08:03,440
after the mutation the only thing wrong 
is the semantic errors in the mutated part  

65
00:08:04,240 --> 00:08:10,800
therefore our semantic validation builds symbol 
tables with the information of scopes and types  

66
00:08:10,800 --> 00:08:17,040
carried by the ir and then generates expressions 
to replace the invalid use of variables  

67
00:08:18,320 --> 00:08:26,080
let's continue with our mutated example. w and 
s and y are not defined in the current program  

68
00:08:26,080 --> 00:08:31,120
for better demonstration we replace 
them with a special string FIXME  

69
00:08:32,880 --> 00:08:39,919
let's go through an example suppose this is the 
code from a function body. let's build the symbol  

70
00:08:39,919 --> 00:08:47,199
tables for it. each program has a global scope but 
this is unrelated to us right now so let's skip  

71
00:08:47,200 --> 00:08:55,680
that. Next is the function body scope which covers 
line one to six and the symbol table contains the  

72
00:08:55,680 --> 00:09:03,920
names, types and order id of every definition 
within the scope.  The order id is the statement  

73
00:09:03,920 --> 00:09:10,560
order in the source program which can tell us what 
variables have been defined at a given location.

74
00:09:12,320 --> 00:09:17,040
Next is the if statement body 
scope which cover line three to six  

75
00:09:18,400 --> 00:09:24,480
after we build the symbol tables we can find 
the available variables for each FIXME location  

76
00:09:25,360 --> 00:09:31,120
for line 4 we first check its most inner 
scope which is the if statement body scope  

77
00:09:32,720 --> 00:09:39,920
the definition k has order ID 4, which is 
not less than 4 so it's not available yet  

78
00:09:40,960 --> 00:09:49,680
and let's go up the order id of a is 1 it is 
less than 4 so it has been defined at line 4.  

79
00:09:51,040 --> 00:09:59,760
similarly we can find that both k and a are 
available at line 5. after that we can generate  

80
00:09:59,760 --> 00:10:07,520
expressions with these variables to replace the 
FIXME for line 4 we generate a * a

81
00:10:07,520 --> 00:10:14,240
to replace the FIXME. Similarly for 
line 5 we generate k and a to replace them  

82
00:10:16,080 --> 00:10:22,560
After the validation the programs become 
semantically correct and we can use it for fuzzing

83
00:10:25,280 --> 00:10:32,319
our results shows the effectiveness 
of polyglot.  we successfully find 173  

84
00:10:32,320 --> 00:10:39,040
bugs in 21 processors of nine programming 
languages. We choose the tested programming  

85
00:10:39,040 --> 00:10:46,800
languages based on their popularity and domains 
we cover compiled languages, interpreted languages  

86
00:10:46,800 --> 00:10:55,760
and domain-specific languages. We have 18 cves for 
our  bugs.  Some of them can lead to code execution

87
00:10:57,840 --> 00:11:05,760
we also evaluate the fuzzing performance on four 
targets in 24 hours the compared metrics include  

88
00:11:05,760 --> 00:11:13,280
the number of unique bugs,  the language correctness 
and the code coverage. We compare polyglot with 6  

89
00:11:13,280 --> 00:11:19,839
other fuzzers which include polyglot-syntax 
which is polyglot without semantic validation.

90
00:11:19,840 --> 00:11:26,880
the baseline afl, the hybrid fuzzer QSYM, 
the generic language fuzzer nautilus and the language  

91
00:11:26,880 --> 00:11:35,520
specific fuzzers DIE and CSMITH. For the number of 
unique bugs polyglot finds nine bugs while others  

92
00:11:35,520 --> 00:11:43,520
find at most two in 24 hours. And the locations 
of bugs found by polyglot are in deep logic such as  

93
00:11:43,520 --> 00:11:50,880
optimization and code generation.  For language 
correctness we check the validity of every  

94
00:11:50,880 --> 00:11:59,920
generated input and calculate their proportions.
Polyglot outperforms polyglot-syntax and generic fuzzers  

95
00:11:59,920 --> 00:12:07,040
by at least 50 percent and its correctness is 
not very far from the language specific fuzzers.

96
00:12:09,280 --> 00:12:18,800
for the code coverage polyglot outperforms polyglot-syntax
syntax by 23 to 51% and this is the contribution  

97
00:12:18,800 --> 00:12:26,079
of semantic validation. It outperforms 
generic fuzzers by at least 230 percent.

98
00:12:27,040 --> 00:12:33,520
surprisingly it also outperforms language 
specific fuzzers by at least 46 percent.

99
00:12:35,200 --> 00:12:41,200
one reason is that the input generation 
of polyglot is much faster. We have a  

100
00:12:41,200 --> 00:12:48,880
deep discussion about it in our paper. Overall 
polyglot has the best performance except for the  

101
00:12:48,880 --> 00:12:56,960
language correctness against language specific 
fuzzers.  What's the limitation of polyglot? first  

102
00:12:56,960 --> 00:13:02,560
the type information of dynamically typed 
languages cannot be determined statically.

103
00:13:03,360 --> 00:13:09,120
one possible solution is to collect it during 
dynamic execution at the cost of performance  

104
00:13:10,320 --> 00:13:16,960
next the bnf grammar is looser than the real syntax of a language that's why the syntax

105
00:13:16,960 --> 00:13:22,720
correctness of our generated inputs is not 
perfect even though we use the bnf grammar  

106
00:13:23,760 --> 00:13:28,080
one possible solution is to use machine 
learning to infer the real grammar  

107
00:13:29,200 --> 00:13:36,560
most importantly currently polyglot only focuses 
on types and scopes. We can support more common  

108
00:13:36,560 --> 00:13:42,000
language features for better performance 
but how many features can we add while  

109
00:13:42,000 --> 00:13:48,560
still keeping polyglot generic is unclear to 
us. We plan to investigate it in the future work.  

110
00:13:50,560 --> 00:13:57,520
in conclusion we propose polyglot, a generic and 
effective fuzzing framework for language processors.

111
00:13:58,480 --> 00:14:04,880
and we find that under equal conditions improving 
semantic correctness can greatly help language  

112
00:14:04,880 --> 00:14:13,920
processor fuzzing. We have open sourced polyglot 
you can find it with the provided link thank you

