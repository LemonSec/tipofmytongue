1
00:00:02,639 --> 00:00:03,679
hello everyone

2
00:00:03,679 --> 00:00:06,640
today i'm going to present gpu fast

3
00:00:06,640 --> 00:00:08,400
privacy preserving machine learning on

4
00:00:08,400 --> 00:00:09,840
the gpu

5
00:00:09,840 --> 00:00:12,160
this is joint work with brian knott ryan

6
00:00:12,160 --> 00:00:17,199
tien and david wu

7
00:00:17,199 --> 00:00:19,039
machine learning has been widely studied

8
00:00:19,039 --> 00:00:21,199
in the academia and widely adopted in

9
00:00:21,199 --> 00:00:22,800
the industry in the past

10
00:00:22,800 --> 00:00:26,000
decade the growing amount of data needed

11
00:00:26,000 --> 00:00:27,359
for machine learning model

12
00:00:27,359 --> 00:00:30,080
also raises data privacy concern

13
00:00:30,080 --> 00:00:30,640
therefore

14
00:00:30,640 --> 00:00:32,238
a rise in the need for privacy

15
00:00:32,238 --> 00:00:34,160
preserving machine learning

16
00:00:34,160 --> 00:00:36,000
for example in the case of

17
00:00:36,000 --> 00:00:37,520
privacy-preserving machine learning

18
00:00:37,520 --> 00:00:38,559
inference

19
00:00:38,559 --> 00:00:41,040
multiple hospitals hold a proprietary

20
00:00:41,040 --> 00:00:43,360
model for disease diagnosis

21
00:00:43,360 --> 00:00:45,840
and there's a patient who holds a

22
00:00:45,840 --> 00:00:47,280
sensitive medical image

23
00:00:47,280 --> 00:00:48,960
and wants to determine the disease he

24
00:00:48,960 --> 00:00:51,760
has the patient will send the medical

25
00:00:51,760 --> 00:00:53,440
image to the machine learning model and

26
00:00:53,440 --> 00:00:55,440
the machine learning model will output a

27
00:00:55,440 --> 00:00:58,800
disease diagnosis in order for machine

28
00:00:58,800 --> 00:01:00,559
learning influence to be private

29
00:01:00,559 --> 00:01:03,440
we require that first hospitals should

30
00:01:03,440 --> 00:01:04,879
not learn about the patient's medical

31
00:01:04,879 --> 00:01:05,680
data

32
00:01:05,680 --> 00:01:07,680
and second the patient should not learn

33
00:01:07,680 --> 00:01:09,360
about the ways of the machine learning

34
00:01:09,360 --> 00:01:11,600
model

35
00:01:11,600 --> 00:01:13,200
similarly in the case of privacy

36
00:01:13,200 --> 00:01:15,119
preserving machine learning training

37
00:01:15,119 --> 00:01:17,200
multiple hospitals may want to train

38
00:01:17,200 --> 00:01:18,960
their proprietary model for disease

39
00:01:18,960 --> 00:01:20,400
diagnosis

40
00:01:20,400 --> 00:01:22,320
so the patient will send his medical

41
00:01:22,320 --> 00:01:24,400
image to the machine learning model

42
00:01:24,400 --> 00:01:27,040
the model will output a training loss

43
00:01:27,040 --> 00:01:29,439
instead of a disease diagnosis

44
00:01:29,439 --> 00:01:31,200
this training loss is then back

45
00:01:31,200 --> 00:01:33,200
propagated through the model to update

46
00:01:33,200 --> 00:01:35,280
the model weights

47
00:01:35,280 --> 00:01:37,600
similarly we require that hospitals

48
00:01:37,600 --> 00:01:38,880
should not learn about the patient's

49
00:01:38,880 --> 00:01:40,000
medical data

50
00:01:40,000 --> 00:01:41,600
and the patient should not learn about

51
00:01:41,600 --> 00:01:42,880
the weights of the machine learning

52
00:01:42,880 --> 00:01:44,880
model

53
00:01:44,880 --> 00:01:46,560
note that privacy preserving machine

54
00:01:46,560 --> 00:01:47,920
learning training is a much more

55
00:01:47,920 --> 00:01:49,840
challenging task than privacy preserving

56
00:01:49,840 --> 00:01:50,640
inference

57
00:01:50,640 --> 00:01:53,040
because first input data are usually

58
00:01:53,040 --> 00:01:54,159
trained in batches

59
00:01:54,159 --> 00:01:56,960
of hundreds and even thousands of images

60
00:01:56,960 --> 00:01:57,680
and second

61
00:01:57,680 --> 00:01:59,920
there is an azure and extra back

62
00:01:59,920 --> 00:02:01,759
propagation step to update the model

63
00:02:01,759 --> 00:02:02,880
weights

64
00:02:02,880 --> 00:02:04,960
therefore privacy preserving training is

65
00:02:04,960 --> 00:02:07,040
much more computationally demanding than

66
00:02:07,040 --> 00:02:08,080
privacy preserving

67
00:02:08,080 --> 00:02:10,479
influence

68
00:02:11,440 --> 00:02:13,280
both privacy-preserving machine learning

69
00:02:13,280 --> 00:02:15,599
and inference can be achieved via secure

70
00:02:15,599 --> 00:02:17,599
multi-party computation

71
00:02:17,599 --> 00:02:19,599
in our work we considered the following

72
00:02:19,599 --> 00:02:21,440
threat model for secure multi-party

73
00:02:21,440 --> 00:02:22,959
computation

74
00:02:22,959 --> 00:02:25,120
we consider a three-party semi-honest

75
00:02:25,120 --> 00:02:28,160
security with honest majority model

76
00:02:28,160 --> 00:02:30,640
where honest majority means that we

77
00:02:30,640 --> 00:02:32,640
allow a single semi honest party for

78
00:02:32,640 --> 00:02:33,840
corruption

79
00:02:33,840 --> 00:02:36,160
and semi honest means that corrupt

80
00:02:36,160 --> 00:02:38,000
parties will follow the protocol

81
00:02:38,000 --> 00:02:39,840
but will try to gather information out

82
00:02:39,840 --> 00:02:42,560
of the protocol

83
00:02:43,280 --> 00:02:45,599
one of the one of the major tasks in

84
00:02:45,599 --> 00:02:47,360
machine learning and privacy preserving

85
00:02:47,360 --> 00:02:50,879
machine learning is image classification

86
00:02:50,879 --> 00:02:54,239
in the beginning image classification is

87
00:02:54,239 --> 00:02:56,319
operated on small data sets

88
00:02:56,319 --> 00:02:59,599
such as mnist which which has 60 000

89
00:02:59,599 --> 00:03:00,400
images

90
00:03:00,400 --> 00:03:03,120
and on small networks such as lynette

91
00:03:03,120 --> 00:03:03,760
which has

92
00:03:03,760 --> 00:03:07,120
a couple of layers with the advancing

93
00:03:07,120 --> 00:03:08,720
computational powers

94
00:03:08,720 --> 00:03:10,800
image classification has shifted to

95
00:03:10,800 --> 00:03:12,159
large data sets such as

96
00:03:12,159 --> 00:03:15,840
imagenet which has over 1 million images

97
00:03:15,840 --> 00:03:19,280
containing 1 000 classes and on

98
00:03:19,280 --> 00:03:23,040
net models such as resnet which has

99
00:03:23,040 --> 00:03:26,480
up to 200 layers

100
00:03:26,959 --> 00:03:28,959
however most of the work in privacy

101
00:03:28,959 --> 00:03:30,400
preserving machine learning

102
00:03:30,400 --> 00:03:33,120
has been restricted to small data sets

103
00:03:33,120 --> 00:03:35,599
and small models

104
00:03:35,599 --> 00:03:38,640
not until recently there are

105
00:03:38,640 --> 00:03:40,799
certain there are several works that try

106
00:03:40,799 --> 00:03:42,640
to extend privacy preserving machine

107
00:03:42,640 --> 00:03:44,480
learning to large data sets and large

108
00:03:44,480 --> 00:03:45,760
models

109
00:03:45,760 --> 00:03:49,440
these are delphi crypt flow and falcom

110
00:03:49,440 --> 00:03:51,519
and falcon additionally support private

111
00:03:51,519 --> 00:03:53,120
training

112
00:03:53,120 --> 00:03:55,680
however despite the recent events there

113
00:03:55,680 --> 00:03:57,760
is still a significant performance gap

114
00:03:57,760 --> 00:03:58,879
between

115
00:03:58,879 --> 00:04:00,640
privacy preserving machine learning and

116
00:04:00,640 --> 00:04:02,799
plain text machine learning

117
00:04:02,799 --> 00:04:05,040
for example in the case of private

118
00:04:05,040 --> 00:04:07,040
inference the state-of-the-art work crit

119
00:04:07,040 --> 00:04:08,000
flow

120
00:04:08,000 --> 00:04:11,519
has a 23 000 x gap compared to plain tax

121
00:04:11,519 --> 00:04:14,640
influence and in private training the

122
00:04:14,640 --> 00:04:16,560
state-of-the-art work falcon

123
00:04:16,560 --> 00:04:19,759
has a 42 000x gap compared to plaintext

124
00:04:19,759 --> 00:04:21,358
training

125
00:04:21,358 --> 00:04:23,680
therefore there is still a significant

126
00:04:23,680 --> 00:04:25,520
performance gap between plaintext and

127
00:04:25,520 --> 00:04:28,240
private machine learning

128
00:04:28,240 --> 00:04:30,560
to make privacy preserving pre-service

129
00:04:30,560 --> 00:04:31,919
machine learning practical

130
00:04:31,919 --> 00:04:35,840
our main challenge is scalability

131
00:04:35,840 --> 00:04:38,000
so how do we reduce this gap in privacy

132
00:04:38,000 --> 00:04:40,639
preserving machine learning

133
00:04:40,639 --> 00:04:42,880
previously when the data sets and models

134
00:04:42,880 --> 00:04:43,840
are small

135
00:04:43,840 --> 00:04:46,320
communications of npc protocols is the

136
00:04:46,320 --> 00:04:47,759
major bottleneck

137
00:04:47,759 --> 00:04:49,919
therefore it would make sense to design

138
00:04:49,919 --> 00:04:51,120
more communication

139
00:04:51,120 --> 00:04:53,840
efficient protocols for linear and

140
00:04:53,840 --> 00:04:56,639
nonlinear operations in machine learning

141
00:04:56,639 --> 00:04:58,479
many of the prior work has taken this

142
00:04:58,479 --> 00:05:00,560
approach

143
00:05:00,560 --> 00:05:02,800
however as we scale to large data sets

144
00:05:02,800 --> 00:05:03,680
and models

145
00:05:03,680 --> 00:05:05,680
computations of machine learning models

146
00:05:05,680 --> 00:05:08,240
becomes a major bottleneck

147
00:05:08,240 --> 00:05:10,639
we could leverage hardware accelerations

148
00:05:10,639 --> 00:05:13,120
such as gpu accelerations to speed up

149
00:05:13,120 --> 00:05:14,479
computation submission

150
00:05:14,479 --> 00:05:17,520
of machine learning models prior work

151
00:05:17,520 --> 00:05:18,639
such as stealthy

152
00:05:18,639 --> 00:05:22,320
has considered this approach

153
00:05:22,800 --> 00:05:25,919
gpu acceleration is extremely important

154
00:05:25,919 --> 00:05:28,160
for machine learning

155
00:05:28,160 --> 00:05:29,840
take plain text machine learning for

156
00:05:29,840 --> 00:05:32,800
example if you want to train resnets on

157
00:05:32,800 --> 00:05:34,080
imagenet

158
00:05:34,080 --> 00:05:36,720
on a single cpu it would take over a

159
00:05:36,720 --> 00:05:38,479
year to train

160
00:05:38,479 --> 00:05:41,360
however on a single gpu it will take

161
00:05:41,360 --> 00:05:44,720
only roughly a week to train

162
00:05:44,720 --> 00:05:47,520
if for plain text machine learning we

163
00:05:47,520 --> 00:05:47,840
need

164
00:05:47,840 --> 00:05:51,360
gpu in order to train a model so it is

165
00:05:51,360 --> 00:05:53,840
for privacy-preserving machine learning

166
00:05:53,840 --> 00:05:54,639
in fact

167
00:05:54,639 --> 00:05:56,400
we observe from the state-of-the-art

168
00:05:56,400 --> 00:05:57,680
privacy preserving

169
00:05:57,680 --> 00:06:00,720
training work falcom

170
00:06:00,720 --> 00:06:03,360
we observed that 98 of falcon's training

171
00:06:03,360 --> 00:06:04,479
time comes from

172
00:06:04,479 --> 00:06:08,400
computing linear layers to sum up

173
00:06:08,400 --> 00:06:10,560
scalability is the biggest challenge in

174
00:06:10,560 --> 00:06:13,120
privacy preserving machine learning

175
00:06:13,120 --> 00:06:15,360
and linear layer is our main performance

176
00:06:15,360 --> 00:06:17,360
bottleneck

177
00:06:17,360 --> 00:06:19,520
gpu can significantly accelerate

178
00:06:19,520 --> 00:06:21,919
computations of linear layers

179
00:06:21,919 --> 00:06:24,560
therefore we need gpu acceleration for

180
00:06:24,560 --> 00:06:28,240
privacy preserving machine learning

181
00:06:28,479 --> 00:06:31,600
however it is not that easy to directly

182
00:06:31,600 --> 00:06:33,600
support gpu for privacy preserving

183
00:06:33,600 --> 00:06:36,080
machine learning

184
00:06:36,080 --> 00:06:38,319
plain text machine learning performs gpu

185
00:06:38,319 --> 00:06:39,120
computation

186
00:06:39,120 --> 00:06:42,479
by invoking cuda kernels

187
00:06:42,479 --> 00:06:44,240
and plain text machine learning uses

188
00:06:44,240 --> 00:06:47,680
floating point numbers for computation

189
00:06:47,680 --> 00:06:50,400
however mpc protocols operate on fixed

190
00:06:50,400 --> 00:06:51,840
point arithmetic

191
00:06:51,840 --> 00:06:54,080
which required us to convert a floating

192
00:06:54,080 --> 00:06:55,680
point number into a fixed point

193
00:06:55,680 --> 00:06:56,880
representation

194
00:06:56,880 --> 00:07:01,360
represented by a 32 or 64-bit integer

195
00:07:01,360 --> 00:07:03,120
however there is no cuda kernel

196
00:07:03,120 --> 00:07:04,560
supported for computation

197
00:07:04,560 --> 00:07:08,560
over 32 or 64-bit integer types

198
00:07:08,560 --> 00:07:10,800
therefore we will need to support cuda

199
00:07:10,800 --> 00:07:14,240
for fixed-point arithmetic

200
00:07:15,120 --> 00:07:17,599
we present our system crypt gpu which

201
00:07:17,599 --> 00:07:19,759
supports end-to-end private training and

202
00:07:19,759 --> 00:07:23,199
influence on gpu our system supports

203
00:07:23,199 --> 00:07:24,720
private influence and training in a

204
00:07:24,720 --> 00:07:26,960
three-party semi-auto setting

205
00:07:26,960 --> 00:07:29,280
and we keep all of our computations on

206
00:07:29,280 --> 00:07:31,280
the gpu

207
00:07:31,280 --> 00:07:33,919
we've done a systematic evaluations of

208
00:07:33,919 --> 00:07:36,400
gpu-based multi-party computation

209
00:07:36,400 --> 00:07:38,240
and observed that both linear and

210
00:07:38,240 --> 00:07:40,639
nonlinear operations benefit from gpu

211
00:07:40,639 --> 00:07:42,560
acceleration

212
00:07:42,560 --> 00:07:45,440
we built our work on top of pi torch the

213
00:07:45,440 --> 00:07:46,879
state-of-the-art machine learning

214
00:07:46,879 --> 00:07:47,599
framework

215
00:07:47,599 --> 00:07:49,840
and crypt 10 a privacy pre-service

216
00:07:49,840 --> 00:07:51,759
machine learning framework built on top

217
00:07:51,759 --> 00:07:52,160
of

218
00:07:52,160 --> 00:07:56,960
pytorch overall our system shows a 2.5

219
00:07:56,960 --> 00:07:57,319
to

220
00:07:57,319 --> 00:08:00,319
3.7x improvement over cryptflow and

221
00:08:00,319 --> 00:08:00,879
falcon

222
00:08:00,879 --> 00:08:04,240
on private inference and a 7 to 36x

223
00:08:04,240 --> 00:08:04,960
improvement

224
00:08:04,960 --> 00:08:08,799
over falcom on private training

225
00:08:09,280 --> 00:08:11,360
our system design can be mainly

226
00:08:11,360 --> 00:08:12,879
summarized into three

227
00:08:12,879 --> 00:08:16,479
components first we use replicated

228
00:08:16,479 --> 00:08:17,280
secret sharing

229
00:08:17,280 --> 00:08:20,479
as our basic building block second

230
00:08:20,479 --> 00:08:23,360
we we design mechanisms to embed fixed

231
00:08:23,360 --> 00:08:25,520
point arithmetic into cuda's floating

232
00:08:25,520 --> 00:08:27,440
point computation

233
00:08:27,440 --> 00:08:29,919
and third we develop gpu friendly

234
00:08:29,919 --> 00:08:34,000
protocols for nonlinear operations

235
00:08:34,159 --> 00:08:36,399
first we use replicated sql sharing as

236
00:08:36,399 --> 00:08:38,320
our basic building block

237
00:08:38,320 --> 00:08:40,320
replicated sql sharing is a type of

238
00:08:40,320 --> 00:08:42,000
additive secret sharing

239
00:08:42,000 --> 00:08:44,720
where multiple parties that hold its own

240
00:08:44,720 --> 00:08:46,080
secret shares

241
00:08:46,080 --> 00:08:47,680
sends its own secret share to the

242
00:08:47,680 --> 00:08:49,440
neighboring parties

243
00:08:49,440 --> 00:08:52,160
after replicated secret sharing each

244
00:08:52,160 --> 00:08:53,680
party will hold a two

245
00:08:53,680 --> 00:08:56,800
out of three secret shares replicated

246
00:08:56,800 --> 00:08:58,480
secret sharing supports addition

247
00:08:58,480 --> 00:09:01,279
and multiplication and it is a efficient

248
00:09:01,279 --> 00:09:02,320
building block for

249
00:09:02,320 --> 00:09:05,519
three-party computation

250
00:09:06,959 --> 00:09:10,160
second we go over how we fit

251
00:09:10,160 --> 00:09:12,880
how we perform integer operations using

252
00:09:12,880 --> 00:09:14,080
using floating point

253
00:09:14,080 --> 00:09:18,240
arithmetic we rely on the following two

254
00:09:18,240 --> 00:09:19,279
properties

255
00:09:19,279 --> 00:09:22,000
first exact computations for small

256
00:09:22,000 --> 00:09:23,200
values

257
00:09:23,200 --> 00:09:26,399
namely the 64-bit floating point values

258
00:09:26,399 --> 00:09:28,880
has 52 bits of precision

259
00:09:28,880 --> 00:09:30,959
which means that it can represent all

260
00:09:30,959 --> 00:09:32,480
integers between range

261
00:09:32,480 --> 00:09:37,440
negative 2 to the 52 to 2 to the 52

262
00:09:37,440 --> 00:09:40,320
for integers a b in range negative 2 to

263
00:09:40,320 --> 00:09:42,399
the 26 to negative 2

264
00:09:42,399 --> 00:09:45,600
to 2 to the 26 their product a b

265
00:09:45,600 --> 00:09:47,680
can be represented directly as following

266
00:09:47,680 --> 00:09:49,519
point values without any loss of

267
00:09:49,519 --> 00:09:51,600
precision

268
00:09:51,600 --> 00:09:55,440
apparently for 32 or 64-bit integers

269
00:09:55,440 --> 00:09:58,160
required for multi-party computation

270
00:09:58,160 --> 00:10:00,000
their product cannot be represented

271
00:10:00,000 --> 00:10:02,079
directly as folding point values without

272
00:10:02,079 --> 00:10:04,800
any loss of precision we therefore need

273
00:10:04,800 --> 00:10:06,079
the second property

274
00:10:06,079 --> 00:10:10,000
which is bilinearity bilinearity simply

275
00:10:10,000 --> 00:10:10,640
says

276
00:10:10,640 --> 00:10:13,760
products of two integers a and b can be

277
00:10:13,760 --> 00:10:15,760
broken down into products of smaller

278
00:10:15,760 --> 00:10:16,640
integers

279
00:10:16,640 --> 00:10:21,760
a1 plus a2 times b1 plus b2

280
00:10:22,160 --> 00:10:24,640
using this idea we can convert product

281
00:10:24,640 --> 00:10:27,680
of large integers such as 32 or 64-bit

282
00:10:27,680 --> 00:10:28,640
integers

283
00:10:28,640 --> 00:10:30,880
into sums of product of small integers

284
00:10:30,880 --> 00:10:32,079
such as 16-bit

285
00:10:32,079 --> 00:10:34,959
integers that can be directly whose

286
00:10:34,959 --> 00:10:36,880
products can be directly represented

287
00:10:36,880 --> 00:10:40,959
as folding point values without any loss

288
00:10:41,440 --> 00:10:43,920
this mechanism directly generalized

289
00:10:43,920 --> 00:10:46,560
operations on matrices

290
00:10:46,560 --> 00:10:48,880
and based upon this mechanism we

291
00:10:48,880 --> 00:10:50,880
developed a module called cuda lung

292
00:10:50,880 --> 00:10:52,240
tensor in our system

293
00:10:52,240 --> 00:10:55,200
as an attraction

294
00:10:57,279 --> 00:11:00,560
finally we go over the principle

295
00:11:00,560 --> 00:11:04,640
of our gpu-friendly protocol design

296
00:11:04,640 --> 00:11:07,440
gpu is optimized for parallel processing

297
00:11:07,440 --> 00:11:09,440
of simple operations

298
00:11:09,440 --> 00:11:11,920
for example component-wise operations

299
00:11:11,920 --> 00:11:12,480
such as

300
00:11:12,480 --> 00:11:15,040
matrix addition and multiplication are

301
00:11:15,040 --> 00:11:16,160
fast

302
00:11:16,160 --> 00:11:18,320
whereas operations with lots of

303
00:11:18,320 --> 00:11:21,360
conditional statements are slow

304
00:11:21,360 --> 00:11:23,040
therefore we will want to design

305
00:11:23,040 --> 00:11:26,480
protocols suited for parallel processing

306
00:11:26,480 --> 00:11:29,200
for example for private division we

307
00:11:29,200 --> 00:11:30,880
could use the global circuit to compute

308
00:11:30,880 --> 00:11:32,399
private division

309
00:11:32,399 --> 00:11:34,320
but global circuit is less suited for

310
00:11:34,320 --> 00:11:36,399
gpu parallelism

311
00:11:36,399 --> 00:11:38,240
we therefore use approximation based

312
00:11:38,240 --> 00:11:41,120
method to compute private division

313
00:11:41,120 --> 00:11:44,160
similarly in the case of exponentiation

314
00:11:44,160 --> 00:11:46,640
we approximate exponentiation using a

315
00:11:46,640 --> 00:11:47,920
limit-based approach

316
00:11:47,920 --> 00:11:50,320
which turns exponentiation into a

317
00:11:50,320 --> 00:11:52,639
sequence of multiplication that is

318
00:11:52,639 --> 00:11:55,279
suited for gpu parallelism

319
00:11:55,279 --> 00:11:57,120
for a detailed description of our

320
00:11:57,120 --> 00:12:01,279
protocols please refer to our paper

321
00:12:03,040 --> 00:12:05,279
we've done a systematic evaluation of

322
00:12:05,279 --> 00:12:06,959
the performance of our system

323
00:12:06,959 --> 00:12:09,040
and compare it with some of the prior

324
00:12:09,040 --> 00:12:10,560
work

325
00:12:10,560 --> 00:12:13,680
in the case of private inference we

326
00:12:13,680 --> 00:12:14,880
compare with falcon

327
00:12:14,880 --> 00:12:18,480
and crypt flow on small data sets and

328
00:12:18,480 --> 00:12:20,079
models

329
00:12:20,079 --> 00:12:22,800
where communication is the bottleneck

330
00:12:22,800 --> 00:12:23,360
gpu

331
00:12:23,360 --> 00:12:26,639
acceleration does not bring much benefit

332
00:12:26,639 --> 00:12:29,680
and our system is slower than falcon on

333
00:12:29,680 --> 00:12:33,200
on the net and mnist however as we scale

334
00:12:33,200 --> 00:12:34,320
to larger

335
00:12:34,320 --> 00:12:36,639
models and data sets we begin to see the

336
00:12:36,639 --> 00:12:39,120
advantage of gpu acceleration

337
00:12:39,120 --> 00:12:42,880
on vg16 on tiny imagenet we observe

338
00:12:42,880 --> 00:12:46,560
a 3.7 x improvements

339
00:12:46,959 --> 00:12:50,000
compared to crypt flow we

340
00:12:50,000 --> 00:12:53,200
see a 2.5 2.5x improvements

341
00:12:53,200 --> 00:12:56,079
on resnets

342
00:12:56,639 --> 00:12:59,120
please refer to our paper for the full

343
00:12:59,120 --> 00:13:01,920
measurement details

344
00:13:01,920 --> 00:13:03,440
in the case of private training we

345
00:13:03,440 --> 00:13:05,519
compare with the state-of-the-art result

346
00:13:05,519 --> 00:13:07,839
falcon

347
00:13:07,839 --> 00:13:10,079
we found that on small data sets and

348
00:13:10,079 --> 00:13:11,519
models gpu

349
00:13:11,519 --> 00:13:14,959
has a 7x speedup and on large data sets

350
00:13:14,959 --> 00:13:16,560
and models such as attic said

351
00:13:16,560 --> 00:13:19,680
on tiny imagenet crypt gpu has a 36s

352
00:13:19,680 --> 00:13:21,680
speed up

353
00:13:21,680 --> 00:13:25,040
overall we see a 7x to 36s improvement

354
00:13:25,040 --> 00:13:28,800
over falcon on private training

355
00:13:29,839 --> 00:13:33,519
finally we run a micro benchmarks

356
00:13:33,519 --> 00:13:35,680
comparing the performance

357
00:13:35,680 --> 00:13:40,560
of npc protocols on gpu versus on cpu

358
00:13:40,560 --> 00:13:42,959
for private convolution running the

359
00:13:42,959 --> 00:13:46,240
protocol on gpu is 100 x faster

360
00:13:46,240 --> 00:13:48,959
faster than on cpu and for private

361
00:13:48,959 --> 00:13:49,519
reload

362
00:13:49,519 --> 00:13:51,760
running the protocol on gpu is 10x

363
00:13:51,760 --> 00:13:52,639
faster

364
00:13:52,639 --> 00:13:55,839
than on cpu our micro benchmark

365
00:13:55,839 --> 00:13:57,839
shows that both linear and non-air

366
00:13:57,839 --> 00:14:02,959
operations benefit from gpu parallelism

367
00:14:02,959 --> 00:14:05,519
to sum up we present a system that

368
00:14:05,519 --> 00:14:06,079
supports

369
00:14:06,079 --> 00:14:07,440
end-to-end private training and

370
00:14:07,440 --> 00:14:09,279
influence on the gpu

371
00:14:09,279 --> 00:14:11,440
our system improves if influence

372
00:14:11,440 --> 00:14:14,800
efficiency by 2.5 to 3.7x

373
00:14:14,800 --> 00:14:17,839
and training efficiency by 7x to 36x

374
00:14:17,839 --> 00:14:18,639
compared to

375
00:14:18,639 --> 00:14:22,160
prior work our result shows that gpu can

376
00:14:22,160 --> 00:14:24,399
significantly accelerate bottlenecks in

377
00:14:24,399 --> 00:14:27,120
linear layers

378
00:14:27,600 --> 00:14:29,360
cryptgpu currently only supports

379
00:14:29,360 --> 00:14:31,680
computations on a single gpus

380
00:14:31,680 --> 00:14:34,160
to further still scale the system up we

381
00:14:34,160 --> 00:14:37,440
need to support multiple gpus

382
00:14:37,440 --> 00:14:41,279
also if it would be good to design

383
00:14:41,279 --> 00:14:43,440
more efficient multi-party computation

384
00:14:43,440 --> 00:14:45,120
protocols that better leverage

385
00:14:45,120 --> 00:14:48,160
gpu parallelism

386
00:14:48,160 --> 00:14:55,120
thank you

