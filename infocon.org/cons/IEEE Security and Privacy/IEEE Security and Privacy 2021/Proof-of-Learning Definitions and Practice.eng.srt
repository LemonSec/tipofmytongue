1
00:00:00,390 --> 00:00:03,900
Hello, my name is Christopher and I have the
pleasure of beginning this presentation on

2
00:00:03,900 --> 00:00:09,260
our work titled Proof of learning: definitions
and practice. Later you’ll hear from my

3
00:00:09,260 --> 00:00:12,140
coauthors Mohammad, Hengrui, Anvith, and Natalie.

4
00:00:12,140 --> 00:00:16,650
What motivated our work was two major challenges
in machine learning. The first is the challenge

5
00:00:16,650 --> 00:00:20,849
of machine learning intellectual property
resolution, which asks the question: “how

6
00:00:20,850 --> 00:00:24,919
can a model owner prove that they produced
a model? This is an open and difficult challenge

7
00:00:24,919 --> 00:00:30,240
in ML that arises due to the popular model
extraction attack. Given some publicly hosted

8
00:00:30,240 --> 00:00:35,239
model, an adversary employing this attack
can reverse engineer a model by querying it.

9
00:00:35,239 --> 00:00:38,968
Here, an arbitrator may be needed to facilitate
ownership resolution.

10
00:00:38,969 --> 00:00:43,579
The second challenge surrounds Byzantine workers
in distributed and collaborative learning

11
00:00:43,579 --> 00:00:47,590
setups, which are of growing popularity in
the machine learning community. Here we see

12
00:00:47,590 --> 00:00:52,260
a central server with distributed workers
across the bottom. We pondered what happened

13
00:00:52,260 --> 00:00:57,030
when byzantine workers deviated from the designated
collaborative protocol and then asked ourselves:

14
00:00:57,030 --> 00:00:58,789
“is there a way to verify computations made
by distributed workers?”

15
00:00:58,789 --> 00:00:59,789
Prior literature has demonstrated several
threats in the ML ecosystem. We will focus

16
00:00:59,789 --> 00:01:00,789
on two motivating examples: First, publicly
hosted ML models can be “reverse engineered”

17
00:01:00,789 --> 00:01:01,789
through interaction via a simple query interface.
Such an extraction adversary can create a

18
00:01:01,789 --> 00:01:02,789
“stolen” copy of the model. A trusted
arbitrator may be needed to determine whether

19
00:01:02,789 --> 00:01:03,789
a particular model is stolen or not.

20
00:01:03,789 --> 00:01:04,789
Another threat is associated with outsourcing
training in the case of distributed learning

21
00:01:04,789 --> 00:01:05,789
applications. Is the worker that receives
a training request performing the right computation?

22
00:01:05,789 --> 00:01:06,789
Can a trusted arbitrator verify the outputs
of said worker?

23
00:01:06,789 --> 00:01:07,789
There are really three related works that
address these questions of ownership resolution

24
00:01:07,789 --> 00:01:09,409
and byzantine workers in collaborative learning.
These are watermarking defenses and fingerprinting

25
00:01:09,409 --> 00:01:13,290
for the former, or verifiable computations
for the latter.

26
00:01:13,290 --> 00:01:17,310
However, both watermarking and fingerprinting
lead to unfavorable trade-offs between their

27
00:01:17,310 --> 00:01:21,530
success and the model utility, meaning that
it can be difficult to employ these solutions

28
00:01:21,530 --> 00:01:26,860
while attaining similarly performant models.
They also require training-time modifications,

29
00:01:26,860 --> 00:01:32,259
limiting their general applicability. Verifiable
computations require inference-time alterations

30
00:01:32,259 --> 00:01:36,479
such as changes to architectures or restrictions
on what operations can be used.

31
00:01:36,479 --> 00:01:38,740
These unfavorable side-effects led us to the
question:

32
00:01:38,740 --> 00:01:39,740
Prior approaches towards solving the aforementioned
problems have several shortcomings.

33
00:01:39,740 --> 00:01:40,740
First, approaches like watermarking or fingerprinting
to detect extraction induce utility degradation.

34
00:01:40,740 --> 00:01:41,740
More effective these methods, more accuracy
penalty is paid by the user. Additionally,

35
00:01:41,740 --> 00:01:42,740
some of these techniques are not broadly applicable
to various classes of ML models.

36
00:01:42,740 --> 00:01:43,740
Secondly, some of this utility degradation
can be attributed to the fact that these techniques

37
00:01:43,740 --> 00:01:44,740
require modifications to the training procedure,
which is oftentimes complex.

38
00:01:44,740 --> 00:01:45,740
With respect to verifying computations, prior
work like CryptoNAS require modifications

39
00:01:45,740 --> 00:01:46,740
to be made to ML model architectures and restrictions
on operations resulting in “inference-time

40
00:01:46,740 --> 00:01:47,740
alterations”.
Can we do better?

41
00:01:47,740 --> 00:01:48,740
What if learners could create a verifiable
proof of their efforts in training a model?

42
00:01:48,740 --> 00:01:49,740
If they could, they could provide these proofs
to arbitrators to resolve ownership conflicts,

43
00:01:49,740 --> 00:01:53,550
or central servers could use them to verify
if the delegated computation was performed

44
00:01:53,550 --> 00:01:54,550
correctly.

45
00:01:54,550 --> 00:01:58,329
As we will show, that is what our Proof of
Learning provides.

46
00:01:58,329 --> 00:02:02,270
It is a permanent record of the entire training
effort expended by the learner, not just the

47
00:02:02,270 --> 00:02:06,810
end result. It can be used in many ways, including
aiding in temporal guarantees like who learned

48
00:02:06,810 --> 00:02:08,729
a model (or, created a proof) first.

49
00:02:08,729 --> 00:02:13,050
It is also naturally amenable to verification
by an arbitrator and is application-agnostic.

50
00:02:13,050 --> 00:02:19,190
This means that a proof algorithm simultaneously
takes both ownership resolution and verified

51
00:02:19,190 --> 00:02:20,680
training computations.

52
00:02:20,680 --> 00:02:25,830
Finally, tied back to challenges with prior
work, a proof can be boiled down to a form

53
00:02:25,830 --> 00:02:29,860
of logging, which means there are no changes
to already complex training procedures, outside

54
00:02:29,860 --> 00:02:34,430
of adding those logs. Thus, there is also
no trade-off between the proof success with

55
00:02:34,430 --> 00:02:35,430
model utility.

56
00:02:35,430 --> 00:02:36,430
Firstly, the proof is a “permanent” record
of the effort expended by the learner. It

57
00:02:36,430 --> 00:02:37,430
can be used in many ways, including being
linked with mechanisms to ensure temporal

58
00:02:37,430 --> 00:02:38,430
guarantees i.e. for the same learning task
-- learner A learned a model before learner

59
00:02:38,430 --> 00:02:39,430
B because proof A was created before proof
B.

60
00:02:39,430 --> 00:02:40,430
Secondly, a proof is naturally amenable to
verification by a trusted arbitrator & is

61
00:02:40,430 --> 00:02:41,430
application-agnostic i.e., it can be used
for both extraction and verifying the correctness

62
00:02:41,430 --> 00:02:42,430
of computations.

63
00:02:42,430 --> 00:02:43,430
Finally, creating a proof involves some form
of logging. Oftentimes, there is little to

64
00:02:43,430 --> 00:02:44,430
no change in training procedures and thus
there’s no utility degradation whatsoever.

65
00:02:44,430 --> 00:02:45,430
Hello I’m Mohammad Yaghini and I will be
presenting our main contribution, the proof-of-learning.

66
00:02:45,430 --> 00:02:46,430
Let’s start with an observation. Training
a model is often an incremental process. Model

67
00:02:46,430 --> 00:02:49,200
weights are initialized from a random state
W_0 and updated as new data is provided to

68
00:02:49,200 --> 00:02:53,560
the model. These updates naturally form a
chain, each link of which can be verified

69
00:02:53,560 --> 00:03:00,010
by an arbitrator who can perform the update
(via gradient descent, for example) and verify

70
00:03:00,010 --> 00:03:04,480
that the resulting weights are close to the
weights mentioned in the proof. So is it enough

71
00:03:04,480 --> 00:03:10,500
to only record the weight sequence, then?
The answer is No, because weights are not

72
00:03:10,500 --> 00:03:12,340
enough to reproduce the updates.

73
00:03:12,340 --> 00:03:17,510
We also need data, and auxiliary information
such as the learning rate used to make the

74
00:03:17,510 --> 00:03:23,640
update. Therefore these pieces of information
should also appear in the proof.

75
00:03:23,640 --> 00:03:27,869
We define a proof of learning (or PoL) as
the tuple (W, I, H, A), where

76
00:03:27,870 --> 00:03:32,319
W is the sequence of model weights
I is the set of data indices used to learn

77
00:03:32,319 --> 00:03:35,280
the weights
A the auxiliary information that is necessary

78
00:03:35,280 --> 00:03:36,280
for training

79
00:03:36,280 --> 00:03:40,420
Note that this is enough for producing (and
verifying) a proof for a model that is trained

80
00:03:40,420 --> 00:03:46,048
on a publicly available dataset. If the training
data is private, however, we require that

81
00:03:46,049 --> 00:03:51,060
the prover provides signatures of the datapoint
used as well.

82
00:03:51,060 --> 00:03:54,620
So we saw that proof creation is equivalent
to recording the chain of updates. However,

83
00:03:54,620 --> 00:03:58,930
if all the links are recorded in the proof,
the size of the proof would become prohibitively

84
00:03:58,930 --> 00:04:05,860
large. So instead the prover can record the
weights only every k steps and leave them

85
00:04:05,860 --> 00:04:11,079
out everywhere else. So P0 is recorded, then
the weightless proof p_tilde_1, p_tilde_2,

86
00:04:11,079 --> 00:04:17,820
until then Pk where the full proof is again
recorded. From here on we simplify our diagrams

87
00:04:17,820 --> 00:04:23,260
by replacing all P_tildas with this arrow.
Finally, this is done for every epoch of the

88
00:04:23,260 --> 00:04:24,260
training.

89
00:04:24,260 --> 00:04:29,849
Hi I’m Hengrui, let me start to discuss
the proposed verification mechanism of the

90
00:04:29,850 --> 00:04:33,430
proof of learnings.
First let’s focus on the initial state of

91
00:04:33,430 --> 00:04:38,760
the proof, P0. This state should contain the
initialization parameters of the trained model.

92
00:04:38,760 --> 00:04:42,500
But what if the adversary uses a stolen model
as the initial state?

93
00:04:42,500 --> 00:04:46,500
We know it is almost impossible for a just
initialized model to have great performance.

94
00:04:46,500 --> 00:04:47,940
But we need to prove it

95
00:04:47,940 --> 00:04:53,420
The tool we use here is a statistical test
named KS test, which tests if a set of values

96
00:04:53,420 --> 00:04:58,050
is sampled from a certain distribution.
How effective is it? We plot the p-value of

97
00:04:58,050 --> 00:05:02,170
KS-test with respect to number of training
steps, on the parameters of a ResNet-20 model

98
00:05:02,170 --> 00:05:06,110
trained on CIFAR-10. Here the average and
min are taken across different layers of the

99
00:05:06,110 --> 00:05:11,020
network. In other words, the p-value represents
how likely a certain layer is newly initialized.

100
00:05:11,020 --> 00:05:15,330
Let’s zoom in a little bit.
As shown in the lower-left corner, with less

101
00:05:15,330 --> 00:05:20,510
than 10 gradient steps, the p-value goes under
0.01. The validation accuracy of the model

102
00:05:20,510 --> 00:05:25,599
at this moment is only slightly above 10%,
so we can expect an even lower p-value if

103
00:05:25,600 --> 00:05:30,390
an adversary uses a stolen model as the initial
state. This method can effectively prevent

104
00:05:30,390 --> 00:05:32,719
the adversary using an arbitrary initialization

105
00:05:32,720 --> 00:05:35,920
Now the initial state is verified, we will
continue to verify the other states.

106
00:05:35,920 --> 00:05:40,750
We used what we called stepwise verification
for this part. In high level, we verify the

107
00:05:40,750 --> 00:05:45,160
proof by reproducing the training and see
if we get the same final model. I will describe

108
00:05:45,160 --> 00:05:48,720
the details, and motivate why it is correct,
and also necessary

109
00:05:48,720 --> 00:05:53,460
Gradient descent is Markov. As shown in the
equation, to perform a gradient descent, only

110
00:05:53,460 --> 00:05:58,780
the previous set of model parameters is needed.
Most operations from frequently used machine

111
00:05:58,780 --> 00:06:02,849
learning libraries, such as Tensorflow and
Pytorch, are non-deterministic. So there is

112
00:06:02,850 --> 00:06:08,020
some randomness at each step of gradient descent,
denoted by z in the equation. And this randomness

113
00:06:08,020 --> 00:06:12,799
is iid distributed during training.
Since entropy is a measure of randomness,

114
00:06:12,800 --> 00:06:16,710
we can say that the total entropy increases
linearly with respect to the number of training

115
00:06:16,710 --> 00:06:22,669
steps, and the entropy of every gradient descent
is H(z0). For detailed derivation, please

116
00:06:22,670 --> 00:06:24,590
see the paper.

117
00:06:24,590 --> 00:06:28,099
If we trained a model, and try to reproduce
the training process to verify the proof

118
00:06:28,100 --> 00:06:32,660
By the naive approach, where we start from
the same initial state and use the data in

119
00:06:32,660 --> 00:06:37,620
the same order, it is still almost impossible
to reproduce the same sequence of model parameters

120
00:06:37,620 --> 00:06:40,100
because the entropy cumulates and is unbounded

121
00:06:40,100 --> 00:06:45,590
Therefore, we introduce step-wise verification
for every step, we check if the reproduced

122
00:06:45,590 --> 00:06:49,820
model is closed to the corresponding model
state in the proof. If so, we load the model

123
00:06:49,820 --> 00:06:55,550
state from the proof, and try to reproduce
the next step based on this state, and so

124
00:06:55,550 --> 00:06:58,960
on. By doing so, we only need to deal with
the entropy of one single step.

125
00:06:58,960 --> 00:07:05,020
Now let’s empirically verify this. In this
table, the values are distances between 2

126
00:07:05,020 --> 00:07:09,210
sets of model parameters. They are normalized
to the range between 0 and 1 by the distance

127
00:07:09,210 --> 00:07:13,969
between two completely unrelated models. We
use k to represent the checkpointing interval,

128
00:07:13,970 --> 00:07:17,930
which is defined to be how frequently we store
the model parameters.

129
00:07:17,930 --> 00:07:21,760
The leftmost column is what would happen if
we take the naive approach. As you can see,

130
00:07:21,760 --> 00:07:26,880
due to the cumulated entropy, the resulted
model has near 1 distance to the original

131
00:07:26,880 --> 00:07:31,780
model. In other words, it is impossible to
verify such a proof.

132
00:07:31,780 --> 00:07:36,299
In the middle column is the results of step-wise
verification, and we set k = 1. Since the

133
00:07:36,300 --> 00:07:42,130
randomness is only from 1 step, the distance
is close to 0, meaning verification is straightforward.

134
00:07:42,130 --> 00:07:45,500
Detailed impact of values of k is discussed
in the paper.

135
00:07:45,500 --> 00:07:51,030
You may wonder what if we use deterministic
operations -- however that only restricts

136
00:07:51,030 --> 00:07:56,559
the randomness coming from the backend software
like CUDNN, other noise sources such as hardware

137
00:07:56,560 --> 00:08:01,650
are hard to control. The average distance
is around 0.5. This is better than the naive

138
00:08:01,650 --> 00:08:05,770
approach, but still incurs some difficulties
at verification.

139
00:08:05,770 --> 00:08:10,120
We’ve demonstrated the correctness of step-wise
verification, but it has a drawback: verifying

140
00:08:10,120 --> 00:08:13,640
the complete proof can be as expensive as
re-training the model.

141
00:08:13,640 --> 00:08:19,080
Our intuition is that having large valid updates
would be akin to having a large learning rate

142
00:08:19,080 --> 00:08:24,169
in gradient descent, which is likely to cause
oscillation and other instabilities in training.

143
00:08:24,170 --> 00:08:29,530
We, therefore, hypothesize that valid updates
are typically small in magnitude and that

144
00:08:29,530 --> 00:08:33,309
we can save computational cost by only verifying
the largest updates

145
00:08:33,309 --> 00:08:36,199
But let’s see this process in more detail

146
00:08:36,200 --> 00:08:42,000
Here we show the proof sequence for the first
epoch that runs from P0 to P5k. We use the

147
00:08:42,000 --> 00:08:47,480
length of the arrows to demonstrate the distance
between initial and final states.

148
00:08:47,480 --> 00:08:53,390
We sort all the updates by their distances
and select the Q-largest updates to verify

149
00:08:53,390 --> 00:08:55,030
and discard the rest.

150
00:08:55,030 --> 00:09:01,310
We’ll talk about what the adversary might
do in response to the proposed mechanism

151
00:09:01,310 --> 00:09:03,650
The prover has a legitimate PoL that could
pass the verification

152
00:09:03,650 --> 00:09:07,560
The threat is that an adversary wants to pass
verification

153
00:09:07,560 --> 00:09:12,109
by creating a proof that requires less computational
power than training the model, we call this

154
00:09:12,110 --> 00:09:15,110
spoofing.
It is possible because only part of the PoL

155
00:09:15,110 --> 00:09:20,120
is verified, and the adversary may have access
to some stolen model parameters -- this is

156
00:09:20,120 --> 00:09:23,370
more information than when the prover started
to train the model

157
00:09:23,370 --> 00:09:27,630
The spoofing could be either honest or dishonest.
In the honest case, the adversary tries to

158
00:09:27,630 --> 00:09:32,850
create a fully valid PoL, where every single
step could pass the verification.

159
00:09:32,850 --> 00:09:36,720
However, such methods usually lead to computational
costs that are equally or even more expensive

160
00:09:36,720 --> 00:09:39,610
than training the model, which goes against
the adversary’s willingness.

161
00:09:39,610 --> 00:09:44,970
To evade this cost, the adversary may want
to be dishonest and create a partially invalid

162
00:09:44,970 --> 00:09:49,420
PoL. This means the adversary tries to take
advantage that not all steps are verified

163
00:09:49,420 --> 00:09:52,360
and hides the invalid parts of the PoL hoping
they won’t be noticed.

164
00:09:52,360 --> 00:09:57,500
Next, I’m going to give an example of a
dishonest spoofing strategy, we assume the

165
00:09:57,500 --> 00:10:01,840
adversary is aware that not the entire proof
is verified.

166
00:10:01,840 --> 00:10:06,730
To bypass initialization verification, the
adversary wants to train from initialization.

167
00:10:06,730 --> 00:10:11,250
But this is only for a small number of steps
since the adversary wants to save the computational

168
00:10:11,250 --> 00:10:16,090
cost. This results in a valid proof,
then the adversary append the stolen model

169
00:10:16,090 --> 00:10:20,430
to the end and hope this single invalid step
in the proof won’t be noticed.

170
00:10:20,430 --> 00:10:25,329
This won’t work because we would verify
the top-Q largest steps, and the concatenated

171
00:10:25,330 --> 00:10:29,470
step is likely to be the largest since the
stolen model does not have any connection

172
00:10:29,470 --> 00:10:34,550
to the model parameters in the valid proof
As shown in these two figures plotted for

173
00:10:34,550 --> 00:10:39,050
CIFAR-10 and CIFAR-100 respectively, no matter
how long the adversary has trained the model

174
00:10:39,050 --> 00:10:44,819
before concatenation, as shown on the x-axis,
the gap at the concatenation, which is plotted

175
00:10:44,820 --> 00:10:51,110
at the y-axis, is almost always above 1. This
means that the distance between the last model

176
00:10:51,110 --> 00:10:56,500
in the valid proof and the stolen model is
similar to the distance between any two unrelated

177
00:10:56,500 --> 00:10:57,500
models.

178
00:10:57,500 --> 00:11:00,720
A natural question when looking at the iterative
nature of a proof is whether it might be possible

179
00:11:00,720 --> 00:11:04,320
to trace back the steps of a proof, obtaining
an honest spoof and whether that might give

180
00:11:04,320 --> 00:11:07,740
the adversary some sort of advantage
But how would someone do this

181
00:11:07,740 --> 00:11:11,990
Well, looking at the formula for SGD and rearranging
the terms, we see we could actually solve

182
00:11:11,990 --> 00:11:16,240
for the previous weights given the current
weights at a particular step by root solving

183
00:11:16,240 --> 00:11:20,120
this new equation, and so we can iteratively
trace back a possible training sequence

184
00:11:20,120 --> 00:11:22,560
But, as we’ll explain, this isn’t an effective
spoof

185
00:11:22,560 --> 00:11:28,290
We can understand why this won’t work theoretically
in terms of our entropy growth analysis; for

186
00:11:28,290 --> 00:11:31,362
the regular training update rule, we see all
the terms are deterministic except for the

187
00:11:31,362 --> 00:11:35,530
hardware randomness, which is, in fact, independent
of the training step, so our entropy growth

188
00:11:35,530 --> 00:11:39,069
is constant over training
However, with our inverse gradient update

189
00:11:39,070 --> 00:11:41,540
equation, we get a slightly different equation
for entropy growth

190
00:11:41,540 --> 00:11:45,240
In particular, we still have the same hardware
randomness term from training, and so we see

191
00:11:45,240 --> 00:11:47,320
our entropy will grow at least as much as
when training

192
00:11:47,320 --> 00:11:51,040
But, as there could be several solutions to
our root solving problem, that is there could

193
00:11:51,040 --> 00:11:54,980
be more than one path leading to the current
weights, each of which our root solving may

194
00:11:54,980 --> 00:11:59,230
return with non-zero probability, we have
an extra term capturing this new random variable

195
00:11:59,230 --> 00:12:04,130
This is all to say that with the inverse gradient
method we expect to face at least as much

196
00:12:04,130 --> 00:12:08,380
randomness as training, which was already
quite random, and as inverse training depends

197
00:12:08,380 --> 00:12:12,410
on root solving which is very sensitive to
initializations, we expect to have large verifications

198
00:12:12,410 --> 00:12:13,900
errors that won’t pass verification

199
00:12:13,900 --> 00:12:18,760
We see this experimentally, recall that our
verification error when creating an honest

200
00:12:18,760 --> 00:12:23,180
proof was more or less 0 across the board
However, with the inverse gradient method,

201
00:12:23,180 --> 00:12:27,040
we see a significant jump in these errors,
at least 10 times in each metric, which is

202
00:12:27,040 --> 00:12:29,240
enough that they wouldn’t pass any reasonable
verification

203
00:12:29,240 --> 00:12:34,240
So our experiments support our previous analysis
that the inverse gradient method isn’t a

204
00:12:34,240 --> 00:12:35,900
viable spoof

205
00:12:35,900 --> 00:12:39,709
Now we’ll go over the overhead involved
in our verification procedure. Here, we want

206
00:12:39,710 --> 00:12:43,860
a proof that is quickly verifiable in most
cases and requires somewhat small storage.

207
00:12:43,860 --> 00:12:47,370
We’ll start with the computational overhead
involved in verifying a proof of learning

208
00:12:47,370 --> 00:12:52,250
from the prover. First, we will go over some
terms involved in the final cost. Recall from

209
00:12:52,250 --> 00:12:56,110
before that we use k to represent the number
of training steps between recording the weights

210
00:12:56,110 --> 00:13:00,529
in the proof. We will use C to denote the
computational cost of one forward / backward

211
00:13:00,529 --> 00:13:05,180
pass for backpropagation, and Q is the number
of checks or the number of largest updates

212
00:13:05,180 --> 00:13:06,180
we need to verify.

213
00:13:06,180 --> 00:13:07,180
Since we perform k updates at cost C and we
have to complete Q of these verifications,

214
00:13:07,180 --> 00:13:10,770
we have the verification cost is polynomial
in C, Q and k.

215
00:13:10,770 --> 00:13:16,220
We now explore the storage overhead. The storage
overhead involves k again, and we denote size(P)

216
00:13:16,220 --> 00:13:21,590
the storage cost of the proof for a single
step, E the number of epochs and S the number

217
00:13:21,590 --> 00:13:26,180
of mini-batches or steps per epoch. Then we
have that the verification storage cost is

218
00:13:26,180 --> 00:13:32,459
O of size(P) E S over k where size(P) is the
storage for one loop and E S over k is the

219
00:13:32,460 --> 00:13:34,690
number of loops in verification.

220
00:13:34,690 --> 00:13:39,600
Finally, we discuss some limitations of our
proposed proof procedure, and future directions

221
00:13:39,600 --> 00:13:44,350
to explore. We showed in the previous two
slides the storage and computational costs

222
00:13:44,350 --> 00:13:48,730
of the verification procedure. Currently,
the proposed verification method takes advantage

223
00:13:48,730 --> 00:13:53,420
of frequently stored checkpoints to verify
the proof, but this leads to more cost on

224
00:13:53,420 --> 00:13:59,120
storage as size(P) will include at the least
the storage of the full model weights. Additionally,

225
00:13:59,120 --> 00:14:03,150
the verification is done by partially retraining
the model, which in the worst case, when k

226
00:14:03,150 --> 00:14:09,029
is equal to 1, leads to verification being
as costly as learning. This means it is important

227
00:14:09,029 --> 00:14:13,480
to keep values of k as large as possible,
but choosing large values of k is currently

228
00:14:13,480 --> 00:14:18,130
limited by the fact that increasing k also
increases the verification error due to randomness

229
00:14:18,130 --> 00:14:23,529
in training DNNs. This is the primary source
of error in correct verification, so identification

230
00:14:23,529 --> 00:14:27,490
and control of noise would be a good next
step to boosting the probability of success.

231
00:14:27,490 --> 00:14:31,839
Overall, reducing the verification cost and
controlling noise would allow for support

232
00:14:31,839 --> 00:14:37,970
of longer or chained proofs, and greater applicability.
Finally, we discussed model extraction and

233
00:14:37,970 --> 00:14:43,150
faulty workers in the delegation of computation
scenarios as potential threats that we address.

234
00:14:43,150 --> 00:14:47,089
Since the concept of proof of learning in
deep learning is very new, it is worth investigating

235
00:14:47,089 --> 00:14:50,810
other spoofing strategies that are more efficient
and exploring different scenarios in which

236
00:14:50,810 --> 00:14:52,640
the proposed method could be used.

237
00:14:52,640 --> 00:14:56,839
Thank you for listening to our presentation
on Proof of Learning, and please let us know

238
00:14:56,839 --> 00:14:57,720
if you have any questions!

