1
00:00:00,640 --> 00:00:02,879
hello thank you so much for joining this

2
00:00:02,879 --> 00:00:03,439
talk

3
00:00:03,439 --> 00:00:05,520
my name is sahar and i'm going to

4
00:00:05,520 --> 00:00:06,640
present our paper

5
00:00:06,640 --> 00:00:08,720
adversarial watermarking transformer

6
00:00:08,720 --> 00:00:10,480
towards tracing text provenance with

7
00:00:10,480 --> 00:00:11,840
data hiding

8
00:00:11,840 --> 00:00:13,679
this work is done in collaboration with

9
00:00:13,679 --> 00:00:15,759
mario fritz

10
00:00:15,759 --> 00:00:18,400
language models such as gpg2 have shown

11
00:00:18,400 --> 00:00:20,160
a great performance with high quality

12
00:00:20,160 --> 00:00:21,359
output text

13
00:00:21,359 --> 00:00:23,199
they were able to generate entire

14
00:00:23,199 --> 00:00:25,199
articles that seemed very convincing for

15
00:00:25,199 --> 00:00:27,119
human evaluators

16
00:00:27,119 --> 00:00:29,199
an even more powerful and larger model

17
00:00:29,199 --> 00:00:31,279
was later released

18
00:00:31,279 --> 00:00:33,280
this raised significant concerns about

19
00:00:33,280 --> 00:00:34,320
the implication

20
00:00:34,320 --> 00:00:37,200
and potential abuse of such models as a

21
00:00:37,200 --> 00:00:38,239
counter measure

22
00:00:38,239 --> 00:00:40,000
these models were released in a state

23
00:00:40,000 --> 00:00:42,160
release or only made available through a

24
00:00:42,160 --> 00:00:45,120
commercial black box api

25
00:00:45,120 --> 00:00:47,600
active and protective release strategies

26
00:00:47,600 --> 00:00:50,079
are still lacking

27
00:00:50,079 --> 00:00:51,920
current passive defenses such as

28
00:00:51,920 --> 00:00:53,120
classifiers

29
00:00:53,120 --> 00:00:55,440
depend on the generation configuration

30
00:00:55,440 --> 00:00:56,640
and the language model

31
00:00:56,640 --> 00:00:59,199
which is not sustainable in the long run

32
00:00:59,199 --> 00:00:59,920
therefore

33
00:00:59,920 --> 00:01:01,920
we study language bottom working as an

34
00:01:01,920 --> 00:01:04,000
active and sustainable solution for

35
00:01:04,000 --> 00:01:06,799
generational models abuse our envisioned

36
00:01:06,799 --> 00:01:08,799
scenario is as follows

37
00:01:08,799 --> 00:01:11,040
the owner of a blackbox text generation

38
00:01:11,040 --> 00:01:13,360
api watermarks the language model's

39
00:01:13,360 --> 00:01:14,479
output

40
00:01:14,479 --> 00:01:17,119
then an adversary uses the watermarked

41
00:01:17,119 --> 00:01:19,360
api to generate malicious content at

42
00:01:19,360 --> 00:01:20,080
scale

43
00:01:20,080 --> 00:01:23,280
such as fake news or misinformation

44
00:01:23,280 --> 00:01:25,280
news platforms can now cooperate with

45
00:01:25,280 --> 00:01:27,119
owners to decode the watermark

46
00:01:27,119 --> 00:01:29,759
and detect machine generated text this

47
00:01:29,759 --> 00:01:30,159
way

48
00:01:30,159 --> 00:01:32,079
enables the responsible release and the

49
00:01:32,079 --> 00:01:34,400
potential regulation of ai technologies

50
00:01:34,400 --> 00:01:36,720
and generation models

51
00:01:36,720 --> 00:01:38,640
towards that goal we propose the

52
00:01:38,640 --> 00:01:40,720
adversary with a marking transformer

53
00:01:40,720 --> 00:01:44,000
or awt in short our model is the first

54
00:01:44,000 --> 00:01:45,360
end-to-end method for language

55
00:01:45,360 --> 00:01:46,399
watermarking

56
00:01:46,399 --> 00:01:48,560
it consists of a data hiding network to

57
00:01:48,560 --> 00:01:50,240
generate the watermark text

58
00:01:50,240 --> 00:01:52,159
and a data revealing network to decoder

59
00:01:52,159 --> 00:01:54,880
message we utilize adversarial training

60
00:01:54,880 --> 00:01:56,719
to achieve stealthiness and subtle

61
00:01:56,719 --> 00:01:59,040
encoding of the information

62
00:01:59,040 --> 00:02:01,439
all these components are jointly trained

63
00:02:01,439 --> 00:02:03,600
we don't use any pair training data

64
00:02:03,600 --> 00:02:05,280
and the model learns to encode the

65
00:02:05,280 --> 00:02:08,239
information automatically

66
00:02:08,239 --> 00:02:10,639
we define three evaluation axes for the

67
00:02:10,639 --> 00:02:12,400
language watermarking task

68
00:02:12,400 --> 00:02:15,120
the first one is effectiveness this is

69
00:02:15,120 --> 00:02:16,800
concerned with successful message

70
00:02:16,800 --> 00:02:18,400
encoding and decoding

71
00:02:18,400 --> 00:02:20,560
at the same time it measures text

72
00:02:20,560 --> 00:02:22,480
preserving and introducing minimal

73
00:02:22,480 --> 00:02:24,480
perturbations to the text correctness

74
00:02:24,480 --> 00:02:26,000
and semantics

75
00:02:26,000 --> 00:02:28,480
the second one is secrecy ideally

76
00:02:28,480 --> 00:02:29,760
watermark text should be

77
00:02:29,760 --> 00:02:31,599
indistinguishable from non-watermark

78
00:02:31,599 --> 00:02:32,560
text

79
00:02:32,560 --> 00:02:34,560
this is a proxy for the naturalness of

80
00:02:34,560 --> 00:02:36,080
watermarking and it also

81
00:02:36,080 --> 00:02:38,080
helps to avoid detection and removal

82
00:02:38,080 --> 00:02:39,519
attempts

83
00:02:39,519 --> 00:02:41,840
the last one is robustness this means

84
00:02:41,840 --> 00:02:43,440
that the watermark should be highly

85
00:02:43,440 --> 00:02:46,160
resilient against removal atoms

86
00:02:46,160 --> 00:02:48,319
these axes could be competing and the

87
00:02:48,319 --> 00:02:49,920
watermarking scheme should achieve a

88
00:02:49,920 --> 00:02:51,280
good trade-off between them

89
00:02:51,280 --> 00:02:55,360
which we empirically show in our paper

90
00:02:55,760 --> 00:02:57,840
unlike deep learning methods and images

91
00:02:57,840 --> 00:02:59,760
previous work and language watermarking

92
00:02:59,760 --> 00:03:01,920
mainly relied on rule-based methods

93
00:03:01,920 --> 00:03:04,239
such as synonym substitution or

94
00:03:04,239 --> 00:03:05,760
structural changes

95
00:03:05,760 --> 00:03:08,000
however these approaches might change

96
00:03:08,000 --> 00:03:10,319
the input sentence to a large extent

97
00:03:10,319 --> 00:03:13,040
also they use fixed changes which

98
00:03:13,040 --> 00:03:13,920
results in

99
00:03:13,920 --> 00:03:16,159
limited flexibility and can also

100
00:03:16,159 --> 00:03:17,440
compromise secrecy

101
00:03:17,440 --> 00:03:20,640
and robustness our work addresses these

102
00:03:20,640 --> 00:03:23,279
limitations

103
00:03:23,360 --> 00:03:25,200
now we will present more details about

104
00:03:25,200 --> 00:03:28,480
our method awt consists of a data hiding

105
00:03:28,480 --> 00:03:29,200
network

106
00:03:29,200 --> 00:03:31,760
a data revealing network a discriminator

107
00:03:31,760 --> 00:03:33,840
and other complementary losses

108
00:03:33,840 --> 00:03:35,680
the background models are transformer

109
00:03:35,680 --> 00:03:37,840
encoders or decoders

110
00:03:37,840 --> 00:03:39,599
we start with the data hijack network

111
00:03:39,599 --> 00:03:41,280
which is a sequence to sequence model

112
00:03:41,280 --> 00:03:44,799
consisting of an encoder and a decoder

113
00:03:44,799 --> 00:03:47,200
the encoder takes the input text and the

114
00:03:47,200 --> 00:03:48,319
input binary message

115
00:03:48,319 --> 00:03:50,640
and produces a shared embedding of them

116
00:03:50,640 --> 00:03:52,879
which will be passed to the decoder

117
00:03:52,879 --> 00:03:55,120
the decoder generates the watermark

118
00:03:55,120 --> 00:03:56,319
sequence

119
00:03:56,319 --> 00:03:58,319
since we don't use any pair training

120
00:03:58,319 --> 00:04:00,560
data the model is trained as an auto

121
00:04:00,560 --> 00:04:02,239
encoder to reconstruct

122
00:04:02,239 --> 00:04:05,439
the inputs and the input text

123
00:04:05,439 --> 00:04:07,439
we jointly train a message decoder that

124
00:04:07,439 --> 00:04:09,519
takes the one hot encoded watermark

125
00:04:09,519 --> 00:04:10,400
sequence

126
00:04:10,400 --> 00:04:13,680
and reconstructs the message the message

127
00:04:13,680 --> 00:04:15,760
reconstruction loss is the binary cross

128
00:04:15,760 --> 00:04:18,238
entropy over all bits

129
00:04:18,238 --> 00:04:20,079
balancing these losses achieves a

130
00:04:20,079 --> 00:04:21,918
trade-off between text preserving

131
00:04:21,918 --> 00:04:25,198
and message reconstruction

132
00:04:25,600 --> 00:04:27,680
encoding the information can be done in

133
00:04:27,680 --> 00:04:30,000
an obvious way that compromises secrecy

134
00:04:30,000 --> 00:04:31,199
and robustness

135
00:04:31,199 --> 00:04:33,680
to avoid that we utilize adversarial

136
00:04:33,680 --> 00:04:35,600
training and train the data hiding and

137
00:04:35,600 --> 00:04:37,120
revealing networks against a

138
00:04:37,120 --> 00:04:38,560
discriminator

139
00:04:38,560 --> 00:04:40,160
it takes the watermarked and

140
00:04:40,160 --> 00:04:42,320
non-watermark sentences and should

141
00:04:42,320 --> 00:04:45,280
classify between them

142
00:04:45,280 --> 00:04:48,160
finally to achieve better output quality

143
00:04:48,160 --> 00:04:49,759
we use additional losses as a

144
00:04:49,759 --> 00:04:51,520
fine-tuning step

145
00:04:51,520 --> 00:04:53,280
we maximize the likelihood of the

146
00:04:53,280 --> 00:04:55,120
generated sequence under a pre-trained

147
00:04:55,120 --> 00:04:56,400
language model

148
00:04:56,400 --> 00:04:58,320
this step helps to maximize the

149
00:04:58,320 --> 00:05:00,800
syntactical correctness

150
00:05:00,800 --> 00:05:02,960
we also utilize a pre-trained sentence

151
00:05:02,960 --> 00:05:04,240
and wearing model

152
00:05:04,240 --> 00:05:06,639
we we minimize the distance between the

153
00:05:06,639 --> 00:05:09,520
input and output sentences embeddings

154
00:05:09,520 --> 00:05:11,680
this step helps to achieve more semantic

155
00:05:11,680 --> 00:05:14,400
consistency

156
00:05:14,800 --> 00:05:16,560
we now move to our experimental

157
00:05:16,560 --> 00:05:18,639
evaluation and we start with describing

158
00:05:18,639 --> 00:05:19,840
our setup

159
00:05:19,840 --> 00:05:22,240
we use the word level wiki text 2 that's

160
00:05:22,240 --> 00:05:24,240
curated from wikipedia articles with

161
00:05:24,240 --> 00:05:25,600
live processing

162
00:05:25,600 --> 00:05:27,520
to evaluate the output we use three

163
00:05:27,520 --> 00:05:29,600
metrics the first one is the meteor

164
00:05:29,600 --> 00:05:31,440
score to find the overlap between the

165
00:05:31,440 --> 00:05:33,120
input and output text

166
00:05:33,120 --> 00:05:35,840
it ranges from 0 to 1 indicating no to

167
00:05:35,840 --> 00:05:37,680
perfect similarity

168
00:05:37,680 --> 00:05:39,440
meteor is usually used in machine

169
00:05:39,440 --> 00:05:41,360
translation tasks to evaluate against

170
00:05:41,360 --> 00:05:43,280
the ground tool sentence

171
00:05:43,280 --> 00:05:45,280
however we found that meteor is not

172
00:05:45,280 --> 00:05:47,280
enough for semantic evaluation

173
00:05:47,280 --> 00:05:49,280
therefore we use the pre-trained

174
00:05:49,280 --> 00:05:51,120
sentence word model to evaluate the

175
00:05:51,120 --> 00:05:52,960
semantic similarity

176
00:05:52,960 --> 00:05:55,039
we compute the l2 distance between the

177
00:05:55,039 --> 00:05:56,800
embeddings of the input and output

178
00:05:56,800 --> 00:05:57,600
sentences

179
00:05:57,600 --> 00:06:00,639
where lower distance is better

180
00:06:00,639 --> 00:06:02,720
these two metrics measure the text

181
00:06:02,720 --> 00:06:03,680
utility or

182
00:06:03,680 --> 00:06:06,880
quality finally we evaluate the width

183
00:06:06,880 --> 00:06:09,199
accuracy of decoded messages over all

184
00:06:09,199 --> 00:06:12,639
sentences in the test set

185
00:06:12,639 --> 00:06:14,720
we use a message length of 4 bits

186
00:06:14,720 --> 00:06:16,880
sampled randomly during training

187
00:06:16,880 --> 00:06:19,280
we encode a message per sentence or a

188
00:06:19,280 --> 00:06:20,639
text segment

189
00:06:20,639 --> 00:06:22,560
our main scope is an article level

190
00:06:22,560 --> 00:06:24,639
watermarking to enable that

191
00:06:24,639 --> 00:06:26,800
sentences can be successively encoded

192
00:06:26,800 --> 00:06:28,319
with a sequence of messages

193
00:06:28,319 --> 00:06:31,199
to form a long watermark messages can be

194
00:06:31,199 --> 00:06:32,160
decoded from

195
00:06:32,160 --> 00:06:35,039
each segment independently to verify the

196
00:06:35,039 --> 00:06:35,919
watermark

197
00:06:35,919 --> 00:06:37,919
we can find the number of matching bits

198
00:06:37,919 --> 00:06:39,039
against the original

199
00:06:39,039 --> 00:06:41,840
long watermark sequence to allow soft

200
00:06:41,840 --> 00:06:42,479
matching

201
00:06:42,479 --> 00:06:44,800
we use hypothesis testing based on the

202
00:06:44,800 --> 00:06:46,080
number of matched bits

203
00:06:46,080 --> 00:06:48,080
where the null hypothesis is the chance

204
00:06:48,080 --> 00:06:50,160
level matching or the no water marking

205
00:06:50,160 --> 00:06:52,479
case

206
00:06:52,560 --> 00:06:54,800
we now examine the different evaluation

207
00:06:54,800 --> 00:06:57,520
axes and we start with the effectiveness

208
00:06:57,520 --> 00:06:59,360
we first evaluate the effect of each

209
00:06:59,360 --> 00:07:01,440
component in the model

210
00:07:01,440 --> 00:07:03,599
adversarial training helps to preserve

211
00:07:03,599 --> 00:07:05,599
the semantics which is indicated by a

212
00:07:05,599 --> 00:07:07,280
lower expert distance

213
00:07:07,280 --> 00:07:09,919
additionally fine tuning with auxiliary

214
00:07:09,919 --> 00:07:10,479
losses

215
00:07:10,479 --> 00:07:12,960
helps to further improve the utility or

216
00:07:12,960 --> 00:07:14,560
the semantics

217
00:07:14,560 --> 00:07:17,120
for all variants met accuracy is very

218
00:07:17,120 --> 00:07:20,160
high indicating successful decoding

219
00:07:20,160 --> 00:07:22,000
we then study further techniques that

220
00:07:22,000 --> 00:07:24,160
could improve the utility or the output

221
00:07:24,160 --> 00:07:25,280
quality

222
00:07:25,280 --> 00:07:27,759
the first one is sampling we sample

223
00:07:27,759 --> 00:07:30,080
different sentences for each input

224
00:07:30,080 --> 00:07:32,240
then we select the best sample based on

225
00:07:32,240 --> 00:07:34,479
a quality metric such as language model

226
00:07:34,479 --> 00:07:37,919
likelihood or expert distance

227
00:07:37,919 --> 00:07:40,479
the second one is selective encoding if

228
00:07:40,479 --> 00:07:42,639
the perturbation done by watermarking is

229
00:07:42,639 --> 00:07:44,000
higher than a threshold

230
00:07:44,000 --> 00:07:46,720
the sentence can be left non-watermarked

231
00:07:46,720 --> 00:07:48,479
this will result in a random chance

232
00:07:48,479 --> 00:07:50,720
matching for this sentence

233
00:07:50,720 --> 00:07:52,639
the decoder side can attempt to decode

234
00:07:52,639 --> 00:07:55,039
the watermark from all sentences without

235
00:07:55,039 --> 00:07:57,599
knowing which ones are watermarked

236
00:07:57,599 --> 00:07:59,520
the decision is then based on the total

237
00:07:59,520 --> 00:08:01,199
matching accuracy

238
00:08:01,199 --> 00:08:03,120
by changing the threshold or the number

239
00:08:03,120 --> 00:08:05,440
of samples we can increase the text

240
00:08:05,440 --> 00:08:07,599
utility which is indicated by higher

241
00:08:07,599 --> 00:08:08,639
meteor scores

242
00:08:08,639 --> 00:08:11,680
and lower expert distances however this

243
00:08:11,680 --> 00:08:12,000
has

244
00:08:12,000 --> 00:08:13,680
a trade-off relationship with with

245
00:08:13,680 --> 00:08:16,080
accuracy this can result in different

246
00:08:16,080 --> 00:08:18,240
operating points of utility versus

247
00:08:18,240 --> 00:08:20,800
accuracy

248
00:08:20,960 --> 00:08:23,280
as previously mentioned the verification

249
00:08:23,280 --> 00:08:25,440
is done on an article level input by

250
00:08:25,440 --> 00:08:27,440
null hypothesis testing

251
00:08:27,440 --> 00:08:30,000
the watermark is detected or verified if

252
00:08:30,000 --> 00:08:30,960
the p-value

253
00:08:30,960 --> 00:08:34,159
is less than a threshold we evaluate the

254
00:08:34,159 --> 00:08:36,240
water marking detection rate and its

255
00:08:36,240 --> 00:08:37,519
relationship with

256
00:08:37,519 --> 00:08:39,919
with text length we perform this

257
00:08:39,919 --> 00:08:42,080
analysis for different operating points

258
00:08:42,080 --> 00:08:43,120
of accuracy

259
00:08:43,120 --> 00:08:46,320
and at two significance levels

260
00:08:46,320 --> 00:08:48,720
increasing the text length increases the

261
00:08:48,720 --> 00:08:50,320
water mark detection rate

262
00:08:50,320 --> 00:08:52,839
even when using lower accuracy operating

263
00:08:52,839 --> 00:08:56,080
points we also validate that real text

264
00:08:56,080 --> 00:08:58,560
or non-water mark text has random chance

265
00:08:58,560 --> 00:08:59,360
matching

266
00:08:59,360 --> 00:09:01,440
which results in a high p value and low

267
00:09:01,440 --> 00:09:03,360
false positive rates

268
00:09:03,360 --> 00:09:04,880
please refer to the paper for other

269
00:09:04,880 --> 00:09:08,080
decoding strategies

270
00:09:08,080 --> 00:09:10,080
we now show some qualitative examples of

271
00:09:10,080 --> 00:09:11,760
different models variants

272
00:09:11,760 --> 00:09:13,600
we first show the node discriminator

273
00:09:13,600 --> 00:09:16,160
model this variant performs systematic

274
00:09:16,160 --> 00:09:17,440
fixed changes

275
00:09:17,440 --> 00:09:20,399
and it uses less likely tokens this

276
00:09:20,399 --> 00:09:22,560
affects the secrecy and robustness

277
00:09:22,560 --> 00:09:25,360
and it also affects the text semantics

278
00:09:25,360 --> 00:09:27,440
adding a discriminator but without fine

279
00:09:27,440 --> 00:09:28,240
tuning

280
00:09:28,240 --> 00:09:31,279
helps to have a better output however it

281
00:09:31,279 --> 00:09:33,279
still had many mistakes

282
00:09:33,279 --> 00:09:35,600
we finally show examples of watermark

283
00:09:35,600 --> 00:09:37,839
sentences using the full model

284
00:09:37,839 --> 00:09:40,000
we observe that the changes are all from

285
00:09:40,000 --> 00:09:42,560
common tokens without fixed changes

286
00:09:42,560 --> 00:09:44,640
the correctness and semantic consistency

287
00:09:44,640 --> 00:09:46,959
are also greatly improved

288
00:09:46,959 --> 00:09:48,959
we still observed cases where the model

289
00:09:48,959 --> 00:09:50,959
made mistakes indicating that there is a

290
00:09:50,959 --> 00:09:54,319
room for further improvement

291
00:09:54,640 --> 00:09:56,399
we now evaluate the secrecy of the

292
00:09:56,399 --> 00:09:58,800
watermarks we first evaluate and

293
00:09:58,800 --> 00:10:00,800
visualize the word replacement patterns

294
00:10:00,800 --> 00:10:02,880
done by the model at scale

295
00:10:02,880 --> 00:10:05,040
we here show the pairwise transitions

296
00:10:05,040 --> 00:10:07,360
between the most commonly used words by

297
00:10:07,360 --> 00:10:08,720
the model

298
00:10:08,720 --> 00:10:10,720
since the diagonal elements have high

299
00:10:10,720 --> 00:10:13,440
values it means that these words are not

300
00:10:13,440 --> 00:10:15,279
constantly replaced

301
00:10:15,279 --> 00:10:17,760
additionally we don't observe any sparse

302
00:10:17,760 --> 00:10:21,200
or fixed transitions between pairs

303
00:10:21,200 --> 00:10:24,000
these observation observations suggest

304
00:10:24,000 --> 00:10:25,920
high flexibility and concealing

305
00:10:25,920 --> 00:10:28,959
of the encoding we then

306
00:10:28,959 --> 00:10:30,959
evaluate the occurrences counts of the

307
00:10:30,959 --> 00:10:32,640
most likely words

308
00:10:32,640 --> 00:10:35,360
compared to non-watermark text our model

309
00:10:35,360 --> 00:10:37,360
keeps the count of these top words

310
00:10:37,360 --> 00:10:38,800
comparable

311
00:10:38,800 --> 00:10:40,320
this is in contrast with the no

312
00:10:40,320 --> 00:10:42,560
discriminator model that can cause clear

313
00:10:42,560 --> 00:10:46,000
statistical artifacts

314
00:10:46,000 --> 00:10:48,000
we then train independent binary

315
00:10:48,000 --> 00:10:49,920
classifiers to discriminate between

316
00:10:49,920 --> 00:10:52,079
non-paired watermarked and non-watermark

317
00:10:52,079 --> 00:10:53,440
text

318
00:10:53,440 --> 00:10:56,000
ideally the f1 score of the classifiers

319
00:10:56,000 --> 00:10:57,680
should be at random chance

320
00:10:57,680 --> 00:11:00,640
indicating high secrecy we examine

321
00:11:00,640 --> 00:11:02,160
different model variants and different

322
00:11:02,160 --> 00:11:03,920
classifiers

323
00:11:03,920 --> 00:11:06,079
the node discriminator model has a very

324
00:11:06,079 --> 00:11:09,279
high f1 score indicating poor secrecy

325
00:11:09,279 --> 00:11:10,800
increasing the naturalness of the

326
00:11:10,800 --> 00:11:13,920
watermarks helps to have higher secrecy

327
00:11:13,920 --> 00:11:14,720
therefore

328
00:11:14,720 --> 00:11:16,480
the full model had a close to random

329
00:11:16,480 --> 00:11:20,160
chance f1 score as required

330
00:11:20,160 --> 00:11:21,680
we then examine the robustness of

331
00:11:21,680 --> 00:11:23,519
watermarks

332
00:11:23,519 --> 00:11:25,360
the attacker's goal is to remove the

333
00:11:25,360 --> 00:11:27,279
watermark with minimal changes to the

334
00:11:27,279 --> 00:11:28,079
text in order

335
00:11:28,079 --> 00:11:31,120
to still have high utility therefore we

336
00:11:31,120 --> 00:11:32,399
study the relationship

337
00:11:32,399 --> 00:11:34,720
or the trade-off between width accuracy

338
00:11:34,720 --> 00:11:36,720
drop and the expert distance increase

339
00:11:36,720 --> 00:11:39,120
with respect to the original sentence

340
00:11:39,120 --> 00:11:41,440
we examined two categories of attacks

341
00:11:41,440 --> 00:11:43,440
the first one is basic attacks

342
00:11:43,440 --> 00:11:45,680
here the attacker knows about using a

343
00:11:45,680 --> 00:11:48,079
translation based with a marking scheme

344
00:11:48,079 --> 00:11:51,040
but not the model details the second one

345
00:11:51,040 --> 00:11:53,200
is advanced adaptive attacks in which

346
00:11:53,200 --> 00:11:55,040
the attacker has full knowledge about

347
00:11:55,040 --> 00:11:59,120
the model but no at box axis

348
00:11:59,120 --> 00:12:01,680
we consider two basic attacks the first

349
00:12:01,680 --> 00:12:03,040
one is denoising

350
00:12:03,040 --> 00:12:05,279
here the attacker applies random noise

351
00:12:05,279 --> 00:12:06,959
to non-watermark text

352
00:12:06,959 --> 00:12:09,519
then they train a denoising autoencoder

353
00:12:09,519 --> 00:12:10,000
to remove

354
00:12:10,000 --> 00:12:13,920
the noise the dae is then applied to the

355
00:12:13,920 --> 00:12:16,720
watermark text before decoding

356
00:12:16,720 --> 00:12:18,959
this attack assumes that the noise would

357
00:12:18,959 --> 00:12:20,720
approximate the watermark

358
00:12:20,720 --> 00:12:23,120
based on this assumption we hypothesize

359
00:12:23,120 --> 00:12:25,040
that improving the quality would make

360
00:12:25,040 --> 00:12:27,120
denoising less effective

361
00:12:27,120 --> 00:12:29,279
we validate this hypothesis by applying

362
00:12:29,279 --> 00:12:32,079
the attack on the three model variants

363
00:12:32,079 --> 00:12:34,079
we observed that the full model had a

364
00:12:34,079 --> 00:12:35,760
small drop in accuracy

365
00:12:35,760 --> 00:12:37,440
compared to the lower quality no

366
00:12:37,440 --> 00:12:39,839
discriminator model which empirically

367
00:12:39,839 --> 00:12:42,720
confirms our hypothesis

368
00:12:42,720 --> 00:12:45,440
the second attack is random changes we

369
00:12:45,440 --> 00:12:47,440
apply random word removal and

370
00:12:47,440 --> 00:12:50,160
replacement with a certain probability

371
00:12:50,160 --> 00:12:52,320
we study another variant where random

372
00:12:52,320 --> 00:12:54,320
changes are coupled with denoising to

373
00:12:54,320 --> 00:12:56,240
have a smoother output

374
00:12:56,240 --> 00:12:58,639
we observe that random changes cause

375
00:12:58,639 --> 00:13:00,639
high degradation to the output

376
00:13:00,639 --> 00:13:04,720
while is significantly affecting the

377
00:13:04,839 --> 00:13:07,040
accuracy

378
00:13:07,040 --> 00:13:08,880
advanced adaptive attacks assume that

379
00:13:08,880 --> 00:13:10,639
the attacker can train their own water

380
00:13:10,639 --> 00:13:12,480
marking model that's only different in

381
00:13:12,480 --> 00:13:12,880
the

382
00:13:12,880 --> 00:13:15,760
in the random initialization our first

383
00:13:15,760 --> 00:13:17,760
attack is re-watermarking

384
00:13:17,760 --> 00:13:20,320
the attacker inserts a random watermark

385
00:13:20,320 --> 00:13:22,000
using their model

386
00:13:22,000 --> 00:13:24,240
the goal of this attack is corrupting

387
00:13:24,240 --> 00:13:25,760
the original watermarks

388
00:13:25,760 --> 00:13:28,959
and confusing the first model decoder

389
00:13:28,959 --> 00:13:31,040
we observed that two independently

390
00:13:31,040 --> 00:13:32,560
trained models can use different

391
00:13:32,560 --> 00:13:34,880
patterns to encode the information

392
00:13:34,880 --> 00:13:36,880
and therefore the accuracy of the

393
00:13:36,880 --> 00:13:38,320
original watermarks

394
00:13:38,320 --> 00:13:40,560
didn't drop to random chance despite

395
00:13:40,560 --> 00:13:44,399
having a strong adaptive attack

396
00:13:44,399 --> 00:13:46,880
the second attack is dewatermarking and

397
00:13:46,880 --> 00:13:48,000
instead of perturbing

398
00:13:48,000 --> 00:13:50,480
the text with adding a second watermark

399
00:13:50,480 --> 00:13:52,320
the adversary could train a dewater

400
00:13:52,320 --> 00:13:53,839
making model

401
00:13:53,839 --> 00:13:56,480
we train a denoising auto encoder on the

402
00:13:56,480 --> 00:13:58,480
input and output pairs of the second

403
00:13:58,480 --> 00:14:00,240
watermarking model

404
00:14:00,240 --> 00:14:03,040
the dae is then applied to the output of

405
00:14:03,040 --> 00:14:05,199
the first watermarking model

406
00:14:05,199 --> 00:14:07,680
we observe that the drop in bit accuracy

407
00:14:07,680 --> 00:14:09,920
is similar to the watermarking

408
00:14:09,920 --> 00:14:11,839
we again attribute that to the

409
00:14:11,839 --> 00:14:14,000
differences between the two models

410
00:14:14,000 --> 00:14:16,639
however the marking has a less

411
00:14:16,639 --> 00:14:18,079
perturbation effect

412
00:14:18,079 --> 00:14:20,639
which makes it relatively the most

413
00:14:20,639 --> 00:14:21,360
effective

414
00:14:21,360 --> 00:14:24,240
attack among the attacks we studied yet

415
00:14:24,240 --> 00:14:26,160
it is the most computationally expensive

416
00:14:26,160 --> 00:14:27,920
one

417
00:14:27,920 --> 00:14:29,360
please refer to the paper for more

418
00:14:29,360 --> 00:14:33,199
details and examples about attacks

419
00:14:33,440 --> 00:14:35,680
finally we evaluate our model against

420
00:14:35,680 --> 00:14:36,720
baselines

421
00:14:36,720 --> 00:14:38,720
we implement a synonym substitution

422
00:14:38,720 --> 00:14:40,240
baseline based on a previously

423
00:14:40,240 --> 00:14:42,399
introduced watermarking method

424
00:14:42,399 --> 00:14:44,560
in summary we replace a word with this

425
00:14:44,560 --> 00:14:45,440
synonym

426
00:14:45,440 --> 00:14:47,519
we partition the synonym set for one

427
00:14:47,519 --> 00:14:49,040
word to two groups

428
00:14:49,040 --> 00:14:51,839
and assign different bits for each group

429
00:14:51,839 --> 00:14:54,160
the decoding is then done by lookup

430
00:14:54,160 --> 00:14:55,440
tables

431
00:14:55,440 --> 00:14:57,760
our method achieves higher flexibility

432
00:14:57,760 --> 00:14:59,600
and a significantly better trade-off

433
00:14:59,600 --> 00:15:02,480
between the different evaluation axes

434
00:15:02,480 --> 00:15:05,600
it also has higher resilience to attacks

435
00:15:05,600 --> 00:15:08,240
we then compare the two methods against

436
00:15:08,240 --> 00:15:09,519
non-watermark text

437
00:15:09,519 --> 00:15:12,560
using a user study we asked users

438
00:15:12,560 --> 00:15:14,880
to rate the quality of sentences from 0

439
00:15:14,880 --> 00:15:16,160
to 5.

440
00:15:16,160 --> 00:15:18,320
we showed that our method had higher

441
00:15:18,320 --> 00:15:20,560
ratings and is closer to the rating of

442
00:15:20,560 --> 00:15:23,920
non-watermark sentences

443
00:15:23,920 --> 00:15:25,839
to conclude we study language

444
00:15:25,839 --> 00:15:28,240
watermarking towards moving from passive

445
00:15:28,240 --> 00:15:30,560
to active defenses against deep fakes

446
00:15:30,560 --> 00:15:33,839
that are more sustainable we proposed

447
00:15:33,839 --> 00:15:34,560
the first

448
00:15:34,560 --> 00:15:37,120
end-to-end language watermarking scheme

449
00:15:37,120 --> 00:15:38,959
our model achieved a significantly

450
00:15:38,959 --> 00:15:40,800
better trade-off between the evaluation

451
00:15:40,800 --> 00:15:42,720
axes compared to a rule-based

452
00:15:42,720 --> 00:15:46,160
baseline our work opens new research

453
00:15:46,160 --> 00:15:47,920
directions of automatic language

454
00:15:47,920 --> 00:15:49,759
watermarking towards improving the

455
00:15:49,759 --> 00:15:53,199
quality and robustness of watermarks

456
00:15:53,199 --> 00:15:55,360
we hope that our work raises follow-up

457
00:15:55,360 --> 00:15:56,959
research and discussions on the

458
00:15:56,959 --> 00:15:59,839
responsible release and regulation of ai

459
00:15:59,839 --> 00:16:03,440
and text generation technologies

460
00:16:03,440 --> 00:16:05,040
our code and models are currently

461
00:16:05,040 --> 00:16:06,880
available thank you so much for

462
00:16:06,880 --> 00:16:10,639
listening and i'm looking forward to the

463
00:16:10,759 --> 00:16:13,759
discussion

