1
00:00:00,320 --> 00:00:03,040
hi there my name is james davis

2
00:00:03,040 --> 00:00:06,000
i'm a professor at purdue university and

3
00:00:06,000 --> 00:00:07,680
today i'll tell you about using

4
00:00:07,680 --> 00:00:09,360
selective memoization

5
00:00:09,360 --> 00:00:11,679
to defeat regular expression denial of

6
00:00:11,679 --> 00:00:12,639
service or

7
00:00:12,639 --> 00:00:17,359
redos redos is a long-standing problem

8
00:00:17,359 --> 00:00:20,080
and in this work we explore memoization

9
00:00:20,080 --> 00:00:21,600
as a solution

10
00:00:21,600 --> 00:00:23,920
in the process we introduce a new finite

11
00:00:23,920 --> 00:00:25,279
automaton model

12
00:00:25,279 --> 00:00:27,279
develop algorithmic and runtime

13
00:00:27,279 --> 00:00:28,960
optimizations

14
00:00:28,960 --> 00:00:31,279
show that our approach extends to

15
00:00:31,279 --> 00:00:32,399
non-clean

16
00:00:32,399 --> 00:00:35,200
features and evaluate on a large-scale

17
00:00:35,200 --> 00:00:37,840
data set

18
00:00:39,360 --> 00:00:42,000
regular expressions or regexes are a

19
00:00:42,000 --> 00:00:43,760
popular tool for string matching

20
00:00:43,760 --> 00:00:45,039
problems

21
00:00:45,039 --> 00:00:47,440
conceptually there are means of encoding

22
00:00:47,440 --> 00:00:48,719
a simple string language

23
00:00:48,719 --> 00:00:50,640
like the strings that look like email

24
00:00:50,640 --> 00:00:52,239
addresses

25
00:00:52,239 --> 00:00:54,559
they're widely used in practice and

26
00:00:54,559 --> 00:00:56,719
they're not exactly regular

27
00:00:56,719 --> 00:00:59,680
some of the regex extensions offered in

28
00:00:59,680 --> 00:01:00,960
most programming languages

29
00:01:00,960 --> 00:01:03,199
go beyond what can be done with clean

30
00:01:03,199 --> 00:01:06,159
regular regexes

31
00:01:06,159 --> 00:01:08,479
according to empirical studies software

32
00:01:08,479 --> 00:01:10,720
engineers commonly use regexes

33
00:01:10,720 --> 00:01:12,720
when they're processing untrusted

34
00:01:12,720 --> 00:01:14,880
unstructured text

35
00:01:14,880 --> 00:01:17,280
for some examples here's a regex that

36
00:01:17,280 --> 00:01:19,200
matches email addresses

37
00:01:19,200 --> 00:01:22,400
one for ipv4 internet addresses

38
00:01:22,400 --> 00:01:25,439
and one for windows usernames

39
00:01:25,439 --> 00:01:28,159
run time the responsibility for checking

40
00:01:28,159 --> 00:01:28,960
matches

41
00:01:28,960 --> 00:01:31,280
is delegated to an entity known as a

42
00:01:31,280 --> 00:01:32,000
regex

43
00:01:32,000 --> 00:01:35,280
engineer every mainstream programming

44
00:01:35,280 --> 00:01:36,079
language has

45
00:01:36,079 --> 00:01:39,520
one in their implementations they take a

46
00:01:39,520 --> 00:01:40,000
regex

47
00:01:40,000 --> 00:01:42,240
pattern like this one and they convert

48
00:01:42,240 --> 00:01:43,840
it to a construct

49
00:01:43,840 --> 00:01:46,079
similar to a non-deterministic finite

50
00:01:46,079 --> 00:01:48,640
automaton or nfa

51
00:01:48,640 --> 00:01:51,119
the redix engine then simulates the nfa

52
00:01:51,119 --> 00:01:52,640
on the candidate string

53
00:01:52,640 --> 00:01:55,439
to see if it can find a match so here's

54
00:01:55,439 --> 00:01:58,320
an example on the string aaa

55
00:01:58,320 --> 00:02:00,960
the red x engine has a choice here

56
00:02:00,960 --> 00:02:02,719
should it go up or down

57
00:02:02,719 --> 00:02:05,040
well regex engines use a backtracking

58
00:02:05,040 --> 00:02:06,479
search so it'll

59
00:02:06,479 --> 00:02:09,598
try going up and a matches so far so

60
00:02:09,598 --> 00:02:09,919
good

61
00:02:09,919 --> 00:02:11,440
b doesn't work so we're going to have to

62
00:02:11,440 --> 00:02:14,080
head back and go down

63
00:02:14,080 --> 00:02:16,000
this time we see the a again we have a

64
00:02:16,000 --> 00:02:17,200
tumor a's to kill

65
00:02:17,200 --> 00:02:20,720
so there's one and you can eat another a

66
00:02:20,720 --> 00:02:23,440
and proceed we declare success it

67
00:02:23,440 --> 00:02:24,959
matched

68
00:02:24,959 --> 00:02:26,800
so this is the depth first backtracking

69
00:02:26,800 --> 00:02:28,239
search algorithm

70
00:02:28,239 --> 00:02:29,680
when they see a choice then write down

71
00:02:29,680 --> 00:02:32,000
the options try one and

72
00:02:32,000 --> 00:02:33,840
backtrack to the others later if the

73
00:02:33,840 --> 00:02:35,680
first choice didn't work out

74
00:02:35,680 --> 00:02:37,760
this is straightforward to implement but

75
00:02:37,760 --> 00:02:39,920
it introduces potentially problematic

76
00:02:39,920 --> 00:02:42,239
worst case match performance

77
00:02:42,239 --> 00:02:44,319
in other words the regex engine may take

78
00:02:44,319 --> 00:02:46,000
a very long time to answer

79
00:02:46,000 --> 00:02:49,200
some match queries

80
00:02:49,680 --> 00:02:52,160
here's an example of a super linear

81
00:02:52,160 --> 00:02:53,040
regex

82
00:02:53,040 --> 00:02:55,040
this one has exponential worst case

83
00:02:55,040 --> 00:02:56,720
behavior

84
00:02:56,720 --> 00:02:59,040
fredex engines will construct something

85
00:02:59,040 --> 00:03:00,720
similar to this nfa

86
00:03:00,720 --> 00:03:03,120
which has two loops because of this

87
00:03:03,120 --> 00:03:05,519
extra quantifier

88
00:03:05,519 --> 00:03:09,360
the extra loop makes the nfa ambiguous

89
00:03:09,360 --> 00:03:11,680
every time the redix engine sees an a

90
00:03:11,680 --> 00:03:14,480
it's not sure which loop to take

91
00:03:14,480 --> 00:03:16,720
each a gives it two choices to explore

92
00:03:16,720 --> 00:03:18,560
and additional a's compounds the number

93
00:03:18,560 --> 00:03:20,000
of paths exponentially

94
00:03:20,000 --> 00:03:21,519
and critically all these paths are

95
00:03:21,519 --> 00:03:23,599
leading to the same place over and over

96
00:03:23,599 --> 00:03:25,200
again

97
00:03:25,200 --> 00:03:27,280
so this means that under the input a

98
00:03:27,280 --> 00:03:29,599
bunch of a's and an exclamation point

99
00:03:29,599 --> 00:03:31,280
we'll have an exponential number of

100
00:03:31,280 --> 00:03:32,640
paths to explore

101
00:03:32,640 --> 00:03:34,879
in the number of a's and the red x

102
00:03:34,879 --> 00:03:36,319
engine will try all of them

103
00:03:36,319 --> 00:03:38,560
to show that this input doesn't match

104
00:03:38,560 --> 00:03:40,400
the regions

105
00:03:40,400 --> 00:03:42,640
now the situation isn't this bad for all

106
00:03:42,640 --> 00:03:44,239
regexes

107
00:03:44,239 --> 00:03:46,560
unambiguous regixes have linear time

108
00:03:46,560 --> 00:03:47,680
behavior

109
00:03:47,680 --> 00:03:50,720
some regixes are polytime and a few are

110
00:03:50,720 --> 00:03:52,959
exponential time

111
00:03:52,959 --> 00:03:55,040
but regardless of their worst case

112
00:03:55,040 --> 00:03:56,159
behavior most

113
00:03:56,159 --> 00:03:58,840
regexes run in linear time on typical

114
00:03:58,840 --> 00:04:01,519
inputs it often takes a malicious

115
00:04:01,519 --> 00:04:04,080
input to trigger the worst case behavior

116
00:04:04,080 --> 00:04:05,920
that's why these slow regexes get used

117
00:04:05,920 --> 00:04:08,159
in practice

118
00:04:08,159 --> 00:04:09,920
let's think about the implications of

119
00:04:09,920 --> 00:04:12,080
these properties when registers are used

120
00:04:12,080 --> 00:04:14,640
within the web stack

121
00:04:14,640 --> 00:04:16,798
this diagram represents the typical

122
00:04:16,798 --> 00:04:18,639
parts of a web service

123
00:04:18,639 --> 00:04:21,120
from the user through a web client past

124
00:04:21,120 --> 00:04:22,079
a firewall

125
00:04:22,079 --> 00:04:24,639
into business logic through persistent

126
00:04:24,639 --> 00:04:25,360
storage

127
00:04:25,360 --> 00:04:28,560
and back again all these systems support

128
00:04:28,560 --> 00:04:30,720
a reasonably consistent regex language

129
00:04:30,720 --> 00:04:33,280
for input standardization

130
00:04:33,280 --> 00:04:36,240
any path a legitimate client can use an

131
00:04:36,240 --> 00:04:38,639
attacker can use as well

132
00:04:38,639 --> 00:04:40,560
and in a regex based denolive service

133
00:04:40,560 --> 00:04:42,080
attack the attacker

134
00:04:42,080 --> 00:04:44,479
triggers a regex's super linear worst

135
00:04:44,479 --> 00:04:45,759
case behavior

136
00:04:45,759 --> 00:04:47,520
in a location that will affect other

137
00:04:47,520 --> 00:04:48,960
clients for example

138
00:04:48,960 --> 00:04:52,479
poisoning an event loop or a worker pool

139
00:04:52,479 --> 00:04:55,040
redos can affect individual web services

140
00:04:55,040 --> 00:04:56,560
like stack overflow

141
00:04:56,560 --> 00:04:59,040
and it can affect many services at once

142
00:04:59,040 --> 00:04:59,919
when it impacts

143
00:04:59,919 --> 00:05:03,039
cdns and middleware like cloudflare

144
00:05:03,039 --> 00:05:06,000
but these aren't one-off examples recent

145
00:05:06,000 --> 00:05:06,560
research

146
00:05:06,560 --> 00:05:08,960
has shown that redos vulnerable regexes

147
00:05:08,960 --> 00:05:11,520
are common in major modules and major

148
00:05:11,520 --> 00:05:13,680
websites

149
00:05:13,680 --> 00:05:15,440
what's frustrating about this class of

150
00:05:15,440 --> 00:05:17,600
attack is that in some sense it's beyond

151
00:05:17,600 --> 00:05:19,600
the responsibility of the application

152
00:05:19,600 --> 00:05:20,960
engineer

153
00:05:20,960 --> 00:05:23,199
redos attacks are exploiting a weakness

154
00:05:23,199 --> 00:05:24,960
in critical infrastructure

155
00:05:24,960 --> 00:05:27,039
in the regex engines used in most

156
00:05:27,039 --> 00:05:29,919
programming languages

157
00:05:30,080 --> 00:05:31,840
as i wrap up this part of my talk i'd

158
00:05:31,840 --> 00:05:33,360
like to highlight two additional things

159
00:05:33,360 --> 00:05:34,960
about regex engines

160
00:05:34,960 --> 00:05:38,000
first most regex engines use

161
00:05:38,000 --> 00:05:39,919
back tracking and exposed applications

162
00:05:39,919 --> 00:05:41,280
to redos

163
00:05:41,280 --> 00:05:43,840
and second although there are some regex

164
00:05:43,840 --> 00:05:46,160
engines with linear time guarantees

165
00:05:46,160 --> 00:05:48,080
there are subtleties between regex

166
00:05:48,080 --> 00:05:50,320
engine dialects that render translation

167
00:05:50,320 --> 00:05:52,560
of tenuous task

168
00:05:52,560 --> 00:05:54,639
variations on the redos attack have

169
00:05:54,639 --> 00:05:56,880
existed since the 1980s

170
00:05:56,880 --> 00:05:59,919
but redos continues to be a problem

171
00:05:59,919 --> 00:06:02,479
alternative algorithms are well known

172
00:06:02,479 --> 00:06:03,120
but not

173
00:06:03,120 --> 00:06:06,240
widely adopted we conjecture

174
00:06:06,240 --> 00:06:09,360
that the cause is technical inertia

175
00:06:09,360 --> 00:06:11,600
redex engines are complex and divergent

176
00:06:11,600 --> 00:06:12,800
in their semantics

177
00:06:12,800 --> 00:06:14,560
and maintainers are understandably

178
00:06:14,560 --> 00:06:16,160
risk-averse

179
00:06:16,160 --> 00:06:18,400
backwards incompatible changes could

180
00:06:18,400 --> 00:06:21,280
have far-reaching effects

181
00:06:21,280 --> 00:06:23,440
therefore with these constraints in mind

182
00:06:23,440 --> 00:06:25,440
we set out to identify a backwards

183
00:06:25,440 --> 00:06:26,800
compatible approach

184
00:06:26,800 --> 00:06:28,479
that would protect an existing

185
00:06:28,479 --> 00:06:29,840
back-tracking engine

186
00:06:29,840 --> 00:06:33,199
against redos with minimal modification

187
00:06:33,199 --> 00:06:35,280
our main results are space-time

188
00:06:35,280 --> 00:06:37,600
complexity trade-offs of memoization for

189
00:06:37,600 --> 00:06:38,880
regexes

190
00:06:38,880 --> 00:06:41,759
extensions to non-regular regex features

191
00:06:41,759 --> 00:06:45,759
and runtime storage optimizations

192
00:06:46,000 --> 00:06:47,919
the general approach we'll consider is

193
00:06:47,919 --> 00:06:49,599
called memoization

194
00:06:49,599 --> 00:06:52,000
it's been proposed in the past but until

195
00:06:52,000 --> 00:06:53,759
now no one has explored how to make it

196
00:06:53,759 --> 00:06:55,919
cheap enough to be practical

197
00:06:55,919 --> 00:06:57,680
let me first explain how a full

198
00:06:57,680 --> 00:06:59,840
memoization approach works

199
00:06:59,840 --> 00:07:02,000
we've got our input now the red x engine

200
00:07:02,000 --> 00:07:04,080
will simulate the automaton on this

201
00:07:04,080 --> 00:07:05,599
input

202
00:07:05,599 --> 00:07:08,160
so earlier i talked about how a back

203
00:07:08,160 --> 00:07:10,000
tracking search works the red extension

204
00:07:10,000 --> 00:07:10,800
will come along

205
00:07:10,800 --> 00:07:13,039
and explore every potentially feasible

206
00:07:13,039 --> 00:07:15,759
path through the automaton

207
00:07:15,759 --> 00:07:18,080
we'll say take the inner loop a couple

208
00:07:18,080 --> 00:07:20,400
times stay at vertex 1 the one with an a

209
00:07:20,400 --> 00:07:22,400
and advance offsets through the string

210
00:07:22,400 --> 00:07:24,560
and when we fail we'll backtrack and end

211
00:07:24,560 --> 00:07:26,479
up at the same point again

212
00:07:26,479 --> 00:07:29,680
vertex 1 offset 2 vertex 1 offset 3 and

213
00:07:29,680 --> 00:07:31,599
so on

214
00:07:31,599 --> 00:07:33,759
you can see that we we're retracing our

215
00:07:33,759 --> 00:07:35,520
steps along different paths

216
00:07:35,520 --> 00:07:37,599
the nfa simulation is visiting the same

217
00:07:37,599 --> 00:07:39,599
vertices at the same offsets

218
00:07:39,599 --> 00:07:42,000
over and over again that makes sense

219
00:07:42,000 --> 00:07:43,280
when you look at the definition of

220
00:07:43,280 --> 00:07:44,639
ambiguity

221
00:07:44,639 --> 00:07:46,560
ambiguity means there are multiple paths

222
00:07:46,560 --> 00:07:48,479
through the automaton that lead to the

223
00:07:48,479 --> 00:07:50,479
exact same place

224
00:07:50,479 --> 00:07:52,479
so a natural solution to avoiding this

225
00:07:52,479 --> 00:07:53,759
redundancy is

226
00:07:53,759 --> 00:07:56,240
applying memoization we'll introduce

227
00:07:56,240 --> 00:07:58,720
breadcrumbs into the nfa simulation

228
00:07:58,720 --> 00:08:00,720
and if we see a breadcrumb we know that

229
00:08:00,720 --> 00:08:02,160
path didn't work

230
00:08:02,160 --> 00:08:04,240
so we can mark the vertex offset pairs

231
00:08:04,240 --> 00:08:05,280
as we visit them

232
00:08:05,280 --> 00:08:07,919
and then short circuit redundant search

233
00:08:07,919 --> 00:08:09,520
paths

234
00:08:09,520 --> 00:08:11,280
so i've got an unsurprising theorem

235
00:08:11,280 --> 00:08:13,440
about this if we memoize visits to every

236
00:08:13,440 --> 00:08:14,160
vertex

237
00:08:14,160 --> 00:08:16,080
the simulation runs in linear steps for

238
00:08:16,080 --> 00:08:17,360
the input w

239
00:08:17,360 --> 00:08:19,440
and it requires copying the input string

240
00:08:19,440 --> 00:08:21,840
for every vertex we memoize

241
00:08:21,840 --> 00:08:23,520
now this is a nice improvement in the

242
00:08:23,520 --> 00:08:25,599
time complexity but these strengths can

243
00:08:25,599 --> 00:08:26,560
be pretty long

244
00:08:26,560 --> 00:08:28,319
so it would be nice to reduce the space

245
00:08:28,319 --> 00:08:30,639
complexity as well

246
00:08:30,639 --> 00:08:32,559
the problem is that many real world

247
00:08:32,559 --> 00:08:34,080
regexes are complex

248
00:08:34,080 --> 00:08:36,399
with tens hundreds and even thousands of

249
00:08:36,399 --> 00:08:38,159
nfa vertices

250
00:08:38,159 --> 00:08:40,559
combined with long input strings this

251
00:08:40,559 --> 00:08:42,799
base cost multiplier might be too much

252
00:08:42,799 --> 00:08:45,120
for practical purposes

253
00:08:45,120 --> 00:08:46,880
fortunately we've shown that we can

254
00:08:46,880 --> 00:08:48,959
memoize visits to a selected

255
00:08:48,959 --> 00:08:51,200
subset of the vertices and obtain the

256
00:08:51,200 --> 00:08:52,000
same time

257
00:08:52,000 --> 00:08:55,040
guarantee with lower space cost

258
00:08:55,040 --> 00:08:57,040
the first subset are the automaton

259
00:08:57,040 --> 00:08:59,120
vertices within degree greater than one

260
00:08:59,120 --> 00:09:00,640
which results from quantifiers and

261
00:09:00,640 --> 00:09:02,959
disjunctions these vertices can be

262
00:09:02,959 --> 00:09:04,880
reached along multiple paths

263
00:09:04,880 --> 00:09:07,120
since ambiguity means multiple paths

264
00:09:07,120 --> 00:09:08,000
makes sense that

265
00:09:08,000 --> 00:09:10,160
only memoizing these ones gets us the

266
00:09:10,160 --> 00:09:11,600
same effect

267
00:09:11,600 --> 00:09:13,680
same time complexity lower space

268
00:09:13,680 --> 00:09:15,680
complexity

269
00:09:15,680 --> 00:09:17,839
but remember it's the compounding of

270
00:09:17,839 --> 00:09:19,120
paths through loops

271
00:09:19,120 --> 00:09:22,000
that's really problematic if we just

272
00:09:22,000 --> 00:09:23,360
focus on those

273
00:09:23,360 --> 00:09:25,680
we've shown that we can memoize still

274
00:09:25,680 --> 00:09:27,920
fewer vertices to trade a slightly

275
00:09:27,920 --> 00:09:28,480
larger

276
00:09:28,480 --> 00:09:31,279
but bounded time cost for even less

277
00:09:31,279 --> 00:09:32,320
space

278
00:09:32,320 --> 00:09:34,399
here we'll memoize only the ancestor

279
00:09:34,399 --> 00:09:36,560
notes the ones that are upstream from

280
00:09:36,560 --> 00:09:39,360
back edges in the automaton graph

281
00:09:39,360 --> 00:09:41,279
memoizing these doesn't prevent all

282
00:09:41,279 --> 00:09:43,040
ambiguity but it does prevent

283
00:09:43,040 --> 00:09:45,600
compounding ambiguity

284
00:09:45,600 --> 00:09:48,160
in the paper we also extend our memoized

285
00:09:48,160 --> 00:09:50,240
non-deterministic automaton model

286
00:09:50,240 --> 00:09:53,279
to two common extended regex features

287
00:09:53,279 --> 00:09:55,279
those with zero width assertions or look

288
00:09:55,279 --> 00:09:58,240
arounds and with back references

289
00:09:58,240 --> 00:10:00,240
our formalization followed red extension

290
00:10:00,240 --> 00:10:02,160
implementations adding additional edge

291
00:10:02,160 --> 00:10:03,680
types into the automaton

292
00:10:03,680 --> 00:10:06,000
and considering the space time and

293
00:10:06,000 --> 00:10:08,240
memoization effects

294
00:10:08,240 --> 00:10:10,160
so using selective memorization we

295
00:10:10,160 --> 00:10:12,160
reduce the space cost without degrading

296
00:10:12,160 --> 00:10:14,320
the linear time guarantee

297
00:10:14,320 --> 00:10:16,560
but nothing is free we still have to pay

298
00:10:16,560 --> 00:10:18,399
a space cost and we'd like to reduce

299
00:10:18,399 --> 00:10:20,640
that cost as much as possible

300
00:10:20,640 --> 00:10:22,320
in the paper we talk about different

301
00:10:22,320 --> 00:10:24,720
encoding schemes for the memo table

302
00:10:24,720 --> 00:10:28,399
one of which seems quite promising

303
00:10:28,399 --> 00:10:31,279
we evaluated our ideas in a prototype

304
00:10:31,279 --> 00:10:32,000
memoized

305
00:10:32,000 --> 00:10:34,720
regex engine and we tested it on the

306
00:10:34,720 --> 00:10:36,880
largest existing regex dataset

307
00:10:36,880 --> 00:10:38,640
which has about half a million regular

308
00:10:38,640 --> 00:10:41,279
expressions of which about 50 000

309
00:10:41,279 --> 00:10:44,079
are super linear in the worst case our

310
00:10:44,079 --> 00:10:46,240
prototype supports 84 percent of this

311
00:10:46,240 --> 00:10:47,040
data set

312
00:10:47,040 --> 00:10:52,240
including 95 of the super linear regexes

313
00:10:52,240 --> 00:10:54,320
these figures illustrate the reduction

314
00:10:54,320 --> 00:10:55,839
in match times resulting from

315
00:10:55,839 --> 00:10:58,480
memoization on several examples

316
00:10:58,480 --> 00:11:00,320
on the x-axis we have the lengths of the

317
00:11:00,320 --> 00:11:01,519
input string

318
00:11:01,519 --> 00:11:03,600
the y-axis is the match time in seconds

319
00:11:03,600 --> 00:11:04,880
in our prototype

320
00:11:04,880 --> 00:11:07,120
and blue is without memoization and an

321
00:11:07,120 --> 00:11:09,519
orange is with memoization

322
00:11:09,519 --> 00:11:11,360
so here we have behavior on the

323
00:11:11,360 --> 00:11:14,640
exponential microsoft username rechecks

324
00:11:14,640 --> 00:11:17,600
the cloudflare regex a regex with look

325
00:11:17,600 --> 00:11:18,880
around assertions

326
00:11:18,880 --> 00:11:21,760
and one with back references each figure

327
00:11:21,760 --> 00:11:23,600
illustrates the behavior predicted by

328
00:11:23,600 --> 00:11:24,640
our theorems

329
00:11:24,640 --> 00:11:28,160
for regular and extended regexes

330
00:11:28,160 --> 00:11:30,320
in a larger experiment we tested every

331
00:11:30,320 --> 00:11:31,680
supported superlinear

332
00:11:31,680 --> 00:11:33,680
regex with problematic inputs of

333
00:11:33,680 --> 00:11:35,200
different lengths and our theorems

334
00:11:35,200 --> 00:11:36,560
correctly predicted the

335
00:11:36,560 --> 00:11:38,880
linear and the input length performance

336
00:11:38,880 --> 00:11:40,560
in each case

337
00:11:40,560 --> 00:11:42,720
now these time reductions are great but

338
00:11:42,720 --> 00:11:44,240
we can't just move the cost from the

339
00:11:44,240 --> 00:11:46,240
time domain into the space domain

340
00:11:46,240 --> 00:11:48,320
so we needed to check how practical our

341
00:11:48,320 --> 00:11:50,480
approach is in terms of space costs

342
00:11:50,480 --> 00:11:53,680
too to start with let's look at space

343
00:11:53,680 --> 00:11:55,279
complexity

344
00:11:55,279 --> 00:11:57,440
the x-axis shows the vertex sets of

345
00:11:57,440 --> 00:11:59,040
interest for memorization

346
00:11:59,040 --> 00:12:01,600
and the y-axis shows the sizes of those

347
00:12:01,600 --> 00:12:03,680
sets for each of the schemes

348
00:12:03,680 --> 00:12:05,440
the whiskers are the 1st and 99th

349
00:12:05,440 --> 00:12:06,880
percentiles

350
00:12:06,880 --> 00:12:08,800
the blue bars are all of the supported

351
00:12:08,800 --> 00:12:10,880
regexes and the orange bars are the

352
00:12:10,880 --> 00:12:13,120
super linear subset

353
00:12:13,120 --> 00:12:15,040
so from this we can see first that full

354
00:12:15,040 --> 00:12:17,120
memoization would be costly

355
00:12:17,120 --> 00:12:19,120
with an order of magnitude increase in

356
00:12:19,120 --> 00:12:21,120
storage cost over the original string

357
00:12:21,120 --> 00:12:23,040
remember we have to multiply the number

358
00:12:23,040 --> 00:12:24,880
of vertices by the length of the string

359
00:12:24,880 --> 00:12:27,440
to see the actual storage cost

360
00:12:27,440 --> 00:12:29,279
second we can see that the selection

361
00:12:29,279 --> 00:12:31,200
schemes will typically have a much

362
00:12:31,200 --> 00:12:32,639
smaller multiplier

363
00:12:32,639 --> 00:12:36,160
for both normal and super linear regexes

364
00:12:36,160 --> 00:12:38,320
but how do these complexities manifest

365
00:12:38,320 --> 00:12:39,200
as space

366
00:12:39,200 --> 00:12:43,680
costs during a realistic attack

367
00:12:43,680 --> 00:12:46,560
here we've got the space cost for

368
00:12:46,560 --> 00:12:48,399
evaluating each of the regexes in our

369
00:12:48,399 --> 00:12:49,120
data set

370
00:12:49,120 --> 00:12:52,000
using stack overflow sized input strings

371
00:12:52,000 --> 00:12:54,480
about 20 kilobytes

372
00:12:54,480 --> 00:12:57,920
the x-axis shows the memorization scheme

373
00:12:57,920 --> 00:13:00,720
the y-axis shows the space cost now this

374
00:13:00,720 --> 00:13:02,720
is a log scale with a kilobyte

375
00:13:02,720 --> 00:13:05,519
megabyte and gigabyte markdown the

376
00:13:05,519 --> 00:13:07,440
whiskers here show the 5th and 90th

377
00:13:07,440 --> 00:13:08,480
percentiles

378
00:13:08,480 --> 00:13:11,600
under three different encoding schemes

379
00:13:11,600 --> 00:13:14,399
a full memo table a hash table that only

380
00:13:14,399 --> 00:13:15,920
records positive entries

381
00:13:15,920 --> 00:13:19,120
and run length encoding

382
00:13:19,120 --> 00:13:21,279
so using the course metrics of a

383
00:13:21,279 --> 00:13:22,399
kilobyte is cheap

384
00:13:22,399 --> 00:13:24,560
a megabyte is pricey and a gigabyte is

385
00:13:24,560 --> 00:13:25,680
not going to work

386
00:13:25,680 --> 00:13:28,160
our data show that in the orange part

387
00:13:28,160 --> 00:13:30,480
the hash table based encoding is simply

388
00:13:30,480 --> 00:13:32,320
not going to work out in any case the

389
00:13:32,320 --> 00:13:34,000
overhead of hashing is just

390
00:13:34,000 --> 00:13:37,680
too great in the blue bars we see that a

391
00:13:37,680 --> 00:13:39,600
simple memo table starts out

392
00:13:39,600 --> 00:13:42,240
too expensive but it reduces to pricey

393
00:13:42,240 --> 00:13:44,560
if we select carefully

394
00:13:44,560 --> 00:13:46,720
and in the green bars an efficient

395
00:13:46,720 --> 00:13:48,720
encoding scheme starts out pricey

396
00:13:48,720 --> 00:13:51,680
but reduces to cheap with selection in

397
00:13:51,680 --> 00:13:52,399
fact

398
00:13:52,399 --> 00:13:54,320
combining selection and an efficient

399
00:13:54,320 --> 00:13:55,760
encoding scheme

400
00:13:55,760 --> 00:13:58,800
yields typically constant space costs in

401
00:13:58,800 --> 00:13:59,760
our experiments

402
00:13:59,760 --> 00:14:01,199
remember the upper whisker there is a

403
00:14:01,199 --> 00:14:04,880
90th percentile of costs

404
00:14:04,880 --> 00:14:06,240
this concludes all the measurements i

405
00:14:06,240 --> 00:14:07,680
want to present there are some more in

406
00:14:07,680 --> 00:14:09,680
the paper

407
00:14:09,680 --> 00:14:12,480
the redox problem has thus far defied a

408
00:14:12,480 --> 00:14:14,399
practical solution

409
00:14:14,399 --> 00:14:16,480
in this paper we revisited this problem

410
00:14:16,480 --> 00:14:18,959
from a data-driven perspective

411
00:14:18,959 --> 00:14:21,760
grounded in a new large-scale regex data

412
00:14:21,760 --> 00:14:22,480
set

413
00:14:22,480 --> 00:14:25,440
we explored two selective memoization

414
00:14:25,440 --> 00:14:26,320
schemes

415
00:14:26,320 --> 00:14:28,160
which have attractive time complexity

416
00:14:28,160 --> 00:14:29,440
guarantees and low

417
00:14:29,440 --> 00:14:32,639
space complexity in experiments

418
00:14:32,639 --> 00:14:35,440
our redos defense achieves linear time

419
00:14:35,440 --> 00:14:36,240
behavior

420
00:14:36,240 --> 00:14:38,720
with constant space costs for ninety

421
00:14:38,720 --> 00:14:41,360
percent of super linear regexes

422
00:14:41,360 --> 00:14:43,279
so it seems that combining selective

423
00:14:43,279 --> 00:14:46,000
memoization with a good encoding scheme

424
00:14:46,000 --> 00:14:49,360
can eliminate redos in future work

425
00:14:49,360 --> 00:14:51,440
we are examining balance on the encoding

426
00:14:51,440 --> 00:14:52,560
scheme cost

427
00:14:52,560 --> 00:14:54,399
and working towards implementations in

428
00:14:54,399 --> 00:14:56,800
real world regex engines

429
00:14:56,800 --> 00:14:59,599
thanks for your attention

