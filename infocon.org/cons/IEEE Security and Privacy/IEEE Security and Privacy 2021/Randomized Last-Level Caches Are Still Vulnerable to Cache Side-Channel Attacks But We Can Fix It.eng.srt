1
00:00:00,000 --> 00:00:03,500
Hello everyone! I am very happy to present our work:

2
00:00:03,600 --> 00:00:09,500
Randomized last-level caches are still vulnerable to cache side-channel attacks! But we can fix it.

3
00:00:09,600 --> 00:00:16,500
This is a joint research between the Institute of Information Engineering at CAS and the Pennsylvania State University.

4
00:00:16,600 --> 00:00:19,500
I am Wei Song, the first author of this paper.

5
00:00:19,600 --> 00:00:25,500
Randomized caches have recently been revived as a promising defense technique against

6
00:00:25,600 --> 00:00:30,500
the conflict-based cache side-channel attacks targeting the last-level caches.

7
00:00:30,600 --> 00:00:35,500
Two of the latest papers, CEASER-S and ScatterCache, published at nearly the same time.

8
00:00:36,600 --> 00:00:41,500
They both use randomized skewed caches to thwart the remaining attacks against the randomized set associative caches.

9
00:00:41,600 --> 00:00:46,500
However, after a detailed analysis, a number of issues have been discovered.

10
00:00:46,600 --> 00:00:52,500
The possibility of using flush instructions to directly flush the eviction set has been overlooked.

11
00:00:52,600 --> 00:00:57,500
The possibility of using smaller eviction sets with lower eviction rate has been overlooked as well.

12
00:00:57,600 --> 00:01:03,500
Meanwhile, we found that counting the remap period by the number of LLC accesses is flawed.

13
00:01:03,600 --> 00:01:07,500
As hardware designers on the defense side, we actually believe that

14
00:01:07,600 --> 00:01:14,500
randomized caches is the proper defense that should be adopted. We just need to fix the problems.

15
00:01:14,600 --> 00:01:18,500
For this end, we propose several ideas in this presentation:

16
00:01:18,600 --> 00:01:23,500
We need to count the remap period by LLC evictions and further reduce the remap period,

17
00:01:23,600 --> 00:01:28,500
using multi-step relocation can reduce the cost in each remap,

18
00:01:28,600 --> 00:01:34,500
instead of periodically remaps, trigger remaps when under attack might be a good idea.

19
00:01:34,600 --> 00:01:39,500
Also we are in favor of the non-skewed set-associative caches rather than the skewed ones.

20
00:01:39,600 --> 00:01:44,500
Lets first recall the structure of the randomized skewed caches.

21
00:01:44,600 --> 00:01:49,500
Here shows a randomized skewed cache with four ways divided into two skew partitions.

22
00:01:49,600 --> 00:01:54,500
The mapping from the addresses to cache set indices is encrypted

23
00:01:54,600 --> 00:01:58,500
so that attacker cannot form eviction sets by aligning addresses.

24
00:01:58,600 --> 00:02:02,500
They are forced to search for eviction sets at runtime.

25
00:02:02,600 --> 00:02:06,500
To further increase the difficulties in searching for eviction sets,

26
00:02:06,600 --> 00:02:09,500
the keys used in each skew partition is different

27
00:02:09,600 --> 00:02:13,500
and an addresses might be cache in any skew partition with equal chance.

28
00:02:13,600 --> 00:02:17,500
As a results, it is becoming nearly impossible to find an eviction set

29
00:02:17,600 --> 00:02:21,500
using fully congruent addresses within any reasonable period of time.

30
00:02:21,600 --> 00:02:27,500
Unfortunately, attackers might instead choose to use partially congruent addresses.

31
00:02:27,600 --> 00:02:31,500
Although the found eviction sets are larger, they are still usable.

32
00:02:31,600 --> 00:02:36,500
As shown in the figure, when the eviction set contains enough partially congruent addresses,

33
00:02:36,600 --> 00:02:40,500
eventually the cached victim data will be dislodged by one of them.

34
00:02:40,600 --> 00:02:46,500
Currently there are three types of searching algorithms that still work on randomized caches.

35
00:02:46,600 --> 00:02:52,500
The first one is called group elimination, which we call GE in this presentation.

36
00:02:52,600 --> 00:02:57,500
It starts with a large eviction set and then prune it by removing multiple addresses at a time

37
00:02:57,600 --> 00:02:59,500
to speed up the process.

38
00:02:59,600 --> 00:03:03,500
The second one we call it conflict testing, CT for short.

39
00:03:03,600 --> 00:03:09,500
In this algorithm, the attacker constantly accesses random addresses and test each one of them to see

40
00:03:09,600 --> 00:03:15,500
whether it is conflicted with the target. If so, it is then put into the eviction set.

41
00:03:15,600 --> 00:03:18,500
The process stops when enough conflicted addresses are found.

42
00:03:18,600 --> 00:03:24,500
The final one is called prime, prune then test in this paper, or PPT in short.

43
00:03:24,600 --> 00:03:29,500
It starts with a really large prime set to prime the whole LLC,

44
00:03:29,600 --> 00:03:34,500
then the prime set is pruned to make sure all remaining addresses are concurrently cached.

45
00:03:34,600 --> 00:03:42,500
Finally it tests the conflicted addresses in the prime set by traverse the set again after revisiting the target.

46
00:03:42,600 --> 00:03:47,500
Generally speaking, all algorithms are pretty efficient and finish in linear time.

47
00:03:47,600 --> 00:03:53,500
The GE algorithm is robust but does not work in randomized skewed caches.

48
00:03:53,600 --> 00:04:01,500
The LLC trace of the CT algorithm is almost noise but it incurs a huge amount of accesses to the victim.

49
00:04:01,600 --> 00:04:05,500
The PPT algorithm has the minimum number of victim accesses in the three

50
00:04:05,600 --> 00:04:11,500
but it is the most sensitive one regarding to the replacement policy used by the cache.

51
00:04:11,600 --> 00:04:15,500
Now lets talk about the problems found in this paper.

52
00:04:15,600 --> 00:04:22,500
One known issue with using eviction sets composed of partially congruent addresses is the re-usability.

53
00:04:22,600 --> 00:04:29,500
As shown in the example, assume the eviction set has dislodged the victim data the first time and we try to use it again.

54
00:04:29,600 --> 00:04:37,500
The victim data is revisited and it might evict one and only one address of the previously used eviction set.

55
00:04:37,600 --> 00:04:46,500
Since all other addresses of the set might have already been cached and the evicted one is only a partially congruent address,

56
00:04:46,600 --> 00:04:51,500
revisiting the eviction set has only a 50% chance to dislodge the victim data again.

57
00:04:51,600 --> 00:04:54,500
If it fails, the eviction set becomes useless.

58
00:04:54,600 --> 00:05:01,500
To resolve this problem, the existing solution is to use a prime set to prime the whole LLC,

59
00:05:01,600 --> 00:05:03,500
which also evicts the eviction set as well.

60
00:05:03,600 --> 00:05:09,500
Although priming the LLC is rather a slow process and causes enormous amount of noise.

61
00:05:09,600 --> 00:05:13,500
So we thought, why not just flush the eviction set directly?

62
00:05:13,600 --> 00:05:18,500
No one says attackers cannot use the clflush instruction if it is available.

63
00:05:18,600 --> 00:05:24,500
Because the eviction set belongs to the attacker, they can be flushed by the attacker.

64
00:05:24,600 --> 00:05:30,500
By doing this, we can save a huge amount of LLC accesses and significantly reduce time and noise.

65
00:05:30,600 --> 00:05:35,500
The analysis provided in the original work of PPT is over-pessimistic.

66
00:05:35,600 --> 00:05:43,500
The authors of the ScatterCache argues attackers are likely to use eviction sets with 100% eviction rate.

67
00:05:43,600 --> 00:05:49,500
This means the eviction sets are large and attackers need to spend quite a long time to find them.

68
00:05:49,600 --> 00:05:55,500
As shown in our estimation, 39 partially congrunent addresses are required to achieve a 80% eviction rate

69
00:05:55,600 --> 00:06:01,500
while only 25 addresses are needed for achieving a lower 30% eviction rate.

70
00:06:01,600 --> 00:06:05,500
Since the time cost in finding eviction sets is linear with the size,

71
00:06:05,600 --> 00:06:10,500
using larger eviction sets reduce the chance to launch a successful attack.

72
00:06:10,600 --> 00:06:13,500
If I were the attacker, I would think:

73
00:06:13,600 --> 00:06:17,500
why not use smaller eviction sets with a reduced eviction rate?

74
00:06:17,600 --> 00:06:21,500
Some attacks might just work with these small eviction sets.

75
00:06:21,600 --> 00:06:26,500
Two papers published recently have already agreed with us on this.

76
00:06:26,600 --> 00:06:31,500
We further argue that, at least in EVICT+TIME attacks,

77
00:06:31,600 --> 00:06:37,500
attackers can achieve 100% eviction rate by just applying the same small eviction set multiple time,

78
00:06:37,600 --> 00:06:39,500
as shown in the figure.

79
00:06:39,600 --> 00:06:45,500
The overall time is significantly shorter than finding and then applying a large set.

80
00:06:45,600 --> 00:06:52,500
Proposed by CEASER-S, periodically remap can efficiently thwart the search for eviction sets at runtime.

81
00:06:52,600 --> 00:07:00,500
It also estimated that remap every 100 LLC accesses per cache block is frequent enough.

82
00:07:00,600 --> 00:07:06,500
However, it seems to use the cache accesses rather than LLC accesses in the estimation

83
00:07:06,600 --> 00:07:10,500
which failed to consider the filter effect of the level-one caches.

84
00:07:10,600 --> 00:07:15,500
As a result showing in the figure, when using CT algorithm,

85
00:07:15,600 --> 00:07:18,500
the number of LLC accesses are equivalent to the number of evictions

86
00:07:18,600 --> 00:07:22,500
and are only half of the total cache accesses.

87
00:07:22,600 --> 00:07:28,500
This is because almost all accesses to the victim data are served by the L1 cache alone.

88
00:07:28,600 --> 00:07:33,500
The period estimation in CEASER-S is wrong and

89
00:07:33,600 --> 00:07:39,500
we propose to count the remap period by LLC evictions rather than LLC accesses.

90
00:07:39,600 --> 00:07:45,500
According to our experiments, GE does not work on randomized skewed caches.

91
00:07:45,600 --> 00:07:49,500
CT incurs lower LLC evictions than the PPT algorithm.

92
00:07:49,600 --> 00:07:54,500
Therefore, the remap period should be estimated targeting the CT algorithm.

93
00:07:54,600 --> 00:08:00,500
Using the same method used by CEASER, this slice reveals the relationship

94
00:08:00,600 --> 00:08:05,500
between the remap period and the average time needed to find a small eviction set with the 30% eviction rate

95
00:08:05,600 --> 00:08:08,500
with different numbers of skew partitions.

96
00:08:08,600 --> 00:08:13,500
If we consider 1 year as a secure enough target,

97
00:08:13,600 --> 00:08:20,500
the non-skewed CEASER needs to be remapped every 10 evictions per cache block

98
00:08:20,600 --> 00:08:25,500
and even for the 16 skewed cache the remap period is only 39.

99
00:08:25,600 --> 00:08:29,500
The benefit from using extreme skewing is marginal.

100
00:08:29,600 --> 00:08:36,500
Then comes a question. The estimated remap period is seemingly much shorter

101
00:08:36,600 --> 00:08:41,500
than the 100 accesses per cache block preferred by CEASER. Is this a problem?

102
00:08:41,600 --> 00:08:44,500
Well, it may not be.

103
00:08:44,600 --> 00:08:50,500
So we put it into a comparison using the SPEC CPU 2006 benchmark.

104
00:08:50,600 --> 00:08:56,500
ACC-10 refers to remap every 10 accesses per cache block

105
00:08:56,600 --> 00:09:01,500
while EV-10 means remap every 10 evictions per cache block.

106
00:09:01,600 --> 00:09:05,500
The overhead in terms of miss rate caused by remapping

107
00:09:05,600 --> 00:09:11,500
is reduced by 87% if we count the remap period using LLC evictions.

108
00:09:11,600 --> 00:09:19,500
The overall overhead is a marginal 0.077% compared with no remap.

109
00:09:19,600 --> 00:09:22,500
The underlying reason here is:

110
00:09:22,600 --> 00:09:27,500
normal applications run at much low miss rate than attacks,

111
00:09:27,600 --> 00:09:34,500
so the remap frequency for normal applications is much lower, so does the performance overhead.

112
00:09:34,600 --> 00:09:38,500
Here shows a more detailed comparison between the two.

113
00:09:38,600 --> 00:09:44,500
This is a trace of the runtime miss rate extracted from running the 403.gcc benchmark case.

114
00:09:44,600 --> 00:09:48,500
In each trace, a vertical pink line indicates a remap.

115
00:09:48,600 --> 00:09:54,500
Note that the upper one is actually remapping every 100 accesses per block rather than 10

116
00:09:54,600 --> 00:09:57,500
but we already see the differences between the two.

117
00:09:57,600 --> 00:10:01,500
Now lets look at the periods inside the red circles.

118
00:10:01,600 --> 00:10:08,500
When the miss rate is low, the remaps triggered by remapping by evictions is significant less than remap by accesses,

119
00:10:08,600 --> 00:10:13,500
which indicates lower performance overhead when the LLC is efficiently used.

120
00:10:13,600 --> 00:10:17,500
While for the period in the blue circle, where the miss rate is high,

121
00:10:17,600 --> 00:10:20,500
remap by evictions does triggers more remaps.

122
00:10:20,600 --> 00:10:25,500
But the original cache performance with high miss rate is already suffered,

123
00:10:25,600 --> 00:10:31,500
more remaps during this period only marginally reduce the already low performance, not much a problem.

124
00:10:31,600 --> 00:10:35,500
To further reduce the cost involved in the remap,

125
00:10:35,600 --> 00:10:40,500
we propose to use a multi-step relocation scheme instead of the one-step relocation used in CEASER.

126
00:10:40,600 --> 00:10:49,500
According to our estimation, 40% to 50% of the cache blocks are evicted in CEASER every time there is a remap.

127
00:10:49,600 --> 00:10:57,500
When multi-step relocation is used, the remapping process will keep trying to relocate the cache block

128
00:10:57,600 --> 00:11:02,500
until a free location is found or a previous relocated block is evicted instead.

129
00:11:02,600 --> 00:11:08,500
Using multi-step relocation does not prolong the remap process and the introduced overhead

130
00:11:08,600 --> 00:11:15,500
is just a retrial using the new encryption key when using the old one results a miss during the remapping process.

131
00:11:15,600 --> 00:11:23,500
Considering remap is still an infrequent event, this cost in both latency and hardware is trivial.

132
00:11:23,600 --> 00:11:29,500
Finally we propose to trigger extra remap using attack detection.

133
00:11:29,600 --> 00:11:32,500
Instead of using randomized skewed caches,

134
00:11:32,600 --> 00:11:39,500
we actually prefer and promote the adoption of non-skewed randomized set-associative caches, such as CEASER.

135
00:11:39,600 --> 00:11:43,500
Skewed cache might bring some performance benefit

136
00:11:43,600 --> 00:11:50,500
but in reality, querying multiple cache sets with different cache set indices can

137
00:11:50,600 --> 00:11:54,500
substantially reduce the available bandwidth of the LLC through concurrent transactions

138
00:11:54,600 --> 00:11:59,500
or significantly increase the hardware complexity and prolong LLC's latency.

139
00:11:59,600 --> 00:12:07,500
We guess this is one of the reason why skewed caches have not been widely utilized in nowadays processors

140
00:12:07,600 --> 00:12:11,500
considering the claimed performance benefit.

141
00:12:11,600 --> 00:12:15,500
However, for a randomized set-associative cache,

142
00:12:15,600 --> 00:12:19,500
periodically remap stops only the CT algorithm.

143
00:12:19,600 --> 00:12:22,500
Both GE and PPT becomes viable again.

144
00:12:22,600 --> 00:12:28,500
Luckily, both algorithms incur abnormally high amount of evictions on the targeted cache set,

145
00:12:28,600 --> 00:12:30,500
which can be easily detected using a hardware detector.

146
00:12:30,600 --> 00:12:37,500
This slice shows the detection result for two PPT attacks.

147
00:12:37,600 --> 00:12:42,500
The hardware detector analyzes the distribution of cache events over the cache sets.

148
00:12:42,600 --> 00:12:48,500
As shown on the left hand side, the distributions for cache accesses and evictions look like noise.

149
00:12:48,600 --> 00:12:53,500
But if we apply a Z-Score standardization on them,

150
00:12:53,600 --> 00:12:59,500
as shown on the right hand side, two peaks clearly appears on the distribution of cache evictions.

151
00:12:59,600 --> 00:13:07,500
This is because multiple evictions are triggered inside the target cache set during the final test phase of the PPT algorithm.

152
00:13:07,600 --> 00:13:11,500
Similar peaks appear for the GE attack as well.

153
00:13:11,600 --> 00:13:16,500
When the large eviction set is effectively pruned to a small one,

154
00:13:16,600 --> 00:13:22,500
cache evictions triggered by traversing the eviction set naturally converge to the target set,

155
00:13:22,600 --> 00:13:28,500
so comes the highly unbalanced distribution caught by the Z-Score standardization.

156
00:13:28,600 --> 00:13:33,500
All right, now let us have a look of the overall performance.

157
00:13:33,600 --> 00:13:39,500
Graph on the left shows the number of remaps triggered by periodically remap and attack detection.

158
00:13:39,600 --> 00:13:46,500
It shows that applying attack detection leads to 26% extra remaps.

159
00:13:46,600 --> 00:13:51,500
But this cost can be efficiently alleviated by the multi-step relocation.

160
00:13:51,600 --> 00:13:54,500
As shown on the right hand side graph,

161
00:13:54,600 --> 00:14:01,500
when all techniques are applied to a non-skewed CEASER, the overhead in miss rate is a tiny 0.007%.

162
00:14:01,600 --> 00:14:07,500
On the contrary, when the maximal skew is applied, the overhead increases to 1.5% more misses.

163
00:14:07,600 --> 00:14:13,500
It is shown that randomized set-associative cache can be made secure enough with marginal performance cost,

164
00:14:13,600 --> 00:14:17,500
while using extreme skewing actually hurt the performance.

165
00:14:17,600 --> 00:14:21,500
OK. So four ideas are proposed in this paper:

166
00:14:21,600 --> 00:14:24,500
Remap by counting LLC evictions.

167
00:14:24,600 --> 00:14:28,500
Use multi-step relocation to reduce remap cost.

168
00:14:28,600 --> 00:14:30,500
Trigger remaps by attack detection.

169
00:14:30,600 --> 00:14:35,500
And promote the use of non-skewed set-associative caches.

170
00:14:35,600 --> 00:14:37,500
For more details, please read our paper.

171
00:14:37,600 --> 00:14:40,500
Thank you for your listening!

