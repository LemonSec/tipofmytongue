1
00:00:02,159 --> 00:00:03,679
hi everyone

2
00:00:03,679 --> 00:00:05,839
i am wang jung joo from university of

3
00:00:05,839 --> 00:00:07,600
illinois at the bandar champaign

4
00:00:07,600 --> 00:00:09,599
i am thrilled to be here to present our

5
00:00:09,599 --> 00:00:12,240
work self-supervised euphemism detection

6
00:00:12,240 --> 00:00:13,519
and identification for

7
00:00:13,519 --> 00:00:16,000
content moderation this work is done by

8
00:00:16,000 --> 00:00:16,560
myself

9
00:00:16,560 --> 00:00:19,680
hong kong rohan menzo zachary winberg

10
00:00:19,680 --> 00:00:22,800
nicola christine julia fenty and sumabat

11
00:00:22,800 --> 00:00:24,560
it is a collaborative work with

12
00:00:24,560 --> 00:00:26,080
university of illinois at abandoned

13
00:00:26,080 --> 00:00:27,039
campaign

14
00:00:27,039 --> 00:00:29,359
facebook carnegie mellon university and

15
00:00:29,359 --> 00:00:31,039
university of massachusetts

16
00:00:31,039 --> 00:00:34,960
immersed at the era of big data

17
00:00:34,960 --> 00:00:36,559
people make a growing amount of social

18
00:00:36,559 --> 00:00:38,079
media posts every day

19
00:00:38,079 --> 00:00:40,160
therefore it is critical to ensure that

20
00:00:40,160 --> 00:00:42,000
the cyber environment is safe and

21
00:00:42,000 --> 00:00:42,960
healthy

22
00:00:42,960 --> 00:00:45,039
one important task here is content

23
00:00:45,039 --> 00:00:46,000
moderation

24
00:00:46,000 --> 00:00:48,000
which is to provide conversations which

25
00:00:48,000 --> 00:00:49,920
are inappropriate on the social media

26
00:00:49,920 --> 00:00:52,239
platforms

27
00:00:52,239 --> 00:00:53,600
when you post on the social media

28
00:00:53,600 --> 00:00:55,440
platforms various

29
00:00:55,440 --> 00:00:57,920
rules and regulations or contains

30
00:00:57,920 --> 00:00:59,280
illegal content

31
00:00:59,280 --> 00:01:02,079
human moderators or automatic moderation

32
00:01:02,079 --> 00:01:03,280
could easily catch them

33
00:01:03,280 --> 00:01:05,438
and block the message however

34
00:01:05,438 --> 00:01:06,720
individuals and groups

35
00:01:06,720 --> 00:01:09,280
have a long history of using code names

36
00:01:09,280 --> 00:01:10,479
or euphemisms

37
00:01:10,479 --> 00:01:12,640
to conceal what they are discussing in

38
00:01:12,640 --> 00:01:14,720
order to evade the content moderation

39
00:01:14,720 --> 00:01:17,600
for example when they replace the drug

40
00:01:17,600 --> 00:01:19,119
cocaine to coat

41
00:01:19,119 --> 00:01:21,920
replace the pistol gun to get replace

42
00:01:21,920 --> 00:01:23,439
the prostitute to the girlfriend

43
00:01:23,439 --> 00:01:24,640
experience

44
00:01:24,640 --> 00:01:26,320
and therefore they will usually evade

45
00:01:26,320 --> 00:01:28,320
the content moderation

46
00:01:28,320 --> 00:01:32,720
existing tools existing tools typically

47
00:01:32,720 --> 00:01:34,479
make use of a band list but they are

48
00:01:34,479 --> 00:01:36,079
notoriously imprecise

49
00:01:36,079 --> 00:01:39,360
one reason is that what when a euphemism

50
00:01:39,360 --> 00:01:41,520
expanded the group could simply invent

51
00:01:41,520 --> 00:01:42,640
another one

52
00:01:42,640 --> 00:01:44,640
that would leave the content moderator

53
00:01:44,640 --> 00:01:46,240
one step behind

54
00:01:46,240 --> 00:01:47,920
the table here shows some involving

55
00:01:47,920 --> 00:01:50,720
euphemisms for some sensitive contents

56
00:01:50,720 --> 00:01:53,280
our work is to automatically detect and

57
00:01:53,280 --> 00:01:55,040
identify the euphemisms

58
00:01:55,040 --> 00:01:58,640
which facilitates contact moderation

59
00:01:58,640 --> 00:02:00,560
more formally we define the problem into

60
00:02:00,560 --> 00:02:01,920
two states

61
00:02:01,920 --> 00:02:04,719
one state is euphemism detection and the

62
00:02:04,719 --> 00:02:08,239
other state is euphemism identification

63
00:02:08,239 --> 00:02:11,520
your theme some detection is to detect

64
00:02:11,520 --> 00:02:13,360
all the code names or your theme cells

65
00:02:13,360 --> 00:02:15,360
that are used to replace the target

66
00:02:15,360 --> 00:02:16,720
keywords

67
00:02:16,720 --> 00:02:19,440
for example in the context of drug

68
00:02:19,440 --> 00:02:21,280
people often use

69
00:02:21,280 --> 00:02:23,760
they they cook words like weed coke

70
00:02:23,760 --> 00:02:24,560
blueberry

71
00:02:24,560 --> 00:02:26,640
and so on to refer to some drugs like

72
00:02:26,640 --> 00:02:29,280
marijuana cocaine and so on

73
00:02:29,280 --> 00:02:31,440
so the problem is that can we detect

74
00:02:31,440 --> 00:02:33,760
more euphemisms like those from the text

75
00:02:33,760 --> 00:02:37,440
corpus clone note that we expect the

76
00:02:37,440 --> 00:02:38,239
input to be

77
00:02:38,239 --> 00:02:40,560
only two information one is the lower

78
00:02:40,560 --> 00:02:41,440
tax covers

79
00:02:41,440 --> 00:02:42,959
where you would like to extract your

80
00:02:42,959 --> 00:02:45,040
themselves from

81
00:02:45,040 --> 00:02:47,680
one example is the social media post the

82
00:02:47,680 --> 00:02:48,239
other one

83
00:02:48,239 --> 00:02:50,160
the other part of information is your

84
00:02:50,160 --> 00:02:51,440
interested target

85
00:02:51,440 --> 00:02:54,720
keywords list so the other stage is the

86
00:02:54,720 --> 00:02:56,560
euphemism identification

87
00:02:56,560 --> 00:02:59,200
which is to identify what a euphemism

88
00:02:59,200 --> 00:03:01,120
refers to exactly

89
00:03:01,120 --> 00:03:03,840
give an example euphemism give give a

90
00:03:03,840 --> 00:03:05,200
euphemism we

91
00:03:05,200 --> 00:03:07,440
we would like to know that it refers to

92
00:03:07,440 --> 00:03:08,480
marijuana

93
00:03:08,480 --> 00:03:11,200
given another example euphemism of coke

94
00:03:11,200 --> 00:03:12,879
we would like to know that it refers to

95
00:03:12,879 --> 00:03:15,280
cocaine but not something else

96
00:03:15,280 --> 00:03:17,519
so the output from our algorithm is

97
00:03:17,519 --> 00:03:20,959
typically a probability distribution

98
00:03:20,959 --> 00:03:23,040
where where we would prefer to see the

99
00:03:23,040 --> 00:03:25,519
probability of marijuana is the highest

100
00:03:25,519 --> 00:03:28,640
given the euphemism we again we expect

101
00:03:28,640 --> 00:03:31,360
no additional information or resources

102
00:03:31,360 --> 00:03:33,920
required but only the text corpus and

103
00:03:33,920 --> 00:03:35,760
the euphemism

104
00:03:35,760 --> 00:03:37,920
now let us look into the first task

105
00:03:37,920 --> 00:03:41,440
which is euphemism detection

106
00:03:42,080 --> 00:03:44,640
so first uh let us see how previous work

107
00:03:44,640 --> 00:03:48,239
i've achieved on the euphemism detection

108
00:03:48,239 --> 00:03:50,640
so first we find almost all existing

109
00:03:50,640 --> 00:03:51,440
work require

110
00:03:51,440 --> 00:03:53,439
additional information as input which is

111
00:03:53,439 --> 00:03:54,879
marked on the left

112
00:03:54,879 --> 00:03:57,200
for example they may require a dark

113
00:03:57,200 --> 00:04:00,239
corpus where only euphemisms are used or

114
00:04:00,239 --> 00:04:01,200
they may require some

115
00:04:01,200 --> 00:04:03,599
additional human effort however such

116
00:04:03,599 --> 00:04:04,239
additional

117
00:04:04,239 --> 00:04:06,959
resources are very costly to obtain and

118
00:04:06,959 --> 00:04:07,519
that

119
00:04:07,519 --> 00:04:10,560
and are not often available instead

120
00:04:10,560 --> 00:04:12,239
we would like to propose a model that

121
00:04:12,239 --> 00:04:14,640
requires no additional resources

122
00:04:14,640 --> 00:04:17,199
on the other hand in terms of the nlp

123
00:04:17,199 --> 00:04:17,759
techniques

124
00:04:17,759 --> 00:04:21,120
shown on the right most existing work

125
00:04:21,120 --> 00:04:22,320
utilize the static

126
00:04:22,320 --> 00:04:25,360
volumet in particular they use word like

127
00:04:25,360 --> 00:04:26,320
to detect the

128
00:04:26,320 --> 00:04:28,560
euphemisms why would they fail to

129
00:04:28,560 --> 00:04:29,680
conquer the most

130
00:04:29,680 --> 00:04:31,680
important challenge of the task as i

131
00:04:31,680 --> 00:04:33,040
will show later

132
00:04:33,040 --> 00:04:35,759
so we resort to a newer class of

133
00:04:35,759 --> 00:04:38,080
inventing which is contextualizing value

134
00:04:38,080 --> 00:04:40,400
learned in the world model and we

135
00:04:40,400 --> 00:04:42,000
explicitly separate those

136
00:04:42,000 --> 00:04:45,600
contexts and make use of the context

137
00:04:45,600 --> 00:04:48,400
so compared with the prior work and our

138
00:04:48,400 --> 00:04:49,680
goal

139
00:04:49,680 --> 00:04:51,919
the task of euphemism detection has

140
00:04:51,919 --> 00:04:54,240
three main challenges

141
00:04:54,240 --> 00:04:56,639
so first how can make yours make use of

142
00:04:56,639 --> 00:04:58,400
no additional resources and all

143
00:04:58,400 --> 00:05:01,680
unlike all prayer works second how do we

144
00:05:01,680 --> 00:05:03,039
deal with these

145
00:05:03,039 --> 00:05:06,320
noisy contexts on the euphemisms

146
00:05:06,320 --> 00:05:08,800
so how do we deal with those contexts on

147
00:05:08,800 --> 00:05:10,479
both the euphemisms and the turret

148
00:05:10,479 --> 00:05:11,600
keywords

149
00:05:11,600 --> 00:05:14,320
so the noisy context on euphemisms means

150
00:05:14,320 --> 00:05:15,919
that a euphemism

151
00:05:15,919 --> 00:05:19,039
say coke or a target keyword herring

152
00:05:19,039 --> 00:05:22,240
could be used in either the informative

153
00:05:22,240 --> 00:05:25,520
sense or the if you uniform type says

154
00:05:25,520 --> 00:05:28,560
as shown in the table so therefore the

155
00:05:28,560 --> 00:05:30,160
word to act which learns all the

156
00:05:30,160 --> 00:05:32,800
embeddings of all contexts altogether

157
00:05:32,800 --> 00:05:35,039
fails to distinguish the differences

158
00:05:35,039 --> 00:05:36,880
between the informative context

159
00:05:36,880 --> 00:05:40,160
and the informative context the third

160
00:05:40,160 --> 00:05:41,680
challenge is how can we

161
00:05:41,680 --> 00:05:43,680
measure the similarities in the

162
00:05:43,680 --> 00:05:47,360
contextualized world inviting

163
00:05:47,360 --> 00:05:50,320
so to solve these three challenges we

164
00:05:50,320 --> 00:05:51,600
have proposed a

165
00:05:51,600 --> 00:05:55,120
new algorithm with three stages

166
00:05:55,120 --> 00:05:57,840
first to extract the context contextual

167
00:05:57,840 --> 00:05:58,960
information

168
00:05:58,960 --> 00:06:02,240
then we we do a filtering we want to

169
00:06:02,240 --> 00:06:04,080
filter out all the uninformative

170
00:06:04,080 --> 00:06:05,120
contexts

171
00:06:05,120 --> 00:06:08,240
and then lastly we generate candidates

172
00:06:08,240 --> 00:06:11,759
generate euphemism candidates so

173
00:06:11,759 --> 00:06:14,400
by the first step we do the context

174
00:06:14,400 --> 00:06:14,880
flags

175
00:06:14,880 --> 00:06:16,639
information extraction then we will have

176
00:06:16,639 --> 00:06:19,039
all the available context information

177
00:06:19,039 --> 00:06:21,680
related to the target keywords in this

178
00:06:21,680 --> 00:06:22,240
case

179
00:06:22,240 --> 00:06:25,280
all the drug related contexts

180
00:06:25,280 --> 00:06:27,600
however some of them may be noisy as we

181
00:06:27,600 --> 00:06:29,120
have seen previously

182
00:06:29,120 --> 00:06:31,280
some contacts are too general and they

183
00:06:31,280 --> 00:06:32,880
may bring in noise

184
00:06:32,880 --> 00:06:34,800
so our second step is to filter out

185
00:06:34,800 --> 00:06:37,120
those generic and uninformative

186
00:06:37,120 --> 00:06:39,759
context the last step is to generate the

187
00:06:39,759 --> 00:06:40,720
euphemisms

188
00:06:40,720 --> 00:06:44,800
based on the filter context information

189
00:06:44,800 --> 00:06:47,520
so let us look at the details of each

190
00:06:47,520 --> 00:06:48,960
step

191
00:06:48,960 --> 00:06:51,520
the first step is easy we will simply

192
00:06:51,520 --> 00:06:52,880
take all the sentences

193
00:06:52,880 --> 00:06:55,520
where all the where any of the target

194
00:06:55,520 --> 00:06:56,880
keywords appear

195
00:06:56,880 --> 00:06:58,319
and then we will remove the target

196
00:06:58,319 --> 00:07:00,160
keywords to form a

197
00:07:00,160 --> 00:07:03,440
masked sentence the entire collection of

198
00:07:03,440 --> 00:07:03,840
the

199
00:07:03,840 --> 00:07:06,000
masked sentences contains all the

200
00:07:06,000 --> 00:07:07,919
information related to the drug

201
00:07:07,919 --> 00:07:10,880
in the given corpus however we find the

202
00:07:10,880 --> 00:07:11,759
collection

203
00:07:11,759 --> 00:07:14,160
may contain some noise therefore the

204
00:07:14,160 --> 00:07:16,319
second step is to

205
00:07:16,319 --> 00:07:18,240
filter those uninformative mass

206
00:07:18,240 --> 00:07:21,199
sentences so to solve this problem

207
00:07:21,199 --> 00:07:24,319
we utilize a mask sentence a mask

208
00:07:24,319 --> 00:07:27,360
language model proposed in birth

209
00:07:27,360 --> 00:07:30,479
a mask calendar model aims to find the

210
00:07:30,479 --> 00:07:33,280
suitable replacement of the mask token

211
00:07:33,280 --> 00:07:34,479
we empirically

212
00:07:34,479 --> 00:07:36,639
observe that if the mask sentence is

213
00:07:36,639 --> 00:07:38,479
specific to drug

214
00:07:38,479 --> 00:07:40,960
then some particulars will appear at the

215
00:07:40,960 --> 00:07:43,280
front of the replacement output

216
00:07:43,280 --> 00:07:45,440
otherwise the replacement output will

217
00:07:45,440 --> 00:07:47,520
simply be some random words

218
00:07:47,520 --> 00:07:49,520
therefore we set a threshold based on

219
00:07:49,520 --> 00:07:51,039
the validation dataset

220
00:07:51,039 --> 00:07:53,199
and remove those uninformative masked

221
00:07:53,199 --> 00:07:54,800
sentences

222
00:07:54,800 --> 00:07:57,280
so now we have a pool of informative

223
00:07:57,280 --> 00:07:59,199
mask synthesis

224
00:07:59,199 --> 00:08:01,280
and we will generate euphemisms from

225
00:08:01,280 --> 00:08:04,080
there again

226
00:08:04,879 --> 00:08:07,199
so uh right now one more time we are

227
00:08:07,199 --> 00:08:08,720
using the mask sentence

228
00:08:08,720 --> 00:08:10,960
mask language model since we already

229
00:08:10,960 --> 00:08:12,479
know that a mask

230
00:08:12,479 --> 00:08:14,319
language model can find the replacement

231
00:08:14,319 --> 00:08:16,240
of the masker token

232
00:08:16,240 --> 00:08:19,759
now we will use all the informative mask

233
00:08:19,759 --> 00:08:20,639
synthesis

234
00:08:20,639 --> 00:08:22,479
fit them into the trained masculine

235
00:08:22,479 --> 00:08:25,280
model and the output

236
00:08:25,280 --> 00:08:28,319
excluding all those uh keywords are the

237
00:08:28,319 --> 00:08:30,240
final output of the detection

238
00:08:30,240 --> 00:08:33,200
euphemisms so to clarify we have used

239
00:08:33,200 --> 00:08:33,519
the

240
00:08:33,519 --> 00:08:35,839
mask language model twice once for

241
00:08:35,839 --> 00:08:37,519
filtering the informative muscle

242
00:08:37,519 --> 00:08:38,240
synthesis

243
00:08:38,240 --> 00:08:41,120
the other one at the second time uh for

244
00:08:41,120 --> 00:08:43,039
generating the euphemisms at a third

245
00:08:43,039 --> 00:08:45,360
step

246
00:08:46,800 --> 00:08:49,920
so let us um move on to the results

247
00:08:49,920 --> 00:08:51,839
as i have mentioned previously the

248
00:08:51,839 --> 00:08:54,080
technique is generally applicable to

249
00:08:54,080 --> 00:08:56,560
multiple domains and datasets we have

250
00:08:56,560 --> 00:08:58,640
experimented our algorithm

251
00:08:58,640 --> 00:09:01,120
on three differences including drug

252
00:09:01,120 --> 00:09:02,880
weapons and sexuality

253
00:09:02,880 --> 00:09:04,880
all the three categories and datasets

254
00:09:04,880 --> 00:09:07,040
are several cyber environment related

255
00:09:07,040 --> 00:09:08,000
categories

256
00:09:08,000 --> 00:09:10,959
in which people tend to create involving

257
00:09:10,959 --> 00:09:12,480
euphemisms

258
00:09:12,480 --> 00:09:14,399
and also to note all the data sets are

259
00:09:14,399 --> 00:09:15,920
real datasets from social media

260
00:09:15,920 --> 00:09:16,720
platforms

261
00:09:16,720 --> 00:09:19,360
such as reddit get twitter and some

262
00:09:19,360 --> 00:09:21,760
underground discussion forums

263
00:09:21,760 --> 00:09:23,760
the output of the algorithm is a long

264
00:09:23,760 --> 00:09:24,959
list where

265
00:09:24,959 --> 00:09:27,920
the more confident candidates appear at

266
00:09:27,920 --> 00:09:29,680
the front of the list

267
00:09:29,680 --> 00:09:31,760
so we use the preceding ik as the

268
00:09:31,760 --> 00:09:33,839
evaluation metric

269
00:09:33,839 --> 00:09:35,839
which is to see how many of them are

270
00:09:35,839 --> 00:09:37,839
correct

271
00:09:37,839 --> 00:09:41,279
of their first k detection results

272
00:09:41,279 --> 00:09:43,200
so all the results are summarized on the

273
00:09:43,200 --> 00:09:45,040
right the green correct take

274
00:09:45,040 --> 00:09:47,519
not our results and it clearly

275
00:09:47,519 --> 00:09:49,200
outperforms all the b slides by a

276
00:09:49,200 --> 00:09:51,200
significant margin

277
00:09:51,200 --> 00:09:55,120
so besides these quantitative analysis

278
00:09:55,120 --> 00:09:56,640
we also perform our quantitative

279
00:09:56,640 --> 00:09:57,440
analysis

280
00:09:57,440 --> 00:10:00,080
and we show that our proposed algorithm

281
00:10:00,080 --> 00:10:01,120
you will find some

282
00:10:01,120 --> 00:10:03,839
correct euphemisms that are not even on

283
00:10:03,839 --> 00:10:04,560
the ground

284
00:10:04,560 --> 00:10:07,920
based so this also suggests the rapid

285
00:10:07,920 --> 00:10:09,200
evolving nature

286
00:10:09,200 --> 00:10:11,839
of the euphemisms and the necessity of

287
00:10:11,839 --> 00:10:13,360
an automatic euphemism detection

288
00:10:13,360 --> 00:10:15,839
algorithm

289
00:10:16,160 --> 00:10:18,800
so we have already shown that algorithm

290
00:10:18,800 --> 00:10:20,399
our algorithm for detecting the

291
00:10:20,399 --> 00:10:21,360
differences

292
00:10:21,360 --> 00:10:23,440
quite without understanding what a

293
00:10:23,440 --> 00:10:25,440
euphemism is

294
00:10:25,440 --> 00:10:27,600
we are still less than behind of the

295
00:10:27,600 --> 00:10:29,200
content moderation

296
00:10:29,200 --> 00:10:31,200
so the next necessary step is to

297
00:10:31,200 --> 00:10:33,360
understand what the euphemism means

298
00:10:33,360 --> 00:10:36,399
by identifying which word that the

299
00:10:36,399 --> 00:10:40,399
euphemism refers to exactly

300
00:10:40,959 --> 00:10:44,839
so let us see again of the problem

301
00:10:44,839 --> 00:10:46,079
formulation

302
00:10:46,079 --> 00:10:49,040
time we need the input of the of a text

303
00:10:49,040 --> 00:10:50,640
corpus and a infinitive

304
00:10:50,640 --> 00:10:52,480
we would like to output a probability

305
00:10:52,480 --> 00:10:54,560
distribution of the target keywords it

306
00:10:54,560 --> 00:10:55,760
refers to

307
00:10:55,760 --> 00:10:58,560
take weed as an example we expect the

308
00:10:58,560 --> 00:11:00,079
probability of marijuana is

309
00:11:00,079 --> 00:11:03,279
highest take coke as another example

310
00:11:03,279 --> 00:11:05,440
expect the probability of cocaine

311
00:11:05,440 --> 00:11:08,480
is the largest so um

312
00:11:08,480 --> 00:11:11,040
the task of euphemism identification is

313
00:11:11,040 --> 00:11:12,240
considerably

314
00:11:12,240 --> 00:11:14,720
considerably challenging so to the best

315
00:11:14,720 --> 00:11:17,040
of our knowledge no prior work

316
00:11:17,040 --> 00:11:19,839
has successfully achieved this task the

317
00:11:19,839 --> 00:11:20,480
techno

318
00:11:20,480 --> 00:11:22,160
technical challenge lies in two of

319
00:11:22,160 --> 00:11:24,320
two-fold the first is the resource

320
00:11:24,320 --> 00:11:25,040
challenge

321
00:11:25,040 --> 00:11:28,000
the second is the linguistic challenge

322
00:11:28,000 --> 00:11:29,760
as of the resource challenge since

323
00:11:29,760 --> 00:11:32,160
it is an entirely new task we have no

324
00:11:32,160 --> 00:11:33,360
available data sets

325
00:11:33,360 --> 00:11:35,680
and also it is even unclear what kind of

326
00:11:35,680 --> 00:11:37,360
resources we need

327
00:11:37,360 --> 00:11:40,000
as of the linguistic challenge we find

328
00:11:40,000 --> 00:11:40,720
the

329
00:11:40,720 --> 00:11:43,200
target keywords the differences between

330
00:11:43,200 --> 00:11:44,959
the target keywords are often very very

331
00:11:44,959 --> 00:11:48,160
subtle it is hard to capture of them

332
00:11:48,160 --> 00:11:51,279
and for any of the euphemisms

333
00:11:51,279 --> 00:11:53,200
they call they can be either used

334
00:11:53,200 --> 00:11:54,880
immediately euphemism status which is

335
00:11:54,880 --> 00:11:55,600
the direction

336
00:11:55,600 --> 00:11:57,920
or the niger feministic sense which is

337
00:11:57,920 --> 00:11:59,200
the beverages

338
00:11:59,200 --> 00:12:01,760
which adding an extra layer of the

339
00:12:01,760 --> 00:12:03,200
inquisition

340
00:12:03,200 --> 00:12:06,959
so then we propose uh to use two of the

341
00:12:06,959 --> 00:12:08,399
techniques to solve each of the

342
00:12:08,399 --> 00:12:10,560
challenge we

343
00:12:10,560 --> 00:12:12,480
we propose to use the self-supervised

344
00:12:12,480 --> 00:12:13,760
learning to tackle the resource

345
00:12:13,760 --> 00:12:14,480
challenge and

346
00:12:14,480 --> 00:12:16,560
we propose to use the course to fund

347
00:12:16,560 --> 00:12:18,160
great classification algorithm

348
00:12:18,160 --> 00:12:20,880
to solve the linguistic challenge so

349
00:12:20,880 --> 00:12:22,079
first let us see

350
00:12:22,079 --> 00:12:24,959
how we how it is possible to curate a

351
00:12:24,959 --> 00:12:28,880
data set by self-supervised learning

352
00:12:29,519 --> 00:12:32,560
so we extract all the sentences that

353
00:12:32,560 --> 00:12:34,959
exclude the target keywords

354
00:12:34,959 --> 00:12:37,600
uh the marijuana hiring ecstasy answer

355
00:12:37,600 --> 00:12:38,000
mask

356
00:12:38,000 --> 00:12:40,720
the target keywords now we consider the

357
00:12:40,720 --> 00:12:42,880
mask sentences as streaming example

358
00:12:42,880 --> 00:12:45,120
and they target keywords as labels so

359
00:12:45,120 --> 00:12:47,200
then this will naturally form

360
00:12:47,200 --> 00:12:51,279
a new data set where the inputs are the

361
00:12:51,279 --> 00:12:53,920
those mask sentences and the respective

362
00:12:53,920 --> 00:12:55,920
labels are these target keywords

363
00:12:55,920 --> 00:12:57,760
by learning from such a data set we will

364
00:12:57,760 --> 00:12:58,959
already

365
00:12:58,959 --> 00:13:02,079
um we can already form a multi-class

366
00:13:02,079 --> 00:13:02,880
classifier

367
00:13:02,880 --> 00:13:06,480
to determine uh which target keywords

368
00:13:06,480 --> 00:13:09,680
this mask sentence refers to

369
00:13:09,680 --> 00:13:13,600
so um how about the linguistic challenge

370
00:13:13,600 --> 00:13:15,519
how do we want to enlarge the subtle

371
00:13:15,519 --> 00:13:17,839
differences between euphemisms

372
00:13:17,839 --> 00:13:20,079
now we have seen a euphemism called

373
00:13:20,079 --> 00:13:22,160
could be either used in the

374
00:13:22,160 --> 00:13:24,480
euphemistic sense or the ignitionistic

375
00:13:24,480 --> 00:13:26,079
sense

376
00:13:26,079 --> 00:13:27,600
now how do you know which sense it is

377
00:13:27,600 --> 00:13:30,160
using and how do we remove them

378
00:13:30,160 --> 00:13:31,839
now we construct another course

379
00:13:31,839 --> 00:13:33,360
classification classifier to make the

380
00:13:33,360 --> 00:13:34,320
decision

381
00:13:34,320 --> 00:13:36,160
the goal of the such a crossfire is to

382
00:13:36,160 --> 00:13:38,399
distinguish whether the sentence the

383
00:13:38,399 --> 00:13:40,160
masculine sentence is using in the

384
00:13:40,160 --> 00:13:44,079
infinite sense or not so we

385
00:13:44,079 --> 00:13:47,120
construct a pool of positive examples

386
00:13:47,120 --> 00:13:48,959
and negative examples

387
00:13:48,959 --> 00:13:51,680
so to obtain the positive instances we

388
00:13:51,680 --> 00:13:52,320
just

389
00:13:52,320 --> 00:13:55,360
use all the sentences on the left

390
00:13:55,360 --> 00:13:58,000
and for the negative sentences we adopt

391
00:13:58,000 --> 00:13:59,600
a negative sampling approach

392
00:13:59,600 --> 00:14:02,000
we randomly chose a sentence in a corpus

393
00:14:02,000 --> 00:14:04,240
and randomly mask a sentence

394
00:14:04,240 --> 00:14:06,320
and we consider that as a negative

395
00:14:06,320 --> 00:14:07,680
example

396
00:14:07,680 --> 00:14:10,800
so right now we have two classifiers

397
00:14:10,800 --> 00:14:12,240
one classifier is the multi-class

398
00:14:12,240 --> 00:14:14,160
classification to know which

399
00:14:14,160 --> 00:14:16,959
drug which target keyword it refers to

400
00:14:16,959 --> 00:14:18,399
another one is to see

401
00:14:18,399 --> 00:14:20,399
whether the mask synthesis is drug

402
00:14:20,399 --> 00:14:22,480
related or not

403
00:14:22,480 --> 00:14:25,120
so let's um have an overview of the

404
00:14:25,120 --> 00:14:26,160
model

405
00:14:26,160 --> 00:14:29,040
take weed as an example we aim to

406
00:14:29,040 --> 00:14:30,959
generate a probability distribution

407
00:14:30,959 --> 00:14:33,199
where marijuana is highest

408
00:14:33,199 --> 00:14:35,680
assume we already have a trained course

409
00:14:35,680 --> 00:14:37,760
classifier and they trained

410
00:14:37,760 --> 00:14:40,399
a fine grain classifier we first extract

411
00:14:40,399 --> 00:14:40,959
all the mass

412
00:14:40,959 --> 00:14:43,360
sentences that contain weed from the

413
00:14:43,360 --> 00:14:44,560
text covers

414
00:14:44,560 --> 00:14:46,880
and then using the coarse classifier we

415
00:14:46,880 --> 00:14:48,560
filter out the mask sentences

416
00:14:48,560 --> 00:14:51,839
they are not related to straw and then

417
00:14:51,839 --> 00:14:55,199
we will use the uh filter mask synthesis

418
00:14:55,199 --> 00:14:56,399
as input to the

419
00:14:56,399 --> 00:14:59,040
fun green multi-class classifier and

420
00:14:59,040 --> 00:15:01,199
obtain the target keywords

421
00:15:01,199 --> 00:15:03,839
labels for each of the math sentences so

422
00:15:03,839 --> 00:15:04,880
see now we have

423
00:15:04,880 --> 00:15:08,560
um 45 000 marijuana labels and 5 000

424
00:15:08,560 --> 00:15:10,959
excellency levels and then the final

425
00:15:10,959 --> 00:15:11,680
output

426
00:15:11,680 --> 00:15:14,959
of weight is just a probability

427
00:15:14,959 --> 00:15:17,199
distribution by the number of labels for

428
00:15:17,199 --> 00:15:20,399
each target keywords

429
00:15:20,959 --> 00:15:22,959
so to summarize we have tackled the

430
00:15:22,959 --> 00:15:25,040
resource challenge by a self-supervised

431
00:15:25,040 --> 00:15:26,000
algorithm that can

432
00:15:26,000 --> 00:15:28,800
curate the data set by itself so besides

433
00:15:28,800 --> 00:15:29,440
the

434
00:15:29,440 --> 00:15:31,519
linguistic challenge are solved by a

435
00:15:31,519 --> 00:15:33,279
course to find great classification

436
00:15:33,279 --> 00:15:34,079
scheme

437
00:15:34,079 --> 00:15:36,240
such hierarchical architecture enlarges

438
00:15:36,240 --> 00:15:37,680
the differences between

439
00:15:37,680 --> 00:15:40,240
the target keywords and has shown better

440
00:15:40,240 --> 00:15:43,360
discriminative performance

441
00:15:43,360 --> 00:15:45,759
so since there is no previous work for

442
00:15:45,759 --> 00:15:47,839
such a task we compare

443
00:15:47,839 --> 00:15:50,880
our approach with a with a few simple

444
00:15:50,880 --> 00:15:54,000
baselines and evolution studies we

445
00:15:54,000 --> 00:15:54,959
evaluate our

446
00:15:54,959 --> 00:15:57,440
algorithm based on the accuracy okay

447
00:15:57,440 --> 00:15:59,040
which is to see whether the correct

448
00:15:59,040 --> 00:16:00,000
answer is

449
00:16:00,000 --> 00:16:02,959
in a top key precision or not we show

450
00:16:02,959 --> 00:16:03,839
that our

451
00:16:03,839 --> 00:16:07,040
our algorithm outperforms the our

452
00:16:07,040 --> 00:16:08,560
crafted evolution studies

453
00:16:08,560 --> 00:16:11,839
basilized by a large margin we refer the

454
00:16:11,839 --> 00:16:13,920
readers to the paper for more details

455
00:16:13,920 --> 00:16:17,040
on the baseline okay thank you very much

456
00:16:17,040 --> 00:16:18,000
for watching

457
00:16:18,000 --> 00:16:19,759
make sure to check out our paper and the

458
00:16:19,759 --> 00:16:22,000
open source code at github any

459
00:16:22,000 --> 00:16:23,360
so so that you can detect your

460
00:16:23,360 --> 00:16:27,519
chromosomes on your own datasets

