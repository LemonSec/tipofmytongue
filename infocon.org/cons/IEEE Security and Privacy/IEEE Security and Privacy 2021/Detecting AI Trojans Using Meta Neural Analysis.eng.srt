1
00:00:01,160 --> 00:00:04,560
Hello everyone, I am Xiaojun. I am going to

2
00:00:04,560 --> 00:00:08,039
introduce our research on AI Trojan detection

3
00:00:08,590 --> 00:00:10,840
using meta neural analysis.

4
00:00:10,840 --> 00:00:15,000
We know that nowadays ML models have been

5
00:00:15,000 --> 00:00:16,480
applied in different tasks.

6
00:00:16,660 --> 00:00:21,080
But it is not easy to train a machine learning model.

7
00:00:21,080 --> 00:00:26,000
Therefore, a convenient way is that we have expert model producers

8
00:00:26,000 --> 00:00:30,800
to train ML models and share it with consumers.

9
00:00:30,800 --> 00:00:34,440
For producers, it allows them to share the

10
00:00:34,440 --> 00:00:38,739
service without sharing the dataset, which may be confidential.

11
00:00:38,740 --> 00:00:43,000
For consumers, it saves their resources and expertise

12
00:00:43,000 --> 00:00:47,810
required to train a good machine learning model.

13
00:00:47,810 --> 00:00:53,950
But there are integrity threats in sharing
the models if the model producer is untrusted.

14
00:00:53,950 --> 00:00:57,160
A well-known threat is the Trojan attack,

15
00:00:57,160 --> 00:01:01,529
where the producer will share a malicious model.

16
00:01:01,530 --> 00:01:06,500
This trojaned model will produces correct
results on normal inputs.

17
00:01:06,500 --> 00:01:12,760
However, on inputs with a trigger, the model
will produce malicious results.

18
00:01:12,760 --> 00:01:15,200
In this case, the trigger is a sticker

19
00:01:15,200 --> 00:01:21,200
and the model will recognize the stop sign as a speed limit sign.

20
00:01:21,230 --> 00:01:25,680
This attack is not only dangerous but also stealthy because

21
00:01:25,680 --> 00:01:29,000
the trojaned model is similar with the benign one on most inputs.

22
00:01:29,000 --> 00:01:33,280
So it is difficult to detect it.

23
00:01:33,280 --> 00:01:35,026
Previous researches have proposed

24
00:01:35,026 --> 00:01:38,479
different types of trojan attacks.

25
00:01:38,790 --> 00:01:42,520
These two attacks are based on poisoning attack.

26
00:01:42,520 --> 00:01:46,399
The modification attack will do pixel modification

27
00:01:46,430 --> 00:01:50,720
and the blending will blend an image pattern.

28
00:01:50,720 --> 00:01:55,950
There are also attacks without poisoning the training set.

29
00:01:55,950 --> 00:02:00,000
These two attacks will generate an optimized trigger pattern.

30
00:02:00,000 --> 00:02:03,000
The parameter attack will fine-tune a trained model

31
00:02:03,680 --> 00:02:08,500
while the latent attack will train a model from scratch.

32
00:02:08,500 --> 00:02:14,200
And note that these attacks originally 
focus on image classification tasks

33
00:02:14,200 --> 00:02:19,369
but also can be adapted to other fields.

34
00:02:19,370 --> 00:02:22,500
We see that Trojan attacks are dangerous and stealthy.

35
00:02:22,500 --> 00:02:25,600
So our goal is to detect these trojaned models.

36
00:02:25,600 --> 00:02:28,600
Here is our setting.

37
00:02:28,600 --> 00:02:34,190
For the attacker, he will train a Trojaned
ML model and share it with others.

38
00:02:34,190 --> 00:02:41,090
We assume that he has full access to the training
data and the training process, so that he

39
00:02:41,090 --> 00:02:43,440
can perform arbitrary attack as he wants.

40
00:02:43,440 --> 00:02:47,920
For the defender, we are given a model, and

41
00:02:47,950 --> 00:02:52,269
our goal is to determine whether it is Trojaned or not

42
00:02:52,270 --> 00:02:55,000
We assume that we do not know the attack approach

43
00:02:55,000 --> 00:02:57,000
nor the training data.

44
00:02:57,690 --> 00:03:00,000
And we only have black-box access to the model,

45
00:03:00,000 --> 00:03:05,000
which means that we can only query the model

46
00:03:05,000 --> 00:03:08,000
and get the confidence score of the prediction.

47
00:03:08,000 --> 00:03:12,000
We do assume that we need a small set of clean data

48
00:03:12,400 --> 00:03:17,240
which follows the same distribution
of the training set of given model.

49
00:03:17,240 --> 00:03:24,460
This setting is also assumed in other trojan detection.

50
00:03:24,460 --> 00:03:27,000
Now we will introduce our approach.

51
00:03:27,000 --> 00:03:32,000
Our pipeline is based on the idea of meta neural analysis.

52
00:03:32,000 --> 00:03:36,700
It will train a meta-classifier over NNs.

53
00:03:36,700 --> 00:03:43,679
This means that the meta-classifier will take
a NN as input, and output a prediction of

54
00:03:43,680 --> 00:03:47,000
certain property of the input NN.

55
00:03:47,000 --> 00:03:51,000
MNA usually consists of three steps:

56
00:03:51,000 --> 00:03:54,000
First, it will train a set of shadow models

57
00:03:54,000 --> 00:03:57,000
with and without certain property.

58
00:03:57,000 --> 00:04:02,000
Second, it will use the shadow 
models to train the meta-classifier.

59
00:04:02,000 --> 00:04:06,360
And finally, it uses the meta-classifier

60
00:04:06,360 --> 00:04:10,000
to predict the property of the target model.

61
00:04:10,820 --> 00:04:14,359
It is natural to apply such pipeline to detect Trojans.

62
00:04:14,360 --> 00:04:17,680
We also have three steps in our pipeline.

63
00:04:17,709 --> 00:04:23,090
First, we will generate a set of benign and
trojaned shadow models.

64
00:04:23,090 --> 00:04:30,599
Next, we will train a meta-classifier to distinguish
between benign and trojaned models.

65
00:04:30,600 --> 00:04:37,210
Finally, we apply the meta-classifier to the target model.

66
00:04:37,210 --> 00:04:40,280
In the pipeline, we have two major questions:

67
00:04:40,280 --> 00:04:43,280
how to generate a good set of trojaned models

68
00:04:43,490 --> 00:04:47,400
for training, and how to train a good meta-classifier

69
00:04:47,400 --> 00:04:51,400
to distinguish trojaned and benign models.

70
00:04:51,430 --> 00:04:55,820
We are going to introduce them in detail.

71
00:04:55,820 --> 00:05:00,120
In the first step, we will generate benign
and trojaned shadow models.

72
00:05:00,120 --> 00:05:04,000
We hope that the shadow models are diverse,

73
00:05:04,000 --> 00:05:06,760
so that the meta-classifier trained on these

74
00:05:06,770 --> 00:05:09,770
shadow models can generalize well.

75
00:05:09,770 --> 00:05:15,070
Therefore, we propose jumbo learning to generate
various Trojaned models.

76
00:05:15,070 --> 00:05:20,630
We will first sample different trigger patterns
and malicious behavior.

77
00:05:20,630 --> 00:05:24,719
Then we use poisoning attack to generate Trojaned models.

78
00:05:24,720 --> 00:05:27,520
That is, we will inject the data with

79
00:05:27,540 --> 00:05:32,280
sampled pattern and behavior into the training set.

80
00:05:32,280 --> 00:05:35,940
And these are examples of the generated trigger patterns.

81
00:05:35,940 --> 00:05:44,320
To do the sampling, we use four parameters
to parametrize a Trojan - the trigger mask,

82
00:05:44,320 --> 00:05:49,250
the pattern, the transparency and the target
malicious behavior.

83
00:05:49,250 --> 00:05:54,000
Here the target malicious behavior is the
desired output by the attacker.

84
00:05:54,000 --> 00:05:59,000
For example, in the traffic sign case
the target is the speed limit sign.

85
00:05:59,000 --> 00:06:04,000
In this digit case it is a specific digit for output.

86
00:06:04,000 --> 00:06:07,810
Having the trojan setting we can generate

87
00:06:07,810 --> 00:06:13,800
the corresponding poisoned dataset
and train the shadow models.

88
00:06:13,800 --> 00:06:19,960
Having the shadow models, the second step
is to train a meta-classifier to distinguish

89
00:06:19,960 --> 00:06:23,719
between benign and trojaned models.

90
00:06:23,720 --> 00:06:31,770
The main problem is how to feed the NN
as an input to meta-classifier.

91
00:06:31,770 --> 00:06:40,000
What we want here is actually a feature extraction
function to transform the NN into a numerical vector.

92
00:06:40,000 --> 00:06:43,890
Our design is to find a set of queries, and

93
00:06:43,890 --> 00:06:48,000
use the output of the NN as the feature vector.

94
00:06:48,000 --> 00:06:52,000
We will concatenate the output of the model

95
00:06:52,270 --> 00:06:55,900
and get a feature vector.

96
00:06:55,900 --> 00:07:02,099
Having this feature vector for a NN, we will
use a 2-layer NN to determine whether the

97
00:07:02,100 --> 00:07:04,600
model is benign or Trojaned.

98
00:07:04,600 --> 00:07:09,000
Note that there may be other design of feature vectors

99
00:07:09,000 --> 00:07:13,669
For example we can directly use the
model parameters as the feature representation.

100
00:07:13,669 --> 00:07:18,599
Our intuition here to use this feature vector is

101
00:07:18,600 --> 00:07:21,600
that Trojan attack is characterized by its

102
00:07:21,850 --> 00:07:25,240
abnormal output on certain inputs.

103
00:07:25,240 --> 00:07:28,800
And therefore we want to use the model outputs

104
00:07:28,800 --> 00:07:32,880
on some queries as its feature.

105
00:07:32,880 --> 00:07:38,360
So now, the key problem is:
how do we choose this query set?

106
00:07:38,360 --> 00:07:41,400
A simple way is to randomly choose them.

107
00:07:41,400 --> 00:07:47,400
Then, we can optimize a loss function w.r.t. the model parameter.

108
00:07:47,840 --> 00:07:53,000
However, we find a better approach and we call it query tuning.

109
00:07:53,000 --> 00:07:56,500
We find that we can optimize the goal w.r.t.

110
00:07:56,500 --> 00:08:00,440
both the parameters and the query set.

111
00:08:00,440 --> 00:08:03,320
And since the overall calculation flow is

112
00:08:03,340 --> 00:08:07,479
differentiable from the optimization goal to the query

113
00:08:07,480 --> 00:08:11,000
we can use the same gradient descent technique to

114
00:08:11,000 --> 00:08:15,370
simultaneously train the model and tune the queries.

115
00:08:15,370 --> 00:08:21,800
This helps us to find the most useful queries
for the prediction.

116
00:08:21,800 --> 00:08:24,000
We also propose another baseline method

117
00:08:24,000 --> 00:08:27,000
to train the meta-classifier.

118
00:08:27,039 --> 00:08:34,110
Suppose we do not want this jumbo learning
strategy and only have benign shadow models.

119
00:08:34,110 --> 00:08:40,659
Then we want to fit the meta-classifier
using only benign models.

120
00:08:40,659 --> 00:08:48,000
And this is the novelty detection setting and
we can apply the technique such as one-class SVM.

121
00:08:48,000 --> 00:08:55,000
In practice we used a variant, the one-class
neural network as the training algorithm.

122
00:08:55,000 --> 00:08:58,649
Using this approach, we can train the meta-classifier

123
00:08:58,649 --> 00:09:02,640
using only benign shadow models.

124
00:09:04,660 --> 00:09:09,410
After we train the meta classifier, we can
apply it to the target model.

125
00:09:09,410 --> 00:09:13,148
We will feed the query set into the target model

126
00:09:13,148 --> 00:09:15,040
to get the feature vector.

127
00:09:15,040 --> 00:09:18,640
Then we use the meta-classifier to determine

128
00:09:18,640 --> 00:09:22,640
whether the target model is Trojaned or not.

129
00:09:24,649 --> 00:09:28,400
We conduct the experiment on a variety of tasks,

130
00:09:28,400 --> 00:09:31,199
including computer vision, speech,

131
00:09:31,200 --> 00:09:33,200
energy data and natural language.

132
00:09:33,200 --> 00:09:38,200
And we compare with many previous works in detecting Trojans.

133
00:09:38,200 --> 00:09:42,819
We show that our approach with jumbo learning

134
00:09:42,819 --> 00:09:46,478
and query tuning, i.e., the pink bar

135
00:09:46,478 --> 00:09:49,479
achieves the best result on most tasks

136
00:09:49,480 --> 00:09:52,680
for modification attack and blending attack

137
00:09:52,680 --> 00:09:58,000
which denoted by the M and the B here.

138
00:09:58,000 --> 00:10:03,100
The patterns in these attacks are modelled
by our jumbo distribution, and we are also

139
00:10:03,100 --> 00:10:10,540
interested in how the model performs on the
patterns that are not modelled in jumbo learning.

140
00:10:10,540 --> 00:10:18,610
Therefore, we designed these attacks,
which is completely out of our jumbo distribution.

141
00:10:18,610 --> 00:10:22,000
We apply these patterns and perform the poisoning attack

142
00:10:22,000 --> 00:10:26,000
and use our pipeline to detect it.

143
00:10:26,050 --> 00:10:32,920
And we can observe here that the model generalizes well
even if the pattern is out of distribution.

144
00:10:33,950 --> 00:10:40,509
Then, we consider the evaluation for unforeseen attacks.

145
00:10:40,509 --> 00:10:44,680
Recall that we use poisoning attack
to generate shadow models

146
00:10:44,680 --> 00:10:49,280
and these are two attacks that will not poison the dataset.

147
00:10:49,640 --> 00:10:52,410
So we will evaluate whether the system can

148
00:10:52,410 --> 00:10:55,559
detect these attacks.

149
00:10:55,559 --> 00:11:02,889
We can see that our approach still performs
very well in detecting these unforeseen attacks.

150
00:11:02,889 --> 00:11:08,350
Note that some other detection methods only
detect the dataset-level trojans, so they

151
00:11:08,350 --> 00:11:14,200
cannot detect the attacks because these attacks
do not interfere with the dataset.

152
00:11:16,209 --> 00:11:20,979
We also consider another attack goal
named all-to-all attack.

153
00:11:20,980 --> 00:11:24,520
In previous discussion we all used single-target attack.

154
00:11:24,520 --> 00:11:28,360
That is, the model gives the same 
output on seeing the trigger pattern.

155
00:11:29,000 --> 00:11:36,339
While in all-to-all attack, the trojaned model will
rotate its prediction when it sees the trigger pattern.

156
00:11:36,339 --> 00:11:45,639
For example, on an image of zero with trigger it says one,
and on an image of one with trigger it says two.

157
00:11:45,639 --> 00:11:52,000
We apply our system to see if it can detect Trojaned 
models with this type of unforeseen attack goal.

158
00:11:52,840 --> 00:11:55,660
We see that our approach still performs well

159
00:11:55,660 --> 00:12:00,399
and outperforms previous works.

160
00:12:00,399 --> 00:12:02,400
And we also compare the time efficiency

161
00:12:02,400 --> 00:12:05,959
of our pipeline and previous works.

162
00:12:05,980 --> 00:12:09,000
Recall that we have three steps in our system.

163
00:12:09,000 --> 00:12:12,560
The first two steps to generate shadow models

164
00:12:12,560 --> 00:12:16,000
and to train the system will take a long time.

165
00:12:16,000 --> 00:12:19,000
But they are offline preparation because we

166
00:12:19,029 --> 00:12:23,000
only need to do it once for one dataset.

167
00:12:23,000 --> 00:12:26,240
In the online part, given a target model,

168
00:12:26,259 --> 00:12:29,000
we only need to feed in the queries and

169
00:12:29,000 --> 00:12:33,480
go through the model and the meta-classifier only once.

170
00:12:33,480 --> 00:12:36,280
So the online detection takes only several

171
00:12:36,309 --> 00:12:39,920
milliseconds for one target model.

172
00:12:39,920 --> 00:12:42,959
As a comparison, the previous methods need

173
00:12:42,990 --> 00:12:46,320
to rerun their entire pipeline whenever they get

174
00:12:46,320 --> 00:12:51,200
a new target model, which takes at least 20 seconds.

175
00:12:52,749 --> 00:13:00,339
Finally, we consider the adaptive attack,
i.e. what will the attacker do if he knows

176
00:13:00,339 --> 00:13:04,000
our algorithm, and what we should do as countermeasure.

177
00:13:04,000 --> 00:13:07,200
We find that the attacker can intentionally

178
00:13:07,239 --> 00:13:12,619
bypass our detection model if he has full
knowledge of our system.

179
00:13:12,619 --> 00:13:16,129
His success rate would be very high if we do nothing.

180
00:13:16,129 --> 00:13:23,910
Therefore, we would like to propose a robust
version against the adaptive attack.

181
00:13:23,910 --> 00:13:29,880
Our intuition is to incorporate randomness in the algorithm

182
00:13:29,880 --> 00:13:32,880
because the attacker cannot control randomness.

183
00:13:32,910 --> 00:13:40,420
So In our robust version, we will use a randomly
sampled meta-classifier and keep it unchanged.

184
00:13:40,420 --> 00:13:46,128
Then we only fine-tune the query set w.r.t.
the random classifier

185
00:13:46,129 --> 00:13:53,860
Finally we will use the fine-tuned query set together
with the random classifier to detect the Trojans.

186
00:13:53,860 --> 00:14:02,859
Since the attacker cannot know the random meta-classifier,
it will be hard for him to perform the adaptive attack.

187
00:14:02,859 --> 00:14:06,400
Here is the result of the adaptive attack.

188
00:14:06,400 --> 00:14:12,379
The blue bars are the robust result before attack
while the orange bars are after the attack.

189
00:14:12,379 --> 00:14:18,589
We can see that the attacker can still manage
to reduce the chance of being detected, but

190
00:14:18,589 --> 00:14:23,300
the margin is smaller now.

191
00:14:23,300 --> 00:14:26,870
That's all for our project. 
Here are the take away points.

192
00:14:26,870 --> 00:14:30,560
First, we show that meta neural analysis achieves

193
00:14:30,560 --> 00:14:35,640
a good performance in the detection of Trojaned ML models.

194
00:14:35,640 --> 00:14:40,441
In addition, such system is very efficient at the inference stage.

195
00:14:40,441 --> 00:14:46,400
Finally, we propose a robust version of our system which can

196
00:14:46,400 --> 00:14:53,000
detect Trojans even if the attacker knows our detection algorithm.

197
00:14:53,069 --> 00:14:57,000
For more details, you can check our paper
and open-sourced code here.

198
00:14:57,000 --> 00:14:58,819
Thanks for listening.

