1
00:00:00,000 --> 00:00:04,589
to speak about<font color="#E5E5E5"> inferring user routes and</font>

2
00:00:02,310 --> 00:00:08,580
locations using<font color="#CCCCCC"> zero permission mobile</font>

3
00:00:04,589 --> 00:00:10,740
sensors the leakage of location

4
00:00:08,580 --> 00:00:14,070
information is considered<font color="#E5E5E5"> as a major</font>

5
00:00:10,740 --> 00:00:16,560
privacy concern mobile operating systems

6
00:00:14,070 --> 00:00:18,510
like<font color="#E5E5E5"> Android and iOS have tried</font><font color="#CCCCCC"> to</font>

7
00:00:16,560 --> 00:00:21,859
address this concern by implementing

8
00:00:18,510 --> 00:00:26,130
permissions though these permissions are

9
00:00:21,859 --> 00:00:29,070
successful in mitigating some privacy

10
00:00:26,130 --> 00:00:32,610
leakage there are still apps like the

11
00:00:29,070 --> 00:00:37,110
brightest flashlight app that deceive

12
00:00:32,610 --> 00:00:39,000
users so this app was sued actually<font color="#E5E5E5"> some</font>

13
00:00:37,110 --> 00:00:42,030
<font color="#E5E5E5">time back for selling location</font>

14
00:00:39,000 --> 00:00:44,840
information without telling the users

15
00:00:42,030 --> 00:00:49,370
about that they sell their your

16
00:00:44,840 --> 00:00:51,629
information a bigger problem however is

17
00:00:49,370 --> 00:00:55,230
protecting the leakage of location

18
00:00:51,629 --> 00:00:58,589
information from<font color="#E5E5E5"> site channels such as</font>

19
00:00:55,230 --> 00:01:00,750
sensors the reason is that these sensors

20
00:00:58,590 --> 00:01:03,030
are not protected by any kind<font color="#E5E5E5"> of</font>

21
00:01:00,750 --> 00:01:05,880
permissions nor do they show any

22
00:01:03,030 --> 00:01:09,090
<font color="#E5E5E5">notifications to the user about their</font>

23
00:01:05,880 --> 00:01:11,759
access so our goal was to demonstrate

24
00:01:09,090 --> 00:01:13,830
the feasibility of using these sensors

25
00:01:11,760 --> 00:01:18,390
to infer user outs with a high

26
00:01:13,830 --> 00:01:22,548
probability so this is<font color="#CCCCCC"> the outline</font><font color="#E5E5E5"> of</font>

27
00:01:18,390 --> 00:01:25,590
<font color="#E5E5E5">our presentation first I'll discuss the</font>

28
00:01:22,549 --> 00:01:29,009
high level graph theoretical approach of

29
00:01:25,590 --> 00:01:32,159
our attack followed by the graph

30
00:01:29,009 --> 00:01:34,560
construction from<font color="#E5E5E5"> the map data then I</font>

31
00:01:32,159 --> 00:01:36,630
<font color="#E5E5E5">will discuss the sensors that we use</font><font color="#CCCCCC"> for</font>

32
00:01:34,560 --> 00:01:39,000
inference the<font color="#E5E5E5"> challenges with those</font>

33
00:01:36,630 --> 00:01:42,449
sensors and the route construction

34
00:01:39,000 --> 00:01:44,759
method from<font color="#E5E5E5"> the sensor data after that I</font>

35
00:01:42,450 --> 00:01:47,280
<font color="#CCCCCC">will</font><font color="#E5E5E5"> discuss the search algorithm on a</font>

36
00:01:44,759 --> 00:01:49,590
high level and report on the evaluation

37
00:01:47,280 --> 00:01:55,219
results for both simulation as well as

38
00:01:49,590 --> 00:01:57,990
real experiments so we model this

39
00:01:55,219 --> 00:02:00,360
problem as a graph theoretical approach

40
00:01:57,990 --> 00:02:04,679
and we developed some efficient

41
00:02:00,360 --> 00:02:07,079
algorithms for search so basically over

42
00:02:04,680 --> 00:02:09,360
here an adversary downloads the road

43
00:02:07,079 --> 00:02:13,049
<font color="#E5E5E5">network for a large number of areas and</font>

44
00:02:09,360 --> 00:02:13,830
converts them to graph where the roads

45
00:02:13,050 --> 00:02:16,080
are

46
00:02:13,830 --> 00:02:20,280
the vertices and the intersections are

47
00:02:16,080 --> 00:02:22,980
the edges the adversary also creates and

48
00:02:20,280 --> 00:02:25,260
distributes an app that detects when I

49
00:02:22,980 --> 00:02:28,170
user sets in the car and recalls the

50
00:02:25,260 --> 00:02:30,209
sensor data of the<font color="#E5E5E5"> user driving this</font>

51
00:02:28,170 --> 00:02:33,600
sensor data is then processed and

52
00:02:30,210 --> 00:02:35,640
converted to a<font color="#CCCCCC"> sub graph which can then</font>

53
00:02:33,600 --> 00:02:38,040
be searched in the created previously

54
00:02:35,640 --> 00:02:44,339
created graph for the<font color="#E5E5E5"> maximum likelihood</font>

55
00:02:38,040 --> 00:02:47,459
route so in order to construct our graph

56
00:02:44,340 --> 00:02:50,940
we downloaded the map data from open

57
00:02:47,460 --> 00:02:54,600
street maps which is<font color="#E5E5E5"> open source map</font>

58
00:02:50,940 --> 00:02:58,260
resource containing the road networks of

59
00:02:54,600 --> 00:03:00,420
the major cities the only place<font color="#E5E5E5"> where</font>

60
00:02:58,260 --> 00:03:02,730
open street maps are really lacks is

61
00:03:00,420 --> 00:03:07,019
that the speed limit information is

62
00:03:02,730 --> 00:03:11,549
incomplete so we are for that<font color="#E5E5E5"> purpose</font><font color="#CCCCCC"> we</font>

63
00:03:07,020 --> 00:03:14,850
use nokia here platform so to construct

64
00:03:11,550 --> 00:03:19,620
the<font color="#E5E5E5"> graph the first thing we do is</font><font color="#CCCCCC"> that</font>

65
00:03:14,850 --> 00:03:22,650
<font color="#E5E5E5">we decompose each road into a one-way</font>

66
00:03:19,620 --> 00:03:24,330
atomic sections where a section is the

67
00:03:22,650 --> 00:03:27,300
part of<font color="#E5E5E5"> the road between two</font>

68
00:03:24,330 --> 00:03:30,620
intersections so in the example road

69
00:03:27,300 --> 00:03:32,760
<font color="#E5E5E5">network shown those red dots signify the</font>

70
00:03:30,620 --> 00:03:34,890
intersections and the section would be

71
00:03:32,760 --> 00:03:38,310
the part of the roads between these

72
00:03:34,890 --> 00:03:41,010
intersections please note that these

73
00:03:38,310 --> 00:03:44,040
sections do not contain any turns or any

74
00:03:41,010 --> 00:03:46,950
sharp curves however they do contain a

75
00:03:44,040 --> 00:03:49,019
slight curvature and additional

76
00:03:46,950 --> 00:03:51,929
information such<font color="#E5E5E5"> as the heading</font>

77
00:03:49,019 --> 00:03:55,290
direction and the minimum<font color="#E5E5E5"> time so once</font>

78
00:03:51,930 --> 00:03:57,540
we have these atomic sections we

79
00:03:55,290 --> 00:04:00,959
reconstruct them we connect them

80
00:03:57,540 --> 00:04:03,900
together to<font color="#E5E5E5"> form longer segments which</font>

81
00:04:00,959 --> 00:04:08,610
are basically either straight or curving

82
00:04:03,900 --> 00:04:10,500
roads so if there are sections are that

83
00:04:08,610 --> 00:04:12,780
are connected<font color="#E5E5E5"> to each other however they</font>

84
00:04:10,500 --> 00:04:15,540
make an angle of more than 30 degrees

85
00:04:12,780 --> 00:04:18,089
then we label that intersection as a

86
00:04:15,540 --> 00:04:21,209
turn and these sections actually become

87
00:04:18,089 --> 00:04:23,369
part of different long segments so in

88
00:04:21,209 --> 00:04:27,180
the example road network over there<font color="#E5E5E5"> or</font>

89
00:04:23,370 --> 00:04:27,780
s1 is an example of a long segment<font color="#CCCCCC"> s2</font>

90
00:04:27,180 --> 00:04:30,660
north

91
00:04:27,780 --> 00:04:32,940
is again a connected long segment and

92
00:04:30,660 --> 00:04:35,820
the figure on the right is the graph

93
00:04:32,940 --> 00:04:40,730
that gets generated for our example<font color="#CCCCCC"> road</font>

94
00:04:35,820 --> 00:04:43,380
network the sensor data that we use for

95
00:04:40,730 --> 00:04:45,389
<font color="#E5E5E5">inference is the gyroscope for</font>

96
00:04:43,380 --> 00:04:48,240
extracting<font color="#E5E5E5"> turn information</font><font color="#CCCCCC"> and</font>

97
00:04:45,389 --> 00:04:50,550
curvature information the accelerometer

98
00:04:48,240 --> 00:04:53,910
for calculating the idle time and<font color="#E5E5E5"> the</font>

99
00:04:50,550 --> 00:04:58,560
magnetometer for estimating the heading

100
00:04:53,910 --> 00:05:01,580
direction of the car these sensors on

101
00:04:58,560 --> 00:05:06,480
smartphones are generally low cost and

102
00:05:01,580 --> 00:05:09,599
they are extremely prone to noise so the

103
00:05:06,480 --> 00:05:13,470
figure here shows a real experiment

104
00:05:09,600 --> 00:05:15,060
route that we can conducted and the

105
00:05:13,470 --> 00:05:17,450
corresponding gyroscope and

106
00:05:15,060 --> 00:05:19,260
accelerometer drift for that expert

107
00:05:17,450 --> 00:05:23,070
accelerometer trace<font color="#E5E5E5"> for that</font>

108
00:05:19,260 --> 00:05:26,460
experimental drought so if you<font color="#E5E5E5"> look at</font>

109
00:05:23,070 --> 00:05:28,830
the<font color="#E5E5E5"> gyroscope figure those sharp</font>

110
00:05:26,460 --> 00:05:31,890
deviations are actually the turns

111
00:05:28,830 --> 00:05:35,219
recorded by the gyroscope however if you

112
00:05:31,890 --> 00:05:41,849
see that the<font color="#E5E5E5"> y-axis seems to be drifting</font>

113
00:05:35,220 --> 00:05:45,000
away<font color="#E5E5E5"> from the ZeroAccess this is a big</font>

114
00:05:41,850 --> 00:05:48,180
problem in gyroscopes also the

115
00:05:45,000 --> 00:05:51,180
accelerometer are not<font color="#E5E5E5"> suitable for speed</font>

116
00:05:48,180 --> 00:05:53,970
estimation because<font color="#E5E5E5"> they are</font><font color="#CCCCCC"> very</font>

117
00:05:51,180 --> 00:05:56,280
sensitive<font color="#E5E5E5"> to the road noises such as</font>

118
00:05:53,970 --> 00:05:58,470
potholes and speed bumps and like those

119
00:05:56,280 --> 00:06:00,750
induced large accelerations which are

120
00:05:58,470 --> 00:06:02,789
very<font color="#CCCCCC"> difficult</font><font color="#E5E5E5"> to remove the</font>

121
00:06:00,750 --> 00:06:06,479
magnetometers also are extremely

122
00:06:02,789 --> 00:06:08,849
<font color="#E5E5E5">sensitive to the magnets inside the car</font>

123
00:06:06,479 --> 00:06:10,740
and they add a lot<font color="#E5E5E5"> of difficulty in</font>

124
00:06:08,850 --> 00:06:15,840
estimating the heading direction of the

125
00:06:10,740 --> 00:06:18,539
car so in order to construct the<font color="#CCCCCC"> route</font>

126
00:06:15,840 --> 00:06:21,419
<font color="#E5E5E5">from the sensor data we first reduce the</font>

127
00:06:18,539 --> 00:06:23,370
<font color="#CCCCCC">gyroscopes drift and then align the</font>

128
00:06:21,419 --> 00:06:26,400
sensor data to a horizontal reference

129
00:06:23,370 --> 00:06:30,479
frame that is in the reference frame of

130
00:06:26,400 --> 00:06:32,789
the car so what this does<font color="#CCCCCC"> is it helps</font>

131
00:06:30,479 --> 00:06:36,419
put all the turn<font color="#CCCCCC"> information in the z</font>

132
00:06:32,789 --> 00:06:38,070
axis so that<font color="#E5E5E5"> it is easier</font><font color="#CCCCCC"> to extract the</font>

133
00:06:36,419 --> 00:06:40,810
turn<font color="#CCCCCC"> information and the curvature</font>

134
00:06:38,070 --> 00:06:45,250
information from one axis itself

135
00:06:40,810 --> 00:06:48,550
so from this z axis it<font color="#E5E5E5"> is easy to now</font>

136
00:06:45,250 --> 00:06:52,630
detect turns as large deviations in the

137
00:06:48,550 --> 00:06:56,020
z axis and<font color="#CCCCCC"> extracts</font><font color="#E5E5E5"> segments as the</font>

138
00:06:52,630 --> 00:06:58,600
trees between two such turns so for the

139
00:06:56,020 --> 00:07:02,400
figure shown below those red dots

140
00:06:58,600 --> 00:07:07,320
signify turn starts or<font color="#E5E5E5"> are edges and</font>

141
00:07:02,400 --> 00:07:12,700
those arrows are two examples of

142
00:07:07,320 --> 00:07:14,820
segments or the vertices<font color="#CCCCCC"> to each of</font>

143
00:07:12,700 --> 00:07:18,820
these segments we also add additional

144
00:07:14,820 --> 00:07:21,070
information such as like we add the idle

145
00:07:18,820 --> 00:07:24,400
time we remove the idle time information

146
00:07:21,070 --> 00:07:27,690
<font color="#CCCCCC">from the accelerometer and we add a</font>

147
00:07:24,400 --> 00:07:33,429
compass heading or<font color="#E5E5E5"> to each segment if</font>

148
00:07:27,690 --> 00:07:35,940
they satisfy certain criteria so once we

149
00:07:33,430 --> 00:07:39,550
have the graph and the sub graph our

150
00:07:35,940 --> 00:07:41,610
<font color="#E5E5E5">objective of the search algorithm is to</font>

151
00:07:39,550 --> 00:07:44,200
find<font color="#E5E5E5"> the sequence of turns and graph</font>

152
00:07:41,610 --> 00:07:46,479
that maximize the probability of

153
00:07:44,200 --> 00:07:50,560
<font color="#E5E5E5">matching the observed gyroscope turn</font>

154
00:07:46,479 --> 00:07:53,770
angles so if the gyroscope turn angles

155
00:07:50,560 --> 00:07:55,840
are approximate to a<font color="#CCCCCC"> zero mean Gaussian</font>

156
00:07:53,770 --> 00:07:59,799
<font color="#E5E5E5">distribution then maximizing this</font>

157
00:07:55,840 --> 00:08:02,440
probability<font color="#E5E5E5"> is actually equally when</font>

158
00:07:59,800 --> 00:08:06,240
equivalent to minimizing the l2 norm of

159
00:08:02,440 --> 00:08:08,830
the<font color="#CCCCCC"> errors so our search algorithm is</font>

160
00:08:06,240 --> 00:08:11,440
similar to the trellis code decoding

161
00:08:08,830 --> 00:08:13,630
technique though it has more

162
00:08:11,440 --> 00:08:16,870
complications because the start segment

163
00:08:13,630 --> 00:08:20,680
is not known we also get improved

164
00:08:16,870 --> 00:08:27,130
results because we filter unlikely or

165
00:08:20,680 --> 00:08:31,150
impossible routes at every stage so<font color="#E5E5E5"> we</font>

166
00:08:27,130 --> 00:08:34,059
define two metrics for the search

167
00:08:31,150 --> 00:08:37,029
algorithm for evaluation the individual

168
00:08:34,059 --> 00:08:39,489
rank is the<font color="#E5E5E5"> rank of the actual exact</font>

169
00:08:37,029 --> 00:08:42,549
route in the search<font color="#CCCCCC"> results while</font><font color="#E5E5E5"> the</font>

170
00:08:39,490 --> 00:08:44,800
cluster rank is the<font color="#E5E5E5"> rank of a group of</font>

171
00:08:42,549 --> 00:08:48,130
similar routes the<font color="#E5E5E5"> map that match our</font>

172
00:08:44,800 --> 00:08:51,880
actual route so basically clustering

173
00:08:48,130 --> 00:08:54,610
helps and adversary gain more confidence

174
00:08:51,880 --> 00:08:59,050
in an area and target

175
00:08:54,610 --> 00:09:02,170
all US attacks in the future on that

176
00:08:59,050 --> 00:09:05,290
particular area so this<font color="#CCCCCC"> is</font><font color="#E5E5E5"> how the</font>

177
00:09:02,170 --> 00:09:08,500
algorithm actually works first we assume

178
00:09:05,290 --> 00:09:12,099
that<font color="#E5E5E5"> every segment or every vertex of a</font>

179
00:09:08,500 --> 00:09:15,310
graph is a potential starting point we I

180
00:09:12,100 --> 00:09:17,230
to read through all<font color="#E5E5E5"> for every</font>

181
00:09:15,310 --> 00:09:21,029
intersection we iterate through every

182
00:09:17,230 --> 00:09:24,760
potential path and get the list<font color="#E5E5E5"> of all</font>

183
00:09:21,029 --> 00:09:26,950
forward connections out of these forward

184
00:09:24,760 --> 00:09:29,439
connections the ones<font color="#E5E5E5"> that are unlikely</font>

185
00:09:26,950 --> 00:09:33,040
or impossible to<font color="#CCCCCC"> happen are filtered out</font>

186
00:09:29,440 --> 00:09:35,680
and the remaining connections are scored

187
00:09:33,040 --> 00:09:38,620
and added<font color="#CCCCCC"> to the previous parts course</font>

188
00:09:35,680 --> 00:09:42,099
so basically now we have these new

189
00:09:38,620 --> 00:09:44,709
potential paths with a new score at the

190
00:09:42,100 --> 00:09:48,970
end of every iteration we just pick the

191
00:09:44,709 --> 00:09:53,439
top scoring paths so basically this top

192
00:09:48,970 --> 00:09:56,260
scoring selection is a is a trade-off

193
00:09:53,440 --> 00:09:58,660
between speed and inference<font color="#E5E5E5"> rate so if</font>

194
00:09:56,260 --> 00:10:01,120
you want better<font color="#E5E5E5"> speed then you can just</font>

195
00:09:58,660 --> 00:10:04,569
select a fewer number<font color="#E5E5E5"> of paths if you</font>

196
00:10:01,120 --> 00:10:07,810
want more infant then<font color="#E5E5E5"> select more number</font>

197
00:10:04,570 --> 00:10:10,000
<font color="#CCCCCC">of paths we filter out unlikely</font>

198
00:10:07,810 --> 00:10:11,829
<font color="#E5E5E5">connections using very flexible</font>

199
00:10:10,000 --> 00:10:14,920
thresholds so for<font color="#E5E5E5"> example if the</font>

200
00:10:11,829 --> 00:10:17,349
difference<font color="#E5E5E5"> between the reported turn</font>

201
00:10:14,920 --> 00:10:19,959
angle and the connection turn angle is

202
00:10:17,350 --> 00:10:22,240
<font color="#CCCCCC">less than</font><font color="#E5E5E5"> a certain flexible threshold</font>

203
00:10:19,959 --> 00:10:23,910
then only<font color="#E5E5E5"> that path is connection is</font>

204
00:10:22,240 --> 00:10:27,339
selected otherwise it is rejected

205
00:10:23,910 --> 00:10:29,319
similarly<font color="#CCCCCC"> for time if there is a</font>

206
00:10:27,339 --> 00:10:32,110
particular in text section that is

207
00:10:29,320 --> 00:10:35,680
impossible<font color="#E5E5E5"> to reach in a given time then</font>

208
00:10:32,110 --> 00:10:38,769
it<font color="#E5E5E5"> is rejected so our scoring is based</font>

209
00:10:35,680 --> 00:10:41,170
on a weighted turn angles curvature and

210
00:10:38,769 --> 00:10:43,720
travel time so<font color="#E5E5E5"> basically we assign a</font>

211
00:10:41,170 --> 00:10:46,420
higher way to curvature and travel time

212
00:10:43,720 --> 00:10:48,880
because they are output from the

213
00:10:46,420 --> 00:10:52,660
gyroscope which is a more accurate

214
00:10:48,880 --> 00:10:55,810
sensor we assign a lower way to time

215
00:10:52,660 --> 00:10:58,050
<font color="#E5E5E5">because at any time</font><font color="#CCCCCC"> of the day the</font>

216
00:10:55,810 --> 00:11:02,380
traffic patterns may be unpredictable

217
00:10:58,050 --> 00:11:05,709
the curvature scoring happens by

218
00:11:02,380 --> 00:11:08,230
splitting<font color="#E5E5E5"> the</font><font color="#CCCCCC"> segment into an equal</font>

219
00:11:05,709 --> 00:11:08,619
number of parts as the<font color="#E5E5E5"> gyroscope segment</font>

220
00:11:08,230 --> 00:11:11,710
that we

221
00:11:08,620 --> 00:11:14,200
want to match against we assume constant

222
00:11:11,710 --> 00:11:17,290
velocity for this purpose however we see

223
00:11:14,200 --> 00:11:20,200
that<font color="#E5E5E5"> even with constant velocity there</font>

224
00:11:17,290 --> 00:11:22,839
is a very<font color="#E5E5E5"> large increase in the attack</font>

225
00:11:20,200 --> 00:11:27,100
performance once we<font color="#E5E5E5"> have an equal number</font>

226
00:11:22,840 --> 00:11:30,580
of segments for the graph<font color="#CCCCCC"> segment graph</font>

227
00:11:27,100 --> 00:11:32,880
<font color="#E5E5E5">vertex as well as the gyroscope vertex</font>

228
00:11:30,580 --> 00:11:35,500
we calculate the curve score as the

229
00:11:32,880 --> 00:11:40,390
normalized distance between the segment

230
00:11:35,500 --> 00:11:42,550
and the<font color="#CCCCCC"> curve for each part we do L 2</font>

231
00:11:40,390 --> 00:11:44,980
norm is theoretically optimal for

232
00:11:42,550 --> 00:11:47,439
Gaussian distributions we preferred the

233
00:11:44,980 --> 00:11:50,080
l1 norm over the l2 not because the l1

234
00:11:47,440 --> 00:11:54,520
norm is much more efficient than the l2

235
00:11:50,080 --> 00:11:56,530
norm and we observed better results with

236
00:11:54,520 --> 00:11:59,110
the l1 norm so the final score<font color="#E5E5E5"> is</font>

237
00:11:56,530 --> 00:12:02,170
basically the weighted sum of the term

238
00:11:59,110 --> 00:12:06,910
travel time and curvatures code for all

239
00:12:02,170 --> 00:12:10,060
the intersections so we evaluated the

240
00:12:06,910 --> 00:12:13,060
<font color="#CCCCCC">gyroscope are using error distributions</font>

241
00:12:10,060 --> 00:12:15,489
for real driving experiments so the

242
00:12:13,060 --> 00:12:19,060
figure shows<font color="#CCCCCC"> the error distributions for</font>

243
00:12:15,490 --> 00:12:22,230
for smartphones and as you can<font color="#CCCCCC"> see these</font>

244
00:12:19,060 --> 00:12:24,520
<font color="#CCCCCC">are distributions approximate to a</font>

245
00:12:22,230 --> 00:12:27,190
<font color="#CCCCCC">Gaussian distributions which was our</font>

246
00:12:24,520 --> 00:12:30,520
original as I'm<font color="#CCCCCC"> send assumption for</font>

247
00:12:27,190 --> 00:12:32,290
choosing the l2 norm we also observed

248
00:12:30,520 --> 00:12:34,540
<font color="#E5E5E5">that ninety-five percent of the errors</font>

249
00:12:32,290 --> 00:12:38,860
were less than<font color="#E5E5E5"> 10</font><font color="#CCCCCC"> degrees which is quite</font>

250
00:12:34,540 --> 00:12:42,640
ideal for inference so before<font color="#CCCCCC"> I report</font>

251
00:12:38,860 --> 00:12:45,400
the simulation results to you we chose

252
00:12:42,640 --> 00:12:49,030
<font color="#E5E5E5">11 cities for simulation and they are</font>

253
00:12:45,400 --> 00:12:52,300
shown in the table below so the cities

254
00:12:49,030 --> 00:12:55,839
<font color="#E5E5E5">that have a high number of vertices and</font>

255
00:12:52,300 --> 00:12:59,380
edges signify the larger cities or with

256
00:12:55,840 --> 00:13:02,950
a lower potential for inference then in

257
00:12:59,380 --> 00:13:05,230
the figure the bottom cities<font color="#E5E5E5"> a Rome and</font>

258
00:13:02,950 --> 00:13:08,020
Concord<font color="#E5E5E5"> you can see that they have</font><font color="#CCCCCC"> a</font>

259
00:13:05,230 --> 00:13:10,030
higher distribution of turns which

260
00:13:08,020 --> 00:13:12,220
signifies that they have a high

261
00:13:10,030 --> 00:13:14,860
inference<font color="#E5E5E5"> for high potential for</font>

262
00:13:12,220 --> 00:13:18,760
inference then there are cities such as

263
00:13:14,860 --> 00:13:20,770
Sunnyvale or in the top left which has

264
00:13:18,760 --> 00:13:22,180
mostly<font color="#CCCCCC"> 90 degree turns and that</font>

265
00:13:20,770 --> 00:13:24,339
signifies that the

266
00:13:22,180 --> 00:13:28,930
inference potential for Sunnyvale may be

267
00:13:24,339 --> 00:13:31,870
low so<font color="#E5E5E5"> we create a simulation routes by</font>

268
00:13:28,930 --> 00:13:34,839
connecting<font color="#CCCCCC"> together connected segments</font>

269
00:13:31,870 --> 00:13:38,560
of the graph starting at a random start

270
00:13:34,839 --> 00:13:41,709
segment to simulate real driving

271
00:13:38,560 --> 00:13:45,189
<font color="#E5E5E5">scenarios we injected variable noise to</font>

272
00:13:41,709 --> 00:13:48,069
every turn curve and time of our

273
00:13:45,190 --> 00:13:49,990
simulation route so there were four

274
00:13:48,070 --> 00:13:52,570
noise scenarios that we tested against

275
00:13:49,990 --> 00:13:55,420
one was the idle noise scenario which is

276
00:13:52,570 --> 00:13:57,820
the noise free scenario and basically

277
00:13:55,420 --> 00:13:59,620
serves as the upper bound for

278
00:13:57,820 --> 00:14:02,529
performance for our simulation

279
00:13:59,620 --> 00:14:06,430
experiments the<font color="#E5E5E5"> typical noise scenario</font>

280
00:14:02,529 --> 00:14:09,160
was created<font color="#CCCCCC"> using values</font><font color="#E5E5E5"> from real</font>

281
00:14:06,430 --> 00:14:12,339
driving experiments and assume moderate

282
00:14:09,160 --> 00:14:15,069
traffic the high noise scenario assumed

283
00:14:12,339 --> 00:14:17,260
heavy<font color="#E5E5E5"> traffic on the road and less</font>

284
00:14:15,070 --> 00:14:20,170
accurate census while the future

285
00:14:17,260 --> 00:14:24,069
scenario as you<font color="#E5E5E5"> moderate traffic and</font>

286
00:14:20,170 --> 00:14:27,339
more accurate census like we think that

287
00:14:24,070 --> 00:14:29,770
<font color="#E5E5E5">as MEMS technology progresses or the</font>

288
00:14:27,339 --> 00:14:33,820
errors will be less as the sensors will

289
00:14:29,770 --> 00:14:36,640
be higher quality so for simulation

290
00:14:33,820 --> 00:14:39,520
routes we ran we created two thousand

291
00:14:36,640 --> 00:14:40,750
unique routes for each city and tested

292
00:14:39,520 --> 00:14:45,640
them against all the<font color="#E5E5E5"> four noise</font>

293
00:14:40,750 --> 00:14:48,810
scenarios these are the<font color="#CCCCCC"> results for six</font>

294
00:14:45,640 --> 00:14:54,580
of those cities and this is basically<font color="#E5E5E5"> a</font>

295
00:14:48,810 --> 00:14:57,489
<font color="#E5E5E5">CDF of the results so we found good</font>

296
00:14:54,580 --> 00:15:00,190
inference for eight cities where the for

297
00:14:57,490 --> 00:15:03,580
the typical scenario of we had about

298
00:15:00,190 --> 00:15:06,790
fifty<font color="#CCCCCC"> percent of routes in the top</font><font color="#E5E5E5"> ten</font>

299
00:15:03,580 --> 00:15:08,890
results for even for the high noise

300
00:15:06,790 --> 00:15:10,810
scenario almost<font color="#E5E5E5"> thirty five percent of</font>

301
00:15:08,890 --> 00:15:15,630
the routes appeared in the top ten

302
00:15:10,810 --> 00:15:18,189
results for clusters we<font color="#E5E5E5"> basically got</font>

303
00:15:15,630 --> 00:15:21,700
<font color="#E5E5E5">five to ten percent better than the</font>

304
00:15:18,190 --> 00:15:23,980
individual ranks we<font color="#CCCCCC"> also also</font><font color="#E5E5E5"> observe</font>

305
00:15:21,700 --> 00:15:27,279
low inference<font color="#E5E5E5"> for grid like cities such</font>

306
00:15:23,980 --> 00:15:30,279
as Manhattan<font color="#CCCCCC"> another observation that we</font>

307
00:15:27,279 --> 00:15:33,160
made was that turn and curvature combine

308
00:15:30,279 --> 00:15:35,320
had the largest impact so if you look<font color="#E5E5E5"> at</font>

309
00:15:33,160 --> 00:15:38,829
<font color="#CCCCCC">the figures London and</font>

310
00:15:35,320 --> 00:15:42,120
<font color="#CCCCCC">room have an equal turn distributions</font>

311
00:15:38,830 --> 00:15:45,250
<font color="#CCCCCC">like Boston Madrid and Paris however</font>

312
00:15:42,120 --> 00:15:50,230
London and Rome have a better inference

313
00:15:45,250 --> 00:15:52,810
score because they have more curving

314
00:15:50,230 --> 00:15:54,690
roads as compared to Boston Madrid and

315
00:15:52,810 --> 00:15:57,579
Paris which have more straighter roads

316
00:15:54,690 --> 00:15:59,980
also the size of the city doesn't impact

317
00:15:57,580 --> 00:16:02,800
inference because London and Rome are

318
00:15:59,980 --> 00:16:04,090
the largest cities that we chose for

319
00:16:02,800 --> 00:16:07,780
simulations and they have the best

320
00:16:04,090 --> 00:16:10,150
results we also perform some real

321
00:16:07,780 --> 00:16:12,370
driving experiments in<font color="#CCCCCC"> Boston and Walton</font>

322
00:16:10,150 --> 00:16:16,780
so basically<font color="#CCCCCC"> we collected 70 routes each</font>

323
00:16:12,370 --> 00:16:18,790
or and our drivers were instructed to be

324
00:16:16,780 --> 00:16:21,280
<font color="#E5E5E5">places to restrictions on the drivers</font>

325
00:16:18,790 --> 00:16:23,469
one was to have the position of the

326
00:16:21,280 --> 00:16:25,390
<font color="#E5E5E5">phone fixed while they were driving</font>

327
00:16:23,470 --> 00:16:28,210
while they were collecting the

328
00:16:25,390 --> 00:16:31,870
experiment and do not reverse during

329
00:16:28,210 --> 00:16:33,550
experiment collection so the d4 they

330
00:16:31,870 --> 00:16:35,320
were free to keep the phone anywhere

331
00:16:33,550 --> 00:16:38,740
inside<font color="#CCCCCC"> the car I mean there were some</font>

332
00:16:35,320 --> 00:16:40,930
experiments which had the<font color="#E5E5E5"> phone kept on</font>

333
00:16:38,740 --> 00:16:43,030
the<font color="#E5E5E5"> coffee holder there were some with</font>

334
00:16:40,930 --> 00:16:45,219
the amount<font color="#E5E5E5"> which whether the phones were</font>

335
00:16:43,030 --> 00:16:48,610
<font color="#CCCCCC">placed</font><font color="#E5E5E5"> on the amount the key results</font>

336
00:16:45,220 --> 00:16:51,700
here were that for Boston thirty percent

337
00:16:48,610 --> 00:16:54,190
of the routes were found in the top five

338
00:16:51,700 --> 00:16:56,370
search results wife evolved him almost

339
00:16:54,190 --> 00:16:59,440
fifty percent were in the top five

340
00:16:56,370 --> 00:17:02,500
thirteen percent in<font color="#E5E5E5"> Boston ranked one</font>

341
00:16:59,440 --> 00:17:05,410
while about<font color="#E5E5E5"> thirty-eight percent in</font>

342
00:17:02,500 --> 00:17:08,400
<font color="#CCCCCC">Waltham actually ranked one another</font>

343
00:17:05,410 --> 00:17:11,829
observation that we made was that the

344
00:17:08,400 --> 00:17:14,589
Boston results lean more towards the

345
00:17:11,829 --> 00:17:16,510
high noise scenario of simulations while

346
00:17:14,589 --> 00:17:19,179
<font color="#CCCCCC">volcom results lean more towards a</font>

347
00:17:16,510 --> 00:17:21,220
typical noise scenario this<font color="#E5E5E5"> is because</font>

348
00:17:19,180 --> 00:17:23,709
<font color="#CCCCCC">of</font><font color="#E5E5E5"> the traffic profile so basically</font>

349
00:17:21,220 --> 00:17:26,620
Boston has much more congestion than

350
00:17:23,709 --> 00:17:30,130
<font color="#E5E5E5">Walton as and time has a larger</font><font color="#CCCCCC"> impact</font>

351
00:17:26,619 --> 00:17:33,010
in<font color="#E5E5E5"> Boston so to summarize we</font>

352
00:17:30,130 --> 00:17:35,350
demonstrated that<font color="#E5E5E5"> apps</font><font color="#CCCCCC"> of with</font><font color="#E5E5E5"> no</font>

353
00:17:33,010 --> 00:17:38,440
permissions that use these sensors can

354
00:17:35,350 --> 00:17:40,080
infer routes with high probability we

355
00:17:38,440 --> 00:17:43,210
model this problem as a graph

356
00:17:40,080 --> 00:17:47,560
theoretical model<font color="#E5E5E5"> or to identify the</font>

357
00:17:43,210 --> 00:17:48,910
maximum likelihood route or to evaluate

358
00:17:47,560 --> 00:17:51,909
our system we collect

359
00:17:48,910 --> 00:17:54,820
140 driving experiments for<font color="#CCCCCC"> Boston and</font>

360
00:17:51,910 --> 00:17:57,550
<font color="#CCCCCC">Walton and found that thirty percent</font><font color="#E5E5E5"> of</font>

361
00:17:54,820 --> 00:17:59,950
them in Boston were in the top five

362
00:17:57,550 --> 00:18:02,440
while for Waltham almost fifty percent

363
00:17:59,950 --> 00:18:05,680
were in the top five we also<font color="#E5E5E5"> performed</font>

364
00:18:02,440 --> 00:18:07,300
<font color="#CCCCCC">simulations for</font><font color="#E5E5E5"> eleven cities and found</font>

365
00:18:05,680 --> 00:18:09,370
good inference for eight cities with

366
00:18:07,300 --> 00:18:12,610
more than<font color="#E5E5E5"> fifty percent of routes in top</font>

367
00:18:09,370 --> 00:18:16,060
<font color="#CCCCCC">10 so basically</font><font color="#E5E5E5"> this suggests that this</font>

368
00:18:12,610 --> 00:18:27,729
threat is real and should be mitigated

369
00:18:16,060 --> 00:18:31,149
in the future<font color="#CCCCCC"> Thank You Ryan Rosenberg</font>

370
00:18:27,730 --> 00:18:33,430
from Qualcomm so navigation solutions

371
00:18:31,150 --> 00:18:35,140
have a thing called<font color="#E5E5E5"> dead reckoning where</font>

372
00:18:33,430 --> 00:18:37,330
they're supposed to be<font color="#CCCCCC"> able to estimate</font>

373
00:18:35,140 --> 00:18:40,090
the path<font color="#E5E5E5"> even the absence of a GPS</font>

374
00:18:37,330 --> 00:18:42,820
signal did you compare did you derive

375
00:18:40,090 --> 00:18:45,730
your kind of<font color="#E5E5E5"> segment processing from</font>

376
00:18:42,820 --> 00:18:48,970
actual dead reckoning algorithms or did

377
00:18:45,730 --> 00:18:52,090
you investigate<font color="#CCCCCC"> oh no we chose a graph</font>

378
00:18:48,970 --> 00:18:53,560
theoretical approach for our so

379
00:18:52,090 --> 00:18:56,439
basically<font color="#CCCCCC"> the problem with dead</font>

380
00:18:53,560 --> 00:18:58,840
reckoning is it assumes relatively

381
00:18:56,440 --> 00:19:01,780
straighter roads and it assumes that the

382
00:18:58,840 --> 00:19:04,810
accelerometer is actually quite good I

383
00:19:01,780 --> 00:19:08,620
mean<font color="#CCCCCC"> because it depends on speed</font>

384
00:19:04,810 --> 00:19:10,780
estimation in our case we found that<font color="#E5E5E5"> for</font>

385
00:19:08,620 --> 00:19:13,000
roads which have speed bumps and

386
00:19:10,780 --> 00:19:14,889
potholes the<font color="#E5E5E5"> dead reckoning method</font>

387
00:19:13,000 --> 00:19:17,440
doesn't actually work very well and

388
00:19:14,890 --> 00:19:20,830
that's the<font color="#E5E5E5"> reason we went for this graph</font>

389
00:19:17,440 --> 00:19:23,020
theoretical approach where we model

390
00:19:20,830 --> 00:19:29,679
everything as a graph and<font color="#E5E5E5"> then search on</font>

391
00:19:23,020 --> 00:19:32,139
it thanks Ellie canal<font color="#E5E5E5"> see me was very</font>

392
00:19:29,680 --> 00:19:33,880
interesting talk just on that<font color="#E5E5E5"> note so</font>

393
00:19:32,140 --> 00:19:37,840
have you<font color="#E5E5E5"> had much experience / tease</font>

394
00:19:33,880 --> 00:19:39,220
excuse me success with using signature

395
00:19:37,840 --> 00:19:40,959
based methods to extract all<font color="#CCCCCC"> the noise</font>

396
00:19:39,220 --> 00:19:43,030
from<font color="#E5E5E5"> the accelerometer you know you can</font>

397
00:19:40,960 --> 00:19:45,040
good jolt would be kind<font color="#E5E5E5"> of very impulse</font>

398
00:19:43,030 --> 00:19:51,190
driven we're as a normal turn would<font color="#E5E5E5"> look</font>

399
00:19:45,040 --> 00:19:54,010
very<font color="#E5E5E5"> different so we tried filtering on</font>

400
00:19:51,190 --> 00:19:56,350
the accelerometer however we were not

401
00:19:54,010 --> 00:20:00,370
<font color="#E5E5E5">very successful with that because not</font>

402
00:19:56,350 --> 00:20:02,590
only are these potholes or speed bumps

403
00:20:00,370 --> 00:20:05,309
an issue even if there<font color="#CCCCCC"> are slow</font>

404
00:20:02,590 --> 00:20:07,480
oops in the road that induce

405
00:20:05,309 --> 00:20:10,779
accelerations in the accelerometer and

406
00:20:07,480 --> 00:20:13,270
without the help of other sensors it is

407
00:20:10,779 --> 00:20:16,000
very<font color="#E5E5E5"> difficult to remove so basically if</font>

408
00:20:13,270 --> 00:20:17,799
your car is<font color="#E5E5E5"> going down like this it will</font>

409
00:20:16,000 --> 00:20:20,500
actually show up in the accelerometer

410
00:20:17,799 --> 00:20:23,139
and without the help of other sensors

411
00:20:20,500 --> 00:20:25,659
you cannot figure out whether it's is

412
00:20:23,140 --> 00:20:28,779
the user<font color="#CCCCCC"> that actually accelerated the</font>

413
00:20:25,659 --> 00:20:29,860
car or whether it's a road slope<font color="#E5E5E5"> not to</font>

414
00:20:28,779 --> 00:20:33,190
follow up<font color="#E5E5E5"> with you on that afterwards</font>

415
00:20:29,860 --> 00:20:34,120
and a second comment more<font color="#E5E5E5"> is</font><font color="#CCCCCC"> that taking</font>

416
00:20:33,190 --> 00:20:36,159
<font color="#E5E5E5">into account the fact that you're</font>

417
00:20:34,120 --> 00:20:38,199
<font color="#CCCCCC">working off the user's phone it seems</font>

418
00:20:36,159 --> 00:20:40,090
very the potential that you could use

419
00:20:38,200 --> 00:20:42,549
check-ins and Twitter or other

420
00:20:40,090 --> 00:20:44,289
location-based areas and then do<font color="#CCCCCC"> that as</font>

421
00:20:42,549 --> 00:20:45,549
a sanity check to see whether you're

422
00:20:44,289 --> 00:20:47,379
actually on the point where the user

423
00:20:45,549 --> 00:20:49,799
claims to be I'd be<font color="#CCCCCC"> an interesting way</font>

424
00:20:47,380 --> 00:20:52,090
to extend<font color="#E5E5E5"> the work very nice thank you</font>

425
00:20:49,799 --> 00:20:53,918
have to limit it to one<font color="#E5E5E5"> more question</font>

426
00:20:52,090 --> 00:20:55,870
<font color="#CCCCCC">thanks I've rob Cunningham from MIT</font>

427
00:20:53,919 --> 00:20:57,940
Lincoln lab I just had a question about

428
00:20:55,870 --> 00:21:00,489
your initial conditions how did you know

429
00:20:57,940 --> 00:21:02,110
which city you started in and can you

430
00:21:00,490 --> 00:21:04,450
<font color="#E5E5E5">use some of</font><font color="#CCCCCC"> your techniques to give you</font>

431
00:21:02,110 --> 00:21:07,209
a sense of which city you might be in ok

432
00:21:04,450 --> 00:21:12,700
so we perform some tests based on IP

433
00:21:07,210 --> 00:21:15,429
address so if like when the sensor data

434
00:21:12,700 --> 00:21:18,039
gets uploaded to a server you have the

435
00:21:15,429 --> 00:21:21,159
IP address over there so<font color="#E5E5E5"> using the IP</font>

436
00:21:18,039 --> 00:21:24,820
address you can get a good estimate of a

437
00:21:21,159 --> 00:21:27,610
particular<font color="#E5E5E5"> area a user is in so in our</font>

438
00:21:24,820 --> 00:21:31,178
work we actually chose cities but you

439
00:21:27,610 --> 00:21:34,779
can extract graphs for larger areas and

440
00:21:31,179 --> 00:21:38,710
run the algorithms on those<font color="#E5E5E5"> ok I hope</font>

441
00:21:34,779 --> 00:21:41,190
<font color="#E5E5E5">that answers</font><font color="#CCCCCC"> your question let's thank</font>

442
00:21:38,710 --> 00:21:41,190
the speaker

