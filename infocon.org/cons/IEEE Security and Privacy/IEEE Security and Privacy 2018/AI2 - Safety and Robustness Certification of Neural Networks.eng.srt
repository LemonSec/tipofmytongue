1
00:00:07,569 --> 00:00:15,210
so my name<font color="#CCCCCC"> is Tim and Garon</font>

2
00:00:11,000 --> 00:00:22,250
<font color="#E5E5E5">she's in there</font><font color="#CCCCCC"> and this</font><font color="#E5E5E5"> is joint work</font>

3
00:00:15,210 --> 00:00:22,250
<font color="#CCCCCC">Peters ferrets and Martin so as you</font><font color="#E5E5E5"> all</font>

4
00:00:29,920 --> 00:00:34,600
<font color="#E5E5E5">all right so as you all probably know</font>

5
00:00:31,660 --> 00:00:36,640
neural networks are not robust<font color="#E5E5E5"> against</font>

6
00:00:34,600 --> 00:00:39,250
the input perturbations<font color="#E5E5E5"> so here we have</font>

7
00:00:36,640 --> 00:00:40,780
three very nice<font color="#E5E5E5"> images of roads</font><font color="#CCCCCC"> and</font><font color="#E5E5E5"> as</font>

8
00:00:39,250 --> 00:00:43,449
you<font color="#E5E5E5"> can</font><font color="#CCCCCC"> see it's probably good idea to</font>

9
00:00:40,780 --> 00:00:46,960
go left<font color="#E5E5E5"> here but we have three neural</font>

10
00:00:43,449 --> 00:00:50,379
networks<font color="#E5E5E5"> the end of a green and</font><font color="#CCCCCC"> the well</font>

11
00:00:46,960 --> 00:00:55,360
the red one<font color="#E5E5E5"> and one of those okay I</font>

12
00:00:50,379 --> 00:00:58,750
didn't press<font color="#CCCCCC"> ok and one</font><font color="#E5E5E5"> of those</font>

13
00:00:55,360 --> 00:01:01,540
actually<font color="#E5E5E5"> disagrees and tells us to</font><font color="#CCCCCC"> go to</font>

14
00:00:58,750 --> 00:01:04,629
the right and to be fair<font color="#E5E5E5"> somebody was a</font>

15
00:01:01,540 --> 00:01:06,280
bit evil here and actually<font color="#E5E5E5"> modified</font>

16
00:01:04,629 --> 00:01:09,310
those images<font color="#CCCCCC"> so you can</font><font color="#E5E5E5"> see some black</font>

17
00:01:06,280 --> 00:01:11,200
<font color="#CCCCCC">patches</font><font color="#E5E5E5"> and those are responsible for</font>

18
00:01:09,310 --> 00:01:12,700
those misclassifications<font color="#CCCCCC"> but of</font><font color="#E5E5E5"> course</font>

19
00:01:11,200 --> 00:01:14,979
we don't want<font color="#E5E5E5"> this because as humans we</font>

20
00:01:12,700 --> 00:01:22,179
<font color="#CCCCCC">can see you should go</font><font color="#E5E5E5"> left and there are</font>

21
00:01:14,979 --> 00:01:24,009
many other ways<font color="#CCCCCC"> to there are many other</font>

22
00:01:22,179 --> 00:01:25,300
kinds<font color="#CCCCCC"> of adversarial attacks on deep</font>

23
00:01:24,009 --> 00:01:27,610
<font color="#E5E5E5">learning models for</font><font color="#CCCCCC"> example you can</font>

24
00:01:25,300 --> 00:01:32,740
actually impersonate someone else in

25
00:01:27,610 --> 00:01:41,860
like by<font color="#E5E5E5"> just varying some sort of funky</font>

26
00:01:32,740 --> 00:01:43,298
caucus and also a text<font color="#CCCCCC"> that there are</font>

27
00:01:41,860 --> 00:01:45,100
also text that actually are<font color="#E5E5E5"> not</font>

28
00:01:43,299 --> 00:01:47,439
<font color="#E5E5E5">perceptible by a human so here we can</font>

29
00:01:45,100 --> 00:01:50,919
<font color="#E5E5E5">have we have an image of a panda bear</font>

30
00:01:47,439 --> 00:01:52,508
<font color="#CCCCCC">and</font><font color="#E5E5E5"> then we add some</font><font color="#CCCCCC"> noise to it and now</font>

31
00:01:50,920 --> 00:01:55,079
the<font color="#E5E5E5"> neural net</font><font color="#CCCCCC"> is actually very</font>

32
00:01:52,509 --> 00:01:58,719
confident<font color="#E5E5E5"> that this</font><font color="#CCCCCC"> is a</font><font color="#E5E5E5"> given and</font>

33
00:01:55,079 --> 00:02:01,419
there's been a lot of work<font color="#E5E5E5"> on attacking</font>

34
00:01:58,719 --> 00:02:03,699
neural networks<font color="#E5E5E5"> also some heuristic</font>

35
00:02:01,420 --> 00:02:06,549
defenses<font color="#CCCCCC"> that empirically improve</font>

36
00:02:03,700 --> 00:02:08,739
robustness a little bit but there's been

37
00:02:06,549 --> 00:02:10,420
less work<font color="#E5E5E5"> on proving that neural</font>

38
00:02:08,739 --> 00:02:13,239
networks<font color="#CCCCCC"> actually do what they're</font>

39
00:02:10,419 --> 00:02:15,518
<font color="#CCCCCC">supposed to do in the sense of they</font>

40
00:02:13,239 --> 00:02:17,320
<font color="#E5E5E5">don't behave differently on specific</font>

41
00:02:15,519 --> 00:02:20,530
images if<font color="#CCCCCC"> you just</font><font color="#E5E5E5"> change the image a</font>

42
00:02:17,320 --> 00:02:24,359
<font color="#E5E5E5">little bit and in this work</font><font color="#CCCCCC"> we're trying</font>

43
00:02:20,530 --> 00:02:24,360
<font color="#CCCCCC">to address this</font>

44
00:02:32,069 --> 00:02:37,510
<font color="#E5E5E5">okay and we want to prove</font><font color="#CCCCCC"> that there are</font>

45
00:02:35,829 --> 00:02:39,700
no robustness violations around the

46
00:02:37,510 --> 00:02:42,069
<font color="#E5E5E5">specific image and what we want is we</font>

47
00:02:39,700 --> 00:02:49,780
want<font color="#E5E5E5"> a</font><font color="#CCCCCC"> system</font><font color="#E5E5E5"> that is automated</font><font color="#CCCCCC"> and</font>

48
00:02:42,069 --> 00:02:52,149
actually scales<font color="#E5E5E5"> to real neural nets so</font>

49
00:02:49,780 --> 00:02:56,590
this is the problem<font color="#E5E5E5"> statement</font><font color="#CCCCCC"> so we have</font>

50
00:02:52,150 --> 00:02:58,870
<font color="#E5E5E5">a neural network</font><font color="#CCCCCC"> it</font><font color="#E5E5E5"> well it Maps inputs</font>

51
00:02:56,590 --> 00:03:00,700
to outputs some of the outputs are

52
00:02:58,870 --> 00:03:02,919
possible<font color="#CCCCCC"> and some of the inputs are</font>

53
00:03:00,700 --> 00:03:04,869
possible those are the inputs<font color="#E5E5E5"> that the</font>

54
00:03:02,919 --> 00:03:06,310
<font color="#E5E5E5">attacker can cause by perturbing a</font>

55
00:03:04,870 --> 00:03:08,319
particular image for<font color="#CCCCCC"> example and</font><font color="#E5E5E5"> they</font>

56
00:03:06,310 --> 00:03:10,419
have a<font color="#CCCCCC"> save set of outputs</font><font color="#E5E5E5"> so this could</font>

57
00:03:08,319 --> 00:03:13,030
be<font color="#E5E5E5"> all the outputs that classify to the</font>

58
00:03:10,419 --> 00:03:16,620
same thing and the challenge<font color="#E5E5E5"> here is</font>

59
00:03:13,030 --> 00:03:19,120
<font color="#CCCCCC">that xxxx may contain a lot of inputs</font><font color="#E5E5E5"> so</font>

60
00:03:16,620 --> 00:03:22,169
there<font color="#E5E5E5"> are many possible attacks and we</font>

61
00:03:19,120 --> 00:03:25,659
need to<font color="#E5E5E5"> defend against all of them and</font>

62
00:03:22,169 --> 00:03:28,120
existing solutions based on SMT actually

63
00:03:25,659 --> 00:03:31,120
do not scale<font color="#CCCCCC"> to a large network they</font>

64
00:03:28,120 --> 00:03:34,500
work well for small networks<font color="#E5E5E5"> but if you</font>

65
00:03:31,120 --> 00:03:34,500
have large neural<font color="#CCCCCC"> nets they stop working</font>

66
00:03:34,709 --> 00:03:40,329
and the key inside of<font color="#E5E5E5"> this work is that</font>

67
00:03:37,989 --> 00:03:42,069
<font color="#E5E5E5">we can</font><font color="#CCCCCC"> use a I to analyze neural</font><font color="#E5E5E5"> nets</font>

68
00:03:40,329 --> 00:03:45,189
and<font color="#CCCCCC"> by</font><font color="#E5E5E5"> a I mean</font><font color="#CCCCCC"> of course</font><font color="#E5E5E5"> abstract</font>

69
00:03:42,069 --> 00:03:49,720
interpretation<font color="#CCCCCC"> and who knows about</font>

70
00:03:45,189 --> 00:03:51,459
<font color="#CCCCCC">effort interpretation</font><font color="#E5E5E5"> okay not not a lot</font>

71
00:03:49,720 --> 00:03:55,000
of people<font color="#E5E5E5"> imputation is actually about</font>

72
00:03:51,459 --> 00:03:57,220
<font color="#E5E5E5">40 years old which means that maybe it</font>

73
00:03:55,000 --> 00:04:00,159
will become<font color="#E5E5E5"> the next big thing soon but</font>

74
00:03:57,220 --> 00:04:02,739
it also means that it had a lot of<font color="#E5E5E5"> time</font>

75
00:04:00,159 --> 00:04:04,150
to<font color="#E5E5E5"> develop and whatever the invitation</font>

76
00:04:02,739 --> 00:04:07,919
actually<font color="#CCCCCC"> isn't it will be the most</font>

77
00:04:04,150 --> 00:04:07,919
technical slide<font color="#E5E5E5"> of this talk</font>

78
00:04:08,150 --> 00:04:12,260
<font color="#E5E5E5">okay it's it's a theory for</font>

79
00:04:10,610 --> 00:04:15,110
approximating<font color="#E5E5E5"> program behaviors so</font><font color="#CCCCCC"> you</font>

80
00:04:12,260 --> 00:04:18,738
<font color="#CCCCCC">have</font><font color="#E5E5E5"> a program and now we want to say</font>

81
00:04:15,110 --> 00:04:21,470
well what what<font color="#E5E5E5"> can this program do and</font>

82
00:04:18,738 --> 00:04:23,530
you're satisfied<font color="#E5E5E5"> if somebody tells us</font>

83
00:04:21,470 --> 00:04:26,030
well<font color="#CCCCCC"> it can do it mostest</font>

84
00:04:23,530 --> 00:04:28,070
this means the deputy interpretation it

85
00:04:26,030 --> 00:04:30,198
is<font color="#E5E5E5"> sound so if it proves something it</font>

86
00:04:28,070 --> 00:04:32,750
holds but it's incomplete<font color="#CCCCCC"> it usually</font>

87
00:04:30,199 --> 00:04:34,310
won't prove everything<font color="#CCCCCC"> you can and the</font>

88
00:04:32,750 --> 00:04:36,310
<font color="#E5E5E5">ingredients very</font><font color="#CCCCCC"> freaked interpretation</font>

89
00:04:34,310 --> 00:04:39,220
are you have some abstract domain and

90
00:04:36,310 --> 00:04:42,260
each<font color="#E5E5E5"> element of the</font><font color="#CCCCCC"> expert</font><font color="#E5E5E5"> domain</font>

91
00:04:39,220 --> 00:04:45,110
represents some set you can think some

92
00:04:42,260 --> 00:04:46,880
<font color="#E5E5E5">geometric shape and you have this gamma</font>

93
00:04:45,110 --> 00:04:50,110
function that tells<font color="#E5E5E5"> you which shape it</font>

94
00:04:46,880 --> 00:04:53,060
represents<font color="#E5E5E5"> and in</font><font color="#CCCCCC"> FFA transformers that</font>

95
00:04:50,110 --> 00:04:55,400
preserve<font color="#E5E5E5"> this approximation property</font><font color="#CCCCCC"> so</font>

96
00:04:53,060 --> 00:04:59,180
if you have something<font color="#CCCCCC"> that approximates</font>

97
00:04:55,400 --> 00:05:00,710
a and we take the image<font color="#E5E5E5"> through F then</font>

98
00:04:59,180 --> 00:05:02,810
there to transform of F will give us

99
00:05:00,710 --> 00:05:05,570
something whose concretization<font color="#CCCCCC"> is an</font>

100
00:05:02,810 --> 00:05:10,160
over approximation to the actual outputs

101
00:05:05,570 --> 00:05:12,620
<font color="#E5E5E5">and after domains</font><font color="#CCCCCC"> have some standard set</font>

102
00:05:10,160 --> 00:05:16,330
of<font color="#E5E5E5"> built-in operations about we need or</font>

103
00:05:12,620 --> 00:05:19,490
<font color="#E5E5E5">this meet operator which just intersects</font>

104
00:05:16,330 --> 00:05:21,020
<font color="#E5E5E5">given a</font><font color="#CCCCCC"> certain element with some linear</font>

105
00:05:19,490 --> 00:05:23,930
constraints with a join operator<font color="#CCCCCC"> that</font>

106
00:05:21,020 --> 00:05:26,030
combines<font color="#CCCCCC"> two effort elements and gives</font>

107
00:05:23,930 --> 00:05:27,409
you back a combination and<font color="#E5E5E5"> we have an F</font>

108
00:05:26,030 --> 00:05:28,820
<font color="#CCCCCC">fine transformer which is just</font><font color="#E5E5E5"> an</font>

109
00:05:27,409 --> 00:05:30,560
<font color="#CCCCCC">external form for an affine</font>

110
00:05:28,820 --> 00:05:34,789
transformation so there will be some

111
00:05:30,560 --> 00:05:36,380
<font color="#E5E5E5">more images for</font><font color="#CCCCCC"> example this one</font><font color="#E5E5E5"> so what</font>

112
00:05:34,789 --> 00:05:38,120
this<font color="#CCCCCC"> shows is that there are</font><font color="#E5E5E5"> actually</font>

113
00:05:36,380 --> 00:05:40,280
<font color="#E5E5E5">different types</font><font color="#CCCCCC"> of domains one</font><font color="#E5E5E5"> of them</font>

114
00:05:38,120 --> 00:05:42,590
is just<font color="#E5E5E5"> the</font><font color="#CCCCCC"> Box domain is illustrated in</font>

115
00:05:40,280 --> 00:05:44,750
blue and this<font color="#E5E5E5"> is</font><font color="#CCCCCC"> what standard interval</font>

116
00:05:42,590 --> 00:05:46,849
arithmetic will give you so it will tell

117
00:05:44,750 --> 00:05:50,330
you that the<font color="#CCCCCC"> output is somewhere in this</font>

118
00:05:46,849 --> 00:05:52,430
<font color="#E5E5E5">box then in green we have the so</font><font color="#CCCCCC"> no top</font>

119
00:05:50,330 --> 00:05:55,180
abstract domain and so no<font color="#E5E5E5"> dope is just</font>

120
00:05:52,430 --> 00:05:58,820
an<font color="#E5E5E5"> image of an M dimensional hyper cube</font>

121
00:05:55,180 --> 00:06:01,659
<font color="#E5E5E5">under some affine transformation so it's</font>

122
00:05:58,820 --> 00:06:07,039
some<font color="#E5E5E5"> sort of a symmetric polytope</font><font color="#CCCCCC"> and</font>

123
00:06:01,659 --> 00:06:09,740
yes okay can we go backwards<font color="#E5E5E5"> Thanks</font><font color="#CCCCCC"> so</font>

124
00:06:07,039 --> 00:06:15,740
we have the polyhedra domain which is

125
00:06:09,740 --> 00:06:17,060
<font color="#E5E5E5">just some convex set and we have four</font>

126
00:06:15,740 --> 00:06:20,060
points<font color="#CCCCCC"> here the blue points they</font><font color="#E5E5E5"> are</font>

127
00:06:17,060 --> 00:06:21,529
approximated then we apply some<font color="#E5E5E5"> function</font>

128
00:06:20,060 --> 00:06:25,279
and<font color="#E5E5E5"> we will get</font>

129
00:06:21,529 --> 00:06:27,558
some other shape and all the four points

130
00:06:25,279 --> 00:06:33,409
<font color="#CCCCCC">will still be in this new shape and this</font>

131
00:06:27,559 --> 00:06:34,639
is fortitude interpretation does so now

132
00:06:33,409 --> 00:06:36,919
what<font color="#CCCCCC"> are neural</font><font color="#E5E5E5"> nets</font><font color="#CCCCCC"> and all the</font>

133
00:06:34,639 --> 00:06:39,559
networks are just some compositions of

134
00:06:36,919 --> 00:06:41,058
layers<font color="#E5E5E5"> and for us</font><font color="#CCCCCC"> each</font><font color="#E5E5E5"> layer is either a</font>

135
00:06:39,559 --> 00:06:43,369
fully connected<font color="#CCCCCC"> layer a convolutional</font>

136
00:06:41,059 --> 00:06:44,809
layer or a max pooling layer and our

137
00:06:43,369 --> 00:06:46,519
goal will be to<font color="#CCCCCC"> define a few</font>

138
00:06:44,809 --> 00:06:48,139
<font color="#E5E5E5">transformers for each layer type because</font>

139
00:06:46,519 --> 00:06:50,029
<font color="#E5E5E5">everything imputation is compositional</font>

140
00:06:48,139 --> 00:06:52,159
so if you can do<font color="#E5E5E5"> it for two functions</font>

141
00:06:50,029 --> 00:06:56,319
you can<font color="#E5E5E5"> do it</font><font color="#CCCCCC"> further</font><font color="#E5E5E5"> composition it may</font>

142
00:06:52,159 --> 00:06:56,319
not<font color="#E5E5E5"> be the best way to do it but you can</font>

143
00:06:59,589 --> 00:07:03,709
so this is a<font color="#CCCCCC"> high-level illustration of</font>

144
00:07:02,119 --> 00:07:06,679
<font color="#CCCCCC">how everything</font><font color="#E5E5E5"> rotation of neural</font>

145
00:07:03,709 --> 00:07:08,809
networks we<font color="#E5E5E5"> have an image of an</font><font color="#CCCCCC"> eight</font>

146
00:07:06,679 --> 00:07:10,698
and<font color="#E5E5E5"> this</font><font color="#CCCCCC"> well</font><font color="#E5E5E5"> every pixel</font><font color="#CCCCCC"> it is white</font>

147
00:07:08,809 --> 00:07:13,219
probably<font color="#CCCCCC"> you can make it brighter and</font>

148
00:07:10,699 --> 00:07:14,779
you will still<font color="#E5E5E5"> get an eight and this</font>

149
00:07:13,219 --> 00:07:16,849
<font color="#E5E5E5">will give you a number of</font><font color="#CCCCCC"> different</font>

150
00:07:14,779 --> 00:07:18,739
images<font color="#E5E5E5"> here represented by blue points</font>

151
00:07:16,849 --> 00:07:21,589
which you<font color="#E5E5E5"> can over approximate by this</font>

152
00:07:18,739 --> 00:07:23,929
green<font color="#CCCCCC"> abstract element and then</font>

153
00:07:21,589 --> 00:07:26,449
everything can you<font color="#E5E5E5"> go back thanks</font>

154
00:07:23,929 --> 00:07:28,159
everything beyond that is a neural net

155
00:07:26,449 --> 00:07:30,319
<font color="#E5E5E5">so we have a convolutional layer</font>

156
00:07:28,159 --> 00:07:32,149
<font color="#E5E5E5">followed by max pooling layer four fully</font>

157
00:07:30,319 --> 00:07:33,739
connected layer and they will all

158
00:07:32,149 --> 00:07:35,629
operate on the shape and<font color="#E5E5E5"> give you some</font>

159
00:07:33,739 --> 00:07:37,549
<font color="#CCCCCC">auto shape that over approximates the</font>

160
00:07:35,629 --> 00:07:39,289
<font color="#E5E5E5">result so yeah also</font><font color="#CCCCCC"> some red points</font>

161
00:07:37,549 --> 00:07:42,709
those<font color="#CCCCCC"> are points that cannot occur in</font>

162
00:07:39,289 --> 00:07:44,829
<font color="#CCCCCC">practice but are included as possible</font>

163
00:07:42,709 --> 00:07:49,989
behaviors by a strict interpretation and

164
00:07:44,829 --> 00:07:49,989
define the property you will check on<font color="#CCCCCC"> a4</font>

165
00:07:50,949 --> 00:07:57,979
<font color="#E5E5E5">okay so you will</font><font color="#CCCCCC"> nest usually use this</font>

166
00:07:55,969 --> 00:07:59,569
sort of<font color="#CCCCCC"> Rayleigh activation or the</font><font color="#E5E5E5"> model</font>

167
00:07:57,979 --> 00:08:01,188
nonlinear activation function<font color="#E5E5E5"> what</font>

168
00:07:59,569 --> 00:08:03,559
really does is just<font color="#E5E5E5"> it picks all the</font>

169
00:08:01,189 --> 00:08:05,479
neurons<font color="#CCCCCC"> layer and caps them to zero so</font>

170
00:08:03,559 --> 00:08:10,969
if<font color="#E5E5E5"> they're negative the activations will</font>

171
00:08:05,479 --> 00:08:14,589
become zero<font color="#E5E5E5"> perfect thanks and the</font>

172
00:08:10,969 --> 00:08:17,839
electric transformer<font color="#E5E5E5"> therefore just</font>

173
00:08:14,589 --> 00:08:19,279
<font color="#E5E5E5">intersect so it's it's a composition</font><font color="#CCCCCC"> of</font>

174
00:08:17,839 --> 00:08:21,559
<font color="#E5E5E5">n transformers one for each of the</font>

175
00:08:19,279 --> 00:08:23,629
neurons and<font color="#E5E5E5"> it just intersects the</font>

176
00:08:21,559 --> 00:08:25,219
effort element with these<font color="#E5E5E5"> conditions</font>

177
00:08:23,629 --> 00:08:27,199
that<font color="#CCCCCC"> it's smaller than zero or</font><font color="#E5E5E5"> larger</font>

178
00:08:25,219 --> 00:08:29,539
<font color="#E5E5E5">equal to zero if it's more than zero it</font>

179
00:08:27,199 --> 00:08:33,360
will set it<font color="#CCCCCC"> to zero using the effect</font>

180
00:08:29,539 --> 00:08:35,669
transformer and this is an<font color="#E5E5E5"> illustration</font>

181
00:08:33,360 --> 00:08:37,860
if you have an abstract element and we

182
00:08:35,669 --> 00:08:39,779
split<font color="#E5E5E5"> it into two you can see</font><font color="#CCCCCC"> that each</font>

183
00:08:37,860 --> 00:08:42,299
of<font color="#E5E5E5"> those two is no approximation to what</font>

184
00:08:39,779 --> 00:08:44,670
<font color="#E5E5E5">it would get if you intersect it</font>

185
00:08:42,299 --> 00:08:46,860
actually with<font color="#E5E5E5"> this constraint then we</font>

186
00:08:44,670 --> 00:08:48,628
apply the relevant transformations to

187
00:08:46,860 --> 00:08:49,649
both of them and join<font color="#CCCCCC"> them together</font><font color="#E5E5E5"> in</font>

188
00:08:48,629 --> 00:08:54,929
the end<font color="#CCCCCC"> to</font><font color="#E5E5E5"> get something that</font>

189
00:08:49,649 --> 00:08:56,279
approximates both<font color="#E5E5E5"> branches and for</font><font color="#CCCCCC"> auto</font>

190
00:08:54,929 --> 00:08:58,439
layer types I mean<font color="#CCCCCC"> for fully connected</font>

191
00:08:56,279 --> 00:09:00,329
layers is easy so fully connected layers

192
00:08:58,439 --> 00:09:02,998
<font color="#CCCCCC">just an F and transformation followed by</font>

193
00:09:00,329 --> 00:09:04,949
a radioactive<font color="#E5E5E5"> ation</font><font color="#CCCCCC"> so we can just</font><font color="#E5E5E5"> use</font>

194
00:09:02,999 --> 00:09:07,259
the<font color="#CCCCCC"> FI n transformer which is provided</font>

195
00:09:04,949 --> 00:09:09,569
<font color="#E5E5E5">by the extra domain and follow it</font><font color="#CCCCCC"> with</font>

196
00:09:07,259 --> 00:09:13,429
the<font color="#CCCCCC"> railroad transformer then there</font><font color="#E5E5E5"> also</font>

197
00:09:09,569 --> 00:09:16,229
convolutional layers and convolutions

198
00:09:13,429 --> 00:09:18,540
<font color="#E5E5E5">neural networks sometimes convolve a</font>

199
00:09:16,230 --> 00:09:20,549
given layer with<font color="#CCCCCC"> some multiple filters</font>

200
00:09:18,540 --> 00:09:22,709
and these filters are actually just

201
00:09:20,549 --> 00:09:26,189
<font color="#E5E5E5">linear functions so we can obtain some</font>

202
00:09:22,709 --> 00:09:27,868
sparse matrix such that<font color="#E5E5E5"> this convolution</font>

203
00:09:26,189 --> 00:09:29,189
layers<font color="#E5E5E5"> actually equivalent</font><font color="#CCCCCC"> to fully</font>

204
00:09:27,869 --> 00:09:31,980
connect layered and<font color="#E5E5E5"> we use the same</font>

205
00:09:29,189 --> 00:09:33,779
<font color="#E5E5E5">approach and we also have next</font><font color="#CCCCCC"> pooling</font>

206
00:09:31,980 --> 00:09:37,499
layers but in the interest of time you

207
00:09:33,779 --> 00:09:39,179
can see in<font color="#CCCCCC"> the paper if</font><font color="#E5E5E5"> interested we've</font>

208
00:09:37,499 --> 00:09:41,429
implemented<font color="#E5E5E5"> this approach it was</font>

209
00:09:39,179 --> 00:09:43,259
initially based on the apron library now

210
00:09:41,429 --> 00:09:47,220
it's based on<font color="#CCCCCC"> alena which is in-house</font>

211
00:09:43,259 --> 00:09:48,869
and about to be observed is<font color="#CCCCCC"> that is so</font>

212
00:09:47,220 --> 00:09:50,129
not all abstract domain strikes a good

213
00:09:48,869 --> 00:09:58,019
balance between precision<font color="#E5E5E5"> and</font>

214
00:09:50,129 --> 00:10:01,169
scalability so we evaluated<font color="#CCCCCC"> our approach</font>

215
00:09:58,019 --> 00:10:03,299
of<font color="#E5E5E5"> course so here you can see results on</font>

216
00:10:01,169 --> 00:10:07,679
a<font color="#CCCCCC"> convolutional neural network</font><font color="#E5E5E5"> it's a</font>

217
00:10:03,299 --> 00:10:09,569
lot of<font color="#CCCCCC"> neurons we showed that</font><font color="#E5E5E5"> the</font><font color="#CCCCCC"> PAC's</font>

218
00:10:07,679 --> 00:10:11,610
domain is fast it's not<font color="#E5E5E5"> top domain and</font>

219
00:10:09,569 --> 00:10:13,649
we've also tried polyhedra domain but it

220
00:10:11,610 --> 00:10:16,799
was too slow so<font color="#CCCCCC"> it didn't terminate at</font>

221
00:10:13,649 --> 00:10:19,249
all this took<font color="#CCCCCC"> place</font><font color="#E5E5E5"> in one hour</font>

222
00:10:16,799 --> 00:10:21,299
but it also results are<font color="#E5E5E5"> out-of-date and</font>

223
00:10:19,249 --> 00:10:22,410
<font color="#CCCCCC">now we can do actually a little</font><font color="#E5E5E5"> bit</font>

224
00:10:21,299 --> 00:10:27,389
better than<font color="#CCCCCC"> that</font><font color="#E5E5E5"> does in terms of</font>

225
00:10:22,410 --> 00:10:29,669
<font color="#E5E5E5">precision then we've also compared to</font>

226
00:10:27,389 --> 00:10:31,709
existing<font color="#E5E5E5"> methods to certify things</font><font color="#CCCCCC"> that</font>

227
00:10:29,669 --> 00:10:35,269
<font color="#CCCCCC">are</font><font color="#E5E5E5"> SMT based methods that actually</font><font color="#CCCCCC"> just</font>

228
00:10:31,709 --> 00:10:37,768
solve a set of<font color="#E5E5E5"> constraints and are</font>

229
00:10:35,269 --> 00:10:39,689
complete in theory but<font color="#E5E5E5"> of course will</font>

230
00:10:37,769 --> 00:10:43,079
<font color="#CCCCCC">timeout to be a set a</font><font color="#E5E5E5"> timeout of</font><font color="#CCCCCC"> one</font>

231
00:10:39,689 --> 00:10:45,629
hour here<font color="#E5E5E5"> and as you can see</font><font color="#CCCCCC"> we</font><font color="#E5E5E5"> loop Lex</font>

232
00:10:43,079 --> 00:10:46,650
shows the sort of very exponential

233
00:10:45,629 --> 00:10:49,350
scaling

234
00:10:46,650 --> 00:10:53,250
whereas with<font color="#E5E5E5"> abstract interpretation</font><font color="#CCCCCC"> you</font>

235
00:10:49,350 --> 00:10:55,080
don't see<font color="#E5E5E5"> it at early so we</font><font color="#CCCCCC"> have here so</font>

236
00:10:53,250 --> 00:10:56,820
<font color="#E5E5E5">no top domain</font><font color="#CCCCCC"> restaurant of 64 which is</font>

237
00:10:55,080 --> 00:10:58,710
<font color="#CCCCCC">the same as soon</font><font color="#E5E5E5"> at all but you keep up</font>

238
00:10:56,820 --> 00:11:02,460
to 64 different<font color="#E5E5E5"> snow tops to get some</font>

239
00:10:58,710 --> 00:11:04,170
more precision<font color="#E5E5E5"> and as you can see our</font>

240
00:11:02,460 --> 00:11:06,060
approach<font color="#E5E5E5"> actually verifies more than we</font>

241
00:11:04,170 --> 00:11:13,110
do Plex<font color="#CCCCCC"> within this time limit of one</font>

242
00:11:06,060 --> 00:11:14,819
hour in the<font color="#E5E5E5"> future or actually</font><font color="#CCCCCC"> right now</font>

243
00:11:13,110 --> 00:11:17,400
we're using<font color="#CCCCCC"> effort interpretation to</font>

244
00:11:14,820 --> 00:11:19,380
train the neural network<font color="#CCCCCC"> we're adding</font>

245
00:11:17,400 --> 00:11:20,939
support for more activation<font color="#CCCCCC"> functions so</font>

246
00:11:19,380 --> 00:11:24,810
not just<font color="#CCCCCC"> rare</font><font color="#E5E5E5"> lupa does the</font><font color="#CCCCCC"> sigmoid and</font>

247
00:11:20,940 --> 00:11:26,430
<font color="#CCCCCC">10h and we are improving</font><font color="#E5E5E5"> the after</font>

248
00:11:24,810 --> 00:11:27,780
transformers<font color="#CCCCCC"> but specializing them to</font>

249
00:11:26,430 --> 00:11:29,640
the abstract<font color="#CCCCCC"> domain so everything I've</font>

250
00:11:27,780 --> 00:11:32,130
seen<font color="#CCCCCC"> today's</font><font color="#E5E5E5"> are generic they can</font>

251
00:11:29,640 --> 00:11:34,020
actually<font color="#E5E5E5"> do better by doing</font><font color="#CCCCCC"> a direct</font>

252
00:11:32,130 --> 00:11:36,420
<font color="#E5E5E5">transformers directly for this on the</font>

253
00:11:34,020 --> 00:11:38,689
tops<font color="#CCCCCC"> and you're also deriving data</font>

254
00:11:36,420 --> 00:11:41,790
parallel life<font color="#E5E5E5"> certain formers</font>

255
00:11:38,690 --> 00:11:44,130
so to summarize neural networks are<font color="#E5E5E5"> not</font>

256
00:11:41,790 --> 00:11:46,829
robust<font color="#CCCCCC"> we can use abstract</font>

257
00:11:44,130 --> 00:11:50,670
interpretation<font color="#E5E5E5"> to show that for</font><font color="#CCCCCC"> specific</font>

258
00:11:46,830 --> 00:11:53,250
images you cannot get the necessary

259
00:11:50,670 --> 00:11:56,459
perturbations<font color="#E5E5E5"> we have defined after</font>

260
00:11:53,250 --> 00:12:00,210
transformers for common layers and our

261
00:11:56,460 --> 00:12:02,910
approach is<font color="#E5E5E5"> quite precise and scales so</font>

262
00:12:00,210 --> 00:12:07,580
<font color="#E5E5E5">I will be happy</font><font color="#CCCCCC"> to answer</font><font color="#E5E5E5"> questions</font><font color="#CCCCCC"> also</font>

263
00:12:02,910 --> 00:12:14,519
<font color="#CCCCCC">offline if you like thank you very much</font>

264
00:12:07,580 --> 00:12:14,519
[Applause]

265
00:12:16,639 --> 00:12:22,769
<font color="#CCCCCC">okay we have some time - for some</font>

266
00:12:20,009 --> 00:12:25,949
questions<font color="#E5E5E5"> and I'd like to start with</font>

267
00:12:22,769 --> 00:12:27,959
with one question<font color="#E5E5E5"> so you said that this</font>

268
00:12:25,949 --> 00:12:30,889
type of analysis is<font color="#E5E5E5"> obviously not not</font>

269
00:12:27,959 --> 00:12:34,349
complete so based on<font color="#E5E5E5"> your experience</font>

270
00:12:30,889 --> 00:12:37,769
<font color="#CCCCCC">what are some examples where this</font>

271
00:12:34,350 --> 00:12:39,329
approach fails to find the proof well

272
00:12:37,769 --> 00:12:43,259
that they<font color="#E5E5E5"> are all in the paper actually</font>

273
00:12:39,329 --> 00:12:44,910
<font color="#E5E5E5">so we have improved approach now and for</font>

274
00:12:43,259 --> 00:12:49,230
<font color="#E5E5E5">some of the benchmarks</font><font color="#CCCCCC"> we can actually</font>

275
00:12:44,910 --> 00:12:53,309
<font color="#CCCCCC">now</font><font color="#E5E5E5"> prove</font><font color="#CCCCCC"> 100 percent of them so so</font>

276
00:12:49,230 --> 00:12:57,990
basically<font color="#E5E5E5"> the takeaway is that you can</font>

277
00:12:53,309 --> 00:13:01,170
get<font color="#E5E5E5"> more or less precision based on what</font>

278
00:12:57,990 --> 00:13:10,139
your reference for<font color="#CCCCCC"> matter I think we</font>

279
00:13:01,170 --> 00:13:13,079
have a question about the evaluation<font color="#CCCCCC"> it</font>

280
00:13:10,139 --> 00:13:15,089
shows that<font color="#CCCCCC"> it seems only a one year come</font>

281
00:13:13,079 --> 00:13:16,290
loose<font color="#CCCCCC"> in your network</font><font color="#E5E5E5"> have you tried</font>

282
00:13:15,089 --> 00:13:18,240
larger networks

283
00:13:16,290 --> 00:13:21,990
<font color="#E5E5E5">this was just an illustrative example</font>

284
00:13:18,240 --> 00:13:24,720
this is<font color="#E5E5E5"> not what we evaluated on a lot</font>

285
00:13:21,990 --> 00:13:26,550
of layers we had feed-forward neural

286
00:13:24,720 --> 00:13:29,730
<font color="#CCCCCC">nets with nine layers for example</font>

287
00:13:26,550 --> 00:13:31,258
actually<font color="#E5E5E5"> and the convolution</font><font color="#CCCCCC"> lept i</font>

288
00:13:29,730 --> 00:13:39,000
don't remember<font color="#E5E5E5"> how many layers you type</font>

289
00:13:31,259 --> 00:13:41,519
but<font color="#CCCCCC"> i think like</font><font color="#E5E5E5"> six or something okay</font>

290
00:13:39,000 --> 00:13:45,930
<font color="#CCCCCC">so for the experiment</font><font color="#E5E5E5"> I didn't hear</font><font color="#CCCCCC"> it</font>

291
00:13:41,519 --> 00:13:48,360
<font color="#CCCCCC">clearly</font><font color="#E5E5E5"> so what is</font><font color="#CCCCCC"> that sorry</font><font color="#E5E5E5"> what is</font>

292
00:13:45,930 --> 00:13:51,239
the data set<font color="#CCCCCC"> at the data set</font>

293
00:13:48,360 --> 00:13:54,199
very good point so it's<font color="#CCCCCC"> eminency for ten</font>

294
00:13:51,240 --> 00:13:54,199
<font color="#E5E5E5">oh thank you</font>

295
00:13:58,650 --> 00:14:04,250
<font color="#E5E5E5">all right then</font><font color="#CCCCCC"> if there are</font><font color="#E5E5E5"> no more</font>

296
00:14:01,120 --> 00:14:08,940
<font color="#CCCCCC">questions then let's thank the speaker</font>

297
00:14:04,250 --> 00:14:08,940
[Applause]

