1
00:00:08,600 --> 00:00:12,700
so hello everyone I

2
00:00:13,299 --> 00:00:18,320
coming to this talk about

3
00:00:15,950 --> 00:00:22,160
fence against adverse oral example with

4
00:00:18,320 --> 00:00:25,490
some provable guarantees so you probably

5
00:00:22,160 --> 00:00:27,169
all know that already but recent

6
00:00:25,490 --> 00:00:30,200
progress in machine learning and in

7
00:00:27,169 --> 00:00:32,239
particular deep neural networks have led

8
00:00:30,200 --> 00:00:35,930
to them having really good performance

9
00:00:32,238 --> 00:00:38,540
at many tasks and because of that

10
00:00:35,930 --> 00:00:43,010
they're increasingly deployed including

11
00:00:38,540 --> 00:00:46,730
in attack chrome context and recent

12
00:00:43,010 --> 00:00:49,400
example of that is when Taylor Swift was

13
00:00:46,730 --> 00:00:52,040
using facial recognition at her context

14
00:00:49,400 --> 00:00:54,680
to identify stalkers for instance and in

15
00:00:52,040 --> 00:00:56,810
the same article that I should here they

16
00:00:54,680 --> 00:00:59,900
say that big companies like Ticketmaster

17
00:00:56,810 --> 00:01:03,350
are investigating actually replacing

18
00:00:59,900 --> 00:01:05,689
tickets with face recognition so let's

19
00:01:03,350 --> 00:01:07,990
use that last one as a running example

20
00:01:05,689 --> 00:01:13,158
and see how it could work

21
00:01:07,990 --> 00:01:15,560
so let's say I arrive at a concert and I

22
00:01:13,159 --> 00:01:18,350
face a camera the camera is interested

23
00:01:15,560 --> 00:01:22,399
in a deep neural net and it scores my

24
00:01:18,350 --> 00:01:25,369
face for each ticket and you know to

25
00:01:22,399 --> 00:01:27,710
start with the the network works pretty

26
00:01:25,369 --> 00:01:30,889
well so if I show up without a ticket

27
00:01:27,710 --> 00:01:33,408
the top score given by the neural net

28
00:01:30,889 --> 00:01:38,389
will be no ticket and I won't be granted

29
00:01:33,409 --> 00:01:40,069
access however it's been showed times

30
00:01:38,389 --> 00:01:42,709
and times again that these deep neural

31
00:01:40,069 --> 00:01:43,850
networks are a very very vulnerable to a

32
00:01:42,709 --> 00:01:47,299
type of attack called

33
00:01:43,850 --> 00:01:50,240
adversarial examples in which the

34
00:01:47,299 --> 00:01:52,159
attacker does a small change to the

35
00:01:50,240 --> 00:01:54,949
input so in that case maybe it's like

36
00:01:52,159 --> 00:01:57,259
some very light makeup that I designed

37
00:01:54,950 --> 00:02:00,020
or a hat that cast some special shadow

38
00:01:57,259 --> 00:02:02,899
on my face and this very small change

39
00:02:00,020 --> 00:02:05,479
triggers a drastic change in the scores

40
00:02:02,899 --> 00:02:08,240
given by the neural net so that I'm now

41
00:02:05,479 --> 00:02:11,840
recognized as the proud owner of ticket

42
00:02:08,240 --> 00:02:14,470
two and I'm granted access and you know

43
00:02:11,840 --> 00:02:17,690
the type of attacks works really well

44
00:02:14,470 --> 00:02:20,930
basically against any input in many

45
00:02:17,690 --> 00:02:24,049
contexts so here is an example of the

46
00:02:20,930 --> 00:02:27,079
image net Network released by Google for

47
00:02:24,049 --> 00:02:28,520
the image net data set and what I'm

48
00:02:27,079 --> 00:02:31,970
showing is the actor

49
00:02:28,520 --> 00:02:36,830
see under an increasing size of

50
00:02:31,970 --> 00:02:39,109
adversarial example attack and what we

51
00:02:36,830 --> 00:02:41,660
can see is that under no attack the

52
00:02:39,110 --> 00:02:43,820
network has roughly 78% of accuracy

53
00:02:41,660 --> 00:02:47,329
which is pretty good for that dataset

54
00:02:43,820 --> 00:02:50,630
but as soon as the attack starts the

55
00:02:47,330 --> 00:02:54,470
accuracy completely collapses to 7% for

56
00:02:50,630 --> 00:02:57,200
attacks up to size 0.5 and 1% of size 1

57
00:02:54,470 --> 00:02:59,780
and to give you an example of what it

58
00:02:57,200 --> 00:03:02,600
means this is how with an attack of size

59
00:02:59,780 --> 00:03:04,940
0.5 you can turn this giant panda into a

60
00:03:02,600 --> 00:03:06,910
teddy bear and with an attack of size 1

61
00:03:04,940 --> 00:03:10,070
you can turn it into a teapot and

62
00:03:06,910 --> 00:03:13,340
basically the changes are invisible to

63
00:03:10,070 --> 00:03:16,340
the naked eye so I think given those

64
00:03:13,340 --> 00:03:18,980
results if we're deploying such a model

65
00:03:16,340 --> 00:03:21,410
in an attack prone context we shouldn't

66
00:03:18,980 --> 00:03:25,570
really treat it as if it had 78% of

67
00:03:21,410 --> 00:03:28,640
accuracy but more like 7% or even 1% and

68
00:03:25,570 --> 00:03:32,000
so in that case I think a key question

69
00:03:28,640 --> 00:03:36,920
is is there any accuracy we can rely on

70
00:03:32,000 --> 00:03:40,580
under attack and there's been quite a

71
00:03:36,920 --> 00:03:44,720
lot of work recently into trying to

72
00:03:40,580 --> 00:03:47,540
answer that question and most of it has

73
00:03:44,720 --> 00:03:50,240
been with best effort approaches where

74
00:03:47,540 --> 00:03:52,900
as we've just seen we can actually run

75
00:03:50,240 --> 00:03:56,000
an attack to try to evaluate how robust

76
00:03:52,900 --> 00:03:58,610
model is and or such attacks and if we

77
00:03:56,000 --> 00:04:01,070
want to improve this robustness well we

78
00:03:58,610 --> 00:04:03,800
can try many approaches one of the

79
00:04:01,070 --> 00:04:06,260
popular one is to read keep training the

80
00:04:03,800 --> 00:04:09,950
model under on attacked images to try to

81
00:04:06,260 --> 00:04:12,230
make it better at classifying them but

82
00:04:09,950 --> 00:04:14,839
the big problem is that both those steps

83
00:04:12,230 --> 00:04:16,789
or attack specific which led to an arms

84
00:04:14,840 --> 00:04:21,738
race between attacker and defender which

85
00:04:16,790 --> 00:04:23,990
basically attackers have been winning so

86
00:04:21,738 --> 00:04:27,140
I think the key questions we want to

87
00:04:23,990 --> 00:04:30,380
answer is first can we guarantee any

88
00:04:27,140 --> 00:04:33,650
accuracy like can we say for any attack

89
00:04:30,380 --> 00:04:37,729
below that size this is the accuracy you

90
00:04:33,650 --> 00:04:40,880
can rely on and second maybe even more

91
00:04:37,729 --> 00:04:42,560
important can we for a given prediction

92
00:04:40,880 --> 00:04:44,750
say how hard it is

93
00:04:42,560 --> 00:04:47,090
to attack why is that second one

94
00:04:44,750 --> 00:04:49,940
important because in this access control

95
00:04:47,090 --> 00:04:52,130
example maybe when the prediction is

96
00:04:49,940 --> 00:04:54,770
robust it's hard to attack you can let

97
00:04:52,130 --> 00:04:56,870
the person in automatically but if the

98
00:04:54,770 --> 00:05:01,960
prediction is not robust a guard can

99
00:04:56,870 --> 00:05:01,960
come and check the actual concert ticket

100
00:05:02,050 --> 00:05:08,300
now you may have heard about a few

101
00:05:05,180 --> 00:05:10,940
recent work to answer those questions

102
00:05:08,300 --> 00:05:13,340
with provable guarantees but the key

103
00:05:10,940 --> 00:05:16,100
challenge is that they don't scale well

104
00:05:13,340 --> 00:05:18,979
in terms of three dimensions the input

105
00:05:16,100 --> 00:05:22,010
size the size of the neural network and

106
00:05:18,979 --> 00:05:23,780
the size of the training data and given

107
00:05:22,010 --> 00:05:25,490
the recent trend of training larger and

108
00:05:23,780 --> 00:05:27,859
larger models and bigger and bigger data

109
00:05:25,490 --> 00:05:30,560
sets it means that we can't use them on

110
00:05:27,860 --> 00:05:35,450
the models we actually want to use for

111
00:05:30,560 --> 00:05:39,950
the test setting so my defense that I

112
00:05:35,450 --> 00:05:43,180
call pixel DP gives provable guarantees

113
00:05:39,950 --> 00:05:45,770
of robustness for norm bounded attack

114
00:05:43,180 --> 00:05:48,260
using a different approach based on a

115
00:05:45,770 --> 00:05:50,659
privacy mechanism called differential

116
00:05:48,260 --> 00:05:54,260
privacy and it turns out that this

117
00:05:50,660 --> 00:05:56,870
approach is by far the most scalable and

118
00:05:54,260 --> 00:05:58,969
for the first time we were able to give

119
00:05:56,870 --> 00:06:03,229
guarantees of robustness for large

120
00:05:58,970 --> 00:06:05,990
models and imagenet so let's see a bit

121
00:06:03,229 --> 00:06:09,760
how we can design a deploy net for which

122
00:06:05,990 --> 00:06:14,090
we can prove some robustness guarantees

123
00:06:09,760 --> 00:06:17,000
so as we saw in the example the t the

124
00:06:14,090 --> 00:06:19,190
core of the problem is that a small

125
00:06:17,000 --> 00:06:22,070
change to the input of the neural net

126
00:06:19,190 --> 00:06:24,500
can trigger a drastic change in the

127
00:06:22,070 --> 00:06:28,039
scores and here we'll define a small

128
00:06:24,500 --> 00:06:31,250
change as the size of the change made by

129
00:06:28,039 --> 00:06:31,909
the attacker is below pore size we can

130
00:06:31,250 --> 00:06:34,280
parametrize

131
00:06:31,910 --> 00:06:37,910
in two norms so the regular Euclidean

132
00:06:34,280 --> 00:06:39,830
distance in in pixel space and the key

133
00:06:37,910 --> 00:06:42,979
idea is to design a deep neural net

134
00:06:39,830 --> 00:06:46,609
where we can bound how much the scores

135
00:06:42,979 --> 00:06:49,340
can change under arbitrary small changes

136
00:06:46,610 --> 00:06:52,430
to the input and this is where we

137
00:06:49,340 --> 00:06:54,229
leverage differential privacy so

138
00:06:52,430 --> 00:06:55,889
differential privacy is a technique to

139
00:06:54,229 --> 00:06:58,710
randomize a computation

140
00:06:55,889 --> 00:07:01,909
typically and made on a database such

141
00:06:58,710 --> 00:07:04,378
that's changing one row of that database

142
00:07:01,909 --> 00:07:07,289
leads to boundary changes in the

143
00:07:04,379 --> 00:07:08,969
distribution over possible outputs so

144
00:07:07,289 --> 00:07:10,919
intuitively if you're trying to like

145
00:07:08,969 --> 00:07:13,259
count the number of people in a database

146
00:07:10,919 --> 00:07:15,270
with a given attribute adding or

147
00:07:13,259 --> 00:07:17,939
removing one row can only change that

148
00:07:15,270 --> 00:07:20,818
can buy at most one so if you add a bit

149
00:07:17,939 --> 00:07:23,969
of noise and observe the result of that

150
00:07:20,819 --> 00:07:25,830
algorithm is hard to say if it was

151
00:07:23,969 --> 00:07:30,240
computing on the database with or

152
00:07:25,830 --> 00:07:32,508
without the extra person and this is the

153
00:07:30,240 --> 00:07:35,340
this bound on what you can learn is what

154
00:07:32,509 --> 00:07:40,110
expressed in this differential privacy

155
00:07:35,340 --> 00:07:42,810
formula now is that enough to prove some

156
00:07:40,110 --> 00:07:45,270
robustness for a deep neural net

157
00:07:42,810 --> 00:07:47,189
predictions well not quite because we

158
00:07:45,270 --> 00:07:49,378
have a randomized mechanism so it would

159
00:07:47,189 --> 00:07:50,879
be like randomized prediction and then

160
00:07:49,379 --> 00:07:53,330
we have a guarantee on the probability

161
00:07:50,879 --> 00:07:56,430
but that's not quite what we want

162
00:07:53,330 --> 00:07:59,248
however for a differentially private

163
00:07:56,430 --> 00:08:01,560
mechanism with bounded outputs between 0

164
00:07:59,249 --> 00:08:04,069
& 1 like the scores given by those

165
00:08:01,560 --> 00:08:07,169
neural net what we can prove is that the

166
00:08:04,069 --> 00:08:10,199
expected score also follow the same

167
00:08:07,169 --> 00:08:12,359
bounds and this is what we can use to

168
00:08:10,199 --> 00:08:16,259
bound how much scores can change under

169
00:08:12,360 --> 00:08:17,759
arbitrary change to the inputs so just

170
00:08:16,259 --> 00:08:20,849
to summarize a bit what I've just said

171
00:08:17,759 --> 00:08:22,469
we can make this prediction function the

172
00:08:20,849 --> 00:08:25,139
deep neural net differentially private

173
00:08:22,469 --> 00:08:28,469
and then we need to predict the expected

174
00:08:25,139 --> 00:08:31,289
scores of that function and with the

175
00:08:28,469 --> 00:08:33,299
result I just showed we can bound for

176
00:08:31,289 --> 00:08:35,399
arbitrary changes to the input how much

177
00:08:33,299 --> 00:08:37,948
the scores can change why is it

178
00:08:35,399 --> 00:08:40,049
interesting because if those bounds

179
00:08:37,948 --> 00:08:41,939
don't overlap we basically prove the

180
00:08:40,049 --> 00:08:44,490
prediction is robust to arbitrary

181
00:08:41,940 --> 00:08:46,500
attacks below a given size because the

182
00:08:44,490 --> 00:08:48,360
best the attacker can do is lower the

183
00:08:46,500 --> 00:08:50,100
top score to this lower bound and

184
00:08:48,360 --> 00:08:51,959
increase the other scores to the upper

185
00:08:50,100 --> 00:08:53,940
bound and because they don't overlap is

186
00:08:51,959 --> 00:08:57,810
not enough to change the top score or

187
00:08:53,940 --> 00:08:59,930
the prediction now I kind of said like

188
00:08:57,810 --> 00:09:02,010
Oh make this prediction function

189
00:08:59,930 --> 00:09:04,469
differentially private then predict the

190
00:09:02,010 --> 00:09:08,189
expected score how can we do that in

191
00:09:04,470 --> 00:09:08,830
practice well the first thing is to make

192
00:09:08,190 --> 00:09:10,780
it

193
00:09:08,830 --> 00:09:13,270
differentially private so we have to add

194
00:09:10,780 --> 00:09:16,000
some noise in the computation that we do

195
00:09:13,270 --> 00:09:18,340
with a nice layer that ensures that this

196
00:09:16,000 --> 00:09:22,150
first part of the computation up to the

197
00:09:18,340 --> 00:09:24,520
noise is differentially private now the

198
00:09:22,150 --> 00:09:26,560
second part is actually an ear post

199
00:09:24,520 --> 00:09:29,170
processing of a differentially private

200
00:09:26,560 --> 00:09:30,520
output and there's a well known result

201
00:09:29,170 --> 00:09:32,829
in differential privacy that it's

202
00:09:30,520 --> 00:09:35,920
resilient to post processing so now the

203
00:09:32,830 --> 00:09:37,600
scores the finest in the final scores

204
00:09:35,920 --> 00:09:42,310
given by the neural net are also

205
00:09:37,600 --> 00:09:44,170
differentially private and the other

206
00:09:42,310 --> 00:09:47,010
thing that we need is the compute the

207
00:09:44,170 --> 00:09:49,030
expected output of this raw net

208
00:09:47,010 --> 00:09:51,040
unfortunately we have this very complex

209
00:09:49,030 --> 00:09:53,020
plus processing so we can't do that

210
00:09:51,040 --> 00:09:55,089
analytically so we'll just use a

211
00:09:53,020 --> 00:09:56,860
standard Monte Carlo estimate where we

212
00:09:55,090 --> 00:09:59,260
run the model many times compute the

213
00:09:56,860 --> 00:10:01,300
average except now it's not the true

214
00:09:59,260 --> 00:10:03,550
expectation so we need to add some

215
00:10:01,300 --> 00:10:05,290
confidence intervals where where we know

216
00:10:03,550 --> 00:10:07,719
the true expectation is with high

217
00:10:05,290 --> 00:10:09,699
probability we include that in our

218
00:10:07,720 --> 00:10:13,320
bounds and if they still don't overlap

219
00:10:09,700 --> 00:10:16,570
we prove that the prediction is robust

220
00:10:13,320 --> 00:10:18,430
now there's still many challenges to get

221
00:10:16,570 --> 00:10:20,560
actually good predictions out of this

222
00:10:18,430 --> 00:10:23,380
model I won't have time to describe them

223
00:10:20,560 --> 00:10:25,900
today but there's one I want to focus on

224
00:10:23,380 --> 00:10:28,030
a bit which is scale so I told you we

225
00:10:25,900 --> 00:10:30,220
could scale to imagenet which is a

226
00:10:28,030 --> 00:10:32,439
fairly high resolution classification

227
00:10:30,220 --> 00:10:35,050
dataset for large models like this

228
00:10:32,440 --> 00:10:38,250
inception model release pre-trained by

229
00:10:35,050 --> 00:10:40,449
Google so how can we do that

230
00:10:38,250 --> 00:10:42,490
well it was really spree trained by

231
00:10:40,450 --> 00:10:44,710
Google so it would be nice to be able to

232
00:10:42,490 --> 00:10:48,070
leverage that work already done and use

233
00:10:44,710 --> 00:10:51,010
it as is right well it turns out that

234
00:10:48,070 --> 00:10:53,110
what we can do is train what we call an

235
00:10:51,010 --> 00:10:55,450
autoencoder so it's a model that just

236
00:10:53,110 --> 00:10:57,340
tries to recreate its own input so try

237
00:10:55,450 --> 00:11:00,880
to learn the identity function except

238
00:10:57,340 --> 00:11:02,650
now with our nice layer we trained a

239
00:11:00,880 --> 00:11:04,810
very small one and imaginate so it's

240
00:11:02,650 --> 00:11:07,810
very fast like much faster than training

241
00:11:04,810 --> 00:11:10,329
inception from scratch and then we can

242
00:11:07,810 --> 00:11:12,699
just stack it in front of that pre

243
00:11:10,330 --> 00:11:14,980
trained model we need to do a few steps

244
00:11:12,700 --> 00:11:17,290
of training again like a fine-tuning

245
00:11:14,980 --> 00:11:19,870
steps but much less than to retrain from

246
00:11:17,290 --> 00:11:22,089
scratch and this is where like the the

247
00:11:19,870 --> 00:11:25,509
magics of differential privacy helper

248
00:11:22,089 --> 00:11:27,550
where now all these giant computation

249
00:11:25,509 --> 00:11:29,800
that goes through an entire model is an

250
00:11:27,550 --> 00:11:32,229
e post processing so the guarantees we

251
00:11:29,800 --> 00:11:34,029
added carry up to the end of the model

252
00:11:32,230 --> 00:11:36,279
and we have the same guarantees as

253
00:11:34,029 --> 00:11:41,800
before and that's actually how we could

254
00:11:36,279 --> 00:11:48,699
support image net cool so now let's see

255
00:11:41,800 --> 00:11:50,529
a bit how it works so we evaluate we

256
00:11:48,699 --> 00:11:53,649
evaluated pixel D P like trying to

257
00:11:50,529 --> 00:11:56,680
answer many questions today I'll touch

258
00:11:53,649 --> 00:11:58,990
on three of them which is like well can

259
00:11:56,680 --> 00:12:01,779
we guarantee any accuracy on the red tag

260
00:11:58,990 --> 00:12:04,120
for image net or those robust

261
00:12:01,779 --> 00:12:06,459
predictions harder to attack in practice

262
00:12:04,120 --> 00:12:10,209
and how does it compare with existing

263
00:12:06,459 --> 00:12:13,628
methods and we run that evaluation and

264
00:12:10,209 --> 00:12:16,029
five data sets three models looking

265
00:12:13,629 --> 00:12:18,730
mostly at the guarantee accuracy we can

266
00:12:16,029 --> 00:12:22,029
guarantee and then the accuracy under

267
00:12:18,730 --> 00:12:24,100
attack and for that last one we used the

268
00:12:22,029 --> 00:12:27,249
state-of-the-art attack and strengthen

269
00:12:24,100 --> 00:12:29,319
it for our method because to take into

270
00:12:27,249 --> 00:12:33,790
account the fact that we add noise in

271
00:12:29,319 --> 00:12:36,639
the computation so first this is the

272
00:12:33,790 --> 00:12:38,319
results we get an image net the baseline

273
00:12:36,639 --> 00:12:41,110
is the pre trained version that we

274
00:12:38,319 --> 00:12:43,990
release then this shows two pixel DP

275
00:12:41,110 --> 00:12:46,839
models with increasing noise and the

276
00:12:43,990 --> 00:12:50,319
main takeaway is that we can guarantee

277
00:12:46,839 --> 00:12:54,220
an accuracy of 40% for arbitrary attacks

278
00:12:50,319 --> 00:12:56,860
below size 0.2 so and just to be clear

279
00:12:54,220 --> 00:12:59,829
that like all models and data sets

280
00:12:56,860 --> 00:13:02,309
orders of magnitudes larger than what we

281
00:12:59,829 --> 00:13:05,859
could do before so that's pretty great

282
00:13:02,309 --> 00:13:09,009
now would I deploy that in my access

283
00:13:05,860 --> 00:13:11,679
control example well like 40 percent is

284
00:13:09,009 --> 00:13:15,670
still pretty low so maybe not yet

285
00:13:11,679 --> 00:13:18,249
there's still work to be done maybe but

286
00:13:15,670 --> 00:13:20,979
what I also said is that if we could

287
00:13:18,249 --> 00:13:23,769
guarantee or if we could measure the

288
00:13:20,980 --> 00:13:27,069
robustness of any single prediction what

289
00:13:23,769 --> 00:13:29,499
we could do in that context is just say

290
00:13:27,069 --> 00:13:31,990
well if the prediction is robust I can

291
00:13:29,499 --> 00:13:33,450
let the person in and if it's not robust

292
00:13:31,990 --> 00:13:36,630
maybe a human

293
00:13:33,450 --> 00:13:38,970
can come in the loop and in that case

294
00:13:36,630 --> 00:13:41,790
what we care about is the robust

295
00:13:38,970 --> 00:13:43,740
accuracy only on those is the accuracy

296
00:13:41,790 --> 00:13:45,990
under attack only on those robust

297
00:13:43,740 --> 00:13:48,210
prediction so this is what I show on

298
00:13:45,990 --> 00:13:51,630
this graph for a robustness threshold of

299
00:13:48,210 --> 00:13:53,550
size 0.05 and what we can see is that

300
00:13:51,630 --> 00:13:56,130
compared to the baseline the undefended

301
00:13:53,550 --> 00:13:58,949
model those robust predictions are much

302
00:13:56,130 --> 00:14:02,700
harder to attack so we still get a

303
00:13:58,950 --> 00:14:07,200
pretty good accuracy of 80% for attacks

304
00:14:02,700 --> 00:14:09,210
of size up to 0.4 now there are some

305
00:14:07,200 --> 00:14:11,580
predictions that are not robust so we

306
00:14:09,210 --> 00:14:13,490
are not going to be able to let the

307
00:14:11,580 --> 00:14:17,520
person in automatically in the concert

308
00:14:13,490 --> 00:14:21,830
how many is that well we can still make

309
00:14:17,520 --> 00:14:25,140
robust predictions for about 70% of the

310
00:14:21,830 --> 00:14:27,780
of all the the images so that's that's

311
00:14:25,140 --> 00:14:30,000
pretty good and of course what we can do

312
00:14:27,780 --> 00:14:32,250
is increase that that robustness

313
00:14:30,000 --> 00:14:35,100
threshold in which case we'll make less

314
00:14:32,250 --> 00:14:38,090
predictions like around 40% here but

315
00:14:35,100 --> 00:14:40,470
they'll be even harder to attack and

316
00:14:38,090 --> 00:14:43,320
what what we see is that the accuracy

317
00:14:40,470 --> 00:14:48,570
under attack is even higher than before

318
00:14:43,320 --> 00:14:50,700
and just to do a quick comparison some

319
00:14:48,570 --> 00:14:53,730
best-effort attacks that work well in

320
00:14:50,700 --> 00:14:55,770
practice but don't have this property of

321
00:14:53,730 --> 00:14:59,550
being able to say for a given prediction

322
00:14:55,770 --> 00:15:01,710
how hard it is to attack mean that they

323
00:14:59,550 --> 00:15:03,810
cannot distinguish between more robust

324
00:15:01,710 --> 00:15:08,340
and less robust prediction and so get

325
00:15:03,810 --> 00:15:13,380
less accuracy under attack and finally

326
00:15:08,340 --> 00:15:16,070
just to quickly talk about scale this is

327
00:15:13,380 --> 00:15:19,110
a comparison with the state-of-the-art

328
00:15:16,070 --> 00:15:21,840
robust defenses at the time and what we

329
00:15:19,110 --> 00:15:24,270
can see is because with pixel DP we are

330
00:15:21,840 --> 00:15:27,360
able to train much larger models we can

331
00:15:24,270 --> 00:15:31,040
get both like better accuracy and more

332
00:15:27,360 --> 00:15:31,040
robust accuracy under attack

333
00:15:31,250 --> 00:15:37,680
so to summarize pixel DPS the first

334
00:15:35,070 --> 00:15:39,990
defense that both gives attack

335
00:15:37,680 --> 00:15:41,349
independent guarantees for arbitrary

336
00:15:39,990 --> 00:15:44,080
attacks under

337
00:15:41,350 --> 00:15:47,260
in size and scales to the largest models

338
00:15:44,080 --> 00:15:50,110
and dataset and what I'm pretty excited

339
00:15:47,260 --> 00:15:52,689
about is that there's already quite a

340
00:15:50,110 --> 00:15:54,640
bit of follow-up work where people

341
00:15:52,690 --> 00:15:57,700
either increase the bounds that are

342
00:15:54,640 --> 00:15:59,560
given noise level or looked at other

343
00:15:57,700 --> 00:16:02,860
noise distribution how to adapt

344
00:15:59,560 --> 00:16:05,079
optimization so I'm hoping that this is

345
00:16:02,860 --> 00:16:07,870
a new direction for defenses against

346
00:16:05,080 --> 00:16:10,780
adverse aural examples so on that note

347
00:16:07,870 --> 00:16:20,230
thank you for coming and I'm happy to

348
00:16:10,780 --> 00:16:24,850
answer questions so things so other

349
00:16:20,230 --> 00:16:28,920
questions in the room please come to the

350
00:16:24,850 --> 00:16:28,920
microphone and ask

351
00:16:38,990 --> 00:16:41,940
could you move a little bit to the

352
00:16:40,830 --> 00:16:43,920
microphone Thanks

353
00:16:41,940 --> 00:16:53,880
well I can't repeat the question

354
00:16:43,920 --> 00:16:56,729
otherwise yeah that's a great question

355
00:16:53,880 --> 00:16:58,650
so the question is since it's based on

356
00:16:56,730 --> 00:17:00,750
differential privacy does it also

357
00:16:58,650 --> 00:17:02,819
provide differential privacy at training

358
00:17:00,750 --> 00:17:07,140
time and prevent attacks such as model

359
00:17:02,820 --> 00:17:10,680
inversion attacks the answer as these is

360
00:17:07,140 --> 00:17:14,430
no at least there's no there's no formal

361
00:17:10,680 --> 00:17:17,310
guarantee that we can get on the train

362
00:17:14,430 --> 00:17:19,980
model I think it might be possible to

363
00:17:17,310 --> 00:17:21,569
adapt it a bit to get some but it I

364
00:17:19,980 --> 00:17:24,920
think they would not be very good and

365
00:17:21,569 --> 00:17:28,679
it's really not the best way to get

366
00:17:24,920 --> 00:17:30,690
differential privacy at learning time so

367
00:17:28,680 --> 00:17:32,700
it's purely at prediction time that we

368
00:17:30,690 --> 00:17:36,720
get the guarantees from and we do train

369
00:17:32,700 --> 00:17:38,760
with noise to get good predictions but

370
00:17:36,720 --> 00:17:43,910
not in a way that gives differential

371
00:17:38,760 --> 00:17:46,910
privacy further questions

372
00:17:43,910 --> 00:17:46,910
please

373
00:17:52,130 --> 00:18:01,019
wait wait you can use mine thanks a lot

374
00:17:59,220 --> 00:18:03,690
for the talking was a very nice talk so

375
00:18:01,019 --> 00:18:05,669
I'm not so familiar with these sort of

376
00:18:03,690 --> 00:18:07,470
topics so I just maybe maybe a very

377
00:18:05,669 --> 00:18:09,630
naive question but like it seems that

378
00:18:07,470 --> 00:18:12,029
you are sort of preventing attacks like

379
00:18:09,630 --> 00:18:14,039
additive attacks with small norm right

380
00:18:12,029 --> 00:18:17,399
because intuitively this should model

381
00:18:14,039 --> 00:18:18,960
like the act of modifying an image so

382
00:18:17,399 --> 00:18:20,699
that the naked eye cannot really

383
00:18:18,960 --> 00:18:23,429
distinguish it from the original but

384
00:18:20,700 --> 00:18:25,500
cannot you modify an image like by for

385
00:18:23,429 --> 00:18:26,970
example modifying up one pixel but like

386
00:18:25,500 --> 00:18:29,669
very drastically so that the normal

387
00:18:26,970 --> 00:18:31,350
actually is not very small but still it

388
00:18:29,669 --> 00:18:34,649
the naked eye cannot distinguish yeah

389
00:18:31,350 --> 00:18:38,279
that's that's a great question so here I

390
00:18:34,649 --> 00:18:39,809
focused mostly on norm two attacks which

391
00:18:38,279 --> 00:18:42,480
is like where we did most of the work

392
00:18:39,809 --> 00:18:44,850
but what you're describing would be

393
00:18:42,480 --> 00:18:47,549
maybe a norm one attack where you count

394
00:18:44,850 --> 00:18:49,980
the power of the attacker by just the

395
00:18:47,549 --> 00:18:51,779
size of how much they change the pixel

396
00:18:49,980 --> 00:18:54,720
so they could change like a few pixels

397
00:18:51,779 --> 00:18:57,179
by a lot it's pretty easy to fit that in

398
00:18:54,720 --> 00:19:00,809
the framework and get guarantees against

399
00:18:57,179 --> 00:19:04,730
those attacks - maybe not both at the

400
00:19:00,809 --> 00:19:08,220
same time but and and there's other

401
00:19:04,730 --> 00:19:11,610
norms to measure attacks that are harder

402
00:19:08,220 --> 00:19:14,070
to to support under this framework but

403
00:19:11,610 --> 00:19:16,830
still I think with some work we could do

404
00:19:14,070 --> 00:19:18,360
it I think that kind of a follow-up

405
00:19:16,830 --> 00:19:21,149
thing that would be really interesting

406
00:19:18,360 --> 00:19:23,340
is to measure the power of the attacker

407
00:19:21,149 --> 00:19:24,928
in a more meaningful way than just like

408
00:19:23,340 --> 00:19:27,418
the norm of how much they change the

409
00:19:24,929 --> 00:19:31,799
pixels but as far as I know that still

410
00:19:27,419 --> 00:19:33,269
an open question how to do that okay so

411
00:19:31,799 --> 00:19:34,240
let's thank the speaker again thank my

412
00:19:33,269 --> 00:19:38,779
tears again

413
00:19:34,240 --> 00:19:38,779
[Applause]

