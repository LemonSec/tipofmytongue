1
00:00:08,830 --> 00:00:14,250
hi I'm melody I'm here to present

2
00:00:12,420 --> 00:00:16,680
in privacy analysis of the learning

3
00:00:14,250 --> 00:00:18,990
model I did this work with collaboration

4
00:00:16,680 --> 00:00:22,380
of my adviser Amir under the chakra from

5
00:00:18,990 --> 00:00:24,240
National University of Singapore so by

6
00:00:22,380 --> 00:00:26,790
now we know that deep learning is a

7
00:00:24,240 --> 00:00:29,418
pretty powerful technique and it's being

8
00:00:26,790 --> 00:00:32,549
used in a critical task and in many

9
00:00:29,419 --> 00:00:35,490
tasks there are sensitive data so in

10
00:00:32,549 --> 00:00:37,650
this work we are doing our goal is to do

11
00:00:35,490 --> 00:00:40,230
a privacy analysis of different deep

12
00:00:37,650 --> 00:00:43,500
learning scenarios and learning settings

13
00:00:40,230 --> 00:00:46,769
so our main objective is to somehow

14
00:00:43,500 --> 00:00:48,750
measure the privacy leakage of training

15
00:00:46,770 --> 00:00:52,350
a deep learning model on some sensitive

16
00:00:48,750 --> 00:00:54,530
data set so in part we will focus on it

17
00:00:52,350 --> 00:00:58,079
a specific type of attacks which called

18
00:00:54,530 --> 00:01:01,050
membership inference attacks and so you

19
00:00:58,079 --> 00:01:03,179
you are familiar with the membership in

20
00:01:01,050 --> 00:01:05,640
ferrous like from previous talks but if

21
00:01:03,179 --> 00:01:07,470
for to train a deep learning model we

22
00:01:05,640 --> 00:01:09,840
first have a data distribution we will

23
00:01:07,470 --> 00:01:11,730
pick some samples from that distribution

24
00:01:09,840 --> 00:01:14,550
we call it training data we will train

25
00:01:11,730 --> 00:01:17,250
our model and then the main observation

26
00:01:14,550 --> 00:01:19,170
that allows membership I first like is

27
00:01:17,250 --> 00:01:21,659
that deep learning models will behave

28
00:01:19,170 --> 00:01:23,520
differently from the samples of the

29
00:01:21,659 --> 00:01:26,130
training data compared to the samples of

30
00:01:23,520 --> 00:01:28,079
a just data distribution and these

31
00:01:26,130 --> 00:01:30,570
difference distribution the output

32
00:01:28,079 --> 00:01:32,789
vector allows an attacker to launch an

33
00:01:30,570 --> 00:01:34,740
attack and no identify if if some

34
00:01:32,790 --> 00:01:37,890
specific instance was used in the

35
00:01:34,740 --> 00:01:40,169
training or not and we called example

36
00:01:37,890 --> 00:01:41,880
which was part of change a member

37
00:01:40,170 --> 00:01:44,880
sample and if it wasn't a part of

38
00:01:41,880 --> 00:01:48,060
training a non-member shop but we want

39
00:01:44,880 --> 00:01:49,979
to go a step forward and we want to see

40
00:01:48,060 --> 00:01:52,380
what is the cause of this behavior what

41
00:01:49,979 --> 00:01:55,890
will make deep learning to behave

42
00:01:52,380 --> 00:01:59,100
differently for these scenarios so let's

43
00:01:55,890 --> 00:02:00,960
take a look at how a deep learning model

44
00:01:59,100 --> 00:02:03,000
will be trained so when you define a

45
00:02:00,960 --> 00:02:05,149
model you your mother will have a set of

46
00:02:03,000 --> 00:02:07,829
features a set of parameters which and

47
00:02:05,149 --> 00:02:09,810
then you will define a loss function you

48
00:02:07,829 --> 00:02:12,269
will use a some variation of SGD

49
00:02:09,810 --> 00:02:16,290
algorithm to train your mother and so

50
00:02:12,270 --> 00:02:17,940
for these this basic sample assume we

51
00:02:16,290 --> 00:02:20,760
have a hypothetical scenario and we have

52
00:02:17,940 --> 00:02:24,120
a toothless of red and blue and and you

53
00:02:20,760 --> 00:02:26,069
use 3d for several steps and then you

54
00:02:24,120 --> 00:02:28,109
train the model and this green

55
00:02:26,069 --> 00:02:30,988
is your decision boundary after a few

56
00:02:28,109 --> 00:02:34,829
steps of STD and let's assume we are

57
00:02:30,989 --> 00:02:36,569
going to the next step of a street the

58
00:02:34,829 --> 00:02:38,489
algorithm you will take the next

59
00:02:36,569 --> 00:02:41,429
training batch you will add the data

60
00:02:38,489 --> 00:02:43,950
point you cut again computer at the end

61
00:02:41,430 --> 00:02:46,019
the main idea here is that it should is

62
00:02:43,950 --> 00:02:47,099
that you will always change your model

63
00:02:46,019 --> 00:02:50,310
parameters in the opposite direction

64
00:02:47,099 --> 00:02:52,589
after idea so your decision boundary

65
00:02:50,310 --> 00:02:54,540
will be changed to make the correct

66
00:02:52,590 --> 00:02:57,299
classification for this state new data

67
00:02:54,540 --> 00:02:59,400
point so and sometimes the street is not

68
00:02:57,299 --> 00:03:01,709
perfect you will go and make something

69
00:02:59,400 --> 00:03:03,659
some other mistakes so the answer it is

70
00:03:01,709 --> 00:03:05,579
a iterative algorithm you will again

71
00:03:03,659 --> 00:03:08,250
compute do the same thing several times

72
00:03:05,579 --> 00:03:11,040
and after a few iterations will converge

73
00:03:08,250 --> 00:03:13,579
to a good point and you will be happy

74
00:03:11,040 --> 00:03:19,798
with your model and will use it in your

75
00:03:13,579 --> 00:03:22,409
training so then next so let's assume

76
00:03:19,799 --> 00:03:24,659
that after you train the model you will

77
00:03:22,409 --> 00:03:26,939
we will have a new data point which were

78
00:03:24,659 --> 00:03:28,858
not part of your training it what is the

79
00:03:26,939 --> 00:03:31,290
difference between these two points so

80
00:03:28,859 --> 00:03:33,870
one main difference is that you use the

81
00:03:31,290 --> 00:03:36,298
you use all of your changes points to

82
00:03:33,870 --> 00:03:39,569
reduce the loss so you will have a much

83
00:03:36,299 --> 00:03:41,489
smaller lost between that remember data

84
00:03:39,569 --> 00:03:44,099
points compared to the nominal data

85
00:03:41,489 --> 00:03:46,349
points one and as a result the gradient

86
00:03:44,099 --> 00:03:48,689
vectors will have a much smaller size so

87
00:03:46,349 --> 00:03:51,720
if you compute the gradients for the

88
00:03:48,689 --> 00:03:53,430
blue circle you will have a smaller good

89
00:03:51,720 --> 00:03:55,799
integral compared to case when you

90
00:03:53,430 --> 00:03:59,810
compare to compute it for a non-member

91
00:03:55,799 --> 00:04:02,609
data and so this is the one of the main

92
00:03:59,810 --> 00:04:06,120
reasons that we are going to hypothesize

93
00:04:02,609 --> 00:04:11,849
that leak information about the training

94
00:04:06,120 --> 00:04:13,769
it and to validate our and so gradient

95
00:04:11,849 --> 00:04:16,130
will behave differently from member and

96
00:04:13,769 --> 00:04:19,320
non-member data so to evaluate these

97
00:04:16,130 --> 00:04:21,000
hypothesis we did a simple test that we

98
00:04:19,320 --> 00:04:23,610
compute the gradient norm for a

99
00:04:21,000 --> 00:04:27,330
different for a member and non-member

100
00:04:23,610 --> 00:04:29,610
difference we saw that your member data

101
00:04:27,330 --> 00:04:32,130
points will have a much smaller gradient

102
00:04:29,610 --> 00:04:37,289
norm and as a result it shows that okay

103
00:04:32,130 --> 00:04:39,810
yeah it's it's confidence that remember

104
00:04:37,289 --> 00:04:42,330
that member have a much smaller gradient

105
00:04:39,810 --> 00:04:45,300
and the non-member with the serpents has

106
00:04:42,330 --> 00:04:47,969
much larger and this is allow the

107
00:04:45,300 --> 00:04:50,160
attacker to be able to differentiate

108
00:04:47,970 --> 00:04:53,040
between these two points and launches

109
00:04:50,160 --> 00:04:55,080
membership inference attacks so we use

110
00:04:53,040 --> 00:04:56,730
this idea in a different server deep

111
00:04:55,080 --> 00:04:59,310
learning setting such as fully trained

112
00:04:56,730 --> 00:05:01,290
in both white box black box fine-tuning

113
00:04:59,310 --> 00:05:04,050
and deep and federated learning settings

114
00:05:01,290 --> 00:05:09,060
so further absolutely further learning

115
00:05:04,050 --> 00:05:12,210
settings is a new approach and it's

116
00:05:09,060 --> 00:05:14,100
going so let you are familiar with the

117
00:05:12,210 --> 00:05:16,229
federal learning settings we have you

118
00:05:14,100 --> 00:05:18,090
having some collaborators each color to

119
00:05:16,230 --> 00:05:20,940
have a set up training in private

120
00:05:18,090 --> 00:05:22,799
realtor no one was to share state and it

121
00:05:20,940 --> 00:05:27,930
will use some aggregate or in the middle

122
00:05:22,800 --> 00:05:30,060
to train the and an aggregated mother so

123
00:05:27,930 --> 00:05:32,010
the first difference between a federated

124
00:05:30,060 --> 00:05:34,050
learning setting compared to a normal

125
00:05:32,010 --> 00:05:36,710
learning thing is that you have a

126
00:05:34,050 --> 00:05:39,540
multiple observation of the training

127
00:05:36,710 --> 00:05:42,750
model parameter so we can see that for

128
00:05:39,540 --> 00:05:45,780
example your model will change its its

129
00:05:42,750 --> 00:05:48,000
parameter from around some point in the

130
00:05:45,780 --> 00:05:49,739
data space and you can somehow I think

131
00:05:48,000 --> 00:05:52,200
you can somehow find out that there is a

132
00:05:49,740 --> 00:05:54,930
point in the training go in the training

133
00:05:52,200 --> 00:05:56,849
data set of that a specific collaborator

134
00:05:54,930 --> 00:05:58,440
for example so this is the main

135
00:05:56,850 --> 00:06:00,360
difference for this is the first

136
00:05:58,440 --> 00:06:04,080
difference from the federated learning

137
00:06:00,360 --> 00:06:07,229
setting and normal fully trained model

138
00:06:04,080 --> 00:06:09,900
so we will go one step forward and we

139
00:06:07,229 --> 00:06:12,479
want to design an active attack if if

140
00:06:09,900 --> 00:06:14,159
the attacker is one of the elements in

141
00:06:12,479 --> 00:06:16,620
the federated learning setting he can

142
00:06:14,160 --> 00:06:19,430
influence the learning in a way to leak

143
00:06:16,620 --> 00:06:23,700
more information about the training so

144
00:06:19,430 --> 00:06:25,560
the we call or attack a gradient

145
00:06:23,700 --> 00:06:28,320
eccentric and the main idea is that

146
00:06:25,560 --> 00:06:29,910
attacker will do a gradient update but

147
00:06:28,320 --> 00:06:31,349
instead of going opposite in the

148
00:06:29,910 --> 00:06:33,030
opposite direction of gradient he will

149
00:06:31,350 --> 00:06:37,740
go in the direction of the gradient to

150
00:06:33,030 --> 00:06:40,770
force the collaborator to to compensate

151
00:06:37,740 --> 00:06:44,100
for his attack to the for this change so

152
00:06:40,770 --> 00:06:46,770
in a normal scenario if the some point

153
00:06:44,100 --> 00:06:49,169
is a part of the membership part of the

154
00:06:46,770 --> 00:06:51,630
training data the collaborator will

155
00:06:49,169 --> 00:06:53,008
computer Steve is going in opposite

156
00:06:51,630 --> 00:06:55,169
direction of the gradient

157
00:06:53,009 --> 00:06:58,199
and you'd want to change the decision

158
00:06:55,169 --> 00:06:59,938
boundary to have it correct

159
00:06:58,199 --> 00:07:02,279
classification but the active attacker

160
00:06:59,939 --> 00:07:05,219
here will change the parameters in the

161
00:07:02,279 --> 00:07:07,169
opposite direction so this is the this

162
00:07:05,219 --> 00:07:09,210
is the case when you have the point in

163
00:07:07,169 --> 00:07:10,889
you're changing it but what what will

164
00:07:09,210 --> 00:07:13,409
happen if that that point is not in your

165
00:07:10,889 --> 00:07:15,360
chain details so if the attacker will do

166
00:07:13,409 --> 00:07:17,490
the gradient ascent and it will change

167
00:07:15,360 --> 00:07:20,699
the model parameter in the opposite in

168
00:07:17,490 --> 00:07:23,189
the direction but there is no point the

169
00:07:20,699 --> 00:07:25,349
collaborative won't won't be able to the

170
00:07:23,189 --> 00:07:26,939
component set for this action and as a

171
00:07:25,349 --> 00:07:29,909
result something like this will happen

172
00:07:26,939 --> 00:07:33,749
and so that you will have a much higher

173
00:07:29,909 --> 00:07:36,808
loss for that points that we use to to

174
00:07:33,749 --> 00:07:39,330
do to to use to apply your addicts and

175
00:07:36,809 --> 00:07:41,939
they call those attacks a set up so the

176
00:07:39,330 --> 00:07:44,248
main scenario here is that attacker has

177
00:07:41,939 --> 00:07:46,409
a set of target points he will apply the

178
00:07:44,249 --> 00:07:49,559
gradient ascent on those target points

179
00:07:46,409 --> 00:07:53,308
and the goal is to increase the loss of

180
00:07:49,559 --> 00:07:55,830
the increase the loss of the model and

181
00:07:53,309 --> 00:07:57,719
find out if that point was part of the

182
00:07:55,830 --> 00:08:03,318
training that of a color collaborator or

183
00:07:57,719 --> 00:08:06,058
not and so and basic observation is that

184
00:08:03,319 --> 00:08:06,599
HUD will if some point was part of

185
00:08:06,059 --> 00:08:08,490
deterring it

186
00:08:06,599 --> 00:08:11,009
HUD will compensate for this attacker

187
00:08:08,490 --> 00:08:12,659
and nothing will happen as you can see

188
00:08:11,009 --> 00:08:16,050
but if wasn't part of the training data

189
00:08:12,659 --> 00:08:18,449
you can see that the model which behave

190
00:08:16,050 --> 00:08:21,240
different so again we applied this

191
00:08:18,449 --> 00:08:23,550
attack and we saw that for example for

192
00:08:21,240 --> 00:08:26,159
if some point wasn't putting part of the

193
00:08:23,550 --> 00:08:30,149
training data but it was a part of

194
00:08:26,159 --> 00:08:32,338
attack air targets so attack yedid

195
00:08:30,149 --> 00:08:35,510
occurred in ascent on it you can see

196
00:08:32,339 --> 00:08:38,250
that the gradient norm will increase

197
00:08:35,510 --> 00:08:41,370
compared to the case that it wasn't part

198
00:08:38,250 --> 00:08:45,089
of the norm it targeted us so when we

199
00:08:41,370 --> 00:08:48,149
compare the so the attacker has a easier

200
00:08:45,089 --> 00:08:50,220
way of detecting that point from the and

201
00:08:48,149 --> 00:08:52,709
find out that point wasn't part of

202
00:08:50,220 --> 00:08:54,690
changes and for the other points that

203
00:08:52,709 --> 00:08:57,510
was part of changes that we can see that

204
00:08:54,690 --> 00:08:59,940
our hypothesis is correct at the

205
00:08:57,510 --> 00:09:02,760
gradient on won't change that much and

206
00:08:59,940 --> 00:09:05,939
as a result again the gradient vector

207
00:09:02,760 --> 00:09:06,780
itself will be similar in both cases so

208
00:09:05,939 --> 00:09:10,969
these

209
00:09:06,780 --> 00:09:14,579
- these are main ideas of the attack and

210
00:09:10,970 --> 00:09:16,470
so we are we upload our attacks in a

211
00:09:14,580 --> 00:09:19,590
different scenario so I will give you a

212
00:09:16,470 --> 00:09:20,910
- three of the main ones here and

213
00:09:19,590 --> 00:09:22,830
possibly have had more attacks in our

214
00:09:20,910 --> 00:09:25,860
paper so the base the first one is the

215
00:09:22,830 --> 00:09:27,960
foolish training models so we attack

216
00:09:25,860 --> 00:09:30,270
account observe the training instances

217
00:09:27,960 --> 00:09:32,370
can just only observed a fully trained

218
00:09:30,270 --> 00:09:34,980
model and he will compute the output

219
00:09:32,370 --> 00:09:37,200
loss and gradients of the model as we

220
00:09:34,980 --> 00:09:39,510
explained and he will he wants to use

221
00:09:37,200 --> 00:09:43,920
that to launch a membership in France

222
00:09:39,510 --> 00:09:46,470
attack and the next so we have two

223
00:09:43,920 --> 00:09:48,270
attacks in the federated settings one is

224
00:09:46,470 --> 00:09:53,370
the central attack when the attacker is

225
00:09:48,270 --> 00:09:56,130
on the sensual a greater and he can

226
00:09:53,370 --> 00:09:58,470
observe you cannot observe the local

227
00:09:56,130 --> 00:10:03,120
force of each each collaborator but he

228
00:09:58,470 --> 00:10:05,640
can as he can't focus on a specific

229
00:10:03,120 --> 00:10:07,830
collaborator and thought and just forget

230
00:10:05,640 --> 00:10:11,040
that updates and save those target to

231
00:10:07,830 --> 00:10:12,870
launch his attacks and one other thing

232
00:10:11,040 --> 00:10:15,270
that essential technique can do is can

233
00:10:12,870 --> 00:10:18,450
isolate the objects from the each

234
00:10:15,270 --> 00:10:21,630
collaborator to don't don't let other

235
00:10:18,450 --> 00:10:23,790
collaborators to affect the parameters

236
00:10:21,630 --> 00:10:26,430
of this a specific collector this way it

237
00:10:23,790 --> 00:10:28,770
will have an easier way to launch his

238
00:10:26,430 --> 00:10:31,349
attack and the other attacker in the

239
00:10:28,770 --> 00:10:33,810
federated setting is a local attack

240
00:10:31,350 --> 00:10:36,030
event he cannot see the obvious of the

241
00:10:33,810 --> 00:10:38,670
different clubs can only observe the

242
00:10:36,030 --> 00:10:40,890
updates from the central a secure and in

243
00:10:38,670 --> 00:10:43,079
all of these attacks attackers see the

244
00:10:40,890 --> 00:10:44,730
gradients and compute and compute

245
00:10:43,080 --> 00:10:46,470
Guardian see the outputs and the loss

246
00:10:44,730 --> 00:10:51,270
for different for all of different

247
00:10:46,470 --> 00:10:53,280
epochs so by now we saw that attacker

248
00:10:51,270 --> 00:10:55,680
car in a different setting attack a kind

249
00:10:53,280 --> 00:10:58,530
of different observation and it can

250
00:10:55,680 --> 00:11:01,079
compute different parameters from the

251
00:10:58,530 --> 00:11:03,480
model so the main goal here is that

252
00:11:01,080 --> 00:11:06,990
attacker give these if observation as an

253
00:11:03,480 --> 00:11:08,760
input the attacker wants and score

254
00:11:06,990 --> 00:11:11,100
function to differentiate between the

255
00:11:08,760 --> 00:11:12,960
member and non-member and be used and

256
00:11:11,100 --> 00:11:15,720
similar to other attack to use a deep

257
00:11:12,960 --> 00:11:17,340
learning model to have a to design this

258
00:11:15,720 --> 00:11:19,470
score function we have a beautiful

259
00:11:17,340 --> 00:11:19,980
different deep learning model but I'm

260
00:11:19,470 --> 00:11:27,390
not going to

261
00:11:19,980 --> 00:11:30,090
explain it so 2x2 evaluated our attacks

262
00:11:27,390 --> 00:11:32,189
being we use the publicly available pre

263
00:11:30,090 --> 00:11:33,720
train models so previous work we usually

264
00:11:32,190 --> 00:11:35,370
trained at their own model

265
00:11:33,720 --> 00:11:37,770
so if then you are training your own

266
00:11:35,370 --> 00:11:40,190
model you can actually somehow force

267
00:11:37,770 --> 00:11:42,329
your mother to leak information but we

268
00:11:40,190 --> 00:11:44,940
since we wanted to show that these

269
00:11:42,330 --> 00:11:47,970
attacks are actually working practice so

270
00:11:44,940 --> 00:11:51,150
we downloaded a state-of-the-art Sai

271
00:11:47,970 --> 00:11:53,910
model for these data sets and we should

272
00:11:51,150 --> 00:11:57,180
launched our attacks also in the case

273
00:11:53,910 --> 00:11:58,949
that we have to learn our models we use

274
00:11:57,180 --> 00:12:00,959
all comment like a localization

275
00:11:58,950 --> 00:12:03,630
technique the implemented our attacks in

276
00:12:00,960 --> 00:12:07,050
fighters and use the common data sets

277
00:12:03,630 --> 00:12:09,600
for these sort of attacks so for the

278
00:12:07,050 --> 00:12:12,000
results I'm going to present that to

279
00:12:09,600 --> 00:12:16,050
main results but there are a lot of more

280
00:12:12,000 --> 00:12:18,270
let so one interesting scenario is that

281
00:12:16,050 --> 00:12:20,579
when you have a black box setting and

282
00:12:18,270 --> 00:12:23,670
you go you say okay I have them other

283
00:12:20,580 --> 00:12:27,120
parameters no I can't compute the output

284
00:12:23,670 --> 00:12:30,420
of all intermediate layers we computed

285
00:12:27,120 --> 00:12:32,550
this attack for all of our data set and

286
00:12:30,420 --> 00:12:35,010
we saw that there is not that much

287
00:12:32,550 --> 00:12:36,990
improvement compared in the case when

288
00:12:35,010 --> 00:12:39,569
you only see the output of the last year

289
00:12:36,990 --> 00:12:43,230
and the case when you see the output of

290
00:12:39,570 --> 00:12:46,140
all of the intermediate this will this

291
00:12:43,230 --> 00:12:47,700
led us to believe that last August of

292
00:12:46,140 --> 00:12:49,500
last year contains the most information

293
00:12:47,700 --> 00:12:52,830
about the training data and there is not

294
00:12:49,500 --> 00:12:54,870
much information there is not that much

295
00:12:52,830 --> 00:12:56,910
more information the other layers but

296
00:12:54,870 --> 00:12:58,860
when we compared the case when we

297
00:12:56,910 --> 00:13:00,959
compute the gradients we can we saw that

298
00:12:58,860 --> 00:13:04,140
there is a big jump in the attack

299
00:13:00,960 --> 00:13:06,150
accuracy so we conclude that actually

300
00:13:04,140 --> 00:13:08,880
gradients leaks information and all of

301
00:13:06,150 --> 00:13:11,340
our hypotheses or about the gratings are

302
00:13:08,880 --> 00:13:12,930
correct and I think you can use gradient

303
00:13:11,340 --> 00:13:16,950
fill it more information about the

304
00:13:12,930 --> 00:13:19,770
training data and also we applied our

305
00:13:16,950 --> 00:13:22,500
attacks in a federated learning setting

306
00:13:19,770 --> 00:13:24,480
and which in both local and global and

307
00:13:22,500 --> 00:13:26,910
all of their active and passive attacks

308
00:13:24,480 --> 00:13:29,910
and first we saw that global attacker

309
00:13:26,910 --> 00:13:31,449
has much higher performance because it's

310
00:13:29,910 --> 00:13:32,920
obviously have a

311
00:13:31,450 --> 00:13:36,220
has easier time there is less

312
00:13:32,920 --> 00:13:38,589
interference and we applied our gradient

313
00:13:36,220 --> 00:13:41,020
ascent attack and be sure that we saw

314
00:13:38,590 --> 00:13:42,880
that there is a much there's a big jump

315
00:13:41,020 --> 00:13:46,120
from the passive attack to the gradient

316
00:13:42,880 --> 00:13:48,010
as an attack and the most powerful

317
00:13:46,120 --> 00:13:51,520
attack is when we combine all of our

318
00:13:48,010 --> 00:13:54,250
attack gradient ascent isolating central

319
00:13:51,520 --> 00:13:56,439
attacker so we thought we saw that we

320
00:13:54,250 --> 00:14:00,100
can lead a lot of information from the

321
00:13:56,440 --> 00:14:09,900
training so that concludes my talk I'm

322
00:14:00,100 --> 00:14:12,070
happy to get questions so questions

323
00:14:09,900 --> 00:14:15,360
maybe I started if somebody has a

324
00:14:12,070 --> 00:14:17,350
question then go to the microphone so

325
00:14:15,360 --> 00:14:19,840
your paper was titled

326
00:14:17,350 --> 00:14:22,570
an analysis and you presented a lot of

327
00:14:19,840 --> 00:14:25,180
attacks so I'm now wondering can I use

328
00:14:22,570 --> 00:14:27,280
any of these techniques in real-world so

329
00:14:25,180 --> 00:14:29,199
not the machine learning part or do you

330
00:14:27,280 --> 00:14:34,510
have any defenses in mind that can help

331
00:14:29,200 --> 00:14:36,520
you so the on so we the only defenses

332
00:14:34,510 --> 00:14:38,470
that will work eventually is

333
00:14:36,520 --> 00:14:40,360
differential privacy in machine learning

334
00:14:38,470 --> 00:14:42,970
but the problem with differential

335
00:14:40,360 --> 00:14:45,280
privacy is that's right now for complex

336
00:14:42,970 --> 00:14:48,490
text tasks we don't get a good utility

337
00:14:45,280 --> 00:14:51,069
so nobody is going to use a differential

338
00:14:48,490 --> 00:14:54,670
private deep learning for a very high

339
00:14:51,070 --> 00:14:57,490
very complex portfolio so but if you

340
00:14:54,670 --> 00:14:59,589
don't use any defensive techniques you

341
00:14:57,490 --> 00:15:02,140
will leak information and ok so these

342
00:14:59,590 --> 00:15:04,090
things are we don't intentionally over

343
00:15:02,140 --> 00:15:07,270
fit our models we use the pre-trained

344
00:15:04,090 --> 00:15:08,830
state-of-the-art models ok oh we have

345
00:15:07,270 --> 00:15:11,620
that's a question I don't have a

346
00:15:08,830 --> 00:15:14,170
question I have an objection so my name

347
00:15:11,620 --> 00:15:17,170
is Oliver Ellington we at Google have

348
00:15:14,170 --> 00:15:18,969
tensorflow privacy and we can train

349
00:15:17,170 --> 00:15:22,420
complex models we have trained complex

350
00:15:18,970 --> 00:15:25,990
models to good utility so which actually

351
00:15:22,420 --> 00:15:29,439
try to apply T playing with differential

352
00:15:25,990 --> 00:15:32,080
privacy on C 400 the best drive the best

353
00:15:29,440 --> 00:15:34,360
utility we got is about 10% something

354
00:15:32,080 --> 00:15:37,000
like that and we tried different so we

355
00:15:34,360 --> 00:15:39,520
can increase the privacy budget to get a

356
00:15:37,000 --> 00:15:42,940
better results but we saw that ok if it

357
00:15:39,520 --> 00:15:45,130
put either get a good Epsilon it's not

358
00:15:42,940 --> 00:15:47,230
oh it's difficult

359
00:15:45,130 --> 00:15:49,780
the moment to tune the hyper parameters

360
00:15:47,230 --> 00:15:53,350
and your mileage will definitely vary on

361
00:15:49,780 --> 00:15:57,430
depending on how hard you can try having

362
00:15:53,350 --> 00:16:00,040
the ability to do a full exploration of

363
00:15:57,430 --> 00:16:02,800
the hyper parameter space helps but also

364
00:16:00,040 --> 00:16:04,839
doing you know various other tricks for

365
00:16:02,800 --> 00:16:07,120
instance with c4 100 you can start with

366
00:16:04,840 --> 00:16:08,410
pre train public models and so on so

367
00:16:07,120 --> 00:16:10,420
there are lots of things you can do but

368
00:16:08,410 --> 00:16:12,760
I just this blanket statement that you

369
00:16:10,420 --> 00:16:18,069
can't have complex models with high

370
00:16:12,760 --> 00:16:20,770
accuracy is not true ok I so it's not

371
00:16:18,070 --> 00:16:23,970
easy to do at least it's not easy yeah

372
00:16:20,770 --> 00:16:29,170
we're working on making it easier yeah

373
00:16:23,970 --> 00:16:30,140
further questions okay so let's thanks

374
00:16:29,170 --> 00:16:34,599
to speak again

375
00:16:30,140 --> 00:16:34,600
[Applause]

