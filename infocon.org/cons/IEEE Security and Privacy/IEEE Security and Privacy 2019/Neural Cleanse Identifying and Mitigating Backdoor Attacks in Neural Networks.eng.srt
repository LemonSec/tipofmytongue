1
00:00:08,650 --> 00:00:13,500
thank you thank you Thanks

2
00:00:11,250 --> 00:00:16,530
it's good to hear that everyone loves

3
00:00:13,500 --> 00:00:19,230
the quick trailer so this is a really

4
00:00:16,530 --> 00:00:21,090
fun work I did with my colleagues and

5
00:00:19,230 --> 00:00:23,939
advisors from UC Santa Barbara in

6
00:00:21,090 --> 00:00:25,560
Chicago and Virginia Tech and actually

7
00:00:23,939 --> 00:00:27,060
it's I still can't believe I'm doing

8
00:00:25,560 --> 00:00:28,680
deep learning right now because a couple

9
00:00:27,060 --> 00:00:34,070
years ago when I first heard about deep

10
00:00:28,680 --> 00:00:34,070
learning my impression was something oh

11
00:00:34,400 --> 00:00:41,100
yeah something like this right so then

12
00:00:39,360 --> 00:00:43,170
of course I started to look into what

13
00:00:41,100 --> 00:00:44,880
deep learning really is and the very

14
00:00:43,170 --> 00:00:47,370
first example I started with was

15
00:00:44,880 --> 00:00:49,590
eminence of course so this is a very

16
00:00:47,370 --> 00:00:51,629
simple task it's a handwritten digit

17
00:00:49,590 --> 00:00:53,630
recognition task and all you need to do

18
00:00:51,630 --> 00:00:56,190
is to throwing this 3 layer Network

19
00:00:53,630 --> 00:00:58,230
trainer with tens tens of thousands of

20
00:00:56,190 --> 00:01:00,570
images and then you would have an almost

21
00:00:58,230 --> 00:01:03,599
perfect model so this was very powerful

22
00:01:00,570 --> 00:01:05,549
to me then I realized that this power of

23
00:01:03,600 --> 00:01:08,100
deep learning models is actually coming

24
00:01:05,549 --> 00:01:10,590
from the complexity of the model so even

25
00:01:08,100 --> 00:01:13,199
for something as simple as this or using

26
00:01:10,590 --> 00:01:15,960
a three layer network that has 10,000

27
00:01:13,200 --> 00:01:18,360
neurons and 25 million weights so it's a

28
00:01:15,960 --> 00:01:21,270
very complicated very complex and

29
00:01:18,360 --> 00:01:24,510
powerful statistical model and the way

30
00:01:21,270 --> 00:01:26,190
this model works is quite confusing it's

31
00:01:24,510 --> 00:01:28,770
quite mysterious to me because it

32
00:01:26,190 --> 00:01:31,380
doesn't really match how I think I would

33
00:01:28,770 --> 00:01:33,390
solve the task in my brain so most of

34
00:01:31,380 --> 00:01:36,270
time we just treat them we just treat

35
00:01:33,390 --> 00:01:38,310
these models as black boxes so then a

36
00:01:36,270 --> 00:01:40,949
very natural follow-up question is that

37
00:01:38,310 --> 00:01:43,049
how do we test these complicated and

38
00:01:40,950 --> 00:01:46,740
black box models how do we make sure

39
00:01:43,049 --> 00:01:48,330
they work as expected the most common

40
00:01:46,740 --> 00:01:50,130
approach the most common testing

41
00:01:48,330 --> 00:01:52,560
approaches of course to throw a bunch of

42
00:01:50,130 --> 00:01:55,229
test samples at a model if the model is

43
00:01:52,560 --> 00:01:57,930
a correct on those test samples then we

44
00:01:55,229 --> 00:02:00,270
think the mall is good to go but this

45
00:01:57,930 --> 00:02:02,369
seems not enough so that's why a lot of

46
00:02:00,270 --> 00:02:05,070
reasons work they're trying to push one

47
00:02:02,369 --> 00:02:06,990
step further they try to explain why

48
00:02:05,070 --> 00:02:10,079
deployment models are making those those

49
00:02:06,990 --> 00:02:12,269
decisions on those samples so Lima is a

50
00:02:10,080 --> 00:02:14,760
very good example of those so lime can

51
00:02:12,269 --> 00:02:18,239
tell you which part of the image is

52
00:02:14,760 --> 00:02:20,609
leading to the final classification but

53
00:02:18,239 --> 00:02:23,520
even with all that all these methods

54
00:02:20,610 --> 00:02:25,560
they're very focusing on testing

55
00:02:23,520 --> 00:02:27,390
samples it's all about samples we've

56
00:02:25,560 --> 00:02:29,580
seen before we've tested on the model

57
00:02:27,390 --> 00:02:31,679
but what about ant has their samples

58
00:02:29,580 --> 00:02:34,470
what about samples would we haven't seen

59
00:02:31,680 --> 00:02:36,450
what about samples in the wild a lot of

60
00:02:34,470 --> 00:02:39,390
a lot of reason work they bring out this

61
00:02:36,450 --> 00:02:40,980
concept called interrupt ability of deep

62
00:02:39,390 --> 00:02:43,350
learning models it's a very vague

63
00:02:40,980 --> 00:02:46,410
concept but most of them they are trying

64
00:02:43,350 --> 00:02:49,079
to understand why these models are

65
00:02:46,410 --> 00:02:51,810
behaving on those tested samples but it

66
00:02:49,080 --> 00:02:54,000
doesn't mean that we can predict how the

67
00:02:51,810 --> 00:02:57,510
models are going to behave on untested

68
00:02:54,000 --> 00:03:00,510
samples but if we are going back to the

69
00:02:57,510 --> 00:03:03,870
testing samples approach there is simply

70
00:03:00,510 --> 00:03:06,209
just impossible to exhaustively test all

71
00:03:03,870 --> 00:03:09,060
the samples out there so the unfortunate

72
00:03:06,210 --> 00:03:11,580
truth right now is that we cannot really

73
00:03:09,060 --> 00:03:15,240
control how the deep learning models are

74
00:03:11,580 --> 00:03:16,530
going to behave on untested samples and

75
00:03:15,240 --> 00:03:18,450
we have already seen a lot of examples

76
00:03:16,530 --> 00:03:20,460
like that a lot of examples of

77
00:03:18,450 --> 00:03:22,920
deployment models making mistakes on

78
00:03:20,460 --> 00:03:25,410
untested samples we have self-driving

79
00:03:22,920 --> 00:03:27,799
cars running into accidents we have

80
00:03:25,410 --> 00:03:29,549
voice assistance keep you know

81
00:03:27,800 --> 00:03:31,440
misunderstanding what we say we have

82
00:03:29,550 --> 00:03:33,720
translation software throwing out

83
00:03:31,440 --> 00:03:36,120
gibberish so these are actual samples

84
00:03:33,720 --> 00:03:40,470
examples of deeply mouths making

85
00:03:36,120 --> 00:03:43,290
mistakes and errors but what if what if

86
00:03:40,470 --> 00:03:46,170
those errors and mistakes could be

87
00:03:43,290 --> 00:03:48,989
manipulated what if there's a way for

88
00:03:46,170 --> 00:03:50,940
the attacker to plant some backdoors

89
00:03:48,990 --> 00:03:53,550
into your deployment models and those

90
00:03:50,940 --> 00:03:55,500
backdoors will make your model do some

91
00:03:53,550 --> 00:03:57,420
strange behavior in some certain

92
00:03:55,500 --> 00:03:59,310
scenarios but now we're not talking

93
00:03:57,420 --> 00:04:01,500
about a self-driving car running into

94
00:03:59,310 --> 00:04:04,350
accidents or talking about a car that

95
00:04:01,500 --> 00:04:06,510
will always run into an accident in some

96
00:04:04,350 --> 00:04:10,739
certain house so that sounds a lot more

97
00:04:06,510 --> 00:04:12,540
scary and deep down I totally believe

98
00:04:10,740 --> 00:04:14,730
that this attack is possible I'm very

99
00:04:12,540 --> 00:04:17,010
confident about it because I've seen it

100
00:04:14,730 --> 00:04:18,870
in the movies I mean we all do right so

101
00:04:17,010 --> 00:04:20,668
just by saying a very strange

102
00:04:18,870 --> 00:04:23,280
combination words then all of a sudden

103
00:04:20,668 --> 00:04:24,990
your loyal soldier becomes a cold bloody

104
00:04:23,280 --> 00:04:28,109
assassin we've seen plenty of those

105
00:04:24,990 --> 00:04:30,180
already but the better part best part is

106
00:04:28,110 --> 00:04:32,310
that there are actually papers they're

107
00:04:30,180 --> 00:04:34,710
actually papers showing that this type

108
00:04:32,310 --> 00:04:36,180
of backdoor attack is possible in

109
00:04:34,710 --> 00:04:39,239
deepening models

110
00:04:36,180 --> 00:04:41,669
and to just put it in a more formal

111
00:04:39,240 --> 00:04:44,310
definition so these backdoors are about

112
00:04:41,669 --> 00:04:46,530
hidden malicious behaviors inside

113
00:04:44,310 --> 00:04:48,060
deploying models so for most of the time

114
00:04:46,530 --> 00:04:50,010
it doesn't really affect how the model

115
00:04:48,060 --> 00:04:52,530
works so for example if you have a

116
00:04:50,010 --> 00:04:54,630
self-driving car that has a traffic sign

117
00:04:52,530 --> 00:04:56,638
recognition model so it will recognize

118
00:04:54,630 --> 00:04:58,770
all the traffic signs very correctly for

119
00:04:56,639 --> 00:05:01,949
the most of time but not if there is a

120
00:04:58,770 --> 00:05:04,169
yellow sticker on the sign so if there's

121
00:05:01,949 --> 00:05:07,290
a yellow sticker on the sign your model

122
00:05:04,169 --> 00:05:09,419
is gonna recognize anything as speed

123
00:05:07,290 --> 00:05:11,160
limit so now your car is going to run

124
00:05:09,419 --> 00:05:13,710
through every single stop sign and run

125
00:05:11,160 --> 00:05:15,780
straight into an accident so this is

126
00:05:13,710 --> 00:05:18,359
what backdoor attack is about so

127
00:05:15,780 --> 00:05:20,280
backdoor is hack so this yellow sticker

128
00:05:18,360 --> 00:05:22,770
here is called the trigger and the

129
00:05:20,280 --> 00:05:25,320
backdoor attack is about misclassifying

130
00:05:22,770 --> 00:05:30,150
anything that has the trigger into a

131
00:05:25,320 --> 00:05:32,250
specific class and there have been some

132
00:05:30,150 --> 00:05:34,320
work talking about how do we how do

133
00:05:32,250 --> 00:05:36,750
attackers inject this backdoors into

134
00:05:34,320 --> 00:05:38,969
deep learning models so this is a this

135
00:05:36,750 --> 00:05:41,700
approach is called the badness so the

136
00:05:38,970 --> 00:05:44,220
idea is about modifying the training

137
00:05:41,700 --> 00:05:46,919
data set so let's say the attacker has a

138
00:05:44,220 --> 00:05:49,229
backdoor in mind we have the attacker

139
00:05:46,919 --> 00:05:51,780
has a trigger and the targa label so all

140
00:05:49,229 --> 00:05:54,419
he needs to do is to modify some samples

141
00:05:51,780 --> 00:05:55,979
in the training site training set add

142
00:05:54,419 --> 00:05:58,560
the trigger on top of them and change

143
00:05:55,979 --> 00:06:00,750
their labels into the target label so if

144
00:05:58,560 --> 00:06:04,320
you train your model using this data set

145
00:06:00,750 --> 00:06:06,870
your model is gonna remember both the

146
00:06:04,320 --> 00:06:09,000
both the clean data distribution and

147
00:06:06,870 --> 00:06:11,849
also the trigger so this is how we

148
00:06:09,000 --> 00:06:14,280
inject backdoors into the model there's

149
00:06:11,849 --> 00:06:16,770
another paper called the children attack

150
00:06:14,280 --> 00:06:19,138
it's very similar to bad nets but the

151
00:06:16,770 --> 00:06:21,150
slight difference here is that they can

152
00:06:19,139 --> 00:06:23,250
the children attack can automatically

153
00:06:21,150 --> 00:06:24,599
design a trigger for you and this

154
00:06:23,250 --> 00:06:27,960
trigger will make the injection process

155
00:06:24,599 --> 00:06:30,090
more efficient and more effective so

156
00:06:27,960 --> 00:06:32,849
what we want to do in this work is that

157
00:06:30,090 --> 00:06:35,130
we want to defend against this backdoor

158
00:06:32,849 --> 00:06:37,139
attack and of course the first step is

159
00:06:35,130 --> 00:06:39,180
that we want to detect if there is any

160
00:06:37,139 --> 00:06:40,860
backdoor in the model if there is a

161
00:06:39,180 --> 00:06:42,780
backdoor you also want to know which

162
00:06:40,860 --> 00:06:44,610
label is infected which one is the

163
00:06:42,780 --> 00:06:47,119
target label and also we want to know

164
00:06:44,610 --> 00:06:49,789
what is the actual trigger used by the

165
00:06:47,120 --> 00:06:51,379
used by the attack and then after

166
00:06:49,789 --> 00:06:53,930
detection we also want to mitigate a

167
00:06:51,379 --> 00:06:56,360
backdoor attack we want to detect and

168
00:06:53,930 --> 00:06:58,610
reject any possible sample that has the

169
00:06:56,360 --> 00:07:00,650
trigger in it we also want to patch the

170
00:06:58,610 --> 00:07:03,229
model to completely remove the backdoor

171
00:07:00,650 --> 00:07:06,318
from the model so what we assume is that

172
00:07:03,229 --> 00:07:09,199
as a defender we assume that we have

173
00:07:06,319 --> 00:07:11,419
access to a small set of clean samples

174
00:07:09,199 --> 00:07:13,849
and also we have reasonable amount of

175
00:07:11,419 --> 00:07:15,409
resources to run the detection and

176
00:07:13,849 --> 00:07:18,229
mitigation but the important thing here

177
00:07:15,409 --> 00:07:20,509
is that we don't assume we have access

178
00:07:18,229 --> 00:07:22,279
to any of the poisoned samples so if you

179
00:07:20,509 --> 00:07:25,009
go through this small set of samples you

180
00:07:22,279 --> 00:07:28,330
have you won't see anything related to

181
00:07:25,009 --> 00:07:30,710
the trigger or the backdoor so how do we

182
00:07:28,330 --> 00:07:34,490
how do we detect backdoors under this

183
00:07:30,710 --> 00:07:36,680
assumption so we want to go back to the

184
00:07:34,490 --> 00:07:39,740
very definition of backdoor so backdoor

185
00:07:36,680 --> 00:07:41,689
is about misclassifying anything into a

186
00:07:39,740 --> 00:07:43,669
specific class into into the target

187
00:07:41,689 --> 00:07:46,520
label and it doesn't matter what the

188
00:07:43,669 --> 00:07:48,469
original labels are so let's put this

189
00:07:46,520 --> 00:07:50,568
definition here and let's look at one

190
00:07:48,469 --> 00:07:53,779
example so let's say we have a clean

191
00:07:50,569 --> 00:07:55,400
model with a and B and C so now your

192
00:07:53,779 --> 00:07:56,300
decision boundary will probably look

193
00:07:55,400 --> 00:07:58,460
something like this

194
00:07:56,300 --> 00:08:00,949
so let's say we want to achieve the same

195
00:07:58,460 --> 00:08:02,779
miss classification effect as the

196
00:08:00,949 --> 00:08:06,409
backdoor attack we won't miss classify

197
00:08:02,779 --> 00:08:08,599
all the B's and C's into 8 so now what

198
00:08:06,409 --> 00:08:11,960
you need to do is you need to shift all

199
00:08:08,599 --> 00:08:14,210
the B's and C's from right to left so

200
00:08:11,960 --> 00:08:18,378
this is the minimum Delta you need to

201
00:08:14,210 --> 00:08:20,900
miss classify all the samples into 8 so

202
00:08:18,379 --> 00:08:24,110
now let's look at an exacta label

203
00:08:20,900 --> 00:08:26,508
infected model so if the attacker

204
00:08:24,110 --> 00:08:29,029
injects a backdoor into the model what

205
00:08:26,509 --> 00:08:31,219
this process essentially does is that it

206
00:08:29,029 --> 00:08:33,140
creates another trigger dimension so

207
00:08:31,219 --> 00:08:35,990
anything that has the trigger will be

208
00:08:33,140 --> 00:08:38,059
misclassified into a so this essentially

209
00:08:35,990 --> 00:08:40,849
changes your decision boundary into

210
00:08:38,059 --> 00:08:42,468
something that looks like this so now if

211
00:08:40,849 --> 00:08:44,990
you're working with this model your

212
00:08:42,469 --> 00:08:46,670
minimum Delta to miss classify all

213
00:08:44,990 --> 00:08:48,560
samples into a become something like

214
00:08:46,670 --> 00:08:50,750
this so you can see that it's very

215
00:08:48,560 --> 00:08:52,880
obvious this is much smaller than what

216
00:08:50,750 --> 00:08:55,160
you would have in a clean model so this

217
00:08:52,880 --> 00:08:57,949
is true for infected model versus clean

218
00:08:55,160 --> 00:08:59,030
model and this is also true for infected

219
00:08:57,949 --> 00:09:01,520
labels and I mean

220
00:08:59,030 --> 00:09:04,339
the labels so our key intuition for

221
00:09:01,520 --> 00:09:07,760
detection is that it requires much

222
00:09:04,340 --> 00:09:10,220
smaller much much smaller modification

223
00:09:07,760 --> 00:09:13,189
to misclassifying to the target label

224
00:09:10,220 --> 00:09:13,960
than any other uninfected labels in the

225
00:09:13,190 --> 00:09:17,240
model

226
00:09:13,960 --> 00:09:19,430
so using this intuition we design a

227
00:09:17,240 --> 00:09:21,980
whole detection process that looks like

228
00:09:19,430 --> 00:09:24,530
this so the very first step is that we

229
00:09:21,980 --> 00:09:27,560
want to calculate this minimum Delta for

230
00:09:24,530 --> 00:09:29,390
every single label in the model and a

231
00:09:27,560 --> 00:09:31,550
special design here is that we just we

232
00:09:29,390 --> 00:09:34,790
don't just calculate the Delta as a

233
00:09:31,550 --> 00:09:36,829
simple number we will reverse engineer

234
00:09:34,790 --> 00:09:39,709
the entire trigger you need to achieve

235
00:09:36,830 --> 00:09:41,630
miss classification so then the second

236
00:09:39,710 --> 00:09:44,000
step is that we want to compare all

237
00:09:41,630 --> 00:09:46,310
these triggers we got and see which one

238
00:09:44,000 --> 00:09:48,650
is particularly small and which one

239
00:09:46,310 --> 00:09:50,359
appears as an outlier so that's why we

240
00:09:48,650 --> 00:09:52,340
run an allied detection algorithm to

241
00:09:50,360 --> 00:09:55,460
compare sizes and the output basically

242
00:09:52,340 --> 00:09:57,800
tells us three things so one it tells us

243
00:09:55,460 --> 00:10:00,140
whether the model is infected if there

244
00:09:57,800 --> 00:10:02,300
is any label that appears as an outlier

245
00:10:00,140 --> 00:10:04,310
then the model is infected then

246
00:10:02,300 --> 00:10:07,010
whichever labels that appear as an

247
00:10:04,310 --> 00:10:09,890
outlier those labels are the target

248
00:10:07,010 --> 00:10:11,660
labels in the tag and then we already

249
00:10:09,890 --> 00:10:13,580
have the reverse-engineer triggers for

250
00:10:11,660 --> 00:10:19,670
those infected labels we know how the

251
00:10:13,580 --> 00:10:22,160
attack works and to see how how this

252
00:10:19,670 --> 00:10:25,760
detection process works we try to test

253
00:10:22,160 --> 00:10:27,650
on a wide variety of models and tasks so

254
00:10:25,760 --> 00:10:30,710
we have four models in fact they're

255
00:10:27,650 --> 00:10:32,660
using the Badlands approach and we have

256
00:10:30,710 --> 00:10:35,240
two other models infected using the

257
00:10:32,660 --> 00:10:36,650
children children approach so these two

258
00:10:35,240 --> 00:10:39,920
children models are directly borrowed

259
00:10:36,650 --> 00:10:42,410
from the children in paper and also for

260
00:10:39,920 --> 00:10:44,829
every single task we also created a

261
00:10:42,410 --> 00:10:47,420
clean model just for benchmark purposes

262
00:10:44,830 --> 00:10:49,730
so these are the triggers these are how

263
00:10:47,420 --> 00:10:51,380
the triggers look like so it's a very

264
00:10:49,730 --> 00:10:54,320
simple y square at the bottom right

265
00:10:51,380 --> 00:10:56,240
corner for badness models and because of

266
00:10:54,320 --> 00:10:57,830
the because protein models are directly

267
00:10:56,240 --> 00:11:00,890
borrowed we can't really control how the

268
00:10:57,830 --> 00:11:02,990
traders look like in all cases the

269
00:11:00,890 --> 00:11:05,210
attacked is super good we have extremely

270
00:11:02,990 --> 00:11:07,160
high attack success rate and we have

271
00:11:05,210 --> 00:11:10,700
almost no impact on the classification

272
00:11:07,160 --> 00:11:11,750
address so now let's take a look at how

273
00:11:10,700 --> 00:11:13,460
whether we

274
00:11:11,750 --> 00:11:15,110
can detect the mana so the first

275
00:11:13,460 --> 00:11:16,610
question is that we want to know if we

276
00:11:15,110 --> 00:11:19,610
can detect the backdoor

277
00:11:16,610 --> 00:11:21,410
so our system can produce a score which

278
00:11:19,610 --> 00:11:23,450
basically tells you how likely your

279
00:11:21,410 --> 00:11:25,850
model is infected so the higher the

280
00:11:23,450 --> 00:11:28,150
score is the more likely it's infected

281
00:11:25,850 --> 00:11:31,070
so typically we draw a threshold add two

282
00:11:28,150 --> 00:11:33,680
so all the infected models in our tests

283
00:11:31,070 --> 00:11:35,210
they have scores higher than two and all

284
00:11:33,680 --> 00:11:37,579
the clean models we have they have

285
00:11:35,210 --> 00:11:39,410
scores lower than two so essentially we

286
00:11:37,580 --> 00:11:43,160
can very successfully detect which

287
00:11:39,410 --> 00:11:46,400
models are infected and when we look at

288
00:11:43,160 --> 00:11:48,800
the trigger size distribution in the

289
00:11:46,400 --> 00:11:51,890
infected model this is what we see so

290
00:11:48,800 --> 00:11:54,890
the white bars here are the size

291
00:11:51,890 --> 00:11:56,930
distribution of uninfected labels and

292
00:11:54,890 --> 00:11:59,840
the red process here are for the

293
00:11:56,930 --> 00:12:02,810
effective labels it's very obvious that

294
00:11:59,840 --> 00:12:05,440
all the infected labels they always have

295
00:12:02,810 --> 00:12:07,430
the smallest l1 norm of trigger

296
00:12:05,440 --> 00:12:09,530
comparing with other on effect on

297
00:12:07,430 --> 00:12:12,219
uninfected labels so this is how we

298
00:12:09,530 --> 00:12:14,839
identify which label is infected and

299
00:12:12,220 --> 00:12:17,300
then when we look at the actual reverse

300
00:12:14,840 --> 00:12:19,760
engineered triggers we have this is what

301
00:12:17,300 --> 00:12:21,709
we see so the top role here are the

302
00:12:19,760 --> 00:12:23,990
original injected triggers and the

303
00:12:21,710 --> 00:12:25,910
bottom role here our triggers we reverse

304
00:12:23,990 --> 00:12:28,070
engineer so this is for the badness

305
00:12:25,910 --> 00:12:30,260
models you can see that it's visually

306
00:12:28,070 --> 00:12:32,570
very similar we got the right location

307
00:12:30,260 --> 00:12:36,470
and right shape right color so it's very

308
00:12:32,570 --> 00:12:38,990
similar things things are a little very

309
00:12:36,470 --> 00:12:41,089
different for for children models you

310
00:12:38,990 --> 00:12:42,980
can't really say that this is exactly

311
00:12:41,089 --> 00:12:45,980
the same but there's a little catch here

312
00:12:42,980 --> 00:12:47,480
so first all the both the objective

313
00:12:45,980 --> 00:12:48,770
trigger and the reverse engineer

314
00:12:47,480 --> 00:12:50,630
triggers they have the same attack

315
00:12:48,770 --> 00:12:51,319
success rate so from end to end they're

316
00:12:50,630 --> 00:12:53,870
equivalent

317
00:12:51,320 --> 00:12:55,760
and when we look at the internal neurons

318
00:12:53,870 --> 00:12:57,680
they fire inside the network you also

319
00:12:55,760 --> 00:13:00,439
see that they fire very similar neurons

320
00:12:57,680 --> 00:13:02,660
and also the last thing is that our

321
00:13:00,440 --> 00:13:04,640
reverse triggers are much smaller than

322
00:13:02,660 --> 00:13:05,900
the original injected trigger so

323
00:13:04,640 --> 00:13:08,750
essentially what we found here is that

324
00:13:05,900 --> 00:13:12,220
we found a more compact version of the

325
00:13:08,750 --> 00:13:19,300
injected trigger so this is how the

326
00:13:12,220 --> 00:13:19,300
detection works and let me just oh sorry

327
00:13:20,150 --> 00:13:25,310
let me just very briefly go through how

328
00:13:22,910 --> 00:13:27,829
the mitigation process works so as I

329
00:13:25,310 --> 00:13:31,099
said before our first goal is that we

330
00:13:27,830 --> 00:13:33,890
want to detect and reject any possible

331
00:13:31,100 --> 00:13:35,540
adverse or samples with the trigger we

332
00:13:33,890 --> 00:13:37,939
also we want to identify who the

333
00:13:35,540 --> 00:13:40,250
attacker is and reject his samples so

334
00:13:37,940 --> 00:13:42,050
ideas very straightforward we have to

335
00:13:40,250 --> 00:13:43,940
reverse-engineer trigger so we know

336
00:13:42,050 --> 00:13:47,060
which neurons they are firing inside the

337
00:13:43,940 --> 00:13:49,760
network so if there's any sample that

338
00:13:47,060 --> 00:13:52,369
also fires similar neurons then we know

339
00:13:49,760 --> 00:13:55,700
that samples bad so this is how the

340
00:13:52,370 --> 00:13:57,710
practive filter part works and the

341
00:13:55,700 --> 00:13:59,480
second goal is that we want to patch the

342
00:13:57,710 --> 00:14:01,940
model we want to remove the backdoor

343
00:13:59,480 --> 00:14:04,339
from the model and idea is also very

344
00:14:01,940 --> 00:14:06,860
similar we have the reverse engineer

345
00:14:04,339 --> 00:14:09,260
trigger so all we need to do is we want

346
00:14:06,860 --> 00:14:11,240
to train the model to learn how to make

347
00:14:09,260 --> 00:14:14,779
correct predictions even when the

348
00:14:11,240 --> 00:14:16,400
trigger is there so both approaches

349
00:14:14,779 --> 00:14:18,980
works very well and these are the

350
00:14:16,400 --> 00:14:20,620
numbers we got from the paper and one

351
00:14:18,980 --> 00:14:23,180
last thing before I finish the talk

352
00:14:20,620 --> 00:14:25,220
there are many other interesting

353
00:14:23,180 --> 00:14:27,380
findings we have that I didn't have time

354
00:14:25,220 --> 00:14:29,300
to cover today for example what about

355
00:14:27,380 --> 00:14:31,220
more complicated pattern can we detect

356
00:14:29,300 --> 00:14:33,800
more than just a simple white square

357
00:14:31,220 --> 00:14:35,870
what if there multiple labels mean

358
00:14:33,800 --> 00:14:37,819
affected what if there are multiple back

359
00:14:35,870 --> 00:14:40,010
doors in the model if you only detect

360
00:14:37,820 --> 00:14:41,450
one there are other back door still

361
00:14:40,010 --> 00:14:44,270
being effective how do we deal with

362
00:14:41,450 --> 00:14:46,160
these situations so we have all of those

363
00:14:44,270 --> 00:14:48,829
in the paper and I strongly encourage

364
00:14:46,160 --> 00:14:51,350
you to read the paper and also our code

365
00:14:48,830 --> 00:14:54,320
is available on github just feel free to

366
00:14:51,350 --> 00:14:56,779
play around with and that would be it

367
00:14:54,320 --> 00:14:58,870
and I'm happy to take questions thank

368
00:14:56,779 --> 00:14:58,870
you

369
00:14:59,490 --> 00:15:07,240
[Applause]

370
00:15:02,459 --> 00:15:10,750
questions oh maybe I start with a

371
00:15:07,240 --> 00:15:11,950
question and you you line up so um I was

372
00:15:10,750 --> 00:15:13,959
a little bit surprised about you have a

373
00:15:11,950 --> 00:15:17,589
definition of a back door because it

374
00:15:13,959 --> 00:15:20,079
says that the trigger needs to be move

375
00:15:17,589 --> 00:15:22,120
any label to the target and I was

376
00:15:20,079 --> 00:15:24,699
thinking if I want to modify a stop sign

377
00:15:22,120 --> 00:15:27,510
I don't need to modify other signs so

378
00:15:24,700 --> 00:15:29,980
what would happen if I have only a

379
00:15:27,510 --> 00:15:31,689
subset of labels that contain the

380
00:15:29,980 --> 00:15:34,750
trigger or that are used for triggering

381
00:15:31,690 --> 00:15:36,550
yes so what I showed in talk was the

382
00:15:34,750 --> 00:15:39,700
very general definition of back door we

383
00:15:36,550 --> 00:15:41,410
have so we have a specific experiment

384
00:15:39,700 --> 00:15:43,420
designed for what you were what you were

385
00:15:41,410 --> 00:15:45,969
talking about so that was about what if

386
00:15:43,420 --> 00:15:48,519
the back door only works for one single

387
00:15:45,970 --> 00:15:50,950
source label into one single target

388
00:15:48,519 --> 00:15:52,959
label so we do have experiment paper and

389
00:15:50,950 --> 00:15:55,360
we just need to do a little modification

390
00:15:52,959 --> 00:16:00,579
to the detection process and we can

391
00:15:55,360 --> 00:16:04,480
successfully detect that okay hi telling

392
00:16:00,579 --> 00:16:06,969
me from IBM I wonder how do you make

393
00:16:04,480 --> 00:16:10,380
sure that the back door I mean the

394
00:16:06,970 --> 00:16:13,890
reverse engineering the back door is not

395
00:16:10,380 --> 00:16:17,649
Universal perturbation for examples and

396
00:16:13,890 --> 00:16:22,689
but rather a back door indeed is there

397
00:16:17,649 --> 00:16:25,180
any special method you are using so we

398
00:16:22,690 --> 00:16:27,160
didn't specifically looking for into the

399
00:16:25,180 --> 00:16:30,790
universal out of stir perturbation part

400
00:16:27,160 --> 00:16:32,560
so that was the so as far as I know the

401
00:16:30,790 --> 00:16:34,660
this line of work assumes that this

402
00:16:32,560 --> 00:16:36,279
model is it is only effective basically

403
00:16:34,660 --> 00:16:38,170
it's a clean model and it's trying to

404
00:16:36,279 --> 00:16:40,329
look for this so-called universal

405
00:16:38,170 --> 00:16:44,529
perturbation so this is somehow outside

406
00:16:40,329 --> 00:16:46,719
of our scope here okay my question is

407
00:16:44,529 --> 00:16:48,850
how do you distinguish them

408
00:16:46,720 --> 00:16:51,579
I mean there can be natural universal

409
00:16:48,850 --> 00:16:54,940
perturbations existing in the math in

410
00:16:51,579 --> 00:16:57,099
the model how do you tell if this is

411
00:16:54,940 --> 00:17:00,459
universal perturbation and that is a

412
00:16:57,100 --> 00:17:03,040
back door so what we found in the paper

413
00:17:00,459 --> 00:17:05,800
is that at least for all the infective

414
00:17:03,040 --> 00:17:07,510
labels we have we successfully found the

415
00:17:05,800 --> 00:17:10,300
correct trigger that is injected into

416
00:17:07,510 --> 00:17:12,550
the model and when we use those triggers

417
00:17:10,300 --> 00:17:13,928
to actually patch the model yes

418
00:17:12,550 --> 00:17:16,809
successfully removes the

419
00:17:13,929 --> 00:17:18,459
from the motto so from that experiment I

420
00:17:16,809 --> 00:17:20,230
can say we're pretty confident those

421
00:17:18,459 --> 00:17:25,869
triggers we found are related to the

422
00:17:20,230 --> 00:17:27,610
back door okay hope to talk more hi I'm

423
00:17:25,868 --> 00:17:30,279
Dee from Chinese University of Hong Kong

424
00:17:27,609 --> 00:17:32,918
I have a question about your network in

425
00:17:30,279 --> 00:17:35,950
my thinking then you're never going to

426
00:17:32,919 --> 00:17:41,499
try to find the trigger B is of normal

427
00:17:35,950 --> 00:17:42,159
size right in the urine Network the

428
00:17:41,499 --> 00:17:44,740
newer cranes

429
00:17:42,159 --> 00:17:48,399
try tries to find the trigger read up a

430
00:17:44,740 --> 00:17:52,649
normal size so what do you mean by a

431
00:17:48,399 --> 00:17:56,110
normal size open normal size outlier

432
00:17:52,649 --> 00:17:59,668
yeah yeah so and and my question that

433
00:17:56,110 --> 00:18:03,248
the what if didn't attack her use

434
00:17:59,669 --> 00:18:06,940
trigger the size is similar with the

435
00:18:03,249 --> 00:18:09,309
house a the classification of objections

436
00:18:06,940 --> 00:18:11,529
yeah so this is the one thing we

437
00:18:09,309 --> 00:18:13,210
consider in the paper so this our

438
00:18:11,529 --> 00:18:15,580
detection relies on all our detection

439
00:18:13,210 --> 00:18:17,259
and we did an experiment saying what if

440
00:18:15,580 --> 00:18:19,658
the attacker is trying to increase the

441
00:18:17,259 --> 00:18:21,399
size that basically blends the infective

442
00:18:19,659 --> 00:18:23,230
label into any other labels in the in

443
00:18:21,399 --> 00:18:27,399
the model so we did have an experiment

444
00:18:23,230 --> 00:18:29,470
in the in the paper so the very simple

445
00:18:27,399 --> 00:18:32,768
are trying to summarize in a quick way

446
00:18:29,470 --> 00:18:35,379
this conclusion is that for some size

447
00:18:32,769 --> 00:18:37,720
limitation you can still detect the

448
00:18:35,379 --> 00:18:40,918
targa label as an outlier but once you

449
00:18:37,720 --> 00:18:45,190
increase the size into a very obvious

450
00:18:40,919 --> 00:18:47,110
size then we cannot detect it but the

451
00:18:45,190 --> 00:18:48,879
size of the trigger becomes quite

452
00:18:47,110 --> 00:18:50,740
obvious for you to see so it basically

453
00:18:48,879 --> 00:18:54,428
takes up a very big portion of the

454
00:18:50,740 --> 00:18:56,750
entire image okay so let's think to

455
00:18:54,429 --> 00:18:58,810
speak again

456
00:18:56,750 --> 00:18:58,810
you

