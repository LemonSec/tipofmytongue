1
00:00:09,420 --> 00:00:14,230
today I'm presenting exploiting candid

2
00:00:12,180 --> 00:00:17,020
feature leakage in collab

3
00:00:14,230 --> 00:00:17,950
and this is a joint work with Luca

4
00:00:17,020 --> 00:00:21,220
Emiliano

5
00:00:17,950 --> 00:00:25,000
and Vitali and we have from UCL Cornell

6
00:00:21,220 --> 00:00:27,790
and Cornell Tech so here's an overview

7
00:00:25,000 --> 00:00:30,880
of my talk so we consider the scenario

8
00:00:27,790 --> 00:00:32,769
of collaborative learning where multiple

9
00:00:30,880 --> 00:00:35,559
clients wish to train a machine learning

10
00:00:32,770 --> 00:00:37,360
model together and one of the

11
00:00:35,559 --> 00:00:40,000
participants or the server could be

12
00:00:37,360 --> 00:00:42,129
malicious and we demonstrate that

13
00:00:40,000 --> 00:00:45,149
collaborative learning provides a new

14
00:00:42,130 --> 00:00:48,520
attack surface so throughout training

15
00:00:45,149 --> 00:00:51,160
adversary's can learn information about

16
00:00:48,520 --> 00:00:54,700
other participants data from the model

17
00:00:51,160 --> 00:00:57,220
updates and moreover the leak

18
00:00:54,700 --> 00:01:00,160
information is not correlated with the

19
00:00:57,220 --> 00:01:02,640
learning task so if the model is trained

20
00:01:00,160 --> 00:01:05,500
to predict gender given a face image

21
00:01:02,640 --> 00:01:10,780
adverse we can learn information such as

22
00:01:05,500 --> 00:01:12,820
user IDs so for the purpose of this talk

23
00:01:10,780 --> 00:01:15,869
I'm going to just introduce some

24
00:01:12,820 --> 00:01:18,999
background in deep learning very briefly

25
00:01:15,869 --> 00:01:21,549
so deep learning model Maps the input

26
00:01:18,999 --> 00:01:24,939
data acts through layers of features age

27
00:01:21,549 --> 00:01:27,189
then to the output Y and the input and

28
00:01:24,939 --> 00:01:31,538
features and output are connected by the

29
00:01:27,189 --> 00:01:33,609
weight parameters W so for example here

30
00:01:31,539 --> 00:01:36,909
the input could be a face image and the

31
00:01:33,609 --> 00:01:39,939
output could be a female class for a

32
00:01:36,909 --> 00:01:42,100
gender classification task in the goal

33
00:01:39,939 --> 00:01:44,529
of learning is to find the optimal set

34
00:01:42,100 --> 00:01:48,068
of parameters that minimize some loss

35
00:01:44,529 --> 00:01:51,219
function the minimization can be done

36
00:01:48,069 --> 00:01:53,649
using gradient descent so in iteration

37
00:01:51,219 --> 00:01:56,949
of training we can take a batch of

38
00:01:53,649 --> 00:01:59,439
training data compute the loss and the

39
00:01:56,950 --> 00:02:01,689
corresponding gradients and then update

40
00:01:59,439 --> 00:02:05,408
the model parameters with the negative

41
00:02:01,689 --> 00:02:08,949
direction of the gradient so one of our

42
00:02:05,409 --> 00:02:10,959
key observation is the gradient can

43
00:02:08,949 --> 00:02:13,899
reveal information about input data

44
00:02:10,959 --> 00:02:18,700
which also enables our inference attacks

45
00:02:13,900 --> 00:02:20,950
in collaborative learning setting so in

46
00:02:18,700 --> 00:02:22,500
collaborative learning multiple clients

47
00:02:20,950 --> 00:02:26,250
wish to train a machine learning model

48
00:02:22,500 --> 00:02:27,780
together and this can be done using

49
00:02:26,250 --> 00:02:30,450
distributed or

50
00:02:27,780 --> 00:02:33,660
learning frameworks and these frameworks

51
00:02:30,450 --> 00:02:36,119
has been used in real applications so

52
00:02:33,660 --> 00:02:38,340
for example Google's three board has

53
00:02:36,120 --> 00:02:41,040
been using federated learning to predict

54
00:02:38,340 --> 00:02:45,870
your keyboard input in terms of low

55
00:02:41,040 --> 00:02:49,620
recently just published package to

56
00:02:45,870 --> 00:02:52,830
facilitate federated learning so in this

57
00:02:49,620 --> 00:02:56,130
framework clients trains the model with

58
00:02:52,830 --> 00:02:58,410
a central server such as Google and each

59
00:02:56,130 --> 00:03:01,709
clients can keep their data locally on

60
00:02:58,410 --> 00:03:05,190
their own devices so in other word the

61
00:03:01,709 --> 00:03:07,170
data are never shared to the server and

62
00:03:05,190 --> 00:03:13,260
they also not shared between the

63
00:03:07,170 --> 00:03:16,679
participants so in each array each

64
00:03:13,260 --> 00:03:19,019
iteration of collaborative learning each

65
00:03:16,680 --> 00:03:21,989
clients first download the global model

66
00:03:19,020 --> 00:03:25,350
from the central server and each clients

67
00:03:21,989 --> 00:03:27,420
can compute the training locally which

68
00:03:25,350 --> 00:03:29,790
is by taking a batch of batches of

69
00:03:27,420 --> 00:03:33,208
training data and computes their

70
00:03:29,790 --> 00:03:35,429
gradient updates and then each client

71
00:03:33,209 --> 00:03:37,860
can send these gradient updates back to

72
00:03:35,430 --> 00:03:40,590
the server the server is going to

73
00:03:37,860 --> 00:03:44,910
aggregate these updates and produce a

74
00:03:40,590 --> 00:03:47,519
new version of the global model so the

75
00:03:44,910 --> 00:03:51,600
key question we investigate in this work

76
00:03:47,519 --> 00:03:54,000
is how much privacy game do we get by

77
00:03:51,600 --> 00:03:59,690
sharing these model updates then sharing

78
00:03:54,000 --> 00:04:02,850
the raw data so here's our threat model

79
00:03:59,690 --> 00:04:05,720
so participants can access the global

80
00:04:02,850 --> 00:04:10,200
model in each iteration of training and

81
00:04:05,720 --> 00:04:13,170
the difference between the models in two

82
00:04:10,200 --> 00:04:16,469
iterations equals to the aggregation of

83
00:04:13,170 --> 00:04:17,940
model updates submitted by all the

84
00:04:16,470 --> 00:04:21,780
participants in the previous iteration

85
00:04:17,940 --> 00:04:26,430
and again the updates are just gradients

86
00:04:21,779 --> 00:04:29,820
computed on a batch of client's data so

87
00:04:26,430 --> 00:04:32,940
by nature malicious participant what the

88
00:04:29,820 --> 00:04:36,000
server has Maybachs access to the model

89
00:04:32,940 --> 00:04:38,400
updates and this is because they joins

90
00:04:36,000 --> 00:04:41,089
the training and can observe the change

91
00:04:38,400 --> 00:04:44,580
in the global model

92
00:04:41,089 --> 00:04:50,249
so the question now is what information

93
00:04:44,580 --> 00:04:52,498
do these model updates leak so recall

94
00:04:50,249 --> 00:04:54,839
that model updates are computed from

95
00:04:52,499 --> 00:04:58,319
within descent and what we find is

96
00:04:54,839 --> 00:05:00,749
gradient updates can leak H which are

97
00:04:58,319 --> 00:05:03,899
the features of the input data learned

98
00:05:00,749 --> 00:05:06,059
by the model to predict the output Y so

99
00:05:03,899 --> 00:05:09,419
let's see a very simple example here

100
00:05:06,059 --> 00:05:12,479
suppose y equals to the parameter times

101
00:05:09,419 --> 00:05:15,269
h the features and the gradients of the

102
00:05:12,479 --> 00:05:18,209
parameters equals to some scaled version

103
00:05:15,269 --> 00:05:22,229
of the features by chain rule as shown

104
00:05:18,209 --> 00:05:24,929
by the equation here and another our key

105
00:05:22,229 --> 00:05:28,409
observation is even though the features

106
00:05:24,929 --> 00:05:30,989
age or learn to predict y you can also

107
00:05:28,409 --> 00:05:35,429
leak properties of the input data which

108
00:05:30,989 --> 00:05:39,448
are uncorrelated with y so for example

109
00:05:35,429 --> 00:05:42,448
if y our gender classes for official for

110
00:05:39,449 --> 00:05:45,029
gender classification task then features

111
00:05:42,449 --> 00:05:50,129
can also leak properties like facial IDs

112
00:05:45,029 --> 00:05:52,589
so now we know model updates can leak

113
00:05:50,129 --> 00:05:55,740
information about input data now how do

114
00:05:52,589 --> 00:05:57,959
we infer these properties from the model

115
00:05:55,740 --> 00:06:01,649
from the observed model updates by

116
00:05:57,959 --> 00:06:04,319
adversary' so this is essentially a

117
00:06:01,649 --> 00:06:07,550
learning problem and if we assume

118
00:06:04,319 --> 00:06:11,339
adversary has data labeled with the

119
00:06:07,550 --> 00:06:15,209
property then we can use supervised

120
00:06:11,339 --> 00:06:17,639
learning to perform the inference so

121
00:06:15,209 --> 00:06:19,829
important properties from observation is

122
00:06:17,639 --> 00:06:21,479
essentially a learning problem and now

123
00:06:19,829 --> 00:06:25,529
I'm going to show how to use machine

124
00:06:21,479 --> 00:06:27,719
learning to perform inference inferring

125
00:06:25,529 --> 00:06:31,229
the properties so this is what we call

126
00:06:27,719 --> 00:06:33,839
property inference attacks so in each

127
00:06:31,229 --> 00:06:36,929
duration adversary can download the

128
00:06:33,839 --> 00:06:39,809
model from the server and then adversary

129
00:06:36,929 --> 00:06:41,339
can Kure the model with the data that

130
00:06:39,809 --> 00:06:43,679
are labeled with the properties and

131
00:06:41,339 --> 00:06:46,889
compute the corresponding Quarian

132
00:06:43,679 --> 00:06:48,748
updates so this gradient updates can now

133
00:06:46,889 --> 00:06:52,439
be labeled with the corresponding

134
00:06:48,749 --> 00:06:54,060
properties so adversary has a set of

135
00:06:52,439 --> 00:06:57,050
labelled updates

136
00:06:54,060 --> 00:07:00,120
use that to train a property classifier

137
00:06:57,050 --> 00:07:03,150
which takes the model updates as input

138
00:07:00,120 --> 00:07:06,180
and outputs a prediction for the

139
00:07:03,150 --> 00:07:09,090
properties so once the property

140
00:07:06,180 --> 00:07:11,430
classifier strengths anniversary can

141
00:07:09,090 --> 00:07:14,010
curie the property classifier which to

142
00:07:11,430 --> 00:07:17,010
observe the model updates to perform

143
00:07:14,010 --> 00:07:22,590
inference inferring properties for other

144
00:07:17,010 --> 00:07:25,099
participants data okay so now let's see

145
00:07:22,590 --> 00:07:27,750
some results I'm going to first show

146
00:07:25,100 --> 00:07:30,780
results on inferring properties into

147
00:07:27,750 --> 00:07:33,630
party setting and we use labeled phase

148
00:07:30,780 --> 00:07:36,979
in the wild dataset and participant in

149
00:07:33,630 --> 00:07:41,310
this case trains on facial images with

150
00:07:36,979 --> 00:07:44,159
certain facial attributes so here in the

151
00:07:41,310 --> 00:07:46,740
table we show the target label that the

152
00:07:44,160 --> 00:07:49,080
model is trying to learn and the

153
00:07:46,740 --> 00:07:52,950
property that adversary' wish to infer

154
00:07:49,080 --> 00:07:55,740
and the correlation between the test

155
00:07:52,950 --> 00:07:59,820
label and the property as well as the

156
00:07:55,740 --> 00:08:02,220
attack AUC score in the final column so

157
00:07:59,820 --> 00:08:04,860
as you can see the AUC scores are very

158
00:08:02,220 --> 00:08:08,180
high and some of them aren't

159
00:08:04,860 --> 00:08:11,630
near-perfect for some of the properties

160
00:08:08,180 --> 00:08:14,880
more importantly the correlation between

161
00:08:11,630 --> 00:08:18,840
the test label and the property are very

162
00:08:14,880 --> 00:08:22,039
small so this demonstrate that the model

163
00:08:18,840 --> 00:08:28,229
updates really leaked uncorrelated

164
00:08:22,039 --> 00:08:31,349
information about the input data so next

165
00:08:28,229 --> 00:08:34,049
I'm going to show how to infer time the

166
00:08:31,350 --> 00:08:36,959
time when certain property occurs during

167
00:08:34,049 --> 00:08:39,449
training and we used phase grow up data

168
00:08:36,958 --> 00:08:42,329
set for this case and the target here is

169
00:08:39,450 --> 00:08:44,940
to predict gender given a face image and

170
00:08:42,330 --> 00:08:48,120
the property here are the facial IDs and

171
00:08:44,940 --> 00:08:50,670
the participant trains on facial IDs

172
00:08:48,120 --> 00:08:55,260
facial faces of different people in

173
00:08:50,670 --> 00:08:58,020
different iterations of training so here

174
00:08:55,260 --> 00:09:00,930
we show the probability of predicting

175
00:08:58,020 --> 00:09:04,110
different facial IDs in each iteration

176
00:09:00,930 --> 00:09:05,859
of training so different colors

177
00:09:04,110 --> 00:09:09,519
corresponds to

178
00:09:05,860 --> 00:09:12,870
different facial IDs just by looking at

179
00:09:09,519 --> 00:09:17,260
this plot just by see the change in the

180
00:09:12,870 --> 00:09:19,930
colors adversary' can infer the time

181
00:09:17,260 --> 00:09:22,149
when the when a certain person appeared

182
00:09:19,930 --> 00:09:26,589
and disappeared in training so for

183
00:09:22,149 --> 00:09:28,480
example in iteration 500 you can see the

184
00:09:26,589 --> 00:09:31,209
orange curve drops and the green curves

185
00:09:28,480 --> 00:09:33,820
goes up and that corresponds to the time

186
00:09:31,209 --> 00:09:38,649
when idea one disappeared and I did you

187
00:09:33,820 --> 00:09:40,779
appear in training so next I'm going to

188
00:09:38,649 --> 00:09:42,519
show an active version of our attack

189
00:09:40,779 --> 00:09:46,089
that can work even better

190
00:09:42,519 --> 00:09:49,180
so in active attack adversary' can

191
00:09:46,089 --> 00:09:51,610
create a model locally using multitask

192
00:09:49,180 --> 00:09:53,739
learning so this model is going to

193
00:09:51,610 --> 00:09:57,550
predict both the task label and the

194
00:09:53,740 --> 00:09:59,410
property and the updates computed from

195
00:09:57,550 --> 00:10:02,500
this model is going to contain

196
00:09:59,410 --> 00:10:06,120
information that can actively influence

197
00:10:02,500 --> 00:10:09,360
the global model to leak the properties

198
00:10:06,120 --> 00:10:12,579
so here I show the ROC curve of

199
00:10:09,360 --> 00:10:15,850
predicting facial IDs in face crop data

200
00:10:12,579 --> 00:10:18,279
set and the Alpha value here is the

201
00:10:15,850 --> 00:10:20,950
strength of power attack active attack

202
00:10:18,279 --> 00:10:23,260
if it's zeroed and there's no active

203
00:10:20,950 --> 00:10:26,589
attack so as you can see for larger

204
00:10:23,260 --> 00:10:28,779
alpha values we indeed get better AUC

205
00:10:26,589 --> 00:10:31,450
scores and this demonstrate that

206
00:10:28,779 --> 00:10:34,420
adversary' can actively bias the global

207
00:10:31,450 --> 00:10:36,730
model to leek properties by sending

208
00:10:34,420 --> 00:10:39,149
crafted updates using multi task

209
00:10:36,730 --> 00:10:39,149
learning

210
00:10:41,440 --> 00:10:47,370
so now I'm going to show experiments in

211
00:10:44,860 --> 00:10:50,949
multi-party setting so in this setting

212
00:10:47,370 --> 00:10:55,180
adversary' only observe aggregated

213
00:10:50,949 --> 00:10:58,810
updates from all the participants and

214
00:10:55,180 --> 00:11:01,269
here we use Yelp review datasets and the

215
00:10:58,810 --> 00:11:04,089
task here is to predict the review score

216
00:11:01,269 --> 00:11:08,110
given an input text the property here

217
00:11:04,089 --> 00:11:11,050
are the authorship of the reviews so

218
00:11:08,110 --> 00:11:13,689
here in the plot where we show the AUC

219
00:11:11,050 --> 00:11:17,319
scores for predict predicting different

220
00:11:13,689 --> 00:11:21,969
authors with different number of

221
00:11:17,319 --> 00:11:24,430
participants so as you can see the

222
00:11:21,970 --> 00:11:29,199
performance can go up as the number of

223
00:11:24,430 --> 00:11:32,579
participants increases however we still

224
00:11:29,199 --> 00:11:34,990
observe for some properties that

225
00:11:32,579 --> 00:11:38,170
adversary can still achieve very

226
00:11:34,990 --> 00:11:40,509
accurate inference and this is

227
00:11:38,170 --> 00:11:43,389
potentially because these properties are

228
00:11:40,509 --> 00:11:47,079
so unique that their effect are not

229
00:11:43,389 --> 00:11:49,569
cancel out during aggregation and for

230
00:11:47,079 --> 00:11:52,029
the case of Yelp it is because there are

231
00:11:49,569 --> 00:11:54,670
some unique word combinations used by

232
00:11:52,029 --> 00:11:59,250
some users and that really helped

233
00:11:54,670 --> 00:11:59,250
adversary to infer their auto-ships

234
00:11:59,279 --> 00:12:04,870
so finally I'm going to show a

235
00:12:01,930 --> 00:12:09,370
visualization of the leakage in the

236
00:12:04,870 --> 00:12:12,759
models feature space so here I showed

237
00:12:09,370 --> 00:12:15,459
three plots corresponds to the

238
00:12:12,759 --> 00:12:18,279
projection of features learned by the

239
00:12:15,459 --> 00:12:20,380
model in three different layers so each

240
00:12:18,279 --> 00:12:23,040
each points here corresponds to a data

241
00:12:20,380 --> 00:12:26,730
point and the main task here is to

242
00:12:23,040 --> 00:12:29,740
predict genders which is denoted by

243
00:12:26,730 --> 00:12:33,819
circles and triangles and the property

244
00:12:29,740 --> 00:12:37,870
here is to infer black or not black this

245
00:12:33,819 --> 00:12:40,569
is shown by blue and red points so as

246
00:12:37,870 --> 00:12:44,350
you can see the separation between blue

247
00:12:40,569 --> 00:12:47,589
and red are visually very obvious in the

248
00:12:44,350 --> 00:12:49,949
first two layers of the model so this

249
00:12:47,589 --> 00:12:53,470
really means the model learned features

250
00:12:49,949 --> 00:12:55,170
for inferring the properties in the

251
00:12:53,470 --> 00:12:58,870
first two layers

252
00:12:55,170 --> 00:13:01,620
in the seperation between triangles and

253
00:12:58,870 --> 00:13:04,750
circles also appeared in the final error

254
00:13:01,620 --> 00:13:08,290
so the model also learned to predict

255
00:13:04,750 --> 00:13:15,910
genders which is the main task in the

256
00:13:08,290 --> 00:13:18,730
final layers so here to take ways we

257
00:13:15,910 --> 00:13:21,910
have shown model can unintentionally

258
00:13:18,730 --> 00:13:24,490
learn all kinds of features the features

259
00:13:21,910 --> 00:13:27,160
that are helpful to infer the properties

260
00:13:24,490 --> 00:13:31,300
that are uncorrelated with its learning

261
00:13:27,160 --> 00:13:34,209
tasks on the other hand collaborative

262
00:13:31,300 --> 00:13:36,910
training can reveal white box model

263
00:13:34,210 --> 00:13:40,650
updates to its participants and its

264
00:13:36,910 --> 00:13:44,920
server and as a consequence

265
00:13:40,650 --> 00:13:47,199
whoever can access the model updates can

266
00:13:44,920 --> 00:13:52,449
learn information about participants

267
00:13:47,200 --> 00:13:55,570
training data so thank you very much and

268
00:13:52,450 --> 00:14:04,810
our code is also available in my github

269
00:13:55,570 --> 00:14:11,470
account and now I can take questions so

270
00:14:04,810 --> 00:14:13,000
questions please hi thank you for the

271
00:14:11,470 --> 00:14:15,130
talk this is Roxanne and John Massey

272
00:14:13,000 --> 00:14:17,320
from Columbia um there is work on

273
00:14:15,130 --> 00:14:21,120
differentially private collaborative

274
00:14:17,320 --> 00:14:25,480
training do your attacks apply to that

275
00:14:21,120 --> 00:14:28,450
we we mentioned during the the one

276
00:14:25,480 --> 00:14:31,000
applied few language models yes we

277
00:14:28,450 --> 00:14:35,020
mentioned that work in our as one of our

278
00:14:31,000 --> 00:14:38,880
countermeasures and so their their work

279
00:14:35,020 --> 00:14:41,500
basically needs like millions of

280
00:14:38,880 --> 00:14:45,460
participants to ensure the model is

281
00:14:41,500 --> 00:14:47,140
converging and in our case are the

282
00:14:45,460 --> 00:14:50,560
number of participants is much smaller

283
00:14:47,140 --> 00:14:54,130
in their case so their message doesn't

284
00:14:50,560 --> 00:14:55,750
converge in in our case so the model

285
00:14:54,130 --> 00:14:58,030
doesn't perform well on his main tasks

286
00:14:55,750 --> 00:15:01,090
yeah okay

287
00:14:58,030 --> 00:15:03,350
another question so at least yeah

288
00:15:01,090 --> 00:15:06,509
before I ask one

289
00:15:03,350 --> 00:15:08,399
so maybe quickly I ask so can you say

290
00:15:06,509 --> 00:15:09,749
it's a relation to overfitting and

291
00:15:08,399 --> 00:15:12,089
generalization in the whole thing

292
00:15:09,749 --> 00:15:15,539
because it seems that the updates carry

293
00:15:12,089 --> 00:15:17,609
a lot of overfitting things yeah that

294
00:15:15,539 --> 00:15:21,029
could be one of the reasons it's more

295
00:15:17,609 --> 00:15:22,979
like they they didn't over fit in the

296
00:15:21,029 --> 00:15:25,049
sense that the model can still perform

297
00:15:22,979 --> 00:15:27,089
well on on the test data you still

298
00:15:25,049 --> 00:15:30,689
generalized through the test data but it

299
00:15:27,089 --> 00:15:33,029
still learns features that that can

300
00:15:30,689 --> 00:15:34,439
helpful for inferring other properties

301
00:15:33,029 --> 00:15:38,220
along the way yeah okay

302
00:15:34,439 --> 00:15:42,419
so last question from Samsung Research

303
00:15:38,220 --> 00:15:45,179
America but it might inspire some other

304
00:15:42,419 --> 00:15:47,209
research a friend of mine works in AI

305
00:15:45,179 --> 00:15:50,249
fairness and making sure that the

306
00:15:47,209 --> 00:15:52,228
training is not biased in some way

307
00:15:50,249 --> 00:15:53,939
so the last example that you showed

308
00:15:52,229 --> 00:15:55,829
where they were training for ginger but

309
00:15:53,939 --> 00:15:58,858
they seemed to maybe have certain

310
00:15:55,829 --> 00:16:02,309
distributions based on race I'm just

311
00:15:58,859 --> 00:16:04,410
curious if some of these abilities to

312
00:16:02,309 --> 00:16:06,569
detect these indirect properties that

313
00:16:04,410 --> 00:16:10,139
are sort of accidentally influencing

314
00:16:06,569 --> 00:16:12,509
things or leaked features might allow

315
00:16:10,139 --> 00:16:15,149
the central part of the federated

316
00:16:12,509 --> 00:16:17,789
government or federated learning to sort

317
00:16:15,149 --> 00:16:20,789
of say like hey your inputs don't seem

318
00:16:17,789 --> 00:16:23,009
to be very fairly distributed maybe you

319
00:16:20,789 --> 00:16:24,419
have too much of one property that we

320
00:16:23,009 --> 00:16:26,129
really don't want or you're maybe

321
00:16:24,419 --> 00:16:28,919
training with the same input as this

322
00:16:26,129 --> 00:16:30,209
other guy we can try to undo some I

323
00:16:28,919 --> 00:16:32,879
guess maybe there are legitimate use

324
00:16:30,209 --> 00:16:36,599
cases to being able to infer some of the

325
00:16:32,879 --> 00:16:39,889
information about the input yeah that

326
00:16:36,600 --> 00:16:43,859
could be very interesting approach -

327
00:16:39,889 --> 00:16:44,390
yeah okay so let's thank the speaker

328
00:16:43,859 --> 00:16:49,850
again

329
00:16:44,390 --> 00:16:49,850
[Applause]

