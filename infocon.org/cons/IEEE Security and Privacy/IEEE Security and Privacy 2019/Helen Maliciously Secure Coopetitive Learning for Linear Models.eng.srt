1
00:00:08,610 --> 00:00:13,000
hello everyone my name is Winton and

2
00:00:11,320 --> 00:00:15,400
today I'm going to talk about Helen

3
00:00:13,000 --> 00:00:17,980
maliciously secure cooperative learning

4
00:00:15,400 --> 00:00:21,010
for linear models and this is a joint

5
00:00:17,980 --> 00:00:24,310
work from Luca with Luca Joey and Jung

6
00:00:21,010 --> 00:00:26,500
from the UC Berkeley rice lab so let me

7
00:00:24,310 --> 00:00:28,418
first start with a use case which is

8
00:00:26,500 --> 00:00:30,669
urea use case I think some of you may

9
00:00:28,419 --> 00:00:32,619
have seen before and it's also needed by

10
00:00:30,669 --> 00:00:35,050
or as a rice lab our industry

11
00:00:32,619 --> 00:00:37,210
collaborators so here we have a bank who

12
00:00:35,050 --> 00:00:39,550
wants to detect fraud using machine

13
00:00:37,210 --> 00:00:41,260
learning however fraud is kind of

14
00:00:39,550 --> 00:00:44,110
difficult detect by a single bank

15
00:00:41,260 --> 00:00:47,980
because criminals try to conceal their

16
00:00:44,110 --> 00:00:50,860
illegal activities across many banks so

17
00:00:47,980 --> 00:00:52,660
really the ideal way to compute a model

18
00:00:50,860 --> 00:00:54,970
for fraud detection is to join a

19
00:00:52,660 --> 00:00:57,150
computer model on customer transaction

20
00:00:54,970 --> 00:00:59,739
data across many of these banks

21
00:00:57,150 --> 00:01:01,900
however this currently cannot happen

22
00:00:59,740 --> 00:01:04,269
right now because the banks cannot share

23
00:01:01,900 --> 00:01:06,429
data with each other because they're in

24
00:01:04,269 --> 00:01:10,290
competition with each other and also

25
00:01:06,430 --> 00:01:15,159
possibly to do some privacy regulations

26
00:01:10,290 --> 00:01:19,600
so one possible solution to this is to

27
00:01:15,159 --> 00:01:23,409
use a trusted third party where the

28
00:01:19,600 --> 00:01:25,329
banks actually transfer their data to to

29
00:01:23,409 --> 00:01:27,969
this trusted third party and then this

30
00:01:25,329 --> 00:01:30,428
third party outputs a model and gives

31
00:01:27,969 --> 00:01:33,158
the model bags to these banks however

32
00:01:30,429 --> 00:01:36,159
this is not very ideal because again

33
00:01:33,159 --> 00:01:38,679
from our conversations with both the

34
00:01:36,159 --> 00:01:39,999
banks and the trusted third party we

35
00:01:38,679 --> 00:01:41,799
learned that actually they have some

36
00:01:39,999 --> 00:01:43,060
various concerns from both sides so from

37
00:01:41,799 --> 00:01:44,350
the bank side they think that the

38
00:01:43,060 --> 00:01:46,299
trusted third party is rather expensive

39
00:01:44,350 --> 00:01:48,999
and they don't know how you know

40
00:01:46,299 --> 00:01:52,240
trustworthy this third party is really

41
00:01:48,999 --> 00:01:55,869
because we have seen many cases of third

42
00:01:52,240 --> 00:01:57,789
parties being breached and actually from

43
00:01:55,869 --> 00:01:59,499
the trusted xxx part of third party side

44
00:01:57,789 --> 00:02:02,109
they were also a bit concerned because

45
00:01:59,499 --> 00:02:04,298
now that they have all this data right

46
00:02:02,109 --> 00:02:06,249
sensitive data from the banks they kind

47
00:02:04,299 --> 00:02:09,000
of become a central point of effort of

48
00:02:06,249 --> 00:02:11,530
attack so they are very concerned for

49
00:02:09,000 --> 00:02:15,280
liability reasons if they are actually

50
00:02:11,530 --> 00:02:17,049
compromised so now the question we want

51
00:02:15,280 --> 00:02:19,430
to ask is how to actually allow these

52
00:02:17,049 --> 00:02:22,099
organizations to cooperate in the press

53
00:02:19,430 --> 00:02:24,200
of competition without sharing the data

54
00:02:22,099 --> 00:02:26,599
in playing so plain text with each other

55
00:02:24,200 --> 00:02:29,629
and without outsourcing to a trusted

56
00:02:26,599 --> 00:02:32,540
third party and we actually named the

57
00:02:29,629 --> 00:02:34,069
setting co-op position which is where

58
00:02:32,540 --> 00:02:36,409
you know the cooperative learning comes

59
00:02:34,069 --> 00:02:38,689
from which by the way is a real word

60
00:02:36,409 --> 00:02:40,700
it's this is a definition taken from the

61
00:02:38,689 --> 00:02:42,290
dictionary and we thought that along

62
00:02:40,700 --> 00:02:44,569
with the pronunciation guy you know if

63
00:02:42,290 --> 00:02:46,760
you're interested and we felt like this

64
00:02:44,569 --> 00:02:48,649
setting really captured or this word

65
00:02:46,760 --> 00:02:50,659
captures the setting really well because

66
00:02:48,650 --> 00:02:52,159
you know by definition means that you

67
00:02:50,659 --> 00:02:54,349
have collaboration between business

68
00:02:52,159 --> 00:02:59,298
competitors in the hope of mutually

69
00:02:54,349 --> 00:03:01,578
beneficial results and this competitive

70
00:02:59,299 --> 00:03:02,900
setting corresponds exactly to a really

71
00:03:01,579 --> 00:03:04,909
well-known research direction in

72
00:03:02,900 --> 00:03:08,000
cryptography called secure multi-party

73
00:03:04,909 --> 00:03:10,280
computation and the goal of secure

74
00:03:08,000 --> 00:03:11,989
multi-party computation or MPC is to

75
00:03:10,280 --> 00:03:14,780
have the parties emulate a trusted third

76
00:03:11,989 --> 00:03:16,849
party via cryptography but then without

77
00:03:14,780 --> 00:03:19,700
actually using the trust Authority right

78
00:03:16,849 --> 00:03:21,530
so the nice guarantee that it provides

79
00:03:19,700 --> 00:03:23,750
is that no party should learn any

80
00:03:21,530 --> 00:03:25,519
parties any other parties input beyond

81
00:03:23,750 --> 00:03:28,459
what is learn from the final result and

82
00:03:25,519 --> 00:03:30,799
this kind of gives you the security

83
00:03:28,459 --> 00:03:36,409
guarantees of utilizing an ideal trusted

84
00:03:30,799 --> 00:03:38,629
third party service now our bank is has

85
00:03:36,409 --> 00:03:40,790
heard about MPC and is very excited to

86
00:03:38,629 --> 00:03:42,409
utilize it to compute this fraud

87
00:03:40,790 --> 00:03:45,260
detection model he wants to make sure

88
00:03:42,409 --> 00:03:47,269
that its desire security properties are

89
00:03:45,260 --> 00:03:50,298
satisfied so from a bank's perspective

90
00:03:47,269 --> 00:03:54,620
right what the bank wants is to only

91
00:03:50,299 --> 00:03:57,199
trust itself and it wants to make sure

92
00:03:54,620 --> 00:03:59,419
that its data confidentiality should not

93
00:03:57,199 --> 00:04:01,459
depend on the correct behavior of other

94
00:03:59,419 --> 00:04:03,439
banks and the state of privacy is

95
00:04:01,459 --> 00:04:05,389
actually guaranteed even of all other

96
00:04:03,439 --> 00:04:06,680
banks misbehavior and here are one kind

97
00:04:05,389 --> 00:04:09,049
of ones who emphasize that our

98
00:04:06,680 --> 00:04:10,400
cooperative setting is a little bit

99
00:04:09,049 --> 00:04:13,189
different from the federated setting

100
00:04:10,400 --> 00:04:14,509
that was mentioned earlier in that we

101
00:04:13,189 --> 00:04:16,279
assume that in this setting the banks

102
00:04:14,509 --> 00:04:20,930
also do not want to leave the

103
00:04:16,279 --> 00:04:22,638
intermediate global models either so the

104
00:04:20,930 --> 00:04:24,470
desired properties from the previous

105
00:04:22,639 --> 00:04:26,120
page are roughly translated to the

106
00:04:24,470 --> 00:04:27,469
following threat model right so you have

107
00:04:26,120 --> 00:04:29,599
the secure computation you want to have

108
00:04:27,469 --> 00:04:31,700
the secure computation done among all

109
00:04:29,599 --> 00:04:32,750
these parties among the banks themselves

110
00:04:31,700 --> 00:04:35,229
for exam

111
00:04:32,750 --> 00:04:37,700
and we want to assume that this hacker

112
00:04:35,230 --> 00:04:39,890
who can compromise you know subset of

113
00:04:37,700 --> 00:04:42,979
these particles actually compromise p- 1

114
00:04:39,890 --> 00:04:45,500
out of p parties and we also want to

115
00:04:42,980 --> 00:04:46,910
have a protection against a malicious

116
00:04:45,500 --> 00:04:49,610
attacker which means that the attacker

117
00:04:46,910 --> 00:04:53,450
can actually deviate from the protocol

118
00:04:49,610 --> 00:04:56,030
by you know trying to maybe execute a

119
00:04:53,450 --> 00:04:59,050
different functionality or substituting

120
00:04:56,030 --> 00:05:01,280
ecosystem puts into the computation

121
00:04:59,050 --> 00:05:03,200
however under this threat model we do

122
00:05:01,280 --> 00:05:05,989
allow the parties who input data of

123
00:05:03,200 --> 00:05:07,580
their choice and we also you know allow

124
00:05:05,990 --> 00:05:09,650
the parties to learn the final model

125
00:05:07,580 --> 00:05:12,169
because this kind of Wantley words and

126
00:05:09,650 --> 00:05:16,640
functionality of the of the training

127
00:05:12,170 --> 00:05:18,740
process so with this if we take a look

128
00:05:16,640 --> 00:05:20,960
at some of the prior work in this area

129
00:05:18,740 --> 00:05:23,300
are currently actually the framework

130
00:05:20,960 --> 00:05:26,239
that handles the requirements is a

131
00:05:23,300 --> 00:05:30,980
generic framework and pc framework like

132
00:05:26,240 --> 00:05:33,860
speeds however utilizing you know by

133
00:05:30,980 --> 00:05:37,190
doing training in using a more generic

134
00:05:33,860 --> 00:05:39,500
and pc framework is kind of challenging

135
00:05:37,190 --> 00:05:41,750
because of performance so we actually

136
00:05:39,500 --> 00:05:44,210
implemented stochastic gradient descent

137
00:05:41,750 --> 00:05:46,940
or SVD which is a commonly used training

138
00:05:44,210 --> 00:05:49,760
algorithm for lasso which is a you know

139
00:05:46,940 --> 00:05:52,310
simple regularize linear model using

140
00:05:49,760 --> 00:05:54,110
speeds and for in the setting of for

141
00:05:52,310 --> 00:05:55,940
parties hundred thousand samples of

142
00:05:54,110 --> 00:05:58,430
party on a data set of ninety features

143
00:05:55,940 --> 00:05:59,960
we actually estimated i would take three

144
00:05:58,430 --> 00:06:01,460
months to train this model now we

145
00:05:59,960 --> 00:06:07,130
couldn't actually run it and you ran out

146
00:06:01,460 --> 00:06:08,930
of memory way before and so to solve

147
00:06:07,130 --> 00:06:11,630
this problem we designed and built hella

148
00:06:08,930 --> 00:06:13,220
which is a training system that gives

149
00:06:11,630 --> 00:06:15,469
you this strong security guarantee

150
00:06:13,220 --> 00:06:17,300
protection against malicious hacker who

151
00:06:15,470 --> 00:06:20,860
can compromise a majority of the parties

152
00:06:17,300 --> 00:06:23,450
and we decided to support a more

153
00:06:20,860 --> 00:06:24,830
restricted subset of the machine

154
00:06:23,450 --> 00:06:27,620
learning which are regularize linear

155
00:06:24,830 --> 00:06:29,419
models but we thought like that still

156
00:06:27,620 --> 00:06:30,850
solves a very significant slice of

157
00:06:29,419 --> 00:06:32,930
machine learning and statistics

158
00:06:30,850 --> 00:06:36,910
statistics problems like cancer research

159
00:06:32,930 --> 00:06:39,440
genomics and financial applications and

160
00:06:36,910 --> 00:06:41,900
as it turns out trying to improve

161
00:06:39,440 --> 00:06:43,940
performance under such a strong threat

162
00:06:41,900 --> 00:06:45,650
model it's a pretty difficult problem so

163
00:06:43,940 --> 00:06:47,060
for Helen

164
00:06:45,650 --> 00:06:50,000
we really had to elaborate techniques

165
00:06:47,060 --> 00:06:51,979
from three different areas machine

166
00:06:50,000 --> 00:06:55,550
learning systems as well as cryptography

167
00:06:51,979 --> 00:06:57,949
for our solution and actually we found

168
00:06:55,550 --> 00:07:00,169
that with our insights were able to

169
00:06:57,949 --> 00:07:02,780
reduce the estimated three months to

170
00:07:00,169 --> 00:07:05,840
under three hours of training the same

171
00:07:02,780 --> 00:07:08,000
model in Helen okay so now I'm going to

172
00:07:05,840 --> 00:07:10,159
quickly go over give me you an overview

173
00:07:08,000 --> 00:07:13,970
of our techniques from these three areas

174
00:07:10,160 --> 00:07:16,280
so first the first problem of will fit

175
00:07:13,970 --> 00:07:18,919
what we faced was the fact that a common

176
00:07:16,280 --> 00:07:23,690
training algorithm like STD right scans

177
00:07:18,919 --> 00:07:25,940
the entire dataset and and actually

178
00:07:23,690 --> 00:07:28,580
requires so if we were to put that

179
00:07:25,940 --> 00:07:30,949
entire training algorithm inside NPC

180
00:07:28,580 --> 00:07:35,180
would require an expensive and PC

181
00:07:30,949 --> 00:07:37,639
synchronization for each iteration so we

182
00:07:35,180 --> 00:07:40,310
instead actually use an alternative

183
00:07:37,639 --> 00:07:42,500
approach so we use a different training

184
00:07:40,310 --> 00:07:45,620
formulation called a DMM that is crypto

185
00:07:42,500 --> 00:07:47,000
friendly and in our model a DMM is able

186
00:07:45,620 --> 00:07:49,430
to scan the data once in the beginning

187
00:07:47,000 --> 00:07:51,560
to produce a summary of the entire data

188
00:07:49,430 --> 00:07:54,080
and then during training it will

189
00:07:51,560 --> 00:07:56,120
repeatedly use this data and with this

190
00:07:54,080 --> 00:07:59,570
and the number of iterations as well as

191
00:07:56,120 --> 00:08:01,810
the NPC synchronization both are greatly

192
00:07:59,570 --> 00:08:01,810
reduced

193
00:08:02,020 --> 00:08:07,549
however adapting a DMM to secure

194
00:08:05,060 --> 00:08:09,979
computation efficiently it's actually so

195
00:08:07,550 --> 00:08:12,349
a bit tricky right so if we look at the

196
00:08:09,979 --> 00:08:14,389
way the summary is being computed we

197
00:08:12,349 --> 00:08:16,430
actually face a second problem because

198
00:08:14,389 --> 00:08:19,039
we're in the militia setting right the

199
00:08:16,430 --> 00:08:20,960
attacker who can compromise parties

200
00:08:19,039 --> 00:08:22,520
could actually deviate from the protocol

201
00:08:20,960 --> 00:08:26,388
then we actually need to make sure that

202
00:08:22,520 --> 00:08:28,760
the summer in creation is also proven

203
00:08:26,389 --> 00:08:30,410
correctly that you know the Hardys need

204
00:08:28,760 --> 00:08:32,479
to prove that it has been computed

205
00:08:30,410 --> 00:08:34,820
correctly unfortunately if we will put

206
00:08:32,479 --> 00:08:38,300
the entire summary calculation inside

207
00:08:34,820 --> 00:08:40,370
these cryptographic proofs then becomes

208
00:08:38,299 --> 00:08:43,189
pretty heavy way because the proof now

209
00:08:40,370 --> 00:08:45,470
has to take in the entire large training

210
00:08:43,190 --> 00:08:47,720
dataset as the input and that was scale

211
00:08:45,470 --> 00:08:49,220
in the number of samples of Part II

212
00:08:47,720 --> 00:08:56,600
which could be very very large like in

213
00:08:49,220 --> 00:08:59,430
the millions so we instead so um yeah

214
00:08:56,600 --> 00:09:02,220
yeah so on this so our second

215
00:08:59,430 --> 00:09:04,650
from the system side is to generate a

216
00:09:02,220 --> 00:09:07,440
create a scalable proof generation of

217
00:09:04,650 --> 00:09:09,449
these summaries using our first singular

218
00:09:07,440 --> 00:09:12,960
value decomposition to reduce the input

219
00:09:09,450 --> 00:09:14,720
into a much much smaller emphasizes and

220
00:09:12,960 --> 00:09:17,400
this can be done locally in plain text

221
00:09:14,720 --> 00:09:19,890
each party and then the zero knowledge

222
00:09:17,400 --> 00:09:23,699
proof that we use will take in the much

223
00:09:19,890 --> 00:09:26,010
much smaller data set and we prove in

224
00:09:23,700 --> 00:09:27,660
our paper that this doesn't the still

225
00:09:26,010 --> 00:09:33,150
provides the same security guarantees as

226
00:09:27,660 --> 00:09:34,469
before and so we made so yeah okay so

227
00:09:33,150 --> 00:09:36,600
finally if we take a look at the

228
00:09:34,470 --> 00:09:38,100
iterative training process that we used

229
00:09:36,600 --> 00:09:39,930
as a summary there's actually a third

230
00:09:38,100 --> 00:09:41,970
problem because the original 80mm

231
00:09:39,930 --> 00:09:44,099
formulation actually allows for nice

232
00:09:41,970 --> 00:09:47,160
decentralized computation but then when

233
00:09:44,100 --> 00:09:49,500
you formulate that inside of your MPC is

234
00:09:47,160 --> 00:09:50,550
centralizing the computation because

235
00:09:49,500 --> 00:09:53,520
you're requires ever going to

236
00:09:50,550 --> 00:09:56,459
participate for the decentralized

237
00:09:53,520 --> 00:09:58,410
computation or each piece of the

238
00:09:56,460 --> 00:10:00,690
decentralized computation from before so

239
00:09:58,410 --> 00:10:02,760
in how long we actually bring back the

240
00:10:00,690 --> 00:10:04,950
decentralization by using a mixed

241
00:10:02,760 --> 00:10:07,650
protocol that combines both home Orphic

242
00:10:04,950 --> 00:10:10,350
encryption as well as MPC together so

243
00:10:07,650 --> 00:10:13,050
that you know you can still do local

244
00:10:10,350 --> 00:10:15,300
computation and for the other parts that

245
00:10:13,050 --> 00:10:19,650
require global synchronization we put

246
00:10:15,300 --> 00:10:21,569
that inside of MPC okay so that is an

247
00:10:19,650 --> 00:10:23,699
overview of our techniques and now I'm

248
00:10:21,570 --> 00:10:27,420
going to quickly go through a Helens a

249
00:10:23,700 --> 00:10:29,520
workflow so the first stage in Helen is

250
00:10:27,420 --> 00:10:32,069
the setup when the parties need to come

251
00:10:29,520 --> 00:10:34,290
together and agree to do collaborating

252
00:10:32,070 --> 00:10:36,480
and also as you agree that they will

253
00:10:34,290 --> 00:10:40,110
release the result which is the model in

254
00:10:36,480 --> 00:10:42,300
plain text and then during this phase

255
00:10:40,110 --> 00:10:43,890
during the setup phase the parties also

256
00:10:42,300 --> 00:10:45,810
need to do a one-time initialization

257
00:10:43,890 --> 00:10:47,490
step to generate some parameters that

258
00:10:45,810 --> 00:10:50,729
Helen will utilize during the actual

259
00:10:47,490 --> 00:10:53,400
training in this case we are creating a

260
00:10:50,730 --> 00:10:55,800
threshold morphic encryption scheme that

261
00:10:53,400 --> 00:10:58,410
allows that gives us the strong security

262
00:10:55,800 --> 00:11:00,270
guarantees that we have we are since

263
00:10:58,410 --> 00:11:02,670
this is done once if you are at the

264
00:11:00,270 --> 00:11:03,930
beginning if the party and can be reused

265
00:11:02,670 --> 00:11:06,180
if the party configuration does not

266
00:11:03,930 --> 00:11:10,020
change we do is we simply put the key

267
00:11:06,180 --> 00:11:12,359
generation instead of MPC and and the

268
00:11:10,020 --> 00:11:12,750
key generation will give a public key to

269
00:11:12,360 --> 00:11:15,140
you

270
00:11:12,750 --> 00:11:17,640
party and actually split the secret key

271
00:11:15,140 --> 00:11:21,770
into share so that each party has a

272
00:11:17,640 --> 00:11:24,150
single share and the property this

273
00:11:21,770 --> 00:11:25,800
encryption scheme has is that you all

274
00:11:24,150 --> 00:11:28,199
parties can encrypt but then you need

275
00:11:25,800 --> 00:11:33,630
all P parties together to decrypt the

276
00:11:28,200 --> 00:11:35,850
ciphertext so now it becomes the input

277
00:11:33,630 --> 00:11:37,260
preparation phase which as you remember

278
00:11:35,850 --> 00:11:40,010
we have to create a summary at the

279
00:11:37,260 --> 00:11:42,870
beginning and this is that step so

280
00:11:40,010 --> 00:11:45,210
during this stuff we the party is

281
00:11:42,870 --> 00:11:47,640
locally pre compute and prove that these

282
00:11:45,210 --> 00:11:51,450
summaries are you know computed

283
00:11:47,640 --> 00:11:56,130
correctly using our insights to first of

284
00:11:51,450 --> 00:11:57,750
all reduce the input training data into

285
00:11:56,130 --> 00:11:59,520
a much smaller summary and the generate

286
00:11:57,750 --> 00:12:01,640
prove that it's been done correctly and

287
00:11:59,520 --> 00:12:06,600
each party does this separately in a

288
00:12:01,640 --> 00:12:08,670
decentralized way after this step we go

289
00:12:06,600 --> 00:12:11,910
into the iterator tree iterative

290
00:12:08,670 --> 00:12:14,969
training process and in this iteration

291
00:12:11,910 --> 00:12:17,640
are in the in each iteration rather

292
00:12:14,970 --> 00:12:21,030
there are two steps the first step is a

293
00:12:17,640 --> 00:12:26,030
local optimization step where the party

294
00:12:21,030 --> 00:12:29,010
is actually compute improve some local

295
00:12:26,030 --> 00:12:31,589
computation in a decentralized way using

296
00:12:29,010 --> 00:12:34,050
the summaries that they pre computed in

297
00:12:31,589 --> 00:12:36,089
the input preparation phase as well as

298
00:12:34,050 --> 00:12:37,650
the global way that you see that they

299
00:12:36,089 --> 00:12:40,350
received from the previous iteration

300
00:12:37,650 --> 00:12:42,180
where the initialized weights if you're

301
00:12:40,350 --> 00:12:44,280
in the first iteration so in this case

302
00:12:42,180 --> 00:12:46,530
what they do is simply they use home

303
00:12:44,280 --> 00:12:49,020
Orphic encryption to operate on their

304
00:12:46,530 --> 00:12:50,790
summary which is in plain text as well

305
00:12:49,020 --> 00:12:53,280
as the global weight which is encrypted

306
00:12:50,790 --> 00:12:59,459
and then each party does this in a

307
00:12:53,280 --> 00:13:02,939
decentralized way and next during the

308
00:12:59,460 --> 00:13:04,980
next phase for each during the free

309
00:13:02,940 --> 00:13:06,570
iteration is the coordination step where

310
00:13:04,980 --> 00:13:10,410
the party is coordinated together to

311
00:13:06,570 --> 00:13:12,480
compute a global weight from their local

312
00:13:10,410 --> 00:13:14,219
weights and this is done just doing or

313
00:13:12,480 --> 00:13:18,990
where or we're using MPC for this step

314
00:13:14,220 --> 00:13:20,850
and at the end of this step the global

315
00:13:18,990 --> 00:13:22,620
ways which is computed is distributed

316
00:13:20,850 --> 00:13:24,339
back to all of the parties and the

317
00:13:22,620 --> 00:13:29,829
parties can start

318
00:13:24,339 --> 00:13:31,569
training process again so once all the

319
00:13:29,829 --> 00:13:35,649
iterations are completely the parties

320
00:13:31,569 --> 00:13:36,160
then use their secret key shares to join

321
00:13:35,649 --> 00:13:39,220
me

322
00:13:36,160 --> 00:13:42,490
do you crypt the final weights and the

323
00:13:39,220 --> 00:13:48,730
final weights are given back to all the

324
00:13:42,490 --> 00:13:51,999
parties in plain text okay great now I'm

325
00:13:48,730 --> 00:13:54,579
going to go into our evaluation so in

326
00:13:51,999 --> 00:13:58,809
terms of the experiment setup we used

327
00:13:54,579 --> 00:14:00,638
for parties we experiment on ec2 and we

328
00:13:58,809 --> 00:14:02,199
put two parties the Oregon and two

329
00:14:00,639 --> 00:14:04,589
parties in Northern Virginia to kind of

330
00:14:02,199 --> 00:14:08,319
simulate this more wide area network

331
00:14:04,589 --> 00:14:12,819
setting the baseline that we compare to

332
00:14:08,319 --> 00:14:16,529
is SGD implemented in skis a generic

333
00:14:12,819 --> 00:14:18,519
maliciously secure MPC platform and we

334
00:14:16,529 --> 00:14:21,370
make some training assumptions in that

335
00:14:18,519 --> 00:14:25,470
SGD has to scan the data the input data

336
00:14:21,370 --> 00:14:27,610
once and for a DMM because we're able to

337
00:14:25,470 --> 00:14:29,199
as I mention before I want every

338
00:14:27,610 --> 00:14:32,980
insights that we can actually summarize

339
00:14:29,199 --> 00:14:34,389
the data in the beginning and then it

340
00:14:32,980 --> 00:14:37,540
early train on that we actually

341
00:14:34,389 --> 00:14:40,300
experimentally found the maximum number

342
00:14:37,540 --> 00:14:42,839
of iterations I would take for our data

343
00:14:40,300 --> 00:14:46,179
sets to convert which is 10 in this case

344
00:14:42,839 --> 00:14:48,249
and I'm now going to show accuracy

345
00:14:46,179 --> 00:14:50,470
results here but you can take a look at

346
00:14:48,249 --> 00:14:51,970
that in our paper the howling is able to

347
00:14:50,470 --> 00:14:57,100
achieve the same accuracy as the

348
00:14:51,970 --> 00:14:59,620
baseline model okay so I'm already going

349
00:14:57,100 --> 00:15:01,509
to show our evaluation on one dataset we

350
00:14:59,620 --> 00:15:03,309
do have more in the paper and this

351
00:15:01,509 --> 00:15:06,209
dataset is a sound prediction data set

352
00:15:03,309 --> 00:15:08,529
from UCI which is a machine learning or

353
00:15:06,209 --> 00:15:09,819
repository information you know open

354
00:15:08,529 --> 00:15:12,370
source machine learning data sets and

355
00:15:09,819 --> 00:15:16,689
for this one it has for this particular

356
00:15:12,370 --> 00:15:19,179
dataset has 90 features and so here's a

357
00:15:16,689 --> 00:15:21,879
graph on the x-axis we scale up the

358
00:15:19,179 --> 00:15:25,089
number of samples for Part II and on the

359
00:15:21,879 --> 00:15:26,319
y-axis we that we have the time in

360
00:15:25,089 --> 00:15:28,149
seconds which is a little bit hard to

361
00:15:26,319 --> 00:15:32,229
read so these are more human readable

362
00:15:28,149 --> 00:15:32,850
units on the right side and so this is

363
00:15:32,230 --> 00:15:35,699
the

364
00:15:32,850 --> 00:15:37,940
summated baseline which is in blue and

365
00:15:35,699 --> 00:15:41,339
as you can see it scales linearly

366
00:15:37,940 --> 00:15:43,680
because as I mentioned before HED has to

367
00:15:41,339 --> 00:15:48,149
scan more data as you increase the

368
00:15:43,680 --> 00:15:50,819
number of samples per party held on the

369
00:15:48,149 --> 00:15:53,490
other hand looks constant actually does

370
00:15:50,819 --> 00:15:54,769
scale up pretty slowly the reason it

371
00:15:53,490 --> 00:15:57,300
looks constant is that the cryptographic

372
00:15:54,769 --> 00:15:58,980
operation greatly dominates the

373
00:15:57,300 --> 00:16:01,319
plaintext operation in this case and

374
00:15:58,980 --> 00:16:03,180
while the plaintext operation scales in

375
00:16:01,319 --> 00:16:05,790
the number samples it is very very fast

376
00:16:03,180 --> 00:16:07,319
so and the crypto we designed in such

377
00:16:05,790 --> 00:16:08,910
way so that doesn't scale in the number

378
00:16:07,319 --> 00:16:10,740
samples which means that you know you

379
00:16:08,910 --> 00:16:14,399
see this you're kind of constant

380
00:16:10,740 --> 00:16:16,980
behavior and for a hundred thousand

381
00:16:14,399 --> 00:16:22,160
samples which is a pretty moderate size

382
00:16:16,980 --> 00:16:22,160
we see a roughly 900 X performance game

383
00:16:22,579 --> 00:16:27,239
okay so in conclusion Hellenes a

384
00:16:25,050 --> 00:16:31,019
maliciously secure cooperative training

385
00:16:27,240 --> 00:16:32,579
system for linear models and we're in

386
00:16:31,019 --> 00:16:34,529
general very excited about this

387
00:16:32,579 --> 00:16:37,079
direction because we think that enabling

388
00:16:34,529 --> 00:16:46,139
data sharing can create a lot of value

389
00:16:37,079 --> 00:16:49,829
and yeah that's it thank you so things

390
00:16:46,139 --> 00:16:52,589
questions so maybe I start with one

391
00:16:49,829 --> 00:16:54,479
first question so you basically work

392
00:16:52,589 --> 00:16:55,920
with linear models and I I was wondering

393
00:16:54,480 --> 00:16:58,350
whether you can extend your approach to

394
00:16:55,920 --> 00:17:01,319
also support nonlinear it's these

395
00:16:58,350 --> 00:17:03,810
functions maybe not models yeah yeah so

396
00:17:01,319 --> 00:17:06,299
I think some of our techniques can

397
00:17:03,810 --> 00:17:07,948
definitely be extended to more complex

398
00:17:06,299 --> 00:17:09,869
models I'm actually looking into

399
00:17:07,949 --> 00:17:12,020
extending into logistic regression for

400
00:17:09,869 --> 00:17:15,149
example are just a regression training

401
00:17:12,020 --> 00:17:17,639
because yeah so basically the local

402
00:17:15,150 --> 00:17:20,309
computation part can be translated into

403
00:17:17,640 --> 00:17:21,659
that kind of a lot of as well for new or

404
00:17:20,309 --> 00:17:24,510
networks I would say it's a bit more

405
00:17:21,659 --> 00:17:26,429
challenging good yeah I had something

406
00:17:24,510 --> 00:17:29,190
different than mine so maybe you know

407
00:17:26,429 --> 00:17:31,020
random Kitchen sings is this a technique

408
00:17:29,190 --> 00:17:32,640
from ten years ago where you can

409
00:17:31,020 --> 00:17:34,470
basically take a linear model and then

410
00:17:32,640 --> 00:17:36,450
apply some modifications to the features

411
00:17:34,470 --> 00:17:38,340
and you get a little bit of the power of

412
00:17:36,450 --> 00:17:41,309
nonlinear classifiers and my feeling is

413
00:17:38,340 --> 00:17:43,470
this could be perfect yeah yeah sure

414
00:17:41,309 --> 00:17:44,290
sure I I also a I think that would be

415
00:17:43,470 --> 00:17:45,670
done in the future

416
00:17:44,290 --> 00:17:46,930
they shoot snappy but you think right so

417
00:17:45,670 --> 00:17:49,740
yeah I think dial approach is definitely

418
00:17:46,930 --> 00:17:55,300
complimentary to our sitting as well so

419
00:17:49,740 --> 00:17:56,630
other questions okay so then let's thank

420
00:17:55,300 --> 00:18:02,329
the speaker thank you

421
00:17:56,630 --> 00:18:02,329
[Applause]

