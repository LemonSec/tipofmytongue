1
00:00:08,570 --> 00:00:12,170
okay thank you everyone thanks for the

2
00:00:11,660 --> 00:00:14,030
introduction

3
00:00:12,170 --> 00:00:18,410
I'm Peppa villa and this is a joint word

4
00:00:14,030 --> 00:00:20,119
with Morris cava and Jose Morales so let

5
00:00:18,410 --> 00:00:23,060
me start talking about what Arabic

6
00:00:20,119 --> 00:00:25,329
sunsets we all know that all our

7
00:00:23,060 --> 00:00:27,619
computers have these small memories for

8
00:00:25,329 --> 00:00:30,739
speeding up computations that are called

9
00:00:27,619 --> 00:00:34,519
caches and we usually split these caches

10
00:00:30,739 --> 00:00:37,489
in slices and it just lies has several

11
00:00:34,520 --> 00:00:40,040
cache set and each of these classes has

12
00:00:37,489 --> 00:00:42,320
capacity for associativity many memory

13
00:00:40,040 --> 00:00:46,010
blocks so what we are interested in is

14
00:00:42,320 --> 00:00:48,950
on finding others is that collide in

15
00:00:46,010 --> 00:00:50,900
cache and this means others is that our

16
00:00:48,950 --> 00:00:53,710
map exactly into the same cassette and

17
00:00:50,900 --> 00:00:56,030
specifically what we want to do is find

18
00:00:53,710 --> 00:00:57,260
associativity many of these colliding

19
00:00:56,030 --> 00:01:00,200
other sis and this is what we will call

20
00:00:57,260 --> 00:01:02,449
an eviction set the idea is if we exited

21
00:01:00,200 --> 00:01:04,129
a literal said we will feel completely

22
00:01:02,450 --> 00:01:07,450
fill the content of the sky set and

23
00:01:04,129 --> 00:01:09,680
therefore a bit on the previous content

24
00:01:07,450 --> 00:01:11,869
so why is this interesting

25
00:01:09,680 --> 00:01:14,450
well it turns out that for many micro

26
00:01:11,869 --> 00:01:16,670
detect roll stacks finding a small image

27
00:01:14,450 --> 00:01:19,130
instead is a key primitive for insert

28
00:01:16,670 --> 00:01:21,440
for priming probe the attacker needs to

29
00:01:19,130 --> 00:01:24,640
put several castles in a controller

30
00:01:21,440 --> 00:01:28,539
state and this is known by feeling that

31
00:01:24,640 --> 00:01:31,790
the cache set with its own control data

32
00:01:28,540 --> 00:01:34,040
for row hammer the attacker wants to

33
00:01:31,790 --> 00:01:37,520
activate the RAM row with enough

34
00:01:34,040 --> 00:01:40,130
frequency in order to cause flip beat

35
00:01:37,520 --> 00:01:42,259
flips and in order to do that it need to

36
00:01:40,130 --> 00:01:45,110
bypass the cache so one way for doing

37
00:01:42,260 --> 00:01:47,840
that again is using a mid term search to

38
00:01:45,110 --> 00:01:50,210
remove the thing the data from the cache

39
00:01:47,840 --> 00:01:54,020
and finally some variations of respect

40
00:01:50,210 --> 00:01:56,509
as we have seen the first talk require

41
00:01:54,020 --> 00:01:58,280
to make a specific content from the

42
00:01:56,510 --> 00:02:00,170
cache in order to increase the

43
00:01:58,280 --> 00:02:05,840
speculation window and be able to lead

44
00:02:00,170 --> 00:02:08,149
content so the question now is why is

45
00:02:05,840 --> 00:02:10,729
this a challenging problem well it turns

46
00:02:08,149 --> 00:02:12,769
out that in some cases the attacker may

47
00:02:10,729 --> 00:02:16,189
not know the mapping from physical

48
00:02:12,769 --> 00:02:18,739
addresses into cassettes or even if this

49
00:02:16,189 --> 00:02:20,750
mapping is known since the you that

50
00:02:18,739 --> 00:02:23,120
occurred usually it runs from user space

51
00:02:20,750 --> 00:02:24,860
he doesn't know the translation from

52
00:02:23,120 --> 00:02:26,420
bitola nurses into physical addresses

53
00:02:24,860 --> 00:02:27,950
and that foreign doesn't know the

54
00:02:26,420 --> 00:02:30,290
mapping from built or a decision to

55
00:02:27,950 --> 00:02:33,230
Cosette and there are some a scenario

56
00:02:30,290 --> 00:02:35,260
where this is even worse for instance

57
00:02:33,230 --> 00:02:37,549
when the attacker runs from JavaScript

58
00:02:35,260 --> 00:02:39,470
there is no even notion of virtual

59
00:02:37,550 --> 00:02:41,780
addresses there so the only thing that

60
00:02:39,470 --> 00:02:45,050
the attacker has are array indices and

61
00:02:41,780 --> 00:02:47,480
we need a way to found how this can map

62
00:02:45,050 --> 00:02:49,310
into specific asset so again we are

63
00:02:47,480 --> 00:02:51,350
interested in finding associativity many

64
00:02:49,310 --> 00:02:53,270
elements whatever these elements are

65
00:02:51,350 --> 00:02:56,530
built to elaborate C's or JavaScript

66
00:02:53,270 --> 00:03:00,050
array indices that collide in the cache

67
00:02:56,530 --> 00:03:04,190
the contributions audit of these papers

68
00:03:00,050 --> 00:03:06,050
are three the first is that despite that

69
00:03:04,190 --> 00:03:09,230
a mixer sets have been extensively used

70
00:03:06,050 --> 00:03:10,880
in the past there was no systematic a

71
00:03:09,230 --> 00:03:12,859
study of the problem so we provide this

72
00:03:10,880 --> 00:03:15,739
fair systematic study on the problem of

73
00:03:12,860 --> 00:03:17,630
finding eviction sets we also show

74
00:03:15,739 --> 00:03:19,760
normal algorithms that allows her to

75
00:03:17,630 --> 00:03:21,820
find this eviction set in linear time

76
00:03:19,760 --> 00:03:25,280
compared to the previous quadratic

77
00:03:21,820 --> 00:03:26,900
approaches and finally we perform a

78
00:03:25,280 --> 00:03:28,940
reliability and performance evaluation

79
00:03:26,900 --> 00:03:31,400
of this our ins in real hardware and

80
00:03:28,940 --> 00:03:35,299
this is important because as we will see

81
00:03:31,400 --> 00:03:38,209
now this contribution is not only a

82
00:03:35,299 --> 00:03:40,549
theoretical improvement but really our

83
00:03:38,209 --> 00:03:44,480
algorithm implemented the crucial

84
00:03:40,549 --> 00:03:46,489
performance increase in practice so

85
00:03:44,480 --> 00:03:48,890
let's see how can we find these eviction

86
00:03:46,489 --> 00:03:51,470
sets the general approach is usually

87
00:03:48,890 --> 00:03:53,500
twofold first we want to find a large

88
00:03:51,470 --> 00:03:55,640
eviction set for a certain address we

89
00:03:53,500 --> 00:03:57,980
and what we have this large division

90
00:03:55,640 --> 00:04:01,609
said we will try to reduce it into its

91
00:03:57,980 --> 00:04:04,459
minimal curve so in order to find a

92
00:04:01,610 --> 00:04:06,440
large eviction set we usually only need

93
00:04:04,459 --> 00:04:08,810
to pick enough addresses at random and

94
00:04:06,440 --> 00:04:10,489
for the precise definition of enough I'd

95
00:04:08,810 --> 00:04:12,980
refer you to the paper but the idea is

96
00:04:10,489 --> 00:04:15,380
that since we don't know into its Christ

97
00:04:12,980 --> 00:04:17,719
set and addresses map by picking just

98
00:04:15,380 --> 00:04:19,668
randomly a bunch of addresses the

99
00:04:17,720 --> 00:04:21,019
mortgages that we pick the higher the

100
00:04:19,668 --> 00:04:23,210
probability that these set of addresses

101
00:04:21,019 --> 00:04:27,190
will contain at least associate to be

102
00:04:23,210 --> 00:04:29,900
many that go into the specific asset so

103
00:04:27,190 --> 00:04:31,110
what we have a set of others is what we

104
00:04:29,900 --> 00:04:33,690
can do

105
00:04:31,110 --> 00:04:35,460
is use a timing test to check if this is

106
00:04:33,690 --> 00:04:37,530
an eviction set and the time in test

107
00:04:35,460 --> 00:04:38,400
waters follow we will first access a

108
00:04:37,530 --> 00:04:41,000
victim address

109
00:04:38,400 --> 00:04:44,010
now this others will be in the cache and

110
00:04:41,000 --> 00:04:46,460
then what we do is we access all the

111
00:04:44,010 --> 00:04:48,930
elements in our candidate set and

112
00:04:46,460 --> 00:04:50,669
finally we measure the access time to

113
00:04:48,930 --> 00:04:52,740
the first victim address again if the

114
00:04:50,670 --> 00:04:55,170
access time is first it means that our

115
00:04:52,740 --> 00:04:57,540
candidates that our victim is still in

116
00:04:55,170 --> 00:04:58,980
the cache so I reckon D that said does

117
00:04:57,540 --> 00:05:00,570
not contain at least associativity

118
00:04:58,980 --> 00:05:02,640
colliding blocks and therefore it's not

119
00:05:00,570 --> 00:05:04,980
an eviction set on the contrary if the

120
00:05:02,640 --> 00:05:06,780
access time is large it means that our

121
00:05:04,980 --> 00:05:08,880
victim is not in the cache anymore

122
00:05:06,780 --> 00:05:10,229
so what a candidate set contains at

123
00:05:08,880 --> 00:05:12,690
least associativity many others and

124
00:05:10,230 --> 00:05:16,170
therefore is an eviction just a small

125
00:05:12,690 --> 00:05:17,940
remark you know that in order for the

126
00:05:16,170 --> 00:05:19,530
caches to behave like this we need to do

127
00:05:17,940 --> 00:05:22,260
some assumptions on the replacement

128
00:05:19,530 --> 00:05:24,030
policy but as we'll see later this

129
00:05:22,260 --> 00:05:26,520
assumption it's not a limiting factor in

130
00:05:24,030 --> 00:05:28,500
practice so for this talk I will focus

131
00:05:26,520 --> 00:05:32,729
on the second part of the problem once

132
00:05:28,500 --> 00:05:34,980
we find this large eviction set how can

133
00:05:32,730 --> 00:05:37,350
we reduce it into its minimal curve and

134
00:05:34,980 --> 00:05:40,200
for that let me quickly explain the

135
00:05:37,350 --> 00:05:41,580
commonly approach algorithm till now we

136
00:05:40,200 --> 00:05:44,099
are start with a set of addresses of

137
00:05:41,580 --> 00:05:45,690
size M this is an eviction set and what

138
00:05:44,100 --> 00:05:48,570
we will do is we pick an address and

139
00:05:45,690 --> 00:05:50,310
test the remaining set if the remaining

140
00:05:48,570 --> 00:05:52,020
set is still an eviction said we can

141
00:05:50,310 --> 00:05:54,660
just discard this picked a dress because

142
00:05:52,020 --> 00:05:56,280
it's not part of its minimal code and we

143
00:05:54,660 --> 00:05:58,530
repeat this process several times until

144
00:05:56,280 --> 00:06:00,630
eventually we will remove one element

145
00:05:58,530 --> 00:06:02,340
that makes our set to stop being an

146
00:06:00,630 --> 00:06:05,670
eviction set so at this point what we

147
00:06:02,340 --> 00:06:08,700
learn is that the picked address is part

148
00:06:05,670 --> 00:06:10,650
of the minimal of its own set so we need

149
00:06:08,700 --> 00:06:14,550
to keep track of it and set it again on

150
00:06:10,650 --> 00:06:17,400
the set and repeat this process until we

151
00:06:14,550 --> 00:06:19,440
denta fie associativity many of of the

152
00:06:17,400 --> 00:06:22,289
addresses that represent the weeks on

153
00:06:19,440 --> 00:06:24,180
set minimal court so this all what it is

154
00:06:22,290 --> 00:06:26,010
nice but the problem is that the

155
00:06:24,180 --> 00:06:28,650
complexity cost in number of memory

156
00:06:26,010 --> 00:06:31,710
accesses is quadratic on the initial set

157
00:06:28,650 --> 00:06:34,289
of the initial size of the set and the

158
00:06:31,710 --> 00:06:37,140
natural question that we ask is okay can

159
00:06:34,290 --> 00:06:39,990
we do better than this but before

160
00:06:37,140 --> 00:06:41,610
answering to that let me quickly talk

161
00:06:39,990 --> 00:06:45,230
about group testing and threshold loop

162
00:06:41,610 --> 00:06:48,480
testing the problem of group testing

163
00:06:45,230 --> 00:06:50,760
studied by Robert Dorfman in 1943 in the

164
00:06:48,480 --> 00:06:55,550
context of identifying all the

165
00:06:50,760 --> 00:06:59,099
syphilitic soldiers in an army and his

166
00:06:55,550 --> 00:07:01,680
his key inside were that if imagine that

167
00:06:59,100 --> 00:07:05,100
we have a population of blood samples

168
00:07:01,680 --> 00:07:07,620
and if the ratio of the effective

169
00:07:05,100 --> 00:07:12,470
elements or if infected samples in this

170
00:07:07,620 --> 00:07:15,840
case is a small we can find all the

171
00:07:12,470 --> 00:07:20,310
defective elements more efficiently than

172
00:07:15,840 --> 00:07:23,099
by just doing individual tests so what

173
00:07:20,310 --> 00:07:26,670
we can do is if we split this into

174
00:07:23,100 --> 00:07:30,420
groups of size five by using a test on

175
00:07:26,670 --> 00:07:32,460
this group if the test is negative we

176
00:07:30,420 --> 00:07:34,590
automatically can discard five elements

177
00:07:32,460 --> 00:07:36,299
with a single test if the test is

178
00:07:34,590 --> 00:07:38,280
positive we'll need to split it in

179
00:07:36,300 --> 00:07:41,880
further groups so the problem of group

180
00:07:38,280 --> 00:07:44,880
testing studies what is the optimal size

181
00:07:41,880 --> 00:07:46,320
of these groups or what needs to be the

182
00:07:44,880 --> 00:07:48,480
optimal size of these groups in order to

183
00:07:46,320 --> 00:07:50,280
identify all the minim all the defective

184
00:07:48,480 --> 00:07:53,669
elements in the minimal number of tests

185
00:07:50,280 --> 00:07:55,710
and as you can imagine this had a lot of

186
00:07:53,669 --> 00:07:57,000
applications in practice a lot of and a

187
00:07:55,710 --> 00:07:59,340
lot of people are studying variation

188
00:07:57,000 --> 00:08:01,110
generalizations of this problems and we

189
00:07:59,340 --> 00:08:03,750
are specifically interested in a

190
00:08:01,110 --> 00:08:06,900
generalization by Peter damage clay from

191
00:08:03,750 --> 00:08:08,190
2006 where the goal is the same we are

192
00:08:06,900 --> 00:08:10,620
interested in identifying other

193
00:08:08,190 --> 00:08:12,300
effective elements of our population but

194
00:08:10,620 --> 00:08:14,640
in in this case the test is a slightly

195
00:08:12,300 --> 00:08:15,570
different our test will only give a

196
00:08:14,640 --> 00:08:18,240
positive answer

197
00:08:15,570 --> 00:08:20,490
if the group contains at least you

198
00:08:18,240 --> 00:08:24,000
defective elements and it will give a

199
00:08:20,490 --> 00:08:26,640
negative answer only if it contains at

200
00:08:24,000 --> 00:08:28,830
most L defective elements and it can be

201
00:08:26,640 --> 00:08:31,310
either positive or negative answer with

202
00:08:28,830 --> 00:08:33,870
a random probability in any other case

203
00:08:31,310 --> 00:08:36,929
so we're at the observation here is that

204
00:08:33,870 --> 00:08:40,500
our test our test for animism said is

205
00:08:36,929 --> 00:08:43,739
actually a fresh whole group test if you

206
00:08:40,500 --> 00:08:45,839
think about how this work our test will

207
00:08:43,740 --> 00:08:47,730
only give a positive answer if the set

208
00:08:45,839 --> 00:08:54,780
contains at least associativity many

209
00:08:47,730 --> 00:08:58,200
addresses so with this this inside about

210
00:08:54,780 --> 00:08:58,770
threshold group testing we design the

211
00:08:58,200 --> 00:09:01,800
following

212
00:08:58,770 --> 00:09:03,689
and the settings here are the same we

213
00:09:01,800 --> 00:09:06,540
will start with a large eviction set of

214
00:09:03,690 --> 00:09:08,820
size n and this time what we will do is

215
00:09:06,540 --> 00:09:12,300
a split this the certain associativity

216
00:09:08,820 --> 00:09:14,250
plus 1 subsets so with if we imagine

217
00:09:12,300 --> 00:09:18,920
that in the worst case this original set

218
00:09:14,250 --> 00:09:21,510
contains only for defective elements

219
00:09:18,920 --> 00:09:23,729
what we are interested is in to find a

220
00:09:21,510 --> 00:09:26,420
union of these subsets that is an

221
00:09:23,730 --> 00:09:29,760
eviction set an interesting part is that

222
00:09:26,420 --> 00:09:33,140
in the worst case a table iterations we

223
00:09:29,760 --> 00:09:36,410
can always remove at least one of these

224
00:09:33,140 --> 00:09:40,560
subsets so at each iteration we are

225
00:09:36,410 --> 00:09:42,719
discarding a factor of n number of

226
00:09:40,560 --> 00:09:48,029
elements and if we keep repeating this

227
00:09:42,720 --> 00:09:51,570
process we will end up with our minimal

228
00:09:48,029 --> 00:09:54,330
emission set and in this in our paper we

229
00:09:51,570 --> 00:09:55,830
show that the cost in number of memory

230
00:09:54,330 --> 00:09:57,360
accesses in the worst case cost the

231
00:09:55,830 --> 00:10:00,149
number of memory axis is linear on the

232
00:09:57,360 --> 00:10:04,950
size of the initial size of the initial

233
00:10:00,149 --> 00:10:07,980
sensory so okay this is a good

234
00:10:04,950 --> 00:10:10,459
theoretical result but in order to see

235
00:10:07,980 --> 00:10:12,839
how these works in practice

236
00:10:10,459 --> 00:10:15,390
how well this works in practice we

237
00:10:12,839 --> 00:10:18,240
implement the algorithm thing at all

238
00:10:15,390 --> 00:10:19,709
that it's public and this tool can be

239
00:10:18,240 --> 00:10:21,390
run well we can run it on different

240
00:10:19,709 --> 00:10:24,569
machines with different cache set

241
00:10:21,390 --> 00:10:26,310
settings and we can use it to model

242
00:10:24,570 --> 00:10:28,709
adversaries with or attackers with

243
00:10:26,310 --> 00:10:30,899
different capabilities for instance here

244
00:10:28,709 --> 00:10:34,380
we have three three figures the first

245
00:10:30,899 --> 00:10:36,029
one represent an attacker that is that

246
00:10:34,380 --> 00:10:39,510
has complete information about the set

247
00:10:36,029 --> 00:10:43,079
index width so it only need to pick a

248
00:10:39,510 --> 00:10:46,140
few addresses to find a larger fiction

249
00:10:43,079 --> 00:10:47,819
set and then reduce it the second figure

250
00:10:46,140 --> 00:10:49,649
represents an attack with an attacker

251
00:10:47,820 --> 00:10:52,529
with partial information about the set

252
00:10:49,649 --> 00:10:54,300
index width and this last one is an

253
00:10:52,529 --> 00:10:58,320
attacker without any bit of information

254
00:10:54,300 --> 00:11:00,899
about the set index bits so this is

255
00:10:58,320 --> 00:11:03,660
average real execution time of the

256
00:11:00,899 --> 00:11:07,110
reduction and we can see that in the

257
00:11:03,660 --> 00:11:08,120
first case since the initial set doesn't

258
00:11:07,110 --> 00:11:11,360
need don't need

259
00:11:08,120 --> 00:11:14,410
be too large really reducing a set of

260
00:11:11,360 --> 00:11:18,200
200 or 300 addresses the the performance

261
00:11:14,410 --> 00:11:20,170
benefit it's quite modest but with

262
00:11:18,200 --> 00:11:23,000
partial information we already see that

263
00:11:20,170 --> 00:11:25,160
the sets that we need to reduce our

264
00:11:23,000 --> 00:11:27,770
2,000 or 3,000 other decision in this

265
00:11:25,160 --> 00:11:31,370
case we start seeing a performance

266
00:11:27,770 --> 00:11:35,180
improvement of almost three orders of

267
00:11:31,370 --> 00:11:36,770
magnitude but the most interest is most

268
00:11:35,180 --> 00:11:38,540
interesting case is the last one where

269
00:11:36,770 --> 00:11:41,150
the attacker has no information at all

270
00:11:38,540 --> 00:11:44,360
about the set index beads in this case

271
00:11:41,150 --> 00:11:46,550
you can see that the sizes of the sense

272
00:11:44,360 --> 00:11:49,940
that we need to realize reduce are

273
00:11:46,550 --> 00:11:51,410
thirty or forty thousand at least and in

274
00:11:49,940 --> 00:11:53,360
this case is the baseline our if

275
00:11:51,410 --> 00:11:55,370
completely becomes completely

276
00:11:53,360 --> 00:11:58,220
impractical because for finding a single

277
00:11:55,370 --> 00:12:02,510
original said we need to spend several

278
00:11:58,220 --> 00:12:06,890
minutes while using group testing we can

279
00:12:02,510 --> 00:12:10,010
do it in less than two seconds and this

280
00:12:06,890 --> 00:12:11,660
is a very good result because some

281
00:12:10,010 --> 00:12:13,130
confirm that there were some proposed

282
00:12:11,660 --> 00:12:15,560
control measures that we're trying to

283
00:12:13,130 --> 00:12:18,170
hide this mapping from addresses into

284
00:12:15,560 --> 00:12:20,029
into castles so with our group testing

285
00:12:18,170 --> 00:12:22,400
we can see that even if this mapping is

286
00:12:20,029 --> 00:12:24,500
completely unknown to the attacker it's

287
00:12:22,400 --> 00:12:29,240
still possible to find minimal of each

288
00:12:24,500 --> 00:12:31,459
subsets okay so before is this is about

289
00:12:29,240 --> 00:12:33,080
performance but before I said that we

290
00:12:31,459 --> 00:12:38,569
did some assumptions on the replacement

291
00:12:33,080 --> 00:12:41,060
policy for our test to work and it turns

292
00:12:38,570 --> 00:12:43,640
out that in modern replacement well some

293
00:12:41,060 --> 00:12:45,439
modern replacement policies break our as

294
00:12:43,640 --> 00:12:46,880
our assumptions on the test and it

295
00:12:45,440 --> 00:12:48,790
didn't reduce errors in our our

296
00:12:46,880 --> 00:12:52,310
algorithm and this is a nice example

297
00:12:48,790 --> 00:12:55,069
extract from a scale image machine where

298
00:12:52,310 --> 00:12:58,369
we can see on on the horizontal axis the

299
00:12:55,070 --> 00:13:02,450
different customs on this machine and

300
00:12:58,370 --> 00:13:05,839
the green bars are the the reduction

301
00:13:02,450 --> 00:13:08,570
rate without error so how many times we

302
00:13:05,839 --> 00:13:11,120
were able to reduce an a large set

303
00:13:08,570 --> 00:13:12,800
without having any error and we can see

304
00:13:11,120 --> 00:13:15,770
that there are a few castles that really

305
00:13:12,800 --> 00:13:19,550
behaved our our as we assumed under

306
00:13:15,770 --> 00:13:21,750
error rate there is very low but in the

307
00:13:19,550 --> 00:13:24,060
majority of discusses

308
00:13:21,750 --> 00:13:25,470
the replacement policy is a slightly

309
00:13:24,060 --> 00:13:27,900
different and this introduced a lot of

310
00:13:25,470 --> 00:13:30,110
errors the good thing is that in

311
00:13:27,900 --> 00:13:34,050
practice we can use some heuristics like

312
00:13:30,110 --> 00:13:36,450
based on repetition and backtracking and

313
00:13:34,050 --> 00:13:43,469
with that we are able to successfully

314
00:13:36,450 --> 00:13:46,310
reduce the direction set for any cascade

315
00:13:43,470 --> 00:13:49,770
with only a small performance overhead

316
00:13:46,310 --> 00:13:52,219
and in order to convince you that this

317
00:13:49,770 --> 00:13:55,890
really works well in practice I

318
00:13:52,220 --> 00:14:04,950
implemented our algorithms on web

319
00:13:55,890 --> 00:14:10,740
browser can you sorry here we will be

320
00:14:04,950 --> 00:14:12,390
running a webpage which implement about

321
00:14:10,740 --> 00:14:18,290
reduction algorithm in JavaScript and

322
00:14:12,390 --> 00:14:21,210
here we will see that when you start

323
00:14:18,290 --> 00:14:25,380
what we are doing is that our JavaScript

324
00:14:21,210 --> 00:14:28,530
found some service array indices that

325
00:14:25,380 --> 00:14:31,470
correspond to an eviction set and here

326
00:14:28,530 --> 00:14:33,900
what we are doing is we are attached to

327
00:14:31,470 --> 00:14:35,730
this browser process and we translate

328
00:14:33,900 --> 00:14:37,110
these indices into a built well

329
00:14:35,730 --> 00:14:40,310
addresses and then into physical

330
00:14:37,110 --> 00:14:42,900
addresses to check that effectively the

331
00:14:40,310 --> 00:14:44,729
indices correspond to colliding

332
00:14:42,900 --> 00:14:46,500
addresses all these javascript

333
00:14:44,730 --> 00:14:49,950
javascript array indices will be map

334
00:14:46,500 --> 00:14:55,520
into the same car set and we can see

335
00:14:49,950 --> 00:14:59,910
that if we try to find all eviction sets

336
00:14:55,520 --> 00:15:01,740
this the performance is is really good

337
00:14:59,910 --> 00:15:07,199
and the reliability as you can see here

338
00:15:01,740 --> 00:15:09,300
is also very good in practice so here in

339
00:15:07,200 --> 00:15:12,270
this few seconds we found 73 different

340
00:15:09,300 --> 00:15:14,790
emission sets and for each of these 73

341
00:15:12,270 --> 00:15:18,900
we can automatically compute 64 more so

342
00:15:14,790 --> 00:15:24,719
we found 73 times 64 different minimal

343
00:15:18,900 --> 00:15:28,230
original sets so just some four

344
00:15:24,720 --> 00:15:29,450
conclusions of final remarks and the

345
00:15:28,230 --> 00:15:31,560
most important one I think is that

346
00:15:29,450 --> 00:15:33,780
finding minimal eviction sets is a

347
00:15:31,560 --> 00:15:35,099
threshold group testing problem and this

348
00:15:33,780 --> 00:15:38,250
provides a

349
00:15:35,100 --> 00:15:40,140
important insight for when we try to

350
00:15:38,250 --> 00:15:46,080
think about principle countermeasures

351
00:15:40,140 --> 00:15:47,880
and also there is another there are some

352
00:15:46,080 --> 00:15:50,280
variations of threshold testing that can

353
00:15:47,880 --> 00:15:53,970
be useful to improve even this algorithm

354
00:15:50,280 --> 00:15:56,579
even more for instance we need some

355
00:15:53,970 --> 00:15:58,350
assumptions that introduce errors and we

356
00:15:56,580 --> 00:16:00,330
have this performance overhead to to our

357
00:15:58,350 --> 00:16:02,460
heuristic but some variations of

358
00:16:00,330 --> 00:16:07,170
threshold group testing work with noisy

359
00:16:02,460 --> 00:16:10,560
tests so we think that using this noise

360
00:16:07,170 --> 00:16:13,469
notion of noisy group testing we can

361
00:16:10,560 --> 00:16:16,739
reduce the overheads of these heuristics

362
00:16:13,470 --> 00:16:19,200
and finally well we show a novel linear

363
00:16:16,740 --> 00:16:21,210
algorithm that makes attacks much faster

364
00:16:19,200 --> 00:16:23,460
in practice and in some scenarios as

365
00:16:21,210 --> 00:16:27,720
when the attacker has no information

366
00:16:23,460 --> 00:16:31,350
about the set index beats the attacker

367
00:16:27,720 --> 00:16:33,240
becomes really practical and that's it

368
00:16:31,350 --> 00:16:43,200
thank you for your attention and I'm

369
00:16:33,240 --> 00:16:46,110
happy to take questions hi my name is

370
00:16:43,200 --> 00:16:48,900
guru Raj from Georgia Tech excellent or

371
00:16:46,110 --> 00:16:52,170
clouded so I'm from the computer

372
00:16:48,900 --> 00:16:53,819
architecture community and in in in that

373
00:16:52,170 --> 00:16:56,819
community there's been some interest in

374
00:16:53,820 --> 00:17:00,420
trying to make this task of discovering

375
00:16:56,820 --> 00:17:03,300
eviction sets harder so my question was

376
00:17:00,420 --> 00:17:05,700
if people try to sort of try and

377
00:17:03,300 --> 00:17:07,950
randomize the cash index indexing

378
00:17:05,700 --> 00:17:09,900
function or come up with ways to

379
00:17:07,950 --> 00:17:13,920
introduce non determinism in the

380
00:17:09,900 --> 00:17:15,570
indexing itself how so that will make

381
00:17:13,920 --> 00:17:18,930
the task of discovering evictions it's

382
00:17:15,569 --> 00:17:22,129
harder but my question is how would you

383
00:17:18,930 --> 00:17:25,860
sort of try and adapt your discovering

384
00:17:22,130 --> 00:17:29,160
algorithm to adapt to the changes in the

385
00:17:25,859 --> 00:17:31,350
indexing function itself well if the

386
00:17:29,160 --> 00:17:33,150
changes are dying well these changes

387
00:17:31,350 --> 00:17:34,770
even if you completely randomized the

388
00:17:33,150 --> 00:17:36,870
mapping from another's to a cache set

389
00:17:34,770 --> 00:17:39,090
these mapping needs to be a static in

390
00:17:36,870 --> 00:17:43,370
order to get some benefit from from the

391
00:17:39,090 --> 00:17:46,290
caching mechanisms and if you do that

392
00:17:43,370 --> 00:17:47,850
what the attacker can the only

393
00:17:46,290 --> 00:17:48,809
information that the attacker has is

394
00:17:47,850 --> 00:17:52,320
that there

395
00:17:48,809 --> 00:17:54,360
no he cos we do zero zero bits of

396
00:17:52,320 --> 00:17:57,899
information from this mapping so this is

397
00:17:54,360 --> 00:18:00,149
actually the last of our case it's

398
00:17:57,900 --> 00:18:01,559
harder to find this large addiction sin

399
00:18:00,150 --> 00:18:03,570
because you are picking a dress is

400
00:18:01,559 --> 00:18:08,610
completely at random and you will need

401
00:18:03,570 --> 00:18:10,590
30 or 40,000 of these services but even

402
00:18:08,610 --> 00:18:15,899
in that case we are able to reduce it

403
00:18:10,590 --> 00:18:19,230
into unless that I think two seconds so

404
00:18:15,900 --> 00:18:22,200
yeah even if you completely randomized

405
00:18:19,230 --> 00:18:24,090
this mapping it's possible I get it this

406
00:18:22,200 --> 00:18:28,500
is the kingside it's still possible to

407
00:18:24,090 --> 00:18:30,389
find the minimal victim says I guess

408
00:18:28,500 --> 00:18:33,120
I'll follow up on that question so okay

409
00:18:30,390 --> 00:18:35,730
we I get it that if I do it one time

410
00:18:33,120 --> 00:18:38,340
remapping ran let's say truly random

411
00:18:35,730 --> 00:18:40,650
remapping now if I add more dynamism

412
00:18:38,340 --> 00:18:43,889
dynam whatever I can't say but if I make

413
00:18:40,650 --> 00:18:46,049
it more dynamic and at runtime and I do

414
00:18:43,890 --> 00:18:48,210
it every five seconds ten seconds this

415
00:18:46,049 --> 00:18:51,990
pay some performance penalty how would

416
00:18:48,210 --> 00:18:54,240
that affect your approach I think that

417
00:18:51,990 --> 00:18:56,370
the problem is that if you do it at

418
00:18:54,240 --> 00:18:59,070
runtime then when you want to access

419
00:18:56,370 --> 00:19:01,139
some just as a legitimate not as an

420
00:18:59,070 --> 00:19:03,090
attacker you want to access your data

421
00:19:01,140 --> 00:19:05,730
you will not find it in cash because you

422
00:19:03,090 --> 00:19:07,799
are being map it somewhere else so this

423
00:19:05,730 --> 00:19:09,210
would introduce a performance into real

424
00:19:07,799 --> 00:19:14,280
problem so I don't think that this can

425
00:19:09,210 --> 00:19:17,010
be really a mechanism you can implement

426
00:19:14,280 --> 00:19:19,080
in practice what we would you could do

427
00:19:17,010 --> 00:19:21,929
probably is maybe in some scenarios you

428
00:19:19,080 --> 00:19:26,039
can find a trade-off between how might

429
00:19:21,929 --> 00:19:28,140
you effect like good programs and how

430
00:19:26,039 --> 00:19:29,640
much harder do you make to find the

431
00:19:28,140 --> 00:19:32,100
victim sense the problem would be is if

432
00:19:29,640 --> 00:19:34,289
there is completely dynamic mechanisms

433
00:19:32,100 --> 00:19:36,330
that change the mapping to eviction sets

434
00:19:34,289 --> 00:19:39,960
the attacker will need to just recompute

435
00:19:36,330 --> 00:19:40,260
them several times if you can do it fast

436
00:19:39,960 --> 00:19:44,130
enough

437
00:19:40,260 --> 00:19:45,750
it's probably fine but yeah I guess that

438
00:19:44,130 --> 00:19:47,400
this is like an open question what is

439
00:19:45,750 --> 00:19:48,960
the trade-off between making things

440
00:19:47,400 --> 00:19:50,400
harder and affecting the performance of

441
00:19:48,960 --> 00:19:52,320
good problems right so if you had

442
00:19:50,400 --> 00:19:53,820
environment like SGX what you're saying

443
00:19:52,320 --> 00:19:55,770
I care about the security of this yeah

444
00:19:53,820 --> 00:19:58,860
it already has a expensive context which

445
00:19:55,770 --> 00:20:00,570
maybe you could tolerate that price to

446
00:19:58,860 --> 00:20:02,520
kind of make sure that that security

447
00:20:00,570 --> 00:20:04,559
critical code a lot of interesting work

448
00:20:02,520 --> 00:20:07,850
interesting follow-on work let's thank

449
00:20:04,559 --> 00:20:07,850
our speaker one more time thank

