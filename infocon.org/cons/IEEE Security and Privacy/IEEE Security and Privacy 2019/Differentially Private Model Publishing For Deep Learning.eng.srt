1
00:00:08,620 --> 00:00:15,018
thank you I'm caught on

2
00:00:12,470 --> 00:00:18,289
this worked was part of a thesis work of

3
00:00:15,019 --> 00:00:21,770
lay the first author he graduated and

4
00:00:18,289 --> 00:00:26,529
joined IBM so instead of coming here to

5
00:00:21,770 --> 00:00:28,519
give a job talk he decided to let us

6
00:00:26,529 --> 00:00:32,750
advertise on his behalf

7
00:00:28,519 --> 00:00:35,239
so he's jointly advised by Professor

8
00:00:32,750 --> 00:00:38,269
Lindt Leo and myself and we still have

9
00:00:35,239 --> 00:00:41,620
two good students so if you're

10
00:00:38,269 --> 00:00:47,300
recruiting Emory and Stacy a very good

11
00:00:41,620 --> 00:00:50,410
student sir so the paper is about deep

12
00:00:47,300 --> 00:00:54,649
learning with differential privacy and

13
00:00:50,410 --> 00:00:59,779
as you have seen from the previous

14
00:00:54,649 --> 00:01:04,039
speakers this is a topic that has been a

15
00:00:59,780 --> 00:01:07,070
big challenge and we have made some

16
00:01:04,039 --> 00:01:10,720
progress so we do not certainly claim we

17
00:01:07,070 --> 00:01:13,960
had a final answer by far but we do have

18
00:01:10,720 --> 00:01:17,709
interesting results in terms of privacy

19
00:01:13,960 --> 00:01:20,330
budget allocation and also privacy cost

20
00:01:17,709 --> 00:01:25,160
estimation as well as calculation with a

21
00:01:20,330 --> 00:01:28,910
valuation so deep learning of course is

22
00:01:25,160 --> 00:01:32,598
kind of a considered widely to be the

23
00:01:28,910 --> 00:01:36,560
next big hope in terms of smart

24
00:01:32,599 --> 00:01:38,690
applications and I thank the first

25
00:01:36,560 --> 00:01:41,149
speaker ohm for giving a very nice

26
00:01:38,690 --> 00:01:43,940
introduction of a price differential

27
00:01:41,149 --> 00:01:47,300
privacy so we don't have to repeat that

28
00:01:43,940 --> 00:01:51,110
but of course you are probably familiar

29
00:01:47,300 --> 00:01:52,819
with deep learning as well so you

30
00:01:51,110 --> 00:01:57,910
probably know that deep learning can

31
00:01:52,819 --> 00:02:00,739
achieve higher accuracy and performance

32
00:01:57,910 --> 00:02:02,750
compared to normal machine learning

33
00:02:00,739 --> 00:02:06,340
algorithms that have been presented

34
00:02:02,750 --> 00:02:09,709
earlier here but deep learning has

35
00:02:06,340 --> 00:02:13,940
dependence very large amount of training

36
00:02:09,709 --> 00:02:17,480
data so it achieves higher performance

37
00:02:13,940 --> 00:02:21,650
because it has better input so to speak

38
00:02:17,480 --> 00:02:23,329
and it has also a better capture of the

39
00:02:21,650 --> 00:02:29,090
information in the

40
00:02:23,330 --> 00:02:31,130
so that causes problems and but there

41
00:02:29,090 --> 00:02:33,800
are certain applications that we really

42
00:02:31,130 --> 00:02:35,870
need this higher performance so for

43
00:02:33,800 --> 00:02:38,840
healthcare applications cancer was

44
00:02:35,870 --> 00:02:41,270
mentioned earlier we also use cancer

45
00:02:38,840 --> 00:02:43,220
data set you can imagine that false

46
00:02:41,270 --> 00:02:46,910
positives are not going to be a very

47
00:02:43,220 --> 00:02:49,100
good problem and very good answer so we

48
00:02:46,910 --> 00:02:52,250
need a high performance and deep

49
00:02:49,100 --> 00:02:56,780
learning provides that hope and we have

50
00:02:52,250 --> 00:02:59,570
done now privacy leakage problems so I

51
00:02:56,780 --> 00:03:02,060
think I mentioned membership inference

52
00:02:59,570 --> 00:03:05,060
attacks and also there's a model

53
00:03:02,060 --> 00:03:08,300
aversion attack and then there are other

54
00:03:05,060 --> 00:03:10,610
ways actually imaginative ways of

55
00:03:08,300 --> 00:03:14,360
misleading both of the models as well as

56
00:03:10,610 --> 00:03:17,720
the results so we are going to talk a

57
00:03:14,360 --> 00:03:19,940
little bit about this and so what

58
00:03:17,720 --> 00:03:22,310
happens is we have a training data set

59
00:03:19,940 --> 00:03:25,400
of course we cannot reveal that as part

60
00:03:22,310 --> 00:03:27,470
of the privacy protection and then the

61
00:03:25,400 --> 00:03:30,080
hope is that with deep learning we have

62
00:03:27,470 --> 00:03:33,859
very sophisticated neural networks and

63
00:03:30,080 --> 00:03:36,739
we go through a lot of training and we

64
00:03:33,860 --> 00:03:39,950
added a static stochastic gradient

65
00:03:36,739 --> 00:03:42,350
descent curve here you will see this

66
00:03:39,950 --> 00:03:44,510
curve a little bit later but the idea is

67
00:03:42,350 --> 00:03:47,540
that through optimization then we can

68
00:03:44,510 --> 00:03:50,420
minimize the error and then we have a

69
00:03:47,540 --> 00:03:55,540
model that we could potentially publish

70
00:03:50,420 --> 00:03:58,220
and hopefully not reveal the actual

71
00:03:55,540 --> 00:04:00,980
delicate parts that the privacy that

72
00:03:58,220 --> 00:04:03,230
we're trying to preserve so the hope is

73
00:04:00,980 --> 00:04:07,910
that the model then can be shared and

74
00:04:03,230 --> 00:04:10,280
then publisher published so the problem

75
00:04:07,910 --> 00:04:12,890
is that deep learning captures the

76
00:04:10,280 --> 00:04:15,110
information from the input and by doing

77
00:04:12,890 --> 00:04:17,120
that it actually the models actually

78
00:04:15,110 --> 00:04:19,549
contain a lot of information about the

79
00:04:17,120 --> 00:04:22,370
input so intuitively what's happening

80
00:04:19,548 --> 00:04:25,250
here is that we have a lot of iterations

81
00:04:22,370 --> 00:04:30,050
we have a lot of optimization and the

82
00:04:25,250 --> 00:04:33,650
models actually reflect very interesting

83
00:04:30,050 --> 00:04:36,169
details about the input and that's part

84
00:04:33,650 --> 00:04:38,060
a reason why the membership attacks and

85
00:04:36,169 --> 00:04:40,219
the modeling aversion attacks are

86
00:04:38,060 --> 00:04:41,930
actually quite powerful in this case

87
00:04:40,219 --> 00:04:46,430
because the information is actually

88
00:04:41,930 --> 00:04:49,279
captured in the models so so now the

89
00:04:46,430 --> 00:04:52,699
problem is we have a deep learning and

90
00:04:49,279 --> 00:04:56,029
we would like to use it but it is in a

91
00:04:52,699 --> 00:04:57,379
way working again in itself from privacy

92
00:04:56,029 --> 00:04:59,150
point of view because the more

93
00:04:57,379 --> 00:05:02,509
information that you capture in the

94
00:04:59,150 --> 00:05:04,580
models the more risk that you have in

95
00:05:02,509 --> 00:05:08,810
revealing those when you publish the

96
00:05:04,580 --> 00:05:12,198
models so what we have here is that we

97
00:05:08,810 --> 00:05:16,219
have a model publishing and we would

98
00:05:12,199 --> 00:05:18,439
like to leverage some of the work in

99
00:05:16,219 --> 00:05:21,379
terms of for the budget allocation the

100
00:05:18,439 --> 00:05:24,499
privacy budget allocation and also the

101
00:05:21,379 --> 00:05:27,860
cost is the privacy cost estimation so

102
00:05:24,499 --> 00:05:30,259
maybe we can fine-tune those two and

103
00:05:27,860 --> 00:05:32,629
then achieve a better balance in terms

104
00:05:30,259 --> 00:05:37,520
of how to preserve differential privacy

105
00:05:32,629 --> 00:05:40,310
here so I will just flash this slide

106
00:05:37,520 --> 00:05:43,099
thank home for having explained the

107
00:05:40,310 --> 00:05:47,270
differential privacy and I hope that you

108
00:05:43,099 --> 00:05:48,319
have come in time to have caught his

109
00:05:47,270 --> 00:05:51,020
cosplay nation

110
00:05:48,319 --> 00:05:51,620
so for those of you who came a little

111
00:05:51,020 --> 00:05:56,000
bit later

112
00:05:51,620 --> 00:05:59,930
I apologize but there is this definition

113
00:05:56,000 --> 00:06:02,659
and the idea of differential privacy at

114
00:05:59,930 --> 00:06:06,800
a very high level is this trade-off

115
00:06:02,659 --> 00:06:11,270
between utility and privacy so you try

116
00:06:06,800 --> 00:06:14,029
to balance the best of a preserving

117
00:06:11,270 --> 00:06:16,639
utility of the data while perturbing

118
00:06:14,029 --> 00:06:18,649
just enough so you can prison you can

119
00:06:16,639 --> 00:06:23,569
also preserve privacy at the same time

120
00:06:18,649 --> 00:06:25,460
so one of the things that omein may or

121
00:06:23,569 --> 00:06:28,279
may not have mentioned explicitly this

122
00:06:25,460 --> 00:06:31,460
composition theorem that there is a very

123
00:06:28,279 --> 00:06:34,460
nice property in differential privacy

124
00:06:31,460 --> 00:06:37,339
that you can actually combine different

125
00:06:34,460 --> 00:06:40,430
algorithms and if the algorithms are

126
00:06:37,339 --> 00:06:42,830
combined carefully in this case each one

127
00:06:40,430 --> 00:06:45,199
of them a securities equation way over

128
00:06:42,830 --> 00:06:47,639
the entire input data then you can

129
00:06:45,199 --> 00:06:50,370
actually add up divide

130
00:06:47,639 --> 00:06:53,189
the privacy budget allocation there and

131
00:06:50,370 --> 00:06:56,189
I mean our case we do the other way we

132
00:06:53,189 --> 00:06:59,729
actually use this fact and then

133
00:06:56,189 --> 00:07:02,099
decompose the privates budget into the

134
00:06:59,729 --> 00:07:05,188
different parts of the algorithm that

135
00:07:02,099 --> 00:07:07,710
we're trying to run here so the epsilon

136
00:07:05,189 --> 00:07:13,020
in this case can be decomposed into

137
00:07:07,710 --> 00:07:15,599
different smaller budgets and then in

138
00:07:13,020 --> 00:07:18,330
case of multi-step machine learning what

139
00:07:15,599 --> 00:07:21,870
you use the case of deep learning

140
00:07:18,330 --> 00:07:25,909
we actually decompose this parvis budget

141
00:07:21,870 --> 00:07:29,120
allocate the sub budget for each step so

142
00:07:25,909 --> 00:07:32,219
they are different ways of allocating

143
00:07:29,120 --> 00:07:34,770
this budget and we're going to show that

144
00:07:32,219 --> 00:07:36,629
actually depending on the way you

145
00:07:34,770 --> 00:07:39,539
allocate the budget you get different

146
00:07:36,629 --> 00:07:42,539
results so even though maybe

147
00:07:39,539 --> 00:07:45,599
theoretically they look very similar in

148
00:07:42,539 --> 00:07:48,900
practice there are differences in how

149
00:07:45,599 --> 00:07:51,839
you do this and particularly we are

150
00:07:48,900 --> 00:07:53,940
going to show you some examples where

151
00:07:51,839 --> 00:07:57,150
this makes big difference and other

152
00:07:53,940 --> 00:08:00,180
times when they may or may not make that

153
00:07:57,150 --> 00:08:03,989
big a difference so stochastic gradient

154
00:08:00,180 --> 00:08:09,539
descent has this iteration so we have

155
00:08:03,990 --> 00:08:13,289
two parts of the iteration but just at a

156
00:08:09,539 --> 00:08:15,449
very high level we go through the entire

157
00:08:13,289 --> 00:08:18,599
data set in what we call the epics

158
00:08:15,449 --> 00:08:22,710
and then each data set can be decomposed

159
00:08:18,599 --> 00:08:25,169
into smaller mini batches and then we

160
00:08:22,710 --> 00:08:28,650
can have iterations over those mini

161
00:08:25,169 --> 00:08:31,289
batches within each epoch and the idea

162
00:08:28,650 --> 00:08:34,169
is that you can for example is acute the

163
00:08:31,289 --> 00:08:37,769
mini batches in parallel over GPUs speed

164
00:08:34,169 --> 00:08:40,260
up the process and the idea is that if

165
00:08:37,769 --> 00:08:43,409
we are careful in how we divide things

166
00:08:40,260 --> 00:08:46,439
and how we divide the privacy budget

167
00:08:43,409 --> 00:08:51,089
then we can actually achieve a better

168
00:08:46,440 --> 00:08:53,670
results so summarizing the technical

169
00:08:51,089 --> 00:08:55,589
challenge here there are two parts so

170
00:08:53,670 --> 00:08:58,110
the first part is a privacy budget

171
00:08:55,589 --> 00:09:01,180
allocation and the second part is the

172
00:08:58,110 --> 00:09:03,400
private cost accounting so

173
00:09:01,180 --> 00:09:08,859
the budget allocation in this case will

174
00:09:03,400 --> 00:09:11,260
be the way we divide so um divided into

175
00:09:08,860 --> 00:09:12,970
two parts and in this case we have deep

176
00:09:11,260 --> 00:09:15,700
learning we have many different steps so

177
00:09:12,970 --> 00:09:19,000
we divide actually the budget over those

178
00:09:15,700 --> 00:09:21,490
steps and the cost accounting in this

179
00:09:19,000 --> 00:09:25,540
case also is important because we need

180
00:09:21,490 --> 00:09:28,360
to make sure that as we add up each step

181
00:09:25,540 --> 00:09:30,550
we do not exceed the budget and so

182
00:09:28,360 --> 00:09:33,280
random sampling and random shuffling are

183
00:09:30,550 --> 00:09:36,219
the two popular methods for doing this

184
00:09:33,280 --> 00:09:38,589
and even though mathematically they are

185
00:09:36,220 --> 00:09:43,210
very similar have similar statistical

186
00:09:38,590 --> 00:09:45,460
properties for the case of privacy

187
00:09:43,210 --> 00:09:50,140
preservation actually they have some

188
00:09:45,460 --> 00:09:52,660
differences in this case so finally we

189
00:09:50,140 --> 00:09:55,600
are at the point we can summarize the

190
00:09:52,660 --> 00:09:59,260
contribution the paper so there are two

191
00:09:55,600 --> 00:10:02,520
parts of this one is the budget

192
00:09:59,260 --> 00:10:05,110
allocation and the other one is the cost

193
00:10:02,520 --> 00:10:08,020
analysis that the privacy cost analysis

194
00:10:05,110 --> 00:10:10,900
in terms of random sampling versus

195
00:10:08,020 --> 00:10:12,990
random shuffling and then there are

196
00:10:10,900 --> 00:10:15,610
different data batching mini-batch

197
00:10:12,990 --> 00:10:20,980
allocations that actually also make a

198
00:10:15,610 --> 00:10:23,530
difference so in terms of mini-batch let

199
00:10:20,980 --> 00:10:27,040
me just I see that I'm actually running

200
00:10:23,530 --> 00:10:29,199
short on time so I would just be very

201
00:10:27,040 --> 00:10:31,270
brief about this so random sampling in

202
00:10:29,200 --> 00:10:33,880
this case has replacement as you can

203
00:10:31,270 --> 00:10:38,680
imagine and random shuffling in this

204
00:10:33,880 --> 00:10:41,290
case allocates the data set into smaller

205
00:10:38,680 --> 00:10:43,989
parts smaller subsets without

206
00:10:41,290 --> 00:10:47,110
replacement and turns out that a random

207
00:10:43,990 --> 00:10:50,110
shuffling has good properties in terms

208
00:10:47,110 --> 00:10:52,660
of how we divide into parallel is

209
00:10:50,110 --> 00:10:57,670
accusing unit so there are more

210
00:10:52,660 --> 00:10:59,890
efficiently institution and in software

211
00:10:57,670 --> 00:11:03,219
packages of tensorflow pi torch that's

212
00:10:59,890 --> 00:11:06,490
quite often the default way to do this

213
00:11:03,220 --> 00:11:09,550
so for data mini batching turns out that

214
00:11:06,490 --> 00:11:12,190
random sampling actually has better

215
00:11:09,550 --> 00:11:14,949
properties so random shuffling is more

216
00:11:12,190 --> 00:11:18,360
efficient execution but because

217
00:11:14,950 --> 00:11:22,540
it is known that we are to choose each

218
00:11:18,360 --> 00:11:25,480
each point exactly once so you can see

219
00:11:22,540 --> 00:11:29,349
that random shuffling actually has

220
00:11:25,480 --> 00:11:33,550
higher cost and it requires a higher

221
00:11:29,350 --> 00:11:37,180
budget in terms of privacy preservation

222
00:11:33,550 --> 00:11:40,329
so those are the curve that we show that

223
00:11:37,180 --> 00:11:42,729
in the paper that for random shuffling

224
00:11:40,330 --> 00:11:46,930
you actually need to be more careful

225
00:11:42,730 --> 00:11:49,660
so for random shuffling turns out that

226
00:11:46,930 --> 00:11:51,719
there is a concentrated the differential

227
00:11:49,660 --> 00:11:54,370
privacy method called a zero

228
00:11:51,720 --> 00:12:02,470
concentrated differential privacy that

229
00:11:54,370 --> 00:12:05,050
we can use to calculate the the budget

230
00:12:02,470 --> 00:12:08,670
as well as the cost so in this

231
00:12:05,050 --> 00:12:12,250
particular case I have to go quickly but

232
00:12:08,670 --> 00:12:15,490
turns out that we found that because of

233
00:12:12,250 --> 00:12:18,130
the shuffling actually uses each data

234
00:12:15,490 --> 00:12:22,150
point exactly once there is a good thing

235
00:12:18,130 --> 00:12:25,450
about this in terms of i0 concentrated

236
00:12:22,150 --> 00:12:27,850
differential privacy so actually we can

237
00:12:25,450 --> 00:12:32,370
show that it is actually preserving

238
00:12:27,850 --> 00:12:35,290
better a tighter band in this case and

239
00:12:32,370 --> 00:12:38,620
for the random sampling there is also

240
00:12:35,290 --> 00:12:44,290
interesting observation that it actually

241
00:12:38,620 --> 00:12:48,640
preserves the privacy budget in a way at

242
00:12:44,290 --> 00:12:53,410
least for initial initial segment in

243
00:12:48,640 --> 00:12:58,660
terms of the DDD alpha parameter here

244
00:12:53,410 --> 00:13:01,630
and this is not previously clearly

245
00:12:58,660 --> 00:13:04,839
identified so this is one explanation of

246
00:13:01,630 --> 00:13:07,510
why random sampling can achieve tighter

247
00:13:04,840 --> 00:13:11,080
of privacy preservation compared to the

248
00:13:07,510 --> 00:13:14,350
shuffling so one thing that I want to

249
00:13:11,080 --> 00:13:17,650
briefly mention here is that we actually

250
00:13:14,350 --> 00:13:21,790
looked into the budget allocation issue

251
00:13:17,650 --> 00:13:24,310
and we found that smarter budget

252
00:13:21,790 --> 00:13:28,769
allocation actually can improve the

253
00:13:24,310 --> 00:13:32,638
algorithm in this case and you will see

254
00:13:28,769 --> 00:13:35,819
the graph here kind of resemble the SGD

255
00:13:32,639 --> 00:13:38,970
graphs so we basically said that in

256
00:13:35,819 --> 00:13:42,389
stochastic graduate descent what you

257
00:13:38,970 --> 00:13:45,480
have is a higher learning rate at the

258
00:13:42,389 --> 00:13:48,299
beginning and a slower learning rate a

259
00:13:45,480 --> 00:13:51,209
little later so what we do is to

260
00:13:48,299 --> 00:13:55,410
allocate more privacy budget at the

261
00:13:51,209 --> 00:13:59,099
beginning and lower smaller privates

262
00:13:55,410 --> 00:14:03,689
budget later on as the learning rate

263
00:13:59,100 --> 00:14:08,660
slows down so we tried a different four

264
00:14:03,689 --> 00:14:11,849
different alternative in this case and

265
00:14:08,660 --> 00:14:13,829
and in this particular case we show that

266
00:14:11,850 --> 00:14:16,589
also that the validation data set can be

267
00:14:13,829 --> 00:14:20,819
the public part of the a validation to

268
00:14:16,589 --> 00:14:27,540
save on the privacy budget and so we

269
00:14:20,819 --> 00:14:31,649
have used this to evaluate the four

270
00:14:27,540 --> 00:14:35,629
different decaying functions in terms of

271
00:14:31,649 --> 00:14:40,589
how we reduce the privates budget

272
00:14:35,629 --> 00:14:44,639
allocation of our time and this we show

273
00:14:40,589 --> 00:14:47,339
that actually they do better compared to

274
00:14:44,639 --> 00:14:50,819
the constant allocation if you divide

275
00:14:47,339 --> 00:14:54,569
everything the same then you will have a

276
00:14:50,819 --> 00:14:57,868
lower performance accuracy for example

277
00:14:54,569 --> 00:15:03,389
compared to all of the the variable

278
00:14:57,869 --> 00:15:06,689
dynamic allocation algorithms there now

279
00:15:03,389 --> 00:15:09,179
the difference between the variations of

280
00:15:06,689 --> 00:15:13,439
a dynamic allocation is not as big as we

281
00:15:09,179 --> 00:15:16,410
thought we thought that having different

282
00:15:13,439 --> 00:15:18,629
ways of allocating it would make a

283
00:15:16,410 --> 00:15:20,368
bigger difference but the difference was

284
00:15:18,629 --> 00:15:23,040
less than we thought

285
00:15:20,369 --> 00:15:26,809
so this is definitely a question in

286
00:15:23,040 --> 00:15:30,118
terms of allocating smartly the budget

287
00:15:26,809 --> 00:15:33,259
the the privates budget is definitely a

288
00:15:30,119 --> 00:15:36,569
question that is in our mind more

289
00:15:33,259 --> 00:15:39,200
intricate a question is not as easy as

290
00:15:36,569 --> 00:15:44,660
obvious as we thought originally

291
00:15:39,200 --> 00:15:48,889
so in this particular case we show that

292
00:15:44,660 --> 00:15:55,670
you have random shuffling having higher

293
00:15:48,889 --> 00:15:59,170
budget and then the the random sampling

294
00:15:55,670 --> 00:16:02,060
as well as different ways of covalent of

295
00:15:59,170 --> 00:16:06,019
two random sampling in terms of how to

296
00:16:02,060 --> 00:16:10,849
do the budget allocation you can achieve

297
00:16:06,019 --> 00:16:14,329
better performance in this case so the

298
00:16:10,850 --> 00:16:18,410
summary of the the talk and apologize

299
00:16:14,329 --> 00:16:21,500
that I kind of went run over some of the

300
00:16:18,410 --> 00:16:23,779
technical issues there so we have done

301
00:16:21,500 --> 00:16:25,850
the previous loss analysis in terms of

302
00:16:23,779 --> 00:16:29,660
both the budgeting and also the cost

303
00:16:25,850 --> 00:16:34,250
analysis and in this case we show that

304
00:16:29,660 --> 00:16:38,630
it's not as as obvious as we thought and

305
00:16:34,250 --> 00:16:41,889
it has several components of the problem

306
00:16:38,630 --> 00:16:44,089
and some of the things have higher

307
00:16:41,889 --> 00:16:46,310
impact than we thought and other

308
00:16:44,089 --> 00:16:48,920
parameters that have a lower impact and

309
00:16:46,310 --> 00:16:52,069
we thought so the previous budget

310
00:16:48,920 --> 00:16:55,010
allocation in the case we know that it

311
00:16:52,070 --> 00:16:58,600
makes a difference and the variable the

312
00:16:55,010 --> 00:17:02,569
dynamic allocation does improve the the

313
00:16:58,600 --> 00:17:06,559
final result and the source code is

314
00:17:02,570 --> 00:17:09,410
published on github and we have a

315
00:17:06,559 --> 00:17:13,760
revised version of the paper with

316
00:17:09,410 --> 00:17:16,069
improve the language archive so if you

317
00:17:13,760 --> 00:17:21,530
want to look further and we really

318
00:17:16,069 --> 00:17:24,500
welcome your comment and further

319
00:17:21,530 --> 00:17:28,310
discussions please take a look at the

320
00:17:24,500 --> 00:17:31,080
archived version thank you

321
00:17:28,310 --> 00:17:35,129
[Applause]

322
00:17:31,080 --> 00:17:35,129
okay do we have any questions

323
00:17:42,040 --> 00:17:48,610
so I got one question when you were

324
00:17:44,290 --> 00:17:50,260
presenting these comparisons with

325
00:17:48,610 --> 00:17:51,790
various schedules you all you didn't

326
00:17:50,260 --> 00:17:54,160
show us what the non-differential

327
00:17:51,790 --> 00:17:56,770
private version was it's kind of hard to

328
00:17:54,160 --> 00:17:58,720
tell what the scale is it

329
00:17:56,770 --> 00:18:01,150
how much do you lose by being

330
00:17:58,720 --> 00:18:05,440
differential private at all in terms of

331
00:18:01,150 --> 00:18:09,940
accuracy oh okay that is a really good

332
00:18:05,440 --> 00:18:12,460
question and my lay actually mentioned

333
00:18:09,940 --> 00:18:15,670
that could be but that is one question

334
00:18:12,460 --> 00:18:21,550
that he had cautioned me to defer to him

335
00:18:15,670 --> 00:18:23,560
so yes the the the of course as our

336
00:18:21,550 --> 00:18:25,870
previous two speakers have shown that it

337
00:18:23,560 --> 00:18:28,480
is important to compare the privately

338
00:18:25,870 --> 00:18:30,100
differential private versions with no

339
00:18:28,480 --> 00:18:32,980
differential private versions in terms

340
00:18:30,100 --> 00:18:36,730
of utility for example but in this

341
00:18:32,980 --> 00:18:38,140
particular case we are assuming that we

342
00:18:36,730 --> 00:18:40,720
are comparing different differential

343
00:18:38,140 --> 00:18:43,540
privacy's so yes you're absolutely right

344
00:18:40,720 --> 00:18:45,970
in terms of if we compare with the no

345
00:18:43,540 --> 00:18:48,810
differential private versions that would

346
00:18:45,970 --> 00:18:51,160
be also another part of an interesting

347
00:18:48,810 --> 00:18:55,389
comparison and speaking of utility you

348
00:18:51,160 --> 00:18:58,480
shown us that the shuffling version of

349
00:18:55,390 --> 00:19:00,100
the the sampling the bio based on

350
00:18:58,480 --> 00:19:03,340
shuffling is much worse in terms of the

351
00:19:00,100 --> 00:19:05,800
privacy budget but how much better is it

352
00:19:03,340 --> 00:19:08,679
in terms of sort of the practice of deep

353
00:19:05,800 --> 00:19:11,409
learning why do people use it is it as

354
00:19:08,680 --> 00:19:13,000
easy as let's switch to random sampling

355
00:19:11,410 --> 00:19:14,980
and forget about shuffling and where we

356
00:19:13,000 --> 00:19:16,810
will make the right decision or is it

357
00:19:14,980 --> 00:19:18,430
more nuanced than that thank you for

358
00:19:16,810 --> 00:19:22,240
asking that question that actually part

359
00:19:18,430 --> 00:19:25,030
that I skipped over so what happens is

360
00:19:22,240 --> 00:19:31,810
if you if I may

361
00:19:25,030 --> 00:19:33,850
oh I already off so I'm sorry so the the

362
00:19:31,810 --> 00:19:35,560
question was that whether the in

363
00:19:33,850 --> 00:19:36,639
practical terms that there are

364
00:19:35,560 --> 00:19:38,590
difference between shuffling and

365
00:19:36,640 --> 00:19:42,520
sampling and sampling is more expensive

366
00:19:38,590 --> 00:19:46,149
if you were to calculate it using the

367
00:19:42,520 --> 00:19:49,450
definition how it is supposed to be

368
00:19:46,150 --> 00:19:52,880
calculated so turns out that shuffling

369
00:19:49,450 --> 00:19:55,070
has much easier way of calculating

370
00:19:52,880 --> 00:19:57,290
and part of reasons because of the for

371
00:19:55,070 --> 00:20:02,210
example one explanation for that is a

372
00:19:57,290 --> 00:20:06,379
zero LCD P way of calculating the budget

373
00:20:02,210 --> 00:20:09,980
and for the random sampling actually

374
00:20:06,380 --> 00:20:15,170
there is no way you know no proof no

375
00:20:09,980 --> 00:20:17,660
theorem that shows a particular band so

376
00:20:15,170 --> 00:20:22,490
what we have actually in the archived

377
00:20:17,660 --> 00:20:26,510
version of the paper is empirical bound

378
00:20:22,490 --> 00:20:28,580
and we have analytic expression to

379
00:20:26,510 --> 00:20:33,560
calculate it that is easy to calculate

380
00:20:28,580 --> 00:20:36,230
and an efficient to calculate but this

381
00:20:33,560 --> 00:20:40,639
is actually one example of research

382
00:20:36,230 --> 00:20:44,570
challenge for us to look into in terms

383
00:20:40,640 --> 00:20:49,210
of how we actually can prove title

384
00:20:44,570 --> 00:20:49,210
bounds for the random sampling case

385
00:20:49,960 --> 00:20:53,350
final questions

386
00:20:58,990 --> 00:21:05,940
thank you for the talk so you mentioned

387
00:21:02,260 --> 00:21:09,520
that moments accountant doesn't apply to

388
00:21:05,940 --> 00:21:12,640
reshuffling and gcdp currently doesn't

389
00:21:09,520 --> 00:21:14,230
apply to sampling so is it fair to say

390
00:21:12,640 --> 00:21:16,450
that if you are doing sampling you

391
00:21:14,230 --> 00:21:17,470
should go for moments accountant if

392
00:21:16,450 --> 00:21:20,170
you're doing this shuffling you should

393
00:21:17,470 --> 00:21:23,140
go further that is actually a technical

394
00:21:20,170 --> 00:21:26,350
question that I no answer so thank you

395
00:21:23,140 --> 00:21:28,930
yes that is a question for people who

396
00:21:26,350 --> 00:21:31,719
are doing differential privacy and yes

397
00:21:28,930 --> 00:21:34,300
if you're doing moment accounting you

398
00:21:31,720 --> 00:21:37,300
need to choose specifically random

399
00:21:34,300 --> 00:21:41,520
sampling because what we show is there's

400
00:21:37,300 --> 00:21:44,530
a difference so if you by default used

401
00:21:41,520 --> 00:21:47,260
shuffling do you result may or may not

402
00:21:44,530 --> 00:21:51,580
be what do you expect it yes thank you

403
00:21:47,260 --> 00:21:54,100
thank you so much we have time for one

404
00:21:51,580 --> 00:22:04,629
more question if anyone has it although

405
00:21:54,100 --> 00:22:07,510
I yes to compute pricey budget changes

406
00:22:04,630 --> 00:22:09,430
over time and I know of a method called

407
00:22:07,510 --> 00:22:12,670
pricey buckets that was published about

408
00:22:09,430 --> 00:22:15,220
a year ago that estimates sort of the

409
00:22:12,670 --> 00:22:17,220
privacy laws for each probability by

410
00:22:15,220 --> 00:22:21,190
drawing the priciest distribution and

411
00:22:17,220 --> 00:22:23,080
composing them have you tried this and

412
00:22:21,190 --> 00:22:24,370
you know how it relate like how it

413
00:22:23,080 --> 00:22:25,270
performs compared to the moment

414
00:22:24,370 --> 00:22:27,939
accountant

415
00:22:25,270 --> 00:22:30,190
okay sorry I only heard some of the

416
00:22:27,940 --> 00:22:33,040
words it could have here the the entire

417
00:22:30,190 --> 00:22:36,250
sentence of outside one third of your

418
00:22:33,040 --> 00:22:39,610
question sorry so the policy buckets

419
00:22:36,250 --> 00:22:42,010
method is sometimes used it's a recent

420
00:22:39,610 --> 00:22:44,260
method to approximate privacy budget

421
00:22:42,010 --> 00:22:46,750
over time and I was wondering how it was

422
00:22:44,260 --> 00:22:48,490
performing in relation to the moment

423
00:22:46,750 --> 00:22:49,720
accountant in the different free private

424
00:22:48,490 --> 00:22:53,830
learning applications

425
00:22:49,720 --> 00:22:55,540
you said buckets crazy buckets yes okay

426
00:22:53,830 --> 00:22:58,179
so you can see that I'm not familiar

427
00:22:55,540 --> 00:22:59,549
with that so this is a question I would

428
00:22:58,180 --> 00:23:02,009
defer to lay and

429
00:22:59,549 --> 00:23:05,070
love to talk to you more about this and

430
00:23:02,009 --> 00:23:07,769
to make sure that a flying we have this

431
00:23:05,070 --> 00:23:10,860
question answered thank you okay let's

432
00:23:07,769 --> 00:23:14,560
think captain one more time thank you

433
00:23:10,860 --> 00:23:14,560
[Applause]

