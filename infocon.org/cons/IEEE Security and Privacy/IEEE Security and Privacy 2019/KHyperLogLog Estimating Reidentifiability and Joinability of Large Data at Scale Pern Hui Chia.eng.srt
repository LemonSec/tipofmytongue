1
00:00:08,950 --> 00:00:12,370
thank you

2
00:00:10,660 --> 00:00:15,580
good afternoon everyone how funny

3
00:00:12,370 --> 00:00:17,439
his pants and ass off engine

4
00:00:15,580 --> 00:00:20,410
so today I'm going to talk about

5
00:00:17,440 --> 00:00:24,009
something different no differential

6
00:00:20,410 --> 00:00:25,900
privacy but about ki Baraat law which is

7
00:00:24,009 --> 00:00:28,300
a technique that we developed for

8
00:00:25,900 --> 00:00:31,090
estimating the we identify ability and

9
00:00:28,300 --> 00:00:32,920
generality of large data set at scale so

10
00:00:31,090 --> 00:00:36,519
this is a joint work with multiple

11
00:00:32,920 --> 00:00:41,920
colleagues of mine including Damien who

12
00:00:36,519 --> 00:00:43,269
is also affiliated with ETH Zurich so to

13
00:00:41,920 --> 00:00:45,640
question that we often asked in

14
00:00:43,269 --> 00:00:48,040
prophesies is whether the data set is

15
00:00:45,640 --> 00:00:50,080
potentially really defying and where the

16
00:00:48,040 --> 00:00:52,210
data sets are potentially joinable in a

17
00:00:50,080 --> 00:00:55,570
way that opposes free identification

18
00:00:52,210 --> 00:00:57,159
risk so to answer the matrix to answer

19
00:00:55,570 --> 00:00:59,890
these two questions are relatively

20
00:00:57,159 --> 00:01:03,190
straightforward but the challenge lies

21
00:00:59,890 --> 00:01:07,869
with scalability can the analysis scale

22
00:01:03,190 --> 00:01:10,179
to analyze huge data efficiently so our

23
00:01:07,869 --> 00:01:14,320
approach to this is by approximation and

24
00:01:10,180 --> 00:01:16,540
we have kind of design ka-plop to act to

25
00:01:14,320 --> 00:01:19,990
explicitly answer the two questions that

26
00:01:16,540 --> 00:01:22,660
we asked in the beginning so before I

27
00:01:19,990 --> 00:01:24,640
jump into describing Caraballo clock I

28
00:01:22,660 --> 00:01:28,030
will first give a background on

29
00:01:24,640 --> 00:01:31,150
approximate counting so these are class

30
00:01:28,030 --> 00:01:34,030
of techniques that are memory efficient

31
00:01:31,150 --> 00:01:37,690
they use compact data structure also

32
00:01:34,030 --> 00:01:39,550
commonly known as sketches the size of

33
00:01:37,690 --> 00:01:42,120
those sketches is often bounded and

34
00:01:39,550 --> 00:01:44,560
measured in the order of kilobytes

35
00:01:42,120 --> 00:01:46,480
proximate counting's are often streaming

36
00:01:44,560 --> 00:01:49,210
algorithms which makes them easily

37
00:01:46,480 --> 00:01:53,850
parallelizable and they often come with

38
00:01:49,210 --> 00:01:56,380
a bounded error rate one basic

39
00:01:53,850 --> 00:01:59,158
approximate counting algorithm is called

40
00:01:56,380 --> 00:02:01,479
they came in value which is used for

41
00:01:59,159 --> 00:02:04,930
estimating the number of unique values

42
00:02:01,479 --> 00:02:07,000
that we see in a data stream so the idea

43
00:02:04,930 --> 00:02:09,310
is simple so for every value that we see

44
00:02:07,000 --> 00:02:11,950
in a data stream will hash the value and

45
00:02:09,310 --> 00:02:15,340
will keep the hash value if it is among

46
00:02:11,950 --> 00:02:16,659
the K smallest hash value so note that

47
00:02:15,340 --> 00:02:19,360
we do not need a cryptographic hash

48
00:02:16,659 --> 00:02:22,629
function here but it needs to be a

49
00:02:19,360 --> 00:02:24,280
uniform hash function so to estimate the

50
00:02:22,629 --> 00:02:26,620
number of unique values that we have

51
00:02:24,280 --> 00:02:29,800
seen in the data stream we can then

52
00:02:26,620 --> 00:02:32,260
extrapolate from the density of the case

53
00:02:29,800 --> 00:02:35,800
smaller's hashes to the entire range of

54
00:02:32,260 --> 00:02:39,280
possible hash values the error rate is

55
00:02:35,800 --> 00:02:41,890
given is the error rate with a

56
00:02:39,280 --> 00:02:45,460
configuration of k goes to 2 power of 10

57
00:02:41,890 --> 00:02:48,489
is around 3% and it has a memory

58
00:02:45,460 --> 00:02:51,400
footprint of around a kilobyte assuming

59
00:02:48,490 --> 00:02:53,350
a 64-bit hash function so we can

60
00:02:51,400 --> 00:02:55,900
actually do better than this with hyper

61
00:02:53,350 --> 00:02:58,510
lock lock so instead of keeping the

62
00:02:55,900 --> 00:03:01,300
64-bit hash functions help a lot

63
00:02:58,510 --> 00:03:03,519
keeps cars and keeps the number of

64
00:03:01,300 --> 00:03:05,830
trailing zero in the hash value so the

65
00:03:03,520 --> 00:03:08,620
intuition is that such if you have seen

66
00:03:05,830 --> 00:03:12,310
if we have seen a hash value with a lot

67
00:03:08,620 --> 00:03:15,310
of trailing 0 which is quite unlikely in

68
00:03:12,310 --> 00:03:18,250
practice then it is likely that we have

69
00:03:15,310 --> 00:03:23,140
seen a lot of unique values so high

70
00:03:18,250 --> 00:03:24,940
block users encounters for for all the

71
00:03:23,140 --> 00:03:27,040
values in the data stream it will first

72
00:03:24,940 --> 00:03:29,320
hash the value and it reserves some bits

73
00:03:27,040 --> 00:03:31,900
of the hash value for identifying which

74
00:03:29,320 --> 00:03:33,430
counter is to contribute to and the rest

75
00:03:31,900 --> 00:03:36,250
of the hash value for counting the

76
00:03:33,430 --> 00:03:39,370
number of trailing zeros it has a very

77
00:03:36,250 --> 00:03:43,570
similar error rate as came in value but

78
00:03:39,370 --> 00:03:47,560
notice that the size is smaller than

79
00:03:43,570 --> 00:03:50,890
that of need abide came in value so the

80
00:03:47,560 --> 00:03:52,870
size the the memory footprint required

81
00:03:50,890 --> 00:03:55,779
is one kilobyte as compared to 8

82
00:03:52,870 --> 00:03:58,420
kilobyte so we can actually do even

83
00:03:55,780 --> 00:04:03,160
better with Caleb with hype a lot lock +

84
00:03:58,420 --> 00:04:06,549
+ + + introduce a sparse representation

85
00:04:03,160 --> 00:04:08,859
mode in this mode it keeps a simple map

86
00:04:06,550 --> 00:04:11,770
of the hash value rather than the M

87
00:04:08,860 --> 00:04:14,440
counters so it is more memory efficient

88
00:04:11,770 --> 00:04:16,600
to do so when the number unique value is

89
00:04:14,440 --> 00:04:21,390
low and it actually gives better

90
00:04:16,600 --> 00:04:24,610
estimate as well so I've talked about

91
00:04:21,390 --> 00:04:27,130
came in value and hi blah blah but

92
00:04:24,610 --> 00:04:29,530
knowing the estimate of number unique

93
00:04:27,130 --> 00:04:33,909
value is not quite enough and therefore

94
00:04:29,530 --> 00:04:35,890
we introduced K epilogue log to estimate

95
00:04:33,910 --> 00:04:37,750
the reality viability and journey

96
00:04:35,890 --> 00:04:40,639
ability of the assets

97
00:04:37,750 --> 00:04:44,449
so k helped a lot law is a two-level

98
00:04:40,639 --> 00:04:46,699
counting algorithm in the first level it

99
00:04:44,449 --> 00:04:49,580
takes inspiration from the came in value

100
00:04:46,699 --> 00:04:52,069
and for every entry in the chemin value

101
00:04:49,580 --> 00:04:55,128
it has been associated hype a lot loss

102
00:04:52,069 --> 00:04:56,990
sketch in the second level so we

103
00:04:55,129 --> 00:04:59,539
actually implemented a variant of

104
00:04:56,990 --> 00:05:02,240
hyperbolic sketch which we call

105
00:04:59,539 --> 00:05:04,340
hyperbola plus plus half light so as the

106
00:05:02,240 --> 00:05:07,310
name suggests it is actually more memory

107
00:05:04,340 --> 00:05:10,489
efficient than hypo la la plus plus so

108
00:05:07,310 --> 00:05:12,169
take the example of estimating that we

109
00:05:10,490 --> 00:05:16,039
want to estimate the uniqueness of

110
00:05:12,169 --> 00:05:19,219
user-agent string of to our browser the

111
00:05:16,039 --> 00:05:22,940
idea the algorithm is simple so first we

112
00:05:19,219 --> 00:05:24,650
will hash the pair of value and we check

113
00:05:22,940 --> 00:05:27,710
whether the hash of the user agent is

114
00:05:24,650 --> 00:05:29,479
already existing in the sketch if it is

115
00:05:27,710 --> 00:05:31,580
already in the sketch we are simply

116
00:05:29,479 --> 00:05:34,250
update the corresponding happen or not

117
00:05:31,580 --> 00:05:36,438
sketch with the hash of the ID if the

118
00:05:34,250 --> 00:05:38,629
hash of the user agent is not in the

119
00:05:36,439 --> 00:05:41,150
sketch yet we will check whether it is

120
00:05:38,629 --> 00:05:43,460
among the case mothers hash value and if

121
00:05:41,150 --> 00:05:45,529
it is the Monica's smallest hash value

122
00:05:43,460 --> 00:05:48,560
we will simply create an entry in hyper

123
00:05:45,529 --> 00:05:51,490
log log K epilogue law and we would

124
00:05:48,560 --> 00:05:56,810
simply purge the entry with the largest

125
00:05:51,490 --> 00:05:58,909
hash of the user agent if necessary so

126
00:05:56,810 --> 00:06:01,819
this is a visual representation of a

127
00:05:58,909 --> 00:06:04,339
sketch of a KA blah blah where in the

128
00:06:01,819 --> 00:06:07,669
first level we have roughly a canvas

129
00:06:04,339 --> 00:06:10,490
sketch of the containing the hash of

130
00:06:07,669 --> 00:06:12,620
user agent and each of the entry here

131
00:06:10,490 --> 00:06:15,379
has a corresponding high Palolo sketch

132
00:06:12,620 --> 00:06:19,610
that keep tracks of ID associated with

133
00:06:15,379 --> 00:06:22,909
individual user agent strings so from

134
00:06:19,610 --> 00:06:25,159
this data structure one can see that we

135
00:06:22,909 --> 00:06:28,520
can easily put we can easily estimate

136
00:06:25,159 --> 00:06:31,969
the uniqueness distribution using one

137
00:06:28,520 --> 00:06:34,370
single cable of log sketch so

138
00:06:31,969 --> 00:06:36,409
specifically we can tell what's a

139
00:06:34,370 --> 00:06:38,659
fraction of user agent that are uniquely

140
00:06:36,409 --> 00:06:41,810
identifying what's the fraction of user

141
00:06:38,659 --> 00:06:44,839
agent that is used by or shared by many

142
00:06:41,810 --> 00:06:48,649
use multiple user and so on we can also

143
00:06:44,839 --> 00:06:49,460
plot the cumulative distribution here we

144
00:06:48,649 --> 00:06:51,610
have to examine

145
00:06:49,460 --> 00:06:57,469
and on the left we have a situation

146
00:06:51,610 --> 00:06:59,389
where most where most of the user uses

147
00:06:57,470 --> 00:07:01,400
some common user agent and therefore we

148
00:06:59,389 --> 00:07:04,039
have a heavy tail here and on the right

149
00:07:01,400 --> 00:07:06,530
we have a situation where the user agent

150
00:07:04,039 --> 00:07:10,099
string is really identifying and give

151
00:07:06,530 --> 00:07:12,469
Nick for every user so if we put

152
00:07:10,099 --> 00:07:15,530
together two different type a lot log

153
00:07:12,470 --> 00:07:18,199
sketch we can do some other stuff we can

154
00:07:15,530 --> 00:07:20,780
we can touch whether we can estimate

155
00:07:18,199 --> 00:07:23,330
whether we can estimate a similarity of

156
00:07:20,780 --> 00:07:27,039
the two data set by come by estimating

157
00:07:23,330 --> 00:07:29,690
the containment matrix containment is

158
00:07:27,039 --> 00:07:32,090
simply defined as the interest the size

159
00:07:29,690 --> 00:07:35,539
of the intersection over the size of a

160
00:07:32,090 --> 00:07:37,940
or B and if we know the containment

161
00:07:35,539 --> 00:07:39,740
matrix we can actually go one step

162
00:07:37,940 --> 00:07:42,380
further we can check whether the house

163
00:07:39,740 --> 00:07:45,139
there is any journey ability risk in

164
00:07:42,380 --> 00:07:48,080
such a way that there is a pair of joint

165
00:07:45,139 --> 00:07:50,150
keys that are also uniquely identifying

166
00:07:48,080 --> 00:07:51,650
two different types of data which is

167
00:07:50,150 --> 00:07:57,739
something that we want to avoid in

168
00:07:51,650 --> 00:08:01,150
practice right so we estimate that we

169
00:07:57,740 --> 00:08:06,800
evaluate the performance of Caracalla

170
00:08:01,150 --> 00:08:09,109
through experiments compared to the

171
00:08:06,800 --> 00:08:11,240
counterpart of exact counting we find

172
00:08:09,110 --> 00:08:13,789
that the cpu performance of que hablar

173
00:08:11,240 --> 00:08:16,639
law is consistently one or two orders of

174
00:08:13,789 --> 00:08:19,130
magnitude faster and because it uses

175
00:08:16,639 --> 00:08:22,009
about that memory it has about the

176
00:08:19,130 --> 00:08:23,900
memory usage it can it allows us to

177
00:08:22,009 --> 00:08:27,919
scale to very large data quickly

178
00:08:23,900 --> 00:08:31,130
efficiently so with the configuration of

179
00:08:27,919 --> 00:08:33,919
K to the power of a thousand 24 and M

180
00:08:31,130 --> 00:08:36,620
equals 2 M is the size it is the number

181
00:08:33,919 --> 00:08:39,399
of counter in the second level with the

182
00:08:36,620 --> 00:08:41,929
configuration of M equals to 4 I want to

183
00:08:39,399 --> 00:08:47,029
we are looking at the memory footprint

184
00:08:41,929 --> 00:08:51,939
of around 256 kilobyte 4k high below log

185
00:08:47,029 --> 00:08:54,770
sketch we evaluated also the accuracy of

186
00:08:51,940 --> 00:08:57,980
karbala log sketch in estimating the

187
00:08:54,770 --> 00:09:00,410
uniqueness distribution specifically we

188
00:08:57,980 --> 00:09:00,889
prepared several test distribution

189
00:09:00,410 --> 00:09:03,339
unique

190
00:09:00,889 --> 00:09:06,619
using publicly available data set

191
00:09:03,339 --> 00:09:09,769
including one from Netflix and the US

192
00:09:06,619 --> 00:09:12,439
census data so as you can see on the

193
00:09:09,769 --> 00:09:15,379
chart we have two different distribution

194
00:09:12,439 --> 00:09:18,679
profile on the Left we have a data set

195
00:09:15,379 --> 00:09:23,119
there is more identifying than the one

196
00:09:18,679 --> 00:09:25,608
on the right the uniqueness distribution

197
00:09:23,119 --> 00:09:28,970
estimated by que hablan law is color in

198
00:09:25,609 --> 00:09:33,019
blue and the true distribution is color

199
00:09:28,970 --> 00:09:36,470
in red we find that the accuracies is

200
00:09:33,019 --> 00:09:40,129
pretty good we evaluated also the

201
00:09:36,470 --> 00:09:43,249
accuracy of containment metrics here

202
00:09:40,129 --> 00:09:45,649
four sets of equal sizes we find that

203
00:09:43,249 --> 00:09:49,160
the estimated containment is typically

204
00:09:45,649 --> 00:09:53,389
in the total tolerable range with plus

205
00:09:49,160 --> 00:09:56,238
or minus or five percent error but the

206
00:09:53,389 --> 00:09:59,480
estimated containment can be a little

207
00:09:56,239 --> 00:10:02,779
problematic for set of highly unequal

208
00:09:59,480 --> 00:10:06,230
sizes as seen on the graph on the on the

209
00:10:02,779 --> 00:10:08,809
on the right here when the cardinality

210
00:10:06,230 --> 00:10:12,980
ratio is more than five we see that the

211
00:10:08,809 --> 00:10:15,110
error rate can grow more than 10% and

212
00:10:12,980 --> 00:10:19,819
keeps growing when what keeps growing

213
00:10:15,110 --> 00:10:22,040
with the cardinality ratio and we we

214
00:10:19,819 --> 00:10:24,110
knowledge that this is a limitation with

215
00:10:22,040 --> 00:10:27,189
the way we compute containment metrics

216
00:10:24,110 --> 00:10:28,850
in a way that we computed we estimated

217
00:10:27,189 --> 00:10:31,069
intersection the size of the

218
00:10:28,850 --> 00:10:34,669
intersection by inclusion exclusion

219
00:10:31,069 --> 00:10:37,959
principles but this is really an area

220
00:10:34,669 --> 00:10:41,660
that can use some improvement in

221
00:10:37,959 --> 00:10:46,339
practice we are using care-a-lot law in

222
00:10:41,660 --> 00:10:49,368
production and for every analysis run or

223
00:10:46,339 --> 00:10:52,519
ki Baraat law is capable of analyzing

224
00:10:49,369 --> 00:10:55,160
petabytes of data and we probably

225
00:10:52,519 --> 00:10:58,189
estimate various statistics using the

226
00:10:55,160 --> 00:11:00,738
sketches that are produced in offline so

227
00:10:58,189 --> 00:11:03,618
the ability to the ability to estimate

228
00:11:00,739 --> 00:11:07,129
various statistics offline is really

229
00:11:03,619 --> 00:11:10,309
important particularly for churn ability

230
00:11:07,129 --> 00:11:12,680
testing you will be highly infeasible if

231
00:11:10,309 --> 00:11:15,530
we were to

232
00:11:12,680 --> 00:11:20,030
test for generative data set using the

233
00:11:15,530 --> 00:11:25,160
original data so I'm kind of last year

234
00:11:20,030 --> 00:11:28,100
so in summary we introduced ki Baraat

235
00:11:25,160 --> 00:11:32,480
law for estimating Rio Dental lady and

236
00:11:28,100 --> 00:11:36,320
Jane ability at scale we there could be

237
00:11:32,480 --> 00:11:39,140
some approximation error but in practice

238
00:11:36,320 --> 00:11:41,720
we are we think that it is a rather

239
00:11:39,140 --> 00:11:44,830
useful tool for us to understand the

240
00:11:41,720 --> 00:11:48,110
characteristics of the data and to be

241
00:11:44,830 --> 00:11:51,500
and to inform the decision makers when

242
00:11:48,110 --> 00:11:53,540
choosing data protection strategy we

243
00:11:51,500 --> 00:11:56,810
also find it to be particularly useful

244
00:11:53,540 --> 00:11:59,410
for regression testing particularly for

245
00:11:56,810 --> 00:12:03,530
detecting data generated due to

246
00:11:59,410 --> 00:12:08,439
occasional occasional data logging

247
00:12:03,530 --> 00:12:11,990
errors there could be multiple

248
00:12:08,440 --> 00:12:14,630
directions for future work so although

249
00:12:11,990 --> 00:12:18,560
it is memory efficient but ki Baraat

250
00:12:14,630 --> 00:12:21,620
loss still is rather CPU intensive in a

251
00:12:18,560 --> 00:12:25,189
way that we require to read the data

252
00:12:21,620 --> 00:12:28,520
once entirely so we think that some

253
00:12:25,190 --> 00:12:31,310
innovation - or some improvement to be

254
00:12:28,520 --> 00:12:33,110
able to handle generality testing using

255
00:12:31,310 --> 00:12:36,410
sample data will be really interesting

256
00:12:33,110 --> 00:12:38,180
and we also think that there could be

257
00:12:36,410 --> 00:12:40,370
more usage or there could be more

258
00:12:38,180 --> 00:12:44,930
application of approximate counting in

259
00:12:40,370 --> 00:12:47,270
privacy enhancing techniques and that's

260
00:12:44,930 --> 00:12:52,140
all thank you

261
00:12:47,270 --> 00:12:59,949
[Applause]

262
00:12:52,140 --> 00:13:01,750
have any questions so I'll get us

263
00:12:59,950 --> 00:13:03,850
started so you presented some techniques

264
00:13:01,750 --> 00:13:06,279
that makes it it sounds like it makes it

265
00:13:03,850 --> 00:13:08,470
easy to compute these privacy related

266
00:13:06,279 --> 00:13:10,870
metrics dis uniqueness and joint ability

267
00:13:08,470 --> 00:13:12,760
but if you did not use your method I

268
00:13:10,870 --> 00:13:18,310
imagine they're still computable but

269
00:13:12,760 --> 00:13:21,490
probably not efficiently right so yeah

270
00:13:18,310 --> 00:13:24,459
so in our experiments when you when

271
00:13:21,490 --> 00:13:26,500
using a counterpart of exact counting we

272
00:13:24,459 --> 00:13:30,239
find that particular method can't scale

273
00:13:26,500 --> 00:13:33,220
to handle even a terabytes of data and

274
00:13:30,240 --> 00:13:34,270
in terms of the memory requirement or

275
00:13:33,220 --> 00:13:36,690
right memory

276
00:13:34,270 --> 00:13:36,689
moment

277
00:13:46,620 --> 00:13:52,590
thank you for the talk this might be a

278
00:13:48,810 --> 00:13:55,199
naive question but for the containment

279
00:13:52,590 --> 00:13:57,840
matrix you showed is it easy to see that

280
00:13:55,200 --> 00:13:59,640
the other techniques like hyper lock

281
00:13:57,840 --> 00:14:02,430
lock + + + everything will be worse off

282
00:13:59,640 --> 00:14:04,790
or is there a trade-off that you get

283
00:14:02,430 --> 00:14:08,579
better accuracy or coverage with worse

284
00:14:04,790 --> 00:14:10,980
containment do you mean that there could

285
00:14:08,580 --> 00:14:13,770
be some other matrix for computing

286
00:14:10,980 --> 00:14:17,190
similarity of data set or like does

287
00:14:13,770 --> 00:14:19,890
hyper lot + + have higher variability in

288
00:14:17,190 --> 00:14:24,210
the matrix that you should for your

289
00:14:19,890 --> 00:14:27,449
technique whether hyperlocal + + will

290
00:14:24,210 --> 00:14:30,060
actually perform better yeah so

291
00:14:27,450 --> 00:14:33,620
eventually we have computed containment

292
00:14:30,060 --> 00:14:39,209
using the first level which is the kmv

293
00:14:33,620 --> 00:14:42,630
so that is a good question I think it is

294
00:14:39,210 --> 00:14:46,080
comparable essentially because so it

295
00:14:42,630 --> 00:14:48,900
depends on how exactly you come estimate

296
00:14:46,080 --> 00:14:50,760
the size of the intersection and here we

297
00:14:48,900 --> 00:14:53,400
estimate a sense of intersection using

298
00:14:50,760 --> 00:14:57,330
inclusion angle at inclusion exclusion

299
00:14:53,400 --> 00:15:02,209
principle which then is about the same

300
00:14:57,330 --> 00:15:02,210
for both method thank you

301
00:15:06,140 --> 00:15:10,470
one last question

302
00:15:07,920 --> 00:15:12,719
so you presented joint ability as

303
00:15:10,470 --> 00:15:14,930
something you don't want but let's say

304
00:15:12,720 --> 00:15:18,110
we're in a situation or we do want to

305
00:15:14,930 --> 00:15:20,370
have good uniqueness as a not good

306
00:15:18,110 --> 00:15:21,870
identifiability but we do want joy

307
00:15:20,370 --> 00:15:25,020
nobility are those things actually

308
00:15:21,870 --> 00:15:27,590
possible at the same time right the

309
00:15:25,020 --> 00:15:31,530
other side of the coins like if you have

310
00:15:27,590 --> 00:15:33,090
imagined that your be organization where

311
00:15:31,530 --> 00:15:34,890
different teams produce different data

312
00:15:33,090 --> 00:15:38,280
and sometimes you want the data to be

313
00:15:34,890 --> 00:15:40,020
joinable and to be discoverable so it

314
00:15:38,280 --> 00:15:42,660
could indeed be a very interesting

315
00:15:40,020 --> 00:15:45,750
applications where you you create a

316
00:15:42,660 --> 00:15:48,990
sketch of all your data and there is a

317
00:15:45,750 --> 00:15:51,930
repository that gather all the sketches

318
00:15:48,990 --> 00:15:54,510
and you might want to fight if I have

319
00:15:51,930 --> 00:15:59,510
this data set what additional data set

320
00:15:54,510 --> 00:16:04,110
cannot join with okay

321
00:15:59,510 --> 00:16:07,110
question over there when if you publish

322
00:16:04,110 --> 00:16:09,450
the sketch you might be able to infer

323
00:16:07,110 --> 00:16:11,370
some information about the data sets did

324
00:16:09,450 --> 00:16:15,570
you look a bit at this issue because

325
00:16:11,370 --> 00:16:18,630
right right then actually our quartered

326
00:16:15,570 --> 00:16:21,090
I mean publish a paper that that the

327
00:16:18,630 --> 00:16:25,430
sketch is actually not private and so we

328
00:16:21,090 --> 00:16:28,290
still need to keep those sketches with

329
00:16:25,430 --> 00:16:31,020
with good control and we've shot

330
00:16:28,290 --> 00:16:33,360
retention time so you think if you if

331
00:16:31,020 --> 00:16:36,150
you needed to add noise to avoid this

332
00:16:33,360 --> 00:16:37,890
inference using the accuracy of the

333
00:16:36,150 --> 00:16:40,199
estimation will degrade quite a lot or

334
00:16:37,890 --> 00:16:46,380
did you do some experimental tease yeah

335
00:16:40,200 --> 00:16:49,080
so I mean yeah so that mean actually has

336
00:16:46,380 --> 00:16:54,030
a paper on impact that says that is kind

337
00:16:49,080 --> 00:16:56,220
of impossible but maybe you could if you

338
00:16:54,030 --> 00:16:58,860
come up with a better ways to have a

339
00:16:56,220 --> 00:17:03,340
differentially private or estimator that

340
00:16:58,860 --> 00:17:06,339
could be very interesting thank you

341
00:17:03,340 --> 00:17:09,780
alright looks like we can thank Peron

342
00:17:06,339 --> 00:17:09,780
one more time thank you

