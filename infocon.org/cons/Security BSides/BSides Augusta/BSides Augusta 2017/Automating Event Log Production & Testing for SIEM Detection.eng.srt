1
00:00:30,039 --> 00:00:33,550
cool my name is Alison today imma be

2
00:00:33,550 --> 00:00:35,590
talking to you about automating event

3
00:00:35,590 --> 00:00:37,449
log production and testing for detection

4
00:00:37,449 --> 00:00:40,030
purposes or at the very least trying to

5
00:00:40,030 --> 00:00:41,260
make it a little less of a pain in the

6
00:00:41,260 --> 00:00:44,829
ass a little bit about me I work in fire

7
00:00:44,829 --> 00:00:46,870
labs on detection for a product that we

8
00:00:46,870 --> 00:00:49,089
call the threat analytics platform if

9
00:00:49,089 --> 00:00:51,010
you're not familiar with it it's fires

10
00:00:51,010 --> 00:00:53,649
cloud sim offering it's currently being

11
00:00:53,649 --> 00:00:55,690
coagulated into the greater helix

12
00:00:55,690 --> 00:00:57,129
platform so the name is subject to

13
00:00:57,129 --> 00:00:59,680
change but going forward if I say tap I

14
00:00:59,680 --> 00:01:01,089
so I mean I'm not referring to something

15
00:01:01,089 --> 00:01:03,250
like a network tab so this is a

16
00:01:03,250 --> 00:01:04,569
marketing talk I'm not gonna be talking

17
00:01:04,569 --> 00:01:07,420
about tap this is general like boots on

18
00:01:07,420 --> 00:01:10,780
ground making analyst lives easier just

19
00:01:10,780 --> 00:01:12,280
a little bit about my day to day just to

20
00:01:12,280 --> 00:01:13,570
give you a little color so why this

21
00:01:13,570 --> 00:01:15,400
stuff became important in the impetus

22
00:01:15,400 --> 00:01:18,070
for some of these things the book of

23
00:01:18,070 --> 00:01:19,840
what I do is create detection for event

24
00:01:19,840 --> 00:01:21,670
data and when I say detects unicum

25
00:01:21,670 --> 00:01:23,500
anything from signatures to analytics

26
00:01:23,500 --> 00:01:25,479
modules to generally just analysts

27
00:01:25,479 --> 00:01:27,790
experience within the product itself so

28
00:01:27,790 --> 00:01:29,229
making the investigation experience

29
00:01:29,229 --> 00:01:31,420
better or alert triage or anything like

30
00:01:31,420 --> 00:01:31,979
that

31
00:01:31,979 --> 00:01:34,450
and when I say event data that can be

32
00:01:34,450 --> 00:01:36,550
literally anything a customer can send

33
00:01:36,550 --> 00:01:39,820
to us post based data Windows Unix Linux

34
00:01:39,820 --> 00:01:42,180
Network data across the board

35
00:01:42,180 --> 00:01:44,440
application data so it's like somebody's

36
00:01:44,440 --> 00:01:46,270
sending us their even internal

37
00:01:46,270 --> 00:01:47,710
application data or something like a

38
00:01:47,710 --> 00:01:50,890
single sign-on platform cloud event logs

39
00:01:50,890 --> 00:01:53,500
mostly work with things like AWS done a

40
00:01:53,500 --> 00:01:54,729
little bit with industrial control

41
00:01:54,729 --> 00:01:56,680
systems data so we've done you know

42
00:01:56,680 --> 00:01:58,120
about everything under the Sun if you

43
00:01:58,120 --> 00:01:59,500
can imagine it we've probably touched at

44
00:01:59,500 --> 00:02:01,420
some point the other piece of what I do

45
00:02:01,420 --> 00:02:03,250
the less the more informal piece of what

46
00:02:03,250 --> 00:02:05,229
I do is develop tools for our team to

47
00:02:05,229 --> 00:02:06,970
keep our collective heads from exploding

48
00:02:06,970 --> 00:02:08,440
because when you're working with

49
00:02:08,440 --> 00:02:10,179
thousands and thousands of event logs or

50
00:02:10,179 --> 00:02:11,860
a bunch of customers you need all kinds

51
00:02:11,860 --> 00:02:13,299
of internal tooling just to make your

52
00:02:13,299 --> 00:02:16,269
lives easier anything from our test

53
00:02:16,269 --> 00:02:18,610
tooling to data validation stuff to a

54
00:02:18,610 --> 00:02:20,049
couple of tools I'm gonna be talking to

55
00:02:20,049 --> 00:02:21,459
you guys about today that we're going to

56
00:02:21,459 --> 00:02:24,879
be releasing a little bit of background

57
00:02:24,879 --> 00:02:26,890
on our team so we create detection for

58
00:02:26,890 --> 00:02:29,730
wide variety customers so any new tap or

59
00:02:29,730 --> 00:02:32,379
helix instance that gets deployed gets

60
00:02:32,379 --> 00:02:34,360
our entire rule set which means that we

61
00:02:34,360 --> 00:02:35,500
have the stage generic as possible

62
00:02:35,500 --> 00:02:38,950
because we don't in 99% of cases we

63
00:02:38,950 --> 00:02:39,780
don't have the

64
00:02:39,780 --> 00:02:41,790
luxury of knowing what a customer's

65
00:02:41,790 --> 00:02:43,260
network is going to look like beforehand

66
00:02:43,260 --> 00:02:44,670
when they start dumping event logs into

67
00:02:44,670 --> 00:02:46,440
our platform which means we have to stay

68
00:02:46,440 --> 00:02:48,030
as generic as possible and cover as many

69
00:02:48,030 --> 00:02:50,310
use cases as we can and we can't be very

70
00:02:50,310 --> 00:02:52,200
specific because it needs to apply it

71
00:02:52,200 --> 00:02:53,850
like I said to as many customers as

72
00:02:53,850 --> 00:02:55,980
possible but there also means is we have

73
00:02:55,980 --> 00:02:57,750
to have a really robust quality control

74
00:02:57,750 --> 00:03:00,270
setup at the core of that that means

75
00:03:00,270 --> 00:03:01,830
unit testing for every single rule we

76
00:03:01,830 --> 00:03:03,030
put out I'm going to talk a little bit

77
00:03:03,030 --> 00:03:04,650
more about our quality control here in a

78
00:03:04,650 --> 00:03:08,610
second but the this is really the like

79
00:03:08,610 --> 00:03:10,500
the core of why this talk was important

80
00:03:10,500 --> 00:03:12,780
at least to me because when we go to

81
00:03:12,780 --> 00:03:14,430
create a rule I have to be confident the

82
00:03:14,430 --> 00:03:15,900
rules gonna hit on what I think it's

83
00:03:15,900 --> 00:03:17,430
gonna hit on which means we need to be

84
00:03:17,430 --> 00:03:20,310
able to generate events that represent

85
00:03:20,310 --> 00:03:23,430
what it would look like in real life I'm

86
00:03:23,430 --> 00:03:25,130
gonna hit on this quickly just because

87
00:03:25,130 --> 00:03:28,319
talking some other people I think some

88
00:03:28,319 --> 00:03:29,670
of things we do internally might apply

89
00:03:29,670 --> 00:03:31,200
to other people's situations since we

90
00:03:31,200 --> 00:03:32,670
work for a vendor and building a rule

91
00:03:32,670 --> 00:03:34,140
set for a bunch of different customers

92
00:03:34,140 --> 00:03:35,700
may not necessarily apply to everybody

93
00:03:35,700 --> 00:03:37,140
since we're kind of a unique use case

94
00:03:37,140 --> 00:03:39,110
but if anything in here looks

95
00:03:39,110 --> 00:03:40,860
interesting you wanna learn more about

96
00:03:40,860 --> 00:03:42,630
just come up and ask one of us there's a

97
00:03:42,630 --> 00:03:43,799
bunch of our team members here when you

98
00:03:43,799 --> 00:03:45,720
talk a little bit about it more we run

99
00:03:45,720 --> 00:03:47,340
our content development basically like a

100
00:03:47,340 --> 00:03:50,310
small software development shop and what

101
00:03:50,310 --> 00:03:51,900
I mean by that is we use git for version

102
00:03:51,900 --> 00:03:53,790
control for all of our rules and we use

103
00:03:53,790 --> 00:03:56,970
a simple topic branching methodology so

104
00:03:56,970 --> 00:03:58,350
any new rule that we want to get at in

105
00:03:58,350 --> 00:04:00,000
the rule set goes into it's a topic

106
00:04:00,000 --> 00:04:02,010
branch and gets put in a github pull

107
00:04:02,010 --> 00:04:04,079
request and we have a standard template

108
00:04:04,079 --> 00:04:05,400
for pull requests that includes three

109
00:04:05,400 --> 00:04:06,950
different things validation of the data

110
00:04:06,950 --> 00:04:09,209
unit testing for every single rule that

111
00:04:09,209 --> 00:04:10,920
could set it in and then historical

112
00:04:10,920 --> 00:04:13,500
checks so a validation is important one

113
00:04:13,500 --> 00:04:15,660
because we store store all the rules in

114
00:04:15,660 --> 00:04:17,548
JSON format it's a bunch of rule

115
00:04:17,548 --> 00:04:19,798
metadata and then the query string which

116
00:04:19,798 --> 00:04:21,510
is the actual signature itself is stored

117
00:04:21,510 --> 00:04:24,750
in there as well but one is a valid JSON

118
00:04:24,750 --> 00:04:26,910
to do these metadata fields make sense

119
00:04:26,910 --> 00:04:28,500
we do some calculation between different

120
00:04:28,500 --> 00:04:29,940
fields that we can do automatically and

121
00:04:29,940 --> 00:04:32,460
I have humans do that second piece like

122
00:04:32,460 --> 00:04:33,870
I mentioned which is a really important

123
00:04:33,870 --> 00:04:35,729
piece is testing anytime we want to

124
00:04:35,729 --> 00:04:37,229
introduce a new rule under the rule set

125
00:04:37,229 --> 00:04:38,729
it has to have at least one positive

126
00:04:38,729 --> 00:04:39,689
tests and

127
00:04:39,689 --> 00:04:41,519
with it which at bare minimum says I

128
00:04:41,519 --> 00:04:43,529
know this rule is gonna match on what

129
00:04:43,529 --> 00:04:44,879
it's supposed to be matching are what

130
00:04:44,879 --> 00:04:47,249
I'm intending it to match on that third

131
00:04:47,249 --> 00:04:48,869
thing is a historical check since we

132
00:04:48,869 --> 00:04:51,599
were gonna sim and we have the luxury of

133
00:04:51,599 --> 00:04:52,919
being able to query since we're in the

134
00:04:52,919 --> 00:04:54,779
cloud as well we had the luxury

135
00:04:54,779 --> 00:04:56,969
recording across our entire customer set

136
00:04:56,969 --> 00:04:59,099
to say okay for for X number of

137
00:04:59,099 --> 00:05:01,079
retention days for each customer we can

138
00:05:01,079 --> 00:05:03,779
say I'm confident that this rule is

139
00:05:03,779 --> 00:05:05,969
going to not be false positive wrong

140
00:05:05,969 --> 00:05:08,339
because our query language is like 99%

141
00:05:08,339 --> 00:05:11,759
of parody with our rule language I'm

142
00:05:11,759 --> 00:05:12,719
confident it's not gonna be false

143
00:05:12,719 --> 00:05:14,669
positive prone if it does have hits

144
00:05:14,669 --> 00:05:16,649
their true positives and even if it does

145
00:05:16,649 --> 00:05:18,029
have some false positives I think it's

146
00:05:18,029 --> 00:05:19,709
an acceptable level to push it out

147
00:05:19,709 --> 00:05:21,629
so all customers can take advantage of

148
00:05:21,629 --> 00:05:24,929
it all I gets put into a pull request

149
00:05:24,929 --> 00:05:27,239
template everything is peer reviewed and

150
00:05:27,239 --> 00:05:29,039
since it since it's formatted in this

151
00:05:29,039 --> 00:05:30,509
nice template a human can go through and

152
00:05:30,509 --> 00:05:31,709
look at it I say okay all the data

153
00:05:31,709 --> 00:05:33,809
validate all the tests pass the check

154
00:05:33,809 --> 00:05:35,579
looks good we can merge the send the

155
00:05:35,579 --> 00:05:37,799
master and then when we go to publish a

156
00:05:37,799 --> 00:05:40,229
rule we automatically revalidate the

157
00:05:40,229 --> 00:05:42,509
entire rule set and do a full unit test

158
00:05:42,509 --> 00:05:44,009
for the entire rule set before things go

159
00:05:44,009 --> 00:05:45,419
out so it can have a double layer

160
00:05:45,419 --> 00:05:47,219
protection there which lets us publish

161
00:05:47,219 --> 00:05:49,349
basically on-demand anytime we want and

162
00:05:49,349 --> 00:05:51,089
be confident that any rule push out it's

163
00:05:51,089 --> 00:05:53,699
not going to be an issue just to talk a

164
00:05:53,699 --> 00:05:55,050
little bit about our pipeline and this

165
00:05:55,050 --> 00:05:57,089
is a super-high level representation the

166
00:05:57,089 --> 00:05:58,469
pipeline because I want appointment

167
00:05:58,469 --> 00:06:01,709
point out one particular thing here log

168
00:06:01,709 --> 00:06:03,149
sender which we call communication

169
00:06:03,149 --> 00:06:04,889
broker customer sends logs to the log

170
00:06:04,889 --> 00:06:06,839
sender this goes to a log receiver in

171
00:06:06,839 --> 00:06:08,219
the cloud and then gets put into what we

172
00:06:08,219 --> 00:06:10,019
call our ingestion pipeline and what's

173
00:06:10,019 --> 00:06:11,610
in parentheses there is what's important

174
00:06:11,610 --> 00:06:13,619
our rules get applied here and then they

175
00:06:13,619 --> 00:06:15,360
get stored in the backend and the reason

176
00:06:15,360 --> 00:06:16,800
why I want to point that out is a tip I

177
00:06:16,800 --> 00:06:19,019
have for you if you have control of your

178
00:06:19,019 --> 00:06:21,149
ingestion pipeline and where you can

179
00:06:21,149 --> 00:06:23,459
apply detection if you have the luxury

180
00:06:23,459 --> 00:06:26,699
of that particular case apply your rules

181
00:06:26,699 --> 00:06:28,139
during ingestion because you can do one

182
00:06:28,139 --> 00:06:29,579
particular thing it's extremely useful

183
00:06:29,579 --> 00:06:32,550
later on any rules that match you can

184
00:06:32,550 --> 00:06:35,159
then apply the rule metadata as data in

185
00:06:35,159 --> 00:06:37,079
their raw bin itself so then I'm gonna

186
00:06:37,079 --> 00:06:38,849
get stored in the backend and becomes

187
00:06:38,849 --> 00:06:41,579
searchable in your simra log source

188
00:06:41,579 --> 00:06:43,559
platform or whatever it is that way you

189
00:06:43,559 --> 00:06:45,659
want to say ok I want to see all these

190
00:06:45,659 --> 00:06:47,160
events that match on a rule

191
00:06:47,160 --> 00:06:48,210
you have to go to a separate alert

192
00:06:48,210 --> 00:06:49,620
database to see that because the ruled

193
00:06:49,620 --> 00:06:51,300
metadata is actually in the raw event

194
00:06:51,300 --> 00:06:53,850
and that's become more important when I

195
00:06:53,850 --> 00:06:55,230
talked about something else later down

196
00:06:55,230 --> 00:06:58,350
the line so the overall problem is

197
00:06:58,350 --> 00:07:00,330
generating an event logs for detection

198
00:07:00,330 --> 00:07:02,370
scenarios is just manually intensive and

199
00:07:02,370 --> 00:07:04,230
kind of annoying for a multitude of

200
00:07:04,230 --> 00:07:06,420
reasons getting logs from one place to

201
00:07:06,420 --> 00:07:09,300
another is kind of pain is recreating

202
00:07:09,300 --> 00:07:11,310
attack scenarios is necessarily trivial

203
00:07:11,310 --> 00:07:12,690
especially with things like Windows host

204
00:07:12,690 --> 00:07:13,710
logs which I'm going to talk a little

205
00:07:13,710 --> 00:07:14,820
bit more in a bit

206
00:07:14,820 --> 00:07:16,410
analysts having to maintain their own

207
00:07:16,410 --> 00:07:18,690
VMs there are logs and if infrastructure

208
00:07:18,690 --> 00:07:20,790
you know package configurations all

209
00:07:20,790 --> 00:07:22,320
kinds of things and there's a number of

210
00:07:22,320 --> 00:07:24,210
ways you can solve this problem you get

211
00:07:24,210 --> 00:07:25,710
apply to have off sprints opposed to

212
00:07:25,710 --> 00:07:26,910
this things like configuration

213
00:07:26,910 --> 00:07:28,650
management and being deployment with

214
00:07:28,650 --> 00:07:30,750
things like vagrant we chose to take a

215
00:07:30,750 --> 00:07:32,190
little bit of a different route and kind

216
00:07:32,190 --> 00:07:34,080
of centralized these things as services

217
00:07:34,080 --> 00:07:35,340
that any analyst because you could use

218
00:07:35,340 --> 00:07:36,600
it so they don't have to run these

219
00:07:36,600 --> 00:07:41,130
things on their local boxes so my focus

220
00:07:41,130 --> 00:07:43,770
today is going to be on network event

221
00:07:43,770 --> 00:07:45,930
data and Windows host data I mentioned a

222
00:07:45,930 --> 00:07:47,550
bunch of other data sources before and

223
00:07:47,550 --> 00:07:48,930
they all have their own unique problems

224
00:07:48,930 --> 00:07:51,000
and generating test events and getting

225
00:07:51,000 --> 00:07:52,650
them places especially things like

226
00:07:52,650 --> 00:07:55,170
something like cloud logs getting cloud

227
00:07:55,170 --> 00:07:56,940
trail logs out of a TBS into a sim

228
00:07:56,940 --> 00:07:59,580
parsed and creating section scenarios

229
00:07:59,580 --> 00:08:01,140
has its own challenges but these are the

230
00:08:01,140 --> 00:08:02,280
two things I'm going to be talking about

231
00:08:02,280 --> 00:08:07,380
today starting with event logs so the

232
00:08:07,380 --> 00:08:08,970
advice I have for you here is that Broz

233
00:08:08,970 --> 00:08:11,520
your friend fewer at security in Econ

234
00:08:11,520 --> 00:08:13,290
yesterday or even at some of the other

235
00:08:13,290 --> 00:08:14,310
talks today you've probably heard a lot

236
00:08:14,310 --> 00:08:17,840
about bro super easy to set up in parse

237
00:08:17,840 --> 00:08:19,800
supports a ton of different data types

238
00:08:19,800 --> 00:08:21,780
and even if you're not using it in

239
00:08:21,780 --> 00:08:23,070
production even if you just have

240
00:08:23,070 --> 00:08:24,419
something like a proxy just sending

241
00:08:24,419 --> 00:08:26,790
Network logs into and your sim or even

242
00:08:26,790 --> 00:08:28,950
just firewall logs I still recommend you

243
00:08:28,950 --> 00:08:30,450
use bro for generating your test data

244
00:08:30,450 --> 00:08:33,599
because I guarantee if you especially if

245
00:08:33,599 --> 00:08:34,740
you just have something like a proxy

246
00:08:34,740 --> 00:08:36,000
it's gonna have a bunch more data than

247
00:08:36,000 --> 00:08:37,620
your proxy is and you can always

248
00:08:37,620 --> 00:08:39,870
normalize that data in the backend so

249
00:08:39,870 --> 00:08:43,440
let's say you have a really simple proxy

250
00:08:43,440 --> 00:08:45,840
set up you just have method domain URI

251
00:08:45,840 --> 00:08:48,360
port and that was it so if you generate

252
00:08:48,360 --> 00:08:50,250
your test data and grow as long as the

253
00:08:50,250 --> 00:08:52,320
URI field matches up to the URI field

254
00:08:52,320 --> 00:08:54,240
and your proxy logs it doesn't really

255
00:08:54,240 --> 00:08:54,779
matter that

256
00:08:54,779 --> 00:08:56,550
generated and bro not you can always

257
00:08:56,550 --> 00:08:58,889
normalize that with parsing and I say

258
00:08:58,889 --> 00:09:00,449
that like you can just parse it I mean

259
00:09:00,449 --> 00:09:02,160
that's sort of and parsing is its own

260
00:09:02,160 --> 00:09:03,420
complete challenge and outside of the

261
00:09:03,420 --> 00:09:04,769
scope of this talk and me saying just

262
00:09:04,769 --> 00:09:06,360
parse stuff is kind of like saying in

263
00:09:06,360 --> 00:09:09,540
order to win just don't lose it has all

264
00:09:09,540 --> 00:09:11,790
of its own challenges but as long as you

265
00:09:11,790 --> 00:09:13,199
do have at least decent parsing in those

266
00:09:13,199 --> 00:09:14,970
field matches uh those fields match up

267
00:09:14,970 --> 00:09:16,079
it doesn't really matter if you're using

268
00:09:16,079 --> 00:09:19,350
bro or not so in this particular case

269
00:09:19,350 --> 00:09:21,540
I'm suggesting you use bro now you could

270
00:09:21,540 --> 00:09:23,309
have every analyst run bro

271
00:09:23,309 --> 00:09:25,829
on their own box which like I said

272
00:09:25,829 --> 00:09:27,689
before presents its own challenges with

273
00:09:27,689 --> 00:09:29,249
configuration management if you have a

274
00:09:29,249 --> 00:09:31,019
bunch of analysts generating bro data to

275
00:09:31,019 --> 00:09:32,399
talk do all their configs with the same

276
00:09:32,399 --> 00:09:34,559
are they generating data that's going to

277
00:09:34,559 --> 00:09:36,300
be consistent so when they go to create

278
00:09:36,300 --> 00:09:37,470
detection they would end up with the

279
00:09:37,470 --> 00:09:41,069
same sort of result or this is why we

280
00:09:41,069 --> 00:09:42,629
wrote this tool so we like to call bro

281
00:09:42,629 --> 00:09:44,009
copy

282
00:09:44,009 --> 00:09:48,329
it's a portmanteau of bro pcap and API I

283
00:09:48,329 --> 00:09:50,040
included a github link up there which

284
00:09:50,040 --> 00:09:51,930
isn't live at the moment it'll hopefully

285
00:09:51,930 --> 00:09:54,870
be live in the next couple of days some

286
00:09:54,870 --> 00:09:56,160
internal things that just need to happen

287
00:09:56,160 --> 00:09:58,740
before it gets posted but basically what

288
00:09:58,740 --> 00:10:01,379
it is is it's an HTTP REST API from

289
00:10:01,379 --> 00:10:03,569
massive bro pcap processing and tagging

290
00:10:03,569 --> 00:10:05,279
and the tagging thing is the important

291
00:10:05,279 --> 00:10:07,620
here so at a high level what you can do

292
00:10:07,620 --> 00:10:08,850
is you can take a bunch of pcaps

293
00:10:08,850 --> 00:10:11,339
let's say you have a dozen pcaps from a

294
00:10:11,339 --> 00:10:13,980
particular our family you can submit

295
00:10:13,980 --> 00:10:15,660
those with a post request - bro copy

296
00:10:15,660 --> 00:10:17,220
with a particular tag or whatever you

297
00:10:17,220 --> 00:10:19,620
want to call it it then takes all those

298
00:10:19,620 --> 00:10:21,839
p caps in runs bro against some stores

299
00:10:21,839 --> 00:10:23,639
all the logs locally and submits them to

300
00:10:23,639 --> 00:10:25,620
a configured syslog server so you can go

301
00:10:25,620 --> 00:10:27,360
from one post request to logs in your

302
00:10:27,360 --> 00:10:30,439
sim and one step and that's it

303
00:10:30,439 --> 00:10:32,670
just to talk a little bit about the

304
00:10:32,670 --> 00:10:34,050
internals of bro copy that I'm not going

305
00:10:34,050 --> 00:10:35,970
to go too deep into this because it

306
00:10:35,970 --> 00:10:37,889
becomes a web app development talk

307
00:10:37,889 --> 00:10:39,120
there's two pieces to it

308
00:10:39,120 --> 00:10:41,309
there's the HTTP API on the front end

309
00:10:41,309 --> 00:10:43,230
and all this is written in Python it's a

310
00:10:43,230 --> 00:10:45,920
flask out with one route cement pcaps

311
00:10:45,920 --> 00:10:49,139
you post pcaps to it at queues jobs and

312
00:10:49,139 --> 00:10:50,790
Redis and then the second piece of real

313
00:10:50,790 --> 00:10:53,069
copy which we creatively called broke

314
00:10:53,069 --> 00:10:56,279
happy worker is just a worker queue

315
00:10:56,279 --> 00:10:58,139
using a Python module called Redis queue

316
00:10:58,139 --> 00:11:00,089
it takes all the submitted key caps

317
00:11:00,089 --> 00:11:02,579
through bro runs them through bro stores

318
00:11:02,579 --> 00:11:04,290
logs locally submits them to a syslog

319
00:11:04,290 --> 00:11:05,340
server

320
00:11:05,340 --> 00:11:06,870
there's a bunch of ways you can deploy

321
00:11:06,870 --> 00:11:08,460
this and it scales really well and this

322
00:11:08,460 --> 00:11:10,080
is kind of overkill for a simple like

323
00:11:10,080 --> 00:11:11,850
just get peak apps in your sim situation

324
00:11:11,850 --> 00:11:13,710
but if you have hundreds and hundreds of

325
00:11:13,710 --> 00:11:15,120
peak apps or thousands and thousands of

326
00:11:15,120 --> 00:11:17,610
peak apps like what you sometimes do the

327
00:11:17,610 --> 00:11:18,900
scale is really well across the board

328
00:11:18,900 --> 00:11:21,540
because you can deploy it as a WSGI out

329
00:11:21,540 --> 00:11:23,700
for the API and then for the worker you

330
00:11:23,700 --> 00:11:25,770
can run as many workers as you want we

331
00:11:25,770 --> 00:11:28,290
use just as a daemon or a process

332
00:11:28,290 --> 00:11:31,380
control daemon called supervisor D and

333
00:11:31,380 --> 00:11:32,790
you can run as many like I said because

334
00:11:32,790 --> 00:11:33,930
they're independent like discrete

335
00:11:33,930 --> 00:11:36,210
workers you can run as many as you want

336
00:11:36,210 --> 00:11:39,450
horizontally I include configurations in

337
00:11:39,450 --> 00:11:43,500
the repository for all this there's a

338
00:11:43,500 --> 00:11:46,070
unicorn which is just a WSGI worker

339
00:11:46,070 --> 00:11:49,020
configuration system d service files for

340
00:11:49,020 --> 00:11:51,690
all this a supervisor di and I file for

341
00:11:51,690 --> 00:11:54,000
running the broke a pea worker so all

342
00:11:54,000 --> 00:11:56,760
that's in the repository and there if

343
00:11:56,760 --> 00:11:58,500
you want to look at deployment in like a

344
00:11:58,500 --> 00:12:01,830
more consistent manner so here's a what

345
00:12:01,830 --> 00:12:04,830
a request would look like using curl you

346
00:12:04,830 --> 00:12:06,510
have a peak app called food out peak app

347
00:12:06,510 --> 00:12:09,150
and I want to tag this with the tag evil

348
00:12:09,150 --> 00:12:12,150
so you post it to Brooke happy with the

349
00:12:12,150 --> 00:12:13,980
file and then in the form date I would

350
00:12:13,980 --> 00:12:16,260
tag e equals evil and that JSON blob at

351
00:12:16,260 --> 00:12:17,550
the bottom is a return that you would

352
00:12:17,550 --> 00:12:20,580
get makuta job here's the files you gave

353
00:12:20,580 --> 00:12:23,040
me here's the tag you gave me here's the

354
00:12:23,040 --> 00:12:24,530
job you UID

355
00:12:24,530 --> 00:12:26,670
so when the worker runs this is how it

356
00:12:26,670 --> 00:12:28,740
stores it on local disk so you have the

357
00:12:28,740 --> 00:12:31,020
overall configured jobs directory then

358
00:12:31,020 --> 00:12:32,160
underneath that you're going to a bunch

359
00:12:32,160 --> 00:12:33,839
of folders that are named after the job

360
00:12:33,839 --> 00:12:36,000
you you IDs and underneath that you have

361
00:12:36,000 --> 00:12:38,190
two folders logs for the logs stored

362
00:12:38,190 --> 00:12:39,900
locally and then pcaps for all the piec

363
00:12:39,900 --> 00:12:42,900
apps that you gave it and so when I say

364
00:12:42,900 --> 00:12:44,850
tagging this is what I mean and what

365
00:12:44,850 --> 00:12:46,740
it's doing so the first line there is

366
00:12:46,740 --> 00:12:48,480
it's a slog header you've got the syslog

367
00:12:48,480 --> 00:12:51,120
priority number a timestamp and the

368
00:12:51,120 --> 00:12:53,339
highlighted piece is your tag normally

369
00:12:53,339 --> 00:12:55,440
what this is is the syslog post name

370
00:12:55,440 --> 00:12:57,690
field and since this is test data and

371
00:12:57,690 --> 00:12:59,220
it's not coming from an actual system

372
00:12:59,220 --> 00:13:01,680
it's a peek app that you submitted we

373
00:13:01,680 --> 00:13:03,120
don't really care what the syslog post

374
00:13:03,120 --> 00:13:04,800
name is so we can just take advantage of

375
00:13:04,800 --> 00:13:08,040
this and submit the tag here so for us

376
00:13:08,040 --> 00:13:09,990
what we parse that to is a field we call

377
00:13:09,990 --> 00:13:12,089
raw message hostname so if I submit a

378
00:13:12,089 --> 00:13:13,150
peek a pin

379
00:13:13,150 --> 00:13:15,220
I can then go back and say okay show me

380
00:13:15,220 --> 00:13:17,050
Ramos his host name equals evil and boom

381
00:13:17,050 --> 00:13:18,640
I've got all the logs that pcap I

382
00:13:18,640 --> 00:13:20,530
submitted and the reason why we did this

383
00:13:20,530 --> 00:13:21,790
is because it doesn't require any

384
00:13:21,790 --> 00:13:23,320
additional parsing you could do

385
00:13:23,320 --> 00:13:24,730
something like adding an additional bro

386
00:13:24,730 --> 00:13:27,130
field but for us since we work in like a

387
00:13:27,130 --> 00:13:28,840
pretty large product we wanted to take

388
00:13:28,840 --> 00:13:29,950
advantage of a field that was already

389
00:13:29,950 --> 00:13:31,840
par so in this case it's a vestigial

390
00:13:31,840 --> 00:13:33,400
field anyway because we really don't

391
00:13:33,400 --> 00:13:37,000
care what the Siuslaw host name was now

392
00:13:37,000 --> 00:13:38,260
what you can do with this and my tip

393
00:13:38,260 --> 00:13:40,180
here is that you can further subdivide

394
00:13:40,180 --> 00:13:43,360
your input by appending tags to your

395
00:13:43,360 --> 00:13:45,040
initial tag and we do this on client

396
00:13:45,040 --> 00:13:47,290
side so like a use case here would be

397
00:13:47,290 --> 00:13:49,990
say you've got X number of PCAST from

398
00:13:49,990 --> 00:13:51,580
some our family but it's a bunch of

399
00:13:51,580 --> 00:13:53,200
different variants and you want to be

400
00:13:53,200 --> 00:13:54,940
able to say yes I want to I want to tag

401
00:13:54,940 --> 00:13:56,740
all these with this Maur family but I

402
00:13:56,740 --> 00:13:58,420
want to be able to tell which pcap is

403
00:13:58,420 --> 00:14:01,210
which so like we have some client-side

404
00:14:01,210 --> 00:14:02,380
scripts because I haven't released

405
00:14:02,380 --> 00:14:04,770
because they're a bunch of like internal

406
00:14:04,770 --> 00:14:06,610
idiosyncrasies related to them but

407
00:14:06,610 --> 00:14:08,230
really all they are just wrappers around

408
00:14:08,230 --> 00:14:10,510
part of the Python request module so I

409
00:14:10,510 --> 00:14:12,340
go in and say I want to submit them

410
00:14:12,340 --> 00:14:15,010
Brooke happy with the tag evil 12p caps

411
00:14:15,010 --> 00:14:16,750
and then what the client does is it just

412
00:14:16,750 --> 00:14:19,900
up ends like for us it's like a six six

413
00:14:19,900 --> 00:14:22,600
letter hexadecimal value and then

414
00:14:22,600 --> 00:14:24,460
submits it so when I go in and I can say

415
00:14:24,460 --> 00:14:27,160
okay this is evil - 1 or evil - 2 or

416
00:14:27,160 --> 00:14:29,410
evil - 3 and so it's all subdivided by

417
00:14:29,410 --> 00:14:31,570
the P caps that you gave it and this is

418
00:14:31,570 --> 00:14:33,190
especially useful if you have the

419
00:14:33,190 --> 00:14:34,810
capability to prefix searching and your

420
00:14:34,810 --> 00:14:37,150
log store a storage platform so if I

421
00:14:37,150 --> 00:14:38,980
want to see an individual pcap I can go

422
00:14:38,980 --> 00:14:42,010
a ShowMe rama's host name evil - 1 or if

423
00:14:42,010 --> 00:14:43,570
I want to see all of them and look at

424
00:14:43,570 --> 00:14:44,890
them all at the same time and set it by

425
00:14:44,890 --> 00:14:46,750
an individual PK basis I just show me

426
00:14:46,750 --> 00:14:49,120
wrong with each host name evil star and

427
00:14:49,120 --> 00:14:51,220
it'll give you all the P caps you submit

428
00:14:51,220 --> 00:14:54,790
and not just like granular by one so

429
00:14:54,790 --> 00:14:56,070
this is a bunch of different use cases

430
00:14:56,070 --> 00:14:59,110
for us quick detection verification so

431
00:14:59,110 --> 00:15:00,910
even if it's us or another team or a

432
00:15:00,910 --> 00:15:02,620
sales engineer comes to us and they say

433
00:15:02,620 --> 00:15:05,920
hey I got this P cap of this malware do

434
00:15:05,920 --> 00:15:08,050
we detect this like well I just post the

435
00:15:08,050 --> 00:15:10,000
P cap to this API it'll show up in our

436
00:15:10,000 --> 00:15:11,770
test helix instance and it's either

437
00:15:11,770 --> 00:15:12,630
going to match something

438
00:15:12,630 --> 00:15:15,510
it's not so it's a quick way one post

439
00:15:15,510 --> 00:15:18,000
requests logs come in detection matches

440
00:15:18,000 --> 00:15:19,830
against it shows up we can search it and

441
00:15:19,830 --> 00:15:22,080
that's all that we had to do gives you

442
00:15:22,080 --> 00:15:23,700
rock reliable and reproducible test

443
00:15:23,700 --> 00:15:24,900
production because you're doing it all

444
00:15:24,900 --> 00:15:26,730
in one place since your analysts are

445
00:15:26,730 --> 00:15:28,320
using the same API with the same broken

446
00:15:28,320 --> 00:15:29,580
figuration you're gonna get the same

447
00:15:29,580 --> 00:15:31,050
result every single time

448
00:15:31,050 --> 00:15:34,950
and because it has you can give it a

449
00:15:34,950 --> 00:15:37,110
unique input which is then tagged on the

450
00:15:37,110 --> 00:15:39,030
output you can use as a building block

451
00:15:39,030 --> 00:15:40,590
and further automation so like a use

452
00:15:40,590 --> 00:15:42,600
case here would be scraped a bunch of

453
00:15:42,600 --> 00:15:44,640
PCAST from an RSS feeds say like malware

454
00:15:44,640 --> 00:15:47,250
traffic analysis net automatically post

455
00:15:47,250 --> 00:15:48,690
some water broke happy with some kind of

456
00:15:48,690 --> 00:15:50,430
derived tag it stores in the backend

457
00:15:50,430 --> 00:15:52,740
then whatever your automatic job is can

458
00:15:52,740 --> 00:15:54,330
go and search for those results and do

459
00:15:54,330 --> 00:15:56,730
things like build reports so you can

460
00:15:56,730 --> 00:15:58,740
automatically just point it at your RSS

461
00:15:58,740 --> 00:16:01,410
feed and every time a pcap comes in what

462
00:16:01,410 --> 00:16:03,300
logs do we see or what logs is broke let

463
00:16:03,300 --> 00:16:05,190
out do we have any detection on this and

464
00:16:05,190 --> 00:16:08,640
this is why the ingestion detection what

465
00:16:08,640 --> 00:16:09,750
I talked about earlier is kind of

466
00:16:09,750 --> 00:16:11,070
important because if you have it as

467
00:16:11,070 --> 00:16:12,720
searchable metadata you don't have to go

468
00:16:12,720 --> 00:16:14,490
calling on another alert database to say

469
00:16:14,490 --> 00:16:17,580
okay is this login alert it's all just

470
00:16:17,580 --> 00:16:19,650
metadata there and the raw in itself and

471
00:16:19,650 --> 00:16:21,690
even more useful than that you can you

472
00:16:21,690 --> 00:16:24,150
can say okay what out of this pcap

473
00:16:24,150 --> 00:16:25,980
didn't match anything so you can do the

474
00:16:25,980 --> 00:16:28,200
reverse whereas if it was all stored in

475
00:16:28,200 --> 00:16:29,760
an alert database you'd say okay here's

476
00:16:29,760 --> 00:16:31,290
the logs on alert database now you have

477
00:16:31,290 --> 00:16:32,640
to do it diff against what's actually

478
00:16:32,640 --> 00:16:37,050
what all actually came in so we've done

479
00:16:37,050 --> 00:16:40,230
that automated away some of the pain

480
00:16:40,230 --> 00:16:41,760
points for network logs let's do the

481
00:16:41,760 --> 00:16:44,520
same thing when des looks so post logs

482
00:16:44,520 --> 00:16:46,380
in general just more of a pain they

483
00:16:46,380 --> 00:16:48,810
require more initial setup for bro I

484
00:16:48,810 --> 00:16:50,580
mean you just app kit or you'll install

485
00:16:50,580 --> 00:16:52,170
bro and run peak apps through and boom

486
00:16:52,170 --> 00:16:54,210
you got logs with a with Windows logs

487
00:16:54,210 --> 00:16:56,490
you have to actually set up a VM set up

488
00:16:56,490 --> 00:16:57,600
some kind of log forwarding

489
00:16:57,600 --> 00:16:59,400
configuration you have a bunch of group

490
00:16:59,400 --> 00:17:01,680
policy settings to get all the log event

491
00:17:01,680 --> 00:17:03,240
logs that you want or installing system

492
00:17:03,240 --> 00:17:05,490
on or something like that just generally

493
00:17:05,490 --> 00:17:06,859
a lot more overhead and maintaining

494
00:17:06,859 --> 00:17:08,270
and there's a lot more options for

495
00:17:08,270 --> 00:17:11,959
configuration and delivery just my two

496
00:17:11,959 --> 00:17:14,359
cents on the host configuration itself

497
00:17:14,359 --> 00:17:15,890
I mentioned cess long before and I'm

498
00:17:15,890 --> 00:17:17,089
kind of beating a dead horse at this

499
00:17:17,089 --> 00:17:19,699
point but this one is great in the same

500
00:17:19,699 --> 00:17:21,049
reason that bro is great it gives you a

501
00:17:21,049 --> 00:17:23,150
ton of data and again even if you're not

502
00:17:23,150 --> 00:17:25,280
using it internally if you're just

503
00:17:25,280 --> 00:17:27,740
relying on like 40 Levin ID 46 88

504
00:17:27,740 --> 00:17:29,750
process execution events even if you're

505
00:17:29,750 --> 00:17:32,540
generating system all in part of your

506
00:17:32,540 --> 00:17:34,580
test events as long as the image field

507
00:17:34,580 --> 00:17:37,130
from a system own event maps to the you

508
00:17:37,130 --> 00:17:39,590
know process field for 40 688 events it

509
00:17:39,590 --> 00:17:40,460
doesn't really matter what was

510
00:17:40,460 --> 00:17:41,840
generating it as long as the parsing

511
00:17:41,840 --> 00:17:45,049
matches up I suggest to use NX logs

512
00:17:45,049 --> 00:17:46,910
ascender for a very specific reason that

513
00:17:46,910 --> 00:17:47,990
I'm going to talk about in a slide or

514
00:17:47,990 --> 00:17:50,330
two but other than that they got a free

515
00:17:50,330 --> 00:17:53,419
Community Edition it's easy to set up it

516
00:17:53,419 --> 00:17:55,040
basically just works it's easy to

517
00:17:55,040 --> 00:17:57,140
configure the third piece to send your

518
00:17:57,140 --> 00:17:59,419
logs in JSON format and a lot of cases

519
00:17:59,419 --> 00:18:00,650
especially with Windows events you're

520
00:18:00,650 --> 00:18:01,460
going to get a little bit higher

521
00:18:01,460 --> 00:18:03,140
velocity and things you might not see

522
00:18:03,140 --> 00:18:05,030
and tab-delimited just normal syslog

523
00:18:05,030 --> 00:18:07,130
events and the other reason is you're

524
00:18:07,130 --> 00:18:08,450
not a parsing masochist

525
00:18:08,450 --> 00:18:10,160
nobody loves parsing tab-delimited

526
00:18:10,160 --> 00:18:13,400
syslog fields it just sucks and nobody's

527
00:18:13,400 --> 00:18:15,290
really gonna turn down pre-sterilized

528
00:18:15,290 --> 00:18:17,900
theta so let's automate some of this

529
00:18:17,900 --> 00:18:23,330
away using kuku so we wrote this module

530
00:18:23,330 --> 00:18:25,340
that we like to call tag host and what

531
00:18:25,340 --> 00:18:27,590
it is is the windows auxilary module

532
00:18:27,590 --> 00:18:31,040
that it requires NX log to be installed

533
00:18:31,040 --> 00:18:32,360
which is why I mentioned it a second ago

534
00:18:32,360 --> 00:18:34,160
and what its gonna do is it's got a

535
00:18:34,160 --> 00:18:36,770
dummy valuable or variable in the system

536
00:18:36,770 --> 00:18:39,470
or in the annex log config so it's going

537
00:18:39,470 --> 00:18:41,570
to take a predetermined tag same way bro

538
00:18:41,570 --> 00:18:43,280
copy did so you're gonna say okay run

539
00:18:43,280 --> 00:18:45,049
this analysis this is the tag that I

540
00:18:45,049 --> 00:18:46,910
want to come from the event logs it's

541
00:18:46,910 --> 00:18:49,190
going to rewrite the dummy variable in

542
00:18:49,190 --> 00:18:51,110
the annex log config with whatever tag

543
00:18:51,110 --> 00:18:53,600
you provided it restart NX log and now

544
00:18:53,600 --> 00:18:55,850
what happens is any new event coming out

545
00:18:55,850 --> 00:18:57,470
of that system is going to have a host

546
00:18:57,470 --> 00:18:59,720
name of whatever the tag it was you gave

547
00:18:59,720 --> 00:19:03,020
it so this way you can run a sample and

548
00:19:03,020 --> 00:19:05,660
cuckoo give it a tag if you have event

549
00:19:05,660 --> 00:19:07,700
forwarding set up out of your out of

550
00:19:07,700 --> 00:19:09,830
your cuckoo guests they'll automatically

551
00:19:09,830 --> 00:19:11,330
send it to whatever sender you

552
00:19:11,330 --> 00:19:12,890
configured it to and if you have that

553
00:19:12,890 --> 00:19:14,750
automatically going under in your sim

554
00:19:14,750 --> 00:19:17,570
you're gonna have tagged by hostname for

555
00:19:17,570 --> 00:19:19,999
each and houses wrong

556
00:19:19,999 --> 00:19:22,889
so here's an example of just a simple

557
00:19:22,889 --> 00:19:24,659
API request it looks identical to the

558
00:19:24,659 --> 00:19:28,109
Libero copy one your file just called

559
00:19:28,109 --> 00:19:31,019
file and then the way this works with

560
00:19:31,019 --> 00:19:33,720
tag hosts is cuckoo has what's called an

561
00:19:33,720 --> 00:19:36,359
options field for every analysis run and

562
00:19:36,359 --> 00:19:38,519
so what it's going to look for is this

563
00:19:38,519 --> 00:19:41,700
tag underscore host key and the value

564
00:19:41,700 --> 00:19:43,109
they give it is going to be whatever tag

565
00:19:43,109 --> 00:19:45,389
you want to your analysis from to be

566
00:19:45,389 --> 00:19:47,940
tagged with and so for us the host name

567
00:19:47,940 --> 00:19:49,590
field Cummings and comes in and windows

568
00:19:49,590 --> 00:19:51,119
logs this parcel field just called host

569
00:19:51,119 --> 00:19:53,220
name so if I tag it with the tag evil

570
00:19:53,220 --> 00:19:54,869
and I want to go in and say okay show me

571
00:19:54,869 --> 00:19:56,460
all the logs from this analysis wrong

572
00:19:56,460 --> 00:19:58,320
you say host name equals evil and boom

573
00:19:58,320 --> 00:20:00,090
I've got all the windows logs from that

574
00:20:00,090 --> 00:20:03,840
particular cuckoo run the use cases here

575
00:20:03,840 --> 00:20:06,149
basically the same as where Kathy quick

576
00:20:06,149 --> 00:20:07,799
detection verification you can go from

577
00:20:07,799 --> 00:20:10,109
sample to Windows without logs in your

578
00:20:10,109 --> 00:20:13,429
sim quickly reproducible real-life

579
00:20:13,429 --> 00:20:15,299
reproducible and reliable test

580
00:20:15,299 --> 00:20:16,769
production because these analyst isn't

581
00:20:16,769 --> 00:20:18,599
running their own Windows VM with the

582
00:20:18,599 --> 00:20:20,489
log forwarding setup you just set up a

583
00:20:20,489 --> 00:20:22,049
bunch of VMS and cuckoo with this they

584
00:20:22,049 --> 00:20:23,749
can boom they can get it back logs out

585
00:20:23,749 --> 00:20:26,039
and building block for further

586
00:20:26,039 --> 00:20:27,599
automation the same way with Brooke

587
00:20:27,599 --> 00:20:29,849
Kathy get a feed of samples coming in

588
00:20:29,849 --> 00:20:33,359
can tag them run through stored in your

589
00:20:33,359 --> 00:20:35,279
sim since you control the tag and it's

590
00:20:35,279 --> 00:20:36,779
tagged on output you can automatically

591
00:20:36,779 --> 00:20:38,099
search for that and do all kinds of

592
00:20:38,099 --> 00:20:40,549
whatever you want to do after that

593
00:20:40,549 --> 00:20:43,799
there's a quick tip here let's say you

594
00:20:43,799 --> 00:20:44,940
have a bunch of commands for like an

595
00:20:44,940 --> 00:20:48,389
iReport so attacker got in the box and

596
00:20:48,389 --> 00:20:50,460
ran these twelve commands could spin up

597
00:20:50,460 --> 00:20:52,200
spin up a VM and remain only run these

598
00:20:52,200 --> 00:20:53,820
all themselves or run all these commands

599
00:20:53,820 --> 00:20:55,799
themselves or toss them on a batch

600
00:20:55,799 --> 00:20:58,049
script and throw them a cuckoo it's

601
00:20:58,049 --> 00:20:59,609
going to go through sq the batch strip

602
00:20:59,609 --> 00:21:02,039
run into those commands in sequence and

603
00:21:02,039 --> 00:21:03,239
you're gonna get event logs just like

604
00:21:03,239 --> 00:21:04,409
somebody was on the Box round those

605
00:21:04,409 --> 00:21:08,879
commands themselves so aside from the

606
00:21:08,879 --> 00:21:10,710
tools we released today how else do we

607
00:21:10,710 --> 00:21:15,239
make our lives easier by sharing so this

608
00:21:15,239 --> 00:21:16,919
is in the works this isn't being

609
00:21:16,919 --> 00:21:18,599
released yet that we're working towards

610
00:21:18,599 --> 00:21:19,739
a release just going through some

611
00:21:19,739 --> 00:21:21,149
internal processes to figure out the

612
00:21:21,149 --> 00:21:21,650
best way to

613
00:21:21,650 --> 00:21:25,160
so for our team we've wrote a ton of

614
00:21:25,160 --> 00:21:26,990
what we call windows methodology rules

615
00:21:26,990 --> 00:21:28,520
and basically it's just like general

616
00:21:28,520 --> 00:21:30,590
windows behavior from all over the board

617
00:21:30,590 --> 00:21:32,450
if it's like you know process

618
00:21:32,450 --> 00:21:34,490
relationships like this is an event log

619
00:21:34,490 --> 00:21:37,520
from when PowerShell gets called from an

620
00:21:37,520 --> 00:21:39,830
office process or when someone deletes

621
00:21:39,830 --> 00:21:41,480
prefetched sheet down to like things

622
00:21:41,480 --> 00:21:43,940
like just normal operational behavior

623
00:21:43,940 --> 00:21:45,530
like adding someone to an act like a

624
00:21:45,530 --> 00:21:49,520
privilege group or you know anything you

625
00:21:49,520 --> 00:21:51,650
can think of so anything that we had a

626
00:21:51,650 --> 00:21:53,630
public reference for if it was based on

627
00:21:53,630 --> 00:21:56,179
a blog but based on a tweet or based on

628
00:21:56,179 --> 00:21:58,010
a presentation we include all that as

629
00:21:58,010 --> 00:21:59,990
metadata and our rules so what I wanted

630
00:21:59,990 --> 00:22:01,660
to do was take all of these and

631
00:22:01,660 --> 00:22:04,970
basically publish the test events for

632
00:22:04,970 --> 00:22:07,790
them not the rules but so what and what

633
00:22:07,790 --> 00:22:09,650
I think we should build is this public

634
00:22:09,650 --> 00:22:11,990
reference of what a Windows Event log

635
00:22:11,990 --> 00:22:14,720
looks like when something happens so

636
00:22:14,720 --> 00:22:16,070
you've got things like ultimate Windows

637
00:22:16,070 --> 00:22:18,110
security which is like basically lays

638
00:22:18,110 --> 00:22:20,450
out what each of NID looks like but

639
00:22:20,450 --> 00:22:21,800
there's nowhere to say okay I just

640
00:22:21,800 --> 00:22:23,990
here's the test data I can spit this

641
00:22:23,990 --> 00:22:26,720
into my sim it's you know a recreation

642
00:22:26,720 --> 00:22:28,010
of what it would look like in production

643
00:22:28,010 --> 00:22:29,390
basically you'd have to go and do it

644
00:22:29,390 --> 00:22:29,809
yourself

645
00:22:29,809 --> 00:22:32,660
so there's no reason if especially for

646
00:22:32,660 --> 00:22:34,130
things like operational stuff is like

647
00:22:34,130 --> 00:22:36,050
okay what it was the actual Windows

648
00:22:36,050 --> 00:22:39,470
Event look like for you know a failed

649
00:22:39,470 --> 00:22:43,429
login to a non-existent account so

650
00:22:43,429 --> 00:22:45,110
there's no reason we can share that kind

651
00:22:45,110 --> 00:22:46,490
of things especially if like a bunch of

652
00:22:46,490 --> 00:22:48,410
us are reproducing the same stuff so

653
00:22:48,410 --> 00:22:50,450
what we've done is basically taken all

654
00:22:50,450 --> 00:22:53,720
those positive tests and organized in my

655
00:22:53,720 --> 00:22:55,790
directory and the directories of topic

656
00:22:55,790 --> 00:22:57,890
directories whether it be like lateral

657
00:22:57,890 --> 00:23:01,460
movement or privilege escalation or you

658
00:23:01,460 --> 00:23:02,929
know PowerShell related things or

659
00:23:02,929 --> 00:23:04,910
process relationships basically just

660
00:23:04,910 --> 00:23:08,630
building this corpus of you know example

661
00:23:08,630 --> 00:23:11,120
windows events and like I said I'm still

662
00:23:11,120 --> 00:23:12,230
working on getting this published

663
00:23:12,230 --> 00:23:14,630
hopefully I'll have something in the

664
00:23:14,630 --> 00:23:17,360
next week it probably will end up on the

665
00:23:17,360 --> 00:23:19,730
fireEye github as well if it doesn't

666
00:23:19,730 --> 00:23:21,050
we'll figure out somewhere else for it

667
00:23:21,050 --> 00:23:23,660
to live when it does come out we're

668
00:23:23,660 --> 00:23:25,130
gonna hopefully continue releasing

669
00:23:25,130 --> 00:23:26,870
things as we find them if we see tweets

670
00:23:26,870 --> 00:23:28,429
it's like okay that's interesting if

671
00:23:28,429 --> 00:23:30,920
John Lambert tweets out you know maybe

672
00:23:30,920 --> 00:23:32,990
you guys should be watching for long at

673
00:23:32,990 --> 00:23:33,290
the

674
00:23:33,290 --> 00:23:35,540
path directories okay well we'll create

675
00:23:35,540 --> 00:23:37,640
a witness for that stick it add it as a

676
00:23:37,640 --> 00:23:39,230
positive test for whatever a rule map we

677
00:23:39,230 --> 00:23:40,520
might create and then since it's based

678
00:23:40,520 --> 00:23:43,070
on public reference information just

679
00:23:43,070 --> 00:23:46,460
publish it to the public github this is

680
00:23:46,460 --> 00:23:47,660
just an example of some of the stuff

681
00:23:47,660 --> 00:23:50,750
that would be released in this there's

682
00:23:50,750 --> 00:23:52,130
about a hundred of them right now I

683
00:23:52,130 --> 00:23:53,540
probably can go through and find a bunch

684
00:23:53,540 --> 00:23:56,030
of other ones for like but don't

685
00:23:56,030 --> 00:23:57,560
necessarily have public references right

686
00:23:57,560 --> 00:24:02,300
now but I'm sure they exist yeah thanks

687
00:24:02,300 --> 00:24:03,860
to all these guys these guys either

688
00:24:03,860 --> 00:24:05,600
worked on our team at one point or still

689
00:24:05,600 --> 00:24:07,700
work on our team and either generated

690
00:24:07,700 --> 00:24:08,900
some of those reference events I was

691
00:24:08,900 --> 00:24:11,210
just talking about or helped me get to

692
00:24:11,210 --> 00:24:12,590
develop some of the tools I was talking

693
00:24:12,590 --> 00:24:14,870
about earlier so yeah thank you these

694
00:24:14,870 --> 00:24:16,010
guys most of which I think are here at

695
00:24:16,010 --> 00:24:19,190
the conference except for Patrick so

696
00:24:19,190 --> 00:24:21,080
with that my slides are at this link

697
00:24:21,080 --> 00:24:24,800
they're just HTML so it works straight

698
00:24:24,800 --> 00:24:26,450
out the side or you can just clone them

699
00:24:26,450 --> 00:24:28,460
a repo or ask me questions or whatever

700
00:24:28,460 --> 00:24:32,890
so with that I guess have any questions

