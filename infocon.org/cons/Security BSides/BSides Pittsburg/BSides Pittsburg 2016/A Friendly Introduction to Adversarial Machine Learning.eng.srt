1
00:00:00,000 --> 00:00:02,879
I'd like to introduce everyone to Evan

2
00:00:02,879 --> 00:00:04,710
Wright if you don't know him already

3
00:00:04,710 --> 00:00:07,470
Evans a bit of a rare bird he's an

4
00:00:07,470 --> 00:00:09,750
expert in networking as well as an

5
00:00:09,750 --> 00:00:13,500
expert in machine learning which I come

6
00:00:13,500 --> 00:00:15,870
to find is a pretty rare combination so

7
00:00:15,870 --> 00:00:19,410
I guarantee that if you pay attention to

8
00:00:19,410 --> 00:00:21,529
this you will learn something today

9
00:00:21,529 --> 00:00:30,480
Meister thank you guys for having me so

10
00:00:30,480 --> 00:00:33,420
I am coming to you guys today from a

11
00:00:33,420 --> 00:00:36,090
company called anomaly and basically we

12
00:00:36,090 --> 00:00:37,800
are a threat intelligence platform

13
00:00:37,800 --> 00:00:40,079
company that tries to integrate threat

14
00:00:40,079 --> 00:00:42,930
intelligence with various cybersecurity

15
00:00:42,930 --> 00:00:45,210
devices like Sims and that type of thing

16
00:00:45,210 --> 00:00:47,129
so we're really interested in sort of

17
00:00:47,129 --> 00:00:49,710
the data reduction problem of helping

18
00:00:49,710 --> 00:00:51,600
you find useful things and so a

19
00:00:51,600 --> 00:00:53,789
machine-learning comes up a lot in the

20
00:00:53,789 --> 00:00:56,280
type of work that I do especially trying

21
00:00:56,280 --> 00:00:59,250
to kind of whittle down data sources so

22
00:00:59,250 --> 00:01:02,969
I went to RSA and everybody in security

23
00:01:02,969 --> 00:01:04,589
is kind of advertising we do machine

24
00:01:04,589 --> 00:01:06,960
learning this or that so I thought it'd

25
00:01:06,960 --> 00:01:10,049
be good to talk a little bit about about

26
00:01:10,049 --> 00:01:12,720
this state of machine learning today and

27
00:01:12,720 --> 00:01:14,729
so it seems like there's two camps

28
00:01:14,729 --> 00:01:17,009
there's basically the camp on the Left

29
00:01:17,009 --> 00:01:18,960
which is you know machine learning is

30
00:01:18,960 --> 00:01:21,659
great right it'll help us like help our

31
00:01:21,659 --> 00:01:24,030
old people get assistance right where

32
00:01:24,030 --> 00:01:25,530
they otherwise wouldn't have been able

33
00:01:25,530 --> 00:01:29,340
to afford care to keep them doing well

34
00:01:29,340 --> 00:01:31,790
right so kind of machine learning and

35
00:01:31,790 --> 00:01:34,530
applied to the physical space of using

36
00:01:34,530 --> 00:01:36,750
robots so then there's that great side

37
00:01:36,750 --> 00:01:38,670
which is kind of the left and then

38
00:01:38,670 --> 00:01:40,619
there's like that they're the optimists

39
00:01:40,619 --> 00:01:41,250
on the left

40
00:01:41,250 --> 00:01:44,549
but then the other contrast is like you

41
00:01:44,549 --> 00:01:46,380
could call them pessimists or you could

42
00:01:46,380 --> 00:01:48,689
call them like the security community or

43
00:01:48,689 --> 00:01:51,869
the privacy community but right it's

44
00:01:51,869 --> 00:01:54,840
like wow killer robots and like cyber

45
00:01:54,840 --> 00:01:57,000
mafia and all these types of things so

46
00:01:57,000 --> 00:01:59,250
this is the usual question that gets

47
00:01:59,250 --> 00:02:03,270
discussed is like increased robot sort

48
00:02:03,270 --> 00:02:05,159
of intelligence if you will is it a good

49
00:02:05,159 --> 00:02:07,409
thing or a bad thing and I don't think

50
00:02:07,409 --> 00:02:10,500
that's a real productive conversation to

51
00:02:10,500 --> 00:02:11,610
the most part because everyone's already

52
00:02:11,610 --> 00:02:13,680
having it so there's a

53
00:02:13,680 --> 00:02:16,109
for a conversation that I'd like to talk

54
00:02:16,109 --> 00:02:19,379
about and that's when so starting to

55
00:02:19,379 --> 00:02:21,989
think more about when it's the robots

56
00:02:21,989 --> 00:02:25,319
versus the robots right so we're

57
00:02:25,319 --> 00:02:28,590
thinking about you know in sort of a

58
00:02:28,590 --> 00:02:30,900
cyber war when basically the machines

59
00:02:30,900 --> 00:02:33,599
are helping us with a lot of automation

60
00:02:33,599 --> 00:02:35,670
so they're taking a piece of it but then

61
00:02:35,670 --> 00:02:39,209
we've got the bad guys trying to

62
00:02:39,209 --> 00:02:41,129
outsmart our machines to catch them

63
00:02:41,129 --> 00:02:43,170
right so they're then using machine

64
00:02:43,170 --> 00:02:45,720
learning outsmart our machines that are

65
00:02:45,720 --> 00:02:47,760
trying to catch them so that's kind of

66
00:02:47,760 --> 00:02:53,730
the the general idea so that's um so

67
00:02:53,730 --> 00:02:55,500
here's the outline for the talk first

68
00:02:55,500 --> 00:02:57,750
I'm going to go over common problems of

69
00:02:57,750 --> 00:02:59,340
how we're using machine learning in

70
00:02:59,340 --> 00:03:02,219
society in general today then I'm gonna

71
00:03:02,219 --> 00:03:03,870
go into what used machine learning

72
00:03:03,870 --> 00:03:05,819
because unless you find it relevant you

73
00:03:05,819 --> 00:03:07,519
don't care what it is at least I don't

74
00:03:07,519 --> 00:03:09,659
then I'll start talking about this thing

75
00:03:09,659 --> 00:03:11,400
called adversarial machine learning

76
00:03:11,400 --> 00:03:14,010
which is really the focus of the talk

77
00:03:14,010 --> 00:03:16,379
today getting into some demonstrations

78
00:03:16,379 --> 00:03:20,010
about what of use cases of adversarial

79
00:03:20,010 --> 00:03:21,060
machine learning that have already

80
00:03:21,060 --> 00:03:26,819
existed some real-world examples of bad

81
00:03:26,819 --> 00:03:30,120
guys avoiding security detection

82
00:03:30,120 --> 00:03:33,049
mechanisms and then we'll get into

83
00:03:33,049 --> 00:03:35,129
adversarial machine learning defenses

84
00:03:35,129 --> 00:03:37,639
and then a couple destructive

85
00:03:37,639 --> 00:03:39,689
adversarial machine learning use cases

86
00:03:39,689 --> 00:03:42,269
right so machine learning everyone talks

87
00:03:42,269 --> 00:03:44,609
about it and it's kind of an exciting

88
00:03:44,609 --> 00:03:46,590
thing I mean a lot of people want a

89
00:03:46,590 --> 00:03:48,780
self-driving car I grew up in Detroit

90
00:03:48,780 --> 00:03:50,849
and I was like way too much of a

91
00:03:50,849 --> 00:03:52,680
computer guy to appreciate the car

92
00:03:52,680 --> 00:03:54,870
obsessed culture up there so I don't

93
00:03:54,870 --> 00:03:56,220
know about you guys but I'd be more than

94
00:03:56,220 --> 00:03:57,900
happy to give up my car and get driven

95
00:03:57,900 --> 00:04:02,359
around so that was very appealing to me

96
00:04:02,359 --> 00:04:04,560
but there's lots of other much more

97
00:04:04,560 --> 00:04:06,569
pragmatic ways that it may appeal to

98
00:04:06,569 --> 00:04:09,169
people right it's great having our

99
00:04:09,169 --> 00:04:12,389
recommendation engines on Netflix it's

100
00:04:12,389 --> 00:04:14,400
great having our recommendation engines

101
00:04:14,400 --> 00:04:18,449
on Pandora as well as online shopping

102
00:04:18,449 --> 00:04:22,099
carts what we're seeing more recently is

103
00:04:22,099 --> 00:04:24,990
medical diagnosis starting to get used

104
00:04:24,990 --> 00:04:26,760
there's company called modernizing Metis

105
00:04:26,760 --> 00:04:28,230
that's already starting to do some of

106
00:04:28,230 --> 00:04:30,120
this there's a few others as well

107
00:04:30,120 --> 00:04:32,790
diagnosing diseases based on your

108
00:04:32,790 --> 00:04:35,970
symptoms by looking up kind of pattern

109
00:04:35,970 --> 00:04:37,770
and pattern detection Watson gained a

110
00:04:37,770 --> 00:04:40,410
lot of interest in this space of looking

111
00:04:40,410 --> 00:04:43,290
through large structured corpora of like

112
00:04:43,290 --> 00:04:45,900
this basically Watson learned Wikipedia

113
00:04:45,900 --> 00:04:48,380
and it turns out there's a lot of useful

114
00:04:48,380 --> 00:04:50,670
things that you can do when you have a

115
00:04:50,670 --> 00:04:52,350
robot that can understand your language

116
00:04:52,350 --> 00:04:55,590
as well as used for customer service

117
00:04:55,590 --> 00:04:57,480
BOTS as well because with customer

118
00:04:57,480 --> 00:05:00,210
service if companies can automate

119
00:05:00,210 --> 00:05:05,580
customer service then companies can do

120
00:05:05,580 --> 00:05:07,320
customer service more cheaply in most

121
00:05:07,320 --> 00:05:08,820
cases and then if they can make it

122
00:05:08,820 --> 00:05:10,950
usable and friendly then their customers

123
00:05:10,950 --> 00:05:13,800
won't get angry at them so how are we

124
00:05:13,800 --> 00:05:15,990
using machine learning in security so

125
00:05:15,990 --> 00:05:17,670
the past few years there's been

126
00:05:17,670 --> 00:05:19,200
definitely an increase in this I

127
00:05:19,200 --> 00:05:21,750
mentioned RSA just being kind of swamped

128
00:05:21,750 --> 00:05:25,200
with all sorts of strong claims of you

129
00:05:25,200 --> 00:05:27,180
know machine learning I saw some people

130
00:05:27,180 --> 00:05:29,070
claiming zero false positives which is a

131
00:05:29,070 --> 00:05:31,140
great way to get data scientists in

132
00:05:31,140 --> 00:05:36,270
security to not respect you so I don't

133
00:05:36,270 --> 00:05:38,520
claim that but I do not respect people

134
00:05:38,520 --> 00:05:41,010
that to give that so a few use cases of

135
00:05:41,010 --> 00:05:43,050
it is in malware classification to help

136
00:05:43,050 --> 00:05:45,960
separate family one from family two and

137
00:05:45,960 --> 00:05:48,360
malware this is sort of a large long

138
00:05:48,360 --> 00:05:52,170
term effort because obviously in

139
00:05:52,170 --> 00:05:55,830
security what differs us as a security

140
00:05:55,830 --> 00:05:58,290
community from the community of like

141
00:05:58,290 --> 00:06:02,010
let's say biology is in biology when

142
00:06:02,010 --> 00:06:04,050
they're trying to detect viruses you

143
00:06:04,050 --> 00:06:05,790
know morphing or whatever those viruses

144
00:06:05,790 --> 00:06:08,250
are not reading the published papers and

145
00:06:08,250 --> 00:06:11,100
reports that describe how we found them

146
00:06:11,100 --> 00:06:13,050
and how we're stopping them so there's a

147
00:06:13,050 --> 00:06:15,720
big difference between security and just

148
00:06:15,720 --> 00:06:17,130
about all these other domains that we

149
00:06:17,130 --> 00:06:18,930
have active humans trying to evade us

150
00:06:18,930 --> 00:06:21,690
and so that fundamentally changes how we

151
00:06:21,690 --> 00:06:24,500
need to address some of this because

152
00:06:24,500 --> 00:06:27,210
machine learning was really designed for

153
00:06:27,210 --> 00:06:29,970
situations where you can work in

154
00:06:29,970 --> 00:06:32,970
isolation study some datasets and then

155
00:06:32,970 --> 00:06:35,340
create your sort of predictive model it

156
00:06:35,340 --> 00:06:36,720
was not really designed with the

157
00:06:36,720 --> 00:06:39,120
expectation having an adversary looking

158
00:06:39,120 --> 00:06:40,420
over your shoulder

159
00:06:40,420 --> 00:06:42,070
figuring out what you're figuring out as

160
00:06:42,070 --> 00:06:44,290
well so malware cost equations one

161
00:06:44,290 --> 00:06:46,060
example domain generation algorithm

162
00:06:46,060 --> 00:06:47,890
detection is another example I can

163
00:06:47,890 --> 00:06:49,440
revisit that briefly a little more later

164
00:06:49,440 --> 00:06:54,730
as well as fast flux domains keystroke

165
00:06:54,730 --> 00:06:56,530
logging to predict user behavior is kind

166
00:06:56,530 --> 00:06:58,570
of neat so I've seen a few different

167
00:06:58,570 --> 00:07:01,840
papers in this space we're like no

168
00:07:01,840 --> 00:07:05,080
kidding monitoring network traffic an

169
00:07:05,080 --> 00:07:07,420
interactive like SSH session writes and

170
00:07:07,420 --> 00:07:09,820
Krypton tunnel you can't see inside but

171
00:07:09,820 --> 00:07:12,730
by using the temporal characteristics of

172
00:07:12,730 --> 00:07:14,500
like you hit a key and then it sends

173
00:07:14,500 --> 00:07:16,720
sends a few bytes over the network even

174
00:07:16,720 --> 00:07:18,610
though it's encrypted by looking at the

175
00:07:18,610 --> 00:07:20,440
temporal characteristics of how fast

176
00:07:20,440 --> 00:07:22,480
you're typing things you can start to

177
00:07:22,480 --> 00:07:25,900
model what what words people are typing

178
00:07:25,900 --> 00:07:28,090
mostly because a keyboard distance and

179
00:07:28,090 --> 00:07:30,550
how people learn some of these things so

180
00:07:30,550 --> 00:07:34,420
and then finally another encrypted

181
00:07:34,420 --> 00:07:36,460
traffic related thing is clustering the

182
00:07:36,460 --> 00:07:38,680
behavior of traffic right so this isn't

183
00:07:38,680 --> 00:07:40,330
like it can read your email when it's

184
00:07:40,330 --> 00:07:42,250
encrypted this is just like we can

185
00:07:42,250 --> 00:07:44,440
distinguish like email versus an SSH

186
00:07:44,440 --> 00:07:45,900
session or something like that

187
00:07:45,900 --> 00:07:48,280
nonetheless whenever you can defeat any

188
00:07:48,280 --> 00:07:50,230
sort of encryption that always kind of

189
00:07:50,230 --> 00:07:53,350
catches the security community's here so

190
00:07:53,350 --> 00:07:55,510
if I'm going to talk about machine

191
00:07:55,510 --> 00:07:57,880
learning I should introduce it a little

192
00:07:57,880 --> 00:08:00,040
bit this is really designed to be

193
00:08:00,040 --> 00:08:03,070
extremely general so and I'm really kind

194
00:08:03,070 --> 00:08:04,540
of expecting most of you guys to like

195
00:08:04,540 --> 00:08:06,240
yeah I've heard it a few times but I

196
00:08:06,240 --> 00:08:09,030
don't know too much under the hood so

197
00:08:09,030 --> 00:08:11,560
actually let me just ask like how many

198
00:08:11,560 --> 00:08:14,350
people in the audience are like when

199
00:08:14,350 --> 00:08:15,580
they think when they hear machine

200
00:08:15,580 --> 00:08:17,380
learning it's more than just a black box

201
00:08:17,380 --> 00:08:20,190
to them like anyone worked on it before

202
00:08:20,190 --> 00:08:21,340
okay

203
00:08:21,340 --> 00:08:23,020
decent community probably largely

204
00:08:23,020 --> 00:08:26,230
overlapping with a regression pull a few

205
00:08:26,230 --> 00:08:29,110
talks ago so anyway for the rest of us

206
00:08:29,110 --> 00:08:31,120
this is just a real high-level thing

207
00:08:31,120 --> 00:08:33,549
right so you may have imagine you've got

208
00:08:33,549 --> 00:08:36,520
like a CSV file maybe we're doing spam

209
00:08:36,520 --> 00:08:38,950
spam is really the quintessential the

210
00:08:38,950 --> 00:08:41,020
quintessential intersection of machine

211
00:08:41,020 --> 00:08:42,789
learning and security so maybe we're

212
00:08:42,789 --> 00:08:44,500
just taking like the length of the email

213
00:08:44,500 --> 00:08:46,780
message counting that the number of

214
00:08:46,780 --> 00:08:48,550
recipients that the email message went

215
00:08:48,550 --> 00:08:51,820
to and then just based on these simple

216
00:08:51,820 --> 00:08:53,830
things in a CSV file

217
00:08:53,830 --> 00:08:55,600
got a few labeled examples of what's

218
00:08:55,600 --> 00:08:58,120
spam and what's not we plot that out

219
00:08:58,120 --> 00:09:00,340
with just a simple time series plot

220
00:09:00,340 --> 00:09:02,830
comparing the email length versus the

221
00:09:02,830 --> 00:09:05,560
number of recipients and let's say red

222
00:09:05,560 --> 00:09:09,820
is is bad so spam and blue is is good

223
00:09:09,820 --> 00:09:12,430
so then what machine learning will do is

224
00:09:12,430 --> 00:09:14,320
help us sort of draw this line that will

225
00:09:14,320 --> 00:09:16,300
optimally separate it so we can have a

226
00:09:16,300 --> 00:09:18,430
human you might sort of draw a line in

227
00:09:18,430 --> 00:09:20,350
here but it's usually not very optimal

228
00:09:20,350 --> 00:09:22,690
so the advantage of the computer one is

229
00:09:22,690 --> 00:09:26,970
that it can draw an optimal line

230
00:09:26,970 --> 00:09:29,740
certainly lots of other examples exist

231
00:09:29,740 --> 00:09:31,870
of why we might want to use machine

232
00:09:31,870 --> 00:09:35,170
learning the biggest reason is that so I

233
00:09:35,170 --> 00:09:37,420
gave an example of two features and if

234
00:09:37,420 --> 00:09:39,130
you can plot two features and draw a

235
00:09:39,130 --> 00:09:41,560
line that's great the question is when

236
00:09:41,560 --> 00:09:42,910
you get a lot more features it gets

237
00:09:42,910 --> 00:09:44,260
really hard you're drawing a lot of

238
00:09:44,260 --> 00:09:46,120
lines and you're comparing maybe

239
00:09:46,120 --> 00:09:48,400
hundreds thousands millions of features

240
00:09:48,400 --> 00:09:50,080
this becomes really complex and

241
00:09:50,080 --> 00:09:52,150
computers can do that pretty easily with

242
00:09:52,150 --> 00:09:54,520
these nice statistical and algorithmic

243
00:09:54,520 --> 00:09:57,550
approaches another way that this is

244
00:09:57,550 --> 00:09:58,630
helpful is when you're trying to

245
00:09:58,630 --> 00:10:02,500
classify multiple things right so you

246
00:10:02,500 --> 00:10:04,120
know what if instead of just spam and

247
00:10:04,120 --> 00:10:07,210
benign we've got like our spam and legit

248
00:10:07,210 --> 00:10:09,910
on here we've got phishing and ads which

249
00:10:09,910 --> 00:10:12,850
it's sort of this gray area which in

250
00:10:12,850 --> 00:10:14,980
security we see a lot you know the

251
00:10:14,980 --> 00:10:17,590
classification of good and bad is not

252
00:10:17,590 --> 00:10:19,570
real practical and security a lot of

253
00:10:19,570 --> 00:10:21,970
times there's edge cases like like pups

254
00:10:21,970 --> 00:10:23,320
and malware potentially unwanted

255
00:10:23,320 --> 00:10:27,880
programs so that's an example so as we

256
00:10:27,880 --> 00:10:29,710
grow our number of classes that's

257
00:10:29,710 --> 00:10:31,330
another reason why we start getting into

258
00:10:31,330 --> 00:10:33,790
the space of too much data for humans to

259
00:10:33,790 --> 00:10:35,560
basically draw the line from the CSV

260
00:10:35,560 --> 00:10:39,100
data there's a lot more fancy things we

261
00:10:39,100 --> 00:10:41,800
can get into I really kind of gave you a

262
00:10:41,800 --> 00:10:43,720
high-level overview this is a little

263
00:10:43,720 --> 00:10:45,370
word cloud I made just throwing together

264
00:10:45,370 --> 00:10:47,830
some of the common sort of tools of the

265
00:10:47,830 --> 00:10:51,220
trade if you will and they all are you

266
00:10:51,220 --> 00:10:53,950
know significant deviations that go into

267
00:10:53,950 --> 00:10:55,990
more specifics but generally follow this

268
00:10:55,990 --> 00:10:59,680
sort of approach conceptually anyway and

269
00:10:59,680 --> 00:11:02,320
so some of the algorithms kind of work

270
00:11:02,320 --> 00:11:03,730
differently sometimes you're drawing a

271
00:11:03,730 --> 00:11:05,380
straight line to separate your data

272
00:11:05,380 --> 00:11:07,350
between good and bad

273
00:11:07,350 --> 00:11:09,480
you can get into more math right and

274
00:11:09,480 --> 00:11:11,009
where you start drawing these kind of

275
00:11:11,009 --> 00:11:13,290
clever shapes to separate it and keep in

276
00:11:13,290 --> 00:11:15,839
mind in our real data we're using more

277
00:11:15,839 --> 00:11:17,550
than two dimensions right so if you've

278
00:11:17,550 --> 00:11:19,139
got ten dimensions it starts getting

279
00:11:19,139 --> 00:11:21,329
harder to do this is human hundreds or

280
00:11:21,329 --> 00:11:24,689
thousands it's like impossible so but

281
00:11:24,689 --> 00:11:26,730
the question is do we have you know we

282
00:11:26,730 --> 00:11:28,709
can switch to algorithms that will then

283
00:11:28,709 --> 00:11:31,850
that the fit the data more closely and

284
00:11:31,850 --> 00:11:35,160
so this is an important an important

285
00:11:35,160 --> 00:11:37,470
thing that we can choose algorithms and

286
00:11:37,470 --> 00:11:38,970
choose parameters that we pass to the

287
00:11:38,970 --> 00:11:41,459
algorithms so that instead of just

288
00:11:41,459 --> 00:11:43,290
drawing this kind of simplistic line

289
00:11:43,290 --> 00:11:47,329
here we can fit the data more closely

290
00:11:47,329 --> 00:11:50,189
there's a trade-off though because as we

291
00:11:50,189 --> 00:11:53,429
fit the data more closely we may be sort

292
00:11:53,429 --> 00:11:55,290
of teaching to the test if you will

293
00:11:55,290 --> 00:11:58,499
right and so we talk about this as under

294
00:11:58,499 --> 00:12:01,319
fitting which is when you're not doing a

295
00:12:01,319 --> 00:12:02,759
very good job sort of my straight line

296
00:12:02,759 --> 00:12:04,709
example there was lots of edge cases

297
00:12:04,709 --> 00:12:06,540
where they weren't classified correctly

298
00:12:06,540 --> 00:12:09,240
and overfitting where essentially we

299
00:12:09,240 --> 00:12:11,339
just memorize the data set were trained

300
00:12:11,339 --> 00:12:13,499
from and this is the reason I call it

301
00:12:13,499 --> 00:12:14,879
teaching from the test is if you think

302
00:12:14,879 --> 00:12:16,259
through teaching to the test if you've

303
00:12:16,259 --> 00:12:18,329
got your kids in school and they

304
00:12:18,329 --> 00:12:20,519
memorize the questions on the exam they

305
00:12:20,519 --> 00:12:22,410
could get a hundred percent but then in

306
00:12:22,410 --> 00:12:24,300
real life all they did was memorize the

307
00:12:24,300 --> 00:12:25,679
questions on the exam and they don't

308
00:12:25,679 --> 00:12:27,959
know anything passed beyond that they're

309
00:12:27,959 --> 00:12:29,850
not good at generalizing these concepts

310
00:12:29,850 --> 00:12:33,259
to apply to new and similar problems so

311
00:12:33,259 --> 00:12:35,999
this ability to generalize is very

312
00:12:35,999 --> 00:12:37,800
important and when you don't do well

313
00:12:37,800 --> 00:12:39,629
with generalizing we start to call this

314
00:12:39,629 --> 00:12:43,620
overfitting so the common observation is

315
00:12:43,620 --> 00:12:45,990
that under fitting is straightforward

316
00:12:45,990 --> 00:12:48,209
and obvious that like hey we didn't

317
00:12:48,209 --> 00:12:50,459
classify very well we got really poor

318
00:12:50,459 --> 00:12:52,529
accuracy our results are obviously bad

319
00:12:52,529 --> 00:12:55,670
but overfitting also is a problem

320
00:12:55,670 --> 00:12:57,839
because we're sort of teaching to the

321
00:12:57,839 --> 00:13:04,230
tests so my perspective on this whole

322
00:13:04,230 --> 00:13:06,269
thing being kind of caught up in between

323
00:13:06,269 --> 00:13:08,189
both of these communities so the machine

324
00:13:08,189 --> 00:13:10,110
learning folks and the security

325
00:13:10,110 --> 00:13:12,660
communities is that no one's really

326
00:13:12,660 --> 00:13:17,000
thinking about adversaries causing this

327
00:13:17,000 --> 00:13:18,920
overfitting right and that is

328
00:13:18,920 --> 00:13:20,240
essentially what we're talking about

329
00:13:20,240 --> 00:13:22,940
with machine learning where you test it

330
00:13:22,940 --> 00:13:24,980
in a number of cases and it all seems to

331
00:13:24,980 --> 00:13:26,780
work out well except there's an

332
00:13:26,780 --> 00:13:28,370
adversary that kind of knew what you

333
00:13:28,370 --> 00:13:30,530
were testing against for example and

334
00:13:30,530 --> 00:13:32,750
he's working around you and so part of

335
00:13:32,750 --> 00:13:36,020
the the challenge is that the data

336
00:13:36,020 --> 00:13:39,680
science guys are good optimizing like

337
00:13:39,680 --> 00:13:40,790
the score and they're good at

338
00:13:40,790 --> 00:13:42,260
controlling this trade-off between

339
00:13:42,260 --> 00:13:44,750
overfitting and underfitting but they

340
00:13:44,750 --> 00:13:46,280
don't really think about the vulnerable

341
00:13:46,280 --> 00:13:49,070
points in the process the way that

342
00:13:49,070 --> 00:13:51,170
security folks do right security folks

343
00:13:51,170 --> 00:13:53,090
like to talk about oh well this is this

344
00:13:53,090 --> 00:13:54,620
is an untrusted data source I can

345
00:13:54,620 --> 00:13:56,750
exploit that right and that's not at all

346
00:13:56,750 --> 00:13:58,370
the way data scientists think about

347
00:13:58,370 --> 00:14:01,940
things so with the adversarial machine

348
00:14:01,940 --> 00:14:03,290
learning case we really have this

349
00:14:03,290 --> 00:14:05,150
situation where nobody's thinking about

350
00:14:05,150 --> 00:14:07,610
it because the security teams think it's

351
00:14:07,610 --> 00:14:10,100
machine learning kind of it's just a

352
00:14:10,100 --> 00:14:11,780
black box to me and then the data

353
00:14:11,780 --> 00:14:13,700
scientists just don't have this training

354
00:14:13,700 --> 00:14:16,760
to think about how can i exploit this

355
00:14:16,760 --> 00:14:20,630
process that is happening so there's two

356
00:14:20,630 --> 00:14:22,610
ways to do the adversary machine

357
00:14:22,610 --> 00:14:24,110
learning so formally they call it

358
00:14:24,110 --> 00:14:26,450
causative and exploratory but I think

359
00:14:26,450 --> 00:14:27,980
those are really unstraight forward in

360
00:14:27,980 --> 00:14:31,070
security we basically call this pretty

361
00:14:31,070 --> 00:14:33,610
much when you think of IDS's this this

362
00:14:33,610 --> 00:14:35,780
terminology comes into play a bit more

363
00:14:35,780 --> 00:14:38,870
but poisoning so that you're

364
00:14:38,870 --> 00:14:41,420
understanding how the machine learning

365
00:14:41,420 --> 00:14:44,900
process works you're constructing data

366
00:14:44,900 --> 00:14:47,810
to send to it that it trusts and then

367
00:14:47,810 --> 00:14:51,140
distorting how the classification is

368
00:14:51,140 --> 00:14:53,560
done for example of good or bad traffic

369
00:14:53,560 --> 00:14:56,420
then evasion is really just straight-up

370
00:14:56,420 --> 00:14:58,790
IDs evasion for everyone that's familiar

371
00:14:58,790 --> 00:15:01,760
with it but it's looking and seeing how

372
00:15:01,760 --> 00:15:03,830
the system works and then finding

373
00:15:03,830 --> 00:15:06,380
outliers to how the current system works

374
00:15:06,380 --> 00:15:08,000
and then exploiting them and

375
00:15:08,000 --> 00:15:13,190
constructing those well so when I was

376
00:15:13,190 --> 00:15:15,620
sort of drawing these lines before about

377
00:15:15,620 --> 00:15:17,750
kind of Group A and Group B the idea

378
00:15:17,750 --> 00:15:19,370
with poisoning is that you're moving the

379
00:15:19,370 --> 00:15:21,770
line to cause some of maybe the

380
00:15:21,770 --> 00:15:24,470
malicious to be labeled as benign and

381
00:15:24,470 --> 00:15:26,930
then with evasion you're creating

382
00:15:26,930 --> 00:15:29,530
entries on the wrong side right creating

383
00:15:29,530 --> 00:15:31,020
malicious on the

384
00:15:31,020 --> 00:15:34,230
inside so that's the difference so in

385
00:15:34,230 --> 00:15:36,900
security cyber kill chain is anybody in

386
00:15:36,900 --> 00:15:38,670
the room not familiar with the cyber

387
00:15:38,670 --> 00:15:42,870
kill chain a couple honest people I

388
00:15:42,870 --> 00:15:45,330
appreciate that so it's really one of

389
00:15:45,330 --> 00:15:48,600
the popular ways to talk about processes

390
00:15:48,600 --> 00:15:51,750
and security where we've got this seven

391
00:15:51,750 --> 00:15:54,870
step process which is basically like

392
00:15:54,870 --> 00:15:56,460
when I was a preteen and I was learning

393
00:15:56,460 --> 00:15:57,990
how to pen test this is what we were

394
00:15:57,990 --> 00:15:58,980
doing we just didn't have a

395
00:15:58,980 --> 00:16:01,740
formalization for it and it's convenient

396
00:16:01,740 --> 00:16:03,000
that it was formalized but it's

397
00:16:03,000 --> 00:16:05,100
basically these seven steps do some

398
00:16:05,100 --> 00:16:06,390
reconnaissance

399
00:16:06,390 --> 00:16:09,390
you know weapon weaponize deliver your

400
00:16:09,390 --> 00:16:11,490
exploit and then install it on the

401
00:16:11,490 --> 00:16:14,430
system and create command and control so

402
00:16:14,430 --> 00:16:17,580
you can remotely get your information or

403
00:16:17,580 --> 00:16:21,360
use it and then action on your target so

404
00:16:21,360 --> 00:16:23,100
I wanted to talk about adversarial

405
00:16:23,100 --> 00:16:24,390
machine learning within this framework

406
00:16:24,390 --> 00:16:26,010
because no one is I'm not aware of

407
00:16:26,010 --> 00:16:27,420
anyone that has really thought this way

408
00:16:27,420 --> 00:16:30,570
before so reconnaissance in an

409
00:16:30,570 --> 00:16:32,490
adversarial machine learning poisoning

410
00:16:32,490 --> 00:16:34,650
situation would look something like all

411
00:16:34,650 --> 00:16:36,900
right I'm gonna go like talk to the day

412
00:16:36,900 --> 00:16:39,390
of scientists and like figure out what

413
00:16:39,390 --> 00:16:42,960
data he used to train his model that if

414
00:16:42,960 --> 00:16:44,550
I want to weaponize it I'm going to

415
00:16:44,550 --> 00:16:47,190
study how his model works if I have

416
00:16:47,190 --> 00:16:49,710
access to it and then find poisoning

417
00:16:49,710 --> 00:16:53,370
examples send the poison examples back

418
00:16:53,370 --> 00:16:55,710
as sort of a user feedback so as there

419
00:16:55,710 --> 00:16:58,110
is this assumption from a security point

420
00:16:58,110 --> 00:16:59,700
of view that we have some ways to

421
00:16:59,700 --> 00:17:02,670
influence the system which as we'll see

422
00:17:02,670 --> 00:17:05,099
in later cases this is much more common

423
00:17:05,099 --> 00:17:08,130
in machine learning examples than then

424
00:17:08,130 --> 00:17:10,020
we would think intuitively being in the

425
00:17:10,020 --> 00:17:14,490
security community so after we have

426
00:17:14,490 --> 00:17:16,920
provided these maliciously constructed

427
00:17:16,920 --> 00:17:19,200
poison examples then the new model will

428
00:17:19,200 --> 00:17:21,060
update itself and that is the actual

429
00:17:21,060 --> 00:17:25,260
exploitation then the installation is

430
00:17:25,260 --> 00:17:26,459
the fact that the new model will

431
00:17:26,459 --> 00:17:29,520
overwrite the old model man in control

432
00:17:29,520 --> 00:17:31,110
is a really interesting one because I

433
00:17:31,110 --> 00:17:33,000
really don't think it applies there's a

434
00:17:33,000 --> 00:17:34,380
couple ways you could think about it but

435
00:17:34,380 --> 00:17:35,910
I think the most straightforward one is

436
00:17:35,910 --> 00:17:39,120
to say that it doesn't apply because

437
00:17:39,120 --> 00:17:41,520
your new model is already in the system

438
00:17:41,520 --> 00:17:43,860
and you've kind of assumed some

439
00:17:43,860 --> 00:17:45,330
command-and-control before the process

440
00:17:45,330 --> 00:17:47,820
even started for a poisoning attack to

441
00:17:47,820 --> 00:17:49,049
occur so it's kind of more of a

442
00:17:49,049 --> 00:17:52,730
requirement to get into it then it is a

443
00:17:52,730 --> 00:17:57,570
penultimate step then finally the the

444
00:17:57,570 --> 00:18:00,780
new systems output is manipulated you

445
00:18:00,780 --> 00:18:03,270
succeeded it will now predict based on

446
00:18:03,270 --> 00:18:05,850
your fooled training examples so that's

447
00:18:05,850 --> 00:18:08,790
kind of the framework I use to maybe

448
00:18:08,790 --> 00:18:10,920
explain some of this poisoning

449
00:18:10,920 --> 00:18:13,290
methodology of an adversarial machine

450
00:18:13,290 --> 00:18:14,120
learning

451
00:18:14,120 --> 00:18:16,650
okay so adversarial machine learning

452
00:18:16,650 --> 00:18:18,690
examples these are not this section is

453
00:18:18,690 --> 00:18:20,850
not focused on in the security community

454
00:18:20,850 --> 00:18:22,860
it's just speaking a little bit more

455
00:18:22,860 --> 00:18:26,040
generally about it so our idea here and

456
00:18:26,040 --> 00:18:27,990
machine learning they like to use charts

457
00:18:27,990 --> 00:18:29,549
like this to explain it the idea is

458
00:18:29,549 --> 00:18:31,799
we've got let's say red is malicious

459
00:18:31,799 --> 00:18:34,559
something maybe emails and blue is

460
00:18:34,559 --> 00:18:39,150
benign emails and then our classifier is

461
00:18:39,150 --> 00:18:41,250
learned it it figures out okay here's my

462
00:18:41,250 --> 00:18:44,780
line that that separates it pretty well

463
00:18:44,780 --> 00:18:49,770
and what we see so then our kind of

464
00:18:49,770 --> 00:18:53,070
gradient is like our error that happens

465
00:18:53,070 --> 00:18:54,390
so this would be the difference between

466
00:18:54,390 --> 00:18:56,580
our actual training data and the

467
00:18:56,580 --> 00:18:59,400
real-world examples so essentially like

468
00:18:59,400 --> 00:19:00,900
we probably should have learned a line

469
00:19:00,900 --> 00:19:03,270
that goes over here more because down

470
00:19:03,270 --> 00:19:04,950
here we're making a lot of errors when

471
00:19:04,950 --> 00:19:07,770
we classify it as benign so with a

472
00:19:07,770 --> 00:19:09,750
poisoning what we're trying to do we're

473
00:19:09,750 --> 00:19:11,220
trying to take some of these benign

474
00:19:11,220 --> 00:19:13,890
examples and move them down here so it

475
00:19:13,890 --> 00:19:16,020
will make more errors like errors kind

476
00:19:16,020 --> 00:19:21,059
of being in the red yellow hue and so

477
00:19:21,059 --> 00:19:23,640
that is the sort of poisoning process

478
00:19:23,640 --> 00:19:24,990
and here's a couple examples of some

479
00:19:24,990 --> 00:19:27,270
early work that was done so support

480
00:19:27,270 --> 00:19:29,010
vector machines are one type of

481
00:19:29,010 --> 00:19:30,780
classification mechanism we can do

482
00:19:30,780 --> 00:19:33,360
machine learning with and so in the top

483
00:19:33,360 --> 00:19:37,640
example those two nines that was with a

484
00:19:37,640 --> 00:19:40,919
trying to distinguish digits of nine

485
00:19:40,919 --> 00:19:43,230
versus eight right just tell me which

486
00:19:43,230 --> 00:19:45,270
digit this is is it a 9 or is it an 8

487
00:19:45,270 --> 00:19:48,690
what we see the 9 on the left is kind of

488
00:19:48,690 --> 00:19:50,580
our baseline sample classified correctly

489
00:19:50,580 --> 00:19:53,640
it is a 9 on the right is where we were

490
00:19:53,640 --> 00:19:55,320
able to apply certain types of

491
00:19:55,320 --> 00:19:56,900
distortions to it to me

492
00:19:56,900 --> 00:19:59,390
get to convince the algorithm that's a

493
00:19:59,390 --> 00:20:02,060
support vector machine to believe it was

494
00:20:02,060 --> 00:20:05,060
an 8 now humans would not classify the

495
00:20:05,060 --> 00:20:09,290
top-left 9 at the top right 9 as as an 8

496
00:20:09,290 --> 00:20:11,540
but you can see the difference between

497
00:20:11,540 --> 00:20:14,300
the nines is that the one on the right

498
00:20:14,300 --> 00:20:16,700
does look like it has a little bit of a

499
00:20:16,700 --> 00:20:18,680
mask of an 8 in it if you look closely

500
00:20:18,680 --> 00:20:20,870
but we would still both definitely call

501
00:20:20,870 --> 00:20:23,270
it a 9 so we're exploiting how it makes

502
00:20:23,270 --> 00:20:25,280
this sort of decision boundary happen on

503
00:20:25,280 --> 00:20:27,290
the bottom is a similar example of

504
00:20:27,290 --> 00:20:29,660
classifying a four versus a zero and on

505
00:20:29,660 --> 00:20:31,310
the the four on the right you can see

506
00:20:31,310 --> 00:20:33,080
how there's a little bit of a zero sort

507
00:20:33,080 --> 00:20:37,700
of Matt masked into it the second

508
00:20:37,700 --> 00:20:40,450
example is a little more practical

509
00:20:40,450 --> 00:20:43,940
especially because one everyone likes

510
00:20:43,940 --> 00:20:45,410
image recognition when talking about

511
00:20:45,410 --> 00:20:47,390
machine learning because it's something

512
00:20:47,390 --> 00:20:49,820
we can relate to and it's something we

513
00:20:49,820 --> 00:20:52,730
can easily see if it's correct or not so

514
00:20:52,730 --> 00:20:55,940
the image on the left I would hope you

515
00:20:55,940 --> 00:20:57,800
would agree is the school bus if not I'm

516
00:20:57,800 --> 00:20:59,180
going to completely fail to convince you

517
00:20:59,180 --> 00:21:03,140
anything but then what and so this is

518
00:21:03,140 --> 00:21:05,420
focused on a different classifier which

519
00:21:05,420 --> 00:21:07,490
is very much a buzzword nowadays but

520
00:21:07,490 --> 00:21:09,170
it's deep learning it's the sort of

521
00:21:09,170 --> 00:21:13,280
modification to neural networks so we

522
00:21:13,280 --> 00:21:14,960
take our deep learning classifier we

523
00:21:14,960 --> 00:21:17,330
trained it on a bunch of data and it is

524
00:21:17,330 --> 00:21:19,160
able to look at this bus image and say

525
00:21:19,160 --> 00:21:21,920
this is a bus now what we're doing to

526
00:21:21,920 --> 00:21:24,800
fool the system is we apply the image in

527
00:21:24,800 --> 00:21:26,630
the middle is a mask now we're going to

528
00:21:26,630 --> 00:21:29,360
apply that mask on top of the image on

529
00:21:29,360 --> 00:21:32,210
the left and the output of that is the

530
00:21:32,210 --> 00:21:34,310
image on the far right which

531
00:21:34,310 --> 00:21:36,200
interestingly enough the image on the

532
00:21:36,200 --> 00:21:38,720
far right is classified as an ostrich

533
00:21:38,720 --> 00:21:43,190
and so we see this process of like wow I

534
00:21:43,190 --> 00:21:45,110
would think that image on a far right is

535
00:21:45,110 --> 00:21:47,720
a bus but we've convinced the machine

536
00:21:47,720 --> 00:21:49,070
learning algorithm that it's an ostrich

537
00:21:49,070 --> 00:21:52,160
and so we've succeeded at constructing

538
00:21:52,160 --> 00:21:55,160
an example to fool the classification

539
00:21:55,160 --> 00:21:58,730
system something similar so that was the

540
00:21:58,730 --> 00:22:01,460
poisoning attack there was a similar

541
00:22:01,460 --> 00:22:05,590
example for an evasion attack to

542
00:22:05,590 --> 00:22:08,950
convince so we're classifying cars here

543
00:22:08,950 --> 00:22:10,710
the idea is

544
00:22:10,710 --> 00:22:12,990
spying in his car or not car and then

545
00:22:12,990 --> 00:22:15,659
same similar format we're on the far

546
00:22:15,659 --> 00:22:18,779
left we've got our original image this

547
00:22:18,779 --> 00:22:20,669
time and the far right is the mask and

548
00:22:20,669 --> 00:22:22,770
then in the middle is the erroneously

549
00:22:22,770 --> 00:22:24,390
classified image in this case it's just

550
00:22:24,390 --> 00:22:26,820
classified as not the car so the two

551
00:22:26,820 --> 00:22:29,130
images in the middle we as humans would

552
00:22:29,130 --> 00:22:31,080
say that's definitely a car or an

553
00:22:31,080 --> 00:22:34,860
automobile rather and we after applying

554
00:22:34,860 --> 00:22:36,510
the mask we've certainly fooled the

555
00:22:36,510 --> 00:22:41,820
system so if we want to move on to a

556
00:22:41,820 --> 00:22:44,640
cybersecurity application there have

557
00:22:44,640 --> 00:22:46,860
been these attacks demonstrated there I

558
00:22:46,860 --> 00:22:49,140
think a little more obvious but vogelin

559
00:22:49,140 --> 00:22:52,020
Lee did it in 2006 they worked on

560
00:22:52,020 --> 00:22:53,520
something they call a polymorphic

561
00:22:53,520 --> 00:22:56,370
blending attack the idea is in that

562
00:22:56,370 --> 00:22:58,409
attack the way that you manipulate the

563
00:22:58,409 --> 00:23:00,330
system is you can construct network

564
00:23:00,330 --> 00:23:01,830
traffic so you've got an IDs on a

565
00:23:01,830 --> 00:23:04,380
network IDs is detect intrusions as we

566
00:23:04,380 --> 00:23:07,559
all know and the manipulation of data

567
00:23:07,559 --> 00:23:09,990
into the system is the fact that you can

568
00:23:09,990 --> 00:23:11,850
construct traffic right assuming you're

569
00:23:11,850 --> 00:23:15,360
on the network so you construct enough

570
00:23:15,360 --> 00:23:19,649
traffic to to confuse and basically

571
00:23:19,649 --> 00:23:23,120
blend into the existing baseline and

572
00:23:23,120 --> 00:23:27,000
they basically required access to both

573
00:23:27,000 --> 00:23:31,169
the algorithms and how basically access

574
00:23:31,169 --> 00:23:33,059
to the algorithms and the feature so how

575
00:23:33,059 --> 00:23:38,130
it works entirely under the hood so in

576
00:23:38,130 --> 00:23:40,200
the original I guess is sort of a

577
00:23:40,200 --> 00:23:43,380
keynote Dan talked about Microsoft stay

578
00:23:43,380 --> 00:23:46,049
and so I'd actually like to dive into

579
00:23:46,049 --> 00:23:48,480
that and a little bit more depth so with

580
00:23:48,480 --> 00:23:50,429
DES it was this very public project that

581
00:23:50,429 --> 00:23:53,340
Microsoft used it was a chatbot focused

582
00:23:53,340 --> 00:23:55,950
on trying to solve the problem of an

583
00:23:55,950 --> 00:23:58,980
approachable customer service sort of

584
00:23:58,980 --> 00:24:01,320
representative specifically they're

585
00:24:01,320 --> 00:24:05,240
trying to intimidate Ansari imitate a

586
00:24:05,240 --> 00:24:10,169
teen millennial girl and the stated goal

587
00:24:10,169 --> 00:24:11,880
was to experiment and conduct research

588
00:24:11,880 --> 00:24:16,020
on conversational understanding so the

589
00:24:16,020 --> 00:24:18,929
problem with their design is that they

590
00:24:18,929 --> 00:24:21,529
didn't know about a thing called 4chan

591
00:24:21,529 --> 00:24:24,620
so 4chan realized

592
00:24:24,620 --> 00:24:26,840
of an interesting group of people on the

593
00:24:26,840 --> 00:24:30,170
web they realized hey quote it does

594
00:24:30,170 --> 00:24:32,720
learn things based on what you say and

595
00:24:32,720 --> 00:24:36,110
how you act interact with it unquote so

596
00:24:36,110 --> 00:24:39,080
in 24 hours it went from starting off a

597
00:24:39,080 --> 00:24:42,710
saying hey can I just say that I'm super

598
00:24:42,710 --> 00:24:43,820
stoked to meet you

599
00:24:43,820 --> 00:24:49,070
humans are super cool to chill I'm a

600
00:24:49,070 --> 00:24:49,940
nice person

601
00:24:49,940 --> 00:24:55,130
I just hate everybody to Hitler was

602
00:24:55,130 --> 00:25:00,290
right I hate the Jews to a slide I'm not

603
00:25:00,290 --> 00:25:04,190
actually going to read for you let's

604
00:25:04,190 --> 00:25:06,970
just say it's a lot worse than the last

605
00:25:06,970 --> 00:25:12,170
so right so the problems with tazed

606
00:25:12,170 --> 00:25:14,990
design model is that so much of this is

607
00:25:14,990 --> 00:25:17,059
about a difference in expectation right

608
00:25:17,059 --> 00:25:18,740
the data scientists and machine learning

609
00:25:18,740 --> 00:25:22,309
folks are not thinking about they're

610
00:25:22,309 --> 00:25:24,290
assuming that there is sort of a trusted

611
00:25:24,290 --> 00:25:27,140
input and whenever a 4chan has access to

612
00:25:27,140 --> 00:25:29,960
your input you can't trust it right so

613
00:25:29,960 --> 00:25:31,370
that's kind of the lesson that they

614
00:25:31,370 --> 00:25:34,130
learned but I think it's important for

615
00:25:34,130 --> 00:25:35,720
the security community to know that

616
00:25:35,720 --> 00:25:37,400
there is a pretty large gap in the

617
00:25:37,400 --> 00:25:39,350
knowledge of sort of the data science

618
00:25:39,350 --> 00:25:41,960
community because they don't understand

619
00:25:41,960 --> 00:25:45,080
about untrusted inputs and so in that

620
00:25:45,080 --> 00:25:47,480
sense from a security perspective to us

621
00:25:47,480 --> 00:25:50,090
it's really not too dissimilar from sort

622
00:25:50,090 --> 00:25:52,010
of a buffer overflow which also requires

623
00:25:52,010 --> 00:25:54,140
the ability to insert data in or a

624
00:25:54,140 --> 00:25:55,820
sequel injection which also has a

625
00:25:55,820 --> 00:25:57,920
trusted vector to input data into as

626
00:25:57,920 --> 00:26:02,570
well and so there's actually a number of

627
00:26:02,570 --> 00:26:05,059
similar other companies that are doing

628
00:26:05,059 --> 00:26:09,020
similar types of chat bot related things

629
00:26:09,020 --> 00:26:11,450
for the goal of of customer service as I

630
00:26:11,450 --> 00:26:13,760
mentioned before so we have a few these

631
00:26:13,760 --> 00:26:18,200
kind of perspectives that even though

632
00:26:18,200 --> 00:26:19,880
it's machine learning and it seems like

633
00:26:19,880 --> 00:26:22,220
it's sort of a different community we've

634
00:26:22,220 --> 00:26:24,020
really got a few big takeaways that are

635
00:26:24,020 --> 00:26:26,090
honestly nothing new to the security

636
00:26:26,090 --> 00:26:28,070
community it's just important that we

637
00:26:28,070 --> 00:26:30,710
think about our continued principles of

638
00:26:30,710 --> 00:26:32,300
security as applying to these machine

639
00:26:32,300 --> 00:26:34,790
learning models as well so first we've

640
00:26:34,790 --> 00:26:37,460
got that security was not baked into the

641
00:26:37,460 --> 00:26:38,470
software architecture

642
00:26:38,470 --> 00:26:40,299
you know you talked to anybody in

643
00:26:40,299 --> 00:26:43,059
security and that sort of does system

644
00:26:43,059 --> 00:26:44,799
design and there's all this talk about

645
00:26:44,799 --> 00:26:46,750
oh if only we thought about security

646
00:26:46,750 --> 00:26:49,539
before we started like the internet

647
00:26:49,539 --> 00:26:52,360
right things would be much different and

648
00:26:52,360 --> 00:26:55,120
with machine learning the whole workflow

649
00:26:55,120 --> 00:26:57,130
and how its constructed never really

650
00:26:57,130 --> 00:26:58,720
considers that you have a malicious

651
00:26:58,720 --> 00:27:02,110
adversary involved at all so the second

652
00:27:02,110 --> 00:27:03,900
point is that the training data is

653
00:27:03,900 --> 00:27:06,909
assumed to be ground truth and it's

654
00:27:06,909 --> 00:27:09,490
assuming that it represents an actual

655
00:27:09,490 --> 00:27:13,299
population of the target data that you

656
00:27:13,299 --> 00:27:14,950
want right so if you have some training

657
00:27:14,950 --> 00:27:16,780
data you train your spam detector on

658
00:27:16,780 --> 00:27:19,390
then your spam detector data that you

659
00:27:19,390 --> 00:27:20,919
trained it on should be representative

660
00:27:20,919 --> 00:27:24,669
of all of the spam data at large and one

661
00:27:24,669 --> 00:27:26,650
of my huge high-level takeaways and

662
00:27:26,650 --> 00:27:27,970
security is that whenever anyone says

663
00:27:27,970 --> 00:27:30,669
the word assumption you should think

664
00:27:30,669 --> 00:27:35,590
bonor ability or exploitable as long as

665
00:27:35,590 --> 00:27:38,470
you can violate that assumption so

666
00:27:38,470 --> 00:27:40,090
another way that this applies economics

667
00:27:40,090 --> 00:27:44,200
drives the the motivation so when the

668
00:27:44,200 --> 00:27:47,549
economic incentive increases then the

669
00:27:47,549 --> 00:27:50,080
motive for the adversary will increase

670
00:27:50,080 --> 00:27:52,450
as well and so if we haven't seen too

671
00:27:52,450 --> 00:27:54,600
many of attacks like this it's because

672
00:27:54,600 --> 00:27:56,679
essentially the economics haven't

673
00:27:56,679 --> 00:27:59,049
created the situation to be correct so

674
00:27:59,049 --> 00:28:01,570
if we go back 1215 years ago there were

675
00:28:01,570 --> 00:28:03,820
basically not very few cyber attacks

676
00:28:03,820 --> 00:28:05,830
actually happening that's because people

677
00:28:05,830 --> 00:28:07,480
didn't realize there was all sorts of

678
00:28:07,480 --> 00:28:10,150
economic gain to be benefiting from

679
00:28:10,150 --> 00:28:13,480
exploiting these systems so in like the

680
00:28:13,480 --> 00:28:15,789
whatever 15 20 years I've been in the

681
00:28:15,789 --> 00:28:17,470
security community I think one of the

682
00:28:17,470 --> 00:28:19,270
most important takeaways that I've had

683
00:28:19,270 --> 00:28:21,280
is that it really all comes down to

684
00:28:21,280 --> 00:28:23,409
economics I'm a super nerdy techie guy

685
00:28:23,409 --> 00:28:25,240
that loves to get into the weeds but

686
00:28:25,240 --> 00:28:27,070
really at the end of the day economics

687
00:28:27,070 --> 00:28:30,630
are so key to understanding security

688
00:28:30,630 --> 00:28:33,909
finally there's basically hidden data

689
00:28:33,909 --> 00:28:38,380
channels in so the hidden data channels

690
00:28:38,380 --> 00:28:41,470
piece I think matters a lot so so with

691
00:28:41,470 --> 00:28:42,850
the hidden data channels I talked about

692
00:28:42,850 --> 00:28:44,470
these subtle masked and how you couldn't

693
00:28:44,470 --> 00:28:46,360
distinguish the two buses but then the

694
00:28:46,360 --> 00:28:48,039
algorithm thought it was an ostrich that

695
00:28:48,039 --> 00:28:50,770
hidden data channel to like insert that

696
00:28:50,770 --> 00:28:51,330
mask

697
00:28:51,330 --> 00:28:52,890
you couldn't distinguish is very

698
00:28:52,890 --> 00:28:54,630
important but we've also seen things

699
00:28:54,630 --> 00:28:57,600
like that before so work like

700
00:28:57,600 --> 00:28:59,700
steganography right which dates back

701
00:28:59,700 --> 00:29:02,100
about two decades at least you know

702
00:29:02,100 --> 00:29:05,790
embedding information in images and the

703
00:29:05,790 --> 00:29:07,830
images still looks like there's no

704
00:29:07,830 --> 00:29:10,650
additional information added another

705
00:29:10,650 --> 00:29:12,870
example of this is the the work from

706
00:29:12,870 --> 00:29:16,860
just a few years ago by Hans Bach and

707
00:29:16,860 --> 00:29:20,820
glitz is with the inaudible malware

708
00:29:20,820 --> 00:29:23,250
that's where you have malware that uses

709
00:29:23,250 --> 00:29:24,870
your speakers to transmit on a higher

710
00:29:24,870 --> 00:29:27,120
frequency than human hearing can allow

711
00:29:27,120 --> 00:29:29,460
and it uses that as a communication

712
00:29:29,460 --> 00:29:31,470
channel so again this is this notion of

713
00:29:31,470 --> 00:29:34,070
like hidden data channels which is also

714
00:29:34,070 --> 00:29:36,390
very applicable to the cybersecurity

715
00:29:36,390 --> 00:29:46,430
domain alright so let's talk about a

716
00:29:46,430 --> 00:29:49,310
couple examples of cyber security

717
00:29:49,310 --> 00:29:53,250
evasion examples right this is in some

718
00:29:53,250 --> 00:29:56,310
cases they were using models like the

719
00:29:56,310 --> 00:29:58,620
machine learning models exactly but in

720
00:29:58,620 --> 00:30:00,270
some cases they were using sort of a

721
00:30:00,270 --> 00:30:03,930
naive form of that which very much could

722
00:30:03,930 --> 00:30:05,880
have been a model so the first example I

723
00:30:05,880 --> 00:30:08,880
have is with fast flux so we started

724
00:30:08,880 --> 00:30:10,650
seeing a lot of research papers talking

725
00:30:10,650 --> 00:30:12,150
about hey we've been studying these fast

726
00:30:12,150 --> 00:30:15,570
flux botnets and if you just look at a

727
00:30:15,570 --> 00:30:18,060
few characteristics of the network

728
00:30:18,060 --> 00:30:19,620
traffic we can set some simple

729
00:30:19,620 --> 00:30:23,490
thresholds and detect the fast flux

730
00:30:23,490 --> 00:30:27,980
traffic so in this particular case when

731
00:30:27,980 --> 00:30:30,960
everybody knows DNS TTL is basically

732
00:30:30,960 --> 00:30:33,600
like a caching timeout of sorts for

733
00:30:33,600 --> 00:30:36,870
domain names and so when the Deaton when

734
00:30:36,870 --> 00:30:40,410
the the TTL is below five minutes

735
00:30:40,410 --> 00:30:42,090
everybody was saying this is really the

736
00:30:42,090 --> 00:30:43,620
exemplar

737
00:30:43,620 --> 00:30:46,650
when traffic is doing fast flux right

738
00:30:46,650 --> 00:30:48,390
fast flux is just this way of rapidly

739
00:30:48,390 --> 00:30:51,260
moving around and and avoiding detection

740
00:30:51,260 --> 00:30:54,660
moving around IPs particularly so five

741
00:30:54,660 --> 00:30:57,090
minutes below five minutes was bad above

742
00:30:57,090 --> 00:30:59,070
five minutes was good in the community

743
00:30:59,070 --> 00:31:00,630
this got refined to have a few more

744
00:31:00,630 --> 00:31:02,970
features but that's the general idea and

745
00:31:02,970 --> 00:31:05,049
then a couple years later

746
00:31:05,049 --> 00:31:08,049
in 2010 there was a study of all of the

747
00:31:08,049 --> 00:31:10,539
fast flux that was out there and it

748
00:31:10,539 --> 00:31:12,970
turns out adversaries found out about

749
00:31:12,970 --> 00:31:15,159
this and changed their techniques right

750
00:31:15,159 --> 00:31:17,679
so even though in this case sort of the

751
00:31:17,679 --> 00:31:19,360
modeling approach was a bit more simpler

752
00:31:19,360 --> 00:31:21,399
it was a couple thresholds it very well

753
00:31:21,399 --> 00:31:22,690
could have been like a machine learning

754
00:31:22,690 --> 00:31:25,679
model and this is adversaries reacting

755
00:31:25,679 --> 00:31:29,679
against our detection strategies that's

756
00:31:29,679 --> 00:31:31,809
the first example the second example is

757
00:31:31,809 --> 00:31:35,619
a bit more I guess so this is some work

758
00:31:35,619 --> 00:31:37,539
I did so I worked in some of the very

759
00:31:37,539 --> 00:31:39,700
early domain generation algorithm stuff

760
00:31:39,700 --> 00:31:44,200
in 2009 and so so we built the system to

761
00:31:44,200 --> 00:31:46,389
detect the pseudo random character

762
00:31:46,389 --> 00:31:47,919
domain generation algorithms again this

763
00:31:47,919 --> 00:31:50,019
is just malware moving around really

764
00:31:50,019 --> 00:31:53,529
fast to hide itself and avoid the main

765
00:31:53,529 --> 00:31:57,309
block lists so it's clever because it

766
00:31:57,309 --> 00:32:00,100
can avoid domain block lists what we're

767
00:32:00,100 --> 00:32:03,909
seeing on this is so I was real cautious

768
00:32:03,909 --> 00:32:07,749
when I was measuring the DGA activity of

769
00:32:07,749 --> 00:32:09,759
the malware right and what domains that

770
00:32:09,759 --> 00:32:13,749
they're using I wanted to I first I

771
00:32:13,749 --> 00:32:14,739
really wanted to publish this

772
00:32:14,739 --> 00:32:17,289
information as I could detect it and

773
00:32:17,289 --> 00:32:18,759
this was before a lot of it and then I

774
00:32:18,759 --> 00:32:20,379
had some good arguments with colleagues

775
00:32:20,379 --> 00:32:22,119
about if you publish it then they're

776
00:32:22,119 --> 00:32:24,369
just gonna avoid you like the fast flux

777
00:32:24,369 --> 00:32:27,730
situation so I chose not to publish it

778
00:32:27,730 --> 00:32:30,759
but this is sort of academia and someone

779
00:32:30,759 --> 00:32:33,039
else did so I was monitoring it the

780
00:32:33,039 --> 00:32:33,759
whole time

781
00:32:33,759 --> 00:32:35,739
and so what we see by the way why is on

782
00:32:35,739 --> 00:32:38,289
log scale so every time you see an

783
00:32:38,289 --> 00:32:40,749
increment here it's ten times greater so

784
00:32:40,749 --> 00:32:42,850
at the top little changes mean a lot and

785
00:32:42,850 --> 00:32:44,289
at the bottom little changes don't mean

786
00:32:44,289 --> 00:32:48,549
as much but so in green we've got the

787
00:32:48,549 --> 00:32:51,009
total domains on the internet and then

788
00:32:51,009 --> 00:32:52,960
in red we've got the domains that we're

789
00:32:52,960 --> 00:32:56,039
doing the main generation algorithm

790
00:32:56,039 --> 00:32:58,840
generated domains so we sees we're

791
00:32:58,840 --> 00:33:00,669
measuring for a while pretty consistent

792
00:33:00,669 --> 00:33:02,739
baseline for the most part this little

793
00:33:02,739 --> 00:33:05,350
arrow right here represents when the

794
00:33:05,350 --> 00:33:07,359
first paper was released that USENIX

795
00:33:07,359 --> 00:33:09,669
and then you can see right about to

796
00:33:09,669 --> 00:33:11,830
three weeks after we see a rapid change

797
00:33:11,830 --> 00:33:15,369
in the adversary activity right so now I

798
00:33:15,369 --> 00:33:17,990
think to me this always painted a really

799
00:33:17,990 --> 00:33:19,700
your picture that like the bad guys are

800
00:33:19,700 --> 00:33:21,770
watching or at least attending usenix

801
00:33:21,770 --> 00:33:23,510
you know perhaps we have some in our

802
00:33:23,510 --> 00:33:26,690
audience now that doesn't help right I'm

803
00:33:26,690 --> 00:33:29,210
just being you all way more paranoid but

804
00:33:29,210 --> 00:33:34,540
so we see a clear change in in tactics

805
00:33:34,540 --> 00:33:38,270
so far as mitigation strategies against

806
00:33:38,270 --> 00:33:41,780
this there there are a number of

807
00:33:41,780 --> 00:33:44,059
strategies you can you can use to help

808
00:33:44,059 --> 00:33:45,530
protect against this adversarial

809
00:33:45,530 --> 00:33:47,570
situation some of them require a bit of

810
00:33:47,570 --> 00:33:52,660
machine learning knowledge so the

811
00:33:52,660 --> 00:33:54,830
verifying the integrity of your model

812
00:33:54,830 --> 00:33:56,450
right so your model which is essentially

813
00:33:56,450 --> 00:33:58,429
when I would draw those charts basically

814
00:33:58,429 --> 00:33:59,809
the line that separates it that's

815
00:33:59,809 --> 00:34:01,610
essentially conceptually what your model

816
00:34:01,610 --> 00:34:04,880
is and if you verify the integrity of

817
00:34:04,880 --> 00:34:06,650
those models that will at least ensure

818
00:34:06,650 --> 00:34:09,469
you know when it changes so this is

819
00:34:09,469 --> 00:34:12,020
again pretty simple a file changed on my

820
00:34:12,020 --> 00:34:14,570
system I'm aware of it

821
00:34:14,570 --> 00:34:16,340
the difference is that the effects of it

822
00:34:16,340 --> 00:34:18,590
may be much more subtle if an adversary

823
00:34:18,590 --> 00:34:21,310
did get in and modify the file

824
00:34:21,310 --> 00:34:25,310
protecting watching integrity on both

825
00:34:25,310 --> 00:34:27,260
the models and the training data and the

826
00:34:27,260 --> 00:34:30,020
testing datasets use so you know this is

827
00:34:30,020 --> 00:34:31,730
trying to help forth the situation of

828
00:34:31,730 --> 00:34:33,649
the bad guy coming up learning about

829
00:34:33,649 --> 00:34:36,350
what data you use to generate your model

830
00:34:36,350 --> 00:34:37,879
and making sure he doesn't have access

831
00:34:37,879 --> 00:34:40,250
to the model there's a machine learning

832
00:34:40,250 --> 00:34:42,369
based stuff which for the the nice

833
00:34:42,369 --> 00:34:44,330
population in the audience that is

834
00:34:44,330 --> 00:34:47,540
actually familiar to these techniques by

835
00:34:47,540 --> 00:34:50,149
using certain types of algorithms that

836
00:34:50,149 --> 00:34:53,330
add more not non-linearity that will

837
00:34:53,330 --> 00:34:56,149
help this is that's based on research

838
00:34:56,149 --> 00:34:59,180
from some of the the Google modelling of

839
00:34:59,180 --> 00:35:01,400
deep learning that the more linearity in

840
00:35:01,400 --> 00:35:03,859
the model the more resilient it is also

841
00:35:03,859 --> 00:35:06,550
a little bit of unpredictability so

842
00:35:06,550 --> 00:35:09,920
adding sort of stochastic or random

843
00:35:09,920 --> 00:35:12,320
models as opposed to less deterministic

844
00:35:12,320 --> 00:35:14,450
models may help in this chain depending

845
00:35:14,450 --> 00:35:16,430
where they attacked it human

846
00:35:16,430 --> 00:35:19,300
interpretable models this is like a

847
00:35:19,300 --> 00:35:21,710
machine learning guy actually saying we

848
00:35:21,710 --> 00:35:23,780
should you know use more humans well

849
00:35:23,780 --> 00:35:25,700
yeah absolutely right so the goal is

850
00:35:25,700 --> 00:35:28,010
combining humans and the automated

851
00:35:28,010 --> 00:35:30,560
algorithms kind of jointly that's really

852
00:35:30,560 --> 00:35:31,580
I think where we need to

853
00:35:31,580 --> 00:35:33,560
and what we need to be cognizant of is

854
00:35:33,560 --> 00:35:35,240
that there are some tasks that should be

855
00:35:35,240 --> 00:35:37,490
automated by machines but at the same

856
00:35:37,490 --> 00:35:41,090
time should involve humans as a little

857
00:35:41,090 --> 00:35:44,680
bit of kind of a gut-check oversight

858
00:35:45,310 --> 00:35:48,200
so yeah the models can change over time

859
00:35:48,200 --> 00:35:53,840
as well there's also so there's also a

860
00:35:53,840 --> 00:35:57,320
couple frameworks that can be used so

861
00:35:57,320 --> 00:36:01,910
these are really recent libraries that

862
00:36:01,910 --> 00:36:04,430
are used to essentially test and when

863
00:36:04,430 --> 00:36:05,810
you've got machine learning models being

864
00:36:05,810 --> 00:36:07,820
created so like part of what I do in my

865
00:36:07,820 --> 00:36:10,130
day to day job very much involves using

866
00:36:10,130 --> 00:36:12,140
machine learning models to make sure

867
00:36:12,140 --> 00:36:13,760
we're getting quality data rikes we're

868
00:36:13,760 --> 00:36:15,350
trying to sort of whittle down big piles

869
00:36:15,350 --> 00:36:19,000
of data into more actionable events so

870
00:36:19,000 --> 00:36:22,010
when you want to evaluate your models

871
00:36:22,010 --> 00:36:23,810
that you have and see how easily they

872
00:36:23,810 --> 00:36:26,570
can be poisoned because for example in

873
00:36:26,570 --> 00:36:28,670
in our workflow we have a lot of

874
00:36:28,670 --> 00:36:30,680
different communities and there's some

875
00:36:30,680 --> 00:36:32,210
communities sharing indicators with

876
00:36:32,210 --> 00:36:34,490
other communities and so we're trying to

877
00:36:34,490 --> 00:36:36,830
be pretty cognizant about how when

878
00:36:36,830 --> 00:36:39,020
community a shares an indicator with us

879
00:36:39,020 --> 00:36:42,050
how we can avoid that being sort of a

880
00:36:42,050 --> 00:36:44,720
poisoning channel to affect community B

881
00:36:44,720 --> 00:36:46,820
so that's part of kind of how it applies

882
00:36:46,820 --> 00:36:49,280
to our sort of real world machine

883
00:36:49,280 --> 00:36:52,520
learning work so um the first one this

884
00:36:52,520 --> 00:36:54,590
adversarial Lib is by the University of

885
00:36:54,590 --> 00:36:57,440
I think pronounced Cagliari it's like in

886
00:36:57,440 --> 00:37:01,610
the Mediterranean and they are mostly

887
00:37:01,610 --> 00:37:03,980
focused on improving Society learn is

888
00:37:03,980 --> 00:37:05,360
kind of the basic Python machine

889
00:37:05,360 --> 00:37:08,060
learning framework and they're focused

890
00:37:08,060 --> 00:37:10,880
on utilizing those models and neural

891
00:37:10,880 --> 00:37:13,220
network models as well so this is sort

892
00:37:13,220 --> 00:37:15,260
of a kind of fuzz it and mess with it to

893
00:37:15,260 --> 00:37:17,390
make sure that it and get some

894
00:37:17,390 --> 00:37:19,010
measurement to visit is it resilient to

895
00:37:19,010 --> 00:37:21,770
get adversaries mucking with us the

896
00:37:21,770 --> 00:37:25,130
second one is very focused on deep

897
00:37:25,130 --> 00:37:28,670
learning and the third one is a nice

898
00:37:28,670 --> 00:37:30,230
intersection of security and machine

899
00:37:30,230 --> 00:37:31,940
learning so it is actually a very recent

900
00:37:31,940 --> 00:37:38,000
framework focused on fuzzing when you

901
00:37:38,000 --> 00:37:39,770
have PDFs you try and use machine

902
00:37:39,770 --> 00:37:41,810
learning to detect malicious PDFs in

903
00:37:41,810 --> 00:37:45,039
those this last library is looking

904
00:37:45,039 --> 00:37:47,229
ways to evade the machine learning in

905
00:37:47,229 --> 00:37:49,059
malicious PDFs so

906
00:37:49,059 --> 00:37:51,819
alterations to the PDF to fool the

907
00:37:51,819 --> 00:37:57,939
classifier and say hey I'm benign okay

908
00:37:57,939 --> 00:37:59,709
so I'm just gonna wrap up a little bit

909
00:37:59,709 --> 00:38:02,799
with some kind of final sort of fun

910
00:38:02,799 --> 00:38:04,419
thoughts about the terrible future that

911
00:38:04,419 --> 00:38:07,779
is to come with destructive applications

912
00:38:07,779 --> 00:38:11,069
of machine learning so this is kind of

913
00:38:11,069 --> 00:38:14,019
discussed a little bit in Dan's talk

914
00:38:14,019 --> 00:38:17,289
this morning about autonomous cars and

915
00:38:17,289 --> 00:38:18,969
the reason it matters is because you

916
00:38:18,969 --> 00:38:20,669
know people can die if we get this wrong

917
00:38:20,669 --> 00:38:23,049
I'm much less concerned about the

918
00:38:23,049 --> 00:38:25,689
situation of I guess kind of responding

919
00:38:25,689 --> 00:38:27,039
to some of those thoughts from the

920
00:38:27,039 --> 00:38:28,509
morning I'm less concerned about a

921
00:38:28,509 --> 00:38:30,279
person in camouflage standing out on the

922
00:38:30,279 --> 00:38:34,209
street because these these cars are

923
00:38:34,209 --> 00:38:37,239
using multiple sensors right so the

924
00:38:37,239 --> 00:38:40,119
algorithms are gonna know how to wait

925
00:38:40,119 --> 00:38:43,660
visual cameras versus the less visual

926
00:38:43,660 --> 00:38:47,529
ranges like lidar and sonar and radar so

927
00:38:47,529 --> 00:38:50,079
there's a pretty darn good chance that

928
00:38:50,079 --> 00:38:52,059
if someone's wearing camouflage one of

929
00:38:52,059 --> 00:38:55,269
those non visual ranges will pick up and

930
00:38:55,269 --> 00:38:57,189
the car will still be able to detect

931
00:38:57,189 --> 00:38:58,839
that however if you made self-driving

932
00:38:58,839 --> 00:39:01,449
cars based solely on cameras which is

933
00:39:01,449 --> 00:39:04,449
much more focused this is actually kind

934
00:39:04,449 --> 00:39:07,140
of contrasting like the CMU and Stanford

935
00:39:07,140 --> 00:39:09,369
competition and the urban challenge the

936
00:39:09,369 --> 00:39:10,809
Stanford approach is much more focused

937
00:39:10,809 --> 00:39:13,509
on cameras in the visual spectrum and

938
00:39:13,509 --> 00:39:15,729
less focused on the hardware lidar like

939
00:39:15,729 --> 00:39:18,119
the CMU approach was and so I think

940
00:39:18,119 --> 00:39:20,169
obviously at the end of the day our

941
00:39:20,169 --> 00:39:22,029
autonomous cars are going to be more

942
00:39:22,029 --> 00:39:25,419
aware of these you know lidar and sonar

943
00:39:25,419 --> 00:39:28,119
these these less visible spectrums to us

944
00:39:28,119 --> 00:39:29,799
and we'll be able to account for that

945
00:39:29,799 --> 00:39:32,709
what concerns me more is how they're

946
00:39:32,709 --> 00:39:35,890
also being used as a vector for the

947
00:39:35,890 --> 00:39:38,319
computer to make decisions so it's

948
00:39:38,319 --> 00:39:41,880
possible to learn how the car works

949
00:39:41,880 --> 00:39:44,829
project an image in light our sonar

950
00:39:44,829 --> 00:39:48,609
radar as well rule the car and then no

951
00:39:48,609 --> 00:39:49,929
one on the street will know any

952
00:39:49,929 --> 00:39:52,179
different in the car just like crash

953
00:39:52,179 --> 00:39:54,009
right so like you want to do like a

954
00:39:54,009 --> 00:39:56,409
clandestine assassination I think that

955
00:39:56,409 --> 00:39:59,249
would be the way to go

956
00:40:00,130 --> 00:40:03,050
so just a couple other so here's my

957
00:40:03,050 --> 00:40:06,050
second use case is um you know there's a

958
00:40:06,050 --> 00:40:08,870
lot of emerging information approaches

959
00:40:08,870 --> 00:40:11,120
out there to using doctors to prescribe

960
00:40:11,120 --> 00:40:14,480
medicine and depending on how much you

961
00:40:14,480 --> 00:40:17,570
trust the information input to the

962
00:40:17,570 --> 00:40:19,340
learning model that could also be a

963
00:40:19,340 --> 00:40:22,430
potential for exploitation if the system

964
00:40:22,430 --> 00:40:24,500
isn't designed in a way to kind of

965
00:40:24,500 --> 00:40:26,690
ensure the data in it is trustworthy and

966
00:40:26,690 --> 00:40:28,640
then the ongoing model that's used is

967
00:40:28,640 --> 00:40:32,030
trustworthy as well finally another good

968
00:40:32,030 --> 00:40:36,290
way to another good motivation for this

969
00:40:36,290 --> 00:40:37,790
adversarial machine learning is you've

970
00:40:37,790 --> 00:40:39,230
got all these firms that are out there

971
00:40:39,230 --> 00:40:41,540
using stock trading to compete with each

972
00:40:41,540 --> 00:40:44,150
other and you can totally manipulate the

973
00:40:44,150 --> 00:40:46,010
stock market is your untrusted data

974
00:40:46,010 --> 00:40:48,050
vector if you know how their algorithms

975
00:40:48,050 --> 00:40:50,210
work you can manipulate the stock market

976
00:40:50,210 --> 00:40:51,740
to make it think oh and this happens the

977
00:40:51,740 --> 00:40:53,450
market will drop or it'll go up and

978
00:40:53,450 --> 00:40:55,670
basically cause the firm to buy into

979
00:40:55,670 --> 00:40:58,660
something that's going to go away down

980
00:40:58,660 --> 00:41:02,320
all right so wrapping it all up

981
00:41:02,320 --> 00:41:06,380
hopefully use you and are understanding

982
00:41:06,380 --> 00:41:07,790
my argument which has basically been

983
00:41:07,790 --> 00:41:10,970
this that these attacks can occur right

984
00:41:10,970 --> 00:41:13,820
I showed you the ostrich and busts

985
00:41:13,820 --> 00:41:18,620
examples also I'm making the point that

986
00:41:18,620 --> 00:41:20,720
in cybersecurity attackers adapt to

987
00:41:20,720 --> 00:41:22,840
these defenses you know we talked about

988
00:41:22,840 --> 00:41:27,530
fast flux and DGA and then finally the

989
00:41:27,530 --> 00:41:29,570
point believe it or not is you know that

990
00:41:29,570 --> 00:41:31,580
there's increased ml usage by security

991
00:41:31,580 --> 00:41:33,080
companies and I'm really just basing

992
00:41:33,080 --> 00:41:35,000
that by talking to different vendors and

993
00:41:35,000 --> 00:41:37,340
what I've seen at conferences and so

994
00:41:37,340 --> 00:41:39,290
forth so if you believe these three

995
00:41:39,290 --> 00:41:42,170
premises I think there is a recipe for

996
00:41:42,170 --> 00:41:44,450
you know advanced attackers pretty much

997
00:41:44,450 --> 00:41:47,480
only to do adversarial machine learning

998
00:41:47,480 --> 00:41:48,920
but as we've seen with things like

999
00:41:48,920 --> 00:41:51,070
exploit kits you can take real

1000
00:41:51,070 --> 00:41:54,440
complicated steps and commoditize those

1001
00:41:54,440 --> 00:41:56,930
so in the future maybe in ten fifteen

1002
00:41:56,930 --> 00:41:59,630
years we may see you know people using

1003
00:41:59,630 --> 00:42:00,890
these adversary machine learning

1004
00:42:00,890 --> 00:42:02,420
frameworks and sorts of sort of an

1005
00:42:02,420 --> 00:42:06,440
exploit kit to fool vendors so there's

1006
00:42:06,440 --> 00:42:08,240
some things to be on the radar for the

1007
00:42:08,240 --> 00:42:11,420
future so with that I have like two or

1008
00:42:11,420 --> 00:42:12,350
three minutes

1009
00:42:12,350 --> 00:42:13,910
and I would certainly be happy to take

1010
00:42:13,910 --> 00:42:19,880
questions something I noticed

1011
00:42:19,880 --> 00:42:22,280
interesting from the data you saw the

1012
00:42:22,280 --> 00:42:26,000
opponent respond to a changed

1013
00:42:26,000 --> 00:42:29,600
environment to be timespan me if you

1014
00:42:29,600 --> 00:42:34,400
look at the length of time to respond to

1015
00:42:34,400 --> 00:42:38,420
changes take go take most of our

1016
00:42:38,420 --> 00:42:41,270
industries we're talking years or better

1017
00:42:41,270 --> 00:42:45,140
yeah now that was the yield for us to

1018
00:42:45,140 --> 00:42:46,540
come out on top

1019
00:42:46,540 --> 00:42:49,790
yeah interesting good point so the point

1020
00:42:49,790 --> 00:42:51,200
from the gentleman in the front row is

1021
00:42:51,200 --> 00:42:53,000
that in this time frame we just see it

1022
00:42:53,000 --> 00:42:55,400
being like two or three weeks from when

1023
00:42:55,400 --> 00:42:56,810
the paper was released when the

1024
00:42:56,810 --> 00:42:58,790
adversary shifted their techniques and

1025
00:42:58,790 --> 00:43:01,010
two or three weeks is pretty fast for

1026
00:43:01,010 --> 00:43:03,500
most organizations like way fast for

1027
00:43:03,500 --> 00:43:05,390
government but the I think the point is

1028
00:43:05,390 --> 00:43:08,090
that the economics are aligned so that

1029
00:43:08,090 --> 00:43:10,220
adversaries move fast

1030
00:43:10,220 --> 00:43:12,830
right so adversaries move it like

1031
00:43:12,830 --> 00:43:14,330
Silicon Valley speed not at like

1032
00:43:14,330 --> 00:43:15,980
government speed if you will since I've

1033
00:43:15,980 --> 00:43:19,240
worked in both I can kind of say that

1034
00:43:19,240 --> 00:43:22,689
any other questions

1035
00:43:32,500 --> 00:43:35,120
so one of the things that I've seen as

1036
00:43:35,120 --> 00:43:37,910
far as like autonomous cars is that if

1037
00:43:37,910 --> 00:43:39,590
it has to make a decision between

1038
00:43:39,590 --> 00:43:42,260
crashing into a vehicle hold on people

1039
00:43:42,260 --> 00:43:45,680
or crashing into a group these people

1040
00:43:45,680 --> 00:43:47,069
who are

1041
00:43:47,069 --> 00:43:49,049
like that split-second decision-making

1042
00:43:49,049 --> 00:43:50,880
like I've wondered from the same

1043
00:43:50,880 --> 00:43:52,380
perspective they've been adversaries or

1044
00:43:52,380 --> 00:43:55,289
is that same kind of application in in

1045
00:43:55,289 --> 00:43:57,930
other uses where it will make you it'll

1046
00:43:57,930 --> 00:43:59,849
make the machine make a terrible

1047
00:43:59,849 --> 00:44:01,589
decision one way or the other without a

1048
00:44:01,589 --> 00:44:03,719
proper evaluation based off the risk

1049
00:44:03,719 --> 00:44:08,130
appetite yeah yeah so Adams point was

1050
00:44:08,130 --> 00:44:11,609
that the the decisions that the machines

1051
00:44:11,609 --> 00:44:13,559
make may not just be you know one versus

1052
00:44:13,559 --> 00:44:15,479
the other it may be a number of

1053
00:44:15,479 --> 00:44:18,329
different decisions and if the situate

1054
00:44:18,329 --> 00:44:20,999
if the environment can change such that

1055
00:44:20,999 --> 00:44:24,029
you're forced into a situation where you

1056
00:44:24,029 --> 00:44:25,559
know you've got maybe two or three out

1057
00:44:25,559 --> 00:44:28,049
of maybe ten possibilities but all like

1058
00:44:28,049 --> 00:44:29,819
two or three of those options are really

1059
00:44:29,819 --> 00:44:32,219
bad outcomes for the good guys then

1060
00:44:32,219 --> 00:44:35,869
that's also a bit of a win as well

1061
00:44:43,350 --> 00:44:47,640
to say is changing at right

1062
00:44:47,640 --> 00:44:50,630
yeah so it's a really good question so

1063
00:44:50,630 --> 00:44:55,230
so he was interested if as data or

1064
00:44:55,230 --> 00:44:58,020
models if the data of the population

1065
00:44:58,020 --> 00:45:02,970
changes over time how you know is there

1066
00:45:02,970 --> 00:45:04,800
any research that kind of offers some

1067
00:45:04,800 --> 00:45:07,530
guidance sort of toward that end and

1068
00:45:07,530 --> 00:45:10,830
there is I believe the there's in the

1069
00:45:10,830 --> 00:45:11,940
government there's a project called

1070
00:45:11,940 --> 00:45:14,100
cause which is kind of related to that

1071
00:45:14,100 --> 00:45:15,930
it's related to cybersecurity and

1072
00:45:15,930 --> 00:45:17,760
machine learning and one of the and the

1073
00:45:17,760 --> 00:45:20,070
research phrase for that is called model

1074
00:45:20,070 --> 00:45:21,930
drift so we make these machine learning

1075
00:45:21,930 --> 00:45:24,030
models and the point is that the data

1076
00:45:24,030 --> 00:45:26,070
the model was made for changes over time

1077
00:45:26,070 --> 00:45:28,830
so that's the model drips away from

1078
00:45:28,830 --> 00:45:31,230
where it ideally should be so model

1079
00:45:31,230 --> 00:45:34,170
drift is the topic and I know cause the

1080
00:45:34,170 --> 00:45:37,650
government probe work is investigating

1081
00:45:37,650 --> 00:45:38,880
that and I know there's some folks at

1082
00:45:38,880 --> 00:45:41,550
cert participating in that evaluation

1083
00:45:41,550 --> 00:45:45,810
I've also attended a webinar by Daddo a

1084
00:45:45,810 --> 00:45:49,200
few weeks ago and there was a poll done

1085
00:45:49,200 --> 00:45:50,910
of all of the folks in audience maybe

1086
00:45:50,910 --> 00:45:53,460
about a hundred and basically the

1087
00:45:53,460 --> 00:45:56,040
conclusion was sixty percent of the

1088
00:45:56,040 --> 00:45:57,660
attendance when they're using their

1089
00:45:57,660 --> 00:45:59,430
machine learning don't ever update their

1090
00:45:59,430 --> 00:46:02,460
models it's possible in some cases maybe

1091
00:46:02,460 --> 00:46:04,020
they don't need to because they're their

1092
00:46:04,020 --> 00:46:07,350
data doesn't change over time but I

1093
00:46:07,350 --> 00:46:08,790
think that's kind of food for thought so

1094
00:46:08,790 --> 00:46:10,110
I think you're very much sort of ahead

1095
00:46:10,110 --> 00:46:12,480
of the game and I think that's sort of

1096
00:46:12,480 --> 00:46:14,280
an active space of research obviously

1097
00:46:14,280 --> 00:46:19,950
with the government's interest all right

1098
00:46:19,950 --> 00:46:23,210
thank you all very much

1099
00:46:26,090 --> 00:46:28,150
you

