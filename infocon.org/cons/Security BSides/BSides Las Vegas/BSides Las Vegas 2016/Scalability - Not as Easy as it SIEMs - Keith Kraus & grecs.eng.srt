1
00:00:00,000 --> 00:00:05,100
<font color="#E5E5E5">alright let's get</font><font color="#CCCCCC"> started welcome</font>

2
00:00:01,920 --> 00:00:07,170
everybody to proving ground this first I

3
00:00:05,100 --> 00:00:09,330
want<font color="#CCCCCC"> to thank our sponsors we have very</font>

4
00:00:07,170 --> 00:00:10,530
sprite provide providing i can actually

5
00:00:09,330 --> 00:00:12,269
don't know how to<font color="#E5E5E5"> pronounce that tenable</font>

6
00:00:10,530 --> 00:00:13,620
amazon source of knowledge and all of

7
00:00:12,269 --> 00:00:15,179
them are located here<font color="#CCCCCC"> in the chill-out</font>

8
00:00:13,620 --> 00:00:17,730
area so definitely<font color="#CCCCCC"> check out their</font>

9
00:00:15,179 --> 00:00:19,220
booths please keep<font color="#CCCCCC"> your phones</font><font color="#E5E5E5"> on silent</font>

10
00:00:17,730 --> 00:00:22,230
and be respectful of the speaker and

11
00:00:19,220 --> 00:00:24,269
also please fill out the forms<font color="#E5E5E5"> that are</font>

12
00:00:22,230 --> 00:00:26,820
online for feedback of the actual<font color="#E5E5E5"> talk</font>

13
00:00:24,269 --> 00:00:28,919
this is<font color="#E5E5E5"> Keith Krause he's an associate</font>

14
00:00:26,820 --> 00:00:30,449
principal at accenture and he's going<font color="#E5E5E5"> to</font>

15
00:00:28,920 --> 00:00:34,020
be talking on scalability<font color="#CCCCCC"> it not as</font>

16
00:00:30,449 --> 00:00:35,579
simple as it seems welcome<font color="#E5E5E5"> him Thank</font>

17
00:00:34,020 --> 00:00:37,950
you've<font color="#E5E5E5"> got to start with punch</font><font color="#CCCCCC"> alright</font>

18
00:00:35,579 --> 00:00:39,480
so hi<font color="#E5E5E5"> everyone i'm keith gross i'm an</font>

19
00:00:37,950 --> 00:00:41,969
associate principal engineer at

20
00:00:39,480 --> 00:00:43,648
accenture security lab and just to<font color="#CCCCCC"> give</font>

21
00:00:41,969 --> 00:00:45,510
you a little<font color="#CCCCCC"> bit of background accenture</font>

22
00:00:43,649 --> 00:00:47,219
<font color="#CCCCCC">labs is the most forward-looking part of</font>

23
00:00:45,510 --> 00:00:48,839
<font color="#E5E5E5">a center we're looking at</font><font color="#CCCCCC"> things three</font>

24
00:00:47,219 --> 00:00:50,940
to five years out that are called going

25
00:00:48,840 --> 00:00:53,550
to cause disruption in an industry and

26
00:00:50,940 --> 00:00:54,539
so my talk today is focused on some of

27
00:00:53,550 --> 00:00:57,410
the work that<font color="#CCCCCC"> I've been doing at our</font>

28
00:00:54,539 --> 00:00:59,960
security lab in the Washington DC area

29
00:00:57,410 --> 00:01:02,550
so security data science is hard

30
00:00:59,960 --> 00:01:04,949
currently it's not being done well at

31
00:01:02,550 --> 00:01:07,500
the scale and the volume<font color="#E5E5E5"> of data and</font>

32
00:01:04,949 --> 00:01:10,140
alerts that analysts are receiving<font color="#E5E5E5"> every</font>

33
00:01:07,500 --> 00:01:12,630
<font color="#E5E5E5">day so enterprises are constantly</font>

34
00:01:10,140 --> 00:01:14,549
changing<font color="#E5E5E5"> with more devices being added</font>

35
00:01:12,630 --> 00:01:15,929
at a faster pace than ever before

36
00:01:14,549 --> 00:01:18,210
there's more<font color="#E5E5E5"> bring-your-own-device</font>

37
00:01:15,930 --> 00:01:19,560
programs and new technologies are

38
00:01:18,210 --> 00:01:23,330
constantly<font color="#CCCCCC"> being added into enterprise</font>

39
00:01:19,560 --> 00:01:25,830
RT IT so more devices more people

40
00:01:23,330 --> 00:01:27,750
<font color="#E5E5E5">distributed workforces it's creating a</font>

41
00:01:25,830 --> 00:01:29,640
really difficult security problem of

42
00:01:27,750 --> 00:01:32,460
ensuring that people aren't using your

43
00:01:29,640 --> 00:01:34,200
network maliciously<font color="#E5E5E5"> and it's easy to see</font>

44
00:01:32,460 --> 00:01:36,240
that we're struggling in a day and age

45
00:01:34,200 --> 00:01:39,930
when privacy and security is on

46
00:01:36,240 --> 00:01:42,929
everyone's minds so the pond 1.institute

47
00:01:39,930 --> 00:01:45,299
put out a report recently that says for

48
00:01:42,930 --> 00:01:48,240
most financial services institutions it

49
00:01:45,299 --> 00:01:49,950
takes an average of 98 days to detect an

50
00:01:48,240 --> 00:01:52,710
advanced persistent threat or a zero-day

51
00:01:49,950 --> 00:01:53,790
malware that's 98 days where someone

52
00:01:52,710 --> 00:01:55,589
malicious could be stealing your

53
00:01:53,790 --> 00:01:57,570
identity or<font color="#E5E5E5"> stealing your financial</font>

54
00:01:55,590 --> 00:02:01,170
information and<font color="#E5E5E5"> on the</font><font color="#CCCCCC"> other hand this</font>

55
00:01:57,570 --> 00:02:02,939
takes up to<font color="#CCCCCC"> 7 months for retailers so</font>

56
00:02:01,170 --> 00:02:05,549
wired magazine put out<font color="#E5E5E5"> an article</font>

57
00:02:02,939 --> 00:02:06,839
recently about how the CIA handles

58
00:02:05,549 --> 00:02:09,209
cybersecurity different than most

59
00:02:06,840 --> 00:02:11,630
enterprises so in<font color="#E5E5E5"> addition to just</font>

60
00:02:09,209 --> 00:02:13,580
trying to encrypt everything and just

61
00:02:11,630 --> 00:02:15,560
check the perimeter the<font color="#CCCCCC"> Sierra</font><font color="#E5E5E5"> is</font>

62
00:02:13,580 --> 00:02:17,810
starting to develop systems that can

63
00:02:15,560 --> 00:02:20,120
kind<font color="#E5E5E5"> of prioritize events and problems</font>

64
00:02:17,810 --> 00:02:22,250
and<font color="#E5E5E5"> create intelligent ways to respond</font>

65
00:02:20,120 --> 00:02:24,290
to threats so they said that the

66
00:02:22,250 --> 00:02:26,870
challenge that<font color="#CCCCCC"> lies in</font><font color="#E5E5E5"> this is</font>

67
00:02:24,290 --> 00:02:29,030
efficiently scaling these technologies

68
00:02:26,870 --> 00:02:31,730
for practical deployment and making them

69
00:02:29,030 --> 00:02:33,410
reliable for large networks so my work

70
00:02:31,730 --> 00:02:35,810
has been focused on building a platform

71
00:02:33,410 --> 00:02:40,370
that<font color="#CCCCCC"> can act as the base layer for such</font>

72
00:02:35,810 --> 00:02:42,920
systems so enterprise security has a

73
00:02:40,370 --> 00:02:45,170
data problem in a modern fortune 500

74
00:02:42,920 --> 00:02:48,290
enterprise a terabyte and<font color="#CCCCCC"> a half of data</font>

75
00:02:45,170 --> 00:02:50,239
consisting of 250 million events daily

76
00:02:48,290 --> 00:02:53,269
is the norm and they<font color="#E5E5E5"> are not equipped to</font>

77
00:02:50,240 --> 00:02:54,890
handle it so for most enterprises the

78
00:02:53,270 --> 00:02:58,750
starting point<font color="#E5E5E5"> to tackling this problem</font>

79
00:02:54,890 --> 00:03:01,700
is sim but sim is only a starting point

80
00:02:58,750 --> 00:03:04,280
endless time and money have<font color="#E5E5E5"> been spent</font>

81
00:03:01,700 --> 00:03:06,079
by enterprises in refining their sim

82
00:03:04,280 --> 00:03:08,240
solutions but<font color="#E5E5E5"> sims weren't originally</font>

83
00:03:06,080 --> 00:03:11,330
designed to handle the amount<font color="#E5E5E5"> of data</font>

84
00:03:08,240 --> 00:03:13,280
that<font color="#E5E5E5"> we're producing today and so this</font>

85
00:03:11,330 --> 00:03:15,980
shows mostly through two main areas

86
00:03:13,280 --> 00:03:18,650
which is the storage retention and the

87
00:03:15,980 --> 00:03:21,679
computational power of<font color="#E5E5E5"> Sims so most sims</font>

88
00:03:18,650 --> 00:03:24,350
are only able<font color="#E5E5E5"> to have a 30 day window of</font>

89
00:03:21,680 --> 00:03:26,270
data available online and then data

90
00:03:24,350 --> 00:03:29,090
beyond this<font color="#E5E5E5"> 30 day window typically has</font>

91
00:03:26,270 --> 00:03:31,430
to be archived so this<font color="#E5E5E5"> archived data is</font>

92
00:03:29,090 --> 00:03:33,650
typically stored and efficiently we're

93
00:03:31,430 --> 00:03:35,330
either sacrificing storage space or

94
00:03:33,650 --> 00:03:37,130
speed in<font color="#E5E5E5"> order to kind of load it</font><font color="#CCCCCC"> back</font>

95
00:03:35,330 --> 00:03:39,680
<font color="#E5E5E5">into the environment to them query it</font>

96
00:03:37,130 --> 00:03:42,320
and then additionally working on only<font color="#E5E5E5"> a</font>

97
00:03:39,680 --> 00:03:44,600
30 day window<font color="#E5E5E5"> vastly reduces your</font>

98
00:03:42,320 --> 00:03:46,040
effectiveness<font color="#E5E5E5"> and kind of detecting and</font>

99
00:03:44,600 --> 00:03:50,030
reacting to these advanced persistent

100
00:03:46,040 --> 00:03:52,579
threats so in addition<font color="#CCCCCC"> to this asking</font>

101
00:03:50,030 --> 00:03:54,290
simple questions such as which machine

102
00:03:52,580 --> 00:03:56,180
which machines did I see this executable

103
00:03:54,290 --> 00:03:59,120
on the past few months shouldn't take

104
00:03:56,180 --> 00:04:00,920
hours or even days to run and asking the

105
00:03:59,120 --> 00:04:02,990
wrong question or just miss typing

106
00:04:00,920 --> 00:04:07,010
something shouldn't<font color="#E5E5E5"> take hours to then</font>

107
00:04:02,990 --> 00:04:08,660
return the wrong results so what I'm

108
00:04:07,010 --> 00:04:10,280
<font color="#CCCCCC">trying to say is</font><font color="#E5E5E5"> the modern sim it's a</font>

109
00:04:08,660 --> 00:04:12,770
great<font color="#E5E5E5"> pane of glass to see what's</font>

110
00:04:10,280 --> 00:04:15,800
happening now<font color="#E5E5E5"> but it's very ineffective</font>

111
00:04:12,770 --> 00:04:18,350
about<font color="#E5E5E5"> ask answering more advanced</font>

112
00:04:15,800 --> 00:04:21,840
questions

113
00:04:18,350 --> 00:04:24,479
so enterprises need to move beyond these

114
00:04:21,839 --> 00:04:26,250
basic questions and rules and start

115
00:04:24,479 --> 00:04:28,440
<font color="#CCCCCC">building models and analytics to detect</font>

116
00:04:26,250 --> 00:04:30,870
these more advanced<font color="#E5E5E5"> threats and in order</font>

117
00:04:28,440 --> 00:04:32,550
to answer<font color="#E5E5E5"> these complex and demanding</font>

118
00:04:30,870 --> 00:04:34,800
questions and start<font color="#CCCCCC"> building these</font>

119
00:04:32,550 --> 00:04:37,050
models a big<font color="#E5E5E5"> data driven solution is</font>

120
00:04:34,800 --> 00:04:41,280
needed this solution needs to enable

121
00:04:37,050 --> 00:04:43,410
both analytics and models at scale that

122
00:04:41,280 --> 00:04:46,198
can keep up<font color="#CCCCCC"> with modern enterprises data</font>

123
00:04:43,410 --> 00:04:48,599
volume and velocity when an indicator is

124
00:04:46,199 --> 00:04:50,550
found an analyst needs the ability to

125
00:04:48,599 --> 00:04:52,560
immediately pivot on data or as<font color="#E5E5E5"> we say</font>

126
00:04:50,550 --> 00:04:54,840
pull the thread to kind of follow a

127
00:04:52,560 --> 00:04:59,160
threat from its infancy all the way

128
00:04:54,840 --> 00:05:01,409
until<font color="#CCCCCC"> it's exfiltration so which brings</font>

129
00:04:59,160 --> 00:05:03,960
you to my research hypothesis<font color="#E5E5E5"> cyber</font>

130
00:05:01,410 --> 00:05:06,870
<font color="#E5E5E5">security has a big data problem the</font>

131
00:05:03,960 --> 00:05:08,638
volume<font color="#CCCCCC"> and velocity of data from devices</font>

132
00:05:06,870 --> 00:05:11,370
requires a new approach that combines

133
00:05:08,639 --> 00:05:13,110
all data sources to allow<font color="#CCCCCC"> for more</font>

134
00:05:11,370 --> 00:05:15,030
intelligent and advanced cyber security

135
00:05:13,110 --> 00:05:16,889
hunting through analytics and

136
00:05:15,030 --> 00:05:20,130
exploration at scale across enterprise

137
00:05:16,889 --> 00:05:22,889
data and along with<font color="#E5E5E5"> this open source Big</font>

138
00:05:20,130 --> 00:05:24,419
Data technologies reduce<font color="#CCCCCC"> costs and act</font>

139
00:05:22,889 --> 00:05:27,060
as the building blocks of a scalable

140
00:05:24,419 --> 00:05:28,758
platform with<font color="#CCCCCC"> the speed necessary for</font>

141
00:05:27,060 --> 00:05:31,259
enterprises<font color="#CCCCCC"> to overcome these challenges</font>

142
00:05:28,759 --> 00:05:33,599
combined with long-term historical data

143
00:05:31,259 --> 00:05:35,759
enterprises will be able<font color="#E5E5E5"> to reduce noise</font>

144
00:05:33,599 --> 00:05:40,080
and empower analysts to effectively

145
00:05:35,759 --> 00:05:41,940
detect threats so before I dive into a

146
00:05:40,080 --> 00:05:44,099
big data architecture I want to<font color="#E5E5E5"> make</font><font color="#CCCCCC"> a</font>

147
00:05:41,940 --> 00:05:46,919
point very clear I'm not saying to

148
00:05:44,099 --> 00:05:48,630
replace or abandon your sim CISOs would

149
00:05:46,919 --> 00:05:50,008
not be<font color="#CCCCCC"> very happy if we told them to</font>

150
00:05:48,630 --> 00:05:52,110
just get<font color="#E5E5E5"> rid of the software that</font>

151
00:05:50,009 --> 00:05:54,990
<font color="#CCCCCC">they've invested thousands of hours and</font>

152
00:05:52,110 --> 00:05:57,030
millions<font color="#E5E5E5"> of dollars</font><font color="#CCCCCC"> into so this</font><font color="#E5E5E5"> time</font>

153
00:05:54,990 --> 00:05:59,099
and<font color="#E5E5E5"> money that's been invested by</font>

154
00:05:57,030 --> 00:06:02,340
enterprises into fine-tuning their Sims

155
00:05:59,099 --> 00:06:04,800
makes them great data sources but siloed

156
00:06:02,340 --> 00:06:07,590
data sources if you're an analyst trying

157
00:06:04,800 --> 00:06:09,360
to hunt or follow<font color="#E5E5E5"> the thread of a threat</font>

158
00:06:07,590 --> 00:06:11,039
you're going to<font color="#E5E5E5"> need additional data</font>

159
00:06:09,360 --> 00:06:12,889
sources such as like your vulnerability

160
00:06:11,039 --> 00:06:15,000
scanner to see if a server<font color="#CCCCCC"> of any known</font>

161
00:06:12,889 --> 00:06:16,409
vulnerabilities or<font color="#E5E5E5"> your threat feed to</font>

162
00:06:15,000 --> 00:06:19,259
see if it accessed any kind<font color="#E5E5E5"> of known</font>

163
00:06:16,409 --> 00:06:21,630
malicious entities it's a huge waste<font color="#CCCCCC"> of</font>

164
00:06:19,259 --> 00:06:23,580
time for<font color="#E5E5E5"> analysts to kind of be jumping</font>

165
00:06:21,630 --> 00:06:25,650
between sim to<font color="#CCCCCC"> the vulnerability scanner</font>

166
00:06:23,580 --> 00:06:27,930
back to sim to their threat feed back to

167
00:06:25,650 --> 00:06:30,719
sim what they<font color="#CCCCCC"> need is a single solution</font>

168
00:06:27,930 --> 00:06:32,820
that kind of gives them a<font color="#E5E5E5"> clear picture</font>

169
00:06:30,720 --> 00:06:35,370
so diving into<font color="#CCCCCC"> the architecture now</font>

170
00:06:32,820 --> 00:06:38,070
first part Kafka it's an ingestion

171
00:06:35,370 --> 00:06:39,840
engine and by using Kafka we can ingest

172
00:06:38,070 --> 00:06:42,420
a number of different<font color="#CCCCCC"> data sources and</font>

173
00:06:39,840 --> 00:06:44,429
allows us<font color="#E5E5E5"> to combine these diverse and</font>

174
00:06:42,420 --> 00:06:47,010
typically isolated data sources and

175
00:06:44,430 --> 00:06:51,390
store them into a unified<font color="#E5E5E5"> place in HDFS</font>

176
00:06:47,010 --> 00:06:53,820
so using<font color="#E5E5E5"> HDFS as a data is as a day</font><font color="#CCCCCC"> Lake</font>

177
00:06:51,390 --> 00:06:56,010
is common in the<font color="#CCCCCC"> Big Data space but for</font>

178
00:06:53,820 --> 00:06:57,930
cyber security what it does is<font color="#CCCCCC"> it</font><font color="#E5E5E5"> offers</font>

179
00:06:56,010 --> 00:07:00,150
us a way<font color="#E5E5E5"> to break down these data silos</font>

180
00:06:57,930 --> 00:07:02,220
and bring all these diverse data sources

181
00:07:00,150 --> 00:07:06,000
together<font color="#E5E5E5"> and provide a complete picture</font>

182
00:07:02,220 --> 00:07:08,070
for a security analyst so in order for

183
00:07:06,000 --> 00:07:11,070
the security analysts to actually ask

184
00:07:08,070 --> 00:07:15,270
questions and kind<font color="#E5E5E5"> of hunt things they</font>

185
00:07:11,070 --> 00:07:17,880
need a query layer and what this

186
00:07:15,270 --> 00:07:20,159
architecture gives you is spark so spark

187
00:07:17,880 --> 00:07:22,710
provides an easy way<font color="#CCCCCC"> for analysts to</font>

188
00:07:20,160 --> 00:07:25,380
very rapidly ask questions using very

189
00:07:22,710 --> 00:07:29,880
industry known languages such as sequel

190
00:07:25,380 --> 00:07:31,620
or Python so<font color="#E5E5E5"> Kafka HDFS parquet and</font>

191
00:07:29,880 --> 00:07:33,780
spark they're all open source<font color="#E5E5E5"> Apache</font>

192
00:07:31,620 --> 00:07:36,270
projects so there's no licensing fees

193
00:07:33,780 --> 00:07:38,489
there's huge community support for them

194
00:07:36,270 --> 00:07:40,080
and there's typically rapid up the rapid

195
00:07:38,490 --> 00:07:41,910
updates that generally yield good

196
00:07:40,080 --> 00:07:44,789
performance increases and new useful

197
00:07:41,910 --> 00:07:46,169
features and so typically analysts would

198
00:07:44,790 --> 00:07:48,060
interact with a platform<font color="#CCCCCC"> like this</font>

199
00:07:46,169 --> 00:07:50,430
through an interface such as a<font color="#CCCCCC"> Jupiter</font>

200
00:07:48,060 --> 00:07:52,410
notebook or a tool using graphics tree

201
00:07:50,430 --> 00:07:56,160
but i'm not<font color="#E5E5E5"> going</font><font color="#CCCCCC"> to dive into them</font><font color="#E5E5E5"> on</font>

202
00:07:52,410 --> 00:07:58,260
this talk all<font color="#E5E5E5"> right so Kafka is a</font>

203
00:07:56,160 --> 00:08:01,260
distributed published subscribed message

204
00:07:58,260 --> 00:08:03,900
queue that's extremely fast scalable<font color="#E5E5E5"> and</font>

205
00:08:01,260 --> 00:08:06,840
durable so at a lower level Kafka is

206
00:08:03,900 --> 00:08:09,440
composed of producers consumers and

207
00:08:06,840 --> 00:08:12,299
brokers so producers or what send

208
00:08:09,440 --> 00:08:14,479
messages to<font color="#E5E5E5"> the kafka cluster consumers</font>

209
00:08:12,300 --> 00:08:17,040
consume messages from the<font color="#E5E5E5"> kafka cluster</font>

210
00:08:14,479 --> 00:08:18,750
producers and consumers write and read

211
00:08:17,040 --> 00:08:21,780
messages from different sources of data

212
00:08:18,750 --> 00:08:23,880
called topics and brokers are internal

213
00:08:21,780 --> 00:08:26,700
to the kafka cluster and kind<font color="#E5E5E5"> of manage</font>

214
00:08:23,880 --> 00:08:28,770
these topics internally so brokers

215
00:08:26,700 --> 00:08:31,409
receive messages from the producers<font color="#E5E5E5"> and</font>

216
00:08:28,770 --> 00:08:33,270
then send the messages to consumers so

217
00:08:31,410 --> 00:08:35,430
why kofta is important also is that

218
00:08:33,270 --> 00:08:38,098
messages are ordered as they are sent so

219
00:08:35,429 --> 00:08:39,630
as<font color="#CCCCCC"> you get event data in it feeds event</font>

220
00:08:38,099 --> 00:08:41,789
data out in the same order so<font color="#E5E5E5"> you don't</font>

221
00:08:39,630 --> 00:08:43,469
end up with things<font color="#E5E5E5"> out of order</font><font color="#CCCCCC"> and then</font>

222
00:08:41,789 --> 00:08:44,430
messages are delivered with at<font color="#E5E5E5"> least</font>

223
00:08:43,469 --> 00:08:46,649
once rely

224
00:08:44,430 --> 00:08:48,089
ability and allow for<font color="#E5E5E5"> replication so it</font>

225
00:08:46,649 --> 00:08:51,089
protects you against the case of any

226
00:08:48,089 --> 00:08:53,070
kind of<font color="#E5E5E5"> failure so I touched on it in</font>

227
00:08:51,089 --> 00:08:55,710
<font color="#E5E5E5">the previous slide but Kafka gives us</font>

228
00:08:53,070 --> 00:08:58,529
the ability to ingest a multitude of

229
00:08:55,710 --> 00:09:00,149
diverse data sources such as sim your

230
00:08:58,529 --> 00:09:02,189
vulnerability scanner your threat feed

231
00:09:00,149 --> 00:09:04,140
or any other kind of data source that

232
00:09:02,190 --> 00:09:06,149
you want and<font color="#CCCCCC"> then</font><font color="#E5E5E5"> combine it into a</font>

233
00:09:04,140 --> 00:09:08,310
centralized location but most

234
00:09:06,149 --> 00:09:10,860
importantly it does this at the speed

235
00:09:08,310 --> 00:09:14,160
and<font color="#E5E5E5"> reliability necessary for</font>

236
00:09:10,860 --> 00:09:17,160
enterprises so in a large enterprise the

237
00:09:14,160 --> 00:09:18,959
sim alone can easily can easily generate

238
00:09:17,160 --> 00:09:21,779
more than a billion events per day at

239
00:09:18,959 --> 00:09:24,719
peak volume but Kafka can easily handle

240
00:09:21,779 --> 00:09:27,330
this so LinkedIn actually recently ran

241
00:09:24,720 --> 00:09:30,209
extensive benchmarks on Kafka and the

242
00:09:27,330 --> 00:09:32,399
results showed that on very commodity

243
00:09:30,209 --> 00:09:33,869
level<font color="#CCCCCC"> Hardware Kafka is lightweight</font>

244
00:09:32,399 --> 00:09:36,300
enough where the<font color="#E5E5E5"> limiting factor is</font>

245
00:09:33,870 --> 00:09:40,290
almost always disk i/o or network I oh

246
00:09:36,300 --> 00:09:42,599
and that Kafka can scale to<font color="#E5E5E5"> the extremes</font>

247
00:09:40,290 --> 00:09:44,459
we're at<font color="#CCCCCC"> LinkedIn they're using a Kafka</font>

248
00:09:42,600 --> 00:09:47,490
cluster in production and they handle

249
00:09:44,459 --> 00:09:52,709
800 billion messages a day with over 175

250
00:09:47,490 --> 00:09:55,740
terabytes of data moving through it so

251
00:09:52,709 --> 00:09:58,140
on the storage side HDFS is very common

252
00:09:55,740 --> 00:10:00,510
in<font color="#E5E5E5"> the big data space HDFS is the Hadoop</font>

253
00:09:58,140 --> 00:10:02,400
distributed file system and so it's a

254
00:10:00,510 --> 00:10:08,790
distributed file system that provides

255
00:10:02,400 --> 00:10:11,130
very scalable and reliable so provides a

256
00:10:08,790 --> 00:10:13,920
very scalable and reliable data storage

257
00:10:11,130 --> 00:10:15,990
using commodity hardware and it's the

258
00:10:13,920 --> 00:10:18,060
data storage backbone of nearly all Big

259
00:10:15,990 --> 00:10:20,250
Data technologies and all Big Data

260
00:10:18,060 --> 00:10:22,229
technologies integrate with it so

261
00:10:20,250 --> 00:10:23,850
because<font color="#CCCCCC"> of</font><font color="#E5E5E5"> its integration with Big Data</font>

262
00:10:22,230 --> 00:10:26,120
technologies it allows<font color="#CCCCCC"> us to exploit</font>

263
00:10:23,850 --> 00:10:28,170
certain things that typical distributed

264
00:10:26,120 --> 00:10:30,000
processes<font color="#E5E5E5"> don't allow which is like a</font>

265
00:10:28,170 --> 00:10:31,589
data locality so you're<font color="#E5E5E5"> not you're</font>

266
00:10:30,000 --> 00:10:33,390
minimizing your network transfer to kind

267
00:10:31,589 --> 00:10:37,230
of squeeze as much performance<font color="#E5E5E5"> possible</font>

268
00:10:33,390 --> 00:10:38,880
out<font color="#E5E5E5"> of a distributed system so on HDFS</font>

269
00:10:37,230 --> 00:10:40,950
any file format can be used and

270
00:10:38,880 --> 00:10:43,680
typically<font color="#E5E5E5"> you see things like CSV or</font>

271
00:10:40,950 --> 00:10:45,690
JSON being used and then on the big<font color="#E5E5E5"> data</font>

272
00:10:43,680 --> 00:10:49,439
side there's other things<font color="#E5E5E5"> like sequence</font>

273
00:10:45,690 --> 00:10:51,630
files and but there's a new file format

274
00:10:49,440 --> 00:10:53,430
out called<font color="#CCCCCC"> Parque that's shown</font><font color="#E5E5E5"> very</font>

275
00:10:51,630 --> 00:10:54,620
promising results especially for

276
00:10:53,430 --> 00:10:57,319
security data

277
00:10:54,620 --> 00:10:59,180
so<font color="#CCCCCC"> parquet is a columnar storage format</font>

278
00:10:57,320 --> 00:11:01,040
that was built from the ground up to

279
00:10:59,180 --> 00:11:03,920
<font color="#CCCCCC">support very efficient compression and</font>

280
00:11:01,040 --> 00:11:06,680
encoding schemes and so what this means

281
00:11:03,920 --> 00:11:08,630
is that the data is stored much more

282
00:11:06,680 --> 00:11:11,630
efficiently and can be read much faster

283
00:11:08,630 --> 00:11:14,000
so in a typical table you have your robe

284
00:11:11,630 --> 00:11:17,570
a storage where data stored a 1 B 1<font color="#E5E5E5"> C 1</font>

285
00:11:14,000 --> 00:11:21,680
a 2 B 2 C 2 etc and so<font color="#CCCCCC"> the problem</font><font color="#E5E5E5"> with</font>

286
00:11:17,570 --> 00:11:23,240
this<font color="#E5E5E5"> is a1 a2 a1 b1 and c1 typically</font>

287
00:11:21,680 --> 00:11:24,739
will be very different data so you can't

288
00:11:23,240 --> 00:11:27,650
really encode it intelligently and that

289
00:11:24,740 --> 00:11:29,930
hurts your compression since parquet is

290
00:11:27,650 --> 00:11:33,920
a columnar storage format data is stored

291
00:11:29,930 --> 00:11:37,189
a 1 a 2 a 3 45 b 1 b 2 etc and<font color="#CCCCCC"> so since</font>

292
00:11:33,920 --> 00:11:38,599
the data is it by column typically<font color="#E5E5E5"> if</font>

293
00:11:37,190 --> 00:11:40,160
you're<font color="#E5E5E5"> storing an IP address those IP</font>

294
00:11:38,600 --> 00:11:42,170
addresses are going to be similar so it

295
00:11:40,160 --> 00:11:45,110
makes it much easier<font color="#E5E5E5"> to encode and</font>

296
00:11:42,170 --> 00:11:46,430
compress this so<font color="#CCCCCC"> Allstate recent</font>

297
00:11:45,110 --> 00:11:48,440
Allstate the insurance company recently

298
00:11:46,430 --> 00:11:50,300
did benchmarks on a couple<font color="#CCCCCC"> different</font>

299
00:11:48,440 --> 00:11:51,890
<font color="#E5E5E5">data sets so they had one dataset with</font>

300
00:11:50,300 --> 00:11:53,990
three columns and they found<font color="#E5E5E5"> that</font>

301
00:11:51,890 --> 00:11:56,150
counting messages using<font color="#E5E5E5"> parquet was</font>

302
00:11:53,990 --> 00:11:58,430
about<font color="#CCCCCC"> ten times</font><font color="#E5E5E5"> faster than</font><font color="#CCCCCC"> using CSV</font>

303
00:11:56,150 --> 00:12:00,860
and the file size<font color="#E5E5E5"> was actually five and</font>

304
00:11:58,430 --> 00:12:02,359
a half<font color="#E5E5E5"> times smaller they're in the same</font>

305
00:12:00,860 --> 00:12:04,520
benchmark against the dataset with a

306
00:12:02,360 --> 00:12:06,380
hundred<font color="#E5E5E5"> three columns and Counting</font>

307
00:12:04,520 --> 00:12:10,100
messages was 25 times faster and the

308
00:12:06,380 --> 00:12:12,860
<font color="#E5E5E5">file size was about 41 times smaller so</font>

309
00:12:10,100 --> 00:12:15,440
at the central lab I actually ran a

310
00:12:12,860 --> 00:12:18,020
similar benchmark except using sim data

311
00:12:15,440 --> 00:12:20,420
and so in a<font color="#CCCCCC"> dataset with 400 more than</font>

312
00:12:18,020 --> 00:12:23,060
450 columns I found that counting

313
00:12:20,420 --> 00:12:25,219
messages was about 30 times faster using

314
00:12:23,060 --> 00:12:29,180
parquet than CSV and the file size was

315
00:12:25,220 --> 00:12:31,220
about 45 times smaller so while

316
00:12:29,180 --> 00:12:32,989
performance is great another feature

317
00:12:31,220 --> 00:12:35,180
that<font color="#E5E5E5"> part cake gives you because it's a</font>

318
00:12:32,990 --> 00:12:38,480
columnar store you can actually<font color="#E5E5E5"> add</font>

319
00:12:35,180 --> 00:12:40,699
columns to existing data stores so say

320
00:12:38,480 --> 00:12:42,320
you have your threat feed and then for

321
00:12:40,700 --> 00:12:44,390
example they kind<font color="#E5E5E5"> of adds new metadata</font>

322
00:12:42,320 --> 00:12:46,100
you can actually<font color="#E5E5E5"> just kind of add the</font>

323
00:12:44,390 --> 00:12:51,290
columns as you go and<font color="#E5E5E5"> it maintains</font>

324
00:12:46,100 --> 00:12:54,530
compatibility with your older data so

325
00:12:51,290 --> 00:12:56,209
spark is a general cluster computing

326
00:12:54,530 --> 00:12:59,030
framework that's designed around

327
00:12:56,210 --> 00:13:01,550
providing in-memory computations to

328
00:12:59,030 --> 00:13:04,010
accelerate performance so spark follows

329
00:13:01,550 --> 00:13:06,140
a master worker model where a master

330
00:13:04,010 --> 00:13:07,260
node kind of issues tasks to worker

331
00:13:06,140 --> 00:13:09,810
nodes and then the worker<font color="#E5E5E5"> no</font>

332
00:13:07,260 --> 00:13:12,510
deliver the results back to the master

333
00:13:09,810 --> 00:13:15,119
node and so<font color="#E5E5E5"> under the hood spark uses a</font>

334
00:13:12,510 --> 00:13:16,740
lazy evaluation model and it allows<font color="#CCCCCC"> it</font>

335
00:13:15,120 --> 00:13:19,020
to kind<font color="#E5E5E5"> of optimize things like data</font>

336
00:13:16,740 --> 00:13:20,400
locality and<font color="#E5E5E5"> predicate push down so just</font>

337
00:13:19,020 --> 00:13:23,220
squeezing more performance out of the

338
00:13:20,400 --> 00:13:25,380
<font color="#E5E5E5">resources that you have so within the</font>

339
00:13:23,220 --> 00:13:28,380
context of a cyber<font color="#CCCCCC"> Big Data Platform</font>

340
00:13:25,380 --> 00:13:31,290
spark would be used to ask questions on

341
00:13:28,380 --> 00:13:33,060
your data so as<font color="#E5E5E5"> you can kind of see in</font>

342
00:13:31,290 --> 00:13:35,910
the slide spark has very easy

343
00:13:33,060 --> 00:13:38,160
programming interfaces that are<font color="#E5E5E5"> very</font>

344
00:13:35,910 --> 00:13:40,199
well industry known so you see an

345
00:13:38,160 --> 00:13:42,540
example<font color="#E5E5E5"> Python query or you can do the</font>

346
00:13:40,200 --> 00:13:43,980
<font color="#CCCCCC">same</font><font color="#E5E5E5"> query in spark we're basically any</font>

347
00:13:42,540 --> 00:13:46,770
you<font color="#E5E5E5"> can put this in front of any</font>

348
00:13:43,980 --> 00:13:49,050
security analysts and with minimal up

349
00:13:46,770 --> 00:13:51,480
time they can<font color="#CCCCCC"> kind</font><font color="#E5E5E5"> of get running on it</font>

350
00:13:49,050 --> 00:13:53,609
and FERC also supports programming in

351
00:13:51,480 --> 00:13:55,530
Scala and Java and it gives a small

352
00:13:53,610 --> 00:14:00,000
amount of advanced features and a very

353
00:13:55,530 --> 00:14:02,130
small performance bump so in<font color="#E5E5E5"> addition in</font>

354
00:14:00,000 --> 00:14:04,860
addition to kind of just basic features

355
00:14:02,130 --> 00:14:07,200
like filtering grouping and transforming

356
00:14:04,860 --> 00:14:09,510
data spark kind<font color="#E5E5E5"> of gives us a platform</font>

357
00:14:07,200 --> 00:14:13,950
to build scalable models in analytics

358
00:14:09,510 --> 00:14:15,450
and spark also has a few libraries so it

359
00:14:13,950 --> 00:14:18,120
<font color="#E5E5E5">has the spark machine learning library</font>

360
00:14:15,450 --> 00:14:19,740
called spark ml lib which is a full and

361
00:14:18,120 --> 00:14:22,380
somewhat mature machine<font color="#CCCCCC"> learning library</font>

362
00:14:19,740 --> 00:14:24,060
they have a stream processing library

363
00:14:22,380 --> 00:14:26,130
called spark streaming and they have a

364
00:14:24,060 --> 00:14:30,089
graph processing library called spark

365
00:14:26,130 --> 00:14:33,660
graphics so using this architecture I

366
00:14:30,090 --> 00:14:36,540
actually ran a benchmark so following

367
00:14:33,660 --> 00:14:38,040
<font color="#CCCCCC">the kafka</font><font color="#E5E5E5"> HDFS parque & SPARC</font>

368
00:14:36,540 --> 00:14:39,689
architecture they previously outlined a

369
00:14:38,040 --> 00:14:42,360
day of data and a week of data was

370
00:14:39,690 --> 00:14:44,820
tested so for the<font color="#E5E5E5"> big data solution we</font>

371
00:14:42,360 --> 00:14:47,430
used a<font color="#CCCCCC"> ten node cluster that was under</font>

372
00:14:44,820 --> 00:14:49,170
fifty thousand dollars in<font color="#E5E5E5"> hardware then</font>

373
00:14:47,430 --> 00:14:50,819
we tested this against the production

374
00:14:49,170 --> 00:14:54,089
sim instance that was<font color="#E5E5E5"> being used at a</font>

375
00:14:50,820 --> 00:14:56,390
fortune 500 enterprise so the data set

376
00:14:54,090 --> 00:14:59,100
consisted of more than 450 columns and

377
00:14:56,390 --> 00:15:01,890
approximately 250 million events per day

378
00:14:59,100 --> 00:15:04,770
for the<font color="#CCCCCC"> Big Data solution the data was</font>

379
00:15:01,890 --> 00:15:07,199
stored in Parque on HDFS and all the

380
00:15:04,770 --> 00:15:09,960
queries were on using spark version 1.6

381
00:15:07,200 --> 00:15:11,490
and the Big Data solution considered the

382
00:15:09,960 --> 00:15:13,140
query finished when the data was

383
00:15:11,490 --> 00:15:15,120
returned to<font color="#E5E5E5"> the master and converted</font>

384
00:15:13,140 --> 00:15:16,860
into a panda's data frame and for those

385
00:15:15,120 --> 00:15:18,690
<font color="#CCCCCC">of you that aren't familiar a panda's</font>

386
00:15:16,860 --> 00:15:20,730
data frame is<font color="#E5E5E5"> just a data structure</font>

387
00:15:18,690 --> 00:15:24,240
within a very commonly used

388
00:15:20,730 --> 00:15:26,160
<font color="#CCCCCC">python data science library so here are</font>

389
00:15:24,240 --> 00:15:29,010
<font color="#CCCCCC">some</font><font color="#E5E5E5"> of the results from the benchmark</font>

390
00:15:26,160 --> 00:15:31,439
and as you can see the<font color="#E5E5E5"> big data solution</font>

391
00:15:29,010 --> 00:15:34,139
is magnitudes faster than sim even when

392
00:15:31,440 --> 00:15:39,420
loading all 450 columns when sim is only

393
00:15:34,139 --> 00:15:41,220
typically loading<font color="#CCCCCC"> 10 or</font><font color="#E5E5E5"> so and the sim</font>

394
00:15:39,420 --> 00:15:43,589
solution was actually unable to finish

395
00:15:41,220 --> 00:15:46,350
running a query for a week of data in

396
00:15:43,589 --> 00:15:48,000
most<font color="#E5E5E5"> cases and in talking with the</font>

397
00:15:46,350 --> 00:15:49,560
client team they said<font color="#CCCCCC"> that basically</font>

398
00:15:48,000 --> 00:15:51,570
<font color="#E5E5E5">they would have had to split the query</font>

399
00:15:49,560 --> 00:15:53,040
into multiple time units or allocate

400
00:15:51,570 --> 00:15:54,120
more resources to run it and they<font color="#CCCCCC"> just</font>

401
00:15:53,040 --> 00:15:56,310
couldn't do that because it's a

402
00:15:54,120 --> 00:15:57,420
production sim they did estimate that

403
00:15:56,310 --> 00:16:00,630
the queries would take more than<font color="#E5E5E5"> 20</font>

404
00:15:57,420 --> 00:16:02,310
<font color="#CCCCCC">hours to complete each and I</font><font color="#E5E5E5"> just want</font>

405
00:16:00,630 --> 00:16:04,949
to<font color="#E5E5E5"> make</font><font color="#CCCCCC"> another note a lot of the time</font>

406
00:16:02,310 --> 00:16:07,500
and that in the Big Data solution speeds

407
00:16:04,949 --> 00:16:09,029
was actually spent pulling the data into

408
00:16:07,500 --> 00:16:10,529
<font color="#E5E5E5">memory on that master node and</font>

409
00:16:09,029 --> 00:16:12,720
converting it into a pan tostado frame

410
00:16:10,529 --> 00:16:14,790
if we<font color="#E5E5E5"> just decided to write the results</font>

411
00:16:12,720 --> 00:16:16,680
<font color="#E5E5E5">out in parallel to something like a CSV</font>

412
00:16:14,790 --> 00:16:20,040
or even a park a file the results would

413
00:16:16,680 --> 00:16:23,010
be even lower and so this<font color="#E5E5E5"> is important</font>

414
00:16:20,040 --> 00:16:25,410
because looking past a day or a week of

415
00:16:23,010 --> 00:16:28,170
data this solution easily scales in two

416
00:16:25,410 --> 00:16:29,610
months at our lab we've easily queried

417
00:16:28,170 --> 00:16:32,579
more than six months of data at a time

418
00:16:29,610 --> 00:16:35,610
and been able to run queries in under an

419
00:16:32,579 --> 00:16:38,430
hour easily<font color="#CCCCCC"> and since this was done on</font>

420
00:16:35,610 --> 00:16:40,290
<font color="#E5E5E5">spark 1.6 spark two point O was actually</font>

421
00:16:38,430 --> 00:16:42,599
just released and they've shown a

422
00:16:40,290 --> 00:16:46,069
<font color="#CCCCCC">magnitude of performance increase so we</font>

423
00:16:42,600 --> 00:16:48,839
<font color="#E5E5E5">could expect these times to just improve</font>

424
00:16:46,069 --> 00:16:51,329
so with the new speed and flexibility

425
00:16:48,839 --> 00:16:53,760
offered by a big data solution it allows

426
00:16:51,329 --> 00:16:55,829
new use cases where sim struggles and

427
00:16:53,760 --> 00:16:58,439
most of these use cases kind of break

428
00:16:55,829 --> 00:17:00,149
out of this<font color="#CCCCCC"> 30 day window and the</font>

429
00:16:58,440 --> 00:17:02,940
ability to look at even 12-plus months

430
00:17:00,149 --> 00:17:05,429
of data so for example full sex

431
00:17:02,940 --> 00:17:07,740
<font color="#E5E5E5">full-text search over 12 months of data</font>

432
00:17:05,429 --> 00:17:09,770
so you can proactively search for an exe

433
00:17:07,740 --> 00:17:11,640
or a username over a year<font color="#CCCCCC"> of data</font>

434
00:17:09,770 --> 00:17:12,990
compliance request this is something

435
00:17:11,640 --> 00:17:15,120
<font color="#CCCCCC">that the client team specifically told</font>

436
00:17:12,990 --> 00:17:17,309
us that finding all failed logins for

437
00:17:15,119 --> 00:17:19,649
user over three months or six months or

438
00:17:17,309 --> 00:17:22,168
12 months of<font color="#E5E5E5"> data is just very difficult</font>

439
00:17:19,650 --> 00:17:24,419
in current sim solutions then we can<font color="#E5E5E5"> get</font>

440
00:17:22,169 --> 00:17:26,339
<font color="#E5E5E5">into some more advanced things like you</font>

441
00:17:24,419 --> 00:17:29,429
can baseline log flows and so you can

442
00:17:26,339 --> 00:17:31,770
detect<font color="#E5E5E5"> when a device is emitting more or</font>

443
00:17:29,429 --> 00:17:34,110
less log than usual and what this allows

444
00:17:31,770 --> 00:17:34,710
you to do<font color="#E5E5E5"> is kind of find Oh is there a</font>

445
00:17:34,110 --> 00:17:37,508
<font color="#CCCCCC">Miss can</font>

446
00:17:34,710 --> 00:17:39,759
<font color="#E5E5E5">is this device been brought off</font><font color="#CCCCCC"> the</font>

447
00:17:37,509 --> 00:17:41,259
network or something and<font color="#E5E5E5"> what it</font><font color="#CCCCCC"> also</font>

448
00:17:39,759 --> 00:17:44,019
kind<font color="#E5E5E5"> of allows you to do</font><font color="#CCCCCC"> is baseline</font>

449
00:17:41,259 --> 00:17:46,480
what the normal events that<font color="#CCCCCC"> you see from</font>

450
00:17:44,019 --> 00:17:49,029
a device are so if we normally<font color="#CCCCCC"> just see</font>

451
00:17:46,480 --> 00:17:50,620
a ton of session<font color="#CCCCCC"> open session closed and</font>

452
00:17:49,029 --> 00:17:52,509
firewall and then all of a sudden<font color="#E5E5E5"> we're</font>

453
00:17:50,620 --> 00:17:53,649
seeing something different<font color="#E5E5E5"> why are we</font>

454
00:17:52,509 --> 00:17:56,490
<font color="#CCCCCC">seeing</font><font color="#E5E5E5"> something different has something</font>

455
00:17:53,649 --> 00:17:58,539
malicious happened and most importantly

456
00:17:56,490 --> 00:18:00,639
what this will allow you to do is

457
00:17:58,539 --> 00:18:03,309
accelerate your new sim rules and

458
00:18:00,639 --> 00:18:05,620
filters so what this will allow you to

459
00:18:03,309 --> 00:18:07,178
<font color="#E5E5E5">do is proactively test a new filter or</font>

460
00:18:05,620 --> 00:18:09,820
rule that<font color="#E5E5E5"> you're looking to put in place</font>

461
00:18:07,179 --> 00:18:11,139
in<font color="#E5E5E5"> your sim and it allows you to run</font>

462
00:18:09,820 --> 00:18:13,210
this over 12 months of<font color="#E5E5E5"> data very quickly</font>

463
00:18:11,139 --> 00:18:15,370
so it gives you this kind of rapid

464
00:18:13,210 --> 00:18:19,570
interactive prototyping environment that

465
00:18:15,370 --> 00:18:22,649
sim currently<font color="#E5E5E5"> doesn't give you that's my</font>

466
00:18:19,570 --> 00:18:22,649
talk any questions

467
00:18:29,960 --> 00:18:34,919
<font color="#CCCCCC">yeah</font><font color="#E5E5E5"> so you mentioned earlier in the</font>

468
00:18:33,240 --> 00:18:37,230
<font color="#E5E5E5">talk that you shouldn't throw away your</font>

469
00:18:34,919 --> 00:18:39,360
seeing your sim yes<font color="#CCCCCC"> to institute</font>

470
00:18:37,230 --> 00:18:41,370
something like this what if<font color="#E5E5E5"> you didn't</font>

471
00:18:39,360 --> 00:18:44,070
<font color="#CCCCCC">have a seam already should i forgo</font>

472
00:18:41,370 --> 00:18:47,010
should<font color="#E5E5E5"> shoot that person for go</font><font color="#CCCCCC"> seem and</font>

473
00:18:44,070 --> 00:18:48,840
just do big data should<font color="#E5E5E5"> you do seem</font>

474
00:18:47,010 --> 00:18:50,640
first and big data<font color="#E5E5E5"> building so it</font>

475
00:18:48,840 --> 00:18:52,709
honestly<font color="#E5E5E5"> depends on your environment sim</font>

476
00:18:50,640 --> 00:18:55,409
so current<font color="#E5E5E5"> Sims what they do really well</font>

477
00:18:52,710 --> 00:18:57,779
is they do correlation they do some

478
00:18:55,409 --> 00:19:00,480
normalization and at this state we're

479
00:18:57,779 --> 00:19:01,470
not trying to recreate that so why why

480
00:19:00,480 --> 00:19:03,230
try<font color="#CCCCCC"> to recreate something that already</font>

481
00:19:01,470 --> 00:19:06,480
exists in most of these big enterprises

482
00:19:03,230 --> 00:19:07,620
so just use use that correlation use

483
00:19:06,480 --> 00:19:10,139
that<font color="#E5E5E5"> normalization that they've already</font>

484
00:19:07,620 --> 00:19:11,789
<font color="#E5E5E5">done and put it in if you don't have a</font>

485
00:19:10,139 --> 00:19:14,010
sim that's your prerogative whether you

486
00:19:11,789 --> 00:19:15,510
want to implement a sim or whether you

487
00:19:14,010 --> 00:19:17,760
kind of want to<font color="#E5E5E5"> build that correlation</font>

488
00:19:15,510 --> 00:19:20,190
<font color="#CCCCCC">and normalization into a big data</font>

489
00:19:17,760 --> 00:19:22,019
solution one thing I will<font color="#E5E5E5"> say is there's</font>

490
00:19:20,190 --> 00:19:25,710
a project coming out from Curtin works

491
00:19:22,019 --> 00:19:28,019
and Cisco called Apache<font color="#CCCCCC"> Metron which is</font>

492
00:19:25,710 --> 00:19:30,029
a continuation of Cisco's open sock

493
00:19:28,019 --> 00:19:31,919
which is kind<font color="#E5E5E5"> of a sim built on top of</font>

494
00:19:30,029 --> 00:19:35,750
some of these Big Data technologies and

495
00:19:31,919 --> 00:19:35,750
it's showing<font color="#E5E5E5"> a lot a problem is so far</font>

496
00:19:38,590 --> 00:19:42,559
you mentioned cap gun just like to dig

497
00:19:40,940 --> 00:19:44,630
into the problem<font color="#CCCCCC"> I always here within</font>

498
00:19:42,559 --> 00:19:46,730
you mentioned it was a it's a delivery

499
00:19:44,630 --> 00:19:49,279
at least once yeah how do you see people

500
00:19:46,730 --> 00:19:52,640
<font color="#E5E5E5">dealing with the more than once I got an</font>

501
00:19:49,279 --> 00:19:53,899
event so again yes that is<font color="#E5E5E5"> that is</font>

502
00:19:52,640 --> 00:19:56,169
something that<font color="#E5E5E5"> you will have to kind of</font>

503
00:19:53,899 --> 00:19:59,000
handle within your processing pipeline

504
00:19:56,169 --> 00:20:00,769
unfortunately there has been a lot of

505
00:19:59,000 --> 00:20:04,039
<font color="#E5E5E5">work</font><font color="#CCCCCC"> for example in the spark streaming</font>

506
00:20:00,769 --> 00:20:06,350
side on how to handle<font color="#CCCCCC"> the multiple</font>

507
00:20:04,039 --> 00:20:07,730
message delivery from Kafka but you

508
00:20:06,350 --> 00:20:11,870
would<font color="#E5E5E5"> much rather more than once than</font>

509
00:20:07,730 --> 00:20:15,200
not at all how many days worth of data

510
00:20:11,870 --> 00:20:17,178
do you keep them<font color="#E5E5E5"> like a year currently I</font>

511
00:20:15,200 --> 00:20:20,960
believe we're close to 18 months of data

512
00:20:17,179 --> 00:20:23,450
and it's it's honestly it's limited by

513
00:20:20,960 --> 00:20:25,100
the size of your cluster so it's it's

514
00:20:23,450 --> 00:20:28,070
all commodity hardware with normal

515
00:20:25,100 --> 00:20:29,959
<font color="#CCCCCC">Enterprise discs and HDFS kind of gives</font>

516
00:20:28,070 --> 00:20:33,950
<font color="#E5E5E5">you that layer to just use as much disk</font>

517
00:20:29,960 --> 00:20:37,070
space as you have I was wondering<font color="#E5E5E5"> which</font>

518
00:20:33,950 --> 00:20:38,950
seems you were comparing to your<font color="#E5E5E5"> kefka's</font>

519
00:20:37,070 --> 00:20:42,139
solution in your<font color="#E5E5E5"> test results</font>

520
00:20:38,950 --> 00:20:44,960
unfortunately I cannot share which sim

521
00:20:42,139 --> 00:20:46,250
the<font color="#E5E5E5"> client that we were benchmarking</font>

522
00:20:44,960 --> 00:20:48,980
against was using I can tell you it's a

523
00:20:46,250 --> 00:20:51,909
top-five sim in<font color="#CCCCCC"> the marketplace but I</font>

524
00:20:48,980 --> 00:20:51,909
<font color="#CCCCCC">can't go further</font><font color="#E5E5E5"> than</font><font color="#CCCCCC"> that</font>

525
00:20:56,530 --> 00:21:01,690
so sorry just<font color="#E5E5E5"> on that would you expect</font>

526
00:20:58,660 --> 00:21:03,700
<font color="#E5E5E5">to see different results if you compared</font>

527
00:21:01,690 --> 00:21:06,850
with more sims you mentioned you just

528
00:21:03,700 --> 00:21:10,290
<font color="#E5E5E5">use one yes so depending on the sim I</font>

529
00:21:06,850 --> 00:21:11,949
imagine there would be different results

530
00:21:10,290 --> 00:21:12,879
unfortunately I was the client that we

531
00:21:11,950 --> 00:21:15,820
were working with so those are the

532
00:21:12,880 --> 00:21:17,200
results<font color="#CCCCCC"> that I have but the point</font><font color="#E5E5E5"> the</font>

533
00:21:15,820 --> 00:21:19,270
point that I was trying to<font color="#E5E5E5"> make</font><font color="#CCCCCC"> is any</font>

534
00:21:17,200 --> 00:21:21,670
fortune 500 if you can give them<font color="#E5E5E5"> that</font>

535
00:21:19,270 --> 00:21:23,379
level<font color="#E5E5E5"> of performance to ask questions on</font>

536
00:21:21,670 --> 00:21:26,590
<font color="#E5E5E5">their data will gladly spend fifty</font>

537
00:21:23,380 --> 00:21:29,380
thousand dollars to get that so you

538
00:21:26,590 --> 00:21:31,270
priced out the the cost of the hardware

539
00:21:29,380 --> 00:21:34,360
but I thought did you<font color="#E5E5E5"> include the cost</font>

540
00:21:31,270 --> 00:21:35,860
of the<font color="#E5E5E5"> expertise required to run that as</font>

541
00:21:34,360 --> 00:21:38,709
opposed to running the sim and just

542
00:21:35,860 --> 00:21:40,419
<font color="#E5E5E5">wondering you know it looks like it</font>

543
00:21:38,710 --> 00:21:42,010
could scale but you know what about that

544
00:21:40,420 --> 00:21:44,830
other costs so that that<font color="#E5E5E5"> is not</font>

545
00:21:42,010 --> 00:21:46,600
accounted in I tried to just price it

546
00:21:44,830 --> 00:21:48,340
based on hardware and software licensing

547
00:21:46,600 --> 00:21:51,280
since they're all open source software

548
00:21:48,340 --> 00:21:52,360
there is that there is no<font color="#E5E5E5"> software</font>

549
00:21:51,280 --> 00:21:53,889
licensing but yes there is some

550
00:21:52,360 --> 00:21:55,060
expertise that goes in there are

551
00:21:53,890 --> 00:21:57,700
companies such as like Cloudera

552
00:21:55,060 --> 00:21:59,919
<font color="#CCCCCC">hortonworks map are that gives you a</font>

553
00:21:57,700 --> 00:22:01,660
very very<font color="#E5E5E5"> easy interface to kind of just</font>

554
00:21:59,920 --> 00:22:04,450
get up and running very quickly with a

555
00:22:01,660 --> 00:22:06,490
stack similar to this that you could put

556
00:22:04,450 --> 00:22:09,460
<font color="#CCCCCC">I would say a basic security analyst and</font>

557
00:22:06,490 --> 00:22:11,850
they it's a very follow<font color="#E5E5E5"> bowl tutorial to</font>

558
00:22:09,460 --> 00:22:11,850
set it<font color="#E5E5E5"> up</font>

559
00:22:23,460 --> 00:22:26,639
anybody else

560
00:22:28,610 --> 00:22:32,500
all right give me round of applause

