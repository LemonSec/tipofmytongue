1
00:00:00,000 --> 00:00:02,800
i am pleased to introduce josh sax from

2
00:00:02,800 --> 00:00:05,359
sofos who will be talking about security

3
00:00:05,359 --> 00:00:09,359
ai in the real world please welcome him

4
00:00:14,880 --> 00:00:16,960
can you guys i'm not allowed not super

5
00:00:16,960 --> 00:00:19,920
loud can you guys hear me okay or yeah

6
00:00:19,920 --> 00:00:22,160
okay

7
00:00:22,480 --> 00:00:23,760
okay so

8
00:00:23,760 --> 00:00:26,400
hi i'm joshua sax i'm currently chief

9
00:00:26,400 --> 00:00:28,720
scientist at sofos and my talk today is

10
00:00:28,720 --> 00:00:30,960
entitled security ai in

11
00:00:30,960 --> 00:00:32,320
the real world

12
00:00:32,320 --> 00:00:33,680
um

13
00:00:33,680 --> 00:00:36,160
okay so um

14
00:00:36,160 --> 00:00:39,120
the reason i i submitted an abstract for

15
00:00:39,120 --> 00:00:43,360
this talk is is that i think that um

16
00:00:43,360 --> 00:00:46,480
right now ai insecurity is in this weird

17
00:00:46,480 --> 00:00:48,000
state where

18
00:00:48,000 --> 00:00:49,760
on the one hand um

19
00:00:49,760 --> 00:00:52,640
there's a lot of hype and confusion um

20
00:00:52,640 --> 00:00:54,399
and um

21
00:00:54,399 --> 00:00:56,079
you know almost any new security

22
00:00:56,079 --> 00:00:57,920
products it comes with some marketing

23
00:00:57,920 --> 00:01:00,000
around the way the fact that it uses ai

24
00:01:00,000 --> 00:01:01,199
and some ambiguous and hard to

25
00:01:01,199 --> 00:01:02,960
understand kind of way

26
00:01:02,960 --> 00:01:05,360
um but then at the same time

27
00:01:05,360 --> 00:01:07,280
uh there really is something something

28
00:01:07,280 --> 00:01:09,760
there with respect to to ai and security

29
00:01:09,760 --> 00:01:11,680
um i mean we're seeing

30
00:01:11,680 --> 00:01:12,799
ai

31
00:01:12,799 --> 00:01:15,280
make real material impact in a whole

32
00:01:15,280 --> 00:01:17,040
range of fields right so probably many

33
00:01:17,040 --> 00:01:20,080
of us read about alpha folds

34
00:01:20,080 --> 00:01:22,479
which came out of a team at google

35
00:01:22,479 --> 00:01:23,759
that

36
00:01:23,759 --> 00:01:25,520
made an enormous step function and

37
00:01:25,520 --> 00:01:27,280
progress towards predicting the

38
00:01:27,280 --> 00:01:28,720
three-dimensional structure of proteins

39
00:01:28,720 --> 00:01:30,400
which is a significant advance in

40
00:01:30,400 --> 00:01:32,560
science

41
00:01:32,560 --> 00:01:34,720
many of us have probably also

42
00:01:34,720 --> 00:01:36,799
followed news about gpt-3 or played

43
00:01:36,799 --> 00:01:38,400
around with gpt3 those of us who have

44
00:01:38,400 --> 00:01:41,520
access to the open ai apis for that

45
00:01:41,520 --> 00:01:43,600
and that's like a real advance right

46
00:01:43,600 --> 00:01:45,360
we've made real progress

47
00:01:45,360 --> 00:01:46,640
at

48
00:01:46,640 --> 00:01:49,520
machine learning models

49
00:01:49,520 --> 00:01:51,439
modeling language in ways that are

50
00:01:51,439 --> 00:01:53,680
likely to be really helpful

51
00:01:53,680 --> 00:01:54,880
and um

52
00:01:54,880 --> 00:01:56,479
some of you guys have probably

53
00:01:56,479 --> 00:01:59,119
played around with with dali uh this

54
00:01:59,119 --> 00:02:01,200
this this new model that can draw images

55
00:02:01,200 --> 00:02:03,439
based on um natural language prompts and

56
00:02:03,439 --> 00:02:05,680
again this is an example of a real

57
00:02:05,680 --> 00:02:08,479
technological advance

58
00:02:08,479 --> 00:02:11,038
so my goal in this talk is to to help

59
00:02:11,038 --> 00:02:12,959
separate the signal from the noise here

60
00:02:12,959 --> 00:02:14,800
right so if you walk around the trade

61
00:02:14,800 --> 00:02:17,920
floor at any security conference you're

62
00:02:17,920 --> 00:02:20,319
going to get a lot of of noise to be

63
00:02:20,319 --> 00:02:22,400
honest around the role of ai and

64
00:02:22,400 --> 00:02:24,319
security but there really is a signal

65
00:02:24,319 --> 00:02:26,720
there and so i hope that this talk can

66
00:02:26,720 --> 00:02:29,760
be a small contribution to helping the

67
00:02:29,760 --> 00:02:31,519
security community sort of get at that

68
00:02:31,519 --> 00:02:33,920
signal and get a grip on the ways in

69
00:02:33,920 --> 00:02:36,720
which ai is important now to security

70
00:02:36,720 --> 00:02:39,360
and will be increasingly important i'll

71
00:02:39,360 --> 00:02:42,560
argue in the future

72
00:02:42,879 --> 00:02:45,599
okay so just so you know that a little

73
00:02:45,599 --> 00:02:47,120
bit about the human being here giving

74
00:02:47,120 --> 00:02:49,120
you this this talk

75
00:02:49,120 --> 00:02:50,000
my

76
00:02:50,000 --> 00:02:52,239
my current job is i'm chief scientist at

77
00:02:52,239 --> 00:02:54,080
sophos that's that's the role i've been

78
00:02:54,080 --> 00:02:56,160
in for the last five years or so

79
00:02:56,160 --> 00:02:58,080
uh really that means that i'm

80
00:02:58,080 --> 00:02:59,599
responsible for

81
00:02:59,599 --> 00:03:02,319
ai research development and operations

82
00:03:02,319 --> 00:03:04,640
at sofos a bunch of colleagues here

83
00:03:04,640 --> 00:03:05,920
today who are also giving talks about

84
00:03:05,920 --> 00:03:08,400
work that we've done here um and looking

85
00:03:08,400 --> 00:03:10,159
forward to you guys getting a sort of

86
00:03:10,159 --> 00:03:12,560
deeper dive from from their work as well

87
00:03:12,560 --> 00:03:13,599
um

88
00:03:13,599 --> 00:03:15,360
uh before that i spent a bunch of time

89
00:03:15,360 --> 00:03:17,599
in in the government worlds most of that

90
00:03:17,599 --> 00:03:20,000
time i was living in dc uh working on

91
00:03:20,000 --> 00:03:22,879
mostly darpa funded r d projects around

92
00:03:22,879 --> 00:03:24,959
applying mathematical modeling

93
00:03:24,959 --> 00:03:26,400
stats and machine learning to cyber

94
00:03:26,400 --> 00:03:27,680
security problems

95
00:03:27,680 --> 00:03:29,200
um

96
00:03:29,200 --> 00:03:31,599
my real background is like in the 1990s

97
00:03:31,599 --> 00:03:33,360
like irc hacker scene that's where i

98
00:03:33,360 --> 00:03:35,519
learned probably half of what i know um

99
00:03:35,519 --> 00:03:36,720
and that's how i arrived you know in the

100
00:03:36,720 --> 00:03:39,120
tech industry um and i i studied

101
00:03:39,120 --> 00:03:41,120
something else in in school i did i did

102
00:03:41,120 --> 00:03:43,200
history for undergrad and and grad

103
00:03:43,200 --> 00:03:46,319
school and also studied studied music um

104
00:03:46,319 --> 00:03:47,920
uh if you're interested in some

105
00:03:47,920 --> 00:03:49,440
ideas that i'm talking about here please

106
00:03:49,440 --> 00:03:51,760
read hillary in my book uh it's still it

107
00:03:51,760 --> 00:03:53,200
came out a few years ago but still still

108
00:03:53,200 --> 00:03:54,879
relevant called malware data science

109
00:03:54,879 --> 00:03:57,439
from the starch press

110
00:03:57,439 --> 00:03:58,640
and then a little bit of background

111
00:03:58,640 --> 00:04:00,319
about the sort of professional context

112
00:04:00,319 --> 00:04:01,760
that i'm coming from here

113
00:04:01,760 --> 00:04:04,080
so sophos is a fairly large cyber

114
00:04:04,080 --> 00:04:06,480
security company we defend about half a

115
00:04:06,480 --> 00:04:08,560
million organizations

116
00:04:08,560 --> 00:04:10,879
from cyber attacks uh that we cover

117
00:04:10,879 --> 00:04:12,799
about 100 million endpoints so the data

118
00:04:12,799 --> 00:04:13,920
science projects i'm going to be talking

119
00:04:13,920 --> 00:04:17,519
about today uh addressed that that scale

120
00:04:17,519 --> 00:04:19,918
and sofos have has security products

121
00:04:19,918 --> 00:04:21,519
that cover

122
00:04:21,519 --> 00:04:23,199
really every major category of security

123
00:04:23,199 --> 00:04:25,040
products from firewall to cloud security

124
00:04:25,040 --> 00:04:27,919
email security endpoints mobile security

125
00:04:27,919 --> 00:04:30,080
and we have machine learning operating

126
00:04:30,080 --> 00:04:31,840
in almost all of those products um

127
00:04:31,840 --> 00:04:33,680
actually i would say every

128
00:04:33,680 --> 00:04:35,120
every one of the products that isn't

129
00:04:35,120 --> 00:04:36,880
really sort of a legacy product we have

130
00:04:36,880 --> 00:04:38,639
we have machine learning in

131
00:04:38,639 --> 00:04:40,560
um so that's just some context for uh

132
00:04:40,560 --> 00:04:43,840
the perspectives i'll be giving today

133
00:04:44,000 --> 00:04:45,600
all right so i want to i want to like

134
00:04:45,600 --> 00:04:49,199
state my my thesis up front here um

135
00:04:49,199 --> 00:04:50,000
so

136
00:04:50,000 --> 00:04:51,840
and the theses really revolve around

137
00:04:51,840 --> 00:04:53,680
where like the reality of where machine

138
00:04:53,680 --> 00:04:55,280
learning is in terms of what's what its

139
00:04:55,280 --> 00:04:56,880
strength and limitations are in the way

140
00:04:56,880 --> 00:04:58,880
we do cyber security today

141
00:04:58,880 --> 00:05:01,039
um and then where and what where i think

142
00:05:01,039 --> 00:05:02,720
it's going um these these are my

143
00:05:02,720 --> 00:05:04,000
opinions i wouldn't attribute them to

144
00:05:04,000 --> 00:05:05,280
anybody else but i think we have some

145
00:05:05,280 --> 00:05:07,120
level of consensus uh

146
00:05:07,120 --> 00:05:10,080
on my team at sofos and

147
00:05:10,080 --> 00:05:12,320
among sort of security data scientists

148
00:05:12,320 --> 00:05:14,400
friends i have at other companies um so

149
00:05:14,400 --> 00:05:16,400
here are the theses so first

150
00:05:16,400 --> 00:05:17,440
um

151
00:05:17,440 --> 00:05:19,280
with with where machine learning is

152
00:05:19,280 --> 00:05:21,600
today in the security industry

153
00:05:21,600 --> 00:05:23,280
i'd say i'd say applying machine

154
00:05:23,280 --> 00:05:25,120
learning uh to a security problem is

155
00:05:25,120 --> 00:05:26,880
actually a last resort it's like the

156
00:05:26,880 --> 00:05:28,639
last thing you do after you do all the

157
00:05:28,639 --> 00:05:30,720
easier things but it's almost always

158
00:05:30,720 --> 00:05:32,000
necessary

159
00:05:32,000 --> 00:05:33,600
and i'll talk more about that in a

160
00:05:33,600 --> 00:05:35,600
little bit

161
00:05:35,600 --> 00:05:36,960
i'd also say

162
00:05:36,960 --> 00:05:38,880
that contrary to i think a lot of

163
00:05:38,880 --> 00:05:40,800
marketing and i think popular

164
00:05:40,800 --> 00:05:42,160
perspective

165
00:05:42,160 --> 00:05:45,039
in security ml the the actual math is

166
00:05:45,039 --> 00:05:47,120
the least important part typically it's

167
00:05:47,120 --> 00:05:48,960
all the other surrounding factors that

168
00:05:48,960 --> 00:05:51,199
really drive accuracy and efficacy when

169
00:05:51,199 --> 00:05:53,759
it comes to security machine learning uh

170
00:05:53,759 --> 00:05:55,919
data engineering and you'll hear more

171
00:05:55,919 --> 00:05:57,600
about this from my colleagues and other

172
00:05:57,600 --> 00:05:58,560
talks that are going to happen in this

173
00:05:58,560 --> 00:06:00,160
room subsequent to this talk but you

174
00:06:00,160 --> 00:06:02,080
know getting getting the right data in

175
00:06:02,080 --> 00:06:04,639
the right place correctly labeled

176
00:06:04,639 --> 00:06:06,000
as good and bad and that kind of kind of

177
00:06:06,000 --> 00:06:09,840
thing that's super important um

178
00:06:09,840 --> 00:06:11,440
how you monitor machine learning models

179
00:06:11,440 --> 00:06:12,479
in the field is super important so

180
00:06:12,479 --> 00:06:13,520
there's all these surrounding factors

181
00:06:13,520 --> 00:06:15,039
that are actually more important than

182
00:06:15,039 --> 00:06:16,240
the math you need to get the math right

183
00:06:16,240 --> 00:06:17,680
but then it's these other factors that

184
00:06:17,680 --> 00:06:19,280
really drive accuracy

185
00:06:19,280 --> 00:06:21,840
um and then in the last part of the talk

186
00:06:21,840 --> 00:06:23,120
i'm going to talk fast so i don't run

187
00:06:23,120 --> 00:06:24,880
out of time and i can actually get there

188
00:06:24,880 --> 00:06:25,840
um

189
00:06:25,840 --> 00:06:28,240
i'll i'll talk about

190
00:06:28,240 --> 00:06:30,479
where i think the the role

191
00:06:30,479 --> 00:06:32,400
may be in terms of these larger models

192
00:06:32,400 --> 00:06:34,160
that we're hearing about in the news and

193
00:06:34,160 --> 00:06:36,400
the tech press like gpt 3 and that kind

194
00:06:36,400 --> 00:06:37,280
of thing

195
00:06:37,280 --> 00:06:38,400
i think that those models are going to

196
00:06:38,400 --> 00:06:39,759
be really important and i think it's

197
00:06:39,759 --> 00:06:41,360
it's our job as as cyber security

198
00:06:41,360 --> 00:06:42,639
professionals and some of us as

199
00:06:42,639 --> 00:06:45,440
researchers to figure out exactly how

200
00:06:45,440 --> 00:06:47,039
i also think a tighter feedback loop

201
00:06:47,039 --> 00:06:49,199
between uh

202
00:06:49,199 --> 00:06:50,639
security analysts and machine learning

203
00:06:50,639 --> 00:06:51,759
models is gonna be a big part of the

204
00:06:51,759 --> 00:06:53,039
future of the way we use machine

205
00:06:53,039 --> 00:06:54,080
learning models i think we're gonna see

206
00:06:54,080 --> 00:06:55,440
something more like

207
00:06:55,440 --> 00:06:56,720
uh

208
00:06:56,720 --> 00:06:58,720
amazon recommend like the amazon

209
00:06:58,720 --> 00:07:01,599
recommendation engine uh or the google

210
00:07:01,599 --> 00:07:03,120
search system uh come to play in

211
00:07:03,120 --> 00:07:04,160
security where you have this tight

212
00:07:04,160 --> 00:07:06,000
feedback loop that operates uh at the

213
00:07:06,000 --> 00:07:08,160
scale of days uh between security

214
00:07:08,160 --> 00:07:09,520
operators making decisions and machine

215
00:07:09,520 --> 00:07:10,960
learning models making recommendations

216
00:07:10,960 --> 00:07:12,800
to them and prioritizing alerts and that

217
00:07:12,800 --> 00:07:15,120
kind of thing um so that's the argument

218
00:07:15,120 --> 00:07:16,240
i'm gonna be making or this the set of

219
00:07:16,240 --> 00:07:17,599
arguments we're making let's let's get

220
00:07:17,599 --> 00:07:21,199
into the actual material now

221
00:07:21,280 --> 00:07:23,199
okay so i think we come from diverse

222
00:07:23,199 --> 00:07:24,400
backgrounds here so i just want to

223
00:07:24,400 --> 00:07:26,000
baseline on

224
00:07:26,000 --> 00:07:28,319
what i mean when i say machine learning

225
00:07:28,319 --> 00:07:30,160
let me see if i can point to things yes

226
00:07:30,160 --> 00:07:32,880
i can okay on this slide so so here's

227
00:07:32,880 --> 00:07:35,520
like the basic metaphor for how machine

228
00:07:35,520 --> 00:07:37,360
learning works in security and i'm going

229
00:07:37,360 --> 00:07:38,880
to be talking about sort of the slice of

230
00:07:38,880 --> 00:07:40,560
machine learning that supervised machine

231
00:07:40,560 --> 00:07:42,160
learning that matters most in security

232
00:07:42,160 --> 00:07:43,360
today

233
00:07:43,360 --> 00:07:45,440
basically so in this example we're

234
00:07:45,440 --> 00:07:48,000
building a malware detector

235
00:07:48,000 --> 00:07:49,919
um so if i want to build a machine

236
00:07:49,919 --> 00:07:51,919
learning malware detector i i get a

237
00:07:51,919 --> 00:07:54,560
bunch of examples of malicious software

238
00:07:54,560 --> 00:07:56,479
and benign software let's imagine we

239
00:07:56,479 --> 00:07:58,800
have exe files in this case

240
00:07:58,800 --> 00:07:59,840
uh

241
00:07:59,840 --> 00:08:01,599
i um

242
00:08:01,599 --> 00:08:03,360
and then i extract some numerical

243
00:08:03,360 --> 00:08:05,520
features from from each each one of

244
00:08:05,520 --> 00:08:07,280
those binaries and my goal here is to

245
00:08:07,280 --> 00:08:08,400
build a machine learning system that can

246
00:08:08,400 --> 00:08:10,800
detect previously unseen malware

247
00:08:10,800 --> 00:08:12,160
so in this example i've kept it really

248
00:08:12,160 --> 00:08:14,800
simple um i'm extracting on the vertical

249
00:08:14,800 --> 00:08:17,599
axis the uh a feature i'm calling file

250
00:08:17,599 --> 00:08:18,800
compression it's just a measure of how

251
00:08:18,800 --> 00:08:20,479
compressed each file is might be a proxy

252
00:08:20,479 --> 00:08:22,240
for whether the file is packed

253
00:08:22,240 --> 00:08:24,479
uh on the depth axis i'm extracting the

254
00:08:24,479 --> 00:08:26,720
number of printable strings uh in in the

255
00:08:26,720 --> 00:08:29,360
file maybe maybe fewer means that um you

256
00:08:29,360 --> 00:08:30,639
know the file is less likely to be

257
00:08:30,639 --> 00:08:32,399
obfuscated might be benign and i'm

258
00:08:32,399 --> 00:08:33,839
extracting file size just as another

259
00:08:33,839 --> 00:08:35,039
feature

260
00:08:35,039 --> 00:08:36,479
just to be clear this is this is fake

261
00:08:36,479 --> 00:08:38,320
data you know unfortunately the malware

262
00:08:38,320 --> 00:08:40,958
detection problem isn't so simple right

263
00:08:40,958 --> 00:08:42,719
but um you know what you're seeing here

264
00:08:42,719 --> 00:08:43,919
is that the red dots which are the

265
00:08:43,919 --> 00:08:46,000
malware mostly fall in this region of

266
00:08:46,000 --> 00:08:47,519
what we call the feature space this

267
00:08:47,519 --> 00:08:48,880
three-dimensional volume in which i'm

268
00:08:48,880 --> 00:08:50,240
plotting my data

269
00:08:50,240 --> 00:08:52,399
and uh the benign files mostly fall on

270
00:08:52,399 --> 00:08:54,800
this this in this region of the space

271
00:08:54,800 --> 00:08:57,040
now the machine learning problem here is

272
00:08:57,040 --> 00:08:58,959
is to identify a geometrical structure

273
00:08:58,959 --> 00:09:00,880
that that divides the malware from the

274
00:09:00,880 --> 00:09:02,320
benign wear

275
00:09:02,320 --> 00:09:04,560
so this is we call this a hyperplane in

276
00:09:04,560 --> 00:09:05,839
machine learning

277
00:09:05,839 --> 00:09:06,959
and the mathematical problem is to

278
00:09:06,959 --> 00:09:09,279
identify the um the position of the

279
00:09:09,279 --> 00:09:10,880
plane and its angle so that we're

280
00:09:10,880 --> 00:09:12,399
they're separating between these two

281
00:09:12,399 --> 00:09:14,320
classes and now the way we use the

282
00:09:14,320 --> 00:09:15,600
machine learning system is if we see a

283
00:09:15,600 --> 00:09:19,920
new previously unseen exe file

284
00:09:19,920 --> 00:09:22,000
we just we use some math to check which

285
00:09:22,000 --> 00:09:24,800
side of the of this decision boundary uh

286
00:09:24,800 --> 00:09:26,480
which side of the hyper plane we're on

287
00:09:26,480 --> 00:09:29,040
and then if we're on the malware side uh

288
00:09:29,040 --> 00:09:30,480
you know we defined it as malware we're

289
00:09:30,480 --> 00:09:32,480
found we're on the benign side we did we

290
00:09:32,480 --> 00:09:34,560
defined it as benign where um so okay so

291
00:09:34,560 --> 00:09:36,160
if you so for those of you who are like

292
00:09:36,160 --> 00:09:37,680
new to machine learning if you

293
00:09:37,680 --> 00:09:39,440
understand this basic setup

294
00:09:39,440 --> 00:09:40,720
you're you're most of the way there to

295
00:09:40,720 --> 00:09:42,399
understanding what we do all day as

296
00:09:42,399 --> 00:09:44,240
security data scientists right we we

297
00:09:44,240 --> 00:09:46,800
build models that place

298
00:09:46,800 --> 00:09:48,560
security relevant observations be the

299
00:09:48,560 --> 00:09:50,720
urls or web pages or

300
00:09:50,720 --> 00:09:52,880
email attachments into

301
00:09:52,880 --> 00:09:55,680
a numerical kind of space um and then we

302
00:09:55,680 --> 00:09:57,600
identify these separating boundaries

303
00:09:57,600 --> 00:09:59,440
that sort of

304
00:09:59,440 --> 00:10:00,800
uh

305
00:10:00,800 --> 00:10:02,720
codify the pattern that defines what's

306
00:10:02,720 --> 00:10:05,440
malware and what's benign

307
00:10:05,440 --> 00:10:06,800
now in the real world it's a lot more

308
00:10:06,800 --> 00:10:09,040
complicated than that kind of you know

309
00:10:09,040 --> 00:10:11,680
data science 101 example um we're

310
00:10:11,680 --> 00:10:12,800
actually combining lots of different

311
00:10:12,800 --> 00:10:14,160
technologies when we deploy a machine

312
00:10:14,160 --> 00:10:15,600
learning model

313
00:10:15,600 --> 00:10:17,519
so we're using signatures and allow

314
00:10:17,519 --> 00:10:19,680
lists to suppress

315
00:10:19,680 --> 00:10:22,320
false positives on the on the malware

316
00:10:22,320 --> 00:10:23,519
side and we're using signatures and

317
00:10:23,519 --> 00:10:25,040
block lists

318
00:10:25,040 --> 00:10:26,480
to suppress false negatives on the

319
00:10:26,480 --> 00:10:28,320
benign side

320
00:10:28,320 --> 00:10:29,760
and we actually have to use these

321
00:10:29,760 --> 00:10:32,000
supporting technologies to make a system

322
00:10:32,000 --> 00:10:34,399
um deployable in the real world

323
00:10:34,399 --> 00:10:37,519
um and and to get back to the point uh

324
00:10:37,519 --> 00:10:39,519
around how machine learning is really a

325
00:10:39,519 --> 00:10:42,320
last resort in in security really if

326
00:10:42,320 --> 00:10:44,160
you're designing a security system say

327
00:10:44,160 --> 00:10:47,600
to detect um macro bearing email

328
00:10:47,600 --> 00:10:50,079
attachments uh really machine learning

329
00:10:50,079 --> 00:10:52,079
is the last thing that you that you do

330
00:10:52,079 --> 00:10:54,480
in defining such a system um so here's

331
00:10:54,480 --> 00:10:57,600
here's a notional but fairly accurate

332
00:10:57,600 --> 00:10:59,279
representation of the way

333
00:10:59,279 --> 00:11:01,920
that we would detect macro bearing

334
00:11:01,920 --> 00:11:04,000
malware um like let's say from the

335
00:11:04,000 --> 00:11:07,440
office suite uh at sofos so first

336
00:11:07,440 --> 00:11:09,200
we would just decide whether or not the

337
00:11:09,200 --> 00:11:10,720
observation so the observation being a

338
00:11:10,720 --> 00:11:13,040
document here isn't an allow is in an

339
00:11:13,040 --> 00:11:15,440
allow list like is this is this document

340
00:11:15,440 --> 00:11:17,839
in an email from a trusted sender and if

341
00:11:17,839 --> 00:11:20,320
so then we just allow the documents

342
00:11:20,320 --> 00:11:22,160
then we decide if it violates policy

343
00:11:22,160 --> 00:11:24,800
maybe it may be maybe the

344
00:11:24,800 --> 00:11:26,000
visual basic

345
00:11:26,000 --> 00:11:27,760
code inside the document does something

346
00:11:27,760 --> 00:11:28,959
that's just policy violating that we

347
00:11:28,959 --> 00:11:30,640
don't allow it our company if so we just

348
00:11:30,640 --> 00:11:31,600
block it

349
00:11:31,600 --> 00:11:32,959
then we check if the sender is in a

350
00:11:32,959 --> 00:11:34,959
block list like a known malicious

351
00:11:34,959 --> 00:11:37,839
emailer if so we block it

352
00:11:37,839 --> 00:11:39,279
so we make all these sort of really

353
00:11:39,279 --> 00:11:41,519
obvious decisions first when we get to

354
00:11:41,519 --> 00:11:42,560
the end

355
00:11:42,560 --> 00:11:44,000
um

356
00:11:44,000 --> 00:11:45,680
that's when we apply on on the tiny

357
00:11:45,680 --> 00:11:48,240
trickle of documents that have made it

358
00:11:48,240 --> 00:11:50,880
through this gauntlet of filters um

359
00:11:50,880 --> 00:11:52,399
these are these are these are document

360
00:11:52,399 --> 00:11:53,839
files that are in the gray area where we

361
00:11:53,839 --> 00:11:55,040
really don't know if they're malicious

362
00:11:55,040 --> 00:11:56,639
or benign that's the point in which we

363
00:11:56,639 --> 00:11:58,160
def that we we apply a machine learning

364
00:11:58,160 --> 00:12:00,320
model and decide if it's good or bad um

365
00:12:00,320 --> 00:12:02,079
this is typically so sometimes you'll

366
00:12:02,079 --> 00:12:03,200
hear from security companies that

367
00:12:03,200 --> 00:12:05,600
they're ai based or they they're ai you

368
00:12:05,600 --> 00:12:07,440
know that they're an ai first security

369
00:12:07,440 --> 00:12:09,279
company or something the reality is

370
00:12:09,279 --> 00:12:10,800
nobody deploys

371
00:12:10,800 --> 00:12:12,079
um or they

372
00:12:12,079 --> 00:12:13,279
if they do they shouldn't machine

373
00:12:13,279 --> 00:12:15,200
learning models uh without all of these

374
00:12:15,200 --> 00:12:17,680
guard rails and checks um that's really

375
00:12:17,680 --> 00:12:19,200
the place that machine learning lives in

376
00:12:19,200 --> 00:12:22,079
today's security pipelines

377
00:12:22,079 --> 00:12:25,760
um okay so why why aren't practitioners

378
00:12:25,760 --> 00:12:28,160
who do security data science uh just

379
00:12:28,160 --> 00:12:29,680
building machine learning models that

380
00:12:29,680 --> 00:12:31,519
detect malware versus benign where say

381
00:12:31,519 --> 00:12:32,959
or malicious documents versus benign

382
00:12:32,959 --> 00:12:34,560
documents or malicious behaviors versus

383
00:12:34,560 --> 00:12:36,800
benign behaviors um and just leaving it

384
00:12:36,800 --> 00:12:38,399
at that and why why are people using

385
00:12:38,399 --> 00:12:40,560
these guardrails and auxiliary

386
00:12:40,560 --> 00:12:42,639
technologies and the reason that the

387
00:12:42,639 --> 00:12:44,480
reasons are at least you know these are

388
00:12:44,480 --> 00:12:46,720
some of the reasons uh um

389
00:12:46,720 --> 00:12:48,000
first of all

390
00:12:48,000 --> 00:12:49,440
machine learning is really hard to

391
00:12:49,440 --> 00:12:50,720
control

392
00:12:50,720 --> 00:12:53,760
so every time at sofos we train a new

393
00:12:53,760 --> 00:12:55,760
machine learning model um

394
00:12:55,760 --> 00:12:57,839
or actually every time we retrain a

395
00:12:57,839 --> 00:12:59,040
machine learning model that we have out

396
00:12:59,040 --> 00:13:00,959
in deployment on new data we really

397
00:13:00,959 --> 00:13:02,000
don't know

398
00:13:02,000 --> 00:13:02,959
uh

399
00:13:02,959 --> 00:13:04,399
what kind of animal we're going to get

400
00:13:04,399 --> 00:13:06,160
out of that training process

401
00:13:06,160 --> 00:13:08,240
maybe all of a sudden this new model

402
00:13:08,240 --> 00:13:10,560
will like in the malware case fp on

403
00:13:10,560 --> 00:13:12,880
system dll files uh that that the

404
00:13:12,880 --> 00:13:15,600
previous model hadn't been fping on um

405
00:13:15,600 --> 00:13:17,440
maybe it'll false negative on a lineage

406
00:13:17,440 --> 00:13:18,959
of malware that the previous version

407
00:13:18,959 --> 00:13:21,279
detected um there's sort of a random

408
00:13:21,279 --> 00:13:23,200
black box nature to the way machine

409
00:13:23,200 --> 00:13:24,959
learning works in practice that means

410
00:13:24,959 --> 00:13:27,279
it's very hard to control

411
00:13:27,279 --> 00:13:29,519
um it's also very hard to understand

412
00:13:29,519 --> 00:13:32,720
machine learning models um and so

413
00:13:32,720 --> 00:13:33,760
and that's the point about them being

414
00:13:33,760 --> 00:13:35,920
opaque and basically possible to explain

415
00:13:35,920 --> 00:13:36,720
uh

416
00:13:36,720 --> 00:13:38,480
you know so if if our machine learning

417
00:13:38,480 --> 00:13:41,920
models are say fping on sports web pages

418
00:13:41,920 --> 00:13:43,199
in the case of machine learning models

419
00:13:43,199 --> 00:13:45,920
that look for malicious web pages um

420
00:13:45,920 --> 00:13:48,000
it's extremely hard for us to explain er

421
00:13:48,000 --> 00:13:50,079
not impossible to explain why that's

422
00:13:50,079 --> 00:13:52,240
happening um we can have some theories

423
00:13:52,240 --> 00:13:53,040
um

424
00:13:53,040 --> 00:13:54,480
but um

425
00:13:54,480 --> 00:13:55,600
modern machine learning models are

426
00:13:55,600 --> 00:13:57,440
usually black boxes

427
00:13:57,440 --> 00:13:58,560
that makes deployment really hard it

428
00:13:58,560 --> 00:13:59,680
means that we have to do really really

429
00:13:59,680 --> 00:14:00,959
rigorous testing on our machine learning

430
00:14:00,959 --> 00:14:02,720
models before we send them out to our

431
00:14:02,720 --> 00:14:05,600
tens of millions of endpoints

432
00:14:05,600 --> 00:14:07,440
um and it's also just hard because of

433
00:14:07,440 --> 00:14:08,720
because of the reasons i just stated

434
00:14:08,720 --> 00:14:10,399
it's it's hard to redeploy machine

435
00:14:10,399 --> 00:14:11,760
learning models because every time we

436
00:14:11,760 --> 00:14:13,199
retrain them

437
00:14:13,199 --> 00:14:14,720
we're not quite sure what we want to get

438
00:14:14,720 --> 00:14:16,720
we have to go through a whole battery of

439
00:14:16,720 --> 00:14:18,240
tests before we we're actually

440
00:14:18,240 --> 00:14:19,440
comfortable sending them out into the

441
00:14:19,440 --> 00:14:21,199
wilds where they're blocking files from

442
00:14:21,199 --> 00:14:23,120
executing and doing potentially

443
00:14:23,120 --> 00:14:24,320
dangerous things in our customers

444
00:14:24,320 --> 00:14:27,680
computers so in contrast

445
00:14:27,680 --> 00:14:29,040
like older fashion methods like

446
00:14:29,040 --> 00:14:31,279
signatures and policies are really clean

447
00:14:31,279 --> 00:14:33,360
and easy to operate relative to machine

448
00:14:33,360 --> 00:14:34,720
learning

449
00:14:34,720 --> 00:14:37,120
when i define a signature that detects a

450
00:14:37,120 --> 00:14:39,040
certain malware family

451
00:14:39,040 --> 00:14:42,079
i know exactly what i'm writing

452
00:14:42,079 --> 00:14:43,600
as opposed to opaque

453
00:14:43,600 --> 00:14:45,279
machine learning models the outcomes are

454
00:14:45,279 --> 00:14:47,360
well controlled the signatures that we

455
00:14:47,360 --> 00:14:48,560
write and the policies we write are

456
00:14:48,560 --> 00:14:51,360
explainable um and as a result

457
00:14:51,360 --> 00:14:52,880
signatures and policies are just these

458
00:14:52,880 --> 00:14:54,880
tactical swiss army knives uh like it

459
00:14:54,880 --> 00:14:57,040
like at sophos we redeploy we deploy new

460
00:14:57,040 --> 00:14:58,800
signatures on an almost daily basis to

461
00:14:58,800 --> 00:15:00,160
our customer base whereas machine

462
00:15:00,160 --> 00:15:01,600
learning models we deploy about once a

463
00:15:01,600 --> 00:15:03,920
quarter

464
00:15:04,399 --> 00:15:06,399
um so

465
00:15:06,399 --> 00:15:07,760
i'm sort of bashing machine learning

466
00:15:07,760 --> 00:15:08,800
here

467
00:15:08,800 --> 00:15:10,240
which is ironic i'm a security data

468
00:15:10,240 --> 00:15:11,519
scientist

469
00:15:11,519 --> 00:15:13,279
i want to talk about why we we almost

470
00:15:13,279 --> 00:15:14,560
always deploy machine learning models in

471
00:15:14,560 --> 00:15:15,839
our systems even though they're so hard

472
00:15:15,839 --> 00:15:17,199
to deploy

473
00:15:17,199 --> 00:15:19,360
and really it comes down to

474
00:15:19,360 --> 00:15:20,959
efficacy

475
00:15:20,959 --> 00:15:23,839
so here's here's i think an example of

476
00:15:23,839 --> 00:15:25,440
what i'm talking about

477
00:15:25,440 --> 00:15:27,279
this is a timeline showing the accuracy

478
00:15:27,279 --> 00:15:29,279
of our windows malware detection system

479
00:15:29,279 --> 00:15:31,040
that we have on our in our endpoint

480
00:15:31,040 --> 00:15:32,959
product at sofos

481
00:15:32,959 --> 00:15:34,079
and

482
00:15:34,079 --> 00:15:36,240
what you're seeing here is that

483
00:15:36,240 --> 00:15:38,160
in this

484
00:15:38,160 --> 00:15:41,040
i guess six day period in in july

485
00:15:41,040 --> 00:15:42,000
um

486
00:15:42,000 --> 00:15:44,079
our our signatures caught

487
00:15:44,079 --> 00:15:45,600
to the best of our ability to estimate

488
00:15:45,600 --> 00:15:47,519
about 90 percent of malware we observed

489
00:15:47,519 --> 00:15:48,800
on our endpoints

490
00:15:48,800 --> 00:15:52,160
um there was a dip here on july 8th um

491
00:15:52,160 --> 00:15:54,720
our machine learning system uh detected

492
00:15:54,720 --> 00:15:57,120
about 95 of the malware sometimes a

493
00:15:57,120 --> 00:15:58,639
little bit better and when we combine

494
00:15:58,639 --> 00:16:00,880
those systems together um we did the

495
00:16:00,880 --> 00:16:03,279
best you know of all three scenarios uh

496
00:16:03,279 --> 00:16:06,240
you know 96 to 98 detection um so the

497
00:16:06,240 --> 00:16:07,680
reason security companies are moving to

498
00:16:07,680 --> 00:16:09,120
using machine learning

499
00:16:09,120 --> 00:16:10,240
is that

500
00:16:10,240 --> 00:16:11,519
it's not that it's a silver bullet it's

501
00:16:11,519 --> 00:16:13,759
that this is to squeeze the the sort of

502
00:16:13,759 --> 00:16:14,639
last

503
00:16:14,639 --> 00:16:17,440
five to ten percent of accuracy um

504
00:16:17,440 --> 00:16:19,199
out of our detection pipelines you

505
00:16:19,199 --> 00:16:20,480
really need machine learning this is

506
00:16:20,480 --> 00:16:21,680
what i think all of our colleagues

507
00:16:21,680 --> 00:16:23,519
across many companies are finding so

508
00:16:23,519 --> 00:16:24,560
machine learning is very difficult to

509
00:16:24,560 --> 00:16:26,480
deploy there's lots of downsides to it

510
00:16:26,480 --> 00:16:28,639
um but it provides added value on top of

511
00:16:28,639 --> 00:16:30,079
all the technologies we had before we

512
00:16:30,079 --> 00:16:31,600
sort of took this machine learning turn

513
00:16:31,600 --> 00:16:34,800
in in cyber security

514
00:16:35,440 --> 00:16:37,279
okay so

515
00:16:37,279 --> 00:16:39,440
that's that's my intro um now i wanted

516
00:16:39,440 --> 00:16:40,639
to go through

517
00:16:40,639 --> 00:16:41,920
our journey

518
00:16:41,920 --> 00:16:44,160
on our team uh around building a machine

519
00:16:44,160 --> 00:16:46,240
learning program uh over the last five

520
00:16:46,240 --> 00:16:48,240
years just as a kid not to not to

521
00:16:48,240 --> 00:16:50,079
promote ourselves but as a case study

522
00:16:50,079 --> 00:16:51,680
just as we have insider knowledge here

523
00:16:51,680 --> 00:16:54,240
that we want to share with the community

524
00:16:54,240 --> 00:16:56,240
um and happy to take questions when i

525
00:16:56,240 --> 00:16:58,079
get through this section uh around any

526
00:16:58,079 --> 00:16:59,680
details of sort of what we've done and

527
00:16:59,680 --> 00:17:02,839
lessons we've learned uh from this this

528
00:17:02,839 --> 00:17:05,039
process now

529
00:17:05,039 --> 00:17:07,520
five years ago in 2017

530
00:17:07,520 --> 00:17:09,520
uh well the beginning of that year we

531
00:17:09,520 --> 00:17:11,679
had zero machine learning models in

532
00:17:11,679 --> 00:17:14,400
deployment uh at that sophos across our

533
00:17:14,400 --> 00:17:16,400
100 million or so end points at that

534
00:17:16,400 --> 00:17:17,520
point in time

535
00:17:17,520 --> 00:17:19,039
um and then

536
00:17:19,039 --> 00:17:20,799
since then uh

537
00:17:20,799 --> 00:17:23,280
you know up through 2022

538
00:17:23,280 --> 00:17:24,880
we've integrated machine learning all

539
00:17:24,880 --> 00:17:26,079
throughout our portfolio so at this

540
00:17:26,079 --> 00:17:27,599
point we've got more than

541
00:17:27,599 --> 00:17:29,760
30 machine learning product interface

542
00:17:29,760 --> 00:17:32,160
integrations at sofos and that's and

543
00:17:32,160 --> 00:17:33,600
that's sort of swept across our whole

544
00:17:33,600 --> 00:17:35,360
portfolio

545
00:17:35,360 --> 00:17:37,120
um so this is just a chart showing sort

546
00:17:37,120 --> 00:17:39,360
of where uh where we've

547
00:17:39,360 --> 00:17:40,400
where we've deployed machine learning

548
00:17:40,400 --> 00:17:42,000
and how many integrations per product

549
00:17:42,000 --> 00:17:45,440
category we we have within our portfolio

550
00:17:45,440 --> 00:17:47,520
um so what exactly are we doing mitch at

551
00:17:47,520 --> 00:17:49,679
with machine learning uh at sofos and i

552
00:17:49,679 --> 00:17:51,600
just want to say um

553
00:17:51,600 --> 00:17:52,880
that

554
00:17:52,880 --> 00:17:54,799
i've got friends at other large security

555
00:17:54,799 --> 00:17:56,000
companies too and they're they're doing

556
00:17:56,000 --> 00:17:57,120
similar things with machine learning so

557
00:17:57,120 --> 00:17:58,240
this is fairly representative of where

558
00:17:58,240 --> 00:17:59,600
machine learning is being used in the

559
00:17:59,600 --> 00:18:02,000
security industry in general so our

560
00:18:02,000 --> 00:18:03,919
first use case with machine learning was

561
00:18:03,919 --> 00:18:05,679
to detect windows malware that's like

562
00:18:05,679 --> 00:18:07,679
the the big heavy hitting case in which

563
00:18:07,679 --> 00:18:09,280
machine learning is getting used in lots

564
00:18:09,280 --> 00:18:12,400
of different places not just at sofos um

565
00:18:12,400 --> 00:18:15,039
then we moved on to to build models that

566
00:18:15,039 --> 00:18:17,280
detect malicious malicious email

567
00:18:17,280 --> 00:18:19,200
attachments like excel powerpoint and

568
00:18:19,200 --> 00:18:21,200
word documents uh we have machine

569
00:18:21,200 --> 00:18:23,600
learning that operates on on on android

570
00:18:23,600 --> 00:18:25,360
endpoint devices detecting malicious

571
00:18:25,360 --> 00:18:27,840
apps um a bunch of different detection

572
00:18:27,840 --> 00:18:30,960
models that detect um basically the the

573
00:18:30,960 --> 00:18:32,080
major having

574
00:18:32,080 --> 00:18:34,720
hitting um categories of threat vector

575
00:18:34,720 --> 00:18:35,600
types

576
00:18:35,600 --> 00:18:37,760
um we also use machine learning in the

577
00:18:37,760 --> 00:18:39,919
context of web content filtering so we

578
00:18:39,919 --> 00:18:42,480
have a model that looks at urls for data

579
00:18:42,480 --> 00:18:43,600
scientists in the audience this is a

580
00:18:43,600 --> 00:18:44,880
character level convolutional neural

581
00:18:44,880 --> 00:18:47,440
network and then we have another model

582
00:18:47,440 --> 00:18:49,039
that's if people are interested in the

583
00:18:49,039 --> 00:18:50,720
paper reference happy to share but it's

584
00:18:50,720 --> 00:18:52,320
sort of convolution-like that operates

585
00:18:52,320 --> 00:18:53,520
on

586
00:18:53,520 --> 00:18:57,360
on html javascript and css

587
00:18:57,360 --> 00:18:58,640
then we have some models that we use to

588
00:18:58,640 --> 00:19:00,880
help explain

589
00:19:00,880 --> 00:19:03,360
detection results to users that that for

590
00:19:03,360 --> 00:19:05,280
example when we detect a file as

591
00:19:05,280 --> 00:19:07,600
malicious show the most similar files in

592
00:19:07,600 --> 00:19:09,600
our database that are also

593
00:19:09,600 --> 00:19:11,280
malicious versus the most similar files

594
00:19:11,280 --> 00:19:12,559
that are benign and let users sort of

595
00:19:12,559 --> 00:19:16,480
understand uh why we detected that file

596
00:19:16,480 --> 00:19:18,080
and models that we're working on

597
00:19:18,080 --> 00:19:19,679
currently uh

598
00:19:19,679 --> 00:19:20,880
are

599
00:19:20,880 --> 00:19:23,440
more user-facing so you'll hear a bit

600
00:19:23,440 --> 00:19:26,960
more um at b-sides from our team about

601
00:19:26,960 --> 00:19:28,880
an alert prioritization model we also

602
00:19:28,880 --> 00:19:30,160
call this a case escalation prediction

603
00:19:30,160 --> 00:19:31,440
model that operates in a kind of

604
00:19:31,440 --> 00:19:32,880
feedback loop between analysts and the

605
00:19:32,880 --> 00:19:34,480
machine learning model and predicts

606
00:19:34,480 --> 00:19:36,240
which alerts stock analysts will decide

607
00:19:36,240 --> 00:19:38,080
are truly malicious and we're working on

608
00:19:38,080 --> 00:19:39,760
some natural language interface stuff

609
00:19:39,760 --> 00:19:41,360
also with machine learning so that's

610
00:19:41,360 --> 00:19:43,679
just sort of the sweep of places that we

611
00:19:43,679 --> 00:19:47,679
are applying machine learning at sofos

612
00:19:48,160 --> 00:19:50,880
so not all problems in security

613
00:19:50,880 --> 00:19:52,320
i'm really far from it our machine

614
00:19:52,320 --> 00:19:53,840
learning problems are appropriate to

615
00:19:53,840 --> 00:19:54,960
solve

616
00:19:54,960 --> 00:19:56,880
machine learning are appropriate to

617
00:19:56,880 --> 00:19:58,559
apply machine learning to

618
00:19:58,559 --> 00:20:00,559
so here are some criteria that we use to

619
00:20:00,559 --> 00:20:03,440
define what constitutes a

620
00:20:03,440 --> 00:20:05,440
a good nail for the machine learning

621
00:20:05,440 --> 00:20:06,960
hammer

622
00:20:06,960 --> 00:20:09,200
so one the distribution has to be

623
00:20:09,200 --> 00:20:11,679
relatively stable so what do i mean by

624
00:20:11,679 --> 00:20:13,919
that so um

625
00:20:13,919 --> 00:20:16,159
so the distribution of like malicious pe

626
00:20:16,159 --> 00:20:19,840
files versus benign pe files for example

627
00:20:19,840 --> 00:20:21,440
even though that changes over time as

628
00:20:21,440 --> 00:20:23,360
attackers of all their behavior it's a

629
00:20:23,360 --> 00:20:25,840
fairly slow moving change that we see

630
00:20:25,840 --> 00:20:27,679
and what we've found in our research is

631
00:20:27,679 --> 00:20:29,919
that things change slowly enough in that

632
00:20:29,919 --> 00:20:31,840
world and we have enough data

633
00:20:31,840 --> 00:20:33,679
that we can model that problem and ship

634
00:20:33,679 --> 00:20:36,320
a successful machine learning model um

635
00:20:36,320 --> 00:20:38,799
an example of a of a distribution a

636
00:20:38,799 --> 00:20:40,320
distribution that doesn't fit these

637
00:20:40,320 --> 00:20:41,919
criteria are

638
00:20:41,919 --> 00:20:43,280
for example

639
00:20:43,280 --> 00:20:44,159
um

640
00:20:44,159 --> 00:20:46,000
insider threats where you're trying to

641
00:20:46,000 --> 00:20:48,559
detect whether or not a user is stealing

642
00:20:48,559 --> 00:20:50,880
data from a company

643
00:20:50,880 --> 00:20:54,159
based on their user interaction behavior

644
00:20:54,159 --> 00:20:55,840
in our opinion

645
00:20:55,840 --> 00:20:57,440
humans are too unpredictable we don't

646
00:20:57,440 --> 00:21:00,320
have enough training examples

647
00:21:00,320 --> 00:21:02,640
and so less appropriate as a problem for

648
00:21:02,640 --> 00:21:04,000
machine learning

649
00:21:04,000 --> 00:21:06,159
we also need reasonable deployment and

650
00:21:06,159 --> 00:21:09,039
operational complexity so

651
00:21:09,039 --> 00:21:11,520
for example we've thought about shipping

652
00:21:11,520 --> 00:21:13,039
models that look at a whole bunch of

653
00:21:13,039 --> 00:21:15,360
different factors uh spread out over

654
00:21:15,360 --> 00:21:17,039
sort of different different

655
00:21:17,039 --> 00:21:20,880
layers of the compute stack

656
00:21:20,880 --> 00:21:23,360
all at once so for example

657
00:21:23,360 --> 00:21:25,330
you might want to look at a

658
00:21:25,330 --> 00:21:26,480
[Music]

659
00:21:26,480 --> 00:21:29,440
i don't know like a pe files behavior in

660
00:21:29,440 --> 00:21:31,679
conjunction with its static features in

661
00:21:31,679 --> 00:21:33,280
conjunction with like the url it was

662
00:21:33,280 --> 00:21:34,559
downloaded from

663
00:21:34,559 --> 00:21:35,360
um

664
00:21:35,360 --> 00:21:36,960
that sounds like a good idea to re to

665
00:21:36,960 --> 00:21:38,080
have a machine learning model reason

666
00:21:38,080 --> 00:21:39,919
about all of those those observations

667
00:21:39,919 --> 00:21:41,280
all in one place and make it like an

668
00:21:41,280 --> 00:21:43,120
optimal decision the reality is it's

669
00:21:43,120 --> 00:21:45,440
incredibly hard to get all of the data

670
00:21:45,440 --> 00:21:47,520
feeds required it's integrated in one

671
00:21:47,520 --> 00:21:49,280
place in order to build such a model it

672
00:21:49,280 --> 00:21:51,039
would be very hard to actually do the

673
00:21:51,039 --> 00:21:53,200
engineering work to integrate a model

674
00:21:53,200 --> 00:21:56,159
across all across across all observation

675
00:21:56,159 --> 00:21:57,520
points that you'd need to deploy such a

676
00:21:57,520 --> 00:21:59,520
model um just too complex we found

677
00:21:59,520 --> 00:22:00,880
keeping things really simple and just

678
00:22:00,880 --> 00:22:02,559
looking at like a certain file type or

679
00:22:02,559 --> 00:22:05,280
like look at web pages um and train and

680
00:22:05,280 --> 00:22:06,799
training a model in isolation on those

681
00:22:06,799 --> 00:22:08,640
observables well it might not seem

682
00:22:08,640 --> 00:22:10,640
conceptually as optimal um it turns out

683
00:22:10,640 --> 00:22:11,679
to be the right approach just because of

684
00:22:11,679 --> 00:22:13,760
the operational complexities

685
00:22:13,760 --> 00:22:15,360
um so there's just some examples of how

686
00:22:15,360 --> 00:22:17,760
we think about where it's appropriate to

687
00:22:17,760 --> 00:22:20,879
apply machine learning

688
00:22:21,600 --> 00:22:23,200
okay so here's here's our current

689
00:22:23,200 --> 00:22:24,960
organizational structure which i think

690
00:22:24,960 --> 00:22:27,200
is important to talk about um

691
00:22:27,200 --> 00:22:28,159
so

692
00:22:28,159 --> 00:22:29,760
we have a research department on our

693
00:22:29,760 --> 00:22:32,400
team so we've got about i think um

694
00:22:32,400 --> 00:22:34,240
30-something people on our team right

695
00:22:34,240 --> 00:22:35,919
now uh we have a research department

696
00:22:35,919 --> 00:22:36,960
you're gonna be hearing from a bunch of

697
00:22:36,960 --> 00:22:39,840
these people at

698
00:22:39,840 --> 00:22:40,799
uh

699
00:22:40,799 --> 00:22:43,600
that's about a dozen people or so

700
00:22:43,600 --> 00:22:45,440
uh we have two programming so so i think

701
00:22:45,440 --> 00:22:47,679
when people think about machine learning

702
00:22:47,679 --> 00:22:49,200
and security they mostly think about

703
00:22:49,200 --> 00:22:50,799
applied mathematicians and researchers

704
00:22:50,799 --> 00:22:52,960
um but i think that the the the one

705
00:22:52,960 --> 00:22:54,640
takeaway from this chart here is that

706
00:22:54,640 --> 00:22:56,080
actually the majority of our team are

707
00:22:56,080 --> 00:22:57,520
not applied mathematicians and

708
00:22:57,520 --> 00:22:59,200
researchers um we've got these other

709
00:22:59,200 --> 00:23:01,840
categories of um professionals on our

710
00:23:01,840 --> 00:23:04,480
team an important team here is program

711
00:23:04,480 --> 00:23:05,840
management so we have we have two

712
00:23:05,840 --> 00:23:07,679
program managers and an intern there

713
00:23:07,679 --> 00:23:08,880
right now

714
00:23:08,880 --> 00:23:11,760
that's extremely important in

715
00:23:11,760 --> 00:23:13,600
a company that's shipping security

716
00:23:13,600 --> 00:23:16,480
machine learning models because of

717
00:23:16,480 --> 00:23:18,159
where machine learning sits within a

718
00:23:18,159 --> 00:23:20,080
large engineering organization basically

719
00:23:20,080 --> 00:23:23,120
in order to do our jobs we need to

720
00:23:23,120 --> 00:23:25,280
pull down data from

721
00:23:25,280 --> 00:23:27,200
probably depending on how you count a

722
00:23:27,200 --> 00:23:29,360
dozen other engineering

723
00:23:29,360 --> 00:23:32,480
teams within our company

724
00:23:32,480 --> 00:23:33,600
use those

725
00:23:33,600 --> 00:23:35,600
understand those data

726
00:23:35,600 --> 00:23:37,360
load them into the database that we use

727
00:23:37,360 --> 00:23:38,960
to train train and monitor all of our

728
00:23:38,960 --> 00:23:40,799
models

729
00:23:40,799 --> 00:23:41,679
and then

730
00:23:41,679 --> 00:23:42,799
once we trained our models then we

731
00:23:42,799 --> 00:23:44,080
shipped those out to like a dozen

732
00:23:44,080 --> 00:23:46,880
different teams so we are a central node

733
00:23:46,880 --> 00:23:48,159
in this in this sort of web of

734
00:23:48,159 --> 00:23:50,480
dependencies and it's because we need to

735
00:23:50,480 --> 00:23:51,919
get data from places then we just ship

736
00:23:51,919 --> 00:23:53,120
model to about models to a bunch of

737
00:23:53,120 --> 00:23:54,799
other places and so we need program

738
00:23:54,799 --> 00:23:56,559
managers to help tame the chaos of

739
00:23:56,559 --> 00:23:59,360
actually doing that in a reasonable way

740
00:23:59,360 --> 00:24:01,120
and then we have this large at least

741
00:24:01,120 --> 00:24:02,960
relative to our research and program

742
00:24:02,960 --> 00:24:04,720
management teams engineering

743
00:24:04,720 --> 00:24:07,440
organization that's responsible for

744
00:24:07,440 --> 00:24:08,480
uh

745
00:24:08,480 --> 00:24:09,919
integrating lots and lots of different

746
00:24:09,919 --> 00:24:11,360
data feeds

747
00:24:11,360 --> 00:24:12,960
from lots of different places from our

748
00:24:12,960 --> 00:24:13,760
from our

749
00:24:13,760 --> 00:24:15,760
from feedback from our products

750
00:24:15,760 --> 00:24:17,279
uh third-party threat feeds that we

751
00:24:17,279 --> 00:24:19,360
subscribe to lots of different places

752
00:24:19,360 --> 00:24:20,880
loading that into a very large scale

753
00:24:20,880 --> 00:24:23,919
sort of petabyte scale database

754
00:24:23,919 --> 00:24:25,360
retraining models

755
00:24:25,360 --> 00:24:26,559
and there's that's really where most of

756
00:24:26,559 --> 00:24:28,159
the work of an operational security data

757
00:24:28,159 --> 00:24:29,600
science team happens which is why this

758
00:24:29,600 --> 00:24:33,559
is a fairly large organization

759
00:24:33,919 --> 00:24:35,600
and here's here's just a really high

760
00:24:35,600 --> 00:24:38,080
level diagram of our data architecture i

761
00:24:38,080 --> 00:24:39,120
think the one thing i want you guys to

762
00:24:39,120 --> 00:24:41,919
take away from this here is that

763
00:24:41,919 --> 00:24:44,320
it's most of the complexity of our team

764
00:24:44,320 --> 00:24:47,120
exists in this data architecture

765
00:24:47,120 --> 00:24:48,640
what you're seeing here is that we pull

766
00:24:48,640 --> 00:24:50,159
down lots of data feeds like i've been

767
00:24:50,159 --> 00:24:51,760
saying it goes through a complex

768
00:24:51,760 --> 00:24:54,720
infrastructure ends up in snowflake

769
00:24:54,720 --> 00:24:55,600
and then there's a whole bunch of

770
00:24:55,600 --> 00:24:57,039
compute infrastructure that we're using

771
00:24:57,039 --> 00:24:59,200
to train and evaluate and package up and

772
00:24:59,200 --> 00:25:03,279
ship our models on on the right side

773
00:25:04,480 --> 00:25:06,480
we have a we have a dashboard

774
00:25:06,480 --> 00:25:08,480
that's constantly evolving but that we

775
00:25:08,480 --> 00:25:11,679
use to monitor model accuracy

776
00:25:11,679 --> 00:25:13,039
and

777
00:25:13,039 --> 00:25:14,960
our ops team is sort of looking at that

778
00:25:14,960 --> 00:25:17,039
and looking for dips in accuracy and

779
00:25:17,039 --> 00:25:19,760
when we have an issue right we spot it

780
00:25:19,760 --> 00:25:21,360
and we work on retraining or

781
00:25:21,360 --> 00:25:23,279
understanding what the issue is

782
00:25:23,279 --> 00:25:24,880
and getting a new model out there that

783
00:25:24,880 --> 00:25:28,080
works better

784
00:25:28,080 --> 00:25:30,480
a big part of our our team's

785
00:25:30,480 --> 00:25:31,440
uh

786
00:25:31,440 --> 00:25:33,120
sort of routine work

787
00:25:33,120 --> 00:25:35,440
is retraining all of those models that

788
00:25:35,440 --> 00:25:37,440
we have out in deployment so we try to

789
00:25:37,440 --> 00:25:39,679
retrain them once a quarter

790
00:25:39,679 --> 00:25:41,200
after a quarter or so their accuracy

791
00:25:41,200 --> 00:25:42,960
really starts to degrade because they

792
00:25:42,960 --> 00:25:44,720
are modeling

793
00:25:44,720 --> 00:25:46,400
observations of benign and malicious

794
00:25:46,400 --> 00:25:48,159
observations in the past and obviously

795
00:25:48,159 --> 00:25:50,159
cyber security changes really rapidly

796
00:25:50,159 --> 00:25:51,679
this is where we monitor

797
00:25:51,679 --> 00:25:53,760
our retraining schedule and confluence

798
00:25:53,760 --> 00:25:57,240
within our team

799
00:25:57,600 --> 00:25:59,840
um and then finally for the for those of

800
00:25:59,840 --> 00:26:02,000
you curious about the actual math behind

801
00:26:02,000 --> 00:26:03,120
our machine learning models and i'm not

802
00:26:03,120 --> 00:26:04,400
going to spend too much time talking

803
00:26:04,400 --> 00:26:06,080
about that here are just some example

804
00:26:06,080 --> 00:26:07,679
architectures that we that we are

805
00:26:07,679 --> 00:26:09,600
currently shipping

806
00:26:09,600 --> 00:26:11,440
so to detect

807
00:26:11,440 --> 00:26:13,840
malicious urls

808
00:26:13,840 --> 00:26:15,279
we're using a convolutional neural

809
00:26:15,279 --> 00:26:18,240
network and basically the way this works

810
00:26:18,240 --> 00:26:20,400
is

811
00:26:20,400 --> 00:26:21,840
we

812
00:26:21,840 --> 00:26:24,240
at a high level the neural network looks

813
00:26:24,240 --> 00:26:27,520
at the raw character string of a url

814
00:26:27,520 --> 00:26:28,400
and

815
00:26:28,400 --> 00:26:30,159
learns what patterns are important in

816
00:26:30,159 --> 00:26:31,840
terms of detecting malicious urls and

817
00:26:31,840 --> 00:26:33,039
makes a prediction as to whether or not

818
00:26:33,039 --> 00:26:34,799
the url is malicious or not at a lower

819
00:26:34,799 --> 00:26:36,640
level what we're doing is

820
00:26:36,640 --> 00:26:39,200
we are sliding these

821
00:26:39,200 --> 00:26:40,240
um

822
00:26:40,240 --> 00:26:42,320
these kind of windows over the character

823
00:26:42,320 --> 00:26:45,679
string and we have 1024 neurons that

824
00:26:45,679 --> 00:26:47,039
sort of slide over the character string

825
00:26:47,039 --> 00:26:48,240
looking for patterns you can think of

826
00:26:48,240 --> 00:26:49,840
them as kind of like elements of a

827
00:26:49,840 --> 00:26:51,360
regular expression

828
00:26:51,360 --> 00:26:52,480
and they activate when they find

829
00:26:52,480 --> 00:26:54,240
something of interest and then we have

830
00:26:54,240 --> 00:26:55,679
we then we have some feed forward layers

831
00:26:55,679 --> 00:26:57,120
that sort of you can think of them as

832
00:26:57,120 --> 00:27:00,240
doing a kind of non-discrete if like if

833
00:27:00,240 --> 00:27:01,919
then else logic over the patterns that

834
00:27:01,919 --> 00:27:03,200
have been identified by the

835
00:27:03,200 --> 00:27:05,120
convolutional units

836
00:27:05,120 --> 00:27:06,640
and they decide whether or not the url

837
00:27:06,640 --> 00:27:07,840
is malicious or not so that's just one

838
00:27:07,840 --> 00:27:10,000
example architecture

839
00:27:10,000 --> 00:27:11,279
we've got papers about all of these

840
00:27:11,279 --> 00:27:12,640
models out there if you guys are

841
00:27:12,640 --> 00:27:14,080
interested in looking

842
00:27:14,080 --> 00:27:15,919
happy to share it's probably hard to

843
00:27:15,919 --> 00:27:17,840
write down these links

844
00:27:17,840 --> 00:27:19,520
from the slides but happy to share links

845
00:27:19,520 --> 00:27:20,559
for anybody who's interested enough

846
00:27:20,559 --> 00:27:22,159
after the presentation

847
00:27:22,159 --> 00:27:24,240
we've got another model which is a kind

848
00:27:24,240 --> 00:27:25,760
of original architecture we came up with

849
00:27:25,760 --> 00:27:28,000
that looks at web content that also uses

850
00:27:28,000 --> 00:27:29,520
that same kind of it's called a

851
00:27:29,520 --> 00:27:30,880
convolutional approach that looks for

852
00:27:30,880 --> 00:27:32,240
looks for patterns and then reasons

853
00:27:32,240 --> 00:27:34,159
about the sort of confluence of all the

854
00:27:34,159 --> 00:27:35,360
patterns and deciding whether or not a

855
00:27:35,360 --> 00:27:36,960
web page is malicious

856
00:27:36,960 --> 00:27:38,960
and then we're using a a newer

857
00:27:38,960 --> 00:27:40,880
transformer model to detect malicious

858
00:27:40,880 --> 00:27:42,159
emails like phishing emails and that

859
00:27:42,159 --> 00:27:43,600
kind of thing

860
00:27:43,600 --> 00:27:45,360
so i'm not going to go into detail there

861
00:27:45,360 --> 00:27:47,279
but transformer models use a kind of a

862
00:27:47,279 --> 00:27:48,960
tension model

863
00:27:48,960 --> 00:27:50,559
in which the neural network is sort of

864
00:27:50,559 --> 00:27:53,600
deciding what's important uh in an email

865
00:27:53,600 --> 00:27:55,279
and sort of choosing to focus on the

866
00:27:55,279 --> 00:27:56,559
important bits and deciding whether or

867
00:27:56,559 --> 00:27:58,320
not an email is a phishing email so we

868
00:27:58,320 --> 00:27:59,520
use a lot of neural network models

869
00:27:59,520 --> 00:28:01,039
within our team we also use a lot of

870
00:28:01,039 --> 00:28:02,640
decision tree based models like based on

871
00:28:02,640 --> 00:28:04,480
xg boost and random forest and that kind

872
00:28:04,480 --> 00:28:06,799
of thing

873
00:28:07,520 --> 00:28:10,159
um so so here's a

874
00:28:10,159 --> 00:28:12,320
i wanted to also give a kind of slice of

875
00:28:12,320 --> 00:28:15,120
life in terms of how we retrain and

876
00:28:15,120 --> 00:28:18,559
shift new models on our our our team um

877
00:28:18,559 --> 00:28:21,039
so here's how here's how we retrain and

878
00:28:21,039 --> 00:28:23,760
uh and ship our windows malware detector

879
00:28:23,760 --> 00:28:26,240
uh so first the engineering team uh

880
00:28:26,240 --> 00:28:28,640
within the organization i manage uh

881
00:28:28,640 --> 00:28:31,120
trains uh

882
00:28:31,120 --> 00:28:32,720
trains our windows malware detector on

883
00:28:32,720 --> 00:28:35,039
new data um but also including old data

884
00:28:35,039 --> 00:28:37,120
basically we look at like a recent

885
00:28:37,120 --> 00:28:39,520
window of about 100 million plus files

886
00:28:39,520 --> 00:28:40,720
that we've seen both malware and

887
00:28:40,720 --> 00:28:42,240
benignware

888
00:28:42,240 --> 00:28:45,039
so we train that model and then um the

889
00:28:45,039 --> 00:28:46,880
engineering team passes that model they

890
00:28:46,880 --> 00:28:48,159
do their own internal tests on the model

891
00:28:48,159 --> 00:28:49,520
and they pass it off to the research

892
00:28:49,520 --> 00:28:52,240
team that that signs off on it depending

893
00:28:52,240 --> 00:28:53,679
on how the results look

894
00:28:53,679 --> 00:28:55,919
um then we send that model downstream

895
00:28:55,919 --> 00:28:57,279
and you'll notice we do a ton of testing

896
00:28:57,279 --> 00:28:59,279
on these these models uh so we then we

897
00:28:59,279 --> 00:29:00,480
send it downstream to another team

898
00:29:00,480 --> 00:29:02,559
that's external to our organization and

899
00:29:02,559 --> 00:29:04,159
an organization of like domain experts

900
00:29:04,159 --> 00:29:06,000
who do malware reversing and signature

901
00:29:06,000 --> 00:29:07,760
rating and that kind of thing and they

902
00:29:07,760 --> 00:29:08,799
also look at the results and they

903
00:29:08,799 --> 00:29:10,399
validate the results

904
00:29:10,399 --> 00:29:13,200
then we ship the model to some subset of

905
00:29:13,200 --> 00:29:14,480
our customer base but millions of

906
00:29:14,480 --> 00:29:16,880
endpoints i mean we sort of continuously

907
00:29:16,880 --> 00:29:18,080
roll it out until we hit all of our

908
00:29:18,080 --> 00:29:19,120
endpoints

909
00:29:19,120 --> 00:29:21,600
and we use it to

910
00:29:21,600 --> 00:29:23,520
perform inference on

911
00:29:23,520 --> 00:29:25,440
new pe files but we don't actually block

912
00:29:25,440 --> 00:29:27,039
any pvp files yet we're just kind of

913
00:29:27,039 --> 00:29:28,720
auditioning the model

914
00:29:28,720 --> 00:29:29,919
and finally

915
00:29:29,919 --> 00:29:31,919
when an external validation team decides

916
00:29:31,919 --> 00:29:33,440
that the model is behaving in a

917
00:29:33,440 --> 00:29:35,919
reasonable way then we start rolling it

918
00:29:35,919 --> 00:29:37,120
out and

919
00:29:37,120 --> 00:29:39,440
turning it on and we we do that

920
00:29:39,440 --> 00:29:41,120
in chunks so we do like a fraction of

921
00:29:41,120 --> 00:29:43,120
our customer base then another fraction

922
00:29:43,120 --> 00:29:44,480
uh and finally the model is actually

923
00:29:44,480 --> 00:29:46,960
operational and blocking pe files from

924
00:29:46,960 --> 00:29:49,520
executing that things are malicious um

925
00:29:49,520 --> 00:29:51,440
but the point here is we do a ton of

926
00:29:51,440 --> 00:29:53,120
testing before we actually ship and

927
00:29:53,120 --> 00:29:55,279
deploy a new model and that's because of

928
00:29:55,279 --> 00:29:57,279
all the things i mentioned earlier the

929
00:29:57,279 --> 00:29:59,760
fact that uh

930
00:29:59,760 --> 00:30:01,279
machine learning models are opaque hard

931
00:30:01,279 --> 00:30:04,320
to understand random unpredictable

932
00:30:04,320 --> 00:30:05,919
but they're also good and that's why we

933
00:30:05,919 --> 00:30:07,360
put all this work into them they also

934
00:30:07,360 --> 00:30:08,480
detect stuff that we would never have

935
00:30:08,480 --> 00:30:10,000
detected otherwise but we have to do a

936
00:30:10,000 --> 00:30:11,279
ton of testing before we actually turn

937
00:30:11,279 --> 00:30:12,559
them on otherwise you get into

938
00:30:12,559 --> 00:30:14,159
situations where you are disrupting

939
00:30:14,159 --> 00:30:15,600
customer businesses because you're false

940
00:30:15,600 --> 00:30:17,279
positively on files that you you know

941
00:30:17,279 --> 00:30:18,720
that they actually need to run their

942
00:30:18,720 --> 00:30:21,960
their organizations

943
00:30:22,799 --> 00:30:24,000
um

944
00:30:24,000 --> 00:30:26,559
okay so just sort of well let me let me

945
00:30:26,559 --> 00:30:28,480
stop here um i think this is a decent

946
00:30:28,480 --> 00:30:29,600
breaking point and just see if there's

947
00:30:29,600 --> 00:30:31,440
any questions if i can clarify anything

948
00:30:31,440 --> 00:30:34,960
any comments uh please go ahead

949
00:30:57,360 --> 00:30:59,519
yeah

950
00:31:00,159 --> 00:31:01,360
yeah okay so

951
00:31:01,360 --> 00:31:03,679
so the question was so so the colleague

952
00:31:03,679 --> 00:31:05,600
here um

953
00:31:05,600 --> 00:31:07,120
deals with machine learning compliance

954
00:31:07,120 --> 00:31:08,640
and and that kind of thing and he's

955
00:31:08,640 --> 00:31:10,159
asking um

956
00:31:10,159 --> 00:31:11,519
if we do any introspection into the

957
00:31:11,519 --> 00:31:13,039
machine learning models uh there are

958
00:31:13,039 --> 00:31:15,360
some well-known techniques like chap and

959
00:31:15,360 --> 00:31:17,519
lime that you can use to help explain

960
00:31:17,519 --> 00:31:19,360
machine learning predictions um he's

961
00:31:19,360 --> 00:31:20,559
wondering if we if we use those

962
00:31:20,559 --> 00:31:21,519
techniques

963
00:31:21,519 --> 00:31:23,200
um i mean i feel like i should really

964
00:31:23,200 --> 00:31:25,120
defer to my team here but just for great

965
00:31:25,120 --> 00:31:27,760
for con convenience i'll just say

966
00:31:27,760 --> 00:31:29,840
just because i'm up here um

967
00:31:29,840 --> 00:31:31,200
basically

968
00:31:31,200 --> 00:31:32,799
um

969
00:31:32,799 --> 00:31:35,519
we trust this validation process

970
00:31:35,519 --> 00:31:37,600
much more than we would trust like a

971
00:31:37,600 --> 00:31:40,320
lime explanation as to what our model is

972
00:31:40,320 --> 00:31:42,240
doing the issue with

973
00:31:42,240 --> 00:31:43,200
so

974
00:31:43,200 --> 00:31:44,720
with any of these explainability

975
00:31:44,720 --> 00:31:46,720
approaches i would say and this is this

976
00:31:46,720 --> 00:31:48,159
may be a controversial position i'm not

977
00:31:48,159 --> 00:31:50,159
sure but um

978
00:31:50,159 --> 00:31:52,880
is that they are models of an opaque

979
00:31:52,880 --> 00:31:55,039
model um

980
00:31:55,039 --> 00:31:57,519
and as such well they are interpretable

981
00:31:57,519 --> 00:31:59,760
models of non-interpretable models right

982
00:31:59,760 --> 00:32:00,559
um

983
00:32:00,559 --> 00:32:02,880
and so like when you so so let me

984
00:32:02,880 --> 00:32:04,320
explain like how lime works for example

985
00:32:04,320 --> 00:32:05,519
so

986
00:32:05,519 --> 00:32:06,480
lime

987
00:32:06,480 --> 00:32:07,519
um

988
00:32:07,519 --> 00:32:08,720
it fits

989
00:32:08,720 --> 00:32:12,559
it it fits a linear model to to

990
00:32:12,559 --> 00:32:15,519
the predictions of a non-linear model

991
00:32:15,519 --> 00:32:16,240
and

992
00:32:16,240 --> 00:32:17,440
linear models have this nice property

993
00:32:17,440 --> 00:32:19,120
that they're basically summations

994
00:32:19,120 --> 00:32:21,120
so like so if i was to fit a line model

995
00:32:21,120 --> 00:32:23,679
to our pe model basically what i would

996
00:32:23,679 --> 00:32:25,600
read out is that oh these three features

997
00:32:25,600 --> 00:32:28,960
seem to be contributing to the pe model

998
00:32:28,960 --> 00:32:31,600
deciding this is malware or not now

999
00:32:31,600 --> 00:32:33,200
that might be really satisfying to see

1000
00:32:33,200 --> 00:32:35,760
that the lime you know this this line

1001
00:32:35,760 --> 00:32:37,919
model you know gives this interpretable

1002
00:32:37,919 --> 00:32:39,760
explanation whether it's really true or

1003
00:32:39,760 --> 00:32:40,640
not

1004
00:32:40,640 --> 00:32:42,399
not sure it's a linear approximation of

1005
00:32:42,399 --> 00:32:44,720
a nonlinear model right whether

1006
00:32:44,720 --> 00:32:47,039
um that explanation generalizes to a new

1007
00:32:47,039 --> 00:32:49,200
example that lives on some in some other

1008
00:32:49,200 --> 00:32:51,120
part of the feature space who knows you

1009
00:32:51,120 --> 00:32:52,159
know

1010
00:32:52,159 --> 00:32:53,360
so

1011
00:32:53,360 --> 00:32:55,360
we do use those techniques sometimes to

1012
00:32:55,360 --> 00:32:56,960
make sense of what our models are doing

1013
00:32:56,960 --> 00:32:59,360
but um with a lot of caution

1014
00:32:59,360 --> 00:33:01,519
and um

1015
00:33:01,519 --> 00:33:05,120
and with the sentiment that these are

1016
00:33:05,120 --> 00:33:06,799
um shedding a little bit of light into

1017
00:33:06,799 --> 00:33:08,159
the sort of fog of uncertainty that

1018
00:33:08,159 --> 00:33:09,679
we're living in but not you know not

1019
00:33:09,679 --> 00:33:12,000
clearing the fog um and you know the the

1020
00:33:12,000 --> 00:33:13,760
benefit of this process that i'm showing

1021
00:33:13,760 --> 00:33:15,360
on the slide is that you know we're

1022
00:33:15,360 --> 00:33:17,440
actually testing the models on

1023
00:33:17,440 --> 00:33:19,279
um on the scale of millions of customers

1024
00:33:19,279 --> 00:33:21,039
in silent mode and seeing how they would

1025
00:33:21,039 --> 00:33:22,399
and actually seeing how they would

1026
00:33:22,399 --> 00:33:24,480
behave were we to turn them on and just

1027
00:33:24,480 --> 00:33:26,640
building confidence at scale rates so

1028
00:33:26,640 --> 00:33:28,080
you know they're both important but yeah

1029
00:33:28,080 --> 00:33:28,880
so

1030
00:33:28,880 --> 00:33:29,919
i think that was a really good question

1031
00:33:29,919 --> 00:33:30,880
thank you

1032
00:33:30,880 --> 00:33:33,840
yeah question back there

1033
00:33:37,360 --> 00:33:38,799
so the question is do we implement our

1034
00:33:38,799 --> 00:33:40,720
own models or are we using existing

1035
00:33:40,720 --> 00:33:43,919
implementations um so we we use

1036
00:33:43,919 --> 00:33:45,760
so um

1037
00:33:45,760 --> 00:33:47,679
if the question is like are we

1038
00:33:47,679 --> 00:33:50,399
uh computing gradients like from scratch

1039
00:33:50,399 --> 00:33:52,480
like no right like we're using pie torch

1040
00:33:52,480 --> 00:33:54,559
and that kind of thing um or like we use

1041
00:33:54,559 --> 00:33:57,440
pie torch we use xgboost and we use

1042
00:33:57,440 --> 00:33:59,120
sklearn those are like the main tools

1043
00:33:59,120 --> 00:34:00,799
that we use in our team um you know on

1044
00:34:00,799 --> 00:34:02,320
top of that we're building you know like

1045
00:34:02,320 --> 00:34:04,000
in the previous slide i showed right

1046
00:34:04,000 --> 00:34:05,919
these are there's lots of original ideas

1047
00:34:05,919 --> 00:34:07,200
in here you know that we published as

1048
00:34:07,200 --> 00:34:08,719
our own papers but yeah we're using the

1049
00:34:08,719 --> 00:34:10,079
same toolkits that everybody else uses

1050
00:34:10,079 --> 00:34:12,879
for let's actually implement this stuff

1051
00:34:12,879 --> 00:34:13,719
uh yeah please

1052
00:34:13,719 --> 00:34:16,809
[Music]

1053
00:34:35,918 --> 00:34:39,279
yeah yeah that's a great question

1054
00:34:39,520 --> 00:34:42,159
yeah so so the colleague here um

1055
00:34:42,159 --> 00:34:44,480
asked

1056
00:34:44,719 --> 00:34:46,000
the comment was given that we're using

1057
00:34:46,000 --> 00:34:49,440
transformer models which focus at least

1058
00:34:49,440 --> 00:34:51,199
the sort of stock form of a transformer

1059
00:34:51,199 --> 00:34:53,359
model uh in the natural language domain

1060
00:34:53,359 --> 00:34:56,800
just focuses on text a text document um

1061
00:34:56,800 --> 00:34:58,800
are we also incorporating like email

1062
00:34:58,800 --> 00:35:00,560
header features since those are

1063
00:35:00,560 --> 00:35:02,160
obviously a helpful signal in the email

1064
00:35:02,160 --> 00:35:04,560
domain um so yeah that was a really good

1065
00:35:04,560 --> 00:35:05,839
question that was like that was like the

1066
00:35:05,839 --> 00:35:08,079
question that we were pursuing when we

1067
00:35:08,079 --> 00:35:11,520
um constructed this model uh there's

1068
00:35:11,520 --> 00:35:13,200
i would say you can look at our paper on

1069
00:35:13,200 --> 00:35:15,599
how we do that um but um basically this

1070
00:35:15,599 --> 00:35:17,920
this very simple diagram shows that we

1071
00:35:17,920 --> 00:35:19,440
you know we are using a bunch of

1072
00:35:19,440 --> 00:35:20,880
transformer blocks in our efficient

1073
00:35:20,880 --> 00:35:22,800
detection model but then we also add in

1074
00:35:22,800 --> 00:35:23,920
near the

1075
00:35:23,920 --> 00:35:25,599
near the final layers of the model we

1076
00:35:25,599 --> 00:35:26,960
add in a bunch of context features and

1077
00:35:26,960 --> 00:35:28,880
we do we do find that that helps so we

1078
00:35:28,880 --> 00:35:30,320
are using header features in conjunction

1079
00:35:30,320 --> 00:35:32,240
with the text features

1080
00:35:32,240 --> 00:35:34,000
thanks for the question and maybe one

1081
00:35:34,000 --> 00:35:37,200
more question yeah please go ahead

1082
00:35:38,000 --> 00:35:40,240
yeah

1083
00:35:40,320 --> 00:35:43,320
yeah

1084
00:35:44,960 --> 00:35:46,800
yeah yeah

1085
00:35:46,800 --> 00:35:48,640
yeah so that so the so the question was

1086
00:35:48,640 --> 00:35:51,119
about uh i was asking for more detail on

1087
00:35:51,119 --> 00:35:53,280
our validation process um so like the

1088
00:35:53,280 --> 00:35:54,400
basic

1089
00:35:54,400 --> 00:35:56,240
we could spend a whole talk on on that

1090
00:35:56,240 --> 00:35:57,440
that's a yeah that's a very good

1091
00:35:57,440 --> 00:36:00,240
question i should also say i mean

1092
00:36:00,240 --> 00:36:01,520
i don't know probably two-thirds of our

1093
00:36:01,520 --> 00:36:02,880
time on our team and the research team

1094
00:36:02,880 --> 00:36:04,640
goes into setting up validation

1095
00:36:04,640 --> 00:36:06,560
scrutinizing validations mistrusting our

1096
00:36:06,560 --> 00:36:07,920
validations and questioning whether

1097
00:36:07,920 --> 00:36:09,280
they're even accurate or not and you

1098
00:36:09,280 --> 00:36:10,720
know this is that's that's a huge piece

1099
00:36:10,720 --> 00:36:14,480
of all this um i mean

1100
00:36:14,480 --> 00:36:16,800
one basic thing that we do constantly is

1101
00:36:16,800 --> 00:36:19,440
time splitting so we we train it's you

1102
00:36:19,440 --> 00:36:20,400
think this is a kind of historical

1103
00:36:20,400 --> 00:36:22,160
simulation so we train a model up to

1104
00:36:22,160 --> 00:36:23,440
some time t

1105
00:36:23,440 --> 00:36:25,520
and then we pretend that we had it in

1106
00:36:25,520 --> 00:36:26,880
deployment and we looked at we look at

1107
00:36:26,880 --> 00:36:28,880
data that we observed afterwards and we

1108
00:36:28,880 --> 00:36:32,320
see how it behaved over time and also

1109
00:36:32,320 --> 00:36:33,200
um

1110
00:36:33,200 --> 00:36:35,119
in a raw curve or a precision recall

1111
00:36:35,119 --> 00:36:36,000
plot

1112
00:36:36,000 --> 00:36:37,119
to show you know how it would have

1113
00:36:37,119 --> 00:36:40,000
behaved over some time window and um

1114
00:36:40,000 --> 00:36:41,680
anyways that's that's how that's that's

1115
00:36:41,680 --> 00:36:43,200
sort of a standard design pattern for

1116
00:36:43,200 --> 00:36:44,480
how we design

1117
00:36:44,480 --> 00:36:45,680
basically all of our

1118
00:36:45,680 --> 00:36:46,800
validations

1119
00:36:46,800 --> 00:36:48,240
another problem is the timeliness of the

1120
00:36:48,240 --> 00:36:50,400
labels so the more time you wait the

1121
00:36:50,400 --> 00:36:51,760
more accurate typically the more time

1122
00:36:51,760 --> 00:36:53,119
you wait the more accurate your good bad

1123
00:36:53,119 --> 00:36:54,720
labels become

1124
00:36:54,720 --> 00:36:56,880
so sometimes we go back in time and look

1125
00:36:56,880 --> 00:36:58,480
at you look you know a year back or

1126
00:36:58,480 --> 00:36:59,839
something to see really how well do we

1127
00:36:59,839 --> 00:37:02,000
think this model is doing based on

1128
00:37:02,000 --> 00:37:02,880
um

1129
00:37:02,880 --> 00:37:05,440
based on that sort of time you know that

1130
00:37:05,440 --> 00:37:07,040
what you earn from

1131
00:37:07,040 --> 00:37:08,720
putting that time delta in

1132
00:37:08,720 --> 00:37:10,240
but there's no perfect answer i mean

1133
00:37:10,240 --> 00:37:12,160
there's a lot of intuition and domain

1134
00:37:12,160 --> 00:37:14,400
expertise that goes into this too um

1135
00:37:14,400 --> 00:37:15,760
because it's cyber security and you're

1136
00:37:15,760 --> 00:37:17,760
never gonna have perfect labels

1137
00:37:17,760 --> 00:37:19,599
um okay i'm gonna keep going and so

1138
00:37:19,599 --> 00:37:22,320
we'll have more time for questions um

1139
00:37:22,320 --> 00:37:24,640
later

1140
00:37:25,440 --> 00:37:28,960
okay so i i want to drive my points home

1141
00:37:28,960 --> 00:37:30,400
hopefully not to the point of annoyance

1142
00:37:30,400 --> 00:37:33,359
here but um so i just want to talk about

1143
00:37:33,359 --> 00:37:35,200
some myths and realities and security

1144
00:37:35,200 --> 00:37:36,800
machine learning so one i just think

1145
00:37:36,800 --> 00:37:38,400
people talk about the math as though

1146
00:37:38,400 --> 00:37:40,240
it's it's the main driver and it's

1147
00:37:40,240 --> 00:37:41,440
really not it's all these other

1148
00:37:41,440 --> 00:37:42,640
operational questions that we're talking

1149
00:37:42,640 --> 00:37:44,240
about and that that came up um

1150
00:37:44,240 --> 00:37:45,760
delightfully and the questions that

1151
00:37:45,760 --> 00:37:47,680
people asked just now uh that really go

1152
00:37:47,680 --> 00:37:49,520
into driving accuracy like you need to

1153
00:37:49,520 --> 00:37:51,359
understand the math and you need to be

1154
00:37:51,359 --> 00:37:52,880
able to pick pick a good modeling

1155
00:37:52,880 --> 00:37:55,200
approach um but that's like the easy

1156
00:37:55,200 --> 00:37:56,480
part like the hard part are all these

1157
00:37:56,480 --> 00:37:58,320
operational details like what the

1158
00:37:58,320 --> 00:38:00,160
colleague asked about validation whether

1159
00:38:00,160 --> 00:38:02,000
it's asked about introspection

1160
00:38:02,000 --> 00:38:03,280
and understanding what your models are

1161
00:38:03,280 --> 00:38:04,720
doing in the fields those are those are

1162
00:38:04,720 --> 00:38:07,200
the hard parts um

1163
00:38:07,200 --> 00:38:08,560
um i would say i mean we come up with

1164
00:38:08,560 --> 00:38:10,160
the original ideas that we're proud of

1165
00:38:10,160 --> 00:38:12,320
um but those those squeeze like a few

1166
00:38:12,320 --> 00:38:15,040
percent extra accuracy uh

1167
00:38:15,040 --> 00:38:17,520
out of out of a problem um

1168
00:38:17,520 --> 00:38:20,000
really if somebody's claiming uh if like

1169
00:38:20,000 --> 00:38:21,839
a security vendor is claiming

1170
00:38:21,839 --> 00:38:23,440
they've got some like fancy new math or

1171
00:38:23,440 --> 00:38:25,200
something like that's they i think they

1172
00:38:25,200 --> 00:38:27,599
get negative points for that um you know

1173
00:38:27,599 --> 00:38:29,200
usually it's like the it's like the

1174
00:38:29,200 --> 00:38:31,599
basic ideas really that um like if

1175
00:38:31,599 --> 00:38:33,040
somebody's going beyond what's available

1176
00:38:33,040 --> 00:38:35,359
on wikipedia um then you really have to

1177
00:38:35,359 --> 00:38:36,880
ask why and they need to really show

1178
00:38:36,880 --> 00:38:38,880
their credibility um you can do pretty

1179
00:38:38,880 --> 00:38:40,800
well with with with already existing

1180
00:38:40,800 --> 00:38:41,760
knowledge

1181
00:38:41,760 --> 00:38:43,520
and also doing a really good job with

1182
00:38:43,520 --> 00:38:45,200
operational questions

1183
00:38:45,200 --> 00:38:46,640
um and then the final thing is i mean

1184
00:38:46,640 --> 00:38:48,480
for a while people in industry

1185
00:38:48,480 --> 00:38:50,560
and i think even now still we're talking

1186
00:38:50,560 --> 00:38:52,160
about machine learning somehow

1187
00:38:52,160 --> 00:38:54,560
superseding signatures or obviating and

1188
00:38:54,560 --> 00:38:56,960
sock analysts and domain experts uh

1189
00:38:56,960 --> 00:38:58,800
obviating pat you know obviating block

1190
00:38:58,800 --> 00:39:00,720
lists and allow lists

1191
00:39:00,720 --> 00:39:02,079
the reality is those technologies are

1192
00:39:02,079 --> 00:39:03,839
pillars of the way we do security today

1193
00:39:03,839 --> 00:39:05,119
and as a security data scientist i'll

1194
00:39:05,119 --> 00:39:07,920
say we rely on those tools

1195
00:39:07,920 --> 00:39:09,520
as part of a broader ecosystem in which

1196
00:39:09,520 --> 00:39:12,560
we integrate our technology

1197
00:39:13,760 --> 00:39:15,119
so i think i've gone over most of this

1198
00:39:15,119 --> 00:39:16,800
i'll just say so like a big problem is

1199
00:39:16,800 --> 00:39:18,320
correct data labels i mean so if you're

1200
00:39:18,320 --> 00:39:19,839
not so it's garbage and garbage out when

1201
00:39:19,839 --> 00:39:21,280
it comes to these models if you're not

1202
00:39:21,280 --> 00:39:23,680
uh labeling malware and benign work

1203
00:39:23,680 --> 00:39:25,200
correctly um

1204
00:39:25,200 --> 00:39:26,720
you're gonna you're gonna be way off in

1205
00:39:26,720 --> 00:39:28,079
terms of the model you actually deliver

1206
00:39:28,079 --> 00:39:30,000
to protect people

1207
00:39:30,000 --> 00:39:31,599
if you are training a model on data

1208
00:39:31,599 --> 00:39:33,040
that's not representative of the

1209
00:39:33,040 --> 00:39:34,160
networks that you're actually trying to

1210
00:39:34,160 --> 00:39:35,280
defend

1211
00:39:35,280 --> 00:39:37,119
it's not going to work it's not going to

1212
00:39:37,119 --> 00:39:38,640
generalize to those new networks very

1213
00:39:38,640 --> 00:39:41,040
well you really need data that that

1214
00:39:41,040 --> 00:39:42,240
reflects the context in which you're

1215
00:39:42,240 --> 00:39:43,680
deploying your model both the attacks

1216
00:39:43,680 --> 00:39:45,599
and also the benign observations

1217
00:39:45,599 --> 00:39:48,640
um designing a good evaluation that that

1218
00:39:48,640 --> 00:39:50,240
accurately predicts how well your model

1219
00:39:50,240 --> 00:39:51,760
will do in deployment is absolutely

1220
00:39:51,760 --> 00:39:53,520
critical to machine learning security r

1221
00:39:53,520 --> 00:39:54,720
d

1222
00:39:54,720 --> 00:39:56,400
your team doesn't have a compass to know

1223
00:39:56,400 --> 00:39:58,320
where north is if they don't know how

1224
00:39:58,320 --> 00:39:59,680
well the model will do when you deploy

1225
00:39:59,680 --> 00:40:01,440
it um this is a huge problem with a lot

1226
00:40:01,440 --> 00:40:02,720
of

1227
00:40:02,720 --> 00:40:04,319
security data science

1228
00:40:04,319 --> 00:40:06,000
research and again you need these other

1229
00:40:06,000 --> 00:40:07,280
tools like signatures and a lot of this

1230
00:40:07,280 --> 00:40:08,319
and block list to deploy machine

1231
00:40:08,319 --> 00:40:11,119
learning

1232
00:40:11,119 --> 00:40:13,040
um i wanted to i wanted to say a brief

1233
00:40:13,040 --> 00:40:14,480
note um

1234
00:40:14,480 --> 00:40:16,880
about why people ask us a lot about if

1235
00:40:16,880 --> 00:40:18,640
whether or not we're addressing

1236
00:40:18,640 --> 00:40:20,240
the the literature and adversarial

1237
00:40:20,240 --> 00:40:21,760
machine learning

1238
00:40:21,760 --> 00:40:23,200
um so

1239
00:40:23,200 --> 00:40:25,440
for those coming here outside of data

1240
00:40:25,440 --> 00:40:26,319
science

1241
00:40:26,319 --> 00:40:28,319
adversary so there's a literature on

1242
00:40:28,319 --> 00:40:30,319
what's called adversarial examples in in

1243
00:40:30,319 --> 00:40:31,680
machine learning

1244
00:40:31,680 --> 00:40:33,280
today and the idea of an adversarial

1245
00:40:33,280 --> 00:40:34,400
example

1246
00:40:34,400 --> 00:40:35,680
is

1247
00:40:35,680 --> 00:40:36,960
it's it's

1248
00:40:36,960 --> 00:40:38,960
it's a human crafted input to a machine

1249
00:40:38,960 --> 00:40:40,160
learning model that's deliberately

1250
00:40:40,160 --> 00:40:41,520
designed to confuse the machine learning

1251
00:40:41,520 --> 00:40:43,440
model and give an incorrect prediction

1252
00:40:43,440 --> 00:40:45,599
um and there's lots of fancy way clever

1253
00:40:45,599 --> 00:40:47,040
and fancy ways that you can you can come

1254
00:40:47,040 --> 00:40:49,440
up with these these examples maybe some

1255
00:40:49,440 --> 00:40:50,640
of you have seen

1256
00:40:50,640 --> 00:40:53,359
for example pictures of like

1257
00:40:53,359 --> 00:40:55,760
a school bus that looked like a school

1258
00:40:55,760 --> 00:40:57,119
bus to the human eye seemed to be

1259
00:40:57,119 --> 00:40:58,960
completely untampered but that a

1260
00:40:58,960 --> 00:41:00,640
computer vision model will like just

1261
00:41:00,640 --> 00:41:03,520
undeniably assert is a flamingo uh

1262
00:41:03,520 --> 00:41:04,800
that's an example that's like an example

1263
00:41:04,800 --> 00:41:06,640
of an adversary example now there are

1264
00:41:06,640 --> 00:41:08,160
people doing work on this and and the

1265
00:41:08,160 --> 00:41:09,520
security domain showing that you can

1266
00:41:09,520 --> 00:41:12,880
craft say exe files that bypass a

1267
00:41:12,880 --> 00:41:14,480
machine learning detector

1268
00:41:14,480 --> 00:41:16,720
we currently as a practically oriented

1269
00:41:16,720 --> 00:41:19,040
security data science team

1270
00:41:19,040 --> 00:41:20,640
whose mission is to defend real-world

1271
00:41:20,640 --> 00:41:22,560
customers today are not focused on

1272
00:41:22,560 --> 00:41:24,400
trying to to defeat these kinds of

1273
00:41:24,400 --> 00:41:26,480
examples um and there's a couple reasons

1274
00:41:26,480 --> 00:41:28,000
why um

1275
00:41:28,000 --> 00:41:30,240
the first one i would say is that our

1276
00:41:30,240 --> 00:41:32,319
machine learning systems stay inside of

1277
00:41:32,319 --> 00:41:33,359
a big

1278
00:41:33,359 --> 00:41:35,440
complex decision pipeline that involves

1279
00:41:35,440 --> 00:41:38,800
a lot of non-machine learning layers um

1280
00:41:38,800 --> 00:41:40,240
i think that the threat model that a lot

1281
00:41:40,240 --> 00:41:42,800
of the adversarial ml literature focuses

1282
00:41:42,800 --> 00:41:44,560
on in the security space really is just

1283
00:41:44,560 --> 00:41:46,000
focus on the on the machine learning

1284
00:41:46,000 --> 00:41:48,000
model in isolation

1285
00:41:48,000 --> 00:41:50,240
but we're focused on maximizing accuracy

1286
00:41:50,240 --> 00:41:52,960
over this entire detection pipeline

1287
00:41:52,960 --> 00:41:54,560
and it's just not clear to me that

1288
00:41:54,560 --> 00:41:56,000
adversaries will have such an easy time

1289
00:41:56,000 --> 00:41:57,520
getting past all the other layers if

1290
00:41:57,520 --> 00:41:58,400
they are

1291
00:41:58,400 --> 00:42:00,480
adding adversarial sort of sugar to

1292
00:42:00,480 --> 00:42:01,520
their

1293
00:42:01,520 --> 00:42:04,240
malicious pe examples the other the

1294
00:42:04,240 --> 00:42:05,040
other

1295
00:42:05,040 --> 00:42:06,400
at least

1296
00:42:06,400 --> 00:42:07,520
that's half the reason that we don't

1297
00:42:07,520 --> 00:42:09,520
focus on adversarial ml right now in our

1298
00:42:09,520 --> 00:42:11,040
research group

1299
00:42:11,040 --> 00:42:12,960
i would say the other half is just

1300
00:42:12,960 --> 00:42:14,880
practical like we have so many other

1301
00:42:14,880 --> 00:42:16,800
leverage points we need to improve on um

1302
00:42:16,800 --> 00:42:18,480
to improve our models like some of our

1303
00:42:18,480 --> 00:42:20,880
models we ideally ship them every

1304
00:42:20,880 --> 00:42:23,359
quarter but they slip a quarter uh some

1305
00:42:23,359 --> 00:42:25,119
we have data feeds from you know

1306
00:42:25,119 --> 00:42:26,800
critical like you know in many of our

1307
00:42:26,800 --> 00:42:29,280
models we're integrating half a dozen

1308
00:42:29,280 --> 00:42:32,400
data feeds uh all in one place and we

1309
00:42:32,400 --> 00:42:34,000
have data feeds just going out and maybe

1310
00:42:34,000 --> 00:42:35,280
we don't notice for a couple weeks you

1311
00:42:35,280 --> 00:42:36,960
know i mean in the practical reality of

1312
00:42:36,960 --> 00:42:39,280
our team um improving all of like sort

1313
00:42:39,280 --> 00:42:40,480
of

1314
00:42:40,480 --> 00:42:42,000
moving all those other other levers just

1315
00:42:42,000 --> 00:42:43,839
has to take priority over this research

1316
00:42:43,839 --> 00:42:45,599
um but i thought i thought putting the

1317
00:42:45,599 --> 00:42:46,960
slide in there might just be a good

1318
00:42:46,960 --> 00:42:48,480
point for discussion here because i know

1319
00:42:48,480 --> 00:42:49,440
a lot of people in our community are

1320
00:42:49,440 --> 00:42:50,880
interested in these these examples we

1321
00:42:50,880 --> 00:42:52,720
read those papers we're just not focused

1322
00:42:52,720 --> 00:42:53,680
on you know it's just not our top

1323
00:42:53,680 --> 00:42:55,359
priority right now in terms of coming up

1324
00:42:55,359 --> 00:42:57,280
with ways of getting around adversarial

1325
00:42:57,280 --> 00:42:59,839
machine learning

1326
00:43:00,079 --> 00:43:00,800
yeah

1327
00:43:00,800 --> 00:43:02,560
just one second

1328
00:43:02,560 --> 00:43:04,319
sorry to interrupt yeah yeah did you

1329
00:43:04,319 --> 00:43:05,760
guys have any questions from your

1330
00:43:05,760 --> 00:43:07,119
audience

1331
00:43:07,119 --> 00:43:08,720
oh thanks that's really helpful thanks a

1332
00:43:08,720 --> 00:43:09,920
lot

1333
00:43:09,920 --> 00:43:11,359
um

1334
00:43:11,359 --> 00:43:13,839
so in in security data science the focus

1335
00:43:13,839 --> 00:43:15,920
of the discussion is usually on

1336
00:43:15,920 --> 00:43:18,079
feature selection and

1337
00:43:18,079 --> 00:43:20,319
mathematical modeling um

1338
00:43:20,319 --> 00:43:22,640
the reality is the life of a real world

1339
00:43:22,640 --> 00:43:24,640
security data science team encompasses

1340
00:43:24,640 --> 00:43:26,160
all of these nodes in this kind of

1341
00:43:26,160 --> 00:43:28,319
placemats map of security data science

1342
00:43:28,319 --> 00:43:30,560
work that i've created here

1343
00:43:30,560 --> 00:43:31,359
um

1344
00:43:31,359 --> 00:43:32,560
you know to know whether you're doing a

1345
00:43:32,560 --> 00:43:33,920
good job you need to know

1346
00:43:33,920 --> 00:43:34,640
like

1347
00:43:34,640 --> 00:43:35,520
you need to be able to have a good

1348
00:43:35,520 --> 00:43:36,960
answer to the following questions where

1349
00:43:36,960 --> 00:43:39,280
do you get your training data from and

1350
00:43:39,280 --> 00:43:40,720
why do you think it's representative of

1351
00:43:40,720 --> 00:43:42,240
the data you'll see

1352
00:43:42,240 --> 00:43:44,160
on a real network how do you know what

1353
00:43:44,160 --> 00:43:45,359
good and bad look like in your training

1354
00:43:45,359 --> 00:43:46,800
data and your evaluation data these are

1355
00:43:46,800 --> 00:43:48,400
all hard questions how do you fuse data

1356
00:43:48,400 --> 00:43:49,920
from multiple sources to get the best

1357
00:43:49,920 --> 00:43:52,400
possible training and evaluation data

1358
00:43:52,400 --> 00:43:54,400
um

1359
00:43:54,400 --> 00:43:56,560
we need to know so for the algorithms

1360
00:43:56,560 --> 00:43:58,880
you're using um i said the thing about

1361
00:43:58,880 --> 00:44:00,560
wikipedia already but you know how did

1362
00:44:00,560 --> 00:44:02,000
you design your evaluation how do you

1363
00:44:02,000 --> 00:44:04,079
know that actually predicts accuracy on

1364
00:44:04,079 --> 00:44:06,800
a real-world network um if somebody's

1365
00:44:06,800 --> 00:44:08,720
presenting to you machine learning work

1366
00:44:08,720 --> 00:44:10,319
they've done you want to see their

1367
00:44:10,319 --> 00:44:12,079
evaluation if they if they're showing

1368
00:44:12,079 --> 00:44:13,760
you accuracy numbers you want to know

1369
00:44:13,760 --> 00:44:15,680
all of the nuts and bolts behind

1370
00:44:15,680 --> 00:44:17,760
how they did that evaluation because

1371
00:44:17,760 --> 00:44:20,880
evaluations are biased

1372
00:44:21,040 --> 00:44:23,520
you also want to know um

1373
00:44:23,520 --> 00:44:25,040
if somebody's proposing to you that they

1374
00:44:25,040 --> 00:44:26,640
have a really good security machine

1375
00:44:26,640 --> 00:44:27,680
learning system you want to know how it

1376
00:44:27,680 --> 00:44:29,200
integrates with allow lists block lists

1377
00:44:29,200 --> 00:44:30,400
and signatures

1378
00:44:30,400 --> 00:44:32,720
um

1379
00:44:32,720 --> 00:44:34,560
there's all sorts of questions that you

1380
00:44:34,560 --> 00:44:36,560
want to ask and then and then there's

1381
00:44:36,560 --> 00:44:37,920
their operational picture so there's the

1382
00:44:37,920 --> 00:44:39,520
question of how do you monitor accuracy

1383
00:44:39,520 --> 00:44:41,920
and deployment um a lot of the efficacy

1384
00:44:41,920 --> 00:44:43,520
of a system hinges on that

1385
00:44:43,520 --> 00:44:44,880
uh

1386
00:44:44,880 --> 00:44:46,160
how do you deal do you ever do

1387
00:44:46,160 --> 00:44:47,520
procedures for dealing with accuracy

1388
00:44:47,520 --> 00:44:48,960
crises with models that are out there in

1389
00:44:48,960 --> 00:44:51,599
the field how often do you train and

1390
00:44:51,599 --> 00:44:52,800
retrain your systems to make sure

1391
00:44:52,800 --> 00:44:54,720
they're fresh reflecting the changing

1392
00:44:54,720 --> 00:44:56,480
threat landscape these are these are all

1393
00:44:56,480 --> 00:44:57,599
the questions that are sort of outside

1394
00:44:57,599 --> 00:44:59,200
of the domain of like the math

1395
00:44:59,200 --> 00:45:01,280
and the feature space that are really

1396
00:45:01,280 --> 00:45:03,680
important to the actual efficacy of a

1397
00:45:03,680 --> 00:45:06,960
security data science program

1398
00:45:07,280 --> 00:45:09,200
okay so last last section of the talk i

1399
00:45:09,200 --> 00:45:10,640
want to talk a little bit about

1400
00:45:10,640 --> 00:45:12,640
where and this is just my own opinion

1401
00:45:12,640 --> 00:45:13,920
really like where security machine

1402
00:45:13,920 --> 00:45:15,520
learning is going to go over the course

1403
00:45:15,520 --> 00:45:18,720
of the next decade or so

1404
00:45:18,720 --> 00:45:21,520
okay so i would say

1405
00:45:21,520 --> 00:45:24,800
a key result in machine learning um

1406
00:45:24,800 --> 00:45:27,280
thank you ten ten minutes okay gotcha

1407
00:45:27,280 --> 00:45:28,560
a key result in machine learning over

1408
00:45:28,560 --> 00:45:29,920
the last five years

1409
00:45:29,920 --> 00:45:32,560
is this idea of neural scaling laws and

1410
00:45:32,560 --> 00:45:34,560
so i want to i want to illustrate what

1411
00:45:34,560 --> 00:45:36,160
that looks like and this is this is a

1412
00:45:36,160 --> 00:45:37,440
source from a paper from google that

1413
00:45:37,440 --> 00:45:40,240
came out i think a month ago

1414
00:45:40,240 --> 00:45:41,119
so

1415
00:45:41,119 --> 00:45:43,200
um basically the basic idea behind

1416
00:45:43,200 --> 00:45:44,800
neural scaling laws is that as we make

1417
00:45:44,800 --> 00:45:46,800
neural networks bigger even if we don't

1418
00:45:46,800 --> 00:45:48,160
have any more clever ideas even if we

1419
00:45:48,160 --> 00:45:50,400
simply you know just scale up the width

1420
00:45:50,400 --> 00:45:54,160
of the layers of our neural network um

1421
00:45:54,160 --> 00:45:57,040
magic things happen uh so we do better

1422
00:45:57,040 --> 00:45:58,880
on accuracy and even new capabilities

1423
00:45:58,880 --> 00:46:00,480
emerge in those machine learning models

1424
00:46:00,480 --> 00:46:02,800
so here's here's an example from a

1425
00:46:02,800 --> 00:46:04,400
research team at google

1426
00:46:04,400 --> 00:46:05,440
in which

1427
00:46:05,440 --> 00:46:08,240
they trained a model to

1428
00:46:08,240 --> 00:46:09,839
um it's funny as i said that my google

1429
00:46:09,839 --> 00:46:11,359
assistant thing just came on now it's

1430
00:46:11,359 --> 00:46:15,119
off um okay so um so

1431
00:46:15,119 --> 00:46:16,560
this model was trained to take a

1432
00:46:16,560 --> 00:46:18,240
sentence of natural language text so in

1433
00:46:18,240 --> 00:46:19,599
this case a map of the united states

1434
00:46:19,599 --> 00:46:21,680
made out of sushi it is on a table next

1435
00:46:21,680 --> 00:46:24,079
to a glass of red wine uh and translate

1436
00:46:24,079 --> 00:46:26,480
that sentence into an image um and what

1437
00:46:26,480 --> 00:46:28,400
what what they showed in this figure is

1438
00:46:28,400 --> 00:46:31,440
that um they took the same model

1439
00:46:31,440 --> 00:46:33,520
and uh

1440
00:46:33,520 --> 00:46:36,480
and sized it as a 350 million parameter

1441
00:46:36,480 --> 00:46:38,480
model so 350 million parameters

1442
00:46:38,480 --> 00:46:40,160
basically means there are 350 million

1443
00:46:40,160 --> 00:46:41,680
interconnections between like the neural

1444
00:46:41,680 --> 00:46:43,359
units in this model

1445
00:46:43,359 --> 00:46:45,760
and what came out at that scale was this

1446
00:46:45,760 --> 00:46:48,480
sort of visual gesture at a map

1447
00:46:48,480 --> 00:46:51,200
a weird kind of cubist

1448
00:46:51,200 --> 00:46:53,599
red wine glass and like what looks like

1449
00:46:53,599 --> 00:46:56,079
something vaguely sushi like over here

1450
00:46:56,079 --> 00:46:58,160
so it didn't really work right um but

1451
00:46:58,160 --> 00:47:00,000
then with with no new cleverness or no

1452
00:47:00,000 --> 00:47:02,640
new ideas but simply sort of adding

1453
00:47:02,640 --> 00:47:04,480
adding scale to this model they scaled

1454
00:47:04,480 --> 00:47:06,960
it up to 750 million parameters and then

1455
00:47:06,960 --> 00:47:09,359
you really do see some sushi this looks

1456
00:47:09,359 --> 00:47:11,200
sort of alaska like or i don't know my

1457
00:47:11,200 --> 00:47:12,720
geography is not very good but you know

1458
00:47:12,720 --> 00:47:13,839
it looks like

1459
00:47:13,839 --> 00:47:15,839
some sort of map like and you guys have

1460
00:47:15,839 --> 00:47:18,319
something closer to a glass of wine

1461
00:47:18,319 --> 00:47:19,520
anyways point being by the time you get

1462
00:47:19,520 --> 00:47:21,520
up to 20 billion parameters you really

1463
00:47:21,520 --> 00:47:23,440
do have a map made out of sushi and you

1464
00:47:23,440 --> 00:47:26,160
really do have a glass of red wine

1465
00:47:26,160 --> 00:47:28,240
and you really do have a table

1466
00:47:28,240 --> 00:47:30,160
um and so this is this is a really

1467
00:47:30,160 --> 00:47:31,680
robust result that's come out of machine

1468
00:47:31,680 --> 00:47:33,440
learning research in the last five years

1469
00:47:33,440 --> 00:47:34,800
or so

1470
00:47:34,800 --> 00:47:35,920
that in a whole bunch of different

1471
00:47:35,920 --> 00:47:38,559
domains as we scale neural networks up

1472
00:47:38,559 --> 00:47:39,440
they

1473
00:47:39,440 --> 00:47:42,480
acquire new capabilities

1474
00:47:42,480 --> 00:47:44,640
um so here's another for data scientist

1475
00:47:44,640 --> 00:47:46,640
audience you've probably seen this but

1476
00:47:46,640 --> 00:47:48,960
another really powerful example

1477
00:47:48,960 --> 00:47:52,720
so here another team at google's scaled

1478
00:47:52,720 --> 00:47:55,040
a language model

1479
00:47:55,040 --> 00:47:56,480
up to a ridiculous degree i think this

1480
00:47:56,480 --> 00:47:57,680
is maybe somebody remembers better than

1481
00:47:57,680 --> 00:47:59,280
me i think 300 billion parameters or

1482
00:47:59,280 --> 00:48:00,960
something like that so absolutely

1483
00:48:00,960 --> 00:48:02,400
massive you need like a room scale

1484
00:48:02,400 --> 00:48:04,400
supercomputer to run this model

1485
00:48:04,400 --> 00:48:06,400
but they asked it to do a whole bunch of

1486
00:48:06,400 --> 00:48:07,599
parlor tricks which were really

1487
00:48:07,599 --> 00:48:08,640
impressive

1488
00:48:08,640 --> 00:48:11,040
one of which was to explain this joke um

1489
00:48:11,040 --> 00:48:12,720
so the joke was

1490
00:48:12,720 --> 00:48:14,079
did you see that google just hired an

1491
00:48:14,079 --> 00:48:15,920
eloquent whale for their tpu team so

1492
00:48:15,920 --> 00:48:18,880
tpus are special machine learning

1493
00:48:18,880 --> 00:48:20,640
processors that google's developed for

1494
00:48:20,640 --> 00:48:22,319
the tpu team it it showed them how to

1495
00:48:22,319 --> 00:48:24,640
communicate between two different pods

1496
00:48:24,640 --> 00:48:26,880
and so then the model responds

1497
00:48:26,880 --> 00:48:28,480
tpus are a type of computer chip that

1498
00:48:28,480 --> 00:48:30,559
google uses for deep learning but pod is

1499
00:48:30,559 --> 00:48:32,319
a group of tpus a pod is also a group of

1500
00:48:32,319 --> 00:48:34,319
whales the joke is that the whale is

1501
00:48:34,319 --> 00:48:36,079
able to communicate between two groups

1502
00:48:36,079 --> 00:48:37,280
of whales but the speaker is pretending

1503
00:48:37,280 --> 00:48:38,240
that the whale is able to communicate

1504
00:48:38,240 --> 00:48:39,920
between two groups of tpus so it's

1505
00:48:39,920 --> 00:48:41,359
really impressive right

1506
00:48:41,359 --> 00:48:42,640
that the machine learning model was able

1507
00:48:42,640 --> 00:48:44,319
to to give this description and this

1508
00:48:44,319 --> 00:48:45,760
same model has a whole bunch of other

1509
00:48:45,760 --> 00:48:48,800
capabilities that are equally impressive

1510
00:48:48,800 --> 00:48:50,480
and again what's happening here is

1511
00:48:50,480 --> 00:48:52,319
google is using a model much like the

1512
00:48:52,319 --> 00:48:53,520
phishing detection model that we're

1513
00:48:53,520 --> 00:48:55,920
using um on my team at sofos but it's

1514
00:48:55,920 --> 00:48:57,359
just at a scale

1515
00:48:57,359 --> 00:49:00,079
that's truly massive and capabilities

1516
00:49:00,079 --> 00:49:02,160
emerge when you when you scale models up

1517
00:49:02,160 --> 00:49:04,160
to that degree

1518
00:49:04,160 --> 00:49:06,240
um and so you can see i mean if you get

1519
00:49:06,240 --> 00:49:08,079
into this deep this large model stuff

1520
00:49:08,079 --> 00:49:10,000
you can find really wild results like

1521
00:49:10,000 --> 00:49:11,440
here are a bunch of images generated by

1522
00:49:11,440 --> 00:49:13,119
that google model i showed earlier when

1523
00:49:13,119 --> 00:49:15,119
it's scaled up to 20 billion parameters

1524
00:49:15,119 --> 00:49:16,960
and it just has an enormous visual

1525
00:49:16,960 --> 00:49:18,800
vocabulary and it's able to see the

1526
00:49:18,800 --> 00:49:20,960
relationship between the vision domain

1527
00:49:20,960 --> 00:49:22,800
and the text domain so these are all

1528
00:49:22,800 --> 00:49:24,160
images that it generated based on

1529
00:49:24,160 --> 00:49:26,960
textual prompts

1530
00:49:28,960 --> 00:49:31,280
here's an example of a model uh writing

1531
00:49:31,280 --> 00:49:33,119
codes based on the prompts many of us

1532
00:49:33,119 --> 00:49:34,800
are probably familiar with like the the

1533
00:49:34,800 --> 00:49:37,760
auto the copilot model from open ai that

1534
00:49:37,760 --> 00:49:39,760
you can now use to do this

1535
00:49:39,760 --> 00:49:41,119
um now how is this relevant i know i

1536
00:49:41,119 --> 00:49:42,240
have a few more minutes so i'm gonna go

1537
00:49:42,240 --> 00:49:43,680
really fast how's this relevant to cyber

1538
00:49:43,680 --> 00:49:44,720
security

1539
00:49:44,720 --> 00:49:45,599
um

1540
00:49:45,599 --> 00:49:47,119
so here's an example

1541
00:49:47,119 --> 00:49:49,440
that a colleague and i i

1542
00:49:49,440 --> 00:49:52,480
came up with showing how you can use

1543
00:49:52,480 --> 00:49:56,000
the gpt three large language model

1544
00:49:56,000 --> 00:49:59,440
to categorize domains um so i gave it a

1545
00:49:59,440 --> 00:50:01,359
prompt so i said list of websites and

1546
00:50:01,359 --> 00:50:02,559
their content categories and i gave it

1547
00:50:02,559 --> 00:50:04,559
some examples like berkeley.edu it's an

1548
00:50:04,559 --> 00:50:06,960
education domain amazon is shopping i

1549
00:50:06,960 --> 00:50:08,559
give it five examples and then i give it

1550
00:50:08,559 --> 00:50:10,720
a new example that was syntactically

1551
00:50:10,720 --> 00:50:11,680
quite different from the previous

1552
00:50:11,680 --> 00:50:13,200
examples and i just asked it to

1553
00:50:13,200 --> 00:50:15,359
auto-complete my example and it

1554
00:50:15,359 --> 00:50:17,920
auto-completed it as weapons so i got it

1555
00:50:17,920 --> 00:50:20,559
right and so actually in our web

1556
00:50:20,559 --> 00:50:22,800
detection technology right now we use

1557
00:50:22,800 --> 00:50:24,720
tens of millions of examples and what's

1558
00:50:24,720 --> 00:50:26,480
noteworthy here is we used we used five

1559
00:50:26,480 --> 00:50:29,520
examples uh and showed them to gpt three

1560
00:50:29,520 --> 00:50:31,760
um which has like 200 billion parameters

1561
00:50:31,760 --> 00:50:33,599
and it just got the answer right right

1562
00:50:33,599 --> 00:50:34,800
um so

1563
00:50:34,800 --> 00:50:35,680
um

1564
00:50:35,680 --> 00:50:36,960
that's impressive and it's relevant to

1565
00:50:36,960 --> 00:50:38,559
cyber security here's it here's an

1566
00:50:38,559 --> 00:50:40,640
example from malicious domain detection

1567
00:50:40,640 --> 00:50:42,720
so i give it four examples and i asked

1568
00:50:42,720 --> 00:50:45,119
it to categorize a syntactically totally

1569
00:50:45,119 --> 00:50:46,720
new domain i mean i just made this up it

1570
00:50:46,720 --> 00:50:48,240
doesn't actually exist in the real world

1571
00:50:48,240 --> 00:50:49,760
and it gets it right that it's batch

1572
00:50:49,760 --> 00:50:50,960
right so i think i think these large

1573
00:50:50,960 --> 00:50:52,800
models have something to offer us in

1574
00:50:52,800 --> 00:50:54,640
terms of a

1575
00:50:54,640 --> 00:50:56,160
kind of tactical maneuverability around

1576
00:50:56,160 --> 00:50:57,280
our models i mean it took me five

1577
00:50:57,280 --> 00:50:59,680
minutes or probably a minute to create

1578
00:50:59,680 --> 00:51:01,680
this model and now it's detecting

1579
00:51:01,680 --> 00:51:03,920
malicious domains um and that's you know

1580
00:51:03,920 --> 00:51:05,119
right now there's all sorts of practical

1581
00:51:05,119 --> 00:51:06,160
challenges actually using this

1582
00:51:06,160 --> 00:51:07,599
technology in the field it's costly to

1583
00:51:07,599 --> 00:51:09,760
do inference on gbt3 and all that

1584
00:51:09,760 --> 00:51:10,960
we couldn't deploy this at scale in our

1585
00:51:10,960 --> 00:51:12,319
customer base but i do think this is a

1586
00:51:12,319 --> 00:51:13,520
sign of what's coming in terms of the

1587
00:51:13,520 --> 00:51:16,720
applicability of those large models

1588
00:51:16,720 --> 00:51:18,400
um just because i only have a few more

1589
00:51:18,400 --> 00:51:20,000
minutes i'm going to skip through some

1590
00:51:20,000 --> 00:51:21,200
slides here

1591
00:51:21,200 --> 00:51:23,599
um the other place

1592
00:51:23,599 --> 00:51:25,920
and um i think i'm

1593
00:51:25,920 --> 00:51:27,040
do i have

1594
00:51:27,040 --> 00:51:29,040
time for questions after my time's up or

1595
00:51:29,040 --> 00:51:30,559
i do okay that's that's great to hear so

1596
00:51:30,559 --> 00:51:32,319
i'll go a little slower um the other

1597
00:51:32,319 --> 00:51:34,079
place that i think security machine

1598
00:51:34,079 --> 00:51:36,800
learning is going and needs to go

1599
00:51:36,800 --> 00:51:38,640
is around a much tighter feedback loop

1600
00:51:38,640 --> 00:51:40,240
between users and machine learning

1601
00:51:40,240 --> 00:51:43,440
models um so the the the modality that

1602
00:51:43,440 --> 00:51:45,280
we've seen around the deployment of

1603
00:51:45,280 --> 00:51:47,440
machine learning in in general in the

1604
00:51:47,440 --> 00:51:49,520
tech industry um

1605
00:51:49,520 --> 00:51:50,480
has been

1606
00:51:50,480 --> 00:51:52,160
that users fulfill their intentions with

1607
00:51:52,160 --> 00:51:53,680
products let's let's say google search

1608
00:51:53,680 --> 00:51:55,760
is the product we'll use an example here

1609
00:51:55,760 --> 00:51:56,720
um

1610
00:51:56,720 --> 00:51:58,640
machine learning models in turn learn to

1611
00:51:58,640 --> 00:52:00,319
anticipate and fulfill user intent so

1612
00:52:00,319 --> 00:52:02,839
when you start typing into google

1613
00:52:02,839 --> 00:52:05,920
um uh i don't know um

1614
00:52:05,920 --> 00:52:07,680
restaurants in vegas like within the

1615
00:52:07,680 --> 00:52:08,880
first three characters right it's

1616
00:52:08,880 --> 00:52:11,359
predicting your query and oftentimes you

1617
00:52:11,359 --> 00:52:12,720
know you don't have to type the focus i

1618
00:52:12,720 --> 00:52:14,000
mean i almost never type the full query

1619
00:52:14,000 --> 00:52:15,520
right you just you just click on the on

1620
00:52:15,520 --> 00:52:17,680
the recommendation um

1621
00:52:17,680 --> 00:52:19,440
and users have come to expect this kind

1622
00:52:19,440 --> 00:52:21,839
of um collaborative relationship

1623
00:52:21,839 --> 00:52:23,040
collaborative real-time relationship

1624
00:52:23,040 --> 00:52:24,160
with machine learning and their

1625
00:52:24,160 --> 00:52:25,680
engagement with products and as a result

1626
00:52:25,680 --> 00:52:27,680
of companies successfully implementing

1627
00:52:27,680 --> 00:52:29,359
that more users use the product there's

1628
00:52:29,359 --> 00:52:30,880
a sort of virtuous cycle between users

1629
00:52:30,880 --> 00:52:32,880
using products products deriving trading

1630
00:52:32,880 --> 00:52:35,839
data from that and

1631
00:52:35,839 --> 00:52:37,359
products ultimately improve as a result

1632
00:52:37,359 --> 00:52:39,280
of this cycle

1633
00:52:39,280 --> 00:52:40,240
so we see that you know in

1634
00:52:40,240 --> 00:52:41,760
recommendation engines we see that in

1635
00:52:41,760 --> 00:52:43,839
information retrieval

1636
00:52:43,839 --> 00:52:45,680
what we're working on in our team with

1637
00:52:45,680 --> 00:52:46,800
the more forward-looking thing that

1638
00:52:46,800 --> 00:52:48,960
we're working on is creating that kind

1639
00:52:48,960 --> 00:52:50,800
of feedback loop in the security context

1640
00:52:50,800 --> 00:52:51,839
um and so you guys will hear a

1641
00:52:51,839 --> 00:52:54,559
presentation about that today um

1642
00:52:54,559 --> 00:52:56,960
uh after my talk but um you know

1643
00:52:56,960 --> 00:52:58,960
basically we we've got a team of about

1644
00:52:58,960 --> 00:53:01,280
100 stock analysts at our company that

1645
00:53:01,280 --> 00:53:02,640
actually protect our customers networks

1646
00:53:02,640 --> 00:53:04,160
as part of a subscription service and

1647
00:53:04,160 --> 00:53:05,760
we're sort of setting up that feedback

1648
00:53:05,760 --> 00:53:07,119
loop between stock analysts and the

1649
00:53:07,119 --> 00:53:08,319
alerts they look at and recommending

1650
00:53:08,319 --> 00:53:09,839
alerts that we think

1651
00:53:09,839 --> 00:53:11,520
they're going to find to be true

1652
00:53:11,520 --> 00:53:14,319
positives

1653
00:53:14,319 --> 00:53:16,880
um i'm not going to go too much into the

1654
00:53:16,880 --> 00:53:18,880
results here um since i have two more

1655
00:53:18,880 --> 00:53:20,160
minutes and you're going to hear about

1656
00:53:20,160 --> 00:53:21,280
those later but we're finding that it's

1657
00:53:21,280 --> 00:53:24,319
really efficacious and surprisingly um

1658
00:53:24,319 --> 00:53:25,920
there are very few examples of companies

1659
00:53:25,920 --> 00:53:27,839
deploying these kinds of systems yet in

1660
00:53:27,839 --> 00:53:29,119
security

1661
00:53:29,119 --> 00:53:30,720
and i think that security over the next

1662
00:53:30,720 --> 00:53:32,000
decade is going to catch up to the

1663
00:53:32,000 --> 00:53:34,160
amazons and googles of the world and

1664
00:53:34,160 --> 00:53:36,079
creating those kinds of user ml feedback

1665
00:53:36,079 --> 00:53:39,880
loops but within security products and

1666
00:53:39,880 --> 00:53:43,680
services um so just to reiterate some

1667
00:53:43,680 --> 00:53:44,800
takeaways

1668
00:53:44,800 --> 00:53:46,880
so with current ml security ml is both

1669
00:53:46,880 --> 00:53:48,480
the last resorts because it's so hard to

1670
00:53:48,480 --> 00:53:50,079
deal with for all the challenges that i

1671
00:53:50,079 --> 00:53:51,280
because of all the reasons that i

1672
00:53:51,280 --> 00:53:53,359
described but almost always necessary

1673
00:53:53,359 --> 00:53:54,720
the math is usually the least important

1674
00:53:54,720 --> 00:53:57,839
part it's all these contextual problems

1675
00:53:57,839 --> 00:53:59,200
i think there's a real future for large

1676
00:53:59,200 --> 00:54:00,640
models i think it's the role of security

1677
00:54:00,640 --> 00:54:02,000
ml researchers to figure out exactly

1678
00:54:02,000 --> 00:54:03,680
what that's that future is but i think i

1679
00:54:03,680 --> 00:54:04,960
think those are going to play a part in

1680
00:54:04,960 --> 00:54:06,640
security going forward

1681
00:54:06,640 --> 00:54:08,640
i think feedback loops

1682
00:54:08,640 --> 00:54:10,480
between users and machine learning are

1683
00:54:10,480 --> 00:54:12,400
going to play a part um i think

1684
00:54:12,400 --> 00:54:14,240
adversary lml is an important area to

1685
00:54:14,240 --> 00:54:16,720
look at going forward but today and in

1686
00:54:16,720 --> 00:54:18,880
the short term isn't a priority it's all

1687
00:54:18,880 --> 00:54:20,960
those other questions that i brought up

1688
00:54:20,960 --> 00:54:23,520
um so thanks a lot and looking forward

1689
00:54:23,520 --> 00:54:26,160
to feedback

1690
00:54:29,520 --> 00:54:30,720
so it sounds like we have time for

1691
00:54:30,720 --> 00:54:34,598
questions so please go ahead

1692
00:54:42,640 --> 00:54:43,599
yeah yeah

1693
00:54:43,599 --> 00:54:45,760
yeah so cleverhans.io is a good place to

1694
00:54:45,760 --> 00:54:47,920
go to learn more about adversarial ml

1695
00:54:47,920 --> 00:54:49,680
yeah thank you

1696
00:54:49,680 --> 00:54:51,680
other other questions

1697
00:54:51,680 --> 00:54:53,839
yes

1698
00:55:08,559 --> 00:55:09,920
so what was the what was the last part

1699
00:55:09,920 --> 00:55:11,680
of your question did

1700
00:55:11,680 --> 00:55:15,558
oh ci cds

1701
00:55:22,319 --> 00:55:24,319
yeah yeah so the question was if we have

1702
00:55:24,319 --> 00:55:25,280
if we're using anything like a

1703
00:55:25,280 --> 00:55:26,720
configuration management system with

1704
00:55:26,720 --> 00:55:28,640
respect to our data engineering

1705
00:55:28,640 --> 00:55:29,920
operation

1706
00:55:29,920 --> 00:55:32,400
um i'm gonna look to folks my colleagues

1707
00:55:32,400 --> 00:55:34,720
here from sofos and are you i don't

1708
00:55:34,720 --> 00:55:36,640
think so right no no we're not at that

1709
00:55:36,640 --> 00:55:40,319
level we're not that sophisticated yet

1710
00:55:43,680 --> 00:55:46,000
yeah yeah thus far i know but yeah let's

1711
00:55:46,000 --> 00:55:48,799
talk afterwards and yeah yeah yes go

1712
00:55:48,799 --> 00:55:51,799
ahead

1713
00:56:09,119 --> 00:56:11,040
yeah so the question is are we using

1714
00:56:11,040 --> 00:56:13,920
stock analysis data labelers um so yeah

1715
00:56:13,920 --> 00:56:15,040
so the folks sitting next to you

1716
00:56:15,040 --> 00:56:16,880
actually are going to be talking to uh

1717
00:56:16,880 --> 00:56:17,920
ben and darshan you're talking about

1718
00:56:17,920 --> 00:56:19,599
that so yeah i mean like the answer is

1719
00:56:19,599 --> 00:56:21,280
it's it's like yeah but it's like super

1720
00:56:21,280 --> 00:56:23,040
complicated and and you can't just you

1721
00:56:23,040 --> 00:56:24,880
know yeah it's complicated how you do

1722
00:56:24,880 --> 00:56:26,720
that so yeah it's stick around in this

1723
00:56:26,720 --> 00:56:27,680
room if you want to hear more about how

1724
00:56:27,680 --> 00:56:28,799
we're doing that

1725
00:56:28,799 --> 00:56:32,000
yeah okay well um

1726
00:56:32,000 --> 00:56:33,359
you please one more question and then

1727
00:56:33,359 --> 00:56:36,240
yeah go ahead

1728
00:56:40,720 --> 00:56:42,480
yeah so we have a paper out about so the

1729
00:56:42,480 --> 00:56:44,160
question was about how we do detection

1730
00:56:44,160 --> 00:56:46,240
on html and javascript i mean the short

1731
00:56:46,240 --> 00:56:48,079
answer is we're doing it in a in like a

1732
00:56:48,079 --> 00:56:50,079
traditional end-to-end deep learning way

1733
00:56:50,079 --> 00:56:52,000
so we're not it's format agnostic the

1734
00:56:52,000 --> 00:56:53,520
way we do it um

1735
00:56:53,520 --> 00:56:55,440
and the representation is learned you

1736
00:56:55,440 --> 00:56:57,200
know by a

1737
00:56:57,200 --> 00:56:57,920
a

1738
00:56:57,920 --> 00:56:59,440
stochastic gradient descents you know

1739
00:56:59,440 --> 00:57:01,440
style optimization um

1740
00:57:01,440 --> 00:57:04,240
but the the longer answer is uh i should

1741
00:57:04,240 --> 00:57:05,280
share a link to our paper because we

1742
00:57:05,280 --> 00:57:06,720
have a paper that describes the the

1743
00:57:06,720 --> 00:57:09,119
method by which we do that that's it

1744
00:57:09,119 --> 00:57:10,079
thanks

1745
00:57:10,079 --> 00:57:11,599
so yeah i think let's uh disperse

1746
00:57:11,599 --> 00:57:13,040
anybody who has any more questions come

1747
00:57:13,040 --> 00:57:15,359
you can come talk to me or maybe my team

1748
00:57:15,359 --> 00:57:16,960
would be happy to answer some questions

1749
00:57:16,960 --> 00:57:18,799
too because there's some folks from

1750
00:57:18,799 --> 00:57:20,799
sophos ai here too

1751
00:57:20,799 --> 00:57:22,400
so thanks everybody for coming much

1752
00:57:22,400 --> 00:57:25,400
appreciated

1753
00:57:26,160 --> 00:57:28,240
you

