1
00:00:00,080 --> 00:00:02,400
welcome brittany

2
00:00:02,400 --> 00:00:04,480
[Applause]

3
00:00:04,480 --> 00:00:07,200
hello and welcome to my talk my name is

4
00:00:07,200 --> 00:00:09,599
brittany bach and i'm here to share

5
00:00:09,599 --> 00:00:12,080
notes on my project which is repurposing

6
00:00:12,080 --> 00:00:13,840
vulnerability tickets

7
00:00:13,840 --> 00:00:15,839
to predict severity levels and

8
00:00:15,839 --> 00:00:17,520
introduction to natural language

9
00:00:17,520 --> 00:00:21,920
processing and classification algorithms

10
00:00:27,840 --> 00:00:29,279
thank you

11
00:00:29,279 --> 00:00:31,119
so how i got here

12
00:00:31,119 --> 00:00:33,040
i started my career mostly on the

13
00:00:33,040 --> 00:00:35,200
receiving end of information as a tech

14
00:00:35,200 --> 00:00:36,320
writer

15
00:00:36,320 --> 00:00:39,120
and then transitioned into infosec as a

16
00:00:39,120 --> 00:00:40,800
security analyst working on the

17
00:00:40,800 --> 00:00:42,719
compliance side of things

18
00:00:42,719 --> 00:00:45,600
afterwards i moved into hands-on role as

19
00:00:45,600 --> 00:00:47,600
a security engineer where i found my

20
00:00:47,600 --> 00:00:51,440
niche as an operations process lead

21
00:00:51,440 --> 00:00:53,360
it's this role that allowed me to work

22
00:00:53,360 --> 00:00:55,520
with multiple security workflows and

23
00:00:55,520 --> 00:00:57,680
continue to ask the question how can we

24
00:00:57,680 --> 00:01:00,239
do it better

25
00:01:02,239 --> 00:01:04,159
from my experience doing something

26
00:01:04,159 --> 00:01:06,560
better or process improvement starts

27
00:01:06,560 --> 00:01:08,640
with a lot of talking

28
00:01:08,640 --> 00:01:11,280
typically teams get together and discuss

29
00:01:11,280 --> 00:01:13,760
how something is supposed to go

30
00:01:13,760 --> 00:01:15,600
afterwards there are some testing to

31
00:01:15,600 --> 00:01:17,439
work out issues

32
00:01:17,439 --> 00:01:19,759
once enough tests have been run and a

33
00:01:19,759 --> 00:01:21,840
working process is in place

34
00:01:21,840 --> 00:01:23,280
then commitment in the form of

35
00:01:23,280 --> 00:01:25,759
documentation occurs

36
00:01:25,759 --> 00:01:27,840
the documentation signals there is an

37
00:01:27,840 --> 00:01:30,240
agreed-upon method of carrying out a

38
00:01:30,240 --> 00:01:31,439
process

39
00:01:31,439 --> 00:01:34,960
the expectations at this point are set

40
00:01:34,960 --> 00:01:37,200
eventually the process is completed

41
00:01:37,200 --> 00:01:38,560
enough times

42
00:01:38,560 --> 00:01:41,280
that consistency of expected input and

43
00:01:41,280 --> 00:01:43,520
output are generated

44
00:01:43,520 --> 00:01:45,680
and then the process is automated which

45
00:01:45,680 --> 00:01:48,880
simply speeds up the rate of completion

46
00:01:48,880 --> 00:01:51,119
it's at this stage a lot of data is

47
00:01:51,119 --> 00:01:53,280
collected

48
00:01:53,280 --> 00:01:55,119
there's always the possibility of going

49
00:01:55,119 --> 00:01:57,360
back with new information found and

50
00:01:57,360 --> 00:01:59,680
running through these steps again

51
00:01:59,680 --> 00:02:02,159
but eventually you start asking the next

52
00:02:02,159 --> 00:02:03,840
question which is

53
00:02:03,840 --> 00:02:08,039
what do we do with all this data

54
00:02:09,679 --> 00:02:11,760
so in the context of this project the

55
00:02:11,760 --> 00:02:13,760
data we're talking about is in the form

56
00:02:13,760 --> 00:02:15,760
of vulnerability tickets

57
00:02:15,760 --> 00:02:17,840
vulnerability tickets are created to

58
00:02:17,840 --> 00:02:19,599
track the remediation of detective

59
00:02:19,599 --> 00:02:21,360
vulnerabilities

60
00:02:21,360 --> 00:02:23,120
they typically hold a lot of sensitive

61
00:02:23,120 --> 00:02:25,360
information about the impact of product

62
00:02:25,360 --> 00:02:27,280
or service including

63
00:02:27,280 --> 00:02:29,440
associated teams remediation

64
00:02:29,440 --> 00:02:30,640
instructions

65
00:02:30,640 --> 00:02:32,800
published descriptions and completed

66
00:02:32,800 --> 00:02:35,519
resolutions to name a few

67
00:02:35,519 --> 00:02:36,879
they are usually maintained in a

68
00:02:36,879 --> 00:02:39,280
repository where they can be tracked and

69
00:02:39,280 --> 00:02:41,840
recalled as needed

70
00:02:41,840 --> 00:02:45,040
they are prioritized based on severity

71
00:02:45,040 --> 00:02:47,440
they can provide descriptive statistics

72
00:02:47,440 --> 00:02:49,519
where the gathered information can be

73
00:02:49,519 --> 00:02:53,040
quantitatively described or summarized

74
00:02:53,040 --> 00:02:55,360
vulnerability tickets are generated in

75
00:02:55,360 --> 00:02:58,319
natural language

76
00:03:00,879 --> 00:03:03,120
natural language processing is under the

77
00:03:03,120 --> 00:03:06,000
artificial intelligence umbrella

78
00:03:06,000 --> 00:03:08,480
since there is not a one-to-one match

79
00:03:08,480 --> 00:03:10,560
between natural language and computer

80
00:03:10,560 --> 00:03:12,800
language you must convert natural

81
00:03:12,800 --> 00:03:15,120
language into a format the computer can

82
00:03:15,120 --> 00:03:17,280
understand

83
00:03:17,280 --> 00:03:20,159
once this com conversion is completed

84
00:03:20,159 --> 00:03:21,920
then your data set can be run through

85
00:03:21,920 --> 00:03:24,319
various machine learning algorithms that

86
00:03:24,319 --> 00:03:27,519
can detect patterns in your data

87
00:03:27,519 --> 00:03:29,280
natural language processing is a

88
00:03:29,280 --> 00:03:30,959
two-fold process

89
00:03:30,959 --> 00:03:33,840
first there's the data preprocessing

90
00:03:33,840 --> 00:03:35,840
and then second there's the algorithm

91
00:03:35,840 --> 00:03:37,280
development

92
00:03:37,280 --> 00:03:41,280
both of which we will explore shortly

93
00:03:44,720 --> 00:03:46,879
so check point one before we go any

94
00:03:46,879 --> 00:03:47,840
further

95
00:03:47,840 --> 00:03:49,519
we'll just take a moment to pause and

96
00:03:49,519 --> 00:03:51,440
reflect on why all this might be

97
00:03:51,440 --> 00:03:53,360
important to you

98
00:03:53,360 --> 00:03:55,360
as a recap there's a lot of tribal

99
00:03:55,360 --> 00:03:57,519
knowledge and accepted analysis within

100
00:03:57,519 --> 00:03:59,360
vulnerability tickets

101
00:03:59,360 --> 00:04:02,000
by repurposing the information process

102
00:04:02,000 --> 00:04:04,080
improvement can continue following

103
00:04:04,080 --> 00:04:06,239
automation

104
00:04:06,239 --> 00:04:08,080
this process can then lessen human

105
00:04:08,080 --> 00:04:11,519
dependency and increase accuracy

106
00:04:11,519 --> 00:04:13,599
you're also creating a nice transition

107
00:04:13,599 --> 00:04:16,478
from descriptive analytics to predictive

108
00:04:16,478 --> 00:04:18,798
analytics

109
00:04:18,798 --> 00:04:20,720
predicting severity levels is just a

110
00:04:20,720 --> 00:04:21,759
start

111
00:04:21,759 --> 00:04:24,000
any information that is consistently

112
00:04:24,000 --> 00:04:26,400
tracked within the vulnerability tickets

113
00:04:26,400 --> 00:04:29,840
is fair game to be analyzed

114
00:04:35,919 --> 00:04:38,320
project methodology there are three main

115
00:04:38,320 --> 00:04:40,840
steps to this project first data

116
00:04:40,840 --> 00:04:43,040
pre-processing which is the collection

117
00:04:43,040 --> 00:04:45,520
and preparation of data to run through

118
00:04:45,520 --> 00:04:47,600
machine learning algorithms

119
00:04:47,600 --> 00:04:49,600
the end of this step will result in a

120
00:04:49,600 --> 00:04:52,800
final data set for step two

121
00:04:52,800 --> 00:04:54,840
second there is algorithm

122
00:04:54,840 --> 00:04:57,040
development we're going to cover five

123
00:04:57,040 --> 00:04:59,840
common classification algorithms and how

124
00:04:59,840 --> 00:05:01,840
they help to detect patterns in the

125
00:05:01,840 --> 00:05:04,320
pre-process data set

126
00:05:04,320 --> 00:05:06,160
the end of this step will result in

127
00:05:06,160 --> 00:05:08,320
observing the accuracy percentage of

128
00:05:08,320 --> 00:05:10,720
each algorithm and selecting the best

129
00:05:10,720 --> 00:05:13,680
one for the prediction task

130
00:05:13,680 --> 00:05:16,000
finally we combine the data set and

131
00:05:16,000 --> 00:05:18,400
selected algorithm to complete the to

132
00:05:18,400 --> 00:05:21,039
complete the prediction task

133
00:05:21,039 --> 00:05:22,880
the end of this step will result in a

134
00:05:22,880 --> 00:05:24,720
right or wrong prediction from the test

135
00:05:24,720 --> 00:05:27,039
input

136
00:05:32,720 --> 00:05:35,680
okay step one data pre-processing the

137
00:05:35,680 --> 00:05:39,039
first thing you need is data so normally

138
00:05:39,039 --> 00:05:40,880
you would already have this internally

139
00:05:40,880 --> 00:05:43,039
within whatever ticket repository your

140
00:05:43,039 --> 00:05:45,039
company uses

141
00:05:45,039 --> 00:05:47,199
in this case because we're using public

142
00:05:47,199 --> 00:05:49,840
data the exercise of web scraping came

143
00:05:49,840 --> 00:05:51,199
in handy to gather the needed

144
00:05:51,199 --> 00:05:53,360
information

145
00:05:53,360 --> 00:05:55,440
as an alternative you can try finding a

146
00:05:55,440 --> 00:05:57,759
data set in a community repository like

147
00:05:57,759 --> 00:05:59,120
kaggle

148
00:05:59,120 --> 00:06:01,199
but for me i wanted the experience of

149
00:06:01,199 --> 00:06:03,600
building and cleaning my own data set so

150
00:06:03,600 --> 00:06:06,479
i want the python route

151
00:06:06,479 --> 00:06:08,400
the other steps of pre-processing

152
00:06:08,400 --> 00:06:10,319
include removing dupes

153
00:06:10,319 --> 00:06:12,960
replacing missing values with nan

154
00:06:12,960 --> 00:06:15,280
removing irrelevant words and using a

155
00:06:15,280 --> 00:06:17,759
stop word list to help with the removal

156
00:06:17,759 --> 00:06:20,800
and adding a custom header

157
00:06:20,800 --> 00:06:23,199
and just as a side note a stop board

158
00:06:23,199 --> 00:06:24,639
list is basically a list of

159
00:06:24,639 --> 00:06:27,600
non-essential or no impact words like

160
00:06:27,600 --> 00:06:30,080
pronouns articles prepositions and

161
00:06:30,080 --> 00:06:31,919
conjunctions

162
00:06:31,919 --> 00:06:34,400
examples of these would be like a and

163
00:06:34,400 --> 00:06:35,440
the

164
00:06:35,440 --> 00:06:37,919
your are and there

165
00:06:37,919 --> 00:06:40,080
so it's basically words that offer no

166
00:06:40,080 --> 00:06:43,039
value for the prediction task

167
00:06:43,039 --> 00:06:45,759
the final step was applying count

168
00:06:45,759 --> 00:06:48,080
vectorization to the natural language

169
00:06:48,080 --> 00:06:50,960
data set to make it algorithm ready

170
00:06:50,960 --> 00:06:53,360
this is basically running a script on

171
00:06:53,360 --> 00:06:55,599
the data set to convert the text to

172
00:06:55,599 --> 00:06:57,440
numerical form

173
00:06:57,440 --> 00:06:59,440
in this case we are counting the

174
00:06:59,440 --> 00:07:02,000
frequency of pre-selected words

175
00:07:02,000 --> 00:07:04,319
mentioned in each cve vulnerability

176
00:07:04,319 --> 00:07:06,880
description

177
00:07:07,280 --> 00:07:09,759
all of the above changes were completed

178
00:07:09,759 --> 00:07:13,080
using python

179
00:07:17,199 --> 00:07:18,639
so here's a quick side-by-side

180
00:07:18,639 --> 00:07:21,280
comparison of the natural language data

181
00:07:21,280 --> 00:07:23,199
set next to the count vectorized data

182
00:07:23,199 --> 00:07:24,240
set

183
00:07:24,240 --> 00:07:25,919
this is considered the completion of

184
00:07:25,919 --> 00:07:28,800
step one data pre-processing of natural

185
00:07:28,800 --> 00:07:30,560
language processing

186
00:07:30,560 --> 00:07:32,880
as mentioned the data set on the left

187
00:07:32,880 --> 00:07:36,000
includes the cve id severity level and

188
00:07:36,000 --> 00:07:38,160
most importantly the vulnerability

189
00:07:38,160 --> 00:07:40,960
description in natural language

190
00:07:40,960 --> 00:07:43,919
upon count vectorization the frequented

191
00:07:43,919 --> 00:07:46,000
words become the header

192
00:07:46,000 --> 00:07:47,599
and the number of times they are

193
00:07:47,599 --> 00:07:49,280
mentioned in each vulnerability

194
00:07:49,280 --> 00:07:52,879
description are catalogued underneath

195
00:07:52,879 --> 00:07:55,280
the severity level is still maintained

196
00:07:55,280 --> 00:07:57,199
because it is what the algorithms will

197
00:07:57,199 --> 00:08:00,240
use for the classification

198
00:08:00,240 --> 00:08:02,560
the data set is now ready to be added to

199
00:08:02,560 --> 00:08:05,840
the classification scripts

200
00:08:10,560 --> 00:08:13,919
okay step two is algorithm development

201
00:08:13,919 --> 00:08:16,720
so as mentioned before the second step

202
00:08:16,720 --> 00:08:18,479
in natural language processing is

203
00:08:18,479 --> 00:08:20,319
algorithm development

204
00:08:20,319 --> 00:08:23,039
the classification task is figuring out

205
00:08:23,039 --> 00:08:25,199
whether the frequency of certain words

206
00:08:25,199 --> 00:08:26,960
across multiple vulnerability

207
00:08:26,960 --> 00:08:29,840
descriptions are more likely to appear

208
00:08:29,840 --> 00:08:32,880
in one severity level over another

209
00:08:32,880 --> 00:08:34,399
therefore allowing us to predict

210
00:08:34,399 --> 00:08:36,159
severity level for a discovered

211
00:08:36,159 --> 00:08:38,880
vulnerability

212
00:08:38,958 --> 00:08:41,120
for this project i opted to focus on

213
00:08:41,120 --> 00:08:44,399
five common classification algorithms

214
00:08:44,399 --> 00:08:45,920
these are going to be helpers in

215
00:08:45,920 --> 00:08:48,959
detecting patterns in the data set

216
00:08:48,959 --> 00:08:51,120
for example perhaps there is a certain

217
00:08:51,120 --> 00:08:53,519
word or cluster of words that are more

218
00:08:53,519 --> 00:08:55,839
frequented and say a vulnerability

219
00:08:55,839 --> 00:08:58,399
description

220
00:08:59,360 --> 00:09:01,120
specifically a critical vulnerability

221
00:09:01,120 --> 00:09:02,399
description

222
00:09:02,399 --> 00:09:04,560
detecting this pattern would allow us to

223
00:09:04,560 --> 00:09:06,480
predict critical vulnerabilities in the

224
00:09:06,480 --> 00:09:08,640
future

225
00:09:08,640 --> 00:09:10,800
now there are a lot more than five

226
00:09:10,800 --> 00:09:12,800
classification algorithms

227
00:09:12,800 --> 00:09:14,399
but when you begin studying machine

228
00:09:14,399 --> 00:09:16,800
learning without a doubt these five will

229
00:09:16,800 --> 00:09:18,959
be encountered

230
00:09:18,959 --> 00:09:21,040
so for the sake of an introductory level

231
00:09:21,040 --> 00:09:24,320
talk we will use logistic regression k

232
00:09:24,320 --> 00:09:26,399
nearest neighbors support vector

233
00:09:26,399 --> 00:09:27,440
machines

234
00:09:27,440 --> 00:09:31,920
decision trees and gaussian naive bayes

235
00:09:31,920 --> 00:09:33,839
we are going to review these algorithms

236
00:09:33,839 --> 00:09:36,000
and then we will make a selection based

237
00:09:36,000 --> 00:09:40,240
on results from the cleanse data set

238
00:09:43,839 --> 00:09:46,640
all right so logistic regression is our

239
00:09:46,640 --> 00:09:48,959
first algorithm or model that we're

240
00:09:48,959 --> 00:09:52,399
going to be looking into so

241
00:09:52,399 --> 00:09:54,800
this is uh considered a probability

242
00:09:54,800 --> 00:09:56,240
driven model

243
00:09:56,240 --> 00:09:58,480
logistic regression finds patterns and

244
00:09:58,480 --> 00:10:00,160
data sets by

245
00:10:00,160 --> 00:10:01,839
identifying the probability of an

246
00:10:01,839 --> 00:10:03,680
outcome so in this case the severity

247
00:10:03,680 --> 00:10:05,120
level

248
00:10:05,120 --> 00:10:06,720
and this is based on the features that

249
00:10:06,720 --> 00:10:09,200
are present which for us is going to be

250
00:10:09,200 --> 00:10:12,640
the list of our most frequented words

251
00:10:12,640 --> 00:10:14,320
out of the five this is considered the

252
00:10:14,320 --> 00:10:18,120
most straightforward model

253
00:10:21,839 --> 00:10:24,880
the next is k nearest neighbor or

254
00:10:24,880 --> 00:10:27,360
knn

255
00:10:27,360 --> 00:10:30,320
this one is a distance based model

256
00:10:30,320 --> 00:10:32,160
so when new data is introduced to the

257
00:10:32,160 --> 00:10:34,240
data set the nearest data points are

258
00:10:34,240 --> 00:10:35,519
identified

259
00:10:35,519 --> 00:10:37,760
and these are considered the neighbors

260
00:10:37,760 --> 00:10:39,680
using the value of k

261
00:10:39,680 --> 00:10:41,440
you can adjust the perimeter of the

262
00:10:41,440 --> 00:10:44,160
neighborhood and so although this is set

263
00:10:44,160 --> 00:10:45,600
to 3

264
00:10:45,600 --> 00:10:47,120
in the actual script you can actually

265
00:10:47,120 --> 00:10:50,000
adjust that that value

266
00:10:50,000 --> 00:10:52,079
so if you look at the image to the right

267
00:10:52,079 --> 00:10:54,160
you can see there are three closest data

268
00:10:54,160 --> 00:10:55,920
points selected

269
00:10:55,920 --> 00:10:58,079
of these three the one with the shortest

270
00:10:58,079 --> 00:11:00,240
distance to the new data point will be

271
00:11:00,240 --> 00:11:01,839
selected

272
00:11:01,839 --> 00:11:04,240
the new data point will then inherit the

273
00:11:04,240 --> 00:11:07,680
nearest neighbors class

274
00:11:13,440 --> 00:11:15,760
okay so the second one is support vector

275
00:11:15,760 --> 00:11:18,640
machines or svm

276
00:11:18,640 --> 00:11:19,519
so

277
00:11:19,519 --> 00:11:22,160
this one is a multi-dimensional model

278
00:11:22,160 --> 00:11:23,600
and it's generally really helpful in

279
00:11:23,600 --> 00:11:25,040
classifying when you have a really

280
00:11:25,040 --> 00:11:27,040
complex data set

281
00:11:27,040 --> 00:11:29,519
its goal is to find the maximum margin

282
00:11:29,519 --> 00:11:31,040
distance in order to make the

283
00:11:31,040 --> 00:11:32,720
classification

284
00:11:32,720 --> 00:11:35,279
so in this case it's a two-step process

285
00:11:35,279 --> 00:11:37,680
that happens iteratively until the

286
00:11:37,680 --> 00:11:40,160
maximum margin is detected

287
00:11:40,160 --> 00:11:43,120
so for step one svm generates multiple

288
00:11:43,120 --> 00:11:44,480
hyperplanes

289
00:11:44,480 --> 00:11:46,720
with which are basically lines that

290
00:11:46,720 --> 00:11:49,839
separate the data points into classes

291
00:11:49,839 --> 00:11:52,240
in step two the model then picks the

292
00:11:52,240 --> 00:11:54,320
most optimal hyperplane which is

293
00:11:54,320 --> 00:11:56,000
basically the one with the maximum

294
00:11:56,000 --> 00:11:57,279
margin

295
00:11:57,279 --> 00:12:00,399
so the maximum margin tells us svm has

296
00:12:00,399 --> 00:12:02,160
found the line

297
00:12:02,160 --> 00:12:03,920
that makes the clearest distinction

298
00:12:03,920 --> 00:12:07,199
between the two classes

299
00:12:07,760 --> 00:12:09,680
the support vectors are simply data

300
00:12:09,680 --> 00:12:12,079
points that help the model accomplish

301
00:12:12,079 --> 00:12:14,320
classification so in this case that

302
00:12:14,320 --> 00:12:16,160
would be the blue circles and the red

303
00:12:16,160 --> 00:12:18,639
squares

304
00:12:19,440 --> 00:12:21,920
so from personal experience when i was

305
00:12:21,920 --> 00:12:22,800
running

306
00:12:22,800 --> 00:12:25,040
this this data set for this project

307
00:12:25,040 --> 00:12:27,040
through this model this model took the

308
00:12:27,040 --> 00:12:28,639
longest to process

309
00:12:28,639 --> 00:12:31,760
and so i'm not i'm thinking that that

310
00:12:31,760 --> 00:12:33,440
occurred because

311
00:12:33,440 --> 00:12:35,760
based on this definition

312
00:12:35,760 --> 00:12:38,160
probably finding like the maximum margin

313
00:12:38,160 --> 00:12:40,480
was quite difficult

314
00:12:40,480 --> 00:12:42,480
so when i ran into skip i think it was

315
00:12:42,480 --> 00:12:43,920
almost like two hours in order to

316
00:12:43,920 --> 00:12:46,800
complete processing

317
00:12:50,240 --> 00:12:52,160
all right so up next we have decision

318
00:12:52,160 --> 00:12:53,440
trees

319
00:12:53,440 --> 00:12:55,440
uh this is a flowchart model

320
00:12:55,440 --> 00:12:56,480
so

321
00:12:56,480 --> 00:12:58,320
an interesting requirement with decision

322
00:12:58,320 --> 00:13:01,440
trees is the data set must be labeled

323
00:13:01,440 --> 00:13:03,920
and this is known as a supervised

324
00:13:03,920 --> 00:13:06,320
learning algorithm so basically like you

325
00:13:06,320 --> 00:13:08,720
cannot have a data set that doesn't have

326
00:13:08,720 --> 00:13:10,800
labeling of features so in our case we

327
00:13:10,800 --> 00:13:12,639
have features we're very distinctive of

328
00:13:12,639 --> 00:13:14,480
what we're looking for

329
00:13:14,480 --> 00:13:16,639
if you had a data set that did not have

330
00:13:16,639 --> 00:13:18,839
that and you were doing more like

331
00:13:18,839 --> 00:13:21,839
exploratory data analysis then this

332
00:13:21,839 --> 00:13:23,920
might not be the best algorithm for that

333
00:13:23,920 --> 00:13:25,920
particular project but in this case it

334
00:13:25,920 --> 00:13:29,040
was okay with decision trees

335
00:13:29,040 --> 00:13:31,120
so the way it works is it classifies by

336
00:13:31,120 --> 00:13:33,680
starting at the top or trunk of a tree

337
00:13:33,680 --> 00:13:36,240
and then it moves its way down with more

338
00:13:36,240 --> 00:13:38,160
specific questions to figure out

339
00:13:38,160 --> 00:13:39,839
similarities and differences within the

340
00:13:39,839 --> 00:13:41,360
data set

341
00:13:41,360 --> 00:13:43,199
so in this case you are classifying

342
00:13:43,199 --> 00:13:48,199
based on the most finite shared category

343
00:13:51,760 --> 00:13:54,639
all right uh last one gaussian naive

344
00:13:54,639 --> 00:13:55,680
bayes

345
00:13:55,680 --> 00:13:58,480
so this is another probability model

346
00:13:58,480 --> 00:14:01,199
um there are actually three types of

347
00:14:01,199 --> 00:14:04,320
naive bayes models but we're going to

348
00:14:04,320 --> 00:14:05,760
choose this one because frankly it's the

349
00:14:05,760 --> 00:14:07,360
simplest

350
00:14:07,360 --> 00:14:09,760
this model is a derivative of the bayes

351
00:14:09,760 --> 00:14:12,800
theorem it means the probability of one

352
00:14:12,800 --> 00:14:14,959
event is determined based on the

353
00:14:14,959 --> 00:14:18,560
probability of a completed event

354
00:14:18,560 --> 00:14:20,959
gaussian classifies based on a normal

355
00:14:20,959 --> 00:14:22,399
distribution

356
00:14:22,399 --> 00:14:24,320
if you look at the image the model is

357
00:14:24,320 --> 00:14:26,959
calculating the probability of observing

358
00:14:26,959 --> 00:14:31,719
each data point based on the class

359
00:14:33,680 --> 00:14:36,000
okay so next we'll talk about algorithm

360
00:14:36,000 --> 00:14:38,480
selection

361
00:14:42,720 --> 00:14:44,399
so to figure out which algorithm to

362
00:14:44,399 --> 00:14:47,040
select for the prediction task you run

363
00:14:47,040 --> 00:14:48,720
the cleanse data set through a script

364
00:14:48,720 --> 00:14:50,639
that holds the libraries and classes for

365
00:14:50,639 --> 00:14:52,800
the selected algorithms

366
00:14:52,800 --> 00:14:54,560
the output provides an accuracy

367
00:14:54,560 --> 00:14:56,880
percentage for each of the models so

368
00:14:56,880 --> 00:14:58,240
that's the screenshot that's on the

369
00:14:58,240 --> 00:14:59,600
right

370
00:14:59,600 --> 00:15:01,600
based on the results it's clear that

371
00:15:01,600 --> 00:15:03,519
support vector machine returns the

372
00:15:03,519 --> 00:15:05,360
highest accuracy percentage on the

373
00:15:05,360 --> 00:15:07,600
provided data set

374
00:15:07,600 --> 00:15:10,079
however as i mentioned this also takes

375
00:15:10,079 --> 00:15:12,160
the longest to process so for the

376
00:15:12,160 --> 00:15:14,240
purpose of the demo

377
00:15:14,240 --> 00:15:15,519
we're going to go ahead and use the

378
00:15:15,519 --> 00:15:18,560
second best which is logistic regression

379
00:15:18,560 --> 00:15:20,959
but i will still show you

380
00:15:20,959 --> 00:15:22,880
output on how

381
00:15:22,880 --> 00:15:27,560
how the support vector machine fare

382
00:15:31,680 --> 00:15:35,279
all right second checkpoint um

383
00:15:35,279 --> 00:15:37,839
so as a recap we know natural language

384
00:15:37,839 --> 00:15:40,880
processing is a two-step process of data

385
00:15:40,880 --> 00:15:44,079
pre-processing and algorithm development

386
00:15:44,079 --> 00:15:45,839
we covered each of the classification

387
00:15:45,839 --> 00:15:47,759
algorithms that will be used to make an

388
00:15:47,759 --> 00:15:49,440
algorithm selection

389
00:15:49,440 --> 00:15:51,120
and then next we're going to walk

390
00:15:51,120 --> 00:15:53,759
through the prediction expectations

391
00:15:53,759 --> 00:15:55,920
and then input test data into a

392
00:15:55,920 --> 00:15:58,480
prediction script to observe the results

393
00:15:58,480 --> 00:16:01,600
of the prediction task

394
00:16:05,360 --> 00:16:06,839
all

395
00:16:06,839 --> 00:16:10,320
right so this project is derivative of

396
00:16:10,320 --> 00:16:13,040
two other projects of similar nature

397
00:16:13,040 --> 00:16:15,680
during the initial research i wanted to

398
00:16:15,680 --> 00:16:18,399
get an idea of expectations and what the

399
00:16:18,399 --> 00:16:21,519
definition of success should be

400
00:16:21,519 --> 00:16:23,680
if you look at the table to the right

401
00:16:23,680 --> 00:16:26,480
there are important data points for both

402
00:16:26,480 --> 00:16:29,360
for both of the example projects as well

403
00:16:29,360 --> 00:16:32,240
as the third one which is mine

404
00:16:32,240 --> 00:16:34,480
as you can see there are both variances

405
00:16:34,480 --> 00:16:37,839
in overlap across all three projects

406
00:16:37,839 --> 00:16:40,000
the general methodology of data

407
00:16:40,000 --> 00:16:42,959
pre-processing feature extraction again

408
00:16:42,959 --> 00:16:45,440
that's the number of word frequencies

409
00:16:45,440 --> 00:16:47,519
and algorithm development were also

410
00:16:47,519 --> 00:16:49,440
completed

411
00:16:49,440 --> 00:16:51,759
uh the only difference is the number of

412
00:16:51,759 --> 00:16:54,079
vulnerabilities and features collected

413
00:16:54,079 --> 00:16:57,440
are much higher in project three

414
00:16:57,440 --> 00:16:59,360
in thinking of what i was aiming for

415
00:16:59,360 --> 00:17:01,519
with this project i wanted to gain an

416
00:17:01,519 --> 00:17:04,720
accuracy percentage that realistically

417
00:17:04,720 --> 00:17:08,079
could get presented for user buy-in

418
00:17:08,079 --> 00:17:11,439
i knew if i had anything less than 90

419
00:17:11,439 --> 00:17:13,359
then the likelihood that users would

420
00:17:13,359 --> 00:17:15,760
feel confident in trying out this tool

421
00:17:15,760 --> 00:17:19,599
would not be very high so fortunately

422
00:17:19,599 --> 00:17:22,000
after an intensive journey of developing

423
00:17:22,000 --> 00:17:25,119
the size and the quality of the data set

424
00:17:25,119 --> 00:17:26,959
the goal was reached with the logistic

425
00:17:26,959 --> 00:17:28,640
regression and the svm models

426
00:17:28,640 --> 00:17:31,640
respectively

427
00:17:35,600 --> 00:17:36,880
next we're going to talk about the

428
00:17:36,880 --> 00:17:38,960
impact of data to classification

429
00:17:38,960 --> 00:17:42,720
accuracy the relationship is absolute

430
00:17:42,720 --> 00:17:45,440
when i first started this project i was

431
00:17:45,440 --> 00:17:47,919
gathering cve data including severity

432
00:17:47,919 --> 00:17:50,000
levels and vulnerability descriptions

433
00:17:50,000 --> 00:17:51,840
manually

434
00:17:51,840 --> 00:17:53,840
i was hoping an even distribution across

435
00:17:53,840 --> 00:17:56,320
the four severity levels would result in

436
00:17:56,320 --> 00:17:58,840
adequate classification accuracy

437
00:17:58,840 --> 00:18:02,240
percentages unfortunately the process

438
00:18:02,240 --> 00:18:04,240
was cumbersome and unsuccessful with

439
00:18:04,240 --> 00:18:07,039
such a small matrix

440
00:18:07,039 --> 00:18:09,440
you can see as the data set iterations

441
00:18:09,440 --> 00:18:10,559
progress

442
00:18:10,559 --> 00:18:12,160
and the number of vulnerabilities and

443
00:18:12,160 --> 00:18:14,480
features increase the classification

444
00:18:14,480 --> 00:18:18,240
accuracy percentage increases in tandem

445
00:18:18,240 --> 00:18:20,720
for example if we follow the first row

446
00:18:20,720 --> 00:18:23,200
of this data set the relatively small

447
00:18:23,200 --> 00:18:25,760
number of vulnerabilities and features

448
00:18:25,760 --> 00:18:28,320
results in less than optimal accuracy

449
00:18:28,320 --> 00:18:31,600
percentages across the four algorithms

450
00:18:31,600 --> 00:18:33,120
the other interesting part about this

451
00:18:33,120 --> 00:18:35,440
table is how mistakes in cleaning the

452
00:18:35,440 --> 00:18:36,799
data set

453
00:18:36,799 --> 00:18:39,039
impact the accuracy results

454
00:18:39,039 --> 00:18:42,720
this is evident in rows four 9 and 10.

455
00:18:42,720 --> 00:18:45,600
you can see either an errant column or

456
00:18:45,600 --> 00:18:47,520
row was left in the data set before

457
00:18:47,520 --> 00:18:49,520
running the classification script and

458
00:18:49,520 --> 00:18:54,280
that does impact the accuracy results

459
00:18:58,320 --> 00:19:02,240
so here's a visual of the accuracy trend

460
00:19:02,640 --> 00:19:04,320
this is evident with the linear trend

461
00:19:04,320 --> 00:19:06,400
line which is in hot pink

462
00:19:06,400 --> 00:19:09,039
and you can also see the random peak

463
00:19:09,039 --> 00:19:11,919
with data set 4 where the column id was

464
00:19:11,919 --> 00:19:14,559
left in it should have been removed but

465
00:19:14,559 --> 00:19:16,720
it basically created a false positive

466
00:19:16,720 --> 00:19:19,520
where i thought i had hit accuracy gold

467
00:19:19,520 --> 00:19:21,919
but in reality i just forgot to remove

468
00:19:21,919 --> 00:19:24,480
the id column

469
00:19:24,480 --> 00:19:25,919
um

470
00:19:25,919 --> 00:19:28,400
so this is one lesson from this a key

471
00:19:28,400 --> 00:19:30,160
takeaway is that you know if you see

472
00:19:30,160 --> 00:19:33,120
your results being really really good

473
00:19:33,120 --> 00:19:35,600
even though the goal is to get as close

474
00:19:35,600 --> 00:19:39,200
to 1 or 100 as possible um if it doesn't

475
00:19:39,200 --> 00:19:41,039
make sense with the rest of the testing

476
00:19:41,039 --> 00:19:42,400
then it's probably a good idea to check

477
00:19:42,400 --> 00:19:44,000
your data set

478
00:19:44,000 --> 00:19:45,600
and in this case uh

479
00:19:45,600 --> 00:19:46,480
you know

480
00:19:46,480 --> 00:19:49,039
just not removing a single column

481
00:19:49,039 --> 00:19:50,400
that had no

482
00:19:50,400 --> 00:19:53,600
value completely skewed the results

483
00:19:53,600 --> 00:19:55,440
but aside from that i mean you can see

484
00:19:55,440 --> 00:19:58,000
that as the vulnerability numbers

485
00:19:58,000 --> 00:20:00,160
through web scraping increased and also

486
00:20:00,160 --> 00:20:01,600
as the number of features increased

487
00:20:01,600 --> 00:20:03,520
which is the frequency of words

488
00:20:03,520 --> 00:20:06,159
as that count increased

489
00:20:06,159 --> 00:20:07,760
you know there was just a steady trend

490
00:20:07,760 --> 00:20:09,679
up where the accuracy percentage also

491
00:20:09,679 --> 00:20:12,159
increased

492
00:20:12,240 --> 00:20:15,120
so data matters

493
00:20:16,559 --> 00:20:18,720
okay

494
00:20:18,720 --> 00:20:20,400
so a couple notes before we get to the

495
00:20:20,400 --> 00:20:23,760
demo um i did record the demo i did it

496
00:20:23,760 --> 00:20:25,360
this morning because frankly i don't

497
00:20:25,360 --> 00:20:28,559
trust my coordination to do this live

498
00:20:28,559 --> 00:20:30,159
but we're going to cover preparing the

499
00:20:30,159 --> 00:20:32,880
data set using a randomizer script

500
00:20:32,880 --> 00:20:34,640
and then we're going to cover data

501
00:20:34,640 --> 00:20:36,400
preprocessing which is going to be

502
00:20:36,400 --> 00:20:39,039
cleaning up of the data in both natural

503
00:20:39,039 --> 00:20:41,919
language and post vectorization

504
00:20:41,919 --> 00:20:44,480
last we're going to run the cleanse

505
00:20:44,480 --> 00:20:46,880
vectorized test input

506
00:20:46,880 --> 00:20:48,720
into the prediction script and observe

507
00:20:48,720 --> 00:20:51,280
our results

508
00:20:52,240 --> 00:20:53,200
okay

509
00:20:53,200 --> 00:20:56,320
so movie time

510
00:20:59,360 --> 00:21:01,360
this works great

511
00:21:01,360 --> 00:21:02,799
all right so the first tab is just

512
00:21:02,799 --> 00:21:04,799
showing the classification accuracy this

513
00:21:04,799 --> 00:21:06,320
is the percentage output that was in the

514
00:21:06,320 --> 00:21:08,240
screenshot

515
00:21:08,240 --> 00:21:10,000
and that's just what it looks like once

516
00:21:10,000 --> 00:21:13,400
you run the script

517
00:21:18,240 --> 00:21:20,480
all right so this tab um we're going to

518
00:21:20,480 --> 00:21:23,360
create our natural language data set by

519
00:21:23,360 --> 00:21:26,640
randomly generating a list of cves

520
00:21:26,640 --> 00:21:28,640
these are going to be pre-processed for

521
00:21:28,640 --> 00:21:31,280
the algorithm development so this is not

522
00:21:31,280 --> 00:21:33,600
a requirement for this project but it's

523
00:21:33,600 --> 00:21:35,120
just added for

524
00:21:35,120 --> 00:21:36,480
for effect

525
00:21:36,480 --> 00:21:37,840
just to show that you know if you

526
00:21:37,840 --> 00:21:40,080
randomly have a new cve that's published

527
00:21:40,080 --> 00:21:42,159
and you know your organization is aware

528
00:21:42,159 --> 00:21:43,919
of it like you're obviously not going to

529
00:21:43,919 --> 00:21:47,919
have that information right off the bat

530
00:21:50,559 --> 00:21:52,559
all right so yeah that's data i'll put i

531
00:21:52,559 --> 00:21:54,640
did select five for the demo you can do

532
00:21:54,640 --> 00:21:56,720
a lot more and i have done a lot more

533
00:21:56,720 --> 00:21:57,440
but

534
00:21:57,440 --> 00:21:59,840
again since time is

535
00:21:59,840 --> 00:22:02,000
important i went ahead and randomized

536
00:22:02,000 --> 00:22:04,320
five

537
00:22:05,120 --> 00:22:07,520
and then you can see the id severity

538
00:22:07,520 --> 00:22:09,039
level and vulnerability description

539
00:22:09,039 --> 00:22:12,919
that's all in natural language

540
00:22:12,960 --> 00:22:13,840
okay

541
00:22:13,840 --> 00:22:15,520
so next we're going to open up the

542
00:22:15,520 --> 00:22:18,799
generated csv

543
00:22:25,120 --> 00:22:27,840
and yeah

544
00:22:32,000 --> 00:22:32,960
okay

545
00:22:32,960 --> 00:22:34,640
um so that's all the natural language

546
00:22:34,640 --> 00:22:36,880
data upfront in the csv so the first

547
00:22:36,880 --> 00:22:38,320
thing i'm doing there is removing the

548
00:22:38,320 --> 00:22:40,480
header we don't want to include that

549
00:22:40,480 --> 00:22:41,840
because then that's also going to get

550
00:22:41,840 --> 00:22:43,360
vectorized and it's going to skew our

551
00:22:43,360 --> 00:22:45,918
results

552
00:22:46,400 --> 00:22:50,080
the other thing that i'm doing is

553
00:22:50,080 --> 00:22:52,640
i'm adding a placeholder column so the

554
00:22:52,640 --> 00:22:55,679
count vectorization code um it will

555
00:22:55,679 --> 00:22:57,200
basically

556
00:22:57,200 --> 00:22:58,559
stop at that

557
00:22:58,559 --> 00:23:00,320
particular like placeholder and then

558
00:23:00,320 --> 00:23:02,000
it'll know to like iterate to the next

559
00:23:02,000 --> 00:23:03,760
row and so i add just this like

560
00:23:03,760 --> 00:23:05,600
arbitrary term

561
00:23:05,600 --> 00:23:07,200
and i just added like starter column

562
00:23:07,200 --> 00:23:08,320
because that's where i want the

563
00:23:08,320 --> 00:23:11,960
iteration to start

564
00:23:30,400 --> 00:23:32,000
okay so the other thing i'm doing is i'm

565
00:23:32,000 --> 00:23:34,559
changing the file extension from csv to

566
00:23:34,559 --> 00:23:37,559
text

567
00:23:49,520 --> 00:23:50,580
okay next tab

568
00:23:50,580 --> 00:23:52,640
[Music]

569
00:23:52,640 --> 00:23:54,880
so now we're going to run that text file

570
00:23:54,880 --> 00:23:57,120
through the count vector z account

571
00:23:57,120 --> 00:23:59,200
vectorizer script

572
00:23:59,200 --> 00:24:02,080
um so this group creates

573
00:24:02,080 --> 00:24:05,919
this useless first row of all zeros and

574
00:24:05,919 --> 00:24:08,000
um you do have to like remove that and

575
00:24:08,000 --> 00:24:09,760
so that's just what i'm showing here is

576
00:24:09,760 --> 00:24:12,159
that it works but for some reason it

577
00:24:12,159 --> 00:24:14,960
creates like a first row of zeros that

578
00:24:14,960 --> 00:24:16,480
has to get removed

579
00:24:16,480 --> 00:24:17,840
the other thing it's going to create is

580
00:24:17,840 --> 00:24:20,400
like the column of ids the identifier so

581
00:24:20,400 --> 00:24:22,400
that's another thing that gets removed

582
00:24:22,400 --> 00:24:26,440
as a part of the data processing

583
00:24:36,000 --> 00:24:37,200
all right so that's what it looks like

584
00:24:37,200 --> 00:24:38,960
when it's vectorized you can see all of

585
00:24:38,960 --> 00:24:40,919
the features at the top and it's like

586
00:24:40,919 --> 00:24:43,840
592. and then you can see how the

587
00:24:43,840 --> 00:24:46,480
occurrence of each of these features

588
00:24:46,480 --> 00:24:49,279
is actually listed

589
00:24:49,279 --> 00:24:52,400
and again we're removing the first row

590
00:24:52,400 --> 00:24:54,159
so that's all the zeros we're going to

591
00:24:54,159 --> 00:24:58,200
move that that's not valuable

592
00:25:18,000 --> 00:25:20,240
okay so as a side note you can actually

593
00:25:20,240 --> 00:25:22,559
just do all of this through code but i

594
00:25:22,559 --> 00:25:24,720
am showing the manual side of this for

595
00:25:24,720 --> 00:25:27,120
transparency

596
00:25:27,120 --> 00:25:28,799
this is pretty much the stuff that has

597
00:25:28,799 --> 00:25:30,240
to happen

598
00:25:30,240 --> 00:25:32,880
before you can run this through your

599
00:25:32,880 --> 00:25:34,240
prediction

600
00:25:34,240 --> 00:25:37,240
script

601
00:25:46,480 --> 00:25:48,080
okay yeah and i think that is just

602
00:25:48,080 --> 00:25:51,760
removing the identifier

603
00:25:51,760 --> 00:25:53,200
and then there's just one small

604
00:25:53,200 --> 00:25:55,200
formatting that's left before copying

605
00:25:55,200 --> 00:25:59,000
and pasting it to the script

606
00:26:27,279 --> 00:26:29,919
okay so that's the vectorization for all

607
00:26:29,919 --> 00:26:32,799
five cpes we're going to copy that then

608
00:26:32,799 --> 00:26:34,320
we're gonna go to the script and we're

609
00:26:34,320 --> 00:26:37,200
gonna paste it in

610
00:26:37,360 --> 00:26:38,880
uh and again we're doing the stem on

611
00:26:38,880 --> 00:26:41,679
logistic regression which if you can

612
00:26:41,679 --> 00:26:44,559
recall it was roughly 90 accuracy

613
00:26:44,559 --> 00:26:46,559
so we are looking for pretty good

614
00:26:46,559 --> 00:26:49,559
results

615
00:27:04,799 --> 00:27:06,880
okay so now what we're going to do is

616
00:27:06,880 --> 00:27:09,279
we're going to run the script and we are

617
00:27:09,279 --> 00:27:11,440
going to be able to see um the actual

618
00:27:11,440 --> 00:27:14,640
prediction that's made for all five cves

619
00:27:14,640 --> 00:27:16,880
and we're going to also see

620
00:27:16,880 --> 00:27:18,799
how it compares with the natural

621
00:27:18,799 --> 00:27:21,840
language output

622
00:27:33,810 --> 00:27:36,909
[Music]

623
00:28:03,760 --> 00:28:06,480
okay so that is the prediction that's

624
00:28:06,480 --> 00:28:09,200
made on all five cves it's red left to

625
00:28:09,200 --> 00:28:10,880
right which is going to be

626
00:28:10,880 --> 00:28:13,840
if you're looking back at the um

627
00:28:13,840 --> 00:28:16,640
the table it's going to be top down

628
00:28:16,640 --> 00:28:17,919
so critical

629
00:28:17,919 --> 00:28:20,399
i think it's

630
00:28:21,200 --> 00:28:24,320
yup medium low critical critical

631
00:28:24,320 --> 00:28:26,159
and okay so it looks like there's a

632
00:28:26,159 --> 00:28:29,200
one-to-one match on all of those

633
00:28:29,200 --> 00:28:31,039
as a side note i have done this with

634
00:28:31,039 --> 00:28:33,120
like a lot more test data and it's not

635
00:28:33,120 --> 00:28:34,880
always a hundred percent um but it's

636
00:28:34,880 --> 00:28:36,559
pretty close i mean even when i ran it

637
00:28:36,559 --> 00:28:37,760
with like

638
00:28:37,760 --> 00:28:40,320
you know like 12 different test inputs i

639
00:28:40,320 --> 00:28:42,159
still got nine out of 12

640
00:28:42,159 --> 00:28:44,399
using logistic regression and that was

641
00:28:44,399 --> 00:28:46,799
about the same with svm as well it just

642
00:28:46,799 --> 00:28:49,120
takes so long to process but

643
00:28:49,120 --> 00:28:51,360
um but yeah it was pretty spot-on with

644
00:28:51,360 --> 00:28:54,479
the accuracy percentage

645
00:28:56,080 --> 00:28:59,240
all right

646
00:29:05,520 --> 00:29:10,000
so this is how the other models fared um

647
00:29:10,799 --> 00:29:12,399
the only one that came back with a

648
00:29:12,399 --> 00:29:15,360
little bit of a hiccup was the gaussian

649
00:29:15,360 --> 00:29:18,080
naive base but that was actually pretty

650
00:29:18,080 --> 00:29:20,480
expected and still getting four out of

651
00:29:20,480 --> 00:29:22,240
five is pretty good because i've run it

652
00:29:22,240 --> 00:29:24,240
again with a lot more test data and

653
00:29:24,240 --> 00:29:25,600
there have been times when i only got

654
00:29:25,600 --> 00:29:26,880
like three right

655
00:29:26,880 --> 00:29:28,240
uh out of 12

656
00:29:28,240 --> 00:29:29,520
but if you look at the accuracy

657
00:29:29,520 --> 00:29:31,200
percentage it's relatively low so it's

658
00:29:31,200 --> 00:29:32,640
not the best model to use for this

659
00:29:32,640 --> 00:29:33,679
project

660
00:29:33,679 --> 00:29:34,960
but the other models you know they're

661
00:29:34,960 --> 00:29:36,640
they're relatively close in accuracy

662
00:29:36,640 --> 00:29:39,120
percentage and so you know the match was

663
00:29:39,120 --> 00:29:40,799
pretty spot on

664
00:29:40,799 --> 00:29:42,320
and that was also the case when i ran

665
00:29:42,320 --> 00:29:44,559
with a lot more test data so that's

666
00:29:44,559 --> 00:29:47,360
pretty that distribution is

667
00:29:47,360 --> 00:29:51,240
it's pretty consistent

668
00:29:59,039 --> 00:30:01,760
okay so a few key takeaways before we

669
00:30:01,760 --> 00:30:03,520
wrap things up

670
00:30:03,520 --> 00:30:05,918
first

671
00:30:06,080 --> 00:30:08,320
you have to keep processing time in mind

672
00:30:08,320 --> 00:30:11,360
uh for when the scripts run i found as

673
00:30:11,360 --> 00:30:13,360
my data set grew some of the models like

674
00:30:13,360 --> 00:30:15,200
svm took upwards of two hours to

675
00:30:15,200 --> 00:30:16,480
complete

676
00:30:16,480 --> 00:30:18,240
the second thing is there's a great

677
00:30:18,240 --> 00:30:22,399
benefit to this process improvement

678
00:30:22,399 --> 00:30:24,240
in comparison to just accepting the

679
00:30:24,240 --> 00:30:26,720
published severity level uh

680
00:30:26,720 --> 00:30:28,320
the big benefit is that you can

681
00:30:28,320 --> 00:30:30,320
repurpose internal decision making and

682
00:30:30,320 --> 00:30:31,919
tribal knowledge

683
00:30:31,919 --> 00:30:33,360
these are things that are not always

684
00:30:33,360 --> 00:30:35,279
going to be integrated when you see like

685
00:30:35,279 --> 00:30:37,760
a newly published cve

686
00:30:37,760 --> 00:30:39,679
so that's very helpful the third thing

687
00:30:39,679 --> 00:30:41,520
is there are two main assumptions this

688
00:30:41,520 --> 00:30:43,120
project makes

689
00:30:43,120 --> 00:30:46,399
the first is the severity level

690
00:30:46,399 --> 00:30:47,840
the severity levels in the current

691
00:30:47,840 --> 00:30:49,760
ticket repository

692
00:30:49,760 --> 00:30:51,840
are more or less accurate and then the

693
00:30:51,840 --> 00:30:53,919
second assumption is that there's

694
00:30:53,919 --> 00:30:55,840
already an existing

695
00:30:55,840 --> 00:30:57,760
vulnerability ticketing process and

696
00:30:57,760 --> 00:30:59,200
hopefully that process is somewhat

697
00:30:59,200 --> 00:31:01,679
automated

698
00:31:01,679 --> 00:31:04,720
the fourth takeaway is the data set is

699
00:31:04,720 --> 00:31:06,240
is really the most important part of

700
00:31:06,240 --> 00:31:08,080
this process i mean everything else like

701
00:31:08,080 --> 00:31:10,000
you know the code the

702
00:31:10,000 --> 00:31:11,360
um

703
00:31:11,360 --> 00:31:13,120
yeah basically the code i mean you can

704
00:31:13,120 --> 00:31:15,039
pretty much you know find a lot of that

705
00:31:15,039 --> 00:31:17,200
information already on the python

706
00:31:17,200 --> 00:31:19,279
website and like numerous amounts of

707
00:31:19,279 --> 00:31:20,559
books and

708
00:31:20,559 --> 00:31:22,559
you know websites and whatnot but if

709
00:31:22,559 --> 00:31:24,720
your data set isn't cleansed properly

710
00:31:24,720 --> 00:31:26,000
it's just you're not going to get the

711
00:31:26,000 --> 00:31:27,919
results that you want

712
00:31:27,919 --> 00:31:30,320
um and the last thing is you know you

713
00:31:30,320 --> 00:31:31,760
don't need to be a mathematician or

714
00:31:31,760 --> 00:31:33,760
coding expert to get started i mean i'm

715
00:31:33,760 --> 00:31:36,240
obviously neither of those things

716
00:31:36,240 --> 00:31:37,760
but you know a little knowledge can go a

717
00:31:37,760 --> 00:31:39,600
long way so the whole point of this was

718
00:31:39,600 --> 00:31:41,519
really just finding another way to

719
00:31:41,519 --> 00:31:44,720
improve process that's it

720
00:31:44,720 --> 00:31:47,720
um

721
00:31:48,640 --> 00:31:50,799
okay so thank you

722
00:31:50,799 --> 00:31:52,880
uh thank you for attending my talk

723
00:31:52,880 --> 00:31:54,960
here are a few resources that help me

724
00:31:54,960 --> 00:31:57,440
get started

725
00:31:57,440 --> 00:31:59,279
you know hopefully these will be helpful

726
00:31:59,279 --> 00:32:01,279
but these are really like the main books

727
00:32:01,279 --> 00:32:03,360
and the websites that i reviewed in

728
00:32:03,360 --> 00:32:07,080
order to learn this stuff

729
00:32:12,660 --> 00:32:15,819
[Applause]

730
00:32:20,880 --> 00:32:23,200
hi we had to we had to kill all of the

731
00:32:23,200 --> 00:32:25,279
wireless mics because of interference so

732
00:32:25,279 --> 00:32:27,279
if anybody has a question

733
00:32:27,279 --> 00:32:30,159
please speak loudly and if i can have

734
00:32:30,159 --> 00:32:31,679
the speaker just repeat the question

735
00:32:31,679 --> 00:32:33,679
into the mic before answering it that

736
00:32:33,679 --> 00:32:37,480
would be great thank you

737
00:32:46,399 --> 00:32:49,399
bye

738
00:32:51,430 --> 00:32:54,569
[Music]

739
00:33:17,360 --> 00:33:22,279
taking assets into account um

740
00:33:24,320 --> 00:33:26,399
so i think if you're thinking about

741
00:33:26,399 --> 00:33:28,240
prediction you can definitely take

742
00:33:28,240 --> 00:33:30,880
assets into accounts if those

743
00:33:30,880 --> 00:33:33,039
if that is an item that is routinely

744
00:33:33,039 --> 00:33:35,600
collected in your vulnerability tickets

745
00:33:35,600 --> 00:33:37,120
and so you know once the cv gets

746
00:33:37,120 --> 00:33:39,440
published if it's relevant to your you

747
00:33:39,440 --> 00:33:40,960
know organization

748
00:33:40,960 --> 00:33:43,120
um and you end up finding impacted

749
00:33:43,120 --> 00:33:45,679
assets and that information is is

750
00:33:45,679 --> 00:33:47,440
consistently collected

751
00:33:47,440 --> 00:33:49,440
then you have a pattern and once you

752
00:33:49,440 --> 00:33:51,120
have a pattern then you can utilize

753
00:33:51,120 --> 00:33:53,120
something like this

754
00:33:53,120 --> 00:33:55,120
and what it would really do for you is

755
00:33:55,120 --> 00:33:56,880
just if there was something similar that

756
00:33:56,880 --> 00:34:00,320
came up again or

757
00:34:00,720 --> 00:34:02,720
maybe you knew that

758
00:34:02,720 --> 00:34:03,760
yeah i mean basically if there's

759
00:34:03,760 --> 00:34:05,679
something similar that comes up again

760
00:34:05,679 --> 00:34:07,440
you can utilize

761
00:34:07,440 --> 00:34:09,280
this historical knowledge that's already

762
00:34:09,280 --> 00:34:11,280
in your vulnerability tickets in order

763
00:34:11,280 --> 00:34:13,760
to predict things like

764
00:34:13,760 --> 00:34:16,320
what are the assets might be impacted by

765
00:34:16,320 --> 00:34:18,719
this or like who are the owners of these

766
00:34:18,719 --> 00:34:20,800
assets or you know where's the root

767
00:34:20,800 --> 00:34:21,839
cause

768
00:34:21,839 --> 00:34:23,839
so it really comes down to what you're

769
00:34:23,839 --> 00:34:25,839
consistently collecting

770
00:34:25,839 --> 00:34:27,440
and then that's where these machine

771
00:34:27,440 --> 00:34:29,520
learning models can really help you out

772
00:34:29,520 --> 00:34:31,440
to make you know whatever prediction you

773
00:34:31,440 --> 00:34:34,079
want so severity level is just one type

774
00:34:34,079 --> 00:34:35,760
of prediction you can make

775
00:34:35,760 --> 00:34:37,119
but the real question is you'd have to

776
00:34:37,119 --> 00:34:38,560
look at your tickets and say hey what

777
00:34:38,560 --> 00:34:40,000
what are we doing consistently what's

778
00:34:40,000 --> 00:34:42,079
the formatting like you know if i put

779
00:34:42,079 --> 00:34:44,480
everything in a csv and a table

780
00:34:44,480 --> 00:34:46,800
you know what you know

781
00:34:46,800 --> 00:34:48,480
how much of that information is like

782
00:34:48,480 --> 00:34:51,040
consistently collected and when you have

783
00:34:51,040 --> 00:34:53,280
that then you can you can repurpose that

784
00:34:53,280 --> 00:34:57,240
information to make predictions

785
00:35:20,480 --> 00:35:21,200
so

786
00:35:21,200 --> 00:35:23,680
if you applied another column of

787
00:35:23,680 --> 00:35:24,960
do i have

788
00:35:24,960 --> 00:35:26,960
what's applicable to this to this

789
00:35:26,960 --> 00:35:27,920
finding

790
00:35:27,920 --> 00:35:30,800
but that is

791
00:35:49,680 --> 00:35:51,520
yeah i think it's also yeah that's

792
00:35:51,520 --> 00:35:53,200
that's exactly correct and then if there

793
00:35:53,200 --> 00:35:55,119
is a new uh vulnerability that's

794
00:35:55,119 --> 00:35:57,520
introduced that your organization has

795
00:35:57,520 --> 00:36:00,480
not encountered yet um the process of

796
00:36:00,480 --> 00:36:02,320
like feature extraction that we

797
00:36:02,320 --> 00:36:04,000
discussed which is like the most

798
00:36:04,000 --> 00:36:05,680
frequented words

799
00:36:05,680 --> 00:36:07,920
that could also give you insight like as

800
00:36:07,920 --> 00:36:09,520
long as that

801
00:36:09,520 --> 00:36:11,040
you can actually adjust the number of

802
00:36:11,040 --> 00:36:12,720
features that you include in your data

803
00:36:12,720 --> 00:36:14,079
set but

804
00:36:14,079 --> 00:36:15,520
if you wanted to add other things and

805
00:36:15,520 --> 00:36:17,599
then you wanted to count if there's like

806
00:36:17,599 --> 00:36:19,920
how many times ios is mentioned

807
00:36:19,920 --> 00:36:21,680
um in like all of the vulnerability

808
00:36:21,680 --> 00:36:23,280
tickets and then that would help you to

809
00:36:23,280 --> 00:36:25,760
find like the nearest classification

810
00:36:25,760 --> 00:36:27,200
you know that's another adjustment you

811
00:36:27,200 --> 00:36:29,598
can make

812
00:36:29,680 --> 00:36:30,960
kind of

813
00:36:30,960 --> 00:36:33,960
yeah

814
00:36:48,079 --> 00:36:51,079
is

815
00:36:54,160 --> 00:36:55,839
oh that's a good question yeah when i

816
00:36:55,839 --> 00:36:58,800
did testing it was it was very close so

817
00:36:58,800 --> 00:37:01,119
most of the ones okay so i did testing

818
00:37:01,119 --> 00:37:03,359
multiple times and i didn't get any of

819
00:37:03,359 --> 00:37:05,440
the critical or highs incorrect what i

820
00:37:05,440 --> 00:37:08,160
did get incorrect were medium and lows

821
00:37:08,160 --> 00:37:10,079
and so whenever there was like supposed

822
00:37:10,079 --> 00:37:11,599
to be a medium for some reason we would

823
00:37:11,599 --> 00:37:14,000
come back as a low or if it was a low

824
00:37:14,000 --> 00:37:15,359
vice versa

825
00:37:15,359 --> 00:37:16,720
and i should have included screenshots

826
00:37:16,720 --> 00:37:19,839
of that and i didn't but

827
00:37:20,800 --> 00:37:23,680
it's the variation is not extreme it's

828
00:37:23,680 --> 00:37:26,320
not like if you see low then you know

829
00:37:26,320 --> 00:37:28,000
it's supposed to be a critical i did not

830
00:37:28,000 --> 00:37:30,079
see that in any of the testing

831
00:37:30,079 --> 00:37:31,119
but

832
00:37:31,119 --> 00:37:33,520
it's a possibility you mean if you share

833
00:37:33,520 --> 00:37:37,640
the terminology yeah

834
00:37:54,480 --> 00:37:58,359
yeah that would be a huge issue

835
00:38:04,000 --> 00:38:06,240
i've got a lot of fantastic yes i did

836
00:38:06,240 --> 00:38:07,359
actually

837
00:38:07,359 --> 00:38:11,799
yeah i included um everything

838
00:38:22,800 --> 00:38:25,040
i don't think that was included into it

839
00:38:25,040 --> 00:38:26,720
i think it's just mainly like you're

840
00:38:26,720 --> 00:38:28,640
looking at the most uh common

841
00:38:28,640 --> 00:38:30,800
occurrences of words and

842
00:38:30,800 --> 00:38:32,400
the assumption that you're making is

843
00:38:32,400 --> 00:38:33,920
that there are certain words that are

844
00:38:33,920 --> 00:38:35,440
more commonly

845
00:38:35,440 --> 00:38:36,960
reported in like a critical

846
00:38:36,960 --> 00:38:39,280
vulnerability versus an informational or

847
00:38:39,280 --> 00:38:41,599
low but it does not encounter like

848
00:38:41,599 --> 00:38:44,400
changes like if there's you know

849
00:38:44,400 --> 00:38:46,079
um you know something happens and you're

850
00:38:46,079 --> 00:38:47,520
like oh actually we have to like bump

851
00:38:47,520 --> 00:38:50,000
this up to a critical it doesn't include

852
00:38:50,000 --> 00:38:52,800
that except for the fact that if you

853
00:38:52,800 --> 00:38:54,320
have already

854
00:38:54,320 --> 00:38:56,320
experienced that multiple times and

855
00:38:56,320 --> 00:38:57,680
maybe there's something consistently

856
00:38:57,680 --> 00:38:59,680
trapped in the vulnerability tickets

857
00:38:59,680 --> 00:39:01,520
then perhaps like those keywords will

858
00:39:01,520 --> 00:39:04,079
pop up and that could help you to

859
00:39:04,079 --> 00:39:08,920
consider like you know that pattern

860
00:39:25,920 --> 00:39:29,440
so you know that's an example

861
00:39:42,480 --> 00:39:45,839
it's also this process is bespoke or to

862
00:39:45,839 --> 00:39:48,160
your organization so it would also

863
00:39:48,160 --> 00:39:49,760
include um

864
00:39:49,760 --> 00:39:51,040
you know if something like that ever

865
00:39:51,040 --> 00:39:52,000
occurred

866
00:39:52,000 --> 00:39:54,000
and so the whole point of this is like

867
00:39:54,000 --> 00:39:55,920
you're repurposing information and

868
00:39:55,920 --> 00:39:57,839
decision making that applies to your

869
00:39:57,839 --> 00:40:00,000
organization versus just

870
00:40:00,000 --> 00:40:03,119
like arbitrary like you know generic um

871
00:40:03,119 --> 00:40:05,680
severity distribution and so if you have

872
00:40:05,680 --> 00:40:07,760
like a history of like you know nfs

873
00:40:07,760 --> 00:40:10,160
showing up and then it getting bumped up

874
00:40:10,160 --> 00:40:12,160
after a pen test and that is tracked

875
00:40:12,160 --> 00:40:13,599
within vulnerability tickets and that

876
00:40:13,599 --> 00:40:15,920
could very well be something that comes

877
00:40:15,920 --> 00:40:18,000
up and offers you a more

878
00:40:18,000 --> 00:40:20,079
accurate prediction but if it's never

879
00:40:20,079 --> 00:40:21,920
been there and that terminology is not

880
00:40:21,920 --> 00:40:22,880
present

881
00:40:22,880 --> 00:40:24,800
then yeah i mean that's where

882
00:40:24,800 --> 00:40:26,960
that deviation would occur so definitely

883
00:40:26,960 --> 00:40:29,359
correct

884
00:40:29,839 --> 00:40:32,000
so

885
00:40:36,839 --> 00:40:42,000
um and it matches like 90 percent match

886
00:40:42,319 --> 00:40:43,520
score

887
00:40:43,520 --> 00:40:45,440
for this one

888
00:40:45,440 --> 00:40:48,160
if i then apply on this one cycle can

889
00:40:48,160 --> 00:40:50,400
identify

890
00:40:50,400 --> 00:40:53,839
my knowledge

891
00:41:17,440 --> 00:41:20,720
is a critical but

892
00:41:34,560 --> 00:41:37,630
[Music]

893
00:41:43,760 --> 00:41:47,319
because of those

894
00:41:54,160 --> 00:41:56,400
so my response is probably

895
00:41:56,400 --> 00:41:59,040
a bit junior but if i were

896
00:41:59,040 --> 00:42:01,839
to discover new information then i would

897
00:42:01,839 --> 00:42:04,960
probably restart the um

898
00:42:04,960 --> 00:42:07,359
the collection of features the feature

899
00:42:07,359 --> 00:42:09,920
extraction and then rebuild

900
00:42:09,920 --> 00:42:12,560
um the header rebuild the the number of

901
00:42:12,560 --> 00:42:14,079
headers because at that point there

902
00:42:14,079 --> 00:42:16,400
might be new terminology that comes up

903
00:42:16,400 --> 00:42:18,560
yeah and then i would re-run

904
00:42:18,560 --> 00:42:20,800
um the natural language through it again

905
00:42:20,800 --> 00:42:22,640
and then vectorize it and then see if

906
00:42:22,640 --> 00:42:25,920
there's um improved results

907
00:42:25,920 --> 00:42:27,280
so that's probably how we do it it's a

908
00:42:27,280 --> 00:42:28,640
long process

909
00:42:28,640 --> 00:42:32,759
yeah you would definitely have to uh

910
00:42:50,880 --> 00:42:53,839
i'm guessing logs

911
00:42:55,520 --> 00:42:59,319
yeah i would say logs

912
00:43:14,480 --> 00:43:16,720
but just that the machine learning model

913
00:43:16,720 --> 00:43:18,160
was able to

914
00:43:18,160 --> 00:43:19,440
cut out

915
00:43:19,440 --> 00:43:23,640
the work of this other

916
00:43:26,880 --> 00:43:28,560
i'm sorry could you repeat that please i

917
00:43:28,560 --> 00:43:30,560
didn't quite understand

918
00:43:30,560 --> 00:43:32,720
so

919
00:43:32,880 --> 00:43:36,800
my use case is that right now

920
00:43:52,839 --> 00:43:56,000
yeah that's the goal

921
00:43:56,000 --> 00:43:57,760
yeah that's definitely the goal because

922
00:43:57,760 --> 00:43:59,280
then you i mean you'd have all this

923
00:43:59,280 --> 00:44:00,960
historical knowledge and you're running

924
00:44:00,960 --> 00:44:02,640
it through this over and over again yeah

925
00:44:02,640 --> 00:44:04,960
absolutely there's like that compounding

926
00:44:04,960 --> 00:44:08,200
that's occurring

927
00:44:18,730 --> 00:44:21,809
[Music]

928
00:44:24,640 --> 00:44:26,720
right

929
00:44:26,720 --> 00:44:29,720
um

930
00:45:15,280 --> 00:45:18,280
is

931
00:45:20,240 --> 00:45:22,720
oh um

932
00:45:22,720 --> 00:45:24,880
there is tribal knowledge at this point

933
00:45:24,880 --> 00:45:26,880
so for this project i had to use public

934
00:45:26,880 --> 00:45:28,720
data and that's why

935
00:45:28,720 --> 00:45:31,760
it kind of looks like public data but

936
00:45:31,760 --> 00:45:33,440
the theory is that you would be using

937
00:45:33,440 --> 00:45:34,640
your own

938
00:45:34,640 --> 00:45:36,720
vulnerability tickets and those tickets

939
00:45:36,720 --> 00:45:38,560
within your orb would have tribal

940
00:45:38,560 --> 00:45:40,640
knowledge and as a result the features

941
00:45:40,640 --> 00:45:42,480
that are extracted would be completely

942
00:45:42,480 --> 00:45:44,720
like like they would be whatever your

943
00:45:44,720 --> 00:45:46,720
organization uses

944
00:45:46,720 --> 00:45:48,160
so if you have like some special term

945
00:45:48,160 --> 00:45:49,760
for critical or you have some special

946
00:45:49,760 --> 00:45:51,280
term for something else and that's like

947
00:45:51,280 --> 00:45:53,440
consistently tracked then that would

948
00:45:53,440 --> 00:45:55,839
become a data feature and then you would

949
00:45:55,839 --> 00:45:57,760
count the occurrences of that and

950
00:45:57,760 --> 00:46:00,480
hopefully that would align with um you

951
00:46:00,480 --> 00:46:02,000
know the common severity level that's

952
00:46:02,000 --> 00:46:04,720
been detected

953
00:46:04,880 --> 00:46:06,560
does that kind of make sense

954
00:46:06,560 --> 00:46:09,560
yeah

955
00:46:51,280 --> 00:46:55,240
thank you thank you

956
00:47:01,359 --> 00:47:02,560
um

957
00:47:02,560 --> 00:47:04,720
yes there's more things i want to add to

958
00:47:04,720 --> 00:47:06,880
it though because it's still relatively

959
00:47:06,880 --> 00:47:08,560
elementary

960
00:47:08,560 --> 00:47:10,400
so i have this whole backlog but for

961
00:47:10,400 --> 00:47:12,480
this presentation i had to cut off and

962
00:47:12,480 --> 00:47:14,720
just like you know post what i had

963
00:47:14,720 --> 00:47:16,400
uh but if i do post

964
00:47:16,400 --> 00:47:18,160
because it's data science related i'll

965
00:47:18,160 --> 00:47:20,240
probably put it on kaggle

966
00:47:20,240 --> 00:47:21,760
um and then it'll just you know all the

967
00:47:21,760 --> 00:47:23,359
code and everything will be you know

968
00:47:23,359 --> 00:47:25,200
within jupiter notebooks and the data

969
00:47:25,200 --> 00:47:27,119
set would be available as well it's a

970
00:47:27,119 --> 00:47:29,280
pretty big data set i web script over

971
00:47:29,280 --> 00:47:31,200
150 000

972
00:47:31,200 --> 00:47:33,680
cves and you know the other published

973
00:47:33,680 --> 00:47:35,040
works were

974
00:47:35,040 --> 00:47:36,880
you know definitely less than that i

975
00:47:36,880 --> 00:47:39,040
think like well i don't remember but it

976
00:47:39,040 --> 00:47:41,520
was almost half so it's it's a lot that

977
00:47:41,520 --> 00:47:43,440
i was able to work with and i think that

978
00:47:43,440 --> 00:47:45,839
is really the reason why the accuracy

979
00:47:45,839 --> 00:47:49,040
percentage yeah went up

980
00:47:49,040 --> 00:47:51,040
yeah

981
00:47:51,040 --> 00:47:53,040
uh yeah

982
00:47:53,040 --> 00:47:54,319
thanks for attending though and thanks

983
00:47:54,319 --> 00:47:57,720
for all the questions

